Journal Artificial Intelligence Research 54 (2015) 535-592

Submitted 07/15; published 12/15

Pay-As-You-Go Description Logic Reasoning
Coupling Tableau Saturation Procedures
Andreas Steigmiller
Birte Glimm

andreas.steigmiller@uni-ulm.de
birte.glimm@uni-ulm.de
Institute Artificial Intelligence, University Ulm, Germany

Abstract
Nowadays, saturation-based reasoners OWL EL profile Web Ontology
Language able handle large ontologies SNOMED efficiently. However,
currently unclear saturation-based reasoning procedures extended
expressive Description Logics SROIQthe logical underpinning current
second iteration Web Ontology Language. Tableau-based procedures,
hand, limited specific Description Logic languages OWL profiles,
even highly optimised tableau-based reasoners might efficient enough handle large
ontologies SNOMED. paper, present approach tightly coupling
tableau- saturation-based procedures implement OWL DL reasoner
Konclude. detailed evaluation shows combination significantly improves
reasoning performance wide range ontologies.

1. Introduction
current version Web Ontology Language (OWL 2) (W3C OWL Working Group,
2009) based expressive Description Logic (DL) SROIQ (Horrocks, Kutz,
& Sattler, 2006). Sound complete tableau algorithms, easily extensible
adaptable, typically used handle (standard) reasoning tasks. Moreover, use
wide range optimisation techniques allows handling many expressive, real-world
ontologies. Since standard reasoning tasks SROIQ N2EXPTIME-complete worstcase complexity (Kazakov, 2008), is, however, surprising larger ontologies easily
become impractical existing systems.
contrast, OWL 2 profiles define language fragments SROIQ
standard reasoning tasks tractable specialised reasoning procedures available.
example, OWL 2 EL profile based DL EL++ , efficiently
handled completion- consequence-based reasoning procedures (Baader, Brandt, &
Lutz, 2005; Kazakov, 2009). saturation algorithms also extended handle
expressive DLs Horn-SHIQ (Kazakov, 2009) often able
outperform general tableau algorithms. Even saturation procedures DLs
non-deterministic language features, ALCI (Simanck, Kazakov, & Horrocks, 2011),
ALCH (Simanck, Motik, & Horrocks, 2014), SHIQ (Bate, Motik, Cuenca Grau, Simanck,
& Horrocks, 2015), developed implementations reasoning
systems show remarkable performance. particular, allow one-pass handling
several reasoning tasks classification (i.e., task arranging classes
ontology hierarchy), whereas idea tableau procedures based pairwise
testing individual class subsumptions. Although optimised classification algorithms
c
2015
AI Access Foundation. rights reserved.

fiSteigmiller & Glimm

developed tableau-based reasoning systems (Baader, Hollunder, Nebel, Profitlich,
& Franconi, 1994; Glimm, Horrocks, Motik, Shearer, & Stoilos, 2012), still use
multitude separate consistency tests order decide subsumption relations. Since
complete handling DLs providing disjunctions, cardinality restrictions, inverse
roles causes several difficulties, saturation procedures yet extended
expressive DLs SROIQ. Hence, less efficient tableau-based reasoning
systems currently used handle ontologies expressive language
features.
Unfortunately, combination tableau algorithms saturation procedures
straightforward since techniques work quite differently. Hence, ontology engineers
decide whether use restricted features certain language fragments
ontologies handled specialised reasoners saturation-based
procedures face possible performance losses using general reasoning
systems based tableau algorithms. especially unfavourable language
features required axioms ontology. case, completeness
longer ensured specialised procedures, fully-fledged, tableau-based
reasoners possibly efficient enough. Ideally, reasoning systems better pay-asyou-go behaviour could used, part ontology affected
axioms outside tractable fragment still handled efficiently. led recent
development approaches combine saturation procedures fully-fledged reasoners
black box manner (Armas Romero, Cuenca Grau, & Horrocks, 2012; Song, Spencer,
& Du, 2012; Zhou, Nenov, Cuenca Grau, & Horrocks, 2014; Zhou, Cuenca Grau, Nenov,
Kaminski, & Horrocks, 2015). approaches try delegate much work possible
specialised efficient reasoner, allows reducing workload
fully-fledged tableau algorithm, still guaranteeing completeness.
paper, present much tighter coupling saturation tableau algorithms, whereby performance improvements achieved. introducing
preliminaries (Section 2), present saturation procedure adapted data
structures tableau algorithm (Section 3). allows easily passing information
saturation tableau algorithm within reasoning system. Moreover, saturation partially handles features expressive DLs order efficiently
derive many consequences possible (Section 3.1). show parts
ontology identified saturation procedure possibly incomplete
necessary fall-back tableau procedure (Section 3.2). Subsequently,
present several optimisations based passing information saturation
tableau algorithm (Section 4) back (Section 5). Finally, discuss related work
(Section 6) present results detailed evaluation including comparisons
approaches state-of-the-art reasoners (Section 7) conclude (Section 8).
paper based previous conference publication (Steigmiller, Glimm, & Liebig,
2014a), contains significantly extended explanations, additional examples, proofs
correctness integrated saturation procedure incompleteness detection.
Due space limitations conference publication, information passing
tableau algorithm saturation procedure could sketched, whereas
described detail paper. Moreover, coupling saturation procedure
extended paper consider handle language features DL
536

fiPay-As-You-Go Description Logic Reasoning

SROIQ. Furthermore, show new detailed evaluation based
updated implementation compare results approaches fullyfledged specialised reasoners combined.

2. Preliminaries
order describe techniques detail, first give, based original presentation Horrocks et al. (2006), brief introduction DL SROIQ section.
detailed introduction DLs, refer Description Logic Handbook (Baader,
Calvanese, McGuinness, Nardi, & Patel-Schneider, 2007). Subsequently, describe tableau algorithm typically used reasoning systems also refer work
Horrocks et al. (2006) details.
2.1 Description Logic SROIQ
first define syntax roles, concepts (also called classes), individuals,
go axioms ontologies/knowledge bases. Additionally, sketch typically
used restrictions combination different axioms, necessary ensure
decidability many inference problems SROIQ. Note describe
details restrictions since well-known DL literature (Horrocks et al.,
2006) particularly relevant proposed optimisation techniques. Subsequently,
define semantics components.
Definition 1 (Syntax Individuals, Concepts, Roles). Let NC , NR , NI countable, infinite, pairwise disjoint sets concept names, role names, individual
names, respectively. call = (NC , NR , NI ) signature. set Rols() SROIQroles (or roles short) NR {r | r NR }, role form r
called inverse role r. Since inverse relation roles symmetric, define
function inv, returns inverse role and, therefore, consider
roles r . r NR , let inv(r) = r inv(r ) = r.
set SROIQ-concepts (or concepts short) smallest set built
inductively symbols using following grammar, NI , n IN0 ,
NC , r Rols():
C ::= > | | | {a} | C | C1 u C2 | C1 C2 | r.C | r.C | r.Self | > n r.C | 6 n r.C.
roles, concepts, individuals used build ontology axioms follows:
Definition 2 (Syntax Axioms Ontologies). C, concepts, general concept
inclusion (GCI) axiom expression C v D. finite set GCIs called TBox.
role inclusion (RI) axiom expression form u v r, r role u
composition roles, i.e., u = s1 . . . sn roles s1 , . . . , sn n 1. r,
roles, role assertion (RA) axiom form Disj(r, s). RBox finite set RIs
RAs. Given RBox R, use v transitive-reflexive closure r v
inv(r) v inv(s) axioms R. call role r sub-role super-role r
r v s.
(ABox) assertion expression form C(a) r(a, b), C concept,
r role, a, b NI individual names. ABox finite set assertions.
537

fiSteigmiller & Glimm

knowledge base ontology K tuple (T , R, A) TBox, R RBox,
ABox.
Note, also possible allow types RAs RBox, e.g., axioms
specify roles asymmetric, irreflexive, transitive, reflexive, symmetric. However,
axioms expressed indirectly ways and, therefore, omit presentation
here. example, axiom form Refl(r), role r interpreted
reflexive, encoded axioms r0 v r > v r0 .Self.1 Analogously,
allow frequently used ABox assertions since, presence nominals,
ABox assertion also expressed GCIs (which also utilise eliminate
ABox assertions simplify presentation algorithms). Furthermore, SROIQ usually
allows usage universal role u, u also simulated fresh transitive,
reflexive, symmetric super role, i.e., role implied roles.
following, use K also abbreviation collection axioms knowledge
base. example, write C v K instead C v K.
order ensure decidability (Horrocks, Sattler, & Tobies, 1999; Horrocks & Sattler,
2004), simple roles allowed concepts form > n r.C, 6 n r.C, r.Self
axioms form Disj(r, s), where, roughly speaking, role simple
implied RI uses role composition. Furthermore, RBox regular,
i.e., RI axioms allowed limited form (Horrocks & Sattler, 2004), restricts
cyclic dependencies RIs.
Next, define semantics concepts go semantics axioms
ontologies/knowledge bases.
Definition 3 (Semantics Individuals, Concepts, Roles). interpretation =
(I , ) consists non-empty set , domain I, function , maps
every concept name NC subset AI , every role name r NR binary
relation rI , every individual name NI element aI .

role name r NR , interpretation inverse role (r ) consists pairs
h, 0 h 0 , rI .
interpretation I, semantics SROIQ-concepts signature defined function follows:
>I
(C)I
(r.Self)I
(r.C)I
(r.C)I
(6 n r.C)I
(> n r.C)I

=
=
=
=
=
=
=


\ C
{
{
{
{
{

=
({a})I =



(C u D) = C
(C D)I =

| h, r }
| h, 0 rI , 0 C }
| h, 0 rI 0 C }
| ]{ 0 | h, 0 rI 0 C } n}
| ]{ 0 | h, 0 rI 0 C } n},

{aI }
C DI

]M denotes cardinality set .
Finally, define semantics ontologies/knowledge bases.
1. Note use fresh sub-role r0 r axiom > v r0 .Self since r might complex,
r.Self expressions allowed simple roles.

538

fiPay-As-You-Go Description Logic Reasoning

Definition 4 (Semantics Axioms Ontologies). Let = (I , ) interpretation,
satisfies TBox/RBox axiom ABox assertion , written |=
1. GCI C v C DI ,
2. RI s1 . . . sn v r sI1 . . . sIn rI , denotes composition
binary relations sI1 . . . sIn ,
3. RA form Disj(r, s) rI sI = ,
4. ABox assertion C(a) aI C ,
5. ABox assertion r(a, b) haI , bI rI .
satisfies TBox (RBox R, ABox A) satisfies GCI (each RI/RA axiom
R, assertion A). say satisfies K = (T , R, A) satisfies , R,
A. case, say model K write |= K. say K
consistent K model.
2.2 Normalisation Preprocessing
remainder assume knowledge base normalised following form:
1. TBox contains axioms form A1 u A2 v C H v C H = A,
H = {a}, H = >, C negation normal form A, A1 , A2 denote concept
names.
2. RBox contains RAs simple role inclusion axioms form r1 v r2 .
3. ABox empty.
assumption without loss generality. Item 1: concept transformed
equivalent one negation normal form (NNF) pushing negation inwards, making
use de Morgans laws duality existential universal restrictions,
at-most at-least cardinality restrictions (Horrocks, Sattler, & Tobies, 2000).
use nnf(C) denote equivalent concept C NNF. Furthermore, GCI C v
correspond normal form equivalently written > v
nnf(C tD). rewriting GCIs creates (possibly many) disjunctions, potentially
causes lot non-determinism reasoning procedure and, therefore, easily decreases
reasoning performance. counteract this, preprocessing step called absorption
often used (Horrocks & Tobies, 2000; Hudek & Weddell, 2006; Steigmiller, Glimm, & Liebig,
2013, 2014b; Tsarkov & Horrocks, 2004), tries rewrite axioms possibly
several simpler concept inclusion axioms. example, instead treating u r.B v C
> v r.(B) C, sophisticated absorption algorithm avoid non-determinism
rewriting axiom B v r .F u F v C, F fresh atomic
concept used preserve semantics original axiom. Item 2: RIs
use compositions eliminated using encoding based automata (Horrocks &
Sattler, 2004) regular expressions (Simanck, 2012). Note explicit encodings
propagations complex roles might blow knowledge base exponentially,
539

fiSteigmiller & Glimm

cannot avoided worst-case, i.e., could try delay actual
reasoning process (Kazakov, 2008) indeed utilised many reasoners (although
blow seems hardly caused real-world ontologies). Item 3, C(a) (r(a, b))
equivalently expressed {a} v C ({a} v r.{b}).
2.3 Tableau Algorithm SROIQ
Model construction calculi, tableaux, decide consistency knowledge base K
trying construct abstraction model K, so-called completion graph.
following, describe, based original presentation SROIQ tableau algorithm (Horrocks et al., 2006), model construction process used data structures,
beginning completion graphs.
Definition 5 (Completion Graph). concept C, use sub(C) denote set
sub-concepts C (including C). Let K normalised SROIQ knowledge base
let Cons(K) set concepts occurring TBox K, i.e., Cons(K) = {C, |
C v K}. define closure clos(K) K as:
clos(K) = {C sub(D) | Cons(K)} {nnf(C) | C sub(D), Cons(K)}.
). node v V
completion graph K directed graph G = (V, E, L, 6=
labelled set L(v) fclos(K),
fclos(K) = clos(K) {6 r.C |6 n r.C clos(K) n}.
edge hv, v 0 E labelled set L(hv, v 0 i) Rols(K), Rols(K)
used keep track inequalities
roles occurring K. symmetric binary relation 6=
nodes V .
following, often use r L(hv1 , v2 i) abbreviation hv1 , v2 E
r L(hv1 , v2 i).
Definition 6 (Successor, Predecessor, Neighbour). hv1 , v2 E, v2 called
successor v1 v1 called predecessor v2 . Ancestor transitive closure
predecessor, descendant transitive closure successor. node v2 called ssuccessor node v1 r L(hv1 , v2 i) r sub-role s; v2 called s-predecessor
v1 v1 s-successor v2 . node v2 called neighbour (s-neighbour) node
v1 v2 successor (s-successor) v1 v1 successor (inv(s)-successor) v2 .
role r node v V , define set vs r-neighbours concept C
label, written mneighbs(v, r, C), {v 0 V | v 0 r-neighbour v C L(v 0 )}.
Note, many inference problems DL SROIQ easily reduced consistency
checking and, therefore, indirectly handled although often consistency
checking reasoning task specified tableau algorithm. example, order
test satisfiability concept C, introduce fresh individual assert
concept C axiom form {a} v C. nodes represent (fresh)
individuals typically called root nodes.
order test consistency knowledge base, completion graph initialised
creating one node individual/nominal input knowledge base. particular,
540

fiPay-As-You-Go Description Logic Reasoning

v1 , . . . , v` nodes individuals a1 , . . . , a` K, create initial
completion graph G = ({v1 , . . . , v` }, , L, ) add individual ai nominal {ai }
concept > label vi , i.e., L(vi ) = {{ai }, >} 1 `.
tableau algorithm works decomposing/unfolding concepts completion
graph set expansion rules (see Table 1). rule application add new
concepts node labels and/or new nodes edges completion graph, thereby
explicating structure model input knowledge base. rules repeatedly
applied either graph fully expanded (no rules applicable), case
graph used construct model witness consistency K,
obvious contradiction (called clash) discovered (e.g., C C node label),
proving completion graph correspond model. input knowledge
base K consistent rules (some non-deterministic) applied
build fully expanded clash-free completion graph.
) knowledge base K contains
Definition 7 (Clash). completion graph G = (V, E, L, 6=
clash nodes v w
1. L(v),
2. {C, nnf(C)} L(v) concept C,
3. v r-neighbour v r.Self L(v),
4. Disj(r, s) K w r- s-neighbour v,
5. concept 6 n r.C L(v) {w1 , . . . , wn+1 } mneighbs(v, r, C)
wj 1 < j n + 1,
wi 6=
w.
6. {a} L(v) L(w) v 6=
Unrestricted application -rule >-rule lead introduction infinitely
many new tableau nodes and, thus, prevent calculus terminating. counteract
that, cycle detection technique called (pairwise) blocking (Horrocks & Sattler, 1999)
used restricts application rules. apply blocking, distinguish blockable
nodes nominal nodes, either original nominal knowledge base
new nominal introduced calculus label.
Definition 8 (Pairwise Blocking). node blocked either directly indirectly
blocked. node v indirectly blocked ancestor v blocked; v predecessor
v 0 directly blocked exists ancestor node w v predecessor w0
1. v, v 0 , w, w0 blockable,
2. w, w0 blocked,
3. L(v) = L(w) L(v 0 ) = L(w0 ),
4. L(hv 0 , vi) = L(hw0 , wi).
case, say w directly blocks v w blocker v.
541

fiSteigmiller & Glimm

v1 -rule

v2 -rule







u-rule
t-rule
-rule


Self-rule

-rule

ch-rule

>-rule




1.
2.


6-rule

1.
2.


o-rule



NN-rule 1.
2.



H L(v), H v C K H = A, H = {a}, H = >, C
/ L(v),
v indirectly blocked
L(v) = L(v) {C}
{A1 , A2 } L(v), A1 u A2 v C K, C
/ L(v),
v indirectly blocked
L(v) = L(v) {C}
C1 u C2 L(v), v indirectly blocked, {C1 , C2 } 6 L(v)
L(v) = L(v) {C1 , C2 }
C1 C2 L(v), v indirectly blocked, {C1 , C2 } L(v) =
L(v 0 ) = L(v 0 ) {H} H {C1 , C2 }
r.C L(v), v blocked,
v r-neighbour v 0 C L(v 0 )
create new node v 0 edge hv, v 0
L(v 0 ) = {>, C} L(hv, v 0 i) = {r}
r.Self L(v), v blocked, v r-neighbour v
create new edge hv, vi L(hv, vi) = {r}
r.C L(v), v indirectly blocked,
r-neighbour v 0 v C
/ L(v 0 )
0
0
L(v ) = L(v ) {C}
6 n r.C L(v), v indirectly blocked,
r-neighbour v 0 v {C, nnf(C)} L(v 0 ) =
L(v 0 ) = L(v 0 ) {H} H {C, nnf(C)}
> n r.C L(v), v blocked,
vj
n r-neighbours v1 , . . . , vn v C L(vi ) vi 6=
1 < j n, v1 , . . . , vn blocked v nominal node
create n new nodes v1 , . . . , vn L(hv, vi )i = {r}, L(vi ) = {>, C}
vj 1 < j n.
vi 6=
6 n r.C L(v), v indirectly blocked,
]mneighbs(v, r, C) > n two r-neighbours v1 , v2 v
v2
C (L(v1 ) L(v2 )) v1 6=
a. v1 nominal node, merge(v2 , v1 )
b. else v2 nominal node ancestor v1 , merge(v1 , v2 )
c. else merge(v2 , v1 )
v0
two nodes v, v 0 {a} (L(v) L(v 0 )) v 6=
merge(v, v 0 )
6 n r.C L(v), v nominal node, blockable
r-neighbour v 0 v C L(v 0 ) v successor v 0 ,
1 n, (6 r.C) L(v),
exist nominal r-neighbours v1 , . . . , vm v
vj 1 < j
C L(vi ) vi 6=
1. guess 1 n L(v) = L(v) {6 r.C}
0 L(hv, v 0 i) = {r},
2. create new nodes v10 , . . . , vm

0
L(vi ) = {>, C, {ai }} ai NI new G K,
v 0 1 < j m.
vi0 6=
j

Table 1: Tableau expansion rules normalised SROIQ knowledge bases
542

fiPay-As-You-Go Description Logic Reasoning

expansion sometimes necessary merge two nodes delete (prune)
part completion graph (Horrocks & Sattler, 2007). Roughly speaking, node
w merged node v, e.g., application 6-rule, written merge(w, v),
add L(w) L(v), move edges leading w lead v move
edges leading w nominal nodes lead v nominal
nodes; remove w (and blockable sub-trees w) completion graph,
written prune(w), prevent rule application nodes.
Note, order ensure termination tableau algorithm, principle necessary
apply certain crucial rules higher priority. example, o-rule applied
highest priority NN-rule applied 6-rule. priority
rules relevant long applied lower priority
crucial rules.

3. Saturation Compatible Tableau Algorithms
section, describe saturation method adaptation completionbased procedure (Baader et al., 2005) generates data structures compatible usage within fully-fledged tableau algorithm. Roughly speaking,
saturation approximates completion graphs compressed form and, therefore, directly
allows extraction transfer results saturation tableau algorithm.
precise, ensure saturation generates nodes are, similarly
nodes completion graphs, labelled sets concepts. saturated labels
used initialise labels new nodes completion graphs block processing
successors. Moreover, cases, directly possible build model data
structures saturation, makes construction completion graphs
tableau algorithm unnecessary.
Note, adapted saturation method designed cover certain OWL 2 profile specific DL language. contrast, saturate parts knowledge bases
easily supported efficient algorithm (see Section 3.1), i.e., simply
ignore unsupported concept constructors process partially, afterwards
(see Section 3.2), dynamically detect parts completely handled
saturation. Hence, results saturation possibly incomplete, since
know incomplete, use results saturation
appropriately.
easy integration highly optimised tableau procedure, (usually simpler)
saturation procedure adapted work (normalised preprocessed) knowledge base data structures tableau algorithm (but, principle, also opposite
direction would possible). enables, example, direct use node labels
saturation tableau algorithm. coupling technique, use good
absorption algorithm crucial since saturation handles deterministic parts
knowledge base.
3.1 Saturation Based Tableau Rules
adapted saturation method generates so-called saturation graphs, approximate
completion graphs compressed form, e.g., reusing nodes.
543

fiSteigmiller & Glimm

Definition 9 (Saturation Graph). saturation graph knowledge base K directed
graph = (V, E, L) nodes V {vC | C fclos(K)}. node vC V labelled
set L(v) fclos(K) L(vC ) {>, C}. call vC representative node
concept C. edge hv, v 0 E labelled set L(hv, v 0 i) Rols(K).
Obviously, saturation graph data structure similar completion
-relation, omitted since
graph. major difference is, however, missing 6=
saturation designed completely handle cardinality restrictions. Furthermore,
node saturation graph representative node specific concept,
allows reusing nodes successors. example, instead creating new successors
existential restrictions, reuse representative node existentially restricted
concept successor.
Since nodes, edges, labels used completion graphs, use terms
(r-)neighbour, (r-)successor, (r-)predecessor, ancestor, descendant analogously. Please
note, however, nodes saturation graph several predecessors due
reuse nodes, whereas completion graphs, nominal nodes several
predecessors.
initialise saturation graph representative nodes concepts
saturated. example, satisfiability concept C tested,
add node vC label L(vC ) = {>, C} saturation graph.
later also interested saturation concept D, simply extend existing
saturation graph vD . knowledge bases contain nominals, initialise
saturation graph node v{a} L(v{a} ) = {>, {a}} nominal {a} occurring
knowledge base. saturation simply applies rules depicted Table 2
saturation graph.
Definition 10 (Saturation). Let = (V, E, L) saturation graph knowledge base
K, saturation exhaustively applies rules Table 2 S. saturation
graph called fully saturated rules applicable. use function
saturate denote saturation saturation graph S, i.e., saturate(S) returns 0 ,
0 fully saturated extension S.
Note saturation rule refers representative node concept C
node vC yet exist, assume saturation graph automatically
extended node. Although saturation rules similar corresponding
expansion rules tableau algorithm, differences. example,
number nodes limited number (sub-)concepts occurring knowledge
base due reuse nodes satisfying existentially restricted concepts. Consequently,
saturation terminates since rules applied add new concepts
roles node edge labels. Moreover, cycle detection blocking required,
makes rule application fast. Note also -rule propagates concepts
predecessors node. restriction necessary order allow reuse
nodes existentially restricted concepts.
efficiently derive many sound inferences possible, saturation rules
Table 2 partially support expressive features SROIQ. full saturation,
check saturation graph possibly incomplete. Although often
544

fiPay-As-You-Go Description Logic Reasoning

v1 -rule:

v2 -rule:

u-rule:

-rule:


-rule:


t-rule:

>-rule:

Self-rule:

o-rule:


-rule:

H L(v), H v C K H = A, H = {a}, H = >, C
/ L(v)
L(v) = L(v) {C}
{A1 , A2 } L(v), A1 u A2 v C K, C
/ L(v)
L(v) = L(v) {C}
C1 u C2 L(v) {C1 , C2 } 6 L(v)
L(v) = L(v) {C1 , C2 }
r.C L(v) r
/ L(hv, vC i)
L(hv, vC i) = L(hv, vC i) {r}
r.C L(v), inv(r)-predecessor v 0 v, C
/ L(v 0 )
L(v 0 ) = L(v 0 ) {C}
/ L(v)
C1 C2 L(v), L(vC1 ) L(vC2 ),
L(v) = L(v) {D}
> n r.C L(v) n 1 r
/ L(hv, vC i)
L(hv, vC i) = L(hv, vC i) {r}
r.Self L(v) v r-successor v
L(hv, vi) = L(hv, vi) {r}
{a} L(v),
/ L(v),
L(v{a} ) descendant v 0 v {{a}, D} L(v 0 )
L(v) = L(v) {D}


/ L(v),
1. {C, nnf(C)} L(v),
2. v r-successor {r.Self, r .Self} L(v) 6= ,
3. v 0 r-successor v {a} L(v) L(v 0 )
r.Self L(v) r .Self L(v),
4. {> n r.C, 6 s.D} L(v) n > m, r v s, L(vC ),
5. > n r.C L(v) n > 1 {a} L(vC ),
6. v 0 r-successor v, r v s, Disj(r, s) K,
7. v 0 r-successor v v 00 s-successor v
{a} L(v 0 ) L(v 00 ) Disj(r, s) K,
8. exists successor node v 0 v L(v 0 ),
9. exists node v{a} L(v{a} )
L(v) = L(v) {}

Table 2: Saturation rules (partially) handling SROIQ knowledge bases

several ways integrate support expressive concept constructors, chose
simple one allows partial saturation, implemented
efficiently. instance, t-rule adds concepts implied
disjuncts. Hence, addition concepts rule obviously sound, handling
disjunctions often incomplete. at-least cardinality restrictions, build edges
(possibly reused) successor nodes similarly -rule. Thereby, actual cardinality
ignored, possibly causes incompleteness also at-most cardinality restrictions
related super-roles label. order (partially) handle nominal {a}
label node v, use o-rule adds concepts derived v{a}
545

fiSteigmiller & Glimm

descendant nodes also {a} label (instead merging nodes
tableau procedures). consequence, unsatisfiability concepts form
r.(Au{a})ur.(Au{a}) cannot discovered. simple implementation does, however,
require repeated saturation concepts extended small influences
nominals. tractable complete saturation algorithms nominals
possible (Kazakov, Krotzsch, & Simanck, 2012), many ontologies use nominals
simple way o-rule already sufficient (e.g., using nominals concepts
form r.{a}).
need explicit -rule uses similar conditions ones clashes
completion graphs SROIQ (cf. Definition 7). -rule used handle several
independent concepts one-pass manner within saturation graph distinguish nodes unsatisfiable concepts nodes (possibly) still satisfiable.
-rule detects trivial reasons unsatisfiability C C label
node (Condition 1), also involved cases. Violations regarding concepts
form r.Self r .Self handled Conditions 2 3. former handles
straightforward self-loops, latter handles cases would lead loop
saturation merge neighbouring nodes nominal label. Conditions 4 5 handle problematic cases cardinality restrictions. actual cardinalities
ignored saturation, clear clash would occur completion graph
presence conflicting at-least at-most cardinalities (Condition 4) at-least
number restrictions form > n r.C n > 1, node representing C contains
nominal. latter case, one instance C, nominal, exist model
knowledge base. Conditions 6 7 handle problems Disj(r, s) axioms.
former condition treats trivial case, r-successor also s-successor
due super-role r. latter condition considers saturation
merge nodes nominal label, would merge labels
edges nodes. Note node vC r- s-successor
due node reuse saturation concepts r.C s.C label
node. is, however, problem even presence Disj(r, s) axioms since
required models knowledge base. -rule also propagates ancestor
nodes (Condition 8) and, case occurs label nominal node, propagated
every node since knowledge base inconsistent (Condition 9).
principle possible detect also several kinds clashes incompletely
handled parts saturation (e.g., concept C propagated successor
node v C L(v)), presented conditions -rule, combination
detection incompleteness (see Section 3.2), already sufficient identify potential
causes unsatisfiability. Hence, omit clash conditions ease presentation.
Example 1. Let us assume TBox T1a contains following axioms:
v .B

B v s.{a}

interested satisfiability concept A, saturation graph initialised
representative node A, say vA , L(vA ) = {>, A} v{a} L(v{a} ) =
{>, {a}} representation individual a. v1 -rule (cf. Table 2) applicable
first axiom vA , results addition .B L(vA )
546

fiPay-As-You-Go Description Logic Reasoning


vA vB




vA vB

v{a}

L(vA ) = {>, A, .B}
L(vB ) = {>, B, s.{a}}
L(v{a} ) = {>, {a}}


v{a}

r
L(vA ) = {>, A, .B, B {a}, C}
L(vB ) = {>, B, s.{a}, C, 6 1 s.C}
L(v{a} ) = {>, {a}, C, > 2 r.B}

Figure 1: Generated saturation graphs testing satisfiability w.r.t. T1a (left)
T1c (right) Example 1

application -rule generates node vB L(vB ) = {>, B} -labelled edge
vA vB . Now, v1 -rule applied unfold B label vB s.{a}
-rule applicable. Hence, obtain fully saturated saturation graph
depicted left-hand side Figure 1. Note saturation procedure starts rule
application nodes vA v{a} ; nodes, e.g., vB , created demand.
let T1b = T1a plus axioms:
v B {a}

BvC

{a} v C

v1 -rule extends L(vA ) B {a} L(vB ) well L(v{a} ) C.
t-rule applicable adds concept C label vA C label
representative nodes disjuncts disjunction B {a}. Note although
concept C added node labels, node C created since C used
way requires this.
Finally, let T1c = T1b plus axioms:
B v 6 1 s.C

{a} v > 2 r.B

v1 -rule extends L(vB ) 6 1 s.C L(v{a} ) > 2 r.B. latter addition
triggers >-rule, adds r-edge v{a} vB saturation graph
depicted right-hand side Figure 1 obtained. Note saturation inherently
incomplete. example, saturation rule handles concept 6 1 s.C.
tableau rules would merge vA v{a} since vB one s-successor C
label, possibly leads conclusions saturation misses. cover
incomplete handling nodes detected next section.
suitable absorption technique, saturation usually able derive add
majority concepts would added tableau algorithm equivalent
node. especially case ontologies primarily use features DL EL++
saturation-based procedures particularly well-suited. Since EL++ covers many
important often used constructors (e.g., u, ), saturation already majority
work many ontologies confirmed evaluation Section 7.
3.2 Saturation Status Detection
Similarly saturation procedures, presented method Section 3.1 easily becomes
incomplete expressive DLs. order nevertheless gain much information
547

fiSteigmiller & Glimm

possible saturation, identify nodes saturation possibly incomplete. call nodes critical. principle, nodes detected testing
whether actual tableau rule still applicable. However, since saturate expressive concept constructors partially, approach often conservative. example,
at-least cardinality restrictions form > n r.C n > 1, saturation already
creates reuses successor node concept C and, therefore, consequences
propagated back successor node already considered. Nevertheless,
tableau expansion rule at-least cardinality restriction still applicable, since
created n successors stated pairwise different. is, however,
relevant restrictions limit number allowed r-successors
concept C label. DL SROIQ, limitations possible
nominals at-most cardinality restrictions. Therefore, sufficient check
limitations instead testing whether tableau expansion rules applicable. Similar
relaxations also possible concept constructors.
use rules Table 3 4 detect saturation status saturation graph,
incompletely handled nodes identified information relevant
supporting tableau algorithm extracted. precise, rules applied
saturation graph gather nodes sets , , S! , represents nodes
depend nominals, represents nodes tight at-most restrictions, S!
represents critical nodes potentially completely handled saturation.
order specify saturation status detail, first define number merging
candidates facilitate treatment possibly incompletely handled at-most cardinality
restrictions.
Definition 11 (Merging Candidates). Let = (V, E, L) saturation graph. role
concept D, number merging candidates
Pfor node v V w.r.t. D,
written function ]mcands(v, s, D), defined >n r.CL n
L ={> n r.C L(v) | r v L(vC )}
{> 1 r.C | r.C L(v), r v s, L(vC )}.
at-most cardinality restriction 6 s.D label node v, merging
candidates s-successors concept label. used
C -rule (see Table 3) identify nodes tight at-most restrictions, case
node v at-most cardinality restriction 6 s.D label number
merging candidates v w.r.t. m, i.e., = ]mcands(v, s, D). nodes,
still necessary merge merging candidates, every additional candidate
might require merging and, therefore, nodes cannot used arbitrarily.
Co -rule adds set nodes directly indirectly depend nominals,
i.e., identifies nodes directly nominal label descendant
node nominal label.
rules Table 4 used identify critical nodes saturation procedure
might incomplete, i.e., nodes added set S! follows:
C - Ct -rule identify nodes critical - t-rule
tableau algorithm applicable. Note, C -rule necessary check
548

fiPay-As-You-Go Description Logic Reasoning

C -rule:

Co -rule:


v
/ , 6 s.D L(v), ]mcands(v, s, D) =
= {v}
v
/ either {a} L(v) v successor node v 0 v 0
= {v}

Table 3: Rules detecting nodes tight at-most restrictions nodes nominal
dependency saturation graph

whether concepts propagated successor nodes since propagation
predecessors ensured saturation procedure.
C6-rule checks every node v whether potentially unsatisfied at-most
cardinality restriction form 6 s.D label v, i.e., ]mcands(v, s, D) > m.
Analogously ch-rule tableau algorithm, Cch -rule identifies nodes
incompletely handled s-successor nodes neither nnf(D)
label. addition, consider successors may
merged predecessor. Note checked perspective
predecessors due reuse nodes. Therefore, check C6and Cch -rule node v whether exists inv(s)-successor node v 0
tight at-most restriction s, i.e., 6 s.D L(v 0 ) ]mcands(v, s, D) = m.
v merging candidate, i.e., L(v), would necessary apply
ch-rule v, consider v critical. example, v -successor v 0
{> 3 s.D, 6 3 s.D} L(v 0 ), C6-rule (Cch -rule) identifies v critical
L(v) ({D, nnf(D)} L(v) 6= ).
also need several rules detection incompleteness related nominals.
First, check Coo -rule whether two nodes saturation graph
nominal different concepts label. case,
handling nominal possibly incomplete since merging nodes would also
merge labels. Note saturate several independent concepts
saturation graph, merging nodes nominal label
always necessary. However, detecting merge really required would involve
expensive test. Since many ontologies less expressive DLs use nominals
simple ways, opt simple efficient solution. addition, node v
nominal dependent, i.e., descendant node nominal label, arbitrary consequences could propagated via individuals/nominals. Hence,
Co! -rule adds v set S! critical nodes representative node individual completely handled since cannot guarantee saturation
derived consequences v. contrast, Co6-rule checks possible interactions nominals at-most cardinality restrictions. interaction
handled NN-rule tableau algorithm, cannot easily handled
saturation rather identify nodes critical.
Finally, C -rule marks predecessors critical nodes critical.
549

fiSteigmiller & Glimm

C -rule:

Ct -rule:

C6-rule:

Cch -rule:

C6-rule:

Cch -rule:

Coo -rule:

Co! -rule:

Co6-rule:

C -rule:





v
/ S! , r.C L(v), r-successor v 0 v, C
/ L(v 0 )
S! = S! {v}
v
/ S! , C L(v), {C, D} L(v) =
S! = S! {v}
v
/ S! , 6 s.D L(v), ]mcands(v, s, D) >
S! = S! {v}
v
/ S! , 6 s.D L(v), s-successor v 0 v,
L(v 0 ) {D, nnf(D)} =
S! = S! {v}
v
/ S! , L(v), v 0 inv(s)-successor v, 6 s.D L(v 0 ),
]mcands(v 0 , s, D) =
S! = S! {v}
v
/ S! , v 0 inv(s)-successor v, 6 s.D L(v 0 ),
L(v) {D, nnf(D)} =
S! = S! {v}
v
/ S! , {a} L(v), {a} L(v 0 ), L(v) 6 L(v 0 )
S! = S! {v}
v
/ S! , v , exist node v{a} S!
S! = S! {v}
v
/ S! , v 0 inv(s)-successor v, {a} L(v 0 ), 6 s.D L(v 0 ),
nnf(D)
/ L(v)
S! = S! {v}
v
/ S! , successor v 0 v, v 0 S!
S! = S! {v}

Table 4: Rules detecting incompleteness saturation graph
sets , , S! used define saturation status saturation
graph follows:
Definition 12 (Saturation Status). saturation status saturation graph =
(V, E, L) defined tuple (So , , S! ). use status function creates
exhaustive application rules Table 3 4. node v V critical
v S! , v nominal dependent v , v tight at-most restrictions v .
call v clashed L(v).
Note concept C unsatisfiable representative node vC clashed.
satisfiability C can, however, guaranteed vC critical knowledge
base consistent. Consistency required, concept satisfiable
knowledge base consistent, determined saturation nominal
node critical.
course, satisfiability/completeness saturated concepts considered
context arbitrary concepts handled saturation (e.g., completion graph constructed tableau algorithm), nominal dependency becomes
relevant. particular, new consequences propagated nominal nodes,
550

fiPay-As-You-Go Description Logic Reasoning

nominal dependent nodes saturation graph could affected
considered incompletely handled. Hence, also satisfiability hinges status
nominal nodes node depends on.
problem practice critical node nominal also makes nominal
dependent nodes critical. Hence, easily get many critical nodes ontologies
use nominals saturation cannot completely handle individuals. However,
improve saturation graph initial consistency check tableau algorithm
(see Section 5 details) replacing node labels critical nominal nodes
saturation graph ones obtained completion graph. Although
distinguish deterministically non-deterministically derived concepts labels,
know correspond clash-free fully expanded completion graph and,
therefore, consider critical.
Example 2. Consider TBoxes Example 1. start fully saturated
saturation graph T1a (left-hand side Figure 1). Co -rule Table 3
applicable, identifies nodes nominal dependent adds (iteratively)
, nodes completely handled saturation node critical
tight at-most restrictions.
situation changes extension T1b T1a . Since B {a} L(vA ), neither
disjunct is, vA identified critical (vA S! ) Ct -rule Table 4.
nodes are, however, still completely handled saturation.
Finally, consider extension T1c T1b (right-hand side Figure 1). C -rule
Table 3 adds vB since number merging candidates vB w.r.t.
C 1, i.e., ]mcands(vB , s, C) = 1. used identify nodes critical use vB
-successor merging s-successor vB potentially required.
particular, concept .B L(vA ) problematic connects vB vA
via role s, 6 1 s.C L(vB ), ]mcands(vB , s, C) = 1. Hence, vA already
identified critical due incompletely handled disjunction, C6-rule would add vA
S! . Note, however, vB v{a} still completely handled saturation.
3.3 Correctness
straightforward see saturation rules Table 2 produce sound inferences.
particular, saturation rules add concepts label node
also added tableau algorithm equivalently labelled node completion
graph. termination saturation rules ensured since number nodes
edges well size labels bounded number concepts, roles, size
closure concepts knowledge base. Furthermore, rules applied
add new facts saturation graph. Analogously, application rules
Table 3 4 generation saturation status terminating, rule
application adds node corresponding set (either , , S! ) rules
applicable node already belong corresponding set.
remains show completeness, i.e., show node vD nodes
representing individuals neither critical clashed, build model knowledge base extension non-empty. Note direct transformation
saturation graph completion graph possible since reuse nodes
551

fiSteigmiller & Glimm

saturation graph possibly causes problems certain features SROIQ. example,
two roles r stated disjoint super role r, saturation graph
contain node r- s-successor another node. However, principle,
would possible rebuild completion graph recursively creating corresponding
successor nodes used nodes saturation graph would reach nominal
nodes nodes would blocked.
Given fully saturated saturation graph, nodes representing individuals
neither critical clashed, show completeness non-critical node vD providing
interpretation = (I , ) model knowledge base non-empty
extension D, i.e., DI 6= interpretation witnesses satisfiability D.
ease presentation, assume existentially quantified concepts form r.C
equivalently expressed > 1 r.C. w.l.o.g., since normalised knowledge bases contain
simple roles.
Since occurrence nominal {a} label node vC means node vC
represents element v{a} , need one representative element,
ensure defining suitable equivalence relation nodes saturation graph.
Definition 13 (Canonical Saturation Model). saturation graph = (V, E, L), let
following relation: {(vC , vC ) | vC V } {(vC , v{a} ) | vC V, {a} L(vC ), L(v{a} ) =
*
L(vC )} let
transitive, reflexive, symmetric closure . Since relation
*
equivalence relation nodes V , use v[C] , vC V , denote
*
*
equivalence class vC
. use relation
(recursively) define elements

. first define set Nom(S) = {v[{a}] | NI } nodes nominals
labels. non-nominal elements obtained unravelling parts
saturation graph paths usual (Horrocks & Sattler, 2007). set
PathsS (D) = {v[D] } Nom(S)

{p v[C]
| p PathsS (D), > n r.C L(v), v tail(p), v[C]
/ Nom(S), 1 n},
)=v .
denotes concatenation tail(v[C] ) = tail(p v[C]
[C]

define interpretation = (I , ) follows:
= PathsS (D),
and, NI , set
aI = v[{a}] ,
NC
AI = {p | v tail(p) L(v)},
552

fiPay-As-You-Go Description Logic Reasoning

r NR

> n s.C L(v)
rI = {hp, qi PathsS (D) PathsS (D) | q = p v[C]

v tail(p) n v r}

{hq, pi PathsS (D) PathsS (D) | q = p v[C]
> n inv(s).C L(v)

v tail(p) n v r}
{hp, xi PathsS (D) Nom(S) | v x v 0 tail(p)
v r-successor v 0 }
{hx, pi Nom(S) PathsS (D) | v x v 0 tail(p)
v inv(r)-successor v 0 }
{hp, pi PathsS (D) PathsS (D) | v tail(p)
v r-neighbour }

Note domain elements (paths of) equivalent classes ensure
extension role r correctly contains edges derived saturation, use nodes
equivalent class construction rI .
order show |= K, first show that, every p , holds p C
C L(v) v tail(p).
Lemma 1. Let = (V, E, L) fully saturated saturation graph concept w.r.t.
K vD well nodes representing individuals neither critical clashed.
Furthermore, let = (I , ) denote interpretation constructed described Definition 13. p holds p C C L(v) v tail(p).
Proof 1. observe nodes involved construction interpretation
neither critical clashed. particular, node v tail(p) p clashed,
v would descendant node v[D] node representing individual and, hence,
would identified clashed -rule, contradicts assumption
nodes clashed. Analogously, v critical, v[D] node representing
individual would identified critical C -rule (which also contradictory
assumption). Hence, considering different types concepts proving
lemma following, safe assume nodes used construction
interpretation neither clashed critical and, hence, = .
base case C = L(v) v tail(p) trivially holds p
definition , i.e., p AI L(v) v tail(p). Also note = >I
due definition saturation algorithm (in particular due initialisation
nodes) due fact never remove concepts labels saturation graph.
base cases hold elements p v tail(p) C L(v) follows:
C = {a}, observe p = v[{a}] due use equivalence classes,
*
definition
, , , fact Coo -rule cannot identify v critical
assumption. Hence, p C .
553

fiSteigmiller & Glimm

C = B, observe p
/ B due definition fact
used nodes clashed saturation graph. Hence, p C .
C = r.Self, observe node v r-successor due
application Self-rule. definition , hp, pi rI . Hence,
p CI .
C = r.Self, observe nodes saturation graph clashed.
Hence, exclude loops caused last part definition since
node would clashed due Condition 2 -rule. nominal nodes
observe nodes V nominal label represented
one element . possibly introduces loops neighbouring nominal nodes
saturation graph. is, however, excluded Condition 3 -rule. Hence,
hp, pi
/ rI and, therefore, p C .
complex cases hold elements p v tail(p) C L(v)
induction follows:
C = C1 u C2 , application u-rule ensures L(v) {C1 , C2 }.
induction, p C1I p C2I . Hence, p C .
C = C1 C2 , observe must concept C 0 L(v) C 0
{C1 , C2 } since Ct -rule would otherwise identified node v tail(p)
critical, contradicts assumption. Hence, induction, p C 0I
and, consequence, p C .
C = > n r.C 0 , observe saturation algorithm creates saturates
node vC 0 r-successor v n 1 consider two cases: First,
v[C 0 ] Nom(S), n = 1 since v would clashed n > 1 due
Condition 5 -rule, contradicts assumption. definition
PathsS (D), , , exists element q q Nom(S) hp, qi
rI . Furthermore, induction, q C 0I and, consequently, p C . Second,

1 , . . . , p vn
v[C
/ Nom(S), construction PathsS (D), p v[C
0]
0]
[C 0 ] PathsS (D)

and, definition , elements n r-successors p. Finally, induction,
1 , . . . , p vn
0I and, consequently, p C .
p v[C
0]
[C 0 ] C
C = r.C 0 , observe application -rule guarantees inv(r)predecessors C 0 label. Furthermore, also r-successors C 0
label, otherwise C -rule would identified v critical, contradictory
assumption. Hence, definition PathsS (D), , , induction,
holds every r-neighbour element q q C 0I and, consequently, p C .
C = 6 n r.C 0 , C6-rule guarantees every node v p n merging
candidates, i.e., n r-successor nodes C 0 labels, otherwise
node would critical, contradicts assumption. Analogously, Cch -rule
guarantees every r-successor v either C 0 nnf(C 0 ) label.
p = v[D] p 6 Nom(S), observe p predecessors and,
554

fiPay-As-You-Go Description Logic Reasoning

definition PathsS (D), , , induction, n rneighbour elements q1 , . . . , qn q1 , . . . , qn C 0I since every r-neighbour
element q holds q nnf(C 0 )I and, therefore, q
/ C 0I . Hence, p C
p = v[D] . p Nom(S), holds, every inv(r)-predecessor element q p,
q
/ C 0I due induction, definition PathsS (D), , , Co6-rule
(otherwise node would identified critical). Hence, p C p Nom(S).
. Moreover, C
p
/ {v[D] } Nom(S), p = q v[D
0]
ch -rule guarantees
0
0
w tail(q) either C nnf(C ) label. induction definition
PathsS (D), , , either q C 0I q nnf(C 0 )I . consider
cases: First, q
/ C 0I w tail(q) nnf(C 0 ) L(w),
argue analogously case p = v[D] . Second, q C 0I w tail(q)
C 0 L(w), guaranteed C6-rule v n 1 rsuccessors C labels and, definition PathsS (D), , ,
n r-neighbour elements q1 , . . . , qn p holds induction
q1 , . . . , qn C 0I . Consequently, p C .
using Lemma 1, show completeness, i.e., show
constructed interpretation satisfies axioms knowledge base:
Lemma 2 (Completeness). Let = (V, E, L) fully saturated saturation graph
concept w.r.t. K vD well nodes representing individuals neither critical
clashed, exists interpretation = (I , ) |= K DI 6= .
Proof 2. assume interpretation = (I , ) built Definition 13.
Note due definition saturation algorithm, node vD V
L(vD ), definition , v[D] and, definition tail , v[D]
DI . Hence, DI 6= . observe satisfies every axiom normalised
knowledge base K (cf. Section 2.2) follows:
= H v C H = {a}, H = A, H = >, observe that, every p
*
v tail(p), H H L(v) due definition
, PathsS (D),


, . Due applications v1 -rule, C L(v) H L(v),
and, Lemma 1, p C p H . Hence, |= .
= A1 u A2 v C, analogously observe that, every p v tail(p),
*
(A1 uA2 )I {A1 , A2 } L(v) due definition
, PathsS (D), ,

. Due applications v2 -rule, C L(v) {A1 , A2 } L(v),
and, Lemma 1, p C p H . Hence, |= .
= r v s, observe |= due definition successors/predecessors
definition PathsS (D), , .
= Disj(r, s), assume contradiction elements p, q
hp, qi rI sI . definition saturation graph edges
S, either q Nom(S) r v s. However, cases v tail(p) clashed due
Condition 6 Condition 7 -rule, respectively. Since contradictory
assumption v clashed, |= .
555

fiSteigmiller & Glimm

Since satisfies, elements , every axiom K, |= K.
language features completely supported presented saturation
algorithm fact r.C concepts left-hand side GCIs
transformed universal restrictions right-hand side (with propagations
predecessors inverse roles used), one observe completeness presented
saturation algorithm guaranteed least ELH knowledge bases.

4. Supporting Tableau Algorithms
section, present range optimisations directly indirectly support reasoning tableau algorithms DL SROIQ. already mentioned, reasoning systems
expressive DLs usually complex integrate many sophisticated
optimisations necessary make reasoning many real-world ontologies practicable. consequence, important development new optimisations
consider interaction already existing techniques. example, important
well-known optimisation technique dependency directed backtracking allows
evaluating relevant non-deterministic alternatives tableau algorithm.
typical realisation dependency directed backtracking backjumping every fact
added completion graph labelled non-deterministic branches
fact depends (Baader et al., 2007; Tsarkov, Horrocks, & Patel-Schneider, 2007).
clash discovered, jump back last non-deterministic decision
referenced clashed facts completion graph and, consequently,
evaluate non-deterministic alternatives clear would result clashes. Hence, new optimisation techniques manipulate completion
graphs must obviously also add dependencies correctly, otherwise dependency directed
backtracking cannot completely supported presence optimisations.
optimisation techniques present section fully compatible
dependency directed backtracking and, best knowledge, also
negatively influence well-known optimisations. Moreover, since saturation optimisations allow lot reasoning work efficiently, often reduce
effort optimisation techniques. example, optimisations directly perform
many simple expansions completion graph and, therefore, effort conventional
caching methods often reduced. particular, tableau-based reasoning systems often
cache satisfiable node labels order block expansion successors identically
labelled nodes subsequent completion graphs. reuse labels non-critical
non-clashed nodes saturation graph, directly know
processing completion graph required them, even without checking whether
corresponding node labels satisfiability cache.
4.1 Transfer Saturation Results Completion Graphs
Since presented saturation method uses compatible data structures, directly
transfer saturation results completion graphs. improves tableau algorithm
faster clash detection optimises construction completion graph.
example, directly use unsatisfiability information detected -rule
556

fiPay-As-You-Go Description Logic Reasoning

saturation. particular, application tableau expansion rule adds concept
C completion graph, check saturation status vC and, case
clashed, immediately initiate backtracking dependencies
unsatisfiable concept C completion graph. Analogously, utilise derived
consequences form saturation. instance, expansion rule adds concept C
label node completion graph, add concepts L(vC )
label. course, order support dependency directed backtracking,
also add correct dependencies. However, since concepts L(vC )
deterministic consequences C, simply use deterministically depends C
every additionally added concept L(vC ).
nice side effect, addition derived concepts saturation improves
backtracking processing disjunctions. Basically, t-rule saturation
extracts shared (super-)concepts disjuncts disjunction. example,
disjunction A1 A2 axioms A1 v B A2 v B, derive saturation
L(vA1 A2 ) {A1 A2 , B}, i.e., B super-concept disjuncts add
deterministic consequence disjunction A1 A2 . Although still process
disjunction, add consequences (e.g., B) deterministically. Hence,
backtracking identify processing alternatives disjunction relevant
deterministic consequences involved clash.
transfer derived consequences directly add many consequences possible
helpful several ways. First, application expansion rules tableau algorithms
might become unnecessary. example, disjunct disjunction already
added, necessary apply t-rule. Second, specific concepts
label node, then, least expansion rules tableau algorithm, optimised
rule applications possible. instance, concepts r.C, r.D, 6 1 r.>
label node, second application -rule tableau
algorithm directly add existentially restricted concept already present rsuccessor instead creating new one merged afterwards. Third, concepts
propagated back ancestor nodes, necessary check whether one
modified ancestor nodes blocked rules descendant nodes applied.
Due transfer derived consequences, many concepts propagated
back successors already added and, therefore, amount blocking tests
significantly reduced. Last least, transfer derived consequences allows
blocking much earlier. Blocking node v usually possible node could
replaced another non-blocked node completion graph influence
ancestor v. simple blocking condition guarantees completeness
expressive DLs pairwise blocking. However, pairwise blocking refined achieve
precise blocking conditions possibly allow blocking earlier (Horrocks & Sattler,
2001). Since many concepts propagated back successors added
transfer derived consequences saturation, likely creation
processing new successor nodes influence ancestor nodes. result, might
possible block nodes even without creation processing many successors.
Besides transfer derived consequences, cases also possible directly
block processing successor nodes completion graph. this, node
completion graph, say v, labelled concepts node v 0
557

fiSteigmiller & Glimm

saturation graph v 0 must neither clashed, critical, nominal dependent.
exists v 0 , processing successors v blocked since v could
expanded way v 0 saturation graph. Obviously, enforce
v 0 nominal dependent, dependent nominal could influenced
completion graph new consequences propagated back v would
considered processing successor nodes blocked. Furthermore, indeed
necessary create successors blocking processing, may
merged ancestor node. However, saturation node v 0
tight at-most restriction, i.e., at-most cardinality restriction 6 r.C L(v 0 ), v 0
1 r-successors nnf(C) label, also creation
successor nodes blocked, every at-most cardinality restriction label
node allows least one additional neighbour nodes
merged. Since nodes easily large number successors (e.g., due at-least
cardinality restrictions big cardinalities), blocking creation new successors
significant improvement terms memory consumption building time
completion graph. course, new concepts propagated v label v
differs v 0 , blocking becomes invalid processing successors
reactivated find another compatible blocker node.
4.2 Subsumer Extraction
tableau-based reasoning systems, many higher level reasoning task often reduced
consistency checking. example, naive classification algorithm tests satisfiability classes checks pairwise subsumption relations
classes (which also reduced satisfiability/consistency tests) order build
class hierarchy ontology. practice, number required satisfiability tests
significantly reduced optimised classification approaches enhanced traversal
(Baader et al., 1994) known/possible set classification (Glimm et al., 2012). optimised classification algorithms use specific testing orders exploit information
extracted constructed models. optimise testing order, algorithms
usually initialised told subsumptions, i.e., subsumption relations
syntactically extracted ontology axioms, and, typically, told subsumers
extracted, larger benefit classification algorithms. However,
detailed extraction told subsumers ontology axioms usually less efficient
simple one. instance, ontology axioms A1 v r.C u r.C v A2 imply
A2 subsumer A1 , detected, parts axioms compared
other.
saturation, significantly improve told subsumers initialisation tableau-based classification algorithm since also (some) semantic consequences
considered. new accurate told subsumers, simply use,
concept classified, (atomic) concepts L(vA ). Moreover, vA
clashed, know unsatisfiable without performing satisfiability test.
Analogously, vA neither clashed critical knowledge base consistent,
know satisfiable L(vA ) contains subsumers. Note vA nominal
dependent representative node nominal critical, also vA identified
558

fiPay-As-You-Go Description Logic Reasoning

critical. Hence, extraction subsumers, consider criticality
status considered node, whereas nominal dependency matter. node
ontology critical, already get subsumers saturation and, therefore,
transitive reduction (i.e., elimination subsumptions indirectly
implied transitivity property subsumption relation) necessary build
class hierarchy. Thus, preceding saturation automatically get one-pass
classification simple ontologies.
Note completeness proof Section 3.3 principally covers satisfiability
concepts. However, quite obvious presented saturation approach also computes subsumers nodes neither critical clashed. particular, assume
subsumption v B derived saturation follows knowledge
base K, obtain contradiction considering saturation knowledge base
K0 extends K axiom B v . Since B v added v1 -rule
B node label K0 differs K axiom B v , saturation
would derive consequences and, hence, would incomplete w.r.t. testing
satisfiability A, contradictory w.r.t. assumption completeness proof.
4.3 Model Merging
Many ontologies contain axioms form C D, seen abbreviation
C v v C. described Section 2.2, utilise get normalised
knowledge base consider axioms. Treating axioms
form atomic concept v v can, however, downgrade
performance tableau algorithms since absorption might apply v A, i.e.,
axiom internalised > v nnf(D A). avoid this, many implemented
tableau algorithms explicitly support axioms additional unfolding rule,
concept label node unfolded nnf(D) (exploiting
v equivalent v nnf(D)) (Horrocks & Tobies, 2000).2 Unfortunately, using
unfolding rule also comes price since tableau algorithm longer forced
add either nnf(D) node completion graph, i.e., might know
nodes whether represent instances A. means cannot
exclude possible subsumer (atomic) concepts nodes completion graphs
contain (and also A), important optimisation classification
procedures (Glimm et al., 2012).
compensate this, create candidate concept A+ A, example
partially absorbing (Steigmiller et al., 2014b), automatically added
node label completion graph node possibly instance A, i.e., candidate
concepts indicate completely defined concepts possibly satisfied. Hence, A+
added node label, know (possible) subsumer
concepts label node (even allow knowledge base contain concept
equivalence axioms form D). Formally, define requirements
candidate concepts follows:
2. Note works long axioms form v D0 , u A0 v D0 ,
A0 u v D0 , D0 D0 6= knowledge base.

559

fiSteigmiller & Glimm

Definition 14 (Candidate Concept). Let K knowledge base containing complete
definition form D. say A+ candidate concept every fully
) (fully saturated saturation graph
expanded clash-free completion graph G = (V, E, L, 6=
+
= (V, E, L)), holds L(v) K |= C1 u. . .uCn v A, {C1 , . . . , Cn } = L(v)
({C1 , . . . , Cn } = L(v) v well nodes representing individuals neither critical
clashed).
course, axioms form > v A+ , enforce A+ added node
labels and, hence, represents valid candidate concept A. useful practice,
is, however, desired concepts added node labels possible (without
introducing additional overhead) and, therefore, reasoners usually employ sophisticated
absorption techniques generate better candidate concepts (Steigmiller et al., 2014b).
consequence, handy identification non-subsumptions illustrated
following example.
Example 3. Let us assume TBox T2 consists axioms
A1 v r.B

A2 v s.B u (r. B)

A3 s.B u r.B,

interested classification T2 . order get (automatic) indication
concepts could subsumed completely defined concept A3 , create candidate
concept A3 (partially) absorbing negation A3 definition. Hence, partially
absorbing s.B r.B B v .A+
3 (the part r.B cannot absorbed trivially),
+
obtain candidate concept A3 A3 . Note absorption adds B v .A+
3


label
indicates
T2 without removing rewriting axioms. Now, absence A+
3
A3 subsumer concepts label, used classification.
particular, saturate A1 , A2 , A3 , B, .A+
3 added label

propagated


representative
nodes A2 A3 .
representative node B A+
3
particular, since label representative node A1 contain A+
3 , know
A1 v A3 hold without special consideration axiom A3 s.B ur.B.
However, still determine whether A2 satisfiable concepts
subsumers A2 complete classification T2 . this, (have to) fall back
tableau algorithm and, principle, perform satisfiability test A2
subsumption test possible subsumer A2 , i.e., (atomic) concepts
(possibly non-deterministically) added root node satisfiability test,
checks whether actual subsumers models.
Although candidate concepts already allow significant pruning subsumption
tests, still ontologies candidate concepts added many node
labels, especially limited absorption axiom form
possible. Hence, still possible subsumer many concepts.
saturation graph can, however, used improve identification (more
less obvious) non-subsumptions. Basically, candidate concept A+
label node v completion graph, test whether merging v
saturated node vnnf(D) possible. Since often conjunction, also try merge
v representative node disjunct nnf(D). models merged
defined below, v obviously instance A.
560

fiPay-As-You-Go Description Logic Reasoning

Definition 15 (Model Merging). Let = (V, E, L) fully saturated saturation graph
) fully expanded clash-free completion graph knowledge
G = (V 0 , E 0 , L0 , 6=
base K. node v V mergeable node v 0 V 0
v critical, nominal dependent, clashed;
L(v) L0 (v 0 ) contain {C, nnf(C)} concept C;
L(v) L0 (v 0 ) contain concepts A1 A2 A1 u A2 v C K
C
/ (L(v) L0 (v 0 ));
v 0 r-neighbour v 0 concept r.Self L(v);
v r-neighbour v concept r.Self L0 (v 0 );
C L0 (w0 ) every r-neighbour w0 v 0 r.C L(v);
C L(w) every r-successor w v r.C L0 (v 0 );
nnf(C) L0 (w0 ) every r-neighbour w0 v 0 6 r.C L(v);
nnf(C) L(w) every r-successor w v 6 r.C L0 (v 0 ).
Note conditions designed checked efficiently
clear conditions relaxed further. instance, necessary
enforce v nominal dependent. principle, ensure
interaction generated completion graph, can, example, also
guaranteed concept tested satisfiability use nominals completion
graph. addition, model merging fails due concepts completion graph
interaction tested node saturation graph, simply extend
saturation graph new node, also problematic concepts considered,
retest model merging node. instance, node v 0 completion
graph mergeable node v saturation graph due axiom A1 u A2 v C
knowledge base A1 L0 (v 0 ), A2 L(v), C
/ (L0 (v 0 ) L(v)),
saturate new node w L(w) L(v) {C} check whether w mergeable.
contrast, concepts tested node saturation graph interact
completion graph, often easily possible extend model merging approach
non-subsumption guaranteed. particular, interested
modifying completion graph since also used model merging tests.
addition, recursive model merging test, check whether neighbours
node completion graph mergeable propagated concepts saturation
graph, non-trivial since exclude interactions already tested nodes.
example, node v 0 completion graph mergeable node v
saturation graph due r-neighbour w0 v 0 concept r.C label v
C
/ L(w0 ), recursive model merging could test whether w0 mergeable
vC . However, would also necessary guarantee merging w0 vC
cause new consequences propagated back v 0 , especially non-trivial
several universal restrictions label v would affect w0 .
561

fiSteigmiller & Glimm

Example 4. continue classification TBox T2 Example 3, (have to)
build completion graph A2 tableau algorithm, straightforward.
particular, directly see A2 satisfiable A3 possible subsumer
A2 (since candidate concept A+
3 propagated root node A2 existentially restricted s-successor B label). apply model merging, saturation
different alternatives/disjuncts correspond A3 required, i.e., assume
concepts s.B r.B also saturated, trivial since new
consequences implied created referred nodes completely handled
saturation. tableau algorithm added disjunct r. satisfy r. B,
model merging fails since vs.B interaction r-successor
completion graph constructed satisfy s.B vr.B interaction
r. obviously excluded. Hence, would required test whether A3
subsumer A2 checking satisfiability A2 u A3 . contrast, B
added, none model merging conditions satisfied vr.B and, therefore,
directly conclude A3 subsumer A2 .
Note, although proposed (pseudo) model merging techniques (Haarslev, Moller,
& Turhan, 2001) work, principle, similar way, also significant
differences. example, presented merging test applied corresponding candidate concepts label nodes, already reduces number tests.
addition, test merging nodes saturation graph and, therefore,
significant overhead creating appropriate (pseudo) models. contrast,
approaches often necessary build separate completion graphs
concepts model merging applied. Moreover, presented approach
also applicable expressive DLs SROIQ, whereas approaches often
deactivate model merging certain language features used (e.g., nominals). course,
expressive DLs may produce critical nodes and, therefore, potentially reduce
model merging possibilities, necessary completely deactivate it,
results good pay-as-you-go behaviour.

5. Saturation Improvements
Obviously, support tableau algorithm saturation works better
nodes possible marked critical. However, since saturation procedure
completely support language features, easily get critical nodes even
unsupported language features rarely used knowledge base.
especially problematic critical nodes referenced many nodes, whereby
also considered critical. following, present different approaches
saturation improved number critical nodes
reduced. result, better support tableau algorithm possible.
5.1 Supporting Expressive Language Features
known literature, saturation procedures extended expressive
Horn DLs, e.g., Horn-SHIQ (Kazakov, 2009) even Horn-SROIQ (Ortiz, Rudolph, &
Simkus, 2010). Although shown extensions efficient
562

fiPay-As-You-Go Description Logic Reasoning

ontologies fragments, completely clear perform ontologies
use language features outside fragments, example, used
partially saturate ontologies approach. particular, worst-case complexity
procedures polynomial and, therefore, easily cause construction
large saturation graphs corresponding large memory requirements. However,
practical implementations, simply limit number nodes processed
saturation directly marking remaining nodes critical. Hence, easily
support features Horn-languages without risking memory consumption
increased much without gaining benefits.
particular, interesting relax restriction concepts propagated predecessor nodes universal restrictions form r.C. required
saturation procedure presented Section 3 enable reuse nodes,
extended full support universal restrictions possible. course,
allowed directly modify existing r-successors, easily create saturate copies
existing r-successors extend propagated concept C. addition,
remove edges previous r-successors incompleteness detection rule
C concept r.C mark node critical, obviously case
newly connected r-successors include concept C completely handled.
Note copies extensions nodes realised efficiently. Basically,
first apply default saturation rules and, afterwards, extend successors
saturation already added concept C. addition, use,
successor node extended, mapping concepts, node
extended, copied extended nodes, whereby reuse already created
node extensions. Thus, several predecessors propagate concepts
successors, create node corresponding extension once.
seen (efficient) implementation so-called node contexts, serve basis
many saturation procedures fully handle universal restrictions (Simanck et al.,
2011, 2014; Bate et al., 2015). particular, extension mapping, i.e., mapping
nodes copies nodes extended additional concepts, seen
representation node contexts. example, node vA r-successor
v r.B L(v), create copy node vA , say vA,B , B added
used r-successor v instead vA . extension mapping,
also store extension vA B found node vA,B
reuse it. Note, however, create copy B already label
vA . Moreover, directly copying nodes (with derived consequences), repetition
many rule applications necessary.
Support at-most restrictions form 6 1r.> achieved analogously.
labels corresponding r-successors easily merged new node,
used replace r-successors. Again, use mapping merging
certain successors always results (possibly new) node. remaining
r-successor v 0 also merged predecessor v 00 node v, add
concepts label v 0 label v 00 make v also inv(r)-successor
v 00 . Thus, Horn-SHIF (almost) completely supported rather small extensions
presented saturation procedure.
563

fiSteigmiller & Glimm

difficult support nominals. Already complete nominal support
DL EL++ would potentially introduce significant overhead. particular, would
necessary store, every node v every nominal {a}, descendants v using
nominal {a}, i.e., descendant v nominal {a} label, would
store v nominal {a} used descendant. would find node v,
stored nominal {a} used several descendant nodes, say v 1 , . . . , v n ,
would create new node u labels v 1 , . . . , v n merged,
would reproduce paths predecessors merged nodes v
potentially new consequences also propagated v. However, since
majority EL ontologies use nominals much simpler ways (e.g., concepts
form r.{a}) presented saturation procedure already sufficient,
sophisticated nominal handling currently seem required.
Saturation procedures extended non-Horn DLs, instance, saturation procedures proposed DLs ALCH (Simanck et al., 2011), ALCI
(Simanck et al., 2014), even SHIQ (Bate et al., 2015). this, also
handle non-determinism is, example, caused disjunctions, typically
realised simply considering/saturating non-deterministic alternatives.
concepts derived alternatives, interpreted actual consequences
knowledge base. number alternatives large, (naive)
saturation approach might become impractical. Although also tableau algorithm
consider alternatives worst-case, successively, i.e., trading
memory requirements potentially increased runtime. Moreover, tableau algorithms usually implement large amount optimisations reduce non-deterministic
alternatives considered. notably, dependency directed backtracking
allows evaluating alternatives non-deterministic decisions indeed
relevant, i.e., involved creation clashes. Since saturation algorithms
track dependencies derived facts, ability determine alternatives
non-deterministic decisions considered (since would result
clashes) limited. Unfortunately, tracking dependency information makes
simple reuse nodes (which foundation saturation procedures) impossible or,
least, much involved.
Although shown saturation procedures extended non-deterministic
language features work well range ontologies (Simanck et al., 2011),
investigations required order understand whether (or cases)
better tableau algorithms expressive DLs. However, development
implementation saturation-based reasoning systems expressive DLs seems
challenging and, best knowledge, saturation-based procedure/reasoner
expressive DLs SROIQ yet exist. Hence, interesting compromise, presented paper, keep (basic) saturation algorithm deterministic
process remaining parts tableau algorithm, typically coupled
several well-established optimisations (e.g., semantic branching, Boolean constraint
propagation, dependency directed backtracking, unsatisfiability caching) handle nondeterminism. Alternatively, one process non-deterministic language features
saturation procedure long certain limits reached (e.g., memory limit
564

fiPay-As-You-Go Description Logic Reasoning

L(vA2 ) =
L(vA1 ) =



>, A1 , s.A2





vA1



vA2

r

vA3


L(v{a} ) =



>, {a}, B, .B

v{a}



L(v{b} ) =





>, A2 , s.{b}, r.A3 , A1 A3

r

v{b}

L(vA3 ) =



>, A3 , s.{c}




r

v{c}

L(v{c} ) =

>, {b}, r.{a}, r.{c}, 6 1r.>



>, {c}





Figure 2: Incompletely handled saturation graph testing satisfiability concept
A1 Example 5

upper bound number saturated, non-deterministic alternatives), simply
mark remaining nodes critical processed tableau algorithm.
5.2 Improving Saturation Results Completion Graphs
already mentioned, even one node individual critical,
presented saturation procedure also marks nominal dependent nodes critical.
easily limits improvement saturation ontologies intensively use
nominals. Analogously, nodes incompletely handled concepts (e.g.,
disjunctions) nodes referenced many nodes,
nodes also critical although necessarily concepts label
cannot handled completely. issues also illustrated following example:
Example 5. Let us assume TBox T3 contains following axioms:
A1 v s.A2
A3 v s.{c}
{a} v B

A2 v s.{b}

A2 v r.A3

A2 v A1 A3

{b} v r.{c}

{b} v 6 1r.>



B v .B
{b} v r.{a}

testing satisfiability concept A1 w.r.t. TBox T3 , generate saturation
graph depicted Figure 2. Note, node v{b} individual b cannot
completely handled saturation due concept 6 1r.> label v{b} ,
would require v{a} v{c} merged. Therefore, v{b} critical also
consider nodes critical refer critical nodes, is, example,
case node vA2 . Moreover, since one node individual critical, cannot
exclude consequences propagated individuals and, therefore, possibly
also nominal dependent nodes. instance, merging v{a} v{c} would
propagate concept B label vA3 . Thus, also vA3 critical although
directly contain concept cannot handled saturation. Analogously,
label vA2 contains disjunction A1 A3 , also completely processed
565

fiSteigmiller & Glimm

saturation and, therefore, mark ancestor nodes vA2 critical (if
already case), even contain problematic concepts. consequence,
obtain saturation status = (So , , S! ), v{b} tight at-most restriction,
i.e., = {v{b} }, nodes nominal dependent well critical, i.e., = S! =
{v{a} , v{b} , v{c} , vA1 , vA2 , vA3 }.
course, saturation extended several ways better support features
expressive DLs (see Section 5.1), but, best knowledge, exists
saturation algorithm completely covers features expressive DLs
SROIQ. Hence, knowledge base uses unsupported features, easily
run problem saturation becomes incomplete possibly get many
critical nodes.
approach overcome issues critical nodes patch, i.e., update,
saturation graph results fully expanded clash-free completion graphs
generated consistency satisfiability checks. Roughly speaking, idea
replace labels critical nodes saturation graph corresponding labels
completion graphs, know completely handled tableau
algorithm. call nodes patched nodes. Then, apply saturation rules
update saturation status, hopefully results improved saturation graph
fewer critical nodes. Note, however, simply adding non-deterministically derived
concepts labels completion graphs saturation easily leads unsound results.
Hence, distinguish deterministically non-deterministically derived concepts
updating saturation simultaneously managing two saturation graphs: one
deterministically derived concepts added, i.e., deterministic saturation graph,
second one, also non-deterministically derived concepts consequences
considered, i.e., non-deterministic saturation graph. non-deterministic
consequences locally limited influence, i.e., non-deterministically added
concepts propagate new consequences limited number ancestor nodes, then,
comparing saturation graphs, possibly identify ancestor nodes
influenced, considered non-critical. reducing
number critical nodes saturation, approach allows improving
construction new completion graphs transferring new results
updated saturation.
order describe approach detail, first define saturation patch,
constitutes data structure managing information necessary
updating saturation graph.
Definition 16 (Saturation Patch). Let fclos(K) (Rols(K)) denote concepts (roles)
possibly occur completion graphs knowledge base K defined Definition 5.
saturation patch P saturation graph = (V, E, L) w.r.t. K tuple P = (Vp , Ld , Ln ,
Mc , Vo ),
Vp V denotes set patched nodes saturation graph,
Ld : Vp 2fclos(K) mapping patched nodes set deterministically derived
concepts,
566

fiPay-As-You-Go Description Logic Reasoning

Ln : Vp 2fclos(K) analogously mapping patched nodes set nondeterministically derived concepts,
Mc : Vp Rols(K) fclos(K) IN0 mapping at-most cardinality restrictions
form 6 r.C patched nodes (represented tuple node v, role
r, qualification concept C) number merging candidates,
Vo Vp denotes patched nodes nominal dependent.
saturation patch obviously identify nodes patched/updated,
realised set Vp . node Vp , mappings Ld Ln contain
concepts nodes label completion graph derived deterministically
non-deterministically, respectively. Hence, mappings determine nodes
label saturation graph extended longer critical. also
store number merging candidates (Mc ) patched nodes
nominal dependent (Vo ) information required generation updated
saturation status. Note consider number merging candidates nominal
dependencies non-deterministic information since often possible correctly
extract corresponding deterministic information completion graphs. example,
state-of-the-art reasoners usually searching blocker nodes checking detailed
conditions defined pairwise blocking, whereby node possibly also blocked
label subset label blocker node (Horrocks & Sattler, 2001).
blocker node directly indirectly using nominals, i.e., nominal dependent, also
blocked node considered nominal dependent. Hence, consider
nominal dependency non-deterministic information since nominal dependency
could caused concept label blocker node label
blocked one.
Especially root nodes completion graphs constructed satisfiability consistency tests suitable extraction patches. instance, fully expanded
clash-free completion graph testing satisfiability concept C, root node
C label used patch node vC saturation graph.
completion graph consistency check used patch representative nodes nominals. course, patching nominal dependent nodes, ensure kind
consistency, i.e., dependent nominals compatible representative
nodes nominals saturation graph already applied patches
nodes. simple form compatibility defined follows:
Definition 17 (Saturation Patch Compatibility). Let P 1 = (Vp1 , L1d , L1n , Mc1 , Vo1 ), . . . , P n =
(Vpn , Lnd , Lnn , Mcn , Von ) saturation patches saturation graph w.r.t. knowledge base
} 1 n. say P 1 , . . . , P n compatible
K, Vpi = {v1i , . . . , vm

) built K
fully expanded clash-free completion graph G = (V, E, L, 6=
1
1
n
n
contains nodes w1 , . . . wm1 , . . . , w1 , . . . wmn L(wji ) = Lid (vji ) Lin (vji )
1 j mi 1 n.
principle, limited root nominal nodes extraction
patches, detailed analysis completion graph required nodes.
example, tableau algorithm apply -rule concept r.C
567

fiSteigmiller & Glimm

label node v v already r -predecessor v 0 C label. Hence,
predecessor v 0 directly indirectly uses nominals, also v considered
nominal dependent. Moreover, nodes completion graph, often
clear concepts considered non-deterministically derived consequences.
instance, create, concepts r.C r.D label node v,
r-successor v 0 extract patch vC v 0 , identified
non-deterministically derived concept. this, principle necessary track
analyse dependencies facts causes completion graph.
efficiently supported reasoning system, extraction patches also
extended nodes completion graph. Otherwise, patch creation simply
restricted appropriate.
saturation patches applied saturation graph follows:
Definition 18 (Saturation Patch Application). Let = (V, E, L) saturation graph
P = (Vp , Ld , Ln , Mc , Vo ) saturation patch S. deterministic (non-deterministic)
application P yields deterministically (non-deterministically) extended saturation
graph Sd (Sn ) obtained saturating saturation graph (V, E, L0 ),
L0 = {v 7 L(v) | v V \ Vp } {v 7 Ld (v) | v Vp } (L0 = {v 7 L(v) | v V \ Vp } {v 7
Ld (v) Ln (v) | v Vp }).
Since interested deterministic non-deterministic saturation graph,
create copy saturation graph soon patch non-deterministically
derived concepts and, then, use non-deterministic application patches
copy. Although also fully saturate non-deterministic saturation graph simply
using presented saturate function, potentially derives unwanted consequences since
application saturation rules nodes possibly propagates new consequences
patched nodes. unfavourable patched consequences derived
processing different non-deterministic alternatives. particular, node v
ancestor v patched, saturation rules might propagate new consequences
obtained patching v ancestors. ancestor is, however, patched
concepts another completion graph, different non-deterministic alternatives
processed, possibly mix consequences different alternatives saturation
graph, easily limits effectiveness approach. example, v contains
disjunction r.A r.A, patch v non-deterministic extension r.A,
patching r -predecessor v 0 v non-deterministic extension allows
application -rule concept r.A label node v
concept propagated v 0 . consequence, would infer saturation
v 0 (possibly) clashed since label. Since (new) consequences
non-deterministic saturation graph considered non-deterministic,
produce incorrect results. order to, nevertheless, avoid derivation unwanted
consequences, saturate non-deterministic saturation graph contains
patched nodes V modified saturate\V (S) function, -rule applied
nodes V -rule modified propagate concepts
node v V . (and precise detection saturation status), gather
patches one combined patch keep patch addition deterministic
568

fiPay-As-You-Go Description Logic Reasoning

non-deterministic saturation graph. patches straightforwardly combined
using -operator defined follows:
Definition 19 (Saturation Patch Composition). Given two saturation patches P P 0
P = (Vp , Ld , Ln , Mc , Vo ) P 0 = (Vp0 , Ld 0 , Ln 0 , Mc0 , Vo0 ), saturation patch P P 0
defined tuple consisting
Vp Vp0 ,
Ld {v 7 C | v 7 C Ld 0 v
/ Vp },
/ Vp },
Ln {v 7 C | v 7 C Ln 0 v
Mc {hv, s, Ci 7 n | hv, s, Ci 7 n Mc0 v
/ Vp },
Vo (Vo0 \ Vp ).
Note patches contain information node, keep
information node one (the new) patch instead mixing information.
Thus, information patch gets lost common nodes, is,
however, problematic since patches describe valid extensions.
addition modified saturation function, reprocess ancestors
patched nodes non-deterministic saturation graph patching removes previously
added non-deterministic consequences order avoid mixing consequences
different non-deterministic alternatives. example, non-deterministically derived
concept r.C added propagated node label removed patching
node, r -predecessor also rebuilt deterministic saturation
graph unnecessary non-deterministic consequences (e.g., C) also removed.
practical implementations, obviously limit number ancestor nodes
updated processed new non-deterministic consequences non-deterministic
saturation graph order limit overhead patch application. limit
reached, remaining ancestors simply marked critical. Also note
reuse data deterministic saturation graph non-deterministic one
nodes influenced patch non-deterministic consequences.
order able use patched saturation graphs support tableau
algorithm, e.g., transfer results completion graphs, update
saturation statuses application patches. Similarly rule application
non-deterministic saturation graph, want propagate status patched
node successors. Therefore, analogously use modified status\V function
instead status, rules Table 3 4 applied nodes
V . also requires use modified ]mcands0 function status\V since,
patched nodes, use correct information given patch.
precise, Mc denotes mapping number merging candidates
considered patch, ]mcands0 (v, s, D) return Mc (hv, s, Di) v patched node,
]mcands(v, s, D) otherwise. addition, correctly initialise sets ,
, S! patched nodes information applied patches.
non-deterministic saturation graph, patched nodes obviously non-critical since
labels extracted fully expanded clash-free completion graphs. Hence,
569

fiSteigmiller & Glimm

initialise (combined) patch P = (Vp , Ld , Ln , Mc , Vo ),
realised setting = {v | v Vo },
= {v | v Vp 6 r.C (Ld (v) Ln (v)) Mc (hv, r, Ci) = m}.
deterministic saturation graph, additionally set S! {v | Ln (v) 6= }
order mark patched nodes directly critical could depend non-deterministic
consequences. initialisation, call function status\Vp obtain full
status corresponding saturation graph, used improve
support tableau algorithm.
Analogously deterministic non-deterministic saturation graphs, every new
saturation status incrementally updated last generated status last
saturation graphs sequentially updating ancestors newly patched nodes.
Hence, also generation new saturation statuses causing significant overhead
practice.
patching saturation graphs enables sophisticated support tableau algorithms. one hand, patching reduces number critical nodes and, therefore,
optimisations described Section 4, blocking expansion successors nodes
completion graph extraction subsumers, better applicable.
hand, also use non-deterministic saturation graph support,
e.g., classification process. node vA non-deterministic saturation graph
critical, label vA non-deterministic saturation graph describes
possible subsumers A. Thus, vA critical non-deterministic saturation
graph, label used prune possible subsumers. Moreover, use
non-deterministic saturation graph find identical labels used blocking
processing/expansion successors nodes completion graph. course, still
require corresponding nodes (non-deterministic) saturation graph
critical. contrast, restriction nodes saturation graph allowed
nominal dependent blocking relaxed works sufficiently well
many real world ontologies. Basically, patch nodes represent individuals
saturation graph consistency check corresponding nodes
obtained fully expanded clash-free completion graph. Furthermore, ensure,
one hand, subsequent saturation patch compatible initial patch, i.e.,
create patches nominal dependent nodes labels nodes
individuals completion graph identical subsets corresponding labels
initial completion graph consistency check. hand, create
patches nodes depend new nominals, i.e., nominals introduced
NN-rule. ensures nodes saturation graphs used blocking
long expand nodes individuals way initial completion graph. Thus, nominal dependent nodes used blocking, collect blocked
nodes queue reactive nodes becomes necessary expand nodes
individuals another way completion graph initial consistency
check. course, exact tracking dependent nominals, e.g., exactly
saving nominals node possibly depends, refine improve technique significantly. Obviously, use node blocking exactly known
nominals depends, reactivate processing node
570

fiPay-As-You-Go Description Logic Reasoning

nodes corresponding individuals expanded differently. Although approach
keeps patching saturation graphs consistent, i.e., compatibility patches
automatically ensured, restrictive required Definition 17. However,
allows identifying potential incompatibilities techniques tableau-based
reasoning systems, e.g., variants completion graph caching techniques (Steigmiller,
Glimm, & Liebig, 2015).
Due non-deterministic decisions tableau algorithm, critical node
saturation graph patched several ways. Moreover, patch already
patched node (hopefully) improve non-deterministic saturation graph, i.e., try
reduce number nodes influenced non-deterministic consequences
and/or marked critical. Thus, need strategy decides nodes
extract patches fully expanded clash-free completion graph
non-deterministic saturation graph improved. already described,
extract patches nodes information safely extracted
make non-deterministic saturation graph inconsistent. addition, strategy
keep number patches small possible since update data
structures every patch.
simple example strategy create patches reduce
number non-deterministic propagation concepts patched nodes. strategy
would prefer patch adds non-deterministic set concepts {r.C, A1 , A2 }
comparison patch non-deterministic extension {s.D, t.D}.
strategy ensures, least, create arbitrary patches, avoids oscillation
different possibilities, clearly favour creation patches
influence nodes. However, cannot guarantee non-deterministic saturation
graph actually improved. example, concept r.C could propagate C several
predecessors also processing C could influence many ancestors, whereas
patch {s.D, t.D} might influence predecessors. Therefore, node
already patched {s.D, t.D} create new patch {r.C, A1 , A2 } due
fewer propagation concepts, even worsen non-deterministic saturation graph.
order counteract this, also extract patches saturation graph
detect critical node deterministic saturation graph labelled
concepts non-deterministic saturation graph node non-deterministic
saturation graph critical. kind internal patch ensure
saturation identified node neither critical influenced non-deterministic
consequences, remember solved state node overwrite
state integrating patches non-deterministic saturation graph. course,
strategy creation extraction patches optimally also considers nominal
dependency tight at-most restrictions trying reduce number nodes.
Example 6. mentioned, nodes saturation graph Figure 2, generated
testing satisfiability concept A1 w.r.t. TBox T3 (p. 564), critical.
consequence, check satisfiability A1 tableau algorithm detail.
this, first check consistency individuals a, b, c, results simple
completion graph, nodes c merged. completion graph,
extract initial saturation patch P 1 individuals, i.e., P 1 = (Vp1 , Ld 1 , Ln 1 , Mc1 , Vo1 )
571

fiSteigmiller & Glimm

Vp1 = {v{a} , v{b} , v{c} }, Ld 1 = {v{a} 7 {>, {a}, {c}, B, .B}, v{c} 7 {>, {a}, {c}, B,
.B}, v{b} 7 {>, {b}, r.{a}, r.{c}, 6 1 r.>}}, Ln 1 = , Mc1 = {hv{b} , r, >i 7 1},
Vo1 = {v{a} , v{b} , v{c} }. Note, although nodes individuals c merged
completion graph, patch v{a} v{c} separately since saturation
support merging nodes. Also note completion graph consistency
check deterministic and, therefore, mapping nodes non-deterministically derived
concepts required, i.e., node mapped Ln 1 . However, ease
presentation, omit uninteresting patch data simply use Ln 1 .
deterministically applying P 1 initial saturation graph, obtain new deterministic saturation graph, nodes extended data applied patch.
particular, v{c} extended concepts {a}, B, .B deterministic saturation graph, whereby concept B also propagated vA3 and, consequence,
label vA3 extended set {>, A3 , s.{c}, B, .B}. saturation status
new deterministic saturation graph reveals nodes v{a} , v{b} , v{c} , vA3
critical. Thus, have, principle, already shown satisfiability concept A3 .
contrast, vA1 still indirectly critical due incompletely handled disjunction A1 A3
label vA2 .
order test satisfiability A1 tableau algorithm, initialise
new completion graph node v concept A1 asserted. Since
disjunction A1 A3 added completion graph s-successor v 0 v,
tableau algorithm choose disjuncts A1 A3 . Independently
decision, obtain fully expanded clash-free completion graph shows
satisfiability A1 , non-deterministic decision influences patching
saturation graph. example, non-deterministically adding A3 v 0 , tableau algorithm add s-edge node representing c due A3 v s.{c} T3
B propagated label v 0 and, subsequently, also label v due
B v .B T3 since node c B label. Thus, extract
patch P 2 = ({vA1 }, {vA1 7 {>, A1 , s.A2 }}, {vA1 7 {B, .B}}, , {vA1 }). Since P 2 contains non-deterministically derived consequences, apply patch deterministically
non-deterministically. Although node vA1 considered fully handled nondeterministic saturation graph, remains critical deterministic saturation graph and,
therefore, usage supporting (e.g., blocking expansion successor nodes new
completion graphs, identification (possible) subsumers) tableau algorithm limited.
contrast, disjunct A1 non-deterministically added v 0 , could extract
saturation patch P 3 = ({vA1 }, {vA1 7 {>, A1 , s.A2 }}, , , {vA1 }) applying P 3 ,
could also consider node vA1 non-critical deterministic saturation graph.
Hence, prefer saturation patch P 3 would also extract apply P 3 , even
extracted applied P 2 earlier constructed completion graph.

now, considered patching fully expanded clash-free completion
graphs. course, also possible integrate unsatisfiability results completion
graphs saturation graphs. particular, tableau algorithm cannot find
fully expanded clash-free completion graph concept C, create patch
deterministically extend vC concept . management unsatisfiable
572

fiPay-As-You-Go Description Logic Reasoning

concepts saturation graph benefit also propagated nodes
immediately identify many unsatisfiable concepts.
also worth pointing that, especially extraction application
patches, support tableau algorithm information provided
saturation graphs seen intelligent caching technique. Although corresponds limited caching certain nodes influenced predecessors,
also works, extent, nominals inverse roles. Moreover, fast
automatically propagate unsatisfiability satisfiability statuses concepts.

6. Related Work
already approaches combine reasoning techniques fully-fledged
DL reasoners specialised procedures specific fragments. instance, reasoning
system (Armas Romero et al., 2012) uses module extraction identify part
ontology completely handled efficient reasoning system
fully-fledged reasoner used remaining parts ontology. Note,
approach works opposite direction: apply saturation simply
ignore (or partially process) unsupported features and, then, detect parts
completely handled. Since uses reasoners black-boxes, is, principle,
possible combine arbitrary reasoning procedures adapting module extraction.
However, now, fully-fledged OWL 2 reasoners based variants tableau
calculi efficient reasoning systems interesting fragments usually using variants
saturation procedures (e.g., completion- consequence-based reasoning), whereby
combination tableau saturation algorithms currently seems interesting
one.
Due black-box approach, technique realised flexible.
example, easily possible exchange fully-fledged reasoner reasoning system
known works best certain kinds ontologies. approach,
hand, implemented one single reasoning system requires
support certain techniques, binary absorption, work well. Moreover, compatible
data structures used kinds procedures, usually means
appropriate saturation algorithm integrated tableau-based reasoning system.
approach, however, also various advantages. example, saturation uses
representation ontologies tableau algorithms and, therefore, ontology
loaded once. contrast, reasoners used separately load
ontology (or parts thereof) since used black-boxes and, usually, also
compatible data structures. Furthermore, approach much tolerant
usage features outside efficiently supported fragment. optimisations
also used saturated nodes critical, could, example, case
ontology contains non-absorbable GCIs. addition, presented extension
allows fixing critical parts saturation, whereby unsupported features
problematic rarely used ontology. contrast,
reduce module efficient reasoner long module contains unsupported
features. Thus, approach promises better pay-as-you-go behaviour. Moreover,
use intermediate results saturation, whereas technique relies
573

fiSteigmiller & Glimm

externally provided interfaces reasoners, usually provides basic information
satisfiability concepts subsumers classes. Therefore, integration
saturation procedure obviously allows sophisticated optimisation techniques
transfer inferred consequences blocking processing
tableau algorithm.
Although approaches principle applicable different reasoning tasks,
technique automatically improves reasoning long reasoning task reduced
consistency checking tableau algorithm. example, order support
satisfiability testing complex concepts, approach need adaptations.
MORe, however, would necessary check whether complex concept
module handled efficient reasoner order achieve improvement.
Last least, need module extraction technique approach,
also take significant amount time. especially advantage ontologies
almost completely efficiently supported fragment since approach
similar overhead module extraction ontologies.
Another reasoning system combines different reasoning techniques WSReasoner
(Song et al., 2012), uses weakening strengthening approach classification ontologies. precise, ontology first rewritten simpler one
(the weakening ontology), supported language features (partially) expressed fragment handled efficient reasoner. Then, strengthened
version weakened ontology created, axioms added least
also consequences original ontology implied. weakened strengthened ontologies classified specialised reasoner possible differences
obtained subsumtion relations verified fully-fledged reasoner. Also WSReasoner, fragment specific reasoner (usually based saturation procedure)
fully-fledged reasoner (usually based tableau calculus) used black-boxes,
makes them, principle, exchangeable. However, weakening strengthening also
adapted language fragment efficient reasoner.
Although technique WSReasoner different one MORe, advantages
disadvantages comparison approach principle same. However,
approach WSReasoner easily extendible language features and,
now, presented DL ALCHIO (with elimination/encoding
transitive roles also SHIO). Moreover, since nominals simplified fresh atomic
concepts, approach cannot straightforwardly used reasoning tasks.
simplification is, however, applicable, often improves reasoning performance
corresponding ontologies.
Similarly WSReasoner, PAGOdA (Zhou et al., 2014, 2015) also uses weakening
strengthening approach, however, different reasoning tasks delegating
different fragment ontology specialised reasoner. particular, PAGOdA
designed ABox reasoning (e.g., conjunctive query answering) delegates majority
computational workload efficient datalog reasoner. lower bound
datalog reasoner match upper bound, PAGOdA delegates
query relevant parts ABox fully-fledged reasoner. order keep
relevant parts small possible, PAGOdA uses additional optimisations relevant
subset extraction, summarisation, dependency analysis. However, additional
574

fiPay-As-You-Go Description Logic Reasoning

optimisations also carry risk every use fully-fledged reasoner introduces
additional overhead, could problematic ontologies lot work still
done fully-fledged reasoner. Moreover, maintaining several naive representations
entire ABox easily multiply memory requirements.

7. Implementation Evaluation
extended Konclude3 (Steigmiller, Liebig, & Glimm, 2014) saturation procedure
shown Section 3 optimisations presented Section 4 5. Konclude
tableau-based reasoner SROIQ (Horrocks et al., 2006) extensions handling
nominal schemas (Steigmiller et al., 2013). Konclude integrates many state-of-the-art optimisations lazy unfolding, dependency directed backtracking, caching, etc. Moreover,
Konclude uses partial absorption (Steigmiller et al., 2014b) order significantly reduce
non-determinism ontologies and, therefore, Konclude well-suited proposed
coupling saturation procedures.
integration saturation Konclude completely covers language features
DL Horn-SHIF using saturation extensions described Section 5.1,
universal restrictions propagate concepts successors merging successors/predecessors due functional at-most restrictions handled. number nodes
additionally processed handling saturation extensions mainly
limited number concepts occurring knowledge base. However, Koncludes
saturation procedure supports limited handling ABox data. due
design decision try avoid several representations individuals (and
derived consequences these) reasoning system. Since saturation could easily
incomplete ABox (e.g., since disjunctions asserted individuals due
datatypes), ABox often also handled tableau algorithm several
representations ABox multiply memory requirements. Hence, Konclude primarily handles ABox individuals tableau algorithm uses patches completion
graphs (as presented Section 5.2) improve parts saturation graph
depend nominals.
addition, Konclude saturates concepts might required certain reasoning task upfront batch processing mode, whereby switches tableau
saturation algorithm reduced significantly. Moreover, sort concepts
occur knowledge base saturate specific order maximise amount
data shared saturated nodes. example, knowledge base
contains axiom v B, first saturate B use data vB initiate
vA . particular, copying node labels, many rule applications skipped,
significantly improves performance saturation procedure. Furthermore, also
reduces effort saturation status detection. instance, vB satisfy
at-most restriction, at-most restriction also satisfied vA .
following, present detailed evaluation shows effects Koncludes
integrated saturation procedure presented optimisations support fullyfledged tableau algorithm. addition, compare reasoning times Konclude
ones state-of-the-art reasoners support TBox reasoning (almost) features
3. Konclude freely available http://www.konclude.com/

575

fiSteigmiller & Glimm

Repository

#
Axioms
Ontologies

Q0.5
Gardiner
292
5, 842
96
NCBO BioPortal
403 27, 180
1, 116
NCIt
185 178, 818 167, 667
OBO Foundry
502 37, 349
1, 292
Oxford
394 73, 921
3, 433
TONES
203
7, 707
352
Google crawl
414
6, 869
255
OntoCrawler
548
2, 574
136
OntoJCrawl
1, 696
6, 281
311
Swoogle crawl
1, 638
2, 778
132
ORE2014 dataset
16, 555 16, 017
594

22, 830 16, 647
594

Classes

Q0.5
1, 788
14
7, 518
339
69, 720 68, 862
6, 753
509
8, 543
500
2, 864
96
1, 127
39
124
17
1, 772
50
416
21
3, 846
87
4, 020
74

Properties
Q0.5
44
7
48
13
116 123
24
4
53
11
40
5
102
44
93
20
62
9
36
10
101
53
97
50

Individuals
Q0.5
85
2
1, 766
0
0
0
20, 905
6
18, 273
5
65
0
824
1
633
0
838
0
879
0
1, 801 50
2, 271 29

Table 5: Statistics ontology metrics evaluated ontology repositories ( stands
average Q0.5 median)

DLs SROIQ, namely FaCT++ 1.6.3 (Tsarkov & Horrocks, 2006), HermiT 1.3.8
(Glimm, Horrocks, Motik, Stoilos, & Wang, 2014), 0.1.6 (Armas Romero et al., 2012),
Pellet 2.3.1 (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2007). evaluation uses
large test corpus ontologies,4 obtained collecting downloadable
parsable ontologies
Gardiner ontology suite (Gardiner, Horrocks, & Tsarkov, 2006),
NCBO BioPortal (Whetzel et al., 2011),
National Cancer Institute thesaurus (NCIt) archive (National Cancer Institute,
2003),
Open Biological Ontologies (OBO) Foundry (Smith et al., 2007),
Oxford ontology library (Information Systems Group, 2012),5
TONES repository (Information Management Group, 2008),
subsets OWLCorpus (Matentzoglu, Bail, & Parsia, 2013) gathered
crawlers Google, OntoCrawler, OntoJCrawl, Swoogle,6
ORE2014 dataset (Matentzoglu & Parsia, 2014).
4. test corpus evaluated version(s) Konclude v0.6.1 found online http://www.
derivo.de/en/products/konclude/paper-support-pages/tableau-saturation-coupling.html
5. Note Oxford ontology library also contains repositories (e.g., Gardiner ontology suite),
ignored order avoid much redundancy.
6. order avoid many redundant ontologies, used subsets OWLCorpus
gathered crawlers OntoCrawler, OntoJCrawl, Swoogle, Google.

576

fiPay-As-You-Go Description Logic Reasoning

Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

Expressiveness
ALE+
ALEH+
ALEHIF+
SRIF
SRI
ALCH
SH
SH
ALCOIF
SHOIN

Axioms
1, 170, 573
60, 633
61, 782
847, 794
731, 482
141, 957
229, 713
109, 959
165, 000
32, 157

Classes
518, 196
23, 136
23, 136
187, 520
106, 398
58, 771
95, 701
54, 974
41, 648
3, 533

Properties
16
950
950
70
469
124
110
9
148
84

Individuals
1
0
0
220, 948
0
0
0
0
85
160

Table 6: Ontology metrics selected benchmark ontologies

Note ORE2014 dataset collection ontologies several sources
redundantly contains many ontologies also contained repositories. However, many ontologies ORE2014 dataset adapted approximated fit requirements certain OWL 2 profiles (e.g., removing datatypes
OWL 2 datatype map, enforcing regular role hierarchy, adding
declarations undeclared entities). used OWL API parsing converted
ontologies self-contained OWL/XML files, created, 1,380
ontologies imports, version resolved imports another version,
import directives simply removed (which allows testing reasoning performance
main ontology content without imports, frequently shared many ontologies). Table 5 shows overview obtained test corpus overall 22,830 ontologies
including statistics ontology metrics source repositories.
addition test corpus, present results explicitly selected ontologies (shown
Table 6) frequently used many evaluations. allows directly showing
effects approach well-known benchmark ontologies enables concrete
comparison. Note Table 6 separated EL (upper part) non-EL ontolgies
(lower part). EL ontologies, chose well-known Gazetteer EL-GALEN,
latter one obtained removing functionality inverses Full-GALEN
ontology, also selected benchmarking. addition, evaluated Biomodels
Cell Cycle v2.01, large mainly deterministic ontologies NCBO
BioPortal, NCI v06.12d NCI v12.11d, different versions NCI-Thesaurus
ontology NCIt archive, SCT-SEP, denotes SNOMED CT anatomical
model ontology (Kazakov, 2010), FMA v2.0-CNS, version Foundational
Model Anatomy (Golbreich, Zhang, & Bodenreider, 2006), OBI, represents
recent version Ontology Biomedical Investigations (Brinkman et al., 2010).
evaluation carried Dell PowerEdge R420 server running two Intel
Xeon E5-2440 hexa core processors 2.4 GHz Hyper-Threading 144 GB RAM
64bit Ubuntu 12.04.2 LTS. evaluation focuses classification, central reasoning task supported many reasoners and, thus, ideal comparing results.
principle, measured classification time, i.e., time spent parsing
loading ontologies well writing classification output files included pre577

fiSteigmiller & Glimm

sented results. advantage reasoners already perform preprocessing
loading, is, however, case Konclude since Konclude uses lazy processing approach also preprocessing triggered classification request.
also seems confirmed accumulated loading times ontologies
evaluated repositories, 6, 304 Konclude, 10, 877 MORe, 13, 210
FaCT++, 22, 458 Pellet, 61, 293 HermiT. Note HermiT directly
clausifies ontologies loading (i.e., converts axioms HermiTs internal
representation based DL-clauses), easily take lot time ontologies intensively use cardinality restrictions. also ignored errors reported (other)
reasoners, i.e., reasoner stopped processing ontology (e.g., due unsupported
axioms program crashes), measured actual processing time. also
disadvantage Konclude since Konclude processed ontologies (however, Konclude
also ignored parts role inclusion axioms regular specified OWL 2
DL). contrast, reported errors 803, FaCT++ 944, Pellet 1, 285,
HermiT 1, 483 ontologies corpus. reasoners often cancelled processing
due unsupported malformed datatypes. Another frequently reported error consisted
different individual axioms one individual specified. addition, HermiT completely refused processing ontologies irregular role inclusion axioms (which
are, however, rarely present test corpus).
evaluation ontology repositories, used time limit 5 minutes.
selected benchmark ontologies, cancelled classification task 15 minutes
since ontologies relatively large. Moreover, averaged results selected
benchmark ontologies 3 separate runs, necessary evaluated repositories since large amount ontologies automatically compensates non-deterministic
behaviours reasoners, i.e., accumulated (classification) times separate runs
many ontologies almost identical. Although reasoners support parallelisation,
configured reasoners use one worker thread, allows comparison independently number CPU cores facilitates presentation improvements
saturation.
7.1 Evaluation Saturation Optimisations
presented optimisations integrated Konclude way separately activated deactivated. Hence, evaluate compare performance
improvements different optimisations. Please note deactivating optimisations
Konclude cause disproportionate performance losses since appropriate replacement
optimisations, could compensate deactivated techniques extent, often integrated Konclude. example, many reasoning systems use completely
defined concepts optimisation (Tsarkov & Horrocks, 2005) identify classes
ontology subsumption relations directly extracted ontology
axioms and, thus, satisfiability subsumption tests necessary correctly insert
classes class hierarchy. Clearly, optimisation necessary Konclude, extract subsumers class saturation saturated
representative node critical. Hence, performance deactivated optimisations
578

fiPay-As-You-Go Description Logic Reasoning

might worse be. Nevertheless, evaluated versions Konclude,

saturation optimisations activated (denoted ALL),
none saturation optimisations activated (denoted NONE),
combination activation/deactivation (denoted +/) following modifications:
RT (standing result transfer), transfer (possibly intermediate) results saturation completion graphs (as presented Section 4.1) activated/deactivated. precisely, initialise new nodes completion graph
consequences available saturation graph block processing
(successor) nodes long identically labelled non-critical nodes
(deterministic non-deterministic) saturation graph. described Section 5.2,
nominal dependent nodes handled reactivating processing nodes
dependent nominals become modified completion graph. this, implemented exact tracking nominal dependent nodes completion graph
well saturation graph.
SE (standing subsumer extraction), extraction subsumers
saturation (as presented Section 4.2) activated/deactivated. representative nodes atomic concepts critical, Konclude use extract
subsumers (besides completely defined concepts) and, otherwise, derived atomic
concepts used told subsumers. Note that, completely defined concepts
node label, candidate concepts interpreted corresponding completely defined concepts non-deterministically derived, i.e., possible subsumers. SE optimisation deactivated, Konclude extracts
simple told subsumers axioms knowledge base order initialise
classification algorithm, also Konclude based known possible
sets classification (Glimm et al., 2012).
MM (standing model merging), model merging saturation
graph (as presented Section 4.3) activated/deactivated. candidate concepts
obtained Konclude partial absorption technique (Steigmiller et al.,
2014b) model merging applied first initialisation known
possible subsumers atomic concept. particular, avoid repeated
model merging possible subsumption relation different nodes (in
possibly different completion graphs) since could result significant overhead
possibly new non-subsumptions identified.
ES (standing extended saturation), handling universal restrictions
functional at-most restrictions successors saturation (as presented
Section 5.1) activated/deactivated. Note integrated saturation procedure
becomes complete Horn-SHIF knowledge bases optimisation activated,
whereas completeness guaranteed ELH knowledge bases deactivated.
579

fiSteigmiller & Glimm

PS (standing patched saturation), patching saturation graph
data completion graphs (as presented Section 5.2) activated/deactivated.
ensure patch compatibility nominal dependent nodes, use completion
graph caching technique integrated Konclude (Steigmiller et al., 2015)
nominal nodes identified possibly different consequences
derived initial completion graph. Since Konclude supports exact tracking
nominal dependency completion graph, save dependent nominals
patches propagate saturation graphs processing
node reactivated node dependent nominal becomes modified.
Konclude incorporates exact tracking facts causes
derived facts (Steigmiller, Liebig, & Glimm, 2012) used also extract
patches non-root nodes (as also sketched Section 5.2). Moreover,
discovered satisfiability test concept result fully expanded
clash-free completion graph, i.e., concept unsatisfiable, Konclude patches
saturation graphs -concept unsatisfiable concepts
also revealed.
example, NONE+MM denotes version Konclude, saturation optimisations except model merging saturation graph deactivated.
Based version NONE, Table 7 shows performance improvements activation saturation optimisations RT, SE, MM. addition, results
shown, optimisations activated simultaneously. Please note ES
PS optimisations improve saturation procedure and, therefore, evaluation makes sense combination saturation optimisations.
significant improvements achieved transfer saturation results completion
graphs (RT), often reduces effort tableau algorithm significantly.
model merging optimisation (MM) primarily improves classification performance
NCI-Thesaurus ontologies NCIt archive, similar significant
impact repositories. Since many NCI-Thesaurus ontologies contain complete
definitions form r.B1 u s.B2 , model merging candidate concepts
(as demonstrated Section 4.3) allows pruning many subsumptions performing
satisfiability tests atomic concepts. also improvements extraction subsumers saturation (SE), but, compared improvements
optimisations, significantly better NCBO BioPortal. particular,
NCBO BioPortal contains many large relatively simple ontologies almost
completely handled saturation and, therefore, necessary perform satisfiability tests every class tableau algorithm SE optimisation activated
order determine (possible) subsumers. Nevertheless, saturation optimisations
activated, often able achieve much larger performance improvements
almost repositories. one hand, caused additionally activated ES
PS optimisations, hand, reasoning system utilise several synergy
effects saturation (obviously, concepts saturated
optimisations).
Table 8 analogously shows performance improvements activating saturation
optimisations RT, SE, MM selected benchmark ontologies. saturation optimisations significantly improve classification performance several ontologies.
580

fiPay-As-You-Go Description Logic Reasoning

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset


NONE
526
2, 259
28, 603
3, 020
7, 976
1, 734
798
27
3, 405
3, 477
115, 494
167, 320

NONE+RT
508
2, 039
28, 434
812
4, 639
1, 481
463
29
1, 166
2, 670
80, 673
122, 914

NONE+SE
414
580
27, 940
877
5, 866
1, 568
670
30
2, 232
2, 820
98, 630
141, 628

NONE+MM
490
2, 326
3, 163
2, 829
8, 013
756
794
31
2, 504
2, 283
115, 232
138, 421


108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674

Table 7: Accumulated classification times (in seconds) separately activated saturation
optimisations evaluated ontology repositories
Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

NONE

NONE+RT

NONE+SE

NONE+MM



34.8
761.0
900.0
241.5
900.0
900.0
17.7
900.0
900.0
1.3

30.1
5.5
900.0
50.6
900.0
900.0
14.6
339.8
900.0
0.8

14.0
1.6
900.0
18.2
7.6
900.0
8.8
279.4
900.0
0.7

37.9
762.6
900.0
148.7
900.0
17.9
16.0
383.1
900.0
2.1

13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
72.7
0.6

Table 8: Classification times (in seconds) separately activated saturation optimisations
evaluated benchmark ontologies

particular, optimisations, Konclude handle ontologies reasonable
amount time, whereas Konclude timed five ontologies saturation optimisations used. difficult ontologies Full-GALEN FMA v2.0-CNS
handled sophisticated saturation optimisations used (e.g., SE, PS).
also observed that, many ontologies, specific optimisations crucial,
is, however, also surprising. example, clear MM optimisation cannot improve performance deterministic ontologies since
possible subsumers model merging could applied.
Table 9 shows performance changes separate deactivation saturation optimisations based configuration. evaluation optimisations also
interesting perspective, saturation many concepts easily require
significant amount reasoning time and, separately deactivating single optimisations,
overhead saturation associated separately activated optimisa581

fiSteigmiller & Glimm

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset



108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674

ALLRT
134
624
2, 041
1, 052
3, 987
143
790
30
2, 017
1, 427
56, 128
68, 374

ALLSE
201
1, 980
2, 580
453
3, 701
366
731
62
1, 445
1, 209
56, 469
69, 286

ALLMM
90
618
27, 952
473
2, 398
1, 377
706
30
702
2, 456
37, 760
71, 562

ALLES
396
582
2, 000
774
3, 658
226
412
30
764
1, 348
51, 400
61, 590

ALLPS
106
709
1, 960
453
2, 537
633
733
35
879
1, 201
61, 036
70, 281

Table 9: Accumulated classification times (in seconds) separately deactivated saturation optimisations evaluated ontology repositories

Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI


13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
72.7
0.6

ALLRT
13.6
1.5
12.7
17.2
7.5
15.0
8.7
280.4
60.3
0.8

ALLSE
27.9
4.8
25.1
47.1
900.0
15.7
13.0
337.1
28.2
0.8

ALLMM
13.2
1.4
11.8
15.6
7.1
900.0
7.6
161.9
180.3
0.7

ALLES
13.7
1.5
900.0
16.2
7.3
13.8
7.4
167.5
66.6
0.7

ALLPS
13.5
1.5
12.7
16.6
6.9
13.2
7.7
168.7
900.0
0.7

Table 10: Classification times (in seconds) separately deactivated saturation optimisations selected benchmark ontologies

tion. Furthermore, allows evaluating whether optimisations superfluous
effects caused saturation improvements ES PS, useful combination saturation optimisations. Table 9 reveals saturation
optimisations completely irrelevant repositories. Moreover, deactivation
optimisations also improve performance several repositories, e.g., deactivation RT results better reasoning times ontologies TONES repository
deactivation MM causes minor performance improvements OntoJCrawl
ontologies. However, considering repositories, optimisation indeed justified.
particular, presented saturation optimisations deactivated, reasoning times increase least 55 %. also caused several difficult ontologies
ORE2014 dataset, variants KB Bio 101 ontology (Chaudhri, Wessel, &
Heymans, 2013), handled Konclude almost saturation optimisations used. patching saturation graph (PS) data initial
582

fiPay-As-You-Go Description Logic Reasoning

consistency test often required complete/sufficient handling nominals within
saturation procedure, saturation extensions (ES) enable primitive handling
(qualified) cardinality restrictions even big cardinalities (due reuse nodes
saturation graph), result transfer (RT) well subsumer extraction
(SE) reduce avoid work tableau algorithm, particularly useful
big highly cyclic ontologies. Although MM optimisation similar important
ORE2014 dataset, optimisation significantly reduces effort
Konclude NCI-Thesaurus ontologies NCIt archive.
performance changes separate deactivation saturation optimisations
evaluated benchmark ontologies depicted Table 10. Again, observed
often specific optimisations important ontologies. example,
deactivation SE optimisations significantly decreases performances
Biomodels Cell Cycle v2.01 ontologies. Since Full-GALEN highly cyclic
many consequences caused functional cardinality restrictions well inverse
roles, tableau algorithm difficulties find appropriate blocker nodes completion graph and, therefore, handled saturation extended
language features (as realised ES optimisation). contrast, FMA v2.0-CNS
many unsatisfiable classes and, soon tableau algorithm find unsatisfiable class, saturation graph patched (realised PS optimisation)
-concept directly propagated many classes, whereby many satisfiability
tests tableau algorithm become unnecessary.
7.2 Evaluation Saturation Effort
Table 11 shows distribution processing times w.r.t. Koncludes processing stages
classification evaluated repositories version ALL. Unsurprisingly,
majority processing time (61.5 %) spent classification process itself. contrast, saturation concepts potentially required classification
requires 12.1 % together detection saturation status. latter one
can, however, usually neglected terms processing time since implementation
efficient. example, node detected critical, criticality status
immediately propagated dependent nodes and, consequence,
tested. Moreover, use criticality testing queue filled saturation
concepts added node labels potentially influence criticality status. Hence,
status detection iterate node labels. Although
principle possible design ontologies saturation relatively inefficient (in
particular w.r.t. memory requirements), ontologies hardly occur practice.
particular, data sharing node labels realised Konclude, saturation
cause significant problems evaluated repositories, also reflected
short processing time saturation stage. Consistency checking usually
also performed efficiently, several evaluated repositories (e.g., Swoogle crawl)
also contain difficult ontologies tableau algorithm cannot find fully
expanded clash-free completion graph within time limit. Building internal representation well preprocessing also realised efficiently Konclude
cause problems evaluated repositories.
583

fiSteigmiller & Glimm

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset


Building Preprocessing
10.8
30.6
27.4
17.1
7.5
11.0
16.3
5.0
5.6
10.6
2.6
3.3
11.6
7.9
38.3
10.7
8.2
4.6
2.2
1.0
4.8
5.8
5.4
6.3

Saturation
32.5
23.6
11.2
6.7
12.2
5.6
19.4
17.5
4.4
1.8
12.5
12.1

Consistency
1.8
3.1
1.9
46.6
20.3
0.6
8.3
21.1
48.1
26.7
13.4
14.6

Classification
24.3
28.8
68.4
25.4
51.3
87.9
52.9
12.5
34.7
68.2
63.4
61.5

Table 11: Distribution processing time w.r.t. different processing stages (in %)

7.3 Comparison Approaches
mentioned Section 6, exist approaches also use saturation-based
reasoning techniques improve fully-fledged tableau algorithms. example,
uses module extraction delegate much work possible efficient reasoner
specialised specific fragment order classify ontologies. Since early development
version available, evaluated test corpus compare
results approach following. used combination ELK 0.4.1
(Kazakov, Krotzsch, & Simanck, 2014) HermiT 1.3.8, combinations also
possible since reasoners used black-boxes.
left-hand side Table 12 shows accumulated classification times (in seconds)
versions Konclude saturation optimisations deactivated (version NONE
Column 2) saturation optimisations activated (version Column 3)
different repositories. Furthermore, improvement version NONE
version given percent (Column 4 Table 12). example, using saturation
optimisations presented here, accumulated reasoning time repositories reduced
77.5 % Konclude. right-hand side Table 12, analogously depicted
accumulated reasoning times HermiT (Column 5) (Column 6), also
percentage HermiTs reasoning time reduced (Column 7).
Note, accumulated loading times repositories 61, 293 HermiT
10, 877 MORe, difference 50, 416 explained additional
preprocessing directly performed HermiTs loading stage, whereas starts
processing ontologies classification request. course, also
uses HermiT internally process parts ontologies cannot handled OWL
2 EL reasoner ELK, required time loading parts HermiT
counted reasoning/classification time MORe. Hence, made comparison fair
adding additional preprocessing time loading stage HermiTs classification
time, i.e., shown classification times HermiT extended difference
loading times HermiT MORe.
584

fiPay-As-You-Go Description Logic Reasoning

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset


NONE [s]
526
2, 259
28, 603
3, 020
7, 976
1, 734
798
27
3, 405
3, 477
115, 494
167, 320

[%]
79.5
88.5
93.2
75.2
68.9
85.6
86.0
0.0
79.0
65.9
74.2
77.5

[s]
108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674

HermiT [s]
1, 773
5, 901
26, 435
6, 654
12, 865
2, 342
1, 917
1, 863
8, 555
4, 857
294, 124
367, 283

[s]
1, 537
4, 187
26, 600
4, 474
8, 083
2, 184
1, 629
893
4, 546
4, 270
166, 589
224, 982

[%]
13.3
29.0
0.6
32.8
37.2
6.7
15.0
52.1
46.9
12.1
43.9
38.7

Table 12: Comparison improvements saturation approaches realised
Konclude accumulated classification times evaluated
ontology repositories (in seconds %)

Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

NONE [s]
34.8
761.0
900.0
241.5
900.0
900.0
17.7
900.0
900.0
1.3

[%]
51.2
98.0
98.7
93.3
99.2
98.5
53.8
80.1
75.9
53.8

[s]
13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
72.7
0.6

HermiT [s]
900.0
900.0
900.0
788.8
900.0
211.9
92.7
900.0
900.0
32.5

[s]
18.2
2.6
900.0
648.8
900.0
208.0
83.3
900.0
900.0
2.37

[%]
98.0
99.7

17.7

1.9
10.1


93.0

Table 13: Improvements saturation approaches Konclude
classification times selected benchmark ontologies (in seconds
%)

Table 12 reveals significantly improve reasoning time HermiT
almost repositories. particular, saves 52.1 % HermiTs classification time
ontologies OntoCrawler. Nevertheless, still many ontologies
repositories, able reduce effort HermiT
classified within time limit (HermiT timed 786 590 ontologies,
respectively). contrast, Konclude integrates sophisticated interaction
tableau algorithm saturation procedure and, therefore, improvements
saturation optimisations significantly better many repositories. result,
7. class hierarchy computed OBI coincide results HermiT
Konclude.

585

fiSteigmiller & Glimm

version Konclude reached time limit 69 ontologies. Note, already
version NONE Konclude, saturation optimisations deactivated, outperforms
HermiT many ontologies, probably due difference integrated optimisations. example, Konclude uses several caching techniques save
reuse intermediate results, usually improves reasoning performance lot. Moreover, partial absorption (Steigmiller et al., 2014b) integrated Konclude significantly
reduces non-determinism also expressive ontologies. Also note
yet completely support language features SROIQ and, therefore, always output correct class hierarchy (for evaluated ontology repositories, 1, 441 class
hierarchies computed coincide results HermiT Konclude).
Table 13 analogously shows performance improvements selected benchmark
ontologies. Again, observed improvements saturation
often better Konclude MORe, especially ontologies considered
version NONE Konclude still requires lot reasoning time.
7.4 Comparison State-of-the-Art Reasoners
also evaluated classification times state-of-the-art reasoners FaCT++
Pellet, compared reasoners HermiT, Konclude,
Table 14. Note, Table 14 shows accumulated classification times actually
reported reasoners, i.e., compensate differences loading times.
observed Konclude outperforms reasoners evaluated
repositories, mainly due integrated saturation optimisations. FaCT++
reasoner efficiently handle majority NCI-Thesaurus ontologies
NCIt archive also without saturation optimisations. Nevertheless, model merging
saturation graph allows pruning many possible subsumers Konclude, whereby
classification performance improved and, therefore, Konclude able
outperform FaCT++ also NCIt archive. Considering repositories, Konclude
produced fewest timeouts (69), followed (590), HermiT (786), FaCT++ (822),
Pellet (1, 470).
Nevertheless, ontologies Koncludes performance optimal
reasoners sometimes able outperform Konclude. example,
Konclude requires 54.6 classification atom-complex-proton-2.0 ontology
TONES repository, whereas Pellet requires 11.4 (FaCT++ HermiT timed
out). particular, handling (large) cardinalities easily cause problems since,
worst-case, tableau algorithm used create merge corresponding
numbers successor nodes. Although saturation limited handling at-least cardinality restrictions using one representative node successor, easily becomes
incomplete also at-most cardinality restrictions used ontologies. remedy, one
could try extend saturation procedure better handle cardinality restrictions
combine tableau algorithm algebraic methods, cardinality restrictions
handled system linear (in)equations (Haarslev, Sebastiani, & Vescovi, 2011).
Moreover, much non-determinism, e.g., caused non-absorbable GCIs, still cause
serious issues tableau-based systems. Examples ontologies variants enzyo
Swoogle crawl, cannot classified evaluated reasoners except
586

fiPay-As-You-Go Description Logic Reasoning

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset


FaCT++
1, 108
5, 413
4, 393
7, 225
20, 761
1, 684
2, 178
964
11, 580
2, 864
241, 402
299, 573

HermiT
1, 688
5, 570
25, 203
6, 258
12, 255
1, 943
1, 761
1, 723
6, 757
4, 073
249, 637
316, 867

Konclude
108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674


1, 537
4, 187
26, 600
4, 474
8, 083
2, 184
1, 629
893
4, 546
4, 270
166, 589
224, 982

Pellet
4, 006
9, 877
17, 647
12, 031
27, 461
1, 755
7, 496
9, 999
31, 776
9, 212
397, 794
528, 891

Table 14: Comparison accumulated classification times state-of-the-art reasoners
(in seconds) evaluated ontology repositories
Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

FaCT++
900.0
900.0
900.0
2.78
900.0
13.9
57.8
900.0
900.0
900.0

HermiT
900.0
900.0
900.0
788.8
900.0
206.1
78.8
900.0
900.0
31.5

Konclude
13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
28.3
0.6


18.2
2.6
900.0
648.8
900.0
208.0
83.3
900.0
900.0
2.39

Pellet
480.2
135.1
900.0
900.0
900.0
69.6
306.9
900.0
900.0
900.0

Table 15: Comparison classification times state-of-the-art reasoners (in seconds)
selected benchmark ontologies

FaCT++ (but also FaCT++ needs significant amount time even reaches
time limit 5 minutes variants) although less 20, 000 axioms
expressiveness ranging ALIN ALCOIN . Also large SROIQ
ontologies Oxford ontology library, Mus musculus consisting 221, 484
axioms, seem currently reach existing reasoning systems. Due
size complexity, even difficult analyse kinds problems reasoners
running, intensive use nominals often limits applicability optimisation
techniques and, hence, often results poor performance.
Analogously, Table 15 shows comparison classification times evaluated reasoners selected benchmark ontologies seconds. Again, activated
8. FaCT++ 1.6.3 crashed classification Biomodels 2.7 seconds.
9. class hierarchy computed OBI coincide results
reasoners.

587

fiSteigmiller & Glimm

saturation optimisations, Konclude outperform reasoners almost ontologies able classify benchmark ontologies within time limit. Compared
reasoners, also achieve good results several ontologies.
particular, timed 4 ontologies, whereas HermiT Pellet could
classify 6 ontologies time, FaCT++ failed classification 7 ontologies.
Hence, support saturation seems pay off.

8. Conclusions
paper, presented technique tightly coupling saturation- tableaubased procedures. Unlike standard completion- consequence-based saturation procedures, approach applicable arbitrary OWL 2 DL ontologies. Furthermore,
good pay-as-you-go behaviour, i.e., axioms use features problematic saturation-based procedures (e.g., disjunction), tableau procedure still
benefit significantly saturation.
good pay-as-you-go behaviour seems confirmed evaluation
several thousand ontologies, integration presented saturation optimisations
reasoning system Konclude significantly improves classification performance.
particular, optimisations, Konclude able outperform many state-ofthe-art reasoners wide range ontologies often one order magnitude.

Acknowledgments
first author acknowledges support doctoral scholarship Postgraduate Scholarships Act Land Baden-Wuerttemberg (LGFG). work done
within Transregional Collaborative Research Centre SFB/TRR 62 CompanionTechnology Cognitive Technical Systems funded German Research Foundation (DFG).

References
Armas Romero, A., Cuenca Grau, B., & Horrocks, I. (2012). MORe: Modular combination
OWL reasoners ontology classification. Proc. 11th Int. Semantic Web Conf.
(ISWC12), Vol. 7649 LNCS, pp. 116. Springer.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. 19th Int. Joint
Conf. Artificial Intelligence (IJCAI05), pp. 364369. Professional Book Center.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2007).
Description Logic Handbook: Theory, Implementation, Applications (Second
edition). Cambridge University Press.
Baader, F., Hollunder, B., Nebel, B., Profitlich, H.-J., & Franconi, E. (1994). empirical
analysis optimization techniques terminological representation systems. J.
Applied Intelligence, 4 (2), 109132.
588

fiPay-As-You-Go Description Logic Reasoning

Bate, A., Motik, B., Cuenca Grau, B., Simanck, F., & Horrocks, I. (2015). Extending
consequence-based reasoning SHIQ. Proc. 28th Int. Workshop Description
Logics (DL15).
Brinkman, R. R., Courtot, M., Derom, D., Fostel, J., He, Y., Lord, P. W., Malone, J.,
Parkinson, H. E., Peters, B., Rocca-Serra, P., Ruttenberg, A., Sansone, S., Soldatova,
L. N., Jr., C. J. S., Turner, J. A., Zheng, J., & OBI consortium (2010). Modeling
biomedical experimental processes OBI. J. Biomedical Semantics, 1 (S-1), S7.
Chaudhri, V. K., Wessel, M. A., & Heymans, S. (2013). KB Bio 101: challenge OWL
reasoners. Proc. 2nd Int. Workshop OWL Reasoner Evaluation (ORE13).
CEUR.
Gardiner, T., Horrocks, I., & Tsarkov, D. (2006). Automated benchmarking description
logic reasoners. Proc. 19th Int. Workshop Description Logics (DL06), Vol. 198.
CEUR.
Glimm, B., Horrocks, I., Motik, B., Shearer, R., & Stoilos, G. (2012). novel approach
ontology classification. J. Web Semantics, 14, 84101.
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., & Wang, Z. (2014). HermiT: OWL 2
reasoner. J. Automated Reasoning, 53 (3), 125.
Golbreich, C., Zhang, S., & Bodenreider, O. (2006). foundational model anatomy
OWL: Experience perspectives. J. Web Semantics, 4 (3), 181195.
Haarslev, V., Moller, R., & Turhan, A.-Y. (2001). Exploiting pseudo models TBox
ABox reasoning expressive description logics. Proc. 1st Int. Joint Conf.
Automated Reasoning (IJCAR01), Vol. 2083 LNCS, pp. 6175. Springer.
Haarslev, V., Sebastiani, R., & Vescovi, M. (2011). Automated reasoning ALCQ via
SMT. Proc. 23rd Int. Conf. Automated Deduction (CADE11), pp. 283298.
Springer.
Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Proc.
10th Int. Conf. Principles Knowledge Representation Reasoning (KR06),
pp. 5767. AAAI Press.
Horrocks, I., & Sattler, U. (1999). description logic transitive inverse roles
role hierarchies. J. Logic Computation, 9 (3), 385410.
Horrocks, I., & Sattler, U. (2001). Optimised reasoning SHIQ. Proc. 15th European
Conf. Artificial Intelligence (ECAI02), pp. 277281. IOS Press.
Horrocks, I., & Sattler, U. (2004). Decidability SHIQ complex role inclusion axioms.
Artificial Intelligence, 160 (1), 79104.
Horrocks, I., & Sattler, U. (2007). tableau decision procedure SHOIQ. J. Automated Resoning, 39 (3), 249276.
Horrocks, I., Sattler, U., & Tobies, S. (1999). Practical reasoning expressive description logics. Proc. 6th Int. Conf. Logic Programming Automated Reasoning
(LPAR99), Vol. 1705 LNCS, pp. 161180. Springer.
589

fiSteigmiller & Glimm

Horrocks, I., Sattler, U., & Tobies, S. (2000). Reasoning individuals description
logic SHIQ. Proc. 17th Int. Conf. Automated Deduction (CADE00), Vol. 1831
LNCS, pp. 482496. Springer.
Horrocks, I., & Tobies, S. (2000). Reasoning axioms: Theory practice.. Proc.
7th Int. Conf. Principles Knowledge Representation Reasoning (KR00),
pp. 285296. Morgan Kaufmann.
Hudek, A. K., & Weddell, G. E. (2006). Binary absorption tableaux-based reasoning
description logics. Proc. 19th Int. Workshop Description Logics (DL06), Vol.
189. CEUR.
Information Management Group (2008). TONES ontology repository. University Manchester. Available http://owl.cs.manchester.ac.uk/repository/. Accessed: July
2012; Mirrored http://ontohub.org/repositories/tones.
Information Systems Group (2012). Oxford ontology library. University Oxford. Available
http://www.cs.ox.ac.uk/isg/ontologies/. Accessed: August 2012;.
Kazakov, Y. (2008). RIQ SROIQ harder SHOIQ. Proc. 11th Int.
Conf. Principles Knowledge Representation Reasoning (KR08), pp. 274
284. AAAI Press.
Kazakov, Y. (2009). Consequence-driven reasoning Horn-SHIQ ontologies. Proc.
21st Int. Conf. Artificial Intelligence (IJCAI09), pp. 20402045. IJCAI.
Kazakov, Y. (2010).
ConDOR project site.
condor-reasoner/. Accessed: July 2013;.

https://code.google.com/p/

Kazakov, Y., Krotzsch, M., & Simanck, F. (2012). Practical reasoning nominals
EL family description logics. Proc. 13th Int. Conf. Principles Knowledge
Representation Reasoning (KR12). AAAI Press.
Kazakov, Y., Krotzsch, M., & Simanck, F. (2014). incredible ELK - polynomial
procedures efficient reasoning EL ontologies. J. Automated Reasoning, 53,
161.
Matentzoglu, N., Bail, S., & Parsia, B. (2013). corpus OWL DL ontologies. Proc.
26th Int. Workshop Description Logics (DL13), Vol. 1014. CEUR.
Matentzoglu, N., & Parsia, B. (2014). ORE 2014 reasoner competition dataset. Zenodo.
Available http://dx.doi.org/10.5281/zenodo.10791.
National Cancer Institute (2003). Nci thesaurus archive. Available http://ncit.nci.
nih.gov/. Accessed: December 2012;.
Ortiz, M., Rudolph, S., & Simkus, M. (2010). Worst-case optimal reasoning HornDL fragments OWL 1 2. Proc. 12th Int. Conf. Principles Knowledge
Representation Reasoning (KR10), pp. 269279. AAAI Press.
Simanck, F. (2012). Elimination complex RIAs without automata. Proc. 25th Int.
Workshop Description Logics (DL12), Vol. 846. CEUR.
Simanck, F., Kazakov, Y., & Horrocks, I. (2011). Consequence-based reasoning beyond
horn ontologies. Proc. 22nd Int. Joint Conf. Artificial Intelligence (IJCAI11),
pp. 10931098. IJCAI/AAAI.
590

fiPay-As-You-Go Description Logic Reasoning

Simanck, F., Motik, B., & Horrocks, I. (2014). Consequence-based fixed-parameter
tractable reasoning description logics. J. Artificial Intelligence, 209, 2977.
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: practical
OWL-DL reasoner. J. Web Semantics, 5 (2), 5153.
Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, W., Ceusters, W., Goldberg, L. J.,
Eilbeck, K., Ireland, A., Mungall, C. J., Consortium, T. O., Leontis, N., Rocca-Serra,
P., Ruttenberg, A., Sansone, S.-A., Scheuermann, R. H., Shah, N., Whetzeland, P. L.,
& Lewis, S. (2007). OBO Foundry: coordinated evolution ontologies support
biomedical data integration. J. Nature Biotechnology, 25, 12511255.
Song, W., Spencer, B., & Du, W. (2012). WSReasoner: prototype hybrid reasoner
ALCHOI ontology classification using weakening strengthening approach.
Proc. 1st Int. Workshop OWL Reasoner Evaluation (ORE12), Vol. 858. CEUR.
Steigmiller, A., Glimm, B., & Liebig, T. (2013). Nominal schema absorption. Proc. 23rd
Int. Joint Conf. Artificial Intelligence (IJCAI13), pp. 11041110. AAAI Press.
Steigmiller, A., Glimm, B., & Liebig, T. (2014a). Coupling tableau algorithms expressive
description logics completion-based saturation procedures. Proc. 7th Int.
Joint Conf. Automated Reasoning (IJCAR14), Vol. 8562 LNCS, pp. 449463.
Springer.
Steigmiller, A., Glimm, B., & Liebig, T. (2014b). Optimised absorption expressive
description logics. Proc. 27th Int. Workshop Description Logics (DL14), Vol.
1193. CEUR.
Steigmiller, A., Glimm, B., & Liebig, T. (2015). Completion graph caching expressive
description logics. Proc. 28th Int. Workshop Description Logics (DL15).
Steigmiller, A., Liebig, T., & Glimm, B. (2012). Extended caching, backjumping merging expressive description logics. Proc. 6th Int. Joint Conf. Automated
Reasoning (IJCAR12), Vol. 7364 LNCS, pp. 514529. Springer.
Steigmiller, A., Liebig, T., & Glimm, B. (2014). Konclude: system description. J. Web
Semantics, 27 (1).
Tsarkov, D., & Horrocks, I. (2004). Efficient reasoning range domain constraints.
Proc. 17th Int. Workshop Description Logics (DL04), Vol. 104. CEUR.
Tsarkov, D., & Horrocks, I. (2005). Optimised classification taxonomic knowledge bases.
Proc. 18th Int. Workshop Description Logics (DL05), Vol. 147. CEUR.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ description logic reasoner: System description.
Proc. 3rd Int. Joint Conf. Automated Reasoning (IJCAR06), Vol. 4130
LNCS, pp. 292297. Springer.
Tsarkov, D., Horrocks, I., & Patel-Schneider, P. F. (2007). Optimizing terminological reasoning expressive description logics. J. Automated Reasoning, 39, 277316.
W3C OWL Working Group (27 October 2009). OWL 2 Web Ontology Language: Document Overview. W3C Recommendation. Available http://www.w3.org/TR/
owl2-overview/.
591

fiSteigmiller & Glimm

Whetzel, P. L., Noy, N. F., Shah, N. H., Alexander, P. R., Nyulas, C., Tudorache, T., &
Musen, M. A. (2011). BioPortal: enhanced functionality via new web services
national center biomedical ontology access use ontologies software
applications. Nucleic Acids Research, 39 (Web-Server-Issue), 541545.
Zhou, Y., Cuenca Grau, B., Nenov, Y., Kaminski, M., & Horrocks, I. (2015). PAGOdA:
Pay-as-you-go ontology query answering using datalog reasoner. J. Artificial
Intelligence Research, 54, 309367.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2014). Pay-as-you-go OWL query
answering using triple store. Proc. 28th AAAI Conf. Artificial Intelligence
(AAAI14), pp. 11421148.

592

fiJournal Artificial Intelligence Research 54 (2015) 159-192

Submitted 02/15; published 09/15

Leveraging Online User Feedback Improve
Statistical Machine Translation
Llus Formiga

lformiga@verbio.com

Verbio Technologies, S.L.,
Loreto, 44, 08029 Barcelona

Alberto Barron-Cedeno
Llus Marquez

albarron@qf.org.qa
lmarquez@qf.org.qa

Qatar Computing Research Institute
Hamad Bin Khalifa University,
Tornado Tower, Floor 10, P.O. Box 5825, Doha, Qatar

Carlos A. Henrquez
Jose B. Marino

carlos.henriquez@upc.edu
jose.marino@upc.edu

TALP Research Center - Universitat Politecnica de Catalunya,
Jordi Girona, 1-3, 08034 Barcelona

Abstract
article present three-step methodology dynamically improving statistical machine translation (SMT) system incorporating human feedback form
free edits system translations. target feedback provided casual users,
typically error-prone. Thus, first propose filtering step automatically identify
better user-edited translations discard useless ones. second step produces
pivot-based alignment source user-edited sentences, focusing errors
made system. Finally, third step produces new translation model combines
linearly one original system. perform thorough evaluation
real-world dataset collected Reverso.net translation service show every step methodology contributes significantly improve general purpose SMT
system. Interestingly, quality improvement due increase lexical
coverage, better lexical selection, reordering, morphology. Finally, show
robustness methodology applying different scenario, new
examples come automatically Web-crawled parallel corpus. Using exactly
architecture models provides significant improvement translation quality
general purpose baseline SMT system.

1. Introduction
Statistical machine translation (SMT) become widespread technology, used millions people satisfy multiplicity needs daily interactions information
seeking. contrast business-oriented translation services, on-line machine translation
services (e.g., Google Translate, see Google Inc., 2015; Bing, see Microsoft Inc., 2015; Reverso, see Reverso-Softissimo, 2015) offer free general-purpose translations fairly acc
2015
AI Access Foundation. rights reserved.

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

ceptable levels quality large number language pairs. fact
easily accessible computer, tablet smartphone connected Internet
contributed create huge community heterogeneous users.
However, SMT systems significant limitations produce translation errors
different levels (e.g., morphology agreement, phrase structure reordering, lexical
selection, fluency). due inherent complexity task also
limitations currently available translation models training corpora,
might fully representative domain, genre style texts translated.
behavior may cause frustration fatigue users; users
right position spot mistakes. response machine translation (MT)
systems developers allow users provide feedback proposing corrections
system-generated translations. Gathering new improved information users
edits shown valuable resource improve translation systems online cost-free services (Simard, Goutte, & Isabelle, 2007; Ambati, Vogel, & Carbonell, 2010;
Potet, Esperanca-Rodier, Blanchon, & Besacier, 2011) professional computer-assisted
translation frameworks (Bertoldi, Cettolo, & Federico, 2013; Mathur, Mauro, & Federico,
2013). raising interest topic MT community emerged also
form research projects (e.g., MateCat, FAUST, Casmacat, see European Comission
- 7th Framework Program, 2010) specialized workshops (e.g., Workshop Post-editing
Technology Practice MT Summit XIV) special issues (such Machine
Translation Journal Special Issue Machine Translation MT Post-editing).
article explore use real translation feedback non-professional
users. aim automatically improve general translation quality underlying MT system. approach differs significantly common setting
professional post-editors used produce high-quality translations imperfect
automatic output. setting users casual, limited skills, sometimes
low command languages translated. perform free edits
machine-translated text produce supposedly better alternative translation. translation sometimes proper post-edition, frequently partial, contains errors,
simply piece unrelated text. challenge particularly noisy setting
able filter part noise select potentially useful translation edits.
One advantage crowd-sourcing approach MT enrichment potential
reach vast community contributors. scenario conveys mutual-interest framework:
one side, active user wills correct translations long system responds
better needs future; side, system requires input
intelligent agent, able provide information improve translation models.
high level engagement achieved, committed live community constantly
contribute improve free on-line translators. Another fundamental aspect necessary
reach circle mechanism efficiently accurately incorporates user feedback
translation engine. Accurately, want MT system repeat
mistakes and, time, worsen overall translation quality; efficiently
engage users, need system react quickly (if instantaneously) feedback.
Even though exploitation user edits (UE) widespread practice increasing
interest, methods aim improving existing MT models. work highly focused translation dictionaries centered minimizing out-of-vocabulary (OOV)
160

fiLeveraging Online User Feedback Improve SMT

ratio translations (Cettolo, Federico, Servan, & Bertoldi, 2013). Studying performance enriched translation models variety aspects translation quality
(e.g., morphology, word ordering, lexical selection, etc.) issue deserves
attention MT community.
work explore extent use translation edits collected
non-professional users commercial on-line translation portal improve translation
quality general purpose SMT system. main contribution twofold. First,
address noisy crowd-sourcing scenario training supervised classifiers identify useful
UE instances. Second, devise SimTer, pivot-based method aligning user-edited
translations source text original automatic translations, aim
detect specific corrected errors build enriched translation models accordingly.
aspects, UE filtering pivot-based selection phrase pairs, novel show
contribute significantly better translation quality. support claim
extensive experimentation analysis. improvement achieved remarkable compared
simple corpus-concatenation strategy, since work relatively low quantity
UEs. Additionally, conduct manual evaluation output enriched system.
study reveals strengths weaknesses enriched system source
improved results, achieved means reduction OOV
ratio. Interestingly, linguistically-founded aspects translation quality also
improved. Finally, show generality approach successfully applying
architecture models noisy domain-adaptation scenario, new examples
come automatically-crawled bilingual corpora filtering noisy examples
key aspect adaptation.
rest article organized follows. Section 2 puts current article context
overviewing related work. Section 3 describes locally evaluates classification
approach identifying useful UE instances. Section 4 discusses proposal improving
machine translation models UEs. Section 5 presents experiments real datasets
showcasing proposed methodology. Finally, conclusions presented Section 6.

2. Related Work
section overview relevant work related two main contributions
article: automatically identifying useful user edits (UEs) improving existing
translation models (TMs) basis UEs.
Identifying useful UE instances necessary many ones collected
non-professional users may represent better translations compared ones
produced system itself. best knowledge, research
particular topic exists. analog tasks found either obtaining quality material
filtering automatically gathered corpora domain adaptation. Usually, subsampling selection method (Foster, Goutte, & Kuhn, 2010; Axelrod, He, & Gao, 2011)
used dealing corpus selection problems. consists simple rationale:
language model (LM) created reliable in-domain data and, subsequently,
parts corpus lower perplexity respect model selected.
scenario, select best UE instead.
161

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

previous research topic (Pighin, Marquez, & May, 2012; Barron-Cedeno
et al., 2013) defined basic set features perform identification. aim
capturing different aspects, as: (i) whether UE text adequate translation
source sentence automatic translation (computed simple surface
similarity features), (ii) whether UE includes mistakes typos, (iii) whether
source contains mistakes typos prevent obtaining sensible translations.
identification cast supervised classification problem using mentioned
features. work extends approach, explained evaluated Section 3.
Assuming set good edited translations (revised expert), enrichment
translation system involves two separate steps. First, alignment source
sentences edited translation computed (alignment). Second, improved translation
models learned using alignments (adaptation). Regarding alignment step,
widely accepted best possible alignment obtained adding training corpus, new sentence pairs edited sentence treated target-side models
estimated schratch (Hardt & Elming, 2010). However, large amount training
data makes approach computationally expensive; obstacle goal reacting
quickly user feedback. overcome problem, several incremental alignment
models proposed literature(Levenberg, Callison-Burch, & Osborne, 2010).
exception stream-based translation approach, adds updates
original TM scores according new material (Ortiz-Martnez, Garca-Varea, & Casacuberta, 2010; Martnez-Gomez, Sanchis-Trilles, & Casacuberta, 2012; Mathur et al., 2013),
adaptation step usually carried creating specific translation tables
edited translations (using standard phrase-extraction phrase-scoring algorithms)
combining original translation tables. important note
work incremental adaptation tested scenarios references used instead UEs. Hence, related work mainly addresses simulated artificial
scenarios conclusions might totally representative. models use
human-edited translations. Mathur et al. (2013) along Bertoldi et al.(2013) rely
business-oriented specific corpus computer-assisted translation. Simard et al. (2007)
introduced so-called automatic post-editors (i.e., monolingual SMT systems designed
improve translation errors). Potet et al. (2011) considered small corpus UEs
non-professional users, incremental methodology. basis previous
aspects describe next selected papers.
Hardt Elming (2010) proposed approach produce approximate alignments.
defined approximate alignment one allows extend phrase table,
even perfect exact. Given hsource, user-editi pair baseline alignment
(Och & Ney, 2003), explored available links selecting ones producing
highest probability according IBM model 4. Moreover, improved alignments
applying two heuristics: first non-aligned word source aligned first
non-aligned word UE unlinked fragment pairs surrounded corresponding
alignments linked. One drawback method alignments produced
noise-sensitive, built upon heuristics. makes methodology unpredictable
unstable deal words seen baseline alignment model.
adaptation step, built separate phrase table UEs decoded
phrase tables. used approximate exact GIZA++ alignments showed
162

fiLeveraging Online User Feedback Improve SMT

performance approximate alignment yields half improvement obtained
GIZA++ alignment. However, simulated data, using references instead actual
human UEs, considered work.
Ortiz-Martnez et al. (2010) Martnez-Gomez et al. (2012) applied incremental version Expectation Maximization (EM) algorithm (Neal & Hinton, 1998)
minimizes error function small sequences mini-batched data. paradigm
commonly known stream-based translation, small portions data processed
time. specifically, incrementally adapted seven models including language models, length penalty, phrase translation models, distortion. strategy reported
perform reasonably well non-stationary environments fast adaptation required,
interactive machine translation (Ortiz-Martnez, Sanchis-Trilles, Gonzalez-Rubio, &
Casacuberta, 2013). general-purpose web-based scenario, resulting models
sensitive lately integrated data. approach normalization carried
due computational cost. Mathur Federico (2013) opted leaving original
features unaltered added extra (normalized) feature reflects impact UEs
adaptation system.
Simultaneously Ortiz-Martnez et al. (2010), Levenberg et al. (2010) proposed
on-line strategy phrase-based model enrichment using stepwise EM alignment algorithm (Cappe & Moulines, 2009). incorporation new knowledge approach
based re-estimation scores phrase table re-computing counts
throughout computationally efficient dynamic suffix array (Callison-Burch, Bannard, &
Schroeder, 2005; Lopez, 2008). suffix array contains starting index suffix
string containing phrases lexicographical order, allowing easy computation on-the-fly translation probabilities given source phrase. dynamic variant
suffix array supports deletions insertions, making suitable stream-based
approach. Moses uses algorithm provide incremental training strategy (Haddow
& Germann, 2011).
mGIZA++ parallel implementation IBM HMM models (Gao & Vogel, 2008).
byproduct, performs forced alignment,1 alternative step-wise
incremental EM approaches. mGIZA++ builds multiple alignment models parallel,
allowing filtering merging afterwards produce exact alignment
aggregation. Consequently, one obtain forcedly-aligned material improved
alignment models quality concatenated dataset used
beginning. tool efficient terms processing time storage requirements,
making suitable exact incremental alignment.
contrast alignment methods described above, pivot-based approach tries
identify specific edits performed user original translation project
source (Henrquez, Marino, & Banchs, 2011). precisely, uses original
translation pivot obtain alignments source UE translation,
enrich existing translation models. advantage approach
allows spot incorrect fragments translation, potential source errors
MT system. Therefore, new alignments obtained edited fragments
ones promoted within TMs enriched translator. Formiga et al. (2012)
1. term forced alignment refers coercively aligning unseen parallel text selecting maximum
probability given model, even value low.

163

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Blain, Schwenk, Senellart Systran (2012) studied idea perform adaptation
good results. recently developed MateCat tool (Matecat, 2015), Web-based CAT
tool, good example category. approach three advantages: fast,
rely baseline alignment models, low memory requirements.
previous pivot-based alignment implementations, edit distance computed
translation-error-rate (TER) alignment algorithm, takes account reordering operations paraphrasing. strategy basis methodology presented
article, explain detail Section 4.1. Concerning adaptation strategy,
Formiga et al. (2012) combined UE-specific translation models baseline models using Foster Kuhns (2007) interpolation empirically-set weights. Blain et al. (2012)
studied two decoding strategies considering baseline UE phrase-tables separately:
back-off, phrase found baseline translation model, phrase table considered multiple decoding, phrase found translation models,
translations scores used. found multiple decoding best strategy
restricting TER alignment substitution operations (i.e., neglecting addition,
deletion, shifting edits). refined combination methods presented recently. Sennrich (2012) used L-BFGS algorithm (Byrd, Lu, Nocedal, & Zhu, 1995)
find optimal interpolation values feature function TMs. Bisazza, Ruiz
Federico (2011) defined fill-up strategy complement missing information
original TMs. Nevertheless, methods challenged incremental
scenario.

3. Learning Classify User Edits
section describes strategy identify useful user-edits improving machine
translation system. approach task binary classification problem identifying
cases edited translation adequate system-produced translation
positive. so, follow previous work human feedback filtering
(cf. Section 2).
3.1 Training Corpus
training material used EnglishSpanish Faust Feedback Filtering (FFF+ ) 2 corpus, developed within FAUST EU project.It contains 550 examples real translation
requests user-edits Reverso.net translation Web service. example contains seven fields:
SRC source sentence English (users translation request system);
TGT automatic translation SRC Spanish;
UE potentially improved user-edit TGT provided user;
BTGT automatic translation TGT back English;
BUE automatic translation UE back English;
LANG language set translators interface;
2. Available ftp://mi.eng.cam.ac.uk/data/faust/UPC-Mar2013-FAUST-feedback-annotation.tgz.

164

fiLeveraging Online User Feedback Improve SMT

CL class label, i.e., whether UE adequate translation SRC TGT.

Observe language set translators interface likely indicator
users native language; factor may indicate user edits language
reliable. 550 examples FFF+ independently labeled acceptable
unnaceptable edits two human annotators native Spanish speakers
high command English. cases disagreement (100) discussed consensus
reached. main criterion decide whether user-edit acceptable based
translation adequacy (i.e., degree meaning source sentence
conveyed translation). Concretely, UE considered acceptable strictly
adequate TGT, even still imperfect. believed lax criterion acceptability
could work well proxy usefulness, thinking enriching MT system.
detailed description annotation guidelines, including examples, one may refer
work FAUST (2013, Section 3.2). levels inter-annotator agreement achieved
(Cohens kappa 0.50.6) considered moderately high. fact evinces
inherent difficulty task, even humans. positivenegative distribution
corpus almost balanced: 50.5% versus 49.5%, indicating edits provided casual
users noisy.

3.2 Learning Features
considered five sets features characterize examples fields relationships
them: surface comparison, back-translation, noise-based, similarity-based, quality estimation. first four sets require external resources MT system
target back source language (having system hand likely,
necessary resources build practically original-direction
system). fact makes particularly appealing under-resourced languages.
fifth set composed combination well-known MT quality estimation measures.
cases, features extracted text pre-processing, including case-folding diacritics
elimination original texts incorporated translation system. Following,
provide short description main principles guiding family features.
full list, comprising 90 individual features, described detail FAUST (2013,
Section 3.2).

3.2.1 Surface Features
consider text strings SRC, TGT UE, compute surface similarities among
level word tokens characters (length, length ratios, Levehnstein distance,
vocabulary containment, etc.). also binary feature account language
interface (LANG).
165

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

3.2.2 Back-Translation Features
features account also surface properties, considered back-translations
TGT (BTGT) UE (BUE). Levenshtein distances token character level
well vocabulary intersections included.3
3.2.3 Noise-Based Features
binary features intended determine likelihood text fragments include noisy sections. consider instances SRC UE include characters repetitions
(longer three characters) tokens whose length high regular word (>10
chars). try determine length-based translation difficulty asuming
longer sentences harder translate (we consider features ranges [1 5],
[6 10], [11, ) words).
3.2.4 Similarity-Based Features
set includes different similarity measures SRC, TGT, UE back-translations.
Borrowing concepts cross-language information retrieval estimate cosine similarities
across languages using character 3-grams (Mcnamee & Mayfield, 2004). machine
translation consider models parallel corpora alignment based pseudocognates lengths (Simard, Foster, & Isabelle, 1992; Gale & Church, 1993; Pouliquen,
Steinberger, & Ignat, 2003). features intend alternative Levenshtein-based
features surface back-translation sets.
3.2.5 Quality-Estimation-Based Features
applied 26 system-independent quality estimation (QE) measures provided
Asiya (Gimenez & Marquez, 2010) SRCTGT SRCUE pairs. measures intend estimate translation quality without human references, making appealing
current framework. QE measures quite shallow, incorporate linguistic
information level part speech, syntactic phrases, named entities. bilingual
external dictionary also used. Consequently, say set features contains
linguistically-oriented generalizations previous ones. Perplexity, number
out-of-vocabulary words translation sentence, well bilingual-dictionary-based
similarity SRC UE (or TGT) included among 26 measures.
3.3 Classifier Learning Intrinsic Evaluation
trained support vector machines (SVM) previously described features learn
classifiers. used SVMlight (Joachims, 1999) linear, polynomial, RBF kernels
tuned classifiers 90% FFF+ corpus. remaining 10% left
aside testing purposes. Feature values clipped fit range 3 2
decrease impact outliers. Normalization applied means z-score:
x = (x )/. Later on, mean standard deviation tuning dataset used
normalize remaining test set instances.
3. back-translations produced Spanish-to-English MT engines using Reverso.net
technology.

166

fiLeveraging Online User Feedback Improve SMT

Table 1: Cross-validation results linear SVMs trained incremental sets features,
without application feature selection. Best results italic faced.

feature sets
surface + bt
+ noise
+ similarity
+ QE

F1
64.6
70.1
69.3
72.0

features
P
R Acc
63.5 65.7 63.0
63.7 78.0 65.9
64.3 75.2 65.9
67.2 77.6 69.1

feature selection
F1
P
R Acc
67.8 65.7 70.1 65.9
73.5 67.0 81.5 69.9
73.6 68.1 79.9 70.5
76.1 71.0 81.9 73.5

evaluated basis standard measures: classification accuracy, precision (ratio predicted useful instances instances classified useful), recall (ratio
predicted useful instances useful instances dataset), F1 (harmonic
mean precision recall). training strategy aimed optimizing F1 consisted
two iterative steps: (a) parameter tuning: grid search appropriate SVM
parameters (Hsu, Chang, & Lin, 2003), (b) feature selection: wrapper strategy, implementing backward elimination discard redundant irrelevant features (Witten & Frank,
2005, p. 294). short, process performs iterative search remove worst feature
dataset time, according performance obtained classifier
neglects feature. See details work Barron-Cedeno et al. (2013).
present incremental evaluation see contribution every feature
family effect feature selection. base feature set composed surface
back-translation (bt) features. Then, incrementally add noise-based, similarity-based
quality estimation (QE) features. Table 1 presents results. Figure 1 displays
corresponding precisionrecall curves. learning setting restricted linear SVMs
experiment. first observation feature selection procedure consistently
provides better accuracy F1 scores; i.e., allows discard irrelevant features also
harming ones. Results show feature families contribute positively final
performance classifiers, gains accumulative. improvement especially
noticeable quality estimation features come play feature filtering applied.
numerical results backed shape precision-recall curves: including
feature sets leads better results, precision levels clearly 70% recall levels
6070%.
complementary study using non-linear kernels task (not included
brevity), revealed even though slightly better accuracy F1 results obtained
using non-linear kernels, shape precisionrecall curve better linear
classifier high-precision zone.4 Avoiding false positives important property
thinking selecting useful user edits MT improvement. Therefore, used linear
classifiers translation experiments Section 5. extended study, including
kernel variants, available description report FAUST (2013, Section 3.4).
report also includes detailed experiments relevance individual features
4. values 0.6 precision, curve linear classifier smoother contains much larger
area it.

167

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

surface+bt
+noise
+similarity
+QE

1

Recall

0.8

0.6

0.4

0.2

0

0.5

0.6

0.7
0.8
Precision

0.9

1

Figure 1: Precision-Recall curves different learning settings feature sets
development partition. Black dots represent optimal F1 values.

comparison example rankings produced different classifier variants
set examples.
Finally, evaluated different classifiers 10% test partition FFF+
corpus. Absolute resuts slightly lower: linear SVM feature sets obtains F1
accuracy values 73.2 69.9, respectively,5 results observed.
is, precisionrecall curves linear SVMs including feature sets better rest,
allowing obtain higher precision scores similar levels recall.
3.4 Discussion
classifiers analyzed section showed modest levels precision, slightly
70% acceptable levels recall. Significantly higher precision values reached
price lowering recall 10%. behavior reflects difficulty confidently
characterizing positive examples type features used train classifiers.
worth noting task also difficult humans. agreement achieved
annotators (Cohens kappa 0.6) considered moderately high, certainly
fully satisfactory, evincing inherent difficulty task. Translation quality
multi-faceted concept, encompasses adequacy (i.e., whether translation conveys
meaning source), fluency (i.e., whether translation fluent utterance
target language) many aspects; defined application basis
(e.g., vocabulary usage, language register, post-editing effort, etc.). result, human
perception translation quality highly subjective matter, depending small details,
difficult capture delimit set simple annotation guidelines. effect
amplified corpus noisy nature input text, input sentences
5. FFF+ test set small susceptible statistical unstability computing performance
scores.

168

fiLeveraging Online User Feedback Improve SMT

often lack necessary context make fully reliable decisions. Fortunately, classifying
good user edits end task. ultimate goal UE classifiers perform
selection, i.e., rank UE instances acceptability set appropriate threshold
select useful UEs improve SMT engine. following sections empirically
show that, regardless limited performance local classifiers, proper inclusion
selected highest-ranked user-edited translations general purpose SMT
significantly improve quality. Finally, one might argue rather training classifiers
optimize accuracy local task, would better define joint UE-selection
MT-enrichment learning setting, classifiers optimized directly
translation quality enriched SMT system. Although attractive theory,
would extremely inefficient practically infeasible pipeline.

4. Method Incorporating User-Edits SMT System
section describe method incorporating user-edits machine translation
model. approach developed assumption translation model
user-edited materials data disposal improve translation system.
approach divided two steps: (i) using original automatic translation
pivot align source text edited translation extracting new phrase pairs
(Section 4.1), (ii) interpolation new phrase pairs original translation
model (Section 4.2). validation experiments discussed Section 4.3.
4.1 Pivot-Based Word Alignment
order detect correct translation errors, consider three pieces information:
source input text SRC, target output translation given translation system
TGT, user-edited version UE. monolingual alignment TGT UE allows
predicting translation errors, identifying parts edited. projection
alignment SRC allows extraction new translation pairs, representing
corrections provided UE.
propose three-step alignment procedure. First, compute TER path (Snover,
Madnani, Dorr, & Schwartz, 2009) TGT UE, aligning identical words
sides. Second, estimate best alignment among possible combinations
unaligned TGT UE words maximizing similarity function pairs words
TGT UE sim(wt , wu ). Finally, monolingual alignment made, pivot
alignment towards SRC: taking advantage decoder-provided word alignment
SRC TGT, link words SRC UE word TGT
connects them. alignment process, call SimTer described Algorithm 1.
comparison translated edited sentences based translation
edit rate, TER (Snover et al., 2009).6 addition minimum number edits, TER
obtains alignment edit path: required sequence edits change output
translation reference user-edited sentence case. Figure 2 shows
6. TER error metric estimates number edits required convert translation output
one references. Although based Levenshtein distance, TER additionally allows word
movements considering usual addition, deletion, substitution operations order reduce
number changes.

169

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Algorithm 1

SimTer. pivot-based algorithm align SRC UE TGT

1: Translate SRC TGT decoder obtain corresponding word alignments

align(ws , wt ) every ws SRC wt TGT;
2: Compute TER path TGT UE align identical words (cf. Section 4.1);
3: Align every non-aligned word wt TGT words wu UE summation

similarities new pairs, Sim(wt , wu ), maximized;
4: every pair alignments al(ws , wt ) al(wt , wu ), create new alignment al(ws , wu );
easy find right mix
right mix easy find


e

e

e

e

e

e

e dd

Finding right mix easy
easy find right mix

Finding right mix easy

Figure 2: TER-path computed monolingual sentence pair. d, e stand deletion,
exact (no change), substitution. minimum number edits three: two deletions
one substitution.

example. First, upper part figure, TER allows word movements, right
mix moved next first word. minimum number edits computed,
resulting one substitution two deletions. Finally, lower part figure,
word movements roll back original positions obtain word alignment
two sentences.
Although TER able align words correctly, may fail circumstances words affected edit actually aligned according
position sentence, rather semantic similarity. example Figure 2,
finding find aligned. counter measure issue, propose
consider similarity wt wu linear combination simple similarity
features:
8
X
sim(wt , wu ) =
hi (wt , wu ) ,
(1)
i=1

hi (wt , wu ) similarity features corresponding contribution
weights. hi models different relationship wt wu follows:
h1 - binary feature indicates wt wu identical.
h2,3 - Two binary features accounting existence links wt1 wu1 (h2 )
wt+1 wu+1 (h3 ).
170

fiLeveraging Online User Feedback Improve SMT

h4 - feature penalize multiple links (onetomany). considers possibility
link(wt0 , wu ) aligning user-edited word wu TGT word wt link
wt0 wu already set. feature penalizes new link proportionally
distance wt wt0 :


kpos(wt ) pos(wt0 )k
h4 (wt , wu ) = max
,
(2)
|TGT|t
link(wt0 ,wu )
pos() position TGT |TGT|t number tokens TGT.
h5,6 - Two lexical features designed evaluate strength semantic relationship
wt wu according proximity ws . done considering
bidirectional conditional probabilities (ws , wt ) (ws , wu )
translation table. Feature h5 (wt , wu ) approximated normalized conditional
probability based bilingual dictionaries:

h5 (wt , wu ) =

X
s:link(ws ,wt )

=

p(wt | ws )p(ws | wu )
0
0 UE p(wt | ws )p(ws | wu )
wu

(3)

p(ws | wu )
,
0
0 UE p(ws | wu )
wu

(4)

P
X

P
s:link(ws ,wt )

normalization factor included order consider contribution p(ws |
wu ) context sentence UE. Feature h6 analogous h5 , considering
p(ws | wt ) p(wu | ws ) instead. ws unknown word (i.e., replicated
SRC TGT without mapping bilingual dictionaries), take h5 = h6 = 0.
h7 - feature applied wt unknown word duplicated
translation system input sentence output. wu wt
same, force linked giving h7 arbitrarily large value. Otherwise,
feature takes real value function Levenshtein distance (LD) character
level unknown wt wu :
h7 (wt , wu ) = 1

LD(wt , wu )
,
|wu |

(5)

|wu | represents length wu characters. feature becomes penalty
Levensthein distance exceeds length wu , preventing linking
longer unrelated words.
h8 - penalty feature prevent alignments unknown word wt stopword
wu . takes large negative value wu stopword; zero otherwise. take
stopwords determiners, articles, pronouns, prepositions, auxilary verbs.
weights relative hi feature obtained downhill simplex
algorithm (Nelder & Mead, 1965). give details Section 4.3.2.
171

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Figure 3: Process compute interpolation weight () re-tune TM feature
coefficients ().

4.2 Incorporation Aligned Phrase Pairs
computing SimTer word alignment source edited translations,
use new parallel corpus (SRC,UE) enrich retrain translator.
deal two types newly extracted phrase pairs (translation units): (i) previouslyseen
phrase pairs already included original translation model; relatively
up-weighted within translation models, favoring selection facing similar
situation, (ii) new phrase pairs missing original translation model
represent principal resource improve translator. New phrase pairs must
added final translation model order provide decoder new translation
options facing similar source sentences.
include new phrase pairs original model using Foster Kuhns (2007)
interpolation method, initially designed address domain adaptation problems. relatively small translation models extracted user edits estimated means
symmetrization phrase-extraction standard algorithms grow-diag-final-and heuristic. Then, original new translation models linearly interpolated according
to:
TM(phrasei ) = TMoriginal (phrasei ) + (1 ) TMpe (phrasei )
(6)
TM, TMoriginal TMpe final, original, UE-based translation model
scores phrasei pair. setting interpolation parameter strongly coupled
re-tuning classical set weights () used combine SMT features.
applied iterative two-step process outlined Figure 3. process starts set
weights 1 original translation system iterates updates
weight yield BLEU improvements. two-step iterative process. First, best
172

fiLeveraging Online User Feedback Improve SMT

Table 2: Statistics EnglishSpanish corpora used obtain SimTer similarity
function weights .
Corpus
EPPS
EPPS UE

Sent.
Eng
Spa
Eng
Spa

1.90
100

Words
49.40
52.66
2,862
3,022

Vocab.
124.03 k
154.67 k
1,017
1,089

avg.len.
26.05
27.28
28.6
30.2

weighting computed using best i1 available. case = 1 interpolation
taken account (0 = 0). Afterwards, optimum interpolation parameter
computed using best set translation feature weights obtained.
combination procedure simpler reordering models. follows fill-up
strategy preserves every entry score original model adds new entries
scores new phrases (Bisazza et al., 2011). Units appear
original model one obtained user-edits preserve reordering score
original model.
4.3 Validation Experiments
objective experiments described section twofold: (i) tuning metaparameters algorithms (ii) validating proposed methodology well established domain-adaptation task. consider experiments preliminary, translation
references used instead proper user edits.
4.3.1 Datasets
selected different datasets experiments. order optimize parameters
similarity function Equation (1), used Europarl v6 corpus, EPPS (Koehn,
2005), build base phrase-based SMT system. evaluate alignments, small
manually-aligned corpus, EPPS UE (Lambert, de Gispert, Banchs, & Marino, 2005),
used perform pivot-translations subsequent SimTer alignments. small corpus
belongs domain EPPS intersection them. Table 2
shows statistics.
order tune parameters, validate proposed methodology,
used corpora WMT12 campaign (Callison-Burch, Koehn, Monz, Post, Soricut,
& Specia, 2012). contains parallel sentences EPPS corpus already mentioned,
News Commentary (NC), United Nations (UN). also includes monolingual version
Europarl monolingual corpora based News (broken years: 2007-2011).
Tables 3 4 provide descriptive statistics datasets, computed cleaning
pre-processing. Additionally, used WMT08-11 test material tuning
TMs (dev), WMT12/13 tests testing methodology (test12 test13).
Table 5 shows statistics tuning/test material.
173

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Table 3: Statistics additional WMT12 EnglishSpanish parallel corpora training
translation models (used tuning validation purposes).
Corpus
NC
UN

Eng
Spa
Eng
Spa

Sent.
Words Vocab.
Preliminary Experiments
3.73
62.70 k
0.15
4.33
73.97 k
205.68 575.04 k
8.38
239.40 598.54 k

avg.len.
24.20
28.09
24.54
28.56

Table 4: Statistics Spanish monolingual corpora used build language models.
Corpus
EPPS
NC
UN
News.07
News.08
News.09
News.10
News.11

Sent.
2.12
0.18
11.20
0.05
1.71
1.07
0.69
5.11

Words
61.97
5.24
372.21
1.33
49.97
30.57
19.58
151.06

Vocab.
174.92 k
81.56 k
725.73 k
64.10 k
377.56 k
287.81 k
226.76 k
668.63 k

avg.len.
29.18
28.55
33.24
28.91
19.19
28.63
28.54
29.55

Table 5: Statistics development test corpora used tune test translation
system.
Corpus
dev
test12
test13

Eng
Spa
Eng
Spa
Eng
Spa

Sent.
Words Vocab.
WMT based dev/test
189.01 k 18.61 k
7,567
202.80 k 21.75 k
63.78 k 14.34 k
3,003
69.45 k 16.47 k
56.09 k 13.34 k
3,000
62.05 k 15.16 k

avg.len.
25.0
26.8
21.2
23.1
18.7
20.7

4.3.2 Tuning Parameters Similarity Function
built baseline SMT system following standard pipeline Moses training
EPPS (Koehn & Hoang, 2007). applied resulting system translate source
side manually-aligned corpus (EPPS UE). Then, carried downhill simplex
process adjust weights (Nelder & Mead, 1965), except 8 fixed
1. Recall h8 assigns large costs prevent alignments unknown words
function words. activated, works hard constraint, pruning hypotheses
space, value 8 chosen arbitrarily positive number different
zero. SimTer applied three completed iterations. final goal
produce alignment source sentence edited translation. Therefore,
evaluated using Alignment Error Rate (AER) respect manual sourcereference
174

fiLeveraging Online User Feedback Improve SMT

Table 6: Contribution weights similarity function features.
1
2
3
4
5
6
7
8
0.08 0.75 0.91 3.08 0.47 2.02 1.50 1.00
Table 7: Translation quality different values .

BLEU

0.1
27.86

0.2
28.18

0.3
28.33

0.4
28.49

0.5
28.68

0.6
28.75

0.7
28.69

0.8
28.62

0.9
28.45

alignment (Och & Ney, 2003). AER reduced 20.12% 17.57% (13% relative error
reduction), reaching performance equivalent mGIZA++ corpora.
Table 6 includes resulting s. 4 value shows, penalization distant links
(h4 ) important feature. lexical features, h6 significantly
relevant h5 . Interestingly, feature h1 (same word) considered relevant
compared others.
4.3.3 Tuning Validating Combination Method
last step fully testing strategy compute parameter equation (6). mentioned before, used corpus WMT12 campaign (CallisonBurch et al., 2012). trained baseline English Spanish system factored Moses
phrase-based system (Koehn & Hoang, 2007) words words POS tags (Formiga
et al., 2012).7 base system considered EPPS UN concatenated whole corpus.
Regarding monolingual data, language model (LM) built corpus
interpolated minimizing perplexity development set (Schwenk & Koehn,
2008). experiment translated English sentences NC parallel corpus
took Spanish references simulate user edited translations (UE). performed
SimTer word alignment build UE-specific translation reordering models. Finally,
applied optimization method depicted Section 4.2.
Table 7 shows BLEU scores obtained different values . combining
translation models, BLEU improved 27.86 28.75, achieving highest value
= 0.6 (i.e., 6040% distribution weight base edited translation
models, respectively). setting , validated adaptation method
obtained hyper-parameters. also compared combination method (referred
Linear Interpolation ( = 0.6)) methods available Moses, namely:
Concatenation NC, EPPS, UN aggregated single training corpus.
Multiple Tables Either Two parallel decodings corresponding TM launched
separately, selecting one best score.
Multiple Tables One decoding launched considering options available
phrase tables, doubling number translation features log-linear
model.
7. text POS-tagged Freeling suite NLP analyzers (Padro, Collado, Reese, Lloberes, &
Castellon, 2010).

175

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Moses Incremental training base phrase-table updated approximate alignments (see Levenberg et al., 2010 Section 2) . alignments computed
Moses incremental inc-giza-pp algorithm instead SimTer algorithm.

Table 8: Results different combinations base SimTer-specific translation models.
BLEU NIST columns, indicate significant differences
Concatenation system 0.95 0.90 confidence levels, respectively. Best
absolute results depicted bold face. Moses incremental training shown
comparison purposes although use SimTer alignment

test12
Base
Concatenation
Linear Interpolation ( = 0.6)
Multiple tables either
Multiple tables
Moses Incremental training
test13
Base
Concatenation
Linear Interpolation ( = 0.6)
Multiple tables either
Multiple tables
Moses Incremental training

BLEU

NIST

TER

METEOR

ULC

32.77
33.03
33.25
33.20
31.72
32.58

8.63
8.66
8.70
8.70
8.51
8.61

48.65
48.48
48.24
48.19
49.42
48.86

55.48
55.64
55.84
55.76
54.88
55.40

70.78
71.16
71.66
71.61
69.21
71.04

28.74
28.96
29.15
29.13
28.02
28.57

8.01
8.07
8.07
8.07
7.91
7.97

51.60
51.29
51.28
51.29
52.40
52.01

52.82
53.11
53.15
53.03
52.26
52.67

70.94
71.56
71.73
71.66
69.52
70.74

used BLEU, NIST, TER METEOR (Papineni, Roukos, Ward, & Zhu, 2002;
NIST, 2002; Snover et al., 2009; Denkowski & Lavie, 2011) automatic translation
quality measures. Additionally, considered linear combination former ones:
ULC (Gimenez & Marquez, 2010). Table 8 presents obtained results. adaptation
strategy outperforms Baseline Concatenation configurations practically every test
corpora quality measure. precisely, found significant differences BLEU
(test12 test13) NIST (test12) metrics.8 performance either combination close linear interpolation method. However, either combination
computationally expensive, almost doubling time required linear interpolation
method. Table 9 includes translation times reference.9 incremental approach
fastest one, yields improvement baseline.
summary, alignment combination methods proposed work offer significantly better results without sacrificing computational efficiency, compared
alternative methods provided Moses. Therefore, consider multiple decoding
incremental strategies remaining experiments paper.
8. work, significances computed paired bootstrap resampling (Riezler & Maxwell, 2005)
9. figures computed Linux server 96 GB RAM 24-core CPU Xeon processors
1.6 GHz (134064 Bogomips total). Multi-threading used compute decoding times.

176

fiLeveraging Online User Feedback Improve SMT

Table 9: Translation times seconds (Collecting+Decoding) combination method.
Combination
method
Moses Incremental training
Linear Interpolation ( = 0.6)
Multiple tables
Multiple tables either

Num.
sent.
3,003

Total time
test12
test13
7,502.52
6,553.45
8,284.79
7,684.65
13,353.80 12,291.70
13,097.40 11,364.80

Time per Sentence
test12
test13
2.50
2.18
2.76
2.56
4.10
4.45
4.36
3.79

Table 10: Statistics EnglishSpanish parallel corpora used FAUST scenario.
Corpus
FAUST UE
FAUST dev Clean

FAUST test Raw

FAUST test Clean
FAUST Monolingual

Sent.
Eng
Spa
Eng
Spa ref0
Spa ref1
Eng
Spa ref0
Spa ref1
Eng
Spa ref0
Spa ref1
Spa

6,610
1,998

998

1,996
98,199

Words
43,310
47,800
24,588
24,588
25,270
9,941
10,135
10,333
19,773
20,270
20,666
1.15

Vocab.
8,250
10,430
3,758
3,758
3,743
4,184
4,484
4,499
4,737
4,484
4,499
89,378

avg. length
6.6
7.2
12.3
12.3
12.6
10.0
10.2
10.4
9.9
10.2
10.4
11.67

5. Experiments Real Data
section present experiments using methodology improve already existing
MT systems real data. describe two experiments. first scenario, new
material comes collection user-edited translations submitted Reverso.net
MT Web service (cf. Section 5.1). second scenario, new material selected (cf.
Section 5.2) CommonCrawl (Smith et al., 2013)
5.1 User-Provided Edited Translations (FAUST)
FAUST project (cf. Section 3) goal improve quality on-line MT
services leveraging users feedback, mainly form suggested improved translations.
experiments, take advantage parallel monolingual data supplied set
user translation queries edits. users belong on-line community
motivated edit response translation queries.
FAUST parallel corpora composed two non-overlapping collections translation requests gathered Reverso.net website: FAUST UE FAUST dev/test.10
FAUST UE includes triplets composed input source, MT output, user edit.
use corpus training purposes. FAUST dev/test corpus includes target refer10. sample FAUST UE available ftp://mi.eng.cam.ac.uk/data/faust/FaustFeedbackSample.
xls.gz FAUST User-edited corpus. FAUST dev/test available ftp://mi.eng.cam.ac.
uk/data/faust/FAUST-1.0.tgz.

177

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

ences provided two professional translators. Moreover, translators processed
source inputs reduce noise (e.g., removing slang words, misspellings, smileys, etc.).
process resulted two versions dev/test corpus: Raw, source inputs
original ones, Clean. experiments considered FAUST dev Clean version tuning (less error prone), real FAUST test Raw testing. FAUST
monolingual corpus composed 98,199 translation requests Reverso.net website
Spanish source language. selection respect target language
made. Table 10 shows statistics FAUST corpora.
first experiment took Concatenation MT system Section 4.3.3
adapted target language model FAUST scenario follows: (i) built
specific web-domain language model FAUST Monolingual corpus, (ii) obtained
language model means new interpolation language models according
perplexity minimization FAUST dev Clean corpus, (iii) tuned weights
translation features using MERT maximize BLEU FAUST dev Clean
corpus. goal set strong baseline system experimental comparison
datset. refer Base FAUST.
order select suitable feedback material improve Base FAUST
system, ranked FAUST UE collection 6,610 user-edited instances according
SVM classifier scores (cf. Section 3). SVM labeled 61% data useful feedback
(we call point TH0, decision threshold=0). However, guarantee
level selection maximizes translation quality adapted system. Therefore,
carried analysis quality function percentage selected user-edited
instances.
5.1.1 Results
Figure 4 depicts performance obtained FAUST test Raw corpus different
percentages selected user-edited data. figure focuses two evaluation metrics:
NIST, based classical n-gram matching approach improved brevity
penalty providing robustness noise BLEU TER, tries mimic
editing effort would addressed humans order obtain high-quality
translation. similar curves observed using FAUST test Clean (not shown
brevity). Table 11 presents complete comparative results FAUST Raw corpus,
set extended evaluation metrics Section 4.3.3: BLEU, NIST, METEOR,
TER, ULC. Different adaptation filtering strategies also presented
table, including: (i) different filtering methods (FFF+ Subsampling), (ii) adaptation
methods (Concatenation vs. SimTer), (iii) different percentages included user-edits:
50%, 61%(TH0), 100%. Significances computed way described
Section 4.3.3. performed subsampling computing perplexity
existing models UE part, wanted select best user edits.
analyzing results, worth noting already strong baseline,
tuned in-domain using domain specific monolingual data. Several observations
drawn. first block (SimTer 0.6 & Subsamp.) shows adding new material
subsampling filtering provides none little improvement baseline depending
evaluation metric analysis. precisely n-gram based metrics BLEU NIST
178

fiLeveraging Online User Feedback Improve SMT

8.3

8.25
8.22

8.24

51.5

51.25

51.1

51.0
8.11

50.5

8.1

8.09

50.64

TER

NIST

8.2

8.1

50.02

50.0

49.9

49.67

49.5
RAW

8
0

25

50

75

RAW
100

0

25

50
75
% best PE selected

100

Figure 4: NIST TER scores function percentage best ranked user-edits
used.

Table 11: Results obtained considering feedback instances depending
amount user-edits used different filtering adaptation methods.
indicate significant differences Baseline system 0.99
0.95 confidence levels. Best absolute results highlighted.
Translation system
Adaptation
Filtering
Baseline

Subsamp.
SimTer 0.6
FFF+

Concatenation

FFF+

% edits

BLEU

NIST

TER

METEOR

ULC

0%
+25%
+50%
+75%
+50%
+TH0-61%
100%
+50%
+TH0-61%

33.34
33.26
33.41
32.95
34.01
33.68
33.18
32.89
33.13

8.11
8.11
8.16
8.13
8.25
8.22
8.10
8.15
8.08

51.25
51.27
50.48
50.58
49.67
50.02
50.64
50.29
50.63

55.10
55.06
55.55
55.32
56.06
56.14
55.54
55.79
55.46

71.05
70.96
71.85
71.39
73.23
72.78
71.68
71.83
71.44

capture improvement TER METEOR slightly do. important
evidence provided second block (SimTer 0.6 & FFF+), strategy
propose paper. results evince appropriateness FFF+ filtering
yields significantly better results metrics compared Subsampling. However,
remarkable FFF+ Subsampling learning curves obtain best results
50% total user-edits considered. filtering strategy results crucial
obtaining final improvement. regarding method, also using
edits filtering (+100%) impact n-gram-based metrics (BLEU
NIST), marginally improves metrics (TER METEOR).
last block (Concatenation & FFF+) confirms important contribution SimTer 0.6
adaptation strategy compared straightforward approach adding new material
concatenation. case, using exactly filtered material, SimTer 0.6 yields
better results compared concatenation strategy.
179

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Alignment / Interpolation Method

Alignment / Interpolation Method
51.5

8.30

51.0

8.20

50.5

NIST

TER

8.25

8.15

50.0

8.10
49.5
8.05
SimTer 0.6
RAW

8.25

mGIZA++ 0.6

8.24

SimTer PPL

8.13

mGIZA++ PPL

8.12

SimTer 0.6
RAW

49.67

mGIZA++ 0.6

49.94

SimTer PPL

51.09

mGIZA++ PPL

51.17

Figure 5: NIST TER translation performance FAUST Raw Clean tests
achieved different alignment/phrase-table interpolation methods.
short, UE-enriched translation models following SimTer 0.6 & FFF+ yield
significant final improvement (+0.67) BLEU points (-1.57) TER points test
set. Introducing user edits without pre-selection boosting scheme allow
MT system achieve consistent improvements.
Additionally, compared alignment/phrase-table interpolation approach
competitive variants present literature. Concerning alignment, considered
forced alignment capability mGIZA++ defined Gao Vogel (2008): malignments obtained re-trained IBM 4 model iterated data
original training edited material. Regarding weight-interpolation strategy,
considered perplexity minimization method (PPL) Sennrich (2012), especially
suited domain adaptation. approach, development translation model (TM)
(i.e., phrase-table) built small development set. development TM used
minimize perplexity combining different TMs case base UE-based
models. strength approach granularity weights: instead giving
different weight phrase-table, assigns different weights feature function
within phrase-table, trying lower perplexity much possible. used
optimization method L-BFGS numerically approximated gradients (Byrd et al., 1995).
Figure 5 presents results obtained four alignment/interpolation combined
scenarios. Concerning comparison alignment methods, NIST shows significant
differences SimTer pivot-based mGIZA++ approaches. However, TER
reflects bigger difference favor SimTer. behavior coherent alignment
strategy, focused finding particular edits given user. terms computational
time, big differences: alignments computed less minute
6,610 sentences. One advantages using SimTer require
depend previous alignment models (mGIZA++ alike). 11 Comparing terms
alignment error rate beyond scope paper. SimTer specifically tailored
find differences original edited translations order extract
11. claiming SimTer work well general purpose alignment algorithm, competitive mGIZA++ state-of-the-art aligners.

180

fiLeveraging Online User Feedback Improve SMT

Table 12: Real examples translations changed (but necessarily improved)
SimTer 0.6 & FFF+ (50%) system categorization according
linguistic phenomena studied

Function
Words
Additions/
Omissions
(worse)
Lexical

Reorder.

Bad Feedback
Morphol.
Combined

Source

Baseline

Applicants
authorized
representative
information:
Tell Im supposed
breathe air.

El representante autorizado solicitante informacion:
DIME como se supone que
tengo para respirar sin aire.

SimTer 0.6
& FFF+ (50%)
Representante autorizado
del solicitante de la informacion:
DIME como supone
para respirar sin aire.

use letters
English alphabet
write Persian?
measures updated developed
2005 strategy review.

utilizar todas las letras del
alfabeto persa para escribir
en espanol?
Estas medidas fueron actualizados la revision de la
estrategia desarrollada en
el 2005.
verdad?
Si un extranjero llega (. . . ),
dejan por el?
Informamos que vamos
darle garanta de 5 anos
sobre los materiales la
mano de obra.

utilizar todas las letras
del alfabeto ingles para escribir en persa?
Estas medidas se han actualizado desarrollado
en la revision de la estrategia de 2005.
Hiza intentoHazlo?
Si un extranjero llega (. . . ),
vas dejarme por el?
Informamos que vamos
proporcionarle 5 anos de
garanta sobre materiales
mano de obra.

it?
alien comes (. . . ),
leave him?
Please informed
provide 5 years
warranty material
workmanship.

new phrase pairs, useful improve translation models. Thus, two tools serve
different purpose.12 Finally, regarding interpolation strategy, setting interpolation
weights perplexity minimization method Sennrich (2012) provide
enough boosting UE-based models. issue particularly observed looking
weights set L-BFGS algorithm: order 0.99 baseline
model 1 103 UE-based model. contrary, optimization based
quality provided translation addresses point directly.
5.1.2 Qualitative Output Analysis
results presented far based automatic evaluation metrics. complement study set human assessments order verify improvement
also perceived humans identify characteristics make new translations
better. Five expert annotators analyzed 414 instances FAUST test Clean corpus.
annotators observed triplets composed source sentence, translation produced
Baseline, translation better performing SimTer 0.6 & FFF+ (50%)
system. determine two translations better or, instead,
quality. additional option cannot tell possible well. Annotators
12. sake completeness, mention experiments conducted evaluate performance SimTer general aligner showed AER results significantly lower mGIZA++
(Henriquez, 2014)

181

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Table 13: Results comparative analysis carried five human annotators
translation 414 sentences FAUST test Clean corpus Baseline
SimTer 0.6 & FFF+ (50%) systems. criterion (row), Better indicates user-adapted models provide better translation compared
non-adapted system, Worse indicates conversely Changed indicates
translations different.
Adequacy
Fluency
Function Words
Additions / Omissions
Lexical
Reordering
Bad Feedback
Morphology

Better
34.54%
32.61%
Better
50.00%
31.54%
47.59%
57.50%

35.77%


40.58%
49.76%

18.56%
20.53%
31.99%
18.37%

40.73%

Worse
15.70%
17.63%
Worse
31.52%
47.93%
20.40%
24.13%
100.00%
23.45%

Cannot Tell
9.18%

Changed
13.04%
17.63%
60.39%
21.01%
5.31%
19.57%

Table 14: Progression OOV ratio BLEU FAUST test Clean corpus along
different acceptance levels user-edited instances.
Selection
Ratio
0% (no-feedback)
25%
50%
61%
75%
100% (all-feedback)

OOV
Words
563
559
557
554
550
550

Total
Words

22,898

OOV
Ratio
2.46%
2.44%
2.43%
2.42%
2.40%
2.40%

BLEU
37.85
37.86
38.65
38.32
38.47
37.79

know system produced translation, order presentation
two options randomized. overall quality assessment based translation adequacy fluency, annotators also asked provide detailed information
linguistic aspects made one translation better one; e.g., changes function
words, addition omission spurious words, lexical coverage, reordering, morphology,
presence harmful elements (bad feedback, i.e., mistranslations clearly introduced
erroneous user edits). Table 12 includes real samples studied phenomena. Cohens
kappa agreement annotators selection 10 common phrases = 0.57.
Table 13 presents overall results. percentage sentences translation changes significantly terms adequacy fluency around 50%-60%. number
translations adequacy fluency improved SimTer 0.6 & FFF+ (50%)
system doubled number cases lowered quality. fact confirms
results obtained automatic evaluation measures. Table 13 shows that,
60% sentences underwent lexical modification. aspects received less bus
significant impact: reordering (21.01% sentences affected), morphology (19.57%), addi182

fiLeveraging Online User Feedback Improve SMT

Table 15: Statistics EnglishSpanish parallel CommonCrawl corpus.
Corpus
CommonCrawl

Sent.
Eng
Spa

1.84

Words
46.54
50.33

Vocab.
750.01 K
775.75 K

avg. length
25.22
27.30

tions/omissions (17.63%), function words (13.04%) and, least frequent, bad feedback
(5.31%). aspects improved, except additions/omissions and, bad feedback.
worth mentioning frequent changes (lexical reordering) also
changes whose benefit doubles cases quality decrease. Contrary could
thought, lexical correction addresses mistranslations greater deal Out-ofVocabulary words (OOV), OOVs reduced 0.03% best performance
(cf. Table 14 detailed analysis).
5.2 Using Web-Crawled Parallel Corpus
application scenario, use CommonCrawl corpus, collection parallel texts
automatically mined Web (Smith et al., 2013). corpus offers two interesting
characteristics experiments: (i) vocabulary expressions go far beyond
controlled scenario EPPS acts formality News UN corpora (ii)
noisy corpus. large vocabulary size Table 15 gives intuition nature
content, high presence noise spurious words. addition, size allows set
trade-off quantity (amount new material selected) quality (the threshold
selection algorithm). Moreover, selection method FAUST corpus also
applicable due analogy scenarios: CommonCrawl, compare
automatic translation source sentences references automatically obtained
crawler select cases latter better. perform selection
use classifiers FAUST scenario without retraining adaptation
want study generalization ability specific training material
CommonCrawl corpus available selection task. experiment represents
double challenge: (i) determining presented proposal also suitable crawled
parallel corpora, (ii) studying whether trained selection models generalize well
across corpora domain. CommonCrawl experimental setting seen
artificial post-editing scenario: references represent edits, ideally
provide adequate translations.
experiment, consider baseline best obtained system far translation
news texts (cf. Section 4.3.3). call baseline Base News SimTer 0.6. baseline
system might considered already strong, since 0.25 BLEU points better pure
baseline system (trained data). order assess trade-off
quantity quality using parallel text, enrich baseline adapting
original models different portions CommonCrawl corpus filtered either
subsampling FFF+ strategies. FAUST scenario, analyzed translation
performance depending several factors: (i) ratio CommonCrawl data selected,
(ii) data selection strategy (FFF+ vs Subsampling), (iii) adaptation strategy.
Table 16 Figure 6 show evaluation results test12 test13 datasets
Section 4.3.1. curves Figure 6 show consistent pattern observed
183

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Figure 4 FAUST scenario, reinforcing evidences: FFF+ selection
SimTer 0.6 adaptation important order obtain final gain. Using CommonCrawl
without selection result performance gain, worsens results slightly.
Subsampling improves metrics 75% point cost worsening others.
Moreover, concatening 25% best ranked CommonCrawl training data Concat.
FFF+(25%) provides slight improvement. However, improvements obtained
Concat. FFF+(25%) SimTer 0.6 Subsampling(75%) significant.
Table 16: Results obtained base CommonCrawl-enriched SMT systems depending different filtering adaptation methods. BLEU NIST, ,
indicate significant differences News SimTer 0.6 system (experiment baseline) 0.99, 0.95 0.90 confidence levels, respectively.
best results corpus boldfaced.
Translation system
Adaptation
Filtering
test12
News SimTer 0.6
SubSamp.
SimTer 0.6
FFF+

Concat.

FFF+

test13
News SimTer 0.6


SubSamp.

SimTer 0.6
FFF+

Concat.

FFF+

% edits

BLEU

NIST

TER

METEOR

ULC

0%
+25%
+50%
+75%
+25%
+TH0-60%
+100%
+25%
+TH0-60%

33.25
33.21
33.38
33.47
33.73
33.74
33.19
33.41
33.20

8.70
8.67
8.68
8.70
8.78
8.75
8.68
8.71
8.68

48.24
48.38
48.25
48.27
47.84
47.87
48.46
48.09
48.18

55.84
55.48
55.54
55.81
56.21
56.05
55.50
55.88
55.72

71.66
71.86
72.04
72.32
72.52
72.42
71.20
72.44
72.15

0%
+25%
+50%
+75%
+25%
+TH0-60%
+100%
+25%
+TH0-60%

29.15
29.17
29.22
29.29
29.61
29.57
29.32
29.27
29.00

8.07
8.05
8.03
8.05
8.13
8.10
8.07
8.07
8.04

51.28
51.49
51.43
51.36
50.99
51.12
51.36
51.11
51.14

53.15
52.87
52.71
52.88
53.39
53.13
52.87
53.14
53.01

71.73
71.71
71.63
71.85
72.41
72.07
71.53
72.26
71.94

Results also show selecting 25%-best CommonCrawl data produces
best improvement. approximately +0.50 BLEU 0.40 TER test sets.
important recall Base News SimTer 0.6 strong baseline. remarkable
selection models trained data FAUST scenario generalize well
CommonCrawl domain adaptation scenario. optimal 25% selection threshold
represents much stricter selection required FAUST scenario (50%). However, FAUST selecting around 3,000 sentences among 6,000, case
selecting around 460 thousand sentences 1.84 million. Hence, final selection
threshold compromise aggressiveness method minimum
amount new material necessary cause real impact. also analyzed effect
184

fiLeveraging Online User Feedback Improve SMT

8.9

NIST

8.74 8.75

8.74

8.7

48.46

TER

8.8

48.9

49.0

8.78

48.5

8.68

8.7

47.84

48.0
47.5

test_12

8.6
0

25
50
75
% best CommonCrawl corpus used

47.9 47.87

100

test_12
0

8.2

48.04

25
50
75
% best CommonCrawl corpus used

100

52.5
52.06

8.1

8.07

52.0
8.08

8.1

8.09

TER

NIST

8.13

51.5

8.07

50.99

51.19 51.12

51.29

51.36

51.0
8
0

25
50
75
% best CommonCrawl corpus used

test_13

50.5

test_13
100

0

25
50
75
% best CommonCrawl corpus used

100

Figure 6: NIST TER scores test12 test13 corpora function
percentage best ranked CommonCrawl segments used.
selecting 25% CommonCrawl data randomly (averaged 10 times) selecting 25% data selection classifiers considered worst. analysis
allows assess good selection algorithm ranking parallel examples
CommonCrawl corpus. Figure 7(a) shows results, indicate classifiers
able generate sensible rankings detect best worst examples. allows
properly enrich translation models, avoiding negative effect using really bad
instances. results obtained 25% examples selected random
results obtained selecting best 25% according classifier.
Lastly, repeated comparison alignment/adaptation strategy methods considered FAUST scenario experiment. Figure 7(b) shows obtained results.
Similar Figure 5, differences small favor SimTer. However, SimTer 60-40%
performs significantly better alignment/adaptation strategies test
sets (p < 0.01).

185

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

test_12
test_13

Selection Method
52.7
52.1
51.5
50.9
50.4
49.8
49.2
48.6
48
47.4

TER

NIST

Selection Method
8.8
8.6
8.5
8.4
8.3
8.2
8.1
8
7.9
7.8
25%_best

baseline



8.78*
8.13*

8.7
8.07

8.68
8.07

25%_rand 25%_worst

8.65
8.03

8.44
7.83

test_12
test_13

25%_best

baseline

50.99
47.84

51.28
48.24



51.36
48.46

25%_rand 25%_worst

51.57
48.55

52.81
49.72

(a) Achieved using different selection criteria: baseline (0%), (100%), best 25%, random 25%
worst 25%.
Alignment / Interpolation Method

TER

NIST

Alignment / Interpolation Method
8.8
8.7
8.6
8.5
8.4
8.3
8.2
8.1
8

51.3
50.9
50.5
50
49.6
49.1
48.7
48.2
47.8
47.4

SimTer 0.6 mGIZA++ PPL SimTer PPL mGIZA++ 0.6
test_12
test_13

8.78*
8.13*

8.72
8.08

8.72
8.07

8.74
8.08

SimTer 0.6 mGIZA++ PPL SimTer PPL mGIZA++ 0.6
test_12
test_13

50.99
47.84

51.18
48.07

51.19
48.08

51.26
48.08

(b) Achieved applying different alignment/adaptation methods.

Figure 7: NIST TER scores test12 test13 corpora. indicates significant
differences Baseline system 0.99 confidence level.

6. Conclusions
article proposed new automatic strategy incrementally train machine translation (MT) models edited translations coming casual unreliable users.
strategy builds upon three main blocks, namely: (i) automatic identification useful useredited instances (UE); (ii) alignment UEs source text, focusing
errors made original MT system; (iii) incorporation new parallel segments specific translation models trained UEs. proposal novel
application techniques information retrieval, quality estimation domain
adaptation problem MT system enrichment. datasets explored also
interesting challenging properties.
selection useful UEs important filter noisy feedback users.
accomplish training classifier supervised data using features derived
similarity metrics used information retrieval MT quality estimation. Although
186

fiLeveraging Online User Feedback Improve SMT

classification results achieved moderate, classification scores allow approximately
rank UEs quality tune different selection thresholds. Experiments show
useful strategy select examples which, combined two steps, yields
significant improvements original MT system.
Regarding sourcetranslationUE alignment, proposed SimTer, simple incremental approach based pivoting, uses TER alignment augmented similarity
features. approach two advantages: depend previous softwarespecific alignment models (e.g., GIZA++, mGIZA++ Berkeley Aligner) monolingual nature alignment implicitly allows algorithm focus correcting translation errors, rather achieving optimal alignment words. experimenting
two real datasets, showed positive contribution SimTer MT enrichment pipeline. particular application, using SimTer better using existing
general-purpose aligners, mGIZA++, especially CommonCrawl scenario.
experiments also confirmed validity proposal, terms computational efficiency.
third step deals building UE-specific translation models using standard phrase
extraction scoring tools. experimentally analyzing different ways combining
UE-based original translation models, concluded simple linear interpolation good efficient strategy. properly tuning parameters, combination
real impact final translation models, something that, instance, perplexity
minimization able achieve.
complete architecture thoroughly tested real UEs collected nonprofessional users commercial on-line translation portal (the called FAUST
scenario). experimented different thresholds select examples alternative
ways perform alignment integration new aligned sentences. Results
showed approach significantly improves translation quality basic, general
purpose SMT system, generally superior alternative methods. Apart evaluating several automatic quality measures, also conducted manual analysis order
verify quality improvements gain insight cases enriched MT system performs better worse. improvements come mainly
reduction out-of-vocabulary words, actually reduced marginally.
major improvements translation quality came much better lexical selection,
reordering, morphology. side, enriched system introduces time
time incorrect words expressions learned wrongly selected aligned examples.
also performed poorly terms adding omitting spurious words, slightly worsening
quality baseline system.
approach general enough applied different scenarios. finally used
CommonCrawl, collection parallel texts automatically extracted Web,
enrich general purpose baseline SMT system. three steps applied;
selection also necessary case corpus noisy, due automatic
extraction. Exactly classifiers trained FAUST corpus used identify
examples automatically extracted target sentence better automatic
translation source provided baseline translation system. classifiers showed
robustness even noisy references used instead UEs evincing capacity
deal human automatically-generated noise.The conclusions drawn
187

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

adaptation experiment, shows methodology works well across different
corpora sources types noise.

Acknowledgments
major part work carried authors worked TALP Research
Center - Universitat Politecnica de Catalunya. would like thank Nadir Durrani
proofreading paper also anonymous reviewers valuable feedback.
work partially funded Spanish Ministerio de Economa Competitividad,
contracts TEC2012-38939-C03-02 TIN2012-38523-C02-02, well
European Regional Development Fund (ERDF/FEDER) European Communitys
FP7 (2007-2013) program following grants: 247762 (FAUST, FP7-ICT-2009-4247762) 246016 (ERCIM Alain Bensoussan Fellowship).

References
Ambati, V., Vogel, S., & Carbonell, J. (2010). Active Learning Crowd-Sourcing
Machine Translation. Proceedings LREC, pp. 21692174.
Axelrod, A., He, X., & Gao, J. (2011). Domain adaptation via pseudo in-domain data
selection. Proceedings 2011 Conference Empirical Methods Natural
Language Processing, pp. 355362, Edinburgh, Scotland, UK. Association Computational Linguistics.
Barron-Cedeno, A., Marquez, L., Henrquez, Q. C. A., Formiga, L., Romero, E., & May,
J. (2013). Identifying useful human correction feedback on-line machine
translation service. Proceedings Twenty-Third International Joint Conference
Artificial Intelligence, IJCAI 13, pp. 20572063. AAAI Press.
Bertoldi, N., Cettolo, M., & Federico, M. (2013). Cache-based online adaptation machine
translation enhanced computer assisted translation. Proc. MT Summit, pp. 35
42.
Bisazza, A., Ruiz, N., & Federico, M. (2011). Fill-up versus Interpolation Methods
Phrase-based SMT Adaptation. Proceedings IWSLT, pp. 136143.
Blain, F., Schwenk, H., Senellart, J., & Systran, S. (2012). Incremental adaptation using
translation information post-editing analysis. Proceedings IWSLT 2012, pp.
229236.
Byrd, R. H., Lu, P., Nocedal, J., & Zhu, C. (1995). Limited Memory Algorithm Bound
Constrained Optimization. SIAM Journal Scientific Computing, 16 (5), 11901208.
Callison-Burch, C., Bannard, C., & Schroeder, J. (2005). Scaling phrase-based statistical
machine translation larger corpora longer phrases. Proceedings 43rd
Annual Meeting Association Computational Linguistics, ACL 05, pp. 255262,
Stroudsburg, PA, USA. Association Computational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., & Specia, L. (2012). Findings 2012 Workshop Statistical Machine Translation. Proceedings
188

fiLeveraging Online User Feedback Improve SMT

Seventh Workshop Statistical Machine Translation, pp. 1051, Montreal, Canada.
Association Computational Linguistics.
Cappe, O., & Moulines, E. (2009). On-line expectation-maximization algorithm latent
data models. Journal Royal Statistical Society: Series B (Statistical Methodology), 71 (3), 593613.
Cettolo, M., Federico, M., Servan, C., & Bertoldi, N. (2013). Issues Incremental Adaptation Statistical MT Human Post-edits. Proceedings MT Summit XIV
Workshop Post-editing Technology Practice, pp. 111118.
Denkowski, M., & Lavie, A. (2011). Meteor 1.3: Automatic Metric Reliable Optimization
Evaluation Machine Translation Systems. Proceedings 6th Workshop
Statistical Machine Translation, pp. 8591, Edinburgh, Scotland.
European Comission - 7th Framework Program (2010). Matecat, FAUST Casmacat Projects. http://www.matecat.com http://www.faust-fp7.eu http://www.
casmacat.eu. Accessed: 2015-02-01.
FAUST (2013). Final report methods evaluation translation requests, system outputs, modelling user feedback.
Tech. rep. D4.6,
FAUST Feedback Analysis User-Adaptive Statistical Translation. ftp://svrftp.eng.cam.ac.uk/pub/pub/faust-pub/Deliverables/FAUSTD4.6.pdf.
Formiga, L., Henrquez Q., C., Hernandez, A., Marino, J., Monte, E., & Fonollosa, J. (2012).
talp-upc phrase-based translation systems wmt12: Morphology simplification
domain adaptation. Proceedings Seventh Workshop Statistical Machine Translation, pp. 275282, Montreal, Canada. Association Computational
Linguistics.
Foster, G., Goutte, C., & Kuhn, R. (2010). Discriminative instance weighting domain
adaptation statistical machine translation. Proceedings 2010 Conference
Empirical Methods Natural Language Processing, pp. 451459, Cambridge, MA.
Association Computational Linguistics.
Foster, G., & Kuhn, R. (2007). Mixture-Model Adaptation SMT. Proceedings
Second Workshop Statistical Machine Translation, pp. 128135.
Gale, W., & Church, K. (1993). Program Aligning Sentences Bilingual Corpora.
Computational Linguistics, 19, 75102.
Gao, Q., & Vogel, S. (2008). Parallel implementations word alignment tool. Software
Engineering, Testing, Quality Assurance Natural Language Processing, pp.
4957, Columbus, Ohio. Association Computational Linguistics.
Gimenez, J., & Marquez, L. (2010). Asiya: Open Toolkit Automatic Machine Translation (Meta-)Evaluation. Prague Bulletin Mathematical Linguistics, pp. 7786.
Google Inc. (2015). Google Translate. http://translate.google.com. Accessed: 201502-01.
Haddow, B., & Germann, U. (2011).
Moses Incremental Training.
http://www.statmt.org/moses/?n=Advanced.Incremental. Accessed: 2015-08-11.
189

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Hardt, D., & Elming, J. (2010). Incremental re-training post-editing smt. proc.
AMTA 2010: Ninth conference Association Machine Translation
Americas, Denver, CO. USA.
Henriquez, C. (2014). Improving statistical machine translation adaptation
learning. Ph.D. thesis, Universitat Politecnica de Catalunya.
Henrquez, C., Marino, J., & Banchs, R. (2011). Deriving translation units using small
additional corpora. Proceedings 15th Conference European Association
Machine Translation, pp. 121128.
Hsu, C.-W., Chang, C.-C., & Lin, C.-J. (2003). practical guide support vector classification. Tech. rep., Department Computer Science, National Taiwan University.
http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.
Joachims, T. (1999). Making large-scale support vector machine learning practical.
Scholkopf, B., Burges, C. J. C., & Smola, A. J. (Eds.), Advances Kernel Methods,
pp. 169184. MIT Press, Cambridge, MA, USA.
Koehn, P. (2005). Europarl: Parallel Corpus Statistical Machine Translation.
Machine Translation Summit X, pp. 7986, Phuket, Thailand.
Koehn, P., & Hoang, H. (2007). Factored Translation Models. Proceedings 2007
Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 868876, Prague, Czech
Republic.
Lambert, P., de Gispert, A., Banchs, R., & Marino, J. B. (2005). Guidelines word
alignment evaluation manual alignment. Language Resources Evaluation,
39 (4), 267285.
Levenberg, A., Callison-Burch, C., & Osborne, M. (2010). Stream-based translation models
statistical machine translation. Human Language Technologies: 2010 Annual Conference North American Chapter Association Computational
Linguistics, pp. 394402, Los Angeles, California. Association Computational Linguistics.
Lopez, A. (2008). Tera-scale translation models via pattern matching. Proceedings
22Nd International Conference Computational Linguistics - Volume 1, COLING
08, pp. 505512, Stroudsburg, PA, USA. Association Computational Linguistics.
Martnez-Gomez, P., Sanchis-Trilles, G., & Casacuberta, F. (2012). Online adaptation
strategies statistical machine translation post-editing scenarios. Pattern Recognition, 45 (9), 3193 3203. Best Papers Iberian Conference Pattern Recognition
Image Analysis (IbPRIA2011).
Matecat (2015). Matecat official repository. https://github.com/matecat/MateCat. Accessed: 2015-07-24.
Mathur, P., Mauro, C., & Federico, M. (2013). Online learning approaches computer
assisted translation. Proceedings Eighth Workshop Statistical Machine
Translation, pp. 301308, Sofia, Bulgaria. Association Computational Linguistics.
190

fiLeveraging Online User Feedback Improve SMT

Mcnamee, P., & Mayfield, J. (2004). Character N-Gram Tokenization European Language Text Retrieval. Information Retrieval, 7 (1-2), 7397.
Microsoft Inc. (2015). Bing Translator. http://www.bing.com/translator. Accessed:
2015-02-01.
Neal, R. M., & Hinton, G. E. (1998). view em algorithm justifies incremental,
sparse, variants. Learning graphical models, pp. 355368. Springer.
Nelder, J. A., & Mead, R. (1965). Simplex Method Function Minimization.
Computer Journal, 7, 308313.
NIST (2002). Automatic Evaluation Machine Translation Quality Using N-gram CoOccurrence Statistics. Tech. rep., National Institute Standards Technology.
http://www.itl.nist.gov/iad/mig/tests/mt/doc/ngram-study.pdf.
Och, F. J., & Ney, H. (2003). Systematic Comparison Various Statistical Alignment
Models. Computational Linguistics, 29, 1951.
Ortiz-Martnez, D., Garca-Varea, I., & Casacuberta, F. (2010). Online learning interactive statistical machine translation. Human Language Technologies: 2010 Annual Conference North American Chapter Association Computational
Linguistics, pp. 546554, Los Angeles, California. Association Computational Linguistics. http://www.aclweb.org/anthology/N10-1079.
Ortiz-Martnez, D., Sanchis-Trilles, G., Gonzalez-Rubio, J., & Casacuberta, F. (2013).
Progress report adaptive translation models. Tech. rep. D4.2, Casmacat: Cognitive Analysis Statistical Methods Advanced Computer Aided Translation.
Padro, L., Collado, M., Reese, S., Lloberes, M., & Castellon, I. (2010). FreeLing 2.1: Five
Years Open-Source Language Processing Tools. Proceedings 7th Language
Resources Evaluation Conference (LREC 2010), La Valletta, MALTA.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: Method Automatic
Evaluation Machine Translation. Proceedings Annual Meeting
Association Computational Linguistics (ACL).
Pighin, D., Marquez, L., & May, J. (2012). Analysis (and Annotated Corpus) User
Responses Machine Translation Output. Proceedings Eight International
Conference Language Resources Evaluation (LREC12), Istanbul, Turkey.
Potet, M., Esperanca-Rodier, E., Blanchon, H., & Besacier, L. (2011). Preliminary experiments using users post-editions enhance smt system. Proceedings
15th Conference European Association Machine Translation, pp. 161168.
Pouliquen, B., Steinberger, R., & Ignat, C. (2003). Automatic Identification Document
Translations Large Multilingual Document Collections. Proceedings International Conference Recent Advances Natural Language Processing (RANLP2003), pp. 401408, Borovets, Bulgaria.
Reverso-Softissimo (2015). Reverso Free online translator dictionary based SDL
technology. http://www.reverso.net. Accessed: 2015-02-01.
Riezler, S., & Maxwell, J. (2005). pitfalls automatic evaluation significance testing MT. ACL-05 Workshop Intrinsic Extrinsic Evaluation
191

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Measures Machine Tranlsation and/or Summarization (MTSE05) 43rd Annual Meeting Association Computational Linguistics, Ann Arbor, Michigan,
USA.
Schwenk, H., & Koehn, P. (2008). Large diverse language models statistical machine
translation.. Proceedings IJCNLP, pp. 661666, Hyderabad, India.
Sennrich, R. (2012). Mixture-Modeling Unsupervised Clusters Domain Adaptation
Statistical Machine Translation. Proceedings 16th EAMT Conference.
Simard, M., Foster, G. F., & Isabelle, P. (1992). Using cognates align sentences bilingual corpora. Proceedings Fourth International Conference Theoretical
Methodological Issues Machine Translation, pp. 6781.
Simard, M., Goutte, C., & Isabelle, P. (2007). Statistical phrase-based post-editing.
Human Language Technologies 2007: Conference North American Chapter
Association Computational Linguistics; Proceedings Main Conference,
pp. 508515, Rochester, New York. Association Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., & Lopez, A.
(2013). Dirt cheap web-scale parallel text Common Crawl. Proceedings
2013 Conference Association Computational Linguistics (ACL 2013),
Sofia, Bulgaria. Association Computational Linguistics.
Snover, M., Madnani, N., Dorr, B., & Schwartz, R. (2009). TER-Plus: Paraphrase, Semantic,
Alignment Enhancements Translation Edit Rate. Machine Translation, 23 (2),
117127.
Witten, I., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools Techniques (2 edition). Morgan Kaufmann, San Francisco, CA.

192

fiJournal Artificial Intelligence Research 54 (2015) 437-469

Submitted 11/14; published 11/15

Constraining Information Sharing Improve
Cooperative Information Gathering
Igor Rochlin

IGOR . ROCHLIN @ GMAIL . COM

School Computer Science,
College Management, Rishon LeZion, Israel.

David Sarne

DAVID . SARNE @ GMAIL . COM

Department Computer Science,
Bar-Ilan University, Ramat-Gan, Israel.

Abstract
paper considers problem cooperation self-interested agents acquiring
better information regarding nature different options opportunities available them.
sharing individual findings others, agents potentially achieve substantial improvement overall individual expected benefits. Unfortunately, well known
self-interested agents equilibrium considerations often dictate solutions far fully
cooperative ones, hence agents manage fully exploit potential benefits encapsulated cooperation. paper introduce, analyze demonstrate benefit five
methods aiming improve cooperative information gathering. Common five constrain limit information sharing process. Nevertheless, decrease benefit due
limited sharing outweighed resulting substantial improvement equilibrium individual information gathering strategies. equilibrium analysis given paper, which,
important contribution study cooperation self-interested agents, enables
demonstrating wide range settings improved individual expected benefit achieved
agents applying five methods.

1. Introduction
many settings agents benefit cooperating information gathering (Rochlin, Aumann,
Sarne, & Golosman, 2014; Kephart & Greenwald, 2002; Rochlin, Sarne, & Mash, 2014; Hazon,
Aumann, Kraus, & Sarne, 2013). example, consider two travel agents, city,
plan participate international tourism conference, taking place highly traveled
destination. many airlines offering flights nearby destinations, setting price
according various external factors seat availability agreements airlines
partners. Similarly, depending airport arrival, one get conference train, bus,
ferry, taxi combination different segments trip. means
transportation may characterized different availability fare, depending, example,
time day required. Checking feasibility cost different alternatives
traveling conference, thus, potentially involves several time consuming activities,
checking locations map checking companies web-sites routes, timetables,
fares availability, thus incurs opportunity cost. Since agents benefit
information gathers regarding different options getting conference,
strong incentive share findings, i.e., execute information gathering process
(hereafter denoted IGP) cooperatively.
c
2015
AI Access Foundation. rights reserved.

fiROCHLIN & ARNE

Cooperative information gathering used many real-life applications different domains.
example, consider two friends, interested buying big TV screen. friends visit
shopping mall, together, checks offers different stores, eventually
meet share findings. Alternatively, consider oil drilling company sending multiple
agents explore possible drilling sites, order develop best site discovered. Similarly
position needs filled, HR personnel interview candidates parallel recruit
best candidate found. Students jointly look references assignment receive
eventually use best source found them.
benefits multi-agent cooperative information gathering twofold. First, since
alternative (hereafter termed opportunity) reviewed benefit many agents, relative cost
information gathering reduced, overall welfare increases. Secondly, task
potentially divided according expertise different agents, expertise exists.1
Cooperative information gathering seen type public goods game,
agents contribute individual IGP collective result influences welfare
them. Like public goods games, costs IGP cooperative information gathering,
basically born individual agents although benefits (better information case) societal.
public goods games, general, inefficiencies private giving commonly occur whenever
agents self-interested (de Jong, Tuyls, & Verbeeck, 2008; de Jong & Tuyls, 2011). Similarly,
shown prior work cooperative information gathering, carried self-interested
agents, result amount cooperation optimal fully cooperative case (Rochlin
et al., 2014). particular demonstrated methods instruments (termed enhancers)
easily proved beneficial fully cooperative case, actually negative
impact, individual overall performance, self-interested case. enhancers
included increase size group agents gather information jointly, increase
number opportunities agent time potentially gather information on, improvement agents information gathering competence, increase level
heterogeneity individual information gathering competence group members
ability communicate throughout process. Alas, prior work mostly descriptive
sense outlined potential problems may arise cooperative information gathering
self-interested agent. research reported current paper aims provide solutions
problems, form five somehow non-intuitive methods essentially constrain limit
ISP individual benefit participating agents substantially increase.
five cooperative information gathering methods reported paper differ constraints put information sharing process (henceforth denoted ISP). first, denoted
Enforced probabilistic information sharing prevents individual agents taking part ISP
according probabilistic function. second, denoted Threshold restricted information
sharing, prevents agents found highly favorable values along individual IGP
taking part ISP. third, denoted Cost filtered information sharing, introduces
cost taking part ISP (where proceeds wasted returned agents)
allows agents choose whether take part ISP not. fourth, denoted Random finding
sharing, allows agents take part ISP, however restricts disclosing
one values set known each, random. Finally, fifth, denoted Subgroup restricted information sharing, initially divides agents subgroups allows local ISPs,
1. buyers cooperation, agents also benefit volume discount cooperation; however
property holds specific domain.

438

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

i.e., information sharing subgroup level. methods may seem counter intuitive,
absence agents ISP restriction amount information
shared could harmful agents. Yet, many settings, use methods
highly beneficial. paradox embedded ISP option - sharing
information benefits agents, fact information gathered going shared,
discourages agents investing much resources individual IGP (Rochlin et al., 2014).
Therefore, use methods individual benefit agent taking part
ISP decreases, however IGP carried agent individually becomes efficient.
Therefore, intelligently managing tradeoff two, beneficial equilibrium
achieved, improves overall individual benefits.
paper provides comprehensive analysis individual information gathering strategies
used agents, given strategy others, different methods. Enforced probabilistic information sharing, Random finding sharing Subgroup information sharing methods
agents individual strategy proven similar structure one used standard
cooperative information gathering method agent resume information gathering long
best value obtained far lower reservation value (a threshold), regardless
much information potentially gathered. Threshold restricted information
sharing method agents individual strategy proven threshold-based, however threshold changes function amount information potentially still gathered.
Cost filtered information sharing method, individual strategies proven based
single reservation value determining benefit additional information gathering set
intervals deciding whether take part ISP. allow characterization resulting equilibria. Using synthetic environments, numerically demonstrate five methods
result substantial improvement agents individual expected benefit wide range
settings.
results contribute advancement theories cooperation MAS. discussed later
paper, methods easily applied use benefit individuals planning
engage cooperative information gathering designers multi-agent systems (MAS)
cooperative information gathering likely take place.
following section formally introduce cooperative information gathering model.
Section 3 detail model analysis, equilibrium strategies different model variants
considered supply numerical examples benefit achieved using them.
Related work reviewed Section 4, emphasizing uniqueness analysis provided
paper. Finally, conclude discuss directions future research Section 5.

2. Model
model considers set K = {A1 , ..., Ak } fully-rational self-interested agents.
agents needs gather information pertaining value (e.g., benefit) different opportunities
access eventually choose one. values different opportunities priori
unknown information gathered one opportunity time. individual information
gathering problem, defined above, standard follows assumptions commonly used
literature (Chhabra & Das, 2011; Kephart & Greenwald, 2002; Hazon et al., 2013; Rothschild, 1974;
McMillan & Rothschild, 1994; Morgan & Manning, 1985). uncertainty associated
value opportunities available agent Ai modeled, costly information gathering
439

fiROCHLIN & ARNE

literature (McMillan & Rothschild, 1994; Burdett & Malueg, 1981; Carlson & McAfee, 1984;
Lippman & McCall, 1976; Morgan, 1983), probability distribution function (p.d.f.) fi (x)
(i.e., value opportunity individual IGP Ai drawn fi (x)),
agents familiar (Tang, Smith, & Montgomery, 2010; Waldeck, 2008; Janssen, Moraga-Gonzalez,
& Wildenbeest, 2005). Due resource consuming nature process considered costly
sense revealing value opportunity incurs fixed cost, denoted ci . model
assumes agent Ai constrained number opportunities accessible agent,
denoted ni . cost ci , distribution fi (x) number opportunities ni , defined
agents level support settings different agents different skills capabilities.
agent thus needs gather information, i.e., explore value opportunities
eventually pick one values revealed (i.e., recall permitted) (Carlson & McAfee, 1984;
McMillan & Rothschild, 1994).
settings opportunities applicable agents agents incentive
cooperate information gathering sense individual findings eventually shared
others. many ways share information, focus paper setups
ISP takes place pre-specified time, agents completed individual
IGPs needs decide opportunity choose. choice sharing findings
end individual IGPs mostly natural customary real life. importantly,
alternative sharing information throughout process major setback sense
individual agent finds information sharing beneficial receiving end,
i.e., one informed favorable opportunity found;
reporting end, agent loses communication since report potentially encourage
agents terminate individual information gathering. hand sharing
information concluding individual IGPs always beneficial agent gains
information, time information discloses affect behavior others
thereafter since also already concluded IGPs.
prior models cooperative information gathering, also assume that: (a) agents
truthful sense always report true values obtain;2 (b) agent Ai
fall-back value v0i , i.e., even becoming acquainted opportunity values (in case
gathering information individually receiving others findings) agent
presumably benefit v0i ;3 (c) either opportunities agent check unique
agents priori divide opportunities among assigned different
set. assumed information gathering costs opportunity values additive agent
Ai interested maximizing expected benefit, denoted EBi . benefit agent therefore
best value obtained group minus costs accumulated individually along agents
individual IGP. Finally, assumed engaging cooperative information gathering
process agents priori acquainted probability distribution functions information
gathering costs agents, i.e., possible difference information available
different agents throughout cooperative IGP findings findings others.
cooperative information gathering model detailed found full
variations prior literature (Hazon et al., 2013; Rochlin et al., 2014; Gatti, 1999; Carlson &
2. truthfulness assumption commonly justified substantial potential reputation loss, easily enforceable
using fines.
3. similarly, taking part ISP necessarily disclosing value absence better one.

440

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Application

Individual goal

Opportunity

Value

Product acquisition

Minimize individual expense

Complex service/product

Cost purchase

Choosing

oildrilling
site

Maximize
oil revenues
minus cost
exploratory
drills
Minimize
cost production R&D
expenses
Maximize individual utility
(grade minus
individual
effort)

Potential
drilling sites

Amount
oil found

Production
technology

Cost production

specific
technology
Expected
grade
source

used

R&D

Information
Search
(students
assignment)

Information
source (e.g.,
online, textbook, library
resource)

Information gathering cost
Time spent finding
options evaluating
Time resources
spent exploratory drills

Source uncertainty

R&D cost specific technology

Uncertainty concerning implementation aspects
desired technology

Time spent evaluating
different
sources

Differences coverage
topic, relevance, accuracy, level details

Sellers competition, seasonal effects, service constraints
Uncertainty regarding
amount oil
drilling site

Table 1: mapping different applications cooperative information gathering problem.
McAfee, 1984).4 Taking travel agents example, given previous section, opportunities
represent different alternatives reaching conference location value total cost.
information gathering cost agents cost time needed explore alternatives.
goal agent minimize expected expense, defined cost best alternative
found two plus cost time spent individually review different alternatives.
Similarly, model mapped applications mentioned introduction (e.g.,
see Table 1).

3. Analysis
divide analysis according five cooperative information gathering enhancing methods.
method, first determine individual optimal information gathering strategy
agent taking part process, best response agents strategies. Then, show
collective behavior derived extract equilibrium set strategies.
appropriate equilibrium concept depends define type space.
define agents type specific vector values would encounter fully exhausting
IGP, appropriate concept would ex-ante Bayesian Nash equilibrium, since agents
priori unaware types information revealed (individually) along
IGP. However, prove paper definition, agents type would affect
strategy (strategies turn based thresholds set prior conducting IGP).
Therefore, outcomes stochastic, one could theory build direct stochastic mapping
individual strategies global outcome therefore need go
type so. Therefore added value Bayesian Nash equilibrium
characterization. equilibrium concept would subgame-perfection Stackelberg (where
system designer sets rules participation ISP, agents respond choosing
optimal strategies). result similar equilibrium characterization case.
4. model variants consider task executed representative agent, acting behalf group,
essence gathering costly information trading-off costs benefit same.

441

fiROCHLIN & ARNE

Finally, demonstrate expected individual benefit agents improves,
method used, compared standard cooperative IGP. order illustrate performance
achieved different methods, use setting agents homogeneous terms
information gathering environment. Meaning agent samples opportunities
probability distribution function f (y) (i.e., f1 (y) = .. = fk (y) = f (y)), agents
constrained number opportunities n sample overall (i.e., n1 = .. = nk = n)
share information gathering cost c (i.e., c1 = .. = ck = c) fallback
v0 (i.e., v01 = .. = v0k = v0 ). setting quite common real-life often (and especially
Internet age) people potentially access opportunities similar effort (e.g., time
spent navigating web-site). example, travel agents running example, likely
agents access web-sites resources need used identifying
gathering information different options available getting conference,
none them, trained experienced travel agent, specific advantage
so. simplicity ease exposition figures, use f (y) uniform distribution
function (between 0 1). stress even though homogeneous setting standard
costly information gathering literature (McMillan & Rothschild, 1994; Lippman & McCall, 1976),
common real-life ISP argued above, use case merely illustration purposes
results concerning individual strategies equilibrium structures given
paper based formal theoretical proofs.
3.1 Enforced Probabilistic Information Sharing
method agent Ai priori assigned probability PiIS which, completed
individual IGP, used determine whether allowed take part ISP. assumed
agents priori acquainted probabilities PiIS used enabling information sharing
agent. determination whether agent take part ISP must made proximity
time information actually shared requires enforcing mechanism since
individual IGP completed, agents obviously benefit taking part ISP,
incur cost time improve best finding. Furthermore,
since process takes place individual IGPs terminated, information
agent discloses influence individual IGP strategies used agents.
enforcement easy achieve simple means. example, travel agents running
example equivalent agents send findings designated secured server.
server select eligible information sharing, according pre-defined probabilities,
remove database information coming not. Then,
server allow eligible information sharing access data stores.
agents state throughout individual IGP represented subset opportunities
already gathered information, associated values, consequently remaining opportunities values still unknown. agents strategy thus mapping
world state choice {resume, terminate} resume suggests agent needs
gather information additional opportunity (a random one, since opportunities available given agent priori alike) terminate means agent needs proceed
ISP. Theorem 1 proves state representation case compacted best value
found far (including fallback v0i ), v, optimal strategy represented terms
single reservation value, independent number remaining opportunities.

442

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Theorem 1. Given probability distribution function maximal value obtained
agents take part ISP, denoted fi (x), agent Ai optimal individual information gathering
strategy set reservation value ri v0i , ri solution to:5


ci = PiIS
fi (y)
(max(y, x) max(ri , x))fi (x)dxdy
(1)
y=ri
x=


+ (1 Pi )
(y ri )fi (y)dy
y=ri

agent always choose gather information additional opportunity (if one available) best value obtained far ri otherwise proceed ISP.
Proof. See appendix A.
Theorem 1 specifies optimal strategy agent given strategy others. solution
set k equations similar (1), one agent Ai , provide set pure equilibria
form {ri |1 k} exist. mixed equilibrium case defined probability
pi (v, j) assigned state (v, j), defining whether agent resume terminate information gathering state. may seem infeasible extract, based infinite number
states (as value distributions continuous). Nevertheless, order solution hold,
agents expected benefit actions (resume terminate information gathering
state) must equal. Based optimality reservation-value rule, hold
states value v equals ri calculated according (1). However, due continuous
nature v, probability actually reaching states satisfy condition zero, thus
assigning probabilities effect agents. exception
agents strategy beginning individual IGP. Here, state priori known
(v0i , 0) hence adding probability actually gathering information one opportunity
continuing according ri (or otherwise going straight ISP due indifference resuming terminating information gathering) actual effect others. Consequently,
mixed equilibrium problem form:
{(pi , ri )|1 k}
pi probability agent Ai initiate individual IGP (0 pi 1) ri
reservation value used agent. solution considered stable (i.e., equilibrium),
none agents find beneficial deviate individually. equilibrium pure
strategies problem would require pi {0, 1} i. solution mixed equilibrium
strategy.
individual strategy equilibrium defined complete form (i.e.,
including probabilistic aspect), formulate fi (x) (the probability distribution function
maximal value obtained along IGP agents take part ISP).
purpose make use probability maximum value found
agents take part ISP, except Ai , smaller equal x, denoted Fi (x).
5. Notice equation solution necessarily form v0i ri . case solution
equation (e.g., case substantial ci ), onwards, simply set ri = v0 information gathering
take place.

443

fiROCHLIN & ARNE

calculation Fi (x) makes use probability maximum value obtained along IGP
agent Aj (that chooses engage IGP uses rj ), less x, denoted Fjreturn (x), calculated
according to:6




0
nj
return
F
(x)
Fj
(x) =
j

F (r )nj +
j j

x < v0j
v0j x rj
nj
1Fj (rj )
1Fj (rj ) (Fj (x) Fj (rj )) x > rj

(2)

case x < v0j trivial, v0j lower bound best value agent ends with.
case v0j x rj , value nj opportunities must result value
x. x > rj two possible scenarios. first nj opportunities result
value reservation value rj , i.e., Fj (rj )nj probability. second,
information gathering terminates right revealing value lth opportunity
rj < < x (otherwise, < rj information gathering resume) former l 1
values obtained smaller rj (otherwise lth opportunity reached). probability
latter case occurring (summing values l nj ) calculated using geometric
nj
1F (r )nj
series l=1
(Fj (x) Fj (rj ))Fj (rj )l1 = 1Fj j (rj j ) (Fj (x) Fj (rj )).
probability distribution function maximum value obtained throughout agent Ai IGP,
denoted fireturn (x), definition, first derivative Fireturn (x):
fireturn (x) =

d(Fireturn (x))
dx

Thus, formulate probability maximum value found
agents taking part ISP, except Ai , smaller equal x, Fi (x):
Fi (x) =



(PjIS (pj Fjreturn (x) + (1 pj )) + (1 PjIS ))

(3)

Aj Kj=i

calculation based probability given agent either: (a) take part
ISP (hence contribute value) or, (b) take part ISP best value obtained
individual IGP x. probability (a) (1 PiIS ). calculate probability
(b) first need calculate probability agents best value x.
happen either agent initiated IGP best value obtain lower (or
equal to) x, i.e., probability pj Fjreturn (x), agent opted IGP, i.e.,
probability 1 pj . Therefore probability b given PjIS (pj Fjreturn (x) + (1 pj )).
Consequently, probability distribution function fi (x) derivative Fi (x):
dFi (x)
fi (x) =
dx
enable us calculate expected benefit agent Ai agents use set
strategies {(pi , ri ) |1 k}. agent Ai chooses engage IGP expected benefit,
6. maximum value found includes also fallback v0j . degenerate case Fj (v0i ) = 0 use
Fjreturn (x) = 0 v0i x rj .

444

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

denoted EBi (IGP ), given by:7


EBi (IGP ) = PiIS
fireturn (y)
max(v0i , y, x)fi (x)dxdy+
y=
x=

1 Fi (ri )ni
(1 PiIS )
max(v0i , y)fireturn (y)dy ci
1 Fi (ri )
y=

(4)

first term right hand side expected maximum best value found
agent (i.e., associated distribution fireturn (y)) best value returned
agents (associated distribution fi (x)) agent Ai participates ISP (i.e., PiIS
probability). second term expected best (i.e., maximum) opportunity-value found
agent along information gathering agent allowed take part ISP (i.e.,
1 PiIS probability). last term expected cost incurred throughout IGP carried
n

(ri )
Ai , calculated as: ci nj=1
(Fi (ri ))j1 = ci 1F
1Fi (ri ) , number opportunities
information gathered geometric random variable bounded ni , 1 Fi (ri ) success
probability IGP terminates upon receiving value greater ri (or ni opportunities
explored) probability value greater ri 1 Fi (ri ).
agent opts execute individual IGP all, expected benefit, denoted EBi (
IGP ), simply expected value maximum value returned agents, taking
part ISP, takes part process, otherwise v0i , i.e.:


EBi (IGP ) = Pi
max(v0i , x)fi (x)dx + (1 PiIS )v0i
(5)
x=

point, everything needed formulate equilibrium stability conditions.
set strategies {(pi , ri )|1 k} equilibrium following conditions hold:
(a) every agent Ai pi = 0, EBi (IGP ) EBi ( IGP ).
(b) every agent Ai pi = 1, EBi (IGP ) EBi ( IGP ).
(c) every agent Ai 0 < pi < 1, EBi (IGP ) = EBi (IGP ).
Therefore, order find equilibrium, stability 3k possible solutions type {(pi , ri )
|1 k} differing value pi obtains (pi = 0, pi = 1 0 < pi < 1) needs
checked. every combination, reservation values different agents probability
pi agent uses non-pure mixed strategy (i.e., 0 < pi < 1) calculated
solving set equations type (1) (one agent characterized pi = 0) EBi (IGP ) =
EBi (IGP ) (one every agent Ai 0 < pi < 1). appropriate reservation
values probabilities obtained given set, stability conditions need validated.
note guarantee equilibrium actually exist (either pure mixed,
since infinite number strategies). Also, guarantee one exists
equilibria (i.e., multiple equilibria may exist). latter case, one
7. none agents engage IGP (i.e., pj = 0 Aj = Ai ) fi (x) = 0 therefore expected

n
(ri )
benefit EBi (IGP ) calculated EBi (IGP ) = y= max(v0i , y)fireturn (y)dy ci 1F
,
1Fi (ri )

ri solution ci = y=r (y ri )fi (y)dy (according single agents optimal IGP (Rochlin et al., 2014)),

EBi (IGP ) = v0i .

445

fiROCHLIN & ARNE

Figure 1: Enforced probabilistic information sharing - effect P individual expected
benefit, different: (a) numbers agents, k, setting: c = 0.35 n = 5; (b)
information gathering costs, c setting: k = 15 n = 4.

equilibrium dominates others terms individual expected benefit every
agent obtains likely one used. Otherwise, way deciding
equilibria one use, question included scope current paper.
emphasize analysis generalizes analysis standard cooperative information gathering model (Hazon et al., 2013; Rochlin et al., 2014) sense latter
specific case first, probability agent allowed take part ISP
one (i.e., PiIS = 1 1 k). Furthermore, probability agent allowed
take part ISP zero, solution obtained one known single-agent
information gathering problem (McMillan & Rothschild, 1994) (since agent relies solely
values obtains throughout individual IGP).
Figure 1 depicts agents individual expected benefit function probability PiIS
used, different group sizes (k) information gathering costs (c). setting used
homogeneous setting described beginning section value PiIS
agents (i.e., PiIS = P i). model parameters set to: c = 0.35 n = 5
(Figure 1(a)) k = 15 n = 4 (Figure 1(b)). depicted figure, maximum expected
benefit (agent-wise, agents alike case) obtained participation
agents ISP certain rather determined probabilistically (i.e., 0 < P < 1).
typical pattern exhibited figure increase decrease expected individual
benefit probability P increases. explained follows. P = 0 agent
actually executes individual IGP without information sharing others. P increases,
agent relies agents findings. Thus, pi ri become lower, bad
group since everybody gains less participation agent ISP. However,
time probability agent actually take part ISP increases, thus, overall,
individual expected benefit increases. Nevertheless, value P , loss due
resulting decrease pi ri becomes dominant benefit due increase
value P .
446

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Another interesting behavior observed Figure 1 increase information gathering
costs, c, increase number agents, k, results decrease value P
maximizes expected benefit. may seem non-intuitive since greater information
gathering cost greater potential benefit achieved information sharing
similarly, greater number agents greater chances obtaining favorable values
ISP. Therefore limiting information sharing settings high k c values may
seem unnatural. phenomena explained fact positive effect increase
P participation probabilities pi reservation value ri used agent
equilibrium case, substantially poor begin with, greater loss due
uncertain information sharing.
method requires agent either completely avoid fully participate
ISP, many variants considered, e.g., partially limiting information sharing.
example, agent requested, probability, contribute information
gathered, without receiving group members information. Alternatively, agent
allowed receive information however share gathered data. variants can,
settings, result substantially superior performance illustrate Appendix B.
3.2 Threshold Restricted Information Sharing
method agents found highly favorable values along individual IGP
prevented taking part ISP. implemented setting threshold ViIS ,
agent Ai , requires agent opt-out ISP obtained value greater ViIS
individual IGP. assumed agents priori acquainted thresholds
agents. travel agents running example take form Enforced probabilistic information sharing, slight modifications, e.g., agents send findings
designated secured server. server select eligible information sharing, according
pre-set thresholds, remove database information coming
not. Then, server allow eligible information sharing access data
stores.
choice excluding agents findings greater threshold may seem
counter intuitive. Seemingly favorable findings, i.e., actually performed well
contribute others punished. One may argue suitable choice
would set threshold agents found good values would excluded
ISP, thereby encouraging try harder. Nonetheless, also suggests discouragement agents IGPs greater expectations high values
reported ISP. Obviously, time ISP ought take place, opting dominated
taking part ISP, individually globally. agents benefit participation
agents ISP. Moreover, fact require agents favorable values
take part ISP seems intensify potential negative effect method discussed
above. Nevertheless, since agents may find situation requested
take part information sharing, due decrease potential improvement
encapsulated information sharing agents eventually take part it, likely
agent individually motivated towards efficient IGP.
Overall, Threshold restricted information sharing method bit complicated enforce since agents may choose declare value different best found, effort
447

fiROCHLIN & ARNE

Figure 2: schematic illustration optimal strategy Threshold restricted information sharing.

take part information sharing. Nevertheless, whenever true value validated (e.g.,
travel agents example, agent requested post receipt indicating way got
conference amount paid, ISP) method easily enforced using
fines.
case, prove following paragraphs, agents strategy needs take
consideration best value obtained thus far, also number opportunities
information already gathered. whenever value v > ViIS encountered,
whereby agent excluded ISP, agent still benefit resuming individual
IGP, benefit additional information gathering depends number remaining
opportunities.
structure agents optimal strategy, given strategy others, given Theorem
2.
Theorem 2. Given probability distribution function maximal value obtained
agents take part ISP, fi (x), number opportunities information
already gathered, j ni , agent Ai optimal individual information gathering strategy
described pair (ri (j), riresume ), v0i ri (j) ViIS riresume (see Figure 2),
where:

)
(
resume
(y r)fi (y)dy
(6)
ri
= max Vi , r|ci =
y=r

ri (j) solution to:



fi (y)
(max(y, x) max(ri (j), x))fi (x)dxdy
y=ri (j)
x=


+
fi (y)
(EB j+1
(y) max(ri (j), x))fi (x)dxdy

ViIS

ci =

y=ViIS

(7)

x=

EB ji (y) given by:
{
v
j

EBi (v) =
ci + y=EB j+1
(max(v0i , y, v))fi (y)dy


riresume v j > ni
v < riresume j ni

(8)

Given best value obtained far, v, Agent Ai resume IGP v < ri (j) ViIS <
v riresume otherwise terminate (and proceed ISP v ViIS ).
448

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Proof. See appendix C.
Intuitively, reservation value riresume used determine IGP resumed
cases favorable value v > ViIS found agent own.
reservation value ri (j) used determine IGP resumed v ViIS , i.e.,
agent still potentially take part ISP. Resuming IGP latter case lead
better values, however time also lead exclusion ISP, case
agent end own. Since fallback case finding v > ViIS depends
number remaining opportunities j, use different reservation value ri (j) j value.
case riresume = ViIS , benefit agent resume IGP value v > ViIS
found. case ri (j) = r j.
again, solution set equations consisting (6-8) provide set pure
equilibria form {(ri (1), .., ri (ni )), riresume |1 k}, exist. Similarly,
considerations given Section 3.1, mixed equilibrium case form:
{(pi , (ri (1), .., ri (ni )), riresume )|1 k}.
equilibrium analysis case generally resembles one given 3.1, calculation Fireturn (x), probability maximum value obtained agent Ai
individual IGP (including v0j ) less x substantially complicated. order
formulate Fireturn (x) use Fireturn (x, v, j) denote probability agent Ai obtain
maximum value x below, given current state (v, j), v best value obtained
far IGP (including v0i ) j number opportunities information
gathered. function Fireturn (x, v, j) calculated recursively according to:

v>x )
0
(

resume
return
1
v x ri (j) < v Vi ri
v j = ni
Fi
(x, v, j) =

return (x, max(v , y, v), j + 1)f (y)dy
F
otherwise

0
y=
(9)
case x < v trivial since maximum value agent obtain least
v. Therefore, probability obtaining x zero. Similarly, ri (j) < v ViIS
riresume v additional opportunities (j = ni ) agent inevitably
terminate IGP (according Theorem 2) maximum value obtained
v. case, v x function obtains 1. cases, IGP resumes, hence
probability given recursively based new state (x, max(v, y), j + 1) agent
gathering information one additional opportunity. Using Fireturn (x, v, j) calculate
probability maximum value obtained agent Ai individual IGP
less (or equal to) x, as: Fireturn (x) = Fireturn (x, , 0).
Thus, formulate probability maximum value provided agent Ai

ISP less x, denoted Fireturn (x):

0
x < v0i


return
return
return

F
(x) + (1 Fi
(Vi )) v0i x ViIS
Fi
(x) =
(10)

1
otherwise
case x < v0i trivial, v0i lower bound best value agent ends with.
case x ViIS satisfied either Ai best value less x (in case take
449

fiROCHLIN & ARNE

part ISP since x ViIS ), i.e., probability Fireturn (x), agent take part
ISP, i.e., probability 1Fireturn (ViIS ). case x > ViIS straightforward
since agent Ai take part ISP best value lower ViIS , probability
obtaining value v (where x > ViIS ) 1.

Using Fireturn (x), calculate function Fi (x) (a modification (3)):
Fi (x) =





(pj Fjreturn (x) + (1 pj ))

(11)

Aj Kj=i

probability distribution function fi (x) first derivative Fi (x) before. Similarly,
probability distribution function fireturn (x) first derivative Fireturn (x).
turn calculating expected cost incurred throughout IGP carried agent
Ai , denoted ECi . order calculate ECi use ECi (v, j) denote expected cost
agent Ai , state (v, j), v best value obtained gathering information j
opportunities. value ECi (v, j) calculated recursively according to:
{
ECi (v, j) =

0
ri (j) v < ViIS riresume v j = ni


ci + y= ECi (max(v0 , y, v), j + 1)fi (y)dy
otherwise
(12)

first case agent unavoidably terminates IGP (according Theorem 2).
second case where, IGP resumes, hence expected cost given recursively based
new state (max(v, y), j + 1) agent gathering information one additional
opportunity. allows us calculate ECi (x), as: ECi (x) = ECic (, 0).
enable us calculate expected benefit agent Ai agents use set
strategies {(pi , (ri (1), .., ri (ni )), riresume )|1 k = j}. agent Ai choose engage
IGP expected benefit, EBi (IGP ), given by:

max(v0i , y)fireturn (y)dy
(13)
EBi (IGP ) = ECi +

+

ViIS

fireturn (y)

y=



y=ViIS



max(v0i , y, x)fi (x)dxdy

x=

similar (4), except differentiation second third terms
right according ViIS rather PiIS .
agent opts gather information all, expected benefit, EBi (IGP ),
simply expected value maximum value returned agents taking part ISP:

EBi (IGP ) =
max(y, v0i )fi (y)dy
(14)
y=

equilibrium stability conditions remain Section 3.1, replacing calculation EBi (
IGP ) EBi (IGP ) (13) (14). former method, guarantee
equilibrium actually exist (either pure mixed) one exists
equilibria. Also, method presented above, analysis Threshold restricted
method generalizes analysis standard cooperative information gathering model
450

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Figure 3: Threshold restricted information sharing - effect V individual expected benefit, different information gathering costs, c, setting: k = 2 n = 1.

sense latter specific case threshold set taking part ISP ViIS
i. Similarly, ViIS i, solution obtained one known
single-agent information gathering problem (McMillan & Rothschild, 1994).
Figure 3 illustrates agents individual expected benefit function thresholds ViIS
used, different information gathering costs (c). setting used homogeneous setting
described beginning section, parameters c = 0.4 n = 1. values ViIS
similar agents (i.e., ViIS = V i). depicted figure, maximum expected
individual benefit obtained value threshold excluding agents ISP
is, cases, substantially small, leaving many favorable findings outside ISP. typical
pattern exhibited figure similar one depicted Figure 1: increase
decrease expected individual benefit threshold V increases. explained
fact V = 0 agent actually executes individual IGP without sharing information
others. V increases, agent relies information gathered agents
considerations explained 3.1 regarding tradeoff negative effect
pi ri positive effect chance actually taking part ISP hold.
3.3 Cost Filtered Information Sharing
method introduces cost cIS agent incurs chooses take part ISP. example,
travel agents running example equivalent agents interested
taking part ISP present evidence donating fixed amount money charity,
prerequisite accessing designated secured server used sharing. Unlike two
prior methods, agents get decide whether opt-out information sharing; hence
require enforcement whatsoever. require, however, means introducing cost
ISP, e.g., donation setting meeting place sharing information
participant need spend time money getting there.
451

fiROCHLIN & ARNE

assumed agents priori acquainted ISP participation costs agents.
introduction cost (that eventually returned agents) requires appropriate
balance form compensation agents taking part ISP. Without
compensation, agent willing take part ISP, proven following proposition.
Proposition 1. agents incur cost taking part ISP, absence
compensation taking part ISP, none agents take part ISP.
Proof. Consider highest value v known agents, completing individual IGP, warrants participation agent ISP. show absence
appropriate compensation agent, value v cannot hold since none agents
contribute value greater v ISP, agent found v gain anything
ISP however incur cost; consequently choose take part ISP.
One option compensate agents taking part ISP offer agent best
value takes part ISP compensation B. amount B collected agents
(e.g., equal shares) prior IGP, collected considered sunk cost
agents strategies become affected chance receiving bonus B. structure
best response strategy individual agent case, given strategy others, given
Theorem 3.
Theorem 3. Given probability distribution function maximal value obtained
agents take part ISP, fi (x), agent Ai optimal individual information gathering strategy
described pair (ri , RiIS ), RiIS set intervals x
RiIS x v0i :
x



fi (y)dy
(15)
(y x)fi (y)dy + B
c
y=

y=x

set value ri v0i , ri solution to:

ci = (EBi (y) EBi (ri ))fi (y)dy

(16)

y=ri

EBi (v) given by:


v
v
EBi (v) = cIS + B y= fi (y)dy


+ y= max(v0i , y, v)fi (y)dy

v
/ RiIS
otherwise

(17)

agent resume individual IGP long value found far ri ,
otherwise terminate IGP. Upon terminating IGP (or obtaining value
opportunities) agent participate ISP best value found individual
IGP one intervals set RiIS otherwise opt taking part ISP.
Proof. set RiIS defined (15) contains values v
( expected benefit)from

taking part ISP - calculated potential value improvement,
(y x)fi (y)dy plus
y=x

452

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

x
expected compensation B y= fi (y)dy, independent reservation value ri used
agent - greater cost cIS incurred. remainder proof, concerning
optimality reservation-value-based strategy correctness (16) one
provided Theorem 1, differing way expected benefit information gathering
resumed calculated.
solution set equations consisting (16)-(17) agent provide set
pure equilibria form {(ri , RiIS )|1 k}, exist. considerations given
Section 3.1, mixed equilibrium case form {(pi , ri , RiIS )|1 k}.
equilibrium analysis Cost filtered information sharing follows analysis given
previous methods, therefore provide differences. calculation Fireturn (x)
remains (2). probability distribution function fireturn (x) first order derivative

Fireturn (x). function Fireturn (x), calculated modification (10):


Fireturn (x) = Fireturn (x) +
fireturn (y)dy
yxy R
/ iIS

consequently function Fi (x) given by:
Fi (x) =





(pj Fjreturn (x) + (1 pj ))

Aj Kj=i

probability distribution function fi (x) first order derivative Fi (x) before.
enable us calculate expected benefit agent Ai agents use set
strategies {(pj , rj , RjIS )|1 j k = j}. agent Ai choose engage IGP
expected benefit, EBi (IGP ), given by:

1 Fi (ri )ni
+ EBi (max(y, v0i ))fireturn (y)dy
(18)
EBi (IGP )=ci
1 Fi (ri )
y=
first term expected cost incurred throughout IGP carried Ai , calculated as:
n

(ri )
ci nj=1
(Fi (ri ))j1 = ci 1F
1Fi (ri ) , number opportunities information gathered geometric random variable bounded ni , 1 Fi (ri ) success probability.
second term expected benefit executing IGP. agent opts gather information all, expected benefit, EBi (IGP ), simply maximum expected
value maximum value returned agents v0i , agent choose take
part ISP:
EBi (IGP ) = EBi (v0i )

(19)

equilibrium stability conditions remain Section 3.1, replacing calculation EBi (
IGP ) EBi (IGP ) (18) (19). former methods, guarantee
equilibrium actually exist (either pure mixed) one exists
equilibria. Also, methods presented above, analysis cost filtered method
generalizes analysis standard cooperative information gathering model sense
latter specific case cIS = B = 0. Similarly, B = 0 solution obtained
453

fiROCHLIN & ARNE

Figure 4: Cost filtered information sharing - effect cIS individual expected benefit,
different information gathering costs, c setting: k = 2, n = 3 B = 0.04.

one known single-agent information gathering problem (regardless value
cIS , based Proposition 1).
Figure 4 illustrates agents individual expected benefit function cost cIS used,
different information gathering costs (c). setting used homogeneous setting described
beginning section, using parameters k = 2, n = 3 B = 0.04. illustrated
figure, maximum expected individual benefit obtained substantial cost incurred
taking part ISP. typical pattern exhibited figure similar one depicted
Figures 1 3, explained similar considerations.
3.4 Random Finding Sharing
method agent Ai , completed individual IGP, allowed take part
ISP, however restricted disclose one values come across IGP,
randomly selected. downside restriction obvious - cases agents
benefit best individual findings rather become exposed small, necessarily
optimal, subset information gathered. Still, demonstrate following paragraphs,
settings individual performance improves. travel agents running example
implementation method quite straight forward relies, before, agents
send findings designated secured server used sharing. difference, however,
server pick single random finding agent discard rest
findings. Here, again, enforcement necessary since individual agent benefit
disclosing information required disclose.
structure best response strategy individual agent case, given strategy
others, given Theorem 4.
Theorem 4. Given probability distribution function maximal value obtained ISP
based reports agents, denoted fi (x), agent Ai optimal individual information
454

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

gathering strategy follow reservation value ri v0i , ri solution to:


ci =
fi (y)
(max(y, x) max(ri , x))fi (x)dxdy
y=ri

(20)

x=

agent always choose gather information additional opportunity (if one still
available) best value obtained far ri otherwise proceed ISP.
Proof. proof similar one given Theorem 1, differing way expected
benefit gathering information additional opportunity calculated.
solution set k equations type (20), one agent, provide set pure
equilibria form {ri |1 k} exist. considerations given Section 3.1,
mixed equilibrium case form {(pi , ri )|1 k}.

equilibrium analysis method follows one given 3.1, except Fireturn (x)
used 3.2. function Fireturn (x) given (2). probability distribution function

fireturn (x) first order derivative Fireturn (x). order formulate Fireturn (x) use
different state definition Fireturn (x, v, j, l). state (v, j, l) defined according
number values x obtained agent Ai gathering information j opportuni
ties, denoted l, best value obtained far (including v0i ), v. function Fireturn (x, v, j, l)
calculated recursively according to:

0
x < v0i



l

v ri ni = j

j
x

Fireturn (x, v, j, l) =
(21)
return
F
(x,
max(y,
v),
j+1,
l+1)f
(y)dy
otherwise



y=



+ F return (x, max(y, v), j+1, l)f (y)dy

y=x
case x < v0i trivial, v0i lower bound best value agent ends with.
second case (21) straightforward best value obtained far v ri ,
additional opportunities (ni = j), agent necessarily terminates IGP (according
Theorem 4). cases, l values total j values x, probability
Ai return value x jl . cases, IGP resume; hence probability
given recursively based new state (x, max(y, v), j + 1, l + 1) case new value obtained
x (x, max(y, v), j + 1, l) otherwise.

Thus, formulate Fireturn (x):




Fireturn (x) = Fireturn (x, , 0, 0)

(22)

consequently Fi (x) given (11) probability distribution function fi (x) first
order derivative Fi (x) before.
enable us calculate expected benefit agent Ai agents use set
strategies {(pj , rj )|1 j k = j}. agent Ai choose engage IGP expected
benefit, EBi (IGP ), given by:


1 Fi (ri )ni
EBi (IGP ) = ci
+
fireturn (y)
max(v0i , y, x)fi (x)dxdy
(23)
1 Fi (ri )
y=
x=
455

fiROCHLIN & ARNE

Figure 5: Random finding sharing Standard information sharing (according Rochlin et al.,
2014) function of: (a) information gathering costs, c, setting: k = 5 n = 3;
(b) number agents, k, setting: c = 0.1 n = 3.

agent opts gather information all, expected benefit, EBi (IGP ),
simply expected value maximum value returned agents:

EBi (IGP ) =
max(v0i , x)fi (x)dx
(24)
x=

equilibrium stability conditions remain Section 3.1, replacing calculation EBi (
IGP ) EBi (IGP ) (23) (24). former method, guarantee
equilibrium actually exist (either pure mixed) one exists
equilibria.
Figure 5 depicts agents individual expected benefit function information gathering
costs c (left graph) number agents k (right graph). setting used homogeneous
setting described beginning section, parameters k = 5 n = 3 (Figure 5(a))
c = 0.1 n = 3 (Figure 5(b)). graph depicts performance Random finding
sharing standard non-restricted sharing method (according Rochlin et al., 2014).
figure shows random finding sharing strategy dominates standard information sharing
strategy, far expected individual benefit concerned.
3.5 Subgroup Restricted Information Sharing
method agents divided sub groups share findings separately, i.e.,
agent executes individual IGP separately end subgroup carry separate
ISP.
Formally, consider
wthe case |K| agents divided w subgroups {K1 , , Kw }
( w
K
=


j
j=1
j=1 Kj = K). travel agents running example done
providing sub-group designated server serve subgroup members or,
physical environments, setting different meeting places sharing information group
partitioned sub-groups.
456

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Figure 6: Subgroup restricted information sharing - individual expected benefit function
number subgroups, w, setting: c = 0.45, k = 20 n = 10.

best response strategy individual agent Aji subgroup Kj = {Aj1 , . . . Aj|kj | } K
well equilibrium analysis within subgroup level given (Rochlin
et al., 2014), agents within subgroup fully share findings. optimal strategy
thus obtained checking expected benefit possible divisions K subgroups
selecting partition associated highest expected benefit. computational complexity
evaluating subgroups combinatorial number agents. Although focus
paper computational aspects rather analyzing structure equilibrium
cooperative strategies, note ISP settings computational complexity becomes
non-issue since number agents taking part ISP relatively small.
note method results similar performance (Rochlin et al., 2014)
number subgroups w = 1 (i.e., k agents take part ISP one group). Similarly,
number subgroups w = |K| (i.e., subgroup contains one agent) solution obtained one known single-agent information gathering problem
(McMillan & Rothschild, 1994).
Figure 6 illustrates agents individual expected benefit function number subgroups used (w). setting used homogeneous setting described beginning
section, using parameters k = 20, n = 10 c = 0.45. graph uses partitioning equalsize subgroups, i.e., w obtains values {1, 2, 4, 5, 10}. depicted figure, maximum
expected individual benefit obtained example number subgroups w = 2.

4. Related Work
model analyzed paper based two important concepts: multi-agent cooperation
costly information gathering. Multi-agent cooperation shown widely effective
better achieving agents individual goals (Stone & Kraus, 2010) improve performance measures (Kraus, Shehory, & Taase, 2003; Dutta & Sen, 2003), especially differences
agents capabilities, knowledge resources agent incapable completing
457

fiROCHLIN & ARNE

task (Stone & Kraus, 2010; Saad, Han, Debbah, & Hjorungnes, 2009; Conitzer, 2012;
Breban & Vassileva, 2001). also main driving force behind many coalition formation models area cooperative game theory MAS (Shehory & Kraus, 1998). Yet, majority
cooperation coalition formation MAS-related research tends focus way coalitions
formed consequently concerns issues optimal division agents disjoint
exhaustive coalitions, division coalition payoffs enforcement methods interaction protocols. coalition formation coordination models widely found electronic
market domain, work domain emphasizes mechanisms forming cooperation
purpose aggregating demands order obtain volume discounts (Tsvetovat, Sycara, Chen, &
Ying, 2000; Yamamoto & Sycara, 2001; Sarne & Kraus, 2003). Several authors considered
problem determining strategy group formed (Ito, Ochi, & Shintani, 2002; Sarne,
Manisterski, & Kraus, 2010; Rochlin, Sarne, & Zussman, 2011; Mash, Rochlin, & Sarne, 2012),
however focus mostly fully-cooperative agents. None works considered cooperation problem group self-interested agents costly exploration settings findings
benefit agents.
Group-based cooperation self-interested agents also found public goods games
allocation games general (Aumann, 1998; Nagel & Tang, 1998; McKelvey & Palfrey, 1992;
de Jong et al., 2008; de Jong & Tuyls, 2011). Common games according
equilibrium agent individually opt cooperation soon possible invest
minimum allowed. Therefore research cooperation domain mostly studied
repeated games (Selten & Stoecker, 1986) settings bounded-rational participants (e.g., people) cooperation extent commonly exhibited. Much effort placed
developing reciprocity-based mechanisms, e.g., tit-for-tat (Axelrod, 1984) facilitate cooperation
even agents find momentarily beneficial act selfishly. way, long-term considerations
override short-term greedy behavior. Many extended basic mechanism support various
variants model, asymmetric costs, heterogeneously repeating instances factors (Sen, 1996). works dealt inducing cooperation non-repeated settings showing
rewards somewhat less effective sanctions enforcing cooperation (Walker & Halloran, 2004). main difference public goods games work case
public goods fact information lead better economic decisions. Obtaining
information requires carrying active sequential information gathering process. Therefore,
settings much room individual information gathering, extent, even
others free riders. Moreover, simplistic settings used public goods games
issue information sharing ways carried (when considering self-interested agents)
becomes irrelevant.
second concept upon paper relies, i.e., costly information gathering, great
importance central source supply agent full immediate reliable information environment state agents (Sarne & Aumann, 2014). general,
introduction information gathering costs MAS models leads realistic description
environments. agents typically required invest/consume
resources order obtain information concerning opportunities available environment
(Bakos, 1997; Sarne & Kraus, 2008; Kephart & Greenwald, 2002; Rochlin & Sarne, 2013; Rochlin,
Sarne, & Zussman, 2013; Manisterski, Sarne, & Kraus, 2008).
Optimal strategies settings individuals need search applicable opportunity
information gathering costly widely studied (Grosfeld-Nir, Sarne, & Spiegler,
458

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

2009; Elmalech, Sarne, & Grosz, 2015; Elmalech & Sarne, 2012), prompting several literature reviews (Smith, 2011; McMillan & Rothschild, 1994; Morgan & Manning, 1985). models,
often termed costly search models economic search models developed
point total contribution referred search theory. years, many
information gathering model variants considered, focusing different aspects
model, decision horizon (finite versus infinite) (Lippman & McCall, 1976), presence recall option (McMillan & Rothschild, 1994), option disambiguate noisy signals
(Chhabra, Das, & Sarne, 2014a; Alkoby, Sarne, & Das, 2015; Chhabra, Das, & Sarne, 2014b),
distribution values extent findings remain valid along process (Landsberger
& Peled, 1977). particular, models integrated study strategic information platforms (Hajaj, Hazon, Sarne, & Elmalech, 2013; Sarne, 2013; Hajaj & Sarne, 2014).
Another strand search-based models two-sided search (Sarne & Arponen, 2007; Hendrix &
Sarne, 2007) deals distributed (search-based) formation pairwise (or general size)
partnerships (Nahum, Sarne, Das, & Shehory, 2015). analysis models derive
equilibrium considerations, different model reflect
cooperative aspect.
Many cooperative information gathering models studied, extending theories
multi-agent (or multi-goal) environments (Sarne & Kraus, 2005). Examples include, among others,
attempt purchase several commodities facing imperfect information concerning prices
operating several robots order evaluate opportunities different locations. works
differ either consider fully cooperative agents attempt maximize
overall utility (Sarne et al., 2010; Gatti, 1999; Burdett & Malueg, 1981; Carlson & McAfee, 1984),
thus lack equilibrium considerations, assume agents IGP constrained
findings agents, rather augmented/improved findings
case (Rochlin, Sarne, & Laifenfeld, 2012; Rochlin & Sarne, 2014b). Consequently constitute substantially different equilibrium strategies. Models consider cooperative information
gathering, rely assumptions similar (e.g., Rochlin et al., 2014; Hazon et al., 2013),
focus primarily extraction equilibrium strategies investigate influence
different model parameters agents performance equilibrium. None works, however, suggested methods improving cooperative information gathering settings,
kind suggest analyze paper.
broadly, problem seen part field planning uncertainty, hence
related Markov decision processes (MDP) (Bellman, 1957; Puterman, 1994) decentralized
Markov decision processes (Bernstein, Givan, Immerman, & Zilberstein, 2002). models
goal maximize expected cumulative reward, also objective case. Alas,
use MDPs case complicated continuous nature value probability distribution
functions. importantly, analysis proofs result threshold-based (or interval-based)
solutions simpler terms strategy state representation derived
substantially lesser complexity compared solving via MDPs.
Finally, note non-intuitive findings whereby methods essentially limit information sharing cooperation actually positive impact self-interested case follows,
spirit, earlier results settings. particular, ones shown socalled inefficiencies increase market performance, certain circumstances. example,
Masters (1999) shows increase minimum wage, often considered inefficiency
economics, positive employment effects. transportation economics (e.g., congestion
459

fiROCHLIN & ARNE

games) equilibrium frequently overall optimum. cases, shown taxation change equilibrium desirable one (Penn, Polukarov, & Tennenholtz, 2009b,
2009a; Fotakis, Karakostas, & Kolliopoulos, 2010). Similarly, taxes facilitate desirable
equilibria Boolean games (Endriss, Kraus, Lang, & Wooldridge, 2011) centralized matching schemes (Anshelevich, Das, & Naamad, 2013). work show somewhat similar
phenomenon also occurs context cooperative information gathering, though model
analysis are, course, totally different mentioned.

5. Discussion Conclusions
demonstrated Section 3, five methods proposed analyzed paper
substantially increase benefit self-interested agents achieve information sharing
gathering information cooperatively. five methods based different restriction
made agents ability willingness take part ISP. Intuitively restrictions may
seem negative effect performance. Yet, since agent gains less information
sharing itself, greater incentive invest resources individual information gathering,
hence overall performance improves.
results suggest important inputs designers markets systems cooperative
information gathering applicable, enabling predict strategies used
resulting system performance. primarily facilitate proper design system
determination elements included systems order
achieve specific goals promote certain behavior. particular, introduction
seemingly non-beneficial elements may actually productive. note paper generally
attempt find optimal parameter values method (e.g., probability, group
partitioning, threshold cost taking part information sharing, payment received
agent associated best value), since concept optimality sense
properly defined. Indeed settings equilibrium solution preferred
agents (e.g., examples given former sections, agents homogeneous)
choice parameter values clear. Nevertheless general, possible certain value
preferred one agents whereas others prefer another. latter case role
system designer decide parameters based goals.
numerous extensions model considered. straightforward require minor changes analysis. example, agents buyers
interested single unit product searching for, change
required individual strategy equations multiplication expense purchasing item
number items agent interested. Another example composition
several methods. example, case Subgroup restricted information sharing,
subgroups adopt four methods subgroup level. extensions,
much interest, complex analyze. example, consider model agents
continuously share findings along individual IGPs. case, discussed Section 1, essential first define method provide incentive agents share
findings despite negative influence terms discouraging others
information gathering.
460

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Acknowledgments
Preliminary results work appeared Proceedings Thirteenth International Conference Autonomous Agents Multiagent Systems (Rochlin & Sarne, 2014a). authors would
like thank Barbara Grosz insightful comments helped improve paper substantially.
first coauthor student Bar-Ilan University research reported paper
carried out. research partially supported ISRAEL SCIENCE FOUNDATION (grant
No. 1083/13) ISF-NSFC joint research program (grant No. 2240/15).

Appendix A. Proof Theorem 1
Proof. first prove reservation-value nature optimal strategy. continue
inductive proof, show reservation value used agent remain stationary along
IGP calculated according Equation 1.
absence new information along IGP, agents strategy
mapping S(x, j) {terminate, resume}, x set values obtained far (including v0i ) j number opportunities information already gathered. Since
agent interested merely maximum opportunity value, strategy affected
maximum value x, hence strategy defined S(v, j) {terminate, resume},
v maximum value x. Obviously, according optimal strategy agent needs
resume IGP upon reaching state (v, j) true state (v , j)
v < v. Similarly, according optimal strategy exploration terminate state (v, j)
hold state (v , j) v > v. Therefore, given number
opportunities information gathered, j, optimal individual IGP strategy
agent Ai characterized reservation value rij agent resume IGP
best value obtained far rij otherwise terminate IGP.
begin case j = ni 1. best value obtained thus far agent Ai v
gathering information one last opportunity, according strategy, incur cost ci
expected value agents obtain be:

PiIS







fi (y)
y=

+ (1 PiIS )



(max(y, v, z)fi (z))dzdy

z=

max(y, v)fi (y)dy

y=

first term relates case agent Ai participate ISP, probability
PiIS , whereas second term relates case required opt ISP,
i.e., probability 1 PiIS (where new value obtained agent
z best value obtained agents IGP).
hand, terminating IGP point result benefit:

PiIS





max(v, z)fi (z)dz + (1 PiIS ) v

z=

461

fiROCHLIN & ARNE

Therefore, agent gather information last opportunity if:


Pi
max(v, z)fi (z)dz + (1 PiIS ) v <
z=


PiIS
fi (y)
(max(y, v, z)fi (z))dzdy
y=
z=


+ (1 Pi )
max(y, v)fi (y)dy ci

(25)

y=

left hand side equation captures expected benefit individual IGP terminated
right hand side captures expected benefit information gathered last opportunity. terms distinguish case agent Ai participates ISP i.e.,
probability PiIS , allowed to. Using simple mathematical manipulations
obtains:



0 < Pi
fi (y)
(max(y, z) max(v, z)fi (z))dzdy
(26)
y=v
z=

+ (1 PiIS )
(y v)fi (y)dy ci
y=v

value v (26) becomes equality fact value ri according (1). Therefore, since right hand side (26) decreasing function v, agents gather
information last opportunity whenever value v less value r according
(1). establishes first part proof.
assume ri (according (1)) holds j > j, j, consider
agents decision regarding gathering information one opportunity, best value
obtained thus far v number opportunities values already obtained
j. v > ri v0i agent gathers information one additional opportunity, regardless
value obtained next definitely terminate individual IGP thereafter (as already
value greater ri according induction assumption optimal strategy thereafter
reservation value ri ). Therefore benefit obtained information gathering given
by:



Pi
fi (y)
(max(y, z) max(v, z)fi (z))dzdy
y=
z=

+ (1 PiIS )
(y v)fi (y)dy ci
y=

Alas, since latter term decreases v increases, obtains zero v = ri (according (1)),
since v > ri v0i term obtains negative value, hence additional information gathering
cannot preferred choice.
Similarly, consider case v0i v < ri j agent chooses gather additional information. expected benefit resuming information gathering necessarily
greater resuming state (v, j > j). However, according induction assumption
agent resume information gathering state (v, j > j), leading contradiction. Therefore, optimal strategy j also reservation value strategy optimal reservation value
calculated, again, according (1).
462

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Figure 7: Enforced probabilistic information sharing - effect P H individual expected
benefit, different: (a) numbers agents, k, setting: c = 0.35 n = 5; (b)
information gathering costs, c, setting: k = 15 n = 4.

Appendix B. Partially Limiting Information Sharing
Consider case agent Ai , completed individual IGP, shares
findings however obtain information others probability PiH ,
priori assigned. structure best response strategy individual agent case,
given strategy others, identical one given Theorem 1, replacing PiIS PiH .
Similarly rest analysis given hold case, except calculation Fi (x),
given by:


Fi (x) =

(pj Fjreturn (x) + (1 pj ))

(27)

Aj Kj=i

Figure 7 depicts agents individual expected benefit function probability PiH
used, different group sizes (k) information gathering costs (c) using setting used
Figure 1. comparison Figures 1 7 reveals case partial limitation
information sharing dominates enforced probabilistic information sharing strategy, far
expected individual benefit concerned. Nonetheless, certainly general result.

Appendix C. Proof Theorem 2
Proof. proof generally resembles one given Theorem 1. best value agent Ai
found far (including v0i ) v > ViIS allowed take
part ISP. j = ni 1
resuming IGP result expected benefit ci + y= max(y, v)fi (y)dy whereas
terminating IGP guarantee v. agent explore v first
463

fiROCHLIN & ARNE

term greater latter, represented, mathematical manipulations, as:




ci <

(y v)fi (y)dy

y=v

equivalent value v greater riresume according 6.8
remainder proof deals showing reservation-value nature optimal
strategy v > ViIS reservation value used agent remains stationary along
IGP Theorem 1.
consider case best value agent Ai found far v0i v ViIS
number opportunities information already gathered j. Resuming IGP
result in:

ci +



ViIS



fi (y)
y=


(max(y, v, z)fi (z))dzdy +

z=



y=ViIS

EB j+1
(y)fi (y)dy


EB j+1
(y) expected benefit agent Ai , given best value obtained thus far,

(including v0i ), number opportunities information already gathered, j.
term EBij (v) calculated recursively according (8). remaining opportunities
(j > n) best value found v riresume expected benefit simply v. Otherwise,
IGP resumes (incurring cost ci ) revised best value (max(y, v)), however j + 1
opportunities information already gathered, i.e., expected benefit
EBij+1 (max(y, v)). hand, terminating IGP point result expected
benefit:

max(v, z)fi (z)dz
z=

Therefore, agent explore v first term greater
second, mathematical manipulations becomes:

ci <

+

ViIS

y=v


y=ViIS





(max(y, z) max(v, z))fi (z)dzdy
fi (y)
z=

fi (y)
(EBij+1 (y) max(v, z))fi (z)dzdy
z=

equivalent value greater ri (j) according 7. reason different threshold
used different j values calculation threshold depends among others
expected benefit case value obtained v > ViIS , case expected benefit
depends number remaining opportunities.

8. value riresume resulting ci = y=v (yv)fi (y)dy lower ViIS , agent inevitably terminate
IGP (as v > riresume ) hence value riresume ViIS used, particular riresume = ViIS
theorem.

464

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

References
Alkoby, S., Sarne, D., & Das, S. (2015). Strategic free information disclosure search-based
information platforms. Proceedings 2015 International Conference Autonomous
Agents Multiagent Systems (AAMAS 2015), pp. 635643.
Anshelevich, E., Das, S., & Naamad, Y. (2013). Anarchy, stability, utopia: creating better
matchings. Autonomous Agents Multi-Agent Systems, 26(1), 120140.
Aumann, R. (1998). centipede game. Games Economic Behavior, 23(1), 97105.
Axelrod, R. (1984). evolution cooperation. Basic Books.
Bakos, Y. (1997). Reducing buyer search costs: Implications electronic marketplaces. Management Science, 42, 16761692.
Bellman, R. (1957). Markovian decision process. Indiana University Mathematics Journal, 6(4),
679684.
Bernstein, D., Givan, D., Immerman, N., & Zilberstein, S. (2002). complexity decentralized
control Markov decision processes. Mathematics Operations Research, 27(4), 819840.
Breban, S., & Vassileva, J. (2001). Long-term coalitions electronic marketplace. Proceedings E-Commerce Applications Workshop, Canadian AI Conference.
Burdett, K., & Malueg, D. A. (1981). theory search several goods. Journal Economic
Theory, 24(3), 362376.
Carlson, J. A., & McAfee, R. P. (1984). Joint search several goods. Journal Economic Theory,
32(2), 337345.
Chhabra, M., Das, S., & Sarne, D. (2014a). Competitive information provision sequential search
markets. Proceedings International conference Autonomous Agents MultiAgent Systems (AAMAS 2014), pp. 565572.
Chhabra, M., Das, S., & Sarne, D. (2014b). Expert-mediated sequential search. European Journal
Operational Research, 234(3), 861873.
Chhabra, M., & Das, S. (2011). Learning demand curve posted-price digital goods auctions.
Proceedings 10th International Conference Autonomous Agents Multiagent
Systems (AAMAS 2011), pp. 6370.
Conitzer, V. (2012). Computing game-theoretic solutions applications security. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence, pp. 21062112.
de Jong, S., & Tuyls, K. (2011). Human-inspired computational fairness. Autonomous Agents
Multi-Agent Systems, 22(1), 103126.
de Jong, S., Tuyls, K., & Verbeeck, K. (2008). Fairness multi-agent systems. Knowledge Engineering Review, 23(2), 153180.
Dutta, P., & Sen, S. (2003). Forming stable partnerships. Cognitive Systems Research, 4(3), 211
221.
Elmalech, A., Sarne, D., & Grosz, B. J. (2015). Problem restructuring better decision making
recurring decision situations. Autonomous Agents Multi-Agent Systems, 29(1), 139.
465

fiROCHLIN & ARNE

Elmalech, A., & Sarne, D. (2012). Evaluating applicability peer-designed agents mechanisms evaluation. Proceedings 2012 IEEE/WIC/ACM International Joint Conferences Web Intelligence Intelligent Agent Technology-Volume 02, pp. 374381.
Endriss, U., Kraus, S., Lang, J., & Wooldridge, M. (2011). Designing incentives boolean games.
Proceedings 10th International Joint Conference Autonomous Agents Multiagent Systems (AAMAS 2011), pp. 7986.
Fotakis, D., Karakostas, G., & Kolliopoulos, S. G. (2010). existence optimal taxes network congestion games heterogeneous users. Proceedings Third international
conference Algorithmic game theory (SAGT10), pp. 162173.
Gatti, J. (1999). Multi-commodity consumer search. Journal Economic Theory, 86(2), 219244.
Grosfeld-Nir, A., Sarne, D., & Spiegler, I. (2009). Modeling search least costly opportunity. European Journal Operational Research, 197(2), 667674.
Hajaj, C., Hazon, N., Sarne, D., & Elmalech, A. (2013). Search more, disclose less. Proceedings
Twenty-Seventh AAAI Conference Artificial Intelligence (AAAI 2013).
Hajaj, C., & Sarne, D. (2014). Strategic information platforms: selective disclosure price
"free". ACM Conference Economics Computation (EC14), pp. 839856.
Hazon, N., Aumann, Y., Kraus, S., & Sarne, D. (2013). Physical search problems probabilistic
knowledge. Artificial Intelligence, 196, 2652.
Hendrix, P., & Sarne, D. (2007). effect mediated partnerships two-sided economic search.
Klusch, M., Hindriks, K., Papazoglou, M., & Sterling, L. (Eds.), Cooperative Information Agents XI (CIA 2007), Vol. 4676 Lecture Notes Computer Science, pp. 224240.
Springer Berlin Heidelberg.
Ito, T., Ochi, H., & Shintani, T. (2002). group-buy protocol based coalition formation
agent-mediated e-commerce. International Journal Computing Information Sciences,
3(1), 1120.
Janssen, M. C. W., Moraga-Gonzalez, J. L., & Wildenbeest, M. R. (2005). Truly costly sequential
search oligopolistic pricing. International Journal Industrial Organization, 23(5-6),
451466.
Kephart, J., & Greenwald, A. (2002). Shopbot economics. Journal Autonomous Agents
Multi-Agent Systems, 5(3), 255287.
Kraus, S., Shehory, O., & Taase, G. (2003). Coalition formation uncertain heterogeneous
information. Proceedings Second International Conference Autonomous Agents
Multi-agent Systems (AAMAS 2003), pp. 18.
Landsberger, M., & Peled, D. (1977). Duration offers, price structure, gain search.
Journal Economic Theory, 16(1), 1737.
Lippman, S., & McCall, J. (1976). economics job search: survey. Economic Inquiry,
14(3), 347368.
Manisterski, E., Sarne, D., & Kraus, S. (2008). Enhancing cooperative search concurrent
interactions. Journal Artificial Intelligence Research (JAIR), 32(1), 136.
466

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Mash, M., Rochlin, I., & Sarne, D. (2012). Join weakest partner, please. Proceedings
2012 IEEE/WIC/ACM International Conference Intelligent Agent Technology (IAT
2012), pp. 1724.
Masters, A. M. (1999). Wage posting two-sided search minimum wage. International
Economic Review, 40(4), 809826.
McKelvey, R., & Palfrey, T. (1992). experimental study centipede game. Econometrica,
60(4), 803836.
McMillan, J., & Rothschild, M. (1994). Search. Proceedings Handbook Game Theory
Economic Applications, pp. 905927.
Morgan, P. (1983). Search optimal sample sizes. Review Economic Studies, 50(4), 659675.
Morgan, P., & Manning, R. (1985). Optimal search. Econometrica, 53(4), 923944.
Nagel, R., & Tang, F. (1998). Experimental results centipede game normal form:
investigation learning. Journal Mathematical Psychology, 42(2-3), 356384.
Nahum, Y., Sarne, D., Das, S., & Shehory, O. (2015). Two-sided search experts. Autonomous
Agents Multi-Agent Systems, 29(3), 364401.
Penn, M., Polukarov, M., & Tennenholtz, M. (2009a). Random order congestion games. Mathematics Operations Research, 34(3), 706725.
Penn, M., Polukarov, M., & Tennenholtz, M. (2009b). Taxed congestion games failures. Annals
Mathematics Artificial Intelligence, 56(2), 133151.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley-Interscience.
Rochlin, I., Aumann, Y., Sarne, D., & Golosman, L. (2014). Efficiency fairness team search
self-interested agents. Proceedings International conference Autonomous
Agents Multi-Agent Systems (AAMAS 2014), pp. 365372.
Rochlin, I., & Sarne, D. (2014a). Constraining information sharing improve cooperative information gathering. Proceedings 13th International Joint Conference Autonomous
Agents Multiagent Systems (AAMAS 2014), pp. 237244.
Rochlin, I., & Sarne, D. (2014b). Utilizing costly coordination multi-agent joint exploration.
Multiagent Grid Systems, 10(1), 2349.
Rochlin, I., Sarne, D., & Laifenfeld, M. (2012). Coordinated exploration shared goal costly
environments. Proceedings ECAI 2012 - 20th European Conference Artificial
Intelligence. Including Prestigious Applications Artificial Intelligence (PAIS-2012) System
Demonstrations Track, pp. 690695.
Rochlin, I., Sarne, D., & Mash, M. (2014). Joint search self-interested agents failure
cooperation enhancers. Artificial Intelligence, 214, 4565.
Rochlin, I., Sarne, D., & Zussman, G. (2011). Sequential multilateral search common goal.
Proceedings 2011 IEEE/WIC/ACM International Conference Intelligent Agent
Technology (IAT 2011), pp. 349356.
Rochlin, I., Sarne, D., & Zussman, G. (2013). Sequential multi-agent exploration common
goal. Web Intelligence Agent Systems, 11(3), 221244.
467

fiROCHLIN & ARNE

Rochlin, I., & Sarne, D. (2013). Information sharing costly communication joint exploration. Proceedings Twenty-Seventh AAAI Conference Artificial Intelligence, pp.
847853.
Rothschild, M. (1974). Searching lowest price distribution prices unknown.
Journal Political Economy, 82(4), 689711.
Saad, W., Han, Z., Debbah, M., & Hjorungnes, A. (2009). Coalitional games distributed collaborative spectrum sensing cognitive radio networks. Proceedings IEEE INFOCOM,
pp. 21142122.
Sarne, D. (2013). Competitive shopbots-mediated markets. ACM Transactions Economics
Computation, 1(3), 17.
Sarne, D., & Arponen, T. (2007). Sequential decision making parallel two-sided economic search.
Proceedings Sixth International Joint Conference Autonomous Agents Multiagent Systems (AAMAS 2007), p. 69.
Sarne, D., & Aumann, Y. (2014). Exploration costs means improving performance
multiagent systems. Annals Mathematics Artificial Intelligence, 72(3-4), 297329.
Sarne, D., & Kraus, S. (2005). Cooperative exploration electronic marketplace. Proceedings Twentieth National Conference Artificial Intelligence Seventeenth
Innovative Applications Artificial Intelligence Conference (AAAI 2005), pp. 158163.
Sarne, D., Manisterski, E., & Kraus, S. (2010). Multi-goal economic search using dynamic search
structures. Autonomous Agents Multi-Agent Systems, 21(1-2), 204236.
Sarne, D., & Kraus, S. (2003). search coalition formation costly environments.
Proceedings Cooperative Information Agents VII, 7th International Workshop, CIA,
pp. 117136.
Sarne, D., & Kraus, S. (2008). Managing parallel inquiries agents two-sided search. Artificial
Intelligence, 172(4-5), 541569.
Selten, R., & Stoecker, R. (1986). End behavior sequences finite prisoners dilemma supergames learning theory approach. Journal Economic Behavior & Organization, 7(1),
4770.
Sen, S. (1996). Reciprocity: foundational principle promoting cooperative behavior among
self-interested agents. Proceedings Second International Conference Multi-Agent
Systems, pp. 322329.
Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation. Artificial
Intelligence, 101(1-2), 165200.
Smith, L. (2011). Frictional matching models. Annual Reviews Economics, 3(1), 319338.
Stone, P., & Kraus, S. (2010). teach teach? decision making uncertainty ad
hoc teams. Proceedings Ninth International Conference Autonomous Agents
Multiagent Systems (AAMAS 2010), pp. 117124.
Tang, Z., Smith, M. D., & Montgomery, A. (2010). impact shopbot use prices
price dispersion: Evidence online book retailing. International Journal Industrial
Organization, 28(6), 579590.
468

fiC ONSTRAINING NFORMATION HARING MPROVE C OOPERATIVE NFORMATION G ATHERING

Tsvetovat, M., Sycara, K., Chen, Y., & Ying, J. (2000). Customer coalitions electronic markets.
Proceedings Agent-Mediated Electronic Commerce III, Current Issues Agent-Based
Electronic Commerce Systems (includes revised papers AMEC 2000 Workshop), pp.
121138.
Waldeck, R. (2008). Search price competition. Journal Economic Behavior Organization, 66(2), 347357.
Walker, J., & Halloran, M. (2004). Rewards sanctions provision public goods
one-shot settings. Experimental Economics, 7(3), 235247.
Yamamoto, J., & Sycara, K. (2001). stable efficient buyer coalition formation scheme
e-marketplaces. Proceedings 5th international conference Autonomous agents
(AGENTS 01), pp. 576583.

469

fiJournal Artificial Intelligence Research 54 (2015) 593-629

Submitted 07/15; published 12/15

Compressing Optimal Paths Run Length Encoding
Ben Strasser

STRASSER @ KIT. EDU

Karlsruhe Institute Technology
Karlsruhe, Germany

Adi Botea

ADIBOTEA @ IE . IBM . COM

IBM Research
Dublin, Ireland

Daniel Harabor

DANIEL . HARABOR @ NICTA . COM . AU

NICTA
Sydney, Australia

Abstract
introduce novel approach Compressed Path Databases, space efficient oracles used
quickly identify first edge shortest path. algorithm achieves query running times
100 nanosecond scale, significantly faster state-of-the-art first-move oracles
literature. Space consumption competitive, due compression approach rearranges
rows columns first-move matrix performs run length encoding (RLE)
contents matrix. One variant implemented system was, convincing margin,
fastest entry 2014 Grid-Based Path Planning Competition.
give first tractability analysis compression scheme used algorithm.
study complexity computing database minimum size general directed undirected
graphs. find cases problem NP-complete. also show that, graphs
decomposed along articulation points, problem decomposed independent
parts, corresponding reduction level difficulty. particular, leads simple
tractable algorithms linear running time yield optimal compression results trees.

1. Introduction
Compressed Path Database (CPD) index-based data-structure graphs used
quickly answer first-move queries. query takes input pair nodes, namely source node
target node t, asks first edge shortest st-path (i.e., path t). CPDs
successfully applied number contexts important AI. instance, Copa (Botea,
2012), CPD-based pathfinding algorithm, one joint winners 2012 edition
Grid-Based Path Planning Competition, shorter GPPC (Sturtevant, 2012b). related algorithm,
MtsCopa, fast method moving target search known partially known terrain (Botea,
Baier, Harabor, & Hernandez, 2013; Baier, Botea, Harabor, & Hernandez, 2014).
Given graph G = (V, E), trivial CPD consists square matrix dimensions
|V | |V |. matrix m, constructed precomputation step, stores cell m[s, t]
identity first edge shortest st-path. call first-move matrix. convention
say rows correspond fixed source nodes columns fixed target nodes.
optimal terms query time O(|V |2 ) space consumption quickly becomes prohibitive
larger graphs. challenge design compact representation trades small increase
query times large decrease space consumption.
c
2015
AI Access Foundation. rights reserved.

fiS TRASSER , B OTEA , & H ARABOR

number different techniques compress first-move matrix suggested
purpose (Sankaranarayanan, Alborzi, & Samet, 2005; Botea, 2011; Botea & Harabor, 2013a).
case objective conserve space grouping together entries share
common source node store first-edge information.
work present Single-Row-Compression (SRC) Multi-Row-Compression
(MRC) indexing algorithms compressing all-pairs shortest paths. 2014s GPPC, SRC outperformed competitors terms query running time. contributions presented article
go three main directions: new approach compressing first-move matrix; experiments
demonstrate advancing state-of-the-art terms response time memory consumption;
thorough theoretical analysis, discussing NP-hardness results islands tractability.
introduce new matrix compression technique based run-length encoding (RLE).
main idea algorithm simple: compute order nodes input graph
assign numeric IDs nodes (e.g., 1 |V |) order. purpose ordering
nodes located close proximity graph small ID difference. ordering
used order rows columns first-move matrix, also computed
preprocessing. Then, apply run-length encoding (RLE) row first-move matrix.
study three types heuristic orderings: graph-cut order, depth-first order input-graph order.
also study two types run-length encoding. first involves straightforward application
algorithm row. second type sophisticated multi-row scheme eliminates
redundancies adjacent RLE-compressed rows. answer first-move queries employ
binary search fragment compressed result.
undertake detailed empirical analysis including comparisons techniques stateof-the-art variants CPDs (Botea, 2012), Hub-Labeling (Delling, Goldberg, Pajor, & Werneck,
2014). Copa recent fast CPD oracle among joint winners 2012
International Grid-Based Path Planning Competition (GPPC). Using variety benchmarks
competition show techniques improve Copa, terms storage query
time. Hub-Labeling technique initially developed speedup queries roads, also
work graphs, gridmaps. Hub-Labeling best knowledge fastest
technique known roads. experiments, show approach leads better query times
Hub-Labeling graphs reasonably compute m.
technique relies all-pairs-shortest-path pre-computation, plays tradeoff
query-response speed, preprocessing time memory required store compressed
path database. Thus, algorithm faster, also requires larger preprocessing time
memory techniques literature. words, memory
preprocessing time available, technique provide state-of-the-art speed performance. hand, larger larger graphs create memory preprocessing time
bottleneck, techniques considered. See detailed comparison experiments
section.
theoretical analysis, formally define study optimal RLE-compression first-move
matrices produced input graphs. consider case directed input graphs case
undirected weighted input graphs. show versions NP-complete. Focusing
distinct types graphs, result brings something new compared other. Related (Kou,
1977; Oswald & Reinelt, 2009) weaker, less specific (Mohapatra, 2009) results RLE-based
matrix compression available literature. However, known, NP-hardness class
problems necessarily imply NP-hardness subset class. Thus, despite
594

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

previous related results (Mohapatra, 2009), open question whether optimal RLEcompression first-move matrix computed input graph tractable.
also show that, graphs decomposed along articulation points, problem
decomposed independent subproblems. optimal orderings available subproblems, global optimal ordering easily obtained. particular, depth-first preorder
optimal trees, general ordering problem fixed-parameter tractable size
largest 2-connected component.
approach part evaluation previously reported shorter conference
paper (Strasser, Harabor, & Botea, 2014). theoretical analysis topic another conference paper (Botea, Strasser, & Harabor, 2015). Putting together current submission
provides unique source describes method, performance theoretical properties.
Compared previous conference papers, provide complete proofs theoretical
results. included details examples presentation, better clarity.
report additional results, performance pathfinding competition GPPC 2014,
originally published paper competition (Sturtevant, Traish, Tulip, Uras,
Koenig, Strasser, Botea, Harabor, & Rabin, 2015).

2. Related Work
Many techniques literature employed order quickly answer first-move queries.
Standard examples include optimal graph search techniques Dijkstras algorithm (Dijkstra,
1959) A* (Hart, Nilsson, & Raphael, 1968). Significant improvements methods
achieved preprocessing input graph, done CPDs, instance. shortest paths
numerous applications various fields, plethora different preprocessing-based algorithms
proposed. overview, refer interested reader recent survey article (Bast,
Delling, Goldberg, MullerHannemann, Pajor, Sanders, Wagner, & Werneck, 2015). common
approach consists adding online pruning rules Dijkstras algorithm, rely data computed preprocessing phase, significantly reducing explored graphs size. approach
significantly differs technique described paper, omit details refer
interested reader aforementioned survey article.
SILC (Sankaranarayanan et al., 2005) Copa (Botea & Harabor, 2013a) CPD-based techniques fast first-move computation. SILC employs recursive quad-tree mechanism compression Copa uses simpler effective (Botea, 2011) decomposition rectangles.
Hub Labels (HL) initially introduced 2-Hop Labels (Cohen, Halperin, Kaplan, & Zwick,
2002). nearly decade much research topic, Abraham, Delling,
Goldberg, Werneck (2011) showed technique practical huge road networks,
coined term Hub Labels. realization drastically increased interest HL thus
spawned numerous follow works, (Abraham, Delling, Goldberg, & Werneck, 2012;
Delling, Goldberg, & Werneck, 2013; Abraham, Delling, Fiat, Goldberg, & Werneck, 2012; Akiba,
Iwata, & Yoshida, 2013). context, relevant one probably RXL (Delling et al.,
2014), HL variant. authors show algorithm works well road
graphs variety graphs different sources including graphs derived maps used
GPPC. compare algorithm RXL.
HL index consists forward backward label node, contains list hub
nodes exact distances them. st-pair must exist meeting hub h
595

fiS TRASSER , B OTEA , & H ARABOR

forward hub backward hub shortest st-path. shortest distance query
node node answered enumerating common hubs t. labeling
good labels contain hubs. Computing labeling minimizing index size
NP-hard (Babenko, Goldberg, Kaplan, Savchenko, & Weller, 2015).
works consider HL general form, consider restrictive variant
called Hierarchical Hub Labels (HHL). term introduced Abraham et al. (2012)
labels used previous work (Abraham et al., 2011) already hierarchical. labeling called
hierarchical ordering vertices exists, every hub h vertex v comes v
order. Given fixed node order, optimal labeling computed efficiently (Abraham
et al., 2012). difficult task HHL consists computing node order. Computing node
order minimizing index size also NP-hard task (Babenko et al., 2015).
HHL deeply coupled different popular speedup technique shortest path computations called Contraction Hierarchies (CH) (Geisberger, Sanders, Schultes, & Delling, 2008). CH
achieve query speeds HHL significantly smaller index sizes. However,
applications even CH query times already faster necessary, makes CH
strong competitor. CH iteratively contracts nodes inserting shortcuts maintain shortest
path distances remaining graph. following inserted shortcuts small fraction
graph needs explored every node. node order good CH search
spaces every node small. Again, computing optimal order NP-hard (Bauer, Columbus,
Katz, Krug, & Wagner, 2010). first HL paper road graphs (Abraham et al., 2011) computed
label v explicitly storing nodes reachable v CH search space applying pruning rules. Later papers refined rules, every hierarchical label
viewed explicitly stored pruned CH search space. consequence node orders
good CH also good HHL vice versa, even though formal optimization
criteria differ therefore optimal order one respect criterion
slightly suboptimal other.
node orders used HHL original CH depend weights input graph.
Substantial changes weights requires recomputing node ordering. recent work
(Bauer, Columbus, Rutter, & Wagner, 2013; Dibbelt, Strasser, & Wagner, 2014) introduced
Customizable Contraction Hierarchies (CCH) shown node orders exist work well
depend structure input graph. node orders exploit input graph
small balanced node-separators comparative small treewidth.
paper also consider two types node orders. first depth first search preorder
second based small balanced edge-cuts. thus also independent input
graphs weights. However, confuse orders CCH node orders.
interchangeable. Using CCH ordering result bad performance technique,
using one node orders CCH work well. fact, using preorder CCH
maximizes maximum search space terms vertices instead minimizing it. is,
order works well technique CCH worst case node order. Further, orders
also interchanged weight-dependent orders needed HHL CH.
described literature, HL answers distance queries. However, hinted Abraham
et al. (2012), easy extend hub labels first move queries. achieve this, entries
forward backward labels extended third component: first move edge ID. h
forward hub corresponding entry extended using first edge ID shortest
sh-path. h backward hub entry extended first edge shortest ht-path.
596

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

st-query first corresponding meeting hub h determined. 6= h first move
edge ID stored forward label otherwise first move contained backward
label t. slightly increases memory consumption negligible impact
performance. Note distance values needed even one wishes compute first-moves
need distances determine right hub several hubs common.
context program analysis sometimes desirable construct oracle determines particular section code ever reached. PWAH (van Schaik & de Moor, 2011)
one example. Similarly work, authors precompute quadratic matrix employ
compression scheme based run-length encoding. main difference reachability
oracles return yes-no answer every query rather identity first-edge.
Another speedup technique low average query times Transit Node Routing (TNR) (Bast,
Funke, & Matijevic, 2009; Bast, Funke, Matijevic, Sanders, & Schultes, 2007; Antsfeld, Harabor,
Kilby, & Walsh, 2012). However, two independent studies (Abraham et al., 2011; Arz, Luxen, &
Sanders, 2013) come conclusion (at least roads) TNR dominated HL
terms query time. Further, TNR optimize short range queries. scenario often
arises unit chases another unit. situations units tend close,
results many short range queries. TNR rather ineffective scenario.
Bulitko, Bjornsson, Lawrence (2010) present subgoal-based approach pathfinding. Similarities work include preprocessing stage paths map precomputed,
results compressed stored database. database used speed
response time path query posed system. substantial differences
two approaches well. method precomputes all-pairs shortest paths, eliminating graph
search entirely production mode (i.e., stage system queried provide full
shortest paths fragments shortest paths). contrast, Bulitko et al. restrict precomputed
database subset nodes, turn requires additional search production mode.
compression method different case. system provides optimal paths,
guaranteed case Bulitko et al.s method. Besides Bulitko et al. (2010) work, pathfinding
sub-goals turned popular successful idea recent work (Hernandez &
Baier, 2011; Bulitko, Rayner, & Lawrence, 2012; Lawrence & Bulitko, 2013; Uras, Koenig, &
Hernandez, 2013).
Pattern databases (PDBs) (Culberson & Schaeffer, 1998) lookup tables provide heuristic
estimations true distance search node goal state. obtained abstracting
original search space smaller space. Optimal distances abstracted space, every
state pre-established goal, precomputed stored pattern database estimations
distances original space. such, techniques memory-based enhancements
problems solution represented path graph. several key
distinctions PDBs CPDs. PDBs lossy abstractions, specific goal
subset goals. CPDs lossless compressions, encode shortest paths every starttarget
pair. Given lossy nature, PDBs need used heuristic within search algorithm,
example A*, opposed complete optimal method own. PDBs commonly
used large graphs, implicitly defined search spaces, exploring entire graph
preprocessing impractical. PDBs, coarseness abstraction impacts accurracy
heuristic estimations. finer abstraction better quality, also result larger PDB.
Work addressing bottleneck include compressing pattern databases (Felner, Korf, Meshulam,
597

fiS TRASSER , B OTEA , & H ARABOR

& Holte, 2007; Samadi, Siabani, Felner, & Holte, 2008). contrast, CPDs compress all-pairs
shortest paths.

3. Preliminaries
denote G = (V, E) graph node set V edge1 set E V V . denote
deg(u) number outgoing edges u.2 maximum out-degree denoted . node
order : V [1, |V |] assigns every node v unique node ID o(v). out-going edges every
node ordered arbitrary fixed order position (index ordering) referred
out-edge ID.
Further, weight function w : E R>0 3 . st-path sequence edges a1 . . . ak
a1 starts ak ends tP
every edge ai ends node ai+1
starts. weight (or cost) path w(ai ). st-path shortest st-path exists
strictly smaller weight. distance two nodes weight shortest
st-path, one exists. st-path exists, distance . Notice may multiple
shortest st-paths weight.
Without loss generality assume duplicate edges (multi-edges) exist
graphs, were, could drop shortest edge, edges used
shortest path. Further, using similar argument, assume without loss generality
reflexive loops exist.
6= t, st-first-move first edge shortest st-path. multiple shortest
st-paths, may also multiple st-first-moves. st-path exists, st-first-move exists.
formal problem consider following: Given pair nodes t, find st-first-move.
several valid first-moves, algorithm freely choose return.
Given oracle answers first move queries, easily extract shortest paths. Compute
st-first move a. words, first edge shortest path. Next, set end a.
long 6= t, apply procedure iteratively. Notice, works edge weights
guaranteed non-zero. allowed zero-weights, could run infinite-loop problem,
following example illustrates: Consider graph G two nodes x connected edges
xy yx weights zero. Denote node G. valid xt-first-move using xy.
valid yt-first-move using yx. oracle always returned two first-moves,
path extraction algorithm would oscillate x would terminate.
depth first search (DFS) way traversing graph constructing special sort
spanning tree using backtracking. depth-first preorder node order orders nodes
way DFS first sees them. search parameterized root node order
neighbors node visited. work regularly refer depthfirst preorders without stating parameters. always implicitly assume root
arbitrary node neighbors visited arbitrary order.
1. term arc also used literature. Sometimes, distinction made whether graph directed (in
case authors prefer say arcs) undirected. paper, stick term edge cases.
2. directed graph, every ordered pair (u, v) E outgoing edge u. undirected graph, every edge
incident u outgoing edge u.
3. assume function E R>0 able apply Dijkstras algorithm preprocessing phase.
However, one could consider arbitrary weights without negative cycles replace every occurrence Dijkstras
algorithm algorithm Bellman Ford (Bellman, 1958; Ford, 1956).

598

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

Run length encoding (RLE) compresses string symbols representing compactly
substrings, called runs, consisting repetitions symbol. instance, string aabbbaaa
three runs, namely aa, bbb, aaa. run replaced pair contains start
value run. start index first element substring, whereas value
symbol contained substring. example, first run aa start 1 value a.
Run bbb start 3 value b, whereas last run start 6 value a.4
first last run value, need encode both. first
run easily reconstructed constant time case. First, decide whether first run
removed not, done checking first run among preserved ones
start equal 1. Secondly, needed, reconstruct first run, using 1 start position value
equal value last encoded run. Another way looking that, first
last run value, allow merge, wrapped around string
form cycle. allow this, say using cyclic runs. Otherwise (never consider
merging ends string), say use sequential runs. See Example 1 below.
Given ordered sequence elements (string), say two positions are: adjacent
next other; cyclic-adjacent adjacent one first last
position ordering; separated otherwise.
Let ordered sequence elements (symbols) dictionary (or alphabet) . Given
symbol , let -run RLE run containing symbol . every string , denote
N () total number occurrences symbol . Further, number sequential -runs
denoted Rs () number cyclic Rc (). Notice 0 Rs ()Rc () 1.
words, number sequential runs number cyclic runs never differ
1. Finally, denote Rs () total number sequential runs Rc () total number
cyclic runs. paper, assume first-move compression uses cyclic runs, unless
explicitly say otherwise.
Example 1. Consider string = aabbbaaa. Compressing yields 1, a; 3, b; 6, a.
means position 1 string consists as. Similarily position 3 bs
finally position 6 elements string ends. Na () = 5 Nb () = 3.
three sequential runs, namely aa, bbb aaa. first third ones a-runs,
whereas middle one b-run. Thus, Ras () = 2, Rbs () = 1, Rs () = 2 + 1 = 3.
time, one cyclic a-run. Indeed, put next two ends
string, string cyclic, occurrences string become one solid block (i.e.,
one cyclic a-run). Thus, Rac () = 1, Rbc () = 1, Rc () = 1 + 1 = 2.

4. Basic Idea
mentioned introduction, algorithm starts building |V | |V | all-pairs first-move
matrix m. entry position m[i, j] ij-first-move. central idea algorithm
compress row using RLE. compression performed gradually, matrix
rows computed, uncompressed matrix kept memory.
answer st-first-move query, run binary search row s. However, achieve
good compression ratio, first reorder columns decrease total number runs.
columns correspond nodes, regard problem reordering columns problem
4. Alternative encodings exist, value followed run length. E.g., a, 2; b, 3; a, 3 example.

599

fiS TRASSER , B OTEA , & H ARABOR

1
b,5

a,2
e,3
3

c,3
5

d,6
(a) Input

2
f ,4
4


12345
1a
2 ae f e
3 e ed c
4 f f dd
5 c c c c
(b) First-Move Matrix

1
2
3
4
5

1/a
1/a 3/e 4/f 5/e
1/e 4/d 5/c
1/f 3/d
1/c

(c) Compressed Path Database

Figure 1: toy example algorithm
computing good node order. Computing optimal node order minimizes number
runs NP-hard, show theoretical analysis. Fortunately, simple depth-first preorder
works well practice.
Sometimes, formal analysis, technical details annoying sense
make presentation somewhat complicated. question symbol
use m[i, i] example. practical implementation, say care
symbol, never query it. reduce number runs therefore assign either
value m[i 1, i] m[i + 1, i]. theoretical analysis, make similar assumption (i.e.,
dont care symbol) Sections 5 6. state Section 7, assumption
m[i, i] symbol different edge symbol. every case, assumptions
purpose keeping analysis simple possible.
Example 2. Figure 1a shows toy weighted undirected graph, 5 nodes 6 edges.
edge, show weight (cost), number, unique label, letter. first-move
matrix graph, corresponding node ordering 1, 2, 3, 4, 5, shown Figure 1b.
Recall entry m[r, c], r row c column, id first move
shortest path node r node c. example, m[3, 1] = e e first step ea,
optimal path node 3 node 1. Another optimal path would single-step path b,
ea b optimal weight (cost) 5. Thus, free choose m[3, 1] = e
m[3, 1] = b. prefer e leads better compression row 3 m, since
first two symbols third row, identical, part RLE run. show
Section 9 breaking ties optimal way feasible computationally easy.
compression given node ordering (or equivalently, matrix column ordering) shown
Figure 1c.
Notice ordering nodes impacts size compressed matrix. Example 2,
swapping nodes 3 4, illustrated Figure 2, would reduce number RLE runs
row 2, two e symbols become adjacent. total number runs decreases 11
runs 10 runs. Thus, challenge find optimal least good enough node ordering,
objective function size compressed first-move matrix.
compression strategy RLE illustrated Example 2 key component approach. study theoretically next three sections, showing computing optimal
node ordering NP-hard general, identifying tractability islands. present number
effective heuristic node orderings Section 8. variant implemented method, called SRC,
600

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

1
b,5

a,2
e,3
4

c,3
5

d,6
(a) Input

2
f ,4
3


12345
1a
2 af e e
3 f f
4 e e dc
5 c c c c
(b) First-Move Matrix

1
2
3
4
5

1/a
1/a 3/f 4/e
1/f 3/d
1/e 3/d 4/c
1/c

(c) Compressed Path Database

Figure 2: toy example Figure 1 different node ordering (i.e., nodes 3
4 swapped).
performs compression illustrated example. Another version program, called
MRC, goes beyond idea compressing row independently, implementing multi-row
compression strategy. discussed Section 9 evaluated empirically Section 12.

5. First-Move Compression Directed Graphs
Recall ordering columns first-move matrix affects number RLE runs
matrix. section show obtaining optimal ordering intractable general
input graph directed. construction works uniform edge weights. simplicitly
therefore omit weights section.
Definition 1. FMComp-d (First Move CompressionDirected) problem:
Input: directed graph G = (V, E); matrix size |V | |V | cell m[i, j] encodes
first move optimal path node node j; integer k.
Question: ordering columns that, apply RLE row,
total number cyclic RLE runs summed rows k?
Theorem 1. FMComp-d problem NP-complete.
Proof. easy see problem belongs NP, solution guessed verified
polynomial time.
NP-hardness shown reduction Hamiltonian Path Problem (HPP)
undirected graph. Let GH = (VH , EH ) arbitrary undirected graph, define n = |VH |
e = |EH |. Starting GH , build instance FMComp-d problem. According
Definition 1, instance includes directed graph, call GF , first-move matrix
GF , number.
GF = (VF , EF ) defined follows. node u VH , define node VF . call
nodes VF type-n nodes, indicate created original nodes VH .
edge (u, v) EH , define new node nuv VF (type-e nodes). new node nuv , define
two edges EF , one nuv u one nuv v. edges EF . See
Figure 3 example.
Table 1 shows first-move matrix running example. Given type-n node u,
nodes unreachable u graph GF . Thus, matrix row corresponding u
601

fiS TRASSER , B OTEA , & H ARABOR

nxy



x

x



w

z

nxw
w

z

nwz
Figure 3: Left: sample graph GH . Right: GF built GH . GF , x, y, w, z type-n nodes.
Nodes nij type e.

x

w
z
nxy
nxw
nwz

x
2
2
2
0
0
2


2
2
2
1
2
2

w
2
2
2
2
1
0

z
2
2
2
2
2
1

nxy
2
2
2
2
2
2

nxw
2
2
2
2
2
2

nwz
2
2
2
2
2
2
-

Nr. cyclic runs
1
1
1
1
3
4
3

Table 1: First-move matrix running example. rows columns follow node
ordering x, y, w, z, nxy , nxw , nwz .
one non-trivial symbol,5 chose symbol 2, denotes node
reachable. rows one RLE run each, regardless node ordering.
matrix row corresponding type-e node nuv three distinct (non-trivial) symbols total:
one symbol edge node u, another symbol edge node v, non-reachable
symbol 2 every node. Without generality loss, use symbol 0 edge u,
symbol 1 edge v. easy see that, nodes u v cyclic-adjacent given
ordering, nuv row 3 RLE runs. u v separated, row 4 RLE runs.
See Table 1 sample orderings.
claim HPP solution iff FMComp-d solution 4e + 1 RLE runs. Let
vi1 , vi2 . . . , vin solution HPP (i.e., Hamiltonian path GH ), let P EH set
edges included solution. show node ordering VF starting vi1 , . . . , vin ,
followed type-e nodes arbitrary order, result 4e+1 = 3(n1)+4(en+1)+n
runs, 3n 3 runs total type-e rows6 corresponding edges P ; 4(e n + 1) runs
total remaining type-e rows; n runs total type-n rows.
5. trivial symbol mean dont care symbol . Recall impact number runs.
simplicity, safely ignore symbol discussion.
6. say row type-n (or type-e) iff associated node type.

602

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

Indeed, edge (u, v) P , type-e row corresponding node nuv VF
3 RLE runs, since u v adjacent ordering. n 1 edges Hamiltonian
path, total number RLE runs 3(n 1) rows.
edge (u, v)
/ P , two nodes separated therefore corresponding matrix row
4 runs. sums 4(e n + 1) RLE runs rows corresponding edges
included Hamiltonian path.
Conversely, consider node ordering creates 4e + 1 = 3(n 1) + 4(e n + 1) + n RLE
runs total. show ordering type-n nodes contiguous block,7
ordering Hamiltonian path GH . equivalent saying exist n 1 pairs
type-n nodes u v u v cyclic-adjacent ordering, (u, v) EH .
proof contradiction. Assume p < n 1 pairs type-n nodes u v
u v cyclic-adjacent ordering, (u, v) edge EH .
p pairs, row corresponding type-e node nuv 3 RLE runs. remaining e p
type-e rows 4 RLE runs each. mentioned earlier, type-n rows n runs total,
regardless ordering. Thus, total number RLE runs 3p + 4(e p) + n = 4e p + n >
4e (n 1) + n = 4e + 1. Contradiction.

6. Compression Undirected Weighted Graphs
turn attention undirected weighted graphs, showing computing optimal ordering
NP-complete.
Definition 2. FMComp-uw problem (First Move CompressionUndirected, Weighted) defined follows.
Input: undirected weighted graph G = (V, E); matrix size |V | |V | cell m[i, j]
stores first move optimal path node node j; integer k.
Question: ordering ms columns that, apply run length encoding (RLE)
row, total number cyclic RLE runs matrix k?
stepping stone proving NP-hardness FMComp-uw, introduce problem
call SimMini1Runs (Definition 3), prove NP-completeness. SimMini1Runs inspired
work Oswald Reinelt (2009), studied complexity problem involving
so-called k-augmented simultaneous consecutive ones property (C1Sk ) 0/1 matrix (i.e.,
matrix two symbols, 0 1). definition, 0/1 matrix C1Sk property if,
replacing k 1s 0s, columns rows matrix ordered that,
row column, 1s row column come one contiguous block.
Oswald Reinelt (2009) proven checking whether 0/1 matrix C1Sk property
NP-complete. proof SimMini1Runs related, point later proof.
Given 0/1 matrix o, ordering columns, ordering rows, let global
sequential 1-runs count Gs1 (o) number sequential 1-runs summed rows
columns. is,
X
Gs1 (o) =
R1s (),


7. Here, notion contiguous block allows case part block end sequence,
part beginning, sequence cyclic.

603

fiS TRASSER , B OTEA , & H ARABOR

o=

r1
r2



c1

c2

c3

0
1

1
0

1
1



Figure 4: Running example 0/1 matrix o. Rows labelled ri , whereas cj represent column
labels.
iterated os rows columns. instance, Gs1 (o) = 6 matrix shown
Figure 4.
Definition 3. Simultaneous Mini 1-Runs (SimMini1Runs) problem defined follows.
Input: 0/1 matrix every row column contain least one value 1; integer k.
Question: ordering columns, ordering rows, Gs1 (o) k?
Theorem 2. SimMini1Runs NP-complete.
proof available Appendix A.
Lemma 1. Let 0/1 string starts 0, ends 0, both.
R1s () = R0c ().
Proof. Case (i): starts 0 ends 1. two end symbols different, sequential
runs cyclic runs identical. 0-runs 1-runs alternate, numbers identical. Case
(ii), starts 1 ends 0, similar previous one.
Case (iii): 0 ends. 0-runs 1-runs alternate, 0-runs
ends, follows R1s () = R0s () 1 = R0c ().
Theorem 3. FMComp-uw NP-complete.
Proof. NP-hardness shown reduction SimMini1Runs. Consider arbitrary
SimMini1Runs instance rows n columns. Figure 4 shows running example.
build undirected weighted graph G = (V, E) follows. V 3 types nodes, total
+ n + 1 nodes. column generates one node V . call c-nodes. row
generates one node well (r-nodes). extra node p called hub node.
One r-node ri one c-node cj connected unit-cost edge iff o[ri , cj ] = 1.
addition, edge weight 0.75 p every node. edges
exist graph G. See Figure 5 example.
Let first-move matrix G. row p fixed number runs, namely + n,8
regardless ordering ms columns. Let v c-node r-node. Apart vs adjacent
nodes, nodes reached shortest path cost 1.5 whose first move edge
(v, p). matrix running example shown Figure 6.
Let T1 total number occurrences symbol 1 matrix o. claim
ordering os rows columns results k sequential 1-runs (summed rows
columns) iff ordering columns resulting k + 2T1 + + n
8. Recall ignore dont care symbol m[p, p] = , impact number RLE runs.

604

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

p

c1

c2

c3

r1

r2

Figure 5: Graph running example. Dashed edges weight .75, whereas solid lines
unit-cost edges.

r1

m=

r1



r2









c1
c2
c3
p

r2

c1

c2

c3

p

0 0 1 2 0
0 1 0 2 0
0 1 0 0 0
1 0 0 0 0
1 2 0 0 0
1 2 3 4 5










Figure 6: first-move matrix running example. Without generality loss, 0 move
towards p. incident edges given node counted starting 1.
cyclic RLE runs total (summed rows). Thus rows m, except ps row,
k + 2T1 runs total.
Let ri1 , . . . rim cj1 , . . . cjn row column orderings result k
sequential RLE runs rows columns. show ordering ri1 , . . . rim , cj1 , . . . cjn , p
ms columns generates k + 2T1 + + n cyclic runs. Clearly, every row column
o, corresponding row 0 (see Figures 4 6 example). According
steps explained earlier illustrated Figures 4 6, 0 obtained follows.
original 0s preserved. original 1s replaced distinct consecutive integers starting
1. addition, 0 padded 0s one ends. Since 0 0s one
ends, follows R1s () = R0c ( 0 ).9 follows Rc ( 0 ) = R0c ( 0 )+N1 () = R1s ()+N1 ().
Summing Rc ( 0 ) rows 0 m, except ps row, obtain
X
0 (m)\{p}

Rc ( 0 ) =

X

R1s () +

(o)

X
(o)

N1 () k + 2T1 ,

denotes set rows matrix, set columns, = . follows
ms rows k + 2T1 + + n cyclic RLE runs total (that is, summed rows).
Conversely, assume ordering ms columns k + 2T1 + + n cyclic RLE runs
total (for rows). means summing runs rows m, except node ps
row, results k + 2T1 runs. exactly 2T1 distinct runs different 0-runs,
9. R1s () = R0c () Lemma 1, R0c () = R0c ( 0 ) construction.

605

fiS TRASSER , B OTEA , & H ARABOR

follows k 0-runs total:
X
0 (m)\{p}

R0c ( 0 ) k.

Let ri1 , . . . rim , cj1 , . . . cjn , p re-arragement ms columns that: r-nodes come
one contiguous block, relative ordering preserved; c-nodes one contiguous block,
relative ordering preserved.
Since G restricted c-nodes r-nodes bi-partite, rearrangement cannot possibly increase number RLE runs. (If anything, could eliminate 0-runs). hard
prove. example, current matrix row corresponds r-node source node,
m[a, b] = 0 every r-node b, since a, p, b optimal path b. Also,
m[a, p] = 0. rearrangement moves nodes b block cyclic-adjacent p,
create new run. case c-node source similar.
order os columns cj1 , . . . cjn , os rows ri1 , . . . rim . orderings,
relation row column corresponding row 0 follows.
non-zero values 0 converted 1s . 0 0s one ends cut
away . Since 0 contains 0s one ends, R1s () = R0c ( 0 ), according Lemma 1.
follows
X
X
R1s () =
R0c ( 0 ) k.
(o)(o)

0 (m)\{p}

7. Fighting Complexity Decomposition
far results negative. shown computing optimal order large
class graphs NP-hard. section identify tractability islands. show problem
decomposed along articulation points (which related cuts size 1). particular,
implies (as shown section) depth-first preorder optimal node ordering trees.
able construct optimal orders efficiently broader class graphs trees:
show problem fixed-parameter tractable size largest component
graph articulation points.
Definition 4. say node x graph G articulation point removing x adjacent edges G would split graph two disjoint connected subgraphs G1 . . . Gn .
Figure 7 shows example. rest section focus graphs G articulation
points x. consider cyclic runs. previous sections, treated m[s, s] dont care symbol,
impact number runs. section, make different assumption. Every cell
m[s, s] gets distinct symbol, called s-singleton, always creates run,
merged adjacent symbols common run. makes proofs easier
clearly significant impact number runs.
Definition 5. call x-block ordering node ordering x comes first, nodes G1
come next contiguous block, way block Gn .
606

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

Figure 7: graph articulation point x. Removing x would decompose graph four
disjoint components, depicted G1 G4 .

example shown Figure 7, ordering = x, a, b, c, d, e, f, g example
x-block ordering.
use o|G0 denote projection node ordering subset nodes corresponding
subgraph G0 G. use denote subgraph induced nodes Gi {x}.10
say order rotation another order o0 obtained o0 taking block
o0 elements beginning appending end. instance, d, e, f, g, x, a, b, c
rotation x, a, b, c, d, e, f, g. formally, rotation o0 two sub-orders
exist o0 = , = , .
Lemma 2. Let x articulation point graph G. Every node order rearranged
x-block ordering o0 without increasing number runs row.
Given graph G, node ordering row subset S, let N (o, G, S) number runs
restricted subset S. Clearly, N (o, G, G) total number runs.
Lemma 3. Given x-block ordering o, that:
1. N (o, G, Gi ) = N (o|i , , Gi );
P
2. N (o, G, {x}) = 1 n + N (o|i , , {x});
P
3. N (o, G, G) = 1 n + N (o|i , , ).
proofs Lemmas 2 3 available Appendix B.
Theorem 4. Given optimal order oi every subgraph induced , construct
optimal global ordering G following. Obtain new orderings o0i rotating oi x
comes first, removing x. Then, = x, o01 , . . . , o0n optimal.
Proof. show, contradiction, global ordering optimal. Notice o|i optimal
. Assume strictly better ordering o0 . According Lemma 2, exists x-block
10. subgraph induced subset nodes contains nodes edges whose ends belong S.

607

fiS TRASSER , B OTEA , & H ARABOR

ordering o00 least good o0 .
N (o, G, G) = 1 n +

X

1n+

X

N (o|i , , )





N (o00 |i , , )

= N (o00 , G, G) N (o0 , G, G)
contradiction o0 strictly better (i.e., N (o0 , G, G) < N (o, G, G)).
Lemma 4. G tree depth-first preorder G (with arbitrary root) rotated
x-block order every node x.
Proof. Every preorder induces rooted tree. respect root every node x (except root)
parent p possibly empty sequence direct children c1 . . . cn ordered way
depth-first search visited them. removing x, G decomposed subgraphs Gp ,
Gc1 . . . Gcn . x root Gp empty graph. order following structure:
nodes Gp , x, nodes Gc1 . . . nodes Gcn , remaining nodes Gp . Clearly
rotated x-block ordering.
Theorem 5. G = (V, E) tree depth-first preorder G N (o, G, G) = 3|V | 2.
Proof. direct consequence Lemma 4 every node v many runs d(v) + 1,
d(v) degree node. +1 comes v-singleton. thus
N (o, G, G) =

X
vV

(d(v) + 1) = 2|E| + |V | = 3|V | 2.

Theorem 6. Computing optimal order graph G fixed-parameter tractable size
largest two-connected component G (i.e., largest component articulation points).
Proof. Recursively decompose G articulation points two-connected parts left.
size parts depend size G enumerate orders pick
best one. Given optimal orders every part use Theorem 4 construct optimal global
order.
able decompose graphs along articulation points useful real-world road networks.
graphs tend large two-connected component many small trees attached. example Europe graph made available 9th DIMACS challenge (Demetrescu, Goldberg,
& Johnson, 2009) 18M nodes total 11.8M within largest twoconnected component. result allows us position 6.2M nodes order fast optimally
using local information.
608

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

8. Heuristic Node Orderings
Sections 5 6 shown computing optimal order NP-hard theory. Fortunately, NP-hardness rule existence good heuristic orderings
computed quickly. Indeed, simple depth-first preorder works well practice. observation partially explained fact that, shown Section 7, depth-first preorder
optimal trees. However, also explain using informal intuitive terms.
ordering good neighboring nodes graph assigned neighboring IDs.
consistent previous observation (Sankaranarayanan et al., 2005; Botea, 2011) that, two
target nodes close other, chances first move current node towards
targets same. depth-first preorder achieves goal assigning close IDs
neighboring nodes low degree graphs. node either interior node, root, leaf
DFS tree. nodes graph tend interior nodes. these, depth-first preorder
assign two neighboring nodes adjacent IDs. Denote v internal node, p parent
c first child v. ID p ID v minus 1, whereas ID c ID
v plus one. guarantee nothing children. However, average node degree
low, case example road graphs, many children.
Besides using depth-first preorders, also propose another heuristic based intuition assigning close IDs close nodes. based cuts. formulated intuitive
optimization criterion also formulated following: every edge, endpoints
close ID. Obviously fulfilled edges once. reason proposed ordering tries identify small set edges property may violated.
using balanced edge cuts. Given graph n nodes want assign IDs range [1, n]
using recursive bisection. first step algorithm bisects graph two parts nearly
equal node counts small edge cut size. divides ID range middle assigns
lower IDs one part upper IDs part. continues recursively bisecting
parts dividing associated ID ranges parts constant size left.
described far algorithm free decide part assigns lower
upper ID ranges. reason augment tracking every node v two counters h(v)
`(v) representing number neighbors guaranteed higher lower IDs. Initially
counters zero. every bisection ranges assigned algorithm iterates
edge cut increasing counters border nodes. deciding two parts p q
gets ranges uses counters estimate ID distance parts nodes around
them. evaluates
X
X
X
X
h(v)
`(v) <
h(v)
`(v)
vq

vq

vp

vp

assigns higher IDs p condition holds. algorithm encounters part
small bisected assigns IDs ordered `(v) h(v).

9. Compression
Let a1 . . . denote uncompressed row first-move matrix. stated previously, SRC
compresses list runs ordered start. compressed rows vary size, need
additional index array maps source node onto memory offset first run
609

fiS TRASSER , B OTEA , & H ARABOR

row corresponding s. arrange rows consecutively memory therefore end ss
row also start + 1s row. therefore need store row ends.
9.1 Memory Consumption
required node IDs encodable 28 bits out-edge IDs 4 bits. encode runs
start upper 28 bits 32-bit machine word value lower 4 bits. total memory
consumption therefore 4 (|V | + 1 + r) bytes r total number runs rows
|V | + 1 number offsets index array. Notice that, implementation, assume
4 bytes per index entry sufficient, equivalent saying r < 232 . formula
easily adapted sizes (i.e., number bits) node IDs, edge IDs, index entries.
instance, sum one node ID one edge ID K bytes, J bytes sufficient
encode index run (in words, number r fits J bytes), formula becomes
J (|V | + 1) + K r bytes.
9.2 Computing Rows
Rows computed individually running variant Dijkstras one-to-all algorithm every
source node compressed described detail Section 9.3. However, depending
graph possible shortest paths unique may differ first edge. therefore
possible multiple valid uncompressed rows exist tie-break paths differently. rows
may also differ number runs therefore different compressed sizes. minimize
compressed size row, instead using Dijkstras algorithm compute one specific row
a1 . . . modify compute sets A1 . . . valid first move edges. require
shortest st-path must exist uses first edge. algorithm maintains alongside
tentative distance array d(t) node set valid first move edges . algorithm
relaxes edge (u, v) decreasing d(v) performs Av Au . d(u) + w(u, v) = d(v)
performs Av Av Au . restricted out-degree node 15 store
sets 16-bit bitfields. Set union performed using bitwise-or operation.
9.3 Compressing Rows Run Length Encoding
every target compression method given set valid first move edges may pick one
minimizes compressed size. formalize subproblem following: Given sequence
sets A1 . . . find sequence a1 . . . ai Ai minimizes number runs.
show subproblem solved optimally using greedy algorithm. algorithm begins
determining longest run
includes a1 .
done scanning A1 . . . Ai Ai+1
intersection empty: j[1,i] Aj 6= j[1,i+1] Aj = . algorithm chooses
value intersection (it matter which) assigns a1 . . . ai . continues
determining longest run starts contains ai+1 way. procedure
iterated rows end reached. approach optimal show
optimal solution longest first run exists. valid solution longer first run.
optimal solution shorter first run transformed increasing first runs length
decreasing second ones without modifying values. subsequences exchanged
without affecting surroundings conclude greedy strategy optimal.
610

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

1
b,5

a,2
e,3
4

c,3
5

d,6

2
f ,4
3

1
2
3
4
5

1/a
1/a 3/f 4/e
1/f 3/d
1/e 3/d 4/c
1/c

(a) Input

(b) SRC

1
2
3
4
5

X
X 3/f 4/e
1/f
1/e 4/c
Z

(c) MRC per row info.

X 1/a
3/d
Z 1/c

(d) MRC per group info.

Figure 8: MRC applied toy graph Figure 2, reproduced convenience, left.
Part (b) illustrates SRC input full runs Rs every row. Part (c) show groups (X,
, Z) row row-specific runs R0 . Finally, part (d) depicts runs R0 g shared
rows group.

9.4 Merging Rows using Groups
compress individual rows exploited shortest paths t1 t2 often
first move t1 t2 close. similar observation made close source nodes
s1 s2 . compressed rows tend resemble other. want compress
data exploiting redundancy. call technique multi-row compression (MRC)
illustrate Figure 8. partition nodes groups store group information
shared nodes group. row store information unique it. Denote
g(s) unique group node s. Two runs different rows start value
32-bit pattern. Denote Rs set runs row s. Instead storing
row
whole set Rs store group h intersection rows. is,
0
store R h = ih Ri . row store R0 = Rs \ Rg(s) . Recall query target
consists finding max{x Rs | x < t0 } (where t0 = 15t + 16). Notice formula
rewritten using basic set logic max{max{x R0 | x < t0 }, max{x R0 g(s) | x < t0 }}
implemented using two binary searches R0 stored ordered arrays. Note
need second index array lookup R0 g groups g.
9.5 Computing Row Groups
design close source nodes close node IDs thus neighbouring rows. motivates
restricting row-run groupings. is, group h rows j
rows [i, j] belong group. optimal row-run grouping computed using
dynamic programming. Denote S(n) maximum number runs saved compared using
group-compression restricted first n rows. Notice S(1) = 0. Given S(1) . . . S(n)
want compute S(n + 1). Obviously n + 1s row must part theT
last group. Suppose
last group length ` save total S(n + 1 `) + (` 1) | i[n+1`,n+1] Ri | runs.
n different values ` enumerate, brute force, possible values,
resulting algorithm running time (n2 ). observe intersection large
groups often seems nearly empty therefore test values ` 100 resulting
(n) heuristic.
611

fiS TRASSER , B OTEA , & H ARABOR

10. Queries
Given source node target node (with 6= t) algorithm determines first edge
shortest st-path. first determining start end compressed row
using index array. runs binary search determine run containing
corresponding out-edge ID. precisely algorithm searches run largest start
still smaller equal t. Recall encode run single 32-bit machine word
higher 28 bits runs start. reinterpret 32-bits unsigned integers.
algorithm consists binary search ordered 32-bit integer largest element
larger 16t + 15 (i.e., higher 28 bits 4 lower bits set).
Extracting path using CPDs extremely simple recursive procedure: beginning start
node extract first move toward target. follow resultant edge neighbouring
node repeat process target reached.

11. Experimental Setup
evaluate work consider two types graphs: road graphs grid-based graphs.
cases assume node IDs encoded within 28-bit integers. assume
15, use distinct value (15) indicate invalid edge. allows us encode
out-edge IDs within 4 bits. Note concatenation node ID out-edge ID fits
single 32-bit machine word.
experiments performed quad core i7-3770 CPU @ 3.40GHz 8MB combined cache, 8GB RAM running Ubuntu 13.10. algorithms compiled using g++ 4.8.1
-O3. reported query times use single core.
11.1 Grid Graphs
chosen three benchmark problem sets drawn real computer games. first two sets
benchmark instances appeared 2012 Grid-Based Path Planning Competition. third
benchmark set consists two worst case maps terms size. two maps available part
Nathan Sturtevants extended problem repository http://movingai.com/benchmarks/
part 2012 competition set.
first benchmark set features 27 maps come game Dragon Age Origins.
maps 16K nodes 119K edges, average.
second benchmark set features 11 maps come game StarCraft.
maps 288K nodes 2.24M edges, average.
third benchmark set comprises two large grids evaluate separately.
largest maps available two games. extended Dragon Age Origins
problem set choose map called ost100d. 137K nodes 1.1M edges.
extended StarCraft problem set choose map called TheFrozenSea.
754K nodes 5.8M edges. Note ost100d, largest Dragon Age
Origins map, smaller average StarCraft map.
grid maps evaluation undirected feature two types
edges: straight edges
weight 1.0 diagonal edges weight 2.
612

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

11.2 Road Graphs
case road graphs chosen several smaller benchmarks made available
9th DIMACS challenge (Demetrescu et al., 2009).
New York City map (henceforth, NY) 264K nodes 730K edges.
San Francisco Bay Area (henceforth, BAY) 321K nodes 800K edges.
Finally, State Colorado (henceforth, COL) 436K nodes 1M edges.
three graphs travel time weights (denoted using -t suffix) geographic distance weights
(denoted using -d) available.
11.3 Comparisons
implemented algorithm two variants: single-row-compression (SRC) using row
merging optimization, multi-row-compression (MRC), using optimization. compare
approaches two recent state-of-the-art methods: Copa (Botea & Harabor, 2013b)
RXL (Delling et al., 2014). evaluate two variants Copa. first variant,
denote Copa-G, appeared 2012 GPPC optimised grid-graphs. use original
C++ implementation available competition repository (Sturtevant, 2012a).
second variant, denote Copa-R, optimised road graphs. algorithm described
(Botea & Harabor, 2013a); used original C++ implementation program version
well.
RXL newest version Hub-Labeling algorithm. asked original authors
run experiments us presented below. experiments carried Xeon E52690 @ 2.90 GHz. compensate lower clock speed, compared test machine,
scale query times RXL factor 2.90/3.40 = 85%. important note
implementation RXL computes path distances instead first-moves. discussed Section 2
make significant difference query times. However unclear us whether
possible incorporate additional data needed first-move computation compression
schemes presented Delling et al. (2014). reported RXL database sizes therefore
regarded lower bounds.

12. Results
evaluate two algorithms (SRC MRC) terms preprocessing time, compression
performance query performance. also study impact range heuristic node orderings
using metrics. three variants, distinguished suffix. suffix +cut
indicates node ordering based balanced edge-separators graph cutting technique described
Section 8. suffix +dfs indicates node ordering based depth-first search traversal,
described Section 8. suffix +input (or shorter +inp) indicates order nodes taken
associated input file. case grid graphs ordered nodes lexicographically, first
y- x-coordinates. applicable compare work state-of-the-art
first-move algorithms Copa-R Copa-G. also compare recent hub labeling
technique known RXL space efficient (but fast) variant called CRXL.
613

fiS TRASSER , B OTEA , & H ARABOR

Benchmark
DIMACS
Dragon Age Origins
StarCraft
ost100d
TheFrozenSea

Average Preprocessing Time (seconds)
Compute Order
Single Row Compression
Multi Row Compression
+cut
+dfs
+input
+cut
+dfs
+input
+cut
+dfs
+input
16
<1
0
1950
1982
2111
1953
1985
2125
2
<1
0
32
35
38
33
36
40
18
<1
0
1979
2181
2539
1993
2195
2574
19
<1
n/m
101
100
n/m
104
105
n/m
110
<1
n/m
3038
3605
n/m
3133
3690
n/m

Table 2: Preprocessing time road grid graphs. give results (i) average time required
compute node ordering; (ii) total time required compute entire database SRC
MRC. Values given nearest second. ost100d TheFrozenSea preprocessing experiments
run AMD Opteron 6172 48 cores @ 2.1 GHz accelerate APSP computation.
experiments smaller graphs clearly show input order fully dominated. therefore omit
numbers two larger test graphs. n/m stands measured.
Graph
|V |

|E|
|V |

CopaG

+cut

Min < 1K
7
Q1
2K 7.2
Med
5K 7.4
Avg 31K 7.4
Q3
52K 7.6
Max 100K 7.7

<1
<1
1
12
18
75

<1
<1
<1
5
6
31

Min
Q1
Med
Avg
Q3
Max

60
128
183
351
510
934

20
28
69
148
189
549

105K
173K
274K
288K
396K
494K

7.7
7.7
7.8
7.8
7.8
7.8

DB Size (MB)
Query Time (nano seconds)
MRC
SRC
MRC
SRC
UM CopaG
+dfs +inp +cut +dfs +inp
+cut +dfs +inp +cut +dfs
Dragon Age: Origins (27 maps)
<1 <1 <1 <1 <1
<1
34
19 26 26
14 19
<1 <1 <1 <1 <1
2
63
22 31 35
16 22
1
1
<1 1
2
12.5
81
30 44 54
20 31
7
23
6
8
53
480.5
156
34 50 72
25 36
10
29
7
12
65
1352
266
36 62 106 28 45
39 106 35 44 349 5000
316
95 116 176 67 78
StarCraft (11 maps)
35
89
25 42 187 5512.5 304
63 93 130 47 63
61 144 33 71 281 14964
324
70 103 142 51 69
111 393 83 126 956 37538
334
95 121 187 66 77
203 444 172 222 983 41472
358
105 130 195 66 82
282 621 222 308 1318 78408
396
126 146 226 72 90
626 1245 630 660 2947 122018 436
197 195 311 108 118

+inp
18
26
38
54
82
138
88
102
133
132
156
208

Table 3: Performance SRC MRC grid graphs. use two problem sets taken 2012
GPPC compare Copa-G, one winners competition. measure (i) size
compressed database (in MB) and; (ii) time needed extract first query (in nanos). values
rounded nearest whole number (either MB nano, respectively). baseline, column UM shows
size naive, non-compressed first-move matrix.

12.1 Preprocessing Time
Table 2 gives average preprocessing time SRC MRC 6 road graphs two
competition sets. time case dominated need compute full APSP table.
previously commented, APSP compression central point work;
APSP-computation. preprocessing approach involves executing Dijkstras algorithm repeatedly
resulting total running time O(n2 log n) sparse graphs non-negative weights; using
modern APSP techniques (e.g., Delling, Goldberg, Nowatzyk, Werneck 2013) succeeded
significantly reducing hidden constants behind big-O able exploit specific
graphs structures (e.g., road graphs) get running time down. However, techniques
give benefit repeatedly running Dijkstras algorithm asymptotic worst-case.
614

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

Graph
Name

|V |

|E|
|V |

BAY-d
BAY-t
COL-d
COL-t
NY-d
NY-t

321K
321K
436K
436K
264K
264K

2.5
2.5
2.4
2.4
2.8
2.8

DB Size (MB)
Query Time (nano seconds)
Copa Hub Labels
MRC
SRC
Copa Hub Labels
MRC
SRC
UM
-R RXL CRXL +cut +dfs +cut +dfs
-R RXL CRXL +cut +dfs +cut +dfs
317 90
19
141 129 160 144 51521 527 488 3133 89 100 62 69
248 65
17
102 95 117 107 51521 469 371 1873 74 87 52 60
586 138 24
228 206 268 240 95048 677 564 3867 125 111 68 85
503 90
22
162 150 192 175 95048 571 390 2131 88 97 58 65
363 99
21
226 207 252 229 34848 617 621 4498 112 122 75 83
342 66
18
192 177 217 198 34848 528 425 2529 98 111 67 75

Table 4: Comparative performance SRC, MRC, Copa-R two recent Hub Labeling algorithms.
also report size UM uncompressed matrix. test one six graphs 9th DIMACS
challenge. measure (i) database sizes (in MB); (ii) time needed extract first query (in nanos).
Values rounded nearest whole number. Graph sizes rounded nearest thousand nodes.

Creating node order fast; +dfs requires fractions second. Even +cut order
requires 18 seconds average using METIS (Karypis & Kumar, 1998). Meanwhile,
difference running times SRC MRC indicate multi-row compression
add small overhead total time. test instances
recorded preprocessing overhead order seconds.
12.2 Compression Query Performance
Table 3 give overview compression query time performance Copa-G
range SRC MRC variants competition benchmark sets. measure query
performance run 108 random queries source target nodes picked uniformly random
average running times.
MRC outperforms SRC terms compression expense query time. Node orders
significantly impact performance SRC MRC. cases +cut yields smaller database
faster queries. SRC MRC using +cut +dfs convincingly outperform Copa-G
majority test maps, terms space consumption query time.
naive, non-compressed first-move matrix impractical due large memory requirements. size uncompressed matrix would 4 |V |2 bits, reflecting assumption
outgoing edge stored 4 bits. Tables 3, 4, 5 specify column UM memory consumption uncompressed matrix. example, ost100d game graph 137K,
non-compressed matrix requires bit 9GB memory, two orders magnitude higher 49MB, respectively 39MB SRC+cut respectively MRC+cut
need. larger graphs, difference striking. example TheFrozenSea, largest
game graph, 754K nodes leads 576MB MRC+cut database compared 284GB noncompressed matrix. hand, smaller graphs matrix would fit memory,
fetching moves would extremely fast, one table lookup per move. comparison,
fetch one move, method performs binary search compressed string whose length
larger, usually much smaller |V |.
Table 4 look performance 6 road graphs compare Copa-R, SRC, MRC,
RXL CRXL. main observations road graphs +dfs leads smaller CPDs
+cut. Surprisingly, lower average row lengths yield faster query times. Copa-R dominated RXL, SRC, MRC. SRC+cut outperforms competitors several factors terms
615

fiS TRASSER , B OTEA , & H ARABOR

Graph
Name
ost100d
FrozenSea

|V |

|E|
|V |

137K
754K

7.7
7.7

DB Size (MB)
MRC
SRC
UM
RXL CRXL +cut +dfs +cut +dfs
62
24
39
50
49
57
9436
429 135
576 634 753 740 284405
Hub Labels

Query Time (nano seconds)
Hub Labels
MRC
SRC
RXL CRXL +cut +dfs +cut +dfs
598 5501
89 110
58
71
814 9411 176 192 104 109

Table 5: Performance SRC MRC large grid graphs Nathan Sturtevants extended repository.
compare Hub Labeling methods RXL, CRXL. also report size UM uncompressed matrix. run tests TheFrozenSea, drawn game StarCraft, ost100d, comes
game Dragon Age Origins. measure (i) database sizes (in MB); (ii) time needed extract
first query (in nanos). Values rounded nearest whole number. RXL & CRXL exploit graphs
undirected SRC & MRC not. directed graphs space consumption RXL would double.

Graph
BAY-d
BAY-t
COL-d
COL-t
NY-d
NY-t
FrozenSea
ost100d

Average Row Label
Length
Space (Bytes)
SRC
SamPG
SRC
SamPG
+cut +dfs + Plain +cut +dfs
51
129 108
816
516 432
34
94
79
544
376 316
59
160 131
944
640 524
35
114
96
560
456 384
70
248 203
1120
992 812
44
214 175
704
856 700
92
260 256
1472
1040 1024
80
91
108
1280
364 432

Table 6: report average number hubs per label (length), number runs per row (length),
average space usage per node SamPG+Plain SRC.

speed. RXL wins terms database size. However factor gained space smaller
factor lost query time compared SRC. CRXL clearly wins terms space two
orders magnitude slower competition. road graphs distance weights harder
travel time weights. already known algorithms exploit similar graph features
RXL. However, interesting seemingly unrelated first-move compression based algorithms
incur penalties.
Table 5 evaluate performance SRC, MRC, RXL CRXL larger game
maps. dropped Copa-R experiments smaller graphs clear
fully dominated. road graphs, space consumption SRC MRC lower
+cut order +dfs. result +cut order clearly superior +dfs game maps.
ost100d SRC MRC beat RXL terms query time space consumption.
TheFrozenSea RXL needs less space SRC MRC. However, note game maps RXL
gains factor 2 exploiting graphs undirected SRC MRC not.
CRXL employs powerful compression techniques specific shortest paths. RXL use
uncompressed uses basic encoding techniques delta encoding.
basic HL variant stores nodes distances explicitly needs memory. refer
basic variant SamPG+Plain. SamPG ordering algorithm used RXL11 Plain refers
11. RXL-paper describes several node orders. However, SamPG order suggest using. RXL
CRXL numbers paper use SamPG.

616

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

1
b,5

a,2
e,3

2
f ,4

4
c,3
5

d,6

3

(a) Input Graph

(b) Computing rectangles

(c) List rectangles

Figure 9: Copas rectangle decomposition toy example Figures 2 8 first
moves source node 2. Similar decompositions needed every source node.

elementary HL encoding 4 bytes per hub ID 4 bytes per distance value. want
comparse SRC SamPG+Plain. therefore report Table 6 average number hubs
per label average number runs per row using SRC. reported number hubs per
label. Note directed graphs every node needs two labels: forward backward label.
undirected graphs two labels coincide one stored. contrasts
SRC cannot exploit input graph undirected. numbers table therefore
assume two HL-labels needed per node better comparability. HL need store
32-bit distance value, 28-bit node ID 4-bit out-edge ID times 2
two labels per node. total space consumption thus 16h bytes h average
number hubs per label. SRC need store 28-bit node ID 4-bit out-edge ID per
run. results 4r bytes r average number runs per row. Table 6
seen SamPG+Plain consistently occupies space SRC, even though experiments
thus far suggest RXL compact. basic compression techniques RXL therefore
important enough make performance ordering algorithms tip respect
space consumption.
RXL advantages visible tables. example require computing
APSP preprocessing step significantly reducing preprocessing time. computes
besides first move also shortest path distance.
12.3 Discussion
compared SRC MRC Copa RXL. Copa recent successful technique creating compressed path databases. one joint winners 2012 Grid-Based Path Planning
Competition (GPPC-12) regard Copa current state-of-the-art range pathfinding
problems including efficient storage extraction optimal first-moves. RXL newest
version Hub-Labeling algorithm knowledge state-of-the-art terms minimizing query times road graphs.
617

fiS TRASSER , B OTEA , & H ARABOR

SRC MRC illustrated Figures 1, 2 8. Figure 9 illustrates Copa
works preprocessing, better understanding section, without intention fully
detailed description. Copa assumes every node graph labelled x, coordinates.
toy example 3 rows 3 columns, shown Figure 9 (a). Copa iterates
nodes graph. Let n current node (source node) given iteration. Copa splits
map rectangles, labels rectangle id outgoing edge n.
target belongs given rectangle, optimal move n towards precisely label
rectangle. Figure 9 (a) shows map decomposition source node 2, rectangles depicted
dashed line. three rectangles constructed figure: one bottom left,
size 2 2 label e; one top left, size 1 1 label a; one bottom right,
size 1 1 label f . part (b), rectangle represented 5 symbols each: upper
row, left column, width, height, label. show rectangles source node 2,
concatenated lists nodes. rectangles safely removed (list
trimming) skip example. Then, 5 columns Figure 9 (a) treated
separate string, compressed sliding window compression run-length
encoding.
performed experiments large number realistic grid-graphs used GPPC-12 find
SRC MRC significantly improve query time compression power
Copa. large number experiments broad range input maps able extract
first move tens hundreds nano-seconds (a factor 3 5 faster Copa).
two main reasons SRC MRC performant vs. Copa: approach uses less memory
query running time logarithmic (cf. linear) label size.
approach requires less memory Copa. Part explanation stems differences sizes building blocks approach. SRC MRC, building
block RLE run represented two numbers: start run, node id
thus requires log2 (|V |) bits, value run, out-edge id requires log2 ()
bits. Copa, building block rectangle requires 2 log2 (|V |) + log2 () bits. actual
implementations, SRC MRC store single 32-bit machine word per run, allows graphs 228 nodes. Copa code used 2012 Grid-Based Path Planning
Competition stores rectangle 48 bits, corresponding max node count 222 .
Clearly, size building blocks reason different compression
results. number RLE runs SRC MRC differ total number rectangles
Copa. one optimal out-edge exists, SRC MRC select edge
improve compression, whereas Copa sticks one arbitrary optimal out-edge.
hand, besides rectangle decomposition, Copa implements additional compression methods,
list trimming, run length encoding sliding window compression, performed top
original rectangle decomposition (Botea & Harabor, 2013a).
approach asymptotic query time O(log2 (k)) k number compressed
labels must searched. comparison, given source node, Copa stores corresponding
list rectangles decreasing order size. Rectangles checked order. While,
worst-case, total number rectangle checks linear size list, average number
much improved due ordering mentioned (Botea, 2011; Botea & Harabor, 2013a).
reason CPD faster RXL due basic query algorithm. algorithm
underlying RXL consists merge-sort like merge two integer arrays formed forward
label backward label t. fast cache friendly operation needs look
618

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

entry resulting inherently linear time operation. SRC hand builds upon
binary search slightly less cache friendly memory accesses sequential
logarithmic running time.
One regard compressed SRC rows one-sided labels. st-pair first move
determined using label s. HL hand needs forward label
backward label t. HL-labels tend less entries SRC labels. However,
HL-entry needs space need store distance values addition node-IDs.

13. Results 2014 Grid-Based Path Planning Competition
recently submitted algorithms SRC+cut SRC+dfs 2014 edition GridBased Path Planning Competition GPPC (Sturtevant, 2014). section give brief overview
competition short summary results. full description methodology employed
organisers, well full account results, given (Sturtevant et al., 2015).
13.1 Competition Setup
Grid-Based Path Planning Competition features one hundred grid maps
three hundred thousand distinct problem instances drawn. Individual maps differ size,
ranging several thousand several million nodes. topography maps also varied
many maps originating computer games StarCraft, Dragon Age: Origins Dragon
Age 2. maps appearing part competition synthetically generated grids; mazes,
rooms randomly placed obstacles varying density. 2014 edition competition
total 14 different entries, submitted 6 different teams. Several entries variants
algorithm submitted team.
6 entries employ symmetry breaking speed search. entries BLJPS, BLJPS2, JPS+
JPS+Bucket roughly described extensions Jump Point Search (Harabor &
Grastien, 2011) JPS+ (Harabor & Grastien, 2014). entry NSubgoal makes use
multi-level Subgoal Graphs (Uras & Koenig, 2014). Finally entry named BLJPS Sub
hybrid algorithm makes use Jump Point Search Subgoal Graphs.
1 entry (CH) employs variation Contraction Hierarchies (Dibbelt et al., 2014).
3 entries directly improve performance A* algorithm; either use faster
priority queues (A* Bucket) trading optimality speed (RA* RA*-Subgoal).
4 entries use Compressed Path Databases. Two (SRC+dfs-i SRC+cut-i)
incremental algorithms return optimal path one segment time; is, must
called repeatedly target location returned. two algorithms (SRC+dfs
SRC+cut) non-incremental queried return complete path.
Unfortunately, 3 entries contained bugs therefore finish instances.
SRC+cut one them. therefore omit tables discussion.
13.2 Results
summary results competition given Table 7. observe following:
619

fiS TRASSER , B OTEA , & H ARABOR

Entry


RA*
BLJPS
JPS+
BLJPS2

RA*-Subgoal
JPS+ Bucket
BLJPS2 Sub
NSubgoal
CH
SRC-dfs
SRC-dfs-i

Averaged Query Time Test Paths (s)
Slowest Move
First 20 Moves
Full Path
Path
path
Extraction
282 995
282 995
282 995
14 453
14 453
14 453
7 732
7 732
7 732
7 444
7 444
7 444
1 688
1 688
1 688
1 616
1 616
1 616
1 571
1 571
1 571
773
773
773
362
362
362
145
145
145
1
4
189

Preprocessing Requirements
DB Size
Time
(MB)
(Minutes)
0
0.0
20
0.2
947
1.0
47
0.2
264
0.2
947
1.0
524
0.2
293
2.6
2 400
968.8
28 000
11649.5
28 000
11649.5

Table 7: Results 2014 Grid-Based Path Planning Competition. Figures summarised
official competition results, appear (Sturtevant et al., 2015). Entries denoted
indicate approximate algorithms guaranteed always find shortest path.
measurments bold indicate entry performed best regard single criterion.
entries whose name bold fully Pareto-dominated respect every
criterion. preprocessing running times time needed process 132 test maps.
1. CPD-based entries fastest methods competition across query time metrics. includes fastest (average) time required extract complete optimal path,
fastest (average) time extract first 20 steps optimal path fastest (average)
time required extract single step optimal path.
2. Performing first move query algorithm faster resolution competitions microsecond timer. Even iteratively extracting 20 edges path barely
measurable without finer timer resolution. testing observed significant
amount query running time spent within benchmarking code provided
competition. therefore opted submit two variants. SRC+dfs extracts path
whole. benchmarking code thus run per path. hand SRC+dfs-i
extracts path one edge time. allows measuring time needed individual
first move query. Unfortunately, also requires executing benchmarking code per
edge. difference path extraction running times SRC-dfs SRC-dfs-i (i.e.,
44s) time spent benchmarking code.
3. algorithm competitor able answer first-move queries faster
full path extraction.
4. entries database driven require generous amounts preprocessing time
storage space. SRC+dfs therefore largest total preprocessing time largest
total storage cost entries competition.

14. Conclusion Future Work
study problem creating efficient compressed path database (CPD): shortest path
oracle which, given two nodes weighted directed graph, always returns first-move
620

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

optimal path connecting them. Starting all-pairs first-move matrix, assume
given, create oracles compact answer arbitrary first-move queries
optimally many orders faster otherwise possible using conventional online graph search
techniques. employ run-length encoding (RLE) compression scheme throughout analyse
problem theoretical perspective empirical one. main idea simple:
look re-order nodes (i.e., columns) input first-move matrix good way
RLE-compressed size subsequently reduced.
theoretical side show problem finding optimal node ordering,
general case directed directed graphs, NP-complete. specific cases,
graphs decomposed along articulation points, problem efficiently tackled
solving series independent sub-problems. particular show depth-first traversal
tree provides optimal node ordering. results give first theoretical underpinning
problem creating space-efficient CPDs using RLE. work also extends theoretical results
areas information processing databases (Oswald & Reinelt, 2009; Mohapatra, 2009).
empirical side study efficacy three heuristic node orderings: (i) depth-first
ordering; (ii) graph-cut ordering based balanced edge separators; (iii) naive baseline given
ordering specified input graph. Given ordering first-move matrix, describe
two novel approaches creating CPDs. first these, SRC, uses simple run-length encoding
compress individual rows matrix. second approach, MRC, sophisticated
identifies commonalities sets labels compressed SRC.
range experiments show SRC MRC compress APSP matrix graphs
hundreds thousands nodes little 1-200MB. Associated query times regularly
require less 100 nanoseconds. also compare approaches Copa (Botea, 2012;
Botea & Harabor, 2013a), RXL (Delling et al., 2014). range experiments grid
road graphs show SRC MRC competitive Copa often several
factors better, terms compression query times. also show SRC MRC
outperform RXL terms query time. also summarise results 2014 Grid-Based
Path Planning Competition. particular report SRC fastest method
competition across query-time metrics SRC performed better resolution
competitions microsecond timer.
appear several promising directions current work could extended. One
immediate possibility harness available results efficient appproximate TSP algorithms
order compute better space-efficient node orderings. Another immediate possibility
improve current MRC compression scheme devising algorithm optimizes
assignment first-move IDs.
Looking broadly, strength CPDs have, addition fast move extraction,
compress kind path network-distance optimal.12
multi-agent pathfinding example sometimes useful guarantee properties like must
always local detour available (Wang & Botea, 2011). Another example turn-costs road
graphs. Thus one possible possible direction future work create CPDs store
paths satisfying constraints.
weakness approach preprocessing APSP-computation required.
Delling et al. (2013) shown APSP sometimes computed reasonably fast
12. Suboptimal paths, however, introduce additional challenge avoiding infinite loops extracting path
CPD.

621

fiS TRASSER , B OTEA , & H ARABOR

graphs many nodes, APSP remains inherently quadratic number nodes
graph class output size quadratic. approach would therefore hugely profit
algorithm directly compute compressed CPD without first computing first-move
matrix intermediate step.

15. Acknowledgments
thank Patrik Haslum, Akihiro Kishimoto, Jakub Marecek, Anika Schumman Jussi Rintanen
feedback earlier versions parts work. would like thank Daniel Delling &
Thomas Pajor running Hub-Labeling experiments us.

Appendix A. Proof Theorem 2
Theorem 2. SimMini1Runs NP-complete.
Proof. membership NP straightforward. hardness proof uses reduction
Hamiltonian Path Problem (HPP) undirected graph. Let G = (V, E) arbitrary undirected
graph, without duplicate edges, define n = |V | e = |E|. Figure 10 shows toy graph used
running example.
Starting G, build SimMini1Runs instance follows. define 0/1 matrix
e rows n columns. Let r row corresponding edge (u, v), let cu cv
columns associated nodes u v. m[r, cv ] = m[r, cu ] = 1 m[r, c] = 0
columns. Notice least value 1 every row column. Figure 11 shows
matrix running example.
x



w

z

Figure 10: Sample graph G.
Let r matrix row corresponding edge (u, v). easy see that, given
ordering columns (nodes) makes two nodes u v adjacent, number sequential

(x, y)
(x, w)
(x, z)
(w, z)

x
1
1
1
0


1
0
0
0

w
0
1
0
1

z
0
0
1
1

Figure 11: Matrix built G.
622

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

(x, z)
(w, z)
(x, w)
(x, y)


0
0
0
1

x
0
0
1
1

w
0
1
1
0

z
1
1
0
0

Figure 12: matrix after: i) converting 1s 0s (shown bold); ii) re-ordering columns
Hamiltonian path; iii) re-ordering rows lexicographically.

(x, z)
(w, z)
(x, w)
(x, y)


0
0
0
1

x
1
0
1
1

w
0
1
1
0

z
1
1
0
0

Figure 13: Matrix restoring back previously replaced 1s (shown bold).
RLE 1-runs13 row r 1. nodes adjacent, number sequential RLE 1-runs
row r 2.
claim HPP solution iff SimMini1Runs solution = 3e n + 2 RLE
1-runs. Let vi1 , . . . , vin solution HPP (i.e., Hamiltonian path), let P set
edges included solution. running example, let P contain (y, x), (x, w) (w, z).
every row corresponding edge contained P , switch one two 1-entries
0. Then, order columns respect sequence nodes Hamiltonian path
rearrange rows lexicographical order. Figure 12 illustrates changes.
construction matrix, trick converting 1s 0s, ordering
rows columns reused Oswald Reinelts proof hardness deciding
whether 0/1 matrix C1Sk property (Oswald & Reinelt, 2009). rest proof,
coming below, significantly different.
Now, restore previously replaced 1s, shown Figure 13. e n + 1 1s
replaced restored adjacent 1s matrix.14 such, counts two
1-runs, one horizontal one vertical. sums total 2(e n + 1) 1-runs corresponding
1s replaced restored. addition, row column one 1-run. follows
matrix 3e n + 2 1-runs.
Conversely, consider row column ordering creates 3e n + 2 RLE 1-runs total.
show matrix least e + 1 vertical 1-runs, regardless row ordering. Consider
rows, order, starting top. first row introduces exactly 2 vertical 1-runs, one
column contains value 1. subsequent row introduces least one vertical
1-run. Otherwise, new row would identical previous one, contradicts fact
graph duplicate edges.
13. runs used proof sequential.
14. adjacent 1 column, would imply two identical rows, would mean
G duplicate edges. adjacent 1 row, would mean edge hand belongs
Hamiltonian path, contradicts fact 1s replaced restored complementary set edges.

623

fiS TRASSER , B OTEA , & H ARABOR

least e + 1 vertical 1-runs, number horizontal 1-runs
(3e n + 2) (e + 1) = 2e n + 1. show column ordering Hamiltonian path.
Assuming contrary, p < n 1 edges nodes adjacent
ordering. follows number horizontal 1-runs p + 2(e p) = 2e p > 2e n + 1.
Contradiction.

Appendix B. Proofs Lemma 2 Lemma 3
start pointing two simple important properties stemming notion articulation point:
Remark 1. Given graph G, let x articulation point, let G1 . . . Gn corresponding
connected components obtained removing x.
1. Given source node Gi , first optimal move towards anywhere outside Gi
same.15
2. Given two distinct components Gi Gj , first optimal move x towards anywhere
Gi different first optimal move x towards anywhere Gj .
Remark 1 follows easily obvious observation way going one
subgraph Gi another subgraph Gj passing x. See Figure 7 illustration.
Lemma 2. Let x articulation point graph G. Every node order rearranged
x-block ordering o0 without increasing number runs row.
Proof. construct desired ordering o0 applying following steps:
1. Rotate x comes first.
2. every {1 . . . n} project resulting order onto Gi , obtaining suborder o0i .
3. Define o0 as: o0 = x, o01 , . . . , o0n .
clear construction nodes every subgraph Gi consecutive o0 . remains
show number runs per row grow.
Denote source node. distinguish two cases:
Case Gi i. rotation,
step 1 impact number

cyclic runs. Steps 2 3 take nodes k6=i Gk put one two blocks
cyclic
adjacent x. know Remark 1, point 1 m[s, x] = m[s, n]
nodes n k6=i Gk . Thus, re-arrangement brings next x nodes n
first-move symbol x. Clearly, increase number runs.
Case = x. previous case, rotation performed step 1 increase
number cyclic runs. step 1, cyclic runs sequential runs equivalent, since
first position contains distinct symbol, namely x-singleton. Steps 2 3 separate
15. Assuming split ties among optimal paths consistent manner, easy ensure.

624

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

Gi contiguous block. increase number sequential runs since,
according Remark 1, point 2, every two blocks corresponding Gi Gj , 6= j,
common symbol. follows number cyclic runs increase either.

Lemma 3. Given x-block ordering o, that:
1. N (o, G, Gi ) = N (o|i , , Gi );
P
2. N (o, G, {x}) = 1 n + N (o|i , , {x});
P
3. N (o, G, G) = 1 n + N (o|i , , ).
Proof. prove point follows.
1. x-block ordering, nodes come order x, G1 , . . . Gi , . . . Gn . Consider
node Gi corresponding row first-move matrix. pointed Remark 1,
every path node outside Gi pass x, therefore first move
anywhere outside Gi same. follows nodes sequence x, G1 , . . . Gi1 ,
together nodes sequence Gi+1S
, . . . Gn , form one cyclic run. effect, removing
consideration nodes contained k6=i Gk leaves number runs unchanged,
completes proof case.
2. case focused x start node. According Remark 1, Gi 6= Gj ,
first move x towards anywhere Gi different first move x towards
anywhere Gj . follows two runs two adjacent subsets Gi Gi+1 never merge
one run. Thus,
X
N (o, G, {x}) = 1 +
(N (o|i , , {x}) 1)


= 1n+

X


N (o|i , , {x}).

3. case follows previous two, standard arithmetic manipulation.
X
N (o, G, G) = N (o, G, {x}) +
N (o, G, Gi )


= 1n+

X

= 1n+

X
(N (o|i , , {x}) + N (o|i , , Gi ))



N (o|i , , {x}) +

X



X
= 1n+
(N (o|i , , {x} Gi )


= 1n+

X

N (o|i , , ).



625

N (o|i , , Gi )



fiS TRASSER , B OTEA , & H ARABOR

References
Abraham, I., Delling, D., Fiat, A., Goldberg, A. V., & Werneck, R. F. (2012). HLDB: Locationbased services databases. Proceedings 20th ACM SIGSPATIAL International
Symposium Advances Geographic Information Systems (GIS12), pp. 339348. ACM
Press. Best Paper Award.
Abraham, I., Delling, D., Goldberg, A. V., & Werneck, R. F. (2011). hub-based labeling algorithm
shortest paths road networks. Proceedings 10th International Symposium
Experimental Algorithms (SEA11), Vol. 6630 Lecture Notes Computer Science, pp.
230241. Springer.
Abraham, I., Delling, D., Goldberg, A. V., & Werneck, R. F. (2012). Hierarchical hub labelings
shortest paths. Proceedings 20th Annual European Symposium Algorithms
(ESA12), Vol. 7501 Lecture Notes Computer Science, pp. 2435. Springer.
Akiba, T., Iwata, Y., & Yoshida, Y. (2013). Fast exact shortest-path distance queries large networks pruned landmark labeling.. Proceedings 2013 ACM SIGMOD International
Conference Management Data (SIGMOD13), pp. 349360. ACM Press.
Antsfeld, L., Harabor, D., Kilby, P., & Walsh, T. (2012). Transit routing video game maps..
AIIDE.
Arz, J., Luxen, D., & Sanders, P. (2013). Transit node routing reconsidered. Proceedings
12th International Symposium Experimental Algorithms (SEA13), Vol. 7933 Lecture
Notes Computer Science, pp. 5566. Springer.
Babenko, M., Goldberg, A. V., Kaplan, H., Savchenko, R., & Weller, M. (2015). complexity hub labeling. Proceedings 40th International Symposium Mathematical
Foundations Computer Science (MFCS15), Lecture Notes Computer Science. Springer.
Baier, J., Botea, A., Harabor, D., & Hernandez, C. (2014). fast algorithm catching prey
quickly known partially known game maps. Computational Intelligence AI
Games, IEEE Transactions on, PP(99).
Bast, H., Delling, D., Goldberg, A. V., MullerHannemann, M., Pajor, T., Sanders, P., Wagner, D., &
Werneck, R. F. (2015). Route planning transportation networks. Tech. rep. abs/1504.05140,
ArXiv e-prints.
Bast, H., Funke, S., & Matijevic, D. (2009). Ultrafast shortest-path queries via transit nodes.
Shortest Path Problem: Ninth DIMACS Implementation Challenge, Vol. 74 DIMACS
Book, pp. 175192. American Mathematical Society.
Bast, H., Funke, S., Matijevic, D., Sanders, P., & Schultes, D. (2007). transit constant shortestpath queries road networks. Proceedings 9th Workshop Algorithm Engineering
Experiments (ALENEX07), pp. 4659. SIAM.
Bauer, R., Columbus, T., Katz, B., Krug, M., & Wagner, D. (2010). Preprocessing speed-up
techniques hard. Proceedings 7th Conference Algorithms Complexity
(CIAC10), Vol. 6078 Lecture Notes Computer Science, pp. 359370. Springer.
Bauer, R., Columbus, T., Rutter, I., & Wagner, D. (2013). Search-space size contraction hierarchies. Proceedings 40th International Colloquium Automata, Languages,
626

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

Programming (ICALP13), Vol. 7965 Lecture Notes Computer Science, pp. 93104.
Springer.
Bellman, R. (1958). routing problem. Quarterly Applied Mathematics, 16, 8790.
Botea, A. (2011). Ultra-fast optimal pathfinding without runtime search. Proceedings Seventh AAAI Conference Artificial Intelligence Interactive Digital Entertainment (AIIDE11), pp. 122127. AAAI Press.
Botea, A. (2012). Fast, optimal pathfinding compressed path databases. Proceedings
Symposium Combinatorial Search (SoCS12).
Botea, A., Baier, J. A., Harabor, D., & Hernandez, C. (2013). Moving target search compressed
path databases. Proceedings International Conference Automated Planning
Scheduling ICAPS.
Botea, A., & Harabor, D. (2013a). Path planning compressed all-pairs shortest paths data.
Proceedings 23rd International Conference Automated Planning Scheduling.
AAAI Press.
Botea, A., & Harabor, D. (2013b). Path planning compressed all-pairs shortest paths data.
Proceedings International Conference Automated Planning Scheduling ICAPS.
Botea, A., Strasser, B., & Harabor, D. (2015). Complexity Results Compressing Optimal Paths.
Proceedings National Conference AI (AAAI15).
Bulitko, V., Bjornsson, Y., & Lawrence, R. (2010). Case-based subgoaling real-time heuristic
search video game pathfinding. J. Artif. Intell. Res. (JAIR), 39, 269300.
Bulitko, V., Rayner, D. C., & Lawrence, R. (2012). case base formation real-time heuristic
search. Proceedings Eighth AAAI Conference Artificial Intelligence Interactive Digital Entertainment, AIIDE-12, Stanford, California, October 8-12, 2012.
Cohen, E., Halperin, E., Kaplan, H., & Zwick, U. (2002). Reachability distance queries via
2-hop labels. Proceedings Thirteenth Annual ACM-SIAM Symposium Discrete
Algorithms, SODA 02, pp. 937946, Philadelphia, PA, USA. Society Industrial Applied Mathematics.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence, 14(3),
318334.
Delling, D., Goldberg, A. V., Nowatzyk, A., & Werneck, R. F. (2013). PHAST: Hardwareaccelerated shortest path trees. Journal Parallel Distributed Computing, 73(7), 940
952.
Delling, D., Goldberg, A. V., Pajor, T., & Werneck, R. F. (2014). Robust distance queries massive
networks. Proceedings 22nd Annual European Symposium Algorithms (ESA14),
Vol. 8737 Lecture Notes Computer Science, pp. 321333. Springer.
Delling, D., Goldberg, A. V., & Werneck, R. F. (2013). Hub label compression. Proceedings
12th International Symposium Experimental Algorithms (SEA13), Vol. 7933
Lecture Notes Computer Science, pp. 1829. Springer.
Demetrescu, C., Goldberg, A. V., & Johnson, D. S. (Eds.). (2009). Shortest Path Problem: Ninth
DIMACS Implementation Challenge, Vol. 74 DIMACS Book. American Mathematical
Society.
627

fiS TRASSER , B OTEA , & H ARABOR

Dibbelt, J., Strasser, B., & Wagner, D. (2014). Customizable contraction hierarchies. Proceedings
13th International Symposium Experimental Algorithms (SEA14), Vol. 8504
Lecture Notes Computer Science, pp. 271282. Springer.
Dijkstra, E. W. (1959). note two problems connexion graphs. Numerische Mathematik,
1, 269271.
Felner, A., Korf, R. E., Meshulam, R., & Holte, R. C. (2007). Compressed pattern databases.. J.
Artif. Intell. Res. (JAIR), 30, 213247.
Ford, Jr., L. R. (1956). Network flow theory. Tech. rep. P-923, Rand Corporation, Santa Monica,
California.
Geisberger, R., Sanders, P., Schultes, D., & Delling, D. (2008). Contraction hierarchies: Faster
simpler hierarchical routing road networks. Proceedings 7th International
Conference Experimental Algorithms (WEA08), pp. 319333.
Harabor, D. D., & Grastien, A. (2011). Online graph pruning pathfinding grid maps. Burgard, W., & Roth, D. (Eds.), Proceedings Twenty-Fifth AAAI Conference Artificial
Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press.
Harabor, D. D., & Grastien, A. (2014). Improving jump point search. Chien, S., Do, M. B.,
Fern, A., & Ruml, W. (Eds.), Proceedings Twenty-Fourth International Conference
Automated Planning Scheduling, ICAPS 2014, Portsmouth, New Hampshire, USA, June
21-26, 2014. AAAI.
Hart, P. E., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions Systems Science Cybernetics, 4, 100107.
Hernandez, C., & Baier, J. A. (2011). Fast subgoaling pathfinding via real-time search..
Proceedings International Conference Automated Planning Scheduling ICAPS11.
Karypis, G., & Kumar, V. (1998). Metis, software package partitioning unstructured graphs,
partitioning meshes, computing fill-reducing orderings sparse matrices, version 4.0..
Kou, L. T. (1977). Polynomial complete consecutive information retrieval problems. SIAM Journal
Computing, 6(1), 6775.
Lawrence, R., & Bulitko, V. (2013). Database-driven real-time heuristic search video-game
pathfinding. Computational Intelligence AI Games, IEEE Transactions on, 5(3), 227
241.
Mohapatra, A. (2009). Optimal Sort Ordering Column Stores NP-Complete. Tech. rep., Stanford University.
Oswald, M., & Reinelt, G. (2009). simultaneous consecutive ones problem. Theoretical Computer Science, 410(21-23), 19861992.
Samadi, M., Siabani, M., Felner, A., & Holte, R. (2008). Compressing pattern databases
learning. Proceedings 2008 Conference ECAI 2008: 18th European Conference
Artificial Intelligence, pp. 495499, Amsterdam, Netherlands, Netherlands. IOS
Press.
628

fiC OMPRESSING PTIMAL PATHS RUN L ENGTH E NCODING

Sankaranarayanan, J., Alborzi, H., & Samet, H. (2005). Efficient query processing spatial networks. Proceedings 13th Annual ACM International Workshop Geographic Information Systems (GIS05), pp. 200209.
Strasser, B., Harabor, D., & Botea, A. (2014). Fast First-Move Queries Run Length Encoding. Proceedings Symposium Combinatorial Search (SoCS14).
Sturtevant, N. (2012a). 2012 Grid-Based Path Planning Competition. https://code.google.
com/p/gppc-2012/.
Sturtevant, N. (2012b). Website Grid-Based Path Planning Competition 2012. http:
//movingai.com/GPPC/.
Sturtevant, N. (2014). Website Grid-Based Path Planning Competition 2014. http:
//movingai.com/GPPC/.
Sturtevant, N., Traish, J., Tulip, J., Uras, T., Koenig, S., Strasser, B., Botea, A., Harabor, D., &
Rabin, S. (2015). grid-based path planning competition: 2014 entries results.
Proceedings 6th International Symposium Combinatorial Search (SoCS15). AAAI
Press.
Uras, T., & Koenig, S. (2014). Identifying hierarchies fast optimal search. Brodley, C. E., &
Stone, P. (Eds.), Proceedings Twenty-Eighth AAAI Conference Artificial Intelligence,
July 27 -31, 2014, Quebec City, Quebec, Canada., pp. 878884. AAAI Press.
Uras, T., Koenig, S., & Hernandez, C. (2013). Subgoal graphs optimal pathfinding eightneighbor grids.. Proceedings International Conference Automated Planning
Scheduling ICAPS-13.
van Schaik, S. J., & de Moor, O. (2011). memory efficient reachability data structure bit
vector compression. Proceedings 2011 ACM SIGMOD International Conference
Management Data, SIGMOD 11, pp. 913924, New York, NY, USA. ACM.
Wang, K.-H. C., & Botea, A. (2011). MAPP: Scalable Multi-Agent Path Planning Algorithm
Tractability Completeness Guarantees. Journal Artificial Intelligence Research (JAIR),
42, 5590.

629

fiJournal Artificial Intelligence Research 54 (2015) 369435

Submitted 9/15; published 11/15

Continuing Plan Quality Optimisation
Fazlul Hasan Siddiqui
Patrik Haslum

fazlul.siddiqui@anu.edu.au
patrik.haslum@anu.edu.au

Australian National University &
NICTA Optimisation Research Group
Canberra, Australia

Abstract
Finding high quality plans large planning problems hard. Although current
anytime planners often able improve plans quickly, tend reach limit
plans produced still far best possible, planners fail
find improvement, even given several hours runtime.
present approach continuing plan quality optimisation larger time scales,
implementation system called BDPO2. Key approach decomposition
subproblems improving parts current best plan. decomposition based
block deordering, form plan deordering identifies hierarchical plan structure.
BDPO2 seen application large neighbourhood search (LNS) local search
strategy planning, neighbourhood plan defined replacing one
subplans improved subplans. On-line learning also used adapt strategy
selecting subplans subplanners course plan optimisation.
Even starting best plans found means, BDPO2 able continue
improving plan quality, often producing better plans anytime planners
given enough runtime. best results, however, achieved combination
different techniques working together.

1. Introduction
classical AI planning problem involves representing models world (initial
goal states) available actions formal modelling language, reasoning
preconditions effects actions. Given planning problem, planning system
(or planner, short) generates sequence actions, whose application transforms
world initial state desired goal state. Thus, planning makes intelligent
system autonomous construction plans action achieve goals.
key concern automated planning producing high quality plans. Planners using
optimal bounded suboptimal (heuristic) search methods offer guarantees plan quality,
unable solve large problems. Fast planners, using greedy heuristic search
techniques, hand, solve large problems often find poor quality plans.
gap capabilities two kinds planners means producing high
quality plans large problems still challenge. example gap shown
Figure 1. seek address gap proposing new approach continuing plan
improvement, able tackle large problems works varying time scales.
Anytime search tries strike balance optimal (or bounded suboptimal)
greedy heuristic search methods. Anytime search algorithms finding initial
solution, possibly poor quality, quickly continuing search better solutions
c
2015
AI Access Foundation. rights reserved.

fi20
0

10

Plan cost

30

Siddiqui & Haslum

**
**********
*
*
****
0

50

100

150

Problem (sorted)

Figure 1: Illustration plan quality gap. dashed line represents best (lowestcost) plan 156 problems Genome Edit Distance (GED) domain (Haslum, 2011)
found different non-optimal planners, including anytime planners. solid line represents corresponding highest known lower bound. difference two
optimality gap. ? points represent plans found optimal planners,
vertical bars show optimality gap obtained problem-specific algorithm (GRIMM).

time given. Anytime search algorithms as, example, RWA*
(Richter, Thayer, & Ruml, 2010) AEES (Thayer, Benton, & Helmert, 2012b)
successfully used anytime planners. However, planners often effective
making use increasing runtime beyond first minutes. Xie, Valenzano, & Muller
(2010) define unproductive time planner amount time remaining
finds best plan, total time given. show four IPC-2011 domains
(Barman, Elevators, Parcprinter, Woodworking), unproductive time LAMA
planner (which uses RWA*), given 30 minutes per problem, 90%.
observed similar results, shown Figure 2. figure shows average
IPC quality score function time several anytime planners plan optimisation
methods, including LAMA planner. (A full description experiment setup,
results even anytime planners, presented Section 3, page 392.) LAMA
finds first solution quickly: 92.3% problems solves (within maximum 7
hours CPU time per problem), first plan found less 10 minutes. quality
LAMAs plans improve rapidly early on, later trend one flattening out, i.e.,
decreasing increase. (The drop beginning due figure showing average
plan quality solved problems: initial, low-quality, plans problems found
average drops, increasing better plans found.) 1 7
hours CPU time, LAMA improves plans 21.3% solved problems. Yet
51.6% problems better plans exist, found methods. time
interval, LAMAs average plan quality score increases 2.7%, increase
370

fiContinuing Plan Quality Optimisation

0.96
0.95

Average Quality Score (Relative IPC Quality Score / Coverage)

0.94
0.93
0.92
0.91




































































































0.9
0.89
0.88
0.87
0.86
0.85
0.84


0.83









































BDPO2 PNGS base plans
BDPO2 base plans
PNGS base plans
IBCS base plans
BSS base plans
LAMA scratch
IBaCoP2 scratch








0.82



7

6.5

6

5.5

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0.81

Time (hours)

Figure 2: Average IPC quality score function time per problem, set 182
large-scale planning problems. quality score plan cref/c, c cost
plan cref reference cost (least cost plans problem); hence
higher score represents better plan quality. Anytime planners (LAMA, IBaCoP2) start
scratch, post-processing (PNGS, BDPO) bounded-cost search (IBCS, BeamStack Search) methods start set base plans. curves delayed 1 hour
account maximum time given generating base plan. experiment setup
results additional planners described Section 3.1 (page 392).

371

fiSiddiqui & Haslum

Figure 3: General framework BDPO2

least 14.6% possible. Memory-limited branch-and-bound algorithms, like Beam Stack
Search (Zhou & Hansen, 2005) may run indefinitely, find improvements slowly.
increase average plan quality made BSS entire time depicted Figure
2 1.8%.
Plan optimisation approaches based post-processing start valid plan seek
improve it. Figure 2 shows results Plan Neighbourhood Graph Search (Nakhost
& Muller, 2010). PNGS searches shortcuts subgraph state space
problem, constructed around current plan. (The PNGS implementation used
experiment also applies Nakhosts Mullers action elimination technique.) Applying
PNGS results substantial plan quality improvements quickly 94.8% improved plans
found less 10 minutes stops, runs memory.
summary, experiment shows current anytime plan optimisation methods
become unproductive runtime increases, suffer slow rate plan quality
improvement.
present post-processing approach plan optimisation, implementation
system called BDPO2. (The source code BDPO2 provided on-line appendix
article.) post-processor, BDPO2 work own: depends
methods providing initial plan. experiment, set input plans (referred
base plans) best plans found LAMA 1 hour, plan found IBaCoP2
2014 IPC. Figure 2 shows switching approach time
overcome limitation current anytime planning techniques, continue improve
plan quality allotted time increases. best result, shown, obtained chaining
several techniques together, applying first PNGS base plans, BDPO2
best result produced PNGS. result could achieved previous anytime
planning approaches alone.
BDPO2 uses Large Neighborhood Search (LNS), local search technique. local
search explores neighbourhood around current solution plan better quality valid
plan. LNS, neighbourhood solution defined destroy repair methods,
together replace part current solution, keeping rest unchanged.
BDPO2, destroy step selects subsequence linearisation deordering
current plan (we call window) repair step applies bounded-cost
planner subproblem finding better replacement subplan. focus
solving smaller subproblems makes local search, LNS particular, scale better
large problems. size structure neighborhood, however, plays crucial
372

fiContinuing Plan Quality Optimisation

role performance local search (Hoffmann, 2001). setting, neighbourhood determined strategies used select windows subplanners. destroy
methods used LNS algorithms often contain element randomness, local
search may accept moves lower-quality solutions (Ropke & Pisinger, 2006; Schrimpf,
Schneider, Stamm-Wilbrandt, & Dueck, 2000). contrast, explore neighbourhood
systematically, examining candidate windows generated ordered several heuristics,
accept moves strictly better plans. also introduce LNS idea
delayed restarting, meaning search combine multiple local improvements
restarting next iteration new best plan. found delayed
restarts allow better exploration subplans different parts current plan,
helps avoid local minima otherwise occur system attempts re-optimise
part plan successive iterations.
BDPO2 framework, shown Figure 3, broadly consists four components: plan
decomposition, LNS (i.e., repeated destroy repair steps), windowing, on-line
adaptation. first step, decomposition, uses deordering produce partially ordered
plan. Deordering enables windowing strategies find subplans easier
improve on, leading much better anytime performance. use block deordering (Siddiqui
& Haslum, 2012), simultaneously decomposes given plan coherent subplans,
called blocks, relaxes ordering constraints blocks. Block deordering removes
inherent limitations existing, step-wise deordering techniques, able
deorder sequential plans cases step-wise deordering possible.
windowing component collection strategies extracting windows block
deordered plan, ranking policies order windows system attempts
optimise promising windows first.
BDPO2 extends earlier system, BDPO (Siddiqui & Haslum, 2013b), mainly using
variety alternatives task: BDPO used single windowing strategy (with
ranking) single subplanner, BDPO2 uses portfolios window generation
ranking strategies several subplanners. improves capability robustness
system, since single alternative (windowing strategy, subplanner, etc.) dominates
others across problems. Furthermore, take advantage fact system
solves many subproblems course local search learn on-line
best alternatives current problem. particular, use UCB1 multi-armed
bandit learning policy (Auer, Cesa-Bianchi, & Fischer, 2002) subplanner selection,
sequential portfolio window ranking policies.
remainder article structured follows: Section 2 describes block deordering. theory block deordering presented slightly different earlier
account (Siddiqui & Haslum, 2012), allowing deordering cases better contrasting traditional partially ordered plan semantics. Section 3 presents
overview BDPO2 system main empirical results, Sections 4 5 give
details windowing on-line adaptation components, respectively, including
empirical analysis impact performance system whole. Section 6
reviews related work, Section 7 presents conclusions outlines ideas future work.
373

fiSiddiqui & Haslum

2. Plan Decomposition
approach continuing plan quality improvement based optimising plan
parts, one time. Every subplan consider local optimisation subsequence
linearisation partially ordered plan. Therefore, key step removing unnecessary ordering constraints the, typically sequential, input plan. process called
plan deordering. importance deordering demonstrated one experiments
(presented Section 3.6, page 402), apply BDPO2 input plans
already high quality: total plan quality improvement (measured increase
average IPC plan quality score) achieved BDPO2 without deordering 28.7%
less achieved BDPO2 using plan deordering technique.
standard notion valid partially ordered plan requires unordered steps
plan non-interfering (i.e., two subsequences plan unordered, every
interleaving steps two must form valid execution). limits amount
deordering done, cases extent deordering sequential
plan possible. (An example situation shown Figure 6 page 381.)
remedy this, introduced block deordering (Siddiqui & Haslum, 2012), creates
hierarchical decomposition plan non-interleaving blocks deorders
blocks. makes possible deorder plans further, including cases
conventional, step-wise, deordering possible. (Again, example found
Figure 6 page 381.) section, present new, slightly different account
theory practice block deordering. First, relaxes restriction block deordered
plans, thereby allowing deordering plans. Second, contrasts semantics
block decomposed partially ordered plans traditional partially ordered plan
semantics clearer way.
Sections 2.12.3 describe necessary background, Sections 2.42.6 introduce block
decomposed partially ordered plans block deordering algorithm.
2.1 Planning Problem, Sequential Plan Validity
consider standard STRIPS representation classical planning problems action
costs. planning problem tuple = hM, A, C, I, Gi, set atoms
(alternatively called fluents propositions), set actions, C : R0+ cost
function actions, assigns action non-negative cost, initial
state, G goal.
action characterised triple hpre(a), add(a), del(a)i, pre(a), add(a),
del(a) preconditions, add delete effects respectively. also say
action consumer atom pre(a), producer add(a),
deleter del(a). action applicable state pre(a) S,
applied S, results state apply(a, S) = (S \ del(a)) add(a). sequence
actions = hai , ai+1 , ..., aj applicable state Si (1) pre(ak ) Sk k j,
(2) Si+1 = apply(ai , Si ), Si+2 = apply(ai+1 , Si+1 ), on; resulting state
apply(, Si ) = Si+j+1 .
valid sequential plan (also totally ordered plan) seq = ha1 , ..., planning
problem sequence actions applicable G apply(seq , I).
actions seq must executed specified order.
374

fiContinuing Plan Quality Optimisation

2.2 Partially Ordered Plan Validity
Plans partially ordered, case actions unordered respect
other. partially ordered plan (p.o. plan) tuple, pop = hS, i,
set steps (each labelled action A) represents strict (i.e.,
irreflexive) partial order S. unordered steps pop executed order.
+ denotes transitive closure . element hsi , sj (also si sj ) basic
ordering constraint iff transitively implied constraints . plan
step s, use pre(s), add(s) del(s) denote preconditions, add delete effects
action associated s. also use terms producer, consumer, deleter,
cost plan steps, referring associated actions. include two steps,
sI sG . sI ordered steps, consumes nothing produces initially
true atoms, sG ordered steps, consumes goal atoms produces
nothing.
linearisation pop total ordering steps respects . p.o. plan
pop valid (for planning problem ) iff every linearisation pop valid sequential
plan (for ). words, p.o. plan viewed compact representation
set totally ordered plans, namely linearisations.
Every basic ordering constraint, si sj , pop set associated reasons, denoted
Re(si sj ). reasons explain ordering necessary plan
valid: Re(si sj ) non-empty, step precondition may unsatisfied
execution linearisations pop violate si sj . reasons three
types:
PC(m) (producerconsumer atom m): first step, si , produces precondition second step, sj . Thus, order changed, sj executed si ,
precondition sj may established required.
CD(m) (consumerdeleter m): second step, sj deletes m, precondition
si . Thus, order changed, may deleted required.
DP(m) (deleterproducer m): first step, si deletes m, produced
second step, sj . order changed, add effect producer step may
undone deleter, causing later step fail. is, however, necessary
order producer deleter step may occur producer plan
depends added atom.
Note ordering constraint several associated reasons, including several
reasons type referring different atoms. producerconsumer relation
PC(m) Re(si sj ) usually called causal link si sj (McAllester &
Rosenblitt, 1991), denoted triple hsi , m, sj i. causal link hsi , m, sj threatened
deleter may ordered last producer sj
sj , since implies possibility false required execution
sj . formal definition follows.
Definition 1. Let pop = hS, p.o. plan, hsp , m, sc causal link pop .
hsp , m, sc threatened step sd deletes neither (1) sc + sd
(2) s0p : add(s0p ) sd + s0p + sc true.
375

fiSiddiqui & Haslum

mentioned above, p.o. plan, pop = hS, planning problem valid iff
every linearisation pop valid sequential plan . However, following theorem
gives alternative, equivalent, condition p.o. plan validity.
Theorem 1 (e.g., Nebel & Backstrom, 1994). p.o. plan valid iff every step precondition
supported causal link threat causal link.
condition Chapmans (1987) modal truth criterion,
sc S, pre(sc ) :
sp : (PC(m) Re(sp sc )
st : del(st ) sc + sd s0p : add(s0p ) sd + s0p + sc



.

2.3 Deordering
process deordering converts sequential plan p.o. plan removing ordering
constraints steps, steps plan successfully executed
order consistent partial order still achieve goal (Backstrom, 1998).
refer step-wise deordering, distinguish block decomposition
deordering introduce later section. Since current state space search planners
produce sequential plans efficiently, deordering plays important role efficient
generation p.o. plans.
Let pop = hS, valid p.o. plan. (step-wise) deordering pop valid plan
0
0
pop
= hS, 0 (0 )+ + . is, pop
result removing basic
ordering constraints without invalidating plan. sequential plan seq = ha1 , ...,
represented p.o. plan one step si action ai seq ordering
si sj whenever < j, two steps unordered. Thus, deordering
sequential plan different (further) deordering p.o. plan.
Computing (step-wise) deordering minimum number ordering constraints
NP-hard (Backstrom, 1998), several non-optimal algorithms (e.g., Pednault,
1986; Veloso, Perez, & Carbonell, 1990; Regnier & Fade, 1991). used variant
explanation-based generalisation algorithm Kambhampati Kedar (1994).
algorithm works two phases: first phase constructs validation structure,
exactly one causal link hsp , m, sc precondition step sc . sp chosen
earliest producer preceding sc input plan, intervening threatening
step (i.e., deletes m) sp sc . (The algorithm Veloso, Perez Carbonell
similar, selects latest producer instead.) second phase, algorithm builds
partial ordering, keeping orderings original plan either correspond
causal links validation structure required prevent threatening step
becoming unordered w.r.t. steps causal link.
Kambhampati Kedars deordering algorithm, due greedy strategy,
guarantee optimality. example fails transform totally ordered plan
least-constrained plan shown Figure 4. However, recent study found
algorithm produce optimal step-wise plan deorderings plans
tested (Muise, McIlraith, & Beck, 2012).
However, motivation plan deordering find deordering adequate
generating useful candidate subplans local optimisation. important achieving
376

fiContinuing Plan Quality Optimisation

Figure 4: example Kambhampati Kedars (1994) algorithm fails find
least constrained plan. (Derived Figure 14 Backstroms 1998 article plan
deordering.) Figure (a) sequential input plan, (b) plan produced algorithm
choosing earliest producer (for validation structure) preconditions p
q D, (c) minimally ordered version (a). simplicity, goal atoms
produced steps A, B, C shown figure.

optimal step-wise deordering overcoming inherent limitation step-wise deordering, allows plan steps unordered non-interfering. Block
deordering, described next two sections, remove orderings input
plans forming blocks, helps generate decomposed plan suitable
extracting subplans local optimisation.
2.4 Block Decomposition
conventional p.o. plan, whenever two subplans unordered every interleaving steps
two forms valid execution. limits deordering cases individual steps
non-interfering. remove restriction, proposed block decomposed partial
ordering, restricts interleaving steps dividing plan steps blocks,
steps block must interleaved steps block. However,
steps within block still partially ordered. illustrated example
Figure 5. figure shows difference linearisations p.o. plan block
decomposed p.o. plan. b, a, c, valid linearisation standard partial ordering
block decomposed p.o. plan. formal definition block follows.
Definition 2. Let pop = hS, p.o. plan. block w.r.t. , subset b steps
two steps s, s0 b, exists step s00 (S \b) + s00 + s0 .
decomposition plan blocks recursive, i.e., block wholly
contained another. However, blocks cannot partially overlapping. Two blocks
ordered bi bj exist steps si bi sj bj si sj neither block
contained (i.e., bi 6 bj bj 6 bi ).
Definition 3. Let pop = hS, p.o. plan. set B subsets block
decomposition pop iff (1) b B block w.r.t. (2) every bi , bj B,
either bi bj , bj bi , bi bj disjoint. block decomposed plan denoted
bdp = hS, B, i.
377

fiSiddiqui & Haslum

Figure 5: normal p.o. plan (left) represents set sequential plans linearisations plan steps, example ha, b, c, di, hb, a, c, di, hb, c, a, di, hb, c, d, ai.
block decomposed p.o. plan (shown right dashed outlines blocks) allows
unordered blocks executed order, steps different blocks
interleaved. Thus, ha, b, c, di, hb, c, a, di, hb, c, d, ai possible linearisations
plan.

semantics block decomposed plan defined restricting linearisations (for
must valid) respect block decomposition, i.e.,
interleave steps disjoint blocks. bi bj , steps bi must precede steps bj
linearisation block decomposed plan.
Definition 4. Let bdp = hS, B, block decomposed p.o. plan planning problem
. linearisation bdp total order lin (1) lin (2) every
b B block w.r.t. lin . bdp valid iff every linearisation bdp plan .
Blocks behave much like (non-sequential) macro steps, preconditions, add
delete effects subset union constituent steps.
enables blocks encapsulate plan effects preconditions, reducing interference
thus allowing deordering. following definition captures preconditions
effects visible outside block, i.e., give rise dependencies
interference parts plan. need consider
deciding two blocks unordered. (Note responsible step step block
causes produce, consume threaten atom.)
Definition 5. Let bdp = hS, B, block decomposed p.o. plan, b B block.
block semantics defined as:
b adds iff b precondition m, responsible step b
add(s), s0 b, s0 deletes s0 s.
b precondition iff responsible step b pre(s),
step s0 b causal link hs0 , m, si without active threat.
b deletes iff responsible step b del(s), step
s0 b s0 adds m.
Note block consumes proposition, cannot also produce proposition.
reason taking black box view block execution, proposition
simply persists: true execution block begins remains true
finished. steps within block totally ordered, preconditions effects
block according Definition 5 nearly cumulative preconditions
378

fiContinuing Plan Quality Optimisation

effects action sequence defined Haslum Jonsson (2000), difference
consumer block cannot also producer proposition.
conventional p.o. plan, valid, must contain threat causal link.
contrast, block decomposed p.o. plan allows threat causal link exist plan,
long causal link protected threat block structure. causal
link protected threat iff either (i) causal link contained block
contain threat, (ii) threat contained block contain
causal link delete threatened atom (i.e., encapsulates delete effect).
threat causal link active link protected it, otherwise inactive.
formal definition follows.
Definition 6. Let bdp = hS, B, block decomposed p.o. plan, st threat
causal link hsp , m, sc bdp . hsp , m, sc protected st iff exist block
b B either following true: (1) sp , sc b; st
/ b; (2) st b, sp , sc
/ b,

/ del(b).
example block decomposition protects causal link seen Figure
7(i) page 382.
following theorem provides alternative criterion validity block decomposed p.o. plan, analogy condition conventional p.o. plan given
theorem cited above. difference block decomposed p.o. plan allows
threats causal links, long threats inactive. Let bdp = hS, B, block
decomposed p.o. plan. Analogously Chapmans modal truth criterion, condition
stated follows:
sc S, pre(sc )
sp : (m add(sp )
st : (m del(st ) st 6+ sp sc 6+ st hsp , m, sc protected st )).
Theorem 2. block decomposed p.o. plan valid iff every step precondition supported
causal link active threat.
Proof. Let bdp = hS, B, block decomposed p.o. plan planning problem . Let
us first prove part, i.e., every step precondition supported causal
link active threat every linearisation bdp valid plan . Let
seq = h. . . , sc , . . .i arbitrary linearisation bdp total order seq S,
pre(sc ). Then, according validity criteria sequential plan, show
must satisfied execution sc seq . Since every step precondition
supported causal link bdp active threat, must supported
causal link hsp , m, sc active threat. Moreover, since seq sp seq sc .
Let st threat hsp , m, sc bdp . Clearly, sp seq st seq sc possibility
may cause unsatisfied execution sc . Since hsp , m, sc active
threat, hsp , m, sc protected st , therefore, according Definition 6, either (1)
sp , sc b st
/ b, (2) st b, sp , sc
/ b,
/ del(b), must hold. (1) true,
sp seq st seq sc occur valid linearisation bdp , since interleaves
steps sp , sc b st
/ b, thus b block w.r.t. seq . second case, since
379

fiSiddiqui & Haslum


/ del(b) must producer m, s0p b, st seq s0p . Moreover,
since sp , sc
/ b, sp seq st seq sc true sp seq st seq s0p seq sc . also
makes true execution sc seq .
Let us prove part, i.e., bdp valid every step precondition
supported causal link active threat. Let sc S, pre(sc ),
seq = h. . . , sc , . . .i linearisation bdp total order seq S. consider two
possible situations: (1) producer s0 causal link hs0 , m, sc bdp
constructed, (2) least one producer construct causal
link sc atom causal link active threat bdp . show
none situations happen long bdp valid. According situation
(1), s0 seq well s0 seq sc . causes unsatisfied
execution sc seq , i.e., seq become invalid. Consequently, bdp become invalid
(since one linearisation invalid), contradicts assumption. Therefore,
must exist least one producer s0 construct causal link hs0 , m, sc bdp .
Now, situation (2), assume sp last producer execution sc
seq , i.e., s0p \ sp : add(s0p ) (s0p seq sp sc seq s0p ). Let sp producer
causal link hsp , m, sc bdp (which possible, since sp ordered sc
bdp ). Assume hsp , m, sc active threat st bdp . Since hsp , m, sc active
threat st (i.e., hsp , m, sc protected st ), neither (i) sp , sc b; st
/ b,
(ii) st b; sp , sc
/ b,
/ del(b), true. Therefore, sp seq st seq sc possible
linearisation bdp . Moreover, since producer sp sc ,
must unsatisfied execution sc , i.e., seq becomes invalid. Consequently,
bdp invalid since one linearisations invalid. Therefore, hsp , m, sc must
active threat.
2.5 Block Deordering
Block deordering (Siddiqui & Haslum, 2012) process removing orderings
plan steps adding blocks block decomposed p.o. plan. may also add plan
new ordering constraints, transitively implied ordering
constraints. Block deordering often remove ordering constraints step-wise deordering not. no-interleaving restriction among blocks affords
us simplified, black box, view blocks localises interactions,
preconditions effects executing block whole important. Thus, allows deordering able ignore dependencies effects matter
internally within block. addition providing linearisations, improving
deordering, blocks formed block deordering often correspond coherent, selfcontained subplans, form basis windowing strategies (described detail
Section 4) use generate candidate subplans local optimisation.
subsection presents conditions adding blocks block decomposition allows removal basic ordering constraints. complete block deordering
algorithm presented next subsection.
simple example block deordering, Figure 6(i) shows sequential plan small
Logistics problem. plan deordered conventional p.o. plan,
plan step reason ordered previous. Block deordering, however,
380

fiContinuing Plan Quality Optimisation

Figure 6: sequential plan block deordering plan two unordered blocks
b1 b2. Ordering constraints labelled reasons: producerconsumer (PC),
i.e., causal link, deleterproducer (DP), consumerdeleter (CD). Note ordering
constraint sequential plan removed without invalidating it. Thus, step-wise
deordering plan possible.

able break ordering s3 s4 removing reason PC(at P1 A) based
formation two blocks b1 b2 shown Figure 6(ii). Neither two blocks
delete add atom P1 (although precondition both). removes
interference them, allows two blocks executed order
without interleaving. Therefore, possible linearisations block decomposed
p.o. plan hs1, s2, s3, s4i hs4, s1, s2, s3i. Note b2 ordered b1,
b1 optimised removing step s3.
Besides necessary orderings pair steps plan due reasons PC,
CD, DP (stated Section 2.2), valid block decomposed p.o. plan must maintain one
type necessary ordering, called threat protection ordering. removing ordering
sx + sy causes block containing steps delete effect,
ordering, delete effect causes causal link outside block become
unprotected (not satisfying either two conditions Definition 6), sx + sy
threat protection ordering, may removed. threat protection ordering
introduced block deordering process, introduced removed.
demonstrated Figure 7, removing kind ordering leads invalid
block decomposed p.o. plan. threat protection ordering defined formally follows.
Definition 7. Let bdp = hS, B, block decomposed p.o. plan, hsp , m, sc
causal link protected st bdp . Let b B; st , s0 b; sp , sc
/ b; add(s0 );

/ del(b); st + s0 . st + s0 threat protection ordering breaking ordering
causes del(b) causes hsp , m, sc become unprotected st .
381

fiSiddiqui & Haslum

Figure 7: Two block decompositions plan containing five steps: s1, s2, s3, s4, s5.
decomposition (i), three (transitively reduced) necessary orderings: s1 s2,
s2 s3, s4 s5, Re(s1 s2) = {DP(m), DP(n)}, Re(s2 s3) = {PC(m)},
Re(s4 s5) = {PC(n)}. decomposition valid since every step precondition
satisfied causal link without active threats. threat s1 causal link hs4, n,
s5i inactive, since link protected block bx = {s1, s2, s3} contains s1
delete m, disjoint causal link. forming two blocks, = {s1}
bz = {s2, s3} would possible remove s1 s2, shown (ii), since hs2, m, s3i
protected s1 bz . However, decomposition delete effect block bx
becomes del(bx ) = {m, n}, block therefore longer protects hs4, n, s5i. Therefore,
decomposition deordering invalid. ordering s1 s2 threat protection
ordering, must broken. Note (i) s2 consumers produced
atom n, yet acts white knight hs4, n, s5i protect n deleter s1.

notion threat protection ordering missing earlier block deordering
procedure (Siddiqui & Haslum, 2012), relied (implicitly) stronger restriction
delete effects block change due subsequent deordering inside block.
Explicitly checking necessary threat protection orderings allows deordering
inside created blocks take place.
remove basic ordering, si sj , block decomposed p.o. plan bdp = hS, B, i,
create two blocks, bi bj , si bi , sj bj , bi bj = . Note one
two blocks consist single step. blocks must consistent existing
decomposition, i.e., B {bi , bj } must still valid block decomposition, sense
Definition 2. remainder subsection, define four rules state conditions
blocks bi bj allow different reasons ordering si sj eliminated.
Since ordering si sj exist several reasons (including several reasons
type, referring different atoms), blocks bi bj found
allow us remove every reason Re(si sj ) ordering steps
removed.
Rule 1. Let bdp = hS, B, valid block decomposed p.o. plan, si sj basic
ordering whose removal cause threat protection ordering removed,
PC(m) Re(si sj ). Let bi block, si bi , sj
/ bi , s0 bi : si s0 . PC(m)
removed Re(si sj ) pre(bi ) sp
/ bi sp establish
causal link bi sj .
382

fiContinuing Plan Quality Optimisation

Figure 8: Formation block {s,p} addition causal link hr,m,qi (ii) order
remove reason PC(m) behind basic ordering constraint p q (i). Different
situations, (iii iv), threat, t, may active hr,m,qi.

explanation Rule 1, PC(m) Re(si sj ), bi must produce m. Since
si produces followed deleter within bi (because si sj basic
ordering sj
/ bi ) way happen bi consumes m. Since plan
valid, must producer, sp
/ bi , necessarily precedes step (in bi )
+
consumes m. Note sp sj . adding causal link PC(m) Re(sp sj ) (i.e.,
adding hsp , sj already present) allows PC(m) removed Re(si sj ).
Theorem 3. Deordering according Rule 1 preserves plan validity.
Proof. Let bdp = hS, B, valid block decomposed p.o. plan. Therefore, according
Theorem 2, every step precondition bdp supported causal link active
threat. Let p q basic ordering constraint (where p, q S), bp , bq B blocks
meet conditions removing PC(m) Re(p q), bp , bq ordered
ordering constraints. show removing PC(m) Re(p q) results
0
new plan, bdp
= hS, B 0 , 0 i, meets condition Theorem 2, therefore
remains valid.
Assume PC(m) Re(p q) removed, precondition q supplied
step r based newly established causal link hr,m,qi deordering formulating
0
bp = {s,...,p}, bq = {q} bdp
, shown Figure 8 (ii). show hr,m,qi
0
0
active threat bdp , therefore, bdp
valid. Assume, active threat,
0
+
t, hr,m,qi bdp . Then, course, r q + t. examine every
situation, active threat hr,m,qi.
Situation (1): assume + t, shown Figure 8 (iii). Since active threat
hr,m,si bdp , according Theorem 2, either contained block
383

fiSiddiqui & Haslum

Figure 9: Formation blocks removing reason CD(m) behind basic ordering
p q.

delete threatened atom contain hr,m,si, hr,m,si contained
0
block b0 = {r, s, ...} contain t. first case, also holds true bdp
,
0
therefore, active threat hr,m,qi. second case, b partially
overlap bp = {s,...,p}, therefore, either bp b0 b0 bp . bp b0 , bp must contain r,
happen according PC removing criteria (i.e., r
/ bp must hold) stated
Rule 1. b0 bp , b0 must contain least r, s, p, b0 partially
overlap bp = {s,...,p}. Since also active threat hp,m,qi bdp , hp,m,qi
must contained block b00 = {p, q, ...} contain t. Now, since b0
b00 partially overlap, b0 b00 (whichever bigger) must contain least r, s, p,
q, b0 b00 (whichever bigger) protects hr,m,qi t.
Situation (2): assume + p, also shown Figure 8 (iii). Since also active
threat hp,m,qi bdp , like before, show either contained block
encapsulates threatened atom (i.e., delete m) contain hp,m,qi,
hp,m,qi contained block b0 = {r, s, p, q, ...} contain t. cases,
hr,m,qi protected t.
Situation (3): assume + + p shown Figure 8 (iv). possible bp ,
since interleave steps bp
/ bp . Therefore, bp , causes
hr,m,qi protected t. bp contain hr,m,qi
delete (since add(p) + p).
Therefore, conclude never active threat hr,m,qi
situation.
Rule 2. Let bdp = hS, B, valid block decomposed p.o. plan, si sj basic ordering
whose removal cause threat protection ordering removed, CD(m)
384

fiContinuing Plan Quality Optimisation

Re(si sj ). Let bi bj two blocks, si bi , sj bj , bi bj = .
CD(m) removed Re(si sj ) bi consume m.
Theorem 4. Deordering according Rule 2 preserves plan validity.
Proof. Let bdp = hS, B, valid block decomposed p.o. plan, p q basic
ordering constraint, p, q CD(m) Re(p q). order meet
condition Rule 2, let us assume bp block includes r p hr,m,pi
causal link every consumer bp (if exist) ordered r bdp (as
shown Figure 9 (i)). Therefore meets condition bp must consume m. Also,
assume bq block contains {q} bp , bq ordered ordering
constraints. Therefore, CD(m) Re(p q) well p q removed, results
0
0
new plan bdp
= hS, B 0 , 0 i. show bdp
valid according Theorem 2.
Since bdp valid, active threat causal link bdp according
Theorem 2, due deordering p q, deleter q becomes new threat
0
. However, hr,m,pi contained bp contain
causal link hr,m,pi bdp
q, therefore, according Definition 6, hr,m,pi protected q, i.e., q becomes
0
remains valid.
inactive threat. result, bdp
Rule 3. Let bdp = hS, B, valid block decomposed p.o. plan, si sj basic ordering
whose removal cause threat protection ordering removed, CD(m)
Re(si sj ). Let bi bj two blocks, si bi , sj bj , bi bj = .
CD(m) removed Re(si sj ) bj delete m.
Theorem 5. Deordering according Rule 3 preserves plan validity.
Proof. Let bdp = hS, B, valid block decomposed p.o. plan, p q basic
ordering constraint, p, q CD(m) Re(p q). order meet condition
Rule 3, let us assume bq block includes q DP(m) Re(q s)
every deleter bq (if exist) ordered bdp (as shown
Figure 9 (ii)). Therefore meets condition bq must delete m. Also, assume bp
block contains {p}, bp , bq ordered ordering constraints.
Therefore, CD(m) Re(p q) well p q removed, results new plan
0
0
valid according Theorem 2.
= hS, B 0 , 0 i. show bdp
bdp
Since bdp valid, active threat causal link bdp according
Theorem 2, due deordering p q, deleter q becomes new threat
0
causal link hr,m,pi bdp
. However, q contained bq contain hr,m,pi,
delete m; therefore, according Definition 6, hr,m,pi protected q, i.e.,
0
q becomes inactive threat. result, bdp
satisfies condition Theorem 2
therefore remains valid.
Rule 4. Let bdp = hS, B, valid block decomposed p.o. plan, si sj basic ordering
whose removal cause threat protection ordering removed, DP(m)
Re(si sj ). Let bj block, sj bj si
/ bj . DP(m) removed
Re(si sj ) bj includes every step s0 PC(m) Re(sj s0 ).
Theorem 6. Deordering according Rule 4 preserves plan validity.
385

fiSiddiqui & Haslum

Figure 10: Formation blocks removing reason DP(m) behind basic ordering
p q.

Proof. Let bdp = hS, B, valid block decomposed p.o. plan, let p q basic
ordering constraint (where p, q S). Let bq block includes steps r
hq,m,ri, hq,m,si causal links bdp (as shown Figure 9 (ii)). Hence,
meets condition Rule 4. Also, assume bp block contains {p} bp , bq
ordered ordering constraints. result, DP(m) Re(p q) well
0
0
= hS, B 0 , 0 i. show bdp
p q removed, results new plan bdp
satisfies condition Theorem 2 therefore remains valid.
Since bdp valid, active threat causal link bdp according
Theorem 2, due deordering p q, deleter p becomes new threat
0
. However, causal links contained
causal links hq,m,ri hq,m,si bdp
bq contain p, therefore, according Definition 6, protected p,
0
i.e., p becomes inactive threat. result, bdp
remains valid.

Even when, applying four rules above, find blocks bi bj remove
reasons ordering si sj , thus permitting ordering removed,
guaranteed two blocks bi bj unordered. may ordered
bi contains step si ordered step bj (whether sj
another). Even not, block b B contains bi (or bj both),
b still ordered bj (resp. bi ) due constraint +
hsi , sj i, blocks bi bj still ordered, sense bi appear bj
linearisation consistent block decomposition.
386

fiContinuing Plan Quality Optimisation

2.6 Block Deordering Algorithm
previous subsection described four conditions (Rules 14) adding blocks
decomposition allows reasons ordering constraints, thus ultimately ordering
constraints themselves, removed preserving plan validity. Next, describe
algorithm uses rules perform block deordering, i.e., convert sequential
plan seq block decomposed p.o. plan bdp .
algorithm divided two phases. First, apply step-wise deordering procedure convert seq p.o. plan pop = (S, 0 ). used Kambhampati
Kedars (1994) algorithm this, simple shown produce
good results (Muise et al., 2012), even though optimality guarantee.
step-wise plan deordering, extend ordering blocks: two blocks ordered
bi bj exist steps si bi sj bj si sj neither block contained
(i.e., bi 6 bj bj 6 bi ). case, steps bi must precede steps bj
linearisation block decomposed plan. also extend reasons ordering
(PC, CD DP) ordering constraints blocks, set propositions
produced, consumed deleted block given Definition 5. Recall responsible
step step block causes produce, consume delete proposition.
example, b produces p, must step b produces p, step
block ordered deletes p; say step responsible b producing p.
next phase block deordering, converts p.o. plan pop = (S, )
block decomposed p.o. plan bdp = (S, B, 0 ). done greedy procedure,
examines basic ordering constraint bi bj turn attempts create blocks
consistent decomposition built far allow ordering
removed. core algorithm Resolve procedure (Algorithm 1). takes
input two blocks, bi bj , ordered (one blocks may consist single step),
tries break ordering extending larger blocks, b0i b0j . procedure
examines reason ordering constraint extends one blocks remove
reason, following rules given previous subsection. this, sets
propositions produced, consumed deleted new blocks (b0i b0j ) recomputed
(following Definition 5) new reasons ordering constraint arisen
steps included added Re(b0i b0j ). repeated
either reason ordering remains, case new blocks returned
procedure safely unordered, reason cannot removed, case
deordering possible (signalled returning null). function Intermediate(bi , bj )
returns set steps ordered bi bj , i.e., {s | bi + + bj }. Algorithm
1 refers nearest step s0 preceding following another step s, means step
smallest number basic ordering constraints s0 s.
applied Resolve procedure basic ordering constraint would obtain
collection blocks break orderings. collection
necessarily valid decomposition, since blocks may partial overlap.
find valid decomposition, use greedy procedure. repeatedly examine basic
ordering constraint bi bj call Resolve find two extended blocks b0i bi b0j bj
allow ordering removed. iteration, constraints checked order
beginning plan. block, added bdp , removed
387

fiSiddiqui & Haslum

Algorithm 1 Resolve ordering constraints pair blocks.
1: procedure Resolve(bi , bj )
2:
Initialise b0i = bi , b0j = bj .
3:
Re(b0i b0j ) 6=
4:
r Re(b0i b0j )
5:
r = PC(p)
// try Rule 1
6:
Find responsible step b0i nearest s0 6 b0i consumes
p s0 + s.
7:
s0 exists
8:
Set b0i = b0i {s0 } Intermediate(s0 , b0i ).
9:
else return null
10:
else r = DP(p)
// try Rule 4
11:
Find responsible step b0j s0 6 b0j
hs, p, s0 causal link.
12:
s0 exists
13:
Set b0j = b0j {s0 } Intermediate(b0j , s0 ).
14:
else return null
15:
else r = CD(p)
// try Rule 3
16:
Find responsible step b0j nearest s0 6 b0j produces p,
+ s0 .
17:
s0 exists
18:
Set b0j = b0j {s0 } Intermediate(b0j , s0 ).
19:
else
// try Rule 2
20:
Find responsible step b0i nearest s0 6 b0i produces
p, s0 + s.
21:
s0 exists
22:
Set b0i = b0i {s0 } Intermediate(s0 , b0i ).
23:
else return null.
24:
Recompute Re(b0i b0j ).
25:

return (b0i , b0j ).

accommodate another block partially overlaps existing block throughout
procedure, even later (rejected) block could produce deordering one
created earlier. Since choice deordering apply greedy, result guaranteed
optimal. b0i b0j cannot added decomposition (because one
partially overlaps existing block), consider blocks ordered immediately
bi , check orderings broken simultaneously, using union
blocks returned Resolve ordering constraint. (Symmetrically, also check
set blocks immediately bj , though rarely useful.) additional
388

fiContinuing Plan Quality Optimisation

heuristic, discard two blocks basic ordering constraint step
internal one blocks (i.e., preceding following steps within
block) step outside block.
ordering removed, inner loop exits ordering relation updated
new constraints b0i blocks ordered bj b0j blocks
ordered bi . done checking three reasons (PC, CD DP) based
sets propositions produced, consumed deleted b0i b0j . inner loop
restarted, ordering constraints previously could broken checked
again. done removing ordering constraints make possible resolution
constraints, since removal orderings change set steps intermediate
two steps.
main loop repeats deordering consistent current decomposition found. iteration runs polynomial time, know upper
bound number iterations. Note, however, procedure anytime,
sense interrupted running completion, result end last completed iteration still block deordering plan. BDPO2, use time-limit 5
minutes whole deordering procedure. However, almost every problem considered
experiments (described Section 3.1), block deordering finishes seconds
(except problems Visitall domain, takes couple minutes).
summary, deordering makes structure plan explicit, showing us parts
necessarily sequential (because dependency interference) independent non-interfering. Block deordering improves creating on-the-fly
hierarchical decomposition plan, encapsulating dependencies interferences
within block. Considering blocks, instead primitive actions, units partial
ordering thus enables deordering plans greater extent, including cases deordering possible using standard, step-wise, partial order plan notion. impact
block decomposition anytime performance plan quality optimisation system
discussed Section 3.6.

3. System Overview
BDPO2 post-processing-based plan quality optimisation system. Starting initial
plan, seeks optimise parts plan, i.e. subplans, replacing lower-cost
subplans. refer subplans candidates replacement windows.
better plan found certain conditions met, starts new
plan. viewed local search, using large neighborhood search (LNS)
strategy, neighborhood plan defined set plans
reached replacing window new subplan. local search plain hill-climbing:
move strictly better neighbouring plan. LNS algorithms, searching
better plan neighbourhood done formulating local optimisation problems,
solved using bounded-cost subplanners.
Block deordering, described previous section, helps identify candidate windows
providing large set possible plan linearisations; block decomposition also used
windowing strategies. window subsequence linearisation
block deordered input plan. However, represent window slightly different
389

fiSiddiqui & Haslum

way, partitioning blocks part replaced (w), ordered
(p) (q) part.
Definition 8. Let bdp = (S, B, ) block decomposed p.o. plan. window bdp
partitioning B sets p, w, q, bdp linearisation consistent
{bp bw bq | bp p, bw w, bq q}.
window defines subproblem, problem finding plan
fill gap left removing steps w linearisation bdp consistent
window. problem formally defined follows.
Definition 9. Let bdp = (S, B, ) block decomposed p.o. plan planning problem
, hp, w, qi window bdp , s1 , . . . , s|p| , s|p|+1 , . . . , s|p|+|w| , s|p|+|w|+1 , . . . , sn linearisation bdp consistent window. subproblem corresponding hp, w, qi,
sub , atoms actions . initial state sub , Isub , result
progressing initial state s1 , . . . , s|p| (i.e., applying s1 , . . . , s|p| I),
goal sub , Gsub , result regressing goal sn , . . . , s|p|+|w|+1 .
Theorem 7. Let bdp = (S, B, ) block decomposed p.o. plan planning problem , hp, w, qi window bdp , sub subproblem corresponding window,
s1 , . . . , s|p| , s|p|+1 , . . . , s|p|+|w| , s|p|+|w|+1 , . . . , sn linearisation sub constructed
0
0
0 = s0 , . . . , s0 plan
from. Let w
sub . s1 , . . . , s|p| , s1 , . . . , sk , s|p|+|w|+1 , . . . , sn
1
k
valid sequential plan .
Proof. proof straightforward. subsequence s1 , . . . , s|p| applicable initial
state , I, and, construction sub , results initial state sub , Isub . Hence
s1 , . . . , s|p| , s01 , . . . , s0k applicable I, and, construction sub , results state
sG satisfies goal sub , Gsub . Since Gsub result regressing goal ,
G, s|p|+|w|+1 , . . . , sn reverse, follows subsequence applicable sG ,
applying results state satisfying G. (For relevant properties regression,
see, example, Ghallab, Nau, & Traverso, 2004, Section 2.2.2.)
subproblem corresponding window hp, w, qi always solution, form
linearisation steps w. improve plan quality, however, replacement
subplan must cost strictly lower cost w, C(w). amounts
solving bounded-cost subproblems. subplanners used BDPO2
described Section 3.3. return question multiple windows
within plan simultaneously replaced Section 3.5.
Algorithm 2 describes BDPO2 performs one step local search, exploring
neighbourhood current plan. first step block deorder current plan
(line 3). Next, optimisation using bounded-cost subplanner tried systematically
candidate windows (lines 419), restart condition met (line 18),
local improvements possible, time limit reached. point difference
LNS algorithms used delayed restart, meaning exploration
neighbourhood continue better plan found. helps avoid
local minima, driving exploration different parts current plan. restart
conditions, impact local search, described Section 3.4.
390

fiContinuing Plan Quality Optimisation

Algorithm 2 neighbourhood exploration procedure BPDO2.
1: procedure BDPO2(in , tlimit , banditPolicy, rankPolicy, optSubprob)
2:
Initialize: telapsed = 0, last = , trialLimit[1...n] = 1, windowDB =
3:
bdp = BlockDeorder(in )
4:
telapse < tlimit last locally optimal
5:
windows needed
6:
ExtractMoreWindows(bdp , windowDB, optSubprob)
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

p = SelectPlanner(banditPolicy)
w = SelectWindow(p, rankPolicy, trialLimit, windowDB)
w = null windows extract trialLimit[p] += 1
w = null continue
wnew , searchResult = OptimiseWindow(p, w)
UpdateWindowDB(p, w, wnew , optSubprob, searchResult, windowDB)
C(wnew ) < C(w)
new = Merge(bdp , windowDB)
C(new ) < C(last ) last = new
UpdateBanditPolicy(p, w, wnew , searchResult, banditPolicy)
UpdateRankPolicy(p, searchResult, rankPolicy)
C(last ) < C(in ) restart condition true
return BDPO2 (last , tlimt telapsed , banditPolicy, rankPolicy, optSubprob)
return last

key design goal procedure avoid unproductive time, meaning spending
much time one step trying optimise one window options could
lead improvement left waiting. Therefore, steps done incrementally,
time limit step could take unbounded time.
database (windowDB) stores unique window extracted block deordered
plan, records status (how many times optimisation window tried
subplanner result), structural summary information window. window database populated incrementally (lines 56), applying different
windowing strategies limit time spent number windows added.
limits used 120 seconds 20 windows, respectively. balances time
window extraction optimisation, prevent procedure spending unproductive time. windowing strategies described Section 4. also compute lower
bound cost replacement plan window, using admissible LM-Cut
heuristic (Helmert & Domshlak, 2009). window proven optimal current subplan cost equals bound, previous attempt optimise window exhausted
bounded-cost search space. Already optimal windows are, course, excluded
optimisation. windows added database number windows eligible selected optimisation one subplanner (defined next paragraph)
drops threshold. used 75% current window database size
threshold.
391

fiSiddiqui & Haslum

subplanner use selected using UCB1 multi-armed bandit policy (Auer et al.,
2002), learns repeated trials select often subplanner succeeds
often finding improvements. next window try chosen, among eligible
ones database, according ranking policy. Windows eligible optimisation
chosen subplanner (1) already proven optimal; (2)
tried chosen subplanner current trial limit; (3) overlap
improved window already found. ranking policy heuristic aimed selecting
windows likely improved chosen subplanner. use several ranking
policies switch one next subplanner fails find improvement
number consecutive tries, since indicates current ranking policy may
recommending right windows current problem. threshold used
switching ranking policy 13. (This 2/3 maximum number windows added
window database call ExtractMoreWindows.) ranking policies
described Section 4.6. subplanner given time limit, increased time
retried window. used limit 15 seconds, increasing another
15 seconds retry. limit number times retried
window kept subplanner. Initially set 1, limit increased
subplanner tried every window database (excluding windows
already proven optimal overlap windows better replacement
found) strategy generate new windows (line 9). lower-cost
replacement subplan window found, together improvements already
found current neighbourhood fed Merge procedure, tries
combine several replacements achieve greater overall plan cost reduction. Merge
procedure described Section 3.5.
procedure restarts new best plan, learned bandit policy subplanner selection current ranking policy (for subplanner) carried
next iteration. also keep database subproblems (defined initial
state goal) whose plan cost proven optimal, avoid trying fruitlessly optimise further. window database, contains information specific
current input plan, reset.
remainder section organised follows: next two sections describe
settings used experiments overview main results, respectively.
describe subplanners used BDPO2 (Section 3.3), restart conditions
(Section 3.4) Merge procedure (Section 3.5). Section 3.6 discusses impact
block deordering performance system. windowing strategies ranking
policies described Section 4, details on-line adaptation methods
used presented Section 5.
3.1 Experiment Setup
presenting overview results, outline three different experimental
setups used. experiment setup 2 3 used 182 large-scale instances
21 IPC domains. selection domains instances described below.
experiment 1, included additional medium-sized instances total 219 instances
21 domains. used domains sequential satisficing track
392

fiContinuing Plan Quality Optimisation

2008, 2011, 2014 IPC, except CyberSec, CaveDiving CityCar domains.
(The CyberSec domain slow system parse. two conditional
effects, implementation handle.) also used Alarm Processing
Power Networks (APPN) domain (Haslum & Grastien, 2011). plans used input
BDPO2 plan produced IBaCoP2 (Cenamor, de la Rosa, & Fernandez, 2014)
2014 IPC problems competition, best plan found LAMA
(Richter & Westphal, 2010, IPC 2011 version) 1 hour CPU time problems.
refer base plans. experiments 2 3, selected domain
10 last instances base plan exists. (In domains less 10 instances
solved LAMA/IBaCoP2, total 182 rather 210.) domains
appeared one competition, used instances IPC 2011 set.
experiments run 6-Core, 3.1GHz AMD CPUs 6M L2 cache, 8
GB memory limit every system. comparing anytime performance BDPO2
systems require input plan, count time generate base plan
1 hour CPU time. maximum time allocated generating base plan;
found much quickly.
first experiment, use BDPO2 system. Instead, ran two
subplanners, PNGS IBCS, 30 seconds every subproblem corresponding
window extracted (by six windowing strategies) base plans, excluding
subproblems window proven optimal lower bound obtained
admissible LM-Cut heuristic (Helmert & Domshlak, 2009). experiment provided
information inform design combined window extraction procedure, window
ranking policies, aspects system. present results here,
refer later discuss system components detail.
experiment 2, compare BDPO2 eight anytime planners plan optimisation systems: LAMA (Richter & Westphal, 2010, IPC 2011 version); AEES (implemented
Fast Downward code base; cf. Thayer et al., 2012b); IBCS (as described Section
3.3); Beam-Stack Search (BSS) (Zhou & Hansen, 2005); PNGS, including Action Elimination (Nakhost & Muller, 2010); IBaCoP2 (Cenamor et al., 2014); LPG (Gerevini &
Serina, 2002); Arvand (Nakhost & Muller, 2009). BDPO2 uses PNGS IBCS
subplanners, configured described above. AEES uses LM-Cut (Helmert & Domshlak, 2009) admissible heuristic, FF heuristic, without action costs
inadmissible estimates. BSS uses LM-Cut heuristic. implementation BSS
use divide-and-conquer solution reconstruction, run beam width
500. systems described Section 6.
system run 7 hours CPU time per problem. BDPO2 PNGS
use base plans input, IBCS Beam-Stack Search use base
plan cost initial cost bound. mentioned above, allocated 1 hour CPU time
generating base plan. Therefore, comparing systems planners
starting scratch (LAMA, AEES, IBaCoP2, LPG Arvand), add 1 hour start
delay runtime. Beam-Stack Search much slower planners used
experiment. Therefore, ran 24 hours CPU time, reporting
results divide runtime 4. words, results shown hypothetical
implementation Beam-Stack Search amount search, faster
constant factor 4.
393

fiSiddiqui & Haslum

Experiment 3 uses setup experiment 2, except input BDPO2
best plan found running PNGS 1 hour CPU time, 8 GB memory
limit, base plans. (As mentioned previously, vast majority cases PNGS runs
memory much less time that, cases run 1 hour
limit.) use setup primarily run different configurations BDPO2 analyse
impact different designs (e.g., planner selection window ranking policies,
immediate vs. delayed restart, on) setting input plans already good
quality. comparing anytime result BDPO2 experiment
systems, add 2 hours runtime.
3.2 Overview Results
Figure 11 shows headline result, form average plan quality achieved
BDPO2 systems time-per-problem increases. IPC quality score plan
calculated cref /c, c cost plan cref cost best plan
problem instance found runs systems used experiments. Thus,
higher score reflects lower-cost plan. results Figure 11 experiment 2
3, described previous section. shown Figure 2 (on page 371),
including results compared anytime planning systems. None planners
starting scratch find solution 182 problems: LAMA solves 155 problems,
IBaCoP2 144, Arvand 134, AEES 87 LPG 49. planners, average quality
score shown Figure 11 average problems have,
time, found least one plan. (As previously mentioned, also reason
average quality sometimes falls: first plan, low quality, previously unsolved
problem found, average decrease.) words, metric unaffected
differences coverage. Likewise, none post-processing bounded cost search
methods improve base plans: BDPO2 finds plan lower cost base plan
147 problems, PNGS 133, IBCS 66 Beam-Stack Search 14.
systems, average quality shown Figure 11 taken 182 problems, using
base plan quality score problems system improved on.
majority compared systems show trend similar LAMA, i.e.,
improving quickly early flattening ultimately stagnating. reasons
vary: Memory limiting factor algorithms, notably PNGS, exhausts
8 GB available memory reaching 7 hour CPU time limit 93.7% problems,
LAMA, 67% problems. AEES runs memory
50% problems. hand, planners use limited-memory algorithms,
Beam-Stack Search, LPG Arvand (both use local search), never run
memory thus could conceivably run indefinitely. However, rate
find plan quality improvements small: 4 7 hours, average quality produced
LPG Arvand increases 0.0049 0.0094, respectively. (The latter excludes three
problems solved Arvand first time 4 7 hours; including
brings average down, making increase less 0.002.) increase average
quality achieved BDPO2, starting high-quality plans generated PNGS
base plans, time interval 0.0115.
394

fiContinuing Plan Quality Optimisation

0.96
0.94
0.92


Average Quality Score (Relative IPC Quality Score / Coverage)

0.9













































































































































0.88
0.86



0.84





0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68
0.66
0.64



0.62



BDPO2 PNGS base plans
BDPO2 base plans
PNGS base plans
IBCS base plans
BSS base plans
LAMA scratch
AEES scratch
IBaCoP2 scratch
Arvand scratch
LPG scratch

0.6
0.58



0.56
0.54










































































7

6.5

6

5.5

5

4.5

4

3.5

3

2.5

2

1.5



0.5



0

0.5



1

0.52





Time (hours)

Figure 11: Average IPC quality score function time per problem, set 182
large-scale planning problems. quality score plan cref/c, c cost
plan cref least cost plans problem; hence higher score represents
better plan quality. LAMA, AEES, LPG, Arvand IBaCoP2 planners start
scratch, whereas post-processing (PNGS, BDPO2) bounded cost search (IBCS,
Beam-Stack Search) methods start set base plans; curves delayed 1
hour, maximum time allocated generating base plan. experimental
setup described detail Section 3.1.

395

fiSiddiqui & Haslum

BDPO2
BDPO2
PNGS
= < ? = < ?
Appn
50
20 40
10
Barman
100 90
10
Childsnack
100 30
70
Elevators
60 60
10 10
Floortile
67
78 22
GED
30
20
Hiking
50
70 20
Maintenance 100
100
Nomystery
100
100 100
100
Openstacks
Parcprinter
100
22 100
22
43
43 14
Parking
Scanalyzer
75 12
38 12
Sokoban
100
100
Tetris
80 40
60 20
Thoughtful
80 30
50 20
Tidybot
43
29
Transport
60 60
40 40
Visitall
60 60
30 30
Woodworking 70 30
50 20
Overall
66 24 4 47 12 3
Domains

LAMA

AEES

Arvand

LPG

IBCS

BSS

PNGS

IBaCoP2

= < ? = < ? = < ? = < ? = < ? = < ? = < ? = <
40
10
40
10 70 20 20

?

10
10

10
11

80 70
40 30

20 20
11 11

10

33

10
50
29

50

50 50

33

11

50

50

50 50

50

22 33

67

22

88 88
11 33

43 43

20
71 29

33

33

43

43

12 12
67
11
29
75 12
67

29 14

10

12

14

10 10
18 14

8

1

1

1

2

1

8

1 12 2 3 13 1

20 10
8 2

1

Table 1: plan improvement method, percentage instances found
plan cost matching best plan (=); found plan strictly better method
(<); found plan known optimal, i.e., matched highest lower bound
(?). percentage instances domain shown Figure 12. (Zeros
omitted improve readability.) BDPO2 PNGS result BDPO2 experiment
3; results experiment 2 (see Section 3.1).

draw two main conclusions: First, BDPO2 achieves aim continuing quality
improvement even time limit grows. fact, continues find better plans, though
decreasing rate, even beyond 7 hour time limit used experiment. Second,
combination PNGS BDPO2 achieves better result either alone. Partly
work well different sets problems figure showing
average, BDPO2 sometimes produces better result started best plan
found PNGS also domains BDPO2 already outperforms PNGS start
base plans (e.g., Elevators Transport). However, also seen
opposite domains (e.g., Floortile Hiking), starting BDPO2 worse
input plan often yields better final plan. seen Figure 12, provides
detailed view. shows problem cost best plan found
system 7 hour total time limit, scaled interval base plan cost
highest known lower bound (HLB) plan problem. (Lower bounds
obtained variety methods, including several optimal planners; cf. Haslum,
2012.) 18 182 problems excluded Figure 12: 3 cases, base plan cost
already matches lower bound, improvement possible; another 15 problems,
method improves base plans within stipulated time. (The Pegsol domain
appear graph, base plans one optimal, method
improves cost last one.)
396

fiContinuing Plan Quality Optimisation

Base Plans









































Best cost achieved (normalised)



































Nomystery

Maintenance

Ged

Floortile

Barman

Hiking



Elevators



Childsnack



Appn

HLB







LAMA scratch
AEES scratch
Arvand scratch
LPG scratch
IBaCoP2 scratch
PNGS base plans
IBCS base plans
BSS base plans
BDPO2 base plans
BDPO2 PNGS base plans

Base Plans




































Best cost achieved (normalised)

































HLB



LAMA scratch
AEES scratch
Arvand scratch
LPG scratch
IBaCoP2 scratch
PNGS base plans
IBCS base plans
BSS base plans
BDPO2 base plans
BDPO2 PNGS base plans
Woodworking

Visitall

Transport

Tidybot

Thoughtful

Tetris

Sokoban

Scanalyzer

Parking

Parcprinter

Openstacks





Figure 12: Best plan cost, normalised interval cost base plan
corresponding highest known lower bound, achieved different anytime plan
optimisation methods experiment 2, BDPO2 experiments 2 & 3 (see Section
3.1).

397

fiSiddiqui & Haslum

Table 1 provides different summary information Figure 12, showing
domain system percentage instances found plan cost (1)
matching best plan instance; (2) strictly better method;
(3) matching lower bound, i.e., known optimal. aggregate, combination
BDPO2 PNGS base plans achieves best result three measures.
However, 5 domains (GED, Hiking, Openstacks, Parking, Tidybot), LAMA finds
plans strictly better method. tried using LAMA
one subplanners BDPO2, lead better results overall.
domains, OpenStacks GED, smallest improvable subplan often whole,
almost whole, plan, LAMA finds improvement plan searching
longer time. Although BDPO2 increases time limit given subplanners
retry, average time limit, across local optimisation attempts experiment,
18.48 seconds. Thus, strategy searching quick improvements plan parts
work well domains.
3.3 Subplanners Used Window Optimisation
subplanners used BDPO2 used find plan window subproblem,
stated Definition 9, cost less cost current window, C(w).
considered three subplanners:
(1) Iterated bounded-cost search (IBCS), using greedy search admissible heuristic pruning.
(2) Plan neighbourhood graph search (PNGS), including action elimination technique (Nakhost & Muller, 2010).
(3) Restarting weighted A? (Richter et al., 2010), implemented LAMA planner.
However, experimental setups described previous section, BDPO2 uses
two subplanners, IBCS PNGS. two reasons choosing two: First,
show good complementarity across domains. example, IBCS significantly better PNGS APPN, Barman, Floortile, Hiking, Maintenance, Parking, Sokoban,
Thoughtful Woodworking domains, PNGS better Elevators, Scanalyzer,
Tetris, Transport Visitall domains. Second, learning policy use subplanner selection learns faster smaller number options. Therefore, adding third
subplanner improve overall performance BPDO2, given limited time per
problem, subplanner complements two well, i.e., performs well
significant fraction instances two not. set benchmark
problems used experiment, case. (A different set benchmarks
could course yield different outcome.) experiment comparing effectiveness
three subplanners, individually well combination IBCS PNGS
learning policy, BDPO2 presented Section 5.2 page 420.
solve bounded-cost problem, IBCS uses greedy best-first search guided
unit-cost FF heuristic, pruning states cannot lead plan within cost bound
using f-value based admissible LM-Cut heuristic (Helmert & Domshlak, 2009).
implemented Fast Downward planner. search complete: plan
398

fiContinuing Plan Quality Optimisation

within cost bound, prove exhausting search space, given sufficient
time memory. bounded-cost search return plan within cost
bound. get best subplan possible within given time limit, iterate it: whenever
plan found, long time remains, search restarted bound set
strictly less cost new plan.
PNGS (Nakhost & Muller, 2010) plan improvement technique. searches subgraph state space around input plan, limited bound number states,
lower cost plan. better plan found exploration limit increased (usually
doubled); continues time memory limit reached. Like IBCS,
iterate PNGS get best subplan possible within given time limit. improves
current subplan, process repeated around new best plan.
LAMA (Richter & Westphal, 2010) finds first solution using greedy best-first search.
switches RWA? (Richter et al., 2010) search better quality solutions.
3.4 Restart
restart condition determines trade-off exploring neighbourhood
current solution continuing local search different parts solution space.
obvious choice, one used LNS algorithms, restart
new best solution soon one found. call immediate restart. However,
found continuing explore neighbourhood current plan even better
plan found, merging together several subplan improvements, described
Section 3.5 below, often produces better results. call delayed restart.
Setting right conditions make delayed restart critical success
approach. used disjunction two conditions: First, union
improved windows found neighbourhood covers 50% steps input plan.
Recall continue exploration loop (Algorithm 2) improvement
found, windows overlap already improved window excluded
optimisation. drives procedure search improvements different
parts current plan, helps avoid certain myopic behaviour occur
immediate restarts: restarting new best plan, get new block
decomposition new set windows; lead attempting re-optimise
part plan improved, even several restarts, may lead
local optimum time-consuming escape. second condition 39 consecutive
subplanner calls failed find improvement. threshold 39 three
times threshold switching ranking policy (cf. description Algorithm 2
beginning section). means 39 attempts tried optimise
13 promising windows, among remaining eligible ones, recommended
ranking policies, without success. suggests improvable windows
found, none ranking policies good current neighbourhood.
Making restart point allows exploration return parts plan
intersect already improved windows, thus increasing set eligible windows.
average plan quality, function time-per-problem, achieved BDPO2 using
immediate restart delayed restart based conditions shown top
two lines Figure 13 (page 403). experiment, configurations run using
399

fiSiddiqui & Haslum

Algorithm 3 Merge Improved Windows
1: procedure Merge(bdp , windowDB)
2:
Initialise bdp = bdp
3:
W = improved windows windowDB sorted cost reduction (C(w) C(wnew ))
4:
W 6=
5:
(hp, w, qi, wnew ) = pop window highest C(w) C(wnew ) W
6:
bdp = ReplaceIfPossible(bdp , hp, w, qi, wnew )
7:
W = RemoveConflictingWindows(W, bdp )
8:

return bdp

setup experiment 3, described Section 3.1 page 392. seen,
delayed restart yields better results overall. Compared BDPO2 immediate restart,
achieves total improvement 12% higher. However, found immediate restart
work better instances, especially Visitall Woodworking domains,
BDPO2 immediate restart found better final plan nearly 20% instances.
average number iterations (i.e., steps LNS) done BDPO2 using
delayed restart condition 3.48 per problems across domains considered
experiment; highest average single domain 8.7, Thoughtful solitaire.
immediate restart average domains increases 4.87. words,
configurations BDPO2 spend significant time exploring neighbourhood plan.
anytime performance curve Figure 13 shows additional time spent
neighbourhood using delayed restarts pays off.
3.5 Merging Improved Windows
Delayed restarting would benefit without ability simultaneously replace
several improved windows current plan. improved windows always nonoverlapping (because better subplan window found, windows overlap
longer considered optimisation) corresponding subproblems may
generated different linearisations block deordered plan.
this, replacement subplans may additional preconditions delete effects
replaced windows not, lack add effects. Thus, may
linearisation permits two windows simultaneously replaced.
Merge procedure shown Algorithm 3 greedy procedure. maintains
times valid block deordered plan (bdp ), meaning precondition block
supported causal link active threat. (Recall block context
block consists single step.) Initially, input plan (bdp ),
causal links, ordering constraints, computed block deordering.
procedure gets improved windows (W ) window database, tries replace
current plan bdp order contribution decreasing plan cost, i.e.,
cost replaced window (C(w)) minus cost new subplan (C(wnew )).
first replacement always succeeds, since, construction subproblem,
linearisation input plan wnew valid (cf. Theorem 7). Subsequent
replacements may fail, case Merge proceeds next improved window W .
400

fiContinuing Plan Quality Optimisation

Since replacing window different subplan may impose new ordering constraints,
remaining improved windows conflict partial order current plan
removed W .
ReplaceIfPossible function takes current plan (bdp ), returns updated plan (which becomes current plan), plan replacement
possible. replacement subplan (wnew ) made single block whose steps totally ordered. preconditions effects block, replaced window
(w), computed according Definition 5 (page 378). atom pre(wnew )
also w, existing causal link kept; likewise, causal links effect add(w)
also add(wnew ) kept. links unthreatened consistent
order, since plan valid replacement. additional precondition
new subplan, pre(wnew ) \ pre(wi ), causal link hbp , m, bc bdp
producer replaced window (bp w), consumer (bc 6 w),
atom link produced replacement subplan (m 6 add(wnew )), new
causal link must found. Given consumer (bc ) atom requires (m pre(bc )),
procedure tries following two ways creating unthreatened causal link:
(C1) block b0 + bc add(b0 ), every threatening block (i.e.,
b00 del(b00 )), either b00 b0 bc b00 added existing plan ordering
without contradiction, b0 chosen, ordering constraints necessary resolve
threats (if any) added.
(C2) Otherwise, block b0 add(b0 ) unordered w.r.t. bc ,
every threatening block either b00 b0 bc b00 enforced, b0 chosen,
causal link (implying new ordering b0 bc ) threat resolution ordering constraints
(if any) added plan.
two tried order, C1 first C2 C1 fails. neither rule find
required causal link, replacement fails. wnew may also threaten existing causal
links bdp w not. threatened link, hbp , m, bc i, procedure tries
resolve threat three ways:
(T1) consumer bc ordered w linearisation corresponding
subproblem (bc p), bc wnew consistent, threat removed adding
ordering.
(T2) producer bp ordered w linearisation corresponding subproblem (bp q), wnew bp consistent, threat removed adding ordering.
(T3) new, unthreatened causal link supplying bc found one two
rules C1 C2 above, threatened link replaced new causal link.
rules tried order, none resolve threat, replacement
fails.
non-basic ordering constraints blocks w may disappear w
replaced wnew ; likewise, ordering constraints w rest
plan may become unnecessary, wnew may delete every atom w deletes
may preconditions w, thus removed. may make pairs
blocks b, b0 plan ordered replacement unordered, thus create
new threats. new threats checked ReplaceIfPossible, found
resolved restoring ordering constraint lost.
401

fiSiddiqui & Haslum

Lemma 8. current plan bdp valid, wnew solves subproblem corresponding
window hp, w, qi, plan returned ReplaceIfPossible valid.
Proof. procedure ensures every precondition every step supported causal
link active threat: link either existed plan replacement (and
new threats created replacement resolved ordering constraints),
added procedure. Thus, replacement succeeds, resulting plan valid
according Theorem 2. replacement fails, plan returned current plan,
bdp , unchanged, valid assumption.
Theorem 9. input plan, bdp valid, plan returned Merge.
Proof. Immediate Lemma 8 induction sequence accepted replacements.

3.6 Impact Plan Decomposition
neighbourhood explored step LNS BDPO2 defined substituting
improved subplans current plan. subplan considered local optimisation
subsequence linearisation block deordering current plan. Obviously,
also restrict windows consecutive subsequences totally ordered input
plan; fact, similar approaches plan optimisation adopted restriction (Ratner
& Pohl, 1986; Estrem & Krebsbach, 2012; Balyo, Bartak, & Surynek, 2012). section,
address question much block deordering contributes performance
BDPO2.
preliminary experiment (setup 1, described Section 3.1 page 392)
observed 75% subproblems improved subplan found
correspond non-consecutive part sequential input plan. However, prove optimising 25% subplans found without
deordering would lead equally good end result.
Therefore, conducted another experiment, using setup experiment 3 (described Section 3.1). experiment, ran BDPO2 separately different degrees
plan decomposition: (1) block deordering (as default BDPO2 configuration,
one used experiments 2 3 presented Section 3.2 page 394). (2) standard, i.e., step-wise, plan deordering only. configuration, used Kambhampati
Kedars (1994) algorithm (described Section 2.3) plan deordering. (3) Without
deordering, i.e., passing totally ordered input plan directly LNS process.
addition, configurations run immediate restarting
delayed restarting, described Section 3.4.
Figure 13 shows average IPC plan quality score function time-per-problem
achieved configurations BDPO2. shows simple clear picture:
immediate restart, LNS applied block deordered plans outperforms LNS applied
step-wise deordered plans, turn outperforms use totally ordered plans.
total improvement, measured increase average IPC plan quality score,
achieved BDPO2 without deordering 28.7% less achieved best
configuration. also see deordering enabler delayed restarting:
block step-wise deordering, delayed restarting boosts performance LNS
402

fiContinuing Plan Quality Optimisation

0.962












0.958








0.954







0.95



0.946










0.942






















0.938









0.934



0.93

6

5.5

5

4.5

1

0.5

0

0.922

4



3.5



3



0.926

BDPO2 delayed restart block deordered plans
BDPO2 immediate restart block deordered plans
BDPO2 delayed restart standard partially ordered plans
BDPO2 immediate restart standard partially ordered plans
BDPO2 delayed restart totally ordered plans
BDPO2 immediate restart totally ordered plans
2.5



2




1.5

Average Quality Score (Relative IPC Quality Score / Coverage)



Time (hours)

Figure 13: Average IPC quality score function time per problem BDPO2 applied
totally ordered input plan; standard (step-wise) deordering plan;
block deordering plan. plan type, system run two configurations: delayed restarting immediate restarting (cf. Section 3.4
page 399). experiment run setup 3, described Section 3.1 page 392.
time shown runtime BDPO2 (i.e., without 2 hour delay
generating input plans, shown Figure 11). Note also y-axis truncated:
curves start average quality input plans, 0.907.

403

fiSiddiqui & Haslum

plan optimisation 12% 14.7%, respectively, totally ordered plans
significant effect.
Deordering increases number linearisations therefore enables many
distinct candidate windows created. However, recall BDPO2s neighbourhood
exploration procedure (Algorithm 2) interleaves incremental window generation optimisation attempts; many windows could generated current plan may
never generated restart occurs. Thus, average number windows generated
iteration reflect difference performance. (With block deordering,
average number windows generated 277.23, 183.19 remain filtering,
totally ordered plans 376.8, 149.94 filtering; using immediate
restart.) deordering helps windowing strategies generate windows
easily optimised. Recall neighbourhood exploration retry subplanner
window (with higher time limit) windows tried
subplanner. average number optimisation attempts, using either subplanner,
window selected optimisation least once, around 1.7 either block deordering
standard deordering used input plan. Without deordering, however,
average number attempts higher, high domains: leaving
highest 5% neighbourhoods encountered, average slightly 2;
10% plan neighbourhoods average number attempts 5,
cases 10. words, generating windows totally ordered plan
causes procedure spend, average, time window improving
plan found.
hand, noted Section 3.2, domains subplanners need
runtime find better plans improvable windows, BDPO2 configuration without
deordering find better plan default configuration 26 182 problems.
current BDPO2 system, subplanner time limit increased window
retried. procedure either attempts candidate windows likely improved
(for example, indicated window ranking policies described Section 4.6)
frequently, varies amount time given optimise window may perform better.
optimal amount deordering plan may well different problem problem. averaged across set benchmark problems, deordering
unarguably better none.

4. Windowing Strategies Ranking Windows
window subplan linearisation block deordered plan, extracted order
attempt local optimisation. section describes strategies use generate
rank windows, experimental evaluation impact systems performance.
Recall Definition 8 (page 390) window represented triple hp, w, qi,
w set blocks replaced, p q sets blocks ordered
w, respectively, linearisation. block decomposed p.o. plan
many linearisations, producing many possible windows typically far many
attempt optimise all. windowing heuristic procedure extracts reduced
set windows, hopefully including promising ones, systematic way.
404

fiContinuing Plan Quality Optimisation

Figure 14: block deordered plan transformation extended blocks: blocks b1
b3 merged single block, blocks b5 b6.

Windowing heuristics
Rule-based
Cyclic thread
Causal followers

Generated
Basic
Ext.
108
35
59
47
72
45

filtered
Basic
Ext.
59
22
45
31
41
20

Improved
Basic
Ext.
23
9
15.5
11
15.5
7

Impr./Gen.
Basic
Ext.
0.21
0.26
0.26
0.23
0.22
0.16

Table 2: total number (in thousands) windows generated, filtered out,
finally improved, using different windowing heuristics different block types (basic
extended). number possible windows sequential input plans,
even considering deordering, 1.47 million. rightmost pair columns shows
rate success, meaning fraction improved windows generated windows.
numbers results experiment 1 (described Section 3.1 page 392).
present three windowing heuristics, called rule-based, cyclic thread, causal followers
heuristics. described detail following subsections.
heuristic applied two types block basic extended one time.
Basic blocks blocks generated block deordering. (For purpose windowing,
step included block created block deordering considered
block own.) Extended blocks created merging basic blocks block
deordered plan form complete non-branching subsequences. block bi
immediate predecessor block bj , bj immediate successor bi ,
merged one extended block. Algorithm 4 shows procedure extended block
formation. (IP(b) denotes set bs immediate predecessors, IS(b) bs immediate
successors.)
Algorithm 4 Computing extended blocks.
1: Bext Bbasic
2: bi , bj Bext : IP(bj ) = {bi }, IS(bi ) = {bj }
3:
Bext Bext {bi bj } \ {bi , bj }
process illustrated example Figure 14. Note blocks b5
b2 merged one extended block. although b5
immediate successor b2, b2 immediate predecessor b5. Extended blocks
useful allow windowing heuristics capture larger windows.
experiment results show windows different sizes useful different domains:
405

fiSiddiqui & Haslum

Algorithm 5 Extract Candidate windows
/* global array strategy[1..6] stores state windowing strategy */
1: procedure ExtractMoreWindows(bdp , windowDB, optSubprob)
2:
W =
3:
tlimit = initial time limit Tincrement
4:
telapsed < tlimit |W | < nWindowsLimit
5:
= NextWindowingStrategy()
6:
= null break /* windowing strategies exhausted */
7:
W = strategy[i].GetWindows(bdp , windowDB, optSubprob,
nWindowsLimit |W |, tlimit telapsed )
8:
telapsed tlimit W = tlimit += Tincrement
9:

windowDB.Insert(W )

example, larger windows likely improved Pegsol, Openstacks
Parcprinter domains, optimising smaller windows better Elevators, Transport,
Scanalyzer Woodworking domains.
windowing strategy windowing heuristic applied block type. Thus, use
total six different strategies. strategies contributes improvable
windows generated strategies (cf. Section 4.4,
particular Table 3 page 411). Thus, are, sense, useful.
hand, size set windows generates fraction improvable
windows set varies strategies, sense useful
others.
Table 2 shows results first experiment, systematically tried two
subplanners (PNGS IBCS) every window generated (and filtered out)
windowing strategy 219 input plans. table shows total number (in thousands)
windows generated, remain filtering, finally improved
least one two subplanners. experiment, windows filtered
window cost matched lower bound given admissible LM-Cut heuristic
(Helmert & Domshlak, 2009). experiment setup described Section 3.1 (on
page 392). first observation strategies selective. number
windows could potentially generated, even without considering deordering, i.e.,
taking subsequences totally ordered input plans, 1.47 million. Thus,
even prolific strategy generates less tenth possible windows. Second,
used rate success, meaning fraction windows generated improved
subplanners used experiment, order strategies. order
follows:
1. Rule-based heuristic extended blocks.
2. Cyclic thread heuristic basic blocks.
3. Cyclic thread heuristic extended blocks.
4. Causal followers heuristic basic blocks.
5. Rule-based heuristic basic blocks.
6. Causal followers heuristic extended blocks.
406

fiContinuing Plan Quality Optimisation

neighbourhood exploration procedure (Algorithm 2 page 391) adds windows
database incrementally, calling ExtractMoreWindows procedure shown
Algorithm 5. procedure selects next strategy try, cycling
order above, asks strategy generate specified number windows,
limited time. strategy keeps state (what part heuristic
applied part plan), next time queried resume
generating new windows. windows possible given strategy
generated, say strategy exhausted. windowing strategies discard (1)
windows known optimal, either cost matches lower bound
given admissible LM-Cut heuristic (Helmert & Domshlak, 2009),
stored set optimally solved subproblems, (2) windows overlap
already improved window. windows eligible optimisation (cf. Section 3),
generating redundant. selected strategy finishes without generating enough
windows time remains, next not-yet-exhausted strategy order queried,
on, either |W | = nWindowsLimit time up. windows generated,
strategies still exhausted, time limit increased.
4.1 Rule-Based Windowing Heuristic
first version BDPO (Siddiqui & Haslum, 2013b) used single windowing strategy,
based applying fixed set rules extended blocks. strategy complements new windowing heuristics well, kept BDPO2.
rule applied block b block deordered plan bdp selects set
blocks go replaced part (w) based relation b. ensure
window consistent block deordering (i.e., consistent linearisation,
stated Definition 8 page 390), blocks constrained ordered
blocks window must also included. call intermediate blocks, formally
defined follows.
Definition 10. Let bdp = hS, B, block decomposed p.o. plan. intermediate
blocks B B IB(B) = {b | b0 , b00 B : b0 b b00 }.
Let b block bdp , let Un(b) set blocks ordered w.r.t. b,
IP(b) immediate predecessors b, IS(b) immediate successors. rules used
windowing heuristic are:
1. w0 {b}.
2. w0 {b} IP(b).
3. w0 {b} IS(b).
4. w0 {b} Un(b).
5. w0 {b} Un(b) IP(b).
6. w0 {b} Un(b) IS(b).
7. w0 {b} Un(b) IP(b) IS(b).
8. w0 {b} Un(b) IP({b} Un(b)).
9. w0 {b} Un(b) IS({b} Un(b)).
407

fiSiddiqui & Haslum

Figure 15: Window formation applying 1st rule rule-based windowing heuristic
block b1, i.e., w {b1}, p Un(b1). unordered block b2 placed
predecessor set. Note window optimised removing s3 step
causal link successors.

10. w0 {b} Un(b) IP({b} Un(b)) IS({b} Un(b)).
Given blocks selected one rules above, partitioning blocks hp, w, qi
made setting w = w0 IB (w0 ) assigning p block ordered
unordered w, q block ordered w. Figure 15 shows example
rule-based windowing, 1st rule applied block b1. Applied blocks,
rules produce duplicates; course, unique windows kept.
first rules, include fewer blocks, generally produce smaller windows,
later rules tend produce larger window (though exact relation, since
number actions block varies). heuristic applies rules block
block deordered plan bdp turn. Rules applied order 1,10,2,9,3,8,4,7,5,6, i.e.,
starting first, last, second, second last, on. blocks
ordered size (descending), ties broken order input plan (in opposite
direction extended blocks).
Recall ExtractMoreWindows repeatedly asks windowing strategy generate limited number windows. ordering blocks rules described helps
ensure heuristic generates varied set windows, including small
large, covering different parts current plan, time queried.
4.2 Cyclic Thread Windowing Heuristic
discover new windowing heuristics, noted key changes decomposed plan
structure frequently occur plan improved. One significant observation
multiple steps input plan add effects, steps together
steps necessarily ordered form subplan often im408

fiContinuing Plan Quality Optimisation

proved. call cyclic behavior. one experiment, found cycles type
either removed plan replaced different cycles 87%
improvements across domains. definition cyclic behavior based
individual atom. Intuitively, atom cyclic behavior multiple producers (as
defined below).
Definition 11. Let bdp = hS, B, block decomposed p.o. plan, Pm
set producers atom m, i.e., sPm add(s). cyclic behavior iff |Pm | > 1.
Note Pm contains init step sI iff I. However, since window never contains
initial step sI , candidate windows formed extended producers instead. step

/ {sI , sG } extended producer atom iff produces m, consumes
s0 6= sI produces ordered block deordered plan.
formal definition follows.
Definition 12. Let bdp = hS, B, block decomposed p.o. plan. step
extended producer atom iff
/ {sI , sG } and:
1. add(s)
2. pre(s) kS\sI add(k) + k.
order form candidate windows respect atom cyclic behavior,
first extract blocks contain least one extended producer atom m.
cyclic thread (cf. Definition 14) formed taking linearisation blocks,
consistent input plan.
Definition 13. Let bdp = hS, B, block deordering sequential plan seq ,
bx , B two blocks bx = . Let hbx , linearisation {bx , }.
hbx , consistent seq least one step bx appears step seq .
way linearise blocks consistent input plan clarified
following example. Assume bx : {sa , sc } : {sb , sd } two blocks
linearise, orderings constituent steps input plan
sa sb sc sd . linearisation starts block contains first
element , i.e., bx case (since contains sa ); updated \bx ,
linearisation continues fashion empty. resulting linearisation
example blocks hbx , i. multiple (nested) blocks contain first element
, innermost one picked. formal definitions thread cyclic thread
follows.
Definition 14. Let bdp = hS, B, block deordering sequential plan seq , EPm
set extended producers atom m, Bm B set blocks,
element Bm contains least one element EPm . thread m, Tm ,
linearisation blocks Bm linearisation consistent seq . thread
called cyclic iff cyclic behavior.
example, plan shown Figure 15(i), atom (at t1 A) cyclic behaviour,
since holds initial state added step s3. extended producers s1, s3
s4, cyclic thread T(at t1 A) = hb1, b2i.
409

fiSiddiqui & Haslum

Finally, candidate windows formed taking consecutive subsequence blocks
(and intermediate blocks, necessary) cyclic thread. Like rule-based windowing,
blocks unordered respect window assigned set blocks
precede window.
Definition 15. Let Tm = b1 , ..., bk cyclic thread atom m. cyclic thread-based
windows cyclic thread Tm Wl,m = {B IB(B) | B = bi , ..., bi+l consecutive
subsequence Tm }, unordered blocks always placed predecessor set.
Also like rule-based windowing heuristic, cyclic thread heuristic generates windows order aims ensure returns varied set windows time
called. first identifies cyclic threads block deordered plan generates
stream candidate windows one cyclic thread another. mentioned,
candidate window formed taking consecutive subsequence blocks (and intermediate blocks required form consistent window) cyclic thread. Given
thread |Tm | blocks, subsequences generated according following order sizes:
1, |Tm |, 2, |Tm | 1, . . . , |Tm |/2. words, subsequence lengths ordered
smallest, biggest, second smallest, second biggest, on. size
order, windows generated moving beginning end thread.
4.3 Causal Followers Windowing Heuristic
third strategy use obtain broader range potentially improvable
windows similar cyclic thread heuristic creates windows subsequences linearisation blocks connected particular atom, different
connections via causal links.
Definition 16. Let bdp = hS, B, block decomposed p.o. plan, c set
causal links . causal followers atom producer p CFhm,pi =
{p, sj , ..., sk |{hp, m, sj i, ..., hp, m, sk i} c } \ {sI , sG }. causal followers (for
producers), CFm , sequence hCFhm,p1 , ..., CFhm,pn i, p1 , ..., pn linearisation
producers m.
words, causal followers atom list sets steps.
set steps, one producer others consumers sj m,
causal link every sj m, i.e., PC(m) Re(s sj ). example, atom (at t1 B)
block deordered plan Figure 15(i) appears two causal links,
producer: hs1, (at t1 B), s2i hs1, (at t1 B), s3i. Thus, causal followers
CF(at t1 B) = h{s1, s2, s3}i.
block deordered plan extract sequence sets blocks corresponding
causal follower steps, according definition below. example, sequence
causal follower blocks CF(at t1 B) plan Figure 15(i) CFB(at t1 B) = h{b1}i,
since steps CF(at t1 B) contained block b1.
Definition 17. Let bdp = hS, B, block decomposed p.o. plan, CFhm,pi
causal followers atom respect producer p S. causal follower
blocks respect producer p atom m, CFBhm,pi , set blocks,
block contains least one element CFhm,pi . causal follower blocks
410

fiContinuing Plan Quality Optimisation

Exclusive


Basic block
66.52%
91.86%

Ext. block
8.14%
33.48%

Rule-based
24.50%
63.22%

Cyclic thread
6.09%
34.01%

Causal followers
17.78%
66.34%

Table 3: Percentage improvable windows found using two block types three
windowing heuristics, total number improvable windows found using blocks
types windowing heuristics. first row gives percentage improvable windows
found one block type (or one windowing heuristic
others), second row gives percentage improvable windows found one
block type (or windowing heuristic). results first experiment, described
Section 3.1.
(for producers), CFBm , sequence hCFBhm,p1 , ..., CFBhm,pn i, p1 , ..., pn
linearisation producers bdp .
Candidate windows formed taking consecutive subsequences sequence
causal follower blocks (with intermediate blocks, necessary). formal definition
given below. Like windowing heuristics, blocks unordered respect
window assigned set blocks precede window.
Definition 18. Let bdp = hS, B, block decomposed p.o. plan, CFBm =
hCFBhm,p1 , ..., CFBhm,pn causal follower blocks m. causal followers-based
windows CFBm Wl,m = {B IB(B) | B = CFBhm,pi ... CFBhm,pi+l
consecutive subsequence CFBm length l}, unordered blocks always placed
predecessor list.
order windows generated causal followers heuristic based
principle cyclic thread heuristic. generates stream candidate
windows causal follower blocks CFBm associated atom turn.
windows consecutive subsequences sets blocks CFBm , lengths chosen
according pattern 1, l, 2, l 1, ..., (l/2), l length CFBm .
4.4 Impact Windowing Heuristics
one single windowing heuristic block type, combination them, guaranteed
find improvable windows. first row Table 3 shows percentage improvable
windows found using one block type (or one windowing heuristic
others), total number improvable windows found using blocks types
windowing heuristics. (The results first experiment, described Section
3.1). shows every windowing heuristic block type contributes improvable
windows found strategies. example, 24.5% improvable windows
found rule-based windowing heuristic (using basic extended blocks).
hand, 36.78% improvable windows found heuristic.
windowing heuristics strengths limitations. rule-based heuristic,
example, generate windows contain sequences extended blocks
fixed length, cyclic thread causal followers heuristics make windows
blocks connected single atom.
411

fiSiddiqui & Haslum

0.963

0.955

0.951




0.947














































0.943




















0.939


































0.935



















0.931





0.927



3

2.5

2

1.5

1

0.5

0

6



0.919

5.5




5

0.923

BDPO2 (combined windowing heuristics)
BDPO2 (random windowing)
BDPO2 (rulebased windowing only)
BDPO2 (causal followers windowing only)
BDPO2 (cyclic thread windowing only)
4.5



4



3.5

Average Quality Score (Relative IPC Quality Score / Coverage)

0.959

Time (hours)

Figure 16: Average IPC quality score function time separate runs BDPO2 using
three windowing heuristics alone, three heuristics combined, random
window generation. run done using setup experiment 3, described
Section 3.1 (on page 392). x-axis shows runtime BDPO2 (i.e., without
2 hour delay generating input plans, shown Figure 11). Note also
y-axis truncated: average quality input plans 0.907.

412

fiContinuing Plan Quality Optimisation

Figure 16 shows impact different windowing heuristics anytime performance
BDPO2, measured average IPC plan quality score achieved function timeper-problem. experiment, ran BDPO2 three windowing heuristics
alone, three combined sequential portfolio, described beginning
section. (The combined portfolio windowing heuristic configuration
BDPO2 presented experimental results Section 3.2, page 394.) also
compare non-heuristic, random windowing strategy, window
formed taking random subsequence blocks random linearisation
block deordered plan. Subsequences chosen distribution window sizes
(measured number actions window) roughly produced
combined heuristics. experiment uses setup 3 (described Section 3.1 page
392), i.e., input plans BDPO2 already high quality. (Their average IPC plan
quality score 0.907.)
predicted data Table 3, using three windowing heuristics
results much worse system performance, since fails find substantial
fraction improvable windows. fact, random window generation better
heuristics own. However, combined portfolio heuristics outperforms
random windowing good margin: total quality improvement achieved
random windowing strategy 17.1% less best BDPO2 configuration.
demonstrates heuristics capture information useful guide selection
windows.
4.5 Possible Extensions Windowing Strategies
Since window formed partitioning plan steps three disjoint sets blocks,
number possible windows exponential. challenge good windowing heuristic
extract reduced set contains windows likely improved. Every windowing
strategy limitations. Hence, always scope developing new windowing
heuristics extending existing ones; one extension discussed section.
combination strategies use may miss improvable windows. example,
long sequence blocks form part cyclic thread causal followers sequence
respect single atom captured heuristics. example
shown Figure 17, three candidate windows, W1, W2 W3, found causal
followers windowing heuristic improvable separately. situation, forming
window union separate windows, found one several strategies, overcome
limitations strategies. example, union W1 W2 improvable.
type composite windows could formed later stages plan improvement
process, individual windowing heuristics exhausted. However,
number composite windows created large set candidate windows
combinatorial thus optimising take long time.
4.6 Window Ranking
Although windowing strategies generate fraction possible windows,
number candidate windows still often large (cf. Table 2). order speed
413

fiSiddiqui & Haslum

Figure 17: Three candidate windows, W1, W2, W3, found causal followers
windowing heuristic atoms (at t1 B), (in p1 t2), (in p2 t3) respectively. None
improvable. However, composite window formed merging W1 W2
improvable substituting delivery package p1 (from location B C) provided
truck t2 truck t1. atom (at t2 C) required
successors (i.e., goal example).

plan improvement process, helpful order windows likely
improved optimised first. role window ranking.
Ranking windows made difficult fact properties improvable windows
vary one another, lot domain domain. example, mentioned
beginning section, larger windows likely improved Pegsol,
Openstacks Parcprinter domains, smaller windows better Elevators,
Transport, Scanalyzer, Woodworking domains. Sokoban domain,
hand, medium-sized windows better. Moreover, improvable window may
improved particular subplanner within given time bound. noted
domains, e.g., Pegsol Scanalyzer, subplanners require, average, time
find lower-cost plan.
developed set window ranking policies examining structural properties
generated candidate windows generated results first experiment (cf.
Section 3.1) ran two subplanners (IBCS PNGS) generated window
30 second time limit, excluding windows whose cost already shown
optimal admissible LM-Cut heuristic (Helmert & Domshlak, 2009). Investigating
properties improved unimproved windows, identified four metrics work
relatively well across domains:
414

fiContinuing Plan Quality Optimisation

0.74
Random ranking
Outgoing causal links per length (min max)
Incoming causal links per length (min max)
Pairwise ordering disagreement (min max)
Gap cost & admissible heuristic (max min)

Fraction improvable windows selected windows



0.72

0.7

0.68

0.66

0.64

0.62






























400

375

350

325

300

275

250

225

200

175

150

125

100

75

50

25

0.6

Number selected (top ranked) windows

Figure 18: Fraction improvable windows, across domains, selected top
windows ranked orders generated ranking policies (see text).

(1) total number causal links whose producers reside window whose consumers outside window, divided length window lower
value higher rank. call property outgoing causal links per length.
(2) total number causal links whose consumers reside window whose producers outside window, divided length window lower
value higher rank. call property incoming causal links per length.
(3) gap cost window lower bound cost plan
corresponding subproblem given admissible heuristic higher value
higher rank.
(4) number pairwise ordering (of steps) disagreements window hp, w, qi
sequential input plan lower value higher rank. calculate
first take linearisation hp, w, qi used generate corresponding
subproblem. Then, every pair plan steps, ordering
linearisation input plan call pairwise ordering
disagreement. lower total number disagreements window,
higher rank. words, ordering steps window different
input plan less likely improved.

415

fiSiddiqui & Haslum

0.64
Fraction improvable windows selected windows

Random ranking
Outgoing causal links per length (min max)
Incoming causal links per length (min max)
Pairwise ordering disagreement (min max)
Gap cost & admissible heuristic (max min)



0.62
0.6
0.58
0.56
0.54
0.52
0.5
0.48
0.46
0.44
0.42












0.4

















0.38




0.36
0.34
0.32

400

375

350

325

300

275

250

225

200

175

150

125

100

75

50

25

0.3

Number selected (top ranked) windows

Figure 19: Fraction improvable windows Parking domain, selected top
windows ranked orders generated ranking policies (see text).

infer first two ranking policies disconnected window
blocks decomposed plan likely improved. Figure 18
compares ranking policies performance random ordering windows.
average across domains, four ranking policies good picking improvable
windows. example, take top 25 windows order generated
incoming causal links per length policy, nearly 74% windows improvable (by
least one subplanner), top 25 windows random order contain
61% improvable windows. random ranking Figure 18 best result three
separate random rankings values x-axis. expected, exhibits
roughly ratio improvable windows ranges (from 25 400). Nearly 61%
selected windows, across domains, improvable. However, performance
individual ranking policies varies domain, policy find domain
good. example, Figure 19 shows ranking results instances
Parking domain only: Here, outgoing causal links per length policy work
well. Considering top 90 windows ranked order, even worse random.
However, ranking policies quite beneficial domain.
BDPO2 uses first three ranking policies sequential portfolio (as explained
Section 3). subplanner, BDPO2 uses current ranking policy select next
416

fiContinuing Plan Quality Optimisation

0.963

Average Quality Score (Relative IPC Quality Score / Coverage)

0.961
0.959
0.957



0.955



0.953












0.951





0.949








0.947





0.945





0.943




0.941





0.939




0.937





0.935


0.933




0.931

BDPO2 (rankbased)
BDPO2 (randomranked)
6

5.5

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0



Time (hours)

Figure 20: Average IPC plan quality score function time two separate runs:
without window ranking. second case, order candidate windows
randomised. run done using experimental setup 3, described Section 3.1
page 392. time shown runtime BDPO2 (excluding 2 hour delay
generating input plans, shown Figure 11). Also, y-axis truncated:
curves start average quality score input plans, 0.907.

window chosen subplanner (from eligible optimisation subplanner).
improvement found subplanner certain number attempts (13,
current configuration), system switches different ranking policy, produce different
ordering candidate windows subplanner.
use window ranking beneficial effect anytime performance
plan improvement process, shown Figure 20. achieve higher quality scores,
particular, achieve faster, using window ranking compared random ranking.
experiment, ran BDPO2 portfolio ranking policies, described
417

fiSiddiqui & Haslum

above, windows chosen optimisation random order. experiment
used setup experiment 3 (described Section 3.1 page 392).
tried many alternative methods combining ordered lists generated different
ranking policies, order achieve ranking stable performance across domains.
problem combining rankings, often called rank aggregation, studied many
disciplines, social choice theory, sports competitions, machine learning, information retrieval, database middleware, on. Rank aggregation techniques range
quite simple (based rank average number pairwise wins) complex procedures
require solving optimisation problem. tried five simple popular rank aggregation techniques, namely Bordas (1781) method, Kemenys (1978) optimal
ordering, Copelands (1951) majority graph, MC4 (Dwork, Kumar, Naor, & Sivakumar,
2001), multivariate Spearmans rho (Bedo & Ong, 2014). result experiments, however, rank aggregation produce better, stable, window
rankings, especially cases one individual policy relatively bad. Hence choice
using ranking policies cyclic portfolio instead.

5. On-line Adaptation
LNS approach optimisation repeatedly solving local subproblems gives us
opportunity adapting process on-line current problem. noted
different subplanners, windowing strategies, ranking policies work better different
domains. example, Figure 21 shows fraction local improvements found
three subplanners different domains. seen, IBCS subplanner
productive, compared PNGS LAMA, APPN, Barman, Maintenance, Parking,
Sokoban, Woodworking domains. PNGS, hand, better Scanalyzer
Visitall domains, LAMA Elevators Openstacks domains. Therefore,
learn course local search relative success rate different subplanners
current problem, system perform better. similar fashion, window
generation strategies ranking policies may also adapted current problem,
system likely select subplans optimisation improvable.
use on-line machine learning technique multi-armed bandit (MAB) model,
specific select subplanner local optimisation attempt. technique, impact anytime performance BPO2 described following
subsections.
window selection, on-line adaptation limited switching alternative
ranking policies. window selected optimisation subplanner top one
order given current ranking policy subplanner (cf. Section 4.6).
long improvements found among windows, consider current
policy useful. subplanner reaches certain number attempts
improvements found, switch using next policy subplanner. number
windows neighbourhood optimised typically small compared
number candidate windows generated. average across problems experiment 3
(cf. Section 3.1 page 392) optimisation least one subplanner tried 24.8%
generated windows. this, adapting ranking policy influence
418

fiContinuing Plan Quality Optimisation

Figure 21: percentage improved windows found subplanners (PNGS,
IBCS, LAMA), total number improved windows found subplanners. experiment, BDPO2 run three times, time one subplanner.
setup experiment 3 (described Section 3.1 page 392).

windows tried adapting windowing strategies. effect adaptive
window ranking anytime performance BDPO2 shown Figure 20 (page 417).
5.1 Bandit Learning
multi-armed bandit (MAB) model popular machine learning formulation dealing
exploration versus exploitation dilemma. MAB problem, algorithm
presented sequence trials. round, algorithm chooses one set
alternatives (often called arms) based past history, receives reward
choice. goal maximise total reward time. bandit learning algorithm
balances exploiting arms highest observed average reward exploring poorly
understood arms discover yield better reward.
MAB found numerous applications diverse fields (e.g., control, economics, statistics, learning theory) influential paper Robbins (1952). Many policies
proposed MAB problem different assumptions, example, independent (Auer et al., 2002) dependent arms (Pandey, Chakrabarti, & Agarwal, 2007),
exponentially infinitely many arms (Wang, Audibert, & Munos, 2008), finite infinite
time horizon (Jones & Gittins, 1974), without contextual information (Slivkins,
2014), on.
cast problem selecting subplanner local optimisation attempt
multi-armed bandit problem. goal maximise total number improved
windows time. use learning algorithm based optimistic exploration strategy, chooses arm favorable environments high probability
best, given observed far. strategy often called optimism
face uncertainty. trial t, arm k, strategy use past
observations probabilistic argument define high-probability confidence intervals
expected reward k . favorable environment arm k thus upper
419

fiSiddiqui & Haslum

confidence bound (UCB) k . simple policy based strategy play arm
highest UCB.
number algorithms developed optimistic exploration bandit arms,
UCB1, UCB2 UCB1-NORMAL Auer et al. (2002), UCB-V Audibert,
Munos Szepesvari (2009), KL-UCB Garivier Cappe (2011). use
UCB1 algorithm planner selection. UCB1 algorithm
selects trial arm
q
2 ln
highest upper confidence bound Bk,t =
bk,t +
nk , sum exploitation term
exploration term, respectively.
bk,t empirical mean rewards received
arm k

trial
t,

n


number
times arm k tried far.
k
q
2 ln
second term,
nk , confidence interval average reward, within true
expected reward falls almost certain probability. Hence, Bk,t upper confidence
bound. UCB1 algorithm achieve logarithmic regret uniformly number
trials without preliminary knowledge reward distributions (Auer et al.,
2002).
Applied subplanner selection BDPO2, algorithm works follows: First,
select subplanner p once, initialise average reward
bp . optimisation
attempt, give reward 1 chosen subplanner found improvement
reward 0 otherwise. could use scheme assigning rewards rather
simply 0 1, example, making reward proportional amount improvement
(or time taken find it). However, observed assigning varying rewards
subplanners makes bandit learning system complicated, help
achieving better overall result. Next, select
q attempt subplanner p
maximises upper confidence bound p, Bp,t =
bp + 2nlnp , explained above. Here,
np number times p tried far, total number optimisation
attempts (by subplanners) done far. see Bp,t grows shrinks
np increase uniformly. ensures alternative tried infinitely often
still balances exploration exploitation. words, try p,
smaller size confidence interval closer gets mean value
bp .

p cannot tried becomes smaller p , p planner best
average reward.
5.2 Impact Bandit Learning
response bandit policy subplanner selection shown Figure 22. figure
shows fraction total number optimisation attempts one subplanner, IBCS,
selected, fraction total number window improvements found
subplanner. Since BDPO2 experiment uses two subplanners, IBCS PNGS,
corresponding fraction PNGS 1 y. example, third problem (from
left) APPN domain, 100% window improvements found IBCS,
bandit policy selects subplanner 84% total number optimisation attempts.
PNGS chosen 16%, finds improvement. see bandit
policy selects promising subplanner often across problems. However,
bandit policy somewhat conservative, ensures rule
subplanners fare poorly early on. Moreover, current plan improved
420

fiContinuing Plan Quality Optimisation

1
improvement ratio
exploitation ratio



0.9
Exploitation improvement ratio IBCS



0.8
0.7




0.6
0.5

























































0.4



0.3














0.2
0.1

Woodworking

Visitall

Transport

Thoughtful

Tetris

Sokoban

Parking

Scanalyzer

Parcprinter

Nomystery

Maintenance

Hiking

Ged

Floortile

Elevators

Childsnack

Barman

Appn

0

Figure 22: response bandit policy subplanner success rates. exploitation
ratio fraction total number optimisation attempts IBCS
subplanner chosen, total number attempts subplanners.
improvement ratio fraction total number improved windows found IBCS,
total number improved windows found subplanners. Since IBCS
PNGS two subplanners used experiment, corresponding ratios
PNGS opposite (i.e., 1 y). experiment run setup
experiment 2, described Section 3.1 page 392.

becomes harder find improvements (within given time bound), average
reward subplanners decreases. forces bandit policy switch
subplanners often.
Figure 23 shows impact combining subplanners using UCB1 bandit policy,
compared simply alternating subplanners using subplanner alone,
anytime performance BDPO2. experiment ran BDPO2 IBCS,
PNGS LAMA subplanner, combining two (IBCS PNGS)
using simple alternation policy, selects two turn, combining
two using bandit policy. run done experiment setup 3 (as described
Section 3.1 page 392), i.e., input plans high quality. (The IPC plan quality
score plan calculated before; see page 394). average score input
plans 0.907.) expected, combining IBCS PNGS subplanners fashion
leads quality improvement across entire time scale achieved running
BDPO2 individual subplanner. figure also shows combining multiple
subplanners using bandit policy better strategy simply alternating
421

fiSiddiqui & Haslum

0.963

Average Quality Score (Relative IPC Quality Score / Coverage)

0.959




















0.955






















0.951















0.947






0.943







0.939




0.935


0.931




0.927



















































0.923

BDPO2 (PNGS+IBCS: Bandit)
BDPO2 (PNGS+IBCS: Alternating)
BDPO2 (PNGS only)
BDPO2 (IBCS only)
BDPO2 (LAMA only)









0.919


6

5.5

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0.915

Time (hours)

Figure 23: Average IPC quality score function time per problem five different
runs BDPO2: using one three subplanners, using two (IBCS
PNGS) combined UCB1 bandit policy, without (using simple alternation
instead). experiment run setup 3 described Section 3.1 (on page 392).
Note y-axis truncated: curves start average quality input plans,
0.907. time shown runtime BDPO2 only, excluding 2 hour
delay generating input plans shown Figure 11).

422

fiContinuing Plan Quality Optimisation

them. total quality improvement achieved BDPO2 using alternation policy
6.8% less BDPO2 using bandit policy.

6. Related Work
survey four areas related work: Anytime search algorithms post-processing approaches, common approach aim continuing plan quality
improvement; uses local search planning; finally, uses algorithm portfolios
planning.
6.1 Anytime Search
Large state-space search problems, kind frequently arise planning problems,
often cannot solved optimally optimal search algorithms exhaust memory
finding solution. Anytime search algorithms try deal problems finding
first solution quickly, possibly using greedy suboptimal heuristic search, continue
(or restart) searching better quality solution. Anytime algorithms attractive allow users stop computation time, i.e., good enough solution
found, long wait. contrasts algorithms require
user decide advance deadline, suboptimality bound, parameter
fixes trade-off time solution qualty.
Bounded suboptimal search problem finding solution cost less
equal user specified factor w optimal. Weighted A* (WA*) search (Pohl, 1970)
Explicit Estimation Search (EES) (Thayer & Ruml, 2011) two algorithms
kind used planning. Iteratively applying bounded suboptimal
search algorithm lower value w whenever new best solution found provides
anytime improvement plan quality. Restarting WA* (Richter et al., 2010) this, using
schedule decreasing weights. RWA* used LAMA planner (Richter & Westphal,
2010) LAMA finds first plan using greedy best-first search (Bonet & Geffner, 2001).
also uses several search enhancements, like preferred operators deferred evaluation
(Richter & Helmert, 2009). EES conducts bounded suboptimal best-first search restricted
expanding nodes may lead solution cost given factor w
times optimal. Among open nodes set, expands one estimated
fewest remaining actions goal. uses admissible heuristic plan
cost informative inadmissible estimates guide search. AEES (Thayer
et al., 2012b) anytime version EES. achieve anytime behavior, AEES lowers
value w whenever new best solution found.
bounded-cost search (Stern, Puzis, & Felner, 2011) problem, subproblems solved approach example, requires finding solution cost less
equal user-specified cost bound C. aim bounded-cost search algorithm find solution quickly possible. Iteratively applying bounded-cost
search algorithm bound less cost best solution found far provides
anytime quality improvement. IBCS algorithm, used one subplanners BDPO2, does. BEES BEEPS algorithms (Thayer, Stern, Felner, &
Ruml, 2012a) adapt EES setting bounded cost search. algorithms expand
423

fiSiddiqui & Haslum

best open node among whose inadmissible cost estimate C, falling back
expanding node best admissible estimate set empty.
Branch-and-bound algorithms explore search space systematic fashion, using
admissible heuristic (lower bound cost) prune nodes cannot lead solution
better best found far. Branch-and-bound implemented linearmemory, depth-first search strategy well top strategies. experiment
reported Section 3.2 (page 394) used Beam-Stack Search (BSS) (Zhou & Hansen,
2005) bounded-cost search algorithm providing initial upper bound cost
base plan problem. BSS combines backtracking branch-and-bound beam
search, behaves like breadth-first search limits size open list
layer user-specified parameter, known beam width. forced backtrack,
BSS reconstructs nodes pruned open list search complete. beam
width parameter used control memory consumption BSS never
exceeds available memory. planning problems, however, whose state spaces often
dense transpositions accurate admissible heuristics expensive compute,
repeatedly reconstructing paths unexplored nodes becomes time-consuming.
Anytime search planners aim provide continuing improvement plan quality given
time, often succeed early stages search. However,
observed results experiments, algorithms often stagnate, reaching
point find better plans even several hours CPU time. (cf.
Figure 11 page 395 Section 3.2 page 394.) example, experiment LAMA
AEES found better plans 8.7% 6.1%, respectively, total number
problems 3 hours 6 hours CPU time, BDPO2 found better plans
30.4% problems time interval. Memory one limiting factor,
one. almost half problems, AEES ran full 7 hours CPU time
without running memory, yet found improved plans. BSS found plans
cost less initial upper bound (the cost base plans) 14 182
problems even 24 hours CPU time per problem.
6.2 Local Search
Local search explores space searching small neighbourhood current element
search space one is, way, better, moving neighbour
repeating process. Compared systematic search algorithms, advantage local
search needs much less memory. Therefore, local search algorithms widely
used solve hard optimisation problems. However, local search algorithms cannot offer
guarantees global optimality, bounded suboptimality. planning, local search
used mainly find plans quickly, rarely improve plan quality, though
post-processing methods discussed next section viewed local searches.
FF (Hoffmann & Nebel, 2001) forward-chaining heuristic state space search planner.
heuristic used FF estimates distance state nearest goal state. FF
uses local search strategy, called enforced hill-climbing, state uses breadthfirst search find neighbour state (which may several steps away current
state) strictly better heuristic value, i.e., believed closer goal.
commits state starts new search neighbour better yet
424

fiContinuing Plan Quality Optimisation

heuristic value. local search fails, due getting trapped dead end, FF falls back
complete best-first search algorithm. RW-LS planning algorithm (Xie, Nakhost,
& Muller, 2012) similar FFs hill-climbing approach, uses combination greedy
best-first search exploration random walks find better next state local
search step. Nakhost Muller (2009) developed planning system, called Arvand,
uses random walk-based local exploration conjunction FF search heuristic.
showed Arvand outperforms FF hard problems many domains. execution
Arvand consists series search episodes. episode starts set random
walks initial state. endpoint random walk evaluated using
heuristic function choose next state. search episode continues set
random walks state. process repeats either goal reached,
enough transitions made without heuristic progress, case process
restarted. IPC 2011 2014 versions Arvand apply post-processing improve
quality generated plan. post-processing techniques Action Elimination
Plan Neighborhood Graph Search (Nakhost & Muller, 2010); discussed next
subsection. Arvands search randomised, system continue generating
alternative plans, optmised, indefinitely, storing times best plan
generated far. provides certain anytime capability. manner
used experiment reported Section 3.2 page 394.
LPG planner (Gerevini & Serina, 2002) based local search space
action graphs, represent partial plans. neighbourhood defined operators
modify action graph, inserting removing actions. function
evaluates nodes neighbourhood combines terms estimate far action
graph becoming valid plan, termed search cost, expected quality
plan may become. choice neighbour move also involves element
randomness. LPG also performs continuing search better plans; this, similar
anytime search algorithms discussed last subsection. Whenever finds plan,
local search restarts partial plan obtained removing randomly selected
actions current plan. numerical constraint forcing cost next plan
lower also added. provides guidance towards better quality next plan.
close relationship local search approaches planning plan repair
adaptation methods (Garrido, Guzman, & Onaindia, 2010). LPG planner originated
method plan repair (Gerevini & Serina, 2000), iterative repair methods also
used plan generation (Chien, Knight, Stechert, Sherwood, & Rabideau, 2000).
key difference use local search previous uses planning
carry local search space valid plans. permits neighbourhood
evaluation focus exclusively plan quality. Searching space partial plans (represented states) done FF, incomplete (invalid) plans, done LPG, requires
neighbourhood evaluation consider close element becoming valid plan,
balancing quality.
large neighbourhood search (LNS) strategy formulates problem finding
good neighbor optimisation problem, rather simply enumerating evaluating
neighbours. allows much larger neighbourhood considered. LNS used
successfully solve hard combinatorial optimisation problems like vehicle routing
time windows (Shaw, 1998) scheduling (Godard, Laborie, & Nuijten, 2005). Theoretical
425

fiSiddiqui & Haslum

experimental studies shown increased neighborhood size may improve
effectiveness (quality solutions) local search algorithms (Ahuja, Goodstein, Mukherjee,
Orlin, & Sharma, 2007). neighbourhood current solution small
difficult escape local minima. case, additional meta-heuristic techniques,
Simulated Annealing Tabu Search, may needed escape local minimum.
LNS, size neighborhood may sufficient allow search process
avoid escape local minima.
LNS literature, neighborhood solution usually defined set
solutions reached applying destroy heuristic repair method.
destroy heuristic selects part current solution removed (unassigned),
repair method rebuilds destroyed part, keeping rest current solution
fixed. destroy heuristic often includes element randomness, enabling search
explore modifications different parts current solution. role destroy
heuristic system played windowing strategies, select candidate windows (subplans) re-optimisation. explore windows systematically. LNS
algorithms (e.g., Ropke & Pisinger, 2006; Schrimpf et al., 2000) allow local search
move neighbouring solution lower quality (e.g., using simulated annealing).
consider strictly improving moves. However, difference previous LNS algorithms,
immediately move better plan restart neighbourhood exploration
local improvement found. Instead, use delayed restarting, allows better
solution found one local search step destroying repairing multiple parts
current plan. Experimentally, found delayed restarting produces better quality
plans, produces faster, immediate restarts (cf. Section 3.4 page 399).
6.3 Plan Post-Processing
post-processing method, mean one takes valid plan input attempts
improve it, making modifications. also related plan repair adaptation
(Chien et al., 2000; Fox, Gerevini, Long, & Serina, 2006; Garrido et al., 2010),
key difference plan repair adaptation starts plan valid
current situation focuses making work; discrepancy current
state goals plan originally built provide guidance repairs
needed. contrast, post-processing plan optimisation may require modifications
anywhere current plan.
Nakhost Muller (2010) proposed two post-processing techniques Action Elimination (AE) Plan Neighborhood Graph Search (PNGS). Action elimination identifies
removes unnecessary actions given plan. PNGS constructs plan neighborhood graph, subgraph state space problem, built around
path state space induced current plan expanding limited number
states state path. searches least-cost plan subgraph.
finds plan better current, process repeated around new best
plan; otherwise, exploration limit increased, time memory limit exceeded.
Furcys (2006) Iterative Tunneling Search A* (ITSA*) similar PNGS. ITSA*
explores area, called tunnel, state space using A* search, restricted fixed
distance current plan. methods seen creating neighborhood
426

fiContinuing Plan Quality Optimisation

includes small deviations current plan, anywhere along plan.
contrast, BDPO2 focuses one section decomposed plan time, often grouping
together different parts input plan, puts restriction much section
changes; hence, creates different neighbourhood. experiments show best
results obtained exploring neighbourhoods. example, PNGS often finds
plan improvements quickly, running additional 6 hours improves average
IPC plan quality score, best plans finds first hour, 0.01%.
Running instead BDPO2, using PNGS subplanner taking best plans
found PNGS 1 hour input, improves average plan quality score 3% 6
hours.
Ratner Pohl (1986) used local optimisation shortening solutions sequential
search problems. select subpath optimise, used sliding window predefined size dmax consecutive segments current path. Estrem Krebsbach
(2012) instead used form windowing heuristic: select local optimisation pairs
states current path maximise estimate redundancy, based ratio
estimated distances two states, given state space heuristic,
cost current path. Balyo, Bartak Surynek (2012) used sliding window
approach minimise parallel plan length (that is, makespan, assuming actions
unit duration). Rather take segments single path state space, use block
deordering input plan create candidate windows local optimisation. shown
experimental results, important success BDPO2: total
improvement average plan quality achieved without deordering 28.7% less
achieved BDPO2 using block deordering input plans (cf. Section 3.6 page 402).
planning-by-rewriting approach (Ambite & Knoblock, 2001) also uses local modifications partially ordered plans improve quality. Plan modifications defined
domain-specific rewrite rules, provided domain designer learned
many examples good bad plans. Hence, technique effective
solving many problem instances domain. Using planner solve subproblems may time-consuming applying pre-defined rules, makes process
automatic. However, consider solving many problems domain may
possible reduce average planning time learning (generalised) rules subplan
improvements discover using applicable avoid invoking subplanner.
6.4 Portfolio Planning Automatic Parameter Tuning
portfolio planning system runs several subplanners sequence (or parallel) short
timeouts, hope least one component planners find solution
time allotted it. Portfolio planning systems motivated observations
single planner dominates others domains, planner solve
planning task quickly, often solve all. Therefore, many todays
successful planners run sequential portfolio planners (Coles, Coles, Olaya, Celorrio,
Linares Lopez, Sanner, & Yoon, 2012).
Gerevini, Saetti Vallati (2009) introduced PbP planner, learns portfolio
given set planners specific domain, well domain-specific macro-actions.
Fast Downward Stone Soup (FDSS, Helmert, Roger, Seipp, Karpas, Hoffmann, Keyder,
427

fiSiddiqui & Haslum

Nissim, Richter, & Westphal, 2011) uses fixed portfolio, computed optimise performance
large sample training domains, domains. IBaCoP2 (Cenamor et al., 2014)
dynamically configures portfolio using predictive model planner success.
Another recent trend use automatic algorithm configuration tools, like
ParamILS framework (Hutter, Hoos, Leyton-Brown, & Stutzle, 2009), enhance planner
performance specific domain. ParamILS local search space configurations, using suite training problems evaluate performance different parameter
settings. combinatorial explosion caused many parameters many different values managed varying one parameter time. ParamILS used configure
LPG planner (Vallati, Fawcett, Gerevini, Hoos, & Saetti, 2011) Fast Downward planner (Fawcett, Helmert, Hoos, Karpas, Roger, & Seipp, 2011). PbP2 portfolio
planner (Gerevini, Saetti, & Vallati, 2011), successor PbP, includes version LPG
customised domain ParamILS learned portfolio.
BDPO2, course, uses portfolio subplanners, and, shown, selecting
right subplanner current problem important (cf. Section 5). Much important,
however, focus subproblems approach brings: comparing Figures 11 (page
395) 23 (page 422), clear using even single subplanner within BDPO2
effective using subplanners own. multiple window ranking
policies used BDPO2 (cf. Section 4.6) also viewed simple sequential portfolio.
Compared previous portfolio planners, iterated use subplanners, windowing strategies components approach offers possibility learn best portfolio
configuration on-line; is, rather spend time configuring system using
training problems, learn experience solving several subproblems,
actually working optimising current plan.
Finally, although explored great depth, results suggest combining different anytime search post-processing methods, effectively kind
sequential portfolio (such running BDPO2 result running PNGS result
LAMA IBaCoP2, results experiment 3, shown Figure 2 page 371),
often achieves better quality final plans investing available time single
method.

7. Conclusions Future Work
Plan quality optimisation, particularly large problems, central concern automated
planning. Anytime planning, aims deliver continuing stream better plans
given time, attractive idea, offering flexibility stop process
point, best plan found good enough wait next plan
becomes long. presented approach anytime plan improvement,
realisation BDPO2 system. approach based large neighbourhood local
search strategy (Shaw, 1998), using windowing heuristics select candidate windows
block deordering current plan, local optimisation using off-the-shelf bounded-cost
planning techniques.
Experiments demonstrate BDPO2 achieves continuing plan quality improvement
even large time scales (several hours CPU time), anytime planners stagnate.
Key achieving focus optimising subproblems, corresponding windows.
428

fiContinuing Plan Quality Optimisation

mentioned Section 4.5, extending windowing heuristics improving on-line
learning effective window rankings one way improve approach. Also, complementing window ranking, estimates promising window is,
estimate difficult windows optimise, using inform time allocated subplanners, currently uniform windows, may contribute better
performance. best result, however, achieved chaining several techniques together
(for example, applying BDPO2 best plan found PNGS applied best plan
found LAMA IBaCoP2). result cannot achieved previous anytime planning approaches alone. Thus, another area future work examine greater
depth best way combine different plan improvement methods,
learned on-line optimising plan. example, conducted study
optimal time switch base plan generation, using LAMA, post-processing
using PNGS BDPO, function total runtime (Siddiqui & Haslum, 2013a).
demonstrated experimentally, block deordering step essential
good performance BDPO2 (cf. Section 3.6 page 402). Block deordering creates
decomposition plan non-interleaving blocks removing ordering constraints
blocks. lifts limitation conventional, step-wise, deordering,
requires unordered steps plan non-interfering. shown, validity
condition block decomposed partially ordered plans stated almost
Chapmans (1987) modal truth criterion, allowing threats causal link
remain unordered long link protected block structure (Theorem 2
page 379). Therefore, block deordering yield less order-constrained plans, including
cases conventional deordering possible.
plan structure uncovered block decomposition also uses. Recently used planner independent macro generation system BloMa (Chrpa &
Siddiqui, 2015) find longer macros capture compound activities order improve
planners coverage efficiency. domains (e.g., Barman, ChildSnack, Scanalyzer,
Parcprinter, Gripper, Woodworking, etc.), block deordering often identifies structurally similar subplans, also symmetric improvement patterns. could potentially
exploited learning plan rewrite rules (Ambite, Knoblock, & Minton, 2000). structure
block deordered plans, often comprises nested, hierarchical decomposition
meaningful subplans, reminiscent Hierarchical Task Network (HTN) representations.
Hence, block deordering technique could potentially applied generating (or helping
generate) HTN structures domain independent way, reducing knowledge-engineering
effort. Recent work Scala Torasso (2015) extends deordering plans planning
domains numeric state variables, identifying numeric dependencies capture
additional reasons necessary orderings. Defining conditions blocks sufficient
encapsulate dependencies would allow block deordering also numeric plans.
may synergy block deordering numeric planning, since numeric dependencies often involve groups plan steps, rather single producerconsumer pair.
Acknowledgment
work partially supported Australian Research Council discovery project
DP140104219 Robust AI Planning Hybrid Systems. NICTA funded Aus429

fiSiddiqui & Haslum

tralian Government Department Communications Australian Research Council ICT Centre Excellence Program.

References
Ahuja, R. K., Goodstein, J., Mukherjee, A., Orlin, J. B., & Sharma, D. (2007).
large-scale neighborhood search algorithm combined through-fleet-assignment
model. INFORMS Journal Computing, 19 (3), 416428.
Ambite, J. L., & Knoblock, C. A. (2001). Planning rewriting. Journal Artificial
Intelligence Research (JAIR), 15 (1), 207261.
Ambite, J. L., Knoblock, C. A., & Minton, S. (2000). Learning plan rewriting rules.
Proc. 5th International Conference Artificial Intelligence Planning Systems,
AIPS 2000, Breckenridge, CO, USA, April 14-17, 2000, pp. 312. AAAI Press.
Audibert, J.-Y., Munos, R., & Szepesvari, C. (2009). Explorationexploitation tradeoff using
variance estimates multi-armed bandits. Theoretical Computer Science, 410 (19),
18761902.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmed
bandit problem. Machine Learning, 47 (2-3), 235256.
Backstrom, C. (1998). Computational aspects reordering plans. Journal Artificial
Intelligence Research (JAIR), 9, 99137.
Balyo, T., Bartak, R., & Surynek, P. (2012). improving plan quality via local enhancements. Proc. 5th International Symposium Combinatorial Search, SOCS
2012, Niagara Falls, Canada, July 19-21, 2012. AAAI Press.
Bedo, J., & Ong, C. S. (2014). Multivariate Spearmans rho aggregating ranks using
copulas. CoRR, abs/1410.4391.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2014). IBaCoP IBaCoP2 planners.
Proc. 8th International Planning Competition, IPC 2014, Deterministic Part,
pp. 3538.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32 (3), 333377.
Chien, S., Knight, R., Stechert, A., Sherwood, R., & Rabideau, G. (2000). Using iterative
repair improve responsiveness planning scheduling. Proc.
5th International Conference Artificial Intelligence Planning Systems, AIPS 2000,
Breckenridge, CO, USA, April 14-17, 2000, pp. 300307. AAAI Press.
Chrpa, L., & Siddiqui, F. H. (2015). Exploiting block deordering improving planners efficiency. Proc. 24th International Joint Conference Artificial Intelligence,
IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 15371543. AAAI Press.
Coles, A. J., Coles, A., Olaya, A. G., Celorrio, S. J., Linares Lopez, C., Sanner, S., & Yoon,
S. (2012). survey seventh international planning competition. AI Magazine,
33 (1), 8388.
430

fiContinuing Plan Quality Optimisation

Copeland, A. H. (1951). reasonable social welfare function. University Michigan
Seminar Applications Mathematics social sciences.
de Borda, J. C. (1781). Memory election ballot. History Royal Academy
Sciences, Paris, 657664.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods
web. Proc. 10th International Conference World Wide Web, WWW
2001, Hong Kong, May 1-5, 2001, pp. 613622, New York, NY, USA. ACM.
Estrem, S. J., & Krebsbach, K. D. (2012). AIRS: Anytime iterative refinement solution. Proc. 25th International Florida Artificial Intelligence Research Society
Conference, Marco Island, Florida. May 23-25, 2012.
Fawcett, C., Helmert, M., Hoos, H., Karpas, E., Roger, G., & Seipp, J. (2011). FD-Autotune:
Domain-specific configuration using Fast Downward. Proc. 2011 ICAPS
Workshop Planning Learning, PAL 2011, Freiburg, Germany, June 11-16,
2011, pp. 1320. AAAI Press.
Fox, M., Gerevini, A., Long, D., & Serina, I. (2006). Plan stability: Replanning versus plan
repair. Proc. 16th International Conference Automated Planning
Scheduling, ICAPS 2006, Cumbria, UK, June 6-10, 2006., pp. 212221. AAAI Press.
Furcy, D. (2006). ITSA*: Iterative tunneling search A*. Proc. 2006 AAAI
Workshop Heuristic Search, Memory-Based Heuristics Applications,
July 1620, 2006, Boston, Massachusetts, pp. 2126. AAAI Press.
Garivier, A., & Cappe, O. (2011). KL-UCB algorithm bounded stochastic bandits
beyond. CoRR, abs/1102.2490.
Garrido, A., Guzman, C., & Onaindia, E. (2010). Anytime plan-adaptation continuous
planning. Proc. joint 28th Workshop UK Special Interest Group
Planning Scheduling 4th Italian Workshop Planning Scheduling, pp.
4754.
Gerevini, A., Saetti, A., & Vallati, M. (2009). automatically configurable portfolio-based
planner macro-actions: PbP. Proc. 19th International Conference
Automated Planning Scheduling, ICAPS 2009, Thessaloniki, Greece, September
19-23, 2009, pp. 350353. AAAI Press.
Gerevini, A., Saetti, A., & Vallati, M. (2011). PbP2: Automatic configuration portfoliobased multi-planner. 7th International Planning Competition (IPC 2011), Learning
Track. http://www.plg.inf.uc3m.es/ipc2011-learning.
Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphs
action costs. Proc. 6th International Conference Artificial Intelligence
Planning Scheduling, AIPS 2002, April 23-27, 2002, Toulouse, France, pp. 281
290. AAAI Press.
Gerevini, A. E., & Serina, I. (2000). Fast plan adaptation planning graphs: Local
systematic search techniques. Proc. 5th International Conference
Artificial Intelligence Planning Systems, AIPS 2000, Breckenridge, CO, USA, April
14-17, 2000, pp. 112121. AAAI Press.
431

fiSiddiqui & Haslum

Ghallab, M., Nau, D. S., & Traverso, P. (2004). Automated Planning: Theory & Practice.
Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.
Godard, D., Laborie, P., & Nuijten, W. (2005). Randomized large neighborhood search
cumulative scheduling. Proc. 15th International Conference Automated
Planning Scheduling, ICAPS 2005, Monterey, California, USA, June 5-10 2005,
pp. 8189. AAAI Press.
Haslum, P. (2011). Computing genome edit distances using domain-independent planning.
Proc. 2011 ICAPS Workshop Scheduling Planning Applications,
SPARK 2011, Freiburg, Germany, June 11-16, 2011. AAAI Press.
Haslum, P. (2012). Incremental lower bounds additive cost planning problems. Proc.
22nd International Conference Automated Planning Scheduling, ICAPS
2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012, pp. 7482. AAAI Press.
Haslum, P., & Grastien, A. (2011). Diagnosis planning: Two case studies. Proc.
2011 ICAPS Workshop Scheduling Planning Applications, SPARK 2011,
Freiburg, Germany, June 11-16, 2011. AAAI Press.
Haslum, P., & Jonsson, P. (2000). Planning reduced operator sets. Proc.
5th International Conference Artificial Intelligence Planning Systems, AIPS 2000,
Breckenridge, CO, USA, April 14-17, 2000, pp. 150158. AAAI Press.
Helmert, M., Roger, G., Seipp, J., Karpas, E., Hoffmann, J., Keyder, E., Nissim, R., Richter,
S., & Westphal, M. (2011). Fast Downward Stone Soup (planner abstract).
Proc. 7th International Planning Competition, IPC 2011, Deterministic Part.
http://www.plg.inf.uc3m.es/ipc2011-deterministic.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats
difference anyway?. Proc. 19th International Conference Automated
Planning Scheduling, ICAPS 2009, Thessaloniki, Greece, September 19-23, 2009,
pp. 162169. AAAI Press.
Hoffmann, J. (2001). Local search topology planning benchmarks: empirical analysis.
Proc. 17th International Joint Conference Artificial Intelligence, IJCAI
2001, Seattle, Washington, USA, August 4-10, 2001, pp. 453458, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research (JAIR), 14, 253302.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic
algorithm configuration framework. Journal Artificial Intelligence Research (JAIR),
36 (1), 267306.
Jones, D. M., & Gittins, J. (1974). dynamic allocation index sequential design
experiments. University Cambridge, Department Engineering.
Kambhampati, S., & Kedar, S. (1994). unified framework explanation-based generalization partially ordered partially instantiated plans. Artificial Intelligence,
67 (1), 2970.
432

fiContinuing Plan Quality Optimisation

McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proc. 9th
National Conference Artificial Intelligence, AAAI 1991, Anaheim, CA, USA, July
14-19, 1991, Volume 2., pp. 634639. AAAI Press / MIT Press.
Muise, C. J., McIlraith, S. A., & Beck, J. C. (2012). Optimally relaxing partial-order plans
maxsat. Proc. 22nd International Conference Automated Planning
Scheduling, ICAPS 2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012, pp. 358
362. AAAI Press.
Nakhost, H., & Muller, M. (2009). Monte-carlo exploration deterministic planning.
Proc. 21st International Joint Conference Artificial Intelligence, IJCAI
2009, Pasadena, California, USA, July 11-17, 2009, Vol. 9, pp. 17661771.
Nakhost, H., & Muller, M. (2010). Action elimination plan neighborhood graph search:
Two algorithms plan improvement. Proc. 20th International Conference
Automated Planning Scheduling, ICAPS 2010, Toronto, Canada, May 12-16,
2010, pp. 121128. AAAI Press.
Nebel, B., & Backstrom, C. (1994). computational complexity temporal projection, planning, plan validation. Artificial Intelligence, 66 (1), 125160.
Pandey, S., Chakrabarti, D., & Agarwal, D. (2007). Multi-armed bandit problems
dependent arms. Proc. 24th International Conference Machine Learning,
ICML 2007, Corvallis, Oregon, USA, June 20-24, 2007, Vol. 227, pp. 721728. ACM.
Pednault, E. P. D. (1986). Formulating multiagent, dynamic-world problems classical
planning framework. Reasoning actions plans, 4782.
Pohl, I. (1970). Heuristic search viewed path finding graph. Artificial Intelligence,
1 (3), 193204.
Ratner, D., & Pohl, I. (1986). Joint LPA*: Combination approximation search.
Proc. 5th National Conference Artificial Intelligence, AAAI 1986, Philadelphia, PA, August 11-15, 1986. Volume 1: Science., pp. 173177. Morgan Kaufmann.
Regnier, P., & Fade, B. (1991). Complete determination parallel actions temporal
optimization linear plans action. Proc. European Workshop Planning,
EWSP 1991, Sankt Augustin, FRG, March 18-19, 1991, Vol. 522 Lecture Notes
Computer Science, pp. 100111. Springer.
Richter, S., & Helmert, M. (2009). Preferred operators deferred evaluation satisficing
planning. Proc. 19th International Conference Automated Planning
Scheduling, ICAPS 2009, Thessaloniki, Greece, September 19-23, 2009, pp. 273280.
AAAI Press.
Richter, S., Thayer, J. T., & Ruml, W. (2010). joy forgetting: Faster anytime search
via restarting. Proc. 20th International Conference Automated Planning
Scheduling, ICAPS 2010, Toronto, Canada, May 12-16, 2010, pp. 137144. AAAI
Press.
Richter, S., & Westphal, M. (2010). LAMA planner: Guiding cost-based anytime
planning landmarks. Journal Artificial Intelligence Research (JAIR), 39, 127
177.
433

fiSiddiqui & Haslum

Robbins, H. (1952). aspects sequential design experiments. Herbert
Robbins Selected Papers, Vol. 58, pp. 527535. Springer.
Ropke, S., & Pisinger, D. (2006). adaptive large neighborhood search heuristic
pickup delivery problem time windows. Transportation Science, 40 (4),
455472.
Scala, E., & Torasso, P. (2015). Deordering numeric macro actions plan repair.
Proc. 24th International Joint Conference Artificial Intelligence, IJCAI
2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 16731681. AAAI Press.
Schrimpf, G., Schneider, J., Stamm-Wilbrandt, H., & Dueck, G. (2000). Record breaking
optimization results using ruin recreate principle. Journal Computational
Physics, 159 (2), 139171.
Shaw, P. (1998). Using constraint programming local search methods solve vehicle
routing problems. Proc. 4th International Conference Principles
Practice Constraint Programming, CP 1998, , Pisa, Italy, October 26-30, 1998,
Vol. 1520 Lecture Notes Computer Science, pp. 417431. Springer.
Siddiqui, F. H., & Haslum, P. (2012). Block-structured plan deordering. Proc. 25th
Australasian Joint Conference Advances Artificial Intelligence, AI 2012, Sydney,
Australia, December 4-7, 2012, Vol. 7691 Lecture Notes Computer Science, pp.
803814, Berlin, Heidelberg. Springer.
Siddiqui, F. H., & Haslum, P. (2013a). Local search space valid plans. Proc.
2013 ICAPS Workshop Evolutionary Techniques Planning Scheduling, EVOPS 2013, Rome, Italy, June 10-14, 2013, pp. 2231. http://icaps13.icapsconference.org/wp-content/uploads/2013/05/evops13-proceedings.pdf.
Siddiqui, F. H., & Haslum, P. (2013b). Plan quality optimisation via block decomposition.
Proc. 23rd International Joint Conference Artificial Intelligence, IJCAI
2013, Beijing, China, August 3-9, 2013, pp. 23872393. AAAI Press.
Slivkins, A. (2014). Contextual bandits similarity information. Journal Machine
Learning Research, 15 (1), 25332568.
Stern, R. T., Puzis, R., & Felner, A. (2011). Potential search: bounded-cost search
algorithm. Proc. 21st International Conference Automated Planning
Scheduling, ICAPS 2011, Freiburg, Germany June 11-16, 2011, pp. 234241. AAAI
Press.
Thayer, J., Stern, R., Felner, A., & Ruml, W. (2012a). Faster bounded-cost search using
inadmissible heuristics. Proc. 22nd International Conference Automated
Planning Scheduling, ICAPS 2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012,
pp. 270278. AAAI Press.
Thayer, J. T., Benton, J., & Helmert, M. (2012b). Better parameter-free anytime search
minimizing time solutions. Proc. 5th International Symposium
Combinatorial Search, SOCS 2012, Niagara Falls, Canada, July 19-21, 2012, pp.
120128. AAAI Press.
434

fiContinuing Plan Quality Optimisation

Thayer, J. T., & Ruml, W. (2011). Bounded suboptimal search: direct approach using
inadmissible estimates. Proc. 22nd International Joint Conference Artificial Intelligence, IJCAI 2011, Barcelona, Catalonia, Spain, July 16-22, 2011, pp.
674679. AAAI Press.
Vallati, M., Fawcett, C., Gerevini, A., Hoos, H., & Saetti, A. (2011). ParLPG: Generating domain-specific planners automatic parameter configuration LPG.
Proc. 7th International Planning Competition, IPC 2011, Deterministic Part.
http://www.plg.inf.uc3m.es/ipc2011-deterministic.
Veloso, M. M., Perez, A., & Carbonell, J. G. (1990). Nonlinear planning parallel
resource allocation. Proc. DARPA Workshop Innovative Approaches
Planning, Scheduling Control, San Diego, California, November 5-8, 1990, pp.
207212. Morgan Kaufmann.
Wang, Y., Audibert, J., & Munos, R. (2008). Algorithms infinitely many-armed bandits.
Proc. 22nd Annual Conference Neural Information Processing Systems,
NIPS 2008, Vancouver, British Columbia, Canada, December 8-11, 2008, pp. 1729
1736. Curran Associates, Inc.
Xie, F., Nakhost, H., & Muller, M. (2012). Planning via random walk-driven local search.
Proc. 22nd International Conference Automated Planning Scheduling,
ICAPS 2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012, pp. 315322. AAAI Press.
Xie, F., Valenzano, R. A., & Muller, M. (2010). Better time constrained search via randomization postprocessing. Proc. 23rd International Conference
Automated Planning Scheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013,
pp. 269277. AAAI Press.
Young, H. P., & Levenglick, A. (1978). consistent extension condorcets election principle. SIAM Journal Applied Mathematics, 35 (2), 285300.
Zhou, R., & Hansen, E. A. (2005). Beam-stack search: Integrating backtracking beam
search. Proc. 15th International Conference Automated Planning
Scheduling, ICAPS 2005, Monterey, California, USA, June 5-10, 2005, pp. 9098.
AAAI Press.

435

fiJournal Artificial Intelligence Research 54 (2015) 233275

Submitted 06/15; published 10/15

Decision Making Dynamic Uncertain Events
Meir Kalech

KALECH @ BGU . AC . IL

Department Information Systems Engineering,
Ben-Gurion University Negev, Beer-Sheva, Israel

Shulamit Reches

SHULAMIT. RECHES @ GMAIL . COM

Department Applied Mathematics,
Jerusalem College Technology, Israel

Abstract
make decision key question decision making problems characterized
uncertainty. paper deal decision making environments information arrives dynamically. address tradeoff waiting stopping strategies. one
hand, waiting obtain information reduces uncertainty, comes cost. Stopping
making decision based expected utility reduces cost waiting, decision
based uncertain information. propose optimal algorithm two approximation algorithms. prove one approximation optimistic - waits least long optimal
algorithm, pessimistic - stops later optimal algorithm. evaluate
algorithms theoretically empirically show quality decision approximations near-optimal much faster optimal algorithm. Also, conclude
experiments cost function key factor chose effective algorithm.

1. Introduction
many real-world domains agent choose alternative among multiple
candidates, based utility. problem becomes complicated utility depends
events occur dynamically therefore decision based dynamically changing
uncertain information. domains, question whether stop particular point
make best decision given information available, wait information arrives
enable making better decision. problem trivial cost associated
waiting.
example, consider problem finding best stock buy stock market.
values stocks may change time due future events, publication companys
sales report change interest rate, etc. longer wait, information becomes
available and, result, decision made certainty. many real-world domains,
cost waiting. instance, example, cost stock reflects loss
caused investing money one candidate stocks. Thus, tradeoff
waiting strategy enables one acquire information decreases uncertainty
stopping strategy, reduces cost.
Another example relates scheduling systems meetings. Determining best time
meeting could depend many factors, times meetings, location,
schedule attendees. Typically, factors may change dynamically influence decision
best time meeting; obviously, longer one waits, information becomes
c
2015
AI Access Foundation. rights reserved.

fiK ALECH & R ECHES

available probability choosing best time meeting higher. However, waiting
incurs cost possibility chosen time slot might longer available. goal
paper determine best time make decision maximizes expected gain
considers cost waiting.
question, whether wait get information not, also raised context
real estate investment. many unknown factors may influence decision
real estate property buy. example, infrastructure development area (like railway
station), raising/reducting municipal taxes area, construction polluting factory area,
etc. question whether choose real estate property based expected gain
wait get information next factor taking risk properties prices may
increase.
tradeoff uncertainty cost related optimal stopping problem (Ferguson,
1989; Peskir & Shiryaev, 2006), problem decision making bounded-resource (Horvitz,
2001, 2013), problem decision making multiple informative expensive observations
(Krause & Guestrin, 2009; Tolpin & Shimoni, 2010), Max K-Armed Bandit problem (Cicirello
& Smith, 2005) ranking selection problem (Powell & Ryzhov, 2012). work copes
challenge stopping problem multiple alternatives affected uncertain
information arrives dynamically. decision whether stop wait, problem, depends
utility affected result certain events occur next time stamps. Since
alternative affected different events consider combination possible
events, makes problem hard different others.
paper, we:
1. Develop model representing arrival dynamic information influence
utilities candidates.
2. Present optimal exponential algorithm (OP IM AL) guarantees best decision
tradeoff certainty waiting costs.
3. Propose two polynomial approximation algorithms solve problem provide bounds
error. prove algorithms evaluate expected utility stopping optimally. However, one approximation algorithm optimistic (OP IM IST IC)
sense waiting evaluation overestimated. algorithm pessimistic
(P ESSIM IST IC), namely waiting evaluation underestimated.
4. Empirically evaluate optimal two approximation algorithms illustrate
advantages one them.
empirically evaluate three algorithms simulating stock market scenario. compare optimal approximation algorithms four baseline algorithms; one algorithm
makes decision beginning decision process, second algorithm decides
information obtained, third algorithm makes decision random time fourth
one makes decision half time steps. examine algorithms terms
quality decision (utility) runtime.
empirical evaluation shows cost function much influences quality
decision. cost function increases moderately (i.e. root function), pessimistic
algorithm becomes less effective optimistic algorithm improves quality decision
234

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

even slightly better pessimistic one. However, cost functions grow linearly
polynomially time pessimistic approach much better optimistic one,
cases even significant difference quality decision made
pessimistic approximation algorithm optimal algorithm. quality approximations much better baseline algorithms. runtime approximation algorithms
polynomial rather exponential runtime optimal algorithm.
work extension previous work (Kalech & Pfeffer, 2010; Reches, Kalech, &
Stern, 2011). work expand theoretical empirical parts research.
particular, theoretical aspect find prove approximation error expected
gain expected wait algorithms, prove complexity properties
pessimistic optimistic algorithms. greatly expanded evaluation presenting
influence different parameters, cost waiting, distribution variables.
Furthermore, empirically show pessimistic optimistic behaviour algorithms.
paper organized follows. next section present basic fundamentals
problem formally define it. Section 3 present optimal algorithm. optimistic approach illustrated Section 4 pessimistic approach Section 5. empirical evaluation
algorithms provided Section 6. Section 7 discuss related work conclusion
presented Section 8.

2. Model Description
describe model clearly use example inspired stock market. Assume decision
maker wishes choose one stock purchase among three stocks (c1 , c2 , c3 ). value
stocks influenced future events, consumer price index (CPI), interest rates, etc.
decision maker cannot evaluate influence future events stocks certainty
degree probability. Obviously, sooner decision taken, lesser
loss investing money. hand, longer waiting time,
information gathered knowing outcome expected events consequently
decision greater degree certainty made.
model, decision designated candidate; throughout paper refer
candidate set C = {c1 , c2 , ..., cm }. agent desires choose alternative set
C. candidates utility influenced information arrives dynamically. represent
dynamic information random variables. random variable event occurs specific
time. different outcomes event may influence utility candidate different
ways. described extensively later, candidate may influenced multiple events. Let us
define formally timed variable.
Definition 1 (timed variable) timed variable pair Xi , t, Xi discrete, finite random variable taking values xi1 , ..., xik represents time stamp, discrete
time horizon = {0, ..., h} horizon h.
Given timed variable Xi , t, time stamp random variable Xi assigned
one possible values xi1 , ..., xik .
example, timed variables stock market future events influence utility
stocks. X1 , 1 may represent expected decrease percentage interest rate time
1, time 1 represents one month now. X1 takes discrete values {x11 = 0.1, x12 =
235

fiK ALECH & R ECHES

0}. Another timed variable, X2 , 2, represents expected prospectus specific company
two months now, takes values {x21 = positive, x22 = negative}. One timed
variable X3 , 3, represents expected change percentage CPI three months. X3 takes
values {x31 = 0.1, x32 = 0.2}. example, X1 , 1 X3 , 3 influence candidate c1
X2 , 2 influences candidate c2 . timed variable may affect two candidates.
Definition 2 (assignment) assignment timed variable X, outcome xi X.
global assignment time t, denoted , assignment values timed variables whose
time stamp less equal t.
example, global assignment time 3 ( 3 ) may X1 = 0, X2 = positive,
X3 = 0.1, i.e., time 1 (after month) interest change, time 2 (after two months)
prospectus company positive, time 3 (after three months) CPI increased
0.1%.
candidates utility depends set timed variables, different sets lead different
utilities. use tree represent effect timed variables utility.
Definition 3 (candidate tree) candidate tree cti candidate ci tree. nj,i node cti ,
stands index candidate j index tree. internal nodes
associated timed variables. random variable corresponding node n denoted X(n)
time denoted (n). nj,i descendant nk,i (nk,i ) < (nj,i ). edges going
node n represent possible assignments X(n). edge X = x labeled
probability outcome denoted p(X = x). leaf n labeled utility U(n), representing
utility candidate affected assignments root leaf. CT represents set
candidate trees.
call assignments path starts root candidate tree cti ends
node nj,i time = (ni,j ) local assignment candidate ci denoted . Note
candidate tree represents estimate effect timed variables utility
candidates. Obviously, estimate may change time, due addition removal times
variables re-estimation probabilities utilities. case candidate trees
updated.

X3,t=3 0.4
n1,1

X1,t=1
n0,1

0.6

0.8

0.2

0.9

n2,1

n3,1

n5,1

80

55

X4,t=3 0.3
n1,2

X5,t=4
n4,1
0.8

0.1

Figure 1: Candidate tree ct1 .

X6,t=4
n4,2
0.6

n5,2

n3,2
75

65

0.7

0.4

0.2

n2,2

n6,1
60

X2,t=2
n0,2

40

n6,2
70

45

Figure 2: Candidate tree ct2 .

Figures 1 2 present two candidate trees built time stamp = 0. extension
three timed variables demonstrated above. n0,1 root candidate tree ct1 . time stamp
= 1, represents fact random variable X1 obtain outcomes time 1.
236

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

node n1,1 ct1 represents timed variable X3 , = 3. One possible outcome change
CPI (X3 ) 0.1% (left edge). probability outcome 0.8. alternative outcome
change CPI 0.2% (right edge) probability outcome 0.2. value
80 left leaf ct1 represents utility candidate c1 local assignment time 3
(13 ) is: interest decreased 0.1% (X1 = 0.1) CPI increased 0.1% (X3 = 0.1).
probability utility 0.4 0.8 = 0.32.
utility candidate known certainly leaves. However, expected utility
candidate calculated beforehand time (depth candidate tree) consider
subtree depth. expected utility trivially computed recursive function.
expected utility leaf utility internal node expected utilities
children. Formally:
Definition 4 (expected utility) Given node n cti , function EU(cti , n) returns expected
utility n:
{
U(n)
n leaf
EU(cti , n) =
j p(X(n) = xj )EU(cti , nj ) otherwise
nj represents successor node n via assignment X(n) = xj .
instance, expected utility root Figure 1 is: EU(ct1 , n0,1 ) = 0.4 (0.8 80 +
0.2 55) + 0.6 (0.9 60 + 0.1 65) = 66.3.
expected utility estimate real utility based information known
current time. Waiting next time reduces uncertainty candidates utilities
hence increases probability making good decision. However, waiting incurs cost. cost
either function assignments function time. sake simplicity,
paper represent cost function time. reality, cost waiting specific
time stamp usually higher utility gained time. Thus, enforce realistic cost,
bound cost maximum utility.
Definition 5 (cost) Given time stamp , CST (t) increasing function returns
approximated cost waiting time t.
expected gain node difference expected utility node
cost waiting node.
Definition 6 (expected gain) Given node n

cti , GN (cti , n)
=
EU (cti , n)
CST ((parent(n))).1 n root candidate tree then: GN (cti , n) = EU(cti , n).
reason reduce cost parent n, rather n EU(cti , n) represents
expected gain subtree rooted n without waiting outcomes X(n).
tradeoff first component GN , expected utility, second
component, waiting cost. objective paper present algorithm find
1. Since utility cost necessarily given scale normalized reduction.
normalization domain dependent.

237

fiK ALECH & R ECHES

time maximizes gain2 . Unfortunately, unable separate computation
optimal time make decision selection best candidate, since utilities
candidates depend future events. Therefore, define policy determine
situations decision maker might face.
Beside outcomes timed variables, cost function also influences decision
whether stop wait. cost function returns much smaller values difference
expected utilities time eventually lead wait decision.
hand, cost function returns values high eventually lead stop decision. examples paper use cost function CST (t) = t. function ensures
proportional cost values (0,1,2,3,4) relation utility values (4580). instance, according
estimate time stamp = 0, given global assignment: X1 = 0.1, X2 = positive,
expected gain candidate c1 GN (ct1 , n1,1 ) = 75 1 = 74 expected gain c2
GN (c2 , n1,2 ) = 68 2 = 66. result, decision maker decides stop choose c1 ,
otherwise wait next time stamp. policy dictates decision whether stop
wait.
Definition 7 (policy) policy function : {stop, wait}, set global
assignments.
policy specifies stop, decision maker chooses candidate current highest
expected gain.
Finally, define expected gain decision maker using policy , referred global
expected gain. understand definition first introduce another definition nodes
corresponding certain time.
Definition 8 (N ODEStj ) set N ODEStj represents following nodes ctj : (1) leaves
whose parents time less equal t: {n ctj |(parent(n)) t, n leaf}. (2)
internal nodes whose parents time less equal time greater {n
ctj |(parent(n)) (n) > t, n internal node}.
example, Figure 1 N ODES31
=
{n2,1 , n3,1 , n4,1 }, N ODES41
=
{n2,1 , n3,1 , n5,1 , n6,1 }.
global expected gain function obtains candidate trees, global assignment,
policy. policy specifies stop, global expected gain maximum expected gain
among candidates. Otherwise, recursively computes expectation global expected
gain different combinations roots children next time stamp. Formally:
Definition 9 (global expected gain) global expected gain function returns expected
gain choosing policy :
GEG(CT, , ) =


GN (ctj , nj )
( ) = stop
max
j

GEG(CT , ty , )P r(y) ( ) = wait

y{N ODES 1 ,...,N ODES }
t+1

t+1


2. Although decision making theory common maximize expected utility, use term expected
gain, incorporates cost order distinguish expected utility.

238

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

1. nj root ctj .
2. CT set candidate trees rooted nodes y.
3. P r(y) probability nodes given assignment .
4. ty represents union assignments X(nj ) represented y.
running example, global assignment t=2 policy ( t=2 ) = stop
= (n1,1 , n1,2 ), GEG(CT , , ) = max(73, 66) = 73. case policy ( ) = wait,
sum GEG possible assignments next time. set N ODES rooted
nodes n1,1 n1,2 time = 3 N ODES31 = {n2,1 , n3,1 } N ODES32 = {n2,2 , n3,2 },
correspondingly, {n2,1 , n3,1 } {n2,2 , n3,2 }. Therefore:
GEG(CT {n1,1 ,n1,2 } , , ) =
GEG(CT {n2,1 ,n2,2 } , , ) 0.8 0.8+
GEG(CT {n2,1 ,n3,2 } , , ) 0.8 0.2+
GEG(CT {n3,1 ,n2,2 } , , ) 0.2 0.8+
GEG(CT {n3,1 ,n3,2 } , , ) 0.2 0.2
Based definitions, define timed decision making problem (TDM):
Definition 10 (Timed Decision Making (TDM) problem) Given set candidate trees CT ,
TDM problem find policy maximizes GEG(CT, 0 , ).
Table 1 summarizes notation use describing model.
hardness TDM problem due computation expected gain waiting next time stamp. computation needs take consideration utility
candidate possible assignment time stamp. Specifically, time t0 , decision maker decide whether stop choose best candidate wait next
time stamp comparing expected gain stopping expected gain waiting.
expected gain stopping time stamp t0 computed immediately taking maximum
expected gain candidate trees (maxGN (ctj , nj ), n1 , ..., nm roots
j

candidate trees). hand, computation expected gain waiting next
time stamp hard. expected wait considers combination possible assignments
candidate trees time t1 . combination, need take consideration
utility stopping time t1 utility waiting time stamp t2 include additional
combinations assignments on. problem hard number combinations
exponential number candidates. Specifically, certain time stamp, k possible
assignments different variables different candidate trees, number combinations k .
order prove TDM NP-hard, present timed decision making problem
decision problem . Given CT, 0 non-negative integer K. Answer yes exist
policy global expected gain GEG(CT, 0 , ) K.
Theorem 1: TDM problem NP-hard. (The proof appears Appendix A).
One option representing problem would use Markov Decision Processes (MDP).
model, states time would global assignments time ( ) actions would
either select best candidate time (stop) wait one time step. transition
239

fiK ALECH & R ECHES

Parameter
C = {c1 , ..., cm }
Xi ,
cti
CT


nj,i
X(n)
(n)
= (0, ..., h)
U (nij )
EU(cti , n)
CST (t)
GN (cti , n)
policy
N ODEStj
GEG
EW( , )
ES( , )
PT H(cti , n)
P rPT H(cti , n)


Description
set candidates.
timed variable, Xi discrete, finite random variable
time stamp.
candidate tree ci .
set candidate trees.
assignment values timed variables whose time stamp less
equal t.
local assignment. assignments path stats root
candidate cti end node whose time t.
node candidate tree cti candidate ci .
random variable corresponding node n.
time random variable corresponding node n.
time horizon.
utility candidate cti affected assignments root
leaf nij .
expected utility candidate tree cti node n.
approximated cost waiting time t.
expected gain node n candidate tree cti difference
expected utility node cost waiting node.
function : {stop, wait}.
set nodes whose parents time less equal
leaves, time greater t.
Global expected gain.
expected gain waiting next time stamp (t+1), using policy
.
expected gain stopping time stamp t, using policy .
path, set local assignments root cti node n.
probability path.
Local policy candidate ci .
Table 1: notation used description model.

240

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

function time state time + 1 state wait action would given product
probabilities time + 1 assignments. stop action leads terminal state reward
received equal gain winning candidate.
usual advantage MDP formulation possibility using dynamic programming
methods, value policy iteration. However, problem, dynamic programming
provides benefits state cannot reached different paths number
states exponential total number timed variables trees.
One methods addresses large MDPs means factored MDPs (Boutilier,
Dearden, & Goldszmidt, 2000; Guestrin, Koller, Parr, & Venkataraman, 2003). approach
viable domain utility stopping maximization utilities
trees depends timed variables. show Section 4, special structure
problem readily apparent MDP factored MDP formulation.

3. Optimal Algorithm
optimal gain calculated straightforwardly decision tree approach. optimal
decision tree merges candidate trees single decision tree whose depth maximal time
timed variables candidate trees. decision tree, define three kinds nodes:
Decision nodes, decision maker must decide whether stop wait; decision
stop, candidate choose.
Stop nodes, decision maker stopped chosen one candidates.
Wait nodes, decision maker decided wait.
node marked time stamp. Edges leading wait nodes labeled conjunctions assignments. Every node tree marked set assignments,
assignments path leading node.
tree constructed offline time follows:
Procedure 1 Optimal:
1. root decision node time stamp 0.
2. time stamp, children decision nodes time stamp wait nodes
time stamp t; candidate stop node child. final time stamp, wait
node child included.
3. Stop nodes leaves tree. stop node corresponds node n cti time t,
value node GN (cti , n) (Definition 6).
4. children wait node time stamp given global assignment determined
follows:
(a) local assignment passes path assignments ending node. Let us
denote node ni candidate tree cti .

(b) Let Xn = X(ni ).
241

fiK ALECH & R ECHES

Decision Time Horizon
66.84

0
66.84

66.3

58.9
0.4

0.6

73.8

57.7

73.8
8

59.3

73.26

0.3

65.6
6 0.64

76.4

76.4 71.4
4
4

76.4 36.4
4
4

75.2

75.2

70.2

75.2
2
1

70.2

75.2

75.2

35.2

51.4 36.4
4
4

50.2

70.2

50.2
2
1
50.2

50.2

35.2

0.7

0.8

72.36
66
0.2

58.1

68.5

65.6
6
0.8

56.2

75.2 75.2

65.2

50.2

56.9

71.4 70.2
4
2
0.9 0.1
70.2

70.2

52.6
6

58.1

59.5
1

0.2

71.4

76.4 51.4 75.2 51.4 51.4 56.2
2
4
4
2
4
4
0.4 0.6
0.4 0.6

2

59.5

68.5

76.4

51.4

51.4 71.4 70.2
4
4
2
1

52.6
6

0.04

71.4

76.4

75.2
2
1

0.16

72.6
6

62.24
0.3

72.6

74.6
0.16

57.7

0.7

74.6

72.6
6

1

62.2

56.9

56.9

36.4 55.7
4
0.9 0.1
55.2

59.5

57.9

60.2

52.4
4

3

59.5

0.36 0.54 0.04 0.06
65.2

55.2

65.2 60.2

4

40.2
75.2 65.2 75.2 40.2 50.2 65.2 50.2 40.2 55.2 70.2 60.2 70.2 55.2 35.2 60.2 35.2 55.2 65.2 55.2 40.2 60.2 65.2 60.2

Figure 3: Optimal decision tree ct1 ct2 candidate trees.
(c) possible joint outcome timed variables Xn , wait node child,
labeled joint probability. child decision node time stamp + 1.
tree constructed, evaluated using simple bottom-up process.
gains leaves, i.e., stop nodes, already calculated. gain wait node
expectation utilities children. gain decision node maximum gain
children optimal decision one leads maximal gain. decision tree
generated evaluated advance assignments undertaken. solution
represents policy (Definition 7).
Figure 3 presents optimal decision tree decision problem candidate trees ct1
(Figure 1) ct2 (Figure 2). time line right graph represents time horizon
decision. rectangular nodes represent decision nodes; shaded ellipse nodes represent
wait nodes empty ellipse nodes represent stop nodes3 . stop nodes come pairs, one
candidate; node c1 left. numbers nodes represent expected
gains computed using bottom-up algorithm. example, used cost function
linearly grows time: CST (t) = 1.2t, cost reaching leaf 4.8 (since four
time stamps).
example, consider dashed triangle right-hand side figure. root
subtree shown triangle wait node time stamp 3. determining children
node, consider timed variables candidate trees time stamp 4. two
timed variables, X5 , 4 candidate c1 X6 , 4 candidate c2 . need split
joint outcomes two candidates wait node four children.
children decision node time stamp 4. Since last time stamp, decision nodes
stop nodes children.
One particular course events shown bold figure. Since expected gain waiting
(66.84) higher expected gain stopping, expected utility choosing
3. stop nodes final time ellipses readability. also omitted assignment labels
edges.

242

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

best stop node (66.3), agent wait. Accordingly, assignment X1 = 0 (left)
occur. next decision node child, expected gain stopping choosing candidate
c1 (73.88) higher expected gain waiting (73.26), agent may stop = 1
choose c1 . Assume sequence assignments occurred depicted bold figure.
eventually lead leaf node gain 75.2 shown. Note, however, gain
incorporates cost waiting 4 time stamps. Thus, agent omniscient known
outcomes timed variables advance, would obtained gain 80, since would
wait all. rational agent stops time 1, gain choosing c1
80 1.2 = 78.8.
Denote optimal policy. Given global assignment , use EW( , ) represent
expected gain waiting next time stamp executing optimal algorithm. equals
wait node given . Similarly, use ES( , ) represent expected gain stopping
time stamp executing optimal algorithm; equals maximal stop node given .
example, Figure 3, EW( 0 , ) = 66.84 ES( 0 , ) = 66.3.
optimal decision tree explicitly represents state space MDP model, described
Section 2. Since state space represented tree (rather graph), intelligent value
iteration process equivalent backward induction algorithm use optimal decision
tree.
3.1 Analysis
time complexity optimal algorithm affected fact optimal decision tree
considers different combinations paths candidate trees. Let maximum size
candidate tree number candidates m. Notice size candidate tree
exponential depth local candidate tree. Specifically, given depth candidate
tree, h (the horizon), number outcomes timed variable, k (the branching factor)
= k h . total number timed variables size depth optimal
decision tree bounded log(M ). worst-case time complexity computing optimal
tree O(M ).
mentioned above, backward induction optimal decision tree equivalent
value iteration MDPs. Since state space represented tree, value iteration simply
scans state space. Thus, complexity value iteration size state space.
complexity state space, described Section 2, sum global assignment alternatives time. worst case, every candidate depends different timed variables


time, k alternatives time 1, (k 2 ) alternatives time 2, (k h ) alternatives
time h. complexity identical complexity optimal decision tree O(M ) (since
O(k h )).
best-case complexity archived candidates affected timed variable, since consider combinations timed variables. case
trees candidates identical except utilities leaves complexity
thus mM .
Beyond exponential complexity optimal algorithm, another disadvantage algorithm stems fact every change candidate trees demands rebuilding decision
tree. Unfortunately, due exponential complexity rebuilding feasible. cope
exponential complexity optimal algorithm fact feasible rebuild de243

fiK ALECH & R ECHES

cision tree timed variables change, propose two approximation algorithms following
sections.
Beyond exponential complexity optimal algorithm, another disadvantage algorithm stems fact every change candidate trees demands rebuilding decision
tree. optimal algorithm computes whole combinations future events advance
result, makes optimal solution long initial evaluation probabilities
utilities event valid. Since complexity optimal algorithm exponential, may
infeasible rebuild new decision tree time stamp. However, realistic scenarios
evaluation events utilities may change time. cope exponential
complexity optimal algorithm fact feasible rebuild decision tree
timed variables change, following sections propose two approximation polynomial
algorithms.

4. Optimistic Approach
optimal algorithm presented Section 3 considers candidates simultaneously thus
grows exponentially number candidates. section, present alternative algorithmic framework considers candidates separately dynamically. alternative viewpoint
lead efficient approximation algorithm.
4.1 OPTIMISTIC Algorithm
main idea behind alternative framework calculating utility candidate tree
separately combining utilities together obtain evaluation global gain.
way avoid complexity comparing assignment assignments
candidates; candidate contributes separately overall utility. Specifically, candidate
contributes overall utility actually prevails candidates. Thus
estimate utility node candidate tree product expected gain
probability candidate win, given node reached. Then, order estimate
overall gain, sum utility candidates. formally describe algorithm
present following definitions:
Definition 11 (path) Given node n cti , function PT H(cti , n) returns set local assignments root n {Xi1 = xi1 , Xi2 = xi2 , ...} candidate tree cti .
Definition
12 (probability path) Given

jPT H(cti ,n) P r(j).



node

n



cti ,

P rPT H(cti , n)

=

probability ci prevail specific candidate cj time sum probabilities prevail cj possible assignment, i.e., node N ODEStj .
probability candidate ci prevail candidates, given specific node nx,i cti
specific time stamp, = (nx,i ), sum probabilities prevail
candidate current time t. Formally4 :
4. mathematical calculations probabilities approximation algorithms, assume candidates
disjointed sets timed variables probabilistically independent; means two candidates
affected time variable. Nevertheless, shown results experiments, algorithms
perform well even common variables.

244

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

Definition 13 (probability winning) Given node nx,i cti , probability ci win is5 :
P r(ciwins|nx,i ) =

IsW in(EU(cti , nx,i ), EU (ctj , ny,j ))P rPT H(ctj , ny,j )

j{1,...,m},i=j ny,j N ODES j



1. = (parent(nx,i )).
2. IsW in(EU (cti , nx,i ), EU(ctj , ny,j )) =
{

1 EU(cti , nx,i ) > EU(ctj , ny,j )
0 else

example, using Figures 1 2 above, recall N ODES31 = {n2,1 , n3,1 , n4,1 }.
P r(c2 wins|n2,2 ) =
IsW in(EU (ct2 , n2,2 ), EU(ct1 , n2,1 )) 0.4 0.8+
IsW in(EU (ct2 , n2,2 ), EU(ct1 , n3,1 )) 0.4 0.2+
IsW in(EU (ct2 , n2,2 ), EU(ct1 , n4,1 )) 0.6 =
0 0.32 + 1 0.08 + 1 0.6 = 0.68
define relative expected gain candidate contribution global expected gain given specific node.
Definition 14 (relative expected gain) Given node nx,i cti , relative expected gain candidate ci GN (cti , nx,i ) P r(ci wins|nx,i ).
Notice probability candidate ci win time stamp is:

P r(ci wins) =
P r(ci wins|nx,i ) P r(nx,i )
nx,i N ODESti

thus according law total probability:


P r(ci wins|nx,i ) P r(nx,i ) = 1
nx,i N ODESti

computation relative expected gain nx,i presented Algorithm 1. line 3
go candidate trees except candidate tree cti . line 5 go candidate
trees nodes time nx,i time. sum probabilities
nodes whose expected utility less nx,i (lines 68). sum represents
probability ci win cj , given node nx,i . Finally, line 10 multiply probability ci
prevail candidates (given node nx,i ), since winning candidate prevail
candidates. return product probability expected gain node
5. Ties candidates broken consistent manner.

245

fiK ALECH & R ECHES

Algorithm 1 RELATIVE EXPECTED GAIN
(input: candidate trees CT = {ct1 , ..., ctm }
input: node nx,i
output: relative expected gain nx,i )

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

(nx,i )
prob 1
ctj CT (i = j)
temp 0
ny,j N ODEStj
EU (cti , nx,i ) > EU (ctj , ny,j )
temp temp + P rPT H(ctj , ny,j )
end
end
prob prob temp
end
return prob GN (cti , nx,i )

nx,i . instance, relative expected gain choosing candidate c2 given node n2,2 time
3 (with cost function CST (t) = 1.2t) (75 3.6) 0.68 = 48.552.
optimistic approach determines policy (Definition 7) constructing separate decision
tree candidate. policy determined based local assignment time.
mentioned earlier local assignment certain candidate tree derived global
assignment. estimate global expected gain stopping waiting depends
policy expected gain candidate separately. call policy local policy.
Obviously, may possible certain time local policies candidates
different.
Definition 15 (local policy) local policyi candidate ci rule dictates either stopping
waiting local assignment .
assignment time t, optimistic decision maker decides policy building
individual decision trees based relative expected gain candidate. expected gain
stopping time sum relative expected gain candidates. optimistic
procedure invoked first time 0:
Procedure 2 Optimistic:
1. Generate individual candidate decision tree candidate ci based cti manner
similar optimal decision tree except relative expected gain used instead
expected gain.
2. Denote stop node current time stamp candidate tree cti ES ( , )
wait node current time stamp candidate tree cti EW ( , ).
Denote also:

ES( , ) =
ES ( , )
i{1,...,m}

EW( , ) =
EW ( , ).
i{1,...,m}

246

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

3. ES( , ) EW( , )
= stop, return argmax ES ( , )
i{1,...,m}

else = wait.
4. Prune candidate tree according global assignment tree rooted
node reached local assignment. Invoke Procedure 2 next time stamp + 1.
consider stop values root, exactly one candidate probability winning 1 others probability 0. Therefore, expected gain winning
candidate equals sum values stop nodes root decision trees. Intuitively, value wait node candidate estimate candidates contribution
benefit waiting. Therefore, algorithm evaluates expected utility waiting summing expected wait candidates. summation value greater maximum
immediate expected gain, total expected gain waiting greater expected gain
stopping. case decision wait. Otherwise, decision stop
choose candidate highest expected gain.
agent decides wait decision trees must updated according new assignments obtained waiting. new assignments prune parts candidate trees.
instance, consider candidate tree ct1 Figure 1. Assume agent decided wait
outcome variables time 1. Assume assignment timed variable X1 time 1
left, right subtree ct1 pruned since longer influences utility ct1 .
N ODESt1 set, associated specific time, changes result pruning
well computation relative expected gain. Therefore rebuilding decision trees
necessary.
Figures 4 5 present decision tree ct1 ct2 , respectively (Figures 1 2). leaves
contain two numbers; first represents expected utility second (in bold) represents
relative expected gain.
example, let us compute relative expected gain rightmost bottom-level node n6,1
Figure 1. cost function CST (t) = 1.2 t. node, utility 65, greater
two nodes ct2 n3,2 utility 40 n6,2 utility 45. total probability
two nodes ct2 defeated n6,1 0.3 0.2 + 0.7 0.6 = 0.48. probability
c1 defeat c2 given utility 65. compute relative expected gain node
(see Definition 14), multiply gain, 65 4.8 = 60.2, probability winning,
resulting 28.9 (see rightmost bottom-level node Figure 4).
Based decision trees Figures 4 5 find Optimistic algorithm decision
time 0 wait, since sum wait nodes = 0 (89.3) greater sum
stop nodes (66). Suppose timed variables X1 , 1 assigned left outcome.(as
example optimal algorithm). decision trees updated. Candidate c1 tree
pruned includes subtree rooted n1,1 result relatives expected
gain candidate trees c1 c2 updated. Figures 6 7 present obtained trees
Optimistic algorithm case. According trees, algorithm time stamp = 0, decides
wait, since sum wait nodes (84), higher sum stop nodes(73.8).
next decision node child, expected gain stopping choosing candidate c1 (73.88)
higher expected gain waiting (73.2), agent may stop = 1 choose c1 .
247

fiK ALECH & R ECHES

Decision Time Horizon
0

24.2
24.2

(58.9) 0

1

1
24.2

(58.9) 0
0.3

24.2
0.7

39.4

38.8

(68) 39.4
0.8

(55) 0

1
0

47.7 (40) 0

0

47.7

0.4
0

44.3

(40) 0

(70) 44.3

0.6
0

4

(45) 0

Figure 5: Candidate decision tree ct2 (built
= 0).

Decision Time Horizon
1

11.4

(58.9) 0
0.3

11.4
0.7

13.4

10.5

11.6

(68) 13.4
0.8

(75) 14.5

(55) 0

1
0

10.5

14.3 (40) 0
1
14.3

(75) 57.1

2
10.5

0.2

14.5

Figure 6: Candidate decision tree ct1 (rebuilt
= 1).

17.7

(55) 4.1

1

(75) 47.7

3

17.7

1

Figure 4: Candidate decision tree ct1 (built
= 0).

17.7

0.2

48.5

(75) 48.5

2

17.7

0
1

3

5.3

(55) 10.5
0.4

0

13.3

(40) 0

(70) 13.3

0.6
0

4

(45) 0

Figure 7: Candidate decision tree ct2 (rebuilt
= 1).

4.2 Analysis
time complexity optimistic approximation polynomial number candidates
since build decision tree every candidate separately.
248

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

Theorem 1 time complexity building decision trees optimistic approximation
O(M 2 m2 ), number candidates maximal size among candidate
trees.
Proof: evaluating candidate tree, must compute probability winning
O(M ) nodes. node, perform summation O(M ) nodes
candidate trees cost O(M 2 m). perform candidate trees. Thus,
total cost algorithm O(M 2 m2 ). 2
result compares favorably O(M ) optimal algorithm number candidates
large. addition, due polynomial complexity due fact optimistic algorithm rebuilds decision trees update probabilities, easily update decision tree
new dynamic events updated probabilities utilities. instance, assume time
= 0 prediction interest rate increase time = 2 0.1% probability
0.8 0.2% probability 0.2. time = 1 prediction may change, instance,
0.15% probability 0.6 0.2% probability 0.4. Since optimistic algorithm
rebuilds polynomial time decision trees easily consider updated probabilities
values.
show Procedure 2 returns stopping policy, optimal algorithm would
decide same. algorithm waits, implies expected gain waiting greater
expected gain stopping. case, optimal waiting expectation could lower.
Theorem 2 Given global assignment , policy obtained Procedure 2
optimal policy, ES( , ) = ES( , ) EW( , ) EW( , ).
Proof: First prove ES( , ) = ES( , ). Procedure 2 calculates ES( , ) time
stamp
stop nodes nx,i candidate trees cti time: ES( , ) =
summing

ES ( , ). nodes relative expected gain stopping time and,
i{1,...,m}

according definition 14, GN (cti , nx,i )P r(ci wins|nx,i ). Since global assignment
time known, exactly one candidate ci (the candidate highest expected gain time t)
confirms P r(ci wins|nx,i ) = 1 others confirm P r(cj wins|nx,j ctj ) = 0. result,
ES( , ) = GN (cti , nx,i ), ci candidate highest expected gain time t. Thus,
ES( , ) = ES( , ).
, ). optimistic approach estimates expected
prove EW( , )
EW(


waiting EW( , ) sum EW ( , ). Since every decision tree cti
global assignment optimistic approach chooses policy maximizes EW ( , )
(looks optimal time stop local assignment takes combination
utilities) independently candidate trees, possibility sum EW ( , )
includes relative expected gain one candidate stopping specific time stamp
relative expected gain another candidate waiting till time stamp + 1 global
assignment. Since relative expected gain optimal, EW( , ) EW( , ).2
Corollary 1 Based last theorem, given policy obtained Procedure 2, ( ) = stop,
optimal policy would decide same. optimistic policy decides
stop, ES( , ) > EW( , ). Then, based last theorem, EW( , ) EW( , )
ES( , ) = ES( , ), thus ES( , ) > EW( , ), namely optimal policy declare
249

fiK ALECH & R ECHES

stopping policy too. Therefore, optimistic approach guarantees optimal expected gain
stopping.
prove approximation error expected wait. Notice following theorem
discuss error optimistic algorithm focuses worst case error
estimating waiting gain optimistic algorithm.
Theorem 3 Given time horizon = (0, ..., h), policy obtained Procedure 2
f 1
policy obtained optimal algorithm, EW( 0 , ) EW( 0 , ) i=1
EU(cti , ni ) +
CST (h), ni root candidate tree cti , EU (cti , ni ) expected utility node
nij candidate tree cti f = in(m, h) (m number candidates).

Proof: According optimistic approach, EW( , ) = EW ( , ). Since expected
wait candidate EW ( , ) computed independently, global expected wait EW( , )
may include, specific assignment, stopping gain one candidate waiting gain
another candidate simultaneously (even though combination impossible).
worst case scenario, whereby EW( , ) highest value occurs time
stamp, exactly one local policy ( ) = stop. situation, expected wait sum
expected stop different time stamps f candidates, f = in(m, h). Thus, (1)

EW( , ) fi=1 EU(cti , ni ), ni root candidate tree cti . Now, EW( , )
EU(ctj , nj ) CST (h) EU (ctj , nj ) highest expected utility among roots
candidate trees. Thus, (2) EW( , ) EU(cti , nj ) + CST (h) result, summing
1
(1) (2), EW( , ) EW( , ) fi=1
EU(cti , ni ) + CST (h). particular, = 0:

1
EW( 0 , ) EW( 0 , ) fi=1
EU (cti , ni ) + CST (h). 2
Finally, prove approximation error global expected gain. actually cost
waiting till level lh1 , lh last level.
Theorem 4 Given time horizon = (0, ..., h), policy obtained Procedure 2,a cost function
CST (t), set candidate trees CT , global assignment optimal policy , global
expected gain GEG holds: GEG(CT, 0 , ) GEG(CT, 0 , ) CST (h 1).
Proof: According Corollary 1, Procedure 2 guarantees optimal policy ( ) = stop.
result, error obtained ( ) = wait ( ) = stop. Since
waiting next time stamp decreases uncertainty, error cost waiting.
worst case scenario, Procedure 2 may wait last time stamp optimal policy
would stop immediately. However, policy obtained Procedure 2 time h 1 optimal,
GEG(CT, h1 , ) = GEG(CT, h1 , ). reason behind value
EW( h1 , ) considers local policies ( h1 ) = wait, estimated expected wait
policy, sum local expected wait, optimal, since include
relative gain waiting stopping assignment.
result, absolute approximation error GEG(CT, 0 , ) GEG(CT s, 0 , ) <
CST (h 1).2

5. Pessimistic Approach
section present alternative approximation algorithm which, contrast former,
presents pessimistic approach. algorithm considers expected utility time stamp
250

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

separately. result, avoid exponential complexity optimal algorithm considers possible combinations waiting stopping time stamp.
5.1 PESSIMISTIC Algorithm
approximation gain calculated united decision tree. approach merge
candidate utility functions single decision tree, level tree represents time
stamp associated timed variable, i.e., level li tree represents time point ti
decide whether stop wait. decision tree two nodes level:
Stop node, decision maker stops chooses one candidates. stop node, ES ,
expected utility stopping level li .
Wait node, decision maker decides wait. wait node, EW , expected utility
waiting next time level. maximum stop node wait
node level li+1 .
approximate solution time stamp compute expected utility stopping
(ES ) level. stopping, optimal choice candidate highest expected
utility. compute expected utility stopping (ES ) optimally, compute expected
utility winning candidate possible assignment multiply probability
assignment. brute force approach consider combinations assignments
timed variables one return product winners expected utility
probability assignment. approach obviously exponential number candidates
since size assignment combinations exponentially affected number candidates.
Alternatively, relax time complexity sorting expected utilities candidate. way easily find winner multiply expected utility probabilities
assignments candidates lower expected utility. Since expected utilities
sorted, computation linear number candidates. Theorem 5 analyze
time complexity detail.
describe calculation Algorithm 2, use definition N ODES (see Definition
8). algorithm obtains time set candidate trees CT returns expected utility
stopping time. one candidate trees, lines 811 sort nodes times
less equal (N ODEStj ) according expected utility. sorting done inverse
order ordered nodes inserted array sj []. arrays added set S. order
iterate arrays, initiate pointers arrays; indx[] contains pointers arrays,
indx[j] contains pointer array sj []. pointers initiated point first node
corresponding array (lines 1214). main loop (lines 1519), find node
highest expected utility among nodes pointed at. compute probability winning,
multiply probability lower probabilities nodes candidates.
Namely, one candidate trees, sum probabilities nodes
lower expected utility winner (line 17) multiply summation probability
node currently wins. sum probabilities array candidate ci (si [])
lower cj actually probability cj greater ci thus probability
beat it. Since arrays sorted, summation actually done current pointer
end arrays. follows law total probability. Finally, line 18, increment
251

fiK ALECH & R ECHES

Algorithm 2 EXPECTED STOPPING
(input: time t)
(input: candidate trees CT = {ct1 , ..., ctm })
output: expected stopping ES( , )

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

Internal variables:

indx[m]
i1
j1
exp 0
best
ctj CT
j
sj [] sort
N ODESt inverse order
sj []
end
j
indx[j] 1
end
j m, indx[j] reach end sj []
best k, k confirmssk [indx[k]] si [indx[i]] {1, ..., m}
exp

exp
+
EU (ctbest , sbest [indx[best]])

|si []|
P
rPT
H(ct
,

[k])

i=best
k=indx[i]
18:
indx[best] indx[best] + 1
19: end
20: return exp CST (t)

s0=66.3



P rPT H(ctbest , sbest [indx[best]])



w1=65.904

s1=65.1

w1=65.904
s2=65.255

w2=65.904

s3=65.17

w3=65.904

s4=65.904
Figure 8: Pessimistic approach: decision tree based ct1 ct2 .
pointer local winner next node find winner next iteration. continue
loop one candidate nodes scanned. case, unscanned utilities
arrays less last utility array completely scanned. line
20 function subtracts cost waiting exp. algorithm demonstrated
net page.
next procedure describes pessimistic decision tree time stamp t. tree rebuilt
time stamp. invoked first time 0:
Procedure 3 Pessimistic:
252

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

1. Generate decision tree bottom-up manner:ES {0, ..., h} computed based
Algorithm 2. Then, EW h1 equal ES h EW {0, ..., h 2} maximum
ES i+1 EW i+1 . building iterates root time stamp t. Denote ES
EW ES( , ) EW( , ) respectively.
2. ES( , ) EW( , )
then, = stop, return argmax ES ( , )
i{1,...,m}

else = wait.
3. Prune candidate tree according global assignment tree rooted
node reached local assignment invoke Procedure 3 time stamp + 1.
Let us demonstrate approximate decision tree (Figure 8). Figure 1 Figure 2 represent
two candidate trees CST (t) = 1.2 t. generate decision tree bottom-up manner
since waiting node actually maximum nodes next level. last level l4 ,
one node, ES 4 . order calculate expected utility stopping, use Algorithm
2. N ODES41 = {n2,1 , n3,1 , n5,1 , n6,1 }, N ODES42 = {n2,2 , n3,2 , n5,2 , n6,2 }. Algorithm 2 sorts
sets s1 s2 : s1 = [80, 65, 60, 55], s2 = [75, 70, 45, 40]. first iteration (lines
1519), pointer winner best = 1 since s1 [1] = 80 > s2 [1] = 75. Thus,
exp = s1 [1] P rPT H(ct1 , n2,1 )
(P rPT H(ct2 , n2,2 ) + P rPT H(ct2 , n3,2 )+
P rPT H(ct2 , n5,2 ) + P rPT H(ct2 , n6,2 )) =
80 (0.4 0.8) (0.3 0.8 + 0.7 0.4 + 0.7 0.6 + 0.3 0.2) = 25.6
pointer s1 incremented point s1 [2]. next iteration, best = 2 since s2 [1] =
75 > s1 [2] = 65. Thus,
exp = exp + s2 [1] P rPT H(ct2 , n2,2 )
(P rPT H(ct1 , n6,1 ) + P rPT H(ct1 , n5,1 ) + P rPT H(ct1 , n3,1 )) =
exp + 75 (0.8 0.3) (0.6 0.1 + 0.6 0.9 + 0.4 0.2) =
exp + 12.24 = 37.84
Lastly, exp = 70.704 expected utility stopping ES 4 = 70.704 4.8 = 65.904.
EW 3 = ES 4 , since wait node time t4 . Similarly, according Algorithm 2,
calculate ES 3 based N ODES31 = {n2,1 , n3,1 , n4,1 }, N ODES32 = {n2,2 , n3,2 , n4,2 }: ES 3 =
65.172. EW 2 = max(ES 3 , EW 3 ) = 65.904. complete decision tree presented Figure 8.
runtime, decision maker decides wait stop according ES 0 EW 0 (in first
iteration = 0). agent decides stop ES 0 > EW 0 chooses candidate
highest expected utility. agent decides wait, several assignments occur.
point decision tree needs recomputed several nodes become irrelevant.
presented example agent decide stop time stamp = 0 since expected stop, ES 0 (66.3)
higher expected wait EW 0 (65.904).
5.2 Analysis
First, show time complexity pessimistic approach.
253

fiK ALECH & R ECHES

Theorem 5 Given time horizon = (0, ..., h), time complexity building decision tree
pessimistic approximation O(h (m2 + mM log )), number candidates
maximal size among candidate trees.
Proof: time stamp ti , algorithm sorts nodes set N ODEStji candidate tree
ctj . Since maximum number nodes N ODEStji , worst case complexity sort
log . Since perform sort candidate tree, complexity O(mM log ).
compare sorted sets set S, algorithm goes candidates finds maximum
among pointed nodes candidates. computation m2 . algorithm stops
reaches end one candidates array (line 15). worst case . Finding P rPT H
node calculated loop complexity mM log . Thus, worst case
time complexity Algorithm 2 O(m2 + mM log ). perform Algorithm 2 time
stamp result time complexity O(h (m2 + mM log )).2
Similar optimistic algorithm, pessimistic algorithm rebuilds decision tree
time stamp polynomial time thus address changed additional timed variables.
show Procedure 3 decides wait optimal algorithm would operate similarly.
expected gain stopping greater expected gain waiting, Procedure 3
returns stopping policy. case optimal policy could return waiting policy.
Theorem 6 Given cost function CST (t), set candidate trees CT , global assignment
optimal policy , policy taken Procedure 3 optimal policy , global expected
gain GEG holds: ES( , ) = ES( , ) EW( , ) EW( , ).
Proof: time t, Algorithm 2 calculates expected gain stopping, ES( , ),
summing possible assignment expected utility candidate highest value
(the winner candidate) times probability assignment. Since time stamp sum
probabilities possible assignments 1, according law total probability (Beaver
& Mendenhall, 1983), ES( , ) = ES( , ). waiting node maximum
wait node stop node next time level. Therefore, algorithm take
consideration combination waiting stopping different assignments. contrast,
optimal policy algorithm considers combinations takes maximal value
assignment. Thus, value expected wait may higher result wait nodes value
less equal optimal expected gain waiting, EW( , ) EW( , ).2
Corollary 2 Based last theorem, ( ) = wait, optimal policy would result
decision. due fact policy obtained Procedure 3 ( ) =
wait, EW( , ) > ES( , ). Based last theorem EW( , ) EW( , )
ES( , ) = ES( , ), thus EW( , ) > ES( , ) therefore, policy decides wait.
Consequently, pessimistic approach guarantees optimal expected gain waiting.
prove approximation error expected wait.
Theorem 7 Given time horizon = (0, ..., h), cost function CST (t), set candidate trees
CT , global assignment global assignment , policy obtained Procedure
3 policy obtained optimal algorithm, EW( , ) EW( , ) CST (h)
CST (t + 1).
254

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

Proof: optimal expected gain obtained stopping waiting specific time
stamp expected gain waiting last time stamp h (where uncertainty) without considering cost waiting, ES( h , ) + [CST (h) CST (t)]. Thus,
expected gain waiting time holds, (1) EW( , ) ES( h , )+[CST (h)CST (t+1)]
(since expected wait EW( , ) already includes cost waiting time time stamp
+ 1). hand, wait nodes calculated maximum among
children since last level lh stop node, ES( h , ) EW( , ) thus,
(2) EW( , ) ES( h , ). result, (by summing two inequalities (1) (2)),
EW( , ) EW( , ) CST (h) CST (t + 1). 2
Finally, prove approximation error global expected gain.
Theorem 8 Given time horizon = (0, ..., h), policy taken Procedure 3 optimal
policy , cost function CST (t), set candidate trees CT , global assignment holds:
GEG(CT, 0 , ) GEG(CT, 0 , ) < CST (h) CST (1).
Proof:
According Corollary 2 , pessimistic policy optimal except assignment , ( ) = stop optimal policy holds ( ) = wait. case,
GEG(CT, , ) = EW( , ) GEG(CT, , ) = ES( , ). ( ) = stop time
stamp t, ES( , ) > EW( , ) according Theorem 7, EW( , ) EW( , )
CST (h) CST (t + 1). Thus, EW( , ) ES( , ) CST (h) CST (t + 1).
result, GEG(CT, , ) GEG(CT, , ) < CST (h) CST (t + 1). particular, t=0,
GEG(CT, 0 , ) GEG(CT s, 0 , ) < CST (h) CST (1).2

6. Evaluation
presenting empirical evaluation, summarize theoretical analysis algorithms
Table 2.
Policy

OPTIMAL
OPTIMISTIC
PESSIMISTIC

#trees Complexity
m-#candidates,
M-size candidate
tree
1
O(M )

O(M 2 m2 )
1
O(m2 + mM log )

Approximation
error GEG
(h-max time horizon)
0
CST (h 1)
CST (h) CST (1)

Expected wait

Approximation error EW

optimal
overestimate
underestimate

0
f

i=1 EU (cti , ni ) + CST (h)
CST (h) CST (t + 1)

Table 2: Summary theoretical evaluation algorithms.
three algorithms, OPTIMAL, OPTIMISTIC, PESSIMISTIC, based decision
tree approach. However, optimal pessimistic algorithms use single decision
tree merges candidates, optimistic algorithm implements decision trees, one
candidate tree. time complexity approximation error global expected gain
presented columns three four, respectively. fifth column presents evaluation
expected wait. Obviously, optimal algorithm computes expected wait optimally.
optimistic algorithm overestimates expected wait thus waiting decision optimal
since real expected wait may less stop. pessimistic algorithm underestimates
expected wait although waiting decision optimal, stopping decision since
real expected wait may higher consequently optimal decision would wait. last
column presents approximation error expected wait.
255

fiK ALECH & R ECHES

shown table 2, error expected wait estimated optimistic algorithm
much higher pessimistic algorithm. result estimate pessimistic
performances closer optimal algorithm situations. However, since
expected wait optimistic algorithm overestimated, may frequently choose wait
obtain information thus case cost function increases moderately time,
performance optimistic algorithm increase.
6.1 Experimental Settings
experimentally validated algorithm within systematic artificial framework inspired
stock market. varied number candidate stocks (230) time horizon
economic events (15) (i.e., timed variables). ran combination 25 times. test,
possible profits stocks (the utility) randomly selected uniform distribution
range [$10K . . . $100K]. Later present experiments additional distributions.
ran scenario (of 25 tests) 25 random assignments timed variables. data point
graphs average 625 tests (25 random utilities 25 random assignments).
compared three algorithms (OPTIMAL, OPTIMISTIC PESSIMISTIC) four baseline algorithms:
1. trivial stopping strategy; determining winning candidate beginning based
expected utility (STOP).
2. trivial waiting strategy; determining winning candidate end based full information (WAIT).
3. algorithm stops middle (horizon/2) chooses best candidate based
expected gain (MIDDLE).
4. algorithm stops random time (RANDOM).
compared algorithms using two metrics: (1) runtime, (2) outcome utility.
runtime OPTIMAL runtime building decision tree; runtime approximations average runtime building decision trees level. normalize utility,
divided utility gained omniscient decision maker cost. following
experiments presented next sections apart presented Section 6.4 deal disjoint timed variables, namely, two candidate trees share timed variable. Notice that,
using disjoint time variables, worst case scenario OPTIMAL since finding optimal
time stop requires taking consideration combinations timed variables
candidate trees. timed variables disjoint number comparisons exponential
number candidates (see Table 2).
6.2 Effect Cost
cost function key factor selecting affective algorithm. examine factor,
set simple cost function grows linearly time CST (t) = varied coefficient
time stamp (a) 0.01K 2.91K, jumps 0.15K. fixed number
candidates horizon 5. STOP strategy, presented Figure 9, affected cost
since stops time = 0 case. hand, utility WAIT, MIDDLE
256

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

1
Depth percents

1
Normalized utility

OPTIMAL

0.95
0.9
0.85
0.8

0.8
0.6
0.4

0.2
0

0.01
0.16
0.31
0.46
0.61
0.76
0.91
1.04
1.16
1.31
1.46
1.61
1.76
1.91
2.04
2.16
2.31
2.46
2.61
2.76
2.91

0.01
0.16
0.31
0.46
0.61
0.76
0.91
1.04
1.16
1.31
1.46
1.61
1.76
1.91
2.04
2.16
2.31
2.46
2.61
2.76
2.91

0.75

Cost per time stamp (in thousands)

Cost per time stamp (in thousands)

Figure 9: Normalized utility cost Figure 10:
time step, CST (t) = x
K t. Time horizon 5 levels
number candidates 5.

Depth decision cost
time step, CST (t) = x
K t. Time horizon 5 levels
number candidates 5.

RANDOM, linearly decreases cost increases. Figure 10 shows OPTIMAL
approximations make decision earlier cost increases since becomes less worthwhile
wait. However, depth decision decreases faster OPTIMAL PESSIMISTIC
OPTIMISTIC. depth decision influences utility algorithms. interesting
see gap utility OPTIMISTIC PESSIMISTIC grows cost increases,
similarly gap depth. explained fact OPTIMISTIC overestimates
expected wait thus makes decision later loss cost significant.
Nevertheless, cost = 2 approximations optimal algorithm stop time stamp 0
achieve utility.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP
OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

0.96

Normalized utility

Normalized utility

1

OPTIMAL

0.95

0.9
0.85
0.8

0.95
0.94
0.93

0.92

0.75

2

2

3

4
#candidates

5

6

Figure 11: Normalized
utility


number candidates,


CST (t) = 0.28K 3 time
horizon 5 levels.

3

4
#candidates

5

6

Figure 12: Normalized utility number candidates,
CST (t) =

0.28K 3 time horizon 5 levels: zoom utility range 0.92
0.96.

ran experiments additional cost functions. Figures 11 13 present
non
3
linear cost functions. cost Figure 11 increases moderately (CST (t) = 0.28K t)
cost Figure 13 increases fast (CST (t) = 0.28K t2 ). show that, cost function
257

fiK ALECH & R ECHES

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

0.97

Normalized utility

1

Normalized utility

OPTIMAL

0.95
0.9

0.85
0.8
0.75

0.96
0.95
0.94
0.93

0.92
0.91

2

3

4
#candidates

5

6

1/7

Figure 13: Normalized utility num- Figure 14:
ber candidates, CST (t) =
0.28K t2 time horizon 5 levels.

1/6

1/5
1/4
1/3
1/2
cost function: cost(t)=0.28k*t^x

1

2

Normalized utility cost function: cost(t) = 0.28K tx ,
time horizon 5 levels number
candidates 5.

increases moderately (a root function), pessimistic algorithm becomes less effective
OPTIMISTIC becomes better PESSIMISTIC. shown Figure 11 zoom-in view
Figure 12, functions OPTIMAL significantly better PESSIMISTIC
situations better OPTIMISTIC (tested 95% confidence value).
examine influence cost function algorithms, varied consistently
cost function. choose cost function cost(t) = 0.28K tx changing x range
{ 17 , 16 , 15 , 14 , 13 , 21 , 1, 2}. Obviously, decreasing x function increases moderately. Figure
14 presents results. clear shown OPTIMISTIC better PESSIMISTIC root
functions smaller square root. Then, cost function increases faster gap
OPTIMISTIC PESSIMISTIC increases favor PESSIMISTIC.

expected utility

0.28K*t

0.28K*t^(1/3)

Averaged expected utility

9.5
9
8.5
8

7.5
7
0

1

2

3

4

5

Time

Figure 15: Averaged expected utility time, time horizon 5 levels number candidates 5.

258

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

examine reason behavior approximations dependent cost function,
observed growth averaged expected utility function time (Figure 15).
x-axis time horizon y-axis averaged expected utility. Obviously, expected
utility increase time increases since events discovered uncertainty decreases.
seems averaged expected utility grows moderately, approximately logarithmically,
function time. addition averaged expected utility, present Figure 15 two
cost functions. first linear cost function CST (t) = 0.28K t, pessimistic

approximation better, second root cost function CST (t) = 0.28K 3 t,
optimistic approximation better. set cost functions start value
expected utility y-axis. Figure 15 compares growth behavior two
cost functions expected utility. comparison may explain fact cost
function grows linearly pessimistic algorithm, usually stops earlier, better
optimistic algorithm, since cost function grows faster utility function. However,
cost function grows moderately (a root function), meaning, cost function utility
function similar trend, optimistic algorithm, usually stops later, becomes better.
rest experiments examine factors influence performance
algorithms. showed, difference algorithms root cost function
small thus might hard examine impact factors. Therefore, rest
experiments use linear cost function fixing waiting cost events constant
value $2.8K time stamp (CST (t) = 2.8K t).
6.3 Effect Number Candidates
present subset results time horizon 5 levels. Figure 16 presents utility
test setting six candidates. Due memory limitations, optimal algorithm failed deal
larger candidate sets. utility gained PESSIMISTIC close OPTIMAL
difference significant. result much better results baseline
algorithms even better OPTIMISTIC.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Runtime (ms)

Normalized utility

1
0.95

0.9
0.85
0.8
0.75
2

3

4
#candidates

5

6

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

10000
1000
100
10
1
0.1
0.01
0.001
2

3

4
#candidates

5

6

Figure 16: Normalized utility 6 candidates Figure 17: Runtime 6 candidates
time horizon 5 levels.
time horizon 5 levels.
runtime presented logarithmic scale Figure 17. runtime algorithms
polynomial, except OPTIMAL, exponential. instance, average runtime
OPTIMAL six candidates 5836 milliseconds, algorithms less
two milliseconds.
259

fiK ALECH & R ECHES

compared algorithms, excluding optimal algorithm, larger sets 30
candidates. utility PESSIMISTIC always significantly better others, shown
Figure 18. may explained approximation error expected wait. comparing
approximation error two algorithms (see Table 2), clear approximation error
expected wait OPTIMISTIC much greater PESSIMISTIC thus OPTIMISTIC expected make decision later PESSIMISTIC. Note statistically
significant difference OPTIMISTIC MIDDLE. Later present experiments
larger cost values horizon OPTIMISTIC much better MIDDLE.
Although complexity PESSIMISTIC OPTIMISTIC polynomial, PESSIMISTIC better OPTIMISTIC terms runtime, shown Figure 19.
justified complexity analysis algorithms, shown Table 2. OPTIMISTIC
square m, PESSIMISTIC square .
illustrate significance results, consider instance stock market five
candidate stocks. Based experiments, average utility optimal algorithm $92.8K,
97.6% utility obtained omniscient decision maker. PESSIMISTICs utility is,
average, less optimal amount $300, OPTIMISTIC reduces utility
amount $2, 800. Obviously, baseline algorithms reduce utility drastically. wait
strategy, instance, produces profit $80.5K.
OPTIMISTIC

PESSIMISTIC

STOP

OPTIMISTIC

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

WAIT

MIDDLE

RANDOM

10

0.95

Runtime (ms)

Normalized utility

1

0.9

0.85
0.8

1
0.1
0.01
0.001

0.75
2

4

6

8

2

10 12 14 16 18 20 22 24 26 28 30
#candidates

4

6

8

10 12 14 16 18 20 22 24 26 28 30
#candidates

Figure 18: Normalized utility 30 candi- Figure 19: Runtime 30 candidates
dates time horizon 5 levels.
time horizon 5 levels.

6.4 Shared Timed Variables
previous experiments run settings different candidates affected
timed variable. next experiment show that, even different candidates affected
timed variables, approximations achieve similar utility previously. generated candidate trees horizon 5 time stamps. set 50% variables affect multiple
candidates. Figures 20 21 present normalized utility 6 30 candidates.
comparing results results without shared variables (Figures 16 18) see
baseline algorithms MIDDLE RANDOM significantly improve utility.
statistically significant difference one algorithms without shared
variables (tested 95% confidence value). reason improvement MIDDLE
RANDOM shared variables less uncertainty thus expected utilities
candidates accurate. hand computation expected stopping
260

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

1
Normalized utility

Normalized utility

1

OPTIMISTIC

0.95
0.9
0.85
0.8

0.75

0.95
0.9
0.85
0.8
0.75

2

3

4
#candidates

5

6

2

4

6

8

10 12 14 16 18 20 22 24 26 28 30
#candidates

Figure 20: 50% Shared timed variables: Nor- Figure 21: 50% Shared timed variables: Normalized utility 6 candidates
malized utility 30 candidates
time horizon 5 levels.
time horizon 5 levels.

approximation algorithms relies independence variables thus decrease
uncertainty improve results.
also checked difference algorithms found statistically
significant difference OPTIMAL PESSIMISTIC. algorithms better OPTIMISTIC statistically significant difference OPTIMISTIC MIDDLE.
results approximation algorithms significantly better baseline algorithms (tested 95% confidence value).
analyzed Section 3, optimal algorithm addresses shared timed variables efficiently
fact reduces computational complexity. Figure 22 show runtime 50%
shared timed variables. Compared Figure 17 see OPTIMAL runs two orders
magnitude faster experiments shared timed variables.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

1
Depth percents

10
Runtime (ms)

OPTIMAL

1
0.1
0.01

0.8
0.6
0.4
0.2

0

0.001
2

3

4
#candidates

5

2

6

3

4
#candidates

5

6

Figure 22: 50% Shared timed variables: runtime Figure 23: Normalized depth 6 candidates
6 candidates time horizon
time horizon 5 levels.
5 levels.

6.5 Depth Decision
Figure 23 illustrates attributes PESSIMISTIC OPTIMISTIC algorithms run
candidate trees time horizon 5 levels. y-axis represents depth (in percentage
261

fiK ALECH & R ECHES

relative maximal horizon) algorithms stop decide. Figure 23 presents
results candidate trees time horizon 5 levels. analyzed, PESSIMISTIC always stops
optimal algorithm, since expected wait underestimated. OPTIMISTIC always stops
optimal algorithm since overestimating expected wait continues wait, although
optimal algorithm decides stop.
6.6 Effect Horizon

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

1

Normalized utility

Normalized Utility

1

OPTIMISTIC

0.95
0.9
0.85
0.8
0.75

0.95
0.9
0.85

0.8
0.75

1

2

3

4

5
6
Horizon

7

8

9

10

1

3

5

7

9

11 13 15 17 19 21 23 25 27 29
Horizon

Figure 24: Normalized utility horizon (1- Figure 25: Normalized utility horizon (130) number candidates 5.
10) number candidates 5.

Next examine influence horizon candidate trees utility. grow
candidate trees large horizon, generated chain; tree every left branch leads
leaf every right branch leads another internal node. Thus, size candidate tree
grows linearly horizon. experimental setting experiments includes 5 candidates. Figures 24, 26, 28 present results horizon 110 comparing algorithms.
Since feasible run OPTIMAL larger horizon, ran rest algorithms
horizon 130 (see Figures 25, 27 29). shown Figures 24 25, utility dramatically affected horizon optimal, approximations stop algorithms. different
WAIT, MIDDLE RANDOM, lose constant cost every time stamp since
intelligently compute stop. stop strategy affected horizon since
always makes decision first time, thus see low horizon levels wait strategy
better, high levels stop strategy outperforms wait strategy well MIDDLE
RANDOM.
Although chain topology candidate trees grows linearly horizon, runtime
OPTIMAL increases exponentially horizon, since level splits possible assignments candidates timed variables (Figure 26, runtime presented logarithmic scale).
shown Figure 27, OPTIMISTIC PESSIMISTIC grow polynomially OPTIMISTIC
grows faster. Naturally, relative depth decision OPTIMAL approximations
decrease horizon, since cost function grows horizon thus less worthwhile
wait information (Figure 28 29). Nevertheless, decision time converge
0 since cost high, cost waiting time stamps may less
expected utility. Obviously, discussed above, higher cost less wait. Note
262

fiOPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

100

10000
1000
100
10
1
0.1
0.01
0.001

Runtime (ms)

Runtime (ms)

ECISION AKING DYNAMIC U NCERTAIN E VENTS

10
1

0.1
0.01
0.001

1

2

3

4

5
6
Horizon

7

8

9

10

1

3

5

7

9

11 13 15 17 19 21 23 25 27 29
Horizon

Figure 26: Runtime horizon (1-10) Figure 27: Runtime horizon (1-30)
number candidates 5.
number candidates 5.

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

1
Depth percents

Depth percents

1

OPTIMISTIC

0.8
0.6
0.4
0.2
0

0.8
0.6

0.4
0.2

0

1

2

3

4

5
6
Horizon

7

8

9

10

1

3

5

7

9

11 13 15 17 19 21 23 25 27 29
Horizon

Figure 28: Depth decision horizon (1- Figure 29: Depth decision horizon (130) number candidates 5.
10) number candidates 5.

decrease OPTIMISTIC compared PESSIMISTIC moderate (Figure 29) since
approximation error expected wait OPTIMISTIC greater PESSIMISTIC.
6.7 Effect Utility Distribution
experiments, simulated utilities taking uniform distribution.
simulate varied range domains present additional results utilities taken
Beta distribution symmetric asymmetric cases. Beta distribution = provides
symmetric distribution. = 2 Beta distribution similar normal distribution defined
interval [0, 1] thus reflects distribution many real-world domains. larger
value = lower variance. Running experiments Beta distribution allows us
examine: (1) influence variance - controlling value = , (2) influence
skewness - setting fixed value varying .
first experiment set = = 2 number candidates horizon 5.
comparing results (Figure 31) results experiments uniform distribution (Figure
16), see algorithms except WAIT, improve utility. explained
fact variance candidates utilities Beta distribution smaller
variance uniform distribution, thus choosing non-optimal candidate, utility
263

fiK ALECH & R ECHES

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1
0.95
0.9
0.85
0.8
0.75
2

3

4

5

#candidates

Figure 31: Utilities taken Beta distribution = 2 = 2
Figure 30: Beta distribution = 2
skewness=0. Normalized utility
= 2.
candidates time horizon 5
levels 5 candidates.

closer optimal candidate. Therefore, algorithm stops selects non-optimal
candidate, utility selection closer optimal (and higher) Beta distribution
uniform distribution. also explains Beta distribution utility STOP
higher OPTIMISTIC, uniform distribution lower. also supported
depth decision. decision made earlier Beta distribution uniform
distribution. utility WAIT strategy distributions
decision made end optimal thus cost reduced,
distributions.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1

0.95
0.9
0.85

0.8
0.75
2

3

4
value =

5

6

Figure 32: Utilities taken Beta distribution varied = . Normalized utility
candidates time horizon 5 levels 5 candidates.

examine insight run experiments varied range = 2
6. increasing variance decreased. Figure 32 presents average utility
264

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

experiment. seems utility algorithms significantly influenced
variance. possible explanation tradeoff two trends. one hand, smaller
variance utilities candidates, less difference utilities chosen
candidate best candidate. hand, difference expected utility
candidates, decreases variance, thus possibility selecting wrong candidate
increases. Consequently, probability error choosing best candidate increases
payoff error decreased thus utility significantly influenced variance.
exception WAIT algorithm decreases variance. reason makes
decision end thus always chooses best candidate. However, since Beta
distribution = symmetric distribution variance increases best utility
decreases (close middle), hand, pays full cost waiting last time
stamp.

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1
0.95
0.9
0.85
0.8
0.75
2

3

4

5

#candidates

Figure 34: Utilities taken Beta distribution = 2 = 6
Figure 33: Beta distribution = 2
skewness=0.69. Normalized utility
= 6.
candidates time horizon
5 levels 5 candidates.

repeated experiments Beta distribution changing parameters
distribution first candidate = 6 = 2. difference influences skewness
distribution (-0.69) gives high probability gaining higher values (Figure 33). Since
parameters candidates remain ( = 2 = 2), skewness 0
thus first candidate likely chosen. shown Figure 34, low skewness first
candidate significantly increases utility algorithms. reason independently
stopping time, cases, expected utility first candidate highest thus
selected algorithms. cost waiting reduces utility. insight
significantly shown high utility STOP algorithm.
experimented influence skewness algorithms. Figure 35
presents normalized utility changing skewness first candidate. lower
skewness, likely first candidate chosen. increase utility algorithms clear since likely first candidate chosen, fewer errors
choosing best candidate.
265

fiK ALECH & R ECHES

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1
0.95
0.9
0.85
0.8

0.75
0.00

-0.29

-0.47
Skewness

-0.60

-0.69

Figure 35: Utilities taken Beta distribution = 2 varied skewness.
Normalized utility candidates time horizon 5 levels 5 candidates.

6.8 Conclusions
summarize, conclusions experiments are:
1. cost function increases moderately (a root function), PESSIMISTIC algorithm becomes less effective OPTIMISTIC becomes better PESSIMISTIC.
2. utility PESSIMISTIC close OPTIMAL cases (for functions
grow polynomially) statistically significant difference them.
3. runtime OPTIMAL exponential actually feasible candidates
small candidate trees.
4. runtime approximations polynomial PESSIMISTIC runs much faster
OPTIMISTIC.
5. statistically significant difference experiments include shared
timed variable experiments include disjoint timed variables (except MIDDLE RANDOM).
6. OPTIMAL runs much faster experiments include candidates shared timed variable.
7. PESSIMISTIC strategy makes decision slightly earlier OPTIMAL.
8. OPTIMISTIC strategy makes decision much later OPTIMAL. gap increased
increasing horizon candidate trees cost waiting. large
horizon cost depth close time=0.
9. cases (low horizon, high number agents, cost functions grow polynomially
shared timed variables) MIDDLE better OPTIMISTIC algorithm.
266

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

10. greater expected utility one candidate higher others, greater
utility achieved different algorithms.
11. normalized utilities achieved algorithms almost affected variance
candidates utilities. However, significant improvement achieved Beta distribution ( = 2) relation uniform distribution.

7. Related Work
section discuss relation work research related optimal
stoping problem explorationexploitation problems.
7.1 Optimal Stopping Problem
problem related Optimal Stopping Problem (OSP). OSP goal choose
time take particular action order maximize expected reward (Ferguson, 1989; Gilboa
& Schmeidler, 1989; Peskir & Shiryaev, 2006). classical stopping problem defined two
objects: (i) sequence independent random variables, X1 , X2 , ... , known joint distribution, (ii) sequence real-valued reward functions, y0 , y1 (x1 ), y2 (x1 , x2 ), ...,.
n = 1, 2, ..., observing X1 = x1 , X2 = x2 , ..., Xn = xn , agent may stop receive
known reward, yn (x1 , ..., xn ), may continue observe Xn+1 . agent chooses
make observation, receive constant amount, y0 . Take example house-selling
problem agent wishes sell house. day receives offer Xi . agent
decide either accept offer wait next offer. Waiting associated cost
living. Offers assumed independent. goal get highest offer (Lippman &
McCall, 1976).
class problems seems similar problem, since address problem finding best time stop. However, deeper perspective, three significant
differences problems:
(1) Stopping reward: OSP, agent decide whether stop obtain known
reward, based prior random variables wait next time stamp observe
next random variable. agent chooses stop, make observation, receive
constant reward dependent random variables. contrast OSP,
model, utility (which considering waiting cost) candidate depend
decision stop wait, future events. Even gain solely dependent
stopping time, since different future events influence gain differently. waiting decision enables
decision maker acquire information reward candidate, although
reward affected waiting decision. example house-selling problem, agent
wishes sell house. day receives offer Xi . agent decide either accept
offer wait next offer. decides stop accept offer obtains reward
offer. reward affected future offers. model, hand,
reward stopping depends future random variables. difference reward obviously
affects way approach maximizes reward. example, assume omniscient agent
knows outcomes variables advance; certainly stop time stamp
highest value OSP. Contrastingly, problem, best time stop first time
267

fiK ALECH & R ECHES

stamp since time agent knows exact utility candidates rather expected
utility.
second aspect related independency stopping rewards. OSP, stopping
rewards time stamp assumed independent. example, house-selling
problem, offers independent. problem, although variables independent,
rewards stopping dependent. difference significant since, independency
assumption, stopping rule OSP depends probability agent stop
current time stamp multiplied probability stop now. model,
hand, rewards stopping depend future random variables calculated taking
expected rewards candidates therefore dependent. result, able
use OSP model solve problem vice versa.
(2) Multiple candidates: OSP one reward observing X1 , X2 , ..., Xn .
problem, hand, considers multiple candidates. result, candidate different
reward observing variables. two different challenges face: (1) finding
best time stop, (2) choosing candidate highest expected utility time.
Although two different challenges, cannot treated two steps: finding best
time stop advance, time reached choosing candidate highest
expected utility time. approach would made challenge much simpler
thus fact multiple candidates would insignificant. Nevertheless, problem
cannot solved two separate steps since decision whether stop wait depends
assignments occur next time. Since candidate may affected different
assignments, consider combination assignments makes problem
hard.
(3) Joint distribution: classical stopping problem random variables known
joint distribution. previous example, offers assumed known
distribution thus case infinity convergence. problem different since
considers random variables different distributions.
last years several researchers generalized classical stopping problem order
deal cases multiple distributions, i.e., multiple optimal stopping problems. Riedel (2009)
presents unified general theory optimal stopping multiple priors discrete time
extends theory continuous time (da Rocha & Riedel, 2010; Cheng & Riedel, 2010).
developed theory optimal stopping one joint distribution unknown distribution variables using extending suitable results martingale theory (Williams,
1991). Still two differences (Stopping reward, Multiple candidates) specified above, maintain. addition, Riedel presents specific constraints random variables considering
Martingale theory assuming set stopping rewards time-consistent (Cheridito
& Stadje, 2009). contrast, work distribution variables known advance
require specific constraints random variables.
three differences demonstrate spite similarity problem OSP,
two models comparable cannot reduced other.
7.2 ExplorationExploitation Problems
addition classical stopping problem, also consider subset family
explorationexploitation problems kind stopping problem. problems agent
268

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

decide stop acquiring information (exploration) specific issue make
decision (exploitation). Sequential hypothesis testing method based statistical
tests. method enables stopping rule defined soon significant results observed.
method based mainly uncertain information, using multiple observations samples.
instance, Wald Woldforwitz (1948) present problem chance variable, Xs distribution, either p0 (X) p1 (X). required decision choose two
options based acquired observations variable. research goal obviously, make
decision minimal number observations.
multi-armed bandit problem (Katehakis & Veinott, 1987) agent allocates trials
slot machines machine provides random reward distribution specific
machine. objective allocate trials slot machines order maximize expected
reward using machines. One version multi armed bandit relevant
research Max K-Armed Bandit. Max K-Armed Bandit problem (Cicirello &
Smith, 2005; Streeter & Smith, 2006) objective allocate trials K arms order
identify best arm. Cicirello Smith (2005) extend problem agent runs trials
repeatedly, trial tries improve reward achieved thus far.
Similarly, ranking selection problem agent supposed select alternative
among several options. demonstrate problem, Powell Ryzhov present following example (Powell & Ryzhov, 2012): physician choose type drug among several medicines
reduces cholesterol patient,. order decide best drug might require
making physical experiments might need run number medical laboratory simulations. Testing alternative might involve running time-consuming computer simulation,
require physical experiment. Obviously, experimental process costly thus challenge
allocate experiments efficiently accurately make selection. Usually
limited budget evaluating alternatives budget exhausted, agent
choose alternative appears best, according obtained knowledge (Swisher,
Jacobson, & Yucesan, 2003). Frazier Powell (2008) present version problem
prior information units new obtained information units alternative sampled
specific distribution unknown mean variance. model provides new heuristic
sampling stopping rule relies distribution samples.
additional problem statistical analysis change detection tries identify change
parameters stochastic system. changes probability distribution
stochastic process time series (Basseville & Nikiforov, 1993). general, problem concerns
detecting whether change occurred (several changes also might occur), identifying time change. model decide stop obtaining observations
find closest time stamp distribution changed. problem works
detect disorder time quickly possible happens minimize rate false alarms
time (Dayanik, Poor, & Sezer, 2007).
Another problem decision making concerning multiple observations informative
expensive. challenge decision making problems decide variables observe order maximize expected utility. Krause Guestrin studied problem
domain sensor placement consider sensor network utility sensor determined certainty measured quantity. task efficiently select informative subsets observations. Specifically, propose optimal nonmyopic value information
chain graphical models (Krause & Guestrin, 2009). Bilgic Getoor (2011) address similar
269

fiK ALECH & R ECHES

problem efficiently acquiring classification features domains costs associated
acquisition. objective minimize sum information acquisition cost.
propose data structure known value information lattice (VOILA). VOILA exploits dependencies missing features, making possible share information value computations
different feature subsets.
Similarly, another work (Tolpin & Shimoni, 2010; Radovilsky & Shimoni, 2010) deals
selection uncertainty develops algorithms based value information (VOI)
semi-myopic approximation scheme problems real-valued utilities. particular, Tolpin
Shimoni (2010) interpret VOI expected difference expected utility meta-level
action expected utility current base-level action. Radovilsky Shimoni (2010) deal
optimizing selection set observations. aim bring objective function
optimum taking consideration cost observation remaining uncertainty
executing observation.
Recently, Chen et al. (2014) proposed use computation Same-decision Probability
(SDP) order compute whether additional information gathered. particular,
compute stopping criterion computing SDP, SDP probability decision
made even observations. information gathered propose
pieces information gather next.
work also related, aspects, Horvitzs work (2001, 2013) decision making
bounded resources. execution task associated utility cost, depending
resources. resources bounded, question stopping point best
will, part, satisfy task. Horvitz presents use expected value computation
determine best time stop. Similarly, monitoring anytime algorithms (Boddy & Dean,
1994; Zilberstein, 1996; Zhang, 2001) search best possible answer constraint
limited time and/or resources. major question arises utilizing class algorithms
optimally decide stop. instance, Finkelstein Markovitch (2001) developed
algorithms design optimal query schedule detect given goal fulfilled.
aim minimize number queries (which time consuming) reach goal.
joint objective works maximize goal function considering cost
observations/acquisition/actions extent uncertainty. also attempt maximize
utility choosing best candidate consider cost uncertainty timed
variables. However, work differs works two significant aspects:
1. Time variables: previous studies change detection problem
K-Armed bandit problem consider set observations without considering order.
works consider time observations. work variables associated
time selection dynamically determined according time progress outcomes
previous variables. point important due fact candidates may influenced disjoint variables. cannot order variables according time select subset
variables since available variables time depend assignment time 1. Thus,
decision whether stop wait depends assignments occur next time period.
Since candidate may affected different assignments consider combination
assignments complicates problem makes dissimilar previous work.
2. Finite small horizon: model utility candidate affected finite
small set discrete random variables. result, decision maker actually achieve absolute
information optimal choice waiting last time stamp. last time stamp,
270

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

complete knowledge assignments random variables explore
exact value utility. Due cost information research problem model
determine optimal time make decision reaching end. related research,
hand, basic assumption partial information obtained thus
impossible compute exact utility candidate. obtained information contains
observations samples different alternatives help decision maker
statistically approximate utility distribution different alternatives. potential population
observations may infinite large thus impossible practice obtain
information necessary compute exact utility. research problem models
thus use statistical methods decide required information different alternatives.

8. Summary Future Work
paper presented problem decision making among multiple candidates
information arrives dynamically. focused question stop make decision
maximizes utility taking consideration cost waiting. presented three
algorithms; optimal algorithm exponential number candidates two alternative
polynomial approximation algorithms. proved one approximation algorithm optimistic.
Namely computation expected utility waiting equal to, higher than, expected
utility computed optimal algorithm, algorithm pessimistic thus stops
earlier. empirical evaluation algorithms showed cost function much influences
results. cost function increases moderately (a root function), PESSIMISTIC
algorithm becomes less effective OPTIMISTIC becomes better PESSIMISTIC.
polynomial cost functions significant difference outcome utility
optimal algorithm pessimistic algorithm. also illustrated exponential growth
optimal algorithm polynomial growth optimistic pessimistic algorithms.
future plan continue two directions: 1) work assumed discrete variables, however practice, variables may continuous. One option solving problem
discretize variables, however, lose optimality. plan find optimal way address question, 2) plan investigate problem presented
paper domains involving multi-agent decision making. domains multiple agents
share decision based different variables utilities. multi-agent version
approximation grows exponentially number agents thus plan reduce
complexity.

Appendix A. TDM problem NP-hard
Proof: present reduction 3-SAT problem (Cook, 1971). instance 3-SAT

given propositional logic formula (z1 , ..., zn ) = 1 ... k , clause
disjunction exactly three literals. aim answer yes assignment
Boolean variables z1 , ..., zn satisfies formula. construct instance DM
follows.

1. Boolean variable zi create timed variable Xi .
271

fiK ALECH & R ECHES

Figure 36: Structure candidate trees accordance next 3-SAT formula:
(z1 , z2 , z3 , z4 ) = (z1 z2 z3 ) (z1 z3 z4 ) (z1 z2 z4 ).

2. every clause j j {1, ..., k} create candidate tree ctj three timed
variables:Xj1 , Xj2 Xj3 , corresponding variables clause. example,
clause j = (z1 z4 z5 ) create candidate tree ctj variables: X1 , X4
X5 .
3. root candidate tree ctj includes additional timed variable Yj , time stamp
(Yj ) = 1 three possible assignments. probability assignment 13
one assignments leads one timed variables:Xj1 , Xj2 Xj3 .
4. Every timed variable Xi corresponds literal zi two possible assignments
Xi = 1 utility > 0 Xi = 0 utility 2a, probability 0.5
(Xi ) = 2. timed variable corresponding literal zi also two possible
assignments Xi = 1 utility 2a > 0 Xi = 0 utility a, one
probability 0.5 (Xi ) = 2.
Figure 36 presents example structure candidate trees accordance 3SAT formula. left outgoing edge random variable Xi represents assignment Xi = 1
right outgoing edge represents assignment Xi = 0.
5. set time horizon = [0, 2].
6. set cost function be: CST (t) = 0.1 t.
7. set constant C C = 1.8a.
prove exists policy global expected gain GEG(CT, 0 , )
C, (x1 , ..., xn ) satisfiable.
expected utility stopping time = 0 well = 1 exactly 3a
2 less
C. highest expected utility 2a obtained waiting time stamp 2.
expected gain less equal 1.8a (after considering cost waiting). Therefore,
rest proof consider waiting policy waits time stamp 2.
1. guarantee GEG(CT, 0 , ) C time stamp 2, must confirm assignment
combination trees branches, least one candidate tree utility
2a.
272

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

2. construction, may happens assignments least one
candidate tree utilities 2a.
3. construction, candidate trees assignment guarantees utility 2a entails
least one false clause, meaning literals clause obtain 0 result (z1 , ..., zn )
satisfiable.
result obtain Timed Decision Making (TDM) NP-hard.2

References
Basseville, M., & Nikiforov, I. V. (1993). Detection abrupt changes: theory application.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Beaver, B. M., & Mendenhall, W. (1983). Introduction probability statistics, sixth edition,
William Mendenhall, study guide. Statistics Series. Duxbury Press.
Bilgic, M., & Getoor, L. (2011). Value information lattice: Exploiting probabilistic independence
effective feature subset acquisition. Journal Artificial Intelligence Research (JAIR), 41,
6995.
Boddy, M., & Dean, T. L. (1994). Deliberation scheduling problem solving time-constrained
environments. Artificial Intelligence, 67(2), 245285.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming factored representations. Artificial Intelligence, 121, 49107.
Chen, S. J., Choi, A., & Darwiche, A. (2014). Algorithms applications same-decision
probability. Journal Artificial Intelligence Research (JAIR), 49, 601633.
Cheng, X., & Riedel, F. (2010). Optimal stopping ambiguity continuous time. Working
papers 429, Bielefeld University, Institute Mathematical Economics.
Cheridito, P., & Stadje, M. (2009). Time-inconsistency var time-consistent alternatives.
Finance Research Letters, 6(1), 4046.
Cicirello, V. A., & Smith, S. F. (2005). max k-armed bandit: new model exploration
applied search heuristic selection. Veloso, M. M., & Kambhampati, S. (Eds.), AAAI, pp.
13551361.
Cook, S. A. (1971). complexity theorem-proving procedures. STOC 71: Proceedings
third annual ACM symposium Theory computing, pp. 151158, New York, NY,
USA. ACM.
da Rocha, V. F. M., & Riedel, F. (2010). equilibrium prices continuous time. Journal
Economic Theory, 145(3), 10861112.
Dayanik, S., Poor, H. V., & Sezer, S. O. (2007). Multisource bayesian sequential change detection.
CoRR, abs/0708.0224.
Ferguson, T. S. (1989). solved secretary problem?. Statistical Science, 4(3), 282289.
Finkelstein, L., & Markovitch, S. (2001). Optimal schedules monitoring anytime algorithms.
Artificial Intelligence, 126, 63108.
273

fiK ALECH & R ECHES

Frazier, P., & Powell, W. (2008). knowledge-gradient stopping rule ranking selection.
Simulation Conference, 2008. WSC 2008. Winter, pp. 305312.
Gilboa, I., & Schmeidler, D. (1989). Maxmin expected utility non-unique prior. Journal
Mathematical Economics, 18(2), 141153.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
factored MDPs. Journal Artificial Intelligence Research (JAIR), 19, 399468.
Horvitz, E. (2001). Principles applications continual computation. Artificial Intelligence,
126, 1261.
Horvitz, E. (2013). Reasoning beliefs actions computational resource constraints.
CoRR, abs/1304.2759.
Kalech, M., & Pfeffer, A. (2010). Decision making dynamically arriving information.
van der Hoek, W., Kaminka, G. A., Lesperance, Y., Luck, M., & Sen, S. (Eds.), AAMAS, pp.
267274.
Katehakis, M., & Veinott, J. A. (1987). multi-armed bandit problem: decomposition computation. Mathematics Operations Research, 12(2), 262268.
Krause, A., & Guestrin, C. (2009). Optimal value information graphical models. Journal
Artificial Intelligence Research (JAIR), 35, 557591.
Lippman, S. A., & McCall, J. J. (1976). economics job search: survey. Economic Inquiry,
14(3), 155189.
Peskir, G., & Shiryaev, A. (2006). Optimal Stopping Free-Boundary Problems. Birkhauser
Basel.
Powell, W., & Ryzhov, I. (2012). Optimal Learning. Wiley Series Probability Statistics.
Wiley.
Radovilsky, Y., & Shimoni, S. E. (2010). Observation subset selection optimization uncertainty. Tech. rep., Lynne William Frankel Center Computer Science Ben Gurion
University Negev.
Reches, S., Kalech, M., & Stern, R. (2011). stop? question. Burgard, W., &
Roth, D. (Eds.), AAAI. AAAI Press.
Riedel, F. (2009). Optimal stopping multiple priors. Econometrica, 77(3), 857908.
Streeter, M. J., & Smith, S. F. (2006). asymptotically optimal algorithm max k-armed
bandit problem. AAAI, pp. 135142. AAAI Press.
Swisher, J. R., Jacobson, S. H., & Yucesan, E. (2003). Discrete-event simulation optimization
using ranking, selection, multiple comparison procedures: survey. ACM Trans. Model.
Comput. Simul., 13(2), 134154.
Tolpin, D., & Shimoni, S. E. (2010). Semi-myopic observation selection optimization
uncertainty. Tech. rep. 10-01, Lynne William Frankel Center Computer Science
Ben Gurion University Negev.
Wald, A., & Wolfowitz, J. (1948). Optimum Character Sequential Probability Ratio Test.
Annals Mathematical Statistics, 19(3), 326339.
274

fiD ECISION AKING DYNAMIC U NCERTAIN E VENTS

Williams, D. (1991). Probability Martingales. Cambridge mathematical textbooks. Cambridge
University Press.
Zhang, W. (2001). Iterative state-space reduction flexible computation. Artificial Intelligence,
126(1-2), 109138.
Zilberstein, S. (1996). Using anytime algorithms intelligent systems. AI Magazine, 17(3), 7383.

275

fiJournal Artificial Intelligence Research 54 (2015) 493-534

Submitted 05/15; published 12/15

Possible Necessary Winners Partial Tournaments
Haris Aziz

haris.aziz@nicta.com.au

Data61 University New South Wales
Australia

Markus Brill

brill@cs.duke.edu

Computer Science Department
Duke University, USA

Felix Fischer

fischerf@math.tu-berlin.de

Institut fur Mathematik
Technische Universitat Berlin, Germany

Paul Harrenstein

paul.harrenstein@cs.ox.ac.uk

Computer Science Department
University Oxford, UK

Jerome Lang

lang@lamsade.dauphine.fr

LAMSADE
Universite Paris-Dauphine, France

Hans Georg Seedig

seedigh@in.tum.de

Institut fur Informatik
Technische Universitat Munchen, Germany

Abstract
study problem computing possible necessary winners partially specified weighted unweighted tournaments. problem arises naturally elections
incompletely specified votes, partially completed sports competitions, generally
scenario outcome pairwise comparisons yet fully known.
specifically consider number well-known solution conceptsincluding uncovered set, Borda, ranked pairs, maximinand show them, possible
necessary winners identified polynomial time. positive algorithmic results
stand sharp contrast earlier results concerning possible necessary winners given
partially specified preference profiles.

1. Introduction
Many multi-agent situations modeled analyzed using weighted unweighted
tournaments. Prime examples voting scenarios pairwise comparisons
alternatives decided majority rule sports competitions organized
round-robin tournaments. application areas include webpage journal ranking,
biology, psychology, AI. generally, tournaments solution concepts tournaments used mathematical tool analysis kinds situations
choice among set alternatives made exclusively basis pairwise
comparisons.
choosing tournament, relevant information may partly available.
could preferences yet elicited, matches yet played,
c
2015
AI Access Foundation. rights reserved.

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

certain comparisons yet made. cases, natural speculate
potential inevitable outcomes basis information already hand.
tournaments, number attractive solution concepts proposed (Brandt,
Brill, & Harrenstein, 2016; Laslier, 1997). Given solution concept S, define
possible winners partial tournament G alternatives selected
completion G, necessary winners alternatives selected completions.
completion understand (complete) tournament extending G.
article address computational complexity identifying possible
necessary winners number solution concepts whose winner determination problem tournaments tractable. consider five common solution concepts
tournamentsnamely, Condorcet winners (COND), Condorcet non-losers (CNL),
Copeland set (CO), top cycle (TC ), uncovered set (UC )and three common
solutions weighted tournamentsBorda (BO), maximin (MM ), ranked pairs (RP ).
solution concepts, consider computational complexity following problems: deciding whether given alternative possible winner (PW), deciding
whether given alternative necessary winner (NW), well deciding whether
given subset alternatives equals set winners (the winning set) completion (PWS). problems challenging, even unweighted partial tournaments
may allow exponential number completions. results encouraging,
sense problems solved polynomial time. Table 1 summarizes
findings.
Similar problems considered before. Condorcet winners, voting trees
top cycle, shown possible necessary winners computable
polynomial time (Konczak & Lang, 2005; Lang et al., 2012). holds
computation possible Copeland winners, problem considered
context sports tournaments (Cook, Cunningham, Pulleyblank, & Schrijver, 1998).
Another specific setting also frequently considered within area computational social choice differs setting subtle important way
worth pointed out. There, tournaments assumed arise pairwise majority
comparisons basis profile individual voters preferences.1
Since partial preference profile R need conclusively settle every majority comparison, may give rise partial tournament only. two natural ways define
possible necessary winners partial preference profile R solution concept
illustrated Figure 1. first consider completions R winners
corresponding tournaments. secondcovered general settingis
consider completions partial tournament G(R) corresponding R
winners these. Since every tournament corresponding completion R
also completion G(R) necessarily way round, second definition
1. See, e.g., work Baumeister Rothe (2010), Betzler Dorn (2010), Konczak Lang (2005),
Walsh (2007), Xia Conitzer (2011) basic setting, Betzler, Hemmann, Niedermeier
(2009) parameterized complexity results, Bachrach, Betzler, Faliszewski (2010), Hazon, Aumann,
Kraus, Wooldridge (2012), Kalech, Kraus, Kaminka, Goldman (2011) probabilistic
settings, Chevaleyre, Lang, Maudet, Monnot (2011) Chevaleyre, Lang, Maudet, Monnot,
Xia (2012) settings variable set alternatives, Baumeister, Faliszewski, Lang, Rothe
(2012), Kalech et al. (2011), Lu Boutilier (2011), Oren, Filmus, Boutilier (2013), Filmus
Oren (2014) settings truncated ballots Lu Boutilier (2013) multiwinner rules.

494

fiPossible Necessary Winners Partial Tournaments



PWS

COND
CNL
CO
TC
UC







BO
MM
RP

P (Thm. 8)a
P (Thm. 11)a
NP-C (Thm. 14)



P
P
P
P
P

NWS
(Konczak & Lang, 2005)
(Thm. 2)
(Cook et al., 1998)a
(Lang et al., 2012)a
(Thm. 5)







P
P
P
P
P

PWSS
(Konczak & Lang, 2005) P
(Thm. 2)
P
(Thm. 3)a
P
(Lang et al., 2012)
P
(Thm. 6)
NP-C

P
(Thm. 10)
P
(Thm. 13)
coNP-C (Thm. 15)

(Thm.
(Thm.
(Thm.
(Thm.
(Thm.

1)
2)
3)
4)
7)

P (Thm. 9)
P (Thm. 12)
NP-C (Cor. 2)

P-time result contrasts intractability problem partial preference
profiles (Lang et al., 2012; Xia & Conitzer, 2011).

Table 1: Complexity computing possible winners (PW) necessary winners (NW)
checking whether given subset alternatives possible winning set (PWS)
following solution concepts: Condorcet winners (COND), Condorcet non-losers (CNL),
Copeland (CO), top cycle (TC ), uncovered set (UC ), Borda (BO), maximin (MM ),
ranked pairs (RP ).
gives rise stronger notion possible winner weaker notion necessary winner. Interestingly, sharp contrast results, determining stronger possible
weaker necessary winners computationally hard many voting rules (Lang et al.,
2012; Xia & Conitzer, 2011). contrast foreshadowed work Lang et
al. (2012) Pini, Rossi, Venable, Walsh (2011), compared two ways
defining possible necessary winners (both theoretically experimentally) three
solution concepts: Condorcet winners, voting trees, top cycle.
context article, assume tournaments arise majority
comparisons voting specific procedure. approach number
advantages. Firstly, matches diversity settings solution concepts
tournaments applicable, goes well beyond social choice voting. instance,
results also apply question commonly encountered sports competitions, namely,
teams still win cup future results depends (Cook et al.,
1998; Kern & Paulusma, 2004; B. L. Schwartz, 1966). Secondly, (partial) tournaments
provide informationally sustainable way representing relevant aspects many
situations maintaining workable level abstraction conciseness. instance,
social choice setting described above, partial tournament induced partial
preference profile much succinct piece information, discloses less information, preference profile itself. generally, gives canonical way extending
tournament solutions incomplete tournaments (a line pursued Brandt,
Brill, & Harrenstein, 2014). Finally, specific settings may impose restrictions feasible
extensions partial tournaments. positive algorithmic results article
used efficiently approximate sets possible necessary winners settings,
corresponding problems may intractable. voting setting discussed
serves illustrate point.
also point computing possible outcomes considered domains social choice example matching allocations (Aziz, Walsh, & Xia, 2015;
495

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

1 1 1


c

b b
b
c

completions

completions
1 1 1



c
b b b
c c
1 1 1
1 1 1
c
b b c
c b

c

b

b


c c
b b
c b

b



c
b


c

b

c

c

Figure 1: non-commutative diagram illustrates two approaches possible
necessary winners partial preference profiles majoritarian social choice functions.
First, completions partial profile full preference profiles shown
bottom left. corresponding majority tournaments dashed box bottom
right. work, start partial majority tournament top right
induced partial preference profile. Then, consider possible completions
tournaments depicted solid box bottom right.

Rastegari, Condon, Immorlica, & Leyton-Brown, 2013) knockout tournaments (Aziz
et al., 2014; Vu, Altman, & Shoham, 2009).

2. Preliminaries
partial tournament pair G = (V, E) V nonempty finite set alternatives
E V V asymmetric relation V , i.e., (y, x)
/ E whenever (x, y) E.
(x, y) E say x dominates y. tournament partial tournament (V, E)
E also complete, i.e., either (x, y) E (y, x) E distinct x, V .
denote set tournaments .
Let G = (V, E) partial tournament. Another partial tournament G0 = (V 0 , E 0 )
called extension G, denoted G G0 , V = V 0 E E 0 . E 0 complete, G0
called completion G. write [G] set completions G, i.e.,
[G] = {T : G }.
496

fiPossible Necessary Winners Partial Tournaments

say alternative x V dominated (y, x) E V , undominated
+
otherwise. define dominion x G DG
(x) = {y V : (x, y) E},

+
dominators x G DGS
(x) = {y V : (y, x) E}. X V , let DG
(X) =

+



(x)


(X)
=

(x).

nonempty
subset
X

V

alternatives

xX
xX
G
G
G
partial complete tournament (V, E) dominant every alternative X dominates
every alternative outside X. given G = (V, E) X V , write E X
set edges obtained E adding missing edges alternatives X
alternatives X, i.e.,
E X = E {(x, y) X V :
/ X (y, x)
/ E}.
use E X abbreviation E V \X , write E x , E x , GX , GX
E {x} , E {x} , (V, E X ), (V, E X ), respectively. G = (V, E) X V ,
use E|X G|X denote restriction E(XX) E X restriction (X, E|X )
G X, respectively.
Let n positive integer. partial n-weighted tournament pair G = (V, w)
consisting finite set V alternatives weight function w : V V {0, . . . , n}
pair (x, y) V V x 6= y, w(x, y) + w(y, x) n. say
= (V, w) n-weighted tournament x, V x 6= y, w(x, y) + w(y, x) = n.
call (partial) weighted tournament (partial) n-weighted tournament
n N. class n-weighted tournaments denoted Tn . Observe
partial 1-weighted tournament (V, w) associate partial tournament (V, E)
setting E = {(x, y) V : w(x, y) = 1}. Thus, (partial) n-weighted tournaments seen
generalize (partial) tournaments, may identify T1 .
notations G G0 [G] extended naturally partial n-weighted tournaments G = (V, w) G0 = (V 0 , w0 ) letting (V, w) (V 0 , w0 ) V = V 0
w(x, y) w0 (x, y) x, V , [G] = {T Tn : G }.
given G = (V, w) X V , define wX x, V ,

w

X

(
n w(y, x) x X
/ X,
(x, y) =
w(x, y)
otherwise,

set wX = wV \X . Moreover, wx , wx , GX , GX defined obvious
way.
use term solution concept functions associate tournament = (V, E), weighted tournament = (V, w), choice set S(T ) V .2
solution concept called resolute |S(T )| = 1 tournament . article
consider following solution concepts: Condorcet winners (COND), Condorcet nonlosers (CNL), Copeland (CO), top cycle (TC ), uncovered set (UC ) tournaments,
maximin (MM ), Borda (BO), ranked pairs (RP ) weighted tournaments.
ranked pairs resolute. Formal definitions provided later article.
2. avoid otherwise natural term tournament solution common definition requires
choice set nonempty (Laslier, 1997). would exclude COND.

497

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

3. Possible Necessary Winners
solution concept selects unique set alternatives complete tournament.
holds particular completions partial tournament. However,
completion partial tournament, solution concept may select another set alternatives. similar remark concerns weighted tournaments completions. given
solution concept S, thus define set possible winners partial (weighted)
tournament G set alternatives selected completion G, i.e.,
[
S(T ).
PWS (G) =
[G]

Analogously, set necessary winners G set alternatives selected
every completion G, i.e.,
\
NWS (G) =
S(T ).
[G]

furthermore write
PWSS (G) = {S(T ) : [G]}
possible winning sets, i.e., set sets alternatives selects
different completions G. sake completeness, also mention necessary winning
sets. set X necessary winning set partial tournament G X = S(T )
[T ]. Accordingly, conditions set necessary winning set
strong satisfied relatively seldom. Necessary winning sets also straightforwardly
characterized means sets possible necessary winners: X necessary
winning set X = PW (G) = NW (G). implies solution
concepts addressed article, computational results surrounding necessary winning
sets follow easy corollaries.3 consider necessary winning sets.
Note NWS (G) may empty even selects nonempty set alternatives
tournament [G], number |PWSS (G)| possible winning sets may
exponential number alternatives G.
following lemmas, relate useful structural properties
sets possible necessary winners. proofs straightforward therefore
omitted.
Lemma 1. Let solution concept G G0 partial tournaments. Then,
(i) G G0 implies PWS (G0 ) PWS (G),
(ii) G G0 implies NWS (G) NWS (G0 ).
say solution concept refines another solution concept 0 , denoted 0 ,
S(G) 0 (G) G. find following monotonicity properties hold.
3. Given results Table 1, fact X necessary winning set X = PW (G) =
NW (G) immediately implies concepts apart ranked pairs deciding whether set
necessary winning set achieved polynomial time. Since ranked pairs resolute, every set
NWSRP singleton {x} {x} PWSRP x NWRP . Consequently,
problem deciding whether set X contained NWSRP coNP-complete.

498

fiPossible Necessary Winners Partial Tournaments

Lemma 2. Let 0 solution concepts G G0 partial tournaments. Then,
(i) 0 implies PWS (G) PWS 0 (G),
(ii) 0 implies NWS (G) NWS 0 (G).
next lemma concerns way sets possible necessary winners
defined terms one another.
Lemma 3. Let solution concept G partial tournament. Then,

(i) PWS (G) = GG0 NWS (G0 ),

(ii) NWS (G) = GG0 PWS (G0 ).
Observe that, 0 generally imply PWSS (G) PWSS 0 (G), following
hold:
0 X PWSS (G) exists X 0 PWSS 0 (G) X X 0 .
Deciding membership sets PWS (G), NWS (G), PWSS (G) given solution
concept partial (weighted) tournament G natural computational problems.
Overloading notation, refer problems PWS , NWS , PWSS , respectively.
PWS (Possible Winners)
Input:
partial tournament G = (V, E) n-weighted partial tournament
G = (V, w) along positive integer n; alternative x V .
Output: Yes, exists completion [G] x S(T ).
No, otherwise.

NWS (Necessary Winners)
Input:
partial tournament G = (V, E) n-weighted partial tournament
G = (V, w) along positive integer n; alternative x V .
Output: Yes, x S(T ) completions [G].
No, otherwise.

PWSS (Possible Winning Set)
Input:
partial tournament G = (V, E) n-weighted partial tournament
G = (V, w) along positive integer n; subset alternatives
X V.
Output: Yes, exists completion [G] X = S(T ).
No, otherwise.
Note PWSS decided polynomial time, means
polynomial-time algorithm decides whether given subset alternatives possible
499

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

winning set. Outputting set PWSS possible winning sets may much difficult,
PWSS may exponential size.4
irresolute solution concepts, PWSS may appear complex problem
PWS . are, however, aware generic polynomial-time reduction PWS
PWSS . relationship problems may also interest
classic possible winner setting partial preference profiles.
complete tournaments [T ] = {T } thus PWS (T ) = NWS (T ) = S(T )
PWSS (T ) = {S(T )}. consequence, solution concepts NP-hard
winner determination problemlike Banks, Slater, tournament equilibrium set
problems PWS , NWS , PWSS NP-hard well.5 therefore restrict
attention solution concepts winners computed polynomial time.

4. Unweighted Tournament Solutions
section, consider following well-known solution concepts unweighted tournaments: Condorcet winners, Condorcet non-losers, Copeland set, top cycle,
uncovered set. use partial tournament depicted Figure 2(i ) running
example.


b



b



b

c



c



c



(i ) Partial tournament G

(ii ) Completion T1

(iii ) Completion T2

Figure 2: Example partial unweighted tournament G possible completions T1
T2 . Initially, (dotted) edges pairs {a, b}, {b, c}, {c, d} yet
specified.

4.1 Condorcet Winners Condorcet Non-losers
Condorcet winners Condorcet non-losers fundamental solution concepts
provide nice warm-up. alternative x V Condorcet winner complete tournament = (V, E) dominates alternatives, i.e., (x, y) E V \ {x}.
set Condorcet winners tournament denoted COND(T ); obviously
set always either singleton empty. alternative x Condorcet loser
dominated every alternative, i.e., (y, x) E V \ {x}. Consequently,
x Condorcet non-loser = (V, E) x Condorcet loser V = {x}.
set Condorcet non-losers tournament denoted CNL(T ); obviously
set always cardinality |V | |V | 1.
4. instance, G = (V, ) PWS TC (G ) = {X V : |X| 6= 2}, even though PWSTC P
(Theorem 4).
5. exclude possibility computing (arbitrary) possible winner possible winning
set solution concepts could done polynomial time.

500

fiPossible Necessary Winners Partial Tournaments

Let G = (V, E) partial tournament. alternative x dominant G, x
obviously Condorcet winner completions G. hand,
V \ {x} case (x, y) E, completion G x
Condorcet winner. Hence,
x NWCOND (G) (x, y) E V \ {x}

x PWCOND (G) (y, x) E V \ {x}.
Obviously, criteria right-hand side equivalences checked polynomial time.
turn problem PWSCOND . sets PWSCOND (G) either
singleton empty set, determining membership singleton obviously
tractable. Checking whether PWSCOND (G) quite simple. following
result gives exact characterization PWSCOND (G), interesting
right.
Lemma 4. Let U set undominated alternatives partial tournament G = (V, E).
Then,
every alternative x V , {x} PWSCOND (G) x U ;
6 PWSCOND (G) 1 |U | 2 U dominant.
Proof. Since complete tournament either one Condorcet winner none, set
PWSCOND (G) cardinality 0 1. Clearly, {x} PWSCOND (G) x U .
remains shown PWSCOND (G) contains U = , |U | 3,
1 |U | 2 U dominant.
U = , COND(T ) = every [G]. follows PWSCOND (G).
|U | 3, consider directed cycle C U U visits every alternative U .6 Then,
set undominated alternatives G0 = (V, E C) empty. follows
PWSCOND (G).
U = {x} x dominant, x Condorcet winner every [G]. Therefore,

/ PWSCOND (G).
U = {x} {x} dominant, (x, y)
/ E 6= x. Consider
completion G containing (y, x). completion, set undominated alternatives
empty. follows PWSCOND (G).
U = {x, y} {x, y} dominant, every [G], either (x, y) x
Condorcet winner , (y, x) Condorcet winner . follows

/ PWSCOND (G).
Finally, U = {x, y} {x, y} dominant, z 6= x,
(x, z)
/ E (y, z)
/ E. Without loss generality, assume (x, z)
/ E. Consider
completion G containing (z, x) (x, y). completion exists, (x, z)
/ E,
(y, x)
/ E (since x U ). completion, set undominated alternatives
empty. follows PWSCOND (G).
6. cycle C subgraph G. fact, G|U contain edges.

501

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

position prove following theorem.
Theorem 1. PWCOND , NWCOND , PWSCOND solved polynomial time.
results PWCOND NWCOND also follow Corollary 2 Konczak
Lang (2005).
note Theorem 1 corollary corresponding results maximin
Section 5.2. reason Condorcet winner maximin winner 1weighted tournament, tournament admit Condorcet winner
alternatives maximin winners.
conclude section observing problems PWCNL , NWCNL ,
PWSCNL reducible NWCOND , PWCOND , PWSCOND , respectively.
straightforwardly checked partial tournaments G = (V, E) |V | > 1
X V ,
X PWSCNL (G) V \ X PWSCOND (G1 ),
G1 = (V, E 1 ) G set edges inverted, i.e., E 1 = {(x, y) : (y, x) E}.
also follows that,
PWCNL (G) = V \ NWCOND (G1 ),
NWCNL (G) = V \ PWCOND (G1 ).
Since complement set computed polynomial time edges
reversed polynomial time well, obtain following result corollary
Theorem 1.
Theorem 2. PWCNL , NWCNL , PWSCNL solved polynomial time.
example, consider partial tournament G depicted Figure 2(i )
dominating alternative set undominated alternatives G U = {a, b}.
Therefore,
PWCOND (G) = {a, b}
NWCOND (G) = .
PWSCOND (G), note set U dominant (b, c)
/ E. Lemma 4,
gives
PWSCOND (G) = {{a}, {b}, }.
Condorcet non-losers, observe G1 = (V, E 1 ) E 1 = {(c, a), (d, a), (d, b)}.
Now, PWCOND (G1 ) = {c, d}, NWCOND (G1 ) = , (from Lemma 4)
PWSCOND (G1 ) = {{c}, {d}, }. Therefore,
PWCNL (G) = {a, b, c, d},
NWCNL (G) = {a, b},
PWSCNL (G) = {{a, b, d}, {a, b, c}, {a, b, c, d}}.

502

fiPossible Necessary Winners Partial Tournaments

4.2 Copeland
Copelands solution selects alternatives based number alternatives
dominate. Define Copeland score alternative x tournament = (V, E)
sCO (x, ) = |DT+ (x)|.
set CO(T ) consists alternatives maximal Copeland score.
illustrative example, consider partial tournament G shown Figure 2(i ). completions G (respectively b) Condorcet winner, (respectively b) sole Copeland winner completion shown Figure 2(ii ).
two completions neither b Condorcet winner
{(a, c), (a, d), (b, a), (b, d), (c, b), (c, d)},
set Copeland winners {a, b, c},
{(a, c), (a, d), (b, a), (b, d), (c, b), (d, c)},
also depicted Figure 2(iii ), set Copeland winners {a, b}. Therefore,
PWCO (G) = {a, b, c},
NWCO (G) = ,
PWSCO (G) = {{a}, {b}, {a, b}, {a, b, c}}.
Since Copeland scores coincide Borda scores case 1-weighted tournaments,
following direct corollary results Section 5.1.7
Theorem 3. PWCO , NWCO , PWSCO solved polynomial time.
PWSCO solvable polynomial time, get following corollary,
may independent interest graph theorists.
Corollary 1. exists polynomial-time algorithm check whether partial tournament admits regular completion, i.e., completion every alternative
out-degree.
see this, merely observe completion = (V, E) partial tournament
regular CO(T ) = V .
4.3 Top Cycle
top cycle tournament = (V, E), denoted TC (T ), unique minimal
dominant subset V .
Lang et al. shown possible necessary winners TC computed
efficiently greedy algorithms (Lang et al., 2012, Corollaries 1 2). Still, give
following characterization prove useful come consider possible
7. PWCO alternatively solved via polynomial-time reduction maximum network flow (Cook et
al., 1998, p. 51).

503

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

winning sets TC . alternative possible TC -winner
reach every alternative via existing unspecified edges. Formally, given partial
tournament G = (V, E), alternative x V PWTC (G) every
alternative V , exists path x0 , x1 , x2 , . . . , xk x = x0 , = xk ,
(xi+1 , xi )
/ E {0, . . . , k 1}. call path possible path. possible
path x exists, denote x
y.
Observe pair b alternatives partial tournament G = (V, E),
possible path b, (b, a) E. set alternatives
reach every alternative via possible path partial tournament G = (V, E) also
known Good set (Good, 1971) denoted GO(G).8 follows
GO(G) polynomial-time computable. Moreover, following lemma.
Lemma 5. Let G = (V, E) partial tournament |V | 3, GO(G) = V , x
alternatives (x, y), (y, x)
/ E. Let Gxy = (V, E {(x, y)})
Gyx = (V, E {(y, x)}). Then, GO(Gxy ) = V GO(Gyx ) = V .
Proof. Assume contradiction GO(Gxy ) GO(Gyx ) strict subsets
V . Clearly, x GO(Gxy ) GO(Gyx ). Moreover, claim x
/
GO(Gyx )
/ GO(Gxy ). see x
/ GO(Gyx ) holds, assume contradiction
x GO(Gyx ). Then, possible path x Gyx . path
used replace edge (x, y) (which available possible paths G,
Gyx ). Therefore, possible path two alternatives Gyx whenever
one G. Since GO(G) = V , GO(Gyx ) = V well, contradicting
assumption. analogous argument shows
/ GO(Gxy ).
established x GO(Gxy ) \ GO(Gyx ) GO(Gyx ) \ GO(Gxy ),
know possible path x Gyx , neither possible
path x Gxy . consider z V \ {x, y}. either
(i) (x, z) E (y, z) E,
(ii) (z, x) E (z, y) E,
otherwise would either possible path x Gyx possible path
x Gxy .
(i), recall assumed GO(G) = V . Hence, G possible
paths z
x z
y. Observe may assume either possible path z
x contain (y, x), possible path z contain (x, y).
former case, y, z
x possible path Gxy . latter case, x, z
possible
yx
path G
. Thus, either case yields contradiction.
(ii), GO(G) = V implies possible paths x
z
z G,
may assume either possible path x z contain (y, x)
possible path z contain (x, y). former case, possible
path x
z, Gxy . latter case, possible path
z, x Gyx . Again,
either case leads contradiction. concludes proof.
8. Equivalently, Good set partial tournament G = (V, E) unique minimal dominant subset
V . Good set also known Smith set (Smith, 1973) GETCHA (T. Schwartz, 1986).

504

fiPossible Necessary Winners Partial Tournaments

ready show PWSTC solved efficiently. Note
check exists completion set question dominating,
also smaller dominating set.
Theorem 4. PWSTC solved polynomial time.
Proof. Let set consideration X. set X cannot empty C(T ) 6=
every [G]. |X| = 1, problem PWSTC equivalent PWCOND .
|X| = 2, answer already top cycle never size two. may
therefore assume |X| 3.
Consider graph GX . X dominate V \ X GX , X
/ PWSTC (G)
alternative V \ X beats alternative X. Therefore, need check
whether X PWSTC (G|X ), i.e., whether X possible top cycle set partial
tournament G restricted X. essence, problem PWSTC reduced restricted
problem PWSTC set alternatives.
prove V PWSTC (G) GO(G) = V . Obviously, V 6= GO(G)
V
/ PWSTC (G). direction, start partial tournament G =
(V, E) GO(G) = V . iteratively applying Lemma 5, new edges successively
added G maintaining GO(G) = V G tournament.
example, consider partial tournament G depicted Figure 2(i ),
show
PWTC (G) = {a, b, c, d},
NWTC (G) = ,
PWSTC (G) = {{a}, {b}, {a, b, c}, {a, b, c, d}}.
result PWTC (G) witnessed completion shown Figure 2(iii ) every
alternative top cycle. NWTC (G), statement follows observation every alternative, exists completion another alternative
Condorcet winner. Regarding PWSTC (G), consider subset separately. Since
PWSCOND PWSTC , get {a} {b} PWSTC (G). {a, b, c}, apply
result shown second paragraph proof Theorem 4: a, b, c undominated
d, Good set G|{a,b,c} {a, b, c}. Likewise, Good set G {a, b, c, d}.
remains shown subsets size three PWSTC (G).
end, note Good set G|{a,b,d} {a, b} neither {a, c, d} {b, c, d}
undominated G.
4.4 Uncovered Set
Given tournament = (V, E), alternative x V said cover another alternative
V DT+ (y) DT+ (x), i.e., every alternative dominated also dominated x.
uncovered set , denoted UC (T ), set alternatives covered
alternative. useful alternative characterization uncovered set via
two-step principle: alternative uncovered set reach
every alternative two steps.9 Formally, x UC (T )
9. graph theory, vertices satisfying property often called kings.

505

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

V \ {x}, either (x, y) E z V (x, z), (z, y) E. denote
+
+
++
two-step dominion DE
(DE
(x)) alternative x DE
(x).
first consider PWUC , check alternative whether
reinforced reach every alternative two steps.
Theorem 5. PWUC solved polynomial time.
Proof. given partial tournament G = (V, E) alternative x V , check
whether x UC (T ) completion [G].
Consider graph G0 = (V, E 00 ) E 00 derived E follows. First,
let D+ (x) grow much possible letting E 0 = E x . Then,
+
two-step dominion defining E 00 E 0DE0 (x) . claim x PWUC (G)
+
++
V = {x} DE
00 (x) DE 00 (x).
() First, let x PWUC (G). definition, completion (V, E )
+
++
00
V \ {x} DE
(x) DE (x). definition E , follows
++
+
++
++
+
+
DE (x) DE 00 (c) DE (x) DE 00 (x). Consequently, also DE 00 (x) DE
00 (x).
++
+
() direction, let V \ {x}, DE 00 (x) DE 00 (x). completion
G0 , x trivially UC (T ), implying x PWUC (G).
similar argument yields following.
Theorem 6. NWUC solved polynomial time.
Proof. given partial tournament G = (V, E) alternative x V , check
whether x UC (T ) completions [G].
Consider graph G0 = (V, E 00 ) E 00 defined follows. First, let E 0 = E x . Then,

expand E 00 = E 0DE0 (x) . Intuitively, makes hard possible x beat
alternatives outside dominion two steps. claim x NWUC (G)
+
++
V = {x} DE
00 (x) DE 00 (x) equivalently, 6= x path
length one two x G.
() First, let x NWUC (G). Assume contradiction exists V \ {x}
++
+

0

/ DE
00 (x) DE 00 (x). Then, completion (V, E ) G , x cannot reach

two steps consequently x
/ UC (V, E ), contradiction.
++
+
completion (V, E ) G,
() Now, let V \ {x} = DE
00 (x) DE 00 (x).
+
+
++
++
DE 00 (x) DE (x) DE 00 (x) DE (x). Consequently, x UC (V, E )
x NWUC (G).
+
++
checked polynomial time whether V = {x} DE
00 (x) DE 00 (x),
completes proof.
Consider partial tournament G Figure 3(i ) example.
checked NWUC (G) = .10 PWUC , consider alternative separately.
a, E 0 = E = {(a, b), (a, c), (a, d), (b, d)}, E 00 = E 0 , therefore
+
DE
Likewise, b PWUC (G). Now, c,
00 (a) = {b, c, d} PWUC (G).
E 0 = {(a, c), (a, d), (b, d), (c, b), (c, d)} E 00 = {(a, c), (a, d), (b, d), (c, b), (c, d), (b, a)},
+
++
see also Figure 3(ii ). gives us DE
00 (c) = {b, d} DE 00 (c) = {a}, there0
fore, c PWUC (G). Lastly, d, E = {(a, c), (a, d), (b, d), (d, c)} E 00 =
10. also consequence NWTC (G) = (Section 4.3) NWUC NWTC (Lemma 2).

506

fiPossible Necessary Winners Partial Tournaments



b



b



b

c



c



c



(i )

(ii )

(iii )

Figure 3: partial unweighted tournament G possible extensions. center,
alternative c dominion maximally reinforced resulting c reaching every
alternative two steps. Therefore, c PWUC (G). right,
done alternative cannot reach two steps therefore contained
PWUC (G).
+
{(a, c), (a, d), (b, d), (d, c), (c, b)} depicted Figure 3(iii ). gives us DE
00 (d) = {c}
++
/ PWUC (G). summary,
DE 00 (d) = {b}, implying

PWUC [G] = {a, b, c},
NWUC (G) = ,
PWSUC (G) = {{a}, {b}, {a, b, c}},
PWSUC (G) obtained ad hoc argument.
solution concepts considered farCondorcet winners, Condorcet non-losers,
Copeland, top cyclePW PWS complexity. One might wonder
whether result like holds generally, whether could polynomialtime reduction PWS PW. following find case, unless
P=NP, show PWSUC , problem deciding whether subset alternatives
partial tournament G uncovered set completion G, NP-complete.
proof result proceeds reduction Sat involves construction partial
tournaments basis formulas conjunctive normal form. propositional
variable p every clause c, gadget based partial tournament Gp
depicted Figure 4(i ).
hard see exactly two completions Gp {p , p+ , 1}
uncovered set. first, + positive completion, depicted Figure 4(ii )
other, negative completion, Figure 4(iii ). verify
ones, consider arbitrary completion (V, E 0 ) Gp . Then, either (p , p+ ) E 0
(p+ , p ) E 0 . former case, observe p must covered 1. Hence,
(1, p ) E 0 (c, p ) E 0 . follows c covered p+ . Therefore, also
(p+ , 1) E 0 (p+ , c) E 0 . entails p covers p+ and, (p+ , p+ ) E 0
finally obtain (p , p+ ) E 0 . resulting tournament + . analogous argument
seen results assume (p , p+ ) E 0 . construction below,
positive completion + correspond setting propositional variable p true
negative completion setting p false.
Besides c, construction also involves alternative c clause. c related
alternatives Gp depends whether respective clause contains p p
literal. may assume clause contains p p, three cases remain,
507

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

p

p+

1

p

p+

1

p

p+

1

p

p+

c

p

p+

c

p

p+

c

(i ) variable gadget variable p

(ii ) completion + p set true

(iii ) completion p set false

Figure 4: partial tournament Gp two completions, + ,
uncovered set given {p , p+ , 1}. Dotted edges missing omitted edges point
downwards.

p

p+

1

p

p+

c

p

p+

(i ) c contains p p

1

p

p+

c

c

p

p+

(ii ) c contains p p

1

c

c

p

p+

c

(iii ) c contains neither p p

Figure 5: Gp -gadget alternative c added. Figure 4, dotted edges missing
omitted edges point downwards.
depicted Figure 5. reflection reveals clause contains p positive
literal, c covered p+ partial tournament completed positively,
p completed negatively. Similarly, clause contains p negative literal, c
covered p Gp completed negatively, p+ Gp completed
positively. c contains neither p p literal, c covered either p+ p
irrespective whether Gp completed positively negatively.
construction below, every clause, alternative c covered
clause contains literal p Gp -gadget completed positively literal q
Gq -gadget completed negatively.
Theorem 7. PWSUC NP-complete.
Proof. Given partial tournament G = (V, E), set X V , completion [G],
checked polynomial time whether X = UC (T ). Hence, PWSUC obviously
NP.
NP-hardness shown reduction Sat. Let formula conjunctive
normal form. Without loss generality may assume clause contains
508

fiPossible Necessary Winners Partial Tournaments

literal negation, least two clauses, every literal occurs
least one clause. construct partial tournament G = (V , E ) follows.
propositional variable p introduce five alternatives denoted p, p , p+ , p , p+ .
clause c, introduce two alternatives denoted c c. also two auxiliary
alternatives denoted 1 0. Thus,
V = {p, p , p+ , p , p+ : p variable} {c, c : c clause} {1, 0}.
give description edge set E . every propositional variable p
every clause c alternatives p , p+ , p , p+ , 1, c, c organized Figure 5.
remaining edges set way make construction work properly. Formally,
define edge set E every propositional variable p every clause c:
p dominates every clause well q , q + , q , q + every q 6= p;
p+ dominates 0, p, p along q , q + , q 6= p clauses d.
Moreover, every clause d, alternative p+ dominates alternative p
occurs literal clause d;
p dominates 0, p, p+ along q , q + , q 6= p clauses d.
Moreover, every clause d, alternative p dominates alternative p
occurs literal clause d;
p+ dominates 0, p, p+ ;
p dominates 0, p, p ;
c dominates 0, q , q + every variable q, every clause 6= c. Moreover,
variable q, alternative c dominates q + whenever c contain q literal,
q c contain q literal;
c

dominates 0, c, 1;

1 dominates 0 well q, q , q + variables q, clauses d;
0 dominates alternative q every variable q, otherwise 0 dominated
alternatives.
Moreover, every variable p, edges among p , p+ , 1 missing well
p , p+ , every clause d. Finally, edges specified
description set arbitrarily. example construction reader referred
Figure 6.
let
X = {p, p , p+ : p propositional variable} {c : c clause} {1}.
Table 2 summarizes alternatives reach alternatives two
steps G . thus find that, every completion G , set X contained
UC (T ) 0 covered 1. propositional variables p clauses c, alternatives p , p+ , c covered alternatives {p , p+ , 1}, i.e., whether
509

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

p

p

p

q

p+

p+

1

pq

pr

pq

pr

q

q+

q

q+

0

Figure 6: Part dominance relation partial tournament G associated
CNF formula = (p q) (p r). alternatives pq pr represent two clauses .
part involving variable r, i.e., alternatives r, r , r+ , r , r+ , omitted.
dashed edges dependent clauses . Omitted edges point downwards or,
level, arbitrary direction. Dotted edges missing.
covered depends subtournament {p , p+ , 1, p , p+ , c} completed. saw discussion preceding theorem, done positively
negatively, positive completion corresponds setting variable p true
negative completion setting p false. complete proof showing
X = UC (T ) [G ]

satisfiable.

() First assume satisfiable let v satisfying assignment .
propositional variable p v sets true clause c, complete subtournament {p , p+ , 1, p , p+ , c} positively, i.e., add edges (p , p+ ), (p+ , 1), (1, p )
well (p , p+ ), (p+ , c), (c, p ). Thus, p covered 1, p+ p , and, provided
p occurs literal c, c also p+ . Similarly, propositional variable q v
sets false clause c, complete subtournament {p , p+ , 1, p , p+ , c} negatively, i.e., add edges (1, q + ), (q + , q ), (q , 1) well (c, q + ), (q + , q ),
(q , c). Accordingly, q covered q + , q + 1, and, provided q occurs literal
c, c also q . Observe procedure induces well-defined completion G ,
denote Tv . v satisfies , every clause contains literal p v sets p
true literal q v sets q false. follows every clause c, alternative c
covered Tv . Observe p p+ covered Tv irrespective whether v sets p
510

fiPossible Necessary Winners Partial Tournaments

p q

p

p+

1

q q+ c

p

p+

c

q q+ 0

p
p
p+
1
c[p]
c[p]





0
0

q
0
0

0
0

c[p]

p
p

p

c[p]
p+

p+
p+


c
c
c






c
c


p
p
p p c[p]


p
p
p p
c[p]

q q+

p
p




p




p+

p
p+
c[p]
c[p]
0



0
0


0
0
0
0




c

q




c
q







p
p
c
c
p

p
p
c
c
p

p
p
1
1
p

p
p
1
1
p


p+
1
1
q

p

1
1
q

p
p


p

p
p
c
c
p















c






p
p
1
1
p

p
p
1
1
p







Table 2: Table summarizing types alternatives reach types alternatives one two steps (all completions of) partial tournament G . assume p
q distinct variables neither q q occurs literal c. Furthermore, c
assumed distinct clauses, c[p] denotes clause c understanding
p occurs literal c. Similarly, c[p] denotes clause c understanding p
occurs literal c. alternative x entry row r column c means r
reach c via x. entry dot (), r reach c directly, i.e., one zero
steps. box () signifies depends G completed whether
via alternative r reach c. minus () entry 0 1 means 0
cannot reach 1 two steps, matter G completed. Thus, 0 covered
1 every completion G . may assume clause contains literal
negation, least two clauses, every literal occurs least one
clause.
true false. Hence, c, p+ , p
/ UC (Tv ). Recalling 1 covers 0 X UC (T )
completions G , may conclude UC (Tv ) = X, desired.
() opposite direction, assume completion G
every propositional variable p every clause c, alternatives p , p+ , c
covered , i.e., UC (T ) = X. Define assignment vT sets
propositional variable p true clause c containing p literal
p+ covers c sets p false, otherwise. Observe vT well-defined
assignment.
show vT satisfies every clause hence well. end
consider arbitrary clause c. assumption, c covered alternative x. Recall
c reaches alternatives two steps except alternatives p+ p occurs
literal c alternatives q q occurs literal c (also see Table 2).
Hence, either x = p+ variable p occurring literal c x = q
variable q q occurs literal c.
former, vT sets p true consequently also satisfies clause c. latter,
demonstrate vT sets q false way satisfies clause c. suffices
511

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

2

4

q

q+

6

1

5
3
1

q

q+

c



Figure 7: Illustration concluding argument proof Theorem 7. double
edge alternative x alternative indicates x covers y. numbers
edges labelled correspond order existence demonstrated
proof Theorem 7.

show clause q + covers . end, consider arbitrary
clause d. prove q + cover refer Figure 7 illustration
reasoning. Let ET denote edge set . extends G , obviously E ET . First
recall assumed q covers c . (c, 1) E , also (q , 1) ET .
Since (q , q ) E , therefore case 1 covers q . Reaching every
alternative two steps G , alternative q must therefore covered q +
. (q , q ) E , follows (q + , q ) ET . Since, moreover, (q + , q + ) E ,
also q cover q + . Rather, q + reaches every alternative except 1
two steps . follows q + covered 1. Moreover, since (q + , q + ) E ,
also (1, q + ) ET . consider alternative observe that, construction, (d, 1) E .
Thus, reach q + two steps may conclude q + cover .
follows vT sets q false thus satisfies c, desired.

5. Weighted Tournament Solutions
turn weighted tournaments, particular consider solution concepts
Borda, maximin, ranked pairs.
5.1 Borda
Borda solution (BO) typically used voting context set N n voters,
voter equipped linear ranking individual preference. Then,
alternative receives |V | 1 points time ranked first voter, |V | 2
points time ranked second, forth. total number points
alternative x constitute Borda score sBO (x, (i )iN ) written
sBO (x, (i )iN ) =

X


512

|{y V : x y}|.

fiPossible Necessary Winners Partial Tournaments

generally, Borda solution extended n-weighted tournaments
Borda scores defined
X
sBO (x, (V, w)) =
w(x, y)
yV \{x}

BO(V, w) chooses alternatives maximum Borda score. subsumes
voting setting
X
sBO (x, (i )iN ) =
|{i N : x y}| = sBO (x, (V, w))
yV

weight edge x defined number voters rank x
higher y, i.e.,
w(x, y) = |{i N : x y}|.
proceed, define notion b-matching, used proofs
several results section. Let H = (VH , EH ) undirected graph
vertex capacities b : VH N0 . Then, b-matching H function : EH N0
v VH ,
X
m(e) b(v).
e{e0 EH :ve0 }

P
size b-matching defined
eEH m(e). easy see b(v) = 1
v VH , maximum-size b-matching equivalent maximum-cardinality
matching. b-matching problem upper lower bounds, function
: VH N0 . feasible b-matching function : EH N0
X
a(v)
m(e) b(v).
e{e0 EH :ve0 }

H bipartite, problem computing maximum-size feasible b-matching
lower upper bounds solved strongly polynomial time (Schrijver, 2003,
ch. 21). use result show PWBO PWSBO solved
polynomial time. following result PWBO also shown using Theorem 6.1
Kern Paulusma (2004), still give direct proof extended
PWSBO .
Theorem 8. PWBO solved polynomial time.
Proof. Observe BO satisfies following (weak) monotonicity property: making
winner x stronger increasing weight edge another alternative, cannot make x
losing alternative.
Let G = (V, w) partial n-weighted tournament, x V . previous observation,
x PWBO (G) x PWBO (Gx ). Therefore, assume w.l.o.g
G = Gx , i.e., edges incident x completely specified already. Moreover,
exists V \ {x} sBO (y, Gx ) > sBO (x, Gx ), already know
x
/ PWBO (G). thus assume sBO (y, Gx ) sBO (x, Gx ) V \ {x}.
513

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

give polynomial-time algorithm checking whether x PWBO (Gx ) via
reduction problem computing maximum-size b-matching bipartite graph.
Let = sBO (x, Gx ) Borda score x Gx . construct bipartite graph
H = (VH , EH ) vertices
VH = V \ {x} E x ,
E x = {{i, j} V \ {x} : 6= j}
edges
EH = {{i, {i, j}} : {i, j} V \ {x}, 6= j}.
define vertex capacities b : VH N0
b({i, j}) = n w(i, j) w(j, i) {i, j} E x
b(v) = sBO (v, Gx ) v V \ {x}.
observe completion = (V, w0 ) [Gx ], w0 (i, j) + w0 (j, i) = n
i, j V 6= j. sum Borda scores therefore n|V |(|V | 1)/2.
weight already used Gx ; weight yet used
equal
X
= n|V |(|V | 1)/2
sBO (v, Gx ).
vV

(Gx )

claim x PWBO
H b-matching size least .
0
x
() Let = (V, w ) [G ] completion x BO(T ). Consider bmatching m(i, {i, j}) = w0 (i, j) w(i, j). verify feasible b-matching.
Let v VH . v V \ {x},
X

m(e) = sBO (v, ) sBO (v, Gx ) sBO (v, Gx ) = b(v).

e{e0 EH :ve0 }

Otherwise, v = {i, j} E x
X

m(e) = m({i, {i, j}}) + m({j, {i, j}}) = n w(i, j) w(j, i) = b({i, j}).

e{e0 EH :{i,j}e0 }

size
X
X
X X
X
m(e) =
w0 (i, j) + w0 (j, i) w(i, j) w(j, i) =
n
w(i, j) = ,
eEH

i6=j

i6=j

iV jV \{i}

statement shown.
() direction, assume feasible b-matching size least exists.
construct completion = (V, w0 ) [Gx ] x BO(T ). Let
w0 (i, j) = m(i, {i, j}) + w(i, j)
0

{i, j} V \ {x},

0

w (x, i) = w(x, i), w (i, x) = w(i, x)
514

V \ {x}.

fiPossible Necessary Winners Partial Tournaments

w(i, j) w0 (i, j) w0 (i, j)+w0 (j, i) w(i, j)+w(j, i)+b({i, j}) = n {i, j} V ,
extension Gx .
X
X
X
b({i, j})
m(e) =
,
=
{i,j}E x

eEH

i6=j

know upper capacities b({i, j}) {i, j} E x exactly met (and
cannot matching size ). implies
w0 (i, j) + w0 (j, i) = w(i, j) + w(j, i) + b({i, j}) = n,
showing indeed completion Gx .
Since H constructed efficiently, since maximum-size b-matching
computed strongly polynomial time, algorithm runs polynomial time.

1
1



1
1



b

1

5

1

5

5

4

4

2

c

2
2

2

cap.

3
2

c



cap.



3

{a, b}

1
4



3

b

5



2

1

{a, d}

2

5

{b, d}

1

c

b
5

3

3



(ii ) partial tournament Gc .

(i ) partial 5-weighted tournament G.

0

b

1 4
2

(iii )

constructed
bipartite
graph H target Borda score
= sBO (c, Gc ) = 8. Capacities
given next vertices. Thick
edges weights indicate unique
maximum b-matching.

3
2



(iv ) completion G corresponds maximum b-matching.
case, BO(T ) = {a, b, c}.

Figure 8: Illustration algorithm checking whether alternative c contained
PWBO (G) partial 5-weighted tournament G.
Figure 8 illustrates described steps determining whether alternative contained PWBO (G).
idea extended polynomial-time algorithm PWSBO use
similar construction given G = (V, w), candidate set X V target Borda
score . Binary search used efficiently search interval possible target scores.
515

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Theorem 9. PWSBO solved polynomial time.
Proof. Let G = (V, w) partial n-weighted tournament, X V . give
polynomial-time algorithm checking whether X PWSBO (G), via bisection method
reduction problem computing maximum b-matching graph lower
upper bounds.
Assume target Borda score completion [G] X
PWSBO (T ) sBO (x, ) = x X. Then, maximum possible Borda score
alternative X 1.
given target Borda score , construct bipartite graph H = (VH , EH )
vertices VH = V E x ,
E x = {{i, j} V : 6= j},
edges
EH = {{i, {i, j}} : {i, j} V, 6= j, w(i, j) + w(j, i) < n}.
lower bounds : VH N0 upper bounds bs : VH N0 depend
defined follows. vertices x X, lower upper bounds coincide given

(x) = bs (x) = sBO (x, G).
vertices v VH \ X lower bound (v) = 0. Upper bounds
vertices defined
bs (v) = sBO (v, G) 1
v V \ X,
bs ({i, j}) = n w(i, j) w(j, i)
{i, j} E x .
proof Theorem 8, holds feasible b-matching H corresponds
extension G. extension completion [G] b-matching
size
X
= n|V |(|V | 1)/2
sBO (v, G),
vV

equals weight yet used G. Then, satisfies X PWSBO (T )
sBO (x, ) = x X. If, hand, gives rise graph
b-matching size , X 6 PWSBO (G).
order obtain polynomial-time algorithm, need check whether exists
target score corresponding graph H upper lower bounds admits
b-matching size . easily verified contained integer interval
= [ max sBO (x, G), n(|V | 1) ].
xX

Observe |I| depends n thus polynomially bounded size G.
Checking every integer therefore feasible polynomial time. However,
show perform binary search order find efficiently. need following
516

fiPossible Necessary Winners Partial Tournaments

two observations interval I. I, say admits feasible b-matching
corresponding graph H feasible b-matching.
First, s0 admits feasible b-matching, every s00 s00 s0 also
admits feasible b-matching. removing weight edges exceeds
(reduced) upper bounds gives feasible b-matching s00 .
Second, s0 0 size corresponding maximum feasible bmatching m0 , cannot s00 s00 s0 size 00 maximum
feasible b-matching m00 s00 smaller 0 . either (i ) m00 exists
since lower bounds met, (ii ) m00 exists size least 0 .
see latter, note decrease size maximum feasible matching cannot
caused upper bounds bs00 (v) bs0 (v) v VH . remains shown
increase as00 (v) v X result smaller maximum b-matching. Since
weight edges incident vertex X b-matching completely determined
bounds increases m0 m00 , total decrease size due edges
{j, {i, j}} V \ X, j V whose weight bounded bs00 ({i, j}) m00 (i, {i, j}).
then,
m00 (i, {i, j}) + m00 (j, {i, j}) = bs00 ({i, j}) bs0 ({i, j}) m0 (i, {i, j}) + m0 (j, {i, j})
therefore 00 0 .
two observations show partitioned two non-overlapping integer
intervals I1 I2 . Here, I1 admits feasible b-matching whose size increases
grows, whereas I2 admit feasible b-matchings. Therefore, either
I1 empty desired exist, = max(I1 ).
check existence following binary search algorithm. Let
[Imin , Imax ] interval initialized = [maxxX sBO (x, G), n(|V | 1)]. Consider
median value interval. corresponding graph H feasible b-matching,
continue interval [Imin , 1]. Otherwise, maximum feasible b-matching
size least , return yes. size less , continue [s+1, Imax ]. [Imin , Imax ]
empty, return no.
number queries algorithm bounded dlog2 |I|e dlog2 n|V |e and,
therefore, polynomial size G.
conclude section, show NWBO solved polynomial time well.
worth noting result follow directly polynomial-time result
NWBO case preference profiles (Xia & Conitzer, 2011).
Theorem 10. NWBO solved polynomial time.
Proof. Let G = (V, w) partial weighted tournament, x V . give polynomial-time
algorithm checking whether x NWBO (G).
Let G = Gx . want check whether alternative V \ {x}
achieve Borda score = sBO (x, G). done separately
V \ {x} reinforcing much possible G. y, sBO (y, Gy ) > ,
x
/ NWBO (G). If, hand, sBO (y, Gy ) V \ {x},
x NWBO (G).
517

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

example, consider partial 5-weighted tournament G Figure 8(i ). fact
{a, b, c} PWBO (G) follows already completion shown Figure 8(iv ). Also
note completion c chosen. Alternative possible
Borda winner since sBO (d, Gd ) = 7 < 8 = sBO (a, G). determine PWSBO (G), still
check subsets {a, b, c} possible winning sets. singletons, easy
see {a} {b} PWSBO (G). {a, b}, could employ binary
search method described Theorem 9. Here, argue moving one unit weight
(c, d) (d, c) completion shown Figure 8(iv ) gives completion
{a, b} winning set. NWBO (G), straightforward check alternative
necessary Borda winner. Altogether,
PWBO (G) = {a, b, c},
NWBO (G) = ,
PWSBO (G) = {{a}, {b}, {a, b}, {a, b, c}}.
5.2 Maximin
maximin score sMM (x, ) alternative x weighted tournament = (V, w),
given worst pairwise comparison, i.e., sMM (x, ) = minyV \{x} w(x, y). maximin solution, also known Simpsons method denoted MM , returns set
alternatives highest maximin score.
example, consider partial 5-weighted tournament depicted Figure 9(i ).
easy see (or b) unique maximin winners completions Ga (or Gb ).
Also, c cannot possible maximin winner always maximin score 0
whereas always least 1. Similarly, alternative never higher maximin
score a. Figure 9(iii ) shows completion {a, d} set maximin winners.
one unit weight shifted (c, b) (b, c), resulting completion {a, b, d}
maximin winners. also straightforward find completion G{a,b} {a, b}
set maximin winners. easy verify alternative necessary maximin
winner.
Together, gives
PWMM (G) = {a, b, d},
NWMM (G) = ,
PWSMM (G) = {{a}, {b}, {a, b}, {a, d}, {a, b, d}}.
first show PWMM polynomial-time solvable reducing problem
finding maximum-cardinality matching graph.
Theorem 11. PWMM solved polynomial time.
Proof. show check whether x PWMM (G) partial n-weighted tournament
G = (V, w). Consider graph Gx = (V, wx ). Then, sMM (x, Gx ) best possible
maximin score x get among completions G. sMM (x, Gx ) n2 ,
sMM (y, ) wx (y, x) n2 every V \ {x} every completion [Gx ],
therefore x PWMM (G).
518

fiPossible Necessary Winners Partial Tournaments

1
1



b

1
5

4

c

2
2
2

c





(i ) partial 5-weighted tournament G.

1
4



{a, b}
{a, c}
{a, d}
{b, c}
{b, d}
{c, d}



b

(ii )

constructed
bipartite

graph H = 1 X = {a, d}
proof Theorem 12.
maximum-cardinality matching given
thick edges.

b



14

5

5

3
5
3
2

b

3

2

c

3
2

14
2

5

c



(iii ) completion G could
obtained matching. Indeed,
MM (T ) = {a, d} sMM (T ) = 1.

3
2



(iv ) completion G witness
{a, b} PWSMM (G).

Figure 9: Example 5-weighted partial tournament completions relevant possible
maximin winners.
consider sMM (x, Gx ) < n2 . reduce problem checking whether x
PWMM (G) finding maximum-cardinality matching undirected unweighted
graph, known solvable polynomial time (Edmonds, 1965). want
find completion [Gx ] sMM (x, ) sMM (y, ) V \ {x}.
words, want complete weights edges alternatives V \ {x}
balanced way x still winner. exists V \ {x}
sMM (y, Gx ) > sMM (x, Gx ), already know x
/ PWMM (G). Otherwise,
V \ {x} derives maximin score least one particular edge (y, z)
z V \ {x, y} w(y, z) sMM (x, Gx ). Moreover, clear completion,
z cannot achieve maximin score less sMM (x, Gx ) edges (y, z)
(z, y) time. Let H = (VH , EH ) undirected unweighted graph
vertices
VH = V \ {x} {{i, j} V : 6= j}
edges
EH = {{i, {i, j}} : V \ {x}, j V \ {i}, wx (i, j) sMM (x, Gx )}.
way, matched {i, j} H, derives maximin score less
equal sMM (x, Gx ) comparison j. Clearly, size H polynomial
519

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

size G. show x PWMM (G) exists matching
cardinality |V | 1 H.
() First, assume x PWMM (G). exists completion = (V, w0 )
Gx maximin score V \ {x} sMM (x, Gx ) < n2 .
alternative derives maximin score comparison j 6= V \ {x}, i.e.,
sMM (i, ) = w0 (i, j), j cannot derive maximin score comparison
w0 (j, i) n sMM (x, Gx ) implies w0 (j, i) > n2 . Therefore, H, VH V
matched vertex {i, j} VH {i, j} matched vertex
VH . resulting matching H cardinality |V | 1.
() Now, assume exists matching cardinality |V | 1 H. Then,
V \ {x} matched {i, j} w(i, j) sMM (x, Gx ). Consider
completion = (V, w0 ) [Gx ] (i, j) V V {i, {i, j}} ,
set w0 (i, j) = w(i, j) w0 (j, i) = n w(i, j). Moreover, weights edges
set arbitrary completion edges Gx . Clearly, proper completion
Gx therefore G. , maximin score V \ {x} less equal
maximin score x. Therefore x MM (T ) implies x PWMM (G).
Next, show PWSMM solved polynomial time. proof proceeds
identifying maximin values could potentially achieved simultaneously
elements set question, solving problem values using
similar techniques proof Theorem 11. polynomially bounded number
problems need considered.
Theorem 12. PWSMM solved polynomial time.
Proof. Let G = (V, w) partial n-weighted tournament X V . give
polynomial-time algorithm checking whether X PWSMM (G).
X PWSMM (G) must completion [G] {0, . . . , n}
sMM (x, ) = x X sMM (i, ) < V \ X.
First, note > n w(j, i) X, j V w(i, j)

/ X, j V , X 6 PWSMM (G). Therefore, assume
n w(j, i) X, j V
w(i, j) <
/ X, j V.
treat cases > n2 , = n2 , <

n
2

separately.

Case 1: > n2 . Then, X PWSMM X singleton, x V ,
whether {x} PWSMM maximin score checked easily.
Case 2: = n2 . assumptions above, define G0 = (V, w0 ) extension
GX w0 (i, j) = w0 (j, i) = n2 = i, j X. Note every completion
G0 , sMM (i, ) = X X PWSMM (G) maximin score n2
corresponding completion X PWSMM (G0 ) maximin score
respective completion.
520

fiPossible Necessary Winners Partial Tournaments

addition, need check whether alternatives X forced
strictly smaller maximin score n2 . end, construct unweighted undirected
bipartite graph H = (VH , EH ) vertices
VH = V {{i, j} V : 6= j}
edges
EH = {{i, {i, j}} : V \ X, j V \ {i}, w(i, j) < }.
claim X PWSMM (G0 ) maximin score = n2 corresponding
completion maximum-cardinality matching size |V \ X| H.
() Let = (V, w00 ) completion G0 (and thereby G) X set
maximin winners sMM (i, ) = = n2 X.
/ X, needs
j 6= w00 (i, j) < . Collecting {i, {i, j}} pair gives matching
size |V \ X| H maximum since vertex one side bipartite graph
contained it.
() direction, assume maximum matching size |V \ X|.
construct completion = (V, w00 ) G0 X set maximin winners.
Note every (VH V ) \ X contained edge {i, {i, j}} matching.
edge, let w00 (i, j) = w0 (i, j) < w00 (j, i) = n w00 (i, j), implying
sMM (i, ) < . Otherwise, arbitrary completion G.
Together, sMM (i, ) = X sMM (i, ) <
/ X.
Figure 10 illustrates procedure 2-weighted tournament set X = {a}.
Case 3: < n2 . given , construct undirected unweighted bipartite

). Let V
graph H = (VH , EH
H
[
[

EH
=
{{i, {i, j}} : w(i, j) n w(j, i)}
{{i, {i, j}} : w(i, j) 1}.
iX
j6=i

iV
j6=i

claim X PWSMM (G) maximin score corresponding

completion maximum-cardinality matching size |V | H .
() Let = (V, w0 ) completion G X set maximin winners
maximum maximin score . every vertex V , j 6=
w0 (i, j) accounts maximin score i. Also, since < n2 , cannot case j
also derives maximin score w0 (j, i). Therefore, set pairs {i, {i, j}}
valid matching size |V |. obviously maximal.
() direction, assume maximum matching size |V |. Note
every (VH V ) matched define j(i) V edge {i, {i, j(i)}
contained matching. construct completion = (V, w0 ) X set
maximin winners. end, define
w0 (i, j(i)) = w0 (j(i), i) = n X,
w0 (i, j(i)) = 1 w0 (j(i), i) = n (s 1) V \ X.
long unspecified edges (i, j) completion, define
w0 (i, j) = max{w(i, j), } w0 (j, i) = n w(i, j) X, j V ,
w0 (i, j) = max{w(i, j), 1} w0 (j, i) = n w(i, j) otherwise.
521

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Note proper completion G. Now, sMM (i, ) = X
sMM (i, ) <
/ X. completes Case 3.


remains shown limited number possible (and thereby H )
considered. contrast proof Theorem 9, cannot employ binary search
method since clear cut feasible infeasible integer interval.
However, see gradually incremented 0 n2 1, whether
changes twice due definition
edge {i, {i, j}} contained EH

. partitions integer interval = [0, n 1] possible finite number
EH
2

subintervals Ik within single Ik induce H . Therefore,
sufficient consider one per Ik choose minimum. set
possibly relevant target scores given
[
[
[
=
min Ik
{w(i, j), n w(j, i) + 1}
{w(i, j) + 1}.
iX
j6=i

k

iV
j6=i

size obviously bounded 3n2 .
cases handled polynomial time.




b
1

1

1

1

1

b

2

11

1

1

1

c

1

c



(ii ) extension G0 reinforcing {a}.

(i ) partial 2-weighted tournament G.

b
c




{a, b}
{a, c}
{a, d}
{b, c}
{b, d}
{c, d}



b

2
1

1
11

2
1

1

c

2



(iv ) completion G0 G
MM (G0 ) = {a}

(iii ) constructed undirected bipartite graph H. Thick edges indicate
maximum-cardinality matching.

Figure 10: Illustration algorithm checking whether singleton {a} contained
PWSMM (G) partial 2-weighted tournament G. obvious cannot
maximin score 2 completion sole maximin winner maximin score
0. Therefore, check case = n2 = 1.
Lastly, consider NWMM , apply similar technique NWBO :
see whether x NWMM (G), start graph Gx check whether
alternative achieve higher maximin score x completion Gx .
522

fiPossible Necessary Winners Partial Tournaments

Theorem 13. NWMM solved polynomial time.
Proof. show check whether x NWMM (G) partial n-weighted tournament
G = (V, w). maximin score x Gx worst case maximin score x among
proper completions G.
V \ {x}, maximin score Gy best possible maximin score
among completions G. maximin score corresponding
Gy maximin score x Gx , x NWMM (G), otherwise
x
/ NWMM (G).
5.3 Ranked Pairs
method ranked pairs (RP ) resolute solution concept considered
article. Given weighted tournament = (V, w), returns unique undominated
alternative transitive tournament 0 V constructed following manner. First
order (directed) edges decreasing order weight, breaking ties according
exogenously given tie-breaking rule. start empty graph 0 consider
edges one one according ordering. current edge added 0
without creating cycle, so; otherwise discard edge.11
example, consider partial 5-weighted tournament depicted Figure 11(i ),
slightly modified version tournament considered Figures 8 9.
easy see ranked pairs winner completions Ga , likewise b
ranked pairs winner completions Gb . hand, completion
c ranked pairs winner. Whether possible ranked pairs winner depends
tie-breaking rule used, particular tie-breaking rule ranks
edges (d, c) (b, d): alternative possible ranked pairs winner (d, c)
considered (b, d) (see Figure 11(iv )). Since RP resolute, (assuming
tie-breaking rule ranks (d, c) (b, d))
PWRP (G) = {a, b, d}
NWRP (G) =
PWSRP (G) = {{a}, {b}, {d}}.
readily appreciated winner determination problem RP computationally tractable. possible winner problem, hand, turns NP-hard.
also shows tractability winner determination problem, necessary
tractability PW, generally sufficient.
Theorem 14. PWRP NP-complete.
Proof. work alternative characterization ranked pairs winners
introduced Zavist Tideman (1989). given weighted tournament = (V, w)
11. variant ranked pairs originally proposed Tideman (1987), also used Xia
Conitzer (2011), instead chooses set alternatives, containing alternative selected
procedure way breaking ties among edges equal weight. consider
irresolute version ranked pairs winner determination variant NP-hard (Brill
& Fischer, 2012). mentioned Section 3, immediately implies problems concerning
possible necessary winners NP-hard well.

523

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

1
1



4
1



b

1

5

1

5

5

4

5

2
1
2

c

4

1
4

b



5

5

4
1

14

4
3
2

b
5

3

5



(ii ) completion G ranked pairs
winner a.

1
5

c

3
2

c



(i ) partial 5-weighted tournament G.



b

2

c



(iii ) completion G ranked
pairs winner b.

1
4



(iv ) completion G ranked pairs
winner d. Here, assume edge
(d, c) considered edge (b, d).

Figure 11: Example 5-weighted partial tournament completions relevant possible
ranked pairs winners. completion, transitive tournament constructed
ranked pairs procedure indicated thick edges.
given tie-breaking rule, let denote order edges considered
ranked pairs procedure. is, (x, y) (u, v) either w(x, y) > w(u, v)
w(x, y) = w(u, v) tie-breaking rule ranks (x, y) higher (u, v). Given ranking
L V , two alternatives b, say attains b L exists
sequence distinct alternatives a1 , a2 , . . . , , 2, a1 = a, = b,
ai L ai+1 ,
(ai , ai+1 ) (b, a) 1 < t.
case, say attains b via (a1 , a2 , . . . , ). ranking L called stack
pair alternatives b holds L b implies attains b L.
Zavist Tideman (1989) shown alternative ranked pairs winner
top element stack.12 Intuitively, defining properties stack L
ensure pairs (a, b) alternatives L b, point time edge (b, a)
considered, discarded would create cycle.
Membership PWRP NP obvious, given completion given tiebreaking rule, ranked pairs winner found efficiently.
12. characterization Zavist Tideman (1989) addresses irresolute version ranked pairs
discussed previous footnote. adaptation resolute version ranked pairs straightforward corollary.

524

fiPossible Necessary Winners Partial Tournaments



p01

p02

p1

p1

p2

p2

p01

p02

c1

c2

c3

Figure 12: partial 8-weighted tournament G Boolean formula = {p1 , p2 }
{p1 , p2 } {p1 , p2 }. Double-shafted arrows represent heavy edges, standard arrows represent
medium edges, dashed arrows represent light edges, dotted lines represent partial edges.
pairs (a, b) connected arrow, w (x, y) = w (y, x) = 4.

NP-hardness shown reduction Sat. construction based
proof Theorem 1 Brill Fischer (2012). Boolean formula conjunctive
normal-form set C clauses set P propositional variables, construct
partial 8-weighted tournament G = (V , w ) follows. variable p P , V
contains two literal alternatives p p two auxiliary alternatives p0 p0 .
clause c C, alternative c. Finally, alternative
membership PWRP (G ) decided.
order conveniently describe weight function w , let us introduce following
terminology. two alternatives x, V , say heavy edge x
w (x, y) = 8 (and therefore w (y, x) = 0). medium edge x means w (x, y) = 6
w (y, x) = 2, light edge x means w (x, y) = 5 w (y, x) = 3.
Finally, partial edge x means w (x, y) = w (y, x) = 1.
ready define w . variable p P , heavy edges
p p0 p p0 , partial edges p p0 p p0 .
clause c C, medium edge c heavy edge literal
alternative ` (with ` = p ` = p p P ) c corresponding literal ` appears
clause c. Finally, heavy edges auxiliary alternatives light edges
literal alternatives. pairs x, edge specified,
define w (x, y) = w (y, x) = 4. example shown Figure 12. Observe
pairs alternatives w fully specified pairs connected
partial edge.
525

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig





`0

`0
`

`

`

`

`0

`0

c

c
0

(ii ) Sc = (d, `0 , `, c)

(i ) Sc = (d, ` , `, `0 , `, c)

Figure 13: Two possibilities sequence Sc .
show alternative possible ranked pairs winner G
satisfiable. Intuitively, choosing completion (p0 , p) (p0 , p) corresponds
setting variable p true.
() First assume PWRP let [G ] completion G
RP (T ) = {d}. Consider stack L top element alternative c corresponding
clause . Since L stack L c, attains c though L via sequence Sc . (If
attains c via several sequences, fix one arbitrarily.) Since w (c, d) = 6, edges
sequence Sc must heavy, medium, appropriate completions partial edges.
Therefore, Sc must one following two forms (depicted Figure 13):
0

Sc = (d, ` , `, `0 , `, c)

Sc = (d, `0 , `, c),
0

` literal. former fact possible w (`, ` ) = 8 implies
0
` attain ` L. Therefore, Sc form Sc = (d, `0 , `, c)
literal `.
define assignment setting true literals contained one
sequences Sc , c C. claim satisfying assignment .
order show well-defined, suppose exists literal ` `
` set true . implies exist c1 c2 attains c1
0
0
via Sc1 = (d, `0 , `, c1 ) attains c2 via Sc2 = (d, ` , `, c2 ). particular, `0 L ` ` L `.
0
0
However, easily verified stack ranks ` higher ` (because w (`, ` ) = 8)
0
` higher `0 (because w (`, `0 ) = 8). Thus, L-cycle ` L ` L ` L `0 L `,
contradicting assumption L stack.
order show satisfies , consider arbitrary clause c. attains c via
Sc = (d, `0 , `, c) w (c, d) = 6, w (`, yj ) 6. definition w (, ),
implies literal ` appears clause c. Furthermore, ` set true `
contained Sc .
() direction, assume satisfiable let satisfying
assignment. use construct completion = (V , w ) [G ] RP (T ) = {d}.
526

fiPossible Necessary Winners Partial Tournaments

partial edges, weight function w defined follows. literal ` set true
, let w (`0 , `) = 7 w (`, `0 ) = 1. Otherwise, let w (`0 , `) = 1 w (`, `0 ) = 7.
show RP (T ) = {d} going procedure constructs
transitive tournament 0 , starting empty tournament V .13 First, set
edges weight 7 added, cycles among
edges. set consists heavy edges (previously) partial edges. Next,
medium edges considered. edges form (c, d) c alternative
corresponding clause. Since satisfying assignment, 0 already contains paths
every clause alternative c. Therefore, edges (c, d) c C discarded.
next step, light edges (i.e., edges weight 5) considered. edges
form (d, `) literal `. Therefore, edges added 0 without
creating cycle (d ingoing edges 0 ). adding light edges,
outgoing edge literal alternatives ` auxiliary alternatives `0 . Furthermore,
edges clause candidate c already discarded. Thus, unique
undominated alternative 0 , i.e., RP (T ) = {d}.
Since ranked pairs method resolute, hardness PWSRP follows immediately.
Corollary 2. PWSRP NP-complete.
Computing necessary ranked pairs winners turns coNP-complete.
somewhat surprising, computing necessary winners often considerably easier computing possible winners, partial tournaments partial preference profiles (Xia
& Conitzer, 2011).
Theorem 15. NWRP coNP-complete.
Proof. Membership coNP obvious. hardness, give reduction
UnSat slight variation reduction proof Theorem 14. Let G0
partial 8-weighted tournament results form G adding new alternative
heavy edges alternatives V except d. Furthermore, light edge
. show necessary ranked pairs winner G0
unsatisfiable.
() Assume contradiction NWRP (G0 ) = {d } satisfiable. Let
satisfying assignment define tournament = (V {d }, w0 ) [G0 ]
w0 coincides w (as defined proof Theorem 14) partial edges.
arguments proof Theorem 14, follows ingoing
edges tournament 0 constructed ranked pairs procedure. point
time edge (d, ) considered, added 0 . yields RP (T ) = {d},
contradicting assumption NWRP (G0 ) = {d }.
() Assume contradiction unsatisfiable exists completion
[G0 ] RP (T ) = {x} 6= {d }. follows x = d. (All alternatives V \ {d}
incoming heavy edge (from ), heavy edges added
cycle among them.) argument proof Theorem 14, follows
satisfiable, contradicting assumption.
13. following arguments independent choice particular tie-breaking rule.

527

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

6. Possible Winning Subsets
considered problem whether subset alternatives possible winning set (PWS).
addition, may interest whether subset alternatives among winners
completion, i.e., whether completion alternatives
subset (and possibly alternatives) choice set. refer latter
problem PWSS (possible winning subset). note oracle solve PWSS
used solve PW. want check whether PW (G), simply check whether
{i} PWSS (G). aware direct algorithmic relation problems
PWS PWSS.
examined computational complexity PWSSS solution concepts
considered article. Since arguments often similar proofs already given,
briefly summarize findings here.
COND never one Condorcet winner, every X PWSSCOND (G)
singleton problem reduces computing PWCOND (G).
CNL PWSSCNL , note nonempty set X V , X
/ PWSSCNL (G)
1
|V | > 1 every completion G
Condorcet winner
furthermore located X. Therefore,



|V | = 1
1
X PWSSCNL (G)
PWSCOND (G )



1
PWCOND (G ) \ X 6= .
CO problems, polynomial computability PWSSCO follows
corresponding result PWSSBO .
TC problem PWSSTC solved polynomial time. fact, shown
partial tournament G set alternatives X, sufficient check
whether X PWTC (G) (with additional argument |X| = 2) order
determine whether X PWSSTC (G).
BO argument algorithm checking whether X PWSSBO (G) almost
argument PWSBO Theorem 9. difference
sBO (v, ) may instead 1 v V \ X [G].
Consequently, need redefine bs (v) sBO (v, G) v V \ X.
MM proof efficient computability checking whether X PWSMM (G)
modified accommodate PWSSMM . precisely, second basic assumption
w(i, j)
/ X, j V . = n2 , sufficient check whether G0
extension G. < n2 , edges {i, {i, j}} X contained
w(i, j) . rest argument adjusted appropriately.
EH
> n2 , nothing changes.
RP Since PWRP NP-complete (Theorem 14), get NP-hardness PWSSRP
oracle argument above. Since membership NP obvious, problem
NP-complete.
528

fiPossible Necessary Winners Partial Tournaments

complexity PWSSUC left open. Minor modification hardness construction PWSUC trick. argument, crucial question whether
completion excludes certain alternatives choice set.
help PWSSUC .

7. Discussion
problem computing possible necessary winners partial preference profiles
recently received lot attention. article, investigated problem
setting partially specified (weighted unweighted) tournaments instead profiles
given input. summarized findings Table 1.
key conclusion computational problems partial tournaments significantly easier counterparts partial profiles. example, possible Borda
maximin winners found efficiently partial tournaments, whereas corresponding problems partial profiles NP-complete (Xia & Conitzer, 2011). Furthermore,
computing possible necessary Copeland winners NP-hard coNP-hard respectively partial preference profiles (Xia & Conitzer, 2011). contrast, showed
even PWSCO solved polynomial time partial tournaments. negative
(hardness) results, tempered fact parameters
problem bounded constant, hard problems may solved polynomial time. particular, Yang Guo (2013) shown PWSUC polynomial-time
solvable size given subset X bounded constant.14
tractability winner determination problem necessary tractability
possible necessary winners problems, results ranked pairs Section 5.3 show
sufficient. considered problem deciding whether given
subset alternatives equals winning set completion partial tournament.
results uncovered set Section 4.4 imply problem cannot reduced
polynomial time computation possible necessary winners; whether reduction
exists opposite direction remains open problem.
Partial tournaments also studied right, independent
possible completions. instance, Peris Subiza (1999) Dutta Laslier (1999)
generalized several solution concepts tournaments partial tournaments.
common point approach follow nature input, namely, partial tournaments. However, Peris Subiza (1999) Dutta Laslier (1999) define
solution concepts partial tournaments directly generalizing usual definition
tournaments. contrast definitions, based completions
input partial tournament. notion possible winners suggests canonical way
generalize solution concept defined tournaments partial tournaments. way
extending tournament solutions partial tournaments referred conservative
extension inherits various axiomatic properties original tournament solutions satisfies tournaments (Brandt et al., 2014). positive computational results
article indication may promising approach.
14. Yang Guo (2013) also give hardness fixed-parameter tractability results generalization
Banks set partial tournaments.

529

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

also highlight another way viewing algorithmic results concerning possible
necessary winners. burgeoning literature computational social choice
deals problem manipulation control voting (Bartholdi, III, Tovey, & Trick,
1989, 1992; Faliszewski & Procaccia, 2010). given alternative already necessary winner, need invest effort influencing remaining comparisons votes
make winning. Moreover, results also implications partial tournament
version coalitional manipulation problem: coalitional tournament manipulation,
constructive version, defined follows. Given partial tournament (V, E), subset X V , distinguished alternative x, way complete missing edges
X X x winner? Informally, players X way fixing
winners matches among make x win?
Constructive coalitional tournament manipulation polynomial-time solvable whenever
PW is. Likewise, destructive version coalitional tournament manipulation (is
way complete edges within X candidate x winning?) polynomial
whenever NW is.
Regarding future work, yet examined complexity computing possible
necessary winners attractive tournament solutions minimal covering
set weighted versions top cycle uncovered set (De Donder, Le Breton, &
Truchon, 2000).15
interesting related question goes beyond computation possible necessary winners following: winners yet fully determined, unknown
comparisons need learned, pairs candidates compare,
matches played? problem seen tournament-based version
preference elicitation problem (Conitzer & Sandholm, 2002; Ding & Lin, 2013; Walsh,
2008). standard version problem looks minimal sets queries voters
pairwise preferences candidates, tournament version query bears
pair candidates output edge two candidates, one direction
other. Procaccia (2008) considers similar question COND. construction
policy tree defining optimal protocol minimizing number questions asked
number matches played, worst case average, even
challenging issue leave future research.

Acknowledgments
Previous versions paper presented 11th International Conference
Autonomous Agents Multi-Agent Systems (AAMAS 2012) 4th International Workshop Computational Social Choice (COMSOC 2012). grateful Felix
Brandt extensive discussions useful advice. also thank Gerhard Woeginger
hints towards improving previous pseudo-polynomial time algorithms PWSBO
PWSMM various anonymous reviewers, whose comments greatly helped us improve
paper. material based work supported Deutsche Forschungsgemeinschaft grants BR 2312/9-1, BR 2312/10-1, FI 1664/1-1. Haris Aziz supported
Australian Government represented Department Broadband, Commu15. Brill, Freeman, Conitzer (2016) recently shown computing possible necessary winners
bipartisan set (Laffond, Laslier, & Le Breton, 1993) intractable.

530

fiPossible Necessary Winners Partial Tournaments

nications Digital Economy Australian Research Council ICT
Centre Excellence program. Markus Brill supported Feodor Lynen research
fellowship Alexander von Humboldt Foundation ERC Starting Grant
639945 (ACCORD). Jerome Lang supported ANR project CoCoRICoCoDec. Paul Harrenstein supported ERC Advanced Grant 291528
(RACE).

References
Aziz, H., Gaspers, S., Mackenzie, S., Mattei, N., Stursberg, P., & Walsh, T. (2014). Fixing
balanced knockout tournament. Proceedings 28th AAAI Conference
Artificial Intelligence (pp. 552558). AAAI Press.
Aziz, H., Walsh, T., & Xia, L. (2015). Possible necessary allocations via sequential
mechanisms. Proceedings 23rd International Joint Conference Artificial
Intelligence (pp. 468474).
Bachrach, Y., Betzler, N., & Faliszewski, P. (2010). Probabilistic possible winner determination. Proceedings 24th AAAI Conference Artificial Intelligence (pp.
697702). AAAI Press.
Bartholdi, III, J., Tovey, C. A., & Trick, M. A. (1989). computational difficulty
manipulating election. Social Choice Welfare, 6 (3), 227241.
Bartholdi, III, J., Tovey, C. A., & Trick, M. A. (1992). hard control election?
Mathematical Computer Modelling, 16 (89), 2740.
Baumeister, D., Faliszewski, P., Lang, J., & Rothe, J. (2012). Campaigns lazy voters:
truncated ballots. Proceedings 11th International Conference Autonomous
Agents Multi-Agent Systems (pp. 577584). IFAAMAS.
Baumeister, D., & Rothe, J. (2010). Taking final step full dichotomy possible
winner problem pure scoring rules. Proceedings 19th European Conference
Artificial Intelligence (pp. 10191020).
Betzler, N., & Dorn, B. (2010). Towards dichotomy possible winner problem
elections based scoring rules. Journal Computer System Sciences, 76 (8),
812836.
Betzler, N., Hemmann, S., & Niedermeier, R. (2009). multivariate complexity analysis
determining possible winners given incomplete votes. Proceedings 21st
International Joint Conference Artificial Intelligence (pp. 5358). AAAI Press.
Brandt, F., Brill, M., & Harrenstein, P. (2014). Extending tournament solutions.
Proceedings 28th AAAI Conference Artificial Intelligence (pp. 580586).
AAAI Press.
Brandt, F., Brill, M., & Harrenstein, P. (2016). Tournament solutions. F. Brandt,
V. Conitzer, U. Endriss, J. Lang, & A. D. Procaccia (Eds.), Handbook Computational Social Choice (chap. 3). Cambridge University Press. (Forthcoming)
531

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Brill, M., & Fischer, F. (2012). price neutrality ranked pairs method.
Proceedings 26th AAAI Conference Artificial Intelligence (pp. 12991305).
AAAI Press.
Brill, M., Freeman, R., & Conitzer, V. (2016). Computing possible necessary equilibrium actions (and bipartisan set winners). Proceedings 30th AAAI Conference
Artificial Intelligence. AAAI Press. (Forthcoming)
Chevaleyre, Y., Lang, J., Maudet, N., & Monnot, J. (2011). Compilation communication protocols voting rules dynamic set candidates. Proceedings
13h Conference Theoretical Aspects Rationality Knowledge (pp. 153160).
Chevaleyre, Y., Lang, J., Maudet, N., Monnot, J., & Xia, L. (2012). New candidates welcome! Possible winners respect addition new candidates. Mathematical
Social Sciences, 64 (1), 7488.
Conitzer, V., & Sandholm, T. (2002). Vote elicitation: Complexity strategy-proofness.
Proceedings 18th National Conference Artificial Intelligence (pp. 392
397). AAAI Press.
Cook, W. J., Cunningham, W. H., Pulleyblank, W. R., & Schrijver, A. (1998). Combinatorial optimization. Wiley Sons.
De Donder, P., Le Breton, M., & Truchon, M. (2000). Choosing weighted tournament. Mathematical Social Sciences, 40 (1), 85109.
Ding, N., & Lin, F. (2013). Voting partial information: questions ask?
Proceedings 12th International Conference Autonomous Agents MultiAgent Systems (pp. 12371238). IFAAMAS.
Dutta, B., & Laslier, J.-F. (1999). Comparison functions choice correspondences. Social
Choice Welfare, 16 (4), 513532.
Edmonds, J. (1965). Paths, trees flowers. Canadian Journal Mathematics, 17 ,
449467.
Faliszewski, P., & Procaccia, A. D. (2010). AIs war manipulation: winning? AI
Magazine, 31 (4), 5364.
Filmus, Y., & Oren, J. (2014). Efficient voting via top-k elicitation scheme: probabilistic approach. Proceedings 15th ACM Conference Economics
Computation (pp. 295312). ACM Press.
Good, I. J. (1971). note Condorcet sets. Public Choice, 10 (1), 97101.
Hazon, N., Aumann, Y., Kraus, S., & Wooldridge, M. (2012). evaluation election
outcomes uncertainty. Artificial Intelligence, 189 , 118.
Kalech, M., Kraus, S., Kaminka, G. A., & Goldman, C. V. (2011). Practical voting rules
partial information. Journal Autonomous Agents Multi-Agent Systems,
22 (1), 151182.

532

fiPossible Necessary Winners Partial Tournaments

Kern, W., & Paulusma, D. (2004). computational complexity elimination
problem generalized sports competitions. Discrete Optimization, 1 (2), 205214.
Konczak, K., & Lang, J. (2005). Voting procedures incomplete preferences.
Proceedings Multidisciplinary Workshop Advances Preference Handling
(pp. 124129).
Laffond, G., Laslier, J.-F., & Le Breton, M. (1993). bipartisan set tournament
game. Games Economic Behavior , 5 (1), 182201.
Lang, J., Pini, M. S., Rossi, F., Salvagnin, D., Venable, K. B., & Walsh, T. (2012). Winner
determination voting trees incomplete preferences weighted votes. Journal
Autonomous Agents Multi-Agent Systems, 25 (1), 130157.
Laslier, J.-F. (1997). Tournament solutions majority voting. Springer-Verlag.
Lu, T., & Boutilier, C. (2011). Vote elicitation probabilistic preference models: Empirical estimation cost tradeoffs. Proceedings 2nd International Conference
Algorithmic Decision Theory (pp. 135149). Springer-Verlag.
Lu, T., & Boutilier, C. (2013). Multiwinner social choice incomplete preferences.
Proceedings 23rd International Joint Conference Artificial Intelligence (pp.
263270). AAAI Press.
Oren, J., Filmus, Y., & Boutilier, C. (2013). Efficient vote elicitation candidate
uncertainty. Proceedings 23rd International Joint Conference Artificial
Intelligence (pp. 309316). AAAI Press.
Peris, J. E., & Subiza, B. (1999). Condorcet choice correspondences weak tournaments.
Social Choice Welfare, 16 (2), 217231.
Pini, M. S., Rossi, F., Venable, K. B., & Walsh, T. (2011). Possible necessary winners
voting trees: Majority graphs vs. profiles. Proceedings 10th International
Conference Autonomous Agents Multi-Agent Systems (pp. 311318). IFAAMAS.
Procaccia, A. (2008). note query complexity Condorcet winner. Information
Processing Letters, 108 (6), 390393.
Rastegari, B., Condon, A., Immorlica, N., & Leyton-Brown, K. (2013). Two-sided matching
partial information. Proceedings 14th ACM Conference Electronic
Commerce (pp. 733750). ACM Press.
Schrijver, A. (2003). Combinatorial optimizationpolyhedra efficiency. Springer.
Schwartz, B. L. (1966). Possible winners partially completed tournaments. SIAM Review ,
8 (3), 302308.
Schwartz, T. (1986). logic collective choice. Columbia University Press.
Smith, J. H. (1973). Aggregation preferences variable electorate. Econometrica,
41 (6), 10271041.

533

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Tideman, T. N. (1987). Independence clones criterion voting rules. Social Choice
Welfare, 4 (3), 185206.
Vu, T., Altman, A., & Shoham, Y. (2009). complexity schedule control problems
knockout tournaments. Proceedings 8th International Conference
Autonomous Agents Multi-Agent Systems (pp. 225232). IFAAMAS.
Walsh, T. (2007). Uncertainty preference elicitation aggregation. Proceedings
22nd AAAI Conference Artificial Intelligence (pp. 38). AAAI Press.
Walsh, T. (2008). Complexity terminating preference elicitation. Proceedings
7th International Conference Autonomous Agents Multi-Agent Systems (pp.
967974). IFAAMAS.
Xia, L., & Conitzer, V. (2011). Determining possible necessary winners common
voting rules given partial orders. Journal Artificial Intelligence Research, 41 , 25
67.
Yang, Y., & Guo, J. (2013). Possible winner problems partial tournaments: parameterized study. Proceedings 3rd International Conference Algorithmic
Decision Theory (Vol. 8176, pp. 425439). Springer-Verlag.
Zavist, T. M., & Tideman, T. N. (1989). Complete independence clones ranked
pairs rule. Social Choice Welfare, 6 (2), 167173.

534

fiJournal Artificial Intelligence Research 54 (2015) 1-57

Submitted 09/14; published 09/15

Knowledge-Based Textual Inference via
Parse-Tree Transformations
Roy Bar-Haim

barhair@gmail.com

Ido Dagan

dagan@cs.biu.ac.il

Computer Science Department, Bar-Ilan University
Ramat-Gan 52900, Israel

Jonathan Berant

yonatan@cs.stanford.edu

Computer Science Department, Stanford University

Abstract
Textual inference important component many applications understanding
natural language. Classical approaches textual inference rely logical representations
meaning, may regarded external natural language itself. However,
practical applications usually adopt shallower lexical lexical-syntactic representations,
correspond closely language structure. many cases, approaches lack principled meaning representation inference framework. describe inference formalism
operates directly language-based structures, particularly syntactic parse trees. New
trees generated applying inference rules, provide unified representation
varying types inferences. use manual automatic methods generate rules,
cover generic linguistic structures well specific lexical-based inferences. also
present novel packed data-structure corresponding inference algorithm allows
efficient implementation formalism. proved correctness new algorithm
established efficiency analytically empirically. utility approach
illustrated two tasks: unsupervised relation extraction large corpus,
Recognizing Textual Entailment (RTE) benchmarks.

1. Introduction
Textual inference Natural Language Processing (NLP) concerned deriving target
meanings texts. textual entailment framework (Dagan, Roth, Sammons, &
Zanzotto, 2013), reduced inferring textual statement (the hypothesis h)
source text (t). Traditional approaches formal semantics perform inferences
logical forms derived text. contrast, practical NLP applications avoid
complexities logical interpretation. Instead, operate shallower representations
parse trees, possibly supplemented limited semantic information named
entities, semantic roles, forth. clearly demonstrated recent PASCAL
Recognizing Textual Entailment (RTE) Challenges (Dagan, Glickman, & Magnini, 2006b;
Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, & Szpektor, 2006; Giampiccolo,
Magnini, Dagan, & Dolan, 2007; Giampiccolo, Trang Dang, Magnini, Dagan, & Dolan,
2008; Bentivogli, Dagan, Dang, Giampiccolo, & Magnini, 2009; Bentivogli, Clark, Dagan,
c
2015
AI Access Foundation. rights reserved.

fiBar-Haim, Dagan & Berant

Dang, & Giampiccolo, 2010), popular framework evaluating application-independent
semantic inference.1
Inference representations commonly made applying transformations
substitutions tree graph representing text. transformations based
available knowledge paraphrases, lexical relations synonyms hyponyms,
syntactic variations, (de Salvo Braz, Girju, Punyakanok, Roth, & Sammons,
2005; Haghighi, Ng, & Manning, 2005; Kouylekov & Magnini, 2005; Harmeling, 2009).
transformations may generally viewed inference rules. available semantic knowledge bases composed manually, either experts, example WordNet
(Fellbaum, 1998), large community contributors, Wikipedia-based
DBPedia resource (Lehmann et al., 2009). knowledge bases learned automatically distributional pattern-based methods, using aligned monolingual
bilingual parallel texts (Lin & Pantel, 2001; Shinyama, Sekine, Sudo, & Grishman,
2002; Szpektor, Tanev, Dagan, & Coppola, 2004; Chklovski & Pantel, 2004; Bhagat &
Ravichandran, 2008; Ganitkevitch, Van Durme, & Callison-Burch, 2013). Overall, applied
knowledge-based inference prominent line research gained much interest. Recent examples include series workshops Knowledge Reasoning Answering
Questions (Saint-Dizier & Mehta-Melkar, 2011) evaluation knowledge resources
recent Recognizing Textual Entailment challenges (Bentivogli et al., 2010).
many applied systems use semantic knowledge inference rules,
use typically limited, application-specific, somewhat heuristic. Formalizing
practices important textual inference research, analogous role well-formalized
models parsing machine translation. take step direction introducing
generic inference formalism parse trees. formalism uses inference rules capture
wide variety inference knowledge simple uniform manner, specifies small
set operations suffice broadly utilize knowledge.
formalism, applying inference rule clear, intuitive interpretation generating new sentence parse (a consequent), semantically entailed source sentence.
inferred consequent may subject rule applications, on. Rule applications may independent other, modifying disjoint parts source tree,
may specify mutually-exclusive alternatives (e.g., different synonyms source
word). Deriving hypothesis text analogous proof search logic,
propositions parse trees deduction steps correspond rule applications.
nave implementation formalism would generate consequent explicitly
separate tree. However, discuss Section 5, implementation raises
severe efficiency issues, since number consequents may grow exponentially
number possible rule applications. Previous work proposed partial solutions
problem (cf. Section 8). work present novel data-structure, termed compact
forest, packed representation entailed consequents, corresponding inference
algorithm. prove new algorithm valid implementation formalism,
establish efficiency analytically, showing typical exponential-to-linear reduction,
empirically, showing improvement orders magnitude. Together, formalism
1. See, instance, listing techniques per submission provided organizers first
three challenges (Dagan et al., 2006b; Bar-Haim et al., 2006; Giampiccolo et al., 2007).

2

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

novel efficient inference algorithm open way large-scale rule application within
well-formalized framework.
Based formalism inference algorithm, built inference engine
incorporates variety semantic syntactic knowledge bases (cf. Section 6).
evaluated inference engine following tasks:
1. Unsupervised relation extraction large corpus. setting allows evaluation
knowledge-based inferences real-world distribution texts.
2. Recognizing textual entailment (RTE). cope complex RTE examples, complemented knowledge-based inference engine machine-learningbased entailment classifier, provides necessary approximate matching capabilities.
inference engine shown substantial contribution tasks, illustrating
utility approach.
Bar-Haim, Dagan, Greental, Shnarch (2007) Bar-Haim, Berant, Dagan
(2009) described earlier versions inference framework algorithm efficient implementation, respectively. current article includes major enhancements
contributions. formalism presented detail, including
examples pseudo-code algorithms. present several extensions formalism, including treatment co-reference, traces long-range dependencies, enhanced
modeling polarity. efficient inference algorithm also presented detail,
including pseudo-code. addition, provide complete proofs theorems,
establish correctness algorithm. Finally, article contains extended analysis
inference component RTE system, terms applicability, coverage,
correctness rule applications.

2. Background
section, provide background textual entailment. survey various
approaches applied task Recognizing Textual Entailment (RTE). particular,
focus use semantic knowledge within current RTE systems.
2.1 Textual Entailment
Many semantic applications need identify meaning expressed by,
inferred from, various language expressions. example, Question-Answering systems
need verify retrieved passage text entails selected answer. Given question
John Lennons widow?, text Yoko Ono unveiled bronze statue late
husband, John Lennon, complete official renaming Englands Liverpool Airport
Liverpool John Lennon Airport. entails expected answer Yoko Ono John Lennons
widow 2 . Similarly, Information Extraction systems need validate given text
indeed entails semantic relation expected hold extracted slot fillers
(e.g., X works ). Information Retrieval queries Alzheimers drug treatment3
2. example taken RTE-2 dataset (Bar-Haim et al., 2006).
3. one topics TREC-6 IR benchmark (Voorhees & Harman, 1997).

3

fiBar-Haim, Dagan & Berant

rephrased propositions (e.g., Alzheimers disease treated using drugs),
expected entailed relevant documents. selecting sentences
included summary, multi-document summarization systems verify
meaning candidate sentence entailed sentences already summary,
avoid redundancy.
observation led Dagan Glickman propose unifying framework modeling
language variability, termed Textual Entailment (TE) (Dagan & Glickman, 2004). Dagan
et al. (2006b) define TE follows:
say entails h if, typically, human reading would infer h
likely true. somewhat informal definition based (and assumes) common human understanding language well common background knowledge.
Dagan et al. (2013) discuss TE definition relation classical semantic
entailment linguistics literature. Recognizing Textual Entailment Challenges (RTE),
held annually since 2004 (Dagan et al., 2006b; Bar-Haim et al., 2006;
Giampiccolo et al., 2007, 2008; Bentivogli et al., 2009, 2010), formed growing research
community around task.
holy grail TE research development entailment engines, used
generic modules within different semantic applications, similar current use
syntactic parsers morphological analyzers. Since textual entailment defined
relation surface texts, bound particular semantic representation.
allows black-box view entailment engine, input/output interface
independent internal implementation, may employ different types
semantic representations inference methods.
2.2 Determining Entailment
Consider following (t,h) pair4 :

h

oddest thing UAE 500,000 2 million
people living country UAE citizens.
population United Arab Emirates 2 million.

Understanding h involves several inference steps. First, infer
reduced relative clause 2 million people living country proposition:
(1) 2 million people live country.
Next, observe country refers UAE, rewrite (1)
(2) 2 million people live UAE.
Knowing UAE acronym United Arab Emirates, obtain:
(3) 2 million people live United Arab Emirates.
4. Taken RTE1 test set (Dagan et al., 2006b).

4

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

finally paraphrase obtain h:
(4) population United Arab Emirates 2 million.
general, textual inference involves diverse linguistic world knowledge, including
knowledge relevant syntactic phenomena (e.g., relative clauses), paraphrasing (X people
live population X ), lexical knowledge (UAE United Arab Emirates),
on. may also require co-reference resolution, example, substituting country UAE. may think types knowledge representing inference
rules define derivation new entailed propositions consequents. work
introduce formal inference framework based inference rule application. current
discussion, however, informal notion inference rules would suffice.
example illustrates derivation h sequence inference
rule applications, procedure generally known forward chaining. Finding sequence
rule applications would get us h (or close possible) thus search
problem, defined space possible rule application chains.
Ideally, would like base entailment engine solely trusted knowledge-based
inferences. practice, however, available knowledge incomplete, full derivation h
often feasible. Therefore, requiring strict knowledge-based proofs likely
yield limited recall. Alternatively, may back heuristic approximate
entailment classification.
next two sections survey two complementary inference types: knowledgebased inference, focus research, approximate entailment matching
classification.
2.3 Knowledge-Based Inference
section, describe common resources inference rules (2.3.1),
use textual entailment systems (2.3.2).
2.3.1 Semantic Knowledge Resources
Lexical Knowledge Lexical-semantic relations words phrases play important role textual inference. prominent lexical resource WordNet (Fellbaum,
1998), manually composed wide-coverage lexical-semantic database. following WordNet relations typically used inference: synonyms (buy purchase), antonyms (win
lose), hypernyms/hyponyms (is-a relations, violin musical instrument), meronyms
(part-of relations, Provence France) derivations meeting meet.
Many researchers aimed deriving lexical relations automatically, using diverse methods sources. Much automatically-extracted knowledge complementary
WordNet, however, typically less accurate. Snow, Jurafsky, Ng (2006a) presented
method automatically expanding WordNet new synsets, achieving high precision.
Lins thesaurus (Lin, 1998) based distributional similarity. Recently, several works
aimed extract lexical-semantic knowledge Wikipedia, using metadata, well
textual definitions (Kazama & Torisawa, 2007; Ponzetto & Strube, 2007; Shnarch, Barak,
& Dagan, 2009; Lehmann et al., 2009, others). recent empirical study
5

fiBar-Haim, Dagan & Berant

inferential utility common lexical resources, see work Mirkin, Dagan, Shnarch
(2009).
Paraphrases Lexical-Syntactic Inference Rules rules typically represent
entailment equivalence predicates, including correct mapping
arguments (e.g., acquisition X X purchase ). Much work dedicated
unsupervised learning relations comparable corpora (Barzilay & McKeown, 2001; Barzilay & Lee, 2003; Pang, Knight, & Marcu, 2003), querying Web
(Ravichandran & Hovy, 2002; Szpektor et al., 2004), local corpus (Lin & Pantel,
2001; Glickman & Dagan, 2003; Bhagat & Ravichandran, 2008; Szpektor & Dagan, 2008;
Yates & Etzioni, 2009). particular, textual entailment systems widely used
DIRT resource Lin Pantel. common idea underlying algorithms,
predicates sharing argument instantiations likely semantically related.
NomLex-Plus (Meyers, Reeves, Macleod, Szekeley, Zielinska, & Young, 2004) lexicon containing mostly nominalizations verbs, allowed argument structures (e.g.,
Xs acquisition Y/Ys acquisition X etc.). Argument-mapped WordNet (AmWN)
(Szpektor & Dagan, 2009) resource inference rules verbal nominal predicates, including argument mapping. based WordNet NomLex-Plus,
verified statistically intersection unary-DIRT algorithm (Szpektor &
Dagan, 2008).
Syntactic Transformations Textual entailment often involves inference generic
syntactic phenomena passive/active transformations, appositions, conjunctions, etc.,
illustrated following examples:
John smiled laughed John laughed (conjunction)
neighbor, John, came John neighbor (apposition)
paper Im reading interesting Im reading paper (relative clause).
Syntactic transformations addressed extent de Salvo Braz et al.
(2005) Romano, Kouylekov, Szpektor, Dagan, Lavelli (2006). describe novel
syntactic rule base entailment, based survey relevant linguistic literature, well
extensive data analysis (Sections 6.16.2).
2.3.2 Use Semantic Knowledge Textual Entailment Systems
Following description common knowledge sources textual inference, discuss
use knowledge textual entailment systems.
Textual entailment systems usually represent h trees graphs, based
syntactic parse, predicate-argument structure, various semantic relations. Entailment
determined measuring well h matched (or embedded ) t, estimating
distance h, commonly defined cost transforming h.
next section, briefly cover various methods proposed approximate
matching heuristic transformations graphs trees. role semantic knowledge
general scheme bridge gaps h stem language
variability. example, applying lexical-semantic rule purchase buy allows
matching word buy appearing h word purchase appearing t.
6

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

RTE systems restrict type allowed inference rules search space.
Systems based lexical (word-based phrase-based) matching h (Haghighi et al.,
2005; MacCartney, Galley, & Manning, 2008) heuristic transformation h
(Kouylekov & Magnini, 2005; Harmeling, 2009) typically apply lexical rules (without
variables), sides rule matched directly h.
Hickl (2008) derived given (t, h) pair small set consequents terms
discourse commitments. commitments generated several different tools
techniques, based syntax (conjunctions, appositions, relative clauses, etc.), co-reference,
predicate-argument structure, extraction certain relations, paraphrase acquisition
Web. Pairs commitments derived h fed next stages
RTE system lexical alignment entailment classification. Prior commitment
generation, several linguistic preprocessing modules applied text, including
syntactic dependency parsing, semantic dependency parsing, named entity recognition,
co-reference resolution. Hickl employed probabilistic finite-state transducer (FST)-based
extraction framework commitment generation, extraction rules modeled
series weighted regular expressions. commitments textual form fed
back system, additional commitments generated.
De Salvo Braz et al. (2005) first incorporate syntactic semantic inference
rules comprehensive entailment system. system, inference rules applied
hybrid syntactic-semantic structures called concept graphs. left hand side (LHS)
rule matched concept graph, graph augmented instantiation
right hand side (RHS) rule. several iterations rule application,
system attempts embed hypothesis augmented graph. types semantic
knowledge, verb normalization lexical substitutions, applied either
rule application (at preprocessing time) rule application, part hypothesis
subsumption (embedding).
Several entailment systems based logical inference. Bos Markert (2005, 2006)
represented h DRS structures used Discourse Representation Theory (Kamp &
Reyle, 1993), translated first-order logic. Background knowledge
(BK) encoded axioms, comprised lexical relations WordNet, geographical
knowledge, small set manually composed axioms encoding generic knowledge.
Bos Markert used logic theorem prover find proof entails h (alone
together background knowledge BK), h inconsistent
(implying non-entailment) background knowledge. logic prover
complemented model builder aimed find counter-examples (e.g., model
h holds). logical inference system suffered low coverage, due limited
background knowledge available, able find proofs small fraction
RTE2 dataset. Therefore, RTE system Bos Markert combined logical inference
shallow approximate matching method, based mainly word overlap.
LCCs logic-based entailment system (Tatu & Moldovan, 2006) one top performers RTE2 RTE3 (Tatu, Iles, Slavick, Novischi, & Moldovan, 2006; Tatu &
Moldovan, 2007). based proprietary tools deriving rich semantic representations, extensive knowledge engineering. syntactic parses h
transformed logic forms (Moldovan & Rus, 2001), representation enriched
variety relations extracted semantic parser, well named entities
7

fiBar-Haim, Dagan & Berant

temporal relations. Inference knowledge included on-demand axioms based extended
WordNet lexical chains, WordNet glosses, NLP rewrite rules. Additional knowledge
types included several hundreds world knowledge axioms, temporal axioms, semantic composition axioms (e.g., encoding transitivity kinship relation). Based
rich semantic representation extensive set axioms, theorem prover aimed
prove refutation entails h. proof failed, h repeatedly simplified
proof found, reducing proof score simplification.
2.4 Approximate Entailment Classification
Semantic knowledge always incomplete. Therefore, cases, knowledge-based inference must complemented approximate, heuristic methods determining entailment. RTE systems employ limited amount semantic knowledge,
focus methods approximate entailment classification. common architecture
RTE systems (Hickl, Bensley, Williams, Roberts, Rink, & Shi, 2006; Snow, Vanderwende,
& Menezes, 2006b; MacCartney, Grenager, de Marneffe, Cer, & Manning, 2006) comprises
following stages:
1. Linguistic processing: Includes syntactic (and possibly semantic) parsing, namedentity recognition, co-reference resolution, etc. Often, h represented trees
graphs, nodes correspond words edges represent relations
words.
2. Alignment: Find best mapping h nodes nodes, taking account
node edge matching.
3. Entailment classification: Based alignment found, set features extracted
passed classifier determining entailment. features measure
alignment quality, also try detect cues false entailment. example,
node h negated aligned node negated, may indicate false
entailment.
alternative approach aims transform text hypothesis, rather
aligning them. Kouylekov Magnini (2005) applied tree edit distance algorithm
textual entailment. edit operation (node insertion/deletion/substitution) assigned
cost. algorithm aims find minimum-cost sequence operations transform
h. Mehdad Magnini (2009b) proposed method estimating cost
edit operation based Particle Swarm Optimization. Wang Manning (2010)
presented probabilistic tree-edit approach models edit operations using structured
latent variables. Tree edits represented state transitions Finite-State Machine
(FSM), model parameterized Conditional Random Field (CRF). Harmeling
(2009) developed probabilistic transformation-based approach. defined fixed set
operations, including syntactic transformations, WordNet-based substitutions,
heuristic transformations adding/removing verb noun. probability
transformation estimated development set. Similarly, Heilman Smith
(2010) classify entailment based sequence edits transforming h. employ
generic edit operations greedy search heuristic, guided cost function
measures remaining distance h using tree kernel.
8

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Zanzotto, Pennacchiotti, Moschitti (2009) aimed classify given (t, h) pair
analogy similar pairs training set. method based finding intra-pair
alignment (i.e., h) capturing transformation h, interpair alignment, capturing analogy new pair (t, h) previously seen
pair (t0 , h0 ). cross-pair similarity kernel computed, based tree kernel similarity
applied aligned texts aligned hypotheses. Another cross-pair similarity kernel
proposed Wang Neumann (2007). extracted tree skeletons h,
consisting left right spines, defined unlexicalized paths starting root.
found sections h spines differ compared sections across pairs
using subsequence kernel.

3. Research Goal
goal textual entailment research develop entailment engines used
generic inference components within various text-understanding applications. Logic-based
entailment systems provide formalized expressive framework textual inference.
However, deriving logic representations text complex task, available tools
match accuracy robustness current syntactic parsers (which often basis
semantic parsing). Furthermore, interpretation logic forms often unnecessary,
many common inferences modeled shallower representations.
follows textual entailment systems (and text-understanding applications
general) operate lexical-syntactic representations, possibly supplemented
partial semantic annotation. However, unlike logic-based approaches, systems
lack clear, unified formalism knowledge representation inference; instead
employ multiple representations inference mechanisms. notable exception
natural logic framework MacCartney Manning (2009), rather different
focus current work. discuss Section 8.
work, develop well-formalized entailment approach lexical-syntactic
level. formalism models wide variety inference rules composition, based
unified representation small set inference operations. Moreover, present
efficient implementation formalism using novel data structure algorithm
allow compact representation proof search space.
see contribution work practical theoretical. practical
(or engineering) perspective, formalism may simplify development entailment
systems, number representations inference mechanisms need dealt
minimal. Furthermore, efficient implementation may allow entailment engines
explore much larger search spaces. theoretical perspective, concise, formal modeling
leads better insight phenomenon investigation. particular,
formal model entailment engine makes possible apply formal methods investigating properties. enabled us prove correctness efficient implementation
formalism (cf. Appendix A). next present inference formalism.
9

fiBar-Haim, Dagan & Berant

Rule
Type
Syntactic

Sources

Examples

Manually-composed

Lexical

Learned unsupervised algorithms (DIRT, TEASE),
derived automatically integrating information WordNet
Nomlex, verified using corpus
statistics (AmWN)
WordNet, Wikipedia

Passive/active, apposition, relative
clause, conjunctions
Xs wife, X married

Syntactic

Lexical

X bought sold X

X maker X produces
steal take, AlbanianAlbania
Janis Joplinsinger
AmazonSouth America

Table 1: Representing diverse knowledge types inference rules

4. Inference Formalism Parse Trees
previous sections highlighted need principled, well-formalized approach
textual inference lexical-syntactic level. section, propose step towards
filling gap, defining formalism textual inference parse-based representations. semantic knowledge required inference represented inference rules,
encode parse tree transformations. rule application generates new consequent sentence (represented parse tree) source tree. Figure 1b shows sample inference
rule, representing passive-to-active transformation.
knowledge representation usage perspective, inference rules provide simple
unifying formalism representing applying broad range inference knowledge.
examples breadth illustrated Table 1. knowledge acquisition
perspective, representing inference rules lexical-syntactic level allows easy incorporation rules learned unsupervised methods, important scaling inference
systems. Interpretation stipulated semantic representations, often difficult
inherently supervised semantic task learning, circumvented altogether.
historical machine translation perspective, approach similar transfer-based translation, contrasted semantic interpretation Interlingua. overall research goal
explore reach inference approach, identify scope
semantic interpretation may needed.
Given syntactically parsed source text set inference rules, formalism
defines set consequents derivable text using rules. consequent
obtained sequence rule applications, generating intermediate parse
tree, similar proof process logic. addition, new consequents may inferred based
co-reference relations identified traces. formalism also includes annotation rules
add features existing trees. According formalism, text entails hypothesis
h h consequent t.
rest section, define illustrate formalism components:
sentence representation (Section 4.1), inference rules application (Sections 4.2
4.3), inference based co-reference relations traces (Section 4.4), annotation
10

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Input: source tree ; rule E : L R
Output: set derived trees
set matches L

f
l subtree matched L according match f
// R instantiation
r copy R
variable v r
Instantiate v f (v)
aligned pair nodes uL l uR r
daughter uL
/ l
Copy subtree rooted uR r, dependency relation
// Derived tree generation
substitution rule
copy l (and descendants nodes) replaced r
else // introduction rule
dr
add

Algorithm 1: Applying rule tree
rules (Section 4.5). components form inference process specifies set
inferable consequents given text set rules (Section 4.6). Section 4.7 extends
hypothesis definition, allowing h template rather proposition. Finally,
Section 4.8 discusses limitations possible extensions formalism.
4.1 Sentence Representation
assume sentences represented form parse trees. work, focus
dependency tree representation, often preferred directly capture predicateargument relations. Two dependency trees shown Figure 1a. Nodes represent words
hold set features values. features include word lemma
part-of-speech, additional features may added inference process.
Edges annotated dependency relations.
4.2 Inference Rules
entailment (or inference) rule L R primarily composed two templates, lefthand-side (LHS) L right-hand-side (RHS) R. Templates dependency subtrees,
may contain POS-tagged variables, matching lemma. Figure 1 shows passiveto-active transformation rule, illustrates application.
rule application procedure given Algorithm 1. Rule application generates set
derived trees (consequents) source tree steps described below.
11

fiBar-Haim, Dagan & Berant

root




rain VERB

expletive

r

wha



,

ADJ


r

Mary NOUN
mod

see VERB

obj

q


mod

bysubj



VERB

PREP

,

yesterday NOUN

pcompn


little ADJ

John NOUN

Source: rained little Mary seen John yesterday.

root




rain VERB

r

expletive

wha



,

ADJ



subj

r

John NOUN

see VERB
obj

mod

,

Mary NOUN yesterday NOUN
mod



little ADJ
Derived: rained John saw little Mary yesterday.

(a) Passive-to-active tree transformation


V VERB
obj

L

u

N1 NOUN

V VERB

bysubj


subj

obj



)

u

)

VERB

PREP

N2 NOUN

N1 NOUN

pcompn

R



N2 NOUN
(b) Passive active substitution rule.
Figure 1: Application inference rule. POS relation labels based Minipar
(Lin, 1998). N 1, N 2 V variables, whose instances L R implicitly aligned.
by-subj dependency relation indicates passive sentence.

12

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

root

root









V1 VERB V2 VERB
L



wha

R

ADJ




V2 VERB
Figure 2: Temporal clausal modifier extraction (introduction rule)

4.2.1 L Matching
First, matches L source tree sought. L matched exists
one-to-one node mapping function f L s, that:
1. node u L, f (u) features feature values u. Variables
match lemma value f (u).
2. edge u v L, edge f (u) f (v) s, dependency
relation.
matching fails, rule applicable s. example, variable V matched
verb see, N 1 matched Mary N 2 matched John. matching succeeds,
following performed match found.
4.2.2 R Instantiation
copy R generated variables instantiated according matching node
L. addition, rule may specify alignments, defined partial function L nodes
R nodes. alignment indicates modifier source node
part rule structure, subtree rooted also copied modifier
target node. addition explicitly defining alignments, variable L implicitly
aligned counterpart R. example, alignment V nodes implies
yesterday (modifying see) copied generated sentence, similarly
little (modifying Mary) copied N 1.
4.2.3 Derived Tree Generation
Let r instantiated R, along descendants copied L alignment,
l subtree matched L. formalism two methods generating
derived tree d: substitution introduction, specified rule type. Substitution
rules specify modification subtree s, leaving rest unchanged. Thus,
formed copying replacing l (and descendants ls nodes) r.
case passive rule, well lexical rules buy purchase.
contrast, introduction rules used make inferences subtree s,
parts ignored affect d. typical example inferring proposition
embedded relative clause s. case, derived tree simply taken
13

fiBar-Haim, Dagan & Berant

root


root




buy VERB
subj



purchase VERB

obj

subj

obj

v

(

v

(

John NOUN

books NOUN

John NOUN

books NOUN

John bought books.

L

buy VERB

John purchased books.



purchase VERB

R

Figure 3: Application lexical substitution rule. dotted arc represents explicit
alignment.

r. Figure 2 presents rule, enables deriving propositions embedded
within temporal modifiers. Note derived tree depend main clause.
Applying rule right part Figure 1a yields proposition John saw little
Mary yesterday.
4.3 Examples Rule Application
section illustrate rule representation application additional
examples.
4.3.1 Lexical Substitution Rule Explicit Alignment
Figure 3 shows derivation consequent John purchased books sentence
John bought books using lexical substitution rule buy purchase. example
illustrates role explicit alignment: since buy purchase variables,
implicitly aligned. However, need aligned explicitly, otherwise daughters
buy would copied purchase.
4.3.2 Lexical-Syntactic Introduction Rule
Figure 4 illustrates application lexical-syntactic rule, derives sentence
husband died knew late husband. defined introduction rule, since
resulting tree derived based solely phrase late husband, ignoring
rest source tree. example illustrates leaf variable L (variable
leaf node) may become non-leaf R vice versa. alignment
instances variable N (matched husband ) allows copying modifier, (recall
alignments defined implicitly formalism). note correctness
rule application may depend context applied. instance,
rule example correct late meaning longer alive given
context. discuss context-sensitivity rule application Section 4.8.
14

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

root

root







know VERB
subj

die VERB

obj

subj

v

(

NOUN

husband NOUN
gen




husband NOUN

mod

v

(

NOUN

late ADJ

gen



NOUN

knew late husband.

husband died.

root


L



N NOUN

die VERB



subj

mod

late ADJ



R

N NOUN

Figure 4: Application lexical-syntactic introduction rule

4.4 Co-Reference Trace-Based Inference
Aside primary inference mechanism rule application, formalism also allows
inference based co-reference relations long-distance dependencies. view coreference equivalence relation complete subtrees, either within tree
different trees, linked co-reference chain. practice, relations
obtained external co-reference resolution tool, part text pre-processing.
co-reference substitution operation similar application substitution rule.
Given pair co-referring subtrees, t1 t2 , derived tree generated copying
tree containing t1 , replacing t1 t2 ; operation symmetrically
applicable t2 .5 example, given sentences [My brother] musician. [He] plays
drums, infer brother plays drums.
Long-distance dependencies another type useful relation inference, illustrated following examples:
(1) Relative clause: boyi [I saw ti ] went home.
( saw boy.)
(2) Control verbs: Johni managed [ti open door].
( John opened door.)
5. view co-referring expressions substitutional also found seminal paper van
Deemter Kibble (2000), noun phrases shown non-substitutable evidence
co-referring.

15

fiBar-Haim, Dagan & Berant

(3) Verbal conjunction: [Johni sang] [ti danced].
( John danced.)
parsers including Minipar, use current work, recognize annotate
long distance dependencies. instance, Minipar generates node representing
trace (ti examples), holds pointer antecedent (e.g., Johni (2)).
shown examples, inference sentences may involve resolving long- distance
dependencies, traces substituted antecedent. Thus, generalize
co-reference substitution operate trace-antecedent pairs, well. mechanism
works together inference rule application. instance, substituting trace
antecedent (2) obtain John managed [John opened door].
apply introduction rule N managed extract embedded clause John
opened door.
4.5 Polarity Annotation Rules
addition inference rules, formalism implementation includes mechanism
adding semantic features parse tree nodes. However, many cases natural
way define semantic features classes. Hence, often difficult agree right
set semantic annotations (a common example definition word senses).
approach, aim keep semantic annotation minimum, sticking lexicalsyntactic representation, widely-agreed schemes exist.
Consequently, semantic annotation employ predicate polarity. feature
marks truth predicate, may take one following values: positive(+),
negative(-) unknown(?). examples polarity annotation shown below:
(4) John called[+] Mary.
(5) John hasnt called[] Mary yet.
(6) John forgot call[] Mary.
(7) John might called[?] Mary.
(8) John wanted call[?] Mary.
Sentences (5) (6) entail John didnt call Mary, hence negative annotation
call. contrast, truth John called Mary cannot determined (7) (8),
therefore predicate call marked unknown. general, polarity predicates
may affected existence modals, negation, conditionals, certain verbs, etc.
Technically, annotation rules right-hand-side R, rather node L
may contain annotation features. L matched tree, annotations contains
copied matched nodes. Figure 5 shows example annotation rule application.
Predicates assumed positive polarity default. polarity rules used
mark negative unknown polarity. one rule applies predicate
(as sentence John forgot call Mary), may applied order,
following simple calculus employed combine current polarity new polarity:
16

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

root


V[]
L





listen[]
subj

VERB

VERB





v

(

VERB

John NOUN

VERB

neg

neg



ADJ



ADJ
John listening[] .

(a) Annotation rule

(b) Annotated sentence

Figure 5: Application annotation rule (a), marking predicate listen negative
polarity (b)

Current polarity
+

?
+/ /?

New polarity



?

Result

+
?
?

Annotation rules used detecting polarity mismatches text hypothesis. Incompatible polarity would block hypothesis matched text.
case approximate entailment classification, polarity mismatches detected
annotation rules used features classifier, discuss Section 7.3.
addition, existence polarity annotation features may prevent inappropriate inference
rule applications, blocking L matching. discuss Section 6.1.
4.6 Inference Process
Let set dependency trees representing text, along co-reference
trace information. Let h dependency tree representing hypothesis, let R
collection inference rules (including inference polarity rules). Based
previously defined components inference framework, next give procedural
definition set trees inferable using R, denoted I(T, R). inference
process comprises following steps:
1. Initialize I(T, R) .
2. Apply matching polarity rules R trees I(T, R) (cf. Section 4.5).
3. Replace trace nodes copy antecedent subtree (cf. Section 4.4).
4. Add I(T, R) trees derivable co-reference substitution (cf. Section 4.4).
17

fiBar-Haim, Dagan & Berant

5. Apply matching inference rules R trees I(T, R) (cf. Section 4.2),
add derived trees I(T, R). Repeat step iteratively newly added
trees, new trees added.
Steps 2 3 performed h well.6 h inferable using R h I(T, R).
Since I(T, R) may infinite large, practical implementation process must
limit search space, example restricting number iterations applied
rules iteration.
inference rule applied, polarity annotation propagated source
tree derived tree follows. First, nodes copied retain original
polarity. Second, node gets polarity aligned node s.
4.7 Template Hypotheses
many applications useful allow hypothesis h template rather
proposition, is, contain variables. variables case existentially quantified: entails h exists proposition h0 , obtained h variable instantiation,
entails h0 . variable X instantiated (replaced) subtree SX . X
modifiers h (i.e., X leaf), become modifiers SX root. obtained
variable instantiations may stand answers sought questions slots filled relation extraction. example, applying framework question-answering setting,
question killed Kennedy? may transformed hypothesis X killed Kennedy.
successful proof h sentence assassination Kennedy Oswald shook
nation would instantiate X Oswald, providing sought answer.
4.8 Limitations Possible Extensions
conclude section discussing limitations presented inference formalism,
well possible extensions address limitations. First, inference rules match
single subtree, therefore less expressive logic axioms used Bos
Markert (2005) Tatu Moldovan (2006), may combine several predicates
originating text representation well background knowledge.
allows logic-based systems make inferences combine multiple pieces information.
instance, text says person X lives city , background knowledge
tells us city country Z, infer X lives country Z, using
rule person(X) location(Y) location(Z) live(X,Y) in(Y,Z) live(X,Z) .
Schoenmackers, Etzioni, Weld, Davis (2010) describe system acquires rules
(first-order horn clauses) Web text. Allowing rules match multiple subtrees
t, well information background knowledge, seems plausible future extension
formalism.
Another limitation formalism lack context disambiguation. Word sense
mismatch potential cause incorrect rule applications. example, rule hit
score applied correctly (9) (10):
6. Step 4 applied h since hypothesis typically short, simple sentence usually
include co-referring NPs. Moreover, presented formalism h single tree. Applying co-referencebased inference would resulted additional trees inferred h, thus would required
extending formalism accordingly.

18

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

(9) team hit home run. team scored home run.
(10) car hit tree. ; car scored tree.
Several works past years addressed problem context-dependent rule application (Dagan, Glickman, Gliozzo, Marmorshtein, & Strapparava, 2006a; Pantel, Bhagat,
Coppola, Chklovski, & Hovy, 2007; Connor & Roth, 2007; Szpektor, Dagan, Bar-Haim, &
Goldberger, 2008; Dinu & Lapata, 2010; Ritter, Mausam, & Etzioni, 2010; Berant, Dagan,
& Goldberger, 2011; Melamud, Berant, Dagan, Goldberger, & Szpektor, 2013). Szpektor
et al. (2008) proposed comprehensive framework modeling context matching, termed
Contextual Preferences (CP). Given text t, hypothesis h (possibly template hypothesis) inference rule r bridging h, objects annotated
two context components: (a) global (topical) context, (b) preferences constraints instantiation objects variables (for r template h). CP requires
h r matched t, h matched r7 , context component
matched counterpart. Szpektor et al. also proposed concrete implementations
components. example, could model global context
r sets content words, compute semantic relatedness
two sets, using methods Latent Semantic Analysis (LSA) (Deerwester, Dumais,
Furnas, Landauer, & Harshman, 1990), Explicit Semantic Analysis (ESA) (Gabrilovich
& Markovitch, 2007). would expect semantic relatedness {score}
{team, home run} much higher {score} {car, tree}, would
permit inference (9) (10).
RTE systems (including system RTE experiments, described
Section 7.3) lexicalized rules bridge h directly, rules LHS
RHS matched h, respectively. Since RTE benchmarks h tend
semantic context, setting alleviates context matching problems
extent. However, analysis, presented later work (Subsection 7.5.2), shows
context matching remains issue even setting, expected become even
important chaining lexicalized rules attempted. Adding contextual preferences
formalism important direction future work.
validity rule application also depends monotonicity properties application site. instance, hypernym rule poodle dog applicable upward
monotone contexts. Monotonicity may affected presence quantifiers, negation, certain verbs implicatives counterfactives (Nairn, Condoravdi, &
Karttunen, 2006). common textual entailment systems, assume upward monotonicity anywhere. assumption usually holds true, cases may lead
incorrect inferences. following examples show correct applications rule
upward monotone contexts ((11),(14)), incorrect applications downward monotone
contexts ((12),(13),(15)):
(11) bought poodle. bought dog.
(12) didnt buy poodle ; didnt buy dog
(13) Poodles smart. ; Dogs smart.
7. Context matching, like textual entailment, directional relation.

19

fiBar-Haim, Dagan & Berant

(14) failed avoid buying poodle failed avoid buying dog.
(15) fail avoid buying poodle ; fail avoid buying dog.
MacCartney Manning (2009) address monotonicity well semantic relations
exclusion, Natural Logic framework based syntactic representation.
discuss work detail Section 8.
Finally, since polarity annotation rules applied locally, may fail complex
cases, computing polarity buying sentences (14) (15), polarity
information need propagated along syntactic structure sentence.
TruthTeller system (Lotan, Stern, & Dagan, 2013), computes predicate polarity (truth
value) combination annotation rules global polarity propagation algorithm,
extending previous work Nairn et al. (2006) MacCartney Manning (2009).
4.9 Summary
section, presented well-formalized approach textual inference parsebased representations, core paper. framework, semantic knowledge
represented uniformly inference rules specifying tree transformations. provided
detailed definitions representation rules well inference mechanisms
apply them. formalism also models inferences based co-reference relations
traces. addition, includes annotation rules used detect contexts affecting
polarity predicates. next section present efficient implementation
formalism.

5. Compact Forest Scalable Inference
According formalism, rule application generates new sentence parse (a consequent), semantically entailed source sentence. inferred consequent may
subject rule applications, on. straightforward implementation
formalism would generate consequent separate tree. Unfortunately, nave
approach raises severe efficiency issues, since number consequents may grow exponentially number rule applications. Consider, example, sentence Children
fond candies, following rules: childrenkids, candiessweets, X
fond YX likes Y. number derivable sentences, including source sentence,
would 23 (the power set size), rule either applied not, independently.
found exponential explosion leads poor scalability nave implementation
approach practice.
Intuitively, would like rule application add entailed part rule
(e.g., kids) packed sentence representation. Yet, still want resulting structure
represent set entailed sentences, rather mixture sentence fragments
unclear semantics. discussed Section 8, previous work proposed partial solutions
problem.
section, introduce novel data structure, termed compact forest, corresponding inference algorithm, efficiently generate represent consequents
preserving identity individual one. data structure allows compact representation large set inferred trees. rule application generates explicitly
20

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

nodes rules right-hand-side. rest consequent tree shared source
sentence, also reduces number redundant rule applications, explained later
section. show representation based primarily disjunction edges,
extension dependency edges specify set alternative edges multiple trees.
Since follow well-defined inference formalism, able prove inference
operations formalism equivalently applied compact forest. compare
inference cost compact forests explicit consequent generation theoretically,
illustrating exponential-to-linear complexity ratio, empirically, showing improvement
orders magnitude (empirical results reported Section 7.2).
5.1 Compact Forest Data Structure
compact forest F represents set dependency trees. Figure 6d shows example
compact forest containing trees sentences Little Mary seen John yesterday
John saw little Mary yesterday. first define general data structure
directed graphs, narrow definition case trees.
Compact Directed Graph (cDG) pair G = (V, E) V set nodes E
set disjunction edges (d-edges). Let set dependency relations. d-edge
triple (Sd , reld , Td ), Sd Td disjoint sets source nodes target
nodes; reld : Sd function specifying dependency relation corresponds
source node. Graphically, d-edges shown point nodes, incoming edges
source nodes outgoing edges target nodes. instance, let bottommost
d-edge Figure 7. Sd = {of, like}, Td = {candy, sweet}, rel(of ) = pcomp-n,
rel(like) = obj .
d-edge represents, si Sd , set alternative directed edges {(si , tj ) : tj
Td }, labeled relation given reld (si ). edges,
termed embedded edge (e-edge), would correspond different graph represented G.
obj

obj

pcompn

previous example, e-edges likecandy, likesweet, ofcandy
pcompn
ofsweet (the definition implies source nodes Sd set
alternative target nodes Td ). d-edge called outgoing d-edge node v v Sd
incoming d-edge v v Td . Compact Directed Acyclic Graph (cDAG)
cDG contains cycles e-edges.
DAG G rooted node v V cDAG G embedded G derived
follows: initialize G v alone; then, expand v choosing exactly one target
node Td outgoing d-edge v, adding corresponding e-edge
(v, t) G. expansion process repeated recursively new node added G.
set choices results different DAG v root. Figure 6d,
may choose connect root either left see, resulting source passive
sentence, right see, resulting derived active sentence.
Compact Forest F cDAG single root r (i.e., r incoming d-edges)
embedded DAGs rooted r trees. set trees, termed embedded
trees, denoted (F) comprise set trees represented F.
Figure 7 shows another example compact forest efficiently representing 23 sentences resulting three independently applied rules presented beginning
section.
21

fiBar-Haim, Dagan & Berant

ROOT

ROOT





see

V

by-subj obj





Mary

pcomp-n

John

see

mod



by-subj obj

yesterday



mod

pcomp-n

little

mod



yesterday

little

(b) Variable instantiation

ROOT

ROOT





see
obj



mod

John

(a) Right-hand-side generation

by-subj

Mary

see

see

see



by-subj

mod mod

see
mod

mod

obj

obj

subj


pcomp-n

John

Mary



yesterday



mod



yesterday
pcomp-n

little

John

(c) Alignment sharing

Mary
mod

little

(d) Dual-leaf variable sharing

Figure 6: Step-by-step construction compact forest containing source sentence Little Mary seen John yesterday sentence John saw little Mary
yesterday derived via application passive rule Figure 1b. Parts
speech omitted.

5.2 Inference Process
next describe algorithm implementing inference process described Section 4.6
compact forest (henceforth, compact inference), illustrated Figures 1b (the
passive-to-active rule) 6.
22

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

ROOT



pred

fond

like

mod subj

subj
obj



child

kid
pcomp-n

candy

sweet

Figure 7: compact forest representing 23 sentences derivable sentence Children fond candies using following three rules: childrenkids, candiessweets,
X fond YX likes Y.

5.2.1 Forest Initialization
F initialized set dependency trees representing text sentences,
roots connected forest root target nodes single d-edge. Dependency
edges transformed trivially d-edges single source target. Annotation
rules applied stage initial F. Figure 6a, without node labeled V
incoming edge, corresponds initial forest (containing single sentence
example).
5.2.2 Inference Rule Application
Inference rule application comprises steps described below, summarized
Algorithm 2.
L Matching first find matches rules LHS L forest F (line 1).
sake brevity, omitted technical details L matching implementation
pseudocode Algorithm 2. following high-level description matching
procedure, focusing key algorithmic points.
L matched F exists embedded tree F L matched
t, Section 4.2. denote l subtree L matched (line 3).
23

fiBar-Haim, Dagan & Berant

Input: compact forest F ; inference rule E : L R
Output: modified F, denoted F 0 , (F 0 ) = (F) D, set trees derived
applying E subset Ls matches trees (F)
1: set matches L F
2: match f
3:
l subtree F L matched according f
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

// Right-hand-side generation
SR copy R excluding dual leaf variable nodes
Add SR F
SL l excluding dual leaf variable nodes
rR root(SR )
rL root(l)
E substitution rule
incoming d-edge rL // set SR alternative SL
else // introduction rule
outgoing d-edge root(F) // set SR alternative trees (F)
Add rR Td

15:
16:
17:
18:
19:

// Variable instantiation
variable X held node xR SR // Rs variables excluding dual leaves
X leaf L
xL f (X) // node SL matched X
(xR .lemma, xR .polarity) (xL .lemma, xL .polarity)

20:
21:
22:
23:
24:
25:
26:

else // X leaf L matched whole target node set
(xR .lemma, xR .polarity) (n.lemma, n.polarity) node n f (X)
n0 f (X); n0 6= n
generate substitution rule n n0 n n0 aligned, apply xR
x0R instantiation n0
u SL u aligned xR
add alignment u x0R

27:
28:
29:
30:
31:
32:

// Alignment sharing
aligned pair nodes nL SL nR SR
nR .polarity nL .polarity
outgoing d-edge nL whose e-edges part SL
Add nR Sd
reld (nR ) reld (nL )

33:
34:
35:
36:

// Dual leaf variable sharing
dual-leaf variable X matched node v l
incoming d-edge v
p parent node X SR

37:
38:
39:
40:
41:

// go p alternatives p generated variable instantiation
P set target nodes ps incoming d-edge
p0 P
Add p0 Sd
reld (p0 ) relation X p

Algorithm 2: Applying inference rule compact forest

24

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

subtree may shared multiple trees represented F, case rule
applied simultaneously trees. Section 4.2, match example
(V, N 1, N 2)=(see, Mary, John). definition allow l scattered
multiple embedded trees. Matches constructed incrementally, aiming add Ls nodes
one one partial matches constructed far, verifying candidate node
F node content corresponding edge labels match. also verified
match contain one e-edge d-edge. nodes F
indexed using hash table enable fast lookup.
target nodes d-edge specify alternatives position tree,
parts-of-speech expected substitutable. assume target nodes
d-edge part-of-speech8 polarity. Consequently, variables
leaves L may match certain target node d-edge mapped whole
set target nodes Td rather single node. yields compact representation
multiple matches, prevents redundant rule applications. instance, given compact
representation {Children/kids} fond {candies/sweets} (cf. Figure 7), rule X
fond YX likes matched applied once, rather four times (for
combination matching X ).
Right-Hand-Side Generation Given inference rule L R, define dual-leaf
variable variable leaf L R. example, N 1 N 2
dual-leaf variables passive-to-active rule Figure 1b. Variables
node R (and hence root leaf), variables additional
alignments (other implicit alignment occurrences L R)
considered dual-leaves. explained below, instantiations dual leaf variables
shared source target trees.
right-hand-side generation step, template SR (line 5), consisting R
excluding dual-leaf variables, generated inserted F (line 6). example,
SR includes node V passive rules RHS. Similarly, define SL l
excluding dual-leaf variables (line 7).
case substitution rule (as example), SR set alternative SL
adding SR root Td , incoming d-edge SL root (line 11). case
introduction rule, set alternative trees forest adding
SR root target node set forest roots outgoing d-edge (line 13). Figure 6a
illustrates results step example. SR gray node labeled
variable V , becomes additional target node d-edge entering original
(left) see.
Variable Instantiation variable SR (i.e., non dual-leaf) instantiated (lines
16-26) according match L (as Section 4.2). example, V instantiated
see (Figure 6b, lines 17-19). specified above, variable SR leaf L (which
case example) matched set nodes,
instantiated SR (lines 20-26). decomposed sequence simpler
operations: first, SR instantiated representative set (line 21).
apply ad-hoc lexical substitution rules creating new node additional node
8. case current implementation, based coarse tag-set Minipar.

25

fiBar-Haim, Dagan & Berant

set (line 22-26). nodes, addition usual alignment source nodes
SL (lines 25-26), share daughters SR (due alignment n
n0 , defined line 23).
Alignment Sharing Modifiers aligned nodes shared (rather copied) follows.
Given node nL SL aligned node nR SR , outgoing d-edge nL
part l, share nL nR adding nR Sd setting
reld (nR ) = reld (nL ) (lines 28-32). example (Figure 6c), aligned nodes nL
nR left right see nodes, respectively, shared modifier yesterday.
dependency relation mod copied right see node. also copy polarity annotation
nL nR (line 29).
note point instantiation variables dual leaves cannot
shared typically different modifiers two sides rule. Yet,
modifiers, part rule, shared alignment operation
(recall common variables always considered aligned). Dual leaf variables,
hand, might shared, described next, since rule doesnt specify modifiers
them.
Dual Leaf Variable Sharing final step (lines 34-41) performed similarly
alignment sharing. Suppose dual leaf variable X matched node v l whose
incoming d-edge d. simply add parent p X SR Sd set reld (p)
relation p X (in R). Since v shared, modifiers become shared
well, implicitly implementing alignment operation. subtrees little Mary John
shared way variables N 1 N 2 (Figure 6d). ad-hoc substitution rules
applied p variable instantiation phase, generated nodes serve alternative
parents X, thus sharing procedure applied p repeated them.
Applying rule example added single node linked four d-edges,
compared duplicating whole tree explicit inference.
5.2.3 Co-reference Substitution
Section 4.4 defined co-reference substitution, inference operation allows replacing subtree t1 co-referring subtree t2 . operation implemented generating
on-the-fly substitution rule t1 t2 applying t1 . implementation,
initial compact forest annotated co-reference relations obtained external
co-reference resolution tool, substitutions performed prior rule applications.
Substitutions t2 pronoun ignored, usually useful.
5.3 Correctness
section, present two theorems proving inference process presented
valid implementation inference formalism. provide full proofs Appendix A.
Theorem 1, argue applying rule compact forest results compact
forest. Since begin valid compact forest created initialization step, follows
induction sequence rule applications result inference process
compact forest. fact embedded DAGs generated inference
process indeed trees trivial, since nodes generally many incoming e-edges
26

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

many nodes. However, show pair parent nodes cannot part
embedded DAG. example, Figure 7, node candy incoming
e-edge node like node . However, nodes like
part embedded DAG. d-edge emanating root
forces us choose node like node be. Thus, see reason
correctness local: two incoming e-edges leaf node candies cannot
embedded DAG rule applied root tree. turn
theorem proof scheme:
Theorem 1 Applying rule compact forest results compact forest.
Proof scheme prove applying rule compact forest creates cycle
embedded DAG tree, cycle non-tree DAG already existed
prior rule application. contradicts assumption original structure
compact forest. crucial observation proof directed path
node u node v passes SR , u v outside SR , also
analogous path u v passes SL instead.
next theorem main result. argue inference process compact
forest complete sound, is, generates exactly set consequents derivable
text according inference formalism.
Theorem 2 Given rule base R set initial trees , tree represented
compact forest derivable inference process consequent according
inference formalism.
Proof scheme first show completeness induction number explicit rule
applications. Let tn+1 tree derived tree tn using rule rn according
inference formalism. inductive assumption determines tn embedded
derivable compact forest F. easy verify applying rn F yield compact
forest F 0 tn+1 embedded.
Next, show soundness induction number rule applications
compact forest. Let tn+1 tree represented derived compact forest Fn+1 (tn+1
(F n+1 )). Fn+1 derived compact forest Fn , using rule rn . inductive
assertion states trees (F n ) consequents according formalism.
Hence, tn+1 already (F n ) consequent . Otherwise, shown
exists tree tn (F n ) applying rn tn yield tn+1 according
formalism. tn consequent according inductive assertion therefore
tn+1 consequent well.
two theorems guarantee compact inference process valid, is,
yields compact forest represents exactly set consequents derivable given
text given rule set.
27

fiBar-Haim, Dagan & Berant

5.4 Complexity
section, explain compact inference exponentially reduces time space
complexity typical scenarios.
consider set rule matches tree independent matched left-handsides (excluding dual-leaf variables) overlap , application
chained order. example, three rule matches presented Figure 7
independent.
Let us consider explicit inference first. Assume start single tree k
independent rules matched. Applying k rules yield 2k trees, since subset
rules might applied . Therefore, time space complexity applying k
independent rule matches (2k ). Applying rules newly derived consequents
behaves similar manner.
Next, examine compact inference. Applying rule using compact inference adds
right-hand-side rule shares existing d-edges. Since size
right-hand-side number outgoing d-edges per node practically bounded
low constants, applying k rules tree yields linear increase size forest.
Thus, resulting size O(|T | + k), see Figure 7.
time complexity rule application composed matching rule forest
applying matched rule. Applying matched rule linear size. Matching
rule size r forest F takes O(|F|r ) time even performing exhaustive
search matches forest. Since r tends quite small bounded
low constant9 , already gives polynomial time complexity. Furthermore, matches
constructed incrementally, step aim extend partial matches found.
Due typical low connectivity forest, well various constraints imposed
rule (lemma, POS, dependency relation), number candidates extending
matches step << |F|, candidates retrieved efficiently using
proper indexing. Thus, matching procedure fast practice, illustrated
empirical evaluation described Section 7.2.
5.5 Related Work Packed Representations
Packed representations various NLP tasks share common principles, also underlie
compact forest: factoring common substructures representing choice local
disjunctions. Applying general scheme individual problems typically requires specific representations algorithms, depending type alternatives
represented specified operations creating them. create alternatives rule
application, newly derived subtree set alternative existing subtrees.
Alternatives specified locally using d-edges.
Packed chart representations parse forests introduced classical parsing algorithms CYK Earley (Jurafsky & Martin, 2008), extended later
work various purposes (Maxwell III & Kaplan, 1991; Kay, 1996). Alternatives
parse chart stem syntactic ambiguities, specified locally possible decompositions phrase sub-phrases.
9. RTE system, average rule LHS size found 2 nodes, maximal size 7
nodes, experimental setting described Section 7.2.2, applied RTE3 test set.

28

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Packed representations also utilized transfer-based machine translation.
Emele Dorna (1998) translated packed source language representation packed target
language representation avoiding unnecessary unpacking transfer. Unlike
rule application, work transfer rules preserve ambiguity stemming source
language, rather generating new alternatives. Mi et al. (2008) applied statistical
machine translation source language parse forest, rather 1-best parse.
transfer rules tree-to-string, contrary tree-to-tree rules, chaining
attempted (rules applied single top-down pass source forest). Thus,
representation algorithms quite different ours.

6. Incorporated Knowledge Bases
section, describe various knowledge bases used inference engine.
first describe novel rule base addressing generic linguistic structures. rule base
composed manually, based formalism, includes inference rules (Section 6.1)
polarity annotation rules (Section 6.2). addition, derived inference rules
several large scale semantic resources (Section 6.3). Overall, variety illustrates
suitability formalism representing diverse types inference knowledge.
6.1 Inference Rules Generic Linguistic Phenomena
rules capture inferences associated common syntactic structures,
summarized Table 2. rules three major functions:
1. Simplification canonization source tree (categories 6 7 Table 2).
2. Extracting embedded propositions (categories 1, 2, 3).
3. Inferring propositions non-propositional subtrees source tree (category 4).
Inference rules merely extract subtree source tree without changing
structure (such relative clause rule) useful exact inference aims generate
hypothesis, used evaluation inferences (cf. Section 7.1). However, currently implemented approximate classification features focused matching
substructures hypothesis forest (as described Section 7.3), hence
take advantage extractions. Therefore, rules excluded rest
experiments, reported Sections 7.27.3.
rules categories 1-7 depend solely syntactic structure closed-class words,
referred generic rules. contrast, verb complement extraction rules (category
8) considered lexicalized rules, since specific certain verbs: replace forced
advised example, entailment would hold. extracted PARC
polarity lexicon (Nairn et al., 2006) list verbs allow inference appearing
positive polarity contexts, generated inference rules verbs. list
complemented reporting verbs, say announce, since information
news domain, rules applied experiments (cf. Section 7.1)
often given reported speech, speaker usually considered reliable.
sidestep issue polarity propagation applying rules main
clause, implemented including tree root node rule LHS.
29

fiBar-Haim, Dagan & Berant

#
1

Category
Conjunctions

2

Clausal extraction
connectives
Relative
clauses

3

4

Appositives

5

Determiner
Canonization

6

Passive

7

Genitive
modifier

8

Verb complement clause
extraction

Example: source
Helenas experienced
played long time
tour.
celebrations muted
many Iranians observed
Shiite mourning month.
assailants fired six bullets car, carried
Vladimir Skobtsov.
Frank Robinson, onetime manager Indians, distinction
NL.
plaintiffs filed lawsuit last year U.S. District
Court Miami.
approached
investment banker.
Malaysias crude palm oil
output estimated
risen six percent.
Yadav forced resign.

Example: derived
Helena played long
time tour.
Many Iranians observed
Shiite mourning month.
car carried Vladimir
Skobtsov.
Frank Robinson onetime manager Indians.

plaintiffs filed lawsuit last year U.S. District
Court Miami.
investment banker approached us.
crude palm oil output Malaysia estimated
risen six percent.
Yadav resigned.

Table 2: Inference rules generic linguistic structures

embedded clause extracted, becomes main clause derived tree, rules
extract embedded clauses. polarity verb detected applying
annotation rules, described next. verb annotated negative unknown
polarity, matching complement extraction rules fails. example, last sentence
Table 2 Yadav forced resign, forced would annotated negative
polarity, consequently matching corresponding complement extraction rule
would fail, Yadav resigned would entailed. Hence, annotation rules may block
erroneous inference rule applications. polarity important correct application
rules, case rule types, passive-to-active transformation.
therefore checked polarity matching rule application exact inference
experiment (Section 7.1), verb complement extraction rules used. leave
analysis polarity-dependence rules future work.
6.2 Polarity Annotation Rules
use annotation rules mark negative unknown polarity predicates (cf. Section 4.5). Table 3 summarizes polarity-inducing contexts address. Like inference rules, annotation rules also comprise generic rules (categories 1-4) lexicalized
30

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

#
1

Category
Explicit Negation

2
3
4

Implied Negation
Modal Auxiliaries
Overt Conditionals

5
6
7

Verb complements
Adjectives
Adverbs

Example
weve never seen[] actual costs come
down.
one stayed[] last lecture.
could eat[?] whale now!
Venus wins[?] game, meet[?] Sarena
finals.
pretend know[] calculus.
impossible survived[] fall.
probably danced[?] night.

Table 3: Polarity annotation rules

rules (categories 5-7). verb complement embedded clause negative unknown
polarity, extracted, however, polarity annotated (category 5; compare
category 8 Table 2). list verbs imply negative/unknown polarity
clausal complements taken PARC lexicon, well VerbNet (Kipper,
2005).
6.3 Lexical Lexical-Syntactic Rules
addition manually-composed generic rules, system integrates inference knowledge variety large-scale semantic resources, introduced Section 2.3. information derived resources represented uniformly inference rules
formalism. examples rules shown Table 1. following resources
used:
WordNet: extracted WordNet (Fellbaum, 1998) lexical rules based synonym, hyponym (a word entailed hyponym, e.g., dog animal ), instance
hyponym 10 derivation relations.
Wikipedia: used lexical rulebase Shnarch et al. (2009), extracted rules
Janis Joplin singer Wikipedia based metadata (e.g.,
links redirects) text definitions, using patterns X .11
DIRT: DIRT algorithm (Lin & Pantel, 2001) learns corpus inference rules
binary predicates, example, X fond YX likes Y. used
version learns canonical rule forms (Szpektor & Dagan, 2007).
Argument-Mapped WordNet (AmWN): resource inference rules predicates, covering verbal nominal forms (Szpektor & Dagan, 2009), includ10. According WordNet glossary, instance proper noun refers particular, unique
referent (as distinguished nouns refer classes). specific form hyponym.
example, Ganges instance river.
11. addition extraction methods described Shnarch et al. (2009), employed two additional
methods. First, extraction entailments among terms redirected page. Second,
generalization rules RHS common LHS head, different modifiers. instance,
rules Ferrari F430 car Ferrari Ascari car generalized Ferrari car .

31

fiBar-Haim, Dagan & Berant

ing argument mapping. based WordNet NomLex-plus (Meyers
et al., 2004), verified statistically intersection unary-DIRT algorithm (Szpektor & Dagan, 2008). AmWN rules defined unary templates,
example, kill XX die
automatically-extracted inference rules lack two attributes defined formalism: rule type (substitution/introduction) explicit alignments (beyond alignments
Rs variables L counterparts, defined default). attributes added automatically using following heuristics:
1. roots L R part-of-speech, substitution rule
(e.g., X buy sold X ). Otherwise (e.g., Ys acquisition X
sold X ), introduction rule.
2. roots L R assumed aligned.
Note application rules, (e.g., WordNet derivations
rules learned DIRT), result valid parse tree. rules
used aiming exact derivation h t. However, may useful
inference engine used together approximate matching component,
RTE system. approximate matcher (described Section 7.3) employs features
coverage words subtrees h F, therefore benefit
inferences. rules preferably applied last step inference
process, avoid cascading errors.

7. Evaluation
section, present empirical evaluation entailment system whole,
well evaluation individual components. evaluate quality systems
output (in terms accuracy, precision, recall) computational efficiency (in terms
running time space, using various application settings.
first evaluate knowledge-based inference engine. Section 7.1, describe
experiment engine aims prove simple template hypotheses, representing
binary predicates, texts sampled large corpus. Next, Section 7.2 evaluate
efficiency engine implementation using compact forest data structure.
evaluate complete entailment system, including approximate entailment classifier
(Section 7.3). Finally, Sections 7.47.5 provide in-depth analysis performance
inference component RTE data.
7.1 Proof System Evaluation
experiment, evaluate inference engine finding strict proofs. is,
inference process must derive precisely target hypothesis (or instantiation
it, case template hypotheses, contain variables defined Section 4.7).
Thus, evaluate precision text-hypothesis pairs complete proof
chain found, using available rules. note PASCAL RTE datasets
suitable purpose. rather small datasets include many text-hypothesis pairs
32

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

available inference rules would suffice deriving complete proofs. Furthermore,
since focus research applied textual inference, inference engine
evaluated NLP application setting texts represent realistic distribution
linguistic phenomena. Manually-composed benchmarks FraCas test suite
(Cooper et al., 1996), contains synthetic examples specific semantic phenomena,
clearly suitable evaluation.
alternative, chose Relation Extraction (RE) setting, complete
proofs achieved large number corpus sentences. setting, system
needs identify pairs arguments sentences target semantic relation (e.g., X buy
).
7.1.1 System Configuration
experiment, first reported Bar-Haim et al. (2007), used earlier
version engine rule bases. engine experiment make use
compact forest, rather generates consequent explicitly. Polarity annotations
propagated source derived trees. Instead, polarity annotation rules
applied original text t, inferred consequent, prior application
inference rule. following rule bases used experiment:
Generic Linguistic Rules used generic rule base presented Section 6, including inference polarity annotation rules. early version include
lexicalized polarity rules derived VerbNet PARC lexicon (category 5
Table 3).
Lexical-Syntactic Rules Nominalization rules: inference rules Xs acquisition
X acquired capture relations verbs nominalizations.
rules derived automatically (Ron, 2006) Nomlex, hand-coded database
English nominalizations (Macleod, Grishman, Meyers, Barrett, & Reeves, 1998),
WordNet.
Automatically Learned Rules: used DIRT paraphrase collection, well
output TEASE (Szpektor et al., 2004), another unsupervised algorithm learning
lexical-syntactic rules. TEASE acquires entailment relations Web given
input template identifying characteristic variable instantiations shared
templates. algorithms provide ranked list output templates given input
template. learned rules linguistic paraphrases, (e.g., X confirm X
approve ), others capture world knowledge, (e.g., X buy X ).
algorithms learn entailment direction rule, reduces accuracy
applied given direction. system, considered top 15 bi-directional
rules learned template.
Generic Default Rules rules used define default behavior, situations
case-by-case rules available. used one default rule allows removal
modifiers nodes. Ideally, rule would replaced future work
specific rules removing modifiers.
33

fiBar-Haim, Dagan & Berant

7.1.2 Evaluation Process
use sample test template hypotheses correspond typical relations,
X approve Y. identify large test corpus, sentences instantiation
test hypothesis proved. example, sentence budget approved
parliament found prove instantiated hypothesis parliament approve budget
(via passive-to-active inference rule). Finally, sample candidate sentenceshypothesis pairs judged manually true entailment. repeated process compare
different system configurations.
Since publicly available sample output TEASE much smaller
resources12 randomly selected resource 9 transitive verbs may correspond
typical predicates13 . formed test templates adding subject object varisubj

able nodes. example, verb accuse constructed template XNOUN
obj

accuse VERB YNOUN .
test template h identify sentences corpus template
proved system. efficiently find proof chains generate h corpus
sentences combine forward backward (Breadth-First) searches available
rules. First, use backward search lexical-syntactic rules, starting rules
whose right-hand-side identical test template. process backward chaining
DIRT/TEASE nominalization rules generates set templates ti ,
proving (deriving) h. example, hypothesis X approve may generate
template X confirm Y, backward application DIRT/TEASE rule,
generate template confirmation X, nominalization rule.
Since templates ti generated lexical-syntactic rules, modify open-class
lexical items, may considered lexical expansions h.
Next, specific ti generate search engine query composed open-class
words ti . query fetches candidate sentences corpus, ti might
proven using generic linguistic rules (recall rules modify openclass words). end, use forward search applies generic rules, starting
candidate sentence trying derive ti sequence rule applications.
successful, variables ti instantiated (cf. Section 4.7). Consequently, know
variable instantiations, h proven (since derives ti turn
derives h).
performed search sentences prove test template
Reuters RCV1 corpus, CD#2, applying Minipar parsing. random sampling,
obtained 30 sentences prove (according tested system configuration)
9 test templates, yielding total 270 pairs sentence, instantiated hypothesis, four tested configurations, described (1080 pairs overall).
pairs split entailment judgment two human annotators (graduate students
Bar-Ilan NLP group). annotators achieved, sample 100 shared exam12. output TEASE DIRT, well many knowledge resources, available RTE
knowledge resources page:
http://aclweb.org/aclwiki/index.php?title=RTE_Knowledge_Resources
13. verbs approach, approve, consult, lead, observe, play, seek, sign, strike.

34

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

#
1
2
3
4

Configuration
Baseline (embed h anywhere s)
Proof (embed h root s)
Proof + Generic
Proof + Generic + Lexical-Syntactic

Precision
67.0%
78.5%
74.8%
23.6%

Yield
2,414
1,426
2,967
18,809

Table 4: Proof system evaluation

ples, agreement level 87%, Kappa value 0.71 (corresponding substantial
agreement).
7.1.3 Results
tested four configurations proof system:
1. Baseline: baseline configuration follows prominent approach graph-based
entailment systems: system tries embed given hypothesis anywhere
candidate sentence tree s, negative unknown polarity (detected
annotation rules) may block embedding.
2. Proof: configuration h strictly generated candidate sentence s. inference rule available default rule removing modifiers
(polarity annotation rules active Baseline). configuration equivalent
embedding h root h matched root s, since modifiers
part match removed default rule. However,
h embedded elsewhere extracted, opposed Baseline
configuration.
3. Proof + Generic: Proof, plus generic linguistic rules.
4. Proof + Generic + Lexical-Syntactic: previous configuration, plus
lexical-syntactic rules.
system configuration measure precision, percentage examples judged
correct (entailing), average extrapolated yield, expected number
truly entailing sentences corpus would proven system.
extrapolated yield specific template calculated number sample sentences
judged entailing, multiplied sampling proportion. average calculated
test templates. note that, similar IR evaluations, possible compute
true recall setting since total number entailing sentences corpus
known (recall equal yield divided total). However, straightforward
measure relative recall differences among different configurations based yield. Thus,
using two measures estimated large corpus possible conduct robust
comparison different configurations, reliably estimate impact different
rule types. analysis possible RTE datasets, rather small,
hand-picked examples represent actual distribution linguistic phenomena.
35

fiBar-Haim, Dagan & Berant

results reported Table 4. First, comparing results Proof
results Baseline, observe requirement matching h root (i.e.,
main clause s), rather allowing matched anywhere s, improves
precision considerably baseline (by 11.5%), reducing yield nearly 40%.
Proof configuration avoids errors resulting improper extraction embedded
clauses.
Remarkably, using generic inference rules, system able gain back lost
yield Proof surpass yield baseline configuration. addition,
obtain higher precision baseline (a 7.8% difference), statistically
significant p < 0.05 level, using z test proportions. demonstrates
principled proof approach appears superior heuristic baseline embedding
approach, exemplifies contribution generic rule base. Overall, generic rules
used 46% proofs.
Adding lexical-syntactic rules increased yield factor six. shows
importance acquiring lexical-syntactic variability patterns. However, precision
DIRT TEASE currently quite low, causing overall low precision. Manual filtering
rules learned systems currently required obtain reasonable precision.
Error analysis revealed third configuration Proof + Generic rules,
significant 65% errors due parsing errors, notably incorrect dependency
relation assignment, incorrect POS assignment, incorrect argument selection, incorrect analysis complex verbs (e.g., play text vs. play hypothesis) ungrammatical sentence fragments. Another 30% errors represent conditionals, negation,
modality phenomena, could handled additional rules, making use elaborate syntactic information verb tense. remaining,
rather small, 5% errors represent truly ambiguous sentences would require
considerable world knowledge successful analysis.
7.2 Compact Forest Efficiency Evaluation
Next, evaluate efficiency compact inference (cf. Section 5) setting recognizing textual entailment, using RTE-3 RTE-4 datasets (Giampiccolo et al., 2007,
2008). datasets consist (text, hypothesis) pairs, need classified
entailing/non entailing. first experiment, using generic inference rule set, shows
compact inference outperforms explicit inference (efficiency-wise) orders magnitude (Section 7.2.1). second experiment shows compact inference scales well
full-blown RTE setting several large-scale rule bases, hundreds rules
applied per text (Section 7.2.2).
7.2.1 Compact vs. Explicit Inference
compare explicit compact inference randomly sampled 100 pairs RTE-3
development set, parsed text pair using Minipar (Lin, 1998). avoid
memory overflow explicit inference, applied sentences subset
generic inference rules described Section 6.1. fair comparison, aimed make
explicit inference implementation reasonably efficient, example preventing multiple
generations tree different permutations rule applications.
36

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Time (msec)
Rule applications
Node count
Edge endpoints

Compact
61
12
69
141

Explicit
24,184
123
5,901
11,552

Ratio
396
10
86
82

Table 5: Compact vs. explicit inference, using generic rules. Results averaged per
text-hypothesis pair.

configurations perform rule application iteratively, new matches found.
iteration, first find rule matches apply matching rules. compare run
time, number rule applications, overall generated size nodes edges,
edge size represented sum endpoints (2 regular edge, |Sd | + |Td |
d-edge).
results summarized Table 5. expected, results show compact
inference orders magnitude efficient explicit inference. avoid memory
overflow, inference terminated reaching 100,000 nodes. Three 100 test
texts reached limit explicit inference, maximal node count compact
inference 268. number rule applications reduced due sharing
common subtrees compact forest, single rule application operates
simultaneously large number embedded trees. results suggest scaling
larger rule bases longer inference chains would feasible compact inference,
prohibitive explicit inference.
7.2.2 Application RTE System
goal second experiment test compact inference scales well broad
inference rule bases. experiment used Bar-Ilan RTE system (Bar-Haim et al.,
2008). system operates two primary stages:
Inference: inference rules first applied initial compact forest F, aiming bring
closer hypothesis h. experiment, use knowledge bases
described Section 6. Overall, rule bases contain millions rules.
current system implemented simple search strategy, spirit
(de Salvo Braz et al., 2005): first, applied three exhaustive iterations generic
rules. Since rules low fan-out (few possible right-hand-sides given
left-hand-side), affordable apply chain freely. iteration
first find rule matches, apply matched rules. avoid repeated
identical rule applications, mark newly added nodes iteration,
next iteration consider matches containing new nodes. perform single
iteration lexical lexical-syntactic rules, applying L
part matched F R part matched h. investigation
effective search heuristics representation left future research.
Classification: Following inference, set features extracted resulting F
h fed SVM classifier, determines entailment. describe
37

fiBar-Haim, Dagan & Berant

Rule applications
Node count
Edge endpoints

RTE3-Dev
Avg. Max.
14
275
71
606
155 1,741

RTE4
Avg. Max.
15
110
80
357
173 1,062

Table 6: Application compact inference RTE-3 Dev. RTE-4 datasets, using
rule types

classification stage detail next section, discusses performance
RTE system.
Table 6 provides statistics rule applications using rule bases, RTE-3
development set RTE-4 dataset14 . Overall, primary result compact
forest indeed accommodates well extensive rule applications large-scale rule bases.
resulting forest size kept small, even maximal cases causing memory
overflow explicit inference.
7.3 Complete RTE System Evaluation
previous sections, evaluated knowledge-based inference engine (the proof system) respect quality output (precision, recall) well computational
efficiency (time, space). evaluate complete RTE system, combines
inference engine approximate classification module.
classification setting features quite typical RTE literature. Features broadly categorized two subsets: (a) lexical features solely depend
lexical items F h, (b) lexical-syntactic features also take account
syntactic structures dependency relations F h. brief description
features. complete description appears RTE system report (Bar-Haim et al.,
2008).
Lexical features: Coverage features check words h present (covered) F.
assume high degree lexical coverage correlates entailment.
features measure proportion uncovered content words, verbs, nouns, adjectives
adverbs, named entities numbers. Polarity mismatch features detect cases
nouns verbs h matched F incompatible polarity.
features assumed indicate non-entailment.
Edge coverage features: say edge h matched F edge
F matching relation, source node target node. say edge h
loosely-matched path F matching source node matching
target node. Based definitions extract two features: proportion h
edges matched/loosely matched F.15
14. Running time included since dedicated rule fetching, rather slow
available implementation resources. elapsed time seconds per (t, h) pair.
15. look subset edges labeled relevant dependency relations.

38

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Predicate-argument features: F entails h, predicates h matched
F along arguments. Predicates include verbs (except verb be)
subject complements copular sentences, example, smart Joseph smart.
Arguments daughters predicate node h.16 Four features computed
F, h pair. categorize every predicate h match F one
four possible categories:
1. complete match - matching predicate exists F matching arguments
dependency relations.
2. partial match - matching predicate exists F matching arguments
dependency relations.
3. opposite match - matching predicate exists F matching arguments
incorrect dependency relations.
4. match - matching predicate F matching arguments.
predicate categorized complete match category.
Finally, compute four features F, h pair: proportion predicates
h complete match F, three binary features, checking
predicate h categorized partial match/opposite match/no match. Since
subject object arguments crucial textual entailment, compute four
similar features subset predicates arguments (ignoring
arguments).
global lexical-syntactic feature: feature measures well subtrees h
covered F, weighted according proximity root h. feature
somewhat similar dependency tree kernel Collins Duffy (2001),
measures similarity two dependency trees counting common
subtrees. However, measure several distinct properties makes suitable
needs: (a) directional measure, estimating coverage h F,
vice versa (b) operates compact forest tree, rather pair
trees. (c) takes account distance root h, assuming nodes
closer root important.
system trained RTE-3 development set, tested RTE3
RTE-4 test sets (no development set released RTE-4). Co-reference substitution
disabled due insufficient accuracy co-reference resolution tool used.
first report overall performance, provide analysis inference module,
focus work.
accuracies obtained experiment shown Table 7 (under inference
column). results RTE-3 quite competitive: compared 66.4%, 3 teams
26 participated RTE-3 scored higher 67%, three systems
scored 66% 67%. results RTE4 rank 9-10 26, 6 teams
scoring higher 1%. Overall, results show system well-situated
state art RTE task.
Table 8 provides detailed view systems performance. Precision, recall,
F1 results given entailing non-entailing pairs, well overall accuracy.
16. dependent preposition clause take complement preposition head
clause respectively dependent.

39

fiBar-Haim, Dagan & Berant

table also shows results per task (IE, IR, QA SUM). Overall, system tends
predict entailment often non-entailment. recall entailing pairs much
higher recall non-entailing pairs, precision non-entailing pairs
much higher entailing pairs. Performance varies considerably among different tasks.
RTE3 accuracy results QA IR considerably higher average results
achieved RTE3 submissions, reported organizers (Giampiccolo et al., 2007)
(0.71 0.66, respectively), IE SUM, results bit average
(0.52 0.58). RTE4 results better IR SUM, seem easier
tasks RTE4 (Giampiccolo et al., 2008).17
7.4 Usage Contribution Knowledge Bases
evaluate accuracy gain knowledge-based inference, ran system
inference module disabled, entailment classification applied directly initial
parse tree text. results shown inference column Table 7.
Comparing results full system accuracy (inference), see applying
inference module resulted higher accuracy test sets. contribution
prominent RTE-4 dataset. results illustrate typical contribution current
knowledge sources current RTE systems. contribution likely increase
current near future research, topics extending improving knowledge
resources, applying semantically suitable contexts, improved classification
features, broader search strategies.
Tables 9 10 illustrate usage contribution individual rule bases. Table 9
shows distribution rule applications various rule bases. Table 10 presents
ablation study showing marginal accuracy gain rule base. results show
rule bases applicable large portion pairs, contributes
overall accuracy. note results highly dependent search
strategy. instance, chaining lexical rules expected increase number lexical
rule applications, reduce accuracy. provide detailed analysis rule
applications system next section.
7.5 Manual Analysis
conclude evaluation two manual analyses inference component within
RTE system. first analysis (Subsection 7.5.1) assesses applicability inference
framework RTE task well actual coverage current system. also
categorizes cases formalism falls short. (Subsection 7.5.2) assess
correctness applied rules, analyze various causes incorrect applications.
analyses done one authors randomly sampled subsets RTE-3
test set.

17. According RTE4 organizers, IE task appeared difficult task, SUM
IR seemed easier tasks. However, report average accuracy per task.

40

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Test set
RTE3
RTE4

Accuracy
inference Inference
64.6%
66.4%
57.5%
60.6%


1.8 %
*3.1%

Lexical
Overlap
62.4%
56.6%

Best RTE
Result
80.0%
74.6%

Table 7: Inference contribution RTE performance. system trained RTE3 development set. * indicates statistically significant difference (at level p < 0.02, using
McNemars test). best results achieved RTE3 RTE4 challenges (Hickl &
Bensley, 2007; Bensley & Hickl, 2008), well lexical overlap baseline results (Mehdad
& Magnini, 2009a), also given reference. Mehdad Magnini tested eight
configurations lexical overlap baselines, chose one performs best average
RTE1-4 test sets.

RTE3

RTE4

Task
IE
IR
QA
SUM

IE
IR
QA
SUM


Non-Entailing Pairs
Precision Recall
F1
0.500
0.095 0.159
0.764
0.743 0.753
0.822
0.787 0.804
0.545
0.341 0.420
0.722
0.505 0.594
0.596
0.187 0.284
0.721
0.587 0.647
0.636
0.210 0.316
0.685
0.630 0.656
0.680
0.400 0.504

Entailing Pairs
Precision Recall
F1
0.527
0.914 0.669
0.678
0.701 0.689
0.818
0.849 0.833
0.600
0.777 0.677
0.634
0.815 0.713
0.518
0.873 0.650
0.652
0.773 0.707
0.527
0.880 0.659
0.657
0.710 0.683
0.575
0.812 0.673

Accuracy
0.525
0.725
0.820
0.585
0.664
0.530
0.680
0.545
0.670
0.606

Table 8: RTE results breakdown task pair type

Rule base
WordNet
AmWN
Wikipedia
DIRT
Generic
Polarity

RTE3-Dev
Rules App
0.6
1.2
0.3
0.4
0.6
1.7
0.5
0.7
4.7 10.4
0.2
0.2

RTE4
Rules App
0.6
1.1
0.3
0.4
0.6
1.3
0.5
1.0
5.4 11.5
0.2
0.2

Table 9: Average number rule applications per (t, h) pair, rule base. App counts
rule application, Rules ignores multiple matches rule
iteration.

7.5.1 Applicability Coverage
analysis assesses ability inference framework derive complete proofs
RTE (t,h) pairs idealized setting perfect knowledge bases co-reference
resolution available. provides upper bound coverage inference
41

fiBar-Haim, Dagan & Berant

Rule base
WordNet
AmWN
Wikipedia
DIRT
Generic
Polarity

Accuracy (RTE4)
0.8%
0.7%
1.0%
0.9%
0.4%
0.9%

Table 10: Contribution various rule bases. Results show accuracy loss RTE-4, obtained
removing rule base (ablation tests).

engine. similar analysis previously done Bar-Haim, Szpektor, Glickman
(2005) subset RTE-1 dataset. However, go (a) assess
actual coverage required inferences implemented RTE system, (b) present
classification uncovered cases different categories.
carried analysis follows: 80 positive (entailing) pairs randomly
sampled RTE-3 test set. pair aimed manually derive proof
comprising inference steps expressible formalism, similar example
Section 2.2. complete proof could derived, pair classified inferable.
Otherwise, classified one following categories:
Discourse references: Complete proof requires incorporating pieces information
discourse, including event co-reference bridging (Mirkin et al., 2010). Nominal co-reference substitution included, covered formalism.
instance, text Titanics sinking hitting iceberg April 14,
1912. . . , year 1912 explicitly specified time Titanics sinking,
relation derived discourse order infer hypothesis
Titanic sank 1912.
Non-decomposable: inference cannot reasonably decomposed sequence
local rewrites. case, example, text black plague lasted
four years killed one-third population Europe, approximately 20
million people hypothesis Black plague swept Europe.
Other: cases fall categories.
distribution categories shown Table 11. found 60%
pairs could proven formalism given appropriate inference rules co-reference
information, demonstrates utility approach. results somewhat
higher 50% reported Bar-Haim et al. (2005), may attributed
fact RTE1 considered difficult dataset, entailment systems consistently
perform better RTE3.
remaining 40% pairs, analysis highlights significance discourse
references, occur 16.3% pairs. previous analysis discourse references
textual entailment applied RTE-5 search task, text sentences
interpreted context full discourse (Mirkin et al., 2010), analysis shows
42

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Category
Inferable
Non-decomposable
Discourse references


Count
48
14
13
5

%
60.0%
17.5%
16.3%
6.3%

Table 11: Applicability inference framework RTE task. 80 randomly selected
entailing pairs RTE-3 test set analyzed.

significance discourse references even short, self-contained texts, RTE3 composed. Mirkin et al. show framework, similar methods based
tree transformations, extended utilize discourse references. Several works
last years targeted implied predicate-argument relationships, notable
SemEval-2010 Task Linking Events Participants Discourse
(Ruppenhofer, Sporleder, Morante, Baker, & Palmer, 2009). particular, Stern Dagan
(2014) recently showed identifying relations improves performance
RTE system. Finally, entailment 17.5% pairs could established
sequence local rewrites, thus cases likely require deeper methods semantic
analysis inference.
manually-derived proofs 48 inferable pairs included total 79 rule applications, average 1.65 rule applications per pair.18 maximal number rules per
pair 3. 28 rules (35.4%) applied system. 21% proofs
inferable pairs fully derived RTE system. Partial proofs derived
additional 25% pairs. remaining 54% pairs, system apply
rules manual proof. results demonstrate utility inference
mechanisms rule bases system, hand suggest still
much room improvement coverage existing rule bases.
7.5.2 Correctness Applied Rules
next assess correctness rules applied inference engine. focus
four lexical lexical-syntactic rule bases described Section 6.3: WordNet, Wikipedia,
DIRT, Argument-Mapped WordNet (AmWN). Except WordNet, rule bases
generated automatically, therefore accuracy issue accuracy
manually-composed generic inference rules polarity annotation rules. Furthermore, lexicalized rules often context sensitive, additional potential source
incorrect rule applications.
evaluation randomly sampled 75 pairs RTE-3 test set, analyzed
lexical lexical-syntactic rule applications performed system pairs,
total 201 rule applications. define two levels rule application correctness:

18. previously mentioned, RTE system apply rules merely extract subtree
given source tree. Accordingly, rules ignored analysis well.

43

fiBar-Haim, Dagan & Berant

Propositional: derived tree resulting rule application grammatical
entailed source tree. level correctness assumed
formalism.
Referential: case propositional correctness hold, turn weaker criterion Referential Correctness, following notion Lexical Reference (Glickman,
Shnarch, & Dagan, 2006; Shnarch et al., 2009), extend case
template-based rules variables. Let rule E : L R inference rule matched
source tree s. Let l r instantiations L R respectively, according
variable matching L s. say referential correctness holds l generates reference possible meaning r. examples rules found
analyzed sample are: popepapal, TurkishTurkey fishermenfishing.
rule applications result valid entailed tree, still useful
context RTE system applies approximate matching (as previously
discussed end Section 6).
Incorrect rule applications classified one following categories:
1. Bad rule: rule a-priori incorrect (e.g., Walesyear ).
2. Bad context: rule incorrect context source sentence. example,
WordNet rule strikecreate corresponds rare sense strike defined
produce ignition blow (as strike fire flint stone).
3. Bad match: rule applied due incorrect matching left-hand-side,
resulting incorrect parse source tree.
results summarized Table 12. Overall, 52.7% rule applications correct.
Interestingly, referential (29.4%) propositional (23.4%) rule applications. Unsurprisingly, accurate knowledge resource manually composed
WordNet (75.9% correct applications), followed AmWN (57.9%) Wikipedia
(57.4%) rule bases, derived automatically human-generated resources.
least accurate resource DIRT (21.4%), makes use human knowledge engineering, rather learned automatically based corpus statistics. accuracy DIRT
considerably lower accuracy resources, substantially decreasing
overall accuracy well. errors DIRT Wikipedia due bad rules.
also overall dominant cause incorrect applications, WordNet
AmWN a-priori rule quality high errors due bad context. Wikipedia rules suffer bad context, explained fact
left-hand-side often unambiguous named entity (Madrid, Antelope Valley
Freeway, Microsoft Office). analysis highlights need improving accuracy
automatically-generated rule bases, whose quality still far human generated resources. analysis also shows context-sensitivity lexicalized rules still issue
even rules applied conservatively experiment (no chaining, L
R matched F h). addressed future research.
44

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

% rule applications
Propositional
Referential
Correct
Bad rule
Bad context
Bad matching
Incorrect

DIRT
27.9%
17.9%
3.6%
21.4%
58.9%
7.1%
12.5%
78.6%

AmWN
9.5%
21.1%
36.8%
57.9%
5.3%
31.6%
5.3%
42.1%

Wikipedia
33.8%
19.1%
38.2%
57.4%
42.6%
0.0%
0.0%
42.6%

WordNet
28.9%
34.5%
41.4%
75.9%
0.0%
17.2%
6.9%
24.1%


100.0%
23.4%
29.4%
52.7%
31.3%
10.0%
6.0%
47.3%

Table 12: Analysis lexical lexical-syntactic rule applications

8. Discussion: Comparison Related Approaches
section, compare work several closely-related inference methods,
described Section 2.3.2.
discourse commitments derived Hickl (2008) quite similar kind consequents generate applying syntactic, lexical-syntactic, co-reference substitution rules. However, work differs Hickls several respects. First foremost,
Hickls work fully describe knowledge representation inference framework,
main focus work. Hickl briefly mentions commitments
generated using probabilistic FST-based extraction framework, explanations examples given paper. Second, framework allows unified modeling
variety inference types addressed various tools components Hickls
system (FST, relation extraction, paraphrase acquisition, etc.). addition, system
operates lexical-syntactic representations, rely semantic parsing. Finally, consequents generated formalism packed efficient data structure,
whereas Hickls commitments generated explicitly discuss commitment
generation efficiency. noted, however, explicit generation commitments restricts search space, may simplify approximate matching (e.g., finding
alignment h given consequent vs. aligning h whole compact forest).
De Salvo Braz et al. (2005) presented semantic inference framework augments
text representation right-hand-side applied rule, respect
similar ours. However, work, rule application semantics
resulting augmented structure fully specified. particular, distinction
individual consequents lost augmented graph. contrast, compact
inference fully formalized proved equivalent expressive, well-defined
formalism operating individual trees, inferred consequent recovered
compact forest.
MacCartney Manning (2009) proposed model natural language inference which,
similar framework, operates directly parse-based representations. work extends previous work natural logic (Valencia, 1991), focused semantic containment monotonicity, incorporating semantic exclusion implicativity. model
inference h sequence atomic edits; thought generating
intermediate premise. calculus computes semantic relation source
45

fiBar-Haim, Dagan & Berant

derived premise propagating semantic relation local edit upward
parse tree according properties intermediate nodes. example,
correctly infer first-year students arrived students arrived ,
Every first-year student arrived Every student arrived . composition semantic relations along inference chain yields semantic relation holding
h. contribution complementary ours. approaches, inference
h modeled sequence atomic steps (rule applications edits). focus
framework representation application diverse types transformations
needed textual inference, well efficient representation possible inference chains.
Application inference rule assumed always generate entailed consequent,
polarity rules may used detect situations assumption hold
block rule application. comparison, formalism MacCartney Manning assumes
rather simple edit operations, focused precise predication semantic relation
h given sequence edits transform h. Thus, combining
two complementary approaches natural direction future research.

9. Conclusion
subject work representation use semantic knowledge textual
inference lexical-syntactic level. defined novel inference framework parse
trees, represents diverse semantic knowledge inference rules. proof process
aims transform source text target hypothesis sequence rule
applications, generating intermediate parse tree. complementary contribution
work novel data structure associated rule application algorithm,
proved valid implementation inference formalism. illustrated inference
efficiency analytically empirically.
approach several advantageous properties. First, ability represent
apply wide variety inferences combine rule chaining makes framework expressive previous RTE architectures. Second, expressive
power obtained well-formalized compact framework, based unified knowledge
representation inference mechanisms. Finally, shown RTE experiments,
compact forest data structure allows approach scale well practical settings
involve large rule bases hundreds rule applications per text-hypothesis pair.
demonstrated utility approach two different semantic tasks. Experiments unsupervised relation extraction showed exact proofs outperform
heuristic common practice hypothesis embedding. also achieved competitive
results RTE benchmarks, adding simple approximate matching module
inference engine. contribution semantic knowledge illustrated tasks.
Limitations possible extensions formalism discussed Section 4.8.
Manual analysis inference engines performance relation extraction RTE
tasks suggested promising directions future research, discussed Subsections
7.1.3 7.5. Two additional major areas research approximate matching
heuristics proof search strategy. Stern Dagan (2011) Stern, Stern, Dagan,
Felner (2012) extended work address two aspects, respectively.
46

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Acknowledgments
article based doctoral dissertation first author, completed
guidance second author Bar-Ilan University (Bar-Haim, 2010).
work partially supported Israel Science Foundation grants 1095/05 1112/08,
IST Programme European Community PASCAL Network Excellence IST-2002-506778, PASCAL-2 Network Excellence European Community
FP7-ICT-2007-1-216886, Israel Internet Association (ISOC-IL), grant 9022,
FBK-irst/Bar-Ilan University collaboration. third author grateful Azrieli
Foundation award Azrieli Fellowship. authors wish thank Cleo Condoravdi making polarity lexicon developed PARC available research.
grateful Eyal Shnarch help implementing experimental setup described
Section 7.1. also thank Iddo Greental collaboration developing generic rule
base. Finally, would like thank Dan Roth, Idan Szpektor, Yonatan Aumann, Marco
Pennacchiotti, Marc Dymetman anonymous reviewers valuable feedback
work.

Appendix A: Compact Forest Complete Proofs
section, provide complete proofs correctness compact inference
algorithm presented Section 5. start definitions.
Definition Let L R rule matched applied compact forest F. Section 5.2, let l subtree represented tree (F), L matched. Recall
SL defined l excluding nodes matched dual-leaf variables, similarly SR
defined copy R without dual-leaf variables generated inserted
F part rule application. roots SL SR denoted rL rR respectively.
say node SR tied node s0 SL , set source node one
outgoing d-edges s0 , due alignment sharing dual leaf variable sharing.
graph operations performed applying rule L R compact forest F
summarized follows:
1. Adding subtree SR F.
2. Setting rR target node d-edge F.
3. Setting nodes SR tied nodes SL source nodes d-edges F,
according rules variable sharing dual leaf variable sharing. Recall
d-edges part SL .
First, show simple property cDGs generated inference process:
Lemma 1 Every node cDG generated inference process one incoming d-edge.
47

fiBar-Haim, Dagan & Berant

Proof construction, initial forest node one incoming d-edge.
rule application adds subtree SR , whose nodes one incoming d-edge.
Last, root rR , initially incoming edges, set target single
d-edge rule application (the incoming d-edge rL ). Therefore, lemma follows
induction number rule applications.
Using following theorem show inference process generates compact
forest:
Theorem 1 Applying rule compact forest results compact forest.
Proof Let F 0 cDG generated applying rule L R compact forest F.
show F 0 compact forest, is, cDAG single root r
embedded DAGs rooted r trees. First, show F 0 cDAG, (i.e.,
contain cycle e-edges).
Assume contradiction F 0 contains simple cycle e-edges C. Applying
rule L R add e-edges nodes F. Therefore, C must pass
rR , root SR contain e-edge (p, rR ). Since SR tree, C must also leave SR
e-edge (u, v) (u SR v
/ SR ). cycle written p rR ...
u v ... p. Notice path v p fully contained F since cycle
C simple entering SR possible rR .
L R must substitution rule, otherwise p would root F.
impossible, since root incoming d-edges. Therefore, rR rL
single incoming d-edge, e-edge (p, rL ) exists F. addition, u added
source node d-edge F since tied u0 SL , also source node d.
Therefore, path rL ... u0 v exists F. Finally, know path v
p fully contained F, therefore construct cycle p rL ... u0 v ... p
F, contradiction assumption F compact forest.
shown F 0 cDAG. Next, define generalization embedded DAGs,
help us show embedded DAGs F 0 rooted r trees.
Definition embedded partial DAG G = (V, E) cDAG G rooted node v V
similar embedded DAG generated using following process:
1. Initialize G v alone
2. Repeat number iterations:
(a) choose node V
(b) choose outgoing d-edge already chosen previous
iteration. d-edges chosen - halt.
(c) choose target node Td add e-edge (s, t)d G.
show embedded partial DAGs F 0 rooted node trees. Since
embedded DAG also embedded partial DAG, proves embedded DAGs
F 0 rooted r trees. Assume contradiction applying L R
48

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

embedded partial DAG 0 rooted node n tree. assume n
SR , otherwise, extend 0 adding path p rR ... n, p
node outside SR source node incoming edge rR .
Since 0 tree, two simple paths P1 P2 n reach
node z two different e-edges. z cannot SR , since two paths meet
subtree SR , must first meet root rR entering incoming d-edge. However, could
construct F two paths, selecting rL instead rR , contradiction
assumption F compact forest. Clearly, either P1 P2 must pass
new subtree SR , otherwise two paths already existed F.
first handle case where, without loss generality, P1 passes SR
P2 not. P1 passes SR contains e-edge (p, rR ). Since z
/ SR ,
also contains e-edge (u, v) u SR v
/ SR . P1 written
n ... p rR ... u v ... z. paths n p v z
F, way enter SR rR P1 simple.
incrementally construct F following embedded partial DAG : First, construct
P2 section P1 n p 0 . Next, expand p e-edge (p, rL )
instead (p, rR ). would like expand rL reach z possible.
previously explained, u tied node u0 SL therefore e-edge (u0 , v) exists
F. Therefore, path P 0 SL , rL (u0 , v) z. However,
guaranteed whole P 0 added . try expand incrementally
P 0 , step adding next e-edge path. succeed, embedded
graph F two paths z, contradiction. fail, due e-edge
(z 0 , t) P 0 cannot add. Thus, z 0 must already P2 , node
two distinct paths embedded graph , contradiction. path constructed
indeed different P2 since contains e-edge (p, rL ) cannot part P2 , since
P1 contains disjoint edge (p, rR ).
remaining case, P1 P2 pass SR reach node z
/ SR . P1
written n ... u1 v1 ... z P2 n ... u2 v2 ... z,
u1 , u2 SR , v1 , v2
/ SR . Assume first e-edges (u1 , v1 ) (u2 , v2 )
originate d-edge d. u1 6= u2 , otherwise (u1 , v1 ) (u2 , v2 ) could
embedded partial DAG. u1 ,u2 tied nodes u01 , u02 SL .
show u01 6= u02 : Assume contradiction u01 = u02 = u0 . u0 tied u1
u2 due alignment sharing dual leaf variable sharing. u0 cannot tied u1
u2 due alignment sharing since alignment function nodes SL nodes
SR . cannot tied due dual leaf variable sharing, since variable appears
R. Finally, u0 tied u1 (without loss generality) due dual leaf
variable sharing, d-edge part l. Therefore, u2 include
aligned modifier, thus u2 tied u0 due alignment.
construct embedded graph rooted rL F: SL part
match L F, construct embedded graph rooted rL path
node SL , particular paths u01 u02 . Since u01 6= u02 , u01
u02 source nodes d, part SL , expand two paths
e-edges (u01 , v1 ) (u02 , v1 ) get embedded graph Gn tree,
contradiction.
49

fiBar-Haim, Dagan & Berant

Suppose e-edges (u1 , v1 ) (u2 , v2 ) originate different d-edges d1
d2 respectively. u1 u2 tied u01 u02 . Therefore, v1 6= v2 construct
following embedded graph rooted rL : previous case, expand
paths SL rL u01 u02 . Next, add e-edges (u01 , v1 ) d1 (u02 , v2 )
d2 . Recall d1 d2 SL therefore used expansion. try
expand embedded graph include paths v1 v2 z. succeed,
two paths leading z. fail two paths Tn meeting
node z 0 , explained above. Last, v1 = v2 = v, v node F two
incoming d-edges, contradicting Lemma 1.
case introduction rule quite similar simpler. P1 passes SR
P2 not, n must root compact forest (the node path
rR ). However, case n single outgoing d-edge, therefore outgoing
e-edges disjoint (i.e. cannot part embedded DAG). Thus, P2 must also
pass rR - contradiction. P1 P2 pass SR , proof identical
case substitution rule.
shown F 0 cDAG whose embedded DAGs rooted r trees. F 0
also single root new nodes added applying L R incoming
edge. Hence, F 0 compact forest.
Corollary 1 inference process generates compact forest.
Proof easy verify initialization generates compact forest. Since applying
rule compact forest results compact forest, inference process generates
compact forest induction number rule applications.
Theorem 2 Given rule base R set initial trees , tree represented
compact forest derivable inference process consequent according
inference formalism.
Proof () first show completeness induction number rule applications n.
n = 0 one initial trees represented initial compact forest.
Let tn+1 tree derived formalism applying sequence n + 1 rules. show
tn+1 represented derivable compact forest. tn+1 derived applying
rule L R tree tn . According inductive assumption, tn represented
compact forest F derivable inference process. Therefore, rule L R
matched applied F. assume L R substitution rule since case
introduction rule similar. tn+1 almost identical tn except contains subtree
R instead L instantiated variables aligned modifiers. easy verify
application L R F resulting F 0 , F 0 contain embedded tree
almost identical tn , except root SR , rR , chosen instead root
SL , rL , rest SR also chosen appropriate instantiated variables
modifiers. Therefore, tn+1 = contained F 0 required. guaranteed
tree according Corollary 1.
() Next, prove soundness induction number rule applications
forest. initialization, initial trees consequents. Let Fn+1 compact
forest derived n + 1 rule applications (Corollary 1 guarantees Fn+1 indeed
50

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

compact forest). Given tree tn+1 represented Fn+1 , show tn+1 consequent
formalism.
tn+1 already represented compact forest n rule applications,
according assumption induction consequent formalism. not,
tn+1 new embedded tree created application rule L R. Therefore,
tn+1 contains entire subtree SR . incrementally construct embedded tree tn
represented Fn tn+1 result applying L R tn .
substitution rule, first construct part tn+1 include
subtree rooted rR . introduction rule, take path forests root
rL . Next, construct SL rL instead SR rR . possible since
according Corollary 1 embedded graphs trees, therefore nodes SL
already tn . look set e-edges (s, t) tn+1 SR
/ SR .
Let (s, z) edge originating d-edge Sz subtree rooted z
tn+1 . Notice Sz already part Fn . tied s0 SL therefore s0 source
node d. expand tn include edge (s0 , z) Sz s0 already used
d-edge tn . guaranteed part SL (only d-edges
part SL shared). Finally, complete construction tn arbitrarily
expanding unused outgoing d-edge tn nodes, obtain complete embedded
tree.
constructed embedded tree tn Fn . Therefore, according inductive
assumption, tn consequent formalism. tn contains SL instantiation
dual leaf variables. Therefore, matched L rule L R applied.
easy verify application rule tn yield tn+1 , required. Thus, tn+1
also consequent formalism.

sake simplicity, proofs ignored case one leaf
variables L match multiple target nodes l appear R non-leaves. described
Section 5.2, case matched target nodes inserted SR alternatives (with
proper sharing modifiers). Consequently, SR becomes compact forest containing
multiple trees. Similarly, SL compact forest, whose represented trees correspond
possible choices matching leaf variables. mapping nodes matched
leaf variables SL nodes generated SR defines one-to-one
mapping trees SL SR .
proofs easily adapted handle case, follows. First, proof
Lemma 1 need change. Theorem 1, proof rule application create
cycles still holds underlying graph SR DAG rather tree. prove
embedded partial DAG 0 tree, observe exactly one trees embedded
SR part 0 . Thus, consider tree SR corresponding tree
SL , ignoring rest SR SL , proceed original proof. Similarly,
prove completeness Theorem 2, refer tree represented SL , part
tn , corresponding tree SR . prove soundness, consider subtrees
SR corresponding tree SL .
51

fiBar-Haim, Dagan & Berant

References
Bar-Haim, R. (2010). Semantic Inference Lexical-Syntactic Level. Ph.D. thesis,
Department Computer Science, Bar-Ilan University, Ramat-Gan, Israel.
Bar-Haim, R., Berant, J., & Dagan, I. (2009). compact forest scalable inference
entailment paraphrase rules. Proceedings EMNLP.
Bar-Haim, R., Berant, J., Dagan, I., Greental, I., Mirkin, S., Shnarch, E., & Szpektor, I.
(2008). Efficient semantic deduction approximate matching compact parse
forests. Proceedings TAC 2008 Workshop.
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., & Szpektor,
I. (2006). Second PASCAL Recognising Textual Entailment Challenge.
Second PASCAL Challenges Workshop Recognizing Textual Entailment.
Bar-Haim, R., Dagan, I., Greental, I., & Shnarch, E. (2007). Semantic inference
lexical-syntactic level. Proceedings AAAI.
Bar-Haim, R., Szpektor, I., & Glickman, O. (2005). Definition analysis intermediate
entailment levels. Proceedings ACL Workshop Empirical Modeling
Semantic Equivalence Entailment.
Barzilay, R., & Lee, L. (2003). Learning paraphrase: unsupervised approach using
multiple-sequence alignment. Proceedings HLT-NAACL.
Barzilay, R., & McKeown, K. R. (2001). Extracting paraphrases parallel corpus.
Proceedings ACL.
Bensley, J., & Hickl, A. (2008). Workshop: Application LCCs GROUNDHOG system
RTE-4. Proceedings TAC 2008 Workshop.
Bentivogli, L., Clark, P., Dagan, I., Dang, H. T., & Giampiccolo, D. (2010). Sixth
PASCAL Recognizing Textual Entailment Challenge. Proceedings TAC 2010
Workshop.
Bentivogli, L., Dagan, I., Dang, H. T., Giampiccolo, D., & Magnini, B. (2009). Fifth
PASCAL Recognizing Textual Entailment Challenge. Proceedings TAC 2009
Workshop.
Berant, J., Dagan, I., & Goldberger, J. (2011). Global learning typed entailment rules.
Proceedings ACL.
Bhagat, R., & Ravichandran, D. (2008). Large scale acquisition paraphrases learning
surface patterns. Proceedings ACL-08: HLT.
Bos, J., & Markert, K. (2005). Recognising textual entailment logical inference techniques. Proceedings EMNLP.
Bos, J., & Markert, K. (2006). logical inference helps determining textual entailment
(and doesnt). Proceedings Second PASCAL Recognising Textual
Entailment Challenge.
Chklovski, T., & Pantel, P. (2004). VerbOcean: Mining web fine-grained semantic
verb relations. Proceedings EMNLP.
52

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Collins, M., & Duffy, N. (2001). Convolution kernels natural language. Advances
Neural Information Processing Systems 14.
Connor, M., & Roth, D. (2007). Context sensitive paraphrasing single unsupervised
classifier. ECML.
Cooper, R., Crouch, R., van Eijck, J., Fox, C., van Genabith, J., Jaspars, J., Kamp, H.,
Pinkal, M., Milward, D., Poesio, M., Pulman, S., Briscoe, T., Maier, H., & Konrad, K.
(1996). Using framework. Tech. rep., FraCaS: Framework Computational
Semantics.
Dagan, I., & Glickman, O. (2004). Probabilistic textual entailment: Generic applied modeling language variability. PASCAL workshop Text Understanding Mining.
Dagan, I., Glickman, O., Gliozzo, A., Marmorshtein, E., & Strapparava, C. (2006a). Direct
word sense matching lexical substitution. Proceedings COLING-ACL.
Dagan, I., Glickman, O., & Magnini, B. (2006b). PASCAL Recognising Textual Entailment Challenge. Quinonero-Candela, J., Dagan, I., Magnini, B., & dAlche Buc, F.
(Eds.), Machine Learning Challenges. Lecture Notes Computer Science, Vol. 3944,
pp. 177190. Springer.
Dagan, I., Roth, D., Sammons, M., & Zanzotto, F. M. (2013). Recognizing Textual Entailment: Models Applications. Synthesis Lectures Human Language Technologies.
Morgan & Claypool Publishers.
de Salvo Braz, R., Girju, R., Punyakanok, V., Roth, D., & Sammons, M. (2005). inference
model semantic entailment natural language.. Proceedings AAAI.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990).
Indexing latent semantic analysis. Journal American Society Information
Science, 41 (6), 391407.
Dinu, G., & Lapata, M. (2010). Topic models meaning similarity context. Proceedings Coling 2010: Posters.
Emele, M. C., & Dorna, M. (1998). Ambiguity preserving machine translation using packed
representations. Proceedings COLING-ACL.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. Language, Speech
Communication. MIT Press.
Gabrilovich, E., & Markovitch, S. (2007). Computing semantic relatedness using Wikipediabased Explicit Semantic Analysis. Proceedings IJCAI.
Ganitkevitch, J., Van Durme, B., & Callison-Burch, C. (2013). PPDB: paraphrase
database. Proceedings HLT-NAACL.
Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. (2007). Third PASCAL Recognizing Textual Entailment Challenge. Proceedings ACL-PASCAL Workshop
Textual Entailment Paraphrasing.
Giampiccolo, D., Trang Dang, H., Magnini, B., Dagan, I., & Dolan, B. (2008). Fourth
PASCAL Recognizing Textual Entailment Challenge. Proceedings TAC 2008
Workshop.
53

fiBar-Haim, Dagan & Berant

Glickman, O., & Dagan, I. (2003). Identifying lexical paraphrases single corpus:
case study verbs. Proceedings RANLP.
Glickman, O., Shnarch, E., & Dagan, I. (2006). Lexical reference: semantic matching
subtask. Proceedings EMNLP.
Haghighi, A. D., Ng, A. Y., & Manning, C. D. (2005). Robust textual inference via graph
matching. Proceedings EMNLP.
Harmeling, S. (2009). Inferring textual entailment probabilistically sound calculus.
Natural Language Engineering, 15 (4), 459477.
Heilman, M., & Smith, N. A. (2010). Tree edit models recognizing textual entailments,
paraphrases, answers questions. Proceedings HLT-NAACL.
Hickl, A. (2008). Using discourse commitments recognize textual entailment. Proceedings COLING.
Hickl, A., & Bensley, J. (2007). discourse commitment-based framework recognizing textual entailment. Proceedings ACL-PASCAL Workshop Textual
Entailment Paraphrasing.
Hickl, A., Bensley, J., Williams, J., Roberts, K., Rink, B., & Shi, Y. (2006). Recognizing textual entailment LCCs GROUNDHOG system. Second PASCAL
Challenges Workshop Recognizing Textual Entailment.
Jurafsky, D., & Martin, J. H. (2008). Speech Language Processing: Introduction
Natural Language Processing, Computational Linguistics Speech Recognition
(Second edition). Prentice Hall.
Kamp, H., & Reyle, U. (1993). Discourse Logic. Introduction Modeltheoretic
Semantics Natural Language, Formal Logic Discourse Representation Theory.
Kluwer Academic Publishers, Dordrecht.
Kay, M. (1996). Chart generation. Proceedings ACL.
Kazama, J., & Torisawa, K. (2007). Exploiting Wikipedia external knowledge named
entity recognition. Proceedings EMNLP-CoNLL.
Kipper, K. (2005). VerbNet: broad-coverage, comprehensive verb lexicon. Ph.D. thesis,
University Pennsylvania.
Kouylekov, M., & Magnini, B. (2005). Tree edit distance textual entailment. Proceedings RANLP.
Lehmann, J., Bizer, C., Kobilarov, G., Auer, S., Becker, C., Cyganiak, R., & Hellmann,
S. (2009). DBpedia - crystallization point web data. Journal Web
Semantics.
Lin, D. (1998). Dependency-based evaluation minipar. Proceedings Workshop
Evaluation Parsing Systems LREC.
Lin, D., & Pantel, P. (2001). Discovery inference rules question answering. Natural
Language Engineering, 7 (4), 343360.
Lotan, A., Stern, A., & Dagan, I. (2013). TruthTeller: Annotating predicate truth.
Proceedings HLT-NAACL.
54

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

MacCartney, B., Galley, M., & Manning, C. D. (2008). phrase-based alignment model
natural language inference. Proceedings EMNLP.
MacCartney, B., Grenager, T., de Marneffe, M.-C., Cer, D., & Manning, C. D. (2006).
Learning recognize features valid textual entailments. Proceedings HLTNAACL.
MacCartney, B., & Manning, C. D. (2009). extended model natural logic. Proceedings IWCS-8.
Macleod, C., Grishman, R., Meyers, A., Barrett, L., & Reeves, R. (1998). Nomlex: lexicon
nominalizations. Proceedings Euralex98.
Maxwell III, J. T., & Kaplan, R. M. (1991). method disjunctive constraint satisfaction. Tomita, M. (Ed.), Current Issues Parsing Technology. Kluwer Academic
Publishers.
Mehdad, Y., & Magnini, B. (2009a). word overlap baseline recognizing textual
entailment task. Unpublished manuscript.
Mehdad, Y., & Magnini, B. (2009b). Optimizing textual entailment recognition using particle swarm optimization. Proceedings 2009 Workshop Applied Textual
Inference.
Melamud, O., Berant, J., Dagan, I., Goldberger, J., & Szpektor, I. (2013). two level
model context sensitive inference rules. Proceedings ACL.
Meyers, A., Reeves, R., Macleod, C., Szekeley, R., Zielinska, V., & Young, B. (2004).
cross-breeding dictionaries. Proceedings LREC.
Mi, H., Huang, L., & Liu, Q. (2008). Forest-based translation. Proceedings ACL-08:
HLT.
Mirkin, S., Dagan, I., & Pado, S. (2010). Assessing role discourse references
entailment inference. Proceedings ACL.
Mirkin, S., Dagan, I., & Shnarch, E. (2009). Evaluating inferential utility lexicalsemantic resources. Proceedings EACL.
Moldovan, D. I., & Rus, V. (2001). Logic form transformation WordNet applicability question answering. Proceedings ACL.
Nairn, R., Condoravdi, C., & Karttunen, L. (2006). Computing relative polarity textual
inference. Proceedings International workshop Inference Computational
Semantics (ICoS-5).
Pang, B., Knight, K., & Marcu, D. (2003). Syntax-based alignment multiple translations:
Extracting paraphrases generating new sentences. Proceedings HLT-NAACL.
Pantel, P., Bhagat, R., Coppola, B., Chklovski, T., & Hovy, E. (2007). ISP: Learning
inferential selectional preferences. Proceedings HLT-NAACL.
Ponzetto, S. P., & Strube, M. (2007). Deriving large-scale taxonomy wikipedia.
Proceedings AAAI.
Ravichandran, D., & Hovy, E. (2002). Learning surface text patterns question answering system. Proceedings ACL.
55

fiBar-Haim, Dagan & Berant

Ritter, A., Mausam, & Etzioni, O. (2010). latent dirichlet allocation method selectional
preferences. Proceedings ACL.
Romano, L., Kouylekov, M., Szpektor, I., Dagan, I., & Lavelli, A. (2006). Investigating
generic paraphrase-based approach relation extraction. Proceedings EACL.
Ron, T. (2006). Generating entailment rules based online lexical resources. Masters
thesis, Computer Science Department, Bar-Ilan University.
Ruppenhofer, J., Sporleder, C., Morante, R., Baker, C., & Palmer, M. (2009). Semeval2010 task 10: Linking events participants discourse. Proceedings
Workshop Semantic Evaluations: Recent Achievements Future Directions
(SEW-2009).
Saint-Dizier, P., & Mehta-Melkar, R. (Eds.). (2011). Proceedings Joint Workshop FAM-LbR/KRAQ11. Learning Reading Applications Intelligent
Question-Answering.
Schoenmackers, S., Etzioni, O., Weld, D. S., & Davis, J. (2010). Learning first-order horn
clauses web text. Proceedings EMNLP.
Shinyama, Y., Sekine, S., Sudo, K., & Grishman, R. (2002). Automatic paraphrase acquisition news articles. Proceedings HLT.
Shnarch, E., Barak, L., & Dagan, I. (2009). Extracting lexical reference rules
Wikipedia. Proceedings ACL-IJCNLP.
Snow, R., Jurafsky, D., & Ng, A. Y. (2006a). Semantic taxonomy induction heterogenous evidence. Proceedings COLING-ACL.
Snow, R., Vanderwende, L., & Menezes, A. (2006b). Effectively using syntax recognizing
false entailment. Proceedings HLT-NAACL.
Stern, A., & Dagan, I. (2011). confidence model syntactically-motivated entailment
proofs. Proceedings RANLP.
Stern, A., & Dagan, I. (2014). Recognizing implied predicate-argument relationships
textual inference. Proceedings ACL.
Stern, A., Stern, R., Dagan, I., & Felner, A. (2012). Efficient search transformation-based
inference. Proceedings ACL.
Szpektor, I., & Dagan, I. (2007). Learning canonical forms entailment rules. Proceedings
RANLP.
Szpektor, I., & Dagan, I. (2008). Learning entailment rules unary templates. Proceedings COLING.
Szpektor, I., & Dagan, I. (2009). Augmenting WordNet-based inference argument
mapping. Proceedings ACL-IJCNLP Workshop Applied Textual Inference
(TextInfer).
Szpektor, I., Dagan, I., Bar-Haim, R., & Goldberger, J. (2008). Contextual preferences.
Proceedings ACL-08: HLT.
Szpektor, I., Tanev, H., Dagan, I., & Coppola, B. (2004). Scaling web based acquisition
entailment patterns. Proceedings EMNLP.
56

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Tatu, M., Iles, B., Slavick, J., Novischi, A., & Moldovan, D. (2006). COGEX Second Recognizing Textual Entailment Challenge. Second PASCAL Challenges
Workshop Recognizing Textual Entailment.
Tatu, M., & Moldovan, D. (2006). logic-based semantic approach recognizing textual
entailment. Proceedings COLING-ACL.
Tatu, M., & Moldovan, D. (2007). COGEX RTE3. Proceedings ACL-PASCAL
Workshop Textual Entailment Paraphrasing.
Valencia, V. S. (1991). Studies Natural Logic Categorial Grammar. Ph.D. thesis,
University Amsterdam.
van Deemter, K., & Kibble, R. (2000). coreferring: Coreference MUC related
annotation schemes. Computational Linguistics, 26 (4), 629637.
Voorhees, E. M., & Harman, D. (1997). Overview sixth Text REtrieval Conference
(TREC-6). Proceedings TREC.
Wang, M., & Manning, C. (2010). Probabilistic tree-edit models structured latent
variables textual entailment question answering. Proceedings COLING.
Wang, R., & Neumann, G. (2007). Recognizing textual entailment using subsequence
kernel method. Proceedings AAAI.
Yates, A., & Etzioni, O. (2009). Unsupervised methods determining object relation
synonyms web. Journal Artificial Intelligence Research (JAIR), 34, 255296.
Zanzotto, F. m., Pennacchiotti, M., & Moschitti, A. (2009). machine learning approach
textual entailment recognition. Natural Language Engineering, 15 (4), 551582.

57

fiJournal Artificial Intelligence Research 54 (2015) 631-677

Submitted 7/15; published 12/15

Practical, Integer-Linear Programming Model Delete-Free
Tasks Use Heuristic Cost-Optimal Planning
Tatsuya Imai

TATSUYA . IMAI .30100041@ GMAIL . COM

Graduate School Information Science Engineering
Tokyo Institute Technology
Tokyo, Japan

Alex Fukunaga

FUKUNAGA @ IDEA . C . U - TOKYO . AC . JP

Graduate School Arts Sciences
University Tokyo
Tokyo, Japan

Abstract
propose new integer-linear programming model delete relaxation cost-optimal
planning. straightforward IP delete relaxation impractical, enhanced model
incorporates variable reduction techniques based landmarks, relevance-based constraints, dominated action elimination, immediate action application, inverse action constraints, resulting
IP used directly solve delete-free planning problems. show IP model
competitive previous state-of-the-art solvers delete-free problems. LP-relaxation
IP model often good approximation IP, providing approach approximating optimal value delete-free task complementary well-known LM-cut
heuristic. also show constraints partially consider delete effects added
IP/LP models. embed new IP/LP models forward-search based planner, show
performance resulting planner standard IPC benchmarks comparable
state-of-the-art cost-optimal planning.

1. Introduction
delete relaxation classical planning problem relaxation planning problem
delete effects eliminated operators. delete relaxation, every proposition
becomes true remains true never becomes false again. delete relaxation
studied extensively classical planning literature used estimate cost
optimal plan original planning problem therefore useful basis heuristic
functions search-based domain-independent planning algorithms. solution original
planning problem solution delete relaxation, cost optimal solution
delete-relaxed problem lower cost original problem relaxation,
every proposition needs established once. Thus, optimal cost delete relaxation
planning problem (denoted h+ ) lower bound optimal cost original planning
problem. Despite fact computing h+ easier solving original planning problem,
computing h+ NP-equivalent (Bylander, 1994) poses challenging problem.
addition importance basis heuristic functions standard classical planning,
delete relaxation also interesting right, problems
naturally modeled delete-free problems (i.e., problems actions delete
effects). example, minimal seed set problem, problem systems biology seeks
c
2015
AI Access Foundation. rights reserved.

fiI MAI & F UKUNAGA

minimal set nutrients necessary organism fully express metabolism,
mapped delete-free planning problem (Gefen & Brafman, 2011). Another application
relational database query plan generation (Robinson, McIlraith, & Toman, 2014),
problem determining join orders modeled delete-free problem.
paper, propose new, integer programming (IP) approach computing h+ .1 show
model allows fast computation h+ , linear programming (LP) relaxation
model used successfully heuristic function A* -based planner. rest
paper structured follows: begin review previous work delete relaxation
well applications LP planning. introduce IP(T + ), basic integer programming
model delete-free planning problem (Section 3) show correctly computes h+ . Since
straightforward IP(T + ) model often intractable useful practice computing
h+ , develop enhanced model, IPe (T + ), reduces number variables IP
using techniques landmark-based constraints, relevance analysis (Section 4). evaluate
performance basic IP(T + ) enhanced IPe (T + ) models Section 5, show
IPe (T + ) competitive state-of-the-art methods computing h+ .
objective use IP models basis heuristic forward state-space
search based planning, solving IP every node search algorithm computationally daunting, Section 6, propose evaluate two relaxations IP(T + )-based IP models.
consider LP(T + ) LPe (T + ), LP-relaxation IP(T + ) IPe (T + ), show
LP-relaxations usually closely approximate h+ . also introduce time-relaxation IP
LP models (IPetr (T + ) LPetr (T + ), respectively) reduces number variables,
cost sometimes underestimating h+ , show time-relaxations usually closely
approximate h+ . experimentally compare closely relaxed, delete-free models approximate h+ LM-cut heuristic (Helmert & Domshlak, 2009) show approaches
complementary.
Next, Section 7, evaluate utility IP LP models heuristics forwardsearch based planning embedding A* -based planner. results show although
LPetr (T + ) competitive LM-cut heuristic overall, domains
LPetr (T + ) yields state-of-the-art performance, outperforming LM-cut.
turn strengthening IP LP models partially considering delete effects
(Section 8). add constraints enforce lower bounds number times action must
used. correspond net change constraints recently proposed Pommerening
et al. (2014), well action order relaxation van den Briel et al. (2007). tightened
bound IPc (T ) dominates IP(T + ). Counting constraints also added LP-relaxation


LPec (T ), well time-relaxed LP-relaxation LPectr (T ). However, additional counting
constraints makes IP LP difficult, A* -based planner uses bounds,
tradeoff tighter bound (fewer nodes searched A* ) time spent per node.
result, find although counting constraints result enhanced performance domains,
significantly degrades performance domains. experimentally compare countingconstraint enhanced models LMC-SEQ LP model Pommerening et al. (2014)
combines landmark net-change constraints, show that, like LM-cut vs delete-free LPs,
models complementary.
1. paper revises extends work originally reported authors paper presented ECAI2014 (Imai &
Fukunaga, 2014). Formal results proofs ECAI paper included, paper contains
much thorough experimental evaluation models (all experimental data new).

632

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

Table 1 provides overview IP/LP models discussed Sections 3-8, also
serves roadmap paper . model, indicate section text
model introduced, constraints used model, variable elimination optimizations
used model. Figure 1 directed graph showing dominance relationships among
optimal costs IP/LP models.
Finally, clear dominance relationship among LP models (with respect
performance A* -based planners use LP models heuristic function),
propose evaluate simple automatic configuration heuristic selects LP use
heuristic A* (Section 9). simple automated bound selection significantly boosts performance, resulting ensemble-based LP-heuristic competitive state-of-the-art heuristics. Section 10 concludes paper summary discussion results directions future work.

2. Background Related Work
section first introduces notation planning tasks used rest paper, surveys related work solving delete-free planning tasks well previous applications
IP/LP domain-independent planning.
2.1 Preliminary Definitions
STRIPS planning task defined 4-tuple = hP, A, I, Gi. P set propositions.
set actions. state represented subset P , applying action state adds
propositions removes propositions state. action composed
three subsets P , hpre(a), add(a), del(a)i called preconditions, add effects,
delete effects. action applicable state iff satisfies pre(a) S. applying
S, propositions change S(a) = ((S \ del(a)) add(a)). sequence actions
= (a0 , , ), use S() denote ((((S \ del(a0 )) add(a0 )) \ del(a1 )) ) add(an ).
Let P initial state G P goal. solution planning task sequence
actions transform state satisfies G S. Formally, feasible solution, i.e.,
plan, sequence actions = (a0 , , ) satisfies (i) i, pre(ai ) I((a0 , , ai1 )),
(ii) G I().
basic STRIPS planning task extended STRIPS planning action costs,
action associated (non-negative) cost c(a). objective cost-optimal planning STRIPS
model action costs find plan minimizes sum costs
P
actions i=n
c(a
).
i=0
delete relaxation task , denoted + , task hP, A+ , I, Gi A+ set
delete-free actions defined A+ = {hpre(a), add(a), | A}. also use + denote
task delete-free beginning without relaxed.
2.2 Previous Work Computing h+ Relaxations
delete relaxation used basis planning heuristics since beginning
recent era interest forward-state space search based planning (Bonet & Geffner, 2001). Unfortunately, computing h+ known NP-equivalent reduction vertex cover (Bylander,
633

fiI MAI & F UKUNAGA

Model
IP(T + ) (Sec. 3)
IPe (T + ) (Sec. 4)

Constraints
C1, C2, C3, C4,
C5, C6,
C1, C2a C3, C4,
C5, C6

Variable Eliminations
None
Landmarks (4.1), relevance (4.2), dominated
action elimination (4.3),
immediate action application (4.4)
None
IPe (T + )
IPe (T + )

LP(T + ) (Sec. 6.1)
LPe (T + ) (Sec. 6.1)
LPetr (T + ) (Sec. 6.2)

IP(T + )
IPe (T + )
C1, C2a C3, C4,

IPc (T ) (Sec. 8)

C1, C2, C3, C4,
C5, C6, C7 C8

None

IPec (T + ) (Sec. 8)

C1, C2a C3, C4,
C5, C6 C7 C8

LPc (T ) (Sec. 8)

LPec (T ) (Sec. 8)

LPectr (T ) (Sec. 8)

IPc (T )

IPec (T )
C1, C2a C3, C4,
C7 C8

Landmarks (4.1), relevance
(4.2), modified dominated
action elimination (Definition 2)
None

IPec (T )

IPec (T )

A* /autoconf (Sec. 9)

Selects among LPe (T + ), LPetr (T + ), LPec (T ),

LPectr (T ).



Basic delete-free task IP
model (computes h+ )
Enhanced IP model (computes h+ )

LP relaxation IP(T + )
LP relaxation IPe (T + )
LP-relaxation timerelaxation IPe (T + )
Basic delete-free task
IP model enhanced
counting constraints
Enhanced IP model
counting constraints

LP relaxation IPc (T )

LP relaxation IPec (T )
LP-relaxation time
relaxation IPec (T )



Automatic LP Model Selection

Table 1: Overview delete-relaxation based IP/LP models paper

LP(T+)

LPtr(T+)

LPe(T+)

IP(T+) = IPe(T+) =aaa
h+

IPcec(T)

LPec
c(T)

LPtre(T+)

IPtre(T+)

ec(T)
IPctr

e (T)
c
LPctr

IPtr(T+)

Figure 1: Dominance relationships among IP/LP models. Edge modeli modelj indicates
optimal cost modeli optimal cost modelj . 4 highlighted LPs components
A* /autoconf model.

634

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

1994), therefore, beginning, researchers avoided direct computation h+ , instead
sought approximations h+ .
satisficing planning, optimal solutions required, successful approach deriving heuristics approximate delete relaxation. additive heuristic (hadd ) assumes
subgoals independent computes sum achieving subgoal delete-relaxed
model (Bonet & Geffner, 2001). FF heuristic (Hoffmann & Nebel, 2001) constructs planning
graph (Blum & Furst, 1997) delete-relaxed problem, extracts relaxed plan, computes
number actions relaxed plan, upper bound h+ .
case cost-optimal planning, action assigned cost objective
find minimal cost plan, lower bounds h+ basis several admissible heuristic functions
used literature. Bonet Geffner (2001) proposed hmax heuristic,
computes highest cost associated achieving costly, single proposition.
hmax admissible, often informative (i.e, gap hmax h+ large)
considers single costly goal proposition. admissible landmark cut
(LM-cut) heuristic (Helmert & Domshlak, 2009), approximates h+ follows. state s, LMcut heuristic first computes hmax (s), zero infinite, h+ zero infinite,
hLM cut (s) = hmax (s). Otherwise, disjunctive action landmark L (a set actions least one
must included relaxed plan) computed, cost actions L reduced
c(m), cost minimal-cost action L, hLM cut increased c(m). process
repeated hmax (s) (for remaining, reduced problem) becomes 0. approximations
h+ informative hmax include set-additive heuristic (Keyder & Geffner, 2008)
cost-sharing approximations hmax (Mirkis & Domshlak, 2007).
Previous planners avoided direct computation h+ extra search efficiency
gained using h+ offset high cost computing h+ . far aware, first
actual use h+ inside cost-optimal planner Betz Helmert (2009), implemented
domain-specific implementations h+ several domains. recently, Haslum et al. evaluated
use domain-independent algorithm h+ (Haslum, Slaney, & Thiebaux, 2012)
heuristic function A* -based cost-optimal planning, found performance relatively
poor (Haslum, 2012).
recent years, several advances computation h+ . Since, described
above, LM-cut heuristic (Helmert & Domshlak, 2009) lower bound h+ , cost-optimal
planner using A* search algorithm LM-cut heuristic directly applied delete
relaxation classical planning problem order compute h+ . possible improve upon
developing methods exploit delete-free property specifically tailored
solving delete relaxation. Pommerening Helmert (2012) developed approach uses
IDA* branch-and-bound incrementally computed LM-cut heuristic. Gefen Brafman
(2012) proposed action pruning delete-free problems.
different approach computing h+ based observation h+ could formulated
problem finding minimal hitting set sets disjunctive action landmarks (Bonet &
Helmert, 2010). led methods computing h+ searching minimum-cost hitting set
complete set action landmarks delete-relaxed planning problem (Bonet & Castillo,
2011; Haslum et al., 2012). original implementation Haslum et al.s hitting-set based
h+ solver used problem-specific branch-and-bound algorithm (Haslum et al., 2012), improved
implementation (which use experimental evaluation Section 5) uses integer programming solve hitting set problem (Haslum, 2014a).
635

fiI MAI & F UKUNAGA

2.3 Integer/Linear Programming Classical Planning
Another related line research modeling classical planning integer/linear programs
(ILP). use high-performance, general problem solvers solve planning problems
pioneered Kautz Selman, solved planning problems encoding propositional
satisfiability (SAT) applied state-of-the-art SAT solvers. basic approach instantiate
SAT formula satisfying assignment implies t-step plan. SATPLAN starts small
value (e.g., trivially, 1, lower bound), instantiates propositional formula F (t)
satisfiable plan parallel steps less exists. F (t) satisfiable,
minimal parallel makespan plan found. Otherwise, incremented, process
repeated plan found. initial encodings modestly successful (Kautz &
Selman, 1992), advances SAT solver technology well improvements encoding
integration planning graphs (Blum & Furst, 1997) led dramatic performance improvements (Kautz & Selman, 1996, 1999). Recent work SAT-based planning includes improved
encodings well execution strategies SAT strategies improve upon simply incrementing
(Rintanen, Heljanko, & Niemela, 2006). addition, improvements SAT solvers
specifically target domain-independent planning investigated (Rintanen, 2012)
Since expressiveness integer programming (IP) subsumes SAT, SAT encodings
straightforwardly translated IP. However, direct translation SAT encodings IP resulted
poor performance, state-change formulation replaces original fluents SAT
encoding set variables directly expresses addition, deletion, persistence
fluents shown successful basis IP model planning (Vossen, Ball,
Lotem, & Nau, 1999). formulation strengthened additional mutual exclusion constraints (Dimopoulos, 2001). Optiplan model (van den Briel & Kambhampati, 2005) combined
state-change IP formulation planning-graph based model refinement strategies improvements Dimopoulous (2001). SAT-based approaches described above, IP models
feasible plan steps exists constructed. However, unlike
SAT formulation, easy directly encode action costs objective function IP
model, IP models used directly solve cost-optimal planning problems. Another
approach decomposes planning instance set network flow problems, subproblem corresponds state variable original planning problem (van den Briel, Vossen, &
Kambhampati, 2008).
Instead modeling directly solving classical planning problem IP, another approach, adopt paper, uses ILP models provide heuristic function guides
state-space search planning algorithms A* . early instance approach (which,
knowledge, also earliest application LP classical planning) LPlan,
LP encoding classical planning problem used heuristic function partial order
planner (Bylander, 1997). Van den Briel et al. (2007) developed admissible heuristic based
LP model represents planning problem order actions executed
relaxed, variable represents number times action executed. Delete effects
considered, constraints number actions delete values
incremented actions add value. Although LP-based heuristic
integrated planning system, compared relaxed problem cost found model
Bylanders LPlan LP model, well LP model h+ .
636

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

knowledge, h+ implementation van den Briel et al. (2007) first implementation IP model h+ . First, relaxed planning graph (Blum & Furst, 1997) expanded
quiescence, results instantiation actions relevant optimal
delete-free task well upper bound number steps optimal delete-free task.
Then, h+ computed using delete-relaxed, step-based encoding planning problem
Optiplan (van den Briel, 2015).
Cooper et al. (2011) showed optimal solution dual LP model relaxes action ordering corresponds best lower bound obtained applying
transformations original planning problem shift costs among actions affect
fluents.
Bonet proposed hSEQ , admissible, flow-based LP heuristic based Petri Net state equations (Bonet, 2013) used heuristic A* -based planner. Bonet van den
Briel (2014) enhanced Bonets flow-based LP model adding action landmark constraint implementing variable merging strategies, resulting competitive, admissible heuristic. Karpas
Domshlak (2009) proposed LP formulation compute optimal partitioning landmarks. Pommerening et al. (2014) proposed operator counting framework enabled unification
number ideas, including state equation formulation (Bonet, 2013), post-hoc optimization constraints (Pommerening, Roger, & Helmert, 2013), well landmarks (the formulation Bonet
& Helmert, 2010, dual formulation Karpas & Domshlak, 2009) state
abstraction heuristics (Katz & Domshlak, 2010). showed combinations constraints resulted strong heuristics significantly outperformed LM-cut heuristic. recent survey
Roger Pommerening (2015) presents survey LP-based heuristics planning
includes earlier conference version paper (Imai & Fukunaga, 2014) suggests
delete-relaxation model could incorporated operator counting framework associating
operator-counting variable action variable (see below) delete-relaxed problem.

3. IP(T + ): Basic IP Formulation Delete-Free Task
define integer program IP(T + ), IP formulation delete free task
+ = hP, A+ , I, Gi. Note feasible solution IP(T + ) (not optimal solution),
derive corresponding, feasible non-redundant (i.e., action appears once)
plan + cost IP(T + ) solution.
First, define variables IP(T + ). addition able derive plan IP(T + ),
always exists injective mapping feasible non-redundant plan IP(T + ) solution.
Thus, also show feasible assignments variables derived feasible plan
+ , well meanings roles variables. use = (a0 , , ) denote
plan + corresponding solution IP(T + ). say first achiever p plan
p 6 I, first action achieves (establishes) p.
proposition: p P, U (p) {0, 1}. U (p) = 1 iff p I(). U (p) indicates whether proposition p
achieved relaxed plan + .
action: A, U (a) {0, 1}. U (a) = 1 iff holds. U (a) indicates whether action
used relaxed plan.
add effect: A, p add(a), E(a, p) {0, 1}. E(a, p) = 1 iff holds first
achiever p. E(a, p) = 0 p true I, p achieved.
637

fiI MAI & F UKUNAGA

time (proposition): p P, (p) {0, , |A|}. (p) = p I() p added
at1 first. (p) = 0 p member I. (p) indicates time step p first
achieved first achiever.
time (action): A, (a) {0, , |A|}. (a) = = . (a) = |A| 6 .
(a) indicates time step first used.
initial proposition: p P, I(p) {0, 1}. I(p) = 1 iff p I.
p P achieved once, i.e., p appears add effects multiple actions ,
assign (p) index first action . p achieved, i.e., p 6 I() holds,
assign arbitrary value {0, , |A|} (p). Given delete-free task + feasible
non-redundant plan , call assignment solution derived .
use
Pthe following fact later proofs: solution derived feasible solution satisfies (a) s.t.padd(a ) E(a , p) 1 proposition p U (p) = 1, (b)
P

s.t.padd(a ) E(a , p) = 0 proposition p U (p) = 0.
Variables I(p) auxiliary variables computing h+ . Although redundant
solving delete-free task one time, useful avoid reconstructing constraints
state IP(T + ) LP(T + ) embedded heuristic function forward-search planner
called state.
objective function defined follows:
X
minimize:
c(a)U (a).
(1)
aA

objective function, cost solution IP(T + ) equal cost
corresponding (delete-free) plan.
Finally define following six constraints.
(C1) p G, U (p) = 1. (The goals must achieved).
(C2) A, p pre(a), U (p) U (a). (Actions require preconditions).
(C3) A, p add(a), U (a) E(a, p). (An action first achiever used).
P
(C4) p P, I(p) + s.t.padd(a ) E(a , p) = U (p). (If proposition achieved, must
true initial state effect action).
(C5) A, p pre(a), (p) (a). (Actions must preceded satisfaction
preconditions).
(C6) A, p add(a), (a) + 1 (p) + (|A| + 1)(1 E(a, p)). (If first achiever
p, must precede p).
show solution IP(T + ) derived feasible non-redundant plan
feasible. variable V IP(T + ), VF describes assignment V solution F
IP(T + ).
T+

Proposition 1. Given delete-free task + feasible, non-redundant plan + ,
solution F IP(T + ) derived feasible solution IP(T + ).
638

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

Proof. F clearly satisfies constraint C1 since satisfies G I().
Constraint C2 satisfied exists action proposition p pre(a)
U (a)F = 1 U (p)F = 0. However, U (a)F = 1, U (p)F = 1
delete-free feasible plan p established point. show F satisfies
constraints C3 C4 similar arguments. exists action proposition
p add(a) E(a, p)F = 1, U (a)F = 1 must hold according definition F .
addition, exists proposition p U (p)F = 1, exists first achiever p
E(a, p)F = 1, p member initial state I.
action member , propositions precondition must achieved
used. Hence, according definition F , (p)F (a)F action
plan . action member , (a)F = |A|. Thus, constraint
C5 satisfied action plan , regardless values (p)F .
Finally F satisfies constraint C6 action proposition precondition
p pre(a). first achiever p, i.e., E(a, p) = 0, constraint C6 satisfied
regardless values (p)F (a)F . first achiever p, then, according
definition F , (p)F = (a)F + 1 , satisfies constraint C6.
addition, exists feasible plan IP(T + ) feasible solution. IP(T + )
solved optimally, optimal plan + obtained according following proposition.
Proposition 2. Given feasible solution F IP(T + ), action sequence = (a0 , , )
obtained ordering actions set {a | U (a)F = 1} ascending order (a)F feasible
plan + .
Proof. First show satisfies condition (ii) plan (i.e., G I()) using proof
contradiction. Assume exists proposition g G satisfies g 6 I(). Then,
exists action achieving g . Since F solution IP(T + ), U (g)F = 1 due constraint
C1. Since g 6 I() implies g 6 I, I(g)F = 0. Therefore, satisfy constraint C4, must
exist action g add(a) E(a, g)F = 1. However, satisfy constraint C3,
U (a)F = 1 hold. means , contradicts assumption.
Next show satisfies condition (i) (i.e., i, pre(ai ) I((a0 , , ai1 ))). base
case inductive proof, assume exists proposition p P satisfying p pre(a0 )
p 6 I. Since a0 , U (a0 )F = 1 hold, U (p)F = 1 hold according
constraint U (p)F U (a0 )F . Then, similar proof condition (ii), must exist action
p add(a), U (a)F = 1, E(a, p)F = 1. However, satisfy constraint C5,
(p) (a0 ) must true, (a) + 1 (p) hold satisfy constraint C6. Therefore
U (a)F = 1 (a) < (a0 ), a0 first action , contradiction.
Similar case = 0, > 0, pre(ai ) I((a0 , , ai1 )) true, must
exist action 6 (a0 , , ai1 ) U (a)F = 1 (a) < (ai ), contradicting fact
ai i-th action sequence .
Corollary 1. Given optimal solution F IP(T + ), sequence actions built ordering
actions set {a | U (a)F = 1} ascending order (a)F optimal plan + .
P
+ ) 3|P | + 2|A| +
number variables

IP(T
|add(a)|. number constraints
P
P

Pless 2|P
P| + 2 aA |pre(a)| + 2 aA |add(a)|. number terms also O(|P | +
|pre(a)| + |add(a)|).
639

fiI MAI & F UKUNAGA

4. Enhanced IP Model
IP(T + ) provides IP model exactly computing h+ , shall see Section 5
IP(T + ) competitive previous methods computing h+ . Thus, section,
introduce variable elimination techniques modifications constraints order
speed computation h+ . show experimental results, IPe (T + ),
incorporates enhancements, computes h+ significantly faster IP(T + ). enhancements adopted IP framework previous work planning research.
particular, landmark-based variable reduction method plays key role.
Note enhancements introduce constraints render solutions IP(T + )
mapped feasible plans + infeasible. However, show cases, least one
optimal plan always remain valid enhanced model, optimal cost enhanced
model still corresponds h+ .
4.1 Landmark-Based IP Model Reduction
landmark element needs used every feasible solution (Hoffmann, Porteous,
& Sebastia, 2004). use two kinds landmarks, called fact landmarks action landmarks
work Gefen Brafman (2012). fact landmark planning task proposition
becomes true state every feasible plan, action landmark planning task
action included every feasible plan. also say fact action landmark
l landmark proposition p l landmark task hP, A, I, {p}i. Similarly say
landmark l landmark action l landmark task hP, A, I, pre(a)i.
IP model delete-free task + , proposition p fact landmark proposition
goal G, substitute U (p) = 1. Similarly, action action landmark,
substitute U (a) = 1. Landmark extraction substitution clearly prune feasible
solutions IP(T + ).
actually extract set landmarks satisfy intensional definitions, landmark
extraction algorithm necessary. easy see given
feasible delete-free task,
ff proposition
add
p P fact landmark p holds P, \ Ap , \ {p}, G infeasible,
= {a | p add(a)}. Similarly action action landmark
Aadd
p
hP, \ {a}, I, Gi infeasible. Hence, landmark candidate, test whether
landmark checking feasibility delete-free task excludes candidate.
feasibility delete-free task checked using following, straightforward algorithm based
delete-relaxed planning graph method Hoffmann Nebel (2001): fluent, let
e(p) {0, 1} represent whether p achievable not. action, let e(a) {0, 1} represent
whether preconditions satisfied not. Initially, e(p) = 1 p , e(p) = 0
fluents. e(a) = 0 a. step algorithm, actions
e(a) = 0 whose preconditions satisfied; (1) set e(a) = 1, (2) set e(p) = 1
e add(a). algorithm terminates reaches quiescence, i.e., actions
e(a) = 0 whose preconditions satisfied found. takes |A| steps.
repeating feasibility check facts actions, algorithm collects fact
landmarks action landmarks satisfying definitions O(|T + |2 )-time.
interested computing h+ once, straightforward method one
described would sufficient. However, since intend use h+ -based models
heuristic functions forward state-space search planning, landmark extraction needs
640

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

performed repeatedly search, efficiency extraction procedure important.
experimented several methods, describe effective method below.
method extracting landmarks based method Zhu Givan (2003),
proposed planning based propagation method collecting causal landmarks. method
later generalized Keyder et al. AND-OR graph based landmark extraction method (Keyder,
Richter, & Helmert, 2010).
Zhu Givan (2003) define proposition p causal landmark hP, \ Apre
p , \ {p}, Gi
pre
infeasible, Ap = {a | p pre(a)}. focus causal landmarks, ignoring
(non-causal) landmarks nonessential (even misleading) point view
guiding search algorithm uses landmark-based heuristic. contrast, use landmarks
order reduce number variables IP model delete relaxation. Thus, instead
focusing causal landmarks using Zhu Givans criteria, seek larger set landmarks
slightly modifying
criterion landmark
detection. hP, \ Apre
p , \ {p}, Gi
ff
solution, P, \ Aadd
,

\
{p},
G
must
also

infeasible,
furthermore, using
p
pre
add
Ap instead Ap extract larger set fact landmarks. addition, Zhu Givan
used forward propagation algorithm based layered planning graph delete-free task
+ , use following, open-list based propagation algorithm.
proposition p, compute set fact landmarks p, using iterative method
based following update equations characterizing fact landmarks:
p member initial state I, {p} set fact landmarks achieve p.

p member I, set fact landmarks p {p} aA s.t.padd(a) (add(a)


p pre(a) (fact landmarks p )).
pseudocode open list based propagation algorithm shown Algorithm 1.
initialization phase, candidate set proposition p 6 set P , fact landmarks
p set {p} (Lines 1-3). addition, action inserted FIFO queue Q
satisfies pre(a) (Lines 4-7). main loop iterative method similar straightforward method described above. iteration, action retrieved Q, candidate
set fact landmarks updated p add(a) based second equation (Lines 12-14).
Moreover, method memorizes achievability p (Line 11), action inserted Q
members pre(a ) achievable candidate set p pre(a ) changed (Lines
15-17). process continues Q becomes empty. clarity simplicity, implementation details/optimizations omitted Algorithm 1, e.g., instead literally inserting every
member P L[p] Line 3, use single flag represent L[p] = P Updating candidate set always reduces number elements, method always terminates. Unlike
simpler O|T + |2 algorithm described above, algorithm complete (not landmarks
extracted). However, soundness method guaranteed following proposition.
Proposition 3. Given delete-free STRIPS planning task hP, A+ , I, Gi, assume propositions
P achieved. Let L(p) set fact landmark candidates p computed
landmark extracting method.
(i) L(p) = {p} p I,


(ii) L(p) = {p} aA s.t.padd(a) (add(a) p pre(a) L(p )) p 6
641

fiI MAI & F UKUNAGA

Algorithm 1 landmark extracting method
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

// L[p] sets candidates fact landmarks p P .
L[p] P p 6 I;
L[p] {p} p I;
I;

insert FIFO queue Q pre(a) S;
end
Q empty
retrieve action Q.
p add(a)
{p}.

X L[p] (add(a) p pre(a) L[p ]);
L[p] 6= X
L[p] X.
Apre
p

insert Q pre(a ) 6 Q;
end
end
end
end
// point, L[p] contain sets fact landmarks p P .

satisfied, elements L(p) fact landmarks p.
Proof. Assume proposition q satisfies q L(p) q fact landmark p.
p 6= q since proposition fact landmark itself. Then, L(p) one
proposition, condition (i) (ii), p 6 holds. Since q landmark, exists
non-empty feasible plan delete-free task hP, A+ , I, {p}i achieve q.
Let = (a0 , , ) plan, let ai action achieves p first.
p 6= q stated above, andS
q 6 add(ai ) since achieve q. Hence, according
condition (ii), q p pre(ai ) L(p ). Let p member pre(ai ) satisfies q L(p ).
Since feasible plan achieve q, p achieved , thus p 6= q holds. Then,
L(p ) one proposition, again, p 6 holds. Hence, = (a0 , , ai1 )
non-empty feasible plan delete-free task hP, A+ , I, {p }i achieve q.
argument extended ad infinitum, length clearly finite,
contradiction. Thus, members L(p) fact landmarks p proposition p P .

addition fact landmarks extracted using procedure, algorithm
extracts action landmarks using criterion: proposition p fact landmark G,
one action achieve p, used action landmark G.
642

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

4.2 Relevance Analysis
Backchaining relevance analysis widely used eliminate propositions actions irrelevant task. action relevant (i) add(a) G 6= , (ii) exists relevant action
satisfying add(a) pre(a ) 6= . proposition p relevant (i) p G, (ii) exists
relevant action p pre(a) holds.
addition, noted Haslum et al. (2012), sufficient consider relevance respect subset first achievers add effect. Although defined first achiever
achievability proposition, equivalent following definition: action first
achiever proposition p p add(a) p fact landmark a. Let fadd(a) denote
{p add(a) | first achiever p}. sufficient use fadd instead add
definition relevance.
p P relevant, eliminate variable U (a) = 0 U (p) = 0.
addition this, p add(a) first achiever p, eliminate variable
E(a, p) = 0. possible fact landmark fact irrelevant, case set U (p) = 1.
variable elimination prunes feasible solutions, clearly prune optimal
solutions.
4.3 Dominated Action Elimination
delete-free task, two actions add effects, clearly sufficient use
one two actions. idea generalized following reduction,
eliminates useless (dominated) actions.
Proposition 4. Given feasible delete-free task + , exists optimal plan
contain exists action satisfying following: (i) fadd(a) fadd(a ), (ii)
p pre(a ), p fact landmark p I, (iii) c(a) c(a ).
Proof. plan = (a0 , , ai1 , a, ai+1 , , ) + , show sequence actions
= (a0 , , ai1 , , ai+1 , , ) also feasible plan. proposition pre(a ) fact
landmark a, hence, pre(a) I((a0 , , ai1 )), pre(a ) I((a0 , , ai1 )) also
holds. definition first achievers, add(a) \ fadd(a) I((a0 , , ai1 )), also
I((a0 , , ai1 , a)) I((a0 , , ai1 , ). Therefore G I( ) ( feasible plan).
Finally, c() c( ) c(a) c(a ). Therefore, plan contains a, optimal,
exists another optimal plan contain a.
exists dominated action a, eliminate variable setting U (a) = 0.
variable elimination prunes feasible solutions IP(T + ). Moreover, sometimes prunes
optimal solutions c(a) = c(a ) holds condition (iii). However, shown proof
above, least one optimal solution remains.
slight generalization similar set constraints Robinson (2012)[Definition
5.3.4, p. 108] MaxSAT-based planner. Robinsons dominance condition checks whether (R1)
add(a) \ add(a ) \ I, (R2) pre(a ) \ pre(a) \ I, (R3) c(a) c(a ).
condition (iii) (R3) equivalent, condition (i) less strict condition (R1)
instead checking add effects, condition (i) tests whether propositions
first achiever subsumed . Furthermore, condition (ii) subsumes (R2)
proposition pre(a ) fact landmark a, pre(a) I((a0 , , ai1 )), pre(a )
I((a0 , , ai1 )) also holds, satisfying (R2).
643

fiI MAI & F UKUNAGA

4.4 Immediate Action Application
delete-free task + , actions immediately applied initial state without
affecting optimality relaxed plan. adopt immediate application zero-cost actions
(Gefen & Brafman, 2011) well immediate application action landmarks (Gefen & Brafman,
2012). delete-free task + , action satisfies c(a) = 0 pre(a) I,
sequence made placing optimal plan hP, \ {a}, add(a), Gi optimal
plan + . Similarly, action action landmark + applicable I,
applied immediately.
IP(T + ) model, variables (p) p eliminated substituting zero
values. Given sequence immediately applicable actions (a0 , , ak ) (it must correct
applicable sequence), eliminate variables follows: (i) U (ai ) = 1, (ii) (ai ) = i,
(iii) p pre(ai ), U (p) = 1, (iv) p add(ai ) \ I((a0 , , ai1 )), U (p) = 1, (p) =
E(ai , p) = 1, (v) p add(ai ) \ I((a0 , , ai1 )), \ {a0 , , ai }, E(a, p) = 0.
4.5 Iterative Application Variable Eliminations
variable elimination techniques described interact synergistically
resulting cascade eliminations. Therefore, used iterative variable elimination algorithm
applies eliminations quiescence. order elimination applied shown
Algorithm 2. full landmark extraction pass variable elimination would extremely
expensive. Therefore, perform landmark extraction iterative application
eliminations.
Algorithm 2 Iterative Variable Elimination
relevance analysis;
landmark extraction;
variable eliminated
immediate action application;
dominated actions elimination;
relevance analysis;

4.6 Inverse Action Constraints
define following inverse relationship pair actions delete-free task + .
Definition 1 (inverse action). two actions a1 , a2 A, a1 inverse action a2 if: (i)
add(a1 ) pre(a2 ), (ii) add(a2 ) pre(a1 ).
definition, clear a1 inverse action a2 , a2 inverse action a1 .
Inverse actions satisfy following fact.
Proposition 5. Given delete-free task + , let = (a0 , , ) feasible plan. ai
inverse action aj , < j holds, = (a0 , , aj1 , aj+1 , , ) also
feasible plan.
Proof. Since feasible plan + , pre(ai ) I((a0 , , ai1 )) I((a0 , , aj1 )).
definition inverse actions, add(aj ) pre(ai ) holds, add(aj ) pre(ai ) I((a0 , , aj1 )) =
644

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

I((a0 , , aj )). Hence (aj+1 , , ) applicable I((a0 , , aj1 )), G I( ) =
I().
Corollary 2. delete-free task + , feasible solution = (a0 , , ) optimal
ai inverse action aj ai aj non-zero cost.
several possible ways use proposition (e.g., U (a) + U (a ) 1,
inv(a), inv(a) set inverse actions a). order avoid adding large number
constraints IP(T + ) model (|A/2|2 worst case half actions inverses
other), modify constraint C2 follows:
P
(C2a) A, p pre(a), U (p) inv(a,p) E(a , p) U (a), inv(a, p) denotes set
inverse actions p add effect.


Proposition 6. Given delete-free task + , IP(T + ) constraint C2 feasible solution,
optimal solution IP(T + ) constraint C2 also feasible IP(T + ) constraint
C2a.
Proof. Let F optimal solution IP(T + ) constraint C2 derived optimal plan
+ . Since F satisfies constraints IP(T + ) constraint C2, suffices show
F satisfies constraint C2a action proposition p pre(a).
P
Recall feasible solution derived feasible plan satisfies s.t.padd(a ) E(a , p)
P
1 proposition p U (p) = 1, also satisfies s.t.padd(a ) E(a , p) = 0
P
P
proposition p U (p) = 0. Since s.t.padd(a ) E(a , p) inv(a,p) E(a , p)
action proposition p pre(a), F clearly satisfies constraint C2a U (p)F = 1


U (a)F = 0,
Pif U (p)F = 0 U (a)F = 0 hold.

show inv(a,p) E(a , p)F = 0 holds U (a)F = U (p)F = 1, assume
exists action inv(a, p) E(a , p)F = 1. According constraint C3, U (a )F = 1.
However, since F derived optimal plan + , must exist optimal plan +
contains . contradicts Corollary 2.
Since F feasible solution, exist action proposition p pre(a)
U (a)F = 1 U (p)F = 0. Hence F satisfies constraint C2a
p pre(a).
4.7 IPe (T + ): Enhanced IP Model h+
define IPe (T + ) integer programming model result first adding inverse
action constraints described Section 4.6 basic IP(T + ) model applying iterative reduction algorithm Algorithm 2 (which applies reductions Sections 4.1-4.4)
quiescence. previously noted, IPe (T + ) computes h+ . shall see below, cumulative
effects enhancements quite significant, resulting much practical IP model
computing h+ . See Table 1 summary relationship IPe (T + ) IP(T + ).

5. Experimental Evaluation IP Models Delete-Free Planning (Exact
Computation h+ )
section, evaluate effectiveness integer programming model delete relaxation method solving delete-free tasks computing h+ exactly. evaluate following
models:
645

fiI MAI & F UKUNAGA

IP(T + ): basic IP model (Section 3).
IP(T + )+LM: IP(T + ) landmark-based variable reduction method (Section 4.1).
IPe (T + ): enhanced model includes enhancements described Sections
4.1-4.6 designed speed computation h+ (landmark-based reduction,
relevance analysis, dominated action elimination, immediate action application, inverse action constraints).
emphasize (unlike models evaluated later sections)
IP models compute h+ exactly.
Following previous work solvers delete-free problems, main results based
evaluation using delete-free versions standard IPC benchmark problems (Section 5.1).
addition, Section 5.2, also present results much smaller scale study set natural,
delete-free problems systems biology (Gefen & Brafman, 2011).
5.1 Evaluation Delete-Free Versions IPC Benchmark Instances
Following methodology evaluating delete-free planning previous work (Haslum et al.,
2012; Pommerening & Helmert, 2012; Gefen & Brafman, 2012), evaluate IP models
solving International Planning Contest (IPC) benchmark instances delete effects
actions ignored. Below, experiments used CPLEX 12.61 solver solve integer
linear programs. experiments single-threaded executed Xeon E5-2680, 2.8GHz.
previous work computing h+ evaluated using several different sets
experimental settings (different CPU limits different problem instances), present results
4 sets comparisons. first 3 sets comparisons, compare benchmark results reported
previous publications results obtained running solvers problem instances,
fourth set results compares models improved implementation minimal
hitting set based approach (Haslum et al., 2012) one original authors.
Comparison results Pommerening Helmert (2012) (experimental setup described Section 5.1.1, results shown Table 2).
Comparison results Gefen Brafman (2012) (experimental setup described
Section 5.1.2, results shown Table 3).
Comparison results Haslum et al. (2012) (experimental setup described Section
5.1.2, results shown Table 4).
Comparison HST/CPLEX, improved implementation algorithm (Haslum
et al., 2012) (experimental setup described Section 5.1.3, results shown Table 5
Figures 2-3).
results copied previous work (Pommerening & Helmert, 2012; Haslum et al., 2012;
Gefen & Brafman, 2012) Tables 2-4 obtained using hardware available several years ago
original papers written, results IP(T + ), IPe (T + ), HST/CPLEX
obtained slightly recent hardware. Since coverage coarse metric based binary results (solved/unsolved), significantly impacted differences machine speed,
646

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

e.g., many problems threshold slightly faster machine (equivalent running
slightly longer) results many instances solved. order eliminate possibility
improvements hardware since 2010 (when first results compared
published) explain improvements obtained using approach, also include results running best IP model (IPe (T + )) significantly shorter CPU time limit previous
experiments, addition results use CPU time limit previous experiments.
5.1.1 C OMPARISON R ESULTS P OMMERENING
ELETE -F REE V ERSIONS IPC B ENCHMARKS



H ELMERT (2012)



first comparison results Pommerening Helmert (2012). Table 2 shows
results running IP(T + ), IP(T + )+LM, IPe (T + ) 5 minute time limit 2GB memory
limitation. Coverage (# problem instances solved) domain shown. columns
solver name contains PH12 Table 2 copied paper Pommerening Helmert
(2012). FD/PH12 Fast Downward using A* LM-cut heuristic applied deleterelaxed problems, BC/PH12 hitting set based approach Bonet Castillo (2011),
BnB/PH12 IDA*/PH12 best performing strategies using incremental LM-cut
heuristic delete-free problems proposed Pommerening Helmert (2012). Pommerening
Helmert obtained results using AMD Opteron 2356 processor 2GB memory limit
5 minute time limit.
Table 2 includes column IPe (T + )/1min, shows results 1-minute runs
IPe (T + ). columns Table 4 5 minute runs.
5.1.2 C OMPARISONS R ESULTS G EFEN B RAFMAN (2012) H ASLUM ET AL .
(2012) ELETE -F REE V ERSIONS IPC B ENCHMARKS
Next, evaluated h+ solvers previous results obtained 30-minute time
limit 2GB memory limit. Table 3 compares IP(T + ), IP(T + )+LM, IPe (T + )
results (Gefen & Brafman, 2012, p. 62, Table 2). LM-cut/GB12 column A*
LM-cut heuristic (Helmert & Domshlak, 2009) applied directly delete-free instances order
compute h+ . LM-cut+Pruning/GB12 column A* LM-cut using pruning techniques
delete-free instances proposed Gefen Brafman (2012). Table 4 compares IP(T + )
IPe (T + ) results Haslum et al. (2012, p. 356, Table 1). BC/HST12 column
method Bonet Castillo (2011). ML/HST12 column minimal landmark method
proposed Haslum et al.. original work Haslum et al. (2012), minimum-cost hitting
set problem solved using specialized branch-and-bound algorithm, ML/HST12 column reflects performance original algorithm. However, Minimal Landmark method
later significantly improved replacing hitting set solver CPLEX-based solver
(Haslum, 2014b), Table 4 also includes HST/CPLEX column, shows results
Minimal Landmark method using CPLEX hitting set solver. obtained HST/CPLEX
results running HST/CPLEX code machine used run IP models.
Table 4 includes column IPe (T + )/5min, shows results 5-minute runs
IPe (T + ) (all columns Table 4 30 minute runs).
Note Table 4, instances IPC2008 IPC2011 sequential satisfying track (i.e., -sat08 -sat11 domain names), accordance original paper
(Haslum et al., 2012).
647

fiI MAI & F UKUNAGA

5.1.3 C OMPARISON



HST/CPLEX



ELETE -F REE V ERSIONS IPC B ENCHMARKS

detailed comparison improved implementation hitting-set based method
Haslum et al. (2012). Although original version algorithm used problem-specific
branch-and-bound method solve hitting set problems, used recent version
Haslums h+ solver (source dated 2014-1-17), configured use CPLEX 12.61 solve hitting set subproblem. configuration abbreviated HST/CPLEX. shown Table
4, HST/CPLEX significantly outperforms original HST implementation described (Haslum
et al., 2012), compares favorably vs. previous methods.
Tables 5-6 Figures 2-3 compare IP(T + ), IPe (T + ), IP(T + )+LM, HST/CPLEX 1376
IPC benchmark instances. algorithms run 2GB memory limit. Table 5 shows results
30 minute time limit, Table 6 shows results 5 minute time limit. Tables 5
6 compares coverage runtimes per domain, Figure 2 compares cumulative number
instances solved function time, Figure 3 compares runtimes individual
instances.
contrast previous set experiments described Section 5.1.2, used optimal track
instances (-opt08 -opt11 domain names) satisficing optimal track
instances available benchmark sets. subsequent sections,
focus applying models basis heuristics forward-search, cost-optimal planning.
5.1.4 ISCUSSION R ESULTS ELETE -F REE V ERSIONS IPC B ENCHMARKS
surprisingly, basic IP(T + ) model competitive previous state-of-the-art methods specifically developed computing h+ (Haslum et al., 2012; Pommerening &
Helmert, 2012). However, Table 3 shows basic IP(T + ) model least competitive
A* LM-cut enhanced Gefen Brafmans pruning methods delete-free instances
(Prune/GB12). IP(T + ) also significantly outperforms standard A* LM-cut (Table 3, LMcut/GB12 Table 2, FD/PH12).
hand, enhancing IP(T + ) landmark-based model reduction method results
significant improvement, IP(T + )+LM competitive previous methods except
HST/CPLEX.
IPe (T + ) model, includes enhancement described Section 4 reducing
model order compute h+ faster, performs well overall, competitive
previous methods. example, Table 4, IPe (T + ) highest coverage (or tied highest)
19/28 domains. Table 5, Figure 2, Figure 3 show IPe (T + ) HST/CPLEX
similar coverage 30-minute time limit, IPe (T + ) tends somewhat faster overall.
However, clear dominance relationship IPe (T + ) HST/CPLEX, since
domains IPe (T + ) clearly performs better (e.g., rovers, satellite, freecell) ,
domains HST/CPLEX performs better (e.g., airport, pegsol, scanalyzer, transport). Thus,
IP-based approach minimal landmark approaches seem complementary strengths
respect solving delete-free problems.
Aside coverage, Figure 3 shows many delete-free instances solved much faster
IPe (T + ) HST/CPLEX. difference solving easy delete-free instance 0.1
vs. 0.5 seconds may seem important need solve instance once. However,
speed difference IPe (T + ) HST/CPLEX easy delete-free instances
significant implication consider using h+ solvers heuristic functions A* -based
648

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

planners, may need solve delete-free problems many thousands times course
single A* search. result, see Section 7, A* using IPe (T + ) heuristic
significantly outperforms A* using HST/CPLEX heuristic.
order eliminate possibility CPU speed differences account qualitative improvements coverage obtained IP models compared previously published results, Table
2 includes column IPe (T + )/1min, result 1-minute runs IPe (T + ), Table
4 includes column IPe (T + )/5min, result 5-minute runs IPe (T + ) effect,
simulate machines run 1/5 1/6 (respectively) speed machine used
experiments Tables 2 4. offsets improvements single-core CPU
performance 2010-2015. coverage achieved IPe (T + )/1min (753) Table 2
higher solvers Table 2 given 5 minutes. Similarly, coverage
achieved IPe (T + )/5min (847) Table 4 higher solvers Table 4
given 30 minutes.
Therefore, overall, IPe (T + ) competitive previous state-of-the-art delete-free solvers,
results indicate direct computation h+ using integer programming viable approach,
least computing delete-free task once.
5.2 Comparison HST/CPLEX Minimal Seed Set Problem
assess performance best IP model, IPe (T + ) natural, delete-free task, also
compared IPe (T + ) HST/CPLEX set minimal seed set problem instances systems
biology (Gefen & Brafman, 2011). consist 22 instances originally evaluated Gefen
Brafman, well three additional versions 22 instances also provided
original authors, version uses different set action costs (Gefen & Brafman, 2011, p.
322), total 22 4 = 88 instances. solvers run 1 hour CPU time limit per
instance 2GB RAM limit.
Figure 4 shows scatter plot comparing runtimes problem instance. coverage
IPe (T + ) 87 instances, coverage HST/CPLEX 88 instances. one hand,
Figure 4 shows majority instances solved significantly faster IPe (T + ),
IPe (T + ) solves 22 instances 10 times faster HST/CPLEX. hand,
one instance HST/CPLEX 10 times faster IPe (T + ),
one instance solved 40.7 seconds HST/CPLEX solved within
time limit IPe (T + ) (The dre instance type 2 preprocessing Gefen & Brafman,
2011, p. 322).

6. Relaxations h+ Models
Although delete-free planning problems interesting right, main motivation
developing efficient IP model delete-free problems able use basis
heuristic function forward-state space search based domain-independent planner. far,
presented IP(T + ), basic IP model computes h+ , proposed IPe (T + ),
incorporates number enhancements which, shown experimental results Section
5, significantly increase scalability model provide new approach computing h+
competitive previous state-of-the-art methods. possible simply use IPe (T + )
heuristic function forward search based planner. However, shown Section 5,
computing h+ remains relatively expensive even using IPe (T + ), surprising, given
649

fiI MAI & F UKUNAGA

(Pommerening & Helmert, 2012, Table 2)
Domain (# problems)
airport(50)
blocks(35)
depot(22)
driverlog(20)
freecell(80)
grid(5)
gripper(20)
logistics00(28)
logistics98(35)
miconic(150)
no-mprime(35)
no-mystery(30)
openstacks-opt08(30)
pathways-noneg(30)
pipes-notankage(50)
pipes-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
tpp(30)
trucks(30)
zenotravel(20)
Total coverage (876)
# Best Domains

IP(T + )

IP(T + )+LM

IPe (T + )

IPe (T + )/1min

FD/PH12

BC/PH12

BnB/PH12

IDA*/PH12

solved
22
35
6
14
11
0
20
24
8
150
15
15
2
30
8
5
50
40
31
11
30
14
541
7

solved
36
35
19
14
17
4
20
28
21
150
20
21
30
30
13
9
50
40
30
24
30
14
655
9

solved
36
35
21
15
80
5
20
28
27
150
31
30
30
30
11
9
50
40
34
30
30
20
762
19

solved
35
35
21
14
80
5
20
28
24
150
30
28
30
30
10
9
50
40
34
30
30
20
753
15

solved
34
35
7
14
6
1
20
23
9
150
27
26
5
5
17
10
50
13
6
13
7
13
491
5

solved
50
35
5
2
1
1
20
26
7
150
14
16
0
4
3
2
50
12
6
12
3
8
427
5

solved
50
35
14
15
2
2
20
28
16
150
27
28
5
5
18
9
50
19
8
23
9
13
546
7

solved
50
35
14
15
3
2
20
28
15
150
26
28
4
5
19
10
50
19
9
24
9
13
548
9

Table 2: Coverage (# instances solved) delete-free problems (exact computation h+ ).
5-minute time limit (except IPe (T + )/1min run 1-minute time limit), 2GB
RAM. Comparison data Table 2 paper Pommerening Helmert (2012). #
Best domains number domains solver achieves highest coverage
(including ties).
(Gefen & Brafman, 2012, Table 2)
Domain (# problems)
blocks(35)
depot(22)
driverlog(20)
freecell(80)
gripper(20)
logistics00(28)
logistics98(35)
miconic(150)
no-mystery(30)
pipesworld-notankage(50)
pipesworld-tankage(50)
rovers(40)
Total coverage (560)
# Best Domains

IP(T + )

IP(T + )+LM

IPe (T + )

LM-cut/GB12

Prune/GB12

solved
35
8
14
12
20
24
8
150
21
11
7
40
350
4

solved
35
19
14
20
20
28
23
150
23
17
9
40
398
6

solved
35
21
15
80
20
28
28
150
30
17
9
40
473
11

solved
35
7
14
6
20
23
10
150
26
17
10
13
331
5

solved
35
12
15
2
20
28
16
150
26
9
9
23
345
5

Table 3: Coverage (# instances solved) delete-free problems (exact computation h+ ).
30-minute time limit, 2GB RAM. Comparison data Table 2 paper Gefen
Brafman (2012).

650

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

(Haslum et al, 2012,
Table 2)
IP(T + )

Domain (# problems)
airport(50)
barman-sat11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-sat08(30)
floortile-sat11(20)
freecell(80)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
nomystery-sat11(20)
parcprinter-08(30)
pegsol-08(30)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
sokoban-sat08(30)
transport-sat08(30)
trucks(30)
visitall-sat11(20)
woodworking-sat08(30)
zenotravel(20)
Total coverage (1041)
# Best Domains

solved
22
7
35
8
14
1
19
12
20
8
24
150
20
11
30
25
11
7
50
40
31
10
25
2
30
8
29
15
664
7

IP(T + )+LM

solved
40
8
35
19
14
5
20
20
20
23
28
150
23
13
30
24
17
9
50
40
31
10
29
3
30
7
30
15
743
10

IPe (T + )

solved
39
9
35
21
15
30
20
80
20
28
28
150
34
19
30
26
17
9
50
40
34
10
29
7
30
8
30
20
868
19

HST/CPLEX

solved
50
20
35
20
14
30
12
76
20
20
28
150
31
7
30
30
24
10
50
32
14
21
30
15
30
16
29
14
858
16

IPe (T + )

HST/CPLEX

5min

5min

solved
36
6
35
21
15
30
19
80
20
27
28
150
31
19
30
25
11
9
50
40
34
9
29
6
30
7
30
20
847
16

solved
50
20
35
20
14
30
12
48
20
18
28
150
26
4
30
30
17
10
50
31
11
16
30
12
30
10
29
12
793
12

ML/HST12

BC/HST12

solved
50
18
35
18
13
27
12
17
20
15
27
150
28
5
30
30
20
15
50
18
8
15
30
6
30
2
19
13
721
10

solved
50
5
35
12
8
11
9
0
20
6
27
99
17
4
30
30
9
6
50
19
5
4
30
6
30
0
9
10
541
8

Table 4: Coverage (# instances solved) delete-free problems (exact computation h+ ).
30-minute time limit (except IPe (T + )/5min HST/CPLEX/5min run 5minute time limit), 2GB RAM. Comparison data Table 2 paper Haslum et al.
(2012).

651

fiI MAI & F UKUNAGA

IP(T + )/30min

Domain (# problems)
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best Domains

solved

22
8
35
8
14
2
1
20
12
0
20
8
24
150
20
21
13
5
3
0
30
20
2
30
25
13
11
7
50
40
31
10
7
29
20
13
4
0
30
20
30
20
15

time
253.97
1616.97
0.08
151.07
19.05
294.94
525.76
4.67
130.82
0
0.02
194.01
12.21
0.08
202.01
187.66
180.88
114.55
506.63
0
0.08
0.06
529.75
1.50
229.13
360.87
370.96
154.58
0.03
11.77
35.88
306.24
442.44
34.12
39.14
256.03
289.63
0
1.94
3.97
2.04
2.40
35.54
843
14

IP(T + )+LM/30min
solved

40
8
35
19
14
20
13
20
20
4
20
23
28
150
23
23
17
25
30
20
30
20
18
30
24
14
17
9
50
40
31
10
7
29
20
24
4
0
30
20
30
20
15

time
173.58
1522.41
0.00
12.75
15.77
201.74
179.16
1.76
259.96
5.59
0.02
89.77
0.03
0.09
221.48
129.89
224.40
82.48
0.08
0.04
0.04
0.03
172.21
1.13
39.01
105.91
198.51
22.87
0.02
0.34
38.40
292.41
439.54
0.61
0.47
55.71
45.00
0
0.70
1.76
0.52
0.47
36.69
1044
17

IPe (T + )/30min
solved

39
20
35
21
15
30
20
20
80
5
20
28
28
150
34
30
20
30
30
20
30
20
20
30
26
15
17
9
50
40
34
10
7
30
20
30
15
16
30
20
30
20
20

time sd
134.68 452.99
14.29 40.80
0.00 0.00
0.92 1.90
5.47 18.32
0.38 0.46
0.32 0.42
1.08 3.02
0.32 0.21
6.50 11.29
0.00 0.00
39.07 132.25
0.01 0.02
0.01 0.01
53.06 132.87
12.84 44.49
0.11 0.11
0.39 1.09
0.01 0.01
0.01 0.01
0.02 0.01
0.01 0.01
0.30 0.23
0.05 0.03
40.72 126.79
86.91 183.21
221.80 306.90
18.39 44.42
0.01 0.05
0.13 0.22
0.96 1.64
86.52 173.64
129.49 213.41
56.97 305.42
0.23 0.28
4.58 9.54
151.31 421.56
203.80 424.60
0.03 0.02
1.07 2.93
0.02 0.01
0.02 0.01
3.21 9.13
1214
34

HST/CPLEX/30min
solved

50
20
35
20
14
30
20
15
76
5
20
20
28
150
31
30
8
27
30
20
30
20
20
30
30
20
24
10
50
32
14
21
13
30
20
28
27
20
30
20
30
20
14

time sd
9.99 36.34
0.04 0.08
0.00 0.00
3.50 8.70
17.30 56.93
0.09 0.07
0.07 0.04
54.56 193.72
320.71 433.35
1.41 1.61
0.01 0.01
146.85 339.48
0.03 0.06
0.04 0.05
106.60 242.37
12.43 29.27
0.36 0.49
81.80 258.73
0.04 0.04
0.03 0.02
0.07 0.12
0.04 0.05
15.97 30.89
2.55 3.08
0.01 0.01
0.01 0.01
223.55 358.14
4.32 11.94
0.01 0.05
34.36 123.88
205.10 384.71
242.91 460.55
338.77 536.07
0.07 0.12
0.07 0.13
142.13 272.08
100.16 146.57
18.30 35.03
1.32 2.10
0.21 0.38
0.15 0.27
0.09 0.07
179.65 453.63
1202
31

Table 5: Detailed comparison IP(T + ), IP(T + )+LM, IPe (T + ), HST/CPLEX 1376 deletefree tasks (exact computation h+ ). 30-minute time limit, 2GB RAM. Coverage mean
standard deviation runtimes (average successful runs only, excludes unsuccessful runs).

652

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

IP(T + )/5min

Domain (# problems)
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best Domains

solved

22
0
35
6
14
1
0
20
11
0
20
8
24
150
15
15
11
5
2
0
30
20
0
30
22
9
8
5
50
40
31
7
4
28
19
11
3
0
30
20
30
20
14

time sd
0.82
0
0.08
29.35
17.33
25.40
0
4.74
73.07
0
0.02
20.45
11.64
0.08
28.01
9.35
37.85
66.39
16.89
0
0.07
0.05
0
1.53
74.55
131.78
5.53
31.71
0.03
10.26
29.87
57.26
34.20
27.27
22.22
12.42
8.64
0
1.60
3.80
1.98
2.17
4.02
790
13

IP(T + )+LM/5min

time
0.33
0
0.00
12.85
17.04
41.09
30.13
1.64
43.14
5.39
0.02
19.56
0.03
0.08
30.02
27.74
42.15
31.37
0.08
0.04
0.03
0.03
69.50
1.14
16.60
36.09
37.58
21.57
0.02
0.33
28.81
48.99
15.28
0.58
0.46
49.31
43.16
0
0.67
1.83
0.49
0.46
1.04

solved

36
0
35
19
14
16
11
20
17
4
20
21
28
150
20
21
14
24
30
20
30
20
15
30
23
12
13
9
50
40
30
7
4
29
20
24
4
0
30
20
30
20
14
994
17

IPe (T + )/5min
solved

36
20
35
21
15
30
20
20
80
5
20
27
28
150
31
30
20
30
30
20
30
20
20
30
25
13
11
9
50
40
34
9
6
29
20
30
13
13
30
20
30
20
20

time sd
4.10 23.83
13.60 38.23
0.00 0.00
0.93 1.95
5.86 19.83
0.39 0.47
0.31 0.41
1.05 2.93
0.30 0.20
6.35 11.05
0.00 0.00
13.94 36.73
0.01 0.02
0.01 0.01
14.91 51.52
12.04 41.80
0.10 0.10
0.37 1.00
0.01 0.01
0.01 0.01
0.01 0.01
0.01 0.01
0.29 0.22
0.04 0.03
16.24 37.01
16.65 20.17
16.55 52.18
14.65 34.93
0.01 0.04
0.13 0.23
1.03 1.79
41.39 56.98
52.22 66.89
0.25 0.33
0.23 0.28
4.60 9.66
11.60 24.25
28.55 42.33
0.03 0.02
1.11 3.08
0.02 0.01
0.02 0.01
3.38 9.70
1190
33

HST/CPLEX/5min
solved

50
20
35
20
14
30
20
14
48
5
20
18
28
150
26
30
8
24
30
20
30
20
20
30
30
20
17
10
50
31
11
16
9
30
20
24
24
20
30
20
30
20
12

time sd
9.44 34.94
0.04 0.08
0.00 0.00
3.47 8.57
17.03 56.02
0.08 0.07
0.07 0.04
2.80 4.21
60.87 80.81
1.37 1.54
0.01 0.00
34.28 67.68
0.03 0.06
0.04 0.05
11.27 23.42
12.88 30.69
0.34 0.46
12.20 35.90
0.04 0.04
0.03 0.02
0.07 0.11
0.04 0.05
15.07 28.61
2.47 2.92
0.01 0.01
0.01 0.01
21.71 35.68
4.31 11.93
0.01 0.05
12.55 28.76
16.92 40.99
21.91 45.07
23.20 54.45
0.07 0.12
0.07 0.13
46.47 81.70
56.58 87.08
17.45 32.84
1.74 3.16
0.21 0.38
0.14 0.26
0.08 0.07
20.86 65.51
1134
31

Table 6: Detailed comparison IP(T + ), IP(T + )+LM, IPe (T + ), HST/CPLEX 1376 deletefree tasks (exact computation h+ ). 5-minute time limit, 2GB RAM. Coverage mean
standard deviation runtimes (average successful runs only, excludes unsuccessful runs).

653

fiI MAI & F UKUNAGA

1400
1200

Instances solved

1000
800
600
400
IPe(T+)
HST/CPLEX

200
0
0.0001

IP(T+)+LM
IP(T+)

0.001

0.01

0.1

1

10

100

1000

Time (seconds)
Figure 2: Comparison IP(T + ), IP(T + )+LM, IPe (T + ), HST/CPLEX delete-free tasks
(exact computation h+ ). 30-minute time limit, 2GB RAM. cumulative number instances
(out 1376 instances Table 5) solved within ime seconds shown.
computing h+ NP-equivalent (Bylander, 1994). Haslum (2012) reported previous, baseline
results using direct computation h+ using hitting-set method proposed earlier work
(Haslum et al., 2012) heuristic A* , reported poor results. Although show Section
7 A* using IPe (T + ) performs well domains, using h+ directly heuristic A*
continues pose significant challenge. Thus, turn next relaxations IP(T + ) IPe (T + )
lower bounds h+ computed faster, making suitable admissible
heuristics forward-search planner IP models.
6.1 LP(T + ) LPe (T + ): LP Relaxations Delete-Relaxation (h+ ) Models
linear programming (LP) relaxations IP models obvious candidates tractable
alternatives computing h+ using IP(T + ) IPe (T + ). LP-relaxations trivially derived
IP models eliminating integer constraints variables, optimal cost
LP-relaxation lower bound optimal cost IP. denote LP relaxation
IP(T + ) LP(T + ) LP relaxation IPe (T + ) LPe (T + ) (see Table 1). case
problem domains integer action costs, ceiling LP costs used.
Although LPe (T + ) solved quickly, tight theoretical bounds gap IP(T + )
LP(T + ) gap IPe (T + ) LPe (T + ) difficult obtain proven
Betz Helmert (2009) exists constant c > 0 polynomial-time algorithm
computing lower bound h states s, h(s) ch+ , unless P = N P (i.e.,
h+ polynomial-time approximable constant factor c). Fortunately, worst-case
654

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

100*x

10*x

x

x/100

x/10

1

10

100

1000
100

IPe(T+)

10
1
0.1
0.01
0.001
0.001

0.01

0.1

1000

HST/CPLEX

Figure 3: Comparison runtimes IPe (T + ) HST/CPLEX 1376 delete-free instances (exact computation h+ , instances Table 5). 30-minute time limit, 2GB RAM. point represents problem instance. x-axis represents runtime HST/CPLEX, y-axis represents runtime
IPe (T + ). example, point diagonal (y = x) indicates IPe (T + ) solved problem represented point faster HST/CPLEX, point = x/10 line indicates IPe (T + )
solved problem represented point least 10 times faster HST/CPLEX. algorithm failed
solve instance within 30-minute time limit, runtime shown 1800 seconds.

theoretical approximation results necessarily apply real-world problem instances. fact,
experimental results show LP-relaxations often provide fast, accurate, lower
bounds h+ standard planning benchmark problems.
6.2 Time-Relaxation h+ Models
motivation embed computation h+ (or approximation thereof) admissible
heuristic A* , necessarily interested actual optimal delete-free plan + ,
cost plan (or approximation). particular, exact order actions
executed delete-relaxed plan matter, necessity time-related variables
brought question.
time-relaxation IP(T + ), IP(T + ) without constraints C5 C6, denoted
IPtr (T + ). LP relaxation IPtr (T + ) denoted LPtr (T + ). Table 1 summarizes relationships among models.
propositions actions task satisfy conditions, eliminating time-related
variables affect cost optimal solution IP(T + ). example, relaxed
causal AND/OR graph (Gefen & Brafman, 2012) task cycle,
decide values (p) (a) constraints C5 C6 IP(T + ) satisfied in655

fiI MAI & F UKUNAGA

1000

IPe(T+)

100
10
1

0.1
0.01
0.001
0.001

x
10*x
x/10
0.01

0.1

1

10

100

1000

HST/CPLEX

Figure 4: Runtime comparisons IPe (T + ) HST/CPLEX minimal seed set problem (88 natural,
delete-free instances Gefen & Brafman, 2011). 60-minute time limit, 2GB RAM. point represents
problem instance. algorithm failed solve instance within 60-minute time limit, runtime
shown 3600 seconds. coverage IPe (T + ) 87 instances, coverage HST/CPLEX
88 instances.

dependently values variables, case optimal costs IP(T + )
LP(T + ) optimal costs IPtr (T + ) LPtr (T + ), respectively.
Indeed, shall show experimentally Section 6.3 relaxation quite tight, i.e.,
IP(T + ) IPtr (T + ) often cost, IPtr (T + ) computed significantly faster IP(T + ). Similarly, LPtr (T + ), LPetr (T + ), IPetr (T + ), time-relaxations
LP(T + ), LPe (T + ), IPe (T + ), computed much faster non-time-relaxed
counterparts.
6.3 Experimental Evaluation LP Time Relaxation Gaps
evaluated quality LP(T + ), LPe (T + ), LPetr (T + ) linear programming bounds described comparing optimal costs computed bounds exact h+ values (computed
using IPe (T + )). used set 1376 instances Table 5. Table 7 shows mean ratio
optimal cost LP model h+ , instances h+ could computed using
IPe (T + ). perfect columns indicate fraction instances optimal cost
LP model equal h+ . Note used ceiling LP cost, since IPC benchmark
instances integer costs. stacked histogram representation data (aggregated
domains) classifies ratios optimal costs LP relaxations value h+
shown Figure 5.
expect variable-fixing constraints enhanced LPe (T + ) model would
tend increase value optimal solution LPe (T + ) compared optimal value
base LP relaxation, LP(T + ). addition, would also expect optimal value LPe (T + )
would tend greater optimal value time relaxation, LPetr (T + ). Table 7 shows
656

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

general, LPe (T + ) LPetr (T + ) LP(T + ). 10/43 domains, LPe (T + ) matches h+ perfectly,
i.e., LPe (T + )/h+ = 1. 20/43 domains, LPe (T + )/h+ 0.95. almost every single domain,
optimal LP value enhanced model LPe (T + ) significantly better (higher) basic
formulation LP(T + ), confirming variable elimination additional constraints serve
tighten LP bound. Thus, enhancements basic model described Section 4 provide
significant benefit beyond speedups demonstrated Section 5. time-relaxation
LPetr (T + ) usually close LPe (T + ), indicating time relaxation potentially
achieve good tradeoff computation cost accuracy (and fact, see later
Section 7, LPetr (T + ) performs quite well used heuristic A* ).
comparison, also evaluated ratio value LM-cut heuristic (Helmert &
Domshlak, 2009) h+ . Comparing average ratios lower bound h+ , see that:
LP(T + ) less informative LM-cut 31 domains, informative LM-cut 5
domains, equivalent 6 domains.
LPe (T + ) less informative LM-cut 16 domains, informative LM-cut
19 domains, equivalent 8 domains.
LPetr (T + ) less informative LM-cut 17 domains, informative LM-cut
17 domains, equivalent 9 domains.
Thus, LM-cut better approximation h+ basic LP-relaxation, LP(T + ),
LPetr (T + ) roughly equivalent LM-cut. Interestingly, LP-relaxation approach appears highly complementary cost-partitioning approach LM-cut,
LP-relaxation LM-cut informative roughly half cases
compared other.

LPe (T + )

1

1.0
[0.8-1.0)
[0.6-0.8)
[0.4-0.6)
[0.2-0.4)
[0.0-0.2)

0.9

Fraction Instances

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

LP

LP

e

LP

e
tr

Figure 5: Ratio optimal LP costs h+ , categorized buckets. [x:y) = fraction
instances ratio LP/h+ range [x:y). example, fraction instances
ratio optimal value LPetr (T + ) h+ range [0.8,1,0) approximately
0.24 (this stacked histogram based data Table 6.3).
657

fiI MAI & F UKUNAGA

LM-cut
perfect
1.00
.74
.74
0
.99
.97
.64
0
.89
.20
.77
.06
.80
.10
.94
.05
.29
0
.67
.40
1.00
1.00
.98
.40
.99
.92
1.00
1.00
.76
.20
.79
.28
.93
.50
.61
0
1.00
1.00
1.00
1.00
.99
.70
.99
.65
.87
0
.87
.13
.60
.26
.55
.05
.68
.02
.75
0
1.00
1.00
.87
.12
.95
.23
.95
.32
.97
.26
.94
.53
.96
.60
.98
.55
.87
.03
.84
.05
.92
0
.69
.10
.89
.13
.88
.10
.95
.50

LM-cut/h+

airport
barman-opt11
blocks
depot
driverlog
elevators-opt08
elevators-opt11
floortile-opt11
freecell
grid
gripper
logistics98
logistics00
miconic
no-mprime
no-mystery
nomystery-opt11
openstacks
openstacks-opt08
openstacks-opt11
parcprinter-08
parcprinter-opt11
parking-opt11
pathways-noneg
pegsol-08
pegsol-opt11
pipesworld-notankage
pipesworld-tankage
psr-small
rovers
satellite
scanalyzer-08
scanalyzer-opt11
sokoban-opt08
sokoban-opt11
tpp
transport-opt08
transport-opt11
trucks
visitall-opt11
woodworking-opt08
woodworking-opt11
zenotravel

LP(T + )
perfect
.46
.02
.17
0
.92
.20
.50
0
.85
.10
.21
0
.20
0
.95
.10
.12
0
.31
.20
1.00
1.00
.39
.02
.46
.03
1.00
1.00
.42
0
.39
0
.96
.60
.23
.03
1.00
1.00
1.00
1.00
.99
.66
.99
.70
.88
0
.90
.13
.26
.03
.20
0
.52
0
.58
0
.87
.82
.48
0
.82
.13
.94
.30
.96
.25
.33
.13
.28
.15
.28
.13
.08
0
.09
0
.40
0
.98
.65
.81
0
.80
0
.91
.25

LP(T + )/h+

LPe (T + )
perfect
.98
.94
.38
0
1.00
1.00
.92
.22
.87
.21
.65
0
.64
0
.95
.10
.94
.35
.81
.20
1.00
1.00
.89
.11
.99
.85
1.00
1.00
.71
.33
.77
.33
1.00
.95
1.00
.96
1.00
1.00
1.00
1.00
.99
.66
.99
.70
.92
.10
.98
.60
.64
.03
.65
0
.83
.38
.93
.55
1.00
1.00
.65
.35
.82
.21
.94
.75
.96
.71
.95
.73
.97
.80
.85
.26
.35
.08
.41
0
1.00
1.00
.98
.65
1.00
1.00
1.00
1.00
.92
.31

LPe (T + )/h+

LPetr (T + )
+
LPe
(T
)/h+
perfect
tr
.98
.38
1.00
.91
.83
.64
.62
.95
.92
.79
1.00
.88
.99
1.00
.63
.72
1.00
.88
1.00
1.00
.99
.99
.87
.98
.64
.65
.79
.91
1.00
.65
.82
.94
.96
.94
.97
.85
.35
.41
1.00
.97
1.00
1.00
.89

.70
0
1.00
.18
.05
0
0
.10
.23
.20
1.00
.05
.78
1.00
.17
.30
.95
1.00
1.00
1.00
.66
.70
0
.60
.03
0
.08
.36
1.00
.30
.20
.34
.29
.66
.75
.26
.03
0
1.00
.65
1.00
1.00
.30

Table 7: Gaps LP models h+ : mean ratio LP model h+ (on 1228
instances solved using IPe (T + ) shown. perfect columns indicate fraction instances
optimal cost LP model equal h+ .

658

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

Figure 6 compares runtimes CPLEX LP solver relaxed h+ models. LPe (T + )
significantly faster LP(T + ), solving many instances 2-10 times faster (and solving instances 10 times faster), demonstrating benefits enhanced model. comparison LPetr (T + ) LPe (T + ) shows using time relaxation results addition speedup
factor 2. additional speedup may seem significant solving
single LP instance takes fraction second, cumulative effects using LP models heuristic forward-search based planning significant, show Section 7,
results increased coverage using LPetr (T + ) heuristic A* , compared LPe (T + ).

100

100

10

10

LPetr(T+)

1000

LPe(T+)

1000

1

0.1

0.1

x
10*x
2*x
x/2
x/10

0.01
0.001
0.001

1

0.01

0.1

1

LP(T+)

10

100

x
10*x
2*x
x/2
x/10

0.01

1000

0.001
0.001

0.01

0.1

1

LPe(T+)

10

100

1000

Figure 6: Runtime comparisons relaxed h+ models. 1376 delete-free instances (exact computation
h+ , instances Table 5). 30-minute time limit, 2GB RAM. point represents problem instance.
left subfigure compare LP(T + ) vs LPe (T + ), showing impact enhancements basic LP
model, right subfigure compares LPe (T + ) vs LPetr (T + ), showing impact time relaxation.
algorithm failed solve instance within 30-minute time limit, runtime shown 1800
seconds.

7. Cost-Optimal Planners Using h+ -Based Heuristics
embedded IP LP models introduced far A* -based, cost-optimal
forward search planner (our planner implementation, uses propositional representation
internally) evaluated performance. Note particular experiment limited admissible heuristics whose value bounded h+ . later results Section 8 9 include
heuristics necessarily bounded h+ . Specifically, evaluated following
solver configurations:
A* /IP(T + ) : A* basic delete-free IP model IP(T + ) heuristic.
A* /IPe (T + ) : A* enhanced delete-free IP model IPe (T + ) heuristic.
A* /LPe (T + ) : A* LP relaxation enhanced delete-free IP model IPe (T + )
heuristic.
A* /LPetr (T + ) : A* LP relaxation time-relaxed, enhanced delete-free IP model
IPe (T + ) heuristic.
659

fiI MAI & F UKUNAGA

hsp/HST/CPLEX : A* heuristic hitting-set based h+ solver HST/CPLEX
(Haslum et al., 2012) using CPLEX solve hitting set instances (hsp planner provided
Patrik Haslum).
FD/hmax : Fast Downward using hmax heuristic (Bonet & Geffner, 2001).
FD/LM-cut : Fast Downward using landmark cut heuristic (Helmert & Domshlak, 2009)
(the standard seq-opt-lmcut configuration)
per standard IPC sequential optimal track settings, solver configurations run
30 minute time limit per problem 2GB RAM limit. set 1376 instances IPC1998IPC-2011 used. planner currently handles STRIPS subset PDDL action costs.
Table 8 compares coverage heuristics. Figure 7a shows cumulative coverage
(out 1376) solved function time solver configurations compared Table 8,
Figure 7b shows cumulative coverage function number node evaluations (calls
heuristic function A* ).
compare IP/LP-based A* -heuristics planners, note significant implementation-level differences heuristic function affect execution
speed. example, Fast Downward uses multi-valued SAS+ representation (Backstrom & Nebel,
1995) internally represent states, planner uses STRIPS propositional representation,
significant differences internal data structures implementation details. Thus,
results used qualitative comparisons.
Table 8 shows A* /IP(T + ), uses basic IP(T + ) model, worst coverage
among IP models (403), comparable A* /HST/CPLEX(398). noted Haslum
(2012), straightforward use h+ heuristic unsuccessful (even worse FD using
hmax , coverage 540) cost computing h+ search node high.
However, shown Section 5, solving IPe (T + ) IP model significantly faster
IP(T + ) A* /HST/CPLEX. makes much viable heuristic function A* ,
result, A* /IPe (T + ) coverage 635, significantly outperforming A* /HST/CPLEX
well FD/hmax.
shown Section 6.3, LP relaxations IP models provide relatively tight lower
bounds h+ . Since LP models solved much faster IP, quite effective
used heuristics A* . Thus, A* /LPe (T + ), uses LP-relaxation enhanced
IPe (T + ) model, coverage 696, A* /LPetr (T + ), uses LP-relaxation
time-relaxed, enhanced IP model, coverage 705.
Section 6.3, showed LPe (T + ) LPetr (T + ) models complementary LMcut respect informativeness, suggests least respect search efficiency,
LP models competitive LM-cut. Figure 7b shows fact, A* /LPe (T + )
A* /LPetr (T + ) tend search quite efficiently, seen lines
LM-cut line (i.e., problems solved using given number evaluations) 105 106 node evaluations, point overtaken LM-cut line.
informativeness comparison Section 6.3 showed LP models comparable complementary LM-cut respect informativeness, FD/LM-cut outperforms A* /LPetr (T + )
A* /LPetr (T + ) domains. LM-cut implementation Fast Downward
often significantly faster current implementation LP-based heuristics. Nevertheless, several domains (freecell, parcprinter-08, parcprinter-opt11, satellite, trucks, visitall),
660

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

Domain (# problems)
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best domains

FD/hmax

FD/LM-cut

hsp/HST/CPLEX

A* /IP(T + )

A* /IPe (T + )

A* /LPe (T + )

solved
21
4
18
4
9
15
13
4
15
2
7
2
10
50
23
17
8
7
19
14
14
10
0
4
27
17
16
7
49
6
6
9
6
27
20
6
11
6
7
9
9
4
8
540
15

solved
28
4
28
7
14
22
18
7
15
2
7
6
20
141
23
16
14
7
19
14
19
14
3
5
27
17
17
8
49
7
7
15
12
30
20
6
11
6
10
11
17
12
13
748
36

solved
24
0
17
1
7
3
1
1
19
1
2
3
10
79
15
15
8
5
7
2
19
14
0
4
17
4
9
6
19
4
5
5
2
6
3
5
7
2
3
15
14
8
7
398
0

solved
14
0
19
2
9
0
0
2
8
0
4
3
16
137
10
5
8
0
2
0
19
14
0
5
1
0
3
2
43
7
8
5
2
3
1
5
2
0
7
9
12
7
9
403
0

solved
24
0
27
7
10
9
7
4
54
2
5
5
19
140
20
15
14
7
10
5
21
16
2
5
10
2
10
8
48
7
10
5
2
17
13
6
7
2
13
10
17
11
9
635
13

solved
25
0
28
7
11
13
10
6
44
2
6
6
20
140
18
13
14
7
11
6
20
16
1
5
26
16
12
7
48
7
10
8
5
23
19
6
9
4
15
16
16
10
10
696
14

+
A* /LPe
tr (T )

solved
25
0
28
7
13
13
10
7
43
2
6
6
20
141
17
12
14
7
11
6
20
16
1
5
26
16
13
7
48
7
10
8
5
25
19
6
10
5
15
16
17
11
11
705
17

Table 8: Comparison forward search (A* ) planners, part 1: Number problems solved 30
minute, 2GB RAM limit using A* IP/LP models bounded h+ (Sections
3-7) heuristic functions. Comparison Fast Downward hmax , Fast Downward
Landmark Cut, hsp planner using HST/CPLEX (Haslum et al., 2012) compute h+ ,
heuristic function.

A* /LPetr (T + ) achieves higher coverage FD/LM-cut. Thus, A* /LPetr (T + ), best
model among bounded h+ , considered fairly powerful, admissible heuristic function forward-state search based planning.
661

fiI MAI & F UKUNAGA

800
700

Instances solved

600
500
400
300
FD/LMcut
A*/LPetr(T+)
A*/LPe(T+)
A*/IPe(T+)
FD/hmax

200
100
0
0.1

1

10

100

1000

Time (seconds)
(a) Cumulative number problems solved (out 1376) vs time (30 minute time limit).

800
700

Instances solved

600
500
400
300
FD/LMcut
A*/LPetr(T+)
A*/LPe(T+)
A*/IPe(T+)
FD/hmax

200
100
0

1

10

100

1000

10000 100000 1e+06

1e+07

1e+08

Evaluations
(b) Cumulative number problems solved (out 1376) vs number search nodes evaluated (30 minute
time limit).

Figure 7: Comparison forward search (A* ) planners, part 1 ( heuristics bounded
h+ ).

662

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

8. Incorporating Counting Constraints
far, concentrated efficient computation h+ well relaxations h+ ,
models far bounded h+ . However, IP model extended
constraints consider delete effects. adding variables constraints related delete effects
actions, model also calculate lower bounds number times action must
applied. New variables defined follows:
A, N (a) {0, 1, } : N (a) = n iff used n times.
p P, G(p) {0, 1} : G(p) = 1 iff p G.
G(p) auxiliary variable similar I(p). Furthermore, extended model, meaning
U (a) {0, 1} slightly modified mean action used least optimal
solution (in basic model proposed Section 3, pure delete-free model, U (a)
denoted whether used exactly optimal solution).
New constraints defined follows:
(C7) A, N (a) U (a).
P
P
(C8) p P, G(p) + as.t.ppredel(a) N (a) I(p) + as.t.padd(a) N (a),

predel(a) = pre(a) del(a). Finally, objective function modified minimize
P
aA c(a)N (a). Given planning task , use IPc (T ) denote IP problem adds
new variables constraints IP(T + )
idea types constraints previously proposed several times (for SAS+
formulation), correspond action order relaxation van den Briel et al. (2007), state
equation heuristic Bonet (2013), net change constraints Pommerening et al. (2014).
Intuitively, final constraint states number uses actions adding p must greater
equal number uses actions requiring deleting p time feasible
plan . feasible plan STRIPS planning task always satisfies condition. Hence,
task feasible plan , clearly derive feasible solution IPc (T )
cost . addition this, stronger proposition proved modifications
models enhancements Section 4.
Proposition 7. Given task , feasible plan = (a0 , , ) , exists feasible
solution IPc (T ) cost cost . addition this, exists feasible solution IPc (T ) combination landmark extraction substitution, relevance
analysis, inverse action constraints cost cost .
Proof. Let + delete relaxation subsequence plan extracted Algorithm 3.
First show subsequence + feasible delete-free plan + , show
assignment derived + satisfies constraints.
+
+
+
+
use (a+
0 , , ) denote elements . show feasible , assume
+
+
a+
first infeasible action . Let p proposition p pre(ai ) p 6
+
+
I((a0 , , ai1 )). Since valid feasible plan , delete-relaxation entire sequence
valid feasible plan + . Hence, a+
feasible, Algorithm 3
+
skipped actions add p ai applied. Since line 5 Algorithm 3 equal
+
I((a+
0 , , ai1 )) i, skipped actions add p satisfy add(ai ) \ 6= , thus
663

fiI MAI & F UKUNAGA

Algorithm 3 Extracting subsequence = (a0 , , ) (for proof Proposition 7)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:

+ (); // empty
I;
= a0 , ,
Let delete-relaxation a.
relevant + add(a ) \ 6=
append end + ;
add(a );
end
end
return + ;

irrelevant + . However contradicts definition relevance analysis
+
+
fact a+
relevant. Similar argument, G I( ). Hence valid feasible
plan + .
Define assignment F IPc (T ) as:
VF := VF + variable V defined IP(T + ), F + assignment
derived + IP(T + ),
N (a)F := (the number occurrences ) A.
assignment F clearly satisfies constraints C1 C6. assignment F also satisfies
constraint C8 since valid plan , F satisfies constraint C7 since U (a)F = 0
included . Hence F feasible solution IPc (T ) cost .
addition, F also feasible solution IPc (T ) combination landmark extraction
substitution, relevance analysis, inverse action constraints. see checking
feasibility F type modified constraints independently. F satisfies
modified constraints, satisfies combination constraints.
F satisfies constraints added landmark extraction substitution (i.e. substituting 1 variables corresponding landmarks) since + valid feasible plan + . F also
satisfies constraints added relevance analysis (i.e. substituting 0 irrelevant actions
propositions) since + contains relevant actions. Finally, show
P F satisfies inverse
action constraints similarly proof Proposition 6. inv(a,p) E(a , p)F = 0
P
U (a)F = 0 U (p)F = 0 hold, also inv(a,p) E(a , p)F 1
P
U (a)F = 0 U (p)F = 1 hold. addition, show inv(a,p) E(a , p)F = 0
U (a)F = U (p)F = 1. Assume exists inv(a, p) E(a , p)F = 1. Then,
constraint C3, U (a )F = 1, means also member + . Without loss generality,
assume applied applied + . Since add(a ) pre(a) definition inverse
actions, nothing new added state applying . line 5 Algorithm 3 equal

I((a+
0 , , )), contradicts add(ai ) \ 6= .
Unfortunately, counting constraints conflict dominated action elimination (Section 4.3)
zero cost immediate action application (Section 4.4). counting constraints used,
necessary disable zero cost immediate action application modify condition
dominated actions follows:
664

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

Definition 2 (modified dominated action definition). Given feasible task , action dominated action (i) add(a) add(a ), (ii) p pre(a ), p fact landmark
p I, (iii) c(a) c(a ), (iv) pre(a ) del(a ) pre(a) del(a).
longer use modified dominated actions make feasible plan , since fact
landmarks sometimes deleted achieved. However following fact proved.
Proposition 8. Given task , let = (a0 , , ) feasible solution . exists
feasible solution IPc (T ) combination landmark extraction substitution, relevance
analysis, inverse action constraints, modified dominated action elimination cost
equal less cost .
Proof. Recall dominated action elimination constraints substitute 0s U (a) dominated action a. contain modified dominated actions, proposition holds
due Proposition 7.
Otherwise, derive feasible solution using sequence actions made replacing
modified dominated actions corresponding dominating actions. Let
sequence. Note sum costs actions clearly less equal .
Let + relaxation subsequence extracted Algorithm 3. Since
prove delete-relaxation feasible plan + argument similar proof
Proposition 4, prove + also feasible plan + argument similar
proof Proposition 7.
+ feasible plan, derive feasible solution IPc (T ) constraints
proof Proposition 7. solution satisfies constraints C1 C6
combination landmark extraction substitution, relevance analysis, inverse action
constraints. satisfies constraint C7 U (a) = 0 included , satisfies
constraint C8 replacing dominated actions invalidate constraint C8 feasible
plan . also satisfies dominated action elimination constraints (i.e. U (a) = 0
dominated action a) since contain modified dominated action.




IPec (T ) LPec (T ) denote models constructed applying valid reductions
IPc (T ) LPc (T ) respectively. LP time relaxations IP(T + ) described Section 6

applied IPc (T ) well, LPectr (T ) time-relaxed, LP-relaxation enhanced

IPec (T ) model. Table 1 summarizes relationships among models.
8.1 Experimental Results Models Enhanced Counting Constraints


see impact adding counting constraints, evaluated informativeness LPec (T ),

LPectr (T ), LPe (T + ), LPetr (T + ) comparing values LM-cut heuristic values


(Helmert & Domshlak, 2009). Table 9 shows values LPec (T ), LPectr (T ), LPe (T + ),
LPetr (T + ) multiple LM-cut values (means domain shown). Note
contrast Table 7, limited 1228 instances h+ could computed
exactly, Table 9 includes 1376 instances (because LM-cut values could computed
1376 instances).
majority domains, counting constraints result informative heuristic,

compared models without counting constraints, cases, LPe (T + ) LPec (T )

LPetr (T + ) LPectr (T ). sometimes possible optimal value LPe (T + ) larger
665

fiI MAI & F UKUNAGA





optimal value LPec (T ) LPetr (T + ) larger optimal value LPectr (T )
explained Section 8, additional constraints part IPe (T + )

incompatible IPc (T ) excluded IPec (T ), resulting different LP polytopes
LP-relaxations.
Next, see impact adding counting constraints forward-search planning using


delete-relaxation LP models, compare A* /LPec (T ) A* /LPe (T + ), A* /LPectr (T )
A* /LPetr (T + ). Coverage instances previous experiment shown Table 10.
tradeoff improved search efficiency due additional informativeness
heuristic provided counting constraints, additional time required solve LPs
(because additional constraints make LP difficult solve). Table 10 shows

overall effects enhancing delete-relaxation model mixed. A* /LPec (T ) attains coverage

672 instances, lower coverage A* /LPe (T + ), A* /LPectr (T ) solves 716
problems compared 705 problems solved A* /LPetr (T + ). domains
adding counting constraints significantly improved coverage, including parcprinter, pathwaysnoneg, rovers, woodworking. hand, coverage dropped significantly elevators, freecell, openstacks result adding counting constraints. time relaxation seems
advantageous overall, resulting increase 672 instances A* /LPe (T + ) 716 problems
A* /LPetr (T + ).
Table 9 also shows value LMC-SEQ LP value (Pommerening et al., 2014). combination landmark constraints net change constraints operator-counting framework analogous combination delete-free model counting constraints,


interesting compare optimal LP values. LPec (T ) LPectr (T ) higher average value
LMC-SEQ 16 15 domains, respectively, LMC-SEQ higher value


LPec (T ) LPectr (T ) 17 domains. Thus, previous comparison LM-cut
LPe (T + ) LPetr (T + ) Section 6.2, delete-relaxation approach seems complementary
LMC-SEQ combination operator-counting framework. hand, comparing
results forward search based optimal planning using LP models, see FD/LMC


SEQ significantly higher coverage A* /LPec (T ) A* /LPectr (T ), well A* /LPec (T )

A* /LPectr (T ).

9. Automatic LP Model Selection
definitions models, know STRIPS planning task action
costs, relationships among IP models follows: IPtr (T + ) IPetr (T + ) IP(T + ) =

IPe (T + ) = h+ IPc (T ) = IPec (T ). LP relaxations, know LP(T + )




LPe (T + ), LPetr (T + ) LPe (T + ), LPectr (T ) LPec (T ), LPectr (T ) LPec (T ). Note

LPec (T ) always dominate LPe (T + ), dominated action elimination immediate action application eliminate different sets variables two LP models. Figure 1
illustrates dominance relationships among bounds.

time-relaxed LPetr (T + ) LPectr (T ) dominated non-time-relaxed models

LPe (T + ) LPec (T ), respectively, time-relaxed LPs significantly cheaper compute
non-relaxed counterparts.

Similarly, although IPec (T ) dominates IPe (T + ), possible LPe (T + ) larger

LPec (T ). Furthermore, two LPs optimal value, one solved faster
clearly preferable LPs must solved node A* search. Thus, set
666

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

airport
barman-opt11
blocks
depot
driverlog
elevators-opt08
elevators-opt11
floortile-opt11
freecell
grid
gripper
logistics98
logistics00
miconic
no-mprime
no-mystery
nomystery-opt11
openstacks
openstacks-opt08
openstacks-opt11
parcprinter-08
parcprinter-opt11
parking-opt11
pathways-noneg
pegsol-08
pegsol-opt11
pipesworld-notankage
pipesworld-tankage
psr-small
rovers
satellite
scanalyzer-08
scanalyzer-opt11
sokoban-opt08
sokoban-opt11
tpp
transport-opt08
transport-opt11
trucks
visitall-opt11
woodworking-opt08
woodworking-opt11
zenotravel

LMC-SEQ

LPe (T + )

1.00
2.23
1.07
1.10
1.04
1.02
1.01
1.05
2.64
1.09
1.00
1.00
1.00
1.00
1.00
1.01
1.03
1.36
1.00
1.00
1.08
1.05
1.00
1.53
1.34
1.33
1.45
1.32
2.60
1.23
1.00
1.00
1.01
1.15
1.11
1.43
1.11
1.08
1.00
1.50
1.04
1.05
1.00

.85
.51
1.00
1.43
1.01
.84
.80
1.01
3.14
1.20
1.00
.91
.99
1.00
.89
.98
1.07
1.61
1.00
1.00
1.00
1.00
1.04
1.13
1.09
1.10
1.18
1.27
1.00
.72
.83
.98
.98
1.01
1.01
.89
.49
.49
1.08
1.42
1.12
1.13
.96

+
LPe
tr (T )

.85
.51
1.00
1.42
.99
.82
.77
1.01
3.07
1.19
1.00
.90
.99
1.00
.78
.90
1.07
1.43
1.00
1.00
1.00
1.00
.99
1.13
1.05
1.10
1.16
1.26
1.00
.72
.75
.94
.98
1.00
1.01
.89
.49
.49
1.08
1.41
1.12
1.13
.94





LPe
c (T )

LPe
ctr (T )

.98
3.59
1.07
1.54
1.12
.71
.67
1.08
3.08
1.55
1.00
1.01
1.00
1.00
.82
.84
1.10
1.61
1.00
1.00
1.08
1.05
1.06
1.72
1.25
1.22
1.73
1.35
2.61
.81
.85
.97
.97
1.13
1.12
1.42
.18
.18
1.08
1.48
1.18
1.19
.94

.98
3.59
1.07
1.54
1.12
.71
.67
1.08
3.07
1.55
1.00
1.01
1.00
1.00
.82
.81
1.10
1.43
1.00
1.00
1.08
1.05
1.00
1.72
1.23
1.22
1.70
1.20
2.61
.81
.75
.93
.97
1.13
1.12
1.42
.18
.18
1.08
1.47
1.18
1.19
.93

Table 9: Optimal values LP models relative LM-cut value 1376 IPC instances. Means
domain shown. E.g., barman-opt11, mean LMC-SEQ value 2.23 times LM
cut value, LPe (T + ) LPetr (T + ) values 0.51 times LM-cut value, LPec (T )

LPectr (T ) values 3.59 times LM-cut value.

667

fiI MAI & F UKUNAGA

800
700

Instances solved

600
500
400
300

FD/LMC-SEQ
A*/Autoconf
FD/LMC
A*/LPe
ctr(T)
A*/LPetr(T+)
FD/SEQ

200
100
0
0.1

1

10

100

1000

Time (seconds)
(a) Cumulative number problems solved (out 1376) vs time (30 minute time limit).

800
700

Instances solved

600
500
400
300

FD/LMC-SEQ
A*/Autoconf

200

FD/LMC
A*/LPe
ctr(T)

100

A*/LPetr(T+)
FD/SEQ

0

1

10

100

1000

10000 100000 1e+06

1e+07

1e+08

Evaluations
(b) Cumulative number problems solved (out 1376) vs number search nodes evaluated (30 minute
time limit).

Figure 8: Comparison forward search (A* ) planners, part 2.

668

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

4 viable LP heuristics, none dominate others considering accuracy
time. best choice optimize tradeoff heuristic accuracy node expansion
rate depends problem instance. difficult choose best heuristic priori
general, know (1) whether worthwhile use counting constraints not, (2)
whether time-relaxation tight particular problem instance.
Thus, implemented simple mechanism automatically selecting LP used

problem works follows: First, compute LPe (T + ), LPec (T ), LPetr (T + ),

LPectr (T ) problem instance (i.e., root node A* search). select one
based following rule: Choose heuristic highest value, break ties choosing heuristic cheapest compute. Although cheapest heuristic could identified
according CPU time required compute heuristic, many problems, computations fast robust timing measurements, simply break ties order LPetr (T + ),


LPectr (T ), LPe (T + ), LPec (T ), ordering usually accurately reflects timing order.
mechanism makes simplistic assumption ranking behavior LP bounds
root node similar ranking LP bounds throughout search graph. sophisticated method heuristic selection may result better performance (c.f. Domshlak, Karpas,
& Markovitch, 2012), avenue future work.
9.1 Experimental Results Automated Model Selection Comparison
State-of-the-Art
compared A* using LP-based heuristics, including A* /autoconf, state-of-the-art heuristics. Specifically, compared:
FD/LM-cut : Fast Downward using landmark cut heuristic (Helmert & Domshlak, 2009)
(the standard seq-opt-lmcut configuration)
FD/LMC : Fast Downward using LP-model optimal cost partitioning landmark
cut constraints (Pommerening et al., 2014)
FD/SEQ : Fast Downward using lower-bound net change constraints (Pommerening et al.,
2014), corresponding state-equation heuristic Bonet (2013).
FD/OPT-SYS1, FD/PHO-SYS1, FD/PHO-SYS2 : Fast Downward using optimal cost partitioning constraints projections goal variables (OPT-SYS1), post-hoc optimization
constraints (PHO-SYS1, PHO-SYS2) (Pommerening et al., 2014).
FD/LMC-SEQ : Fast Downward using landmark cut net change constraints.
A* /LPe (T + ) : A* LP relaxation enhanced delete-free IP model IPe (T + )
(Section 4) heuristic.
A* /LPetr (T + ) : A* LP relaxation time-relaxed, enhanced delete-free IP model
IPe (T + ) heuristic.


A* /LPec (T ) : A* LP relaxation enhanced delete-free IP model counting

constraints IPec (T ) heuristic.


A* /LPectr (T ) : A* LP relaxation time-relaxed, enhanced delete-free IP model

counting constraints IPec (T ) heuristic.
669

fiI MAI & F UKUNAGA

9.1.1 C OVERAGE R ESULTS
coverage results (number problems solved) shown Tables 10. time spent
root node A* /autoconf LP model selection included runtimes, also counts
30-minute runtime limit. Figures 8a-8b show cumulative number instances solved
function number time number node evaluations, respectively (for legibility,
subset algorithms included Figures 8a-8b). Table 11 shows summary total coverage
results forward-search configurations included Tables 8 10.
results indicate automatic LP model selection significantly boosts performance
A* -based planner compared relying single LP model. A* /autoconf achieved coverage 761 1376 instances, significantly better 4 individual components.
Furthermore, A* /autoconf attained higher coverage solver configurations Table
10 except FD/LMC-SEQ (Pommerening et al., 2014), solved 781 instances. Note
A* /autoconf higher coverage FD/LMC-SEQ 11/43 domains (floortile-opt11, freecell,
grid, logistics98, nomystery-opt11, pathways-noneg, rovers, satellite, trucks, woodworking-opt08,
woodworking-opt11).
9.1.2 ACCURACY A* / AUTOCONF ODEL ELECTION
analyzed accuracy model selection evaluating performance A* /autoconf
problem instance vs performance four component models. coverage
considered, 96.4% instances, A* /autoconf made correct decision respect
coverage, model selection A* /autoconf deemed correct either A* /autoconf
solved problem instance, none 4 components solved problem instance.
hand, runtimes considered well coverage, 83.0% instances, A* /autoconf
made correct decision, selection deemed correct A* /autoconf selected
model best runtime (including ties), none 4 components solved problem

instance. baseline, LPectr (T ), best coverage among component models,
correct choice according criterion 49.9% time. Mistakes selections made
A* /autoconf seen Table 10 coverage results example, woodworking-opt11

domain, A* /autoconf solved 18 instances compared 20 instances solved LPectr (T ). Thus,
significant room improvement runtimes considered addition coverage,
improving model selection using machine learning techniques direction future work.

10. Discussion Conclusion
paper proposed new, integer-linear programming formulation delete relaxation h+
cost-optimal, domain-independent planning. started basic IP model IP(T + ),
showed enhanced model IPe (T + ), incorporates landmark-based variable reduction,
relevance analysis, action elimination, competitive previous methods solving deletefree versions standard IPC planning benchmarks tasks (i.e., exact computation h+ ).
results embedding IP model heuristic function A* -based forward search
planner confirmed plain IP(T + ) model practical (coverage 403/1367 instances
vs. 540 Fast Downward using hmax ). However, showed IPe (T + ) model,
uses variable reduction methods reduce size IP models exactly computes h+ ,
performed much better, coverage 635 instances. According summary results
670

fiFD/SEQ

A* /LPe (T + )

+
A* /LPe
tr (T )

22
4
28
7
12
11
9
2
15
1
7
4
16
50
21
15
12
7
19
14
15
11
5
4
27
17
14
8
49
6
6
12
9
24
19
6
11
6
6
16
10
5
9
571
10

28
4
27
7
13
19
16
2
15
2
7
5
21
54
21
15
16
7
19
14
17
13
1
4
27
17
16
8
49
6
6
7
4
29
20
6
11
6
7
16
16
11
11
620
12

22
4
28
7
12
10
8
4
39
1
7
4
16
52
20
15
10
7
17
12
28
20
4
4
28
18
15
8
50
6
6
14
11
20
17
8
11
6
9
17
14
9
9
627
12

25
0
28
7
11
13
10
6
44
2
6
6
20
140
18
13
14
7
11
6
20
16
1
5
26
16
12
7
48
7
10
8
5
23
19
6
9
4
15
16
16
10
10
696
5

25
0
28
7
13
13
10
7
43
2
6
6
20
141
17
12
14
7
11
6
20
16
1
5
26
16
13
7
48
7
10
8
5
25
19
6
10
5
15
16
17
11
11
705
6

A* /autoconf

FD/PHO-SYS2

20
4
26
4
10
8
6
2
8
1
6
2
14
45
19
13
8
7
11
6
11
7
1
4
22
12
13
7
48
6
5
10
7
18
15
6
9
4
3
15
8
3
8
462
2



FD/PHO-SYS1

30
4
29
7
13
19
16
6
33
2
6
6
20
141
22
16
12
7
16
11
29
20
2
5
28
18
14
8
50
7
7
14
11
29
20
8
11
6
10
19
21
16
12
781
18

A* /LPe
ctr (T )

FD/OPT-SYS1

28
4
28
7
13
20
16
6
15
2
6
6
20
141
23
16
14
7
19
14
18
13
2
5
27
17
17
8
49
7
7
14
11
28
20
6
11
6
10
10
16
11
12
730
13



FD/LMC-SEQ

28
4
28
7
14
22
18
7
15
2
7
6
20
141
23
16
14
7
19
14
19
14
3
5
27
17
17
8
49
7
7
15
12
30
20
6
11
6
10
11
17
12
13
748
22

25
0
29
7
12
6
4
6
17
2
6
7
20
139
15
11
8
7
6
2
29
20
1
14
22
12
12
7
50
11
9
7
4
22
19
8
6
1
12
17
30
20
10
672
12

25
3
29
7
13
8
6
7
21
3
6
7
20
140
16
11
11
7
10
5
29
20
1
14
26
16
13
7
50
11
9
8
5
26
19
8
6
1
15
17
30
20
10
716
15

25
2
29
7
13
13
10
7
44
3
6
7
20
141
18
12
14
7
11
6
29
20
1
14
26
16
13
7
50
11
10
8
5
25
19
8
10
5
15
17
28
18
11
761
16

A* /LPe
c (T )

FD/LMC

Domain
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best domains

FD/LM-cut

N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

Table 10: Comparison forward search (A* ) planners, part 2: Number problems solved
30 minute, 2GB RAM limit using A* IP/LP models heuristic functions. Includes LP


models incorporate counting constraints (LPec (T ), LPectr (T ), Section 8), well A* /autoconf
(Section 9). Comparison Fast Downward using operator-counting LP models (Pommerening
et al., 2014).

671

fiI MAI & F UKUNAGA

Configuration
FD/LM-cut

# solved
748

FD/hmax
FD/SEQ
FD/PHO-SYS1
FD/PHO-SYS2
FD/LMC

540
627
571
620
730

FD/OPT-SYS1
FD/LMC-SEQ
A* /HST/CPLEX

462
781
398

A* /IP(T + )
A* /IPe (T + )
A* /LPe (T + )
A* /LPetr (T + )

A* /LPec (T )
*
e
/LPctr (T )
A* /autoconf

403
635
696
705
672
716
761

Description
Fast Downward (FD) using standard Landmark Cut heuristic
(seq-opt-lmcut)
FD using hmax heuristic
FD using SEQ LP heuristic (Pommerening et al., 2014)
FD using PHO-SYS1 LP heuristic (Pommerening et al., 2014)
FD using PHO-SYS2 LP heuristic (Pommerening et al., 2014)
FD using LP model optimal cost partitioning landmark constraints (Pommerening et al., 2014)
FD using OPT-SYS1 LP heuristic (Pommerening et al., 2014)
FD using LMC+SEQ LP heuristic (Pommerening et al., 2014)
hsp planner using A* h+ heuristic (Haslum et al., 2012; Haslum,
2012)
basic IP formulation h+
IP(T + ) enhancements Sections 4.1-4.6
LP relaxation IPe (T + )
LP relaxation time-relaxed model IPetr (T + )

LP relaxation IPec (T )

LP relaxation time-relaxed model IPectr (T )
Automated selection LP root node(Section 9)

Table 11: Summary coverage (# solved) 1376 IPC benchmark problems instances 30
minute time limit 2GB RAM (see Tables 8-10 detailed results)
Table 11, aggregate coverage IPe (T + ) comparable coverage obtained LPbased SEQ, OPT-SYS1, PHO-SYS1, PHO-SYS2 heuristics recently implemented using
operator-counting framework Pommerening et al. (2014). However, aggregate coverage
IPC benchmarks skewed miconic domain, SEQ, OPT-SYS1, PHO-SYS1,
PHO-SYS2 perform particularly poorly compared heuristics. miconic domain
included, IPe (T + ) competitive LP-based models. Note freecell
domain, A* IPe (T + ) heuristic solved 54/80 instances, significantly higher
methods, least 1 domain exact h+ computation using IPe (T + ) model
performs extremely well compared state-of-the-art heuristics.
showed gap optimal value LP relaxations IP models
h+ tended quite small (the gap often zero), suggesting LP relaxations,
computed much faster IP models, could used heuristic A* -based planning.
time-relaxation eliminates time-related constraints also proposed another way
reduce model order solvable faster. comparison LP-relaxed delete relaxation
models LM-cut (Helmert & Domshlak, 2009) heuristic values showed approaches
complementary respect closely approximate h+ . Thus, LP-relaxation
delete-free models provides novel, practical alternative approximating h+ . showed
A* search using LPe (T + ) (LP-relaxation delete-free task) LPetr (T + ) (time relaxed,
LP-relaxation delete-free task) significantly improves upon IP models, solving 696 705
instances, respectively, making usable practical heuristics.
major advantage LP-based heuristics relative ease additional constraints
added order obtain improved heuristics. showed counting constraints,
corresponding net change constraints proposed previous work (van den Briel et al., 2007;

Pommerening et al., 2014), could added LP model. resulting heuristic, LPectr (T )
mixed results, improving performance domains, degrading performance

domains, i.e., LPetr (T + ) LPectr (T ) complementary heuristics.
672

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL



Since dominance relationship among A* /LPe (T + ), A* /LPetr (T + ), A* /LPec (T )

*
/LPectr (T ), proposed A* /autoconf , simple method automatically selects among
4 heuristics computing 4 heuristic values root node using accurate heuristic
(breaking ties according speed). showed overall, A* /autoconf significantly improves upon
4 components, competitive landmark-cut heuristic, solving 761/1367 instances
achieving state-of-the-art performance several domains.
A* /autoconf lower total coverage compared Fast Downward using LMC-SEQ
LP-based heuristic (Pommerening et al., 2014), LP(T + )-based approach outperforms LMCSEQ several domains including freecell, pathways-noneg, rovers, satellite, trucks, woodworking. Although A* /autoconf includes LP models counting constraints consider
delete effects, note A* /LPetr (T + ), uses pure delete-free LP, performs quite well, obtaining higher coverage operator-count based heuristics Pommerening et al. (2014)
floortile, freecell, nomystery-opt11, satellite, trucks domains, counting constraints
required order A* using delete-relaxation based LPs achieve state-of-the-art
performance domains.
comparison optimal values counting-constraint enhanced delete-relaxation LP


models LPec (T ) LPectr (T ) optimal LP values LMC-SEQ model showed
complementary, class models outperforming roughly
number domains (Section 8.1). Thus, integrating two approaches single LP model
promising direction future work. recent survey LP-based heuristics planning,
Roger Pommerening (2015) noted delete-relaxation model incorporated
operator counting framework Pommerening et al. (2014) adding operator-counting variables
operator delete-relaxed problem promising direction future work. Note
Pommerening et al. (2014) approach use landmarks, used
different purposes. landmark constraints used Pommerening et al. (2014) used directly
operator counting constraints. contrast, approach uses landmarks order decrease
size IP/LP models delete-free task used purpose speeding
computation IP/LP models, i.e., landmark based reduction change optimal value
IP(T + ).

showed adding counting constraints consider delete effects (i.e., LPec (T )

LPectr (T )) improve performance domains, domains, coverage dropped
significantly. additional constraints make LP difficult solve,
increased search efficiency due tighter bound enough overcome increased cost
solving LP search node. A* /autoconf attempts address selecting models
counting constraints return higher value model without counting constraints root node, otherwise uses model include counting constraints
(i.e., LPe (T + ) LPetr (T + )). hand, strengthening delete-relaxation considering
delete effects active area research, recently, two frameworks allow flexible interpolation delete relaxation original model proposed.
Keyder, Hoffmann, Haslum (2014) propose approach adds new fluents represent
conjunctions fluents original planning task. Red-black planning (Domshlak, Hoffmann, &
Katz, 2015) framework separates state variables two groups red variables
relaxed, black variables relaxed. Combining flexible relaxation frameworks
IP approach developing principled approach deciding use counting
constraints avenue future work.
673

fiI MAI & F UKUNAGA

current implementation uses CPLEX solver naively, relying entirely default control
parameters. Systematically tuning improving implementation IP/LP models order
make better use incremental IP/LP solving capabilities promising direction future work.
Although shown LP models often compute h+ exactly, domains
significant gaps h+ optimal cost LP models. Improved
modeling techniques may allow tighter LP bounds. example, Constraint C6 uses straightforward
big-M encoding, may possible obtain tighter bounds using methods.
Furthermore, although solving IP node forward-search based planner previously considered impractical, shown IPe (T + ) model, computes h+
exactly, almost useful practical heuristic, improving techniques used solve IP
IPe (T + ) may result balance accuracy speed necessary practical general
purpose heuristic. example, significant performance improvements might obtainable improving use IP solver. example, contrast LP solvers, parallel speedups
often difficult obtain, IP solvers often sped significantly parallelization, current
IP solvers already provide parallel search algorithms (which use paper
limited experiments single threads). number cores per processor continues increase, possible cases, IP-based heuristics may become useful LP-based
heuristics.

Acknowledgments
Thanks Patrik Haslum assistance code computing h+ hsp f planner.
Thanks Florian Pommerening assistance code LP heuristic-based Fast Downward (Pommerening et al., 2014). Thanks anonymous reviewers numerous helpful suggestions significantly improved paper. research supported JSPS Grant-in-Aid
JSPS Fellows JSPS KAKENHI grant.

References
Backstrom, C., & Nebel, B. (1995). Complexity Results SAS+ Planning. Computational Intelligence, 11(4), 625655.
Betz, C., & Helmert, M. (2009). Planning h+ theory practice. KI 2009, pp. 916.
Springer.
Blum, A., & Furst, M. (1997). Fast Planning Planning Graph Analysis. Artificial Intelligence, 90(1-2), 281300.
Bonet, B. (2013). admissible heuristic SAS+ planning obtained state equation.
Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp.
22682274.
Bonet, B., & Castillo, J. (2011). complete algorithm generating landmarks. Proceedings
International Conference Automated Planning Scheduling (ICAPS).
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(1-2),
533.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets. Proceedings
European Conference Artificial Intelligence (ECAI), pp. 329334.
674

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

Bonet, B., & van den Briel, M. (2014). Flow-based heuristics optimal planning: Landmarks
merges. Proceedings International Conference Automated Planning
Scheduling (ICAPS).
Bylander, T. (1994). Computational Complexity Propositional STRIPS Planning. Artificial
Intelligence, 69(12), 165204.
Bylander, T. (1997). linear programming heuristic optimal planning. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 694699.
Cooper, M. C., de Roquemaurel, M., & Regnier, P. (2011). Transformation optimal planning
problems. Journal Experimental & Theoretical Artificial Intelligence, 23(2), 181199.
Dimopoulos, Y. (2001). Improved integer programming models heuristic search ai planning.
Proceedings 6th European Conference Planning (ECP), pp. 5057.
Domshlak, C., Karpas, E., & Markovitch, S. (2012). Online speedup learning optimal planning.
Journal Artificial Intelligence Research, 44, 709755.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: new systematic approach
partial delete relaxation. Artificial Intelligence, 221, 73114.
Gefen, A., & Brafman, R. (2011). minimal seed set problem. Proceedings International Conference Automated Planning Scheduling (ICAPS), pp. 319322.
Gefen, A., & Brafman, R. (2012). Pruning methods optimal delete-free planning. Proceedings
International Conference Automated Planning Scheduling (ICAPS), pp. 5664.
Haslum, P. (2012). Incremental lower bounds additive cost planning problems. Proceedings
International Conference Automated Planning Scheduling (ICAPS), pp. 7482.
Haslum, P. (2014a) Personal communication.
Haslum, P. (2014b). Hsp* code documentatoin http://users.cecs.anu.edu.au/
patrik/un-hsps.html..
Haslum, P., Slaney, J., & Thiebaux, S. (2012). Minimal landmarks optimal delete-free planning. Proceedings International Conference Automated Planning Scheduling
(ICAPS), pp. 353357.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats difference anyway?. Proceedings International Conference Automated Planning
Scheduling (ICAPS), pp. 162169.
Hoffmann, J., & Nebel, B. (2001). FF Planning System: Fast Plan Generation Heuristic Search. Journal Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. Journal
Artificial Intelligence Research, 22, 215278.
Imai, T., & Fukunaga, A. (2014). practical, integer-linear programming model deleterelaxation cost-optimal planning. Proceedings European Conference Artificial
Intelligence (ECAI).
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning landmarks. Proceedings
International Joint Conference Artificial Intelligence (IJCAI), pp. 17281733.
675

fiI MAI & F UKUNAGA

Katz, M., & Domshlak, C. (2010). Optimal admissible composition abstraction heuristics. Artificial Intelligence, 174(12-13), 767798.
Kautz, H., & Selman, B. (1992). Planning Satisfiability. Proceedings European Conference Artificial Intelligence (ECAI), pp. 359363.
Kautz, H. A., & Selman, B. (1996). Pushing envelope: Planning, propositional logic stochastic search. Proceedings National Conference Artificial Intelligence (AAAI), pp.
11941201.
Kautz, H. A., & Selman, B. (1999). Unifying sat-based graph-based planning. Proceedings
International Joint Conference Artificial Intelligence (IJCAI), pp. 318325.
Keyder, E., Richter, S., & Helmert, M. (2010). Sound complete landmarks and/or graphs.
Proceedings European Conference Artificial Intelligence (ECAI), pp. 335340.
Keyder, E., & Geffner, H. (2008). Heuristics planning action costs revisited. Proceedings
European Conference Artificial Intelligence (ECAI), pp. 588592.
Keyder, E. R., Hoffmann, J., & Haslum, P. (2014). Improving delete relaxation heuristics
explicitly represented conjunctions. Journal Artificial Intelligence Research, 50, 487533.
Mirkis, V., & Domshlak, C. (2007). Cost-sharing approximations h+. Proceedings
International Conference Automated Planning Scheduling (ICAPS), pp. 240247.
Pommerening, F., & Helmert, M. (2012). Optimal planning delete-free tasks incremental LM-cut. Proceedings International Conference Automated Planning
Scheduling (ICAPS), pp. 363367.
Pommerening, F., Roger, G., Helmert, M., & Bonet, B. (2014). LP-based heuristics costoptimal planning. Proceedings International Conference Automated Planning
Scheduling (ICAPS).
Pommerening, F., Roger, G., & Helmert, M. (2013). Getting pattern databases
classical planning. Proceedings International Joint Conference Artificial
Intelligence (IJCAI).
Rintanen, J. (2012). Planning satisfiability: Heuristics. Artificial Intelligence, 193, 4586.
Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning satisfiability: parallel plans algorithms plan search. Artificial Intelligence, 170(12-13), 10311080.
Robinson, N. (2012). Advancing Planning-as-Satisfiability. Ph.D. thesis, Griffith University.
Robinson, N., McIlraith, S. A., & Toman, D. (2014). Cost-based query optimization via AI planning.
Proceedings Twenty-Eighth AAAI Conference Artificial Intelligence, July 27 -31,
2014, Quebec City, Quebec, Canada., pp. 23442351.
Roger, G., & Pommerening, F. (2015). Linear programming heuristics optimal planning.
AAAI2015 Workshop Planning, Search, Optimization.
van den Briel, M. (2015) Personal communication.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristic
optimal planning. Proceedings International Conference Principles Practice
Constraint Programming (CP).
676

fiO N P RACTICAL , NTEGER -L INEAR P ROGRAMMING ODEL

van den Briel, M., & Kambhampati, S. (2005). Optiplan: planner based integer programming.
Journal Artificial Intelligence Research, 24, 919931.
van den Briel, M., Vossen, T., & Kambhampati, S. (2008). Loosely coupled formulation automated planning: integer programming perspective. Journal Artificial Intelligence
Research, 31, 217257.
Vossen, T., Ball, M. O., Lotem, A., & Nau, D. S. (1999). use integer programming models
AI planning. Proceedings International Joint Conference Artificial Intelligence
(IJCAI), pp. 304309.
Zhu, L., & Givan, R. (2003). Landmark extraction via planning graph propagation. Proceedings
ICAPS Doctoral Consortium, pp. 156160.

677

fiJournal Artificial Intelligence Research 54 (2015) 309-367

Submitted 03/15; published 11/15

PAGOdA: Pay-As-You-Go Ontology Query Answering
Using Datalog Reasoner
Yujiao Zhou
Bernardo Cuenca Grau
Yavor Nenov
Mark Kaminski
Ian Horrocks

yujiao.zhou@cs.ox.ac.uk
bernardo.cuenca.grau@cs.ox.ac.uk
yavor.nenov@cs.ox.ac.uk
mark.kaminski@cs.ox.ac.uk
ian.horrocks@cs.ox.ac.uk

Department Computer Science, University Oxford
Parks Road, Oxford OX1 3QD, United Kingdom

Abstract
Answering conjunctive queries ontology-enriched datasets core reasoning task
many applications. Query answering is, however, computationally expensive,
led development query answering procedures sacrifice either expressive
power ontology language, completeness query answers order improve
scalability. paper, describe hybrid approach query answering OWL 2
ontologies combines datalog reasoner fully-fledged OWL 2 reasoner order
provide scalable pay-as-you-go performance. key feature approach
delegates bulk computation datalog reasoner resorts expensive
OWL 2 reasoning necessary fully answer query. Furthermore, although
main goal efficiently answer queries OWL 2 ontologies data, technical
results general approach applicable first-order knowledge representation languages captured rules allowing existential quantification
disjunction head; assumption availability datalog reasoner
fully-fledged reasoner language interest, used black boxes.
implemented techniques PAGOdA system, combines datalog
reasoner RDFox OWL 2 reasoner HermiT. extensive evaluation shows
PAGOdA succeeds providing scalable pay-as-you-go query answering wide range
OWL 2 ontologies, datasets queries.

1. Introduction
Ontologies increasingly used rich conceptual schemas wide range application
domains (Staab & Studer, 2004). One widely used ontology languages OWL,
description logic based language standardised World Wide Web Consortium
(W3C) 2004 revised (as OWL 2) 2009 (Baader, Calvanese, McGuinness, Nardi,
& Patel-Schneider, 2003; Horrocks, Patel-Schneider, & van Harmelen, 2003; Cuenca Grau,
Horrocks, Motik, Parsia, Patel-Schneider, & Sattler, 2008). OWL ontology consists
set axioms, correspond first-order sentences containing unary binary
predicates (called classes properties OWL), structure axioms/sentences
restricted ensure decidability basic reasoning problems.
applications, main focus conceptual model itself, class subsumption key reasoning problem. increasing number applications, however,
main focus using conceptual model access data, often form RDF
c 2015 AI Access Foundation. rights reserved.

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

graph (Manola & Miller, 2004). data-centric applications key reasoning problem
answer conjunctive queries (CQs)sentences constructed function-free atoms using
conjunction existential quantification (Abiteboul, Hull, & Vianu, 1995)which
constitute core component standard query languages SQL SPARQL
(W3C SPARQL Working Group, 2013).
Conjunctive query answering ontology-enriched datasets is, however, high worstcase complexity (Glimm, Lutz, Horrocks, & Sattler, 2008; Eiter, Ortiz, & Simkus, 2012),
even measured respect size data (so called data complexity).
Although heavily optimised, existing systems query answering respect (RDF)
data unrestricted OWL 2 ontology process small medium size datasets
(Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2007; Moller, Neuenstadt, Ozcep, &
Wandelt, 2013; Wandelt, Moller, & Wessel, 2010; Kollia & Glimm, 2013). led
development query answering procedures sacrifice expressive power
ontology language completeness query answers order improve scalability.
former case (sacrificing expressive power), query answering procedures
developed various fragments OWL 2 conjunctive query answering tractable
respect data complexity, three fragments standardised so-called
profiles OWL 2 (Motik, Cuenca Grau, Horrocks, Wu, Fokoue, & Lutz, 2012). OWL 2
QL OWL 2 EL profiles based DL-Lite (Calvanese, De Giacomo, Lembo,
Lenzerini, & Rosati, 2007) EL (Baader, Brandt, & Lutz, 2005) families description
logics; OWL 2 RL profile corresponds fragment rule-based language datalog
(Grosof, Horrocks, Volz, & Decker, 2003; Dantsin, Eiter, Gottlob, & Voronkov, 2001).
Conjunctive query answering systems profiles shown highly scalable
practice (Bishop, Kiryakov, Ognyano, Peikov, Tashev, & Velkov, 2011; Wu, Eadon, Das,
Chong, Kolovski, Annamalai, & Srinivasan, 2008; Motik, Nenov, Piro, Horrocks, & Olteanu,
2014; Erling & Mikhailov, 2009; Rodriguez-Muro & Calvanese, 2012; Lutz, Seylan, Toman,
& Wolter, 2013; Stefanoni, Motik, & Horrocks, 2013). favourable computational
properties fragments make natural choice data-intensive applications,
also come expense loss expressive power, many ontologies used
applications captured profiles.
latter case (sacrificing completeness), query answering procedures
developed exploit scalable reasoning techniques, expense computing
approximate query answers (Thomas, Pan, & Ren, 2010; Tserendorj, Rudolph, Krotzsch,
& Hitzler, 2008; Wandelt et al., 2010; Bishop et al., 2011). cases, computed
answers sound (only correct answer tuples identified) incomplete (some correct
answer tuples may identified). One way realise procedure weaken
ontology falls within one OWL 2 profiles, use scalable
procedure relevant fragment. required weakening trivially achieved
simply discarding (parts of) out-of-profile axioms, sophisticated techniques may
try reduce even minimise information loss (Console, Mora, Rosati, Santarelli, & Savo,
2014). approach clearly sound (if answer tuple entailed weakened
ontology, entailed original ontology), incomplete general,
ontologies outside relevant profile, answer returned systems therefore
understood providing lower-bound correct answer; however, procedures

310

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

cannot general provide complementary upper bound even indication
complete computed answer (Cuenca Grau, Motik, Stoilos, & Horrocks, 2012).
paper, describe novel hybrid approach query answering combines
scalable datalog (or OWL 2 RL) reasoner fully-fledged OWL 2 reasoner provide
scalable performance still guaranteeing sound complete answers cases.
procedure uses datalog reasoner efficiently compute lower bound (sound
possibly incomplete) upper bound (complete possibly unsound) answers input query. lower upper bound answers coincide, obviously provide sound
complete answer. Otherwise, relevant subsets ontology data computed
guaranteed sufficient test correctness tuples gap
lower upper bounds. subsets computed using datalog reasoner,
typically much smaller input ontology data. Finally, fully-fledged
reasoner used check gap tuples w.r.t. relevant subset. still computationally expensive, load fully-fledged reasoner reduced exploiting
summarisation techniques inspired SHER system quickly identify spurious gap
tuples (Dolby, Fokoue, Kalyanpur, Kershenbaum, Schonberg, Srinivas, & Ma, 2007; Dolby,
Fokoue, Kalyanpur, Schonberg, & Srinivas, 2009), analysing dependencies
remaining gap tuples reduce number checks need performed.
key feature approach pay-as-you-go behaviour: bulk computational workload delegated datalog reasoner, extent
fully-fledged reasoner needed depend solely ontology, interactions
ontology, dataset query. Thus, even using expressive
ontology, queries often fully answered using datalog reasoner, even
fully-fledged reasoner required, relevant subset extraction, summarisation
dependency analysis greatly reduce number size reasoning problems. Moreover,
approach additional advantage lower bound answer tuples quickly
returned, even cases completion answer requires time consuming computations. Finally, although main goal efficiently answer queries OWL 2
ontologies datasets, technical results general approach
restricted ontology languages based description logics. precisely, given KR
language L captured first-order rules allowing existential quantification
disjunction head, want answer conjunctive queries,
assumption availability fully-fledged reasoner L datalog reasoner,
used black box.
implemented techniques PAGOdA system1 using RDFox datalog
reasoner (Motik et al., 2014) HermiT fully-fledged OWL 2 reasoner (Glimm,
Horrocks, Motik, Stoilos, & Wang, 2014),2 conducted extensive evaluation using
wide range realistic benchmark datasets queries. evaluation suggests
techniques eective providing scalable pay-as-you-go query answering: tests
4,000 queries 8 ontologies, none contained within
OWL profiles, 99% queries fully answered without resorting fullyfledged reasoner. Moreover, even fully-fledged reasoner used, relevant subset
1. http://www.cs.ox.ac.uk/isg/tools/PAGOdA/
2. Although techniques proved correct general conjunctive queries, practice limited
current query capabilities OWL 2 reasoners.

311

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

extraction, summarisation dependency analysis greatly reduced number size
reasoning problems: tests, size dataset typically reduced order
magnitude, often several orders magnitude, seldom required
single test resolve status gap tuples. Taken together, experiments show
PAGOdA provide efficient conjunctive query answering service scenarios requiring
expressive ontologies datasets containing hundreds millions facts, something
far beyond capabilities pre-existing state-of-the-art ontology reasoners.
remainder paper organised follows. Section 2 introduce key
concepts definitions. Section 3 present high-level overview approach.
Section 4 describe lower bound answers computed prove
sound, Section 5 describe upper bound answers computed prove
complete. Section 6 present technique reducing size
ontology dataset processed fully-fledged reasoner prove
preserves completeness. Section 7 present summarisation dependency analysis
optimisations prove preserve completeness. Section 8 describe
implementation techniques PAGOdA system discuss additional
optimisations. Finally, positioning work within state-of-the-art Section 9,
present extensive evaluation Section 10, draw conclusions Section 11.

2. Preliminaries
section briefly introduce rule-based first-order languages description logics
(DLs)a family knowledge representation formalisms underpinning OWL OWL 2
ontology languages (Baader et al., 2003).
use standard notions first-order logic constant, predicate, function,
term, substitution, atom, formula, sentence. also adopt standard definitions
(Herbrand) interpretation model, well (un)satisfiability entailment (written
|=) sets first-order sentences. denote ? nullary predicate false
interpretations. Formulas may also contain special equality predicate . assume
first-order knowledge base F function-free signature uses axiomatises
semantics usual way; is, F must contain following first-order sentences,
(EQ1) (EQ4) instantiated n-ary predicate P F 1 n:
8x1 , . . . , xn (P (x1 , . . . , xi , . . . , xn ) ! xi xi )

(EQ1)

8x, y(x ! x)

8x, y, z(x ^ z ! x z)

8x1 , . . . , xn , y(P (x1 , . . . , xi , . . . , xn ) ^ xi ! P (x1 , . . . , xi

(EQ2)
(EQ3)
1 , y, xi+1 , . . . , xn ))

(EQ4)

Finally, also exploit following notion homomorphism applicable sets
atoms, formulas substitutions. Given sets ground atoms , define
homomorphism mapping ground terms ground terms s.t.
(c) = c constant c S, P (t1 , . . . , tn ) 2 atom P (t1 , . . . , tn ) 2 S.
application homomorphism naturally extended ground atoms, ground
formulas ground substitutions, e.g. atom = P (t1 , . . . , tn ), = P (t1 , . . . , tn )
ground substitution , substitution {x 7! x | x 2 dom( )}.
312

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

2.1 Rule-Based Knowledge Representation
Rule languages well-known knowledge representation formalisms strongly connected ontology languages (Dantsin et al., 2001; Cal, Gottlob, Lukasiewicz, Marnette,
& Pieris, 2010; Bry, Eisinger, Eiter, Furche, Gottlob, Ley, Linse, Pichler, & Wei, 2007).
define fact function-free ground atom dataset finite set facts.
rule r function-free first-order sentence form
8~x, ~y (


x, ~y )
(~

x, ~y )
1 (~

^ ^

x, ~y )
n (~

!


_

i=1

9~zi 'i (~x, ~zi ))

(1)

atom dierent ? free variables ~x [ ~y , either

= 1 '1 (~x, ~z1 ) = ?,
1, 1 formula 'i (~x, ~zj ) conjunction atoms dierent
? free variables ~x [ ~zj .
conjunction
atoms 1 (~x, ~y ) ^ ^ n (~x, ~y ) body r, denoted body(r).
W
formula
9~
z
'
x, ~zi ) head r, denoted head(r). assume rules
(~
i=1
safe; is, every variable ~x mentioned body(r). brevity, universal quantifiers
omitted rules.
Rules form general able capture first-order rule languages
knowledge representation, including datalog (Abiteboul et al., 1995), existential rules
datalog (Cal et al., 2010), well datalog,_ (Alviano, Faber, Leone, & Manna,
2012b; Bourhis, Morak, & Pieris, 2013).
say rule r
disjunctive datalog head(r) contains existential quantifiers conjunction;
existential = 1;
datalog disjunctive datalog = 1.
knowledge base K = K [ DK consists finite set rules K dataset DK
predicate DK assumed occur K .
order simplify presentation technical results, sometimes restrict
knowledge bases particular normal form, specify next. say
rule r normalised one following forms,
1
x, ~zi ) single atom dierent ?:
(~
x, ~y )
1 (~
x, ~y )
1 (~
x, ~y )
1 (~

^ ^

x, ~y )
n (~

^ ^

x, ~y )
n (~

^ ^

x, ~y )
n (~

!?

(2)

! 9~z1 1 (~x, ~z1 )
!

x)
1 (~

_ _

(3)
x)
(~

(4)

knowledge base K [ DK normalised rules K normalised. restriction
normalised knowledge bases w.l.o.g. since every set rules form (1)
transformed polynomial time set normalised rules norm() conservative
extension given next. rule r 2 1 m, let ~xi tuple
313

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

free variables subformulas 9~zi 'i (~x, ~zi ) head(r), ~xi ~x. Furthermore,
let E'i fresh predicates arity |~xi | let C'i fresh predicates arity |~xi | + |~zi |
uniquely associated r i. Then, norm() consists following rules:3

x, ~y )
1 (~

^ ^

x, ~y )
n (~

!


_

E'i (~xi ),

(5)

i=1

E'i (~xi ) ! 9~zi C'i (~xi , ~zi ) 1 m,
C'i (~xi , ~zi ) !

1 atom

'i (~x, ~zi ) ! E'i (~xi ) 1 m,

'i (~x, ~zi ) ! C'i (~xi , ~zi ) 1 m.

(6)
'i (~x, ~zi ),

(7)
(8)
(9)

frequently use Skolemisation interpret rules Herbrand interpretations.
rule r form (1) existentially quantified variable zij , let fijr function
symbol globally unique r zij arity ~x. Furthermore, let sk substitution
sk (zij ) = fijr (~x) zij 2 ~zi . Skolemisation sk(r) r following
first-order sentence, slight abuse notation refer Skolemised rule:
x, ~y )
1 (~

^ ^

x, ~y )
n (~

!


_

'i (~x, ~zi )sk

i=1

Skolemisation sk() set rules obtained Skolemising individual rule
. extend definitions head body rules Skolemised rules naturally.
well-known Skolemisation entailment-preserving transformation.
2.2 Description Logics Ontology Languages
next present brief overview DLs underpinning W3C standard ontology
language OWL 2 (Horrocks, Kutz, & Sattler, 2006; Cuenca Grau et al., 2008). Typically,
predicates DL signatures restricted unary binary; former called atomic
concepts, whereas latter typically referred atomic roles. DLs typically provide
two special concepts ? (the bottom concept) > (the top concept), mapped
every interpretation empty set interpretation domain, respectively.
Every OWL 2 DL ontology normalised set axioms form given
left-hand-side Table 1 (Motik, Shearer, & Horrocks, 2009).4 Thus, w.l.o.g.,
define OWL 2 DL ontology finite set axioms form (O1)(O13) Table 1.
Every OWL 2 DL ontology must satisfy certain additional requirements order ensure
decidability reasoning (Horrocks et al., 2006). restrictions, however, immaterial
technical results.
normalised axiom corresponds single rule, given right-hand-side
Table 1. Concept ? translated special nullary predicate ?, whereas > translated
3. Although rules (5)(7) sufficient express normal form, also introduce rules (8)(9) order
facilitate computation upper bound query answers (see Sections 5.2 5.3).
4. convenience, omit axioms form v n R.B simulated v 9R.Bi ,
Bi v B Bi u Bj v ? 1 < j n Bi fresh concept.

314

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Axioms
dn
Ai v F
?
di=1
n


v
i=1
j=1 Bj
9R.A v B
v Self(R)
Self(R) v
R vS
R vS
R vT
RuS v?
v 9R.B
v R.B
v {a}
> v 8R.A

Rules
Vn
Ai (x) ! W
?
Vi=1
n


(x)
!
i=1
j=1 Bj (x)
R(x, y) ^ A(y) ! B(x)
A(x) ! R(x, x)
R(x, x) ! A(x)
R(x, y) ! S(x, y)
R(x, y) ! S(y, x)
R(x, z) ^ S(z, y) ! (x, y)
R(x, y) ^ S(x, y) ! ?
A(x) ! 9y(R(x, y) ^ B(y))
V
W
A(x) ^ m+1
i=1 [R(x, yi ) ^ B(yi )] !
1i<jm+1 yi yj
A(x) ! x
R(x, y) ! A(y)

(O1)
(O2)
(O3)
(O4)
(O5)
(O6)
(O7)
(O8)
(O9)
(O10)
(O11)
(O12)
(O13)

Table 1: Normalised DL axioms translation rules n, > 0, B
atomic concepts >, R, S, atomic roles.
ordinary unary predicate, meaning axiomatised. Let function
maps OWL 2 axiom corresponding rule Table 1, let
ontology. Then, (O) smallest knowledge base containing:
() 2 O;
rule A(x) ! >(x) atomic concept O;
rules R(x, y) ! >(x) R(x, y) ! >(y) atomic role R O.
Note since (O) knowledge base, must contain axioms equality
signature whenever required translate axiom O.
recent years, growing interest ontology languages favourable
computational properties, led standardisation RL, QL, EL
profiles OWL 2 (Motik et al., 2012). say ontology Horn = 1
axioms (O2) (O11). Additionally, say Horn ontology
RL contain axioms (O4), (O5), (O10).
QL contain axioms (O4), (O5), (O8), (O9), (O11), (O12); furthermore, axioms (O1) (O2) satisfy n 2 axioms (O3) satisfy = >.
EL contain axioms (O7), (O9) (O11). Additionally, say
EL ontology ELHOr? contain axioms (O4), (O5) (O8).
2.3 Conjunctive Queries
conjunctive query (CQ) formula q(~x) form 9~y '(~x, ~y ), '(~x, ~y )
conjunction function-free atoms. query Boolean |~x| = 0, atomic '(~x, ~y )
315

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

consists single atom |~y | = 0. simplicity, sometimes omit free variables
write q instead q(~x).
Let K knowledge base. tuple ~a constants possible answer q(~x) w.r.t. K
arity ~x constant ~a occurs K. Furthermore, say
possible answer ~a certain answer K |= q(~a); set certain answers denoted
cert(q, K). Note that, '(~x, ~y ) Boolean, set certain answers either empty
consists tuple length zero. treat unsatisfiability Boolean query
'(~x, ~y ) nullary falsehood symbol ?; query holds w.r.t. K K unsatisfiable.
CQs alternatively represented using datalog rules. end, query q(~x)
uniquely associated predicate Pq arity |~x| (where take P? = ?) set
Rq rules defined follows:

;
q=?
Rq =
(10)
{'(~x, ~y ) ! Pq (~x)} otherwise
Then, ~a 2 cert(q, K) K [ Rq |= Pq (~a). way, certain answers characterised
means entailment single facts.
Answering CQs w.r.t. knowledge bases computationally hard, decidability knowledge bases stemming OWL 2 DL ontologies remains open. Decidability
obtained ensuring ontology stays within one standardised profiles
OWL 2. restriction also ensures tractability respect data complexity,
makes profiles natural choice ontology language data-intensive applications.
standard language SPARQL 1.1 (W3C SPARQL Working Group, 2013) allows users
formulate CQs OWL 2 ontologies; however, ensure decidability reduce
complexity query answering, CQs interpreted SPARQL 1.1 ground semantics.
say possible answer ~a q(~x) = 9~y '(~x, ~y ) ground answer w.r.t. satisfiable
knowledge base K exists tuple ~e constants K K |= '(~a, ~e). Clearly,
every ground answer certain answer vice versa. denote ground(q, K)
set ground answers q w.r.t. K.
Many reasoning systems currently support SPARQL 1.1 hence compute ground(q, K)
given CQ q OWL 2 DL ontology K input. Additionally, systems
able compute certain answers q suitably restricted. precisely, say q
internalisable Kq = K [ Rq corresponds OWL 2 DL knowledge base. Internalisation
amounts transforming query ontology axiom typically referred
rolling-up DL literature (Horrocks & Tessaris, 2000).
paper, focus general problem computing certain answers CQ
w.r.t. knowledge base K, theoretical results generally applicable regardless
rule-based language K expressed.
2.4 Hyperresolution
Reasoning knowledge bases realised means hyperresolution calculus
(Robinson & Voronkov, 2001), briefly discuss next. treatment hyperresolution consider standard basic notions theorem proving (ground) clause
general unifier (MGU). Furthermore, treat disjunctions ground atoms
sets hence allow duplicated atoms disjunction. assume ?
316

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

occur clauses denote empty clause. Skolemisation sk(r)
normalised rule r logically equivalent clause containing atom dierent
? head(sk(r)) negation atom body(sk(r)), sometimes abuse
notation use sk(r) refer Skolemised rule corresponding clause.
Let C = 1 _ _ n _ 1 _ _ clause, j atoms
(possibly containing functional terms). Furthermore 1 n, let = _
positive ground clause. Finally, let MGU pairs , , 1 n. Then,
positive ground clause 1 _ _ _ 1 _ _ n hyperresolvent C 1 , . . . , n .
inference called hyperresolution step, clause C main premise.
Let K = K [ DK normalised knowledge base let C positive ground clause.
derivation C K pair = (T, ) tree, labeling function
maps node ground clause, v :
(1)

(v) = C v root;

(2)

(v) 2 DK v leaf;

(3) v children w1 , . . . , wn , (v) hyperresolvent sk(r) (w1 ), . . . , (wn )
rule r 2 K .
support , written support(), set facts rules participating hyperresolution steps . write K ` C denote hyperresolution derivation
C K. Hyperresolution sound complete: K unsatisfiable K ` ;
furthermore, K satisfiable K ` K |= ground atom .
2.5 Skolem Chase
Answering CQs knowledge base K = K [ DK K consists existential
rules realised using chase technique (Abiteboul et al., 1995; Cal, Gottlob, &
Kifer, 2013). paper, use Skolem chase variant (Marnette, 2009; Cuenca Grau,
Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang, 2013).
Skolem chase sequence K sequence sets ground atoms {B }i 0 ,
0
B = DK , B i+1 inductively defined follows:
B i+1 = B [ {head(sk(r)) | r 2 K ,

substitution, B |= body(r) }.

Skolem chase K, written ChaseK , defined 0 B .
key property Skolem chase computes universal Herbrand model
K, used database answering CQs. Formally, K satisfiable
?2
/ ChaseK ; furthermore, K satisfiable, ChaseK homomorphically embeddable
every Herbrand model K (seen set atoms). follows K satisfiable
q Boolean CQ K |= q ChaseK |= q.
Note ChaseK might contain infinitely many atoms. K datalog, however,
ChaseK guaranteed finite contains precisely facts logically entailed K.
case, often refer ChaseK materialisation K.

317

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

3. Overview
section provide high-level overview approach conjunctive query answering. assume availability two reasoners:
datalog reasoner sound complete answering conjunctive queries
datalog knowledge bases;
fully-fledged reasoner sound complete answering given class
conjunctive queries Q (which includes unsatisfiability query) w.r.t. knowledge
bases given ontology language L.
describe approach general form, make assumptions
two reasoners, treating black-box query answering procedures.
kind queries knowledge bases dealt using approach
ultimately depends capabilities fully-fledged reasoner. instance, OWL
2 DL reasoners typically process arbitrary OWL 2 DL knowledge bases; however,
query language limited internalisable queries. turn, scalability approach
ultimately depends much reasoning workload delegated datalog
reasoner; goal delegate bulk computation datalog reasoner
restrict (expensive) use fully-fledged reasoner bare minimum.
Here, rest paper, fix arbitrary normalised knowledge base
K = K [ DK . Given arbitrary query q (which may special unsatisfiability query)
containing symbols K, core approach relies exploiting datalog
reasoner accomplishing following tasks:
Lower Upper Bound Computation, exploit datalog reasoner
compute lower bound Lq upper bound U q certain answers
q w.r.t. K. bounds match (i.e. Lq = U q ), query fully
answered datalog reasoner; otherwise, dierence Gq = U q \ Lq provides
set gap answers need verified using fully-fledged reasoner.
relevant techniques computing bounds described Sections 4 5.
Knowledge Base Subset Computation, exploit datalog reasoner
compute (hopefully small) subset Kq K sufficient check answers Gq
cert(q, K); is, ~a 2 cert(q, K) ~a 2 cert(q, Kq ) ~a 2 Gq . details
compute Kq given Section 6.
proceed according following steps given query q:
Step 1. Check satisfiability K.
(a) Compute bounds L? U ? unsatisfiability query ?. L? 6= ;,
terminate report K unsatisfiable. U ? = ;, proceed Step 2
(K satisfiable).
(b) Compute subset K? K.

(c) Use fully-fledged reasoner check satisfiability K? . minimise
computational workload fully-fledged reasoner, proceed follows:
318

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

i. Construct summary K? (See Section 7), use fully-fledged reasoner check satisfiable; is, proceed Step 2 (K satisfiable).
ii. Use fully-fledged reasoner check satisfiability K? ; unsatisfiable, terminate report K unsatisfiable. Otherwise,
proceed Step 2 (K satisfiable).
Step 2. Compute bounds Lq U q . Gq = ;, terminate return Lq . Otherwise,
proceed Step 3.
Step 3. Compute subset Kq K.
Step 4. ~a 2 Gq , use fully-fledged reasoner check whether Kq |= q(~a).
minimise computational workload, step carried follows:
(a) Construct summary Kq Kq (see Section 7). ~a 2 Gq , use
fully-fledged reasoner check whether ~a certain answer q w.r.t.
summary Kq , remove ~a Gq case.
(b) Compute dependency relation remaining answers Gq s.t. ~b
depends ~a ~a spurious answer, ~b. (See Section 7).
(c) Remove remaining spurious answers Gq , answer spurious
entailed Kq depends spurious answer; use fullyfledged reasoner check relevant entailments, arranging checks heuristics
w.r.t. dependency relation.
Step 5. Return Lq [ Gq .
following sections, describe steps formally. also introduce
number improvements optimisations, rely additional assumption
datalog reasoner materialisation-basedthat is, datalog knowledge base K0
query q 0 , computes query answers cert(q 0 , K0 ) first computing materialisation
ChaseK0 evaluating q 0 resulting materialisation. reasonable
assumption practice since datalog reasoners Semantic Web applications (e.g.,
OWLim, RDFox, Oracles native inference engine) materialisation-based. cases,
assume direct access materialisation. PAGOdA system
combines HermiT materialisation-based reasoner RDFox, hence able
exploit improvements optimisations described below; realisation
approach PAGOdA discussed detail Section 8.
illustrate techniques using running example consisting knowledge
base Kex = Kex [ DKex query qex (x) given Table 2. Note rules (R6)
(R8) Kex normalised; however, easily brought normal form
introducing fresh binary predicates eatsH eatsL follows:
MeatEater(x) ! 9y eatsH (x, y) (R6a)

eats(x, y) ^ Herbivore(y) ! eatsH (x, y)
eatsH (x, y) ! eats(x, y)

eatsH (x, y) ! Herbivore(y)

(R6b)
(R6c)

(R6d)

319

Folivore(x) ! 9y eatsL (x, y) (R8a)

eats(x, y) ^ Leaf(y) ! eatsL (x, y)
eatsL (x, y) ! eats(x, y)
eatsL (x, y) ! Leaf(y)

(R8b)

(R8c)
(R8d)

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Mammal(tiger)

(D1)

Mammal(wolf )

(D6)

Mammal(howler)

(D11)

Mammal(lion)

(D2)

MeatEater(wolf )

(D7)

MeatEater(python)
eats(python, rabbit)

(D3)
(D4)

eats(wolf , sheep)
Herbivore(sheep)

(D8)
(D9)

Folivore(howler)
Mammal(a hare)

(D12)
(D13)

Folivore(a hare)

(D14)

Herbivore(rabbit)

(D5)

eats(sheep, grass)

(D10)

eats(a hare, willow)

(D15)

Carnivore(x) ! Mammal(x)

Herbivore(x) ! Mammal(x)

Folivore(x) ^ MeatEater(x) ! ?

Herbivore(x) ^ eats(x, y) ! Plant(y)

(R1)
(R2)
(R3)
(R4)

Mammal(x) ! Herbivore(x) _ MeatEater(x)

(R5)

Mammal(x) ! 9y eats(x, y)

(R7)

MeatEater(x) ! 9y[eats(x, y) ^ Herbivore(y)]
Folivore(x) ! 9y[eats(x, y) ^ Leaf(y)]
Leaf(x) ! Plant(x)

(R6)
(R8)
(R9)

qex (x) = 9y[eats(x, y) ^ Plant(y)]

Table 2: Running example knowledge base Kex query qex (x). set Kex consists
rules (R1)(R9), dataset DKex consists facts (D1)(D15).
core techniques described Sections 4-6 applicable knowledge base
query. order simplify presentation definitions technical results
sections fix, addition knowledge base K = K [ DK , arbitrary query
q(~x) = 9~y '(~x, ~y ) (which may unsatisfiability query ?).

4. Lower Bound Computation
straightforward way compute lower bound answers using datalog reasoner
evaluate q w.r.t. datalog subset K consisting facts DK datalog rules
K . case OWL 2 ontologies, amounts considering subset OWL 2
RL axioms ontology. monotonicity property first-order logic certain
answers w.r.t. subset also certain answers w.r.t. K. Furthermore, subset
unsatisfiable, K.
Example 4.1. datalog subset example Kex consists rules (R1)(R4)
(R9), together facts (D1)(D15). materialisation datalog subset
Kex results following dataset: Dex [ {Mammal(rabbit), Mammal(sheep), Plant(grass)}
evaluating qex (x) materialisation obtain sheep answer.
}
basic lower bound rather imprecise practice since rules featuring disjunction existential quantification typically abound OWL 2 DL ontologies. improve

320

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

bound, exploit techniques allow us deterministically derive (also via datalog
reasoning) additional consequences K follow datalog subset.
4.1 Dealing Disjunctive Rules: Program Shifting
deal disjunctive rules, adopt variant shiftinga polynomial program transformation commonly used Answer Set Programming (Eiter, Fink, Tompits, & Woltran,
2004). next illustrate intuition behind transformation example.
Example 4.2. Let us consider information Kex Arctic hares (a hare).
(R3) (D14), one deduce hare MeatEater, follows
rule (R5) fact (D13) hare Herbivore. Since hare eats willow,
deduce Plant(willow) (R4) hence hare answer qex . Although (R5)
disjunctive rule, reasoning process fully deterministic captured
datalog. end, introduce predicate MeatEater intuitively stands
complement MeatEater. extend datalog subset Kex rules encoding
intended meaning fresh predicate. particular, (R3) (R5) two rules,
obtained (R3) (R5), respectively.
Folivore(x) ! MeatEater(x)

(R3)

Mammal(x) ^ MeatEater(x) ! Herbivore(x)

(R5)

exploit rules derive MeatEater(a hare) Herbivore(a hare).

}

define shifting transformation formally.
Definition 4.3. Let r normalised disjunctive datalog rule. predicate P r
let P fresh predicate arity. Furthermore, given atom = P (~t) let
P (~t). shifting r, written shift(r), following set rules:
r form (2), shift(r) = {r}[{ 1 ^ ^

1 ^ i+1 ^ ^ n

!



| 1 n};

r form (4), shift(r) consists following rules: (i) rule (S1);
(ii) rules (S2) 1 j m; (iii) rules (S3) 1 n s.t.
variable also occurs atom rule.
1
1
1

^ ^
^ ^
^ ^

n
n

^
^

1

1
1

^

^ ^
^ ^
i+1



!?

j 1

^ ^

^
n

(S1)
j+1

^

1

^ ^

^ ^




!

!

j

(S2)



(S3)

Let set normalised disjunctive datalog rules. Then, shifting defined
following set datalog rules:
[
shift() =
shift(r)
}
r2

Note shifting polynomial transformation. r disjunctive datalog rule
n atoms body atoms head, shift(r) contains + n + 1 datalog
rules. Furthermore, shown following theorem, also sound.
321

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Theorem 4.4. Let DD
subset disjunctive datalog rules K ; furthermore, let
K
K0 = shift(DD
)
[

.
Then,
cert(q, K0 ) cert(q, K).
K
K
0
Proof. Let ChaseK0 = {B }L
i=1 L non-negative integer (recall K datalog
knowledge base hence Skolem chase finite). show induction
following properties hold 0 L 2 B :

(a) = ?, K unsatisfiable;
(b) = P (~a), K |= P (~a);
(c) = P (~a), K |= P (~a).
Base case: Clearly, B 0 = DK properties trivially follow fact DK K.

Inductive step: Assume properties (a)(c) hold every 2 B . show
also hold every 2 B i+1 \ B . must exist rule r0 2 K0 substitution
B |= body(r0 ) = head(r0 ) . Since every atom body(r0 ) B ,
properties (a)-(c) hold atoms induction hypothesis. Furthermore,
must exist rule r 2 K form 1 ^ ^ n ! 1 _ _ r0 2 shift(r).
(a) = ?, distinguish two cases. (i) head(r) = ?, case r = r0
induction hypothesis, K |= { 1 , . . . , n } hence K |= ?; (ii) head(r) 6= ?,
case r0 form (S1) 1 , . . . , n 1 , . . . , B .
induction hypothesis, K entails 1 , . . . , n 1 , . . . , . then, rule r
cannot satisfied model K since r 2 K, obtain K unsatisfiable.
(b) = P (~a), r0 form (S2) = P (~a). Hence, B contains atoms
i+1 , . . . , . induction hypothesis, K entails
1 ,..., n , 1 ,... 1
,
.
.
.
,
,

,
.
.
.
,

a)
1
n
1
1 , i+1 , . . . , . Since r 2 K = P (~
must case K |= P (~a).
(c) = P (~a), following cases. (i) head(r) = ?, case induction
K |= { 1 , . . . , 1 , i+1 , . . . , n }; then, since 1 ^ ^ n ! ? also
rule K, obtain K |= , required. (ii) head(r) 6= ?, case
r0 form (S3) = P (~a); then, B contains atoms 1 , . . . , 1 ,
induction hypothesis K entails atoms
i+1 , . . . , n , 1 , . . . ,
,
.
.
.
,
,
,
.
.
.
,


a).
1
1
i+1
n
1 , . . . , . Since r 2 K obtain K |= P (~
V
q = ?, theorem follows property (a). Otherwise, let q(~x) = 9~y ( ni=1 (~x, ~y ))
let ~a possible answer K0 |= q(~a). Since K0 datalog, exists tuple
~e constants K0 non-negative integer L (~a, ~e) 2 B L 0 n.
then, (b) K |= (~a, ~e), hence K |= q(~a).
Shifting captures consequences disjunctive datalog rules K.
Furthermore, note refinement shifting ensures preservation
consequences; indeed, well-known disjunctive datalog express queries (e.g.,
non-3-colorabilility) cannot captured means datalog program (Afrati, Cosmadakis, & Yannakakis, 1995).
322

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Example 4.5. Consider disjunctive datalog knowledge base consisting fact
GreenSeaTurtle(turtle), rules (R1), (R2)
GreenSeaTurtle(x) ! Herbivore(x) _ Carnivore(x).
Clearly, Mammal(turtle) follows knowledge base. shifting consists fact
GreenSeaTurtle(turtle) following rules, predicates Carnivore, GreenSeaTurtle,
Herbivore Mammal abbreviated as, respectively, C, G, H M:
C(x) ^ M(x) ! ?

C(x) ! M(x)

M(x) ! C(x)

G(x) ^ H(x) ^ C(x) ! ?

G(x) ^ H(x) ! C(x)

G(x) ^ C(x) ! H(x)

H(x) ^ M(x) ! ?

H(x) ! M(x)

H(x) ^ C(x) ! G(x)

M(x) ! H(x)

}

checked fact Mammal(turtle) follow shifting.
4.2 Dealing Existential Rules: Combined Approach OWL 2 EL

Existentially quantified rules ubiquitous large-scale complex ontologies, especially
life sciences applications. EL profile OWL 2 specifically designed
applications, many large ontologies used practice seen consisting large
EL backbone extended small number axioms outside profile.
Given prevalence EL axioms realistic ontologies, natural consider
OWL 2 EL subset K computing lower bound answers. CQ answering OWL
2 EL is, however, PSpace-complete (Stefanoni, Motik, Krotzsch, & Rudolph, 2014)
system currently supports CQ answering whole OWL 2 EL. Complexity,
however, drops NP case ELHOr? (Stefanoni et al., 2014). setting,
restriction ELHOr? ontologies added practical benefit exploit socalled combined approach delegate computational work associated CQ
answering datalog reasoner (Stefanoni et al., 2013; Lutz, Toman, & Wolter, 2009)a
technique currently supported systems KARMA.5 Although datalog-based CQ
answering techniques also available richer languages, extension ELHOr?
inverse roles (i.e., axioms (O7) Table 1), resulting datalog programs hard
compute exponential size worst case (Perez-Urbina, Motik, & Horrocks,
2010). contrast combined approach ELHOr? , relevant datalog
programs straightforwardly constructed without need reasoning,
linear size (see Related Work section details).
Thus, compute query answers depend existentially quantified rules consider
r
subset EL
K ELHO ? rules K, syntactically characterised follows.
Definition 4.6. rule ELHOr? one following forms, '(x) either
?, form A(x), x c, 9yR(x, y):
p
^

i=1

Ai (x) ^

q
^

j=1

[Rj (x, yj ) ^

lj
^

k=1

5. http://www.cs.ox.ac.uk/isg/tools/KARMA/

323

Bjk (yj )] ! '(x),

(EL1)

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

R1 (x, y) ! R2 (x, y),
R(x, y) ! A(y).

(EL2)
(EL3)

combined approach exploit CQ answering conceptualised
three-step process.
1. first step compute materialisation datalog program obtained
EL
K respect DK . contains ?, knowledge base unsatisfiable. Otherwise model knowledge base. model, however,
universal cannot homomorphically embedded every model. Thus,
evaluation CQs may lead unsound answers.
2. second step evaluate query q . step intractable query
size, well-known database techniques exploited.
3. third step, unsound answers obtained second step discarded using
polynomial time filtration algorithm.
next specify transformation knowledge bases datalog used first
step, transformation also exploited later Section 5 computing upper
bound query answers. computation datalog program knowledge base
Step 1 relies form Skolemisation existentially quantified variables mapped
fresh constants (instead functional terms).
Definition 4.7. rule r form (1) existentially quantified variable
zij , let crij constant globally unique r zij , let c-sk substitution
c-sk (zij ) = crij zij 2 ~zi . c-Skolemisation c-sk(r) r given follows:
x, ~y )
1 (~

^ ^

x, ~y )
n (~

!


_

'i (~x, ~zi )c-sk .

i=1

Then, define c-sk(K) = {c-sk(r) | r 2 K } [ DK .

}

Note application c-Skolemisation ELHOr? rule always results
datalog rule. Note also that, contrast standard Skolemisation, c-Skolemisation
satisfiability entailment preserving transformation, may query answers
w.r.t. c-sk(K) unsound w.r.t. K. shown, however, c-Skolemisation
satisfiability-preserving ELHOr? knowledge bases; thus, c-sk(EL
K ) [ DK satisfiable
EL
[


satisfiable
(Stefanoni
et
al.,
2013).

next
sketch

filtration step,
K
K
refer interested reader work Stefanoni et al. details.
main source spurious answers evaluating query materialisation
obtained Step 1 presence forksconfluent chains binary atoms involving
Skolem constantsin image query materialisation. due
fact ELHOr? so-called forest-model property, forks cannot manifest
forest-shaped models. say constant c EL
K [DK auxiliary
dierent constant b exists c-sk(EL
)
[

|=
c

b;

is,
auxiliary constants
K
K
introduced c-Skolemisation entailed equal
324

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

constant present original ELHOr? knowledge base. Let substitution
mapping free variables ~x q constants ~a K |= q . Then, relation
q ~a smallest reflexive-transitive binary relation terms q satisfying
following fork rule
(fork)

0 t0
st

R(s, s0 ) P (t, t0 ) occur q,
(s0 ) auxiliary constant.

Clearly equivalence relation, computed polynomial time size
q. term q, let [t] equivalence class w.r.t. , let mapping
term q arbitrary fixed representative [t]. auxiliary graph
q ~a directed graph G = hV, Ei
V contains vertex (t) term q (t) auxiliary;
E contains directed edge h (s), (t)i atom form R(s, t) q
{ (s), (t)} V .
Now, ready define filtration. say ~a spurious answer either
auxiliary graph q contains cycle, terms occurring q exist
c-sk(EL

K ) [ DK 6|= (s) (t). Clearly, filtration candidate answer ~
done polynomial time size q.
assume availability procedure soundAnswers solves Steps 2 3;
is, given q model computed Step 1, returns answers q w.r.t.
input ELHOr? knowledge base. Consequently, given K q, obtain lower bound
query answers follows:
r
extract subset EL
K ELHO ? rules K;

compute materialisation c-sk(EL
K ) [ DK ;
q = ? return unsatisfiable ? 2 ; otherwise, return soundAnswers(q, ).
Example 4.8. Consider running example. ELHOr? subset Kex consists
facts (D1)(D15) together rules except (R4) (R5). fact (D12)
rule (R8) deduce howler eats leaf, must plant rule (R9). Hence
howler answer qex . answer identified using aforementioned steps.
c-Skolemisation (R8a) leads datalog rule
Folivore(x) ! eatsL (x, c3 )

(R8aU)

materialisation datalog program consisting facts rule (R8aU) contains
fact Plant(c3 ) hence tuple (howler, c3 ) matches qex materialisation.
match deemed sound filtration procedure.
}
4.3 Aggregated Lower Bound
techniques section seamlessly combined obtain lower bound Lq
hopefully close actual set certain answers. Given K q, proceed follows:
325

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

DD subset
1. Construct datalog knowledge base shift(DD
K ) [ DK , K
L
disjunctive datalog rules K . Compute materialisation M1 .
L
L
2. Construct datalog program c-sk(EL
K ) [ M1 compute materialisation M2 .

3. q = ?, Lq = cert(q, M2L ). Otherwise, Lq = soundAnswers(q, M2L ).
Theorem 4.4 ensures K |= 2 M1L signature K, hence M1L
used initial dataset second step. properties c-Skolemisation
filtration discussed Section 4.2 ensure every answer Lq indeed certain
answer q w.r.t. K. Furthermore, ? 2 M2L , K indeed unsatisfiable. Finally, note
materialisation M1L obtained first step pipelined second step;
result, Lq (sometimes strict) superset answers would obtain simply
EL
computing answers q w.r.t. shift(DD
K ) [ DK c-sk(K ) [ DK independently
union results.
Example 4.9. running example Kex , aggregated lower bound Lex consists
sheep (which follows datalog subset Kex ), hare (which follows shift(Kex )),
howler (which follows ELHOr? fragment Kex ).
}

5. Upper Bound Computation
many practical cases lower bound Lq described Section 4.3 constitutes rather
precise approximation actual set certain answers. Furthermore, also
computed efficiently resorting datalog reasoner. lower bound
computation, however, gives indication accuracy answers: without
corresponding upper bound, every possible answer remains candidate answer,
needs either confirmed discarded.
section, describe approach efficiently computing upper bound
set certain answers. lower upper bounds coincide, fully answered
query; otherwise, gap lower upper bounds provides margin
error lower bound, also narrows set candidate answers whose
verification may require powerful computational techniques.
5.1 Strengthening Knowledge Base
first step towards computing upper bound construct (polynomial size)
datalog knowledge base K0 K unsatisfiable, K0 entails nullary predicate
?s , cert(q, K) cert(q, K0 ) otherwise. Roughly speaking, K0 , refer
datalog strengthening K, obtained K
1. replacing ? fresh nullary predicate ?s predefined meaning;
2. splitting disjuncts occurring head position dierent datalog rules;
3. Skolemising existentially quantified variables constants Definition 4.7.
convenient subsequent definitions proofs explicitly define splitting
K, written split(K), intermediate knowledge base resulting Steps 1 2 above,
326

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

satisfiable disjunction-free. datalog strengthening K defined
result applying Step 3 replacing existentially quantified rule
split(K) c-Skolemisation.
Definition 5.1. splitting rule r form (1) following set rules:
head(r) = ?, split(r) = { 1 ^ ^
predicate predefined meaning;
otherwise, split(r) = {

! ?s }, ?s fresh nullary

! 9~zj 'j (~x, ~zj ) | 1 j }.

splitting K = K [ DK defined split(K) = r2K split(r) [ DK . Finally,
datalog strengthening K defined str(K) = c-sk(split(K)).
}
1

^ ^

n

n

Example 5.2. Consider example knowledge base Kex . splitting Kex obtained
replacing rule (R5) rules (R5Ua) (R5Ub), rule (R3) (R3U).
Mammal(x) ! Herbivore(x)

Mammal(x) ! MeatEater(x)

Folivore(x) ^ MeatEater(x) ! ?s

(R5Ua)
(R5Ub)
(R3U)

Finally, str(K) obtained replacing existentially quantified rules (R6a), (R7)
following rules (R6aU), (R7U)
MeatEater(x) ! eatsH (x, c1 )
Mammal(x) ! eats(x, c2 )

well rule (R8a) rule (R8aU) given Example 4.8.

(R6aU)
(R7U)
}

Note K contain rules ? head, str(K) logically entails
K: splitting amounts turning disjunctions head rules conjunctions,
c-Skolemisation restricts possible values existentially quantified variables fixed
constants. Thus, cert(q, str(K)) constitutes upper bound cert(q, K). is, however,
longer case ? replaced ordinary predicate ?s without predefined
meaning. rationale behind replacement provide meaningful upper bound
even cases splitting disjunctions c-Skolemising existentially quantified variables
would make strengthened knowledge base unsatisfiable.
0 = str(K ) example knowledge base.
Example 5.3. Consider strengthening Kex
ex
Since howler Mammal, Rule (R5Ub) also MeatEater. then,
since Folivore(howler) fact Kex derive ?s using Rule (R3U). Note that,
replaced falsehood predicate ? ?s , strengthening Kex would
unsatisfiable, case meaningful upper bound could obtained query. }

next show str(K) exploited compute meaningful upper bound
input query, despite fact ? stripped built-in semantics first-order
logic. following lemma establishes key property splitting transformation
Definition 5.1: ground clause ' = 1 _ _ n derivable K via hyperresolution,
Skolem chase split(K) contains every atom 1 n.
327

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Lemma 5.4. Let = (T, ) hyperresolution derivation K let H = split(K).
Then, every node v 2 ground atom occurring (v), 2 ChaseH .
Proof. prove claim structural induction .
Base case: v leaf , (v) 2 DK . Since DK H 2 ChaseH .

Inductive step: Assume induction hypothesis holds children w1 , . . . , wn
node v 2 . exists rule r 2 K substitution , sk(r) form
1 _ _ n _ disjunction atoms, (v) =
_ 1 _ _ n
hyperresolvent sk(r) (wi ) = _ 1 n. induction
hypothesis, disjuncts ChaseH , need show claim
disjunct
. distinguish following cases depending form
normalised rule r
r form (2),

empty. claim holds vacuously.

r form (3), = 1 sk . induction hypothesis,
ChaseH , since split(r) = r hence r 2 H, obtain 1 sk 2 ChaseH .





r form (4), = 1 _ _ . induction hypothesis,
ChaseH , 1 m, since rule 1 ^ ^ n ! H, obtain
atom also ChaseH , required.
exploit completeness hyperresolution show split(K) satisfies
required properties. Furthermore, fact str(K) |= split(K) immediately implies
str(K) satisfies properties well hence may exploited compute upper
bound query answers.
Theorem 5.5. following properties hold H = split(K) well H = str(K):
(i) cert(?, K) cert(?s , H), i.e. K unsatisfiable, H |= ?s ; (ii) K
satisfiable cert(q, K) cert(q, H).
Proof. first show Properties (i) (ii) hold H = split(K). K unsatisfiable,
hyperresolution derivation empty clause K. Thus, must
exist rule r form (2) K substitution
atom
1 n also derivable K. then, Lemma 5.4 2 ChaseH .
Since H contains rule 1 ^ ^ n ! ?s ?s 2 ChaseH H |= ?s , required.
Assume K satisfiable. cert(q, K) = ;, cert(q, K) cert(q, H) holds trivially;
otherwise let ~a certain answer q w.r.t. K. K |= q(~a) hence K [ Rq |= Pq (~a).
Since cert(?, K) = ;, q 6= ?. Using completeness hyperresolution
Lemma 5.4 obtain Pq (~a) chase K [ Rq . then, aforementioned
splitting also entails Pq (~a) since split(K [ Rq ) = H [ Rq ~a 2 cert(q, H),
required. Finally, Properties (i) (ii) hold str(K) direct consequence fact
str(K) |= split(K).
Example 5.6. Figure 1 depicts materialisation str(Kex ), edges predicates
introduced normalisation ignored edges figure represent
binary predicate eats. Explicit facts Kex depicted black; implicit facts depicted
328

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

c1

tiger
Mammal
Herbivore
MeatEater

MeatEater
Mammal
Herbivore
Plant

lion
Mammal
Herbivore
MeatEater

c2

python
MeatEater

Plant

grass Plant

wolf

rabbit
Herbivore
Mammal
MeatEater

Mammal
MeatEater
Herbivore

c3

sheep
Herbivore
Mammal
MeatEater
Plant

Leaf
Plant

willow Plant

howler

hare

Mammal
Folivore
Herbivore
MeatEater

Mammal
Folivore
Herbivore
MeatEater

Figure 1: Materialisation Datalog strengthening Kex
using dierent colours facilitate subsequent illustration refinements
materialisation allow us tighten upper bound. obtain following
upper bound cert(qex , Kex ) evaluating qex materialisation:
cert(qex , str(Kex )) = {tiger, lion, python, rabbit, wolf , sheep, howler, hare, c1 }
already mentioned, str(Kex ) |= ?s ; however, obtained upper bound still meaningful
since contain possible answers Kex , grass willow. Please note
c1 certain answer qex w.r.t. str(Kex ); however, constant c1 signature
Kex hence possible answer qex w.r.t. K.
}
5.2 Tightening Upper Bound: Existential Rules
upper bound obtained str(K) rather coarse-grained practice: discussed
Example 5.6, python, tiger, lion wolf contained upper bound, none
certain answer qex . section, show refine upper bound
restricting application c-Skolemisation existential rules. Instead computing
upper bound q constructing strengthened knowledge base str(K)
evaluating q (the materialisation of) str(K), proceed follows.
1. Apply K variant Skolem chase, refer c-chase first
splitting disjuncts occurring head position dierent rules applying
Skolem chasing split(K) following modifications: (i) similarly
restricted chase (Cal et al., 2013), existential rules applied rule
head already satisfied; (ii) rather Skolemising head atom (using
functional term) whenever existential rule applied, resort c-Skolemisation
instead. Due latter modification, c-chase compute least
Herbrand Model split(K), rather model split(K).
2. Evaluate q result aforementioned chase, thus obtaining upper bound
certain answers q w.r.t. split(K), thus also w.r.t. K.
following example motivates practical advantages approach.
Example 5.7. Consider materialisation str(Kex ) Figure 1. already
mentioned, python returned upper bound answer since qex matches facts
329

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

eats(python, c1 ) Plant(c1 ) materialisation. fact eats(python, c1 ) obtained
eatsH (python, c1 ), included materialisation satisfy c-Skolemised
rule (R6aU) str(Kex ), also existentially quantified rule (R6a) Kex . case
python, however, rule (R6a) Kex already satisfied fact eatsH (python, rabbit),
derived eats(python, rabbit) Herbivore(rabbit) dataset, rule
(R6b). Please note rule (R6b) form (9) normalisation (R6).
Rule (R6b) ensures (R6) satisfied substitution, (R6a) also satisfied substitution. obtain upper bound suffices construct model
Kex (rather model str(Kex )); thus, prevent application rule (R6aU)
python chase, dispense eats(python, c1 ) materialisation. }
ready define c-chase formally.
Definition 5.8. Let H = split(K), let dH subset datalog rules H ,
eH = H \ dH . c-chase sequence K sequence sets ground atoms {B }i 0 ,
B 0 = DH (i.e. B 0 = DK ), B i+1 inductively defined given next. Let Sdi+1
Sei+1 defined follows:
Sdi+1 = {head(r) | r 2 dH ,

Sei+1 = {head(c-sk(r)) | r 2 eH ,

substitution, B |= body(r) B 6|= head(r)}
substitution, B |= body(r) B 6|= head(r)}

Then, B i+1 = B [ Sdi+1 Sdi+1
6 ;, B i+1 = B [ Sei+1 otherwise. Finally, define
=
c-chase K c-ChaseK = 0 B .
}

Note c-chase K finite set since terms occur
constants c-sk(split(K)).
Example 5.9. c-chase Kex depicted Figure 2. materialisation strict
subset Figure 1, orange-coloured binary facts longer derived.
Consequently, python longer derived answer qex .
}
relevant properties c-chase summarised following lemma.
Theorem 5.10. following properties hold: (i) cert(?, K) cert(?s , c-ChaseK ), i.e.
K unsatisfiable, ?s 2 c-ChaseK ; (ii) K satisfiable, cert(q, K) cert(q, c-ChaseK ).
Proof. first prove c-ChaseK model split(K). Since DK c-ChaseK clear
satisfies facts split(K). Let r 2 split(K); distinguish two cases:
rule r datalog. c-ChaseK |= body(r) substitution definition
c-chase ensures head(r) 2 c-ChaseK hence rule satisfied.
Otherwise, r form (3). c-ChaseK |= body(r) substitution
definition c-ChaseH ensures head(c-sk(r)) 2 c-ChaseK ; thus, c-ChaseK |=
head(r) hence rule satisfied.
show contrapositive first property. Assume ?s 62 c-ChaseK .
c-ChaseK model split(K), split(K) 6|= ?s hence K satisfiable
Theorem 5.5. Finally, assume K satisfiable. cert(q, K) = ;, cert(q, K) cert(q, H)
holds trivially; otherwise let ~a certain answer q w.r.t. K. Theorem 5.5, obtain
~a 2 cert(q, split(K)). c-ChaseK |= split(K), ~a 2 cert(q, c-ChaseK ).
330

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

c1

tiger
Mammal
Herbivore
MeatEater

MeatEater
Mammal
Herbivore
Plant

lion
Mammal
Herbivore
MeatEater

c2

python

Plant

grass Plant

wolf

rabbit

MeatEater

Herbivore
Mammal
MeatEater

Mammal
MeatEater
Herbivore

c3

sheep
Herbivore
Mammal
MeatEater
Plant

Leaf
Plant

willow Plant

howler

hare

Mammal
Folivore
Herbivore
MeatEater

Mammal
Folivore
Herbivore
MeatEater

Figure 2: c-chase Kex
5.3 Tightening Upper Bound: Disjunctive Rules
Although technique described previous section quite eective practice,
main limitation split(K) disjunctions heads rules K eectively turned conjunctions. section, show refine upper bound
exploiting extension c-chase uses similar approach deal disjunctive
rules well existential rules.
Specifically, extend c-chase deal disjunctive rules r form (4)
(i) r applied none disjuncts head rule already
satisfied; (ii) r applied, one disjuncts included chase (rather
them). order avoid non-determinism chase expansion reduce
computational cost, disjuncts selected deterministically means (efficiently
implementable) choice function.
Example 5.11. Consider running example. First observe wolf answer
qex w.r.t. c-chase Kex shown Figure 2. Indeed, Herbivore(wolf ) derived
Mammal(wolf ) rules split (R5); thus, Plant(sheep) also derived using
rule (R4). Note, however, wolf spurious answer: given MeatEater(wolf )
explicit fact Kex , rule (R5) already satisfied wolf hence dispense
fact Herbivore(wolf ) materialisation.
Finally, since goal construct model Kex reasonable pick disjuncts
whose predicate unrelated ? Kex . Since ? depends MeatEater Folivore
(by rule (R3)), makes sense include fact Herbivore(b) materialisation whenever
disjunctive rule (R5) applied constant b.
}
details refer reader Section 8, specific choice function
implemented PAGOdA described.
define extended notion c-chase, efficiently implementable
choice function given additional parameter.
Definition 5.12. Let H knowledge base obtained K replacing ?
nullary predicate ?s , let dH set datalog rules H , let nH = H \ dH .
Furthermore, let f polynomially computable choice function given ground clause
set ground atoms returns disjunct . c-chase sequence K w.r.t. f
331

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

c2

tiger
Mammal
Herbivore

lion
Mammal
Herbivore

python
MeatEater

Plant

grass Plant

wolf

rabbit
Herbivore
Mammal

Mammal
MeatEater

c3

sheep
Herbivore
Mammal

Leaf
Plant

willow Plant

howler

hare

Mammal
Folivore
Herbivore

Mammal
Folivore
Herbivore

Figure 3: c-chasef Kex
sequence sets ground atoms {B }i 0 , B 0 = DH (i.e., B 0 = DK ), B i+1
defined given next. Let Sdi+1 Sni+1 follows:
Sdi+1 = {head(r) | r 2 dH ,

Sni+1 = {f (head(c-sk(r)) , B ) | r 2 nH ,

substitution, B |= body(r) B 6|= head(r)}

substitution , B |= body(r) B 6|= head(r)}

Then, B i+1 = B [ Sdi+1 Sdi+1 6= ;, B i+1 = B [ Sni+1 otherwise. Finally, define

c-chase K w.r.t. f c-ChasefK = 0 B .
}
Example 5.13. Consider aforementioned choice function f picks Herbivore(b)
whenever rule (R5) applied fact Mammal(b). Figure 3 depicts facts c-ChasefKex .
observed c-ChasefKex strict subset materialisation Figure 2,
brown-colored facts longer derived. see wolf answer
qex w.r.t. c-ChasefKex hence identified spurious. Furthermore, nullary
predicate ?s derived hence determine Kex satisfiable. }
relevant properties variant c-chase follows.
Theorem 5.14. Let f choice function Definition 5.12. ?s 62 c-ChasefK ,
c-ChasefK model K cert(q, K) cert(q, c-ChasefK ).
Proof. dataset DK contained c-ChasefK , suffices show c-ChasefK satisfies
rule r 2 K. distinguish following cases:
r form (2). Since ?s 2
/ c-ChasefK , cannot exist substitution
c-ChasefK |= body(r) hence c-ChasefK satisfies r vacuously.



r form (3). Pick c-ChasefK |= body(r) . definition c-ChasefK
ensures head(c-sk(r)) 2 c-ChasefK hence c-ChasefK satisfies r.
r form (4). Pick c-ChasefK |= body(r) . definition
c-ChasefK , f (head(c-sk(r)), Sni ) 2 c-ChasefK set atoms Sni
chase sequence, c-ChasefK satisfies r.

q = ?, cert(q, K) = ; cert(q, K) cert(q, c-ChasefK ) holds trivially; otherwise,
cert(q, K) cert(q, c-ChasefK ) follows fact c-ChasefK model K.
332

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

5.4 Combined Upper Bound
introduced three dierent techniques computing upper bound cert(q, K).
1. Compute materialisation M1U str(K), evaluate q w.r.t. M1U obtain set
possible answers U1q q w.r.t. K (c.f. Section 5.1).
2. Compute c-chase K, denoted M2U , evaluate q w.r.t. M2U obtain set
possible answers U2q q w.r.t. K (c.f. Section 5.2).
3. Fix choice function f , compute c-chase K w.r.t. f , denoted M3U , evaluate q w.r.t. M3U obtain set possible answers U3q q w.r.t. K (c.f. Section 5.3).

trivially seen U2q U3q precise U1q , i.e. U2q U1q U3q U1q .
shown following example, U2q U3q are, however, incomparable.

Example 5.15. Consider knowledge base H consisting facts A(a1 ), R(a1 , b1 ), B(b1 ),
A(a2 ), R(a2 , b2 ), B(b2 ) rules B(x) ! C(x) _ D(x), R(x, y) ^ C(y) ! S(x, y)
A(x) ! 9yS(x, y). Let c freshly introduced constant A(x) ! 9yS(x, y), let
f choice function picks disjunct D(bi ) every clause C(bi ) _ D(bi ). Then,
c-ChaseH = DH [ {C(b1 ), D(b1 ), S(a1 , b1 ), C(b2 ), D(b2 ), S(a2 , b2 )},
c-ChasefH = DH [ {D(b1 ), S(a1 , c), D(b2 ), S(a2 , c)}.

q1 (x) = 9y(S(x, y) ^ C(y) ^ D(y)), upper bound computed using c-ChaseH
contains two additional answers a1 a2 compared computed using c-ChasefH .
q2 (x1 , x2 ) = 9y(S(x1 , y) ^ S(x2 , y)), upper bound computed using c-ChasefH
additional answers (a1 , a2 ) (a2 , a1 ) compared computed using c-ChaseH . }

are, however, tradeos considered. Clearly, upper bound U1q
convenient ease implementation point view: str(K) constructed,
bound directly computed using o-the-shelf datalog reasoner without modification. Furthermore, upper bound U3q important shortcoming: use
whenever ?s derived, show following example.
Example 5.16. Consider choice function g picks MeatEater(a) disjunction form Herbivore(a) _ MeatEater(a). c-chase Kex w.r.t. g derive
MeatEater(howler) fact Mammal(howler) disjunctive rule (R5). Using
fact Folivore(howler) rule (R3U) derive ?s . Thus see that, although
howler cert(qex , Kex ), Herbivore(howler) c-chase Kex w.r.t. g, hence
howler upper bound computed using it; contrast two
upper bounds, Herbivore(howler) materialisation str(Kex ) c-chase
Kex , hence howler upper bound computed w.r.t. them.
}
Therefore, ?s 62 c-ChasefK , combine U2q U3q compute hopefully
precise upper bound; otherwise, use U2q . combined upper bound query answer
U q q K formally defined follows:
8 ?s
?
< U2 \ U3 q = ?;
q
q
q
U =
(13)
U \ U3
q 6= ? ?s 62 c-ChasefK ;
: 2q
U2
otherwise.
333

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Example 5.17. combined upper bound qex Kex gives:
Uex = {tiger, lion, rabbit, sheep, howler, hare}.
compare upper bound aggregated lower bound Lex given Example 4.9
identify gap Gex = {tiger, lion, rabbit}.
}

6. Reducing Size Knowledge Base
Whenever non-empty gap Gq lower upper bound (e.g., running
example) need verify whether answer Gq spurious not. Accomplishing
task using fully-fledged reasoner computationally expensive: verifying
answer Gq typically involves satisfiability test, infeasible practice
large-scale knowledge bases.
section propose technique identifying (typically small) subset Kq
knowledge base K sufficient verifying answers Gq (i.e. ~a 2 cert(q, K)
~a 2 cert(q, Kq ) ~a 2 Gq ). essential subsets be, one hand,
small possible and, hand, efficiently computable. requirements
conflict: computing minimal-sized subsets hard answering query, whereas
subsets easily computed may almost large initial knowledge base.
main idea behind approach construct datalog knowledge base whose
materialisation identifies rules facts Kq . knowledge base size polynomial
sizes K q include predicates arity higher K
q. way, subset computation fully delegated scalable datalog reasoner,
hence addressing efficiency requirement. key property Kq , ensures
contains relevant information K, following: rule fact 2
/ Kq
show occur hyperresolution proof (resp. gap answer
Gq ) K [ Rq q = ? (resp. q 6= ?). completeness hyperresolution
guarantees excluded facts rules indeed irrelevant.
6.1 Overview Approach
Let us motivate main ideas behind approach using running example. Since ?s
derived M2U \ M3U , know cert(?, Kex ) = ;, hence Kex
satisfiable (see Example 5.13). However, still need determine whether answers
Gex = {tiger, lion, rabbit} combined upper bound cert(qex , Kex ), i.e.,
certain answers qex .
sketch construction datalog knowledge base track(Kex , qex , Gex )
subset Kex relevant answers Gex derived. key property
knowledge base materialisation tracks rules facts may
participate hyperresolution proof gap answer thus encodes contents
subset Kqex . relevant information recorded using fresh predicates constants:
fresh predicate P R predicate P Kex , extension
materialisation track(Kex , qex , Gex ) give us facts subset.

334

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

fresh constant dr rule r Kex special unary predicate Rel,
extension materialisation track(Kex , qex , Gex ) give us rules
subset.
key step construction knowledge base invert rule r 2 Kex
set datalog rules (r) (i) moving head atoms r body
replacing predicates corresponding fresh ones (e.g., replace P P R );
(ii) copying atoms originally body r (now empty) head
replacing predicates corresponding fresh ones adding special atom
Rel(dr ) additional conjunct; (iii) eliminating conjunction head r
splitting r multiple rules, one head conjunct.
Consider first example datalog rule (R4) Kex , inverted
following rules:
PlantR (y) ^ Herbivore(x) ^ eats(x, y) ! HerbivoreR (x)
R

R

Plant (y) ^ Herbivore(x) ^ eats(x, y) ! eats (x, y)
R

Plant (y) ^ Herbivore(x) ^ eats(x, y) ! Rel(dR4 )

(14)
(15)
(16)

head Plant(y) (R4) moved body predicate Plant replaced
PlantR ; body Herbivore(x) ^ eats(x, y) copied head conjunction
HerbivoreR (x) ^ eatsR (x, y), conjoined special atom Rel(dR4 ); finally
head conjunction eliminated splitting rule three separate rules.
rules reflect intuitive meaning freshly introduced predicates. fact
PlantR (c) holds constant c, means fact Plant(c) may participate
hyperresolution proof Kex answer gap. Additionally, Herbivore(b)
eats(b, c) also hold b, facts rule (R4) could also participate
one proof since Plant(c) hyperresolvent facts Herbivore(b) eats(b, c)
rule (R4), recorded facts HerbivoreR (b), eatsR (b, c), Rel(dR4 ). Thus, rules
(14)(16) faithfully invert hyperresolution steps involving rule (R4).
Similarly, disjunctive rule (R5) inverted following two rules:
HerbivoreR (x) ^ MeatEaterR (x) ^ Mammal(x) ! MammalR (x)
R

R

Herbivore (x) ^ MeatEater (x) ^ Mammal(x) ! Rel(dR5 )

(17)
(18)

case, disjunctive head Herbivore(x)_MeatEater(x) (R5) moved
body conjunction HerbivoreR (x) ^ MeatEaterR (x) fresh predicates HerbivoreR
MeatEaterR . facts HerbivoreR (c) MeatEaterR (c) hold c (which means
facts Herbivore(c) MeatEater(c) may participate relevant proof Kex )
Mammal(c) holds, also deem fact Mammal(c) rule (R5) relevant.
situation dierent comes inverting existentially quantified rules,
case longer capture relevant hyperresolution steps Kex faithfully. Consider
rule (R7), inverted follows:
eatsR (x, y) ^ Mammal(x) ! MammalR (x)
R

eats (x, y) ^ Mammal(x) ! Rel(dR7 )
335

(19)
(20)

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

case, existentially quantified head 9y eats(x, y) moved body atom
eatsR (x, y). eatsR (b, c) holds b c (and hence fact may participate
relevant proof), Mammal(b) also holds, record (R7) Mammal(b)
relevant (the latter means fact MammalR (b)). hyperresolvent Mammal(b)
(R7) atom eats(b, t), functional term, may unrelated eats(b, c)
hence irrelevant proving answer gap.
addition inverting rules Kex , construction track(Kex , qex , Gex ) also
needs take query gap answers account. this, encode query
eats(x, y) ^ Plant(y) ! Pqex (~x) rules
PqRex (x) ^ eats(x, y) ^ Plant(y) ! eatsR (x, y)

(21a)

PqRex (x) ^ eats(x, y) ^ Plant(y) ! PlantR (y)

(21b)

add fact PqRex (c) c 2 Gex . query-dependent rules used initialise extension fresh predicates, subsequently makes rules
track(Kex , qex , Gex ) applicable.
query answers gap stem upper bound; consequently, order
rules (21a) (21b) applicable data track(Kex , qex , Gex ) obtained
upper bound materialisation Kex . following section show suffices
include facts c-chase Kex order ensure computed subset
contain necessary facts rules.
6.2 Subset Definition Properties
ready formally define datalog knowledge base used subset computation
well corresponding relevant subset.
Definition 6.1. Let G set possible answers q, let Rel fresh unary predicate
let dr fresh constant unique r K [ Rq . Furthermore, predicate
P K [ Rq , let P R fresh predicate arity P and, atom = P (~t),
let R denote P R (~t). normalised rule r 2 K [ Rq , let move(r) following
conjunction atoms:
P?R r form (2);


R x, ~
z1 )
1 (~



R x)
1 (~

Then,

r form (3);

^ ^

R x)
(~

r form (4).

(r) following set rules:

(r) = {move(r) ^ body(r) ! Rel(dr )} [ {move(r) ^ body(r) !

R
k

|

k

body(r)}.

tracking knowledge base track(K, q, G) smallest knowledge base containing
(i) facts c-chase K;

(ii) rules r2K[Rq (r);
336

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

(iii) fact PqR (~a) ~a 2 G;
(iv) fact P?R q 6= ?.
subset K relevant q G, denoted Kq,G , smallest knowledge base
containing
rule r 2 K track(K, q, G) |= Rel(dr );
fact 2 DK track(K, q, G) |= R .
brevity, write Kq particular case G set gap answers Uq \ Lq
defined Sections 4.3 5.4.
}
Note K? subset Kq since track(K, ?, G? ) subset track(K, q, Gq ):
Definition 6.1, point (i) track(K, ?, G? ) track(K, q, Gq ); furthermore,
set rules (ii) track(K, ?, G? ) subset track(K, q, Gq ) since
K [ R? K [ Rq ; finally, fact P?R , included track(K, ?, G? ) point (iii),
also belongs track(K, q, Gq ) point (iv).
Example 6.2. Consider running example, Gex = {tiger, lion, rabbit}.
subset Kex relevant qex Gex consists rules R2, R4, R5, R6, R7 facts
D1, D2, D3, D5, D7, D9, D11.
}
key properties computed subsets established following theorem.
Theorem 6.3. following properties hold:
(1) Assume L? = ;. Then, K unsatisfiable K? unsatisfiable.
(2) Let q dierent ? let G non-empty set possible answers q w.r.t.
K. K satisfiable, ~a 2 cert(q, K) ~a 2 cert(q, Kq,G ) every ~a 2 G.
Proof. direction (1) (2) follows directly monotonicity firstorder logic. direction (1) (2) follows completeness
hyperresolution following claim, establishes q non-empty
G, Kq,G contains support hyperresolution derivations clause (q, G)
K [ Rq

{}
q = ?;
(q, G) =
{Pq (~a) | ~a 2 G} otherwise.
Claim (|) = ( , ) derivation 2 (q, G) K [ Rq , support() Kq,G .
show direction (1), assume K unsatisfiable. Theorem 5.10,
Theorem 5.14 (13), U ? 6= ; thus G? 6= ;. exists hyperresolution
derivation 1 K. Since (?, G? ) = {}, know support(1 ) K?
(|). K? unsatisfiable. show direction (2), assume ~a 2 G
~a 2 cert(q, K). exists hyperresolution 2 Pq (~a) K [ Rq . Similarly,
(|), know support(2 ) Kq,G hence ~a 2 cert(q, Kq,G ).
337

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

next show inductively statement (|) follow. Let = ( , )
derivation clause (q, G) K [ Rq , let H = split(K). already
established (see proof Theorem 5.10) c-ChaseK model H. Since ChaseH
universal model H exists homomorphism ChaseH c-ChaseK . show
following properties inductively every node v .
a. track(K, q, G) |= R , atom (v);
b. track(K, q, G) |= Rel(dr ), sk(r) main premise used obtain parent u v.
proceed induction distance v root .
Base case: base case v root . Property (b) follows vacuously
since v parent .
q = ?, derivation empty clause (v) empty disjunction
and. property (a) also follows vacuously.
Otherwise, (v) = Pq (~a) ~a 2 G. definition track(K, q, G) (point
(iii)) ( (v))R 2 track(K, q, G) hence property (a) also holds.
Inductive step: Assuming properties (a) (b) hold node u, show
also hold children v1 , . . . , vn u. Let r rule K sk(r)
main premise relevant hyperresolution step MGU , i.e., (u) = 1 _ _
_ 1 _ _ n hyperresolvent sk(r) = 1 _ _ n _ 1 _ _
(vi ) = _ 1 n, using . easy observation composition
substitution homomorphism used later rest proof.
(

) = ( ) arbitrary function-free atom .

(22)

Lemma 5.4 Section 5.1 2 ChaseH 1 n. Since
homomorphism ChaseH c-ChaseK ( ) 2 c-ChaseK
(22), ( ) 2 c-ChaseK 1 n. next show track(K, q, G) |= move(r) .
= 0, move(r) = P?R . distinguish two cases.
q 6= ?, P?R 2 track(K, q, G) point (iv);

q = ?, ?s 2 c-ChaseK hence PqR 2 track(K, q, G) point (iii).
Otherwise, induction hypothesis, also track(K, q, G) |= (
(22), track(K, q, G) |= jR ( ) 1 j m.

j

)R

Therefore track(K, q, G) |= move(r) . body rules (r) satisfied
substitution hence track(K, q, G) |= Rel(dr ), track(K, q, G) |= iR ( )
1 n. (22), track(K, q, G) |= ( iR ) 1 n. addition,
induction hypothesis, track(K, q, G) |= R
, 1 n. Hence, shown
(a), (b) hold child vi u.
remains shown (a) (b) imply (|). Indeed, take 2 support().
338

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

fact K, leaf node ; hence, property (a)
track(K, q, G) |= R . then, since fact DK definition homomorphism ensures R = R . definition Kq,G implies 2 Kq,G .
rule K, Property (b) track(K, q, G) |= Rel(d ). Again,
definition Kq,G ensures 2 Kq,G .
completes proof theorem.
Note Claim (|) proof theorem also establishes important property
computed subsets, namely proof-preserving; is, support every
hyperresolution proof relevant gap answer original knowledge base K also
contained computed subset. two key implications. First, every justification
(i.e., minimal subset K entailing gap answer) contained also subset;
way, subsets preserve formulas K relevant gap answers,
formulas disregarded seen irrelevant. Second, fully-fledged reasoner
whose underpinning calculus cast framework resolution able
compute subset derivations gap answers K. Consequently,
practice reasonable expect fully-fledged reasoner uniformly display
better performance computed subsets Kan expectation borne
experiments.
conclude section example illustrating dataset track(K, q, G)
(point 1 Definition 6.1) obtained c-ChaseK materialisation underpinning
upper bound Section 5.2rather c-ChasefK Section 5.3.
Example 6.4. Consider query q(x) = E(x) knowledge base K consisting
following rules facts.
A(x) ! B(x) _ D(x)

D(x) ! E(x)

B(x) ! E(x)

A(a)

Let f function always choosing B(a) D(a), c-ChasefK = {A(a), B(a), E(a)}
constant answer q(x) gap lower upper bound. Suppose
define track(K, q, G) Definition 6.1 replacing facts point (i)
c-ChasefK . Since D(a) hold c-ChasefK corresponding subset
contain rule D(x) ! E(x), essential derive E(a).
}
6.3 Optimisations Datalog Encoding
conclude section, present two optimisations datalog encoding Definition
6.1 exploit system PAGOdA.
first optimisation aims reducing size computed subsets. Recall
key step construction tracking knowledge base track(K, q, G) invert
rules K capture hyperresolution proofs backwards fashion. Consider
inversion (17) rule (R5) running example. eect inversion capture applicability hyperresolution: facts Mammal(rabbit), HerbivoreR (rabbit)
MeatEaterR (rabbit) hold, include rule (R5) subset since may proof
339

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

K involving step ground clause Herbivore(rabbit) _ MeatEater(rabbit) _
obtained resolving (R5) Mammal(rabbit) _ .
Note, however, step redundant Herbivore(rabbit) already contained K, case (R5) may needed relevant subset. capture
observation distinguishing tracking knowledge base facts c-chase
K already present original dataset DK . encode implied
facts instantiating fresh predicates P predicate P K. running example,
fact MeatEaterI (rabbit) tracking knowledge base establishes MeatEater(rabbit)
present original data. use atoms predicates guards
inverted rules, e.g. rule (17) would written follows:
HerbivoreI (x) ^ MeatEaterI (x) ^ HerbivoreR (x)

^ MeatEaterR (x) ^ Mammal(x) ! MammalR (x)

Formally, Definition 6.1 optimised given next.
Definition 6.5. Let K, q, G predicates P R Definition 6.1. predicate
P , let P fresh predicate arity P . redefine move(r)
rule r following conjunction atoms:
P?R r form (2);


x, ~
z1 )
1 (~



x)
1 (~

^

R x, ~
z1 )
1 (~

^ ^

x)
(~

r form (3);

^

R x)
1 (~

^ ^

R x)
(~

r form (4).

Then, (r) Definition 6.1, track(K, q, G) also Definition 6.1, extended
addition fact P (~a) fact P (~a) c-ChaseK DK . }
easy see optimisation aect correctness Theorem 6.3:
disjunction atoms derived via hyperresolution, one atoms already
present data, disjunction subsumed dispensed with.
second optimisation used obtain succinct encoding datalog
reasoners support equality reasoning natively (such RDFox). already mentioned,
built-in semantics equality predicate axiomatised within datalog. However,
axiomatisation lead performance issues, scalability improved native
treatment equality equal objects merged single representative
whole equivalence class.
axiomatisation equality significant eect tracking encoding.
example, replacement rules r form (EQ4) inverted following rules
(r) predicate P :
P R (x1 , . . . , xi

1 , y, xi+1 , . . . , xn )

P R (x1 , . . . , xi

1 , y, xi+1 , . . . , xn )

^ P (x1 , . . . , xn ) ^ xi ! P R (x1 , . . . , xn )
^ P (x1 , . . . , xn ) ^ xi ! R (xi , y)

(23)
(24)

(23) tautology dispensed with, rule (24) required.
datalog reasoner native support equality, need include
340

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

tracking knowledge base inversion equality axioms (EQ1), (EQ2) (EQ3),
need include rules (24) order ensure computed subset required
properties. result succinct encoding materialised efficiently.
Example 6.6. Consider knowledge base K consists facts {R(a1 , b), R(a2 , b), A(a1 )}
following rules.
A(x) ! B(x) _ C(x)

R(x1 , y) ^ R(x2 , y) ! x1 x2

(25)
(26)

B(x) ! D(x)
C(x) ! D(x)

(27)
(28)

Let q = D(x), gap G lower upper bounds q {a1 , a2 }. easy
see rule (26) essential derive q(a2 ). ensure rule fragment
Kq,G , track a1 a2 using instance rule (24).
}
6.4 Comparison Magic Sets
idea inverting rules recording relevant information heavily exploited
Logic Programming. particular, magic set transformation (Bancilhon, Maier, Sagiv,
& Ullman, 1986) technique that, given program query, optimises materialisation process derive facts relevant query. Similarly
tracking encoding, magic sets technique uses auxiliary predicates, called magic predicates, identify relevant facts. technique originally developed datalog,
subsequently extended handle also negation failure (Beeri, Naqvi, Ramakrishnan, Shmueli, & Tsur, 1987; Kemp, Srivastava, & Stuckey, 1995) disjunctions (Alviano,
Faber, Greco, & Leone, 2012a).
contrast magic sets, goal transformation reduce size
materialisation, rather compute relevant fragment knowledge base potentially
given expressive (even undecidable) language, reduce computation
datalog reasoning. sense, technique orthogonal magic sets. Indeed,
benefits technique relevant knowledge bases containing existentially
quantified and/or disjunctive rules (if K datalog, query would fully
answered lower bound).
Furthermore, worth noticing way invert (datalog) rules also dierent
magic sets yields precise tracking. assumption
tracking starts already computed materialisation (see Point (i) Definition
6.1). instance, given already adorned rule A(x) ^ B(x) ! C(x), magic sets would
produce following rules deriving magic predicates B B:
C (x) ! (x)

C (x) ^ A(x) ! B (x)

rules used derive fact (a) C (a), even A(a) cannot used
derive C(a) aforementioned rule applicable (e.g., B(a) hold
C(a) derived using rules). transformation, contrast, would yield
restrictive rules
C R (x) ^ A(x) ^ B(x) ! AR (x)

C R (x) ^ A(x) ^ B(x) ! B R (x)

applicable A(a) B(a) hold materialisation.
341

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

7. Summarisation Analysis Answer Dependencies
section, let q input query dierent unsatisfiability query ?.
K? Kq computed, still need check, using fully-fledged reasoner,
satisfiability K? well whether Kq entails candidate answer Gq .
computationally expensive subsets large complex, many
candidate answers verify. therefore exploit summarisation techniques (Dolby et al.,
2007) eort reduce number candidate answers.
idea behind summarisation shrink data knowledge base merging
constants instantiate unary predicates. Since summarisation equivalent
extending knowledge base equality assertions constants, summary
knowledge base entails original one monotonicity first-order logic. Consequently, exploit summarisation follows:
1. satisfiability K remains undetermined, construct summary K?
check satisfiability. satisfiable, K? (and thus also K) also satisfiable.
2. Construct summary Kq use fully-fledged reasoner check whether
summary ~a entailed certain answer q summary Kq ,
discarding answers entailed.
Formally, summarisation defined follows.
Definition 7.1. type set unary predicates; given constant c K, say
= {A | A(c) 2 K} type c. type , let fresh constant
uniquely associated . summary function K substitution mapping
constant c K , type c. Finally, summary K K . }
following proposition shows summarisation exploited detect spurious
answers setting. Since summarisation significantly reduce data size practice,
relevant subsets K? Kq already significantly smaller K, checking
satisfiability K? gap answer Kq becomes feasible many cases, even
though implies resorting fully-fledged reasoner.
Proposition 7.2. Let summary function K. Satisfiability K? implies
following: (i) K satisfiable; (ii) cert(q, K) cert(q , Kq ) every CQ q.
Example 7.3. case running example, constants tiger lion
type {Mammal}, therefore mapped fresh constant, say tMammal , uniquely
associated {Mammal}. Since tMammal certain answer qex w.r.t. summary
Kex , determine tiger lion spurious answers.
}
summarisation succeed pruning candidate answers G, try
last step reduce calls fully-fledged reasoner exploiting dependencies
remaining candidate answers that, answer ~a depends answer ~c,
~a spurious, ~c.
Consider two tuples ~c d~ constants Gq . Suppose find endomor~ determine (by calling fully-fledged
phism dataset DK ~c = d.
~
reasoner) spurious answer, must ~c; result, longer need
call fully-fledged reasoner verify ~c. endomorphisms defined next.
342

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Definition 7.4. Let ~c = (c1 , . . . , cn ) d~ = (d1 , . . . , dn ) n-tuples constants K.
endomorphism ~c d~ K mapping constants constants
(i) ci = di 1 n; (ii) P (t1 , . . . , tm ) 2 DK fact P (t1 , . . . , tm ) 2 DK ;
(iii) r 2 K r 2 K .
}
relevant property endomorphisms given following proposition.
Proposition 7.5. Let ~c, d~ possible answers q let endomorphism ~c
d~ K. Then, ~c 2 cert(q, K) implies d~ 2 cert(q, K).
Proof. Since ~c 2 cert(q, K), know K |= q(~c). hyperresolution derivation
= (T, ) Pq (~c) K [ Rq . easy check (T,
) hyperresolution
~ K [ Rq . Then, K |= q(d)
~ hence d~ 2 cert(q, K).
derivation Pq (d)
exploit idea compute dependency graph candidate answer tuples
~ whenever endomorphism DK exists mapping ~c d.
~ Since
nodes edge (~c, d)
computing endomorphisms hard resort practice sound greedy algorithm
approximate dependency graph, describe Section 8.

8. Implementation: PAGOdA System
implemented approach system called PAGOdA, written Java
available academic license. system integrates datalog reasoner
RDFox (Motik et al., 2014) fully-fledged OWL 2 reasoner HermiT (Glimm et al.,
2014) black-boxes, also exploit combined approach ELHOr? (see Section
4.2) implemented KARMA (Stefanoni et al., 2014).
PAGOdA accepts input arbitrary OWL 2 DL ontologies, datasets turtle format
(PrudHommeaux & Carothers, 2014) CQs SPARQL. Queries interpreted
ground certain answer semantics. former case, PAGOdA sound
complete. latter case, however, PAGOdA limited capabilities HermiT,
check entailment ground DL concept queries; hence, PAGOdA
guarantee completeness lower upper bounds match, query
transformed DL concept query via internalisation (see Section 2.3). Otherwise,
PAGOdA returns sound (but possibly incomplete) set answers, along bound
incompleteness computed answer set.
architecture PAGOdA depicted Figure 4. box Figure 4 represents
component PAGOdA, indicates external systems exploited within
component. could, principle, use materialisation-based datalog reasoner
supports CQ evaluation incremental addition facts, fully-fledged OWL
2 DL reasoner supports fact entailment.
PAGOdA uses four instances RDFox (one lower bound, c-chase, cchasef subset extractor components) two instances HermiT (one
summary filter dependency graph components).
process fully answering query divided several steps. Here, distinguish query independent steps query dependent ones. see Figure
4, loading ontology materialisation steps query independent. Therefore,

343

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

cert(q, [ D)
heuristic planner

G0 Gq

HermiT
q, Gq

summary filter
HermiT
q, Gq

endomorphism
checker

Full reasoning

Kq

Lq

subset extractor

tracking encoder

Extracting subsets

RDFox
track(, q, Gq )

, q, Gq

Gq

Lq
F



Computing query bounds

soundAnswers(q, [ D)
certU3 (q, [ D)

M2L
q
lower store
KARMA
RDFox

certU2 (q, [ D)

M3U

M2U
q

q
c-chase

f

*

c-chase
RDFox

RDFox



Materialisation



shift

Loading ontology & data

profile checker

normaliser
HermiT clausifier


Figure 4: architecture PAGOdA
counted pre-processing steps. Computing query bounds, extracting subset
full reasoning query dependent, called query processing steps.
next describe component, following process flow PAGOdA.
8.1 Loading Ontology Data
PAGOdA uses OWL API parse input ontology O. dataset given
separately turtle format. normaliser computes set rules corresponding
axioms ontology. PAGOdAs normaliser extension HermiTs clausification
component (Glimm et al., 2014), transforms axioms so-called DL-clauses (Motik
et al., 2009). dataset loaded directly (the four instances of) RDFox.
normalisation, ontology checked determine inside OWL 2 RL
ELHOr? . input ontology OWL 2 RL (resp. ELHOr? ), RDFox (resp.
KARMA) already sound complete, cases PAGOdA simply processes
344

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

ontology, dataset queries using relevant component. Otherwise, PAGOdA uses
dedicated program shifting component enrich deterministic part ontology
additional information disjunctive rules (see Section 4.1), resulting set rules .
8.2 Materialisation
three components involved step, namely lower bound, c-chase cchasef . takes input D, computes materialisation (shown
Figure 4 ellipses). lower bound component performs Steps 1 2 Section 4.3
order compute aggregated lower bound M2L . c-chase c-chasef components
compute M2U M3U upper bound materialisations described Section 5.4 using
dedicated implementation c-chase algorithm. chase sequence stored RDFox,
applicability existential disjunctive rules determined posing SPARQL
queries RDFox. applying disjunctive rule (while computing M3U ), PAGOdA
uses choice function select one disjuncts. discussed Section 5.4, choice
function try select disjuncts (eventually) lead contradiction.
end, PAGOdA implements following heuristics.
construct standard dependency graph containing edge predicate P
Q rule P occurs body Q head. Then, compute
preference ordering predicates occurring disjunction according
distance ? dependency graph, preferring furthest ?.
exploit result materialising using shifting enriched rules (see
Section 4.1). fact form P (~a) obtained materialisation, P (~a)
follows knowledge base. Hence, obtained P (~a), try
avoid choosing P (~a) disjunct P (~a) _ chase computation.
M2L contains contradiction, input ontology dataset unsatisfiable,
PAGOdA reports terminates. ?s derived M3U , computation
aborted M3U longer used. M2U contains ?s , PAGOdA checks
satisfiability [ D; eect, computes cert(?, [ D). answer query
non-empty, input ontology dataset unsatisfiable, PAGOdA reports
terminates; otherwise input ontology dataset satisfiable, PAGOdA
able answer queries.
8.3 Computing Query Bounds
Given query q, PAGOdA uses M2L lower bound materialisation compute lower
bound answer Lq . order exploits KARMAs implementation filtration
procedure (algorithm soundAnswers Section 4.2), clarity step shown separately (as circle F it) Figure 4. ?s derived computing
M3U materialisation, U q = cert(q, M2U ) \ cert(q, M3U ); otherwise U q = cert(q, M2U ). either
case U q computed directly using RDFox answer q w.r.t. relevant materialisation.
Extracting Subsets tracking encoder component implements datalog encoding
based Definition 6.1 optimisations described Section 6.3. resulting
datalog knowledge base added rules data c-chase component,
345

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

RDFox used extend c-chase materialisation accordingly. freshly derived facts
(over tracking predicates introduced tracking encoder) passed
subset extractor component, uses facts identify facts rules
relevant checking gap answers, computes intersection relevant facts
input dataset querying instance RDFox containing only.
8.4 Full Reasoning
PAGOdA uses HermiT verify gap answers Gq = U q \ Lq . HermiT accepts
queries given either facts DL concepts, implemented standard rolling-up
technique transform internalisable CQs. summary filter component, PAGOdA uses
HermiT filter gap answers entailed summary Kq (see Section 7).
remaining gap answers G0 Gq passed endomorphism checker,
exploits greedy algorithm compute incomplete dependency graph answers
G0 . graph used heuristic planner optimise order answers
G0 checked using HermiT (see Section 7). Verified answers G0 combined
lower bound Lq give cert(q, [ D).
implementation summarisation straightforward: PAGOdA essentially merges
constants (explicit) types data.

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6
7
8
9
10
11
12

Input: knowledge base K = K [ DK , two tuples (a1 , . . . , ), (b1 , . . . , bn ).
Output: return true endomorphism (a1 , . . . , ) (b1 , . . . , bn ) K found,
otherwise, false.
= ;;
foreach 2 [1..n]
ai locally embeddable bi K return false;
else (ai ) = bi ;
end
foreach 2 [1..n]
check(ai , bi ) return false;
end
K 6= K return false;
else return true;
Subroutine check(a, b)
Oa := {c | P (ai , c) 2 DK }, Ia := {c | P (c, ai ) 2 DK };
Ob := {d | P (bi , d) 2 DK }, Ib := {d | P (d, bi ) 2 DK };
foreach 2 {O, I} c 2 Sa
:= {d 2 Sb | c locally embedded d};
empty return false;
defined c
(c) := similar constant c D;
check(c, d) return false;
end
else (c) 62 return false;
end

Algorithm 1: Greedy endomorphism checker.

346

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

next describe greedy algorithm implemented PAGOdA checking answer
dependencies (see Algorithm 1). Given tuples (a1 , . . . , ) (b1 , . . . , bn ), algorithm
returns True able find endomorphism, False otherwise. algorithm
considers constant ai tries map bi locally, sense
immediate neighbourhoods ai bi considered stage. Formally,
captured following notion local embedding.
Definition 8.1. Given K constant c let Mc multiset containing occurrence
fact A(c) 2 DK , occurrence P binary fact P (c, c0 ) 2 DK ,
occurrence P binary fact P (c0 , c) 2 DK .
Given constants c K, say c locally embeddable predicate
Mc occurs (with cardinality) Md .
}
check(a, b) subroutine implements greedy search looking immediate neighbours b. Specifically, subroutine considers neighbour c picks
neighbour b c locally embedded d. several choices
available, algorithm heuristically chooses one according Jaccard similarity
multisets Mc Md .6 algorithm terminates success manages
compute mapping defined constants reachable {a1 , . . . , }
K. immediate see computed endomorphism ~a ~b K; thus,
algorithm sound. algorithm works polynomial time choices made
construction never revisited local embeddability checked efficiently.

9. Related Work
Conjunctive query answering ontology-enriched datasets received great deal
attention recent years. computational complexity thoroughly investigated
wide range KR languages number practicable algorithms proposed
literature implemented reasoning systems.
9.1 Computational Complexity CQ Answering
decision problem associated CQ answering conjunctive query entailment (CQE),
namely decide whether K |= q(~a) given input CQ q, possible answer ~a,
knowledge base K expressed (fixed) language L. problem well-known
undecidable general, even q restricted atomic L language
existential rules (Dantsin et al., 2001).
CQE knowledge bases stemming OWL DL ontologies decidable
assumption query mention transitive relations (Rudolph & Glimm, 2010).
Decidability CQE unrestricted OWL DL OWL 2 DL ontologies CQs remains
open problem. Even cases CQE decidable, typically high
computational complexity. CQE 2-ExpTime-complete expressive DLs SHIQ
SHOQ (Glimm et al., 2008; Eiter, Lutz, Ortiz, & Simkus, 2009). Hardness results
6. Jaccard similarity multisets 0 defined |M \ 0 |/|M [ 0 |, |M \ 0 | counts
minimum number occurrences common element 0 , whereas |M [ 0 | counts
sum occurrences elements 0 .

347

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

2-ExpTime obtained already ALCI (Lutz, 2008) well Horn-SROIQ,
underpins Horn fragment OWL 2 DL (Ortiz, Rudolph, & Simkus, 2011). CQE
ALC SHQ, involve inverse roles, ExpTime-complete (Lutz, 2008).
Single exponential time results also obtained Horn DLs disallowing complex role
inclusion axioms: CQE ExpTime-complete Horn-SHOIQ, underpins Horn
fragment OWL DL (Ortiz et al., 2011).
Given high complexity CQE, recently increasing interest
lightweight DLs CQE computationally easier. lightweight DLs
incorporated OWL 2 standard profiles (Motik et al., 2012). CQE OWL
2 EL profile PSpace-complete (Stefanoni et al., 2014). Furthermore, complexity
CQE drops NP complex role inclusions (with exception transitivity
reflexivity) disallowed OWL 2 EL (Stefanoni & Motik, 2015). latter complexity
rather benign since CQE databases already NP-hard. Finally, CQE OWL
2 QL profile also NP-complete (Calvanese et al., 2007). Regarding data complexity,
CQE coNP-complete non-Horn DLs, ALE (Schaerf, 1993). contrast,
data complexity PTime-complete Horn DLs encode recursion, HornSROIQ OWL 2 EL (Ortiz et al., 2011; Stefanoni et al., 2014). Finally, data complexity
known AC0 OWL 2 QL profile (Calvanese et al., 2007).
complexity CQE also well understood rule-based KR languages. plain
datalog, ExpTime-complete combined complexity PTime-complete w.r.t. data
complexity. disjunctive datalog, coNExpTime-complete combined complexity
coNP-complete w.r.t. data complexity. Datalog refers family decidable KR
languages based existential rules (Cal, Gottlob, & Lukasiewicz, 2012). includes
guarded (Cal et al., 2013), sticky (Cal, Gottlob, & Pieris, 2011), acyclic (Cuenca Grau
et al., 2013) datalog . extension datalog languages disjunctive rules
recently studied (Alviano et al., 2012b; Bourhis et al., 2013).
Finally, refer ground query entailment (GCQE) problem checking whether
tuple ~a ground answer q(~x) = 9~y '(~x, ~y ) w.r.t. K. KR languages allow
existentially quantified rules, restriction ground answers typically makes CQE easier:
definition ground answers means GCQE trivially reduced satisfiability
checking. Consequently, GCQE decidable OWL 2 DL.
9.2 Practical Query Answering Approaches
o-the-shelf DL reasoners, Pellet (Sirin et al., 2007) HermiT (Glimm
et al., 2014) provide support query answering. Pellet supports SPARQL conjunctive
queries also implements rolling-up technique. contrast, HermiT provide
SPARQL API supports CQs form (complex) DL concepts. Racer
among first DL reasoners implement optimise CQ answering ground
semantics (Haarslev, Hidde, Moller, & Wessel, 2012). Finally, also intensive
work optimising query answering DL systems, including filter-and-refine techniques
(Wandelt et al., 2010), ordering strategies query atoms (Kollia & Glimm, 2013), data
summarisation (Dolby et al., 2009). Optimising CQ answering DL reasoners complementary approach, use optimised DL reasoner could significantly
improve performance PAGOdA queries require full reasoning.

348

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

RDF triple stores typically implement materialisation-based (a.k.a. forward chaining)
reasoning algorithms, answer queries evaluating resulting materialisation. Jena (McBride, 2001) Sesame (Broekstra, Kampman, & van Harmelen, 2002)
among first systems provide support RDF-Schema. Modern triple stores
OWLim (Bishop et al., 2011), Oracles native inference engine (Wu et al., 2008),
provide extended suppport ontologies RL profile. Additionally, RDFox (Motik
et al., 2014) supports arbitrary datalog unary binary predicates. Finally, ASP
engines DLV (Leone, Pfeifer, Faber, Eiter, Gottlob, Perri, & Scarcello, 2006) implement sound complete reasoning (extensions of) disjunctive datalog. Although triple
stores exhibit appealing scalability, support restricted ontology languages;
however, DL reasoners, improving scalability triple stores complementary
approach, advances area directly exploited PAGOdA.
technique CQ answering lightweight DLs receiving increasing attention
so-called combined approach (Lutz et al., 2009; Stefanoni et al., 2013; Kontchakov,
Lutz, Toman, Wolter, & Zakharyaschev, 2011). combined approach dataset
first augmented new facts query-independent way build (in polynomial time)
model ontology. model exploited query answering two equivalent
ways. approach Lutz et al. (2009) Kontchakov et al. (2011) query first
rewritten evaluated constructed model. Alternatively, work
Stefanoni et al. (2013) Lutz et al. (2013) query first evaluated model
unsound answers eliminated means polynomial time filtration process.
Combined approaches applied logics EL family (Lutz et al., 2009;
Stefanoni et al., 2013) well DL-Lite (Kontchakov et al., 2011), PAGOdA,
use implementation (Stefanoni et al., 2013) compute aggregated lower bound.
CQ answering Horn ontologies often realised means query rewriting techniques. rewriting query q w.r.t. ontology another query q 0 captures
information necessary answer q arbitrary dataset. Unions CQs
datalog common target languages query rewriting. Query rewriting enables reuse
optimised data management system: UCQs answered using standard relational
databases, whereas datalog queries evaluated using triple store. Query rewriting
successfully applied OWL 2 QL ontologies, rewritability UCQs
guaranteed. Example systems include QuOnto (Acciarri, Calvanese, De Giacomo, Lembo,
Lenzerini, Palmieri, & Rosati, 2005), Mastro (Calvanese, De Giacomo, Lembo, Lenzerini,
Poggi, Rodriguez-Muro, Rosati, Ruzzi, & Savo, 2011), Rapid (Chortaras, Trivela, & Stamou, 2011), Prexto (Rosati, 2012), Ontop (Bagosi, Calvanese, Hardi, Komla-Ebri,
Lanti, Rezk, Rodriguez-Muro, Slusnys, & Xiao, 2014). systems
successful large scale applications; however, applicable OWL 2 QL
size rewriting exponential size ontology. Datalog-based query
rewriting implemented systems REQUIEM (Perez-Urbina et al., 2010),
supports extension ELHOr? inverse roles. introduction inverse
roles, however, leads significant jump complexity: query answering ELHOr?
NP-complete (and tractable atomic queries), whereas becomes ExpTime-complete
inverse roles introduced (furthermore, ExpTime-hardness holds already unsatisfiability checking atomic queries). practice, restricting ELHOr?
allows us compute datalog program linear size straightforward way Skolemis349

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

ing existentially quantified variables constants. Furthermore, datalog materialisation
query independent queries without existentially quantified variables answered
directly materialisation, complex queries answered using filtration.
Finally, similarly PAGOdA, system Hydrowl (Stoilos, 2014a) combines OWL
2 RL reasoner query rewriting system fully-fledged DL reasoner order
answer conjunctive queries OWL 2 knowledge base. techniques Hydrowl are,
however, rather dierent PAGOdA. Hydrowl uses two dierent query answering
strategies. first one based repairing (Stoilos, 2014b) query rewriting,
applicable ontologies suitable repair exists. second strategy exploits
query base: set atomic queries Hydrowl computes pre-processing phase,
fully answered using triple store given ontology arbitrary
dataset. answering query q, Hydrowl checks q covered query base (Stoilos
& Stamou, 2014); is, q completely evaluated using OWL 2 RL reasoner;
otherwise, fully-fledged reasoner used answer q. However, computation
query base appear correct general,7 believe accounts
apparent incompleteness Hydrowl tests (see Section 10.3.1).
9.3 Approximate Reasoning
idea transforming ontology, data and/or query obtain lower upper bound
answers already explored previous work. Screech system (Tserendorj et al.,
2008) uses KAON2 (Hustadt, Motik, & Sattler, 2007) transform SHIQ ontology
(exponential size) disjunctive datalog program way ground answers
queries preserved. Subsequently, Screech exploit (unsound incomplete) techniques
transform disjunctive datalog plain datalog. way, Screech computes
approximation answer. TrOWL (Thomas et al., 2010) exploits approximation
techniques transform OWL 2 ontology ontology QL profile (Pan &
Thomas, 2007). approximation first computes closure input ontology
entailment OWL 2 QL axioms, disregards axioms outside OWL 2 QL.
Related approximations OWL 2 QL also proposed, e.g., Wandelt et al.
(2010) Console et al. (2014). Efficient approximation strategies OWL 2 ontologies
complementary approach, exploited PAGOdA order
refine lower upper bound query answers.

10. Evaluation
evaluated query answering system PAGOdA range realistic benchmark ontologies, datasets queries, compared performance stateof-the-art query answering systems. test data systems used comparison
introduced Sections 10.1 10.2, respectively. results discussed Section
10.3. Experiments conducted 32 core 2.60GHz Intel Xeon E5-2670 250GB
RAM, running Fedora 20. test ontologies, queries, results available online.8
7. Stoilos (2014a) mentions limitation automatically extracting [the atomic queries].
8. http://www.cs.ox.ac.uk/isg/tools/PAGOdA/2015/jair/

350

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

LUBM(n)
UOBM(n)
FLY
NPD
DBPedia+
ChEMBL
Reactome
Uniprot

]axioms
93
186
14,447
771
1,716
2,593
559
442

]rules
133
234
18,013
778
1,744
2,960
575
459

]9-rules
15
23
8396
128
11
426
13
20

]_-rules
0
6
0
14
5
73
23
43

]facts
n 105
2.6n 105
8 103
3.8 106
2.9 107
2.9 108
1.2 107
1.2 108

Table 3: Statistics test datasets
10.1 Test Ontologies Queries
Table 3 summarises test data. first two columns table indicate total
number DL axioms test ontology well total number rules
normalisation. interested ontologies captured OWL 2 RL
hence cannot fully processed RDFox; thus, number rules containing existential
quantification disjunction especially relevant given third fourth
columns table, respectively. Finally, rightmost column lists number data
facts dataset.
LUBM UOBM widely-used reasoning benchmarks (Guo, Pan, & Heflin, 2005;
Ma, Yang, Qiu, Xie, Pan, & Liu, 2006). ontology axioms benchmarks
manually created considered fixed, whereas data synthetically generated
according parameter n determines size. LUBM UOBM come 14 15
standard queries, respectively. make tests LUBM challenging, extended
benchmark 10 additional queries datalog lower-bound answers
guaranteed complete (as case standard queries).
FLY realistic ontology describes anatomy Drosophila
currently integrated Virtual Fly Brain tool.9 Although data rather small
compared test cases (about 8, 000 facts), ontology rich existentially
quantified rules, makes query answering especially challenging. tested 6 realistic
queries provided developers ontology.
NPD FactPages ontology describing petroleum activities Norwegian
continental shelf. ontology comes realistic dataset containing 3.8 million facts.
Unfortunately, NPD realistic queries tested atomic queries
signature ontology.
DBPedia contains information Wikipedia entries. Although dataset rather
large, ontology axioms simple captured OWL 2 RL. provide
challenging test, used ontology matching system LogMap (JimenezRuiz & Cuenca Grau, 2011) extend DBPedia tourism ontology containing
9. http://www.virtualflybrain.org/site/vfb site/overview.htm

351

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

existential disjunctive rules. case NPD example test queries,
focused evaluation atomic queries.
ChEMBL, Reactome, Uniprot realistic ontologies made publicly available European Bioinformatics Institute (EBI) linked data platform.10
ontologies especially interesting testing purposes. one hand,
ontology axioms data realistic used number applications;
hand, ontologies rich existentially quantified disjunctive rules,
datasets extremely large. Furthermore, EBI website provides number
example queries ontologies. order test scalability datasets
well compare PAGOdA systems implemented data sampling algorithm
based random walks (Leskovec & Faloutsos, 2006) computed subsets data
increasing size. used evaluation example queries correspond CQs
well atomic queries relevant signature.
10.2 Comparison Systems
compared PAGOdA four ontology reasoners: HermiT (v.1.3.8), Pellet (v.2.3.1),
TrOWL-BGP (v.1.2), Hydrowl (v.0.2). single exception TrOWL,
systems implement sound complete algorithms standard reasoning tasks OWL
2 DL ontologies, including ontology consistency checking concept instance retrieval.
Additionally, HermiT provide support SPARQL queries.
pointed Section 9, many systems answer queries
ontologies. However, systems generally designed specific fragments
OWL 2, incomplete ontologies outside fragments. Although TrOWL
also incomplete OWL 2, included evaluation is,
one hand, widely-used system Semantic Web applications and, hand,
similar PAGOdA exploits ontology approximation techniques. follows,
describe capabilities systems detail.
HermiT fully-fledged OWL 2 reasoner based hypertableau calculus (Motik
et al., 2009; Glimm et al., 2014). HermiT focuses standard reasoning tasks DLs.
provide SPARQL conjunctive query answering API, capable
answering atomic queries unary predicates checking fact entailment.
Pellet tableau-based OWL 2 DL reasoner support CQ answering (Sirin et al.,
2007). Pellet provides SPARQL API, hence compute set ground
answers arbitrary conjunctive queries expressed SPARQL. Pellet also capable
computing certain answers internalisable conjunctive queries using rolling-up
technique (see Section 2.3).
TrOWL system based approximated reasoning. accepts input arbitrary
OWL 2 DL ontology CQ SPARQL, aims computing ground answers
given query (Thomas et al., 2010). TrOWL exploits technique approximates
input ontology OWL 2 QL profile, provide completeness guarantees.
10. http://www.ebi.ac.uk/rdf/platform

352

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

correct#

incomplete#

unsound#

error#

Kmeout#

cannot#handle#

100%#
90%#
80%#
70%#
60%#
50%#

Tr

Tr

Pe Hy

Pe Hy

Tr

Pe Hy

Tr

Tr

Pe Hy

Pe Hy

Tr

Pe Hy

Tr

Pe Hy

Tr

Pe Hy

Tr

Pe Hy

Tr

Pe Hy

40%#
30%#
20%#
10%#

#1
%
#
Pr
ot
Un


EM


ac


e#
10
%
#

BL
#1
%
#

#
DB
Pe


ia
Ch

NP
D#

Fa

ct
Pa
ge
s#

le
dU
p#
ro
l
Y_
FL

1_
ro
l

le
dU
p#

1#
UO
BM

UO
BM

le
dU
p#
1_
ro
l
LU
BM

LU
BM

1#

0%#

Figure 5: Quality answers computed system. four bars ontology
represent Trowl, Pellet, HermiT Hydrowl respectively.
Hydrowl (Stoilos, 2014a) hybrid reasoning system similar spirit PAGOdA
(see Section 9.2 detailed comparison). Hydrowl integrates triple store OWLim
HermiT. accepts input arbitrary OWL 2 ontology conjunctive queries rules,
computes ground answers query.
10.3 Experiments Results
performed three dierent experiments. first experiment, compared
PAGOdA mentioned systems, respect quality
answers (i.e., number correctly answered queries) performance relative
PAGOdA. second experiment, evaluated scalability considering datasets
increasing size. Finally, third experiment, evaluated eectiveness
dierent reasoning techniques implemented PAGOdA.
10.3.1 Comparison Systems
compared PAGOdA systems test ontologies. used
LUBM(1) UOBM(1) since already rather hard systems. Similarly, used relatively small samples EBI platform ontologies (1% data
ChEMBL UniProt, 10% Reactome) processed majority
systems. test ontology computed ground answers corresponding
test queries, whenever possible used internalisation (see Section 2.3) additionally
compute certain answers. case FLY, test queries yield empty set
ground answers, case computed certain answers (all FLY queries
internalised). set timeouts 20 minutes answering individual query, 5
hours answering queries given ontology.
Figure 5 summarises quality answers computed reasoner. bar
figure represents performance particular reasoner w.r.t. given ontology
353

fiPellet"

HermiT"

Hydrowl"

DB
Pe


TrOWL"

ct
Pa
ge
s"

Zhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

1000"

100"

10"

"1
%
"
Pr
ot
Un


EM


ac


e"
10
%
"

BL
"1
%
"

"
ia
Ch

Fa
NP
D"

le
dU
p"
ro
l
Y_
FL

1_
ro
l

le
dU
p"

1"
UO
BM

le
dU
p"
1_
ro
l

UO
BM

0"

LU
BM

LU
BM

1"

1"

Figure 6: Performance comparison systems. bar depicts total time
answer test queries relevant ontology comparison PAGOdA.
set test queries. use green indicate percentage queries reasoner
computed correct answers, correctness determined majority voting,
blue (resp. purple) indicate percentage queries reasoner
incomplete (resp. unsound). Red, orange grey indicate, respectively, percentage
queries reasoner reported exception execution, accept
input query, exceeded timeout. criterion correctness, PAGOdA
able correctly compute answers every query test ontology within given
timeouts. Consequently, performance PAGOdA represented figure.
Figure 6 summarises performance system relative PAGOdA,
case considered queries relevant system yields answer (even
computed answer unsound and/or incomplete). ideal, chose
consider queries (rather queries relevant system
yields correct answer) (i) resulting time measurement obviously closer
time would required correctly answer queries; (ii) correctness
relative gold standard query answers. ontology
reasoner, corresponding bar shows t2 /t1 (on logarithmic scale), t1 (resp. t2 )
total time required PAGOdA (resp. compared system) compute answers
queries consideration; missing bar indicates comparison system failed
answer queries within given timeout. Please note two dierent bars
ontology comparable may refer dierent sets queries, bar
needs considered isolation.
draw following conclusions results experiments.
TrOWL faster PAGOdA LUBM rolling up, UOBM rolling
FLY rolling up, incomplete 7 14 LUBM queries 3
4 UOBM queries. ChEMBL, TrOWL exceeds timeout performing
satisfiability check. remaining ontologies, PAGOdA efficient spite
fact TrOWL incomplete queries, even unsound several
UniProt queries.
354

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Pellet one robust systems evaluation. Although times
FLY ontology, succeeds computing answers remaining cases.
observe, however, cases Pellet significantly slower PAGOdA,
sometimes two orders magnitude.
HermiT answer queries one distinguished variable, could
evaluate atomic binary queries. see HermiT exceeds timeout many
cases. tests HermiT succeeds, significantly slower PAGOdA.
Although Hydrowl based theoretically sound complete algorithm,
found incomplete tests. also exceeded timeout queries
three ontologies, ran memory queries another two
ontologies, reported exception ChEMBL 1%. remaining cases,
significantly slower PAGOdA.
10.3.2 Scalability Tests
tested scalability PAGOdA LUBM, UOBM ontologies EBI
linked data platform. LUBM used datasets increasing size step n =
100. UOBM also used increasingly large datasets step n = 100 also
considered smaller step n = 5 hard queries. Finally, case EBIs datasets,
implemented data sampling algorithm based random walks computed subsets
data increasing sizes 1% original dataset 100% steps
10%. used test queries described Section 10.1 ontologies;
Section 10.3.1, computed ground answers and, whenever possible, used internalisation
additionally compute certain answers. test ontology measured following:
Pre-processing time. includes pre-processing steps Section 8 well
satisfiability checking (i.e., query processing Boolean unsatisfiability query).
Query processing time. time perform query processing steps
query given ontology. organise test queries following three
groups depending techniques exploited PAGOdA compute answers:
G1: queries lower upper bounds coincide;
G2: queries non-empty gap, summarisation able filter
remaining candidate answers;
G3: queries fully-fledged reasoner called ontology subset
least one test datasets.
scalability test, set timeout 5 hours answering queries 2.5 hours
individual query. LUBM UOBM, increased size dataset
PAGOdA exceeded timeout; ontologies, PAGOdA able answer
queries within timeout, even largest dataset.
Pellet compared system found sound complete test
ontologies queries, also conducted scalability tests it. scalability
Pellet is, however, limited: already failed LUBM(100), UOBM(5), well ChEMBL
355

fi3.0#

G1(18)"

2.5#

Thousands)seconds)

Thousands)seconds)

Zhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

2.0#
1.5#
1.0#

Q32"

Q34"

9"
8"
7"
6"
5"
4"
3"
2"

0.5#

1"

0.0#

0"

1#

100#

200#

300#

400#

500#

600#

700#

800#

1"

200"

300"

400"

500"

600"

700"

800"

(b) LUBM query processing

14"

G1(18)"

12"

Thousands)seconds)

Thousands)seconds)

(a) LUBM pre-processing

100"

10"
8"
6"

G2(1)"

Q18"

2.5"
2"
1.5"
1"

4"

0.5"

2"
0"
1"

100"

200"

300"

400"

0"

500"

0"

(c) UOBM pre-processing

100"

200"

300"

400"

500"

(d) UOBM query processing

Figure 7: Scalability tests benchmarks
10% Uniprot 10%. dataset Pellet managed process least two data
samples Reactome, succeeded samples smaller 40%. case
Reactome discussed detail later on.
results summarised Figures 7 8. ontology, plot time
size input dataset, query processing distinguish dierent groups
queries discussed above. PAGOdA behaves relatively uniformly queries G1
G2, plot average time per query groups. contrast, PAGOdAs
behaviour queries G3 quite variable, plot time individual query.
LUBM(n) shown Figure 7a, pre-processing fast, times appear scale linearly increasing dataset size. LUBM queries belong either G1 G3
latter group containing two queries. Figure 7b illustrates average query processing
time queries G1, never exceeds 13 seconds, well time
two queries G3 (Q32 Q34), reaches 8,000 seconds LUBM(800),
accounted HermiT.
UOBM(n) shown Figure 7c, pre-processing times significantly higher
LUBM, reflecting increased complexity ontology, still appear scale linearly
dataset size. LUBM, test queries contained G1,
processing times never exceeds 8 seconds UOBM(1) UOBM(500). found one
query G2. Processing times query somewhat longer G1
reached 569s UOBM(500). Finally, found one query (Q18) that, due UOBMs
356

fi12"

G1(1896)#

10"

Seconds(

Thousands))seconds)

PAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

8"
6"
4"
2"
0"
1%"

10%" 20%" 30%" 40%" 50%" 60%" 70%" 80%" 90%" 100%"

0.50#
0.45#
0.40#
0.35#
0.30#
0.25#
0.20#
0.15#
0.10#
0.05#
0.00#
1%# 10%# 20%# 30%# 40%# 50%# 60%# 70%# 80%# 90%# 100%#

(a) ChEMBL pre-processing
Pellet"

G1(128)"

14"

Seconds(

Hundreds(seconds(

PAGOdA"

(b) ChEMBL query processing

12"

G2(1)"

Q65"

Pellet_Q65"

1000"
800"

10"
8"

600"

6"

400"

4"
200"

2"
0"

0"
10%" 20%"

30%"

40%"

50%" 60%"

70%"

80%"

90%" 100%"

10%" 20%" 30%" 40%" 50%" 60%" 70%" 80%" 90%" 100%"

(c) Reactome pre-processing
Unsa9sable#

G1(236)"

2.0#

Seconds(

Thousands)seconds)

Satsiable#

(d) Reactome query processing

1.5#

G2(4)"

25"
20"
15"

1.0#
10"

0.5#

5"

0.0#

0"

1%# 10%# 20%# 30%# 40%# 50%# 60%# 70%# 80%# 90%# 100%#

1%"

(e) Uniprot pre-processing

10%"

20%"

30%"

40%"

(f) Uniprot query processing

Figure 8: Scalability tests EBI linked data platform
randomised data generation, dierent groups dierent datasets: UOBM(1),
UOBM(10) UOBM(50) G3, HermiT called relevant subsets
fully answer query; UOBM(40) G2, HermiT called
summary relevant subset; remaining cases shown Figure 7d
G1, lower upper bounds coincided. query timed UOBM(50),
due time taken HermiT reason relevant subset, shown
times remaining G1 G2 queries UOBM(500).
ChEMBL shown Figure 8a, pre-processing times significant manageable,
appear scale linearly dataset size. test queries contained G1.

357

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Total
L1 + U 1
L2 + U 1
L2 + U 2
L2 + U2|3

LUBM
(100)
35
26
33
33
33

UOBM
(1)
20
4
4
12
16

FLY

NPD

DBPedia

6
0
5
5
5

478
442
442
442
473

1247
1240
1241
1241
1246

ChEMBL
1%
1896
1883
1883
1883
1896

Reactome
10%
130
82
82
98
128

Uniprot
1%
240
204
204
204
236

Table 4: ]Queries answered dierent bounds
Figure 8b illustrates average processing times queries, less 0.5s
datasets increases smoothly dataset size.
Reactome shown Figure 8c, pre-processing times appear scale quite
smoothly. Groups G2 G3 contained one query, remaining queries
belonging G1. Query processing times shown Figure 8d. Average query processing time queries G1 never exceeded 10 seconds. Average processing times G2
queries appeared grow linearly size datasets, average time never exceeded
10 seconds. Finally, seen G3 query (Q65) much challenging,
could still answered less 900 seconds, even largest dataset.
already mentioned, also tested scalability Pellet Reactome, Pellet
able process samples size 10%, 20% 30%. pre-processing time Pellet
datasets comparable PAGOdA shown Figure 8c. Average queryprocessing times queries G1 G2 slightly higher PAGOdA.
contrast, times query Q65 significantly higher: 445s, 518s 2, 626s Reactome
10%, 20% 30%, respectively (see Figure 8d). Processing times Q65 PAGOdA,
however, grow smoothly thanks eectiveness subset extraction technique,
able keep input fully-fledged reasoner small, even largest datasets.
Uniprot contrast cases, Uniprot whole unsatisfiable; sampling
technique can, however, produce satisfiable subset. Figure 8e illustrates pre-processing
times. seen, drop abruptly unsatisfiable samples (50% larger);
unsatisfiability efficiently detected lower bound. figure shows
time detect inconsistency 100% even less 90%;
time dominated loading time, I/O performance varies run run. Query
processing times considered satisfiable samples (see Figure 8f).
queries G3, four G2. observe average times queries
appear scale linearly data size groups.
10.3.3 Effectiveness Implemented Techniques
evaluated eectiveness various reasoning techniques implemented
PAGOdA comparing numbers test queries fully answered using
relevant technique.
Query bounds Sections 4 5 described dierent techniques computing lower
upper bound query answers. Table 4 illustrates eectiveness bounds
358

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Facts
Rules

LUBM
0.5%
3.7%

UOBM
10.4%
10.9%

Fly
7.3%
0.9%

NPD
16.5%
18.4%

DBPedia
9 10 5 %
2.4%

Reactome
5.2%
5.3%

Uniprot
4 10 4 %
1.1%

Table 5: Size largest subsets given percentage input rules facts.
terms number queries bounds coincided test ontologies.
table, refer lower bound described Section 4.1 L1 aggregated
lower bound described Section 4.3 L2 . Similarly, refer three upper bound
computation techniques discussed Section 5.4 U1 , U2 , U3 combined upper
bound U2|3 . observe following experiments:
basic lower upper bounds suffice answer queries many
test ontologies. particular, L1 U1 matched 26 35 queries
LUBM(100), 442 478 NPD, 240 1247 DBPedia, 1883 1896
ChEMBL, 204 240 Uniprot.
aggregated lower bound L2 eective case FLY, basic
bounds match query. also useful LUBM, yielding matching
bounds 7 queries.
refined treatment existential rules described Section 5.2, yields
upper bound U2 , especially eective UOBM(1) Reactome, many
existentially quantified rules already satisfied lower bound materialisation.
Finally, refined treatment disjunctive rules Section 5.3, yields combined upper bound U2|3 , instrumental obtaining additional matching bounds
non-Horn ontologies. could answer additional 4 queries UOBM(1), 31
NPD, 5 DBPedia, 13 ChEMBL, 30 Reactome, 32 Uniprot.
Overall, obtained matching bounds queries test ontologies:
could answer queries ChEMBL, 1 FLY DBPedia, 2
Reactome LUBM(100), 4 UOBM(1) Uniprot, 5 NPD.
Subset extraction Table 5 shows, dataset, maximum percentage facts
rules included relevant subset test queries non-matching
bounds. observe subset extraction eective cases terms facts
rules. Uniprot DBPedia, reduction data size especially dramatic.
also interesting observe large reduction number rules FLY,
rather complex ontology. Finally, subset extraction least eective NPD
UOBM, even cases reduction almost one order magnitude
size ontology dataset.
turn attention summarisation dependency analysis. eectiveness
techniques measured number hard calls HermiT required
fully answer query, call HermiT considered hard knowledge base
passed HermiT summary. first row Table 6 shows number gap
359

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

L2 + U2|3
+ Sum
+ Dep

LUBM
26 14
26 14
1
1

264
264
1

UOBM
112 1470
0 1444
0
1

264
264
1

FLY
344
344
7

DBPedia
10
0
0

NPD
326
0
0

Reactome
18
52
0
52
0
37

UniProt
168
0
0

Table 6: number hard calls HermiT fully answer query
answers query L2 U2|3 bounds match. Without optimisation,
would call HermiT number times fully answer query. Row 2
(resp. row 3) shows number hard calls HermiT applying summarisation (resp.
summarisation plus dependency analysis). mentioned above, respectively 5
4 queries non-matching bounds NPD UniProt. However,
groups, summarisation dependency analysis identical eects queries
group, present one representative query ontology.
Summarisation already discussed, summarisation enables PAGOdA fully answer
number test queries non-empty gaps. instrumental fully answering one
query UOBM(1), DBPedia Reactome, well 5 queries NPD, 4
queries Uniprot. Even cases summarisation suffice fully answer
query, eective reducing size gap. instance, one queries
UOBM(1) obtained 1,470 gap answers, 26 ruled summarisation.
Dependency analysis LUBM(100) two queries gap 26 answers
14 answers, respectively; cases, answers merged single group,
hence single call HermiT sufficed complete computation. Similarly, UOBM(1)
single call HermiT sufficient, even though three queries gap
involved large number candidate answers. FLY, 344 answers remaining
verified summarisation, 7 hard calls HermiT required. Finally,
case Reactome one query 52 gap answers, dependency analysis reduced
number calls HermiT 37.

11. Conclusions
paper, investigated novel pay-as-you-go approach conjunctive query
answering combines datalog reasoner fully-fledged reasoner. key feature
approach delegates bulk computation datalog reasoner
resorts fully-fledged reasoner necessary fully answer query.
reasoning techniques proposed general applicable
wide range knowledge representation languages. main goal practice, however,
realise approach highly scalable robust query answering system
OWL 2 DL ontologies, called PAGOdA. extensive evaluation
confirmed feasibility approach practice, also system PAGOdA
significantly ourperforms state-of-the art reasoning systems terms robustness
scalability. particular, experiments using ontologies EBI linked data
platform shown PAGOdA capable fully answering queries highly complex
expressive ontologies realistic datasets containing hundreds millions facts.
360

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Acknowledgments
extended version conference publications (Zhou, Nenov, Cuenca Grau, &
Horrocks, 2014; Zhou, Nenov, Grau, & Horrocks, 2013). work supported
Royal Society Royal Society Research Fellowship, EPSRC projects
Score!, MaSI3 , DBOnto, well EU FP7 project Optique.

References
Abiteboul, S., Hull, R., & Vianu, V. (Eds.). (1995). Foundations Databases: Logical
Level. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
Acciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &
Rosati, R. (2005). QuOnto: Querying ontologies. Veloso, M. M., & Kambhampati,
S. (Eds.), AAAI 2005, Proceedings Twentieth National Conference Artificial Intelligence Seventeenth Innovative Applications Artificial Intelligence
Conference, July 9-13, 2005, Pittsburgh, Pennsylvania, USA, pp. 16701671. AAAI
Press / MIT Press.
Afrati, F. N., Cosmadakis, S. S., & Yannakakis, M. (1995). datalog vs. polynomial time.
J. Comput. Syst. Sci., 51 (2), 177196.
Alviano, M., Faber, W., Greco, G., & Leone, N. (2012a). Magic sets disjunctive datalog
programs. Artificial Intelligence, 187188, 156192.
Alviano, M., Faber, W., Leone, N., & Manna, M. (2012b). Disjunctive datalog existential quantifiers: Semantics, decidability, complexity issues. Theory Practice
Logic Programming, 12 (4-5), 701718.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. IJCAI 2015,
Proceedings Nineteenth International Joint Conference Artificial Intelligence,
Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 364369.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (2003).
Description Logic Handbook: Theory, Implementation, Applications. Cambridge Univ. Press.
Bagosi, T., Calvanese, D., Hardi, J., Komla-Ebri, S., Lanti, D., Rezk, M., Rodriguez-Muro,
M., Slusnys, M., & Xiao, G. (2014). Ontop framework ontology based data access. Zhao, D., Du, J., Wang, H., Wang, P., Ji, D., & Pan, J. Z. (Eds.), CSWS 2014,
Proceedings Semantic Web Web Science - 8th Chinese Conference, Wuhan,
China, August 8-12, 2014, Revised Selected Papers, Vol. 480 Communications
Computer Information Science, pp. 6777. Springer.
Bancilhon, F., Maier, D., Sagiv, Y., & Ullman, J. D. (1986). Magic sets strange
ways implement logic programs. Silberschatz, A. (Ed.), Proceedings Fifth
ACM SIGACT-SIGMOD Symposium Principles Database Systems, March 2426, 1986, Cambridge, Massachusetts, USA, pp. 115. ACM.
Beeri, C., Naqvi, S. A., Ramakrishnan, R., Shmueli, O., & Tsur, S. (1987). Sets negation
logic database language (LDL1). Vardi, M. Y. (Ed.), Proceedings Sixth
361

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

ACM SIGACT-SIGMOD-SIGART Symposium Principles Database Systems,
March 23-25, 1987, San Diego, California, USA, pp. 2137. ACM.
Bishop, B., Kiryakov, A., Ognyano, D., Peikov, I., Tashev, Z., & Velkov, R. (2011).
OWLIM: family scalable semantic repositories. Semantic Web, 2 (1), 3342.
Bourhis, P., Morak, M., & Pieris, A. (2013). impact disjunction query answering guarded-based existential rules. IJCAI 2013, Proceedings 23rd
International Joint Conference Artificial Intelligence, Beijing, China, August 3-9,
2013, pp. 796802. AAAI Press.
Broekstra, J., Kampman, A., & van Harmelen, F. (2002). Sesame: generic architecture
storing querying RDF RDF schema. Horrocks, I., & Hendler, J. A.
(Eds.), ISWC 2002, Proceedings Semantic Web - First International Semantic
Web Conference, Sardinia, Italy, June 9-12, 2002, Proceedings, Vol. 2342 Lecture
Notes Computer Science, pp. 5468. Springer.
Bry, F., Eisinger, N., Eiter, T., Furche, T., Gottlob, G., Ley, C., Linse, B., Pichler, R., & Wei,
F. (2007). Foundations rule-based query answering. Antoniou, G., Amann, U.,
Baroglio, C., Decker, S., Henze, N., Patranjan, P., & Tolksdorf, R. (Eds.), Reasoning
Web 2007, Vol. 4636 Lecture Notes Computer Science, pp. 1153. Springer.
Cal, A., Gottlob, G., & Kifer, M. (2013). Taming infinite chase: Query answering
expressive relational constraints. Journal Artificial Intelligence Research, 48,
115174.
Cal, A., Gottlob, G., & Lukasiewicz, T. (2012). general datalog-based framework
tractable query answering ontologies. J. Web Sem., 14, 5783.
Cal, A., Gottlob, G., Lukasiewicz, T., Marnette, B., & Pieris, A. (2010). Datalog+/-:
family logical knowledge representation query languages new applications.
LICS 2010, Proceedings 25th Annual IEEE Symposium Logic Computer
Science, 11-14 July 2010, Edinburgh, United Kingdom, pp. 228242. IEEE Computer
Society.
Cal, A., Gottlob, G., & Pieris, A. (2011). New expressive languages ontological query
answering. Burgard, W., & Roth, D. (Eds.), AAAI 2011, Proceedings TwentyFifth AAAI Conference Artificial Intelligence, San Francisco, California, USA,
August 7-11, 2011, Vol. 2, pp. 15411546. AAAI Press.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M.,
Rosati, R., Ruzzi, M., & Savo, D. F. (2011). MASTRO system ontology-based
data access. Semantic Web, 2 (1), 4353.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering description logics: DL-Lite family.
Journal Automated Reasoning, 39 (3), 385429.
Chortaras, A., Trivela, D., & Stamou, G. B. (2011). Optimized query rewriting OWL
2 QL. Bjrner, N., & Sofronie-Stokkermans, V. (Eds.), CADE 23, Proceedings
23rd International Conference Automated Deduction, Wroclaw, Poland, July
31 - August 5, 2011, Vol. 6803 Lecture Notes Computer Science, pp. 192206.
Springer.
362

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Console, M., Mora, J., Rosati, R., Santarelli, V., & Savo, D. F. (2014). Eective computation
maximal sound approximations description logic ontologies. ISWC 2014,
Proceedings Semantic Web - 13th International Semantic Web Conference,
Riva del Garda, Italy, October 19-23, 2014. Proceedings, Part II, pp. 164179.
Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.
(2013). Acyclicity notions existential rules application query answering
ontologies. Journal Artificial Intelligence Research, 47, 741808.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P. F., & Sattler, U.
(2008). OWL 2: next step OWL. Journal Web Semantics, 6 (4), 309322.
Cuenca Grau, B., Motik, B., Stoilos, G., & Horrocks, I. (2012). Completeness guarantees
incomplete ontology reasoners: Theory practice. Journal Artificial Intelligence
Research, 43, 419476.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity expressive
power logic programming. ACM Computing Surveys, 33 (3), 374425.
Dolby, J., Fokoue, A., Kalyanpur, A., Kershenbaum, A., Schonberg, E., Srinivas, K., &
Ma, L. (2007). Scalable semantic retrieval summarization refinement.
AAAI 2007, Proceedings Twenty-Second AAAI Conference Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 299304. AAAI
Press.
Dolby, J., Fokoue, A., Kalyanpur, A., Schonberg, E., & Srinivas, K. (2009). Scalable highly
expressive reasoner (SHER). Journal Web Semantics, 7 (4), 357361.
Eiter, T., Fink, M., Tompits, H., & Woltran, S. (2004). Simplifying logic programs
uniform strong equivalence. LPNMR 2004, Proceedings Logic Programming
Nonmonotonic Reasoning - 7th International Conference, Fort Lauderdale, FL,
USA, January 6-8, 2004, Proceedings, pp. 8799.
Eiter, T., Lutz, C., Ortiz, M., & Simkus, M. (2009). Query answering description logics
transitive roles. Boutilier, C. (Ed.), IJCAI 2009, Proceedings 21st
International Joint Conference Artificial Intelligence, Pasadena, California, USA,
July 11-17, 2009, pp. 759764.
Eiter, T., Ortiz, M., & Simkus, M. (2012). Conjunctive query answering description
logic SH using knots. Journal Computer System Sciences, 78 (1), 4785.

Erling, O., & Mikhailov, I. (2009). Virtuoso: RDF support native RDBMS. Virgilio,
R. D., Giunchiglia, F., & Tanca, L. (Eds.), Semantic Web Information Management
- Model-Based Perspective, pp. 501519. Springer.
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., & Wang, Z. (2014). HermiT: OWL 2
reasoner. Journal Automated Reasoning, 53 (3), 245269.
Glimm, B., Lutz, C., Horrocks, I., & Sattler, U. (2008). Conjunctive query answering
description logic SHIQ. Journal Artificial Intelligence Research, 31, 157204.

Grosof, B. N., Horrocks, I., Volz, R., & Decker, S. (2003). Description logic programs:
combining logic programs description logic. Hencsey, G., White, B., Chen,
Y. R., Kovacs, L., & Lawrence, S. (Eds.), WWW 2003, Proceedings Twelfth
363

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

International World Wide Web Conference, Budapest, Hungary, May 20-24, 2003,
pp. 4857. ACM.
Guo, Y., Pan, Z., & Heflin, J. (2005). LUBM: benchmark OWL knowledge base
systems. Journal Web Semantics, 3 (2-3), 158182.
Haarslev, V., Hidde, K., Moller, R., & Wessel, M. (2012). RacerPro knowledge representation reasoning system. Semantic Web, 3 (3), 267277.
Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. KR
2006, Proceedings Tenth International Conference Principles Knowledge
Representation Reasoning, Lake District United Kingdom, June 2-5, 2006,
pp. 5767.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDF
OWL: making web ontology language. Journal Web Semantics, 1 (1),
726.
Horrocks, I., & Tessaris, S. (2000). conjunctive query language description logic
aboxes. Kautz, H. A., & Porter, B. W. (Eds.), AAAI/IAAI 2000, Proceedings
Seventeenth National Conference Artificial Intelligence Twelfth Conference
Innovative Applications Artificial Intelligence, July 30 - August 3, 2000, Austin,
Texas, USA., pp. 399404. AAAI Press / MIT Press.
Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning description logics reduction
disjunctive datalog. Journal Automated Reasoning, 39 (3), 351384.
Jimenez-Ruiz, E., & Cuenca Grau, B. (2011). LogMap: Logic-based scalable ontology
matching. Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy,
N. F., & Blomqvist, E. (Eds.), ISWC 2011, Semantic Web - 10th International
Semantic Web Conference, Bonn, Germany, October 23-27, 2011, Proceedings, Part
I, Vol. 7031 Lecture Notes Computer Science, pp. 273288. Springer.
Kemp, D. B., Srivastava, D., & Stuckey, P. J. (1995). Bottom-up evaluation query
optimization well-founded models. Theoretical Computer Science, 146 (12), 145
184.
Kollia, I., & Glimm, B. (2013). Optimizing SPARQL query answering OWL ontologies.
Journal Artificial Intelligence Research, 48, 253303.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). combined approach ontology-based data access. Walsh, T. (Ed.), IJCAI 2011,
Proceedings 22nd International Joint Conference Artificial Intelligence,
Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 26562661. IJCAI/AAAI.
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).
DLV system knowledge representation reasoning. ACM Transactions
Computational Logic, 7 (3), 499562.
Leskovec, J., & Faloutsos, C. (2006). Sampling large graphs. KDD 2006, Proceedings
Twelfth ACM SIGKDD International Conference Knowledge Discovery
Data Mining, Philadelphia, PA, USA, August 20-23, 2006, pp. 631636.

364

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Lutz, C. (2008). complexity conjunctive query answering expressive description logics. Armando, A., Baumgartner, P., & Dowek, G. (Eds.), IJCAR 2008,
Proceedings 4th International Joint Conference Automated Reasoning, Sydney,
Australia, August 12-15, 2008, Vol. 5195 Lecture Notes Computer Science, pp.
179193. Springer.
Lutz, C., Seylan, I., Toman, D., & Wolter, F. (2013). combined approach OBDA:
Taming role hierarchies using filters. Alani, H., Kagal, L., Fokoue, A., Groth, P. T.,
Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty, C., & Janowicz, K. (Eds.),
ISWC 2013, Proceedings Semantic Web - 12th International Semantic Web
Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part I, Vol.
8218 Lecture Notes Computer Science, pp. 314330. Springer.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering description logic EL using relational database system. Boutilier, C. (Ed.), IJCAI
2009, Proceedings 21st International Joint Conference Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009, pp. 20702075.
Ma, L., Yang, Y., Qiu, Z., Xie, G. T., Pan, Y., & Liu, S. (2006). Towards complete OWL
ontology benchmark. Sure, Y., & Domingue, J. (Eds.), ESWC 2006, Semantic
Web: Research Applications, 3rd European Semantic Web Conference, Budva,
Montenegro, June 11-14, 2006, Proceedings, Vol. 4011 Lecture Notes Computer
Science, pp. 125139. Springer.
Manola, F., & Miller, E. (2004). RDF primer. W3C Recommendation. Available
http://www.w3.org/TR/rdf-primer/.
Marnette, B. (2009). Generalized schema-mappings: termination tractability.
PODS 2009, Proceedings Twenty-Eigth ACM SIGMOD-SIGACT-SIGART
Symposium Principles Database Systems, June 19 - July 1, 2009, Providence,
Rhode Island, USA, pp. 1322.
McBride, B. (2001). Jena: Implementing RDF model syntax specification.
SemWeb 2001, Proceedings Second International Workshop Semantic
Web.
Moller, R., Neuenstadt, C., Ozcep, O. L., & Wandelt, S. (2013). Advances accessing
big data expressive ontologies. Timm, I. J., & Thimm, M. (Eds.), KI 2013,
Proceedings Advances Artificial Intelligence - 36th Annual German Conference
AI, Koblenz, Germany, September 16-20, 2013, Vol. 8077 Lecture Notes
Computer Science, pp. 118129. Springer.
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2012). OWL 2
Web Ontology Language Profiles (second edition). W3C Recommendation. Available
http://www.w3.org/TR/owl2-profiles/.
Motik, B., Nenov, Y., Piro, R., Horrocks, I., & Olteanu, D. (2014). Parallel materialisation
datalog programs centralised, main-memory RDF systems. Brodley, C. E., &
Stone, P. (Eds.), AAAI 2014, Proceedings Twenty-Eighth AAAI Conference
Artificial Intelligence, July 27 -31, 2014, Quebec City, Quebec, Canada., pp. 129137.
AAAI Press.
365

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Motik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau reasoning description logics.
Journal Artificial Intelligence Research, 36, 165228.
Ortiz, M., Rudolph, S., & Simkus, M. (2011). Query answering horn fragments
description logics SHOIQ SROIQ. IJCAI 2011, Proceedings 22nd
International Joint Conference Artificial Intelligence, Barcelona, Catalonia, Spain,
July 16-22, 2011, pp. 10391044.
Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. AAAI 2007,
Proceedings Twenty-Second AAAI Conference Artificial Intelligence, July
22-26, 2007, Vancouver, British Columbia, Canada, pp. 14341439.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewriting
description logic constraints. Journal Applied Logic, 8 (2), 186209.
PrudHommeaux, E., & Carothers, G. (2014). RDF 1.1 Turtle. W3C Recommendation.
Available http://www.w3.org/TR/turtle/.
Robinson, J. A., & Voronkov, A. (Eds.). (2001). Handbook Automated Reasoning (in 2
volumes). Elsevier MIT Press.
Rodriguez-Muro, M., & Calvanese, D. (2012). High performance query answering
DL-Lite ontologies. Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), KR 2012,
Proceedings Principles Knowledge Representation Reasoning, Thirteenth
International Conference, Rome, Italy, June 10-14, 2012, pp. 308318. AAAI Press.
Rosati, R. (2012). Prexto: Query rewriting extensional constraints DL - lite.
Simperl, E., Cimiano, P., Polleres, A., Corcho, O., & Presutti, V. (Eds.), ESWC
2012, Proceedings Semantic Web: Research Applications - 9th Extended
Semantic Web Conference, Heraklion, Crete, Greece, May 27-31, 2012, Vol. 7295
Lecture Notes Computer Science, pp. 360374. Springer.
Rudolph, S., & Glimm, B. (2010). Nominals, inverses, counting, conjunctive queries or:
infinity friend!. Journal Artificial Intelligence Research, 39, 429481.
Schaerf, A. (1993). complexity instance checking problem concept languages
existential quantification. Komorowski, H. J., & Ras, Z. W. (Eds.), ISMIS
1993, Proceedings Methodologies Intelligent Systems, 7th International Symposium, Trondheim, Norway, June 15-18, 1993, Vol. 689 Lecture Notes Computer
Science, pp. 508517. Springer.
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: practical
OWL-DL reasoner. Journal Web Semantics, 5 (2), 5153.
Staab, S., & Studer, R. (Eds.). (2004). Handbook Ontologies. International Handbooks
Information Systems. Springer.
Stefanoni, G., & Motik, B. (2015). Answering conjunctive queries EL knowledge bases
transitive reflexive roles. Bonet, B., & Koenig, S. (Eds.), AAAI 2015,
Proceedings 29th AAAI Conference Artificial Intelligence, Austin, TX, USA.
AAAI Press. appear.
Stefanoni, G., Motik, B., & Horrocks, I. (2013). Introducing nominals combined
query answering approaches EL. AAAI 2013, Proceedings Twenty-Seventh
AAAI Conference Artificial Intelligence, pp. 11771183.
366

fiPAGOdA: Pay-As-You-Go Query Answering Using Datalog Reasoner

Stefanoni, G., Motik, B., Krotzsch, M., & Rudolph, S. (2014). complexity answering
conjunctive navigational queries OWL 2 EL knowledge bases. Journal
Artificial Intelligence Research, 51, 645705.
Stoilos, G. (2014a). Hydrowl: hybrid query answering system OWL 2 DL ontologies.
RR 2014, Proceedings Web Reasoning Rule Systems - 8th International
Conference, Athens, Greece, September 15-17, 2014, pp. 230238.
Stoilos, G. (2014b). Ontology-based data access using rewriting, OWL 2 RL systems
repairing. Presutti, V., dAmato, C., Gandon, F., dAquin, M., Staab, S., & Tordai,
A. (Eds.), Semantic Web: Trends Challenges - 11th International Conference,
ESWC 2014, Anissaras, Crete, Greece, May 25-29, 2014. Proceedings, Vol. 8465
Lecture Notes Computer Science, pp. 317332. Springer.
Stoilos, G., & Stamou, G. B. (2014). Hybrid query answering OWL ontologies.
Schaub, T., Friedrich, G., & OSullivan, B. (Eds.), ECAI 2014 - 21st European Conference Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic - Including Prestigious Applications Intelligent Systems (PAIS 2014), Vol. 263 Frontiers
Artificial Intelligence Applications, pp. 855860. IOS Press.
Thomas, E., Pan, J. Z., & Ren, Y. (2010). Trowl: Tractable OWL 2 reasoning infrastructure.
ESWC 2010, Proceedings Semantic Web: Research Applications, 7th
Extended Semantic Web Conference, Heraklion, Crete, Greece, May 30 - June 3, 2010,
Part II, pp. 431435.
Tserendorj, T., Rudolph, S., Krotzsch, M., & Hitzler, P. (2008). Approximate OWLreasoning screech. Calvanese, D., & Lausen, G. (Eds.), RR 2008, Proceedings
Web Reasoning Rule Systems, Second International Conference, Karlsruhe,
Germany, October 31-November 1, 2008, Vol. 5341 Lecture Notes Computer
Science, pp. 165180. Springer.
W3C SPARQL Working Group (2013). SPARQL 1.1 Overview. W3C Recommendation.
Available http://www.w3.org/TR/sparql11-overview/.
Wandelt, S., Moller, R., & Wessel, M. (2010). Towards scalable instance retrieval
ontologies. International Journal Software Informatics, 4 (3), 201218.
Wu, Z., Eadon, G., Das, S., Chong, E. I., Kolovski, V., Annamalai, M., & Srinivasan, J.
(2008). Implementing inference engine RDFS/OWL constructs user-defined
rules oracle. Alonso, G., Blakeley, J. A., & Chen, A. L. P. (Eds.), ICDE 2008,
Proceedings 24th International Conference Data Engineering, April 7-12,
2008, Cancun, Mexico, pp. 12391248. IEEE.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2014). Pay-as-you-go OWL query
answering using triple store. Proceedings Twenty-Eighth AAAI Conference
Artificial Intelligence.
Zhou, Y., Nenov, Y., Grau, B. C., & Horrocks, I. (2013). Complete query answering
horn ontologies using triple store. Semantic Web - ISWC 2013 - 12th
International Semantic Web Conference, Sydney, NSW, Australia, October 21-25,
2013, Proceedings, Part I, pp. 720736.

367

fiJournal Artificial Intelligence Research 54 (2015) 83-122

Submitted 02/15; published 09/15

Word vs. Class-Based Word Sense Disambiguation
Ruben Izquierdo

RUBEN . IZQUIERDOBEVIA @ VU . NL

VU University Amsterdam
Amsterdam. Netherlands

Armando Suarez

ARMANDO @ DLSI . UA . ES

University Alicante
Alicante. Spain

German Rigau

GERMAN . RIGAU @ EHU . ES

University Basque Country
San Sebastian. Spain

Abstract
empirically demonstrated Word Sense Disambiguation (WSD) tasks last SensEval/SemEval exercises, assigning appropriate meaning words context resisted
attempts successfully addressed. Many authors argue one possible reason could
use inappropriate sets word meanings. particular, WordNet used de-facto
standard repository word meanings tasks. Thus, instead using word
senses defined WordNet, approaches derived semantic classes representing groups
word senses. However, meanings represented WordNet used WSD
fine-grained sense level coarse-grained semantic class level (also called SuperSenses). suspect appropriate level abstraction could levels.
contributions paper manifold. First, propose simple method automatically
derive semantic classes intermediate levels abstraction covering nominal verbal WordNet meanings. Second, empirically demonstrate automatically derived semantic classes
outperform classical approaches based word senses coarse-grained sense groupings.
Third, also demonstrate supervised WSD system benefits using new semantic classes additional semantic features reducing amount training examples.
Finally, also demonstrate robustness supervised semantic class-based WSD system
tested domain corpus.

1. Introduction
Word Sense Disambiguation (WSD) intermediate Natural Language Processing (NLP) task
consists assigning correct lexical interpretation ambiguous words depending surrounding context (Agirre & Edmonds, 2007; Navigli, 2009). One successful approaches
last years supervised learning examples, Machine Learning classification
models induced semantically annotated corpora (Marquez, Escudero, Martnez, & Rigau,
2006). Quite often, machine learning systems obtained better results knowledge-based
ones, shown experimental work international evaluation exercises Senseval SemEval1 . Nevertheless, lately weakly supervised knowledgebased approaches reaching
performance close supervised techniques specific tasks. tasks,
1. information competitions found http://www.senseval.org.
c
2015
AI Access Foundation. rights reserved.

fiI ZQUIERDO , U AREZ & R IGAU

corpora usually manually annotated experts word senses taken particular lexical
semantic resource, commonly WordNet (Fellbaum, 1998).
However, WordNet widely criticized sense repository often provides
finegrained sense distinctions higher level applications like Machine Translation (MT)
Question & Answering (AQ). fact, WSD low level semantic granularity resisted
attempts inferring robust broad-coverage models. seems many wordsense distinctions
subtle captured automatic systems current small volumes wordsense
annotated examples. Using WordNet sense repository, organizers English all-words
task SensEval-3 reported inter-annotation agreement 72.5% (Snyder & Palmer, 2004). Interestingly, result difficult outperform state-of-the-art sense-based WSD systems.
Moreover, supervised sensebased approaches biased towards frequent sense
predominant sense training data. Therefore, performance supervised sensebased
systems strongly punished applied domain specific texts sense distribution differs considerably respect sense distribution training corpora (Escudero, Marquez,
& Rigau., 2000).
paper try overcome problems facing task WSD Semantic
Class point view instead traditional word sense based approach. semantic class
seen abstract concept groups subconcepts word senses sharing semantic properties features. Examples semantic classes VEHICLE, FOOD ANIMAL. hypothesis
using appropriate set semantic classes instead word-senses could help WSD several
aspects:
higher level abstraction could ease integration WSD systems higher
level NLP applications Machine Translation Question & Answering
Grouping together semantically coherent sets training examples could also increase
robustness supervised WSD systems
socalled bottleneck acquisition problem could also alleviated
points explained along paper. Following hypothesis propose
create classifiers based semantic classes instead word sense experts. One semantic classifier
trained semantic class final system assign proper semantic class
ambiguous word (instead sense traditional approaches). example, using
automatically derived semantic classes (that introduced later), three senses church
WordNet 1.6 subsumed semantic classes R ELIGIOUS RGANIZATION, B UILDING
R ELIGIOUS C EREMONY. Also note semantic classes still discriminate among three
different senses word church. instance, assign semantic class B UILDING
occurrence church context, still know refers second sense. Additionally,
semantic class B UILDING covers six times training examples
covered second sense church.
example text senseval2 automatically annotated semantic classes seen
Figure 1. shows automatic annotations classbased classifiers different semantic classes. BLC stands Basic Level Concepts2 (Izquierdo, Suarez, & Rigau, 2007), SS
2. use following format throughout paper refer particular sense: wordnum
pos , pos
part-of-speech: n nouns, v verbs, adjectives r adverbs, num stands sense number.

84

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

SuperSenses (Ciaramita & Johnson, 2003), WND WordNet Domains (Magnini & Cavaglia,
2000; L. Bentivogli & Pianta, 2004) SUMO Suggested Upper Merged Ontology (Niles &
Pease, 2001). Incorrect assignments marked italics. correct tags included
brackets next automatic ones. Obviously, semantic resources relate senses different
level abstraction using diverse semantic criteria properties could interest subsequent semantic processing. Moreover, combination could improve overall results since
offer different semantic perspectives text.
Id
1
2
3
4
6
7
8

Word

ancient
stone
church
amid

fields

BLC

SS

WND

SUMO

artifact1n
building1n

noun.artifact
noun.artifact

building
building

Mineral
Building

geographic area1n
[physical object1n ]

noun.location
[noun.object]

factotum [geography]

LandArea

9
10
11

,

sound

property2n

noun.attribute

factotum [acoustics]

RadiatingSound
[SoundAttribute]

12
13


bells

device1n

noun.artifact

MusicalInstrument

14
15
16
17
18

cascading


tower
calling

move2v

verb.motion

factotum [acoustics]
factotum

construction3n
designate2v
[request2v ]

noun.artifact
factotum
verb.stative
factotum
[verb.communication]

Building
Communication
[Requesting]

19
20


faithful

[sogroup1n
cial group1n ]

noun.group

person [religion]

Group

21
22


evensong

time day1n
[writing2n ]

noun.communication

religion

TimeInterval
[Text]

Motion

Table 1: Example automatic annotation text several semantic class labels
main goal research investigate performance alternative Semantic Classes
derived WordNet supervised WSD. First, propose system automatically extract sets
semantically coherent groupings nominal verbal senses WordNet. system
allows generate arbitrary sets semantic classes distinct levels abstraction. Second,
also analyze impact respect alternative Semantic Classes performing classbased
WSD. empirical results show automatically generated classes performs better
created manually (WNDomains, SUMO, SuperSenses, etc.) capturing precise
information. Third, also demonstrate supervised WSD system benefits using
new semantic classes additional semantic features reducing amount training
85

fiI ZQUIERDO , U AREZ & R IGAU

examples. Finally, show supervised class-based system adapted particular
domain. Traditional word sense based systems also included comparison purposes.
Summarizing, research empirically investigates:
performance alternative semantic groupings used supervised class-based
WSD system
impact class-based semantic features supervised WSD framework
required amount training examples needed class-based WSD order obtain
competitive results
relative performance class-based WSD systems respect WSD based word
experts
robustness class-based WSD system specific domains
Moreover, tested domain dataset, supervised class-based WSD system obtains slightly better results state-of-the-art word sense based WSD system, ItMakesSense
system presented Zhong Ng (2010).
introduction, present work directly related research supervised
WSD based semantic classes. Then, Section 3 presents sense-groupings semantic classes
used study. Section 4 explains method automatically derive semantic classes
WordNet different levels abstraction. Moreover analysis different semantic groupings
included. Section 5, presents system developed perform supervised class-based
WSD. performance system shown Section 6, system tested several
WSD datasets provided international evaluations. comparison participants
competitions introduced sections 7 8. experiments system applied
specific domain analyzed Section 9. Finally, conclusions future work presented
section 10.

2. Related Work
field WSD broad. large amount publications WSD
last 50 years. section revises relevant WSD approaches dealing appropriate
sets meanings word have.
research focused deriving different word-sense groupings overcome
finegrained distinctions WordNet (Hearst & Schutze, 1993; Peters, Peters, & Vossen, 1998;
Mihalcea & Moldovan, 2001; Agirre & de Lacalle, 2003; Navigli, 2006; Snow, S., D., & A., 2007).
is, provide methods grouping senses word, thus producing coarser word
sense groupings. example, word church three senses WordNet 1.6, sense
grouping presented Snow et al. (2007) produces unique grouping. is, according
approach church monosemous.
OntoNotes project (Hovy, Marcus, Palmer, Ramshaw, & Weischedel, 2006), different
meanings word considered kind tree, ranging coarse concepts root
finegrained meanings leaves. merging increased fine coarse grained
obtaining inter annotator agreement around 90%. coarse-grained repository used
86

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

WSD lexical sample task SemEval-2007 (Pradhan, Dligach, & Palmer, 2007),
systems scored 88.7% Fscore. Note merging created word following
manual costly process.
Similarly previous approach, another task organized within SemEval-2007
consisted traditional WSD word task using another coarsegrained sense repository derived
WordNet (Navigli, Litkowski, & Hargraves, 2007). case WordNet synsets
automatically linked Oxford Dictionary English (ODE) using graph algorithm.
meanings word linked ODE entry merged coarse sense. systems
achieving top scores followed supervised approaches taking advantage different corpora
training, reaching top Fscore 82.50%.
previous cases aimed solving granularity problem word sense
definitions WordNet. However, approaches still word experts (one classifier trained
word). Obviously, decreasing average polysemy word using coarsersenses
makes easier classification choice. result, performance systems increase
cost reducing discriminative power.
Conversely, instead word experts, approach creates semantic class experts.
semantic classifiers exploit diverse information extracted meanings different
words belong class.
Wikipedia (Wikipedia, 2015) also recently used overcome problems supervised learning methods: excessively finegrained definition meanings, lack annotated data
strong domain dependence existing annotated corpora. way, Wikipedia provides
new source annotated data, large constantly expansion (Mihalcea, 2007; Gangemi,
Nuzzolese, Presutti, Draicchio, Musetti, & Ciancarini, 2012).
contrast, research focused using predefined sets sense-groupings
learning classbased classifiers WSD (Segond, Schiller, Greffenstette, & Chanod, 1997; Ciaramita & Johnson, 2003; Villarejo, Marquez, & Rigau, 2005; Curran, 2005; Ciaramita & Altun,
2006; Izquierdo, Suarez, & Rigau, 2009). is, grouping senses different words
explicit comprehensive semantic class. Also work presented Mihalcea, Csomai,
Ciaramita (2007) makes use three different sets semantic classes (WordNet classes two
Named Entity annotated corpora) train sequential classifiers. classifiers trained using
basic features, collocations semantic features, reach performance around 60%
14th position SemEval-2007 allwords task.
semantic classes WordNet (also called SuperSenses) widely used different
works. instance, Paa Reichartz (2009a) apply Conditional Random Fields model
sequential context words relation SuperSenses. also extend model include
potential SuperSenses word training data. F1 score 82.8% reported (both
nouns verbs) potential labels used (no training data all) 1% worse
using training data right labels. Although interesting, evaluate
system applying 5-fold cross validation SemCor.

3. Semantic Classes Levels Abstraction
meanings represented WordNet used WSD fine-grained sense
level coarse-grained semantic class level (also called SuperSenses). suspect
appropriate level abstraction could found levels. section propose
87

fiI ZQUIERDO , U AREZ & R IGAU

simple method automatically derive semantic classes intermediate levels abstraction covering nominal verbal WordNet meanings. First, introduce WordNet, semantic resource
sense repository used WSD systems. Also note semantic classes used
work also linked WordNet.
WordNet (Fellbaum, 1998) online lexical database English contains concepts
represented synsets, sets synonyms content words (nouns, verbs, adjectives
adverbs). One synset groups together several senses different words synonyms.
WordNet different types lexical semantic relations interlink different synsets, creating
way large structured lexical semantic network. important relation
encoded WordNet subclass relation (for nouns called hyponymy relation verbs
troponymy relation). Table 2 shows basic figures different WordNet versions including
total number words, polysemous words, synsets, senses (all possible senses words)
average polysemy.
Version
WN 1.6
WN 1.7
WN 1.7.1
WN 2.0
WN 2.1
WN 3.0

Words
121,962
144,684
146,350
152,059
155,327
155,287

Polysemous
23,255
24,735
25,944
26,275
27,006
26,896

Synsets
99,642
109,377
111,223
115,424
117,597
120,982

Senses
173,941
192,460
195,817
203,145
207,016
206,941

Avg. Polysemy
2.91
2.93
2.86
2.94
2.89
2.89

Table 2: Statistics WordNet versions.

3.1 SuperSenses
SuperSenses name WordNet Lexicographer Files within framework WSD3 .
detail, WordNet synsets organized forty five SuperSenses, based syntactic categories
(nouns, verbs, adjectives adverbs) logical groupings PERSON, PHENOMENON,
FEELING , LOCATION , etc. 26 basic categories nouns, 15 verbs, 3 adjectives
1 adverbs. cases, different senses word grouped high level
SuperSense, reducing polysemy word. often case similar
senses word. classes adjectives adverbs, SuperSense taggers
usually developed nouns verbs. (Tsvetkov, Schneider, Hovy, Bhatia, Faruqui, &
Dyer, 2014) presents interesting study tagging adjectives SuperSenses acquired
GermaNet (Hamp, Feldweg, et al., 1997).
3.2 WordNet Domains
WordNet Domains4 (WND) (Magnini & Cavaglia, 2000; L. Bentivogli & Pianta, 2004) hierarchy 165 domains used label semi-automatically WordNet synsets.
set labels organized taxonomy following Dewey Decimal Classification System5 .
3. information SuperSenses found http://wordnet.princeton.edu/wordnet/
man/lexnames.5WN.html.
4. http://wndomains.itc.it
5. http://www.oclc.org/dewey

88

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

building WND, many labels assigned high levels WordNet hierarchy
automatically inherited across hypernym troponym hierarchy. Thus, semi-automatic
method6 used develop resource free errors inconsistencies (Castillo, Real, &
Rigau, 2004; Gonzalez, Rigau, & Castillo, 2012).
Information brought domain labels complementary already WordNet. WND
present characteristics interesting WSD. First all, domain label may contain
senses different WordNet subhierarchies (derived different SuperSenses). instance,
domain RELIGION contains senses priest, deriving NOUN . PERSON church,
deriving NOUN . ARTIFACT. Second, domain label may also include synsets different
syntactic categories. instance, domain RELIGION also contains verb pray adjective
holy.
Furthermore, single WND label subsume different senses word, reducing
way polysemy. instance, first third senses church WordNet 1.6
domain label RELIGION.
3.3 SUMO Concepts
SUMO7 (Niles & Pease, 2001) created part IEEE Standard Upper Ontology Working
Group. goal develop standard upper ontology promote data interoperability, information search retrieval, automated inference, natural language processing. UMO consists
set concepts, relations, axioms formalize upper ontology. experiments,
used complete WordNet 1.6 mapping 1,019 UMO labels (Niles & Pease, 2003).
case, three noun senses church WordNet 1.6 classified R ELIGIOUS RGANIZATION,
B UILDING R ELIGIOUS C EREMONY according SUMO ontology.
3.4 Example Semantic Classes
example, table 3 presents three senses glosses word church WordNet 1.6.
Sense
1

2
3

WordNet 1.6
gloss
1
Christian churchn group Christians; group professing
Christian doctrine belief: church biblical term assembly
church2n church building1n
public (especially Christian) worship:
church empty
church service1n church3n
service conducted church: dont late
church
word senses
church1n
Christianity2n

Table 3: Glosses examples senses churchn
Table 4 show classes assigned sense according semantic resources introduced previously. instance, considering WordNet Domains, observed senses
number 1 (group Christians) 3 (service conducted church) belong domain
6. based several cycles manual checking automatically labeled data.
7. http://www.ontologyportal.org

89

fiI ZQUIERDO , U AREZ & R IGAU

RELIGION . contrary, SuperSenses SUMO represent three senses church using
different semantic classes. Also note resulting assignment semantic classes identifies
word sense individually.

Sense
1
2
3

SuperSense
NOUN . GROUP
NOUN . ARTIFACT
NOUN . ACT

Semantic Class
WND
SUMO
R ELIGION R ELIGIOUS RGANIZATION
B UILDINGS
B UILDING
R ELIGION
R ELIGIOUS C EREMONY

Table 4: Semantic Classes noun churchn
3.5 Levels Abstraction
Basic Level Concepts (Rosch, 1977) (hereinafter BLC) result compromise two
conflicting principles characterization (general vs. specific):
Represent many concepts possible
Represent many features possible
result conflicting characterization, BLC typically occur middle levels
semantic hierarchies.
notion Base Concepts (hereinafter BC) introduced EuroWordNet (Vossen, 1998).
BC supposed important concepts several language specific wordnets.
importance measured terms two main criteria:
high position semantic hierarchy
many relations concepts
EuroWordNet set 1,024 concepts selected called Common Base Concepts.
Common BC concepts act BC least two languages. local wordnets English,
Dutch Spanish used select set Common BC. later initiatives, similar sets
derived harmonize construction multilingual wordnets.
Considering definitions, next section present method automatically generate
different sets Basic Level Concepts WordNet different levels abstraction.

4. Automatic Selection Basic Level Concepts
Several approaches developed trying alleviate fine granularity problem WordNet
senses obtaining word sense groupings (Hearst & Schutze, 1993; Peters et al., 1998; Mihalcea
& Moldovan, 2001; Agirre & de Lacalle, 2003; Navigli, 2006; Snow et al., 2007; Bhagwani, Satapathy, & Karnick, 2013). cases approach consists grouping different senses
word, resulting decrease polysemy. Obviously, polysemy reduced
WSD task classification problem becomes easier, system using coarse senses
obtain better results systems using word senses. works used predefined sets
semantic classes, mainly SuperSenses (Segond et al., 1997; Ciaramita & Johnson, 2003; Curran,
90

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

2005; Villarejo et al., 2005; Ciaramita & Altun, 2006; Picca, Gliozzo, & Ciaramita, 2008; Paa &
Reichartz, 2009b; Tsvetkov et al., 2014).
section, describe simple method automatically create different sets Basic Level
Concepts WordNet. method exploits nominal verbal structure WordNet.
basic idea synsets WordNet high number relations important, could
candidates BLC. capture relevance synset WordNet considered two
options:
1. All: total number relations encoded WordNet synset
2. Hypo: total number hyponymy relations synset
method follows bottomup approach exploiting hypernymy chains WordNet.
synset, process starts visiting synsets hyperonymy chain selecting (and
stopping walk synset) BLC ancestor first local maximum considering
total number relations (either Hypo)8 . synsets one hyperonym,
method chooses one higher number relations continue process. process
ends preliminary set candidate synsets selected potential BLC.
Additionally, synset selected potential BLC candidate must subsume (or represent)
least certain number descendant synsets. Thus, minimum number synsets BLC must
subsume another parameter algorithm, represented symbol . Candidate
BLCs reach threshold discarded, subsumed synsets reassigned
BLC candidate appearing higher levels abstraction.
Algorithm 1 presents pseudocode algorithm. parameters algorithm are:
WordNet resource, type relations considered (All Hypo), minimum number
concepts must subsumed BLC (). algorithm two phases. first
one selects candidate BLC, following bottomup approach. second phase discards
candidate BLC satisfy threshold.
Figure 1 shows schema illustrate selection process. node represents synset,
edges represent hyperonymy relations (for instance, hyperonym D,
hyperonym F). number synset indicates number hyponymy relations.
schema illustrates selection process BLC candidates synset J using criterion Hypo.
process starts checking hyperonym J, F. F two hyperonyms, B D.
next synset visited hyperonymy chain J since higher number hyponymy
relations (three). algorithm compares number relations hyperonym synset (D
three relations), previous synset (F two). number increasing
process continues. Now, next node visit A. number relations two
number three, process stops synset selected BLC candidate J D.
Table 5 shows real example selection process noun church WordNet 1.6.
hyperonym chain number relations encoded WordNet (All criterion) shown
synset. local maximum chain marked bold.
8. algorithm starts checking first hyperonym synset, synset itself.

91

fiI ZQUIERDO , U AREZ & R IGAU

Figure 1: Example BLC selection
#rel.
18
19
37
10
12
5
#rel.
14
29
39
63
79
11
19
#rel.
20
69
5
11
7
1

synset
group 1,grouping 1
social group 1
organisation 2,organization 1
establishment 2,institution 1
faith 3,religion 2
Christianity 2,church 1,Christian church 1
synset
entity 1,something 1
object 1,physical object 1
artifact 1,artefact 1
construction 3,structure 1
building 1,edifice 1
place worship 1, ...
church 2,church building 1
synset
act 2,human action 1,human activity 1
activity 1
ceremony 3
religious ceremony 1,religious ritual 1
service 3,religious service 1,divine service 1
church 3,church service 1

Table 5: BLC selection noun church WordNet 1.6

92

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Algorithm 1 BLC Extraction
Require: WordNet (WN) , typeOfRelation (T), threshold ()
BlcCandidates =
{synset W N }
cur :=
{Obtaining hypernym chains current synset cur}
H := Hypernyms(W N, cur)
new := SynsetW ithM oreRelations(W N, H, )
{Iterating number relations increased}
N umOf Rels(W N, T, cur) < N umOf Rels(W N, T, new)
cur := new
H := Hypernyms(W N, cur)
new := SynsetW ithM oreRelations(W N, H, )
end while{Store cur candidate BLC}
BlcCandidates := BlcCandidates {cur}
end
{Filtering BLC candidates}
BlcF inal =
{blc BlcCandidates}
< N umberOf Descendants(W N, blc)
BlcF inal := BlcF inal {blc}
end
end
return BlcF inal

Figure 2: Example BLC selection sense 2 church
93

fiI ZQUIERDO , U AREZ & R IGAU

figure 2 see diagram showing partial view selection process candidate
BLC sense number 2 noun church. synset dotted synset
processed (church2n ). synsets bold visited algorithm, one
gray (building1n ) one selected BLC church2n . process stops checking synset
structure1n number relations 63, lower number relations
previous synset (79 relations edifice1n ).
Obviously, combining different values threshold (for example 0, 10, 20 50)
criterion considered algorithm (All Hypo), process ends different sets BLC
extracted automatically WordNet version.
Furthermore, instead number relations consider frequency synsets
corpus measure importance. Synset frequency calculated sum
frequencies word senses contained synset, obtained SemCor (Miller,
Leacock, Tengi, & Bunker, 1993), WordNet.
sum up, algorithm two main parameters, parameter, representing minimum number synsets BLC must represent, criterion used characterizing
relevance synsets. values parameters be:
parameter: integer value greater equal 0
Synset relevance parameter: value considered measure importance synset.
Four possibilities:
Number relations synset
All: relations encoded synset
Hypo: hyponymy relations
Frequency synset
FreqWN: frequency obtained using WordNet
FreqSC: frequency obtained using SemCor
implementation algorithm different sets BLC used paper several
WordNet versions freely available9 .
4.1 Analysis Basic Level Concepts
selected WordNet 1.6 generate several sets BLC, combining four types synset
relevance criteria values 0, 10, 20 50 . values selected since
represent different levels abstraction, ranging = 0 (no filtering) = 50 (each BLC
must subsume least 50 synsets). Table 6 shows, combinations synset relevance
parameters, number concepts set BLC contains, average depth
WordNet hierarchy group. gray highlight two sets BLC (BLC-20 BLC-50
relations parameter) use experiments described paper.
expected, increasing threshold direct effect number BLC
average depth WordNet hierarchy. particular, values decreased, indicating
threshold increased, concepts selected abstract general. instance,
9. http://adimen.si.ehu.es/web/BLC

94

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Threshold

Synset Relevance

0

10

20

50


Hypo
FreqSC
FreqWN

Hypo
FreqSC
FreqWN

Hypo
FreqSC
FreqWN

Hypo
FreqSC
FreqWN

# BLC
Nouns Verbs
3,094 1,256
2,490 1,041
34,865 3,070
34,183 2,615
971
719
993
718
690
731
691
738
558
673
558
672
339
659
340
667
253
633
248
633
94
630
99
631

Depth
Nouns Verbs
7.09
3.32
7.09
3.31
7.44
3.41
7.44
3.30
6.20
1.39
6.23
1.36
5.74
1.38
5.77
1.40
5.81
1.25
5.80
1.21
5.43
1.22
5.47
1.23
5.21
1.13
5.21
1.10
4.35
1.12
4.41
1.12

Table 6: Automatic Base Level Concepts WN1.6

using (All) nominal part WordNet, number concepts selected range 3,094
filtering ( = 0) 253 ( = 50). However, average, depth reduction acute since
varies 7.09 5.21. fact shows robustness method selecting synsets
intermediate level abstraction.
Also expected, verbal part WordNet behave differently. case, since verbal
hierarchies less deep, average depth synsets selected ranges 3.32 1.13
using relations, 3.31 1.10 using Hypo relations.
general, using frequency criteria, observe similar behavior
using relation criteria. However, effect threshold dramatic, specially
nouns. Again, expected, verbs behave differently nouns. number BLC (for
SemCor WordNet frequencies) reaches plateau around 600. fact, number
close verbal top beginners WordNet.
Summing up, devised simple automatic procedure deriving different sets BLC
representing different level abstraction whole set nominal verbal synsets WordNet. following section show explain supervised framework developed WSD
order exploit semantic classes described section previous one.

5. Supervised Class-Based WSD
follow supervised machine learning approach develop set semantic class based WSD
classifiers. systems use implementation Support Vector Machine algorithm train
classifiers, one per semantic class, semantic annotated corpora acquiring positive
negative examples class. classifiers built basis set features defined
representing examples. class-based, training data must collected treated
pretty different way usual word-based approach.
95

fiI ZQUIERDO , U AREZ & R IGAU

First, word-based class-based approaches selects training examples differently.
word-based approach, instances word used training examples. Figure
3 shows distribution training examples used generate word sense classifier noun
house. Following binary definition SVM, one classifier generated word sense.
classifiers, occurrences word sense associated classifier
used positive examples, rest word sense occurrences used negative examples.
Classifier HOUSE

Classifier
sense#1

... house.n#1 ...

Classifier
sense#2

... house.n#2...

... house.n#1 ...

Classifier
sense#3

... house.n#2 ...

... house.n#3 ...

Figure 3: Distribution examples using word-based approach
class-based approach, use examples words belong
particular semantic class. Figure 4 shows distribution examples class-based approach.
case, one classifier created semantic class. occurrences words belonging
semantic class associated classifier used positive examples, rest
occurrences word senses associated different semantic class selected negative
examples.
Obviously, class-based approach number examples training increased. Table
7 shows example sense church2n . Following word-based approach 58 examples
found Semcor church2n . Conversely, 371 positive training examples used
building classifier semantic class building, edifice.
think approach several advantages. First, semantic classes reduce average
polysemy degree words (some word senses might grouped together within semantic
class). Moreover, acquisition bottleneck problem supervised machine learning algorithms
attenuated increase number training examples. However, mixing
one classifier examples different words. instance, building class
grouping together examples hotel, hospital church, could introduce noise
learning process grouping unrelated word senses.
5.1 Learning Algorithm: SVM
Support Vector Machines (SVM) proven robust competitive many NLP
tasks, WSD particular (Marquez et al., 2006). experiments, used SVM-Light
96

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Classifier ANIMAL

Classifier BUILDING

...hospital..
(BUILDING)

...house..
(BUILDING)

...dog...
(ANIMAL)

...cat...
(ANIMAL)

...star..
(PERSON)

Figure 4: Distribution examples using class-based approach

church2n

Classifier
(word-based approach)

building, edifice
(class approach)

Examples
church2n
church2n
building1n
hotel1n
hospital1n
barn1n
.......

# positive examples
58
58
48
39
20
17
......
371 examples

Table 7: Number examples Semcor: word vs. class-based approaches

97

fiI ZQUIERDO , U AREZ & R IGAU

implementation (Joachims, 1998). SVM used induce hyperplane separates positive
negative examples maximum margin. means hyperplane located
intermediate position positive negative examples, trying keep maximum distance
closest positive example, closest negative example. cases, possible
get hyperplane divides space linearly, better allow errors obtain
efficient hyperplane. known soft-margin SVM, requires estimation parameter
(C), represents trade-off allowed training errors margin. set
value 0.01, demonstrated good value SVM WSD tasks.
classifying example, obtain value output function SVM classifier
corresponding semantic class word example, system simply selects class
greatest value.
5.2 Corpora
Three semantic annotated corpora used training testing. Semcor training,
SensEval-2 SensEval-3 English all-words tasks, testing.
SemCor (Miller et al., 1993) subset Brown Corpus plus novel Red Badge
Courage, developed group created WordNet. contains 253
texts around 700,000 running words, 200,000 also lemmatized sensetagged according Princeton WordNet 1.6. sense annotations SemCor also
automatically ported WordNet versions10 .
SensEval-211 English all-words corpus (hereinafter SE2) (Palmer, Fellbaum, Cotton, Delfs, &
Dang, 2001) consists 5,000 words text three Wall Street Journal (WSJ) articles representing different domains Penn TreeBank II. sense inventory used tagging
WordNet 1.7.
SensEval-312 English all-words corpus (hereinafter SE3) (Snyder & Palmer, 2004), made
5,000 words, extracted two WSJ articles one excerpt Brown Corpus. Sense
repository WordNet 1.7.1 used tag 2,041 words proper senses.
also considered alternative evaluation datasets. instance, SemEval-2007 coarse
grained task corpus13 . However, dataset discarded corpus annotated
particular set word sense clusters. Additionally, provide clear simple way
compare orthogonal sets clusterings. Although recent SensEval/SemEval
tasks WSD, think purpose evaluation (different level abstraction
WSD), SensEval-2 SensEval-3 still datasets best fit purposes. recent
SemEval competitions designed address specific topics, multilinguality joint
WSD Named Entity Recognition. However, also make additional experiments
domain adaptation dataset provided SemEval-10 task 17 All-words Word Sense
Disambiguation Specific Domain (WSD-domain)14 (Agirre, Lopez de Lacalle, Fellbaum,
Hsieh, Tesconi, Monachini, Vossen, & Segers, 2010).
10.
11.
12.
13.
14.

http://web.eecs.umich.edu/mihalcea/downloads.html#semcor
http://www.sle.sharp.co.uk/senseval2
http://www.senseval.org/senseval3
Indeed participated task preliminary version system
http://semeval2.fbk.eu/semeval2.php?location=tasks#T25

98

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

5.3 Feature Types
Following previous contributions supervised WSD, selected set basic features
represent training testing examples. also include additional features based semantic
classes.
Basic features
Word-forms lemmas window 10 words around target word.
PoS, concatenation preceding/following three five PoS tags.
Bigrams trigrams formed lemmas word-forms window 5 words
around target word; use tokens regardless PoS build bi/trigrams.
replace target word character X features increase generalization.
Semantic features
frequent semantic class target word, calculated SemCor.
Monosemous semantic class monosemous words window size five words
around target word.
Basic features widely used literature, work presented Yarowsky (1994).
features pieces information occur context target word: local features
including bigrams trigrams (including target word) lemmas, word-forms partof
speech labels (PoS). addition, wordforms lemmas larger window around target
word considered features representing topic discourse.
set features extended semantic information. Several types semantic classes
considered create features. particular, two different sets BLC (BLC20
BLC5015 ), SuperSenses, WordNet Domains (WND) SUMO.
order increase generalization capabilities class-based classifiers filter
irrelevant features. measure relevance feature16 f class c terms frequency
f. class c, feature f class, calculate frequency feature
within class (the number times occurs examples class), also obtain
total frequency feature classes. get relative frequency dividing
values (classFreq / totalFreq) result lower certain threshold t, feature
removed feature list class c17 . way, make sure features selected
class frequently related class others. set threshold
0.25, obtained empirically preliminary versions classifiers applying crossvalidation setting SemCor.
15. selected set since represent different levels abstraction. said section 4, 20 50 refer
threshold minimum number synsets possible BLC must subsume considered proper BLC.
sets BLC built using criterion.
16. is, value feature, example feature type word-form, feature type
houses.
17. Depending experiment, around 30% original features removed filter.

99

fiI ZQUIERDO , U AREZ & R IGAU

6. Semantic ClassBased WSD Experiments
section present performance semantic class-based WSD system
words WSD SensEval-2 (SE2) SensEval3 (SE3) datasets. want analyze behavior
class-based WSD system working different levels abstraction. said
before, level abstraction defined semantic class used build classifiers.
experiment defined two different parameters one involving particular set
semantic classes.
1. Target class: semantic classes used train classifiers (determining abstraction
level system). case, tested: word-sense18 , BLC20, BLC50, WordNet Domains (WND), SUMO SuperSenses (SS).
2. Semantic features class: semantic classes used building semantic features.
case, tested: BLC20, BLC50, WND, SUMO SuperSenses (SS).
target class type classes classifier assigns given ambiguous word.
instance, target class traditional word expert classifiers word senses. Semantic
feature class one used building semantic features, independent target
class. instance, use WordNet Domains extract monosemous words context
target word use WND labels words semantic features building
classifier.
Combining different semantic classes target features, generated set experiments described next sections. way, evaluate independently impact
selecting one semantic class another target class semantic feature class.
Test
SE2
SE3

PoS
N
V
N
V

Sense
4.02
9.82
4.93
10.95

BLC20
3.45
7.11
4.08
8.64

BLC50
3.34
6.94
3.92
8.46

SUMO
3.33
5.94
3.94
7.60

SS
2.73
4.06
3.06
4.08

WND
2.66
2.69
3.05
2.49

Table 8: Average polysemy SE2 SE3
Table 8 shows average polysemy (AP) measured SE2 SE3 respect different semantic classes used evaluation target classes. expected, every corpus behaves
differently average polysemy verbs higher nouns. Also could assume
advance, relevant reductions polysemy degree obtained increasing level
abstraction. fact acute also verbs. Note large reduction polysemy verbs
using SuperSenses also WND. Also note priori SE3 seems difficult
disambiguate SE2, independently abstraction level.
6.1 Baselines
baselines evaluations define frequent classes (MFC) word calculated
SemCor. Ties classes specific word solved obtaining global frequency
18. included word-based evaluation comparison purposes since current system designed
class-based evaluation.

100

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

SemCor tied classes, selecting frequent class whole training
corpus. Semcor occurrences particular word (that is, able calculate
frequent class word), compute global frequency possible
semantic classes (obtained WordNet) SemCor, select frequent one. Table
9 shows baseline semantic class testing corpora.
Class
Sense
BLC20
BLC50
SUMO
SuperSense
WND

Pos
N
V
N
V
N
V
N
V
N
V
N
V

SE2
MFC AP
70.02 4.02
44.75 9.82
75.71 3.45
55.13 7.11
76.65 3.34
54.93 6.94
76.09 3.33
60.35 5.94
80.41 2.73
68.47 4.06
86.11 2.66
90.33 2.69

SE3
MFC
AP
72.30
4.93
52.88 10.95
76.29
4.08
58.82
8.64
76.64
3.92
60.05
8.46
79.55
3.94
64.71
7.60
81.50
3.06
79.07
4.08
83.82
3.05
92.20
2.49

Table 9: Frequent Class baselines average polysemy (AP) SE2 SE3
expected, performances MFC baselines high. particular, corresponding nouns (ranging 70% 80%). nominal baselines seem perform similarly
SE2 SE3, verbal baselines appear consistently much lower SE2 SE3.
SE2, verbal baselines range 44% 68% SE3 verbal baselines range 52%
79%. results WND high due low polysemy degree nouns verbs.
Obviously, increasing level abstraction (from senses WND) results also increase.
6.2 Results Basic System
section present performance supervised semantic classbased WSD system.
Table 10 shows results system trained varying target classes using
basic feature set. values correspond F1 measures (harmonic mean recall
precision) training systems SemCor testing SE2 SE3 test sets. results
improve baselines shown italics. Additionally, results showing statistically
significant positive difference compared corresponding baseline using McNemars
test marked bold.
Interestingly, basic system word-sense level outperforms baselines SE2
SE3 nouns verbs. addition, systems obtain cases significantly better
results verbs. Also interesting verbs word-sense level baselines results
different, class-level differences datasets much smaller.
expected, results systems increase augmenting level abstraction (from
senses WND), cases, baseline results reached outperformed. even
relevant consider baseline results already quite high. However, high
level abstraction (SuperSenses WND) basic systems seem unable outperform
baselines.
101

fiI ZQUIERDO , U AREZ & R IGAU

Class
Sense
BLC20
BLC50
SUMO
SuperSense
WND

Pos
N
V
N
V
N
V
N
V
N
V
N
V

SE2
71.20
45.53
75.52
57.06
74.57
58.03
77.60
62.09
79.94
71.95
80.81
90.14

SE3
73.15
57.02
73.82
61.10
75.84
61.97
76.74
66.21
79.48
78.39
77.64
88.92

Table 10: Results basic system trained SemCor basic set features evaluated
SE2 SE3

general, results obtained BLC20 different BLC50. instance,
consider number classes within BLC20 (558 classes), BLC50 (253 classes) SuperSense (24 classes), BLC classifiers obtain high performance rates maintaining much higher
expressive power SuperSenses (they able classify among much larger number classes).
fact, using SuperSenses (40 classes nouns verbs) obtain accurate semantic tagger performance close 80%. Even interesting, could use BLC20 tagging
nouns (558 semantic classes F1 around 75%) SuperSenses verbs (14 semantic classes
F1 around 75%).
6.3 Results Exploiting Semantic Features
One main goals prove simple semantic features added training process
capable producing significant improvements basic systems. results experiments considering also different types semantic features presented Tables 11 12,
respectively nouns verbs.
tables, column labeled Class refers called target class,
column labeled SF indicates type semantic features included represent examples
within machine learning approach.
Again, values tables correspond F1 measures (harmonic mean recall
precision) training systems SemCor testing SE2 SE3 test sets. results
improving baselines appear italics. Additionally, results showing statistically significant positive difference compared corresponding baseline using McNemars test
marked bold.
Regarding nouns (see Table 11), different behavior observed SE2 SE3. Adding
semantic features mainly improves results SE2. SE3 none systems present
significant improvement baselines, SE2 improvement obtained using
several types semantic features (in particular, using WND features SE2). use
semantic class-based features seems improve systems using target classes intermediate
levels abstraction (specially BLC20 BLC50). Interestingly, SE3 BLC20 BLC50
102

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Class

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

Sense

BLC20

BLC50

SE2
70.02
71.20
71.79
71.69
71.59
71.10
71.20
75.75
75.52
77.69
77.79
77.60
75.14
77.88
76.65
74.57
78.45
76.65
79.58
75.52
78.92

SE3
72.30
73.15
73.15
73.04
73.15
72.70
73.15
76.29
73.82
76.52
75.73
73.71
73.82
74.24
76.74
75.84
76.85
76.74
75.51
74.61
74.83

Class

SUMO

SS

WND

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
76.09
77.60
75.52
75.52
77.88
77.50
77.88
80.41
79.94
81.07
80.22
80.51
80.32
82.47
86.11
80.81
81.85
82.33
83.55
83.08
86.01

SE3
79.55
76.74
76.74
77.19
78.76
76.97
77.42
81.50
79.48
81.39
81.73
81.05
76.46
79.82
83.82
77.64
80.79
80.11
81.24
78.31
83.71

Table 11: Results nouns using extended system
seem provide improvements baselines target classes (for instance,
BLC20, BLC50 SS), although significant.
Regarding verbs (see Table 12), also different behavior observed SE2 SE3.
case, observe almost opposite effect nouns. SE3 semantic
class features improve results obtained baselines. SE2 systems
present significant improvement baselines, SE3 improvement obtained
using several types semantic features. However, case also obtain significantly better
results several semantic features SE2. use semantic class-based features seems
benefit lower levels abstraction (specially word-sense, BLC20, BLC50 also SUMO).
general, results show using semantic features addition basic features helps
reach better performance class-based WSD systems. Additionally, also seems using
semantic features able obtain competitive classifiers sense level.
6.4 Learning Curves
also investigate behavior class-based WSD system respect number training
examples. Although experiments carried nouns verbs,
include results nouns since cases, trend similar.
experiment, Semcor files randomly selected added training
corpus order generate subsets 5%, 10%, 15%, etc. training corpus19 . Then, train
19. portion contains also files previous portion. example, files 25% portion also
contained 30% portion.

103

fiI ZQUIERDO , U AREZ & R IGAU

Class

Sense

BLC20

BLC50

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
44.75
45.53
45.14
45.53
45.73
45.34
45.53
55.13
57.06
56.87
55.90
57.06
56.29
58.61
54.93
58.03
57.45
56.67
57.06
57.45
59.77

SE3
52.88
57.02
56.61
56.47
57.02
56.75
56.75
58.82
61.10
59.92
60.60
61.15
61.29
60.88
60.05
61.97
61.29
61.01
61.83
61.83
62.38

Class

SUMO

SS

WND

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
60.35
62.09
61.12
62.09
60.74
59.96
61.51
68.47
71.95
69.25
69.25
70.21
69.25
71.76
90.33
90.14
90.14
90.14
90.52
89.75
90.52

SE3
64.71
66.21
66.07
66.48
64.98
64.71
66.35
79.07
78.39
77.70
77.70
77.70
77.84
79.75
92.20
88.92
90.42
90.15
89.88
88.78
92.20

Table 12: Results verbs using extended system
system training portions test system SE2 SE3. Finally, also
compare resulting system baseline computed training portion.
Figures 5 6 present learning curves SE2 SE3, respectively. case,
selected BLC20 class-based WSD system using WordNet Domains semantic features20 .
Surprisingly, SE2 system improves F1 measure around 2% increasing
training corpus 25% 100% SemCor. SE3, system improves F1
measure around 3% increasing training corpus 30% 100% SemCor. is,
knowledge required class-based WSD system seems already present
small part SemCor.
Figures 7 8 present learning curves SE2 SE3, respectively, class-based
WSD system based SuperSenses using semantic features built WordNet Domains.
SE2 system improves F1 measure around 2% increasing training corpus
25% 100% SemCor. SE3, system improves F1 measure around 2%
increasing training corpus 30% 100% SemCor. is, 25%
whole corpus, class-based WSD system reaches F1 close performance using corpus.
SE2 ans SE3, using BLC20 (Figures 5 6) SuperSenses (Figures 7 8)
semantic classes WSD, behavior system similar MFC baseline.
interesting since MFC obtains high results due way defined: MFC
total corpus assigned occurrences word training corpus. Without
definition, would large number words test set occurrences using
20. shown previous experiments, combination obtains good performance.

104

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

80

System SV2
MFC SV2

78

76

74

72
F1
70

68

66

64

62
5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

90

95 100

% corpus

Figure 5: Learning curve BLC20 classifier SE2

78

System SV3
MFC SV3

76

74

72

F1

70

68

66

64

62
5

10

15

20

25

30

35

40

45

50 55
% corpus

60

65

70

75

80

85

90

Figure 6: Learning curve BLC20 classifier SE3

105

95 100

fiI ZQUIERDO , U AREZ & R IGAU

84

System SV2
MFC SV2

82

80

78

F1

76

74

72

70

68
5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

90

95 100

% corpus

Figure 7: Learning curve SuperSense classifier SE2

82

System SV3
MFC SV3

80

78

F1

76

74

72

70
5

10

15

20

25

30

35

40

45

50 55
% corpus

60

65

70

75

80

85

90

95 100

Figure 8: Learning curve SuperSense classifier SE3

106

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

small training portions. cases, recall baselines (and turn F1) would much
lower.
evaluation seems indicate class-based approach WSD reduces considerably
required amount training examples.

7. Comparison SensEval Systems: Sense Level
main goal experiments included section verify whether abstraction level
class-based systems maintains discriminative power evaluated sense level. Additionally, compare results results top participant systems SE2 SE3
provided best senselevel outputs. Thus, class-based systems adapted following simple protocol. output based semantic classes converted sense identifiers:
instead semantic class produced systems particular instance, select first
sense word according WordNet sense ranking belonging predicted semantic class.
So, first obtain semantic class means classifiers, obtain restricted set
senses word match semantic class obtained, choose frequent
sense restricted subset.
results first experiment SE2 data shown Table 13. systems
prefix SVM- suffix denotes type semantic class used generate classifier21 .
cases experiments, WND selected target semantic class generate
semantic features. Two baselines marked Italics also included. first sense WordNet (base-WordNet) frequent sense SemCor (base-SemCor). fact, developers
WordNet ranked word senses using SemCor sense-annotated corpora. Thus,
frequencies ranks appearing SemCor WordNet similar, equal. also
include results system working word level (SVM-sense).
cases, nouns verbs, systems outperform frequent baselines.
frequent sense word, according WordNet sense ranking competitive
WSD tasks, extremely hard improve upon even slightly (McCarthy, Koeling, Weeds,
& Carroll, 2004). expected, behavior different semantic features produces slightly
different results. However, independently semantic features used, SE2 sense level,
class-based systems rank third position.
Table 14 shows experiment using SE3 dataset. case, class-based systems
clearly outperform baselines, achieving best results nouns second place verbs.
Interestingly, nouns, best system SE3 achieve SemCor baseline. Also recall
SE3 seems difficult SE2.
worth mention class-based systems use features nouns
verbs. instance, take profit complex feature sets encoding syntactic information
seems important verbs.
experiments show class-based classifiers seem quite competitive evaluated word sense level. perform frequent sense according WordNet
SemCor, achieve higher position nouns second verbs SE3, third
position nouns verbs SE2. Obviously, indicates class-based WSD maintains
high discriminative power word sense level.
21. instance, SVM-BLC20 stands experiment creates classifier considering BLC20 semantic classes.

107

fiI ZQUIERDO , U AREZ & R IGAU

Class Sense SE2
Nouns
Verbs
System
F1
System
SMUam
73.80 SMUaw
AVe-Antwerp
74.40 AVe-antwerp
SVM-semBLC20 71.80 SVM-semSUMO
SVM-semBLC50 71.70 SVM-sense
SVM-semSUMO 71.60 SVM-semWND
SVM-semWND
71.20 SVM-semBLC50
SVM-sense
71.20 SVM-semSS
SVM-semSS
71.10 SVM-semBLC20
base-WordNet
70.10 LIA-Sinequa
base-SemCor
70.00 base-SemCor
LIA-Sinequa
70.00 base-WordNet

F1
52.70
47.90
45.70
45.53
45.50
45.50
45.30
45.10
44.80
44.80
43.80

Table 13: Class Sense results SE2. Class word sense transformation.

Class Sense SE3
Nouns
System
SVM-semWND
SVM-semBLC20
SVM-semSUMO
SVM.sense
SVM-semBLC50
SVM-semSS
base-SemCor
GAMBL-AW
base-WordNet
kuaw
UNTaw
Meaning-allwords
LCCaw

F1
73.20
73.20
73.20
73.15
73.00
72.70
72.30
70.80
70.70
70.60
69.60
69.40
69.30

Verbs
System
GAMBL-AW
SVM-semSUMO
SVM-semWND
SVM-semSS
SVM-sense
SVM-semBLC20
SVM-semBLC50
UNTaw
Meaning-allwords
kuaw
R2D2
base-SemCor
base-WordNet

F1
59.30
57.00
56.80
56.80
56.75
56.60
56.50
56.40
55.20
54.50
54.40
52.90
52.80

Table 14: Class Sense results SE3. Class word sense transformation.

108

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

8. Comparison SensEval Systems: Class Level
experiments presented section explore performance word-based classifiers
participating SE2 SE3 evaluated class level. perform kind evaluation,
word sense output participant systems mapped corresponding semantic
classes. class-based systems modified. Obviously, expect different performances
systems depending semantic class level. Considering results presented tables
11 12, order perform comparison, selected experiments use WND
build semantic features22 . Thus, system results using different target semantic classes
represented SVM-semWND.
Table 15 presents ordered F1-measure results best performing systems SE2 data
evaluated different levels abstraction. previously, italics include
frequent senses according WordNet base-WordNet SemCor base-SemCor.
SE2, independently abstraction level PoS, system (SVM-semWND) scores
first positions ranking. one case system reaches best position, twice
second one. baselines outperformed experiments, except nouns using WND,
baseSemCor high.
Table 16 presents ordered F1-measure results best performing systems SE3 data
evaluated different levels abstraction. italics include frequent senses
according WordNet base-WordNet SemCor base-SemCor. systems represented
SVM-semWND.
SE3, see system performs better baselines cases, except
SemCorbased baseline nouns, obtains high result. particular, system
obtains good results verbs, reaching first second best positions cases,
outperforming baselines cases.
sum up, classbased approach outperforms SensEval participants (both SE2
SE3), sense level semantic class level. suggests good performance
semantic classifiers due polysemy reduction. Actually, confirms
classbased semantic classifiers learning semantic class training examples different
abstraction levels.

9. Domain Evaluation
section describe system SemEval-2 Allwords Word Sense Disambiguation
Specific Domain task (Izquierdo, Suarez, & Rigau, 2010). aim evaluation
show robust semantic class approach tested specific domain, different
domain training material.
Traditionally, SensEval competitions focused general domain texts. Thus, domain
specific texts present fresh challenges WSD. example, specific domains reduce possible meaning word given context. Moreover, distribution word senses data
examples changes compared general domains. problems affect supervised
knowledgebased systems. fact, supervised word-based WSD systems sensitive
corpora used training testing system (Escudero et al., 2000).
22. Remind semantic features frequent class target word, semantic class monosemous words context around target word.

109

fiI ZQUIERDO , U AREZ & R IGAU

Nouns

Verbs
F1
System
Sense BLC20
SMUaw
78.72 SMUaw
SVM-semWND 77.88 SVM-semWND
AVe-antwerp
76.71 LIA-Sinequa
base-SemCor
75.71 AVe-antwerp
base-WordNet
74.29 base-SemCor
LIA-Sinequa
73.39 base-WordNet
Sense BLC50
SMUaw
79.01 SMUaw
SVM-semWND 78.92 SVM-semWND
AVe-antwerp
77.57 LIA-Sinequa
base-SemCor
76.65 AVe-Antwerp
base-WordNet
75.24 base-SemCor
LIA-Sinequa
74.53 base-WordNet
Sense SUMO
SMUaw
79.30 SMUaw
SVM-semWND 77.88 LIA-Sinequa
base-SemCor
76.09 AVe-Antwerp
AVe-Antwerp
75.94 SVM-semWND
LIA-Sinequa
74.92 base-SemCor
base-WordNet
71.74 base-WordNet
Sense SuperSense
SVM-semWND 82.47 SMUaw
SMUaw
81.21 LIA-Sinequa
AVe-Antwerp
80.75 SVM-semWND
base-SemCor
80.41 AVe-Antwerp
LIA-Sinequa
79.58 base-WordNet
base-WordNet
78.16
base-SemCor
Sense WND
SMUaw
88.80 SMUaw
base-SemCor
86.11 SVM-semWND
SVM-semWND 86.01 base-SemCor
AVe-Antwerp
87.30 LIA-Sinequa
base-WordNet
85.82 base-WordNet
LIA-Sinequa
84.85 AVe-Antwerp
System

F1
61.22
58.61
57.42
57.28
55.13
54.16
61.61
59.77
57.81
57.67
54.93
54.55
68.22
64.79
62.56
61.51
61.33
60.35
73.47
72.74
71.76
69.31
69.05
68.47
91.16
90.52
90.33
89.82
89.75
89.74

Table 15: Results sense BLC20, BLC50, SUMO, SuperSense WND semantic classes
SE2

110

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Nouns

Verbs
F1
System
Sense BLC20
base-SemCor
76.29 GAMBL-AW
GAMBL-AW
74.77 SVM-semWND
kuaw
74.69 kuaw
LCCaw
74.44 R2D2
UNTaw
74.40 UNTaw
SVM-semWND
74.24 Meaning-allwords
base-WordNet
74.16
base-SemCor
Meaning-allwords 73.11 base-WordNet
Sense BLC50
base-SemCor
76.74 GAMBL-AW
GAMBL-AW
75.56 SVM-semWND
kuaw
75.25 kuaw
SVM-semWND
74.83 R2D2
LCCaw
74.78 UNTaw
UNTaw
74.73 Meaning-allwords
base-WordNet
74.49 base-SemCor
R2D2
73.93 base-WordNet
Sense SUMO
base-SemCor
79.55 GAMBL-AW
kuaw
78.18 SVM-semWND
LCCaw
77.54 UNTaw
SVM-semWND
77.42 kuaw
UNTaw
77.32 Meaning-allwords
GAMBL-AW
77.14 upv-eaw2
base-WordNet
76.97
base-SemCor
Meaning-allwords 76.75 base-WordNet
Sense SuperSense
base-SemCor
81.50 SVM-semWND
kuaw
79.89 GAMBL-AW
SVM-semWND
79.82 base-SemCor
UNTaw
79.71 base-WordNet
GAMBL-AW
79.62 Meaning-allwords
upv-eaw2
79.27 Meaning-simple
upv-eaw
78.42 kuaw
base-WordNet
78.25 upv-eaw2
Sense WND
base-SemCor
83.80 SVM-semWND
SVM-semWND
83.71 base-SemCor
UNTaw
83.62 UNTaw
kuaw
81.78 GAMBL-AW
GAMBL-AW
81.53 base-WordNet
base-WordNet
81.46
R2D2
LCCaw
80.64 Meaning-simple
Meaning-allwords 80.50 kuaw
System

F1
63.56
60.88
60.66
59.79
59.73
59.37
58.82
58.28
64.38
62.38
61.22
60.35
60.27
60.19
60.06
58.82
68.77
66.35
66.03
65.93
65.43
64.92
64.71
64.02
79.75
79.40
79.07
78.25
78.14
77.72
77.53
77.21
92.20
92.20
91.37
91.01
90.83
90.52
90.50
90.44

Table 16: Results sense BLC20, BLC50, SUMO, SuperSense WND semantic classes
SE3

111

fiI ZQUIERDO , U AREZ & R IGAU

Therefore, main challenge develop specific domain WSD systems adapt
general system particular domain. Following research line, task proposed within
SemEval2 competition: Allwords Word Sense Disambiguation Specific Domain (Agirre
et al., 2010). restricted domain selected task environmental domain. test
corpora consist three texts compiled European Center Nature Conservation23 (ECNC)
World Wildlife Forum24 (WWF). task proposed several languages: Chinese, Dutch,
English Italian, although participation limited English. detail,
total 1,032 noun tokens 366 verb tokens tagged. Moreover, set background
documents related environmental domain provided. texts sense tagged,
plain text, also provided ECNC WWF. could used
systems help adaptation specific domain. English, total 113
background documents, containing 2,737,202 words.
apply kind specific domain adaptation technique supervised classbased
system. order adapt supervised system environmental domain increase automatically training data new training examples domain. acquire examples,
use 113 background documents environmental domain provided organizers.
use TreeTagger (Schmid, 1994) preprocess documents, performing PoStagging lemmatization. Since background documents semantically annotated, supervised system
needs labeled data, selected monosemous instances occurring documents
according BLC20 semantic classes25 . Note approach exploited classbased WSD systems. way, obtained automatically large set examples annotated
BLC20. semantic class selected provided good results previous
experiments. order analyze approach system would work level
abstraction, performed evaluation posteriori using BLC50, WordNet Domains
SuperSenses besides BLC20, official participation SemEval-2. Nevertheless,
section focused BLC20.
Regarding BLC20, Table 17 presents total number training examples extracted SemCor (SC) background documents (BG). expected, method large number
monosemous examples obtained nouns verbs, although, verbs much less productive nouns. However, background examples correspond reduced set 7,646
monosemous words.
SC
BG
Total

Nouns
87,978
193,536
281,514

Verbs
48,267
10,821
59,088

N+V
136,245
204,357
340,602

Table 17: Number training examples BLC20
Table 18 lists ten frequent monosemous nouns verbs occurring background
documents. Remember examples monosemous according BLC20 semantic
classes.
23. http://www.ecnc.org
24. http://wwf.org
25. BLC20 (see section 4) stands Basic Level Concepts obtained relations criterion minimum threshold
subconcepts subsumed equal 20.

112

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

1
2
3
4
5
6
7
8
9
10

Nouns
Lemma
biodiversity
habitat
specie
climate
european
ecosystem
river
grassland
datum
directive

# ex.
7,476
7,206
7,067
3,539
2,818
2,669
2,420
2,303
2,276
2,197

Verbs
Lemma # ex.
monitor
788
achieve
784
target
484
select
345
enable
334
seem
287
pine
281
evaluate 246
explore
200
believe
172

Table 18: frequent monosemous words background documents
SC
BG
Total

Nouns
87,978
116,912
204,890

Verbs
48,267
7,019
55,286

N+V
136,245
123,931
260,176

Table 19: Number training examples word senses
approach applies semantic class architecture shown previous sections,
using examples extracted background documents. case, semantic class used
extract examples generate classifiers BLC2026 . select simple feature set widely
used many WSD systems. particular, use window five tokens around target word
extract word forms, lemmas; bigrams trigrams word forms lemmas; trigrams PoS
tags, also frequent BLC20 semantic class target word training corpus.
analyze contribution monosemous examples performance system three
experiments defined:
BLC20SC: training examples extracted SemCor
BLC20BG: monosemous examples extracted background data
BLC20SCBG: training examples extracted SemCor monosemous background data
first run (BLC20SC) aims show behavior supervised system trained general
corpus, tested specific domain. second one (BLC20BG) analyzes contribution
monosemous examples extracted background data. Finally, third run (BLC20SCBG) studies robustness approach combining training examples SemCor
automatic ones obtained background documents.
Table 20 summarizes ordered recall official results participants English
WSD domain specific task SemEval2. table, Type refers approach followed
corresponding system: Weakly Supervised (WS), Supervised (S) KB (Knowledge Based,
unsupervised). participate system using BLC20 semantic class (the BLC20
SC/BG/SCBG runs). wordbased classifiers (labeled SenseBG, Sense-SC SenseSCBG)
26. case use set BLCs WordNet3.0, also version WN one used
annotation.

113

fiI ZQUIERDO , U AREZ & R IGAU

included evaluation campaign. Finally, mentioned introduction,
also included performance ItMakesSense system, one best performing WSD systems, task comparison purposes (it row table called
ItMakesSense Italics).
Rank
1
2
3
4
5
6
7
8
9
10
11
...
25
...
32

System ID
CFILT2
CFILT1
IIITH1-d.1.ppr.05
IIITH2-d.2.ppr.05
BLC20SCBG
ItMakesSense
BLC20SC
Frequent Sense
CFILT3
Treematch
Treematch2
SenseSCBG
SenseSC
...
BLC20BG
...
Random baseline
SenseBG

Type
WS
WS
WS
WS



KB
KB
KB


...

...


P
0.570
0.554
0.534
0.522
0.513
0.510
0.505
0.505
0.512
0.506
0.504
0.498
0.498
...
0.380
...
0.232
0.045

R
0.555
0.540
0.528
0.516
0.513
0.510
0.505
0.505
0.495
0.493
0.491
0.484
0.484
...
0.380
...
0.232
0.001

Table 20: Precision Recall SemEval2 participants. ItMakesSense results included
comparison purpose
general, results reported SemEval task quite low. best system
achieved precision 0.570, frequent baseline reached precision 0.505.
fact shows domain adaptation WSD systems difficult task.
Analyzing results three runs SemEval, worst result obtained system
using monosemous background examples (BLC20BG). system ranks 23rd27
Precision Recall 0.380 (0.385 nouns 0.366 verbs). system using SemCor
(BLC20SC) ranks 6th Precision Recall 0.505 (0.527 nouns 0.443 verbs).
also performance first sense baseline. expected, best result three
runs obtained combining examples SemCor background (BLC20SCBG).
supervised system obtains 5th position Precision Recall 0.513 (0.534
nouns, 0.454 verbs) slightly baseline. Actually, version system
obtains slightly better results best performing supervised system (ItMakesSense). Also note
could include automatically monosemous examples background test thanks
class-based nature WSD system.
Moreover, system one completely supervised participating task. organizers calculated recall confidence interval 95% using bootstrap re-sampling procedure
(Noreen, 1989). method estimation might strict pairwise methods.
reveals differences four first systems system (BLC20SCBG)
27. table appears 25th position due included wordbased classifier results.

114

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

statistically significant. seen Figure 9, overlapping recall confidence interval four first systems system (ranking 5th), proves
differences statistically significant28 .

Figure 9: Recall confidence intervals.
Possibly, reason low performance BCL20BG system high correlation features target word semantic class. case, features correspond
monosemous word later evaluated polysemous words, kind features. However, also seems class-based systems robust enough incorporate large sets
monosemous examples domain text. fact, knowledge, first time
supervised WSD algorithm successfully adapted specific domain. Furthermore,
system trained SemCor also achieves good performance, reaching frequent
baseline, showing robustness class-based WSD approaches domain variations.
Comparing wordbased classifiers, seems BLC20 classes contribute two main
aspects. First, using set features, classbased classifiers obtain better results
wordbased ones. classifiers built BLC20 robust domain adaptable
wordbased approaches. Second, experiment uses examples extracted background data considering word senses (Sense-BG) obtain accuracy close zero,
experiment using BLC20 semantic classes (BLC20BG) reaches accuracy 0.380.
fact indicates BLCs useful extract good training examples unlabeled data.
mentioned previously, order obtain better insight, evaluation campaign performed
evaluation system using semantic classes represent different levels
abstractions: BLC50, WordNet Domains SuperSenses. Table 21 shows precision (P)
recall (R)29 evaluation considering different training datasets (SemCor only, Background
documents SemCor Background documents: SC, BG SC+BG respectively)
different semantic classes.
seen Table 21, BLC20 leads better performance using three different
corpora training (BG, SC SCBG). training monosemous examples extracted
background documents, BLC20 obtains best result, may indicate level
abstraction adequate other, including WND SS, sets much smaller
much lower polysemy. effect drawn results training
SemCor monosemous examples background (SCBG). best results
obtained BLC20, together SuperSenses two semantic classes seem
28. figure taken directly overview paper task.
29. figures obtained using official scorer script official gold key, without modification.

115

fiI ZQUIERDO , U AREZ & R IGAU

System ID
BLC20SCBG
ItMakesSense
BLC20SC
Frequent Sense
WNDSC
SenseSCBG
SenseSC
SS-SCBG
BLC50SCBG
BLC50SC
SSSC
WNSCBG
BLC20BG
WNDBG
SSBG
BLC50BG
Random baseline

Type
















-

P
0.513
0.510
0.505
0.505
0.495
0.498
0.498
0.484
0.481
0.481
0.472
0.471
0.380
0.362
0.348
0.277
0.232

R
0.513
0.510
0.505
0.505
0.495
0.484
0.484
0.484
0.481
0.481
0.457
0.471
0.380
0.362
0.348
0.277
0.232

Table 21: Results experiments according different semantic classes
benefit background monosemous examples. results seem confirm potential
capabilities BLC20 provide adequate level abstraction perform class-based WSD.
Finally, proved system performs level one state-of-the-art sys30
tem , ItMakesSense system (Zhong & Ng, 2010). Considering set features
system quite simple, apply machine learning optimization feature
engineering, results show use Semantic Classes provides robust behavior
specific domains, reaching state-of-the-art results.

10. Concluding Remarks
Word sense disambiguation difficult task empirically demonstrated SensEval/SemEval exercises. One reason difficulties could use inappropriate sets
word meanings. WordNet de-facto standard repository meanings, several attempts
made grouping senses order achieve higher levels accuracy. Moreover,
approach tries ease hard task creating large enough sets annotated data per domain
language train supervised systems. possible solution would use manual annotation semantic class labels instead fine-grained word senses (Schneider, Mohit, Oflazer, & Smith, 2012;
Schneider, Mohit, Dyer, Oflazer, & Smith, 2013).
Several attempts made obtain word sense groupings alleviate problem
fine granularity word senses, widely using WordNet senses. cases approach
consists grouping different senses word, resulting decrease polysemy,
reducing discriminative capacity. works use predefined sets semantic classes
integrated directly WSD system, mainly SuperSenses.
30. tested offline, ItMakesSense system participate task. downloaded last
version software http://www.comp.nus.edu.sg/nlp/software.html.

116

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

work describe simple method automatically select Basic Level Concepts
WordNet. Based simple structural properties WordNet, method automatically selects
different sets BLC representing different levels abstraction.
aim work explore several allwords WSD tasks performance different
levels abstraction provided Basic Level Concepts, WordNet Domains, SUMO SuperSense
labels. Furthermore, study empirically demonstrates that:
a) word sense groupings cluster senses coherent level abstraction order
perform supervised classbased WSD harming performance,
b) semantic classes successfully used semantic features boost performance
classifiers,
c) classbased approach WSD reduces dramatically required amount training examples obtain competitive classifiers,
d) classbased approach obtains competitive performances compared word-based systems,
e) classbased approach outperforms wordbased systems evaluated class level,
f) robustness class-based WSD system performing domain evaluation,
g) system reaches results comparable state-of-the-art system (ItMakesSense)
tested specific domain.
general, classbased disambiguation nouns verbs achieves better results
wordbased systems presented SensEval2 SensEval3. also showed classbased approach reduces considerably required amount training examples. order prove
type disambiguation possible accurate ranked class-based systems
together SensEval2 Senseval3 official results. order establish fair comparison
mapped necessary word senses semantic classes viceversa.
experiments designed use classbased classifiers perform wordsense
disambiguation. shown simple approach selecting first sense WordNet corresponds class selected classifiers performs well top systems
SensEval2 SensEval3.
Additional experiments carried compare wordbased systems perform
classbased disambiguation. case translated official system outputs corresponding semantic classes.
Different experiments performed using different levels abstraction, ranging
SuperSenses (a small set) SUMO (which 1,000 labels linked WordNet1.6 senses),
WordNet Domains (with 163 labels), Basic Level Concepts (with arbitrary number classes
depending abstraction level selected).
expected differences SensEval2 SensEval3 results, class
based systems outperform baselines nouns verbs. Specially nouns, class-based
systems outperforms SensEval2 SensEval3 systems. general, results obtained
SVM-semBLC20 different results SVM-semBLC50. Thus, select
117

fiI ZQUIERDO , U AREZ & R IGAU

medium level abstraction, without significant decrease performance. Considering number classes, BLC classifiers obtain high performance rates maintaining much
higher expressiveness SuperSenses. However, using SuperSenses (40 classes) obtain
accurate semantic tagger performances around 80%. Even better, use BLC20
tagging nouns (558 semantic classes F1 75%) SuperSenses verbs (14 semantic
classes F1 around 75%).
systems SemEval2 All-words Word Sense Disambiguation Specific Domain task
proved simple features exploiting BLC perform well sophisticated methods.
Comparing wordbased classifiers, see BLC20 classes contribute two main
aspects: classbased classifiers obtain better results wordbased ones semantic classes
contribute effectively results. fact indicates that, particular, BLC20 useful
extract monosemous training examples unlabeled domain data.
next goal exploit inconsistencies different labeling provided different
class-based classifiers order obtain robust accurate class-based WSD system.
main idea study several classifiers, one based different degree abstraction (e.g.
BLC20, BLC50, WordNet Domains, etc.) label concrete context example incompatible
tags. manner, would able predict apply best classifier depending
context.

Acknowledgements
work partially supported NewsReader project31 (ICT-2011-316404), Spanish project SKaTer32 (TIN2012-38584-C06-02).

References
Agirre, E., & de Lacalle, O. L. (2003). Clustering wordnet word senses. Proceedings
RANLP03, Borovets, Bulgaria.
Agirre, E., & Edmonds, P. (2007). Word Sense Disambiguation: Algorithms Applications.
Springer.
Agirre, E., Lopez de Lacalle, O., Fellbaum, C., Hsieh, S.-K., Tesconi, M., Monachini, M., Vossen,
P., & Segers, R. (2010). Semeval-2010 task 17: All-words word sense disambiguation
specific domain. Proceedings 5th International Workshop Semantic Evaluation,
pp. 7580, Uppsala, Sweden. Association Computational Linguistics.
Bhagwani, S., Satapathy, S., & Karnick, H. (2013). Merging word senses. Proceedings Workshop Graph-based Methods Natural Language Processing (TextGraphs-8), pp. 1119.
Castillo, M., Real, F., & Rigau, G. (2004). Automatic assignment domain labels wordnet.
Proceeding 2nd International WordNet Conference, pp. 7582.
Ciaramita, M., & Altun, Y. (2006). Broad-coverage sense disambiguation information extraction supersense sequence tagger. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP06), pp. 594602, Sydney, Australia. ACL.
31. http://www.newsreader-project.eu
32. http://nlp.lsi.upc.edu/skater

118

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Ciaramita, M., & Johnson, M. (2003). Supersense tagging unknown nouns wordnet.
Proceedings Conference Empirical methods natural language processing
(EMNLP03), pp. 168175. ACL.
Curran, J. (2005). Supersense tagging unknown nouns using semantic similarity. Proceedings
43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 26
33. ACL.
Escudero, G., Marquez, L., & Rigau., G. (2000). Empirical Study Domain Dependence
Supervised Word Sense Disambiguation Systems. Proceedings joint SIGDAT
Conference Empirical Methods Natural Language Processing Large Corpora,
EMNLP/VLC, Hong Kong, China.
Fellbaum, C. (Ed.). (1998). WordNet. Electronic Lexical Database. MIT Press.
Gangemi, A., Nuzzolese, A. G., Presutti, V., Draicchio, F., Musetti, A., & Ciancarini, P. (2012).
Automatic typing dbpedia entities. Proceedings 11th International Conference
Semantic Web - Volume Part I, ISWC12, pp. 6581, Berlin, Heidelberg. Springer-Verlag.
Gonzalez, A., Rigau, G., & Castillo, M. (2012). graph-based method improve wordnet domains.
Computational Linguistics Intelligent Text Processing, pp. 1728. Springer.
Hamp, B., Feldweg, H., et al. (1997). Germanet-a lexical-semantic net german. Proceedings
ACL workshop Automatic Information Extraction Building Lexical Semantic Resources
NLP Applications, pp. 915. Citeseer.
Hearst, M., & Schutze, H. (1993). Customizing lexicon better suit computational task.
Proceedingns ACL SIGLEX Workshop Lexical Acquisition, Stuttgart, Germany.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischedel, R. (2006). Ontonotes: 90
Proceedings Human Language Technology Conference NAACL, Companion
Volume: Short Papers, NAACL-Short 06, pp. 5760, Stroudsburg, PA, USA. Association
Computational Linguistics.
Izquierdo, R., Suarez, A., & Rigau, G. (2007). Exploring automatic selection basic level concepts. et al., G. A. (Ed.), International Conference Recent Advances Natural Language
Processing, pp. 298302, Borovets, Bulgaria.
Izquierdo, R., Suarez, A., & Rigau, G. (2009). empirical study class-based word sense disambiguation. Proceedings 12th Conference European Chapter Association
Computational Linguistics, EACL 09, pp. 389397, Stroudsburg, PA, USA. Association
Computational Linguistics.
Izquierdo, R., Suarez, A., & Rigau, G. (2010). Gplsi-ixa: Using semantic classes acquire monosemous training examples domain texts. Proceedings 5th International Workshop
Semantic Evaluation, pp. 402406. Association Computational Linguistics.
Joachims, T. (1998). Text categorization support vector machines: learning many relevant
features. Nedellec, C., & Rouveirol, C. (Eds.), Proceedings ECML-98, 10th European
Conference Machine Learning, No. 1398, pp. 137142, Chemnitz, DE. Springer Verlag,
Heidelberg, DE.
L. Bentivogli, P. Forner, B. M., & Pianta, E. (2004). Revising wordnet domains hierarchy: Semantics, coverage, balancing. COLING 2004 Workshop Multilingual Linguistic
Resources, Geneva, Switzerland.
119

fiI ZQUIERDO , U AREZ & R IGAU

Magnini, B., & Cavaglia, G. (2000). Integrating subject field codes wordnet. Proceedings
LREC, Athens. Greece.
Marquez, L., Escudero, G., Martnez, D., & Rigau, G. (2006). Supervised corpus-based methods
wsd. E. Agirre P. Edmonds (Eds.) Word Sense Disambiguation: Algorithms
applications., Vol. 33 Text, Speech Language Technology. Springer.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding predominant word senses
untagged text. 42nd Annual Meeting Association Computational Linguistics,
Barcelona, Spain.
Mihalcea, R. (2007). Using wikipedia automatic word sense disambiguation. Proceedings
NAACL HLT 2007.
Mihalcea, R., Csomai, A., & Ciaramita, M. (2007). Unt-yahoo: Supersenselearner: Combining
senselearner supersense coarse semantic features. Proceedings 4th
International Workshop Semantic Evaluations, SemEval 07, pp. 406409, Stroudsburg,
PA, USA. Association Computational Linguistics.
Mihalcea, R., & Moldovan, D. (2001). Automatic generation coarse grained wordnet. Proceding NAACL workshop WordNet Lexical Resources: Applications, Extensions Customizations, Pittsburg, USA.
Miller, G., Leacock, C., Tengi, R., & Bunker, R. (1993). Semantic Concordance. Proceedings
ARPA Workshop Human Language Technology.
Navigli, R. (2006). Meaningful clustering senses helps boost word sense disambiguation performance. ACL-44: Proceedings 21st International Conference Computational
Linguistics 44th annual meeting Association Computational Linguistics,
pp. 105112, Morristown, NJ, USA. Association Computational Linguistics.
Navigli, R. (2009). Word Sense Disambiguation: survey. ACM Computing Surveys, 41(2), 169.
Navigli, R., Litkowski, K., & Hargraves, O. (2007). Semeval-2007 task 07: Coarse-grained english
all-words task. Proceedings Fourth International Workshop Semantic Evaluations (SemEval-2007), pp. 3035, Prague, Czech Republic. Association Computational
Linguistics.
Niles, I., & Pease, A. (2001). Towards standard upper ontology. Proceedings 2nd
International Conference Formal Ontology Information Systems (FOIS-2001), pp. 17
19. Chris Welty Barry Smith, eds.
Niles, I., & Pease, A. (2003). Linking lexicons ontologies: Mapping WordNet Suggested
Upper Merged Ontology. Arabnia, H. R. (Ed.), Proc. IEEE Int. Conf. Inf.
Knowledge Engin. (IKE 2003), Vol. 2, pp. 412416. CSREA Press.
Noreen, E. (1989). Computer-intensive methods testing hypotheses: introduction. Wiley
Interscience publication. Wiley.
Paa, G., & Reichartz, F. (2009a). Exploiting semantic constraints estimating supersenses
crfs.. SDM, pp. 485496. SIAM.
Paa, G., & Reichartz, F. (2009b). Exploiting semantic constraints estimating supersenses
crfs.. SDM, pp. 485496. SIAM.
120

fiW ORD VS . C LASS -BASED W ORD ENSE ISAMBIGUATION

Palmer, M., Fellbaum, C., Cotton, S., Delfs, L., & Dang, H. T. (2001). English tasks: All-words
verb lexical sample. Proceedings SENSEVAL-2 Workshop. conjunction
ACL2001/EACL2001, Toulouse, France.
Peters, W., Peters, I., & Vossen, P. (1998). Automatic sense clustering eurowordnet. First International Conference Language Resources Evaluation (LREC98), Granada, Spain.
Picca, D., Gliozzo, A. M., & Ciaramita, M. (2008). Supersense tagger italian.. LREC. Citeseer.
Pradhan, S., Dligach, E. L. D., & Palmer, M. (2007). Semeval-2007 task 17: English lexical sample,
srl words. SemEval 07: Proceedings 4th International Workshop Semantic
Evaluations, pp. 8792, Morristown, NJ, USA. Association Computational Linguistics.
Rosch, E. (1977). Human categorisation. Studies Cross-Cultural Psychology, I(1), 149.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings
International Conference New Methods Language Processing, pp. 4449.
Schneider, N., Mohit, B., Dyer, C., Oflazer, K., & Smith, N. A. (2013). Supersense tagging
arabic: mt-in-the-middle attack.. HLT-NAACL, pp. 661667. Citeseer.
Schneider, N., Mohit, B., Oflazer, K., & Smith, N. A. (2012). Coarse lexical semantic annotation
supersenses: arabic case study. Proceedings 50th Annual Meeting
Association Computational Linguistics: Short Papers-Volume 2, pp. 253258. Association
Computational Linguistics.
Segond, F., Schiller, A., Greffenstette, G., & Chanod, J. (1997). experiment semantic tagging
using hidden markov model tagging. ACL Workshop Automatic Information Extraction
Building Lexical Semantic Resources NLP Applications, pp. 7881. ACL, New
Brunswick, New Jersey.
Snow, R., S., P., D., J., & A., N. (2007). Learning merge word senses. Proceedings Joint
Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 10051014.
Snyder, B., & Palmer, M. (2004). english all-words task. Mihalcea, R., & Edmonds, P.
(Eds.), Senseval-3: Third International Workshop Evaluation Systems Semantic Analysis Text, pp. 4143, Barcelona, Spain. Association Computational Linguistics.
Tsvetkov, Y., Schneider, N., Hovy, D., Bhatia, A., Faruqui, M., & Dyer, C. (2014). Augmenting
english adjective senses supersenses. Proc. LREC, pp. 43594365.
Villarejo, L., Marquez, L., & Rigau, G. (2005). Exploring construction semantic class classifiers wsd. Proceedings 21th Annual Meeting Sociedad Espaola para el
Procesamiento del Lenguaje Natural SEPLN05, pp. 195202, Granada, Spain. ISSN 11365948.
Vossen, P. (Ed.). (1998). EuroWordNet: Multilingual Database Lexical Semantic Networks
. Kluwer Academic Publishers .
Wikipedia (2015). Wikipedia, free encyclopedia. https://en.wikipedia.org.. [Online;
accessed 21-August-2015].
Yarowsky, D. (1994). Decision lists lexical ambiguity resolution: Application accent restoration spanish french. Proceedings 32nd Annual Meeting Association
Computational Linguistics (ACL94).
121

fiI ZQUIERDO , U AREZ & R IGAU

Zhong, Z., & Ng, H. T. (2010). makes sense: wide-coverage word sense disambiguation system
free text. Proceedings ACL 2010 System Demonstrations, ACLDemos 10, pp.
7883, Stroudsburg, PA, USA. Association Computational Linguistics.

122

fiJournal Artificial Intelligence Research 54 (2015) 193231

Submitted 6/15; published 10/15

Expressiveness Two-Valued Semantics
Abstract Dialectical Frameworks
Hannes Strass

strass@informatik.uni-leipzig.de

Computer Science Institute, Leipzig University
Augustusplatz 10, 04109 Leipzig, Germany

Abstract
analyse expressiveness Brewka Woltrans abstract dialectical frameworks
two-valued semantics. expressiveness mean ability encode desired set
two-valued interpretations given propositional vocabulary using atoms
A. also compare ADFs expressiveness (the two-valued semantics of)
abstract argumentation frameworks, normal logic programs propositional logic.
computational complexity two-valued model existence problem
languages (almost) same, show languages form neat hierarchy
respect expressiveness. demonstrate hierarchy collapses
allow introduce linear number new vocabulary elements. finally also analyse
compare representational succinctness ADFs (for two-valued model semantics),
is, capability represent two-valued interpretation sets space-efficient manner.

1. Introduction
often not, different knowledge representation languages conceptually similar partially overlapping intended application areas. faced
application choice several possible knowledge representation languages
could used application? One first axes along compare different
formalisms comes mind computational complexity: language computationally expensive considering problem sizes typically encountered practice,
clear criterion exclusion. available language candidates
computational complexity? expressiveness computationalcomplexity sense kinds problems formalism solve? same,
need fine-grained notion expressiveness. paper, use notion
study expressiveness abstract dialectical frameworks (ADFs) (Brewka & Woltran,
2010; Brewka, Ellmauthaler, Strass, Wallner, & Woltran, 2013), recent generalisation
abstract argumentation frameworks (AFs) (Dung, 1995).
Argumentation frameworks de-facto standard formalism abstract argumentation, field studies (abstract) arguments relate terms directed
conflicts (attacks), conflicts resolved without looking
arguments. AFs popular well-studied, noted many times
literature expressive capabilities somewhat limited. recently
made technically precise Dunne, Dvorak, Linsbichler, Woltran (2014, 2015),
basically showed introducing new, purely technical arguments sometimes inevitable using AFs representation purposes. However, due nature,
dialectical meaning technical arguments might ironically debatable.
c
2015
AI Access Foundation. rights reserved.

fiStrass

surprisingly, quite number generalisations AFs proposed (for
overview refer Brewka, Polberg, & Woltran, 2014). one general AF
alternatives, aforementioned abstract dialectical frameworks (ADFs) emerged.
formalism, arguments (called statements there) abstract, also links
arguments. AFs links necessarily attacks, ADFs statement
associated acceptance condition Boolean function parent statements
specifies exactly statement accepted. way, acceptance
conditions ultimately express meaning links ADF. Even restricted subclass
bipolar ADFs intuitively links supporting attacking proper
generalisation AFs, quite expressive one shall see paper.
ADFs could called lovechild AFs logic programs, since combine
intuitions semantics Dung-style abstract argumentation well logic programming (Brewka et al., 2013; Strass, 2013; Alviano & Faber, 2015). abstract
level, ADFs intended function argumentation middleware sufficiently expressive target formalism translations concrete (application) formalisms.
part ADF success story, mention reconstruction Carneades model
argument (Brewka & Gordon, 2010), instantiation simple defeasible theories
ADFs (Strass, 2015a), recent applications ADFs legal reasoning reasoning
cases Al-Abdulkarim, Atkinson, Bench-Capon (2014, 2015).
paper, approach abstract dialectical frameworks knowledge representation
formalisms, since used represent knowledge arguments relationships
arguments. employ view analyse representational capabilities
ADFs. Due roots AFs logic programs, also compare representational capabilities formalisms setting. initial study restrict
looking two-valued semantics, specifically ADF (stable) model semantics, corresponds AF stable extension semantics, supported stable
model semantics logic programs. add propositional logic well-known reference point. Analysing precise formalisms additionally makes sense us
computational complexity respective model existence problems (with
one exception):
AFs, deciding stable extension existence NP-complete (Dimopoulos, Nebel, &
Toni, 2002);
normal logic programs, deciding existence supported/stable models NPcomplete (Bidoit & Froidevaux, 1991; Marek & Truszczynski, 1991);
ADFs, deciding existence (supported) models NP-complete (Brewka
et al., 2013), deciding existence stable models P2 -complete general
ADFs (Brewka et al., 2013) NP-complete subclass bipolar ADFs (Strass
& Wallner, 2015);
propositional satisfiability problem NP-complete.
view almost identical complexities, use alternative measure
expressiveness knowledge representation formalism F: Given set two-valued
interpretations, knowledge base F exact model set? notion
194

fiExpressiveness Two-Valued Semantics ADFs

lends straightforwardly compare different formalisms (Gogic, Kautz, Papadimitriou,
& Selman, 1995):
Formalism F2 least expressive formalism F1 every
knowledge base F1 equivalent knowledge base F2 .
expressiveness understood terms realisability, kinds model sets
formalism express? (In model theory, known definability.)
easy see propositional logic express set two-valued interpretations,
universally expressive. easy (but less easy) see normal logic programs
supported model semantics. normal logic programs stable model semantics,
clear model sets expressed, since two different stable models
always incomparable respect subset relation.1 paper, study
expressiveness properties mentioned formalisms different semantics.
turns languages form less strict expressiveness hierarchy, AFs
bottom, ADFs LPs stable semantics higher ADFs LPs
supported model semantics top together propositional logic.
show language F2 least expressive language F1 mainly
use two different techniques. best case, use syntactic compact faithful
translation knowledge bases F1 F2 . Compact means translation
change vocabulary, is, introduce new atoms. Faithful means
translation exactly preserves models knowledge base respective semantics
two languages. second best case, assume knowledge base F1
given form set X desired models construct semantic realisation X
F2 , is, knowledge base F2 model set precisely X. show language
F2 strictly expressive F1 , additionally present knowledge base kb
F2 prove F1 cannot express model set kb.
Analysing expressiveness argumentation formalisms quite recent strand
work. ascent attributed Dunne et al. (2014, 2015), studied realisability
argumentation frameworks (allowing introduce new arguments long
never accepted). Likewise, Dyrkolbotn (2014) analysed AF realisability projection
(allowing introduce new arguments) three-valued semantics. Baumann, Dvorak, Linsbichler, Strass, Woltran (2014) studied expressiveness subclass compact
AFs, argument accepted least once. Finally, recently, Puhrer
(2015) analysed realisability three-valued semantics ADFs. Previous preliminary works include Brewka, Dunne, Woltran (2011), translated ADFs
AFs ADF model AF stable extension semantics, however translation
introduces additional arguments therefore compact; (Strass, 2013),
studied syntactic intertranslatability ADFs LPs, look
expressiveness realisability.
gain achieved analysis paper increased
clarity fundamental properties knowledge representation languages
formalisms express, actually? several applications. Dunne
et al. (2015) remarked, major application constructing knowledge bases aim
1. However, stable model semantics becomes universally expressive allow nested expressions
form p rule bodies (Lifschitz, Tang, & Turner, 1999; Lifschitz & Razborov, 2006).

195

fiStrass

encoding certain model set. necessary prerequisite this, must known
intended model set realisable first place. example, recent approach
revising argumentation frameworks (Coste-Marquis, Konieczny, Mailly, & Marquis, 2014),
authors avoid problem assuming produce collection AFs whose model sets
union produce desired model set. work Dunne et al. (2015) showed
indeed necessary case AFs stable extension semantics, work shows
ADFs model semantics, single knowledge base (ADF) always enough
realise given model set. more, assume intended model set
given form propositional formula, size realising ADF
linear size formula. one example several occasions
also consider sizes realisations, uncommon logic-based AI (Darwiche &
Marquis, 2002; Lifschitz & Razborov, 2006; French, van der Hoek, Iliev, & Kooi, 2013; Shen
& Zhao, 2014). Indeed, representation size fundamental practical aspect knowledge
representation languages: universal expressiveness little use model sets express
require exponential-size knowledge bases even best case!
course, fact languages study computational complexity
means principle exist polynomial intertranslations respective decision
problems. intertranslations may involve introduction polynomial number
new atoms. theory, increase n atoms nk atoms k > 1
consequence. practice, profound impact: number n atoms directly
influences search space implementation potentially cover. There, step
2n
k1 n
k
k1
2n = 2n n = 2n
amounts exponential increase search space size. able realise model set
compactly, without new atoms, therefore attests formalism F certain basic
kind efficiency property, sense F-realisation model set
unnecessarily enlarge search space algorithms operating it.
might seem restricting assumption view formalisms sets F knowledge bases kb F associated two-valued semantics. However, language
representation model universal sense another way expressing languages sets words {0, 1}. Using n-element vocabulary = {a1 , . . . , }, binary word w = x1 x2 xn length n encoded set Mw = {ai | xi = 1} .
example, using vocabulary A3 = {a1 , a2 , a3 }, binary word 101 length 3 corresponds set M101 = {a1 , a3 }. Consequently, set Ln words length n
represented set XLn 2An subsets : XLn = {Mw | w Ln }.
example vocabulary, word set L3 = {101, 110, 011} represented model
set XL3 = {{a1 , a3 } , {a1 , a2 } , {a2 , a3 }}. Conversely, sequence (Xn )n0 sets

Xn 2An uniquely determines language L = n0 Ln {0, 1}: n N,
Ln = {wM | Xn } wM = x1 x2 xn {1, . . . , n}, xi = 1
ai xi = 0 ai
/ . paper use language refer object-level
languages formalism refers meta-level languages, propositional logic,
argumentation frameworks, abstract dialectical frameworks, logic programs.
Formally, syntax ADFs defined via Boolean functions. However, interested representations ADFs. fix representation ADFs via fixing
196

fiExpressiveness Two-Valued Semantics ADFs

representation Boolean functions. choose use (unrestricted) propositional formulas, customary literature (Brewka & Woltran, 2010; Brewka et al.,
2013; Polberg et al., 2013; Polberg, 2014; Gaggl & Strass, 2014; Linsbichler, 2014; Strass &
Wallner, 2015; Puhrer, 2015; Gaggl, Rudolph, & Strass, 2015). Exceptions custom
works Brewka et al. (2011), use Boolean circuits, one (Strass,
2013) used characteristic models (that is, used representation equivalent
representing formulas disjunctive normal form). subclass bipolar ADFs,
yet uniform representation exists, another question address paper.
propositional formulas vocabulary mean formulas Boolean
basis {, , }, is, trees whose leaves (sinks) atoms logical constants
true > false , internal nodes either unary () binary (,). also make
occasional use Boolean circuits, trees replaced directed acyclic
graphs; particular, allow unbounded fan-in, is, reusing sub-circuits. usual,
depth formula (circuit) length longest path root leaf
(sink). Figure 1 shows formula circuit examples depth 3.



p



















q

p

q

p

q

Figure 1: Representing (p q) (q p) formula tree (left) circuit (right).

Analysing expressive power representation size Boolean circuits established sub-field computational complexity (Arora & Barak, 2009). led
number language classes whose members recognised Boolean circuits satisfying

certain restrictions. need class AC0 , contains languages L = n0 Ln
exist d, k N n N, exists Boolean circuit Cn
depth size nk models Cn exactly express Ln .2
words, every language L AC0 recognised family polynomial-size Boolean
circuits fixed maximal depth independent word length.
paper proceeds follows. first define notion expressiveness (and succinctness) formally introduce formalisms study. reviewing several
intertranslatability results languages, step-wise obtain results lead
expressiveness hierarchy, times also looking representational efficiency.
finally show allowing linearly expand vocabulary leads collapse
hierarchy. paper concludes discussion possible future work.

2. precise, n N, models Cn exactly XLn , turn expresses Ln .

197

fiStrass

2. Background
presume finite set atoms (statements, arguments), vocabulary. knowledge
representation formalism interpreted set F; (two-valued) semantics

F mapping : F 22 assigns sets two-valued models knowledge bases
kb F. (So implicit .) Strictly speaking, two-valued interpretation mapping
set atoms two truth values true false, technical ease
represent two-valued interpretations sets containing atoms true. Below,
write (F) = {(kb) | kb F}; intuitively, (F) set interpretation sets
formalism F express, knowledge base whatsoever. example, F = PL

propositional logic = mod usual model semantics, (PL) = 22 since
obviously set models realisable propositional logic.3 leads us compare
different pairs languages semantics respect semantics range models.
concept formalism concentrates semantics decidedly remains abstract.
first define expressiveness relation among formalisms.
Definition 1. Let finite vocabulary, F1 , F2 formalisms interpreted


1 : F1 22 2 : F2 22 two-valued semantics. define
F11 e F22

iff

1 (F1 ) 2 (F2 )

Intuitively, formalism F2 semantics 2 least expressive formalism F1
semantics 1 , model sets F1 express 1 also contained
F2 produce 2 . (If semantics clear context
omit them; holds particular argumentation frameworks propositional logic,
look single semantics.) usual,
F1 <e F2 iff F1 e F2 F2 6e F1 ;
F1
=e F2 iff F1 e F2 F2 e F1 .
relation e reflexive transitive definition, necessarily antisymmetric.
is, might different formalisms F1 6= F2 equally expressive: F1
=e F2 .
next introduce succinctness relation defined Gogic et al. (1995).
Definition 2. Let finite vocabulary; let F1 F2 formalisms interpreted A, size measures kk1 kk2 , two-valued semantics 1 2 ,
respectively. Define F11 F22 k N kb1 F1
1 (kb1 ) 1 (F1 ) 2 (F2 ), kb2 F2 1 (kb1 ) = 2 (kb2 ) kkb2 k2 kkb1 kk1 .
Intuitively, F11 F22 means F2 2 least succinct F1 1 .
Put another way, F11 F22 hold, knowledge base F1 equivalent
counterpart F2 must equivalent counterpart polynomially larger.
Note succinctness talks model sets express,
meaningful comparing languages equally expressive, is, whenever
3. set X 2A simply define X =
mod (X ) = X.

W

X

198

=

V





V

aA\M

clearly

fiExpressiveness Two-Valued Semantics ADFs

1 (F1 ) = 2 (F2 ). usual, define F1 <s F2 iff F1 F2 F2 6s F1 , F1
=s F2
iff F1 F2 F2 F1 . relation reflexive, necessarily antisymmetric
transitive.
final general definition formalisms polynomially expressing languages.
Here, already make use previously introduced bijection interpretations
binary words use term languages synonymously refer both.

Definition 3. formalism F polynomially express language L = n0 Ln


semantics : F 22 k N positive n N
knowledge base kbn F formalism (kbn ) = Ln kkbn k O(nk ).
next introduce specific object-level languages use. First all,
language Parity contains odd-element subsets vocabulary. Formally,
= {a1 , . . . , } n 1
Parityn = {M | N : |M | = 2m + 1}

explained before, Parity = nN,n1 Parityn . textbook result Parity
expressible polynomial-size propositional formulas (Jukna, 2012); example,
define Parity
(a1 ) = a1 n 2 set
1
Parity
(a1 , . . . , ) = (Parity
(a1 , . . . , ) Parity
(an +1 , . . . , ))
n
n
n
(Parity
(a1 , . . . , ) Parity
(an +1 , . . . , ))
n
n


n = n2 n = n2 . (This construction yields formula logarithmic depth
therefore polynomial size.) also textbook result (although nearly easy
see) Parity cannot expressed depth-bounded polynomial-size circuits, is,
Parity
/ AC0 (Jukna, 2012).
another important class, threshold languages defined n, k N n 1
k n:
Thresholdn,k = {M | k |M |}
is, Thresholdn,k contains
interpretations n atoms least k atoms
true. special case k = n2 leads majority languages,
Majorityn = Thresholdn,d n e
2

contain interpretations least half atoms vocabulary true.
next introduce particular knowledge representation languages study
paper. make use vocabulary A; results paper considered
parametric given vocabulary.
2.1 Logic Programs
vocabulary define = {not | A} accordingly set literals
= A. normal logic program rule form B
B . set B called body rule, abbreviate B + = B
199

fiStrass

B = {a | B}. logic program (LP) P set logic program rules
A. interpretation satisfies body B rule B P iff B +
B = . supported model P iff = {a | B P, satisfies B}.
logic program P denote set supported models su(P ). intuition
behind semantics atoms true model
kind support.
However, support might cyclic self-support. instance, logic program
{a {a}} two supported models, {a}, latter undesired many
application domains. alternative, Gelfond Lifschitz (1988) proposed stable
model semantics, allow self-support: set stable model P iff
-least supported model P , P obtained P (1) eliminating
rule whose body contains literal , (2) deleting literals
form bodies remaining rules (Gelfond & Lifschitz, 1988).
write st(P ) set stable models P . follows definition st(P )
-antichain: M1 6= M2 st(P
P) M1 6 M2 . size measure define
ka Bk = |B| + 1 rules kP k = rP krk programs.
example, consider vocabulary = {a, b, c} logic program
P = {a {b} , b {a} , c {not a}}. find su(P ) = {{c} , {a, b}} st(P ) = {{c}}.
2.2 Argumentation Frameworks
Dung (1995) introduced argumentation frameworks pairs F = (A, R) set
(abstract) arguments R relation attack arguments.
purpose semantics argumentation frameworks determine sets arguments (called
extensions) acceptable according various standards. given extension
A, arguments considered accepted, attacked
argument considered rejected, others neither, status
undecided. interested so-called stable extensions, sets arguments
attack attack arguments set. stable extensions,
argument either accepted rejected definition, thus semantics two-valued.
formally, set arguments conflict-free iff a, b (a, b) R.
set stable extension (A, R) iff conflict-free \
argument b (b, a) R. AF F , denote set stable extensions
st(F ). Again, follows definition stable extension set st(F ) always
-antichain. size argumentation framework F = (A, R) kF k = |A| + |R|.
example, AF F = ({a, b, c} , {(a, b), (b, a), (b, c)}) visualised using
c set stable extensions st(F ) = {{a, c} , {b}}.
b
directed graph
2.3 Abstract Dialectical Frameworks
abstract dialectical framework tuple = (A, L, C) set statements
(representing positions one take take debate), L set links
(representing dependencies positions), C = {Ca }aA collection total
functions Ca : 2par (a) {t, f }, one statement A. function Ca called
acceptance condition expresses whether accepted, given acceptance
200

fiExpressiveness Two-Valued Semantics ADFs

status parents par (a). paper, represent Ca propositional formula
par (a). mentioned earlier, propositional formulas built using negation ,
conjunction disjunction ; connectives material implication , logical equivalence
exclusive disjunction = regarded abbreviations. specify acceptance
condition, then, take Ca (M par (a)) = hold iff model , |= .
Brewka Woltran (2010) introduced useful subclass ADFs: ADF = (A, L, C)
bipolar iff links L supporting attacking (or both). link (b, a) L supporting iff par (a), Ca (M ) = implies Ca (M {b}) = t.
Symmetrically, link (b, a) L attacking iff par (a),
Ca (M {b}) = implies Ca (M ) = t. link (b, a) supporting attacking
b influence a, link redundant (but violate bipolarity).
sometimes use circumstance searching ADFs; simply assume
L = A, links actually needed expressed acceptance conditions make redundant.
numerous semantics ADFs; interested two them,
(supported) models stable models. set model iff
find iff Ca (M ) = t. definition stable models inspired logic
programming slightly complicated (Brewka et al., 2013). Define operator by4
(X, ) = (ac(X, ), re(X, )) X, A,
ac(X, ) = {a | Z : X Z \ Ca (Z) = t}
re(X, ) = {a | Z : X Z \ Ca (Z) = f }
intuition behind operator follows: pair (X, ) represents partial interpretation set statements X accepted (true), rejected
(false), \ (X ) neither. operator checks statement
whether total interpretations possibly arise (X, ) agree truth
value acceptance condition a. is, accepted matter
statements \ (X ) interpreted, acc(X, ). set rej (X, ) defined
symmetrically, pair (acc(X, ), rej (X, )) constitutes refinement (X, ).
A, reduced ADF DM = (M, LM , C ) defined LM = L
setting
/ ], is, replacing b
/ false
= [b/ : b
acceptance formula a. model stable model iff least fixpoint
operator DM given (M, ). usual, su(D) st(D) denote respective model
sets; ADF models
P-related, ADF stable models cannot. size ADF
given kDk = aA ka k; size kk formula number
nodes.
example ADF D, consider vocabulary = {a, b, c} acceptance formulas
= c, b = c, c = b. single supported model, su(D) = {{a, b, c}},
find st(D) = since atoms model support circularly.
2.4 Translations Formalisms
review known translations mentioned formalisms.
4. operator closely related ultimate approximation operators Denecker, Marek,
Truszczynski (2004), observed earlier (Strass, 2013).

201

fiStrass

2.4.1 AFs BADFs
Brewka Woltran (2010) showed translate AFs ADFs: AF FV= (A, R),
define ADF associated F DF = (A, R, C) C = {a }aA = (b,a)R b
A. Clearly, resulting ADF bipolar: parents always attacking. Brewka
Woltran proved translation faithful AF stable extension ADF
model semantics (Proposition 1). Brewka et al. (2013) later proved AF
stable extension ADF stable model semantics (Theorem 4). easy see
translation computed polynomial time induces linear blowup.
2.4.2 ADFs PL
Brewka Woltran (2010) also showed ADFs supported model semantics
faithfully translated propositional logic: acceptance conditions statements represented propositional formulas , supported models
ADF given classical propositional models formula set
= {a | A}.
2.4.3 AFs PL
combination, previous two translations yield

n polynomial
V
fifaithfulotranslation
fi
chain AFs propositional logic: (A,R) =
(b,a)R b fi .
2.4.4 ADFs LPs
earlier work (Strass, 2013), showed ADFs faithfully translated normal
logic programs. ADF = (A, L, C), standard LP
PD = {a (M (par (a) \ )) | A, Ca (M ) = t}
follows Lemma 3.14 Strass (2013) translation preserves supported
model semantics. translation size-preserving acceptance condition representation Strass (2013) via characteristic models; representing acceptance conditions
via propositional formulas, cannot guaranteed show later.5
2.4.5 AFs LPs
translation chain AFs ADFs LPs compact, faithful AF stable
semantics LP stable semantics (Osorio, Zepeda, Nieves, & Cortes, 2005), AF stable
semantics LP supported semantics (Strass, 2013). size-preserving since single
rule atom contains attackers once: P(A,R) = {a {not b | (b, a) R} | A}.
5. Already complexity reasons, cannot expect translation also faithful stable
semantics. indeed, ADF = ({a} , {(a, a)} , {a = a}) stable model {a}
standard logic program P (D) = {a {a} , {not a}} stable model. However, holds
st(P (D)) st(D) (Denecker et al., 2004; Strass, 2013).

202

fiExpressiveness Two-Valued Semantics ADFs

2.4.6 LPs PL
well-known logic programs supported model semantics translated
propositional logic (Clark, 1978). logic program P becomes propositional theory P ,


_
^
^

P = {a | A} =
b
b A.
aBP

bB +

bB

stable model semantics, additional formulas added, extended
translation works (Lin & Zhao, 2004).
2.4.7 LPs ADFs
Clark completion normal logic program directly yields equivalent ADF
signature (Brewka & Woltran, 2010). Clearly translation computable
polynomial time blowup (with respect original logic program)
linear. resulting translation faithful supported model semantics, follows
Lemma 3.16 Strass (2013).
2.5 Representing Bipolar Boolean Functions
bipolarity hitherto predominantly defined used context
ADFs (Brewka & Woltran, 2010), easy define concept Boolean functions
general. Let set atoms f : 2A {t, f } Boolean function. atom
supporting iff A, f (M ) = implies f (M {a}) = t; write sup(f ).
atom attacking iff A, f (M ) = f implies f (M {a}) = f ;
write att(f ). Boolean function f : 2A {t, f } semantically bipolar iff
supporting attacking both. Throughout paper, sometimes take Boolean
function given interpretation set say set bipolar.
define bipolar propositional formulas representing bipolar ADFs.
important study, also since (for three-valued semantics), bipolarity
key BADFs low complexity comparison general ADFs (Strass & Wallner, 2015).
now, usually assumed specify bipolar ADF, addition statements,
links acceptance conditions, user specifies link whether supporting
attacking (Strass & Wallner, 2015). introduce arguably simpler way,
support attack represented syntax propositional formula encoding
acceptance function.
Formally, polarity atom formula determined number
negations path root formula tree atom. polarity positive
number even negative number odd.
Definition 4. propositional formula syntactically bipolar
atom occurs positively negatively .
Recall use formulas basis {, , } thus hidden
negations, e.g. material implication. formulas negation normal form (that is,
negation applied atomic formulas), polarities atoms read
formula directly.
203

fiStrass

address question represent bipolar Boolean functions. Clearly
Boolean functions represented propositional formulas; modify construction
later thus reproduce here: Boolean function f : 2A {t, f }, associated
formula
^
^
_

(1)
=

f =


A,f (M )=t

aA\M

is, exactly one model , f enumerates models.
particular, bipolar Boolean functions represented propositional formulas well. However, guarantees us existence representations
gives us way actually obtain them. first fundamental result shows
construct syntactically bipolar propositional formula given semantically bipolar
Boolean function. converse straightforward, thus two notions bipolarity
closely related. formula , associated Boolean function f returns
gets input model .
Theorem 1. Let set atoms.
1. syntactically bipolar formula A, Boolean function f semantically
bipolar.
2. semantically bipolar Boolean function f : 2A {t, f }, syntactically bipolar
formula f ff = f given
f =

_



=

A,
f (M )=t

^
aM,
aatt(f
/
)



^



(2)

aA\M,
asup(f
/
)

Proof.
1. Obvious: every atom occurring positively supporting, every atom occurring negatively attacking.
2. Let f : 2A {t, f } semantically bipolar. Note first (2),
|= . easy see f syntactically bipolar: Since f
semantically bipolar, is: (1) attacking supporting,
occurs negatively f ; (2) supporting attacking, occurs
positively f ; (3) supporting attacking, occur f .
remains show ff = f ; show |= f f .
|= f f : Let v : {t, f } v(f ) = t.
f (M ) = v(M ) = t. (Clearly v = vM .) |= get v(M ) =
thus v(f ) = t.
|= f f : model v f , f (M ) =
v(M ) = t. show model f model f , show
f (M ) = t, model v model f . Let |A| = n.
contains exactly n literals. corresponding
k N 0 k n contains exactly n k literals. two
204

fiExpressiveness Two-Valued Semantics ADFs

interpretations v1 : {t, f } v2 : {t, f }, define difference
(v1 , v2 ) = {a | v1 (a) 6= v2 (a)}. (Note |A| = n always
|(v1 , v2 )| n.) use induction k show following:
f (M ) = t, v : {t, f } v(M ) = |(v, vM )| = k
find v(f ) = t. covers models v f (since |(v, vM )| |A|)
thus establishes claim.
k = 0: (v, vM ) = implies v = vM whence v(f ) = vM (f ) = vM (M ) =
definition f .
k
k + 1: Let f (M ) = t, v : {t, f } v(M ) =
|(v, vM )| = k + 1. Since k + 1 > 0, (v, vM ), is,
v(a) 6= vM (a).
(a) supporting attacking. necessarily v(a) = t. (If v(a) = f ,
vM (a) 6= v(a) implies vM (a) = t, is, whence {M } |=
v(M ) = f , contradiction.) Define interpretation w : {t, f }
w(a) = f w(c) = v(c) c \ {a}. Clearly (v, w) = {a}
|(w, vM )| = k. Hence induction hypothesis applies w
w(f ) = t. w(a) = f , v(a) = w(f ) = t. Since supporting, also v(f ) = t.
(b) attacking supporting. Symmetric opposite case above.
(c) supporting attacking. Define interpretation w : {t, f }
w(a) = vM (a) w(c) = v(c) c \ {a}. follows
|(w, vM )| = k, whence induction hypothesis applies w
w(f ) = t. Since supporting attacking (thus redundant),
get v(f ) = w(f ) = t.

result paves way analysing succinctness bipolar ADFs, since
quite natural way representing them.

3. Relative Expressiveness
analyse compare relative expressiveness argumentation frameworks
(AFs), (bipolar) abstract dialectical frameworks ((B)ADFs), normal logic programs (LPs)
propositional logic (PL). first look different families semantics supported
stable models isolation afterwards combine results two semantics.
formalisms F {ADF, LP} supported stable semantics,
indicate semantics via superscript Definition 1. AFs consider
stable semantics, (to date) semantics AFs interpretations
guaranteed map arguments either true (accepted) false (rejected, i.e. attacked
accepted argument). propositional logic PL consider usual model semantics.
syntactic translations reviewed previous section, currently
following expressiveness relationships. supported semantics,
AF e BADFsu e ADFsu
=e LPsu e PL
stable semantics,
205

fiStrass

AF e LPst <e PL AF e BADFst e ADFst <e PL
Note LPst <e PL ADFst <e PL hold since sets stable models antichain
property, contrast model sets propositional logic.
succinctness relation,
AF BADFsu ADFsu PL LPsu ADFsu
3.1 Supported Semantics
depicted above, know expressiveness AFs propositional logic
decrease. However, yet clear relationships strict. follows
show two strict, working way top-down least
expressive.
3.1.1 ADF vs. PL
first show ADFs realise set models showing given propositional
formula used construct equivalent ADF linear size.6
Theorem 2. PL e ADFsu PL ADFsu .
Proof. Let propositional formula vocabulary A. Define ADF
setting, A,
= = (a ) (a )
Thus ka k O(kk), whence kD k O(|A| kk). remains toVshow su(D ) = mod ().
Recall ADF A, su(D) = mod (D ) = aA (a ). Applying
definition yields
V
= aA (a (a ))
A, formula (a (a ))Vis equivalent . (The proof case
distinction a.) Thus equivalent aA , is, , follows
su(D ) = mod (D ) = mod ().

example, consider vocabulary = {a, b} propositional formula = b.
canonical construction yields ADF acceptance formulas = (a b)
b = b (a b). have:
= (a b) = (a (a b)) ((a b) a) (a b) b
Intuitively, = b expresses cannot false, true b true.
symmetrical argument, acceptance formula b equivalent b a. readily
checked su(D ) = {{a, b}} desired. Since know Section 2.4.2
converse translation also possible (ADFsu PL), get following.
Corollary 3. PL
=s ADFsu
6. consider vocabulary part input, size increase quadratic.

206

fiExpressiveness Two-Valued Semantics ADFs

acceptance conditions written propositional formulas, construction
realise X 2A proof Theorem 2 defines space-efficient equivalent
_
_

=

X,aM

A,M X,a
/

/

acceptance formula a, Footnote 3.
3.1.2 ADF vs. LP
Since ADFs supported semantics faithfully translated logic programs,
likewise translated propositional logic, following.
Corollary 4. ADFsu
=e LPsu
=e PL
However, extend succinctness relation, logic programs stipulate
particular syntactic form essentially fixed-depth circuit. specifically,
easy see language polynomially expressible normal logic programs
supported semantics AC0 . stable semantics so-called canonical logic
programs, recently shown Shen Zhao (2014) (Proposition 2.1).
case interested (supported semantics) works similarly, still present
proof completeness. main technical result towards proving lemma showing
turn logic program equivalent Boolean circuit fixed depth.
Lemma 5. every normal logic program P , exists circuit CP basis
{, , } that:
1. CP accepts supported models P ,
2. size CP linear size P ,
3. CP depth 4.
Proof. Let = {a1 , . . . , } vocabulary P , Clark completion P =
{ai | ai A}
V DNFs literals A. Clearly circuit P
must compute CP = ai (ai ) ai replaced (ai )(ai )
CNF literals A. construction depicted follows,
inner layers shown one only, dotted lines represent potential edges.

ai

...




...



a1

a1

...







ai
...



ai

ai
207

...

...







fiStrass

(1) follows since su(P ) = mod (P ) CP accepts models P .
(2), P contains = |P | rules, kP k total number inner gates
bounded n(2m + 3) n(2 kP k + 3). (3) clear.

statement Lemma 5 actually much stronger gives constant upper
bound resulting circuit depth arbitrarily-sized logic programs, readily follows
set polynomially logic-program expressible languages subset languages
expressible alternating Boolean circuits unbounded fan-in constant depth.
Proposition 6. L polynomially expressible normal logic programs supported
semantics, L AC0 .
follows immediately normal logic programs cannot polynomially express
language Parity.7 supported-semantics counterpart Theorem 3.1 (Shen
& Zhao, 2014).
Corollary 7. Parity polynomial size normal logic program representation.
Proof. Proposition 6 Parity
/ AC0 (Jukna, 2012).



follows propositional logic strictly succinct normal logic programs
supported semantics.
Corollary 8. PL 6s LPsu thus LPsu <s PL.
considerations since Theorem 2, follows small conjunctive
normal form (a conjunction clauses) disjunctive normal form (disjunction monomials) representation, also small normal logic program representation
mod ().
3.1.3 ADF vs. BADF
quite obvious canonical ADF constructed Theorem 2 bipolar, since
well every atom mentioned occurs positively negatively .
raises question whether construction adapted bipolar ADFs.
turns subclass bipolar ADFs strictly less expressive. Towards
proof result start new concept: conjugate model set
respect atom. concept used characterise ADF realisability
precisely captures if-and-only-if part ADFs supported model semantics:
translation ADF propositional logic V
(cf. Section 2.4.2) see result
basically conjunction equivalences: = aA (a ). conjunction part
captured set intersection, conjugate capture equivalence part.
Definition 5. Let vocabulary, X 2A A. a-conjugate X set
hai(X) = {M | X, } {M |
/ X,
/ M}
7. Logic programs supported models universally expressive, express Parity,
polynomial size.

208

fiExpressiveness Two-Valued Semantics ADFs

Alternatively, could write hai(X) = {M | X }. Intuitively, hai(X)
contains interpretations containment coincides exactly containment X. Formulated terms propositional formulas, X model set
formula A, hai(X) model set formula . Note vocabulary
implicit conjugate function.
Example 1. Consider vocabulary A2 = {a, b}. functions hai() hbi() operate

set 22 2 interpretation sets A2 shown Table 1.


b
b
b
ab

b

b
a=b
ab
ab
b
ab
ba
>

hai()

b
b
b
ab
>
ab

a=b
b
b
ba
b
ab
ab


hbi()
b
b
b
b
ba
ab
>
a=b



ab
b
ab
ab
b

Table 1: Conjugation functions A2 = {a, b}. Interpretation sets represented using
formulas A2 , connective = denotes exclusive disjunction XOR.
two-valued ADF semantics, conjugation function plays essential semantical
role, since provides bridge models acceptance functions models
ADF. also interesting itself: first show properties conjugation
function associated atom, since used proof later on. First
all, involution, is, inverse (and thus particular bijection). Next,
compatible complement operation (logical negation formula level).
Finally, also preserves evenness cardinality input set.
Proposition 9. Let vocabulary, X 2A A.
1. hai(hai(X)) = X.

(involution)

2. 2A \ hai(X) = hai 2A \ X .


(compatible negation)

3. |X| even iff |hai(X)| even.

(preserves evenness)

Proof. Let |A| = n, X 2A A.
209

fiStrass

1. Let A.
hai(hai(X)) iff hai(X)
iff (M X )
iff X (a )
iff X
2. Denote
S, = {M | X, }
S,/ = {M | X,
/ M}
S,
= {M |
/ X, }
/
S,
/ X,
/ M}
/
/ = {M |
observe
2A = S, ] S,/ ] S,
/ ] S,
/
/
X = S, ] S,/
hai(X) = S, ] S,
/
/
] denotes disjoint union.

2A \ hai(X) = 2A \ S, ] S,
/
/
= S,/ ] S,
/
= {M | X,
/ } ] {M |
/ X, }
fi
fi




= AfiM
/ 2 \ X,
/ ] fi 2A \ X,

= hai 2A \ X
3. show |X| + |hai(X)| even. Firstly,
S,/ ] S,
/ } = 2A\{a}
/
/ = {M |
fi
fi fi
fi
n1 . Thus
fi
whence fiS,/ fi + fiS,
/
/ =2
fi
fi
fi
fi
fi
|X| + |hai(X)| = |S, | + fiS,/ fi + |S, | + fiS,
/
/
fi
fi fi
fi
fi
= 2 |S, | + fiS,/ fi + fiS,
/
/
= 2 |S, | + 2n1
even.



current purpose characterising expressiveness bipolar ADFs,
use concept conjugation make ADF realisability model semantics slightly
accessible. show ADF realisation model set X n-element
vocabulary equivalently characterised n-tuple (Y1 , . . . , Yn ) supersets X
whose intersection exactly X. crux proof result acceptance
conditions realising ADF Yi related conjugation function.
210

fiExpressiveness Two-Valued Semantics ADFs

Proposition 10. Let = {a1 , . . . , } vocabulary X 2A set interpretations. Denote ADF sequence (1 , . . . , n ) acceptance formulas (for
{1, . . . , n}, formula acceptance formula ai ), define
CX = {(mod (1 ), . . . , mod (n )) | su(1 , . . . , n ) = X}
fi
(
!
)
n
fi
\
fi
YX = (Y1 , . . . , Yn ) fi Y1 , . . . , Yn 2A ,
Yi = X
fi
i=1

sets CX YX one-to-one correspondence; particular |CX | = |YX |.
Proof. provide bijection CX YX . Consider function
n
n
(B1 , . . . , Bn ) 7 (ha1 i(B1 ) , . . . , han i(Bn ))
22
f : 22
involution Proposition 9. Using results Section 2.4.2, get
(mod (1 ), . . . , mod (n )) CX iff su(1 , . . . , n ) = X


^
iff mod
(ai ) = X
1in

iff

\

mod (ai ) = X

1in

iff

\

hai i(mod (i )) = X

1in

iff (ha1 i(mod (1 )) , . . . , han i(mod (n ))) YX
iff f (mod (1 ), . . . , mod (n )) YX
Thus f (CX ) = YX whence f (YX ) = f (f (CX )) = CX f |CX : CX YX bijective.



one-to-one correspondence important since later analyse precise
number realisations given model sets. Furthermore, result shows role
conjugation function characterising two-valued model realisability general ADFs.
adapt characterisation result case bipolar ADFs. precisely,
give several necessary sufficient conditions given model set bipolarly realisable.
characterisation hand, later show specific interpretation set fails
necessary conditions thus cannot model set BADF.
Below, fiwe denote

set supersets set X interpretation sets X = 2A fi X .
Proposition 11. Let = {a1 , . . . , } vocabulary X 2A set interpretations. following equivalent:
1. X bipolarly realisable.
2. exist Y1 , . . . , Yn X that:

(a) ( ni=1 Yi ) = X,
211

fiStrass

(b) 1 n, set hai i(Yi ) bipolar.
3. exist Y1 , . . . , Yn X

(a) ( ni=1 Yi ) = X,
(b) 1 i, j n, least one :
A, (M Yi ai ) (M {aj } Yi ai {aj });
N A, (N Yi = ai N ) (N {aj } Yi = ai N {aj }).
Proof. (1) (2): X bipolarly realisable, exists bipolar ADF = (A, L, C)
su(D) = X. particular, exist bipolar Boolean functions C1 , . . . , Cn
X 1 n find ai iff Ci (M ) = t.
1 n define YT
= hai i(Ci ). assumption, hai i(Yi ) = hai i(hai i(Ci )) = Ci bipolar; furthermore ( ni=1 Yi ) = X follows above.
(2) (3): Let {1, . . . , n} assume hai i(Yi ) bipolar. means
aj A, find aj supporting attacking (or both) hai i(Yi ). aj
supporting haj i(Yi ) iff find:
hai i(Yi ) {aj } hai i(Yi ) , is,
(M Yi ai ) (M {aj } Yi ai {aj })
Similarly, aj attacking hai i(Yi ) iff N find:
N
/ hai i(Yi ) N {aj }
/ hai i(Yi ) , is,
(N Yi ai N ) (N {aj } Yi ai N {aj })
Thus aj A, find least one following:
A, (M Yi ai ) (M {aj } Yi ai {aj });
N A, (N Yi = ai N ) (N {aj } Yi = ai N {aj }).
(3) (1): construct ADF = (A, L, C) follows: {1, . . . , n} define Ci = hai i(Yi ) finally set L = A. Ci bipolar equivalences
established previous proof item, su(D)
= X follows fact
hai i(Ci ) = hai i(hai i(Yi )) = Yi presumption ( ni=1 Yi ) = X.


apply characterisation result show interpretation set
three atoms cannot realised bipolar ADFs model semantics.
smallest example terms number atoms (actually, one two smallest
examples) interpretation sets binary vocabulary bipolarly realisable.
Proposition 12. vocabulary A3 = {1, 2, 3}, bipolar ADF realises
X = Even3 = {, {1, 2} , {1, 3} , {2, 3}}.
212

fiExpressiveness Two-Valued Semantics ADFs

Proof. Assume contrary X bipolarly realisable. exist Y1 , Y2 , Y3 X

Proposition 11. 2|2 ||X| = 284 = 24 = 16 candidates Yi , is,
every Yi must form X ] Z
Z {{1} , {2} , {3} , {1, 2, 3}} = 2A \ X
eleven sixteen model set candidates Yi , set hii(Yi )
bipolar. show model set hii(Yi ) bipolar, provide statement j A3
neither supporting attacking; say statement dependent.
1. Y1 = X, get h1i(Y1 ) = {{1, 2} , {1, 3} , {2} , {3}}, bipolar since
statement 2 dependent: 2 supporting, {3} h1i(Y1 ) would imply
{2, 3} h1i(Y1 ); 2 attacking,
/ h1i(Y1 ) would imply {2}
/ h1i(Y1 ).
remaining cases, justifications specific statement dependent
equally easy read model set; brevity indicate statements.
2. Y1 = X {{1}}, get h1i(Y1 ) = {{1, 2} , {1, 3} , {1} , {2} , {3}},
bipolar since statement 2 dependent.
3. Y1 = X {{2}}, get h1i(Y1 ) = {{1, 2} , {1, 3} , {3}}, bipolar since
statement 2 dependent.
4. case Y1 = X {{3}} symmetric previous one: get model set
h1i(Y1 ) = {{1, 2} , {1, 3} , {2}}, bipolar since statement 3 dependent.
5. Y1 = X {{1, 2, 3}}, get h1i(Y1 ) = {{1, 2, 3} , {1, 2} , {1, 3} , {2} , {3}},
bipolar since statement 2 dependent.
6. Y1 = X {{1} , {2}}, get h1i(Y1 ) = {{1, 2} , {1, 3} , {1} , {3}},
bipolar since statement 3 dependent.
7. case Y1 = X {{1} , {3}} symmetric previous one.
8. Y1 = X {{2} , {3}}, get h1i(Y1 ) = {{1, 2} , {1, 3}}, bipolar since
statement 2 dependent.
9. Y1 = X {{1} , {1, 2, 3}}, get h1i(Y1 ) = {{1, 2, 3} , {1, 2} , {1, 3} , {1} , {2} , {3}},
bipolar since statement 2 dependent.
10. Y1 = X {{2} , {1, 2, 3}}, get h1i(Y1 ) = {{1, 2, 3} , {1, 2} , {1, 3} , {3}},
bipolar since statement 2 dependent.
11. Y1 = X {{3} , {1, 2, 3}} symmetric previous case.
remains set C five candidates (due symmetry i):
C = {X ] {{1} , {2} , {3}} ,
X ] {{1} , {2} , {1, 2, 3}} ,
X ] {{1} , {3} , {1, 2, 3}} ,
X ] {{2} , {3} , {1, 2, 3}} ,
X ] {{1} , {2} , {3} , {1, 2, 3}}}
213

fiStrass

Basically, candidates least three four interpretations
= {{1} , {2} , {3} , {1, 2, 3}} contained addition already X. clearly
assumption Yi realise X
1 , Y2 , Y3
C.
T3
Yi 1 3 thus
i=1 Yi = X. However, X = .
Contradiction. Thus Yi exist X bipolarly realisable.

interpretation set A3 bipolarly realisable, found
complement Even3 above, Parity language three atoms.
Proposition 13. vocabulary A3 = {1, 2, 3}, bipolar ADF realises
Parity3 = {{1} , {2} , {3} , {1, 2, 3}}.
Together straightforward statement fact Even3 realised
non-bipolar ADF, Proposition 12 leads next result.
Theorem 14. BADFsu <e ADFsu
Proof. Model set Even3 Proposition 12 realisable model semantics ADF
DEven3 acceptance conditions
1 = (2 = 3),

2 = (1 = 3),

3 = (1 = 2)

However, bipolar ADF realising Even3 , witnessed Proposition 12.



Another consequence characterisation two-valued model realisability Proposition 10 get precise number distinct realisations given model set.
significant illustrates rather intricate difficulty underlying bipolar non-realisability: cannot necessarily use model set Even3 determine
single reason bipolar non-realisability, is, single link (b, a) neither supporting attacking realisations. Rather, culprit(s) might different
realisation, show bipolar non-realisability, prove realisations,
necessarily exists reason non-bipolarity. number different ADF
realisations given model set X considerable.8

Proposition
fi
fi 15. Let vocabulary |A| = n, X 2 interpretation set
fi2 \ X fi = m. number distinct ADFs su(D) = X

r(n, m) = (2n 1)m
Proof. According Proposition
10,T realisation
tufi
fi X characterised
n
n tuples.
ple (Y1 , . . . , Yn ) X X = ni=1 Yi . Since fiX fi = 2m , (2m )T
However,Ttowards r(n, m), wrongly counts tuples (Y1 , . . . , Yn ) ( ni=1 Yi ) ) X,
is, |( ni=1 Yi ) \ X| > 0 (at least once); remains subtract
{1, . . . , n},
n them.
overestimate number tuples (Y1 , . . . , Yn ) X |( ni=1 Yi ) \ X|
expression

n

q(n, m, i) =
2mi
(3)

8. counting ADFs A, take account different link relations, take L =
count different acceptance functions, redundant links modelled.

214

fiExpressiveness Two-Valued Semantics ADFs


seen follows: Let 2A \ X fixed i-element set. (Intuitively, interpretation-set X contains
interpretations many.)
mi sets.
fi exactly
fi

n
I, fiI fi = 2mi . Thus 2mi possible ways choose n
elements (the Y1 , . . . , Yn ) . matter Yj chosen, intersection
contains thus least elements many. However, sets least
+ 1 elements many counted twice subtracted. subtract
q(n, m, + 1), counted sets least + 2 elements many
add q(n, m, + 2),
inclusion-exclusion principle, number
n etc. Hence

tuples (Y1 , . . . , Yn ) X ni=1 Yi = X given
r(n, m) = q(n, m, 0) q(n, m, 1) + q(n, m, 2) . . . q(n, m, m)

X
=
(1)i q(n, m, i)
i=0

X


n

=
(1)
2mi

i=0



X
=
(2n )mi (1)i



(by (3) above)
(reordering factors)

i=0
n

= (2 1)m

(binomial theorem)

main contributing factor number interpretations excluded
desired model set X. Proposition 12, instance, (23 1)4 = 74 = 2401
ADFs model set Even3 . According Theorem 14, none bipolar. Obvin
ously, maximal number realisations achieved X = whence r(n, 2n ) = (2n 1)2 .
hand, model set X = 2A exactly one realisation, r(n, 0) = 1. Note
number (syntactically distinct) realisations universally expressive
formalisms, logic programs propositional logic, unbounded general since
add arbitrary number tautologies.
finally show reduction problem bipolar realisability propositional
satisfiability. approaches problem another angle (a possible implementation
deciding bipolar realisability using SAT solver), provides proof Theorem 3
Strass (2015b), contained work.
given vocabulary set X 2A set interpretations, aim
construct propositional formula X satisfiable X bipolarly
realisable. propositional signature use following: A,
propositional variable pM
expresses whether Ca (M ) = t. allows
encode possible acceptance conditions statements A. enforce bipolarity,
use additional variables model supporting attacking links: a, b A,
a,b
variable pa,b
sup saying supports b, variable patt saying attacks b.
vocabulary X given
fi
n

a,b fi
a,b
P = pM
,
p
,
p


A,


A,
b


fi

sup att
guarantee desired set models, constrain acceptance conditions dictated
X: desired set statement a, containment must correspond
215

fiStrass

exactly whether Ca (M ) = t; encoded
X . Conversely, undesired set
/
statement a, must correspondence,
X expresses.
enforce bipolarity, state link must supporting attacking. model
meaning support attack, encode ground instances definitions.
Definition 6. Let vocabulary X 2A set interpretations. Define
following propositional formulas:

/
BADF
=
X
X X bipolar


^
^
^


pM

pM


X =
X



aA\M




^

/

X =

_


A,M X
/

bipolar =

^

_

pM





pM


aA\M



a,b
a,b
a,b
pa,b
sup patt sup att



a,bA

a,b
sup

= pa,b
sup

a,b
att

pa,b
att

^



(a, b A)



(a, b A)

{a}

pM
b pb



=



^

{a}

pb

pM
b



corresponding result shows reduction correct.
Theorem 16. Let vocabulary X 2A set interpretations. X bipolarly
realisable BADF
satisfiable.
X
Proof. if: Let P model X . A, define acceptance condition follows: A, set Ca (M ) = iff pM
I. easy see bipolar
guarantees acceptance conditions bipolar. ADF given
su = (A, A, C). remains show model su
DX
X
X.
su . Consider A.
if: Let X. show model DX

1. . Since model
X , pa thus definition
Ca (M ) = t.
/ thus definition
2. \ . Since model
X , pa
Ca (M ) = f .
/
if: Let
/ X. Since model
X ,
Ca (M ) = f
/ Ca (M ) = t. case, model
su .
DX

if: Let bipolar ADF su(D) = X. use define model X .
First, A, set pM
iff Ca (M ) = t. Since bipolar, link
supporting attacking a, b find valuation pa,b
sup
pa,b
.

remains

show




model


.
X
att
216

fiExpressiveness Two-Valued Semantics ADFs

1. model
X : Since realises X, X model thus
Ca (M ) = iff .
/
2. model
/ X model
X : Since realises X,
D. Thus , witnessing model
D: (1) Ca (M ) = f , (2)
/ Ca (M ) = t.

3. model bipolar : straightforward since bipolar assumption.



Remarkably, decision procedure give answer, case
positive answer read BADF realisation satisfying evaluation
constructed formula. illustrate construction example seen earlier.
Example 2. Consider A3 = {1, 2, 3} model set Even3 = {, {1, 2} , {1, 3} , {2, 3}}.
construction Theorem 16 yields formulas:
{1}





Even3 = p1 p2 p3
{1,2}

p2

{1,3}

p2

p1
p1

{1,2}

{2,3}

p1

/

Even3 = (p1
{1,2}



(p1

{1,3}



(p1

p3

{1,3}

p3

{2,3}

p3

p2

{2,3}

{1}

p3 )

{2}

p2

{1}

{2}

p2

p3 )

{3}

p2

{3}

p3 )

{1,2,3}

(p1

{2}

{3}

{1,2,3}

p2

{1,2,3}

p3

)

remaining formulas bipolarity independent Even3 , show
here. implemented translation proof Theorem 16 used solver
clasp (Gebser, Kaminski, Kaufmann, Ostrowski, Schaub, & Schneider, 2011) verify
Even3 unsatisfiable.
3.1.4 BADF vs. LP
Earlier, used language Parity show propositional logic (and thus
PL
=s ADFsu general ADFs are) exponentially succinct normal logic programs
(under supported models). However, bipolar ADFs, Proposition 13 BADF
A3 = {1, 2, 3} model set su(D) = Parity3 = {{1} , {2} , {3} , {1, 2, 3}}, is,
BADFs cannot even express Parity. Fortunately, Majority language trick
case.
Theorem 17. BADFsu 6s LPsu
Proof. show language Majority polynomially expressed BADFsu ,
LPsu . latter fact follows Majority
/ AC0 (Jukna, 2012) Proposition 6. show first part constructing series BADFs Dn = {a1 , . . . , }
(n N, n 1) su(Dn ) = Majorityn . use results (Friedman, 1986; Boppana, 1986), show positive n N k n, language Thresholdn,k
negation-free propositional formulas Threshold
polynomial size s, use
n,k


4.27
bound Boppana, k n log n . Define D1 a1 = >, n 2 set k = n2
1 n,
ai = ai Threshold
(a1 , . . . , ai1 , ai+1 , . . . , )
n1,k
217

fiStrass

Intuitively, formula ai checks whether remaining variables could achieve majority
without ai . so, ai set arbitrarily; otherwise, ai must set true. Clearly
Boolean function computed ai bipolar, since ai supporting parents
attacking. size Dn , observe


kDn k n Threshold
n1,k

whence overall size polynomial. remains show su(Dn ) = Majorityn .
: Let Majorityn . show su(Dn ), is, iff |=
. , immediate |= , let aj
/
j {1, .. . ,n}. show 6|= aj . Since Majorityn , |M | =
k = n2 n 1 Thresholdn1,k , is,
|= Threshold
(a1 , . . . , aj1 , aj+1 , . . . , )
n1,k
Together 6|= aj , follows 6|= aj .

: Let
/ Majorityn . |M | = 0 < n2 = k. particular,
aj \ . < k implies N Thresholdn1,k
(a1 , . . . , aj1 , aj+1 , . . . , ) whence follows
|N | = = |M |. Thus 6|= Threshold
n1,k
|= aj . Together 6|= aj conclude
/ su(Dn ).

Since every BADF ADF size, get:
Corollary 18. ADFsu 6s LPsu
combination translation logic programs ADFs (implying relation
LPsu ADFsu ), means also ADFs strictly succinct logic programs.
Corollary 19. LPsu <s ADFsu
3.1.5 BADF vs. AF
comparably easy show BADF models strictly expressive AFs,
since sets supported models bipolar ADFs antichain property.
Proposition 20. AF <e BADFsu
Proof. Consider vocabulary = {a} BADF = (A, {(a, a)} , {a }) = a.
straightforward check model set su(D) = {, {a}}. Since model sets AFs
stable extension semantics satisfy antichain property, equivalent AF
A.

yields following overall relationships:
AF <e BADFsu <e ADFsu
=e LPsu
=e PL
concise overview relative succinctness, present results open problems
glance Table 2 below.9
9. remark three open problems Table 2 really two: easy show ADFs
propositional logic behave equivalently relation bipolar ADFs, since equally expressive
equally succinct; is, holds ADFsu BADFsu PL BADFsu .

218

fiExpressiveness Two-Valued Semantics ADFs

su

BADF
ADFsu
LPsu
PL

BADFsu
=
?
?
?

ADFsu

=
<s

=s

LPsu
6s
6s
=
6s

PL


=s
<s
=

Table 2: Relative succinctness results (bipolar) ADFs model semantics, normal
logic programs supported semantics, classical propositional logic. entry
row F1 column F2 means F1 F2 .
3.2 Stable Semantics
before, recall current state knowledge:
AF e BADFst e ADFst <e PL AF e LPst <e PL
first show BADFs strictly expressive AFs.
Proposition 21. AF <e BADFst
Proof. Consider set X2 = {{a, b} , {a, c} , {b, c}} desired models. Dunne et al. (2015)
proved X2 realisable stable AF semantics. However, model set X2
realisable BADF DX2 stable semantics:
= b c,

b = c,

c = b

Let us exemplarily show = {a, b} stable model (the cases completely
symmetric): reduct DM characterised two acceptance formulas = b
b = . easily find DM (, ) = (M, ) = DM (M, ).

Intuitively, argument AF non-realisability X2 follows: Since b occur
extension together, attack them. holds
pairs a, c b, c. set {a, b, c} conflict-free thus must stable
extension containing three arguments, allowed X2 . reason AFs
restriction individual attack, set attack (also called joint collective attack) suffices
realise X2 seen above.
construction used proof realize X2 comes work
Eiter, Fink, Puhrer, Tompits, Woltran (2013) logic programming,
generalised realise non-empty model set satisfying antichain property.
st = (A, L, C) C
Definition 7. Let X 2A . Define following BADF DX

given


_
^

=
b
X,aM

bA\M

thus L = {(b, a) | X, M, b \ }.
219

fiStrass

next result shows construction indeed works.
st ) = X.
Theorem 22. Let X =
6 X 2A -antichain. find st(DX

Proof. Let A.
st ) st(D st ); use case distinction.
: Let
/ X. show
/ su(DX
X

1. N X ( N . N \ . Consider
acceptance
V formula . Since N N X, formula disjunct
a,N = bA\N b. N implies \ N \ model
st ).
a,N . Thus model although
/ , hence
/ su(DX
2. N X, 6 N . X 6= implies 6= , let .

V N X N , acceptance formula contains disjunct
a,N = bA\N b. assumption, N X bN \ N .
Clearly bN \ N bN evaluated true . Hence N X
N , disjunct a,N evaluated false . Thus false
st ).

/ su(DX
st ), is: A, find
: Let X. first show su(DX
iff model .
st
1. Let .
V construction, DX contains disjunct
form a,M = bA\M b. According interpretation , b \
false thus a,M true whence true.

2. Let \ consider acceptance formula . Assume contrary
model V
. N X N
model a,N = bA\N b, is, \ N \ . Hence N ; and,
since N \ , even ( N , whence X -antichain. Contradiction.
Thus model .
st respect . There, contains
consider reduct DM DX


disjunct a,M = a,M [b/ : b
/ ] b \ replaced false,
= . . . equivalent true. Thus true
whence a,M

st ).
least fixpoint DM thus st(DX


restriction non-empty model sets immaterial relative expressiveness, since
use construction Theorem 2 fact st(D) su(D) ADF
realize empty model set. stable model semantics ADFs logic programs
antichain property, get:
Corollary 23. ADFst e BADFst LPst e BADFst
leads following overall relationships:
AF <e BADFst
=e ADFst
=e LPst <e PL
remark antichain property provides characterisation realisability
stable semantics; is, model set stable-realisable iff -antichain.
220

fiExpressiveness Two-Valued Semantics ADFs

3.3 Supported vs. Stable Semantics
put supported stable pictures together. proof Theorem 22,
st antichain X, supported
read canonical realisation DX
st ) = st(D st ) = X. observation, also
stable semantics coincide, is, su(DX
X
bipolar ADFs supported semantics realize antichain, this:
Proposition 24. BADFst e BADFsu
seen Proposition 20, bipolar ADFs supported-model sets
antichains. get:
Corollary 25. BADFst <e BADFsu
result allows us close last gap put together big picture relative
expressiveness Figure 2 below.
ADFsu
=e PL
=e LPsu
BADFsu
BADFst
=e LPst
=e ADFst
AF
Figure 2: expressiveness hierarchy. Expressiveness strictly increases bottom
top. F denotes formalism F semantics , su supported st
stable model semantics; formalisms among AFs (argumentation frameworks), ADFs
(abstract dialectical frameworks), BADFs (bipolar ADFs), LPs (normal logic programs)
PL (propositional logic).

4. Allowing Vocabulary Expansion
here, considered compact realisations, introduce new vocabulary elements. section, allow introduction small number new
atoms/arguments/statements. precisely, small means number linear
size source knowledge base (representing model set wish realize
target language). purpose realisability, new vocabulary elements projected
resulting models.
turns out, adding additional arguments already makes AFs universally expressive
(under projection). technically, show propositional formula
vocabulary A, exists AF F expanded vocabulary
models stable extensions F correspond one-to-one. Roughly,
possible since AFs regarded syntactic variant classical propositional
logic connective logical (Gabbay, 2011; Brewka et al.,
2011). Using connective, negation expressed = disjunction
221

fiStrass

= ( ) = ( ) ( ). equivalences used translate arbitrary propositional formulas (over , , ) syntactical -fragment; guarantee
size increase linear, introduce names subformulas (Tseitin,
1968). next definition combines ideas.
Definition 8. Let formula using , , vocabulary A. Define sets
R inductively follows:
A> = {a> }

R> =

= {a }

R = {(a , )}

Ap = {p, ap } p

Rp = {(p, ap ), (ap , p)} p

= {a }

R = {(a , )} R

= {a , , } R = {(a , ), (a , )} R R
= {a , }

R = {(a , ), (a , ), (a , )} R R

AF associated given F = (A , R {(a , )} R ).
argument a> unattacked thus part every stable extension (is true every
interpretation); argument attacks thus cannot part stable extension (is false every interpretation). mutually attacking arguments p ap
p serve guess valuation A, guarantee (and all)
valuations models lead stable extensions F : intuitively, must
attacked, candidate . arguments attacks Boolean
connectives express usual truth-theoretic semantics, first technical result
translation shows.
Lemma 26. Let formula vocabulary F associated AF.
stable extension F , , have:
iff
/ M;
iff ;
iff one ;
iff neither .
Proof.
definition, attacker argument form argument
. Thus iff
/ M.
attackers arguments . case above,
iff
/ , iff
/ . Consequently, iff

/
/ iff .
attacker argument . Similarly previous cases,
show iff
/
/ , iff
/ .
combination, iff .
222

fiExpressiveness Two-Valued Semantics ADFs

attackers arguments . directly follows
iff neither .

correspondences used show induction newly introduced
arguments capture semantics formulas encode (for subformulas ).
Lemma 27. Let formula F associated AF. stable extension
F , iff model .
Proof. Let stable extension F . use structural induction .
= >: Trivial: a> since attackers.
= : Trivial:
/ since set {a } conflict-free.
= p A: Trivial: p iff |= p definition.
= : iff iff
/ iff 6|= iff |= iff |= .
= : iff iff iff |= |= iff |=
iff |= .
= : iff iff iff |= |= iff |=
iff |= .
= : iff iff
/
/ iff 6|= 6|= iff |=
iff |= .

lets us show main result section, namely AF stable extension
semantics universally expressive projection.
Theorem 28. Let formula vocabulary F associated AF.
1. model , exists stable extension E F = E A.
2. stable extension E F , set E model .
Proof.

1. Let model . Define set
E = {a | , |= }

Observe = E A. presumption, E. remains show E
stable extension, is, E conflict-free attacks arguments b
/ E.
E conflict-free: Assume contrary attack r = (a, b) R
a, b E. definition, cases:
arbitrary b = . definition E get |= ,
contradiction.
r = (p, ap ) r = (ap , p) p A. definition E get
|= p |= p, contradiction.
223

fiStrass

r = (a , ). definition E get |= |= ,
contradiction.
r = (a , ) r = (a , ). |= , |= |= ,
contradiction.
r = (a , ). |= , whence |= ( ). also |= ,
contradiction.
r = (a , ) r = (a , ). |= , |= |= .
also |= , contradiction.
E attacks arguments E: Let b (A {a }) \ E argument.
definition, formula b = 6|= . use structural
induction.
= E attacks definition.
= , |= whence E attacks definition.
= , |= |= whence E E.
case, E attacks definition.
= , |= whence E attacks definition.
= , |= whence E E.
case, E attacks definition.
2. Let E stable extension F . Since E conflict-free,
/ E. Since E stable,
E attacks , yields E. Lemma 27, E model .

particular, F stable extension iff unsatisfiable. shows
construction Definition 8 works intended, remains show number new
arguments linear formula size. even show total increase
size linear, thus also number new arguments linear.
Proposition 29. formula , find kF k O(kk).
Proof. first note
kF k = k(A , R {(a , )} R )k
= |A | + |R {(a , )} R |
= |A | + 1 + |R | + 2
= |A | + |R | + 3
use structural induction show formulas , find |A | 5 kk
|R | 4 kk. follows kF k (5 + 4) kk + 3 = 9 kk + 3 O(kk).
= >:
|A> | = |{a> }| = 1 5 = 5 k>k
|R> | = || = 0 4 = 4 k>k
224

fiExpressiveness Two-Valued Semantics ADFs

= :
|A | = |{a }| = 1 5 = 5 kk
|R | = |{(a , )}| = 1 4 = 4 kk
= A:
|Aa | = |{a, aa }| = 2 5 = 5 kak
|Ra | = |{(a, aa ), (aa , a)}| = 2 4 = 4 kak
= :
|A | = |A {a }| |A | + 1 (5 kk) + 1 5 (kk + 1) = 5 kk
|R | = |R {(a , )}| |R | + 1 (4 kk) + 1 4 (kk + 1) = 4 kk
= :
|A | |A | + |A | + 3 (|A | + 1) + (|A | + 1) + 3
(5 kk + 1) + (5 kk + 1) + 3 = 5 kk + 5 kk + 5
= 5 (kk + kk + 1) = 5 k k
|R | |R | + |R | + 2 (|R | + 1) + (|R | + 1) + 2
(4 kk + 1) + (4 kk + 1) + 2 = 4 kk + 4 kk + 4
= 4 (kk + kk + 1) = 4 kk
= :
|A | |A | + |A | + 2 5 kk + 5 kk + 2
5 kk + 5 kk + 5 = 5 (kk + kk + 1) = 5 kk
|R | |R | + |R | + 3 (4 kk) + (4 kk) + 3
4 kk + 4 kk + 4 = 4 (kk + kk + 1) = 4 kk



Hence projection, AF stable extension semantics realise much propositional logic can. results previous section (AF e PL), means
allowing introduce linear number new vocabulary elements (that later projected
out), languages considered paper equally (universally) expressive.
However, must note equal expressiveness mean equal efficiency:
assume knowledge base size n leads search space size O(2n ),
linear increase knowledge base size (that is, n c n constant c) leads
polynomial increase search space size (that is, O(2n ) O(2cn ) = O((2n )c ).
225

fiStrass

5. Discussion
compared expressiveness abstract argumentation frameworks, abstract dialectical
frameworks, normal logic programs propositional logic. showed expressiveness
different semantics varies formalisms obtained neat expressiveness hierarchy. results inform us capabilities languages encode sets
two-valued interpretations, help us decide languages use specific applications. Furthermore, seen results sensitive vocabulary one
permitted use, hierarchy collapses allow introduce even linear
number new atoms.
Concerning succinctness, shown ADFs (under model semantics) exponentially succinct normal logic programs (under supported model semantics),
even bipolar ADFs (under model semantics) although less expressive
succinctly express model sets equivalent normal logic programs (under supported model semantics) vocabulary must necessarily blow exponentially
size. open whether converse direction also holds, is, whether BADFs
exponentially succinct logic programs (if LPsu BADFsu ) two
mutually incomparable terms succinctness (if LPsu 6s BADFsu ). stable semantics, relative succinctness logic programs BADFs completely open, partly due
technical aspect two stable semantics conceptually different, ADFs
fact employ ultimate stable models (Denecker et al., 2004; Brewka et al., 2013; Strass
& Wallner, 2015). Furthermore, general ADFs, computational complexity
model existence problem stable semantics higher normal logic programs,10
succinctness comparison regard stable models would limited significance.
easy see AFs somewhat special role representationally
succinct case: vocabulary , syntactically possibility specify knowledge base (an AF) exponential size, since largest AF size
k(An , )k = n + n2 thus polynomially large. anything expressed AF expressed reasonable space definition. However,
strength AFs taken grain salt, since comparably inexpressive. (in addition results presented) already seen simple
counting argument: even syntactically different AFs semantically differ2
ent (which not), could express 2n different model sets,
n
increasing n negligible relation 22 possible model sets .
original paper, Gogic et al. (1995) also used relaxed version succinctness,
allowed introduce linear number new variables. follows results
Section 4 formalisms consider equally succinct relaxed
notion.
Parts expressiveness results normal logic programs carry LP
classes. example, canonical logic programs provide limited form nesting allowing
literals form rule bodies (Lifschitz et al., 1999). makes quite easy
see normal logic programs supported semantics translated equivalent
canonical logic programs, namely replacing positive body atom
10. P
2 -hard ADFs (Strass & Wallner, 2015) opposed NP normal LPs (Bidoit & Froidevaux,
1991; Marek & Truszczynski, 1991).

226

fiExpressiveness Two-Valued Semantics ADFs

rule bodies. Recently, Shen Zhao (2014) showed canonical logic programs
propositional logic programs succinctly incomparable (under assumption11 ), also
provide interesting avenues succinctness studies. also add succinctness
questions own: firstly comparing disjunctive logic programs stable
models general ADFs stable models, since two equally complex
(P2 -complete) model existence problem (Eiter & Gottlob, 1995; Brewka et al., 2013).
more, alternative proposals stable model semantics ADFs:
(Strass, 2013) (Definition 3.2, later called approximate stable models Strass
& Wallner, 2015), model existence NP-complete (Strass & Wallner, 2015)
thus potentially easier stable models Brewka et al. (2013)
(called ultimate stable models Strass & Wallner, 2015);
grounded model semantics Bogaerts, Vennekens, Denecker (2015) (Definition 6.8), whose model existence problem also P2 -complete (Bogaerts et al.,
2015);
F-stable model semantics Alviano Faber (2015) (Definition 10).
follows Theorem 5.9 Bogaerts et al. (2015) grounded models F-stable
models coincide. Still, demonstrably different approximate ultimate stable models ADFs (Alviano & Faber, 2015),12 relative succinctness
comparison normal/disjunctive logic programs unanalysed.
potential work. First all, nice characterisation bipolar
ADF realisability still missing; unsure whether much improvement Proposition 11 possible. Incidentally, AFs exact characterisation compact stable extension realisability constitutes major open problem (Dunne et al., 2015; Baumann et al.,
2014). Second, semantics abstract dialectical frameworks whose expressiveness could studied; Dunne et al. (2015) Dyrkolbotn (2014) already analyse many
argumentation frameworks. work thus start
done remaining semantics. example admissible, complete preferred
semantics defined AFs, (B)ADFs LPs (Strass, 2013; Brewka et al., 2013),
Puhrer (2015) already made huge step direction characterising realisability. Third, formalisms abstract argumentation (Brewka et al., 2014)
whose expressiveness large unexplored best knowledge. Finally,
representational succinctness subclass bipolar ADFs (using bipolar propositional
formulas represent them) supported model semantics mostly open (cf. Table 2),
evidence pointing toward meaningful capabilities.

Acknowledgements
paper combines, extends improves results previous work (Strass, 2014,
2015b, 2015c). wish thank Stefan Woltran providing useful pointer related
1
11. P 6 NC/poly
, Boolean circuit equivalent assumption NP 6 P.
12. terminology Alviano Faber (2015), approximate stable models (Strass, 2013) called
S-stable models ultimate stable models (Brewka et al., 2013) called B-stable models.
shown different F-stable models.

227

fiStrass

work realisability logic programming, Bart Bogaerts pointing grounded
models F-stable models same, Jorg Puhrer several suggestions improvement manuscript, Frank Loebe helpful discussions. research partially
supported Deutsche Forschungsgemeinschaft (DFG, project BR 1817/7-1).

References
Al-Abdulkarim, L., Atkinson, K., & Bench-Capon, T. J. M. (2014). Abstract dialectical
frameworks legal reasoning. Hoekstra, R. (Ed.), Proceedings TwentySeventh Annual Conference Legal Knowledge Information Systems (JURIX),
Vol. 271 Frontiers Artificial Intelligence Applications, pp. 6170. IOS Press.
Al-Abdulkarim, L., Atkinson, K., & Bench-Capon, T. J. M. (2015). Evaluating approach
reasoning cases using abstract dialectical frameworks. Proceedings
Fifteenth International Conference Artificial Intelligence Law (ICAIL).
Alviano, M., & Faber, W. (2015). Stable model semantics abstract dialectical frameworks revisited: logic programming perspective. Yang, Q., & Wooldridge, M.
(Eds.), Proceedings Twenty-Fourth International Joint Conference Artificial
Intelligence (IJCAI), pp. 26842690, Buenos Aires, Argentina. IJCAI/AAAI.
Arora, S., & Barak, B. (2009). Computational Complexity: Modern Approach. Cambridge
University Press.
Baumann, R., Dvorak, W., Linsbichler, T., Strass, H., & Woltran, S. (2014). Compact
argumentation frameworks. Proceedings Twenty-First European Conference
Artificial Intelligence (ECAI), pp. 6974, Prague, Czech Republic.
Bidoit, N., & Froidevaux, C. (1991). Negation default unstratifiable logic programs.
Theoretical Computer Science, 78 (1), 85112.
Bogaerts, B., Vennekens, J., & Denecker, M. (2015). Grounded fixpoints applications knowledge representation. Artificial Intelligence, 224, 5171.
Boppana, R. B. (1986). Threshold functions bounded depth monotone circuits. Journal
Computer System Sciences, 32 (2), 222229.
Brewka, G., Dunne, P. E., & Woltran, S. (2011). Relating semantics abstract dialectical frameworks standard AFs. Proceedings Twenty-Second International
Joint Conference Artificial Intelligence (IJCAI), pp. 780785. IJCAI/AAAI.
Brewka, G., Ellmauthaler, S., Strass, H., Wallner, J. P., & Woltran, S. (2013). Abstract
dialectical frameworks revisited. Proceedings Twenty-Third International
Joint Conference Artificial Intelligence (IJCAI), pp. 803809. IJCAI/AAAI.
Brewka, G., & Gordon, T. F. (2010). Carneades abstract dialectical frameworks: reconstruction. Proceedings Third International Conference Computational
Models Argument (COMMA), Vol. 216 FAIA, pp. 312. IOS Press.
Brewka, G., Polberg, S., & Woltran, S. (2014). Generalizations Dung frameworks
role formal argumentation. IEEE Intelligent Systems, 29 (1), 3038. Special
Issue Representation Reasoning.
228

fiExpressiveness Two-Valued Semantics ADFs

Brewka, G., & Woltran, S. (2010). Abstract dialectical frameworks. Proceedings
Twelfth International Conference Principles Knowledge Representation
Reasoning (KR), pp. 102111.
Clark, K. L. (1978). Negation failure. Gallaire, H., & Minker, J. (Eds.), Logic
Data Bases, pp. 293322. Plenum Press.
Coste-Marquis, S., Konieczny, S., Mailly, J.-G., & Marquis, P. (2014). revision
argumentation systems: Minimal change arguments statuses. Proceedings
Fourteenth International Conference Principles Knowledge Representation
Reasoning (KR), pp. 5261.
Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial
Intelligence Research, 17, 229264.
Denecker, M., Marek, V. W., & Truszczynski, M. (2004). Ultimate approximation
application nonmonotonic knowledge representation systems. Information
Computation, 192 (1), 84121.
Dimopoulos, Y., Nebel, B., & Toni, F. (2002). computational complexity
assumption-based argumentation default reasoning. Artificial Intelligence,
141 (1/2), 5778.
Dung, P. M. (1995). Acceptability Arguments Fundamental Role
Nonmonotonic Reasoning, Logic Programming n-Person Games. Artificial Intelligence, 77, 321358.
Dunne, P. E., Dvorak, W., Linsbichler, T., & Woltran, S. (2014). Characteristics multiple
viewpoints abstract argumentation. Proceedings Fourteenth International
Conference Principles Knowledge Representation Reasoning (KR), pp.
7281, Vienna, Austria.
Dunne, P. E., Dvorak, W., Linsbichler, T., & Woltran, S. (2015). Characteristics multiple
viewpoints abstract argumentation. Artificial Intelligence, 228, 153178.
Dyrkolbotn, S. K. (2014). argue anything: Enforcing arbitrary sets labellings
using AFs. Proceedings Fourteenth International Conference Principles
Knowledge Representation Reasoning (KR), pp. 626629, Vienna, Austria.
Eiter, T., Fink, M., Puhrer, J., Tompits, H., & Woltran, S. (2013). Model-based recasting
answer-set programming. Journal Applied Non-Classical Logics, 23 (12), 75104.
Eiter, T., & Gottlob, G. (1995). computational cost disjunctive logic programming:
Propositional case. Annals Mathematics Artificial Intelligence, 15 (34), 289
323.
French, T., van der Hoek, W., Iliev, P., & Kooi, B. (2013). succinctness
modal logics. Artificial Intelligence, 197, 5685.
Friedman, J. (1986). Constructing O(n log n) size monotone formulae k-th elementary symmetric polynomial n Boolean variables. SIAM Journal Computing, 15,
641654.
Gabbay, D. M. (2011). Dungs argumentation essentially equivalent classical propositional logic Peirce-Quine dagger. Logica Universalis, 5 (2), 255318.
229

fiStrass

Gaggl, S. A., & Strass, H. (2014). Decomposing Abstract Dialectical Frameworks. Parsons, S., Oren, N., & Reed, C. (Eds.), Proceedings Fifth International Conference
Computational Models Argument (COMMA), Vol. 266 FAIA, pp. 281292.
IOS Press.
Gaggl, S. A., Rudolph, S., & Strass, H. (2015). computational complexity naivebased semantics abstract dialectical frameworks. Yang, Q., & Wooldridge, M.
(Eds.), Proceedings Twenty-Fourth International Joint Conference Artificial
Intelligence (IJCAI), pp. 29852991, Buenos Aires, Argentina. IJCAI/AAAI.
Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Schneider, M.
(2011). Potassco: Potsdam Answer Set Solving Collection. AI Communications,
24 (2), 105124. Available http://potassco.sourceforge.net.
Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming.
Proceedings International Conference Logic Programming (ICLP), pp.
10701080. MIT Press.
Gogic, G., Kautz, H., Papadimitriou, C., & Selman, B. (1995). comparative linguistics
knowledge representation. Proceedings Fourteenth International Joint
Conference Artificial Intelligence (IJCAI), pp. 862869. Morgan Kaufmann.
Jukna, S. (2012). Boolean Function Complexity: Advances Frontiers, Vol. 27 Algorithms Combinatorics. Springer.
Lifschitz, V., & Razborov, A. (2006). many loop formulas?. ACM Transactions Computational Logic, 7 (2), 261268.
Lifschitz, V., Tang, L. R., & Turner, H. (1999). Nested expressions logic programs. Annals
Mathematics Artificial Intelligence, 25 (34), 369389.
Lin, F., & Zhao, Y. (2004). ASSAT: Computing answer sets logic program SAT
solvers. Artificial Intelligence, 157 (1-2), 115137.
Linsbichler, T. (2014). Splitting abstract dialectical frameworks. Parsons, S., Oren, N., &
Reed, C. (Eds.), Proceedings Fifth International Conference Computational
Models Argument (COMMA), Vol. 266 FAIA, pp. 357368. IOS Press.
Marek, V. W., & Truszczynski, M. (1991). Autoepistemic logic. Journal ACM, 38 (3),
587618.
Osorio, M., Zepeda, C., Nieves, J. C., & Cortes, U. (2005). Inferring acceptable arguments
answer set programming. Proceedings Sixth Mexican International
Conference Computer Science (ENC), pp. 198205.
Polberg, S. (2014). Extension-based semantics abstract dialectical frameworks. Endriss,
U., & Leite, J. (Eds.), Proceedings Seventh European Starting AI Researcher
Symposium (STAIRS), Vol. 264 FAIA, pp. 240249. IOS Press.
Polberg, S., Wallner, J. P., & Woltran, S. (2013). Admissibility abstract dialectical
framework. Leite, J., Son, T. C., Torroni, P., van der Torre, L., & Woltran, S.
(Eds.), Proceedings Fourteenth International Workshop Computational Logic
Multi-Agent Systems (CLIMA XIV), Vol. 8143 LNAI, pp. 102118. Springer.
230

fiExpressiveness Two-Valued Semantics ADFs

Puhrer, J. (2015). Realizability three-valued semantics abstract dialectical frameworks. Yang, Q., & Wooldridge, M. (Eds.), Proceedings Twenty-Fourth
International Joint Conference Artificial Intelligence (IJCAI), pp. 31713177. IJCAI/AAAI, Buenos Aires, Argentina.
Shen, Y., & Zhao, X. (2014). Canonical logic programs succinctly incomparable
propositional formulas. Proceedings Fourteenth International Conference
Principles Knowledge Representation Reasoning (KR), pp. 665668,
Vienna, Austria.
Strass, H. (2013). Approximating operators semantics abstract dialectical frameworks. Artificial Intelligence, 205, 3970.
Strass, H. (2014). relative expressiveness argumentation frameworks, normal
logic programs abstract dialectical frameworks. Konieczny, S., & Tompits,
H. (Eds.), Proceedings Fifteenth International Workshop Non-Monotonic
Reasoning (NMR).
Strass, H. (2015a). Instantiating rule-based defeasible theories abstract dialectical frameworks beyond. Journal Logic Computation, Advance Access published 11
February 2015, http://dx.doi.org/10.1093/logcom/exv004.
Strass, H. (2015b). relative expressiveness abstract argumentation logic programming. Proceedings Twenty-Ninth AAAI Conference Artificial Intelligence
(AAAI), pp. 16251631, Austin, TX, USA.
Strass, H. (2015c). Representational succinctness abstract dialectical frameworks.
Black, E., Modgil, S., & Oren, N. (Eds.), Proceedings Third International Workshop Theory Applications Formal Argumentation (TAFA).
Strass, H., & Wallner, J. P. (2015). Analyzing computational complexity abstract
dialectical frameworks via approximation fixpoint theory. Artificial Intelligence, 226,
3474.
Tseitin, G. S. (1968). complexity derivations propositional calculus. Structures Constructive Mathematics Mathematical Logic, Part II, Seminars
Mathematics (translated Russian), 115125.

231

fiJournal Artificial Intelligence Research 54 (2015) 471492

Submitted 05/15; published 11/15

Weighted Regret-Based Likelihood: New Approach
Describing Uncertainty
Joseph Y. Halpern

halpern@cs.cornell.edu

Computer Science Department
Cornell University
Ithaca, NY 14853, USA

Abstract
Recently, Halpern Leung suggested representing uncertainty set weighted
probability measures, suggested way making decisions based representation
uncertainty: maximizing weighted regret. paper answer apparently
simpler question: means, according representation uncertainty,
event E likely event E 0 . paper, notion comparative
likelihood uncertainty represented set weighted probability measures
defined. generalizes ordering defined probability (and lower probability)
natural way; generalization upper probability also defined. complete
axiomatic characterization notion regret-based likelihood given.

1. Introduction
Recently, Samantha Leung (Halpern & Leung, 2012) suggested representing uncertainty set weighted probability measures, suggested way making decisions
based representation uncertainty: maximizing weighted regret. However,
answer apparently simpler question: given representation uncertainty,
mean event E likely event E 0 ?
paper. explain issues, start reviewing Halpern-Leung approach.
frequently observed many situations agents uncertainty adequately described single probability measure. Specifically, single
measure may adequate representing agents ignorance. example,
seems big difference coin known fair coin whose bias agent
know, yet agent use single measure represent uncertainty,
cases would seem measure assigns heads probability 1/2
would used.
One approach suggested representing ignorance use set P
probability measures. idea old one, apparently going back work Boole
(1854, ch. 1621) Ostrogradsky (1838); authors (e.g., Campos & Moral, 1995;
Couso, Moral, & Walley, 1999; Gilboa & Schmeidler, 1993; Levi, 1985; Walley, 1991)
additionally required set P convex (so 1 2 P,
a1 + b2 , a, b [0, 1] + b = 1). approach benefit representing
uncertainty general, single number, range numbers. allows us
distinguish certainty coin fair (in case uncertainty heads
represented single number, 1/2) knowing probability heads could
anywhere between, say, 1/3 2/3.
c
2015
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHalpern

approach also problems. example, consider agent believes
coin may slight bias. Thus, although unlikely completely fair,
close fair. represent set probability measures? Suppose
agent quite sure bias 1/3 2/3. could, course, take
P consist measures give heads probability 1/3 2/3.
agent know possible biases exactly 1/3 2/3.
consider 2/3 + possible small ? even confident bias
1/3 2/3, representation cannot take account possibility
views biases closer 1/2 likely biases 1/2.
also second well-known concern: learning. Suppose agent initially
considers possible measures gives heads probability 1/3 2/3.
starts tossing coin, sees that, first 20 tosses, 12 heads. seems
agent consider bias greater 1/2 likely bias less
1/2. use standard approach updating sets probability measures
(Halpern, 2003), condition measures observation, since coin
tosses viewed independent, agent continue believe probability
next coin toss 1/3 2/3. observation impact far learning
predict better. set P stays same, matter observation made.
well-known solution problems: putting measure uncertainty
probability measures P. idea long history. One special case put
second-order probability probability measures; see (Good, 1980) discussion
approach references. example, agent express fact
bias coin likely close 1/2 far 1/2. addition,
problem learning dealt straightforward conditioning. approach
leads problems. Essentially, seems ambiguity agent might feel
outcome coin toss seems disappeared. example, suppose
agent idea bias is. obvious second-order probability use
uniform probability possible biases. cannot talk probability
coin heads (there set probabilities, all, single probability), expected
probability heads 1/2. agent idea bias coin
know believe expected probability heads 1/2? course, one use
single probability measure describe uncertainty, symmetry considerations dictate
one ascribes equal likelihood heads tails; similarly, one
put single second-order probability set possible biases, uniform probability
seems like obvious choice. Moreover, interest making decisions,
maximizing expected utility using expected probability take
agents ignorance account. Kyburg (1988) Pearl (1987) even argued
need second-order probability probabilities; whatever done
second-order probability already done basic probability.
Nevertheless, comes decision-making, seem useful use approach
represents ambiguity, still maintaining features secondorder probability probabilities. idea goes back least Gardenfors Sahlin
(1982, 1983). Walley (1997) suggested putting possibility measure (Dubois & Prade, 1998;
Zadeh, 1978) probability measures; also essentially done Cattaneo (2007),
Chateauneuf Faro (2009), de Cooman (2005). authors others,
472

fiWeighted Regret-Based Likelihood

Klibanoff et al. (2005), Maccheroni et al. (2006), Nau (1992), proposed approaches
decision making using representations uncertainty.
Leung similarly suggested putting weights probability measure P. Since
assumed weights normalized supremum weights 1,
weights also viewed possibility measure. set P finite, also
normalize view weights second-order probabilities. secondorder probabilities, weights vary time, information acquired.
example, start state complete ignorance (modeled assuming
probability measures weight 1), update weights making observation
ob, take weight measure Pr relative likelihood ob Pr
true measure. (See Section 2 details.) approach, called likelihood updating
Halpern Leung (2012), true underlying measure generating data,
time, weight true measure approaches 1, weight measures
approaches 0. Thus, approach allows learning natural way. If, example,
actual bias coin 5/8 example above, matter initial weights,
long 5/8 positive weight, weight would almost surely converge 1
observations made, weight measures would approach 0. This,
course, exactly would happen second-order probability P.
weights also used represent fact probabilities set P
likely others.
Like essentially others considered representation uncertainty based set
probability weights, Leung also suggested way using representation
make decisions. However, approach different suggested earlier.
based approach regret, standard approach decision-making introduced
(independently) Niehans (1948) Savage (1951). uncertainty represented
set P probability measures, regret works follows: act
measure Pr P, compute expected regret respect Pr;
difference expected utility expected utility act
gives highest expected utility respect Pr. associate act
worst-case expected regret a, measures Pr P, compare acts
respect worst-case expected regret. weights picture, modify
procedure multiplying expected regret associated measure Pr weight
Pr, compare acts according worst-case weighted expected regret. approach
making decisions different others mentioned incorporate
likelihood probabilities. Moreover, using weights way means cannot
simply replace set weighted probability measures single probability measure;
objections Kyburg (1988) Pearl (1987) apply.
Leung (Halpern & Leung, 2012) show approach seems reasonable
things number examples interest, provide axiomatization decision-making
approach. Since sets weighted probabilities certainly intended way
representing uncertainty, seems natural ask whether used represent
relative likelihood direct way. Surprisingly, something largely considered
earlier papers using sets weighted probabilities, since focus decision-making
(although work Nau discussed Section 3 exception).
473

fiHalpern

Representing relative likelihood straightforward uncertainty represented
single probability measure: E likely E 0 exactly probability E greater
probability E 0 . using sets probability measures, various approaches
considered literature. common takes E likely
E 0 lower probability E greater lower probability E 0 , lower
probability E worst-case probability, taken measures P (see Section 3).
could also compare E E 0 respect upper probabilities (the best-case
probability respect measures P). Another possibility take E
likely E 0 Pr(E) Pr(E 0 ) measures Pr P; gives partial order
likelihood.1 uncertainty represented set weighted
probability measures?
paper, define notion relative likelihood uncertainty represented
set weighted probability measures generalizes ordering defined lower
probability natural way; also define generalization upper probability.
associate event E two numbers analogues lower upper probability.
uncertainty represented single measure, two numbers coincide; general,
not. interval thought representing degree ambiguity
likelihood E. Indeed, special case weights 1, numbers
essentially lower upper probability (technically, 1 minus lower
upper probability, respectively). Interestingly, approach assigning likelihood
based approach decision-making. Essentially, analogue
defining probability terms expected utility, rather way around.
approach viewed generalizing probability lower probability,
time allowing natural approach updating.
interested representation? ever probability use make decisions, arguably wouldnt much interest;
work Leung already shows sets weighted probabilities used decisionmaking. results paper add nothing question. However, often
talk likelihood events quite independent use decision-making.
clearly many examples physics. issue arises AI applications well: typical
explanation rather B thought event E
likely F . computations expectation, clearly involve representation
uncertainty, arise many AI applications. Thus, analogue probability seems
important useful right.
rest paper organized follows. reviewing relevant material
(Halpern & Leung, 2012) Section 2, define regret-based likelihood Section 3,
compare lower probability. provide axiomatic characterization regret-based
likelihood Section 4, show relates axiomatic characterization lower
probability. conclude Section 5.

1. long tradition considering partially ordered notions likelihood; see (Halpern, 1997)
references therein, work Walley (1991).

474

fiWeighted Regret-Based Likelihood

2. Weighted Expected Regret: Review
Consider standard setup decision theory. state space outcome
space O. act function O; describes outcome state. Suppose
utility function u outcomes set P + weighted probability measures.
is, P + consists pairs (Pr, Pr ), Pr weight [0, 1] Pr probability
S. Let P = {Pr : ((Pr, ) P + )}. Pr P assumed
exactly one , denoted Pr , (Pr, ) P + . assumed weights
normalized least one measure Pr P Pr = 1.
Finally, P + assumed weakly closed, (Prn , n ) Pr+ n = 1, 2, 3, . . .,
(Prn , n ) (Pr, Pr ), Pr > 0, (Pr, Pr ) P + . (I discuss require
P + weakly closed, rather closed.)
assumption least one probability measure weight 1 convenient
comparison approaches; see below. However, making assumption
impact results paper; long restrict sets weight bounded,
results hold without change. assumption is, course, incompatible
weights probabilities. Note assumption weights probabilities
runs difficulties infinite number measures P; example, P
includes measures heads 1/3 2/3, discussed Introduction, using
uniform probability, would forced assign individual probability measure
weight 0, would work well later definitions.
weights P + coming from? general, viewed subjective,
like probability measures. However, Leung (Halpern & Leung, 2012)
observed, important special case weights given natural
interpretation. Suppose that, case biased coin Introduction, make
observations situation probability making given observation determined
objective source. start giving probability measures weight 1.
Given observation ob (e.g., sequence coin tosses example Introduction),
compute Pr(ob) measure Pr P; update weight Pr
Pr(ob)/ supPr0 P Pr0 (ob). Thus, likely observation according Pr,
higher updated weight Pr relative probability measures P.2 (The
denominator normalization ensure measure weight 1.)
approach updating, true underlying measure generating data,
agent makes observations, almost surely, weight true measure approaches
1, weight measures approaches 0.3 addition, approach gives
agent natural way determining weights probability measure P. While,
general, means agent may need carry around lot information (not
2. idea putting possibility probabilities P determined likelihood also appears
work Moral (1992), although consider general approach dealing sets weighted
probability measures.
3. almost surely due fact that, probability approaching 0, observations made, possible agent make misleading observations representative
true measure. also depends set possible observations rich enough allow
agent ultimately discover true measure generating observations; example, agent
never learn distributions outcomes die never gets observe die lands 5 6.
Since learning focus paper, make notion rich enough precise here.

475

fiHalpern

possibly infinite set probabilities, weight associated one),
set P reasonable parametric representation, weight often evaluated
terms parameters, admit compact representation (see Example 3.2).
weight associated probability Pr viewed upper bound
agents confidence Pr actually describes situation. agent
idea going modeled starting placing weight 1 probability
measures. believe weights allow agents express nuances
consider important, weights hard elicit. Whether
case really empirical question, one believe deserves exploration,
beyond scope paper.
review definition weighted regret, introduce notion absolute
(weighted) regret. start regret. regret act state
difference utility best act state utility s. Typically,
act compared acts, acts set , called menu. Thus,
regret state relative menu , denoted reg (a, s), supa0 u(a0 (s)) u(a(s)).4
typically constraints put ensure supa0 u(a0 (s)) finitethis
certainly case finite, convex closure finite set acts,
best possible outcome outcome space O. latter assumption holds paper,
assume throughout supa0 u(a0 (s)) finite.
simplicity, assume state space finite. Given probability measure
Pr S, expected regret act respect Pr relative menu
P

reg
sS reg (a, s) Pr(s). (expected) regret respect P menu
Pr (a) =
worst-case regret, is,

reg
P (a) = sup reg Pr (a).
PrP

Similarly, weighted (expected) regret respect P + menu
worst-case weighted regret, is,

wr
P + (a) = sup Pr reg Pr (a).
PrP

Thus, regret special case weighted regret, weights 1.
Note that, far weighted regret goes, hurt augment set P + weighted
probability measures adding pairs form (Pr, 0) Pr
/ P. start set
+
P unweighted probability measures, set P = {(Pr, 1) : Pr P}{(Pr, 0) : Pr
/ P}
closed general, although weakly closed. may well sequence Prn Pr,
Prn
/ P n, Pr P. would (Prn , 0) P + converging
+
(Pr, 0)
/ P . exactly required weak closedness. Note future reference
that, since P + assumed weakly closed, wr
P + (a) > 0, element

+

(Pr, Pr ) P wr P + (a) = Pr reg Pr (a).
Weighted regret induces obvious preference order acts: act least good
0
0


a0 respect P + , written reg
P + ,M , wr P + (a) wr P + (a ). usual,
4. Recall X set real numbers, sup X, supremum X, smallest real numbers
greater equal elements X. X finite, sup max.
X is, say, interval (0, 1), sup X = 1. Similarly, inf X largest real number
less equal elements X.

476

fiWeighted Regret-Based Likelihood

reg
reg
0
0
0
write reg
P + ,M P + ,M case P + ,M a. standard notion
regret special case weighted regret weights 1. sometimes write
0
+
reg
P,M denote unweighted case (i.e., weights P 1).
setting, using weighted regret gives approach allows agent transition
smoothly regret expected utility. well known regret generalizes expected
0
utility sense P singleton {Pr}, wr
P (a) wr P (a ) iff EUPr (a)
0
EUPr (a ) (where EUPr (a) denotes expected utility act respect probability
Pr); follows observation that, given menu , constant cM
that, acts , wr
{Pr} (a) = cM EUPr (a). (In particular, means P
singleton, regret menu independent.) start weights 1, then,
observed above, weighted regret standard notion regret. agent
makes observations, measure Pr generating uncertainty, weights
get closer closer situation Pr gets weight 1, weights
measures dropping quickly 0, ordering acts converge ordering
given expected utility respect Pr.
another approach similar properties, starts uncertainty represented set P (unweighted) probability measures. Define wc P (a) =
inf PrP EUPr (a). Thus wc P (a) worst-case expected utility a, taken Pr P.
define mm
a0 wc P (a) wc P (a0 ). maxmin expected utility rule, quite
P
often used economics (Gilboa & Schmeidler, 1989). difficulties getting
weighted version maxmin expected utility (Halpern & Leung, 2012) (discussed
Section 3); however, Epstein Schneider (2007) propose another approach
combined maxmin expected utility. fix parameter (0, 1), update P
observation ob retaining measures Pr Pr(ob) .
choice < 1, end converging almost surely single measure,
approach converges almost surely expected utility.
conclude section discussion menu dependence. Maxmin expected utility
menu dependent; preference ordering acts induced regret be,
following example illustrates.

Example 2.1: Take outcome space {0, 1}, utility function
identity, u(1) = 1 u(0) = 0. usual, E S, 1E denotes indicator
function E, where, state S, 1E (s) = 1 E, 1E (s) = 0

/ E. Let = {s1 , s2 , s3 , s4 }, E1 = {s1 }, E2 = {s2 }, E3 = {s2 , s3 }, M1 = {1E1 , 1E2 },
M2 = {1E1 , 1E2 , 1E3 }, P = {Pr1 , Pr2 }, Pr1 (s1 ) = Pr1 (s3 ) = Pr1 (s4 ) = 1/3,
1
Pr2 (s2 ) = 1/4, Pr2 (s3 ) = 3/4. straightforward calculation shows reg
Pr1 (1E1 ) =
M1
M1
M1
M2
M2
0, reg Pr1 (1E2 ) = 1/3, reg Pr2 (1E1 ) = 1/4, reg Pr2 (1E2 ) = 0, reg Pr1 (1E1 ) = 1/3, reg Pr1 (1E2 ) =
M1
M1
M2
2
2/3, reg
Pr2 (1E1 ) = 1, reg Pr2 (1E2 ) = 3/4. Thus, 1/4 = reg P (1E1 ) < reg P (1E2 ) = 1/3,
M2
2
1 = reg
P (1E1 ) > reg P (1E2 ) = 3/4. preference 1E1 1E2 depends
whether consider menu M1 menu M2 .
Suppose outcome gives maximum utility; is,
u(o) O. constant act gives outcomes states,
clearly best act states. best act, absolute,
menu-independent notion weighted expected regret defined always comparing

u(o )

477

fiHalpern

. is, define
reg(s, a) = u(o ) u(a(s));
P
reg Pr (a) = sS (u(o ) u(a(s)) Pr(s) = u(o ) EUPr (a);
P
reg P (a) = supPrP sS (u(o ) u(a(s)) Pr(s) = u(o ) inf PrP (EUPr (a);
P
wr P + (a) = supPrP Pr sS (u(o ) u(a(s)) Pr(s) = supPrP Pr (u(o ) EUPr (a)).
best act, write P + a0 wr P + (a) wr P + (a0 ); similarly
unweighted case, write P a0 wr P (a) wr P (a0 ).
Conceptually, think agent always aware best outcome ,
comparing actual utility u(o ). Equivalently, absolute notion regret
equivalent menu-based notion respect menu includes (since
menu includes , best act every state). shall see, setting,
always reduce menu-dependent regret absolute, menu-independent notion, since
fact best act: 1S .

3. Relative Ordering Events Using Weighted Regret
section, consider notion comparative likelihood defined using sets
weighted probability measures.
Example 2.1, take outcome space {0, 1}, utility function
identity, consider indicator functions. easy see EUPr (1E ) = Pr(E),
setup, recover probability expected utility. Thus, uncertainty
represented single probability measure Pr make decisions preferring
acts maximize expected utility, 1E 1E 0 iff Pr(E) Pr(E 0 ).
Consider happens apply approach maxmin expected utility.
1E mm
1E 0 iff inf PrP Pr(E) inf PrP Pr(E 0 ). literature, inf PrP Pr(E),
P
denoted P (E), called lower probability E, standard approach describing likelihood. dual upper probability, supPrP Pr(E), denoted P (E). easy
calculation shows
P (E) = 1 P (E),
where, usual, E denotes complement E. interval [P (E), P (E)]
thought describing uncertainty E; larger interval, greater ambiguity.
happens apply approach regret? First consider unweighted regret.
restrict acts form 1E , best act clearly 1S ,
constant function 1. Thus, (and do) use absolute notion regret here,
remainder paper. get 1E reg
P 1E 0 iff supPrP (1 Pr(E))
0
0
supPrP (1 Pr(E )) iff supPrP Pr(E) supPrP Pr(E ); is,
0



1E reg
P 1E 0 iff P (E) P (E ).

478

fiWeighted Regret-Based Likelihood

Moreover, easy manipulation shows supPrP (1 Pr(E)) = 1 inf PrP Pr(E) = 1
P (E). follows
1E reg
P 1E 0
iff (1 P (E)) (1 P (E 0 ))
iff P (E) P (E 0 )
iff 1E mm
1E 0 .
P
is, regret maxmin expected utility put ordering events.
+ (E), (weighted) regret-based
extension weighted regret immediate. Let Preg
likelihood E, defined taking
+
Preg
(E) = sup Pr Pr(E).
PrP

P + unweighted, weights 1, write Preg (E) denote supPrP Pr(E).
Note Preg (E) = 1 P (E),
Preg (E) Preg (E 0 ) iff P (E) P (E 0 ).
is, ordering induced Preg opposite induced P . So, example,
Preg () = 1 Preg (S) = 0; smaller sets larger regret-based likelihood. However, since
act smaller regret viewed better, ordering acts form 1E induced
regret induced maxmin expected utility.
Regret-based likelihood provides way associating number event,
probability lower probability do. Moreover, lower probability gives lower
+ (E) giving upper bound uncertainty.
bound uncertainty, think Preg
(It upper bound rather lower bound larger regret means less likely,
smaller lower probability does.) naive corresponding lower bound given
inf PrP Pr Pr(E). lower bound terribly interesting; probability
measures Pr0 P Pr0 close 0, lower bound close 0,
independent agents actual feeling likelihood E. reasonable
+
lower bound given expression P +
reg (E) = 1 Preg (E) (recall analogous
expression relates upper probability lower probability). intuition choice
following. nature conspiring us, would try prove us wrong
making Pr Pr(E) large possiblethat is, make weighted probability
wrong large possible. hand, nature conspiring us, would
try make Pr Pr(E) large possible, or, equivalently, make 1 Pr Pr(E) small
possible. Note different making Pr Pr(E) large possible, unless
Pr = 1 Pr P. easy calculation shows
+ (E) = 1 sup
1 Preg
PrP Pr Pr(E)
= inf PrP (1 Pr Pr(E)).

motivates definition P +
reg .
following lemma clarifies relationship expressions, shows
+
[P +
reg (E), Preg (E)] really give interval ambiguity.
+ (E) P + (E).
Lemma 3.1: inf PrP Pr Pr(E) 1 Preg
reg

479

fiHalpern

Proof: Clearly
inf Pr Pr(E) = inf Pr (1 Pr(E)).

PrP

PrP

Since, observed above,
+
1 Preg
(E) = inf (1 Pr Pr(E)),
PrP

Pr P,
1 Pr Pr(E) Pr (1 Pr(E)),
+ (E).
follows inf PrP Pr Pr(E) 1 Preg
Since, assumption, probability measure Pr0 P Pr0 = 1,
follows
+ (E) = 1 sup
1 Preg
PrP Pr Pr(E)
1 Pr0 (E)
= Pr0 (E)
supPrP Pr Pr(E)
+ (E).
Preg

general, equality hold Lemma 3.1, shown following example.
example also illustrates ambiguity interval decrease weighted regret,
weights updated Leung (Halpern & Leung, 2012) suggested.
Example 3.2: Suppose state space consists {h, t} (for heads tails); let Pr
measure puts probability h. Let P0+ = {(Pr , 1) : 1/3 2/3}. is,
initially consider measures put probability 1/3 2/3 heads.
toss coin observe lands heads. Intuitively, consider likely
probability heads greater 1/2. Indeed, applying likelihood updating,
get set P1+ = {(Pr , 3/2) : 1/3 2/3}; probability measures give h higher
probability get higher weight. particular, weight Pr2/3 still 1, weight
Pr1/3 1/2. (The weight Pr likelihood observing heads according Pr ,
, normalized likelihood observing heads according measure
gives heads highest probability, namely 2/3.) coin tossed
time tails observed, update get P2+ = {(Pr , 4(1 )) : 1/3 2/3}.
going on, worth noting simple parametric form P0+ leads
simple parametric forms P1+ P2+ .
+
+
+
easy calculation shows [P +
0,reg (h), P0,reg (h)] = [1/3, 2/3], [P 1,regret (h), P1,reg (h)] =
+
+
[1/3, 3/8], [P 2,reg (h), P2,reg (h)] = [11/27, 16/27]. detail, since Pr (h) =
Pr (t) = 1 , following:
0
P0,reg
(h) = sup[1/3,2/3] (1 ) = 2/3.

P 00,reg (h) = inf [1/3, 2/3](1 ) = 1/3.
0
P1,reg
(h) = sup[1/3,2/3] (3/2)(1). Taking derivative shows (3/2)(1)
0
maximized = 1/2, P1,reg
(h) = 3/8.

480

fiWeighted Regret-Based Likelihood

P 01,reg (h) = inf [1/3,2/3] (1 (3/2)). 1 (3/2) minimized, (3/2)
maximized; [1/3, 2/3], happens = 2/3, P 01,reg (h) = 1/3.
0
P2,reg
(h) = sup[1/3,2/3] 4(1)(1). Taking derivative shows 4(1)2
maximized = 1/3, case 16/27.

P 02,reg (h) = inf [1/3,2/3] (1 4(1 )). 1 4 2 (1 ) minimized
4 2 (1) maximized; [1/3, 2/3], happens = 2/3, P 01,reg (h) =
11/27.
also easy see inf Pr 4(1 ) Pr (t) = inf [1/3,2/3] 4(1 )2 = 8/27,
+
+
inf 4(1 )Pr (t) < 1 P2,reg
(t) < P2,reg
(h).

PrP2

Thus, P2+ , get strict inequalities expressions Lemma 3.1.
+
width interval [P +
reg (E), Preg (E)] viewed measure ambiguity
agent feels E, interval [P (E), P (E)]. Indeed, weights 1,
+ (E) P (E) = 1 P + (E)
two intervals width, since P (E) = 1 Preg
reg
case.
However, weighted regret significant advantage upper lower probability.
true bias coin is, say 5/8, set Pk+ represents uncertainty
+
k steps, k increases, almost surely, [P +
k,reg (h), Pk,reg (h)] smaller smaller
interval containing 1 5/8 = 3/8. generally, using likelihood updated combined
weighted regret provides natural way model reduction ambiguity via learning.
worth point comparing approach representing likelihood taken
work Nau (1992). Nau starts preference order lotteries (functions
finite state space reals) satisfying certain axioms, derives
calls confidence-weighted (lower upper) probabilities. Roughly speaking, rather
associating event lower upper probability, Nau associate
probabilities
event E, confidence c [0, 1], probability p [0, 1] set Pc,p
give event E lower probability p confidence least c. c0 c, Pc0 ,p Pc,p
(every probability measures gives E lower probability p higher confidence
c0 also give lower probability p confidence c, converse may hold).
Similarly, consider probability measures give E upper probability p
confidence c. set P unweighted probabilities, agents uncertainty regarding
event E characterized single interval [P (E), P (E)]. Naus framework,
agents uncertainty regarding E characterized family intervals [Pc (E), P c (E)],
indexed confidence c, Pc (E) largest p E lower probability
confidence c, P c (E) defined similarly. Clearly intervals nested;
0
c0 > c, [Pc0 (E), P c (E)] contains [Pc (E), P c (E)]. Thus, Naus approach provides
fine-grained representation uncertainty single intervals [P (E), P (E)]
+
[P +
reg (E), Preg (E)]. extent, distinction due fact Naus preference
order lotteries partial order; preference order induced max=min expected
+ , P + put
utility regret total. However, note even though P , P , Preg
reg

+ , P + together,
total order events, considering P P Preg
reg

481

fiHalpern

also obtain partial order events; particular, approaches express
ambiguity.
One benefit regret-based approach provides natural way updating.
Nau consider updating; would interesting see analogue likelihood
updating could defined axiomatically Naus framework, perhaps spirit
characterization Leung (Halpern & Leung, 2012) gave likelihood updating
context regret.
One concern use regret dependence regret menu;
Naus approach, approaches decision-making based regret,
require menu. evidence psychology literature suggesting
people quite sensitive menus, also worth noting dealing likelihood,
sense work absolute notion weighted regret without
loss generality: restrict indicator functions, preference relative menu
always reduced absolute preference. Given menu consisting indicator
functions, let EM = {E : 1E }; is, EM union events
corresponding indicator function . following property shows that, restrict
indicator functions, regret satisfies satisfies axiom similar spirit Naus (1992)
cancellation axiom.
Proposition 3.3: menu consisting indicator functions, 1E1 , 1E2 ,
reg
1E1 reg
P + ,M 1E2 iff 1E1 + 1E P + 1E2 + 1E .
Proof: Let 0 menu consisting indicator functions includes 1E1 + 1E ,
reg
1E2 + 1E , 1S . Recall 1E1 + 1E reg
P + 1E2 + 1E iff 1E1 + 1E 0 ,P + 1E2 + 1E ;
absolute notion regret equivalent menu-based notion, long menu
includes best act, case 1S . clearly suffices show that, states
acts 1E ,
0

reg (1E , s) = reg (1E + 1E , s).
straightforward. two cases, depending whether EM .
EM , then, definition, act 1E 0 E 0 ,
supaM u(a(s)) = u(1). Clearly supaM 0 u(a(s)) = u(1), since 1S 0 . Moreover,
1E (s) = 0, (1E + 1E )(s) = 1E (s). Thus, EM ,
reg (1E , s) = supaM u(a(s)) u(1E (s))
= supaM 0 u(a(s)) u((1E + 1E )(s))
0
= reg (1E + 1E , s).

/ E , a(s) = 0 1E (s) = 0, supaM u(a(s)) u(1E (s)) =
0. hand, supaM 0 u(a(s)) = u(1), u((1E + 1E )(s)) = u(1),
0
supaM 0 u(a(s)) u((1E + 1E )(s)) = 0. Thus, reg (1E , s) = reg (1E +
1E , s).

482

fiWeighted Regret-Based Likelihood

4. Characterizing Weighted Regret-Based Likelihood
goal section characterize weighted regret-based likelihood axiomatically.
order so, helpful review characterizations probability lower
probability. ease exposition discussion, assume sample space
finite sets measurable.
probability measure finite set maps subsets [0, 1] way satisfies
following three properties:
Pr1. Pr(S) = 1.
Pr2. Pr() = 0.5
Pr3. Pr(E E 0 ) = Pr(E) + Pr(E 0 ) E E 0 = .
three properties characterize probability sense function f : 2S [0, 1]
satisfies properties probability measure.
Lower probabilities satisfy analogues properties:
LP1. P (S) = 1.
LP2. P () = 0.
LP30 . P (E E 0 ) P (E) + P (E 0 ) E E 0 = .
However, properties characterize lower probability. functions
satisfy LP1, LP2, LP30 lower probability corresponding set
probability measures. (See (Halpern & Pucella, 2002, Proposition 2.2) example
showing analogous properties characterize P ; example also shows
characterize P .)
Various characterizations P (and P ) proposed literature (Anger &
Lembcke, 1985; Giles, 1982; Huber, 1976, 1981; Lorentz, 1952; Williams, 1976; Wolf, 1977),
similar spirit. discuss one due Anger Lembcke (1985) here, since makes
contrast lower probability regret particularly clear. characterization
based notion set cover: set E said covered n times multiset
every element E appears least n times . important note
multiset, set; elements necessarily distinct. (Of course, set
special case multiset.) Let denote multiset union; thus, M1 M2 multisets,
M1 M2 consists elements M1 M2 , appear multiplicity
sum multiplicities M1 M2 . example, using {{. . .}} notation
denote multiset, {{1, 1, 2}} {{1, 2, 3}} = {{1, 1, 1, 2, 2, 3}}.
E S, (n, k)-cover (E, S) multiset covers k times
covers E n + k times. Multiset n-cover E covers E n times. example,
= {1, 2, 3}, {{1, 1, 1, 2, 2, 3}} (2, 1)-cover ({1}, S), (1, 1)-cover ({1, 2}, S),
3-cover {1}.
interested whether multiset form E 1 . . . E (n, k)-cover
(E, S). perhaps best thought terms indicator functions. E 1 . . . E
5. property actually follows two, using observation Pr(S ) = Pr(S) + Pr();
include ease comparison approaches.

483

fiHalpern

(n, k)-cover (E, S) 1E1 + + 1Em n1E + k1S . use equalities inequalities involving sums indicator functions axiomatic characterizations
uncertainty long history; example, used Scott (1964) characterize
qualitative probability. Set covers special case inequalities. Typically,
axioms make possible apply results linear programming prove characterization
results. shall see, case too.
Consider following property:
LP3. integers m, n, k subsets E1 , . . . , Em S, E1 . . . Em (n, k)P
6
cover (E, S), k + nP (E)
i=1 P (Ei ).
analogous property upper probability, replaced . easy
see LP3 implies LP30 (since E E 0 (1, 0) cover (E E 0 , S)). follows
straightforward induction LP30 E1 , . . . , Em pairwise disjoint,
P (E1 . . . Em ) P (E1 ) + + P (E1 ). LP3 generalizes property allow sets
necessarily disjoint. soundness LP3 lower probability follows using
techniques given soundness property REG3. Anger
Lembcke (1985) show, LP3 property needed characterize lower
probability.
Theorem 4.1: (Anger & Lembcke, 1985) f : 2S [0, 1], exists set P
probability measures f = P f satisfies LP1, LP2, LP3.
Moving regret-based likelihood, clearly
+ (S) = 0.
REG1. Preg
+ () = 1.
REG2. Preg

whole space least regret; empty set greatest regret. Again, see
regret-based likelihood inverts standard ordering probability; larger regret-based
likelihood corresponds probability.
unweighted case, since Preg (E) = P (E), REG1, REG2, following analogue LP3 (appropriately modified P ) clearly characterize Preg :
REG30 . integers m, n, k subsets E1 , . . . , Em S, E 1 . . . E
P
(n, k)-cover (E, S), k + nPreg (E)
i=1 Preg (Ei ).
Note complements sets (E 1 , . . . , E , E) used here, since regret minimized
probability complement maximized. need work complement
makes statement properties (and proofs theorems) slightly less elegant,
seems necessary.
hard see REG30 hold weighted regret-based likelihood.
example, suppose = {a, b, c} P + = ((Pr1 , 2/3), (Pr2 , 2/3), (Pr3 , 1)), where,
identifying probability Pr tuple (Pr(a), Pr(b), Pr(c)),
Pr1 = (2/3, 0, 1/3);
6. Note LP3 implies LP2, using fact (1,0)-cover (, S).

484

fiWeighted Regret-Based Likelihood

Pr2 = (1/3, 0, 2/3);
Pr3 = (1/3, 1/3, 1/3).
+ ({a, b}) = P + ({b, c}) = 4/9, P + ({b}) = 2/3. Since {a, b} {b, c}
Preg
reg
reg
(1,1)-cover ({b}, {a, b, c}), REG30 would require
+
+
+
Preg
({a, b}) + Preg
({b, c}) 1 + Preg
({b}),

clearly case.
must thus weaken REG30 capture weighted regret-based likelihood. turns
appropriate weakening following:
REG3. integers m, n subsets E1 , . . . , Em S, E 1 . . . E n-cover
+ (E) Pm P + (E ).
E, nPreg

i=1 reg
Although REG3 weaker REG30 , still nontrivial consequences.
+ anti-monotonic. E E 0 , E 1-cover
example, follows REG3 Preg
0
+ (E) P + (E 0 ). Since E E 0 trivially 1-cover
E , REG3, must Preg
reg
+ (E) + P + (E 0 ) P + (E E 0 ). REG3 also implies REG1,
E E 0 , also follows Preg
reg
reg
since (= S) n-cover n.
state representation theorem. says representation uncertainty
satisfies REG1, REG2, REG3 iff weighted regret-based likelihood determined
set P + . set P + unique, taken maximal,
sense weighted regret-based likelihood respect set (P 0 )+ gives
representation, pairs (Pr, 0 ) (P 0 )+ , exists 0
(Pr, ) P + . (unique) maximal set P + viewed canonical representation
uncertainty.
Theorem 4.2: f : 2S [0, 1], exists weakly closed set P + weighted
+ f satisfies REG1, REG2, REG3;
probability measures f = Preg
moreover, P + taken maximal.
Proof: Clearly, given weakly closed set P + weighted probability measures, function
+ satisfies REG1 REG2. see satisfies REG3, suppose E . . . E
Preg
1

+ (E) = 0, REG3 trivially holds. P + (E) > 0, since P +
n-cover E. Preg
reg
+ (E) = Pr(E).
weakly closed, must probability Pr P Preg
Pr
Since E 1 t. . .tE n-cover E, easy see Pr(E 1 )+ +Pr(E ) = n Pr(E),
+ (E), construction,
Pr Pr(E 1 ) + + Pr Pr(E ) = nPr Pr(E). Pr Pr(E) = Preg
P

+ (E ), = 1, . . . , n. Thus, nP + (E)
+
Pr Pr(E ) Preg

reg
i=1 Preg (Ei ).

opposite direction, suppose f : 2 [0, 1] satisfies REG1, REG2,
REG3. Let P = (S), set probability measures S, Pr P, define
Pr = sup{ : Pr(E) f (E) E S}.
Note that, Pr P, 0 Pr(E) f (E) E S, since f (E) [0, 1],
1 Pr() = f () = 1. follows Pr [0, 1] Pr P. Let P + = {(Pr, Pr ) :
485

fiHalpern

Pr (S)}. easy see P + weakly closed. Moreover, show P +
+ ), immediate P + maximal among sets weighted
represents f (i.e., f = Preg
probability measures represent f . Thus, suffices show exists Pr (S)
(1) Pr = 1 (since one conditions sets weighted measures)
+ (E) E S.
(2) f (E) = Preg
proof result makes critical use following variant Farkas Lemma
(Farkas, 1902) (see also Schrijver, 1986, pg. 89) linear programming,
matrix, b column vector, x column vector distinct variables:
Lemma 4.3: Ax b unsatisfiable, exists row vector
1. 0
2. = 0
3. b > 0.
Intuitively, witness fact Ax b unsatisfiable.
vector x satisfying Ax b, 0 = (A)x = (Ax) b > 0, contradiction.
prove first claim, suppose = {s1 , . . . , sN }. construct set linear
equations variables x1 , . . . , xN solution equations guarantees
existence probability measure Pr (S) Pr = 1. Intuitively, want
xi Pr(si ). Since must Pr(E) f (E) E S,7 E S,
P
inequality {i:si E}
xi f (E). Note since f () = 1, equation
/
E = x1 + + xN 1. addition, require xi 0 = 1, . . . , N ,
x1 + +xN = 1. suffices require x1 + +xn 1, since, observed earlier,
equation corresponding E = already says x1 + + xn 1. apply Farkas Lemma
inequalities need involve , collection inequalities must rewritten as:
{i:si E}
xi f (E), E
/
xi 0, = 1, . . . , N
x1 + + xN 1.
P

system inequalities expressed form Ax b. Note matrix
whose entries either 1, 0, 1, and, first 2N 1 rows (the lines corresponding
equations E S), entries either 0 1, final N + 1
rows, entries either 0 1.
solution system inequalities provides desired Pr. systems
solution, Farkas Lemma, exists nonnegative vector = 0
b > 0. Since entries either 1, 0, 1, follows standard
observations (cf., Fagin, Halpern, & Megiddo, 1990, Lemma 2.7) take vector
whose entries rational.8 Since multiply term product
7. use denote strict subset.
8. slight subtlety since also satisfy b > 0, b may involve irrational numbers
(since f (E) may irrational sets E). However, nonnegative satisfies = 0
b > 0, nonnegative satisfies = 0 b0 > 0, b0 consists
rational entries b0 b. Thus, vector rational entries = 0 b0 > 0,
b > 0.

486

fiWeighted Regret-Based Likelihood

denominators entries , assume without loss generality
entries natural numbers.
Since 2N + N rows, vector form (1 , . . . , 2N +N ). Let A1 , . . . , A2N +N
rows A; vector length N . Since = 0, means
1 A1 + + 2N +N A2N +N = 0. Suppose 2N , . . . , 2N +N 1 (the coefficients
rows corresponding inequalities xi 0 = 1, . . . , N ) 0; show
below, assumption made without loss generality.
assumption, rewrite equations 1 A1 +. . . 2N 1 A2N 1 = 2N +N A2N +N .
E1 , . . . , E2N 1 subsets correspond equations A1 , . . . , A2N 1 ,
respectively, equation says 1 copies E 1 , 2 copies E 2 , . . . , 2N 1 copies
E 2N 1 form 2N +N -cover S. (Recall A2N +N row 1s, A2N +N corresponds S.) Thus, REG3, 1 f (E1 ) + + 2N 1 f (E2N 1 ) 2N +N f () = 2N +N .
Farkas Lemma requires b > 0, where, construction, bi = f (Ei ) =
1, . . . , 2N 1, bi = 0 = 2N , . . . , 2N + N 1, b2N +N = 1. Thus, must
(1 f (E1 )+ +2N 1 f (E2N 1 )) > 2N +N . Clearly, gives contradiction. Thus,
conclude, desired, equations solvable, exists probability
measure Pr Pr = 1.
N
N
remains show assume without loss generality 2 , . . . , 2 +N 1
0. Note since 0, must nonnegative. prove induction
2N + + 2N +N 1 vector 0 = 0 b > 0,
vector 2N + + 2N +N 1 = 0.
suppose solution 2N + + 2N +N 1 > 0. Suppose without
loss generality 2N > 0. Recall A2N corresponds inequality x1 0.
Choose j {0, . . . , 2N 1} j > 0 s1
/ Ej . must j,
otherwise would = 0. Let j 0 Ej 0 = Ej {s1 }. Define vector
/ {j, j 0 , 2N }.
0 20 N = 2N 1, j0 = j 1, j0 0 = j + 1, i0 =
0
0
0
easy check = 0 2N + + 2N +N 1 < 2N + + 2N +N 1 .
remains show 0 b > 0. Since Ej Ej 0 , must f (Ej ) f (Ej 0 ),
0 b = b + f (Ej ) f (Ej 0 ) b > 0. completes inductive step argument.
+ (E)
must show second required property holds, namely, f (E) = Preg
E S. construction, Pr Pr(E) f (E) E S, suffices show
Pr P Pr Pr(E) = f (E). this, suffices show exists
measure Pr Pr(E) = 1, E 0 S, f (E) Pr(E 0 ) f (E 0 ), since
Pr = f (E), Pr Pr(E) = f (E), desired.
show measure exists, construct set linear inequalities
much above, apply Farkas Lemma. Using notation above, suppose
simplicity E = {s1 , . . . , sM }, N . required inequalities involve
variables x1 , . . . , xM :
0

{i:s EE 0 } xi f (E 0 )/f (E), E 0 E E 6=

xi 0, = 1, . . . ,
x1 + + xM 1.
P

Again, requirement x1 + + xM 1 follows equation E.
system inequalities satisfiable, required probability measure,
suppose satisfiable. Again, writing system equations Ax b,
487

fiHalpern

Farkas Lemma, exists nonnegative vector = 0 b > 0.
proceed much before. Again, assume vector natural numbers.


assume 2 , . . . , 2 +M 1 (the coefficients rows corresponding
inequalities xi 0 = 1, . . . , N ) 0, fact = 0 means
2M +M cover E. get contradiction REG3 almost identical way
above. completes argument.
said earlier, set P + guaranteed exist Theorem 4.2 unique, although
canonical, sense unique maximal set weighted probability measures
represents f . might wonder actually get uniqueness imposing
extra requirements, particularly since Leung able representation
theorem. answer seems no. explain why, helpful review material
(Halpern & Leung, 2012).
Define sub-probability measure p like probability measure (i.e., function
mapping measurable subsets [0, 1] p(T 0 ) = p(T ) + p(T 0 ) disjoint
sets 0 ), without requirement p(S) = 1. identify weighted
probability distribution (Pr, ) sub-probability measure Pr. Conversely, given
sub-probability measure p, unique pair (, Pr) P = Pr: simply
take = p(S) Pr = p/. Thus, sequel, identify set sub-probability
measures set weighted probability measures.
set B sub-probability measures downward-closed if, whenever p B q p,
q B.
One advantage considering sub-probability measures clear
would mean set weighted probabilities convex (indeed, obvious
count convex combination (Pr, ) (Pr0 , 0 )), quite clear counts
convex combination sub-probability measures. Moreover, convex combination
sub-probability measures sub-probability measure.
Call set subprobability measures regular convex, downward-closed, closed,
contains least one proper probability measure. (The latter requirement corresponds
Pr = 1 Pr P + .) Leung provide set axioms preference
orders, show family preference orders indexed menus satisfies
axioms iff unique regular set weighted probability measures P + that,

b iff wr
P + (a) wr P + (b). Thus, might hope get uniqueness
imposing regularity requirement. easy see canonical maximal set P +
constructed proof Theorem 4.2 regular, lends credence hope.
Unfortunately, following example shows, regularity suffice uniqueness.
Example 4.4: Let = {s1 , s2 }, let f defined 2S taking f ({s1 }) = 1/4
f ({s2 }) = 1 (and f (S) = 0 f () = 1). sub-probability measure p
identified pair (p(s1 ), p(s2 )), makes easy think sub-probability
measures geometrically. set sub-probability measures region IR2
contained triangle bounded lines x = 0, = 0, = 1 x. set P +
subprobability measures downward closed if, whenever contains point (x, y),
contains (x0 , 0 ) rectangle defined points (0, 0), (x, 0), (0, y), (x, y).
intuition, let P0+ set subprobabilities quadrilateral bounded
x = 0, = 0, = 1 x, = 1/4 (the region marked vertical lines Figure 1).
488

fiWeighted Regret-Based Likelihood

hard show P0+ maximal set weighted probabilities representing f .
+
clearly regular. Since contains subprobability (1, 0), follows P0,reg
({s2 }) = 1.
+
+
also easy see that, since (0, 1/4) P0 p(s2 ) 1/4 p P0 ,
+
P0,reg
({s1 }) = 1/4.
let P1+ consist sub-probabilities triangle bounded x = 0, = 0,
+
= 1x
4 (the region marked horizontal lines Figure 1). Clearly P1 strict
+
subset P0 , clear figure also regular. Moreover, since
contains points ( 41 , 0) (0, 1), also represents f . Indeed, easily follows
geometry situation uncountably many regular sets weighted
probabilities representing f ; z [0, 34 ], regular set bounded lines x = 0,
= 0, = 14 , line (z, 41 ) (1, 0).

1

( 34 , 14 )

1
4

0

3
4

1

x

Figure 1: Regular sets weighted probability measures represent f .

Intuitively, problem function contain enough information
uniquely determine regular set weighted probability measures. clear whether
natural conditions imposed lead uniqueness.
seems closest come uniqueness consider maximal set.

5. Conclusion
defined approach associating event E numerical representation
likelihood uncertainty represented set weighted probability measures.
representation consists pair numbers, thought upper
lower bounds uncertainty. difference numbers viewed
measure ambiguity. two numbers coincide uncertainty represented
single probability. Moreover, probability measure gets weight 1,
two numbers essentially viewed lower upper probabilities E (more
precisely, 1 P (E) 1 P (E)). Thus, approach viewed generalization
lower upper probability case weighted probability measures, regretbased likelihood corresponding upper probability. definitions show
489

fiHalpern

interesting connection regret-based approaches minimization/maximization
approaches comes defining likelihood; connection breaks comes
general utility calculations (Halpern & Leung, 2012).
main technical result paper complete characterization likelihood
case state space finite. notion likelihood easily extended
case infinite state space (of course, integral used instead sum
calculate expected utility). conjecture characterization theorem still hold
essentially change, although checked details carefully.
course, would useful get better understanding numerical representation, see really captures agents feelings ambiguity risk
associated event, understand technical properties. leave future
work.

Acknowledgments
thank Samantha Leung, reviewers ECSQARU, JAIR referees many useful
comments paper. work supported part NSF grants IIS-0812045, IIS0911036, CCF-1214844, AFOSR grants FA9550-08-1-0438, FA9550-09-1-0266,
FA9550-12-1-0040, ARO grant W911NF-09-1-0281.

References
Anger, B., & Lembcke, J. (1985). Infinitely subadditive capacities upper envelopes
measures. Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete, 68, 403
414.
Boole, G. (1854). Investigation Laws Thought Founded
Mathematical Theories Logic Probabilities. Macmillan, London.
Campos, L. M. d., & Moral, S. (1995). Independence concepts sets probabilities.
Proc. Eleventh Conference Uncertainty Artificial Intelligence (UAI 95), pp.
108115.
Cattaneo, M. E. G. V. (2007). Statistical decisions based directly likeihood function.
Ph.D. thesis, ETH.
Chateauneuf, A., & Faro, J. (2009). Ambiguity confidence functions. Journal
Mathematical Economics, 45, 535 558.
Couso, I., Moral, S., & Walley, P. (1999). Examples independence imprecise probabilities. Proc. First International Symposium Imprecise Probabilities
Applications (ISIPTA 99).
de Cooman, G. (2005). behavioral model vague probability assessments. Fuzzy Sets
Systems, 154 (3), 305358.
Dubois, D., & Prade, H. (1998). Possibility measures: qualitative quantitative aspects.
Gabbay, D. M., & Smets, P. (Eds.), Quantified Representation Uncertainty
490

fiWeighted Regret-Based Likelihood

Imprecision, Vol. 1 Handbook Defeasible Reasoning Uncertainty Management
Systems, pp. 169226. Kluwer, Dordrecht, Netherlands.
Epstein, L., & Schneider, M. (2007). Learning ambiguity. Review Economic
Studies, 74 (4), 12751303.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). logic reasoning probabilities.
Information Computation, 87 (1/2), 78128.
Farkas, J. (1902). Theorie der enfachen ungleichungen. J. Reine und Angewandte Math.,
124, 127.
Gardenfors, P., & Sahlin, N. (1982). Unreliable probabilities, risk taking, decision
making. Synthese, 53, 361386.
Gardenfors, P., & Sahlin, N. (1983). Decision making unreliable probabilities. British
Journal Mathematical Statistical Psychology, 36, 240251.
Gilboa, I., & Schmeidler, D. (1989). Maxmin expected utility non-unique prior.
Journal Mathematical Economics, 18, 141153.
Gilboa, I., & Schmeidler, D. (1993). Updating ambiguous beliefs. Journal Economic
Theory, 59, 3349.
Giles, R. (1982). Foundations theory possibility. Gupta, M. M., & Sanchez, E.
(Eds.), Fuzzy Information Decision Processes, pp. 183195. North-Holland.
Good, I. J. (1980). history hierarchical Bayesian methodology. Bernardo,
J. M., DeGroot, M. H., Lindley, D., & Smith, A. (Eds.), Bayesian Statistic I, pp.
489504. University Press: Valencia.
Halpern, J. Y. (1997). Defining relative likelihood partially-ordered preferential structures. Journal A.I. Research, 7, 124.
Halpern, J. Y. (2003). Reasoning Uncertainty. MIT Press, Cambridge, Mass.
Halpern, J. Y., & Leung, S. (2012). Weighted sets probabilities minimax weighted
expected regret: new approaches representing uncertainty making decisions.
Proc. Twenty-Ninth Conference Uncertainty Artificial Intelligence (UAI 2012),
pp. 336345. appear, Theory Decision.
Halpern, J. Y., & Pucella, R. (2002). logic reasoning upper probabilities.
Journal A.I. Research, 17, 5781.
Huber, P. J. (1976). Kapazitaten statt Wahrscheinlichkeiten? Gedanken zur Grundlegung
der Statistik. Jahresbericht der Deutschen Mathematiker-Vereinigung, 78, 8192.
Huber, P. J. (1981). Robust Statistics. Wiley, New York.
Klibanoff, P., Marinacci, M., & Mukerji, S. (2005). smooth model decision making
ambiguity. Econometrica, 73 (6), 18491892.
491

fiHalpern

Kyburg, Jr., H. E. (1988). Higher order probabilities intervals. International Journal
Approximate Reasoning, 2, 195209.
Levi, I. (1985). Imprecision uncertainty probability judgment. Philosophy Science,
52, 390406.
Lorentz, G. G. (1952). Multiply subadditive functions. Canadian Journal Mathematics,
4 (4), 455462.
Maccheroni, F., Marinacci, M., & Rustichini, A. (2006). Ambiguity aversion, robustness,
variational representation preferences. Econometrica, 74 (6), 14471498.
Moral, S. (1992). Calculating uncertainty intervals conditional convex sets probabilities. Proc. Eighth Conference Uncertainty Artificial Intelligence (UAI
95), pp. 199206.
Nau, R. F. (1992). Indeterminate probabilities finite sets. Annals Statistics, 40 (4),
17371767.
Niehans, J. (1948). Zur preisbildung bei ungewissen erwartungen. Schweizerische Zeitschrift
fur Volkswirtschaft und Statistik, 84 (5), 433456.
Ostrogradsky, M. V. (1838). Extrait dun memoire sur la probabilite des erreurs des tribuneaux. Memoires dAcademie St. Petersbourg, Series 6, 3, xixxxv.
Pearl, J. (1987). need higher-order probabilities and, so, mean?.
Proc. Third Workshop Uncertainty Artificial Intelligence (UAI 87), pp. 4760.
Savage, L. J. (1951). theory statistical decision. Journal American Statistical
Association, 46, 5567.
Schrijver, A. (1986). Theory Linear Integer Programming. Wiley, New York.
Scott, D. (1964). Measurement structures linear inequalities. Journal Mathematical
Psychology, 1, 233247.
Walley, P. (1991). Statistical Reasoning Imprecise Probabilities, Vol. 42 Monographs
Statistics Applied Probability. Chapman Hall, London.
Walley, P. (1997). Statistical inferences based second-order possibility distribution.
International Journal General Systems, 26 (4), 337383.
Williams, P. M. (1976). Indeterminate probabilities. Przelecki, M., Szaniawski, K., &
Wojcicki, R. (Eds.), Formal Methods Methodology Empirical Sciences, pp.
229246. Reidel, Dordrecht, Netherlands.
Wolf, G. (1977). Obere und untere Wahrscheinlichkeiten. Ph.D. thesis, ETH, Zurich.
Zadeh, L. A. (1978). Fuzzy sets basis theory possibility. Fuzzy Sets Systems,
1, 328.

492

fiJournal Artificial Intelligence Research 54 (2015) 277-308

Submitted 12/14; published 10/15

Relations Spatial Calculi
Directions Orientations
Till Mossakowski

till@iws.cs.uni-magdeburg.de

Otto-von-Guericke-University Magdeburg,
Faculty Computer Science
Universittsplatz 2
39106 Magdeburg

Reinhard Moratz

reinhard.moratz@maine.edu

University Maine,
National Center Geographic Information Analysis,
School Computing Information Science,
348 Boardman Hall, Orono, 04469 Maine, USA.

Abstract
Qualitative spatial descriptions characterize essential properties spatial objects
configurations relying relative comparisons rather measuring. Typically,
qualitative approaches relatively coarse distinctions configurations made.
Qualitative spatial knowledge used represent incomplete underdetermined
knowledge systematic way. especially useful task describe features
classes configurations rather individual configurations.
Although reasoning generally NP-hard (even IR-complete), relative directions important play key role human spatial descriptions
several approaches represent using qualitative methods. approaches
directions spatial locations expressed constraints infinite domains,
e.g. Euclidean plane. theory relation algebras successfully applied
field. Viewing relation algebras universal algebras applying modifying
standard tools universal algebra work, (re)define notions qualitative constraint calculus, homomorphism calculi, quotient calculi.Based
method derive important properties spatial calculi corresponding properties
related calculi. conceptual point view formal mappings calculi
means translate different granularities.

1. Introduction
qualitative representation space and/or time provides mechanisms characterize
essential properties objects configurations. advantages quantitative representations be: (1) better match human concepts related natural language,
(2) better efficiency reasoning. two main trends qualitative spatial constraint
reasoning (Ligozat, 2011) topological reasoning regions (Randell & Cohn, 1989;
Randell, Cui, & Cohn, 1992; Egenhofer & Franzosa, 1991; Renz & Nebel, 1999; Worboys
& Clementini, 2001) reasoning directions points straight lines
orientations straight lines configurations derived points (Frank, 1991; Ligozat,
1998; Renz & Mitra, 2004; Freksa, 1992; Clementini, Felice, & Hernandez, 1997; Scivos &
c
2015
AI Access Foundation. rights reserved.

fiMossakowski & Moratz

Nebel, 2004; Moratz, Lcke, & Mossakowski, 2011; Mossakowski & Moratz, 2012; Dubba,
Bhatt, Dylla, Cohn, & Hogg, 2015).
constraint-based reasoning spatial configurations, typically partial initial
knowledge scene represented terms qualitative constraints spatial objects. Implicit knowledge spatial relations derived constraint propagation.
Previous research found mathematical notion relation algebra related
notions well-suited kind reasoning. particular, arbitrary relation algebra, well-known path consistency algorithm (Montanari, 1974) computes algebraic
closure given constraint network, approximates, many cases also decides,
consistency network polynomial time. Intelligent backtracking techniques
study maximal tractable subclasses also allow efficiently deciding networks involving
disjunctions. Starting Allens temporal interval algebra, approach successfully applied several qualitative constraint calculi, supported freely
available toolboxes (Gantner, Westphal, & Wlfl, 2008; Wallgrn, Frommberger, Wolter,
Dylla, & Freksa, 2006). Moreover, people started develop benchmark problem libraries (Nebel & Wlfl, 2009) shown method performs quite well also
compared constraint reasoning techniques (Westphal & Wlfl, 2009).
work, apply universal algebraic tools qualitative calculi. connection
previous investigated literature (Li, Kowalski, Renz, & Li, 2008; Bodirsky, 2008;
Huang, 2012). However, paper deviate standard universal algebra using lax
oplax homomorphisms, weaker properties standard homomorphisms
(and order-theoretic algebraic flavor), better suited transferof
algebraic structure qualitative calculi DRAfp , OPRA1 CYC b .
work, focus calculi binary relations only.

2. Relation Algebras Spatial Reasoning
Standard methods developed finite domains generally apply constraint reasoning infinite domains. theory relation algebras (Ladkin & Maddux, 1994;
Maddux, 2006) allows purely symbolic treatment constraint satisfaction problems
involving relations infinite domains. corresponding constraint reasoning techniques
originally introduced Montanari (1974), applied temporal reasoning (Allen, 1983)
later proved valuable spatial reasoning (Renz & Nebel, 1999; Isli & Cohn, 2000).
central data binary calculus given by:
list (symbolic names for) base-relations, interpreted relations
domain, crucial JEPD properties joint exhaustiveness pairwise disjointness (a general relation simply union base-relations).
table computation converses relations.
table computation compositions relations.
Then, path consistency algorithm (Montanari, 1974) backtracking techniques (van
Beek & Manchak, 1996) tools used tackle problem consistency constraint
networks related problems. algorithms implemented generic
reasoning toolboxes GQR (Gantner, Westphal, & Wlfl, 2008) SparQ (Wallgrn et al.,
278

fiRelations Spatial Calculi

2006). integrate new calculus tools, list base-relations tables
compositions converses (plus compositional identity, however really
used) need provided. Thereby, qualitative reasoning facilities tools become
available calculus.1 Since compositions converses general relations
reduced compositions converses base-relations, tables need given
base-relations. Based tables, tools provide means approximate
consistency constraint networks, list atomic refinements, (see Section 4
details).
Let b base-relation. converse b` = {(x, y)|(y, x) b} often baserelation. Since base-relations generally closed composition, operation
approximated weak composition:
[
b1 b2 = {b base-relation | (b1 b2 ) b 6= }
b1 b2 usual set theoretic composition
b1 b2 = {(x, z)|y . (x, y) b1 , (y, z) b2 }
Composition said strong b1 b2 = b1 b2 base-relations b1 , b2 . Generally,
b1 b2 over-approximates set-theoretic composition, strong composition captures
exactly.
mathematical background composition table-based reasoning given
theory relation algebras (Maddux, 2006; Renz & Nebel, 2007). many calculi, including
dipole calculus (see Ex. 9 below), slightly weaker notion needed, namely
non-associative algebra (Maddux, 2006; Ligozat & Renz, 2004), associativity
dropped. algebras treat spatial relations abstract entities (independently
domain) combined certain operations governed certain equations.
Definition 1 (Maddux 2006; Ligozat & Renz 2004). non-associative algebra tuple
= (A, , , , 0, 1, ,` , ) that:
1. (A, , , , 0, 1) Boolean algebra. called join, meet, 0 bottom, 1 top,
relative complement. Note Boolean algebra carries partial order defined
b iff b = b;
2. constant (called identity relation), ` unary operation (called converse)
binary operation (called weak composition) that, a, b, c A:
(a) (a` )` =
(b) = =
(c) (b c) = b c
(d) (a b)` = a` b` (e) (a b)` = a` b` (f ) (a b)` = b` a`
(g) (a b) c` = 0 (b c) a` = 0

non-associative algebra called relation algebra, weak composition associative.2
1. information calculus, tools provide functionality goes beyond
simple qualitative reasoning constraint calculi.
2. terminology bit misleading, since relation algebras associative non-associative algebras.
precise name non-associative algebras would relation algebras without associativity requirement. Nevertheless, stick terminology established literature.

279

fiMossakowski & Moratz

elements algebra called (abstract) relations. mainly
interested finite non-associative algebras complete atomic, means
set pairwise disjoint minimal relations, atoms, also called base-relations,
relations obtained joins these. Then, following fact well-known
easy prove:
Proposition 2 (Dntsch, 2005). complete atomic non-associative algebra uniquely
determined set base-relations, together converses compositions baserelations. (Note composition two base-relations general base-relation.)
providing examples, easier start partition schemes:
Definition 3 (Ligozat & Renz, 2004; Mossakowski et al., 2006). Let U non-empty set.
partition scheme U defined finite (index) set distinguished element
i0 I, unary operation ` I, family binary relations (Ri )iI U
1. (Ri )iI partition U U sense Ri pairwise disjoint jointly
exhaustive.
2. Ri0 diagonal relation {(x, x) | x U }.
3. Ri` (set-theoretical) converse relation Ri , I.
relations Ri referred basic relations. following often write
U U =

[

Ri

iI

denote partition schemes.
Proposition 4 (Ligozat & Renz, 2004; Mossakowski et al., 2006). Given partition scheme
U U =

[

Ri

iI

obtain non-associative algebra follows: Boolean algebra component P(I),
powerset I. converse given pointwise application ` ; diagonal i0 .
Composition given weak composition defined above.
introduce several qualitative calculi giving domain U set
basic relations; diagonal converse clear.
Example 5. prominent temporal calculus Allens interval algebra IA (Allen,
1983), describes possible relations intervals linear flows time3 .
interval pair (s, t) real numbers < t. 13 basic relations
intervals depicted Fig. 1.
3. also spatial interpretation Allen calculus intervals interpreted
one-dimensional spatial entities

280

fiRelations Spatial Calculi

Figure 1: Allens interval relations

Figure 2: CYC b relations. r B means B right A.
Example 6. CYC b calculus (Isli & Cohn, 2000) based domain CYC = { |
< } cyclic orientations. Equivalently, angles represented
oriented straight lines containing origin 2D Euclidian plane associated
reference system. Using latter representation, Fig. 2 depicts four base-relations r, l,
o, e (e.g. right, left, opposite, equal) CYC b .
converse composition tables follows:
b
e
l

r

b`
e
r

l


e
l

r

e
l
e
l
l {l, o, r}

r
r {e, l, r}

281


r

r
r {e, l, r}
e
l
l {l, o, r}

fiMossakowski & Moratz

Example 7. OPRAn calculus (Moratz, 2006; Mossakowski & Moratz, 2012) based
domain OP = {(p, ) | p R2 , < } oriented points Euclidean plane.
oriented point consists point angle serving orientation. full angle
divided using n axes, leading 4n regions, see Fig. 3. points B differ,
0 15 14

1 0

13
12
11

2
3
4
5

13

3

B


6

10
9

7 8 9 10

7 8

Figure 3: Two o-points relation 4313 B
relation mji B (i, j Z4m 4 ) reads like this: given granularity m, relative position
B respect described relative position respect B
described j. points B coincide, relation mi B expresses
difference Bs orientations (angles) region i.
special case OPRAn calculus n = 1 (e.g. OPRA1 ) cognitively motivated symbolic notation addition general notation OPRAn baserelations introduced above. Fig. 4 depicts oriented point corresponding division
plane regions Front, Left, Right Back (the latter stands
point itself). naming schema OPRA1 base-relations concatenates name
relative position second oriented point w.r.t. first relative position
first oriented point w.r.t. second. Using capitalization first part relation symbol, cognitively motivated schema relation names leads names
16 base-relations OPRA1 : FRONTfront, FRONTleft, FRONTright, FRONTback, LEFTfront, LEFTleft, LEFTright, LEFTback, RIGHTfront, RIGHTleft, RIGHTright,
RIGHTback, BACKfront, BACKleft, BACKright, BACKback. Again, points
coincide, compare orientations. leads relations SAMEfront, SAMEleft,
SAMEright SAMEback.
SAMEfront identity relation. SAMEback analogous opposite relation
CYC b (see Fig. 2). Also SAMEleft SAMEright analogous corresponding CYC b
relations.
Example 8. OPRAm calculus (Dylla, 2008) similar OPRAm . Here, concentrate OPRA1 . important extension refinement applied relations
RIGHTright, RIGHTleft, LEFTleft, LEFTright. relations refined marking
letters + , P A, according whether two orientations
oriented points positive, negative, parallel anti-parallel, similar Fig. 6:
LEFTleft refined LEFTleftA, LEFTleft+ LEFTleft-.
4. Z4m residue ring; simplicity, set Z4m = {0, . . . , 4m 1}.

282

fiRelations Spatial Calculi

Figure 4: OPRA1 base frame
LEFT right refined LEFTrightP, LEFTright+ LEFTright-.
RIGHTright refined RIGHTrightA, RIGHTright+ RIGHTright-.
RIGHT left refined RIGHTleftP, RIGHTleft+ RIGHTleft-.
remaining four options LEFTleftP, LEFTrightA, RIGHTrightP RIGHTleftA
geometrically impossible. Altogether, obtain set 28 base-relations.
Example 9. dipole pair distinct points Euclidean plane. explaining
dipole-dipole relations, first study dipole-point relations. distinguish whether
point lies left, right, one five qualitatively different locations
straight line passes corresponding dipole (Ligozat, 1993; Scivos & Nebel,
2004). corresponding regions shown right side Fig. 5.
Using seven possible relations dipole point, relations
two dipoles may specified according following conjunction four relationships:
R1 sB R2 eB B R3 sA B R4 eA , 5
Ri {l, r, b, s, i, e, f} 1 4. formal combination gives us 2401 relations,
72 relations geometrically possible. constitute DRAf calculus
(Moratz, Renz, & Wolter, 2000; Moratz, Lcke, & Mossakowski, 2011). example,
Fig. 5, relation lrrr B holds.

B
sB

f
eA

eB

e
r

l






sA

b

l r rr B

Figure 5: Orientation two dipoles based four dipole-point relations
Fig. 6 shows refinement DRAf , called DRAfp , additional distinguishing features due parallelism. relations different rrrr, llrr rrll llll, +,
, P already determined original base-relation
mentioned explicitly. base-relations relation symbol DRAf .
leads set 80 DRAfp base-relations. relation sese identity relation.
denote resulting non-associative algebra DRAfp .
5. Note e.g. r sB reads sB right A.

283

fiMossakowski & Moratz

rrrr B

rrrrA

rrrr

rrrr+

rrll B

rrllP

rrll

rrll+

llll B

llllA

llll

llll+

llrr B

llrrP

llrr

llrr+

Figure 6: Refined base-relations DRAfp . solid arrow denotes A, dashed arrow
denotes B.

3. Homomorphisms Weak Representations
presented calculi offer possibility describe scenes different levels granularity.
granularity description context-dependent selection adequate level
detail description (Hobbs, 1985). Granularity plays key role human strategies
deal complexity spatial features real world. demonstrated
nicely example Hobbs (1985). example points humans conceptualize streets one-dimensional entities plan trip, use two-dimensional
conception cross street. contexts pavement dug
street becomes three-dimensional volume. key importance mechanisms
flexibly switch translate granularities successful reasoning world
highlighted following quote Hobbs (1985, p. 432):
ability conceptualize world different granularities switch
among granularities fundamental intelligence flexibility.
enables us map complexities world around us simple theories
computationally tractable reason in.
Imagine scenario involving ships relative positions open sea (see Fig. 7).
Ships modelled elongated, directed entities neglecting width
shape property. resulting DRAfp representation uses single dipole ship
284

fiRelations Spatial Calculi

represented (see left part Fig. 7). OPRA1 representation addition even
lengths ships neglected (see middle part Fig. 7). CYC b representation abstracts away different locations ships focuses relative orientation
(see right part Fig. 7).

abstraction shape

abstraction length

abstraction location

Figure 7: Modelling relative ship directions different levels granularity DRAfp ,
OPRA1 , CYC b .

another example ships represented DRAfp way start point
corresponds position ship end point represents current speed.
specifically, end point denotes future position one minute travel (if speed
heading constant). longer arrows represent faster ships diagram.
alternative representation OPRA1 , representation might focus
location heading ships abstract away speed. several
DRAfp relations one representation map onto single OPRA1 relation alternative
representation. example three relations {flll, ells, illr} mapped FRONTleft
(see Fig. 8).

Figure 8: quotient homomorphism DRAfp OPRA1 three relations
{flll, ells, illr} mapped FRONTleft.

different spatial calculi used represent given spatial situation different
levels granularity, relation calculi typically formalized quotient
homomorphism. Figure 8 exemplifies action quotient homomorphism. Homomorphisms also arise contexts, e.g. embeddings smaller calculus larger
285

fiMossakowski & Moratz

one (for example, Allens interval algebra embedded DRAfp , see Proposition 25
below).
study homomorphisms general. means examination
relationships among calculi. Often, conceptual relations different calculi
domains formalised homomorphism, vice versa, one found homomorphism, often also conceptual relation behind it.
Homomorphisms also used transfer properties (like strength composition,
algebraic closure deciding consistency) one calculus another one, see Propositions
16, 19, 23, 39, 40, 44, 46, 47 48 below. Using homomorphisms, also possible find
errors composition tables (we discovered errors 197 entries composition table
OPRA1 , see Example 38 below).
Homomorphisms studied Ligozat Renz (2004) Ligozat (2005, 2011)
(mainly name representations). introduce systematic treatment
homomorphisms. non-associative algebras, recall refine weaker notion
lax homomorphisms, allow embedding calculus domain,
well relating several calculi other.
Definition 10 (Lax homomorphism, Moratz et al., 2009; Lcke, 2012). Given non-associative
algebras B, lax homomorphism homomorphism h : B underlying
Boolean algebras that:
h(A ) B
h(a` ) = h(a)`
h(a b) h(a) h(b) a, b
lax homomorphism complete atomic non-associative algebras called semistrong (Mossakowski, Schrder, & Wlfl, 2006) atoms a, b
ab=

_
{c | (h(a) h(b)) h(c) 6= 0}

notion inspired definition weak composition used
representation homomorphisms qualitative calculi.
Dually lax homomorphisms, define oplax homomorphisms6 , enable us
define projections one calculus another.
Definition 11 (Oplax homomorphism, Moratz et al., 2009; Lcke, 2012). Given nonassociative algebras B, oplax homomorphism homomorphism h : B
underlying Boolean algebras that:
h(A ) B
h(a` ) = h(a)`
h(a b) h(a) h(b) a, b
6. terminology motivated monoidal functors.

286

fiRelations Spatial Calculi

quotients, introduce strengthening notion oplax homomorphism.
full 7 homomorphism oplax homomorphism even
_
h(c d)
h(a) h(b) =
h(a)=h(c),h(b)=h(d)

proper homomorphism (sometimes called homomorphism) non-associative
algebras homomorphism lax oplax time; inequalities
turn equations. proper homomorphism also full. proper injective
homomorphism also semi-strong.
homomorphism complete atomic non-associative algebras given
action base-relations; extended general relations
_
_
h( bi ) =
h(bi ),
iI

iI

W
arbitrary (possibly infinite) join. sequel, always define homomorphisms way.
semi-strong lax homomorphisms used transfer composition
target source algebra, surjective full oplax homomorphisms used transfer
opposite direction. study latter, former treated Def. 20.
Definition 12. Given complete atomic non-associative algebra equivalence
relation atoms congruence _` , define quotient algebra
A/A equivalence classes A-atoms atoms. General relations sets
atoms. define atoms a, b:
A/ = {[a] | }
[a]` = [a` ]
[a] [b] = {[c] | c a0 b0 , a0 a, b0 b}
usual treat general relations sets atoms; hence general relations A/A
sets equivalence classes A-atoms.
Unfortunately, general, A/A non-associative algebra again:
Example 13. Consider relation algebra CYC b calculus (Example 6)
equivalence relation generated e. quotient algebra fails satisfy identity
laws (laws (b) Def. 1). seen quotient composition table:

{e, o}
{l}
{r}

{e, o}
{l}
{r}
{e, o}
{{l}, {r}}
{{l}, {r}}
{{l}, {r}} {{e, o}, {l}, {r}} {{e, o}, {l}, {r}}
{{l}, {r}} {{e, o}, {l}, {r}} {{e, o}, {l}, {r}}

7. terminology borrowed theory partial algebras (Burmeister, 1986). Burmeister (2002,
p. 101) puts follows: f full iff f fully induces structure direct image f (A).
exactly want here, too.

287

fiMossakowski & Moratz

study method prove A/A non-associative algebra later
additional conditions. now, straightforward prove:
Proposition 14. algebra A/A defined Def. 12 non-associative algebra,
homomorphism q : A/A given 7 [a] surjective full.
naturally leads to:
Definition 15. oplax homomorphism non-associative algebras said quotient
homomorphism full surjective.
easy standard result universal algebra (Grtzer, 1979) gives us:
Proposition 16. Proper quotient homomorphisms preserve holding equations,
particular, associativity.
However, non-proper quotient homomorphisms general preserve holding
equations. See Example 35: DRAfp associative, quotient DRAf not.
raises question use standard constructions results
universal algebra (Grtzer, 1979; Maddux, 2006), homomorphisms always proper
hence quotients preserve equations (Prop. 16) thus quotient non-associative
algebra non-associative algebra again. reason following:
Example 17. Consider point algebra induced three base-relations <, = >,
converse composition tables:

<
=
>

a`
>
=
<


<
=
>

<
<
<
{<, =, >}

=
>
< {<, =, >}
=
>
>
>

Let standard algebraic congruence relation generated <>. < equal
< <, congruent < >, {<, =, >}. Similarly, > congruent
{<, =, >}. Since congruence respects meet, obtain < >, , congruent
{<, =, >}. means congruence trivial standard algebraic quotient
trivial one-point relation algebra.
contrast, notion quotient, obtain following relation algebra,
expected one (we denote equivalence class {<, >} 6=):

6
=
=

a`
6=
=


6
=
=

6
=
=
{6=, =} 6=
6=
=

corresponding quotient homomorphism proper: q(<) q(<) 6= =
6 ,
{6=, =}, q(< <) = q(<), 6=. However, Prop. 14, surjective full.
Proposition 18. context Prop. 14, q proper, A/A non-associative
algebra.
288

fiRelations Spatial Calculi

Proof. Prop. 16, know equations preserved q. axiom Def. 1
equational form (g). Tarski shown (Maddux, 2006) (g) equivalent

(a` (1 (a b))) (1 b) = 1 b

important application quotients quotient homomorphisms lies following
fact:
Proposition 19. Given quotient homomorphism q : B, Bs converse composition tables computed A, using q.
Proof. Use formulas converse resp. composition definition full homomorphism. Since q surjective, formulas work elements B.
Another important application homomorphisms use definition
qualitative calculus. Ligozat Renz (2004) define qualitative calculus terms
so-called weak representation (Ligozat, 2005, 2011):
Definition 20 (Weak representation). weak representation : P(U U)8
identity-preserving (i.e. (A ) = B ) converse-preserving lax homomorphism
complete atomic non-associative algebra relation algebra domain U.
latter given canonical relation algebra powerset P(U U), identity,
converse composition (as well Boolean algebra operations) given
set-theoretic interpretations. weak representation semi-strong semi-strong.
strong, strong.
Example 21. Let = {(s, e) | s, e R2 , 6= e} set dipoles R2 .
weak representation DRAfp lax homomorphism f : DRAfp P(D D) given
f (b) = b.
Here, b left hand-side equation element abstract relation
algebra, b right hand-side set-theoretic extension relation. Since
chosen use set-theoretic relations elements relation algebra,
same.
generalized follows:
Proposition 22. Semi-strong representations partition schemes one-one correspondence.
Proof. Given partition scheme, bySProp. 4, obtain non-associative algebra. Let
map general relation R P(I) iR Ri . definition weak composition ensures
semi-strong lax homomorphism. Conversely, given semi-strong representation
: P(U U), define partition scheme atoms putting Ra := (a).
8. Note domain codomain part weak representation.

289

fiMossakowski & Moratz

Preservation top, bottom meet ensure JEPD property. Moreover, semistrength, composition weak composition according partition scheme.
clear constructions inverses other.
following propositions straightforward.
Proposition 23 (Moratz et al., 2009; Lcke, 2012). calculus strong composition
weak representation proper homomorphism.
Proposition 24 (Ligozat, 2005). weak representation injective (b) 6=
base-relation b.
first sample use homomorphism embedding Allens interval relations (Allen,
1983) DRAfp via homomorphism.
Proposition 25 (Moratz et al., 2011). proper homomorphism Allens interval algebra
DRAfp exists given following mapping base-relations.
equals

meets
overlaps

starts
finishes

7
7

7

7

7
7

7


sese
ffbb
efbs
ifbi
bfii
sfsi
beie

`
meets `
overlaps `
`
starts `
finishes `

7
7

7

7

7

7


bbff
bsef
biif
iibf
sisf
iebe

studying quotients calculi, natural consider homomorphisms weak
representations. refine notion Moratz et al. (2009) Lcke (2012) order
fit better examples:
Definition 26. Given weak representations : P(U U) : B P(V V),
{lax, oplax, full, proper} b {lax, oplax, proper}, (a,b)-homomorphism weak
representations (h, i) : given
a-homomorphism non-associative algebras h : B,
map : U V, diagram




P(U U)

P(i i)

h

B

P(V V)



290

fiRelations Spatial Calculi

commutes according b. Here, lax commutation means R A, (h(R))
P(ii)((R)), oplax commutation means , proper commutation =.
Note P(ii) obvious extension function relation algebras;
note (unless bijective) even homomorphism Boolean algebras (it
may fail preserve top, intersections complements), although satisfies oplaxness
property (and laxness property injective)9 .
Ligozat (2005) defines special notion morphism weak representations;
corresponds notion (proper,oplax) homomorphism weak representations
component h identity.
Example 27. homomorphism Prop. 25 extended (proper, proper)
homomorphism weak representations letting embedding time intervals
dipoles x-axis.
Definition 28. quotient homomorphism weak representations (full,oplax) homomorphism weak representations surjective components.
also refine construction weak representation equivalence relation
domain introduced Moratz et al. (2009) Lcke (2012), whose constructions
typical cases produce trivial one-point quotient, cf. Example 17.
Definition 29. Given weak representation : P(U U) equivalence relation
U congruence _` , obtain quotient representation / follows:




P(U U)

qA

A/A

P(q q)

P(U/ U/)

/

Let q : U U/ set-theoretic factorization U ;
q extends relations: P(q q) : P(U U) P(U/ U/);
let equivalence relation atoms generated
P(q q)((b1 )) P(q q)((b2 )) 6= b1 b2
base-relations b1 , b2 A;
let qA : A/A quotient sense Def. 12;
9. reader background category theory may notice categorically natural formulation would use contravariant powerset functor, yields homomorphisms Boolean algebras
(Mossakowski et al., 2006). However, present formulation fits better examples.

291

fiMossakowski & Moratz

finally, function / defined
1
/(R) = P(q q)((qA
(R))).

called regular w.r.t. kernel P(q q) (i.e. set pairs
made equal P(q q) ). case, base-relation b already generates (via
P(q q) ) full relation equivalence class [b] A/A .
Proposition 30. Let strong representation : P(U U) complete atomic nonassociative algebra equivalence relation U given,
1. identity-regular, ((a) ) 6= implies (a)
2. congruence converse,
3. enjoys following fill-in property: u(a)x u y, exist a0
z x
u

(a)




x


(a0 )

z

A/A defined Def. 29 non-associative algebra, qA : A/A quotient
homomorphism, / semi-strong lax homomorphism non-associative algebras.
Proof. use atoms A/A define partition scheme b At(A/A ) 7 /(b).
Note know A/A Boolean algebra (although know yet
non-associative algebra). straightforward show / preserves bottom
joins; since q surjective, also top preserved. Concerning meets, since general relations
A/A considered sets base-relations, suffices show b1 b2 = 0
1
1
(b2 ))) = . Assume contrary P(q
(b1 ))) P(q q)((qA
implies P(q q)((qA
1
1
q)((qA (b1 )))P(qq)((qA (b2 ))) 6= . already P(qq)((b01 ))P(qq)((b02 )) 6=
1
base-relations b0i qA
(bi ), = 1, 2. b01 b02 , hence qA (b01 ) = qA (b02 ) b1 b2 ,
contradicting b1 b2 = 0. preservation properties, JEPD property follows.
identity-regularity converse congruence, condition partition scheme
identity converse fulfilled.
Prop. 22, obtain semi-strong representation / : B P(U/ U/).
order show A/A non-associative algebra, show A/A = B. already
know atoms thus agree complete atomic Boolean algebras.
show agreement remaining operations:
Identity: Since identity-preserving, atomic. identity-regularity, B = [A ] =
A/A .
Converse: Since congruence converse, atomic relation A, [a]`B =
[a`A ] = [a]`A/A .
292

fiRelations Spatial Calculi

Composition: Given atomic relations a, b A, [c] [a] B [b] iff (by definition
weak composition) exist x, y, z [x] /(a) [y] /(b) [z] [x] /(c) [z]
iff (by definition /) exist x1 , x2 , y1 , x2 , z1 , z2 a0 a, b0 b, c0 c
x1 (a0 ) y1



y2 (b0 ) z1




(c0 )

x2

z2

fill-in property, equivalent (implicitly quantifying variables existentially omitting a0 a, b0 b, c0 c):
x1 (a0 )



(b0 ) z1




(c0 )

x2

z2

strength , equivalent
x1 (a0 b0 ) z1

x2


(c0 )

z2

turn equivalent c0 c00 (a0 b0 ) (for c00 a0 a, b0 b, c0 c).
equivalent [c] [a] A/A [b].
completes proof A/A non-associative algebra. Prop. 14, qA :
A/A quotient homomorphism, Prop. 22, / semi-strong lax homomorphism.
interesting open question whether Prop. 30 also holds semi-strong representations. conjecture answer positive. Note / fail strong even
(consider quotient DRAf DRAfp introduced Example 35).
Example 31. CYC b quotient OPRA1 . level domains, acts follows:
oriented point (p, ) mapped orientation (the point p forgotten).
level non-associative algebras, quotient given table Fig. 9.
Proposition 32. conditions Prop. 30, (qA , q) : / (full, oplax)
quotient homomorphism semi-strong representations. regular w.r.t. , (qA , q)
(full,proper) satisfies following universal property: (qB : B, : U
V) : another (full,proper) homomorphism weak representations injective
ker (i), unique (full,proper) homomorphism weak representations
(h, k) : / (qB , i) = (h, k) (qA , q).
293

fiMossakowski & Moratz

{LEFTleftA, FRONTfront, BACKback, RIGHTrightA, SAMEback} 7
{LEFTleft+, LEFTback, LEFTright+, RIGHTright+, RIGHTleft+,
RIGHTfront, FRONTleft, BACKright, SAMEleft} 7 l
{LEFTleft, LEFTfront, LEFTright, RIGHTright, RIGHTleft,
RIGHTback, FRONTright, BACKleft, SAMEright} 7 r
{LEFTrightP, RIGHTleftP, FRONTback, BACKfront, SAMEfront} 7 e

Figure 9: Mapping OPRA1 CYC b relations
Proof. (full,_) property10 follows Prop. 14. (_,oplax) property (qA , q)
P(q q) / qA , definition / amounts
1
qA ,
P(q q) P(q q) qA

follows surjectivity q. Regularity w.r.t. means kernel P(q q) , turns inequation equality; hence obtain
(_,properness). Concerning universal property, let (qB , i) : mentioned
properties given. Since ker (i), unique function k : U/ V = k q.
homomorphism h looking determined uniquely h(qA (b)) = qB (b);
also ensures (full,proper) homomorphism property. remains shown
well-definedness. Suppose b1 b2 . regularity, P(q q)((b1 )) = P(q q)((b2 )).
Hence also P(i i)((b1 )) = P(i i)((b2 )) (qB (b1 )) = (qB (b2 )). injectivity
, get qB (b1 ) = qB (b2 ).
Example 33. equivalence relation quotient Ex. 31 regular, consequently,
quotient weak representations (full,proper), cf. Fig. 10 illustrating
relation RIGHTright+.
far, studied quotients arising quotienting domain. also
quotients leaving domain intact identifying certain base-relations.
Proposition 34. Let : P(U U) semi-strong representation complete atomic
non-associative algebra 11 equivalence relation atoms (base-relations)
relate relation congruence _` .
leads (full, oplax) quotient weak representation follows:
10. write _ placeholder dont care, i.e. (full,_) refers fullness first component.
11. Note contrast Def. 29, constructed, parameter chosen.

294

fiRelations Spatial Calculi

Figure 10: OPRA1 relation RIGHTright+ generates possible angles CYC b
relation r.





P(U U)

qA

A/A

P(id id)

P(U U)

/

Proof. Let qA : A/A defined Def. 12. / defined similarly
Def. 29 (where q = id). this, get semi-strong representation / : B
P(U U) Prop. 30. proof A/A = B parallels Prop. 30
Boolean algebra structure converse. identity, use assumption ,
implies A/A = {A }. Concerning composition, need semi-strength only:
[c] [a] B [b] iff exist x, y, z x /(a) /(b) z x/(c)z iff exist
x, y, z a0 a, b0 b, c0 c x a0 b0 z x c0 z iff (by semi-strength) exist
a0 a, b0 b, c0 c c0 a0 b0 iff [c] [a] A/A [b].
(full, oplax)-property follows Prop. 14 Prop. 32.
Example 35. DRAf (as semi-strong representation) quotient DRAfp . obtained forgetting labels +, -, P A.
Example 36. OPRAn quotient OPRAnm , identity domain
level. level non-associative algebras, qA maps region OPRAnm region
OPRAn (for even i), regions (i 1) + 1 (i + 1) 1 OPRAnm
region OPRAn (for odd i), see Fig. 11. canonically extended OPRA
relations. equivalence relation kernel qA , i.e. relates
itself, elements (i 1) + 1 (i + 1) 1 related other.
295

fiMossakowski & Moratz

Note yields oplax homomorphism non-associative algebra lax.
counterexample laxness OPRA2 OPRA1 following: h(200 212 ) = {113 },
h(200 ) h(212 ) = {113 , 123 , 133 }.
(i-1)m

i-1


(i-1)m+1...(i+1)m-1
(i+1)m

i+1

Figure 11: OPRAn quotient OPRAmn
(Dylla, Mossakowski, Schneider, & Wolter, 2013), show OPRA1 OPRA8
associative. Prop. 16 Ex. 36, carries OPRAn .
Example 37. OPRA1 quotient OPRA1 . identity domain level.
level non-associative algebras, forgets labels +, -, P A.

296

fiRelations Spatial Calculi

{llllA} 7 LEFTleftA
{llll+, lllr, lllb} 7 LEFTleft+
{llll, lrll, lbll} 7 LEFTleft
{ffff, eses, fefe, fifi, ibib, fbii, fsei, ebis, iifb, eifs, iseb} 7 FRONTfront
{bbbb} 7 BACKback
{llbr} 7 LEFTback
{llfl, lril, lsel} 7 LEFTfront
{llrrP} 7 LEFTrightP
{llrr+} 7 LEFTright+
{llrf, llrl, llrr, lfrr, lrrr, lere, lirl, lrri, lrrl} 7 LEFTright
{rrrrA} 7 RIGHTrightA
{rrrr+, rbrr, rlrr} 7 RIGHTright+
{rrrr, rrrl, rrrb} 7 RIGHTright
{rrllP} 7 RIGHTleftP
{rrll+, rrlr, rrlf, rlll, rfll, rllr, rele, rlli, rilr} 7 RIGHTleft+
{rrll} 7 RIGHTleft
{rrbl} 7 RIGHTback
{rrfr, rser, rlir} 7 RIGHTfront
{ffbb, efbs, ifbi, iibf, iebe} 7 FRONTback
{frrr, errs, irrl} 7 FRONTright
{flll, ells, illr} 7 FRONTleft
{blrr} 7 BACKright
{brll} 7 BACKleft
{bbff, bfii, beie, bsef, biif} 7 BACKfront
{slsr} 7 SAMEleft
{sese, sfsi, sisf} 7 SAMEfront
{sbsb} 7 SAMEback
{srsl} 7 SAMEright

Figure 12: Mapping DRAfp OPRA1 relations
Example 38 (refined Moratz et al. 2009; Lcke 2012). OPRA1 quotient DRAfp .
level non-associative algebras, quotient given table Fig. 12.
level domains, acts follows: Given dipoles d1 , d2 D, relation d1 d2
expresses d1 d2 start point point direction. (This
regular w.r.t. f .) D/ domain OP oriented points R2 . See Fig. 14.
297

fiMossakowski & Moratz

equivalence relation quotient indeed regular: given base-relation b DRAfp ,
(b) already generates (via quotient) whole /([b]): pair oriented
points /([b]), suitable choice dipole end points leads relation (b) (also cf.
Fig. 8). Consequently, quotient weak representations (full,proper).
Prop. 19, construction OPRA1 quotient allows us computation
converse composition tables applying congruence relations tables
DRAfp . Actually, compared result procedure composition
table OPRA1 published Dylla (2008) provided tool SparQ (Wallgrn,
Frommberger, Dylla, & Wolter, 2009). course checking full oplaxness property
quotient homomorphism DRAfp OPRA1 , discovered errors 197 entries
composition table OPRA1 shipped qualitative reasoner SparQ.12
table corrected accordingly meantime.13
example, composition SAMEright= q(srsl) RIGHTright+= q(rrrr+, rbrr,
rlrr) computed q({blrr, lere, lfrr, lirl, llrf, llrl, llrr+, llrr-, llrrp, lrri,
lrrl, lrrr, rbrr, rlrr, rrrr+}) = {LEFTright-, LEFTright+, LEFTrightP, BACKright,
RIGHTright+}. old table additionally contained RIGHTright-. However,
configuration SAMEright b, b RIGHTright+ c RIGHTright- c geometrically
possible. Consider three oriented points oA , oB oC oA SAMEright oB


C





B

Figure 13: OPRA1 configuration
oB RIGHTright+ oC , depicted Fig. 13. picture, oA RIGHTright+ oC .
relation oA RIGHTright- oC hold, oC would need turned counter-clockwise.
turn would lead first oB RIGHTrightA oC oB RIGHTright- oC , even
oA RIGHTright- oC reached.
next result shows also use quotients transfer important property
calculi.
Proposition 39 (refined Moratz et al. 2009; Lcke 2012). Quotient homomorphisms
weak representations bijective second component preserve strength composition.
12. already reported (Moratz et al., 2009; Lcke, 2012). actual computation
table done congruence relation here, quotient construction wrong, resulting
one-point algebra, stated above.
13. See https://github.com/dwolter/SparQ/commit/89bebfc60a https://github.com/dwolter/
SparQ/commit/dad260edd9.

298

fiRelations Spatial Calculi

DRAfp

OPRA?1

fp

opra 1

P(D D)

P(OP OP)



Figure 14: Quotient homomorphism weak representations DRAfp OPRA1
Proof.
Let (h, i) : : P(U U) : B P(V V) quotient
homomorphism weak representations bijective. According Prop. 23,
strength composition equivalent (respectively ) proper homomorphism. assume proper homomorphism need show proper
well. also know h P(i i) proper. Let R2 , S2 two abstract relations
B. surjectivity h, abstract relations R1 , S1 h(R1 ) = R2
h(S1 ) = S2 . (R2 S2 ) = (h(R1 ) h(S1 )) = (h(R1 S1 )) = P(i i)((R1 S1 )) =
P(i i)((R1 )) P(i i)((S1 )) = (h(R1 )) (h(S1 )) = (R2 ) (S2 ), hence
proper.
Corollary 40 (Moratz et al. 2009; Lcke 2012). Composition OPRA1 strong.
Proof.
Composition DRAfp known strong (Moratz, Lcke, & Mossakowski,
2011). Example 38 Prop. 39, strength composition carries OPRA1 .
Corollary 41. Composition CYC b strong.
Example 42. quotient homomorphism Example 31 one-sided inverse, namely
embedding (i.e. proper injective homomorphism) CYC b OPRA1 level
non-associative algebras, quotient given table Fig. 15. level
domains, acts follows: orientation mapped oriented point (0, 0)
orientation.
Altogether, get diagram calculi (semi-strong representations) homomorphisms Fig. 16.

4. Constraint Reasoning
Let us apply relation-algebraic method constraint reasoning. Given nonassociative algebra A, constraint network map : N N A, N set
nodes (or variables) (Ligozat & Renz, 2004). Individual constraints (X, ) = R written
X R , X, variables N R relation A. constraint network
: N N atomic scenario, (X, ) base-relation.
299

fiMossakowski & Moratz

7 SAMEback
l 7 SAMEleft
r 7 SAMEright
e 7 SAMEfront

Figure 15: Mapping CYC b OPRA1 relations
IA

(proper,proper)

DRAfp

(full,proper)

(proper,proper)

OPRA?1

CYC b
(proper, oplax)

(full,oplax)

DRAf

(full,oplax)

(full,proper)

OPRA1

(full,oplax)

OPRAnm

(full,oplax)

OPRAn

Figure 16: Homomorphisms among various calculi.
constraint network consistent assignment variables
elements domain constraints satisfied (a solution). problem
Constraint Satisfaction Problem (CSP) (Mackworth, 1977). rely relation algebraic
methods check consistency, namely mentioned path consistency algorithm.
non-associative algebras, abstract composition relations need coincide
(associative) set-theoretic composition. Hence, case, standard path-consistency
algorithm necessarily lead path consistent networks, algebraic closure
(Renz & Ligozat, 2005):
Definition 43 (Algebraic Closure). constraint network binary relations called
algebraically closed variables X1 , X2 , X3 relations R1 , R2 , R3 constraint
300

fiRelations Spatial Calculi

relations
X1 R1 X2 ,

X2 R2 X3 ,

X1 R3 X3

imply
R 3 R1 R2 .
Algebraic closure enforced successively applying
R3 := R3 (R1 R2 )
X1 R1 X2 , X2 R2 X3 , X1 R3 X3 fixed point reached. Note procedure
leaves set solutions constraint network invariant. means
algebraic closure contains empty relation, original network inconsistent.14
However, general, algebraic closure one-sided approximation consistency:
algebraic closure detects inconsistency, sure constraint network
inconsistent; however, algebraic closure may fail detect inconsistencies: algebraically closed network necessarily consistent. calculi, like Allens interval
algebra, algebraic closure known exactly decide consistency scenarios, others
(Renz & Ligozat, 2005). also shown question completely orthogonal
question whether composition strong.
Constraint networks translated along homomorphisms non-associative algebras
follows: Given h : B : N N A, let h() composition h .
turns oplax homomorphisms preserve algebraic closure.
Proposition 44 (refined Moratz et al. 2009; Lcke 2012). Given non-associative algebras B, oplax homomorphism h : B preserves algebraic closure. injective
lax homomorphism reflects algebraic closure.
Proof. Since oplax homomorphism homomorphism Boolean algebras,
preserves order. three relations X1 R1 X2 , X2 R2 X3 , X1 R3 X3
algebraically closed constraint network A,
R3 R1 R2
preservation order implies:
h(R3 ) h(R1 R2 ).
Applying oplaxness property yields:
h(R3 ) h(R1 ) h(R2 ).
hence image constraint network h also algebraically closed. h
injective lax, reflects equations inequalities, converse implication follows
similar way.
14. scenarios, suffices check whether scenario algebraically closed, proper refinement must contain empty relation.

301

fiMossakowski & Moratz

Given scenario : N N A, following Renz Ligozat (2005), reorganize
function : P(N N ) defining (b) = {(X, ) N N | (X, ) = b}
base-relations b extending relations using joins usual. Note
weak representation iff scenario algebraically closed normalised. Here, constraint
network normalised (X, X) = (Y, X) = (X, )` .
atomic homomorphisms (i.e. mapping atoms atoms), translation
constraint networks lifted scenarios represented : P(N N ) using
correspondence, obtain h() : B P(N N ).
Definition 45. Given scenario : P(N N ), solution weak representation
: P(U U) function j : N U R A, P(j j)((R)) (R),
P(j j) short:
P(j j)

P(N N )





P(U U)




Proposition 46 (refined Moratz et al. 2009; Lcke 2012). (_,oplax) homomorphisms
weak representations preserve solutions scenarios.
Proof.
Let weak representations : P(U U) : B P(V V)
(_,oplax) homomorphism weak representations (h, i) : given.
given solution j : N U defined P(j j) .
oplax commutation property P(i i) h infer P(i j j) h,
implies j solution h().
important question calculus (= weak representation) whether algebraic closure
decides consistency scenarios (Renz & Ligozat, 2005). (Note general, consistent
scenario algebraically closed, vice versa.) prove property
preserved certain homomorphisms.
Proposition 47 (refined Moratz et al. 2009; Lcke 2012). Atomic (lax,oplax) homomorphisms (h, i) weak representations injective h preserve following property
image h:
Algebraic closure decides scenario-consistency.
Proof. Let weak representations : P(U U) : B P(V V) atomic
oplax homomorphism weak representations (h, i) : given. assume
, algebraic closure decides consistency scenarios.
302

fiRelations Spatial Calculi

scenario image h written h() : B P(N N ). h()
algebraically closed, Prop. 44, . Hence, assumption, consistent,
i.e. solution. Prop. 46, h() consistent well.
general scenario consistency problem DRAfp calculus NP-hard even
IR-complete (Wolter & Lee, 2010; Lee, 2014). However, specific scenarios,
better: apply Prop. 47 homomorphism interval algebra DRAfp (see
Example 27) obtain:
Proposition 48 (Moratz et al. 2009; Lcke 2012). Algebraic closure decides consistency
DRAfp scenarios involve interval algebra relations only.
Hence, consistency scenarios decided polynomial time (in spite
NP-hardness general scenario consistency problem). similar remark holds
CYC b relations embedded OPRA1 .
calculi RCC8, interval algebra etc., (maximal) tractable subsets
determined, i.e. sets relations algebraic closure decides consistency also
non-atomic constraint networks involving relations. follows algebraic
closure DRAfp decides consistency constraint network involving (the homomorphic
image of) maximal tractable subset interval algebra only.

5. Conclusion
study investigated calculi application side represent modality
different levels granularity. modality case relative direction. demonstrated model relative directions different levels granularity DRAfp ,
OPRA1 , CYC b . turned case study relative direction
oriented objects formal relation calculi could expressed quotient homomorphisms.
result step application universal algebraic methods qualitative
constraint reasoning. Since explosion qualitative constraint calculi
recent years becomes important study relations calculi
make automatic mappings calculi. contribute
presented work. also contributed new notion quotient (based so-called
oplax homomorphisms) relation algebras captures existing natural quotients
spatial calculi. published Haskell tools used finding checking
homomorphisms calculi public repository.15
concrete results study demonstrated answer questions whether composition strong algebraic closure decides consistency calculi
examined yet. purely algebraic methods, lift properties strength
composition algebraic closure deciding consistency along homomorphisms qualitative calculi. latter particularly important, algebraic closure polynomialtime method, whereas qualitative constraint problems cases turn NP-hard,
even scenarios base-relations.
15. See https://github.com/spatial-reasoning/homer

303

fiMossakowski & Moratz

derived chain calculi homomorphisms DRAfp , OPRA1 , CYC b .
Thereby combined dipole opra calculi cycord approach. Based
new approach could automatically derive composition table OPRA1 based
formally verified composition table DRAfp . compared table
composition table OPRA1 described previous work authors (Dylla, 2008).
turned old composition table shipped qualitative reasoner
SparQ contained errors 197 entries. emphasizes point important
develop sound mathematical theory basis computation composition tables
stay close possible implementation theory.

Acknowledgements
authors would like thank Dominik Lcke, Andr van Delden, Torsten Hahmann, Jay
Lee, Thomas Schneider Diedrich Wolter fruitful discussions Thomas Schneider
valuable comments draft. Also anonymous referees provided valuable hints.
work supported DFG Transregional Collaborative Research Center SFB/TR 8
Spatial Cognition, projects I4-[SPIN] R4-[LogoSpace] (TM), National Science
Foundation Grant No. CDI-1028895 (RM).

References
Allen, J. F. (1983). Maintaining knowledge temporal intervals. Communications
ACM, pages 832843, 1983.
Bodirsky, M. (2008). Constraint satisfaction problems infinite templates. Creignou,
N., Kolaitis, P. G., & Vollmer, H., editors, Complexity Constraints - Overview
Current Research Themes [Result Dagstuhl Seminar]., volume 5250 Lecture Notes
Computer Science, pages 196228. Springer, 2008. ISBN 978-3-540-92799-0. doi: 10.
1007/978-3-540-92800-3_8. URL http://dx.doi.org/10.1007/978-3-540-92800-3_8.
Burmeister, P. (1986). model theoretic approach partial algebras. Akademie Verlag,
Berlin, 1986.
Burmeister, P. (2002). Lecture notes universal algebra many-sorted partial algebras
preliminary version.
see http://www.mathematik.tu-darmstadt.de/Math-Net/
Lehrveranstaltungen/Lehrmaterial/SS2002/AllgemeineAlgebra/download/
LNPartAlg.pdf, 2002.
Clementini, E., Felice, P. D., & Hernandez, D. (1997). Qualitative Represenation Positional Information. Artificial Intelligence, 95:317356, 1997.
Dubba, K. S. R., Bhatt, M., Dylla, F., Cohn, A. G., & Hogg, D. C. (2015). Learning
relational event models video. J. Artif. Intell. Res. (JAIR), 52, 2015. accepted
publication.
Dntsch, I. (2005). Relation algebras application temporal spatial reasoning.
Artif. Intell. Rev., 23(4):315357, 2005.
304

fiRelations Spatial Calculi

Dylla, F. (2008). Agent Control Perspective Qualitative Spatial Reasoning Towards
Intuitive Spatial Agent Development. PhD thesis, University Bremen, 2008. Published Akademische Verlagsgesellschaft Aka GmbH.
Dylla, F., Mossakowski, T., Schneider, T., & Wolter, D. (2013). Algebraic properties
qualitative spatio-temporal calculi. Tenbrink, T., Stell, J. G., Galton, A., & Wood,
Z., editors, COSIT, volume 8116 Lecture Notes Computer Science, pages 516536.
Springer, 2013. ISBN 978-3-319-01789-1. doi: 10.1007/978-3-319-01790-7. URL http:
//dx.doi.org/10.1007/978-3-319-01790-7.
Egenhofer, M. & Franzosa, R. (1991). Point-Set Topological Spatial Relations. International
Journal Geographical Information Systems, 5(2):161174, 1991.
Frank, A. (1991). Qualitative Spatial Reasoning Cardinal Directions. Kaindl, H., editor, Proc. 7th sterreichische Artificial-Intelligence-Tagung, pages 157167. Springer,
1991.
Freksa, C. (1992). Using orientation information qualitative spatial reasoning. Frank,
A. U., Campari, I., & Formentini, U., editors, Theories methods spatio-temporal
reasoning geographic space, volume 639 Lecture Notes Comput. Sci., pages 162
178. Springer, 1992.
Gantner, Z., Westphal, M., & Wlfl, S. (2008). GQR - Fast Reasoner Binary Qualitative Constraint Calculi. Proc. AAAI-08 Workshop Spatial Temporal
Reasoning, 2008.
Grtzer, G. (1979). Universal Algebra. Springer-Verlag, New York, NY, second edition,
1979.
Hobbs, J. R. (1985). Granularity. Proceedings Ninth International Joint Conference Artificial Intelligence, 1985.
Huang, J. (2012). Compactness implications qualitative spatial temporal
reasoning. Brewka, G., Eiter, T., & McIlraith, S. A., editors, Principles Knowledge
Representation Reasoning: Proceedings Thirteenth International Conference,
KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press, 2012. ISBN 978-1-57735-560-1.
URL http://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4494.
Isli, A. & Cohn, A. G. (2000). new approach cyclic ordering 2D orientations using
ternary relation algebras. Artificial Intelligence, 122(1-2):137187, 2000.
Ladkin, P. & Maddux, R. (1994). Binary Constraint Problems. J. ACM, 41(3):435469,
1994.
Lee, J. H. (2014). complexity reasoning relative directions. Schaub, T.,
Friedrich, G., & OSullivan, B., editors, ECAI 2014 - 21st European Conference
305

fiMossakowski & Moratz

Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic - Including Prestigious Applications Intelligent Systems (PAIS 2014), volume 263 Frontiers Artificial Intelligence Applications, pages 507512. IOS Press, 2014. ISBN 978-161499-418-3. doi: 10.3233/978-1-61499-419-0-507. URL http://dx.doi.org/10.3233/
978-1-61499-419-0-507.
Li, J. J., Kowalski, T., Renz, J., & Li, S. (2008). Combining binary constraint networks
qualitative reasoning. Ghallab, M., Spyropoulos, C. D., Fakotakis, N., & Avouris,
N. M., editors, ECAI 2008 - 18th European Conference Artificial Intelligence, Patras,
Greece, July 21-25, 2008, Proceedings, volume 178 Frontiers Artificial Intelligence
Applications, pages 515519. IOS Press, 2008. ISBN 978-1-58603-891-5. doi: 10.3233/
978-1-58603-891-5-515. URL http://dx.doi.org/10.3233/978-1-58603-891-5-515.
Ligozat, G. (1993). Qualitative triangulation spatial reasoning. Frank, A. U. & Campari, I., editors, Proc. International Conference Spatial Information Theory., volume
716 Lecture Notes Comput. Sci., pages 5468. Springer, 1993.
Ligozat, G. (1998). Reasoning Cardinal Directions. J. Vis. Lang. Comput., 9(1):
2344, 1998.
Ligozat, G. (2005). Categorical Methods Qualitative Reasoning: Case Weak
Representations. Cohn, A. G. & Mark, D. M., editors, Proc. COSIT, volume 3693
Lecture Notes Comput. Sci., pages 265282. Springer, 2005.
Ligozat, G. & Renz, J. (2004). Qualitative Calculus? General Framework.
Zhang, C., Guesgen, H. W., & Yeap, W.-K., editors, Proc. PRICAI-04, pages 5364,
2004.
Ligozat, G. (2011). Qualitative Spatial Temporal Reasoning.
9781848212527,9781118601457.

Wiley, 2011.

ISBN

Lcke, D. (2012). Qualitative Spatial Reasoning Relative Orientation: Question
Consistency. PhD thesis, University Bremen, 2012. http://elib.suub.uni-bremen.
de/edocs/00102632-1.pdf.
Mackworth, A. K. (1977). Consistency Networks Relations. Artif. Intell., 8:99118,
1977.
Maddux, R. (2006). Relation Algebras. Stud. Logic Found. Math. Elsevier Science, 2006.
Montanari, U. (1974). Networks constraints: Fundamental properties applications
picture processing. Inf. Sci., 7:95132, 1974.
Moratz, R. (2006). Representing Relative Direction Binary Relation Oriented Points.
Brewka, G., Coradeschi, S., Perini, A., & Traverso, P., editors, Proc. ECAI-06,
volume 141 Frontiers Artificial Intelligence Applications, pages 407411. IOS
Press, 2006.
Moratz, R., Renz, J., & Wolter, D. (2000). Qualitative Spatial Reasoning Line
Segments. Proc. ECAI 2000, pages 234238, 2000.
306

fiRelations Spatial Calculi

Moratz, R., Lcke, D., & Mossakowski, T. (2009). Oriented straight line segment algebra:
Qualitative spatial reasoning oriented objects. CoRR, abs/0912.5533, 2009. URL
http://arxiv.org/abs/0912.5533.
Moratz, R., Lcke, D., & Mossakowski, T. (2011). condensed semantics qualitative
spatial reasoning oriented straight line segments. Artif. Intell., 175(16-17):2099
2127, 2011.
Mossakowski, T., Schrder, L., & Wlfl, S. (2006). categorical perspective qualitative
constraint calculi. Wlfl, S. & Mossakowski, T., editors, Qualitative Constraint Calculi
- Application Integration. Workshop KI 2006, pages 2839, 2006.
Mossakowski, T. & Moratz, R. (2012). Qualitative reasoning relative direction
oriented points. Artificial Intelligence, 180-181(0):34 45, 2012.
Nebel, B. & Wlfl, S., editors (2009). AAAI Spring Symposium Benchmarking Qualitative Spatial Temporal Reasoning Systems. AAAI Technical Report SS-09-02, 2009.
Randell, D. A. & Cohn, A. G. (1989). Modelling topological metrical properties
physical processes. Brachman, R. J., Levesque, H. J., & Reiter, R., editors, Proc.
KR-89, pages 357368. Morgan Kaufmann, 1989.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992). spatial logic based regions
connection. Nebel, B., Rich, C., & Swartout, W., editors, Proc. KR-92, pages
165176. Morgan Kaufmann, 1992.
Renz, J. & Ligozat, G. (2005). Weak Composition Qualitative Spatial Temporal
Reasoning. van Beek, P., editor, Proc. CP-05, volume 3709 Lecture Notes
Comput. Sci., pages 534548. Springer, 2005.
Renz, J. & Mitra, D. (2004). Qualitative Direction Calculi Arbitrary Granularity.
Zhang, C., Guesgen, H. W., & Yeap, W.-K., editors, Proc. PRICAI-04, volume 3157
Lecture Notes Comput. Sci., pages 6574. Springer, September 2004.
Renz, J. & Nebel, B. (1999). Complexity Qualitative Spatial Reasoning:
Maximal Tractable Fragment Region Connection Calculus. Artificial Intelligence,
108(1-2):69123, 1999.
Renz, J. & Nebel, B. (2007). Qualitative Spatial Reasoning Using Constraint Calculi.
Aiello, M., Pratt-Hartmann, I., & van Benthem, J., editors, Handbook Spatial Logics,
pages 161215. Springer, 2007.
Scivos, A. & Nebel, B. (2004). finest class: natural point-based ternary
calculus qualitative spatial reasoning. Freksa, C., Knauff, M., Brckner, B. K.,
Nebel, B., & T.Barkowski, editors, Spatial Cognition, volume 3343 Lecture Notes
Comput. Sci., pages 283303. Springer, 2004.
van Beek, P. & Manchak, D. W. (1996). design experimental analysis algorithms
temporal reasoning. J. Artif. Intell. Res., 4:118, 1996.
307

fiMossakowski & Moratz

Wallgrn, J. O., Frommberger, L., Wolter, D., Dylla, F., & Freksa, C. (2006). Qualitative
Spatial Representation Reasoning SparQ-Toolbox. Barkowsky, T., Knauff,
M., Ligozat, G., & Montello, D. R., editors, Spatial Cognition, volume 4387 Lecture
Notes Comput. Sci., pages 3958. Springer, 2006.
Wallgrn, J. O., Frommberger, L., Dylla, F., & Wolter, D. (2009). SparQ User Manual
V0.7. User manual, University Bremen, January 2009.
Westphal, M. & Wlfl, S. (2009). Qualitative CSP, finite CSP, SAT: Comparing methods
qualitative constraint-based reasoning. Boutilier, C., editor, IJCAI, pages 628633,
2009.
Wolter, D. & Lee, J. H. (2010). Qualitative reasoning directional relations. Artificial
Intelligence, 174(18):14981507, 2010. doi: 10.1016/j.artint.2010.09.004.
Worboys, M. F. & Clementini, E. (2001). Integration Imperfect Spatial Information.
Journal Visual Languages Computing, 12:6180, 2001.

308

fiJournal Artificial Intelligence Research 54 (2015) 123158

Submitted 04/15; published 09/15

Achieving Goals Quickly Using Real-time Search:
Experimental Results Video Games
Scott Kiesel
Ethan Burns
Wheeler Ruml

skiesel cs.unh.edu
eaburns cs.unh.edu
ruml cs.unh.edu

Department Computer Science
University New Hampshire
Durham, NH 03824 USA

Abstract
real-time domains video games, planning happens concurrently execution planning algorithm strictly bounded amount time must
return next action agent execute. explore use real-time heuristic
search two benchmark domains inspired video games. Unlike classic benchmarks
grid pathfinding sliding tile puzzle, new domains feature exogenous change
directed state space graphs. consider setting planning acting
concurrent use natural objective minimizing goal achievement time. Using
classic benchmarks new domains, investigate several enhancements
leading real-time search algorithm, LSS-LRTA*. show experimentally 1)
better plan action use dynamically sized lookahead, 2) A*-based
lookahead cause undesirable actions selected, 3) on-line de-biasing
heuristic lead improved performance. hope work encourages future research
applying real-time search dynamic domains.

1. Introduction
many applications, desirable agent achieve assigned task quickly
possible. Consider common example navigation video game. user
selects destination character move to, expect character begin moving
immediately arrive destination soon possible. suggests planning strategy featuring concurrent planning execution. area real-time heuristic
search developed address problem. Algorithms class perform short
planning episodes limited provided real-time bound, finding partial solutions
beginning execution complete plan goal found. solution
quality search time traditional heuristic search metrics, real-time heuristic search
algorithms usually compared length trajectories execute.
recent work real-time heuristic search focused grid pathfinding problems
simplicity. important, grid pathfinding characteristics
exist search problems: search space undirected small enough
easily fit memory. explore use real-time heuristic search two additional
domains closely reflect features dynamic application domains, robotics.
One platform-based pathfinding domain proposed Burns, Ruml, (2013b),
novel domain call traffic problem, featuring navigation
c
2015
AI Access Foundation. rights reserved.

fiKiesel, Burns, & Ruml

field moving obstacles. Unlike traditional grid pathfinding problem used evaluate
real-time search, benchmarks dynamics large state spaces form
directed graphs.
addition evaluating real-time heuristic search new domains, introduce three
modifications LSS-LRTA* (Koenig & Sun, 2008), among state-of-the-art
real-time heuristic search algorithms. First, show LSS-LRTA*, executes
multiple actions per planning episode, improved executing single action
time. Second, become standard practice construct local search space
real-time search using partial A* search. show that, care taken
compare search nodes correctly, agent may execute unnecessary actions. Third, show
applying on-line de-biasing heuristic used search significantly reduce
overall goal achievement time. Together, modifications easily applied
improve overall performance agent controlled real-time heuristic search
algorithm. Videos illustrating new domains discussed algorithms provided
on-line (Kiesel, Burns, & Ruml, 2015b) well described Appendix B.
Instead comparing techniques based solely solution length convergence time,
evaluate new methods comparing goal achievement timesthe time
problem issued goal achieved. metric follows naturally
benchmark domains allows us easily compare real-time search algorithms offline planning techniques A*. results show A*, performs optimally
respect number expansions required produce optimal solution,
easily outperformed one cares goal achievement time. hope
work methodology encourage future research applying real-time search
dynamic domains.

2. Previous Work
much work area real-time search since initially proposed
Korf (1990). section review real-time search algorithms relevant
study. (Additional related algorithms reviewed Section 7.)
2.1 LRTA*
Many real-time search algorithms considered agent-centered agent performs
bounded amount lookahead search rooted current state acting. Since
size lookahead search bounded, agent respect real-time constraints
restricting lookahead completed time real-time limit
reached. seminal paper, Korf (1990) presents Learning Real-time A* (LRTA*),
complete, agent-centered, real-time search algorithm. select next action perform,
LRTA* uses action costs estimate cost-to-goal, heuristic value,
states resulting applying current applicable actions; chooses execute
action lowest estimated cost-to-goal.
LRTA* estimates heuristic value states two different ways. First, state
never visited before, uses depth-bounded, depth-first lookahead search.
estimated cost state minimum f value among leaves lookahead
search, f cost-so-far (notated g) plus estimated cost-to-goal (notated
124

fiAchieving Goals Quickly Using Real-time Search

LSS-LRTA*(s, expansion limit)
1. goal reached
2.
perform expansion limit expansions best-first search f
3.
update heuristic values nodes CLOSED
4.
state OP EN lowest f
5.
start executing path
6.
OP EN {s}; clear CLOSED
Figure 1: Pseudocode LSS-LRTA*.

h). second way estimates cost learning. time LRTA* performs
search, learns updated heuristic value current state. state encountered
again, learned estimate used instead searching again. Korf (1990) proved
long states heuristic estimate increased move amount bounded
, agent never get infinite cycle, algorithm
complete. original algorithm, second best actions heuristic value used
update cost estimate current state agent moves.
2.2 LSS-LRTA*
Local Search Space Learning Real-time A* (LSS-LRTA*, Koenig & Sun, 2008) currently
one popular real-time search algorithms. LSS-LRTA* two big advantages
original LRTA*: much less variance lookahead times significantly learning. LRTA* large variance lookahead times because,
even depth limit, different searches expand different numbers
nodes due pruning. Instead using bounded depth-first search beneath successor state, LSS-LRTA* uses single A* search rooted agents current state.
A* search limited exact number nodes expand, significantly
less variance lookahead times. second advantage original LRTA*
learns updated heuristics states agent visited; LSS-LRTA* learns updated
heuristics every state expanded lookahead search. accomplished using
Dijkstras algorithm propagate accurate heuristic values fringe
lookahead search back interior agent moves. Koenig Sun showed
LSS-LRTA* find much cheaper solutions LRTA* even competitive
state-of-the-art incremental search, D*Lite (Koenig & Likhachev, 2002).
Another major difference LRTA* LSS-LRTA* agent moves.
LRTA*, lookahead, agent moves performing single action; LSS-LRTA*
agent moves way node fringe current lookahead search
lowest f value. result, agent performs many fewer lookahead searches
reaching goal. one concerned minimizing total number expansions,
may advantageous. However, see below, search execution allowed
happen parallel, movement method LSS-LRTA* actually detrimental
performance. Pseudocode LSS-LRTA* presented Figure 1.
125

fiKiesel, Burns, & Ruml

3. Evaluating Real-time Search Algorithms
Traditionally, real-time heuristic search algorithms evaluated using two criteria:
convergence time solution length. Convergence time measures number repeated
start-to-goal plans algorithm must execute learns optimal path
given start goal pair. useful comparing rate different
algorithms learn accurate heuristic values, seem useful practice;
agents rarely need repeatedly plan exactly start goal states. Often
one solution needed, algorithm finds better first solution preferred
even takes long time converge.
Solution length number actions executed achieve goal. real-time
search, planning action execution happen parallel, solution length
good proxy amount time real-time agent given problem
goal actually achieved. downside simply using solution length,
however, makes comparison offline techniques unfair. example,
comparing algorithms solely solution length, technique perform better
optimal search like A*. But, practice, A* may best method solve
problem. agent using A* could spend long time planning finally begins
executing optimal path, agent using real-time algorithm may start executing
long path right away, consequently arrive goal first.
3.1 Goal Achievement Time
Recently, Hernandez, Baier, Uras, Koenig (2012) introduced game time model
evaluating real-time heuristic search algorithms. game time model, time divided
uniform intervals. interval, agent three choices: search,
execute action, search execute parallel. objective game
time model agent move initial state goal state using fewest
time intervals. advantage game time model allows comparisons
real-time algorithms search execute time step off-line
algorithms, like A*, search first execute search completed.
experiments, compare algorithms directly goal achievement time. Goal
achievement time (GAT) slight generalization game time model allows
real-valued times, fixed-size discrete time intervals. computed planning
time plus execution time minus time spent planning executing parallel:
goal achievement time = time planning + time executing time
Since benchmark domains used experiments natural definition
execution time (e.g., 15-puzzle, exactly much time needed slide tile?),
present results using variety different execution times. define execution time
number seconds required execute unit-cost action. call value unit
action duration; effectively converts action costs units
time. example,
8-way grid pathfinding problem diagonal edges cost 2, simply multiply
edge costs unit action duration convert seconds execution. large
unit action duration models agent moves slowly small unit action duration
models agent moves quickly, relative planning speed.
126

fiAchieving Goals Quickly Using Real-time Search

two following sections, present modifications LSS-LRTA* algorithm.
benefit modification evaluated using goal achievement time.

4. Lookahead Commitment
important step real-time search algorithm selecting far move agent
next phase planning begins. mentioned above, original LRTA*
algorithm agent moves single step, LSS-LRTA* agent moves
way frontier node local search space. Lustrek Bulitko (2006) reported
solution length increased switching single-step multi-step policy using
original LRTA* algorithm. unclear behavior would carry given
increased learning performed LSS-LRTA* use new goal achievement metric.
4.1 Single-step Dynamic Lookahead
implemented standard LSS-LRTA* well version executes single actions
like LRTA*. also implemented LSS-LRTA* using dynamic lookahead strategy
executes multiple actions leading current state selected state fringe
recent local search. dynamic lookahead, agent selects amount
lookahead search perform based duration currently-executing trajectory.
agent commits executing multiple actions, simply adjusts lookahead
fill entire execution time.
learning step, algorithm based LSS-LRTA* cannot simply search
real-time bound expires must leave time learning. account
use offline training determine speed agent searches.
fixed lookahead algorithms, necessary know maximum lookahead size
agent search minimum action execution time. found simply
running search algorithm different fixed lookahead settings representative
set training instances recording per-step search times. case dynamic
lookahead, agent must learn function mapping durations lookahead sizes.
agent commits trajectory requires time execute, must use
function find l(t), maximum lookahead size agent search time t. Note
that, data structures used search often non-linear-time operations,
function may linear. possible create conservative approximation l(t)
running algorithm representative set training instances large variety
fixed lookahead sizes. approximation l(t) selects largest lookahead size
always completed within time t.
4.2 Experimental Evaluation
compare different techniques platform pathfinding benchmark Burns et al.
(2013b). domain inspired popular platform-based video games like Super Mario
Bros. agent must find path, jumping platform platform, maze.
screenshot domain example instance shown Figure 2. Videos also
available on-line (Kiesel et al., 2015b) described Appendix B.
127

fiKiesel, Burns, & Ruml

Figure 2: screenshot problem instance platform path-finding domain (left),
zoomed-out image entire instance (right). knight must find
path starting location, maze, door (on right side
left image, center right image).

available actions different combinations controller keys may pressed
single iteration games main loop: left, right, jump. Left right move
knight respective directions (holding time never considered
search domain, movements would cancel out, leaving knight
place), jump button makes knight jump, applicable. knight jump
different heights holding jump button across multiple actions row
maximum 8. actions unit cost.
state state space contains x, position knight using double
precision floating point values, velocity direction (x velocity stored
determined solely left right actions), number remaining actions
pressing jump button add additional height jump, boolean stating
whether knight currently falling. knight moves speed 3.25 units per
frame horizontal direction, jumps speed 7 units per frame, simulate
gravity falling, 0.5 units per frame added knights downward velocity
maximum 12 units per frame.
benchmark natural fit real-time search algorithms, since agent must
decide action execute forced move due gravity. state space
platform domain directed, air agents actions
reversible. heuristic based visibility navigation (see Burns et al. details)
quite accurate except account players limited jumping
height. C++ source code available GitHub (Kiesel, Burns, & Ruml, 2015a).
128

fiAchieving Goals Quickly Using Real-time Search

factor optimal GAT (log10)

platform
3

multi-step (LSS-LRTA*)
single-step
dynamic

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 3: LSS-LRTA*: multi-step, single-step, dynamic lookahead.
experiments run Core2 duo E8500 3.16 GHz 8GB RAM running Ubuntu
10.04.
experiments used 25 test instances created using level generator described
Burns et al. (2013b), maze instance unique random
start goal location. used offline training techniques described learn
amount time required perform different amounts lookahead search.
offline training, generated additional 25 training instances. lookahead values
used 1, 5, 10, 20, 50, 100, 200, 400, 800, 1000, 1500, 2000, 3000, 4000, 8000 10000,
16000, 32000, 48000, 64000, 128000, 196000, 256000, 512000. algorithms use
fixed-size lookahead, lookahead value selected choosing largest lookahead size
mean step time training instances within single action duration.
none lookahead values fast enough fit within single action time given
action duration, data reported. implementation used mean step time
instead maximum step time, latter usually large due rare, slow
steps. attribute outliers occasional, unpredictable overhead system-related
subroutine calls memory allocation. suspect issue would go away
true real-time operating system used, operations perform predictable
computations, domain-specific optimized implementation used. (Unfortunately,
developing optimized implementations would made much difficult
perform thorough scientific comparisons.)
Figure 3 shows comparison different techniques LSS-LRTA* algorithm.
axis shows goal achievement time factor optimal goal achievement
time instance. optimal goal achievement time computed GAT
optimal solution planning time taken account. One could imagine oracle
able instantly provide optimal set actions execute. plot,
129

fiKiesel, Burns, & Ruml

h
3

3

2

2

1

1

0

g=2
f=5

g=1
f=4

g=2
f=4

g=3
f=5

g=4
f=5

g=5
f=6

g=6
f=6

g=1
f=4



g=1
f=3

g=2
f=4

g=3
f=4

g=4
f=5

g=2
f=5

g=1
f=4

g=2
f=4

g=3
f=5

g=4
f=5

g=5
f=6

g=6
f=6

Figure 4: Example heuristic error f layers.
axis shown log10 scale. consider variety action durations, shown
x axis, also log10 scale. Smaller action durations represent agent move
relatively quickly, spending lot time planning make small decreases solution
cost may worth time. larger values, agent moves slowly,
may worth planning execute cheaper paths. unit action duration used
limit number expansions performed iteration.
point plot shows mean goal achievement time 25 test instances
solved algorithms given factor optimal goal achievement time
action duration. value log10 (0) = 1 indicates given algorithm optimal
time. Error bars show 95% confidence intervals means.
plot, clear multi-step approach (standard LSS-LRTA*) performed
worse single-step dynamic lookahead variants. likely
multi-step technique commits many actions little bit planningthe
amount planning single-step variant uses commit one action.
unit action duration increased 1 second, algorithms started perform
similarly. However, single-step dynamic lookahead still appear perform slightly
better. Note 0.02 seconds per frame game platform
domain derived, values greater log10 (0.02) 1.7 represent agent
moves unusually slow pace.
also important note small unit action durations, one algorithm may perform better another, unit action duration increases,
relationship inverts. small unit action durations, much time
search performed.

5. A*-based Lookahead
standard LSS-LRTA*, lookahead search A*-based, nodes expanded f
order. searching, agent moves node open list lowest f
value. may seem intuitively reasonable, show choice
problematic, see remedied.
130

fiAchieving Goals Quickly Using Real-time Search







Figure 5: f -layered lookahead.
5.1 Heuristic Error
crux problem f -based lookahead doesnt account heuristic error.
admissible heuristic used compute f is, definition, low-biased, f typically
optimistically underestimate true solution cost node.
heuristic error, nodes f value actually lead toward goal node.
Figure 4 shows example using simple grid pathfinding problem. figure, agent
located cell labeled goal node denoted star. admissible
h values column grid; listed across top
columns. g f values shown cell. Cells f = 4 bold, rest
light gray. see nodes equivalent f values form elliptical rings around
start node. heuristic search literature, referred f layers.
nodes f layer closer goal node, many nodes layer
notsome nodes f layer even exactly away goal.
simple problem, optimal solution move agent right goal reached,
however, 7 nodes f = 4, 2 nodes along optimal path;
nodes not, f value heuristic error. agent
move random node f = 4, chances following optimal
path goal.
One way alleviate problem use second criterion breaking ties among
nodes f value. common tie breaker favor nodes lower h values
as, according heuristic, nodes closer goal. see, Figure 4
among nodes f = 4 layer, one lowest h value (h = 1) actually
along optimal path. LSS-LRTA* tie breaking insufficient, LSSLRTA* stops lookahead, may generated nodes largest f layer.
node h = 1 generated, even tie breaking, agent
led astray.
5.2 Incomplete f Layers
incomplete f layers cause problems too. Recall LSS-LRTA*, agent
moves node front open list. low-h tie breaking used order
expansions local search space, best nodes first f layer open list
actually expanded first open list comes time
agent move. Figure 5 shows problem diagrammatically. before, agent
node labeled goal denoted star. ellipse represents different
f layer, shaded portions show closed nodes, darker shading denotes nodes larger
131

fiKiesel, Burns, & Ruml

h(s)

{


heuristic error

(a)
g()


h()

{


h(s) g() + h()

heuristic error

(b)
g()




{

^h(s) g() + h() + d()

h()
heuristic error d()

(c)
Figure 6: (a) standard heuristic error. (b) updated heuristic error.
(c) Using updated heuristic accounting heuristic error.

f values, dotted lines surround nodes open list. see, closed
nodes largest f values cap tip second-largest f layer. caused
low-h tie breaking first open nodes expanded added closed
list lowest h. nodes portion f layer
nearest goal. agent moves node open list lowest
f value, tie breaking low h, select node take best route
toward goal.
5.3 Improving Lookahead Search
demonstrated two problems: 1) heuristic error, f layers contain
large number nodes, many lead toward goal, 2) even good
tie breaking, LSS-LRTA* may miss good nodes considers partial f layers
deciding move. Next present two possible solutions problems.
first quite simple. choosing move, select node lowest
h value completely expanded f layer largest f value, next node
open. Figure 5, corresponds node labeled . call complete
technique, considers completely expanded f layers instead partially expanded,
incomplete layers.
second technique explicitly accounts heuristic errorit orders search
agents action selection f , less-biased estimate solution cost.
Ideally, would prefer use unbiased (and hence inadmissible) estimate accounts
attempts correct heuristic error. inadmissible heuristic could used,
note Thayer, Dionne, Ruml (2011) investigated use inadmissible heuristics
132

fiAchieving Goals Quickly Using Real-time Search

offline search adopt simple heuristic correction technique real-time
search. call estimate f. Like f , f attempts estimate solution cost
node search space. Unlike f , f explicitly biasedit lower bound. f
computed similarly f , however, attempts correct heuristic error adding
additional term:
f
error
z
}|
{ z }| {

f (n) = g(n) + h(n) + d(n)
average single-step error heuristic, additional term d(n)
corrects error adding back cost estimate d(n) steps estimated
remain n goal. Following Thayer et al. (2011), make simplifying
assumption error heuristic distributed evenly among actions
path node goal.
Distance estimates readily available many domains; tend
easy compute heuristic estimates (Thayer & Ruml, 2009). estimate single-step
heuristic error, use average difference f values expanded
node best child. difference accounts amount heuristic error due
single step parent node child. perfect heuristic, one
error, f values parent node best child would equalsome f
simply moved h g:
f (parent ) = f (child ), ideal case,
h(parent ) = h(child ) + c(parent , child ),
g(parent ) = g(child ) c(parent , child )
Since g known exactly, cost edge c(parent , child ), imperfect
heuristic difference f (child ) f (parent ) must caused error
heuristic step. Averaging differences gives us estimate .
Adapting technique real-time search requires subtlety. real-time search
algorithms like LSS-LRTA*, heuristic values nodes expanded lookahead search updated time agent moves. Figure 6a schematically depicts
error default heuristic value node S. error accrued distance
goal. lookahead (Figure 6b), updated heuristics accurate
originals based heuristic values nodes closer
goal, thus less heuristic error. Here, see node fringe
start state inherits updated heuristic value. Since g(), cost
, known exactly, error backed heuristic comes entirely
steps goal. Since closer goal, error less error
original heuristic value S.
computing f(S) real-time search, necessary account fact
error updated heuristic comes node . this, track value called
derr , distance heuristic error accrued, node, use
compute f. Initially, nodes without updated heuristic values derr (n) = d(n).
performing lookahead search, h updated backed-up h values before. h(n)
receives backed value originated node , set derr (n) = d(), since
error updated heuristic comes instance fringe node
133

fiKiesel, Burns, & Ruml

platform
GAT difference LSS-LRTA*

factor optimal GAT (log10)

platform
complete
incomplete (LSS-LRTA*)

3

2

1

0
-4

-3

-2

-1

unit action duration (log10)
(a)

0

0

-40

unit action duration (log10)
(b)

0

Figure 7: LSS-LRTA*: f-based lookahead f-based lookahead.
goal, distance n goal. updated heuristic, accounting
heuristic error, h(s) = g() + h() + derr (n) , g() + h() standard
heuristic backup derr (n) error correction (cf Figure 6b, derr (n) = d() due
update). demonstrated Figure 6c. new technique uses f order
expansions lookahead search LSS-LRTA*, moves agent toward
node open list lowest f value.
Figure 7 shows comparison three node selection techniques: standard incomplete f layer method LSS-LRTA*, complete f -layer method, approach
uses f (denoted fhat). better demonstrate problem standard approach,
plot shows results multi-step movement model commits entire path
current state fringe local search space lookahead. style
plot panel (a) Figure 3.
figure, see complete performed worse standard LSS-LRTA*
algorithm small action durations agent may time expand many
nodes thus ignoring expansions large effect. longer action durations,
however, performance improves complete becomes best performer right
side plot cheaper solutions preferred. clarify improvement
right side plot Figure 7 (a), included Figure 7 (b). plot
different y-axis highlights improvement comparing algorithm directly
incomplete version LSS-LRTA*. indicates using completed f layer
lead fewer extraneous actions gives cheaper solutions. Using f sort open
list lookahead searches performs much better two algorithms left
side plot, although begins perform slightly worse unit action duration
increased. likely inadmissibility f hinders ability find solutions
cheap found complete f layer variant.
134

fiAchieving Goals Quickly Using Real-time Search

factor optimal GAT (log10)

platform
3

LSS-LRTA*
single-step f

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 8: Comparison four new real-time techniques.
Dynamic-f(s, expansion limit)
1. goal reached
2.
perform expansion limit expansions best-first search f
3.
update heuristic values nodes CLOSED
4.
state OP EN lowest f
5.
6.
7.
8.

start executing path
execution time execution time reach
expansion limit number expansions possible within execution time
OP EN {s}; clear CLOSED
Figure 9: Pseudocode LSS-LRTA* dynamic lookahead using f.

Figure 8 shows results comparison four combinations single-step
versus dynamic lookahead f -based versus f-based node ordering. domain, f
dynamic lookahead tended give best goal achievement times portions
plot algorithms significant overlap (i.e., everywhere except
right-half plot).
Figure 7 Figure 8, ordering lookahead search f common
link best performance considered algorithms. Figure 7 demonstrates
initial intuition heuristic correction used real-time search. Figure 8 builds
idea adding dynamic look ahead proposed previous section yield
strongest algorithm seen far. present pseudocode Figure 9
dynamic-f method.
135

fiKiesel, Burns, & Ruml

factor optimal GAT (log10)

platform
3

LSS-LRTA*

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 10: Comparison four new real-time techniques weak heuristic.
5.3.1 Heuristic Accuracy
could argued effects dynamic lookahead heuristic correction distorted strong heuristic visibility graph. short experiment, replace
visibility graph heuristic platform domain much weaker euclidean distance
heuristic. solve instances using heuristic, decrease overall size
instances 50x50 25x25.
Even decreased instance size, dynamic algorithms able solve
25 instances unit action durations. algorithms, unit action
durations, solved 13 full 25 instances. Figure 10 shows results
experiment, data point represents mean instances solved
algorithms unit action duration (between 13 17 instances).
see ranking algorithms remains algorithms
regardless weakening heuristic. note unit action duration 0.001
seconds rise data. attributed solving larger subset
25 instances containing difficult instances, thus increasing GAT.
5.3.2 CPU Usage
paper focus minimizing CPU time assumes time
allocated search may utilized, still instructive compare CPU usage
new techniques. one might imagine, tradeoff CPU usage GAT.
inverse relationship seen examining algorithm utilizes search
time. Single step policies execute search every action goal reached.
behavior increase overall demand CPU. However, using single step
policy, goal achieved quickly using multi-step policy. Dynamic
136

fiAchieving Goals Quickly Using Real-time Search

platform

platform
single-step f
LSS-LRTA*
dynamic fhat

100

50

-4

-3

-2

-1

unit action duration (log10)
(a)

single-step f
dynamic fhat
LSS-LRTA*

0.9

fraction search time used

total raw cpu time (seconds)

150

0.6

0.3

-4

0

-3

-2

-1

unit action duration (log10)
(b)

0

Figure 11: Comparison LSS-LRTA*, single-step LSS-LRTA* dynamic f terms
cpu usage.

lookahead greedily use search time becomes available, arrive
goal quickly.
Figure 11, search time plotted. Figure 11 (a) shows raw cpu time used
algorithm platform domain. axis raw cpu time seconds
x axis log10 unit action duration. seen plot, dynamic f uses
small amount planning time. attributed dynamic f finding
goal quickly. single-step policy able find goal quickly
LSS-LRTA*(see Figure 3), requires cpu time unit action duration
increases.
Figure 11 (b) plots raw cpu time divided goal achievement time. provides
idea active CPU execution algorithms. important
keep mind quick goal achievement times small denominator, causing
utilization appear higher raw cpu time longer goal achievement
time. reduced utilization longer action durations likely dynamic f able
find goals quickly using small number iterations remainder execution
overlapping planning time.
5.3.3 Implementation Details
f technique requires information standard LSS-LRTA*, slightly
greater storage requirements. note, however, experiments
run memory issues, optimize implementation reduce
memory requirements.
137

fiKiesel, Burns, & Ruml

implementation uses two different types nodes: persistent transient. Persistent nodes form agents memory states encountered past lookahead
searches: connectivity, learned heuristic values, distance estimates
heuristic error accrued. Transient nodes exist single round lookahead search
h-cost learning; akin traditional search nodes used in, example, A*
search, however include information required order search f.
persistent node, store information connectivity search graph.
includes set predecessors successors node costs associated edges. store predecessors assume undirected search graph
predecessor function easily computable. predecessors successors
computed lazily. predecessor added first expanded, entire set successors populated first time node expanded. successor nodes,
also store cost reversing edge (which matters case edge costs
symmetric) operator used generate successor. Persistent nodes also
store learned heuristic estimate cached values original h estimates
node, would otherwise need computed time needed.
Transient nodes hold additional information needed perform best first search ordered
f. First, transient node pointer corresponding persistent node.
addition, transient node g-cost, f -cost, f-cost computed
single lookahead search node persists. information contained
transient nodes are: pointer best parent current lookahead search;
nodes index open list (which implemented array-based binary heap), needed
updating nodes position heap encountered via better path; two
booleans used h-cost learning easily determine node already
h value updated determine closed list. detail, refer
reader source code freely available GitHub (Kiesel et al., 2015a).
differences information stored f normal LSS-LRTA*
implementation latter store estimates persistent node set,
store f values transient nodes. information exactly same.
5.3.4 Theoretical Evaluation
prove that, certain conditions, modifications LSS-LRTA* retain
completeness property algorithm. learning cause f converge
f . begin, assume heuristic admissible state space finite.
Proposition 1. Following spirit Korf completeness proof LRTA*, note that,
search algorithm incomplete finite state space, must exist nodes
visited infinite number times. 1
goal show nodes cannot exist.
Lemma 1. dynamic f searches within finite set nodes D, h values
nodes interior reach fixed point (remain static unchanging)
finite time T1 , least long search remains within D.
1. note that, contrary assumptions previous work, algorithm need actually enter
loop, trajectory may vary non-repeating way, digits conjectured to.

138

fiAchieving Goals Quickly Using Real-time Search

Proof.
1. h updated using update rule LSS-LRTA*, h
values state space non-decreasing via Koenig Suns (2008) Theorem 1
regarding LSS-LRTA*.
2. Note h values nodes fringe remain static
h-value learning updates heuristic nodes interior LSS. Every
update learning step obeys h(p) = max(h(p), c(p, bc) + h(bc)), bc
best child p one lowest f value. Thus every h(p) value
sum numbers drawn set C contains: edge costs, set
fringe h values, set initial h(p) values. finite
number costs C, update h(p) must larger minimum positive
difference two possible sums costs drawn C. Thus increases
bounded constant.
3. Similarly step 1, h values remain admissible via Theorem 2 Koenig Sun
regarding LSS-LRTA*, cannot rise true cost go.
4. steps 1, 2, 3, must time T1 , h values change
long search remains within set D.
Lemma 2. search visits finite set nodes infinite number times,
exists time search visits nodes D.
Proof. Consider LSSes formed iteration learning step
parents inherit h values child lowest f . Considering pairs
nodes, two cases: a) two nodes LSS infinite number
times number search iterations approaches infinity, b) nodes
LSS finite number times. pairs case (b), note
must exist time, , never LSS, otherwise two
nodes would covered case (a) instead.
Lemma 3. dynamic f searches within set D, one-step heuristic error
goes 0 time T2 stays least long search remains within
D.
Proof.
1. Consider pairs nodes LSS . Lemma 1,
exists T1 h values converged. T1 , know
h(p) = c(p, bc) + h(bc).
2. Note average, across internal nodes LSS associated
best children, differences f values parents best children.
step 1, f (p) = f (bc), = 0 holds time T2 T1 .
Lemma 4. cannot exist set nodes dynamic f visits infinitely often.
Proof.
1. sake contradiction, let set nodes part LSS
infinite number times.
2. Lemma 3, become 0 time T2 .
139

fiKiesel, Burns, & Ruml

3. time T2 = 0, h = h, f = h, dynamic f behave like LSS-LRTA*
(dynamic-sized lookahead makes difference LSS-LRTA*s theoretical properties
hold without regard lookahead size).
4. LSS-LRTA* complete (Koenig & Sun, 2008, Thm. 3), search eventually
reach goal. contradicts 1, dynamic f visit set states infinitely
often.
Note that, dynamic f escape potential set D, described step 1
proof Lemma 4, circulate within another set nodes infinite time,
would equaled instead. Also, oscillate two
sets, would defined union sets.
Theorem 1. finite search space admissible h, dynamic f eventually reach
goal.
Proof. Proof contradiction:
1. Assume search never reaches goal. dynamic f goes goal
LSS, means goal never LSS.
2. finite space, search never sees goal, must visit states
infinite number times.
3. Lemma 4, dynamic f exhibit behavior.
4. Thus, using dynamic f retains completeness LSS-LRTA*.

6. Comparison Off-line Techniques
previous sections, explored modifications LSS-LRTA* algorithm improve
ability achieve goals quickly. LSS-LRTA*s performance improved applying
heuristic correction either using single-step movement policy using dynamicallysized lookahead searches. section evaluate performance algorithms
standard offline techniques. included three extra domains final
comparison, 15-puzzle, grid pathfinding, novel domain call traffic
domain. 15-puzzle, used 94 instances Korfs 100 instances (Korf, 1985)
implementation A* able solve using 6GB memory limit. grid
pathfinding, ran orz100d grid map video game Dragon Age: Origins
(Sturtevant, 2012). map, shown Figure 12, includes mix open space mazelike areas narrow corridors. used 25 start end locations longest
optimal path lengths scenarios Sturtevant (2012). completeness, results
best performing algorithms random selection 10 additional maps presented
Appendix A.
140

fiAchieving Goals Quickly Using Real-time Search

Figure 12: Grid path-finding video game map.
6.1 Traffic Domain
traffic domain new domain inspired part video games Frogger2 .
goal traffic domain navigate grid given goal location
avoiding obstacles, many motion. state includes x, location
agent x, location obstacle (In implementation, locations
obstacles stored state; instead, store states current time,
obstacle locations computed based initial location, velocity, time).
Time divided discrete intervals called ticks, agent move one four
cardinal directions remain still tick. Obstacles horizontal
vertical velocities either -1, 0, 1 cell per-tick respective direction.
Obstacle locations known agent time future. obstacle
hits edge grid bounces off, reversing velocity direction hit.
Obstacles simply pass other. search space directed, time
ticking forward and, obstacles move, agent cannot perform action move
obstacles back previous locations. domain especially well-suited
real-time techniques agent must action ready execute tick
world transitions obstacles move.
eliminate dead end states (real-time algorithms incomplete presence
dead ends), result executing action agent intersection
obstacle, agent teleported back start state location (at time t=0).
especially important offline algorithms expect begin execution initial
2. domain similar 2011 ICAPS International Probabilistic Planning Competition domain
called crossing traffic constructed MDP also POMDP. version
domain deterministic fully observable.

141

fiKiesel, Burns, & Ruml

15-puzzle
factor optimal GAT (log10)

factor optimal GAT (log10)

platform
2.4

1.6

0.8

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

orz100d

0

traffic

3

factor optimal GAT (log10)

factor optimal GAT (log10)

-1

unit action duration (log10)

2

1

0

3

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 13: Comparison off-line techniques.

state despite passage time planning. experiments, generated 25
random solvable instances consisting 100x100 grids 5,000 obstacles placed randomly
random velocities. start location upper-left corner grid
goal location lower-right corner. average solution length 211.24 moves.
Videos showing traffic domain available on-line (Kiesel et al., 2015b) discussed
Appendix B.
142

0

fiAchieving Goals Quickly Using Real-time Search

6.2 Results
allowing planning execution take place simultaneously, possible
improve offline techniques delay execution planning finished. assess
this, compared best-performing variants LSS-LRTA* A* algorithm
called Speedy (Thayer & Ruml, 2009). Speedy best-first greedy search d(n),
estimated number actions remaining goal. tends find poor plans quickly,
providing informative contrast A*. Figure 13 shows results comparison,
log10 factor optimal goal achievement time axis unit
action duration, log10 scale, x axis.
f-based search dynamic lookahead gave best goal achievement times
platform, 15-puzzle, traffic domains. Speedy strong performer grid
pathfinding domain traffic domain. surprising domains
based grid navigation. traffic domain Speedy able quickly find collision free
path (avoiding additional cost overhead). grid pathfinding A* actually lowest
goal achievement times unit action durations, f dynamic lookahead
tied A* except fastest unit action duration. results grid
pathfinding consistent results presented Hernandez et al. (2012),
best performer good A*. (In study, best performer
TBA*, dont compare work directed graphs.)
likely A* solve grid pathfinding problems quickly, thus short
planning times, still finds optimal solutions.
Even though A* performs well, applicable real-time constraint present
action needs returned within bound. A*-based real-time algorithms give
similar results infinite lookahead, although would waste time learning.

7. Related Work
large body work relating real-time search. section review
work discuss relation techniques presented previous
sections.
7.1 Pruning Dead States
f -LRTA* (Sturtevant & Bulitko, 2011) extension LSS-LRTA* RIBS (Sturtevant,
Bulitko, & Bjornsson, 2010), combining h-cost learning g-cost learning. g-cost
learning enables algorithm label states dead-ends redundant. Determining
types states using basic algorithm relies underlying undirected graph.
arises requirement compute cost successor parent. undirected
graph simply reverse operator, case directed graph, would
require call either heuristic call additional search determine cost
edge. consultation Sturtevant, created small modification include
reverse edge costs easily computable. However, practice
perform well directed graph domains. conclude work needed
adapt ideas behind f -LRTA* directed graphs.
143

fiKiesel, Burns, & Ruml

Sharon, Sturtevant, Felner (2013) introduce technique pruning dead states
real-time agent-centered search. work detects two types dead states: expendable
swamp. determined considering reachability shortest paths
local neighborhood state. dead state pruning shown lead speedups 8-way grid pathfinding, applicable certain domains. Let us first consider
undirected domains considered paper. sliding tile puzzle, example,
expendable swamp states exist locally. neighbors state reach
another neighbor without passing considering local neighborhood.
Without reachability, shortest paths exist, locally expendable swamp states
would pruned. also note grid navigation problem considers
movement four cardinal directions, pruning could occur either reason.
directed graph, predecessors must also considered local neighborhood. case, expendable state would state whose predecessors reach
ss successors without traversing s. Similarly, swamp state would state
whose predecessors shortest paths successors pass s.
traffic domain, example, state contains time, predecessors state
time t, would time 1 successors time + 1.
way traverse state 1 state time + 1 without traversing
state time t. state state local neighborhood time t,
paths exist predecessors successors pass s.
expendable swamp states would pruned domain.
7.2 Minimizing Search Effort
GAT model assumes search occur action execution, appropriate situations separate processor cores responsible planning versus
managing execution. processor resources scarce shared among many tasks,
one may want minimize search effort even execution. Bulitko, Lustrek, Schaeffer,
Bjornsson, Sigmundarson discuss methods dynamically adjusting real-time search
lookahead order minimize search effort still selecting good actions. contrast,
discussed section 5.3.2, dynamic f attempts use available execution time
perform much search possible.
7.3 Time-bounded A*
Time-bounded A* (TBA*, Bjornsson, Bulitko, & Sturtevant, 2009) non-agent-centered
real-time search algorithm. Instead performing bounded amount lookahead search
agents current state, TBA* maintains single A* search agents initial
starting state goal state. iteration, fixed number expansions done
single search agent attempts move toward promising node
search frontier. Since agent may already moved away initial state
previous iterations, A* vacillates many different paths, agents
current state may along current best path. occurs, agent backtracks
toward initial state current best path. experiments, Bjornsson
et al. showed that, grid pathfinding benchmarks, TBA* requires fewer iterations find
quality paths real-time algorithms LRTA*.
144

fiAchieving Goals Quickly Using Real-time Search

mentioned briefly above, Hernandez et al. (2012) found TBA* best
technique optimizing goal achievement time grid pathfinding problems fully-known
grids. state performance A*. experiments, compare TBA*, consider domains form directed graphs,
TBA* works undirected graphs, due agents need backtrack.
suspect, however, dynamic lookahead f technique would quite competitive
TBA*, also matched performance A* grid pathfinding problems
able greatly outperform A* domains.
7.4 Avoiding Depressions Real-time Heuristic Search
Real-time search algorithms become temporarily stuck heuristic local minimum
extended period search execution time (Sturtevant & Bulitko, 2014). agent
typically wander around heuristic minimum learns heuristic
area inaccurate corrects it. behavior results long solutions
aesthetically undesirable.
daLSS-LRTA* daRTAA* (Hernandez & Baier, 2012) attempt actively avoid
escape heuristic depressions. Instead selecting node lowest f value, daLSSLRTA* selects node along frontier whose heuristic value changed least.
daRTAA* similar uses simpler learning phase borrowed Real-Time Adaptive
A* (Koenig & Likhachev, 2006). RTAA* daRTAA* update entire interior
local search f value node open list best f value.
implemented daLSS-LRTA* daRTAA* compared standard
LSS-LRTA* well multi-step dynamic lookahead variants LSS-LRTA* using
f. results comparison shown Figure 14. results grid pathfinding
problem (orz100d) agree results Hernandez Baier (2012) show using
depression avoidance techniques help improve performance (for example, compare LSSLRTA* daLSS-LRTA*). Also, daLSS-LRTA* daRTAA* outperform standard
multi-step variant LSS-LRTA* using f. Dynamic lookahead f clearly gives best
performance fastest unit action duration 0.0001. platform
traffic domains, however, daLSS-LRTA* appears slightly worse LSS-LRTA*,
15-puzzle depression avoidance appears little effect. Overall, found dynamic
f dominate techniques.
Similar Figure 10, see spike 0.001 unit action duration platform
domain. attributed jump number instances solved
algorithms 0.0001 0.001. larger set contains difficult instances
increases factor optimal GAT.
may interesting future work combine depression avoidance dynamic lookahead, especially grid pathfinding domains.
7.5 Weighted Real-time Heuristic Search
Weighted A* (wA*, Pohl, 1970) popular heuristic search algorithm proceeds like
A*, orders nodes open list using f (n) = g(n) + w h(n), w 1.
w increases, search becomes greedy often find solutions faster A*.
solutions may optimal, guaranteed within factor w
145

fiKiesel, Burns, & Ruml

15-puzzle
factor optimal GAT (log10)

factor optimal GAT (log10)

platform

1.8

1.2

0.6

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

orz100d

0

traffic

3

factor optimal GAT (log10)

factor optimal GAT (log10)

-1

unit action duration (log10)

2

1

1.8

1.2

0.6

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 14: Depression avoidance real-time heuristic search.

optimal solution cost. Rivera, Baier, Hernandez (2012) recently showed variant wA*
real-time search called wLSS-LRTA*. One obvious way implement real-time variant
wA* would simply multiply heuristic value w 1 lookahead search
LSS-LRTA*, however, wLSS-LRTA* this. Instead, wLSS-LRTA* multiplies
edge weights w learning phase LSS-LRTA*. update rule becomes:
h(n) minmopen w g(n, m) + h(m), g(n, m) cost node n
updated node open list lookahead search.
146

0

fiAchieving Goals Quickly Using Real-time Search

15-puzzle

2.4

factor optimal GAT (log10)

factor optimal GAT (log10)

platform

1.6

0.8

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

0

traffic
factor optimal GAT (log10)

factor optimal GAT (log10)

orz100d
3

w=16 LSS-LRTA*
w=1 LSS-LRTA*
dynamic fhat
w=2 LSS-LRTA*

2

-1

unit action duration (log10)

1

2.4

1.6

0.8

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 15: Weighted real-time heuristic search.

Rivera et al. (2012) show using increased weight wLSS-LRTA* lead
lower-cost solutions. point that, admissible heuristics lower bounds,
inflating heuristic factor w may make heuristic accurate.
reasoning behind f technique. difference wLSS-LRTA* uses weight
inflate g portion updated heuristic whereas f technique adds correction
based h portion updated heuristic. would argue f approach
makes sense error causing heuristic underestimate come
perfectly-known g portion update, estimated h portion.
147

0

fiKiesel, Burns, & Ruml

implemented wLSS-LRTA* compared standard multi-step dynamic
lookahead variants LSS-LRTA* using f. results comparison shown
Figure 15. results grid pathfinding problem (orz100d) tend agree
Rivera et al. (2012): using larger weights wLSS-LRTA* increase performance.
trend seems depend, however, unit action duration; noticeable
actions fast. Also, grid pathfinding, wLSS-LRTA* outperforms standard
multi-step variant LSS-LRTA* using f, dynamic lookahead f clearly gives
best performance fastest unit action duration 0.0001. However,
platform, 15-puzzle, traffic domains found almost opposite true! Dynamic
f still bestit nearly dominates techniques. fairest comparison
LSS-LRTA* using f (with statically sized lookahead) provides better performance
wLSS-LRTA*, dominating wLSS-LRTA* weight greater 1 15-puzzle
traffic problems, increasing weight wLSS-LRTA* either effect
makes performance worse. traffic domain weights greater 1, wLSSLRTA* unable solve problems lookahead 1, greater lookahead
values tried (including lookahead 2) slow meet real-time deadline
unit action duration 0.0001, thus data point x=0.0001 either
algorithms. Based results, conclude using parameter-free f
technique explicitly attempt account heuristic error recommended approach
weighting edge costs learning user-specified parameter.
7.6 FRIT
Follow Reconnect Ideal Tree, FRIT (Rivera, Illanes, Baier, & Hernandez,
2013), takes another approach dealing heuristic minima real-time search. Rather
applying heuristic learning updating escape local minimum, FRIT instead
tries follow ideal tree state space. ideal tree represents family paths
connect states search space goal state. also thought
implicitly represented heuristic.
ideal tree explored following heuristic greedily operators
applicable states, heuristic suggests operator inapplicable. simple
example following Manhattan distance heuristic grid pathfinding domain
encountering obstacle. inapplicable operator suggested, tree becomes
disconnected agent must reconnect tree. done performing
local search around agents current state state believed Ideal Tree
found. agent moves state continues on. resulting behavior
grid pathfinding domains appear similar wall following.
modifications required make FRIT real-time algorithm. local
search find state Ideal Tree bounded size state space,
rather time bound expansion limit. authors suggest techniques
bounding local search experiments, allowed FRIT thought
offline allowed much time needed looking reconnect Ideal
Tree. also used breadth first search local search algorithm.
implemented FRIT compared standard LSS-LRTA* multistep dynamic versions LSS-LRTA* using f. results shown Figure 16.
148

fiAchieving Goals Quickly Using Real-time Search

factor optimal GAT (log10)

orz100d
3

LSS-LRTA*
dynamic fhat

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 16: Comparison offline FRIT using breadth first search.
present results grid pathfinding problem (orz100d). FRIT able
solve easier instances platform traffic domains within five minute
timeout. sliding tile puzzle domain, unclear adapt algorithm
domain. naive approach results branching factor 44 poor results.
grid pathfinding, even treating FRIT offline algorithm penalizing
search time final goal achievement time, performs worse three variants
LSS-LRTA* presented. exception unit action duration small,
point FRIT competitive algorithms (ignoring search time).
7.7 FALCONS
Furcy Koenig (2000) present two modifications LRTA* speed convergence
time. noticed breaking ties favor successors smaller f -values LRTA*
would converge quickly. also point also use tie-breaking criteria select successor move to, convergence occurs even faster. two modifications yield two new algorithms: Tie Breaking LRTA* (TB-LRA*) FAst Learning
CONverging Search (FALCONS).
Figure 17 compare original LRTA*, TB-LRTA* FALCONS.
domains three algorithms perform worse newer LSS-LRTA* modified
versions LSS-LRTA*.
7.8 RTA*
Korf (1990) proposed LRTA* seminal paper also another algorithm
simply called Real Time A* (RTA*). RTA*, unlike counterpart LRTA*, focused
solving problem getting start state goal state once. LRTA*
149

fiKiesel, Burns, & Ruml

15-puzzle
factor optimal GAT (log10)

factor optimal GAT (log10)

platform

2.4

1.6

0.8

0

3

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

0

traffic
factor optimal GAT (log10)

orz100d
factor optimal GAT (log10)

-1

unit action duration (log10)

4

3

2

1

4

3

LSS-LRTA*
dynamic fhat
2

1

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 17: Comparison LRTA*, TBLRTA* FALCONS.

proven converge optimal heuristic values successive trials. RTA*s learning policy
guarantee convergence heuristic values practice find solutions
quickly LRTA*.
Figure 18 compare RTA*. include LRTA* plots well show
tradeoff convergence initial goal achievement time RTA* makes. RTA*s
lookahead based bounded depth first search, run time difficult predict.
experiments ran RTA* lookahead depths {1, 5, 10, 20, 50, 100, 200, 400,
800, 1000, 1500, 2000, 3000, 4000, 8000, 10000, 16000} chose largest depth
150

0

fiAchieving Goals Quickly Using Real-time Search

15-puzzle
factor optimal GAT (log10)

factor optimal GAT (log10)

platform

3

2

1

4
3
2
1
0

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

0

traffic
factor optimal GAT (log10)

factor optimal GAT (log10)

orz100d
5
4

LSS-LRTA*
dynamic fhat

3

-1

unit action duration (log10)

2
1
0

3

LSS-LRTA*
dynamic fhat

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 18: Comparison RTA*.

instances solved within timeout. interesting note 15-puzzle
depth 1500 able used, platform traffic lookahead 10
could used. attribute platform traffic graphy domains,
tiles fewer cycles. extreme case grid pathfinding orz100d map
maximum lookahead 5 could solve instances within timeout. supplementary
comparison, also provide line RTA* using A* lookahead instead depth first
search grid pathfinding domain. expansion limit 4000 largest lookahead
151

0

fiKiesel, Burns, & Ruml

size solved instances. four plots Figure 18, see newer
algorithms outperform RTA* domains.
7.9 Bugsy
Bugsy (Burns et al., 2013b) real-time search, off-line algorithm
explicitly attempts optimize utility function given linear combination search
time solution cost. solution cost specified units time, Bugsy
explicitly attempt minimize goal achievement time appropriately weighting search
execution times given units. off-line algorithm
optimize goal achievement time objective, interesting see Bugsy
compares real-time algorithms. Since performs global search, may better able
optimize cost, inherently less efficient, cannot plan execute parallel.
Figure 19 shows results. Bugsy tended lowest goal achievement times
domains except traffic domain, dynamic lookahead f method nearly
dominated approaches. However, domains except 15-puzzle, advantage Bugsy small. conclude that, full solution found upfront,
off-line methods like Bugsy often given best results. agent must
respect real-time constraints, however, dynamic lookahead f technique algorithm
choice.
may possible create new algorithm incorporates ideas Bugsy
real-time search. Bugsy proceeds like A*, orders open list utility estimate
u(n) = wf f (n) + wt time(n), time(n) estimate time search
take reach best solution beneath node n (for details, see Burns et al., 2013b).
difficulty incorporating ideas Bugsy real-time search Bugsys utility
estimate assumes none planning time, time(n), occur parallel
execution time f (n) (recall cost units time optimizing goal achievement
time). real-time search, true. solution cost units time
planning happens execution, optimizing cost seems appropriate.

8. Conclusion
paper considered real-time search context minimizing goal achievement
time concurrent planning execution possible. optimizing goal achievement time, important consider tradeoff searching executing.
presented three modifications LSS-LRTA*: 1) taking single steps instead moving
way fringe lookahead search, 2) use multiple steps, dynamically increase
lookahead size match execution time trajectory, 3) using f correct
bias heuristic. evaluated techniques plain LSS-LRTA*, A*,
Speedy, daRTAA*, daLSS-LRTA*, wLSS-LRTA*, FRIT, TBLRTA*, FALCONS, LRTA*,
RTA*, Bugsy four domains. addition 15-puzzle grid pathfinding
domains, classic heuristic search benchmarks, used two video-game-inspired
domains: platform domain new traffic domain.
showed committing single actions time give better performance
using traditional multiple action approach. demonstrated using
multiple action technique even better performing single steps amount
152

fiAchieving Goals Quickly Using Real-time Search

15-puzzle
factor optimal GAT (log10)

factor optimal GAT (log10)

platform
2.4

1.6

0.8

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

orz100d

0

traffic

3

factor optimal GAT (log10)

factor optimal GAT (log10)

-1

unit action duration (log10)

2

1

1.8

1.2

0.6

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 19: Comparison Bugsy.

lookahead search dynamically adjusted use time available execution
currently-executing multi-step trajectory. pointed possible reasons
using A*-based lookahead search may lead poor performance showed f could
used fix issues. Overall, combination dynamically sized lookahead
f gave best performance compared previous real-time techniques. hope
work spur research applying real-time heuristic search dynamic
domains.
153

0

fiKiesel, Burns, & Ruml

9. Acknowledgments
work supported part NSF (grants 0812141 1150068), DARPA
CSSG program (grant D11AP00242), University New Hampshire Dissertation
Year Fellowship. preliminary version work published Burns, Kiesel,
Ruml (2013a).

Appendix A. Grid Pathfinding Results Additional Maps
following random sample 10 maps Sturtevants repository top
performing algorithms plotted. plots similar included paper
log10 factor optimal goal achievement time y-axis log10 unit action
duration x-axis.

0.4

factor optimal GAT (log10)

factor optimal GAT (log10)

arena2
0.6

single-step f

0.2

0.24

single-step f
0.16

0.08

0

0
-3

-4

-2

-1

unit action duration (log10)

0

-4

-3

-2

-1

unit action duration (log10)

0

factor optimal GAT (log10)

factor optimal GAT (log10)

orz201d
0.024

single-step f
daRTAA*

0.016

0.008

0

0.12

single-step f

0.08

0.04

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

154

-3

-2

-1

unit action duration (log10)

0

fi1.2

factor optimal GAT (log10)

factor optimal GAT (log10)

Achieving Goals Quickly Using Real-time Search

single-step f
daRTAA*
daLSS-LRTA*

0.8

0.4

0.024

single-step f
0.016

0.008

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

factor optimal GAT (log10)

factor optimal GAT (log10)

isound1
daLSS-LRTA*
LSS-LRTA*

single-step f

-3

-2

-1

unit action duration (log10)

0

0.15

single-step f

0.1

0.05

0
-4

-3

2.4

-1

0

-4

w=2 LSS-LRTA*
LSS-LRTA*
dynamic fhat

1.6

-3

daRTAA*
daLSS-LRTA*

0.8

-2

-1

unit action duration (log10)

0

rmtst01
LSS-LRTA*
daRTAA*

factor optimal GAT (log10)

factor optimal GAT (log10)

-2

unit action duration (log10)

dynamic fhat

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

155

-3

-2

-1

unit action duration (log10)

0

fiKiesel, Burns, & Ruml

Appendix B. Video Descriptions
describe videos available on-line (Kiesel et al., 2015b).
B.1 Platform Videos
videos numbered 1-10 playlist show algorithms solving Platform domain.
B.1.1 Random Instance
Videos 1-7 provide example random instance Platform platform domain
solved various algorithm configurations.
Video 1 LSS-LRTA* 1,000 node lookahead using multi-step policy.
video see algorithm get stuck local heuristic minima actively trying
update heuristic estimates states minimum.
Video 2 LSS-LRTA* 1,000 node lookahead using single-step policy.
video see algorithm traverse smaller sized local minima much quickly
still become stuck short larger local minimum around 9 seconds.
Video 3 LSS-LRTA* using dynamically sized lookahead. Initially, gets stuck
inside local minimum, similar LSS-LRTA* static lookahead, soon able
escape learning increasing lookahead sizes.
Video 4 LSS-LRTA* using dynamically sized lookahead heuristic correction.
quickly able escape various heuristic minima way goal.
Video 5 Speedy. video see algorithm able find solution
quickly starts moving almost instantly. next 2 minutes video spent
executing highly suboptimal solution.
Video 6 A*. visualize planning time complete solution found
A* begins moving (roughly 1 minute 15 seconds, see video 7). solution
optimal quickly gets agent initial position goal.
Video 7 comparison LSS-LRTA* 1,000 node lookahead, LSS-LRTA*
dynamically sized lookahead heuristic correction, A* Speedy. video,
planning time visualized.
B.1.2 Ladder Instance
Videos 8-10 demonstrate extreme example heuristic minimum. visibility graph
heuristic assumes agent able jump infinitely high, creating large local minimum
agent must learn way of.
Video 8 shows LSS-LRTA* 1,000 node lookahead multi-step policy struggling climb platform ladder.
Video 9 shows LSS-LRTA* 1,000 node lookahead single-step lookahead
policy quickly climbing ladder struggling end.
Video 10 shows optimal example climb ladder using A*.
156

fiAchieving Goals Quickly Using Real-time Search

B.2 Traffic Videos
remaining videos illustrate traffic domain, highly dynamic domain many
moving obstacles agent must avoid. videos provided visualization domain algorithms trying solve.
Video 11 shows example optimal solution found A* never collides
obstacle.
Video 12 shows LSS-LRTA* 1,000 node lookahead solving problem.
video around 20 seconds, agent wanders situation collision occurs
transported back start state.

References
Bjornsson, Y., Bulitko, V., & Sturtevant, N. (2009). TBA*: time-bounded A*. Proceedings
Twenty-first International Joint Conference Artificial Intelligence (IJCAI),
pp. 431436.
Bulitko, V., Lustrek, M., Schaeffer, J., Bjornsson, Y., & Sigmundarson, S. (2008). Dynamic
control real-time heuristic search. Journal Artificial Intelligence Research, 32,
419452.
Burns, E., Kiesel, S., & Ruml, W. (2013a). Experimental real-time heuristic search results
video game. Proceedings Sixth Annual Symposium Combinatorial
Search (SoCS).
Burns, E., Ruml, W., & Do, M. B. (2013b). Heuristic search time matters. Journal
Artificial Intelligence Research (JAIR), 47, 697740.
Furcy, D., & Koenig, S. (2000). Speeding convergence real-time search.
Proceedings National Conference Artificial Intelligence (AAAI), pp. 891
897.
Hernandez, C., & Baier, J. (2012). Avoiding escaping depressions real-time heuristic
search. Journal Artificial Intelligence Research (JAIR), 43, 523570.
Hernandez, C., Baier, J., Uras, T., & Koenig, S. (2012). Time-bounded adaptive A*.
Proceedings Eleventh International Joint Conference Autonomous Agents
Multiagent Systems (AAMAS).
Kiesel, S., Burns, E., & Ruml, W. (2015a).
Research code heuristic search.
https://github.com/eaburns/search. Accessed September 2, 2015.
Kiesel, S., Burns, E., & Ruml, W. (2015b). Videos achieving goals quickly using realtime search. http://bit.ly/1bW3Ey8. Accessed September 2, 2015.
Koenig, S., & Likhachev, M. (2002). lite. Proceedings Eighteenth National
Conference Artificial Intelligence (AAAI), pp. 476483.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. Proceedings International Joint Conference Autonomous Agents Multiagent Systems (AAMAS).
Koenig, S., & Sun, X. (2008). Comparing real-time incremental heuristic search
real-time situated agents. Journal Autonomous Agents Multi-Agent Systems,
pp. 18(3):313341.
157

fiKiesel, Burns, & Ruml

Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Korf, R. E. (1990). Real-time heuristic search. Artificial intelligence, 42 (2-3), 189211.
Lustrek, M., & Bulitko, V. (2006). Lookahead pathology real-time path-finding.
Proceedings National Conference Artificial Intelligence (AAAI), Workshop
Learning Search, pp. 108114.
Pohl, I. (1970). Heuristic search viewed path finding graph. Artificial Intelligence,
1, 193204.
Rivera, N., Illanes, L., Baier, J. A., & Hernandez, C. (2013). Reconnecting ideal
tree: alternative heuristic learning real-time search. Proceedings
Sixth International Symposium Combinatorial Search (SoCS).
Rivera, N., Baier, J. A., & Hernandez, C. (2012). Weighted real-time heuristic search.
Proceedings Twelfth International Conference Autonomous Agents
Multiagent Systems (AAMAS).
Sharon, G., Sturtevant, N. R., & Felner, A. (2013). Online detection dead states
real-time agent-centered search. Proceedings Sixth Annual Symposium
Combinatorial Search (SoCS).
Sturtevant, N. (2012). Benchmarks grid-based pathfinding. Transactions Computational Intelligence AI Games (TCIAIG), 4 (2), 144 148.
Sturtevant, N., & Bulitko, V. (2014). Reaching goal real-time heuristic search: Scrubbing behavior unavoidable. Proceedings Seventh Annual Symposium
Combinatorial Search (SoCS).
Sturtevant, N. R., & Bulitko, V. (2011). Learning going whence
came: h-and g-cost learning real-time heuristic search. Proceedings
Twenty-Second International Joint Conference Artificial Intelligence (IJCAI), pp.
365370.
Sturtevant, N. R., Bulitko, V., & Bjornsson, Y. (2010). learning agent-centered search.
Proceedings Ninth International Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 333340. International Foundation Autonomous
Agents Multiagent Systems.
Thayer, J. T., Dionne, A., & Ruml, W. (2011). Learning inadmissible heuristics
search. Proceedings Twenty-first International Conference Automated
Planning Scheduling (ICAPS).
Thayer, J. T., & Ruml, W. (2009). Using distance estimates heuristic search. Proceedings Nineteenth International Conference Automated Planning Scheduling (ICAPS).

158

fiJournal Artificial Intelligence Research 59 (2015) 59-82

Submitted 05/15; published 09/15

Solving #SAT MaxSAT Dynamic Programming
Sigve Hortemo Sther
Jan Arne Telle
Martin Vatshelle

sigve.sether@ii.uib.no
telle@ii.uib.no
martin.vatshelle@ii.uib.no

Department Informatics, University Bergen
Bergen, Norway

Abstract
look dynamic programming algorithms propositional model counting, also
called #SAT, MaxSAT. Tools graph structure theory, particular treewidth,
used successfully identify tractable cases many subfields AI, including
SAT, Constraint Satisfaction Problems (CSP), Bayesian reasoning, planning.
paper attack #SAT MaxSAT using similar, modern, graph structure
tools. tractable cases include formulas whose class incidence graphs
unbounded treewidth also unbounded clique-width. show algorithms extend
previous results MaxSAT #SAT achieved dynamic programming along
structural decompositions incidence graph input formula. present
limited experimental results, comparing implementations algorithms state-of-the-art
#SAT MaxSAT solvers, proof concept warrants research.

1. Introduction
propositional satisfiability problem (SAT) fundamental problem computer science
AI. Many real-world applications planning, scheduling, formal verification
encoded SAT SAT solver used decide exists solution.
decide many solutions are, propositional model counting problem (#SAT),
finds number satisfying assignments, could useful. solutions,
may interesting know close get solution. propositional
formula encoded Conjunctive Normal Form (CNF) may solved maximum
satisfiability problem (MaxSAT), finds maximum number clauses
satisfied assignment. paper investigate classes CNF formulas
two problems, #SAT MaxSAT, solved polynomial time. Tools
graph structure theory, particular treewidth, used successfully identify
tractable cases many subfields AI, including SAT, Constraint Satisfaction Problems
(CSP), Bayesian reasoning, planning (Bacchus, Dalmao, & Pitassi, 2003; Darwiche,
2001; Fischer, Makowsky, & Ravve, 2008; Samer & Szeider, 2010). paper
attack #SAT MaxSAT using similar, modern, graph structure tools.
tractable cases include formulas whose class incidence graphs unbounded
treewidth also unbounded clique-width.
#SAT MaxSAT significantly harder simply deciding satisfying
assignment exists. #SAT #P-hard (Garey & Johnson, 1979) even restricted
Horn 2-CNF formulas, monotone 2-CNF formulas (Roth, 1996). MaxSAT
NP-hard even restricted Horn 2-CNF formulas (Jaumard & Simeone, 1987),
c
2015
AI Access Foundation. rights reserved.

fiSther, Telle & Vatshelle

2-CNF formulas variable appears 3 times (Raman, Ravikumar, & Rao,
1998). problems become tractable certain structural restrictions obtained
bounding width parameters graphs associated formulas (Fischer, Makowsky, & Ravve,
2008; Ganian, Hlineny, & Obdrzalek, 2013; Samer & Szeider, 2010; Szeider, 2003).
work present inspired recent results work Paulusma, Slivovsky,
Szeider (2013) also work Slivovsky Szeider (2013) showing #SAT
solvable polynomial time incidence graph1 I(F ) input formula F
bounded modular treewidth, strongly, bounded symmetric clique-width.
tractability results work dynamic programming along decomposition I(F ).
two steps involved: (1) find good decomposition, (2) perform dynamic
programming along decomposition. goal fast runtime, usually
expressed function known graph width parameter incidence graph I(F )
formula F , like tree-width. Step (1) solved known graph algorithm
computing decomposition low (tree-)width, step (2) solves #SAT MaxSAT
dynamic programming runtime expressed terms (tree-)width k
decomposition.
algorithms give paper also work dynamic programming along
decomposition, slightly different framework. Since solving graph
theoretic problem, expressing runtime graph theoretic parameter may limitation.
Therefore, strategy develop framework based following strategy
(A) consider, #SAT MaxSAT, amount information needed combine
solutions subproblems global solutions,
(B) define notion good decompositions based parameter minimizes
information,
(C) design dynamic programming algorithm along decomposition runtime
expressed parameter
work Paulusma et al. (2013) Slivovsky Szeider (2013)
two assignments considered equivalent satisfy set clauses.
carrying (A) #SAT MaxSAT led us concept ps-value
CNF formula. Let us define give intuitive explanation. subset C clauses
CNF formula F called projection satisfiable complete assignment
satisfying every clause C satisfying clause C. ps-value F
number projection satisfiable subsets clauses. Let us consider connection
dynamic programming, general applies optimal solution found
combining optimal solutions certain subproblems. #SAT MaxSAT
subproblems, least cases consider, take form subformula F induced
subset clauses variables, i.e. first remove F variables
remove clauses S. Consider simplicity two subproblems FS FS
defined complement S. combining solutions FS FS , order
1. I(F ) bipartite incidence graph clauses F one hand variables F
hand. Information positive negative occurrences variables encoded I(F )
sometimes signed directed version used includes also information.

60

fiSolving #SAT MaxSAT Dynamic Programming

find solutions F , seems clear must consider number cases least
big ps-values two disjoint subformulas crossing S, i.e.
subformulas obtained removing clauses variables S, removing
clauses variables S. See Figure 2 example.
find literature study ps-value CNF formulas, start
asking characterization formulas low ps-value. led concept
mim-value I(F ), size maximum induced matching I(F ),
induced matching subset edges property edge graph
incident one edge . Note value much lower
size maximum matching, e.g. complete bipartite graph mim-value 1. show
ps-value F upper bounded number clauses F raised power
mim-value I(F ), plus 1. CNF formula F I(F ) mim-value 1
interpretation result straightforward: clauses totally ordered
two clauses C < C 0 variables occurring C subset variables occurring
C 0 , implication number subsets clauses
complete assignment satisfies exactly subset number clauses plus 1.
Families CNF formulas small ps-value algorithmic interest,
paper continue part (B) strategy, focus
decompose CNF formula F based concept ps-value. common way decompose
mathematical object recursively partition ground set two parts, giving
binary tree whose root represents ground set whose leaves bijectively mapped
elements ground set. Taking ground set F set containing
clauses variables, decompose F , words binary tree
whose leaves 1-1 correspondence variables clauses. node binary
tree represents subset X variables clauses leaves subtree.
decomposition trees good efficiently solving #SAT MaxSAT? accordance
discussion part (A) answer good decomposition trees
subformulas crossing X X, X defined node
tree, low ps-value. See Figure 2 example. define informal notion
precisely use concept branch decomposition ground set formula
cut function ps-value formulas crossing cut. Branch decompositions
standard notion graph matroid theory, originating work Robertson
Seymour graph minors (Robertson & Seymour, 1991). way arrive
definition ps-width CNF formula F , decompositions F achieve
ps-width. important note formula ps-value exponential
formula size ps-width polynomial, general class formulas low
ps-width much larger class formulas low ps-value.
finish strategy, must carry part (C) show solve #SAT
MaxSAT dynamic programming along branch decomposition formula,
express runtime function ps-width. complicated, dynamic
programming everything defined properly simply becomes exercise
brute-force computation sufficient necessary information, technical
quite tedious. leads following theorem.
Theorem 2. Given formula F n variables clauses, decomposition F
ps-width k, solve #SAT weighted MaxSAT time O(k 3 m(m + n)).
61

fiSther, Telle & Vatshelle

Thus, given decomposition ps-width k polynomially-bounded
number variables n clauses formula, get polynomial-time algorithms.
Let us compare result strongest previous result direction, namely
work Slivovsky Szeider (2013) #SAT. algorithm takes input branch
decomposition vertex set I(F ), ground set F ,
evaluates runtime cut function call index. show cut function
closely related symmetric clique-width scw given decomposition, giving runtime
(n + m)O(scw) . Considering clique-width cw given decomposition runtime
cw
work Slivovsky Szeider (2013) becomes (n + m)O(2 ) since symmetric clique-width
clique-width related essentially tight inequalities 0.5cw scw 2cw (Courcelle,
2004). algorithm thus polynomial-time algorithm given decomposition
constantly bounded scw. result Theorem 2 encompasses this, since Corollary 1
ties ps-width mim-width work Vatshelle (2012) shows mim-width upper
bounded clique-width, see also work Rao (2008) symmetric clique-width,
decomposition I(F ) constantly bounded (symmetric) clique-width also
polynomially bounded ps-width. way, given decomposition assumed input
work Slivovsky Szeider (2013), algorithm Theorem 2 runtime
O(m3cw s), cw clique-width given decomposition.
work Brault-Baron, Capelli, Mengel (2014), appearing preliminary
presentation results (Sther, Telle, & Vatshelle, 2014), argued framework
behind Theorem 2 gives uniform explanation tractability results #SAT
literature, particular using dynamic programming based structural decompositions
incidence graph. work Brault-Baron et al. (2014) also goes beyond this, giving
polynomial-time algorithm, dynamic programming, solve #SAT -acyclic
CNF formulas, exactly formulas whose incidence graphs chordal bipartite.
show formulas bounded ps-width incidence
graphs bounded mim-width. See Figure 1 gives overview results
paper papers.
Using concept mim-width graphs, introduced thesis Vatshelle (2012),
connection ps-value mim-value alluded earlier, show rich class
formulas, including classes unbounded clique-width, polynomially bounded ps-width
thus covered Theorem 2. Firstly, holds classes formulas incidence
graphs represented intersection graphs certain objects, like interval graphs
(Belmonte & Vatshelle, 2013). Secondly, holds also much larger class bipartite
graphs achieved taking bigraph bipartizations intersection graphs, obtained
imposing bipartition vertex set keeping edges partition classes.
bigraph bipartizations studied previously, particular interval
bigraphs. interval bigraphs contain bipartite permutation graphs, latter
graphs shown unbounded clique-width (Brandstadt & Lozin, 2003). See
Figure 1.
Let us discuss step (1), finding good decomposition. Note Theorem 2 assumes
input formula given along decomposition ps-width k. value k
need optimal, heuristic finding reasonable branch decomposition could
used practice. Computing decompositions optimal ps-width probably doable
62

fiSolving #SAT MaxSAT Dynamic Programming

iden
inc

F

h F
ce grap

ps-width mk

#SAT poly. Paper A.
#SAT poly. Paper B.

chordal
bipartite

#SAT MaxSAT poly.
paper.

-acyclic

ps-width m2
MIM-width k
clique-width k
symmetric
clique-width k/2
modular
treewidth k/2

ps-width
k-trapezoid
bigraph

circular arc
bigraph
interval bigraph
bipartite
permutation

Figure 1: believe, argued work Brault-Baron et al. (2014), dynamic
programming approach working along structural decomposition solve #SAT
(or MaxSAT) polynomial time cannot go beyond green box. Paper
Brault-Baron et al. (2014) Paper B Slivovsky Szeider (2013).
left two dashed lines 4 classes graphs bound k/2 k
structural graph width parameter, 5 classes bipartite graphs.
right -acyclic CNF formulas 3 classes CNF formulas ps-width
varying linear number clauses m, m2 mk . arc
P Q formula F incidence graph I(F ) property P also
property Q. Hasse diagram, lack arc transitive closure
means relation provably hold.
polynomial-time, complexity question adressed paper. However,
able efficiently decide CNF formula certain linear structure guaranteeing
low ps-width. combining alternative definition interval bigraphs (Hell & Huang,
2004) fast recognition algorithm (Muller, 1997; Rafiey, 2012) arrive
following. Say CNF formula F interval ordering exists total ordering
variables clauses variable x occurring clause C, x appears
C variable also occurs C, C appears x x
occurs also clause them.
Theorem 6. Given formula F n variables clauses literals.
time O((m + n)mn) decide F interval ordering (yes iff I(F )
interval bigraph), yes solve #SAT weighted MaxSAT additional
runtime O(min{m2 , 4t }(m + n)m).
Formulas interval ordering precisely whose incidence graphs interval
bigraphs, Theorem 6 encompasses classes formulas whose incidence graphs
unbounded clique-width.
Could parts algorithms interest practical applications? Answering
question beyond scope present paper. However, performed limited
testing, particular formulas linear structure, simple proof concept.
code found online (Sther, Telle, & Vatshelle, 2015). designed
implemented heuristic step (1) finding good decomposition, case linear
63

fiSther, Telle & Vatshelle

one binary tree describing decomposition path attached leaves.
also implemented step (2) dynamic programming solving #SAT MaxSAT along
decompositions. run (1) followed (2) compare one
best MaxSAT solvers Max-SAT-2014 event SAT-2014 conference
latest version #SAT solver called sharpSAT (Thurley, 2006). solvers beat
implementation inputs, suprising since code include
techniques beyond algorithm. Nevertheless, able generate classes
CNF formulas interval orderings implementation far better.
lends support belief methods related ps-value warrants research
investigate could useful practice.
paper organized follows. Section 2 give formal definitions ps-value
ps-width CNF formula show central combinatorial lemma linking ps-value
formula size maximum induced matching incidence graph
formula. Section 3 present dynamic programming algorithms given formula
decomposition solves #SAT weighted MaxSAT, proving Theorem 2. Section 4
investigate classes formulas decompositions low ps-width, basically proving
correctness hierarchy presented Figure 1. Section 5 consider formulas
interval ordering prove Theorem 6. Section 6 present results
implementations testing. end Section 7 open problems.

2. Framework
consider propositional formulas Conjunctive Normal Form (CNF). literal
propositional variable negated variable, x x, clause set literals,
formula multiset clauses. formula F , cla(F ) denotes clauses F .
incidence graph formula F bipartite graph I(F ) vertex clause
variable, variable x adjacent clause C occurs. consider
input formulas I(F ) connected, otherwise would solve problems
separate components I(F ). clause C, lit(C) denotes set literals
C var(C) denotes
variables literals lit(C). formula F , var(F )

denotes union Ccla(F ) var(C). set X variables, assignment X
function : X {0, 1}. literal `, define (`) 1 (var(`)) ` negated
variable (` = x variable x) (var) otherwise (` = x variable
x). clause C said satisfied assignment exists least one literal
` lit(C) (`) = 1. clause assignment satisfy said
falsified . notice means empty clause falsified assignments.
formula satisfied assignment satisfies clauses cla(F ).
problem #SAT, given formula F , asks many distinct assignments var(F )
satisfy F . optimization problem weighted MaxSAT, given formula
P F weight
function w : cla(F ) N, asks assignment var(F ) maximizes C w(C)
C cla(F ) satisfied . problem MaxSAT asks maximum number satisfied
clauses achieved, equivalent weighted MaxSAT clauses weight
one. weighted MaxSAT, assume sum weights 2O(cla(F )) ,
thus summation weights time linear cla(F ).
64

fiSolving #SAT MaxSAT Dynamic Programming

set A, elements universe U denote elements U \ A,
universe usually given context.
2.1 Cut Formula
paper, solve MaxSAT #SAT use dynamic programming.
using divide conquer technique solve problem smaller
subformulas original formula F combine solutions smaller
formulas form solution entire formula F . Note however, solutions found
subformula depend interaction subformula remainder
formula. use following notation subformulas.
clause C set X variables, C|X denote clause {` C : var(`) X}.
say C|X clause C induced X. Unless otherwise specified, clauses mentioned
paper set cla(F ) (e.g., write C|X cla(F 0 ), still assume C
cla(F )). formula F subsets C cla(F ) X var(F ), say subformula
FC,X F induced C X formula consisting clauses {Ci |X : Ci C}.
is, FC,X formula get removing clauses C followed removing
literal variable X. set C clauses, denote C|X set {C|X : C C}.
clause, assignment set X variables, say assignment
induced X 0 X assignment |X 0 domain restricted X 0 .
formula F sets C cla(F ), X var(F ), = C X, call cut F
note breaks F four subformulas FC,X , FC,X , FC,X , FC,X . See Figure 2.
One important fact may observe definition clause C F satisfied
assignment var(F ), C (induced X X) satisfied
least one formulas cut F .
2.2 Projection Satisfiable Sets ps-value Formula
formula F assignment variables var(F ), denote sat(F, )
inclusion maximal set C cla(F ) clause C satisfied .
set C cla(F ) sat(F, ) = C variables var(F ), C
known projection (Kaski, Koivisto, & Nederlof, 2012; Slivovsky & Szeider, 2013)
say C projection satisfiable F . denote PS(F ) family projection
satisfiable sets F . is,
PS(F ) = {sat(F, ) : assignment entire set var(F )}.
cardinality set, |PS(F )|, referred ps-value F .
get grasp structure formulas low ps-value consider induced
matchings incidence graph formula. incidence graph formula F
bipartite graph I(F ) vertex clause variable, variable x adjacent
clause C occurs. induced matching graph subset edges
property edge graph incident one edge .
words, 3 vertices a, b, c, ab edge bc edge
exist edge cd . number edges called size induced matching.
following result provides upper bound ps-value formula terms
maximum size induced matching incidence graph.
65

fiSther, Telle & Vatshelle

Lemma 1. Let F CNF formula clause containing literals,
let k maximum size induced matching I(F ). |PS(F )|
min{|cla(F )|k + 1, 2tk }.
Proof. first argue |PS(F )| |cla(F )|k + 1. Let C PS(F ) Cf = cla(F ) \ C.
Thus, exists complete assignment clauses satisfied
Cf = cla(F ) \ sat(F, ). Since every variable var(F ) appears clause F
means |var(Cf ) unique assignment variables var(Cf ) satisfy
0
0
clause Cf . Let Cf Cf inclusion minimal set var(Cf ) = var(Cf ),
hence |var(Cf ) also unique assignment variables var(Cf ) satisfy
0
0
clause Cf . upper bound number different minimal Cf ,
0
C PS(F ), give upper bound |PS(F )|. every C Cf variable vC
0
0
appearing C clause Cf , otherwise Cf would minimal. Note
induced matching I(F ) containing edges vC , C. assumption,
0
induced matching k edges hence |Cf | k. easy show
induction k |cla(F )|k + 1 sets k clauses lemma
follows.
argue |PS(F )| 2tk . maximum induced matching size k
set C k clauses var(C) = var(F ). clause C C |var(C)| t,
|var(F )| = |var(C)| tk. 2|var(F )| assignments F ,
PS-value F upper bounded 2tk .
2.3 ps-width Formula
define branch decomposition formula F pair (T, ) rooted
binary tree bijective function leaves clauses variables
F . non-leaf nodes (also referred internal nodes) induce path,
say (T, ) linear branch decomposition. non-leaf node v , denote
(v) set {(l) : l leaf subtree rooted v}. Based this, say
decomposition (T, ) formula F induces certain cuts F , namely cuts defined (v)
node v .
formula F branch decomposition (T, ), node v , Fv denote
formula induced clauses cla(F ) \ (v) variables (v), Fv
denote formula complement sets; i.e. clauses (v) variables
var(F ) \ (v). words, (v) = C X C cla(F ) X var(F )
Fv = FC,X Fv = FC,X . simplify notation, node v branch
decomposition set C clauses denote C|v set C|var(Fv ) . define ps-value
cut (v)
ps((v)) = max{|P S(Fv )|, |P S(Fv )|}
define ps-width branch decomposition
psw(T, ) = max{ps((v)) : v node }
define ps-width formula F
psw(F ) = min{psw(T, ) : (T, ) branch decomposition F }
66

fiSolving #SAT MaxSAT Dynamic Programming

v
x4
x3 c4 x5 c2
x1

x2

c1

C
c1 = {x1 , x2 }
c3 = {x2 , x4 , x5 }

c3
FC,X = Fv

X

FC,X

FC,X

x1 x2

x3 x4
x5

FC,X = Fv

c2 = {x1 , x2 , x3 }
c4 = {x2 , x3 , x5 }

X

C

Figure 2: top branch decomposition formula F var(F ) = {x1 , x2 , x3 , x4 , x5 }
4 clauses cla(F ) = {c1 , c2 , c3 , c4 } given boxes. node
v tree defines cut (v) = C X C = {c1 , c3 } X =
{x1 , x2 }. 4 subformulas defined cut: FC,X , FC,X , FC,X , FC,X .
example, FC,X = {{x1 , x2 }, {x2 }} FC,X = {, {x4 , x5 }}.
Fv = FC,X Fv = FC,X projection satisfiable sets clauses PS(Fv ) =
{{c2 |v }, {c4 |v }, {c2 |v , c4 |v }} PS(Fv ) = {, {c3 |v }} ps-value cut
ps((v)) = max{|P S(Fv )|, |P S(Fv )|} = 3.

Note ps-value cut symmetric function. is, ps-value cut
equals ps-value cut S. See Figure 2 example.

3. Dynamic Programming MaxSAT #SAT
Given branch decomposition (T, ) CNF formula F n variables clauses
total size s, give algorithms solve MaxSAT #SAT F time
O(psw(T, )3 m(m + n)). algorithms strongly inspired work Slivovsky
Szeider (2013), order achieve runtime polynomial ps-width, also solve
MAXSAT, must make crucial changes. particular, must index dynamic
progranming tables PS-sets rather shapes used work Slivovsky
Szeider (2013).
Let us discuss special terminology used section. dynamic
programming section, combine partial solutions subformulas solutions
input formula F . improve readability introduce notation P 0 sat0
allows us refer directly clauses F , also working subformulas.
67

fiSther, Telle & Vatshelle

Thus, formula F branch decomposition (T, ), node v , induced
subformula Fv F , PS0 (Fv ) denote subsets clauses C cla(F ) \ (v)
PS(Fv ) = C|var(Fv ) . Similarly, assignment var(Fv ), sat0 (Fv , )
denote set clauses C cla(F ) \ (v) sat(Fv , ) = C|var(Fv ) . Note
|PS0 (Fv )| = |PS(Fv )| |sat0 (Fv , )| = |sat(Fv , )|. take liberty call also
sets projection satisfiable refer PS-sets text, clear
context mean clauses cla(F ) cla(Fv ).
Let us discuss implementation details. regard PS-sets boolean vectors
length |cla(F )|, assume identify clauses variables integer numbers. So,
checking clause PS-set done constant time, checking two PS-sets
equal done O(|cla(F )|) time. manage PS-sets, use binary trie
datastructure (Fredkin, 1960). add retrieve PS-set trie
O(|cla(F )|) time. Trying add PS-set trie already containing equivalent PS-set
alter content trie, tries contain distinct PS-sets.
retrieval element trie takes O(|cla(F )|) time, assigning distinct integer
PS-set time added trie, O(|cla(F )|)-time mapping
PS-sets distinct integers. used implicitly algorithms
say index PS-sets; implementing algorithm instead index
corresponding integer PS-set mapped to.
pre-processing step need following which, node v computes
sets projection satisfiable subsets clauses PS0 (Fv ) PS0 (Fv ) two crossing
subformulas Fv Fv .
Theorem 1. Given CNF formula F branch decomposition (T, ) ps-width k,
time O(k 2 m(m + n)) compute sets PS0 (Fv ) PS0 (Fv ) v .

Proof. notice node v children c1 c2 , express PS0 (Fv )


C1 PS0 (Fc1 ),
0
PS (Fv ) = (C1 C2 ) cla(Fv ) :
.
C2 PS0 (Fc2 )

Similarly, sibling parent p v , set PS0 (Fv ) expressed


Cp PS0 (Fp ),
0
PS (Fv ) = (Cp Cs ) cla(Fv ) :
.
Cs PS0 (Fs )
transforming recursive expressions dynamic programming algorithm,
done Procedure 1 Procedure 2 below, able calculate desired sets
long compute sets base cases PS0 (Fl ) l leaf , PS0 (Fr )
root r . However, formulas contain one variable, thus
easily construct set projection satisfiable clauses linear amount time
formulas. rest formulas, construct formulas using Procedure 1
Procedure 2. twice many nodes clauses
variables F , procedures run O(|cla(F )| + |var(F )|) times. run
algorithms, iterate k 2 pairs projection satisfiable sets,
constant number set operations might take O(|cla(F )|) time each. results
total runtime O(k 2 |cla(F )|(|cla(F )| + |var(F )|)) = O(k 2 m(m + n)) nodes
combined.
68

fiSolving #SAT MaxSAT Dynamic Programming

Procedure 1: Generating PS0 (Fv )
input: PS0 (Fc1 ) PS0 (Fc2 ) children c1 c2 v
branch decomposition
output: PS0 (Fv )
L empty trie projection satisfiable clause-sets
(C1 , C2 ) PS0 (Fc1 ) PS0 (Fc2 )
add (C1 C2 ) cla(Fv ) L
return L
Procedure 2: Generating PS0 (Fv )
input: PS0 (Fs ) PS0 (Fp ) sibling parent p v
branch decomposition
output: PS0 (Fv )
L empty trie projection satisfiable clause-sets
(Cs , Cp ) PS0 (Fs ) PS0 (Fp )
add (Cs Cp ) cla(Fv ) L
return L
move dynamic programming proper. first give algorithm
MaxSAT briefly describe changes necessary solving weighted MaxSAT
#SAT.
algorithm uses technique expectation introduced work Bui-Xuan,
Telle, Vatshelle (2010, 2011). partial solutions might good combined
certain partial solutions, bad combined others. technique
expectation categorize partial solutions interact, optimize selection
partial solutions based expectation interaction occurs. dynamic
programming algorithm MaxSAT, apply technique making expectations
cut regarding set clauses satisfied variables opposide side
cut.
node v decomposition F PS-sets C PS0 (Fv ) C 0 PS0 (Fv ),
say assignment var(F ) meets expectation C C 0 sat0 (Fv , |v ) = C
sat0 (Fv , |v ) = C 0 . node v branch decomposition, algorithm uses
table Tabv pair (C, C 0 ) PS0 (Fv ) PS0 (Fv ) stores Tabv (C, C 0 ) maximum
number clauses (v) satisfied, assignments meeting expectation
C C 0 . variables var(F ) \ (v) satisfy exactly C 0 , assignment
meets expectation, equivalent formulation content Tabv (C, C 0 )
must satisfy following constraint:
assignments var(F ) (v) sat0 (Fv , ) = C ,
fi
fi

Tabv (C, C 0 ) = max fi sat0 (F, 0 ) (v) C 0 fi

(1)



bottom-up dynamic programming along tree compute tables
node . leaf l , generating Tabl done easily linear time since
formula Fv contains one variable. internal node v , children c1 , c2 ,
69

fiSther, Telle & Vatshelle

compute Tabv algorithm described Procedure 3. 3 tables involved
update, one child one parent. pair entries, one child
table, may lead update entry parent table. table entry indexed
pair, thus 6 indices involved single potential update. trick first introduced
work Bui-Xuan et al. (2011) allows us loop triples indices
triple compute remaining 3 indices forming 6-tuple involved update, thereby
reducing runtime.
Procedure 3: Computing Tabv inner node v children c1 , c2
input: Tabc1 , Tabc2
output: Tabv
1. initialize Tabv : PS0 (Fv ) PS0 (Fv ) {1}
2. (Cc1 , Cc2 , Cv0 ) PS0 (Fc1 ) PS0 (Fc2 ) PS0 (Fv )
3.
Cc0 1 (Cc2 Cv0 ) (c1 )
4.
Cc0 2 (Cc1 Cv0 ) (c2 )
5.
Cv (Cc1 Cc2 ) \ (v)
6.
Tabc1 (Cc1 , Cc0 1 ) + Tabc2 (Cc2 , Cc0 2 )
7.
Tabv (Cv , Cv0 ) < Tabv (Cv , Cv0 )
8. return Tabv
Lemma 2. CNF formula F clauses inner node v, branch decomposition (T, ) ps-width k, Procedure 3 computes Tabv satisfying Constraint (1) time
O(k 3 m).
Proof. assume Tabc1 Tabc2 satisfy Constraint (1). Procedure 3 loops triples
PS0 (Fc1 ) PS0 (Fc2 ) PS0 (Fv ). definition ps-width (T, )
k 3 triples. operation inside iteration loop take O(m) time
constant number operations. Thus runtime O(k 3 m).
show correctness output, let us look bit workings
Procedure 3. assignment var(F ), cut, assignment meet
expectation single pair PS-sets. Let (X1 , X10 ), (X2 , X20 ) (Xv , Xv0 ) pairs
assignment meets expectation respect cuts induced c1 , c2 , v,
respectively. notice
Xv = sat0 (Fv , |v )

= sat0 (Fv , |c1 ] |c2 )

= sat0 (Fv , |c1 ) sat0 (Fv , |c2 )

= (sat0 (Fc1 , |c1 ) \ (v)) (sat0 (Fc2 , |c2 ) \ (v))

(2)

= (X1 \ (v)) (X2 \ (v))
= (X1 X2 ) \ (v).

also seen Figure 3. symmetry, find similar values X10 X20 ;
namely X10 = (X2 Xv0 ) (c1 ) X20 = (X1 Xv0 ) (c2 ). So, latter three sets
implicit based three former sets respect cuts induced v, c1
c2 . therefore, convenience proof, say assignment meets
70

fiSolving #SAT MaxSAT Dynamic Programming

= X1 = sat (Fc1 , |c1 )

clauses cla(F ) \ (v)

= X2 = sat (Fc2 , |c2 )
= Xv = sat (Fv , |v )

clauses (c2 )

clauses (c1 )

Figure 3: shown chain equalities (2) proof Lemma 2, clauses
sat0 (Fv , |v ) precisely clauses (sat0 (Fc1 , |c1 ) sat0 (Fc2 , |c2 )) \ (v).
expectation triple (C1 , C2 , C 0 ) PS-sets, meets expectation implicit
three pairs respective cuts. notice choice triples
PS-sets (Cc1 , Cc2 , Cv0 ) Procedure 3 computes implicit three sets names
Cc0 1 , Cc0 2 Cv accordingly.
show pairs (C, C 0 ) PS0 (Fv ) PS0 (Fv ) value Tabv (C, C 0 )
correct. Let 0 assignment var(F ) satisfies maximum number
clauses, meeting expectation C C 0 . Thus, value Tabv (C, C 0 ) correct
stores exactly number clauses (v) 0 satisfies.
Let (C1 , C10 ) (C2 , C20 ) pairs PS-sets 0 meet expectation
cut ((c1 ), (c1 )) ((c2 ), (c2 )), respectively. 0 meets expectations, value
Tabc1 (C1 , C10 ) Tabc2 (C2 , C20 ) must least large number clauses 0
satisfies (c1 ) (c2 ), respectively. Thus, number clauses 0 satisfies
(c1 ) (c2 ) large sum two entries. Since Procedure 3,
iteration Cv0 = C 0 , Cc1 = C1 Cc2 = C2 , ensures Tabv (C, C 0 ) least
sum Tabc1 (C1 , C10 ) Tabc2 (C2 , C20 ), know Tabv (C, C 0 ) least large
correct value.
assume contradiction value cell Tabv (C, C 0 ) large.
means iteration Procedure 3 assigned value Tabc1 (Cc1 , Cc0 1 ) +
Tabc2 (Cc2 , Cc0 2 ) sum large. Let 1 2 assignments var(F )
meeting expectation Cc1 Cc0 1 meeting expectation Cc2 , Cc0 2 , respectively,
number clauses (c1 ) (c2 ), respectively, equals according table
entries Tabc1 Tabc2 . take assignment x = 1 |c1 ] 2 |c2 ] 0 |v ,
assignment meets expectation C C 0 , satisfies clauses
(v) 0 , contradicting choice 0 . Tabv (C, C 0 ) neither smaller larger
number clauses (v) 0 satisfies, exactly same.
Theorem 2. Given formula F n variables clauses, branch decomposition
(T, ) F ps-width k, solve MaxSAT, #SAT, weighted MaxSAT time
O(k 3 m(m + n)).
Proof. solve MaxSAT, first compute Tabr root node r . requires
first compute PS0 (Fv ) PS0 (Fv ) nodes v , then, bottom
manner, compute Tabv O(m + n) nodes . former part
71

fiSther, Telle & Vatshelle

O(k 2 m(m + n)) time Theorem 1, latter part O(k 3 m(m + n)) time
Lemma 2.
root r (r) = var(F ) cla(F ). Thus Fr = Fr
variables, P S(Fr ) P S(Fr ) contains (, ). assignments var(F )
meet expectation cut ((r), (r)), cla(F ) (r) = cla(F ),
Constraint (1) value Tabr (, ) maximal number clauses F assignment
var(F ) satisfies. hence, number solution MaxSAT.
weight function w : cla(F ) N, redefining Constraint (1) Tabv maximize
w(sat0 (F, ) (v)) instead |sat0 (F, ))(v)|, able solve general
problem weighted MaxSAT way.
problem #SAT, care assignments satisfying clauses
F , want decide number distinct assignments so. requires
alterations. Firstly, alter definition contents Tabv (C, C 0 ) Constraint
(1) number assignments var(F ) (v) sat0 (Fv , ) = C
clauses (v) either C 0 satisfied . Secondly, computing Tabl
leaves l , set entries Tabl either zero, one, two, according
definition. Thirdly, alter algorithm compute Tabv (Procedure 3) inner nodes.
initialize Tabv (C, C 0 ) zero start algorithm, substitute lines 6
7 Procedure 3 following line increases table value product
table values children
Tabv (Cv , Cv ) Tabv (Cv , Cv ) + Tabc1 (Cc1 , Cc1 ) Tabc2 (Cc2 , Cc2 )
satisfy new constraint Tabv internal nodes v . value Tabr (, )
root r exactly number distinct assignments satisfying clauses
F.
bottleneck giving cubic factor k 3 runtime Theorem 2 number
triples PS0 (Fv ) PS0 (Fc1 ) PS0 (Fc2 ) node v children c1 c2 . (T, )
linear branch decomposition, always case either c1 c2 leaf .
case either |PS0 (Fc1 )| |PS0 (Fc2 )| constant. Therefore, linear branch decompositions
PS0 (Fv ) PS0 (Fc1 ) PS0 (Fc2 ) contain O(k 2 ) triples. Thus reduce
runtime algorithm factor k.
Theorem 3. Given formula F n variables clauses, linear branch
decomposition (T, ) F ps-width k, solve #SAT, MaxSAT, weighted MaxSAT
time O(k 2 m(m + n)).

4. CNF Formulas Polynomial ps-width
section investigate classes CNF formulas decompositions ps-width
polynomially bounded total size formula. particular, show
holds whenever incidence graph formula constant mim-width (maximum
induced matching-width, introduced Vatshelle, 2012). also show large class
bipartite graphs, using call bigraph bipartizations, constant mim-width.
72

fiSolving #SAT MaxSAT Dynamic Programming

order lift upper bound Lemma 1 ps-value F , i.e |PS(F )|,
ps-width F , use mim-width incidence graph I(F ), defined using branch
decompositions graphs. branch decomposition formula F , defined Section
2, also seen branch decomposition incidence graph I(F ). Nevertheless,
completeness, formally define branch decompositions graphs mim-width.
branch decomposition graph G pair (T, ) rooted binary tree
bijection leaf set vertex set G. node w
let subset V (G) bijection leaves subtree rooted w
denoted Vw . say decomposition defines cut (Vw , Vw ). mim-value cut
(Vw , Vw ) size maximum induced matching G[Vw , Vw ]. mim-width (T, )
maximum mim-value cuts (Vw , Vw ) defined node w . mim-width
graph G, denoted mimw(G), minimum mim-width branch decompositions
(T, ) G. linear branch decomposition branch decomposition inner
nodes underlying tree induces path.
Since decomposition I(F ) seen also decomposition F , immediately
get Lemma 1 following corollary.
Corollary 1. CNF formula F clauses, clause containing
literals, ps-width F min{mk + 1, 2tk } k = mimw(I(F )).
Many classes graphs intersection models, meaning represented
intersection graphs certain objects, i.e. vertex associated object
two vertices adjacent iff objects intersect. objects used define intersection
graphs usually consist geometrical objects lines, circles polygons. Many well
known classes intersection graphs constant mim-width, following
lists subset classes proven bounds (Belmonte & Vatshelle, 2013;
Vatshelle, 2012).
Theorem 4. (Belmonte & Vatshelle, 2013; Vatshelle, 2012) Let G graph. G a:
interval graph mimw(G) 1.
circular arc graph mimw(G) 2.
k-trapezoid graph mimw(G) k.
Moreover exist linear decompositions satisfying bound, found polynomial time (for k-trapezoid assume intersection model given).
Let us briefly mention definition graph classes. graph interval graph
intersection model consisting intervals real line. graph circular arc
graph intersection model consisting arcs circle. build k-trapezoid
start k parallel line segments (s1 , e1 ), (s2 , e2 ), ..., (sk , ek ) add two non-intersecting
paths e joining si si+1 ei ei+1 respectively straight lines
{1, ..., k 1}. polygon defined e two line segments (s1 , e1 ), (sk , ek )
forms k-trapezoid. graph k-trapezoid graph intersection model consisting
k-trapezoids. See work Brandstadt, Le, Spinrad (1999) information
graph classes containment relations.
Combining Corollary 1 Theorem 4 get following
Corollary 2. Let F CNF formula containing clauses maximum clause-size t.
I(F ) a:
73

fiSther, Telle & Vatshelle

interval graph psw(F ) min{m + 1, 2t }.
circular arc graph psw(F ) min{m2 + 1, 4t }.
k-trapezoid graph psw(F ) min{mk + 1, 2tk }.
Moreover exist linear decompositions satisfying bound, found polynomial time (for k-trapezoid assume intersection model given).
incidence graphs formulas bipartite graphs, case
majority graphs above-mentioned graph classes. following show
extend results Corollary 2 large classes bipartite graphs. graph G
subset vertices V (G) bipartite graph G[A, A] subgraph G containing
edges G exactly one endpoint A. graph G V (G) call G[A, A]
bigraph bipartization G, note G bigraph bipartization subset
vertices. graph class X define class X bigraphs bipartite graphs H
exists G X H isomorphic bigraph bipartization G.
example, bipartite graph H interval bigraph interval graph G
V (G) H isomorphic G[A, A].
following result allow us lift results Corollary 2 given graphs
bigraph bipartizations graphs.
Theorem 5. Assume given CNF formula F clauses maximum
clause-size t, graph G, subset V (G), (T, G ) (linear) branch decomposition
G mim-width k. I(F ) connected isomorphic G[A, A] (thus I(F ) bigraph
bipartization G) linear time produce (linear) branch decomposition (T, F )
F ps-width min{mk + 1, 2tk }
Proof. Since variable clause F corresponding node I(F ), node
I(F ) corresponding node G, defining F function mapping leaf
l variable clause F corresponding node G (l), get (T, F )
branch decomposition F . Consider cut (B, B) induced node (T, F ). Note
mim-value G[B, B] k. I(F ) connected means
either corresponding set variables F . Assume wlog former. Thus
C = B cla(F ) clauses B, C = cla(F ) \ C X = B var(F )
variables B, X = var(F ) \ X. mim-values G[C, X] G[C, X]
k, since induced subgraphs G[B, B], taking induced subgraphs
cannot increase size maximum induced matching. Hence Lemma 1,
|PS(FC,X )| |cla(F )|k + 1, likewise |PS(FC,X )| |cla(F )|k + 1,
maximum two ps-value cut. Since ps-width decomposition
maximum ps-value cut theorem follows.
Combining Theorems 5 4 immediately get following.
Corollary 3. Let F CNF formula containing clauses maximum clause-size t.
I(F ) a:
interval bigraph psw(F ) min{m + 1, 2t }.
circular arc bigraph psw(F ) min{m2 + 1, 4t }.
k-trapezoid bigraph psw(F ) min{mk + 1, 2tk }.
Moreover exist linear decompositions satisfying bound.
74

fiSolving #SAT MaxSAT Dynamic Programming

next section address question finding linear decompositions
polynomial time. succeed case interval bigraphs, circular arc bigraphs
k-trapezoid bigraphs must leave open problem.

5. Interval Bigraphs Formulas Interval Orders
section show formulas whose incidence graph interval bigraph
polynomial time find linear branch decompositions small ps-width. Let
us recall definition interval ordering. CNF formula F interval ordering
exists linear ordering variables clauses variable x occurring
clause C, x appears C variable also occurs C,
C appears x x occurs also clause them. See Figure 4
example.
Order:
x1 c 1 x2 x3 c 2 c 3 x4 x5

Clauses:

Bipartized interval rep.

Incidence graph

x1
c1
x2
x3
c2
c3
x4
x5

c1 = {x1 , x2 }
c2 = {x2 , x3 , x5 }
c3 = {x3 , x4 , x5 }

c1

x1

c2 c3

x2 x3

x4 x5

Figure 4: CNF formula interval ordering. incidence graph interval
bigraph, since isomorphic bigraph bipartization, defined blue
intervals, interval graph intersection model left.
work Hell Huang (2004) follows formula F interval ordering
I(F ) interval bigraph.
Theorem 6. Given CNF formula F n variables clauses
literals. time O((m + n)mn) decide F interval ordering (yes iff I(F )
interval bigraph), yes solve #SAT weighted MaxSAT additional
runtime O(min{m2 , 4t }(m + n)m).
Proof. Using characterization work Hell Huang (2004) algorithm
Rafiey (2012) time O((m + n)mn) decide F interval ordering
yes, find it. interval ordering build interval graph G I(F )
bigraph bipartization G, construct linear branch decomposition G
mim-width 1 (Belmonte & Vatshelle, 2013). linear branch decomposition
75

fiSther, Telle & Vatshelle

get Theorem 5 construct another linear branch decomposition F
ps-width O(m). run algorithm Theorem 3.

6. Experimental Results
present simple experimental results, intended proof concept. belief
ideas behind algorithms, like notion ps-value, useful
practice, require thorough investigation confirm belief. results
indicate worst-case runtime bounds dynamic programming, Theorems 2
3, probably higher would commonly seen practice.
past decade, SAT solvers become powerful, currently able
handle large practical instances. Techniques SAT solvers
applied develop relatively powerful MaxSAT #SAT solvers (Biere, Heule, & van
Maaren, 2009). experiments compare implementations algorithms
state-of-the-art MaxSAT #SAT solvers. enhance implementations
techniques, even simple pre-processing, vast majority instances
implementations fall far behind comparison. However, focusing formulas
certain linear order implementations compare favorably.
explained Section 1, two steps involved: (1) find good decomposition
input CNF formula F , (2) perform DP (dynamic programming) along
decomposition. Let us start describing simple heuristic step (1). takes
input bipartite graph I(F ) vertex set cla(F ) var(F ), outputs linear
order vertex set. heuristic GreedyOrder greedy algorithm
increasing values chooses (i) vertex highest number already
chosen neighbors, among choosing one fewest non-chosen neighbors.
defines linear branch decomposition (T, ) CNF formula F , non-leaf nodes
binary tree inducing path, rooted one end path,
mapping ith leaf encountered breadth-first search starting root
clause variable (i), 1 |cla(F ) var(F )|.

Algorithm GreedyOrder
input: G = (V, E), (bipartite) graph
output: , linear ordering V
L = , R = V , = 1
v V set Ldegree(v) = 0
R empty
choose v: vertices R max Ldegree take one smallest degree
set (i) = v, increment i, add v L remove v R
w R vw E increment Ldegree(w)

76

fiSolving #SAT MaxSAT Dynamic Programming

implementations found online (Sther, Telle, & Vatshelle, 2015).
implemented GreedyOrder Java, together straight-forward implementation
DP algorithm Theorem 3.
Given CNF formula, allows us solve MaxSAT #SAT first running
GreedyOrder DP. compare implementation best solvers could
find online, respectively CCLS-to-akmaxsat (Luo, Cai, Wu, Jie, & Su, 2014)
among best solvers Ninth Max-SAT Evaluation (2014), latest version
#SAT solver called sharpSAT (Thurley, n.d., 2006). solvers handily beat
implementation inputs. therefore generated CNF formulas
interval orderings, Theorem 6, check least instances better.
Note step (1) implemented polynomial-time algorithm recognizing
formulas interval orders, relying instead GreedyOrder heuristic.
6.1 Generation Instances
presenting results, let us describe generation set instances,
three types. start type 1. generation formulas based
definition interval orderings given interval bigraph definition, see e.g. left side
Figure 4. generate formula type 1 n variables clauses, generate
n + intervals real line iterating points 1 2(n + m) left
right endpoints intervals:
step i, check 4 cases legal (e.g. 3 legal exists
live variable, i.e. left endpoint < right endpoint) randomly make
one legal choices:
1. start interval new variable left endpoint
2. start interval new clause left endpoint
3. end interval randomly chosen live variable right endpoint
4. end interval randomly chosen live clause right endpoint
Towards end process boundary conditions enforced reach exactly
clauses, n expected slightly smaller m. clause interval randomly
choose variable overlapping interval either positive negative
clause. resulting CNF formula interval ordering given rightmost
endpoints intervals. hide ordering clauses variables randomly permuted
make final CNF formula.
formulas type 2 generated similar fashion type 1, except
guarantee clauses size t, Lemma 1 could big help.
change case 4 instead choice becomes enforced live
clause step accumulated exactly overlapping variable intervals. also let
clause interval represent 4 clauses variable set randomly chosen
literals, aim increasing probability instance satisfiable.
formulas type 3 CNF-representation conjunction XOR functions
XOR fixed number literals variables XOR functions
77

fiSther, Telle & Vatshelle

overlap way incidence graph bipartization circular arc
graph.
formula type 3 generated three input parameters n, t, s. n variables
represented successive points 1 n circle. first XOR function interval
1 thus containing variables points 1 t, second interval + 1 + t,
general ith interval + 1 + t, appropriate modulo addition
boundary condition end ensure n/s XOR functions. Variables chosen
randomly appear positive negative XOR. XOR transformed
standard way CNF formula 2t1 clauses give us resulting CNF formula
n/s 2t1 clauses. Again, variables clauses randomly permuted hide
ordering giving circular arc bigraph representation.
Note resulting formulas quite simple structure, state-ofthe-art SAT solver, like lingeling (Biere, 2014), handles generated instances within
seconds.
6.2 Results
ready present results. ran solvers Dell Optiplex 780
running Ubuntu 12.04 64-Bit. machine 8GB memory Intel Core 2 Quad
Q9650 processor OpenJDK java 6 (IcedTea6 1.13.5).
instances type 1 GreedyOrder heuristic fails terribly becomes huge
bottleneck. greedy choice based degrees vertices I(F ) simple. However,
given correct interval order solver(s) performed better.
Instances type 2 generated similar type 1 clauses small size,
Lemma 1 could help. case number clauses approximately

Runtime seconds

600

400

CCLS
sharpSAT
MaxSAT
#SAT
(practically equal)

200

0
0

1000
2000
Number variables

3000

Figure 5: Runtimes instances type 2. MaxSAT solver clearly faster
CCLS akmaxsat. vertical axis represents time seconds. Runs taking
600 seconds stopped completion drawn
dotted line.

78

fiSolving #SAT MaxSAT Dynamic Programming

four times number variables, consequence great number instances
satisfiable, making work #SAT-solvers easier MaxSAT
solvers. generated instances type 2 solved within seconds sharpSAT, see
Figure 5. size instances grow, see clear tendency runtimes
CCLS akmaxsat increase much rapidly solvers. runtimes
two solvers almost identical. GreedyOrder heuristic instances seems
produce decompositions/orders low PS-width.
type 3 instances shown Figure 6 generated k = 5 = 3.
instances satisfiable, may explain CCLS akmaxsat fast. Choosing
k = 3 = 2 satisfiable instances CCLS akmaxsat would
often spend 600 seconds time out. size instances grow,
see clear tendency runtimes sharpSAT increase much rapidly
solvers. runtimes two solvers almost identical.

Runtime seconds

600

400

CCLS
sharpSAT
MaxSAT
#SAT
(practically equal)

200

0
60

80
100
120
Number variables

140

Figure 6: Runtimes instances type 3. #SAT solver clearly faster
sharpSAT. vertical axis represents time seconds. Runs taking
600 seconds stopped completion drawn dotted line.

7. Conclusion
paper proposed structural parameter CNF formulas, called ps-width
projection-satisfiable-width. showed weighted MaxSAT #SAT
solved polynomial time given decomposition formula polynomially bounded
ps-width. Using concept interval bigraphs also showed polynomial time algorithm
actually finds decomposition, formulas interval ordering. Could
one devise algorithm also larger class circular arc bigraphs, maybe
even even larger class k-trapezoid bigraphs? words, problem
recognizing bipartite input graph circular arc bigraph, k-trapezoid bigraph,
polynomial-time solvable?
79

fiSther, Telle & Vatshelle

could practical interest design heuristic algorithm given formula finds
decomposition relatively low ps-width, done boolean-width (Hvidevold,
Sharmin, Telle, & Vatshelle, 2011). One could check benchmarks covering real-world
SAT instances low ps-width, perform study correlation low
ps-width practical hardness MaxSAT #SAT solvers, done
treewidth SAT solvers (Mateescu, 2011). presented simple experimental
results, require thorough investigation check ideas algorithms
could useful practice. Finally, hope essential combinatorial result enabling
improvements paper, Lemma 1, may uses well.

References
Bacchus, F., Dalmao, S., & Pitassi, T. (2003). Algorithms complexity results for#SAT
bayesian inference. Foundations computer science, 2003. proceedings. 44th
annual ieee symposium (pp. 340351).
Belmonte, R., & Vatshelle, M. (2013). Graph classes structured neighborhoods
algorithmic applications. Theor. Comput. Sci., 511 , 54-65.
Biere, A. (2014). Yet another local search solver lingeling friends entering SAT
competition 2014. SAT Competition 2014 , 39.
Biere, A., Heule, M., & van Maaren, H. (2009). Handbook satisfiability. (Vol. 185,
chap. 20). IOS Press.
Brandstadt, A., Le, V. B., & Spinrad, J. P. (1999). Graph classes: survey (Vol. 3).
Philadelphia: SIAM Society Industrial Applied Mathematics.
Brandstadt, A., & Lozin, V. V. (2003). linear structure clique-width bipartite
permutation graphs. Ars Comb., 67 .
Brault-Baron, J., Capelli, F., & Mengel, S. (2014). Understanding model counting
-acyclic CNF-formulas. CoRR, abs/1405.6043 . Retrieved http://arxiv.org/
abs/1405.6043
Bui-Xuan, B.-M., Telle, J. A., & Vatshelle, M. (2010). H-join decomposable graphs
algorithms runtime single exponential rankwidth. Discrete Applied Mathematics,
158 (7), 809-819.
Bui-Xuan, B.-M., Telle, J. A., & Vatshelle, M. (2011). Boolean-width graphs. Theoretical
Computer Science, 412 (39), 51875204.
Courcelle, B. (2004). Clique-width countable graphs: compactness property. Discrete
Mathematics, 276 (1-3), 127148. Retrieved http://dx.doi.org/10.1016/S0012
-365X(03)00303-0 doi: 10.1016/S0012-365X(03)00303-0
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126 (1), 541.
Fischer, E., Makowsky, J. A., & Ravve, E. V. (2008). Counting truth assignments
formulas bounded tree-width clique-width. Discrete Applied Mathematics, 156 (4),
511-529.
80

fiSolving #SAT MaxSAT Dynamic Programming

Fredkin, E. (1960). Trie memory. Communications ACM , 3 (9), 490499.
Ganian, R., Hlineny, P., & Obdrzalek, J. (2013). Better algorithms satisfiability problems
formulas bounded rank-width. Fundam. Inform., 123 (1), 59-76.
Garey, M. R., & Johnson, D. S. (1979). Computers intractability: guide theory
NP-completeness. W. H. Freeman.
Hell, P., & Huang, J. (2004). Interval bigraphs circular arc graphs. Journal Graph
Theory, 46 (4), 313-327.
Hvidevold, E. M., Sharmin, S., Telle, J. A., & Vatshelle, M. (2011). Finding good
decompositions dynamic programming dense graphs. D. Marx & P. Rossmanith
(Eds.), Ipec (Vol. 7112, p. 219-231). Springer.
Jaumard, B., & Simeone, B. (1987). complexity maximum satisfiability
problem Horn formulas. Inf. Process. Lett., 26 (1), 1-4.
Kaski, P., Koivisto, M., & Nederlof, J. (2012). Homomorphic hashing sparse coefficient
extraction. Proceedings 7th international conference parameterized
exact computation (pp. 147158).
Luo, C., Cai, S., Wu, W., Jie, Z., & Su, K. (2014). CCLS: efficient local search
algorithm weighted maximum satisfiability. IEEE Transactions Computers. doi:
10.1109/TC.2014.2346195
Mateescu, R. (2011). Treewidth industrial SAT benchmarks (Tech. Rep.). Tech. rep.
Cambridge, UK: Microsoft Research. Retrieved http://research.microsoft
.com/pubs/145390/MSR-TR-2011-22.pdf
Muller, H. (1997). Recognizing interval digraphs interval bigraphs polynomial time.
Discrete Applied Mathematics, 78 (1-3), 189-205.
Ninth Max-SAT Evaluation. (2014). Retrieved http://www.maxsat.udl.cat/14/
(accessed 16-January-2015)
Paulusma, D., Slivovsky, F., & Szeider, S. (2013). Model counting CNF formulas
bounded modular treewidth. N. Portier & T. Wilke (Eds.), Stacs (Vol. 20, p. 55-66).
Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik.
Rafiey, A. (2012).
abs/1211.2662 .

Recognizing interval bigraphs forbidden patterns.

CoRR,

Raman, V., Ravikumar, B., & Rao, S. S. (1998). simplified NP-complete MAXSAT
problem. Inf. Process. Lett., 65 (1), 1-6.
Rao, M. (2008). Clique-width graphs defined one-vertex extensions. Discrete
Mathematics, 308 (24), 61576165.
Robertson, N., & Seymour, P. D. (1991). Graph minors X. obstructions tree-decomposition.
J. COMBIN. THEORY SER. B , 52 (2), 153190.
Roth, D. (1996). connectionist framework reasoning: Reasoning examples.

81

fiSther, Telle & Vatshelle

W. J. Clancey & D. S. Weld (Eds.), Aaai/iaai, vol. 2 (p. 1256-1261). AAAI Press /
MIT Press.
Sther, S. H., Telle, J. A., & Vatshelle, M. (2014). Solving MaxSAT #SAT
structured CNF formulas. C. Sinz & U. Egly (Eds.), SAT 2014 (Vol. 8561, pp. 16
31). Springer. Retrieved http://dx.doi.org/10.1007/978-3-319-09284-3 3
doi: 10.1007/978-3-319-09284-3 3
Sther, S. H., Telle, J. A., & Vatshelle, M. (2015). online implementations. Retrieved
http://people.uib.no/ssa032/pswidth/
Samer, M., & Szeider, S. (2010). Algorithms propositional model counting. J. Discrete
Algorithms, 8 (1), 50-64.
Slivovsky, F., & Szeider, S. (2013). Model counting formulas bounded clique-width.
L. Cai, S.-W. Cheng, & T. W. Lam (Eds.), Isaac (Vol. 8283, p. 677-687). Springer.
Szeider, S. (2003). fixed-parameter tractable parameterizations SAT. E. Giunchiglia
& A. Tacchella (Eds.), Sat 2003 (Vol. 2919, p. 188-202). Springer.
Thurley, M. (n.d.). sharpSAT. Retrieved https://sites.google.com/site/
marcthurley/sharpsat (accessed 16-January-2015)
Thurley, M. (2006). sharpSATcounting models advanced component caching
implicit BCP. Theory applications satisfiability testing-sat 2006 (pp. 424429).
Springer.
Vatshelle, M. (2012). New width parameters graphs. Unpublished doctoral dissertation,
University Bergen.

82

fi
