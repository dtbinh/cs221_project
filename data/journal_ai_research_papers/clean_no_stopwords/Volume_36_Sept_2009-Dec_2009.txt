Journal Artificial Intelligence Research 36 (2009) 229-266Submitted 03/09; published 10/09Relaxed Survey PropagationWeighted Maximum Satisfiability ProblemHai Leong Chieuchaileon@dso.org.sgDSO National Laboratories,20 Science Park Drive, Singapore 118230Wee Sun Leeleews@comp.nus.edu.sgDepartment Computer Science, School Computing,National University Singapore, Singapore 117590Abstractsurvey propagation (SP) algorithm shown work well large instancesrandom 3-SAT problem near phase transition. shown SP estimatesmarginals covers represent clusters solutions. SP-y algorithm generalizesSP work maximum satisfiability (Max-SAT) problem, cover interpretationSP generalize SP-y. paper, formulate relaxed survey propagation (RSP) algorithm, extends SP algorithm apply weighted Max-SATproblem. show RSP interpretation estimating marginals coversviolating set clauses minimal weight. naturally generalizes cover interpretation SP. Empirically, show RSP outperforms SP-y state-of-the-artMax-SAT solvers random Max-SAT instances. RSP also outperforms state-of-the-artweighted Max-SAT solvers random weighted Max-SAT instances.1. Introduction3-SAT problem archetypical NP-complete problem, difficulty solvingrandom 3-SAT instances shown related clause variable ratio,= M/N , number clauses N number variables. phasetransition occurs critical value c 4.267: random 3-SAT instances < cgenerally satisfiable, instances > c not. Instances close phasetransition generally hard solve using local search algorithms (Mezard & Zecchina,2002; Braunstein, Mezard, & Zecchina, 2005).survey propagation (SP) algorithm invented statistical physics community using approaches used analyzing phase transitions spin glasses (Mezard &Zecchina, 2002). SP algorithm surprised computer scientists ability solveefficiently extremely large difficult Boolean satisfiability (SAT) instances phasetransition region. algorithm also extended SP-y algorithm handlemaximum satisfiability (Max-SAT) problem (Battaglia, Kolar, & Zecchina, 2004).Progress made understanding SP algorithm works well. BraunsteinZecchina (2004) first showed SP viewed belief propagation (BP)algorithm (Pearl, 1988) related factor graph clusters solutions representedcovers non-zero probability. known whether similar interpretationgiven SP-y algorithm. paper, extend SP algorithm handle weightedc2009AI Access Foundation. rights reserved.fiChieu & LeeMax-SAT instances way preserves cover interpretation, call newalgorithm Relaxed Survey Propagation (RSP) algorithm. Empirically, showRSP outperforms SP-y state-of-the-art solvers random Max-SAT instances.also outperforms state-of-the-art solvers benchmark Max-SAT instances.random weighted Max-SAT instances, outperforms state-of-the-art weighted Max-SATsolvers.rest paper organized follows. Section 2, describe backgroundliterature mathematical notations necessary understanding paper. includesbrief review definition joint probability distributions factor graphs,introduction SAT, Max-SAT weighted Max-SAT problem,formulated inference problems probability distribution factor graph.Section 3, give review BP algorithm (Pearl, 1988), plays central rolepaper. Section 4, give description SP (Braunstein et al., 2005)SP-y (Battaglia et al., 2004) algorithm, explaining warning propagation algorithms.Section 5, define joint distribution extended factor graph given weightedMax-SAT instance. factor graph generalizes factor graph defined Maneva,Mossel Wainwright (2004) Chieu Lee (2008). show that, solvingSAT instances, running BP algorithm factor graph equivalent runningSP algorithm derived Braunstein, Mezard Zecchina (2005). weightedMax-SAT problem, gives rise new algorithm call Relaxed SurveyPropagation (RSP) algorithm. Section 7, show empirically RSP outperformsalgorithms solving hard Max-SAT weighted Max-SAT instances.2. BackgroundSP first derived principles statistical physics, understoodBP algorithm, estimating marginals joint distribution defined factor graph.section, provide background material joint distributions definedfactor graphs. define Boolean satisfiability (SAT) problem, maximumsatisfiability (Max-SAT) problem, weighted maximum satisfiability (weighted MaxSAT) problem, show problems solved solving inference problemjoint distributions defined factor graphs. review definition derivationBP algorithm follow next section, describe SP algorithmSection 4.2.1 NotationsFirst, define notations concepts relevant inference problemsfactor graphs. Factor graphs provide framework reasoning manipulating jointdistribution set variables. general, variables could continuous nature,paper, limit discrete random variables.paper, denote random variables using large Roman letters, e.g., X, .random variables always discrete paper, taking values finite domain. Usually,interested vectors random variables, write letters boldface, e.g., X, Y. often index random variables letters i, j, k..., write,example, X = {Xi }iV , V finite set. subset W V , denote230fiRelaxed Survey Propagation Weighted Max-SAT ProblemX1'X2''X3X4Figure 1: simple factor graph p(x) = (x1 , x2 ) 0 (x1 , x3 ) 00 (x2 , x4 ).XW = {Xi }iW . call assignment values variables X configuration,denote small bold letters, e.g. x. often write x represent eventX = x and, probability distribution p, write p(x) mean p(X = x). Similarly,write x denote event X = x, write p(x) denote p(X = x).recurring theme paper defining message passing algorithms jointdistributions factor graphs (Kschischang, Frey, & Loeliger, 2001). joint distributiondefined product local functions (functions defined small subset variables),refer local functions factors. index factors, e.g. , Greekletters, e.g., , (avoiding used symbol clause variable ratio SATinstances). factor , denote V () V subset variablesdefined, i.e. function defined variables XV () . message passingalgorithms, messages vectors real numbers sent factors variablesvice versa. vector message sent variable Xi factor denotedMi , message Xi denoted Mi .2.2 Joint Distributions Factor GraphsGiven large set discrete, random variables X = {Xi }iV , interested jointprobability distribution p(X) variables. set V large, ofteninterest assume simple decomposition, draw conclusions efficientlydistribution. paper, interested joint probability distributiondecomposed followsp(X = x) =1(xV() ),Z F(1)set F indexes set functions { }F . function definedsubset variables XV () set X, maps configurations xV () non-negativereal numbers. Assuming function defined small subset variablesXV () , hope efficient inference distribution, despite large numbervariables X. constant Z normalization constant, ensuresdistribution sums one configurations x X.factor graph (Kschischang et al., 2001) provides useful graphical representationillustrating dependencies defined joint probability distribution Equation 1.factor graph G = ({V, F }, E), bipartite graph two sets nodes, set variablenodes, V , set factor nodes, F . set edges E factor graph connectsvariable nodes factor nodes, hence bipartite nature graph. factor graphrepresenting joint distribution Equation 1, edge e = (, i) E231fiChieu & Leevariable Xi parameter factor (i.e. V ()). denote V (i)set factors depending variable Xi , i.e.V (i) = { F | V ()}(2)show simple example factor graph Figure 1. small example,example, V () = {1, 2} V (2) = {, 00 }. factor graph representation usefulillustrating inference algorithms joint distributions form Equation 1 (Kschischang et al., 2001). Section 3, describe BP algorithm using factorgraph representation.Equation 1 defines joint distribution product local factors. often usefulrepresent distribution following exponential form:p(x) = exp (X(xV () ) )(3)Fequation reparameterization Equation 1, (xV () ) = exp( (xV () ))= ln Z. statistical physics, exponential form often written follows:p(x) =11exp(E(x)),ZkB(4)E(x) Hamiltonian energy function, kB Boltzmanns constant,temperature. simplicity, set kB = 1, Equations 3 4 equivalentPE(x) = F (xV () ).Bayesian (belief) networks Markov random fields two graphical representations often used describe multi-dimensional probability distributions. Factor graphsclosely related Bayesian networks Markov random fields, algorithmsoperating factor graphs often directly applicable Bayesian networks Markovrandom fields. refer reader work Kschischang et al. (2001) comparisonfactor graphs, Bayesian networks Markov random fields.2.3 Inference Joint Distributionsliterature, inference joint distribution refer solving one two problems.define two problems follows:Problem 1 (MAP problem). Given joint distribution, p(x), interested configuration(s) highest probability. configurations, x , called maximuma-posteriori configurations, MAP configurationsx = arg max p(x)x(5)joint distribution Equation 4, MAP configuration minimizes energyfunction E(x), hence MAP problem sometimes called energy minimizationproblem.232fiRelaxed Survey Propagation Weighted Max-SAT ProblemProblem 2 (Marginal problem). Given joint distribution, p(x), central interestcalculation estimation probabilities events involving single variable Xi = xi .refer probabilities marginal probabilities:Xpi (xi ) =p(x).(6)x\xiPnotation x\xi means summing configurations X variable Xi setxi . Marginals important represent underlying distribution individualvariables.general, problems solvable reasonable time currently known methods. Naive calculation pi (xi ) involves summing probabilities configurationsvariables X Xi = xi . example, factor graph n variablescardinality q, finding marginal one variables involve summing q n1configurations. Furthermore, NP-complete problems 3-SAT simply codedfactor graphs (see Section 2.4.1). such, MAP problem general NP-complete,marginal problem equivalent model counting 3-SAT, #P-complete(Cooper, 1990). Hence, general, expect solve inference problems (exactly) reasonable time, unless problems small, special structuresexploited efficient inference.central interest paper particular approximate inference method known(sum-product) belief propagation (BP) algorithm. defer discussionBP algorithm next section. rest section, describe SAT,Max-SAT weighted Max-SAT problems, simply formulatedinference problems joint distribution factor graph.2.4 SAT Max-SAT Problemvariable Boolean takes values {FALSE, TRUE}. paper, followconventions statistical physics, Boolean variables take values {1, +1}, 1corresponding FALSE, +1 corresponding TRUE.Boolean satisfiability (SAT) problem given Boolean propositional formulawritten operators (conjunction), (disjunction), (negation),parenthesis. objective SAT problem decide whether exists configuration propositional formula satisfied (evaluates TRUE). SATproblem first problem shown NP-complete Stephen Cooks seminal paper1971 (Cook, 1971; Levin, 1973).three operators Boolean algebra defined follows: given two propositionalformulas B, OR(A, B) true either B true; AND(A, B) trueB true; NOT(A) true false. rest paper, usestandard notations Boolean algebra Boolean operators: B means OR(A, B),B means AND(A, B), means NOT(A). parenthesis available allownested application operators, e.g. (A B) (B C).conjunctive normal form (CNF) often used standard form writing Booleanformulas. CNF consists conjunction disjunctions literals, literaleither variable negation. example, (X1 X 2 ) (X 3 X4 ) CNF,233fiChieu & LeeX1 X2 (X1 X2 ) (X2 X3 ) not. Boolean formula re-written CNFusing De Morgans law distributivity law, although practice, may leadexponential blowup size formula, Tseitin transformation often usedinstead (Tseitin, 1968). CNF, Boolean formula considered conjunctionset clauses, clause disjunction literals. Hence, SAT problemoften given (X, C), X vector Boolean variables, C setclauses. clause C satisfied configuration evaluates TRUEconfiguration. Otherwise, said violated configuration. use Greekletters (e.g. , ) indices clauses C, denote V () set variablesclause C. K-SAT problem SAT problem clause C consistsexactly K literals. K-SAT problem NP-complete, K 3 (Cook, 1971).maximum satisfiability problem (Max-SAT) problem optimization versionSAT problem, aim minimize number violated constraintsformula. define simple working example Max-SAT problem usethroughout paper:Example 1. Define instance Max-SAT problem CNF following clauses1 = (x1 x2 ), 2 = (x2 x3 ), 3 = (x3 x1 ), 4 = (x1 x2 x3 ), 5 = (x1 x2 x3 ) 6 =(x1 x2 ). Boolean expression representing problem would(x1 x2 ) (x2 x3 ) (x3 x1 ) (x1 x2 x3 ) (x1 x2 x3 ) (x1 x2 ).(7)objective Max-SAT problem would find configuration minimizingnumber violated clauses.2.4.1 Factor Graph Representation SAT InstancesSAT problem CNF easily represented joint distribution factorgraph. following definition, give possible definition joint distributionBoolean configurations given SAT instance, Boolean variables take values{1, +1}.Definition 1. Given instance SAT problem, (X, C) conjunctive normal form,X vector N Boolean variables. define energy, E(x), distribution, p(x), configurations SAT instance (Battaglia et al., 2004)C, C (xV () ) =E(x) =1(1 + J,i xi ),2iV ()(8)XC (xV () ),(9)1exp(E(x)),Z(10)Cp(x) =x {1, +1}N , J,i takes values {1, +1}. J,i = +1 (resp. 1),contains Xi negative (resp. positive) literal. clause satisfied onevariables Xi takes value J,i . clause satisfied, C (xV () ) = 0. OtherwiseC (xV () ) = 1.234fiRelaxed Survey Propagation Weighted Max-SAT Problem613X12X25X34Figure 2: factor graph SAT instance given Example 1. Dotted (resp. solid)lines joining variable clause means variable negative (resp. positive)literal clause.definition, energy function zero satisfying configurations,equals number violated clauses non-satisfying configuration. Hence, satisfyingconfigurations SAT instance MAP configurations factor graph.section, make definitions useful rest paper.clause containing variable Xi (associated value J,i ), sayXi satisfies Xi = J,i . case, clause satisfied regardless valuestaken variables. Conversely, say Xi violates Xi satisfy .case, still possible satisfied variables.Definition 2. clause C, define u,i (resp. s,i ) value Xi {1, +1}violates (resp. satisfies) clause . means s,i = J,i u,i = +J,i .define following setsV + (i)V (i)Vs (i)Vu (i)===={ V (i); s,i = +1},{ V (i); s,i = 1},{ V (i) \ {}; s,i = s,i },{ V (i) \ {}; s,i 6= s,i }.(11)definitions, V + (i) (resp. V (i)) set clauses contain Xipositive literal (resp. negative literal). Vs (i) (resp. Vu (i)) set clauses containingXi agrees (resp. disagrees) clause concerning Xi . sets usefuldefine SP message passing algorithms SAT instances.factor graph representing Max-SAT instance given Example 1 shownFigure 2. example, V + (1) = {3 , 5 , 6 }, V (1) = {1 , 4 }, Vs3 (1) = {5 , 6 },Vu3 (1) = {1 , 4 }. energy example follows:111E(x) = (1 + x1 )(1 x2 ) + (1 + x2 )(1 x3 ) + (1 + x3 )(1 x1 ) +444111(1 + x1 )(1 + x2 )(1 + x3 ) + (1 x1 )(1 x2 )(1 x3 ) + (1 x1 )(1 x2 ) (12)884235fiChieu & Lee2.4.2 Related Work SATSAT problem well studied computer science: archetypical NP-completeproblem, common reformulate NP-complete problems graph coloringSAT problem (Prestwich, 2003). SAT solvers either complete incomplete.best known complete solver solving SAT problem probably Davis-PutnamLogemann-Loveland (DPLL) algorithm (Davis & Putnam, 1960; Davis, Logemann, & Loveland, 1962). DPLL algorithm basic backtracking algorithm runs choosingliteral, assigning truth value it, simplifying formula recursively checkingsimplified formula satisfiable; case, original formula satisfiable; otherwise, recursive check done assuming opposite truth value. VariantsDPLL algorithm Chaff (Moskewicz & Madigan, 2001), MiniSat (Een & Sorensson,2005), RSAT (Pipatsrisawat & Darwiche, 2007) among best performers recent SAT competitions (Berre & Simon, 2003, 2005). Solvers satz (Li & Anbulagan,1997) cnfs (Dubois & Dequen, 2001) also making progress solving hardrandom 3-SAT instances.solvers participated recent SAT competitions complete solvers.incomplete stochastic solvers show SAT instance unsatisfiable,often able solve larger satisfiable instances complete solvers. Incomplete solversusually start randomly initialized configuration, different algorithms differway flip selected variables move towards solution. One disadvantageapproach hard SAT instances, large number variables flippedmove current configuration local minimum, acts local trap. Incompletesolvers differ strategies used move configuration traps. example,simulated annealing (Kirkpatrick, Jr., & Vecchi, 1983) allows search move uphill,controlled temperature parameter. GSAT (Selman, Levesque, & Mitchell, 1992)WalkSAT (Selman, Kautz, & Cohen, 1994) two algorithms developed 1990sallow randomized moves solution cannot improved locally. two algorithmsdiffer way choose variables flip. GSAT makes change minimizesnumber unsatisfied clauses new configuration, WalkSAT selectsvariable that, flipped, results previously satisfied clauses becoming unsatisfied.Variants algorithms WalkSAT GSAT use various strategies, tabusearch (McAllester, Selman, & Kautz, 1997) adapting noise parameter used,help search local minima (Hoos, 2002). Another class approaches basedapplying discrete Lagrangian methods SAT constrained optimization problem(Shang & Wah, 1998). Lagrange mutlipliers used force lead searchlocal traps.SP algorithm (Braunstein et al., 2005) shown beat best incompletesolvers solving hard random 3-SAT instances efficiently. SP estimates marginalsvariables chooses fix truth value. size instancereduced removing variables, SP run remaining instance.iterative process called decimation SP literature. shown empirically SPrarely makes mistakes decimation, SP solves large 3-SAT instanceshard local search algorithms. Recently, Braunstein Zecchina (2006)236fiRelaxed Survey Propagation Weighted Max-SAT Problemshown modifying BP SP updates reinforcement term, effectivenessalgorithms solvers improved.2.5 Weighted Max-SAT Problemweighted Max-SAT problem generalization Max-SAT problem,clause assigned weight. define instance weighted Max-SAT problemfollows:Definition 3. weighted Max-SAT instance (X, C, W) CNF consists X, vectorN variables taking values {1, +1}, C, set clauses, W, set weightsclause C. define energy weighted Max-SAT problemE(x) =w(1 + J,i xi ),2C iV ()X(13)x {1, +1}N , J,i takes values {1, +1}, w weight clause. total energy, E(x), configuration x equals total weight violated clauses.Similarly SAT, also complete incomplete solvers weighted MaxSAT problem. Complete weighted Max-SAT solvers involve branch bound techniquescalculating bounds cost function. Larrosa Heras (2005) introduced frameworkintegrated branch bound techniques Max-DPLL algorithm solvingMax-SAT problem. Incomplete solvers generally employ heuristics similarused SAT problems. example incomplete method min-conflicts hillclimbing random walks algorithm (Minton, Philips, Johnston, & Laird, 1992). ManySAT solvers WalkSAT extended solve weighted Max-SAT problems,weights used criterion selection variables flip.working example paper, define following instance weightedMax-SAT problem:Example 2. define set weighted Max-SAT clauses following table:Id123456ClauseWeightx1 x21x2 x32x3 x13x1 x2 x34x1 x2 x35x1 x26Energy--3333551--+3353359-+3533332-++3353333+-5333331+-+5333331++3533332+++3335334weighted Max-SAT example variables clauses Max-SATexample given Example 1. table, show clauses satisfied (a tick)violated (a cross) 8 possible configurations 3 variables. first237fiChieu & Leerow, symbol corresponds value 1, + corresponds +1. example,string + corresponds configuration (X1 , X2 , X3 ) = (1, 1, +1). last rowtable shows energy configuration column.factor graph weighted Max-SAT example oneMax-SAT example Example 1. differences two examplesclause weights, reflected joint distribution, factor graph.energy example follows:123E(x) = (1 + x1 )(1 x2 ) + (1 + x2 )(1 x3 ) + (1 + x3 )(1 x1 ) +444456(1 + x1 )(1 + x2 )(1 + x3 ) + (1 x1 )(1 x2 )(1 x3 ) + (1 x1 )(1 x2 )(14)8842.6 Phase TransitionsSP algorithm shown work well 3-SAT instances near phase transition,instances known hard solve. term phase transition arisesphysics community. understand notion hardness optimizationproblems, computer scientists physicists studying relationshipcomputational complexity computer science phase transitions statistical physics.statistical physics, phenomenon phase transitions refers abrupt changesone physical properties thermodynamic magnetic systems smallchange value variable temperature. computer science,observed random ensembles instances K-SAT, sharpthreshold randomly generated problems undergo abrupt change properties.example, K-SAT, observed empirically clause variable ratiochanges, randomly generated instances change abruptly satisfiable unsatisfiableparticular value , often denoted c . Moreover, instances generated valueclose c found extremely hard solve.Computer scientists physicists worked bounding calculating precise value c phase transition 3-SAT occurs. Using cavity approach,physicists claim c 4.267 (Mezard & Zecchina, 2002). derivationvalue c non-rigorous, based derivation formulated SPalgorithm. Using rigorous mathematical approaches, upper bounds value cderived using first-order methods. example, work Kirousis, Kranakis,Krizanc, Stamatiou (1998), c 3-SAT upper bounded 4.571. Achlioptas,Naor Peres (2005) lower-bounded value c using weighted second momentsmethod, lower bound close upper bounds K-SAT ensembles largevalues K. However, lower bound 3-SAT 2.68, rather far conjecturedvalue 4.267. better (algorithmic) lower bound 3.52 obtained analyzingbehavior algorithms find SAT configurations (Kaporis, Kirousis, & Lalas, 2006).Physicists also shown rigorously using second moment methods approaches c , search space fractures dramatically, many small solution clustersappearing relatively far apart (Mezard, Mora, & Zecchina, 2005). Clusterssolutions generally defined set connected components solution space,two adjacent solutions Hamming distance 1 (differ one variable). Daude,238fiRelaxed Survey Propagation Weighted Max-SAT Problemk'M'jM''j''MkMjjMiMllFigure 3: Illustration messages BP algorithm.Mezard, Mora, Zecchina (2008) redefined notion clusters using conceptx-satisfiability: SAT instance x-satisfiable exists two solutions differingN x variables, N total number variables. showed near phasetransition, x goes around 12 small values, without going phaseintermediate values. clustering phenomenon explains instances generatedclose c extremely hard solve local search algorithm: difficultlocal search algorithm move local minimum global minimum.3. Belief Propagation AlgorithmBP algorithm reinvented different fields different names. example,speech recognition community, BP algorithm known forward-backwardprocedure (Rabiner & Juang, 1993). tree-structured factor graphs, BP algorithmsimply dynamic programming approach applied tree structure, shownBP calculates marginals variable factor graph (i.e. solving Problem2). loopy factor graphs, BP algorithm found provide reasonableapproximation solving marginal problem algorithm converges. case,BP algorithm often called loopy BP algorithm. Yedidia, Freeman Weiss (2005)shown fixed points loopy BP algorithm correspond stationarypoints Bethe free energy, hence sensible approximate method estimaingmarginals.section, first describe BP algorithm dynamic programmingmethod solving marginal problem (Problem 2) tree-structured factor graphs.also briefly describe BP algorithm applied factor graphs loops,refer reader work Yedidia et al. (2005) underlying theoreticaljustification case.Given factor graph representing distribution p(x), BP algorithm involves iteratively passing messages factor nodes F variable nodes V , vice versa.factor node represents factor , factor joint distribution givenEquation 1. Figure 3, give illustration messages passedfactor nodes variable nodes. Greek alphabet (e.g. F ) square representsfactor (e.g. ) Roman alphabet (e.g. V ) circle represents variable(e.g. Xi ).factor variable messages (e.g. Mi ), variable factor messages (e.g.Mi ) vectors real numbers, length equal cardinality variable Xi .239fiChieu & Leedenote Mi (xi ) Mi (xi ) component vector correspondingvalue Xi = xi .message update equations follows:Mj (xj ) =0 V(15)(j)\XMi (xi ) =0 j (xj )(xV () )xV () \xiMj (xj ),(16)jV ()\iPxV () \xi means summing configurations XV () Xi set xi .tree-structured factor graph, message updates scheduledtwo parses tree structure, messages converge. messages converge,beliefs variable node calculated follows:Bj (xj ) =Mj (xj ).(17)V (j)tree-structured graph, normalized beliefs variable equalmarginals.INPUT: joint distribution p(x) defined tree-structured factor graph ({V, F }, E),variable Xi X.OUTPUT: Exact marginals variable Xi .ALGORITHM :1. Organize tree Xi root tree.2. Start leaves, propagate messages child nodes parent nodesright root Xi Equations 15 16.3. marginals Xi obtained normalized beliefs Equation 17.Figure 4: BP algorithm calculating marginal single variable, Xi ,tree-structured factor graphalgorithm calculating exact marginals given variable Xi , givenFigure 4. algorithm simply dynamic programming procedure calculatingmarginals, pi (Xi ), organizing sums sums leaves done first.simple example Figure 1, calculating p1 (x1 ), sum ordered follows:p1 (x1 ) =Xp(x)x2 ,x3 ,x4= (x1 , x2 )X X.x2x32400 (x1 , x3 )Xx400 (x2 , x4 )fiRelaxed Survey Propagation Weighted Max-SAT ProblemBP algorithm simply carries sum using node X1 roottree-structured factor graph Figure 1.BP algorithm also used calculating marginals variables efficiently,message passing schedule given Figure 5. schedule involves selectingrandom variable node root tree, passing messages leavesroot, back leaves, two parses, message updatesrequired algorithm Figure 4 one variable would performed,beliefs variables calculated messages. normalized beliefsvariable equal marginals variable.INPUT: joint distribution p(x) defined tree-structured factor graph (V, F ).OUTPUT: Exact marginals variables V .ALGORITHM :1. Randomly select variable root.2. Upward pass: starting leaves, propagate messages leaves righttree.3. Downward pass: root, propagate messages back leaves.4. Calculate beliefs variables given Equation 17.Figure 5: BP algorithm calculating marginals variables treestructured factor graphfactor graph tree-structured (i.e. contains loops), message updatescannot scheduled simple way described algorithm Figure 5. case,still apply BP iteratively updating messages Equations 15 16, oftenround-robin manner factor-variable pairs. done messagesconverge (i.e. messages change iterations). guaranteemessages converge general factor graphs. However, converge,observed beliefs calculated Equation 17 often good approximationexact beliefs joint distribution (Murphy, Weiss, & Jordan, 1999).applied manner, BP algorithm often called loopy BP algorithm. Recently,Yedidia, Freeman Weiss (2001, 2005) shown loopy BP underlyingvariational principle. showed fixed points BP algorithm correspondstationary points Bethe free energy. fact serves sense justifyBP algorithm even factor graph operates loops, minimizingBethe free energy sensible approximation procedure solving marginal problem.refer reader work Yedidia et al. (2005) details.241fiChieu & Lee4. Survey Propagation: SP SP-y AlgorithmsRecently, SP algorithm (Braunstein et al., 2005) shown beat bestincomplete solvers solving hard 3-SAT instances efficiently. SP algorithm firstderived principles statistical physics, explained using cavity approach(Mezard & Parisi, 2003). first given BP interpretation work BraunsteinZecchina (2004). section, define SP SP-y algorithmssolving SAT Max-SAT problems, using warning propagation interpretationalgorithms.4.1 SP Algorithm SAT ProblemSection 2.4.1, defined joint distribution SAT problem (X, C),energy function configuration equal number violated clauses configuration. factor graph ({V, F }, E) representing joint distribution, variablenodes V correspond Boolean variables X, factor node F representsclause C. section, provide intuitive overview SP algorithmformulated work Braunstein et al. (2005).SP algorithm defined message passing algorithm factor graph({V, F }, E). factor F passes single real number, neighboring variableXi factor graph. real number called survey. According warningpropagation interpretation given work Braunstein et al. (2005), surveycorresponds probability1 warning factor sending variableXi . Intuitively, close 1, factor warning variable Xitaking value violate clause . close 0, factorindifferent value taken Xi , clause satisfiedvariables V ().first define messages sent variable Xj neighboring factor ,function inputs factors containing Xj , i.e. { 0 j } 0 V (j)\ . SP,message vector three numbers, uj , sj , 0j , followinginterpretations:uj probability Xj warned (by clauses) take value violateclause .sj probability Xj warned (by clauses) take value satisfyclause .0j probability Xj free take value.defintions, update equations follows:uj = [1sj = [1(1 0 j )](1 0 j ),0 Vu (j)0 Vs (j)(1 0 j )]0 Vs (j)(1 0 j ),(18)(19)0 Vu (j)1. SP reasons clusters solutions, probability warning section used looselySP literature refer fraction clusters warning applies. next section,define rigorous probability distribution covers RSP algorithm.242fiRelaxed Survey Propagation Weighted Max-SAT Problem0j =(1 0 j ),(20)0 V (j)=ujjV ()iuj + sj + 0j(21)equations defined using sets factors Vu (j) Vs (j),defined Section 2.4.1. event variable Xj warned take valueviolating , (a) warned least one factor 0 Vu (j) take satisfyingvalue 0 , (b) factors Vs (j) sending warnings. Equation18, probability event, uj , product two terms, first correspondingevent (a) second event (b). definitions sj 0j definedsimilar manner. Equation 21, final survey simply probability jointevent incoming variables Xj violating clause , forcing last variable Xisatisfy .SP algorithm consists iteratively running update equationssurveys converge. surveys converged, calculate local biases follows:= [1+j(1 0 j )]V0j=(1 j ),(22)V (j)V + (j)= [1+j(1 0 j )](j)V(1 j ),(1 j ),(23)+ (j)(24)V (j)Wi+ =Wi =+j(25)0+j + j + jj(26)0+j + j + jsolve instances SAT problem, SP algorithm run converges,variables highly constrained set preferred values. SATinstance reduced smaller instance, SP run smallerinstance. continues SP fails set variables, case, localsearch algorithm WalkSAT run remaining instance. algorithm, calledsurvey inspired decimation algorithm (Braunstein et al., 2005), given algorithmFigure 6.4.2 SP-y Algorithmcontrast SP algorithm, SP-y algorithms objective solve Max-SAT instances, hence clauses allowed violated, price. SP algorithmunderstood special case SP-y algorithm, taken infinity (Battagliaet al., 2004). SP-y, penalty value exp(2y) multiplied distributionviolated clause. Hence, although message passing algorithm allows violationclauses, value increases, surveys prefer configurations violateminimal number clauses.243fiChieu & LeeINPUT: SAT problem, constant k.OUTPUT: satisfying configuration, report FAILURE.ALGORITHM :1. Randomly initialize surveys.2. Iteratively update surveys using Equations 18 21.3. SP converge, go step 7.4. SP converges, calculate Wi+ Wi using Equations 25 26.5. Decimation: sort variables based absolute difference |Wi+ Wi |,set top k variables preferred value. Simplify instancevariables removed.6. surveys equal zero, (no variables removed step 5), outputsimplified SAT instance. Otherwise, go back first step smallerinstance.7. Run WalkSAT remaining simplified instance, output satisfyingconfiguration WalkSAT succeeds. Otherwise output FAILURE.Figure 6: survey inspired decimation (SID) algorithm solving SAT problem(Braunstein et al., 2005)SP-y algorithm still understood message passing algorithm factorgraphs. SP, factor, , passes survey, , neighboring variable Xi ,+corresponding probability warning. simplify notations, define(resp.) probability warning taking value +1 (resp. 1),+0define= 1. practice, since clause warnJJ,i+either +1 1 both, eitherequals zero:= , i,i = 0,J,i defined Definition 1.Since clauses violated, insufficient simply keep track whether variablewarned value not. necessary keep track manytimes variable warned value, know many clauses+violated variable take particular value. Let Hj(resp. Hj)0number times variable Xj warned factors { } 0 V (j)\ value++1 (resp. 1). SP-y, variable Xj forced take value +1 Hj+smaller Hj , penalty case exp(2yHj ). notations used+work Battaglia et al. (2004) describing SP-y, let hj = HjHj.Battaglia et al. (2004) defined SP-y message passing equations calculateprobability distribution hj , based input surveys,{ 0 j } 0 V (j)\ = {1 j , 2 j , ..., (|V (j)|1) j },244(27)fiRelaxed Survey Propagation Weighted Max-SAT Problem|V (j)| refers cardinality set V (j). unnormalized distributionsPej (h) calculated follows:(1)Pej (h) = 01 (h) + +1 (h 1) + 1 (h + 1), (28)[2, |V (j)| 1],()(1)Pej (h) = 0 Pej (h)(1)++ Pej (h 1) exp [2y(h)](1)+ Pej (h + 1) exp [2y(h)],(|V (j)|1)Pej (h) = Pej(h),(29)(30)(h) = 1 h = 0, zero otherwise, (h) = 1 h 0, zero otherwise.equations take account neighbor j excluding , = 1= |V (j)|1. penalties exp(2y) multiplied every time value hj decreasesabsolute value, new neighbor Xj , , added. end procedure,+equivalent multiplying messages factor exp(2ymin(Hj, Hj)).Pej (h) normalized Pj (h) computing Pej (h) possiblevalues h [|V (j)| + 1, |V (j)| 1]. message updates surveys follows:|V (j)|1+Wj=XPj (h),h=11XWj=Pj (h),(31)(32)h=|V (j)|+1Ji,i= 0,J,i=(33)J,jWj,(34)jV (j)\iJ,i0= 1,(35)+quantity Wj(resp. Wj) probability events warning value+1 (resp. 1). Equation 34 reflects fact warning sent variableXi variables warning going violate .SP-y converges, preference variable calculated follows:|V (j)|Wj+ =Wj =XPj (h),h=11XPj (h),(36)(37)h=|V (j)|Pj (h) calculated similar manner Pj (h), exceptexclude calculations.definitions message updates, SP-y algorithm usedsolve Max-SAT instances survey inspired decimation algorithm similar one245fiChieu & LeeSP given algorithm Figure 6. iteration decimation process, SP-ydecimation procedure selects variables fix preferred values based quantitybf ix (j) = |Wj+ Wj |(38)work Battaglia et al. (2004), additional backtracking process introduced make decimation process robust. backtracking process allowsdecimation procedure unfix variables already fixed values. variable Xjfixed value xj , following quantities calculated:bbacktrack (j) = xj (Wj+ Wj )(39)Variables ranked according quantity top variables chosenunfixed. algorithm Figure 7, show backtracking decimation algorithmSP-y (Battaglia et al., 2004), value either given input,determined empirically.INPUT: Max-SAT instance constant k. Optional input: yin backtrackingprobability r.OUTPUT: configuration.ALGORITHM :1. Randomly initialize surveys.2. yin given, set = yin . Otherwise, determine value bisectionmethod.3. Run SP-y convergence. SP-y converges, variable Xi , extractrandom number q [0, 1].(a) q > r, sort variables according Equation 38 fix top kbiased variables.(b) q < r sort variables according Equation 39 unfix top kbiased variables.4. Simplify instance based step (3). SP-y converged return nonparamagnetic solution (a paramagnetic solution refers set {bf ix (j)}jVbiased value variables), go step (1).5. Run weighted WalkSAT remaining instance outputs best configuration found.Figure 7: survey inspired decimation (SID) algorithm solving Max-SAT instance(Battaglia et al., 2004)246fiRelaxed Survey Propagation Weighted Max-SAT Problem5. Relaxed Survey Propagationshown (Maneva et al., 2004; Braunstein & Zecchina, 2004) SP SATproblem reformulated BP algorithm extended factor graph. However,formulation cannot generalized explain SP-y algorithm applicableMax-SAT problems. previous paper (Chieu & Lee, 2008), extended formulationwork Maneva et al. (2004) address Max-SAT problem. section,modify formulation previous paper (Chieu & Lee, 2008) addressweighted Max-SAT problem, setting extended factor graph run BPalgorithm. Theorem 3, show formulation generalizes BP interpretationSP given work Maneva et al. (2004), main theorem (Theorem 2),show running loopy BP algorithm factor graph estimates marginalscovers configurations violating set clauses minimal total weight.first define concept covers Section 5.1, defining extendedfactor graph Section 5.2. rest section, given weighted Max-SAT problem(X, C, W), assume variables X take values {1, +1, }: third valuedont care state, corresponding no-warning message SP algorithm definedSection 4.5.1 Covers Weighted Max-SATFirst, need define semantics value dont care state.Definition 4. (Maneva et al., 2004) Given configuration x, say variable Xiunique satisfying variable clause C assigned s,i whereasvariables Xj clause assigned u,j (see Definition 2 definitions s,iu,i ). variable Xi said constrained clause unique satisfyingvariable . variable unconstrained constrained clauses. DefineCONi, (x ) = Ind(xi constrained ),(40)Ind(P ) equals 1 predicate P true, 0 otherwise.illustration, consider configuration X = (+1, 1, 1) Example 2.configuration, X1 = +1 constrained clauses 5 6 , X2 = 1 constrained2 , X3 = 1 unconstrained: flipping X3 +1 violate additionalclauses configuration.following definition, redefine configuration taking values {1, +1, }satisfies violates clauses.Definition 5. configuration satisfies clause (i) contains variable Xiset value s,i , (ii) least two variables take value . configurationviolates clause variables Xj set u,j . configuration x invalidclause exactly one variables set ,remaining variables set u,i . configuration valid valid clausesC.definition invalid configurations reflects interpretation valuedont care state: clauses containing variable Xi = already satisfied247fiChieu & Leevariables, value Xi matter. Xi = cannot last remainingpossibility satisfying clause. case clause contains two variables set, clause satisfied either one two variables, variabletake dont care value.define partial order set valid configurations follows (Maneva et al.,2004):Definition 6. Let x two valid configurations. write x i, (1) xi = yi(2) xi = yi 6= .partial order defines lattice, Maneva et al. (2004) showed SPpeeling procedure peels satisfying configuration minimal elementlattice. cover minimal element lattice. SAT region, coverdefined follows (Kroc, Sabharwal, & Selman, 2007):Definition 7. cover valid configuration x {1, +1, }N satisfies clauses,unconstrained variables assigned -1 +1.SP algorithm shown return marginals covers (Maneva et al., 2004).principle, two kinds covers: true covers correspond satisfyingconfigurations, false covers not. Kroc et al. (2007) showed empiricallynumber false covers negligible SAT instances. RSP apply weightedMax-SAT instances, introduce notion v-cover:Definition 8. v-cover valid configuration x {1, +1, }N1. total weight clauses violated configuration equals v,2. x unconstrained variables assigned -1 +1.Hence covers defined Definition 7 simply v-covers v = 0 (i.e. 0-covers).5.2 Extended Factor Graphsection, define joint distribution extended factor graphpositive v-covers. First, need define functions useddefine factors extended factor graph.Definition 9. clause, C, following function assigns different valuesconfigurations satisfy, violate invalid (see Definition 5) :VAL (xV () ) =1exp(w y)0xV () satisfiesxV () violatesxV () invalid(41)definition, introduced parameter RSP algorithm, playssimilar role SP-y algorithm. term exp(w y) penaltyviolating clause weight w .248fiRelaxed Survey Propagation Weighted Max-SAT ProblemDefinition 10. (Maneva et al., 2004) Given configuration x, define parent setPi (x) variable Xi set clauses Xi = xi unique satisfyingvariable configuration x, (i.e. set clauses constraining Xi value). Formally,Pi (x) = { C| CONi, (xV() ) = 1}(42)Example 2, configuration x = (+1, 1, 1), parent sets P1 (x) ={5 , 6 }, P2 (x) = {2 }, P3 (x) = .Given weighted Max-SAT instance (X, C, W) factor graph, G = ({V, F }, E),construct another distribution associated factor graph Gs = ({V, Fs }, Es )follows. V , let P (i) set possible parent sets variable Xi . Duerestrictions imposed definition, Pi (x) must contained either V + (i)+V (i), both. Therefore, cardinality P (i) 2|V (i)| +2|V (i)| 1. extendedfactor graph defined set variables = (1 , 2 , ..., n ) X1 X2 ... Xn ,Xi := {1, +1, } P (i). Hence factor graph number variablesoriginal SAT instance, variable large cardinality. Given configurationsx SAT instance, denote configurations (x) = {i (x)}iV , (x) =(xi , Pi (x)).definitions given far define semantics valid configurations parent sets,rest section, define factors extended factor graph Gsensure definitions satisfied configurations .single variable compatibilities (i ) defined following factorvariable (x):(i (x) = {xi , Pi (x)}) =011Pi (x) = , xi 6=Pi (x) = , xi =.valid (xi , Pi (x))(43)first case definition Pi (x) = xi 6= corresponds casevariable Xi unconstrained, yet takes value {1, +1}. Valid configurationsv-covers (with unconstrained variables set 1 +1) zero valuefactor. Hence v-covers positive value factors. lastcase definition, validity (xi , Pi (x)) simply means xi = +1 (resp.xi = 1), Pi (x) V + (i) (resp. Pi (x) V (i).).clause compatibilities ( ) are:((x)V () ) = VAL (xV() )QkV () Ind [ Pk (x)] = CON,k (xV () ) ,(44)Ind defined Definition 4. clause compatibilities introduce penaltiesVAL (xV () ) joint distribution. second term equation enforcesparent sets Pk (x) consistent definitions parent sets Definition 10variable Xk clause .values x determines uniquely values P = {Pi (x)}iV , hencedistribution (x) = {xi , Pi (x)}iV simply distribution x.Theorem 1. Using notation UNSAT(x) represent set clauses violatedx, underlying distribution p() factor graph defined section positive249fiChieu & Lee'6'1'1'31'22'5'23'3'4Figure 8: extended factor graph SAT instance given Example 1. factornodes i0 correspond clause compatibility factors , single variablefactor nodes i0 represents single variable compatibility factors . factorgraph similar original factor graph SAT instance Figure 2, exceptadditional factor nodes i0 .v-covers, v-cover x, have:p(X = x) = p( = (x))exp(w y),(45)UNSAT(x)Proof. Configurations v-covers either invalid contains unconstrainedvariables set 1 +1. invalid configurations, distribution zerodefinition VAL , configurations unconstrained variables set 1 +1,distribution zero due definition factors . v-cover, totalpenalty violated clauses product term Equation 45.definition defines joint distribution factor graph. RSP algorithmmessage passing algorithm defined factor graph:Definition 11. RSP algorithm defined loopy BP algorithm appliedextended factor graph Gs associated MaxSAT instance (X, C, W).Section 6, formulate message passing updates RSP, welldecimation algorithm using RSP solver weighted Max-SAT instances.example, Figure 8 shows extended factor graph weighted Max-SAT instancedefined Example 1.Definition 12. define min-cover weighted Max-SAT instance m-cover,minimum total weight violated clauses instance.Theorem 2. taken , RSP estimates marginals min-coversfollowing sense: stationary points RSP algorithm correspond stationarypoints Bethe free energy distribution uniform min-covers.250fiRelaxed Survey Propagation Weighted Max-SAT Problem-1,-1,-1Energy = 11-1,-1,+1Energy = 9+1,+1,+1Energy = 4-1,+1,+1Energy = 3-1,+1,-1+1,-1,-1+1,-1,+1+1,+1,-1*,+1,-1+1,-1,*Energy = 2Energy = 1Figure 9: Energy landscape weighted Max-SAT instance given Example 2.node represents configuration variables (x1 , x2 , x3 ). example,node (1, +1, 1) represents configuration (x1 , x2 , x3 ) = (1, +1, 1).Proof. ratio probability v-cover (v + )-cover equals exp(y).taken , distribution Equation 45 positive min-covers.Hence RSP, loopy BP algorithm factor graph representing Equation 45,estimates marginals min-covers.application RSP weighted Max-SAT instances, taking would oftencause RSP algorithm fail converge. Taking sufficiently large value oftensufficient RSP used solve weighted Max-SAT instances.Figure 9, show v-covers small weighted Max-SAT example Example 2.example, unique min-cover X1 = +1, X2 = 1, X3 = .Maneva et al. (2004) formulated SP- algorithm, equivalent SPalgorithm (Braunstein et al., 2005) = 1. SP- algorithm loopy BP algorithmextended factor graph defined work Maneva et al. (2004). Comparingdefinitions extended factor graph factors RSP SP-, (Chieu &Lee, 2008):Theorem 3. taking , RSP equivalent SP- = 1.Proof. definitions joint distribution SP- = 1 (Maneva et al., 2004),RSP paper differ Definition 9, RSP,definitions become identical. Since SP- RSP equivalent loopy BPdistribution defined extended factor graphs, equivalence jointdistribution means algorithms equivalent.Taking infinity corresponds disallowing violated clauses, SP- formulatedsatisfiable SAT instances, clauses must satisfied. SP-, clause weightsinconsequential clauses satisfied.paper, disallow unconstrained variables take value . AppendixA, give alternative definition single variable potentials Equation 43.251fiChieu & Leedefinition, Maneva et al. (2004) defines smoothing interpretation SP-.smoothing also applied RSP. See Theorem 6 work Maneva et al. (2004)Appendix details.5.3 Importance Convergencefound message passing algorithms BP SP algorithms performwell whenever converge (e.g., see Kroc, Sabharwal, & Selman, 2009). successRSP algorithm random ensembles Max-SAT weighted Max-SAT instancesbelieved due clustering phenomenon problems, foundRSP could also successful cases clustering phenomenon observed.believe presence large clusters help SP algorithm converge well,long SP algorithm converges, presence clusters necessary goodperformance.covers simply Boolean configurations (with variables taking value),represent singleton clusters. call covers degenerate covers. many structurednon random weighted Max-SAT problems, found covers foundoften degenerate. previous paper (Chieu, Lee, & Teh, 2008), defined modifiedversion RSP energy minimization factor graphs, show Lemma 2paper configurations * zero probability, i.e. covers degenerate.paper, showed value tuned favor convergenceRSP algorithm.Section 7.3, show success RSP benchmark Max-SAT instances.trying recover covers configurations found RSP, foundbenchmark instances used degenerate covers. fact RSP convergedinstances sufficient RSP outperform local search algorithms.6. Using RSP Solving Weighted Max-SAT Problemprevious section, defined RSP algorithm Definition 11 loopy BPalgorithm extended factor graph. section, derive RSP messagepassing algorithm based definition, giving decimation-based algorithmused solving weighted Max-SAT instances.6.1 Message Passing Algorithmvariables extended factor graphs longer Boolean. form(x) = (xi , Pi (x)), large cardinalities. definition BP algorithm,stated message vector passed factors variables lengthequal cardinality variables. section, show messages passedRSP grouped groups, message passed variablesfactors three values.RSP, factor variable messages grouped follows:Mixi = s,i , Pi (x) = {}, Vs (i),(all cases variable xi constrained clause ),252fiRelaxed Survey Propagation Weighted Max-SAT ProblemuMixi = u,i , Pi (x) Vu (i),(all cases variable xi constrained u,i clauses),Mixi = s,i , Pi (x) Vs (i),(all cases variable xi = s,i constrained . least onevariable xj satisfies equals . Otherwise xi constrained),Mixi = , Pi (x) = .last two messages always equal:Mi= Mi= Mi.equality due fact factor constraining variables,matter whether variable satisfying , long least twovariables either satisfying . following, consider two equal.messages single message, Mivariable factor messages grouped follows::=RiSVs (i) Mia (s,i , {}),Variable xi constrained s,i ,PuRi:=Pi (x)Vu (i) Mia (u,i , Pi (x)),Variable xi constrained clauses u,i ,P:=RiPi (x)Vs (i) Mia (s,i , Pi (x)),Variable xi constrained , constrained clauses s,i ,P:=Ri(, ),Variable xi unconstrained equals *.last two messages grouped one message (as done previouspaper, Chieu & Lee, 2008) follows,Ri= Ri+ Ri,since calculating updates Mj messages Ri messages, Rirequired. update equations RSP weighted Max-SAT given Figure 10.update equations derived based loopy BP updates Equations 15 16Section 3. worst case densely connected factor graph, iteration updatesperformed O(M N ) time, N number variables, numberclauses.6.1.1 Factor Variable Messagesbegin update equations messages factors variables, givenEquations 46, 47 48. message Migroups cases Xi constrained253fiChieu & LeeMi=uRj(46)jV ()\{i}uMi=u(Rj+ Rj)+jV ()\{i}w+(eMi=(RkRk)kV ()\{i}uRjjV ()\{i,k}u1)RjjV ()\{i}u(Rj+ Rj)XjV ()\{i}(47)uRj(48)jV ()\{i}Ri=uMi(Mi+ Mi)Vu (i)(49)Vs (i)uRi=(Mi+ Mi)Vs (i)uMiVu (i)MiVau (i)Ri=Vu (i)+(50)uMi(Mi+ Mi)Vs (i)MiVs (i)Mi(51)Vs (i)Vu (i)Bi (1)VuMi+ (i)V(Mi+ Mi)(i)VBi (+1)VBi ()uMi(i)(52)(i)MiV(Mi+ Mi)+ (i)MiVMi(53)+ (i)(54)V (i)Figure 10: update equations RSP. equations BP equations factorgraph defined text.254fiRelaxed Survey Propagation Weighted Max-SAT Problemfactor . means variables violating factor , henceEquation 46Mi=uRj,jV ()\{i}uRjmessages neighbors stating violate .unext equation Mistates variable Xi violating . case,variables possible cases1. Two variables satisfying , message updateu(Rj+ Rj)jV ()\{i}RkXkV ()\{i}uRjjV ()\{i,k}uRj.jV ()\{i}2. Exactly one variable V ()\{i} constrained , variables violating, message updateXRkuRjjV ()\{i,k}kV ()\{i}3. variables violating , case, penalty factorexp(w y), message updateexp(w y)uRjjV ()\{i}sum three cases result Equation 48.third update equation Micase variable Xi uncons ) (for ). meansstrained , satisfying s,i (for case Mileast one satisfying variable unconstrained , messageupdateu(Rj+ Rj)jV ()\{i}uRjjV ()\{i}6.1.2 Variable Factor Messagesconsists case variable Xi constrainedfirst message Rifactor , means satisfies neighboring factors Vs (i), violates factorsVu (i), probabilityVu (i)uMi(Mi+ Mi) .Vs (i)usecond message Ricase Xi violates . case, variablesuV (i) satisfied, clauses Vs (i) violated. case, variable Xi must255fiChieu & Leeconstrained one clauses Vu (i). Hence message updateVs (i)uMi(Mi+ Mi)Vu (i)Vu (i)Mi. messagethird message Risum two messages RiRivariable Xi satisfies constrained , must constrainedfactors:,RiVu (i)uMi(Mi+ Mi)Vs (i)Vs (i)Mi, case X = , :second part message, RiMi,Vs (i)Vu (i)sum two equations results Equation 51.6.1.3 Beliefsbeliefs calculated factor variable messages algorithm converges, obtain estimates marginals min-covers. calculation beliefssimilar calculation variable factor messages.belief Bi (1) belief variable Xi taking value 1. casevariable Xi satisfies clauses V (i), violates clauses V + (i). case,Xi must constrained one factors V (i). Hence belief follows:V + (i)uMi(Mi+ Mi)V (i)Mi.V (i)calculation belief Bi (+1) similar Bi (1). belief Bi () caseXi = , hence calculated follows:Mi.V ( i)6.2 Comparing RSP SP-y Message Passing Algorithmsmessage passing algorithms RSP SP-y share many similarities. algorithms1. include multiplicative penalty distribution violated clause.2. contain mechanism dont care state. SP-y, occurs variablereceives warnings neighboring factors.However, number significant differences two algorithms.256fiRelaxed Survey Propagation Weighted Max-SAT Problem1. RSP, penalties imposed factor passes message variable.SP-y, penalties imposed variable compiles incoming warnings,decides many factors going violate.2. Importantly, RSP, variables participating violated clauses never take *value. SP-y, variable receiving equal number warnings set+factors { 0 } 0 V (i)\ taking +1 1 value (i.e. hj = HjHj = 0) decide pass message warning . Hence SP-y,possible variables violated clauses take dont care state.3. work Battaglia et al. (2004) SP-y formulated cavityapproach, found optimal value given Max-SAT problem=e , complexity statistical physics, e energy density(Mezard & Zecchina, 2002). stated finite value energyMax-SAT problem zero. Theorem 2, show RSP,large possible underlying distribution min-covers.experimental results Figure 12, showed indeed true RSP, longconverges.INPUT: (weighted) Max-SAT instance, constant k, yinOUTPUT: configuration.ALGORITHM :1. Randomly initialize surveys set = yin .2. Run RSP y. RSP converges, sort variables according quantitiesbi = |P (xi = +1) P (xi = 1)|, fix top k variables preferredvalues, subject condition bi > 0.5.3. (For weighted Max-SAT) RSP fails converge, adjust value y.4. RSP converges least one variable set, go back step (1) simplified instance. Otherwise, run (weighted) WalkSAT solver simplifiedinstance output configuration found.Figure 11: decimation algorithm RSP solving (weighted) Max-SAT instance6.3 Decimation Algorithmdecimation algorithm shown Figure 11. algorithm usedexperiments described Section 7. comparing RSP SP-y random Max-SATinstances Section 7.1, run algorithms fixed yin , vary yinrange values. Comparing Figure 11 Figure 7 SP-y, condition used SPy check paramagnetic solution replaced condition given Step (2)Figure 11. experimental results Section 7.1, used SP-y implementation257fiChieu & Leeavailable online (Battaglia et al., 2004), contains mechanism backtrackingdecimation decisions (see Figure 7). Section 7.1, RSP still outperforms SP-y despitebacktracking decisions. running RSP weighted Max-SAT, foundnecessary adjust dynamically decimation process. detailsexperimental settings, please refer Section 7.7. Experimental Resultsrun experiments random Max-3-SAT, random weighted Max-SAT, wellbenchmark Max-SAT instances used work Lardeux, Saubion, Hao (2005).#Viols#ViolsFigure 12: Behaviour SP-y RSP varying values x-axis,number violated clauses (#viols) y-axis. comparison performances RSP SP-y shown Table 1. objective showinggraphs figure show behavior RSP varyingconsistent Theorem 2: long RSP converges, performance improvesincreases. graph, RSP reaches plateau fails converge.Thisproperty allows systematic search good value used.behavior SP-y varying less consistent..7.1 Random Max-3-SATrun experiments randomly generated Max-3-SAT instances 104 variables,different clause-to-variable ratios. random instances generated SP-y codeavailable online (Battaglia et al., 2004). Figure 12, compare SP-y RSP randomMax-3-SAT different clause-to-variable ratio, . vary 4.2 5.2 showperformance SP-y RSP UNSAT region 3-SAT, beyond phase transitionc 4.267. value , number violated clauses (y-axis) plottedvalue used.258fiRelaxed Survey Propagation Weighted Max-SAT Problemperform decimation procedure Figure 11 RSP, fixed value yin ,decimating 100 variables time (i.e. k = 100). SP-y, run SP-y code availableline, option decimating 100 variables iteration, two differentsettings: without backtracking (Battaglia et al., 2004). Backtracking procedure used SP-y improve performance, unfixing previously fixed variables rater = 0.2, errors made decimation process corrected. RSP,run backtracking. Note formulation equals 2y formulationwork Battaglia et al. (Battaglia et al., 2004).SP-y RSP fail converge becomes large enough. happens,output algorithm result returned WalkSAT original instance.Figure 12, see happening curve reaches horizontal line, signifyingalgorithm returning configuration regardless (we seed randomizedWalkSAT results identical instances identical). Figure 12,see RSP performs consistently SP-y: increases, performance RSPimproves, point RSP fails converge. Interestingly Max-3-SAT instances,observed RSP converges value given instance, continueconverge value throughout decimation process. Hence, best valueRSP obtainable without going decimation process: commencedecimation largest value RSP converges. Table 1, show RSPoutperforms SP-y 4.7, despite fact allow backtracking RSP.also compare RSP SP-y local search solvers implemented UBCSAT(Tompkins & Hoos, 2004). run 1000 iterations 20 Max-SAT solversUBCSAT, take best result among 20 solvers. results shown Table 1.see local solvers UBCSAT worse RSP SP-y.also tried running complete solvers toolbar (de Givry, Heras, Zytnicki, & Larrosa,2005) maxsatz (Li, Manya, & Planes, 2006). unable deal instancessize 104 .7.2 Random Weighted Max-3-SATalso run experiments randomly generated weighted Max-3-SAT instances.instances generated way instances Max-3-SAT, addition,weights clause uniformly sampled integers set [1, ],upper bound weights. show experimental results = 5 = 10Figure 13. compare RSP 13 weighted Max-SAT solvers implementedUBCSAT. RSP, run experiments initial set 10, wheneveralgorithm fails converge, lower value 1, halve valueless 1 (see Figure 11). see RSP outperforms UBCSAT consistentlyexperiments Figure 13.7.3 Benchmark Max-SAT Instancescompare RSP UBCSAT instances used work Lardeux et al. (2005),instances used SAT 2003 competition. Among 27 instances, useseven largest instances 7000 variables. run RSP two settings:decimating either 10 100 variables time. run RSP increasing values y:259fiChieu & LeeTable 1: Number violated clauses attained method. SP-y, SP-y (BT) (SPy backtracking), RSP, best result selected y. ,show best performance bold face. column Fix shows numbervariables fixed RSP optimal y, Time time taken RSP(in minutes) fix variables, AMD Opteron 2.2GHz machine.4.24.34.44.54.64.74.84.95.05.15.2UBCSAT476895128140185232251278311358SP-y09426798137204223260294362SP-y(BT)07316789130189211224280349RSP010366590122172193218267325Fix79007200893890249055928792459208930792949361Time (minutes)2443827645765262664248W-violW-violFigure 13: Experimental results weighted Max-SAT instances. x-axis showsvalue , y-axis (W-viol) number violated clauses returnedalgorithm.y, RSP fixes number spins, stop increasing number spinsfixed decreases previous value y. UBCSAT, run 1000 iterations20 solvers. Results shown Table 2. seven instances, RSP failsfix spins first one, outperforms UBCSAT rest. Lardeux et al. (2005)show best performances paper, average results ordermagnitude higher results Table 2. Figure 12 shows finding good SP-yhard. benchmark instances, run SP-y -Y option (Battaglia et al.,2004) uses dichotomic search y: SP-y failed fix spins 7 instances.260fiRelaxed Survey Propagation Weighted Max-SAT ProblemTable 2: Benchmark Max-SAT instances. Columns: instance shows instance namepaper Lardeux et al. (2005), nvar number variables, ubcsatrsp-x (x number decimations iteration) number violatedclauses returned algorithm, fx-x number spins fixed RSP.Best results indicated bold face.instancefwncnw35-4-0335-4-0440-4-0240-4-03nvar ubcsat rsp-100 fx-100 rsp-10family: purdom-1014277239320402393668335703578372743383393585897324856228family: pyhala-braun-unsat73835868729544738362537302419638865795476596387677952141fx-100831685527299730495219568success SP family algorithms random ensembles SAT Max-SATproblem usually due clustering phenomenon random ensembles.benchmark instances random instances, attempted see configurationsfound RSP indeed belong cover representing cluster solutions. Ratherdisappointingly, found 6 solutions RSP outperformed local searchalgorithms, variables solutions constrained least one clause. Hence,v-covers found degenerate covers, i.e. covers contain variables set. appears success RSP benchmark instances dueclustering phenomenon, simply RSP manages converge instances,value y. Kroc, Sabharwal, Selman (2009) made similar observation:convergence BP SP like algorithms often sufficient obtaining good solutiongiven problem. discussed Section 5.3, ability vary improve convergenceuseful feature RSP, one distinct ability exploit clusteringphenomenon.8. Conclusionrecent work Max-SAT weighted Max-SAT tends focus completesolvers, solvers unable handle large instances. Max-SAT competition2007 (Argelich, Li, Manya, & Planes, 2007), largest Max-3-SAT instances used70 variables. large instances, complete solvers still practical, localsearch procedures feasible alternative. SP-y, generalizing SP,shown able solve large Max-3-SAT instances phase transition, lackstheoretical explanations recent work SP generated.3-SAT, easy-hard-easy transition clause-to-variable ratio increases.Max-3-SAT, however, shown empirically beyond phase transitionsatisfiability, instances hard solve (Zhang, 2001). paper, show261fiChieu & LeeRSP outperforms SP-y well local search algorithms Max-SAT weightedMax-SAT instances, well beyond phase transition region.RSP SP-y well Max-SAT instances near phase transition.mechanisms behind SP-y RSP similar: algorithms impose penalty termviolated constraint, reduce SP . SP-y uses populationdynamics algorithm, also seen warning propagation algorithm.paper, formulated RSP algorithm BP algorithm extended factorgraph, enabling us understand RSP estimating marginals min-covers.Acknowledgmentswork supported part NUS ARF grant R-252-000-240-112.Appendix A. Smoothing Interpretation RSPdefinition SP- (Maneva et al., 2004), parameter introduced definewhole family algorithms. = 1, SP- algorithm corresponds SP algorithm,= 0, SP- algorithm corresponds BP algorithm. section,develop general version extended factor graph defined Section 5,incorporates SP-. call corresponding RSP algorithm newfactor graph RSP- algorithm.difference factor graph RSP- one Section 5definition variable compatibilities Equation 43. Following notations workManeva et al. (2004), introduce parameters 0 , restrictcase 0 + = 1 (The SP- RSP- equal ). redefinevariable compatibilities follows(i (x) = {xi , Pi (x)}) =01Pi (x) = , xi 6=Pi (x) = , xi =,valid (xi , Pi (x))(55)0 + = 1. definition Equation 43 corresponds particular case0 = 0 = 1. Section 5, defined factor graph unconstrainedvariables must take value . new definition above, unconstrained variables allowed take values 1 +1 weight 0 , valueweight .definition, joint distribution Equation 45 redefined follows:n (x) n (x)exp(w y).UNSAT(x)P (x) = P ({xk , Pk }k ) 0 0(56)n0 (x) number unconstrained variables x taking +1 1, n (x)number unconstrained variables taking x.Case = 1: studied case main paper: underlying distributiondistribution positive v-covers.262fiRelaxed Survey Propagation Weighted Max-SAT ProblemCase = 0: case, configurations x n (x) = 0 non-zero probability distribution given Equation 56. Hence, value forbidden,variables take values 1, +1. Boolean configuration violating clauses total weightW probability proportional exp(yW ). Hence retreive weighted Max-SATenergy defined Equation 13. case, factor graph equivalent originalweighted Max-SAT factor graph defined Definition 3, hence RSP- equivalentloopy BP algorithm original weighted Max-SAT problem.Case 6= 1 6= 0: case, valid configurations x violating clausesn (x) n (x)total weight W probability proportional 0 0 exp(yW ). Hence,probability v-covers case = 1 spread latticeminimal element.formulation, RSP- seen family algorithms includeBP RSP algorithm, moving BP RSP (or ) varies 0 1.ReferencesAchlioptas, D., Naor, A., & Peres, Y. (2005). Rigorous location phase transitions hardoptimization problems. Nature, 435 (7043), 759764.Argelich, J., Li, C. M., Manya, F., & Planes, J. (2007). Second evaluation max-satsolvers. SAT07: Tenth International Conference Theory ApplicationsSatisfiability Testing.Battaglia, D., Kolar, M., & Zecchina, R. (2004). Minimizing energy glass thresholds. Physical Review E, 70.Berre, D. L., & Simon, L. (2003). essentials SAT 2003 competition. SAT03:Sixth International Conference Theory Applications Satisfiability Testing.Berre, D. L., & Simon, L. (2005). Special volume SAT 2005 competitionsevaluations. Journal Satisfiability, Boolean Modeling Computation, 2.Braunstein, A., Mezard, M., & Zecchina, R. (2005). Survey propagation: algorithmsatisfiability. Random Structures Algorithms, 27 (2).Braunstein, A., & Zecchina, R. (2004). Survey propagation local equilibrium equations.Journal Statistical Mechanics: Theory Experiment, 2004 (06).Braunstein, A., & Zecchina, R. (2006). Learning message-passing networks discretesynapses. Physical Review Letters, 96, 030201.Chieu, H. L., & Lee, W. S. (2008). Relaxed survey propagation: sum-product algorithmMax-SAT. AAAI08: Twenty-Third AAAI Conference Artificial Intelligence.Chieu, H. L., Lee, W. S., & Teh, Y. W. (2008). Cooled relaxed survey propagationMRFs. NIPS07: Advances Neural Information Processing Systems 20, Cambridge, MA. MIT Press.Cook, S. A. (1971). complexity theorem-proving procedures. STOC 71: Thirdannual ACM symposium Theory computing, pp. 151158, New York, NY, USA.ACM.263fiChieu & LeeCooper, G. F. (1990). computational complexity probabilistic inference usingbayesian belief networks (research note). Artif. Intell., 42 (2-3), 393405.Daude, H., Mezard, M., Mora, T., & Zecchina, R. (2008). Pairs sat-assignments randomboolean formul. Theor. Comput. Sci., 393 (1-3), 260279.Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem-proving.Commun. ACM, 5 (7), 394397.Davis, M., & Putnam, H. (1960). computing procedure quantification theory. JournalACM (JACM), 7 (3), 201215.de Givry, S., Heras, F., Zytnicki, M., & Larrosa, J. (2005). Existential arc consistency:Getting closer full arc consistency weighted CSPs.. IJCAI05: NineteenthInternational Joint Conference Artificial Intelligence.Dubois, O., & Dequen, G. (2001). backbone-search heuristic efficient solving hard 3SAT formulae. IJCAI05: Seventeenth International Joint Conference ArtificialIntelligence, pp. 248253.Een, N., & Sorensson, N. (2005). MiniSat - SAT solver conflict-clause minimization.SAT05: Eighth International Conference Theory Applications Satisfiability Testing.Hoos, H. H. (2002). adaptive noise mechanism walksat. AAAI02: EighteenthNational Conference Artificial Intelligence, pp. 655660.Kaporis, A. C., Kirousis, L. M., & Lalas, E. G. (2006). probabilistic analysis greedysatisfiability algorithm. Random Structures Algorithms, 28 (4), 444480.Kirkpatrick, S., Jr., C. D. G., & Vecchi, M. P. (1983). Optimization simulated annealing.Science, 220, 671680.Kirousis, L. M., Kranakis, E., Krizanc, D., & Stamatiou, Y. C. (1998). Approximatingunsatisfiability threshold random formulas. Random Structures Algorithms,12 (3), 253269.Kroc, L., Sabharwal, A., & Selman, B. (2007). Survey propagation revisited. UAI07:Twenty-Third Conference Uncertainty Artificial Intelligence.Kroc, L., Sabharwal, A., & Selman, B. (2009). Message-passing local heuristicsdecimation strategies satisfiability. SAC-09. 24th Annual ACM SymposiumApplied Computing.Kschischang, F. R., Frey, B., & Loeliger, H.-A. (2001). Factor graphs sum-productalgorithm. IEEE Transactions Information Theory, 47 (2).Lardeux, F., Saubion, F., & Hao, J.-K. (2005). Three truth values SAT MAXSAT problems. IJCAI05: Nineteenth International Joint Conference ArtificialIntelligence.Larrosa, J., & Heras, F. (2005). Resolution Max-SAT relation local consistency weighted CSPs. IJCAI05: Nineteenth International Joint ConferenceArtificial Intelligence.Levin, L. A. (1973). Universal search problems. Problemy Peredaci Informacii, 9, 115116.264fiRelaxed Survey Propagation Weighted Max-SAT ProblemLi, C. M., & Anbulagan (1997). Heuristics based unit propagation satisfiability problems. IJCAI97: Fifteenth International Joint Conference Artificial Intelligence,pp. 366371.Li, C. M., Manya, F., & Planes, J. (2006). Detecting disjoint inconsistent subformulascomputing lower bounds max-sat. AAAI06: Twenty-First AAAI ConferenceArtificial Intelligence.Maneva, E., Mossel, E., & Wainwright, M. (2004). new look survey propagationgeneralizations. http://arxiv.org/abs/cs.CC/0409012.McAllester, D., Selman, B., & Kautz, H. (1997). Evidence invariants local search.AAAI97: Proceedings Fourteenth National Conference Artificial Intelligence,pp. 321326, Providence, Rhode Island.Mezard, M., Mora, T., & Zecchina, R. (2005). Clustering solutions random satisfiability problem. Physical Review Letters, 94, 197205.Mezard, M., & Parisi, G. (2003). cavity method zero temperature. JournalStatistical Physics, 111.Mezard, M., & Zecchina, R. (2002). random k-satisfiability problem: analyticsolution efficient algorithm. Physical Review E, 66.Minton, S., Philips, A., Johnston, M. D., & Laird, P. (1992). Minimizing conflicts: heuristicrepair method constraint satisfaction scheduling problems. Artificial Intelligence, 58, 161205.Moskewicz, M. W., & Madigan, C. F. (2001). Chaff: Engineering efficient SAT solver.DAC01: Thirty-Ninth Design Automation Conference, pp. 530535.Murphy, K., Weiss, Y., & Jordan, M. (1999). Loopy belief propagation approximateinference: empirical study. UAI99: Fifteenth Annual Conference UncertaintyArtificial Intelligence, pp. 46747, San Francisco, CA. Morgan Kaufmann.Pearl, J. (1988). Probabilistic reasoning intelligent systems: networks plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.Pipatsrisawat, K., & Darwiche, A. (2007). Rsat 2.0: Sat solver description. Tech. rep.,Automated Reasoning Group, Computer Science Department, UCLA.Prestwich, S. D. (2003). Local search sat-encoded colouring problems.. Giunchiglia,E., & Tacchella, A. (Eds.), SAT, Vol. 2919 Lecture Notes Computer Science, pp.105119. Springer.Rabiner, L., & Juang, B. (1993). Fundamentals Speech Recognition. Prentice-Hall.Selman, B., Kautz, H. A., & Cohen, B. (1994). Noise strategies improving local search.AAAI97: Twelfth National Conference Artificial Intelligence, pp. 337343.Selman, B., Levesque, H. J., & Mitchell, D. (1992). new method solving hard satisfiability problems. AAAI92: Tenth National Conference Artificial Intelligence,pp. 440446. AAAI Press.Shang, Y., & Wah, B. W. (1998). discrete lagrangian-based global-searchmethodsolving satisfiability problems. Journal Global Optimization, 12 (1), 6199.265fiChieu & LeeTompkins, D., & Hoos, H. (2004). UBCSAT: implementation experimentation environment SLS algorithms SAT MAX-SAT. SAT04: Seventh InternationalConference Theory Applications Satisfiability Testing.Tseitin, G. S. (1968). complexity derivations propositional calculus. StudiesMathematics Mathematical Logic, Part II, 115125.Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. NIPS00:Advances Neural Information Processing Systems 13, pp. 689695.Yedidia, J., Freeman, W., & Weiss, Y. (2005). Constructing free-energy approximationsgeneralized belief propagation algorithms. IEEE Transactions InformationTheory, 51 (7).Zhang, W. (2001). Phase transitions backbones 3-SAT maximum 3-SAT.Proceedings Seventh International Conference Principles PracticeConstraint Programming.266fiJournal Artificial Intelligence Research 36 (2009) 471-511Submitted 7/09; published 12/09Role Macros Tractable PlanningAnders Jonssonanders.jonsson@upf.eduDept. Information Communication TechnologiesUniversitat Pompeu FabraRoc Boronat, 13808018 Barcelona, SpainAbstractpaper presents several new tractability results planning based macros.describe algorithm optimally solves planning problems class callinverted tree reducible, provably tractable several subclasses class.using macros store partial plans recur frequently solution, algorithmpolynomial time space even exponentially long plans. generalize invertedtree reducible class several ways describe modifications algorithm dealnew classes. Theoretical results validated experiments.1. Introductiongrowing number researchers planning investigating computational complexitysolving different classes planning problems. Apart theoretical valueknowledge, particular interest tractable planning problems provably solvedpolynomial time. Many, most, state-of-the-art planners based heuristicsearch. Acquiring informative heuristic requires quickly solving relaxed versionplanning problem. Known tractable classes planning problems ideal candidatesprojecting original problem onto order generate heuristics.paper introduces class IR inverted tree reducible planning problemspresents algorithm uses macros solve instances class. algorithmprovably complete optimal IR, complexity depends sizedomain transition graph algorithm constructs. worst case, sizegraph exponential number state variables, show algorithm runspolynomial time several subclasses IR, proving plan generation tractableclasses.tractable subclasses IR include planning problems whose optimal solutionexponential length number state variables. reason algorithm solveproblems polynomial time subsequences operators frequently repeatedsolution. Since algorithm stores subsequence macro, never needsgenerate subsequence again. algorithm viewed compilation schemestate variables compiled away. information kept statevariable set macros acting it.extend class IR two ways modify algorithm solves planningproblems new classes. First, define notion relaxed causal graph,contains less edges conventional causal graph, associated class RIRrelaxed inverted tree reducible planning problems. also define notion reversiblec2009AI Access Foundation. rights reserved.fiJonssonvariables, define class AR planning problems acyclic causal graphsreversible variables. show algorithm modified solve planning problemsclass. Finally, combine results IR AR define class AORplanning problems acyclic causal graphs variables reversible.class IR algorithm solving instances class previously appearedconference paper (Jonsson, 2007). present paper includes new tractability resultsclasses IR, AR, AOR. addition, notion relaxed causal graphsresulting class RIR novel.Perhaps important contribution work establishing several noveltractability results planning. particular, present several classes planning problems solved polynomial time. contribution particularly significantsince number known tractable classes planning problems small. addition, new classes solved optimally. novel approach takenpaper use macros store solutions subproblems, making possible representexponentially long plans polynomial time space.related contribution possibility use tractable classes generate heuristics.discuss two possibilities generating heuristics here. One idea project planningproblem onto IR removing pre-conditions effects operators makecausal graph acyclic inverted tree reducible. fact, strategy usedHelmert (2006) compute causal graph heuristic. Next, could duplicateoperator introducing pre-conditions missing variables. resulting problemtractable according Theorem 3.5. Thus, algorithm could solve projected problempolynomial time compute admissible heuristic original problem. Note,however, number resulting actions exponential number missingvariables, strategy tractable number missing variablessmall constant.Another idea perform reduction causal graph acyclicvariable constant number ancestors. Next, could make current problem partclass AOR introducing actions make certain variables reversible. typeschanges effect making problem easier solve, sense solutionrelaxed problem guaranteed longer solution original problem.could solve resulting problem using algorithm AOR. Althoughalgorithm provably optimal, might informative heuristic cases.Another contribution possibility solving planning problems polynomialtime, since fall one classes planning problems studied paper.example, show possible Gripper Logistics domains. Althoughdomains previously known polynomial-time solvable, knownwhether could solved using type macro approach taken here. planningproblem cannot solved directly, parts causal graph might structurerequired algorithms. case, algorithms could used solve partplanning problem. Since one feature algorithms compiling away variablesreplacing macros, reduced problem could fed standard plannerobtain final solution.Perhaps interesting use algorithms possibility reuse macros.part causal graph structure coincides two problems, macros generated472fiThe Role Macros Planningone problem could immediately substituted problem. Since variablescompiled away replaced macros, resulting problem fewer variablesactions. example, Tower Hanoi, macros generated n 1 disc problemcould immediately substituted n disc problem. resulting problem includesmacros n 1 disc problem single variable corresponding n-th disc.rest paper organized follows. Section 2 introduces notation definitions. Section 3 introduces class IR inverted tree reducible planning problems,presents algorithm solving problems class, proves several theoretical properties algorithm. Section 4 introduces several extensions class IR, wellcorresponding algorithms. Section 5 presents experimental results validating theoreticalproperties algorithms. Section 6 relates paper existing work planning,Section 7 concludes discussion.2. NotationLet V set variables, let D(v) finite domain variable v V . definestate function V maps variable v V value s(v) D(v)domain. partial state x function subset Vx V variables mapsvariable v Vx x(v) D(v). Sometimes use notation (v1 = x1 , . . . , vk = xk )denote partial state x defined Vx = {v1 , . . . , vk } x(vi ) = xi vi Vx .define several operations partial states. subset W V variables, x | Wpartial state obtained restricting scope x Vx W . Two partial states xmatch, denote x y, x | Vy = | Vx , i.e., x(v) = y(v) v Vx Vy .Given two partial states x y, let x y, composition x y, partial state zdefined Vz = Vx Vy , z(v) = y(v) v Vy , z(v) = x(v) v Vx Vy .Note composition symmetric since right operand overrides valuesleft operand.planning problem tuple P = hV, init, goal, Ai, V set variables,init initial state, goal partial goal state, set actions. action= hpre(a); post(a)i consists partial state pre(a) called pre-conditionpartial state post(a) called post-condition. Action applicable statepre(a) results new state post(a).causal graph planning problem P directed graph (V, E) variablesnodes. edge (w, v) E w 6= v exists actionw Vpre(a) Vpost(a) v Vpost(a) . causal graph acyclic, actionunary, i.e., |Vpost(a) | = 1. variable v V , define Anc(v) setancestors v causal graph, Desc(v) set descendents v.macro-action, macro short, ordered sequence actions. macrowell-defined, pre-condition action coincide cumulativepost-condition actions precede sequence. Although action sequenceimplicitly induces pre- post-condition, explicitly associate pre- postcondition macro. Since macro functionally action, define hierarchiesmacros action sequence macro includes macros, longcause definitions macros cyclic.473fiJonssonv1v3v1v3v2v4v5v2v5v4Figure 1: acyclic causal graph transitive reduction.Definition 2.1. sequence actions seq = ha1 , . . . , ak well-defined statepre(a1 ) post(a1 ) . . . post(ai1 ) pre(ai ) {1, . . . , k}.sometimes refer post-condition post(seq) action sequence, definedpost(seq) = post(a1 ) . . . post(ak ). Given two sequences seq1 seq2, hseq1, seq2iconcatenation two sequences, post(hseq1, seq2i) post-conditionresulting sequence, defined post(seq1) post(seq2). ready formallydefine macros use paper:Definition 2.2. macro = hpre(m), seq(m), post(m)i consists pre-condition pre(m),sequence seq(m) = ha1 , . . . , ak actions macros, post-conditionpost(m). macro well-defined seq(m) well-defined pre(m) pre(m)post(seq(m)) = post(m).3. Class IRKatz Domshlak (2008a) defined class planning problems whose causal graphsinverted trees, i.e., outdegree variable causal graph less equal1, unique root variable v outdegree 0. section, studyclass planning problems call IR, stands inverted tree reducible:Definition 3.1. planning problem P inverted tree reducible causalgraph P acyclic transitive reduction causal graph inverted tree.transitive reduction (V, E ) graph (V, E) defined set nodesV , E contains minimal set edges transitive closure E equaltransitive closure E. words, transitive reduction retains edgesnecessary maintain connectivity. acyclic graphs, transitive reduction uniquecomputed efficiently. Figure 1 illustrates causal graph transitivereduction planning problem class IR. Here, root variable v5 .graph G = (V, E) variables V planning problem P variablev V , let P a(v) = {w V : (w, v) E} set parents v G, i.e., variablesincoming edges. paper, always refer parents v transitive reduction(V, E ), opposed causal graph itself. example, Figure 1 parents v5causal graph {v1 , . . . , v4 }, P a(v5 ) = {v3 , v4 }. Note transitive reductionpreserves set ancestors Anc(v) descendants Desc(v) variable v.Without loss generality, follows assume goal state definedroot variable v transitive reduction, i.e., v Vgoal . Since outgoingedges v, pre-condition action changes value variable474fiThe Role Macros Planningw 6= v independent v. v/ Vgoal , plan solves P never needs change valuev, eliminate v problem, resulting one several subgraphscausal graph. trivial show transitive reductions resulting subgraphsinverted trees, problem decomposed one several planning problemsIR solved independently.convenience, add dummy variable v V , well dummy action =hgoal; (v = 1)i A. transitive reduction resulting causal graph containsadditional node v additional edge (v, v ), v original root variable (notev becomes root variable new transitive reduction). Since v Vgoalassumption, causal graph contains edge (v, v ), v reachablevariable transitive reduction via v.rest Section 3 organized follows. Subsection 3.1 presents MacroPlanner, algorithm solves planning problems class IR via decompositionsubproblems. subsection includes detailed descriptions subproblems generated solved. Subsection 3.2 shows examples MacroPlanner solves planningproblems IR. Subsection 3.3 proves several theoretical properties MacroPlanner,Subsection 3.4 discusses algorithm improved pruning searchmacros.3.1 Plan Generation IRsection, present plan generation algorithm class IR called MacroPlanner. planning problem P IR, MacroPlanner generates one several planssolve P form macros. algorithm uses divide-and-conquer strategydefine solve several subproblems variable v V problem. storessolutions subproblem macros, incorporates macros actionsets higher-level subproblems.Algorithm 1 MacroPlanner(P )1: G causal graph P2: R transitive reduction G3: v root variable R4: GetMacros(v, init, A, R)5: return , fail =main routine MacroPlanner appears Algorithm 1. MacroPlanner takesplanning problem P IR input constructs causal graph G P , welltransitive reduction R G. identifies root variable v transitivereduction, calls GetMacros(v, init, A, R) solve P . Note set macros,implying MacroPlanner may return multiple solutions P . containsolution P , MacroPlanner returns fail. Also note v refers originalroot variable transitive reduction (excluding dummy variable v ). However,follows still consider v descendant v.475fiJonssonBt2t3Ct4t1Figure 2: Illustration macros generated MacroPlanner.3.1.1 Defining Subproblemssection describe subroutine GetMacros which, variable v V ,defines several subproblems v. Intuitively, two reasons changevalues v ancestors: satisfy pre-condition action A, reachgoal state. idea exhaustively define subproblems v may neededachieve this.Let Pv = hVv , init , goal , Oi subproblem associated variable v V . setvariables Vv = {v} Anc(v) consists v ancestors causal graph. setactions contains action changes value v macro representingsolution subproblem parent w P a(v) v. Since Vv fixed, twosubproblems v differ initial goal states init goal .define subproblems may needed change values v ancestors, project onto Vv pre-condition action changes valuedescendant v. Let Z = {pre(a) | Vv : Vpost(a) Desc(v)} {()} setprojected pre-conditions. exclude empty partial state () Z, implying|Vz | > 0 z Z. Note Z includes projected goal state goal | Vv since goalpre-condition dummy action changes value v , descendant v.projected pre-condition z Z, GetMacros defines subproblem Pvinitial state init = init | Vv goal state goal = z. GetMacros calls subroutineSolve generate one several solutions Pv , form sequence seq =ha1 , . . . , ak actions macros O. Let = init post(seq) state resultsapplying seq init . Solve returns solution Pv form macro= hinit , seq, si, init fully specified states Vv . macroz Z, GetMacros defines new subproblem Pv initial state init = post(m)goal state goal = z. process continues new subproblems defined.subroutine Solve may generate multiple macros satisfying projectedpre-condition. intuitive understanding macros generated providedLemma B.3. lemma states state u reachable initprojected pre-condition z Z u z, Solve generates macro hinit , seq, tiz shortest path init u prefix seq. words,Solve generates shortest possible macros satisfying z. However, mightgenerate multiple macros ensure completeness optimality.Figure 2 illustrates macros MacroPlanner generates. Let currentstate, let z projected pre-condition, let A, B, C three sets statessatisfy z. t1 shortest path state A, MacroPlanner generatesmacro t1 represents shortest path (the holds t4 C).476fiThe Role Macros Planningstate B shortest path states B, algorithm generatesseveral macros states B, case t2 t3 , either t2 t3shortest path state B.Algorithm 2 GetMacros(v, init, A, G)1:2: L list containing projected initial state init | Vv3: Z {pre(a) | Vv : Vpost(a) Desc(v)} {()}4: {a : v Vpost(a) }5: w P a(v)6:GetMacros(w, init, A, G)7: end8: elements L9:next element L10:Solve(v, s, Z, O, G)11:= hpre(m), seq(m), post(m)i post(m)/ L12:append post(m) L13:end14: end15: returnAlgorithm 2 gives pseudo-code GetMacros. input GetMacros variablev V , initial state init, action set A, graph G = (V, E) variables V .obtain action set O, GetMacros recursively calls parent w P a(v)v. list L initialized projected initial state init | Vv . subroutineSolve, described following section, simultaneously solves subproblemsv initial state returns corresponding macros. distinct postcondition post(m) macro returned Solve, GetMacros adds post(m) listL. Consequently, Solve later called initial state post(m). Finally, GetMacrosreturns macros generated Solve.3.1.2 Solving Subproblemssection describe subroutine Solve, solves subproblems definedGetMacros variable v V . idea construct graph statesVv nodes. graph seen joint domain transition graph variablesVv . initial state projected pre-condition z Z, Solve computesshortest path states match z using straightforward application Dijkstrasalgorithm. However, number states exponential cardinality Vv , Solveconstructs graph implicitly, adding nodes needed.state p graph, Solve generates two types successor states. Successorstates first type change value v. find states, Solvefinds action pre(a) (v = p(v)), i.e., pre-conditionsatisfied respect current value v. parent w P a(v)(p | Vw ) 6 (pre(a) | Vw ), Solve looks macros p | Vw states satisfypre(a) | Vw . sequence seq represents combination macros, Solve477fiJonssonadds successor state p post(hseq, ai) p graph. cost correspondingedge length sequence hseq, ai.Successor states second type match projected pre-condition z Z.find states, Solve finds z Z z (v = p(v)). parentw P a(v) (p | Vw ) 6 (z | Vw ), Solve looks macros p | Vw statesmatch z | Vw . sequence seq represents combination macros,Solve generates successor state p post(seq) p. However, successor stateadded graph. Instead, used generate solution correspondingsubproblem defining macro initial state state p post(seq).Algorithm 3 Solve(v, s, Z, O, G)1:2: Q priority queue containing pair (s, hi)3: Q non-empty4:(p, seq) remove highest priority state-sequence pair Q5:pre(a) (v = p(v))6:Compose(v, p, pre(a), O, G)7:action sequences seq28:insert (p post(hseq2, ai), hseq, seq2, ai) Q9:end10:end11:z Z z (v = p(v))12:Compose(v, p, z, O, G)13:action sequences seq214:{hs, hseq, seq2i, p post(seq2)i}15:end16:end17: end18: returnAlgorithm 3 gives pseudo-code Solve, takes input variable v V ,initial state s, set projected pre-conditions Z, action set O, graph G = (V, E)V . priority queue Q contains state-sequence pairs, elements ordered accordingtotal length associated sequence. Successor states first type generatedlines 510, successor states second type generated lines 1116. Insertingstate-sequence pair (p, seq) Q (line 8) succeeds seq shortest sequencep far, replacing previous state-sequence pair involving p. Likewise, inserting macrohs, seq, pi (line 14) succeeds seq shortest sequence p far, replacingprevious macro p.Algorithm 4 gives pseudo-code subroutine Compose called Solve. inputCompose variable v V , state s, partial state x, action set O, graphG = (V, E) V . Compose generates sequences states match xcomposed using macros parents v. use Cartesian productdenote set sequences obtained appending macro sequence S.478fiThe Role Macros PlanningAlgorithm 4 Compose(v, s, x, O, G)1: {hi}2: w P a(v)3:(s | Vw ) 6 (x | Vw )4:{hs | Vw , seq(m), post(m)i : post(m) (x | Vw )}5:=6:return7:end8:ST9:end10: end11: returnv1v2v3v4v5v1v2v3v4v5Figure 3: causal graph transitive reduction P5 .matches x, Compose returns set containing empty sequence hi. If, parentw P a(v), macro state matching x, Compose returns empty set.3.2 Examplesillustrate MacroPlanner well-known Tower Hanoi problem. instanceTower Hanoi n discs 3 pegs represented planning problemPn = hV, init, goal, Ai, V = {v1 , . . . , vn } variables representing discs (v1smallest vn largest), domain D(vi ) = {A, B, C}. initialstate init = (v1 = A, . . . , vn = A) goal state goal = (v1 = C, . . . , vn = C).vi V permutation (j, k, l) (A, B, C), action moving discvi peg j peg k, formally defined aj,k= h(v1 = l, . . . , vi1 = l, vi = j); (vi = k)i.discs smaller vi thus third peg l perform movement.Figure 3 shows causal graph transitive reduction planning problemP5 , representing 5-disc instance Tower Hanoi. action moving disc vipre-condition variable vj , j < i. Clearly, causal graph acyclictransitive reduction inverted tree, implying P5 IR. Since transitive reductiondirected path, set parents variable vi , > 1, P a(vi ) = {vi1 }. rootvariable v5 or, generally, vn planning problem Pn .solve Tower Hanoi, MacroPlanner(Pn ) calls GetMacros(vn , init, A, G)generate set solution macros. turn, subroutine GetMacros(vn , init, A, G)calls GetMacros(vn1 , init, A, G). recursion continues base case reachedGetMacros(v1 , init, A, G). variable vi V , let xdi = (v1 = d, . . . , vi = d),{A, B, C}, partial state assigning value variable Vvi .479fiJonssonB,...,B,BA,...,A,BA,...,A,CC,...,C,BC,...,C,CB,...,B,CC,...,C,AB,...,B,AA,...,A,AFigure 4: implicit graph traversed Solve variable vi .BCset projected pre-conditions vi Z = {xA, xi , xi }. Since projected precondition specifies value ancestor vi , macro end eitherthree partial states. easy show induction set macros returnedGetMacros(vi , init, A, G) contains precisely 9 macros, corresponding pairprojected pre-conditions (including macros empty action sequence projectedpre-condition itself).Let us study behavior GetMacros(vi , init, A, G). Here, assume 9macros vi1 set described above. first partial state added Lline 2 projected initial state xA. result, GetMacros calls Solve(vi , xi , Z, O, G)line 10. call Solve, first state-sequence pair added Q line 2 (xA, hi).A,BCaction ai pre-condition xi1 (vi = A) matches (vi = xi (vi )) = (vi = A).Cresult, Solve calls Compose(vi , xA, xi1 (xi = A), O, G) line 6.Compose, single parent vi , namely vi1 . projection xA| Vvi1 = xi1CCequal xi1 . single macro set xi1 xi1 , setreturned Compose contains single sequence hmi. result, Solve generatessuccessor state xCi1 (vi = B) first type, added Q line 8 togetheri.associated sequence hm, aA,B(viteration xC= B), single projected pre-conditioni1BBmatches (vi = B), namely xi , Solve calls Compose(vi , xCi1 (vi = B), xi , O, G)BCline 12. parent vi1 , projection xi1 match xi | Vvi1 = xBi1 .B , set returned Composesingle macro set xCxi1i1contains single sequence hm i. result, Solve generates successor state xBA,Bi, xB added . eight,,hm,second type line 14, macro hxAmacros added similar way. next section prove MacroPlannerguaranteed generate solution Tower Hanoi polynomial time.Figure 4 shows implicit graph traversed Solve variable vi , > 1, TowerBHanoi. path corresponding macro xAxi example markedbold. Successor states first type, well incoming edges, denotedsolid lines. Successor states second type, well incoming edges, denoteddashed lines. Note size graph constant depend i.480fiThe Role Macros Planningdifference cost edge (omitted figure), equals lengthcorresponding action sequence Solve depends length macrosvi1 .second example planning domain first suggested Jonsson Backstrom(1998b). Again, use Pn denote instance containing n variables v1 , . . . , vn .variable vi V domain D(vi ) = {0, 1}. initial state (v1 = 0, . . . , vn = 0)goal state (v1 = 0, . . . , vn1 = 0, vn = 1). variable vi , two actions:ai = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 0); (vi = 1)i,ai = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 1); (vi = 0)i.causal graph transitive reduction identical Tower Hanoi, Figure 3applies well, follows Pn IR.Jonsson Backstrom (1998b) showed optimal plan Pn length 2n 1.solve Pn need change value vn 0 1 using action . Sincepre-condition (vn1 = 1), goal state (vn1 = 0), need insert an1an1 , resulting sequence han1 , , an1 i. Actions an1an1 specify pre-condition (vn2 = 1), goal state specify (vn2 = 0),requiring value vn2 change four times. easy show value variablevi change 2ni times, minimum 2n 1 total actions.variable vi V , two projected pre-conditions set Z, namely(v1 = 0, . . . , vi = 0) (v1 = 0, . . . , vi1 = 0, vi = 1). Since specify valuesancestor vi , set macros returned GetMacros contains precisely fourmacros, one pair projected pre-conditions. spite fact optimalsolution exponential length number variables, MacroPlanner guaranteedgenerate solution Pn polynomial time due complexity results nextsection.3.3 Theoretical Propertiessection prove several theoretical properties algorithm MacroPlanner.ease presentation proofs several theorems moved appendix.first two theorems related correctness optimality MacroPlanner.Theorem 3.2. planning problem P IR, macro returnedMacroPlanner(P ) well-defined solves P .proof Theorem 3.2 appears Appendix A. proof induction variablesv V show macro returned Compose Solve well-defined.addition, macro returned Solve satisfies projected pre-condition. rootvariable, Solve called initial state init, projected pre-conditiongoal. macros returned MacroPlanner(P ) precisely returnedcall Solve, thus solve P .Theorem 3.3. planning problem P IR, exists plan solving P ,MacroPlanner(P ) returns optimal plan P , else returns fail.481fiJonssonproof Theorem 3.3 appears Appendix B. proof double inductionvariables v V states Vv visited call Solve, show macrosreturned Solve represent shortest solutions subproblems correspondingcall. Thus macro returned Solve root variable optimalsolution achieving goal starting init, precisely definition optimalplan P .also prove two complexity results MacroPlanner. Specifically, study twosubclasses class IR prove plan generation polynomial classes,provided allowed represent resulting plan using hierarchy macros.variable v V value D(v), define Adv = {a : post(a)(v) = d}set actions change value v (implying v Vpost(a) ). first provelemma relates complexity MacroPlanner number states visitedcalls Solve.Lemma 3.4. Let gv number states visited various calls SolvePGetMacros(v, init, A, G). complexity MacroPlanner O(|A| vV gv3 ).Proof. Solve basically modified version Dijkstras algorithm, quadraticnumber nodes underlying graph, O(gv2 ). state p dequeuedpriority queue Q once. action O, call Compose line 6 returnsset distinct action sequences, generates one edge p successor statesfirst type. However, two different actions may generate two edges pstate. true projected pre-conditions Z successor statessecond type. Since |A| actions |A| projectedpre-conditions Z, worst-case complexity single call Solve O(|A|gv2 ).subroutine GetMacros calls Solve successor state second type.Since gv states, complexity GetMacros O(|A|gv3 ),Poverall complexity MacroPlanner O(|A| vV gv3 ). Note complexityCompose included analysis. Compose called Solve generate successorstates either type, number gv states visited Solve proportionaltotal number sequences returned Compose.Theorem 3.5. Vpre(a) = Vv variable v V {v } actionVpost(a) = {v}, complexity MacroPlanner O(|V ||A|4 ).Theorem 3.6. Assume variable v V , value D(v), pairactions a, Adv , Vpre(a) = Vpre(a ) = {v} P a(v) pre(a) | P a(v) = pre(a ) | P a(v).pre(a) | P a(v) = init | P a(v) action post(a)(v) = init(v), algorithmPruns polynomial time complexity O(|A| vV |D(v)|3 ).proofs Theorems 3.5 3.6 appear Appendix C. proof Theorem 3.5based observation pre-conditions fully specified ancestorsvariable v V , number states visited various calls Solve boundednumber actions. proof Theorem 3.6 follows fact algorithmalways achieves value D(v) domain v V state, numberstates linear size variable domains.482fiThe Role Macros PlanningTwo examples planning problems properties established Theorem 3.5Tower Hanoi domain suggested Jonsson Backstrom (1998b),previous example section.example planning problem properties established Theorem 3.6domain proposed Domshlak Dinitz (2001). set variables V = {v1 , . . . , vn },domain D(vi ) = {0, 1, 2}, initial state init = (v1 = 0, . . . , vn = 0),goal state goal = (v1 = 2, . . . , vn = 2). variable vi V , four actions:a1i = h(vi1 = 2, vi = 0); (vi = 1)i,a2i = h(vi1 = 0, vi = 1); (vi = 2)i,a3i = h(vi1 = 2, vi = 2); (vi = 1)i,a4i = h(vi1 = 0, vi = 1); (vi = 0)i,v1 , pre-condition vi1 dropped. Domshlak Dinitz (2001) showedlength optimal plan solving planning problem 2n+1 2. However, dueTheorem 3.6, MacroPlanner generates optimal solution polynomial time.3.4 Pruningimprove running time, possible prune states visited.Specifically, let (p, seq) state-sequence pair visited call Solve(v, s, Z, O, G).state-sequence pair (t, seq2) previously visited callt(v) = p(v), attempt reach p using Compose(v, t, p, O, G). Since p fullyspecified state, Compose(v, t, p, O, G) returns one action sequence. Call seq3.p reachable hseq2, seq3i least short seq, needvisit p, since state reachable via p also reachable via path equalshorter length. reasoning holds macros state p matches projectedpre-condition z, additional restriction also match z.possible reach projected goal state goal | Vv s, removemacros whose pre-condition equals set macros GetMacros(v, init, A, G),since plan solves original planning problem P could contain macros.addition, remove macros whose post-condition equals set .possible reach projected goal state projected initial state init | Vv ,exists plan solves original planning problem P , immediately returnfail without generating remaining set macros variables.4. Extending MacroPlannersection, study ways extend algorithm MacroPlanner broader classplanning problems. Subsection 4.1, propose relaxed definition causal graphthat, applied, extends class IR. show MacroPlanner modifiedsolve planning problems new class. Subsection 4.2, extend MacroPlannerplanning problems acyclic causal graphs. Subsection 4.3, show combineresults different classes form general class planning problems. Subsection4.4 illustrates algorithms new classes two well-known examples: GripperLogistics.483fiJonsson4.1 Relaxed Causal Graphconventional definition causal graph (V, E) states edge (w, v) Ew 6= v exists action w Vpre(a) Vpost(a)v Vpost(a) . Consider action Vpost(a) = {w, v}. applied, changesvalues w v. conventional definition, induces cycle sincecausal graph includes two edges (w, v) (v, w).section, propose relaxed definition causal graph. newdefinition, acyclic causal graph necessarily imply actions unary.Assume exists least one action Vpost(a) = {w, v}.assume multiple actions changing value w, whose pre- postconditions specify values v. Finally, assume exists actionchanging value v change value w.given assumption, way satisfy pre-condition actionchanges value v first change value w. changed factaction might also change value w. opposite true actionschange value w, since value w change independently v.relaxed definition, causal graph includes edge (w, v) excludes edge (v, w).4.1.1 Definition Class RIRDefinition 4.1. relaxed causal graph (V, E ) planning problem P directedgraph variables V nodes. edge (w, v) E w 6= veither1. exists w Vpre(a) Vpost(a) v Vpost(a) ,2. exists w, v Vpost(a) either(a) exists w Vpost(a ) v/ Vpost(a ) ,(b) exist w/ Vpost(a ) v Vpost(a ) .Edges type 2(b) ensure always least one edge two variablesw, v V appear post-condition action A. possiblechange value either without changing value other, relaxed causalgraph contains cycle before. define class RIR relaxed invertedtree reducible planning problems:Definition 4.2. planning problem P relaxed inverted tree reduciblerelaxed causal graph acyclic transitive reduction relaxed causal graphinverted tree.Consider planning problem P IR. Since action unary, relaxed causalgraph contains edges type 2. Consequently, relaxed causal graph identicalconventional causal graph, tree-reducible planning problem also relaxedtree-reducible, implying IR RIR.example, consider planning problem two variables v w. LetD(v) = {0, 1, 2, 3}, D(w) = {0, 1}, init = (v = 0, w = 0), goal = (v = 3, w = 1).Assume three actions:484fiThe Role Macros Planningh(v = 0); (v = 1)i,h(v = 2); (v = 3)i,h(v = 1, w = 0); (v = 2, w = 1)i.third action unary, since changes value v w. actionschanging value v affect w, opposite true. Hencerelaxed causal graph contains single edge (v, w).4.1.2 Algorithmshow minor modification MacroPlanner sufficient solve planningproblems class RIR. Since actions longer unary, actionchanges value variable v V may also change values variables.generating macros v, consider set actions change value vwithout changing value descendant v relaxed causal graph. Formally,= {a : v Vpost(a) |Vpost(a) Desc(v)| = 0}.before, project pre-conditions actions change valuedescendant v onto Vv order define subproblems Pv v. Let = {a :|Vpost(a) Desc(v)| > 0 (pre(a) | Vv ) 6= ()} set actions whose projectedpre-condition onto Vv non-empty. Let action, let macrov achieves pre-condition a, i.e., post(m) pre(a).reason applying (unless also satisfies projected pre-conditionaction) satisfy pre-condition a. changes value v,longer projected state post(m) result applying a. words,generate macros v starting post(m) before. Rather, generatemacros post(m) (post(a) | Vv ), i.e., projected state results applyingpost(m).Algorithm 5 RelaxedPlanner(P )1: G relaxed causal graph P2: R transitive reduction G3: v root variable R4: RelaxedMacros(v, init, A, R)5: return , fail =Algorithm 5 shows modified version MacroPlanner class RIR,call RelaxedPlanner. difference respect MacroPlannerG refers relaxed causal graph. Algorithm 6 shows modified versionGetMacros class RIR, call RelaxedMacros. modificationsrespect GetMacros appear lines 35 1217. subroutine Solve remainsbefore.4.1.3 Theoretical PropertiesTheorem 4.3. planning problem P RIR, macro returnedRelaxedPlanner(P ) well-defined solves P .485fiJonssonAlgorithm 6 RelaxedMacros(v, init, A, G)1:2: L list containing projected initial state init | Vv3: {a : |Vpost(a) Desc(v)| > 0 (pre(a) | Vv ) 6= ()}4: Z {pre(a) | Vv : }5: {a : v Vpost(a) |Vpost(a) Desc(v)| = 0}6: u P a(v)7:RelaxedMacros(u, init, A, G)8: end9: elements L10:next element L11:Solve(v, s, Z, O, G)12:= hpre(m), seq(m), post(m)i13:{a } post(m) pre(a)14:post(m) (post(a) | Vv )/ L15:append post(m) (post(a) | Vv ) L16:end17:end18:end19: end20: returnv2v1v3Figure 5: Transitive reduction graph outdegree > 1.Theorem 4.4. planning problem P RIR, exists plan solving P ,RelaxedPlanner returns optimal plan P , else returns fail.proofs Theorems 4.3 4.4 simple adaptations corresponding proofsMacroPlanner, appear Appendix D.4.2 Acyclic Causal Graphsection present second extension MacroPlanner, time classplanning problems acyclic causal graphs. words, variables causalgraph may unbounded outdegree, even considering transitive reduction.Unbounded outdegree poses challenge macro compilation approach, illustratedfollowing example.Consider planning problem P V = {v1 , v2 , v3 }, D(v1 ) = {0, 1, 2}, D(v2 ) =D(v3 ) = {0, 1}. initial state (v1 = 0, v2 = 0, v3 = 0), goal state (v2 = 1, v3 = 1),486fiThe Role Macros Planningcontains following actions:a11 = hv1 = 0; v1 = 1i,a21 = hv1 = 0; v1 = 2i,a12 = hv1 = 1, v2 = 0; v2 = 1i,a13 = hv1 = 2, v3 = 0; v3 = 1i.Figure 5 shows causal graph P , identical transitive reduction case. Sincevariable v1 outdegree 2, follows P/ IR (implying P/ RIR since actionsunary). point view variable v2 , possible set v2 1: apply a11change v1 0 1, followed a12 change v2 0 1. point viewv3 , possible set v3 1 applying a21 followed a13 . Nevertheless, validplan solving planning problem.algorithm MacroPlanner solves planning problems decomposingsubproblems variable. seen efficient certain subclassesIR, guarantees optimality. However, variables unbounded outdegreelonger possible define subproblems state variable isolation.run MacroPlanner example planning problem above, generate macrosetting v2 1, macro setting v3 1. might lead us believe Psolution, fact not.4.2.1 Definition Class ARextend MacroPlanner planning problems acyclic causal graphs, imposeadditional restriction planning problems. state variable v V outdegreegreater 1 causal graph, require v reversible.Definition 4.5. state variable v V reversible if, state Vvreachable projected initial state init | Vv , projected initial state init | Vvreachable s.Definition 4.6. planning problem P belongs class AR causal graph Pacyclic variable reversible.4.2.2 Algorithmsection show modify algorithm MacroPlanner solves planning problems AR. first describe algorithm called ReversiblePlannerrequires variables reversible. ReversiblePlanner, appears Algorithm 7,operates directly causal graph planning problem P . Thus, computetransitive reduction causal graph. Another feature subroutine GetMacros omitted. Instead, ReversiblePlanner directly calls ReversibleCompose,equivalent subroutine Compose.Algorithm 8 describes subroutine ReversibleCompose. Compose returnsset sequences reaching partial state x, ReversibleCompose returnspair sequences. Here, U set variables appear post-conditionaction whose pre-condition x wish satisfy. first sequence seq satisfies x487fiJonssonAlgorithm 7 ReversiblePlanner(P )1: G [relaxed] causal graph P2: (seq, seq2) ReversibleCompose(, init, goal, A, G)3: return seqAlgorithm 8 ReversibleCompose(U, s, x, A, G)1: seq hi, seq2 hi2: w Vx topological order3:x(w) 6= s(w)4:ReversibleSolve(w, s, x(w), A, G)5:= fail6:return (fail,fail)7:else8:seq hm, seqi9:w/ U10:ReversibleSolve(w, (w = x(w)), s(w), A, G)11:seq2 hseq2,12:end13:end14:end15: end16: return (seq, seq2)starting state s. fails, ReversibleCompose returns pair sequences(fail,fail). Otherwise, second sequence seq2 returns post-conditionfirst sequence applied s. exception occurs variables U V ,returned values s.ReversibleCompose requires topological sort variables causal graph G,obtained polynomial time acyclic graphs. example topologicalsort v1 , v2 , v3 causal graph Figure 5. values x satisfied reversetopological order way sequence seq constructed, i.e., new macroinserted first. ReversibleCompose calls ReversibleSolve obtain macrochanging value variable w Vx s(w) x(w). succeeds w/U , ReversibleCompose calls ReversibleSolve second time obtain macroresetting value w x(w) s(w). macro appended sequence seq2,resets values variables Vx U topological order.Algorithm 9 describes ReversibleSolve, equivalent subroutine Solve. Unlike Solve, ReversibleSolve generates single macro changing valuev s(v) d. Moreover, ReversibleSolve invoke Dijkstras algorithm; instead, performs breadth-first search values joint domain W . variablew Vv belongs W exists action changing value v one descendants w post-condition. Note actions unary, W = {v},breadth-first search values domain D(v) v.488fiThe Role Macros PlanningAlgorithm 9 ReversibleSolve(v, s, d, A, G)1: W {w Vv : w Vpost(a) Vpost(a) (Desc(v) {v}) 6= }2: {a : v Vpost(a) [Desc(v) Vpost(a) = ] }3: L list containing state-sequence pair (s | W, hi)4: L contains elements5:(p, seq) next element L6:p = (s | W ) (v = d)7:return h(s | Vv , seq, (s | Vv ) (v = d)i8:end9:pre(a) (v = p(v))10:(seq2, seq3) ReversibleCompose(Vpost(a) , p, pre(a), A, G)11:seq2 6= fail 6 (p , seq ) L p = p post(a)(v)12:insert (p post(a)(v), hseq, seq2, a, seq3i) L13:end14:end15: end16: return failpartial state p visited search associated sequence seq used arrivep. p = (s | W ) (v = d), ReversibleSolve returns macro correspondinggiven sequence. Note macro changes value v, leaving valuesvariables unchanged. Otherwise, ReversibleSolve tries actions changingvalue v whose pre-condition compatible (v = p(v)). satisfy pre-conditionaction state p, ReversibleSolve calls ReversibleCompose.ReversibleCompose returns legal pair sequences (seq2, seq3) partial statep post(v)(a) previously visited, ReversibleSolve generates new statesequence pair (ppost(v)(a), hseq, seq2, a, seq3i). Since U = Vpost(a) , seq3 resets variablesvalues p except whose values changed a.Since equivalent GetMacros omitted, ReversibleMacros goal-drivensense ReversibleSolve called whenever needed subroutine ReversibleCompose. use memoization cache results ReversibleComposeReversibleSolve avoid recomputing result once. expressions within square brackets indicate changes necessary algorithmwork planning problems acyclic relaxed causal graphs. Note, however,ReversiblePlanner incomplete planning problems AR non-unary actions.4.2.3 Theoretical PropertiesTheorem 4.7. planning problem P AR, sequence seq returnedReversiblePlanner(P ) different fail, seq well-defined init satisfies (init post(seq)) goal.Theorem 4.8. planning problem P AR unary actions, P solvable,ReversiblePlanner(P ) returns sequence seq solving P , else returns fail.489fiJonssonTheorem 4.9. planning problem P AR, complexity ReversiblePlanner O(Dk |A|(Dk+1 + |V |)), = maxvV D(v) k = maxvV |W |.proofs Theorems 4.7, 4.8, 4.9 appear Appendix E. proofs Theorems4.7 4.8 similar MacroPlanner, show induction variablesv V macros returned ReversibleCompose ReversibleSolvewell-defined. However, instead optimal plans, ReversiblePlanner returns planone exists.proof Theorem 4.9 based fact call ReversibleSolve,state variables Vv W always take initial values. bounds totalnumber states calls ReversibleSolve. Note Theorem 4.9 impliescomplexity ReversiblePlanner O(D|A|(D2 + |V |)) planning problems ARunary actions, since |W | = 1 v V case. general, sizesets W fixed, complexity ReversiblePlanner polynomial. Also noteTower Hanoi belongs AR since variable reversible, planning problemsAR may exponentially long solutions.4.3 Class AORcombine two results classes IR AR. idea useoriginal algorithm exhaustively generate macros variable v, longoutdegree v transitive reduction causal graph less equal 1.Whenever encounter variable v outdegree larger 1, switch algorithmreversible variables previous section. way, handle acyclic causalgraphs even variables reversible. call resulting class AOR.4.3.1 Definition Class AORDefinition 4.10. planning problem P belongs class AOR causal graphP acyclic variable v V outdegree > 1 transitive reductioncausal graph reversible.example planning problem previous section belong classAOR, since variable v1 outdegree 2 reversible. However, assume addfollowing two actions A:a31 = hv1 = 1; v1 = 0i,a41 = hv1 = 2; v1 = 0i.new actions, variable v1 reversible since value v1 reachablevalue. Since v1 state variable outdegree greater 1transitive reduction, P AOR.4.3.2 Algorithmsection, describe AcyclicPlanner, combines ideas MacroPlanner ReversiblePlanner solve planning problems class AOR. AcyclicPlanner appears Algorithm 10. like MacroPlanner, AcyclicPlanner operates490fiThe Role Macros PlanningAlgorithm 10 AcyclicPlanner(P )1: G [relaxed] causal graph P2: R transitive reduction G3: (S, seq, seq2) AcyclicCompose(v , init, goal, A, G, R)4: =5:return fail6: else7:return seq, hseq , seqi seq8: endtransitive reduction R causal graph. However, since ReversiblePlanneroperates causal graph G itself, AcyclicPlanner passes G Rsubroutine AcyclicCompose.Algorithm 11 AcyclicCompose(v, s, x, A, G, R)1: {hi}2: x x projected onto non-reversible variables3: w P a(v)4:outdegree(R, w) 1 (s | Vw ) 6 (x | Vw )5:AcyclicSolve(w, | Vw , A, G, R)6:{hs | Vw , seq(m), post(m)i : post(m) (x | Vw )}7:=8:return (,fail,fail)9:end10:ST11:end12: end13: seq hi, seq2 hi14: w Vx reverse topological order15:outdegree(R, w) > 1 (s | Vw ) 6 (x | Vw )16:(seq3, seq4) ReversibleCompose(, | Vw , x, A, G).17:seq3 = fail18:return (,fail,fail)19:else20:seq hseq, seq3i21:seq2 hseq4, seq2i22:end23:end24: end25: return (S, seq, seq2)seen, AcyclicPlanner calls AcyclicCompose dummy variableNote that, contrary before, transitive reduction causal graph may containseveral edges v . reason single root variable causal graph.v.491fiJonssonInstead, may multiple sink variables causal graph, i.e., variablesoutgoing edges. transitive reduction contains edge sink variable Vgoalv .Algorithm 11 describes subroutine AcyclicCompose. Although looks somewhatcomplex, lines 3 12 really adaptation Compose class AOR.difference AcyclicSolve generate macros satisfyingprojected pre-condition x; instead, generates macros satisfying projection x xonto non-reversible variables. idea reversible variables always remaininitial values. way, treat parents w P a(v) v independent,since common ancestors outdegree larger 1 transitive reduction,thus reversible definition.Lines 13 24 effect satisfying values x variables Vx whoseoutdegree transtitive reduction larger 1. done simply callingsubroutine ReversibleCompose previous section. AcyclicCompose returnsthree values: set sequences satisfying partial state x respect nonreversible variables, sequence seq satisfying partial state x respect reversiblevariables, sequence seq2 resetting reversible variables initial values.Note ReversibleCompose called variable w, predecessor wprocessed using ReversibleSolve ReversibleCompose well. work,predecessors w reversible, guaranteed following lemma:Lemma 4.11. v reversible, every predecessor v.Proof. contrapositive. Assume w non-reversible predecessor v.exists state Vw reachable projected initial state init | Vwinit | Vw reachable s. Since w predecessor v, Vw subset Vv ,state (init | Vv ) reachable init | Vv , vice versa. Thus vreversible.Algorithm 12 describes subroutine AcyclicSolve. differenceSolve AcyclicSolve AcyclicSolve calls AcyclicCompose obtain sequences satisfying projected pre-condition. Moreover, whenever satisfying precondition pre(a) action, AcyclicCompose applies sequence seq3 reset reversible variables initial values. addition, AcyclicCompose satisfyprojected pre-condition z reversible variables, since append sequenceseq2 resulting sequence. before, expressions within square bracketsneed added deal acyclic relaxed causal graphs.4.3.3 Theoretical PropertiesTheorem 4.12. planning problem P AOR unary actions, existsplan solving P , sequence seq set returned AcyclicPlanner well-definedinit satisfies (init post(seq)) goal.Proof. prove Theorem 4.12, sufficient combine results MacroPlannerReversiblePlanner. subroutines AcyclicCompose AcyclicSolve identical Compose Solve variables outdegree less equal 1, Lemmas492fiThe Role Macros PlanningAlgorithm 12 AcyclicSolve(v, s, A, G, R)1:2: {a : v Vpost(a) [Desc(v) Vpost(a) = ] }3: Z {pre(a) | Vv : |Vpost(a) Desc(v)| > 0} {()}4: Q priority queue containing state-sequence pair (s, hi)5: Q non-empty6:(p, seq) remove highest priority state-sequence pair Q7:pre(a) (v = p(v))8:(S, seq2, seq3) AcyclicCompose(v, p, pre(a), A, G, R)9:sequences seq410:insert (p post(hseq4, seq2, a, seq3i), hseq, seq4, seq2, a, seq3i) Q11:end12:end13:z Z z (v = p(v))14:(S, seq2, seq3) AcyclicCompose(v, p, z, P, G, R)15:sequences seq416:{hs, hseq, seq4i, p post(seq4)i}17:end18:end19: end20: returnA.1 A.2 imply resulting sequences macros desired properties.variables outdegree larger 1, ReversibleCompose returns sequencesdesired properties due Lemma E.1.Theorem 4.13. Assume exists constant k non-reversiblevariable v V , number non-reversible ancestors v less equal k.addition, reversible variable v V , |W | k, W set definedReversibleSolve. AcyclicPlanner runs polynomial time.Proof. Again, combine results MacroPlanner ReversiblePlanner.First note call ReversibleCompose polynomial due Theorem 4.9. LemmaP3.4 states complexity MacroPlanner O(|A| vV gv3 ), gv number states visited calls Solve v. Since AcyclicCompose AcyclicSolve identical Compose Solve non-reversible variables, argumentproof Lemma 3.4 holds AcyclicSolve well. fact v knon-reversible predecessors implies gv = O(Dk+1 ), since reversible predecessors always takeinitial values.Note argument Lemma 3.4 take account external callAcyclicCompose AcyclicPlanner, may exponential number|P a(v )| parents v . slight modification algorithm necessary provetheorem. modify external call returns single sequence.words, w P a(v ), keep one macros returned callAcyclicSolve w. Note alter admissibility solution.493fiJonsson(a)(b)(c)v1vlvhv1vlvhv1vlv2v2vhv*v2Figure 6: (a) Conventional (b) relaxed causal graph, (c) transitive reductionrelaxed causal graph Gripper.4.4 Examplessection, provide examples planning problems AR AOR, describeReversiblePlanner AcyclicPlanner solve them.4.4.1 Gripperfirst example well-known Gripper domain, robot transport ballstwo rooms. Gripper, planning problem Pn defined number n ballsrobot transport. set variables V = {vl , vh , v1 , . . . , vn } domainsD(vl ) = {0, 1}, D(vh ) = {0, 1, 2}, D(vi ) = {0, 1, R} 1 n. initial stateinit = (vl = 0, vh = 0, v1 = 0 . . . , vn = 0) goal state goal = (v1 = 1, . . . , vn = 1).two rooms denoted 0 1. Variable vl represents location robot, i.e.,either two rooms. Variable vh represents number balls currently heldrobot, maximum 2. Finally, variable vi , 1 n, represents location balli, i.e., either two rooms held robot (R). set contains followingactions:h(vl = a); (vl = 1 a)i,h(vl = a, vh = b, vi = a); (vh = b + 1, vi = R)i,h(vl = a, vh = b + 1, vi = R); (vh = b, vi = a)i,{0, 1},a, b {0, 1}, 1 n,a, b {0, 1}, 1 n.Actions first type move robot rooms. Actions second type causerobot pick ball current location, incrementing number balls held.Actions third type cause robot drop ball current location, decrementingnumber balls held.1 n, exist actions changing value vi also changevalue vh . Consequently, conventional causal graph contains edges (vh , vi )(vi , vh ), inducing cycles. However, actions changing value vialso change value vh . opposite true; actions vhaffect value vi (assuming n > 1). Thus, relaxed causal graph containsedge (vh , vi ), (vi , vh ). Figure 6 shows conventional relaxed causal graphs,well transitive reduction relaxed causal graph, Gripper n = 2.transitive reduction includes dummy variable v well incoming edges.494fiThe Role Macros Planningrelaxed causal graph acyclic, variable reversible since return initial state state. Thus Gripper belongs class ARconsidering relaxed causal graph. Since actions Gripper non-unary, ReversiblePlanner guaranteed find solution. However, showfact find solution Gripper.ReversiblePlanner calls ReversibleCompose initial goal states.turn, ReversibleCompose goes variable w Vgoal calls ReversibleSolve obtain macro setting value w goal value. Sincedirected paths variables representing balls, topological sort orders variables arbitrarily. Without loss generality, assume topological order v1 , . . . , vn .Thus first variable processed ReversibleCompose v1 . ReversibleComposecalls ReversibleSolve v = v1 , = init = 1.v1 , set W = {vh , v1 } contains variables vh v1 . set containsactions second third type picking dropping ball 1. list L initiallycontains partial state (vh = 0, v1 = 0). two actions whose pre-conditionsmatch (v1 = 0): one picking ball 1 room 0 assuming ball currently held(vh = 0), one picking ball 1 assuming one ball currently held (vh = 1).these, ReversibleSolve calls ReversibleCompose U = Vpost(a) , = init,x = pre(a) obtain sequence satisfying pre-condition action.pre-condition first action already satisfied init, ReversibleComposereturns pair empty sequences. result, ReversibleSolve adds partial state(vh = 1, v1 = R) L, corresponding post-condition action. Noteset empty variable vh , since actions changing value vhalso change value descendants. Thus possible satisfypre-condition (vh = 1) second action starting initial state without changingvalue v1 . Consequently, ReversibleCompose returns (fail,fail), newpartial state added L.(vh = 1, v1 = R), four actions whose pre-conditions match (v1 = R):two dropping ball 1 room 0, two dropping ball 1 room 1. Twopre-condition (vh = 2) impossible satisfy without changing valuev1 . Dropping ball 1 room 0 returns initial state, new partial statesadded L. Dropping ball 1 room 1 requires pre-condition (vl = 1, vh = 1). Since(vh = 1) already holds, ReversibleCompose calls ReversibleSolve change valuevl 0 1. possible using action changing location robot.ReversibleCompose also returns sequence returning robot room 0.result, ReversibleSolve adds partial state (vh = 0, v1 = 1) L, corresponding post-condition action. Note partial state satisfies(vh = 0, v1 = 1) = (vh = 0, v1 = 0) (v1 = 1), ReversibleSolve returns macrochanges value v1 without changing values variables. Since ReversibleSolve successfully returned macro, ReversibleCompose calls ReversibleSolve obtain macro returning ball 1 initial location. continuesballs. final solution contains macros changing location ballroom 0 room 1 one time.Note complexity ReversibleSolve polynomial Gripper due Theorem 4.9, since |W | 2 vi , 1 n. Also note solution495fiJonssont1tnp1v*u1pmuqFigure 7: Causal graph transitive reduction Logistics.optimal, since robot could carry two balls once. could also solve Gripper usingAcyclicPlanner, would treat variables vi , 1 n, non-reversible sinceoutdegree 0 transitive reduction relaxed causal graph. solutionfact case, since single way move ball room 0room 1.4.4.2 Logisticssecond example Logistics domain, number packagesmoved final location using trucks airplanes. Let denote numberpackages, n number trucks, q number airplanes. multi-valued variableformulation planning problem Logistics given V = {p1 , . . . , pm } ,= {t1 , . . . , tn , u1 , . . . , uq }, pi , 1 m, packages, tj , 1 j n, trucks, uk ,1 k q, airplanes.Let L set possible locations packages. partition L1 . . . LrL , 1 l r, corresponds locations city. alsosubset C L corresponding airports, |C | 1 1 l r.pi , domain D(pi ) = L corresponds possible locations plus inside trucksairplanes. tj , D(tj ) = 1 l r. uk , D(uk ) = C.initial state assignment values variables, goal state assignmentpackages pi among values L. set contains following three types actions:h(t = l1 ); (t = l2 )i,h(t = l, pi = l); (pi = t)i,h(t = l, pi = t); (pi = l)i,, l1 6= l2 D(t),, l D(t), 1 m,, l D(t), 1 m.first type action allows truck airplane move two locationsdomain. second type action loads package truck airplanelocation. third type action unloads package truck airplane.Figure 7 shows causal graph transitive reduction planning problemLogistics. transitive reduction identical causal graph case.Gripper, figure shows dummy variable v well incoming edges.easy see causal graph acyclic. addition, variables reversible496fiThe Role Macros PlanningDiscsTime (ms)MacrosLength102030405060318416127945770127/8257/17287/262117/352147/442177/532> 103> 106> 109> 1012> 1015> 1018Table 1: Results Tower Hanoi.since possible return initial state state. Thus could applyReversiblePlanner solve planning problems Logistics.solve planning problem Logistics, ReversiblePlanner calls ReversibleCompose initial goal states. Since directed pathsvariables representing location packages, topological sort orders variables arbitrarily. Without loss generality, assume topological order p1 , . . . , pn . Thusfirst variable processed ReversibleCompose p1 . ReversibleCompose callsReversibleSolve v = p1 , = init equal goal location package.p1 , set W = {p1 } contains p1 since actions unary. set containsactions picking dropping package. Although may many ways movepackage initial location goal location, ReversibleSolve considersway moving package. moving package, trucks airplanesreturned initial locations. ReversibleCompose also calls ReversibleSolvegenerate macro returning package initial location. final solutionconsists macros moving package final location one time.Note complexity ReversibleSolve polynomial Logistics dueTheorem 4.9, since |W | 1 pi , 1 m. Also note solutionoptimal, since trucks airplanes always returned initial locations. couldalso solve Logistics using AcyclicPlanner, would treat variables pi , 1 m,non-reversible since outdegree 0 transitive reduction relaxed causalgraph. Again, solution since ancestor pi reversible, statesAcyclicSolve always trucks airplanes initial locations.5. Experimental Resultstest algorithm, ran experiments two domains: Tower Hanoi extendedversion Gripper. results experiments Tower Hanoi appear Table1. varied number discs increments 10 recorded running timeMacroPlanner. table also shows number macros generated algorithm,those, many used represent resulting global plan. example,27 82 generated macros formed part solution Tower Hanoi 10 discs.second set experiments, modified version Gripper previous section. Instead two rooms, environment consists maze 967 rooms.transport balls, robot must navigate maze initial location497fiJonssonBallsTime (ms)MacrosLength10010110210315281535157029063141041004300 100300 101300 102300 103Table 2: Results Gripper.goal location. robot pick drop balls initial goal locations.results running ReversiblePlanner corresponding planning problems appear Table 2. Since ReversiblePlanner generates macros necessary,used solution. experiments illustrate benefit using macrosstore solution navigating maze. algorithm use macrosrecompute path maze every time goes pick new ball.6. Related Workease presentation, group related work three broad categories: complexityresults, macro-based planning, factored planning. following subsectionspresents related work one categories.6.1 Complexity ResultsEarly complexity results planning focused establishing hardness different formulations general planning problem. STRIPS formalism used, deciding whethersolution exists undecidable first-order case (Chapman, 1987) PSPACEcomplete propositional case (Bylander, 1994). PDDL, representation language used International Planning Competition, fragment including delete listsEXPSPACE-complete (Erol, Nau, & Subrahmanian, 1995).Recently, several researchers studied tractable subclasses planning problemsprovably solved polynomial time. subclasses based notioncausal graph (Knoblock, 1994), models degree independencyproblem variables. However, Chen Gimenez (2008a) showed connected causalgraph causes problem hard (unless established assumptions P = NP fail),additional restrictions problem necessary.common restriction variables problem binary. JonssonBackstrom (1998a) defined class 3S planning problems acyclic causal graphsbinary variables. addition, variables either static, symmetrically reversible,splitting. authors showed possible determine polynomial time whethersolution exists, although solution plans may exponentially long.Gimenez Jonsson (2008) designed macro-based algorithm solve planningproblems 3S polynomial time. algorithm works generating macros changevalue single variable time, maintaining initial values ancestorsvariable. fact, strategy used algorithm ReversiblePlanner,498fiThe Role Macros Planningthus seen extension algorithm multi-valued reversible variablesgeneral acyclic causal graphs.Brafman Domshlak (2003) designed polynomial-time algorithm solving planning problems binary variables polytree acyclic causal graphs bounded indegree.Gimenez Jonsson (2008) showed causal graph unbounded indegree,problem becomes NP-complete. Restricting pre-condition actiontwo variables causes problem class become tractable (Katz & Domshlak, 2008a). Since hardness proof based reduction planning problemsinverted tree causal graphs, results apply class IR restricting invertedtree reducible planning problems.Another result applies class IR hardness planning problemsdirected path causal graphs (Gimenez & Jonsson, 2008). Gimenez Jonsson (2009)extended result multi-valued variables domains size 5. KatzDomshlak (2008b) showed planning problems whose causal graphs inverted forkstractable long root variable domain fixed size. class alsofragment IR. words, class IR includes several known tractableintractable fragments, although tractability results proven present papernovel.tractability results include work Haslum (2008), defined planningproblems terms graph grammars, showed resulting class tractablecertain restrictions grammar. Chen Gimenez (2007) defined width planningproblems designed algorithm solving planning problems whose complexityexponential width. words, planning problems constant widthtractable.idea using reversible variables related work Williams Nayak(1997), designed polynomial-time algorithm solving planning problemsacyclic causal graphs reversible actions. words, action symmetric counterpart pre-condition except reverses effect former.addition, algorithm requires two actions pre-conditions oneproper subset other. definition reversible variables flexiblerequire actions reversible.Since class IR associated algorithm introduced, two tractabilityresults based macros appeared literature. already mentionedwork Gimenez Jonsson (2008) class 3S. Also, Chen Gimenez (2008b)presented polynomial-time algorithm generates macros within Hamming distancek state, defined associated tractable class planning problems constantHamming width. class also contains Tower Hanoi, relationship IRfully established.6.2 Macro-Based Planningalso worth mentioning relationship macro-based algorithms planning.Macros long used planning, beginning advent STRIPS representation (Fikes & Nilsson, 1971). Minton (1985) Korf (1987) developed ideafurther, latter showing macros exponentially reduce search space chosen499fiJonssoncarefully. (Knoblock, 1994) developed abstraction technique similar macros,problem treated different levels abstractions fully solved. Vidal(2004) extracted macros relaxed plans used generate heuristics.Methods Macro-FF (Botea, Enzenberger, Muller, & Schaeffer, 2005) automatically generate macros experimentally shown useful search, provedcompetitive fourth International Planning Competition. Typically, macros introduced flat sequences two three actions. stands stark contrastalgorithms, macros may hierarchical arbitrarily long sequences.words, two approaches different: one tries augment PDDL slightlylonger sequences hope speeding search, attempts generatemacros needed solve subproblems associated variable.Recent work macros admits longer action sequences (Botea, Muller, & Schaeffer, 2007;Newton, Levine, Fox, & Long, 2007).6.3 Factored PlanningAnother related field work factored planning, attempts decompose planningproblem one several subproblems. Typically, planning problem factoredseveral subdomains, organized tree structure. variable actionproblem belongs one subdomains. Amir Engelhardt (2003) introducedalgorithm called PartPlan solves planning problems tree decomposition already exists. algorithm exponential maximum number actionsvariables subdomain.Brafman Domshlak (2006) introduced algorithm called LID-GF decomposes planning problems based causal graph. LID-GF polynomial planningproblems fixed local depth causal graphs fixed tree-width. local depthvariable defined number times value variable changeplan solving problem. Interestingly, Tower Hanoi, solved polynomialtime MacroPlanner, exponential local depth causal graph unboundedtree-width. Kelareva, Buffet, Huang, Thiebaux (2007) proposed algorithm factored planning automatically chooses order solving subproblems. algorithmrequires subproblem clustering given, algorithms automatically derivessubproblem order.7. Conclusionpaper, introduced class IR inverted tree reducible planning problemspresented algorithm called MacroPlanner uses macros solve planningproblems class. algorithm provably complete optimal, runs polynomial time several subclasses IR.also extended class IR several ways. class RIR allows causal graphrelaxed, associated algorithm called RelaxedPlanner able solve planningproblems RIR optimally. class AR allows general acyclic causal graphs longvariables reversible, associated algorithm called ReversiblePlannersolve planning problems unary actions AR non-optimally polynomial time.non-unary actions, ReversiblePlanner complete, although still solve500fiThe Role Macros Planningproblems Gripper. Finally, class AOR allows acyclic causal graphs longvariable outdegree larger 1 transitive reduction reversible.algorithm AcyclicPlanner combines ideas algorithms solve planningproblems class AOR.validated theoretical properties algorithm two sets experiments.first set, algorithm applied Tower Hanoi. Although optimal solutionexponential, algorithm guaranteed solve problems domain polynomialtime. reason macros obviate need solve subproblem multipletimes. second set experiments, applied algorithm extended versionGripper. Again, results demonstrate power using macros store solutionsubproblems.major contribution paper extending set known tractable classesplanning problems. Katz Domshlak (2008b) suggested projecting planning problemsonto known tractable fragments order compute heuristics. perfectly possibleusing algorithms. classes IR RIR, resulting heuristic admissible since corresponding solution optimal. Even solution exponentially long,computing solution length done polynomial time using dynamic programming.two domains share part causal graph structure, macros generated one domaincould used other. could save significant computational effort since schemecompiles away variables replaces macros. Since macros functionallyequivalent standard actions, used place actions variablesreplace. Macros also prove useful domains backtrack findoptimal solution. macros already generated stored variables,need recompute partial plans variables scratch.multi-valued representation included notion objects, would possiblegenerate macros one object share macros among identical objects, likeparametrized operators standard PDDL planning domain. example,Logistics domain, truck operates city functionally equivalent,macro generated one truck applied another. trueairplanes. Finally, obtain optimal plans classes AR AOR, one wouldtest ways interleave subplans different variables, process likelytractable unless number interleaved subplans constant.Acknowledgmentswork partially funded APIDIS MEC grant TIN2006-15387-C03-03.Appendix A. Proof Theorem 3.2appendix prove Theorem 3.2, states macro returnedMacroPlanner(P ) well-defined solves P . First, prove series lemmasstate sequences macros returned Compose Solve well-defined.Lemma A.1. v V , state Vv , partial state x Vvx (v = s(v)), macro parents v well-defined, action sequenceseq returned Compose(v, s, x, O, G) well-defined (s post(seq)) x.501fiJonssonProof. Compose(v, s, x, O, G) returns sequences type seq = hm1 , . . . , mk i,mi = hs | Vw , seq(mi ), post(mi )i macro parent w P a(v) v (s | Vw ) 6(x | Vw ). Since pre(mi ) = | Vw holds s, mi applicable s. Since transitivereduction causal graph inverted tree, parents v common ancestors,sets variables Vw disjoint. consequence, application m1 , . . . , mi1change values variables Vw , | Vw still holds mi applied.Assuming macro mi well-defined, (s | Vw ) post(seq(mi )) = post(mi ),Compose(v, s, x, O, G) considers macros mi w post(mi ) (x | Vw ). Sincemacro seq changes values variables Vw , holds (s post(seq))(x | Vw ). (s | Vw ) (x | Vw ) parent w P a(v) v, seq contain macrow, (s post(seq)) | Vw = | Vw , implying (s post(seq)) (x | Vw ). Finally, macroseq changes value v, (s post(seq))(v) = s(v). Since x (v = s(v)) definition(s post(seq)) (x | Vw ) w P a(v), holds (s post(seq)) x.Lemma A.2. v V state Vv , macro hpre(m), seq(m), post(m)ireturned call Solve(v, s, Z, O, G) well-defined, post(m) z z Z.Proof. proof double induction v state-sequence pairs (p, seq) visitedcall Solve(v, s, Z, O, G). particular, show seq well-definedp = post(seq). first state-sequence pair removed Q line 4(s, hi). Clearly, hi well-defined = post(hi). Otherwise, assumeseq well-defined p = post(seq). line 8, state-sequence pair(p post(hseq2, ai), hseq, seq2, ai) added Q action pre(a)(v = p(v)) sequence seq2 returned Compose(v, p, pre(a), O, G).induction, macros returned Solve(w, , Z , , G) well-definedw P a(v) state Vw . Lemma A.1 implies sequence seq2returned Compose(v, p, pre(a), O, G) well-defined p (p post(seq2))pre(a). words, applicable p post(seq2), implying hseq2, ai welldefined p. assumption, seq well-defined p = post(seq), implyinghseq, seq2, ai well-defined s. applied s, hseq, seq2, ai results statep post(hseq2, ai), implying p post(hseq2, ai) = hseq, seq2, ai.line 14, macro = hs, hseq, seq2i, p post(seq2)i added projected pre-condition z Z z (v = p(v)) sequence seq2 returnedCompose(v, p, z, O, G). Lemma A.1 implies seq2 well-defined p(p post(seq2)) z. Since seq well-defined p = post(seq), hseq, seq2iwell-defined results state p post(seq2) applied s. followsmacro well-defined post(m) = p post(seq2) z.proceed prove Theorem 3.2. First note MacroPlanner(P ) callsGetMacros(v, init, A, G) obtain set macros , G = R transitivereduction causal graph P v root variable R. Since variableancestor v, follows Vv = V init | Vv = init. projectedpre-condition Z dummy action , equals goal | Vv = goal. ThusGetMacros(v, init, A, G) calls Solve(v, s, Z, O, G) = init Z = {goal}.Lemma A.2 implies macro returned Solve(v, init, {goal}, O, G) welldefined post(m) goal. Since pre-condition macro returned502fiThe Role Macros PlanningSolve(v, init, {goal}, O, G) init well-defined, sequence seq(m) = ha1 , . . . , akwell-defined init and, applied init, results state init post(seq(m)) =post(m) goal satisfies goal state. Thus, solves P .Appendix B. Proof Theorem 3.3appendix prove Theorem 3.3, states MacroPlanner(P ) returnsoptimal solution P IR one exists. First, prove series lemmasstate macros returned Solve represent shortest solutionscorresponding subproblems.Definition B.1. v V pair states (s, t) Vv , let denoteexists sequence actions Av = {a : Vpost(a) Vv } that, appliedstate s, results state t.Lemma B.2. v V , let (p, seq) state-sequence pair visited callSolve(v, s, Z, O, G). w P a(v), GetMacros(w, init, A, G) previouslycalled Solve(w, p | Vw , Z , , G), Z values Z w.Proof. GetMacros(w, init, A, G) calls Solve(w, , Z , , G) state listL. p = init | Vv projected initial state Vv , p | Vw = init | Vw added Lline 2 GetMacros. Otherwise, actions changing values variablesVw macros generated GetMacros(w, init, A, G). projection p | Vw thusequal post-condition macro. distinct post-condition post(m)macro w added L line 12 GetMacros.Lemma B.3. v V , pair states (s, u) Vv u,projected pre-condition z Z u z, Solve(v, s, Z, O, G) returns macrohs, seq, ti z shortest path u prefix seq.Proof. induction v. P a(v) = , set actions = Av , u impliesexistence sequence actions Av u. Since projected pre-conditionz Z non-empty, u z implies u = z. Since state-sequence pairs visited ordershortest sequence length, Solve(v, s, Z, O, G) guaranteed return macro hs, seq, uiseq shortest path u. Clearly, u shortest path u,seq prefix itself.|P a(v)| > 0, use induction state-sequence pairs (p, seq) visitedcall Solve(v, s, Z, O, G) prove lemma. Namely, prove p u,Solve(v, s, Z, O, G) returns macro hs, hseq, seq2i, ti z shortestpath p u prefix seq2. exception rule exists actionsequence shorter hseq, seq2i pass p.base case given p(v) = u(v). w P a(v), (p | Vw ) 6 (z | Vw ) impliesz | Vw Z , u z implies (u | Vw ) (z | Vw ), GetMacros(w, init, A, G) previously calledSolve(w, p | Vw , Z , , G) due Lemma B.2, p u implies (p | Vw ) (u | Vw )removing actions Aw . induction, Solve(w, p | Vw , Z , , G) returned macrohp | Vw , seq , (z | Vw ) shortest path p | Vw u | Vwprefix seq . particular, macro hp | Vw , seq , part set O.503fiJonssoniteration Solve(v, s, Z, O, G) (p, seq), Compose(v, p, z, O, G) calledline 12 since u z p(v) = u(v) imply z (v = p(v)). resulting set containssequence seq2 consists macros hp | Vw , seq , w P a(v)(p | Vw ) 6 (z | Vw ). line 14 macro hs, hseq, seq2i, ti added ,= p post(seq2) z due Lemma A.1. Since seq2 change value v, sincesubsets Vw disjoint, since shortest path p | Vw u | Vwprefix seq w P a(v), shortest path p u prefix seq2.p(v) 6= u(v), path p u include actions change valuev. Let first action shortest path p u, assumeapplied state r, implying p r, r post(a) u, r pre(a).w P a(v), (p | Vw ) 6 (pre(a) | Vw ) implies pre(a) | Vw Z , p r implies (p | Vw )(r | Vw ), r pre(a) implies (r | Vw ) (pre(a) | Vw ), GetMacros(w, init, A, G) calledSolve(w, p | Vw , Z , , G) due Lemma B.2. induction, Solve(w, p | Vw , Z , , G)returned macro hp | Vw , seq , (pre(a) | Vw ) shortest pathp | Vw r | Vw prefix seq . macro hp | Vw , seq , part set O.iteration Solve(v, s, Z, O, G) (p, seq), Compose(v, p, pre(a), O, G) calledline 6 since pre(a) (v = p(v)) due fact first action changingvalue v shortest path p u. resulting set contains sequence seq2consists macros hp | Vw , seq , w P a(v) (p | Vw ) 6(pre(a) | Vw ). line 8 state-sequence pair (q post(a), hseq, seq2, ai) added Q,q = p post(seq2) pre(a) due Lemma A.1.Since seq2 change value v shortest path p | Vwr | Vw prefix seq w P a(v), q shortest path p r prefixseq2. Consequently, exists shortest path hseq2, seq3i p r. pathchange value v, else would applied state r shortest path pu. Since applicable q due q pre(a), sequence hseq2, a, seq3i shortestpath p r post(a). follows q post(a) shortest path pr post(a) prefix hseq2, ai. Thus r post(a) u implies q post(a) u.future iteration Solve(v, s, Z, O, G), state-sequence pair removed Qline 4 (q post(a), hseq, seq2, ai). Since q post(a) u, induction state-sequencepairs, Solve(v, s, Z, O, G) returns macro hs, hseq, seq2, a, seq4i, ti ushortest path q post(a) u prefix seq4. exists shortestpath hseq4, seq5i q post(a) u. Since r post(a) shortest path pu assumption, q post(a) shortest path p r post(a) prefixhseq2, ai, hseq2, a, seq4, seq5i shortest path p u. followsshortest path p u prefix hseq2, a, seq4i.proof follows since first iteration Solve(v, s, Z, O, G) (s, hi), implying Solve(v, s, Z, O, G) returns macro hs, hhi, seq2i, ti = hs, seq2, ti ushortest path u prefix seq2. caseexception since sequence passes s. Note v/ Vz , i.e., zspecify value v, set macros returned Solve(v, s, Z, O, G) containsmacros z reachable value v, including u(v) since u reachable s.proceed prove Theorem 3.3. Recall set macros returnedMacroPlanner(P ) equal set macros returned Solve(v, init, {goal}, O, G),504fiThe Role Macros PlanningG = R transitive reduction causal graph v root variable R.optimal plan solving P sequence actions Av = init state uu goal, implying init u. apply Lemma B.3 prove containsmacro = hinit, seq, ti goal shortest path uprefix seq. possible seq optimal plan.exist plan solving P , show empty contradiction.Assume not. Theorem 3.2 implies macro solution P ,contradicts fact P unsolvable. Thus empty, consequence,MacroPlanner(P ) returns fail.Appendix C. Proof Theorems 3.5 3.6section prove Theorems 3.5 3.6, establish two subclasses classIR MacroPlanner generates solutions polynomial time.prove Theorem 3.5, note variable v V action vVpost(a) , holds Vpre(a) = Vv . follows v Vz projected pre-conditionz Z. Let Zvd = {z Z : z(v) = d} set projected pre-conditions specifyvalue v. state domain transition graph specifies valuev match either pre-condition action Adv projected pre-conditionZvd . Otherwise, corresponding node would added graph algorithm.exception projected initial state init | Vv case init(v) = d.Since pre-condition specified ancestor v, one state matchespre-condition. number projected pre-conditions bounded numberactions descendants v, number nodes domain transition graphPO( dD(v) (|Adv | + |Zvd |)) = O(|A| + |Z|) = O(|A|). Lemma 3.4 followsPcomplexity algorithm O(|A| vV |A|3 ) = O(|V ||A|4 ), proving theorem.Theorem 3.6 states action changing value v D(v)pre-condition parents v. Since pre-condition undefinedancestors, projected pre-condition v specified v alone. impliessuccessor states first second type always coincide, since matching projectedpre-condition depends value v.prove induction domain transition graphs v contain one nodevalue D(v). |P a(v)| = 0, proof follows fact nodesvalues D(v). |P a(v)| > 0, induction domain transition graphs parentv P a(v) v one node per value D(v ). implies macropost(m)(v ) = post-condition (else would least twonodes v = ).value D(v), algorithm generates successor state first types(v) = applying action Adv . action pre-conditionparent v P a(v). implies pre-condition always satisfiedstate, since macro post(m)(v ) = pre(a)(v ) postcondition. Thus, successor state s(v) = same, matter actionuse previous state come from.Note = init(v), projected initial state init | Vv could differentsuccessor state si discussed above. However, theorem states pre-condition505fiJonssonactions post(a)(v) = init(v) P a(v) equals init | P a(v). Since domaintransition graphs parent v P a(v) contain one node init(v ), nodecorrespond projected initial state init | Vv . Thus, successor states(v) = init(v) equals projected initial state init | Vv , since pre(a) | P a(v) = init | P a(v)action Adv , = init(v), post(m) = init | Vv macro satisfiespre-condition a. proof theorem follows Lemma 3.4.Appendix D. Proof Theorems 4.3 4.4appendix prove Theorems 4.3 4.4, together state RelaxedPlanner returns well-defined optimal solution P RIR one exists.first show Definition B.1 Lemma B.2 apply RelaxedPlanner. notionreachability Definition B.1 refers set Av actions Vpost(a) Vv , thusexcluding actions change value successor v.Lemma B.2 states (p, seq) state-sequence pair visited callSolve(v, s, Z, O, G) w P a(v) parent v G, GetMacros(w, init, A, G) previously called Solve(w, p | Vw , Z , , G), Z values Zw. show holds RelaxedMacros well, G transitivereduction relaxed causal graph.p equals projected initial state init | Vv , p | Vw = init | Vw added L line 2RelaxedMacros. Otherwise, p successor state first type. words,reached applying action Av Aw . Solve uses macros w satisfypre-condition respect Vw . Consequently, p | Vw equals post-conditionpost(m) macro m, followed application a. precisely statepost(m) (post(a) | Vw ) added L line 15 modified RelaxedMacros.Since subroutines Solve Compose before, Lemmas A.2B.3 apply verbatim RelaxedPlanner. use reasoningMacroPlanner prove two theorems.Appendix E. Proofs Theorems 4.7, 4.8, 4.9appendix prove Theorem 4.7, states sequence seq returnedReversiblePlanner(P ) well-defined init satisfies (init post(seq)) goal.also prove Theorem 4.8, states ReversiblePlanner complete planning problems AR unary actions. Finally, prove Theorem 4.9 regardingcomplexity ReversiblePlanner. first prove lemmas similar Lemmas A.1A.2.Lemma E.1. state partial state x, let (seq, seq2) pair sequences returned ReversibleCompose(U, s, x, P, G). macros returned ReversibleSolve well-defined, seq well-defined post(seq) = x. Furthermore, seq2 well-defined x post(seq2) = | (Vx U ).Proof. Let w1 , . . . , wk topological sort variables Vx x(wi ) 6= s(wi ).first sequence returned ReversibleCompose(U, s, x, P, G) seq = hm1k , . . . , m11 i,m1i = hs | Vwi , seq , (s | Vwi ) (wi = x(wi ))i macro changing value wi506fiThe Role Macros Plannings(wi ) x(wi ). fact wi comes wj topological order implies wj/ V wi .Thus, changing values wk , . . . , wi+1 , pre-condition | Vwi macro m1istill holds, sequence seq well-defined s. Furthermore, seq changes valuevariable w Vx x(w), leaves values variables unchanged,post(seq) = x.Let u1 , . . . , ul topological sort variables W = {w1 , . . . , wk } U .second sequence returned ReversibleCompose(U, s, x, P, G) seq2 = hm21 , . . . , m2l i,m2i = h(s | Vui ) (ui = x(ui )), seq , | Vui macro resetting value uix(ui ) s(ui ). W contains ancestors ui , come ui topological order.Hence values reset prior ui , pre-condition (s | Vui ) (ui = x(ui ))macro m2i holds application m21 , . . . , m2i1 . Since seq2 resets valuevariable w Vx U s(v), post(seq2) = | (Vx U ).Note ancestor u ui part U , value would reset s(u ),consequence, seq2 would well-defined. However, ReversibleSolve alwayscalls ReversibleCompose U = Vpost(a) x = pre(a) action a. Takentogether, u Vpost(a) ui Vpre(a) Vpost(a) imply causal graph containsedge (ui , u ), true even consider relaxed causal graph. Thus u couldancestor ui acyclic causal graph.Lemma E.2. v V , state s, value D(v), macro returnedReversibleSolve(v, s, d, A, G) well-defined.Proof. induction state-sequence pairs (p, seq) visited call subroutine ReversibleSolve(v, s, d, A, G). particular, show seq well-definedsatisfies post(seq) = p. base case given first state-sequence pair(s | W, hi), whose sequence clearly well-defined satisfies spost(hi) = s(s | W ).Otherwise, assume statement holds (p, seq), let actionpre(a) (v = p(v)).Let (seq2, seq3) result ReversibleCompose(Vpost(a) , p, pre(a), A, G).(seq2, seq3) equal (fail,fail), Lemma E.1 states seq2 well-defined spp post(seq2) = p pre(a). implies hseq, seq2, ai well-definedpost(hseq, seq2, ai) = p pre(a) post(a). Lemma E.1 also statesseq3 well-defined p pre(a) post(seq3) = (s p) | (Vpre(a) Vpost(a) ).Since seq3 changes values variables Vpre(a) Vpost(a) , still well-definedstate p pre(a) post(a) results applying a. Thus hseq, seq2, a, seq3iwell-defined post(hseq, seq2, a, seq3i) = p post(a). easy showcomposition commutative, implying p post(a) = (p post(a)). Thusstatement holds new state-sequence pair (p post(a), hseq, seq2, a, seq3i) insertedL line 12 ReversibleSolve.proof follows induction variables v V . v ancestorscausal graph, call ReversibleCompose(Vpost(a) , p, pre(a), A, G) returnspair empty sequences, since pre(a) (v = p(v)) set Vpre(a) Vpost(a)empty. Thus macro returned ReversibleSolve well-defined. Otherwise,hypothesis induction macros generated ReversibleSolve ancestors vwell-defined. Thus ReversibleCompose(Vpost(a) , p, pre(a), A, G) returns well507fiJonssondefined pair sequences due Lemma E.1 consequently, macros generatedReversibleSolve v also well-defined.proof Theorem 4.7 follows straightforward application Lemma E.1call ReversibleCompose(, init, goal, A, G) made ReversiblePlanner, takingadvantage Lemma E.2 ensure macros returned ReversibleSolvewell-defined.prove Theorem 4.8, first prove lemma regarding completeness ReversibleSolve unary actions.Lemma E.3. v V , state s, value D(v), exists stateu u(v) = u, ReversibleSolve(v, s, d, A, G) returns valid macrodifferent fail.Proof. double induction variables v V state-sequence pairs (p, seq)visited call ReversibleSolve(v, s, p, A, G). particular, showp u, ReversibleSolve(v, s, d, A, G) returns valid macro. base case givenp(v) = d, implying p = (s | W ) (v = d) since W = unary actions. case,ReversibleSolve returns macro line 7. Else path u containactions changing values v. Let first action, implyingstate pre(a).induction, w Vpre(a) , ReversibleSolve(w, s, pre(a)(w), A, G) returnsvalid macro. fact w reversible implies exists state uu (w) = s(w) (w = pre(a)(w)) u . consequence, inductionReversibleSolve(w, s(w = pre(a)(w)), s(w), A, G) also returns valid macro. Thuspair sequences (seq2, seq3) returned ReversibleCompose(Vpost(a) , sp, pre(a), A, G)well-defined different (fail,fail). Consequently, ReversibleSolve insertsstate-sequence pair (p post(a)(v), hseq, seq2, a, seq3i) list L line 12.sequence hseq, seq2, a, seq3i generated ReversibleSolve results state(v = post(a)(v)), may different state given path uapplying action a. However, since v reversible, state reachable(v = post(a)(v)), implying (v = post(a)(v)) u. induction state-sequencepairs, ReversibleSolve(v, s, d, A, G) returns corresponding macro.proof Theorem 4.8 follows fact call subroutineReversibleCompose(, init, goal, A, G), value goal state satisfiedstarting init, Lemma E.3 implies ReversibleSolve guaranteed returnmacro satisfies goal state variable Vgoal . Thus, due Lemma E.1,ReversibleCompose successfully return pair well-defined sequences (seq, seq2)property init post(seq) = init goal, seq solution P . valuegoal state unreachable init, ReversibleSolve return correspondingmacro, ReversibleCompose (and ReversiblePlanner) return fail.prove Theorem 4.9, first prove lemma regarding values variables Vv W :Lemma E.4. v V call ReversibleSolve(v, s, d, A, G), holds| (Vv W ) = init | (Vv W ).508fiThe Role Macros PlanningProof. lemma states variables Vv W always take initial valuescall ReversibleSolve. First note ReversiblePlanner calls ReversibleCompose = init. turn, ReversibleCompose makes two calls ReversibleSolvevariable w Vx . calls, variable may different valuew. However, call ReversibleSolve w, holds w W(else action changing value w, remove problem).Thus lemma holds call ReversibleCompose ReversiblePlanner.assume lemma holds call ReversibleSolve(v, s, d, A, G).call ReversibleCompose, value p, p partial statesvariables W . Note definition W , w W v, w Wancestor u v w Vu . Thus value w different init(w)call ReversibleCompose, w W subsequent call ReversibleSolve. Sincevariables W take initial values assumption, take initialvalues subsequent calls ReversibleSolve well.Lemma E.4, number distinct choices calls ReversibleSolvevariable v O(Dk ), k = |W |. Since number distinct choicesO(D), total number calls ReversibleSolve v O(Dk+1 ). callReversibleSolve variable v V breadth-first search graph O(Dk )nodes. action , one edge node, complexitybreadth-first search O(Dk (1 + |A |)) = O(Dk |A |), total complexity callsReversibleSolve v O(D2k+1 |A |). Since sets distinct, total complexityPPcalls ReversibleSolve O( vV D2k+1 |A |) = O(D2k+1 vV |A |) = O(D2k+1 |A|).ReversibleCompose called Dk |A| times, edge onegraphs traversed ReversibleSolve. worst-case complexity ReversibleCompose linear number variables Vx , O(|V |). total complexity ReversibleCompose thus O(Dk |A||V |), total complexity ReversiblePlannerO(D2k+1 |A| + Dk |A||V |) = O(Dk |A|(Dk+1 + |V |)).ReferencesAmir, E., & Engelhardt, B. (2003). Factored Planning. Proceedings 18th International Joint Conference Artificial Intelligence, pp. 929935.Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AIPlanning Automatically Learned Macro-Operators. Journal Artificial Intelligence Research, 24, 581621.Botea, A., Muller, M., & Schaeffer, J. (2007). Fast Planning Iterative Macros.Proceedings 20th International Joint Conference Artificial Intelligence, pp.18281833.Brafman, R., & Domshlak, C. (2003). Structure Complexity Planning UnaryOperators. Journal Artificial Intelligence Research, 18, 315349.Brafman, R., & Domshlak, C. (2006). Factored Planning: How, When, Not..Proceedings 21st National Conference Artificial Intelligence.509fiJonssonBylander, T. (1994). computational complexity propositional STRIPS planning.Artificial Intelligence, 69, 165204.Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32(3), 333377.Chen, H., & Gimenez, O. (2007). Act Local, Think Global: Width Notions TractablePlanning. Proceedings 17th International Conference Automated PlanningScheduling.Chen, H., & Gimenez, O. (2008a). Causal Graphs Structurally Restricted Planning.Proceedings 18th International Conference Automated Planning Scheduling.Chen, H., & Gimenez, O. (2008b). On-the-fly macros. CoRR, abs/0810.1186.Domshlak, C., & Dinitz, Y. (2001). Multi-Agent Off-line Coordination: Structure Complexity. Proceedings 6th European Conference Planning, pp. 277288.Erol, K., Nau, D., & Subrahmanian, V. (1995). Complexity, decidability undecidabilityresults domain-independent planning. Artificial Intelligence, 76(1-2), 7588.Fikes, R., & Nilsson, N. (1971). STRIPS: new approach application theoremproving problem solving. Artificial Intelligence, 5 (2), 189208.Gimenez, O., & Jonsson, A. (2008). Complexity Planning Problems SimpleCausal Graphs. Journal Artificial Intelligence Research, 31, 319351.Gimenez, O., & Jonsson, A. (2009). Planning Chain Causal Graphs VariablesDomains Size 5 NP-Hard. Journal Artificial Intelligence Research, 34,675706.Haslum, P. (2008). New Approach Tractable Planning. Proceedings 18thInternational Conference Automated Planning Scheduling.Helmert, M. (2006). Fast Downward Planning System. Journal Artificial IntelligenceResearch, 26, 191246.Jonsson, A. (2007). Role Macros Tractable Planning Causal Graphs.Proceedings 20th International Joint Conference Artificial Intelligence, pp.19361941.Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions:Algorithms complexity. Artificial Intelligence, 100(1-2), 125176.Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractableplan generation. Annals Mathematics Artificial Intelligence, 22(3-4), 281296.Katz, M., & Domshlak, C. (2008a). New Islands Tractability Cost-Optimal Planning.Journal Artificial Intelligence Research, 32, 203288.Katz, M., & Domshlak, C. (2008b). Structural Patterns Heuristics via Fork Decompositions. Proceedings 18th International Conference Automated PlanningScheduling, pp. 182189.Kelareva, E., Buffet, O., Huang, J., & Thiebaux, S. (2007). Factored Planning Using Decomposition Trees. Proceedings 20th International Joint Conference ArtificialIntelligence, pp. 19421947.510fiThe Role Macros PlanningKnoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68(2), 243302.Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33(1),6588.Minton, S. (1985). Selectively generalizing plans problem-solving. Proceedings9th International Joint Conference Artificial Intelligence, pp. 596599.Newton, M., Levine, J., Fox, M., & Long, D. (2007). Learning Macro-Actions Arbitrary Planners Domains. Proceedings 17th International ConferenceAutomated Planning Scheduling, pp. 256263.Vidal, V. (2004). Lookahead Strategy Heuristic Search Planning. Proceedings14th International Conference Automated Planning Scheduling, pp. 150159.Williams, B., & Nayak, P. (1997). reactive planner model-based executive.Proceedings 15th International Joint Conference Artificial Intelligence, pp.11781185.511fiJournal Artificial Intelligence Research 36 (2009) 415-469Submitted 03/09; published 12/09Friends Foes? Planning SatisfiabilityAbstract CNF EncodingsCarmel Domshlakdcarmel@ie.technion.ac.ilTechnion Israel Institute Technology,Haifa, IsraelJorg Hoffmannjoerg.hoffmann@inria.frINRIA,Nancy, FranceAshish Sabharwalsabhar@cs.cornell.eduCornell University,Ithaca, NY, USAAbstractPlanning satisfiability, implemented in, instance, SATPLAN tool,highly competitive method finding parallel step-optimal plans. bottleneckapproach prove absence plans certain length. Specifically, optimal plann steps, typically costly prove plan length n1.pursue idea leading proof within solution length preserving abstractions (overapproximations) original planning task. promising abstractionmay much smaller state space; related methods highly successful modelchecking. particular, design novel abstraction technique based one can,several widely used planning benchmarks, construct abstractions exponentiallysmaller state spaces preserving length optimal plan.Surprisingly, idea turns appear quite hopeless context planningsatisfiability. Evaluating idea empirically, run experiments almost benchmarksinternational planning competitions IPC 2004, find even hand-madeabstractions tend improve performance SATPLAN. Exploring findingstheoretical point view, identify interesting phenomenon may causebehavior. compare various planning-graph based CNF encodings originalplanning task CNF encodings abstracted planning task. prove that,many cases, shortest resolution refutation never shorter .suggests fundamental weakness approach, motivates investigationinterplay declarative transition-systems, over-approximating abstractions,SAT encodings.1. Introductionareas model checking AI planning well-known closely relateddevelop tools automatic behavior analysis large-scale, declaratively specified transition systems. particular, planning model checking safetypropertieschecking reachability non-temporal formulasproblems given description transition system, initial system state, target condition.solution problem corresponds legal path transitions bringing systeminitial state state satisfying target condition.c2009AI Access Foundation. rights reserved.fiDomshlak, Hoffmann, & Sabharwalmodel checking problem, solution corresponds error path system,is, unwanted system behavior. Proving absence error paths ultimategoal system verification, thus traditional focus field exactly that.Besides clever symbolic representations state space, key technique accomplishambitious task abstraction. System abstraction corresponds over-approximationconsidered transition system, thus abstraction preserves transitionsoriginal system. Hence, abstract transition system contain solution,neither original system. key success model checking that, many cases,one prove absence solutions rather coarse abstractions comparativelysmall state space. Techniques kind explored depth long time.Arguably wide-spread instance model checking predicate abstraction (Graf& Sadi, 1997), system states form equivalence classes defined terms truthvalues number expressions (the predicates), linear expressions integersystem variables. Predicates learned analyzing spurious error paths too-coarseabstractions (Clarke, Grumberg, Jha, Lu, & Veith, 2003). Methods kindextremely successful verification temporal safety properties (e.g., Ball, Majumdar,Millstein, & Rajamani, 2001; Chaki, Clarke, Groce, Jha, & Veith, 2003; Henzinger, Jhala,Majumdar, & McMillan, 2004).contrast system verification, focus AI planning finding solutionsinstances assumed solvable. particular, optimizing planning, taskfind solution optimizes certain criterion (the focus analysishere) sequential/parallel length solution path. Unlike general planningsolution good enough, main bottleneck length-optimizing planning alwaysprove absence solutions certain length. particular, optimal plann steps, hardest bit typically prove plan length n 1.Note plan actually proved optimal, length-optimizingplanner avoid constructing proof, matter computational techniquesbased on.agenda research apply idea model checking lengthoptimizing planning. lead optimality proofnon-existence plan length n1within abstraction. particular, focus interplay abstractionproving optimality parallel step-optimal planning satisfiability. approachoriginally proposed Kautz Selman (1992), later developed SATPLAN tool(Kautz & Selman, 1999; Kautz, 2004; Kautz, Selman, & Hoffmann, 2006). SATPLANperforms iteration satisfiability tests CNF formulas b encoding existenceparallel plan length b, b starts 0 increased incrementally.n first satisfiable formula, n equals length optimal parallel plan,hence SATPLAN parallel optimal, step-optimal, planner. class planners,SATPLAN highly competitive. particular, SATPLAN 1st prizes optimalplanners International Planning Competition (IPC), namely IPC 2004 (Kautz,2004; Hoffmann & Edelkamp, 2005) IPC 2006 (Kautz et al., 2006). One propertyCNF encodings employed SATPLAN plays key role analysis laterbased planning graph structure (Blum & Furst, 1995, 1997).course, objective closely relates many approaches developed planningcomputing lower bounds based over-approximations, (e.g., Haslum & Geffner, 2000;416fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsEdelkamp, 2001; Haslum, Botea, Helmert, Bonet, & Koenig, 2007; Helmert, Haslum, &Hoffmann, 2007; Katz & Domshlak, 2008; Bonet & Geffner, 2008). key difference,however, focus exact lower bounds, is, attempt actually prove optimalitywithin abstraction. want able prove optimality within abstraction,abstraction must term solution length preserving: abstraction mustintroduce solutions shorter optimal solution original problem.lower bound step-optimal planning exact, supportSATPLANs iteration n 1, constitutes main bottleneck.follows, briefly explain initial motivation behind work,summarize empirical theoretical results.1.1 Initial Motivationwould course interesting explore whether predicate abstraction appliedplanning. Indeed, initial idea. However, discussionsidea lead nowhere,1 instead made different discovery. Planning state spacesoften dramatically reduced, without introducing shorter solutions, basedabstraction technique call variable domain abstraction. technique essentiallyadapts work Hernadvolgyi Holte (1999) propositional STRIPS formalism.abstract STRIPS task distinguishing certain values multiplevalued variables underlying STRIPS encoding. is, p q propositionscorresponding non-distinguished values, abstracted STRIPS task acts p qsame. Note generalizes abstraction used Edelkamp (2001) which,multi-valued variable, either abstracts away completely abstractall; see details Section 2.first example noticed compression power variable domain abstraction classical Logistics domain. domain, packages must transported withincities using trucks cities using airplanes. Actions load/unload packages,move vehicles. Importantly, constraints (either of) vehicle capacities, fuel,travel links. consequence, package p starts city destinationcity B, cities C 6= A,B completely irrelevant p. is, one choosearbitrary location x city C, replace facts form at(p,l), llocation outside B, at(p,x). Also, in(p,t), truck outside B,replaced at(p,x). One completely abstract away positions packagesdestination, minor optimizations possible. waylose many distinctions different positions objectswithout introducing shortersolution! optimal plan rely storing package p city ps origindestination. state space reduction dramatic: abstracted state space containsleast (((C 2) S) 1)P states less, C, S, P respectively numbercities, city size (number locations city), number packages. Similar1. still skeptical prospects. Software artifacts (rigid control structure, numeric expressionsessential flow control) rather different nature planning problems (loose controlstructure, numeric expressions non-existent mostly used encode resource consumption). example, major advantage predicate abstraction capture loop invariantsa great feature,seemingly rather irrelevant plan generation.417fiDomshlak, Hoffmann, & Sabharwalabstractions made, similar state space reductions obtained, IPCdomains Zenotravel, Blocksworld, Depots, Satellite, Rovers (see Section 3).1.2 Summary Empirical Resultsfirst experiment, implemented Logistics-specific abstraction abstracting set planning tasks level description, modifying actionsinitial state. planning tasks feature 2 airplanes, 2 locations city,6 packages. number cities scales 1 14. account variancehardness individual instances, took average values 5 random instancesproblem size. increasing number cities introduces increasing amountirrelevance, measure percentage RelFrac facts considered relevant (notabstracted). Note additional cities irrelevant individual packagesthey cant removed completely task like standard irrelevancedetection mechanisms, e.g. RIFO (Nebel, Dimopoulos, & Koehler, 1997), would try do.provided abstracted tasks three optimizing planners, namely Mips.BDD(Edelkamp & Helmert, 1999), IPP (Hoffmann & Nebel, 2001), SATPLAN04,2 order examine abstraction affects different approaches optimizing planning.Mips.BDD searches blindly exploiting sophisticated symbolic representationstate space. IPP equivalent parallel state-space heuristic search widelyused h2 heuristicthe parallel version h2 originally introduced Graphplan (Blum &Furst, 1997; Haslum & Geffner, 2000). Thus, Mips.BDD, IPP, SATPLAN04 representorthogonal approaches optimizing planning.3 abstract task planner,measured runtime, compared latter time taken planneroriginal task. Time-out set 1800 seconds, also used valueaverage computation time-out occurred. stopped evaluating planner 2time-outs within 5 instances one size.Figure 1(a), (b), (c) respectively show results Mips.BDD, IPP, SATPLAN04. Comparing performance original abstracted tasks, apparentFigure 1 proving optimality within abstraction dramatically improvedperformance Mips.BDD, significantly improved performance IPP. rightend scale (with 14 cities), Mips.BDD using abstraction find optimal sequentialplans almost fast SATPLAN04 find step-optimal plans. Given usuallymuch harder find optimal sequential plans optimal parallel plans, especially highlyparallel domains Logistics, performance improvement quite remarkable. (Inaddition reduced state space size, Mips.BDD benefits small state encoding,stops growing point maximal number locations relevantpackage constant.)findings Mips.BDD IPP line original intuition and,SATPLAN04 well, expected see much improved runtime behavior withinabstraction. surprise, not. appears Figure 1(c), improvementobtained SATPLAN04 proving optimality within abstraction hardly dis2. SATPLAN04 version SATPLAN competed 2004 International Planning Competition.3. Importantly, Mips.BDD sequentially optimal SATPLAN04 IPP step-optimal. Henceone compare performance planners directly, particularpurpose here. focus planners reacts abstraction.418fiFriends Foes? Planning Satisfiability Abstract CNF Encodings10000100Mips.BDD-abstractMips.BDD-realRelFrac10000100080100IPP-abstractIPP-realRelFrac9090100080701007060100605010504010403013020120100.1100123456789101112130.1140123456(a)7891011121314(b)10000100SATPLAN-abstractSATPLAN-realRelFrac90100080701006050104030120100.101234567891011121314(c)Figure 1: Runtime performance (a) MIPS, (b) IPP, (c) SATPLAN04, (abstract) without (real) hand-made variable domain abstraction,Logistics instances explicitly scaled increase amount irrelevance. Horizontal axis scales number cities, left vertical axis shows total runtimeseconds, right vertical axis shows percentage RelFrac relevant facts.cernible. right end scale, abstraction yields humble speed-up factor 2.8.particularly insignificant since speed-up obtained drastically smallRelFrac value 24%in IPC 2000 Logistics benchmarks, RelFrac 42% average.latter corresponds 6 cities Figure 1, SATPLAN04 slight advantageoriginal tasks.investigate broadly, conducted experiments almost STRIPSdomains used international planning competitions (IPC) IPC 2004. manycases, tailored abstraction domain hand. results exhaustiveevaluation (discussed detail Section 3) significantly departLogistics-specific abstraction above. Mips.BDD almost consistently obtainedsignificant improvement. IPP improvements happened rarely, typicallysubstantial. (While IPP improved Figure 1, note that, IPC419fiDomshlak, Hoffmann, & Sabharwalaverage RelFrac 42%, improvement yet strong.) Finally, SATPLAN04,hardly ever obtained improvement.causes difference profiting abstraction three differentplanning techniques? intuitive interpretation results informednessabstraction must compete informedness search itself. words,better planner exploiting structure particular example, difficultabstraction exploit structure already exploited. intuitiongood correspondence Logistics results Figure 1: optimizing exactlymeasure, original examples, SATPLAN04 faster IPP, inverserelation holds regarding planner profits abstraction. said,intuitive interpretations results are, point, mere speculation. left openfuture research determine accurately precisely causes difference. Herein,concentrate planning satisfiability, identify fundamental weaknessapproach respect profiting abstraction.1.3 Summary Theoretical ResultsIntrigued results SATPLAN04,4 wondered kind effect abstractionactually CNF encoding planning task formulated Boolean satisfiabilityproblem instance. Recall abstractions over-approximations, is,action sequence applicable original task applicable abstract task,plan original task plan abstract task. So, intuitively, abstract taskgenerous original task. mind, consider CNF formulan1 encoding existence plan one step shorter optimal plan, considerformula, n1 , generated abstract task. need prove n1unsatisfiable. (Note n1 is, fact, unsatisfiable solution length preservingabstraction.) Intuitively, constrained formula is, easier leadproof. n1 generous, hence less constrained, n1 . meanactually harder refute n1 refute n1 ?abstraction methods, fact trivial see answer questionyes. Say abstract n1 ignoring clauses. n1 sub-formulan1 , immediately implying resolution refutation n1 also resolutionrefutation n1 . particular, shortest possible refutation cannot shortern1 . similar situation sometimes occurs interplay abstractions CNFencodings planning problems. instance, suppose abstract ignoring subsetgoals. CNF encodings planning, particular planning-graphbased CNF encodings (Kautz & Selman, 1999) underlying SATPLAN, goal fact yieldsone clause CNF. Hence, goal ignoring abstraction, n1 sub-formulan1 , above.complex example would correspond abstraction ignoring preconditionsdelete effects. encodings used SATPLAN, one several clauses relatedignored precondition/delete effect disappear. However, CNF changes also waysbecause, one precondition/delete effect less, actions facts become possiblelater time steps. Intuitively, additional actions facts help proving4. Translation: Deeply frustrated results SATPLAN04, . . .420fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsunsatisfiability n1 . formal proof intuitive statement, however, less obviousone goal ignoring abstraction above. Matters complicated,much less intuitive, Edelkamps (2001) abstraction variable domain abstraction,changes made abstract task also affect add effects actions. Recallvariable domain abstraction special interest likely satisfyconstraint solution length preservation.investigate issues detail, one consider various possible combinationsCNF encodings abstraction methods. Many different encodings planning SATproposed. focus planning-graph based encodings usedSATPLAN system, appearances international planning competitions (Kautz & Selman, 1999; Long, Kautz, Selman, Bonet, Geffner, Koehler, Brenner,Hoffmann, Rittinger, Anderson, Weld, Smith, & Fox, 2000; Kautz, 2004; Kautz et al., 2006).Indeed, according Kautz Selman (Kautz & Selman, 1999; Long et al., 2000),CNF encodingsin particular mutex relations computed Graphplanare vitalSATPLANs performance. recent results effective encodings may challengeassessment (Rintanen, Heljanko, & Niemela, 2006; Chen, Huang, Xing, & Zhang, 2009;Robinson, Gretton, Pham, & Sattar, 2008), even graphplan-based encodingsinterest simply widely used almost decade. remainscourse important question whether extent results carry alternative CNF encodings. discuss issue depth concluding paperSection 5.consider four different encodings, three used editionIPC; fourth encoding considered sake completeness. encodingsdiffer two parameters: whether use action variables, action well factvariables; whether include planning graph mutexes actionsdirect interferences. (The latter motivated fact often enormousamount action mutexes, seriously blowing size formula.)abstractions side, focus abstraction methods formulatedmanipulating planning tasks language level, i.e., modifying tasks actions and/orinitial/goal states. Many commonly used abstractions propositional STRIPSformulated way. consider six abstractions, namely (1) removing goals, (2)adding initial facts, (3) removing preconditions, (4) removing delete effects, (5) Edelkamps(2001) abstraction (removing entire facts), (6) variable domain abstraction.24 combinations CNF encoding abstraction method, proveshortest possible resolution refutation exponentially longer n1 n1 .20 combinations involving abstractions variable domain abstraction,prove shortest possible resolution refutation cannot shorter n1n1 . abstraction (1), trivial outlined above. abstractions (2)(4),proof exploits fact abstractions lead larger planning graphs containingactions facts. abstraction (5), reasoning workfacts disappear planning graph. However, one start removing factgoal action preconditions; afterwards fact irrelevant one removealso initial state action effects.Matters complicated abstraction (6), is, variable domain abstraction.encoding action variables full mutexes, show that, before,421fiDomshlak, Hoffmann, & Sabharwalshortest possible resolution refutation cannot shorter n1 n1 .encoding action variables direct action mutexes, showpossible improvement bounded effort takes recover indirectaction mutexes. two encodings action fact variables, remainsopen question whether bounds exist.5Importantly, proofs valid general resolution, also manyknown restricted variants resolution, particular tree-like resolution refutations generated DPLL (Davis & Putnam, 1960; Davis, Logemann, & Loveland, 1962).Naturally, proofs separate combinations, rather exploitexhibit common features.practical significance theoretical results is, extent, debatabledirect connection best-case resolution refutation size empiricalSAT solver performance. Even large refutation may easy find mostlyconsists unit resolutions. Vice versa, small refutation exists,mean SAT solver find it. notwithstanding, appears unlikely best-caseresolution refutation size practical SAT solver performance completely unrelated(beyond obvious lower bound). One example indicates opposite planninggraph mutexes. Mutexes reduce best-case refutation size workresolution even invoked.6 words, SAT solvers exploit mutexesprune search trees effectively. aware explicit empirical prooftends happen often, seems little doubt does. alsosuggested explicitly Kautz Selman (Kautz & Selman, 1999; Long et al., 2000)ways explaining improved performacce system run graphplan-basedencodings.interesting situation arises (all) experiments. use variable domain abstraction encoding action variables direct action mutexes (asemployed SATPLANs IPC04 version). setting, resolution refutations getshorter principle, although effort takes recover indirect action mutexes. Further, employ trivial post-abstraction simplification methods (suchremoving action duplicates) which, show, also potential shorten resolution refutations. Still, reported above, discernible empirical improvement.reason might SAT solver find shorter refutations,shorter refutations actually appear significant scale. evidenceindicating latter: mutex recovery becomes necessary rather special situations,abstraction turns indirect mutex direct one. typically concern small fraction indirect mutexes. addition, mutex recoverysimplifications, well-designed variable domain abstraction affected actionstypically irrelevant anyway. example, hand-made Logistics abstraction,5. reason complications answering question requires determining, planning-graphbased encodings general, whether fact variables syntactic sugar may lead succinctrefutations. proof appears quite challenging; say Section 4.6. General resolution recover mutexes effectively, c.f. related investigations Brafman (2001)Rintanen (2008). seem likely case tree-like resolution,best knowledge yet known.422fiFriends Foes? Planning Satisfiability Abstract CNF Encodingseffect potential improvements limited actions appearing redundantplans. get back detail Section 4.view, theoretical results would potential importance evenevidence empirical relevance, simply quite surprising. momentthought, clear resolution refutation become easier ignoring goals.However, variable domain abstraction domains Logistics deflates state spacesimmensely, point tiny fraction original size.performing work, would never expected best-case refutation size remainsame.paper organized follows. Section 2 discusses preliminaries, covering employed notions planning, planning graphs, propositional encodings, resolution, abstraction methods; particular, formally defines variable domain abstraction. Section 3summarizes empirical results. Section 4 presents results regarding resolution refutations abstract CNF encodings. Related work discussed text appropriate. conclude Section 5. Appendix contains proofs, replacedbrief proof sketches main body text. Additional empirical datafound online appendix (see JAIR web page article).2. Preliminariesbegin discussion various concepts needed rest paper: propositionalSTRIPS planning, planning graphs, propositional CNF encodings planning problems,resolution proofs unsatisfiability, abstraction methods used planning. generalrule notation, use variants of: P planning tasks; F, A, G sets facts,actions, goals, respectively; abstractions; P G planning graphs;propositional formulas encodings.2.1 STRIPS Planning GraphsClassical planning devoted goal reachability analysis state transition models deterministic actions complete information. model tuple = hS, s0 , SG , A,finite set states, s0 initial state, SG set alternativegoal states, finite set actions, : transition function,(s, a) specifying state obtained applying s. solution, plan,state transition model sequence actions a1 , . . . , generate sequencestates s0 , . . . , sm that, 0 < m, (si , ai+1 ) = si+1 , sm SG .AI planning targets large-scale state transition models huge numbersstates, models assumed described concise manner via intuitivedeclarative language. use propositional fragment STRIPS language (Fikes& Nilsson, 1971). brevity, refer fragment STRIPS herein. Informally,planning task planning instance STRIPS consists set propositional facts,hold initially must hold simultaneously end planexecution. state system time defined set propositional factshold time. task specifies set actions, definedset precondition facts, set facts added state, set factsremoved state, action taken. Formally, STRIPS planning task423fiDomshlak, Hoffmann, & Sabharwalgiven quadruple P = (P, A, I, G) fact set P , initial state description P , goaldescription G P , action set every action pre(a), add (a),del (a), subset P . planning task defines state transitionmodel = hS, s0 , SG , A, state space = 2P , initial state s0 = I,S, SG iff G s. S, A(s) = {a | pre(a) s}actions applicable s, A(s), (s, a) = (s \ del (a)) add (a).assume actions reasonable sense add (a) del (a) = .satisfied known planning benchmarks; particular satisfied benchmarksused experiments.7Many planning algorithms, including SATPLAN, employ form approximatereachability analysis. One primary tools purpose planning graph, firstintroduced scope Graphplan planner (Blum & Furst, 1997). length boundb, planning graph P G(P) associated P layered graph two kinds nodes:fact nodes action nodes. layers alternate fact layers F (0), F (1), . . . , F (b),action layers A(0), A(1), . . . , A(b 1), pair layers F (t), A(t) formingtime step t. first vertex layer F (0) contains initial state. A(t) F (t + 1)0 < b action sets fact sets, respectively, available time step + 1.precisely, A(t) includes actions pre(a) F (t) pairfacts p, p0 pre(a) mutex layer (c.f. below); further, A(t) contains standard noopaction every fact F (t).8 F (t + 1) contains union add effectsactions A(t). Obviously, A(t) A(t + 1) F (t) F (t + 1). goal facts Glabel appropriate vertices F (b). P G(P) four kinds edges:(1) Epre (t) F (t) A(t) connect actions A(t) preconditions F (t),(2) Eadd (t) A(t) F (t + 1) connect actions A(t) add effects F (t + 1),(3) Ea-mutex (t) A(t) A(t) capture pair-wise mutual exclusion relation actions A(t); (a(t), a0 (t)) Ea-mutex (t), actions a0 cannot appliedsimultaneously time t,(4) Ef -mutex (t) F (t) F (t) capture pair-wise mutual exclusion relation factsF (t); (f (t), f 0 (t)) Ef -mutex (t), facts f f 0 cannot hold together timet.Note P G(P) explicit edges deletion effects actions;effects captured mutual exclusion relation (e.g., p add (a1 ) del (a2 ),(a1 , a2 ) Ea-mutex times). mutex edges Emutex = Ea-mutex Ef -mutexcomputed iterative calculation interfering action fact pairs (Blum & Furst,1997). Namely, two actions (directly) interfere effects one contradict effectsother, one deletes precondition other. Two actions competing7. IPC-2002 domain Rovers, operators add delete artificial fact order preventparallel application. implement restriction via duplicating respective operatorssequentializing original duplicate via two artificial facts. Similar fixes implementedcouple domains well.8. fact p P , associated noop(p) delete effects, {p} preconditionsadd effects. dummy actions simply propagate facts one fact layer next.424fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsneeds mutex preconditions. Combining two scenarios together, saytwo actions mutex either (directly) interfere competing needs.similar spirit, two facts mutex non-mutex pair actions (in graphlayer directly below) together achieving facts. variant interestplanning graph which, iterative computation, Ea-mutex reduced containdirectly interfering actions. call reduced planning graph, denoteP Gred (P). motivation considering that, often, reduced planninggraph results much smaller SAT encodings; get back below.2.2 Propositional Encodingsconsider three CNF encodings used (one version of) SATPLAN, wellfourth encoding fits naturally picture. encodings takes inputplanning task P length bound b, creates formula standard ConjunctiveNormal Form (CNF). CNF formula solved off-the-shelf SAT solver.process constitutes basic step SAT-based approach planning implementedSATPLAN (Kautz & Selman, 1992, 1996, 1999), one starts b = 0 iterativelyincrements b CNF becomes satisfiable first time. plan correspondingsatisfying assignment plan minimal b, hence optimalsense.9CNF formula logically conjunction (and) clauses, clause disjunction (or) literals, literal propositional (Boolean) variable negation.CNF formulas often written set clauses, clause written set literals, underlying logical conjunction disjunction, respectively, implicit.propositional encodings bounded-length planning tasks specified terms variouskinds clauses generated encoding method.Encoding (A) constructed P G(P) uses propositional action variables{a(t) | 0 < b, A(t)}. goal fact g goal clause form{a1 (b 1), . . . , al (b 1)}, a1 , . . . al actions A(b 1) add g.Similarly, every a(t) > 0 every p pre(a) precondition clause{a(t), a1 (t 1), . . . , al (t 1)}, a1 , . . . al actions A(t 1) add p.Finally, mutex clause {a(t), a0 (t)} every (a, a0 ) Ea-mutex (t).(Note dependence initial state taken account alreadyterms actions contained sets A(t), need statedexplicitly CNF.)Encoding (B) similar (A) except uses variables (and appropriate clauses)also facts. specifically, addition action variables, factvariables {f (t) | 0 b, f F (t)}. goal fact g, goal clause simplyunit clause asserting g(b). > 0 fact f (t), effect clauseform {f (t), a1 (t 1), . . . , al (t 1)}, a1 , . . . al actions A(t 1)add f . every a(t) every p pre(a) precondition clause,9. versions SATPLAN use naive incremental update b, shownclever strategies, exploiting typical distribution runtime different values b (Rintanen,2004; Streeter & Smith, 2007).425fiDomshlak, Hoffmann, & Sabharwaltakes form {a(t), p(t)}. action mutex clauses {a(t), a0 (t)} every(a, a0 ) Ea-mutex (t), fact mutex clauses {f (t), f 0 (t)} every(f, f 0 ) Ef -mutex (t). Finally, fact f F (0), initial state clause{f (0)} (these strictly necessary implemented SATPLANinclude here).Encoding (C) like (A) except based reduced planning graph P Gred (P),mutex clauses present action pairs whose preconditions effectsinterfere directly.Encoding (D) like (B) except that, (C), based P Gred (P), mutexclauses action pairs whose preconditions effects interfere directly. Note,however, fact mutexes full planning graph, P G(P).encodings reasonable ways turning planning graph CNF formula. encodings essentially underly competition implementations SATPLAN.detail below. First, note different encodings different benefitsdrawbacks. First, observe encodings characterized two decisions: (1)include action mutexes Graphplan, direct interferences? (2)include action variables, action fact variables? Regarding (1),empirical observation mutexes help one major observationsdesign SATPLAN (then called Blackbox) (Kautz & Selman, 1999; Long et al., 2000),particular comparison earlier encoding methods (Kautz, McAllester, & Selman, 1996).hand, since mutexes talk pairs facts actions, encodings maybecome quite largethere one clause every pair mutex actions mutex facts.particularly critical actions, many planning benchmarksthousands (compared hundred facts). Indeed, turns action mutexes often consume critically large amounts memory. uncommon CNFformulas millions clauses, action mutexes (Kautz & Selman, 1999;Kautz, 2004; Kautz et al., 2006). motivates encodings (C) (D). question(2), make much difference, empirically, planning benchmarks.consider distinction used versions SATPLAN.Let us say words clarify exactly encodings (A)(D) relate SATPLANliterature implementations. Due long history SATPLAN, wellimprecisions literature, little complicated. foremost referenceactual program code underlying SATPLAN04 SATPLAN06, i.e., recentversions used 2004 2006 competitions. encoding methods versionsimplemented one authors paper. four different encodingmethods: action-based, graphplan-based, skinny action-based, skinny graphplan-based.action-based encoding exactly (A), graphplan-based encoding exactly (B),skinny graphplan-based encoding exactly (D).10 skinny action-based encodinglike (C) except that, save runtime, planning graph implementationpropagate mutexes (after all, direct interferences present final encoding),effectively computing relaxed planning graph (Hoffmann & Nebel, 2001). use normal10. 2004 version, skinny graphplan-based encoding feature fact mutexes.consequence encoding used 2004 competition.426fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsplanning graph (C) sake readabilitythe greater similarityencodings significantly simplifies write-up, theoretical results hold statedalso relaxed planning graphs.SATPLAN encodings develop historically, reflected literature, encodings used competitions? answer questionsextent necessary explaining encodings (A)(D). original paper SATPLAN(Kautz & Selman, 1992) introduced rather different encodings. Graphplan-based encodingsdirect action mutexes introduced next, observed yield performancecomparable Graphplan (Kautz & Selman, 1996). Subsequently, observedmodern SAT solvers profit full (fact action) mutex relations actuallybeat planners, several domains (Kautz & Selman, 1999).11 Consequently,graphlan-based encoding, i.e., encoding (B), used 1998 2000 competitions (Long et al., 2000). Prior 2004 competition, encoding methodsre-implemented, yielding four methods explained above. IPC04 booklet paperSATPLAN04 (Kautz, 2004) describes four encodings.12 version run competition skinny action-based encoding (for theoretical results) equivalentencoding (C). running planner IPC 2006, turned fullfact mutexes helped domains, encoding (D) used instead (Kautz et al.,2006). consider encoding (A) sake completeness.2.3 Resolution Refutationstheoretical results respect resolution proof system (Robinson, 1965),forms basis complete SAT solvers around today (cf. Beame, Kautz,& Sabharwal, 2004). sound complete proof system, studied extensively theoretical practical reasons. works CNF formulas onesimple rule inference: given clauses {A, x} {B, x}, one derive clause {A, B}resolving upon variable x. B shorthands arbitrary lists literals.Note choice clauses resolve arbitrary, long share variable,opposite signs. resolution derivation clause C formula consists seriesapplications resolution rule starting clauses one eventuallyderives C; C (unsatisfiable) empty clause, {}, called resolution proof(of unsatisfiability) refutation . size number applicationsresolution rule . unsatisfiable, RC() denotes resolution complexity ,i.e., size smallest resolution proof unsatisfiability . interestedwhether applying abstraction planning task convert encoding onesmaller resolution complexity.commonly studied sub-class (still sound complete) resolution derivationstree-like resolution derivations, derived clause usedwhole derivation; underlying graph structure proof tree.11. Kautz Selman (1999) cite graphplan-based encodings earlier work (which used actionmutexes). However, Blackbox program code includes functions generate full mutexes, KautzSelman explicitly emphasize importance mutexes.12. paper abstract little imprecise description: initial state, goal, factmutex clauses mentioned; skinny action-based encoding stated identicalencoding (C), i.e., based full planning graph rather relaxed planning graph.427fiDomshlak, Hoffmann, & Sabharwalinteresting sub-classes resolution include regular resolution, provably exponentially powerful tree-like resolution variable resolved uponpath root leaf underlying proof graph, orderedresolution, addition variables respect fixed ordering root-to-leaf path.Tree-like resolution captures proofs unsatisfiability generated SAT solversbased DPLL procedure (Davis & Putnam, 1960; Davis et al., 1962) dont employ so-called clause learning techniques; latter kind SAT solvers provablyexponentially powerful even regular resolution although still within realmgeneral resolution (Beame et al., 2004).note arguments presented paper general (unrestricted) resolution. However, since aour constructions affect rely structuralproperties resolution refutations consideration, results hold stated (except slight weakening case Lemma 4.14) known variants resolutionsetting variables True False replacing one variable another preservesproof structure. variants include tree-like (DPLL), regular, ordered resolution.state standard property resolution proofs use arguments,pointing certain modifications (such variable restrictions shorteningclauses) given formula cause proofs become longer general resolutionnatural sub-classes, including mentioned above. Let x variableTrue, False, another (possibly negated) variable . variable restrictionx transformation replaces x throughout , simplifiesresulting formula removing clauses containing True variable negation,removing False duplicate literals clauses. words, variable restrictioninvolves fixing value variable identifying another literal, simplifyingformula. sequence variable restrictions , | denoteoutcome applying .following proposition combines two basic facts together form usefulus: (1) variable restrictions cannot increase resolution complexity formula,(2) lengthening clauses and/or removing clauses cannot decrease resolution complexityformula.Proposition 2.1. Let CNF formulas. exists sequence variablerestrictions every clause | contains sub-clause clause ,RC() RC( ).explanation proposition use order. notationused chosen match way eventually utilize propositionproofs; below. conditions proposition imply one may obtainapplying restriction , possibly throwing away literalsclauses, possibly adding new clauses. Intuitively, three modificationsreduce number solutions cannot make harder proveformula unsatisfiable. property resolution refutations propositional formulaspreviously used (at least indirectly) various contexts. completeness, includeproof Appendix A, based folklore ideas proof complexity literature.alternative proof, somewhat different notation, may also found appendixrecent article Hoffmann, Gomes, Selman (2007).428fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsway use proposition following: CNF encodingabstracted planning task, encoding original planning task,carefully chosen restriction bring focus variables alreadyappearing . Proposition 2.1 imply original encoding harderrefute, using resolution natural sub-classes, abstracted encoding.2.4 Abstraction PlanningAbstraction methods various kinds put use planning, often quite successfully. One line work uses abstraction methods problem decomposition (cf. Sacerdoti,1973; Knoblock, 1990; Koehler & Hoffmann, 2000). best knowledge,approachexamining abstract state space order prove absence solutionspursued before. line work relevant work domainindependent heuristic functions (cf. McDermott, 1999; Bonet & Geffner, 2001; Hoffmann &Nebel, 2001; Edelkamp, 2001; Haslum et al., 2007; Helmert et al., 2007; Katz & Domshlak,2008). There, abstraction means over-approximation state space, work;differs abstractions used. course, kinds over-approximationsuseful either purpose differ lot. use abstractionpaper, one define over-approximations preserve, large extent, realstructure problem. particular, ideal goal find abstractions preserve length optimal solutionsomething one definitely wouldnt expectabstraction underlying heuristic function, since solved every search state.briefly review over-approximation methods usedplanning far; formally introduce novel one, variable domain abstraction.use Logistics domain illustrative example.13One wide-spread over-approximation method planning 2-subset relaxationunderlying computation made planning graph (Blum & Furst, 1997), generalized m-subset relaxation Haslum Geffner (2000). nutshell, one assumesachieving set facts hard achieving hardest m-subset. knownthat, domains, including Logistics, 2-subset solution length (correspondinglength planning graph constructed first fact layer containing mutexesgoal facts) typically strictly lower length optimal plan.> 2, hand, computing m-subset solution length typically costly,still, = o(|P |) one typically guarantee solution length preservation (Helmert& Mattmuller, 2008).second wide-spread over-approximation method ignoring delete lists (McDermott,1999; Bonet & Geffner, 2001). approximation, one simply removes (some of)negative effects actions. negative effects removed, problembecomes solvable time linear instance size. latter basis heuristicfunctions used many modern planners (cf. Bonet & Geffner, 2001; Hoffmann & Nebel,2001; Gerevini, Saetti, & Serina, 2003). Ignoring deletes likely introduce shortersolutions. example, Towers Hanoi problem, leads plans length n instead2n 1 (Hoffmann, 2005). Logistics, one ignores deletes moving actions13. stated, open topic explore model checking abstractions, particular predicate abstraction(Graf & Sadi, 1997; Clarke et al., 2003), instead planning abstractions.429fiDomshlak, Hoffmann, & Sabharwalplans may get shorter vehicles never move back abstraction.Interestingly, ignoring deletes load/unload decrease plan length, sinceactions never undone optimal plan. use observationexperiments.third abstraction introduced Edelkamp (2001) pattern databaseheuristic. approximation, one completely removes facts problemdescription, notably facts corresponding values multi-valued variables.enough facts removed, task becomes sufficiently simple. Obviously, approximation hardly solution length preserving. Logistics, remove, example,fact at(package1,airport2) then, particular, package1 loaded onto airplaneairport2 without actually precondition removed. optimalplan make package pop anywhere.fourth abstraction, finally, involves removing preconditions (Sacerdoti, 1973)and/or goal facts. Edelkamps abstraction, cannot expected solutionlength preserving interesting cases.calls new abstraction method, designed following HernadvolgyiHolte (1999). Considering STRIPS-like state transition systems multiple-valued(instead Boolean) variables, propose reduce variable domains distinguishing certain values. example, content cell 8-puzzle{blank , 1, 2, 3, 4, 6, 7, 8} may replaced {blank , 1, 2, 3} {3, . . . , 8}mapped onto 3. observation that, many planning benchmarks, donewithout introducing shorter plans. example, Logistics unnecessary distinguishpositions packages irrelevant cities. Therefore, replace domain at(p),{A1 , A2 , B1 , B2 , C1 , C2 , . . . }, B initial goal cities p Ai , Bi , . . .locations cities A, B, . . ., abstract domain {A1 , A2 , B1 , B2 , C1 }. STRIPS,amounts replacing set irrelevant facts at(p, l) single fact at(p, C1 ).formalize idea.Let persistently mutex denote standard notion two facts mutexfixpoint layer planning graph: typically, different values multiple-valued variable.Definition 2.2. Let P = (P, A, I, G) STRIPS planning task, p, p0 P pairpersistently mutex facts that, A, ({p, p0 } del (a)) pre(a).((P ), {(a)|a A}, (I), (G)) called variable domain abstraction P,defined follows:1. fact set F , (F ) = F p0 6 F ; otherwise, (F ) = (F \ {p0 }) {p}.2. action = (pre, add , del ), (a) = ((pre), (add ), (del )) p 6 (add )(del );otherwise, (a) = ((pre), (add ), (del ) \ {p}).words, Definition 2.2 simply says replace p0 p. p appearsadd list delete list action, remove delete list.14situation arise, instance, action moves package one irrelevant position14. reader may wonder p remains add list, although prerequisite p (pre). reasondistinguish abstractions simplifications: change planning task;abstractions, simplifications, way may alter tasks semantics. However,simplifications may well affect resolution complexity. get back later paper.430fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsanother irrelevant position. operation, p equivalent originallyp p0 : p True abstracted action sequence p p0 Truecorresponding real action sequence. particular, Proposition 2.3 states variabledomain abstraction over-approximation usual sense.Proposition 2.3. Let P = (P, A, I, G) STRIPS planning task, let (P) =((P ), {(a) | A}, (I), (G)) variable domain abstraction P. Then, wheneverha1 , . . . , plan P, h(a1 ), . . . , (an )i plan (P).Proof. Let state models induced P (P). First, let us showthat, state M, action M, applicable s, (i)(a) applicable (s), (ii) p ((s), (a)) {p, p0 } (s, a) 6= .Note that, since abstraction effect facts {p, p0 }, (ii) implies((s), (a)) = ((s, a)). Thus, together, (i) (ii) imply (iii) homomorphic. Finally, straightforward Definition 2.2 (iv) initial state(I), goal states exactly {(s) | SG }. Together, (iii) (iv)imply claim Proposition 2.3.Let = (pre, add , del ). applicability (a) (s) straightforward; p0 6 pre,(pre) = pre (s)(pre) = spre, otherwise (pre) = (pre\{p0 }){p}(s) = (s \ {p0 }) {p}. cases, pre implies (pre) (s). Considersub-claim (ii) case-by-case basis.{p0 , p} add = , {p0 , p} del = p 6 add ((a)) p 6 del ((a)), thusp ((s), (a)) iff p (s) iff {p, p0 } 6= iff (see assumption del case){p, p0 } (s, a) 6= .{p0 , p} add 6= , {p0 , p} del = p add ((a)) p 6 del ((a)), thusp ((s), (a)). hand, {p, p0 } (s, a) 6= also trivially holds here.{p0 , p} add = , {p0 , p} del 6= p 6 add ((a)) p del ((a)), thusp 6 ((s), (a)). hand, since p, p0 persistently mutex facts P,{p0 , p} del = {p0 , p} pre, also {p, p0 } (s, a) = .{p0 , p} add 6= , {p0 , p} del 6= p add ((a)) p 6 del ((a)), thusp ((s), (a)). hand, add del = {p0 , p} add 6=immediately {p, p0 } (s, a) 6= .completes proof (ii).Arbitrarily coarse variable domain abstractions may generated iterating application Definition 2.2. Note variable domain abstraction refinement Edelkamps(2001) abstractioninstead acting irrelevant positions could totally ignored,distinguish whether package currently position. makesdifference preserving optimal solution length not.15hinted above, variable domain abstraction may able applysimplifications. simplification, terminology, similar abstraction15. topic future work explore whether refined abstraction lead better pattern databaseheuristics STRIPS problems.431fiDomshlak, Hoffmann, & Sabharwalmanipulates planning task language level. However, abstractions may altertasks semantics, simplifications not; i.e., simplifications introduce newtransitions goal states. Concretely, consider two simplifications. planning taskP = (P, A, I, G) duplicate actions exist a, a0 pre(a) = pre(a0 ),add (a) = add (a0 ), del (a) = del (a0 ).16 simplified planning task like P excepta0 removed. planning task P = (P, A, I, G) irrelevant add effectsexists pre(a) add (a) 6= . simplified planning task like P exceptremove pre(a) add (a).Obviously, duplicate actions irrelevant add effects may arise outcome variable domain abstraction. example latter action moving packageirrelevant location irrelevant truck. example former two actions loadingpackage onto airplane distinct irrelevant locations.17 implementation,simple post-abstraction processing perform simplifications.shall see Section 4.3, simplifications lead decreased resolution complexity, thereby offsetting result abstractions such, many cases, cannot.may seem little artificial distinguish abstractions simplifications way, seeingmany abstractions bound enable us simplify. However, note distinctionserves identify borderline can, cannot, reduce resolutioncomplexity. Anyhow, shall see next section, abstraction tend helpempirically performance SATPLAN even post-abstraction simplifications.3. Empirical Resultsperformed broad empirical evaluation effect abstractions efficiencyoptimizing planning algorithms. mostly focus variable domain abstraction,Definition 2.2, since clearly promising obtaining solution length preservingabstractions.Section 3.1 explains specific variable domain abstractions employ experiments. Section 3.2 explains experimental setting chose present hugedata set results. Section 3.3 describes experiments variable domain abstraction IPC benchmarks, Section 3.4 discusses results domain-specificabstractions hand-made instances certain benchmark domains amountirrelevance controlled. Section 3.5 briefly summarizes findings abstractionmethods variable domain abstraction.3.1 Variable Domain Abstractionsdesigned three different methods automatically generate variable domain abstractions. methods listed based increasingly conservative approximations16. Note define actions triples pre, add , del , components; hencetwo actions identical pre, add , del may contained set A. reflects practical plannerimplementations, actions names and/or unique IDs, checks duplicate actionsusually performed.17. similar fashion, duplicate actions may arise outcome Edelkamps (2001) pattern databaseabstraction.432fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsrelevance. common relevance approximations (cf. Nebel et al., 1997), algorithmic basis is, cases, simplified backchaining goals.1Support starts first layer planning graph contains goal facts (possiblymutexes them). goal fact layer, selects one achieverpreceding action layer marks preconditions action new sub-goals;process iterated created sub-goals.AllSupports proceeds similarly 1Support except selects achievers(sub-)goal.AllSupportsNonMutex proceeds similarly AllSupports except starts backchaining first plan graph layer contains goals without mutexes.three methods, selected set relevant facts R P taken setgoals sub-goals created backchaining. set facts turnedvariable domain abstraction follows. First, compute partition problemsfact set P subsets P1 , . . . , Pk pairwise persistently mutex facts. takesubsets correspond underlying multiple-valued variables (e.g., positionpackage). perform abstraction within Pi values relevant,i.e., Pi \ R 6= . Within subset Pi , arbitrarily choose one irrelevant fact p, i.e.,p Pi \ R. replace irrelevant facts, i.e., q Pi \ R q 6= p,p.example, Logistics, 1Support abstracts away in(p, v) facts package p except vehicles v selected supportin particular, singleairplane. contrast, AllSupports mark in(p, v) relevant airplanes v unlessspecial case applies (e.g., p must transported within origin city only). Finally,AllSupportsNonMutex even conservative covers special casesAllSupports abstracts in(p, v) fact away. Note identifying positions insideairplanes positions outside airplanes may well affect length optimal plan.addition domain-independent, automatic variable domain abstractions, sixIPC domains designed domain-specific solution length preserving variable domainabstractions hand. Logistics, domain-specific abstraction explainedintroduction. Zenotravel, use similar abstraction exploiting irrelevant object positions. Blocksworld, on(A, B) considered irrelevant B neither initialgoal position A, B initially clear.18 Depots, combination LogisticsBlocksworld, abstraction combination two individual abstractions.Satellite, abstraction performs simple analysis goal relevance detect directionsirrelevant satellite turn to. direction relevant satellites initial direction, goal direction, potential goal camera calibration target.Similarly, Rovers, waypoint (location) considered relevant rovereither initial position, relevant needed rock sample/soil sample/image,necessarily lies path rover must traverse reach relevant location.18. last conditions necessary avoid possibility clearing block C movingaway C although actually placed third block.433fiDomshlak, Hoffmann, & Sabharwal3.2 Experiment Setup Presentationpresented data generated set work stations running Linux, Pentium 4 processor running 3 GHz 1 GB RAM. used time cutoff 30 minutes.experimented plan-length optimizing planners SATPLAN04, IPP (Koehler,Nebel, Hoffmann, & Dimopoulos, 1997), Mips-BDD (Edelkamp & Helmert, 1999).19choice SATPLAN04 rather SATPLAN06 arbitrary, except that, usingnaive encoding (C), resolution best case SATPLAN04 improved variable domain abstractionmaking bad empirical results even significant.Note also that, although SATPLAN06 could considered recent, containsdevelopments beyond SATPLAN04, switching back older versionencoding method.test examples, took, exceptions listed below, STRIPS domains usedinternational planning competitions including IPC-2004. Precisely, use(IPC-2004) Airport, Dining Philosophers, Optical Telegraph, Pipesworld NoTankage,Pipes- world Tankage, PSR.(IPC-2002) Depots, Driverlog, Freecell, Rovers, Satellite, Zenotravel.(IPC-2000) Blocksworld Logistics. (Miconic-STRIPS simple versionLogistics, Freecell part IPC-2002 set.)(IPC-1998) Grid, Mprime, Mystery. (Movie trivial, Gripper variable domainabstraction cannot preserve solution length, Logistics part IPC-2000 set.)measurements aimed highlighting potential abstraction principlespeeding computation information task. Concretely, givenplanning task , create abstract version , run planner X it.three possible outcomes:(1) X finds plan , abstract plan, happens real plan (that is, plan). record time taken find plan, along time taken Xfind plan given original task .(2) X finds plan real plan. Since planners optimize planlength, information still gain length optimal abstract plan,lower bound length real plan. record time taken computebound (for example, SATPLAN04, time taken last unsatisfiableiteration), along time taken X compute lower bound givenoriginal task .(3) X runs time memory. case, one could record time takenlast lower bound proved successfully. sake readability, omitconsider cases (1) (2) above.19. SATPLAN04 IPP optimize step-length plan, Mips-BDD optimizes sequential planlength. However, again, performance planners stand comparative evaluationhere, refer three simply plan-length optimizing planners.434fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsNote that, spirit optimistic usefulness abstraction,include time taken create abstract task . Note also actually obtainseveral results pair X, namely one result every particular variable domainabstraction. sake readability, include distinctions results(the distinctions mostly inconclusive uninteresting anyway), instead presentresults following best abstraction perspective. skip abstract taskseither solved, abstract since facts considered relevant.abstract task remains, skip instance. Otherwise, select abstract taskproviding best information instance: best case abstract planreal, else select highest lower bound.20 several abstractions providingbest information, choose one lowest runtime.3.3 IPC BenchmarksDue sheer size experiments3 planners multiplied 17 domainsdiscussingentire result set neither feasible would useful. online Appendix (see JAIRweb page article) contains detailed data three optimal planners. Herein,provide summary analysis showing main points, particular focus SATPLAN04.Detailed data SATPLAN04 4 17 IPC domains given Table 1:Depots Satellite selected table 2 17domains abstraction brings somewhat significant advantage.Logistics selected illustrative example.PSR selected due interesting caseunusually, current optimal planners well (or badly) PSR current sub-optimal (satisficing) planners.domain, selected 13 challenging instances, challengingmeasured runtime taken original task. Note problem-instance selectioncriterion presentation also optimistic point view abstraction.instance, Table 1 first specifies whether found abstract plan real plan not.characterizes problem instance terms cases (1) (2) explained above,corresponding runtimes SATPLAN04 abstract real tasks givenrows ta tr . table specifies lower bound lg proved real taskplanning graph (that is, F (t) first fact layer contain goal factsmutexes them), lower bound la proved SATPLAN04 abstract task,and, finally, actual length lr optimal plans real task. last row RelFractable specifies percentage facts considered relevant.Depots, best-case data shown Table 1 scattered across four kindsvariable domain abstractions, automatic abstractions sometimessometimes less aggressive handmade abstraction. example, instances numbers11 15 best case aggressive 1Support strategy. time20. Note quality information essential. abstraction tells us planmust least n 1 steps, real plan length n, must still prove bound n,typically takes time bounds together.435fiDomshlak, Hoffmann, & SabharwalIndexIsReal?tatrlg , l , LRelFracAbsIndexIsReal?tatrlg , l , LRelFracAbsIndexIsReal?tatrlg , l , LRelFracAbsIndexIsReal?tatrlg , l , LRelFracAbs264.4374.266,12,1237%1S16N0.630.85,15,1547%ASnm100.992.1410,15,1533%1S373.2945.7711,12,1288%4112.30104.056,10,1091%HM2264.2971.867,25,2575%ASnm1391.4375.289,13,1348%ASnm4429.74472.0112,14,1488%553.0651.534,7,795%HM2911.1612.057,18,1879%ASnm1425.4132.879,12,1248%ASnm710.4412.257,10,1077%ASnm8310.76250.084,8,890%HM31N1.048.385,16,1649%ASnm15N70.5768.129,13,1355%ASnm8N228.3022.529,13,1476%934.3540.654,6,684%HM33N0.931.335,16,1648%ASnm16N111.6675.799,13,1347%ASnm1047.7733.118,10,1087%ASnmDepots1113N49.739.072.1312.4213,10,?9,9,927%85%1SASnmLogistics1718NN150.37770.44106.61642.979,13,14 9,15,1544%42%ASnmHMPSR36372.442.44.962.268,16,16 7,19,1990%60%ASnmASnmSatellite1011168.0384.47176.47160.034,8,84,8,885%74%HMHM12874.846,14,1476%HM40N0.96.395,14,1548%ASnm19N684.19672.259,15,1530%HM14N118.9012.729,9,?50%HM13931.174,13,1376%HM42N0.730.875,16,1653%ASnm20N820.59721.739,15,1533%HM15N56.834.4510,8,?20%1S14256.16425.444,8,878%HM4717.7517.414,23,2347%ASnm21N615.01430.959,14,?48%1613.816.548,8,889%ASnm15282.79429.814,8,875%HM48125.49131.947,26,2680%ASnm22N929.64721.829,15,?28%HM1718.417.416,7,758%ASnm1765.79152.374,6,667%HM49N3.023.118,19,?37%1S23N965.49769.369,15,?43%19460.758,10,1092%18112.50217.764,8,874%HM500.591.034,16,1630%ASnm39N1.11.99,8,?14%1S21N342.2055.707,7,751%HMTable 1: Full results, selected domains, SATPLAN04 variable domain abstraction (best-of, see text). Notations: Index:index (number) instance respective IPC suite; IsReal: whether abstract plan real (Y) (N); L: lengthoptimal plan (? known), lg : lower bound plan length proved planning graph, la lower bound proved abstract task;ta : runtime (secs) needed prove lower bound la abstract task; tr runtime (secs) needed prove lower bound lareal task; RelFrac: fraction facts considered relevant; dashes: time-out; Abs: corresponding form variabledomain abstraction, 1Support (1S), AllSupports (AS), AllSupportsNonMutex (ASnm), handmade (HM).436fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsDomainAirportBlocksworldDepotsDining PhilosophersDriverlogFreecellGridLogisticsMprimeMysteryOptical TelegraphPipesworld NoTankagePipesworld TankagePSRRoversSatelliteZenotravelIndex2071929131223520131284881213ta33.9118.3460.86.2342.00.896.4965.56.2180.743.6521.7393.8125.574.6874.8338.4SATPLAN04trIsReal?lg , l , l r29.525,32,3211.216,20,208,10,105.57,11,11113.7N9,11,120.84,5,53.1N19,13,?769.4N9,15,155.26,6,6112.47,7,732.0N11,13,13455.58,14,14143.4N5,6,?131.97,26,2697.85,9,96,14,14244.8N4,7,7RelFrac73%47%92%71%61%82%10%43%78%76%53%86%89%80%87%76%67%Table 2: Results SATPLAN04 best-case variable domain abstractionchallenging successful instances domain. Notation Table 1.runtime better original task, yet cases abstractionbrings quite significant advantage. notably, instance number 19 SATPLAN04runs time original task, solves abstract task, finding real plan, withinminutes. Logistics, best-case data mostly, though exclusively, dueconservative AllSupportsNonMutex handmade abstractions. abstract runtimeworse three cases (nos. 10, 14, 39), slightly better. PSR, bestcases almost exclusively due conservative AllSupportsNonMutex abstraction.runtimes, abstraction usually faster, marginally. Satellite one17 domains abstraction brings significant (and largely consistent) runtimeadvantage. best cases almost exclusively due hand-made abstraction.abstract plans real plans, often found significantly faster original task.unclear us results good Satellite, but, example, Logistics,state space reduction much larger.Next, Table 2 provides overview results SATPLAN04 17 IPCdomains. make data presentation feasible, select one instance per domainthechallenging successful instance. successful, mean least one abstracttask instance solved (abstract plan found), abstract task indeedabstract (not facts relevant). challenging, mean maximum runtime originaltask.2121. Another strategy would select task maximizes tr ta , time advantage givenabstraction. However, cases strategy would select trivial instance: namely, tr taconsistently negative, maximal easiest tasks.437fiDomshlak, Hoffmann, & SabharwalDomainAirportBlocksworldDepotsDining PhilosophersDriverlogGridLogisticsMprimeMysteryOptical TelegraphPipesworld NoTankagePipesworldTankagePSRRoversSatelliteZenotravelIndex8717591822257106712ta67.93.1254.4170.41.10.10.20.60.415.70.032.20.0592.5100.3344.3tr0.30.0268.4138.00.70.21.01.00.75.20.00.60.0375.72010.7322.4IPPIsReal?NNNNNNNNlg , l , l r25,26,2616,20,206,7,77,11,117,10,1014,7,149,11,115,4,55,4,511,13,134,6,64,5,65,4,57,12,124,6,64,6,6RelFrac77%47%58%71%84%43%49%62%60%53%88%82%37%90%87%67%Table 3: Similar Table 2, IPP planner.useful discuss 17 domains groups similar behavior. Depots, Logistics,PSR, Satellite already discussed. Airport, Dining Philosophers,Driverlog, Mystery, Mprime, Optical Telegraph, Pipesworld NoTankage, PipesworldTankage, SATPLAN04 runtimes consistently lower original tasks, exceptions mostly among easiest instances. picture less consistent qualitativelysimilar Zenotravel. degree advantage varies. relatively moderateDining Philosophers (up 7% less runtime original task), Optical Telegraph (up23%), Airport (up 28%), Pipesworld Tankage (up 28%), Mprime (up 36%);much stronger Zenotravel (up 75%), Mystery (up 80%), Driverlog (up 89%),Pipesworld NoTankage (up 92%).Rovers, runtime results inconclusive, minor advantages abstractreal depending instance. Blocksworld, SATPLAN04 solves abstract tasks7 blocks only, independently abstraction used; dont know causesbad behavior. Freecell, time AllSupports AllSupportsNonMutexabstract anything, abstractions generated 1Support, SATPLAN04 runstime, leaving instance number 1 successful case, shown Table 2. Grid,finally, IPC 1998 test suite contains 5 instances, become huge quickly.SATPLAN04 solve (abstract real) instances numbers 1 2, lattershown Table 2.Tables 3 4 provide similar snapshot results IPP Mips.BDD,respectively. picture IPP is, roughly, similar SATPLAN04. maindifference is, fact, IPP weaker solver SATPLAN04 many domains,effect domains contain interesting data. Specifically, Driverlog,Mprime, Mystery, Pipesworld NoTankage, PSR, IPP either solves instancestime, all. Like SATPLAN04, see advantage abstraction Depots438fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsDomainAirportBlocksworldDepotsDining PhilosophersDriverlogFreecellGridLogisticsMprimeMysteryOptical TelegraphPipesworld NoTankagePipesworld TankagePSRRoversSatelliteZenotravelIndex2ta1.1Mips.BDDtrIsReal?lg , l , l r7,15,15RelFrac81%10112503.61.87.2N5,17,1714,7,?10,42,4284%43%43%2570.7142.613.5340.64,9,95,18,1837%86%11271.04,14,1466%Table 4: Similar Tables 23, Mips.BDD planner.Satellite, note Satellite difference consistently huge. alsosee vague advantage abstraction Logistics. Mips.BDD, even domainsgave meaningful data. domains dashed Table 4, Mips.BDD runstime even smallest instances. domains left empty, either could runMips.BDD technical reasons, stopped abnormally. remaining dataset 7 domains, however, abstractions (as expected) bring consistent advantageMips.BDD. particular, consider behavior Logistics, Rovers, Zenotravelindomains, Mips.BDD vastly improved abstraction SATPLAN04 IPPless inconclusive.3.4 Constructed Benchmarksshown use abstractionof variable domain abstraction, leastspeed state art planning systems varies quite promising Mips.BDDrather hopeless SATPLAN04. ran number focused experiments examinesubtle reasons phenomenon. experiments done threeIPC benchmarksLogistics, Rovers, Zenotravelwhere results IPC testsuites relatively bad, although possession hand-made abstractions.wanted test happens scale instances irrelevance. respectiveexperiment Logistics, Figure 1, discussed introduction. Rovers, triedlarge number instance size parameters, even minor modifications operators,could find setting contained lot irrelevance challengingSATPLAN04 IPP. short, appears Rovers domain amenableabstraction techniques. Zenotravel, obtained picture shown Figure 2.439fiDomshlak, Hoffmann, & Sabharwal10000100100001009010009080100080701007060100605010504010403013020Mips.BDD-abstractMips.BDD-realRelFrac0.1234567891011120IPP-abstractIPP-realRelFrac100120.11323456(a)78910111210013(b)1000010090100080701006050104030120SATPLAN-abstractSATPLAN-realRelFrac0.12345678910111210013(c)Figure 2: Runtime performance Mips.BDD (a), IPP (b), SATPLAN04 (c),(abstract) without (real) hand-made variable domain abstraction,Zenotravel instances explicitly scaled increase amount irrelevance.Horizontal axis scales number cities, left vertical axis shows total runtimeseconds, right vertical axis shows percentage RelFrac relevant facts.shown Zenotravel instances always feature 2 airplanes 5 persons. numbercities scales 2 13. Logistics, generated 5 random instances per size,show average values time-out 1800 seconds, stopping plots 2 timeouts occurred instance size. all, relative behavior abstract realcurves planner quite similar observed Figure 1 Logistics.SATPLAN04 IPP, abstraction slight disadvantage high RelFrac,becomes much efficient RelFrac decreases. Mips.BDD, advantage broughtabstraction much pronounced, decreasing RelFrac consistently widensgap solving abstract real tasks. average value RelFrac IPC2000 Zenotravel benchmarks 64%, lying 5 cities (67%) 6 cities (63%)Figure 2, yet much gained abstraction.440fiFriends Foes? Planning Satisfiability Abstract CNF Encodingssummary, appears planning benchmarks (like Rovers) goodabstractions, others (like Logistics Zenotravel) enough irrelevanceIPC test suites.important note situation may quite bad unsolvable examples. Consider IPC benchmarks Dining Philosophers Optical Telegraph (Edelkamp,2003). Dining Philosophers extremely basic benchmark cannot abstractedmuch further. contrast, Optical Telegraph essentially version Dining Philosopherscomplex inner life (exchanging data two hands philosopher).inner life affect existence solution (deadlock situation), depends exclusively outer interfaces philosophers, is, taking releasingforks. However, inner life does, course, affect length solution, one exists.constructed unsolvable version domain (without deadlock situation) givingphilosophers flexibility releasing forks. one would expect, settingabstracting inner life away gives huge savings, i.e., tasks proved unsolvablemuch efficiently. suggests may easier abstract unsolvable tasks,without invalidating property interest. Exploring topic future work.planning benchmark domains naturally contain unsolvable instances,over-subscription planning issue may become relevant (Sanchez & Kambhampati, 2005;Meuleau, Brafman, & Benazera, 2006).3.5 Abstractionsdiscussed earlier, one cannot expect removing preconditions, goals, entire factspreserves plan length interesting cases. are, however, certain casesdelete effects safely ignored. Specifically: Driverlog, Logistics, Mprime, Mystery,Zenotravel, one ignore deletes load unload actions stateobject longer origin location (load) respectively objectlonger inside vehicle (unload); Rovers one ignore deletes actions takingrock soil samples, namely deletes stating sample longer originlocation. ran planners respective abstracted tasks. resultssummarized follows.SATPLAN04 clear loss runtime using abstraction Driverlog (e.g.,task number 15 solved abstract vs. real 693.0 vs. 352.3 sec).IPP vast gain abstraction Logistics (e.g., 52.8 vs 5540.1 sec number 12),vast loss Zenotravel (e.g., 318.5 vs 2.5 sec number 12).Mips.BDD vast loss abstraction Driverlog, Logistics, Zenotravel (e.g.,163.8 vs. 8.3 sec Zenotravel number 8).results inconclusive planner/domain pairs.4. Resolution Complexitydiscussed introduction, surprised see little improvement SATPLAN experiments, despite dramatic state space reductions brought441fiDomshlak, Hoffmann, & Sabharwalvariable domain abstraction. shed light issue, examining resolutioncomplexity original vs. abstracted planning tasks. Throughout section,consider situation plan length boundthe number time steps CNFencodingis small, thus CNFs unsatisfiable. Note caseone SAT tests performed SATPLAN. particular, caseSAT tests SATPLAN proves optimality plan, is, non-existenceplan n 1 steps n length optimal plan. proof typicallycostly, accounting large fraction runtime taken SATPLAN.consider abstraction methods introduced Section 2.4, plus (for completeness)hypothetical abstraction method adds new initial facts. show Section 4.1 that,many cases, resolution complexity cannot improved delegating optimalityproof within abstraction. Section 4.2 show that, considered cases,resolution complexity become exponentially worse. Section 4.3 briefly examineseffect post-abstraction simplifications. sake readability, herein proofsreplaced proof sketches. full proofs available Appendix A.Recall resolution complexity defined length shortest possible resolution refutation. proofs, arguments general (unrestricted) resolution.However, constructions affect structure resolution refutations,hence results hold stated (except slight weakening case Lemma 4.14)many known variants resolution, including tree-like (DPLL), regular, orderedresolution. general, results hold variant resolution setting variables True False replacing one variable another preserves proof structure (theslightly exceptional status Lemma 4.14 explained discussresult).remainder paper, P planning task abstraction,P denote respective abstracted planning task, is, planning taskresults applying P.4.1 Resolution Complexity Become Better?prove three main results, captured Theorems 4.14.3 below. firstresult holds four SAT encodings (A)(D) listed Section 2.2; tworesults apply encodings (A) (C), respectively. respective encodingsabstraction methods, results essentially say resolution complexity cannot decreaseapplying abstraction. outlined introduction, catchy (if imprecise) intuitionbehind results over-approximations (abstractions) result less constrainedformulas, harder refute. encoding (C), result offset effortrequired recover Graphplan mutexes; get back below. theoremsfollow, recall Section 2.3 RC() denotes resolution complexity , i.e.,size smallest resolution proof unsatisfiability .Theorem 4.1. Let P planning task. Assume use encoding methods(A)(D). Let abstraction P consists combination of:(a) adding initial facts;(b) ignoring preconditions, goals, deletes;442fiFriends Foes? Planning Satisfiability Abstract CNF Encodings(c) removing fact completely.Let n length shortest plan P , let b < n. Let encodingsb-step plan existence P P , respectively. RC() RC( ).Theorem 4.2. Let P planning task. Assume use encoding method (A). Letabstraction P consists combination of:(a) adding initial facts;(b) ignoring preconditions, goals, deletes;(c) removing fact completely;(d) variable domain abstraction.Let n length shortest plan P , let b < n. Let encodingsb-step plan existence P P , respectively. RC() RC( ).Theorem 4.3. Let P planning task. Assume use encoding method (C). Letabstraction P consists combination of:(a) adding initial facts;(b) ignoring preconditions, goals, deletes;(c) removing fact completely;(d) variable domain abstraction.Let n length shortest plan P , let b < n. Let encodingsb-step plan existence P P , respectively. Let number resolution stepsrequired infer additional mutex clauses appear ,encoding b-step plan existence P per encoding (A). RC() RC( ) + .Note theorems n, defined length shortest plan P ,necessarily satisfies n length shortest plan P. Hence,n b m, satisfiable. Detecting finding P plan length bgive us information length shortest plan P. 0 b < n,however, unsatifiable, tells us b + 1 lower bound plan length P.Hence, theorems say this: either b n coarse disprove existenceplan length b; b < n decrease resolution complexitydisproofat least complexity deriving additional mutexes,case Theorem 4.3.Let us first linger bit Theorem 4.3. general intuition resultsabstractions induce less constrained formulas, hence resolution complexity cannot decrease. hold encoding (A) stated Theorem 4.2 not, strictsense (see Proposition 4.13 later section), encoding (C)? Basically, answerintuition imprecise general formulation, devil details.particular case, issue variable domain abstraction makes use mutex443fiDomshlak, Hoffmann, & Sabharwalrelations encoding (C) aware of. Sometimes, indirect mutex originaltask (omitted encoding (C)) becomes direct mutex abstraction (included encoding (C)). Refuting might involve recovering mutex, refutationneed do. Hence, potential improvement resolution complexity may stempower mutex relations. upper bound specified Theorem 4.3 showsthing improvement due to. Proposition 4.13 provides examplemutex must recovered, hence proves analogue Theorem 4.2hold encoding (C).open question whether analogue Theorem 4.2 holds encoding (B),whether analogue Theorem 4.3 holds encoding (D). discussbelow, open questions appear related intricate properties Graphplanbased encodings vs. without fact variables. know mutexes mayneed recovered also encoding (D): example provided Proposition 4.13 worksencodings (C) (D). Further, establish connection two openquestions: analogue Theorem 4.2 holds encoding (B), immediately getanalogue Theorem 4.3 holds encoding (D).consider detail. Note that, far removal goals concerned,theorems actually trivial: four encoding methods, removes partgoals, sub-formula . abstraction methods, lattercase. treat removal goals together methods since treamentcause overhead, goal clauses need discussed anyway (the setachievers goal may change).proofs, need helper notion captures over-approximated planning graphs. Assume planning task P planning graph P G(P), assumeabstraction. P G(P ) typically many vertices P G(P).captures fact P allows fewer (and often more) facts actions Pdoes. will, general, result many constraints propositional translationplanning task. constraints, may seem like abstraction could,principle, make possible derive easier/shorter proof fact plan existswithin specified bound. However, closer inspection restricted facts actionsalready available original planning graph reveals one often ends fewerweaker constraints original task. introduce notationsmake formal.Definition 4.4. planning task P abstraction it, P G (P) definedsubgraph P G(P ) induced vertices P G(P). Similarly, P Gred (P) definedsubgraph P Gred (P ) induced vertices P Gred (P).Definition 4.5. Let P planning task. abstraction called planning graphabstraction P P G (P) P G(P) identical sets vertices followingconditions hold:(1) Eadd (P G (P)) Eadd (P G(P)),(2) Epre (P G (P)) Epre (P G(P)),(3) Emutex (P G (P)) Emutex (P G(P)),444fiFriends Foes? Planning Satisfiability Abstract CNF Encodings(4) (G) G,G (G) goal states P P , respectively. abstraction calledreduced planning graph abstraction conditions hold P Gred (P) P Gred (P)instead.Lemma 4.6. Let P planning task. Assume use encoding method (A) (B). Letplanning graph abstraction P. Let n length shortest plan P ,let b < n. Let encodings b-step plan existence P P , respectively.RC() RC( ).Proof Sketch. Say set False variables appear , i.e.,fix value variables 0. way defined P G (P),yields precisely propositional encoding P G (P). show that, variablerestriction, clauses surviving also present , either strongerform (i.e., fewer literals). example, encoding (A) precondition clauseC corresponding clause C due condition (2) Definition 4.5,states introduce new preconditions. C C due condition(1) Definition 4.5, states preserves add effectshence set actionsachieving precondition P contains corresponding set P. similar observationholds effect clauses encoding (B), similar arguments applykinds clauses. claim follows Proposition 2.1.Lemma 4.7. Let P planning task. Assume use encoding method (C) (D). Letreduced planning graph abstraction P. Let n length shortest planP , let b < n. Let encodings b-step plan existence P P ,respectively. RC() RC( ).Proof. argument identical proof Lemma 4.6, except underlyingplanning graph encodings (C) (D) reduced planning graph, resulting potentially fewer mutex clauses encodings (A) (B), respectively. This, however,way affect proof arguments.Lemma 4.8. Let P planning task. Let modification P respectsfollowing behavior:(a) shrink list initial facts,(b) grow set goal facts,(c) preserves add lists unchanged,(d) grow pre del lists.planning graph abstraction P well reduced planning graph abstractionP.445fiDomshlak, Hoffmann, & SabharwalProof Sketch. proof straightforward, little tedious details. Supposeabstraction P satisfying prerequisites. must argue P G (P) P Gred (P)satisfy conditions Definition 4.5. Condition (4) involving goal states easily followsproperty (b) . P G(P) P G (P) (as well reduced counterparts)shown set vertices, conditions (1) (2) involving preconditioneffect relations follow directly properties (c) (d). hence remains provefacts actions available P G(P) also available P G (P) (showing (1)(2) said), new mutex relations created factsactions mutex P G(P) (showing (3)). proof little tedious, proceedinginductively construction planning graph. underlying intuition, however,simple: P G (P) layer abstracts P G(P) layer t, properties (a), (c)(d) respected , necessarily P G (P) layer + 1 abstracts P G(P)layer + 1. concludes argument.following immediate consequence Lemmas 4.6, 4.7, 4.8.Corollary 4.9. Let P planning task. Assume use encoding methods(A)(D). Let abstraction P consists combination of:(a) adding initial facts;(b) ignoring preconditions, goals, deletes.Let n length shortest plan P , let b < n. Let encodingsb-step plan existence P P , respectively. RC() RC( ).result essentially states rather intuitive fact that, abstraction anything yields larger planning graph, resulting Graphplan-based encodingsless constrained hence higher resolution complexity (if anything).Matters become much less intuitive consider abstractions remove entirefactsclearly, longer result over-approximated planning graphs, since remove vertices. words, condition Definition 4.5 P G (P)P G(P) identical sets vertices hold, need slightly different linereasoning rely strictly abstracted planning graphs. first showharmless remove fact appear goal pre del list.rely Corollary 4.9 reason requirement fact easily achieved.Lemma 4.10. Let P planning task. Assume use encoding methods(A)(D). Let p fact appear goal pre del lists,let abstraction P removes p initial facts add lists. Letn length shortest plan P , let b < n. Let encodingsb-step plan existence P P , respectively. RC() = RC( ).Proof Sketch. key point that, p appear goal never requireddeleted action, p completely irrelevant planning task, particularresolution refutations consider here. Concretely, first prove every layerplanning graph, available facts mutex fact pairs remain same,facts fact pairs involving p. is, thing lost fact layers446fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsP G (P) p. Since p occur precondition, action layers remain exactlysame; since p occur preconditions delete effects, action mutexesalso remain exactly (they caused p).discussion implies precondition clauses encodingsidentical. Given p appear goal, true goal clauses.Since action mutexes unchanged, follows actually identicalencodings (A) (C). encodings (B) (D), differencecontain initial state, effect, mutex clauses involving p.However, clauses never participate resolution refutation : effectmutex clauses contain p polarity (negative); initial state clausepositive form {p(0)}, time index different p effect mutexclauses. Hence every variable corresponding p occurs one polarity. concludesargument.Corollary 4.11. Let P planning task. Assume use encoding methods(A)(D). Let abstraction P removes fact completely. Let n lengthshortest plan P , let b < n. Let encodings b-step plan existenceP P , respectively. RC() RC( ).Proof. equivalent following two steps. First, remove p goal facts (ifpresent) pre del lists. Corollary 4.9, step cannot improve resolutioncomplexity. Second, p removed goal pre del lists,remove p problem completely removing initial facts add listswell. Lemma 4.10, step well cannot improve resolution complexity,done.Corollaries 4.9 4.11 together prove first main result, Theorem 4.1.move variable domain abstraction, matters complicated,interesting abstraction method enables us construct solution lengthpreserving abstractions exponentially smaller state spaces, many benchmarks. Firstshow that, original form, result holds encoding (A).Lemma 4.12. Let P planning task. Assume use encoding method (A). Letvariable domain abstraction P. Let n length shortest plan P , letb < n. Let encodings b-step plan existence P P , respectively.RC() RC( ).Proof Sketch. combines two persistently mutex facts p p0 single fact p. firstshow action pair (a, a0 ) mutex P , also mutex P.way (a, a0 ) become mutex per requires, w.l.o.g., P, p del (a) pre(a)p0 pre(a0 ) add (a0 ). Supposing (a, a0 ) mutex P, p 6 del (a0 ) pmutex fact pre(a0 ). then, (noop(p), a0 ) mutex P hence(p, p0 ) persistent mutex, contradiction.hand, derive property rather similar planning graphabstractions given Definition 4.5. above, know abstract encodingmutexes appear . Further, set actions achievingfact grows applying abstraction, goal shrink.447fiDomshlak, Hoffmann, & Sabharwalsubtle issue regards precondition clauses. action p0 precondition P,replaced p P , direct correspondence two. However,lack correspondence affect precondition clause encoding (A),takes form {a, a1 , . . . , ak }; omits actual precondition fact achieved,matter whether fact p0 p.Next, proof Lemma 4.6, set variables False appear. arguments, difficult see clauses survivingalso present , either stronger form (i.e., fewer literals).mutex clauses, obvious. goal clauses, argument exactlyproof sketch Lemma 4.6 given above. precondition clauses, observe a1 , . . . , akcontain achievers p0 plus achievers p. claim followsProposition 2.1.Corollaries 4.9 4.11 together Lemma 4.12 prove second main result, Theorem 4.2. encodings (B)(D), matters complicated.Consider first encodings (B) (D), differ (A) alsofact variables. changes precondition clauses. action P p0precondition, p P , longer get clause {a, a1 , . . . , ak } proofsketch. Instead, get clause {a, p}. clause, correspondence .particular, consider case two actions P, action preconditionp action a0 precondition p0 . gives us clauses {a, p}, {a0 , p0 }clauses {a, p}, {a0 , p} . Now, distinguish achieversp p0 , problem regard. fact twoclauses share literalwhich dont exploited obtain shorter resolutionrefutations? open question; discuss implications little detailend sub-section.Consider encoding (C), differs (A) includes direct actionmutexes. invalidates different argument proof Lemma 4.12. still truethat, action pair (a, a0 ) marked mutex P , also mutex P.However, happen (a, a0 ) mutex P due direct interferencea0 , (a, a0 ) mutex P due mutex preconditions, rather directinterference. Since encoding (C) accounts direct interferences,mutex appear . result improved resolution complexity. following proposition proves formally.Proposition 4.13. Assume use encoding method (C). exist planning task P,variable domain abstraction P, b < n RC() > RC( ), nlength shortest plan P , encodings b-step plan existenceP P , respectively.Proof Sketch. construct P, , b specified. key property constructiontwo actions, getg1 getg2 , needed achieve goal factsg1 g2 , respectively. precisely, getg1 = ({x}, {g1 , p0 }, {x}) getg2 = ({p, y},{g2 }, {p}). task constructed, along help actions, wayx, p, p0 pairwise persistently mutex. variable domain abstraction replacesp0 p, b set 2. action layer directly beneath goal layer, i.e., action448fiFriends Foes? Planning Satisfiability Abstract CNF Encodingslayer A(1), planning graph marks getg1 getg2 mutex preconditionsmutex. Encoding (C), however, include mutex clausedirect conflict. situation changes abstraction. getg1 adds p instead p0 ,hence direct conflict delete effect getg2 . consequence,abstraction two resolution steps suffice: applying getg1 getg2 A(1)option achieve goals, new mutex clause immediately excludes option.encoding original task, required mutex must firstderived reasoning preconditions x p.Note reason get shorter refutation variable domain abstraction turns indirect action mutex (due competing preconditions)direct interference. so, abstraction exploits knowledge p p0persistently mutexa fact ignored encoding (C). Hence positive result statedProposition 4.13 less related power abstraction power planninggraph mutexes. capture formally. follows, note usingplan length bound, CNF formula per encoding (C) sub-formula CNFformula per encoding (A), additional clauses (A) inferred it.Lemma 4.14. Let P planning task. Let variable domain abstraction P.Let n length shortest plan P , let b < n. Let Cencodings b-step plan existence P per encoding (A) (C), respectively. Let Cencoding b-step plan existence P per encoding (C). Let numberresolution steps required infer C additional mutex clauses appear .RC(C ) RC(C ) + .Proof. Denote encoding b-step plan existence P per encoding (A).have:(1) preconditions lemma, RC(C ) RC(A ) + : resolution steps,C turned , hence shortest resolution refutationconstruct one C steps longer.(2) Lemma 4.12, RC(A ) RC(A ).(3) C sub-formula , hence RC(A ) RC(C ).Combining observations, have:RC(C ) RC(A ) +observation (1)RC(A ) +RC(C ) +observation (2)observation (3)finishes proof.Clearly, proof argument applies also combination variable domainabstraction abstractions. Hence Corollaries 4.9 4.11 togetherLemmas 4.12 4.14 prove third main result, Theorem 4.3. Note latterresult hold variants resolution. claim Lemma 4.14,449fiDomshlak, Hoffmann, & Sabharwalnumber resolution steps takes derive action mutexes present originalencoding. used resolution refutation. variant resolutionconsideration is, say, DPLL tree-like resolution, deriving mutex clauseenoughit must re-derived many times used tree-like resolutionrefutation. Hence effective value variants resolution would larger.Note case DPLL solver learns mutex clauses virtuewide-spread clause learning technique.Lemma 4.14 particularly relevant empirical results, SATPLAN04 usesencoding (C) experiments mostly focus variable domain abstraction.explicit empirical proof (and proof would difficult come by, requiringdeep analysis SAT solvers search spaces), seems reasonable assume that,least extent, disappointing results SATPLAN04 due whats provedLemma 4.14. abstraction cannot improve resolution complexity beyond effortrequired recover indirect action mutexes. Note bound givenlemma rather pessimistic. mutex (a, a0 ) needs recovered casea0 competing needs P, replacing p0 p results directinterference incur simplifications. Logistics domain, example,abstraction happens actions loading package onto airplane twodifferent irrelevant cities. Since load actions involved redundantsolutions anyway, seems doubtful mutexes play role resolution complexity.generally, interesting consider upper bounds Lemma 4.14.many resolution steps take recover indirect action mutexes? general resolution, number steps polynomially bounded, since inference process conductedplanning graph simulated (for related investigation, see Brafman, 2001).restricted variants resolution, matters complicated. particular interestbehavior DPLL+clause learning, c.f. above. far known formula DPLL+clause learning proofs provably substantially worse generalresolution proofs; would rather surprising planning graph mutexesfirst. Also, Rintanen (2008) provides related investigation, showing mutexesrecovered polynomial time particular 2-step lookahead procedure, related(but identical) clause learning.Concluding sub-section, let us turn attention encodings (B) (D).mentioned before, open question whether analogue Theorem 4.2 holdsencoding (B), whether analogue Theorem 4.3 holds encoding (D).facing two problematic issues:(I) Fact variables. encoding (B) (D), fact variables additionaction variables used encodings (A) (C).(II) Mutexes. Like encoding (C), may happen variable domain abstractionconverts implicit mutex encoding (D) original task explicit oneabstraction.consider first issue (II). situation exactly encoding (C), regard.Proposition 4.13 holds stated encoding (D) well; indeed proved using450fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsexactly example minor adaptations proof arguments.22 Similarly, Lemma 4.14 holds encoding (D), exactly proof argumentsprovidedanalogue Lemma 4.12 (and hence analogue Theorem 4.2) holds encoding (B).Namely, proof arguments Lemma 4.14 remain valid, except need referencoding (B) rather (A), accordingly need corresponding versionLemma 4.12. brings us issue (I).Variable domain abstraction perceived gluing sets facts together. Recallexample clauses {a, p}, {a0 , p0 } sharing literals, clauses{a, p}, {a0 , p} sharing literal p; discussed explainproof Lemma 4.12 work encodings (B) (D). k + 1 facts gluedtogether, groups k clauses become linked together fashion. questionis:(*) resolution fruitfully exploit increased linkage?issue appears related quite intricate properties Graphplan-basedencodings vs. without fact variables. encoding (A), differs encoding(B) use fact variables, Lemma 4.12 tells us resolution cannotexploit variable domain abstraction. Now, appears reasonable think adding factvariables help lot, intuition being:(**) Whatever one encoding (B), one easily simulate encoding (A).statement (**) true, answer question (*) no, yesanswer would contradict Lemma 4.12. Hence, initial attempt prove answer,tried prove statement (**). However, initial investigation indicatedexplicitly keeping fact variables (and non-trivial constraints them) around, encoding (B)might facilitate significantly shorter resolution derivations general, hence statement(**) might false. Namely, appear families formulas suitablyencoded planning tasks yield exponential separation encodings (A)(B). true, suggests reasoning presence fact variables mightpowerful hence might indeed able exploit linkage gain yieldedvariable domain abstraction.Since purpose paper compare relative power variousGraphplan-based encodings (such (A) (B)), detail progresstowards disproving statement (**). Besides, note that, statement (**) indeed false,immediate implications answer question (*). definiteanswer (*) left open future research.4.2 Resolution Complexity Become Worse?answer title sub-section definite yes. four encodings,abstractions consider may exponentially deteriorate resolution complexity.Formally, following theorem.22. include full proof (C) (D) Appendix A.451fiDomshlak, Hoffmann, & SabharwalTheorem 4.15. Assume use encoding methods (A)(D). existinfinite sequence planning tasks P(i), abstractions (i) P(i), b(i) < n(i)RC( (i)) exponential RC((i)) constant independent i, n(i)length shortest plan P (i), (i) (i) encodings b(i)-step planexistence P(i) P (i) respectively, (i) consists one of:(a) adding initial facts;(b) ignoring preconditions, goals, deletes;(c) removing fact completely;(d) variable domain abstraction.Proof Sketch. idea construct P(i) planning task consists two separatesub-tasks, whose overall goal achieve goals sub-tasks.sub-tasks infeasible within given plan length bound b(i). (Thetasks bounds constructed size grows polynomially i.) However,first sub-task constructed require exponential size resolution refutations,second allows constant size refutations. abstraction over-approximates easy-torefute sub-task way becomes feasible within b(i) steps, resolutionrefutation overall task must rely hard-to-refute sub-task. leadsexponential growth, i, resolution complexity (i), opposed constant resolution complexity (i). single one listed abstractions, feasibilityeasy-to-refute sub-task accomplished simple manner, hence proving theorem.order construct planning tasks whose CNF encodings require exponential sizeresolution refutations, resort pigeon hole problem formula PHP(i). wellknown resolution proof PHP(i) must size exponential (Haken, 1985).construct simple pigeon hole planning task PP HP (i) capture problem.show that, four encoding methods (A)(D), CNF encoding b(i) = 1either identical PHP(i), transforms PHP(i) variable restrictions. Hence,Proposition 2.1, resolution refutation must size exponential i. finalconstruction uses combination two tasks: PP HP (i) serves hard-to-refutesub-task, PP HP (1) disjoint sets pigeon hole objects serves easy-to-refutesub-task.Essentially, Theorem 4.15 states intuitive fact abstractions make badchoices, approximating away concise reason planning task cannotsolved particular number steps. illustrate significance this, considercomparison mutex relations. analogue Theorem 4.15 holdthem: adding mutex clause CNF encoding improve resolution complexity.sense, mutex relations considerably less risky abstractions considerhere.pigeon hole problem used proof Theorem 4.15 may seem artificial,indeed contained sub-problem wide-spread domains concernedtransportation. example, Gripper, available time steps serve holesactions picking/dropping balls pigeons (for related investigation, see452fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsHoffmann et al., 2007). also seems quite natural planning task may consist twodisconnected parts, one complex one easy prove unsolvablegiven number steps. think transporting two packages, oneclose many vehicles requires one step bound allows,one already inside vehicle needs transported along single path roadconnections much longer bound (a concrete example latter situationformalized Hoffmann et al., 2007).4.3 Note Simplificationspointed Section 2.4, may actions abstraction obviouslysimplified without altering semantics planning task. particular, abstractionmight create duplicate actions (that is, actions identical preconditions effects),well redundant add effects (that contained respective actions precondition).turns natural post-abstraction simplification planning task leadlower resolution complexity.Proposition 4.16. Assume use encoding methods (A)(D). existplanning task P, planning task P 0 identical P except either irrelevantadd affect duplicate action removed, b < n RC() > RC(0 ),n length shortest plan P, 0 encodings b-step planexistence P P 0 , respectively.Proof Sketch. show claim duplicate actions, consider task P 0 encodingpigeon hole problem 3 pigeons 2 holes, actions put pigeon p hole h.plan length bound 1. point proof works solvable tasks P,extra action a, whose preconditions two goals achieves third goal,ensures solvability two steps. three goals, one pigeon,mutex clauses direct action interference, pair goalsachieved three them. particular, every resolution refutation must resolvethree goal clauses. obtain P adding duplicate action one pigeonsone holes. respective goal clause becomes one literal longer.refutation must get rid literal, hence necessitating one step. constructionworks four encodings.show claim removal redundant add effects, slightly modify P 0 , replacingeffect new fact x including another action achieves third goal givenprecondition x. optimal plan length 3, length bound 2.refutation must resolve three goal clauses. If, P, give one preconditionsadditional add effect, refutations become longer respective goalclause does. Again, construction works four encodings.easy modify constructions used proof Proposition 4.16 wayduplicate action, respectively redundancy add effect, ariseoutcome variable domain abstraction. Hence, via enabling simplifications, variabledomain abstraction may improve resolution best-case behavior. duplicate actions,also true Edelkamps (2001) pattern database abstraction. open question453fiDomshlak, Hoffmann, & Sabharwalwhether improvement may exponential, whether bounded polynomially.conjecture latter case, least unrestricted resolution.also notable examples proof Proposition 4.16 specificallyconstructed include duplicate actions/redundant add effects actions relevantsolving problem form part optimal solution. well-constructed variabledomain abstraction likely happen since abstraction target factsirrelevant solution length. Consider Logistics domain example.actions simplifications apply loads/unloads packages to/from locationscities packages origin destination. actions involvedredundant solutions, seems doubtful simplification affects resolutioncomplexity. course, simplifications might help SAT solvers anyway. This, however,observed, least significantly, experiments.5. ConclusionAbstractions, used here, power allow proving certain properties within muchsmaller state spaces. particular, abstraction preserves length optimalsolution, optimality proved within abstraction. designed novel abstraction method STRIPS planning suitable purpose. Surprisingly,approach yields little benefits planning-as-satisfiability approach represented SATPLAN, even domains featuring hand-made abstractions exponentially smaller state spaces. Towards explaining this, shown that, many cases,abstraction method (as well commonly used abstractions) lacks ability introduce shorter resolution refutationsother exploiting mutexes,enabling certain post-abstraction simplifications. contrast, shownabstractions may exponentially increase size shortest resolution refutations.Several questions left open theoretical results. know whether variable domain abstraction improve resolution complexity combination encoding(B), whether polynomial upper bound improvement variable domainabstraction bring encoding (D), whether polynomial upper boundimprovement result simplifications. Apart answering questions, importantly remains seen extent results generalize. Bluntlystated, intuition behind results over-approximations usually result lessconstrained formulas harder refute. However, actual technicalitiesresults depend quite lot detailsof encoding method abstractionhence largely unclear extent intuitive statement captures reality.particular: hold encodings planning SAT?would interesting, e.g., look alternative encodings described KautzSelman (1992), Kautz et al. (1996), Kautz Selman (1996), Ernst, Millstein,Weld (1997). Many encodings based unit clauses initial goal state,action clauses stating action implies effects preconditions.structureand lack mechanism planning graph propagateschangessome properties proved herein obvious. Removing goals initial statefacts corresponds directly removing clauses; true preconditions. Removingfact completely may cases simply correspond removing clauses mention454fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsfact. Hence, encodings, seems proving results might indeedcomparatively easy. challenging subject may recent developments,encodings Rintanen et al. (2006) often give substantial speed-upsnovel notions parallelity, encodings Chen et al. (2009) introducelong-distance mutex relations, encodings Robinson et al. (2008) make useeffective operator splitting factoring methods.generally: results hold methods employed fields? particular,hold model-checking, abstraction (e.g., Graf & Sadi, 1997; Clarkeet al., 2003) SAT encodings (e.g., Clarke, Biere, Raimi, & Zhu, 2001; Prasad, Biere,& Gupta, 2005) highly successful? one example actually obviousresults hold. Gupta Strichman (2005) abstract ignoring clauses CNFencoding original transition system (the motivation much smaller CNFformula causes less overhead SAT solver).ambitiously: define generic framework formal notions declarativetransition systems, CNF encodings, abstractions suitable captureresults, prove generic statements? questions appear worthwhileresearch challenges. Indeed, think key contribution work may lie askingquestion resolution complexity vs. without abstraction.practical perspective, see mainly four lines research. First,important question whether observations carry modified/extended planningas-SAT systems, one Ray Ginsberg (2008) guarantees planoptimality branching restrictions within single SAT call, rathercalling SAT solver iteratively. Second, remains open explore whether differentabstraction techniquesbased e.g. predicate abstractioncan suitably adaptedplanning. Third, important note empirical results entirely negative.Mips.BDD often substantially improved, even point where, Figure 1,optimal sequential planner highly competitive strong optimal parallel plannerSATPLAN, highly parallel domain Logistics. directionmay well worth exploring depth. Finally, effective abstraction methods mayexist unsolvable examples, could potentially play crucial role over-subscriptionplanning (Sanchez & Kambhampati, 2005; Meuleau et al., 2006).Acknowledgmentsthank anonymous reviewers, whose detailed comments helped greatly improvepaper. preliminary version work appeared ICAPS06, 16th InternationalConference Automated Planning Scheduling (Hoffmann, Sabharwal, & Domshlak,2006). work Carmel Domshlak supported Israel Science Foundation (ISF)Grants 2008100 2009580, well C. Wellner Research Fund. partwork, Jorg Hoffmann employed Max Planck Institute Computer Science,Saarbrucken, Germany, SAP Research, Karlsruhe, Germany. work AshishSabharwal supported IISI, Cornell University (AFOSR Grant FA9550-04-1-0151),National Science Foundation (NSF) Expeditions Computing award (Computational455fiDomshlak, Hoffmann, & SabharwalSustainability Grant 0832782), NSF IIS award (Grant 0514429), Defense AdvancedResearch Projects Agency (DARPA, REAL Grant FA8750-04-2-0216).Appendix A. Proof DetailsProof Proposition 2.1. Suppose sequence transformations consists ` restrictions, 1 , 2 , . . . , ` . Further, let 0 strengthening transformation replacesclause | (not necessarily strict) sub-clause also clause ;transformation exists assumptions proposition. Observe` + 1 transformation steps together convert (not necessarily strict) sub-formula. show ` + 1 transformation steps individually increaseresolution complexity underlying formula. Without loss generality,prove fact single restriction transformation generic strengtheningtransformation. Since individually shown increase resolutioncomplexity formula, applied sequence combination, numbertimes, without increasing resolution complexity. would prove, particular,resolution complexity sub-formula , implyingresolution complexity (as additional initial clausescannot hurt resolution refutation), desired.start single restriction transformation x y. ease notation,assume initial formula F = {C1 , C2 , . . . , Cm } resulting simplified0 }, m0 m. Without lossformula transformation F 0 = {C10 , C20 , . . . , Cm00generality, assume Ci equals empty clause {} duplicateclauses F 0 . Let = (C1 , C2 , . . . , Cm , Cm+1 , . . . , CM = {}) resolution refutationF smallest possible size; note involves initial clauses resolutionsteps. , construct resolution refutation 0 F 0 size larger. following three steps.Step 1. Transform = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), Ci definedfollows. application transformation x results Ci containing Truevariable negation, Ci equals True; results Ci containingFalse duplicate literals, Ci consists Ci False duplicate literal removed;otherwise Ci = Ci . Note Ci contain x either True, emptyclause, non-empty (not necessarily strict) sub-clause C. key propertyCi still logical implicant Cj Ck Ci derived resolvingCj Ck original proof . Ci may necessarily usual resolutionresolvent Cj Ck , next two steps fix.Step 2. Transform = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), Ci equals Ci> defined sequentially, increasing i, follows. SupposeCi derived resolving clauses Cj Ck , j < k < i. Assumewithout loss generality already defined Cj Ck . Ci equalsTrue, Ci equals True well; otherwise, one two clauses Cj Ckequals True, Ci equals clause; otherwise, Cj Ck resolvedtogether variable Ci resolvent two clauses. keyproperty here, seen easily considering sequential nature456fiFriends Foes? Planning Satisfiability Abstract CNF Encodingstransformation, Ci either True (not necessarily proper) subclause Ci ; particular, CM = {}. Further, Ci equal Trueeither resolution resolvent Cj Ck , equals {} one Cj Ckalso {}.Step 3. Finally, transform 0 simply removing clauses equal Trueoccur previously clause sequence, stopping sequence soonfirst empty clause encountered. construction, , , exactlyclauses 0 clauses. Further, first m0 clauses0 exactly clauses F 0 0 resolution refutation startinginitial clauses, desired.consider strengthening transformation 0 , essentially replacesclause formula sub-clause (thereby strengthening clause). showapplying 0 increase resolution complexity underlying formula.Again, ease notation, let initial formula F = {C1 , C2 , . . . , Cm } resolutionrefutation = (C1 , C2 , . . . , Cm , Cm+1 , . . . , CM = {}) smallest possible size.argument along lines simpler restriction transformations.Transform = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), m, Ci equalssub-clause Ci 0 maps Ci to, > m, Ci defined sequentially, increasingi, follows. Suppose Ci derived resolving clauses Cj Ck variable x,j < k < i. Assume without loss generality already defined CjCk . x present Cj Ck , Ci simply resolution resolventtwo clauses; otherwise, x present Cj , Ci equals Cj ; otherwise x mustpresent Ck set Ci equal Ck . key property here, seeneasily considering sequential nature transformation, Ci (notnecessarily proper) sub-clause Ci ; particular, CM = {}. Further, Ci either CjCk resolution resolvent two. transform 0 simply removingclauses occur previously clause sequence. construction,exactly clauses 0 clauses. Further, first m0 clauses0 exactly clauses F 0 0 resolution refutation starting initialclauses, desired.Proof Lemma 4.6. Let P planning task abstraction abstractsP G(P) applied. Let , denote propositional encodings P P , respectively,use either action-only encoding (A) action-fact encoding (B). Let U denote set variables variables . Finally,let variable restriction sets every variable U False. setting,propositional formula | nothing CNF encoding planning graph P G (P)(using encoding method, (A) (B), used ). particular,clauses correspond actions facts P G(P) trivially satisfied, contain negation variable U , set False . Callremaining, yet unsatisfied clauses | surviving clauses. arguesurviving clause already present, perhaps stronger weaker form,itself, showing easier prove | unsatisfiable proveunsatisfiable.457fiDomshlak, Hoffmann, & SabharwalFirst consider encoding (A). conditions (2) (4) Definition 4.5, every survivingprecondition goal clause | corresponding clause itself. Further,condition (1) concerning facts added action, preconditiongoal clause | contains sub-clause corresponding clause . Finally,surviving mutex clauses | , condition (3), also present mutex clause .observations, follows every surviving clause | contains (possiblynon-strict) sub-clause corresponding clause . Applying Proposition 2.1, obtainRC() RC( ), finishing proof encoding (A).consider encoding (B). First, Definition 4.5 P G (P) P G(P)identical sets vertices, particular fact vertices F (0) same, hencesurviving initial state clause | also present initial state clause . Further, precondition clauses binary and, condition (2) Definition 4.5, survivingprecondition clause | also present precondition clause . Similarly,goal clause unit clause and, condition (4), surviving goal clause | alsopresent goal clause . similar vein, surviving mutex clause | alsopresent mutex clause . Finally, surviving effect clause | , condition(1), contains sub-clause corresponding effect clause | . Hence seeevery surviving clause | contains (possibly non-strict) sub-clause corresponding clause . Applying Proposition 2.1 before, obtain RC() RC( ),finishing proof encoding (B).Proof Lemma 4.8. Let P planning task abstraction respectsbehavior specified lemma applied. show four conditionsDefinition 4.5 hold P G (P) P Gred (P). Observe condition (4)Definition 4.5 trivially holds property (b) . therefore focus showingV (P G(P)) = V (P G (P)), V (P Gred (P)) = V (P Gred (P)), conditions (1)(3)Definition 4.5 hold. fact, show P G (P) P Gred (P) setvertices original (reduced) planning graph, conditions (1) (2) Definition 4.5would immediately satisfied due properties (c)23 (d) , wouldremain would condition (3), saying new mutex clauses added applying .Hence, task reduced proving following four new properties holdstep 0, 1, . . . , b planning task:(i) F (t) F (t),(ii) (t) A(t),(iii) Ef-mutex (t)|F (t) Ef -mutex (t),(iv) Ea-mutex(t)|A(t) Ea-mutex (t).F (t), A(t), Ef -mutex (t), Ea-mutex (t) denote sets facts, actions (includingnoops), fact mutexes, action mutexes generated P step t, -versionsdenote corresponding sets P . F (t), Ef-mutex (t)| denotes {(f1 , f2 )23. one needs add lists shrink applying . However, another argumentshortly require add lists grow either, justifying strict requirement property (c).458fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsEf-mutex (t) | f1 , f2 }, subset Ef-mutex (t) restricted facts . Ea-mutex(t)|defined similarly (t).order prove four properties hold, give inductive argument t,alternating F (t) Ef -mutex (t) one hand, A(t) Ea-mutex (t)other. base case = 0, note F (0) F (0) property (a)Ef-mutex (0)|F (0) Ef -mutex (0) sets empty.words, goal prove certain facts actions available Pcertain step, remain available P . Similarly, two facts actions mutuallycompatible P, remain mutually compatible P . seems intuitively justifiablegiven properties . following argument formalizes intuition. termsnotation, use pre, del , add specify actions P pre , del , addspecify actions P .first part inductive step, suppose F (t) F (t) Ef-mutex (t)|F (t)Ef -mutex (t). show (t) A(t) Ea-mutex(t)|A(t) Ea-mutex (t), inductivelyproving conditions (ii) (iv).Let A(t). pre (a) F (t) F (t). Further, f, f 0 pre (a) pre(a)F (t), (f, f 0 ) 6 Ef -mutex (t) Ef-mutex (t)|F (t) hence (f, f 0 ) 6 Ef-mutex (t).Therefore (t), proving (t) A(t).let a, a0 A(t) (a, a0 ) 6 Ea-mutex (t). reduced planning graph(t) due propertiesdirect mutexes, immediately (a, a0 ) 6 Ea-mutex(c) (d) , done proving Ea-mutex (t)|A(t) Ea-mutex (t). Otherwise,general mutexes, several things hold. First, pre (a) pre(a) F (t) F (t),a0 . Likewise, del (a) del (a) del (a0 ) del (a0 ). Finally,property (c), add (a) add (a). Hence (c.1) (pre (a) add (a)) del (a0 )(pre(a)add (a))del (a0 ) = ; last equality holds (a, a0 ) 6 Ea-mutex (t). Similarly,(c.2) (pre (a0 )add (a0 ))del (a) = . Finally, f pre (a) pre(a) F (t) F (t)f 0 pre (a0 ) pre(a0 ) F (t) F (t), (f, f 0 ) 6 Ef -mutex (t)Ef-mutex (t)|F (t) , implies (c.3) (f, f 0 ) 6 Ef-mutex (t). (c.1), (c.2), (c.3),(a, a0 ) 6 Ea-mutex(t), proving Ea-mutex(t)|A(t) Ea-mutex (t).second part inductive step, suppose (t) A(t) Ea-mutex(t)|A(t)Ea-mutex (t). show F (t+1) F (t+1) Ef -mutex (t+1)|F (t+1) Ef -mutex (t+1), proving conditions (i) (iii).Let f F (t + 1). f aA(t) add (a) aA (t) add (a). (Recall noopactions included A(t), need explicitly include F (t) F (t + 1).)follows f F (t + 1), proving F (t + 1) F (t + 1).let f, f 0 F (t + 1) (f, f 0 ) 6 Ef -mutex (t + 1). must exist0a, A(t) (t) (c.1) f add (a) add (a), (c.2) f 0 add (a0 ) add (a),(a, a0 ) 6 Ea-mutex (t) Ea-mutex(t)|A(t) , implies (c.3) (a, a0 ) 6 Ea-mutex(t).0(c.1), (c.2), (c.3), (f, f ) 6 Ef -mutex (t + 1), proving Ef -mutex (t +1)|F (t+1) Ef -mutex (t + 1).finishes inductive argument, showing conditions (i)(iv) outlinedhold. earlier reasoning, proves vertices P G(P) P G (P),well reduced counterparts, (so conditions (1) (2)Definition 4.5 follow directly properties (c) (d) ) mutex relationsP red (P), restricted facts actions P, subset mutex relations459fiDomshlak, Hoffmann, & Sabharwalreduced mutex relations, respectively, P (so condition (3) Definition 4.5holds). Hence abstracts planning graph well reduced planning graphP.Proof Lemma 4.10. Let abstraction removes p initial factsadd lists given planning task P p appear goal factspre del lists. proof Lemma 4.6, let , denote propositionalencodings P P , respectively, use one encodings (A), (B), (C),(D) . show encodings (A) (C), factidentical, encodings (B) (D), differ clauses cannot partresolution proof.end, use planning graph notation proof Lemma 4.8begin arguing induction F (t) = F (t) \ {p}, Ef-mutex (t) = Ef -mutex (t) \ {(p, p0 ) |p0 F (t)}, (t) = A(t), Ea-mutex(t) = Ea-mutex (t). base case = 0,F (0) = F (0)\{p} definition , Ef-mutex (t) = Ef -mutex (0)\{(p, p0 ) | p0 F (0)}sets empty. first part induction, supposeinductive conditions F Ef -mutex hold time step t. Since p pre list,implies (t) = A(t) well. Further, since p del list, alsoEa-mutex(t) = Ea-mutex (t). Hence conditions Ea-mutex hold time stept. second part induction, suppose inductive conditionsEa-mutex hold time step t. implies F (t + 1) consists F (t + 1) possiblyp. Further, Ef -mutex (t + 1) Ef-mutex (t + 1) far mutexes involvingp concerned. follows conditions F Ef -mutex hold time step + 1,finishing induction.summarize, shown every step, sets A, Ea-mutex , Ea-mutexexactly same, sets F, F Ef -mutex , Ef-mutex factspairs facts involving p. words, new actions facts become available mutually excluded planning graph , everything involving p remainsunchanged. Given this, observe four encodings, goal precondition clauses P exactly P p appear goalpre lists all. Similarly, Ea-mutex(t) = Ea-mutex (t) Ef-mutex (t) = Ef -mutex (t)implies action mutexes P , fact mutexes present encoding,P well. Therefore, encodings (A) (C), = .Finally, encodings (B) (D), initial state, effect, mutex clausesget removed applying , i.e., present . However,clauses mention propositional variables corresponding p,variables appears one polarity throughout . Namely, initial state clause{p(0)} clause may contain p positive polarity; effect mutex clausescontain p time index > 0. that, clauses cannot partresolution refutation every variable appearing resolution refutation musteventually resolved away order derive empty clause. followsrespect resolution refutations.Proof Lemma 4.12. Let P planning task , variable domain abstraction,applied. combines two persistently mutex facts p p0 single fact p.brevity, let G denote P G(P). Define G 0 graph obtained unifying p460fiFriends Foes? Planning Satisfiability Abstract CNF Encodingsp0 fact vertices fact layer G single vertex p layer, similarlynoop(p) noop(p0 ) vertices action layers. Finally, let G denote subgraphP G(P ) induced vertices G 0 . show G abstracted planninggraph sense similar Definition 4.5.begin arguing action pair (a, a0 ) mutex P , also mutexP. see this, observe way (a, a0 ) become mutex per requires,w.l.o.g., P, p del (a) pre(a) p0 pre(a0 ) add (a0 ). Suppose sakecontradiction (a, a0 ) already mutex P. particular, meansp 6 del (a0 ) p mutex fact pre(a0 ). This, however, implies(noop(p), a0 ) mutex P fact pair (p, p0 ) mutex next layerP G(P), contradiction p p0 persistently mutex. follows edgesEa-mutex G subset G.Since G 0 P initial facts, argument implies actionsfacts available layer G 0 also available layer P G(P ). particular,G , construction, exactly set vertices G 0 . Further, since variabledomain abstraction, edges Eadd Epre G 0 G exactly same.Define , , U, proof Lemma 4.6. | CNF encoding (A)planning graph G , clauses corresponding actions factsG trivially satisfied . Call remaining clauses | surviving clausesbefore.observation edges Ea-mutex G G, surviving mutex clauses| also mutex clauses . surviving precondition goal clauses involvingp appear unchanged . Since considering variable domain abstraction,actions achieving p G precisely actions achieving either p p0 G. Hence,surviving precondition goal clauses | involving p contain sub-clauseprecondition goal clause itself. follows Proposition 2.1 RC()RC( ).Proof Proposition 4.13. example planning task, denoted P, worksencoding (C) encoding (D). Let denote variable domain abstraction applied.example uses following six facts: facts p p0 , glued together ;goal facts g1 g2 ; helper facts x, y. Based facts, task P definedfollows:Initial state {p}; Goal {g1 , g2 }Action set containing five actions:getx = ({p}, {x}, {p}),gety = (, {y}, ),getg1 = ({x}, {g1 , p0 }, {x}),getg2 = ({p, y}, {g2 }, {p}),getp = ({p0 }, {p}, {p0 }).plan length bound 2, makes problem infeasible: shortest (parallel)plan requires 4 steps: h{getx, gety}, {getg1 }, {getp}, {getg2 }i. Observe pairsactionsexcept (getg1 , getg2 ) pair involves getydirectly interfere461fiDomshlak, Hoffmann, & Sabharwaltherefore mutex. P G(P), get following fact action setsstep 2:F (0) = {p}, A(0) = {noop(p), getx, gety}F (1) = {p, x, y}, A(1) = {noop(p), noop(x), noop(y), getx, gety, getg1 , getg2 }F (2) = {p, x, y, g2 , g1 , p0 }easy verify, iteratively, that, P G(P): p x mutex F (1); p xmutex F (2); p p0 mutex F (2); x p0 mutex F (2); mutexesget also F (3), planning graph reaches fixpoint. particular,getg1 getg2 always (indirectly) mutex preconditions x ppersistently mutex. variable domain abstraction glue p p0 , convertingconflict getg1 getg2 direct interference, thereby allowing shorterresolution refutation.Consider encoding (C) P G(P). contains two goal clauses: {getg1 (1)}{getg2 (1)}. clauses clearly must used resolution refutation formula, possible achieve goal individually within given time bound,together. Hence, shortest refutation must involve least two steps.argue shortest refutation achieved abstracted task PP itself.P , get fact action sets planning graph, except F (2) ={p, x, y, g1 , g2 }, i.e., p0 course present, A(0) A(1) contain also getpacts similarly noop(p). corresponding encoding (C) consists exactlyclauses (plus clauses noop(p) mirrored getp), exceptget additional clause {getg1 (1), getg2 (1)}. mutex clauses arisesgetg1 interferes directly getg2 (rather indirectly incompatible preconditions), getg1 adds p instead p0 , p deleted getg2 . yieldstrivial two-step (tree-like) resolution proof P , using two goal clauses mutexclause (namely, resolve second goal clause mutex clause deriving {getg1 (1)},resolve clause first goal clause). hand, originaltask P, getg1 getg2 marked mutex layer A(1), dont directlyinterfere. Therefore, corresponding mutex clause immediately available,resolution proof takes two steps must reason involve x.Encoding (D) works similarly, lets us derive new mutex clause discussedabove. goal clauses case simply {g1 (2)} {g2 (2)}. these, usingtwo corresponding effect clauses, derive two goal clauses encoding (C) twosteps. here, two-step refutation discussed derives empty clause. Thus,four-step resolution refutation P encoding (D). similarly smallresolution refutation P itself, refutation must, mentioned earlier,reason x figure getg1 (1) getg2 (1) cannot True.Proof Theorem 4.15. construct family STRIPS tasks whose CNF encodingssimilar pigeon hole problem formula PHP(i). well knownresolution proof PHP(i) must size exponential (Haken, 1985). Concretely,PHP(i) unsatisfiable formula encoding fact way assign + 1462fiFriends Foes? Planning Satisfiability Abstract CNF Encodingspigeons holes pigeon assigned least one hole hole getsone pigeon. formula i(i + 1) variables xp,h p {1, . . . , + 1}, h{1, . . . , i}. pigeon p, pigeon clause (xp,1 , xp,2 , . . . , xp,n ), pairpigeons {p, q} hole h, hole clause {xp,h , xq,h }.pigeon hole planning task PP HP (i) defined follows. pigeon p,fact assigned (p). hole h, fact free(h). initial state contains freefacts assigned facts. goal state contains i+1 assigned facts. availableactions (other noops) put(p, h), puts pigeon p free hole h,h longer remains free. Formally, put(p, h) = ({free(h)}, {assigned (p)}, {free(h)}).plan length bound b(i) set 1.Consider one four encoding methods (A)(D), let (i) encodingPP HP (i). Restrict (i) setting noop variables False; real restrictiveimplication terms planning since plan length bound 1 none goal factsavailable time step 0. action-only encodings (A) (C), identifyingaction variables put(p, h) PHP(i) variables xp,h immediately yields preciselyclauses PHP(i): goal clauses (i) become pigeon clauses PHP(i)action mutex clauses become hole clauses. action-fact encodings (B) (D),fix free fact variables time step 0 well assigned fact variables time step 1True, identify put(p, h) action variables xp,h . yields preciselyclauses PHP(i). follows resolution hardness PHP(i) Proposition 2.1resolution proof fact planning task PP HP (i) planlength 1 must require size exponential i.claim follows planning task P 0 (i) consists combination twodisconnected pigeon hole planning sub-tasks, PP HP (i) PP HP (1), two separate setspigeon hole objects. goal P 0 (i) naturally defined follows: put firstset + 1 pigeons first set holes put second set two pigeonssecond set holes (which consists single hole). overall CNF encoding0 (i) P 0 (i) logical conjunction encodings (i) (1) (on disjoint setsvariables) PP HP (i) PP HP (1). Observe 0 (i) proved unsatisfiableproving unsatisfiability either two pigeon hole problems. particular,constant size resolution refutation 0 (i) involves refuting (1) component.hand, argue listed abstractions make one-holecomponent P 0 (i) trivially satisfiable, resolution refutation abstractedtask must resort proof unsatisfiability i-hole component (i) P 0 (i),shown requires exponential size. Hence single example P 0 (i) serves showclaim combinations abstraction method CNF encoding.easily verified PP HP (1) becomes solvable ignoring preconditionfree(1) put(1, 1) put(2, 1): put pigeons single hole.happens ignoring delete effect free(1) put(1, 1) put(2, 1).Whenignoring goal assigned (2), inserting assigned (2) initial state,completely removing assigned (2), one-hole component P 0 (i) requires assignone pigeon, course possible. Finally, variable domain abstraction, noteassigned (1) assigned (2) persistently mutex PP HP (1) actionsachieving put(1, 1) put(2, 1), respectively. According Definition 2.2,hence replace assigned (2) assigned (1). resulting planning task,463fiDomshlak, Hoffmann, & Sabharwalsingle goal assigned (1) achieved 1 step by, example, put(1, 1) action.concludes argument.Proof Proposition 4.16. first consider removal duplicate actions. example planning task, denoted P 0 , works four encodings; P 0 defined follows:Fact set {r1 , r2 , g1 , g2 , g3 }Initial state {r1 , r2 }; Goal {g1 , g2 , g3 }Action set containing seven actions:1 1 = ({r1 }, {g1 }, {r1 }),1 2 = ({r1 }, {g2 }, {r1 }),1 3 = ({r1 }, {g3 }, {r1 }),2 1 = ({r2 }, {g1 }, {r2 }),2 2 = ({r2 }, {g2 }, {r2 }),2 3 = ({r2 }, {g3 }, {r2 }),help = ({g1 , g2 }, {g3 }, ).planning task, actions applicable initial state consume onetwo resources r1 r2 . actions achieves one goals, pairgoals reached, three them. solution perform two steps,second help action serves accomplish g3 . set plan lengthbound 1.planning graph P G(P 0 ) step 1 mutex relations directmutexes actions competing resource. Hence, encoding (A) identicalencoding (C), encoding (B) identical encoding (D). properties clearlyhold also planning task P like P 0 except additional action 1 10identical 1 1.Consider encoding (A) P 0 . goal clauses {1 1(0), 2 1(0)}, {1 2(0), 2 2(0)},{1 3(0), 2 3(0)}. clauses mutex clauses form {i j, k}.difficult verify shortest resolution refutation involves 12 steps. Onederivation proceeds via deriving {2 2(0), 2 1(0)}, {2 3(0), 2 1(0)}, {2 1(0)}, {1 2(0)},{1 3(0)}, {}; derived, sequence, 2 steps involving resolutionone mutex clause. P, thing changes clause{1 1(0), 1 10 (0), 2 1(0)} instead {1 1(0), 2 1(0)}, plus additional mutex clauses. Now,obviously every resolution refutation must resolve three goal clauses. endempty clause, hence additionally need get rid literal 1 10 (0). Clearly,way resolve literal away additional step involvingone new mutex clauses. Hence shortest possible resolution refutation13 steps.encoding (B), resolution proofs first need make three steps resolving goalclauses {g1 (1)}, {g2 (1)}, {g3 (1)} respective effect clauses {g1 (1), 1 1(0), 2 1(0)},{g2 (1), 1 2(0), 2 2(0)}, {g3 (1), 1 3(0), 2 3(0)}; thereafter, mattersbefore.show claim removal redundant add effects, slightly modify example,define P 0 follows:464fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsFact set {r1 , r2 , g1 , g2 , g3 , x}Initial state {r1 , r2 }; Goal {g1 , g2 , g3 }Action set containing eight actions:1 1 = ({r1 }, {g1 }, {r1 }),1 2 = ({r1 }, {g2 }, {r1 }),1 3 = ({r1 }, {g3 }, {r1 }),2 1 = ({r2 }, {g1 }, {r2 }),2 2 = ({r2 }, {g2 }, {r2 }),2 3 = ({r2 }, {g3 }, {r2 }),help1 = ({g1 , g2 }, {x}, ),help2 = ({x}, {g3 }, ).task, single help action replaced two help actions needapplied consecutively. set plan length bound 2. before, planning graphP G(P 0 ) mutex relations direct mutexes actions competingresource; encodings (A)/(C) (B)/(D) respectively identical.properties clearly hold also planning task P like P 0 except help1additional add effect g1 .Consider encoding (A) P 0 . goal clauses {1 1(1), 2 1(1), noop(g1 )(1)}, {1 2(1),2 2(1), noop(g2 )(1)}, {1 3(1), 2 3(1), noop(g3 )(1)}. Refuting involves showingthree goals cannot achieved step 1, step 0, combinationtwo. refutation needs resolve three clauses. before, P getadditional literal first clause, {1 1(1), 2 1(1), help1 , noop(g1 )(1)}.Clearly, getting rid additional literal involves least one resolution step.encoding (B), matters essentially except first need resolve goalfact clauses respective effect clauses.ReferencesBall, T., Majumdar, R., Millstein, T., & Rajamani, S. (2001). Automatic predicate abstraction C programs. PLDI2001: Programming Language Design Implementation, pp. 203213.Beame, P., Kautz, H., & Sabharwal, A. (2004). Towards understanding harnessingpotential clause learning. Journal Artificial Intelligence Research, 22, 319351.Beck, C., Hansen, E., Nebel, B., & Rintanen, J. (Eds.). (2008). Proceedings 18thInternational Conference Automated Planning Scheduling (ICAPS-08). AAAIPress.Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90 (1-2), 279298.Blum, A. L., & Furst, M. L. (1995). Fast planning planning graph analysis.Mellish, S. (Ed.), Proceedings 14th International Joint Conference ArtificialIntelligence (IJCAI-95), pp. 16361642, Montreal, Canada. Morgan Kaufmann.465fiDomshlak, Hoffmann, & SabharwalBoddy, M., Fox, M., & Thiebaux, S. (Eds.). (2007). Proceedings 17th InternationalConference Automated Planning Scheduling (ICAPS-07). AAAI Press.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.Bonet, B., & Geffner, H. (2008). Heuristics planning penalties rewards formulated logic computed circuits. Artificial Intelligence, 172 (12-13),15791604.Brafman, R. (2001). reachability, relevance, resolution planning satisfiability approach. Journal Artificial Intelligence Research, 14, 128.Chaki, S., Clarke, E., Groce, A., Jha, S., & Veith, H. (2003). Modular verification softwarecomponents C. ICSE2003: Int. Conf. Software Engineering, pp. 385395.Chen, Y., Huang, R., Xing, Z., & Zhang, W. (2009). Long-distance mutual exclusionplanning. Artificial Intelligence, 173 (2), 365391.Clarke, E. M., Biere, A., Raimi, R., & Zhu, Y. (2001). Bounded model checking usingsatisfiability solving. Formal Methods System Design, 19 (1), 734.Clarke, E. M., Grumberg, O., Jha, S., Lu, Y., & Veith, H. (2003). Counterexample-guidedabstraction refinement symbolic model checking. Journal AssociationComputing Machinery, 50 (5), 752794.Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem proving.Communications ACM, 5 (7), 394397.Davis, M., & Putnam, H. (1960). computing procedure quantification theory. JournalAssociation Computing Machinery, 7 (3), 201215.Edelkamp, S. (2001). Planning pattern databases. Cesta, A., & Borrajo, D. (Eds.),Recent Advances AI Planning. 6th European Conference Planning (ECP01),pp. 1324, Toledo, Spain. Springer-Verlag.Edelkamp, S. (2003). Promela planning. Ball, T., & Rajamani, S. (Eds.), Proceedings10th International SPIN Workshop Model Checking Software (SPIN-03),pp. 197212, Portland, OR. Springer-Verlag.Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimize state encoding length. Biundo, S., & Fox, M. (Eds.), Recent Advances AIPlanning. 5th European Conference Planning (ECP99), Lecture Notes ArtificialIntelligence, pp. 135147, Durham, UK. Springer-Verlag.Ernst, M., Millstein, T., & Weld, D. (1997). Automatic sat-compilation planning problems. Pollack, M. (Ed.), Proceedings 15th International Joint ConferenceArtificial Intelligence (IJCAI-97), pp. 11691176, Nagoya, Japan. Morgan Kaufmann.Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach application theoremproving problem solving. Artificial Intelligence, 2 (34), 198208.Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local searchtemporal action graphs. Journal Artificial Intelligence Research, 20, 239290.466fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsGraf, S., & Sadi, H. (1997). Construction abstract state graphs PVS. CAV1997:Computer Aided Verification, pp. 7283.Gupta, A., & Strichman, O. (2005). Abstraction refinement bounded model checking.Etessami, K., & Rajamani, S. (Eds.), Proceedings 17th International ConferenceComputer Aided Verification (CAV05), Lecture Notes Computer Science, pp.112124, Edinburgh, UK. Springer-Verlag.Haken, A. (1985). intractability resolution. Theoretical Computer Science, 39, 297308.Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Chien, S.,Kambhampati, R., & Knoblock, C. (Eds.), Proceedings 5th International Conference Artificial Intelligence Planning Systems (AIPS-00), pp. 140149, Breckenridge, CO. AAAI Press, Menlo Park.Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independentconstruction pattern database heuristics cost-optimal planning. ProceedingsTwenty-Second AAAI Conference Artificial Intelligence (AAAI-2007), pp.10071012. AAAI Press.Helmert, M., & Mattmuller, R. (2008). Accuracy admissible heuristic functions selected planning domains. Proceedings 23rd AAAI Conference ArtificialIntelligence, pp. 938943, Chicago, IL. AAAI Press.Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimalsequential planning.. Boddy et al. (Boddy, Fox, & Thiebaux, 2007), pp. 176183.Henzinger, T., Jhala, R., Majumdar, R., & McMillan, K. (2004). Abstractions proofs.POPL2004: Principles Programming Languages, pp. 232244.Hernadvolgyi, I., & Holte, R. (1999). PSVN: vector representation production systems.Tech. rep. 1999-07, University Ottawa.Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. JournalArtificial Intelligence Research, 24, 519579.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Hoffmann, J. (2005). ignoring delete lists works: Local search topology planningbenchmarks. Journal Artificial Intelligence Research, 24, 685758.Hoffmann, J., Gomes, C., & Selman, B. (2007). Structure problem hardness: Goal asymmetry dpll proofs sat-based planning. Logical Methods Computer Science,3 (1:6).Hoffmann, J., Sabharwal, A., & Domshlak, C. (2006). Friends foes? AI planningperspective abstraction search.. Long, & Smith (Long & Smith, 2006), pp.294303.Katz, M., & Domshlak, C. (2008). Structural pattern heuristics via fork decomposition..Beck et al. (Beck, Hansen, Nebel, & Rintanen, 2008), pp. 182189.467fiDomshlak, Hoffmann, & SabharwalKautz, H., & Selman, B. (1999). Unifying SAT-based graph-based planning. Pollack, M. (Ed.), Proceedings 16th International Joint Conference ArtificialIntelligence (IJCAI-99), pp. 318325, Stockholm, Sweden. Morgan Kaufmann.Kautz, H. (2004). SATPLAN04: Planning satisfiability. Edelkamp, S., Hoffmann,J., Littman, M., & Younes, H. (Eds.), Proceedings 4th International PlanningCompetition (IPC-04), Whistler, BC, Canada.Kautz, H., Selman, B., & Hoffmann, J. (2006). SATPLAN: Planning satisfiability.Gerevini, A., Dimopoulos, Y., Haslum, P., Saetti, A., Bonet, B., & Givan, B. (Eds.),Proceedings 5th International Planning Competition (IPC-06), Ambleside, UK.Kautz, H. A., McAllester, D., & Selman, B. (1996). Encoding plans propositional logic.Aiello, L. C., Doyle, J., & Shapiro, S. (Eds.), Principles Knowledge RepresentationReasoning: Proceedings 5th International Conference (KR-96), pp. 374384,Cambridge, MA. Morgan Kaufmann.Kautz, H. A., & Selman, B. (1992). Planning satisfiability. Neumann, B. (Ed.),Proceedings 10th European Conference Artificial Intelligence (ECAI-92),pp. 359363, Vienna, Austria. Wiley.Kautz, H. A., & Selman, B. (1996). Pushing envelope: Planning, propositional logic,stochastic search. Proceedings 13th National Conference AmericanAssociation Artificial Intelligence (AAAI-96), pp. 11941201, Portland, OR. MITPress.Knoblock, C. A. (1990). Learning abstraction hierarchies problem solving. Proceedings8th National Conference American Association Artificial Intelligence(AAAI-90), pp. 923928, Boston, MA. MIT Press.Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphsADL subset.. Steel, & Alami (Steel & Alami, 1997), pp. 273285.Koehler, J., & Hoffmann, J. (2000). reasonable forced goal orderings useagenda-driven planning algorithm. Journal Artificial Intelligence Research,12, 338386.Long, D., Kautz, H. A., Selman, B., Bonet, B., Geffner, H., Koehler, J., Brenner, M.,Hoffmann, J., Rittinger, F., Anderson, C. R., Weld, D. S., Smith, D. E., & Fox, M.(2000). aips-98 planning competition. AI Magazine, 21 (2), 1333.Long, D., & Smith, S. (Eds.)., ICAPS-06 (2006). Proceedings 16th International Conference Automated Planning Scheduling (ICAPS-06), Ambleside, UK. MorganKaufmann.McDermott, D. (1999). Using regression-match graphs control search planning. Artificial Intelligence, 109 (1-2), 111159.Meuleau, N., Brafman, R., & Benazera, E. (2006). Stochastic over-subscription planningusing hierarchies MDPs.. Long, & Smith (Long & Smith, 2006), pp. 121130.Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts operatorsplan generation.. Steel, & Alami (Steel & Alami, 1997), pp. 338350.468fiFriends Foes? Planning Satisfiability Abstract CNF EncodingsPrasad, M. R., Biere, A., & Gupta, A. (2005). survey recent advances sat-basedformal verification. International Journal Software Tools Technlogy Transfer,7 (2), 156173.Ray, K., & Ginsberg, M. L. (2008). complexity optimal planning efficientmethod finding solutions.. Beck et al. (Beck et al., 2008), pp. 280287.Rintanen, J. (2004). Evaluation strategies planning satisfiability. Saitta, L. (Ed.),Proceedings 16th European Conference Artificial Intelligence (ECAI-04), pp.682687, Valencia, Spain. Wiley.Rintanen, J. (2008). Planning graphs propositional clause-learning. Brewka, G., &Doherty, P. (Eds.), Principles Knowledge Representation Reasoning: Proceedings 11th International Conference (KR-08), pp. 535543, Sydney, Australia.AAAI Press.Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning satisfiability: parallel plansalgorithms plan search, artificial intelligence. Artificial Intelligence, 170 (12-13),10311080.Robinson, J. A. (1965). machine oriented logic based resolution principle. JournalAssociation Computing Machinery, 12 (1), 2341.Robinson, N., Gretton, C., Pham, D.-N., & Sattar, A. (2008). compact efficient satencoding planning.. Beck et al. (Beck et al., 2008), pp. 296303.Sacerdoti, E. (1973). Planning hierarchy abstraction spaces. Proceedings3rd International Joint Conference Artificial Intelligence (IJCAI-73), pp. 412422,Stanford, CA. William Kaufmann.Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics selecting objectives over-subscription planning problems. Biundo, S., Myers, K., & Rajan, K.(Eds.), Proceedings 15th International Conference Automated PlanningScheduling (ICAPS-05), pp. 192201, Monterey, CA, USA. Morgan Kaufmann.Steel, S., & Alami, R. (Eds.). (1997). Recent Advances AI Planning. 4th European Conference Planning (ECP97), Vol. 1348 Lecture Notes Artificial Intelligence,Toulouse, France. Springer-Verlag.Streeter, M., & Smith, S. (2007). Using decision procedures efficiently optimization..Boddy et al. (Boddy et al., 2007), pp. 312319.469fiJournal Artificial Intelligence Research 36 (2009) 307-340Submitted 06/09; published 11/09Cross-lingual Annotation Projection Semantic RolesSebastian Padopado@ims.uni-stuttgart.deInstitut fur maschinelle SprachverarbeitungUniversitat Stuttgart, 70174 Stuttgart, GermanyMirella Lapatamlap@inf.ed.ac.ukSchool Informatics, University Edinburgh10 Crichton Street, Edinburgh EH8 10 AB, UKAbstractarticle considers task automatically inducing role-semantic annotationsFrameNet paradigm new languages. propose general framework basedannotation projection, phrased graph optimization problem. relatively inexpensive potential reduce human effort involved creating role-semanticresources. Within framework, present projection models exploit lexicalsyntactic information. provide experimental evaluation English-German parallel corpus demonstrates feasibility inducing high-precision German semanticrole annotation manually automatically annotated English data.1. IntroductionSemantic roles play prominent role linguistic theory (Fillmore, 1968; Jackendoff, 1990;Dowty, 1991). describe relations hold predicate arguments,abstracting surface syntactic configurations. example, consider sentences (1a)(1b) butter uniformly assigned semantic role Undergoer (since undergoes physical change) even though syntactically realized object verbmelt (1a) subject (1b):(1)a.b.[Bob]Agent melted [the butter]Undergoer .[The butter]Undergoer melted.intermediate representation seems promising first step towards text understanding,ultimately benefit many natural language processing tasks require broad coverage semantic processing.Methods automatic identification labeling semantic roles, often referredshallow semantic parsing (Gildea & Jurafsky, 2002), important prerequisitewidespread use semantic role information large-scale applications. developmentshallow semantic parsers1 greatly facilitated availability resources likeFrameNet (Fillmore, Johnson, & Petruck, 2003) PropBank (Palmer, Gildea, & Kingsbury, 2005), document possible surface realization semantic roles. Indeed, semantic1. Approaches building shallow semantic parsers numerous list. refer interested readerproceedings 2005 CoNLL shared task (Carreras & Marquez, 2005) 2008 Computational Linguistics Special Issue Semantic Role Labeling (Marquez, Carreras, Litkowski, & Stevenson,2008) overview state-of-the-art.c2009AI Access Foundation. rights reserved.fiPado & LapataDepartingobject (the Theme) moves away Source.Themeofficer left house.plane leaves seven.departure delayed.Sourcedeparted New York.retreated opponent.woman left house.FEEsabandon.v, desert.v, depart.v, departure.n,emerge.v, emigrate.v, emigration.n, escape.v,escape.n, leave.v, quit.v, retreat.v, retreat.n,split.v, withdraw.v, withdrawal.nTable 1: Abbreviated FrameNet entry Departing frameroles recently found use applications ranging information extraction (Surdeanu,Harabagiu, Williams, & Aarseth, 2003) modeling textual entailment relations (Tatu& Moldovan, 2005; Burchardt & Frank, 2006), text categorization (Moschitti, 2008), question answering (Narayanan & Harabagiu, 2004; Frank, Krieger, Xu, Uszkoreit, Crysmann,Jorg, & Schafer, 2007; Moschitti, Quarteroni, Basili, & Manandhar, 2007; Shen & Lapata,2007), machine translation (Wu & Fung, 2009a, 2009b) evaluation (Gimenez &Marquez, 2007).FrameNet paradigm, meaning predicate (usually verb, noun, adjective) represented reference frame, prototypical representation situationpredicate describes (Fillmore, 1982). semantic roles, called frame elements, correspond entities present situation, therefore frame-specific.frame, English FrameNet database2 lists predicates evoke (calledframe-evoking elements FEEs), gives possible syntactic realizations semanticroles, provides annotated examples British National Corpus (Burnard, 2000).abbreviated example definition Departing frame shown Table 1. semantic roles illustrated example sentences FEEs shown bottomtable (e.g., abandon, desert, depart). PropBank corpus, second major semantic role resource English, provides role realization information verbs similarmanner Wall Street Journal portion Penn Treebank. uses index-based rolenames (Arg0Argn), Arg0 Arg1 correspond Dowtys (1991) proto-agentproto-patient. Higher indices defined verb-by-verb basis.Unfortunately, resources FrameNet PropBank largely absent almostlanguages except English, main reason role-semantic annotationexpensive time-consuming process. current English FrameNet (Version 1.3)developed past twelve years. contains roughly 800 frames covering2. Available http://framenet.icsi.berkeley.edu.308fiCross-lingual Annotation Projection Semantic Rolesaround 150,000 annotated tokens 7,000 frame-evoking elements. Although FrameNetsconstructed German, Spanish, Japanese, resources considerablysmaller. true PropBank-style resources, developedKorean3 , Chinese (Xue & Palmer, 2009), Spanish Catalan (Taule, Mart, & Recasens,2008). Compared English PropBank, covers 113,000 predicate-argument structures, resources languages two three times smaller (e.g., KoreanPropBank provides 33,000 annotations).Given data requirements supervised learning algorithms (Fleischman & Hovy,2003) current paucity data, unsupervised methods could potentially enablecreation annotated data new languages reduce human effort involved.However, unsupervised approaches shallow semantic parsing still early stage,mostly applicable resources FrameNet (Swier & Stevenson, 2004, 2005;Grenager & Manning, 2006). article, propose method employs parallelcorpora acquiring frame elements syntactic realizations new languages (seeupper half Table 1). approach leverages existing English FrameNet overcome resource shortage languages exploiting translational equivalencespresent aligned data. Specifically, uses annotation projection (Yarowsky, Ngai, & Wicentowski, 2001; Yarowsky & Ngai, 2001; Hwa, Resnik, Weinberg, & Kolak, 2002; Hi &Hwa, 2005) transfer semantic roles English less resource-rich languages. keyidea projection summarized follows: (1) given pair sentences E (English)L (new language) translations other, annotate E semantic roles;(2) project roles onto L using word alignment information. manner,induce semantic structure L side parallel text, servedata training shallow semantic parser L independent parallel corpus.annotation projection paradigm faces least two challenges considering semantic roles. Firstly, semantic structure projected must shared twosentences. Clearly, role-semantic analysis source sentence E inappropriatetarget sentence L, simple projection produce valid semantic role annotations.Secondly, even two sentences demonstrate semantic parallelism, semantic role annotationspertain potentially arbitrarily long word spans rather individual words. Recovering word span semantic roles target language challenging givenautomatic alignment methods often produce noisy incomplete alignments.address first challenge showing that, two languages exhibit substantialdegree semantic correspondence, annotation projection feasible. Using EnglishGerman parallel corpus test bed, assess whether English semantic role annotationstransferred successfully onto German. find two languages exhibitdegree semantic correspondence substantial enough warrant projection. tacklesecond challenge presenting framework projection semantic role annotationsgoes beyond single word alignments. Specifically, construct semantic alignmentsconstituents source target sentences formalize search bestsemantic alignment optimization problem bipartite graph. argue bipartitegraphs offer flexible intuitive framework modeling semantic alignments abledeal noise represent translational divergences. present different classes3. Korean PropBank available LDC (http://www.ldc.upenn.edu/).309fiPado & Lapatamodels varying assumptions regarding admissible correspondences sourcetarget constituents. Experimental results demonstrate constituent-based modelsoutperform word-based alternatives large margin.4remainder article organized follows. Section 2 discusses annotationprojection general presents annotation study examining degree semanticparallelism English-German corpus. Section 3, formalize semantic alignmentspresent modeling framework. experiments detailed Section 4. reviewrelated work Section 5 conclude article discussion future work (Section 6).2. Annotation Projection Semantic Correspondencerecent years, interest grown parallel corpora multilingual cross-lingualnatural language processing. Beyond machine translation, parallel corpora exploitedrelieve effort involved creating annotations new languages. One importantparadigm, annotation projection, creates new monolingual resources transferring annotations English (or resource-rich languages) onto resource-scarce languagesuse word alignments. resulting (noisy) annotations usedconjunction robust learning algorithms obtain NLP tools taggerschunkers relatively cheaply. projection approach successfully used transfer wide range linguistic annotations languages. Examples include partsspeech (Yarowsky et al., 2001; Hi & Hwa, 2005), chunks (Yarowsky et al., 2001), dependencies (Hwa et al., 2002), word senses (Diab & Resnik, 2002; Bentivogli & Pianta, 2005),information extraction markup (Riloff, Schafer, & Yarowsky, 2002), coreference chains (Postolache, Cristea, & Orasan, 2006), temporal information (Spreyer & Frank, 2008), LFGf-structures (Tokarczyk & Frank, 2009).important assumption underlying annotation projection linguistic analysesone language also valid another language. however unrealistic expecttwo languages, even family, perfect correspondence.many well-studied systematic differences across languages often referred translationaldivergences (van Leuven-Zwart, 1989; Dorr, 1995). structural,semantic content source target language realized using differentstructures, semantic, content undergoes change translation. Translational divergences (in conjunction poor alignments) major stumbling blocktowards achieving accurate projections. Yarowsky Ngai (2001) find parts speechtransferred directly English onto French contain considerable noise, evencases inaccurate automatic alignments manually corrected (accuracies vary69% 78% depending tagset granularity). syntax, Hwa, Resnik, Weinberg, Cabezas, Kolak (2005) find 37% English dependency relationsdirect counterparts Chinese, 38% Spanish. problem commonly addressedfiltering mechanisms, act post-processing step projection output.example, Yarowsky Ngai (2001) exclude infrequent projections poor alignments4. preliminary version work published proceedings EMNLP 2005 COLING/ACL2006. current article contains detailed description approach, presents several novelexperiments, comprehensive error analysis.310fiCross-lingual Annotation Projection Semantic RolesHwa et al. (2005) apply transformation rules encode linguistic knowledgetarget language.case semantic frames reason optimism. definition, framesbased conceptual structure (Fillmore, 1982). latter constitute generalizationssurface structure therefore ought less prone syntactic variation. Indeed, effortsdevelop FrameNets manually German, Japanese, Spanish reveal largenumber English frames re-used directly describe predicates argumentslanguages (Ohara, Fujii, Saito, Ishizaki, Ohori, & Suzuki, 2003; Subirats & Petruck,2003; Burchardt, Erk, Frank, Kowalski, Pado, & Pinkal, 2009). Boas (2005) even suggestsframe semantics interlingual meaning representation.Computational studies projection parallel corpora also obtained good resultssemantic annotation. Fung Chen (2004) induce FrameNet-style annotations Chinese mapping English FrameNet entries directly onto concepts listed HowNet5 ,on-line ontology Chinese, without using parallel texts. experiment, transfersemantic roles English Chinese accuracy 68%. Basili, Cao, Croce, Coppola, Moschitti (2009) use gold standard annotations transfer semantic rolesEnglish Italian 73% accuracy. Bentivogli Pianta (2005) project EuroWordNetsense tags, represent fine-grained semantic information FrameNet, alsoEnglish Italian. obtain precision 88% recall 71%, without applying filtering. Fung, Wu, Yang, Wu (2006, 2007) analyse automatically annotatedEnglishChinese parallel corpus find high cross-lingual agreement PropBank roles(in range 75%95%, depending role).provide sound empirical justification projection-based approach, conducted manual annotation study parallel English-German corpus. identifiedsemantic role information bi-sentences assessed degree frames semantic roles agree diverge English German. degree divergence providesnatural upper bound accuracy attainable annotation projection.2.1 Sample SelectionEnglish-German bi-sentences drawn second release Europarl (Koehn, 2005),corpus professionally translated proceedings European Parliament. Europarlaligned document sentence level available 11 languages. EnglishGerman section contains 25 million words sides. Even though restrictedgenre (transcriptions spoken text), Europarl fairly open-domain, covering wide rangetopics foreign politics, cultural economic affairs, procedural matters.naive sampling strategy would involve randomly selecting bi-sentences Europarlcontain FrameNet predicate English side aligned wordGerman side. two caveats here. First, alignment two predicatesmay wrong, leading us assign wrong frame German predicate. Secondly, evenalignment accurate, possible randomly chosen English predicate evokesframe yet covered FrameNet. example, FrameNet 1.3 documentsreceive sense verb accept (as sentence Mary accepted gift),entry admit sense predicate (e.g., accept problem5. See http://www.keenage.com/zhiwang/e_zhiwang.html.311fiPado & LapataMeasureFrame MatchRole MatchSpan MatchEnglish89.794.984.4German86.795.283.088.295.083.7Table 2: Monolingual inter-annotator agreement calibration setEU ) relatively frequent Europarl. Indeed, pilot study, inspectedsmall random sample consisting 100 bi-sentences, using publicly available GIZA++software (Och & Ney, 2003) induce English-German word alignments. found 25%English predicates readings documented FrameNet, additional9% predicate pairs instances wrong alignments. order obtain cleanersample, final sampling procedure informed English FrameNet SALSA,FrameNet-compatible database, German (Erk, Kowalski, Pado, & Pinkal, 2003).gathered GermanEnglish sentences corpus least one pairGIZA++-aligned predicates (we , wg ), listed FrameNet wg SALSA,intersection two frame lists wg non-empty. corpuscontains 83 frame types, 696 lemma pairs, 265 unique English 178 unique Germanlemmas. Sentence pairs grouped three bands according frame frequency(High, Medium, Low). randomly selected 380 pairs band annotation.total sample consisted 1,140 bi-sentences. semantic annotation took place,constituency parses corpus obtained Collins (1997) parser EnglishDubeys (2005) German. automatic parses corrected manually, followingannotation guidelines Penn Treebank (English) TIGER corpus (German).2.2 Annotationsyntactic correction, two annotators native-level proficiency German English annotated bi-sentence frames evoked wg semanticroles (i.e., one frame per monolingual sentence). every predicate, task involved twosteps: (a) selecting appropriate frame (b) assigning instantiated semantic rolessentence constituents. Annotators provided detailed guidelines explainedtask multiple examples.annotation took place gold standard parsed corpus proceeded threephases: training phase (40 bi-sentences), calibration phase (100 bi-sentences),production mode phase (1000 bi-sentences). training, annotators acquaintedannotation style. calibration phase, bi-sentence doubly annotatedassess inter-annotator agreement. Finally, production mode, 1000 bi-sentencesmain dataset split half randomly assigned one coderssingle annotation. thus ensured annotator saw parts bi-sentenceavoid language bias role assignment (annotators may prone labelEnglish sentence similar German translation vice versa). coder annotatedapproximately amount data English German accessFrameNet SALSA resources.312fiCross-lingual Annotation Projection Semantic RolesMeasureFrame MatchRole MatchPrecision71.690.5Recall71.692.3F1-Score71.691.4Table 3: Semantic parallelism English Germanresults inter-annotator agreement study given Table 2. widely usedKappa statistic directly applicable task requires fixed set itemsclassified fixed set categories. case, however, fixed items, sincespan frame elements length. addition, categories (i.e., framesroles) predicate-specific, vary item item (for discussion issue, see alsowork Miltsakaki et al., 2004). Instead, compute three different agreement measuresdefined as: ratio common frames two sentences (Frame Match), ratiocommon roles (Role Match), ratio roles identical spans (Span Match).shown Table 2, annotators tend agree frame assignment; disagreements mainlydue fuzzy distinctions closely related frames (e.g., AwarenessCertainty). Annotators also agree roles assign identifying role spans.Overall, obtain high agreement aspects annotation, indicatestask well-defined. aware published agreement figures EnglishFrameNet annotations, results comparable numbers reported Burchardt,Erk, Frank, Kowalski, Pado, Pinkal (2006) German, viz. 85% agreement frameassignment (Frame Match) 86% agreement role annotation.62.3 EvaluationRecall main dataset consists 1,000 English-German bi-sentences annotatedFrameNet semantic roles. Since annotations language created independently, used provide estimate degree semantic parallelismtwo languages. measured parallelism using precision recall, treatingGerman annotations gold standard. evaluation scheme directly gauges usability English source language annotation projection. Less 100% recallmeans target language frames roles present Englishcannot retrieved annotation projection. Conversely, imperfect precision indicatesEnglish frames roles whose projection yields erroneous annotations target language. Frames roles counted matching occur halvesbi-sentence, regardless role spans, comparable across languages.shown Table 3, 72% time English German evokeframe (Frame Match). result encouraging, especially considering framedisagreements also arise within single language demonstrated inter-annotatorstudy calibration set (see row Frame Match Table 2). However, also indicatesnon-negligible number cases translational divergence frame level.often cases one language chooses single predicate express situationwhereas one uses complex predication. following example, Englishtransitive predicate increase evokes frame Cause change scalar position (An6. parallel corpus created available http://nlpado.de/~sebastian/srl_data.html.313fiPado & Lapataagent cause increases position variable scale). German translationfuhrt zu hoheren (leads higher) combines Causation frame evoked fuhreninchoative Change scalar position frame introduced hoher :(2)increase level employment.Dies wird zu einer hoheren Erwebsquotefuhren.higherlevel employment leadlevel semantic roles, agreement (Role Match) reaches F1-Score 91%.means frames correspond across languages, roles agree large extent.Role mismatches frequently cases passivization infinitival constructions leadingrole elision. example below, remembered denkt evoke Memoryframe. English uses passive construction leaves deep subject position unfilled.contrast, German uses active construction deep subject position filledsemantically light pronoun, man (one).(3)ask [Ireland]Content remembered.Ich mochtedarum bitten, dass [man]Cognizer [an Irland]Content denkt.would likeask oneIrelandthinkssum, find substantial cross-lingual semantic correspondenceEnglish German provided predicates evoke frame. enlistedhelp SALSA database meet requirement. Alternatively, could usedexisting bilingual dictionary (Fung & Chen, 2004), aligned frames automatically usingvector-based representation (Basili et al., 2009) inferred FrameNet-style predicate labelsGerman following approach proposed Pado Lapata (2005).3. Modeling Semantic Role Projection Semantic Alignmentsprevious work projection relies word alignments transfer annotationslanguages. surprising, since annotations interest often definedword level (e.g., parts speech, word senses, dependencies) rarely spanone token. contrast, semantic roles cover sentential constituents arbitrary length,simply using word alignments projection likely result wrong role spans.example, consider bi-sentence Figure 1.7 Assume (English) source annotated semantic roles wish project onto(German) target. Although alignments (indicated dotted lines sentence) accurate (e.g., promised versprach, zu), others noisy incomplete(e.g., time punktlich instead time punktlich). Based alignments,Message role would projected German onto (incorrect) word span punktlich zuinstead punktlich zu kommen, since kommen aligned English word.course possible devise heuristics amending alignment errors. However,process scale well: different heuristics need created different errors,7. literal gloss German sentence Kim promises timely come.314fiCross-lingual Annotation Projection Semantic RolesNP SPEAKERVPNP SPEAKERVPMESSAGEMESSAGEKim promised timeKim versprach, pnktlich zu kommenFigure 1: Bilingual projection semantic role information semantic alignments constituents.process repeated new language pair. Instead, projection modelalleviates problem principled manner taking constituency informationaccount. Specifically, induce semantic alignments source target sentencesrelying syntactic constituents introduce bias towards linguistically meaningfulspans. constituent aligned correctly, sufficient subset yieldcorrectly word-aligned. So, Figure 1, align time punktlich zukommen project role Message accurately, despite fact kommenaligned other. following, describe detail semantic alignmentscomputed subsequently guide projection onto target language.3.1 Framework Formalizationbi-sentence represented set linguistic units. distinguishedsource (us Us ) target (ut Ut ) units words, chunks, constituents,groupings. semantic roles source sentence modeled labeling function: R 2Us maps roles sets source units. view projection constructionsimilar role labeling function target sentence, : R 2Ut . Without lossgenerality, limit one frame per sentence, FrameNet.8semantic alignment Us Ut subset Cartesian productsource target units:Us Ut(4)alignment link us Us ut Ut implies us ut semanticallyequivalent. Provided role assignment function source sentence, ,projection consists simply transferring source labels r onto union targetunits semantically aligned source units bearing label r:(r) = {ut k us (r) : (us , ut ) A}(5)8. entails cannot take advantage potentially beneficial dependencies arguments different predicates within one sentence, shown improve semantic rolelabeling (Carreras, Marquez, & Chrupala, 2004).315fiPado & Lapataphrase search semantic alignment optimization problem. Specifically,seek alignment maximizes product bilingual similarities simsource target units:sim(us , ut )(6)= argmaxAA(us ,ut )Aseveral well-established methods literature computing semantic similarity within one language (see work Weeds, 2003, Budanitsky & Hirst, 2006,overviews). Measuring semantic similarity across languages well studiedless consensus appropriate methods are. article, employsimple method, using automatic word alignments proxy semantic equivalence;however, similarity measures used (see discussion Section 6). Followinggeneral convention, assume sim function ranging 0 (minimal similarity)1 (maximal similarity).wealth optimization methods used solve (6). article, treatconstituent alignment bipartite graph optimization problem. Bipartite graphs providesimple intuitive modeling framework alignment problems optimization algorithms well-understood computationally moderate. importantly,imposing constraints bipartite graph, bias model linguisticallyimplausible alignments, example alignments map multiple English roles onto single German constituent. Different graph topologies correspond different constraintsset admissible alignments A. instance, may want ensure sourcetarget units aligned, restrict alignment one-to-one matches (see Section 3.3details).formally, weighted bipartite graph graph G = (V, E) whose node set Vpartitioned two nonempty sets V1 V2 way every edge E joinsnode V1 node V2 labeled weight. projection application,two partitions sets linguistic units Us Ut , source target sentence,respectively. assume G complete, is, source node connectedtarget nodes vice versa.9 Edge weights model (dis-)similarity pairsource target units.optimization problem Equation (6) identifies alignment maximizesproduct link similarities equivalent edges bipartite graph. Findingoptimal alignment amounts identifying minimum-weight subgraph (Cormen, Leiserson,& Rivest, 1990) subgraph G0 G satisfies certain structural constraints (seediscussion below) incurring minimal edge cost:= argminAAXweight(us , ut )(7)(us ,ut )Aminimization problem Equation (7) equivalent maximization problem (6)setting weight(us , ut ) to:weight(us , ut ) = log sim(us , ut )9. Unwanted alignments excluded explicitly setting similarity zero.316(8)fiCross-lingual Annotation Projection Semantic RolesS4S'4VP2NP3NP'3S1Kim promised timeVP'2S'1Kim versprach, pnktlich zu kommen(a) Bi-sentence word alignmentsS1VP2NP3S4S010.580.4500.37VP020.450.6800.55NP030010.18S040.370.550.180.73S1VP2NP3S4(b) Constituent SimilaritiesS010.540.801.00VP020.800.390.60NP0301.70S041.000.601.700.31(c) Edge weightsFigure 2: Example bi-sentence represented edge weight matrixexample, consider Figure 2. shows bi-sentence Figure 1 representation edge weight matrix corresponding complete bipartite graph. nodesgraph (S1 S4 source side S01 S04 target side) model sentential constituents.numbers Figure 2b similarity scores, corresponding edge weights shownFigure 2c. High similarity scores correspond low edge weights. Edges zero similarityset infinity (in practice, large number). Finally, notice alignmentshigh similarity scores (low edge weights) occur diagonal matrix.order obtain complete projection models must (a) specify linguistic unitsalignment takes place; (b) define appropriate similarity function; (c)formulate alignment constraints. following, describe two models, oneuses words linguistic units one uses constituents. also present appropriatesimilarity functions models detail alignment constraints.3.2 Word-based Projectionfirst model linguistic units word tokens. Source target sentences represented sets words, Us = {ws1 , ws2 , . . . } Ut = {wt1 , wt2 , . . . }, respectively. Semanticalignments links individual words. thus conveniently interpretoff-the-shelf word alignments semantic alignments. Formally, achievedfollowing binary similarity function, trivially turns word alignment optimalsemantic alignment.(1 ws wt word-alignedsim(ws , wt ) =(9)0 elseConstraints admissible alignments often encoded word alignment models eitherheuristically (e.g., enforcing one-to-one alignments Melamed, 2000) virtue317fiPado & Lapatatranslation model used computation. example, IBM models introducedseminal work Brown, Pietra, Pietra, Mercer (1993) require target wordaligned exactly one source word (which may empty word), therefore allowone-to-many alignments one direction. experiments use alignments inducedpublicly available GIZA++ software (Och & Ney, 2003). GIZA++ yields alignmentsinterfacing IBM models 14 (Brown et al., 1993) HMM extensions models 12 (Vogel, Ney, & Tillmann, 1996). particular configuration shownoutperform several heuristic statistical alignment models (Och & Ney, 2003). thustake advantage alignment constraints already encoded GIZA++ assumeoptimal semantic alignment given set GIZA++ links. resulting targetlanguage labeling function is:aw(r) = {wt k ws (r) : ws wt GIZA++ word-aligned}(10)labeling function corresponds (implicit) labeling functions employedword-based annotation projection models. models easily derived differentlanguage pairs without recourse corpus-external resources. Unfortunately, discussed Section 2, automatically induced alignments often noisy, thus leadingprojection errors. Cases point function words (e.g., prepositions) multi-wordexpressions, systematically misaligned due high degree cross-lingualvariation.3.3 Constituent-based Projectionsecond model linguistic units constituents. Source target sentencesthus represented constituent sets (Us = {c1s , c2s , . . . }) (Ut = {c1t , c2t , . . . }).constituent-based similarity function capture extent cs projection ct express semantic content. approximate measuring wordalignment-based word overlap cs ct Jaccards coefficient.Let yield(c) denote set words yield constituent c, al(c) setwords target language aligned yield c. word overlapsource constituent cs target constituent ct defined as:o(cs , ct ) =|al(cs ) yield(ct )||al(cs ) yield(ct )|(11)Jaccards coefficient asymmetric: consider well projection sourceconstituent al(cs ) matches target constituent ct , vice versa. order taketarget-source source-target correspondences account, measure word overlapdirections use mean measure similarity:sim(cs , ct ) = (o(cs , ct ) + o(ct , cs ))/2(12)addition similarity measure, constituent-based projection model must also specifyconstraints characterize set admissible alignments A. paper,consider three types alignment constraints affect number alignment links perconstituent (in graph-theoretic terms, degree nodes Us ). focus motivated318fiCross-lingual Annotation Projection Semantic RolesUsUt1122r233r2445e556e66r1(a) Perfect matchingUsUt1122r233r244r1r1r2(b) Edge coverUsUt1122r233r244r1r1r2r1r2(c) Total alignmentFigure 3: Constituent alignments role projections resulting three families alignment constraints (Us , Ut : source target constituents; r1 , r2 : semantic roles).patterns observe gold standard corpus (cf. Section 2). EnglishGerman constituent, determined whether corresponded none, exactly one,several constituents language, according gold standard word alignment.majority constituents correspond exactly one constituent (67%), followedsubstantial number one-to-many/many-to-one correspondences (32%), casesconstituents translated (i.e., corresponding node sidebi-sentence) rare (1%).analysis indicates perfect data, expect best performancemodel allows one-to-many alignments. However, common finding machinelearning restrictive models, even though appropriate data hand,yield better results limiting hypothesis space. spirit, compare three familiesadmissible alignments range restrictive permissive,evaluate other.3.3.1 Alignments Perfect Matchingsfirst consider restrictive case, constituent exactly one adjacentalignment edge. Semantic alignments property thought bijectivefunctions: source constituent mapped one target constituent, vice versa.resulting bipartite graphs perfect matchings. example perfect bipartite matchinggiven Figure 3a. Note target side contains two nodes labeled (e), shorthandempty node. Since bi-sentences often differ size, resulting graph partitionsdifferent sizes well. cases, introduce empty nodes smallerpartition enable perfect matching. Empty nodes assigned similarity zeronodes. Alignments empty nodes (such source nodes (3) (6))ignored purposes projection; gives model possibility abstainingaligning node good correspondence found.Perfect matchings assume strong equivalence constituent structurestwo languages; neither alignments Figure 3(b) 3(c) perfect matching.319fiPado & LapataPerfect matchings cannot model one-to-many matches, i.e., cases semantic materialexpressed one constituent one language split two constituentslanguage. means perfect matchings appropriate sourcetarget role annotations span exactly one constituent. always case,perfect matchings also advantage expressive models: allowingnode one adjacent edge, introduce strong competition edges. result,errors word alignment corrected extent.Perfect matchings computed efficiently using algorithms network optimization (Fredman & Tarjan, 1987) time approximately cubic total number constituents bi-sentence (O(|Us |2 log |Us | + |Us |2 |Ut |)). Furthermore, perfect matchingsequivalent well-known linear assignment problem, many solution algorithmsdeveloped (e.g., Jonker Volgenant 1987, time O(max(|Us |, |Ut |)3 )).3.3.2 Alignments Edge Coversnext consider edge covers, generalization perfect matchings. Edge covers bipartite graphs source target constituent adjacent least one edge.illustrated Figure 3b, source target nodes one adjacent edge (i.e., alignment link), nodes one (see source node (2) target node (4)). Edgecovers impose weaker correspondence assumptions perfect matchings, since allow one-to-many alignments constituents either direction.10 So, theory, edgecovers higher chance delivering correct role projection perfect matchingssyntactic structures source target sentences different. alsodeal better situations semantic roles assigned one constituentone languages (cf. source nodes (3) (4), labeled role r2 , examplegraph). Notice perfect matchings shown Figure 3a also edge covers, whereasgraph Figure 3c not, target-side nodes (1) (3) adjacent edges.Eiter Mannila (1997) develop algorithm computing optimal edge covers.show minimum-weight edge covers reduced minimum weight perfectmatchings (see above) auxiliary bipartite graph two partitions size |Us | + |Ut |.allows computation minimum weight edge covers time O((|Us | + |Ut |)3 ) =O(max(|Us |, |Ut |)3 ), also cubic number constituents bi-sentence.3.3.3 Total Alignmentslast family admissible alignments consider total alignments. Here, sourceconstituent linked target constituent (i.e.,the alignment forms total functionsource nodes). Total alignments impose constraints target nodes,therefore linked arbitrary number source nodes, including none. Totalalignments permissive alignment class. contrast perfect matchingsedge covers, constituents target sentence left unaligned. Total alignments10. general definition edge covers also allows many-to-many alignments. However, optimal edge coversaccording Equation (7) cannot many-to-many, since weight edge covers many-to-manyalignments never minimal: many-to-many edge cover, one edge removed,resulting edge cover lower weight.320fiCross-lingual Annotation Projection Semantic Rolescomputed linking source node maximally similar target node:= {(cs , ct ) | cs Us ct = argmax sim(cs , c0t )}(13)c0t UtDue independence source nodes, local optimization results globally optimalalignment. time complexity procedure quadratic number constituents,O(|Us ||Ut |).example shown Figure 3c, source constituents (1) (2) correspondtarget constituent (2), source constituents (3)(6) correspond (4). Target constituents (1) (3) aligned. quality total alignments relies heavilyunderlying word alignment. Since little competition edges,tendency form alignments mostly high (similarity) scoring target constituents.practice, means potentially important, idiosyncratic, target constituentslow similarity scores left unaligned.3.4 Noise Reductiondiscussed Section 2, noise elimination techniques integral part projectionarchitectures. Although constituent-based model overly sensitive noiseexpect syntactic information compensate alignment errors word-basedmodel error-prone since relies solely automatically obtained alignmentstransferring semantic roles. introduce three filtering techniques eithercorrect discard erroneous projections.3.4.1 Filling-the-gapsAccording definition projection Equation (5), span projected role rcorresponds union target units aligned source units labeled r.definition sufficient constituent-based projection models, roles rarely spanone constituent yield many wrong alignments word-based modelsroles typically span several source units (i.e., words). Due errors gapsword alignment, target span role often non-contiguous set words.repair non-contiguous projections first last word projectedcorrectly applying simple heuristic fills gaps target role span. Let posindex word token given sentence, extension set indicesset words. target span role r without gaps defined as:acc(r) = {u | min(pos(at (r))) pos(u) max(pos(at (r))}(14)apply heuristic word-based models article.3.4.2 Word Filteringtechnique removes words form bi-sentence prior projection according certaincriteria. apply two intuitive instantiations word filtering experiments.first removes non-content words, i.e., words adjectives, adverbs, verbs,nouns, source target sentences, since alignments non-content words321fiPado & LapataNPVPKim versprach, pnktlich zu kommenFigure 4: Argument filtering (predicate boldface, potential arguments boxes).notoriously unreliable may negatively impact similarity computations.second filter removes words remain unaligned output automatic wordalignment. filters aim distinguishing genuine word alignments noisy onesspeed computation semantic alignments.3.4.3 Argument Filteringlast filter applies constituent-based models defined full parse trees. Previouswork shallow semantic parsing demonstrated nodes tree equallyprobable semantic roles given predicate (Xue & Palmer, 2004). fact, assumingperfect parse, set likely arguments, almost semantic rolesassigned. set likely arguments consists constituents childancestor predicate, provided (a) dominate predicate(b) sentence boundary constituent predicate.definition covers long-distance dependencies control constructions verbs,support constructions nouns, extended accommodate coordination.apply filter reduce size target tree. example Figure 4, treenodes removed except NP Kim punktlich zu kommen.3.5 Discussionpresented framework bilingual projection semantic roles basednotion semantic alignment. discussed two broad instantiationsframework, namely word-based constituent-based models. latter case, operationalize search optimal semantic alignment graph optimization problem.Specifically, bi-sentence conceptualized bipartite graph. nodes correspondsyntactic constituents bi-sentence, weighted edges cross-lingualpairwise similarity constituents. Assumptions semantic correspondencelanguages formalized constraints graph structure.also discussed three families constraints. Perfect matching forces correspondences English German constituents bijective. contrast, total alignments assume looser correspondence leaving constituents target side unaligned.Edge covers occupy middle ground, assuming constituents must aligned without strictly enforcing one-to-one alignments. perfect matching linguistically implausible, assuming structural divergence languages, overcome wordalignment errors. Total alignments model structural changes therefore linguis322fiCross-lingual Annotation Projection Semantic RolesLingUnitwordsconstituentsconstituentsconstituentsSimilaritybinaryoverlapoverlapoverlapCorrespondenceone-to-oneone-to-oneone-to-at-least-oneone-to-manyBiGraphn/aperfect matchingedge covertotalComplexitylinearcubiccubicquadraticTable 4: Framework instantiationstically appropriate, time sensitive alignment errors. Findingoptimal alignment corresponds finding optimal subgraph consistent constraints. Efficient algorithms exist problem. Finally, introduced smallnumber filtering techniques reduce impact alignment errors.models properties summarized Table 4. vary along following dimensions: linguistic units employed (LingUnit), similarity measure (Similarity), assumptions semantic correspondence (Correspondence) structurebipartite graph entails (BiGraph). also mention complexity computation (Complexity). empirically assess performance following sections.4. Experimentsdescribe evaluation framework developed Section 3. present twoexperiments, consider projection semantic roles English sentencesonto German translations, evaluate German gold standard role annotation. Experiment 1 uses gold standard data syntactic semantic annotation.oracle setting assesses potential role projection own, separating errors due translational divergence modeling assumptions incurredpreprocessing (e.g., parsing automatic alignment). Experiment 2 investigatespractical setting employs automatic tools syntactic semantic parsing, thusapproximating conditions large-scale role projection parallel corpora.4.1 Setup4.1.1 Datamodels evaluated parallel corpus described Section 2. corpusrandomly shuffled split development test set (each 50% data).Table 5 reports number tokens, sentences, frames, arguments developmenttest set English German.Word alignments computed GIZA++ toolkit (Och & Ney, 2003).used entire English-German Europarl bitext training data induce alignmentsdirections (source-target, target-source), default GIZA++ settings. Followingcommon practice Machine Translation, alignments symmetrized using intersection heuristic (Koehn, Och, & Marcu, 2003), known lead high-precisionalignments. also produced manual word alignments sentences corpus,using GIZA++ alignments starting point following Blinker annotationguidelines (Melamed, 1998).323fiPado & LapataLanguageDev-ENTest-ENDev-DETest-DETokens11,58512,01911,22911,548Sentences491496491496Frames491496491496Roles2,4232,4652,5762,747Table 5: Statistics gold standard parallel corpus broken development (Dev)test (Test) set.4.1.2 Methodimplemented four models shown Table 4 filteringtechniques introduced Section 3.4. resulted total sixteen models,evaluated development set. Results best-performing models nextvalidated test set. found practical runtime experiment dominatedinput/output XML processing rather optimization problem itself.experiments, constituent-based models compared word-basedmodel, treat baseline. latter model relatively simple: projection reliesexclusively word alignments, require syntactic analysis, linear timecomplexity. thus represents good point comparison models take linguisticknowledge account.4.1.3 Evaluation Measuremeasure model performance using labeled Precision, Recall, F1 ExactMatch condition, i.e., label span projected English rolematch German gold standard role count true positive. also assess whether differences performance statistically significant using stratified shuffling (Noreen, 1989),instance assumption-free approximate randomization testing (see Yeh, 2000, discussion).11 Whenever discuss changes F1, refer absolute (rather relative)differences.4.1.4 Upper Boundannotation study (see Table 2, Section 2.2) obtained inter-annotator agreement0.84 Span Match condition (annotation roles span).number seen reasonable upper bound performance automaticsemantic role labeling system within language. difficult determine ceilingprojection task, since addition inter-annotator agreement, takeaccount effect bilingual divergence. annotation study provide estimateformer, latter. default method measuring bilingual agreementspans, adopt monolingual Span Match agreement upper boundprojection experiments. Note, however, upper bound strict systemoracle able outperform it.11. software available http://www.nlpado.de/~sebastian/sigf.html.324fiCross-lingual Annotation Projection Semantic RolesModelWordBLPerfMatchEdgeCoverTotalUpperBndFilterPrec52.075.871.768.985.0Rec46.257.161.861.383.0F148.965.166.464.984.0NA FilterModelPrec RecWordBL52.0 46.2PerfMatch 81.4 69.4EdgeCover77.9 69.3Total78.8 70.0UpperBnd85.0 83.0F148.974.973.374.184.0NCModelWordBLPerfMatchEdgeCoverTotalUpperBndFilterPrec37.179.475.069.785.0Rec32.062.263.060.183.0F134.469.868.564.584.0Arg FilterModelPrec RecWordBL52.0 46.2PerfMatch88.8 56.2EdgeCover 81.4 69.7Total81.2 69.6UpperBnd85.0 83.0F148.968.875.175.084.0Table 6: Model comparison development set using gold standard parses semanticroles four filtering techniques: filtering (No Filter), removal non-alignedwords (NA Filter), removal non-content words (NC Filter), removal nonarguments (Arg Filter). Best performing models indicated boldface.4.2 Experiment 1: Projection Gold Standard DataExperiment 1, use manually annotated semantic roles hand-corrected syntacticanalyses constituent-based projection models. explained Section 4.1, firstdiscuss results development set. best model instantiations next evaluatedtest set.4.2.1 Development SetTable 6 shows performance models (No Filter) combinationfiltering techniques. Filter condition, word-based model (WordBL) yieldsmodest F1 48.9% application filling-the-gaps heuristic12 (see Section 3.4details). condition, constituent-based models deliver F1 increase approximately 20% (all differences WordBL constituent-based models significant,p < 0.01). EdgeCover model performs significantly better total alignments (Total,p < 0.05) comparably perfect matchings (PerfMatch).Filtering schemes generally improve resulting projections constituent-basedmodels. non-aligned words removed (NA Filter), F1 increases 9.8% PerfMatch, 6.9% EdgeCover, 9.2% Total. PerfMatch Total best performing models NA Filter. significantly outperform EdgeCover (p < 0.05)condition constituent-based models Filter condition (p < 0.01).word-based models performance remains constant. definition WordBL considersaligned words only; thus, NA Filter impact performance.12. Without filling-the-gaps, F1 drops 40.8%.325fiPado & LapataModerate improvements observed constituent-based models non-contentwords removed (NC filter). PerfMatch performs best condition. significantlybetter PerfMatch, EdgeCover Total Filter condition (p < 0.01), significantly worse constituent-based models NA filter condition (p < 0.01).NC filter yields significantly worse results WordBL (p < 0.01). surprising,since word-based model cannot recover words deleted filter,role-initial prepositions subordinating conjunctions.Note also combinations different filtering techniques appliedconstituent- word-based models. example, create constituent-based modelnon-aligned content words removed well non-arguments. sakebrevity, present results filter combinations generallyimprove results further. find combining filters tends remove large numberwords, result, good alignments also removed.Overall, obtain best performing models non-argument words removed(Arg Filter). Arg Filter aggressive filtering technique, since removes constituentslikely occupy argument positions. EdgeCover Total significantly betterPerfMatch Arg Filter condition (p < 0.01), perform comparably PerfMatch NA Filter applied. Moreover, EdgeCover Total construct almostidentical alignments. indicates two latter models behave similarlyalignment space reduced removing many possible bad alignments, despite imposingdifferent constraints structure bipartite graph. Interestingly, strict correspondence constraints imposed PerfMatch result substantially different alignments.Recall PerfMatch attempts construct bilingual one-to-one mapping arguments. direct correspondence identified source nodes, abstainsprojecting. result, alignment produced PerfMatch shows highest precisionamong models (88.8%), offset lowest recall (56.2%). results tieearlier analysis constituent alignments (Section 3.3), foundone-third corpus correspondences one-to-many type. Considerfollowing example:(15)Charter means [NP opportunity bring EU closer people].Die Charta bedeutet [NP eine Chance], [S die EU den Burgern naherzubringen].Charter means [NP chance], [S EU citizens bring closer].Ideally, English NP aligned German NP S. EdgeCover,model one-to-many relationships, acts confidently aligns NP Germanmaximize overlap similarity, incurring precision recall error. PerfMatch,hand, cannot handle one-to-many alignments acts cautiously makesrecall error aligning English NP empty node. Thus, accordingevaluation criteria, analysis EdgeCover deemed worse PerfMatch, eventhough former partly correct.sum, filtering improves resulting projections making syntactic analysessource target sentences similar other. Best results observedNA Filter (PerfMatch) Arg Filter conditions (Total EdgeCover). Finally, notebest models obtain precision figures close upper bound.326fiCross-lingual Annotation Projection Semantic RolesIntersective wordModelPrecWordBL52.9EdgeCover 86.6PerfMatch85.1UpperBnd85.0alignmentRecF147.4 50.075.2 80.573.3 78.883.0 84.0Manual word alignmentModelPrec RecF1WordBL76.1 73.9 75.0EdgeCover 86.0 81.8 83.8PerfMatch82.8 76.3 79.4UpperBnd85.0 83.0 84.0Table 7: Model performance test set intersective manual alignments. EdgeCover uses Arg Filter PerfMatch uses NA Filter. Best performing modelsindicated boldface.best recall values around 70%, significantly upper bound 83%. Asidewrongly assigned roles, recall errors due short semantic roles (e.g., pronouns),intersective word alignment often contain alignment links,projection cannot proceed.4.2.2 Test Setexperiments test set focus models obtained best resultsdevelopment set using specific filtering technique. particular, report performanceEdgeCover PerfMatch Arg Filter NA Filter conditions, respectively.addition, assess effect automatic word alignment models usingintersective manual word alignments.results summarized Table 7. intersective alignments used (lefthand side), EdgeCover performs numerically better PerfMatch, differencestatistically significant. corresponds findings development set.13right-hand side shows results manually annotated word alignments used.seen, performance WordBL increases sharply 50.0% 75.0% (F1).underlines reliance word-based model clean word alignments. Despiteperformance improvement, WordBL still lags behind best constituent-based modelapproximately 9% F1. means errors made word-based modelcorrected constituent-based models, mostly cases translation introducedmaterial target sentence cannot word-aligned expressions sourcesentence recovered filling-the-gaps heuristic. example shown below,translation clarification detailed explanation leads introduction twoGerman words, die naheren. words unaligned word level thusform part role word-based projection used.(16)[Commissioner Barniers clarification]Role[die naherenErlauterungen von KommissarBarnier]Role[the detailed explanations Commissioner Barnier]Role13. results test set slightly higher comparison development set. fluctuationreflects natural randomness partitioning corpus.327fiPado & LapataEvaluation conditionpredicatesVerbsPrec81.381.3Rec58.663.8F168.171.5Table 8: Evaluation Giuglea Moschittis (2004) shallow semantic parser English side parallel corpus (test set)Constituent-based models generally profit less cleaner word alignments. performance increases 1%3% F1. improvement due higher recall (approximately5% case EdgeCover) precision. words, main effect manually corrected word alignment make possible projection previously unavailableroles. EdgeCover performs close human upper bound gold standard alignmentsused. noise-free conditions able account one-to-many constituent alignments, thus models corpus better.Aside alignment noise, errors observe output modelsdue translational divergences, problematic monolingual role assignments,pronominal adverbs German. Many German verbs glauben (believe) exhibitdiathesis alternation: subcategorize either prepositional phrase (X glaubt Y,X believes Y), embedded clause must preceded pronominaladverb daran (X glaubt daran, dass Y, X believes Y). Even though pronominaladverb forms part complement clause (and therefore also role assigned it),English counterpart. contrast example (16) above, incomplete span dassforms complete constituent. Unless removed Arg Filter prior alignment,error cannot corrected use constituents.4.3 Experiment 2: Projection Automatic Rolesexperiment, evaluate projection models realistic setting, usingautomatic tools syntactic semantic parsing.4.3.1 Preprocessingexperiment, use uncorrected syntactic analyses bilingual corpusprovided Collins (1997) Dubeys (2005) parsers (cf. Section 2.1). automaticallyassigned semantic roles using state-of-the-art semantic parser developed GiugleaMoschitti (2004). trained parser FrameNet corpus (release 1.2) usingstandard features extended set based PropBank wouldrequired PropBank analysis entire FrameNet corpus.applied shallow semantic parser English side parallel corpusobtain semantic roles, treating frames given.14 task involves locating frameelements sentence finding correct label particular frame element. Table 8shows evaluation parsers output test set English gold standardannotation. Giuglea Moschitti report accuracy 85.2% role classification14. decomposition frame-semantic parsing general practice recent role labeling tasks,e.g. Senseval-3 (Mihalcea & Edmonds, 2004).328fiCross-lingual Annotation Projection Semantic RolesModelWordBLPerfMatchEdgeCoverUpperBndFilterNA FilterArg FilterPrec52.573.070.081.3Rec34.545.445.158.6F141.656.054.968.1Table 9: Performance best constituent-based model test set (automatic syntacticsemantic analysis, intersective word alignment)task, using standard feature set.15 results strictly comparable theirs,since identify role-bearing constituents addition assigning label.performance thus expected worse, since inherit errors frame elementidentification stage. Secondly, test set differs training data vocabulary(affecting lexical features) suffers parsing errors. Since Giuglea Moschittis(2004) implementation handle verbs, also assessed performance subsetverbal predicates (87.5% test tokens). difference complete verbs-onlydata sets amounts 3.4% F1, represents unassigned roles nouns.4.3.2 Setupreport results word-based baseline model, best projection modelsExperiment 1, namely PerfMatch (NA filter) EdgeCover (Arg Filter). usecomplete test set (including nouns adjectives) order make evaluation comparable Experiment 1.4.3.3 Resultsresults summarized Table 9. PerfMatch (NA Filter) EdgeCover(Arg Filter) perform comparably 5556% F1. approximately 25 points F1 worseresults obtained manual annotation (compare Table 9 Table 7). WordBLsperformance (now 41.6% F1) degrades less (around 8% F1), since affectedsemantic role assignment errors. However, consistently Experiment 1, constituentbased models sill outperform WordBL 10% F1 (p < 0.01). resultsunderscore ability bracketing information, albeit noisy, correct extend wordalignment. Although Arg Filter performed well Experiment 1, observe lesseffect here. Recall filter uses bracketing, also dominance information,therefore particularly vulnerable misparses. Like Experiment 1, findmodels yield overall high precision low recall. Precision drops 15% F1automatic annotations used, whereas recall drops 30%; however, note dropincludes 5% nominal roles fall outside scope shallow semantic parser.analysis showed parsing errors form large source problems Experiment 2. German verb phrases particularly problematic. Here, relatively freeword order combines morphological ambiguity produce ambiguous structures, since15. See work Giuglea Moschitti (2006) updated version shallow semantic parser.329fiPado & LapataBandError 0Error 1Error 2+Prec85.175.940.7Rec74.134.618.5F179.247.625.4Table 10: PerfMatchs performance relation error rate automatic semantic rolelabeling (Error 0: labeling errors, Error 1: one labeling error, Error 2+: twolabeling errors).third person plural verb forms (FIN) identical infinitival forms (INF). Considerfollowing English sentence, (17a), two syntactic analyses German translation,(17b)/(17c):(17)a.b.c.recognize [that work issue]wenn wir erkennenFIN , [dass wir [daran arbeitenINF ] mussenFIN ]recognize [that [work it]]wenn wir [erkennenINF , [dass wir daran arbeitenFIN ]] mussenFIN[recognize [we work it]](17b) gives correct syntactic analysis, parser used produced highlyimplausible (17c). result, English sentential complement workissue cannot aligned single German constituent, combination them.situation, PerfMatch generally align constituent thus sacrificerecall. EdgeCover (and Total) produce (wrong) alignment sacrifice precision.Finally, evaluated impact semantic role labeling errors projection. splitsemantic parsers output three bands: (a) sentences role labeling errors(Error 0, 35% test set), (b) sentences one labeling error (Error 1, 33%test set), (c) sentences two labeling errors (Error 2+, 31% test set).Table 10 shows performance best model, PerfMatch (NA filter),bands. seen, output automatic labeler error-free, projectionattains F1 79.2%, comparable results obtained Experiment 1.16 Even thoughprojection clearly degrades quality semantic role input, PerfMatch stilldelivers projections high precision Error 1 band. discussed above, lowrecall values bands Error 1 2+ result labelers low recall.5. Related WorkPrevious work annotation projection primarily focused annotations spanningshort linguistic units. range POS tags (Yarowsky & Ngai, 2001), NP chunks(Yarowsky & Ngai, 2001), dependencies (Hwa et al., 2002), word senses (Bentivogli &Pianta, 2005). different strategy cross-lingual induction frame-semantic information presented Fung Chen (2004), require parallel corpus. Instead,16. reasonable assume sentences, least relevant part syntactic analysiscorrect.330fiCross-lingual Annotation Projection Semantic Rolesuse bilingual dictionary construct mapping FrameNet entries concepts HowNet, on-line ontology Chinese.17 second step, use HowNetknowledge identify monolingual Chinese sentences predicates instantiateconcepts, label arguments FrameNet roles. Fung Chen report high accuracy values, method relies existence resources presumablyunavailable languages (in particular, rich ontology). Recently, Basili et al. (2009)propose approach semantic role projection word-based employ syntactic information. Using phrase-based SMT system, heuristically assembletarget role spans phrase translations, defining phrase similarity terms translation probability. method occupies middle ground word-based projectionconstituent-based projection.work described article relies parallel corpus harnessing informationsemantic correspondences. Projection works creating semantic alignmentsconstituents. latter correspond nodes bipartite graph, search bestalignment cast optimization problem. view computing optimal alignmentsgraph matching relatively widespread machine translation literature (Melamed,2000; Matusov, Zens, & Ney, 2004; Tiedemann, 2003; Taskar, Lacoste-Julien, & Klein, 2005).Despite individual differences, approaches formalize word alignment minimumweight matching problem, pair words bi-sentence associatedscore representing desirability pair. alignment bi-sentencehighest scoring matching constraints, example matchings must oneto-one. work applies graph matching level constituents compares largerclass constraints (see Section 3.3) previous approaches. example, Taskar et al.(2005) examine solely perfect matchings Matusov et al. (2004) edge covers.number studies addressed constituent alignment problem contextextracting translation patterns (Kaji, Kida, & Morimoto, 1992; Imamura, 2001). However,approaches search pairs constituents perfectly word aligned,infeasible strategy alignments obtained automatically. work focusesconstituent alignment problem, uses greedy search techniques guaranteedfind optimal solution (Matsumoto, Ishimoto, & Utsuro, 1993; Yamamoto & Matsumoto,2000). Meyers, Yangarber, Grishman (1996) propose algorithm aligning parsetrees applicable isomorphic structures. Unfortunately, restriction limitsapplication structurally similar languages high-quality parse trees.Although evaluate models semantic role projection task, believealso show promise context SMT, especially systems use syntacticinformation enhance translation quality. example, Xia McCord (2004) exploitconstituent alignment rearranging sentences source language makeword order similar target language. learn tree reordering rules aligningconstituents heuristically, using optimization procedure analogous total alignmentmodel presented article. similar approach described paper Collins, Koehn,Kucerova (2005); however, rules manually specified constituent alignment step reduces inspection source-target sentence pairs. different alignment17. information HowNet, see http://www.keenage.com/zhiwang/e_zhiwang.html.331fiPado & Lapatamodels presented article could easily employed reordering task commonapproaches.6. Conclusionsarticle, argued parallel corpora show promise relieving lexical acquisition bottleneck new languages. proposed annotation projection meansobtaining FrameNet annotations automatically, using resources available Englishexploiting parallel corpora. presented general framework projecting semanticroles capitalizes use constituent information projection, modelledcomputation constituent alignment search optimal subgraphbipartite graph. formalization allows us solve search problem efficiently usingwell-known graph optimization methods. experiments focused three modelingaspects: level noise linguistic annotation, constraints alignments, noisereduction techniques.found constituent information yields substantial improvements wordalignments. Word-based models offer starting point low-density languagesparsers available. However, word alignments noisy fragmentary deliveraccurate projections annotations long spans semantic roles. experiments compared contrasted three constituent-based models differassumptions regarding cross-lingual correspondence (total alignments, edge covers, perfect matchings). Perfect matchings, restrictive alignment model enforces one-to-onealignments, performed reliably across experimental conditions. particular,precision surpassed models. indicates strong semantic correspondenceassumed modelling strategy, least English German parsingtools available languages. side effect, performance constituent-basedmodels increases slightly manual word alignments used, meansnear-optimal results obtained using automatic alignments.far alignment noise reduction techniques concerned, find removingnon-aligned words (NA Filter) non-arguments (Arg Filter) yields best results.filters independent language pair make weak assumptionsunderlying linguistic representations question. choice best filter dependsgoals projection. Removing non-aligned words relatively conservative tendsbalance precision recall. contrast, aggressive filtering non-argumentsyields projections high precision low recall. Arguably, training shallow semanticparsers target language (Johansson & Nugues, 2006), desirableaccess high-quality projections. However, number options increasingprecision subsequent projection explored article. One fully automatic possibility generalization multiple occurrences predicate detectremove suspicious projection instances, e.g. following work Dickinson Lee(2008). Another direction postprocessing annotators, e.g., adopting annotateautomatically, correct manually methodology used provide high volume annotationPenn Treebank project. models could also used semi-supervised setting,e.g., provide training data unknown predicates.332fiCross-lingual Annotation Projection Semantic Rolesextensions improvements framework presented many varied.Firstly, believe models developed article useful semanticrole paradigms besides FrameNet, indeed types semantic annotation. Potential applications include projection PropBank roles18 , discourse structure,named entities. mentioned earlier, models also relevant machine translationcould used reordering constituents. results indicate syntacticknowledge target side plays important role projecting annotations longerspans. Unfortunately, many languages broad-coverage parsers available.However, may necessary obtain complete parses semantic role projectiontask. Two types syntactic information especially valuable here: bracketing information (which guides projection towards linguistically plausible role spans) knowledgearguments sentence predicates. Bracketing information acquiredunsupervised fashion (Geertzen, 2003). Argument structure information could obtaineddependency parsers (e.g., McDonald, 2006) partial parsers able identifypredicate-argument relations (e.g., Hacioglu, 2004). Another interesting direction concernscombination longer phrases, like provided phrase-based SMT systems,constituent information obtained output parser chunker.experiments presented article made use simple semantic similaritymeasure based word alignment. sophisticated approach could combinedalignment scores information provided bilingual dictionary. Inspired crosslingual information retrieval, Widdows, Dorow, Chan (2002) propose bilingual vectormodel. underlying assumption words similar co-occurrencesparallel corpus also semantically similar. Source target words representedn-dimensional vectors whose components correspond frequent content wordssource language. framework, similarity source-target word paircomputed using geometric measure cosine Euclidean distance. recentpolylingual topic models (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009) offerprobabilistic interpretation similar idea.article, limited parallel sentences frame preserved. allows us transfer roles directly source onto target languagewithout acquire knowledge possible translational divergences first. generalization framework presented could adopt strategy formmapping applied projection, akin transfer rules used machine translation.Thus far, explored models applying identity mapping. Knowledgepossible mappings acquired manually annotated parallel corpora (Pado& Erk, 2005). interesting avenue future work identify semantic role mappingsfully automatic fashion.Acknowledgmentsgrateful three anonymous referees whose feedback helped improvepresent article. Special thanks due Chris Callison-Burch linearb word alignment user interface, Ana-Maria Giuglea Alessandro Moschitti providing us18. Note, however, cross-lingual projection PropBank roles raises question interpretation; see discussion Fung et al. (2007).333fiPado & Lapatashallow semantic parser, annotators Beata Kouchnir Paloma Kreischer. acknowledge financial support DFG (Pado; grant Pi-154/9-2) EPSRC(Lapata; grant GR/T04540/01).ReferencesBasili, R., Cao, D., Croce, D., Coppola, B., & Moschitti, A. (2009). Cross-language framesemantics transfer bilingual corpora. Proceedings 10th International Conference Computational Linguistics Intelligent Text Processing, pp. 332345,Mexico City, Mexico.Bentivogli, L., & Pianta, E. (2005). Exploiting parallel texts creation multilingual semantically annotated resources: MultiSemCor corpus. Natural LanguageEngineering, 11 (3), 247261.Boas, H. C. (2005). Semantic frames interlingual representations multilingual lexicaldatabases. International Journal Lexicography, 18 (4), 445478.Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). Mathematicsstatistical machine translation: Parameter estimation. Computational Linguistics,19 (2), 263311.Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures lexical semanticrelatedness. Computational Linguistics, 32 (1), 1347.Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2006). Consistency coverage: Challenges exhaustive semantic annotation. 28. Jahrestagung,Deutsche Gesellschaft fur Sprachwissenschaft. Bielefeld, Germany.Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2009). Framenetsemantic analysis German: Annotation, representation automation.Boas, H. (Ed.), Multilingual FrameNet. Mouton de Gruyter. appear.Burchardt, A., & Frank, A. (2006). Approaching textual entailment LFGFrameNet frames. Proceedings RTE-2 Workshop, Venice, Italy.Burnard, L. (2000). Users Reference Guide British National Corpus (WorldEdition). British National Corpus Consortium, Oxford University Computing Service.Carreras, X., & Marquez, L. (2005). Introduction CoNLL-2005 shared task: Semanticrole labeling. Proceedings 9th Conference Computational Natural LanguageLearning, pp. 152164, Ann Arbor, MI.Carreras, X., Marquez, L., & Chrupala, G. (2004). Hierarchical recognition propositionalarguments perceptrons. Proceedings Eighth Conference Computational Natural Language Learning, pp. 106109, Boston, MA.Collins, M. (1997). Three generative, lexicalised models statistical parsing. Proceedings35th Annual Meeting Association Computational Linguistics, pp. 1623, Madrid, Spain.Collins, M., Koehn, P., & Kucerova, I. (2005). Clause restructuring statistical machine translation. Proceedings 43rd Annual Meeting AssociationComputational Linguistics, pp. 531540, Ann Arbor, MI.334fiCross-lingual Annotation Projection Semantic RolesCormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.Diab, M., & Resnik, P. (2002). unsupervised method word sense tagging usingparallel corpora. Proceedings 40th Annual Meeting AssociationComputational Linguistics, pp. 255262, Philadelphia, PA.Dickinson, M., & Lee, C. M. (2008). Detecting errors semantic annotation. Proceedings 6th International Conference Language Resources Evaluation,Marrakech, Morocco.Dorr, B. (1995). Machine translation divergences: formal description proposed solution. Computational Linguistics, 20 (4), 597633.Dowty, D. (1991). Thematic proto-roles argument selection. Language, 67, 547619.Dubey, A. (2005). lexicalization fails: parsing German suffix analysissmoothing. Proceedings 43rd Annual Meeting AssociationComputational Linguistics, pp. 314321, Ann Arbor, MI.Eiter, T., & Mannila, H. (1997). Distance measures point sets computation..Acta Informatica, 34 (2), 109133.Erk, K., Kowalski, A., Pado, S., & Pinkal, M. (2003). Towards resource lexical semantics: large German corpus extensive semantic annotation. Proceedings41st Annual Meeting Association Computational Linguistics, pp. 537544,Sapporo, Japan.Fillmore, C. J. (1968). case case. Bach, & Harms (Eds.), Universals LinguisticTheory, pp. 188. Holt, Rinehart, Winston, New York.Fillmore, C. J. (1982). Frame semantics. Linguistics Morning Calm, pp. 111137.Hanshin, Seoul, Korea.Fillmore, C. J., Johnson, C. R., & Petruck, M. R. (2003). Background FrameNet. International Journal Lexicography, 16, 235250.Fleischman, M., & Hovy, E. (2003). Maximum entropy models FrameNet classification. Proceedings 8th Conference Empirical Methods Natural LanguageProcessing, pp. 4956, Sapporo, Japan.Frank, A., Krieger, H.-U., Xu, F., Uszkoreit, H., Crysmann, B., Jorg, B., & Schafer, U.(2007). Question answering structured knowledge sources. Journal AppliedLogic, 5 (1), 2048.Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps uses improved networkoptimization algorithms. Journal ACM, 34 (3), 596615.Fung, P., & Chen, B. (2004). BiFrameNet: Bilingual frame semantics resources constructioncross-lingual induction. Proceedings 20th International ConferenceComputational Linguistics, pp. 931935, Geneva, Switzerland.Fung, P., Wu, Z., Yang, Y., & Wu, D. (2006). Automatic learning Chinese-Englishsemantic structure mapping. Proceedings IEEE/ACL Workshop SpokenLanguage Technology, Aruba.335fiPado & LapataFung, P., Wu, Z., Yang, Y., & Wu, D. (2007). Learning bilingual semantic frames: Shallowsemantic parsing vs. semantic role projection. Proceedings 11th ConferenceTheoretical Methodological Issues Machine Translation, pp. 7584, Skovde,Sweden.Geertzen, J. (2003). String alignment grammatical inference: suffix trees do.Masters thesis, ILK, Tilburg University, Tilburg, Netherlands.Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. ComputationalLinguistics, 28 (3), 245288.Gimenez, J., & Marquez, L. (2007). Linguistic features automatic evaluation heterogenous MT systems. Proceedings Second Workshop Statistical MachineTranslation, pp. 256264, Prague, Czech Republic.Giuglea, A.-M., & Moschitti, A. (2004). Knowledge discovery using FrameNet, VerbNetPropBank. Proceedings Workshop Ontology Knowledge Discovering15th European Conference Machine Learning, Pisa, Italy.Giuglea, A.-M., & Moschitti, A. (2006). Semantic role labeling via FrameNet, VerbNetPropBank. Proceedings 44th Annual Meeting AssociationComputational Linguistics, pp. 929936, Sydney, Australia.Grenager, T., & Manning, C. (2006). Unsupervised discovery statistical verb lexicon.Proceedings 11th Conference Empirical Methods Natural LanguageProcessing, pp. 18, Sydney, Australia.Hacioglu, K. (2004). lightweight semantic chunker based tagging. Proceedingsjoint Human Language Technology Conference Annual Meeting NorthAmerican Chapter Association Computational Linguistics, pp. 145148,Boston, MA.Hi, C., & Hwa, R. (2005). backoff model bootstrapping resources non-englishlanguages. Proceedings joint Human Language Technology ConferenceConference Empirical Methods Natural Language Processing, pp. 851858, Vancouver, BC.Hwa, R., Resnik, P., Weinberg, A., & Kolak, O. (2002). Evaluation translational correspondance using annotation projection. Proceedings 40th Annual MeetingAssociation Computational Linguistics, pp. 392399, Philadelphia, PA.Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsersvia syntactic projection across parallel texts. Journal Natural Language Engineering, 11 (3), 311325.Imamura, K. (2001). Hierarchical phrase alignment harmonized parsing.. Proceedings 6th Natural Language Processing Pacific Rim Symposium, pp. 377384,Tokyo, Japan.Jackendoff, R. S. (1990). Semantic Structures. MIT Press, Cambridge, MA.Johansson, R., & Nugues, P. (2006). FrameNet-Based Semantic Role Labeler Swedish.Proceedings 44th Annual Meeting Association Computational Linguistics, pp. 436443, Sydney, Australia.336fiCross-lingual Annotation Projection Semantic RolesJonker, R., & Volgenant, A. (1987). shortest augmenting path algorithm densesparse linear assignment problems. Computing, 38, 325340.Kaji, H., Kida, Y., & Morimoto, Y. (1992). Learning translation templates bilingualtext. Proceedings 14th International Conference Computational Linguistics, pp. 672678, Nantes, France.Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings joint Human Language Technology Conference Annual MeetingNorth American Chapter Association Computational Linguistics, pp.4854, Edmonton, AL.Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. Proceedings MT Summit X, Phuket, Thailand.Marquez, L., Carreras, X., Litkowski, K. C., & Stevenson, S. (2008). Semantic role labeling:introduction special issue. Computational Linguistics, 34 (2), 145159.Matsumoto, Y., Ishimoto, H., & Utsuro, T. (1993). Structural matching parallel texts.Proceedings ACL 31st Annual Meeting Association ComputationalLinguistics, pp. 2330, Columbus, OH.Matusov, E., Zens, R., & Ney, H. (2004). Symmetric word alignments statistical machingtranslation. Proceedings 20th International Conference ComputationalLinguistics, pp. 219225, Geneva, Switzerland.McDonald, R. (2006). Discriminative Training Spanning Tree Algorithms Dependency Parsing. Ph.D. thesis, University Pennsylvania.Melamed, I. D. (1998). Manual annotation translational equivalence: Blinker project.Tech. rep. IRCS TR #98-07, IRCS, University Pennsylvania.Melamed, I. D. (2000). Models translational equivalence among words. ComputationalLinguistics, 2 (23), 221249.Meyers, A., Yangarber, R., & Grishman, R. (1996). Alignment shared forests bilingual corpora. Proceedings 16th International Conference ComputationalLinguistics, pp. 460465, Copenhagen, Denmark.Mihalcea, R., & Edmonds, P. (Eds.). (2004). Proceedings Senseval-3: 3rd International Workshop Evaluation Systems Semantic Analysis Text,Barcelona, Spain.Miltsakaki, E., Prasad, R., Joshi, A., & Webber, B. (2004). Annotating discourse connectivesarguments. Proceedings NAACL/HLT Workshop FrontiersCorpus Annotation, Boston, MA.Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual topic models. Proceedings 14th Conference Empirical MethodsNatural Language Processing, pp. 880889, Singapore.Moschitti, A. (2008). Kernel methods, syntax semantics relational text categorization. Proceedings 17th ACM Conference Information KnowledgeManagement, pp. 253262, Napa Valley, CA.337fiPado & LapataMoschitti, A., Quarteroni, S., Basili, R., & Manandhar, S. (2007). Exploiting syntacticshallow semantic kernels question answer classification. Proceedings45th Annual Meeting Association Computational Linguistics, pp. 776783,Prague, Czech Republic.Narayanan, S., & Harabagiu, S. (2004). Question answering based semantic structures.Proceedings 20th International Conference Computational Linguistics, pp.693701, Geneva, Switzerland.Noreen, E. (1989). Computer-intensive Methods Testing Hypotheses: Introduction.John Wiley Sons Inc.Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignmentmodels. Computational Linguistics, 29 (1), 1952.Ohara, K. H., Fujii, S., Saito, H., Ishizaki, S., Ohori, T., & Suzuki, R. (2003). JapaneseFrameNet project: preliminary report. Proceedings 6th MeetingPacific Association Computational Linguistics, pp. 249254, Halifax, Nova Scotia.Pado, S., & Erk, K. (2005). cause cause: Cross-lingual semantic matchingparaphrase modelling. Proceedings EUROLAN Workshop Cross-LinguisticKnowledge Induction, Cluj-Napoca, Romania.Pado, S., & Lapata, M. (2005). Cross-lingual bootstrapping semantic lexicons.Proceedings 22nd National Conference Artificial Intelligence, pp. 10871092,Pittsburgh, PA.Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: annotatedcorpus semantic roles. Computational Linguistics, 31 (1), 71106.Postolache, O., Cristea, D., & Orasan, C. (2006). Tranferring coreference chainsword alignment. Proceedings 5th International Conference LanguageResources Evaluation, Genoa, Italy.Riloff, E., Schafer, C., & Yarowsky, D. (2002). Inducing information extraction systemsnew languages via cross-language projection. Proceedings 19th InternationalConference Computational Linguistics, pp. 828834, Taipei, Taiwan.Shen, D., & Lapata, M. (2007). Using semantic roles improve question answering.Proceedings 12th Conference Empirical Methods Natural Language Processing, pp. 1221, Prague, Czech Republic.Spreyer, K., & Frank, A. (2008). Projection-based acquisition temporal labeller.Proceedings 3rd International Joint Conference Natural Language Processing,pp. 489496, Hyderabad, India.Subirats, C., & Petruck, M. (2003). Surprise: Spanish FrameNet. ProceedingsWorkshop Frame Semantics, XVII. International Congress Linguists, Prague,Czech Republic.Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using predicate-argumentstructures information extraction. Proceedings 41st Annual MeetingAssociation Computational Linguistics, pp. 815, Sapporo, Japan.338fiCross-lingual Annotation Projection Semantic RolesSwier, R. S., & Stevenson, S. (2004). Unsupervised semantic role labelling. ProceedingsConference Empirical Methods Natural Language Processing, pp. 95102.Bacelona, Spain.Swier, R. S., & Stevenson, S. (2005). Exploiting verb lexicon automatic semanticrole labelling. Proceedings Joint Human Language Technology ConferenceConference Empirical Methods Natural Language Processing, pp. 883890,Vancouver, British Columbia.Taskar, B., Lacoste-Julien, S., & Klein, D. (2005). discriminative matching approachword alignment. Proceedings joint Human Language Technology ConferenceConference Empirical Methods Natural Language Processing, pp. 7380,Vancouver, BC.Tatu, M., & Moldovan, D. (2005). semantic approach recognizing textual entailment.Proceedings joint Human Language Technology Conference ConferenceEmpirical Methods Natural Language Processing, pp. 371378, Vancouver, BC.Taule, M., Mart, M., & Recasens, M. (2008). Ancora: Multilevel annotated corporaCatalan Spanish. Proceedings 6th International Conference LanguageResources Evaluation, Marrakesh, Morocco.Tiedemann, J. (2003). Combining clues word alignment. Proceedings 16thMeeting European Chapter Association Computational Linguistics,pp. 339346, Budapest, Hungary.Tokarczyk, A., & Frank, A. (2009). Cross-lingual projection LFG f-structures: Resourceinduction Polish. Proceedings Lexical Functional Grammar 2009, Cambridge,UK.van Leuven-Zwart, K. M. (1989). Translation original: Similarities dissimilarities.Target, 1 (2), 151181.Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment statistical translation. Proceedings 16th International Conference Computational Linguistics, pp. 836841, Copenhagen.Weeds, J. (2003). Measures Applications Lexical Distributional Similarity. Ph.D.thesis, University Sussex.Widdows, D., Dorow, B., & Chan, C.-K. (2002). Using parallel corpora enrich multilinguallexical resources. Proceedings 3rd International Conference LanguageResources Evaluation, pp. 240245, Las Palmas, Canary Islands.Wu, D., & Fung, P. (2009a). semantic role labeling improve SMT?. Proceedings13th Annual Conference European Association Machine Translation,pp. 218225, Barcelona, Spain.Wu, D., & Fung, P. (2009b). Semantic roles SMT: hybrid two-pass model. Proceedings joint Human Language Technology Conference Annual MeetingNorth American Chapter Association Computational Linguistics, pp.1316, Boulder, CO.339fiPado & LapataXia, F., & McCord, M. (2004). Improving statistical MT system automaticallylearned rewrite patterns. Proceedings 20th International ConferenceComputational Linguistics, pp. 508514, Geneva, Switzerland.Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Proceedings9th Conference Empirical Methods Natural Language Processing, pp. 8894, Barcelona, Spain.Xue, N., & Palmer, M. (2009). Adding semantic roles Chinese treebank. NaturalLanguage Engineering, 15 (1), 143172.Yamamoto, K., & Matsumoto, Y. (2000). Acquisition phrase-level bilingual correspondence using dependency structure. Proceedings 18th International ConferenceComputational Linguistics, pp. 933939, Saarbrucken, Germany.Yarowsky, D., & Ngai, G. (2001). Inducing multilingual POS taggers NP bracketers viarobust projection across aligned corpora. Proceedings 2nd Annual MeetingNorth American Chapter Association Computational Linguistics, pp.200207, Pittsburgh, PA.Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysistools via robust projection across aligned corpora. Proceedings 1st HumanLanguage Technology Conference, pp. 161168, San Francisco, CA.Yeh, A. (2000). accurate tests statistical significance result differences.Proceedings 18th International Conference Computational Linguistics, pp.947953, Saarbrucken, Germany.340fiJournal Artificial Intelligence Research 36 (2009) 547-556Submitted 06/09; published 12/09Research NoteSoft Goals Compiled AwayEmil Keyderemil.keyder@upf.eduUniversitat Pompeu FabraRoc Boronat, 13808018 Barcelona SpainHector Geffnerhector.geffner@upf.eduICREA & Universitat Pompeu FabraRoc Boronat, 13808018 Barcelona SpainAbstractSoft goals extend classical model planning simple model preferences.best plans ones least cost ones maximum utility,utility plan sum utilities soft goals achieved minusplan cost. Finding plans high utility appears involve two linked problems:choosing subset soft goals achieve finding low-cost plan achieve them. Newsearch algorithms heuristics developed planning soft goals,new track introduced International Planning Competition (IPC) testperformance. note, show however extensions needed:soft goals increase expressive power basic model planning actioncosts, easily compiled away. apply compilation problemsnet-benefit track recent IPC, show optimal satisficingcost-based planners better compiled problems optimal satisficing netbenefit planners original problems explicit soft goals. Furthermore, showpenalties, negative preferences expressing conditions avoid, also compiledaway using similar idea.1. ModelsSTRIPS problem tuple P = hF, I, O, Gi F set fluents, FG F initial state goal situation, set actions operatorsprecondition, add, delete lists P re(a), Add(a), Del(a) respectively,subsets F . action sequence = ha0 , . . . , applicable P actions ai ,= 0, . . . , n, O, exists sequence states hs0 , . . . , sn+1 i,s0 = I, P re(ai ) si si+1 = si Add(ai ) \ Del(ai ) = 0, . . . , n. applicableaction sequence achieves fluent g g sn+1 , plan P achieves goalg G, write |= G. classical setting, cost plan c() given||, number actions . cost structure generalized additioncost function operators:c2009AI Access Foundation. rights reserved.fiKeyder & GeffnerDefinition 1 STRIPS problem action costs tuple Pc = hF, I, O, G, ci,+P = hF, I, O, Gi STRIPS problem c function c :7 R+0 R0 standsnon-negative reals.cost plan problem Pc given||Xc() =c(ai )(1)i=1ai denotes th action . cost function c() = || obtained specialcase c(o) = 1 O. Adding utilities soft goals problem formulationresults new model:Definition 2 STRIPS problem action costs soft goals tuple Pu = hF, I, O,G, c, ui, P = hF, I, O, G, ci STRIPS problem action costs, u partialfunction u : F 7 R+ maps subset fluents (the soft goals) positive reals.STRIPS problem soft goals Pu , utility plan given differencetotal utility obtained plan cost:u() =Xu(p) c() .(2)p:|=pplan problem soft goals Pu optimal plan 0 utilityu( 0 ) higher u(). utility optimal plan problem hard goalsnever negative, empty plan non-negative utility zero cost.recent International Planning Competition (IPC6) featured Sequential Optimal Net Benefit Optimal tracks objective find optimal plansrespect models captured Equation 1 Equation 2 respectively (Helmert, Do,& Refanidis, 2008).12. EquivalenceGiven problem P soft goals, equivalent problem P 0 action costs softgoals defined whose plans encode corresponding plans P . transformation,first introduced Keyder Geffner (2007), simple direct, yet seems escaped attention researchers area (Smith, 2004; Sanchez & Kambhampati, 2005;Bonet & Geffner, 2008; Baier, Bacchus, & McIlraith, 2007). Also, unlike compilationsoft goals numeric variables arbitrary plan metrics (Edelkamp, 2006), proposedtransformation makes use neither requires planners ability handle1. PDDL3, soft goals represented expressions form ( u (is-violated hprefi)) appearingproblem metric pref preference soft goal associated formula A.single fluent, expression corresponds u(A) = u terminology used here.competition benchmarks contain preferences form. general case arisescompound formula fluents considered Section 4.548fiSoft Goals Compiled Awayaction costs, basic functionality required satisficing track recent IPC(Helmert et al., 2008).2write actions tuples form = hPre(o), Eff(o)i, effectspositive (Adds) negative (Deletes). assume soft goal fluent p, P alsocontains fluent p representing negation. introduced standard way,adding p initial state p initially true, including p Add Deletelists actions deleting adding p respectively (Gazen & Knoblock, 1997; Nebel, 2000).problem P 0 action costs soft goals equivalent problem Psoft goals obtained following transformation:Definition 3 STRIPS problem action costs soft goals P = hF, I, O, G, c, ui,compiled STRIPS problem action costs P 0 = hF 0 , 0 , O0 , G0 , c0F 0 = F 0 (P ) S(P ) {normal-mode, end-mode}0 = S(P ) {normal-mode}G0 = G 0 (P )O0 = O00 {collect(p), f orgo(p) | p SG(P )} {end}c(o) O000u(p) = forgo(p)c (o) =0= collect(p) = endSG(P ) = {p | (p F ) (u(p) > 0)}0 (P ) = {p0 | p SG(P )}S(P ) = {p0 | p0 0 (P )}end = h{normal-mode}, {end-mode, normal-mode}icollect(p) = h{end-mode, p, p0 }, {p0 , p0 }iforgo(p) = h{end-mode, p, p0 }, {p0 , p0 }iO00 = {hPre(o) {normal-mode}, Eff(o)i | O}2. Edelkamps transformation associates soft goals p1 , . . . , pm numericvariables n1 , . . . , nm ,Pdomain {0, 1}. utility plan expressed U () = ni=1 ni u(pi ) cost(),u(pi ) represents utility associated soft goal pi ni represents value numeric variablefinal state achieved plan. transformation also eliminates soft goals, requiresplace plan metric whose terms (namely, whether variables u(pi ) 1 0) state-dependent.Current heuristics deal metrics (See Sections 3 4).549fiKeyder & Geffnersoft goal p P , transformation adds dummy hard goal p0 P 0achieved two ways: action collect(p) cost 0 requires ptrue, action forgo(p) cost equal utility p yet performedp false, equivalently p true. two actions usedend action makes fluent end-mode true, actions originalproblem P used fluent normal-mode true prior executionend action. Moreover, exactly one {collect(p), forgo(p)} appear soft goalp plan, delete shared precondition p0 , action makes true.way make normal-mode true deleted end action,plans 0 P 0 form 0 = h, end, 00 i, plan P 00 sequence|S 0 (P )| collect(p) forgo(p) actions order, former appearing |= p,latter otherwise.two problems P P 0 equivalent sense correspondenceplans P P 0 , corresponding plans ranked way.specifically, plan P , plan 0 P 0 extends end actionset collect forgo actions, plan cost c( 0 ) = u() + ,constant independent 0 . Finding optimal (maximum utility)plan P therefore equivalent finding optimal (minimum cost) plan 0 P 0 .Proposition 1 (Correspondence plans) applicable action sequenceP , let extension 0 denote sequence obtained appending end actionfollowed permutation actions collect(p) forgo(p) p SG(P ),|= p 6|= p respectively.plan P 0 plan P 0Proof: () new actions P 0 delete p F , hard goal achievedremain true final state reached 0 , 0 |= G. p Fu(p) > 0, either |= p 6|= p. first case, p0 achieved collect(p),second, forgo(p), therefore 0 |= 0 (P ). Since G0 = G 0 (P ), 0 |= G0 .() 0 plan P 0 , hard goals G P must made true 0end action, action collect forgo actions appliedmake p F true. plan obtained removing end action collectforgo actions must therefore achieve G thus valid plan P .2Proposition 2 (Correspondence utilities costs) Let 1 2 twoplans P , let 10 20 extensions 1 2 respectively. Then,u(1 ) > u(2 ) c(10 ) < c(20 )00Proof: LetP plan P extension . demonstrate c( ) =u() + pSG(P ) u(p). Since summation expression constant givenproblem P , assertion follows directly:550fiSoft Goals Compiled AwayXc( 0 ) = c() + c0 (end) +c0 (forgo(p)) +forgo(p) 0X= c() +Xc0 (collect(p))collect(p) 0c0 (forgo(p))forgo(p) 0= c() +Xu(p)p:6|=pX= c() +u(p)Xu(p)p:|=ppSG(P )= u() +Xu(p)pSG(P )2Proposition 3 (Equivalence) Let plan P , 0 plan P 0 extends. Then,optimal plan P 0 optimal plan P 0Proof: Direct two propositions above.2following section, empirically compare performance net-benefit plannersproblems P explicit soft goals sequential planners problems P 0soft goals compiled away. order improve latter, maketransformation Definition 3 effective simple trick. Recall singleplan P , many extensions 0 P 0 , containing actionscost, differing way collect forgo actions ordered.efficiency purposes, implementation enforces fixed arbitrary ordering p1 , . . . , pmsoft goals P adding dummy hard goal p0i precondition actionscollect(pi+1 ) forgo(pi+1 ) = 1, . . . , 1. result single possibleextension 0 every plan P , space plans search therefore reduced.optimization used experiments reported below.3. Experimental Resultsformal results imply best plans problem P action costssoft goals computed looking best plans compiled problem P 0action costs soft goals, standard classical planning techniquesapplied. test practical value transformation, evaluate performanceoptimal satisficing planning techniques soft goals. problems testsuite contain preferences conjunctions rather single fluents. preferenceshandled variant approach described above, detailed Section 4.results shown three columns Table 1 labelled Net-benefit optimal plannersresults reported organizers 2008 International Planning Competition(IPC6) (Helmert et al., 2008). results obtained using machines551fiKeyder & GeffnerDomaincrewplanning(30)elevators (30)openstacks (30)pegsol (30)transport (30)woodworking (30)totalNet-benefit optimal plannersGamer HSP*P Mips-XXL41681154752240231212913119714955Sequential optimal plannersGamer HSP*F HSP*0 Mips-XXL8218198836461222614221515910147717850Table 1: Coverage optimal planners: leftmost three columns give number problemssolved planners Net Benefit Optimal track IPC6, reportedcompetition organizers. rightmost four columns give number compiledproblems solved Sequential Optimal versions planners. Dashes indicateversion planner could run domain.settings used competition: Xeon Woodcrest computers clock speeds 2.33GHz, time limit 30 minutes memory limit 2GB.first set experiments, consider problems used Net Benefit Optimal(NBO) track IPC6, soft goals defined terms goal-state preferences(Gerevini & Long, 2006), compare results obtained three optimal netbenefit planners results obtained Sequential Optimal (SO) variantscompilations.3 three planners entered NBO track IPC6 Gamer,Mips-XXL, HSP*P . planners test compiled versions NBOproblems versions Gamer (Edelkamp & Kissmann, 2008) Mips-XXL(Edelkamp & Jabbar, 2008) two planners HSP*F HSP*0 (Haslum, 2008).4ranked first, fifth, second, third, respectively, track (Helmertet al., 2008). Three six domains NBO track IPC6 involve numericvariables appear preconditions actions. version Gamerhandle numeric variables, therefore unable run Gamer problems.Numeric variables never appear soft goals left untouched compilation.data Table 1 show two HSP* planners track runcompiled problems well as, better than, best planner NBO track runoriginal problems soft goals. maximum number solved problemsdomain higher NBO track planners single domain, openstacks (7 vs. 6).domains, planners able solve larger number problems3. compiled problems currently available http://ipc.informatik.uni-freiburg.de/Domains.4. versions HSP* bug may cause suboptimal invalid solutions computeddomains non-monotonic numeric variables (numeric variables whose values may increasedecrease) occur preconditions actions goals (See http://ipc.informatik.uni-freiburg.de/Planners). variables present transport domain tested, yet planscomputed HSP* versions domain turn valid (as verified VAL planvalidator, Howey & Long, 2003) optimal instances checkedcosts plans computed planners.552fiSoft Goals Compiled AwayDomainelevators (30)openstacks (30)pegsol (30)rovers (20)totalNet-benefit satisficing plannersSGPlan YochanPS Mips-XXL008202052382110734Cost satisficing plannersLama2328291797Table 2: Coverage quality satisficing planners: entries indicate number problemsplanner generated best quality plan.maximum number solved NBO planner. Considering performance NBOvariants planner, compilation benefits two versionsheuristic search planner HSP* , leaving BDD planners Gamer Mips-XXL relativelyunaffected. Interestingly, HSP*0 using compilation ends solving problemsGamer, winner NBO track (78 vs. 71). drastically better performanceversions HSP* compared net-benefit version result simple schemehandling soft goals latter, optimal plans computed possiblesubset soft goals problem (roughly), change search algorithmIDA* A*.second set experiments, consider three domains NBO trackIPC6 contain numeric variables preconditions actions,domain rovers net-benefit track IPC5. Domains containing numeric variablespreconditions actions considered due lack state-of-the-art cost-basedplanners able handle them. Domains rovers NB track IPC5considered contain disjunctive, existentially qualified, universally qualifiedsoft goals current implementation support. satisficing net-benefitplanners test problems SGPlan (Hsu & Wah, 2008), winner netbenefit track IPC5, YochanPS (Benton, Do, & Kambhampati, 2009), receiveddistinguished performance award competition, satisficing variantMIPS-XXL, also received distinguished performance award competitioncompeted optimal track IPC6. solve compiled versions problemsLAMA, winner sequential satisficing track IPC6. YochanPS, MIPS-XXL,LAMA anytime planners, results discussed refer costbest plan found end evaluation period 30 minutes.Entries Table 2 show number problems domain plangenerated planner best plan produced. report data rathershowing graphs plan utilities absolute difference quality plansmeaningful except shortest plans (that ignore costs and/or soft goals)problem significantly costly. results show running state-of-the-artcost-based planner compiled problems yields best plan 98 total110 instances, almost three times number instances best-performingnative soft goals planner, MIPS-XXL, gives best plan. Furthermore, 22 23553fiKeyder & Geffnerproblems MIPS-XXL finds best plan pegsol domain, LAMA finds planquality. problems satisficing net-benefit planners outperformLAMA run compiled problems therefore few.results appear contradict results reported Benton et al. (2009),native net-benefit planner, YochanPS , yields better results cost-based planner,YochanCOST , run problems compiled according earlier version transformation(Keyder & Geffner, 2007). discrepancy appears result non-informativecost-based heuristic used YochanCOST , leads plans forgo soft goals,fact make use optimization discussed end Section 2,results unnecessary blowup state space. analysis differencesrecent cost-based planners, see paper Keyder Geffner (2008).4. Extensionsshown possible compile away positive utilities u(p) associatedsingle fluents p. show compilation extended deal positiveutilities defined formulas fluents negative utilities defined singlefluents formulas. Negative utilities stand conditions avoided rathersought; example, utility u(p q) = 10 penalizes plan results statep q true extra cost 10. compilation soft goals definedformulas based standard compilation goal precondition formulas classicalplanning (Gazen & Knoblock, 1997; Nebel, 1999).positive utility logical formula compiled away introducing newfluent pA achieved zero cost end state holds,assigning utility associated pA . DNF formula D1 . . . Dn ,suffices add n new actions a1 , . . . , ai = hDi , pA = 1, . . . , n.CNF formula C1 . . . Cn , fluent pi introduced = 1, . . . , n, alongactions aij = hCij , pi j = 1, . . . , |Ci |, Cij stands jth fluent Ci . alsointroduce action = h{p1 , . . . , pn }, pA allows addition fluent pA statesholds. newly introduced actions zero cost, must applicableP 0 actions original problem P collect forgo actions.best extensions plan achieves P achieve pA use collectaction achieve hard goal fluent p0A associated pA zero cost.negative utility u(A) < 0 formula DNF CNF compiled awaytwo steps, first substituting positive utility u(A) negationcompiling positive utility formula utility single fluent describedabove. makes use fact negation formula CNF formulaDNF vice versa.5. Summaryshown soft goals add expressive power easily compiledaway. implies new search algorithms heuristics strictly requiredhandling them. practical standpoint, experiments indicate state-of-the-artsequential planners outperform state-of-the-art net-benefit planners compiled versions554fiSoft Goals Compiled Awaybenchmarks used recent planning competitions. Furthermore, similar transformationsused compile away positive negative utilities logical formulas DNFCNF.Acknowledgmentsthank Malte Helmert help compiling running many IPC6 planners,Patrik Haslum help aspects various versions HSP, J. Bentonhelp compiling running YochanPS . H. Geffner partially supported GrantTIN2006-15387-C03-03 MEC, Spain.ReferencesBaier, J. A., Bacchus, F., & McIlraith, S. A. (2007). heuristic search approach planningtemporally extended preferences. Proc. IJCAI-07, pp. 18081815.Benton, J., Do, M., & Kambhampati, S. (2009). Anytime heuristic search partial satisfaction planning. Artificial Intelligence, 173 (5-6), 562592.Bonet, B., & Geffner, H. (2008). Heuristics planning penalties rewards formulated logic computed circuits. Artificial Intelligence, 172 (12-13),15791604.Edelkamp, S. (2006). compilation plan constraints preferences. Proc.ICAPS-06, pp. 374377.Edelkamp, S., & Jabbar, S. (2008). MIPS-XXL: Featuring external shortest path searchsequential optimal plans external branch-and-bound optimal net benefit.6th. Int. Planning Competition Booklet (ICAPS-08).Edelkamp, S., & Kissmann, P. (2008). Gamer: Bridging planning general game playingsymbolic search. 6th. Int. Planning Competition Booklet (ICAPS-08).Gazen, B., & Knoblock, C. (1997). Combining expressiveness UCPOPefficiency Graphplan. Steel, S., & Alami, R. (Eds.), Proc. 4th European Conf.Planning, pp. 221233. Springer.Gerevini, A., & Long, D. (2006). Preferences soft constraints PDDL3. Proc.ICAPS-06 Workshop Preferences Soft Constraints Planning, pp. 4653.Haslum, P. (2008). Additive reversed relaxed reachability heuristics revisited. 6th.Int. Planning Competition Booklet (ICAPS-08).Helmert, M., Do, M., & Refanidis, I. (2008). IPC 2008 deterministic competition. 6th.Int. Planning Competition Booklet (ICAPS-08).Howey, R., & Long, D. (2003). VALs progress: automatic validation tool PDDL2.1used international planning competition. Proc. 2003 ICAPS WorkshopCompetition: Impact, Organization, Evaluation, Benchmarks.Hsu, C.-W., & Wah, B. W. (2008). SGPlan planning system IPC6. 6th. Int.Planning Competition Booklet (ICAPS-08).555fiKeyder & GeffnerKeyder, E., & Geffner, H. (2007). Set-additive TSP heuristics planning action costs soft goals. Proc. ICAPS-06 Workshop Heuristics DomainIndependent Planning.Keyder, E., & Geffner, H. (2008). Heuristics planning action costs revisited.Proc. 18th European Conference Artificial Intelligence, pp. 588592.Nebel, B. (1999). Compilation schemes: theoretical tool assessing expressivepower planning formalisms. Proc. KI-99: Advances Artificial Intelligence, pp.183194. Springer-Verlag.Nebel, B. (2000). compilability expressive power propositional planning.Journal Artificial Intelligence Research, 12, 271315.Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics selecting objectivesover-subscription planning problems. Proc. ICAPS-05, pp. 192201.Smith, D. E. (2004). Choosing objectives over-subscription planning. Proc. ICAPS-04,pp. 393401.556fiJournal Artificial Intelligence Research 36 (2009) 169Submitted 04/09; published 10/09DL-Lite Family RelationsAlessandro ArtaleDiego Calvaneseartale@inf.unibz.itcalvanese@inf.unibz.itKRDB Research CentreFree University Bozen-BolzanoPiazza Domenicani, 3 I-39100 Bolzano, ItalyRoman KontchakovMichael Zakharyaschevroman@dcs.bbk.ac.ukmichael@dcs.bbk.ac.ukDepartment Computer Science Information SystemsBirkbeck CollegeMalet Street, London WC1E 7HX, U.K.Abstractrecently introduced series description logics common moniker DLLite attracted attention description logic semantic web communities duelow computational complexity inference, one hand, ability representconceptual modeling formalisms, other. main aim article carrythorough systematic investigation inference extensions original DL-Litelogics along five axes: (i) adding Boolean connectives (ii) number restrictionsconcept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry,asymmetry, reflexivity, irreflexivity transitivity constraints, (v) adopting dropping unique name assumption. analyze combined complexity satisfiabilityresulting logics, well data complexity instance checking answeringpositive existential queries. approach based embedding DL-Lite logics suitable fragments one-variable first-order logic, provides useful insightsproperties and, particular, computational behavior.1. IntroductionDescription Logic (cf. Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003references therein) family knowledge representation formalisms developedpast three decades and, recent years, widely used various application areas as:conceptual modeling (Bergamaschi & Sartori, 1992; Calvanese et al., 1998b, 1999;McGuinness & Wright, 1998; Franconi & Ng, 2000; Borgida & Brachman, 2003; Berardi, Calvanese, & De Giacomo, 2005; Artale et al., 1996, 2007, 2007b),information data integration (Beeri, Levy, & Rousset, 1997; Levy & Rousset,1998; Goasdoue, Lattes, & Rousset, 2000; Calvanese et al., 1998a, 2002a, 2002b,2008; Noy, 2004; Meyer, Lee, & Booth, 2005),ontology-based data access (Dolby et al., 2008; Poggi et al., 2008a; Heymans et al.,2008),Semantic Web (Heflin & Hendler, 2001; Horrocks, Patel-Schneider, & van Harmelen, 2003).c2009AI Access Foundation. rights reserved.fiArtale, Calvanese, Kontchakov & ZakharyaschevDescription logics (DLs, short) underlie standard Web Ontology Language OWL,1process standardized W3C second edition, OWL 2.widespread use DLs flexible modeling languages stems fact that,similarly traditional modeling formalisms, structure domain interestclasses (or concepts, DL parlance) objects common properties. Propertiesassociated objects means binary relationships (or roles) objects.Constraints available standard DLs also resemble used conceptual modelingformalisms structuring information: is-a hierarchies (i.e., inclusions) disjointnessconcepts roles, domain range constraints roles, mandatory participationroles, functionality general numeric restrictions roles, covering within concepthierarchies, etc. DL knowledge base (KB), constraints combined formTBox asserting intensional knowledge, ABox collects extensional knowledgeindividual objects, whether object instance concept, two objectsconnected role. standard reasoning services DL KB include checkingconsistency (or satisfiability), instance checking (whether certain individual instanceconcept), logic entailment (whether certain constraint logically impliedKB). sophisticated services emerging support modular developmentontologies checking, example, whether one ontology conservative extensionanother respect certain vocabulary (see, e.g., Ghilardi, Lutz, & Wolter, 2006;Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008; Kontchakov, Wolter, & Zakharyaschev,2008; Kontchakov, Pulina, Sattler, Schneider, Selmer, Wolter, & Zakharyaschev, 2009).Description logics recently used provide access large amounts datahigh-level conceptual interface, relevance data integrationontology-based data access. setting, TBox constitutes conceptual, high-levelview information managed system, ABox physically storedrelational database accessed using standard relational database technology (Poggiet al., 2008a; Calvanese et al., 2008). fundamental inference service case answering queries ABox constraints TBox taken account. kindqueries often considered first-order conjunctive queries,correspond commonly used Select-Project-Join SQL queries. key propertiesapproach viable practice (i) efficiency query evaluation,ideal target traditional database query processing, (ii) query evaluationdone leveraging relational technology already used storing data.objectives mind, series description logicsthe DL-Lite familyhasrecently proposed investigated Calvanese, De Giacomo, Lembo, Lenzerini,Rosati (2005, 2006, 2008a), later extended Artale, Calvanese, Kontchakov,Zakharyaschev (2007a), Poggi, Lembo, Calvanese, De Giacomo, Lenzerini, Rosati(2008a). logics family meet requirements and, time,capable representing many important types constraints used conceptual modeling.particular, inference various DL-Lite logics done efficiently sizedata (data complexity) overall size KB (combined complexity):shown KB satisfiability logics polynomial combined complexity,answering queries AC0 data complexitywhich, roughly, means that, given1. http://www.w3.org/2007/OWL/2fiThe DL-Lite Family Relationsconjunctive query KB, query TBox rewritten (independentlyABox) union conjunctive queries ABox alone. (It emphasizeddata complexity measure important application context DL-Litelogics, since one reasonably assume size data largely dominates sizeTBox.) Query rewriting techniques implemented various systemsQuOnto2 (Acciarri, Calvanese, De Giacomo, Lembo, Lenzerini, Palmieri, & Rosati, 2005;Poggi, Rodriguez, & Ruzzi, 2008b), ROWLKit (Corona, Ruzzi, & Savo, 2009), Owlgres(Stocker & Smith, 2008) REQUIEM (Perez-Urbina, Motik, & Horrocks, 2009).also demonstrated (Kontchakov et al., 2008) developing, analyzing re-usingDL-Lite ontologies (TBoxes) supported efficient tools capable checking varioustypes entailment ontologies respect given vocabularies, particular,minimal module extraction tools (Kontchakov et al., 2009)which yet existricher languages.significance DL-Lite family testified fact forms basisOWL 2 QL, one three profiles OWL 2.3 OWL 2 profiles fragmentsfull OWL 2 language designed standardized specific applicationrequirements. According (the current version of) official W3C profiles document,purpose OWL 2 QL language choice applications use largeamounts data query answering important reasoning task.common denominator DL-Lite logics constructed far follows: (i) quantification roles inverses qualified (in words, conceptsform R.C must C = >) (ii) TBox axioms concept inclusions cannot represent kind disjunctive information (say, two concepts cover wholedomain). DL-Lite-related dialects designedwith aim capturingconceptual modeling constraints, somewhat ad hoc mannerby extendingcore language number constructs global functionality constraints,role inclusions restricted Boolean operators concepts (see Section 4 details).Although attempts made (Calvanese et al., 2006; Artale et al., 2007a;Kontchakov & Zakharyaschev, 2008) put original DL-Lite logics generalperspective investigate extensions variety DL constructs requiredconceptual modeling, resulting picture still remains rather fragmentary farcomprehensive. systematic investigation DL-Lite family relatives becomeeven urgent challenging view choice constructs includedspecification OWL 2 QL profile4 (in particular, OWL makeunique name assumption, UNA, usually adopted DL-Lite, uses equalitiesinequalities object names instead).main aim article fill gap provide thorough comprehensive understanding interaction various DL-Lite constructs impactcomputational complexity reasoning. achieve goal, consider spectrumlogics, classified according five mutually orthogonal features:(1) presence absence role inclusions;2. http://www.dis.uniroma1.it/quonto/3. http://www.w3.org/TR/owl2-profiles/4. http://www.w3.org/TR/owl2-profiles/#OWL_2_QL3fiArtale, Calvanese, Kontchakov & Zakharyaschev(2) form allowed concept inclusions, consider four classes, called core,Krom, Horn, Bool, exhibit different computational properties;(3) form allowed numeric constraints, ranging none, global functionalityconstraints only, arbitrary number restrictions;(4) presence absence unique name assumption (and equalities inequalities object names, assumption dropped);(5) presence absence standard role constraints disjointness, symmetry,asymmetry, reflexivity, irreflexivity, transitivity.resulting cases, investigate combined data complexity KB satisfiability instance checking, well data complexity query answering.obtained tight complexity results summarized Section 3.4 (Table 2 Remark 3.1).already mentioned, original motivation distinguishing feature logicsDL-Lite family lite-ness sense low computational complexityreasoning tasks (query answering AC0 data complexity tractable KB satisfiabilitycombined complexity). broader perspective take here, logicsmeet requirement, particular, Krom Bool concept inclusions.5 However,identify another distinguishing feature regarded natural logic-basedcharacterization DL-Lite family: embeddability one-variable fragment firstorder logic without equality function symbols. allows us relate complexityDL-Lite logics complexity corresponding fragments first-order logic, thusobtain deep insight underlying logical properties DL-Lite variant.example, upper complexity bounds established follow embeddingwell-known results classical decision problem (see, e.g., Borger, Gradel, & Gurevich,1997) descriptive complexity (see, e.g., Immerman, 1999).One interesting findings article number restrictions, evenexpressed locally, instead global role functionality, added original DL-Litelogics (under UNA without role inclusions) free, is, without changingcomputational complexity. first-order approach shows casesalso extend DL-Lite logics role constraints mentioned above, keepingcomplexity. also gives framework analyze effect adopting droppingUNA using (in)equalities object names. example, observeequality allowed language DL-Lite (which makes sense without UNA)query answering becomes LogSpace-complete data complexity, thereforefirst-order rewritable. also turns dropping UNA results P-hardnessreasoning (for combined data complexity) presence functionality constraints (NLogSpace-hardness shown Calvanese et al., 2008), NP-hardnessarbitrary number restrictions allowed.Another interesting finding dramatic impact role inclusions, combinednumber restrictions (or even functionality constraints), computational complexity reasoning. already observed Calvanese et al. (2006), combination increases data complexity instance checking membership LogSpace5. Note, way, logics Bool concept inclusions turn quite useful conceptualmodeling reasonably manageable computationally (Kontchakov et al., 2008).4fiThe DL-Lite Family RelationsNLogSpace-hardness. show situation actually even worse: datacomplexity, instance checking turns P-complete case core Hornlogics coNP-complete case Krom Bool logics; moreover, KB satisfiability,NLogSpace-complete combined complexity simplest core casei.e., efficiently tractable, role inclusions number restrictions used separatelybecomesExpTime-completei.e., provably intractable, used together.retain role inclusions functionality constraints language keepcomplexity within required limits, Poggi et al. (2008a) introduced another DL-Litedialect, called DL-LiteA , restricts interaction role inclusions functionality constraints. extend result showing DL-Lite logicslimited interaction role inclusions number restrictions still embedded one-variable fragment first-order logic, exhibit behaviorfragments role inclusions number restrictions.article structured following way. Section 2, introduce logicsextended DL-Lite family illustrate features conceptual modeling formalisms.Section 3, discuss reasoning services complexity measures analyzedfollows, give overview obtained complexity results. Section 4,place introduced DL-Lite logics context original DL-Lite family,discuss relationship OWL 2. Section 5, study combined complexityKB satisfiability instance checking, Section 6, consider data complexityproblems. Section 7, study data complexity query answering.Section 8, analyze impact dropping UNA adding (in)equalitiesobject names complexity reasoning. Section 9 concludes article.2. Extended DL-Lite Family Description LogicsDescription Logic (Baader et al., 2003) family logics studiedused knowledge representation reasoning since 1980s. DLs, elementsdomain interest structured concepts (unary predicates), propertiesspecified means roles (binary predicates). Complex concept role expressions(or simply concepts roles) constructed, starting set concept rolenames, applying suitable constructs, set available constructs dependsspecific description logic. Concepts roles used knowledge baseassert knowledge, intensional level, so-called TBox (T terminological),extensional level, so-called ABox (A assertional). TBox typicallyconsists set axioms stating inclusion concepts roles. ABox, oneassert membership objects (i.e., constants) concepts, pair objectsconnected role. DLs supported reasoning services, satisfiability checkingquery answering, rely logic-based semantics.2.1 Syntax Semantics Logics DL-Lite Familyintroduce (extended) DL-Lite family description logics, initiallyproposed aim capturing typical conceptual modeling formalisms, UMLclass diagrams ER models (see Section 2.2 details), maintaining good computational properties standard DL reasoning tasks (Calvanese et al., 2005). begin5fiArtale, Calvanese, Kontchakov & Zakharyaschevdefining logic DL-LiteHNbool , regarded supremum originalDL-Lite family (Calvanese et al., 2005, 2006, 2007b) lattice description logics.HNDL-LiteHNbool . language DL-Litebool contains object names a0 , a1 , . . . , concept namesA0 , A1 , . . . , role names P0 , P1 , . . . . Complex roles R concepts C languagedefined follows:Pk ,R::=Pk|B::=|Ak|q R,C::=B|C|C1 u C2 ,q positive integer. concepts form B called basic.DL-LiteHNbool TBox, , finite set concept role inclusion axioms (or simplyconcept role inclusions) form:C1 v C2R1 v R2 ,ABox, A, finite set assertions form:Ak (ai ),Ak (ai ),Pk (ai , aj )Pk (ai , aj ).Taken together, constitute DL-LiteHNbool knowledge base K = (T , A).following, denote role(K) set role names occurring A, role (K)set {Pk , Pk | Pk role(K)}, ob(A) set object names A. role R,set:(Pk , R = Pk ,inv(R) =Pk , R = Pk .usual description logic, interpretation, = (I , ), consists nonemptydomain interpretation function assigns object name ai elementaIi , concept name Ak subset AIk domain, role namePk binary relation PkI domain. Unless otherwise stated, adoptunique name assumption (UNA):aIi 6= aIj6= j.(UNA)However, shall always indicate results depend UNAnot, depend assumption, discuss also consequencesdropping (see also Sections 4 8).role concept constructs interpreted standard way:(Pk )I = {(y, x) | (x, y) PkI },( q R)= ,= x | ]{y | (x, y) RI } q ,(C)I = \ C ,(C1 u C2 )=C1I(inverse role)(the empty set)(at least q R-successors)(not C)C2I ,(both C1 C2 )6fiThe DL-Lite Family Relations]X denotes cardinality X. use standard abbreviationsC1 C2 = (C1 u C2 ),> = ,R = ( 1 R),q R = ( q + 1 R).Concepts form q R q R called number restrictions, formR called existential concepts.satisfaction relation |= also standard:|= C1 v C2iffC1I C2I ,|= R1 v R2iffR1I R2I ,|= Ak (ai )iff aIi AIk ,|= Pk (ai , aj )iff(aIi , aIj ) PkI ,|= Ak (ai )iff aIi/ AIk ,|= Pk (ai , aj )iff(aIi , aIj )/ PkI .knowledge base K = (T , A) said satisfiable (or consistent) interpretation, I, satisfying members A. case write |= K (as well|= |= A) say model K (and A).languages DL-Lite family investigate article obtained restricting language DL-LiteHNbool along three axes: (i) Boolean operators (bool )concepts, (ii) number restrictions (N ) (iii) role inclusions, hierarchies (H).Similarly classical logic, adopt following definitions. DL-LiteHNbool TBoxcalled Krom TBox 6 concept inclusions restricted to:B1 v B2 ,B1 v B2B1 v B2(Krom)(here Bi B basic concepts). called Horn TBoxconcept inclusions restricted to:lBk v B(Horn)k(by definition, empty conjunction >). Finally, call core TBoxconcept inclusions restricted to:B1 v B2B1 v B2 .(core)B1 v B2 equivalent B1 u B2 v , core TBoxes regarded sittingintersection Krom Horn TBoxes.Remark 2.1 sometimes use conjunctionsright-hand side concept includsions restricted languages: C v k Bk . Clearly, syntactic sugar addextra expressive power.HNHNHNDL-LiteHNkrom , DL-Litehorn DL-Litecore . fragments DL-Litebool Krom,HNHNHorn, core TBoxes denoted DL-Litekrom , DL-Litehorn DL-LiteHNcore , respectively. fragments obtained limiting use number restrictions roleinclusions.6. Krom fragment first-order logic consists formulas prenex normal form whose quantifier-freepart conjunction binary clauses.7fiArtale, Calvanese, Kontchakov & ZakharyaschevHNDL-LiteH. fragment DL-Lite , {core, krom, horn, bool}, without numberrestrictions q R, q 2, (but role inclusions) denoted DL-LiteH. NoteHthat, DL-Lite , still use existential concepts R (that is, 1 R).HFDL-LiteHFfragment DL-LiteHNnumber. Denote DL-Literestrictions q R, existential concepts (with q = 1) q = 2occur concept inclusions form 2 R v . inclusion calledglobal functionality constraint states role R functional (more precisely,|= ( 2 R v ) (x, y) RI (x, z) RI , = z).FDL-LiteN, DL-Lite DL-Lite . role inclusions excluded language,{core, krom, horn, bool} obtain three fragments: DL-LiteN(with arbitrary number restrictions), DL-LiteF(withfunctionalityconstraintsexistentialconceptsR), DL-Lite (without number restrictions different R).shall see later article, logics form DL-LiteHFDL-LiteHN,even = core, turn computationally rather costly interactionrole inclusions functionality constraints (or, generally, number restrictions). hand, purpose conceptual modeling one may needconstructs; cf. example Section 2.2. compromise found artificiallylimiting interplay role inclusions number restrictions way similarlogic DL-LiteA proposed Poggi et al. (2008a).TBox , let vT denote reflexive transitive closure relation(R, R0 ), (inv(R), inv(R0 )) | R v R0let R R0 iff R vT R0 R0 vT R. Say R0 proper sub-role RR0 vT R R06 R.(HN )(HN )DL-Lite . introduce logics DL-Lite ,{core, krom, horn, bool},HNwhich, one hand, restrict logics DL-Lite limiting interactionrole inclusions number restrictions order reduce complexity reasoning, and,hand, include additional constructs, limited qualified existential quantifiers, role disjointness, (a)symmetry (ir)reflexivity constraints, increaseexpressive power logics affect computational properties.(HN )DL-LiteTBoxes must satisfy conditions (A1 )(A3 ) below. (We remindreader occurrence concept right-hand (left-hand) side conceptinclusion called negative scope odd (even) number negations ;otherwise occurrence called positive.)(A1 ) may contain positive occurrences qualified number restrictions q R.C,C conjunction concepts allowed right-hand side -conceptinclusions;(A2 ) q R.C occurs , contain negative occurrences numberrestrictions q 0 R q 0 inv(R) q 0 2;(A3 ) R proper sub-role , contain negative occurrencesq R q inv(R) q 2.8fiThe DL-Lite Family Relationsroleroleconstraints inclusionsyesdisj.(a)sym.(ir)ref.disj.(a)sym.(ir)ref.tran.a)numberrestrictionsRR/funct.qRRR/funct.qRconcept inclusionsKromHornDL-Litekrom DL-LitehornDL-LiteFDL-LiteFkromhornNDL-Litekrom DL-LiteNhornHDL-LiteHDL-LitekromhornHFDL-Litekrom DL-LiteHFhornHNDL-LiteHNDL-LitekromhorncoreDL-LitecoreDL-LiteFcoreDL-LiteNcoreDL-LiteHcoreDL-LiteHFcoreDL-LiteHNcore(HF )BoolDL-LiteboolDL-LiteFboolDL-LiteNboolDL-LiteHboolDL-LiteHFboolDL-LiteHNbool(HF )(HF ))R.C/funct.a) DL-Lite(HFDL-Litekrom DL-Litehorn DL-Liteboolcore(HN )(HN )(HN )(HN )a)q R.CDL-LitecoreDL-Litekrom DL-Litehorn DL-Liteboolyes+)R.C/funct.a) DL-Lite(HFcoreyesq R.C a)(HF )+DL-Litekrom+(HN )+)DL-Lite(HNDL-Litekromcore(HF )+DL-Litehorn(HN )+DL-Litehorn(HF )+DL-Litebool(HN )+DL-Liteboolrestricted (A1 )(A3 ).Table 1: extended DL-Lite family.(HN )(It follows DL-LiteTBox contain both, say, functionality constraint2 R v occurrence q R.C, q 1.)(HN )Additionally, DL-LiteTBoxes contain role constraints (or axioms) form:Dis(R1 , R2 ),Asym(Pk ),Sym(Pk ),Irr(Pk ),Ref(Pk ).meaning new constructs defined usual: interpretation = (I , ),( q R.C)I = x | ]{y C | (x, y) RI } q ;|= Dis(R1 , R2 )|= Asym(Pk )|= Sym(Pk )|= Irr(Pk )|= Ref(Pk )iffiffiffiffiffR1I R2I = (roles R1 R2 disjoint);PkI (Pk )I = (role Pk asymmetric);PkI = (Pk )I(Pk symmetric);(x, x)/ PkI x(x, x) PkI x(Pk irreflexive);(Pk reflexive).emphasized extra constructs often used conceptual modeling(HN )introduction DL-Litemotivated OWL 2 QL proposal. (Note(HN )DL-Litecontains DL-LiteHDL-LiteNproper fragments.)(HN )+(HN )+DL-Lite.{bool, horn, krom, core}, denote DL-Liteextension(HN )DL-Literole transitivity constraints form Tra(Pk ), meaningexpected:|= Tra(Pk )iff(x, y) PkI (y, z) PkI imply (x, z) PkI , x, y, z(Pk transitive).9fiArtale, Calvanese, Kontchakov & ZakharyaschevDL-LiteboolDL-LiteHN@@DL-LitehornDL-Litekrom@@DL-LitecorePP6DL-LiteHF6DL-LiteHDL-LiteN(HN ) )+DL-LiteDL-Lite(HN1666PP1DL-LiteF6iPPDL-Lite)+(HF ) DL-LiteDL-Lite(HFFigure 1: Language inclusions extended DL-Lite family.remind reader standard restriction limiting use transitive roles DLs(see, e.g., Horrocks, Sattler, & Tobies, 2000):simple roles R allowed concepts form q R, q 2,simple role given TBox understand role without transitive sub-roles(including itself). particular, contains Tra(P ) P P simple,cannot contain occurrences concepts form q P q P , q 2.(HF )(HF )+(HF )DL-LiteDL-Lite .also define languages DL-Litesub-languages(HN )DL-Lite ,number restrictions form R, R.C functionalityconstraints 2 R v allowedprovided, course, satisfy (A1 )(A3 );(HF )+particular, R.C allowed R functional. before, DL-Liteextensions(HF )DL-Literole transitivity constraints (satisfying restriction above).Thus, extended DL-Lite family consider article consists 40 differentlogics collected Table 1. inclusions logics shown Figure 1.obtained taking product left- right-hand parts picture,subscript right-hand part ranges {core, krom, horn, bool}, i.e.,subscripts left-hand part, similarly, superscript left-hand partranges { , F, N , H, HF, HN , (HF), (HN ), (HF)+ , (HN )+ }, i.e., superscriptsright-hand part.position logics relative DL-Lite logics known literatureOWL 2 QL profile discussed Section 4. starting Section 5, beginthorough investigation computational properties logics extended DLLite family, without UNA. illustrate expressivepower DL-Lite logics concrete example.2.2 DL-Lite Conceptual Modelingtight correspondence conceptual modeling formalisms, ER modelUML class diagrams, various description logics pointed variouspapers (e.g., Calvanese et al., 1998b, 1999; Borgida & Brachman, 2003; Berardi et al.,2005). give example showing DL-Lite logics used conceptualmodeling purposes; details see work Artale et al. (2007b).10fiThe DL-Lite Family Relations1..1Employee1..*empCode: Integersalary: IntegerworksOnboss3..*ProjectManagerprojectName: String1..*1..1{disjoint, complete}AreaManagermanagesTopManager1..1Figure 2: UML class diagram.Let us consider UML class diagram depicted Figure 2 representing (a portionof) company information system. According diagram, managers employees partitioned area managers top managers. informationrepresented means following concept inclusions (where brackets specifyminimal DL-Lite language inclusion belongs to):Manager v Employee(DL-Litecore )AreaManager v Manager(DL-Litecore )TopManager v Manager(DL-Litecore )AreaManager v TopManager(DL-Litecore )Manager v AreaManager TopManager(DL-Litebool )employee two functional attributes, empCode salary, integer values.Unlike OWL, distinguish abstract objects data values. Hencemodel datatype, Integer , means concept, attribute,employees salary, means role. Thus, salary represented follows:Employee v salarysalary(DL-Litecore )v Integer(DL-Litecore )(DL-LiteFcore )2 salary vfunctional attribute empCode values Integer represented way.binary relationship worksOn Employee domain Project range:worksOn v EmployeeworksOn(DL-Litecore )v Project(DL-Litecore )binary relationship boss domain Employee range Manager treated analogously. employee works project exactly one boss, project must11fiArtale, Calvanese, Kontchakov & Zakharyaschevinvolve least three employees:Employee v worksOn(DL-Litecore )Employee v boss(DL-Litecore )(DL-LiteFcore )2 boss vProject v 3 worksOn(DL-LiteNcore )top manager manages exactly one project also works project, projectmanaged exactly one top manager:manages v TopManagermanagesv Project(DL-Litecore )TopManager v managesProject v manages(DL-Litecore )(DL-Litecore )(DL-Litecore )2 manages v(DL-LiteFcore )2 manages v(DL-LiteFcore )(DL-LiteHcore )manages v worksOnall, languages extended DL-Lite family capable representing(HN )UML class diagram Figure 2 DL-LiteHNbool DL-Litebool . Note, however, except covering constraint, Manager v AreaManager TopManager , conceptinclusions DL-Lite translation UML class diagram belong variants(HN )core fragments DL-LiteHNcore DL-Litecore . hard imagine situationone needs Horn concept inclusions represent integrity constraints UML class diagrams, example, express (together axioms) chief executiveofficer may work five projects manager one them:CEO u ( 5 worksOn) u manages v(DL-LiteNhorn )context UML class diagrams, Krom fragment DL-Litekrom (with variants)seems useless: extends DL-Litecore concept inclusions form B1 v B2or, equivalently, > v B1 B2 , rarely used conceptual modeling. Indeed,would correspond partitioning whole domain interest two parts,general useful covering constraints form B v B1 Bk require fullBool language. hand, Krom fragments important pinpointingborderlines various complexity classes description logics DL-Lite familyextensions; see Table 2.3. Reasoning DL-Lite Logicsdiscuss reasoning problems consider article, mutual relationships, complexity measures adopt. also provide overview complexityresults DL-Lite logics obtained article.12fiThe DL-Lite Family Relations3.1 Reasoning Problemsconcentrate three fundamental standard reasoning tasks descriptionlogics: satisfiability (or consistency), instance checking, query answering.DL L extended DL-Lite family, define L-concept inclusionconcept inclusion allowed L. Similarly, define notions L-KB L-TBox.Finally, define L-concept concept occur right-hand sideL-concept inclusion conjunction concepts.Satisfiability. KB satisfiability problem check, given L-KB K, whethermodel K. Clearly, satisfiability minimal requirement ontology.well known DL (Baader et al., 2003), many reasoning tasks description logicsreducible satisfiability problem. Consider, example, subsumption problem:given L-TBox L-concept inclusion C1 v C2 , decide whether |= C1 v C2 ,is, C1I C2I , every model . reduce problem (un)satisfiability, takefresh concept name A, fresh object name a, set K = (T 0 , A),0 = {A v C1 , v C2 }= {A(a)}.easysee |= C1 v C2 iff K satisfiable. core, Krom Horn KBs,C2 = k Dk , Dk (possibly negated) basic concept, checking unsatisfiabilityK amounts checking unsatisfiability KBs Kk = (Tk , A), Tk ={A v C1 , v Dk } (for Horn KBs, replace v B equivalent u B v ).concept satisfiability problemgiven L-TBox L-concept C, decidewhether C 6= model also easily reducible KB satisfiability. Indeed,take fresh concept name A, fresh object name a, set K = (T 0 , A),0 = {A v C}= {A(a)}.C satisfiable respect iff K satisfiable.Instance checking. instance checking problem decide, given object name a,L-concept C L-KB K = (T , A), whether K |= C(a), is, aI C , everymodel K. Instance checking also reducible (un)satisfiability: objectinstance L-concept C every model K = (T , A) iff KB K0 = (T 0 , A0 ),0 = {A v C}A0 = {A(a)},notdsatisfiable, fresh concept name. core, Krom Horn KBs,C = k Dk , Dk (possibly negated) basic concept, proceedsubsumption: checking unsatisfiability K0 amounts checking unsatisfiabilityKB Kk0 = (Tk0 , A0 ) Tk0 = {A v Dk }.Conversely, KB satisfiability reducible complement instance checking: Ksatisfiable iff K 6|= A(a), fresh concept name fresh object a.Query answering. positive existential query q(x1 , . . . , xn ) first-order formula(x1 , . . . , xn ) constructed means conjunction, disjunction existential quantification starting atoms Ak (t) Pk (t1 , t2 ), Ak concept name, Pk13fiArtale, Calvanese, Kontchakov & Zakharyaschevrole name, t, t1 , t2 terms taken list variables y0 , y1 , . . . listobject names a0 , a1 , . . . (i.e., positive existential formula). precisely,::=yi|::=Ak (t)ai ,|Pk (t1 , t2 )|1 2|1 2|yi .free variables called distinguished variables q bound ones nondistinguished variables q. write q(x1 , . . . , xn ) query distinguished variablesx1 , . . . , xn . conjunctive query positive existential query contains disjunction(it constructed atoms means conjunction existential quantification only).Given query q(~x) = (~x) ~x = x1 , . . . , xn n-tuple ~a object names,write q(~a) result replacing every occurrence xi (~x) ith member~a. Queries containing distinguished variables called ground (they also knownBoolean).Let = (I , ) interpretation. assignment function associatingevery variable element a(y) . use following notation: aI,a= aIiI,a = a(y). satisfaction relation positive existential formulas respectgiven assignment defined inductively taking:|=a Ak (t)ifftI,a AIk ,|=a Pk (t1 , t2 )iffI,a(tI,a1 , t2 ) Pk ,|=a 1 2iff|=a 1 |=a 2 ,|=a 1 2iff|=a 1 |=a 2 ,|=a yiiff|=b , assignment b may differ yi .ground query q(~a), satisfaction relation depend assignment a,write |= q(~a) instead |=a q(~a). answer query either yesno.KB K = (T , A), say tuple ~a object names certain answerq(~x) respect K, write K |= q(~a), |= q(~a) whenever |= K. queryanswering problem formulated follows: given L-KB K = (T , A), query q(~x),tuple ~a object names A, decide whether K |= q(~a).Note instance checking problem special case query answering: objectinstance L-concept C respect KB K iff answer query A(a)respect K0 yes, K0 = (T 0 , A) 0 = {C v A}, freshconcept name. Horn-concepts B1 u u Bk , consider query A1 (a) Ak (a)respect K0 , K0 = (T 0 , A) 0 = {B1 v A1 , . . . , Bk v Ak },Ai fresh concept names. Similarly, deal Krom-concepts D1 u u Dk ,Di possibly negated basic concept. core-concepts, reduction holdsconjunctions basic concepts.3.2 Complexity Measures: Data Combined Complexitycomputational complexity reasoning problems discussed analyzedrespect different complexity measures, depend parameters14fiThe DL-Lite Family Relationsproblem regarded input (i.e., vary) regardedfixed. satisfiability instance checking, parameters consider sizeTBox size ABox A, number symbols A,denoted |T | |A|, respectively. size |K| knowledge K = (T , A) simply given|T | + |A|. query answering, one parameter consider would sizequery. However, analysis adopt standard database assumption sizequeries always bounded reasonable constant and, case, negligiblerespect size TBox size ABox. Thus countquery part input.Hence, consider reasoning problems two complexity measures. wholeKB K regarded input, deal combined complexity. If, however,ABox counted input, TBox (and query) regardedfixed, concern data complexity (Vardi, 1982). Combined complexity intereststill designing testing ontology. hand, data complexitypreferable cases TBox fixed size (and size query)negligible compared size ABox, case, instance, contextontology-based data access (Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, & Rosati,2007) data intensive applications (Decker, Erdmann, Fensel, & Studer, 1999; Noy,2004; Lenzerini, 2002; Calvanese et al., 2008). Since logics DL-Lite familytailored deal large data sets stored relational databases, data complexityinstance checking query answering particular interest us.3.3 Remarks Complexity Classes LogSpace AC0paper, deal following complexity classes:AC0 ( LogSpace NLogSpace P NP ExpTime.definitions found standard textbooks (e.g., Garey & Johnson, 1979;Papadimitriou, 1994; Vollmer, 1999; Kozen, 2006). remind readertwo smallest classes LogSpace AC0 .problem belongs LogSpace two-tape Turing machine that,starting input length n written read-only input tape, stops accepting rejecting state used log n cells (initially blank) read/write worktape. LogSpace transducer three-tape Turing machine that, startedinput length n written read-only input tape, writes result (of polynomial size)write-only output tape using log n cells (initially blank) read/writework tape. LogSpace-reduction reduction computable LogSpace transducer;composition two LogSpace transducers also LogSpace transducer (Kozen,2006, Lemma 5.1).formal definition complexity class AC0 (see, e.g., Boppana & Sipser, 1990;Vollmer, 1999 references therein) based circuit model, functionsrepresented directed acyclic graphs built unbounded fan-in And,gates (i.e., gates may unbounded number incoming edges).definition assume decision problems encoded alphabet {0, 1}regarded Boolean functions. AC0 class problems definable using15fiArtale, Calvanese, Kontchakov & Zakharyaschevfamily circuits constant depth polynomial size, generateddeterministic Turing machine logarithmic time (in size input); lattercondition called LogTime-uniformity. Intuitively, AC0 allows us use polynomiallymany processors run-time must constant. typical example AC0 problemevaluation first-order queries databases (or model checking first-order sentencesfinite models), database (first-order model) regarded inputquery (first-order sentence) assumed fixed (Abiteboul, Hull, & Vianu, 1995;Vollmer, 1999). hand, undirected graph reachability problem knownLogSpace (Reingold, 2008) AC0 . Boolean function f : {0, 1}n {0, 1}called AC0 -reducible (or constant-depth reducible) function g : {0, 1}n {0, 1}(LogTime-uniform) family constant-depth circuits built And, Or,g gates computes f . case say AC0 -reduction. Notereductions considered Section 3.1 AC0 -reductions. Unless otherwise indicated,follows write reduction AC0 -reduction.3.4 Summary Complexity Resultsarticle, aim investigate (i) combined data complexity satisfiability instance checking problems (ii) data complexity query answeringproblem logics extended DL-Lite family, without UNA.(HF )+obtained known results first 32 logics Table 1 (the logics DL-Lite(HN )+DL-Liteincluded) summarized Table 2 (we remind readersatisfiability instance checking reducible complementsinstance checking special case query answering). fact, resultstable follow lower upper bounds marked [] [], respectively (bytaking account hierarchy languages DL-Lite family): example,NLogSpace membership satisfiability DL-LiteNkrom Theorem 5.7 impliesNFupper bound DL-Litekrom , DL-Litekrom , DL-Litecore , DL-LiteFcore DL-Litecore.sub-languages DL-LiteNkromRemark 3.1 Two complexity results noted (they includedTable 2):(i) equality object names allowed language DL-Lite,makes sense UNA dropped, AC0 memberships Table 2 replaced LogSpace-completeness (see Section 8, Theorem 8.3 8.9); inequalityconstraints affect complexity.(ii) extend languages role transitivity constraints combined complexity satisfiability remains same, data complexity, instancechecking query answering become NLogSpace-hard (see Lemma 6.3), i.e.,membership AC0 data complexity replaced NLogSpace-completeness,complexity results remain same.either case, property first-order rewritabilitythat is, possibility rewritinggiven query q given TBox single first-order query q0 returning certainanswers q (T , A) every ABox A, ensures query answering problemAC0 data complexityis lost.16fiThe DL-Lite Family RelationsComplexityLanguagesUNACombined complexitySatisfiability|H]DL-Lite[core[ |H]DL-Litehorn[ |H]DL-Litekrom[ |H]DL-Litebool|N |(HF )|(HN )]DL-Lite[Fcore[F |N |(HF )|(HN )]DL-Litehorn[F |N |(HF )|(HN )]DL-Litekrom[F |N |(HF )|(HN )]DL-Litebool[F |(HF )]DL-Litecore/horn[F |(HF )]DL-Litekrom[F |(HF )]DL-Litebool[N |(HN )]DL-Litecore/horn[N |(HN )]DL-Litekrom/boolDL-LiteHFcore/hornDL-LiteHFkrom/boolDL-LiteHNcore/hornHNDL-Litekrom/boolyes/noyesyes/noData complexityInstance checking0Query answeringNLogSpace [A]ACAC0P [Th.8.2] [A]AC0AC0 [C]0NLogSpace [Th.8.2]ACcoNP [B]NP [Th.8.2] [A]AC0 [Th.8.3]coNP0NLogSpaceACAC0P [Th.5.8, 5.13]AC0AC0 [Th.7.1]NLogSpace [Th.5.7,5.13]AC0coNP0NP [Th.5.6, 5.13]AC [Cor.6.2]coNPP [Cor.8.8] [Th.8.7]P [Th.8.7]PP [Cor.8.8]PcoNPNPP [Cor.8.8]coNPNP [Th.8.4]coNP [Th.8.4]coNPNP [Th.8.5]coNPcoNPExpTime [Th.5.10]P [Th.6.7]P [D]ExpTimecoNP [Th.6.5]coNPExpTimecoNP [Th.6.6]coNPExpTime [F]coNPcoNP [E][A] complexity respective fragment propositional Boolean logic[B] follows proof data complexity result instance checking ALE (Schaerf, 1993)[C] (Calvanese et al., 2006)[D] follows Horn-SHIQ (Hustadt, Motik, & Sattler, 2005; Eiter, Gottlob, Ortiz, & Simkus, 2008)[E] follows SHIQ (Ortiz, Calvanese, & Eiter, 2006, 2008; Glimm, Horrocks, Lutz, & Sattler, 2007)[F] follows SHIQ (Tobies, 2001)Table 2: Complexity DL-Lite logics (all complexity bounds save AC0 tight).1 ||n ]means DL-Lite1 , . . . , DL-LitenDL-Lite[(in particular, DL-Lite[ |H] either DL-Lite DL-LiteH).DL-Litecore/horn means DL-Litecore DL-Litehorn (likewise DL-Litekrom/bool ).[X] ( [X]) means upper (respectively, lower) bound follows [X].Detailed proofs results given Sections 58. variants logicsinvolving number restrictions, upper bounds hold also assumptionnumbers q concepts form q R given binary. (Intuitively, followsfact proofs use numbers explicitly occur KB.)lower bounds remain unary coding, since corresponding proofsuse numbers exceeding 4.next section consider extended DL-Lite family general contextidentifying place among DL-Lite-related logics, particular OWL 2 profiles.17fiArtale, Calvanese, Kontchakov & Zakharyaschev4. Landscape DL-Lite Logicsoriginal family DL-Lite logics created two goals mind: identifydescription logics that, one hand, capable representing basic featuresconceptual modeling formalisms (such UML class diagrams ER diagrams) and,hand, computationally tractable, particular, matching AC0 datacomplexity database query answering.saw Section 2.2, represent UML class diagrams one need typical quantification constructs basic description logic ALC (Schmidt-Schau & Smolka,1991), namely, universal restriction R.C qualified existential quantification R.C: onealways take role filler C >. Indeed, domain range restrictionsrelationship P expressed concepts inclusions P v B1 P v B2 , respectively. Thus, almost concept inclusions required capturing UML class diagramsform B1 v B2 B1 v B2 . observations motivated introductionCalvanese et al. (2005) first DL-Lite logic, new nomenclature correspondsDL-LiteFcore . main results polynomial-time upper bound combinedcomplexity KB satisfiability LogSpace upper bound data complexityconjunctive query answering (under UNA). results extended CalvaneseHet al. (2006) two larger languages: DL-LiteFhorn DL-Litehorn , originallycalled DL-Liteu,F DL-Liteu,R , respectively. Calvanese et al. (2007b) introduced another member DL-Lite family (named DL-LiteR ), extended DL-LiteHcorerole disjointness axioms form Dis(R1 , R2 ). computational behavior newlogic turned DL-LiteHcore . may worth mentioningDL-LiteHcoversDLfragmentRDFS(Klyne& Carroll, 2004; Hayes, 2004). Notecorealso Calvanese et al. (2006) considered variants DL-Liteu,F DL-Liteu,Rarbitrary n-ary relations (not usual binary roles) showed query answering still LogSpace data complexity. conjecture similar resultsobtained DL-Lite logics introduced paper. Artale et al. (2007b)demonstrated n-ary relations represented DL-LiteFcore means reification.variant DL-Lite, called DL-LiteA (A attributes), introducedPoggi et al. (2008a) aim capturing many features conceptual modelingformalisms possible, still maintaining computational properties basicvariants DL-Lite. One features DL-LiteA , borrowed conceptual modelingformalisms adopted also OWL, distinction (abstract) objects datavalues, consequently, concepts (sets objects) datatypes (sets datavalues), roles (i.e., object properties OWL, relating objects objects)attributes (i.e., data properties OWL, relating objects data values). However,far results paper concerned, distinction conceptsdatatypes, roles attributes impact reasoning whatsoever, sincedatatypes simply treated special concepts mutually disjoint alsodisjoint proper concepts. Instead, relevant reasoning possibilityexpress DL-LiteA role inclusions functionality, i.e., DL-LiteA includesFHFDL-LiteHcore DL-Litecore , DL-Litecore .already mentioned, role inclusions functionality constraints cannotcombined unrestricted way without losing good computational properties:18fiThe DL-Lite Family Relations.bbDL-LiteHNkrombbb...SHIQDL-LiteHNhornbDL-LiteHNboolb...bDL-LitekrombbDL-LiteHFcoreb(HN )bbbPbDL-Litecore(HF )DL-LitecoreDL-LiteNcore(HN )DL-Litehorn(HF )DL-LitehornDL-LiteNhorn+DL-LiteA,ubbbbbDL-LiteHFhornDL-Lite+DL-LiteADL-LiteF = DL-LiteFDL-LiteF ,u = DL-LiteFcorehornHDL-LiteR = DL-LitecoreDL-LiteR,u = DL-LiteHhornDL-LitecorebbAC0DL-LiteboolHorn-SHIQcoNP.bDL-LiteHNcorebbbFigure 3: DL-Lite family relations.Theorems 5.10 6.7, prove satisfiability DL-LiteHFcore KBs ExpTime-hardcombined complexity, instance checking data-hard P (NLogSpace-hardnessshown Calvanese et al., 2006). DL-LiteA , keep query answering AC0data complexity satisfiability NLogSpace combined complexity, functional roles(and attributes) allowed specialized, i.e., used positively right-handside role (and attribute) inclusion axioms. So, condition (A3 ) slight generalizationrestriction. DL-LiteA also allows axioms form B v R.C non-functionalroles R, covered conditions (A1 ) (A2 ). Thus, DL-LiteA regarded(HF )(HN )proper fragment DL-Litecore DL-Litehorn . show Sections 5.3 7three languages enjoy similar computational properties UNA:tractable satisfiability query answering AC0 .conclude section picture Figure 3 illustrating landscape DLLite-related logics grouping according data complexity positive existentialquery answering UNA. original eight DL-Lite logics, called Calvaneseet al. (2007b) DL-Lite family, shown bottom sector picture (the logics+DL-Lite+DL-LiteA,u extend DL-LiteA DL-LiteA,u identification constraints,(HN )scope article). nearest relatives logic DL-Litehornfragments, AC0 well. next layer contains logics DL-LiteHFcoreDL-LiteHF,queryansweringdata-completeP(nomatterwhetherhornUNA adopted not). fact, logics fragments much expressive DLHorn-SHIQ, shown enjoy data complexity query answeringEiter et al. (2008). remains seen whether polynomial query answering practicallyfeasible; recent experiments DL EL (Lutz, Toman, & Wolter, 2008) indicatemay indeed case. Finally, distant relatives DL-Lite family comprise19fiArtale, Calvanese, Kontchakov & Zakharyaschevupper layer picture, query answering data-complete coNP, is,expressive DL SHIQ.4.1 DL-Lite Family OWL 2upcoming version 2 Web Ontology Language OWL7 defines three profiles,8is, restricted versions language suit specific needs. DL-Lite family, notablyDL-LiteHcore (or original DL-LiteR ), basis one OWL 2 profiles, calledOWL 2 QL. According http://www.w3.org/TR/owl2-profiles/, OWL 2 QL aimedapplications use large volumes instance data, query answeringimportant reasoning task. OWL 2 QL, [. . . ] sound complete conjunctive queryanswering performed LogSpace respect size data (assertions)[and] polynomial time algorithms used implement ontology consistencyclass expression subsumption reasoning problems. expressive power profilenecessarily quite limited, although include main features conceptualmodels UML class diagrams ER diagrams. section, briefly discussresults obtained article context additional constructs presentOWL 2.important difference DL-Lite family OWL statusunique name assumption (UNA): assumption quite common data management,hence adopted DL-Lite family, adopted OWL. Instead, OWLsyntax provides explicit means stating object names, say b, supposeddenote individual, b, interpreted differently, 6 b (inOWL, constructs called sameAs differentFrom).complexity results obtain logics form DL-LiteHdependwhether UNA adopted (because every model DL-LiteHKB without UNAuntangled model KB respecting UNA; see Lemma 8.10).NHowever, case logics DL-LiteFDL-Lite , obviousinteraction UNA number restrictions (cf. Table 2). example,0UNA, instance checking DL-LiteFcore AC data complexity, whereas droppingassumption results much higher complexity: Section 8, prove P-complete.Haddition equality construct DL-LiteHcore DL-Litehorn slightly changesdata complexity query answering instance checking, rises membershipAC0 LogSpace-completeness; see Section 8. important, however,case loose first-order rewritability query answering instance checking,result cannot use standard database query engines straightforward manner.Since OWL 2 profiles defined syntactic restrictions language withoutchanging basic semantic assumptions, chosen include OWL 2 QLprofile construct interferes UNA which, absence UNA,would cause higher complexity. OWL 2 QL include number restrictions, even functionality constraints. Also, keys (the mechanism identifying objectsmeans values properties) supported, although impor7. http://www.w3.org/2007/OWL/8. logic, profiles would called fragments defined placing restrictions OWL 2syntax only.20fiThe DL-Lite Family Relationstant notion conceptual modeling. Indeed, keys considered generalizationfunctionality constraints (Toman & Weddell, 2005, 2008; Calvanese, De Giacomo, Lembo,Lenzerini, & Rosati, 2007a, 2008b), since asserting unary key, i.e., one involvingsingle role R, equivalent asserting functionality inverse R. Hence,absence UNA, allowing keys would change computational properties.already mentioned, standard OWL constructs, role disjointness, (a)symmetry (ir)reflexivity constraints, added DL-Lite logicswithout changing computational behavior. Role transitivity constraints, Tra(R), as(HN )serting R must interpreted transitive role, also added DL-Litehornleads increase data complexity reasoning problems NLogSpace,although satisfiability remains P combined complexity. results foundSection 5.3.constructs OWL 2 far supported DL-Lite logicsmention nominals (i.e., singleton concepts), Boolean operators roles, role chains.5. Satisfiability: Combined ComplexityDL-LiteHNbool clearly sub-logic description logic SHIQ, satisfiability problemknown ExpTime-complete (Tobies, 2001).Section 5.1 show, however, satisfiability problem DL-LiteNbool KBs1reducible satisfiability problem one-variable fragment, QL , first-order logicwithout equality function symbols. satisfiability QL1 -formulas NP-complete(see, e.g., Borger et al., 1997) logics consideration contain full Booleansconcepts, satisfiability DL-LiteNbool KBs NP-complete well. shall also seetranslations Horn Krom KBs QL1 belong Horn Krom fragmentsQL1 , respectively, known P- NLogSpace-complete (see, e.g., Papadimitriou, 1994; Borger et al., 1997). Section 5.2, show simulate behaviorpolynomial-space-bounded alternating Turing machines means DL-LiteHFcore KBs.give (optimal) ExpTime lower bound satisfiability KBs languagesfamily containing unrestricted occurrences functionality constraints roleinclusions. Section 5.3, extend embedding QL1 , defined Section 5.1,(HN )logic DL-Litebool , thereby establishing upper bounds DL-LiteNboolfragments. Finally, Section 5.4 investigate impact role transitivity constraints.5.1 DL-LiteNbool Fragments: First-Order Perspectiveaim section construct reduction satisfiability problem DL-LiteNboolKBs satisfiability QL1 -formulas. two steps: first present lengthyyet quite natural transparent (yet exponential) reduction , shall seeproof reduction substantially optimized linear reduction .Let K = (T , A) DL-LiteNbool KB. Recall role (K) denotes set directinverse role names occurring K ob(A) set object names occurring A.R role (K), let QRset natural numbers containing 1 numbers qconcept q R occurs (recall ABox contain numberrestrictions). Note |QR| 2 contains functionality constraint R.21fiArtale, Calvanese, Kontchakov & Zakharyaschevevery object name ai ob(A) associate individual constant ai QL1every concept name Ai unary predicate Ai (x) signature QL1 .role R role (K), introduce |QR|-many fresh unary predicatesq QRT.Eq R(x),intended meaning predicates follows: role name Pk ,Eq Pk (x) Eq Pk (x) represent sets points least q distinct Pk -successorsleast q distinct Pk -predecessors, respectively. particular, E1 Pk (x)E1 Pk (x) represent domain range Pk , respectively.Additionally, every pair roles Pk , Pk role (K), take two fresh individual constantsdpkdpkQL1 , serve representatives points domains PkPk , respectively (provided empty). Let dr(K) = dr | R role (K) .Furthermore, pair object names ai , aj ob(A) R role (K), takefresh propositional variable Rai aj QL1 encode ABox assertion R(ai , aj ).91induction construction DL-LiteNbool concept C define QL -formulaC := ,(Ai ) = Ai (x),( q R) = Eq R(x),(C) = C (x),(C1 u C2 ) = C1 (x) C2 (x).1DL-LiteNbool TBox corresponds QL -sentence x (x),^(x) =C1 (x) C2 (x) .(1)C1 vC2ABox translated following pair QL1 -sentences^^1=Ak (ai )Ak (ai ),2=^(2)Ak (ai )AAk (ai )APk ai aj^Pk ai aj .(3)Pk (ai ,aj )APk (ai ,aj )Aevery role R role (K), need two QL1 -formulas:R (x) = E1 R(x) inv(E1 R)(inv(dr)),^R (x) =Eq0 R(x) Eq R(x) ,(4)(5)0q,q 0 QR, q >q000q >q >q q 00 QR9. follows, slightly abuse notation write R(ai , aj ) indicate Pk (ai , aj )R = Pk , Pk (aj , ai ) R = Pk .22fiThe DL-Lite Family Relations(by overloading inv operator),(Eq Pk , R = Pk ,inv(Eq R) =Eq Pk , R = Pk ,(dpk,inv(dr) =dpk ,R = Pk ,R = Pk .Formula (4) says domain R empty range empty either:contains constant inv(dr), representative domain inv(R).also need formulas representing relationship propositional variables Rai ajunary predicates role domain range: role R role (K), let Rfollowing QL1 -sentence^^^q^Rai ajk Eq R(ai )ai ob(A) qQRaj1 ,...,ajq ob(A) k=1jk 6=jk0 k6=k0^Rai aj inv(R)aj ai , (6)ai ,aj ob(A)inv(R)aj ai propositional variable Pk aj ai R = Pk Pk aj ai R = Pk .Note first conjunct (6) part translation relies UNA.Finally, DL-LiteNbool knowledge base K = (T , A), seth 1h^^2R .K = x (x)R (x) R (x)Rrole (K)Rrole (K)Thus, K universal sentence QL1 .Example 5.1 Consider, example, KB K = (T , A)= v P , P v A, v 2 P, > v 1 P , P v= {A(a), P (a, a0 )}. obtain following first-order translation:K = x (x) A(a) P aa0P aa0 E1 P (a) P aa E1 P (a)P a0 E1 P (a0 ) P a0 a0 E1 P (a0 )P aa0 E1 P (a) P aa E1 P (a)P a0 E1 P (a0 ) P a0 a0 E1 P (a0 )P aa0 P aa E2 P (a) P a0 P a0 a0 E2 P (a0 )P aa0 P aa E2 P (a) P a0 P a0 a0 E2 P (a0 )P aa0 P a0 P a0 P aa0 P aa P aa P a0 a0 P a0 a0 .(x) =A(x) E1 P (x)E1 P (x) A(x) A(x) E2 P (x)> E2 P (x) E1 P (x) A(x)E1 P (x) E1 P (dp ) E1 P (x) E1 P (dp)E2 P (x) E1 P (x) E2 P (x) E1 P (x) . (7)23fiArtale, Calvanese, Kontchakov & Zakharyaschev1Theorem 5.2 DL-LiteNbool knowledge base K = (T , A) satisfiable iff QL -sentenceK satisfiable.Proof () K satisfiable model K whose domain consistsconstants occurring K i.e., ob(A) dr(K) (say, Herbrand model K ).denote domain interpretations (unary) predicates P , propositionalvariables p constants QL1 P , pM , respectively. Thus, everyconstant a, = a. Let D0 set constants a, ob(A). Without lossgenerality may assume D0 6= .construct interpretation DL-LiteNbool based domain D0inductively defined union[=Wm ,W0 = D0 .m=0interpretations object names ai given interpretations M,namely, aIi =W0 . set Wm+1 , 0, constructed adding Wmnew elements fresh copies certain elements \ D0 . new elementw0 copy w \ D0 write cp(w0 ) = w, w D0 let cp(w) = w.set Wm \ Wm1 , 0, denoted Vm (for convenience, let W1 = ,V0 = D0 ).interpretations AIk concept names Ak defined takingAIk = w | |= Ak [cp(w)] .(8)interpretation PkI role name Pk defined inductively unionPkI=[Pkm ,Pkm Wm Wm ,m=0along construction . First, role R role (K), define requiredR-rank r(R, d) point takingr(R, d) = max {0} { q QR| |= Eq R[d] } .follows (5) r(R, d) = q then, every q 0 QR, |= Eq 0 R[d]whenever q 0 q, |= Eq0 R[d] whenever q < q 0 . also define actual R-rankrm (R, w) point w step taking(]{w0 Wm | (w, w0 ) Pkm }, R = Pk ,rm (R, w) =]{w0 Wm | (w0 , w) Pkm }, R = Pk .basis induction set, role name Pk role(K),Pk0 = (aM, aj ) W0 W0 | |= Pk ai aj .(9)Observe that, (6), R role (K) w W0 ,r0 (R, w) r(R, cp(w)).24(10)fiThe DL-Lite Family RelationsSuppose Wm Pkm , 0, already defined.rm (R, w) = r(R, cp(w)), roles R role (K) points w Wm , interpretation need would constructed. However, general casemay defects sense actual rank points smallerrequired rank.role name Pk role(K), consider following two sets defects Pkm :w Vm | rm (Pk , w) < r(Pk , cp(w)) ,k == w Vm | rm (Pk , w) < r(Pk , cp(w)) .kpurpose of, say,k identify defective points w Vm preciselyr(Pk , cp(w)) distinct Pk -arrows start (according M), arrows stillmissing (only rm (Pk , w) many arrows exist). cure defects, extend WmPkm respectively Wm+1 Pkm+1 according following rules:(mk ) Let w k , q = r(Pk , cp(w)) rm (Pk , w) = cp(w). |= Eq 0 Pk [d]0q 0 QRq q > 0. Then, (5), |= E1 Pk [d] and, (4),|= E1 Pk [dpk ]. case take q fresh copies w10 , . . . , wq0 dpk (and set00cp(wi ) = dpk , 1 q), add Wm+1 add pairs (w, wi ), 1 q,Pkm+1 .(mk ) Let w k , q = r(Pk , cp(w)) rm (Pk , w) = cp(w). |= Eq 0 Pk [d]0q 0 QRq q > 0. So, (5), |= E1 Pk [d] and, (4),00|= E1 Pk [dpk ]. Take q fresh copies w1 , . . . , wq dpk (and set cp(wi0 ) = dpk ,1 q), add Wm+1 add pairs (wi0 , w), 1 q, Pkm+1 .Example 5.3 Consider KB K first-order translation K Example 5.1.Consider also model K domain = {a, a0 , dp, dp },= (E1 P )M = (E1 P )M = (E2 P )M = D,(E2 P )M = ,(P aa0 )M = (P a0 a)M = t.begin construction interpretation K setting W0 = V0 = D0 = {a, a0 }P 0 = {(a, a0 )}. compute required actual ranks r(R, w) r0 (R, w),R {P, P } w V0 :(i) r(P, a) = 2 r0 (P, a) = 1,(iii) r(P , a) = 1 r0 (P , a) = 0,(ii) r(P, a0 ) = 2 r0 (P, a0 ) = 0,(iv) r(P , a0 ) = 1 r0 (P , a0 ) = 1.next step, draw P -arrow fresh copy dp cure defect (i), drawtwo P -arrows a0 two fresh copies dp order cure defects (ii), finallytake fresh copy dp connect P -arrow, thereby curing defect (iii).One step unraveling construction shown Figure 4.Observe following important property construction: m, m0 0, w Vm0R role (K),< m0 ,0,rm (R, w) =(11)q,= m0 , q r(R, cp(w)),r(R, cp(w)), > m0 .25fiArtale, Calvanese, Kontchakov & Zakharyaschev.V0a0V1V2dpdp.Figure 4: Unraveling model (first three steps).prove property, consider possible cases:< m0 point w added Wm yet, i.e., w/ Wm ,rm (R, w) = 0.= m0 m0 = 0 rm (R, w) r(R, cp(w)) follows (10).= m0 m0 > 0 w added step m0 cure defect pointw0 Wm0 1 . means Pk role(K) either (w0 , w) Pkm0(m 1)0 1w0(w, w0 ) Pkm0 w0 k 0. Consider former case.k.Sincefreshwitnessespickedevery time rule (km0 1 )cp(w) = dpkapplied, rm0 (Pk , w) = 1, rm0 (Pk , w) = 0 rm0 (R, w) = 0, every R 6= Pk , Pk .0suffices show r(Pk , dpk ) 1. Indeed, |= Eq Pk [cp(w )]R0q QT , have, (5), |= E1 Pk [cp(w )], so, (4), |= E1 Pk [dpk ].)1.lattercaseconsideredanalogously.definition r, r(Pk , dpk= m0 + 1 then, role name Pk , defects w cured step m0 + 1m00applying rules (m). Therefore, rm0 +1 (R, w) = r(R, cp(w)).k ) (k> m0 + 1 (11) follows observation new arrows involving wadded step m0 + 1, is, 0 role name Pk role(K),Pkm+1 \ PkmVm Vm+1Vm+1 Vm .(12)follows that, R role (K), q QRw , have:|= Eq R[cp(w)]iffw ( q R)I .(13)Indeed, |= Eq R[cp(w)] then, definition, r(R, cp(w)) q. Let w Vm0 . Then,(11), rm (R, w) = r(R, cp(w)) q, > m0 . follows definition26fiThe DL-Lite Family Relationsrm (R, w) RI w ( q R)I . Conversely, let w ( q R)I w Vm0 . Then,(11), q rm (R, w) = r(R, cp(w)), > m0 . So, definition r(R, cp(w))(5), |= Eq R[cp(w)].induction construction concepts C K one readily see that, everyw ,|= C [cp(w)]iffw CI .(14)Indeed, basis trivial B = follows (8) B = Ak (13)B = q R, induction step Booleans (C = C1 C = C1 u C2 )immediately follows induction hypothesis.Finally, show A,|=iff|= .case = C1 v C2 follows (14); = Ak (ai ) = Ak (ai )definition AIk . = Pk (ai , aj ) = Pk (ai , aj ), (aIi , aIj ) PkI iff, (12),(aIi , aIj ) Pk0 iff, (9), |= Pk ai aj .Thus, established |= K.() Conversely, suppose |= K interpretation domain . constructmodel K based . every ai ob(A), let= ai and,every R role (K), take ( 1 R)I ( 1 R)I 6= arbitrary elementotherwise, let drM = d. Next, every concept name Ak , letk = Ak= ( q R)I . Finally, everyand, every role R role (K) q QR, set Eq Rrole R role (K) every pair objects ai , aj ob(A), define (Rai aj )M trueiff |= R(ai , aj ). One readily check |= K . Details left reader.qfirst-order translation K K obviously lengthy provide us reasonablylow complexity results: |K | |K| + (2 + qT2 ) |role(K)| + 2 |role(K)| |ob(A)|qT . However,follows proof lot information translation redundantsafely omitted.define concise translation K K = (T , A) QL1 taking:h^12K = x (x)R (x) R (x),Rrole (K)1(x), R (x), R (x) defined means (1), (4), (5) (2),respectively,^^^2=EqR,a R(a)(Pk (ai , aj )) ,(15)aob(A)Rrole (K)a0 ob(A) R(a,a0 )APk (ai ,aj )AqR,a maximum number QRqR,a many distinct aiR(a, ai ) (here use UNA) (Pk (ai , aj )) = Pk (ai , aj ) >2otherwise. size size K linear size K,respectively, matter whether numbers coded unary binary.27fiArtale, Calvanese, Kontchakov & Zakharyaschevimportantly, translation actually done LogSpace. Indeed,12trivially case (x), R (x), R (x), last conjunct .2first conjunct then, R role (K) ob(A), maximum qR,aQRqR,a many distinct ai R(a, ai ) A, computed usinglog min(max QR, |ob(A)|) + log |ob(A)| cells. Initially set q = 0, enumerateobject names ai incrementing current q time find R(a, ai ) A. stopq = max QRreach end object name list. resulting qR,a maximumnumber QRexceeding q.Example 5.4 translation K KB K Example 5.1 looks follows:K = x (x) A(a) E1 P (a) E1 P (a0 ),(x) defined (7).1Corollary 5.5 DL-LiteNbool KB K satisfiable iff QL -sentence K satisfiable.Proof claim follows fact K satisfiable iff K satisfiable. Indeed,|= K clearly |= K . Conversely, |= K one construct new modelM0 based domain taking:0k = Ak , concept names Ak ;0Eq RM = Eq RM , R role (K) q QRT;0(Rai aj )M true iff R(ai , aj ) A;0= ai , ai ob(A);0drM = drM , R role (K).0claim M0 |= K . Indeed, Eq RM = Eq RM , every R role (K) q QR.12follows M0 |= x (x) M0 |= x R (x). definition, M0 |=VA , M0 |=qM0 |= x R (x). remains show M0 |= R . Suppose M0 |= i=1 Raaji ,R(a, aji ) A, distinct aj1 , . . . , ajq , q QR. Clearly, q qR,a|= Eq R(a) thus M0 |= Eq R(a).qimmediate consequence Corollary 5.5, facts translationdone LogSpace, satisfiability problem QL1 -formulas NP-completeDL-Litebool contains Booleansand encode full propositional logicweobtain following result:FTheorem 5.6 Satisfiability DL-LiteNbool , DL-Litebool DL-Litebool knowledge basesNP-complete combined complexity.1Observe K DL-LiteNkrom KB K Krom fragment QL .FTheorem 5.7 Satisfiability DL-LiteN, DL-Lite DL-Lite knowledge bases,{core, krom}, NLogSpace-complete combined complexity.28fiThe DL-Lite Family RelationsProof satisfiability problem Krom formulas prefix form x (asK ) NLogSpace-complete (see, e.g., Borger et al., 1997, Exercise 8.3.7)LogSpace reduction, satisfiability NLogSpace logics mentionedtheorem. lower bound, suffices recall NLogSpace-hardnesssatisfiability propositional Krom formulas proved reduction directed graphreachability problem using core propositional formulas (Borger et al., 1997),satisfiability logics NLogSpace-hard.q1K DL-LiteNhorn KB K belongs universal Horn fragment QL .FTheorem 5.8 Satisfiability DL-LiteNhorn , DL-Litehorn DL-Litehorn KBs P-completecombined complexity.Proof QL1 contains function symbols K universal, satisfiability KLogSpace-reducible satisfiability set propositional Horn formulas, namely,formulas obtained K replacing x constants occurringK . remains recall satisfiability problem propositional Horn formulasP-complete (see, e.g., Papadimitriou, 1994), gives required upper boundqDL-LiteNhorn lower bound DL-Litehorn .5.2 DL-LiteHFcore ExpTime-hardUnfortunately, translation constructed previous section cannot extendedlogics form DL-LiteHNnumber restrictions role inclusions.section show satisfiability problem DL-LiteHFcore KBs ExpTime-hard,matches upper bound satisfiability DL-LiteHNKBseven binary codingboolnatural numbers (Tobies, 2001).Note first that, although intersection allowed left-hand side DL-LiteHFcoreconcept inclusions, certain cases (when right-hand side consistent) simulateusing role inclusions functionality constraints. Suppose knowledge base Kcontains concept inclusion form C1 u C2 v C. Define new KB K0 replacingaxiom K following set new axioms, R1 , R2 , R3 , R12 , R23 freshrole names:C1 v R1C2 v R2 ,(16)R1 v R12 ,R2 v R12 ,(17)2 R12 v ,R1v(18)R3 ,(19)R3 v C,(20)R3 v R23 ,2 R23R2 v R23 ,v .(21)(22)Lemma 5.9 (i) |= K0 |= K, every interpretation I.(ii) |= K C 6= model 0 K0 domainagrees every symbol K.29fiArtale, Calvanese, Kontchakov & ZakharyaschevProof (i) Suppose |= K0 x C1I C2I . (16), (x, y) R1I ,, whence(R1 )I , z (x, z) R2I . (17), {(x, y), (x, z)} R12= z view (18). (19), (R3 )I hence u (u, y) R3I(x, y) RI . Finally, followsu (R3 )I . (20), u C . (21), (u, y) R2323(22) u = x, x C . Thus, |= K.(ii) Take point c C define extension 0 new role namessetting:0R1I = {(x, x) | x C1I },0R2I = {(x, x) | x C2I },0R3I = {(x, x) | x (C1 u C2 )I } {(c, x) | x (C1 u C2 )I },00= RI RIR12120000= RI RI .R2323readily seen 0 satisfies axioms (16)(22), 0 |= K0 .qposition prove following:Theorem 5.10 Satisfiability DL-LiteHFcore KBs ExpTime-hard combined complexity(with without UNA).Proof prove theorem two steps. First consider logic DL-LiteHFhornshow encode behavior polynomial-space-bounded alternating Turing machines (ATMs, short) means DL-LiteHFhorn KBs. APSpace = ExpTime,APSpace class problems recognized polynomial-space-bounded ATMs (see, e.g.,Kozen, 2006), establish ExpTime-hardness satisfiability DL-LiteHFhorn . Then,using Lemma 5.9, show get rid conjunctions left-hand sideconcept inclusions involved encoding ATMs thus establish ExpTime-hardnessDL-LiteHFcore .Without loss generality, consider ATMs binary computationaltrees. means that, every non-halting state q every symbol tapealphabet, precisely two instructions form(q, a) ;0M (q 0 , a0 , d0 )(q, a) ;1M (q 00 , a00 , d00 ),(23)d0 , d00 {, } (resp., ) means move head right (resp., left) one cell.remind reader non-halting state either and-state or-state.Given ATM M, polynomial function p(n) every run everyinput length n use p(n) tape cells, input word ~a = a1 , . . . , ,construct DL-LiteHFhorn knowledge base KM,~a following properties: (i) sizeKM,~a polynomial size M, ~a, (ii) accepts ~a iff KM,~a satisfiable.Denote Q set states tape alphabet M.encode instructions M, need following roles:Sq , Sq0 , Sq1 , q Q: informally, x (Sq )I , interpretation I, meansx represents configuration state q, x (Sqk )I meansnext state, according transition ;kM , q, k {0, 1};30fiThe DL-Lite Family RelationsHi , Hi0 , Hi1 , p(n): x (Hi )I means x represents configurationhead scans ith cell, x (Hik )I that, according transition;kM , k {0, 1}, next configuration head scans ith cell;0 , C 1 , p(n) : x (C )I means x representsCia , Ciaiaiak )I that, accordingconfiguration ith cell contains a, x (Cia;kM , k {0, 1}, next configuration ith cell contains a.intended meaning encoded using following concept inclusions: everyinstruction (q, a) ;kM (q 0 , a0 , ) every < p(n),kkSq u Hi u Ciav Hi+1u Sqk0 u Cia0,(24)every instruction (q, a) ;kM (q 0 , a0 , ) every i, 1 < p(n),kkSq u Hi u Ciav Hi1u Sqk0 u Cia0.(25)preserve symbols tape active cell, use followingconcept inclusions, k {0, 1}, i, j p(n) j 6= i, :kHj u Ciav Cia.(26)synchronize roles, need two (functional) roles Tk number roleinclusions added TBox: k {0, 1}, p(n), q Q, ,kCiav Cia ,Hik v Hi ,Sqk v Sq ,(27)kCiav Tk ,Hik v Tk ,Sqk v Tk ,(28)2 Tk v .(29)remains encode acceptance conditions ~a. done helprole names Yk , k {0, 1}, concept name A:Sq v A,q accepting state,Yk v Tk ,2 TkTk uSq u YkSq(31)v ,v(32)Yk ,v A,u Y0 u Y1 v A,(30)(33)q or-state,(34)q and-state.(35)TBox DL-LiteHFhorn knowledge base KM,~a constructing consistsaxioms (24)(35) together auxiliary axiomu v ,(36)fresh concept name. ABox KM,~a comprised followingassertions, object names u:Sq0 (u, s),q0 initial state,H1 (u, s),Ciai (u, s),(37)(38)p(n), ai ith symbol input tape,(39)(40)D(s).31fiArtale, Calvanese, Kontchakov & ZakharyaschevClearly, KM,~a = (T , A) DL-LiteHFa.horn KB size polynomial size M, ~Lemma 5.11 ATM accepts ~a iff KB KM,~a satisfiable.Proof () Suppose accepts ~a |= KM,~a interpretation I.reconstruct full computation tree ~a induction following way.Let root tree point sI . (37)(39), represents initial configuration ~a accordance intended meaning roles Sq0 , H1 Ciaiexplained (it matter if, instance, also sI (H5 )I ).Assume already found point x representing configurationc = b1 , . . . , bi1 , (q, bi ), bi+1 , . . . , bp(n) ,(41)q current non-halting state head scans ith cell containing bi .meansx (Sq )I (Hi )Ix (Cjb) ,jj p(n).Assume also contains two instructions form (23) (q, bi ), q nonhalting. (q, bi ) ;kM (q 0 , b0 , ), k = 0 1, then, (24) (26),points ys , yh yj , j p(n),(x, ys ) (Sqk0 )I ,k(x, yh ) (Hi+1)I ,k(x, yi ) (Cib0) ,k) ,(x, yj ) (Cjbjj 6= i.0 , C 0 C 0 , j 6= i, sub-roles functional role ,(28)(29), Sq00 , Hi+1kjbjib0ipoints ys , yh yj coincide; denote point xk . (27),have:(x, xk ) TkI ,) (Cibxk (Sq0 )I (Hi+10)) ,xk (Cjbjj 6= i.Similarly, (q, bi ) ;kM (q 00 , b00 , ), k = 0 1, then, (25) (26),point xk(x, xk ) TkI ,) (Cibxk (Sq00 )I (Hi100 )) ,xk (Cjbjj 6= i.Thus, k = 0, 1, xk Tk -successor x representing configuration ckexecuted (q, bi ) ;kM (q 00 , b00 , d) c; case ck called k-successor c.According (30), every point constructed computation tree ~a representing configuration accepting state AI . Suppose now, inductively,x represents configuration c form (41), q or-state, xk represents ksuccessor c (x, xk ) TkI , k = 0, 1, one xk , say x0 , AI .view (33), x0 (Y0 )I . T0 functional (32) Y0 sub-roleT0 (31), (x, x0 ) Y0I , so, (34), x AI . case x and-stateconsidered analogously help (35).Since accepts ~a, conclude sI AI , contrary (36) (40).() Conversely, suppose accept ~a. Consider full computationtree (, <0 <1 ) nodes labeled configurations way rootlabeled initial configuration(q0 , a1 ), a2 , . . . , , an+1 , . . . , ap(n) ,32fiThe DL-Lite Family Relations(where ai , n + 1 p(n), blank), node x tree labelednon-halting c form (41) contains two instructions form (23),x one <0 -successor labeled 0-successor c one <1 -successor labeled1-successor c. (It emphasized (, <0 <1 ) tree, differentnodes may labeled configuration.)use tree construct interpretation = (I , ) follows:= {u}, u/ ;sI root uI = u;DI = {sI };k )I , (x, x ) (C k )I , (x, x ) (C k )I , j 6= i,(x, xk ) (Sqk0 )I , (x, xk ) (Hi+1kkjbjib0iff x labeled c form (41), (q, bi ) ;kM (q 0 , b0 , ) x <k xk , k = 0, 1;k )I , (x, x ) (C k )I , (x, x ) (C k )I , j 6= i,(x, xk ) (Sqk0 )I , (x, xk ) (Hi1kkjbjib0kiff x labeled c form (41), (q, bi ) ;M (q 0 , b0 , ) x <k xk , k = 0, 1;(u, sI ) (Sq0 )I , (u, sI ) (H1 )I , (u, sI ) (Ciai )I , p(n) extensionsroles Sq , Hi Cia defined according (27);TkI = <k , k = 0, 1;Y0I , Y1I AI defined inductively:Induction basis: x labeled accepting configuration, x AI .Induction step: (i) x <k xk , k = 0, 1, xk AI , (x, xk ) YkI ; (ii)x or-state (respectively, and-state) (x, xk ) YkI (respectively,all) k {0, 1}, x AI .follows given definition |= KM,~a . Details left reader.qlemma proved establishes satisfiability DL-LiteHFhorn KBsExpTime-hard. next aim show one eliminate conjunctionsleft-hand side TBox axioms (24)(26), (33)(35). helpLemma 5.9. applying it, check first KM,~a satisfiable satisfiableinterpretation |= KM,~a C 6= , every C occurringaxiom form C1 u C2 v C K. Consider, instance, axiom (24) assume|= KM,~a , (Sqk0 )I = . Then, construct new interpretation 0 adding two00new points, say x y, domain I, setting (x, y) (Sqk0 )I , (x, y) (Sq0 )I ,000(x, y) (Tk )I . Furthermore, q 0 accepting state, also set AI (x, y) YkI .One readily check 0 still model KM,~a . conjuncts (24)remaining axioms considered analogously.application Lemma 5.9 axiom form C1 uC2 v C C2 = C20 uC200obtain, (16)(22), new KB K0 concept inclusion form C20 uC200 v R1 ,also requires treatment means lemma. able this,33fiArtale, Calvanese, Kontchakov & Zakharyaschev00check K0 satisfiable interpretation 00 (R1 )I 6= . Suppose00 |= K0 (R1 )I = . construct 00 adding two new points, say x000 , RI 0 RI 0 .y, domain 0 , adding x C (x, y) R1I , R12233readily seen 00 |= K0 .noted proof depend whether UNA adoptednot.qimmediate consequence obtain:Corollary 5.12 Satisfiability DL-LiteHFDL-LiteHNKBs without UNAExpTime-complete combined complexity, {core, krom, horn, bool}.5.3 Reconciling Number Restrictions Role Inclusionsseen previous section, unrestricted interaction number restrictions role inclusions allowed logics form DL-LiteHNresults highcombined complexity satisfiability. Section 6.2, shall see data complexityinstance checking query answering also becomes unacceptably high logics.quick look proof Theorem 5.10 reveals culprit: interplay roleinclusions R1 v R, R2 v R functionality constraints 2 R v , effectively meanR1 (x, y) R2 (x, z) = z. section study caseinterplay allowed.(HN )Recall Section 2.1 DL-LiteTBoxes , {core, krom, horn, bool},satisfy following conditions:(A1 ) may contain positive occurrences qualified number restrictions q R.C,C conjunction concepts allowed right-hand side -conceptinclusions;(A2 ) q R.C occurs , contain negative occurrences numberrestrictions q 0 R q 0 inv(R) q 0 2;(A3 ) R proper sub-role , contain negative occurrencesq R q inv(R) q 2.(HN )DL-LiteTBoxes contain role constraints Dis(R1 , R2 ), Asym(Pk ), Sym(Pk ),Irr(Pk ), Ref(Pk ).main aim section prove following theorem develop technical(HN )tools need investigate data complexity reasoning DL-Liteboolsublogics later paper.(HN )Theorem 5.13 combined complexity, (i) satisfiability DL-Litebool KBs NP(HN )complete; (ii) satisfiability DL-Litehorn KBs P-complete; (iii) satisfiability(HN )(HN )DL-Litekrom DL-Litecore KBs NLogSpace-complete.34fiThe DL-Lite Family Relations(HN )Let us consider first sub-language DL-Liteboolwithout qualified number restric(HN )tions role constraints mentioned above; denote DL-Litebool . sub-(HN )language required purely technical reasons. Section 7, also use DL-Litehorn ,need core Krom fragments.(HN )Suppose given DL-LiteboolKB K = (T , A). Let Id distinguishedrole name. use simulate identity relation required encoding roleconstraints. assume either K contain Id satisfies followingconditions:(Id1 ) Id(ai , aj ) iff = j, ai , aj ob(A),Id = {1},(Id2 ) > v Id, Id v Id , QId= QT(Id3 ) Id allowed role inclusions form Id v Id Id v R.follows, without loss generality, assume00R(Q) QRQT whenever R vT R0(for case always add missing numbers QR, e.g., introducingfictitious concept inclusions form v q R0 ).Now, way Section 5.1, define two translations e e Kone-variable fragment QL1 first-order logic. former translation, e , retainsinformation relationships ABox objects, show every modelKe unraveled model K. define e taking:hKe = x (x) R (x)^R (x) R (x)Rrole (K)h12^RRrole (K)^Rai aj R0 ai aj,RvR0ai ,aj ob(A)12(x), , , R (x), R (x) R (1)(6)^^R (x) =Eq R(x) Eq R0 (x) .RvR0inv(R)vinv(R0 )T(42)qQRfollowing lemma analogue Theorem 5.2:(HN )Lemma 5.14 DL-LiteboolKB K satisfiable iff QL1 -sentence Ke satisfiable.Proof proof basically follows lines proof Theorem 5.2 modifications. present modified unraveling construction here; converse direction exactlyTheorem 5.2.equivalence class [Ri ] = {Rj | Ri Rj } select single role (a representativeclass) denote repT (Ri ). extending Pkm Pkm+1 , use followingmodified curing rules:35fiArtale, Calvanese, Kontchakov & Zakharyaschev(mk ) Pk 6= repT (Pk ) nothing: defects cured repT (Pk ). Otherwise, letw k , q = r(Pk , cp(w)) rm (Pk , w) = cp(w). |= Eq0 Pk [d]q 0 q > 0. Then, (5), |= E1 Pk [d] and, (4), |= E1 Pk [dpk ].0 ) = dp , 1 q),case take q fresh copies w10 , . . . , wq0 dp(andsetcp(wkkadd Wm+1add pairs (w, wi0 ), 1 q, Pjm+1 Pk vT Pj (includingPj = Pk );add pairs (wi0 , w), 1 q, Pjm+1 Pk vT Pj ;Id occurs K, add pairs (wi0 , wi0 ), 1 q, Pjm+1 Id vT Pj .(mk ) rule mirror image (k ): Pk dpk replaced everywherePk dpk , respectively; see proof Theorem 5.2.follows definition Id neverresultingdefects interpretedinterpretation identity relation IdI = (w, w) | w ; interpretationsroles respect role inclusions, i.e., R1I R2I whenever R1 vT R2 .remains show constructed interpretation indeed model K.First, (11) trivially holds Id required actual ranks equal 1. Second, (11) holds R R 6= Id R proper sub-roles: proof exactlyTheorem 5.2, taking account cure defects single roleequivalence class that, (42), R0 [R], r(R0 , cp(w)) = r(R, cp(w))r(inv(R), cp(w)) = r(inv(R0 ), cp(w)). follows (13) holds Id role Rwithout proper sub-roles. However, (13) necessarily hold roles R propersub-roles: follows construction, actual rank may greater requiredrank, case following:|= Eq R[cp(w)]w ( q R)I .However, enough purposes. induction structure conceptsusing (A3 ), one show |= C1 v C2 whenever |= x (C1 (x) C2 (x)),concept inclusion C1 v C2 , therefore, |= . also |= (see proofTheorem 5.2) thus |= K.qRemark 5.15 follows proofs Theorem 5.2 Lemma 5.14 that,(HN )DL-LiteboolKB K = (T , A), every model Ke induces model IM Kfollowing properties:(ABox) ai , aj ob(A), (aIi , aIj ) RIM iff R(ai , aj ) CleT (A),CleT (A) =R2 (ai , aj ) | R1 (ai , aj ) A, R1 vT R2 .(forest) object names ob(A) induce partitioning IM disjoint labeledtrees Ta = (Ta , Ea , `a ) nodes Ta , edges Ea , root aIM , labeling function`a : Ea role (K) \ {Id, Id }.36fiThe DL-Lite Family Relations(copy) function cp : IM ob(A) dr(K)cp(aIM ) = ob(A),cp(w) = dr if, w0 Ta , (w0 , w) Ea `a (w0 , w) = inv(R).(iso) R role (K), labeled subtrees generated elements w IMcp(w) = dr isomorphic.(concept) w B IM iff |= B [cp(w)], basic concept B K w IM .fi(role) IdIM = (w, w) fi w IM and, every role name Pk ,PkIM =(aIi , aIj ) | R(ai , aj ) A, R vT Pk(w, w) | Id vT Pk[(w, w0 ) Ea | `a (w, w0 ) = R, R vT Pk .aob(A)model called untangled model K (the untangled model K inducedM, precise).translation e generalizes thus suffers exponential blowup.define optimized translation, e , linear size K, taking:h^12e ,R (x) R (x)Ke = x (x) R (x)Rrole (K)1(x), R (x), R (x), R (x) defined (1), (42), (4), (5) (2),respectively,^^^2e R(a)Ae =EqR,a(43)(Pk (ai , aj ))e ,aob(A)Rrole (K)a0 ob(A) R(a,a0 )CleT (A)Pk (ai ,aj )AeeqR,amaximum number QRqR,a many distinct aiR(a, ai ) CleT (A) (here use UNA) (Pk (ai , aj ))e = Pk (ai , aj ) CleT (A)> otherwise; cf. (15). note QR= {1}, roles R role (K),translation depend whether UNA adopted not.following corollary proved similarly Corollary 5.5:(HN )Corollary 5.16 DL-LiteboolKB K satisfiable iff QL1 -sentence Ke satisfiable.clear translation e computed NLogSpace (for combined1complexity). Indeed, readily seen (x), R (x), R (x), R (x), .2order compute Ae , need able check whether R(ai , aj ) CleT (A): testperformed non-deterministic algorithm using logarithmic space |role (K)|(it basically standard directed graph reachability problem,NLogSpace-complete; see, e.g., Kozen, 2006); done using N log |role (K)| +2 log |ob(A)| cells work tape, N constant (in fact, N = 3 enough: one37fiArtale, Calvanese, Kontchakov & Zakharyaschevstore R, current role R0 path length graph reachability subroutine,also bounded log |role (K)|). Therefore, translation e computedNLogSpace transducer.(HN )show satisfiability DL-Litebool KBs easily reduced satisfiability(HN )(HN )DL-LiteboolKBs. First, assume DL-Litebool KBs contain role symmetryasymmetry constraints Asym(Pk ) equivalently replaced Dis(Pk , Pk )Sym(Pk ) Pk v Pk (it noted introduction Pk v PkTBox violate (A3 )). following lemma allows us get rid qualified numberrestrictions well role disjointness, reflexivity irreflexivity constraints:(HN )Lemma 5.17 every DL-LiteboolKB K = (T , A)(HN )KB K0 = (T 0 , A0 ), one construct DL-Liteboolevery untangled model IM K model K0 , providedR1 (ai , aj ), R2 (ai , aj ) CleT (A) Dis(R1 , R2 ) 0 ,R(ai , ai ) CleT (A) Irr(R) 0 ;(44)every model 0 K0 gives rise model K based domain 0agrees 0 symbols K0 .(HN )(HN )K0 DL-Litehorn KB K DL-LitehornKB.Proof First, every pair R, C q R.C occurs 0 , introduce fresh rolename RC . replace (positive) occurrence q R.C 0 q RCadd following concept role inclusions TBox:vCRCRC v R.repeat procedure occurrences qualified number restrictions eliminated. Denote 00 resulting TBox. Observe (A1 ) (A2 ) ensure 00satisfies (A3 ). also notice C occurs right-hand side extraaxioms thus 00 belongs fragment 0 . clear that, sinceq R.C occur positively, every model 00 model 0 . Conversely, everymodel 0 0 , model 00 00 based domain 00 coincides00 = {(w, u) RI 0 | u C 0 }, new role R . So,0 symbols 0 RCCwithout loss generality may assume 0 = 00 .Let000TirrefTdisj,0 = T00 Tref0 , T00Trefirref Tdisj sets role reflexivity, irreflexivity disjointness con(HN )straints 0 T00 remaining DL-LiteboolTBox. Let0T10 = > v Id, Id v IdId v P | Ref(P ) Tref,A01 = Id(ai , ai ) | ai ob(A0 ) .(HN )construct K modifying DL-LiteboolKB K0 = (T00 T10 , A0 A01 ) two steps:0 , take fresh role nameStep 1. every reflexivity constraint Ref(P ) TrefP38fiThe DL-Lite Family Relationsadd new role inclusion SP v P TBox;replace every basic concept B T00 B SP , defined inductively follows:ASP = A, concept name A,( q R)SP = q R, role R/ {P, P },( q P )SP = (q 1) SP ( q P )SP = (q 1) SP , q 2,(P )SP = > (P )SP = >;replace R(ai , aj ) A0 R 0 P SP (ai , aj ) whenever 6= j.Intuitively, split role P irreflexive part SP Id. Note Preflexive proper sub-role then, (A3 ), restrictions maximal numberP -successors P -predecessors, therefore SP Ref(P ) 0 . Let (T1 , A)(HN )resulting DL-LiteboolKB. Clearly, (T1 , A) satisfies (Id1 )(Id3 ). ObserveCleT1 (A) role(K0 ) = CleT 0 0 (A0 ),01(45)role(K0 ) means restriction role names K0 .Let IM untangled model (T1 , A). show IM |= T00 . Consider role PRef(P ) 0 . Notice SP proper sub-roles T1 IdIM disjointSPIM . Thus, SPIM IdIM P IM(*) (B SP )IM B IM , B = q R q 2, whenever Ref(P ) 0 , R {P, P }P proper sub-role 0 .P proper sub-roles 0 (i.e., proper sub-roles T1 different SP Id)SPIM IdIM = P IM . So, basic concepts B T00 covered (*),B IM = (B SP )IM . follows (A3 ) IM |= T00 .00 }Step 2. Next take account set = Tdisj{Dis(Pk , Id) | Irr(Pk ) Tirrefdisjointness constraints modifying KB (T1 , A) constructed previous step.Observe R1 v logical consequence {Dis(R1 , R2 )} whenever R1 vT R2 .Let = T1 T2 , T2 defined takingfiT2 = R1 v fi R1 vT1 R2 either Dis(R1 , R2 ) Dis(R2 , R1 ) .(role), untangled model IM (T , A) R1 , R2 role (K), IM |= Dis(R1 , R2 )R1 (ai , aj ), R2 (ai , aj ) CleT1 (A), which, (45), meansR1 (ai , aj ), R2 (ai , aj ) CleT 0 0 (A0 ). So, (44) holds every untangled model IM010 . IdIM identity relation,(T , A) also model T1 thus, IM |= Tdisj000IM |= Tref Tirref . (45), IM |= shown above, IM |= T00 .Therefore, IM |= K0 .Conversely, suppose 0 model K0 . Let interpretation IdI000identity relation, SPI = P \ IdI , P Ref(P ) 0 , AI = AI ,00P = P aI = aI , concept, role object names A, P K0 . Clearly,|= (T00 T10 , A0 A01 ). definition SP , |= T1 and, since |= D, obtain|= T2 thus |= . (45), |= A, whence |= K.q39fiArtale, Calvanese, Kontchakov & Zakharyaschev(HN )Now, follows Lemma 5.17, given DL-LiteKB K0 , {krom, horn,(HN )bool}, compute DL-LiteboolKB K using LogSpace transducer (whichessentially required checking whether R 0 P ). immediately obtain Theorem 5.13Lemma 5.14 observing that, {krom, horn, bool}, Ke belongsrespective first-order fragment condition (44) checked NLogSpace(HN )(computing CleT (A) requires directed graph accessibility checks). result DL-Litecore(HN )follows corresponding result DL-Litekrom .5.4 Role Transitivity Constraints(HN )+consider languages DL-Lite, {core, krom, horn, bool}, extend(HN )DL-Literole transitivity constraints form Tra(Pk ). remind readerrole called simple (see, e.g., Horrocks et al., 2000) transitive sub-roles(including itself) simple roles R allowed concepts form q R,q 2. particular, contains Tra(P ) P P simple, cannotcontain occurrences concepts form q P q P , q 2.(HN )+DL-LiteKB K = (T , A), define transitive closure TraT (A)takingTraT (A) = P (ai1 , ) | ai2 . . . ain1 P (ai1 , aij+1 ) A, 1 j < n, Tra(P ) .Clearly, TraT (A) computed NLogSpace: pair (ai , aj ) objects ob(A),add P (ai , aj ) TraT (A) iff P -path length < |ob(A)| ai aj(recall directed graph reachability problem NLogSpace-complete).(HN )+(HN )Lemma 5.18 DL-LiteKB (T , A) satisfiable iff DL-LiteKB (T 0 , A0 )0satisfiable, results removing transitivity axiomsA0 = CleT (TraT (CleT (A))).Proof Indeed, KB (T 0 , A0 ) satisfiable construct model describedproofs Lemmas 5.14 5.17 take transitive closure P every PTra(P ) (and update RI P vT R). P P simple, containsaxioms imposing upper bounds number P -successors predecessors,resulting interpretation must model (T , A). converse direction trivial. qnote analogue Remark 5.15 also holds case: replace CleT (A)CleT (TraT (CleT (A))) (ABox) take transitive closure transitive subrole (role).Remark 5.19 noted two different reasons reductionLemma 5.18 NLogSpace rather LogSpace (as reduction is). First,order compute CleT (A), pair ai , aj , one find path directedgraph induced role inclusion axioms. Second, order compute TraT (CleT (A)), onefind path graph induced ABox itself. So, concerneddata complexity, CleT (A) computed LogSpace (in fact, AC0 , shall40fiThe DL-Lite Family Relationssee Section 6.1) role inclusion graph (and hence size) dependA. second reason, however, dangerous data complexity shall seeSection 6.1.consequence Lemma 5.18 Theorem 5.13 obtain following:(HN )+Corollary 5.20 combined complexity, (i) satisfiability DL-Liteboolcomplete; (ii)(HN )+DL-Litekrom(HN )+satisfiability DL-Litehorn KBs P-complete;(HN )+DL-LitecoreKBs NLogSpace-complete.KBs NP-(iii) satisfiabilityNote KBs contain number restrictions form q R,q 2, (as extensions DL-LiteHlanguages) result dependUNA.Remark 5.21 noted role disjointness, symmetry, asymmetry transitivity constraints added logics DL-LiteHFDL-LiteHN,{core, krom, horn, bool}, without changing combined complexity satisfiability problems (which, Corollary 5.12, ExpTime-complete). Indeed, followsTheorem 10 Glimm et al. (2007), KB satisfiability extension SHIQrole conjunction ExpTime length role conjunctions boundedconstant (in case, constant 2 Dis(R1 , R2 ) encoded(R1 u R2 ).> v ; Asym(R) dealt similarly). conjecture role reflexivity irreflexivity constraints change complexity either.6. Instance Checking: Data Complexityfar assumed whole KB K = (T , A) input satisfiability problem. According classification suggested Vardi (1982), consideringcombined complexity. Two types complexity knowledge bases are:schema (or TBox ) complexity, TBox regarded input,ABox assumed fixed;data (or ABox ) complexity, ABox regarded input.easy see schema complexity satisfiability problem logicsconsidered coincides corresponding combined complexity. section,analyze data complexity satisfiability instance checking.(HN )H6.1 DL-LiteNbool , DL-Litebool DL-LiteboolAC0follows, without loss generality assume role concept namesgiven knowledge base K = (T , A) occur TBox write role(T ), role (T )dr(T ) instead role(K), role (K) dr(K), respectively; set concept names(HN )denoted con(T ). section reduce satisfiability DL-Litebool KBs modelchecking first-order logic. end, fix signature containing two unary predicatesAk Ak , concept name Ak , two binary predicates Pk Pk , rolename Pk .41fiArtale, Calvanese, Kontchakov & Zakharyaschev(HN )Consider first case DL-LiteboolKB K. represent ABox Kfirst-order model AA signature. domain AA ob(A) and,ai , aj ob(A) predicates Ak , Ak , Pk Pk signature,AA |= Ak [ai ]iffAk (ai ) A,AA |= Pk [ai , aj ]iffPk (ai , aj ) A,AA |= Ak [ai ]iffAk (ai ) A,AA |= Pk [ai , aj ]iffPk (ai , aj ) A.construct first-order sentence signature (i) dependsdepend A, (ii) AA |= iff Ke satisfiable.simplify presentation, denote ext(T ) extension followingconcept inclusions:0000q 0 R v q R, R role (T ) q, q 0 QRq > q q > q > q00Rq QT ,00q R v q R0 , q QRR v R inv(R) v inv(R ) .VClearly, (ext(T )) (x) equivalent (in first-order logic) (x)T R (x) Rrole (T ) R (x);see (1), (5) (42).Let Bcon(T ) set basic concepts occurring (i.e., concepts formq R, con(T ), R role (T ) q QR). indicate basic conceptshold hold domain element first-order model Ke , use functions: Bcon(T ) {>, }, called types. Denote Tp set types(there 2|Bcon(T )| them). complex concept C, define (C) induction:(C) = (C) (C1 u C2 ) = (C1 ) (C2 ). propositional variable-free formula^=(C1 ) (C2 )C1 vC2 ext(T )ensures type consistent concept role inclusions .emphasized built > using Boolean connectives thereforedepend particular domain element AA . following formula true given12element x AA type (see Ae ; (2) (43), respectively):(x) =^(Ak (x) (Ak )) (Ak (x) (Ak ))Ak con(T )^^Eq RT (x) ( q R)^Rrole (T ) qQRxy PkT (x, y) Pk (x, y) ,Pk role(T )Eq RT (x) RT (x, y), R role (T ), abbreviations defined^^Eq RT (x) = y1 . . . yq(yi 6= yj )RT (x, yi ) ,1i<jqRT (x, y) =_Pk vTPk (x, y)_Pk vTR42(46)1iqPk (y, x).R(47)fiThe DL-Lite Family RelationsClearly, R(ai , aj ) CleT (A) iff AA |= RT [ai , aj ] AA |= Eq RT [a] iffleast q distinct R-successors CleT (A) (and thus every model K).Without loss generality may assume role (T ) = {R1 , . . . , Rk } 6= . DenoteTpk set k-tuples ~ containing type dri Tp role Ri role (T ).set_~x (x),=k~Tp(dr1 ,...,drk )(x)=_(x)^drTpRi role (T )^(Ri )Ri role (T )_ds (Ri ) inv(dri ) (inv(Ri )) .Srole (T )explain meaning subformulas , assume (T , A) satisfiable. orderconstruct model Ke first-order model AA , specify basicconcepts contain given constant Ke . words, select typedri dr(T ) ob(A). formula says one select k-tupletypes ~ = (dr1 , . . . , drk ) Tpk one disjuncts true AA .k-tuple fixes witness part model M, consisting dri , determinesbasic concepts dri belong to. disjunct says (having fixedwitness part model), every ob(A), type (determining basicconcepts belongs to)consistent information (cf. (x));also consistent concept role inclusions (cf. ););dr1 , . . . , drk consistent concept role inclusions (cf. drrole Ri nonempty domain (i.e., either ds > Ri )nonempty range, particular, inv(dri ) (inv(Ri )) = >; see also R (x) defined(4).Lemma 6.1 AA |= iff Ke satisfiable.~Proof () Fix ~ = (dr1 , . . . , drk ) Tpk AA |= x (x). Then,~ob(A), fix type respective disjunct (x) holds AAdenote . Define first-order model domain ob(A) dr(T ) taking:|= B [c] iff c (B) = >, c ob(A) dr(T ) B Bcon(T )(B unary predicate B defined p. 22). easy check |= Ke .() Suppose Ke satisfiable. model Ke domainob(A) dr(T ). see AA |= , suffices take functions dri definedby:43fiArtale, Calvanese, Kontchakov & Zakharyaschevdri (B) = > iff |= B [dri ], dri dr(T ) B Bcon(T ),(B) = > iff |= B [a], ob(A) B Bcon(T ).Details left reader.qfollows Lemmas 6.1 5.17 Corollary 5.16 have:HCorollary 6.2 satisfiability instance checking problems DL-LiteNbool , DL-Litebool(HN )DL-Litebool KBs AC0 data complexity.(HN )HProof DL-LiteNbool DL-Litebool sub-languages DL-Litebool ,(HN )result immediately follows Lemma 6.1 Corollary 5.16. DL-Litebool KB(HN )K0 = (T 0 , A0 ), Lemma 5.17, construct DL-LiteboolKB K = (T , A)0K satisfiable iff K satisfiable (44) holds. latter condition correspondsfollowing first-order sentence^^0 =xy R1T (x, y) R2T (x, y)x PkT (x, x) ,Dis(R1 ,R2 )T 0Irr(Pk )T 0evaluated AA . Therefore, K0 satisfiable iff AA |= 0 . Let = 0 0result replacing SP (t1 , t2 ), Ref(P ) 0 , P (t1 , t2 ) (t1 6= t2 ); seeproof Lemma 5.17. remains observe AA |= iff AA0 |= 0 .qbefore, result depend UNA member DL-Lite familynumber restrictions form q R, q 2 (in particular, DL-LiteHboolfragments).also note transitive roles cannot included languages freeconcerned data complexity:Lemma 6.3 Satisfiability instance checking DL-Litecore KBs extended role transitivity constraints NLogSpace-hard data complexity.Proof Suppose given directed graph. Let P role name. Define ABoxtaking P (ai , aj ) iff edge (ai , aj ) graph. nodereachable node a0 iff DL-Litecore ABox {P (a0 , )} satisfiablemodels transitive P . encoding immediately gives claim lemmadirected graph reachability problem NLogSpace-complete, NLogSpace closedcomplement (see, e.g., Kozen, 2006) TBox {Tra(P )} dependinput.qhand, reduction Lemma 5.18 computable NLogSpace,obtain following:(HN )+Corollary 6.4 Satisfiability instance checking DL-Liteboolcomplete data complexity.KBs NLogSpace-Proof upper bound obtained applying NLogSpace reduction Lemma 5.18using Corollary 6.2. lower bound follows Lemma 6.3.q44fiThe DL-Lite Family Relations6.2 P- coNP-hardness Data ComplexityLet us turn data complexity instance checking DL-Lite logicsarbitrary number restrictions role inclusions. follows results Ortiz et al.(2006) SHIQ, instance checking (and fact query answering) DL-LiteHNboolcoNP data complexity, results Hustadt et al. (2005) Eiter et al. (2008)Horn-SHIQ imply polynomial-time upper bound DL-LiteHFhorn .show upper bounds optimal following sense: onehand, instance checking DL-LiteHFcore P-hard data complexity; hand,HNbecomes coNP-hard DL-LiteHFkrom DL-Litecore (that is, allow negatedconcept names arbitrary number restrictionsin fact, 2 R enough). Noteresults section depend whether adopt UNA not.Theorem 6.5 instance checking (and query answering) problem DL-LiteHFkrom KBsdata-hard coNP (with without UNA).Proof proof reduction unsatisfiability problem 2+2CNF,known coNP-complete (Schaerf, 1993). Given 2+2CNF formula=n^(ak,1 ak,2 ak,3 ak,4 ),k=1ak,j one propositional variables a1 , . . . , , construct KB (T , )whose TBox depend . use object names f , ck , 1 k n,ai , 1 m, role names S, Sf Pj , Pj,t , Pj,f , 1 j 4, concept namesD.Define set following assertions, 1 k n:S(f, ck ),P1 (ck , ak,1 ),P2 (ck , ak,2 ),P3 (ck , ak,3 ),P4 (ck , ak,4 ),let consist axioms2 Pj v ,Pj,f v Pj ,Pj,t v Pj ,Pj,t v Pj,f ,Pj,fPj,tv A,P1,f u P2,f u P3,t u P4,t vSf ,v A,for1 j 4,(48)1 j 4,(49)1 j 4,(50)1 j 4,(51)(52)2 v ,(53)Sf v S,(54)Sf v D.(55)Note axiom (52) belong DL-LiteHFkrom conjunctionsleft-hand side. However, eliminated help Lemma 5.9. let us prove(T , ) |= D(f ) iff satisfiable.() Suppose satisfiable |= (T , ). Define assignmenttruth values f propositional variables taking a(ai ) = iff aIi AI . false45fiArtale, Calvanese, Kontchakov & Zakharyascheva, k, 1 k n, a(ak,1 ) = a(ak,2 ) = f a(ak,3 ) = a(ak,4 ) = t.view (50), j, 1 j 4, cIk (Pj,t )I (Pj,f )I , (49),cIk (Pj )I . Therefore, (48) (51), cIk (Pj,t )I a(ak,j ) = cIk (Pj,f )Ia(ak,j ) = f, hence, (52), cIk (Sf )I . (53) (54), f (Sf )I ,which, (55), f DI . follows (T , ) |= D(f ).() Conversely, suppose satisfiable. assignmenta(ak,1 ) = a(ak,2 ) = a(ak,3 ) = f a(ak,4 ) = f, 1 k n. Define taking= x | 1 yk | 1 k n z ,aIi = xi , 1 m,cIk = yk , 1 k n,f = z,AI = xi | a(ai ) = yk | 1 k n z ,= (y , aI ) | 1 k n, a(a ) = (x , x ) | a(a ) = (z, z) ,Pj,tk k,jk,j= (y , aI ) | 1 k n, a(a ) = f (x , x ) | a(a ) = f ,Pj,fk,jk k,jP , 1 j 4,PjI = Pj,tj,fSfI = (z, yk ) | a(ak,1 ak,2 ak,3 ak,4 ) = f = ,= (z, yk ) | 1 k n ,DI = z | a() = f = .hard check |= (T , ) 6|= D(f ).qTheorem 6.6 instance checking (and query answering) problem DL-LiteHNcoreKBs data-hard coNP (with without UNA).Proof proof reduction unsatisfiability problem 2+2CNF.HFmain difference previous one DL-LiteHNcore , unlike DL-Litekrom , cannot expresscovering conditions like (50). turns out, however, use number restrictionsrepresent constraints kind. Given 2+2CNF formula , take ABoxconstructed proof Theorem 6.5. ( independent) TBox , describingmeaning representation terms , also defined wayproof, except axiom (50) replaced following set axioms:Tj,1 v Tj ,2 Tjv ,Pj v Tj,1 ,Tj,1uTj,2Tj,2 v Tj ,v(56)(57)Pj v Tj,2 ,Tj,3,2 Tj v Pj,tTj,3 v Tj ,(58)(59)Tj,3 v Pj,f ,(60)Tj , Tj,1 , Tj,2 , Tj,3 fresh role names, j, 1 j 4. Note axioms (52)(59) belong DL-LiteHNcore conjunctions left-hand side,46fiThe DL-Lite Family Relationseasily eliminate using Lemma 5.9. remains prove (T , ) |= D(f )iff satisfiable.() Suppose satisfiable |= (T , ). Define assignmenttruth values f propositional variables taking a(ai ) = iff aIi AI .false a, k, 1 k n, a(ak,1 ) = a(ak,2 ) = f, a(ak,3 ) = a(ak,4 ) = t.j, 1 j 4, cIk (Pj )I ; (58), cIk (Tj,1 )I , (Tj,2 )I .(cI , v ) . v 6= v cI ( 2 )Iv1 , v2 (cIk , v1 ) Tj,112jj,2k 2kand, (60), cIk (Pj,t )I . Otherwise, v1 = v2 = v, v (Tj,3) (59),(56) (57), cIk (Tj,3 )I , which, (60), cIk (Pj,f )I . Therefore,cIk (Pj,t )I (Pj,f )I , (49), cIk (Pj )I . Thus, (48) (51), cIk (Pj,t )Ia(ak,j ) = cIk (Pj,f )I a(ak,j ) = f, hence, (52), cIk (Sf )I .(53) (54), f (Sf )I , which, (55), f DI . follows(T , ) |= D(f ).() Conversely, suppose satisfiable. assignmenta(ak,1 ) = a(ak,2 ) = a(ak,3 ) = f a(ak,4 ) = f, 1 k n. Define taking= xi | 1 yk | 1 k n uk,j,1 , uk,j,2 | 1 j 4, 1 k n z ,aIi = xi , 1 m,cIk = yk , 1 k n,f = z,AI = {xi | a(ai ) = t},= (y , aI ) | 1 k n, a(a ) = , 1 j 4,Pj,tk,jk k,j= (y , aI ) | 1 k n, a(a ) = f , 1 j 4,Pj,fk,jk k,jP , 1 j 4,PjI = Pj,tj,f= (y , uTj,1k k,j,1 ) | 1 k n , 1 j 4,= (y , uTj,2k k,j,2 ) | 1 k n, a(ak,j ) =(yk , uk,j,1 ) | 1 k n, a(ak,j ) = f , 1 j 4,= (y , uTj,3k,j,1 ) | 1 k n, a(ak,j ) = f , 1 j 4,TI ,TjI = Tj,1j,2SfI = (z, yk ) | a(ak,1 ak,2 ak,3 ak,4 ) = f = ,= (z, yk ) | 1 k n ,DI = z | a() = f = .hard check |= (T , ) 6|= D(f ).qnext lower bound would follow Theorem 6, item 2 work Calvaneseet al. (2006); unfortunately, proof incorrect cannot repaired.Theorem 6.7 instance checking (and query answering) problem DL-LiteHFcore KBsdata-hard P (with without UNA).47fiArtale, Calvanese, Kontchakov & ZakharyaschevProof proof reduction entailment problem Horn-CNF, knownP-complete (see, e.g., Borger et al., 1997, Exercise 2.2.4). Given Horn-CNF formula=n^(ak,1 ak,2 ak,3 )k=1p^al,0 ,l=1ak,j al,0 one propositional variables a1 , . . . , , constructKB (T , ) whose TBox depend . need object names c1 , . . . , cnvk,j,i , 1 k n, 1 j 3, 1 (for variable, take one object namepossible occurrence variable non-unit clause), role names S, StPj , Pj,t , 1 j 3, concept name A.Define set containing assertions:S(v1,1,i , v1,2,i ), S(v1,2,i , v1,3,i ), S(v1,3,i , v2,1,i ), S(v2,1,i , v2,2,i ), S(v2,2,i , v2,3,i ), . . .. . . , S(vn,2,i , vn,3,i ), S(vn,3,i , v1,1,i ),Pj (vk,j,i , ck )iffA(v1,1,i )al,0 = ai ,iffak,j = ai ,1 m,1 m, 1 k n, 1 j 3,1 m, 1 l p(all objects variable organized S-cycle Pj (vk,j,i , ck ) iffvariable ai occurs kth non-unit clause jth position). Let consistfollowing concept role inclusions:St v S,(61)2 v ,(62)v St ,St(63)v A,(64)2 P1 vP1,t v P1 ,v P1,t ,P1,tu2 P32 P2 v ,(65)P2,t v P2 ,(66)v P2,t ,(67)v ,(68)P3,t v P3 ,(69)P2,tvP3,t,(70)P3,t v A.(71)before, axiom, namely (70), belong DL-LiteHFcoreconjunction left-hand side, eliminated helpLemma 5.9. aim show (T , ) |= A(v1,1,i0 ) iff |= ai0 .() Suppose |= ai0 . Consider arbitrary model (T , ) defineassignment truth values f propositional variables a(ai ) = iffv1,1,iAI , 1 m. (61)(64), i, 1 m, either vk,j,iAI ,k, j 1 k n, 1 j 3, vk,j,i/ , k, j 1 k n,1 j 3. Now, a(ak,1 ) = a(ak,2 ) = t, 1 k n then, (65)(67),cIk (P1,t) , (P2,t) . (70), cIk (P3,t) hence, (68) (69), vk,3,i(P3,t )I ,48fiThe DL-Lite Family RelationsSt ,Pj,t , PjPja1 a2 a3y1a2 a4 a5y2.xk,j,i.zk,j,ia1a2a3a4a5Figure 5: model satisfying (T , ), = (a1 a2 a3 ) (a2 a4 a5 ).ak,3 = ai , means, (71), vk,3,iAI , v1,1,iAI a(ai ) = t.follows a() = t, hence a(ai0 ) = t, which, definition, means v1,1,iAI .0conclude (T , ) |= A(v1,1,i0 ).() Conversely, suppose 6|= ai0 . assignment a() =a(ai0 ) = f. construct model (T , ) 6|= A(v1,1,i0 ). Definetaking= xk,j,i , zk,j,i | 1 k n, 1 j 3, 1 yk | 1 k n ,cIk = yk , 1 k n,= xk,j,i , 1 k n, 1 j 3, 1 m,vk,j,iAI = xk,j,i | 1 k n, 1 j 3, a(ai ) = ,[SI =Si , Si = (xk,1,i , xk,2,i ), (xk,2,i , xk,3,i ), (xk,3,i , xk1,1,i ) | 1 k n1imk 1 = k + 1 k < n, k 1 = 1 k = n,[StI =Si ,1ima(ai )=tPjI = (xk,j,i , yk ) | 1 k n, ai = ak,j(xk,j,i , zk,j,i ) | 1 k n, ai 6= ak,j , 1 j 2,P3I = (xk,3,i , yk ) | 1 k n, ai = ak,3 ,= (xPj,tk,j,i , yk ) | 1 k n, ai = ak,j , a(ai) =(xk,j,i , zk,j,i ) | 1 k n, ai 6= ak,j , 1 j 2,= (xP3,tk,3,i , yk ) | 1 k n, ai = ak,3 , a(ai ) = .routine check indeed |= (T , ) 6|= A(v1,1,i0 ). See Figure 5example.q49fiArtale, Calvanese, Kontchakov & Zakharyaschev7. Query Answering: Data Complexitypositive existential query answering problem known data-complete coNPcase DL-LiteHNbool : upper bound follows results Ortiz et al. (2006),lower bound established DL-Litekrom Calvanese et al. (2006), Schaerf(1993). case DL-LiteHFhorn , query answering data-complete P, followsresults Hustadt et al. (2005) Eiter et al. (2008) Horn-SHIQ,0DL-LiteHhorn AC (Calvanese et al., 2006).fact, coNP upper bound holds extension DL-LiteHNbool role disjointness (a)symmetry constraints (this follows Glimm et al., 2007, Theorem 10;cf. Remark 5.21). conjecture result holds role (ir)reflexivity constraints.main result section following:Theorem 7.1 positive existential query answering problem logics DL-LiteNhorn ,(HN )H0DL-Litehorn DL-Litehorn AC data complexity.(HN )Proof Suppose given consistent DL-Litehorn KB K0 = (T 0 , A0 ) (withconcept role names occurring TBox 0 ) positive existential query prenex(HN )form q(~x) = ~y (~x, ~y ) signature K0 . Consider DL-Litehorn KB K = (T , A)(HN )provided Lemma 5.17 (the language DL-Litehorndefined Section 5.3).Lemma 7.2 every tuple ~a object names K0 , K0 |= q(~a) iff |= q(~a)untangled models K.Proof () Suppose K0 |= q(~a) untangled model K. Lemma 5.17view consistency K0 , ensures (44) holds, |= K0therefore, |= q(~a).() Suppose 0 |= K0 . Lemma 5.17, model K domain0coincides 0 symbols K0 . |= q(~a), must 0 |= q(~a),K0 |= q(~a) required.qNext show that, Ke Horn sentence, enough consider one specialmodel I0 K formulation Lemma 7.2. Let M0 minimal Herbrand model(the universal Horn sentence) Ke . remind reader (for details consult, e.g., Apt, 1990;Rautenberg, 2006) M0 constructed taking intersection Herbrandmodels Ke , is, models based domain consists constant symbolsKe i.e., = ob(A) dr(T ); cf. Remark 5.15. followingM0 |= B [c]iffKe |= B (c),B Bcon(T ) c .Let I0 untangled model K induced M0 . Denote domain I0 I0 .Property (copy) Remark 5.15 provides us function cp : I0 .two consequences Lemma 5.14. First,aIi 0 B I0iffK |= B(ai ),B Bcon(T ) ai ob(A).50(72)fiThe DL-Lite Family RelationsSecond, every R role (T ), RI0 6= RI 6= , models K. Indeed,RI0 6= M0 |= (R) [dr]. Therefore, (T {R v }, A) satisfiable, thusRI 6= , |= K. Moreover, RI0 6=w B I0iffK |= R v B,B Bcon(T ) w I0 cp(w) = dr. (73)Lemma 7.3 I0 |= q(~a) |= q(~a) untangled models K.Proof Suppose |= K. q(~a) positive existential sentence, enough constructhomomorphism h : I0 I. remind reader that, (forest), domain I0I0 partitioned disjoint trees Ta , ob(A). Define depth point w I0length shortest path respective tree root. Denote Wmset points depth m; particular, W0 = {aI0 | ob(A)}. construct hunion maps hm , 0, hm defined Wm following properties:hm+1 (w) = hm (w), w Wm ,(am ) every w Wm , w B I0 hm (w) B , B Bcon(T );(bm ) u, v Wm , (u, v) RI0 (hm (u), hm (v)) RI , R role (T ).basis induction, set h0 (aIi 0 ) = aIi , ai ob(A). Property (a0 ) follows(72) (b0 ) (ABox) Remark 5.15.induction step, suppose hm already defined Wm , 0. Sethm+1 (w) = hm (w) w Wm . Consider arbitrary v Wm+1 \ Wm . (forest),unique u Wm (u, v) Ea , Ta . Let `a (u, v) = S. Then,(copy), cp(v) = inv(ds). (role), u (S)I0 and, (am ), hm (u) (S)I ,means w (hm (u), w) . Set hm+1 (v) = w. cp(v) = inv(ds)(inv(S))I0 6= , follows (73) v B I0 w0 B wheneverw0 (inv(S))I . w (inv(S))I , obtain (am+1 ) v. show (bm+1 ), noticethat, (role), (w, v) RI0 , w Wm+1 , two cases: eitherw Wm+1 \ Wm , w = v Id vT R, w Wm , w = u vT R.former case, (hm+1 (v), hm+1 (v)) RI IdI identity relation (role).latter case, (u, v) I0 ; hence (hm+1 (u), hm+1 (v)) and, vT R,(hm+1 (u), hm+1 (v)) RI .qAssume that, query q(~x) = ~y (~x, ~y ), ~y = y1 , . . . , yk ,quantifier-free formula. next lemma shows case check whether I0 |= q(~a)suffices consider points depth m0 I0 , m0depend |A|.Lemma 7.4 Let m0 = k + |role (T )|. I0 |= ~y (~a, ~y ) assignment a0Wm0 (i.e., a0 (yi ) Wm0 i) I0 |=a0 (~a, ~y ).Proof Suppose I0 |=a (~a, ~y ), assignment I0 , yi ,1 k, a(yi )/ Wm0 . Let minimal subset ~y contains yievery either P (y 0 , y) P (y, 0 ) subformula , 0role name P . Let yj > |role (T )| a(yj ) Wm51fiArtale, Calvanese, Kontchakov & Zakharyascheva(y)/ Wm1 (for convenience, W1 = before). Clearly, exists:a(yi )/ Wm0 , k variables and, (forest), relations P I0 connect pointWn \ Wn1 point Wn+1 \ Wn2 , n 1. Let w = a(yj ) pointTa . w Wm \ Wm1 , cp(w) = dr, R role (T ).|role (T )| distinct labels labeled tree Ta view (copy), point udepth > |role (T )|, point u0 depth |role (T )| Tacp(u) = cp(u0 ); (iso), trees generated u u0 isomorphic. So,isomorphism g labeled tree generated w (which contains a(y), )onto labeled tree generated point depth |role (T )| Ta . Define newassignment aY taking aY (y) = g(a(y)) aY (y) = a(y) otherwise. (copy),(concept) (role) I0 |=aY (~a, ~y ) aY (y) Wm0 , .aY (yj )/ Wm0 j, repeat described construction. k iterationsshall obtain assignment a0 required lemma.qcomplete proof Theorem 7.1, encode problem K |= q(~a)? modelchecking problem first-order formulas. precisely way Section 6.1,fix signature contains unary predicates A, A, concept name A, binarypredicates P , P , role name P , represent ABox K first-ordermodel AA domain ob(A). define first-order formula ,q (~x)signature (i) ,q (~x) depends q A, (ii) AA |= ,q (~a)iff I0 |= q(~a).begin defining formulas B (x), B Bcon(T ), describe typeselements ob(A) model I0 following sense (see also (72)):AA |= B [ai ]iff aIi 0 B I0 ,B Bcon(T ) ai ob(A).(74)0 (x), 1 (x), . . . formulasformulas defined fixed-points sequences BBone free variable,(A(x),B = A,0B (x) =Eq R (x), B = q R,_i1i10B(x) = B(x)B(x) B(x) , 1,1kB1 uuBk vBext(T )Eq RT (x) given (46). (As Section 6.1, simplify presentation use ext(T ) instead .) clear that, B Bcon(T ),(x) i+1 (x) (i.e., every (x) equivalent i+1 (x) first-order logic),BBBB(x) j (x) every B Bcon(T ) j i. minimumBBN (x).exceed N = |Bcon(T )|, set B (x) = BNext introduce sentences B,dr , B Bcon(T ) dr dr(T ), describetypes elements dr(T ) following sense (see also (73)):AA |= B,driffw B I0 , B Bcon(T ) w I0 cp(w) = dr.(75)(By (concept), definition correct.) sentences defined similarly B (x).Namely, B Bcon(T ) dr dr(T ), inductively define sequence52fiThe DL-Lite Family Relations01B,dr, B,dr, . . . taking0B,dr= 0B,drB,dr= iB,dr_i1i1B, 1,,drB,dr1kB1 uuBk vBext(T )iB,dr = , 0, whenever B 6= R0R,dr = x inv(R) (x)iR,dr =_i1inv(R),ds,1.dsdr(T )i+1clear |role (T )|N B,drB,dr, B Bcon(T )|role (T )|Ndr dr(T ). set B,dr = B,dr.consider directed graph GT = (VT , ET ), VT set equivalenceclasses [R], [R] = {R0 | R R0 }, R empty model , ETset pairs ([Ri ], [Rj ])(path) |= inv(Ri ) v q Rjeither inv(Ri ) 6vT Rj q 2,Rj proper sub-role satisfying (path). ([Ri ], [Rj ]) ET iff,ABox A0 , whenever minimal untangled model I0 (T , A0 ) contains copy w inv(dri0 ),Ri0 [Ri ], w connected copy inv(drj0 ), Rj0 [Rj ], relationsRj vT S.Recall given query q(~x) = ~y (~x, ~y ), quantifier-freepositive formula ~y = y1 , . . . , yk . Let ,m0 set paths graph GTlength m0 . precisely,,m0 = ([R1 ], [R2 ], . . . , [Rn ]) | 1 n m0 , ([Rj ], [Rj+1 ]) ET , 1 j < n .R, 0 ,m0 role R role (T ), write 0 one following threeconditions satisfied: (i) = 0 Id vT R, (ii) .[S] = 0 (iii) = 0 .[inv(S)],role vT R.Let kT ,m0 set k-tuples form ~ = (1 , . . . , k ), ,m0 . Intuitively,evaluating query ~y (~x, ~y ) I0 , bound, non-distinguished, variable yimapped point w Wm0 . However, first-order model AA containpoints Wm0 \ W0 , represent them, use following trick. (forest),every point w Wm0 uniquely determined pair (a, ), aI0 roottree Ta containing w, sequence labels `a (u, v) path aI0 w.follows unraveling procedure (path) ,m0 . So, formula,q define assume yi range W0 represent firstcomponent pairs (a, ), whereas second component encoded ith member~ (these yi confused yi original query q, rangeWm0 ). order treat arbitrary terms occurring (~x, ~y ) uniform way,set t~ = , = ob(A) = xi , t~ = , = yi (the distinguished variables xiobject names mapped W0 require second componentpairs).Given assignment a0 Wm0 denote split(a0 ) pair (a, ~ ),assignment AA ~ = (1 , . . . , k ) kT ,m053fiArtale, Calvanese, Kontchakov & Zakharyaschevdistinguished variable xi , a(xi ) = aI0 = a0 (xi );bound variable yi , a(yi ) = = ([R1 ], . . . , [Rn ]), n m0 , aI0root tree containing a0 (yi ) R1 , . . . , Rn sequencelabels `a (u, v) path aI0 a0 (yi ).every pair (a, ~ ), however, corresponds assignment Wm0 paths~ may exist I0 : GT represents possible paths models fixedTBox varying ABox. follows unraveling procedure, point Wm0 \ W0corresponds ob(A) = ([R], . . . ) ,m0 iff enough R-witnesses0RA, i.e., iff AA |= q)R [a] q R [a], q QT . Thus, every (a, ~~ = (1 , . . . , k ), assignment a0 Wm0 split(a0 ) = (a, ~ ) iff AA |=a ~ (~y ),^_0q(1 ,...,k ) (y1 , . . . , yk ) =Ri (yi ) q Ri (yi )R1ik6=qQTRi , 1 k 6= , = ([Ri ], . . . ).define now, every ~ kT ,m0 , concept name role name R,((t),t~ = ,A~ (t) =A,inv(ds) , t~ = 0 .[S], 0 ,m0 ,~~R (t1 , t2 ), t1 = t2 = ,RR~ (t1 , t2 ) =(t1 = t2 ),t~1 t~2 either t~1 6= t~2 6= ,,otherwise,RT (y1 , y2 ) given (47). claim that, every assignment a0 Wm0(a, ) = split(a0 ),I0 |=a0 A(t)iffAA |=a A~ (t),I0 |=a0 R(t1 , t2 )iffAA |=a R~ (t1 , t2 ),concept names terms t,roles R terms t1 , t2 .(76)(77)A(a), A(xi ) A(yi ) = claim follows (74). A(yi ) = 0 .[S],(copy), cp(a(yi )) = inv(dr), R [S]; claim follows (75).R(yi1 , yi2 ) i1 = i2 = , claim follows (ABox). Let us consider caseR(yi1 , yi2 ) i2 6= : a0 (yi2 )/ W0 thus, (role), I0 |=a0 R(yi1 , yi2 ) iffa0 (yi1 ), a0 (yi2 ) tree Ta , ob(A), i.e., AA |=a (yi1 = yi2 ),either (a0 (yi1 ), a0 (yi2 )) Ea `a (a0 (yi1 ), a0 (yi2 )) = vT R,(a0 (yi2 ), a0 (yi1 )) Ea `a (a0 (yi2 ), a0 (yi1 )) = inv(S) vT R,Ra0 (yi1 ) = a0 (yi2 ) Id vT R, i.e., i1 i2 .cases similar left reader.Finally, let ~ (~x, ~y ) result attaching superscript ~ atom_,q (~x) = ~y~ (~x, ~y ) ~ (~y ) .~kT ,m054fiThe DL-Lite Family Relationsfollows (76)(77), every assignment a0 Wm0 , I0 |=a0 (~x, ~y ) iffAA |=a ~ (~x, ~y ) (a, ) = split(a0 ). converse direction notice that, AA |=a ~ (~y )assignment a0 Wm0 split(a0 ) = (a, ~ ).Clearly, AA |= ,q (~a) iff I0 |= q(~a), every tuple ~a. also note that, everypair tuples ~a ~b object names ob(A), ~ (~a, ~b) positive existential sentenceinequalities, domain-independent.10 also easily seen that, ~b,~ (~b) domain-independent. follows minimality I0 ,q (~a) domainindependent, tuple ~a object names ob(A).Finally, note resulting query contains |role (T )|k(k+|role (T )|) disjuncts. q8. DL-Lite without Unique Name Assumptionsection, unless otherwise stated, assume interpretations respectUNA, is, may aIi = aIj distinct object names ai aj . consequencerelation |=noUNA refers class interpretations.Description logics without UNA usually extended additional equalityinequality constraints form:ai ajai 6 aj ,ai , aj object names. semantics quite obvious: |= ai aj iffaIi = aIj , |= ai 6 aj iff aIi 6= aIj . equality inequality constraints supposedbelong ABox part knowledge base. noted, however, reasoningequalities LogSpace-reducible reasoning without them:Lemma 8.1 every KB K = (T , A), one construct LogSpace sizeKB K0 = (T , A0 ) without equality constraints |= K iff |= K0 , everyinterpretation I.Proof Let G = (V, E) undirected graphV = ob(A),E = (ai , aj ) | ai aj aj ai[ai ] set vertices G reachable ai . Define A0 removingequality constraints replacing every ai aj [ai ] minimal j. Noteminimal j computed LogSpace: enumerate object names ajrespect order indexes j check whether current aj reachableai G. remains recall reachability undirected graphs SLogSpace-completeSLogSpace = LogSpace (Reingold, 2008).qmentioned Section 5.3, logics form DL-LiteHfeel whetheradopt UNA not. observation Lemmas 5.17, 5.18 8.1 hand,obtain following result consequence Theorem 5.13:10. query q(~x) said domain-independent case AA |=a q(~x) iff |=a q(~x),domain contains ob(A), active domain AA , AA = AAA P = P AA , conceptrole names P .55fiArtale, Calvanese, Kontchakov & ZakharyaschevTheorem 8.2 without UNA, combined complexity, (i) satisfiabilityHDL-LiteHbool KBs NP-complete; (ii) satisfiability DL-Litehorn KBs P-complete;H(iii) satisfiability DL-LiteHkrom DL-Litecore KBs NLogSpace-complete. results hold even KBs contain role disjointness, (a)symmetry, (ir )reflexivity transitivity constraints, equalities inequalities.hand, Corollary 6.2 Lemmas 5.17, 5.18 8.1 derivefollowing:Theorem 8.3 Without UNA, satisfiability instance checking DL-LiteHbool KBs00AC data complexity. problems also AC KBs contain roledisjointness, (a)symmetry (ir )reflexivity constraints inequalities. However,LogSpace-complete KBs may contain equalities, NLogSpace-completerole transitivity constraints allowed.also note complexity results (Corollary 5.12, Theorems 6.5, 6.6 6.7)logics DL-LiteHFDL-LiteHNdepend UNA.section, analyze combined data complexity reasoning logics(HF )(HN )form DL-LiteDL-Lite(as well fragments) without UNA.obtained known results summarized Table 2 page 17.(HN )8.1 DL-Lite: Arbitrary Number Restrictionsfollowing theorem shows interaction number restrictionspossibility identifying objects ABox results higher complexity.Theorem 8.4 Without UNA, satisfiability DL-LiteNcore KBs (even without equalityinequality constraints) NP-hard combined data complexity.Proof proof reduction following variant 3SAT problemcalled monotone one-in-three 3SAT known NP-complete (Garey & Johnson, 1979):given positive 3CNF formula=n^ak,1 ak,2 ak,3 ,k=1ak,j one propositional variables a1 , . . . , , decide whetherassignment variables aj exactly one variable true clausesk. encode problem language DL-LiteNcore , need object names ai ,1 k n, 1 m, ck tk , 1 k n, role names P , conceptnames A1 , A2 , A3 . Let ABox containing following assertions:S(a1i , a2i ), . . . , S(ain1 , ani ), S(ani , a1i ),1 m,S(t1 , t2 ), . . . , S(tn1 , tn ), S(tn , t1 ),P (ck , tk ),1 k n,P (ck , akk,j ), Aj (akk,j ),1 k n, 1 j 3,56fiThe DL-Lite Family Relationslet TBox following axioms:A1 v A2 ,A2 v A3 ,A3 v A1 ,2 v ,4 P v .Clearly, (T , ) DL-LiteNcore KB depend (so covercombined data complexity). claim answer monotone one-in-three3SAT problem positive iff (T , ) satisfiable without UNA.() Suppose |= (T , ). Define assignment truth values fpropositional variables taking a(ai ) = iff (a1i )I = (t1 )I . aim showa(ak,j ) = exactly one j {1, 2, 3}, k, 1 k n. j {1, 2, 3},(cIk , (akk,j )I ) P . Moreover, (akk,i )I 6= (akk,j )I 6= j. cIk ( 3 P )I(cIk , (tk )I ) P , must (akk,j )I = (tk )I unique j {1, 2, 3}. followsfunctionality that, 1 k n, (a1k,j )I = (t1 )I exactly onej {1, 2, 3}.() Let assignment satisfying monotone one-in-three 3SAT problem. Takeai0 a(ai0 ) = (clearly, i0 exists, otherwise a() = f) constructinterpretation = (I , ) taking:= yk , z k | 1 k n xki | a(ai ) = f, 1 m, 1 k n ,cIk = yk (tk )I = z k , 1 k n,(xki , a(ai ) = f,(aki )I =1 m, 1 k n,z k , a(ai ) = t,= ((a1i )I , (a2i )I ), . . . , ((ain1 )I , (ani )I ), ((ani )I , (a1i )I ) | 1 ,P = (cIk , (tk )I ), (cIk , (akk,1 )I ), (cIk , (akk,2 )I ), (cIk , (akk,3 )I ) | 1 k n .readily checked |= (T , ).qfact, lower bound optimal:(HN )(HN )+Theorem 8.5 Without UNA, satisfiability DL-LiteNDL-Lite, DL-LiteKBs equality inequality constraints NP-complete combined datacomplexity {core, krom, horn, bool}.Proof lower bound immediate Theorem 8.4, matching upper bound(HN )+proved following non-deterministic algorithm. Given DL-LiteboolKBK = (T , A),guess equivalence relation ob(A);select equivalence class ai / representative, say ai , replace every occurrence ai / ai ;fail equalities inequalities violated resulting ABoxi.e.,contains ai 6 ai ai aj , 6= j;57fiArtale, Calvanese, Kontchakov & Zakharyaschevotherwise, remove equality inequality constraints ABox denoteresult A0 ;(HN )+use NP satisfiability checking algorithm DL-LiteboolKB K0 = (T , A0 ) consistent UNA.decide whetherClearly, algorithm returns yes, 0 |= K0 , 0 respecting UNA,construct model K (not necessarily respecting UNA) extending 00following interpretation object names: aI = aIi , whenever ai representativea/ (I coincides 0 symbols). Conversely, |= K takeequivalence relation defined ai aj iff aIi = aIj . Let 0 constructedremoving interpretations object names representativesequivalence classes . follows 0 respects UNA 0 |= K0 , algorithmreturns yes.q(HF )8.2 DL-Lite: Functionality Constraints(HF )+Let us consider DL-Liteboolfragments. following lemma showslogics reasoning without UNA reduced polynomial time sizeABox reasoning UNA.(HF )+Lemma 8.6 every DL-LiteboolKB K = (T , A) equality inequality con(HF )+straints, one construct polynomial time |A| DL-LiteboolKB K0 = (T , A0 )A0 contains equalities inequalities K satisfiable without UNA iff K0satisfiable UNA.Proof follows identifying aj ak mean replacing occurrenceak aj . construct A0 first identifying aj ak , aj ak A,removing equality A, exhaustively applying following procedureA:2 R v R(ai , aj ), R(ai , ak ) CleT (A), distinct aj ak ,identify aj ak (recall functional R cannot transitive sub-rolesthus CleT (A) enough).resulting ABox contains ai 6 ai , ai , then, clearly, K satisfiable,add A(ai ) A(ai ) ABox, concept name A. Finally, removeinequalities ABox denote result A0 . clear A0computed polynomial time that, without UNA, K satisfiable iff K0satisfiable. suffices show K0 satisfiable without UNA iff satisfiableUNA. implication () trivial.() Observe procedure ensureseqR,a1,R 2 v , R vT ob(A0 )(HN )(see page 37 definitions). Let K00 DL-LiteboolKB provided Lemma 5.17K0 . follows property proofs Lemma 5.14 Corollary 5.1658fiThe DL-Lite Family RelationsK00 satisfiable without UNA (K00 )e satisfied first-order modelconstants interpreted domain element. (K00 )e universalfirst-order sentence containing equality, satisfiable first-order modelconstants interpreted distinct elements. follows proofs Lemma 5.14Corollary 5.16 first-order model unraveled model J K00respecting UNA. Lemma 5.17, J model K0 .qreduction cannot done better P, shown next theorem:Theorem 8.7 Without UNA, satisfiability DL-LiteFcore KBs (even without equalityinequality constraints) P-hard combined data complexity.Proof proof reduction entailment problem Horn-CNF (as proofTheorem 6.7). Let=n^ak,1 ak,2 ak,3p^al,0l=1k=1Horn-CNF formula, ak,j al,0 one propositional variablesa1 , . . . , ak,1 , ak,2 , ak,3 distinct, k, 1 k n. encode Pkcomplete problem |= ai ? language DL-LiteFcore need object names t, ai ,1 k n, 1 m, fk gk , 1 k n, role names P , Q, .ABox contains following assertionsS(a1i , a2i ), . . . , S(an1, ani ), S(ani , a1i ),1 m,P (akk,1 , fk ), P (akk,2 , gk ), Q(gk , akk,3 ), Q(fk , akk,1 ),(t, a1l,0 ),1 k n,1 l p,TBox asserts roles functional:2 P v ,2 Q v ,2S v2 v .Clearly, K = (T , A) DL-LiteFcore KB depend . claim|= aj iff (T , {T (t, a1j )}) satisfiable without UNA. show this, sufficesprove |= aj iff K |=noUNA (t, a1j ).() Suppose |= aj . derive aj using following inference rules:|= al,0 l, 1 l p;|= ak,1 |= ak,2 , k, 1 k n, |= ak,3 .show K |=noUNA (t, a1j ) induction length derivation aj .basis induction trivial. assume aj = ak,3 , |= ak,1 , |= ak,2 , k,1 k n, K |=noUNA (t, a1k,1 ) (t, a1k,2 ). Suppose also |= K. Since00functional, (a1k,1 )I = (a1k,2 )I . Since functional, (akk,1 )I = (akk,2 )I , k 0 ,1 k 0 n, particular, k 0 = k. Then, since P functional, fkI = gkI , which,00functionality Q, (akk,3 )I = (akk,1 )I . Finally, since functional, (akk,3 )I = (akk,1 )I ,59fiArtale, Calvanese, Kontchakov & Zakharyaschevk 0 , 1 k 0 n, particular, k 0 = 1. Thus, |= (t, a1j ) thereforeK |=noUNA (t, a1j ).() Suppose 6|= aj . assignment a() =a(aj ) = f. Construct interpretation taking= xki | a(ai ) = f, 1 k n, 1 z k , uk , vk | 1 k n w ,(xki , a(ai ) = f,(aki )I =1 k n 1 m,z k , a(ai ) = t,tI = w, = (w, z 1 ) ,= ((a1i )I , (a2i )I ), . . . , ((an1)I , (ani )I ), ((ani )I , (a1i )I ) | 1 ,(vk , a(ak,2 ) = f,fkI = uk gkI =1 k n,uk , a(ak,2 ) = t,P = ((akk,1 )I , fkI ), ((akk,2 )I , gkI ) | 1 k n ,QI = (gkI , (akk,3 )I ), (fkI , (akk,1 )I ) | 1 k n .readily checked |= K 6|= (t, a1j ), K 6|=noUNA (t, a1j ).qresult strengthens NLogSpace lower bound instance checkingDL-LiteFcore proved Calvanese et al. (2008).(HF )(HF )+Corollary 8.8 Without UNA, satisfiability DL-LiteFDL-Lite, DL-LiteKBs, {core, krom, horn}, equalities inequalities P-complete combined data complexity.(HF )(HF )+Without UNA, satisfiability DL-LiteFKBsbool , DL-Litebool DL-Liteboolequalities inequalities NP-complete combined complexity P-complete datacomplexity.Proof upper bounds follow Lemma 8.6 corresponding upper boundsUNA case. NP lower bound combined complexity obvious polynomiallower bounds follow Theorem 8.7.q8.3 Query Answering: Data ComplexityP coNP upper bounds query answering without UNA follow results Horn-SHIQ (Hustadt et al., 2005; Eiter et al., 2008) SHIQ (Ortiz et al., 2006,2008; Glimm et al., 2007), respectively (see discussion beginning Section 7).present following result:Theorem 8.9 Without UNA, positive existential query answering DL-LiteHhorn KBsrole disjointness, (a)symmetry, (ir )reflexivity constraints inequalities AC0data complexity. problem LogSpace-complete if, additionally, equalitiesallowed KBs.60fiThe DL-Lite Family RelationsProof proof follows lines proof Theorem 7.1 uses observationmodels without UNA give answers untangled counterparts.precisely, let KB K0 = (T 0 , A0 ) above. Suppose consistent. Let q(~x)positive existential query signature K0 . Given K0 , Lemma 5.17 providesus KB K. easy see K DL-LiteHhorn KB extended inequalityconstraints. following analogue Lemma 7.2, also allows us get ridinequalities:Lemma 8.10 every tuple ~a object names K0 , K0 |=noUNA q(~a) iff |= q(~a)untangled models K (respecting UNA).Proof () Suppose K0 |=noUNA q(~a) untangled model K. respectsUNA, Lemma 5.17 view satisfiability K0 , ensures (44) holds,|= K0 therefore, |= q(~a).() Suppose 0 |= K0 . construct interpretation J 0 respecting UNA follows.0000Let J disjoint union ob(A). Define function h : J taking00h(a) = aI , ob(A), h(w) = w, w , let00000aJ = a,AJ = u | h(u) AIP J = (u, v) | (h(x), h(v)) P ,object, concept role name a, A, P . Clearly, J 0 respects UNA J 0 |= K0 .also follows h homomorphism.Lemma 5.17, model K domain J 0 coincidesJ 0 symbols K0 . |= q(~a), must J 0 |= q(~a), since hhomomorphism, 0 |= q(~a). Therefore, K0 |=noUNA q(~a) required.qremaining part proof exactly Theorem 7.1 (since may assumeK DL-LiteHhorn KB containing inequality constraints).LogSpace-completeness case equalities follows Lemma 8.1.q9. Conclusionarticle, investigated boundaries extended DL-Lite family descriptionlogics providing thorough comprehensive understanding interactionvarious DL-Lite constructs impact computational complexity reasoning.studied 40 different logics, classified according five mutually orthogonal features:(1) presence absence role inclusion assertions, (2) form allowed conceptinclusion assertions, distinguishing four main logical groups called core, Krom, Horn,Bool, (3) form allowed numeric constraints, ranging none, global functionality constraints only, arbitrary number restrictions, (4) presence absenceunique name assumption (and equalities inequalities object names,assumption dropped), (5) presence absence standard role constraintsrole disjointness, role symmetry, asymmetry, reflexivity, irreflexivity transitivity. resulting logics, studied combined data complexity KBsatisfiability instance checking, well data complexity answering positiveexistential queries.61fiArtale, Calvanese, Kontchakov & Zakharyaschevquery answering= instance checkingcoNPquery answering.Legendsatisfiabilitycombined complexitywith/without UNArole inclusionsFNExpTimeNPPNLogSpaceUNArole inclusionsFUNArole inclusionscoNPPAC0oolNinstance checkingdata complexityBroKcoHn.FNFigure 6: Complexity basic DL-Lite logics.obtained tight complexity results illustrated Figure 6, combinedcomplexity satisfiability represented height vertical dashed lines,data complexity instance checking size color circle top lines(recall satisfiability instance checking reducible complementother). data complexity query answering core Horn logics, shownleft-hand side separating vertical plane, coincides data complexityinstance checking; Krom Bool logics, shown right-hand side plane,query answering always data-complete coNP. upper layer shows complexitylogics role inclusions, case depend whether adoptUNA not. middle lower layers deal logics without role inclusionsUNA dropped adopted, respectively. layers, twelvelanguages arranged 4 3 grid: one axis shows type concepts inclusionsallowed (Horn, core, Krom, Bool), type number restrictions (none,global functionality F arbitrary N ). observations order:UNA without role inclusions, number restrictions increasecomplexity reasoning, depends form concept inclusions allowed.hand, without form number restrictions, logics roleinclusions insensitive UNA; again, complexity determinedshape concept inclusions only.either cases, instance checking AC0 data complexity,means problems first-order rewritable.62fiThe DL-Lite Family RelationsWithout UNA adopted without either disjunctions role inclusions, functionalityleads P-completeness instance checking data complexity, suggestsreducibility Datalog.data complexity, difference core Horn logics,Krom Bool ones, means core Krom logicsextended conjunctions left-hand side concept inclusions free.(HF )(HN )Finally, logics DL-LiteDL-Lite(qualified) number restrictions role inclusions, whose interaction restricted conditions (A1 )(A3 ),complexity reasoning always coincides complexity fragments DL-LiteFand, respectively, DL-LiteNwithout role inclusions, matter whether adopt UNAnot.Role disjointness, symmetry asymmetry constraints added(HN )(HF )languages without changing complexity. fact, DL-LiteDL-Litelogics contain types constraints together role reflexivity irreflexivity. conjecture (ir)reflexivity constraints added logics withoutaffecting complexity. However, extend DL-Lite logic role transitivityconstraints, combined complexity satisfiability remains same, instancechecking query answering become data-hard NLogSpace. additionequality object nameswhich makes sense UNA droppedleadsincrease membership AC0 LogSpace-completeness data complexity;results remain unchanged.list DL constructs considered paper far complete.example, would interest analyze impact nominals, role chains Booleanoperators roles computational behavior DL-Lite logics. Another interestingpractically important problem investigate depth interaction variousconstructs aim pushing restrictions like (A1 )(A3 ) far possible.One main ideas behind DL-Lite logics provide efficient access largeamounts data high-level conceptual interface. supposed achievedrepresenting high-level view information managed system DL-LiteTBox , data stored relational database ABox A, rewriting positive existential queries knowledge base (T , A) standard first-order queriesdatabase represented A. approach believed viable because, numberDL-Lite logics, query answering problem AC0 data complexity; cf. Theorems 7.1, 8.9 Figure 6. first-order rewriting technique implementedvarious system, notably QuOnto (Acciarri et al., 2005; Poggi et al., 2008b),query, relying ontology-to-relational mappings, data stored standard relationaldatabase management system, Owlgres (Stocker & Smith, 2008), accessABox stored Postgres database (though, best knowledge, latterimplementation incomplete conjunctive query answering). noted, however,size rewritten query substantially larger size originalquery, cause problems even efficient database query engine.positive existential query q TBox , two major sources highcomplexity first-order formula ,q proof Theorem 7.1: (i) formulasB (x) computing whether ABox object instance concept B (and formulas63fiArtale, Calvanese, Kontchakov & ZakharyaschevR,dr computing whether objects outgoing R-arrows instances B), (ii)(HN )disjunction paths ~ graph GT . case DL-Litecore , size(HN )B (x) linear |T |, DL-Litehorn become exponential (however, variousoptimizations possible). size disjunction (ii) exponential numbernon-distinguished variables q. One way removing source (i) would extendgiven database (ABox) precomputing Horn closure ABox respectTBox storing resulting data supplementary database. approachadvocated Lutz et al. (2008) querying databases via description logic EL.could also promising Horn fragments expressive description logicsSHIQ (Hustadt et al., 2005; Hustadt, Motik, & Sattler, 2007)containing DL-LiteHFhornsub-languagefor data complexity instance checking (Hustadt et al., 2005,2007) conjunctive query answering polynomial (Eiter et al., 2008). disadvantageusing supplementary database necessity update every time ABoxchanged. would interesting investigate alternative approach DL-Lite logicscompare approach described above. Another important problemcharacterize queries disjunction (ii) represented formulapolynomial size.unique name assumption replaced OWL constructs sameAsdifferentFrom (i.e., 6), challenging problem investigate possible waysdealing equality (inequality require special treatment shownproof Lemma 8.10). Although reasoning equality LogSpace-reducible reasoning without (cf. Lemma 8.1), lose property first-order rewritability,computing equivalence classes may costly real-world applications.DL-Lite logics among examples DLs usually complexnon-standard reasoning problemssuch checking whether one ontology conservativeextension another one respect given signature (Kontchakov et al., 2008),computing minimal modules ontologies respect (Kontchakov et al., 2009)uniform interpolants (Wang, Wang, Topor, & Pan, 2008)can supported practicalreasoning tools. However, first steps made direction,research needed order include reasoning problems tools standardOWL toolkit. would also interesting investigate unification problem DL-Litelogics (Baader & Narendran, 2001).Finally, exist certain parallels Horn logics DL-Lite family, EL,Horn-SHIQ first-order language tuple equality generating dependencies,TGDs EGDs, used theory databases (see, e.g., Gottlob & Nash, 2008).investigations relationships logics may lead deeper understandingrole description logics play database framework.Acknowledgmentsresearch partially supported FET project TONES (Thinking ONtologiES), funded within EU 6th Framework Programme contract FP6-7603,large-scale integrating project (IP) OntoRule (ONTOlogies meet Business RULEsONtologiES), funded EC ICT Call 3 FP7-ICT-2008-3, contract number FP7231875. thank referees constructive criticism, comments, suggestions.64fiThe DL-Lite Family RelationsReferencesAbiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.Acciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &Rosati, R. (2005). QuOnto: Querying ontologies. Proc. 20th Nat. Conf.Artificial Intelligence (AAAI 2005), pp. 16701671.Apt, K. (1990). Logic programming. van Leeuwen, J. (Ed.), Handbook TheoreticalComputer Science, Volume B: Formal Models Sematics, pp. 493574. ElsevierMIT Press.Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2007a). DL-Litelight first-order logic. Proc. 22nd Nat. Conf. Artificial Intelligence(AAAI 2007), pp. 361366.Artale, A., Calvanese, D., Kontchakov, R., Ryzhikov, V., & Zakharyaschev, M. (2007b).Reasoning extended ER models. Proc. 26th Int. Conf. ConceptualModeling (ER 2007), Vol. 4801 Lecture Notes Computer Science, pp. 277292.Springer.Artale, A., Cesarini, F., & Soda, G. (1996). Describing database objects conceptlanguage environment. IEEE Trans. Knowledge Data Engineering, 8 (2), 345351.Artale, A., Parent, C., & Spaccapietra, S. (2007). Evolving objects temporal informationsystems. Ann. Mathematics Artificial Intelligence, 50, 538.Baader, F., & Narendran, P. (2001). Unification concepts terms description logics. J.Symbolic Computation, 31 (3), 277305.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook: Theory, Implementation Applications.Cambridge University Press. (2nd edition, 2007).Beeri, C., Levy, A. Y., & Rousset, M.-C. (1997). Rewriting queries using views descriptionlogics. Proc. 16th ACM SIGACT SIGMOD SIGART Symp. PrinciplesDatabase Systems (PODS97), pp. 99108.Berardi, D., Calvanese, D., & De Giacomo, G. (2005). Reasoning UML class diagrams.Artificial Intelligence, 168 (12), 70118.Bergamaschi, S., & Sartori, C. (1992). taxonomic reasoning conceptual design. ACMTrans. Database Systems, 17 (3), 385422.Boppana, R., & Sipser, M. (1990). complexity finite functions. van Leeuwen, J.(Ed.), Handbook Theoretical Computer Science, Volume A: Algorithms Complexity, pp. 757804. Elsevier MIT Press.Borger, E., Gradel, E., & Gurevich, Y. (1997). Classical Decision Problem. PerspectivesMathematical Logic. Springer.Borgida, A., & Brachman, R. J. (2003). Conceptual modeling description logics.Baader et al. (Baader et al., 2003), chap. 10, pp. 349372.65fiArtale, Calvanese, Kontchakov & ZakharyaschevCalvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., & Rosati, R. (2007).Ontology-based database access. Proc. 15th Ital. Conf. Database Systems(SEBD 2007), pp. 324331.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rosati, R., & Ruzzi, M.(2008). Data integration DL-Lite ontologies. Schewe, K.-D., & Thalheim,B. (Eds.), Revised Selected Papers 3rd Int. Workshop Semantics DataKnowledge Bases (SDKB 2008), Vol. 4925 Lecture Notes Computer Science, pp.2647. Springer.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite:Tractable description logics ontologies. Proc. 20th Nat. Conf. ArtificialIntelligence (AAAI 2005), pp. 602607.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Datacomplexity query answering description logics. Proc. 10th Int. Conf.Principles Knowledge Representation Reasoning (KR 2006), pp. 260270.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007a). OWLmodel football leagues?. Proc. 3rd Int. Workshop OWL: ExperiencesDirections (OWLED 2007), Vol. 258 CEUR Workshop Proceedings.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007b). Tractablereasoning efficient query answering description logics: DL-Lite family. J.Automated Reasoning, 39 (3), 385429.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2008a). Inconsistency tolerance P2P data integration: epistemic logic approach. InformationSystems, 33 (4), 360384.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2008b). Path-basedidentification constraints description logics. Proc. 11th Int. Conf.Principles Knowledge Representation Reasoning (KR 2008), pp. 231241.Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002a). Description logics informationintegration. Kakas, A., & Sadri, F. (Eds.), Computational Logic: Logic Programming Beyond, Essays Honour Robert A. Kowalski, Vol. 2408 Lecture NotesComputer Science, pp. 4160. Springer.Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002b). framework ontology integration. Cruz, I., Decker, S., Euzenat, J., & McGuinness, D. (Eds.), EmergingSemantic Web Selected Papers First Semantic Web Working Symposium,pp. 201214. IOS Press.Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998a). Descriptionlogic framework information integration. Proc. 6th Int. Conf.Principles Knowledge Representation Reasoning (KR98), pp. 213.Calvanese, D., Lenzerini, M., & Nardi, D. (1998b). Description logics conceptual datamodeling. Chomicki, J., & Saake, G. (Eds.), Logics Databases InformationSystems, pp. 229264. Kluwer Academic Publishers.Calvanese, D., Lenzerini, M., & Nardi, D. (1999). Unifying class-based representation formalisms. J. Artificial Intelligence Research, 11, 199240.66fiThe DL-Lite Family RelationsCorona, C., Ruzzi, M., & Savo, D. F. (2009). Filling gap OWL 2 QLQuOnto: ROWLKit. Proc. 22nd Int. Workshop Description Logics(DL 2009), Vol. 477 CEUR Workshop Proceedings.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse ontologies: Theory practice. J. Artificial Intelligence Research, 31, 273318.Decker, S., Erdmann, M., Fensel, D., & Studer, R. (1999). Ontobroker: Ontology basedaccess distributed semi-structured information. Meersman, R., Tari, Z.,& Stevens, S. (Eds.), Database Semantic: Semantic Issues Multimedia Systems,chap. 20, pp. 351370. Kluwer Academic Publishers.Dolby, J., Fokoue, A., Kalyanpur, A., Ma, L., Schonberg, E., Srinivas, K., & Sun, X. (2008).Scalable grounded conjunctive query evaluation large expressive knowledgebases. Proc. 7th Int. Semantic Web Conf. (ISWC 2008), Vol. 5318 LectureNotes Computer Science, pp. 403418. Springer.Eiter, T., Gottlob, G., Ortiz, M., & Simkus, M. (2008). Query answering description logic Horn-SHIQ. Proc. 11th Eur. Conference Logics ArtificialIntelligence (JELIA 2008), pp. 166179.Franconi, E., & Ng, G. (2000). i.com tool intelligent conceptual modeling. Proc.7th Int. Workshop Knowledge Representation meets Databases (KRDB 2000),Vol. 29 CEUR Workshop Proceedings, pp. 4553.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-Completeness. W. H. Freeman.Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. Doherty, P., Mylopoulos, J., & Welty,C. (Eds.), Proc. 10th Int. Conf. Principles Knowledge RepresentationReasoning (KR 2006), pp. 187197.Glimm, B., Horrocks, I., Lutz, C., & Sattler, U. (2007). Conjunctive query answeringdescription logic SHIQ. Proc. 20th Int. Joint Conf. Artificial Intelligence(IJCAI 2007), pp. 399404.Goasdoue, F., Lattes, V., & Rousset, M.-C. (2000). use CARIN languagealgorithms information integration: Picsel system. Int. J. CooperativeInformation Systems, 9 (4), 383401.Gottlob, G., & Nash, A. (2008). Efficient core computation data exchange. J.ACM, 55 (2), 149.Hayes, P. (2004). RDF semantics. W3C Recommendation. http://www.w3.org/TR/rdf-mt/.Heflin, J., & Hendler, J. (2001). portrait Semantic Web action. IEEE IntelligentSystems, 16 (2), 5459.Heymans, S., Ma, L., Anicic, D., Ma, Z., Steinmetz, N., Pan, Y., Mei, J., Fokoue, A.,Kalyanpur, A., Kershenbaum, A., Schonberg, E., Srinivas, K., Feier, C., Hench, G.,Wetzstein, B., & Keller, U. (2008). Ontology reasoning large data repositories.Hepp, M., De Leenheer, P., de Moor, A., & Sure, Y. (Eds.), Ontology Management,67fiArtale, Calvanese, Kontchakov & ZakharyaschevSemantic Web, Semantic Web Services, Business Applications, Vol. 7 SemanticWeb Beyond Computing Human Experience, pp. 89128. Springer.Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDFOWL: making web ontology language. J. Web Semantics, 1 (1), 726.Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. J. Interest Group Pure Applied Logic, 8 (3), 239264.Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning description logics reductiondisjunctive Datalog. J. Automated Reasoning, 39 (3), 351384.Hustadt, U., Motik, B., & Sattler, U. (2005). Data complexity reasoning expressivedescription logics. Proc. 19th Int. Joint Conf. Artificial Intelligence(IJCAI 2005), pp. 466471.Immerman, N. (1999). Descriptive Complexity. Springer.Klyne, G., & Carroll, J. J. (2004). Resource description framework (RDF): Conceptsabstract syntax. W3C Recommendation. http://www.w3.org/TR/rdf-concepts/.Kontchakov, R., Pulina, L., Sattler, U., Schneider, T., Selmer, P., Wolter, F., & Zakharyaschev, M. (2009). Minimal module extraction DL-Lite ontologies usingQBF solvers. Proc. 21st Int. Joint Conf. Artificial Intelligence (IJCAI 2009), pp. 836840.Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2008). tell differenceDL-Lite ontologies?. Proc. 11th Int. Conf. PrinciplesKnowledge Representation Reasoning (KR 2008), pp. 285295.Kontchakov, R., & Zakharyaschev, M. (2008). DL-Lite role inclusions. Domingue, J.,& Anutariya, C. (Eds.), Proc. 3rd Asian Semantic Web Conf. (ASWC 2008),Vol. 5367 Lecture Notes Computer Science, pp. 1630. Springer.Kozen, D. (2006). Theory Computation. Springer.Lenzerini, M. (2002). Data integration: theoretical perspective. Proc. 21st ACMSIGACT SIGMOD SIGART Symp. Principles Database Systems (PODS 2002),pp. 233246.Levy, A. Y., & Rousset, M.-C. (1998). Combining Horn rules description logicsCARIN. Artificial Intelligence, 104 (12), 165209.Lutz, C., Toman, D., & Wolter, F. (2008). Conjunctive query answering EL usingdatabase system. Proc. 5th Int. Workshop OWL: Experiences Directions (OWLED 2008).McGuinness, D., & Wright, J. R. (1998). Conceptual modelling configuration: description logic-based approach. Artificial Intelligence Engineering Design, Analysis,Manufacturing. Special Issue Configuration, 12, 333344.Meyer, T., Lee, K., & Booth, R. (2005). Knowledge integration description logics.Proc. 20th Nat. Conf. Artificial Intelligence (AAAI 2005), pp. 645650.Noy, N. F. (2004). Semantic integration: survey ontology-based approaches. SIGMODRecord, 33 (4), 6570.68fiThe DL-Lite Family RelationsOrtiz, M., Calvanese, D., & Eiter, T. (2006). Characterizing data complexity conjunctivequery answering expressive description logics. Proc. 21st Nat. Conf.Artificial Intelligence (AAAI 2006), pp. 275280.Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity query answering expressive description logics via tableaux. J. Automated Reasoning, 41 (1), 6198.Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.Perez-Urbina, H., Motik, B., & Horrocks, I. (2009). comparison query rewritingtechniques DL-Lite. Proc. 22nd Int. Workshop Description Logics(DL 2009), Vol. 477 CEUR Workshop Proceedings.Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008a).Linking data ontologies. J. Data Semantics, X, 133173.Poggi, A., Rodriguez, M., & Ruzzi, M. (2008b). Ontology-based database accessDIG-Mastro OBDA Plugin Protege. Clark, K., & Patel-Schneider,P. F. (Eds.), Proc. 4th Int. Workshop OWL: Experiences Directions(OWLED 2008 DC).Rautenberg, W. (2006). Concise Introduction Mathematical Logic. Springer.Reingold, O. (2008). Undirected connectivity log-space. J. ACM, 55 (4).Schaerf, A. (1993). complexity instance checking problem concept languagesexistential quantification. J. Intelligent Information Systems, 2, 265278.Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions complements. Artificial Intelligence, 48 (1), 126.Stocker, M., & Smith, M. (2008). Owlgres: scalable OWL reasoner. Proc. 5thInt. Workshop OWL: Experiences Directions (OWLED 2008).Tobies, S. (2001). Complexity results practical algorithms logics Knowledge Representation. Ph.D. thesis, LuFG Theoretical Computer Science, RWTH-Aachen, Germany.Toman, D., & Weddell, G. E. (2005). interaction inverse features pathfunctional dependencies description logics. Proc. 19th Int. Joint Conf.Artificial Intelligence (IJCAI 2005), pp. 603608.Toman, D., & Weddell, G. E. (2008). keys functional dependencies first-classcitizens description logics. J. Automated Reasoning, 40 (23), 117132.Vardi, M. (1982). complexity relational query languages (extended abstract).Proc. 14th ACM SIGACT Symp. Theory Computing (STOC82), pp.137146.Vollmer, H. (1999). Introduction Circuit Complexity: Uniform Approach. Springer.Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2008). Forgetting concepts DL-Lite.Bechhofer, S., Hauswirth, M., Hoffmann, J., & Koubarakis, M. (Eds.), Proc.5th Eur. Semantic Web Conf. (ESWC 2008), Vol. 5021 Lecture Notes ComputerScience, pp. 245257. Springer.69fiJournal Artificial Intelligence Research 36 (2009) 387-414Submitted 07/09; published 11/09Approximate Strong Equilibrium Job Scheduling GamesMichal Feldmanmfeldman@huji.ac.ilSchool Business AdministrationCenter Study Rationality,Hebrew University Jerusalem, Israel.Tami Tamirtami@idc.ac.ilSchool Computer Science,Interdisciplinary Center, Herzliya, Israel.AbstractNash Equilibrium (NE) strategy profile resilient unilateral deviations,predominantly used analysis multiagent systems. downside NEnecessarily stable deviations coalitions. Yet, show paper,cases, NE exhibit stability coalitional deviations, benefitsjoint deviation bounded. sense, NE approximates strong equilibrium.Coalition formation key issue multiagent systems. provide frameworkquantifying stability performance various assignment policies solutionconcepts face coalitional deviations. Within framework evaluate givenconfiguration according three measures: (i) IR min : maximal number ,exists coalition minimal improvement ratio among coalition members , (ii) IR max : maximal number , exists coalitionmaximal improvement ratio among coalition members , (iii) DR max :maximal possible damage ratio agent outside coalition.analyze measures job scheduling games identical machines. particular,provide upper lower bounds three measures NE wellknown assignment rule Longest Processing Time (LPT).results indicate LPT performs better general NE. However, LPTbest possible approximation. particular, present polynomial time approximationscheme (PTAS) makespan minimization problem provides scheduleIR min 1 + given . respect computational complexity, showgiven NE 3 identical machines 2 unrelated machines, NP-harddetermine whether given coalition deviate every member decreases cost.1. Introductionconsider job scheduling systems, n jobs assigned identical machinesincur cost equal total load machine assigned to.1problems widely studied recent years game theoretic perspective (Koutsoupias & Papadimitriou, 1999; Andelman, Feldman, & Mansour, 2007; Christodoulou,Koutsoupias, & Nanavati, 2004; Czumaj & Vocking, 2002; A. Fiat & Olonetsky., 2007).contrast traditional setting, central designer determines allocationjobs machines participating entities assumed obey protocol, mul1. cost function characterizes systems jobs processed parallel, jobsparticular machine single pick-up time, need share resource simultaneously.c2009AI Access Foundation. rights reserved.fiFeldman & Tamirtiagent systems populated heterogeneous, autonomous agents, often displayselfish behavior. Different machines jobs may owned different strategic entities,typically attempt optimize objective rather global objective.Game theoretic analysis provides us mathematical tools study situations,indeed extensively used recently analyze multiagent systems. trendmotivated part emergence Internet, composed distributedcomputer networks managed multiple administrative authorities shared userscompeting interests (Papadimitriou, 2001).game theoretic models applied job scheduling problems, well network games (e.g., Fabrikant, Luthra, Maneva, Papadimitriou, & Shenker, 2003; Albers,Elits, Even-Dar, Mansour, & Roditty, 2006; Roughgarden & Tardos, 2002; Anshelevich,Dasgupta, Kleinberg, Tardos, Wexler, & Roughgarden, 2004), use solution conceptNash equilibrium (NE), strategy agent best responsestrategies agents. NE powerful tool analyzing outcomes competitive environments, notion stability applies unilateral deviations.numerous multiagent settings, selfish agents stand benefit cooperating formingcoalitions (Procaccia & Rosenschein, 2006). Therefore, even single agent profitunilateral deviation, NE might still stable group agents coordinating joint deviation, profitable members group. strongernotion stability exemplified strong equilibrium (SE) solution concept, coinedAumann (1959). strong equilibrium, coalition deviate improve utilityevery member coalition.M3325M3358M2325M235810M1M155(a)224(b)Figure 1: example configuration (a) Nash equilibrium resilientcoordinated deviations, since jobs load 5 2 profit deviation demonstrated (b).example, consider configuration depicted Figure 1(a). figures,job represented rectangle whose width corresponds jobs length. jobsscheduled specific machine form vertical concatenation rectangles. example,Figure 1(a) three machines, M1 processes two jobs length 5. Noteinternal order jobs effect, since cost job defined loadmachine assigned to. configuration NE since job reduce costunilateral deviation. One might think NE identical machines mustalso sustainable joint deviations. Yet, already observed (Andelman388fiApproximate Strong Equilibriumet al., 2007), may true.2 example, configuration resilientcoordinated deviation coalition consisting four jobs load 52 deviating configuration (b), jobs load 5 decrease costs 10 8,jobs load 2 improve 5 4. Note cost two jobs load 3(which members coalition) increases.example above, every member coalition improves cost (multiplicative) factor 45 . much coalition improve? boundimprovement ratio? turn out, example fact extreme onesense clarified below. Thus, NE completely stable coordinated deviations, settings, provide us notion approximatestability coalitional deviations (or approximate strong equilibrium).also consider subclass NE schedules, produced Longest Processing Time(LPT) rule (Graham, 1969). LPT rule sorts jobs non-increasing orderloads greedily assigns job least loaded machine. easy verifyevery configuration produced LPT rule NE (Fotakis, S. Kontogiannis, &Spiraklis, 2002). also SE? Note instance depicted Figure 1, LPTwould produced SE. However, show, always case.paper provide framework studying notion approximate stabilitycoalitional deviations. analysis, consider three different measures. firsttwo measure stability configuration, uses notion improvement ratiojob, defined ratio jobs costs deviation.third measures worst possible effect non-deviating jobs, explainedbelow.31. Minimum Improvement Ratio: definition, members coalition mustreduce cost. is, improvement ratio every member coalition larger1. Clearly, coalition members might share improvement ratio.minimum improvement ratio particular deviation minimal improvement ratiocoalition member. minimum improvement ratio schedule s, denoted IR min (s),maximum possible deviations originated minimal improvementratio deviation. words, coalitional deviation originatingevery member coalition reduces cost factor greater IR min (s).closely related notion suggested Albers (2009), defined strategyprofile -SE coalition agent improve factor. notation, schedule -SE IR min (s) . Albersstudied notion context SE existence cost-sharing games, showedsufficiently large , -SE always exists. justification behind conceptagents may willing deviate improve sufficiently high factor (dueto, example, overhead associated deviation).2. statement holds 3. 2 identical machines, every NE also SE (Andelman et al.,2007).3. Throughout paper, define approximation multiplicative factor. Since improvementdamage ratios three measures presented constants greater one (as shownbelow), additive ratios unbounded. Formally, value possible construct instances(by scaling instances provide multiplicative ratio) cost jobs reduced,cost jobs increased, least additive factor a.389fiFeldman & Tamirthree machines, show every NE 54 -SE. is, coalitiondeviate every member improves factor larger 54 .case, also provide matching lower bound (recall Figure 1 above), holds23. arbitrary m, show every NE (2 m+1)-SE. proof techniquedraws connection makespan approximation approximate stability,makespan configuration defined maximum load machineconfiguration.next consider schedules obtainedLPT rule. show = 3,every LPT configuration ( 21 + 46 )-SE ( 1.1123), also provide matching lower1bound, holds 3. arbitrary m, show upper bound 43 3m.results indicate LPT stable NE respect coalitionaldeviations. Yet, LPT best possible approximation SE. Similar notionapproximation algorithms, define SE-PTAS assignment algorithmgets input additional parameter , specifying close SE scheduleproduces (1 + )-SE time polynomial n, 1/. paper deviseSE-PTAS fixed number machines, also approximates makespan withinfactor 1 + .2. Maximum Improvement Ratio: maximum improvement ratio particulardeviation maximal improvement ratio experienced coalition member.maximum improvement ratio schedule s, denoted IR max (s), maximumpossible deviations originated maximal improvement ratio deviation.words, coalition deviation originating existsmember coalition reduces cost factor greater IR max (s).notion establishes bounds much agent would gain deviatingcoalition agents gain something deviation. Also, notion similarspirit stability large total improvement. also suits environmentsindividuals willing obey specific player long hurt. Interestingly,find given NE configuration, improvement ratio single agent mayarbitrarily large, 3. contrast, LPT configurations three machines,agent improve factor 53 bound tight. Thus, respectIR max , relative stability LPT compared NE significant respect1, believe tight.IR min . arbitrary m, provide lower bound 23. Maximum Damage Ratio: case jobs load 3 Figure 1, jobsmight get hurt result coalitional deviation. third measure considerworst possible effect deviation jobs members deviatingcoalition. Formally, maximum damage ratio maximal ratio costsnon-coalition member deviation. Variants measureconsidered distributed systems, e.g., Byzantine Generals problem (Lamport, Shostak,& Pease, 1982), rational secret sharing (Halpern & Teague, 2004).4 Section 5,prove maximum damage ratio less 2 NE configuration, less4. rational secret sharing protocol, set players, holding share secret, aims jointlyreconstruct it. Viewing protocol game, players utilities typically assumed satisfyfollowing two basic constraints: (i) player prefers learning secret learning it, (ii)conditioned learned secret, player prefers possible players learn it.390fiApproximate Strong Equilibrium23 LPT configuration. bounds hold 3, casesprovide matching lower bounds.summary, results Sections 3-5 (see Table 1) indicate NE configurationsapproximately stable respect IR min measure. Moreover, performancejobs outside coalition would hurt much result coalitional deviation.IR max , results provide additional strength LPT rule, alreadyknown possess attractive properties (with respect to, e.g., makespan approximationstability unilateral deviations).NELPTIR minupper boundm3m=3252 m+144313m12+64lowerbound1254+64IR maxupperlowerbound boundunbounded53 (m=3)21DR maxupper lowerbound bound223232Table 1: results three measures. Unless specified otherwise, results holdarbitrary number machines m.Section 7, study computational complexity aspects coalitional deviations.find NP-hard determine whether NE configuration 3 identicalmachines SE. Moreover, given particular configuration set jobs, NPhard determine whether set jobs engage coalitional deviation.unrelated machines (i.e., job incurs different load machine),hardness results hold already = 2 machines. results might implicationscoalitional deviations computationally restricted agents.Related work: NE shown paper provide approximate stability coalitional deviations. related body work studies well NE approximates optimaloutcome competitive games. Price Anarchy defined ratioworst-case NE optimum solution (Papadimitriou, 2001; Koutsoupias & Papadimitriou, 1999), extensively studied various settings, including job scheduling(Koutsoupias & Papadimitriou, 1999; Christodoulou et al., 2004; Czumaj & Vocking, 2002),network design (Albers et al., 2006; Anshelevich et al., 2004; Anshelevich, Dasgupta, Tardos, Wexler, & Roughgarden, 2003; Fabrikant et al., 2003), network routing (Roughgarden& Tardos, 2002; Awerbuch, Azar, Richter, & Tsur, 2003; Christodoulou & Koutsoupias,2005), more.notion strong equilibrium (SE) (Aumann, 1959) expresses stability coordinated deviations. downside SE games admit SE, evenamongst admitting Nash equilibrium. Various recent works studied existence SE particular families games. example, shown every jobscheduling game (almost) every network creation game, SE exists (Andelman et al.,2007). addition, several papers (Epstein, Feldman, & Mansour, 2007; Holzman & LawYone, 1997, 2003; Rozenfeld & Tennenholtz, 2006) provided topological characterizationexistence SE different congestion games, including routing cost-sharing391fiFeldman & Tamirconnection games. vast literature SE (e.g., Holzman & Law-Yone, 1997, 2003;Milchtaich, 1998; Bernheim, Peleg, & Whinston, 1987) concentrate pure strategiespure deviations, case paper. job scheduling settings, shownAndelman et al. (2007) mixed deviations allowed, often case SEexists. SE exists, clearly, price anarchy respect SE (denotedstrong price anarchy Andelman et al., 2007) significantly better priceanarchy respect NE (Andelman et al., 2007; A. Fiat & Olonetsky., 2007; Leonardi& Sankowski, 2007).Following work, IR min bounds case = 4 machines providedChen (2009), extended bound 45 NE schedules, provided bound( 12 +34530 )1.119 LPT-based schedules.2. Model Preliminariessection give formal description model provide several useful observations properties deviations coalitions.2.1 Resilience Deviations Coalitionsfirst present general game theoretic setting describe specific job schedulingsetting focus paper.game denoted tuple G = hN, (Sj ), (cj )i, N = {1, . . . , n} setplayers, Sj finite action space player j N , cj cost function player j.joint action space players = ni=1 Si . joint action = (s1 , . . . , sn ) S,denote sj actions players j 0 6= j, i.e., sj = (s1 , . . . , sj1 , sj+1 , . . . , sn ).Similarly, set players , also called coalition, denote actionsplayers , respectively. cost function player j maps joint actionreal number, i.e., cj : R.joint action pure Nash Equilibrium (NE) player j N benefitunilaterally deviating action another action, i.e., j N Sj : cj (sj , a)cj (s). pure joint action coalition N specifies action playercoalition, i.e., s0 j Sj . joint action resilient pure deviationcoalition pure joint action s0 cj (s , s0 ) < cj (s) everyj (i.e., players coalition deviate way player strictlyreduces cost). case say deviation s0 = (s , s0 ) profitabledeviation coalition .pure joint action resilient pure deviations coalitionscoalition N profitable deviation s.Definition 2.1 pure strong equilibrium (SE) pure joint action resilientpure deviations coalitions.Clearly, strong equilibrium refinement notion Nash equilibrium (in particular, strong equilibrium, resilient deviations coalitions size 1,coincides definition NE).392fiApproximate Strong Equilibrium2.2 Job Scheduling Identical Machinesjob scheduling setting identical machines characterized set machines ={M1 , . . . , Mm }, set {1, . . . , n} jobs, job j processing time pj . assignmentmethod produces assignment jobs machines, sj denotes machinejob j assigned to. assignment referred schedule configuration (we usetwo terms interchangeably). load machine Mi inPschedule sumprocessing times jobs assigned Mi , Li (s) = j:sj =Mi pj . set jobs, let s() = j {sj } denote set machines members assignedschedule s.makespan schedule load loaded machines. social optimumminimizes makespan, i.e., OP = mins makespan(s).job scheduling setting define job scheduling game jobs players.action space Sj player j N individual machines, i.e., Sj = .joint action space = nj=1 Sj . joint action constitutes schedule. scheduleplayer j N selects machine sj action incurs cost cj (s),load machine sj , i.e., cj (s) = Li (s), sj = Mi . job scheduling game,makespan also highest cost among players. Formally, makespan(s) = maxj cj (s).0Let s0 two configurations. Let Pis,sbinary indicator whose value 11 ,i2job j sj = Mi1 s0j = Mi2 (i.e., job chooses Mi1Mi2 s0 ), 0 otherwise. clear context, abuse notation0denote Pis,sPi1 ,i2 . addition, denote Li (s) Li (s0 ) Li L0i , respectively.1 ,i2Let s0 = (s , s0 ) profitable deviation coalition . improvementratio job j sj = Mi1 s0j = Mi2 (i.e., job migrating machine0Mi1 machine Mi2 ) denoted IRs,s (j) = Li1 (s)/Li2 (s0 ). Clearly, job j ,0IRs,s (j) > 1. damage ratio job j 6 sj = s0j = Mi denoted0DRs,s (j) = Li (s0 )/Li (s).sj 6= s0j , say job j migrates deviation. Note that, terminology,job member profitable deviation even migrate deviation.Yet, every job migrates deviation member deviating coalitiondefinition.Definition 2.2 Given schedules s0 = (s , s0 ), minimal improvement ratio s00respect IR min (s, s0 ) = minj IRs,s (j). addition, minimal improvementratio schedule IR min (s) = maxs0 =(s ,s0 ),N IR min (s, s0 ).Given schedules s0 = (s , s0 ), maximal improvement ratio s0 respect0IR max (s, s0 ) = maxj IRs,s (j). addition, maximal improvement ratioschedule IR max (s) = maxs0 =(s ,s0 ),N IR max (s, s0 ).Given schedules s0 = (s , s0 ), maximal damage ratio s0 respect0DR max (s, s0 ) = maxjN DRs,s (j). addition, maximal damage ratio scheduleDR max (s) = maxs0 =(s ,s0 ),N DR max (s, s0 ).particular, define notion -SE (Albers, 2009) terms minimumimprovement ratio follows:Definition 2.3 schedule -strong equilibrium (-SE) IR min (s) .393fiFeldman & Tamirnext provide several useful observations claims shall use sequel.refer profitable deviation NE-schedule NE-originated profitable deviation.Similarly, profitable deviations schedule produced LPT rule referredLPT-originated profitable deviation.first observation shows NE-originated profitable deviation, jobmigrates machine, job must migrate machine. Formally:Observation 2.4 Let NE let s0 = (s , s0 ) profitable deviation. s0j = Mij , j 0 sj 0 = Mi s0j 0 = Mi0 i0 6= i.obvious, since job j strictly decreases cost migrating machinejob leaves, also profitably migrates unilaterally, contradicting NE.next define special deviation structure, called flower structuredeviations loaded machine.Definition 2.5 Let M1 loaded machine given schedule s. says,s0s,s0deviation s0 obeys flower structure > 1, P1,i= Pi,1= 1 i, j > 1,0s,sPi,j= 0 (See Figure 2).M2M3M1M5M4Figure 2: graph representation coalition 5 machines obeying flower structure.0s,sedge Mi Mj Pi,j= 1.0s,sparticular, = 3, deviation s0 obeys flower structure P1,2=00000s,ss,ss,ss,ss,sP2,1= P1,3= P3,1= 1 P2,3= P3,2= 0. Recall simplicity presentation,0s,swrite sequel Pi,j denote Pi,jalso write Li L0i denote Li (s)Li (s0 ), respectively.Claim 2.6 NE-originated profitable deviation three machines obeys flower structure.Proof: Let NE M1 loaded machine s, let s0 profitabledeviation. first show P2,3 = P3,2 = 0. Assume first P2,3P = P3,2P= 1.Thus, L02 < L3 L03 < L2 . Clearly, since total load change, Li = L0i .Therefore, must hold L01 > L1 . However, profitable deviation increaseload loaded machine. contradiction. Therefore, one P2,3 , P3,21. Assume w.l.o.g P2,3 = 1. Observation 2.4 job leaves M3 ,cannot M2 . Thus, must P3,1 = 1. Similarly, jobM1 .Pleaves PP1,2 = 1, get L01 < L3 , L02 < L1 , L03 < L2 , contradicting Li = L0i .394fiApproximate Strong Equilibrium000P1,3 = 1P getPthat0 L1 < L3 , L2 < L2 (no job added M2 ), L3 < L1 ,contradicting Li = Li again. Thus, P2,3 = 0. proof P3,2 = 0 analogous.remains show P1,2 = P1,3 = P2,1 = P3,1 = 1. know three machinesassigned jobs s0 assigned s. P2,3 = P3,2 = 0.Observation 2.4 job leaves M2 , M3 , therefore, P2,1 = P3,1 = 1. Also,job leaves M1 , thus least one P1,2 , P1,3 equals 1. Assume w.l.o.g P1,2 = 1.show also P1,3 = 1. particular, show L03 > L3 , since P2,3 = 0 mustP1,3 = 1. Assume opposite, L03 L3 . already knowP2,1 = 1.P P1,2 =PThus, L02 < L1 , L01 < L2 , assumption L03 L3 . is, L0i < Li .contradiction.known NE schedule two identical machines also SE (Andelmanet al., 2007). claim, least four jobs change machines profitabledeviation three machines. Clearly, least four jobs change machines coalition> 3 machines. Therefore,Corollary 2.7 Every NE-schedule stable deviations coalitions size threeless.next two propositions characterize coalition deviation three machines.show M1 loaded machine deviation, becomesleast loaded deviation.Proposition 2.8 NE-originated deviation three machines, loads twoless loaded machines increasing, is, L02 > L2 L03 > L3 .Proof: Assume contrary L02 L2 . Claim 2.6, P1,3 P= P3,1 = P1. Thus,0 <L03 < L1 , L01 < L3 , assumption L02 L2 . is,LLi .0contradiction. proof L3 > L3 analogous.Proposition 2.9 NE-originated deviation three machines loaded machine becomes least loaded machine, is, L01 < min(L02 , L03 ).Proof: Claim 2.6, P1,2 = 1, thus L01 < L2 . proposition, L2 < L02 .Thus, L01 < L02 . proof L01 < L03 symmetric.3. -Strong Equilibriumsection, stability configurations measured minimal improvement ratiomeasure. first provide complete analysis (i.e. matching upper lower bounds)three identical machines5 NE LPT. arbitrary m, provide upperbound NE LPT, show lower bounds = 3 hold m.Theorem 3.1 NE schedule three machines 45 -SE.5. note unrelated machines, improvement ratio cannot bounded within finite factoreven two machines. seen simple example two jobs two machines,load vector job 1 (1, ), load vector job 2 (, 1). job assigned machine (for= 1, 2), resulting configuration NE, load 1 machine. However, jobsreduce load 1 swapping.395fiFeldman & TamirProof: Let NE-configuration three machines, let r = IR min (s). Claim2.6, deviation obeys flower Pstructure. Therefore: L01 L2 /r , L01 L3 /r , L02L1 /r , L03 L1 /r. Let P = j pj (also = L1 + L2 + L3 ). Summinginequalities get r (L1 + P )/(L01 + P ).Proposition 3.2 load loaded machine half total load,is, L1 P/2.Proof: Let g = max(L1 L2 , L1 L3 ). flower structure, least two jobsM1 , thus g L1 /2 - since otherwise job would benefit leaving M1 , contradictingNE. definition g, know 2L1 L2 + L3 + 2g, since 2g L1 , getL1 P/2.Distinguish two cases:1. L01 P/5: case r (L1 + P )/(L01 + P ) (3P/2)/(6P/5) = 5/4.2. L01 < P/5: means L02 + L03 > 4P/5 (M2 M3 rest load),is, least one L02 , L03 > 2P/5. W.l.o.g. let M2 . flower structurejob M1 migrates M2 . jobs improvement ratio L1 /L02 , which,Proposition 3.2, less (P/2)/(2P/5) = 5/4. Thus, again, r < 5/4.analysis tight shown Figure 1. Moreover, lower boundextended > 3 adding 3 machines 3 heavy jobs assignedmachines. Thus,Theorem 3.3 3, exists NE schedule IR min (s) = 54 .LPT configurations, bound minimum improvement ratio lower.proof following theorems appear Appendix A.Theorem 3.4 LPT schedule three machines ( 12 +641.1123)-SE.Theorem 3.5 3, exists LPT schedule IR min (s) = 12 +64 .next provide upper bounds arbitrary m. analysis based drawingconnection makespan approximation SE-approximation. Assumegiven schedule r-approximation minimum makespan. showconditions original schedule, subset jobs form coalitionIR min > r, then, considering subset machines, possible get scheduleapparently better optimal one. first define set assignment rulesconnection exists.Definition 3.6 Let schedule instance = hN, i. Given , let =hN , instance induced s, N = {j|sj }. assignment method,A, said subset-preserving , holds sj = sjj N , assignments produced instances I,respectively.396fiApproximate Strong EquilibriumClaim 3.7 LPT subset-preserving method.Proof: proof induction number jobs N . showk 6= N , first k jobs N 0 assigned machine LPT executedNote since N sublist N , jobs Ninput input I.order N . first job scheduled first empty machine among .job j N , induction hypothesis, j scheduled, loadmachines identical load corresponding machines time jscheduled member N . load generated jobs N come jN . Therefore, LPT, j scheduled least loaded machine among machines, is, sj = sj . assume LPT uses deterministic tie-breaking ruleseveral least loaded machines N . Therefore sj = sj also case.Lemma 3.8 Let assignment method (i) subset-preserving, (ii) yields Nashequilibrium, (iii) approximates minimum makespan within factor r, r 1non-decreasing m. Then, produces r-SE.Proof: Assume contradiction exists instance scheduleproduced A, exists coalition improvement ratio every membergreater r. Let coalition minimum size. job jmigrate, set jobs \ {j} smaller coalition, contradicting minimality; therefore, every j , holds sj 6= s0j . next show s() = s0 ().First, s() s0 (), is, every machine s() job j migrates,must exist job migrating it, otherwise, \ {j} smaller coalition, contradictingminimality . Second, s0 () s(), is, every machine job jmigrates, must exist job migrating (otherwise job j improve unilaterally,contradiction NE). Given s() = s0 (), denote set machines, let = |M |. Finally, let N N set jobs assigned machines A,Consider instance = hN , i. Since subset-preserving, jobs Nscheduled exactly schedule scheduled part I.particular, scheduled A, deviation exists, every jobimproves factor greater r, machines take part it.words, pair machines i, j, Pi,j = 1, Li /L0j > r(m) r(m),r(m) approximation ratio machines. hand, sinceOP (I)produces r(m)-approximation, machine i, Li r(m)OP (I),minimum possible makespan machines. Therefore, Pi,j = 1 r(m) <Li /L0jr(m)OP (I).L0jimplies machine j receives least one job,L0j < OP (I).However, since least one job migrated participating machines,deviation machines assigned jobs N loadcontradiction.less OP (I).Let NE machines. Clearly, , induced schedule2set machines also NE. Also, known NE provides (2 m+1)approximation makespan (Finn & Horowitz, 1979; Schuurman & Vredeveld, 2007).2implies Lemma 3.8 applied r = 2 m+1assignment yieldsNE. Therefore,397fiFeldman & TamirCorollary 3.9 NE schedule identical machines (22m+1 )SE.next result direct corollary Lemma 3.8, Claim 3.7, fact LPT1)-approximation makespan (Graham, 1969).provides ( 43 3m1Corollary 3.10 schedule produced LPT identical machines ( 43 3m)SE.bounds tight, gap lower upper boundssmall constant.4. Maximum Improvement Ratiosection, analyze maximum improvement ratio measure. provide complete analysis NE configurations 3, LPT configurations threemachines. lower bound LPT three machines extended arbitrary m.contrast measures consider paper, NE LPT differsmall constant, turns respect maximum improvement ratio, NELPT significantly different. improvement ratio NE-originated deviations arbitrarily high, deviations LPT configurations, highest possibleimprovement ratio participating job less 35 .Theorem 4.1 Fix r 1. 3 machines, exists instancemachines NE IR max (s) > r.Proof: Given r, consider NE-configuration three machines given Figure 3(a).coalition consists {1, 1, 2r, 2r}. improved schedule given Figure 3(b).improvement ratio jobs load 1 2r/2 = r. > 3, dummy machines jobsadded.M32r-112rM32r-12r4r-1M22r-112rM22r-12r4r-1M12r4rM1 1 12r(a)2(b)Figure 3: NE-originated deviation jobs load 1 improvement ratior.contrast NE-originated deviations, LPT-originated deviations ablebound maximum improvement ratio small constant. proof followingclaim given Appendix A.Theorem 4.2 Let LPT schedule three machines. holds IRmax (s) 53 .398fiApproximate Strong Equilibrium1+2m-2+MmM22m-31+2m-2+M2M12m-322m-12m-322m-12m-3Mm221+ 2m-1+M1 1+ 1+ 1+(a)m+m(b)Figure 4: LPT-originated deviation machines job load 1 + assignedM1 improvement ratio arbitrarily close 21m.bound tight, demonstrated Figure 4 = 3 (where im1provement ratio 2= 35 ). Moreover, figure shows lower boundgeneralized 3. Note coalitional deviation example obeysflower structure. believe example tight, flower structure seemsenable largest possible decrease load single machine. job load 1 +remains M1 improves cost 2m 1 + m(1 + ), is, job, j,1IR(j) = 2m1+m(1+) = 2 . Formally,Theorem 4.3 3, exists LPT configuration IR max (s) =12arbitrarily small > 0.5. Maximum Damage Ratiosection, provide results maximum damage profitable deviationimpose jobs take part coalition. Formally, quality configuration measured maxj6 DR(j). provide complete analysis NE LPTconfigurations 3. again, find LPT provides strictly betterperformance guarantee compared general NE: cost job LPT schedulecannot increase factor 32 larger, increase factor arbitrarily close2 NE schedules.Theorem 5.1 m, DR max (s) < 2 every NE configuration s.Proof: Let s0 = (s0 , ) profitable deviation, let M1 loaded machineamong machines either job migrated job migrated into. Observation 2.4, must job migrated M1 . implies mustleast two jobs M1 s, since single job, could benefitdeviation. Therefore, exists job j sj = M1 pj L1 /2. Using factNE again, get machine 6= 1, Li L1 /2 (otherwise job jimprove unilaterally migrating Mi ).addition, every machine job migrates, must hold L0i < L1 .job migrated Mi left machine j load Lj L1 . Combining399fiFeldman & Tamirbounds, get every job j stays machine job0migrates holds DRs,s (j) = L0i /Li < L1 /Li (2Li )/Li = 2.analysis tight shown Figure 3: damage ratio jobs load2r 1 (4r 1)/(2r), arbitrarily close 2. Formally,Theorem 5.2 3, exists NE configuration DR max (s) =2 arbitrarily small > 0.LPT configurations obtain smaller bound:Theorem 5.3 m, DR max (s) <32every LPT configuration s.Proof: Let s0 = (s0 , ) profitable deviation, let M1 loaded machineamong machines either job migrated job migrated into. Since everyLPT configuration NE, M1 must least two jobs (following argumentsproof Theorem 5.1). Assume w.l.o.g lightest (also last) job assignedM1 load 1, denote job 1-job. assumption validminimum improvement ratio invariant linear transformations. Let ` = L1 1. SinceLPT configuration, every machine i, must hold Li ` (otherwise, 1-jobwould assigned different machine). addition, since every machine jjob migrates, Lj L1 , must hold every machine jobmigrates L0i < ` + 1. distinguish two cases.3case (a): ` 2. Then, every machine Mi job migrates, L0i /Li < `+1` 2.case (b): ` < 2. show profitable deviation exists case. ` < 2,M1 exactly 2 jobs, loads ` 1, since LPT assigns jobs non-increasing order.LPT, every machine must (i) one job load least ` (and possiblysmall jobs), (ii) two jobs load least 1 (and possible small jobs). Let k k 0number machines type (i) (ii), respectively (excluding M1 ). Thus,total k + 1 jobs load ` 2k 0 + 1 jobs load 1. deviation, machinejobs load ` 1 together, three jobs load 1. k + 1 machinesassigned k + 1 jobs load ` deviation cannot assigned jobload x. So, end 2k 0 + 1 jobs load 1 assigned k 0 machines.Thus, must machine least three jobs load 1. Contradiction.M31+21+2+3M31+21M21+21+2+3M21+21+3M11+313+3M11+1(a)1+13+22+52+2(b)Figure 5: LPT-originated deviation, damage ratio job load 1 + 2 M3arbitrarily close 32 .analysis tight shown Figure 5. Moreover, adding dummy machinesjobs extended 3. Formally,400fiApproximate Strong EquilibriumTheorem 5.4 3, exists LPT configuration DR max (s) =32 arbitrarily small > 0.6. Approximation Schemesection present polynomial time approximation scheme provides (1 + )SE. PTAS applied fixed number machines.Definition 6.1 vector (l1 , l2 , . . . lm ) smaller (l1 , l2 , . . . lm ) lexicographicallyi, li < li b < i, lb = lb . configuration lexicographically smallervector machine loads L(s) = (L1 (s), . . . , Lm (s)), sorted non increasing order,smaller lexicographically L(s), sorted non increasing order.PTAS combines lexicographically minimal assignment longest k jobsLPT rule applied remaining jobs. value k depends desired approximation ratio (to defined later).Formally, algorithm Ak defined follows:1. Find lexicographically minimal assignment longest k jobs.2. Add remaining jobs greedily using LPT rule.particular, since lexicographically minimal assignment minimizes makespan(given load loaded machine), algorithm PTASminimum makespan problem, restriction known PTAS Graham (1966).Grahams algorithm, step 1, first k jobs scheduled way minimizesmakespan. scheme, requirement schedule long jobs strict.particular, shown Andelman et al. (2007), schedule longest k jobsSE sub-instance.Given , let denote algorithm k =. first showsubset machines , provides (1 + )-approximation makespansubset jobs scheduled . Formally,Lemma 6.2 Let = hN, instance job scheduling machines jobsN . Let output I. given , let N N set jobsscheduled s. Consider instance = hN , i. Let assignmentinduced s. (1 + )-approximation makespan I.Proof: Let LAmax (M ) denote largest completion time machine setschedule produced , let OP (I) denote minimum makespan I. Letdenote largest completion time long job N scheduled minimallexicographic schedule found step 1. Since minimal lexicographic assignment,minimum makespan long jobs N . particular, lower bound OP (I),thus, makespan increased second step, is, Lmax (M ) = ,Otherwise, makespan larger . Let j joboptimal I.determining makespan (the job completes last N ). definition LPT,implies machines busy job j started execution (otherwise401fiFeldman & Tamirjob j could start earlier). Since optimal schedule step 1 intended idles,holds machines busy time interval [0; LAmax (M ) pj ]PnLet P =j=1 pj total processing time n jobs N . above,P m(Lmax (M ) pj ). Also, since jobs sorted non-increasing order processingtimes, pj pk+1 therefore P m(LAmax (M ) pk+1 ). lower boundoptimal solution schedule load machines balanced;P /m, implies LAthus OP (I)max (M ) OP (I) + pk+1 .order bound Lmax (M ) terms OP (I), need bound pk+1 termsfirst bound gap OP (I) OP (I).following assumptionOP (I).used.Claim 6.3 Let z job determining makespan (I). W.l.o.g., z onek long jobs.Proof: Assume makespan (I) determined one long jobs. Let M1machine z scheduled. particular, M1 processes long jobs. Fixschedule M1 repeat PTAS remaining jobs machinesvalue k. Repeat necessary makespan determined job assigned usingLPT rule.Note algorithm still polynomial, PTAS might repeated1 times, constant. approximation ratio improving subinstance: number jobs considered long, among set fewer jobs, is,larger portion jobs scheduled optimally, therefore approximation ratio proofvalid sub-instance. Finally, merging last PTAS result schedulemachines holding long jobs only, get PTAS whole instance, since longjobs scheduled optimally step. Moreover, load machinelower bound makespan sub-instance considered machinegets jobs.+ pk+1 .Claim 6.4 OP (I) OP (I)Proof: Let z job determining makespan (I). Claim 6.3, z assumedassigned step 2 (by LPT rule). z N (I) = LAmax (M ). Else, loadmachine least (I) pz , since otherwise job z assignedone machines . Therefore, even total load N balanced among(I) pz . Since pz pk+1 , OP (I) (I), get, OP (I)+ pk+1 .OP (I) (I) OP (I)Claim 6.5 pk+1 OP (I)k .Proof: Consider k + 1 longest jobs. optimal schedule, machine assignedleast d(k + 1)/me 1 + bk/mc jobs. Since jobs processingtime least pk+1 , conclude OP (I) (1 + bk/mc)pk+1 , implies402fiApproximate Strong Equilibrium0pk+1 OP (I)/(1 + bk/mc). Claim 6.4, pk+1mOP(I)/(1 + bk/mc) (OP (I ) +pk+1 )/(1 + bk/mc). follows pk+1 OP (I)k .BackLmax (I), conclude Lmax (I) OP (I) + pk+1mbound+ ).OP (I)(1 + k ) = OP (I)(1prove main result section, showing schedule produced, (1 + )-SE. stability proved following theorem. runningtime, fixed m, k, minimal lexicographic schedule first k jobs foundO(mk ) steps. Applying LPT rule takes additional O(nlogn). , getrunning time scheme O(mm/ ), is, exponential (that assumedconstant) 1/.Theorem 6.6 produces (1 + )-SE.Proof: proof similar proof Lemma 3.8. Assume contradictionexists instance machines, schedule produced, exists coalition improvement ratio every member larger1 + . Let coalition minimum size. every machine jobj migrates, must exists job migrating it, otherwise, \ {j} also coalitionIR min > 1 + , contradiction minimality . Let denote setmachines part coalition, let N N set jobs assigned ,let = |M |. Consider instance = hN , i, schedule s. Lemmacoalition exists s,6.2, (1 + )-approximation makespan I.machines take part it. Moreover, jobs improves factor(1 + ). words, pair machines i, j, Pi,j = 1,Li /L0j > 1 + . hand, since (1 + )-approximation, machine i,(I)Therefore, Pi,j = 1 1 + < L0i (1+)OP. words,Li (1 + )OP (I).0LjLjmachine j receives least one job,< OP (I).However, since least one job migrated participating machines,deviation machines assigned jobs N loadcontradiction.less OP (I).note 0, schedule produced algorithm NE. Similarstability proof LPT (Fotakis et al., 2002), easy verify jobbenefit leaving machine Mi also shortest job machinebenefit migration. However, independent whether short job, lengthpj , assigned step 1 algorithm (as part minimal lexicographical schedulelong job) step 2 (by LPT), gap Li load machinepj .L0j7. Computational Complexityeasy see one determine polynomial time whether given configurationNE. Yet, SE, task involved. section, provide hardnessresults coalitional deviations.Theorem 7.1 Given NE schedule 3 identical machines, NP-hard determine SE.403fiFeldman & TamirM3B-1B-22B-3M3B-1Jobs A12B-1M2B-1B-22B-3M2B-1Jobs A22B-12BM1B-22B-4M1JobsB-2(b)(a)Figure 6: Partition induces coalition schedule identical machines.Proof: give reduction Partition. Given set n integers a1 , . . . ,total size 2B, question whether subset total size B, constructschedule Figure 6(a). schedule three machines n + 4 jobs loadsa1 , . . . , , B 2, B 2, B 1, B 1. assume w.l.o.g. mini ai 3, else wholeinstance scaled. Thus, schedule 6(a) NE. 3, add 3 machinessingle job load 2B.Claim 7.2 NE schedule Figure 6(a) SE partition.Proof: partition K1 , K2 , total size B, scheduleFigure 6(b) better jobs originated partition instance two(B 2)-jobs. partition jobs improved cost 2B cost 2B 1, (B 2)jobs improved 2B 3 2B 4.Next, show partition initial schedule SE. Theorem 2.7, action coalition three machines, jobs must migrate M1M2 M3 . order decrease load 2B 3, set jobs migrating M1must set two jobs load B 2. Also, must partition jobs moveaway M1 - otherwise, total load M1 least 2B 4 + 3 = 2B 1,improvement (B 2)-jobs. implies jobs M1 splitM2 M3 . However, since partition, one two subsets total loadleast B + 1. jobs join job load B 1 get total load least 2B,improvement 2B-load initial schedule.establishes proof Theorem.direct corollary proof following:Corollary 7.3 Given NE schedule coalition, NP-hard determine whethercoalition deviate.Theorem 7.1 holds 3 identical machines. 2, configurationNE SE (Andelman et al., 2007), therefore possible determinewhether given configuration SE polynomial time. Yet, following theorem showscase unrelated machines, problem NP-hard already = 2.unrelated machines environment, processing time job depends machineassigned. every job j machine i, pi,j denotes processing time jobj processed machine i.Theorem 7.4 Given NE schedule 2 unrelated machines, NP-harddetermine SE.404fiApproximate Strong EquilibriumProof: give reduction Partition. Given n integers a1 , . . . , total size2B, question whether subset total size B, construct followinginstance scheduling: 2 machines n + 1 jobs following loads (for< 1/(n 1)):pi,1 = ai + pi,2 = 2ai + , {1, . . . , n} ; pn+1,1 = B, pn+1,2 = 2B + n.Consider schedule jobs 1, . . . , n M1 , job n + 1 M2 .completion times machines 2B + n. NE.M2Jn+12B+nM2M1J1,,Jn2B+nM1(a)2B+|A2|Jobs A2Jobs A1Jn+12B+|A1|(b)Figure 7: Partition induces coalition schedule related machines.Claim 7.5 NE schedule Figure 7(a) SE partition.Proof: partition A1 , A2 , total size B, schedule givenFigure 7(b) better everyone. completion time M1 2B + |A1 | < 2B + ncompletion time M2 2B + |A2 | < 2B + n.Next, show partition initial schedule SE. Sincepartition, partition A1 , A2 , one two subsets, w.l.o.g., A1 totalsize least B + 1. A1 increase load migrating M2 even alone (bearingload least 2B + 2 + |A1 | instead 2B + n). Therefore, A1 leave M1 .However, A1 stays M1 , job n + 1 better-off staying M2 (since migrates,bears load least 2B + 1 + |A1 | smaller 2B + n |A1 |1/(n 1)).establishes proof Theorem.direct corollary proof following:Corollary 7.6 Given NE schedule unrelated machines coalition, NP-harddetermine whether coalition deviate.8. Conclusions Open Problemspaper study well NE schedules special subset obtainedoutcome LPT assignment rule approximate SE job scheduling games.using two measures IR min IR max . addition, use DR max measurestudy hurtful coalitional deviations agents outside coalition. presentupper lower bounds NE LPT-based schedules, demonstrate LPTbased schedules perform better general NE schedules, gap significantIR max measure. NE LPT, IR min bounded small constant,405fiFeldman & Tamirimplying notion stability coalitional deviations (assuming existencetransition cost). IR max , bounded constant LPT schedules,universal bound NE schedules. Yet, LPT best possible approximationSE, demonstrated SE-PTAS design, computes schedule IR minarbitrarily close 1.problems remain open are:1. IR min measure, gap upper lower bounds > 4 6 .12. IR max LPT-originated deviations, 3 presented lower bound 25matching upper bound 3 = 3. Closing gap general leftopen problem.3. paper focuses case identical machines. would interesting studytopic approximate strong equilibrium additional job scheduling settings. particular,setting uniformly related machines part ongoing research, alreadycase two machines seems rather involved. Note that, mentioned Section 7,unrelated machines, IR min unbounded already two machines.4. measures defined respect strong equilibrium solution concept,profitable deviation defined one every member coalition strictlybenefits. would interesting consider measures introduce respectadditional solution concepts, coalition-proof Nash equilibrium (Bernheim et al.,1987) (which stable profitable deviations stabledeviations sub-coalitions), also respect profitable deviations nonecoalition members worse-off least one member strictly better-off.summary, introduced three general measures stability performanceschedules coalitional deviations. believe measures usedmeasure stability performance various algorithms coalitional deviationsperformance additional settings games. hope see workmakes use measures within framework algorithmic game theory. wouldinteresting study families games Nash equilibria approximate strong equilibriadefined measures introduced here.Acknowledgments. thank Leah Epstein Alon Rosen helpful discussions.also thank anonymous reviewers insightful remarks suggestions. workpartially supported Israel Science Foundation (grant number 1219/09).Appendix A. Bounding IR min IR max LPT-originated Deviationsfirst provide several observation valid LPT-originated deviation.observations used later analysis. Moreover, observations characterizecoalitions might exist schedules produced LPT-rule. Combinedflower structure (that characterizes NE-originated deviations three machines),get set LPT-originated deviation limited must follow strictstructure.Let M1 loaded machine. Assume w.l.o.g lightest (also last) jobassigned M1 load 1, denote job 1-job. assumption valid6. paper provides tight bounds = 3 case = 4 considered Chen (2009).406fiApproximate Strong Equilibriumminimum improvement ratio invariant linear transformations. = 2, 3, denoteKi set (and also total load) jobs remain Mi . Denote Hi,j set(and also total load) jobs migrating Mi Mj . = 1, let K1 , H1,2 , H1,3above, excluding 1-job.next propositions show total size jobs migrating M2 , M3 M1remaining M2 , M3 least large last job M1 .Proposition A.1 H2,1 , H3,1 least 1.Proof: show H2,1 1, proof H3,1 analogous. Assume contradictionH2,1 < 1. Since LPT schedule jobs non-increasing order, jobs composingH2,1 assigned 1-job. Therefore, 1-job assigned, load M2K2 least H1,2 + H1,3 + K1 (else, LPT would assign 1-job M2 ). Thus,K2 H1,2 + H1,3 + K1 . flower structure, job migrating M1 M2 .migration beneficial L02 < L1 . Distinguish two cases:1. 1-job migrates M2 . case, L02 = K2 + H1,2 + 1. Therefore, K2 +H1,2 + 1 < H1,2 + H1,3 + K1 + 1, K2 < H1,3 + K1 . However, above,K2 H1,2 + H1,3 + K1 H1,3 + K1 . contradiction.2. 1-job migrate M2 . case, L02 = K2 + H1,2 . Therefore, K2 +H1,2 < H1,2 + H1,3 + K1 + 1, K2 < H1,3 + K1 + 1. However, above,K2 H1,2 + H1,3 + K1 1 + H1,3 + K1 . contradiction. last inequality followsfact H1,2 empty consists least one job least bigsmallest job M1 .Proposition A.2 K2 , K3 least 1.Proof: first show K2 1. Assume K2 < 1, means 1-job assignedM1 , load M2 composed jobs subset H2,1 only. Therefore,LPT rule, H2,1 K1 + H1,2 + H1,3 . However, Proposition 2.8, L02 > L2 , thereforeH2,1 < H1,2 + 1. Thus, K1 + H1,3 < 1. However, H1,3 1. contradiction. showK3 1, note K3 > 1 similar argument H3,1 H1,2 + H1,3 .Proposition 2.8, L03 > L3 . Therefore H1,3 > H3,1 , implying K1 + H1,2 < 0. contradiction.Theorem 3.4 LPT configuration three machines ( 21 + 46 1.1123)-SE.Proof: Let M1 loaded machine schedule. Recall lightest (alsolast) job assigned M1 1-job load 1. Let ` = L1 1. give LPT scheduledeviation s0 = (s0 , ), let r = IR min (s, s0 ).Claim 2.6, obeys flower structure. Therefore:(i) r L2 /L01 ; (ii) rP000L3 /L1 ; (iii) r L1 /L2 ; (iv) r L1 /L3 . Let P = j pj , Clearly, P = L1 + L2 + L3 =L01 + L02 + L03 . Summing (i) (ii), getL01L2 + L3P (` + 1)=.2r2r407(1)fiFeldman & TamirLPT, L2 , L3 `, thus P 3` + 1. Summing (iii) (iv), using Equation (1)getr2L12(` + 1)2(` + 1)=001+ L3P L1P (1 2r)+L02Implying,r(3` + 1)`+12r2(` + 1)1(3` + 1)(1 2r)+`+12r.3` + 1 ` + 1+2` + 222and,r3` + 2.3` + 1(2)Case 1: ` 3. case, Equation (2) implies r 1.1.Case 2: ` < 3. case requires closer analysis. Let instance LPTcreates schedule deviation s0 = (s0 , ) achieving maximal IR min ` < 3.= 2, 3, denote Hi total load jobs migrating Mi M1 , Kitotal load jobs remain Mi . flower structure, L01 H2 + H3 , thereforeH2 < K3 H3 < K2 , else would beneficial jobs composing H2 , H3join coalition. Propositions A.1 A.2, H2 , H3 , K2 , K3 least 1.Claim A.3 load ` M1 incurred exactly two jobs.Proof: Clearly, since consider case ` < 3 lightest job M1 load 1,load ` incurred two jobs. Assume contradiction ` consists singlejob. Then, exactly two jobs M1 , loads ` 1. flower structure,`-job must join coalition. W.l.o.g assume migrates M2 . migration profitableK2 < 1, contradicting Proposition A.2.Therefore, assume w.l.o.g instance achieving maximal IR min , M1assigned three jobs loads 1 + , 1 + , 1, , 0.` = 2 + + , bound Equation (2) impliesr8 + 3( + )3` + 2=.3` + 17 + 3( + )(3)Consider first case one two big jobs M1 migrate awayM1 . show coalition deviation beneficial case. W.l.o.g, assumejob length 1 + remains M1 job length 1 + migrates M2 .migration 1 + profitable K2 < 2 + . hand, migrationjobs migrating M2 M1 profitable K2 > H3 + 1 + 2 + .contradiction.Consider next case 1-job migrate away M1 . W.l.o.g,assume job length 1 + migrates M2 job length 1 + migratesM3 . order bound r according Equation (3), find lower bound ( + ).Equation (1),2rL2 + L3K2 + H2 + K3 + H3K2 + K3 + 26++=.0L11 + H2 + H333408(4)fiApproximate Strong Equilibriumthird inequality due fact ratio decreasing H2 + H3 ,known least 2 Proposition A.1. last inequality since migrationsbeneficial jobs leaving M1 , is, K2 < 2 + , K3 < 2 + .Equation (4) implies 6r < 6 + + + > 6r 6. Next, apply bound+ Equation (3) obtainr<18r 10.18r 1161implies r < 109 < 2 + 4 .case analyze yet one three jobs assigned M1 migrateaway M1 deviation. Assume w.l.o.g jobs size 1+ 1 migratingM2 job size 1 + migrating M3 . Clearly, jobs size 1 + , 1 +migrate machine currently assigned additional load1 K2 K3 least 1, Proposition A.2. Figure 8 shows schedulemigration (Figure 8(a)) migration (Figure 8(b)).M3K3H3M2K2H2M11+1+l1K3+H3M3K2+H2M2K21+3+ +M1H2H3K31+K3+1+1K2+2+H2+H 3(b)(a)Figure 8: LPT coalition achieving maximal IR min .Next, find lower bound + . Considering migration M1 M2 ,know r (3 + + )/(2 + + K2 ). Therefore + 2r + r + K2 r 32r + rK2 3 (because 0). Considering migration M2 M1 , knowr (K2 + H2 )/(H2 + H3 ). Therefore, K2 H2 (r 1) + H3 r. LPT assigns 1-jobM1 load 2 + + , load M2 time K2 + H2 .Therefore 2 + + K2 + H2 , implying H2 2 + + K2 . Also, Proposition A.1,H3 1. use bounds H2 , H3 get improved bound K2 . Specifically,K2 (2 + + K2 )(r 1) + r. implies K2 r 3r + r( + ) (2 + ( + )). Backbound + , + 2r + 3r + r( + ) (2 + ( + )) 3. Thus,+ (5r 5)/(2 r). Note (2 r) positive since Theorem 3.1, r < 5/4.Finally, apply bound + Equation (3) obtainr1 + 7r8 + 3(5r 5)/(2 r)=.7 + 3(5r 5)/(2 r)1 + 8rimplies r 12 + 46 .bound tight. Specifically,Theorem 3.5 3, exists LPT schedule IR min (s) = 12 + 46 .61+24 , consider Figure 8,r(3+)2, H2 = 2 + K2 , K3 = 1r10+56,6 6Proof: Let r =substitute =K2 =+ , H3 = 1 (the instance409= 0,fiFeldman & Tamirrounded values appears Figure 9). easy verify three jobs leaving M1improvement ratio exactly r = 12 + 46 , holds two jobs migratingM1 . Thus, instance IR min = 12 + 46 . Moreover, lower bound easilyextended > 3 adding dummy jobs machines. Thus,M31.63312.633M3M21.3671.2662.633M21.266M11.63313.633M11.36711.63311.6333.26613.26612.367(b)(a)Figure 9: LPT-originateddeviation three machines migrating jobs im61prove 2 + 4 .Theorem 4.2 Let LPT schedule three machines. holds IR max (s) 53 .Proof: Let M1 loaded machine. Recall lightest (also last) job assignedM1 1-job load 1. = 2, 3, Ki set (and also total load) jobsremain Mi , Hi,j set (and also total load) jobs migrating MiMj . = 1, let K1 , H1,2 , H1,3 above, excluding 1-job.1-job assigned M1 LPT, meaning load M2 M3 leastK1 + H1,2 + H1,3 time. Since load M2 , M3 could increase time1-job assigned, getK1 + H1,2 + H1,3 K2 + H2,1K1 + H1,2 + H1,3 K3 + H3,1 .(5)Therefore (sum two):2(K1 + H1,2 + H1,3 ) K2 + K3 + H2,1 + H3,1 .(6)Distinguish two cases:(i) 1-job remains M1 . case, L1 = K1 + H1,2 + H1,3 + 1; L2 =K2 +H2,1 ; L3 = K3 +H3,1 , coalition active L01 = K1 +H2,1 +H3,1 +1; L02 =K2 + H1,2 ; L03 = K3 + H1,3 .Since jobs H1,2 H1,3 part coalition, L02 + L03 < 2L1 . DeducingH1,2 H1,3 sides get K2 + K3 < H1,2 + H1,3 + 2K1 + 2. CombiningEquation 6, get:H1,2 + H1,3 < H2,1 + H3,1 + 2.(7)Proposition A.1, H2,1 , H3,1 , K2 , K3 least 1. Proposition 2.9,improvement ratio 1-job, equals L1 /L01 , largest among coalition.ratio bounded follows:K1 + H1,2 + H1,3 + 1K1 + H2,1 + H3,1 + 3L15=<.0L1K1 + H2,1 + H3,1 + 1K1 + H2,1 + H3,1 + 13410fiApproximate Strong Equilibriumleft inequality follows Equation 7. right one follow Proposition A.1fact K1 might empty.(ii) 1-job leaves M1 . assume w.l.o.g 1-job moves M2 .case, L1 = K1 + H1,2 + H1,3 + 1; L2 = K2 + H2,1 ; L3 = K3 + H3,1 , coalitionactive L01 = K1 + H2,1 + H3,1 ; L02 = K2 + H1,2 + 1; L03 = K3 + H1,3 .Since jobs H1,2 H1,3 part coalition, L02 + L03 < 2L1 . Deducing1, H1,2 H1,3 sides get K2 + K3 < H1,2 + H1,3 + 2K1 + 1. CombiningEquation 6, get:H1,2 + H1,3 < H2,1 + H3,1 + 1.(8)Propositions A.1 A.2, H2,1 , H3,1 , K2 , K3 least 1. K1 emptyjobs K1 improvement ratio L1 /L01 is, Proposition 2.9, largestratio among coalition. ratio bounded follows:K1 + H1,2 + H1,3 + 1K1 + H2,1 + H3,1 + 2L15=< .0L1K1 + H2,1 + H3,1K1 + H2,1 + H3,13left inequality follows Equation 8. right one follows Proposition A.1,fact K1 empty includes least one job load least 1.K1 empty, show below, maximal improvement ratio less 3/2.bound separately improvement ratio H1,2 , H1,3 , Hi,1 (i {1, 2}). Denoteri,j IR jobs moving Mi Mj . addition Equations 5 8,Propositions A.1 A.2, also use Proposition 2.8. Specifically, H2,1 < H1,2 + 1H3,1 < H1,3 . Finally, bear mind K1 = .r1,2 =H1,2 + H1,3 + 1K2 + H2,1 + 1K2 + H2,1 + 13L1=<< .0L2K2 + H1,2 + 1K2 + H1,2 + 1K2 + H2,12r1,3 =H1,2 + H1,3 + 1K3 + H3,1 + 1K3 + H3,1 + 13L1=<< .0L3K3 + H1,3K3 + H1,3K3 + H3,12ri,1 =Ki + Hi,1H1,2 + H1,3H2,1 + H3,1 + 1Li3=<<< .0L1H2,1 + H3,1H2,1 + H3,1H2,1 + H3,12Appendix B. List SchedulingList Scheduling (LS) greedy scheduling algorithms jobs assignedmachines arbitrary order, similar LPT, job assigned least loaded1machine time assignment. LS known provide (2)-approximationminimum makespan (Graham, 1966). LS depart qualitativelyLPT respect makespan approximation (i.e., provide constant approximationoptimal makespan), qualitatively different respect game theoreticproperties. First, LS necessarily produce NE. Moreover, next show, LSperforms poorly respect measures introduced paper.improvement ratio job bounded even coalition consists singlejob. Consider example instance 2 machines jobs lengths 1, 1, X (in411fiFeldman & Tamirorder) X > 1. LS produce schedule loads 1, 1 + X. job length1 scheduled long job migrate join short job. improvementratio 1 + X/2 arbitrarily large.damage ratio deviation LS schedule bounded either. Considerinstance three machines jobs lengths {1 2, 1 , 1, 2, X, 2, 3}. easyverify resulting LS-configuration, exists coalition joblength X migrates. Since X arbitrarily large, damage ratio jobmachine X migrates arbitrarily large. note damage ratio causeddeviation single job 2. see this, consider LS configurationassume job j length pj migrates M1 M2 . Denote Bj , Aj totalload jobs M1 assigned j respectively. Aj = 0 (j last)beneficial j migrate (Bj < L2 , else j assignedM2 ). Else, first job j assigned M1 Bj + pj less loadtime M2 . Therefore L2 Bj + pj , particular pj L2 . damageratio (L1 + pj )/L1 2. analysis tight exemplified instance= 2, = {1, 1, X}.ReferencesA. Fiat, H. Kaplan, M. L., & Olonetsky., S. (2007). Strong price anarchy machineload balancing. International Colloquium Automata, Languages Programming(ICALP).Albers, S. (2009). value coordination network design. SIAM J. Comput., 38(6),22732302.Albers, S., Elits, S., Even-Dar, E., Mansour, Y., & Roditty, L. (2006). Nash EquilibriaNetwork Creation Game. Annual ACM-SIAM Symposium Discrete Algorithms(SODA).Andelman, N., Feldman, M., & Mansour, Y. (2007). Strong Price Anarchy. SODA07.Anshelevich, E., Dasgupta, A., Kleinberg, J. M., Tardos, E., Wexler, T., & Roughgarden, T.(2004). price stability network design fair cost allocation.. FOCS,pp. 295304.Anshelevich, E., Dasgupta, A., Tardos, E., Wexler, T., & Roughgarden, T. (2003). Nearoptimal network design selfish agents. ACM Symposium Theory Computing (STOC).Aumann, R. (1959). Acceptable Points General Cooperative n-Person Games. Contributions Theory Games, Vol. 4.Awerbuch, B., Azar, Y., Richter, Y., & Tsur, D. (2003). Tradeoffs Worst-Case Equilibria.1st International Workshop Approximation Online Algorithms.Bernheim, D. B., Peleg, B., & Whinston, M. D. (1987). Coalition-proof nash equilibria:concepts. Journal Economic Theory, 42, 112.Chen, B. (2009). Equilibria load balancing games. Acta Mathematica Applicata Sinica,appear.412fiApproximate Strong EquilibriumChristodoulou, G., Koutsoupias, E., & Nanavati, A. (2004). Coordination mechanisms. J.Daz, J. Karhumaki, A. Lepisto, D. Sannella (Eds.), Automata, LanguagesPro- gramming, Volume 3142 Lecture Notes Computer Science. Berlin: Springer,pp. 345357.Christodoulou, G., & Koutsoupias, E. (2005). Price Anarchy StabilityCorrelated Equilibria Linear Congestion Games. Annual European SymposiumAlgorithms (ESA).Czumaj, A., & Vocking, B. (2002). Tight bounds worst-case equilibria. ACM-SIAMSymposium Discrete Algorithms (SODA), pp. 413420.Epstein, A., Feldman, M., & Mansour, Y. (2007). Strong Equilibrium Cost SharingConnection Games. ACM Conference Electronic Commerce (ACMEC).Fabrikant, A., Luthra, A., Maneva, E., Papadimitriou, C., & Shenker, S. (2003).network creation game. ACM Symposium Principles Distriubted Computing(PODC).Finn, G., & Horowitz, E. (1979). linear time approximation algorithm multiprocessorscheduling. BIT Numerical Mathematics, 19 (3), 312320.Fotakis, D., S. Kontogiannis, M. M., & Spiraklis, P. (2002). Structure Complexity Nash Equilibria Selfish Routing Game. International ColloquiumAutomata, Languages Programming (ICALP), pp. 510519.Graham, R. (1966). Bounds certain multiprocessing anomalies. Bell Systems TechnicalJournal, 45, 15631581.Graham, R. (1969). Bounds multiprocessing timing anomalies. SIAM J. Appl. Math.,17, 263269.Halpern, J. Y., & Teague, V. (2004). Rational secret sharing multiparty computation.ACM Symposium Theory Computing (STOC), pp. 623632.Holzman, R., & Law-Yone, N. (1997). Strong equilibrium congestion games. GamesEconomic Behavior, 21, 85101.Holzman, R., & Law-Yone, N. (2003). Network structure strong equilibrium routeselection games. Mathematical Social Sciences, 46, 193205.Koutsoupias, E., & Papadimitriou, C. H. (1999). Worst-case equilibria.. SymposiumTheoretical Aspects Computer Science (STACS), pp. 404413.Lamport, L., Shostak, R., & Pease, M. (1982). byzantine generals problem. ACMTrans. Prog. Lang. Sys., 4, 382401.Leonardi, S., & Sankowski, P. (2007). Network formation games local coalitions.ACM Symposium Principles Distriubted Computing (PODC).Milchtaich, I. (1998). Crowding games sequentially solvable. International JournalGame Theory, 27, 501509.Papadimitriou, C. H. (2001). Algorithms, games, internet. ACM SymposiumTheory Computing (STOC), pp. 749753.413fiFeldman & TamirProcaccia, A. D., & Rosenschein, J. S. (2006). communication complexity coalitionformation among autonomous agents. Int. Conference Autonomous AgentsMultiagent Systems (AAMAS), pp. 505512.Roughgarden, T., & Tardos, E. (2002). bad selfish routing?. Journal ACM,49 (2), 236 259.Rozenfeld, O., & Tennenholtz, M. (2006). Strong correlated strong equilibria monotone congestion games. Workshop Internet Network Economics (WINE).Schuurman, P., & Vredeveld, T. (2007). Performance guarantees local searchmultiproces- sor scheduling. INFORMS Journal Computing, 19(1), 52 63.414fiJournal Artificial Intelligence Research 36 (2009) 129163Submitted 04/09; published 10/09Content Modeling Using Latent PermutationsHarr ChenS.R.K. BranavanRegina BarzilayDavid R. Kargerharr@csail.mit.edubranavan@csail.mit.eduregina@csail.mit.edukarger@csail.mit.eduComputer Science Artificial Intelligence LaboratoryMassachusetts Institute Technology32 Vassar Street, Cambridge, Massachusetts 02139 USAAbstractpresent novel Bayesian topic model learning discourse-level document structure. model leverages insights discourse theory constrain latent topic assignments way reflects underlying organization document topics. proposeglobal model topic selection ordering biased similar acrosscollection related documents. show space orderings effectively represented using distribution permutations called Generalized Mallows Model.apply method three complementary discourse-level tasks: cross-document alignment,document segmentation, information ordering. experiments show incorporating permutation-based model applications yields substantial improvementsperformance previously proposed methods.1. Introductioncentral problem discourse analysis modeling content structure document.structure encompasses topics addressed order topicsappear across documents single domain. Modeling content structure particularlygermane domains exhibit recurrent patterns content organization, newsencyclopedia articles. models aim induce, example, articlescities typically contain information History, Economy, Transportation,descriptions History usually precede Transportation.Previous work (Barzilay & Lee, 2004; Elsner, Austerweil, & Charniak, 2007) demonstrated content models learned raw unannotated text, usefulvariety text processing tasks summarization information ordering. However, expressive power approaches limited: taking Markovian viewcontent structure, model local constraints topic organization. shortcoming substantial since many discourse constraints described literature globalnature (Graesser, Gernsbacher, & Goldman, 2003; Schiffrin, Tannen, & Hamilton, 2001).paper, introduce model content structure explicitly represents twoimportant global constraints topic selection.1 first constraint posits document follows progression coherent, nonrecurring topics (Halliday & Hasan, 1976).Following example above, constraint captures notion single topic,1. Throughout paper, use topic refer interchangeably discourse unitlanguage model views topic.c2009AI Access Foundation. rights reserved.fiChen, Branavan, Barzilay, & KargerHistory, expressed contiguous block within document, rather spreaddisconnected sections. second constraint states documents domaintend present similar topics similar orders (Bartlett, 1932; Wray, 2002). constraintguides toward selecting sequences similar topic ordering, placing History Transportation. constraints universal across genres humandiscourse, applicable many important domains, ranging newspaper textproduct reviews.2present latent topic model related documents encodes discourseconstraints positing single distribution entirety documents content ordering. Specifically, represent content structure permutation topics.naturally enforces first constraint since permutation allow topic repetition.learn distribution permutations, employ Generalized Mallows Model(GMM). model concentrates probability mass permutations close canonicalpermutation. Permutations drawn distribution likely similar, conforming second constraint. major benefit GMM compact parameterizationusing set real-valued dispersion values. dispersion parameters allow modellearn strongly bias documents topic ordering toward canonical permutation. Furthermore, number parameters grows linearly number topics,thus sidestepping tractability problems typically associated large discrete spacepermutations.position GMM within larger hierarchical Bayesian model explainsset related documents generated. document, model posits topicordering drawn GMM, set topic frequencies drawn multinomial distribution. Together, draws specify documents entire topic structure,form topic assignments textual unit. traditional topic models, wordsdrawn language models indexed topic. estimate model posterior,perform Gibbs sampling topic structures GMM dispersion parametersanalytically integrating remaining hidden variables.apply model three complex document-level tasks. First, alignmenttask, aim discover paragraphs across different documents share topic.experiments, permutation-based model outperforms Hidden Topic MarkovModel (Gruber, Rosen-Zvi, & Weiss, 2007) wide margin gap averaged 28% percentage points F-score. Second, consider segmentation task, goalpartition document sequence topically coherent segments. model yieldsaverage Pk measure 0.231, 7.9% percentage point improvement competitiveBayesian segmentation method take global constraints account (Eisenstein & Barzilay, 2008). Third, apply model ordering task, is, sequencingheld set textual units coherent document. previous two applications, difference model state-of-the-art baseline substantial:model achieves average Kendalls 0.602, compared value 0.267HMM-based content model (Barzilay & Lee, 2004).success permutation-based model three complementary tasks demonstrates flexibility effectiveness, attests versatility general document2. example domain first constraint violated dialogue. Texts domains followstack structure, allowing topics recur throughout conversation (Grosz & Sidner, 1986).130fiContent Modeling Using Latent Permutationsstructure induced model. find encoding global ordering constraintstopic models makes suitable discourse-level analysis, contrast localdecision approaches taken previous work. Furthermore, evaluation scenarios, full model yields significantly better results simpler variants eitheruse fixed ordering order-agnostic.remainder paper proceeds follows. Section 2, describe approach relates previous work topic modeling statistical discourse processing.provide problem formulation Section 3.1 followed overview contentmodel Section 3.2. heart model distribution topic permutations,provide background Section 3.3, employing formal descriptionmodels probabilistic generative story Section 3.4. Section 4 discusses estimation models posterior distribution given example documents using collapsed Gibbssampling procedure. Techniques applying model three tasks alignment,segmentation, ordering explained Section 5. evaluate models performance tasks Section 6 concluding touching upon directionsfuture work Section 7. Code, data sets, annotations, raw outputsexperiments available http://groups.csail.mit.edu/rbg/code/mallows/.2. Related Workdescribe two areas previous work related approach. algorithmicperspective work falls broad class topic models. earlier work topicmodeling took bag words view documents, many recent approaches expandedtopic models capture structural constraints. Section 2.1, describe extensions highlight differences model. linguistic side, workrelates research modeling text structure statistical discourse processing. summarize work Section 2.2, drawing comparisons functionality supportedmodel.2.1 Topic ModelsProbabilistic topic models, originally developed context language modeling,today become popular range NLP applications, text classification document browsing. Topic models posit latent state variable controls generationword. parameters estimated using approximate inference techniquesGibbs sampling variational methods. traditional topic models Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004), documentstreated bags words, word receives separate topic assignment wordsassigned topic drawn shared language model.bag words representation sufficient applications, many casesstructure-unaware view limited. Previous research considered extensionsLDA models two orthogonal directions, covering intrasentential extrasententialconstraints.131fiChen, Branavan, Barzilay, & Karger2.1.1 Modeling Intrasentential ConstraintsOne promising direction improving topic models augment constraintstopic assignments adjoining words within sentences. example, Griffiths, Steyvers,Blei, Tenenbaum (2005) propose model jointly incorporates syntacticsemantic information unified generative framework constrains syntactic classesadjacent words. approach, generation word controlled two hiddenvariables, one specifying semantic topic specifying syntactic class.syntactic class hidden variables chained together Markov model, whereas semantictopic assignments assumed independent every word.another example intrasentential constraints, Wallach (2006) proposes wayincorporate word order information, form bigrams, LDA-style model.approach, generation word conditioned previous wordtopic current word, word topics generated perdocument topic distributions LDA. formulation models text structurelevel word transitions, opposed work Griffiths et al. (2005) structuremodeled level hidden syntactic class transitions.focus modeling high-level document structure terms semantic content.such, work complementary methods impose structure intrasententialunits; possible combine model constraints adjoining words.2.1.2 Modeling Extrasentential ConstraintsGiven intuitive connection notion topic LDA notion topicdiscourse analysis, natural assume LDA-like models useful discourselevel tasks segmentation topic classification. hypothesis motivated researchmodels topic assignment guided structural considerations (Purver, Kording,Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularlyrelationships topics adjacent textual units. Depending application,textual unit may sentence, paragraph, speaker utterance. common propertymodels bias topic assignments cohere within local segments text.Models category vary terms mechanisms used encourage local topiccoherence. instance, model Purver et al. (2006) biases topic distributionsadjacent utterances (textual units) similar. model generates utterancemixture topic language models. parameters topic mixture distribution assumed follow type Markovian transition process specifically, highprobability utterance u topic distribution previous utteranceu 1; otherwise, new topic distribution drawn u. Thus, textual units topicdistribution depends previous textual unit, controlled parameter indicatingwhether new topic distribution drawn.similar vein, Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) positsgenerative process sentence (textual unit) assigned single topic,sentences words drawn single language model. modelPurver et al., topic transitions adjacent textual units modeled Markovianfashion specifically, sentence topic sentence 1 high probability,receives new topic assignment drawn shared topic multinomial distribution.132fiContent Modeling Using Latent PermutationsHTMM model, assumption single topic per textual unit allowssections text related across documents topic. contrast, Purver et al.s modeltailored task segmentation, utterance drawn mixture topics.Thus, model capture utterances topically aligned across relateddocuments. importantly, HTMM model Purver et al. ablemake local decisions regarding topic transitions, thus difficulty respecting longrange discourse constraints topic contiguity. model instead takes global viewtopic assignments textual units explicitly generating entire documents topicordering one joint distribution. show later paper, global view yieldssignificant performance gains.recent Multi-Grain Latent Dirichlet Allocation (MGLDA) model (Titov & McDonald, 2008) also studied topic assignments level sub-document textual units.MGLDA, set local topic distributions induced sentence, dependentwindow local context around sentence. Individual words drawn eitherlocal topics document-level topics standard LDA. MGLDA representslocal context using sliding window, window frame comprises overlapping shortspans sentences. way, local topic distributions shared sentencesclose proximity.MGLDA represent complex topical dependencies models Purveret al. Gruber et al., window incorporate much wider swath localcontext two adjacent textual units. However, MGLDA unable encode longerrange constraints, contiguity ordering similarity, sentences closeproximity loosely connected series intervening window frames.contrast, work specifically oriented toward long-range constraints, necessitatingwhole-document notion topic assignment.2.2 Modeling Ordering Constraints Statistical Discourse Analysisglobal constraints encoded model closely related research discourseinformation ordering applications text summarization generation (Barzilay,Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004;Elsner et al., 2007). emphasis body work learning ordering constraintsdata, goal reordering new text domain. methods buildassumption recurring patterns topic ordering discovered analyzingpatterns word distribution. key distinction prior methods approachexisting ordering models largely driven local constraints limited abilitycapture global structure. Below, describe two main classes probabilistic orderingmodels studied discourse processing.2.2.1 Discriminative ModelsDiscriminative approaches aim directly predict ordering given set sentences.Modeling ordering sentences simultaneously leads complex structure predictionproblem. practice, however, computationally tractable two-step approach taken:first, probabilistic models used estimate pairwise sentence ordering preferences; next,local decisions combined produce consistent global ordering (Lapata, 2003;133fiChen, Branavan, Barzilay, & KargerAlthaus, Karamanis, & Koller, 2004). Training data pairwise models constructedconsidering pairs sentences document, supervision labels basedactually ordered. Prior work demonstrated wide range featuresuseful classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji &Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006). instance, Lapata (2003)demonstrated lexical features, verb pairs input sentences, serveproxy plausible sequences actions, thus effective predictors well-formedorderings. second stage, local decisions integrated global ordermaximizes number consistent pairwise classifications. Since findingordering NP-hard (Cohen, Schapire, & Singer, 1999), various approximations usedpractice (Lapata, 2003; Althaus et al., 2004).two-step discriminative approaches effectively leverage informationlocal transitions, provide means representing global constraints.recent work, Barzilay Lapata (2008) demonstrated certain global properties captured discriminative framework using reranking mechanism.set-up, system learns identify best global ordering given set n possiblecandidate orderings. accuracy ranking approach greatly depends qualityselected candidates. Identifying candidates challenging task given largesearch space possible alternatives.approach presented work differs existing discriminative models twoways. First, model represents distribution possible global orderings. Thus,use sampling mechanisms consider whole space rather limitedsubset candidates ranking models. second difference arisesgenerative nature model. Rather focusing ordering task, order-awaremodel effectively captures layer hidden variables explain underlying structuredocument content. Thus, effectively applied wider variety applications,including sentence ordering already observed, appropriately adjustingobserved hidden components model.2.2.2 Generative Modelswork closer technique generative models treat topics hidden variables.One instance work Hidden Markov Model (HMM)-based content model (Barzilay & Lee, 2004). model, states correspond topics state transitions representordering preferences; hidden states emission distribution language modelwords. Thus, similar approach, models implicitly represent patternslevel topical structure. HMM used ranking framework selectordering highest probability.recent work, Elsner et al. (2007) developed search procedure based simulated annealing finds high likelihood ordering. contrast ranking-based approaches, search procedure cover entire ordering space. hand,show Section 5.3, define ordering objective maximizedefficiently possible orderings prediction model parameterslearned. Specifically, bag p paragraphs, O(pK) calculations paragraphprobabilities necessary, K number topics.134fiContent Modeling Using Latent PermutationsAnother distinction proposed model prior work way globalordering constraints encoded. Markovian model, possible induce globalconstraints introducing additional local constraints. instance, topic contiguityenforced selecting appropriate model topology (e.g., augmenting hidden statesrecord previously visited states). However, global constraints, similarityoverall ordering across documents, much challenging represent. explicitlymodeling topic permutation distribution, easily capture kind globalconstraint, ultimately resulting accurate topic models orderings. showlater paper, model substantially outperforms approach Barzilay Leeinformation ordering task applied HMM-based content model.3. Modelsection, describe problem formulation proposed model.3.1 Problem Formulationcontent modeling problem formalized follows. take input corpus{d1 , . . . dD } related documents, specification number topics K.3document comprised ordered sequence Nd paragraphs (pd,1 , . . . , pd,Nd ).output, predict single topic assignment zd,p {1, . . . , K} paragraph p.4z values reflect underlying content organization documentrelated content discussed within document, across separate documents,receive z value.formulation shares similarity standard LDA setup commonset topics assigned across collection documents. difference LDAwords topic assignment conditionally independent, following bag words viewdocuments, whereas constraints topics assigned let us connect worddistributional patterns document-level topic structure.3.2 Model Overviewpropose generative Bayesian model explains corpus documentsproduced set hidden variables. high level, model first selectsfrequently topic expressed document, topics ordered.topics determine selection words paragraph. Notation usedsubsequent sections summarized Figure 1.document Nd paragraphs, separately generate bag topics tdtopic ordering . unordered bag topics td , contains Nd elements, expressesmany paragraphs document assigned K topics. Equivalently,td viewed vector occurrence counts topic, zero countstopics appear all. Variable td constructed taking Nd samples3. nonparametric extension model would also learn K.4. well structured documents, paragraphs tend internally topically consistent (Halliday & Hasan,1976), predicting one topic per paragraph sufficient. However, note approachapplied modifications levels textual granularity sentences.135fiChen, Branavan, Barzilay, & Kargerparameters distributiontopic countsparameters distributiontopic orderingsvector topic countsvvector inversion countstopic orderingzparagraph topic assignmentlanguage model parameterstopicDirichlet(0 )j = 1 . . . K 1:j GMM0 (0 , 0 )k = 1 . . . K:k Dirichlet(0 )tdvd=zd =w document wordsdocumentMultinomial()GMM()Compute-(vd )Compute-z(td , )paragraph pword w pw Multinomial(zd,p )K number topicsnumber documents corpusNd number paragraphsdocumentNp number words paragraph pAlgorithm: Compute-Input: Inversion count vector vAlgorithm: Compute-zInput: Topic counts t, permutationCreate empty list[1] Kj = K 1 1= K 1 v[j][i + 1] [i][v[j]] jCreate empty list zend 1k = K 1= 1 t[[k]]z[end] [k]end end + 1Output: PermutationOutput: Paragraph topic vector zFigure 1: plate diagram generative process model, along tablenotation reference purposes. Shaded circles figure denote observedvariables, squares denote hyperparameters. dotted arrows indicateconstructed deterministically v according algorithm Compute-,z constructed deterministically according Compute-z.136fiContent Modeling Using Latent Permutationsdistribution topics , multinomial representing probability topicexpressed. Sharing documents captures notion certain topicslikely across documents corpus.topic ordering variable permutation numbers 1 Kdefines order topics appear document. draw GeneralizedMallows Model, distribution permutations explain Section 3.3.see, particular distribution biases permutation selection close singlecentroid, reflecting discourse constraint preferring similar topic structures acrossdocuments.Together, documents bag topics td ordering determine topic assignmentzd,p paragraphs. example, corpus K = 4, seven-paragraphdocument td = {1, 1, 1, 1, 2, 4, 4} = (2, 4, 3, 1) would induce topic sequencezd = (2, 4, 4, 1, 1, 1, 1). induced topic sequence zd never assign topictwo unconnected portions document, thus satisfying constraint topic contiguity.assume topic k associated language model k . wordsparagraph assigned topic k drawn topics language model k .portion similar standard LDA topic relates language model.However, unlike LDA, model enforces topic coherence entire paragraph ratherviewing paragraph mixture topics.turning formal discussion generative process, first providebackground permutation model topic ordering.3.3 Generalized Mallows Model Permutationscentral challenge approach presented modeling distribution possible topic orderings. purpose use Generalized Mallows Model (GMM) (Fligner& Verducci, 1986; Lebanon & Lafferty, 2002; Meila, Phadnis, Patterson, & Bilmes, 2007;Klementiev, Roth, & Small, 2008), exhibits two appealing properties contexttask. First, model concentrates probability mass canonical orderingsmall perturbations (permutations) ordering. characteristic matchesconstraint documents domain exhibit structural similarity. Second,parameter set scales linearly number elements ordered, makingsufficiently constrained tractable inference.first describe standard Mallows Model orderings (Mallows, 1957).Mallows Model takes two parameters, canonical ordering dispersion parameter .sets probability ordering proportional ed(,) ,d(, ) represents distance metric orderings . Frequently, metricKendall distance, minimum number swaps adjacent elements neededtransform ordering canonical ordering . Thus, orderings closecanonical ordering high probability, many elementsmoved less probability mass.Generalized Mallows Model, first introduced Fligner Verducci (1986), refinesstandard Mallows Model adding additional set dispersion parameters.parameters break apart distance d(, ) orderings set independentcomponents. component separately vary sensitivity perturbation.137fiChen, Branavan, Barzilay, & Kargertease apart distance function components, GMM distribution considersinversions required transform canonical ordering observed ordering. firstdiscuss inversions parameterized GMM, turn distributionsdefinition characteristics.3.3.1 Inversion Representation PermutationsTypically, permutations represented directly ordered sequence elementsexample, (3, 1, 2) represents permuting initial order placing third elementfirst, followed first element, second. GMM utilizes alternativepermutation representation defined vector (v1 , . . . , vK1 ) inversion countsrespect identity permutation (1, . . . , K). Term vj counts number timesvalue greater j appears j permutation. Note jth inversioncount vj take integer values 0 K j inclusive. Thus inversion countvector K 1 elements, vK always zero. instance, given standardform permutation (3, 1, 5, 6, 2, 4), v2 = 3 3, 5, 6 appear 2, v3 = 0numbers appear it; entire inversion count vector would (1, 3, 0, 2, 0).Likewise, previous example permutation (2, 4, 3, 1) maps inversion counts (3, 0, 1).sum components entire inversion count vector simply orderingsKendall distance canonical ordering.significant appeal inversion representation every valid, distinct vectorinversion counts corresponds distinct permutation vice versa. see this,note permutation straightforwardly compute inversion counts.Conversely, given sequence inversion counts, construct unique correspondingpermutation. insert items permutation, working backwards item K.Assume already placed items j + 1 K proper order. insertitem j, note exactly vj items j + 1 K must precede it, meaningmust inserted position vj current order (see Compute- algorithmFigure 1). Since one place j inserted fulfills inversioncounts, induction shows exactly one permutation constructed satisfygiven inversion counts.model, take canonical topic ordering always identity ordering(1, . . . , K). topic numbers task completely symmetric linkedextrinsic meaning, fixing global ordering specific arbitrary valuesacrifice representational power. general case GMM, canonical orderingparameter distribution.3.3.2 Probability Mass FunctionGMM assigns probability mass particular order based order permuted canonical ordering. precisely, associates distance everypermutation, canonical ordering distance zero permutations manyinversions respect canonical ordering larger distance. distance assignment based K 1 real-valued dispersion parameters P(1 , . . . , K1 ). distancepermutation inversion counts v defined j j vj . GMMs probability138fiContent Modeling Using Latent Permutationsmass function exponential distance:Pe j j vjGMM(v; ) =()=K1j=1() =Qjej vj,j (j )(1)j (j ) normalization factor value:j (j ) =1 e(Kj+1)j.1 ej(2)Setting j equal single value recovers standard Mallows Model Kendalldistance function. factorization GMM independent probabilities perinversion count makes distribution particularly easy apply; use GMMj referjth multiplicand probability mass function, marginal distributionvj :GMMj (vj ; j ) =ej vj.j (j )(3)Due exponential form distribution, requiring j > 0 constrains GMMassign highest probability mass vj zero, i.e., distributional modecanonical identity permutation. higher value j assigns probability mass vjclose zero, biasing j fewer inversions.3.3.3 Conjugate Priormajor benefit GMM membership exponential family distributions;means particularly amenable Bayesian representation, admitsnatural independent conjugate prior parameter j (Fligner & Verducci, 1990):GMM0 (j | vj,0 , 0 ) e(j vj,0 log j (j ))0 .(4)prior distribution takes two parameters 0 vj,0 . Intuitively, prior states0 previous trials, total number inversions observed 0 vj,0 . distributioneasily updated observed vj derive posterior distribution.vj different range, inconvenient set prior hyperparametersvj,0 directly. work, instead assign common prior value parameter j ,denote 0 . set vj,0 maximum likelihood estimatej 0 . differentiating likelihood GMM respect j , straightforwardverify works setting:vj,0 =e0K j+11(Kj+1).0 11 e139(5)fiChen, Branavan, Barzilay, & Karger3.4 Formal Generative Processfully specify details content model, whose plate diagram appearsFigure 1. observe corpus documents, document orderedsequence Nd paragraphs paragraph represented bag words. numbertopics K assumed pre-specified. model induces set hidden variablesprobabilistically explain words corpus produced. final desiredoutput posterior distributions paragraphs hidden topic assignment variables.following, variables subscripted 0 fixed prior hyperparameters.1. topic k, draw language model k Dirichlet(0 ). LDA,topic-specific word distributions.2. Draw topic distribution Dirichlet(0 ), expresses likely topicappear regardless position.3. Draw topic ordering distribution parameters j GMM0 (0 , 0 ) j = 1K 1. parameters control rapidly probability mass decaysinversions topic. separate j every topic allows us learntopics likely reordered others.4. document Nd paragraphs:(a) Draw bag topics td sampling Nd times Multinomial().(b) Draw topic ordering , sampling vector inversion counts vd GMM(),applying algorithm Compute- Figure 1 vd .(c) Compute vector topic assignments zd document ds paragraphssorting td according , algorithm Compute-z Figure 1.5(d) paragraph p document d:i. Sample word w p according language model p: wMultinomial(zd,p ).3.5 Properties Modelsection describe rationale behind using GMM represent orderingcomponent content model.Representational Power GMM concentrates probability mass around one centroid permutation, reflecting preferred bias toward document structures similar topic orderings. Furthermore, parameterization GMM using vectordispersion parameters allows flexibility strongly model biases towardsingle ordering one extreme ( = ) one ordering nonzero probability, ( = 0) orderings equally likely. comprised5. Multiple permutations contribute probability single documents topic assignments zd ,topics appear td . result, current formulation biased towardassignments fewer topics per document. practice, find negatively impact modelperformance.140fiContent Modeling Using Latent Permutationsindependent dispersion parameters (1 , . . . , K1 ), distribution assign different penalties displacing different topics. example, may learn middlesections (in case Cities, sections Economy Culture) likelyvary position across documents early sections (such IntroductionHistory).Computational Benefits parameterization GMM using vector dispersion parameters compact tractable. Since number parameters growslinearly number topics, model efficiently handle longer documentsgreater diversity content.Another computational advantage model seamless integration largerBayesian model. Due membership exponential family existenceconjugate prior, inference become significantly complexGMM used hierarchical context. case, entire document generativemodel also accounts topic frequency words within topic.One final beneficial effect GMM breaks symmetry topic assignments fixing distribution centroid. Specifically, topic assignmentsinvariant relabeling, probability underlying permutation wouldchange. contrast, many topic models assign probability relabelingtopic assignments. model thus sidesteps problem topic identifiability, issue model may multiple maxima likelihood dueunderlying symmetry hidden variables. Non-identifiable modelsstandard LDA may cause sampling procedures jump maxima producedraws difficult aggregate across runs.Finally, show Section 6 benefits GMM extend theoretical empirical: representing permutations using GMM almost always leadssuperior performance compared alternative approaches.4. Inferencevariables aim infer paragraph topic assignments z, determined bag topics ordering document. Thus, goal estimatejoint marginal distributions given document text integratingremaining hidden parameters:P (t, , | w).(6)accomplish inference task Gibbs sampling (Geman & Geman, 1984; Bishop,2006). Gibbs sampler builds Markov chain hidden variable state space whosestationary distribution actual posterior joint distribution. new sampledrawn distribution single variable conditioned previous samplesvariables. collapse sampler integrating hiddenvariables model, effect reducing state space Markov chain. Collapsedsampling previously demonstrated effective LDA variants (Griffiths& Steyvers, 2004; Porteous, Newman, Ihler, Asuncion, Smyth, & Welling, 2008; Titov &141fiChen, Branavan, Barzilay, & KargerP (td,i = | . . .) P (td,i = | t(d,i) , 0 ) P (wd | td , , wd , zd , 0 )N (t(d,i) , t) + 0P (wd | z, wd , 0 ),|t(d,i) | + K0P (vd,j = v | . . .) P (vd,j = v | j ) P (wd | td , , wd , zd , 0 )= GMMj (v; j ) P (wd | z, wd , 0 ),Pvd,j + vj,0 0P (j | . . .) = GMM0 j ;, N + 0 ,N + 0Figure 2: collapsed Gibbs sampling inference procedure estimating modelsposterior distribution. plate diagram, variable resampledshown double circle Markov blanket highlighted black;variables, impact variable resampled, grayed out.Variables , shown dotted circles, never explicitly dependedre-estimated, marginalized sampler. diagramaccompanied conditional resampling distribution respective variable.142fiContent Modeling Using Latent PermutationsMcDonald, 2008). typically preferred explicit Gibbs sampling hiddenvariables smaller search space generally shorter mixing time.sampler analytically integrates three sets hidden variables: bagstopics t, orderings , permutation inversion parameters . burn-in period,treat last samples draw posterior. samplesmarginalized variables necessary, estimated based topicassignments show Section 5.3. Figure 2 summarizes Gibbs sampling stepsinference procedure.Document Probability preliminary step, consider calculate probabilitysingle documents words wd given documents paragraph topic assignments zdremaining documents topic assignments. Note probability decomposable product probabilities individual paragraphs paragraphsdifferent topics conditionally independent word probabilities. Let wd zd indicate words topic assignments documents d, W vocabularysize. probability words then:K ZP (wd | z, wd , 0 ) =P (wd | zd , k ) P (k | z, wd , 0 ) dk=k=1 kKDCM({wd,i : zd,i = k} | {wd,i : zd,i = k}, 0 ),(7)k=1DCM() refers Dirichlet compound multinomial distribution, resultintegrating multinomial parameters Dirichlet prior (Bernardo & Smith, 2000).Dirichlet prior parameters = (1 , . . . , W ), DCM assigns followingprobability series observations x = {x1 , . . . , xn }:PW( j j )(N (x, i) + )PDCM(x; ) = Q,(8)(|x| + j j )j (j )i=1N (x, i) refers number times word appears x. Here, () Gammafunction, generalization factorial real numbers. algebra showsDCMs posterior probability density function conditioned series observations ={y1 , . . . , yn } computed updating counts often word appearsy:DCM(x | y, ) = DCM(x; 1 + N (y, 1), . . . , W + N (y, W )).(9)Equations 7 9 used compute conditional distributions hiddenvariables. turn individual random variable resampled.Bag Topics First consider resample td,i , ith topic draw documentconditioned parameters fixed (note topic ithparagraph, reorder topics using , generated separately):P (td,i = | . . .) P (td,i = | t(d,i) , 0 ) P (wd | td , , wd , zd , 0 )N (t(d,i) , t) + 0P (wd | z, wd , 0 ),|t(d,i) | + K0143(10)fiChen, Branavan, Barzilay, & Kargertd updated reflect td,i = t, zd deterministically computed last stepusing Compute-z Figure 1 inputs td . first step reflects applicationBayes rule factor term wd ; drop superfluous termsconditioning. second step, former term arises DCM, updatingparameters 0 observations t(d,i) Equation 9 dropping constants.latter document probability term computed using Equation 7. new td,i selectedsampling probability computed possible topic assignments.Ordering parameterization permutation series inversion values vd,jreveals natural way decompose search space Gibbs sampling. documentd, resample vd,j j = 1 K 1 independently successively accordingconditional distribution:P (vd,j = v | . . .) P (vd,j = v | j ) P (wd | td , , wd , zd , 0 )= GMMj (v; j ) P (wd | z, wd , 0 ),(11)updated reflect vd,j = v, zd computed deterministically accordingtd . first term refers Equation 3; second computed using Equation 7.probability computed every possible value v, ranges 0 K j,term vd,j sampled according resulting probabilities.GMM Parameters j = 1 K 1, resample j posterior distribution:Pvd,j + vj,0 0P (j | . . .) = GMM0 j ;, N + 0 ,(12)N + 0GMM0 evaluated according Equation 4. normalization constantdistribution unknown, meaning cannot directly compute invert cumulative distribution function sample distribution. However, distributionunivariate unimodal, expect MCMC technique slicesampling (Neal, 2003) perform well. practice, Matlabs built-in slice samplerprovides robust draw distribution.6Computational Issues inference, directly computing document probabilitiesbasis Equation 7 results many redundant calculations slow runtimeiteration considerably. improve computational performance proposedinference procedure, apply memoization techniques sampling. Withinsingle iteration, document, Gibbs sampler requires computing documentsprobability given topic assignments (Equation 7) many times, computationfrequently conditions slight variations topic assignments. nave approachwould compute probability every paragraph time document probabilitydesired, performing redundant calculations topic assignment sequences sharedsubsequences repeatedly considered.Instead, use lazy evaluation build three-dimensional cache, indexed tuple(i, j, k), follows. time document probability requested, broken independent subspans paragraphs, subspan takes one contiguous topic assignment. possible due way Equation 7 factorizes independent per-topic6. particular, use slicesample function Matlab Statistics Toolbox.144fiContent Modeling Using Latent Permutationsmultiplicands. subspan starting paragraph i, ending paragraph j, assigned topic k, cache consulted using key (i, j, k). example, topic assignmentszd = (2, 4, 4, 1, 1, 1, 1) would result cache lookups (1, 1, 2), (2, 3, 4), (4, 7, 1).cached value unavailable, correct probability computed using Equation 7result stored cache location (i, j, k). Moreover, also record values everyintermediate cache location (i, l, k) l = j 1, values computedsubproblems evaluating Equation 7 (i, j, k). cache reset proceedingnext document since conditioning changes documents. document,caching guarantees O(Nd2 K) paragraph probability calculations.practice, individual Gibbs steps small, bound loosecaching mechanism reduces computation time several orders magnitude.also maintain caches word-topic paragraph-topic assignment frequencies,allowing us rapidly compute counts used equations 7 10. form cachingused Griffiths Steyvers (2004).5. Applicationssection, describe model applied three challenging discourselevel tasks: aligning paragraphs similar topical content documents, segmentingdocument topically cohesive sections, ordering new unseen paragraphscoherent document. particular, show posterior samples producedinference procedure Section 4 used derive solution tasks.5.1 Alignmentalignment task wish find paragraphs document topicallyrelate paragraphs documents. Essentially, cross-document clusteringtask alignment assigns paragraph document one K topically relatedgroupings. instance, given set cell phone reviews, one group may represent textfragments discuss Price, another group consists fragments Reception.model readily employed task: view topic assignmentparagraph z cluster label. example, two documents d1 d2topic assignments zd1 = (2, 4, 4, 1, 1, 1, 1) zd2 = (4, 4, 3, 3, 2, 2, 2), paragraph 1 d1grouped together paragraphs 5 7 d2 , paragraphs 2 3 d1 12 d2 . remaining paragraphs assigned topics 1 3 form separateper-document clusters.Previously developed methods cross-document alignment primarily drivensimilarity functions quantify lexical overlap textual units (Barzilay & Elhadad, 2003; Nelken & Shieber, 2006). methods explicitly model documentstructure, specify global constraints guide search optimalalignment. Pairs textual units considered isolation making alignment decisions. contrast, approach allows us take advantage global structure sharedlanguage models across related textual units without requiring manual specificationmatching constraints.145fiChen, Branavan, Barzilay, & Karger5.2 SegmentationSegmentation well-studied discourse task goal divide documenttopically cohesive contiguous sections. Previous approaches typically relied lexicalcohesion is, similarity word choices within document subspan guidechoice segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, &Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay,2008). model relies notion determining language models topics,connecting topics across documents constraining topics appear allowbetter learn words indicative topic cohesion.output samples models inference procedure map straightforwardlysegmentations contiguous spans paragraphs assigned topic number taken one segment. example, seven-paragraph document topicassignments zd = (2, 4, 4, 1, 1, 1, 1) would segmented three sections, comprisedparagraph 1, paragraphs 2 3, paragraphs 4 7. Note segmentation ignores specific values used topic assignments, heeds paragraphboundaries topic assignments change.5.3 Orderingthird application model problem creating structured documentscollections unordered text segments. text ordering task important stepbroader NLP tasks text summarization generation. task, assumeprovided well structured documents single domain training examples;trained, model used induce ordering previously unseen collectionsparagraphs domain.training, model learns canonical ordering topics documents withincollection, via language models associated topic. GMMconcentrates probability mass around canonical (1, . . . , K) topic ordering, expecthighly probable words language models lower -numbered topics tend appear earlydocument, whereas highly probable words language models higher -numberedtopics tend appear late document. Thus, structure new documents accordingintuition paragraphs words tied low topic numbers placed earlierparagraphs words relating high topic numbers.Formally, given unseen document comprised unordered set paragraphs{p1 , . . . , pn }, order paragraphs according following procedure. First, findprobable topic assignment zi independently paragraph pi , accordingparameters learned training phase:zi = arg max P (zi = k | pi , , )k= arg max P (pi | zi = k, k )P (zi = k | ).(13)kSecond, sort paragraphs topic assignment zi ascending order since (1 . . . K)GMMs canonical ordering, yields likely ordering conditioned singleestimated topic assignment paragraph. Due possible ties topic assignments,146fiContent Modeling Using Latent Permutationsresulting document may partial ordering; full ordering required, tiesbroken arbitrarily.key advantage proposed approach closed-form computationallyefficient. Though training phase requires running inference procedure Section 4,model parameters learned, predicting ordering new set p paragraphsrequires computing pK probability scores. contrast, previous approachesable rank small subset possible document reorderings (Barzilay &Lapata, 2008), performed search procedure space orderings findoptimum (Elsner et al., 2007).7objective function Equation 13 depends posterior estimates giventraining documents. Since collapsed Gibbs sampler integrates two hiddenvariables, need back values known posterior samplesz. easily done computing point estimate distribution basedword-topic topic-document assignment frequencies, respectively, done GriffithsSteyvers (2004). probability mass kw word w language model topic kgiven by:N (k, w) + 0kw =,(14)N (k) + W 0N (k, w) total number times word w assigned topic k, N (k)total number words assigned topic k, according posterior sample z.derive similar estimate k , prior likelihood topic k:k =N (k) + 0,N + K0(15)N (k) total number paragraphs assigned topic k according samplez, N total number paragraphs entire corpus.6. Experimentssection, evaluate performance model three tasks presentedSection 5: cross-document alignment, document segmentation, information ordering.first describe preliminaries common three tasks, covering data sets,reference comparison structures, model variants, inference algorithm settings sharedevaluation. provide detailed examination model performsindividual task.6.1 General Evaluation SetupData Sets experiments use five data sets, briefly described (for additionalstatistics, see Table 1):7. approach describe finding probable paragraph ordering accordingdata likelihood, optimal ordering derived HMM-based content model.proposed ordering technique essentially approximates objective using per-paragraph maximumposteriori estimate topic assignments rather full posterior topic assignment distribution.approximation makes much faster prediction algorithm performs well empirically.147fiChen, Branavan, Barzilay, & KargerArticlesCorpusCitiesEnCitiesEn500CitiesFrlarge cities WikipediaLanguage Documents SectionsEnglish10013.2English50010.5French10010.4Paragraphs66.745.940.7Vocabulary42,00095,40031,000Tokens4,9203,1502,630Articles chemical elements WikipediaCorpusLanguage Documents SectionsElementsEnglish1187.7Paragraphs28.1Vocabulary18,000Tokens1,920Cell phone reviews PhoneArena.comCorpusLanguage Documents SectionsPhonesEnglish1006.6Paragraphs24.0Vocabulary13,500Tokens2,750Table 1: Statistics data sets used evaluations. values except vocabularysize document count per-document averages.CitiesEn: Articles English Wikipedia worlds 100 largest citiespopulation. Common topics include History, Culture, Demographics. articles typically substantial size share similar content organization patterns.CitiesEn500 : Articles English Wikipedia worlds 500 largest citiespopulation. collection superset CitiesEn. Many lower-rankedcities well known English Wikipedia editors thus, compared CitiesEnarticles shorter average exhibit greater variability content selectionordering.CitiesFr : Articles French Wikipedia 100 cities CitiesEn.Elements: Articles English Wikipedia chemical elements periodic table,8 including topics Biological Role, Occurrence, Isotopes.Phones: Reviews extracted PhoneArena.com, popular cell phone review website. Topics corpus include Design, Camera, Interface. reviewswritten expert reviewers employed site, opposed lay users.9heterogeneous collection data sets allows us examine behaviormodel diverse test conditions. sets vary articles generated,language articles written, subjects discuss. result,patterns topic organization vary greatly across domains. instance, within Phonescorpus, articles formulaic, due centralized editorial control website,establishes consistent standards followed expert reviewers. hand,Wikipedia articles exhibit broader structural variability due collaborative nature8. 118 elements http://en.wikipedia.org/wiki/Periodic table, including undiscovered element 117.9. Phones set, 35 documents short express reviews without section headings; includeinput model, evaluate them.148fiContent Modeling Using Latent PermutationsWikipedia editing, allows articles evolve independently. Wikipedia articleswithin category often exhibit similar section orderings, many idiosyncraticinversions. instance, CitiesEn corpus, Geography History sectionstypically occur toward beginning document, History appear eitherGeography across different documents.corpus consider manually divided sections authors,including short textual heading section. Sections 6.2.1 6.3.1, discussauthor-created sections headings used generate reference annotationsalignment segmentation tasks. Note use headings evaluation;none heading information provided methods consideration.tasks alignment segmentation, evaluation performed datasets presentedTable 1. ordering task, however, data used training, evaluationperformed using separate held-out set documents. details held-out datasetgiven Section 6.4.1.Model Variants evaluation, besides comparing baselines literature,also consider two variants proposed model. particular, investigateimpact Mallows component model alternately relaxing tighteningway constrains topic orderings:Constrained : variant, require documents follow exact canonical ordering topics. is, topic permutation inversions allowed, thoughdocuments may skip topics before. case viewed special casegeneral model, Mallows inversion prior 0 approaches infinity.implementation standpoint, simply fix inversion counts v zeroinference.10Uniform: variant assumes uniform distribution topic permutations,instead biasing toward small related set. Again, special case fullmodel, inversion prior 0 set zero, strength prior 0 approachinginfinity, thus forcing item always zero.Note variants still enforce long-range constraint topic contiguity,vary full model capture topic ordering similarity.Evaluation Procedure Parameter Settings evaluation modelvariants, run collapsed Gibbs sampler five random seed states, take10,000th iteration chain sample. Results presented averagefive samples.Dirichlet prior hyperparameters bag topics 0 language models 0 set0.1. GMM, set prior dispersion hyperparameter 0 1, effective10. first glance, Constrained model variant appears equivalent HMM statetransition either + 1. However, case topics may appear zero timesdocument, resulting multiple possible transitions state. Furthermore, transitionprobabilities would dependent position within document example, earlier absolutepositions within document, transitions high-index topics unlikely, would requiresubsequent paragraphs high-index topic.149fiChen, Branavan, Barzilay, & Kargersample size prior 0 0.1 times number documents. values minimallytuned, similar results achieved alternative settings 0 0 . Parameters 00 control strength bias toward structural regularity, tradingConstrained Uniform model variants. values chosen middle groundtwo extremes.model also takes parameter K controls upper bound numberlatent topics. Note algorithm select fewer K topics document,K determine number segments document. general, higher Kresults finer-grained division document different topics, may resultprecise topics, may also split topics together. report resultsevaluation using K = 10 20.6.2 Alignmentfirst evaluate model task cross-document alignment, goalgroup textual units different documents topically cohesive clusters. instance,Cities-related domains, one cluster may include Transportation-related paragraphs. turning results first present details specific evaluation setuptargeted task.6.2.1 Alignment Evaluation SetupReference Annotations generate sufficient amount reference data evaluating alignments use section headings provided authors. assume twoparagraphs aligned section headings identical. headingsconstitute noisy annotations Wikipedia datasets: topical content maylabeled different section headings different articles (e.g., CitiesEn, Placesinterest one article Landmarks another), call reference structurenoisy headings set.clear priori effect noise section headings may evaluation accuracy. empirically estimate effect, also use manually annotatedalignments experiments. Specifically, CitiesEn corpus, manually annotated articles paragraphs consistent set section headings, providing usadditional reference structure evaluate against. clean headings set, foundapproximately 18 topics expressed one document.Metrics quantify alignment output compute recall precision scorecandidate alignment reference alignment. Recall measures, uniquesection heading reference, maximum number paragraphs headingassigned one particular topic. final score computed summingsection heading dividing total number paragraphs. High recall indicatesparagraphs section headings generally assigned topic.Conversely, precision measures, topic number, maximum number paragraphs topic assignment share section heading. Precision summedtopic normalized total number paragraphs. High precision meansparagraphs assigned single topic usually correspond section heading.150fiContent Modeling Using Latent PermutationsRecall precision trade finely grained topics tendimprove precision cost recall. extremes, perfect recall occurs everyparagraph assigned topic, perfect precision paragraphtopic.also present one summary F-score results, harmonic meanrecall precision.Statistical significance setup measured approximate randomization (Noreen,1989), nonparametric test directly applied nonlinearly computed metricsF-score. test used prior evaluations information extractionmachine translation (Chinchor, 1995; Riezler & Maxwell, 2005).Baselinestask, compare two baselines:Hidden Topic Markov Model (HTMM) (Gruber et al., 2007): explained Section 2, model represents topic change adjacent textual units Markovian fashion. HTMM capture local constraints, would allow topicsrecur non-contiguously throughout document. use publicly available implementation,11 priors set according recommendations made originalwork.Clustering: use repeated bisection algorithm find clustering paragraphs maximizes sum pairwise cosine similarities itemscluster.12 clustering implemented using CLUTO toolkit.13 Noteapproach completely structure-agnostic, treating documents bags paragraphs rather sequences paragraphs. types clustering techniquesshown deliver competitive performance cross-document alignmenttasks (Barzilay & Elhadad, 2003).6.2.2 Alignment ResultsTable 2 presents results alignment evaluation. datasets, bestperformance achieved model variants, statistically significant usuallysubstantial margin.comparative performance baseline methods consistent across domainssurprisingly, clustering performs better complex HTMM model. observation consistent previous work cross-document alignment multidocumentsummarization, use clustering main component (Radev, Jing, & Budzikowska,2000; Barzilay, McKeown, & Elhadad, 1999). Despite fact HTMM capturesdependencies adjacent paragraphs, sufficiently constrained. Manual examination actual topic assignments reveals HTMM often assigns topicdisconnected paragraphs within document, violating topic contiguity constraint.one domain full GMM-based approach yields best performance compared variants. one exception Phone domain. Constrained11. http://code.google.com/p/openhtmm/12. particular clustering technique substantially outperforms agglomerative graph partitioningbased clustering approaches task.13. http://glaros.dtc.umn.edu/gkhome/views/cluto/151fiK = 20K = 10K = 20K = 10Chen, Branavan, Barzilay, & KargerClusteringHTMMConstrainedUniformmodelClusteringHTMMConstrainedUniformmodelCitiesEnClean headingsRecallPrecF-score0.5780.439 0.4990.4460.232 0.3050.5790.471 0.5200.5200.440 0.4770.639 0.5090.5660.4860.541 0.5120.2600.217 0.2370.4580.519 0.4860.4990.551 0.5240.578 0.6360.606CitiesEnNoisy headingsRecallPrecF-score0.6110.331 0.4290.4800.183 0.2650.6670.382 0.4850.5990.343 0.4360.705 0.3990.5100.5270.414 0.4640.3040.187 0.2320.5530.415 0.4740.5710.423 0.4860.648 0.4890.557CitiesEn500Noisy headingsRecallPrecF-score0.6090.329 0.4270.4610.269 0.3400.6430.385 0.4810.5820.344 0.4320.722 0.4260.5360.4890.391 0.4350.3510.234 0.2800.5150.394 0.4460.5570.422 0.4800.620 0.4730.537ClusteringHTMMConstrainedUniformmodelClusteringHTMMConstrainedUniformmodelCitiesFrNoisy headingsRecallPrecF-score0.5880.283 0.3820.3380.190 0.2440.6520.3560.4600.5870.310 0.4060.657 0.3600.4640.4530.317 0.3730.2530.195 0.2210.5840.379 0.4590.5710.373 0.4510.633 0.4310.513ElementsNoisy headingsRecallPrecF-score0.5240.361 0.4280.4300.190 0.2640.6030.408 0.4870.5910.403 0.4790.685 0.4600.5510.4770.402 0.4360.2480.243 0.2460.5100.421 0.4610.5500.479 0.5120.569 0.4980.531PhonesNoisy headingsRecallPrecF-score0.5990.456 0.5180.3790.240 0.2940.745 0.5060.6020.6560.422 0.5130.7380.4930.5910.4860.507 0.4960.2740.229 0.2490.652 0.5760.6110.6080.471 0.5380.6830.5460.607Table 2: Comparison alignments produced model series baselinesmodel variations, 10 20 topics, evaluated clean noisy setssection headings. Higher scores better. Within K, methodsmodel significantly outperforms indicated p < 0.001p < 0.01.152fiContent Modeling Using Latent Permutationsbaseline achieves best result K small margin. resultsexpected, given fact domain exhibits highly rigid topic structure acrossdocuments. model permits permutations topic ordering, GMM,flexible highly formulaic domains.Finally, observe evaluations based manual noisy annotations exhibitalmost entirely consistent ranking methods consideration (see cleannoisy headings results CitiesEn Table 2). consistency indicates noisyheadings sufficient gaining insight comparative performance differentapproaches.6.3 SegmentationNext consider task text segmentation. test whether model ableidentify boundaries topically coherent text segments.6.3.1 Segmentation Evaluation SetupReference Segmentations described Section 6.1, datasets usedevaluation manually divided sections authors. annotationsused create reference segmentations evaluating models output. RecallSection 6.2.1 also built clean reference structure CitiesEn set. structure encodes clean segmentation document adjusts granularitysection headings consistent across documents. Thus, also comparesegmentation specified CitiesEn clean section headings.Metrics Segmentation quality evaluated using standard penalty metrics PkWindowDiff (Beeferman, Berger, & Lafferty, 1999; Pevzner & Hearst, 2002). passsliding window documents compute probability words endwindows improperly segmented respect other. WindowDiffstricter, requires number segmentation boundaries endpointswindow correct well.14Baselines first compare BayesSeg (Eisenstein & Barzilay, 2008),15 Bayesiansegmentation approach current state-of-the-art task. Interestingly,model reduces approach every document considered completely isolation,topic sharing documents. Connecting topics across documents makesmuch difficult inference problem one tackled Eisenstein Barzilay.time, algorithm cannot capture structural relatedness across documents.Since BayesSeg designed operated specification number segments,provide baseline benefit knowing correct number segmentsdocument, provided system. run baseline using14. Statistical significance testing standardized usually reported segmentation task,omit tests results.15. evaluate corpora used work, since model relies content similarity acrossdocuments corpus.153fiBayesSegU&IU&IConstrainedUniformmodelConstrainedUniformmodelCitiesEnClean headingsPkWD# Segs0.3210.37612.30.3370.40412.30.3530.3755.80.260 0.2817.70.2680.3008.80.2530.2839.00.2740.31410.90.2340.29414.00.221 0.27814.2CitiesEnNoisy headingsPkWD# Segs0.3170.37613.20.3370.40513.20.3570.3785.80.2670.2887.70.2730.3048.80.257 0.2869.00.2740.31310.90.2340.29014.00.222 0.27814.2CitiesEn500Noisy headingsPkWD# Segs0.2820.33510.50.2920.35010.50.3210.3465.40.2210.2446.80.2270.2577.80.196 0.2258.10.2260.2619.10.2030.25612.30.196 0.24712.1BayesSegU&IU&IConstrainedUniformmodelConstrainedUniformmodelCitiesFrNoisy headingsPkWD# Segs0.2740.33210.40.2820.33610.40.3210.3424.40.2300.2446.40.214 0.2337.30.216 0.2337.40.2300.2507.90.2030.23410.40.201 0.23010.8ElementsNoisy headingsPkWD# Segs0.2790.3167.70.2480.2867.70.2940.3124.80.2270.2445.40.2260.2506.60.201 0.2266.70.2310.2576.60.2090.2488.70.203 0.2438.6PhonesNoisy headingsPkWD# Segs0.3920.4579.60.4120.4639.60.4230.4354.70.312 0.3478.00.3320.3677.50.3090.3498.00.295 0.34810.80.3270.3819.40.3020.35710.4K = 20 K = 10K = 20 K = 10Chen, Branavan, Barzilay, & KargerTable 3: Comparison segmentations produced model series baselinesmodel variations, 10 20 topics, evaluated clean noisysets section headings. Lower scores better. BayesSeg U&I giventrue number segments, segments counts reflect reference structuressegmentations. contrast, U&I automatically predicts number segments.154fiContent Modeling Using Latent Permutationsauthors publicly available implementation;16 priors set using built-in mechanismautomatically re-estimates hyperparameters.also compare method algorithm Utiyama Isahara (2001),commonly used point reference evaluation segmentation algorithms.algorithm computes optimal segmentation estimating changes predictedlanguage models segments different partitions. used publicly availableimplementation system,17 require parameter tuning held-outdevelopment set. contrast BayesSeg, algorithm mechanism predictingnumber segments, also take pre-specified number segments.comparison, consider versions algorithm U&I denotes casecorrect number segments provided model U&I denotes modelestimates optimal number segments.6.3.2 Segmentation ResultsTable 3 presents segmentation experiment results. every data set model outperforms BayesSeg U&I baselines substantial margin regardless K. resultprovides strong evidence learning connected topic models related documents leadsimproved segmentation performance.best performance generally obtained full version model, threeexceptions. two cases (CitiesEn K = 10 using clean headings WindowDiffmetric, CitiesFr K = 10 Pk metric), variant performs betterfull model minute margin. Furthermore, instances,corresponding evaluation K = 20 using full model leads best overallresults respective domains.case variant outperforms full model notable marginPhones data set. result unexpected given formulaic nature datasetdiscussed earlier.6.4 Orderingfinal task evaluate model finding coherent orderingset textual units. Unlike previous tasks, prediction based hidden variabledistributions, ordering observed document. Moreover, GMM model usesinformation inference process. Therefore, need divide data setstraining test portions.past, ordering algorithms applied textual units various granularities, commonly sentences paragraphs. ordering experiments operatelevel relatively larger unit sections. believe granularity suitablenature model, captures patterns level topic distributionsrather local discourse constraints. ordering sentences paragraphsstudied past (Karamanis et al., 2004; Barzilay & Lapata, 2008) two typesmodels effectively combined induce full ordering (Elsner et al., 2007).16. http://groups.csail.mit.edu/rbg/code/bayesseg/17. http://www2.nict.go.jp/x/x161/members/mutiyama/software.html#textseg155fiChen, Branavan, Barzilay, & KargerCorpusCitiesEnCitiesFrPhonesSetTrainingTestingTrainingTestingTrainingTestingDocuments100651006810064Sections13.211.210.47.76.69.6Paragraphs66.750.340.728.224.039.3Vocabulary42,00042,00031,00031,00013,50013,500Tokens4,9203,4602,6301,5802,7504,540Table 4: Statistics training test sets used ordering experiments. valuesexcept vocabulary average per document. training set statisticsreproduced Table 1 ease reference.6.4.1 Ordering Evaluation SetupTraining Test Data Sets use CitiesEn, CitiesFr Phones data setstraining documents parameter estimation described Section 5. introduceadditional sets documents domains test sets. Table 4 provides statisticstraining test set splits (note out-of-vocabulary terms test setsdiscarded).18Even though perform ordering section level, collections still posechallenging ordering task: example, average number sections CitiesEn testdocument 11.2, comparable 11.5 sentences (the unit reordering) per documentNational Transportation Safety Board corpus used previous work (Barzilay & Lee,2004; Elsner et al., 2007).Metrics report Kendalls rank correlation coefficient ordering experiments. metric measures much ordering differs reference orderunderlying assumption reasonable sentence orderings fairly similarit. Specifically, permutation sections N -section document, ()computedd(, )() = 1 2 N ,(16)2d(, ) is, before, Kendall distance, number swaps adjacent textualunits necessary rearrange reference order. metric ranges -1 (inverseorders) 1 (identical orders). Note random ordering yield zero score expectation. measure widely used evaluating information ordering (Lapata,2003; Barzilay & Lee, 2004; Elsner et al., 2007) shown correlate humanassessments text quality (Lapata, 2006).Baselines Model Variants ordering method compared originalHMM-based content modeling approach Barzilay Lee (2004). baseline delivers18. Elements data set limited 118 articles, preventing us splitting reasonably sizedtraining test sets. Therefore consider ordering experiments. Citiesrelated sets, test documents shorter cities lesser population.hand, Phones test set include short express reviews thus exhibits higheraverage document length.156fiContent Modeling Using Latent PermutationsHMM-based Content ModelConstrainedK = 10modelConstrainedK = 20modelCitiesEn0.2450.5870.5710.5830.575CitiesFr0.3050.5960.5410.5750.571Phones0.2560.6760.6780.7110.678Table 5: Comparison orderings produced model series baselinesmodel variations, 10 20 topics, evaluated respective test sets.Higher scores better.state-of-the art performance number datasets similar spirit modelalso aims capture patterns level topic distribution (see Section 2). Again,use publicly available implementation19 parameters adjusted accordingvalues used previous work. content modeling implementation provides A*search procedure use find optimal permutation.include comparison local coherence models (Barzilay & Lapata, 2008;Elsner et al., 2007). models designed sentence-level analysis particular,use syntactic information thus cannot directly applied section-level ordering.state above, models orthogonal topic-based analysis; combining twoapproaches promising direction future work.Note Uniform model variant applicable task, sincemake claims preferred underlying topic ordering. fact, document likelihood perspective, proposed paragraph order reverse order wouldprobability Uniform model. Thus, model variant considerConstrained.6.4.2 Ordering ResultsTable 5 summarizes ordering results GMM- HMM-based content models. Acrossdata sets, model outperforms content modeling large margin. instance,CitiesEn dataset, gap two models reaches 35%. differenceexpected. previous work, content models applied short formulaic texts.contrast, documents collection exhibit higher variability original collections.HMM provide explicit constraints generated global orderings. mayprevent effectively learning non-local patterns topic organization.also observe Constrained variant outperforms full model.difference two small, fairly consistent across domains. Sincepossible predict idiosyncratic variations test documents topic orderings,constrained model better capture prevalent ordering patterns consistentacross domain.19. http://people.csail.mit.edu/regina/code.html157fiChen, Branavan, Barzilay, & Karger6.5 Discussionexperiments three separate tasks reveal common trends results.First, observe single unified model document structure readilysuccessfully applied multiple discourse-level tasks, whereas previous work proposedseparate approaches task. versatility speaks power topic-drivenrepresentation document structure. Second, within task model outperformsstate-of-the-art baselines substantial margins across wide variety evaluation scenarios. results strongly support hypothesis augmenting topic modelsdiscourse-level constraints broadens applicability discourse-level analysis tasks.Looking performance model across different tasks, make notesimportance individual topic constraints. Topic contiguity consistentlyimportant constraint, allowing model variants outperform alternative baselineapproaches. cases, introducing bias toward similar topic ordering, without requiring identical orderings, provides benefits encoded model.flexible models achieve superior performance segmentation alignment tasks.case ordering, however, extra flexibility pay off, model distributesprobability mass away strong ordering patterns likely occur unseen data.also identify properties dataset strongly affect performancemodel. Constrained model variant performs slightly better full modelrigidly formulaic domains, achieving highest performance Phones data set.know priori domain formulaic structure, worthwhile choosemodel variant suitably enforces formulaic topic orderings. Fortunately, adaptationachieved proposed framework using prior Generalized MallowsModel recall Constrained variant special case full model.However, performance model invariant respect data set characteristics. Across two languages considered, model baselines exhibitcomparative performance task. Moreover, consistency also holdsgeneral-interest cities articles highly technical chemical elements articles. Finally, smaller CitiesEn larger CitiesEn500 data sets, observeresults consistent.7. Conclusions Future Workpaper, shown unsupervised topic-based approach capture contentstructure. resulting model constrains topic assignments way requires globalmodeling entire topic sequences. showed Generalized Mallows Modeltheoretically empirically appealing way capturing ordering componenttopic sequence. results demonstrate importance augmenting statistical modelstext analysis structural constraints motivated discourse theory. Furthermore,success GMM suggests could applied modeling orderingconstraints NLP applications.multiple avenues future extensions work. First, empirical resultsdemonstrated certain domains providing much flexibility model mayfact detrimental predictive accuracy. cases, tightly constrainedvariant model yields superior performance. interesting extension current158fiContent Modeling Using Latent Permutationsmodel would allow additional flexibility prior GMM drawinganother level hyperpriors. technical perspective, form hyperparameterre-estimation would involve defining appropriate hyperprior Generalized MallowsModel adapting estimation present inference procedure.Additionally, may cases assumption one canonical topic orderingentire corpus limiting, e.g., data set consists topically related articlesmultiple sources, editorial standards. model extendedallow multiple canonical orderings positing additional level hierarchyprobabilistic model, i.e., document structures generated mixture severalGeneralized Mallows Models, distributional mode. case,model would take additional burden learning topics permutedmultiple canonical orderings. change model would greatly complicateinference re-estimating Generalized Mallows Model canonical ordering general NPhard. However, recent advances statistics produced efficient approximate algorithmstheoretically guaranteed correctness bounds (Ailon, Charikar, & Newman, 2008)exact methods tractable typical cases (Meila et al., 2007).generally, model presented paper assumes two specific global constraintscontent structure. domains satisfy constraints plentiful,domains modeling assumptions hold. example, dialogue wellknown topics recur throughout conversation (Grosz & Sidner, 1986), thereby violatingfirst constraint. Nevertheless, texts domains still follow certain organizationalconventions, e.g. stack structure dialogue. results suggest explicitly incorporating domain-specific global structural constraints content model would likelyimprove accuracy structure induction.Another direction future work combine global topic structure modellocal coherence constraints. previously noted, model agnostic towardrelationships sentences within single topic. contrast, models local coherencetake advantage wealth additional knowledge, syntax, make decisionsinformation flow across adjoining sentences. linguistically rich model would providepowerful representation levels textual structure, could used evengreater variety applications considered here.Bibliographic NotePortions work previously presented conference publication (Chen, Branavan,Barzilay, & Karger, 2009). article significantly extends previous work, notablyintroducing new algorithm applying models output information orderingtask (Section 5) considering new data sets experiments vary genre,language, size (Section 6).Acknowledgmentsauthors acknowledge funding support NSF CAREER grant IIS-0448168grant IIS-0712793, NSF Graduate Fellowship, Office Naval Research, Quanta,Nokia, Microsoft Faculty Fellowship. thank many people offered159fiChen, Branavan, Barzilay, & Kargersuggestions comments work, including Michael Collins, Aria Haghighi, YoongKeok Lee, Marina Meila, Tahira Naseem, Christy Sauper, David Sontag, Benjamin Snyder,Luke Zettlemoyer. especially grateful Marina Meila introducing usMallows model. paper also greatly benefited thoughtful feedbackanonymous reviewers. opinions, findings, conclusions, recommendations expressedpaper authors, necessarily reflect views fundingorganizations.160fiContent Modeling Using Latent PermutationsReferencesAilon, N., Charikar, M., & Newman, A. (2008). Aggregating inconsistent information:Ranking clustering. Journal ACM, 55 (5).Althaus, E., Karamanis, N., & Koller, A. (2004). Computing locally coherent discourses.Proceedings ACL.Bartlett, F. C. (1932). Remembering: study experimental social psychology. Cambridge University Press.Barzilay, R., & Elhadad, N. (2003). Sentence alignment monolingual comparable corpora.Proceedings EMNLP.Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies sentence orderingmultidocument news summarization. Journal Artificial Intelligence Research,17, 3555.Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach.Computational Linguistics, 34 (1), 134.Barzilay, R., & Lee, L. (2004). Catching drift: Probabilistic content models, applications generation summarization. Proceedings NAACL/HLT.Barzilay, R., McKeown, K., & Elhadad, M. (1999). Information fusion contextmulti-document summarization. Proceedings ACL.Beeferman, D., Berger, A., & Lafferty, J. D. (1999). Statistical models text segmentation.Machine Learning, 34, 177210.Bernardo, J. M., & Smith, A. F. (2000). Bayesian Theory. Wiley Series ProbabilityStatistics.Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.Blei, D. M., & Moreno, P. J. (2001). Topic segmentation aspect hidden markovmodel. Proceedings SIGIR.Blei, D. M., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation. Journal MachineLearning Research, 3, 9931022.Bollegala, D., Okazaki, N., & Ishizuka, M. (2006). bottom-up approach sentenceordering multi-document summarization. Proceedings ACL/COLING.Chen, H., Branavan, S., Barzilay, R., & Karger, D. R. (2009). Global models documentstructure using latent permutations. Proceedings NAACL/HLT.Chinchor, N. (1995). Statistical significance MUC-6 results. Proceedings 6thConference Message Understanding.Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning order things. JournalArtificial Intelligence Research, 10, 243270.Eisenstein, J., & Barzilay, R. (2008). Bayesian unsupervised topic segmentation. Proceedings EMNLP.Elsner, M., Austerweil, J., & Charniak, E. (2007). unified local global modeldiscourse coherence. Proceedings NAACL/HLT.161fiChen, Branavan, Barzilay, & KargerFligner, M., & Verducci, J. (1986). Distance based ranking models. Journal RoyalStatistical Society, Series B, 48 (3), 359369.Fligner, M. A., & Verducci, J. S. (1990). Posterior probabilities consensus ordering.Psychometrika, 55 (1), 5363.Galley, M., McKeown, K. R., Fosler-Lussier, E., & Jing, H. (2003). Discourse segmentationmulti-party conversation. Proceedings ACL.Geman, S., & Geman, D. (1984). Stochastic relaxation, gibbs distributions bayesianrestoration images. IEEE Transactions Pattern Analysis Machine Intelligence, 12, 609628.Graesser, A., Gernsbacher, M., & Goldman, S. (Eds.). (2003). Handbook DiscourseProcesses. Erlbaum.Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings NationalAcademy Sciences, 101, 52285235.Griffiths, T. L., Steyvers, M., Blei, D. M., & Tenenbaum, J. B. (2005). Integrating topicssyntax. Advances NIPS.Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions, structure discourse.Computational Linguistics, 12 (3), 175204.Gruber, A., Rosen-Zvi, M., & Weiss, Y. (2007). Hidden topic markov models. ProceedingsAISTATS.Halliday, M. A. K., & Hasan, R. (1976). Cohesion English. Longman.Hearst, M. (1994). Multi-paragraph segmentation expository text. ProceedingsACL.Ji, P. D., & Pulman, S. (2006). Sentence ordering manifold-based classificationmulti-document summarization. Proceedings EMNLP.Karamanis, N., Poesio, M., Mellish, C., & Oberlander, J. (2004). Evaluating centeringbased metrics coherence text structuring using reliably annotated corpus.Proceedings ACL.Klementiev, A., Roth, D., & Small, K. (2008). Unsupervised rank aggregation distancebased models. Proceedings ICML, pp. 472479.Lapata, M. (2003). Probabilistic text structuring: Experiments sentence ordering.Proceedings ACL.Lapata, M. (2006). Automatic evaluation information ordering: Kendalls tau. Computational Linguistics, 32 (4), 471484.Lebanon, G., & Lafferty, J. (2002). Cranking: combining rankings using conditional probability models permutations. Proceedings ICML.Malioutov, I., & Barzilay, R. (2006). Minimum cut model spoken lecture segmentation.Proceedings ACL.Mallows, C. L. (1957). Non-null ranking models. Biometrika, 44, 114130.162fiContent Modeling Using Latent PermutationsMeila, M., Phadnis, K., Patterson, A., & Bilmes, J. (2007). Consensus rankingexponential model. Proceedings UAI.Neal, R. M. (2003). Slice sampling. Annals Statistics, 31, 705767.Nelken, R., & Shieber, S. M. (2006). Towards robust context-sensitive sentence alignmentmonolingual corpora. Proceedings EACL.Noreen, E. W. (1989). Computer Intensive Methods Testing Hypotheses. Introduction. Wiley.Pevzner, L., & Hearst, M. A. (2002). critique improvement evaluation metrictext segmentation. Computational Linguistics, 28, 1936.Porteous, I., Newman, D., Ihler, A., Asuncion, A., Smyth, P., & Welling, M. (2008). Fastcollapsed gibbs sampling latent dirichlet allocation. Proceedings SIGKDD.Purver, M., Kording, K., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topicmodelling multi-party spoken discourse. Proceedings ACL/COLING.Radev, D., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: Sentence extraction, utility-based evaluation user studies.Proceedings ANLP/NAACL Summarization Workshop.Riezler, S., & Maxwell, J. T. (2005). pitfalls automatic evaluation significance testing MT. Proceedings ACL Workshop Intrinsic ExtrinsicEvaluation Measures Machine Translation and/or Summarization.Schiffrin, D., Tannen, D., & Hamilton, H. E. (Eds.). (2001). Handbook DiscourseAnalysis. Blackwell.Titov, I., & McDonald, R. (2008). Modeling online reviews multi-grain topic models.Proceedings WWW.Utiyama, M., & Isahara, H. (2001). statistical model domain-independent text segmentation. Proceedings ACL.van Mulbregt, P., Carp, I., Gillick, L., Lowe, S., & Yamron, J. (1998). Text segmentationtopic tracking broadcast news via hidden markov model approach. ProceedingsICSLP.Wallach, H. M. (2006). Topic modeling: beyond bag words. Proceedings ICML.Wray, A. (2002). Formulaic Language Lexicon. Cambridge University Press, Cambridge.163fiJournal Artificial Intelligence Research 36 (2009) 267-306Submitted 06/09; published 10/09ParamILS: Automatic Algorithm Configuration FrameworkFrank HutterHolger H. HoosKevin Leyton-BrownHUTTER @ CS . UBC . CAHOOS @ CS . UBC . CAKEVINLB @ CS . UBC . CAUniversity British Columbia, 2366 Main MallVancouver, BC, V6T1Z4, CanadaThomas StutzleSTUETZLE @ ULB . AC .Universite Libre de Bruxelles, CoDE, IRIDIAAv. F. Roosevelt 50 B-1050 Brussels, BelgiumAbstractidentification performance-optimizing parameter settings important part development application algorithms. describe automatic framework algorithmconfiguration problem. formally, provide methods optimizing target algorithmsperformance given class problem instances varying set ordinal and/or categorical parameters. review family local-search-based algorithm configuration procedurespresent novel techniques accelerating adaptively limiting time spent evaluating individual configurations. describe results comprehensive experimental evaluationmethods, based configuration prominent complete incomplete algorithmsSAT. also present is, knowledge, first published work automatically configuring C PLEX mixed integer programming solver. algorithms considered defaultparameter settings manually identified considerable effort. Nevertheless, usingautomated algorithm configuration procedures, achieved substantial consistent performanceimprovements.1. IntroductionMany high-performance algorithms parameters whose settings control important aspectsbehaviour. particularly case heuristic procedures used solving computationally hard problems.1 example, consider C PLEX, commercial solver mixed integerprogramming problems.2 CPLEX version 10 80 parameters affect solvers searchmechanism configured user improve performance. many acknowledgements literature finding performance-optimizing parameter configurations heuristic algorithms often requires considerable effort (see, e.g., Gratch & Chien, 1996; Johnson, 2002;Diao, Eskesen, Froehlich, Hellerstein, Spainhower & Surendra, 2003; Birattari, 2004; Adenso-Diaz& Laguna, 2006). many cases, tedious task performed manually ad-hoc way. Automating task high practical relevance several contexts.Development complex algorithms Setting parameters heuristic algorithmhighly labour-intensive task, indeed consume large fraction overall development1. use term heuristic algorithm includes methods without provable performance guarantees wellmethods guarantees, nevertheless make use heuristic mechanisms. latter case, useheuristic mechanisms often results empirical performance far better bounds guaranteed rigoroustheoretical analysis.2. http://www.ilog.com/products/cplex/c2009AI Access Foundation. rights reserved.fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEtime. use automated algorithm configuration methods lead significant timesavings potentially achieve better results manual, ad-hoc methods.Empirical studies, evaluations, comparisons algorithms central question comparing heuristic algorithms whether one algorithm outperforms another fundamentally superior, developers successfully optimized parameters (Johnson, 2002). Automatic algorithm configuration methods mitigate problem unfaircomparisons thus facilitate meaningful comparative studies.Practical use algorithms ability complex heuristic algorithms solve largehard problem instances often depends critically use suitable parameter settings.End users often little knowledge impact algorithms parametersettings performance, thus simply use default settings. Even carefullyoptimized standard benchmark set, default configuration may perform wellparticular problem instances encountered user. Automatic algorithm configurationmethods used improve performance principled convenient way.wide variety strategies automatic algorithm configuration explored literature. Briefly, include exhaustive enumeration, hill-climbing (Gratch & Dejong, 1992), beamsearch (Minton, 1993), genetic algorithms (Terashima-Marn, Ross & Valenzuela-Rendon, 1999),experimental design approaches (Coy, Golden, Runger & Wasil, 2001), sequential parameter optimization (Bartz-Beielstein, 2006), racing algorithms (Birattari, Stutzle, Paquete & Varrentrapp,2002; Birattari, 2004; Balaprakash, Birattari & Stutzle, 2007), combinations fractional experimental design local search (Adenso-Diaz & Laguna, 2006). discussrelated work extensively Section 9. Here, note authors referoptimization algorithms performance setting (typically numerical) parametersparameter tuning, favour term algorithm configuration (or simply, configuration).motivated fact interested methods deal potentially large numberparameters, numerical, ordinal (e.g., low, medium, high) categorical (e.g., choice heuristic). Categorical parameters used select combine discretebuilding blocks algorithm (e.g., preprocessing variable ordering heuristics); consequently,general view algorithm configuration includes automated construction heuristic algorithm building blocks. best knowledge, methods discussed articleyet general ones available configuration algorithms many categoricalparameters.give overview follows highlight main contributions. formally stating algorithm configuration problem Section 2, Section 3 describe ParamILS(first introduced Hutter, Hoos & Stutzle, 2007), versatile stochastic local search approachautomated algorithm configuration, two instantiations, BasicILS FocusedILS.introduce adaptive capping algorithm runs, novel technique usedenhance search-based algorithm configuration procedures independently underlying searchstrategy (Section 4). Adaptive capping based idea avoiding unnecessary runsalgorithm configured developing bounds performance measure optimized.present trajectory-preserving variant heuristic extension technique. discussing experimental preliminaries Section 5, Section 6 present empirical evidence showing adaptive capping speeds BasicILS FocusedILS. also show BasicILS268fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKoutperforms random search simple local search, well evidence FocusedILSoutperforms BasicILS.present extensive evidence ParamILS find substantially improved parameter configurations complex highly optimized algorithms. particular, apply automaticalgorithm configuration procedures aforementioned commercial optimization tool C PLEX,one powerful, widely used complex optimization algorithms aware of.stated C PLEX user manual (version 10.0, page 247), great deal algorithmic development effort devoted establishing default ILOG C PLEX parameter settings achievegood performance wide variety MIP models. demonstrate consistent improvementsdefault parameter configuration wide range practically relevant instance distributions. cases, able achieve average speedup order magnitudepreviously-unseen test instances (Section 7). believe first resultspublished automatically configuring C PLEX piece software comparable complexity.Section 8 review wide range (separately-published) ParamILS applications. Specifically, survey work considered optimization complete incomplete heuristicsearch algorithms problems propositional satisfiability (SAT), probable explanation(MPE), protein folding, university time-tabling, algorithm configuration itself. threecases, ParamILS integral part algorithm design process allowed explorationlarge design spaces. could done effectively manual wayexisting automated method. Thus, automated algorithm configuration general ParamILSparticular enables new way (semi-)automatic design algorithms components.Section 9 presents related work and, finally, Section 10 offers discussion conclusions.distill common patterns helped ParamILS succeed various applications. alsogive advice practitioners would like apply automated algorithm configuration generalParamILS particular, identify promising avenues research future work.2. Problem Statement Notationalgorithm configuration problem consider work informally stated follows:given algorithm, set parameters algorithm set input data, find parametervalues algorithm achieves best possible performance input data.avoid potential confusion algorithms whose performance optimized algorithms used carrying optimization task, refer former target algorithmslatter configuration procedures (or simply configurators). setup illustratedFigure 1. Different algorithm configuration problems also considered literature, including setting parameters per-instance basis adapting parameters algorithmrunning. defer discussion approaches Section 9.following, define algorithm configuration problem formally introducenotation use throughout article. Let denote algorithm, let p1 , . . . , pkparameters A. Denote domain possible values parameter pi . Throughoutwork, assume parameter domains finite sets. assumption metdiscretizing numerical parameters finite number values. Furthermore, parameters269fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEFigure 1: configuration scenario includes algorithm configured collection problem instances. configuration procedure executes target algorithm specified parameter settingsinstances, receives information performance runs, usesinformation decide subsequent parameter configurations evaluate.may ordered, exploit ordering relations. Thus, effectively assumeparameters finite categorical.3problem formulation allows us express conditional parameter dependencies (for example,one algorithm parameter might used select among search heuristics, heuristicsbehaviour controlled parameters). case, values parametersirrelevant heuristic selected. ParamILS exploits effectively searchesspace equivalence classes parameter configuration space. addition, formulation supportsconstraints feasible combinations parameter values. use 1 . . . k denotespace feasible parameter configurations, A() denoting instantiation algorithmparameter configuration .Let denote probability distribution space problem instances, denote element . may given implicitly, random instance generator distributiongenerators. also possible (and indeed common) consist finite sampleinstances; case, define uniform distribution .many ways measuring algorithms performance. example, might interested minimizing computational resources consumed given algorithm (such runtime,memory communication bandwidth), maximizing quality solution found. Sincehigh-performance algorithms computationally-challenging problems often randomized,behaviour vary significantly multiple runs. Thus, algorithm always achieveperformance, even run repeatedly fixed parameters single problem instance. overall goal must therefore choose parameter settings minimize coststatistic algorithms performance across input data. denote statistic c().example, might aim minimize mean runtime median solution cost.intuition mind, define algorithm configuration problem formally.Definition 1 (Algorithm Configuration Problem). instance algorithm configuration problem 6-tuple hA, , D, max , o, mi, where:parameterized algorithm;parameter configuration space A;3. currently extending algorithm configuration procedures natively support parameter types.270fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKdistribution problem instances domain ;max cutoff time (or captime), run terminated still running;function measures observed cost running A() instancecaptime R (examples runtime solving instance, cost solution found)statistical population parameter (such expectation, median, variance).parameter configuration candidate solution algorithm configurationproblem. configuration , denotes distribution costs induced function o,applied instances drawn distribution multiple independent runs randomizedalgorithms, using captime = max . cost candidate solution definedc() := m(O ),(1)statistical population parameter cost distribution . optimal solution, , minimizes c():arg min c().(2)algorithm configuration procedure procedure solving algorithm configurationproblem. Unfortunately, least algorithm configuration problems considered article, cannot optimize c closed form since access algebraic representation function. denote sequence runs executed configurator R =((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , )). ith run described five values:denotes parameter configuration evaluated;denotes instance algorithm run;si denotes random number seed used run (we keep track seeds able blockthem, see Section 5.1.2);denotes runs captime;oi denotes observed cost runNote , , s, , vary one element R next, regardless whetherelements held constant. denote ith run R R[i], subsequenceruns using parameter configuration (i.e., runs = ) R . configurationprocedures considered article compute empirical estimates c() based solely R ,principle methods could used. compute cost estimates online,runtime configurator, well offline, evaluation purposes.Definition 2 (Cost Estimate). Given algorithm configuration problem hA, , D, max , o, mi,define cost estimate cost c() based sequence runs R = ((1 , 1 , s1 , 1 , o1 ), . . . ,(n , n , sn , n , )) c(, R) := m({oi | = }), sample statistic analoguestatistical population parameter m.example, c() expected runtime distribution instances random numberseeds, c(, R) sample mean runtime runs R .configuration procedures paper anytime algorithms, meaning timeskeep track configuration currently believed lowest cost; refer configuration incumbent configuration, short incumbent, inc . evaluate configuratorsperformance time means incumbents training test performance, defined follows.271fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEDefinition 3 (Training performance). time configurator performed sequence runs R = ((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , )) solve algorithm configuration problem hA, , D, max , o, mi, thereby found incumbent configuration inc ,training performance time defined cost estimate c(inc , R).set instances {1 , . . . , n } discussed called training set. true costparameter configuration cannot computed exactly, estimated using training performance. However, training performance configurator biased estimator incumbentstrue cost, instances used selecting incumbent evaluating it.order achieve unbiased estimates offline evaluation, set aside fixed set instances{10 , . . . , T0 } (called test set) random number seeds {s01 , . . . , s0T }, unknownconfigurator, use evaluation.Definition 4 (Test performance). time t, let configurators incumbent algorithmconfiguration problem hA, , D, max , o, mi inc (this found means executing sequence runs training set). Furthermore, let R0 = ((inc , 10 , s01 , max , o1 ), . . . , (inc , T0 ,s0T , max , oT )) sequence runs instances random number seeds test set(which performed offline evaluation purposes), configurators test performancetime defined cost estimate c(inc , R0 ).Throughout article, aim minimize expected runtime. (See Section 5.1.1 discussionchoice.) Thus, configurators training performance mean runtime runsperformed incumbent. test performance mean runtime incumbenttest set. Note that, configurator free use max , test performance alwayscomputed using maximal captime, max .obvious automatic algorithm configurator choose runs order bestminimize c() within given time budget. particular, make following choices:1. parameter configurations 0 evaluated?2. problem instances 0 used evaluating 0 0 ,many runs performed instance?3. cutoff time used run?Hutter, Hoos Leyton-Brown (2009) considered design space detail, focusingtradeoff (fixed) number problem instances used evaluationparameter configuration (fixed) cutoff time used run, well interactionchoices number configurations considered. contrast, here, studyadaptive approaches selecting number problem instances (Section 3.3) cutofftime evaluation parameter configuration (Section 4); also study configurationsselected (Sections 3.1 6.2).3. ParamILS: Iterated Local Search Parameter Configuration Spacesection, address first important previously mentioned dimensionsautomated algorithm configuration, search strategy, describing iterated local searchframework called ParamILS. start with, fix two dimensions, using unvaryingbenchmark set instances fixed cutoff times evaluation parameter configuration. Thus, stochastic optimization problem algorithm configuration reduces simple272fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKoptimization problem, namely find parameter configuration yields lowest mean runtime given benchmark set. Then, Section 3.3, address second question manyruns performed configuration.3.1 ParamILS frameworkConsider following manual parameter optimization process:1. begin initial parameter configuration;2. experiment modifications single parameter values, accepting new configurations whenever result improved performance;3. repeat step 2 single-parameter change yields improvement.widely used procedure corresponds manually-executed local search parameter configuration space. Specifically, corresponds iterative first improvement procedure searchspace consisting possible configurations, objective function quantifies performanceachieved target algorithm given configuration, neighbourhood relation basedmodification one single parameter value time (i.e., one-exchange neighbourhood).Viewing manual procedure local search algorithm advantageous suggestsautomation procedure well improvement drawing ideas stochasticlocal search community. example, note procedure stops soon reaches local optimum (a parameter configuration cannot improved modifying single parameter value).sophisticated approach employ iterated local search (ILS; Lourenco, Martin & Stutzle,2002) search performance-optimizing parameter configurations. ILS prominent stochasticlocal search method builds chain local optima iterating main loop consisting(1) solution perturbation escape local optima, (2) subsidiary local search procedure(3) acceptance criterion decide whether keep reject newly obtained candidate solution.ParamILS (given pseudocode Algorithm 1) ILS method searches parameter configuration space. uses combination default random settings initialization, employsiterative first improvement subsidiary local search procedure, uses fixed number (s) random moves perturbation, always accepts better equally-good parameter configurations,re-initializes search random probability prestart .4 Furthermore, basedone-exchange neighbourhood, is, always consider changing one parameter time.ParamILS deals conditional parameters excluding configurations neighbourhood configuration differ conditional parameter relevant .3.2 BasicILS Algorithmorder turn ParamILS specified Algorithm Framework 1 executable configurationprocedure, necessary instantiate function better determines two parameter settings preferred. ultimately propose several different ways this.Here, describe simplest approach, call BasicILS. Specifically, use termBasicILS(N ) refer ParamILS algorithm function better(1 , 2 ) implementedshown Procedure 2: simply comparing estimates cN cost statistics c(1 ) c(2 )based N runs each.4. original parameter choices hr, s, prestart = h10, 3, 0.01i (from Hutter et al., 2007) somewhat arbitrary,though expected performance quite robust respect settings. revisit issue Section 8.4.273fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEAlgorithm Framework 1: ParamILS(0 , r, prestart , s)Outline iterated local search parameter configuration space; specific variants ParamILSstudy, BasicILS(N) FocusedILS, derived framework instantiating procedurebetter (which compares , 0 ). BasicILS(N) uses betterN (see Procedure 2), FocusedILSuses betterF oc (see Procedure 3). neighbourhood Nbh() configuration setconfigurations differ one parameter, excluding configurations differing conditionalparameter relevant .Input : Initial configuration 0 , algorithm parameters r, prestart , s.Output : Best parameter configuration found.1 = 1, . . . , r2random ;3better(, 0 ) 0 ;456ils IterativeFirstImprovement (0 );TerminationCriterion()ils ;7// ===== Perturbation= 1, . . . , random 0 Nbh();8// ===== Basic local searchIterativeFirstImprovement ();910// ===== AcceptanceCriterionbetter(, ils ) ils ;probability prestart ils random ;11return overall best inc found;1213141516Procedure IterativeFirstImprovement ()repeat0 ;foreach 00 N bh( 0 ) randomized orderbetter( 00 , 0 ) 00 ; break;17180 = ;return ;BasicILS(N ) simple intuitive approach since evaluates every parameter configurationrunning N training benchmark instances using random number seeds.Like many related approaches (see, e.g., Minton, 1996; Coy et al., 2001; Adenso-Diaz &Laguna, 2006), deals stochastic part optimisation problem using estimatebased fixed training set N instances. benchmark instances heterogeneousProcedure 2: betterN (1 , 2 )Procedure used BasicILS(N ) RandomSearch(N ) compare two parameter configurations. Procedure objective(, N ) returns user-defined objective achieved A() first N instanceskeeps track incumbent solution, inc ; detailed Procedure 4 page 279.Input: Parameter configuration 1 , parameter configuration 2Output: True 1 better equal 2 first N instances; false otherwiseSide Effect : Adds runs global caches performed algorithm runs R1 R2 ; potentiallyupdates incumbent inc1 cN (2 ) objective(2 , N )2 cN (1 ) objective(1 , N )3 return cN (1 ) cN (2 )274fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKuser identify rather small representative subset instances, approach findgood parameter configurations low computational effort.3.3 FocusedILS: Adaptively Selecting Number Training Instancesquestion choose number training instances, N , BasicILS(N ) straightforward answer: optimizing performance using small training set leads good trainingperformance, poor generalization previously unseen test benchmarks. hand,clearly cannot evaluate every parameter configuration enormous training setif did,search progress would unreasonably slow.FocusedILS variant ParamILS deals problem adaptively varyingnumber training samples considered one parameter configuration another. denotenumber runs available estimate cost statistic c() parameter configurationN (). performed different numbers runs using different parameter configurations,face question comparing two parameter configurations 0 N () N ( 0 ).One option would simply compute empirical cost statistic based available numberruns configuration. However, lead systematic biases if, example, firstinstances easier average instance. Instead, compare 0 based N () runsinstances seeds. amounts blocking strategy, straight-forwardadaptation known variance reduction technique; see 5.1 detailed discussion.approach comparison leads us concept domination. say dominates 0least many runs conducted 0 , performance A()first N ( 0 ) runs least good A( 0 ) runs.Definition 5 (Domination). 1 dominates 2 N (1 ) N (2 ) cN (2 ) (1 )cN (2 ) (2 ).ready discuss comparison strategy encoded procedure betterF oc (1 , 2 ),used FocusedILS algorithm (see Procedure 3). procedure first acquires oneadditional sample configuration smaller N (i ), one run configurationsnumber runs. Then, continues performing runs way one configuration dominates other. point returns true 1 dominates 2 , false otherwise.also keep track total number configurations evaluated since last improving step (i.e.,since last time betterF oc returned true); denote number B. Whenever betterF oc (1 , 2 )returns true, perform B bonus runs 1 reset B 0. mechanism ensuresperform many runs good configurations, error made every comparison twoconfigurations 1 2 decreases expectation.difficult show limit, FocusedILS sample every parameter configurationunbounded number times. proof relies fact that, instantiation ParamILS,FocusedILS performs random restarts positive probability.Lemma 6 (Unbounded number evaluations). Let N (J, ) denote number runs FocusedILSperformed parameter configuration end ILS iteration J estimate c(). Then,constant K configuration (with finite ||), limJ P [N (J, ) K] = 1.Proof. ILS iteration ParamILS, probability prestart > 0 new configurationpicked uniformly random, probability 1/||, configuration . probability275fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEProcedure 3: betterF oc (1 , 2 )Procedure used FocusedILS compare two parameter configurations. Procedure objective(, N )returns user-defined objective achieved A() first N instances, keeps track incumbent solution, updates R (a global cache algorithm runs performed parameter configuration ); detailed Procedure 4 page 279. , N () = length(R ). B globalcounter denoting number configurations evaluated since last improvement step.Input: Parameter configuration 1 , parameter configuration 2Output: True 1 dominates 2 , false otherwiseSide Effect: Adds runs global caches performed algorithm runs R1 R2 ; updatesglobal counter B bonus runs, potentially incumbent inc1 B B+12 N (1 ) N (2 )3min 1 ; max 24N (1 ) = N (2 ) B B + 1else min 2 ; max 1repeatN (min ) + 1ci (max ) objective(max , i) // N (min ) = N (max ), adds new run Rmax .ci (min ) objective(min , i) // Adds new run Rmin .10 dominates(1 , 2 ) dominates(2 , 1 )11 dominates(1 , 2 )56789// ===== Perform B bonus runs.121314cN (1 )+B (1 ) objective(1 , N (1 ) + B) // Adds B new runs R1 .B 0return true15else return false161718Procedure dominates(1 , 2 )N (1 ) < N (2 ) return falsereturn objective(1 , N (2 )) objective(2 , N (2 ))visiting ILS iteration thus p prestart> 0. Hence, number runs performed||lower-bounded binomial randomvariable B(k; J, p). Then, constant k < K obtainlimJ B(k; J, p) = limJ Jk pk (1 p)Jk = 0. Thus, limJ P [N (J, ) K] = 1.Definition 7 (Consistent estimator). cN () consistent estimator c() iff> 0 : lim P (|cN () c()| < ) = 1.NcN () consistent estimator c(), cost estimates become reliableN approaches infinity, eventually eliminating overconfidence possibility mistakescomparing two parameter configurations. fact captured following lemma.Lemma 8 (No mistakes N ). Let 1 , 2 two parameter configurationsc(1 ) < c(2 ). Then, consistent estimators cN , limN P (cN (1 ) cN (2 )) = 0.Proof. Write c1 shorthand c(1 ), c2 c(2 ), c1 cN (1 ), c2 cN (2 ). Define= 21 (c2 + c1 ) midpoint c1 c2 , = c2 = c1 > 0distance two points. Since cN consistent estimator c, estimatec1 comes arbitrarily close real cost c1 . is, limN P (|c1 c1 | < ) = 1. Since276fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK|m c1 | = , estimate c1 cannot greater equal m: limN P (c1 m) = 0.Similarly, limN P (c2 < m) = 0. SinceP (c1 c2 ) = P (c1 c2 c1 m) + P (c1 c2 c1 < m)= P (c1 c2 c1 m) + P (c1 c2 c1 < c2 < m)P (c1 m) + P (c2 < m),limN P (c1 c2 ) limN (P (c1 m) + P (c2 < m)) = 0 + 0 = 0.Combining two lemmata show limit, FocusedILS guaranteedconverge true best parameter configuration.Theorem 9 (Convergence FocusedILS). FocusedILS optimizes cost statistic c basedconsistent estimator cN , probability finds true optimal parameter configurationapproaches one number ILS iterations goes infinity.Proof. According Lemma 6, N () grows unboundedly . 1 , 2 ,N (1 ) N (2 ) go infinity, Lemma 8 states pairwise comparison, truly betterconfiguration preferred. Thus eventually, FocusedILS visits finitely many parameterconfigurations prefers best one others probability arbitrarily close one.note many practical scenarios cost estimators may consistentthat is,may fail closely approximate true performance given parameter configuration evenlarge number runs target algorithm. example, finite training set, , usedconfiguration rather distribution problem instances, D, even large N , cNaccurately reflect cost parameter configurations training set, . smalltraining sets, , cost estimate based may differ substantially true cost definedperformance across entire distribution, D. larger training set, , smallerexpected difference (it vanishes training set size goes infinity). Thus, important uselarge training sets (which representative distribution interest) whenever possible.4. Adaptive Capping Algorithm Runsconsider last dimensions automated algorithm configuration, cutoff timerun target algorithm. introduce effective simple capping techniqueadaptively determines cutoff time run. motivation capping technique comesproblem encountered configuration procedures considered article: oftensearch performance-optimizing parameter setting spends lot time evaluating parameter configuration much worse other, previously-seen configurations.Consider, example, case parameter configuration 1 takes total 10 secondssolve N = 100 instances (i.e., mean runtime 0.1 seconds per instance), another parameter configuration 2 takes 100 seconds solve first instances. order comparemean runtimes 1 2 based set instances, knowing runtimes 1 ,necessary run 2 100 instances. Instead, already terminate first run 210 + seconds. results lower bound 2 mean runtime 0.1 + /100 sinceremaining 99 instances could take less zero time. lower bound exceeds meanruntime 1 , already certain comparison favour 1 . insightprovides basis adaptive capping technique.277fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE4.1 Adaptive Capping BasicILSsection, introduce adaptive capping BasicILS. first introduce trajectory-preservingversion adaptive capping (TP capping) provably change BasicILSs search trajectory lead large computational savings. modify strategy heuristicallyperform aggressive adaptive capping (Aggr capping), potentially yielding even better performance practice.4.1.1 RAJECTORY- PRESERVING C APPINGObserve comparisons parameter configurations ParamILS pairwise.BasicILS(N ), comparisons based Procedure betterN (1 , 2 ), 2 eitherbest configuration encountered ILS iteration best configuration last ILS iteration. Without adaptive capping, comparisons take long time, since poor parameterconfiguration easily take order magnitude longer good configurations.case optimizing mean non-negative cost functions (such runtime solutioncost), implement bounded evaluation parameter configuration based N runsgiven performance bound Procedure objective (see Procedure 4). procedure sequentiallyperforms runs parameter configuration run computes lower bound cN ()based N runs performed far. Specifically, objective mean runtimesum runtimes runs, divide sum N ; since runtimes mustnonnegative, quantity lower bounds cN (). lower bound exceeds bound passedargument, skip remaining runs . order pass appropriate boundsProcedure objective, need slightly modify Procedure betterN (see Procedure 2 page 274)adaptive capping. Procedure objective bound additional third argument,set line 1 betterN , cN (2 ) line 2.approach results computation exactly function betterN usedoriginal version BasicILS, modified procedure follows exactly search trajectorywould followed without capping, typically requires much less runtime. Hence, withinamount overall running time, new version BasicILS tends able searchlarger part parameter configuration space. Although work focus objectiveminimizing mean runtime decision algorithms, note adaptive capping approachapplied easily configuration objectives.4.1.2 AGGRESSIVE C APPINGdemonstrate Section 6.4, use trajectory-preserving adaptive capping resultsubstantial speedups BasicILS. However, sometimes approach still less efficientcould be. upper bound cumulative runtime used capping computedbest configuration encountered current ILS iteration (where new ILS iteration beginsperturbation), opposed overall incumbent. perturbation resultednew parameter configuration , new iterations best configuration initialized .frequent case new performs poorly, capping criterion apply quicklycomparison performed overall incumbent.counteract effect, introduce aggressive capping strategy terminateevaluation poorly-performing configuration time. heuristic extensionadaptive capping technique, bound evaluation parameter configuration per278fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKProcedure 4: objective(, N, optional parameter bound)Procedure computes cN (), either performing new runs exploiting previous cached runs.optional third parameter specifies bound computation performed; parameterspecified, bound taken . , N () number runs performed ,i.e., length global array R . computing runtimes, count unsuccessful runs 10times cutoff time.Input: Parameter configuration , number runs, N , optional bound boundOutput: cN () cN () bound, otherwise large constant (maxPossibleObjective) plusnumber instances remain unsolved bound exceededSide Effect: Adds runs global cache performed algorithm runs, R ; updates globalincumbent, inc// ===== Maintain invariant: N (inc ) N ()12=6 inc N (inc ) < NcN (inc ) objective(inc , N, ) // Adds N N (inc ) runs Rinc// ===== aggressive capping, update bound.3Aggressive capping bound min(bound, bm cN (inc ))// ===== Update run results tuple R .= 1...Nsum runtime sum runtimes R [1], . . . , R [i 1] // Tuple indices starting 1.0i max(max , N bound sum runtime)N () (, , , oi ) R [i]N () ((i 0i oi = unsuccessful) (i < 0i oi 6= unsuccessful))o0i oi // Previous run longer yet unsuccessful shorter yet successful re-use result9else10o0i objective newly executed run A() instance seed si captime4567811121314R [i] (, , 0i , o0i )1/N (sum runtime + o0i ) > bound return maxPossibleObjective + (N + 1)N = N (inc ) (sum runtimes R ) < (sum runtimes Rinc ) increturn 1/N (sum runtimes R )formance incumbent parameter configuration multiplied factor call boundmultiplier, bm. comparison two parameter configurations 0 performed evaluations terminated preemptively, configuration solvedinstances within allowed time taken better one. (This behaviour achievedline 12 Procedure objective, keeps track number instances solved exceeding bound.) Ties broken favour moving new parameter configuration insteadstaying current one.Depending bound multiplier, use aggressive capping mechanism may changesearch trajectory BasicILS. bm = heuristic method reduces trajectorypreserving method, aggressive setting bm = 1 means know parameterconfiguration worse incumbent, stop evaluation. experiments setbm = 2, meaning lower bound performance configuration exceeds twiceperformance incumbent solution, evaluation terminated. (In Section 8.4, revisitchoice bm = 2, configuring parameters ParamILS itself.)279fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE4.2 Adaptive Capping FocusedILSmain difference BasicILS FocusedILS latter adaptively varies number runs used evaluate parameter configuration. difference complicates,prevent use adaptive capping. FocusedILS always compares pairs parameter configurations based number runs configuration, even thoughnumber differ one comparison next.Thus, extend adaptive capping FocusedILS using separate bounds every numberruns, N . Recall FocusedILS never moves one configuration, , neighbouringconfiguration, 0 , without performing least many runs 0 performed .Since keep track performance number runs N (), boundevaluation 0 always available. Therefore, implement trajectory-preservingaggressive capping BasicILS.BasicILS, FocusedILS inner workings adaptive capping implementedProcedure objective (see Procedure 4). need modify Procedure betterF oc (see Procedure3 page 276) call objective right bounds. leads following changesProcedure betterF oc . Subprocedure dominates line 16 takes bound additionalargument passes two calls objective line 18. two calls dominatesline 10 one call line 11 use bound cmax . three direct calls objectivelines 8, 9, 12 use bounds , cmax , , respectively.5. Experimental Preliminariessection give background information computational experiments presentedfollowing sections. First, describe design experiments. Next, presentconfiguration scenarios (algorithm/benchmark data combinations) studied following section.Finally, describe low-level details experimental setup.5.1 Experimental Designdescribe objective function methods used selecting instances seeds.5.1.1 C ONFIGURATION BJECTIVE : P ENALIZED AVERAGE RUNTIMESection 2, mentioned algorithm configuration problems arise context variousdifferent cost statistics. Indeed, past work explored several them: maximizing solutionquality achieved given time, minimizing runtime required reach given solution quality,minimizing runtime required solve single problem instance (Hutter et al., 2007).work focus objective minimizing mean runtime instancesdistribution D. optimization objective naturally occurs many practical applications.also implies strong correlation c() amount time required obtain goodempirical estimate c(). correlation helps make adaptive capping scheme effective.One might wonder whether means right way aggregate runtimes. preliminaryexperiments, found minimizing mean runtime led parameter configurations overall good runtime performance, including rather competitive median runtimes, minimizingmedian runtime yielded less robust parameter configurations timed large (but < 50%)fraction benchmark instances. However, encounter runs terminate within280fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKgiven cutoff time mean ill-defined. order penalize timeouts, define penalizedaverage runtime (PAR) set runs cutoff time max mean runtimeruns, unsuccessful runs counted p max penalization constant p 1.study, use p = 10.5.1.2 ELECTING NSTANCES EEDSmentioned previously, often finite set instances available upon evaluatealgorithm. case experiments report here. Throughout study, configuration experiments performed training set containing half given benchmark instances.remaining instances solely used test set evaluate found parameter configurations.evaluations within ParamILS based N runs, selected N instancesrandom number seeds used following common blocking technique (see, e.g., Birattariet al., 2002; Ridge & Kudenko, 2006). ensured whenever two parameter configurationscompared, cost estimates based exactly instances seeds.serves avoid noise effects due differences instances use different seeds.example, prevents us making mistake considering configuration betterconfiguration 0 tested easier instances.dealing randomized target algorithms, also tradeoff numberproblem instances used number independent runs performed instance.extreme case, given sample size N , one could perform N runs single instancesingle run N different instances. latter strategy known result minimal varianceestimator common optimization objectives minimization mean runtime (whichconsider study) maximization mean solution quality (see, e.g., Birattari, 2004).Consequently, performed multiple runs per instance wanted acquiresamples cost distribution instances training set.Based considerations, configuration procedures study articleimplemented take list hinstance, random number seedi pairs one inputs. Empiricalestimates cN () cost statistic c() optimized determined first N hinstance,seedi pairs list. list hinstance, seedi pairs constructed follows. Given trainingset consisting problem instances, N , drew sample N instances uniformlyrandom without replacement added list. wished evaluate algorithmsamples training instances, could happen case randomizedalgorithms, repeatedly drew random samples size described before,batch corresponded random permutation N training instances, added final samplesize N mod < , case N . sample drawn, pairedrandom number seed chosen uniformly random set possible seedsadded list hinstance, seedi pairs.5.1.3 C OMPARISON C ONFIGURATION P ROCEDURESSince choice instances (and degree seeds) important final outcomeoptimization, experimental evaluations always performed number independentruns configuration procedure (typically 25). created separate list instances seedsrun explained above, kth run configuration procedure useskth list instances seeds. (Note, however, disjoint test set used measure performanceparameter configurations identical runs.)281fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEConfiguration scenarioP -SWGCPP E R -SWGCPP -QCPP E R -QCPC P L E X -R E G N 100Type benchmark instances & citationGraph colouring (Gent, Hoos, Prosser & Walsh, 1999)Graph colouring (Gent, Hoos, Prosser & Walsh, 1999)Quasigroup completion (Gomes & Selman, 1997)Quasigroup completion (Gomes & Selman, 1997)Combinatorial Auctions (CATS) (Leyton-Brown, Pearson & Shoham, 2000)Table 1: Overview five B R configuration scenarios.AlgorithmAPSPEARC PLEXParameter typeContinuousCategoricalIntegerContinuousCategoricalIntegerContinuous# parameters type4104125085# values considered72205836275735Total # configurations, ||2 4018.34 10171.38 1037Table 2: Parameter overview algorithms consider. information parameters algorithm given text.detailed list parameters values considered found online appendixhttp://www.cs.ubc.ca/labs/beta/Projects/ParamILS/algorithms.html.performed paired statistical test compare final results obtained runs twoconfiguration procedures. paired test required since kth run procedures sharedkth list instances seeds. particular, performed two-sided paired Max-Wilcoxontest null hypothesis difference performances, considering p-values0.05 statistically significant. p-values reported tables derived usingtest; p-values shown parentheses refer cases procedure expected perform betteractually performed worse.5.2 Configuration ScenariosSection 6, analyze configurators based five configuration scenarios, combininghigh-performance algorithm widely-studied benchmark dataset. Table 1 gives overviewthese, dub B R scenarios. algorithms benchmark instance sets usedscenarios described detail Sections 5.2.1 5.2.2, respectively. fiveB R configuration scenarios, set fairly aggressive cutoff times five seconds per runtarget algorithm allowed configuration procedure execute target algorithmaggregate runtime five CPU hours. short cutoff times fairly short times algorithmconfiguration deliberately chosen facilitate many configuration runs B R scenario. contrast, second set configuration scenarios (exclusively focusing C PLEX),set much larger cutoff times allowed time configuration. defer descriptionscenarios Section 7.5.2.1 TARGET LGORITHMSthree target algorithms listed Table 2 along configurable parameters.282fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKAPS first target algorithm used experiments APS, high-performance dynamiclocal search algorithm SAT solving (Hutter, Tompkins & Hoos, 2002) implemented UBCSAT (Tompkins & Hoos, 2004). introduced 2002, APS state-of-the-art solver,still performs competitively many instances. chose study algorithmwell known, relatively parameters, intimately familiar it. APSs fourcontinuous parameters control scaling smoothing clause weights, well probability random walk steps. original default parameters set manually based experimentsprominent benchmark instances; manual experimentation kept percentage randomsteps fixed took one week development time. subsequently gainedexperience APSs parameters general problem classes (Hutter, Hamadi, Hoos &Leyton-Brown, 2006), chose promising intervals parameter, including, centeredat, original default. picked seven possible values parameter spread uniformlyacross respective interval, resulting 2401 possible parameter configurations (these exactlyvalues used Hutter et al., 2007). starting configuration ParamILS, usedcenter point parameters domain.PEAR second target algorithm considered PEAR, recent tree search algorithmsolving SAT problems. PEAR state-of-the-art SAT solver industrial instances,appropriate parameter settings best available solver certain types hardwaresoftware verification instances (Hutter, Babic, Hoos & Hu, 2007). Furthermore, configuredParamILS, PEAR quantifier-free bit-vector arithmetic category 2007 SatisfiabilityModulo Theories Competition. PEAR 26 parameters, including ten categorical, four integer,twelve continuous parameters, default values manually engineered developer. (Manual tuning required one week.) categorical parameters mainly controlheuristics variable value selection, clause sorting, resolution ordering, enable disableoptimizations, pure literal rule. continuous integer parameters mainly dealactivity, decay, elimination variables clauses, well interval randomizedrestarts percentage random choices. discretized integer continuous parameterschoosing lower upper bounds reasonable values allowing three eightdiscrete values spread relatively uniformly across resulting interval, including default,served starting configuration ParamILS. number discrete values chosen according intuition importance parameter. discretization,3.7 1018 possible parameter configurations. Exploiting fact nine parametersconditional (i.e., relevant parameters take certain values) reduced 8.34 1017configurations.C PLEX third target algorithm used commercial optimization tool C PLEX 10.1.1,massively parameterized algorithm solving mixed integer programming (MIP) problems.159 user-specifiable parameters, identified 81 parameters affect C PLEXs search trajectory. careful omit parameters change problem formulation (e.g., changingnumerical accuracy solution). Many C PLEX parameters deal MIP strategy heuristics (such variable branching heuristics, probing, dive type subalgorithms)amount type preprocessing performed. also nine parameters governingfrequently different type cut used (those parameters four allowablemagnitude values value choose automatically; note last value prevents parameters ordinal). considerable number parameters deal simplex283fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEbarrier optimization, various algorithm components. categorical parametersautomatic option, considered categorical values well automatic one. contrast, continuous integer parameters automatic option, chose option insteadhypothesizing values might work well. also identified numerical parametersprimarily deal numerical issues, fixed default values. numericalparameters, chose five possible values seemed sensible, including default.many categorical parameters automatic option, included automatic option choiceparameter, also included manual options. Finally, ended 63 configurable parameters, leading 1.78 1038 possible configurations. Exploiting fact sevenC PLEX parameters relevant conditional parameters taking certain values,reduced 1.38 1037 distinct configurations. starting configuration configurationprocedures, used default settings, obtained careful manual configurationbroad range MIP instances.5.2.2 B ENCHMARK NSTANCESapplied target algorithms three sets benchmark instances: SAT-encoded quasi-groupcompletion problems, SAT-encoded graph-colouring problems based small world graphs,MIP-encoded winner determination problems combinatorial auctions. set consisted2000 instances, partitioned evenly training test sets.QCP first benchmark set contained 23 000 instances quasi-group completion problem (QCP), widely studied AI researchers. generated QCP instancesaround solubility phase transition, using parameters given Gomes Selman (1997).Specifically, order n drawn uniformly interval [26, 43], number holesH (open entries Latin square) drawn uniformly [1.75, 2.3] n1.55 . resultingQCP instances converted SAT CNF format. use complete solver, PEAR,sampled 2000 SAT instances uniformly random. average 1497 variables (standard deviation: 1094) 13 331 clauses (standard deviation: 12 473), 1182satisfiable. use incomplete solver, APS, randomly sampled 2000 instancessubset satisfiable instances (determined using complete algorithm); numbervariables clauses similar used PEAR.SW-GCP second benchmark set contained 20 000 instances graph colouring problem(GCP) based small world (SW) graphs Gent et al. (1999). these, sampled 2000instances uniformly random use PEAR; average 1813 variables (standarddeviation: 703) 13 902 clauses (standard deviation: 5393), 1109 satisfiable.use APS, randomly sampled 2000 satisfiable instances (again, determined usingcomplete SAT algorithm), whose number variables clauses similar usedPEAR.Regions100 third benchmark set generated 2000 instances combinatorial auctionwinner determination problem, encoded mixed-integer linear programs (MILPs). usedregions generator Combinatorial Auction Test Suite (Leyton-Brown et al., 2000),goods parameter set 100 bids parameter set 500. resulting MILP instancescontained 501 variables 193 inequalities average, standard deviation 1.7 variables2.5 inequalities.284fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKScenarioP -SWGCPP E R -SWGCPP -QCPP E R -QCPC P L E X -R E G N 100Default20.419.7412.972.651.61Test performance (penalized average runtime, CPU seconds)mean stddev. 10 runsRun best training performanceBasicILSFocusedILSBasicILSFocusedILS0.32 0.06 0.32 0.050.260.268.05 0.98.3 1.16.86.64.86 0.56 4.70 0.394.854.291.39 0.331.29 0.21.161.210.5 0.30.35 0.040.350.32Fig.2(a)2(b)2(c)2(d)2(e)Table 3: Performance comparison default parameter configuration configurations foundBasicILS FocusedILS (both Aggr Capping bm = 2). configuration scenario, list test performance (penalized average runtime 1000 test instances, CPU seconds) algorithm default, mean stddev test performance across25 runs BasicILS(100) & FocusedILS (run five CPU hours each), test performance run BasicILS FocusedILS best terms training performance. Boldface indicates better BasicILS FocusedILS. algorithm configurations found FocusedILSs run best training performance listed online appendix http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html. Column Fig. gives reference scatter plot comparing performance configurationsalgorithm defaults.5.3 Experimental Setupcarried experiments cluster 55 dual 3.2GHz Intel Xeon PCs 2MBcache 2GB RAM, running OpenSuSE Linux 10.1. measured runtimes CPU timereference machines. configuration procedures implemented Ruby scripts,include runtime scripts configuration time. easy configurationscenarios, algorithm runs finish milliseconds, overhead scriptssubstantial. Indeed, longest configuration run observed took 24 hours execute five hoursworth target algorithm runtime. contrast, harder C PLEX scenarios described Section7 observed virtually overhead.6. Empirical Evaluation BasicILS, FocusedILS Adaptive Cappingsection, use B R scenarios empirically study performance BasicILS(N )FocusedILS, well effect adaptive capping. first demonstrate large speedupsParamILS achieved default parameters study components responsiblesuccess.6.1 Empirical Comparison Default Optimized Parameter Configurationssection, five B R configuration scenarios, compare performancerespective algorithms default parameter configuration final configurations foundBasicILS(100) FocusedILS. Table 3 especially Figure 2 show configurators ledsubstantial speedups.Table 3, report final performance achieved 25 independent runs configurator. independent configuration run, used different set training instances seeds(constructed described Section 5.1.2). note often rather large varianceperformances found different runs configurators, configuration found285fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE4410Runtime [s], autotunedRuntime [s], autotuned103102101100101102102101100101102102110010110102103410210Runtime [s], default10(a) P -SWGCP.531s vs 0.15s; 499 vs timeouts4110010121011001011021010110010110210310Runtime [s], default(c) P -QCP.72s vs 0.17s; 149 vs 1 timeouts4104104310210110010110210231010Runtime [s], autotunedRuntime [s], autotuned310(b) P E R -SWGCP.33s vs 17s; 3 vs 2 timeouts1010210Runtime [s], default410Runtime [s], autotuned31031021011001011021021011001011021010Runtime [s], default(d) P E R -QCP.9.6s vs 0.85s; 1 vs 0 timeouts3410210110010110210310Runtime [s], default410(e) C P L E X -R E G N 100.1.61s vs 0.32s; timeoutsFigure 2: Comparison default vs automatically-determined parameter configurations five B Rconfiguration scenarios. dot represents one test instance; timeouts (after one CPU hour)denoted circles. dashed line five CPU seconds indicates cutoff time targetalgorithm used configuration process. subfigure captions give mean runtimesinstances solved configurations (default vs optimized), well numbertimeouts each.run best training performance also tended yield better test performanceothers. reason, used configuration result algorithm configuration. (Notechoosing configuration found run best training set performance perfectlylegitimate procedure since require knowledge test set. course, improvements thus achieved come price increased overall running time, independent runsconfigurator easily performed parallel.)Figure 2, compare performance automatically-found parameter configurationdefault configuration, runs allowed last hour. speedupsobvious figure Table 3, since penalized average runtime table countsruntimes larger five seconds fifty seconds (ten times cutoff five seconds), whereasdata figure uses much larger cutoff time. larger speedups apparent scenariosP -SWGCP, P -QCP, P E R -QCP: corresponding speedup factors mean runtime3540, 416 11, respectively (see Figure 2).286fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKAlgorithm 5: RandomSearch(N, 0 )Outline random search parameter configuration space; inc denotes incumbent parameterconfiguration, betterN compares two configurations based first N instances trainingset.Input : Number runs use evaluating parameter configurations, N ; initial configuration0 .Output : Best parameter configuration inc found.1 inc 0 ;2 TerminationCriterion()3random ;4betterN (, inc )5inc ;6return inc6.2 Empirical Comparison BasicILS Simple Baselinessection, evaluate effectiveness BasicILS(N ) two components:simple random search, used BasicILS initialization (we dub RandomSearch(N )provide pseudocode Algorithm 5);simple local search, type iterative first improvement search used BasicILS(N )(we dub SimpleLS(N )).evaluate one component time, section Section 6.3 study algorithmswithout adaptive capping. investigate effect adaptive capping methods Section6.4.sufficient structure search space, expect BasicILS outperform RandomSearch. local minima, expect BasicILS perform better simple local search.experiments showed BasicILS indeed offer best performance.Here, solely interested comparing effectively approaches search spaceparameter configurations (and found parameter configurations generalize unseentest instances). Thus, order reduce variance comparisons, compare configurationmethods terms performance training set.Table 4, compare BasicILS RandomSearch B R configuration scenarios.average, BasicILS always performed better, three five scenarios, differencestatistically significant judged paired Max-Wilcoxon test (see Section 5.1.3). Table 4 alsolists performance default parameter configuration scenarios. noteBasicILS RandomSearch consistently achieved substantial (and statistically significant)improvements default configurations.Next, compared BasicILS second component, SimpleLS. basic local searchidentical BasicILS, stops first local minimum encountered. used orderstudy whether local minima pose problem simple first improvement search. Table 5 showsthree configuration scenarios BasicILS time perform multiple ILS iterations,training set performance statistically significantly better SimpleLS. Thus,conclude search space contains structure exploited local search algorithmwell local minima limit performance iterative improvement search.287fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEScenarioP -SWGCPP E R -SWGCPP -QCPP E R -QCPC P L E X -R E G N 100Training performance (penalized average runtime, CPU seconds)Default RandomSearch(100)BasicILS(100)19.930.46 0.340.38 0.1910.617.02 1.116.78 1.7312.713.96 1.1853.19 1.192.770.58 0.590.36 0.391.611.45 0.350.72 0.45p-value0.940.181.4 1050.0071.2 105Table 4: Comparison RandomSearch(100) BasicILS(100), without adaptive capping. tableshows training performance (penalized average runtime N = 100 training instances, CPUseconds). Note approaches yielded substantially better results default configuration, BasicILS performed statistically significantly better RandomSearch threefive B R configuration scenarios judged paired Max-Wilcoxon test (see Section5.1.3).ScenarioP -SWGCPP -QCPP E R -QCPSimpleLS(100)Performance0.5 0.393.60 1.390.4 0.39BasicILS(100)PerformanceAvg. # ILS iterations0.38 0.192.63.19 1.195.60.36 0.391.64p-value9.8 1044.4 1040.008Table 5: Comparison SimpleLS(100) BasicILS(100), without adaptive capping. table showstraining performance (penalized average runtime N = 100 training instances, CPU seconds). configuration scenarios P E R -SWGCP C P L E X -R E G N 100, BasicILS complete first ILS iteration 25 runs; two approaches thus identicallisted here. configuration scenarios, BasicILS found significantly better configurationsSimpleLS.6.3 Empirical Comparison FocusedILS BasicILSsection investigate FocusedILSs performance experimentally. contrast previous comparison RandomSearch, SimpleLS, BasicILS using training performance,compare FocusedILS BasicILS using test performance. becausein contrast BasicILS SimpleLSFocusedILS grows number target algorithm runs used evaluateparameter configuration time. Even different runs FocusedILS (using different training setsrandom seeds) use number target algorithm runs evaluate parameter configurations. However, eventually aim optimize cost statistic, c, thereforetest set performance (an unbiased estimator c) provides fairer basis comparison training performance. compare FocusedILS BasicILS, since BasicILS already outperformedRandomSearch SimpleLS Section 6.2.Figure 3 compares test performance FocusedILS BasicILS(N ) N = 1, 10100. Using single target algorithm run evaluate parameter configuration, BasicILS(1)fast, generalize well test set all. example, configuration scenarioP -SWGCP, BasicILS(1) selected parameter configuration whose test performance turnedeven worse default. hand, using large number target algorithm runsevaluation resulted slow search, eventually led parameter configurationsgood test performance. FocusedILS aims achieve fast search good generalization testset. configuration scenarios Figure 3, FocusedILS started quickly also led bestfinal performance.288fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK22Mean runtime [s], testMean runtime [s], test10110010110BasicILS(1)BasicILS(10)BasicILS(100)FocusedILS010210.504101.510BasicILS(1)BasicILS(10)BasicILS(100)FocusedILS010CPU time used tuner [s]241010CPU time used tuner [s](a) P -SWGCP(b) C P L E X -R E G N 100Figure 3: Comparison BasicILS(N ) N = 1, 10, 100 vs FocusedILS, without adaptivecapping. show median test performance (penalized average runtime across 1 000 testinstances) across 25 runs configurators two scenarios. Performance threeB R scenarios qualitatively similar: BasicILS(1) fastest move awaystarting parameter configuration, performance robust all; BasicILS(10)rather good compromise speed generalization performance, given enough timeoutperformed BasicILS(100). FocusedILS started finding good configurations quickly(except scenario P E R -QCP, took even longer BasicILS(100) improvedefault) always amongst best approaches end configuration process.ScenarioP -SWGCPP E R -SWGCPP -QCPP E R -QCPC P L E X -R E G N 100Test performance (penalized average runtime, CPU seconds)Default BasicILS(100)FocusedILS20.410.59 0.280.32 0.089.748.13 0.958.40 0.9212.974.87 0.344.69 0.402.651.32 0.341.35 0.201.610.72 0.450.33 0.03p-value1.4 104(0.21)0.042(0.66)1.2 105Table 6: Comparison BasicILS(100) FocusedILS, without adaptive capping. table showstest performance (penalized average runtime 1 000 test instances, CPU seconds).configuration scenario, report test performance default parameter configuration, meanstddev test performance reached 25 runs BasicILS(100) FocusedILS, pvalue paired Max-Wilcoxon test (see Section 5.1.3) difference two configuratorsperformance.compare performance FocusedILS BasicILS(100) configuration scenarios Table 6. three APS C PLEX scenarios, FocusedILS performed statistically significantly better BasicILS(100). results consistent past work FocusedILS achieved statistically significantly better performance BasicILS(100) (Hutter et al.,2007). However, found configuration scenarios involving PEAR algorithm,BasicILS(100) actually performed better average FocusedILS, albeit statistically significantly. attribute fact complete, industrial solver PEAR, twobenchmark distributions QCP SWGCP quite heterogeneous. expect FocusedILSproblems dealing highly heterogeneous distributions, due fact frequently triesextrapolate performance based runs per parameter configuration.289fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE22.5BasicILS, cappingBasicILS, TP cappingMean runtime [s], trainMean runtime [s], train10110010110231010BasicILS, cappingBasicILS, TP capping21.510.50 210410CPU time used tuner [s]310410CPU time used tuner [s](a) P -SWGCP, significant.(b) C P L E X -R E G N 100, significant.Figure 4: Speedup BasicILS adaptive capping two configuration scenarios. performed 25runs BasicILS(100) without adaptive capping TP capping. time step,computed training performance run configurator (penalized average runtimeN = 100 training instances) plot median 25 runs.ScenarioP -SWGCPP E R -SWGCPP -QCPP E R -QCPC P L E X -R E G N 100Training performance (penalized average runtime)cappingTP cappingp-value0.38 0.190.24 0.056.1 1056.78 1.736.65 1.480.013.19 1.192.96 1.139.8 1040.361 0.39 0.356 0.440.660.67 0.350.47 0.267.3 104Avg. # ILS iterationscapping TP capping312116102311Table 7: Effect adaptive capping BasicILS(100). show training performance (penalized average runtime N = 100 training instances, CPU seconds). configuration scenario,report mean stddev final training performance reached 25 runs configurator without capping TP capping, p-value paired Max-Wilcoxon test difference(see Section 5.1.3), well average number ILS iterations performed respectiveconfigurator.6.4 Empirical Evaluation Adaptive Capping BasicILS FocusedILSpresent experimental evidence use adaptive capping strong impactperformance BasicILS FocusedILS.Figure 4 illustrates extent TP capping sped BasicILS two configuration scenarios. cases, capping helped improve training performance substantially; P -SWGCP,BasicILS found solutions order magnitude faster without capping.Table 7 quantifies speedups five B R configuration scenarios. TP capping enabledfour times many ILS iterations (in P -SWGCP) improved average performancescenarios. improvement statistically significant scenarios, except P E R -QCP.Aggressive capping improved BasicILS performance one scenario. scenarioP -SWGCP, increased number ILS iterations completed within configuration time12 219, leading significant improvement performance. first ILS iterationBasicILS, capping techniques identical (the best configuration iteration alwaysincumbent). Thus, observe difference configuration scenarios P E R -SWGCPC P L E X -R E G N 100, none 25 runs configurator finished first ILS iteration.remaining two configuration scenarios, differences insignificant.290fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKScenarioP -SWGCPP E R -SWGCPP -QCPP E R -QCPC P L E X -R E G N 100Number ILS iterations performedcappingTP cappingp-value121 12166 151.2 10537 1243 150.0026142 18143 220.54153 49165 410.0336 1340 160.26Aggr capping244 1947 18156 28213 6254 15p-value1.2 1059 1050.0161.2 1051.8 105Number runs performed incumbent parameter configurationScenariocappingTP cappingp-valueAggr cappingp-valueP -SWGCP993 2111258 262 4.7 104 1818 243 1.2 105503 265476 238(0.58)642 2880.009P E R -SWGCPP -QCP1575 385 1701 3180.0651732 3400.084P E R -QCP836 5091130 5570.021215 5010.003761 215795 1840.40866 2320.07C P L E X -R E G N 100Table 8: Effect adaptive capping search progress FocusedILS, measured number ILSiterations performed number runs performed incumbent parameter configuration.configuration scenario, report mean stddev measures across 25runs configurator without capping, TP capping, Aggr capping, wellp-values paired Max-Wilcoxon tests (see Section 5.1.3) differences cappingTP capping; capping Aggr capping.evaluate usefulness capping FocusedILS. Training performance usefulquantity context comparing different versions FocusedILS, since number targetalgorithm runs measure based varies widely runs configurator. Instead,used two measures quantify search progress: number ILS iterations performednumber target algorithm runs performed incumbent parameter configuration. Table 8shows two measures five B R configuration scenarios three capping schemes(none, TP, Aggr). FocusedILS TP capping achieved higher values without cappingscenarios measures (although differences statistically significant).Aggressive capping increased measures scenarios, differencescapping aggressive capping statistically significant. Figure 5 demonstratestwo configuration scenarios FocusedILS capping reached solution qualitiesquickly without capping. However, finding respective configurations, FocusedILSshowed significant improvement.Recall experiments Section 6.2 6.3 compared various configurators without adaptive capping. One might wonder comparisons change presence adaptivecapping. Indeed, adaptive capping also worked box RandomSearch enabledevaluate 3.4 33 times many configurations without capping. improvementsignificantly improved simple algorithm RandomSearch point average performance came within 1% one BasicILS two domains (S P -SWGCP P E R -SWGCP;compare much larger differences without capping reported Table 4). P E R -QCP,still 25% difference average performance, result significant. Finally,P -QCP C P L E X -R E G N 100 difference still substantial significant (22% 55%difference average performance, p-values 5.2 105 0.0013, respectively).Adaptive capping also reduced gap BasicILS FocusedILS. particular,P -SWGCP, where, even without adaptive capping, FocusedILS achieved best performanceencountered scenario, BasicILS caught using adaptive capping. Similarly,291fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE22FocusedILS, cappingFocusedILS, TP cappingFocusedILS, Aggr cappingMean runtime [s], testMean runtime [s], test10110010110210310410FocusedILS, cappingFocusedILS, TPcappingFocusedILS, Aggr capping1.510.50 110510CPU time used tuner [s]210310410510CPU time used tuner [s](a) P -SWGCP(b) C P L E X -R E G N 100Figure 5: Speedup FocusedILS adaptive capping two configuration scenarios. performed 25runs FocusedILS without adaptive capping, TP capping Aggr capping.time step, computed test performance run configurator (penalized averageruntime 1000 test instances) plot median 25 runs. differencesend trajectory statistically significant. However, capping time requiredachieve quality lower two configuration scenarios. three scenarios,gains due capping smaller.C P L E X -R E G N 100, FocusedILS already performed well without adaptive cappingBasicILS not. Here, BasicILS improved based adaptive capping, still could rivalFocusedILS. scenarios, adaptive capping affect relative performance much;compare Tables 6 (without capping) 3 (with capping) details.7. Case Study: Configuring C PLEX Real-World Benchmarkssection, demonstrate ParamILS improve performance commercial optimization tool C PLEX variety interesting benchmark distributions. best knowledge,first published study automatically configuring C PLEX.use five C PLEX configuration scenarios. these, collected wide range MIP benchmarks public benchmark libraries researchers, split 50:50disjoint training test sets; detail following.Regions200 set almost identical Regions100 set (described Section 5.2.2used throughout paper), instances much larger. generated 2 000 MILPinstances generator provided Combinatorial Auction Test Suite (LeytonBrown et al., 2000), based regions option goods parameter set 200bids parameter set 1 000. instances contain average 1 002 variables 385inequalities, respective standard deviations 1.7 3.4.MJA set comprises 343 machine-job assignment instances encoded mixed integerquadratically constrained programs (MIQCP). obtained Berkeley Computational Optimization Lab5 introduced Akturk, Atamturk S. Gurel (2007).instances contain average 2 769 variables 2 255 constraints, respectivestandard deviations 2 133 1 592.5. http://www.ieor.berkeley.edu/atamturk/bcol/, set called conic.sch292fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKCLS set comprises 100 capacitated lot-sizing instances encoded mixed integer linearprograms (MILP). also obtained Berkeley Computational Optimization Labintroduced Atamturk Munoz (2004). 100 instances contain 181 variables180 constraints.MIK set 120 MILP-encoded mixed-integer knapsack instances also obtainedBerkeley Computational Optimization Lab originally introduced Atamturk(2003). instances contain average 384 variables 151 constraints, respective standard deviations 309 127.QP set quadratic programs originated RNA energy parameter optimization (Andronescu, Condon, Hoos, Mathews & Murphy, 2007). Mirela Andronescu generated 2 000 instances experiments. instances contain 9 3667 165 variables 9 1917 186constraints. Since instances polynomial-time solvable quadratic programs, setlarge number inconsequential C PLEX parameters concerning branch cut mechanism default values, ending 27 categorical, 2 integer 2 continuous parameters configured, discretized parameter configuration space size 3.27 1017 .study ParamILSs behavior harder problems, set significantly longer cutoff timesC PLEX scenarios B R scenarios previous section. Specifically,used cutoff time 300 CPU seconds run target algorithm training,allotted two CPU days every run configurators. always, configurationobjective minimize penalized average runtime penalization constant 10.Table 9, compare performance C PLEXs default parameter configurationfinal parameter configurations found BasicILS(100) FocusedILS (both aggressive capping bm = 2). Note that, similar situation described Section 6.1, configurationscenarios (e.g., C P L E X -CLS, C P L E X -MIK) substantial variance different runsconfigurators, run best training performance yielded parameter configurationalso good test set. BasicILS outperformed FocusedILS 3 5scenarios terms mean test performance across ten runs, FocusedILS achieved better testperformance run best training performance one scenario (in performed almost well). scenarios C P L E X -R E G N 200 C P L E X -CLS, FocusedILS performedsubstantially better BasicILS.Note C PLEX configuration scenarios considered, BasicILS FocusedILSfound parameter configurations better algorithm defaults, sometimesorder magnitude. particularly noteworthy since ILOG expended substantial effortdetermine strong default C PLEX parameters. Figure 6, provide scatter plots five scenarios. C P L E X -R E G N 200, C P L E X - C N C . C H , C P L E X -CLS, C P L E X -MIK, speedups quiteconsistent across instances (with average speedup factors reaching 2 C P L E X - C N C . C H23 C P L E X -MIK). Finally, C P L E X -QP see interesting failure mode ParamILS.optimized parameter configuration achieved good performance cutoff time used6. configuration scenario C P L E X -MIK, nine ten runs FocusedILS yielded parameter configurationsaverage runtimes smaller two seconds. One run, however, demonstrated interesting failure mode FocusedILS aggressive capping. Capping aggressively caused every C PLEX run unsuccessful,FocusedILS selected configuration manage solve single instance test set. Counting unsuccessful runs ten times cutoff time, resulted average runtime 10 300 = 3000 seconds run.(For full details, see Section 8.1 Hutter, 2009).293fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEScenarioC P L E X -R E G N 200CP L E X-C N C.S C HC P L E X -CLSC P L E X -MIKC P L E X -QPTest performance (penalized average runtime, CPU seconds)mean stddev. 10 runsRun best training performanceDefaultBasicILSFocusedILS BasicILSFocusedILS7245 2411.4 0.91510.55.372.27 0.112.4 0.292.142.35712443 294327 8608023.464.820 27301 948 61.721.19969755 214827 306528525Fig.6(a)6(b)6(c)6(d)6(e)Table 9: Experimental results C PLEX configuration scenarios.configuration scenario, list test performance (penalized average runtime test instances) algorithm default, mean stddev test performance across ten runs BasicILS(100)& FocusedILS (run two CPU days each), test performance runBasicILS FocusedILS best terms training performance. Boldface indicates better BasicILS FocusedILS. algorithm configurations foundFocusedILSs run best training performance listed online appendixhttp://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html.ColumnFig. gives reference scatter plot comparing performance configurationsalgorithm defaults.configuration process (300 CPU seconds, see Figure 6(f)), performance carryhigher cutoff time used tests (3600 CPU seconds, see Figure 6(e)). Thus, parameter configuration found FocusedILS generalize well previously unseen test data,larger cutoff times.8. Review ParamILS Applicationssection, review number applications ParamILSsome dating backearlier stages development, others recentthat demonstrate utility versatility.8.1 Configuration SAPS, GLS+ SAT4JHutter et al. (2007), first publication ParamILS, reported experiments three target algorithms demonstrate effectiveness approach: SAT algorithm SAPS (which4 numerical parameters), local search algorithm GLS+ solving probable explanation (MPE) problem Bayesian networks (which 5 numerical parameters; Hutter, Hoos &Stutzle, 2005), tree search SAT solver SAT4J (which 4 categorical 7 numericalparameters; http://www.sat4j.org). compared respective algorithms default performance,performance CALIBRA system (Adenso-Diaz & Laguna, 2006), performanceBasicILS FocusedILS. four configuration scenarios studied, FocusedILS significantly outperformed CALIBRA two performed better average third. fourthone (configuring SAT4J), CALIBRA applicable due categorical parameters,FocusedILS significantly outperformed BasicILS.Overall, automated parameter optimization using ParamILS achieved substantial improvementsprevious default settings: GLS+ sped factor > 360 (tuned parameters foundsolutions better quality 10 seconds default found one hour), SAPS factors 8130 SAPS-QWH SAPS-SW, respectively, SAT4J factor 11.294fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK44210110010110210Runtime [s], autotuned31010310210110010110210210110010110210310Runtime [s], default4101100101102103410Runtime [s], default1100101102101011001011021010Runtime [s], default(d) C P L E X -MIK.28s vs 1.2s; timeouts34102110010110210310Runtime [s], default410(c) C P L E X -CLS.309s vs 21.5s; timeouts410310210110010110231021011001011021021102Runtime [s], autotunedRuntime [s], autotuned21001010431101010102104(b) C P L E X - C N C . C H .5.37s vs 2.39.5s; timeouts1031010210(a) C P L E X -R E G N 200.72s vs 10.5s; timeoutsRuntime [s], autotuned410Runtime [s], autotunedRuntime [s], autotuned1010210110010110210310Runtime [s], default(e) C P L E X -QP.296s vs 234s; 0 vs 21 timeouts421010110010110210310Runtime [s], default410(f) C P L E X -QP, test cutoff 300seconds.81s vs 44s; 305 vs 150 timeoutsFigure 6: Comparison default vs automatically-determined parameter configuration five C PLEXconfiguration scenarios. dot represents one test instance; time-outs (after one CPU hour)denoted red circles. blue dashed line 300 CPU seconds indicates cutoff timetarget algorithm used configuration process. subfigure captions give meanruntimes instances solved configurations (default vs optimized), wellnumber timeouts each.8.2 Configuration Spear Industrial Verification ProblemsHutter et al. (2007) applied ParamILS specific real-world application domain: configuring26 parameters tree-search DPLL solver PEAR minimize mean runtime setpractical verification instances. particular, considered two sets industrial probleminstances, bounded model-checking (BMC) instances Zarpas (2005) software verification(SWV) instances generated C ALYSTO static checker (Babic & Hu, 2007).instances problem distributions exhibited large spread hardness PEAR.SWV instances, default configuration solved many instances milliseconds failedsolve others days. despite fact PEAR specifically developed typeinstances, developer generated problem instances (and thus intimatedomain knowledge), week manual performance tuning expended orderoptimize solvers performance.PEAR first configured good general performance industrial SAT instancesprevious SAT competitions. already led substantial improvements default perfor295fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEmance 2007 SAT competition.7 PEAR default solved 82 instances ranked 17thfirst round competition, automatically configured version solved 93 instancesranked 8th, optimized version solved 99 instances, ranking 5th (above MiniSAT).speedup factors due general optimization 20 1.3 SWV BMC datasets,respectively.Optimizing specific instance sets yielded further, much larger improvements (a factor500 SWV 4.5 BMC). encouragingly, best parameter configuration foundsoftware verification instances take longer 20 seconds solve SWVproblem instances (compared multiple timeouts CPU day original default values).Key good performance application perform multiple independent runs FocusedILS, select found configuration best training performance (as also doneSections 6.1 7 article).8.3 Configuration SATensteinKhudaBukhsh, Xu, Hoos Leyton-Brown (2009 ) used ParamILS perform automatic algorithmdesign context stochastic local search algorithms SAT. Specifically, introducednew framework local search SAT solvers, called SATenstein, used ParamILS choosegood instantiations framework given instance distributions. SATenstein spans three broadcategories SLS-based SAT solvers: WalkSAT-based algorithms, dynamic local search algorithmsG2 WSAT variants. combined highly parameterized framework solvertotal 41 parameters 4.82 1012 unique instantiations.FocusedILS used configure SATenstein six different problem distributions,resulting solvers compared eleven state-of-the-art SLS-based SAT solvers. resultsshowed automatically configured versions SATenstein outperformed elevenstate-of-the-art solvers six categories, sometimes large margin.SAT ENSTEIN work clearly demonstrated automated algorithm configuration methodsused construct new algorithms combining wide range components existing algorithms novel ways, thereby go beyond simple parameter tuning. Due lowlevel manual work required approach, believe automated design algorithmscomponents become mainstream technique development algorithms hardcombinatorial problems.Key successful application FocusedILS configuring SAT ENSTEIN carefulselection homogeneous instance distributions, instances could solved withincomparably low cutoff time 10 seconds per run. Again, configuration best trainingquality selected ten parallel independent runs FocusedILS per scenario.8.4 Self-Configuration ParamILSheuristic optimization procedure, ParamILS controlled number parameters:number random configurations, r, sampled beginning search; perturbationstrength, s; probability random restarts, prestart . Furthermore, aggressive cappingmechanism makes use additional parameter: bound multiplier, bm. Throughout article,used manually-determined default values hr, s, prestart , bmi = h10, 3, 0.01, 2i.7. See http://www.cril.univ-artois.fr/SAT07. PEAR allowed participate second roundcompetition since source code publicly available.296fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKrecent work (see Section 8.2 Hutter, 2009), evaluated whether FocusedILSs performance could improved using ParamILS automatically find better parameter configuration.self-configuration task, configuration scenarios play role instances, configurator optimized plays role target algorithm. avoid confusion, referconfigurator target configurator. Here, set fairly short configuration times one CPUhour target configurator. However, still significantly longer cutoff timesused experiments, parallelization turned crucial finish experiment reasonable amount time. BasicILS easier parallelizeFocusedILS, chose BasicILS(100) meta-configurator.Although notion algorithm configurator configure intriguing,case, turned yield small improvements. Average performance improved fourfive scenarios degraded remaining one. However, none differencesstatistically significant.8.5 Applications ParamILSThachuk, Shmygelska Hoos (2007 ) used BasicILS order determine performance-optimizingparameter settings new replica-exchange Monte Carlo algorithm protein folding 2DHP 3D-HP models.8 Even though algorithm four parameters (two categoricaltwo continuous), BasicILS achieved substantial performance improvements. manuallyselected configurations biased favour either short long protein sequences, BasicILSfound configuration consistently yielded good mean runtimes types sequences.average, speedup factor achieved approximately 1.5, certain classes proteinsequences 3. manually-selected configurations performed worse previousstate-of-the-art algorithm problem instances, robust parameter configurationsselected BasicILS yielded uniformly better performance.recent work, Fawcett, Hoos Chiarandini (2009) used several variants ParamILS(including version slightly extended beyond ones presented here) designmodular stochastic local search algorithm post-enrollment course timetabling problem.followed design approach used automated algorithm configuration order explorelarge design space modular highly parameterised stochastic local search algorithms.quickly led solver placed third Track 2 2nd International Timetabling Competition(ITC2007) subsequently produced improved solver shown achieve consistentlybetter performance top-ranked solver competition.9. Related WorkMany researchers us dissatisfied manual algorithm configuration, variousfields developed approaches automatic parameter tuning. start sectionclosely-related workapproaches employ direct search find good parameterconfigurationsand describe methods. Finally, discuss work related problems,finding best parameter configuration algorithm per-instance basis, approachesadapt parameters algorithms execution (see also Hoos, 2008, relatedwork automated algorithm design).8. BasicILS used, FocusedILS yet developed study conducted.297fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE9.1 Direct Search Methods Algorithm ConfigurationApproaches automated algorithm configuration go back early 1990s, numbersystems developed adaptive problem solving. One systems Composer (Gratch& Dejong, 1992), performs hill-climbing search configuration space, taking movesenough evidence gathered render neighbouring configuration statistically significantlybetter current configuration. Composer successfully applied improving fiveparameters algorithm scheduling communication collection ground-basedantennas spacecrafts (Gratch & Chien, 1996).Around time, MULTI-TAC system introduced Minton (1993, 1996). MULTITAC takes input generic heuristics, specific problem domain, distribution problem instances. adapts generic heuristics problem domain automatically generatesdomain-specific LISP programs implementing them. beam search used choose bestLISP program program evaluated running fixed set problem instancessampled given distribution.Another search-based approach uses fixed training set introduced Coy et al. (2001).approach works two stages. First, finds good parameter configuration instance Ii training set combination experimental design (full factorial fractionalfactorial) gradient descent. Next, combines parameter configurations 1 , . . . , N thus determined setting parameter average values taken them. Noteaveraging step restricts applicability method algorithms numerical parameters.similar approach, also based combination experimental design gradient descent,using fixed training set evaluation, implemented CALIBRA system Adenso-DiazLaguna (2006). CALIBRA starts evaluating parameter configuration full factorialdesign two values per parameter. iteratively homes good regions parameterconfiguration space employing fractional experimental designs evaluate nine configurationsaround best performing configuration found far. grid experimental designrefined iteration. local optimum found, search restarted (with coarsergrid). Experiments showed CALIBRAs ability find parameter settings six target algorithmsmatched outperformed respective originally-proposed parameter configurations. maindrawback limitation tuning numerical ordinal parameters, maximum fiveparameters. first introduced ParamILS, performed experiments comparing performance CALIBRA (Hutter et al., 2007). experiments reviewed Section 8.1.Terashima-Marn et al. (1999) introduced genetic algorithm configuring constraint satisfaction algorithm large-scale university exam scheduling. constructed configuredalgorithm works two stages seven configurable categorical parameters. optimized choices genetic algorithm 12 problem instances,found configuration improved performance modified Brelaz algorithm. However, noteperformed optimization separately instance. paper quantifylong optimizations took, stated Issues time delivering solutionsmethod still matter research.Work automated parameter tuning also found numerical optimization literature. particular, Audet Orban (2006) proposed mesh adaptive direct search algorithm.Designed purely continuous parameter configuration spaces, algorithm guaranteed converge local optimum cost function. Parameter configurations evaluated fixed298fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKset large unconstrained regular problems CUTEr collection, using optimization objectives runtime number function evaluations required solving given problem instance.Performance improvements around 25% classical configuration four continuous parameters interior point methods reported.Algorithm configuration stochastic optimization problem, exists large bodyalgorithms designed problems (see, e.g., Spall, 2003). However, many algorithmsstochastic optimization literature require explicit gradient information thus inapplicablealgorithm configuration. algorithms approximate gradient function evaluations(e.g., finite differences), provably converge local minimum cost functionmild conditions, continuity. Still, methods primarily designed dealnumerical parameters find local minima. aware applications generalpurpose algorithms stochastic optimization algorithm configuration.9.2 Methods Algorithm ConfigurationSequential parameter optimization (SPO) (Bartz-Beielstein, 2006) model-based parameter optimization approach based Design Analysis Computer Experiments (DACE; see, e.g.,Santner, Williams & Notz, 2003), prominent approach statistics blackbox function optimization. SPO starts running target algorithm parameter configurations Latinhypercube design number training instances. builds response surface model basedGaussian process regression uses models predictions predictive uncertainties determine next parameter configuration evaluate. metric underlying choice promisingparameter configurations expected improvement criterion used Jones, Schonlau Welch(1998). algorithm run, response surface refitted, new parameter configuration determined based updated model. contrast previously-mentioned methods,SPO use fixed training set. Instead, starts small training set doubles sizewhenever parameter configuration determined incumbent already incumbentprevious iteration. recent improved mechanism resulted robust version, SPO+ (Hutter, Hoos, Leyton-Brown & Murphy, 2009). main drawbacks SPO variants,fact entire DACE approach, limitation continuous parameters optimizingperformance single problem instances, well cubic runtime scaling number datapoints.Another approach based adaptations racing algorithms machine learning (Maron &Moore, 1994) algorithm configuration problem. Birattari et al. (2002; 2004) developed procedure dubbed F-Race used configure various stochastic local search algorithms. F-Racetakes input algorithm A, finite set algorithm configurations , instance distribution D. iteratively runs target algorithm surviving parameter configurationsnumber instances sampled (in simplest case, iteration runs surviving configurations one instance). configuration eliminated race soon enough statisticalevidence gathered it. iteration, non-parametric Friedman test used checkwhether significant differences among configurations. case, inferiorconfigurations eliminated using series pairwise tests. process iteratedone configuration survives given cutoff time reached. Various applications F-Racedemonstrated good performance (for overview, see Birattari, 2004). However, sincestart procedure candidate configurations evaluated, approach limited situationsspace candidate configurations practically enumerated. fact, published ex299fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEperiments F-Race limited applications around 1200 configurations.recent extension presented Balaprakash et al. (2007) iteratively performs F-Race subsetsparameter configurations. approach scales better large configuration spaces, versiondescribed Balaprakash et al. (2007) handles algorithms numerical parameters.9.3 Related Algorithm Configuration Problemspoint, focused problem finding best algorithm configurationentire set (or distribution) problem instances. Related approaches attempt find bestconfiguration algorithm per-instance basis, adapt algorithm parametersexecution algorithm. Approaches setting parameters per-instance basisdescribed Patterson Kautz (2001), Cavazos OBoyle (2006), Hutter et al. (2006).Furthermore, approaches attempt select best algorithm per-instance basisstudied Leyton-Brown, Nudelman Shoham (2002), Carchrae Beck (2005), Gebruers,Hnich, Bridge Freuder (2005), Gagliolo Schmidhuber (2006), Xu, Hutter, HoosLeyton-Brown (2008). related work, decisions restart algorithm madeonline, run algorithm (Horvitz, Ruan, Gomes, Kautz, Selman & Chickering, 2001;Kautz, Horvitz, Ruan, Gomes & Selman, 2002; Gagliolo & Schmidhuber, 2007). So-called reactivesearch methods perform online parameter modifications (Battiti, Brunato & Mascia, 2008).last strategy seen complementary work: even reactive search methods tendparameters remain fixed search hence configured using offline approachesParamILS.9.4 Relation Local Search MethodsSince ParamILS performs iterated local search one-exchange neighbourhood,similar spirit local search methods problems, SAT (Selman, Levesque &Mitchell, 1992; Hoos & Stutzle, 2005), CSP (Minton, Johnston, Philips & Laird, 1992), MPE(Kask & Dechter, 1999; Hutter et al., 2005). Since ParamILS local search method, existingtheoretical frameworks (see, e.g., Hoos, 2002; Mengshoel, 2008), could principle usedanalysis. main factor distinguishing problem ones faced standard localsearch algorithms stochastic nature optimization problem (for discussion localsearch stochastic optimization, see, e.g., Spall, 2003). Furthermore, exists compactrepresentation objective function could used guide search. illustrate this,consider local search SAT, candidate variables flipped limitedoccurring currently-unsatisfied clauses. general algorithm configuration, hand,mechanism cannot used, information available target algorithmperformance runs executed far. While, obviously, (stochastic) local searchmethods could used basis algorithm configuration procedures, chose iterated localsearch, mainly conceptual simplicity flexibility.10. Discussion, Conclusions Future workwork, studied problem automatically configuring parameters complex,heuristic algorithms order optimize performance given set benchmark instances.extended earlier algorithm configuration procedure, ParamILS, new capping mechanism300fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKobtained excellent results applying resulting enhanced version ParamILS twohigh-performance SAT algorithms well C PLEX wide range benchmark sets.Compared carefully-chosen default configurations target algorithms, parameter configurations found ParamILS almost always performed much better evaluatedsets previously unseen test instances, configuration scenarios much two ordersmagnitude. improvements C PLEXs default parameter configuration particularlynoteworthy, though claim found new parameter configuration C PLEXuniformly better default. Rather, given somewhat homogeneous instance set, findconfiguration specific set typically outperforms default, sometimes factor high20. Note achieved results even though intimately familiar C PLEXparameters; chose parameters optimize well values consider basedsingle person-day studying C PLEX user manual. success automated algorithmconfiguration even extreme conditions demonstrates potential approach.ParamILS source code executable freely availablehttp://www.cs.ubc.ca/labs/beta/Projects/ParamILS/,along quickstart guide data configuration scenarios studied article.9order apply ParamILS, automated algorithm configuration methods, practitioner must supply following ingredients.parameterized algorithm must possible set configurable parameters externally, e.g., command line call. Often, search hard-coded parameters hiddenalgorithms source code lead large number additional parameters exposed.Domains parameters Algorithm configurators must provided allowablevalues parameter. Depending configurator, may possible include additional knowledge dependencies parameters, conditional parameterssupported ParamILS. use ParamILS, numerical parameters must discretizedfinite number choices. Depending type parameter, uniform spacingvalues spacing, uniform log scale, typically reasonable.set problem instances homogeneous problem set interest is, betterexpect algorithm configuration procedure perform it. possibleconfigure algorithm good performance rather heterogeneous instance sets (e.g.,industrial SAT instances, PEAR reported Section 8.2), resultshomogeneous subsets interest improve configure instances subset. Whenever possible, set instances split disjoint training test setsorder safeguard over-tuning. configuring small and/or heterogeneousbenchmark set, ParamILS (or configuration procedure) might find configurations perform well independent test set.objective function used median performance first study ParamILS(Hutter et al., 2007), since found cases optimizing median performance ledparameter configurations good median poor overall performance. cases,optimizing mean performance yielded robust parameter configurations. However,optimizing mean performance one define cost unsuccessful runs.article, penalized runs counting ten times cutoff time.deal unsuccessful runs principled manner open research question.9. ParamILS continues actively developed; currently maintained Chris Fawcett.301fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEcutoff time unsuccessful runs smaller cutoff time run targetalgorithm chosen, quickly configuration procedure able exploreconfiguration space. However, choosing small cutoff risks failure mode experienced C P L E X -QP scenario. Recall there, choosing 300 seconds timeoutyielded parameter configuration good judged cutoff time (seeFigure 6(f)), performed poorly longer cutoffs (see Figure 6(e)). experiments, parameter configurations performing well low cutoff times turned scalewell harder problem instances well. many configuration scenarios, fact, noticedautomatically-found parameter configurations showed much better scaling behaviourdefault configuration. attribute use mean runtime configurationobjective. mean often dominated hardest instances distribution. However,manual tuning, algorithm developers typically pay attention easier instances, simplyrepeated profiling hard instances takes long. contrast, patient automaticconfigurator achieve better results avoids bias.Computational resources amount (computational) time required applicationautomated algorithm configuration clearly depends target application. targetalgorithm takes seconds solve instances homogeneous benchmark set interest,experience single five-hour configuration run suffice yield good resultsdomains achieved good results configuration times short halfhour. contrast, runs target algorithm slow performance largecutoff time expected yield good results instances interest, timerequirements automated algorithm configuration grow. also regularly perform multipleparallel configuration runs pick one best training performance order dealvariance across configuration runs.Overall, firmly believe automated algorithm configuration methods ParamILSplay increasingly prominent role development high-performance algorithmsapplications. study methods rich fruitful research area many interesting questions remaining explored.ongoing work, currently developing methods adaptively adjust domainsinteger-valued continuous parameters configuration process. Similarly, planenhance ParamILS dedicated methods dealing continuous parameters require discretization user. Another direction development concerns strategicselection problem instances used evaluation configurations instance-specific cutoff times used context. heuristically preventing configuration procedure spending inordinate amounts time trying evaluate poor parameter settings hard probleminstances, possible improve scalability.believe significant room combining aspects methods studiedconcepts related work similar algorithm configuration problems. particular,believe would fruitful integrate statistical testing methodsas used, e.g., F-RaceParamILS. Furthermore, see much potential use response surface modelsactive learning, believe combined approach. Finally, algorithmconfiguration problem studied article significant practical importance, also muchgained studying methods related problems, particular, instance-specific algorithmconfiguration online adjustment parameters run algorithm.302fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKAcknowledgmentsthank Kevin Murphy many helpful discussions regarding work. also thank DomagojBabic, author PEAR, Dave Tompkins, author UBCSAT APS implementationused experiments. thank researchers provided instances instancegenerators used work, particular Gent et al. (1999), Gomes Selman (1997), LeytonBrown et al. (2000), Babic Hu (2007), Zarpas (2005), Le Berre Simon (2004), Akturket al. (2007), Atamturk Munoz (2004), Atamturk (2003), Andronescu et al. (2007). LinXu created specific sets QCP SWGCP instances used. Thanks also Chris FawcettAshique KhudaBukhsh comments draft article. Finally, thankanonymous reviewers well Rina Dechter Adele Howe valuable feedback. ThomasStutzle acknowledges support F.R.S.-FNRS, Research Associate. HolgerHoos acknowledges support NSERC Discovery Grant 238788.ReferencesAdenso-Diaz, B. & Laguna, M. (2006). Fine-tuning algorithms using fractional experimental designlocal search. Operations Research, 54(1), 99114.Akturk, S. M., Atamturk, A., & Gurel, S. (2007). strong conic quadratic reformulation machine-jobassignment controllable processing times. Research Report BCOL.07.01, University CaliforniaBerkeley.Andronescu, M., Condon, A., Hoos, H. H., Mathews, D. H., & Murphy, K. P. (2007). Efficient parameterestimation RNA secondary structure prediction. Bioinformatics, 23, i19i28.Atamturk, A. (2003). facets mixedinteger knapsack polyhedron. Mathematical Programming,98, 145175.Atamturk, A. & Munoz, J. C. (2004). study lot-sizing polytope. Mathematical Programming, 99,443465.Audet, C. & Orban, D. (2006). Finding optimal algorithmic parameters using mesh adaptive direct searchalgorithm. SIAM Journal Optimization, 17(3), 642664.Babic, D. & Hu, A. J. (2007). Structural Abstraction Software Verification Conditions. W. Damm, H. H.(Ed.), Computer Aided Verification: 19th International Conference, CAV 2007, volume 4590 LectureNotes Computer Science, (pp. 366378). Springer Verlag, Berlin, Germany.Balaprakash, P., Birattari, M., & Stutzle, T. (2007). Improvement strategies F-Race algorithm: Sampling design iterative refinement. Bartz-Beielstein, T., Aguilera, M. J. B., Blum, C., Naujoks,B., Roli, A., Rudolph, G., & Sampels, M. (Eds.), 4th International Workshop Hybrid Metaheuristics (MH07), (pp. 108122).Bartz-Beielstein, T. (2006). Experimental Research Evolutionary Computation: New Experimentalism. Natural Computing Series. Springer Verlag, Berlin, Germany.Battiti, R., Brunato, M., & Mascia, F. (2008). Reactive Search Intelligent Optimization, volume 45Operations research/Computer Science Interfaces. Springer Verlag. Available online http://reactivesearch.org/thebook.Birattari, M. (2004). Problem Tuning Metaheuristics Seen Machine Learning Perspective.PhD thesis, Universite Libre de Bruxelles, Brussels, Belgium.Birattari, M., Stutzle, T., Paquete, L., & Varrentrapp, K. (2002). racing algorithm configuring metaheuristics. Langdon, W. B., Cantu-Paz, E., Mathias, K., Roy, R., Davis, D., Poli, R., Balakrishnan, K.,Honavar, V., Rudolph, G., Wegener, J., Bull, L., Potter, M. A., Schultz, A. C., Miller, J. F., Burke, E.,& Jonoska, N. (Eds.), Proceedings Genetic Evolutionary Computation Conference (GECCO2002), (pp. 1118). Morgan Kaufmann Publishers, San Francisco, CA, USA.303fiH UTTER , H OOS , L EYTON -B ROWN & UTZLECarchrae, T. & Beck, J. C. (2005). Applying machine learning low-knowledge control optimizationalgorithms. Computational Intelligence, 21(4), 372387.Cavazos, J. & OBoyle, M. F. P. (2006). Method-specific dynamic compilation using logistic regression.Cook, W. R. (Ed.), Proceedings ACM SIGPLAN International Conference Object-Oriented Programming, Systems, Languages, Applications (OOPSLA-06), (pp. 229240)., New York, NY, USA.ACM Press.Coy, S. P., Golden, B. L., Runger, G. C., & Wasil, E. A. (2001). Using experimental design find effectiveparameter settings heuristics. Journal Heuristics, 7(1), 7797.Diao, Y., Eskesen, F., Froehlich, S., Hellerstein, J. L., Spainhower, L., & Surendra, M. (2003). Generic onlineoptimization multiple configuration parameters application database server. Brunner, M. &Keller, A. (Eds.), 14th IFIP/IEEE International Workshop Distributed Systems: Operations Management (DSOM-03), volume 2867 Lecture Notes Computer Science, (pp. 315). Springer Verlag,Berlin, Germany.Fawcett, C., Hoos, H. H., & Chiarandini, M. (2009). automatically configured modular algorithm postenrollment course timetabling. Technical Report TR-2009-15, University British Columbia, DepartmentComputer Science.Gagliolo, M. & Schmidhuber, J. (2006). Dynamic algorithm portfolios. Amato, C., Bernstein, D., & Zilberstein, S. (Eds.), Ninth International Symposium Artificial Intelligence Mathematics (AI-MATH-06).Gagliolo, M. & Schmidhuber, J. (2007). Learning restart strategies. Veloso, M. M. (Ed.), ProceedingsTwentieth International Joint Conference Artificial Intelligence (IJCAI07), volume 1, (pp. 792797). Morgan Kaufmann Publishers, San Francisco, CA, USA.Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR select solution strategies constraint programming. Munoz-Avila, H. & Ricci, F. (Eds.), Proceedings 6th International Conference Case Based Reasoning (ICCBR05), volume 3620 Lecture Notes Computer Science, (pp.222236). Springer Verlag, Berlin, Germany.Gent, I. P., Hoos, H. H., Prosser, P., & Walsh, T. (1999). Morphing: Combining structure randomness.Hendler, J. & Subramanian, D. (Eds.), Proceedings Sixteenth National Conference ArtificialIntelligence (AAAI99), (pp. 654660)., Orlando, Florida. AAAI Press / MIT Press, Menlo Park, CA,USA.Gomes, C. P. & Selman, B. (1997). Problem structure presence perturbations. Kuipers, B. &Webber, B. (Eds.), Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97),(pp. 221226). AAAI Press / MIT Press, Menlo Park, CA, USA.Gratch, J. & Chien, S. A. (1996). Adaptive problem-solving large-scale scheduling problems: casestudy. Journal Artificial Intelligence Research, 4, 365396.Gratch, J. & Dejong, G. (1992). Composer: probabilistic solution utility problem speed-uplearning. Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings Tenth National ConferenceArtificial Intelligence (AAAI92), (pp. 235240). AAAI Press / MIT Press, Menlo Park, CA, USA.Hoos, H. H. (2002). mixture-model behaviour SLS algorithms SAT. ProceedingsEighteenth National Conference Artificial Intelligence (AAAI-02), (pp. 661667)., Edmonton, Alberta,Canada.Hoos, H. H. (2008). Computer-aided design high-performance algorithms. Technical Report TR-2008-16,University British Columbia, Department Computer Science.Hoos, H. H. & Stutzle, T. (2005). Stochastic Local Search Foundations & Applications. Morgan KaufmannPublishers, San Francisco, CA, USA.Horvitz, E., Ruan, Y., Gomes, C. P., Kautz, H., Selman, B., & Chickering, D. M. (2001). Bayesianapproach tackling hard computational problems. Breese, J. S. & Koller, D. (Eds.), ProceedingsSeventeenth Conference Uncertainty Artificial Intelligence (UAI01), (pp. 235244). MorganKaufmann Publishers, San Francisco, CA, USA.304fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORKHutter, F. (2009). Automated Configuration Algorithms Solving Hard Computational Problems. PhDthesis, University British Columbia, Department Computer Science, Vancouver, Canada.Hutter, F., Babic, D., Hoos, H. H., & Hu, A. J. (2007). Boosting Verification Automatic Tuning DecisionProcedures. Proceedings Formal Methods Computer Aided Design (FMCAD07), (pp. 2734).,Washington, DC, USA. IEEE Computer Society.Hutter, F., Hamadi, Y., Hoos, H. H., & Leyton-Brown, K. (2006). Performance prediction automatedtuning randomized parametric algorithms. Benhamou, F. (Ed.), Principles Practice Constraint Programming CP 2006: Twelfth International Conference, volume 4204 Lecture NotesComputer Science, (pp. 213228). Springer Verlag, Berlin, Germany.Hutter, F., Hoos, H., & Leyton-Brown, K. (2009). Tradeoffs empirical evaluation competing algorithm designs. Technical Report TR-2009-21, University British Columbia, Department ComputerScience.Hutter, F., Hoos, H. H., Leyton-Brown, K., & Murphy, K. P. (2009). experimental investigationmodel-based parameter optimisation: SPO beyond. Proceedings Genetic EvolutionaryComputation Conference (GECCO-2009), (pp. 271278).Hutter, F., Hoos, H. H., & Stutzle, T. (2005). Efficient stochastic local search MPE solving. ProceedingsNineteenth International Joint Conference Artificial Intelligence (IJCAI05), (pp. 169174).Hutter, F., Hoos, H. H., & Stutzle, T. (2007). Automatic algorithm configuration based local search.Howe, A. & Holte, R. C. (Eds.), Proceedings Twenty-second National Conference ArtificialIntelligence (AAAI07), (pp. 11521157). AAAI Press / MIT Press, Menlo Park, CA, USA.Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient dynamiclocal search SAT. Hentenryck, P. V. (Ed.), Principles Practice Constraint ProgrammingCP 2002: Eighth International Conference, volume 2470 Lecture Notes Computer Science, (pp.233248). Springer Verlag, Berlin, Germany.Johnson, D. S. (2002). theoreticians guide experimental analysis algorithms. Goldwasser,M. H., Johnson, D. S., & McGeoch, C. C. (Eds.), Data Structures, Near Neighbor Searches, Methodology: Fifth Sixth DIMACS Implementation Challenges, (pp. 215250). American Mathematical Society, Providence, RI, USA.Jones, D. R., Schonlau, M., & Welch, W. J. (1998). Efficient global optimization expensive black boxfunctions. Journal Global Optimization, 13, 455492.Kask, K. & Dechter, R. (1999). Stochastic local search Bayesian networks. Seventh InternationalWorkshop Artificial Intelligence Statistics (AISTATS99).Kautz, H., Horvitz, E., Ruan, Y., Gomes, C. P., & Selman, B. (2002). Dynamic restart policies. Dechter,R., Kearns, M., & Sutton, R. (Eds.), Proceedings Eighteenth National Conference ArtificialIntelligence (AAAI02), (pp. 674681). AAAI Press / MIT Press, Menlo Park, CA, USA.KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building local search sat solvers components. Proceedings Twenty-first International Joint ConferenceArtificial Intelligence (IJCAI09), (pp. 517524).Le Berre, D. & Simon, L. (2004). Fifty-five solvers Vancouver: SAT 2004 competition. Hoos, H. H.& Mitchell, D. G. (Eds.), Theory Applications Satisfiability Testing: Proceedings SeventhInternational Conference (SAT04), volume 3542 Lecture Notes Computer Science, (pp. 321344).Springer Verlag.Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning empirical hardness optimizationproblems: case combinatorial auctions. Hentenryck, P. V. (Ed.), Principles PracticeConstraint Programming CP 2002: Eighth International Conference, volume 2470 Lecture NotesComputer Science, (pp. 556572). Springer Verlag, Berlin, Germany.Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards universal test suite combinatorialauction algorithms. Jhingran, A., Mason, J. M., & Tygar, D. (Eds.), EC 00: Proceedings 2nd305fiH UTTER , H OOS , L EYTON -B ROWN & UTZLEACM conference Electronic commerce, (pp. 6676)., New York, NY, USA. ACM.Lourenco, H. R., Martin, O., & Stutzle, T. (2002). Iterated local search. F. Glover & G. Kochenberger(Eds.), Handbook Metaheuristics (pp. 321353). Kluwer Academic Publishers, Norwell, MA, USA.Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search classificationfunction approximation. Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances NeuralInformation Processing Systems 7 (NIPS-94), volume 6, (pp. 5966). Morgan Kaufmann Publishers, SanFrancisco, CA, USA.Mengshoel, O. J. (2008). Understanding role noise stochastic local search: Analysis experiments. Artificial Intelligence, 172(8-9), 955990.Minton, S. (1993). analytic learning system specializing heuristics. Bajcsy, R. (Ed.), ProceedingsThirteenth International Joint Conference Artificial Intelligence (IJCAI93), (pp. 922929). MorganKaufmann Publishers, San Francisco, CA, USA.Minton, S. (1996). Automatically configuring constraint satisfaction programs: case study. Constraints,1(1), 140.Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: heuristic repairmethod constraint-satisfaction scheduling problems. Artificial Intelligence, 58(1), 161205.Patterson, D. J. & Kautz, H. (2001). Auto-WalkSAT: self-tuning implementation WalkSAT. ElectronicNotes Discrete Mathematics (ENDM), 9.Ridge, E. & Kudenko, D. (2006). Sequential experiment designs screening tuning parametersstochastic heuristics. Paquete, L., Chiarandini, M., & Basso, D. (Eds.), Workshop Empirical MethodsAnalysis Algorithms Ninth International Conference Parallel Problem SolvingNature (PPSN), (pp. 2734).Santner, T. J., Williams, B. J., & Notz, W. I. (2003). Design Analysis Computer Experiments.Springer Verlag, New York.Selman, B., Levesque, H. J., & Mitchell, D. (1992). new method solving hard satisfiability problems.Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings Tenth National Conference ArtificialIntelligence (AAAI92), (pp. 440446). AAAI Press / MIT Press, Menlo Park, CA, USA.Spall, J. C. (2003). Introduction Stochastic Search Optimization. New York, NY, USA: John Wiley &Sons, Inc.Terashima-Marn, H., Ross, P., & Valenzuela-Rendon, M. (1999). Evolution constraint satisfaction strategies examination timetabling. Proceedings Genetic Evolutionary Computation Conference(GECCO-1999), (pp. 635642). Morgan Kaufmann.Thachuk, C., Shmygelska, A., & Hoos, H. H. (2007). replica exchange monte carlo algorithm proteinfolding hp model. BMC Bioinformatics, 8, 342342.Tompkins, D. A. D. & Hoos, H. H. (2004). UBCSAT: implementation experimentation environmentSLS algorithms SAT & MAX-SAT. Theory Applications Satisfiability Testing: Proceedings Seventh International Conference (SAT04), volume 3542, (pp. 306320). Springer Verlag,Berlin, Germany.Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: portfolio-based algorithm selectionSAT. Journal Artificial Intelligence Research, 32, 565606.Zarpas, E. (2005). Benchmarking SAT Solvers Bounded Model Checking. Bacchus, F. & Walsh, T.(Eds.), Theory Applications Satisfiability Testing: Proceedings Eighth International Conference (SAT05), volume 3569 Lecture Notes Computer Science, (pp. 340354). Springer Verlag.306fiJournal Artificial Intelligence Research 36 (2009) 513-546Submitted 08/09; published 12/09RoxyBot-06: Stochastic Prediction OptimizationTAC TravelAmy Greenwaldamy@cs.brown.eduDepartment Computer Science, Brown UniversityProvidence, RI 02912 USASeong Jae Leeseongjae@u.washington.eduComputer Science Engineering, University WashingtonSeattle, WA 98195 USAVictor Naroditskiyvnarodit@cs.brown.eduDepartment Computer Science, Brown UniversityProvidence, RI 02912 USAAbstractpaper, describe autonomous bidding agent, RoxyBot, emerged victorious travel division 2006 Trading Agent Competition photo finish.high level, design many successful trading agents summarized follows:(i) price prediction: build model market prices; (ii) optimization: solveapproximately optimal set bids, given model. predict, RoxyBot builds stochastic model market prices simulating simultaneous ascending auctions. optimize,RoxyBot relies sample average approximation method, stochastic optimizationtechnique.1. Introductionannual Trading Agent Competition (TAC) challenges entrants design buildautonomous agents capable effective trading online travel1 shopping game. firstTAC, held Boston 2000, attracted 16 entrants six countries North America,Europe, Asia. Excitement generated event led refinement gamerules, continuation regular tournaments increasing levels competitionnext six years. Year-by-year, entrants improved designs, developing new ideasbuilding previously successful techniques. Since TACs inception, lead authorentered successive modifications autonomous trading agent, RoxyBot. paperreports RoxyBot-06, latest incarnation top scorer TAC-06 tournament.key feature captured TAC travel game goods highly interdependent(e.g., flights hotels must coordinated), yet markets goods operateindependently. second important feature TAC agents trade via three differentkinds market mechanisms, presents distinct challenges. Flights tradedposted-price environment, designated party sets price parties1. four divisions TAC: Travel, Supply Chain Management (SCM), CAT (TAC backwards),Ad Auctions (AA). paper concerned first; description others, seepapers Arunachalam Sadeh (2005), Cai et al. (2009), Jordan Wellman (2009), respectively.paper, say TAC, mean TAC Travel.c2009AI Access Foundation. rights reserved.fiGreenwald, Lee, & Naroditskiymust take leave. Hotels traded simultaneous ascending auctions, like FCCspectrum auctions. Entertainment tickets traded continuous double auctions, likeNew York Stock Exchange. grappling three mechanisms constructingagent strategies, participants confronted number interesting problems.success autonomous trading agent TAC agent often hinges uponsolutions two key problems: (i) price prediction, agent builds modelmarket prices; (ii) optimization, agent solves approximatelyoptimal set bids, given model. example, core RoxyBots 2000 architecture (Greenwald & Boyan, 2005) deterministic optimization problem, namelybid given price predictions form point estimates. spite effectivenessTAC-00 tournament, weakness 2000 design RoxyBot could explicitlyreason variance within prices. years since 2000, recast key challengesfaced TAC agents several different stochastic bidding problems (see, example,paper Greenwald & Boyan, 2004), whose solutions exploit price predictions formdistributions. spite perseverance, RoxyBot fared unimpressively tournamentconditions year year, 2006. Half decade laboratory spent searchingbidding heuristics exploit stochastic information reasonable computational expense finally bore fruit, RoxyBot emerged victorious TAC-06. nutshell, secretRoxyBot-06s success is: (hotel) price prediction simulating simultaneous ascendingauctions, optimization based sample average approximation method. Detailsapproach subject present article.Overview paper organized follows. Starting Section 2, summarizeTAC market game. Next, Section 3, present high-level view RoxyBots 2006architecture. Section 4, describe RoxyBots price prediction techniques flights, hotels, entertainment, turn. Perhaps greatest interest hotel price predictionmethod. Following Wellman et al. (2005), predict hotel prices computing approximate competitive equilibrium prices. Only, instead computing prices runningtatonnement process, simulate simultaneous ascending auctions. proceduresimpler implement tatonnement, yet achieves comparable performance, runssufficiently fast. Section 5, describe RoxyBots optimization technique: sample averageapproximation. argue approach optimal pseudo-auctions, abstractmodel auctions. Section 6.1, describe simulation experiments controlled testing environment show combined approachsimultaneous ascending auctionshotel price prediction sample average approximation bid optimizationperformswell practice comparison reasonable bidding heuristics. Section 6.2,detail results TAC-06 tournament, validating success RoxyBot-06sstrategy, reporting statistics shed light bidding strategies participating agents. Finally, Section 7, evaluate collective behavior autonomousagents TAC finals since 2002. find accuracy competitive equilibriumcalculations varied year year highly dependent particular agentpool. Still, generally speaking, collective appears moving toward competitiveequilibrium behavior.514fiRoxyBot-062. TAC Market Game: Brief Summarysection, summarize TAC game. details, see http://www.sics.se/tac/.Eight agents play TAC game. simulated travel agent whose taskorganize itineraries clients travel TACTown five day (fournight) period. time allotted (nine minutes), agents objective procuretravel goods inexpensively possible, trading fact goodsultimately compiled feasible trips satisfy client preferences greatestextent possible. agents know preferences eight clients only,56.Travel goods sold simultaneous auctions follows:Flight tickets sold TACAir dynamic posted-pricing environments.flights TACTown applicable day. resale flighttickets agents permitted.Flight price quotes broadcast TAC server every ten seconds.Hotel reservations sold TAC seller multi-unit ascending call markets.Specifically, 16 hotel reservations sold hotel auction 16 highestbidders 16th highest price. TAC seller runs eight hotel auctions, one pernight-hotel combination (recall travel takes place four night period;moreover, two hotels: good one bad one). resale hotelreservations agents permitted. bid withdrawal allowed.specifically, eight hotel auctions clear minute exactly one auctionclosing minutes one eight. (The precise auction close chosenrandom, open auctions equally likely selected.) auctioncloses, TAC server broadcasts final closing price, informs agentwinnings. others, TAC server reports current ask price, informsagent hypothetical quantity (HQW).Agents allocated initial endowment entertainment tickets, tradeamong continuous double auctions (CDAs). three entertainment events scheduled day.Although event auctions clear continuously, price quotes broadcast every30 seconds.One primary challenges posed TAC design build autonomous agentsbid effectively interdependent (i.e., complementary substitutable) goodssold separate markets. Flight tickets hotel reservations complementaryflights useful client without corresponding hotel reservations, viceversa. Tickets entertainment events (e.g., Boston Red Sox Boston SymphonyOrchestra) substitutable client cannot attend multiple events simultaneously.515fiGreenwald, Lee, & NaroditskiyREPEAT{start bid interval }0. Download current prices winnings server1. predict: build stochastic modelsa. flights: Bayesian updating/learningb. hotels: simultaneous ascending auctionsc. entertainment: sample historical data2. optimize: sample average approximation3. Upload current bids server(three separate threads){end bid interval }gameTable 1: high-level view RoxyBot-06s architecture.3. RoxyBot-06s Architecture: High-Level Viewapproach problem bidding interdependent goods separate TACmarkets, adopt simplifying assumptions. Rather tackle game-theoreticproblem characterizing strategic equilibria, focus single agents (decision-theoretic)problem optimizing bidding behavior, assuming agents strategiesfixed. addition, assume environment modeled terms agentspredictions market clearing prices. prices serve summarize relevant information hidden agents bidding strategies. two assumptionsfixed otheragent behaviors market information encapsulated pricessupport modular design RoxyBot-06 many successful TAC agents, consists two key stages:(i) price prediction; (ii) optimization.optimization problem faced TAC agents dynamic one incorporatesaspects sequentiality well simultaneity auctions. markets operate simultaneously, addition, prices discovered incrementally time. principle,clairvoyant agentone knowledge future clearing pricescould justifiably employopen-loop strategy: could solve TAC optimization problem startgame place bids accordingly, never reconsidering decisions. practical alternative (and usual approach taken TAC2 ), incorporate agentsarchitecture closed loop, bidding cycle, enabling agent condition behaviorevolution prices. price information revealed, agent improves pricepredictions, reoptimizes bidding decisions, repeatedly.One distinguishing feature RoxyBot-06 builds stochastic models marketclearing prices, rather predicting clearing prices point estimates. Given stochasticprice predictions, stochastic optimization lies heart RoxyBot-06. Assuming time2. exception livingagents (Fritschi & Dorer, 2002), winner TAC 2001.516fiRoxyBot-06discretized stages, bid intervals, iteration bidding cycle, RoxyBot-06faces n-stage stochastic optimization problem, n number stages remaininggame. key input optimization problem sequence n 1 stochasticmodels future prices, one joint probability distribution goods conditionedpast prices past hotel closings. solution optimization problem,output iteration bidding cycle, vector bids, one per good (or auction).Table 1 presents high-level view RoxyBot-06s architecture, emphasizing biddingcycle. start bid interval, current prices winnings downloadedTAC server. Next, key prediction optimization routines run. prediction module, stochastic models flight, hotel, entertainment prices built.optimization module, bids constructed approximate solution n-stagestochastic optimization problem. Prior end bid interval, agents bidsuploaded TAC server using three separate threads: (i) flight thread bidsflight price near predicted minimum; (ii) hotel thread bids openhotels moments end minute; (iii) entertainment threadplaces bids immediately.discuss details RoxyBot-06s price prediction module first, optimizationmodule second.4. Price Predictionsection, describe RoxyBot-06 builds stochastic models flight, hotel,event prices. model discrete probability distribution, represented set scenarios. scenario vector future pricesprices goods boughtsold current stage. flights, price prediction model stochastic:future buy price simply RoxyBot-06s prediction expected minimum pricecurrent stage. hotels, future buy prices predicted Monte Carlo simulationssimultaneous ascending auctions approximate competitive equilibrium prices.current buy prices hotels. entertainment, RoxyBot-06 predicts future buysell prices based historical data. Details price prediction methods focussection.4.1 FlightsEfforts deliberate flight purchasing start understanding TAC modelflight price evolution.4.1.1 TAC Flight Prices Stochastic ProcessFlight prices follow biased random walk. initialized uniformly range[250, 400], constrained remain range [150, 800]. start TACgame instance, bound z final perturbation value selected flight.bounds revealed agents. revealed agents sequencerandom flight prices. Every ten seconds, TACAir perturbs price flightrandom value depends hidden parameter z current time follows:given constants c, R > 0, (intermediate) bound perturbation value517fiGreenwald, Lee, & Naroditskiylinear function t:(z c)(1)perturbation value time drawn uniformly one following ranges (seeAlgorithm 1):x(t, z) = c +U [c, x(t, z)], x(t, z) > 0U [c, +c], x(t, z) = 0U [x(t, z), +c], x(t, z) < 0Observe expected perturbation value case simply averagecorresponding upper lower bounds. particular,x(t, z) > c, expected perturbation positive;x(t, z) (0, c), expected perturbation negative;x(t, z) (c, 0), expected perturbation positive;otherwise, x(t, z) {c, 0, c}, expected perturbation zero.Moreover, using Equation 1, compute expected perturbation value conditionedz:z [0, c], x(t, z) [0, c], prices expected increase;z [c, c + d], x(t, z) [c, c + d], prices expected decrease;z [c, 0], x(t, z) [c, c], prices expected increasecT.expected decrease czcTczTAC parameters set follows: c = 10, = 30, = 540, z uniformlydistributed range [c, d]. Based discussion, note following:given information z, TAC flight prices expected increase (i.e.,expected perturbation positive); however, conditioned z, TAC flight prices mayincrease decrease (i.e., expected perturbation positive negative).4.1.2 RoxyBot-06s Flight Prices Prediction MethodAlthough value hidden parameter z never revealed agents, recallagents observe sample flight prices, say y1 , . . . , yt , depend value.information used model probability distribution Pt [z] P [z | y1 , . . . , yt ].probability distribution estimated using Bayesian updating. RoxyBot06, agents Walverine (Cheng et al., 2005) Mertacor (Toulis et al., 2006) took approach.Walverine uses Bayesian updating compute next expected price perturbationcompares value threshold, postponing flight purchases prices expectedincrease threshold. Mertacor uses Bayesian updating estimatetime flight prices reach minimum value. RoxyBot uses Bayesian updatingcompute expected minimum price, describe.518fiRoxyBot-06Algorithm 1 getRange(c, t, z)compute x(t, z) {Equation 1}x(t, z) > 0= c; b = x(t, z)else x(t, z) < 0= x(t, z); b = +celse= c; b = +cendreturn [a, b] {range}RoxyBot-06s implementation Bayesian updating presented Algorithm 2. LettingQ0 [z] =1c+d= P [z], algorithm estimates Pt+1 [z] = P [z | y1 , . . . , yt+1 ] usual:P [y1 , . . . , yt | z]P [z]z P [y1 , . . . , yt | z ]P [z ] dzP [z | y1 , . . . , yt ] = PP [y1 , . . . , yt | z] ==i=1(2)P [yi | y1 , . . . , yi1 , z](3)P [yi | z](4)i=1Equation 4 follows fact future observations independent past observations; observations depend hidden parameter z.thing left explain set values P [yi | z], = 1, . . . , t.described pseudocode, done follows: yt+1 within appropriate rangetime, probability set uniformly within bounds range; otherwise,set 0. Presumably, Walverines Mertacors implementations Bayesian updatingdifferent one.3 However, alluded above, agents makeuse ensuing estimated probability distributions differ.RoxyBot-06 predicts flights price expected minimum price. valuecomputed follows (see Algorithm 3): possible value hidden parameter z,RoxyBot simulates expected random walk, selects minimum price along walk,outputs prediction expectation minima, averaging accordingPt [z]. call random walk expected, since perturbation value expectation(i.e., = ba2 ) instead sample (i.e., U [a, b]). carrying computation,RoxyBot generates flight price predictions point estimates. implicit decisionmake RoxyBot-06s hotel event price predictions stochastic made basedintuitive sense time vs. accuracy tradeoffs RoxyBots optimization module,hence warrants study.3. provide details here, corresponding details agents seem publiclyavailable.519fiGreenwald, Lee, & NaroditskiyAlgorithm 2 Flight Prediction(c, d, t, yt+1 , Qt )z {c, c + 1, . . . , d}[a, b] = getRange(c, t, z)yt+1 [a, b]1P [yt+1 | z] = baelseP [yt+1 | z] = 0endQt+1 [z] = P [yt+1 | z]Qt [z]end for{update probabilities}z {c, c + 1, . . . , d}t+1 [z]Pt+1 [z] = P Qz Qt+1 [z ] dzend for{normalize probabilities}return Pt+1 {probabilities}Algorithm 3 Expected Minimum Price(c, t, , pt , Pt )z Rmin[z] = += + 1, . . . ,[a, b] = getRange(c, , z)= ba2 {expected perturbation}p = p 1 + {perturb price}p = max(150, min(800, p ))p < min[z]min[z] = pendendend forPreturn z Pt [z] min[z] dz4.2 Hotelscompetitive market individuals effect prices negligible, equilibrium prices prices supply equals demand, assuming producers profitmaximizing consumers utility-maximizing. RoxyBot-06 predicts hotel pricessimulating simultaneous ascending auctions (SimAA) (Cramton, 2006), attemptapproximate competitive equilibrium (CE) prices. approach inspired Walverines (Cheng et al., 2005), tatonnement method (Walras, 1874) usedpurpose.4.2.1 Simultaneous Ascending AuctionsLet p~ denote vector prices. ~y (~p) denotes cumulative supply producers,~x(~p) denotes cumulative demand consumers, ~z(~p) = ~x(~p)~y (~p) denotes520fiRoxyBot-06excess demand market. tatonnement process adjusts price vector iterationn + 1, given price vector iteration n adjustment rate n follows: p~n+1 =p~n + n~z(~pn ). SimAA adjusts price vector follows: p~n+1 = p~n + max{~z(~pn ), 0},fixed value . processes continue excess demand non-positive:i.e., supply exceeds demand.Although competitive equilibrium prices guaranteed exist TAC markets (Cheng et al., 2003), SimAA adjustment process, still guaranteed converge:prices increase, demand decreases supply increases; hence, supply eventually exceeds demand. parameter SimAA method magnitude priceadjustment. smaller value, accurate approximation (assuming CEprices exist), value chosen lowest value time permits.tatonnement process, hand, difficult applyguaranteed converge. Walverine team dealt convergence issue decayinginitial value . However, careful optimization required ensure convergencereasonable solution reasonable amount time. fact, Walverine found helpfulset initial prices certain non-zero values. complexity present usingsimultaneous ascending auctions approximate competitive equilibrium prices.4.2.2 Prediction QualityTAC, cumulative supply fixed. Hence, key computing excess demandcompute cumulative demand. TAC agent knows preferences clients,must estimate demand others. Walverine computes single hotel price prediction (apoint estimate) considering clients demands together 56 expectedclients. Briefly, utility expected client average across travel dates hoteltypes augmented fixed entertainment bonuses favor longer trips (see paperCheng et al., 2005, details). contrast, RoxyBot-06 builds stochastic modelhotel prices consisting scenarios considering clients demands togetherrandom samples 56 clients. (random expected) clients demand simplyquantity good optimal package, given current prices. cumulative demandsum total clients individual demands.Figure 1, present two scatter plots depict quality various hotel pricepredictions beginning TAC 2002 final games. price predictions evaluated using two metrics: Euclidean distance expected value perfect prediction(EVPP). Euclidean distance measure difference two vectors, caseactual predicted prices. value perfect prediction (VPP) clientdifference surplus (value preferred package less price) based actualpredicted prices. EVPP VPP averaged distribution client preferences.4left, plot predictions generated using competitive equilibrium ap1proximation methods, tatonnement SimAA, fixed = 24, making expected,random, exact predictions. exact predictions computed based actualclients games, client distribution; hence, serve lower boundperformance techniques data set. metrics,expected random, SimAAs predictions outperform tatonnements.4. See paper Wellman et al. (2004) details.521fiGreenwald, Lee, & Naroditskiy4470livingagentsPackaTACSouthamptonRoxyBot UMBCTACwhitebearSICS_baselineATTacExpected Value Perfect PredictionExpected Value Perfect Prediction65tatonnement, random42SimAA, random40tatonnement, expected3836SimAA, expectedSimAA, exacttatonnement, exact605550haramicuhk45kavayaHSimAA, random40Walverine353418019020021022030180230Euclidean DistanceATTac01190200210220230240250260Euclidean DistanceFigure 1: EVPP Euclidean Distance CE price prediction methods (tatonnement1; expected, random, exact) TAC 2002 agentsSimAA = 24predictions 2002 finals (60 games). plot left shows SimAAspredictions better tatonnements expecteds betterrandoms. RoxyBot-06s method hotel price prediction (SimAA, Random)plotted right. Note differences scales two plots.Since fixed, tatonnement guaranteed converge condition,outcome entirely surprising. interesting, though, SimAA expectedperforms comparably Walverine (see right plot).5 interesting SimAAfewer parameter settings tatonnementonly single value comparedinitial value together decay scheduleand moreover, optimizeparameter setting. Walverines parameter settings, hand, highlyoptimized.interpret prediction generated using randomly sampled clients samplescenario, set scenarios represents draws probability distributionCE prices. corresponding vector predicted prices evaluated actuallyaverage multiple (40) predictions; is, evaluate estimate meanprobability distribution. predictions generated using sets random clientsgood predictions expected clients (see Figure 1 left), although40 sets random clients, results might improve. Still, predictions randomclients comprise RoxyBot-06s stochastic model hotel prices, key biddingstrategy. Moreover, using random clients helps RoxyBot-06 make better interim predictionslater game explain next.4.2.3 Prediction Quality Time: Interim Price Predictiongraphs depicted Figure 1 pertain hotel price predictions made beginninggame, hotel auctions open. CE computations, prices initialized0. hotel auctions close, RoxyBot-06 updates predicted prices hotel auctions5. exception RoxyBot-06 data point (i.e., SimAA random), plot producedWalverine team (Wellman et al., 2004).522fi22140tatonnement, expected clientsSimAA, expected clientstatonnement, random clientsSimAA, random clientstatonnement, random clients, distributionSimAA, random clients, distribution201816tatonnement, expected clientsSimAA, expected clientstatonnement, random clientsSimAA, random clientstatonnement, random clients, distributionSimAA, random clients, distribution120Euclidean Distance per HotelExpected Value Perfect Prediction per HotelRoxyBot-0614121086410080604020200012345670Minute1234567MinuteFigure 2: EVPP Euclidean Distance TAC 2006 finals (165 games) CE priceprediction methods without distribution game progresses. Distribution improves prediction quality.remain open. experimented two ways constructing interim price predictions.first initialize lower bound prices hotel markets closing(for closed auctions) current ask (for open auctions) prices computing competitiveequilibrium prices.6 second differs treatment closed auctions: simulateprocess distributing goods closed auctions clients want most,exclude closed markets (i.e., fix prices ) computationscompetitive equilibrium prices.Regarding second methodthe distribution methodwe determine distribute goods computing competitive equilibrium prices again. explained Algorithm 4, hotels (in open closed auctions) distributed random clientsdetermining willing pay competitive equilibrium prices what.immediately obvious distribute goods expected clients; hence, enhancedprediction methods random clients distribution.Figure 2, depicts prediction quality time, shows prediction methodsenhanced distribution better predictions obtained merely initializingprices closed hotel auctions closing prices. Hotels close early tendsell less hotels close late; hence, prediction quality methodmakes decent initial predictions bound deteriorate predictions remain relativelyconstant throughout game.4.2.4 Run TimeTable 2 shows run times CE prediction methods TAC 2002 (60 games)TAC 2006 (165 games) finals data set minute 0, well run times6. first blush, may seem sensible fix prices closed hotels closing prices, rathermerely lower bound (i.e., allow increase). hotel closed artificially lowprice, however, price permitted increase, predicted prices hotelscomplementing hotel question would artificially high.523fiGreenwald, Lee, & NaroditskiyAlgorithm 4 Distribute1: hotel auctions h2:initialize price 03:initialize supply 164: end5: compute competitive equilibrium prices {Tatonnement SimAA}6: closed hotel auctions h7:distribute units h demand computed competitive equilibrium prices8:distribute leftover units h uniformly random9: endminutes 17 TAC 2006 finals data set. numbers table conveySimAAs run time, even distribution, reasonable. example, minute 0,SimAA sample takes order 0.1 seconds. minutes 1-7, method withoutdistribution runs even faster. speed increase occurs CE prices boundedcurrent ask prices maximum price client willing payhotel, current ask prices increase time, correspondingly reducing sizesearch space. SimAA sample distribution minutes 1-7 takes twice long SimAAsample without distribution minute 0 time takes distribute goods,run time still (roughly) 0.2 seconds. implementation tatonnement runsslowly fixed instead optimizing tradeoff convergence rateaccuracy, process converge, instead ran maximum numberiterations (10,000). summary, SimAA simpler tatonnement implement, yetperforms comparably optimized version tatonnement (i.e., Walverine), runssufficiently fast.2002, minute 02006, minute 02006, average 17Exp Tat221322522248Exp SimAA507508347Sam Tat134511051138Sam SimAA15713097Dist Tat11112249Dist SimAA128212Table 2: Run times CE price prediction methods, milliseconds. Experimentsrun AMD Athlon(tm) 64 bit 3800+ dual core processors 2M RAM.4.2.5 Summarysimulation methods discussed sectionthe tatonnement process simultaneous ascending auctionswere employed predict hotel prices only. (In simulations,flight prices fixed expected minima, entertainment prices fixed 80.)principle, competitive equilibrium (CE) prices could serve predictions TAC markets. However, CE prices unlikely good predictors flight prices, since flightprices determined exogenously. regard entertainment tickets, CE prices might524fiRoxyBot-06predictive power; however, incorporating entertainment tickets tatonnementSimAA calculations would expensive. (In simulations, following Wellman et al., 2004, client utilities simply augmented fixed entertainment bonusesfavor longer trips.) Nonetheless, future work, could interest evaluatesuccess related methods predicting CDA clearing prices.Finally, note refer methods computing excess demand clientbased compute demands client individual basis. contrast,one could employ agent-based method, whereby demands agents, clients,would calculated. Determining agents demands involves solving so-called completion, deterministic (prices known) optimization problem heart RoxyBot-00sarchitecture (Greenwald & Boyan, 2005). TAC completion NP-hard, agent-basedmethod predicting hotel prices expensive included RoxyBot-06s innerloop. designing RoxyBot-06, reasoned architecture based stochastic pricing model generated using client-based method randomly sampled clients wouldoutperform one based point estimate pricing model generated using agent-basedmethod form expected clients, verify reasoning empirically.4.3 Entertainmentbid interval, RoxyBot-06 predicts current future buy sell prices ticketsentertainment events. price predictions optimistic: agent assumesbuy (or sell) goods least (or most) expensive prices expects seeend game. specifically, current price prediction best predictedprice current bid interval.RoxyBot-06s estimates entertainment ticket prices based historical datapast 40 games. generate scenario, sample game drawn randomcollection, sequences entertainment bid, ask, transaction prices extracted.Given history, auction a, let trade ai denote price last tradetime transacted; value initialized 200 buying 0 selling.addition, let bid ai denote bid price time i, let ask ai denote ask price timei.RoxyBot-06 predicts future buy price auction time follows:future buy =mini=t+1,...,Tmin{trade ai , ask ai }(5)words, future buy price time = + 1, . . . , minimum ask pricetime recent trade price. future buy price time minimumacross future buy prices later times. future sell price time predictedanalogously:(6)future sell = max max{trade ai , bid ai }i=t+1,...,TArguably, RoxyBot-06s entertainment predictions made simplest possible way:past data future predictions. likely one could improve upon naive approachusing generalization technique capable learning distribution data,sampling learned distribution.525fiGreenwald, Lee, & Naroditskiy4.4 Summarysection, described RoxyBot-06s price prediction methods. key ideas,may transferable beyond TAC, least TAC agents, follows:1. RoxyBot makes stochastic price predictions. generating set so-calledscenarios, scenario vector future prices.2. flight, RoxyBot uses Bayesian updating predict expected minimum price.3. hotels, RoxyBot-uses method inspired Walverines: approximates competitiveequilibrium prices simulating simultaneous ascending auctions, ratherusual tatonnement process.5. OptimizationNext, characterize RoxyBot-06s optimization routine. (i) stochastic, (ii) global,(iii) dynamic. takes input stochastic price predictions; considers flight, hotel,entertainment bidding decisions unison; simultaneously reasons bidsplaced current future stages game.5.1 Abstract Auction ModelRecall treatment bidding decision-theoretic, rather game-theoretic.particular, focus single agents problem optimizing bidding behavior, assuming agents strategies fixed. keeping basic agent architecture,assume environment modeled terms agents predictionsmarket clearing prices. introduce term pseudo-auction refer marketmechanism defined two assumptionsfixed other-agent behaviors market information encapsulated prices. optimization problem RoxyBot solves onebidding pseudo-auctions, (true) auctions. section, formally developabstract auction model relate TAC auctions; next, define proposeheuristics solve various pseudo-auction bidding problems.5.1.1 Basic Formalismsection, formalize basic concepts needed precisely formulate bidding uncertainty optimization problem, including: packagessets goods, possiblymultiple units each; function describes much agent values package; pricelinesdata structures store prices unit good;bidspairs vectors corresponding buy sell offers.Packages Let G denote ordered set n distinct goods let N Nn representmultiset goods marketplace, Ng denoting number unitsgood g G. package collection goods, is, submultiset N . writeN whenever Mg Ng g G.instructive interpret notation TAC domain. flights, hotel rooms,entertainment events auction TAC comprise ordered set 28 distinct526fiRoxyBot-06goods. principle, multiset goods TAC marketplace is:N TAC = h, . . . , , 16, . . . , 16, 8, . . . , 8i N28| {z } | {z } | {z }8 flights8 hotels12 eventspractice, however, since agent works satisfy preferences eight clients,suffices consider multiset goods:N TAC8 = h8 . . . , 8, 8, . . . , 8, 8, . . . , 8i N TAC| {z } | {z } | {z }8 flights8 hotels12 eventstrip corresponds package, specifically N TAC8 satisfies TACfeasibility constraints.Given A, B N , rely two basic operations, , defined follows:g G,(A B)g Ag + Bg(A B)g Ag Bgexample, G = {, , } N = h1, 2, 3i, = h0, 1, 2i N B = h1, 1, 1iN . Moreover, (A B) = 1, (A B) = 2, (A B) = 3; (A B) = 1,(A B) = 0, (A B) = 1.Value Let N denote set submultisets N : i.e., packages comprised goodsN . denote v : N R function describes value bidding agent attributesviable package.TAC, agents objective compile packages = 8 individual clients.such, agents value function takes special form. client c characterizedvalue function vc : N R, agents value collection packages sum~ = (X1 , . . . , Xm ),clients respective values packages: given vector packages X~ =v(X)Xvc (Xc ).(7)c=1NPricelines buyer priceline good g vector p~g R+g , kth component,pgk , stores marginal cost agent acquiring kth unit good g. example,agent currently holds four units good g, four additional units gavailable costs $25, $40, $65, $100, corresponding buyer priceline (avector length 8) given p~g = h0, 0, 0, 0, 25, 40, 65, 100i. leading zeros indicatefour goods agent holds may acquired cost.assume buyer pricelines nondecreasing. Note assumption WLOG,since optimizing agent buys cheaper goods expensive ones.Given set buyer pricelines P = {~pg | g G}, define costs additively, is,cost goods multiset N given by:g,Costg (Y, P ) =YgXpgk ,XCostg (Y, P ).k=1Cost(Y, P ) =gG527(8)fiGreenwald, Lee, & NaroditskiyNseller priceline good g vector ~g R+g . Much like buyer priceline, kthcomponent seller priceline g stores marginal revenue agent could earnkth unit sells. example, market demands four units good g,sold prices $20, $15, $10, $5, corresponding seller priceline given~g = h20, 15, 10, 5, 0, 0, 0, 0i. Analogously buyer pricelines, tail zero revenuesindicates market demands four units.assume seller pricelines nonincreasing. Note assumption WLOG,since optimizing agent sells expensive goods cheaper ones.Given set seller pricelines = {~g | g G}, define revenue additively, is,revenue associated multiset Z N given by:g,Revenueg (Z, ) =ZgXgk ,XRevenueg (Z, ).(9)k=1Revenue(Z, ) =(10)gGpriceline constant, say prices linear. refer constant valueunit price. linear prices, cost acquiring k units good g k timesunit price good g.Bids agent submits bid expressing offers buy sell various units goodsmarketplace. divide two components h~b, ~ai, good g bidconsists buy offer, ~bg = hbg1 , . . . , bgNg i, sell offer, ~ag = hag1 , . . . , agNg i. bidprice bgk R+ (resp. agk R+ ) represents offer buy (sell) kth unit good gprice.definition, agent cannot buy (sell) kth unit unless also buys (sells) units1, . . . , k 1. accommodate fact, impose following constraint: Buy offers mustnonincreasing k, sell offers nondecreasing. addition, agent may offersell good less price willing buy good: i.e., bg1 < ag1 .Otherwise, would simultaneously buy sell good g. refer restrictionsbid monotonicity constraints.5.1.2 Pseudo-Auction RulesEquipped formalism, specify rules govern pseudo-auctions.true auction, outcome pseudo-auction dictates quantity goodexchange, prices, conditional agents bid. quantity issue resolvedwinner determination rule whereas price issue resolved payment rule.Definition 5.1 [Pseudo-Auction Winner Determination Rule] Given buyer seller pricelines P , bid = h~b, ~ai, agent buys multiset goods Buy(, P ) sellsmultiset goods Sell(, ),Buyg (, P ) = max k bgk pgkkSellg (, ) = max k agk gkk528fiRoxyBot-06Note monotonicity restrictions bids ensure agents offer betterequal price every unit exchanges, agent simultaneouslybuy sell good.least two alternative payment rules agent may face. first-pricepseudo-auction, agent pays bid price (for buy offers, paid bid price selloffers) good wins. second-price pseudo-auction, agent pays (or paid)prevailing prices, specified realized buyer seller pricelines. terminologyderives analogy standard first- second-price sealed bid auctions (Krishna,2002; Vickrey, 1961). mechanisms, high bidder single item pays bid (thefirst price), highest losing bid (the second price), respectively. salient propertyfirst-price pseudo-auctions, price set bid winner, whereassecond-price pseudo-auctions agents bid price determines whether winsprice pays.paper, focus second-price model. is, basic problem definitionspresume second-price auctions; however, bidding heuristics tailoredcase. true auctions, adopting second-price model pseudo-auctions simplifiesproblem bidder. also provides reasonable approximation situation facedTAC agents, argue:TAC entertainment auctions, agents submit bids (i.e., buy sell offers)form specified above. interpret agents buyer seller pricelinescurrent order book (not including agents bid), agents immediate winnings determined winner determination rule, paymentsaccording second-price rule (i.e., order-book prices prevail).TAC hotel auctions, buy bids allowed. Assuming orderbook reflects outstanding bids agents own, accurate buyerpriceline would indicate agent win k units good paysfork unitsa price (17 k)th existing (other-agent) offer. actualprice pays 16th-highest unit offer (including offer). Sinceagents bid may affect price,7 situation lies first-second-price characterizations pseudo-auctions described above.TAC flight auctions, agents may buy number units posted price.situation given time modeled exactly second-price pseudo-auctionabstraction.5.2 Bidding Problemsready discuss optimization module repeatedly employed RoxyBot-06within bidding cycle construct bids. key bidding decisions are: goodsbid on, price, when?7. two ways. First, agent may submit 16th-highest unit offer, case setsprice. Second, bids multiple units, number wins determines price-setting unit,thus affecting price winning units. Note second effect would present evenauction cleared 17th-highest price.529fiGreenwald, Lee, & NaroditskiyAlthough RoxyBot technically faces n-stage stochastic optimization problem, solvesproblem collapsing n stages two relevant stages, currentfuture, necessitating one stochastic model future prices (current prices known).simplification achieved ignoring potentially useful information hotelauctions close one one random, unspecified order, instead operating (likeTAC agents) assumption hotel auctions close end currentstage. Hence, one model hotel prices: stochastic model future prices.Moreover, pressing decisions regarding hotels goods bidprice. need reason timing hotel bid placement.contrast, since flight entertainment auctions clear continuously, trading agentreason relevant tradeoffs timing placement bids goods.Still, assumption hotel auctions close end current stage,future stages, hotel prices, hence hotel winnings, known, remainingdecisions flight entertainment tickets buy. rational agent timebids markets capitalize best prices. (The best prices minimabuying maxima selling.) Hence, suffices agents model future pricesmarkets predict best prices (conditioned current prices). is,suffices consider one stochastic pricing model. information necessary.established suffices RoxyBot pose solve two-stage, rathern-stage, stochastic optimization problem, proceed define abstract seriesproblems designed capture essence bidding uncertaintyTAC-like hybrid markets incorporate aspects simultaneous sequential, one-shotcontinuously-clearing, auctions. specifically, formulate problems twostage stochastic programs integer recourse (see book Birge & Louveaux, 1997,introduction stochastic programming).two-stage stochastic program, two decision-making stages, hence twosets variables: first- second-stage variables. objective maximize sumfirst-stage objectives (which depend first-stage variables) expectedvalue ensuing second-stage objectives (which depend first- secondstage variables). objective value second stage called recourse value,second-stage variables integer-valued, stochastic program saidinteger recourse.high-level, bidding problem formulated two-stage stochastic programfollows: first stage, current prices known future prices uncertain,bids selected; second stage, uncertainty resolved, goods exchanged.objective maximize expected value second-stage objective, namelysum inherent value final holdings profits earned, less first-stage costs.Since second stage involves integer-valued decisions (the bidder decides goodsbuy sell known prices), bidding problem one integer recourse.section, formulate series bidding problems two-stage stochastic programs integer recourse, one tailored different type auction mechanism,illustrating different type bidding decision. mechanisms study, inspiredTAC, one-shot continuously-clearing variants second-price pseudo-auctions.former, bids placed first stage; latter, opportunity530fiRoxyBot-06recourse. Ultimately, combine decision problems one unified problemcaptures mean bidding uncertainty.formal problem statements, rely following notation:Variables:Q1 multiset goods buyQ2 multiset goods buy laterR1 multiset goods sellR2 multiset goods sell laterConstants:P 1 set current buyer pricelinesP 2 set future buyer pricelines1 set current seller pricelines2 set future seller pricelinesNote P 1 1 always known, whereas P 2 2 uncertain first stageuncertainty resolved second stage.Flight Bidding Problem agents task bidding flight auctions decidemany flights buy current prices later lowest future prices, given (known)current prices stochastic model future prices. Although TAC unitsflight sell price one time, state flight bidding problemgenerally: allow different prices different units flight.Definition 5.2 [Continuously-Clearing, Buying] Given set current buyer pricelines P 1probability distribution f future buyer pricelines P 2 ,FLT(f ) = maxEP 2 f1nQ Zmaxv(Q1 Q2 ) Cost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 )2nQ Z(11)Note two cost terms referring future pricelines (Cost(, P 2 )). firstterms adds total cost goods bought first second stages.second term subtracts cost goods bought first stage. constructionensures that, agent buys k units good now, later purchases good incurcharges units (k + 1, k + 2, ...) goods future priceline.Entertainment Bidding Problem Abstractly, entertainment buying problemflight bidding problem. agent must decide many entertainment ticketsbuy current prices later lowest future prices. entertainment sellingproblem opposite buying problem. agent must decide many ticketssell current prices later highest future prices.531fiGreenwald, Lee, & NaroditskiyDefinition 5.3 [Continuously-Clearing, Buying Selling] Given set current buyerseller pricelines (P, )1 probability distribution f future buyer sellerpricelines (P, )2 ,ENT(f ) = max E(P,)2 fmax v((Q1 Q2 ) (R1 R2 ))Q1 ,R1 ZnQ2 ,R2 ZnCost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 )+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2 ) Revenue(R1 , 2 )(12)subject Q1 R1 Q1 Q2 R1 R2 , (P, )2 .constraints ensure agent sell units good buys.Hotel Bidding Problem Hotel auctions close fixed times, unknown order.Hence, iteration agents bidding cycle, one-shot auctions approximateauctions well. Unlike continuous setup, decisions madefirst second stages, one-shot setup, bids placed first stage;second stage, winnings determined evaluated.Definition 5.4 [One-Shot, Buying] Given probability distribution f future buyerpricelines P 2 ,(13)HOT(f ) = max EP 2 f v(Buy( 1 , P 2 )) Cost(Buy( 1 , P 2 ), P 2 )1 =h~b,0iHotel Bidding Problem, Selling Although possible agents sellTAC hotel auctions, one could imagine analogous auction setup possiblesell goods well buy them.Definition 5.5 [One-Shot, Buying Selling] Given probability distribution ffuture buyer seller pricelines (P, )2 ,max E(P,)2 f v(Buy( 1 , P 2 ) Sell( 1 , 2 )) Cost(Buy( 1 , P 2 ), P 2 ) + Revenue(Sell( 1 , 2 ), 2 )1 =h~b,~ai(14)12122subject Buy( , P ) Sell( , ), (P, ) .Bidding Problem Finally, present (a slight generalization of) TAC bidding problem combining four previous stochastic optimization problems one. abstractproblem models bidding buy sell goods via continuously-clearing one-shotsecond-price pseudo-auctions, follows:Definition 5.6 [Bidding Uncertainty] Given set current buyer seller pricelines (P, )1 probability distribution f future buyer seller pricelines (P, )2 ,BID(f ) =maxQ1 ,R1 Zn , 1 =h~b,~aiE(P,)2 fmaxQ2 ,R2 Znv((Q1 Q2 ) (R1 R2 ) Buy( 1 , P 2 ) Sell( 1 , P 2 ))Cost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 ) + Cost(Buy( 1 , P 2 ), P 2 )+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2 ) Revenue(R1 , 2 ) + Revenue(Sell( 1 , 2 ), 2 )(15)532fiRoxyBot-06subject Q1 R1 Q1 Q2 R1 R2 Buy( 1 , P 2 ) Sell( 1 , 2 ), (P, )2 .again, bidding problem (i) stochastic: takes input stochastic modelfuture prices; (ii) global: seamlessly integrates flight, hotel, entertainment biddingdecisions; (iii) dynamic: facilitates simultaneous reasoning current futurestages game.Next, describe various heuristic approaches solving problem biddinguncertainty.5.3 Bidding Heuristicssection, discuss two heuristic solutions bidding problem: specifically,expected value method (EVM), approach collapses stochastic information,sample average approximation (SAA), approach exploits stochastic informationcharacterizes RoxyBot-06.5.3.1 Expected Value Methodexpected value method (Birge & Louveaux, 1997) standard way approximatingsolution stochastic optimization problem. First, given distribution collapsedpoint estimate (e.g., mean); then, solution corresponding deterministic optimization problem output approximate solution original stochasticoptimization problem. Applying idea problem bidding uncertaintyyields:Definition 5.7 [Expected Value Method] Given probability distribution f buyerseller pricelines, expected values P 2 2 , respectively,BID EVM(P 2 , 2 ) =maxQ1 ,R1 Zn , 1 =h~b,~ai,Q2 ,R2 Znv((Q1 Q2 ) (R1 R2 ) (Buy( 1 , P 2 ) Sell( 1 , P 2 ))Cost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 ) + Cost(Buy( 1 , P 2 ), P 2 )+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2 ) Revenue(R1 , 2 ) + Revenue(Sell( 1 , 2 ), 2 )(16)subject Q1 R1 Q1 Q2 R1 R2 .practice, without full knowledge distribution f , cannot implementexpected value method; particular, cannot compute P 2 2 cannot solveBID EVM(P 2 , 2 ) exactly. can, however, solve approximation problemexpected buyer seller pricelines P 2 2 replaced average scenario(P 2 , 2 ) (i.e., average buyer seller pricelines), defined follows:P 2 =1X 2Pi ,2 =i=11X 2.i=1533fiGreenwald, Lee, & NaroditskiyAlgorithm 5 EVM(G, N, f, S)1: sample scenarios (P, )21 , . . . , (P, )2S fPPS22,2: BID EVMPi=1i=13: return5.3.2 Sample Average ApproximationLike expected value method, sample average approximation intuitive way approximating solution stochastic optimization problem. idea simple: (i) generateset sample scenarios, (ii) solve approximation problem incorporatessample scenarios. Applying SAA heuristic (see Algorithm 6) involves solvingfollowing approximation bidding problem:Definition 5.8 [Sample Average Approximation] Given set scenarios,(P, )21 , . . . , (P, )2S f ,BID SAA((P, )21 , . . . , (P, )2S ) =maxXmaxv((Q1 Q2 ) (R1 R2 ) (Buy( 1 , Pi2 ) Sell( 1 , Pi2 ))22nQ1 ,R1 Zn , 1 =h~b,~ai i=1 Q ,R Z111Cost(Q , P ) + Cost(Q Q2 , Pi2 ) Cost(Q1 , Pi2 ) + Cost(Buy( 1 , Pi2 ), Pi2 )+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2i ) Revenue(R1 , 2i ) + Revenue(Sell( 1 , 2i ), 2i )(17)subject Q1 R1 Q1 Q2 R1 R2 .Algorithm 6 SAA(G, N, f, S)1: sample scenarios (P, )21 , . . . , (P, )2S f2: BID SAA((P, )21 , . . . , (P, )2S )3: returnUsing theory large deviations, Ahmed Shapiro (2002) establish followingresult: , probability optimal solution sample average approximation stochastic program integer recourse optimal solution originalstochastic optimization problem approaches 1 exponentially fast. Given hard time spaceconstraints, however, always possible sample sufficiently many scenarios inferreasonable guarantees quality solution sample average approximation. Hence, propose modified SAA heuristic, SAA fed tailor-madeimportant scenarios, apply idea bidding problem.5.3.3 Modified Sample Average Approximationbids SAA places sample prices appear scenarios. SAA never bidshigher good highest sampled price, far knows, biddingprice enough win good scenarios. However, chance534fiRoxyBot-06highest sampled price falls clearing price. Let us compute probabilitycase single-unit auction, uniform-price multi-unit auction: i.e., oneunits good auctioned clear price.Let F denote cumulative distribution function predicted prices, let f denotecorresponding density function, let G denote cumulative distribution functionclearing prices. Using notation, term 1 G(x) probabilityclearing price greater x. Further, let X random variable representshighest value among sample price predictions. P (X x) = F (x)Sprobability samples (and hence highest among them) less x;P (X = x) = (F (x)S ) = S(F (x))S1 f (x) probability highest value amongsamples equals x. Putting two terms togethernamely, probabilityhighest sample price prediction exactly x, probability clearing price greaterxwe express probability highest SAAs sample price predictions lessclearing price follows:ZS(F (x))S1 f (x)(1 G(x))dx(18)Assuming perfect prediction (so G = F ), complex expression simplies follows:ZS(F (x))S1 f (x)(1 F (x))dxZZS1(F (x))S f (x)dx(F (x))f (x)dx=(F (x))S(F (x))S+1=S+11=S+1Hence, probability SAAs sample price predictions less clearingprice 1/(S + 1). particular, assuming perfect prediction clearing pricesTAC hotel auctions independent, probability SAA agent 49 scenariosbidding TAC Travel chance winning eight hotels (i.e., probabilitysample price least one scenarios greater clearing price)81= 0.988 0.85.1 49+1remedy situation, designed implemented simple variant SAARoxyBot-06. SAA* heuristic (see Algorithm 7) close cousin SAA, differencearising respective scenario sets.P Whereas SAA samples scenarios, SAA* samples|N | scenarios, |N | = g Ng . SAA* creates additional |N | scenariosfollows: unit k good g G, sets price kth unit good gupper limit range possible prices and, conditioning price setting, setsprices goods mean values. Next, describe experimentstest suite bidding heuristics, including SAA SAA*, controlled testing environment.535fiGreenwald, Lee, & NaroditskiyAlgorithm 7 SAA(G, N, f, S)Require: |N |1: hard-code |N | scenarios (P, )21 , . . . , (P, )2|N |2: sample |N | scenarios (P, )2|N |+1 , . . . , (P, )2S f3: BID SAA((P, )21 , . . . , (P, )2S )4: returnAgentSMUAMUPredictionsAverage scenarioscenariosTMUTMU*Average scenarioscenariosAverage scenarioBE*scenariosBidsMarginal utilitiesCalculates marginal utilities scenarioBids average marginal utilities across scenariosMarginal utilitiesBest TMU solutionsMarginal utilities, assuming goodstarget set availableBest TMU*solutionsgoodsgoodsGoods target setGoods target setGoods target setGoods target setTable 3: Marginal-utility-based agents. marginal utility good definedincremental utility achieved winning good, relativeutility set goods already held.5.4 Summarysection, developed series bidding problems, heuristics solutionsproblems, captures essence bidding one-shot continuously-clearingauctions characterize TAC. bulk presentation deliberately abstract,suggest problems solutions applicable well beyond realmTAC: e.g., bidding interdependent goods separate eBay auctions. Still, remainsvalidate approach application domains.6. Experimentsclose paper two sets experimental results, first controlled testingenvironment, second results final round 2006 TAC Travel competition. combined strategy hotel price prediction via SimAA bid optimizationvia SAA emerged victorious settings.6.1 Controlled Experimentsextent least, approach bidding validated successRoxyBot-06 TAC-06. Nonetheless, ran simulations controlled testing environmentvalidate approach. results reported Lee (2007) Greenwald etal. (2008), summarize well.536fiRoxyBot-06built test suite agents, predict using RoxyBot-06s SimAA randommechanism distribution. agents differ bidding strategies; possibilitiesinclude SAA,8 SAA*, six marginal-utility-based heuristics studied Wellman etal. (2007), summarized Table 3.experiments conducted TAC Travel-like setting, modified removeaspects game would obscure controlled study bidding. Specifically,eliminated flight entertainment trading, endowed agents eight flightseight flights day. Further, assumed hotels closed one roundbidding (i.e., hotel auctions one-shot, ensuing bid optimization problemadheres Definition 5.4).designed two sets experiments: one decision-theoretic one game-theoretic.former, hotel clearing prices outcome simulation simultaneous ascendingauctions, depend actual clients game, random sampling. (Oursimulator informed individual agents.) latter, hotel clearing pricesdetermined bids agents submit using mechanism TAC Travel:clearing price 16th highest bid (or zero, fewer 16 bids submitted).first ran experiments 8 agents per game, found hotel pricesoften zero: i.e., insufficient competition. changed setup includerandom number agents drawn binomial distribution n = 32 p = 0.5,requisite number agents sampled uniformly replacement set possibleagents. agents first sample number competitors binomial distribution,generate scenarios assuming sampled number competitors.game-theoretic nature TAC, individual agents performancedepend heavily agents included agent pool. experiments,attempted mitigate artificial effects specific agents chose includepool sampling agents pool play game, replacement. Thus,agents average score games measure agents performancevarious combinations opponents.Figures 3(a) 3(b), plot mean scores obtained agent typesetting, along 95% confidence intervals. averages computed based 1000independent observations, obtained playing 1000 games. Scores averaged acrossagent types game account game dependencies. SAAB SAAT9best performing agents game-theoretic experiments among bestdecision-theoretic setting.6.2 TAC 2006 Competition ResultsTable 4 lists agents entered TAC-06 Table 5 summarizes outcome.TAC-06 finals comprised 165 games three days, 80 games last dayweighted 1.5 times much 85 first two days. first day finals,RoxyBot finished third, behind Mertacor Walverinethe top scorers 2005. happens,RoxyBots optimization routine, designed stochastic hotel entertainment8. particular implementation details explaining RoxyBot-06 applied SAA TAC domainrelegated Appendix A.9. SAAB SAA, SAAT slight variant SAA*. See paper Greenwald et al. (2008)details.537fiGreenwald, Lee, & Naroditskiy10.850.95Score (thousands)Score (thousands)0.80.750.70.650.60.90.850.80.750.70.550.650.50.6SAAT SAABTMUTMU*AgentBE*AMUSMU(a) Decision-theoretic settingSAAT SAABTMUTMU*AgentBE*AMUSMU(b) Game-theoretic settingFigure 3: Mean scores confidence intervals.price predictions, accidentally fed deterministic predictions (i.e., point price estimates)entertainment. Moreover, predictions fixed, rather adapted basedrecent game history.days 2 3, RoxyBot ran properly, basing bidding auctions stochasticinformation. Moreover, agent upgraded day 1 bid flights once,twice, minute. enabled agent delay bidding somewhatend game flights whose prices decreasing. doubt minor modificationenabled RoxyBot emerge victorious 2006, edging Walverine whisker,integer precision reported Table 5. actual margin 0.22a mere 22 parts400,000. Adjusting control variates (Ross, 2002) spreads top two finishers bitfurther.10Agent006kin agentL-AgentMertacorRoxyBotUTTAWalverineWhiteDolphinAffiliationSwedish Inst Comp SciU MacauCarnegie Mellon UAristotle U ThessalonikiBrown UU TehranU MichiganU SouthamptonReferenceAurell et al., 2002Sardinha et al., 2005Toulis et al., 2006; Kehagias et al., 2006Greenwald et al., 2003, 2004, 2005; Lee et al., 2007Cheng et al., 2005; Wellman et al., 2005& Jennings, 2002; Vetsikas & Selman, 2002Table 4: TAC-06 participants.10. Kevin Lochner computed adjustment factors using method described Wellman et al. (2007,ch. 8).538fiRoxyBot-06AgentRoxyBotWalverineWhiteDolphin006MertacorL-Agentkin agentUTTAFinals40324032393639023880386037252680Adjustment Factor517227167014Table 5: TAC-06 final scores, adjustment factors based control variates.Mean scores, utilities, costs (with 95% confidence intervals) last dayTAC-06 finals (80 games) plotted Figure 4 detailed statistics tabulatedTable 6. single metric low hotel flight costs responsibleRoxyBots success. Rather success derives right balance contradictory goals.particular, RoxyBot incurs high hotel mid-range flight costs achieving mid-rangetrip penalty high event profit.11Let us compare RoxyBot two closest rivals: Walverine WhiteDolphin. ComparingWalverine first, Walverine bids lower prices (by 55) fewer hotels (49 less), yet wins (0.8)wastes less (0.42). would appear Walverines hotel bidding strategy outperformsRoxyBots, except RoxyBot earns higher hotel bonus (15 more). RoxyBot also gainsadvantage spending 40 less flights earning 24 total entertainment profit.different competition takes place RoxyBot WhiteDolphin. WhiteDolphinbids lower prices (120 less) hotels (by 52) RoxyBot. RoxyBot spends much(220) hotels WhiteDolphin makes earning higher hotel bonus (by96) lower trip penalty (by 153). seems WhiteDolphins strategy minimizecosts even means sacrificing utility.6.3 Summaryalready noted, TAC Travel bidding, viewed optimization problem, n-stagedecision problem. solve n-stage decision problem sequence 2-stage decisionproblems. controlled experiments reported section establish biddingstrategy, SAA, best test suite setting designed,2 stages. TAC competition results establish strategy also effectiven-stage setting.7. Collective Behaviorhotel price prediction techniques described Section 4.2 designed compute (orleast approximate) competitive equilibrium prices without full knowledge client pop11. agent suffers trip penalties extent assigns clients packages differpreferred.539fiGreenwald, Lee, & Naroditskiy# Hotel BidsAverage Hotel Bids# HotelsHotel Costs# Unused HotelsHotel BonusTrip PenaltyFlight CostsEvent ProfitsEvent BonusTotal Event ProfitsAverage UtilityAverage CostAverage ScoreRox13017015.9911022.24613296461511014701580978756084179Wal8111516.7910651.8259828146552615301556984756934154Whi1825023.218829.485174494592615291535959754684130SIC3351313.6810310.496173404729-614981492977557654010Mer9414718.449024.86590380483412313691492957956283951L-A588814.899871.895923884525-9313991306960456053999kin1535615.0511850.006011454867-162161914571007562133862UTT244989.397860.484242133199-4996992660739892618Table 6: 2006 Finals, Last day. Tabulated Statistics. omit first two daysagents vary across days, cannot vary within. Presumably, entrieslast day teams preferred versions agents.2006 Finals, Last Day2006 Finals, Last Day2006 Finals, Last Day4.56.51063.539872.5625Rox Wal Whi SIC Mer LA kin UTTAgentCost (thousands)Utility (thousands)Score (thousands)45.554.543.5Rox Wal Whi SIC Mer LA kin UTTAgent3Rox Wal Whi SIC Mer LA kin UTTAgentFigure 4: 2006 Finals, Last day. Mean scores, utilities, costs, 95% confidenceintervals.ulation. section, assume knowledge view output tatonnementSimAA calculations predictions ground truth. compare actualprices final games ground truth respective years since 2002 determinewhether TAC market prices resemble CE prices. find depicted Figure 5.nature methods, calculations pertain hotel prices only.results highly correlated metrics (Euclidean distance EVPP).observe accuracy CE price calculations varied year year. 2003year TAC Supply Chain Management (SCM) introduced. Manyparticipants diverted attention away Travel towards SCM year, perhapsleading degraded performance Travel. Things seem improve 2004 2005.540fiRoxyBot-06cannot explain setback 2006, except noting performance highly dependentparticular agent pool, 2006 fewer agents pool.26045tatonnement, exactsimAA, exactExpected Value Perfect Prediction240Euclidean Distance220200180160140120100200220032004Year20052006tatonnement, exactsimAA, expact4035302520200220032004Year20052006Figure 5: comparison actual (hotel) prices output competitive equilibriumprice calculations final games since 2002. label exact means: fullknowledge client population.8. Conclusionforemost aim trading agent research develop body techniques effectivedesign analysis trading agents. Contributions trading agent design includeinvention trading strategies, together models algorithms realizingcomputation methods measure evaluate performance agents characterizedstrategies. Researchers seek specific solutions particular trading problemsgeneral principles guide development trading agents across market scenarios.paper purports contribute research agenda. described designimplementation RoxyBot-06, able trading agent demonstrated performanceTAC-06.Although automated trading electronic markets yet fully taken hold,trend well underway. TAC, trading agent community demonstratingpotential autonomous bidders make pivotal trading decisions effective way.agents offer potential accelerate automation trading broadly,thus shape future commerce.Acknowledgmentspaper extends work Lee et al. (2007). material Section 5.1 basedbook Wellman et al. (2007). grateful several anonymous reviewers whoseconstructive criticisms enhanced quality work. research supportedNSF Career Grant #IIS-0133689.541fiGreenwald, Lee, & NaroditskiyAppendix A. TAC Bidding Problem: SAAproblem bidding simultaneous auctions characterize TAC formulated two-stage stochastic program. appendix, present implementationdetails integer linear program (ILP) encoded RoxyBot-06 approximatesoptimal solution stochastic program.12formulate ILP assuming current prices known, future prices uncertain first stage revealed second stage. Note whenever pricesknown, suffices agent make decisions quantity good buy,rather bid amounts, since choosing bid amount greaterequal price good equivalent decision buy good.Unlike main body paper, ILP formulation bidding TAC assumeslinear prices. Table 7 lists price constants decision variables auction type.hotels, decisions pertain buy offers; flights, agent decides manytickets buy many buy later; entertainment events, agent choosessell quantities well buy quantities.HotelsbidPriceYasFlights Eventsbuybuy laterEventssellsell laterVariable (bid)apqPriceYasPriceNaZasVariable (qty)Variable (qty)Table 7: Auction types associated price constants decision variables.A.1 Index Setsindexes set goods, auctions.af Af indexes set flight auctions.ah Ah indexes set hotel auctions.ae Ae indexes set event auctions.c C indexes set clients.p P indexes set prices.12. precise formulation RoxyBot-06s bidding ILP appears paper Lee et al. (2007).formulation slightly simplified, expect would perform comparably TAC. keydifferences flight entertainment bidding.542fiRoxyBot-06q Q indexes set quantities(i.e., units good auction).indexes set scenarios.indexes set trips.A.2 ConstantsGat indicates quantity good required complete trip t.indicates current buy price af , ae .Na indicates current sell price ae .Yas indicates future buy price af , ah , ae scenario s.Zas indicates future sell price ae scenario s.Ha indicates hypothetical quantity hotel ah .Oa indicates quantity good agent owns.Uct indicates client cs value trip t.A.3 Decision Variables= {cst } set boolean variables indicating whether client c allocatedtrip scenario s.= {apq } set boolean variables indicating whether bid price p qthunit ah .= {a } set integer variables indicating many units af , ae buy now.N = {a } set integer variables indicating many units ae sell now.= {as } set integer variables indicating many units af , ae buy laterscenario s.Z = {as } set integer variables indicating many units ae sell laterscenario s.A.4 Objective Functionflight costcurrent}| future {hotel costz }| {z X }|{X z }| { z }| {XX+ YasUct ctsYas apq +max,,M,N,Y,ZAfAh ,Q,pYasC,Ttrip valuez543(19)fiGreenwald, Lee, & Naroditskiyevent revenueevent cost}|{ zz}|{currentfuturefuturecurrentz}|{zz}|{}|{}| {zXNa + Zas YasAeA.5 ConstraintsXcst 1 c C,(20)allocationbuyz }| { z}|{z }| {Xcst Gat Oa + (a + )Af ,(21)Ah ,(22)C,Tbuyallocationz }| { z}|{z X}|{Xcst Gat Oa +apqC,TQ,pYasallocationsellbuyz }| { z}|{z }| {Xz }| {cst Gat Oa + + +C,TAe ,Xapq Ha(23)Ah(24)apq 1 Ah , q Q(25)P,QXPEquation (20) limits client one trip scenario. Equation (21) preventsagent allocating flights buy. Equation (22) prevents agentallocating hotels buy. Equation (23) prevents agentallocating event tickets buy sell. Equation (24) ensuresagent bids least HQW units hotel auction. Equation (25) prevents agentplacing one buy offer per unit hotel auction.agent might also constrained place sell offers units goodowns, and/or place buy (sell) offers units goodmarket supplies (demands).Note need explicitly enforce bid monotonicity constraintsILP formulation:Buy offers must nonincreasing k, sell offers nondecreasing.ILP need constraint prices assumed linear.effect, decisions ILP makes many units good bidon. Hence, bids (10, 15, 20) (20, 15, 10) equivalent.agent may offer sell less price willing buy.544fiRoxyBot-06ILP would choose place buy offer sell offer goodbuy price good exceeds sell price, would unprofitable.ReferencesAhmed, S., & Shapiro, A. (2002). sample average approximation methodstochastic programs integer recourse. Optimization Online, http://www.optimization-online.org.Arunachalam, R., & Sadeh, N. M. (2005). supply chain trading agent competition.Electronic Commerce Research Applications, 4 (1), 6684.Aurell, E., Boman, M., Carlsson, M., Eriksson, J., Finne, N., Janson, S., Kreuger, P., &Rasmusson, L. (2002). trading agent built constraint programming. EighthInternational Conference Society Computational Economics: ComputingEconomics Finance, Aix-en-Provence.Birge, J., & Louveaux, F. (1997). Introduction Stochastic Programming. Springer, NewYork.Cai, K., Gerding, E., McBurney, P., Niu, J., Parsons, S., & S.Phelps (2009). OverviewCAT: market design competition. Tech. rep. ULCS-09-005, University Liverpool.Cheng, S., Leung, E., Lochner, K., K.OMalley, Reeves, D., Schvartzman, L., & Wellman,M. (2003). Walverine: Walrasian trading agent. Proceedings SecondInternational Joint Conference Autonomous Agents Multi-Agent Systems, pp.465472.Cheng, S., Leung, E., Lochner, K., K.OMalley, Reeves, D., Schvartzman, L., & Wellman,M. (2005). Walverine: Walrasian trading agent. Decision Support Systems, 39 (2),169184.Cramton, P. (2006). Simultaneous ascending auctions. Cramton, P., Shoham, Y., &Steinberg, R. (Eds.), Combinatorial Auctions. MIT Press.Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering successful TAC participation. Proceedings First International Joint Conference AutonomousAgents Multiagent Systems, pp. 4546.Greenwald, A. (2003). Bidding marginal utility simultaneous auctions. WorkshopTrading Agent Design Analysis.Greenwald, A., & Boyan, J. (2004). Bidding uncertainty: Theory experiments.Proceedings 20th Conference Uncertainty Artificial Intelligence, pp.209216.Greenwald, A., Naroditskiy, V., & Lee, S. (2008). Bidding heuristics simultaneousauctions: Lessons tac travel. Workshop Trading Agent Design Analysis.Greenwald, A., & Boyan, J. (2005). Bidding algorithms simultaneous auctions: casestudy. Journal Autonomous Agents Multiagent Systems, 10 (1), 6789.He, M., & Jennings, N. (2002). SouthamptonTAC: Designing successful trading agent.Proceedings Fifteenth European Conference Artificial Intelligence, pp. 812.545fiGreenwald, Lee, & NaroditskiyJordan, P. R., & Wellman, M. P. (2009). Designing ad auctions game tradingagent competition. Workshop Trading Agent Design Analysis.Kehagias, D., Toulis, P., & Mitkas, P. (2006). long-term profit seeking strategy continuous double auctions trading agent competition. Fourth Hellenic ConferenceArtificial Intelligence, Heraklion.Krishna, V. (2002). Auction Theory. Academic Press.Lee, S. J. (2007). Comparison bidding algorithms simultaneous auctions. B.S. honors thesis, Brown University, http://list.cs.brown.edu/publications/theses/ugrad/.Lee, S., Greenwald, A., & Naroditskiy, V. (2007). Roxybot-06: (SAA)2 TAC travel agent.Proceedings 20th International Joint Conference Artificial Intelligence,pp. 13781383.Ross, S. M. (2002). Simulation (Third edition). Academic Press.Sardinha, J. A. R. P., Milidiu, R. L., Paranhos, P. M., Cunha, P. M., & de Lucena, C.J. P. (2005). agent based architecture highly competitive electronic markets.Proceedings Eighteenth International Florida Artificial Intelligence ResearchSociety Conference, Clearwater Beach, Florida, USA, pp. 326332.Toulis, P., Kehagias, D., & Mitkas, P. (2006). Mertacor: successful autonomous tradingagent. Fifth International Joint Conference Autonomous Agents MultiagentSystems, pp. 11911198, Hakodate.Vetsikas, I., & Selman, B. (2002). WhiteBear: empirical study design tradeoffs autonomous trading agents. Workshop Game-Theoretic Decision-Theoretic Agents.Vickrey, W. (1961). Counterspeculation, auctions, competitive sealed tenders. JournalFinance, 16, 837.Walras, L. (1874). Elements deconomie politique pure. L. Corbaz, Lausanne.Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: StrategiesLessons Trading Agent Competition. MIT Press.Wellman, M. P., Reeves, D. M., Lochner, K. M., & Suri, R. (2005). Searching Walverine2005. Workshop Trading Agent Design Analysis, No. 3937 Lecture NotesArtificial Intelligence, pp. 157170. Springer.Wellman, M., Reeves, D., Lochner, K., & Vorobeychik, Y. (2004). Price predictionTrading Agent Competition. Artificial Intelligence Research, 21, 1936.546fiJournal Artificial Intelligence Research 36 (2009) 341-385Submitted 05/09; published 11/09Multilingual Part-of-Speech Tagging: Two Unsupervised ApproachesTahira NaseemBenjamin SnyderJacob EisensteinRegina BarzilayTAHIRA @ CSAIL . MIT. EDUBSNYDER @ CSAIL . MIT. EDUJACOBE @ CSAIL . MIT. EDUREGINA @ CSAIL . MIT. EDUComputer Science Artificial Intelligence LaboratoryMassachusetts Institute Technology77 Massachusetts Avenue, Cambridge 02139Abstractdemonstrate effectiveness multilingual learning unsupervised part-of-speech tagging. central assumption work combining cues multiple languages,structure becomes apparent. consider two ways applying intuitionproblem unsupervised part-of-speech tagging: model directly merges tag structurespair languages single sequence second model instead incorporates multilingual context using latent variables. approaches formulated hierarchical Bayesianmodels, using Markov Chain Monte Carlo sampling techniques inference. results demonstrate incorporating multilingual evidence achieve impressive performance gainsacross range scenarios. also found performance improves steadily numberavailable languages increases.1. Introductionpaper, explore application multilingual learning part-of-speech taggingannotation available.1 fundamental idea upon work based patternsambiguity inherent part-of-speech tag assignments differ across languages. lexical level,word part-of-speech tag ambiguity one language may correspond unambiguous wordlanguage. example, word English may function auxiliary verb,noun, regular verb. However, many languages likely express differentsenses three distinct lexemes. Languages also differ patterns structural ambiguity.example, presence article English greatly reduces ambiguity succeedingtag. languages without articles, however, constraint obviously absent. key ideamultilingual learning combining natural cues multiple languages, structurebecomes apparent.Even expressing meaning, languages take different syntactic routes, leadingcross-lingual variation part-of-speech patterns. Therefore, effective multilingual model mustaccurately represent common linguistic structure, yet remain flexible idiosyncrasieslanguage. tension becomes stronger additional languages added mix. Thus,key challenge multilingual learning capture cross-lingual correlations preservingindividual language tagsets, tag selections, tag orderings.1. Code,datasets,rawoutputshttp://groups.csail.mit.edu/rbg/code/multiling pos.c2009AI Access Foundation. rights reserved.experimentsavailablefiNASEEM , NYDER , E ISENSTEIN & BARZILAYpaper, explore two different approaches modeling cross-lingual correlations.first approach directly merges pairs tag sequences single bilingual sequence, employingjoint distributions aligned tag-pairs; unaligned tags, language-specific distributions stillused. second approach models multilingual context using latent variables instead explicitnode merging. group aligned words, multilingual context encapsulated valuecorresponding latent variable. Conditioned latent variable, tagging decisionslanguage remain independent. contrast first model, architecture hidden variablemodel allows scale gracefully number languages increases.approaches formulated hierarchical Bayesian models underlying trigramHMM substructure language. first model operates simple directed graphicalmodel one additional coupling parameter beyond transition emission parametersused monolingual HMMs. latent variable model, hand, formulatednon-parametric model; viewed performing multilingual clustering aligned setstag variables. latent variable value indexes separate distribution tags language,appropriate given context. models, perform inference using Markov Chain MonteCarlo sampling techniques.evaluate models parallel corpus eight languages: Bulgarian, Czech, English,Estonian, Hungarian, Romanian, Serbian, Slovene. consider range scenarios varycombinations bilingual models single model jointly trained eight languages. results show consistent robust improvements monolingual baselinealmost combinations languages. complete tag lexicon available latent variable model trained using eight languages, average performance increases 91.1% accuracy95%, halving gap unsupervised supervised performance. realistic cases, lexicon restricted frequently occurring words, see even largergaps monolingual multilingual performance. one scenario, average multilingual performance increases 82.8% monolingual baseline 74.8%. languagepairs, improvement especially noteworthy; instance, complete lexicon scenario, Serbianimproves 84.5% 94.5% paired English.find scenarios latent variable model achieves higher performancemerged structure model, even restricted pairs languages. Moreover hiddenvariable model effectively accommodate large numbers languages makesdesirable framework multilingual learning. However, observe latent variable modelsomewhat sensitive lexicon coverage. performance merged structure model,hand, robust respect. case drastically reduced lexicon (with 100words only), performance clearly better hidden variable model. indicatesmerged structure model might better choice languages lack lexicon resources.surprising discovery experiments marked variation level improvementacross language pairs. best pairing language chosen oracle, average bilingualperformance reaches 95.4%, compared average performance 93.1% across pairs.experiments demonstrate variability influenced cross-lingual links languageswell model consideration. identify several factors contributesuccess language pairings, none uniquely predict supplementary languagehelpful. results suggest multi-parallel corpora available, modelsimultaneously exploits languages latent variable model proposed342fiM ULTILINGUAL PART- -S PEECH TAGGINGpreferable strategy selects one bilingual models. found performance tendsimproves steadily number available languages increases.realistic scenarios, tagging resources number languages may already available.models easily exploit amount tagged data subset available languages.experiments show, annotation added, performance increases even languageslacking resources.remainder paper structured follows. Section 2 compares approachprevious work multilingual learning unsupervised part-of-speech tagging. Section 3 presentstwo approaches modeling multilingual tag sequences, along inference proceduresimplementation details. Section 4 describes corpora used experiments, preprocessing stepsvarious evaluation scenarios. results experiments analysis givenSections 5, 6. summarize contributions consider directions future workSection 7.2. Related Workidentify two broad areas related work: multilingual learning inducing part-of-speech tagswithout labeled data. discussion multilingual learning focuses unsupervised approachesincorporate two languages. describe related work unsupervised semisupervised models part-of-speech tagging.2.1 Multilingual Learningpotential multilingual data rich source linguistic knowledge recognized sinceearly days empirical natural language processing. patterns ambiguity vary greatlyacross languages, unannotated multilingual data serve learning signal unsupervisedsetting. especially interested methods leverage two languages jointly,compare approach relevant prior work.Multilingual learning may also applied semi-supervised setting, typically projectingannotations across parallel corpus another language resources exist (e.g.,Yarowsky, Ngai, & Wicentowski, 2000; Diab & Resnik, 2002; Pado & Lapata, 2006; Xi & Hwa,2005). primary focus unsupervised induction cross-linguistic structures,address area.2.1.1 B ILINGUAL L EARNINGWord sense disambiguation (WSD) among first successful applications automated multilingual learning (Dagan et al., 1991; Brown et al., 1991). Lexical ambiguity differs across languagessense polysemous word one language may translate distinct counterpart anotherlanguage. makes possible use aligned foreign-language words source noisy supervision. Bilingual data leveraged way variety WSD models (Brown et al.,1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002;Bhattacharya, Getoor, & Bengio, 2004), quality supervision provided multilingualdata closely approximates manual annotation (Ng et al., 2003). Polysemy one sourceambiguity part-of-speech tagging; thus model implicitly leverages multilingual WSDcontext higher-level syntactic analysis.343fiNASEEM , NYDER , E ISENSTEIN & BARZILAYMultilingual learning previously applied syntactic analysis; pioneering effortinversion transduction grammar Wu (1995). method trained unannotated parallelcorpus using probabilistic bilingual lexicon deterministic constraints bilingual tree structures. inside-outside algorithm (Baker, 1979) used learn parameters manually specifiedbilingual grammar. ideas extended subsequent work synchronous grammar induction hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005).One characteristic family methods designed inherently multilingual tasks machine translation lexicon induction. share goal learningmultilingual data, seek induce monolingual syntactic structures applied evenmultilingual data unavailable.respect, approach closer unsupervised multilingual grammar induction workKuhn (2004). Starting hypothesis trees induced parallel sentencesexhibit cross-lingual structural similarities, Kuhn uses word-level alignments constrain setplausible syntactic constituents. constraints implemented hand-crafted deterministic rules, incorporated expectation-maximization grammar induction assign zerolikelihood illegal bracketings. probabilities productions estimated separatelylanguage, applied monolingual data directly. Kuhn shows formmultilingual training yields better monolingual parsing performance.methods incorporate cross-lingual information fundamentally different manner. Ratherusing hand-crafted deterministic rules may require modification languagepair estimate probabilistic multilingual patterns directly data. Moreover, estimationmultilingual patterns incorporated directly tagging model itself.Finally, multilingual learning recently applied unsupervised morphological segmentation (Snyder & Barzilay, 2008). research related, moving morphologicalsyntactic analysis imposes new challenges. One key difference Snyder & Barzilay modelmorphemes unigrams, ignoring transitions morphemes. syntactic analysis, transition information provides crucial constraint, requiring fundamentally different model structure.2.1.2 B EYOND B ILINGUAL L EARNINGwork multilingual learning focuses bilingual analysis, models operateone pair languages. instance, Genzel (2005) describes method inducingmultilingual lexicon group related languages. work first induces bilingual modelspair languages combines them. take different approach simultaneouslylearning languages, rather combining bilingual results.related thread research multi-source machine translation (Och & Ney, 2001; Utiyama& Isahara, 2006; Cohn & Lapata, 2007; Chen, Eisele, & Kay, 2008; Bertoldi, Barbaiani, Federico,& Cattoni, 2008) goal translate multiple source languages single targetlanguage. using multi-source corpora, systems alleviate sparseness increase translation coverage, thereby improving overall translation accuracy. Typically, multi-source translationsystems build separate bilingual models select final translation output. instance, method developed Och Ney (2001) generates several alternative translationssource sentences expressed different languages selects likely candidate. CohnLapata (2007) consider different generative model: rather combining alternative sentencetranslations post-processing step, model estimates target phrase translation distribu-344fiM ULTILINGUAL PART- -S PEECH TAGGINGtion marginalizing multiple translations various source languages. modelcombines multilingual information phrase level, core estimates phrase tablesobtained using bilingual models.contrast, present approach unsupervised multilingual learning builds singlejoint model across languages. makes maximal use unlabeled data sidestepsdifficult problem combining output multiple bilingual systems without supervision.2.2 Unsupervised Part-of-Speech TaggingUnsupervised part-of-speech tagging involves predicting tags words, without annotationscorrect tags word tokens. Generally speaking, unsupervised setting permituse declarative knowledge relationship tags word types, formdictionary permissible tags common words. setup referred semisupervised Toutanova Johnson (2008), considered unsupervised papers topic (e.g., Goldwater & Griffiths, 2007). evaluation considers tag dictionariesvarying levels coverage.Since work Merialdo (1994), hidden Markov model (HMM) common representation2 unsupervised tagging (Banko & Moore, 2004). Part-of-speech tagsencoded linear chain hidden variables, words treated emitted observations. Recentadvances include use fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007),places prior distributions tag transition word-emission probabilities. Bayesianpriors permit integration parameter settings, yielding models perform well across rangesettings. particularly important case small datasets, many countsused maximum-likelihood parameter estimation sparse. Bayesian setting also facilitates integration data sources, thus serves departure point work.Several recent papers explored development alternative training proceduresmodel structures effort incorporate expressive features permitted generative HMM. Smith Eisner (2005) maintain HMM structure, incorporate large numberoverlapping features conditional log-linear formulation. Contrastive estimation usedprovide training criterion maximizes probability observed sentences comparedset similar sentences created perturbing word order. use large set featuresdiscriminative training procedure led strong performance gains.Toutanova Johnson (2008) propose LDA-style model unsupervised part-of-speechtagging, grouping words latent layer ambiguity classes. ambiguity class corresponds set permissible tags; many languages set tightly constrained morphological features, thus allowing incomplete tagging lexicon expanded. Haghighi Klein(2006) also use variety morphological features, learning undirected Markov Random Fieldpermits overlapping features. propagate information small number labeled prototype examples using distributional similarity prototype non-prototype words.focus effectively incorporate multilingual evidence, require simple modeleasily applied multiple languages widely varying structural properties. viewdirection orthogonal refining monolingual tagging models particular language.2. addition basic HMM architecture, part-of-speech tagging approaches explored (Brill, 1995;Mihalcea, 2004)345fiNASEEM , NYDER , E ISENSTEIN & BARZILAY((b))lvefhrelJ(celnrveflJhepenp)llvefhJrepenpnhevgujhechchlnhFigure 1: Example graphical structures (a) two standard monolingual HMMs, (b) mergednode model, (c) latent variable model three superlingual variables.3. Modelsmotivating hypothesis work patterns ambiguity part-of-speech level differacross languages systematic ways. considering multiple languages simultaneously, totalinherent ambiguity reduced language. potential advantages leveragingmultilingual information comes challenge respecting language-specific characteristicstag inventory, selection order. end, develop models jointly tag parallel streamstext multiple languages, maintaining language-specific tag sets parameterstransitions emissions.346fiM ULTILINGUAL PART- -S PEECH TAGGINGPart-of-speech tags reflect syntactic semantic function tagged words. Acrosslanguages, pairs word tokens known share semantic syntactic functiontags related systematic ways. word alignment task machine translationidentify pairs words parallel sentences. Aligned word pairs serve cross-lingualanchors model, allowing information shared via joint tagging decisions. Researchmachine translation produced robust tools identifying word alignments; use toolblack box treat output fixed, observed property parallel data.Given set parallel sentences, posit hidden Markov model (HMM) language,hidden states represent tags emissions words. unsupervisedmonolingual setting, inference part-of-speech tags performed jointly estimationparameters governing relationship tags words (the emission probabilities) consecutive tags (the transition probabilities). multilingual models built uponstructural foundation, emission transition parameters retain identical interpretation monolingual setting. Thus, parameters learned parallel textlater applied monolingual data.consider two alternative approaches incorporating cross-lingual information. firstmodel, tags aligned words merged single bi-tag nodes; second, latent variablemodel, additional layer hidden superlingual tags instead exerts influence tags clustersaligned words. first model primarily designed bilingual data, second modeloperates number languages. Figure 1 provides graphical model representationmonolingual, merged node, latent variable models instantiated single parallel sentence.merged node latent variable approaches formalized hierarchical Bayesianmodels. provides principled probabilistic framework integrating multiple sourcesinformation, offers well-studied inference techniques. Table 1 summarizes mathematicalnotation used throughout section. describe model depth.3.1 Bilingual Unsupervised Tagging: Merged Node Modelbilingual merged node model, cross-lingual context incorporated creating joint bi-tagnodes aligned words. would strong insist aligned words identicaltag; indeed, may even appropriate assume two languages share identical tag sets.However, two words aligned, want choose tags jointly. enable this,allow values bi-tag nodes range possible tag pairs ht, ,represent tagsets language.tags need identical, believe systematically related.modeled using coupling distribution , multinomial tag pairs.parameter combined standard transition distribution product-of-experts model.Thus, aligned tag pair hyi , yj conditioned predecessors yi1 yj1, well3coupling parameter (yi , yj ). coupled bi-tag nodes serve bilingual anchors dueMarkov dependency structure, even unaligned words may benefit cross-lingual informationpropagates nodes.3. describing merged node model, consider two languages , use simplified notationwrite hy, mean hy , i. Similar abbreviations used language-indexed parameters.347fiNASEEM , NYDER , E ISENSTEIN & BARZILAYNotation used modelsxiyia,00ith word token sentence language .ith tag sentence language .word alignments language pair h, i.transition distribution (over tags), conditioned tag language . describe bigram transition model, though implementation uses trigrams (without bigram interpolations); extension trivial.emission distribution (over words), conditioned taglanguage .parameter symmetric Dirichlet prior transitiondistributions.parameter symmetric Dirichlet prior emissiondistributions.Notation used merged node model0Abcoupling parameter assigns probability mass pairaligned tags.Dirichlet prior coupling parameter.Distribution bilingual alignments.Notation used latent variable modelzjz = hz1 , z2 , . . . , znG0multinomial superlingual tags z.concentration parameter , controlling much probability mass allocated first values.setting j th superlingual tag, ranging set integers, indexing distribution set .z th set distributions tags languages 1n .base distribution z drawn, whose formset n symmetric Dirichlet distributions parameter0 .Distribution multilingual alignments.Table 1: Summary notation used description models. sentence treatedisolation (conditioned parameters), sentence indexing left implicit.348fiM ULTILINGUAL PART- -S PEECH TAGGINGpresent generative account words sentence parametersmodel produced. generative story forms basis sampling-based inferenceprocedure.3.1.1 ERGED N ODE ODEL : G ENERATIVE TORYgenerative story assumes existence two tagsets, , two vocabularies WW one language. ease exposition, formulate model bigramtag dependencies. However, experiments used trigram model (without bigraminterpolation), trivial extension described model.1. Transition Emission Parameters. tag , draw transition distributiontags , emission distribution words W . transition emissiondistributions multinomial, drawn conjugate prior, Dirichlet (Gelman, Carlin, Stern, & Rubin, 2004). use symmetric Dirichlet priors, encodeexpectation uniformity induced multinomials, encode preferencesspecific words tags.tag , draw transition distribution tags , emission distributionwords W , symmetric Dirichlet priors.2. Coupling Parameter. Draw bilingual coupling distribution tag pairs pairs .distribution multinomial dimension |T | |T |, drawn symmetricDirichlet prior 0 tag pairs.3. Data. bilingual parallel sentence:(a) Draw alignment bilingual alignment distribution Ab . following paragraph defines Ab formally.(b) Draw bilingual sequence part-of-speech tags (y1 , ..., ym ), (y1 , ..., yn ) according to:P ((y1 , ..., ym ), (y1 , ..., yn )|a, , , ).4 joint distribution thus conditionsalignment structure, transition probabilities languages, couplingdistribution; formal definition given Formula 1.(c) part-of-speech tag yi first language, emit word vocabulary W :xi yi ,(d) part-of-speech tag yj second language, emit word vocabularyW : xj .jcompletes outline generative story. provide detail alignments handled, distribution coupled part-of-speech tag sequences.Alignments alignment defines bipartite graph words x x two parallelsentences . particular, represent set integer pairs, indicating word indices.Crossing edges permitted, would lead cycles resulting graphical model;thus, existence edge (i, j) precludes additional edges (i + a, j b) (i a, j + b),4. use special end state, rather explicitly modeling sentence length. Thus values n determinedstochastically.349fiNASEEM , NYDER , E ISENSTEIN & BARZILAYa, b 0. linguistic perspective, assume edge (i, j) indicates wordsxi xj share syntactic and/or semantic role bilingual parallel sentences.perspective generative story, alignments treated draws distribution Ab . Since alignments always observed, remain agnostic distributionAb , except require assign zero probability alignments either: (i) align singleindex one language multiple indices language (ii) contain crossing edges.resulting alignments thus one-to-one, contain crossing edges, may sparse evenpossibly empty. technique obtaining alignments display properties describedSection 4.2.Generating Tag Sequences standard hidden Markov model part-of-speech tagging,tags drawn Markov process transition distribution. permits probabilitytag sequence factor across time steps. model employs similar factorization:tags unaligned words drawn predecessors transition distribution, joined tagnodes drawn product involving coupling parameter transition distributionslanguages.formally, given alignment sets transition parameters , factorconditional probability bilingual tag sequence (y1 , ..., ym ), (y1 , ..., yn ) transition probabilities unaligned tags, joint probabilities aligned tag pairs:P ((y1 , ..., ym ), (y1 , ..., yn )|a, , , ) =yi1 (yi )(yj )j1unalignedunaligned jP (yi , yj |yi1 , yj1, , , ).(1)(i,j)aalignment contains crossing edges, still model tags generatedsequentially stochastic process. define distribution aligned tag pairs productlanguages transition probability coupling probability:P (yi , yj |yi1 , yj1, , , )=yi1 (yi )j1(yj )(yi , yj )Znormalization constant defined as:Xyi1 (y)Z=j1.(2)(y ) (y, ).y,yfactorization allows language-specific transition probabilities shared across alignedunaligned tags.Another way view probability distribution product three experts: two transition parameters coupling parameter. Product-of-expert models (Hinton, 1999) allowinformation source exercise strong negative influence probability tagsconsider inappropriate, compared additive models. ideal setting,prevents coupling distribution causing model generate tag unacceptableperspective monolingual transition distribution. preliminary experiments foundmultiplicative approach strongly preferable additive models.350fiM ULTILINGUAL PART- -S PEECH TAGGING3.1.2 ERGED N ODE ODEL : NFERENCEgoal inference procedure obtain transition emission parametersapplied monolingual test data. Ideally would choose parameters highestmarginal probability, conditioned observed words x alignments a,Z, = arg max P (, , y, |x, a, 0 , 0 , 0 )dyd.,structure model permits us decompose joint probability, possible analytically marginalize hidden variables. resort standard Monte Carloapproximation, marginalization performed sampling. repeatedly samplingindividual hidden variables according appropriate distributions, obtain Markov chainguaranteed converge stationary distribution centered desired posterior. Thus,initial burn-in phase, use samples approximate marginal distributiondesired parameter (Gilks, Richardson, & Spiegelhalter, 1996).core element inference procedure Gibbs sampling (Geman & Geman, 1984). Gibbssampling begins randomly initializing unobserved random variables; iteration,random variable ui sampled conditional distribution P (ui |ui ), ui refersvariables ui . Eventually, distribution samples drawn processconverge unconditional joint distribution P (u) unobserved variables. possible,avoid explicitly sampling variables direct interest, rather integratethem. technique known collapsed sampling; guaranteed never increase samplingvariance, often reduce (Liu, 1994).merged node model, need sample part-of-speech tags priors.able exactly marginalize emission parameters approximately marginalize transitioncoupling parameters (the approximations required due re-normalized productexperts see details). draw repeated samples part-of-speech tags,construct sample-based estimate underlying tag sequence. sampling, constructmaximum posteriori estimates parameters interest language, .Sampling Unaligned Tags unaligned part-of-speech tags, conditional sampling equationsidentical monolingual Bayesian hidden Markov model. probability tag decomposes three factors:P (yi |yi , , x, x , 0 , 0 ) P (xi |yi , yi , xi , 0 )P (yi |yi1 , yi , 0 )P (yi+1 |yi , yi , 0 ), (3)follows chain rule conditional independencies model. first factoremission xi remaining two transitions. derive formfactor, marginalizing parameters .emission factor, exactly marginalize emission distribution , whose priorDirichlet hyperparameter 0 . resulting distribution ratio counts, prior actspseudo-count:Zn(yi , xi ) + 0yi (xi )P (yi |y, xi , 0 )dyi =P (xi |y, xi , 0 ) =.(4)n(yi ) + |Wyi |0yiHere, n(yi ) number occurrences tag yi yi , n(yi , xi ) number occurrences tag-word pair (yi , xi ) (yi , xi ), Wyi set word types vocabulary351fiNASEEM , NYDER , E ISENSTEIN & BARZILAYW take tag yi . integral tractable due Dirichlet-multinomial conjugacy,identical marginalization applied monolingual Bayesian HMM Goldwater Griffiths (2007).unaligned tags, also possible exactly marginalize parameter governing transitions. transition 1 i,Zn(yi1 , yi ) + 0yi1 (yi )P (yi |yi , 0 )dyi1 =P (yi |yi1 , yi , 0 ) =.(5)n(yi1 ) + |T |0yi1factors similar emission probability: n(yi ) number occurrencestag yi yi , n(yi1 , yi ) number occurrences tag sequence (yi1 , yi ),tagset. probability transition + 1 analogous.Jointly Sampling Aligned Tags situation tags aligned words complex.sample tags jointly, considering |T | possibilities. begin decomposingprobability three factors:P (yi , yj |yi , yj , x, x , a, 0 , 0 , , , ) P (xi |y, xi , 0 )P (xj |y , xj , 0 )P (yi , yj |yi , yj , a, , , ).first two factors emissions, handled identically unaligned case (Formula 4). expansion final, joint factor depends alignment succeeding tags.neither successors (in either language) aligned, product bilingualcoupling probability four transition probabilities:P (yi , yj |yi , yj, , , ) (yi , yj )yi1 (yi )yi (yi+1 )yj1(yj )y (yj+1).jWhenever one succeeding words aligned, sampling formulas must accounteffect sampled tag joint probability succeeding tags, longersimple multinomial transition probability. give formula one casewhensampling joint tag pair (yi , yj ), whose succeeding words (xi+1 , xj+1 ) also aligned oneanother:"#yi (yi+1 ) (yj+1)jP (yi , yj |yi , yj , a, , , ) (yi , yj )yi1 (yi ) (yj ) P(t ) (t, ) . (6)j1(t)t,tj= ,Intuitively, puts probability mass single assignment yi+1 = t, yj+1transitions + 1 j j + 1 irrelevant, final factor goes one.Conversely, indifferent assigns equal probability pairs ht, i, final factor becomes proportional yi (yi+1 )y (yj+1), xi+1 xj+1jaligned. general, entropy increases, transition succeeding nodes exertsgreater influence yi yj . Similar equations derived cases succeeding tagsaligned other, one aligned another tag, e.g., xi+1 aligned xj+2 .before, would like marginalize parameters , , . parametersinteract product-of-experts model, marginalizations approximations. formmarginalizations identical Formula 5. coupling distribution,P (yi , yj |yi , yj, 0 )352n(yi , yj ) + 0,N (a) + |T |0(7)fiM ULTILINGUAL PART- -S PEECH TAGGINGn(yi , yj ) number times tags yi yj aligned, excluding j, N (a)total number alignments. above, prior 0 appears smoothing factor;denominator multiplied dimensionality , size cross-producttwo tagsets. Intuitively, approximation would exactly correct aligned taggenerated twice transition parameter coupling parameter insteadsingle time product experts.alternative approximately marginalizing parameters would sample usingMetropolis-Hastings scheme work Snyder, Naseem, Eisenstein, Barzilay (2008).use approximate marginalizations represents bias-variance tradeoff, decreasedsampling variance justifies bias introduced approximations, practical numberssamples.3.2 Multilingual Unsupervised Tagging: Latent Variable Modelmodel described previous section designed bilingual aligned data; seeSection 5, exploits data effectively. However, many resources containtwo languages: example, Europarl contains eleven, Multext-East corpus contains eight.raises question best exploit available resources multi-aligned dataavailable.One possibility would train separate bilingual models combine output testtime, either voting heuristic. However, believe cross-lingual informationreduces ambiguity training time, would preferable learn multiple languages jointlytraining. Indeed, results Section 5 demonstrate joint training outperformsvoting scheme.Another alternative would try extend bilingual model developed previoussection. extension possible principle, merged node model scale wellcase multi-aligned data across two languages. Recall use merged nodesrepresent tags aligned words; state space nodes grows |T |L , exponentialnumber languages L. Similarly, coupling parameter dimension,counts required estimation become sparse number languages increases.Moreover, bi-tag model required removing crossing edges word-alignment, avoidcycles. unproblematic pairs aligned sentences, usually requiring removal less5% edges (see Table 16 Appendix A). However, number languages grows,increasing number alignments discarded.Instead, propose new architecture specifically designed multilingual setting.before, maintain HMM substructures language, learned parameterseasily applied monolingual data. However, rather merging tag nodes aligned words,introduce layer superlingual tags. role latent nodes capture cross-lingualpatterns. Essentially perform non-parametric clustering sets aligned tags, encouragingmultilingual patterns occur elsewhere corpus.concretely, every set aligned words, add superlingual tag outgoing edgesrelevant part-of-speech nodes. example configuration shown Figure 1c. superlingual tags generated independently, influence selection part-of-speechtags connected. before, use product-of-experts model combinecross-lingual cues standard HMM transition model.353fiNASEEM , NYDER , E ISENSTEIN & BARZILAYsetup scales well. Crossing many-to-many alignments may used without creatingcycles, cross-lingual information emanates hidden superlingual tags. Furthermore,size model parameter space scale linearly number languages.describe role superlingual tags detail.3.2.1 P ROPAGATING C ROSS - LINGUAL PATTERNS UPERLINGUAL TAGSsuperlingual tag specifies set distributions one languages part-of-speechtagset. order learn repeated cross-lingual patterns, need constrain number valuessuperlingual tags take thus number distributions provide. example,might allow superlingual tags take integer values 1 K, integervalue indexing separate set tag distributions. set distributions corresponddiscovered cross-lingual pattern data. example, one set distributions might favor nounslanguage another might favor verbs, though heterogenous distributions (e.g., favoringdeterminers one language prepositions others) also possible.Rather fixing number superlingual tag values arbitrary size K, leave unbounded, using non-parametric Bayesian model. encourage desired multilingual clusteringbehavior, use Dirichlet process prior (Ferguson, 1973). prior, high posterior probability obtained small number values used repeatedly. actual numbersampled values thus dictated data.draw infinite sequence distribution sets 1 , 2 , . . . base distribution G0 .()set distributions tags, one distribution per language, written .weight sets distributions, draw infinite sequence mixture weights 1 , 2 , . . .stick-breaking process, defines distribution integers probability massplaced initial set values. pair sequences 1 , 2 , . . . 1 , 2 , . . . definedistribution superlingual tags associated distributions parts-of-speech.superlingual tag z N drawn probability z , associated set multinomialshz , z , . . .i.merged node model, distribution aligned part-of-speech tags governedproduct experts. case, incoming edges superlingual tags (if any)predecessor tag. combine distributions via normalized product. Assuming tagposition language connected superlingual tags, part-of-speech tag yi drawnaccording to,Qyi1 (yi )m=1 zm (yi )yi,(8)Zyi1 indicates transition distribution, zm value mth connected superlingualtag, zm (yi ) indicates tag distribution language given zm . normalization Zobtained summing product possible values yi .parameterization allows relatively simple parameter space. also leads desirableproperty: tag high probability, incoming distributions must allow it. is,expert veto potential tag assigning low probability, generally leading consensusdecisions.formalize description giving stochastic generative process observeddata (raw parallel text alignments), according multilingual model.354fiM ULTILINGUAL PART- -S PEECH TAGGING3.2.2 L ATENT VARIABLE ODEL : G ENERATIVE TORYn languages, assume existence n tagsets 1 , . . . , n vocabularies, W 1 , . . . , W n ,one language. Table 1 summarizes relevant parameters. clarity generativeprocess described using bigram transition dependencies, experiments use trigrammodel, without bigram interpolations.1. Transition Emission Parameters. language = 1, ..., n tag, draw transition distribution tags emission distributionwords W , symmetric Dirichlet priors appropriate dimension.2. Superlingual Tag Parameters. Draw infinite sequence sets distributions tags1 , 2 , . . ., set n multinomials hi1 , i2 , . . . i, one nlanguages. multinomial distribution tagset , drawnsymmetric Dirichlet prior; priors together comprise base distribution G0 ,drawn.time, draw infinite sequence mixture weights GEM (),GEM () indicates stick-breaking distribution (Sethuraman, 1994) concentrationparameter = 1. parameters define distribution superlingual tags, equivalently part-of-speech distributions index:Pk k=z(9)zPk(10)k k =k=k defined one = k zero otherwise. Formula 10,say set multinomials drawn Dirichlet process, conventionally writtenDP (, G0 ).3. Data. multilingual parallel sentence:(a) Draw alignment multilingual alignment distribution . alignmentspecifies sets aligned indices across languages; set may consist indicessubset languages.(b) set indices a, draw superlingual tag value z according Formula 9.(c) language , = 1, . . . (until end-tag reached):i. Draw part-of-speech tag yi according Formula 8.ii. Draw word wi W according emission distribution yi .One important difference merged node model generative story distributionmultilingual alignments unconstrained: generate crossing many-to-one alignments needed. perform Bayesian inference model use Gibbs sampling,marginalizing parameters whenever possible.355fiNASEEM , NYDER , E ISENSTEIN & BARZILAY3.2.3 L ATENT VARIABLE ODEL : NFERENCEsection 3.1.2, employ sampling-based inference procedure. Again, standard closed formsused analytically marginalize emission parameters , approximate marginalizationsapplied transition parameters , superlingual tag distributions ; similar techniquesused marginalize superlingual tag mixture weights . before, approximations wouldexact parameters numerator Formula 8 solely responsiblesampled tags.still must sample part-of-speech tags superlingual tags z. remaindersection describes sampling equations variables.Sampling Part-of-speech Tagsdraw from:sample part-of-speech tag language position|yi , y(,i) , a, z)P (yi |y(,i) , a, z)P (yi |y(,i) , x, a, z) P (xi |xi , )P (yi+1(11)y(,i) refers tags except yi . first factor handles emissions, latter twofactors generative probabilities (i) current tag given previous tag superlingualtags, (ii) next tag given current tag superlingual tags. two quantities similarequation 8, except integrate transition parameter yi1 superlingual tagparameters z . end product integrals, compute closed form.Terms involving transition distributions emission distributions identicalbilingual case, described Section 3.1.2. closed form integrating parametersuperlingual tag value z given by:Zn(z, yi , ) + 0z (yi )P (z |0 )dz =n(z, ) + 0n(z, yi , ) number times tag yi observed together superlingual tag zlanguage , n(z, ) total number times superlingual tag z appears edgelanguage , 0 symmetric Dirichlet prior tags language .Sampling Superlingual Tags set aligned words observed alignment needsample superlingual tag z. Recall z index infinite sequenceh11 , . . . , 1n i, h21 , . . . , 2n i, . . . ,z distribution tagset . generative distribution z givenFormula 9. sampling scheme, however, integrate possible settings mixtureweights using standard Chinese Restaurant Process closed form (Escobar & West, 1995):(1fifin(zi ) zi ziP yi fizi , zi , y(,i) k+P zi fizi ,(12)otherwisek+first group factors product closed form probabilities tags connectedsuperlingual tag, conditioned zi . factors calculated mannerequation 11 above. final factor standard Chinese Restaurant Process closed formposterior sampling Dirichlet process prior. factor, k total number sampledsuperlingual tags, n(zi ) total number times value zi occurs sampled superlingualtags, Dirichlet process concentration parameter (see Step 2 Section 3.2.2).356fiM ULTILINGUAL PART- -S PEECH TAGGING3.3 Implementationsection describes implementation details necessary reproduce experiments.present details merged node latent variable models, well monolingual baseline.3.3.1 NITIALIZATIONinitialization phase required generate initial settings word tags hyperparameters,superlingual tags latent variable model. initialization follows:Monolingual ModelTags: Random, uniform probability among tag dictionary entries emittedword.Hyperparameters 0 , 0 : Initialized 1.0Merged Node ModelTags: Random, uniform probability among tag dictionary entries emittedword. joined tag nodes, slot selected tag dictionary emittedword appropriate language.Hyperparameters 0 , 0 , 0 : Initialized 1.0Latent Variable ModelTags: Set final estimate monolingual model.Superlingual Tags: Initially set 14 superlingual tag values assumed valuecorresponds one part-of-speech tag. alignment assigned one 14 valuesbased common initial part-of-speech tag words alignment.Hyperparameters 0 , 0 : Initialized 1.0Base Distribution G0 : Set symmetric Dirichlet distribution parameter valuefixed 1.0Concentration Parameter : Set 1.0 remains fixed throughout.3.3.2 H YPERPARAMETER E STIMATIONmodels symmetric Dirichlet priors 0 0 , emission transition distributions respectively. merged node model also symmetric Dirichlet prior 0 couplingparameter. re-estimate priors inference, based non-informative hyperpriors.Hyperparameter re-estimation applies Metropolis-Hastings algorithm full epochsampling tags. addition, run initial 200 iterations speed convergence. MetropolisHastings sampling technique draws new value u proposal distribution, makesstochastic decision whether accept new sample (Gelman et al., 2004). decisionbased proposal distribution joint probability u observed sampledvariables x .assume improper prior P (u) assigns uniform probability mass positive reals,use Gaussian proposal distribution mean set previous value parameter357fiNASEEM , NYDER , E ISENSTEIN & BARZILAYvariance set one-tenth mean.5 non-pathological proposal distributions, MetropolisHastings algorithm guaranteed converge limit stationary Markov chain centereddesired joint distribution. observe acceptance rate approximately 1/6, linestandard recommendations rapid convergence (Gelman et al., 2004).3.3.3 F INAL PARAMETER E STIMATESultimate goal training learn models applied unaligned monolingual data.Thus, need construct estimates transition emission parameters .sampling procedure focuses tags y. construct maximum posteriori estimates y, indicating likely tag sequences aligned training corpus. predicted tagscombined priors 0 0 construct maximum posteriori estimates transitionemission parameters. learned parameters applied monolingual test data findhighest probability tag sequences using Viterbi algorithm.monolingual merged node models, perform 200 iterations sampling, selectmodal tag settings slot. sampling found produce different results.latent variable model, perform 1000 iterations sampling, select modal tag valueslast 100 samples.4. Experimental Setupperform series empirical evaluations quantify contribution bilingual multilingual information unsupervised part-of-speech tagging. first evaluation follows standardprocedures established unsupervised part-of-speech tagging: given tag dictionary (i.e., setpossible tags word type), model selects appropriate tag token occurringtext. also evaluate tagger performance available dictionaries incomplete (Smith& Eisner, 2005; Goldwater & Griffiths, 2007). scenarios, model trained usinguntagged text.section, first describe parallel data part-of-speech annotations used systemevaluation. Next describe monolingual baseline inference procedure used testing.4.1 Datasource parallel data, use Orwells novel Nineteen Eighty Four original Englishwell translation seven languages Bulgarian, Czech, Estonian, Hungarian, Slovene,Serbian Romanian.6 translation produced different translator publishedprint separately different publishers.dataset representatives four language families Slavic, Romance, UgricGermanic. data distributed part publicly available Multext-East corpus, Version 3(Erjavec, 2004). corpus provides detailed morphological annotation token level, includingpart-of-speech tags. addition, lexicon language provided.5. proposal identical parameter re-estimation applied emission transition priors GoldwaterGriffiths (2007).6. initial publication (Snyder et al., 2008), used subset data, including sentencesone-to-one alignments four languages considered paper. current set-up makes usesentences available corpus.358fiM ULTILINGUAL PART- -S PEECH TAGGINGPercentage AlignedBulgarian (BG)Czech (CS)English (EN)Estonian (ET)Hungarian (HU)Romanian (RO)Slovene (SL)Serbian (SR)Sentences66816750673664776767651966886676Words1011751028341184269490098428118330116908112131BGCSENETHUROSLSR41.043.235.732.235.539.341.441.736.442.432.027.549.444.450.541.942.939.642.545.243.233.539.134.432.623.436.433.631.330.732.933.822.429.126.641.531.742.529.226.931.233.945.456.244.644.834.630.853.445.948.440.939.730.332.151.2-Table 2: Percentage words row language alignments pairedcolumn language.corpus consists 118,426 English words 6,736 sentences (see Table 3). sentences, first 75% used training, taking advantage multilingual alignments.remaining 25% used evaluation. evaluation, monolingual information madeavailable model, simulate performance non-parallel data.4.2 Alignmentsexperiments use sentence-level alignments provided Multext-East corpus. Wordlevel alignments computed language pair using G IZA ++ (Och & Ney, 2003).procedures handling alignments different merged node latent variable models.4.2.1 ERGED N ODE ODELobtain 28 parallel bilingual corpora considering pairings eight languages.generate one-to-one alignments word level, intersect one-to-many alignments goingdirection. process results alignment half tokens bilingual parallelcorpus. automatically remove crossing alignment edges, would induce cyclesgraphical model. employ simple heuristic: crossing alignment edges removed basedorder appear left right; step eliminates average 3.62%edges. Table 2 shows number aligned words language pair removing crossingedges. detailed statistics total number alignments provided Appendix A.4.2.2 L ATENT VARIABLE ODELprevious setting, run GIZA ++ 28 pairings 8 languages, taking intersection alignments direction. Since want latent superlingual variable spanmany languages possible, aggregate pairwise lexical alignments larger sets denselyaligned words assign single latent superlingual variable set. Specifically,word token, consider set word word tokens aligned.pairwise alignments occur 2/3 token pairs set, considered densely359fiNASEEM , NYDER , E ISENSTEIN & BARZILAYFigure 2: example multilingual alignment configuration. Nodes correspond words tokens, labeled language. Edges indicate pairwise alignments producedGIZA ++. Boxes indicate alignment sets, though set C1 subsumed C2eventually discarded, described text.connected admitted alignment set. Otherwise, increasingly smaller subsets considered one densely connected found. procedure repeated word tokenscorpus least one alignment. Finally, alignment sets pruned removingsubsets larger alignment sets. remaining sets considered sitelatent superlingual variable.process illustrated example. sentence know you, eyes seemedsay, see you, appears original English version corpus. English wordtoken seemed aligned word tokens Serbian (cinilo), Estonian (nais), Slovenian (zdelo).Estonian Slovenian tokens aligned other. Finally, Serbian token alignedHungarian word token (mintha), aligned tokens. configurationshown Figure 2, nodes labeled two-letter language abbreviations.construct alignment sets words.Hungarian word, one aligned word, Serbian, alignmentset consists pair (C1 figure).Serbian word aligned partners Hungarian English; overall settwo pairwise alignments possible three, English Hungarian wordsaligned. Still, since 2/3 possible alignments present, alignment set (C2)formed. C1 subsumed C2, eliminated.English word aligned tokens Serbian, Estonian, Slovenian; four six possiblelinks present, alignment set (C3) formed. Note Estonian Slovenianwords aligned would three six links, set360fiM ULTILINGUAL PART- -S PEECH TAGGINGwould densely connected definition; would remove memberalignment set.Estonian token aligned words Slovenian English; three pairwise alignmentspresent, alignment set (C4) formed. identical alignment set formedstarting Slovenian word, one superlingual tag created.Thus, five word tokens, total three overlapping alignment sets created.entire corpus, process results 284,581 alignment sets, covering 76% word tokens.tokens, 61% occur exactly one alignment set, 29% occur exactly two alignment sets,remaining 10% occur three alignment sets. alignment sets, 32% includewords two languages, 26% include words exactly three languages, remaining 42%include words four languages. sets remain fixed sampling treatedmodel observed data.Bulgarian (BG)Czech (CS)English (EN)Estonian (ET)Hungarian (HU)Romanian (RO)Slovene (SL)Serbian (SR)NumberTokens1011751028341184269490098428118330116908112131Tags per token lexicon contains ...words count > 5 count > 10 top 100 words1.394.615.487.331.355.276.378.241.493.113.816.211.364.915.827.341.295.426.417.851.554.495.538.541.334.595.497.231.384.765.737.61TrigramEntropy1.631.641.511.611.621.731.641.73Table 3: Corpus size tag/token ratio language set. last column showstrigram entropy language based annotations provided corpus.4.3 TagsetMultext-East corpus manually annotated detailed morphosyntactic information.experiments, focus main syntactic category encoded first letter providedlabels. annotation distinguishes 14 parts-of-speech, 11 commonlanguages experiments. Appendix B lists tag repository eight languages.first experiment, assume complete tag lexicon available, setpossible parts-of-speech word known advance. use tag dictionaries providedMultext-East corpus. average number possible tags per token 1.39. also experimented incomplete tag dictionaries, entries available words appearingfive ten times corpus. words, entire tagset 14 tags considered.two scenarios, average per-token tag ambiguity 4.65 5.58, respectively. Finallyalso considered case lexicon entries available 100 frequent words.case average tags per token ambiguity 7.54. Table 3 shows specific tag/token ratiolanguage scenarios.361fiNASEEM , NYDER , E ISENSTEIN & BARZILAYMultext-East corpus, punctuation marks annotated part-of-speech tags.expand tag repository defining separate tag punctuation marks. allows modelmake use transition coupling patterns involving punctuation marks. However,consider punctuation tokens computing model accuracy.4.4 Monolingual Comparisonsmonolingual baseline use unsupervised Bayesian hidden Markov model (HMM)Goldwater Griffiths (2007). model, call BHMM1, modifies standard HMMadding priors performing Bayesian inference. performance par state-ofthe-art unsupervised models. Bayesian HMM particularly informative baselinemodel reduces baseline alignments data. impliesperformance gain baseline attributed multilingual aspect model.used implementation verifying performance Penn Treebank corpusidentical reported Goldwater Griffiths.provide additional point comparison, use supervised hidden Markov model trainedusing annotated corpus. apply standard maximum-likelihood estimation perform inference using Viterbi decoding pseudo-count smoothing unknown words (Rabiner, 1989).Appendix C also report supervised results using Stanford Tagger, version 1.67 . Although results slightly lower supervised HMM implementation, notesystem directly comparable set-up, allow use tag dictionaryconstrain part-of-speech selections.4.5 Test Set Inferenceuse procedure apply models (the monolingual model, bilingual mergednode model, latent variable model) test data. training, trigram transition wordemission probabilities computed, using counts tags assigned final training iteration.Similarly, final sampled values hyperparameters selected smoothing parameters.apply Viterbi decoding identify highest probability tag sequences monolingualtest set. report results multilingual monolingual experiments averaged five runsbilingual experiments averaged three runs. average standard-deviation accuracymultiple runs less 0.25 except lexicon limited 100 frequent words.case standard deviation 1.11 monolingual model, 0.85 merged node model1.40 latent variable model.5. Resultssection, first report performance two models full reduced lexiconcases. Next, report results semi-supervised experiment, subset languagesannotated text training time. Finally, investigate sensitivity models hyperparameter values provide run time statistics latent variable model increasing numberslanguages.7. http://nlp.stanford.edu/software/tagger.shtml362fiM ULTILINGUAL PART- -S PEECH TAGGING1.2.3.4.5.6.7.RandomMonolingualERGED N ODE: averageL ATENT VARIABLESupervisedERGED N ODE: votingERGED N ODE: best pairAvg83.391.293.295.097.393.095.4BGCSENETHUROSLSR82.588.791.392.696.891.694.786.993.996.998.298.697.497.880.795.895.995.097.296.196.184.092.793.394.697.094.394.285.795.396.796.797.896.896.978.291.191.995.197.791.694.184.587.489.395.897.087.994.883.584.590.292.396.688.294.5Table 4: Tagging accuracy complete tag dictionaries. first column reports average resultsacross languages (see Table 3 language name abbreviations). latent variablemodel trained using eight languages, whereas merged node models trainedlanguage pairs. latter case, results given averaging pairings (line 3),bilingual models vote tag prediction (line 6), oracleselect best pairing target language (line 7). differences L ATENTVARIABLE, ERGED N ODE: voting, Monolingual (lines 2, 4, 6) statisticallysignificant p < 0.05 according sign test.5.1 Full Lexicon Experimentsexperiments show merged node latent variable models substantially improvetagging accuracy. Since merged node model restricted pairs languages, provideaverage results possible pairings. addition, also consider two methods combiningpredictions multiple bilingual pairings: one using voting scheme employingoracle select best pairings (see additional details).shown Line 4 Table 4, merged node model achieves, average, 93.2% accuracy,two percentage point improvement monolingual baseline.8 latent variable modeltrained eight languages achieves 95% accuracy, nearly two percentage points higherbilingual merged node model. two results correspond error reductions 23%43% respectively, reduce gap unsupervised supervised performance30% 60%.mentioned above, also employ voting scheme combine information multiplelanguages using merged node model. scheme, train bilingual merged node modelslanguage pair. Then, making tag predictions particular language e.g., Romanian consider preferences bilingual model trained Romanian secondlanguage. tag preferred plurality models selected. results methodshown line 6 Table 4, differ significantly average bilingual performance.Thus, simple method combining information multiple language measurejoint multilingual model performance.8. accuracy monolingual English tagger relatively high compared 87% reported GoldwaterGriffiths (2007) WSJ corpus. attribute discrepancy differences tag inventory useddata-set. example, Particles Prepositions merged WSJ corpus (as happentag inventory corpus), performance Goldwaters model WSJ similar report here.363fiNASEEM , NYDER , E ISENSTEIN & BARZILAYimprovement random2520MonolingualMerged NodeLatent Variable15105Full LexiconCounts > 5Counts > 10Top 100Figure 3: Summary model performance full reduced lexicon conditions. Improvementrandom baseline indicated monolingual baseline, merged nodemodel (average performance possible bilingual pairings), latent variablemodel (trained eight languages). Counts > x indicates wordscounts greater x kept lexicon; Top 100 keeps 100 common words.use sign test assess whether statistically significant differences accuracy tag predictions made monolingual baseline (line 2 Table 4), latent variablemodel (line 4), voting-based merged node model (line 6). differences rowsfound statistically significant p < 0.05. Note cannot use sign test compareaverage performance bilingual model (line 3), since result aggregate accuraciesevery language pair.5.2 Reduced Lexicon Experimentsrealistic application scenarios, may tag dictionary coverage across entire lexicon. consider three reduced lexicons: removing words counts five less;removing words counts ten less; keeping top 100 frequent words.Words removed lexicon take tag, increasing overall difficultytask. results shown Table 5 graphically summarized Figure 3. cases,monolingual model less robust reduction lexicon coverage multilingual models.case 100 word lexicon, latent variable model achieves accuracy 57.9%, compared53.8% monolingual baseline. merged node model, hand, achieves slightlyhigher average performance 59.5%. two scenarios, latent variable model trainedeight languages outperforms bilingual merged node model, even oracle selectsbest bilingual pairing target language. example, using lexicon wordsappear greater five times, monolingual baseline achieves 74.7% accuracy, merged nodemodel using best possible pairings achieves 81.7% accuracy, full latent variable modelachieves accuracy 82.8%.364fiTop 100Counts > 10Counts > 5ULTILINGUAL PART- -S PEECH TAGGINGRandomMonolingualERGED N ODE: averageL ATENT VARIABLEERGED N ODE: votingERGED N ODE: best pairRandomMonolingualERGED N ODE: averageL ATENT VARIABLEERGED N ODE: votingERGED N ODE: best pairRandomMonolingualERGED N ODE: averageL ATENT VARIABLEERGED N ODE: votingERGED N ODE: best pairAvg63.674.880.182.880.481.757.970.977.279.777.579.037.353.859.657.962.463.6BGCSENETHUROSLSR62.973.580.281.380.482.757.571.977.878.878.480.236.760.960.165.561.564.76272.27983.078.579.754.766.775.379.475.376.732.144.152.549.355.455.371.887.390.488.190.790.768.384.488.886.189.289.448.969.073.571.674.877.461.672.576.580.676.477.55668.372.977.973.174.936.654.859.554.362.261.561.373.577.380.876.87855.169.073.876.473.375.236.456.859.451.060.960.262.877.182.786.184.084.457.273.080.583.181.782.133.751.461.457.564.369.364.875.778.783.679.780.959.270.476.180.076.177.639.849.456.653.962.363.161.866.375.978.876.479.455.563.772.475.973.176.133.844.053.460.457.556.9Table 5: Tagging accuracy reduced lexicon conditions. Counts > x indicates wordscounts greater x kept lexicon; Top 100 keeps 100common words. latent variable model trained using eight languages, whereasmerged node models trained language pairs. latter case, results givenaveraging pairings, bilingual models vote tag prediction,oracle select best pairing target language.three pairs results marked , , , differences monolingual,L ATENT VARIABLE, ERGED N ODE: voting statistically significant p <0.05 according sign test.Next consider performance bilingual merged node model lexiconreduced one two languages. condition may occur dealing two languages asymmetric resources, terms unannotated text. shown Table 6, mergedmodels average scores 5.7 points higher monolingual model tag dictionaries reduced, 14.3 points higher partner language full tag dictionary.suggests bilingual models effectively transfer additional lexical information availableresource-rich language resource-poor language, yielding substantial performance improvements.Perhaps surprising result resource-rich language gains much averagepairing resource-poor partner language would gained pairinglanguage full lexicon. cases, average accuracy 93.2% achieved, compared91.1% monolingual baseline.365fiNASEEM , NYDER , E ISENSTEIN & BARZILAYMonolingualReduced Full60.988.744.193.969.095.854.892.756.895.351.491.149.487.444.084.553.891.2BGCSENETHUROSLSRAvg.Bilingual (Merged Node)Reduced language Unreduced language71.391.666.797.182.495.865.693.363.096.769.391.563.389.163.690.368.193.2reduced60.152.573.559.559.461.456.653.459.5full91.396.995.993.396.791.989.390.293.2Table 6: Various scenarios reducing tag dictionary 100 frequent terms.5.3 Indirect SupervisionAlthough main focus paper unsupervised learning, also provide results indicating multilingual learning applied scenarios varying amounts annotateddata. scenarios fact quite realistic, previously trained highly accurate taggersusually available least languages parallel corpus. apply latentvariable model scenarios simply treating tags annotated data (in subsetlanguages) fixed observed throughout sampling procedure. strictly probabilisticperspective correct approach. However, note that, practice, heuristics objective functions place greater emphasis supervised portion data may yield betterresults. explore possibility here.supervised language(s)...BGaccuracy for...BGCSENETHUROSLSRAvg50.862.657.250.362.855.064.957.7CSENETHUROSLSRothersNone69.168.052.265.950.268.160.451.261.856.167.151.061.956.451.173.956.680.659.849.862.969.653.169.557.150.059.256.276.276.682.872.562.374.977.772.574.465.549.371.654.351.057.553.960.457.970.558.050.061.656.865.961.757.753.161.355.664.158.951.457.853.263.558.658.554.461.657.754.763.457.969.964.859.2Table 7: Performance latent variable model eight languages supervised annotations others frequent 100 words lexicon.first eight columns report results one eight languages supervised.penultimate column reports results one languages supervised.final column reports results supervision available (repeated Table 5convenience).366fiM ULTILINGUAL PART- -S PEECH TAGGINGTable 7 gives results two scenarios indirect supervision: one eightlanguages annotated data, one languages annotated data.cases, unsupervised languages provided 100 word lexicon, eight languagestrained together. one eight languages supervised, results vary dependingchoice supervised language. one Bulgarian, Hungarian, Romanian supervised,improvement seen, average, seven languages. However, Slovene supervised, improvement seen languages fairly substantial, average accuracyrising 64.8%, 57.9% unsupervised latent variable model 53.8% monolingual baseline. Perhaps unsurprisingly, results impressive onelanguages supervised. case, average accuracy lone unsupervised language rises74.4%. Taken together, results indicate mixture supervised resources mayadded mix simple straightforward way, often yielding substantial improvementslanguages.5.4 Hyperarameter Sensitivity Runtime Statisticsmodels employ hyperparameters emission transition distribution priors (0 0respectively) merged node model employs additional hyperparameter couplingdistribution prior (0 ). hyperparameters updated throughout inference procedure.latent variable model uses two additional hyperparameters remained fixed: concentration parameter Dirichlet process () parameter base distribution superlingual tags (0 ). experiments described used initialization values listedSection 3.3.1. investigate sensitivity models different initializations 0 ,0 , 0 , different fixed values 0 . Tables 8 9 show results obtainedmerged node latent variable models, respectively, using full lexicon. observeacross wide range values, models yield similar results. addition, notefinal sampled hyperparameter values transition emission distributions always fall one,indicating sparse priors preferred.mentioned Section 3.2 one key theoretical benefits latent variable approachsize model parameter space scale linearly number languages.provide empirical confirmation running latent variable model possible subsetseight languages, recording time elapsed run9 . Figure 4 shows average runningtime number languages increased (averaged subsets size). seemodel running time indeed scales linearly languages added, per-language runningtime increases slowly: eight languages included, time taken roughly doubleeight monolingual models run serially. models scale well tagset sizenumber examples. time dependence former cubic, use trigram modelsemploy Viterbi decoding find optimal sequences test-time. training time, however,time scales linearly tagset size latent variable model quadraticallymerged node model. due use Gibbs sampling isolates individual samplingdecision tags (for latent variable model) tag-pairs (for merged node model).dependence number training examples also linear reason.9. experiments single-threaded run using Intel Xeon 3.0 GHz processor367fiNASEEM , NYDER , E ISENSTEIN & BARZILAYERGED N ODE: hyperparameter initializations000BGCSENETHUROSLSRAvg1.01.01.091.396.995.993.396.791.989.390.293.20.11.01.091.397.095.993.496.791.889.390.293.20.011.01.091.397.095.993.396.791.889.390.293.21.00.11.091.396.995.993.496.791.989.390.293.21.00.011.091.296.895.993.296.791.889.490.293.21.01.00.191.196.595.993.496.791.889.390.293.11.01.00.0191.397.195.993.296.891.889.390.293.2Table 8: Results different initializations hyperparameters merged node model. 0 ,0 0 hyperparameters transition, emission coupling multinomialsrespectively. results language averaged possible pairingslanguages.L ATENT VARIABLE : hyperparameter initializations & settings000BGCSENETHUROSLSRAvg1.01.01.01.092.698.295.094.696.795.195.892.395.00.11.01.01.092.698.195.095.096.795.095.892.395.1101.01.01.092.698.294.995.096.795.195.892.395.11001.01.01.092.698.294.894.996.795.195.892.395.01.00.11.01.092.698.195.194.296.795.295.892.495.01.00.011.01.092.798.195.294.896.695.195.892.495.11.01.00.11.092.698.295.095.096.795.095.892.395.11.01.00.011.092.698.194.994.996.794.995.892.395.01.01.01.00.192.698.294.994.996.795.195.892.395.11.01.01.00.0192.698.195.094.596.795.095.892.395.0Table 9: Results different initializations settings hyperparameters latent variablemodel. 0 0 hyperparameters transition emission multinomials respectively updated throughout inference. 0 concentration parameterbase distribution parameter, respectively, Dirichlet process, remain fixed.6. Analysissection provide analysis of: (i) factors influence effectiveness languagepairings bilingual models, (ii) incremental value adding languages latent vari368fiM ULTILINGUAL PART- -S PEECH TAGGINGFigure 4: Average running time 1000 iterations latent variable model. Results averaged possible language subsets size. top line shows averagerunning time entire subset, bottom line shows running time dividednumber languages.able model, (iii) superlingual tags corresponding cross-lingual patterns learnedlatent variable model, (iv) whether multilingual data helpful additional monolingual data. focus full lexicon scenario, though expect analysis extendvarious reduced lexicon cases considered well.6.1 Predicting Effective Language Pairingsfirst analyze cross-lingual variation performance different bilingual language pairings.shown Table 10, performance merged node model target language variessubstantially across pairings. addition, identity optimally helpful language pairingalso depends heavily target language question. instance, Slovene, achieves largeimprovement paired Serbian (+7.4), closely related Slavic language, minor improvement coupled English (+1.8). hand, Bulgarian, bestperformance achieved coupling English (+6) rather closely related Slaviclanguages (+2.4 +0). Thus, optimal pairings correspond simply language relatedness.note applying multilingual learning morphological segmentation best resultsobtained related languages, incorporating declarative knowledgelower-level phonological relations using prior encourages phonologically close alignedmorphemes (Snyder & Barzilay, 2008). too, complex model models lower-levelmorphological relatedness (such case) may yield better outcomes closely related languages.upper bound merged node model performance, line 7 Table 10 shows resultsselecting (with help oracle) best partner language. average accuracy using oracle 95.4%, substantially higher average bilingual pairing accuracy93.2%, even somewhat higher latent variable model performance 95%. gap369fiNASEEM , NYDER , E ISENSTEIN & BARZILAYperformance motivates closer examination relationship languages constituteeffective pairings.ERGED N ODE ODELcoupled with...accuracy for...BGCSENETHUROSLSRAvg91.396.995.993.396.791.989.390.2BG95.396.193.096.894.188.588.5CSENETHUROSLSR90.294.797.592.397.895.890.696.395.892.291.296.495.893.096.891.197.496.194.296.591.388.797.496.093.996.793.994.895.994.096.690.688.188.292.996.892.089.294.596.991.389.894.290.387.589.587.585.091.4Table 10: Merged node model accuracy language pairs. row corresponds performance one language, column indicates language performanceachieved. best result language indicated bold. resultsmarked significantly higher monolingual baseline p < 0.05according sign test.6.1.1 C ROSS - LINGUAL E NTROPYprevious publication (Snyder et al., 2008) proposed using cross-lingual entropy posthoc explanation variation coupling performance. measure calculates entropytagging decision one language given identity aligned tag language.cross-lingual entropy seemed correlate well relative performance four languagesconsidered publication, find correlate strongly eight languagesconsidered here. computed Pearson correlation coefficient (Myers & Well, 2002)relative bilingual performance cross-lingual entropy. target language, rankremaining seven languages based two measures: well paired language contributesimproved performance target, cross-lingual entropy target language givencoupled language. compute Pearson correlation coefficient two rankingsassess degree overlap. See Table 19 Appendix complete list results.average, coefficient 0.29, indicating weak positive correlation relative bilingualperformance cross-lingual entropy.6.1.2 LIGNMENT ENSITYnote even cross-lingual entropy exhibited higher correlation performance,would little practical utility unsupervised scenario since estimation requires taggedcorpus. Next consider density pairwise lexical alignments language pairspredictive measure coupled performance. Since alignments constitute multilingualanchors models, practical matter greater alignment density yield greater opportunities cross-lingual transfer. linguistic viewpoint, measure may also indirectly370fiM ULTILINGUAL PART- -S PEECH TAGGINGcapture correspondence two languages. Moreover, measure benefit computable untagged corpus, using automatically obtained GIZA ++ alignments.before, target language, rank languages relative bilingual performance,well percentage words target language provide alignments.find average Pearson coefficient 0.42, indicating mild positive correlation. fact,use alignment density criterion selecting optimal pairing decisions target language,obtain average accuracy 94.67% higher average bilingual performance, stillsomewhat performance multilingual model.6.1.3 ODEL C HOICEchoice model may also contribute patterns variability observe across languagepairs. test hypothesis, ran latent variable model pairs languages. resultsexperiment shown Table 11. case merged node model, performancetarget language depends heavily choice partner. However, exact patternsvariability differ case observed merged node model. measure variability, compare pairing preferences language two models.specifically, target language rank remaining seven languages contributiontwo models, compute Pearson coefficient two rankings.seen last column Table 19 Appendix, find coefficient 0.49 tworankings, indicating positive, though far perfect, correlation.L ATENT VARIABLE ODELcoupled with...accuracy for...BGCSENETHUROSLSRAvg91.997.295.793.996.893.290.591.6BG97.595.794.897.094.688.694.7CSENETHUROSLSR92.291.997.591.697.695.791.697.495.692.392.197.495.793.996.892.396.595.794.596.694.491.896.895.894.196.894.794.695.794.396.892.187.788.593.496.792.492.494.596.792.395.294.592.187.589.787.688.091.1Table 11: Accuracy latent variable model run language pairs. row correspondsperformance one language, column indicates languageperformance achieved. best result language indicated bold.results marked significantly higher monolingualbaseline p < 0.05 according sign test.6.1.4 U TILITYL ANGUAGE B ILINGUAL PARTNERalso analyze overall helpfulness language. before, target language,rank remaining seven languages degree contribute increased target language performance paired bilingual model. ask whether helpfulness371fiNASEEM , NYDER , E ISENSTEIN & BARZILAYrankings provided eight languages correlated one another words,whether languages tend universally helpful (or unhelpful) whether helpfulness dependsheavily identity target language. consider pairs target languages, compute Pearson rank correlation rankings six supplementary languagescommon (excluding two target languages themselves). average pairwise rank correlations obtain coefficient 0.20 merged node model 0.21latent variable model. low correlations indicate language helpfulness depends cruciallytarget language question. Nevertheless, still compute average helpfulnesslanguage (across target languages) obtain something like universal helpfulness ranking. See Table 20 appendix ranking. ask whether ranking correlateslanguage properties might predictive general helpfulness. compare universal helpfulness rankings10 language rankings induced tag-per-token ambiguity (the averagenumber tags allowed dictionary per token corpus) well trigram entropy (theentropy tag distribution given previous two tags). cases assign highestrank language lowest value, expect lower entropy ambiguity correlategreater helpfulness. Contrary expectations, ranking induced tag-per-token ambiguity actually correlates negatively universal helpfulness rankings small amounts (-0.28merged node model -0.23 latent variable model). models, Hungarian,lowest tag-per-token ambiguity eight languages, worst universal helpfulnessranking. correlations trigram entropy little predictable. caselatent variable model, correlation trigram entropy universalhelpfulness (-0.01). case merged node model, however, moderate positivecorrelation (0.43).6.2 Adding Languages Latent Variable Modelbilingual performance depends heavily choice language pair, latent variablemodel easily incorporate available languages, obviating need choice. testperformance number languages increases, ran latent variable model possible subsets eight languages full lexicon well three reduced lexicon scenarios.Figures 5, 6, 7, 8 plot average accuracy number available languages variesfour lexicon scenarios (in decreasing order lexicon size). comparison, monolingualaverage bilingual baseline results given. scenarios, latent variable model steadilygains accuracy number available languages increases, scenarios seesappreciable uptick going seven eight languages. full lexicon case, gap supervised unsupervised performance cut nearly two thirds unsupervisedlatent variable model eight languages.Interestingly, lexicon reduced size, performance bilingual merged nodemodel gains relative latent variable model pairs. full lexicon case, latent variablemodel clearly superior, whereas two moderately reduced lexicon cases, performancepairs less two models. case drastically reduced lexicon10. note universal helpfulness rankings obtained two multilingual models matchroughly: correlation coefficient one another 0.50. addition, universal context referseight languages consideration rankings could well change wider multilingual context.372fiM ULTILINGUAL PART- -S PEECH TAGGINGFigure 5: performance latent variable model number languages varies (averagedsubsets eight languages size). LEFT: Average performance acrosslanguages. Scores monolingual bilingual merged node models givencomparison. RIGHT: Performance individual language numberavailable languages varies.(100 words), merged node model clear winner. Thus, seems two models,performance gains latent variable model sensitive size lexicon.four figures (5, 6, 7, 8) also show multilingual performance brokenlanguage. languages except English tend increase accuracy additional languagesadded mix. Indeed, two cases moderately reduced lexicons (Figures 6 7) languages except English show steady large gains actually increase size goingseven full set eight languages. full lexicon case (Figure 5), Estonian, Romanian,Slovene display steady increases end. Hungarian peaks two languages, Bulgarianthree languages, Czech Serbian seven languages. drastic reduced lexiconcase (Figure 8), performance across languages less consistent gains languagesadded less stable. languages report gains going one two languages,half increase steadily eight languages. Two languages seem trend downwardtwo three languages, two show mixed behavior.full lexicon case (Figure 5), English language fails improve.scenarios, English gains initially gains partially eroded languagesadded. possible English outlier since significantly lower tag transition entropylanguages (see Table 3). Thus may internal tag transitions simplyinformative English information gleaned multilingual context.373fiNASEEM , NYDER , E ISENSTEIN & BARZILAYFigure 6: performance latent variable model reduced lexicon scenario (Counts >5), number languages varies (averaged subsets eight languagessize). LEFT: Average performance across languages. Scores monolingualbilingual merged node models given comparison. RIGHT: Performanceindividual language number available languages varies.Figure 7: performance latent variable model reduced lexicon scenario (Counts >10), number languages varies (averaged subsets eight languagessize). LEFT: Average performance across languages. Scores monolingualbilingual merged node models given comparison. RIGHT: Performanceindividual language number available languages varies.374fiM ULTILINGUAL PART- -S PEECH TAGGINGFigure 8: performance latent variable model reduced lexicon scenario (100words), number languages varies (averaged subsets eight languages size). LEFT: Average performance across languages. Scoresmonolingual bilingual merged node models given comparison. RIGHT:Performance individual language number available languages varies.6.3 Analysis Superlingual Tag Valuessection analyze superlingual tags corresponding part-of-speech distributions,learned latent variable model. Recall superlingual tag intuitively representsdiscovered multilingual context tags multilingual informationpropagated. formally, superlingual tag provides complete distribution partsof-speech language, allowing encoding primary secondary preferencesseparately language. preferences interact language-specific context(i.e. surrounding parts-of-speech corresponding word). place Dirichlet processprior superlingual tags, number sampled values dictated complexitydata. fact, shown Table 12, number sampled superlingual tags steadily increasesnumber languages. multilingual contexts becomes complex diverse, additionalsuperlingual tags needed.Number languagesNumber superlingual tag values211.07312.57413.87515.07615.79716.13816.50Table 12: Average number sampled superlingual tag values number languages increases.Next analyze part-of-speech tag distributions associated superlingual tag values.superlingual tag values correspond low entropy tag distributions, single dominantpart-of-speech tag across languages. See, example, distributions associated superlin375fiNASEEM , NYDER , E ISENSTEIN & BARZILAYgual tag value 6 Table 13, favor nouns large margins. Similar sets distributionsoccur favoring verbs, adjectives, primary part-of-speech categories. fact, amongseventeen sampled superlingual tag values, nine belong type, cover 80% actualsuperlingual tag instances. remaining superlingual tags correspond complex crosslingual patterns. associated tag distributions cases favor different part-of-speech tagsvarious languages tend higher entropy, probability mass spread evenlytwo three tags. One example set distributions associated superlingualtag value 14 Table 13, seems mixed noun/verb class. six eight languagesfavored tag verb, strong secondary choice cases noun. However,Estonian Hungarian, preference reversed, nouns given higher probability.superlingual tag may captured phenomenon light verbs, whereby verbs onelanguage correspond combination noun verb another language. example English verb whisper/V, translated Urdu, becomes collocation whisper/N do/V.cases, verbs nouns often aligned one another, requiring complex superlingualtag. analysis examples shows superlingual tags effectively learns simplecomplex cross-lingual patternsENETHUROSLSRP (N ) = 0.91,P (N ) = 0.92,P (N ) = 0.97,P (N ) = 0.91,P (N ) = 0.85,P (N ) = 0.90,P (N ) = 0.94,P (N ) = 0.92,P (A) = 0.04,P (A) = 0.03,P (V ) = 0.00,P (V ) = 0.03,P (A) = 0.06,P (A) = 0.04,P (A) = 0.03,P (A) = 0.03,........................BG14CSCSTAG VALUETAG VALUE6BGENETHUROSLSRP (V ) = 0.66,P (V ) = 0.60,P (V ) = 0.55,P (N ) = 0.52,P (N ) = 0.44,P (V ) = 0.45,P (V ) = 0.55,P (V ) = 0.49,P (N ) = 0.21,P (N ) = 0.22,P (N ) = 0.25,P (V ) = 0.29,P (V ) = 0.34,P (N ) = 0.33,P (N ) = 0.24,P (N ) = 0.26,........................Table 13: Part-of-speech tag distributions associated two superlingual latent tag values. Probabilities two probable tags language shown.6.3.1 P ERFORMANCE R EDUCED DATAOne potential objection claims made section improved results may duemerely addition data, multilingual aspect model may irrelevant.test idea evaluating monolingual, merged node, latent variable systems training sets number examples reduced half. multilingual models settingaccess exactly half much data monolingual model original experiment.shown Table 14, monolingual baseline models quite insensitive dropdata. fact, models, trained half corpus, still outperform monolingual model trained entire corpus. indicates performance gains demonstratedmultilingual learning cannot explained merely addition data.376fiM ULTILINGUAL PART- -S PEECH TAGGINGONOLINGUAL: full dataONOLINGUAL: half dataERGED N ODE: (avg.) full dataERGED N ODE: (avg.) half dataL ATENT VARIABLE: full dataL ATENT VARIABLE: half dataAvg91.291.093.293.095.094.7BGCSENETHUROSLSR88.788.891.391.192.692.693.993.896.996.698.297.895.895.795.995.795.094.792.792.693.392.794.693.995.395.396.796.796.796.791.190.291.992.095.194.487.487.589.388.995.895.484.584.590.289.992.392.2Table 14: Tagging accuracy reduced training dataset, complete tag dictionaries; resultsfull training dataset repeated comparison. first column reportsaverage results across languages (see Table 3 language name abbreviations).7. Conclusionskey hypothesis multilingual learning combining cues multiple languages,structure becomes apparent. considered two ways applying intuitionproblem unsupervised part-of-speech tagging: model directly merges tag structurespair languages single sequence second model instead incorporates multilingualcontext using latent variables.results demonstrate incorporating multilingual evidence achieve impressiveperformance gains across range scenarios. full lexicon available, two modelscut gap unsupervised supervised performance nearly one third (merged nodemodel, averaged pairs) two thirds (latent variable model, using eight languages).one language, observe performance gains additional languages added. soleexception English, gains additional languages reduced lexicon settings.scenarios, latent variable model achieves better performance merged nodemodel, additional advantage scaling gracefully number languages.observations suggest non-parametric latent variable structure provides flexible paradigmincorporating multilingual cues. However, benefit latent variable model relativemerged node model (even running models pairs languages) seems decreasesize lexicon. Thus, practical scenarios small lexicon lexiconavailable, merged node model may represent better choice.experiments shown performance vary greatly depending choiceadditional languages. difficult predict priori languages constitute good combinations.particular, language relatedness cannot used consistent predictor sometimesclosely related languages constitute beneficial couplings sometimes unrelated languageshelpful. identify number features correlate bilingual performance, thoughobserve features interact complex ways. Fortunately, latent variable modelallows us bypass question simply using available languages.models, lexical alignments play crucial role determine typologymodel sentence. fact, observed positive correlation alignmentdensity bilingual performance, indicating importance high quality alignments.experiments, considered alignment structure observed variable, produced standard MT377fiNASEEM , NYDER , E ISENSTEIN & BARZILAYtools operate pairs languages. interesting alternative would incorporatealignment structure model itself, find alignments best tuned tagging accuracy basedevidence multiple languages rather pairs.Another limitation two models consider one-to-one lexical alignments.pairing isolating synthetic languages11 would beneficial align short analyticalphrases consisting multiple words single morpheme-rich words language.would involve flexibly aligning chunking parallel sentences throughout learningprocess.important direction future work incorporate even sources multilingualinformation, additional languages declarative knowledge typological properties(Comrie, 1989). paper showed performance improves number languagesincreases. limited corpus eight languages, envision future workmassively parallel corpora involving dozens languages well learning languagesnon-parallel data.Bibliographic NotePortions work previously presented two conference publications (Snyder, Naseem,Eisenstein, & Barzilay, 2008, 2009). current article extends work several ways,notably: present new inference procedure merged node model yields improvedresults (Section 3.1.2) conduct extensive new empirical analyses multilingual results.specifically, analyze properties language combinations contribute successfulmultilingual learning, show adding multilingual data provides much greater benefitincreasing quantity monolingual data, investigate additional scenarios lexicon reduction, investigate scalability function number languages, experimentsemi-supervised settings (Sections 5 6).Acknowledgmentsauthors acknowledge support National Science Foundation (CAREER grant IIS0448168 grants IIS-0835445, IIS-0904684) Microsoft Research Faculty Fellowship. Thanks Michael Collins, Tommi Jaakkola, Amir Globerson, Fernando Pereira, Lillian Lee,Yoong Keok Lee, Maria Polinsky anonymous reviewers helpful comments suggestions. opinions, findings, conclusions recommendations expressedauthors necessarily reflect views NSF.11. Isolating languages morpheme word ratio close one, synthetic languagesallow multiple morphemes easily combined single words. English example isolating language,whereas Hungarian synthetic language.378fiM ULTILINGUAL PART- -S PEECH TAGGINGAppendix A. Alignment StatisticsBGBGCSENETHUROSLSR42163510983384931673420174596946434CSENETHUROSLSR42163510984306733849402074074631673315373901232056420173255950289277092645545969577895286942499340723644246434497404839437681297973800459865430674020731537325595778949740407463901250289528694839432056277094249937681264553407229797364423800459865Table 15: Number alignments per language pairBGBGCSENETHUROSLSR2.776.133.364.044.522.953.48CSENETHUROSLSR2.776.133.673.361.924.354.042.736.122.884.523.615.593.884.132.952.593.542.443.093.783.482.643.862.213.063.924.113.671.922.733.612.592.644.356.125.593.543.862.883.882.442.214.133.093.063.783.924.11Avg.3.892.854.753.013.724.203.223.33Table 16: Percentage alignments removed per language pair379fiNASEEM , NYDER , E ISENSTEIN & BARZILAYAppendix B. Tag RepositoryAdjectiveConjunctionDeterminerInterjectionNumeralNounPronounParticleAdverbAdpositionArticleVerbResidualAbbreviationBGCSENETHUROSLSRxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxTable 17: Tag repository languageAppendix C. Stanford Tagger PerformanceLanguageBGCSENETHUROSLSRAvg.Accuracy96.197.297.697.196.397.696.695.596.7Table 18: Performance (supervised) Stanford tagger full lexicon scenario380fiM ULTILINGUAL PART- -S PEECH TAGGINGAppendix D. Rank CorrelationLanguageBGCSENETHUROSLSRAvg.LanguageBGCSENETHUROSLSRAvg.Performance correlates ERGED N ODE modelCross-lingual entropy Alignment density L ATENT VARIABLE performance-0.290.09-0.090.390.340.240.280.770.420.460.560.560.31-0.020.290.340.830.890.590.660.950.210.130.630.290.420.49Performance correlates L ATENT VARIABLE modelCross-lingual entropy Alignment densityERGED N ODE performance0.580.44-0.09-0.40-0.440.240.670.410.420.140.320.56-0.14-0.720.290.040.680.890.570.540.950.180.100.680.210.170.49Table 19: Pearson correlation coefficients bilingual performance target languagevarious rankings supplementary language. models target language, obtain ranking supplementary languages based bilingualperformance target language. rankings correlated characteristics bilingual pairing: cross-lingual entropy (the entropy tag distributionstarget language given aligned tags supplementary language); alignmentdensity (the percentage words target language aligned words auxiliarylanguage); performance alternative model (target language performancepaired supplementary language alternative model).381fiNASEEM , NYDER , E ISENSTEIN & BARZILAYAppendix E. Universal HelpfulnessERGED N ODE modelET2.43EN2.57SL3.14BG3.43SR3.43RO4.71CS5.00HU5.71L ATENT VARIABLE modelBG1.86SR3.00ET3.14CS3.71EN3.71SL3.71RO4.14HU6.00Table 20: Average helpfulness rank language two models382fiM ULTILINGUAL PART- -S PEECH TAGGINGReferencesBaker, J. (1979). Trainable grammars speech recognition. Proceedings AcousticalSociety America.Banko, M., & Moore, R. C. (2004). Part-of-speech tagging context. ProceedingsCOLING, pp. 556561.Bertoldi, N., Barbaiani, M., Federico, M., & Cattoni, R. (2008). Phrase-based statistical machinetranslation pivot languages. International Workshop Spoken Language TranslationEvaluation Campaign Spoken Language Translation (IWSLT), pp. 143149.Bhattacharya, I., Getoor, L., & Bengio, Y. (2004). Unsupervised sense disambiguation using bilingual probabilistic models. ACL 04: Proceedings 42nd Annual Meeting Association Computational Linguistics, p. 287, Morristown, NJ, USA. Association Computational Linguistics.Brill, E. (1995). Transformation-based error-driven learning natural language processing:case study part-of-speech tagging. Computational Linguistics, 21(4), 543565.Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1991). Word-sense disambiguationusing statistical methods. Proceedings ACL, pp. 264270.Chen, Y., Eisele, A., & Kay, M. (2008). Improving statistical machine translation efficiencytriangulation. Proceedings LREC.Chiang, D. (2005). hierarchical phrase-based model statistical machine translation. Proceedings ACL, pp. 263270.Cohn, T., & Lapata, M. (2007). Machine translation triangulation: Making effective use multiparallel corpora. Proceedings ACL.Comrie, B. (1989). Language universals linguistic typology: Syntax morphology. Oxford:Blackwell.Dagan, I., Itai, A., & Schwall, U. (1991). Two languages informative one. Proceedings ACL, pp. 130137.Diab, M., & Resnik, P. (2002). unsupervised method word sense tagging using parallelcorpora. Proceedings ACL, pp. 255262.Erjavec, T. (2004). MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons corpora. Fourth International Conference Language Resources Evaluation, LREC, Vol. 4, pp. 15351538.Escobar, M., & West, M. (1995). Bayesian density estimation inference using mixtures. Journalamerican statistical association, 90(230), 577588.Ferguson, T. (1973). bayesian analysis nonparametric problems. annals statistics,1, 209230.Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004). Bayesian data analysis. ChapmanHall/CRC.Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, Bayesianrestoration images. IEEE Transactions Pattern Analysis Machine Intelligence,6, 721741.383fiNASEEM , NYDER , E ISENSTEIN & BARZILAYGenzel, D. (2005). Inducing multilingual dictionary parallel multitext related languages.Proceedings HLT/EMNLP, pp. 875882.Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo practice.Chapman & Hall/CRC.Goldwater, S., & Griffiths, T. L. (2007). fully Bayesian approach unsupervised part-of-speechtagging. Proceedings ACL, pp. 744751.Haghighi, A., & Klein, D. (2006). Prototype-driven learning sequence models. ProceedingsHLT-NAACL, pp. 320327.Hinton, G. E. (1999). Products experts. Proceedings Ninth International ConferenceArtificial Neural Networks, Vol. 1, pp. 16.Johnson, M. (2007). doesnt EM find good HMM POS-taggers?.EMNLP/CoNLL, pp. 296305.ProceedingsKuhn, J. (2004). Experiments parallel-text based grammar induction. Proceedings ACL,p. 470.Li, C., & Li, H. (2002). Word translation disambiguation using bilingual bootstrapping. Proceedings ACL, pp. 343351.Liu, J. S. (1994). collapsed Gibbs sampler Bayesian computations applicationsgene regulation problem. Journal American Statistical Association, 89(427), 958966.Merialdo, B. (1994). Tagging english text probabilistic model. Computational Linguistics,20(2), 155171.Mihalcea, R. (2004). Current Issues Linguistic Theory: Recent Advances Natural LanguageProcessing, chap. Unsupervised Natural Language Disambiguation Using Non-AmbiguousWords. John Benjamins Publisher.Myers, J. L., & Well, A. D. (2002). Research Design Statistical Analysis (2nd edition).Lawrence Erlbaum.Ng, H. T., Wang, B., & Chan, Y. S. (2003). Exploiting parallel texts word sense disambiguation:empirical study. Proceedings ACL, pp. 455462.Och, F. J., & Ney, H. (2001). Statistical multi-source translation. MT Summit 2001, pp. 253258.Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment models.Computational Linguistics, 29(1), 1951.Pado, S., & Lapata, M. (2006). Optimal constituent alignment edge covers semantic projection. Proceedings ACL, pp. 1161 1168.Rabiner, L. R. (1989). tutorial hidden markov models selected applications speechrecognition. Proceedings IEEE, 77(2), 257286.Resnik, P., & Yarowsky, D. (1997). perspective word sense disambiguation methodsevaluation. Proceedings ACL SIGLEX Workshop Tagging Text LexicalSemantics: Why, What, How?, pp. 7986.Sethuraman, J. (1994). constructive definition Dirichlet priors. Statistica Sinica, 4(2), 639650.384fiM ULTILINGUAL PART- -S PEECH TAGGINGSmith, N. A., & Eisner, J. (2005). Contrastive estimation: Training log-linear models unlabeleddata. Proceedings ACL, pp. 354362.Snyder, B., & Barzilay, R. (2008). Unsupervised multilingual learning morphological segmentation. Proceedings ACL/HLT, pp. 737745.Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2008). Unsupervised multilingual learningpos tagging. Proceedings EMNLP.Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2009). Adding languages improvesunsupervised multilingual part-of-speech tagging: bayesian non-parametric approach.Proceedings NAACL/HLT.Toutanova, K., & Johnson, M. (2008). Bayesian LDA-based model semi-supervised part-ofspeech tagging. Advances Neural Information Processing Systems 20, pp. 15211528.MIT Press.Utiyama, M., & Isahara, H. (2006). comparison pivot methods phrase-based statisticalmachine translation. Proceedings NAACL/HLT, pp. 484491.Wu, D. (1995). Stochastic inversion transduction grammars, application segmentation,bracketing, alignment parallel corpora. IJCAI, pp. 13281337.Wu, D., & Wong, H. (1998). Machine translation stochastic grammatical channel. Proceedings ACL/COLING, pp. 14081415.Xi, C., & Hwa, R. (2005). backoff model bootstrapping resources non-English languages.Proceedings EMNLP, pp. 851 858.Yarowsky, D., Ngai, G., & Wicentowski, R. (2000). Inducing multilingual text analysis tools viarobust projection across aligned corpora. Proceedings HLT, pp. 161168.385fiJournal Artificial Intelligence Research 36 (2009) 165228Submitted 03/09; published 10/09Hypertableau Reasoning Description LogicsBoris MotikRob ShearerIan Horrocksboris.motik@comlab.ox.ac.ukrob.shearer@comlab.ox.ac.ukian.horrocks@comlab.ox.ac.ukComputing Laboratory, University OxfordWolfson BuildingParks RoadOxford OX1 3QDUnited KingdomAbstractpresent novel reasoning calculus description logic SHOIQ+ knowledgerepresentation formalism applications areas Semantic Web. Unnecessarynondeterminism construction large models two primary sources inefficiencytableau-based reasoning calculi used state-of-the-art reasoners. order reducenondeterminism, base calculus hypertableau hyperresolution calculi,extend blocking condition ensure termination. order reduce sizeconstructed models, introduce anywhere pairwise blocking. also present improvednominal introduction rule ensures termination presence nominals, inverseroles, number restrictionsa combination DL constructs proven notoriouslydifficult handle. implementation shows significant performance improvementsstate-of-the-art reasoners several well-known ontologies.1. IntroductionDescription Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2007)family knowledge representation formalisms well-understood formal properties.DLs applied numerous problems computer science informationintegration metadata management. Recent interest DLs spurredapplication Semantic Web: DL SHOIQ provides logical underpinningWeb Ontology Language (OWL) (Patel-Schneider, Hayes, & Horrocks, 2004),DL SROIQ (Kutz, Horrocks, & Sattler, 2006) used OWL 2an extension OWLcurrently standardized World Wide Web Consortium.central component DL applications efficient scalable reasoner. Modern reasoners, Pellet (Parsia & Sirin, 2004), FaCT++ (Tsarkov & Horrocks, 2006),RACER (Haarslev & Moller, 2001), typically based tableau calculi (Baader &Nutt, 2007), demonstrate (un)satisfiability knowledge base K via constructive search abstraction model K. Numerous optimizations developedeffort reduce size search space (Horrocks, 2007). Despite major advancestableau reasoning algorithms, however, ontologies still encountered practicecannot handled existing DL reasoners. Two main sources complexity tableaucalculi identified literature (Donini, 2007).first source complexity known or-branching: given disjunctive assertionc2009AI Access Foundation. rights reserved.fiMotik, Shearer, & Horrocks(C D)(s), tableau algorithm nondeterministically guesses either C(s) D(s) holds.show unsatisfiability K, every possible guess must lead contradiction:assuming C(s) holds leads contradiction, algorithm must backtrack assumeD(s) holds, give rise exponential behavior. General concept inclusions(GCIs)implications form C v Dare main source disjunctions: ensureC v holds, tableau algorithm adds disjunction (C D)(s) individualmodel. Various absorption optimizations (Horrocks, 1998; Tsarkov & Horrocks, 2004;Hudek & Weddell, 2006; Horrocks, 2007) developed reduce nondeterminismtableau calculi.second source complexity tableau calculi known and-branching:expansion model due existential quantifiers generate large models. Apartmemory consumption problems, and-branching increase or-branching increasingnumber individuals GCIs applied.paper, present reasoning calculus addresses sources complexity. focus DL SHOIQ+ , obtained extending SHOIQ localreflexivity disjoint, reflexive, irreflexive, symmetric, asymmetric roles. SROIQextends SHOIQ+ generalized role inclusions form R1 . . . Rn v R.Generalized role inclusions encoded using standard GCIs proposed Demride Nivelle (2005); thus, adding suitable preprocessing phase, resultspaper allow us handle SROIQ (and hence OWL 2) well.algorithm viewed hybrid resolution tableau, relatedhypertableau (Baumgartner, Furbach, & Niemela, 1996) hyperresolution (Robinson,1965) calculi. first preprocesses SHOIQ+ knowledge base set DL-clausesuniversally quantified implications containing DL concepts roles predicates.main derivation rule DL-clauses hyperresolution: atom consequentDL-clause derived atoms DL-clause antecedent matchedalready derived consequences. Hyperresolution effective restricting or-branching.Consider, example, following example:(1)R(x, y1 ) S(x, y2 ) A(x) B(y1 ) C(y2 )DL-clause derives disjunction applied assertions form R(a, b)S(c, d) = c. presence variables (1) allows us simultaneously workindividuals a, b, c d, check whether = c. contrast, derivation rulestableau algorithms consider pairs individuals; consequently, absorptiontechnique aware localize nondeterminism individuals satisfymentioned constraints. discuss detail Section 3.3.1, calculus generalizesknown absorption variants. Furthermore, contrast absorption techniques,algorithm guaranteed exhibit nondeterminism Horn knowledge bases (Hustadt,Motik, & Sattler, 2005) GALEN, NCI, SNOMED CT (see Section 7). Finally,calculus provides uniform proof-theoretic framework handle several usefulextensions commonly used DLs (see Section 4.1.3).Hyperresolution decides many fragments first-order logic (e.g., Fermuller, Leitsch,Hustadt, & Tammet, 2001; Fermuller, Tammet, Zamov, & Leitsch, 1993), well description modal logics (e.g., Georgieva, Hustadt, & Schmidt, 2003; Hustadt & Schmidt,1999). Unlike fragments, SHOIQ+ allows cyclic GCIs form166fiHypertableau Reasoning Description LogicsC v R.C, hyperresolution generate infinite paths successors. ensuretermination, use pairwise blocking technique (Horrocks, Sattler, & Tobies, 2000b)detect cyclic computations. Due hyper-inferences, soundness correctnessproofs Horrocks et al. (2000b) carry immediately calculus; fact,certain simpler blocking conditions applicable weaker DLs cannot straightforwardlytransferred setting. limit and-branching, extend blocking conditionHorrocks et al. anywhere pairwise blocking: individual blocked anotherindividual necessarily ancestor, reduce sizes constructedmodels. Anywhere blocking already used single blocking (Buchheit, Donini,& Schaerf, 1993; Baader, Buchheit, & Hollunder, 1996; Donini & Massacci, 2000; Donini,Lenzerini, Nardi, & Schaerf, 1998); however, best knowledge,neither used sophisticated pairwise blocking tested practice.Ensuring termination tableau decision procedure DLs nominals, inverseroles, number restrictions proven notoriously difficult. problem finallysolved Horrocks Sattler (2007) extending tableau calculus nominalintroduction rule. certain situations, rule guesses introduces new nominals,thus potential source inefficiency practice. paper, present variantrule simpler efficient.implemented calculus new reasoner called HermiT.1 Even rathernave implementation, deterministic treatment GCIs significantly reduces classification times several real-world ontologies. Furthermore, pairwise anywhere blocking seemseffective limiting model sizes allows HermiT classify several ontologiesthat, best knowledge, reasoner handle.2. Preliminariesdefine syntax semantics description logic SHOIQ+ . signaturetriple = (NR , NC , NI ) consisting mutually disjoint sets atomic roles NR , atomicconcepts NC , individuals NI . set roles NR {R | R NR }. function inv() defined set roles follows, R atomic role: inv(R) = Rinv(R ) = R. RBox R finite set axioms form R1 v R2 (role inclusion),Dis(S1 , S2 ) (role disjointness), Ref(R) (reflexivity), Irr(S) (irreflexivity), Sym(R) (symmetry), Asy(S) (asymmetry), Tra(R) (transitivity), R, R1 , R2 roles,S, S1 , S2 simple roles, defined next. Let vR reflexive-transitive closurefollowing relation: {(R1 , R2 ) | R1 v R2 R inv(R1 ) v inv(R2 ) R}. role Rtransitive R role R0 exists R0 vR R, R vR R0 , either Tra(R0 ) RTra(inv(R0 )) R. role simple transitive role R exists R vR S.set concepts smallest set containing > (the top concept), (the bottom concept),(atomic concept), {a} (nominal ), C (negation), C u (conjunction), C (disjunction), R.C (existential restriction), R.C (universal restriction), S.Self (local reflexivity),n S.C (at-least restriction), n S.C (at-most restriction), atomic concept,individual, C concepts, R role, simple role, n nonnegative integer.TBox finite set general concept inclusions (GCIs) C v C concepts. ABox finite set assertions form C(a) (concept assertion), R(a, b)1. http://www.hermit-reasoner.com/167fiMotik, Shearer, & HorrocksTable 1: Model-Theoretic Semantics SHOIQ+Interpretation Concepts Roles= 4I== {sI }(C)I = 4I \ C=C(C D)I = C DI= {hy, xi | hx, yi R }(S.Self)I = {x | hx, xi }= {x | : hx, yi RI C }= {x | : hx, yi RI C }= {x | ]{y | hx, yi C } n}= {x | ]{y | hx, yi C } n}Satisfaction Axioms Interpretation|= C v iff C DI|= R1 v R2 iff R1I R2I|= Ref(R) iff x 4 : hx, xi R|= Irr(S)iff x 4I : hx, xi 6|= Sym(R) iff RI (inv(R))I|= Asy(S)iff (inv(S))I =+|= Tra(R) iff (R ) R|= Dis(S1 , S2 ) iff S1I S2I =|= C(a)iff aI C|= R(a, b)iff haI , bI RI|= biff = b|= 6 biff aI 6= bI+Note: ]N number elements N , R transitive closure R.>I{s}I(C u D)I(R )I(R.C)I(R.C)I( n S.C)I( n S.C)I(role assertion), b (equality assertion), 6 b (inequality assertion), Cconcept, R role, b individuals. SHOIQ+ knowledge base K triple(R, , A). |K| denote size Kthat is, number symbols requiredencode K input tape Turing machine (numbers coded binary).interpretation K tuple = (4I , ), 4I nonempty set,assigns element aI 4I individual a, set AI 4I atomic concept A,relation RI 4I 4I atomic role R. function extended conceptsroles shown upper part Table 1. model K, written |= K,satisfies axioms K shown lower part Table 1. basic inference problemSHOIQ+ checking whether K satisfiablethat is, checking whether model Kexists. concept C subsumes concept D, written K |= C v D, C DI modelK. easy see K |= C v K {(C u D)(a)} unsatisfiable,individual occur K (Baader & Nutt, 2007).negation-normal form nnf(C) concept C concept obtained Cusing de Morgans laws, dualities existential universal restrictions,dualities at-least at-most restrictions push negations inwardsoccur front atomic concepts, nominals, local reflexivity concepts.concept nnf(C) logically equivalent C, computed C time linearsize C (Baader & Nutt, 2007). use Cdenote nnf(C).mentioned Section 1, extending SHOIQ+ general role inclusions would yieldSROIQ (Kutz et al., 2006)the DL underpins OWL 2. ALCHOIQ+ obtainedSHOIQ+ disallowing transitivity axioms. SHIQ+ obtained SHOIQ+disallowing nominals. SHOQ+ obtained SHOIQ+ disallowing inverse roles.SHOIQ SHIQ obtained SHOIQ+ SHIQ+ , respectively, disallow168fiHypertableau Reasoning Description Logicsing local reflexivity, role disjointness, reflexivity, irreflexivity, symmetry, asymmetryaxioms. Finally, SHOI obtained SHOIQ disallowing at-least at-mostrestrictions.3. Motivation Algorithm Overviewsection, present overview main aspects algorithm. explainSection 3.1 root causes scalability problems encountered tableau algorithms,Section 3.2 outline address them. Finally, Section 3.3 discussrelationship algorithm related approaches.3.1 Causes Scalability Problems Tableau Algorithmsshow knowledge base K = (R, , A) satisfiable, tableau algorithm constructsderivationa sequence ABoxes A0 , A1 , . . . , A0 = Ai obtainedAi1 application one derivation rule.2 derivation rules make information implicit axioms R explicit, thus evolve ABox towards(representation a) model K. algorithm terminates either derivation ruleapplicable , case represents model K, containsobvious contradiction, case model construction failed. followingderivation rules commonly used DL tableau calculi.t-rule: Given (C1 C2 )(s), derive either C1 (s) C2 (s).u-rule: Given (C1 u C2 )(s), derive C1 (s) C2 (s).-rule: Given (R.C)(s), derive R(s, t) C(t) fresh individual.-rule: Given (R.C)(s) R(s, t), derive C(t).v-rule: Given GCI C v individual s, derive (C D)(s).t-rule nondeterministic: (C1 C2 )(s) true, C1 (s) C2 (s) true.Therefore, tableau calculi make nondeterministic guess choose either C1 C2 ;one choice leads contradiction, algorithm must backtrack try choice.Thus, K unsatisfiable choices lead contradiction. next discuss twosources complexity inherent tableau derivation rules.3.1.1 Or-BranchingHanding disjunctions reasoning case often called or-branching. v-ruleadds disjunction GCI individual ABox thus major sourceor-branching inefficiency (Horrocks, 2007). Consider, example, knowledge baseK1 = (, T1 , A1 ), T1 A1 specified follows:(2)T1 = {R.A v A}A1 = {A(a0 ), R(a0 , b1 ), R(b1 , a1 ), . . . , R(an1 , bn ), R(bn , ), A(an )}2. formalizations tableau algorithms work completion graphs (Horrocks & Sattler, 2007),natural correspondence ABoxes.169fiMotik, Shearer, & Horrocksa0(i)(ii)(iii)(iv)R.AR.ARRb1R.AR.Aa1an1R.AR.AR.AR.ARbnR.AR.ARR.AR.AFigure 1: Or-Branching ExampleABox A1 graphically shown Figure 1. individuals occurring ABoxrepresented black dots, assertion form A(a0 ) represented placing nextindividual a0 , assertion form R(a0 , b1 ) represented R-labeledarrow a0 b1 . Initially, A1 contains concept assertions shown line (i ).satisfy GCI T1 , tableau algorithm applies v-rule, thus adding assertions shown line (ii ) Figure 1. Tableau algorithms usually free choose orderprocess assertions ABox; fact, finding order exhibits goodperformance practice requires advanced heuristics (Tsarkov & Horrocks, 2005b). Let usassume algorithm chooses process assertions ai bj . Hence,applying derivation rules ai , tableau algorithm derives assertions shownline (iii ) Figure 1; that, applying derivation rules bi , algorithmderives assertions shown line (iv ) Figure 1. ABox contains A(an )A(an ), contradiction. Thus, algorithm needs backtrackrecent choice, flips guess bn1 A(bn1 ). generates contradictionbn1 , algorithm backtracks guesses bi , changes guess A(an ),repeats work bi . also leads contradiction, algorithm mustrevise guess an1 ; then, two guesses possible . general,revising guess ai , possibilities aj , < j n, must reexamined, resultsexponential behavior. None standard backtracking optimizations (Horrocks, 2007)helpful: problem arises order individuals processedmakes guesses ai independent guesses aj 6= j.GCI R.A v A, however, inherently nondeterministic: equivalentHorn clause x, : [R(x, y) A(y) A(x)], applied bottom-up derive assertions A(bn ), A(an1 ), . . . , A(a0 ) eventually reveal contradiction a0 .inferences deterministic,3 conclude K1 unsatisfiable withoutbacktracking. example suggests processing GCIs tableau algorithmsunnecessarily nondeterministic. Hustadt et al. (2005) identified classknowledge bases without unnecessary nondeterminism: knowledge bases expresseddescription logic Horn-SHIQ always translated Horn clauses, suggestingreasoning without nondeterminism possible principle. Ideally, practical DLreasoning procedure exhibit nondeterminism Horn knowledge bases.3. precisely, inference deterministic, order inferences performeddont-care nondeterministic.170fiHypertableau Reasoning Description Logics(a) Ancestor Blocking(b) Anywhere BlockingFigure 2: And-Branching Examplecontext tableau calculi, various absorption optimizations (Horrocks, 2007)developed control nondeterminism arising application GCIs.discuss optimizations depth Section 3.3.1.3.1.2 And-Branchingintroduction new individuals -rule often called and-branching,another major source inefficiency tableau algorithms (Donini, 2007). Consider,example, (satisfiable) knowledge base K2 = (, T2 , A2 ), T2 A2 specifiedfollows (where n integers):(3)T2 = { A1 v 2 S.A2 , . . . , An1 v 2 S.An , v A1 ,Ai v (B1 C1 ) u . . . u (Bm Cm ) 1 n }A2 = { A1 (a) }At-least restrictions dealt tableau algorithms -rule, quitesimilar -rule: ( n R.C)(s), -rule derives R(s, ti ) C(ti ) 1 n,ti 6 tj 1 < j n. Thus, assertion A1 (a) implies existence least twoindividuals A2 , imply existence least two individuals A3 , on.Given K2 , tableau algorithm thus constructs binary tree, shown Figure 2a,individual labeled Ai element = {B1 , C1 } . . . {Bm , Cm }.individuals tree depth n instances ; GCI v A1 ,individuals must instances A1 well, repeat whole constructiongenerate even deeper tree. Clearly, nave application tableau rulesterminate TBox contains existential quantifiers cycles.ensure termination cases, tableau algorithms employ blocking (Baader & Nutt,2007), based important observation shape ABoxesderived input ABox A. individuals called named (shown blackcircles), connected role assertions arbitrary way. individualsintroduced - -rules called blockable (shown white circles). example,R.C(a) expanded R(a, s) C(s), called blockable individualR-successor a. difficult see that, knowledge base contain171fiMotik, Shearer, & Horrocksblockst0s0u0uFigure 3: Forest-Like Shape ABoxesnominals, tableau derivation rule connect arbitrary named individual:individual participate inferences derive assertion form D(s)concept, create new successor s, connect existing predecessor successor,or, presence (local) reflexivity, connect itself. Hence, ABox A0 obtainedseen forest form shown Figure 3: named individualarbitrarily connected named individuals tree blockable successors.concept label LA (s) defined set concepts C C(s) A,edge label LA (s, s0 ) set atomic roles R(s, s0 ) A.forest-like structure ABoxes enables blocking. Description logics SHIQ+SHOIQ+ allow inverse roles number restrictions, handledliterature ancestor pairwise blocking (Horrocks et al., 2000b): individualss, s0 , t, t0 occurring ABox shown Figure 3, blocks (showndouble border s) LA (s) = LA (t), LA (s0 ) = LA (t0 ), LA (s, s0 ) = LA (t, t0 ),LA (s0 , s) = LA (t0 , t).4 tableau algorithms, - -rules applicablenonblocked individuals, ensures termination: number different conceptedge labels exponential |K|, exponentially long branch forest-like ABox mustcontain blocked individual, thus limiting length branch ABox. LetABox Figure 3 tableau derivation rule applicable,blocked t. construct model unravelingthat is, replicatingfragment infinitely often. Intuitively, blocking ensures partABox s0 behaves like part t0 , unravelingindeed generates model. logic able connect blockable individuals nontree-like way, unraveling would generate model; fact, notion ancestors,descendants, blocking would ill-defined.Consider unlucky run tableau algorithm ancestor pairwise blockingK2 . number elements exponential |K2 |, happen blockingcomes effect algorithm constructs exponentially deep tree; sincetree binary, doubly exponential total. lucky run, algorithm alwayspick Bj instead Cj ; then, algorithm constructs polynomially deep binary tree,4. blocking definition must include edge labels directions because, unliketableau formalizations, edge labels include atomic roles.172fiHypertableau Reasoning Description Logicstree exponential total. Thus, and-branching caused - -ruleslead unnecessary generation ABox doubly exponential sizeinput, limits scalability tableau algorithms practice.3.2 Hypertableau Algorithm Glancesection present informal overview hypertableau algorithm addressesproblems due or- and-branching outlined Section 3.1. formalizealgorithm Section 4.3.2.1 Derivation Ruleshyperresolution calculus (Robinson, 1965) oftenV used forWfirst-order theorem proving. works clausesimplications Vform ni=1 Uij=1 Vj UinVj first-orderatoms. conjunction i=1 Ui called antecedent,WVcalled consequent; sometimes omit antecedentdisjunctionjj=1empty. Di possibly empty disjunction literals general unifier(A1 , B1 ), . . . , (Am , Bm ), hyperresolution derivation rule defined follows (assumingunifier exists):5A1 1...DmB 1 . . . B C1 . . . CkD1 . . . Dm C1 . . . Ckmake calculus refutationally complete first-order logic, one additionally needsfactoring derivation rule, discuss further.hypertableau calculus (Baumgartner et al., 1996) based observation that,literals C1 . . . Cn share variables, replace clausenondeterministically chosen atom Ci assume true. assumeclauses safe (i.e., variable occurring clause also occurs clausesantecedent), Ai Di C1 . . . Cn always ground, alwaysnondeterministically split atoms. hypertableau inference writtenA1...B 1 . . . B C1 . . . CkC1 | . . . | Ckgeneral unifier (A1 , B1 ), . . . , (Am , Bm ) | represents or-branching.Horn clauses, inference deterministic,6 calculus exhibits minimalamount dont-known nondeterminism general clauses.hypertableau calculus Baumgartner et al. (1996) easily applied DLs:GCIs translated first-order formulae (Borgida, 1996), converted clauses, shown following example.v R.Bx : [A(x) : R(x, y) B(y)]A(x) B(f (x))A(x) R(x, f (x))5. usual resolution theorem proving assume notation Ai Di imply Aileft-most disjunct disjunction, follow convention.6. mentioned before, order inferences applied nevertheless dont-care nondeterministic.173fiMotik, Shearer, & HorrocksLet ABox containing assertions A(a), R(a, b), B(b). GCI v R.Bclearly satisfied A, need perform inference. clauses obtainedskolemization, however, satisfied A, hypertableau calculus derivesR(a, f (a)) B(f (a)). Hence, skolemization may make calculus perform unnecessaryinferences, may inefficient.Therefore, instead working skolemized clauses, calculus first preprocessesSHOIQ+ knowledge base K pair (K) = (T R (K), (K)),(K) ABoxVn WR (K) set DL-clausesimplications form i=1 Uij=1 Vj , Uiform R(x, y) A(x), Vj form R(x, y), A(x), R.C(x), n R.C(x),x y. preprocessing step introduced formally Section 4.1. DL-clausesR (K) used Hyp-rule, inspired hypertableau derivation rule.example, GCI R.A v B translated DL-clause R(x, y) B(x) A(y); then,ABox contains R(a, b), Hyp-rule derives either B(a) A(b).At-most restrictions translated approach DL-clauses containing equalities;example, axiom v 2 R.B translated DL-clauseA(x) R(x, y1 ) B(y1 ) R(x, y2 ) B(y2 ) R(x, y3 ) B(y3 ) y1 y2 y1 y3 y2 y3 .concept form n R.B encoded using O(log n) bits, correspondingDL-clause contains O(n2 ) literals; thus, translation incurs exponential blowup.believe, however, issue particular approach: tableau algorithmsdeal at-most restrictions using specialized -rule whose application requires O(n)space; thus, translation merely makes exponential space requirement explicit. Consequently, (hyper)tableau algorithms unlikely able handle large numbersnumber restrictions, specialized algorithms, one proposed Faddoul,Farsinia, Haarslev, Moller (2008), may required.translation described previous paragraph, Hyp-rule deriveequalities form t. dealt using -rule: whenever6= t, -rule replaces vice versa assertions A; usuallycalled merging.Apart Hyp- -rule, calculus contains -rule tableaucalculus deals existential quantifiers, -rule detects obvious contradictions (which form 6 s, A(s) A(s)), NI -rule ensurestermination presence nominals, number restrictions, inverse roles. discussNI -rule detail Section 3.2.4.rules algorithm formalized Definition 7 page 193 Table 5 page196, reader may find useful briefly examine definitions continuing.3.2.2 Anywhere Pairwise Blockingemploy pairwise blocking Section 3.1.2 ensure termination calculus;curb and-branching, however, extend anywhere pairwise blocking. key ideaextend set potential blockers beyond ancestors s. so,must avoid cyclic blocks: allowed block block s, neitherguaranteed successors constructed, would render calculus incomplete. Therefore, parameterize algorithm strict ordering individuals174fiHypertableau Reasoning Description LogicsRR.>RRbR.>RbRRcR.>RcR.>Figure 4: Yo-Yo Examplecontains ancestor relation. allow block if, addition conditionsmentioned Section 3.1.2, s. version blocking formalized Definition 7 page 193. Note that, coincides ancestor relation, anywhereblocking becomes equivalent ancestor blocking.Anywhere blocking reduce and-branching practice. Consider knowledgebase K2 Section 3.1.2. exhaust exponentially many members ,subsequently created individuals blocked. best case, always choose Bjinstead Cj , create polynomial path tree use individualspath block siblings, shown Figure 2b. Hence, derivation K2anywhere blocking constructed polynomial time.3.2.3 Problems Due MergingMerging easily lead termination problems even simple DLs, shownfollowing example. simplicity, present TBox set DL-clauses C3 .(4)A3 = { A(a), R.>(a), R(a, b), R(a, a) }C3 = { R(x, y1 ) R(x, y2 ) y1 y2 ,A(x) R(x, y) R.>(y) }Consider derivation calculus A3 C3 illustrated Figure 4:second DL-clause, Hyp-rule derives R.>(b), -rule expands R(b, c); then,first DL-clause, Hyp-rule derives b a, -rule merges b a. Clearly,resulting ABox isomorphic original one (that c blockable b namedindividual relevant here), repeat sequence inferences,leads nontermination. best knowledge, problem first identifiedBaader Sattler (2001), commonly known yo-yo.problem arises because, due merging, unbounded numberblockable R-successors: blockable individual c created R-successor b,merging b makes c blockable R-successor a. This, turn, allows us applyDL-clauses C3 arbitrary number times, leads nontermination.problem solved always merging descendant ancestor t,pruning mergingthat is, removing assertions containing blockable descendant thus ensuring inherit new successors.7 Pruning formallydefined Definition 7 page 193.7. Horrocks et al. (2000b) physically remove successors, mark present settingrelevant edge labels . exactly effect pruning.175fiMotik, Shearer, & HorrocksBR.CR.BRs1RCS.{c}R.Bs2BR.CRs1cbRR.Bs3BR.CRs4bR.BCS.{c}Rs3CS.{c}Rs2cRBR.CFigure 5: Non-Tree-Like Structures Due MergingThus, merging b example, prune bthat is, removeassertion R(b, c). Merging produces ABox represents model A3 C3 ,algorithm terminates. Note pruning well-defined ABoxesforest-shaped, cf. Figure 3: connections individuals arbitrary and,particular, cyclic, would clear part ABox pruned.3.2.4 Nominalsnominals, possible derive ABoxes forest-like, followingsimple example demonstrates. presentation purposes, use concept R.{c}DL-clauses even though concepts would decomposed algorithm.(5)A4 = { A(a), A(b) }C4 = { A(x) (R.B)(x), B(x) (R.C)(x), C(x) (S.{c})(x) }Successive applications Hyp- -rules A4 C4 produce ABoxA14 shown left-hand side Figure 5. ABox clearly forest-shaped:two paths role atoms A14 start named individuals b end namedindividual c. Nevertheless, role relations blockable individuals remain forestlike, termination derivation ensured using blocking. DLs includenominals produce extended forest-like ABoxes (Horrocks & Sattler, 2001).DL includes inverse roles, number restrictions, nominals, shape ABoxbecomes much involved. end, assume extend C4 DL-clauseS(y1 , x) S(y2 , x) y1 y2 (which axiomatizes inverse-functional effectivelyintroduces number restrictions). A14 , Hyp-rule derives s2 s4 . Notes2 s4 blockable individuals; furthermore, neither individual ancestorother, merge, say, s4 s2 . produces ABox A24 shown righthand side Figure 5, assertion R(s3 , s2 ) makes A24 forest-shaped.extending example, possible use nominals, inverse roles, number restrictionsarrange blockable individuals cycles. derived ABoxes thus forest-shaped,makes defining suitable notions pruning unraveling difficult prevents ususing blocking ensure termination calculus.176fiHypertableau Reasoning Description Logicss1s2s1s2cbs3s4s1s2cbs3s4bcs3Figure 6: Introduction Root Individualssolve problem, need extend arbitrarily interconnected part A24changing status s2 blockable root individual is, individual similar named ones arbitrarily interconnected. extended forest-likeABoxes thus consist set arbitrarily interconnected root individualsroot tree (ignoring reflexive connections connections back root individuals) otherwise consists entirely blockable individuals (see Figure 3 page 172).Named individuals subset root individuals occur input ABox.talk individuals, mean either root blockable ones (see Definition 7page 193 formal definition).Returning example, changing status s2 blockable rootindividual, s1 s3 blockable A24 , ABox extended forest-likeshape apply blocking pruning usual. schematically shownFigure 6. generally, apply following preliminary version NI -rule,denote (*) easier reference:change root individual whenever contains assertions R(s, a)A(s) root named individual, blockable individualsuccessor a, must satisfy at-most restriction n R .A.Note that, successor a, part ABox involving forestshaped, NI -rule need applicable.solution, however, introduces another problem: number root individualsgrow arbitrarily, shown following example.(6)A5 ={ A(b) }A(x) (R.A)(x),A(x) (S.{a})(x),C5 =S(y1 , x) S(y2 , x) S(y3 , x) y1 y2 y2 y3 y1 y3A5 C5 , calculus produce ABox A15 shown left-hand side Figure7. ABox A15 explicitly contain at-most restriction concepts, precondition(*) cannot checked directly; shall discuss issue shortly. moment,however, please note last DL-clause C5 corresponds axiom > v 2 .>,individuals c seen satisfying precondition (*); therefore, changeroot individuals. Furthermore, third DL-clause C5 satisfied,Hyp-rule derives c b, -rule merge c b. Since blockableindividual, cannot prune it, obtain ABox A25 shown middle Figure 7.88. reduce clutter, repeat labels individuals.177fiMotik, Shearer, & HorrocksRbR.AS.{a}bRcRRRbRReFigure 7: Yo-Yo Root IndividualsSince R.A(d) satisfied, extend A25 R(d, e), A(e), R.A(e), S.{a}(e),S(e, a) produce ABox A35 shown right-hand side Figure 7. Individuale seen satisfying precondition (*), changed root individual.ABox isomorphic A15 , repeat inferences forever.solve problem NI -rule refines (*). Assume containsindividual satisfies precondition (*)that is, contains assertions R(s, a)A(s), root named individual, blockable individualsuccessor a, must satisfy at-most restriction n R .A. model A,n different individuals bi participate assertions form R(bi , a)A(bi ). Hence, associate set n fresh root individuals {b1 , . . . , bn }represent R -neighbors a. turn root individual nondeterministicallychoosing bj set merging bj . way, number new rootindividuals introduced result at-most restriction n R .Alimited n. complete definition NI -rule given Table 5 page 196.example Figure 7, NI -rule introduces two fresh root individuals.NI -rule applied third time, instead introducing e, one previouslyintroduced root individuals reused, ensures termination calculus.formulating NI -rule, faced technical problem: at-most restriction concepts translated calculus DL-clauses, makes testingcondition previous paragraph difficult. example, application Hyprule third DL-clause (6) (obtained axiom > v 2 .>) produceequality c b; equality alone reflect fact must satisfyat-most restriction 2 .>. enable application NI -rule, introduceannotated equalities annotations establish association at-mostrestriction. third DL-clause (6) thus represented algorithm follows:(7)S(y1 , x) S(y2 , x) S(y3 , x)y1 y2 @x2 .> y2 y3 @x2 .> y1 y3 @x2 .>Hyp-rule derives c b @a2 .> , meaning c b; however,annotation says that, since must satisfy at-most restriction 2 .>, bc must also merged one (two) individuals reserved -neighbors a.178fiHypertableau Reasoning Description LogicsR.BRBR.CbCS.DR.BcCS.DR.BcR.BRRBR.CRBR.CbcecCS.DCS.DCS.D(a) Nonterminating VariantRRbRRR.BCS.DBR.C(b) Terminating VariantFigure 8: Caterpillar Example3.2.5 Nominals Mergingintroduction NI -rule leads another problem: repeated merging rootindividuals lead nontermination caterpillar derivation. Consider, example,application hypertableau calculus following knowledge base:S(a, a), R.B(a)A6 =B(x) R.C(x),C(x) S.D(x),(8)C6 =D(x) x a,S(y1 , x) S(y2 , x) y1 y2 @x1 .>ABox first DL-clause cause introduction two new blockable individuals b c; next two DL-clauses connect c role S; last DL-clauseproduces c c @x1 .> ; application NI -rule assertion causes cbecome root individual. ABox A16 resulting inferences shownleft-hand side Figure 8a. Since inverse-functional, individuals c mustmerged. individual c root, longer descendant a, choosemerge c. blockable individual b pruned (in order avoid problemsoutlined Section 3.2.3), resulting ABox shown middle part Figure8a. existential restriction R.B c, however, satisfied, similar sequencerule applications constructs ABox A26 shown right-hand side Figure 8a.ABox isomorphic A16 , inferences repeated forever.problem intuitively explained following observation. NI -ruleintroduces fresh root individuals neighbors existing root individual; thus,179fiMotik, Shearer, & Horrocksroot individual ABox seen part chain showing individualcaused introduction root individual. chain initially anchorednamed individual: individuals occur input ABox introducedNI -rule. length path blockable individuals used limit lengthchains root individuals. allow chain anchors removed ABox,chains remain limited length given ABox; however, coursederivation, one end chain extended indefinitely end shortened.solve problem allowing named individuals mergednamed individuals, specified postcondition -rule Table 5 page196. ensures chain root individuals always remains anchored namedindividual. example, instead merging c, merge c a, resultsABox shown Figure 8b. derivation rule applicable ABox,algorithm terminates.3.2.6 NI -Rule UnravelingNI -rule required ensure ABoxes forest shaped, alsoenable application blocking unraveling. Consider, example, knowledgebase shown (9), omit annotations equalities sake clarity.Intuitively, axioms knowledge base state individual R neighbors, infinite chain individuals -neighbora.(9)A7 = {A(a), (R.B)(a), }A(x)R(y,x),B(x)(R.B)(x),B(x)(S.{a})(x),R(y1 , x) R(y2 , x) y1 y2 ,C7 =S(y1 , x) S(y2 , x) S(y3 , x) S(y4 , x)y1 y2 y1 y3 y1 y4 y2 y3 y2 y4 y3 y4 ,Without NI -rule, application calculus A7 C7 might produceABox A17 shown Figure 9a. individual blocked A17 individual c,derivation terminates. Note last DL-clause C7 (which correspondsaxiom > v 3 .>) satisfied: individual A17 -neighborstwo neighbors. construct model A17 , unravel blocked partsABoxthat is, construct infinite path extends past duplicatingfragment model c infinite number times. This, however,creates additional -neighbors a, invalidates last DL-clause C7 ; thus,unraveled ABox define model A7 C7 .NI -rule elegantly solves problem. Since must satisfy at-most restrictionform 3 .>, soon S(b, a), S(c, a), S(d, a) derived, NI -rule appliedturn b, c, root individuals. corrects problems unraveling: rootindividuals become blocked, introduce another fresh blockable individual e.individual merged another -neighbor a, producing individual twoR -neighbors, illustrated Figure 9b. R inverse-functional, however, neighborsmerged. Merging continues b merged a, causing become180fiHypertableau Reasoning Description LogicsR.BRRbBR.BS.{a}cBR.BS.{a}RBR.BS.{a}(a) Premature BlockingR.BRbBR.BS.{a}R.BRRcBR.BS.{a}RRBR.BS.{a}ebBR.BS.{a}BR.BS.{a}RcBR.BS.{a}RRBR.BS.{a}(b) Correct DerivationFigure 9: NI -rule UnravelingR-neighbor, point algorithm correctly determines knowledge baserepresented A7 C7 unsatisfiable.3.3 Related Work3.3.1 Hypertableau vs. AbsorptionAbsorption extensively used tableau calculi address problems orbranching outlined Section 3.1.1 (Horrocks, 2007). basic absorption algorithm triesrewrite GCIs form v C atomic concept. preprocessing,instead deriving C individual ABox, C(s) derived ABoxcontains A(s); thus, nondeterminism introduced absorbed GCIs localized.basic technique refined extended several ways. Negative absorption rewritesGCIs form v C atomic concept; then, C(s) derivedABox contains A(s) (Horrocks, 2007). Role absorption rewrites GCIs formR.> v C; then, C(s) derived ABox contains R(s, t) (Tsarkov & Horrocks,181fiMotik, Shearer, & Horrocks2004). Binary absorption rewrites GCIs form A1 u A2 v C; then, C(s) derivedABox contains A1 (s) A2 (s) (Hudek & Weddell, 2006).techniques proven indispensable practice; however, analysis shows potential improvement. example, axiom R.A v (2) cannot absorbed directly, applying role absorption (2) produces axiom R.> v R.Acontaining disjunction consequent. Binary absorption directly applicable(2) since axiom contain two concepts left-hand side v, algorithm Hudek Weddell (2006) additionally transforms (2) absorbable axiomv R .A. Consider, however, following axiom:> v R.C S.D(10)binary absorption algorithm process two disjuncts (10) two ways.R.C processed S.D, (10) transformed axioms shown (11),applied deterministically tableau algorithm. If, however, S.Dprocessed R.C, (10) transformed axioms shown (12). firstaxiom absorbable, second not, tableau algorithm nondeterministic.(11)(12)C v R .Q1Q1 v S.D> v .Q2Q2 v R.CHeuristics used practice find good absorption (see, e.g., Wu & Haarslev, 2008),guarantees result incur least amount nondeterminism;even Horn knowledge bases, reasoning without nondeterminismpossible principle (Hustadt et al., 2005). contrast, algorithm guaranteedpreprocesses Horn knowledge base Horn DL-clauses always resultdeterministic derivations. example, (10) transformed Horn DL-clause (13).(13)R(x, y1 ) C(y1 ) S(x, y2 ) D(y2 )Even case inherently nondeterministic knowledge bases, absorptionoptimized. Consider axiom (14), translated DL-clause (15):(14)(15)> v R.B S.CR(x, y1 ) S(x, y2 ) A(x) B(y1 ) C(y2 )binary absorption algorithm transforms (14) following axioms:(16)Q1 u Q2 v(17)> v B R .Q1(18)> v C .Q2Axiom (16) absorbable; however, (17) (18) not, application introducesnondeterministic choice point individual occurring ABox. problemameliorated using role absorption transforming (17) (18) (19) (20):(19)R .> v B R .Q1182fiHypertableau Reasoning Description Logics(20).> v C .Q2(19) used derive (B R .Q1 )(b) R(a, b), (20) usedderive (C .Q2 )(d) S(c, d); however, two disjunctions derived even6= c. contrast, DL-clause (15) derives disjunction = c; thus, literalsR(x, y1 ) S(x, y2 ) (15) act guards. presence variables antecedent(the shared variable x example) makes guards selective guardapplied isolation. Furthermore, = c, derive disjunction A(a) B(b) C(d),involves three different individuals (a, b, case); contrast, consequencestableau algorithms typically involve one individual. Thus, usagevariables, DL-clauses global effect tableau rules.best knowledge, known absorption technique localize effectsaxioms number restrictions, (21).(21)2 R.B vorder ensure instances B counted, tableau algorithms need includechoose-rule that, assertion R(a, b), nondeterministically derives B(b) B(b).hypertableau setting, however, (21) translated following DL-clause:(22)R(x, y1 ) R(x, y2 ) B(y1 ) B(y2 ) A(x) y1 y2choose-rule needed, DL-clause simply applied assertions formR(a, b), B(b), R(a, c), B(c); furthermore, conclusion tautology whenever b = c.presence guard atoms antecedent (22) thus significantly reducesnondeterminism introduced number restrictions. Furthermore, Horn knowledgebases number restrictions (which includes common case functional roles),calculus exhibits nondeterminism; contrast, tableau calculi still need choose-rule,introduces nondeterminism even GCIs fully absorbed.hypertableau calculus presented paper generalize negative absorption directly; example, negatively absorbed axiom (23) translatedDL-clause (24) applied individuals ABox.(23)(24)v BA(x) B(x)Negative absorption can, however, easily applied setting: negatively absorbatomic concept A, simply replace input ABox DL-clauses occurrencesA0 A0 fresh concept, move literals involving A0appropriate side DL-clauses. example, (24) would thus converted (25),applied deterministically.(25)A0 (x) B(x)Note transform DL-clause A(x) B(x) A0 (x) B(a); however,similar situation arises tableau calculi, applying negative absorption v Bmeans v B cannot absorbed.183fiMotik, Shearer, & Horrockssummarize, unlike various absorption techniques guided primarily heuristics, hypertableau calculus provides framework captures variants absorptionaware of, guarantees deterministic behavior whenever input knowledge baseHorn, eliminates need nondeterministic choose-rule, allows powerful use guard atoms localize remaining nondeterminism. Furthermore,Section 4.1.3 show calculus provides proof-theoretic framework DLsuniformly handle certain useful extensions SHOIQ+ .3.3.2 Relationship CachingVarious caching optimizations used reduce sizes models constructedknowledge base classification (Ding & Haarslev, 2006; Horrocks, 2007). proposed approaches, caching used parallel blockingthat is, caching aloneguarantee termination calculus, caching must carefully integratedblocking order affect soundness and/or completeness. integration particularly problematic presence inverse roles. contrast, anywhere blocking alonesufficient guarantee termination calculus. Furthermore, Section 6.2 presentoptimization anywhere blocking seen simple effective formgeneral caching. Finally, discuss Section 7, efficient implementation anywhereblocking obtained using simple techniques. Thus, anywhere blocking achievesmany effects caching without much added complexity.Donini Massacci (2000) used anywhere blocking caching unsatisfiableconcepts obtain tableau algorithm DL ALC runs single exponential time.Gore Nguyen (2007) presented algorithm DL SHI also runs exponential time achieves termination solely caching satisfiable unsatisfiableconcepts. algorithms, however, seem incompatible absorption variants,latter essential making tableau algorithms practical. Furthermore,unclear extend algorithms DLs provide number restrictions, nominals,inverse roles, SHOIQ+ .3.3.3 Relationship First-Order Calculioriginal hypertableau calculus first-order logic subsequently extendedequality implemented KRHyper theorem prover (Baumgartner, Furbach, & Pelzer, 2008). calculus used finite model generation, decidesfunction-free clause logic.Hyperresolution splitting used decide several description modallogics (Georgieva et al., 2003; Hustadt & Schmidt, 1999). approaches, however, relyskolemization, which, discussed previously, inefficient practice.Furthermore, approaches deal logics much weaker SHOIQ+ ;particular, aware hyperresolution-based decision procedure handleinverse roles, number restrictions, nominals.hypertableau calculus related Extended Positive (EP) tableau calculusfirst-order logic Bry Torge (1998). Instead relying skolemization, EP satisfiesexistential quantifiers introducing new constants, done way makescalculus complete finite satisfiability. EP is, however, unlikely practical due184fiHypertableau Reasoning Description Logicshigh degree nondeterminism. Furthermore, EP provide decision procedureDLs SHOIQ+ enjoy finite model property (Baader & Nutt,2007). Consider, example, knowledge base whose TBox contains axioms (26)(27), whose ABox contains assertion (28):(26)v R.A(27)> v 1 R .>(28)(A u R.A)(a)EP try satisfy existential quantifier reusing athat is, addingassertions R(a, a) A(a). leads contradiction, EP backtrack, introducefresh individual b, add assertions R(a, b) A(b); satisfy (26), alsoadd R.A(b). satisfy existential quantifier latter assertion, EP tryreuse a; fail, try reuse b adding assertion R(b, b). Due(27), however, b merged a, results contradiction; therefore, EPbacktrack, introduce yet another fresh individual c add assertions R(b, c), A(c),R.A(c). repeating argument, easy see EP generate ever largermodels terminate. unsurprising since knowledge base satisfiedinfinite models. achieve termination knowledge bases, EP would needextended blocking techniques ones described paper.Baumgartner Schmidt (2006) developed so-called blocking transformation firstorder clauses, improve performance bottom-up model generation methods.Roughly speaking, clauses modified way makes bottom-up calculus derive6 term subterm t; then, application paramodulationachieves effect analogous reusing instead EP tableaucalculus. transformation, however, ensure termination DLsfinite model property. example, reasons explained previous paragraph, hyperresolution splitting terminate clauses obtainedapplication blocking transformation (the clauses corresponding to) (26)(28).Furthermore, even DLs enjoy finite model property, unlucky sequenceapplications derivation rules prevent bottom-up model generation methodblocking terminating (please refer Section 3.2.3 details).4. Satisfiability Checking Algorithmpresent hypertableau algorithm used check satisfiabilitySHOIQ+ knowledge base K. algorithm consists two phases: preprocessingphase described Section 4.1, hypertableau phase described Section 4.2.4.1 Preprocessinggoal preprocessing phase transform SHOIQ+ knowledge base KABox (K) set DL-clauses R (K) equisatisfiable K.Definition 1 (DL-Clause). concepts >, , concepts formatomic concept called literal concepts. Let NV set variables disjoint185fiMotik, Shearer, & HorrocksTable 2: Satisfaction DL-Clauses InterpretationI, |= C(s)I, |= R(s, t)I, |=WnVI, |=j=1 Vji=1 UiiffiffiffiffWnV|=j=1 Vji=1 Ui|= CiffiffsI, ChsI, , tI, RIsI, = tI,I, |= Ui 1 impliesI, |= Vj 1 j nWnVI, |=j=1 Vj mappingsi=1 Ui|= r DL-clause r Cset individuals NI . atom expression form B(s), n S.B(s), R(s, t),t, individuals variables, B literal concept, R atomic role, (notnecessarily atomic) role, n positive integer. DL-clause expression formU1 . . . Um V1 . . . VnUi Vj atoms, 0, n 0. conjunction U1 . . . Um calledantecedent, disjunction V1 . . . Vn called consequent. empty antecedentempty consequent DL-clause written > , respectively.Let = (4I , ) interpretation : NV 4I mapping variables elements interpretation domain. Let aI, = aI individual xI, = (x)variable x. Satisfaction atom, DL-clause, set DL-clauses Cdefined Table 2.4.1.1 Elimination Transitivity AxiomsTransitivity axioms handled tableau algorithms + -rule: R transitiveABox contains R.C(s) R(s, t), + -rule derives R.C(t). algorithm,however, concepts form R.C translated DL-clauses, + -rule cannotapplied. Therefore, instead handling transitivity directly, encode SHOIQ+knowledge base K equisatisfiable ALCHOIQ+ knowledge base (K). encodingeliminates transitivity axioms, simulates effects using additional GCIs.Definition 2. Given SHOIQ+ knowledge base K = (R, , A), concept closure Ksmallest set concepts clos(K)C v , nnf(C D) clos(K),C(a) A, nnf(C) clos(K),C clos(K) syntactically occurs C, clos(K),n R.C clos(K), Cclos(K),R.C clos(K), vR R, Tra(S) R, S.C clos(K).186fiHypertableau Reasoning Description Logics-encoding K ALCHOIQ+ knowledge base (K) = (R0 , 0 , A) R0obtained R removing transitivity axioms0 = {R.C v S.(S.C) | R.C clos(K), vR R, Tra(S) R}.Similar encodings known various description (Tobies, 2001) modal (Schmidt& Hustadt, 2003) logics. Note that, order guarantee decidability (Horrocks, Sattler,& Tobies, 2000a), number restrictions local reflexivity allowed SHOIQ+simple rolesthat is, roles transitive subroles; similar reasons, roledisjointness, irreflexivity, asymmetry axioms also allowed simple roles.Lemma 1. SHOIQ+ knowledge base K satisfiable (K) satisfiable,(K) computed time polynomial |K|.full proof analogous result DL SHIQ given Motik (2006) Theorem 5.2.3, generalization result SHOIQ+ straightforward; therefore,omit proof Lemma 1 sake brevity. elimination transitivityaxioms, distinction simple complex roles. Hence, restpaper assume roles simple unless otherwise stated and, without lossgenerality, treat R.B syntactic shortcut 1 R.B.4.1.2 Normalizationtranslation set DL-clauses, knowledge base first broughtnormalized form. done order make negations explicit, ensureresulting DL-clauses compatible blocking.understand first issue, consider axiom v (R.R.R.B). Convertingaxiom DL-clauses straightforward implicit negations; example,concept seemingly negated but, due negation implicit implication,actually occurs positively axiom. Therefore, replace axiom followingequivalent axiom. makes negations explicit, result easily translatedDL-clause.(29)> v R.R.R.BR(x, y1 ) R(y1 , y2 ) R(y2 , y3 ) B(y3 ) A(x)understand second issue, consider knowledge base K8 , consisting ABoxA8 TBox corresponds set DL-clauses C8 .(30)A8 = { A(a), B(a) }C8 = { R(x, y1 ) R(y1 , y2 ) R(y2 , y3 ) B(y3 ) A(x),B(x) R.B(x) }applying rules Section 3.2, algorithm constructs K8 ABox shownFigure 10. According definition blocking introduced Definition 7,9 cblocked b; furthermore, rule applicable ABox, algorithm terminates,leading us believe K8 satisfiable. ABox, however, represent modelK8 : expand R.B(c) R(c, d) B(d), first DL-clause C8 derive9. version blocking introduced Definition 7 differs one presented Section 3.1.2concept label LA (s) individual consists atomic concepts A(s) A.187fiMotik, Shearer, & HorrocksBR.BRbBR.BRcBR.BFigure 10: Incorrect Blocking due Lack NormalizationA(a), contradicts A(a). problem arises antecedentfirst DL-clause C8 checks path three R-successors, whereas pairwise blockingcondition ensures paths length two fully constructed. Intuitively,antecedents DL-clause check paths fit fully constructedmodel fragments. ensure renaming complex concepts simpler ones.Thus, transform culprit DL-clause following ones, checkpaths length one.(31)> v R.Q1R(x, y) Q1 (y) A(x)(32)> v Q1 R.Q2R(x, y) Q2 (y) Q1 (x)(33)> v Q2 R.BR(x, y) B(y) Q2 (x)application DL-clauses ABox shown Figure 10 would additionallyderive Q2 (a), Q2 (b), Q1 (a), c would blocked. calculus would expandR.B(c) discover contradiction.formalize ideas, define normalized form DL knowledge bases.FDefinition 3 (Normalized Form). GCI normalized form > v ni=1 Ci ,Ci form B, {a}, R.B, R.Self, R.Self, n R.B, n R.B,B literal concept, R role, n nonnegative integer.TBox normalized GCI normalized. ABox normalizedconcept assertion contains literal concept, role assertion containsatomic role, contains least one assertion. ALCHOIQ+ knowledgebase K = (R, , A) normalized normalized.following transformation used normalize knowledge base.Definition 4 (Normalization). ALCHOIQ+ knowledge base K, knowledge base(K) computed shown Table 3.Normalization seen variant well-known structural transformation(Plaisted & Greenbaum, 1986; Nonnengart & Weidenbach, 2001). applicationstructural transformation (29) would replace complex subconcept positiveatomic concept, eventually producing > v R.Q1 . axiom cannot translatedHorn DL-clause, whereas (29) can; thus, structural transformation destroy188fiHypertableau Reasoning Description LogicsTable 3: Functions Used Normalization()(> v nnf(C1 C2 ))RAC1 vC2(> v C C 0 ) = (> v C C 0 )(> vC 0 Ci )(K) = {>(a)}1inC 0 form C 0 = C1 u . . . u Cn n 2(> v C R.D) = (> v C R.D ) (> vD)(> v C n R.D) = (> v C n R.D ) (> vD)(> v C n R.D) =(> v C n R.) (> vD)C empty,(> v C {s}) =(C(s)) otherwise.(D(s)) = {D (s)} (> vnnf(D))(R (s, t)) = {R(t, s)}() = {} axiomQCpos(C) = trueC =, QC fresh atomic concept unique CQC pos(C) = falsepos(>) = falsepos() = falsepos(A) = truepos(A) = falsepos({s}) = truepos({s}) = falsepos(R.Self) = truepos(R.Self) = falsepos(C1 u C2 ) = pos(C1 ) pos(C2 )pos(C1 C2 ) = pos(C1 ) pos(C2 )pos(C1 ) n = 0pos(R.C1 ) = pos(C1 )pos( n R.C1 ) =pos( n R.C1 ) = truetrueotherwiseNote: atomic concept, C(i) arbitrary concepts, C possibly emptydisjunction arbitrary concepts, literal concept, fresh individual.Note commutative, C 0 C C 0 necessarily right-most disjunct.Horn-ness. prevent this, introduce function pos(C) (c.f. Table 3) returnsfalse clausification C require adding atoms consequent DLclause. replace occurrence concept C concept negative literalconcept QC pos(C) = false, positive literal concept QC pos(C) = true.Special care must taken replacing concept concept n R.D: sinceoccurs n R.D implicit negation, replaceorder preserveHorn-ness. Horn knowledge base K (Hustadt et al., 2005), normalization performsreplacements one presented Hustadt et al., (K) Horn knowledgebase well.Lemma 2. following properties hold ALCHOIQ+ knowledge base Kcorresponding knowledge base (K):K satisfiable (K) satisfiable;(K) normalized;189fiMotik, Shearer, & Horrocks(K) computed time polynomial |K|.Proof. (Sketch) Since transformation seen syntactic variant structuraltransformation, proof K (K) equisatisfiable completely analogousones Plaisted Greenbaum (1986) Nonnengart Weidenbach (2001),omit sake ofFbrevity. second claim, note essentially rewritesGCI form > v ni=1 Ci keeps replacing nested subconcepts CiGCI becomes normalized; adds >(a) ABox ABox empty;replaces inverse role assertions equivalent assertions atomic roles. Thus,(K) normalized. Finally, occurrence concept K replaced newatomic concept once, necessary syntactic transformations performedpolynomial time, (K) computed polynomial time.4.1.3 Translation DL-Clausesintroduce notion HT-clausessyntactically restricted DL-clauseshypertableau calculus guaranteed terminate. rest paper, oftenuse function ar, which, given role R variables constants t, returnsatom semantically equivalent R(s, t) contains atomic role; is,R(s, t) R atomic rolear(R, s, t) =.S(t, s) R inverse role R =Definition 5 (HT-Clause). assume that, individual a, set atomic concepts NC contains unique nominal guard concept denote Oa ; furthermore,assume nominal guard concepts occur input knowledge base.annotated equality atom form @un S.B , s, t, uconstants variables, n nonnegative integer, role, B literal concept;part @un S.B atom called annotation. atom semantically equivalentt.10HT-clause DL-clause r following form, 0 n 0:(34)U1 . . . Um V1 . . . VnFurthermore, must possible separate variables center variable x, setbranch variables yi , set nominal variables zj following propertieshold, atomic concept, B literal concept containing nominal guard concept,Oa nominal guard concept, R atomic role, role.atom antecedent r form A(x), R(x, x), R(x, yi ), R(yi , x),A(yi ), A(zj ).atom consequent r form B(x), h S.B(x), B(yi ), R(x, x),R(x, yi ), R(yi , x), R(x, zj ), R(zj , x), x zj , yi yj @xh S.B .yi occurs antecedent r atom form R(x, yi ) R(yi , x).10. explained Section 3.2.4, annotations used ensure termination hypertableau phase.190fiHypertableau Reasoning Description Logicszj occurs antecedent r atom form Oa (zj ).equality yi yj @xh S.A consequent r occurs subclause rform (35) 1 , . . . , h+1 branch variables k 1 k h + 1occurs elsewhere r.(35)...h+1^_k=11k<`h+1[ar(S, x, k ) A(y k )] . . . . . .k ` @xh S.A . . .equality yi yj @xh S.A consequent r occurs subclause rform (36) 1 , . . . , h+1 branch variables k 1 k h + 1occurs elsewhere r.(36)...h+1^ar(S, x, k ) . . . . . .k=1h+1_A(y k )k=1_k ` @xh S.A . . .1k<`h+1HT-clauses general strictly needed capture ALCHOIQ+ knowledge bases. example, HT-clauses form R(x, y) A(y) S(x, y) express formrelativized role inclusions, HT-clauses form R(x, y) S(y, x) U (x, y) (y, x)capture safe role expressions (Tobies, 2001).show transform normalized ALCHOIQ+ knowledge base setHT-clauses, explain need nominal guard concepts.Definition 6 (Clausification). clausification normalized ALCHOIQ+ knowledgebase K = (R, , A) pair (K) = (T R (K), (K)) R (K) set DLclauses (K) ABox, obtained shown Table 4.Definition 3, concepts form {a} converted ABox assertionsnormalization, Table 4 need handle them. Positive nominal concepts naturally translated equalities containing constants; example, > v {a} corresponds A(x) x a. DL-clauses impractical: given equality assertionb, -rule would need replace occurrences b assertions, DL-clauses well; thus, mentioned DL-clause replacedA(x) x b. avoid need changing set DL-clauses derivation,extract constants ABox; example, > v {a} transformedDL-clause A(x) Oa (z{a} ) x z{a} assertion Oa (a). constants thuspushed assertions, -rule perform replacements ABox.Lemma 3. Let K normalized ALCHIQ knowledge base. Then, K equisatisfiable(K) = (T R (K), (K)), R (K) contains HT-clauses.Proof. inspecting Table 4, R (KB) clearly contains HT-clauses. followingequivalences DL concepts first-order formulae well known (Borgida, 1996):R.B(x) : R(x, y) B(y)Vn R.B(x) y1 , . . . , yn+1 :[R(x, yi ) B(yi )]1in+1{a}(x) x191W1i<jn+1yi yjfiMotik, Shearer, & HorrocksTable 4: Translation Normalized Knowledge Base HT-Clauses(T ) = {nVlhs(Ci )i=1nWnFrhs(Ci ) | > vi=1Ci }i=1R (R) = {ar(R, x, y) ar(S, x, y) | R v R}{ar(S1 , x, y) ar(S2 , x, y) | Dis(S1 , S2 ) R}{> ar(R, x, x) | Ref(R) R}{ar(S, x, x) | Irr(S) R}{ar(R, x, y) ar(R, y, x) | Sym(R) R}{ar(S, x, y) ar(S, y, x) | Asy(S) R}R (K) = (T ) R (R)(K) = {Oa (a) | {a} occurring K}Note: Whenever lhs(Ci ) rhs(Ci ) undefined, omitted HT-clause.Clhs(C)rhs(C)A(x)A(x){a}Oa (zC )x zCn R.An R.A(x)n R.An R.A(x)R.Selfar(R, x, x)R.Selfar(R, x, x)R.Aar(R, x, yC )R.An R.AA(yC )ar(R, x, yC ) A(yC )n+1VWi=11i<jn+1) A(y )][ar(R, x, yCCn+1Vn R.Ai=1)ar(R, x, yCn+1Wi=1)A(yCj @xyCC n R.AW1i<jn+1j @xyCC n R.A(i)Note: yC zC fresh variable unique C (and i).Let 0T R (K) set HT-clauses defined like R (K), differencelhs({a}) = > rhs({a}) = x a. Then, (0T R (K), (K)) obtained K replacing concepts form R.B, n R.B {a} equivalent first-order formulae,K (0T R (K), (K)) clearly equisatisfiable. show (0T R (K), (K))equisatisfiable (T R (K), (K)).() model 0 (0T R (K), (K)) extended model (T R (K), (K))0setting OaI = {aI } nominal guard concept Oa .192fiHypertableau Reasoning Description Logics() model (K) model (0T R (K), (K)): 0T R (K),R (K) Oak (ak ) (K), form shown below.VWW= V Ui V Vj nk=1 xk aWkW=Ui nk=1 Oak (z{ak } ) Vj nk=1 xk z{ak }Wdisjunction nk=1 xk ak true valuesx1 , . . . , xn , clearly would true values x1 , . . . , xn .4.2 Hypertableau Calculus HT-Clausespresent hypertableau calculus deciding satisfiability ABoxset HT-clauses C. explained Section 3, algorithm uses several typesindividuals. individual either root blockable summarized next; refersimply individual, mean either root blockable one.Root individuals either occur input ABox, introducedNI -rule. important characteristic connected arbitrary,tree-like, ways.Root individuals occur input ABox called named individuals.Root individuals introduced NI -rule defined finite stringsform a.1 . . . . .n named individual, ` formhR.B.ii, n 0. Root individuals introduced applying NI -ruleassertion @un R.B form u.hR.B.ii 1 n.Blockable individuals introduced -rule, make tree-like partsmodel. set blockable individuals disjoint set root individuals.Blockable individuals defined finite strings form s.i1 .i2 . . . . .inroot individual, i` integer, n 1. string representationnaturally induces parentchild relationship individuals; example, s.2second child individual s, either blockable root.introduce algorithm.Definition 7 (Hypertableau Algorithm).Individuals. Given set named individuals NI , set root individualssmallest set NI and, x , x.hR, B, ii roleR, literal concept B, positive integer i. set individuals NA smallestset NA and, x NA , x.i NA positive integer i.individuals NA \ blockable individuals. blockable individual x.i successorx, x predecessor x.i. Descendant ancestor transitive closuressuccessor predecessor, respectively.ABoxes. hypertableau algorithm operates ABoxes obtained extendingstandard definition Section 2 follows.addition assertions Section 2, ABox contain annotated equalityassertions special assertion false interpretations. Furthermore,assertions refer individuals NA NI .193fiMotik, Shearer, & Horrocks(in)equality (s 6 t) also stands symmetric (in)equality (t 6 s).true annotated equalities.ABox contain renamings form 7 b b root individuals. Let 7 reflexive-transitive closure 7 A. individual bcanonical name root individual A, written b = kakA , b individual 7 b exists individual c 6= b b 7 c;individual exists, kakA = a.11input ABox ABox containing named individuals, annotated equalities,renamings, concepts literal roles atomic.Satisfaction ABoxes interpretation obtained straightforward generalization definitions Section 2: individuals interpreted elementsinterpretation domain 4I , |= 7 b iff aI = bI .Pairwise Anywhere Blocking. labels individual individualpair hs, ti ABox defined follows:LA (s) = { | A(s) atomic concept }LA (s, t) = { R | R(s, t) }Let strict ordering (i.e., transitive irreflexive relation) NA containingancestor relationthat is, s0 ancestor s, s0 s. induction ,assign individual status follows:blockable individual directly blocked blockable individualfollowing conditions satisfied, s0 t0 predecessors t, respectively:blocked,s,LA (s) = LA (t) LA (s0 ) = LA (t0 ),LA (s, s0 ) = LA (t, t0 ) LA (s0 , s) = LA (t0 , t);indirectly blocked iff predecessor blocked;blocked iff either directly indirectly blocked.Pruning. ABox pruneA (s) obtained removing assertions containingdescendant s.Merging. ABox mergeA (s t) obtained pruneA (s) replacing individual individual assertions annotations (but renamings)and, root individuals, adding renaming 7 t.Derivation Rules. Table 5 specifies derivation rules that, given ABox setHT-clauses C, derive one ABoxes A1 , . . . , . Hyp-rule, mapping11. show Lemma 4, derivation rules calculus ensure 7 functional acyclicrelation, individual b satisfying definition always exists. second part definitionkakA thus technical aid necessary make definition complete.194fiHypertableau Reasoning Description Logicsset variables NV individuals occurring assertions A, (U )result replacing variable x atom U (x).Rule Precedence. -rule applied (possibly annotated) equalityABox contain equality @un R.B NI-ruleapplicable (with t).Clash. ABox contains clash iff A; otherwise, clash-free.Derivation. set HT-clauses C input ABox A, derivation pair(T, ) finitely branching tree function labels nodesABoxes following properties hold node :(t) = root ;leaf (t) derivation rule applicable (t) C;children t1 , . . . , tn (t1 ), . . . , (tn ) exactly results applyingone (arbitrarily chosen, respecting rule precedence) applicable rule (t)C cases.stress several important aspects Definition 7. preconditions NI -rulesatisfied annotated equality @un R.B , rule must applied even= t; hence, equality plays role derivation even though logicaltautology. Furthermore, even though NI -rule applied @un R.B ublockable individual, equality cannot eagerly simplified usubsequently merged root individual annotation might become important.Finally, C obtained normalization DL knowledge baseuse nominals, inverse roles, number restrictions, precondition NI -rulenever satisfied, need keep track annotations all.Renamings used keep track root individuals merged rootindividuals, necessary make NI -rule sound. example, root individuala.hR, B, 2i merged named individual b, NI -rule must use b insteada.hR, B, 2i future inferences.proof Lemma 6 shows assertions containing least one indirectly blockedindividual used construct model ABox labeling leaf derivation.derivation rules therefore applicable individuals either directly blockedblocked, sufficient completeness. Since rules sound, however, onemay choose disregard restriction makes implementation easier.next introduce notion HT-ABoxes, formalizes idea forest-shapedABoxes introduced Section 3.1.2.Definition 8 (HT-ABoxes). ABox HT-ABox satisfies following conditions, R atomic role, role, B literal concept containing nominal guardconcept, Oa nominal guard concept, s, t, u NA , , b NI , i, j integers.1. role assertion form R(a, s), R(s, a), R(s, s.i), R(s.i, s), R(s, s).195fiMotik, Shearer, & HorrocksTable 5: Derivation Rules Hypertableau CalculusHyp-rule-rule-rule-ruleNI -rule1. r C, r = U1 . . . Um V1 . . . Vn ,2. mapping variables r individuals exists2.1 x NV (x) indirectly blocked,2.2 (Ui ) 1 m,2.3 (Vj ) 6 1 j n,A1 := {} n = 0;Aj := {(Vj )} 1 j n otherwise.1. n R.B(s) A,2. blocked A,3. contain individuals u1 , . . . , un3.1 {ar(R, s, ui ), B(ui ) | 1 n} {ui 6 uj | 1 < j n} A,3.2 1 n, either ui successor ui blocked A,A1 := {ar(R, s, ti ), B(ti ) | 1 n} {ti 6 tj | 1 < j n}t1 , . . . , tn fresh distinct successors s.1. (the equality possibly annotated),2. 6= t,3. neither indirectly blockedA1 := mergeA (s t) named individual, root individualnamed individual, descendant t;A1 := mergeA (t s) otherwise.6 {A(s), A(s)} indirectly blockedA1 := {}.1. @un R.B (the symmetry applies usual),2. u root individual,3. blockable individual successor u,4. blockable individual,5. neither indirectly blockedAi := mergeA (s ku.hR, B, iikA ) 1 n.2. equality either form @an R.B blockable individualsuccessor blockable individual, possibly annotatedequality form s.i s.j, s.i s, s.i.j s, s, a. (The symmetryapplies cases usual.)3. concept assertion form B(s), n S.B(s), Oa (b).4. contains @un R.B , also contains ar(R, u, s) ar(R, u, t).5. contains blockable individual s.i assertion, must containassertion form R(s, s.i) R(s.i, s).6. contains least one assertion.196fiHypertableau Reasoning Description LogicsTable 6: Cases Application Hyp-Rule Role Assertionsar(R, u, s)ar(R, v, a)ar(R, v, a)ar(R, v, a)ar(R, v.n, a)ar(R, u, t)ar(R, v, b)ar(R, v, v.n)ar(R, v, v)ar(R, v.n, v)@uk R.Bb @vk R.Bv.n @vk R.Bv @vk R.Bv @v.nk R.Bar(R, v, v.m)ar(R, v, v.m)ar(R, v.n, v.n.m)ar(R, v, v.n)ar(R, v, v)ar(R, v.n, v)v.m v.n @vk R.Bv.m v @vk R.Bv.n.m v @v.nk R.Bar(R, v, v)ar(R, v.n, v.n)ar(R, v, v)ar(R, v.n, v)v v @vk R.Bv.n v @v.nk R.Bar(R, v.n, v)ar(R, v.n, v)v v @v.nk R.B7. relation 7 acyclic, contains one renaming 7 bindividual a, and, contains 7 b, occur assertion A.Clearly, input ABox HT-ABox. prove that, given HT-ABox,calculus produces HT-ABoxes.Lemma 4 (HT-Preservation). C set HT-clauses HT-ABox, ABoxA0 obtained applying derivation rule C HT-ABox.Proof. Let C, A, A0 stated lemma. analyze derivation ruleTable 5 show A0 satisfies remaining conditions HT-ABoxes.(Hyp-rule) Consider application Hyp-rule HT-clause r type (34)mapping , deriving assertion (V ).Assume V form yi yj @xk R.B , (V ) form @uk R.B .Definition 5, antecedent r contains atoms form ar(R, x, yi ) ar(R, x, yj )so, precondition Hyp-rule, contains assertions ar(R, u, s) ar(R, u, t).u root individual either blockable individual successoru, (V ) clearly satisfies Property (2) HT-ABoxes. Otherwise, since satisfiesProperty (1) HT-ABoxes, possibilities shown Table 6, v blockableindividual, b root individuals. brevity, omit symmetric combinationsroles ar(R, u, s) ar(R, u, t) exchanged. Clearly, (V ) satisfies Property(2) HT-ABoxes. Finally, (V ) obviously satisfies Property (4) HT-ABoxes.Assume V form x zj , (V ) form t. Definition 5,antecedent r contains atom Oa (zj ), either Oa (s) Oa (t) A.Property (3) HT-ABoxes, either named individual, (V ) satisfies Property(2) HT-ABoxes.Assume V form R(x, x). Then, (V ) form R(s, s), satisfiesProperty (1) HT-ABoxes.197fiMotik, Shearer, & HorrocksAssume V form R(x, yi ) R(yi , x), (V ) form R(s, t).Definition 5, antecedent r contains atom form S(x, yi ) S(yi , x),either S(s, t) S(t, s) A; assertions satisfy Property (1) HT-ABoxes,R(s, t) satisfies well.Assume V form R(x, zj ) R(zj , x), (V ) form R(s, t).Definition 5, antecedent r contains atom form Oa (zj ) Oa nominalguard concept, either Oa (s) Oa (t) A; Property (3) HT-ABoxes, eithernamed individual, R(s, t) satisfies Property (1) HT-ABoxes.Assume V form B(x), n S.B(x), B(yi ), (V ) form B(s)n S.B(s). Definition 5, B literal nominal guard concept, (V )satisfies Property (3) HT-ABoxes.(-rule) Consider application -rule assertion n R.B(s). Property(3) HT-ABoxes, B nominal guard concept, assertions B(ti ) introducedrule satisfy Property (3) HT-ABoxes. Furthermore, ti introduced rulefresh blockable successors s, role assertions introduced rule formR(s, ti ) R(ti , s), satisfy Properties (1) (5) HT-ABoxes. inequalitiesintroduced rule trivially satisfy properties HT-ABoxes.(-rule) Consider application -rule possibly annotated equality t,merged (the annotation equality plays role here). conditions7 relation A, ABox contains renaming t, renaming 7renaming A0 , adding renaming introduce cycle7. Merging replaces occurrences A, assertion A0 contains s. Hence,7 relation A0 satisfies Property (7) HT-ABoxes.NI -rule applicable rule precedence, so, preconditionsNI -rule Property (2) HT-ABoxes, form v a, v.i v.j,v.i v, v.i.j v v NA ; denote property (*). Since pruningreplacements applied assertions uniformly, A0 clearly satisfies Property(4) HT-ABoxes. Furthermore, pruning removes successors s, A0 satisfies Property(5) HT-ABoxes. next consider types assertions changemerged t.Consider role assertion R(s, u) changed R(t, u) A0 . eitheru root individual, R(t, u) clearly satisfies Property (1) HT-ABoxes, assumeu blockable individuals. Then, u successor s, since -ruleprunes assertions contain descendant merged individual. then, (*)since R(s, u) satisfies Property (1) HT-ABoxes, possibilities shownTable 7. cases R(u, s) changed R(u, t) A0 merging analogous.consider form equalities derived equalities viamerging. equality u v @sn R.C changed u v @tn R.C , resultingequality always satisfies Property (2) HT-ABoxes. Furthermore, root individual,u @an R.C changed u @an R.C , changed a;however, cases, resulting equality satisfies Property (2) HT-ABoxes.remaining cases, assume possibly annotated equality u changed possiblyannotated equality u. root individual, root individual well (the-rule never merges root individual blockable one), u satisfies Property (2)198fiHypertableau Reasoning Description LogicsTable 7: Cases Application -Rule Role AssertionsR(s, u)R(v.i, v)R(v.i, v)R(t.j.i, t.j)R(v.i, v.i)R(v.i, v.i)R(t.j.i, t.j.i)stv.i v.jv.i vt.j.iv.i v.jv.i vt.j.iR(t, u)R(v.j, v)R(v, v)R(t, t.j)R(v.j, v.j)R(v, v)R(t, t)Table 8: Cases Application -Rule Equalitiessuv.i v.kv.i vu.k.i uv.i v.kv.i vu.k.i ut.j.i t.j.kt.j.i t.jt.j.istv.i v.jv.i v.ju.k.i u.k.jv.i vv.i vu.k.i u.kt.j.it.j.it.j.ituv.j v.kv.j vu.k.j uv v.kvvu.k ut.j.kt.jttHT-ABoxes. Assume blockable individual. Since -rule prunes assertionscontain descendant merged individual, u successor s. (*),Property (2) HT-ABoxes, fact NI -rule applicable A,possibilities shown Table 8. cases, resulting assertion satisfies Property (2)HT-ABoxes. Furthermore, replacing results A0 , A0satisfies Property (6) HT-ABoxes.Consider assertion C(s) changed C(t) A0 . nontrivial caseC nominal guard concept Oa . Property (3) HT-ABoxes, namedindividual. -rule replaces named individuals named individuals,named individual well. Thus, C(t) satisfies Property (3) HT-ABoxes.(NI -rule) Consider application NI -rule equality @un R.B mergesroot individual ku.hR, B, iikA . individual blockable, renaming added7 relation A0 satisfies Property (7) HT-ABoxes. Since replacedroot individual role equality assertions, resulting assertions satisfy Properties(1) (2) HT-ABoxes. Since named individual, assertion involvingnominal guard concept affected merging, A0 satisfies Property (3). Since pruningreplacements applied assertions uniformly, A0 clearly satisfies Property(4) HT-ABoxes. Pruning removes successors s, A0 satisfies Property (5)199fiMotik, Shearer, & HorrocksHT-ABoxes. Finally, A0 clearly empty, satisfies Property (6).next prove soundness completeness calculus. use notionscustomary resolution-based theorem proving: calculus sound derivation rulespreserve satisfiability theory, complete if, whenever calculus terminateswithout detecting contradiction, theory indeed satisfiable.Lemma 5 (Soundness). Let C set HT-clauses input ABox(C, A) satisfiable. Then, derivation C contains branch (t)clash-free node branch.Proof. say model ABox A0 NI-compatible A0 followingconditions satisfied:root individual occurring A0 , concept n R.B, 4IaI ( n R.B)I , haI , RI , B , = (a.hR, B, ii)I1 n.12@un R.B A0 , huI , sI RI , huI , tI RI , sI B , tI B ,uI ( n R.B)I .13prove lemma, first show following property (*): (C, A0 ) satisfiablemodel NI -compatible A0 A1 , . . . , ABoxes obtained applyingderivation rule C A0 , (C, Ai ) satisfiable model NI -compatibleAi . Let model (C, A0 ) NI -compatible A0 , consider possiblederivation rules derive A1 , . . . , A0 C.(Hyp-rule) Consider application Hyp-rule HT-clause r form (34).Since (Ui ) A0 , |= (Ui ) 1 m. then, |= (Vj )1 j n. Since Aj := A0 {(Vj )}, |= (C, Aj ).|= (Vj ) atom Vj form = yk y` @xh R.B , clearly NI compatible Aj . Furthermore, Vj form , clearly h(x)I , (yk )I RI ,h(x)I , (y` )I RI , (yk )I B , (y` )I B . Let (**) denote two properties.Assume NI -compatible Aj 1 j n. (**), 6|= (Vj )Vj form , (x)I 6 ( h R.B)I Vj form . Let: NV 4I variable mapping (x) = (x)I (yk ) = (yk )Ibranch variable yk occurring atom form ; furthermore, setbranch variables y1 , . . . , yh+1 occurring atom form , set (y1 ), . . . , (yh+1 )arbitrarily chosen domain elements verify (x)I 6 ( h R.B)I . Clearly, I, 6|= VjVj occurring subset (35) (36) r; furthermore, definition ,I, 6|= Vj Vj occurring subset (35) (36) r. then,conclude I, 6|= (C, A0 ), contradiction.(-rule) Since n R.B(s) A0 , |= n R.B(s), implies domainelements 1 , . . . , n 4I exist hsI , RI B 1 n, 6= j12. Intuitively, condition ensures root individual a.hR, B, ii interpreted appropriateneighbor aI .13. Intuitively, condition ensures u, s, interpreted accordance annotation.200fiHypertableau Reasoning Description Logics01 < j n. Let 0 interpretation obtained setting tIi = . Clearly,0 |= ar(R, s, ti ), 0 |= B(ti ), 0 |= ti 6 tj 6= j, 0 |= (C, A1 ). individuals tiroot individuals, 0 NI -compatible A1 .(-rule) Assume -rule applied assertion A0 mergedt. Since |= t, sI = tI . Pruning removes assertions, modelpruned ABox monotonicity. Merging simply replaces individual synonym,|= (C, A1 ). Furthermore, Property (7) HT-ABoxes, contain renamingst, kskA1 = t; hence, NI -compatible A1 .(-rule) rule never applicable (C, A0 ) satisfiable.(NI -rule) Assume NI -rule applied @un R.B A0merged root individual. Since NI -compatible A0 , uI ( n R.B)I ,huI , sI RI , sI B , sI = (u.hR, B, ii)I 1 n. Let vi = ku.hR, B, iikA0 ;since NI -compatible, (u.hR, B, ii)I = viI . Thus, NI -rule replacessynonym vi , |= (C, Ai ) like case -rule. vi occurA0 , interpretation may NI -compatible Ai interpretvi .hS, C, `i correctly. extend 0 follows. m, S, CviI ( S.C)I , let 1 , . . . , k elements 4I hviI , j j C ;0clearly, k m. set (vi .hS, C, `i)I = ` 1 ` k. Since none vi .hS, C, `ioccurs Ai , 0 |= (C, Ai ), 0 NI -compatible Aj .completes proof (*). prove main claim lemma, letinput ABox. Similarly NI -rule proof (*), extend model 0(C, A). Since contain annotated equalities, 0 NI -compatible A.claim lemma follows straightforward inductive application (*).Lemma 6 (Completeness). derivation set HT-clauses C input ABoxexists leaf node labeled clash-free ABox A0 , (C, A) satisfiable.Proof. prove lemma constructing A0 model (C, A). Since logicfinite model property, obtain model unraveling A0 intuitivelyexplained Section 3.1.2. usual, elements unraveled model paths (Horrocks& Sattler, 2001, 2007), defined next.Given individual directly blocked A0 , let blocker arbitrarilychosen fixed individual directly blocked t.path finite sequence pairs individuals p = [ ss00 , . . . , ssn0 ]. Let tail(p) = sntail0 (p) = s0n . Furthermore, let q = [p |sn+1]s0n+1n0path [ ss00 , . . . , ssn0 , ssn+1]; say q00nn+1successor p, p predecessor q. set paths P(A0 ) defined inductivelyfollows:[ aa ] P(A0 ) root individual occurring A0 ;0[p | ss0 ] P(A0 ) p P(A0 ), s0 successor tail(p), s0 occurs A0 , s0blocked A0 ;[p | ss0 ] P(A0 ) p P(A0 ), s0 successor tail(p), s0 occurs A0 , s0 directlyblocked A0 , blocker s0 A0 .201fiMotik, Shearer, & HorrocksTable 9: Construction Interpretation A04IaIaIAIRI=====P(A0 )[ aa ] root individual occurs assertion A0bI 6= b kakA0 = b{p 4I | A(tail(p)) A0 }{h[ aa ], pi 4I 4I| root individual R(a, tail(p)) A0 }{hp, [ ]i 4 4| root individual R(tail(p), a) A0 }{hp, [p | s0 ]i 4I 4I | R(tail(p), s0 ) A0 }{h[p | ss0 ], pi 4I 4I | R(s0 , tail(p)) A0 }{hp, pi 4I 4I| R(tail(p), tail(p)) A0 }Let interpretation constructed A0 shown Table 9. A0 HT-ABox,4I empty. show that, ps form [ ss0 ] [qs | ss0 ]individual w, following claims hold (*):R(s, s) A0 (resp. A(s) A0 ) iff hps , ps RI (resp. ps AI ): Immediatedefinition I.B(w) A0 LA0 (w) = LA0 (s0 ) B literal concept, ps B : proofimmediate B atomic. B = A, since -rule applicable A0 ,A(w) 6 A0 ; then, A(s0 ) 6 A0 A(s) 6 A0 , caseatomic concepts implies ps 6 AI .n R.B(s) A0 , ps ( n R.B)I : definition paths, blocked;since -rule applicable n R.B(s), individuals u1 , . . . , un existar(R, s, ui ) A0 B(ui ) A0 1 n, ui 6 uj A0 1 < j n.assertion ar(R, s, ui ) satisfies Property (1) HT-ABoxes, uione following forms.ui = s. Let pui = ps . then, previous two cases concludear(R, s, ui ) A0 B(ui ) A0 imply hps , pui RI pui B .ui successor s. ui directly blocked blocker vi , let pui = [ps | uvii ];otherwise, ui blocked blocked, let pui = [ps | uuii ].Either way, ar(R, tail(ps ), ui ) A0 , which, definition I, implies hps , pui RI . Furthermore, B(ui ) A0 LA0 (ui ) = LA0 (tail(pui )) implypui B .ui blockable predecessor s. Since blockable, ps = [qs | ss0 ];hence, let pui = qs . s0 blocked, = s0 tail(pui ) = ui ,ar(R, s0 , tail(pui )) A0 . s0 blocked blocker s, definitionpairwise blocking LA0 (tail(pui ), s0 ) = LA0 (ui , s) LA0 (s0 , tail(pui )) = LA0 (s, ui ),ar(R, s0 , tail(pui )) A0 . Either way, hps , pui RI202fiHypertableau Reasoning Description Logicsdefinition I. Furthermore, B(ui ) A0 LA0 (ui ) = LA0 (tail(pui )) implypui B .ui satisfy previous three conditions. blockableindividual, ui root individual, let pui = [ uuii ]. root individual,ui blocked A0 Condition 3.2 -rule, pui 4Iexists form pui = [p | uuii ]. Either way, ar(R, s, ui ) A0B(ui ) A0 , imply hps , pui RI pui B .Consider 1 < j n. tail0 (pui ) 6 tail0 (puj ) A0 , since 6 A0-rule applicable, tail0 (pui ) 6= tail0 (puj ), pui 6= puj . Furthermore,tail0 (pui ) 6 tail0 (puj )/ A0 , tail0 (pui ) 6= ui , possible s0directly blocked blocker ui = ui blockable predecessor s. Note,however, one blockable predecessor,one ui ui = s. Therefore, ui 6= uj , implies pui 6= puj ,conclude ps ( n R.B)I .assertion 0 A0 form b 6 b b named individuals,straightforward see |= 0 . Furthermore, 0 form R(a, b) B(a),n R.B(a) named individual, (*) implies |= 0 . Consider A.induction application derivation rules, straightforward show that,6 A0 , A0 contains renamings that, applied , produce assertion 0 A0 .then, since |= 0 , |= definition I.remains shown |= C. Consider HT-clause r C containing atomsform Ai (x), Uk (x, x), ar(Ri , x, yi ), Bi (yi ), Cj (zj ) antecedent. Furthermore,consider variable mapping antecedent r true is,px AIi , hpx , px UkI , hpx , pyi RiI , pyi BiI , pzj CjI px = (x), pyi = (yi ),pzj = (zj ). Let = tail(px ), s0 = tail0 (px ), t0i = tail0 (pyi ). definitionfact LA0 (s0i ) = LA0 (si ), Ai (s) A0 , Uk (s, s) A0 , Bi (t0i ) A0 .Depending relationship px pyi , define ti follows.pyi successor px pyi = px . Let ti = t0i . Clearly, Bi (ti ) A0 ; furthermore,definition hpx , pyi RiI imply ar(Ri , s, t0i ) A0 , ar(Ri , s, ti ) A0 .pyi predecessor px . following cases.directly blocks s0 . Let ti predecessor s; ti exists sinceblockable. definition hpx , pyi RiI imply ar(Ri , s0 , tail(pyi )) A0B(tail(pyi )) A0 , definition pairwise blocking concludear(Ri , s, ti ) A0 Bi (ti ) A0 .s0 blocked. Let ti = t0i . definition I, Bi (ti ) A0ar(Ri , s, ti ) A0 .pyi px match conditions mentioned thus far. definitionI, either px pyi form [ aa ]. Let ti = tail(pyi ). hpx , pyi RiIdefinition I, conclude Bi (ti ) A0 ar(Ri , s, ti ) A0 .203fiMotik, Shearer, & HorrocksDefinition 5, antecedent r contains atom form Oa (zj ) nominalvariable zj . Thus, definition Property (3) HT-ABoxes, pzjuform [ ujj ] uj named individual; furthermore, Cj (uj ) A0 .Let mapping (x) = s, (yi ) = ti , (zj ) = uj . Clearly, neitherti indirectly blocked, (Uj ) A0 atom Uj antecedent r.Hyp-rule applicable r, A0 , , r contains atom Vi consequent(Vi ) A0 . Depending type Vi , following possibilities.Assume Vi form yi yj @xk S.B ; thus, ti tj @sk S.B A0 . Since-rule applicable A0 , ti = tj . Definition 5, r contains subclauseform (35) (36), antecedent r contains atoms ar(S, x, yi ) ar(S, x, yj );therefore, hpx , pyi hpx , pyj . NI -rule applicable ti tj @sk S.Bso, preconditions NI -rule, root individual, ti (tj ) either rootindividual successor s. rules possibility px form [ aa ]pyi (pyj ) neither successor px form [ bb ]. Hence, construction I,pyi (pyj ) either successor px , equal px , predecessor px ,form [ aa ]. consider following cases (w.l.o.g. omit symmetric casesobtained swapping pyi pyj ):pyi form [ aa ]. Then, ti = tj implies pyi = pyj definition paths.pyi successor px . Then, pyi = [px | utii ] ui = ti ti blocked uiblocker ti . Either way, ti different predecessor (if latterexists). following possibilities pyj :upyj successor px . Then, pyj = [px | tjj ], ti = tj clearly implies pyi = pyj .pyj = px pyj predecessor px . tj = tj predecessors, contradicts fact ti 6= tj .pyi = px . ti = s. nontrivial case pyj predecessor px ;then, tj 6= s, contradicts fact ti 6= tj .pyi predecessor px . remaining possibility pyj predecessor px . Since px one predecessor, pyi = pyj .Thus, conclude I, |= r.Assume Vi form x zj ; thus, uj A. Since -ruleapplicable A0 , = uj . Since uj named individual, cannot blockindividuals, s0 = s, implies px = pzj . Thus, I, |= r.Assume Vi form Ti (x, x); thus, Ti (s, s) A0 . (*),hpx , px RiI . Thus, I, |= r.Assume Vi form Di (x) Di literal concept form n T.B;thus, Di (s) A0 . (*), px DiI . Thus, I, |= r.Assume Vi form Ei (yi ) Ei literal concept; thus, Ei (ti ) A0 .already established LA0 (ti ) = LA0 (t0i ); (*), pyi EiI . Thus,I, |= r.204fiHypertableau Reasoning Description LogicsAssume Vi form ar(Si , x, yi ), ar(Si , s, ti ) A0 . definitionblocking, ar(Si , s0 , t0i ) A0 . Finally, definition I, hpx , pyi SiI .Thus, I, |= r.Assume Vi form ar(Sj , x, zj ), ar(Sj , s, uj ) A0 . Since uj namedindividual, definition hpx , pzj SjI . Thus, I, |= r.next prove termination hypertableau calculus.Lemma 7 (Termination). set HT-clauses C input ABox A, let |C, A|sum size A, number concepts roles C, dlog neinteger n occurring C atom form n R.B yi yj @xn R.B . totalnumber individuals introduced path derivation Cdoubly exponential |C, A|, derivation C finite.Proof. prove claim showing (i) derivation rule appliedfixed set individuals derivation path, (ii) number new individualsintroduced derivation path doubly exponential |C, A|. supplyblockable individuals infinite, assume blockable individual introducedtwice derivation path. Furthermore, root individual removed ABoxA0 due merging, renaming added A0 ensures kskA0 6= s. renamingadded A0 , ABoxes occurring A0 derivation contain renamingwell, subsequent application NI -rule reintroduce s.Next, prove (i) considering derivation rule.application Hyp-rule HT-clause r form (34) mappingintroduces assertion (Vi ), prevents subsequent reapplicationHyp-rule r . Merging pruning remove (Vi ) subsequentderivation steps, also removes least one individual occurringset potential premises Hyp-rule, thus preventing reusefuture application Hyp-rule r.application -rule assertion n R.B(s) introduces t1 , . . . , tn freshsuccessors assertions B(ti ), ar(R, s, ti ), ti 6 tj 1 < j n.Thus, individuals u1 , . . . , un Condition 3 -rule matchedt1 , . . . , tn . Furthermore, root individual, none ti become blockedCondition 3.2 always satisfied ti ; moreover, blockable, Condition 3.2trivially satisfied ti . ti merged another individual v, B(v),ar(R, s, v), v 6 tj added ABox, ABox still contains individualsmatched Condition 3 -rule. Finally, ti becomes indirectlyblocked, blocked -rule applicable s.application -rule removes either t, rule cannotreappliedapplication -rule produces ABox labels derivation leaf.application NI -rule equality @un R.B removes s, rulecannot reapplied n R.B, u.205fiMotik, Shearer, & HorrocksNext, prove (ii)that is, total number individuals introduced derivation path doubly exponential |C, A|. path length n individualsABox A0 sequence individuals u0 , u1 , . . . , un u0 = s, un = t,and, 0 n 1, either R(ui , ui+1 ) A0 R(ui+1 , ui ) A0 R atomic role.root path root individual ABox A0 path namedindividual intermediate individuals ui , 1 n 1, root individuals.level lev(t) length shortest root path t. Thus, lev(t) = 0named individual.depth dep(t) individual number ancestors t. Thus, dep(t) = 0root individual. Due Property (5) HT-ABoxes, individual occursABox A0 , A0 contains path length dep(t) root individualindividuals ui , 0 n 1, ancestors t; since individualone predecessor, ui also ancestors t.show maximum level root individual maximum depthevery individual exponential size C A.application derivation rule never increases level individual.named individual never pruned merged another namedindividual,14 root individual merged another root individual.rule applications make root path shorter, longer.Let number atomic concepts n number atomic roles occurC, let = 22m+2n + 1, let A0 ABox labeling node derivationC. next show (1) dep(t) individual occurring A0 , (2)root individual, lev(t) .(Claim 1) pair individuals occurring A0 , 2m differentpossible labels LA0 (s) 2n different possible labels LA0 (s, t). Thus, A0 contains least= 2m 2m 2n 2n + 1 predecessor-successor pairs blockable individuals, A0 mustcontain two pairs hs, s.ii ht, t.ji following conditions satisfied:LA0 (s.i) = LA0 (t.j)LA0 (s, s.i) = LA0 (t, t.j)LA0 (s) = LA0 (t)LA0 (s.i, s) = LA0 (t.j, t)Since contains ancestor relation, path A0 containing blockable individualsmust include least one blocked individual, blockable individual depth mustblocked. -rule applied individuals blocked, rule cannotintroduce individual u dep(u) > .(Claim 2) show following stronger claim (*) holds root individualoccurring assertion A0 (the symmetry applies usual):1. lev(s) ;2. R(s, t) A0 R(t, s) A0 u @sn R.B A0 blockable nonsuccessors, lev(s) + dep(t) ;3. A0 blockable nonsuccessor (where equality annotated), lev(s) + dep(t) + 1.14. derivation rule replaced named individual individual named, levelsroot individuals could increase.206fiHypertableau Reasoning Description Logicsclaim clearly true input ABox labeling root derivation,contains named individuals. assume (*) holds ABox A0consider possible derivation rules applied A0 .Assume Hyp-rule derives assertion R(s, t) R(t, s), rootindividual blockable nonsuccessor s. Let R(x, y) R(y, x) atomconsequent HT-clause r instantiated derivation rule.following two possibilities antecedent r.antecedent r contains atom form S(x, y) S(y, x)matched assertion form S(s, t) S(t, s) A0 . Since A0 satisfies (*),resulting ABox satisfies (*) well.antecedent r contains atom form Oa (x) Oa (y) matchedassertion form Oa (s) A0 (since blockable, A0 cannot containOa (t) Property 3 HT-ABoxes). dep(t) lev(s) = 0,resulting ABox satisfies (*) well.Assume Hyp-rule derives assertion u @sn R.B , root individual blockable nonsuccessor s. Definition 5, antecedentHT-clause contains atoms form ar(R, x, yi ) ar(R, x, yj )matched assertions ar(R, s, t) ar(R, s, u) A0 . Since A0 satisfies (*),lev(s) + dep(t) , resulting equality satisfies Item 2 (*). showu @sn R.B satisfies Item 3 (*), assume u root individualnonsuccessor u. Since A0 contains ar(R, s, u), lev(u) lev(s) + 1;then, lev(u) + dep(t) + 1, required.Hyp-rule derives assertion t, root individualblockable nonsuccessor s, remaining possibility consequentHT-clause contains equality x zj . Definition 5, antecedentcontains Oa (zj ) matched assertion Oa (s) A0 , namedindividual. dep(t) lev(s) = 0, resulting ABox satisfies (*).Assume -rule introduces assertion form R(s, t) R(t, s)fresh individual. Individual always successor s, resulting ABoxtrivially satisfies (*).Assume -rule applied assertion form u umerged s. definition merging, dep(u) dep(s) upruned. blockable individual, u blockable well, resultingABox satisfies (*) u replaced individual equal smaller depth.Therefore, assume root individual consider types assertionsadded A0 result merging.R(u, u) changed R(s, s), resulting ABox clearly satisfies (*).Assume R(u, t) root individual changed R(s, t).inference make root paths shorter longer,levels decrease rather increase. Thus, resultingABox satisfies Item 1 (*).207fiMotik, Shearer, & HorrocksAssume R(u, t), predecessor u, changed R(s, t);nontrivial case blockable nonsuccessor s. Sincepredecessor u, dep(t) + 1 = dep(u); since A0 satisfies (*),lev(s) + dep(u) + 1; then, lev(s) + dep(t) required.cases R(t, u) changed R(t, s) analogous.Assume possibly annotated equality v u changed v s.nontrivial case v blockable nonsuccessor s. u rootindividual, level merging bounded min(lev(s), lev(u))merging, (*) preserved. u v blockable individuals,Property (2) HT-ABoxes, either u ancestor v, u vsiblings, v ancestor u. u ancestor v, pruning uremoves v u A0 . v sibling ancestor u, u mustnonsuccessor s, lev(s) + dep(u) + 1; then, dep(v) dep(u),lev(s) + dep(v) + 1 (*) preserved.Assume v v 0 @un R.B changed v v 0 @sn R.B v @sn R.B .nontrivial case v blockable nonsuccessor s. Since u prunedmerging, Properties (2) (4) HT-ABoxes v must predecessor u, dep(v) + 1 = dep(u). Furthermore, properties umust blockable nonsuccessor s, lev(s) + dep(u) + 1. then,lev(s) + dep(v) , required.application -rule trivially preserves (*).Assume NI -rule applied assertion @un R.B replacingroot individual v = ku.hR, B, iikA0 . v already occurs assertion A0 , vsatisfies Item 1 (*). If, however, v fresh, Property (4) HT-ABoxes vconnected u role assertion, lev(v) lev(u) + 1. Furthermore, sinceblockable nonsuccessor u, lev(u) + dep(s) . Finally, since blockable,dep(s) 1, lev(u) 1. consequence, conclude lev(v) ,proves Item 1 (*). proof assertions introduced merging satisfy(*) analogous case -rule.complete proof claim (ii)that is, total number individualsintroduced derivation rules doubly exponential |C, A|.named individuals level 0 never introduced derivation rules.application NI -rule root individual u level ` introduce n rootindividuals level ` + 1 concept n R.B occurs C. Thus, namedindividual, derivation rules create tree root individuals. maximum depthtree , exponential |C, A|. Furthermore, maximum branching factorb equal sum numbers occurring C atoms form yi yj @xn R.B .Clearly, b exponential |C, A|, tree doubly exponential |C, A|.15Similarly, root individual become root tree blockable individualsdepth . blockable individual introduced applying -rule predecessor.15. numbers coded unary, branching factor would polynomial, treewould still doubly exponential |C, A|.208fiHypertableau Reasoning Description LogicsFurthermore, -rule applied individual conceptform n R.B. Thus, branching factor exponential assuming binary codingnumbers, tree doubly exponential |C, A|.Thus, total number individuals appearing derivation doubly exponential |C, A|. Since branching factor derivation exponentially bounded|C, A|, derivation finite.state main theorem section.Theorem 1. satisfiability SHOIQ+ knowledge base K decided computingK0 = ((K)) checking whether derivation (K0 ) contains leaf nodelabeled clash-free ABox. algorithm implemented runs2NExpTime |K|.Proof. first part theorem follows immediately Lemmas 1, 2, 5, 6.Lemma 7, total number individuals doubly exponential |A (K0 ), R (K0 )|.Since structural transformation polynomial, total number individuals doublyexponential |K|. Thus, existence leaf derivation node labeled clash-freeABox checked nondeterministically applying hypertableau derivation rulesconstruct ABox doubly exponential |K|.5. Discussionsection discuss possibilities optimizing blocking condition singlesubset blocking; furthermore, argue modifying algorithm make optimalw.r.t. worst-case complexity might difficult.5.1 Single BlockingDLs SHOQ+ provide inverse roles, pairwise blockingweakened atomic single blocking, defined follows.Definition 9 (Atomic Single Blocking). Atomic single blocking obtained pairwiseblocking (see Definition 7) changing notion direct blocking: blockable individualdirectly blocked blockable individual blocked, s,LA (s) = LA (t) LA (s) Definition 7.16cases, simpler blocking condition make hypertableau algorithmconstruct smaller ABoxes, lead increased efficiency. next formalizenotion HT-clauses atomic single blocking applicable.Definition 10 (Simple HT-Clause). HT-clause r simple satisfies followingrestrictions, x center variable, yi branch variable, zj nominal variable, B literalconcept, R atomic role:atom antecedent r form A(x), R(x, x), R(x, yi ), A(yi ),A(zj ).16. name atomic reflects fact LA (s) contains atomic concepts.209fiMotik, Shearer, & Horrocksatom consequent r form B(x), h R.B(x), B(yi ), R(x, x),R(x, yi ), R(x, zj ), x zj , yi yj .straightforward see that, K SHOQ+ knowledge base, R (K) containssimple HT-clauses. completeness hypertableau algorithm atomic singleblocking simple HT-clauses straightforward show.Lemma 8. Let C set simple HT-clauses, input ABox. derivationatomic single blocking C exists leaf node labeled clash-freeABox A0 , (C, A) satisfiable.Proof. slightly modifying proof Lemma 4, possible show followingproperty (*): atom A0 involving atomic role form R(s, a), R(s, s),R(s, s.i), named individual individual.Let model constructed way Lemma 6, using singleblocking. Due (*), whenever hp1 , p2 RI , p2 either form [ aa ] namedindividual, successor p1 , p2 = p1 . proof model (C, A)straightforward consequence following observations proof Lemma 6:proof n R.B(s) A0 implies ps ( n R.B)I , individual ui neverblockable predecessor s. Thus, labels LA0 (s, ui ), LA0 (ui , s), LA0 (ui )never relevant.proof |= C, possible pyi predecessor px . Thus, labelsLA0 (s0 , tail(pyi )), LA0 (tail(pyi ), s0 ), LA0 (tail(pyi )) never relevant.proof model (A, C) thus requires LA0 (s) = LA0 (t) holdblocked blocker t; hence, model (A, C) even atomic single blockingused.following variant single blocking also applied DLs inverse rolesnumber restrictions, SHOI.Definition 11 (Full Single Blocking). Full single blocking obtained atomic singleblocking (see Definition 9) changing definition LA (s) follows:LA (s) = { C | C(s) C form 1 R.Batomic B literal concept }directly block atomic single blocking, suffices occuratomic concepts A. Intuitively, model constructionLemma 6 copies nonatomic concepts s; hence, assertions form C(s)C atomic relevant. contrast, full single blocking, mustoccur exactly concepts (apart negated atomic concepts). Intuitively,given clash-free ABox A0 derivation rule applicable, model (A, C)constructed A0 replacing t; result model, two individualsmust occur exactly concepts.210fiHypertableau Reasoning Description LogicsT.CbCR.DRc.CCR.DFigure 11: Problems Single BlockingFull single blocking must applied care hypertableau setting. Considerfollowing knowledge base K9 , consisting ABox A9 set HT-clauses C9 .(37)A9 = { T.C(a) }C9 = {C(x) R.D(x), D(x) .C(x), R(x, y1 ) S(x, y2 ) }K9 , hypertableau algorithm full single blocking produces ABox shownFigure 11. individual blocked b, algorithm terminates; expansionR.D(d), however, would reveal K9 unsatisfiable. problem arisesHT-clause R(x, y1 ) S(x, y2 ) contains two role atoms, allows HT-clauseexamine successor predecessor x. Full single blocking, however,ensure predecessors successors x fully built. correctproblem requiring normalized GCIs contain one R.C concept.example, replace HT-clause R(x, y1 ) Q(x) Q(x) S(x, y2 ) ,first HT-clause would additionally derive Q(b), would blocked b.apply full single blocking DL SHOI provided HT-clause contains one role atom antecedent. always ensure suitablyrenaming complex concepts atomic ones.Lemma 9. Let ABox C set HT-clauses that, r C, ( i) rcontains atoms form R(x, x), ( ii) antecedent r contains one roleatom, ( iii) at-least restriction concepts form 1 S.B role Bliteral concept. derivation full single blocking C exists leafnode labeled clash-free ABox A0 , (C, A) satisfiable.Proof. Let A00 obtained A0 removing assertion containing indirectlyblocked individual. Since derivation rule applicable indirectly blocked individuals,derivation rule applicable A00 C. individual occurring A00 , let[s]A00 = blocked A00 , let [s]A00 = s0 blocked A00 blocker s0 .Note following useful property (*): A(s) A00 , A(s) 6 A00 since -ruleapplicable A00 ; then, A([s]A00 ) 6 A00 well.construct interpretation A00 follows.4IsIAIRI===={s | occurs A00 blocked A00 }[s]A00 individual occurring A00{[s]A00 | A(s) A00 }{h[s]A00 , [t]A00 | R(s, t) A00 }211fiMotik, Shearer, & Horrocksstraightforward see |= A00 . Consider HT-clause r C containsantecedent one atom form R(x, y), well atoms form Ai (x), Bi (y),Ci (zi ). Let mapping variables r individuals A00|= (Ui ) atom Ui antecedent r. definition I, individualsexist R(s, t) A00 , (x) = [s]A00 , (y) = [t]A00 . definitionfull single blocking, Ai (s) A00 Bi (t) A00 well. Furthermore, sincezi occurs nominal guard concept, (zi ) named individual. Let 00 (x) = s, 0 (y) = t, 0 (zi ) = (zi ). Since Hyp-rule applicable C A000 , 0 (Vj ) A00 atom Vj consequent r. Considerpossible forms Vj have.Vj = S(x, y), |= S((x), (y)) definition I. case Vj = S(y, x)analogous.Vj = A(x) atomic concept, A([s]A00 ) A00 definition fullsingle blocking; then, |= A((x)) definition I. case Vj = A(y)analogous.Vj = A(x), A([s]A00 ) 6 A00 (*); then, definition|= A((x)). case Vj = A(y) analogous.Vj = D(x) = 1 R.B, D([s]A00 ) A00 definition full singleblocking. Since -rule applicable [s]A00 , individual existsar(R, s, t) A00 B atomic, B(t) A00 , B = A, A(t) 6 A00 .definition full single blocking, B atomic, B([t]A00 ) A00 ,B = A, A([t]A00 ) 6 A00 . definition I, h[s]A00 , [t]A00 RI ,[t]A00 B ; therefore, |= D((x)). case Vj = D(y) analogous.Vj = x zi , 0 (x) 0 (zi ) A00 ; since -rule applicable A00 ,0 (x) = 0 (zi ). then, since named individuals cannot block individuals,(x) = 0 (x); hence, |= (x) (zi ).Thus, cases |= (Vj ). case r contain role atom R(x, y)antecedent analogous, |= (A, C).5.2 Subset Blockingtableau algorithms DLs without inverse roles, full single blocking conditionDefinition 11 weakened full subset blocking (Baader et al., 1996).Definition 12 (Full Subset Blocking). Full subset blocking obtained full singleblocking (see Definition 11) changing notion direct blocking: blockable individualdirectly blocked individual blocked, s, LA (s) LA (t).Full subset blocking problematic hypertableau setting. Consider knowledgebase consists ABox A10 TBox corresponding HT-clauses C10 .(38)A10 ={ T.C(a) }C(x) R.C(x),C(x) S.D(x),C10 =S(x, y) D(y) E(x), R(x, y) E(y)212fiHypertableau Reasoning Description LogicsT.CCR.CS.DEbRcCR.CS.DFigure 12: Problems Full Subset BlockingK10 , algorithm produce ABox shown Figure 12, blockedb. If, however, expand S.D(d) S(d, e) D(e), derive E(d); togetherR(b, d) HT-clause R(x, y) E(y) , get contradiction.problem arises because, hypertableau setting, syntactic distinction atomic inverse roles lost: atom R (x, y) transformed (by functionar) semantically equivalent atom R(y, x). HT-clause S(x, y) D(y) E(x)seen including implicit inverse role, examines successor xantecedent order derive new information x consequent, thus mimickingbehavior tableau algorithms semantically equivalent GCI v .E.semantically equivalent inverse-free GCI S.D v E would, hypertableaualgorithm, transformed exactly HT-clause. tableau setting, however, GCI would treated differently: would result v-rule deriving (E S.D)(s) individuals s. similar effect could achieved hypertableau setting translating S.D v E two HT-clauses: > E(x) Q(x)Q(x) S(x, y) D(y) . introduces nondeterminism, solves problemfull subset blocking deriving either E(c) Q(c), first leads immediatecontradiction, second delays blocking.general, easy see full subset blocking could used hypertableausetting modifying preprocessing phase ensure HT-clauses includeimplicit inverses. clear, however, would useful: would result(possibly) smaller ABoxes, cost (possibly) larger derivation trees.5.3 Number Blockable IndividualsBuchheit et al. (1993) presented tableau algorithm DL ALCN R which, dueanywhere blocking, runs NExpTime instead 2NExpTime, Donini et al. (1998)presented similar result basic DL ALC. interesting compare algorithmssee whether anywhere blocking improve worst-case complexityalgorithm K SHIQ+ knowledge base. case, HT-clause (K)contains nominal guard concept, prevents derivation assertions satisfyingpreconditions NI -rule; hence, new root individuals introduced derivation,eliminates significant source complexity.following example shows that, unfortunately, anywhere blocking improve worst-case complexity; fact, identify tension and- or213fiMotik, Shearer, & Horrocksbranching. example, use well-known encoding binary numbers concepts B0 , B1 , . . . , Bk1 : assign individual ABox binary number`A (s) = bk1 . . . b1 b0 bi = 1 Bi (s) A. Using k concepts,thus encode 2k different binary numbers. Furthermore, atomic role R, usingwell-known R-successor counting formula (Tobies, 2000), ensure that, wheneverindividual R-successor A, `A (t) = (`A (s) + 1) mod 2k ; omitformula sake brevity. Let K11 following knowledge base. sakebrevity, omit HT-clauses corresponding axioms K11 .(39)C(a)(40)C v L.C u R.C(41)(The R-successor formula B0 , . . . , Bk1 )(42)(The L-successor formula B0 , . . . , Bk1 )(43)B0 u . . . u Bk1 v(44)L.A u R.A vFigure 13 schematically presents derivation K11 doubly exponentialnumber blockable individuals introduced.17 simplicity presentation, usesingle anywhere blocking. Due (39)(42), algorithm create individuals a.1, a.2,a.1.1, a.1.2, a.1.1.1, a.1.1.2, on, s.1 L-successor s, s.2kkR-successor s. creating individuals form a.12 1 .1 a.12 1 .2k12 1 string 2k 1 ones, individual x.1 blocks x.2 (c.f. Figure 13a). then, duekkk(43), a.12 1 .1 a.12 1 .2 become instances A. (44), a.12 1 made instancekkwell, block sibling a.12 2 .2 more; hence, a.12 2 .2expanded exponential depth (c.f. Figure 13b). repeating process, algorithmkkderives a.12 2 instance A, block sibling a.12 3 .2 (c.f.Figure 13d). Eventually, algorithm constructs binary tree exponential depth, thuscreating doubly-exponential number blockable nodes total (c.f. Figure 13d).Buchheit et al. Donini et al. obtained nondeterministic exponential behaviorapplying u-, t-, -, v-rules exhaustively applying -rule.strategy ensures label individual fully constructed introducingsuccessor s, prevents individuals indirectly blocked. K11 ,means GCI (44) applied individual introducing successors.Thus, existentials expanded, assertion (L.A R.A A)(s)introduced one disjunct chosen nondeterministically. choices (L.A)(s)(R.A)(s) lead clash, algorithm eventually derives A(s), expandsexistentials introduces s.1 s.2. Thus, generating exponentialmodels, algorithm incurs massive amount nondeterminism.Nondeterministic exponential behavior guaranteed hypertableau algorithmnondeterministically fixing label individual applying -rule it.technique similar one used Tobies (2001) order obtain PSpace17. Initially, suggested informally algorithm run NExpTime SHIQ (Motik, Shearer,& Horrocks, 2007). example shows, case.214fiHypertableau Reasoning Description Logicsa.2a.1a.2|2 k3|a.1xxx.1x.2x.1(a) exponential path constructedblockable individual blockingsibling. individual containslabel.(b) Adding label x.1 unblocksx.2.a.1x.2a.2xx.1x.2(c) Adding label x.2 makes x.2blocked, forces additionlabel x. unblocks siblingx another subtree created.(d) Derivation terminates exponential numberunblocked individuals, doubly-exponentialnumber indirectly blocked individuals.Figure 13: Creation Exponentially Deep Binary Tree Blockable Individuals215fiMotik, Shearer, & Horrocksdecision procedure concept satisfiability DL inverse roles without GCIs.performance results Section 7, however, seem suggest mightbeneficial practice. Still, might worth exploring whether nondeterministically addingconcepts labels individuals used optimization would detect earlyblocks thus prevent construction large models.5.4 Number Root IndividualsSHOIQ NExpTime-complete (Tobies, 2000), straightforward extendresult SHOIQ+ . Thus, one might wonder whether complexity result Theorem 1sharpened obtain worst-case optimal decision procedure. This, unfortunately,case: present example algorithm generates doubly-exponentialnumber root individuals. construct K12 extending K11 (axioms (39)(44))following two axioms:(45)B0 u . . . u Bk1 v {b}(46)v 2 L .> u 2 R .>shown Section 5.3, axioms K11 cause algorithm constructbinary tree blockable individuals exponential depth. Axiom (45) K12 , however,merges leaves tree single named individual b, axiom (46) ensuresN I-rule applied remaining blockable individuals, beginningneighbors b. If, application N I-rule, always merge blockable individualsroot individuals shown Figure 14a, algorithm constructs ABox shownFigure 14b, contains two binary trees root individuals depth 2k/2 . Unlikecase K11 , fully constructing individual labels avoid double-exponentialbehavior, since promotion blockable individuals root individuals prevents blocking.6. Algorithm OptimizationsDL reasoning algorithms often used practice compute classification knowledgebase Kthat is, determine whether K |= v B pair atomic conceptsB occurring K. nave classification algorithm would involve quadratic number callssubsumption checking algorithm, potentially highly expensive.obtain acceptable levels performance, various optimizations developedreduce number subsumption checks time required check (Baader,Hollunder, Nebel, Profitlich, & Franconi, 1994). well-known dependency-directed backtracking optimization (Horrocks, 2007) readily used hypertableau calculus.Furthermore, developed two simple optimizations that, best knowledge, considered previously literature.6.1 Reading Classification Relationships Concept LabelsLet (A, C) ABox set HT-clauses obtained clausifying knowledge baseK, let B atomic concepts want check whether K |= v B;since B atomic, case (A0 , C) unsatisfiable216fiHypertableau Reasoning Description LogicsLLLRRLb.hL, >, 1i.hL, >, 1iRRLb.hL, >, 1i.hL, >, 2iLRRLb.hL, >, 2i.hL, >, 1ib.hL, >, 1iRb.hL, >, 2i.hL, >, 2ib.hL, >, 2ib(a) root introduction strategy N I-ruleLLRRLR|2k/2 1||2k/2 1|b(b) resulting tree, containing doubly-exponentialnumber root individualsA0 = {A(a), B(a)} fresh individual. Let A1 clash-free ABox labelingleaf derivation (A0 , C). use A1 learn following thingssubsumption K. proofs claims straightforward.1. C(a) A1 concept C derivation C(a) dependnondeterministic choice, K |= v C.217fiMotik, Shearer, & Horrocks2. A1 obtained A0 deterministically, K |= v C C(a) A1 .3. C(b) A1 D(b) 6 A1 C concepts b individualblocked, K 6|= C v D.Thus, K deterministic, classify using linear number calls hypertableau algorithm: atomic concept A, check satisfiability (A {A(a)}, C);algorithm produces clash-free ABox A1 , set subsumers containedLA1 (a). optimizations applicable case tableau algorithms well;however, might less effective due increased or-branching.6.2 Caching Blocking LabelsLet R SHIQ+ TBox RBox, respectively, let C = R (T R); sincecontain nominals, assertions involving nominal guard concepts needed.Furthermore, assume classification R involves n calls hypertableaualgorithm ({Ai (ai ), Bi (ai )}, C). Then, derivation ({Ai (ai ), Bi (ai )}, C) containsleaf node labeled clash-free ABox Ai , use nonblocked individualsAi blockers subsequent satisfiability checks ({Aj (aj ), Bj (aj )}, C) j > i.simple consequence following fact. Let I1 I2 two models R4I1 4I2 = ; furthermore, let defined 4I = 4I1 4I2 , AI = AI1 AI2 ,RI = RI1 RI2 , atomic concept atomic role R. Then, simpleinduction structure axioms R, trivial show |= R.property hold presence nominals, impose boundnumber elements interpretation concept; bound could satisfied I1I2 individually, violated I.optimization correct because, instead ({Ai (ai ), Bi (ai )}, C), checksatisfiability (Ai {Ai (ai ), Bi (ai )}, C), use individualsAi potential blockers due anywhere blocking. optimization seensimple form model caching (Horrocks, 2007), key obtaining resultspresent Section 7. example, GALEN one subsumption test costlycomputes substantial part model TBox; subsequent subsumptiontests reuse large parts model.practice, need keep entire ABox Ai around; rather, nonblocked blockable individual predecessor t0 , simply need retain sets LAi (t),LAi (t0 ), LAi (t, t0 ), LAi (t0 , t).7. Implementation EvaluationBased calculus Section 4, implemented prototype DL reasoner calledHermiT. order estimate well calculus performs practice, comparedHermiT two state-of-the-art tableau reasoners several practical problems.objective evaluation establish superiority HermiT, comparebehavior calculus tableau calculi used many existing systems,demonstrate usefulness calculus realistic problems.important understand HermiT prototype, alwaysoutperform well-established reasoners. particular, HermiT may uncompetitive218fiHypertableau Reasoning Description Logicsontologies specialized optimizations needed good performance. example, HermiT cannot process SNOMED CT ontology due large numberconcepts, many reasoners classify ontology easily. reasoners, however, employ techniques quite different standard tableau algorithm;example, EL++ ontology SNOMED CT, Pellet uses reasoning algorithmBaader, Brandt, Lutz (2005), reasoners employ specialized techniqueswell (Haarslev, Moller, & Wandelt, 2008). Similarly, artificial test problemsused TANCS comparison Tableaux98 conference (Balsiger & Heuerding, 1998;Balsiger, Heuerding, & Schwendimann, 2000) DL98 workshop (Horrocks & PatelSchneider, 1998b) often either easy reasoners employing particular optimizationsdifficult due fact encode large propositional satisfiability problems(Horrocks & Patel-Schneider, 1998a). Since goal demonstrate usefulnesshypertableau calculus realistic problems, chosen ignore ontologiestest problems, mainly test specialized calculi optimizations applicable various sublanguages SHOIQ+ . Instead, focus evaluation practicalontologies main difficulty due nontrivial reasoning problems encounteredclassification.addition hypertableau calculus described Section 4, HermiT also implementsoptimizations Section 6 well-known dependency directed backtrackingoptimization (Horrocks, 2007). Thus, HermiT fully supports SHOIQ+ performsatisfiability subsumption testing well knowledge base classification.extensive discussion implementation techniques beyond scope paper;comment briefly implementation anywhere blocking. DL community,commonly understood anywhere blocking costly ancestor blockingbecause, determine blocking status individual, one may need examineindividuals ABox individuals ancestors. implementation avoidsproblem maintaining hash table individuals indexed fourblocking labels. table created scanning individuals increasingsequence ordering . individual A, parent blocked,indirectly blocked; otherwise, algorithm queries hash table individualwhose blocking labels equal s. hash table contains individualt, directly blocked t; otherwise, blocked addedhash table. blocking status individuals thus determinedlinear number hash table lookups.used Pellet 2.0.0rc4 (Parsia & Sirin, 2004) FaCT++ 1.2.2 (Tsarkov & Horrocks,2006) reference implementations SHOIQ tableau algorithm (Horrocks & Sattler,2007). Pellet employs ancestor blocking, FaCT++ recently extendedanywhere blocking. time writing, however, implementation anywhereblocking FaCT++ known incorrect,18 switched feature usedFaCT++ ancestor blocking well. measure effects ancestor vs. anywhereblocking, also used HermiT-Anca version HermiT ancestor blocking.used collection 392 test ontologies assembled three independentsources.18. Personal communication Dmitry Tsarkov.219fiMotik, Shearer, & HorrocksGardiner ontology suite (Gardiner, Horrocks, & Tsarkov, 2006) collectionOWL ontologies gathered Web includes many commonlyused OWL ontologies.Open Biological Ontologies (OBO) Foundry19 collection biology lifescience ontologies.GALEN (Rector & Rogers, 2006) large complex biomedical ontologyproven notoriously difficult classify existing reasoners.preprocessed ontologies resolve ontology imports eliminate trivialsyntactic errors. Thus, test ontology parsed single file using OWLAPI. test ontologies available online.20measured time needed classify test ontology using mentioned reasoners. tests performed 2.2 GHz MacBook Pro 2 GB physical memory.classification attempt aborted exhausted available memory (Java toolsallowed use 1 GB heap space), exceeded timeout 30 minutes.three reasoners exhibited negligible differences performance testontologies. Therefore, discuss next test results interesting ontologiesis, ontologies classified least one tested reasoners,either trivial tested reasoners exhibited significant differenceperformance. include several ontologies OBO corpus (Molecule Role, XPUber Anatomy, XP Plant Anatomy, Cellular Component, Gazetteer, CHEBI), two versionsNational Cancer Institute (NCI) Thesaurus (Hartel et al., 2005), two versionsGALEN medical terminology ontology, two versions Foundational Model Anatomy(FMA) (Golbreich et al., 2006), Wine ontology OWL Guide,21 two SWEETontologies developed NASA,22 version DOLCE ontology developedInstitute Cognitive Science Technology Italian National Research Council.23Basic statistical information ontologies summarized Table 10.noticed that, three reasoners, classification times may vary run run.Pellet HermiT, due Javas collection library: order iterationcollections often depends objects hash codes, may vary run run;that, turn, may change order derivation rules applied,orders may better others. conjecture FaCT++ susceptible similarvariations. times may vary, noticed case ontology mightsuccessfully classified one run, another. Therefore, Table 11 presentclassification times interesting ontologies obtained one particularrun; times taken typical. identified four groups ontologies,delineate Tables 10 11 horizontal lines.19.20.21.22.23.http://obofoundry.org/http://hermit-reasoner.com/2009/JAIR_benchmarks/http://www.w3.org/TR/owl-guide/http://sweet.jpl.nasa.gov/ontology/http://www.loa-cnr.it/DOLCE.html220fiHypertableau Reasoning Description LogicsTable 10: Statistics Interesting OntologiesOntology NameClassesRolesIndividualsMolecule RoleXP Uber AnatomyXP Plant AnatomyXP RegulatorsCellular ComponentNCI-1GazetteerGALEN-doctoredGALEN-undoctoredCHEBIFMA-LiteSWEET PhenomenaSWEET NumericsWineDOLCE-PlansNCI-2FMA-Constitutional884911427191452552027889276521509792748274820977751411728150613811870576416482828244702413413921451771726418916812805688955860991551691632440214804002439724622517111320627085Number AxiomsTBox RBoxABox9243146693577042896473454680016734939374179383751195582419218435526510030412269518087331402799800232393054094829039512805688955860991551691632440214804002439724622549134049468086ExpressivityALE+ALEHIF +SHIFSHSHALEALE+ALEHIF +ALEHIF +ALE+ALEI+SHOIN (D)SHOIN (D)SHOIN (D)SHOIN (D)ALCH(D)ALCOIF(D)ontologies first group, HermiT performs similarly HermiT-Anc,suggests little impact anywhere blocking performance. Consequently, believeHermiT outperforms reasoners mainly due reduced nondeterminismhypertableau calculus. shown Table 10, Molecule Role, XP Uber Anatomy,NCI-1 use disjunctions, HermiT classifies deterministically using linearnumber calls hypertableau algorithm. FaCT++ outperforms HermiT NCI-1ontology classified using completely defined concepts optimization(Tsarkov & Horrocks, 2005a), FaCT++ implements HermiT not.optimization enables FaCT++ use simpler structural reasoning techniques ontologiessatisfy certain syntactic constraints.ontologies second group, HermiT-Anc significantly slower HermiT.suggests anywhere blocking significantly improves performance since prevents construction large models. Pellet runs memory ontologiesgroup; furthermore, FaCT++ cannot process two significantly slowerHermiT CHEBI. FaCT++, however, faster HermiT-Anc CHEBI GALENdoctored, conjecture mainly due ordering heuristics (Tsarkov &Horrocks, 2005b) used FaCT++. superior performance HermiT ontologiesgroup mainly due fact ontologies classified deterministically using linear number concept satisfiability tests. Furthermore, HermiTsclassification time cases dominated first test, cachingblocking labels described Section 6.2 makes subsequent tests easy.ontologies third group, HermiT significantly slowerreasoners. Table 10 shows, ontologies group contain nominals, preventsHermiT caching blocking labels. Furthermore, due nominals, ABox must221fiMotik, Shearer, & HorrocksTable 11: Results Performance EvaluationClassification Times (seconds)HermiTHermiT-AncPelletFaCT++Molecule Role3.33.425.7304.5XP Uber Anatomy5.44.986.0XP Plant Anatomy12.811.287.222.9XP Regulators14.117.135.466.6Celular Component18.618.040.576.7NCI-114.114.423.23.0Gazetteer131.9132.3GALEN-doctored8.8456.315.9GALEN-undoctored126.3CHEBI24.2397.0FMA-Lite107.2SWEET Phenomena13.511.20.2SWEET Numerics76.772.63.70.2Wine343.7524.619.5162.1DOLCE-Plans1075.1105.1NCI-2172.060.7FMA-Constitutional616.7Note: entry means reasoner unable classify ontology eitherdue time memory exhaustion.Ontology Nametaken account classification, HermiT currently reapplies hypertableaurules entire ABox run. Effectively, HermiT reuse computationdifferent hypertableau runs. two reasoners, however, use completiongraph caching optimization (Sirin, Cuenca Grau, & Parsia, 2006), tableau rulesfirst applied entire ABox, resulting completion graph used startingpoint subsequent run.HermiT unable classify two ontologies fourth group. NCI-2FMA-Constitutional use disjunctions, cannot classified using linear numberconcept satisfiability tests; instead, HermiT uses classification algorithm Baaderet al. (1994). classification tests straightforward (each test takes less 50 ms);however, resulting taxonomy rather shallow, HermiT makes almost quadraticnumber tests. Pellet FaCT++, however, use optimized versionsclassification algorithm reduce number tests need performed.summarize, although HermiT better Pellet FaCT++ ontologies,results clearly demonstrate practical potential reduced nondeterminism anywhere blocking. fact, anywhere blocking mean differencesuccess failure complex ontologies, suggests and-branching significant source inefficiency practice or-branching. Anywhere blocking applicable222fiHypertableau Reasoning Description Logicstableau calculi well (as mentioned earlier, FaCT++ already contains preliminaryversion it), believe results used improve performancetableau reasoners well without need major redesign. Conversely,optimizations used tableau reasoners used hypertableau algorithm,incorporating HermiT would make HermiT competitive Pellet FaCT++cases HermiT currently slower.8. Conclusionpaper presented novel reasoning algorithm DLs. algorithm basedhyperresolution anywhere blocking, reduces nondeterminism due GCIssizes generated models. Furthermore, algorithm uses novel refinementNI -rule reduce amount nondeterminism introduced order handleinteraction nominals, inverse roles, number restrictions (Horrocks & Sattler,2007). refined version NI -rule equally applicable tableau algorithms.implemented calculus conducted extensive performance comparison. results show combination new calculus novel optimizationssignificantly increases performance DL reasoning practice: reasoner currentlyone classify several complex ontologies.Despite advance performance, still ontologies, fullversion GALEN,24 defeat HermiT (and state-of-the-art tableau reasoners).large number cyclic axioms ontologies cause HermiT constructextremely large ABoxes eventually exhaust available memory. alleviateproblem, developed reasoning technique -rule modified nondeterministically reuse individuals ABox generated thus far. Initial experimentstechnique shown promising results (Motik & Horrocks, 2008).Finally, plan extend technique DL SROIQ, extends SHOIQexpressive role inclusion axioms allow us express, example,brother persons father also persons uncle. logic considerable interestunderpins OWL 2the extension OWL currently standardized W3C.Acknowledgmentsextended version paper published CADE 2007 (Motik et al., 2007).thank anonymous reviewer numerous comments contributed qualitypaper.ReferencesBaader, F., Brandt, S., & Lutz, C. (2005). Pushing EL Envelope. Kaelbling, L. P., &Saffiotti, A. (Eds.), Proc. 19th Int. Joint Conference Artificial Intelligence(IJCAI 2005), pp. 364369, Edinburgh, UK. Morgan Kaufmann Publishers.Baader, F., Buchheit, M., & Hollunder, B. (1996). Cardinality Restrictions Concepts.Artificial Intelligence, 88 (12), 195213.24. http://www.co-ode.org/galen/223fiMotik, Shearer, & HorrocksBaader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2007). Description Logic Handbook: Theory, Implementation Applications(2nd edition). Cambridge University Press.Baader, F., Hollunder, B., Nebel, B., Profitlich, H.-J., & Franconi, E. (1994). EmpiricalAnalysis Optimization Techniques Terminological Representation systems or:Making KRIS Get Move on. Applied Intelligence, 4 (2), 109132.Baader, F., & Nutt, W. (2007). Basic Description Logics. Baader, F., Calvanese, D.,McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), Description LogicHandbook: Theory, Implementation Applications (2nd edition)., pp. 47104. Cambridge University Press.Baader, F., & Sattler, U. (2001). Overview Tableau Algorithms Description Logics.Studia Logica, 69, 540.Balsiger, P., & Heuerding, A. (1998). Comparison Theorem Provers Modal LogicsIntroduction Summary. de Swart, H. (Ed.), Proc. 2nd Int. Conf.Analytic Tableaux Related Methods (TABLEAUX98), Vol. 1397 LNAI, pp.2526. Springer.Balsiger, P., Heuerding, A., & Schwendimann, S. (2000). Benchmark MethodPropositional Modal Logics K, KT, S4. Journal Automated Reasoning, 24 (3), 297317.Baumgartner, P., Furbach, U., & Niemela, I. (1996). Hyper Tableaux. Proc.European Workshop Logics Artificial Intelligence (JELIA 96), No. 1126LNAI, pp. 117, Evora, Portugal. Springer.Baumgartner, P., Furbach, U., & Pelzer, B. (2008). Hyper Tableaux CalculusEquality Application Finite Model Computation. Journal LogicComputation.Baumgartner, P., & Schmidt, R. A. (2006). Blocking Enhancements BottomUp Model Generation Methods. Furbach, U., & Shankar, N. (Eds.), Proc.3rd Int. Joint Conf. Automated Reasoning (IJCAR 2006), Vol. 4130 LNCS, pp.125139, Seattle, WA, USA. Springer.Borgida, A. (1996). Relative Expressiveness Description Logics PredicateLogics. Artificial Intelligence, 82 (12), 353367.Bry, F., & Torge, S. (1998). Deduction Method Complete Refutation FiniteSatisfiability. Dix, J., del Cerro, L. F., & Furbach, U. (Eds.), Proc. EuropeanWorkshop Logics Artificial Intelligence (JELIA 98), Vol. 1489 LNCS, pp.122138, Dagstuhl, Germany. Springer.Buchheit, M., Donini, F. M., & Schaerf, A. (1993). Decidable Reasoning TerminologicalKnowledge Representation Systems. Journal Artificial Intelligence Research, 1,109138.Demri, S., & de Nivelle, H. (2005). Deciding Regular Grammar Logics ConverseFirst-Order Logic. Journal Logic, Language Information, 14 (3), 289329.224fiHypertableau Reasoning Description LogicsDing, Y., & Haarslev, V. (2006). Tableau Caching Description Logics InverseTransitive Roles. Parsia, B., Sattler, U., & Toman, D. (Eds.), Proc. 2006 Int.Workshop Description Logics (DL 2006), Vol. 189 CEUR Workshop Proceedings,Windermere, UK.Donini, F. M. (2007). Complexity Reasoning. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), Description Logic Handbook:Theory, Implementation Applications (2nd edition)., pp. 105148. CambridgeUniversity Press.Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1998). AL-log: Integrating DatalogDescription Logics. Journal Intelligent Information Systems, 10 (3), 227252.Donini, F. M., & Massacci, F. (2000). EXPTIME tableaux ALC. Artificial Intelligence,124 (1), 87138.Faddoul, J., Farsinia, N., Haarslev, V., & Moller, R. (2008). Hybrid Tableau AlgorithmALCQ. Ghallab, M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.),Proc. 18th European Conf. Artificial Intelligence (ECAI 2008), Vol. 178Frontiers Artificial Intelligence Applications, pp. 725726, Patras, Greece. IOSPress.Fermuller, C., Tammet, T., Zamov, N., & Leitsch, A. (1993). Resolution MethodsDecision Problem, Vol. 679 LNAI. Springer.Fermuller, C. G., Leitsch, A., Hustadt, U., & Tammet, T. (2001). Resolution Decision Procedures. Robinson, A., & Voronkov, A. (Eds.), Handbook Automated Reasoning,Vol. II, chap. 25, pp. 17911849. Elsevier Science.Gardiner, T., Horrocks, I., & Tsarkov, D. (2006). Automated Benchmarking DescriptionLogic Reasoners. Proc. 2006 Description Logic Workshop (DL 2006), Vol.189 CEUR Workshop Proceedings.Georgieva, L., Hustadt, U., & Schmidt, R. A. (2003). Hyperresolution Guarded Formulae. Journal Symbolic Computation, 36 (12), 163192.Golbreich, C., Zhang, S., & Bodenreider, O. (2006). Foundational Model AnatomyOWL: Experience Perspectives. Journal Web Semantics, 4 (3), 181195.Gore, R., & Nguyen, L. A. (2007). EXPTIME Tableaux Global Caching DescriptionLogics Transitive Roles, Inverse Roles Role Hierarchies. Proc. 16thInt. Conf. Automated Reasoning Tableaux Related Methods (TABLEAUX2007), Vol. 4548 LNCS, pp. 133148, Aix en Provence, France. Springer.Haarslev, V., & Moller, R. (2001). RACER System Description. Gore, R., Leitsch, A., &Nipkow, T. (Eds.), Proc. 1st Int. Joint Conf. Automated Reasoning (IJCAR2001), Vol. 2083 LNAI, pp. 701706, Siena, Italy. Springer.Haarslev, V., Moller, R., & Wandelt, S. (2008). Revival Structural SubsumptionTableau-Based Description Logic Reasoners. Baader, F., Lutz, C., & Motik, B.(Eds.), Proc. 21st Int. Workshop Description Logics (DL 2008), Vol. 353CEUR Workshop Proceedings, Dresden, Germany.225fiMotik, Shearer, & HorrocksHartel, F. W., de Coronado, S., Dionne, R., Fragoso, G., & Golbeck, J. (2005). Modelingdescription logic vocabulary cancer research. Journal Biomedical Informatics,38 (2), 114129.Horrocks, I. (1998). Using Expressive Description Logic: FaCT Fiction?. Cohn,A. G., Schubert, L., & Shapiro, S. C. (Eds.), Proc. 6th Int. Conf. Principles Knowledge Representation Reasoning (KR 98), pp. 636647, Trento,Italy. Morgan Kaufmann Publishers.Horrocks, I. (2007). Implementation Optimization Techniques. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), Description Logic Handbook: Theory, Implementation Applications (2nd edition)., pp.329373. Cambridge University Press.Horrocks, I., & Patel-Schneider, P. F. (1998a). Comparing Subsumption Optimizations.Franconi, E., De Giacomo, G., MacGregor, R. M., Nutt, W., & Welty, C. A. (Eds.),Proc. 1998 Description Logic Workshop (DL98), Vol. 11 CEUR WorkshopProceedings, pp. 9094, PovoTrento, Italy.Horrocks, I., & Patel-Schneider, P. F. (1998b). DL Systems Comparison. Franconi,E., De Giacomo, G., MacGregor, R. M., Nutt, W., & Welty, C. A. (Eds.), Proc.1998 Int. Workshop Description Logic (DL98), Vol. 11 CEUR WorkshopProceedings, pp. 5557, PovoTrento, Italy.Horrocks, I., & Sattler, U. (2001). Ontology Reasoning SHOQ(D) Description Logic.Nebel, B. (Ed.), Proc. 7th Int. Joint Conf. Artificial Intelligence (IJCAI2001), pp. 199204, Seattle, WA, USA. Morgan Kaufmann Publishers.Horrocks, I., & Sattler, U. (2007). Tableau Decision Procedure SHOIQ. JournalAutomated Reasoning, 39 (3), 249276.Horrocks, I., Sattler, U., & Tobies, S. (2000a). Practical Reasoning ExpressiveDescription Logics. Logic Journal IGPL, 8 (3), 239263.Horrocks, I., Sattler, U., & Tobies, S. (2000b). Reasoning Individuals Description Logic SHIQ. MacAllester, D. (Ed.), Proc. 17th Int. Conf. AutomatedDeduction (CADE-17), Vol. 1831 LNAI, pp. 482496, Pittsburgh, USA. Springer.Hudek, A. K., & Weddell, G. (2006). Binary Absorption Tableaux-Based ReasoningDescription Logics. Parsia, B., Sattler, U., & Toman, D. (Eds.), Proc.2006 Int. Workshop Description Logics (DL 2006), Vol. 189 CEUR WorkshopProceedings, Windermere, UK.Hustadt, U., Motik, B., & Sattler, U. (2005). Data Complexity Reasoning Expressive Description Logics. Proc. 19th Int. Joint Conf. Artificial Intelligence(IJCAI 2005), pp. 466471, Edinburgh, UK. Morgan Kaufmann Publishers.Hustadt, U., & Schmidt, R. A. (1999). Issues Decidability Description LogicsFramework Resolution. Caferra, R., & Salzer, G. (Eds.), Selected PapersAutomated Deduction Classical Non-Classical Logics, Vol. 1761 LNAI, pp.191205. Springer.Kutz, O., Horrocks, I., & Sattler, U. (2006). Even Irresistible SROIQ.Doherty, P., Mylopoulos, J., & Welty, C. A. (Eds.), Proc. 10th Int. Conf.226fiHypertableau Reasoning Description LogicsPrinciples Knowledge Representation Reasoning (KR 2006), pp. 6878, LakeDistrict, UK. AAAI Press.Motik, B. (2006). Reasoning Description Logics using Resolution DeductiveDatabases. Ph.D. thesis, Univesitat Karlsruhe, Germany.Motik, B., & Horrocks, I. (2008). Individual Reuse Description Logic Reasoning.Proc. 4th Int. Joint Conf. Automated Reasoning (IJCAR 2008), Sydney,Australia. Springer. appear.Motik, B., Shearer, R., & Horrocks, I. (2007). Optimized Reasoning Description Logicsusing Hypertableaux. Pfenning, F. (Ed.), Proc. 21st Conference Automated Deduction (CADE-21), Vol. 4603 LNAI, pp. 6783, Bremen, Germany.Springer.Nonnengart, A., & Weidenbach, C. (2001). Computing Small Clause Normal Forms.Robinson, A., & Voronkov, A. (Eds.), Handbook Automated Reasoning, Vol. I,chap. 6, pp. 335367. Elsevier Science.Parsia, B., & Sirin, E. (2004). Pellet: OWL-DL Reasoner. Poster 3rd Int. SemanticWeb Conference (ISWC 2004).Patel-Schneider, P. F., Hayes, P., & Horrocks, I. (2004). OWL Web Ontology Language:Semantics Abstract Syntax, W3C Recommendation..http://www.w3.org/TR/owl-semantics/.Plaisted, D. A., & Greenbaum, S. (1986). Structure-Preserving Clause Form Translation.Journal Symbolic Logic Computation, 2 (3), 293304.Rector, A. L., & Rogers, J. (2006). Ontological Practical Issues Using DescriptionLogic Represent Medical Concept Systems: Experience GALEN. Barahona,P., Bry, F., Franconi, E., Henze, N., & Sattler, U. (Eds.), Tutorial Lectures 2ndInt. Summer School 2006, Vol. 4126 LNCS, pp. 197231, Lisbon, Portugal. Springer.Robinson, A. (1965). Automatic Deduction Hyper-Resolution. Int. Journal Computer Mathematics, 1, 227234.Schmidt, R. A., & Hustadt, U. (2003). Principle Incorporating Axioms FirstOrder Translation Modal Formulae. Baader, F. (Ed.), Proc. 19th Int.Conf. Automated Deduction (CADE-19), Vol. 2741 LNAI, pp. 412426, MiamiBeach, FL, USA. Springer.Sirin, E., Cuenca Grau, B., & Parsia, B. (2006). Wine Water: Optimizing Description Logic Reasoning Nominals. Doherty, P., Mylopoulos, J., & Welty, C. A.(Eds.), Proc. 10th Int. Conf. Principles Knowledge RepresentationReasoning (KR 2006), pp. 9099, Lake District, UK. AAAI Press.Tobies, S. (2000). Complexity Reasoning Cardinality Restrictions NominalsExpressive Description Logics. Journal Artificial Intelligence Research, 12, 199217.Tobies, S. (2001). Complexity Results Practical Algorithms Logics KnowledgeRepresentation. Ph.D. thesis, RWTH Aachen, Germany.227fiMotik, Shearer, & HorrocksTsarkov, D., & Horrocks, I. (2004). Efficient Reasoning Range Domain Constraints.Haarslev, V., & Moller, R. (Eds.), Proc. 2004 Int. Workshop DescriptionLogics (DL 2004), Vol. 104 CEUR Workshop Proceedings, Whistler, BC, Canada.Tsarkov, D., & Horrocks, I. (2005a). Optimised Classification Taxonomic KnowledgeBases. Horrocks, I., Sattler, U., & Wolter, F. (Eds.), Proc. 2005 Int. WorkshopDescription Logics (DL 2005), Edinburgh, Scotland, UK.Tsarkov, D., & Horrocks, I. (2005b). Ordering Heuristics Description Logic Reasoning.Kaelbling, L. P., & Saffiotti, A. (Eds.), Proc. 19th Int. Joint Conf. Artificial Intelligence (IJCAI 2005), pp. 609614, Edinburgh, UK. Morgan KaufmannPublishers.Tsarkov, D., & Horrocks, I. (2006). FaCT++ Description Logic Reasoner: System Description. Proc. 3rd Int. Joint Conf. Automated Reasoning (IJCAR 2006),Vol. 4130 LNAI, pp. 292297, Seattle, WA, USA. Springer.Wu, J., & Haarslev, V. (2008). Planning Axiom Absorption. Baader, F., Lutz, C., &Motik, B. (Eds.), Proc. 21st Int. Workshop Description Logics (DL 2008),Vol. 353 CEUR Workshop Proceedings, Dresden, Germany.228fiJournal Artificial Intelligence Research 36 (2009) 71Submitted 12/08; published 10/09Prime Implicates Prime Implicants:Propositional Modal LogicMeghyn Bienvenumeghyn@informatik.uni-bremen.deDepartment Mathematics Computer ScienceUniversity Bremen, GermanyAbstractPrime implicates prime implicants proven relevant number areasartificial intelligence, notably abductive reasoning knowledge compilation.purpose paper examine notions might appropriately extendedpropositional logic modal logic K. begin paper considering numberpotential definitions clauses terms K. different definitions evaluatedrespect set syntactic, semantic, complexity-theoretic properties characteristicpropositional definition. compare definitions respect propertiesnotions prime implicates prime implicants induce.definition perfectly generalizes propositional notions, showexist one definition satisfies many desirable properties propositionalcase. second half paper, consider computational propertiesselected definition. end, provide sound complete algorithms generatingrecognizing prime implicates, show prime implicate recognition taskPspace-complete. also prove upper lower bounds size number primeimplicates. paper focuses logic K, results hold equally wellmulti-modal K concept expressions description logic ALC.1. IntroductionPrime implicates prime implicants important notions artificial intelligence.given rise significant body work automated reasoning appliednumber different sub-areas AI. Traditionally, concepts studiedcontext propositional logic, also considered many-valued(Ramesh & Murray, 1994) first-order logic (Marquis, 1991a, 1991b). muchknown, however, prime implicates prime implicants logics. particular,definition prime implicate prime implicant ever proposed modaldescription logic, shown reasonable definition provided.Given increasing interest modal description logics knowledge representationlanguages, one naturally wonders whether notions suitably generalizedexpressive logics.recall propositional logic prime implicates formula definedlogically strongest clausal consequences. restriction clauses made orderreduce redundant elements formulas set consequences: use keepingaround consequence b one already consequences b. decisionconsider logically strongest clausal consequences motivated desireeliminate irrelevant weaker consequences: already consequence a,c2009AI Access Foundation. rights reserved.fiBienvenupoint retaining consequences b b. Prime implicates thus providecomplete yet compact representation set logical consequences formula.particularly nice representation makes many computational taskssimpler: satisfiability, tautology, entailment, equivalence queries conditioningforgetting transformations tractable formulae represented primeimplicates (Darwiche & Marquis, 2002). prime implicates consideredinteresting target language knowledge compilation (Cadoli & Donini, 1997; Darwiche& Marquis, 2002). Prime implicates also proved relevant sub-areas AI,like distributed reasoning (Adjiman, Chatalic, Goasdoue, Rousset, & Simon, 2006), beliefrevision (Bittencourt, 2007; Pagnucco, 2006), non-monotonic reasoning (cf. Przymusinski,1989), characterizations relevance (Lakemeyer, 1995; Lang, Liberatore, & Marquis,2003).dual notion prime implicates prime implicants, definedlogically weakest terms (= conjunctions literals) imply given formula. mainapplication domain prime implicants abduction diagnosis. recallabduction, one given background theory observation, objectivefind explanation observation. logical terms, explanation formulalogically entails observation taken together background theory.set explanations abduction problem large, important questionselect representative subset explanations. One common approachuse prime implicants: relevant explanations observation respectbackground theory taken prime implicants (de Kleer, Mackworth,& Reiter, 1992; Eiter & Makino, 2002).many applications AI, expressive power propositional logic proves insufficient. First-order logic provides much greater level expressivity, priceundecidability. Modal description logics offer interesting trade-off expressivity complexity, generally expressive propositional logic yetbetter-behaved computationally first-order logic. explains growing trendtowards using languages knowledge representation.prototypical description logic ALC, extends propositional logic restrictedforms universal existential quantification. example expression ALCF emale hasChild.F emale hasChild.(Doctor P rof essor) hasP et.Dogdescribes set individuals female, least one daughter onepet dog, children either doctors professors.concept expression represented equally well modal logic K2 formula:F emale 31 F emale 21 (Doctor P rof essor) 32 DogSchild (1991) proved general result showed description logic ALC nbinary relations fact notational variant multi-modal logic Kn . meansresults concerning Kn transferred ALC, vice-versa.paper, investigate notions prime implicates prime implicantsmodal logic K = K1 , actually results hold formulae Kn , hencealso concept expressions ALC. decision present results terms K72fiPrime Implicates Prime Implicants Modal Logicrather terms Kn ALC motivated solely desire simplify notationincrease readability proofs.question notions prime implicates prime implicants suitablydefined logic K clearly interest theoretical point view. argue,however, question also practically relevant. support claim, brieflydiscuss two application areas study prime implicates prime implicantsK might prove useful.first domain application consider abductive reasoning K. notedabove, one key foundational issues abductive reasoning selectioninteresting subset explanations. issue especially crucial logics like Kallow infinite number non-equivalent formulae, since means numbernon-equivalent explanations abduction problem large fact infinite,making simply impossible enumerate entire set explanations. prime implicantswidely-accepted means characterizing relevant explanations propositional logic,reasonable starting point research abductive reasoning logic K studydifferent possible definitions prime implicant K properties.investigation prime implicates K also relevant development knowledge compilation procedures K. recall knowledge compilation (cf. Darwiche& Marquis, 2002) general technique coping intractability reasoningconsists off-line phase knowledge base rewritten equivalentknowledge base allows tractable reasoning, followed online phasereasoning performed compiled knowledge base. idea initial costcompiling knowledge base offset computational savings later queries.Currently, work knowledge compilation restricted propositional logic, eventhough technique could prove highly relevant modal description logics,generally suffer even higher computational complexity propositional logic.prime implicates one better-known mechanisms compiling formulae propositional logic, certainly makes sense investigate whether approach knowledgecompilation fruitfully extended logics like K.paper organized follows. preliminaries, consider appropriately generalize notions clauses terms K. obvious definition,enumerate list syntactic, semantic, complexity-theoretic properties propositional definitions, use compare different candidate definitions.next consider different definitions light notions prime implicate primeimplicant induce. again, list basic properties propositionalcase would like satisfy, see different definitions measure up.second half paper, investigate computational propertiessatisfactory definition prime implicates. consider problems prime implicategeneration recognition, provide sound complete algorithms tasks.also study complexity prime implicate recognition problem, showingPspace-complete thus complexity satisfiability deduction K.conclude paper discussion relevance results two applicationareas cited directions future research. order enhance readability paper, proofs omitted body text. Full proofsfound appendix.73fiBienvenu2. Preliminary Definitions Notationbriefly recall basics modal logic K (refer Blackburn, de Rijke, & Venema,2001; Blackburn, van Benthem, & Wolter, 2006, good introductions modal logic).Formulae K built set propositional variables V, standard logicalconnectives (, , ), modal operators 2 3. call formulaform 2 (resp. 3) 2-formula (resp. 3-formula). convenient useabbreviation . adopt shorthand 2k (resp. 3k ) referformula consisting preceded k copies 2 (resp. 3), convention20 = 30 = . use var() refer set propositional variables appearingformula . modal depth formula , written (), defined maximalnumber nested modal operators appearing , e.g. (3(a 2a) a) = 2. definelength formula , written ||, number occurrences propositional variables,logical connectives, modal operators . example, would |(a b)| = 4|3(a b) 2a| = 8.Negation normal form (NNF) defined propositional logic: formula saidNNF negation appears directly propositional variables. Every formulaK transformed equivalent formula NNF using recursive procedureNnf defined follows:Nnf (l)=l (for propositional literals l)Nnf (1 2 )=Nnf (1 )Nnf (2 )Nnf ((1 2 ))=Nnf (1 )Nnf (2 )Nnf (1 2 )=Nnf (1 )Nnf (2 )Nnf ((1 2 ))=Nnf (1 )Nnf (2 )Nnf (2)=2Nnf ()Nnf (2)=3Nnf ()Nnf (3)=3Nnf ()Nnf (3)=2Nnf ()Nnf ()=Nnf ()example, applying Nnf formula 2(a 3(b c)) results formula 3(a2(b c)) NNF. transformation Nnf takes linear time, yields formuladouble size original formula modal depthpropositional variables original.model K tuple = hW, R, vi, W non-empty set possible worlds,R W W binary relation worlds, v : W V {true, f alse} valuationpropositional variables world. Models understood labelled directedgraphs, vertices correspond elements W, directed edges representbinary relation R, vertices labeled propositional valuations specifypropositional variables true corresponding possible world.Satisfaction formula model world w (written M, w |= ) definedinductively follows:M, w |= v(w, a) = trueM, w |= M, w 6|=M, w |= M, w |= M, w |=M, w |= M, w |= M, w |=M, w |= 2 M, w |= w wRw74fiPrime Implicates Prime Implicants Modal LogicM, w |= 3 M, w |= w wRwthink models labeled directed graphs, determining satisfactionformula 2 vertex w consists evaluating vertices reachedw via edge; 2 satisfied w case holdssuccessor vertices. Similarly, order decide whether formula 3 holds vertex w,consider successors w graph check whether least onevertices satisfies .formula said tautology, written |= , M, w |= every modelworld w. formula satisfiable model world wM, w |= . w M, w |= , called unsatisfiable,write |= .Ladner (1977) showed satisfiability unsatisfiability K Pspace-complete.Pspace membership, Ladner exhibited polynomial space tableaux-style algorithmdeciding satisfiability K formulae. Pspace-hardness proven means reductionQBF validity (the canonical Pspace-complete problem).modal logic, notion logical consequence (or entailment) defined onetwo ways:formula global consequence whenever M, w |= every world wmodel M, M, w |= every world wformula local consequence M, w |= implies M, w |= every modelworld wpaper, consider notion local consequence, take |=mean local consequence . |= , say entails .Two formulae called equivalent, written , |= |= .formula said logically stronger |= 6|= .highlight basic properties logical consequence equivalence Kplay important role proofs results.Theorem 1. Let , 1 , ..., , , 1 , ..., n formulae K, let propositionalformula.1. |= |= |=2. |= 3 |= 3 2 |= 23. 31 ...3m 21 ...2n |= ( |= 1 ...n |= i)4. |= 31 ... 3m 21 ... 2n (|= |= 1 ... i)5. 2 |= 21 ... 2n |=6. 31 ... 3m 21 ... 2n31 ... 3m 2(1 1 ... ) ... 2(n 1 ... )75fiBienvenuStatement 1 Theorem 1 shows three reasoning tasks entailment, unsatisfiability, tautology-testing rephrased terms one another. Statement 2 tells usentailment two 2- 3-formulae reduced entailmentformulae first modality removed. Statements 3 4 define conditionsconjunction (resp. disjunction) propositional literals 2- 3-formulaeunsatisfiable (resp. tautology). Statement 5 gives us conditions2-formula implies disjunction 2-formulae. Statement 6 demonstrates interaction2- 3-formulae disjunction.Theorem 2. Let disjunction propositional literals 2- 3-formulae.following statements holds:1. |= non-tautological propositional clause , every disjuncteither propositional literal unsatisfiable 3-formula2. |= 31 ... 3n , every disjunct 3-formula3. |= 21 ... 2n 6|= 21 ... 2n , every disjunct either2-formula unsatisfiable 3-formulaTheorem 3. Let = 31 ... 3m 21 ... 2n = 31 ... 3p21 ... 2q formulae K. propositional 6|= ,|=1 ... |= 1 ... p|=every j |= 1 ... p jTheorems 2 3 concern entailment relations formulae disjunctionspropositional literals 2- 3-formulae. Theorem 2 tells us kinds formulaetype entail propositional clause, disjunction 3-formulae, disjunction2-formulae, Theorem 3 outlines conditions two formulaetype related entailment relation. illustrate Theorem 3small example.Example 4. Consider formula = b 3(a 3c) 3(d 2a) 2(c d).according Theorem 3, have:|= b 3(a d) 2c, since b |= b (a 3c) (d 2a) |=c |= c (a d)6|= 3c, since b 6|=6|= b 3(a c), since (a 3c) (d 2a) 6|= c6|= b 3(a 2a) 2c, since c 6|= c (a 2a)76fiPrime Implicates Prime Implicants Modal Logic3. Literals, Clauses, Terms Kseen introduction, notions prime implicates implicantsstraightforwardly defined using notions clauses terms. Thus, aim providesuitable definitions prime implicates implicants logic K, first need decideupon suitable definition clauses terms K. Unfortunately, whereas clausesterms standard notions propositional first-order logic1 , generallyaccepted definition clauses terms K. Indeed, several quite different notionsclauses terms proposed literature different purposes.Instead blindly picking definition hoping appropriate, preferlist number characteristics literals, clauses, terms propositional logic, givingus principled means comparing different candidate definitions. propertiesdescribes something literal, clause, term propositional logic.Although list cannot considered exhaustive, believe covers principalsyntactic, semantic, complexity-theoretic properties propositional definition.P1 Literals, clauses, terms negation normal form.P2 Clauses contain , terms contain , literals contain neither .P3 Clauses (resp. terms) disjunctions (resp. conjunctions) literals.P4 negation literal equivalent another literal. Negations clauses (resp.terms) equivalent terms (resp. clauses).P5 Every formula equivalent finite conjunction clauses. Likewise, every formulaequivalent finite disjunction terms.P6 task deciding whether given formula literal, term, clause accomplished polynomial-time.P7 task deciding whether clause (resp. term) entails another clause (resp. term)accomplished polynomial-time.One may wonder whether exist definitions literals, clauses, terms Ksatisfying properties. Unfortunately, show impossible.Theorem 5. definition literals, clause, terms K satisfies properties P1P2 cannot satisfy P5.proof Theorem 5 makes use fact distribute 3distribute 2, means impossibility result holds equallywell standard modal description logics.consider variety possible definitions evaluate respectcriteria. first definition consider proposed Cialdea1. One might wonder simply translate formulae K first-order formulaeput clausal form. reason simple: looking define clauses terms withinlanguage K, clauses obtain passing first-order logic generally expressible K.Moreover, define clauses K first-order clauses representable K,would obtain set clauses containing 3 modalities, thereby losing much expressivity K.77fiBienvenuMayer Pirri (1995) paper abductive reasoning modal logic. define termsformulae constructed propositional literals using , 2,3. Modal clauses literals used paper defined analogously,yielding following definition2 :D1L ::= | | 2L | 3LC ::= | | 2C | 3C | C C::= | | 2T | 3T |easy see inspection definition satisfies properties P1-P2, P4, P6.Property P3 satisfied, however, since clauses disjunctionsliterals take instance 2(a b). Theorem 5 fact P1 P2satisfied, conclude property P5 cannot hold. first glance, may seementailment clauses terms could accomplished polynomial time,case. fact, show problem NP-complete. proof reliesstrong resemblance terms D1 concept expressions descriptionlogic ALE (for unsatisfiability deduction known NP-complete).using slightly different definition, gain P3:D2L ::= | | 2L | 3LC ::= L | C C::= L |easily verified definition D2 satisfies properties P1-P4 P6. definitionD1 satisfy P5, definition D2 even less expressive, follows D2satisfy P5 either. reduced expressiveness however improve computationalcomplexity: property P7 still satisfied show entailmentclauses terms NP-complete using reduction used definition D1.fact even extremely inexpressive definition like D2 allow polynomialentailment clauses terms suggests property P7 cannot satisfiedreasonable definition clauses terms K.Let us consider expressive options. begin following definitionclauses proposed Enjalbert Farinas del Cerro (1989) purposemodal resolution:D3C ::= | | 2C | 3ConjC | C CConjC ::= C | ConjC ConjCdefinition clauses extended definition terms literals satisfiesP3 P4, extension satisfies properties. Let us first considerone possible extensions satisfies P4 maximal subset P1-P7:D3aL ::= | | 2L | 3LC ::= | | 2C | 3ConjC | C CConjC ::= C | ConjC ConjC::= | | 2DisjT | 3T |DisjT ::= | DisjT DisjT2. Note follows, let range propositional variables L, C, rangesets literals, clauses, terms, respectively.78fiPrime Implicates Prime Implicants Modal Logicdefinition satisfies P1 P4-P6 (satisfaction P5 shown Enjalbert & Farinasdel Cerro, 1989). satisfy P3 clauses disjunctionsliterals take example 2(a b). Given definition D3a strictly expressivedefinitions D1 D2, follows entailment clauses terms mustNP-hard, means D3a satisfy P7. fact, show entailmentclauses terms definition D3a Pspace-complete. so, modifypolynomial translation QBF K used prove Pspace-hardness Ktranslated formula conjunction clauses respect D3a. noticeformula unsatisfiable 3 entails 3(a a). thus reduce QBFvalidity entailment clauses, making task Pspace-hard, hence (beingsubproblem entailment K) Pspace-complete. idea used show Pspacecompleteness definitions D3b D5 below.instead extend D3 enforce property P3, obtain following definition:L ::= | | 2C | 3ConjCC ::= | | 2C | 3ConjC | C CConjC ::= C | ConjC ConjC::= L |definition satisfies properties except P2, P4, P7. Property P4 failshold negation literal 3(a b) equivalent literal. proofP5 holds constructive: use standard logical equivalences rewrite formulaeequivalent conjunctions clauses disjunctions terms (this alsodefinitions D4 D5 below).consider two rather simple definitions satisfy properties P3, P4, P5.first definition, inspired notion modal atom proposed GiunchigliaSebastiani (1996), defines literals set formulae NNF cannot decomposed propositionally.D3bL ::= | | 2F | 3FC ::= L | C C::= L |F ::= | | F F | F F | 2F | 3FD4 satisfies properties except P2 P7. P7, note arbitrary formula NNF unsatisfiable (a Pspace-complete problem) 3 |= 3(a a).Definition D4 liberal, imposing structure formulae behind modal operators. define literals formulae NNF cannot decomposed modally(instead propositionally), obtain much stricter definition satisfies exactlyproperties D4.D4D5L ::= | | 2C | 3TC ::= L | C C::= L |summary analysis different definitions respect properties P1-P7provided following table.Theorem 6. results Figure 1 hold.79fiBienvenuP1P2P3P4P5P6P7D1D2yesyesyesyesyesyesyesyesyes(unless P=NP)D3a D3b D4 D5yesyesyes yesyesyes yesyesyes yesyesyesyes yesyesyesyes yes(unless P=Pspace)Figure 1: Properties different definitions literals, clauses, terms.Clearly deciding different candidate definitions complicated counting number properties definitions satisfy, simple reasonproperties important others. Take instance property P5requires clauses terms expressive enough represent formulae K.use standard propositional definition clauses terms (thereby disregardingmodal operators), find satisfies every property except P5, henceproperties definitions considered section, yet wouldhard-pressed find someone considers propositional definition appropriate definition K. demonstrates expressiveness particularly important property,important fact willing sacrifice properties P2 P7 keep it.Among definitions satisfy P5, prefer definitions D4 D5 definitions D3aD3b, latter definitions less common propositional definitionpresent advantages D4 D5.course, comes it, choice definition must dependparticular application mind. may well circumstances lessexpressive less elegant definition may prove suitable. paperusing clauses terms define prime implicates prime implicants, usimportant criteria choosing definition quality notions primeimplicates prime implicants definition induces.4. Prime Implicates/Implicants Kdefinition clauses terms fixed, define prime implicatesprime implicants exactly manner propositional logic:Definition 7. clause implicate formula |= . primeimplicate if:1. implicate2. implicate |= , |=Definition 8. term implicant formula |= . primeimplicant if:1. implicant80fiPrime Implicates Prime Implicants Modal Logic2. implicant |= , |=course, notion prime implicate (resp. implicant) get determined definition clause (resp. term) chosen. compare different definitions using following well-known properties prime implicates/implicantspropositional logic:Finiteness number prime implicates (resp. prime implicants) formula finitemodulo logical equivalence.Covering Every implicate formula entailed prime implicate formula.Conversely, every implicant formula entails prime implicant formula.Equivalence model model model primeimplicates model prime implicant 3 .Implicant-Implicate Duality Every prime implicant formula equivalentnegation prime implicate negated formula. Conversely, every primeimplicate formula equivalent negation prime implicant negatedformula.Distribution prime implicate 1 ... n , exist prime implicates1 , ..., n 1 , ..., n 1 ... n . Likewise, prime implicant1 ... n , exist prime implicants 1 , ..., n 1 , ..., n1 ... nFiniteness ensures prime implicates/implicants formula finitelyrepresented, Covering means prime implicates provide complete representationformulas implicates. Equivalence guarantees information lost replacing formula prime implicates/implicants, whereas Implicant-Implicate Dualityallows us transfer results algorithms prime implicates prime implicants,vice-versa. Finally, Distribution relates prime implicates/implicants formulaprime implicates/implicants sub-formulae. property play key roleprime implicate generation algorithm presented next section.show definition D4 satisfies five properties. Finiteness Covering, first demonstrate every implicate formula entailed implicatevar( ) var() depth () + 1 (and similarly implicants). finitely many non-equivalent formulae finite languagebounded depth, follows finitely many prime implicates/implicantsgiven formula, infinite chains increasingly stronger implicates (or increasingly weaker implicants). Equivalence follows directly Coveringproperty P5 previous section: use P5 rewrite conjunctionclauses, implied prime implicate Covering.property Implicant-Implicate Duality immediate consequence duality3. property Equivalence commonly taken mean formula equivalent conjunction prime implicates disjunction prime implicants. chosen model-theoreticformulation order allow possibility set prime implicates/implicants infinite.81fiBienvenuclauses terms (P4). Distribution shown using Covering plusfact disjunction clauses clause conjunction terms term (P3).Theorem 9. notions prime implicates prime implicants induced definitionD4 satisfy Finiteness, Covering, Equivalence, Implicant-Implicate Duality,Distribution.remark way contrast first-order logic notion prime implicate induced standard definition clauses shown falsify Finiteness, Covering,Equivalence (Marquis, 1991a, 1991b).show definition D4 one definitions satisfy fiveproperties. definitions D1 D2, show Equivalence hold.fairly straightforward consequence fact definitions satisfyproperty P5.Theorem 10. notions prime implicates prime implicants induced definitionsD1 D2 satisfy Equivalence.notions prime implicates induced definitions D3a, D3b, D5, showappendix clause 2 3k 3(a b 2k a) prime implicate 2(a b)every k 14 . thereby demonstrate definitions admit formulaeinfinitely many distinct prime implicates also allow seemingly irrelevantclauses counted prime implicates. gives us strong grounds dismissingdefinitions much utility prime implicates applications comes abilityeliminate irrelevant consequences.Theorem 11. notions prime implicates prime implicants induced D3a, D3b,D5 falsify Finiteness.comparison last section suggested D5 least suitableD4 definition clauses terms, results section rule D5 suitabledefinition prime implicates prime implicants. remainder paper,concentrate attention notions prime implicates prime implicants induceddefinition D4, shown satisfactory generalizationspropositional case.5. Prime Implicate Generation Recognitionsection, investigate computational aspects modal prime implicates.primarily focusing notion prime implicate induced definition D4,remainder paper use words clause, term, prime implicatemean clause, term, prime implicate respect definition D4, exceptexplicitly stated otherwise.remark that, without loss generality, restrict attention prime implicates since Implicant-Implicate Duality (Theorem 9) algorithm generatingrecognizing prime implicates easily adapted algorithm generatingrecognizing prime implicants.4. D4, prime implicate 2(a b) itself.82fiPrime Implicates Prime Implicants Modal LogicFunction Dnf -4()Input: formulaOutput: set D4-termsReturn set terms output Iter-Dnf-4({Nnf ()}).Function Iter-Dnf -4(S)Input: set formulae NNFOutput: set D4-terms, output one-by-one= { } , Iter-Dnf -4(S {} {})Else = { } , Iter-Dnf -4(S {}), Iter-Dnf -4(S {})VElse output consistent (and nothing otherwise)Figure 2: Helper functions Dnf -4 Iter-Dnf -4.5.1 Generating Prime Implicatesstart considering problem generating set prime implicates givenformula. task important want produce abductive explanations, wantcompile formula set prime implicates.generation algorithm, require means transforming input formulaequivalent disjunction simpler formulae. end, introduce Figure 2helper function Dnf -4() returns set satisfiable terms respect D4whose disjunction equivalent . function Dnf -4 defined terms anotherfunction Iter-Dnf -4 takes input set formulae NNF returnsiterative fashion set satisfiable terms whose disjunction equivalent S. followinglemmas highlight important properties functions.Lemma 12. Iter-Dnf-4 terminates requires polynomial space sizeinput.Lemma 13. output Dnf-4 input set satisfiable terms respectD4 whose disjunction equivalent .Lemma 14. 2|| terms Dnf-4(). terms length2||, depth (), contains propositional variables appearingvar().present Figure 3 algorithm GenPI computes set prime implicatesgiven formula. algorithm works follows: Step 1, check whetherunsatisfiable, outputting contradictory clause case. satisfiable ,set equal set satisfiable terms whose disjunction equivalent .Distribution, know every prime implicate equivalent disjunctionprime implicates terms . Step 2, set (T ) equal propositional83fiBienvenuFunction GenPI()Input: formulaOutput: set clauses(1) unsatisfiable, return {3(a a)}. Otherwise, set = Dnf -4().(2) : let LT set propositional literals let DTset formulae 3 . literalsform 2 , set (T ) = LT {3 | DT }. Otherwise, set (T ) =LT {2T } {3( ) | DT }conjunctionformulae 2 .W(3) Set Candidates = { | (T )}.(4) j Candidates: remove j Candidates k |= jk Candidates k < j, j |= k j 6|= k k > j.(5) Return Candidates.Figure 3: Algorithm prime implicate generation.literals (LT ) plus strongest 2-literal implied (2T ) plus strongest 3literals implied ({3( ) | DT }). hard see every primeimplicate must equivalent one elements (T ). meansStep 3 guaranteed every prime implicate input formula equivalentcandidate prime implicate Candidates. comparison phase Step 4,non-prime candidates eliminated, exactly one prime implicate equivalenceclass retained.illustrate behavior GenPI example:Example 15. run GenPI input = ((3(b c) 3b) (3b 3(c d) 2e 2f )).Step 1: satisfiable, call function Dnf -4 , returns two termsT1 = 3(b c) 3b T2 = 3b 3(c d) 2e 2f .Step 2: LT1 = {a}, DT1 = {b c, b}, 2-literals T1 ,get (T1 ) = {a, 3(b c), 3b}. T2 , LT2 = {a}, DT2 = {b, c d},T2 = e f , giving us (T2 ) = {a, 2(e f ), 3(b e f ), 3((c d) e f )}.Step 3: set Candidates contain different possible disjunctions elements(T1 ) elements (T2 ), 12: aa, a2(ef ), a3(bef ),3((c d) e f ), 3(b c) a, 3(b c) 2(e f ), 3(b c) 3(b e f ),3(bc)3((cd)ef ), 3ba, 3b2(ef ), 3b3(bef ), 3b3((cd)ef ).Step 4: remove Candidates clauses 2(e f ), 3(b e f ),3((c d) e f ), 3(b c) a, 3b since strictly weakera. also eliminate clauses 3b 2(e f ), 3b 3(b e f ),84fiPrime Implicates Prime Implicants Modal Logic3b 3((c d) e f ) since weaker clauses 3(b c) 2(e f ),3(b c) 3(b e f ), 3(b c) 3((c d) e f ).Step 5: GenPI return four remaining clauses Candidates, a,3(b c) 2(e f ), 3(b c) 3(b e f ), 3(b c) 3((c d) e f ).algorithm shown sound complete procedure generating primeimplicates.Theorem 16. algorithm GenPI always terminates outputs exactly setprime implicates input formula.examining prime implicates produced algorithm, place upperbound length formulas prime implicates.Theorem 17. length smallest clausal representation prime implicateformula single exponential length formula.upper bound optimal find formulae exponentially large primeimplicates. situation contrasts propositional logic, length primeimplicates linearly bounded number propositional variables formula.Theorem 18. length smallest clausal representation prime implicateformula exponential length formula.interesting note formula used proof Theorem 18 depth1, means cannot avoid worst-case spatial complexity restrictingattention formulae shallow depth. escape exponential worst-casespatial complexity dropping one less expressive notions prime implicatesexamined previous section, following theorem attests.Theorem 19. prime implicates defined using either D1 D2, lengthsmallest clausal representation prime implicate formula exponentiallength formula.examination set candidate prime implicates constructed algorithmallows us place bound maximal number non-equivalent prime implicatesformula possess.Theorem 20. number non-equivalent prime implicates formuladouble exponential length formula.bound also shown optimal. situation contrasts propositionallogic, single exponentially many non-equivalent prime implicatesgiven formula.Theorem 21. number non-equivalent prime implicates formula may doubleexponential length formula.Again, worst-case result robust improved neither restricting depth formulae, using less expressive notions prime implicate,following theorem demonstrates.85fiBienvenuTheorem 22. prime implicates defined using either D1 D2, numbernon-equivalent prime implicates formula may double exponential lengthformula.Theorems 19 22 together suggest definitions D1 D2 yield especially interesting approximate notions prime implicate, induce significant lossexpressivity without improvement size number prime implicatesworst-case.generation algorithm GenPI corresponds simplest possible implementationdistribution property, quite clear represent practicableway producing prime implicates. One major source inefficiency large numberclauses generated, want design efficient algorithm, needfind ways generate fewer candidate clauses. couple different techniquescould used. One simple method could yield smaller number clauseseliminate (T ) elements prime implicates , therebydecreasing cardinalities (T ) hence Candidates. this, simplytest whether tautology (and remove is) compare 3-literals(T ), discarding weaker elements. apply technique Example 15, wouldremove 3b (T1 ), thereby reducing cardinality Candidates 12 8.substantial savings could achieved using technique developed framework propositional logic (cf. Marquis, 2000) consists calculating prime implicates T1 , prime implicates T1 T2 , T1 T2 T3 ,get prime implicates full disjunction terms. interleaving comparisonconstruction, eliminate early partial clause cannot give rise primeimplicates instead producing extensions partial clause deletingone one comparison phase. example, two terms,imagine third term T3 . applying technique, wouldfirst produce 4 prime implicates T1 T2 would compare 4|(T3 )|candidate clauses T1 T2 T3 . Compare current algorithm generatescompares 12|(T3 )| candidate clauses.Given number elements Candidates double exponentiallength input, cutting size input GenPI could yield significantsavings. One obvious idea would break conjunctions formulae conjuncts,calculate prime implicates conjuncts. Unfortunately, however,cannot apply method every formula prime implicates conjunctsnecessarily prime implicates full conjunction. One solution proposedcontext approximation description logic concepts (cf. Brandt & Turhan, 2002)identify simple syntactic conditions guarantee get resultbreak formula conjuncts. instance, one possible conditionconjuncts share propositional variables. formula example satisfiescondition since variables ((3(b c) 3b) (3b 3(c d) 2e 2f ))disjoint. generating prime implicates conjuncts separately, directlyidentify prime implicate a, 6 candidate clauses ((3(b c) 3b)(3b 3(c d) 2e 2f )) compare. also remove weaker elements (Ti )86fiPrime Implicates Prime Implicants Modal Logicsuggested above, get 3 candidate clauses ((3(bc)3b)(3b3(cd)2e2f )),prime implicates .Another important source inefficiency algorithm comparison phasecompare candidate clauses one-by-one order identify strongest ones.problem course worst-case double exponentialnumber candidate clauses, simply may double exponentially manydistinct prime implicates, prime implicate must equivalent candidateclause. Keeping double exponentially many clauses memory generallyfeasible. Fortunately, however, necessary keep candidate clausesmemory since generate demand sets (T ). Indeed,demonstrate appendix, implementing algorithm clever fashion,obtain algorithm outputs prime implicates iteratively requiringsingle-exponential space (the output algorithm could course double exponentiallylarge Theorem 21).Theorem 23. exists algorithm runs single-exponential space sizeinput incrementally outputs, without duplicates, set prime implicatesinput formula.Although modified algorithm much better spatial complexity original,still yield practicable means generating prime implicates. reasonstill need compare candidate clauses candidate clausesorder decide whether candidate prime implicate not. Given setcandidate clauses may double exponential number, means algorithm mayneed perform double exponentially many entailment tests producing even singleprime implicate. much promising approach would test directly whethercandidate clause prime implicate without considering candidate clauses.order implement approach, must course come proceduredetermining whether given clause prime implicate. objectivefollowing section.5.2 Recognizing Prime Implicatesfocus section problem recognizing prime implicates, is, problemdeciding whether clause prime implicate formula . discussedprevious subsection, problem central importance, algorithmgenerating prime implicates must contain (implicitly explicitly) mechanismensuring generated clauses indeed prime implicates.propositional logic, prime implicate recognition BH2 -complete (Marquis, 2000),hard satisfiability deduction. K, satisfiability unsatisfiabilityPspace-complete, cannot hope find prime implicate recognition algorithmcomplexity less Pspace.Theorem 24. Prime implicate recognition Pspace-hard.order obtain first upper bound, exploit Theorem 17 tells usexists polynomial function f every prime implicate formula87fiBienvenuequivalent clauses length 2f (||) . leads simple proceduredetermining clause prime implicate formula . simply check everyclause length 2f (||) whether implicate impliesimplied . case, prime implicate (we found logicallystronger implicate ), otherwise, exists stronger implicate, primeimplicate. hard see algorithm carried exponentialspace, gives us Expspace upper bound.course, problem naive approach takeaccount structure , end comparing huge amount irrelevant clauses,exactly hoping avoid. algorithm propose latersection avoids problem exploiting information input formula clauseorder cut number clauses test. key algorithmfollowing theorem shows general problem prime implicate recognitionreduced specialized tasks prime implicate recognition propositionalformulae, 2-formulae, 3-formulae. simplify presentation theorem,let () refer set prime implicates , use notation \ {l1 , ..., ln }refer clause obtained removing literals li . example(a b 3c) \ {a, 3c} refers clause b.Theorem 25. Let formula K, let = 1 ... k 31 ... 3n 21 ...2m (j propositional literals) non-tautologous clause (a) 1 ...ni, (b) literal l \ {l}. ()following conditions hold:1. 1 ... k ( ( \ {1 , ..., k }))2. 2(i 1 ... n ) ( ( \ {2i })) every3. 3(1 ... n ) ( ( \ {31 , ..., 3n }))remark restriction clauses 1 ...6 \ {l} l required. drop first requirement,non-prime implicates satisfy three conditions, drop second,prime implicates fail satisfy one conditions5 . restrictionswithout loss generality however since every clause transformed equivalentclause satisfying them. first condition, replace 2i 2(i 1 ... ),thereby transforming clause 1 ... k 31 ...3m 21 ... 2n equivalent1 ... k 31 ...3m 2(1 1 ... ) ... 2(n 1 ... ). makeclause satisfy second condition, simply remove literals\ {l} literal remains.Theorem 25 shows prime implicate recognition split three specialized sub-tasks, tell us carry tasks. Thus, order turn5. first restriction, consider formula = 3(abc)2a clause = 3(ab)2(ab).easily shown implicate , prime implicate since existstronger implicates (e.g. itself). Nonetheless, verified 2(a b (a b))( ( \ {2(a b)})) 3(a b) ( ( \ {3(a b)})). second restriction, considerformula 2a clause 2a2(ab). 2(ab) 6 (2a(2a)) even though 2a2(ab) 2aprime implicate 2a.88fiPrime Implicates Prime Implicants Modal Logictheorem algorithm prime implicate recognition, need figuretest whether propositional clause, 2-formula, 3-formula prime implicateformula.Determining whether propositional clause prime implicate formula Kconceptually difficult determining whether propositional clause primeimplicate propositional formula. first ensure clause implicateformula make sure literals appearing clause necessary.Theorem 26. Let formula K, let non-tautologous propositional clause|= literal l \ {l}. ()6|= \ {l} l .move problem deciding whether clause form 2 primeimplicate formula . remark 2 implied , must also impliedterms Ti Dnf -4(). Ti |= 2, Theorem 1, mustcase conjunction 2-literals Ti implies 2. means formula21 ... 2n (where conjunction formulae 2 Ti )implicate implies 2, moreover strongest implicate. follows2 prime implicate case 2 |= 21 ... 2n ,true |= (by Theorem 1). Thus, comparing formulaformulae associated terms , decide whether 2prime implicate .Theorem 27. Let formula K, let = 2 non-tautologous clause|= . () exists term Dnf-4()|= , conjunction formulae 2 .Finally let us turn problem deciding whether clause 3 prime implicateformula . know Covering 3 implicate , mustprime implicate implies 3. follows Theorem 2 mustW disjunction 3-literals, Theorem 16 equivalent disjunctionDnf -4() 3dT 3dT element (T ) every (refer back Figure 3definition (TW)). According Definition 7, 3 aWprime implicatecase 3 |= Dnf -4() 3dT , equivalently |= Dnf -4() dT . Thus, 3prime implicate Wcase isWa choice 3dT (T )6|= Dnf -4() dT .Dnf -4() Dnf -4() dT |= WTesting directly whether entails formula Dnf -4() dT could take exponentialspace worst case since may exponentially many terms Dnf -4(). Luckily,W however, get around problem exploiting structure formulaDnf -4() dT . remark way (T ) defined formula dT mustconjunction formulae 2 3 appears Nnf () outside scopemodal operators use X Wdenote set formulae satisfying condition.show appendix6|= Dnf -4() dT implies existence subset XW(a) 6|= (b) every dT least Wone conjunct set S. Conversely,existence subset X implies 6|= Dnf -4() dT . observationbasis algorithm Test3PI given Figure 4. basic idea behind algorithmtry different subsets X order see whether subset satisfies89fiBienvenuFunction Test3PI(3, )Input: clause 3 formula |= 3Output: yes(1) |= , return yes |= otherwise.(2) Set X equal set formulae 2 3 appears Nnf () outsidescope modal operators.(3) WX , test whether following two conditions hold:(a) 6|=(b) Ti Dnf -4(), exists conjuncts 3i , 2i,1 , ..., 2i,k(i) Tithat:(i) {i , i,1 , ..., i,k(i) } 6=(ii) 3(i i,1 ... i,k(i) ) |= 3Return satisfies conditions, yes otherwise.Figure 4: Algorithm identifying prime implicates form 3.aforementioned conditions. find suitable subset, proves 3 primeimplicate, subset exists, sure stronger implicate3. algorithm shown run polynomial space since|| elements X , consider terms Dnf -4() one time.Theorem 28. Let formula, let 3 implicate . algorithmTest3PI returns yes input (3, ) 3 prime implicate .Theorem 29. algorithm Test3PI runs polynomial space.illustrate algorithm Test3PI two examples.Example 30. use Test3PI test whether clause = 3(ab) prime implicate= (2(b c) 2(e f )) 3(a b).Step 1: satisfiable, pass directly Step 2.Step 2: set X equal set formulae 2 3 appears Nnf ()outside scope modal operators. case, set X = {b c, e f, b}since =Nnf () b c, e f , b formulae satisfyingrequirements.Step 3: examine different subsets X determine whether satisfyconditions (a) (b). particular, consider subset = {b c, e f }.remark subset satisfies condition (a) since b 6|= (b c) (e f ). ordercheck condition (b), first call function Dnf -4 returns twoterms T1 = 2(b c) 3(a b) T2 = 2(e f ) 3(a b). noticeconjuncts 3(a b) 2(b c) T1 satisfy conditions (i) (ii) since b c3(a b (b c)) |= . notice conjuncts 3(a b) 2(e f )T2 also satisfy conditions (i) (ii) since e f 3(a b (e f )) |= .90fiPrime Implicates Prime Implicants Modal LogicFunction TestPI(, )Input: clause formulaOutput: yes(1) 6|= , return no.(2) |= , return yes |= not.|= , return yes |= otherwise.(3) li = l1 ... ln , test \ {li } , so, remove li .Let = {31 , ..., 3m } set 3-literals . non-empty, replacedisjunct 2 literal 2( 1 ... ).(4) Let P set propositional literals disjuncts .l P, check whether |= \ {l}, return so.(5) Let B set 2-formulae appearing disjuncts . Check 2B whether Dnf -4( ( \ {2})) formula2( 1 ... k ) implies conjunction 2-literals , returnnot.W(6) empty, return yes, otherwise return Test3PI(3(i=1 ), ( \ D)).Figure 5: Algorithm identifying prime implicates.means found subset X satisfies conditions (a) (b),algorithm returns no. correct output since 3(a b ((b c) (e f )))implicate strictly stronger .Example 31. use Test3PI test whether clause = 3(a b c) primeimplicate = (2(b c) 2(e f )) 3(a b) 2(e f (a b c)).Step 1: proceed directly Step 2 since satisfiable.Step 2: set X = {b c, e f, b, e f (a b c))} since Nnf ()=a (2(bc) 2(e f )) 3(a b) 3(e f (a b c)).Step 3: check whether subset X satisfying conditions (a) (b).claim subset. see why, notice 2(b c) 3(a b)3(e f (a b c)) term Dnf -4(). Moreover, oneset conjuncts term implies 3(a b c), namely {3(a b), 2(b c)}.means must contain either b b c order satisfy condition(b)(i). b c implies b b c, guaranteed b cimply disjunction elements S, thereby falsifying condition (a). followssubset X satisfying necessary conditions, Test3PI returnsyes, desired result.Figure 5, present algorithm testing whether clause prime implicateformula . first two steps algorithm treat limit casesimplicate one tautology contradiction. Step 3,91fiBienvenuapply equivalence-preserving transformations make satisfy requirementsTheorem 25. Steps 4, 5, 6 use procedures Theorems 26, 27,28 test whether three conditions Theorem 25 verified. three testssucceed, Theorem 25, clause prime implicate, return yes.test fails, return clause shown prime implicate.Theorem 32. algorithm TestPI always terminates, returns yes input (,) prime implicate .demonstrate use TestPI example.Example 33. use TestPI test clauses 1 = b, 2 = 2b 2(e f ), 3 = 3c,4 = 3(a b), 5 = 3(a b c) 3(a b c f ) 2(e f ) prime implicates= (2(b c) 2(e f )) 3(a b).1 : output Step 1 since 6|= 1 .2 : skip Steps 1 2 since |= 2 neither |= |= 2 . Step 3,make changes 2 since contains redundant literals 3-literals.skip Step 4 since 2 propositional disjuncts. Step 5, return sinceDnf -4( (2 \ {2b})) = {a 2(b c) 3(a b) 3(e f )} 2b 6|= 2(b c).3 : proceed directly Step 3 since |= 3 , 6|= , 6|= 3 . modificationsmade 3 Step 3 contain redundant literals 2-literals.Step 4, test whether |= 3 \ {a}. 6|= 3c, proceed Step5, directly Step 6 since 3 contains 2-literals. Step 6, callTest3PI(3c, (3 \ {3c})), outputs since (3 \ {3c}) |=c 6|= .4 : Steps 1-5 inapplicable, skip directly Step 6. step, callTest3PI input clause 3(a b) formula (4 \{3(a b)}) = .already seen Example 30 Test3PI returns input,means TestPI also returns no.5 : proceed directly Step 3, delete redundant literal 3(a b c f )modify literal 2(ef ). end step, 5 = 3(abc)2((ef )(abc)). Step 4 applicable since propositional disjuncts5 . Step 5, continue since Dnf -4( (5 \ {2((e f (a b c))})) ={a2(ef )3(ab)2(abc)}, 2(((ef (abc))(abc)) |=2(e f ) 2(a b c). Step 6, return yes since call Test3PI input(3(a b c), (5 \ {3(a b c)})), previously shown Example31 Test3PI returns yes input.show appendix algorithm TestPI runs polynomial space.already shown TestPI decides prime implicate recognition, followsproblem Pspace:Theorem 34. Prime implicate recognition Pspace.92fiPrime Implicates Prime Implicants Modal Logicputting together Theorems 24 34, obtain tight complexity boundprime implicate recognition task.Corollary 35. Prime implicate recognition Pspace-complete.6. Conclusion Future Workfirst contribution work detailed comparison several different possibledefinitions clauses, terms, prime implicates, prime implicants modal logic K.results investigation largely positive: although shownperfect definition exists, exhibit simple definition (D4) satisfiesdesirable properties propositional case. second contribution workthorough investigation computational aspects selected definition D4.end, presented sound complete algorithm generating prime implicates, wellnumber optimizations improve efficiency algorithm. examinationstructure prime implicates generated algorithm allowed us place upperbounds length prime implicates number prime implicates formulapossess. showed bounds optimal exhibiting matching lower bounds,proved lower bounds hold even much less expressive notions prime implicates. Finally, constructed polynomial-space algorithm decidingprime implicate recognition, thereby showing problem Pspace-complete,lowest complexity could reasonably expected. Although focus paperlogic K, results easily lifted multi-modal K conceptexpressions well-known description logic ALC.mentioned introduction, one main applications prime implicantspropositional logic area abductive reasoning, prime implicants playrole abductive explanations. results paper directly appliedproblem abduction K: notion prime implicants used definitionabductive explanations K, prime implicate generation algorithm provides meansproducing abductive explanations given abduction problem. Moreover,notion term underlying definition abductive explanationsexpressive used Cialdea Mayer Pirri (1995), able find explanationsoverlooked method. instance, look explanationobservation c given background information 2(a b) c, obtain 2(a b), whereasframework yields 2a 2b. argument favor approach sincegenerally abduction one looking find weakest conditions guaranteeing truthobservation given background information.Also interest results size number prime implicates,yield corresponding lower bounds size number abductive explanations.particular, results imply abductive explanations Cialdea Mayer Pirri(1995) exponential size double exponentially many number worstcase, thus behave better respects notion abductive explanationinduced preferred definition D4. Moreover, fact lower bounds holdeven case extremely inexpressive notion abductive explanations induceddefinition D2 suggests high worst-case complexity results really cannot93fiBienvenuavoided. light intractability results, interesting question future researchwould study problem generating single prime implicate, since applications may prove sufficient produce single minimal explanation observation.Another interesting subject future work relevant point view abduction investigation notion prime implicate fixed vocabulary.development generation algorithms refined notion prime implicate wouldallow one generate abductive explanations built given setpropositional variables.second domain application mentioned introductionarea knowledge compilation. propositional logic, one well-known target languageknowledge compilation prime implicate normal form, formula representedconjunction prime implicates. natural idea would use selecteddefinition prime implicate define analogous manner notion prime implicatenormal form K formulae. Unfortunately, normal form obtain satisfiesnice properties propositional case. instance, find entailment twoformulae prime implicate normal form easier arbitrary K formulae.see why, consider pair formulae negation normal form. formulae3 3 prime implicates hence prime implicate normal formaccording naive definition. |= case 3 |= 3, reduceentailment arbitrary K formulae NNF entailment formulae primeimplicate normal form. former problem known Pspace-complete, followslatter Pspace-complete well.first sight, appears quite disappointing result one would hopecomputational difficulty representing formula prime implicates would offsetgood computational properties resulting formula. turns out, however,problem lies definition prime implicates rather naive waydefining prime implicate normal form. Indeed, continuation present work (Bienvenu, 2008), proposed sophisticated definition prime implicate normal form,specify many different clausal representations prime implicateused. normal form shown enjoy number desirable propertiesmake interesting viewpoint knowledge compilation. notably,proven entailment formulae K prime implicate normal formcarried polynomial time using simple structural comparison algorithmreminiscent structural subsumption algorithms used subpropositional descriptionlogics. noted proof results Bienvenu (2008) makeample use material presented current paper.work, studied prime implicates respect local consequence relation,natural direction future work would investigation prime implicates respect global consequence relation. question particularly interesting givenglobal consequence type consequence used description logic ontologies. Unfortunately, preliminary investigations suggest defining generating prime implicatesrespect global consequence relation likely prove difficultlocal consequence relation. one thing, use definition clause reasonably94fiPrime Implicates Prime Implicants Modal Logicexpressive, notion prime implicate obtain satisfy Covering sinceconstruct infinite sequences stronger stronger implicates. Take instanceformula (a b) (b 3b) implies (using global consequence relation)increasingly stronger clauses infinite sequence 3b, 3(b 3b),3(b 3(b 3b)), ... familiar situation description logic practitioners sinceinfinite sequences responsible non-existence specific conceptsmany common DLs (cf. Kusters & Molitor, 2002) lack uniform interpolationALC TBoxes (Ghilardi, Lutz, & Wolter, 2006). standard solution problemsimply place bound depth formulae considered, effectively blockingproblematic infinite sequences. allow us regain Covering, giveus weaker version property, sufficient applications.development generation algorithms global consequence relation may also provechallenging, since unclear point whether able draw inspirationpre-existing methods. Despite potential difficulties, feel subject worthexploring since could contribute development flexible ways accessingstructuring information description logic ontologies.Finally, another natural direction future research would extend investigation prime implicates prime implicants popular modal description logics.Particularly interest modal logics knowledge belief expressive descriptionlogics used semantic web. confident experience gainedinvestigation prime implicates prime implicants K prove valuable assetexploration modal description logics.Acknowledgmentspaper corrects significantly extends earlier conference publication (Bienvenu,2007). paper written author PhD student working IRIT,Universite Paul Sabatier, France. author would like thank thesis supervisorsAndreas Herzig, Jerome Lang, Jerome Mengin, well anonymous reviewershelpful feedback.Appendix A. ProofsTheorem 1 Let , 1 , ..., , , 1 , ..., n formulae K, let propositionalformula.1. |= |= |=2. |= 3 |= 3 2 |= 23. 31 ...3m 21 ...2n |= ( |= 1 ...n |= i)4. |= 31 ... 3m 21 ... 2n (|= |= 1 ... i)5. 2 |= 21 ... 2n |=95fiBienvenu6. 31 ... 3m 21 ... 2n31 ... 3m 2(1 1 ... ) ... 2(n 1 ... )Proof. first statement well-known property local consequence, provecompleteness:|=M, w |= implies M, w |= M, wM, w 6|= M, w |= M, wM, w |= M, w |= M, w|=M, w 6|= M, w|=second statement, 6|= , M, w M, w |= .Create new model adding new world w placing single arcw w. , w |= 3 2, means 3 2 satisfiable hence3 6|= 3 (since 2 3). direction, suppose 3 6|= 3.exists M, w M, w |= 3 3 3 2. meansw , hence 6|= . complete proof, use following chainequivalences: 2 |= 2 2 |= 2 3 |= 3 |= |= .3, suppose 31 ... 3m 21 ... 2n 6|= . exist M, wM, w |= 31 ...3m 21 ... 2n . M, w |= , cannot|= , 1 ... n |= since wM, w |= 1 ... n . direction suppose1 ... n satisfiable. propositional model w ,i, find Mi , wi Mi , wi |= 1 ... n . construct newKripke structure contains models Mi world w arcsgoing w wi . hard see new model MnewMnew , w |= 31 ...3m 21 ...2n , 31 ...3m 21 ... 2n 6|= .Statement 4 follows easily third statement. simply notice 31... 3m 21 ... 2n tautology case negation 31... 3n 21 ... 2m unsatisfiable.5, use statements 1 4 get following chain equivalences:2 |= 21 ... 2n|= 3 21 ... 2n|=|=first implication equivalence 6 immediate since 31 ... 3m |=31 ... 3m 2i |= 2(i 1 ... ) i. direction, remarkusing statements 1 3, get following equivalences:2(i 1 ... ) |= 2i 31 ... 3m2(i 1 ... ) (2i 31 ... 3m ) |=2(i 1 ... ) 3i 21 ... 2m |=(i 1 ... ) 1 ... |=96fiPrime Implicates Prime Implicants Modal Logic(i 1 ... ) 1 ... clearly unsatisfiable, follows2(i 1 ... ) |= 2i 31 ... 3m every hence 31 ... 3m2(1 1 ... ) ... 2(n 1 ... ) |= 31 ... 3m 21 ... 2n ,completing proof.Theorem 2 Let disjunction propositional literals 2- 3-formulae.following statements holds:1. |= non-tautological propositional clause , every disjuncteither propositional literal unsatisfiable 3-formula2. |= 31 ... 3n , every disjunct 3-formula3. |= 21 ... 2n 6|= 21 ... 2n , every disjunct either2-formula unsatisfiable 3-formulaProof. (1), let non-tautologous propositional clause |= , supposecontradiction contains disjunct 2 disjunct 3 6|= .first case, 2 |= , hence |= 3 . follows Theorem 1|= , contradicting assumption tautology. second case,3 |= , thus |= 2 . Theorem 1, either |= |= . cases,reach contradiction since assumed 6|= 6|= . followscannot 2-formulae satisfiable 3-formulae disjuncts.proofs (2) (3) proceed similarly.Theorem 3 Let = 31 ... 3m 21 ... 2n = 31 ... 3p21 ... 2q formulae K. propositional 6|= ,|=1 ... |= 1 ... p|=every j |= 1 ... p jProof. Since 6|= , know 6|= 6|= 1 ... p i. Usinginformation together Theorem 1, get following equivalences:|=31 ... 3m |=2i |=|= 31 ... 3p 21 ... 2m|=|=3(1 ... ) |=|= 31 ... 3p 2(1 ... ) 21 ... 2q|= 1 ... p (1 ... )1 ... |= 1 ... p|= 3(1 ... p ) 21 ... 2qj |= 1 ... p jj |= 1 ... p jcomplete proof, use fact |= |= , 31 ... 3m |= ,2i |= every i.97fiBienvenuTheorem 5 definition literals, clause, terms K satisfies properties P1P2 cannot satisfy P5.Proof. remark set clauses (resp. terms) respect definition D1precisely set formulae NNF contain (resp. ), i.e. D1expressive definition satisfying P1 P2. Thus, show result, sufficesshow D1 satisfy P5.Suppose contradiction D1 satisfy P5. must exist clauses 1 , ..., n3(a b) 1 ... n . clauses disjunction li,1 .... li,pi .distributing , obtain following:n^_3(a b)li,ji(j1 ,...,jn){1,...,p1 }...{1,...,pn} i=1infer (j1 , ..., jn ) {1, ..., p1 } ... {1, ..., pn }n^li,ji |= 3(a b)i=1VnConsider (j1 , ..., jn ) i=1 li,ji consistent (there must least onetuple, otherwise would 3(a b) ). formulae li,ji eitherVpropositionalliterals formulae form 2 3 clause . follows ni=1 li,ji mustfollowing form:1 ... k 31 ... 3m 21 ... 2n1 , ..., k propositionalliterals 1 , ...,V , 1 , ..., n clauses respectVD1. know ni=1 li,ji |= 3(a b) ni=1 li,ji 6|= , Theorem 1, must3q3q 21 ... 2n |= 3(a b)show 3q 6|= 3(a b) (and hence 6|= 1 ... n ). Supposecontradiction case. must q |= q |= b.Theorem 1, every disjunct q (which recall D1-clause) must either unsatisfiableequal b. latterV impossible, follows q |= ,contradiction since assumed ni=1 li,ji satisfiable. follows orderget 3q 21 ... 2n |= 3(a b), must r tautology.let us consider formula_=2j1 ,...,jn{(j1 ,...,jn)|Vni=1 li,ji 6}V2j1 ,...,jn non-tautological 2-formula appearing ni=1 li,ji (we shownformula must exist). Clearly must casen^_(j1 ,...,jn){1,...,p1 }...{1,...,pn} i=198li,ji |=fiPrime Implicates Prime Implicants Modal Logicget:3(a b) |=according Theorem 2, satisfiable 3-formula cannot imply disjunction 2formulae unless disjunction tautology, must |= . However,impossible since would imply (Theorem 1) j1 ,...,jn tautology,contradicting earlier assumption contrary. thus concludeset clauses 1 , ..., n respect D1 3(a b) 1 ... n , hencedefinition satisfies P1 P2 cannot satisfy P5.order prove Theorem 6, make use following lemmas:Lemma 6.1 Definition D5 satisfies P5.Proof. demonstrate formula K NNF equivalent conjunctionclauses respect definition D5. restriction formulae NNF without lossgenerality every formula equivalent formula NNF. proof proceedsinduction structural complexity formulae. base case propositional literals,already conjunctions clauses since every propositional literal clauserespect D5. suppose statement holds formulae 1 2 showholds complex formulae.first consider = 1 2 . assumption, find clauses j1 1 ... n 2 1 ... . Thus, equivalent formula1 ... n 1 ... , conjunction clauses respect D5.Next consider = 1 2 . induction hypothesis, 1 1 ... n2 1 ... clauses j . Thus, (1 ... n ) (1 ... ),written equivalently (i,j){1,...,n}{1,...,m}(i j ). Since uniontwo clauses produces another clause, j clauses, completing proof.consider case = 21 . assumption, 1 1 ... n ,clause. 2(1 ...n ). also know 2(1 ...n ) 21 ...2n .follows equivalent 21 ... 2n , conjunction clauses since2i clauses.Finally, consider = 31 . Using induction hypothesis, 3(1... n ) clauses . since clauses, disjunction literalsli,1 ... li,pi . distributing 3, find equivalentformula_3(l1,j1 l2,j2 ... ln,jn )(j1 ,...,jn){1,...,p1 }...{1,...,pn}clause respect D5.proof every formula equivalent disjunction terms respect D5proceeds analogously.Lemma 6.2 Every clause (resp. term) respect D5 clause (resp. term)respect definitions D3a, D3b, D4.Proof. show induction structural complexity formulae that:99fiBienvenu1. every clause C respect D5 clause respect definitions D3a, D3b,D4 disjunction terms respect D3a2. every term respect D5 term respect definitions D3a, D3b,D4 conjunction clauses respect D3a D3brequire stronger formulation statement prove sub-cases.base case induction propositional literals, clausesterms respect D5. easy see (1) (2) verified since propositionalliterals clauses terms respect definitions D3a, D3b, D4 (andhence also disjunctions terms respect D3a conjunctions clausesrespect D3a D3b).induction step, show statements hold arbitrary clausesterms respect D5 assumption statments holdproper sub-clauses sub-terms.begin clauses. Let C D5-clause proper sub-clauses subterms C satisfy (1) (2). since C clause respect D5, eitherpropositional literal formula form C1 C2 clauses C1 C2 , 2C1clause C1 , 3T1 term T1 . case C propositional literalalready treated base case. Let us thus consider case C = C1 C2 .first part (1) holds since induction hypothesis C1 C2 clausesrespect definitions D3a, D3b, D4, three definitions disjunctiontwo clauses clause. second half (1) also verified since C1 C2disjunctions terms respect D3a, thus disjunction C1 C2 .next consider case C = 2C1 clause C1 respect D5. firstpart (1) follows easily know C1 must also clause respect D3a,D3b, D4, definitions putting 2 clause yields anotherclause. second part (1) holds well since C1 disjunction terms respectD3a thus 2C1 term respect definition. supposeC = 3T1 term T1 respect D5. definitions D3a D3b, knowinduction hypothesis T1 conjunction clauses respect D3aD3b hence 3T1 clause respect definitions. D4, resultobviously holds since allowed put formula NNF behind 3. second part(1) holds since induction hypothesis T1 term respect D3a hence3T1 also term respect definition.next consider terms. Let D5-term proper sub-clauses subterms satisfy (1) (2). must either propositional literal formulaform T1 T2 terms T1 T2 , 2C1 clause C1 , 3T1 term T1 .= T1 T2 , first half (2) holds since know T1 T2 terms respectD3a, D3b, D4, conjunctions terms also terms three definitions.second half also verified since T1 T2 assumed conjunctions clausesrespect D3a D3b, means also conjunction clausesrespect definitions. Next suppose = 2C1 . definitions D3b D4,easy see literal hence term. D3a, induction hypothesis tells usC1 disjunction terms, deduce 2C1 term. Moreover,since C1 known clause respect D3a D3b, 2C1 must also100fiPrime Implicates Prime Implicants Modal Logicclause respect definitions, conjunction clauses respectD3a D3b. Finally, treat case = 3T1 . D3a, use factT1 term respect D3a, means 3T1 must also term. D3b,use supposition T1 conjunction clauses respect D3b,get 3T1 literal hence term. first part (2) clearly also holds D4since formula behind 3 yields literal thus term. second half (2) followsfact induction hypothesis T1 conjunction clauses respectD3a D3b, 3T1 clause (and hence conjunction clauses) respectdefinitions.U ,S = 1,1 ... 1,mi,j defined inductively follows3i+1,j , either n, ui Sj , > n uin Sji,j =2i+1,j , either n, ui 6 Sj , > n uin 6 Sj{1, ..., 2n} 2n+1,j = , = 2...2| {z } .2nFigure 6: formula U ,S codes instance U = {u1 , ..., un }, = {S1 , ..., Sm }exact cover problem.Lemma 6.3 Entailment terms clauses NP-complete definitions D1D2.Proof. proofs NP-membership NP-hardness, exploit relationship terms respect definitions D1 D2 concept expressionsdescription logic ALE (cf. Baader, McGuiness, Nardi, & Patel-Schneider, 2003).recall concept expressions logic constructed follows (we use modal logicsyntax assume single modal operator order facilitate comparisonformalisms):::= | | | | | 2 | 3semantics symbols one would expect: M, w |= M, w 6|=every model world w. semantics atomic literals, conjunctions, universalexistential modalities exactly K.hard see every term respect D1 D2 concept expressionALE. entailment ALE expressions decidable nondeterministic polynomialtime (cf. Donini, Lenzerini, Nardi, Hollunder, Nutt, & Marchetti Spaccamela, 1992),follows deciding entailment terms respect either D1 D2 alsoaccomplished nondeterministic polynomial time, i.e. problems belong NP.remains shown problems NP-hard. prove this, showpolynomial-time reduction Donini (2003) (adapted original NP-hardnessproof Donini et al., 1992) NP-complete exact cover (XC) problem (Garey &101fiBienvenuJohnson, 1979) unsatisfiability ALE modified give polynomial-timereduction XC entailment terms respect D1 D2.exact cover problem following: given set U = {u1 , ..., un } set ={S1 , ..., Sm } subsets U, determine whether existsSqan exact cover, is, subset{Si1 , ..., Siq } Sih Sik = h 6= k k=1 Sik = U. Donini proven(2003) U, exact cover formula U ,S pictured Figure 6unsatisfiable. Notice U ,S term respect either D1 D2 usessymbols . would like find similar formula term respectdefinitions satisfiable U ,S is. Consider formulaU ,S = 1,1 ... 1,mi,j defined exactly like i,j except replacea. easy verify U ,S indeed term respect D1D2. Moreover, hard see 1,1 ... 1,m |= 32n1,1 ... 1,m |= 32n hence U ,S U ,S equisatisfiable. U,exact cover U ,S unsatisfiable, U ,S unsatisfiable caseU ,S is, follows U, exact cover U ,S unsatisfiable. Moreover,U ,S produced linear time U ,S , polynomial-time reductionXC unsatisfiability terms D1 D2. formula unsatisfiablecase entails term a. So, XC polynomially-reduced entailmentterms respect either D1 D2, making problems NP-hard henceNP-complete.order show NP-completeness clausal entailment, remarkdefinitions D1 D2, function Nnf transforms negations clauses termsnegations terms clauses. means test whether clause entailsclause testing whether term Nnf ( ) entails term Nnf (). Likewise,test whether term entails another term testing whether clause Nnf ( )entails clause Nnf (). NNF transformation polynomial, followsentailment clauses exactly difficult entailment terms, clausalentailment NP-complete.Lemma 6.4 definition D5, entailment clauses terms Pspace-complete.Proof. Membership Pspace immediate since entailment arbitrary formulaeK decided polynomial space. prove Pspace-hardness, adapt existingproof Pspace-hardness K.Figure 7 presents encoding QBF = Q1 p1 ...Qm pm K-formula f ()used section 6.7 (Blackburn et al., 2001) demonstrate Pspace-hardnessK. formula f () property satisfiable caseQBF-validity. formula f () generated polynomial-time ,QBF-validity problem known Pspace-hard, follows satisfiability formulaeK Pspace-hard well.Figure 8, show modified encoding. claim following:(1) f () f () logically equivalent102fiPrime Implicates Prime Implicants Modal Logic(i) q0Vm(ii) i=0 ((qi j6=i qj ) 2(qi j6=i qj ) ... 2m (qi j6=i qj ))Vm(iiia) i=0 ((qi 3qi+1 ) 2(qi 3qi+1 ) ... 2m (qi 3qi+1 ))V(iiib) {i|Qi =} 2i (qi (3(qi+1 pi+1 ) 3(qi+1 pi+1 )))VVm1 j(iv) m1i=1 ( j=i 2 ((pi 2pi ) (pi 2pi )))(v) 2m (qm )Figure 7: formula f () conjunction formulae.(i) q0Vm V(ii) i=0 ( j6=i ((qi qj ) 2(qi qj ) ... 2m (qi qj )))Vm(iiia) i=0 ((qi 3qi+1 ) 2(qi 3qi+1 ) ... 2m (qi 3qi+1 ))V(iiib) {i|Qi =} 2i (qi 3(qi+1 pi+1 )) 2i (qi 3(qi+1 pi+1 ))Vm1 Vm1(iv) i=1 ( j=i (2j (pi 2pi ) 2j (pi 2pi )))(v) 2m (qm 1 ) .... 2m (qm l )Figure 8: formula f () conjunction formulae, formulae(v) propositional clauses 1 ... l .(2) CNF, f () conjunction clauses respect D5(3) CNF, f () generated polynomial time f ()show (1), suffices show (i)(i), (ii)(ii), (iiia)(iiia), (iiib)(iiib), (iv)(iv),(v)(v). first equivalence immediate since (i) (i) identical. (ii)(ii)follows fact 2k (qi j6=i qj ) j6=i 2k (qi qj ). (iiia)(iiia) holdssince (iiia) (iiia) qi 3qi+1 replaced qi 3qi+1 . (iiib)(iiib)since 2i (qi (3(qi+1 pi+1 ) 3(qi+1 pi+1 ))) 2i (qi 3(qi+1 pi+1 )) 2i (qi3(qi+1 pi+1 )). equivalence (iv)(iv) holds 2j ((pi 2pi ) (pi 2pi ))2j (pi 2pi ) 2j (pi 2pi ). Finally, (v)(v) since 1 ... l . Thus, f ()f () logically equivalent.prove (2), show component formulae f () conjunctionclauses respect D5, provided CNF. Clearly case (i)(i) propositional literal. formula (ii) also conjunction clausesrespect D5 since conjunction formulae form 2k (qi qj ). Similarly,(iiia), (iiib), (iv) conjunctions clauses since formulae 2k (qi 3qi+1 ),2i (qi 3(qi+1 pi+1 )), 2i (qi 3(qi+1 pi+1 )), 2k (pi 2pi ), 2k (pi 2pi )clauses respect D5. formula (v) must also conjunction clauses sinceassumed propositional clauses, making 2m (qm ) clauserespect D5, (v) conjunction clauses respect D5.103fiBienvenu(3), clear transform (i), (iiia), (iiib), (iv) (i), (iiia), (iiib),(iv) polynomial time transformations involve simple syntactic operationsresulting formulae twice large. transformation (ii) (ii)slightly involved, hard see resulting formulatimes large original (and greater length f ()).step could potentially result exponential blow-up transformation(v) (v), put CNF. assumption already CNF,transformation executed polynomial time space,separate conjuncts rewrite (qm ) (qm ).let = Q1 p1 ...Qm pm QBF = 1 ... l propositionalclauses . Let f () formula defined Figure 8. (2) above, knowf () = 1 ... p clauses respect D5. consider followingformula= 3(21 ... 2p 32(a a))show f () satisfiable satisfiable follows:unsatisfiable21 ... 2p 32(a a) unsatisfiable1 ... p 2(a a) unsatisfiable1 ... p unsatisfiablef () unsatisfiablealso know (1) f () f (), (Blackburn et al., 2001)f () satisfiable case QBF validity. also easy seesatisfiable entail contradiction 3(a a). Puttingaltogether, find valid case entail 3(a a).3(a a) clauses terms respect D5, shownQBF-validity problem QBF propositional formulae CNF reducedproblems entailment clauses terms respect D5. Moreover,polynomial time reduction since follows (3) transformationaccomplished polynomial time. suffices show Pspace-hardness, sincewell-known QBF-validity remains Pspace-hard even restrict propositionalpart formula CNF (cf. Papadimitriou, 1994).Theorem 6 results Figure 1 hold.Proof. satisfaction dissatisfaction properties P1 P2 immediatelydetermined inspection definitions, satisfaction P3 definitions D2,D3b, D4, D5. Counterexamples P3 definitions D1 D3a providedbody paper: formula 2(a b) clause disjunction literalsrespect definitions.order show definition D3b satisfy P4, remark negationliteral 3(a b) equivalent 2(a b) cannot expressed literalD3b. definitions, shown (by straightforward inductiveproof) Nnf (L) literal whenever L literal, Nnf (C) term whenever104fiPrime Implicates Prime Implicants Modal LogicC clause, Nnf (T ) clause whenever term. enough provedefinitions satisfy P4 since Nnf () equivalent .Since know definitions D1 D2 satisfy properties P1 P2, followsTheorem 5 definitions satisfy P5. seen Lemma 6.1definition D5 satisfy P5, i.e. every formula equivalent conjunctionclauses respect D5 disjunction terms respect D5. everyclause (resp. term) D5 also clause (resp. term) respect definitions D3a, D3b,D4 (by Lemma 6.2), follows every formula equivalent conjunctionclauses disjunction terms respect definitions, meanssatisfy P5.easy see property P6 satisfied definitions sincedefinitions context-free grammars, well-known deciding membershipcontext-free grammars accomplished polynomial time (cf. Younger, 1967).Lemma 6.3, know deciding entailment clauses termsrespect either D1 D2 NP-complete (and hence P, unless P=NP). Entailmentclauses/terms Pspace-complete D5 (Lemma 6.4). every clause (resp.term) D5 also clause (resp. term) respect definitions D3a, D3b, D4(from Lemma 6.2), follows entailment clauses terms Pspace-harddefinitions. Membership Pspace immediate since entailment arbitaryK formulae Pspace.prove Theorem 9 several steps:Lemma 9.1 notions prime implicates prime implicants induced D4 satisfyImplicant-Implicate Duality.Proof. Suppose contradiction prime implicant formulaequivalent negation prime implicate . Let clauseequivalent (there must exist clause property P4, cf. Theorem6). clause implicate since |= . assumedprime implicate, must implicate |=6|= . let term equivalent (here use P4). mustimplicant since |= . Moreover, strictly weaker since |=6|= . means cannot prime implicant,contradicting earlier assumption. Hence, conclude every prime implicantformula equivalent negation prime implicate . proofevery prime implicate formula equivalent negation prime implicantproceeds analogously.Lemma 9.2 clauses terms defined according definition D4, every implicate formula entailed implicate var( ) var()depth () + 1, every implicant entails implicantvar( ) var() depth () + 1.Proof. intend show following statement holds: formulaimplicate , exists clause |= |= var( ) var()105fiBienvenu() () + 1. let arbitrary formula, let implicate .tautology, set = (where var()). , set= 3(a a) (where var()), clause verifies necessary conditions.consider case neither tautology falsehood,show construct clause . first thing use Dnf-4 rewritedisjunction satisfiable terms Ti respect D4 Ti containvariables appearing depth ():= T1 ... Tz|= , must case Ti |= every Ti . aim find clauseterms Ti Ti |= |= var(i ) var(Ti ) (i ) (Ti ).consider Ti . Since Ti term, form 1 ... k 31 ... 3m 21... 2n , 1 , ..., k propositional literals. clause, must form1 ... p 31 ... 3q 21 ... 2r , 1 , ..., p propositional literals.Ti |= , must case formula1 ... k 31 ... 3m 21 ... 2n1 ... p 21 ... 2q 31 ... 3runsatisfiable. Theorem 1, one following must hold:(a) exists u v u v(b) exists u u 1 ... n 1 ... q |=(c) exists u u 1 ... n 1 ... q |=(a) holds, set = u since Ti |= u |= , (u ) = 0 (Ti ), var(u )var(Ti ). (b) holds, must caseu 1 ... n |= 1 ... qhence3(u 1 ... n ) |= 31 ... 3q |=set = 3(u 1 ... n ), since Ti |= 3(u 1 ... n ) |= , (3(u 1... n )) (Ti ), var(3(u 1 ... n )) var(Ti ). Finally, (c) holds,must case1 ... n |= 1 ... q uhence2(1 ... n ) |= 31 ... 3q 2u |=set = 2(1 ... n ), Ti |= 2(1 ... n ) |= , (2(1 ... n )) (Ti ),var(2(1 ... n )) var(Ti ). Thus, shown every Ti ,Ti |= |= var(i ) var(Ti ) (i ) (Ti ). 1 ... zclause implied every Ti , hence , var(i ) var(Ti ) var()(i ) maxi (Ti ) ().let implicant , let formula Nnf (). knowNNF transformation equivalence-preserving, hence , straightforward106fiPrime Implicates Prime Implicants Modal Logicshow must clause respect D4. implicate ,must clause var( ) var() = var() depth() + 1 = () + 1 |= |= . Let Nnf ( ). easily verifiedterm. Moreover, properties NNF transformation, ,var( ) = var( ) = var( ), ( ) = ( ) = ( ). termvar( ) var(), ( ) () + 1, |= |= .Lemma 9.3 notions prime implicates prime implicants induced D4 satisfyFiniteness.Proof. Consider arbitrary formula . Lemma 9.2, know primeimplicate , must implicate containing propositionalatoms appearing ( ) () + 1 |= . since primeimplicate, must also |= hence . Thus, every prime implicateequivalent clause built finite set propositional symbolsdepth () + 1. finitely many non-equivalent formulaefinite alphabet fixed depth, follows finitely manydistinct prime implicates. Lemma 9.1, every prime implicant equivalentnegation prime implicate . follows every formulafinitely many distinct prime implicants.Lemma 9.4 notions prime implicates prime implicants induced D4 satisfyCovering.Proof. Let arbitrary formula. Lemma 9.2, know every implicateentailed implicate whose propositional variables contained var()whose depth () + 1. consider following set= { | |= , clause, var() var(), () () + 1}define another set follows:= { | 6 . |= 6|= }words, set logically strongest implicates depth() + 1 built propositional letters . claim following:(1) every prime implicate(2) every implicate , |=begin proving (1). Suppose (1) hold, is,prime implicate . Since definition implicate , followsmust implicate |= 6|= . Lemma 9.2,implicate ( ) () + 1, var( ) var(), |= .means element implies implied , contradictingassumption . thus conclude every element mustprime implicate .107fiBienvenu(2): let implicate . Lemma 9.2, exists clause|= . , done. Otherwise, must exist|= 6|= . , done, otherwise, find another strongermember . finitely many elements modulo equivalence, finite numbersteps, find element implies . Sinceseen members prime implicates , follows every implicateimplied prime implicate .second part Covering, let implicant , let clauseequivalent (there must one D4 satisfies P4). since |= , mustalso |= . According shown, must primeimplicate |= |= . Lemma 9.1, must equivalentnegation prime implicant . since |= ,follows |= , completing proof.Lemma 9.5 notions prime implicates prime implicants induced D4 satisfyEquivalence.Proof. Let formula K, suppose model every prime implicate. D4 known satisfy property P5 (by Theorem 6), find conjunctionclauses equivalent . Covering (Lemma 9.3), clauses impliedprime implicate , must model clauses. followsmodel . direction, simply note definitionprime implicates model , must also model every prime implicate. thus shown model model every primeimplicate . Using similar argument, show modelmodel prime implicant .Lemma 9.6 notions prime implicates prime implicants induced D4 satisfyDistribution.Proof. Let prime implicate 1 ... n . , must |= .Covering, know must exist prime implicate|= . means formula 1 ... n (which clausedisjunction clauses) entails . since prime implicate, must also case|= 1 ... n , hence 1 ... n . proof prime implicants entirelysimilar.Theorem 9 notions prime implicates prime implicants induced definitionD4 satisfy Finiteness, Covering, Equivalence, Implicant-Implicate Duality,Distribution.Proof. Follows directly Lemmas 9.1-9.6.Theorem 10 notions prime implicates prime implicants induced definitionsD1 D2 satisfy Equivalence.108fiPrime Implicates Prime Implicants Modal LogicProof. proof definitions. Suppose Equivalence holds.every formula , set prime implicates equivalent . meansset {} inconsistent, hence compactness K (cf. Blackburn et al.,2001, p. 86) finite subset {} inconsistent. 6 ,know set must contain set prime implicatescannot inconsistent. conjunction elements \ {} conjunctionclauses equivalent . follows every formula equivalentconjunction clauses. shown earlier proof Theorem 5formulae equivalent conjunction clauses respect D1 D2,follows Equivalence cannot hold definitions.Theorem 11 notions prime implicates prime implicants induced definitionsD3a, D3b, D5 satisfy Finiteness.Proof. Suppose clauses defined respect definition D3a, D3b, D5 (theproof three definitions). Consider formula = 2(a b). followsTheorem 3 implies k = 2(3k a) 3(a b 2k a) every k 1.formulae k clauses (with respect D3a, D3b, D5), k implicates. complete proof, show every k prime implicate . Since kmutually non-equivalent (because 2p 6|= 2q whenever p 6= q), followsinfinitely many prime implicates modulo equivalence.Consider k implicate = 31 ... 3m 21 ... 2nimplies (by Theorem 2 cannot propositional literals ). Using Theorem 3fact |= |= k , get following:(a) b |= ...(b) |= (3k a) (a b 2k a) every(c) 1 ... |= b 2kLet b |= ... . remark must satisfiable sinceotherwise combine (a) (c) get b |= b 2k a. (b), know|= (3k a) (a b 2k a) hence (2k a) (a b 3ka) inconsistent.follows (2k a) (2k a) b inconsistent. Using Theorem 1,find either |= 3k |= b. satisfiable clause respectdefinitions D3a, D3b, D5, cannot imply b, must |= 3k a.putting (a) (c) together, findb |= 1 ... |= b 2kfollows |= 2k a, i.e. 3k |= . thus 3k 1 ...b 2k a. 3k |= b 2k |= 1 ... , Theorem 3 get2(3k a) 3(a b 2k a) |= 2i 3i ... 3m |= hence k . thusshown implicate implies k must equivalent k . meansk prime implicate , completing proof.109fiBienvenuLemmas 12, 13, 14 follow easily known properties disjunctive normalform transformation propositional logic (cf. Bienvenu, 2009, ch. 2).proof Theorem 16, make use following lemmas:Lemma 16.1 algorithm GenPI always terminates.Proof. know Lemma 12 algorithm Dnf -4 always terminates returnsfinite set formulae. means finitely many terms consider., set (T ) contains finitely many elements (this immediate givendefinition (T )), means set Candidates also finite cardinality.final step, compare pair elements Candidates.comparison always terminates, finitely many pairs check, followsalgorithm GenPI terminates.Lemma 16.2 algorithm GenPI outputs exactly set prime implicates inputformula.Proof. first prove every prime implicate satisfiable term equivalentelement (T ). Let = 1 ... k 31 ... 3m 21 ... 2nsatisfiable term, let = 1 ... p 31 ... 3q 21 ... 2r oneprime implicates. restrict attention interesting casenon-tautologous. |= , must case1 ... k 31 ... 3m 21 ... 2n1 ... p 21 ... 2q 31 ... 3runsatisfiable. Theorem 1, one following must hold:(a) exists u v u v(b) exists u u 1 ... n |= 1 ... q(c) exists u 1 ... n |= u 1 ... q(a) holds, u |= , must equivalent u else would foundstronger implicate, contradicting assumption prime implicate .result holds since u (T ). (b) holds, formula 3(u 1 ... n )implicate implies , 3(u 1 ... n ). done since3(u 1 ... r ) member (T ). Finally consider case (c) holds.case, 2(1 ... n ) implicate implies , equivalent (asprime implicate). desired result since 2(1 ... n ) oneelements (T ). Thus conclude every prime implicate termequivalent element (T ). Lemma 13, elements Dnf -4() terms,disjunction equivalent . D4 satisfies Distribution, follows everyprime implicate input equivalent element Candidates. meanselement Candidates prime implicate , primeimplicate implies implied , hence j Candidatesj |= 6|= j . Thus, comparison phase, clauseremoved Candidates. suppose clause prime implicate .110fiPrime Implicates Prime Implicants Modal Logicknow must Candidates , moreover,choose j j < j |= . final stepcompare clauses j j 6= i, never find j |= j < i,j |= 6|= j j > i, otherwise would prime implicate. followsremains set Candidates returned algorithm.thus shown set formulae output GenPI input precisely setprime implicates .Theorem 16 algorithm GenPI always terminates outputs exactly set primeimplicates input formula.Proof. Follows directly Lemmas 16.1 16.2.Theorem 17 length smallest clausal representation prime implicateformula single exponential length formula.Proof. Prime implicates generated GenPI 2|| disjuncts2|| terms Dnf -4() Lemma 14. Moreover, disjunct length2|| (also Lemma 14). gives us total 2|| 2|| symbols, mustadd 2|| 1 disjunction symbols connecting disjuncts. thus findlength smallest representation prime implicate formula2|| 2|| + (2|| 1).Theorem 18 length smallest clausal representation prime implicateformula exponential length formula.Proof. Consider formula=n^(2ai,1 2ai,2 )i=1clause=_(j1 ,...,jn2(a1,j1 a2,j2 ... an,jn )){1,2}nak,l 6= am,p whenever k 6= l 6= p. difficult seeequivalent, means must prime implicate . remainsshown clause equivalent must length least ||. yields resultsince clearly size exponential n, whereas length linear n.Let shortest clause equivalent . equivalent , followsTheorem 2 disjunction 2-literals inconsistent 3-literals.since assumed shortest representation , cannot contain inconsistent3-literals redundant 2-literals, since could remove find equivalentshorter clause. must form 21 ... 2m , l 6|= k whenever l 6= k.since |= , every disjunct 2p must also imply . disjunction 2-literals,follows Theorem 3 every disjunct 2p implies disjunct 2q .means every 2p must length least 2n + 1, since p satisfiableformula implies conjunction n distinct propositional variables. also knowevery disjunct 2q implies disjunct 2p since |= . wish111fiBienvenushow two disjuncts imply disjunct . Supposecase, is, distinct disjuncts 21 22 disjunct 2p21 |= 2p 22 |= 2p . since 21 22 distinct disjuncts,must 21 |= ai,1 22 |= ai,2 21 |= ai,2 22 |= ai,1 .know 2p |= 2q q , every q implies either ai1 ai2 , either2p |= 2ai1 2p |= 2ai2 . know 2q imply either 2ai,1 2ai,2both, one 21 22 must imply 2p . contradicts earlierassumption 21 |= 2p 22 |= 2p , disjunct must imply distinctdisjunct . thus demonstrated contains many disjuncts .already shown disjuncts shorter disjuncts ,follows | | ||, hence | | = ||. conclude every clause equivalentlength least ||, completing proof.Theorem 19, prove following clause_=2q1 ...qn c(q1 ,...,qn){3,2}nprime implicate (with respect D1 D2) formula=( 23(b0 b1 ) 22(b0 b1 ) )n^( 2i 3bi 2i 2bi )i=2n1^2i+1 ( (bi1 bi ) 2bi ) 2n+1 ( (bn1 bn ) c )i=1moreover shorter way represent .proof Theorem 19 makes use following lemmas.Lemma 19.1 Let l1 ... lm D1-clause implies q1 ...qn a, qi {2, 3}propositional variable. l1 ... lm q1 ...qn a.Proof. proof, make use fact every D1-clause satisfiable.straightforwardly shown structural induction. base case propositionalliterals, clearly satisfiable. induction step, consider D1-clauseproper sub-clauses satisfiable. three possibilities: eitherform 2 3 satisfiable D1-clause, disjunction 1 2satisfiable D1-clauses 1 2 . three cases, find must also satisfiable.proof lemma induction n. n = 0, l1 ... lm |= a.According Theorem 2, every disjunct l1 ... lm must either unsatisfiableformula. shown previous paragraph every D1-clause satisfiable,l1 ... lm a.suppose result holds whenever n k, suppose l1 ... lm |=q1 ...qk+1 a. every li , must li |= q1 ...qk+1 a, hence |= li q1 ...qk+1 a. UsingTheorem 1, arrive following four possibilities:112fiPrime Implicates Prime Implicants Modal Logic(a) |= q1 ...qk+1(b) li(c) q1 = 3 li 3li li |= q2 ...qk+1(d) q1 = 2 li 2li li |= q2 ...qk+1eliminate case (a) since 6|= q1 ...qk+1 every string modalities q1 ...qk+1 .also eliminate (b) since li must satisfiable D1-clauses. remark(c) holds, according induction hypothesis, li 3q2 ...qk+1 a. Similarly,(d) holds, li 2q2 ...qk+1 a. follows li equivalent q1 ...qk+1 a,l1 ... lm q1 ...qk+1 a.Vn , let = 2q (b b ) ( nkLemma19.2Fix(q,...,q){2,3}1n101k=2 2 qk bk )Vn1 k+1( (bk1 bk ) 2bk ) 2n+1 ( (bn1 bn ) c ). |= 2r1 ...rn ck=1 2rk = qk 1 k n.Proof. begin showing 1 n 1 formulabi1 bi (n^n1^2ki1 qk bk ) (k=i+12ki ((bk1 bk ) 2 bk ) ) 2ni ((bn1 bn ) c)k=ientails formula ri+1 ...rn c case qi+1 ...qn = ri+1 ...rn .proof induction i. base case = n 1.bn2 bn1 qn bn ((bn2 bn1 ) 2bn1 ) 2((bn1 bn ) c) |= rn c(1)bn2 bn1 qn bn 2bn1 2((bn1 bn ) c) |= rn c(Theorem 1) eitherqn = 3 rn = 2 bn1 ((bn1 bn ) c) |= cqn = rn bn1 bn ((bn1 bn ) c) |= cbn1 ((bn1 bn ) c) 6|= c, cannot first alternative. followsEquation (1) holds, second alternative must hold, case get qn = rn ,desired. direction, simply note bn1 bn ((bn1 bn ) c) |= cvalid entailment, means qn = rn implies Equation (1).Next let us suppose statement holds 1 < j n 1, let usprove statement holds = j 1.bj2 bj1 (n^kj2qk bk ) (k=jn1^2kj+1 (bk1 bk 2bk ) )k=j12nj+1 ((bn1 bn ) c)|= rj ...rn c113(2)fiBienvenuone following holds:(a) qj = 3 rj = 2bj1 (n^n1^2kj1 qk bk ) (k=j+12kj ((bk1 bk ) 2bk ) ) 2nj ((bn1 bn ) c)k=j|= rj+1 ...rn c(b) qj = rjbj1 bj (n^n1^2kj1 qk bk ) (k=j+12kj ((bk1 bk ) 2bk )) 2nj ((bn1 bn ) c)k=j|= rj+1 ...rn cfirst show entailment (a) hold. Consider model =hW, R, vi defined follows:W = {wj , ..., wn }R = {(wj , wj+1 ), ..., (wn1 , wn )}v(c, w) = f alse w Ww 6= wj : v(bk , w) = true w = wkv(bk , wj ) = true k = j 1Notice since world (excepting wn ) exactly one successor, 2- 3quantifiers behaviour (except wn ). easily verified M, wjsatisfies left-hand side aboveVentailment tuple qj+1 ...qn : M, wj |=bj1 definition, M, wj |= nk=j+1 2kj1 qk bk M, wk |= bk k 6= j,Vn1 kjM, wj |= k=j2 ((bk1 bk ) 2bk ) ) since M, wj 6|= bj M, wk 6|= bk1k 6= j, finally M, wj |= 2nj ((bn1 bn ) c) since wn 6|= bn1 . However,right-hand side rj+1 ...rn c satisfied wj : world accessible wj n jsteps wn satisfy c.shown case (a) cannot hold, means Equation (2) holds(b) does. apply induction hypothesis entailment (b),find holds case qj+1 ...qn = rj+1 ...rn . follows Equation (2)qj ...qn = rj ...rn , desired. completes proof statement.proceed proof lemma. Theorem 1,2q1 (b0 b1 ) (n^n1^2k qk bk ) (k=22k+1 ( (bk1 bk ) 2bk ) 2n+1 ( (bn1 bn ) c )k=1|= 2r1 ...rn cholds caseq1 (b0 b1 ) (n^k=22k1 qk bk )n1^2k ( (bk1 bk ) 2bk ) 2n ( (bn1 bn ) c )k=1|= r1 ...rn c114fiPrime Implicates Prime Implicants Modal Logicturn holds one following statements holds:(i) q1 = 3 r1 = 2(n^n1^2k2 qk bk ) (k=22k1 ((bk1 bk ) 2bk ) ) 2n1 ((bn1 bn ) c) |= r2 ...rn ck=1(ii) q1 = r1b0 b1 (n^k=2n1^2k2 qk bk ) (2k1 ((bk1 bk ) 2bk ) ) 2n1 ((bn1 bn ) c)k=1|= r2 ...rn cremark set j = 1 (a) above, left-hand side entailment (i)logically weaker (a), right-hand side matches (a).already shown entailment (a) hold, follows entailment(i) cannot hold either. Thus, find desired entailment relation statementlemma holds (ii) does. completes proof since alreadyshown induction entailment (ii) holds q2 ...qn = r2 ...rn ,i.e. (ii) true case q1 ...qn = r1 = rn .Lemma 19.3 D1-clause equivalent strictly smaller size .Proof. Let D1-clause equivalent . Suppose furthermoreshortest clause. non-tautologous contains 2-literals disjuncts,follows every disjunct must either unsatisfiable 2-literal (cf. Theorem 2).D1-clauses always satisfiable (cf. proof Lemma 19.1), must contain2-literals.Since |= , every disjunct 2l must imply disjunct 2q1 ...qn c . Also,every disjunct 2l must implied disjunct 2q1 ...qn c , since otherwisecould remove 2l preserving equivalence .follows disjunct implied disjunct impliesdisjunct . since disjuncts imply (because Lemma19.1), follows disjunct equivalent disjunct , moreoverevery disjunct equivalent disjunct .completes proof since clear disjuncts 2q1 ...qn c cannotcompactly represented.proof works equally well D2, since every D2-clause also D1-clause.Theorem 19 prime implicates defined using either D1 D2, lengthsmallest clausal representation prime implicate formula exponentiallength formula.Proof. begin definition D1. Let defined page 112. begindistributing order transform equivalent disjunction D4-terms:_Tq1 ,...,qn(q1 ,...,qn){2,3}n115fiBienvenuTq1 ,...,qn equal2q1 (b0 b1 ) (n^2 q bi )n1^2i+1 ( (bi1 bi ) 2bi ) 2n+1 ( (bn1 bn ) c )i=1i=2Lemma 19.2, Tq1 ,...,qn |= 2q1 ...qn c, hence Tq1 ,...,qn |= . thus |= .show stronger clause respect D1 implied. Let D1-clause |= |= . non-tautologous disjunction2-literals, know Lemma 2 every disjunct must form 2ll D1-clause l |= r1 ...rn c quantifier string r1 ...rn . accordingLemma 19.1, l |= r1 ...rn c, l equivalent r1 ...rn c. follows equivalentclause disjuncts forms 2r1 ...rn c.|= , must case terms Tq1 ,...,qn implies , equivalentlyTq1 ,...,qn |= . shown disjuncts 2-literals,follows Theorem 1 term implies disjunct . Moreover, knowpreceding paragraph disjuncts equivalent formulaform 2r1 ...rn c. Lemma 19.2, formula type impliedTq1 ,...,qn formula 2q1 ...qn c. means every tuple quantifiers (q1 , ..., qn ),disjunct equivalent 2q1 ...qn c. follows every disjunctequivalent disjunct , giving us |= . thus concludeprime implicate .completes proof, since already shown Lemma 19.3shorter D1-clause equivalent itself.proof also works definition D2 since every D2-clause also D1-clause.particular means D2-clause prime implicate respect D1also prime implicate respect D2, D2-clause shortestamong equivalent D1-clauses also shortest among D2-clauses.Theorem 20 number non-equivalent prime implicates formula doubleexponential length formula.Proof. know Theorem 16 every prime implicateWof equivalentclause returned GenPI. Every clause form Dnf -4()(T ). 2|| terms Dnf -4() Lemma 14, clauses2|| disjuncts. Moreover, 2|| choices disjunctsince cardinality (T ) bounded size , know Lemma||1.3 2||. follows (2||)2 clauses returned||GenPI, hence (2||)2 non-equivalent prime implicates .Theorem 21 number non-equivalent prime implicates formula may doubleexponential length formula.Proof. Let n natural number, let a11 , a12 , ..., an1 , an2 , b11 , b12 , b12 , ..., bn1 ,bn2 4n distinct propositional variables. Consider formula definedn^((3ai1 2bi1 ) (3ai2 2bi2 ))i=1116fiPrime Implicates Prime Implicants Modal Logichard see 2n terms Dnf -4(), corresponding 2n waysdeciding {1, ..., n} whether take first second disjunct. termDnf -4() formn^(3ai f (i,T ) 2bi f (i,T ) )i=1f (i, ) {1, 2} i. , denote D(T ) set formulae {3(a f (i,T )b1 f (1,T ) ... bn f (n,T ) )) | 1 n}. consider set clauses C defined{_dT | dT D(T )}Dnf -4()nNotice n2 clauses C since clause corresponds choice onen elements D(T ) 2n terms Dnf -4(). number doubleexponential || since length linear n. order complete proof,show (i) clauses C prime implicates (ii) clauses Cmutually non-equivalent.begin showing 1 6|= 2 every pair distinct elements 1 2 C.immediately gives us (ii) prove useful proof (i). Let 1 2distinct clauses C. 1 2 distinct, must term Dnf -4()1 2 choose different elements D(T ). Let d1 elementD(T ) appearing disjunct 1 , let d2 element D(T ) disjunct2 , let aj,k a-literal appears d2 (and hence d1 ). Considerformula = 2(aj,k b1,k1 ... bn,kn ), tuple (k1 , ..., kn ) like tupleassociated except 1s 2s inversed. Clearly d1 consistent,since variables appear d1 . inconsistent every disjunct2 , since construction every disjunct 2 contains literal whose negation appears. follows 2 |= 1 6|= , hence 1 6|= 2 .prove (i). Let clause C, let prime implicateimplies . Theorem 16, know must equivalent one clauses outputGenPI, specifically clause output GenPI disjunction3-literals (because Theorem 2). remark set C composed exactlycandidate clauses disjunctions 3-literals, must equivalentclause C. shown element C implies itself.follows , means prime implicate .Theorem 22 prime implicates defined using either D1 D2, numbernon-equivalent prime implicates formula may double exponential lengthformula.Proof. Let defined page 112. Set equal formula obtainedreplacing c last conjunct c d. Set equal set clausesobtained replacing zero occurrences c d. example, n = 1,n= {23c 22c, 23d 22c, 23c 22d, 23d 22d}. 22 elementssince choose 2n disjuncts whether change c d. intend117fiBienvenushow clauses pairwise non-equivalent prime implicates .proof every element indeed prime implicate (with respect D1D2) proceeds quite similarly proof prime implicate (see proofTheorem 19), repeat here. Instead show elementspairwise non-equivalent. so, consider two distinct elements. Since distinct, must string quantifiers q1 ...qndisjunct 2q1 ...qn ( {c, d}) disjunct . |= ,would 2q1 ...qn |= , hence 2q1 ...qn |= 2r1 ...rn disjunct r1 ...rn .using Lemma 19.1, see happen r1 ...rn = q1 ...qn = ,i.e. 2q1 ...qn disjunct . contradiction, must 6|= . followselements pairwise non-equivalent, hence possesses doubleexponential number prime implicates.Theorem 23 exists algorithm runs single-exponential space sizeinput incrementally outputs, without duplicates, set prime implicatesinput formula.Proof. Let sets Candidates function defined Figure 3.assume ordered: = {T1 , ..., Tn }. Ti , let max denotenumber elements (Ti ), assume ordering elements (Ti ):(Ti ) = {i,1 , ..., i,max }. Notice tuples {1, .., max 1 } ... {1, ..., max n }ordered using standard lexicographic ordering <lex : (a1 , ..., ) <lex (b1 , ..., bn )1 j n aj < bj ak bk 1 k j 1.set maxindex = ni=1 maxi , let f : {1, .., max 1 } ... {1, ..., max n } {1, ..., maxindex }bijection defined follows: f (a1 , ..., ) = (a1 , ..., ) m-thtuple lexicographic ordering {1, .., max 1 } ... {1, ..., max n }. denoteunique clause form 1,a1 ... n,an f (a1 , ..., ) = m. remarkgiven index {1, ..., maxindex } sets (T1 ), ..., (Tn ), possiblegenerate polynomial space (in size sets (T1 ), ..., (Tn )) clause .make use fact modified version algorithm GenPI, defined follows:Function IterGenPI()(1) GenPI.(2) GenPI.(3) = 1 maxindex : j 6|= j < either j 6|= |= jevery < j maxindex , output .proofs termination, correctness, completeness IterGenPI similar corresponding results GenPI (Theorem 16), omit details.instead focus spatial complexity IterGenPI. first step IterGenPIclearly runs single-exponential space ||, since deciding satisfiability takespolynomial space ||, generating elements Dnf -4() takessingle-exponential space || (refer Lemma 14). Step 2 also uses singleexponential space ||, since sets (T ) associated term Tipolynomial size Ti . Finally, Step 3, use observation generation given index done polynomial space size sets118fiPrime Implicates Prime Implicants Modal Logic(T1 ), ..., (Tn ), hence single-exponential space ||. sufficient sincecomparisons Step 3, need keep two candidate clauses memoryone time, deciding whether one candidate clause entails another accomplishedsingle-exponential space (since clauses single-exponential size ||).Theorem 24 Prime implicate recognition Pspace-hard.Proof. reduction simple: formula unsatisfiable 3(a a)prime implicate . suffices problem checking unsatisfiability formulaeK known Pspace-complete.need following two lemmas Theorem 25:Lemma 25.1 Let formula K, let = 1 ... k 31 ... 3m21 ... 2n (j propositional literals) non-tautologous clause. Suppose furthermoreliteral l \ {l}. (), 1 ... k( ( \ {1 , ..., k })) 3(1 ... n ) ( ( \ {31 , ..., 3m })) everyi, 2(i 1 ... ) ( ( \ {2i })).Proof. prove contrapositive: 1 ... k 6 ( ( \ {1 , ..., k }))3(1 ... n ) 6 ( ( \ {31 , ..., 3m })) 2(i 1... ) 6 ( ( \ {2i })), 6 (). consider case|= 6|= immediately get 6 ().Let us first suppose 1 ...k 6 ((\{1 , ..., k })). Since |= , must also(\{1 , ..., k }) |= 1 ...k , 1 ...k implicate (\{1 , ..., k }).1 ... k known prime implicate ( \ {1 , ..., k }), followsmust clause ( \ {1 , ..., k }) |= |= 1 ... k 6|= .consider clause = 31 ... 3m 21 ... 2n . know |= since( \ {1 , ..., k }) |= , |= |= 1 ... k . also 6|=since must equivalent propositional clause (by Theorem 2) propositionalpart (namely 1 ... k ) imply . follows |= |= 6|= ,6 ().Next suppose 3(1 ... n ) 6 ( ( \ {31 , ..., 3m })). 3(1 ... n )must implicate ( \ {31 , ..., 3m }) since assumed |= .3(1 ... n ) prime implicate ( \ {31 , ..., 3m }), follows( \ {31 , ..., 3m }) |= |= 3(1 ... n ) 6|= . Let= 1 ...k 21 ...2n . Theorem 2, know disjunction3-literals, according Theorem 3 must 6|= since 3(1 ... n ) 6|= .also know |= since ( \ {31 , ..., 3m }) |= |= since|= 3(1 ... n ). means |= |= 6|= , 6 ().Finally consider case 2(i 1 ... ) 6( ( \ {2i })). know |= hence ( \ {2i }) |= 2i .Moreover, since ( \ {2i }) |= 3j j, ( \ {2i }) |= 2(i1 ... ). Thus, 2(i 1 ... ) 6 ( ( \ {2i })), must mean( \ {2i }) |= |= 2(i 1 ... ) 6|= .assumption, tautology, 2(i 1 ... ) cannot tautology119fiBienvenueither. |= 2(i 1 ... ) 2(i 1 ... ) tautology,follows Theorem 2 equivalent formula 21 ... 2p . Let= 1 ... k 31 ... 3m 21 ... 2i1 (21 ... 2p ) 2i+1 ... 2n .( \ {2i }) |= 21 ... 2p , must case |= . Also, knowj |= j 1 ... otherwise would1 ... |= j hence 2(i 1 ... ) |= 21 ... 2p . Similarly,k 6= 2i |= 2(k 1 ... ) would mean\ {2i }, contradicting assumption superfluous disjuncts. follows Theorem 3 6|= . Thus, |= |= 6|= , means6 ().Lemma 25.2 Let formula K, let = 1 ...k 31 ...3m 21 ...2n(j propositional literals) non-tautologous clause. Suppose furthermoreliteral l \ {l}. 6 (), either 1 ... k 6 ( ( \{1 , ..., k })) 3(1 ... ) 6 ( (1 ... k 2(1 1 ... ) ... 2(n1 ... ))) 2(i 1 ... ) 6 ( ( \ {2i })) i.Proof. consider case |= 6|= immediatelyget result. Suppose 6 () |= . Definition 7, must= 1 ...o 31 ... 3p 21 ... 2q |= |= 6|= . Since 6|= ,Proposition 3 know either 1 ... k 6|= 1 ... 1 ... 6|= 1 ... p6|= j 1 ... p j.begin case 1 ... k 6|= 1 ... . |= , Theorem 3,1 ...p |= 1 ...m every j |= 1 ...m j .follows (also Theorem 3) |= |= 1 ...o 31 ...3m 21 ...2n ,hence (\{1 , ..., k }) |= 1 ...o . 1 ...o |= 1 ...k 6|= 1 ...o ,found implicate ( \ {1 , ..., k }) stronger 1 ... k ,1 ... k 6 ( ( \ {1 , ..., k })).Next suppose 1 ...m 6|= 1 ...p . |= , follows Theorem 31 ...o |= 1 ...k every j |= 1 ...m j .thereby obtain |= |= 1 ... k 31 ... 3p 2(1 1 ... ) ...2(n 1 ... ). this, infer (1 ... k 2(1 1 ...) ... 2(n 1 ... )) |= 31 ... 3p |= 31 ... 3m 6|= 31 ... 3p .31 ... 3m 3(1 ... ), follows 3(1 ... ) 6 ( (1 ...k 2(1 1 ... ) ... 2(n 1 ... ))).Finally suppose 6|= j 1 ... p j furthermore 1 ... |=1 ... p (we already shown result holds 1 ... 6|= 1 ...p ). 2(i 1 ... ) implicate ( \ {2i })) show2(i 1 ... ) prime implicate ( \ {2i })), must findstronger implicate. Consider set = {s {1, ..., q} : |= 1 ... 6|=k 1 ...m k 6= i}. note must least one elementassumed 6|= \ {2i }. since 1 ... |= 1 ... k , 1 ... p |= 1 ... m,... , |= S,every 6 r 6= sW|= r 1 Wget |= W|= 1 ...k 31 ...3m ( j6=i 2j )( WsS 2s ). follows (\{2i }) |= sS 2(s 1 ...m ), means sS 2(s 1 ...m )120fiPrime Implicates Prime Implicants Modal LogicWimplicate (\{2i }). Moreover, sS 2(s 1 ...m ) |= 2(i 1 ...m )since construction |= 1 ... every S.Wremains shown 2(i 1 ... ) 6|= sS 2(s 1 ... ).Supposecontradiction contrary holds. 2(i 1 ... ) |=WsS 2(s 1 ... ), Theorem 1, must1 ... |= 1 ... . |= 1 ... , thus|= 1 ... p since assumed 1 ... |= 1 ... p . contradictsearlier assumption W6|= j 1 ... p j. Thus, shown2(i 1 ... ) 6|= sS 2(s 1 ... ), 2(i 1 ... ) 6( ( \ {2i })).Theorem 25 Let formula K, let = 1 ...k 31 ...3n 21 ...2m(j propositional literals) non-tautologous clause (a) 1 ... ni, (b) literal l \ {l}. ()following conditions hold:1. 1 ... k ( ( \ {1 , ..., k }))2. 2(i 1 ... n ) ( ( \ {2i })) every3. 3(1 ... n ) ( ( \ {31 , ..., 3n }))Proof. forward direction shown Lemma 25.1. direction followsLemma 25.2 together hypothesis 1 ... n (whichensures (1 ... k 2(1 1 ... ) ... 2(n 1 ... ))( \ {31 , ..., 3n })).Theorem 26 Let formula K, let non-tautologous propositional clause|= literal l \ {l}. ()6|= \ {l} l .Proof. Consider formula non-tautologous propositional clause |=literal l \ {l}. Suppose |= \ {l}l . know 6 \ {l}, follows \ {l} implicatestrictly stronger , prime implicate .direction, suppose 6 (). must case clause|= |= 6|= . Since |= , follows Theorem 2 literalpropositional literal inconsistent. literals inconsistent,must inconsistent, clearly |= \ {l} every l . Otherwise,equivalent propositional clause, specifically propositional clausecontaining literals appearing (since |= ). strictly stronger ,must literal l appear . means |= \ {l}|= \ {l}, completing proof.Theorem 27 Let formula K, let = 2 non-tautologous clause|= . () exists term Dnf-4()|= , conjunction formulae 2 .121fiBienvenuProof. Let formula, let = 2 non-tautologous clause |= .first direction, suppose term Dnf -4() |= ,conjunction formulae 2 . two cases: eitherterms Dnf -4() unsatisfiable, terms none satisfycondition. first case, 2 prime implicate , since contradictoryclauseW(e.g. 3(a a)) stronger. second case, consider clause = 2T ,conjunction formulae 2 . every mustW2T |= 2, otherwise would 6|= 2, henceW 6|= 2. Moreover, |= 2Tsince |= 2T every . Theorem 1, 2 6|= 2T since 6|= .|= |= 6|= , means prime implicate .direction, suppose 2 prime implicate 6|= .must |= 2 Dnf -4(),W Dnf -4() non-empty. |= 2, W2T also implies 2. show. letW 2T primeWimplicate Wimplicate implies 2T . since |= 2T 2Tnon-tautologous, follows Theorem 2 21 ... 2n formulae .|= , must |= 21 ... 2n WDnf -4().Wbecase 2T |= 21 ... 2n , Wmeans 2T |= 21 ... 2n . 2Timplies every implicate Wimplies it, 2T must prime implicate .means 2 6|= 2T , since assumed 2 prime implicate. follows Theorem 1 6|= Dnf -4().order show Theorem 28 need following lemmas:Lemma 28.1 3 implicate prime implicate, algorithmTest3PI returns input (3, ).Proof. Suppose 3 prime implicate . unsatisfiable, mustsatisfiable, return first step. satisfiable, sinceassumed 3 implicate , must clause |= |= 33 6|= . |= 3, follows Theorem 2 equivalent disjunction3-formulae, hence clause 3 .know Lemma 13 equivalent disjunction terms Dnf -4().must thus case Ti |= 3 Ti Dnf -4(). Since Ti satisfiableconjunction propositional literals 2- 3-formulae, follows exists set{3i , 2i,1 , ..., 2i,k(i) } conjuncts Ti 3(i i,1 ...i,k(i) ) |= 3 , otherwiseTi would fail imply 3 . Moreover, elements {3i , 2i,1 , ..., 2i,k(i) } mustappear NNF outside modal operators, formulae , i,1 , ..., i,k(i) mustelements set X . immediate3_(i i,1 ... i,k(i) ) |= 3 |= 33 6|= 3_(i i,1 ... i,k(i) )122(3)fiPrime Implicates Prime Implicants Modal LogicWlatter implies formula 3 (3 (i i,1 ... i,k(i) )) must consistent,means_^( (i i,1 ... i,k(i) )) (i i,1 ... i,k(i) )must consistent well. itVmust case select{i , i,1 , ..., i,k(i) } consistent. Let set . setsatisfies condition algorithm since:SX6|=W(because knowVconsistent)Ti Dnf -4(), conjuncts 3i , 2i,1 , ..., 2i,k(i) Ti that:{i , i,1 , ..., i,k(i) } 6= (since contains {i , i,1 , ..., i,k(i) })3(i i,1 ... i,k(i) ) |= 3 (follows (3) above)Since exists set X satisfying conditions, algorithm returns no.Lemma 28.2 algorithm Test3PI returns input (3, ), 3prime implicate .Proof. Suppose Test3PI returns input (3, ). happens firststep, must case unsatisfiable 3 unsatisfiable, case 3prime implicate . possibility algorithm returns Step 3,means Wmust X satisfying:(a) 6|=(b) Ti Dnf -4(), exist conjuncts 3i , 2i,1 , ..., 2i,k(i) Tithat:(i) {i , i,1 , ..., i,k(i) } 6=(ii) 3(i i,1 ... i,k(i) ) |= 3WLet clause 3(i i,1 W... i,k(i)W ). remark Ti , Ti |=3(i i,1 ...i,k(i) ), hence|=3(i i,1 ...i,k(i) ).WW definitionDnf -4(), also Ti . immediately follows |= 3(i i,1 ...i,k(i) )hence W|= . 2 (b) (ii), 3(i i,1 ... i,k(i) ) |= 3 every i,hence 3(i i,1 ... i,k(i)) |= 3 yields |= 3. 2 (b) (i),{i , i,1 , ..., i,k(i) } 6= hence everyWWi,1 ... i,k(i)W|= . infer 3(i i,1 ... i,k(i) ) |= W3,hence |= 3 . know 2 (a) Theorem 1 3 6|= 3 .follows 3 6|= . Putting together, see exists clause|= |= 3 3 6|= , hence 3 prime implicate .Theorem 28 Let formula, let 3 implicate . algorithmTest3PI returns yes input (3, ) 3 prime implicate .123fiBienvenuProof. clear Test3PI terminates since unsatisfiability testing NNF transformation always terminate, finitely many Ti . Lemmas 28.128.2 show us algorithm always gives correct response.Theorem 29 algorithm Test3PI runs polynomial space.Proof. remark sum lengths elements X boundedlength formula Nnf(), hence Lemma 14 sum theW lengthselements particular X cannot exceed 2||. Testing whether 6|= thusaccomplished polynomial Vspace length involves testingsatisfiability formula whose length clearly polynomial .let us turn Step 3 (b). notice necessary keep Timemory once, since generate terms Ti one time using polynomialspace Lemma 12. Lemma 14, length Ti Dnf -4() 2||.follows checking whether {i , i,1 , ..., i,k(i) } 6= , whether 3(i i,1 ...i,k(i) ) |= 3 accomplished polynomial space length .conclude algorithm Test3PI runs polynomial space.order show Theorem 32, use following lemmas:Lemma 32.1 clause prime implicate , TestPI outputsinput.Proof. Let us begin considering formula clause primeimplicate . two possible reasons this: either implicate ,implicate exists stronger implicate. first case, TestPI returnsStep 1, desired. focus case implicateprime implicate. begin treating limit cases onetautology contradiction. Given know non-prime implicate ,two possible scenarios: either 6|= |= , |= 6|= . cases,algorithm returns Step 2.implicate , neither tautology contradiction,algorithm continue Step 3. step, redundant literals deleted, contains 3-literals, add extra disjunct 2-literalssatisfies syntactic requirements Theorem 25. Let 1 ...k 31 ... 3m 21... 2n clause end Step 3 modifications made.transformations Step 3 equivalence-preserving (Theorem 1), modifiedequivalent original, still non-tautologous non-prime implicate .means satisfy conditions Theorem 25. follows onefollowing holds:(a) 1 ... k 6 ( ( \ {1 , ..., k })(b) 2(i 1 ... n ) 6 ( ( \ {2i }))(c) 3(1 ... n ) 6 ( ( \ {31 , ..., 3n }))124fiPrime Implicates Prime Implicants Modal LogicSuppose (a) holds. 1 ... k non-tautologous propositional clause implied( \ {1 , ..., k }) contains redundant literals. means ( \{1 , ..., k }) 1 ...k satisfy conditions Theorem 26. According theorem,1 ... k 6 ( ( \ {1 , ..., k }), must j ( \{1 , ..., k }) |= 1 ... j1 j+1 ... k . means |= \ {j }, algorithmreturns Step 4.Suppose next (b) holds, let 2(i 1 ... n ) 6 (( \ {2i })). Theorem 27, means Dnf -4()2(i 1 ... n ) entails conjunction 2-formulae conjuncts . followsalgorithm returns Step 5.Finally consider caseStep 6,Wm neither (a) (b) holds (c)Wdoes.call Test3PI(3( i=1 ), ( \ {31 , ..., 3m })). 3( i=1 ) primeimplicate (\{31 , ..., 3m })) shown Test3PI correct (Theorem28), Test3PI return no, TestPI return well. coveredpossible cases, conclude clause prime implicate ,TestPI outputs no.Lemma 32.2 TestPI outputs input (, ) clause,prime implicate .Proof. 5 different ways TestPI return (these occur Steps 1, 2, 4, 5,6). Let us consider turn. first way algorithm returnStep 1 find 6|= . correct since cannot prime implicateconsequence . Step 2, return unsatisfiable not,tautology not. also correct since cases cannot primeimplicate since exist stronger implicates (any contradictory clause ,non-tautologous implicate ). Step 3, may modify , resultingformula equivalent original, prime implicate caseoriginal clause was. Let 1 ...k 31 ... 3m 21 ... 2n clause endStep 3. Step 4, return find propositional literal l|= \{l}. since Step 3, removed redundant literals , sure\ {l} strictly stronger . |= \ {l} |= 6|= \ {l},means prime implicate . consider Step 5 TestPI. step,return disjunct 2i term Dnf -4((\{2i }))2(i 1 ... ) entails conjunction 2-literals . According Theorem 27,means 2(i 1 ... ) prime implicate ( \ {2i }),means prime implicateW Theorem 25. . step, returnTest3PI returns input (3( ki=1 ), ( \ {31 , ..., 3m })). Theorem 28,Wknow happens case 3( ki=1 ) prime implicate( \ {31 , ..., 3m }). follows Theorem 25 prime implicate.Theorem 32 algorithm TestPI always terminates, returns yes input (,) prime implicate .125fiBienvenuProof. algorithm TestPI clearly terminates Steps 1 5 involve finite numbersyntactic operations finite number entailment checks. Moreover, callTest3PI Step 6 known terminate (Theorem 28). Correctness completenessalready shown Lemmas 32.1 32.2.make use following lemma proof Theorem 34:Lemma 34.1 algorithm TestPI provided Figure 5 runs polynomial spacelength input.Proof. clear steps 1 5 carried polynomial space lengthinput, since simply involve testing satisfiability formulae whose lengthspolynomial ||+||. Step 6 alsoW carried polynomial space since Theorem29 deciding whether formula 3(Wi=1 ) prime implicate ( \ {1 , ..., }))takes polynomial space |3(i=1 )| + | ( \ {31 , ..., 3m }))|, hence|| + ||. thus conclude algorithm TestPI runs polynomial spacelength input.Theorem 34 Prime implicate recognition Pspace.Proof. show Theorem 32 TestPI always terminates returns yes whenever clause prime implicate otherwise. means TestPI decisionprocedure prime implicate recognition. Since algorithm shown runpolynomial space (Lemma 34.1), conclude prime implicate recognitionPspace.Corollary 35 Prime implicate recognition Pspace-complete.Proof. Follows directly Theorems 24 34.ReferencesAdjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2006). Distributedreasoning peer-to-peer setting: Application semantic web. JournalArtificial Intelligence Research, 25, 269314.Baader, F., McGuiness, D. L., Nardi, D., & Patel-Schneider, P. (Eds.). (2003). Description Logic Handbook. Cambridge University Press.Bienvenu, M. (2007). Prime implicates prime implicants modal logic. ProceedingsTwenty-Second Conference Artificial Intelligence (AAAI07), pp. 397384.Bienvenu, M. (2008). Prime implicate normal form ALC concepts. ProceedingsTwenty-Third Conference Artificial Intelligence (AAAI08), pp. 412417.Bienvenu, M. (2009). Consequence Finding Modal Logic. Ph.D. thesis, Universite deToulouse.Bittencourt, G. (2007). Combining syntax semantics prime form representation. Journal Logic Computation, 18 (1), 1333.126fiPrime Implicates Prime Implicants Modal LogicBlackburn, P., de Rijke, M., & Venema, Y. (2001). Modal logic. Cambridge UniversityPress.Blackburn, P., van Benthem, J., & Wolter, F. (Eds.). (2006). Handbook Modal Logic.Elsevier.Brandt, S., & Turhan, A. (2002). approach optimized approximation. ProceedingsKI-2002 Workshop Applications Description Logics (KIDLWS01).Cadoli, M., & Donini, F. M. (1997). survey knowledge compilation. AI Communications, 10 (3-4), 137150.Cialdea Mayer, M., & Pirri, F. (1995). Propositional abduction modal logic. LogicJournal IGPL, 3 (6), 907919.Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal ArtificialIntelligence Research, 17, 229264.de Kleer, J., Mackworth, A. K., & Reiter, R. (1992). Characterizing diagnoses systems.Artificial Intelligence, 56, 197222.Donini, F. M. (2003). Description Logic Handbook, chap. Complexity Reasoning.Cambridge University Press.Donini, F. M., Lenzerini, M., Nardi, D., Hollunder, B., Nutt, W., & Marchetti Spaccamela,A. (1992). complexity existential qualification concept languages. ArtificialIntelligence, 53, 309327.Eiter, T., & Makino, K. (2002). computing abductive explanations. ProceedingsEighteenth National Conference Artificial Intelligence (AAAI02), pp. 6267.Enjalbert, P., & Farinas del Cerro, L. (1989). Modal resolution clausal form. TheoreticalComputer Science, 65 (1), 133.Garey, M. R., & Johnson, D. S. (1979). Computers intractability. guide theoryNP-completeness. W. H. Freeman.Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. Proceedings Tenth InternationalConference Principles Knowledge Representation Reasoning (KR06), pp.187197.Giunchiglia, F., & Sebastiani, R. (1996). SAT-based decision procedure ALC.Proceedings Fifth International Conference Principles Knowledge Representation Reasoning (KR96), pp. 304314.Kusters, R., & Molitor, R. (2002). Approximating specific concepts logicsexistential restrictions. AI Communications, 15 (1), 4759.Ladner, R. (1977). computational complexity provability systems modal propositional logic. SIAM Journal Computing, 6 (3), 467480.Lakemeyer, G. (1995). logical account relevance. Proceedings FourteenthInternational Joint Conference Artificial Intelligence (IJCAI95), pp. 853861.127fiBienvenuLang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence: Formulavariable independence forgetting. Journal Artificial Intelligence Research, 18,391443.Marquis, P. (1991a). Contribution letude des methodes de construction dhypotheses enintelligence artificielle. french, Universite de Nancy I.Marquis, P. (1991b). Extending abduction propositional first-order logic. Proceedings Fundamentals Artificial Intelligence Research Workshop, pp. 141155.Marquis, P. (2000). Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5, chap. Consequence Finding Algorithms, pp. 41145. Kluwer.Pagnucco, M. (2006). Knowledge compilation belief change. ProceedingsNineteenth Australian Conference Artificial Intelligence (AI06), pp. 9099.Papadimitriou, C. (1994). Computational Complexity. Addison Welsey.Przymusinski, T. (1989). algorithm compute circumscription. Artificial Intelligence,38 (1), 4973.Ramesh, A., & Murray, N. (1994). Computing prime implicants/implicates regular logics.Proceedings Twenty-Fourth IEEE International Symposium MultipleValued Logic, pp. 115123.Schild, K. (1991). correspondence theory terminological logics: Preliminary report.Proceedings Twelth International Joint Conference Artificial Intelligence(IJCAI91), pp. 466471.Younger, D. H. (1967). Recognition parsing context-free languages time n3 .Information Control, 10 (2), 189208.128fi