Journal Artificial Intelligence Research 36 (2009) 229-266

Submitted 03/09; published 10/09

Relaxed Survey Propagation
Weighted Maximum Satisfiability Problem
Hai Leong Chieu

chaileon@dso.org.sg

DSO National Laboratories,
20 Science Park Drive, Singapore 118230

Wee Sun Lee

leews@comp.nus.edu.sg

Department Computer Science, School Computing,
National University Singapore, Singapore 117590

Abstract
survey propagation (SP) algorithm shown work well large instances
random 3-SAT problem near phase transition. shown SP estimates
marginals covers represent clusters solutions. SP-y algorithm generalizes
SP work maximum satisfiability (Max-SAT) problem, cover interpretation
SP generalize SP-y. paper, formulate relaxed survey propagation (RSP) algorithm, extends SP algorithm apply weighted Max-SAT
problem. show RSP interpretation estimating marginals covers
violating set clauses minimal weight. naturally generalizes cover interpretation SP. Empirically, show RSP outperforms SP-y state-of-the-art
Max-SAT solvers random Max-SAT instances. RSP also outperforms state-of-the-art
weighted Max-SAT solvers random weighted Max-SAT instances.

1. Introduction
3-SAT problem archetypical NP-complete problem, difficulty solving
random 3-SAT instances shown related clause variable ratio,
= M/N , number clauses N number variables. phase
transition occurs critical value c 4.267: random 3-SAT instances < c
generally satisfiable, instances > c not. Instances close phase
transition generally hard solve using local search algorithms (Mezard & Zecchina,
2002; Braunstein, Mezard, & Zecchina, 2005).
survey propagation (SP) algorithm invented statistical physics community using approaches used analyzing phase transitions spin glasses (Mezard &
Zecchina, 2002). SP algorithm surprised computer scientists ability solve
efficiently extremely large difficult Boolean satisfiability (SAT) instances phase
transition region. algorithm also extended SP-y algorithm handle
maximum satisfiability (Max-SAT) problem (Battaglia, Kolar, & Zecchina, 2004).
Progress made understanding SP algorithm works well. Braunstein
Zecchina (2004) first showed SP viewed belief propagation (BP)
algorithm (Pearl, 1988) related factor graph clusters solutions represented
covers non-zero probability. known whether similar interpretation
given SP-y algorithm. paper, extend SP algorithm handle weighted
c
2009
AI Access Foundation. rights reserved.

fiChieu & Lee

Max-SAT instances way preserves cover interpretation, call new
algorithm Relaxed Survey Propagation (RSP) algorithm. Empirically, show
RSP outperforms SP-y state-of-the-art solvers random Max-SAT instances.
also outperforms state-of-the-art solvers benchmark Max-SAT instances.
random weighted Max-SAT instances, outperforms state-of-the-art weighted Max-SAT
solvers.
rest paper organized follows. Section 2, describe background
literature mathematical notations necessary understanding paper. includes
brief review definition joint probability distributions factor graphs,
introduction SAT, Max-SAT weighted Max-SAT problem,
formulated inference problems probability distribution factor graph.
Section 3, give review BP algorithm (Pearl, 1988), plays central role
paper. Section 4, give description SP (Braunstein et al., 2005)
SP-y (Battaglia et al., 2004) algorithm, explaining warning propagation algorithms.
Section 5, define joint distribution extended factor graph given weighted
Max-SAT instance. factor graph generalizes factor graph defined Maneva,
Mossel Wainwright (2004) Chieu Lee (2008). show that, solving
SAT instances, running BP algorithm factor graph equivalent running
SP algorithm derived Braunstein, Mezard Zecchina (2005). weighted
Max-SAT problem, gives rise new algorithm call Relaxed Survey
Propagation (RSP) algorithm. Section 7, show empirically RSP outperforms
algorithms solving hard Max-SAT weighted Max-SAT instances.

2. Background
SP first derived principles statistical physics, understood
BP algorithm, estimating marginals joint distribution defined factor graph.
section, provide background material joint distributions defined
factor graphs. define Boolean satisfiability (SAT) problem, maximum
satisfiability (Max-SAT) problem, weighted maximum satisfiability (weighted MaxSAT) problem, show problems solved solving inference problem
joint distributions defined factor graphs. review definition derivation
BP algorithm follow next section, describe SP algorithm
Section 4.
2.1 Notations
First, define notations concepts relevant inference problems
factor graphs. Factor graphs provide framework reasoning manipulating joint
distribution set variables. general, variables could continuous nature,
paper, limit discrete random variables.
paper, denote random variables using large Roman letters, e.g., X, .
random variables always discrete paper, taking values finite domain. Usually,
interested vectors random variables, write letters bold
face, e.g., X, Y. often index random variables letters i, j, k..., write,
example, X = {Xi }iV , V finite set. subset W V , denote
230

fiRelaxed Survey Propagation Weighted Max-SAT Problem


X1

'
X2

''
X3

X4

Figure 1: simple factor graph p(x) = (x1 , x2 ) 0 (x1 , x3 ) 00 (x2 , x4 ).
XW = {Xi }iW . call assignment values variables X configuration,
denote small bold letters, e.g. x. often write x represent event
X = x and, probability distribution p, write p(x) mean p(X = x). Similarly,
write x denote event X = x, write p(x) denote p(X = x).
recurring theme paper defining message passing algorithms joint
distributions factor graphs (Kschischang, Frey, & Loeliger, 2001). joint distribution
defined product local functions (functions defined small subset variables),
refer local functions factors. index factors, e.g. , Greek
letters, e.g., , (avoiding used symbol clause variable ratio SAT
instances). factor , denote V () V subset variables
defined, i.e. function defined variables XV () . message passing
algorithms, messages vectors real numbers sent factors variables
vice versa. vector message sent variable Xi factor denoted
Mi , message Xi denoted Mi .
2.2 Joint Distributions Factor Graphs
Given large set discrete, random variables X = {Xi }iV , interested joint
probability distribution p(X) variables. set V large, often
interest assume simple decomposition, draw conclusions efficiently
distribution. paper, interested joint probability distribution
decomposed follows
p(X = x) =

1
(xV() ),
Z F

(1)

set F indexes set functions { }F . function defined
subset variables XV () set X, maps configurations xV () non-negative
real numbers. Assuming function defined small subset variables
XV () , hope efficient inference distribution, despite large number
variables X. constant Z normalization constant, ensures
distribution sums one configurations x X.
factor graph (Kschischang et al., 2001) provides useful graphical representation
illustrating dependencies defined joint probability distribution Equation 1.
factor graph G = ({V, F }, E), bipartite graph two sets nodes, set variable
nodes, V , set factor nodes, F . set edges E factor graph connects
variable nodes factor nodes, hence bipartite nature graph. factor graph
representing joint distribution Equation 1, edge e = (, i) E
231

fiChieu & Lee

variable Xi parameter factor (i.e. V ()). denote V (i)
set factors depending variable Xi , i.e.
V (i) = { F | V ()}

(2)

show simple example factor graph Figure 1. small example,
example, V () = {1, 2} V (2) = {, 00 }. factor graph representation useful
illustrating inference algorithms joint distributions form Equation 1 (Kschischang et al., 2001). Section 3, describe BP algorithm using factor
graph representation.
Equation 1 defines joint distribution product local factors. often useful
represent distribution following exponential form:
p(x) = exp (

X

(xV () ) )

(3)

F

equation reparameterization Equation 1, (xV () ) = exp( (xV () ))
= ln Z. statistical physics, exponential form often written follows:
p(x) =

1
1
exp(
E(x)),
Z
kB

(4)

E(x) Hamiltonian energy function, kB Boltzmanns constant,
temperature. simplicity, set kB = 1, Equations 3 4 equivalent
P
E(x) = F (xV () ).
Bayesian (belief) networks Markov random fields two graphical representations often used describe multi-dimensional probability distributions. Factor graphs
closely related Bayesian networks Markov random fields, algorithms
operating factor graphs often directly applicable Bayesian networks Markov
random fields. refer reader work Kschischang et al. (2001) comparison
factor graphs, Bayesian networks Markov random fields.
2.3 Inference Joint Distributions
literature, inference joint distribution refer solving one two problems.
define two problems follows:
Problem 1 (MAP problem). Given joint distribution, p(x), interested configuration(s) highest probability. configurations, x , called maximuma-posteriori configurations, MAP configurations
x = arg max p(x)
x

(5)

joint distribution Equation 4, MAP configuration minimizes energy
function E(x), hence MAP problem sometimes called energy minimization
problem.

232

fiRelaxed Survey Propagation Weighted Max-SAT Problem

Problem 2 (Marginal problem). Given joint distribution, p(x), central interest
calculation estimation probabilities events involving single variable Xi = xi .
refer probabilities marginal probabilities:
X

pi (xi ) =

p(x).

(6)

x\xi

P

notation x\xi means summing configurations X variable Xi set
xi . Marginals important represent underlying distribution individual
variables.
general, problems solvable reasonable time currently known methods. Naive calculation pi (xi ) involves summing probabilities configurations
variables X Xi = xi . example, factor graph n variables
cardinality q, finding marginal one variables involve summing q n1
configurations. Furthermore, NP-complete problems 3-SAT simply coded
factor graphs (see Section 2.4.1). such, MAP problem general NP-complete,
marginal problem equivalent model counting 3-SAT, #P-complete
(Cooper, 1990). Hence, general, expect solve inference problems (exactly) reasonable time, unless problems small, special structures
exploited efficient inference.
central interest paper particular approximate inference method known
(sum-product) belief propagation (BP) algorithm. defer discussion
BP algorithm next section. rest section, describe SAT,
Max-SAT weighted Max-SAT problems, simply formulated
inference problems joint distribution factor graph.
2.4 SAT Max-SAT Problem
variable Boolean takes values {FALSE, TRUE}. paper, follow
conventions statistical physics, Boolean variables take values {1, +1}, 1
corresponding FALSE, +1 corresponding TRUE.
Boolean satisfiability (SAT) problem given Boolean propositional formula
written operators (conjunction), (disjunction), (negation),
parenthesis. objective SAT problem decide whether exists configuration propositional formula satisfied (evaluates TRUE). SAT
problem first problem shown NP-complete Stephen Cooks seminal paper
1971 (Cook, 1971; Levin, 1973).
three operators Boolean algebra defined follows: given two propositional
formulas B, OR(A, B) true either B true; AND(A, B) true
B true; NOT(A) true false. rest paper, use
standard notations Boolean algebra Boolean operators: B means OR(A, B),
B means AND(A, B), means NOT(A). parenthesis available allow
nested application operators, e.g. (A B) (B C).
conjunctive normal form (CNF) often used standard form writing Boolean
formulas. CNF consists conjunction disjunctions literals, literal
either variable negation. example, (X1 X 2 ) (X 3 X4 ) CNF,
233

fiChieu & Lee

X1 X2 (X1 X2 ) (X2 X3 ) not. Boolean formula re-written CNF
using De Morgans law distributivity law, although practice, may lead
exponential blowup size formula, Tseitin transformation often used
instead (Tseitin, 1968). CNF, Boolean formula considered conjunction
set clauses, clause disjunction literals. Hence, SAT problem
often given (X, C), X vector Boolean variables, C set
clauses. clause C satisfied configuration evaluates TRUE
configuration. Otherwise, said violated configuration. use Greek
letters (e.g. , ) indices clauses C, denote V () set variables
clause C. K-SAT problem SAT problem clause C consists
exactly K literals. K-SAT problem NP-complete, K 3 (Cook, 1971).
maximum satisfiability problem (Max-SAT) problem optimization version
SAT problem, aim minimize number violated constraints
formula. define simple working example Max-SAT problem use
throughout paper:
Example 1. Define instance Max-SAT problem CNF following clauses
1 = (x1 x2 ), 2 = (x2 x3 ), 3 = (x3 x1 ), 4 = (x1 x2 x3 ), 5 = (x1 x2 x3 ) 6 =
(x1 x2 ). Boolean expression representing problem would
(x1 x2 ) (x2 x3 ) (x3 x1 ) (x1 x2 x3 ) (x1 x2 x3 ) (x1 x2 ).

(7)

objective Max-SAT problem would find configuration minimizing
number violated clauses.
2.4.1 Factor Graph Representation SAT Instances
SAT problem CNF easily represented joint distribution factor
graph. following definition, give possible definition joint distribution
Boolean configurations given SAT instance, Boolean variables take values
{1, +1}.
Definition 1. Given instance SAT problem, (X, C) conjunctive normal form,
X vector N Boolean variables. define energy, E(x), distribution, p(x), configurations SAT instance (Battaglia et al., 2004)
C, C (xV () ) =
E(x) =

1
(1 + J,i xi ),
2
iV ()

(8)

X

C (xV () ),

(9)

1
exp(E(x)),
Z

(10)



C

p(x) =

x {1, +1}N , J,i takes values {1, +1}. J,i = +1 (resp. 1),
contains Xi negative (resp. positive) literal. clause satisfied one
variables Xi takes value J,i . clause satisfied, C (xV () ) = 0. Otherwise
C (xV () ) = 1.
234

fiRelaxed Survey Propagation Weighted Max-SAT Problem

6

1

3

X1

2

X2

5

X3

4

Figure 2: factor graph SAT instance given Example 1. Dotted (resp. solid)
lines joining variable clause means variable negative (resp. positive)
literal clause.

definition, energy function zero satisfying configurations,
equals number violated clauses non-satisfying configuration. Hence, satisfying
configurations SAT instance MAP configurations factor graph.
section, make definitions useful rest paper.
clause containing variable Xi (associated value J,i ), say
Xi satisfies Xi = J,i . case, clause satisfied regardless values
taken variables. Conversely, say Xi violates Xi satisfy .
case, still possible satisfied variables.
Definition 2. clause C, define u,i (resp. s,i ) value Xi {1, +1}
violates (resp. satisfies) clause . means s,i = J,i u,i = +J,i .
define following sets
V + (i)
V (i)
Vs (i)
Vu (i)

=
=
=
=

{ V (i); s,i = +1},
{ V (i); s,i = 1},
{ V (i) \ {}; s,i = s,i },
{ V (i) \ {}; s,i 6= s,i }.

(11)

definitions, V + (i) (resp. V (i)) set clauses contain Xi
positive literal (resp. negative literal). Vs (i) (resp. Vu (i)) set clauses containing
Xi agrees (resp. disagrees) clause concerning Xi . sets useful
define SP message passing algorithms SAT instances.
factor graph representing Max-SAT instance given Example 1 shown
Figure 2. example, V + (1) = {3 , 5 , 6 }, V (1) = {1 , 4 }, Vs3 (1) = {5 , 6 },
Vu3 (1) = {1 , 4 }. energy example follows:
1
1
1
E(x) = (1 + x1 )(1 x2 ) + (1 + x2 )(1 x3 ) + (1 + x3 )(1 x1 ) +
4
4
4
1
1
1
(1 + x1 )(1 + x2 )(1 + x3 ) + (1 x1 )(1 x2 )(1 x3 ) + (1 x1 )(1 x2 ) (12)
8
8
4

235

fiChieu & Lee

2.4.2 Related Work SAT
SAT problem well studied computer science: archetypical NP-complete
problem, common reformulate NP-complete problems graph coloring
SAT problem (Prestwich, 2003). SAT solvers either complete incomplete.
best known complete solver solving SAT problem probably Davis-PutnamLogemann-Loveland (DPLL) algorithm (Davis & Putnam, 1960; Davis, Logemann, & Loveland, 1962). DPLL algorithm basic backtracking algorithm runs choosing
literal, assigning truth value it, simplifying formula recursively checking
simplified formula satisfiable; case, original formula satisfiable; otherwise, recursive check done assuming opposite truth value. Variants
DPLL algorithm Chaff (Moskewicz & Madigan, 2001), MiniSat (Een & Sorensson,
2005), RSAT (Pipatsrisawat & Darwiche, 2007) among best performers recent SAT competitions (Berre & Simon, 2003, 2005). Solvers satz (Li & Anbulagan,
1997) cnfs (Dubois & Dequen, 2001) also making progress solving hard
random 3-SAT instances.
solvers participated recent SAT competitions complete solvers.
incomplete stochastic solvers show SAT instance unsatisfiable,
often able solve larger satisfiable instances complete solvers. Incomplete solvers
usually start randomly initialized configuration, different algorithms differ
way flip selected variables move towards solution. One disadvantage
approach hard SAT instances, large number variables flipped
move current configuration local minimum, acts local trap. Incomplete
solvers differ strategies used move configuration traps. example,
simulated annealing (Kirkpatrick, Jr., & Vecchi, 1983) allows search move uphill,
controlled temperature parameter. GSAT (Selman, Levesque, & Mitchell, 1992)
WalkSAT (Selman, Kautz, & Cohen, 1994) two algorithms developed 1990s
allow randomized moves solution cannot improved locally. two algorithms
differ way choose variables flip. GSAT makes change minimizes
number unsatisfied clauses new configuration, WalkSAT selects
variable that, flipped, results previously satisfied clauses becoming unsatisfied.
Variants algorithms WalkSAT GSAT use various strategies, tabusearch (McAllester, Selman, & Kautz, 1997) adapting noise parameter used,
help search local minima (Hoos, 2002). Another class approaches based
applying discrete Lagrangian methods SAT constrained optimization problem
(Shang & Wah, 1998). Lagrange mutlipliers used force lead search
local traps.
SP algorithm (Braunstein et al., 2005) shown beat best incomplete
solvers solving hard random 3-SAT instances efficiently. SP estimates marginals
variables chooses fix truth value. size instance
reduced removing variables, SP run remaining instance.
iterative process called decimation SP literature. shown empirically SP
rarely makes mistakes decimation, SP solves large 3-SAT instances
hard local search algorithms. Recently, Braunstein Zecchina (2006)
236

fiRelaxed Survey Propagation Weighted Max-SAT Problem

shown modifying BP SP updates reinforcement term, effectiveness
algorithms solvers improved.
2.5 Weighted Max-SAT Problem
weighted Max-SAT problem generalization Max-SAT problem,
clause assigned weight. define instance weighted Max-SAT problem
follows:
Definition 3. weighted Max-SAT instance (X, C, W) CNF consists X, vector
N variables taking values {1, +1}, C, set clauses, W, set weights
clause C. define energy weighted Max-SAT problem
E(x) =

w
(1 + J,i xi ),
2
C iV ()
X



(13)

x {1, +1}N , J,i takes values {1, +1}, w weight clause
. total energy, E(x), configuration x equals total weight violated clauses.
Similarly SAT, also complete incomplete solvers weighted MaxSAT problem. Complete weighted Max-SAT solvers involve branch bound techniques
calculating bounds cost function. Larrosa Heras (2005) introduced framework
integrated branch bound techniques Max-DPLL algorithm solving
Max-SAT problem. Incomplete solvers generally employ heuristics similar
used SAT problems. example incomplete method min-conflicts hillclimbing random walks algorithm (Minton, Philips, Johnston, & Laird, 1992). Many
SAT solvers WalkSAT extended solve weighted Max-SAT problems,
weights used criterion selection variables flip.
working example paper, define following instance weighted
Max-SAT problem:
Example 2. define set weighted Max-SAT clauses following table:

Id
1
2
3
4
5
6

Clause
Weight
x1 x2
1
x2 x3
2
x3 x1
3
x1 x2 x3
4
x1 x2 x3
5
x1 x2
6
Energy

--3
3
3
3
5
5
1

--+
3
3
5
3
3
5
9

-+3
5
3
3
3
3
2

-++
3
3
5
3
3
3
3

+-5
3
3
3
3
3
1

+-+
5
3
3
3
3
3
1

++3
5
3
3
3
3
2

+++
3
3
3
5
3
3
4

weighted Max-SAT example variables clauses Max-SAT
example given Example 1. table, show clauses satisfied (a tick)
violated (a cross) 8 possible configurations 3 variables. first
237

fiChieu & Lee

row, symbol corresponds value 1, + corresponds +1. example,
string + corresponds configuration (X1 , X2 , X3 ) = (1, 1, +1). last row
table shows energy configuration column.
factor graph weighted Max-SAT example one
Max-SAT example Example 1. differences two examples
clause weights, reflected joint distribution, factor graph.
energy example follows:
1
2
3
E(x) = (1 + x1 )(1 x2 ) + (1 + x2 )(1 x3 ) + (1 + x3 )(1 x1 ) +
4
4
4
4
5
6
(1 + x1 )(1 + x2 )(1 + x3 ) + (1 x1 )(1 x2 )(1 x3 ) + (1 x1 )(1 x2 )(14)
8
8
4
2.6 Phase Transitions
SP algorithm shown work well 3-SAT instances near phase transition,
instances known hard solve. term phase transition arises
physics community. understand notion hardness optimization
problems, computer scientists physicists studying relationship
computational complexity computer science phase transitions statistical physics.
statistical physics, phenomenon phase transitions refers abrupt changes
one physical properties thermodynamic magnetic systems small
change value variable temperature. computer science,
observed random ensembles instances K-SAT, sharp
threshold randomly generated problems undergo abrupt change properties.
example, K-SAT, observed empirically clause variable ratio
changes, randomly generated instances change abruptly satisfiable unsatisfiable
particular value , often denoted c . Moreover, instances generated value
close c found extremely hard solve.
Computer scientists physicists worked bounding calculating precise value c phase transition 3-SAT occurs. Using cavity approach,
physicists claim c 4.267 (Mezard & Zecchina, 2002). derivation
value c non-rigorous, based derivation formulated SP
algorithm. Using rigorous mathematical approaches, upper bounds value c
derived using first-order methods. example, work Kirousis, Kranakis,
Krizanc, Stamatiou (1998), c 3-SAT upper bounded 4.571. Achlioptas,
Naor Peres (2005) lower-bounded value c using weighted second moments
method, lower bound close upper bounds K-SAT ensembles large
values K. However, lower bound 3-SAT 2.68, rather far conjectured
value 4.267. better (algorithmic) lower bound 3.52 obtained analyzing
behavior algorithms find SAT configurations (Kaporis, Kirousis, & Lalas, 2006).
Physicists also shown rigorously using second moment methods approaches c , search space fractures dramatically, many small solution clusters
appearing relatively far apart (Mezard, Mora, & Zecchina, 2005). Clusters
solutions generally defined set connected components solution space,
two adjacent solutions Hamming distance 1 (differ one variable). Daude,
238

fiRelaxed Survey Propagation Weighted Max-SAT Problem

k

'

M'j

M''j
''

Mk

Mj

j



Mi



Ml
l

Figure 3: Illustration messages BP algorithm.
Mezard, Mora, Zecchina (2008) redefined notion clusters using concept
x-satisfiability: SAT instance x-satisfiable exists two solutions differing
N x variables, N total number variables. showed near phase
transition, x goes around 12 small values, without going phase
intermediate values. clustering phenomenon explains instances generated
close c extremely hard solve local search algorithm: difficult
local search algorithm move local minimum global minimum.

3. Belief Propagation Algorithm
BP algorithm reinvented different fields different names. example,
speech recognition community, BP algorithm known forward-backward
procedure (Rabiner & Juang, 1993). tree-structured factor graphs, BP algorithm
simply dynamic programming approach applied tree structure, shown
BP calculates marginals variable factor graph (i.e. solving Problem
2). loopy factor graphs, BP algorithm found provide reasonable
approximation solving marginal problem algorithm converges. case,
BP algorithm often called loopy BP algorithm. Yedidia, Freeman Weiss (2005)
shown fixed points loopy BP algorithm correspond stationary
points Bethe free energy, hence sensible approximate method estimaing
marginals.
section, first describe BP algorithm dynamic programming
method solving marginal problem (Problem 2) tree-structured factor graphs.
also briefly describe BP algorithm applied factor graphs loops,
refer reader work Yedidia et al. (2005) underlying theoretical
justification case.
Given factor graph representing distribution p(x), BP algorithm involves iteratively passing messages factor nodes F variable nodes V , vice versa.
factor node represents factor , factor joint distribution given
Equation 1. Figure 3, give illustration messages passed
factor nodes variable nodes. Greek alphabet (e.g. F ) square represents
factor (e.g. ) Roman alphabet (e.g. V ) circle represents variable
(e.g. Xi ).
factor variable messages (e.g. Mi ), variable factor messages (e.g.
Mi ) vectors real numbers, length equal cardinality variable Xi .
239

fiChieu & Lee

denote Mi (xi ) Mi (xi ) component vector corresponding
value Xi = xi .
message update equations follows:


Mj (xj ) =

0 V

(15)

(j)\

X

Mi (xi ) =

0 j (xj )
(xV () )

xV () \xi



Mj (xj ),

(16)

jV ()\i

P

xV () \xi means summing configurations XV () Xi set xi .
tree-structured factor graph, message updates scheduled
two parses tree structure, messages converge. messages converge,
beliefs variable node calculated follows:


Bj (xj ) =

Mj (xj ).

(17)

V (j)

tree-structured graph, normalized beliefs variable equal
marginals.
INPUT: joint distribution p(x) defined tree-structured factor graph ({V, F }, E),
variable Xi X.
OUTPUT: Exact marginals variable Xi .
ALGORITHM :
1. Organize tree Xi root tree.
2. Start leaves, propagate messages child nodes parent nodes
right root Xi Equations 15 16.
3. marginals Xi obtained normalized beliefs Equation 17.

Figure 4: BP algorithm calculating marginal single variable, Xi ,
tree-structured factor graph

algorithm calculating exact marginals given variable Xi , given
Figure 4. algorithm simply dynamic programming procedure calculating
marginals, pi (Xi ), organizing sums sums leaves done first.
simple example Figure 1, calculating p1 (x1 ), sum ordered follows:
p1 (x1 ) =

X

p(x)

x2 ,x3 ,x4

= (x1 , x2 )

X X

.

x2

x3

240

0 (x1 , x3 )

X
x4

00 (x2 , x4 )

fiRelaxed Survey Propagation Weighted Max-SAT Problem

BP algorithm simply carries sum using node X1 root
tree-structured factor graph Figure 1.
BP algorithm also used calculating marginals variables efficiently,
message passing schedule given Figure 5. schedule involves selecting
random variable node root tree, passing messages leaves
root, back leaves, two parses, message updates
required algorithm Figure 4 one variable would performed,
beliefs variables calculated messages. normalized beliefs
variable equal marginals variable.

INPUT: joint distribution p(x) defined tree-structured factor graph (V, F ).
OUTPUT: Exact marginals variables V .
ALGORITHM :
1. Randomly select variable root.
2. Upward pass: starting leaves, propagate messages leaves right
tree.
3. Downward pass: root, propagate messages back leaves.
4. Calculate beliefs variables given Equation 17.

Figure 5: BP algorithm calculating marginals variables treestructured factor graph

factor graph tree-structured (i.e. contains loops), message updates
cannot scheduled simple way described algorithm Figure 5. case,
still apply BP iteratively updating messages Equations 15 16, often
round-robin manner factor-variable pairs. done messages
converge (i.e. messages change iterations). guarantee
messages converge general factor graphs. However, converge,
observed beliefs calculated Equation 17 often good approximation
exact beliefs joint distribution (Murphy, Weiss, & Jordan, 1999).
applied manner, BP algorithm often called loopy BP algorithm. Recently,
Yedidia, Freeman Weiss (2001, 2005) shown loopy BP underlying
variational principle. showed fixed points BP algorithm correspond
stationary points Bethe free energy. fact serves sense justify
BP algorithm even factor graph operates loops, minimizing
Bethe free energy sensible approximation procedure solving marginal problem.
refer reader work Yedidia et al. (2005) details.
241

fiChieu & Lee

4. Survey Propagation: SP SP-y Algorithms
Recently, SP algorithm (Braunstein et al., 2005) shown beat best
incomplete solvers solving hard 3-SAT instances efficiently. SP algorithm first
derived principles statistical physics, explained using cavity approach
(Mezard & Parisi, 2003). first given BP interpretation work Braunstein
Zecchina (2004). section, define SP SP-y algorithms
solving SAT Max-SAT problems, using warning propagation interpretation
algorithms.
4.1 SP Algorithm SAT Problem
Section 2.4.1, defined joint distribution SAT problem (X, C),
energy function configuration equal number violated clauses configuration. factor graph ({V, F }, E) representing joint distribution, variable
nodes V correspond Boolean variables X, factor node F represents
clause C. section, provide intuitive overview SP algorithm
formulated work Braunstein et al. (2005).
SP algorithm defined message passing algorithm factor graph
({V, F }, E). factor F passes single real number, neighboring variable
Xi factor graph. real number called survey. According warning
propagation interpretation given work Braunstein et al. (2005), survey
corresponds probability1 warning factor sending variable
Xi . Intuitively, close 1, factor warning variable Xi
taking value violate clause . close 0, factor
indifferent value taken Xi , clause satisfied
variables V ().
first define messages sent variable Xj neighboring factor ,
function inputs factors containing Xj , i.e. { 0 j } 0 V (j)\ . SP,
message vector three numbers, uj , sj , 0j , following
interpretations:
uj probability Xj warned (by clauses) take value violate
clause .
sj probability Xj warned (by clauses) take value satisfy
clause .
0
j probability Xj free take value.
defintions, update equations follows:
uj = [1
sj = [1



(1 0 j )]



(1 0 j ),

0 Vu (j)

0 Vs (j)





(1 0 j )]

0 Vs (j)

(1 0 j ),

(18)
(19)

0 Vu (j)

1. SP reasons clusters solutions, probability warning section used loosely
SP literature refer fraction clusters warning applies. next section,
define rigorous probability distribution covers RSP algorithm.

242

fiRelaxed Survey Propagation Weighted Max-SAT Problem

0j =



(1 0 j ),

(20)

0 V (j)

=



uj

jV ()i

uj + sj + 0j

(21)

equations defined using sets factors Vu (j) Vs (j),
defined Section 2.4.1. event variable Xj warned take value
violating , (a) warned least one factor 0 Vu (j) take satisfying
value 0 , (b) factors Vs (j) sending warnings. Equation
18, probability event, uj , product two terms, first corresponding
event (a) second event (b). definitions sj 0j defined
similar manner. Equation 21, final survey simply probability joint
event incoming variables Xj violating clause , forcing last variable Xi
satisfy .
SP algorithm consists iteratively running update equations
surveys converge. surveys converged, calculate local biases follows:
= [1
+
j



(1 0 j )]


V

0j

=



(1 j ),

(22)

V (j)

V + (j)

= [1
+
j



(1 0 j )]

(j)


V

(1 j ),

(1 j ),

(23)

+ (j)

(24)

V (j)

Wi+ =
Wi =

+
j

(25)


0
+
j + j + j


j

(26)


0
+
j + j + j

solve instances SAT problem, SP algorithm run converges,
variables highly constrained set preferred values. SAT
instance reduced smaller instance, SP run smaller
instance. continues SP fails set variables, case, local
search algorithm WalkSAT run remaining instance. algorithm, called
survey inspired decimation algorithm (Braunstein et al., 2005), given algorithm
Figure 6.
4.2 SP-y Algorithm
contrast SP algorithm, SP-y algorithms objective solve Max-SAT instances, hence clauses allowed violated, price. SP algorithm
understood special case SP-y algorithm, taken infinity (Battaglia
et al., 2004). SP-y, penalty value exp(2y) multiplied distribution
violated clause. Hence, although message passing algorithm allows violation
clauses, value increases, surveys prefer configurations violate
minimal number clauses.
243

fiChieu & Lee

INPUT: SAT problem, constant k.
OUTPUT: satisfying configuration, report FAILURE.
ALGORITHM :
1. Randomly initialize surveys.
2. Iteratively update surveys using Equations 18 21.
3. SP converge, go step 7.
4. SP converges, calculate Wi+ Wi using Equations 25 26.
5. Decimation: sort variables based absolute difference |Wi+ Wi |,
set top k variables preferred value. Simplify instance
variables removed.
6. surveys equal zero, (no variables removed step 5), output
simplified SAT instance. Otherwise, go back first step smaller
instance.
7. Run WalkSAT remaining simplified instance, output satisfying
configuration WalkSAT succeeds. Otherwise output FAILURE.

Figure 6: survey inspired decimation (SID) algorithm solving SAT problem
(Braunstein et al., 2005)

SP-y algorithm still understood message passing algorithm factor
graphs. SP, factor, , passes survey, , neighboring variable Xi ,
+
corresponding probability warning. simplify notations, define

(resp.
) probability warning taking value +1 (resp. 1),
+

0
define
= 1

. practice, since clause warn
J

J

,i
+

either +1 1 both, either

equals zero:
= , i,i = 0,
J,i defined Definition 1.
Since clauses violated, insufficient simply keep track whether variable
warned value not. necessary keep track many
times variable warned value, know many clauses
+

violated variable take particular value. Let Hj
(resp. Hj
)
0
number times variable Xj warned factors { } 0 V (j)\ value
+
+1 (resp. 1). SP-y, variable Xj forced take value +1 Hj


+
smaller Hj , penalty case exp(2yHj ). notations used
+

work Battaglia et al. (2004) describing SP-y, let hj = Hj
Hj
.
Battaglia et al. (2004) defined SP-y message passing equations calculate
probability distribution hj , based input surveys,

{ 0 j } 0 V (j)\ = {1 j , 2 j , ..., (|V (j)|1) j },
244

(27)

fiRelaxed Survey Propagation Weighted Max-SAT Problem

|V (j)| refers cardinality set V (j). unnormalized distributions
Pej (h) calculated follows:
(1)

Pej (h) = 01 (h) + +1 (h 1) + 1 (h + 1), (28)
[2, |V (j)| 1],

()

(1)

Pej (h) = 0 Pej (h)
(1)

++ Pej (h 1) exp [2y(h)]
(1)

+ Pej (h + 1) exp [2y(h)],
(|V (j)|1)

Pej (h) = Pej

(h),

(29)
(30)

(h) = 1 h = 0, zero otherwise, (h) = 1 h 0, zero otherwise.
equations take account neighbor j excluding , = 1
= |V (j)|1. penalties exp(2y) multiplied every time value hj decreases
absolute value, new neighbor Xj , , added. end procedure,
+

equivalent multiplying messages factor exp(2ymin(Hj
, Hj
)).
Pej (h) normalized Pj (h) computing Pej (h) possible
values h [|V (j)| + 1, |V (j)| 1]. message updates surveys follows:
|V (j)|1
+
Wj
=

X

Pj (h),

h=1
1
X


Wj
=

Pj (h),

(31)
(32)

h=|V (j)|+1
J

i,i

= 0,

J

,i

=

(33)
J



,j
Wj
,

(34)

jV (j)\i
J

,i
0

= 1
,

(35)

+

quantity Wj
(resp. Wj
) probability events warning value
+1 (resp. 1). Equation 34 reflects fact warning sent variable
Xi variables warning going violate .
SP-y converges, preference variable calculated follows:

|V (j)|

Wj+ =
Wj =

X

Pj (h),

h=1
1
X

Pj (h),

(36)
(37)

h=|V (j)|

Pj (h) calculated similar manner Pj (h), except
exclude calculations.
definitions message updates, SP-y algorithm used
solve Max-SAT instances survey inspired decimation algorithm similar one
245

fiChieu & Lee

SP given algorithm Figure 6. iteration decimation process, SP-y
decimation procedure selects variables fix preferred values based quantity
bf ix (j) = |Wj+ Wj |

(38)

work Battaglia et al. (2004), additional backtracking process introduced make decimation process robust. backtracking process allows
decimation procedure unfix variables already fixed values. variable Xj
fixed value xj , following quantities calculated:
bbacktrack (j) = xj (Wj+ Wj )

(39)

Variables ranked according quantity top variables chosen
unfixed. algorithm Figure 7, show backtracking decimation algorithm
SP-y (Battaglia et al., 2004), value either given input,
determined empirically.
INPUT: Max-SAT instance constant k. Optional input: yin backtracking
probability r.
OUTPUT: configuration.
ALGORITHM :
1. Randomly initialize surveys.
2. yin given, set = yin . Otherwise, determine value bisection
method.
3. Run SP-y convergence. SP-y converges, variable Xi , extract
random number q [0, 1].
(a) q > r, sort variables according Equation 38 fix top k
biased variables.
(b) q < r sort variables according Equation 39 unfix top k
biased variables.
4. Simplify instance based step (3). SP-y converged return nonparamagnetic solution (a paramagnetic solution refers set {bf ix (j)}jV
biased value variables), go step (1).
5. Run weighted WalkSAT remaining instance outputs best configuration found.

Figure 7: survey inspired decimation (SID) algorithm solving Max-SAT instance
(Battaglia et al., 2004)

246

fiRelaxed Survey Propagation Weighted Max-SAT Problem

5. Relaxed Survey Propagation
shown (Maneva et al., 2004; Braunstein & Zecchina, 2004) SP SAT
problem reformulated BP algorithm extended factor graph. However,
formulation cannot generalized explain SP-y algorithm applicable
Max-SAT problems. previous paper (Chieu & Lee, 2008), extended formulation
work Maneva et al. (2004) address Max-SAT problem. section,
modify formulation previous paper (Chieu & Lee, 2008) address
weighted Max-SAT problem, setting extended factor graph run BP
algorithm. Theorem 3, show formulation generalizes BP interpretation
SP given work Maneva et al. (2004), main theorem (Theorem 2),
show running loopy BP algorithm factor graph estimates marginals
covers configurations violating set clauses minimal total weight.
first define concept covers Section 5.1, defining extended
factor graph Section 5.2. rest section, given weighted Max-SAT problem
(X, C, W), assume variables X take values {1, +1, }: third value
dont care state, corresponding no-warning message SP algorithm defined
Section 4.
5.1 Covers Weighted Max-SAT
First, need define semantics value dont care state.
Definition 4. (Maneva et al., 2004) Given configuration x, say variable Xi
unique satisfying variable clause C assigned s,i whereas
variables Xj clause assigned u,j (see Definition 2 definitions s,i
u,i ). variable Xi said constrained clause unique satisfying
variable . variable unconstrained constrained clauses. Define
CONi, (x ) = Ind(xi constrained ),

(40)

Ind(P ) equals 1 predicate P true, 0 otherwise.
illustration, consider configuration X = (+1, 1, 1) Example 2.
configuration, X1 = +1 constrained clauses 5 6 , X2 = 1 constrained
2 , X3 = 1 unconstrained: flipping X3 +1 violate additional
clauses configuration.
following definition, redefine configuration taking values {1, +1, }
satisfies violates clauses.
Definition 5. configuration satisfies clause (i) contains variable Xi
set value s,i , (ii) least two variables take value . configuration
violates clause variables Xj set u,j . configuration x invalid
clause exactly one variables set ,
remaining variables set u,i . configuration valid valid clauses
C.
definition invalid configurations reflects interpretation value
dont care state: clauses containing variable Xi = already satisfied
247

fiChieu & Lee

variables, value Xi matter. Xi = cannot last remaining
possibility satisfying clause. case clause contains two variables set
, clause satisfied either one two variables, variable
take dont care value.
define partial order set valid configurations follows (Maneva et al.,
2004):
Definition 6. Let x two valid configurations. write x i, (1) xi = yi
(2) xi = yi 6= .
partial order defines lattice, Maneva et al. (2004) showed SP
peeling procedure peels satisfying configuration minimal element
lattice. cover minimal element lattice. SAT region, cover
defined follows (Kroc, Sabharwal, & Selman, 2007):
Definition 7. cover valid configuration x {1, +1, }N satisfies clauses,
unconstrained variables assigned -1 +1.
SP algorithm shown return marginals covers (Maneva et al., 2004).
principle, two kinds covers: true covers correspond satisfying
configurations, false covers not. Kroc et al. (2007) showed empirically
number false covers negligible SAT instances. RSP apply weighted
Max-SAT instances, introduce notion v-cover:
Definition 8. v-cover valid configuration x {1, +1, }N
1. total weight clauses violated configuration equals v,
2. x unconstrained variables assigned -1 +1.
Hence covers defined Definition 7 simply v-covers v = 0 (i.e. 0-covers).
5.2 Extended Factor Graph
section, define joint distribution extended factor graph
positive v-covers. First, need define functions used
define factors extended factor graph.
Definition 9. clause, C, following function assigns different values
configurations satisfy, violate invalid (see Definition 5) :

VAL (xV () ) =



1

exp(w y)

0

xV () satisfies
xV () violates
xV () invalid

(41)

definition, introduced parameter RSP algorithm, plays
similar role SP-y algorithm. term exp(w y) penalty
violating clause weight w .
248

fiRelaxed Survey Propagation Weighted Max-SAT Problem

Definition 10. (Maneva et al., 2004) Given configuration x, define parent set
Pi (x) variable Xi set clauses Xi = xi unique satisfying
variable configuration x, (i.e. set clauses constraining Xi value). Formally,
Pi (x) = { C| CONi, (xV() ) = 1}

(42)

Example 2, configuration x = (+1, 1, 1), parent sets P1 (x) =
{5 , 6 }, P2 (x) = {2 }, P3 (x) = .
Given weighted Max-SAT instance (X, C, W) factor graph, G = ({V, F }, E),
construct another distribution associated factor graph Gs = ({V, Fs }, Es )
follows. V , let P (i) set possible parent sets variable Xi . Due
restrictions imposed definition, Pi (x) must contained either V + (i)
+

V (i), both. Therefore, cardinality P (i) 2|V (i)| +2|V (i)| 1. extended
factor graph defined set variables = (1 , 2 , ..., n ) X1 X2 ... Xn ,
Xi := {1, +1, } P (i). Hence factor graph number variables
original SAT instance, variable large cardinality. Given configurations
x SAT instance, denote configurations (x) = {i (x)}iV , (x) =
(xi , Pi (x)).
definitions given far define semantics valid configurations parent sets,
rest section, define factors extended factor graph Gs
ensure definitions satisfied configurations .
single variable compatibilities (i ) defined following factor
variable (x):
(i (x) = {xi , Pi (x)}) =



0

1


1

Pi (x) = , xi 6=
Pi (x) = , xi =
.
valid (xi , Pi (x))

(43)

first case definition Pi (x) = xi 6= corresponds case
variable Xi unconstrained, yet takes value {1, +1}. Valid configurations
v-covers (with unconstrained variables set 1 +1) zero value
factor. Hence v-covers positive value factors. last
case definition, validity (xi , Pi (x)) simply means xi = +1 (resp.
xi = 1), Pi (x) V + (i) (resp. Pi (x) V (i).).
clause compatibilities ( ) are:
((x)V () ) = VAL (xV() )

Q





kV () Ind [ Pk (x)] = CON,k (xV () ) ,

(44)

Ind defined Definition 4. clause compatibilities introduce penalties
VAL (xV () ) joint distribution. second term equation enforces
parent sets Pk (x) consistent definitions parent sets Definition 10
variable Xk clause .
values x determines uniquely values P = {Pi (x)}iV , hence
distribution (x) = {xi , Pi (x)}iV simply distribution x.
Theorem 1. Using notation UNSAT(x) represent set clauses violated
x, underlying distribution p() factor graph defined section positive
249

fiChieu & Lee

'6

'1

'1

'3

1

'2

2

'5

'2

3

'3

'4

Figure 8: extended factor graph SAT instance given Example 1. factor
nodes i0 correspond clause compatibility factors , single variable
factor nodes i0 represents single variable compatibility factors . factor
graph similar original factor graph SAT instance Figure 2, except
additional factor nodes i0 .

v-covers, v-cover x, have:
p(X = x) = p( = (x))



exp(w y),

(45)

UNSAT(x)

Proof. Configurations v-covers either invalid contains unconstrained
variables set 1 +1. invalid configurations, distribution zero
definition VAL , configurations unconstrained variables set 1 +1,
distribution zero due definition factors . v-cover, total
penalty violated clauses product term Equation 45.
definition defines joint distribution factor graph. RSP algorithm
message passing algorithm defined factor graph:
Definition 11. RSP algorithm defined loopy BP algorithm applied
extended factor graph Gs associated MaxSAT instance (X, C, W).
Section 6, formulate message passing updates RSP, well
decimation algorithm using RSP solver weighted Max-SAT instances.
example, Figure 8 shows extended factor graph weighted Max-SAT instance
defined Example 1.
Definition 12. define min-cover weighted Max-SAT instance m-cover,
minimum total weight violated clauses instance.
Theorem 2. taken , RSP estimates marginals min-covers
following sense: stationary points RSP algorithm correspond stationary
points Bethe free energy distribution uniform min-covers.
250

fiRelaxed Survey Propagation Weighted Max-SAT Problem

-1,-1,-1

Energy = 11

-1,-1,+1

Energy = 9

+1,+1,+1

Energy = 4

-1,+1,+1

Energy = 3
-1,+1,-1

+1,-1,-1

+1,-1,+1

+1,+1,-1
*,+1,-1

+1,-1,*

Energy = 2

Energy = 1

Figure 9: Energy landscape weighted Max-SAT instance given Example 2.
node represents configuration variables (x1 , x2 , x3 ). example,
node (1, +1, 1) represents configuration (x1 , x2 , x3 ) = (1, +1, 1).

Proof. ratio probability v-cover (v + )-cover equals exp(y).
taken , distribution Equation 45 positive min-covers.
Hence RSP, loopy BP algorithm factor graph representing Equation 45,
estimates marginals min-covers.
application RSP weighted Max-SAT instances, taking would often
cause RSP algorithm fail converge. Taking sufficiently large value often
sufficient RSP used solve weighted Max-SAT instances.
Figure 9, show v-covers small weighted Max-SAT example Example 2.
example, unique min-cover X1 = +1, X2 = 1, X3 = .
Maneva et al. (2004) formulated SP- algorithm, equivalent SP
algorithm (Braunstein et al., 2005) = 1. SP- algorithm loopy BP algorithm
extended factor graph defined work Maneva et al. (2004). Comparing
definitions extended factor graph factors RSP SP-, (Chieu &
Lee, 2008):
Theorem 3. taking , RSP equivalent SP- = 1.
Proof. definitions joint distribution SP- = 1 (Maneva et al., 2004),
RSP paper differ Definition 9, RSP,
definitions become identical. Since SP- RSP equivalent loopy BP
distribution defined extended factor graphs, equivalence joint
distribution means algorithms equivalent.
Taking infinity corresponds disallowing violated clauses, SP- formulated
satisfiable SAT instances, clauses must satisfied. SP-, clause weights
inconsequential clauses satisfied.
paper, disallow unconstrained variables take value . Appendix
A, give alternative definition single variable potentials Equation 43.
251

fiChieu & Lee

definition, Maneva et al. (2004) defines smoothing interpretation SP-.
smoothing also applied RSP. See Theorem 6 work Maneva et al. (2004)
Appendix details.
5.3 Importance Convergence
found message passing algorithms BP SP algorithms perform
well whenever converge (e.g., see Kroc, Sabharwal, & Selman, 2009). success
RSP algorithm random ensembles Max-SAT weighted Max-SAT instances
believed due clustering phenomenon problems, found
RSP could also successful cases clustering phenomenon observed.
believe presence large clusters help SP algorithm converge well,
long SP algorithm converges, presence clusters necessary good
performance.
covers simply Boolean configurations (with variables taking value),
represent singleton clusters. call covers degenerate covers. many structured
non random weighted Max-SAT problems, found covers found
often degenerate. previous paper (Chieu, Lee, & Teh, 2008), defined modified
version RSP energy minimization factor graphs, show Lemma 2
paper configurations * zero probability, i.e. covers degenerate.
paper, showed value tuned favor convergence
RSP algorithm.
Section 7.3, show success RSP benchmark Max-SAT instances.
trying recover covers configurations found RSP, found
benchmark instances used degenerate covers. fact RSP converged
instances sufficient RSP outperform local search algorithms.

6. Using RSP Solving Weighted Max-SAT Problem
previous section, defined RSP algorithm Definition 11 loopy BP
algorithm extended factor graph. section, derive RSP message
passing algorithm based definition, giving decimation-based algorithm
used solving weighted Max-SAT instances.
6.1 Message Passing Algorithm
variables extended factor graphs longer Boolean. form
(x) = (xi , Pi (x)), large cardinalities. definition BP algorithm,
stated message vector passed factors variables length
equal cardinality variables. section, show messages passed
RSP grouped groups, message passed variables
factors three values.
RSP, factor variable messages grouped follows:

Mi
xi = s,i , Pi (x) = {}, Vs (i),
(all cases variable xi constrained clause ),

252

fiRelaxed Survey Propagation Weighted Max-SAT Problem

u
Mi
xi = u,i , Pi (x) Vu (i),
(all cases variable xi constrained u,i clauses),

Mi
xi = s,i , Pi (x) Vs (i),
(all cases variable xi = s,i constrained . least one
variable xj satisfies equals . Otherwise xi constrained),

Mi
xi = , Pi (x) = .

last two messages always equal:



Mi
= Mi
= Mi
.

equality due fact factor constraining variables,
matter whether variable satisfying , long least two
variables either satisfying . following, consider two equal
.
messages single message, Mi
variable factor messages grouped follows:

:=
Ri
SVs (i) Mia (s,i , {}),
Variable xi constrained s,i ,

P

u
Ri
:=
Pi (x)Vu (i) Mia (u,i , Pi (x)),
Variable xi constrained clauses u,i ,

P

:=
Ri
Pi (x)Vs (i) Mia (s,i , Pi (x)),
Variable xi constrained , constrained clauses s,i ,

P

:=
Ri
(, ),
Variable xi unconstrained equals *.

last two messages grouped one message (as done previous
paper, Chieu & Lee, 2008) follows,



Ri
= Ri
+ Ri
,

since calculating updates Mj messages Ri messages, Ri
required. update equations RSP weighted Max-SAT given Figure 10.
update equations derived based loopy BP updates Equations 15 16
Section 3. worst case densely connected factor graph, iteration updates
performed O(M N ) time, N number variables, number
clauses.

6.1.1 Factor Variable Messages
begin update equations messages factors variables, given

Equations 46, 47 48. message Mi
groups cases Xi constrained
253

fiChieu & Lee




Mi
=

u
Rj

(46)

jV ()\{i}






u
Mi
=

u

(Rj
+ Rj
)+

jV ()\{i}
w

+(e

Mi
=





(Rk
Rk
)

kV ()\{i}



u

Rj

jV ()\{i,k}



u
1)
Rj
jV ()\{i}

u

(Rj
+ Rj
)



X

jV ()\{i}

(47)


u
Rj

(48)

jV ()\{i}




Ri
=





u


Mi
(Mi
+ Mi
)



Vu (i)

(49)

Vs (i)


u
Ri
=






(Mi
+ Mi
)



Vs (i)

u
Mi


Vu (i)




Mi


Vau (i)



Ri
=


Vu (i)

+

(50)







u


Mi
(Mi
+ Mi
)

Vs (i)


Mi




Vs (i)


Mi



(51)

Vs (i)Vu (i)



Bi (1)


V




u

Mi

+ (i)

V



(Mi
+ Mi
)

(i)


V

Bi (+1)

V

Bi ()

u

Mi

(i)



(52)

(i)





Mi




V



(Mi
+ Mi
)

+ (i)


Mi


V


Mi

(53)

+ (i)

(54)

V (i)

Figure 10: update equations RSP. equations BP equations factor
graph defined text.

254

fiRelaxed Survey Propagation Weighted Max-SAT Problem

factor . means variables violating factor , hence
Equation 46



Mi
=

u
Rj
,

jV ()\{i}
u
Rj
messages neighbors stating violate .
u
next equation Mi
states variable Xi violating . case,
variables possible cases

1. Two variables satisfying , message update


u

(Rj
+ Rj
)

jV ()\{i}


Rk

X



kV ()\{i}

u
Rj


jV ()\{i,k}



u
Rj
.

jV ()\{i}

2. Exactly one variable V ()\{i} constrained , variables violating
, message update
X




Rk

u
Rj

jV ()\{i,k}

kV ()\{i}

3. variables violating , case, penalty factor
exp(w y), message update


exp(w y)

u
Rj

jV ()\{i}

sum three cases result Equation 48.

third update equation Mi
case variable Xi uncons ) (for ). means
strained , satisfying s,i (for case Mi

least one satisfying variable unconstrained , message
update


u

(Rj
+ Rj
)

jV ()\{i}



u
Rj

jV ()\{i}

6.1.2 Variable Factor Messages

consists case variable Xi constrained
first message Ri
factor , means satisfies neighboring factors Vs (i), violates factors
Vu (i), probability



Vu (i)



u
Mi





(Mi
+ Mi
) .



Vs (i)

u
second message Ri
case Xi violates . case, variables
u
V (i) satisfied, clauses Vs (i) violated. case, variable Xi must

255

fiChieu & Lee

constrained one clauses Vu (i). Hence message update


Vs (i)





u


Mi
(Mi
+ Mi
)

Vu (i)

Vu (i)


Mi






. message
third message Ri
sum two messages Ri
Ri
variable Xi satisfies constrained , must constrained
factors:

,
Ri








Vu (i)

u
Mi




(Mi
+ Mi
)

Vs (i)



Vs (i)


Mi




, case X = , :
second part message, Ri


Mi
,


Vs (i)Vu (i)

sum two equations results Equation 51.
6.1.3 Beliefs
beliefs calculated factor variable messages algorithm converges, obtain estimates marginals min-covers. calculation beliefs
similar calculation variable factor messages.
belief Bi (1) belief variable Xi taking value 1. case
variable Xi satisfies clauses V (i), violates clauses V + (i). case,
Xi must constrained one factors V (i). Hence belief follows:


V + (i)

u

Mi






(Mi
+ Mi
)

V (i)




Mi
.

V (i)

calculation belief Bi (+1) similar Bi (1). belief Bi () case
Xi = , hence calculated follows:



Mi
.

V ( i)

6.2 Comparing RSP SP-y Message Passing Algorithms
message passing algorithms RSP SP-y share many similarities. algorithms
1. include multiplicative penalty distribution violated clause.
2. contain mechanism dont care state. SP-y, occurs variable
receives warnings neighboring factors.
However, number significant differences two algorithms.
256

fiRelaxed Survey Propagation Weighted Max-SAT Problem

1. RSP, penalties imposed factor passes message variable.
SP-y, penalties imposed variable compiles incoming warnings,
decides many factors going violate.
2. Importantly, RSP, variables participating violated clauses never take *
value. SP-y, variable receiving equal number warnings set
+
factors { 0 } 0 V (i)\ taking +1 1 value (i.e. hj = Hj


Hj = 0) decide pass message warning . Hence SP-y,
possible variables violated clauses take dont care state.
3. work Battaglia et al. (2004) SP-y formulated cavity
approach, found optimal value given Max-SAT problem
=
e , complexity statistical physics, e energy density
(Mezard & Zecchina, 2002). stated finite value energy
Max-SAT problem zero. Theorem 2, show RSP,
large possible underlying distribution min-covers.
experimental results Figure 12, showed indeed true RSP, long
converges.

INPUT: (weighted) Max-SAT instance, constant k, yin
OUTPUT: configuration.
ALGORITHM :
1. Randomly initialize surveys set = yin .
2. Run RSP y. RSP converges, sort variables according quantities
bi = |P (xi = +1) P (xi = 1)|, fix top k variables preferred
values, subject condition bi > 0.5.
3. (For weighted Max-SAT) RSP fails converge, adjust value y.
4. RSP converges least one variable set, go back step (1) simplified instance. Otherwise, run (weighted) WalkSAT solver simplified
instance output configuration found.
Figure 11: decimation algorithm RSP solving (weighted) Max-SAT instance

6.3 Decimation Algorithm
decimation algorithm shown Figure 11. algorithm used
experiments described Section 7. comparing RSP SP-y random Max-SAT
instances Section 7.1, run algorithms fixed yin , vary yin
range values. Comparing Figure 11 Figure 7 SP-y, condition used SPy check paramagnetic solution replaced condition given Step (2)
Figure 11. experimental results Section 7.1, used SP-y implementation
257

fiChieu & Lee

available online (Battaglia et al., 2004), contains mechanism backtracking
decimation decisions (see Figure 7). Section 7.1, RSP still outperforms SP-y despite
backtracking decisions. running RSP weighted Max-SAT, found
necessary adjust dynamically decimation process. details
experimental settings, please refer Section 7.

7. Experimental Results
run experiments random Max-3-SAT, random weighted Max-SAT, well
benchmark Max-SAT instances used work Lardeux, Saubion, Hao (2005).
#Viols

#Viols

Figure 12: Behaviour SP-y RSP varying values x-axis,
number violated clauses (#viols) y-axis. comparison performances RSP SP-y shown Table 1. objective showing
graphs figure show behavior RSP varying
consistent Theorem 2: long RSP converges, performance improves
increases. graph, RSP reaches plateau fails converge.This
property allows systematic search good value used.
behavior SP-y varying less consistent.
.

7.1 Random Max-3-SAT
run experiments randomly generated Max-3-SAT instances 104 variables,
different clause-to-variable ratios. random instances generated SP-y code
available online (Battaglia et al., 2004). Figure 12, compare SP-y RSP random
Max-3-SAT different clause-to-variable ratio, . vary 4.2 5.2 show
performance SP-y RSP UNSAT region 3-SAT, beyond phase transition
c 4.267. value , number violated clauses (y-axis) plotted
value used.
258

fiRelaxed Survey Propagation Weighted Max-SAT Problem

perform decimation procedure Figure 11 RSP, fixed value yin ,
decimating 100 variables time (i.e. k = 100). SP-y, run SP-y code available
line, option decimating 100 variables iteration, two different
settings: without backtracking (Battaglia et al., 2004). Backtracking procedure used SP-y improve performance, unfixing previously fixed variables rate
r = 0.2, errors made decimation process corrected. RSP,
run backtracking. Note formulation equals 2y formulation
work Battaglia et al. (Battaglia et al., 2004).
SP-y RSP fail converge becomes large enough. happens,
output algorithm result returned WalkSAT original instance.
Figure 12, see happening curve reaches horizontal line, signifying
algorithm returning configuration regardless (we seed randomized
WalkSAT results identical instances identical). Figure 12,
see RSP performs consistently SP-y: increases, performance RSP
improves, point RSP fails converge. Interestingly Max-3-SAT instances,
observed RSP converges value given instance, continue
converge value throughout decimation process. Hence, best value
RSP obtainable without going decimation process: commence
decimation largest value RSP converges. Table 1, show RSP
outperforms SP-y 4.7, despite fact allow backtracking RSP.
also compare RSP SP-y local search solvers implemented UBCSAT
(Tompkins & Hoos, 2004). run 1000 iterations 20 Max-SAT solvers
UBCSAT, take best result among 20 solvers. results shown Table 1.
see local solvers UBCSAT worse RSP SP-y.
also tried running complete solvers toolbar (de Givry, Heras, Zytnicki, & Larrosa,
2005) maxsatz (Li, Manya, & Planes, 2006). unable deal instances
size 104 .
7.2 Random Weighted Max-3-SAT
also run experiments randomly generated weighted Max-3-SAT instances.
instances generated way instances Max-3-SAT, addition,
weights clause uniformly sampled integers set [1, ],
upper bound weights. show experimental results = 5 = 10
Figure 13. compare RSP 13 weighted Max-SAT solvers implemented
UBCSAT. RSP, run experiments initial set 10, whenever
algorithm fails converge, lower value 1, halve value
less 1 (see Figure 11). see RSP outperforms UBCSAT consistently
experiments Figure 13.
7.3 Benchmark Max-SAT Instances
compare RSP UBCSAT instances used work Lardeux et al. (2005),
instances used SAT 2003 competition. Among 27 instances, use
seven largest instances 7000 variables. run RSP two settings:
decimating either 10 100 variables time. run RSP increasing values y:
259

fiChieu & Lee

Table 1: Number violated clauses attained method. SP-y, SP-y (BT) (SPy backtracking), RSP, best result selected y. ,
show best performance bold face. column Fix shows number
variables fixed RSP optimal y, Time time taken RSP
(in minutes) fix variables, AMD Opteron 2.2GHz machine.

4.2
4.3
4.4
4.5
4.6
4.7
4.8
4.9
5.0
5.1
5.2

UBCSAT
47
68
95
128
140
185
232
251
278
311
358

SP-y
0
9
42
67
98
137
204
223
260
294
362

SP-y(BT)
0
7
31
67
89
130
189
211
224
280
349

RSP
0
10
36
65
90
122
172
193
218
267
325

Fix
7900
7200
8938
9024
9055
9287
9245
9208
9307
9294
9361

Time (minutes)
24
43
82
76
45
76
52
62
66
42
48

W-viol

W-viol





Figure 13: Experimental results weighted Max-SAT instances. x-axis shows
value , y-axis (W-viol) number violated clauses returned
algorithm.

y, RSP fixes number spins, stop increasing number spins
fixed decreases previous value y. UBCSAT, run 1000 iterations
20 solvers. Results shown Table 2. seven instances, RSP fails
fix spins first one, outperforms UBCSAT rest. Lardeux et al. (2005)
show best performances paper, average results order
magnitude higher results Table 2. Figure 12 shows finding good SP-y
hard. benchmark instances, run SP-y -Y option (Battaglia et al.,
2004) uses dichotomic search y: SP-y failed fix spins 7 instances.
260

fiRelaxed Survey Propagation Weighted Max-SAT Problem

Table 2: Benchmark Max-SAT instances. Columns: instance shows instance name
paper Lardeux et al. (2005), nvar number variables, ubcsat
rsp-x (x number decimations iteration) number violated
clauses returned algorithm, fx-x number spins fixed RSP.
Best results indicated bold face.
instance
fw
nc
nw
35-4-03
35-4-04
40-4-02
40-4-03

nvar ubcsat rsp-100 fx-100 rsp-10
family: purdom-10142772393204023
9366
83
357
0
357
8372
74
33
8339
35
8589
73
24
8562
28
family: pyhala-braun-unsat
7383
58
68
7295
44
7383
62
53
7302
41
9638
86
57
9547
65
9638
76
77
9521
41

fx-10
0
8316
8552
7299
7304
9521
9568

success SP family algorithms random ensembles SAT Max-SAT
problem usually due clustering phenomenon random ensembles.
benchmark instances random instances, attempted see configurations
found RSP indeed belong cover representing cluster solutions. Rather
disappointingly, found 6 solutions RSP outperformed local search
algorithms, variables solutions constrained least one clause. Hence,
v-covers found degenerate covers, i.e. covers contain variables set
. appears success RSP benchmark instances due
clustering phenomenon, simply RSP manages converge instances,
value y. Kroc, Sabharwal, Selman (2009) made similar observation:
convergence BP SP like algorithms often sufficient obtaining good solution
given problem. discussed Section 5.3, ability vary improve convergence
useful feature RSP, one distinct ability exploit clustering
phenomenon.

8. Conclusion
recent work Max-SAT weighted Max-SAT tends focus complete
solvers, solvers unable handle large instances. Max-SAT competition
2007 (Argelich, Li, Manya, & Planes, 2007), largest Max-3-SAT instances used
70 variables. large instances, complete solvers still practical, local
search procedures feasible alternative. SP-y, generalizing SP,
shown able solve large Max-3-SAT instances phase transition, lacks
theoretical explanations recent work SP generated.
3-SAT, easy-hard-easy transition clause-to-variable ratio increases.
Max-3-SAT, however, shown empirically beyond phase transition
satisfiability, instances hard solve (Zhang, 2001). paper, show
261

fiChieu & Lee

RSP outperforms SP-y well local search algorithms Max-SAT weighted
Max-SAT instances, well beyond phase transition region.
RSP SP-y well Max-SAT instances near phase transition.
mechanisms behind SP-y RSP similar: algorithms impose penalty term
violated constraint, reduce SP . SP-y uses population
dynamics algorithm, also seen warning propagation algorithm.
paper, formulated RSP algorithm BP algorithm extended factor
graph, enabling us understand RSP estimating marginals min-covers.

Acknowledgments
work supported part NUS ARF grant R-252-000-240-112.

Appendix A. Smoothing Interpretation RSP
definition SP- (Maneva et al., 2004), parameter introduced define
whole family algorithms. = 1, SP- algorithm corresponds SP algorithm,
= 0, SP- algorithm corresponds BP algorithm. section,
develop general version extended factor graph defined Section 5,
incorporates SP-. call corresponding RSP algorithm new
factor graph RSP- algorithm.
difference factor graph RSP- one Section 5
definition variable compatibilities Equation 43. Following notations work
Maneva et al. (2004), introduce parameters 0 , restrict
case 0 + = 1 (The SP- RSP- equal ). redefine
variable compatibilities follows

(i (x) = {xi , Pi (x)}) =



0





1

Pi (x) = , xi 6=
Pi (x) = , xi =
,
valid (xi , Pi (x))

(55)

0 + = 1. definition Equation 43 corresponds particular case
0 = 0 = 1. Section 5, defined factor graph unconstrained
variables must take value . new definition above, unconstrained variables allowed take values 1 +1 weight 0 , value
weight .
definition, joint distribution Equation 45 redefined follows:
n (x) n (x)
exp(w y).

UNSAT(x)

P (x) = P ({xk , Pk }k ) 0 0

(56)

n0 (x) number unconstrained variables x taking +1 1, n (x)
number unconstrained variables taking x.
Case = 1: studied case main paper: underlying distribution
distribution positive v-covers.
262

fiRelaxed Survey Propagation Weighted Max-SAT Problem

Case = 0: case, configurations x n (x) = 0 non-zero probability distribution given Equation 56. Hence, value forbidden,
variables take values 1, +1. Boolean configuration violating clauses total weight
W probability proportional exp(yW ). Hence retreive weighted Max-SAT
energy defined Equation 13. case, factor graph equivalent original
weighted Max-SAT factor graph defined Definition 3, hence RSP- equivalent
loopy BP algorithm original weighted Max-SAT problem.
Case 6= 1 6= 0: case, valid configurations x violating clauses
n (x) n (x)
total weight W probability proportional 0 0 exp(yW ). Hence,
probability v-covers case = 1 spread lattice
minimal element.
formulation, RSP- seen family algorithms include
BP RSP algorithm, moving BP RSP (or ) varies 0 1.

References
Achlioptas, D., Naor, A., & Peres, Y. (2005). Rigorous location phase transitions hard
optimization problems. Nature, 435 (7043), 759764.
Argelich, J., Li, C. M., Manya, F., & Planes, J. (2007). Second evaluation max-sat
solvers. SAT07: Tenth International Conference Theory Applications
Satisfiability Testing.
Battaglia, D., Kolar, M., & Zecchina, R. (2004). Minimizing energy glass thresholds. Physical Review E, 70.
Berre, D. L., & Simon, L. (2003). essentials SAT 2003 competition. SAT03:
Sixth International Conference Theory Applications Satisfiability Testing.
Berre, D. L., & Simon, L. (2005). Special volume SAT 2005 competitions
evaluations. Journal Satisfiability, Boolean Modeling Computation, 2.
Braunstein, A., Mezard, M., & Zecchina, R. (2005). Survey propagation: algorithm
satisfiability. Random Structures Algorithms, 27 (2).
Braunstein, A., & Zecchina, R. (2004). Survey propagation local equilibrium equations.
Journal Statistical Mechanics: Theory Experiment, 2004 (06).
Braunstein, A., & Zecchina, R. (2006). Learning message-passing networks discrete
synapses. Physical Review Letters, 96, 030201.
Chieu, H. L., & Lee, W. S. (2008). Relaxed survey propagation: sum-product algorithm
Max-SAT. AAAI08: Twenty-Third AAAI Conference Artificial Intelligence.
Chieu, H. L., Lee, W. S., & Teh, Y. W. (2008). Cooled relaxed survey propagation
MRFs. NIPS07: Advances Neural Information Processing Systems 20, Cambridge, MA. MIT Press.
Cook, S. A. (1971). complexity theorem-proving procedures. STOC 71: Third
annual ACM symposium Theory computing, pp. 151158, New York, NY, USA.
ACM.
263

fiChieu & Lee

Cooper, G. F. (1990). computational complexity probabilistic inference using
bayesian belief networks (research note). Artif. Intell., 42 (2-3), 393405.
Daude, H., Mezard, M., Mora, T., & Zecchina, R. (2008). Pairs sat-assignments random
boolean formul. Theor. Comput. Sci., 393 (1-3), 260279.
Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem-proving.
Commun. ACM, 5 (7), 394397.
Davis, M., & Putnam, H. (1960). computing procedure quantification theory. Journal
ACM (JACM), 7 (3), 201215.
de Givry, S., Heras, F., Zytnicki, M., & Larrosa, J. (2005). Existential arc consistency:
Getting closer full arc consistency weighted CSPs.. IJCAI05: Nineteenth
International Joint Conference Artificial Intelligence.
Dubois, O., & Dequen, G. (2001). backbone-search heuristic efficient solving hard 3SAT formulae. IJCAI05: Seventeenth International Joint Conference Artificial
Intelligence, pp. 248253.
Een, N., & Sorensson, N. (2005). MiniSat - SAT solver conflict-clause minimization.
SAT05: Eighth International Conference Theory Applications Satisfiability Testing.
Hoos, H. H. (2002). adaptive noise mechanism walksat. AAAI02: Eighteenth
National Conference Artificial Intelligence, pp. 655660.
Kaporis, A. C., Kirousis, L. M., & Lalas, E. G. (2006). probabilistic analysis greedy
satisfiability algorithm. Random Structures Algorithms, 28 (4), 444480.
Kirkpatrick, S., Jr., C. D. G., & Vecchi, M. P. (1983). Optimization simulated annealing.
Science, 220, 671680.
Kirousis, L. M., Kranakis, E., Krizanc, D., & Stamatiou, Y. C. (1998). Approximating
unsatisfiability threshold random formulas. Random Structures Algorithms,
12 (3), 253269.
Kroc, L., Sabharwal, A., & Selman, B. (2007). Survey propagation revisited. UAI07:
Twenty-Third Conference Uncertainty Artificial Intelligence.
Kroc, L., Sabharwal, A., & Selman, B. (2009). Message-passing local heuristics
decimation strategies satisfiability. SAC-09. 24th Annual ACM Symposium
Applied Computing.
Kschischang, F. R., Frey, B., & Loeliger, H.-A. (2001). Factor graphs sum-product
algorithm. IEEE Transactions Information Theory, 47 (2).
Lardeux, F., Saubion, F., & Hao, J.-K. (2005). Three truth values SAT MAXSAT problems. IJCAI05: Nineteenth International Joint Conference Artificial
Intelligence.
Larrosa, J., & Heras, F. (2005). Resolution Max-SAT relation local consistency weighted CSPs. IJCAI05: Nineteenth International Joint Conference
Artificial Intelligence.
Levin, L. A. (1973). Universal search problems. Problemy Peredaci Informacii, 9, 115116.
264

fiRelaxed Survey Propagation Weighted Max-SAT Problem

Li, C. M., & Anbulagan (1997). Heuristics based unit propagation satisfiability problems. IJCAI97: Fifteenth International Joint Conference Artificial Intelligence,
pp. 366371.
Li, C. M., Manya, F., & Planes, J. (2006). Detecting disjoint inconsistent subformulas
computing lower bounds max-sat. AAAI06: Twenty-First AAAI Conference
Artificial Intelligence.
Maneva, E., Mossel, E., & Wainwright, M. (2004). new look survey propagation
generalizations. http://arxiv.org/abs/cs.CC/0409012.
McAllester, D., Selman, B., & Kautz, H. (1997). Evidence invariants local search.
AAAI97: Proceedings Fourteenth National Conference Artificial Intelligence,
pp. 321326, Providence, Rhode Island.
Mezard, M., Mora, T., & Zecchina, R. (2005). Clustering solutions random satisfiability problem. Physical Review Letters, 94, 197205.
Mezard, M., & Parisi, G. (2003). cavity method zero temperature. Journal
Statistical Physics, 111.
Mezard, M., & Zecchina, R. (2002). random k-satisfiability problem: analytic
solution efficient algorithm. Physical Review E, 66.
Minton, S., Philips, A., Johnston, M. D., & Laird, P. (1992). Minimizing conflicts: heuristic
repair method constraint satisfaction scheduling problems. Artificial Intelligence, 58, 161205.
Moskewicz, M. W., & Madigan, C. F. (2001). Chaff: Engineering efficient SAT solver.
DAC01: Thirty-Ninth Design Automation Conference, pp. 530535.
Murphy, K., Weiss, Y., & Jordan, M. (1999). Loopy belief propagation approximate
inference: empirical study. UAI99: Fifteenth Annual Conference Uncertainty
Artificial Intelligence, pp. 46747, San Francisco, CA. Morgan Kaufmann.
Pearl, J. (1988). Probabilistic reasoning intelligent systems: networks plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.
Pipatsrisawat, K., & Darwiche, A. (2007). Rsat 2.0: Sat solver description. Tech. rep.,
Automated Reasoning Group, Computer Science Department, UCLA.
Prestwich, S. D. (2003). Local search sat-encoded colouring problems.. Giunchiglia,
E., & Tacchella, A. (Eds.), SAT, Vol. 2919 Lecture Notes Computer Science, pp.
105119. Springer.
Rabiner, L., & Juang, B. (1993). Fundamentals Speech Recognition. Prentice-Hall.
Selman, B., Kautz, H. A., & Cohen, B. (1994). Noise strategies improving local search.
AAAI97: Twelfth National Conference Artificial Intelligence, pp. 337343.
Selman, B., Levesque, H. J., & Mitchell, D. (1992). new method solving hard satisfiability problems. AAAI92: Tenth National Conference Artificial Intelligence,
pp. 440446. AAAI Press.
Shang, Y., & Wah, B. W. (1998). discrete lagrangian-based global-searchmethod
solving satisfiability problems. Journal Global Optimization, 12 (1), 6199.
265

fiChieu & Lee

Tompkins, D., & Hoos, H. (2004). UBCSAT: implementation experimentation environment SLS algorithms SAT MAX-SAT. SAT04: Seventh International
Conference Theory Applications Satisfiability Testing.
Tseitin, G. S. (1968). complexity derivations propositional calculus. Studies
Mathematics Mathematical Logic, Part II, 115125.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. NIPS00:
Advances Neural Information Processing Systems 13, pp. 689695.
Yedidia, J., Freeman, W., & Weiss, Y. (2005). Constructing free-energy approximations
generalized belief propagation algorithms. IEEE Transactions Information
Theory, 51 (7).
Zhang, W. (2001). Phase transitions backbones 3-SAT maximum 3-SAT.
Proceedings Seventh International Conference Principles Practice
Constraint Programming.

266

fiJournal Artificial Intelligence Research 36 (2009) 471-511

Submitted 7/09; published 12/09

Role Macros Tractable Planning
Anders Jonsson

anders.jonsson@upf.edu

Dept. Information Communication Technologies
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, Spain

Abstract
paper presents several new tractability results planning based macros.
describe algorithm optimally solves planning problems class call
inverted tree reducible, provably tractable several subclasses class.
using macros store partial plans recur frequently solution, algorithm
polynomial time space even exponentially long plans. generalize inverted
tree reducible class several ways describe modifications algorithm deal
new classes. Theoretical results validated experiments.

1. Introduction
growing number researchers planning investigating computational complexity
solving different classes planning problems. Apart theoretical value
knowledge, particular interest tractable planning problems provably solved
polynomial time. Many, most, state-of-the-art planners based heuristic
search. Acquiring informative heuristic requires quickly solving relaxed version
planning problem. Known tractable classes planning problems ideal candidates
projecting original problem onto order generate heuristics.
paper introduces class IR inverted tree reducible planning problems
presents algorithm uses macros solve instances class. algorithm
provably complete optimal IR, complexity depends size
domain transition graph algorithm constructs. worst case, size
graph exponential number state variables, show algorithm runs
polynomial time several subclasses IR, proving plan generation tractable
classes.
tractable subclasses IR include planning problems whose optimal solution
exponential length number state variables. reason algorithm solve
problems polynomial time subsequences operators frequently repeated
solution. Since algorithm stores subsequence macro, never needs
generate subsequence again. algorithm viewed compilation scheme
state variables compiled away. information kept state
variable set macros acting it.
extend class IR two ways modify algorithm solves planning
problems new classes. First, define notion relaxed causal graph,
contains less edges conventional causal graph, associated class RIR
relaxed inverted tree reducible planning problems. also define notion reversible
c
2009
AI Access Foundation. rights reserved.

fiJonsson

variables, define class AR planning problems acyclic causal graphs
reversible variables. show algorithm modified solve planning problems
class. Finally, combine results IR AR define class AOR
planning problems acyclic causal graphs variables reversible.
class IR algorithm solving instances class previously appeared
conference paper (Jonsson, 2007). present paper includes new tractability results
classes IR, AR, AOR. addition, notion relaxed causal graphs
resulting class RIR novel.
Perhaps important contribution work establishing several novel
tractability results planning. particular, present several classes planning problems solved polynomial time. contribution particularly significant
since number known tractable classes planning problems small. addition, new classes solved optimally. novel approach taken
paper use macros store solutions subproblems, making possible represent
exponentially long plans polynomial time space.
related contribution possibility use tractable classes generate heuristics.
discuss two possibilities generating heuristics here. One idea project planning
problem onto IR removing pre-conditions effects operators make
causal graph acyclic inverted tree reducible. fact, strategy used
Helmert (2006) compute causal graph heuristic. Next, could duplicate
operator introducing pre-conditions missing variables. resulting problem
tractable according Theorem 3.5. Thus, algorithm could solve projected problem
polynomial time compute admissible heuristic original problem. Note,
however, number resulting actions exponential number missing
variables, strategy tractable number missing variables
small constant.
Another idea perform reduction causal graph acyclic
variable constant number ancestors. Next, could make current problem part
class AOR introducing actions make certain variables reversible. types
changes effect making problem easier solve, sense solution
relaxed problem guaranteed longer solution original problem.
could solve resulting problem using algorithm AOR. Although
algorithm provably optimal, might informative heuristic cases.
Another contribution possibility solving planning problems polynomial
time, since fall one classes planning problems studied paper.
example, show possible Gripper Logistics domains. Although
domains previously known polynomial-time solvable, known
whether could solved using type macro approach taken here. planning
problem cannot solved directly, parts causal graph might structure
required algorithms. case, algorithms could used solve part
planning problem. Since one feature algorithms compiling away variables
replacing macros, reduced problem could fed standard planner
obtain final solution.
Perhaps interesting use algorithms possibility reuse macros.
part causal graph structure coincides two problems, macros generated
472

fiThe Role Macros Planning

one problem could immediately substituted problem. Since variables
compiled away replaced macros, resulting problem fewer variables
actions. example, Tower Hanoi, macros generated n 1 disc problem
could immediately substituted n disc problem. resulting problem includes
macros n 1 disc problem single variable corresponding n-th disc.
rest paper organized follows. Section 2 introduces notation definitions. Section 3 introduces class IR inverted tree reducible planning problems,
presents algorithm solving problems class, proves several theoretical properties algorithm. Section 4 introduces several extensions class IR, well
corresponding algorithms. Section 5 presents experimental results validating theoretical
properties algorithms. Section 6 relates paper existing work planning,
Section 7 concludes discussion.

2. Notation
Let V set variables, let D(v) finite domain variable v V . define
state function V maps variable v V value s(v) D(v)
domain. partial state x function subset Vx V variables maps
variable v Vx x(v) D(v). Sometimes use notation (v1 = x1 , . . . , vk = xk )
denote partial state x defined Vx = {v1 , . . . , vk } x(vi ) = xi vi Vx .
define several operations partial states. subset W V variables, x | W
partial state obtained restricting scope x Vx W . Two partial states x
match, denote x y, x | Vy = | Vx , i.e., x(v) = y(v) v Vx Vy .
Given two partial states x y, let x y, composition x y, partial state z
defined Vz = Vx Vy , z(v) = y(v) v Vy , z(v) = x(v) v Vx Vy .
Note composition symmetric since right operand overrides values
left operand.
planning problem tuple P = hV, init, goal, Ai, V set variables,
init initial state, goal partial goal state, set actions. action
= hpre(a); post(a)i consists partial state pre(a) called pre-condition
partial state post(a) called post-condition. Action applicable state
pre(a) results new state post(a).
causal graph planning problem P directed graph (V, E) variables
nodes. edge (w, v) E w 6= v exists action
w Vpre(a) Vpost(a) v Vpost(a) . causal graph acyclic, action
unary, i.e., |Vpost(a) | = 1. variable v V , define Anc(v) set
ancestors v causal graph, Desc(v) set descendents v.
macro-action, macro short, ordered sequence actions. macro
well-defined, pre-condition action coincide cumulative
post-condition actions precede sequence. Although action sequence
implicitly induces pre- post-condition, explicitly associate pre- postcondition macro. Since macro functionally action, define hierarchies
macros action sequence macro includes macros, long
cause definitions macros cyclic.
473

fiJonsson

v1

v3

v1

v3

v2

v4

v5
v2

v5

v4

Figure 1: acyclic causal graph transitive reduction.
Definition 2.1. sequence actions seq = ha1 , . . . , ak well-defined state
pre(a1 ) post(a1 ) . . . post(ai1 ) pre(ai ) {1, . . . , k}.
sometimes refer post-condition post(seq) action sequence, defined
post(seq) = post(a1 ) . . . post(ak ). Given two sequences seq1 seq2, hseq1, seq2i
concatenation two sequences, post(hseq1, seq2i) post-condition
resulting sequence, defined post(seq1) post(seq2). ready formally
define macros use paper:
Definition 2.2. macro = hpre(m), seq(m), post(m)i consists pre-condition pre(m),
sequence seq(m) = ha1 , . . . , ak actions macros, post-condition
post(m). macro well-defined seq(m) well-defined pre(m) pre(m)
post(seq(m)) = post(m).

3. Class IR
Katz Domshlak (2008a) defined class planning problems whose causal graphs
inverted trees, i.e., outdegree variable causal graph less equal
1, unique root variable v outdegree 0. section, study
class planning problems call IR, stands inverted tree reducible:
Definition 3.1. planning problem P inverted tree reducible causal
graph P acyclic transitive reduction causal graph inverted tree.
transitive reduction (V, E ) graph (V, E) defined set nodes
V , E contains minimal set edges transitive closure E equal
transitive closure E. words, transitive reduction retains edges
necessary maintain connectivity. acyclic graphs, transitive reduction unique
computed efficiently. Figure 1 illustrates causal graph transitive
reduction planning problem class IR. Here, root variable v5 .
graph G = (V, E) variables V planning problem P variable
v V , let P a(v) = {w V : (w, v) E} set parents v G, i.e., variables
incoming edges. paper, always refer parents v transitive reduction
(V, E ), opposed causal graph itself. example, Figure 1 parents v5
causal graph {v1 , . . . , v4 }, P a(v5 ) = {v3 , v4 }. Note transitive reduction
preserves set ancestors Anc(v) descendants Desc(v) variable v.
Without loss generality, follows assume goal state defined
root variable v transitive reduction, i.e., v Vgoal . Since outgoing
edges v, pre-condition action changes value variable
474

fiThe Role Macros Planning

w 6= v independent v. v
/ Vgoal , plan solves P never needs change value
v, eliminate v problem, resulting one several subgraphs
causal graph. trivial show transitive reductions resulting subgraphs
inverted trees, problem decomposed one several planning problems
IR solved independently.
convenience, add dummy variable v V , well dummy action =
hgoal; (v = 1)i A. transitive reduction resulting causal graph contains
additional node v additional edge (v, v ), v original root variable (note
v becomes root variable new transitive reduction). Since v Vgoal
assumption, causal graph contains edge (v, v ), v reachable
variable transitive reduction via v.
rest Section 3 organized follows. Subsection 3.1 presents MacroPlanner, algorithm solves planning problems class IR via decomposition
subproblems. subsection includes detailed descriptions subproblems generated solved. Subsection 3.2 shows examples MacroPlanner solves planning
problems IR. Subsection 3.3 proves several theoretical properties MacroPlanner,
Subsection 3.4 discusses algorithm improved pruning search
macros.
3.1 Plan Generation IR
section, present plan generation algorithm class IR called MacroPlanner. planning problem P IR, MacroPlanner generates one several plans
solve P form macros. algorithm uses divide-and-conquer strategy
define solve several subproblems variable v V problem. stores
solutions subproblem macros, incorporates macros action
sets higher-level subproblems.
Algorithm 1 MacroPlanner(P )
1: G causal graph P
2: R transitive reduction G
3: v root variable R
4: GetMacros(v, init, A, R)
5: return , fail =

main routine MacroPlanner appears Algorithm 1. MacroPlanner takes
planning problem P IR input constructs causal graph G P , well
transitive reduction R G. identifies root variable v transitive
reduction, calls GetMacros(v, init, A, R) solve P . Note set macros,
implying MacroPlanner may return multiple solutions P . contain
solution P , MacroPlanner returns fail. Also note v refers original
root variable transitive reduction (excluding dummy variable v ). However,
follows still consider v descendant v.
475

fiJonsson



B
t2

t3

C
t4

t1


Figure 2: Illustration macros generated MacroPlanner.
3.1.1 Defining Subproblems
section describe subroutine GetMacros which, variable v V ,
defines several subproblems v. Intuitively, two reasons change
values v ancestors: satisfy pre-condition action A, reach
goal state. idea exhaustively define subproblems v may needed
achieve this.
Let Pv = hVv , init , goal , Oi subproblem associated variable v V . set
variables Vv = {v} Anc(v) consists v ancestors causal graph. set
actions contains action changes value v macro representing
solution subproblem parent w P a(v) v. Since Vv fixed, two
subproblems v differ initial goal states init goal .
define subproblems may needed change values v ancestors, project onto Vv pre-condition action changes value
descendant v. Let Z = {pre(a) | Vv : Vpost(a) Desc(v)} {()} set
projected pre-conditions. exclude empty partial state () Z, implying
|Vz | > 0 z Z. Note Z includes projected goal state goal | Vv since goal
pre-condition dummy action changes value v , descendant v.
projected pre-condition z Z, GetMacros defines subproblem Pv
initial state init = init | Vv goal state goal = z. GetMacros calls subroutine
Solve generate one several solutions Pv , form sequence seq =
ha1 , . . . , ak actions macros O. Let = init post(seq) state results
applying seq init . Solve returns solution Pv form macro
= hinit , seq, si, init fully specified states Vv . macro
z Z, GetMacros defines new subproblem Pv initial state init = post(m)
goal state goal = z. process continues new subproblems defined.
subroutine Solve may generate multiple macros satisfying projected
pre-condition. intuitive understanding macros generated provided
Lemma B.3. lemma states state u reachable init
projected pre-condition z Z u z, Solve generates macro hinit , seq, ti
z shortest path init u prefix seq. words,
Solve generates shortest possible macros satisfying z. However, might
generate multiple macros ensure completeness optimality.
Figure 2 illustrates macros MacroPlanner generates. Let current
state, let z projected pre-condition, let A, B, C three sets states
satisfy z. t1 shortest path state A, MacroPlanner generates
macro t1 represents shortest path (the holds t4 C).
476

fiThe Role Macros Planning

state B shortest path states B, algorithm generates
several macros states B, case t2 t3 , either t2 t3
shortest path state B.
Algorithm 2 GetMacros(v, init, A, G)
1:
2: L list containing projected initial state init | Vv
3: Z {pre(a) | Vv : Vpost(a) Desc(v)} {()}
4: {a : v Vpost(a) }
5: w P a(v)
6:
GetMacros(w, init, A, G)
7: end
8: elements L
9:
next element L
10:
Solve(v, s, Z, O, G)
11:
= hpre(m), seq(m), post(m)i post(m)
/ L
12:
append post(m) L
13:
end
14: end
15: return
Algorithm 2 gives pseudo-code GetMacros. input GetMacros variable
v V , initial state init, action set A, graph G = (V, E) variables V .
obtain action set O, GetMacros recursively calls parent w P a(v)
v. list L initialized projected initial state init | Vv . subroutine
Solve, described following section, simultaneously solves subproblems
v initial state returns corresponding macros. distinct postcondition post(m) macro returned Solve, GetMacros adds post(m) list
L. Consequently, Solve later called initial state post(m). Finally, GetMacros
returns macros generated Solve.
3.1.2 Solving Subproblems
section describe subroutine Solve, solves subproblems defined
GetMacros variable v V . idea construct graph states
Vv nodes. graph seen joint domain transition graph variables
Vv . initial state projected pre-condition z Z, Solve computes
shortest path states match z using straightforward application Dijkstras
algorithm. However, number states exponential cardinality Vv , Solve
constructs graph implicitly, adding nodes needed.
state p graph, Solve generates two types successor states. Successor
states first type change value v. find states, Solve
finds action pre(a) (v = p(v)), i.e., pre-condition
satisfied respect current value v. parent w P a(v)
(p | Vw ) 6 (pre(a) | Vw ), Solve looks macros p | Vw states satisfy
pre(a) | Vw . sequence seq represents combination macros, Solve
477

fiJonsson

adds successor state p post(hseq, ai) p graph. cost corresponding
edge length sequence hseq, ai.
Successor states second type match projected pre-condition z Z.
find states, Solve finds z Z z (v = p(v)). parent
w P a(v) (p | Vw ) 6 (z | Vw ), Solve looks macros p | Vw states
match z | Vw . sequence seq represents combination macros,
Solve generates successor state p post(seq) p. However, successor state
added graph. Instead, used generate solution corresponding
subproblem defining macro initial state state p post(seq).
Algorithm 3 Solve(v, s, Z, O, G)
1:
2: Q priority queue containing pair (s, hi)
3: Q non-empty
4:
(p, seq) remove highest priority state-sequence pair Q
5:
pre(a) (v = p(v))
6:
Compose(v, p, pre(a), O, G)
7:
action sequences seq2
8:
insert (p post(hseq2, ai), hseq, seq2, ai) Q
9:
end
10:
end
11:
z Z z (v = p(v))
12:
Compose(v, p, z, O, G)
13:
action sequences seq2
14:
{hs, hseq, seq2i, p post(seq2)i}
15:
end
16:
end
17: end
18: return
Algorithm 3 gives pseudo-code Solve, takes input variable v V ,
initial state s, set projected pre-conditions Z, action set O, graph G = (V, E)
V . priority queue Q contains state-sequence pairs, elements ordered according
total length associated sequence. Successor states first type generated
lines 510, successor states second type generated lines 1116. Inserting
state-sequence pair (p, seq) Q (line 8) succeeds seq shortest sequence
p far, replacing previous state-sequence pair involving p. Likewise, inserting macro
hs, seq, pi (line 14) succeeds seq shortest sequence p far, replacing
previous macro p.
Algorithm 4 gives pseudo-code subroutine Compose called Solve. input
Compose variable v V , state s, partial state x, action set O, graph
G = (V, E) V . Compose generates sequences states match x
composed using macros parents v. use Cartesian product
denote set sequences obtained appending macro sequence S.
478

fiThe Role Macros Planning

Algorithm 4 Compose(v, s, x, O, G)
1: {hi}
2: w P a(v)
3:
(s | Vw ) 6 (x | Vw )
4:
{hs | Vw , seq(m), post(m)i : post(m) (x | Vw )}
5:
=
6:
return
7:
end
8:
ST
9:
end
10: end
11: return
v1

v2

v3

v4

v5

v1

v2

v3

v4

v5

Figure 3: causal graph transitive reduction P5 .
matches x, Compose returns set containing empty sequence hi. If, parent
w P a(v), macro state matching x, Compose returns empty set.
3.2 Examples
illustrate MacroPlanner well-known Tower Hanoi problem. instance
Tower Hanoi n discs 3 pegs represented planning problem
Pn = hV, init, goal, Ai, V = {v1 , . . . , vn } variables representing discs (v1
smallest vn largest), domain D(vi ) = {A, B, C}. initial
state init = (v1 = A, . . . , vn = A) goal state goal = (v1 = C, . . . , vn = C).
vi V permutation (j, k, l) (A, B, C), action moving disc
vi peg j peg k, formally defined aj,k
= h(v1 = l, . . . , vi1 = l, vi = j); (vi = k)i.
discs smaller vi thus third peg l perform movement.
Figure 3 shows causal graph transitive reduction planning problem
P5 , representing 5-disc instance Tower Hanoi. action moving disc vi
pre-condition variable vj , j < i. Clearly, causal graph acyclic
transitive reduction inverted tree, implying P5 IR. Since transitive reduction
directed path, set parents variable vi , > 1, P a(vi ) = {vi1 }. root
variable v5 or, generally, vn planning problem Pn .
solve Tower Hanoi, MacroPlanner(Pn ) calls GetMacros(vn , init, A, G)
generate set solution macros. turn, subroutine GetMacros(vn , init, A, G)
calls GetMacros(vn1 , init, A, G). recursion continues base case reached
GetMacros(v1 , init, A, G). variable vi V , let xdi = (v1 = d, . . . , vi = d),
{A, B, C}, partial state assigning value variable Vvi .
479

fiJonsson

B,...,B,B

A,...,A,B

A,...,A,C

C,...,C,B

C,...,C,C

B,...,B,C

C,...,C,A

B,...,B,A

A,...,A,A

Figure 4: implicit graph traversed Solve variable vi .
B
C
set projected pre-conditions vi Z = {xA
, xi , xi }. Since projected precondition specifies value ancestor vi , macro end either
three partial states. easy show induction set macros returned
GetMacros(vi , init, A, G) contains precisely 9 macros, corresponding pair
projected pre-conditions (including macros empty action sequence projected
pre-condition itself).
Let us study behavior GetMacros(vi , init, A, G). Here, assume 9
macros vi1 set described above. first partial state added L

line 2 projected initial state xA
. result, GetMacros calls Solve(vi , xi , Z, O, G)
line 10. call Solve, first state-sequence pair added Q line 2 (xA
, hi).
A,B
C

action ai pre-condition xi1 (vi = A) matches (vi = xi (vi )) = (vi = A).
C
result, Solve calls Compose(vi , xA
, xi1 (xi = A), O, G) line 6.

Compose, single parent vi , namely vi1 . projection xA
| Vvi1 = xi1
C

C
equal xi1 . single macro set xi1 xi1 , set
returned Compose contains single sequence hmi. result, Solve generates
successor state xC
i1 (vi = B) first type, added Q line 8 together
i.
associated sequence hm, aA,B


(v
iteration xC
= B), single projected pre-condition
i1
B
B
matches (vi = B), namely xi , Solve calls Compose(vi , xC
i1 (vi = B), xi , O, G)
B
C
line 12. parent vi1 , projection xi1 match xi | Vvi1 = xB
i1 .
B , set returned Compose
single macro set xC

x
i1
i1
contains single sequence hm i. result, Solve generates successor state xB

A,B
i, xB added . eight
,

,
hm,

second type line 14, macro hxA



macros added similar way. next section prove MacroPlanner
guaranteed generate solution Tower Hanoi polynomial time.
Figure 4 shows implicit graph traversed Solve variable vi , > 1, Tower
B
Hanoi. path corresponding macro xA
xi example marked
bold. Successor states first type, well incoming edges, denoted
solid lines. Successor states second type, well incoming edges, denoted
dashed lines. Note size graph constant depend i.

480

fiThe Role Macros Planning

difference cost edge (omitted figure), equals length
corresponding action sequence Solve depends length macros
vi1 .
second example planning domain first suggested Jonsson Backstrom
(1998b). Again, use Pn denote instance containing n variables v1 , . . . , vn .
variable vi V domain D(vi ) = {0, 1}. initial state (v1 = 0, . . . , vn = 0)
goal state (v1 = 0, . . . , vn1 = 0, vn = 1). variable vi , two actions:
ai = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 0); (vi = 1)i,
ai = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 1); (vi = 0)i.
causal graph transitive reduction identical Tower Hanoi, Figure 3
applies well, follows Pn IR.
Jonsson Backstrom (1998b) showed optimal plan Pn length 2n 1.
solve Pn need change value vn 0 1 using action . Since
pre-condition (vn1 = 1), goal state (vn1 = 0), need insert an1
an1 , resulting sequence han1 , , an1 i. Actions an1
an1 specify pre-condition (vn2 = 1), goal state specify (vn2 = 0),
requiring value vn2 change four times. easy show value variable
vi change 2ni times, minimum 2n 1 total actions.
variable vi V , two projected pre-conditions set Z, namely
(v1 = 0, . . . , vi = 0) (v1 = 0, . . . , vi1 = 0, vi = 1). Since specify values
ancestor vi , set macros returned GetMacros contains precisely four
macros, one pair projected pre-conditions. spite fact optimal
solution exponential length number variables, MacroPlanner guaranteed
generate solution Pn polynomial time due complexity results next
section.
3.3 Theoretical Properties
section prove several theoretical properties algorithm MacroPlanner.
ease presentation proofs several theorems moved appendix.
first two theorems related correctness optimality MacroPlanner.
Theorem 3.2. planning problem P IR, macro returned
MacroPlanner(P ) well-defined solves P .
proof Theorem 3.2 appears Appendix A. proof induction variables
v V show macro returned Compose Solve well-defined.
addition, macro returned Solve satisfies projected pre-condition. root
variable, Solve called initial state init, projected pre-condition
goal. macros returned MacroPlanner(P ) precisely returned
call Solve, thus solve P .
Theorem 3.3. planning problem P IR, exists plan solving P ,
MacroPlanner(P ) returns optimal plan P , else returns fail.
481

fiJonsson

proof Theorem 3.3 appears Appendix B. proof double induction
variables v V states Vv visited call Solve, show macros
returned Solve represent shortest solutions subproblems corresponding
call. Thus macro returned Solve root variable optimal
solution achieving goal starting init, precisely definition optimal
plan P .
also prove two complexity results MacroPlanner. Specifically, study two
subclasses class IR prove plan generation polynomial classes,
provided allowed represent resulting plan using hierarchy macros.
variable v V value D(v), define Adv = {a : post(a)(v) = d}
set actions change value v (implying v Vpost(a) ). first prove
lemma relates complexity MacroPlanner number states visited
calls Solve.
Lemma 3.4. Let gv number states visited various calls Solve
P
GetMacros(v, init, A, G). complexity MacroPlanner O(|A| vV gv3 ).
Proof. Solve basically modified version Dijkstras algorithm, quadratic
number nodes underlying graph, O(gv2 ). state p dequeued
priority queue Q once. action O, call Compose line 6 returns
set distinct action sequences, generates one edge p successor states
first type. However, two different actions may generate two edges p
state. true projected pre-conditions Z successor states
second type. Since |A| actions |A| projected
pre-conditions Z, worst-case complexity single call Solve O(|A|gv2 ).
subroutine GetMacros calls Solve successor state second type.
Since gv states, complexity GetMacros O(|A|gv3 ),
P
overall complexity MacroPlanner O(|A| vV gv3 ). Note complexity
Compose included analysis. Compose called Solve generate successor
states either type, number gv states visited Solve proportional
total number sequences returned Compose.
Theorem 3.5. Vpre(a) = Vv variable v V {v } action
Vpost(a) = {v}, complexity MacroPlanner O(|V ||A|4 ).
Theorem 3.6. Assume variable v V , value D(v), pair
actions a, Adv , Vpre(a) = Vpre(a ) = {v} P a(v) pre(a) | P a(v) = pre(a ) | P a(v).
pre(a) | P a(v) = init | P a(v) action post(a)(v) = init(v), algorithm
P
runs polynomial time complexity O(|A| vV |D(v)|3 ).
proofs Theorems 3.5 3.6 appear Appendix C. proof Theorem 3.5
based observation pre-conditions fully specified ancestors
variable v V , number states visited various calls Solve bounded
number actions. proof Theorem 3.6 follows fact algorithm
always achieves value D(v) domain v V state, number
states linear size variable domains.

482

fiThe Role Macros Planning

Two examples planning problems properties established Theorem 3.5
Tower Hanoi domain suggested Jonsson Backstrom (1998b),
previous example section.
example planning problem properties established Theorem 3.6
domain proposed Domshlak Dinitz (2001). set variables V = {v1 , . . . , vn },
domain D(vi ) = {0, 1, 2}, initial state init = (v1 = 0, . . . , vn = 0),
goal state goal = (v1 = 2, . . . , vn = 2). variable vi V , four actions:
a1i = h(vi1 = 2, vi = 0); (vi = 1)i,
a2i = h(vi1 = 0, vi = 1); (vi = 2)i,
a3i = h(vi1 = 2, vi = 2); (vi = 1)i,
a4i = h(vi1 = 0, vi = 1); (vi = 0)i,
v1 , pre-condition vi1 dropped. Domshlak Dinitz (2001) showed
length optimal plan solving planning problem 2n+1 2. However, due
Theorem 3.6, MacroPlanner generates optimal solution polynomial time.
3.4 Pruning
improve running time, possible prune states visited.
Specifically, let (p, seq) state-sequence pair visited call Solve(v, s, Z, O, G).
state-sequence pair (t, seq2) previously visited call
t(v) = p(v), attempt reach p using Compose(v, t, p, O, G). Since p fully
specified state, Compose(v, t, p, O, G) returns one action sequence. Call seq3.
p reachable hseq2, seq3i least short seq, need
visit p, since state reachable via p also reachable via path equal
shorter length. reasoning holds macros state p matches projected
pre-condition z, additional restriction also match z.
possible reach projected goal state goal | Vv s, remove
macros whose pre-condition equals set macros GetMacros(v, init, A, G),
since plan solves original planning problem P could contain macros.
addition, remove macros whose post-condition equals set .
possible reach projected goal state projected initial state init | Vv ,
exists plan solves original planning problem P , immediately return
fail without generating remaining set macros variables.

4. Extending MacroPlanner
section, study ways extend algorithm MacroPlanner broader class
planning problems. Subsection 4.1, propose relaxed definition causal graph
that, applied, extends class IR. show MacroPlanner modified
solve planning problems new class. Subsection 4.2, extend MacroPlanner
planning problems acyclic causal graphs. Subsection 4.3, show combine
results different classes form general class planning problems. Subsection
4.4 illustrates algorithms new classes two well-known examples: Gripper
Logistics.
483

fiJonsson

4.1 Relaxed Causal Graph
conventional definition causal graph (V, E) states edge (w, v) E
w 6= v exists action w Vpre(a) Vpost(a)
v Vpost(a) . Consider action Vpost(a) = {w, v}. applied, changes
values w v. conventional definition, induces cycle since
causal graph includes two edges (w, v) (v, w).
section, propose relaxed definition causal graph. new
definition, acyclic causal graph necessarily imply actions unary.
Assume exists least one action Vpost(a) = {w, v}.
assume multiple actions changing value w, whose pre- postconditions specify values v. Finally, assume exists action
changing value v change value w.
given assumption, way satisfy pre-condition action
changes value v first change value w. changed fact
action might also change value w. opposite true actions
change value w, since value w change independently v.
relaxed definition, causal graph includes edge (w, v) excludes edge (v, w).
4.1.1 Definition Class RIR
Definition 4.1. relaxed causal graph (V, E ) planning problem P directed
graph variables V nodes. edge (w, v) E w 6= v
either
1. exists w Vpre(a) Vpost(a) v Vpost(a) ,
2. exists w, v Vpost(a) either
(a) exists w Vpost(a ) v
/ Vpost(a ) ,
(b) exist w
/ Vpost(a ) v Vpost(a ) .
Edges type 2(b) ensure always least one edge two variables
w, v V appear post-condition action A. possible
change value either without changing value other, relaxed causal
graph contains cycle before. define class RIR relaxed inverted
tree reducible planning problems:
Definition 4.2. planning problem P relaxed inverted tree reducible
relaxed causal graph acyclic transitive reduction relaxed causal graph
inverted tree.
Consider planning problem P IR. Since action unary, relaxed causal
graph contains edges type 2. Consequently, relaxed causal graph identical
conventional causal graph, tree-reducible planning problem also relaxed
tree-reducible, implying IR RIR.
example, consider planning problem two variables v w. Let
D(v) = {0, 1, 2, 3}, D(w) = {0, 1}, init = (v = 0, w = 0), goal = (v = 3, w = 1).
Assume three actions:
484

fiThe Role Macros Planning

h(v = 0); (v = 1)i,
h(v = 2); (v = 3)i,
h(v = 1, w = 0); (v = 2, w = 1)i.
third action unary, since changes value v w. actions
changing value v affect w, opposite true. Hence
relaxed causal graph contains single edge (v, w).
4.1.2 Algorithm
show minor modification MacroPlanner sufficient solve planning
problems class RIR. Since actions longer unary, action
changes value variable v V may also change values variables.
generating macros v, consider set actions change value v
without changing value descendant v relaxed causal graph. Formally,
= {a : v Vpost(a) |Vpost(a) Desc(v)| = 0}.
before, project pre-conditions actions change value
descendant v onto Vv order define subproblems Pv v. Let = {a :
|Vpost(a) Desc(v)| > 0 (pre(a) | Vv ) 6= ()} set actions whose projected
pre-condition onto Vv non-empty. Let action, let macro
v achieves pre-condition a, i.e., post(m) pre(a).
reason applying (unless also satisfies projected pre-condition
action) satisfy pre-condition a. changes value v,
longer projected state post(m) result applying a. words,
generate macros v starting post(m) before. Rather, generate
macros post(m) (post(a) | Vv ), i.e., projected state results applying
post(m).
Algorithm 5 RelaxedPlanner(P )
1: G relaxed causal graph P
2: R transitive reduction G
3: v root variable R
4: RelaxedMacros(v, init, A, R)
5: return , fail =
Algorithm 5 shows modified version MacroPlanner class RIR,
call RelaxedPlanner. difference respect MacroPlanner
G refers relaxed causal graph. Algorithm 6 shows modified version
GetMacros class RIR, call RelaxedMacros. modifications
respect GetMacros appear lines 35 1217. subroutine Solve remains
before.
4.1.3 Theoretical Properties
Theorem 4.3. planning problem P RIR, macro returned
RelaxedPlanner(P ) well-defined solves P .
485

fiJonsson

Algorithm 6 RelaxedMacros(v, init, A, G)
1:
2: L list containing projected initial state init | Vv
3: {a : |Vpost(a) Desc(v)| > 0 (pre(a) | Vv ) 6= ()}
4: Z {pre(a) | Vv : }
5: {a : v Vpost(a) |Vpost(a) Desc(v)| = 0}
6: u P a(v)
7:
RelaxedMacros(u, init, A, G)
8: end
9: elements L
10:
next element L
11:
Solve(v, s, Z, O, G)
12:
= hpre(m), seq(m), post(m)i
13:
{a } post(m) pre(a)
14:
post(m) (post(a) | Vv )
/ L
15:
append post(m) (post(a) | Vv ) L
16:
end
17:
end
18:
end
19: end
20: return
v2
v1
v3

Figure 5: Transitive reduction graph outdegree > 1.
Theorem 4.4. planning problem P RIR, exists plan solving P ,
RelaxedPlanner returns optimal plan P , else returns fail.
proofs Theorems 4.3 4.4 simple adaptations corresponding proofs
MacroPlanner, appear Appendix D.
4.2 Acyclic Causal Graph
section present second extension MacroPlanner, time class
planning problems acyclic causal graphs. words, variables causal
graph may unbounded outdegree, even considering transitive reduction.
Unbounded outdegree poses challenge macro compilation approach, illustrated
following example.
Consider planning problem P V = {v1 , v2 , v3 }, D(v1 ) = {0, 1, 2}, D(v2 ) =
D(v3 ) = {0, 1}. initial state (v1 = 0, v2 = 0, v3 = 0), goal state (v2 = 1, v3 = 1),
486

fiThe Role Macros Planning

contains following actions:
a11 = hv1 = 0; v1 = 1i,
a21 = hv1 = 0; v1 = 2i,
a12 = hv1 = 1, v2 = 0; v2 = 1i,
a13 = hv1 = 2, v3 = 0; v3 = 1i.
Figure 5 shows causal graph P , identical transitive reduction case. Since
variable v1 outdegree 2, follows P
/ IR (implying P
/ RIR since actions
unary). point view variable v2 , possible set v2 1: apply a11
change v1 0 1, followed a12 change v2 0 1. point view
v3 , possible set v3 1 applying a21 followed a13 . Nevertheless, valid
plan solving planning problem.
algorithm MacroPlanner solves planning problems decomposing
subproblems variable. seen efficient certain subclasses
IR, guarantees optimality. However, variables unbounded outdegree
longer possible define subproblems state variable isolation.
run MacroPlanner example planning problem above, generate macro
setting v2 1, macro setting v3 1. might lead us believe P
solution, fact not.
4.2.1 Definition Class AR
extend MacroPlanner planning problems acyclic causal graphs, impose
additional restriction planning problems. state variable v V outdegree
greater 1 causal graph, require v reversible.
Definition 4.5. state variable v V reversible if, state Vv
reachable projected initial state init | Vv , projected initial state init | Vv
reachable s.
Definition 4.6. planning problem P belongs class AR causal graph P
acyclic variable reversible.
4.2.2 Algorithm
section show modify algorithm MacroPlanner solves planning problems AR. first describe algorithm called ReversiblePlanner
requires variables reversible. ReversiblePlanner, appears Algorithm 7,
operates directly causal graph planning problem P . Thus, compute
transitive reduction causal graph. Another feature subroutine GetMacros omitted. Instead, ReversiblePlanner directly calls ReversibleCompose,
equivalent subroutine Compose.
Algorithm 8 describes subroutine ReversibleCompose. Compose returns
set sequences reaching partial state x, ReversibleCompose returns
pair sequences. Here, U set variables appear post-condition
action whose pre-condition x wish satisfy. first sequence seq satisfies x
487

fiJonsson

Algorithm 7 ReversiblePlanner(P )
1: G [relaxed] causal graph P
2: (seq, seq2) ReversibleCompose(, init, goal, A, G)
3: return seq
Algorithm 8 ReversibleCompose(U, s, x, A, G)
1: seq hi, seq2 hi
2: w Vx topological order
3:
x(w) 6= s(w)
4:
ReversibleSolve(w, s, x(w), A, G)
5:
= fail
6:
return (fail,fail)
7:
else
8:
seq hm, seqi
9:
w
/ U

10:
ReversibleSolve(w, (w = x(w)), s(w), A, G)
11:
seq2 hseq2,
12:
end
13:
end
14:
end
15: end
16: return (seq, seq2)

starting state s. fails, ReversibleCompose returns pair sequences
(fail,fail). Otherwise, second sequence seq2 returns post-condition
first sequence applied s. exception occurs variables U V ,
returned values s.
ReversibleCompose requires topological sort variables causal graph G,
obtained polynomial time acyclic graphs. example topological
sort v1 , v2 , v3 causal graph Figure 5. values x satisfied reverse
topological order way sequence seq constructed, i.e., new macro
inserted first. ReversibleCompose calls ReversibleSolve obtain macro
changing value variable w Vx s(w) x(w). succeeds w
/
U , ReversibleCompose calls ReversibleSolve second time obtain macro
resetting value w x(w) s(w). macro appended sequence seq2,
resets values variables Vx U topological order.
Algorithm 9 describes ReversibleSolve, equivalent subroutine Solve. Unlike Solve, ReversibleSolve generates single macro changing value
v s(v) d. Moreover, ReversibleSolve invoke Dijkstras algorithm; instead, performs breadth-first search values joint domain W . variable
w Vv belongs W exists action changing value v one descendants w post-condition. Note actions unary, W = {v},
breadth-first search values domain D(v) v.
488

fiThe Role Macros Planning

Algorithm 9 ReversibleSolve(v, s, d, A, G)
1: W {w Vv : w Vpost(a) Vpost(a) (Desc(v) {v}) 6= }
2: {a : v Vpost(a) [Desc(v) Vpost(a) = ] }
3: L list containing state-sequence pair (s | W, hi)
4: L contains elements
5:
(p, seq) next element L
6:
p = (s | W ) (v = d)
7:
return h(s | Vv , seq, (s | Vv ) (v = d)i
8:
end
9:
pre(a) (v = p(v))
10:
(seq2, seq3) ReversibleCompose(Vpost(a) , p, pre(a), A, G)
11:
seq2 6= fail 6 (p , seq ) L p = p post(a)(v)
12:
insert (p post(a)(v), hseq, seq2, a, seq3i) L
13:
end
14:
end
15: end
16: return fail
partial state p visited search associated sequence seq used arrive
p. p = (s | W ) (v = d), ReversibleSolve returns macro corresponding
given sequence. Note macro changes value v, leaving values
variables unchanged. Otherwise, ReversibleSolve tries actions changing
value v whose pre-condition compatible (v = p(v)). satisfy pre-condition
action state p, ReversibleSolve calls ReversibleCompose.
ReversibleCompose returns legal pair sequences (seq2, seq3) partial state
p post(v)(a) previously visited, ReversibleSolve generates new statesequence pair (ppost(v)(a), hseq, seq2, a, seq3i). Since U = Vpost(a) , seq3 resets variables
values p except whose values changed a.
Since equivalent GetMacros omitted, ReversibleMacros goal-driven
sense ReversibleSolve called whenever needed subroutine ReversibleCompose. use memoization cache results ReversibleCompose
ReversibleSolve avoid recomputing result once. expressions within square brackets indicate changes necessary algorithm
work planning problems acyclic relaxed causal graphs. Note, however,
ReversiblePlanner incomplete planning problems AR non-unary actions.
4.2.3 Theoretical Properties
Theorem 4.7. planning problem P AR, sequence seq returned
ReversiblePlanner(P ) different fail, seq well-defined init satisfies (init post(seq)) goal.
Theorem 4.8. planning problem P AR unary actions, P solvable,
ReversiblePlanner(P ) returns sequence seq solving P , else returns fail.

489

fiJonsson

Theorem 4.9. planning problem P AR, complexity ReversiblePlanner O(Dk |A|(Dk+1 + |V |)), = maxvV D(v) k = maxvV |W |.
proofs Theorems 4.7, 4.8, 4.9 appear Appendix E. proofs Theorems
4.7 4.8 similar MacroPlanner, show induction variables
v V macros returned ReversibleCompose ReversibleSolve
well-defined. However, instead optimal plans, ReversiblePlanner returns plan
one exists.
proof Theorem 4.9 based fact call ReversibleSolve,
state variables Vv W always take initial values. bounds total
number states calls ReversibleSolve. Note Theorem 4.9 implies
complexity ReversiblePlanner O(D|A|(D2 + |V |)) planning problems AR
unary actions, since |W | = 1 v V case. general, size
sets W fixed, complexity ReversiblePlanner polynomial. Also note
Tower Hanoi belongs AR since variable reversible, planning problems
AR may exponentially long solutions.
4.3 Class AOR
combine two results classes IR AR. idea use
original algorithm exhaustively generate macros variable v, long
outdegree v transitive reduction causal graph less equal 1.
Whenever encounter variable v outdegree larger 1, switch algorithm
reversible variables previous section. way, handle acyclic causal
graphs even variables reversible. call resulting class AOR.
4.3.1 Definition Class AOR
Definition 4.10. planning problem P belongs class AOR causal graph
P acyclic variable v V outdegree > 1 transitive reduction
causal graph reversible.
example planning problem previous section belong class
AOR, since variable v1 outdegree 2 reversible. However, assume add
following two actions A:
a31 = hv1 = 1; v1 = 0i,
a41 = hv1 = 2; v1 = 0i.
new actions, variable v1 reversible since value v1 reachable
value. Since v1 state variable outdegree greater 1
transitive reduction, P AOR.
4.3.2 Algorithm
section, describe AcyclicPlanner, combines ideas MacroPlanner ReversiblePlanner solve planning problems class AOR. AcyclicPlanner appears Algorithm 10. like MacroPlanner, AcyclicPlanner operates
490

fiThe Role Macros Planning

Algorithm 10 AcyclicPlanner(P )
1: G [relaxed] causal graph P
2: R transitive reduction G
3: (S, seq, seq2) AcyclicCompose(v , init, goal, A, G, R)
4: =
5:
return fail
6: else
7:
return seq, hseq , seqi seq
8: end
transitive reduction R causal graph. However, since ReversiblePlanner
operates causal graph G itself, AcyclicPlanner passes G R
subroutine AcyclicCompose.
Algorithm 11 AcyclicCompose(v, s, x, A, G, R)
1: {hi}
2: x x projected onto non-reversible variables
3: w P a(v)
4:
outdegree(R, w) 1 (s | Vw ) 6 (x | Vw )
5:
AcyclicSolve(w, | Vw , A, G, R)
6:
{hs | Vw , seq(m), post(m)i : post(m) (x | Vw )}
7:
=
8:
return (,fail,fail)
9:
end
10:
ST
11:
end
12: end
13: seq hi, seq2 hi
14: w Vx reverse topological order
15:
outdegree(R, w) > 1 (s | Vw ) 6 (x | Vw )
16:
(seq3, seq4) ReversibleCompose(, | Vw , x, A, G).
17:
seq3 = fail
18:
return (,fail,fail)
19:
else
20:
seq hseq, seq3i
21:
seq2 hseq4, seq2i
22:
end
23:
end
24: end
25: return (S, seq, seq2)
seen, AcyclicPlanner calls AcyclicCompose dummy variable
Note that, contrary before, transitive reduction causal graph may contain
several edges v . reason single root variable causal graph.

v.

491

fiJonsson

Instead, may multiple sink variables causal graph, i.e., variables
outgoing edges. transitive reduction contains edge sink variable Vgoal
v .
Algorithm 11 describes subroutine AcyclicCompose. Although looks somewhat
complex, lines 3 12 really adaptation Compose class AOR.
difference AcyclicSolve generate macros satisfying
projected pre-condition x; instead, generates macros satisfying projection x x
onto non-reversible variables. idea reversible variables always remain
initial values. way, treat parents w P a(v) v independent,
since common ancestors outdegree larger 1 transitive reduction,
thus reversible definition.
Lines 13 24 effect satisfying values x variables Vx whose
outdegree transtitive reduction larger 1. done simply calling
subroutine ReversibleCompose previous section. AcyclicCompose returns
three values: set sequences satisfying partial state x respect nonreversible variables, sequence seq satisfying partial state x respect reversible
variables, sequence seq2 resetting reversible variables initial values.
Note ReversibleCompose called variable w, predecessor w
processed using ReversibleSolve ReversibleCompose well. work,
predecessors w reversible, guaranteed following lemma:
Lemma 4.11. v reversible, every predecessor v.
Proof. contrapositive. Assume w non-reversible predecessor v.
exists state Vw reachable projected initial state init | Vw
init | Vw reachable s. Since w predecessor v, Vw subset Vv ,
state (init | Vv ) reachable init | Vv , vice versa. Thus v
reversible.
Algorithm 12 describes subroutine AcyclicSolve. difference
Solve AcyclicSolve AcyclicSolve calls AcyclicCompose obtain sequences satisfying projected pre-condition. Moreover, whenever satisfying precondition pre(a) action, AcyclicCompose applies sequence seq3 reset reversible variables initial values. addition, AcyclicCompose satisfy
projected pre-condition z reversible variables, since append sequence
seq2 resulting sequence. before, expressions within square brackets
need added deal acyclic relaxed causal graphs.
4.3.3 Theoretical Properties
Theorem 4.12. planning problem P AOR unary actions, exists
plan solving P , sequence seq set returned AcyclicPlanner well-defined
init satisfies (init post(seq)) goal.
Proof. prove Theorem 4.12, sufficient combine results MacroPlanner
ReversiblePlanner. subroutines AcyclicCompose AcyclicSolve identical Compose Solve variables outdegree less equal 1, Lemmas
492

fiThe Role Macros Planning

Algorithm 12 AcyclicSolve(v, s, A, G, R)
1:
2: {a : v Vpost(a) [Desc(v) Vpost(a) = ] }
3: Z {pre(a) | Vv : |Vpost(a) Desc(v)| > 0} {()}
4: Q priority queue containing state-sequence pair (s, hi)
5: Q non-empty
6:
(p, seq) remove highest priority state-sequence pair Q
7:
pre(a) (v = p(v))
8:
(S, seq2, seq3) AcyclicCompose(v, p, pre(a), A, G, R)
9:
sequences seq4
10:
insert (p post(hseq4, seq2, a, seq3i), hseq, seq4, seq2, a, seq3i) Q
11:
end
12:
end
13:
z Z z (v = p(v))
14:
(S, seq2, seq3) AcyclicCompose(v, p, z, P, G, R)
15:
sequences seq4
16:
{hs, hseq, seq4i, p post(seq4)i}
17:
end
18:
end
19: end
20: return
A.1 A.2 imply resulting sequences macros desired properties.
variables outdegree larger 1, ReversibleCompose returns sequences
desired properties due Lemma E.1.
Theorem 4.13. Assume exists constant k non-reversible
variable v V , number non-reversible ancestors v less equal k.
addition, reversible variable v V , |W | k, W set defined
ReversibleSolve. AcyclicPlanner runs polynomial time.
Proof. Again, combine results MacroPlanner ReversiblePlanner.
First note call ReversibleCompose polynomial due Theorem 4.9. Lemma
P
3.4 states complexity MacroPlanner O(|A| vV gv3 ), gv number states visited calls Solve v. Since AcyclicCompose AcyclicSolve identical Compose Solve non-reversible variables, argument
proof Lemma 3.4 holds AcyclicSolve well. fact v k
non-reversible predecessors implies gv = O(Dk+1 ), since reversible predecessors always take
initial values.
Note argument Lemma 3.4 take account external call
AcyclicCompose AcyclicPlanner, may exponential number
|P a(v )| parents v . slight modification algorithm necessary prove
theorem. modify external call returns single sequence.
words, w P a(v ), keep one macros returned call
AcyclicSolve w. Note alter admissibility solution.
493

fiJonsson

(a)

(b)

(c)

v1
vl

vh

v1
vl

vh

v1
vl

v2

v2

vh

v*
v2

Figure 6: (a) Conventional (b) relaxed causal graph, (c) transitive reduction
relaxed causal graph Gripper.

4.4 Examples
section, provide examples planning problems AR AOR, describe
ReversiblePlanner AcyclicPlanner solve them.
4.4.1 Gripper
first example well-known Gripper domain, robot transport balls
two rooms. Gripper, planning problem Pn defined number n balls
robot transport. set variables V = {vl , vh , v1 , . . . , vn } domains
D(vl ) = {0, 1}, D(vh ) = {0, 1, 2}, D(vi ) = {0, 1, R} 1 n. initial state
init = (vl = 0, vh = 0, v1 = 0 . . . , vn = 0) goal state goal = (v1 = 1, . . . , vn = 1).
two rooms denoted 0 1. Variable vl represents location robot, i.e.,
either two rooms. Variable vh represents number balls currently held
robot, maximum 2. Finally, variable vi , 1 n, represents location ball
i, i.e., either two rooms held robot (R). set contains following
actions:
h(vl = a); (vl = 1 a)i,
h(vl = a, vh = b, vi = a); (vh = b + 1, vi = R)i,
h(vl = a, vh = b + 1, vi = R); (vh = b, vi = a)i,

{0, 1},
a, b {0, 1}, 1 n,
a, b {0, 1}, 1 n.

Actions first type move robot rooms. Actions second type cause
robot pick ball current location, incrementing number balls held.
Actions third type cause robot drop ball current location, decrementing
number balls held.
1 n, exist actions changing value vi also change
value vh . Consequently, conventional causal graph contains edges (vh , vi )
(vi , vh ), inducing cycles. However, actions changing value vi
also change value vh . opposite true; actions vh
affect value vi (assuming n > 1). Thus, relaxed causal graph contains
edge (vh , vi ), (vi , vh ). Figure 6 shows conventional relaxed causal graphs,
well transitive reduction relaxed causal graph, Gripper n = 2.
transitive reduction includes dummy variable v well incoming edges.
494

fiThe Role Macros Planning

relaxed causal graph acyclic, variable reversible since return initial state state. Thus Gripper belongs class AR
considering relaxed causal graph. Since actions Gripper non-unary, ReversiblePlanner guaranteed find solution. However, show
fact find solution Gripper.
ReversiblePlanner calls ReversibleCompose initial goal states.
turn, ReversibleCompose goes variable w Vgoal calls ReversibleSolve obtain macro setting value w goal value. Since
directed paths variables representing balls, topological sort orders variables arbitrarily. Without loss generality, assume topological order v1 , . . . , vn .
Thus first variable processed ReversibleCompose v1 . ReversibleCompose
calls ReversibleSolve v = v1 , = init = 1.
v1 , set W = {vh , v1 } contains variables vh v1 . set contains
actions second third type picking dropping ball 1. list L initially
contains partial state (vh = 0, v1 = 0). two actions whose pre-conditions
match (v1 = 0): one picking ball 1 room 0 assuming ball currently held
(vh = 0), one picking ball 1 assuming one ball currently held (vh = 1).
these, ReversibleSolve calls ReversibleCompose U = Vpost(a) , = init,
x = pre(a) obtain sequence satisfying pre-condition action.
pre-condition first action already satisfied init, ReversibleCompose
returns pair empty sequences. result, ReversibleSolve adds partial state
(vh = 1, v1 = R) L, corresponding post-condition action. Note
set empty variable vh , since actions changing value vh
also change value descendants. Thus possible satisfy
pre-condition (vh = 1) second action starting initial state without changing
value v1 . Consequently, ReversibleCompose returns (fail,fail), new
partial state added L.
(vh = 1, v1 = R), four actions whose pre-conditions match (v1 = R):
two dropping ball 1 room 0, two dropping ball 1 room 1. Two
pre-condition (vh = 2) impossible satisfy without changing value
v1 . Dropping ball 1 room 0 returns initial state, new partial states
added L. Dropping ball 1 room 1 requires pre-condition (vl = 1, vh = 1). Since
(vh = 1) already holds, ReversibleCompose calls ReversibleSolve change value
vl 0 1. possible using action changing location robot.
ReversibleCompose also returns sequence returning robot room 0.
result, ReversibleSolve adds partial state (vh = 0, v1 = 1) L, corresponding post-condition action. Note partial state satisfies
(vh = 0, v1 = 1) = (vh = 0, v1 = 0) (v1 = 1), ReversibleSolve returns macro
changes value v1 without changing values variables. Since ReversibleSolve successfully returned macro, ReversibleCompose calls ReversibleSolve obtain macro returning ball 1 initial location. continues
balls. final solution contains macros changing location ball
room 0 room 1 one time.
Note complexity ReversibleSolve polynomial Gripper due Theorem 4.9, since |W | 2 vi , 1 n. Also note solution
495

fiJonsson

t1

tn

p1
v*

u1

pm

uq

Figure 7: Causal graph transitive reduction Logistics.
optimal, since robot could carry two balls once. could also solve Gripper using
AcyclicPlanner, would treat variables vi , 1 n, non-reversible since
outdegree 0 transitive reduction relaxed causal graph. solution
fact case, since single way move ball room 0
room 1.
4.4.2 Logistics
second example Logistics domain, number packages
moved final location using trucks airplanes. Let denote number
packages, n number trucks, q number airplanes. multi-valued variable
formulation planning problem Logistics given V = {p1 , . . . , pm } ,
= {t1 , . . . , tn , u1 , . . . , uq }, pi , 1 m, packages, tj , 1 j n, trucks, uk ,
1 k q, airplanes.
Let L set possible locations packages. partition L1 . . . Lr
L , 1 l r, corresponds locations city. also
subset C L corresponding airports, |C | 1 1 l r.
pi , domain D(pi ) = L corresponds possible locations plus inside trucks
airplanes. tj , D(tj ) = 1 l r. uk , D(uk ) = C.
initial state assignment values variables, goal state assignment
packages pi among values L. set contains following three types actions:
h(t = l1 ); (t = l2 )i,
h(t = l, pi = l); (pi = t)i,
h(t = l, pi = t); (pi = l)i,

, l1 6= l2 D(t),
, l D(t), 1 m,
, l D(t), 1 m.

first type action allows truck airplane move two locations
domain. second type action loads package truck airplane
location. third type action unloads package truck airplane.
Figure 7 shows causal graph transitive reduction planning problem
Logistics. transitive reduction identical causal graph case.
Gripper, figure shows dummy variable v well incoming edges.
easy see causal graph acyclic. addition, variables reversible
496

fiThe Role Macros Planning

Discs

Time (ms)

Macros

Length

10
20
30
40
50
60

31
84
161
279
457
701

27/82
57/172
87/262
117/352
147/442
177/532

> 103
> 106
> 109
> 1012
> 1015
> 1018

Table 1: Results Tower Hanoi.

since possible return initial state state. Thus could apply
ReversiblePlanner solve planning problems Logistics.
solve planning problem Logistics, ReversiblePlanner calls ReversibleCompose initial goal states. Since directed paths
variables representing location packages, topological sort orders variables arbitrarily. Without loss generality, assume topological order p1 , . . . , pn . Thus
first variable processed ReversibleCompose p1 . ReversibleCompose calls
ReversibleSolve v = p1 , = init equal goal location package.
p1 , set W = {p1 } contains p1 since actions unary. set contains
actions picking dropping package. Although may many ways move
package initial location goal location, ReversibleSolve considers
way moving package. moving package, trucks airplanes
returned initial locations. ReversibleCompose also calls ReversibleSolve
generate macro returning package initial location. final solution
consists macros moving package final location one time.
Note complexity ReversibleSolve polynomial Logistics due
Theorem 4.9, since |W | 1 pi , 1 m. Also note solution
optimal, since trucks airplanes always returned initial locations. could
also solve Logistics using AcyclicPlanner, would treat variables pi , 1 m,
non-reversible since outdegree 0 transitive reduction relaxed causal
graph. Again, solution since ancestor pi reversible, states
AcyclicSolve always trucks airplanes initial locations.

5. Experimental Results
test algorithm, ran experiments two domains: Tower Hanoi extended
version Gripper. results experiments Tower Hanoi appear Table
1. varied number discs increments 10 recorded running time
MacroPlanner. table also shows number macros generated algorithm,
those, many used represent resulting global plan. example,
27 82 generated macros formed part solution Tower Hanoi 10 discs.
second set experiments, modified version Gripper previous section. Instead two rooms, environment consists maze 967 rooms.
transport balls, robot must navigate maze initial location
497

fiJonsson

Balls

Time (ms)

Macros

Length

100
101
102
103

1528
1535
1570
2906

3
14
104
1004

300 100
300 101
300 102
300 103

Table 2: Results Gripper.

goal location. robot pick drop balls initial goal locations.
results running ReversiblePlanner corresponding planning problems appear Table 2. Since ReversiblePlanner generates macros necessary,
used solution. experiments illustrate benefit using macros
store solution navigating maze. algorithm use macros
recompute path maze every time goes pick new ball.

6. Related Work
ease presentation, group related work three broad categories: complexity
results, macro-based planning, factored planning. following subsections
presents related work one categories.
6.1 Complexity Results
Early complexity results planning focused establishing hardness different formulations general planning problem. STRIPS formalism used, deciding whether
solution exists undecidable first-order case (Chapman, 1987) PSPACEcomplete propositional case (Bylander, 1994). PDDL, representation language used International Planning Competition, fragment including delete lists
EXPSPACE-complete (Erol, Nau, & Subrahmanian, 1995).
Recently, several researchers studied tractable subclasses planning problems
provably solved polynomial time. subclasses based notion
causal graph (Knoblock, 1994), models degree independency
problem variables. However, Chen Gimenez (2008a) showed connected causal
graph causes problem hard (unless established assumptions P = NP fail),
additional restrictions problem necessary.
common restriction variables problem binary. Jonsson
Backstrom (1998a) defined class 3S planning problems acyclic causal graphs
binary variables. addition, variables either static, symmetrically reversible,
splitting. authors showed possible determine polynomial time whether
solution exists, although solution plans may exponentially long.
Gimenez Jonsson (2008) designed macro-based algorithm solve planning
problems 3S polynomial time. algorithm works generating macros change
value single variable time, maintaining initial values ancestors
variable. fact, strategy used algorithm ReversiblePlanner,
498

fiThe Role Macros Planning

thus seen extension algorithm multi-valued reversible variables
general acyclic causal graphs.
Brafman Domshlak (2003) designed polynomial-time algorithm solving planning problems binary variables polytree acyclic causal graphs bounded indegree.
Gimenez Jonsson (2008) showed causal graph unbounded indegree,
problem becomes NP-complete. Restricting pre-condition action
two variables causes problem class become tractable (Katz & Domshlak, 2008a). Since hardness proof based reduction planning problems
inverted tree causal graphs, results apply class IR restricting inverted
tree reducible planning problems.
Another result applies class IR hardness planning problems
directed path causal graphs (Gimenez & Jonsson, 2008). Gimenez Jonsson (2009)
extended result multi-valued variables domains size 5. Katz
Domshlak (2008b) showed planning problems whose causal graphs inverted forks
tractable long root variable domain fixed size. class also
fragment IR. words, class IR includes several known tractable
intractable fragments, although tractability results proven present paper
novel.
tractability results include work Haslum (2008), defined planning
problems terms graph grammars, showed resulting class tractable
certain restrictions grammar. Chen Gimenez (2007) defined width planning
problems designed algorithm solving planning problems whose complexity
exponential width. words, planning problems constant width
tractable.
idea using reversible variables related work Williams Nayak
(1997), designed polynomial-time algorithm solving planning problems
acyclic causal graphs reversible actions. words, action symmetric counterpart pre-condition except reverses effect former.
addition, algorithm requires two actions pre-conditions one
proper subset other. definition reversible variables flexible
require actions reversible.
Since class IR associated algorithm introduced, two tractability
results based macros appeared literature. already mentioned
work Gimenez Jonsson (2008) class 3S. Also, Chen Gimenez (2008b)
presented polynomial-time algorithm generates macros within Hamming distance
k state, defined associated tractable class planning problems constant
Hamming width. class also contains Tower Hanoi, relationship IR
fully established.
6.2 Macro-Based Planning
also worth mentioning relationship macro-based algorithms planning.
Macros long used planning, beginning advent STRIPS representation (Fikes & Nilsson, 1971). Minton (1985) Korf (1987) developed idea
further, latter showing macros exponentially reduce search space chosen
499

fiJonsson

carefully. (Knoblock, 1994) developed abstraction technique similar macros,
problem treated different levels abstractions fully solved. Vidal
(2004) extracted macros relaxed plans used generate heuristics.
Methods Macro-FF (Botea, Enzenberger, Muller, & Schaeffer, 2005) automatically generate macros experimentally shown useful search, proved
competitive fourth International Planning Competition. Typically, macros introduced flat sequences two three actions. stands stark contrast
algorithms, macros may hierarchical arbitrarily long sequences.
words, two approaches different: one tries augment PDDL slightly
longer sequences hope speeding search, attempts generate
macros needed solve subproblems associated variable.
Recent work macros admits longer action sequences (Botea, Muller, & Schaeffer, 2007;
Newton, Levine, Fox, & Long, 2007).
6.3 Factored Planning
Another related field work factored planning, attempts decompose planning
problem one several subproblems. Typically, planning problem factored
several subdomains, organized tree structure. variable action
problem belongs one subdomains. Amir Engelhardt (2003) introduced
algorithm called PartPlan solves planning problems tree decomposition already exists. algorithm exponential maximum number actions
variables subdomain.
Brafman Domshlak (2006) introduced algorithm called LID-GF decomposes planning problems based causal graph. LID-GF polynomial planning
problems fixed local depth causal graphs fixed tree-width. local depth
variable defined number times value variable change
plan solving problem. Interestingly, Tower Hanoi, solved polynomial
time MacroPlanner, exponential local depth causal graph unbounded
tree-width. Kelareva, Buffet, Huang, Thiebaux (2007) proposed algorithm factored planning automatically chooses order solving subproblems. algorithm
requires subproblem clustering given, algorithms automatically derives
subproblem order.

7. Conclusion
paper, introduced class IR inverted tree reducible planning problems
presented algorithm called MacroPlanner uses macros solve planning
problems class. algorithm provably complete optimal, runs polynomial time several subclasses IR.
also extended class IR several ways. class RIR allows causal graph
relaxed, associated algorithm called RelaxedPlanner able solve planning
problems RIR optimally. class AR allows general acyclic causal graphs long
variables reversible, associated algorithm called ReversiblePlanner
solve planning problems unary actions AR non-optimally polynomial time.
non-unary actions, ReversiblePlanner complete, although still solve
500

fiThe Role Macros Planning

problems Gripper. Finally, class AOR allows acyclic causal graphs long
variable outdegree larger 1 transitive reduction reversible.
algorithm AcyclicPlanner combines ideas algorithms solve planning
problems class AOR.
validated theoretical properties algorithm two sets experiments.
first set, algorithm applied Tower Hanoi. Although optimal solution
exponential, algorithm guaranteed solve problems domain polynomial
time. reason macros obviate need solve subproblem multiple
times. second set experiments, applied algorithm extended version
Gripper. Again, results demonstrate power using macros store solution
subproblems.
major contribution paper extending set known tractable classes
planning problems. Katz Domshlak (2008b) suggested projecting planning problems
onto known tractable fragments order compute heuristics. perfectly possible
using algorithms. classes IR RIR, resulting heuristic admissible since corresponding solution optimal. Even solution exponentially long,
computing solution length done polynomial time using dynamic programming.
two domains share part causal graph structure, macros generated one domain
could used other. could save significant computational effort since scheme
compiles away variables replaces macros. Since macros functionally
equivalent standard actions, used place actions variables
replace. Macros also prove useful domains backtrack find
optimal solution. macros already generated stored variables,
need recompute partial plans variables scratch.
multi-valued representation included notion objects, would possible
generate macros one object share macros among identical objects, like
parametrized operators standard PDDL planning domain. example,
Logistics domain, truck operates city functionally equivalent,
macro generated one truck applied another. true
airplanes. Finally, obtain optimal plans classes AR AOR, one would
test ways interleave subplans different variables, process likely
tractable unless number interleaved subplans constant.

Acknowledgments
work partially funded APIDIS MEC grant TIN2006-15387-C03-03.

Appendix A. Proof Theorem 3.2
appendix prove Theorem 3.2, states macro returned
MacroPlanner(P ) well-defined solves P . First, prove series lemmas
state sequences macros returned Compose Solve well-defined.
Lemma A.1. v V , state Vv , partial state x Vv
x (v = s(v)), macro parents v well-defined, action sequence
seq returned Compose(v, s, x, O, G) well-defined (s post(seq)) x.
501

fiJonsson

Proof. Compose(v, s, x, O, G) returns sequences type seq = hm1 , . . . , mk i,
mi = hs | Vw , seq(mi ), post(mi )i macro parent w P a(v) v (s | Vw ) 6
(x | Vw ). Since pre(mi ) = | Vw holds s, mi applicable s. Since transitive
reduction causal graph inverted tree, parents v common ancestors,
sets variables Vw disjoint. consequence, application m1 , . . . , mi1
change values variables Vw , | Vw still holds mi applied.
Assuming macro mi well-defined, (s | Vw ) post(seq(mi )) = post(mi ),
Compose(v, s, x, O, G) considers macros mi w post(mi ) (x | Vw ). Since
macro seq changes values variables Vw , holds (s post(seq))
(x | Vw ). (s | Vw ) (x | Vw ) parent w P a(v) v, seq contain macro
w, (s post(seq)) | Vw = | Vw , implying (s post(seq)) (x | Vw ). Finally, macro
seq changes value v, (s post(seq))(v) = s(v). Since x (v = s(v)) definition
(s post(seq)) (x | Vw ) w P a(v), holds (s post(seq)) x.
Lemma A.2. v V state Vv , macro hpre(m), seq(m), post(m)i
returned call Solve(v, s, Z, O, G) well-defined, post(m) z z Z.
Proof. proof double induction v state-sequence pairs (p, seq) visited
call Solve(v, s, Z, O, G). particular, show seq well-defined
p = post(seq). first state-sequence pair removed Q line 4
(s, hi). Clearly, hi well-defined = post(hi). Otherwise, assume
seq well-defined p = post(seq). line 8, state-sequence pair
(p post(hseq2, ai), hseq, seq2, ai) added Q action pre(a)
(v = p(v)) sequence seq2 returned Compose(v, p, pre(a), O, G).
induction, macros returned Solve(w, , Z , , G) well-defined
w P a(v) state Vw . Lemma A.1 implies sequence seq2
returned Compose(v, p, pre(a), O, G) well-defined p (p post(seq2))
pre(a). words, applicable p post(seq2), implying hseq2, ai welldefined p. assumption, seq well-defined p = post(seq), implying
hseq, seq2, ai well-defined s. applied s, hseq, seq2, ai results state
p post(hseq2, ai), implying p post(hseq2, ai) = hseq, seq2, ai.
line 14, macro = hs, hseq, seq2i, p post(seq2)i added projected pre-condition z Z z (v = p(v)) sequence seq2 returned
Compose(v, p, z, O, G). Lemma A.1 implies seq2 well-defined p
(p post(seq2)) z. Since seq well-defined p = post(seq), hseq, seq2i
well-defined results state p post(seq2) applied s. follows
macro well-defined post(m) = p post(seq2) z.
proceed prove Theorem 3.2. First note MacroPlanner(P ) calls
GetMacros(v, init, A, G) obtain set macros , G = R transitive
reduction causal graph P v root variable R. Since variable
ancestor v, follows Vv = V init | Vv = init. projected
pre-condition Z dummy action , equals goal | Vv = goal. Thus
GetMacros(v, init, A, G) calls Solve(v, s, Z, O, G) = init Z = {goal}.
Lemma A.2 implies macro returned Solve(v, init, {goal}, O, G) welldefined post(m) goal. Since pre-condition macro returned
502

fiThe Role Macros Planning

Solve(v, init, {goal}, O, G) init well-defined, sequence seq(m) = ha1 , . . . , ak
well-defined init and, applied init, results state init post(seq(m)) =
post(m) goal satisfies goal state. Thus, solves P .

Appendix B. Proof Theorem 3.3
appendix prove Theorem 3.3, states MacroPlanner(P ) returns
optimal solution P IR one exists. First, prove series lemmas
state macros returned Solve represent shortest solutions
corresponding subproblems.
Definition B.1. v V pair states (s, t) Vv , let denote
exists sequence actions Av = {a : Vpost(a) Vv } that, applied
state s, results state t.
Lemma B.2. v V , let (p, seq) state-sequence pair visited call
Solve(v, s, Z, O, G). w P a(v), GetMacros(w, init, A, G) previously
called Solve(w, p | Vw , Z , , G), Z values Z w.
Proof. GetMacros(w, init, A, G) calls Solve(w, , Z , , G) state list
L. p = init | Vv projected initial state Vv , p | Vw = init | Vw added L
line 2 GetMacros. Otherwise, actions changing values variables
Vw macros generated GetMacros(w, init, A, G). projection p | Vw thus
equal post-condition macro. distinct post-condition post(m)
macro w added L line 12 GetMacros.
Lemma B.3. v V , pair states (s, u) Vv u,
projected pre-condition z Z u z, Solve(v, s, Z, O, G) returns macro
hs, seq, ti z shortest path u prefix seq.
Proof. induction v. P a(v) = , set actions = Av , u implies
existence sequence actions Av u. Since projected pre-condition
z Z non-empty, u z implies u = z. Since state-sequence pairs visited order
shortest sequence length, Solve(v, s, Z, O, G) guaranteed return macro hs, seq, ui
seq shortest path u. Clearly, u shortest path u,
seq prefix itself.
|P a(v)| > 0, use induction state-sequence pairs (p, seq) visited
call Solve(v, s, Z, O, G) prove lemma. Namely, prove p u,
Solve(v, s, Z, O, G) returns macro hs, hseq, seq2i, ti z shortest
path p u prefix seq2. exception rule exists action
sequence shorter hseq, seq2i pass p.
base case given p(v) = u(v). w P a(v), (p | Vw ) 6 (z | Vw ) implies
z | Vw Z , u z implies (u | Vw ) (z | Vw ), GetMacros(w, init, A, G) previously called
Solve(w, p | Vw , Z , , G) due Lemma B.2, p u implies (p | Vw ) (u | Vw )
removing actions Aw . induction, Solve(w, p | Vw , Z , , G) returned macro
hp | Vw , seq , (z | Vw ) shortest path p | Vw u | Vw
prefix seq . particular, macro hp | Vw , seq , part set O.
503

fiJonsson

iteration Solve(v, s, Z, O, G) (p, seq), Compose(v, p, z, O, G) called
line 12 since u z p(v) = u(v) imply z (v = p(v)). resulting set contains
sequence seq2 consists macros hp | Vw , seq , w P a(v)
(p | Vw ) 6 (z | Vw ). line 14 macro hs, hseq, seq2i, ti added ,
= p post(seq2) z due Lemma A.1. Since seq2 change value v, since
subsets Vw disjoint, since shortest path p | Vw u | Vw
prefix seq w P a(v), shortest path p u prefix seq2.
p(v) 6= u(v), path p u include actions change value
v. Let first action shortest path p u, assume
applied state r, implying p r, r post(a) u, r pre(a).
w P a(v), (p | Vw ) 6 (pre(a) | Vw ) implies pre(a) | Vw Z , p r implies (p | Vw )
(r | Vw ), r pre(a) implies (r | Vw ) (pre(a) | Vw ), GetMacros(w, init, A, G) called
Solve(w, p | Vw , Z , , G) due Lemma B.2. induction, Solve(w, p | Vw , Z , , G)
returned macro hp | Vw , seq , (pre(a) | Vw ) shortest path
p | Vw r | Vw prefix seq . macro hp | Vw , seq , part set O.
iteration Solve(v, s, Z, O, G) (p, seq), Compose(v, p, pre(a), O, G) called
line 6 since pre(a) (v = p(v)) due fact first action changing
value v shortest path p u. resulting set contains sequence seq2
consists macros hp | Vw , seq , w P a(v) (p | Vw ) 6
(pre(a) | Vw ). line 8 state-sequence pair (q post(a), hseq, seq2, ai) added Q,
q = p post(seq2) pre(a) due Lemma A.1.
Since seq2 change value v shortest path p | Vw
r | Vw prefix seq w P a(v), q shortest path p r prefix
seq2. Consequently, exists shortest path hseq2, seq3i p r. path
change value v, else would applied state r shortest path p
u. Since applicable q due q pre(a), sequence hseq2, a, seq3i shortest
path p r post(a). follows q post(a) shortest path p
r post(a) prefix hseq2, ai. Thus r post(a) u implies q post(a) u.
future iteration Solve(v, s, Z, O, G), state-sequence pair removed Q
line 4 (q post(a), hseq, seq2, ai). Since q post(a) u, induction state-sequence
pairs, Solve(v, s, Z, O, G) returns macro hs, hseq, seq2, a, seq4i, ti u
shortest path q post(a) u prefix seq4. exists shortest
path hseq4, seq5i q post(a) u. Since r post(a) shortest path p
u assumption, q post(a) shortest path p r post(a) prefix
hseq2, ai, hseq2, a, seq4, seq5i shortest path p u. follows
shortest path p u prefix hseq2, a, seq4i.
proof follows since first iteration Solve(v, s, Z, O, G) (s, hi), implying Solve(v, s, Z, O, G) returns macro hs, hhi, seq2i, ti = hs, seq2, ti u
shortest path u prefix seq2. case
exception since sequence passes s. Note v
/ Vz , i.e., z
specify value v, set macros returned Solve(v, s, Z, O, G) contains
macros z reachable value v, including u(v) since u reachable s.
proceed prove Theorem 3.3. Recall set macros returned
MacroPlanner(P ) equal set macros returned Solve(v, init, {goal}, O, G),
504

fiThe Role Macros Planning

G = R transitive reduction causal graph v root variable R.
optimal plan solving P sequence actions Av = init state u
u goal, implying init u. apply Lemma B.3 prove contains
macro = hinit, seq, ti goal shortest path u
prefix seq. possible seq optimal plan.
exist plan solving P , show empty contradiction.
Assume not. Theorem 3.2 implies macro solution P ,
contradicts fact P unsolvable. Thus empty, consequence,
MacroPlanner(P ) returns fail.

Appendix C. Proof Theorems 3.5 3.6
section prove Theorems 3.5 3.6, establish two subclasses class
IR MacroPlanner generates solutions polynomial time.
prove Theorem 3.5, note variable v V action v
Vpost(a) , holds Vpre(a) = Vv . follows v Vz projected pre-condition
z Z. Let Zvd = {z Z : z(v) = d} set projected pre-conditions specify
value v. state domain transition graph specifies value
v match either pre-condition action Adv projected pre-condition
Zvd . Otherwise, corresponding node would added graph algorithm.
exception projected initial state init | Vv case init(v) = d.
Since pre-condition specified ancestor v, one state matches
pre-condition. number projected pre-conditions bounded number
actions descendants v, number nodes domain transition graph
P
O( dD(v) (|Adv | + |Zvd |)) = O(|A| + |Z|) = O(|A|). Lemma 3.4 follows
P
complexity algorithm O(|A| vV |A|3 ) = O(|V ||A|4 ), proving theorem.
Theorem 3.6 states action changing value v D(v)
pre-condition parents v. Since pre-condition undefined
ancestors, projected pre-condition v specified v alone. implies
successor states first second type always coincide, since matching projected
pre-condition depends value v.
prove induction domain transition graphs v contain one node
value D(v). |P a(v)| = 0, proof follows fact nodes
values D(v). |P a(v)| > 0, induction domain transition graphs parent
v P a(v) v one node per value D(v ). implies macro
post(m)(v ) = post-condition (else would least two
nodes v = ).
value D(v), algorithm generates successor state first type
s(v) = applying action Adv . action pre-condition
parent v P a(v). implies pre-condition always satisfied
state, since macro post(m)(v ) = pre(a)(v ) postcondition. Thus, successor state s(v) = same, matter action
use previous state come from.
Note = init(v), projected initial state init | Vv could different
successor state si discussed above. However, theorem states pre-condition
505

fiJonsson

actions post(a)(v) = init(v) P a(v) equals init | P a(v). Since domain
transition graphs parent v P a(v) contain one node init(v ), node
correspond projected initial state init | Vv . Thus, successor state
s(v) = init(v) equals projected initial state init | Vv , since pre(a) | P a(v) = init | P a(v)
action Adv , = init(v), post(m) = init | Vv macro satisfies
pre-condition a. proof theorem follows Lemma 3.4.

Appendix D. Proof Theorems 4.3 4.4
appendix prove Theorems 4.3 4.4, together state RelaxedPlanner returns well-defined optimal solution P RIR one exists.
first show Definition B.1 Lemma B.2 apply RelaxedPlanner. notion
reachability Definition B.1 refers set Av actions Vpost(a) Vv , thus
excluding actions change value successor v.
Lemma B.2 states (p, seq) state-sequence pair visited call
Solve(v, s, Z, O, G) w P a(v) parent v G, GetMacros(w, init, A, G) previously called Solve(w, p | Vw , Z , , G), Z values Z
w. show holds RelaxedMacros well, G transitive
reduction relaxed causal graph.
p equals projected initial state init | Vv , p | Vw = init | Vw added L line 2
RelaxedMacros. Otherwise, p successor state first type. words,
reached applying action Av Aw . Solve uses macros w satisfy
pre-condition respect Vw . Consequently, p | Vw equals post-condition
post(m) macro m, followed application a. precisely state
post(m) (post(a) | Vw ) added L line 15 modified RelaxedMacros.
Since subroutines Solve Compose before, Lemmas A.2
B.3 apply verbatim RelaxedPlanner. use reasoning
MacroPlanner prove two theorems.

Appendix E. Proofs Theorems 4.7, 4.8, 4.9
appendix prove Theorem 4.7, states sequence seq returned
ReversiblePlanner(P ) well-defined init satisfies (init post(seq)) goal.
also prove Theorem 4.8, states ReversiblePlanner complete planning problems AR unary actions. Finally, prove Theorem 4.9 regarding
complexity ReversiblePlanner. first prove lemmas similar Lemmas A.1
A.2.
Lemma E.1. state partial state x, let (seq, seq2) pair sequences returned ReversibleCompose(U, s, x, P, G). macros returned ReversibleSolve well-defined, seq well-defined post(seq) = x. Furthermore, seq2 well-defined x post(seq2) = | (Vx U ).
Proof. Let w1 , . . . , wk topological sort variables Vx x(wi ) 6= s(wi ).
first sequence returned ReversibleCompose(U, s, x, P, G) seq = hm1k , . . . , m11 i,
m1i = hs | Vwi , seq , (s | Vwi ) (wi = x(wi ))i macro changing value wi
506

fiThe Role Macros Planning

s(wi ) x(wi ). fact wi comes wj topological order implies wj
/ V wi .
Thus, changing values wk , . . . , wi+1 , pre-condition | Vwi macro m1i
still holds, sequence seq well-defined s. Furthermore, seq changes value
variable w Vx x(w), leaves values variables unchanged,
post(seq) = x.
Let u1 , . . . , ul topological sort variables W = {w1 , . . . , wk } U .
second sequence returned ReversibleCompose(U, s, x, P, G) seq2 = hm21 , . . . , m2l i,
m2i = h(s | Vui ) (ui = x(ui )), seq , | Vui macro resetting value ui
x(ui ) s(ui ). W contains ancestors ui , come ui topological order.
Hence values reset prior ui , pre-condition (s | Vui ) (ui = x(ui ))
macro m2i holds application m21 , . . . , m2i1 . Since seq2 resets value
variable w Vx U s(v), post(seq2) = | (Vx U ).
Note ancestor u ui part U , value would reset s(u ),
consequence, seq2 would well-defined. However, ReversibleSolve always
calls ReversibleCompose U = Vpost(a) x = pre(a) action a. Taken
together, u Vpost(a) ui Vpre(a) Vpost(a) imply causal graph contains
edge (ui , u ), true even consider relaxed causal graph. Thus u could
ancestor ui acyclic causal graph.
Lemma E.2. v V , state s, value D(v), macro returned
ReversibleSolve(v, s, d, A, G) well-defined.
Proof. induction state-sequence pairs (p, seq) visited call subroutine ReversibleSolve(v, s, d, A, G). particular, show seq well-defined
satisfies post(seq) = p. base case given first state-sequence pair
(s | W, hi), whose sequence clearly well-defined satisfies spost(hi) = s(s | W ).
Otherwise, assume statement holds (p, seq), let action
pre(a) (v = p(v)).
Let (seq2, seq3) result ReversibleCompose(Vpost(a) , p, pre(a), A, G).
(seq2, seq3) equal (fail,fail), Lemma E.1 states seq2 well-defined sp
p post(seq2) = p pre(a). implies hseq, seq2, ai well-defined
post(hseq, seq2, ai) = p pre(a) post(a). Lemma E.1 also states
seq3 well-defined p pre(a) post(seq3) = (s p) | (Vpre(a) Vpost(a) ).
Since seq3 changes values variables Vpre(a) Vpost(a) , still well-defined
state p pre(a) post(a) results applying a. Thus hseq, seq2, a, seq3i
well-defined post(hseq, seq2, a, seq3i) = p post(a). easy show
composition commutative, implying p post(a) = (p post(a)). Thus
statement holds new state-sequence pair (p post(a), hseq, seq2, a, seq3i) inserted
L line 12 ReversibleSolve.
proof follows induction variables v V . v ancestors
causal graph, call ReversibleCompose(Vpost(a) , p, pre(a), A, G) returns
pair empty sequences, since pre(a) (v = p(v)) set Vpre(a) Vpost(a)
empty. Thus macro returned ReversibleSolve well-defined. Otherwise,
hypothesis induction macros generated ReversibleSolve ancestors v
well-defined. Thus ReversibleCompose(Vpost(a) , p, pre(a), A, G) returns well507

fiJonsson

defined pair sequences due Lemma E.1 consequently, macros generated
ReversibleSolve v also well-defined.
proof Theorem 4.7 follows straightforward application Lemma E.1
call ReversibleCompose(, init, goal, A, G) made ReversiblePlanner, taking
advantage Lemma E.2 ensure macros returned ReversibleSolve
well-defined.
prove Theorem 4.8, first prove lemma regarding completeness ReversibleSolve unary actions.
Lemma E.3. v V , state s, value D(v), exists state
u u(v) = u, ReversibleSolve(v, s, d, A, G) returns valid macro
different fail.
Proof. double induction variables v V state-sequence pairs (p, seq)
visited call ReversibleSolve(v, s, p, A, G). particular, show
p u, ReversibleSolve(v, s, d, A, G) returns valid macro. base case given
p(v) = d, implying p = (s | W ) (v = d) since W = unary actions. case,
ReversibleSolve returns macro line 7. Else path u contain
actions changing values v. Let first action, implying
state pre(a).
induction, w Vpre(a) , ReversibleSolve(w, s, pre(a)(w), A, G) returns
valid macro. fact w reversible implies exists state u
u (w) = s(w) (w = pre(a)(w)) u . consequence, induction
ReversibleSolve(w, s(w = pre(a)(w)), s(w), A, G) also returns valid macro. Thus
pair sequences (seq2, seq3) returned ReversibleCompose(Vpost(a) , sp, pre(a), A, G)
well-defined different (fail,fail). Consequently, ReversibleSolve inserts
state-sequence pair (p post(a)(v), hseq, seq2, a, seq3i) list L line 12.
sequence hseq, seq2, a, seq3i generated ReversibleSolve results state
(v = post(a)(v)), may different state given path u
applying action a. However, since v reversible, state reachable
(v = post(a)(v)), implying (v = post(a)(v)) u. induction state-sequence
pairs, ReversibleSolve(v, s, d, A, G) returns corresponding macro.
proof Theorem 4.8 follows fact call subroutine
ReversibleCompose(, init, goal, A, G), value goal state satisfied
starting init, Lemma E.3 implies ReversibleSolve guaranteed return
macro satisfies goal state variable Vgoal . Thus, due Lemma E.1,
ReversibleCompose successfully return pair well-defined sequences (seq, seq2)
property init post(seq) = init goal, seq solution P . value
goal state unreachable init, ReversibleSolve return corresponding
macro, ReversibleCompose (and ReversiblePlanner) return fail.
prove Theorem 4.9, first prove lemma regarding values variables Vv W :
Lemma E.4. v V call ReversibleSolve(v, s, d, A, G), holds
| (Vv W ) = init | (Vv W ).
508

fiThe Role Macros Planning

Proof. lemma states variables Vv W always take initial values
call ReversibleSolve. First note ReversiblePlanner calls ReversibleCompose = init. turn, ReversibleCompose makes two calls ReversibleSolve
variable w Vx . calls, variable may different value
w. However, call ReversibleSolve w, holds w W
(else action changing value w, remove problem).
Thus lemma holds call ReversibleCompose ReversiblePlanner.
assume lemma holds call ReversibleSolve(v, s, d, A, G).
call ReversibleCompose, value p, p partial states
variables W . Note definition W , w W v, w W
ancestor u v w Vu . Thus value w different init(w)
call ReversibleCompose, w W subsequent call ReversibleSolve. Since
variables W take initial values assumption, take initial
values subsequent calls ReversibleSolve well.
Lemma E.4, number distinct choices calls ReversibleSolve
variable v O(Dk ), k = |W |. Since number distinct choices
O(D), total number calls ReversibleSolve v O(Dk+1 ). call
ReversibleSolve variable v V breadth-first search graph O(Dk )
nodes. action , one edge node, complexity
breadth-first search O(Dk (1 + |A |)) = O(Dk |A |), total complexity calls
ReversibleSolve v O(D2k+1 |A |). Since sets distinct, total complexity
P
P
calls ReversibleSolve O( vV D2k+1 |A |) = O(D2k+1 vV |A |) = O(D2k+1 |A|).
ReversibleCompose called Dk |A| times, edge one
graphs traversed ReversibleSolve. worst-case complexity ReversibleCompose linear number variables Vx , O(|V |). total complexity ReversibleCompose thus O(Dk |A||V |), total complexity ReversiblePlanner
O(D2k+1 |A| + Dk |A||V |) = O(Dk |A|(Dk+1 + |V |)).

References
Amir, E., & Engelhardt, B. (2003). Factored Planning. Proceedings 18th International Joint Conference Artificial Intelligence, pp. 929935.
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI
Planning Automatically Learned Macro-Operators. Journal Artificial Intelligence Research, 24, 581621.
Botea, A., Muller, M., & Schaeffer, J. (2007). Fast Planning Iterative Macros.
Proceedings 20th International Joint Conference Artificial Intelligence, pp.
18281833.
Brafman, R., & Domshlak, C. (2003). Structure Complexity Planning Unary
Operators. Journal Artificial Intelligence Research, 18, 315349.
Brafman, R., & Domshlak, C. (2006). Factored Planning: How, When, Not..
Proceedings 21st National Conference Artificial Intelligence.
509

fiJonsson

Bylander, T. (1994). computational complexity propositional STRIPS planning.
Artificial Intelligence, 69, 165204.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32(3), 333377.
Chen, H., & Gimenez, O. (2007). Act Local, Think Global: Width Notions Tractable
Planning. Proceedings 17th International Conference Automated Planning
Scheduling.
Chen, H., & Gimenez, O. (2008a). Causal Graphs Structurally Restricted Planning.
Proceedings 18th International Conference Automated Planning Scheduling.
Chen, H., & Gimenez, O. (2008b). On-the-fly macros. CoRR, abs/0810.1186.
Domshlak, C., & Dinitz, Y. (2001). Multi-Agent Off-line Coordination: Structure Complexity. Proceedings 6th European Conference Planning, pp. 277288.
Erol, K., Nau, D., & Subrahmanian, V. (1995). Complexity, decidability undecidability
results domain-independent planning. Artificial Intelligence, 76(1-2), 7588.
Fikes, R., & Nilsson, N. (1971). STRIPS: new approach application theorem
proving problem solving. Artificial Intelligence, 5 (2), 189208.
Gimenez, O., & Jonsson, A. (2008). Complexity Planning Problems Simple
Causal Graphs. Journal Artificial Intelligence Research, 31, 319351.
Gimenez, O., & Jonsson, A. (2009). Planning Chain Causal Graphs Variables
Domains Size 5 NP-Hard. Journal Artificial Intelligence Research, 34,
675706.
Haslum, P. (2008). New Approach Tractable Planning. Proceedings 18th
International Conference Automated Planning Scheduling.
Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence
Research, 26, 191246.
Jonsson, A. (2007). Role Macros Tractable Planning Causal Graphs.
Proceedings 20th International Joint Conference Artificial Intelligence, pp.
19361941.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions:
Algorithms complexity. Artificial Intelligence, 100(1-2), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractable
plan generation. Annals Mathematics Artificial Intelligence, 22(3-4), 281296.
Katz, M., & Domshlak, C. (2008a). New Islands Tractability Cost-Optimal Planning.
Journal Artificial Intelligence Research, 32, 203288.
Katz, M., & Domshlak, C. (2008b). Structural Patterns Heuristics via Fork Decompositions. Proceedings 18th International Conference Automated Planning
Scheduling, pp. 182189.
Kelareva, E., Buffet, O., Huang, J., & Thiebaux, S. (2007). Factored Planning Using Decomposition Trees. Proceedings 20th International Joint Conference Artificial
Intelligence, pp. 19421947.
510

fiThe Role Macros Planning

Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68(2), 243302.
Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33(1),
6588.
Minton, S. (1985). Selectively generalizing plans problem-solving. Proceedings
9th International Joint Conference Artificial Intelligence, pp. 596599.
Newton, M., Levine, J., Fox, M., & Long, D. (2007). Learning Macro-Actions Arbitrary Planners Domains. Proceedings 17th International Conference
Automated Planning Scheduling, pp. 256263.
Vidal, V. (2004). Lookahead Strategy Heuristic Search Planning. Proceedings
14th International Conference Automated Planning Scheduling, pp. 150159.
Williams, B., & Nayak, P. (1997). reactive planner model-based executive.
Proceedings 15th International Joint Conference Artificial Intelligence, pp.
11781185.

511

fiJournal Artificial Intelligence Research 36 (2009) 415-469

Submitted 03/09; published 12/09

Friends Foes? Planning Satisfiability
Abstract CNF Encodings
Carmel Domshlak

dcarmel@ie.technion.ac.il

Technion Israel Institute Technology,
Haifa, Israel

Jorg Hoffmann

joerg.hoffmann@inria.fr

INRIA,
Nancy, France

Ashish Sabharwal

sabhar@cs.cornell.edu

Cornell University,
Ithaca, NY, USA

Abstract
Planning satisfiability, implemented in, instance, SATPLAN tool,
highly competitive method finding parallel step-optimal plans. bottleneck
approach prove absence plans certain length. Specifically, optimal plan
n steps, typically costly prove plan length n1.
pursue idea leading proof within solution length preserving abstractions (overapproximations) original planning task. promising abstraction
may much smaller state space; related methods highly successful model
checking. particular, design novel abstraction technique based one can,
several widely used planning benchmarks, construct abstractions exponentially
smaller state spaces preserving length optimal plan.
Surprisingly, idea turns appear quite hopeless context planning
satisfiability. Evaluating idea empirically, run experiments almost benchmarks
international planning competitions IPC 2004, find even hand-made
abstractions tend improve performance SATPLAN. Exploring findings
theoretical point view, identify interesting phenomenon may cause
behavior. compare various planning-graph based CNF encodings original
planning task CNF encodings abstracted planning task. prove that,
many cases, shortest resolution refutation never shorter .
suggests fundamental weakness approach, motivates investigation
interplay declarative transition-systems, over-approximating abstractions,
SAT encodings.

1. Introduction
areas model checking AI planning well-known closely related
develop tools automatic behavior analysis large-scale, declaratively specified transition systems. particular, planning model checking safety
propertieschecking reachability non-temporal formulasproblems given description transition system, initial system state, target condition.
solution problem corresponds legal path transitions bringing system
initial state state satisfying target condition.
c
2009
AI Access Foundation. rights reserved.

fiDomshlak, Hoffmann, & Sabharwal

model checking problem, solution corresponds error path system,
is, unwanted system behavior. Proving absence error paths ultimate
goal system verification, thus traditional focus field exactly that.
Besides clever symbolic representations state space, key technique accomplish
ambitious task abstraction. System abstraction corresponds over-approximation
considered transition system, thus abstraction preserves transitions
original system. Hence, abstract transition system contain solution,
neither original system. key success model checking that, many cases,
one prove absence solutions rather coarse abstractions comparatively
small state space. Techniques kind explored depth long time.
Arguably wide-spread instance model checking predicate abstraction (Graf
& Sadi, 1997), system states form equivalence classes defined terms truth
values number expressions (the predicates), linear expressions integer
system variables. Predicates learned analyzing spurious error paths too-coarse
abstractions (Clarke, Grumberg, Jha, Lu, & Veith, 2003). Methods kind
extremely successful verification temporal safety properties (e.g., Ball, Majumdar,
Millstein, & Rajamani, 2001; Chaki, Clarke, Groce, Jha, & Veith, 2003; Henzinger, Jhala,
Majumdar, & McMillan, 2004).
contrast system verification, focus AI planning finding solutions
instances assumed solvable. particular, optimizing planning, task
find solution optimizes certain criterion (the focus analysis
here) sequential/parallel length solution path. Unlike general planning
solution good enough, main bottleneck length-optimizing planning always
prove absence solutions certain length. particular, optimal plan
n steps, hardest bit typically prove plan length n 1.
Note plan actually proved optimal, length-optimizing
planner avoid constructing proof, matter computational techniques
based on.
agenda research apply idea model checking lengthoptimizing planning. lead optimality proofnon-existence plan length n1
within abstraction. particular, focus interplay abstraction
proving optimality parallel step-optimal planning satisfiability. approach
originally proposed Kautz Selman (1992), later developed SATPLAN tool
(Kautz & Selman, 1999; Kautz, 2004; Kautz, Selman, & Hoffmann, 2006). SATPLAN
performs iteration satisfiability tests CNF formulas b encoding existence
parallel plan length b, b starts 0 increased incrementally.
n first satisfiable formula, n equals length optimal parallel plan,
hence SATPLAN parallel optimal, step-optimal, planner. class planners,
SATPLAN highly competitive. particular, SATPLAN 1st prizes optimal
planners International Planning Competition (IPC), namely IPC 2004 (Kautz,
2004; Hoffmann & Edelkamp, 2005) IPC 2006 (Kautz et al., 2006). One property
CNF encodings employed SATPLAN plays key role analysis later
based planning graph structure (Blum & Furst, 1995, 1997).
course, objective closely relates many approaches developed planning
computing lower bounds based over-approximations, (e.g., Haslum & Geffner, 2000;
416

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Edelkamp, 2001; Haslum, Botea, Helmert, Bonet, & Koenig, 2007; Helmert, Haslum, &
Hoffmann, 2007; Katz & Domshlak, 2008; Bonet & Geffner, 2008). key difference,
however, focus exact lower bounds, is, attempt actually prove optimality
within abstraction. want able prove optimality within abstraction,
abstraction must term solution length preserving: abstraction must
introduce solutions shorter optimal solution original problem.
lower bound step-optimal planning exact, support
SATPLANs iteration n 1, constitutes main bottleneck.
follows, briefly explain initial motivation behind work,
summarize empirical theoretical results.
1.1 Initial Motivation
would course interesting explore whether predicate abstraction applied
planning. Indeed, initial idea. However, discussions
idea lead nowhere,1 instead made different discovery. Planning state spaces
often dramatically reduced, without introducing shorter solutions, based
abstraction technique call variable domain abstraction. technique essentially
adapts work Hernadvolgyi Holte (1999) propositional STRIPS formalism.
abstract STRIPS task distinguishing certain values multiplevalued variables underlying STRIPS encoding. is, p q propositions
corresponding non-distinguished values, abstracted STRIPS task acts p q
same. Note generalizes abstraction used Edelkamp (2001) which,
multi-valued variable, either abstracts away completely abstract
all; see details Section 2.
first example noticed compression power variable domain abstraction classical Logistics domain. domain, packages must transported within
cities using trucks cities using airplanes. Actions load/unload packages,
move vehicles. Importantly, constraints (either of) vehicle capacities, fuel,
travel links. consequence, package p starts city destination
city B, cities C 6= A,B completely irrelevant p. is, one choose
arbitrary location x city C, replace facts form at(p,l), l
location outside B, at(p,x). Also, in(p,t), truck outside B,
replaced at(p,x). One completely abstract away positions packages
destination, minor optimizations possible. way
lose many distinctions different positions objectswithout introducing shorter
solution! optimal plan rely storing package p city ps origin
destination. state space reduction dramatic: abstracted state space contains
least (((C 2) S) 1)P states less, C, S, P respectively number
cities, city size (number locations city), number packages. Similar
1. still skeptical prospects. Software artifacts (rigid control structure, numeric expressions
essential flow control) rather different nature planning problems (loose control
structure, numeric expressions non-existent mostly used encode resource consumption). example, major advantage predicate abstraction capture loop invariantsa great feature,
seemingly rather irrelevant plan generation.

417

fiDomshlak, Hoffmann, & Sabharwal

abstractions made, similar state space reductions obtained, IPC
domains Zenotravel, Blocksworld, Depots, Satellite, Rovers (see Section 3).
1.2 Summary Empirical Results
first experiment, implemented Logistics-specific abstraction abstracting set planning tasks level description, modifying actions
initial state. planning tasks feature 2 airplanes, 2 locations city,
6 packages. number cities scales 1 14. account variance
hardness individual instances, took average values 5 random instances
problem size. increasing number cities introduces increasing amount
irrelevance, measure percentage RelFrac facts considered relevant (not
abstracted). Note additional cities irrelevant individual packagesthey cant removed completely task like standard irrelevance
detection mechanisms, e.g. RIFO (Nebel, Dimopoulos, & Koehler, 1997), would try do.
provided abstracted tasks three optimizing planners, namely Mips.BDD
(Edelkamp & Helmert, 1999), IPP (Hoffmann & Nebel, 2001), SATPLAN04,2 order examine abstraction affects different approaches optimizing planning.
Mips.BDD searches blindly exploiting sophisticated symbolic representation
state space. IPP equivalent parallel state-space heuristic search widely
used h2 heuristicthe parallel version h2 originally introduced Graphplan (Blum &
Furst, 1997; Haslum & Geffner, 2000). Thus, Mips.BDD, IPP, SATPLAN04 represent
orthogonal approaches optimizing planning.3 abstract task planner,
measured runtime, compared latter time taken planner
original task. Time-out set 1800 seconds, also used value
average computation time-out occurred. stopped evaluating planner 2
time-outs within 5 instances one size.
Figure 1(a), (b), (c) respectively show results Mips.BDD, IPP, SATPLAN04. Comparing performance original abstracted tasks, apparent
Figure 1 proving optimality within abstraction dramatically improved
performance Mips.BDD, significantly improved performance IPP. right
end scale (with 14 cities), Mips.BDD using abstraction find optimal sequential
plans almost fast SATPLAN04 find step-optimal plans. Given usually
much harder find optimal sequential plans optimal parallel plans, especially highly
parallel domains Logistics, performance improvement quite remarkable. (In
addition reduced state space size, Mips.BDD benefits small state encoding,
stops growing point maximal number locations relevant
package constant.)
findings Mips.BDD IPP line original intuition and,
SATPLAN04 well, expected see much improved runtime behavior within
abstraction. surprise, not. appears Figure 1(c), improvement
obtained SATPLAN04 proving optimality within abstraction hardly dis2. SATPLAN04 version SATPLAN competed 2004 International Planning Competition.
3. Importantly, Mips.BDD sequentially optimal SATPLAN04 IPP step-optimal. Hence
one compare performance planners directly, particular
purpose here. focus planners reacts abstraction.

418

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

10000

100

Mips.BDD-abstract
Mips.BDD-real
RelFrac

10000

1000

80

100

IPP-abstract
IPP-real
RelFrac

90

90

1000

80

70
100

70

60

100

60

50
10

50

40

10

40

30
1

30

20

1

20

10
0.1

10

0
1

2

3

4

5

6

7

8

9

10

11

12

13

0.1

14

0
1

2

3

4

5

6

(a)

7

8

9

10

11

12

13

14

(b)
10000

100

SATPLAN-abstract
SATPLAN-real
RelFrac

90

1000

80
70

100

60
50

10

40
30

1

20
10

0.1

0
1

2

3

4

5

6

7

8

9

10

11

12

13

14

(c)

Figure 1: Runtime performance (a) MIPS, (b) IPP, (c) SATPLAN04, (abstract) without (real) hand-made variable domain abstraction,
Logistics instances explicitly scaled increase amount irrelevance. Horizontal axis scales number cities, left vertical axis shows total runtime
seconds, right vertical axis shows percentage RelFrac relevant facts.

cernible. right end scale, abstraction yields humble speed-up factor 2.8.
particularly insignificant since speed-up obtained drastically small
RelFrac value 24%in IPC 2000 Logistics benchmarks, RelFrac 42% average.
latter corresponds 6 cities Figure 1, SATPLAN04 slight advantage
original tasks.
investigate broadly, conducted experiments almost STRIPS
domains used international planning competitions (IPC) IPC 2004. many
cases, tailored abstraction domain hand. results exhaustive
evaluation (discussed detail Section 3) significantly depart
Logistics-specific abstraction above. Mips.BDD almost consistently obtained
significant improvement. IPP improvements happened rarely, typically
substantial. (While IPP improved Figure 1, note that, IPC419

fiDomshlak, Hoffmann, & Sabharwal

average RelFrac 42%, improvement yet strong.) Finally, SATPLAN04,
hardly ever obtained improvement.
causes difference profiting abstraction three different
planning techniques? intuitive interpretation results informedness
abstraction must compete informedness search itself. words,
better planner exploiting structure particular example, difficult
abstraction exploit structure already exploited. intuition
good correspondence Logistics results Figure 1: optimizing exactly
measure, original examples, SATPLAN04 faster IPP, inverse
relation holds regarding planner profits abstraction. said,
intuitive interpretations results are, point, mere speculation. left open
future research determine accurately precisely causes difference. Herein,
concentrate planning satisfiability, identify fundamental weakness
approach respect profiting abstraction.
1.3 Summary Theoretical Results
Intrigued results SATPLAN04,4 wondered kind effect abstraction
actually CNF encoding planning task formulated Boolean satisfiability
problem instance. Recall abstractions over-approximations, is,
action sequence applicable original task applicable abstract task,
plan original task plan abstract task. So, intuitively, abstract task
generous original task. mind, consider CNF formula
n1 encoding existence plan one step shorter optimal plan, consider
formula, n1 , generated abstract task. need prove n1
unsatisfiable. (Note n1 is, fact, unsatisfiable solution length preserving
abstraction.) Intuitively, constrained formula is, easier lead
proof. n1 generous, hence less constrained, n1 . mean
actually harder refute n1 refute n1 ?
abstraction methods, fact trivial see answer question
yes. Say abstract n1 ignoring clauses. n1 sub-formula
n1 , immediately implying resolution refutation n1 also resolution
refutation n1 . particular, shortest possible refutation cannot shorter
n1 . similar situation sometimes occurs interplay abstractions CNF
encodings planning problems. instance, suppose abstract ignoring subset
goals. CNF encodings planning, particular planning-graph
based CNF encodings (Kautz & Selman, 1999) underlying SATPLAN, goal fact yields
one clause CNF. Hence, goal ignoring abstraction, n1 sub-formula
n1 , above.
complex example would correspond abstraction ignoring preconditions
delete effects. encodings used SATPLAN, one several clauses related
ignored precondition/delete effect disappear. However, CNF changes also ways
because, one precondition/delete effect less, actions facts become possible
later time steps. Intuitively, additional actions facts help proving
4. Translation: Deeply frustrated results SATPLAN04, . . .

420

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

unsatisfiability n1 . formal proof intuitive statement, however, less obvious
one goal ignoring abstraction above. Matters complicated,
much less intuitive, Edelkamps (2001) abstraction variable domain abstraction,
changes made abstract task also affect add effects actions. Recall
variable domain abstraction special interest likely satisfy
constraint solution length preservation.
investigate issues detail, one consider various possible combinations
CNF encodings abstraction methods. Many different encodings planning SAT
proposed. focus planning-graph based encodings used
SATPLAN system, appearances international planning competitions (Kautz & Selman, 1999; Long, Kautz, Selman, Bonet, Geffner, Koehler, Brenner,
Hoffmann, Rittinger, Anderson, Weld, Smith, & Fox, 2000; Kautz, 2004; Kautz et al., 2006).
Indeed, according Kautz Selman (Kautz & Selman, 1999; Long et al., 2000),
CNF encodingsin particular mutex relations computed Graphplanare vital
SATPLANs performance. recent results effective encodings may challenge
assessment (Rintanen, Heljanko, & Niemela, 2006; Chen, Huang, Xing, & Zhang, 2009;
Robinson, Gretton, Pham, & Sattar, 2008), even graphplan-based encodings
interest simply widely used almost decade. remains
course important question whether extent results carry alternative CNF encodings. discuss issue depth concluding paper
Section 5.
consider four different encodings, three used edition
IPC; fourth encoding considered sake completeness. encodings
differ two parameters: whether use action variables, action well fact
variables; whether include planning graph mutexes actions
direct interferences. (The latter motivated fact often enormous
amount action mutexes, seriously blowing size formula.)
abstractions side, focus abstraction methods formulated
manipulating planning tasks language level, i.e., modifying tasks actions and/or
initial/goal states. Many commonly used abstractions propositional STRIPS
formulated way. consider six abstractions, namely (1) removing goals, (2)
adding initial facts, (3) removing preconditions, (4) removing delete effects, (5) Edelkamps
(2001) abstraction (removing entire facts), (6) variable domain abstraction.
24 combinations CNF encoding abstraction method, prove
shortest possible resolution refutation exponentially longer n1 n1 .
20 combinations involving abstractions variable domain abstraction,
prove shortest possible resolution refutation cannot shorter n1
n1 . abstraction (1), trivial outlined above. abstractions (2)(4),
proof exploits fact abstractions lead larger planning graphs containing
actions facts. abstraction (5), reasoning work
facts disappear planning graph. However, one start removing fact
goal action preconditions; afterwards fact irrelevant one remove
also initial state action effects.
Matters complicated abstraction (6), is, variable domain abstraction.
encoding action variables full mutexes, show that, before,
421

fiDomshlak, Hoffmann, & Sabharwal

shortest possible resolution refutation cannot shorter n1 n1 .
encoding action variables direct action mutexes, show
possible improvement bounded effort takes recover indirect
action mutexes. two encodings action fact variables, remains
open question whether bounds exist.5
Importantly, proofs valid general resolution, also many
known restricted variants resolution, particular tree-like resolution refutations generated DPLL (Davis & Putnam, 1960; Davis, Logemann, & Loveland, 1962).
Naturally, proofs separate combinations, rather exploit
exhibit common features.
practical significance theoretical results is, extent, debatable
direct connection best-case resolution refutation size empirical
SAT solver performance. Even large refutation may easy find mostly
consists unit resolutions. Vice versa, small refutation exists,
mean SAT solver find it. notwithstanding, appears unlikely best-case
resolution refutation size practical SAT solver performance completely unrelated
(beyond obvious lower bound). One example indicates opposite planning
graph mutexes. Mutexes reduce best-case refutation size work
resolution even invoked.6 words, SAT solvers exploit mutexes
prune search trees effectively. aware explicit empirical proof
tends happen often, seems little doubt does. also
suggested explicitly Kautz Selman (Kautz & Selman, 1999; Long et al., 2000)
ways explaining improved performacce system run graphplan-based
encodings.
interesting situation arises (all) experiments. use variable domain abstraction encoding action variables direct action mutexes (as
employed SATPLANs IPC04 version). setting, resolution refutations get
shorter principle, although effort takes recover indirect action mutexes. Further, employ trivial post-abstraction simplification methods (such
removing action duplicates) which, show, also potential shorten resolution refutations. Still, reported above, discernible empirical improvement.
reason might SAT solver find shorter refutations,
shorter refutations actually appear significant scale. evidence
indicating latter: mutex recovery becomes necessary rather special situations,
abstraction turns indirect mutex direct one. typically concern small fraction indirect mutexes. addition, mutex recovery
simplifications, well-designed variable domain abstraction affected actions
typically irrelevant anyway. example, hand-made Logistics abstraction,
5. reason complications answering question requires determining, planning-graph
based encodings general, whether fact variables syntactic sugar may lead succinct
refutations. proof appears quite challenging; say Section 4.
6. General resolution recover mutexes effectively, c.f. related investigations Brafman (2001)
Rintanen (2008). seem likely case tree-like resolution,
best knowledge yet known.

422

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

effect potential improvements limited actions appearing redundant
plans. get back detail Section 4.
view, theoretical results would potential importance even
evidence empirical relevance, simply quite surprising. moment
thought, clear resolution refutation become easier ignoring goals.
However, variable domain abstraction domains Logistics deflates state spaces
immensely, point tiny fraction original size.
performing work, would never expected best-case refutation size remain
same.
paper organized follows. Section 2 discusses preliminaries, covering employed notions planning, planning graphs, propositional encodings, resolution, abstraction methods; particular, formally defines variable domain abstraction. Section 3
summarizes empirical results. Section 4 presents results regarding resolution refutations abstract CNF encodings. Related work discussed text appropriate. conclude Section 5. Appendix contains proofs, replaced
brief proof sketches main body text. Additional empirical data
found online appendix (see JAIR web page article).

2. Preliminaries
begin discussion various concepts needed rest paper: propositional
STRIPS planning, planning graphs, propositional CNF encodings planning problems,
resolution proofs unsatisfiability, abstraction methods used planning. general
rule notation, use variants of: P planning tasks; F, A, G sets facts,
actions, goals, respectively; abstractions; P G planning graphs;
propositional formulas encodings.
2.1 STRIPS Planning Graphs
Classical planning devoted goal reachability analysis state transition models deterministic actions complete information. model tuple = hS, s0 , SG , A,
finite set states, s0 initial state, SG set alternative
goal states, finite set actions, : transition function,
(s, a) specifying state obtained applying s. solution, plan,
state transition model sequence actions a1 , . . . , generate sequence
states s0 , . . . , sm that, 0 < m, (si , ai+1 ) = si+1 , sm SG .
AI planning targets large-scale state transition models huge numbers
states, models assumed described concise manner via intuitive
declarative language. use propositional fragment STRIPS language (Fikes
& Nilsson, 1971). brevity, refer fragment STRIPS herein. Informally,
planning task planning instance STRIPS consists set propositional facts,
hold initially must hold simultaneously end plan
execution. state system time defined set propositional facts
hold time. task specifies set actions, defined
set precondition facts, set facts added state, set facts
removed state, action taken. Formally, STRIPS planning task
423

fiDomshlak, Hoffmann, & Sabharwal

given quadruple P = (P, A, I, G) fact set P , initial state description P , goal
description G P , action set every action pre(a), add (a),
del (a), subset P . planning task defines state transition
model = hS, s0 , SG , A, state space = 2P , initial state s0 = I,
S, SG iff G s. S, A(s) = {a | pre(a) s}
actions applicable s, A(s), (s, a) = (s \ del (a)) add (a).
assume actions reasonable sense add (a) del (a) = .
satisfied known planning benchmarks; particular satisfied benchmarks
used experiments.7
Many planning algorithms, including SATPLAN, employ form approximate
reachability analysis. One primary tools purpose planning graph, first
introduced scope Graphplan planner (Blum & Furst, 1997). length bound
b, planning graph P G(P) associated P layered graph two kinds nodes:
fact nodes action nodes. layers alternate fact layers F (0), F (1), . . . , F (b),
action layers A(0), A(1), . . . , A(b 1), pair layers F (t), A(t) forming
time step t. first vertex layer F (0) contains initial state. A(t) F (t + 1)
0 < b action sets fact sets, respectively, available time step + 1.
precisely, A(t) includes actions pre(a) F (t) pair
facts p, p0 pre(a) mutex layer (c.f. below); further, A(t) contains standard noop
action every fact F (t).8 F (t + 1) contains union add effects
actions A(t). Obviously, A(t) A(t + 1) F (t) F (t + 1). goal facts G
label appropriate vertices F (b). P G(P) four kinds edges:
(1) Epre (t) F (t) A(t) connect actions A(t) preconditions F (t),
(2) Eadd (t) A(t) F (t + 1) connect actions A(t) add effects F (t + 1),
(3) Ea-mutex (t) A(t) A(t) capture pair-wise mutual exclusion relation actions A(t); (a(t), a0 (t)) Ea-mutex (t), actions a0 cannot applied
simultaneously time t,
(4) Ef -mutex (t) F (t) F (t) capture pair-wise mutual exclusion relation facts
F (t); (f (t), f 0 (t)) Ef -mutex (t), facts f f 0 cannot hold together time
t.
Note P G(P) explicit edges deletion effects actions;
effects captured mutual exclusion relation (e.g., p add (a1 ) del (a2 ),
(a1 , a2 ) Ea-mutex times). mutex edges Emutex = Ea-mutex Ef -mutex
computed iterative calculation interfering action fact pairs (Blum & Furst,
1997). Namely, two actions (directly) interfere effects one contradict effects
other, one deletes precondition other. Two actions competing
7. IPC-2002 domain Rovers, operators add delete artificial fact order prevent
parallel application. implement restriction via duplicating respective operators
sequentializing original duplicate via two artificial facts. Similar fixes implemented
couple domains well.
8. fact p P , associated noop(p) delete effects, {p} preconditions
add effects. dummy actions simply propagate facts one fact layer next.

424

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

needs mutex preconditions. Combining two scenarios together, say
two actions mutex either (directly) interfere competing needs.
similar spirit, two facts mutex non-mutex pair actions (in graph
layer directly below) together achieving facts. variant interest
planning graph which, iterative computation, Ea-mutex reduced contain
directly interfering actions. call reduced planning graph, denote
P Gred (P). motivation considering that, often, reduced planning
graph results much smaller SAT encodings; get back below.
2.2 Propositional Encodings
consider three CNF encodings used (one version of) SATPLAN, well
fourth encoding fits naturally picture. encodings takes input
planning task P length bound b, creates formula standard Conjunctive
Normal Form (CNF). CNF formula solved off-the-shelf SAT solver.
process constitutes basic step SAT-based approach planning implemented
SATPLAN (Kautz & Selman, 1992, 1996, 1999), one starts b = 0 iteratively
increments b CNF becomes satisfiable first time. plan corresponding
satisfying assignment plan minimal b, hence optimal
sense.9
CNF formula logically conjunction (and) clauses, clause disjunction (or) literals, literal propositional (Boolean) variable negation.
CNF formulas often written set clauses, clause written set literals, underlying logical conjunction disjunction, respectively, implicit.
propositional encodings bounded-length planning tasks specified terms various
kinds clauses generated encoding method.
Encoding (A) constructed P G(P) uses propositional action variables
{a(t) | 0 < b, A(t)}. goal fact g goal clause form
{a1 (b 1), . . . , al (b 1)}, a1 , . . . al actions A(b 1) add g.
Similarly, every a(t) > 0 every p pre(a) precondition clause
{a(t), a1 (t 1), . . . , al (t 1)}, a1 , . . . al actions A(t 1) add p.
Finally, mutex clause {a(t), a0 (t)} every (a, a0 ) Ea-mutex (t).
(Note dependence initial state taken account already
terms actions contained sets A(t), need stated
explicitly CNF.)
Encoding (B) similar (A) except uses variables (and appropriate clauses)
also facts. specifically, addition action variables, fact
variables {f (t) | 0 b, f F (t)}. goal fact g, goal clause simply
unit clause asserting g(b). > 0 fact f (t), effect clause
form {f (t), a1 (t 1), . . . , al (t 1)}, a1 , . . . al actions A(t 1)
add f . every a(t) every p pre(a) precondition clause,
9. versions SATPLAN use naive incremental update b, shown
clever strategies, exploiting typical distribution runtime different values b (Rintanen,
2004; Streeter & Smith, 2007).

425

fiDomshlak, Hoffmann, & Sabharwal

takes form {a(t), p(t)}. action mutex clauses {a(t), a0 (t)} every
(a, a0 ) Ea-mutex (t), fact mutex clauses {f (t), f 0 (t)} every
(f, f 0 ) Ef -mutex (t). Finally, fact f F (0), initial state clause
{f (0)} (these strictly necessary implemented SATPLAN
include here).
Encoding (C) like (A) except based reduced planning graph P Gred (P),
mutex clauses present action pairs whose preconditions effects
interfere directly.
Encoding (D) like (B) except that, (C), based P Gred (P), mutex
clauses action pairs whose preconditions effects interfere directly. Note,
however, fact mutexes full planning graph, P G(P).
encodings reasonable ways turning planning graph CNF formula. encodings essentially underly competition implementations SATPLAN.
detail below. First, note different encodings different benefits
drawbacks. First, observe encodings characterized two decisions: (1)
include action mutexes Graphplan, direct interferences? (2)
include action variables, action fact variables? Regarding (1),
empirical observation mutexes help one major observations
design SATPLAN (then called Blackbox) (Kautz & Selman, 1999; Long et al., 2000),
particular comparison earlier encoding methods (Kautz, McAllester, & Selman, 1996).
hand, since mutexes talk pairs facts actions, encodings may
become quite largethere one clause every pair mutex actions mutex facts.
particularly critical actions, many planning benchmarks
thousands (compared hundred facts). Indeed, turns action mutexes often consume critically large amounts memory. uncommon CNF
formulas millions clauses, action mutexes (Kautz & Selman, 1999;
Kautz, 2004; Kautz et al., 2006). motivates encodings (C) (D). question
(2), make much difference, empirically, planning benchmarks.
consider distinction used versions SATPLAN.
Let us say words clarify exactly encodings (A)(D) relate SATPLAN
literature implementations. Due long history SATPLAN, well
imprecisions literature, little complicated. foremost reference
actual program code underlying SATPLAN04 SATPLAN06, i.e., recent
versions used 2004 2006 competitions. encoding methods versions
implemented one authors paper. four different encoding
methods: action-based, graphplan-based, skinny action-based, skinny graphplan-based.
action-based encoding exactly (A), graphplan-based encoding exactly (B),
skinny graphplan-based encoding exactly (D).10 skinny action-based encoding
like (C) except that, save runtime, planning graph implementation
propagate mutexes (after all, direct interferences present final encoding),
effectively computing relaxed planning graph (Hoffmann & Nebel, 2001). use normal
10. 2004 version, skinny graphplan-based encoding feature fact mutexes.
consequence encoding used 2004 competition.

426

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

planning graph (C) sake readabilitythe greater similarity
encodings significantly simplifies write-up, theoretical results hold stated
also relaxed planning graphs.
SATPLAN encodings develop historically, reflected literature, encodings used competitions? answer questions
extent necessary explaining encodings (A)(D). original paper SATPLAN
(Kautz & Selman, 1992) introduced rather different encodings. Graphplan-based encodings
direct action mutexes introduced next, observed yield performance
comparable Graphplan (Kautz & Selman, 1996). Subsequently, observed
modern SAT solvers profit full (fact action) mutex relations actually
beat planners, several domains (Kautz & Selman, 1999).11 Consequently,
graphlan-based encoding, i.e., encoding (B), used 1998 2000 competitions (Long et al., 2000). Prior 2004 competition, encoding methods
re-implemented, yielding four methods explained above. IPC04 booklet paper
SATPLAN04 (Kautz, 2004) describes four encodings.12 version run competition skinny action-based encoding (for theoretical results) equivalent
encoding (C). running planner IPC 2006, turned full
fact mutexes helped domains, encoding (D) used instead (Kautz et al.,
2006). consider encoding (A) sake completeness.
2.3 Resolution Refutations
theoretical results respect resolution proof system (Robinson, 1965),
forms basis complete SAT solvers around today (cf. Beame, Kautz,
& Sabharwal, 2004). sound complete proof system, studied extensively theoretical practical reasons. works CNF formulas one
simple rule inference: given clauses {A, x} {B, x}, one derive clause {A, B}
resolving upon variable x. B shorthands arbitrary lists literals.
Note choice clauses resolve arbitrary, long share variable,
opposite signs. resolution derivation clause C formula consists series
applications resolution rule starting clauses one eventually
derives C; C (unsatisfiable) empty clause, {}, called resolution proof
(of unsatisfiability) refutation . size number applications
resolution rule . unsatisfiable, RC() denotes resolution complexity ,
i.e., size smallest resolution proof unsatisfiability . interested
whether applying abstraction planning task convert encoding one
smaller resolution complexity.
commonly studied sub-class (still sound complete) resolution derivations
tree-like resolution derivations, derived clause used
whole derivation; underlying graph structure proof tree.
11. Kautz Selman (1999) cite graphplan-based encodings earlier work (which used action
mutexes). However, Blackbox program code includes functions generate full mutexes, Kautz
Selman explicitly emphasize importance mutexes.
12. paper abstract little imprecise description: initial state, goal, fact
mutex clauses mentioned; skinny action-based encoding stated identical
encoding (C), i.e., based full planning graph rather relaxed planning graph.

427

fiDomshlak, Hoffmann, & Sabharwal

interesting sub-classes resolution include regular resolution, provably exponentially powerful tree-like resolution variable resolved upon
path root leaf underlying proof graph, ordered
resolution, addition variables respect fixed ordering root-to-leaf path.
Tree-like resolution captures proofs unsatisfiability generated SAT solvers
based DPLL procedure (Davis & Putnam, 1960; Davis et al., 1962) dont employ so-called clause learning techniques; latter kind SAT solvers provably
exponentially powerful even regular resolution although still within realm
general resolution (Beame et al., 2004).
note arguments presented paper general (unrestricted) resolution. However, since aour constructions affect rely structural
properties resolution refutations consideration, results hold stated (except slight weakening case Lemma 4.14) known variants resolution
setting variables True False replacing one variable another preserves
proof structure. variants include tree-like (DPLL), regular, ordered resolution.
state standard property resolution proofs use arguments,
pointing certain modifications (such variable restrictions shortening
clauses) given formula cause proofs become longer general resolution
natural sub-classes, including mentioned above. Let x variable
True, False, another (possibly negated) variable . variable restriction
x transformation replaces x throughout , simplifies
resulting formula removing clauses containing True variable negation,
removing False duplicate literals clauses. words, variable restriction
involves fixing value variable identifying another literal, simplifying
formula. sequence variable restrictions , | denote
outcome applying .
following proposition combines two basic facts together form useful
us: (1) variable restrictions cannot increase resolution complexity formula,
(2) lengthening clauses and/or removing clauses cannot decrease resolution complexity
formula.
Proposition 2.1. Let CNF formulas. exists sequence variable
restrictions every clause | contains sub-clause clause ,
RC() RC( ).
explanation proposition use order. notation
used chosen match way eventually utilize proposition
proofs; below. conditions proposition imply one may obtain
applying restriction , possibly throwing away literals
clauses, possibly adding new clauses. Intuitively, three modifications
reduce number solutions cannot make harder prove
formula unsatisfiable. property resolution refutations propositional formulas
previously used (at least indirectly) various contexts. completeness, include
proof Appendix A, based folklore ideas proof complexity literature.
alternative proof, somewhat different notation, may also found appendix
recent article Hoffmann, Gomes, Selman (2007).
428

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

way use proposition following: CNF encoding
abstracted planning task, encoding original planning task,
carefully chosen restriction bring focus variables already
appearing . Proposition 2.1 imply original encoding harder
refute, using resolution natural sub-classes, abstracted encoding.
2.4 Abstraction Planning
Abstraction methods various kinds put use planning, often quite successfully. One line work uses abstraction methods problem decomposition (cf. Sacerdoti,
1973; Knoblock, 1990; Koehler & Hoffmann, 2000). best knowledge,
approachexamining abstract state space order prove absence solutions
pursued before. line work relevant work domainindependent heuristic functions (cf. McDermott, 1999; Bonet & Geffner, 2001; Hoffmann &
Nebel, 2001; Edelkamp, 2001; Haslum et al., 2007; Helmert et al., 2007; Katz & Domshlak,
2008). There, abstraction means over-approximation state space, work;
differs abstractions used. course, kinds over-approximations
useful either purpose differ lot. use abstraction
paper, one define over-approximations preserve, large extent, real
structure problem. particular, ideal goal find abstractions preserve length optimal solutionsomething one definitely wouldnt expect
abstraction underlying heuristic function, since solved every search state.
briefly review over-approximation methods used
planning far; formally introduce novel one, variable domain abstraction.
use Logistics domain illustrative example.13
One wide-spread over-approximation method planning 2-subset relaxation
underlying computation made planning graph (Blum & Furst, 1997), generalized m-subset relaxation Haslum Geffner (2000). nutshell, one assumes
achieving set facts hard achieving hardest m-subset. known
that, domains, including Logistics, 2-subset solution length (corresponding
length planning graph constructed first fact layer containing mutexes
goal facts) typically strictly lower length optimal plan.
> 2, hand, computing m-subset solution length typically costly,
still, = o(|P |) one typically guarantee solution length preservation (Helmert
& Mattmuller, 2008).
second wide-spread over-approximation method ignoring delete lists (McDermott,
1999; Bonet & Geffner, 2001). approximation, one simply removes (some of)
negative effects actions. negative effects removed, problem
becomes solvable time linear instance size. latter basis heuristic
functions used many modern planners (cf. Bonet & Geffner, 2001; Hoffmann & Nebel,
2001; Gerevini, Saetti, & Serina, 2003). Ignoring deletes likely introduce shorter
solutions. example, Towers Hanoi problem, leads plans length n instead
2n 1 (Hoffmann, 2005). Logistics, one ignores deletes moving actions
13. stated, open topic explore model checking abstractions, particular predicate abstraction
(Graf & Sadi, 1997; Clarke et al., 2003), instead planning abstractions.

429

fiDomshlak, Hoffmann, & Sabharwal

plans may get shorter vehicles never move back abstraction.
Interestingly, ignoring deletes load/unload decrease plan length, since
actions never undone optimal plan. use observation
experiments.
third abstraction introduced Edelkamp (2001) pattern database
heuristic. approximation, one completely removes facts problem
description, notably facts corresponding values multi-valued variables.
enough facts removed, task becomes sufficiently simple. Obviously, approximation hardly solution length preserving. Logistics, remove, example,
fact at(package1,airport2) then, particular, package1 loaded onto airplane
airport2 without actually precondition removed. optimal
plan make package pop anywhere.
fourth abstraction, finally, involves removing preconditions (Sacerdoti, 1973)
and/or goal facts. Edelkamps abstraction, cannot expected solution
length preserving interesting cases.
calls new abstraction method, designed following Hernadvolgyi
Holte (1999). Considering STRIPS-like state transition systems multiple-valued
(instead Boolean) variables, propose reduce variable domains distinguishing certain values. example, content cell 8-puzzle
{blank , 1, 2, 3, 4, 6, 7, 8} may replaced {blank , 1, 2, 3} {3, . . . , 8}
mapped onto 3. observation that, many planning benchmarks, done
without introducing shorter plans. example, Logistics unnecessary distinguish
positions packages irrelevant cities. Therefore, replace domain at(p),
{A1 , A2 , B1 , B2 , C1 , C2 , . . . }, B initial goal cities p Ai , Bi , . . .
locations cities A, B, . . ., abstract domain {A1 , A2 , B1 , B2 , C1 }. STRIPS,
amounts replacing set irrelevant facts at(p, l) single fact at(p, C1 ).
formalize idea.
Let persistently mutex denote standard notion two facts mutex
fixpoint layer planning graph: typically, different values multiple-valued variable.
Definition 2.2. Let P = (P, A, I, G) STRIPS planning task, p, p0 P pair
persistently mutex facts that, A, ({p, p0 } del (a)) pre(a).
((P ), {(a)|a A}, (I), (G)) called variable domain abstraction P,
defined follows:
1. fact set F , (F ) = F p0 6 F ; otherwise, (F ) = (F \ {p0 }) {p}.
2. action = (pre, add , del ), (a) = ((pre), (add ), (del )) p 6 (add )(del );
otherwise, (a) = ((pre), (add ), (del ) \ {p}).
words, Definition 2.2 simply says replace p0 p. p appears
add list delete list action, remove delete list.14
situation arise, instance, action moves package one irrelevant position
14. reader may wonder p remains add list, although prerequisite p (pre). reason
distinguish abstractions simplifications: change planning task;
abstractions, simplifications, way may alter tasks semantics. However,
simplifications may well affect resolution complexity. get back later paper.

430

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

another irrelevant position. operation, p equivalent originally
p p0 : p True abstracted action sequence p p0 True
corresponding real action sequence. particular, Proposition 2.3 states variable
domain abstraction over-approximation usual sense.
Proposition 2.3. Let P = (P, A, I, G) STRIPS planning task, let (P) =
((P ), {(a) | A}, (I), (G)) variable domain abstraction P. Then, whenever
ha1 , . . . , plan P, h(a1 ), . . . , (an )i plan (P).
Proof. Let state models induced P (P). First, let us show
that, state M, action M, applicable s, (i)
(a) applicable (s), (ii) p ((s), (a)) {p, p0 } (s, a) 6= .
Note that, since abstraction effect facts {p, p0 }, (ii) implies
((s), (a)) = ((s, a)). Thus, together, (i) (ii) imply (iii) homomorphic
. Finally, straightforward Definition 2.2 (iv) initial state
(I), goal states exactly {(s) | SG }. Together, (iii) (iv)
imply claim Proposition 2.3.
Let = (pre, add , del ). applicability (a) (s) straightforward; p0 6 pre,
(pre) = pre (s)(pre) = spre, otherwise (pre) = (pre\{p0 }){p}
(s) = (s \ {p0 }) {p}. cases, pre implies (pre) (s). Consider
sub-claim (ii) case-by-case basis.
{p0 , p} add = , {p0 , p} del = p 6 add ((a)) p 6 del ((a)), thus
p ((s), (a)) iff p (s) iff {p, p0 } 6= iff (see assumption del case)
{p, p0 } (s, a) 6= .
{p0 , p} add 6= , {p0 , p} del = p add ((a)) p 6 del ((a)), thus
p ((s), (a)). hand, {p, p0 } (s, a) 6= also trivially holds here.
{p0 , p} add = , {p0 , p} del 6= p 6 add ((a)) p del ((a)), thus
p 6 ((s), (a)). hand, since p, p0 persistently mutex facts P,
{p0 , p} del = {p0 , p} pre, also {p, p0 } (s, a) = .
{p0 , p} add 6= , {p0 , p} del 6= p add ((a)) p 6 del ((a)), thus
p ((s), (a)). hand, add del = {p0 , p} add 6=
immediately {p, p0 } (s, a) 6= .
completes proof (ii).
Arbitrarily coarse variable domain abstractions may generated iterating application Definition 2.2. Note variable domain abstraction refinement Edelkamps
(2001) abstractioninstead acting irrelevant positions could totally ignored,
distinguish whether package currently position. makes
difference preserving optimal solution length not.15
hinted above, variable domain abstraction may able apply
simplifications. simplification, terminology, similar abstraction
15. topic future work explore whether refined abstraction lead better pattern database
heuristics STRIPS problems.

431

fiDomshlak, Hoffmann, & Sabharwal

manipulates planning task language level. However, abstractions may alter
tasks semantics, simplifications not; i.e., simplifications introduce new
transitions goal states. Concretely, consider two simplifications. planning task
P = (P, A, I, G) duplicate actions exist a, a0 pre(a) = pre(a0 ),
add (a) = add (a0 ), del (a) = del (a0 ).16 simplified planning task like P except
a0 removed. planning task P = (P, A, I, G) irrelevant add effects
exists pre(a) add (a) 6= . simplified planning task like P except
remove pre(a) add (a).
Obviously, duplicate actions irrelevant add effects may arise outcome variable domain abstraction. example latter action moving package
irrelevant location irrelevant truck. example former two actions loading
package onto airplane distinct irrelevant locations.17 implementation,
simple post-abstraction processing perform simplifications.
shall see Section 4.3, simplifications lead decreased resolution complexity, thereby offsetting result abstractions such, many cases, cannot.
may seem little artificial distinguish abstractions simplifications way, seeing
many abstractions bound enable us simplify. However, note distinction
serves identify borderline can, cannot, reduce resolution
complexity. Anyhow, shall see next section, abstraction tend help
empirically performance SATPLAN even post-abstraction simplifications.

3. Empirical Results
performed broad empirical evaluation effect abstractions efficiency
optimizing planning algorithms. mostly focus variable domain abstraction,
Definition 2.2, since clearly promising obtaining solution length preserving
abstractions.
Section 3.1 explains specific variable domain abstractions employ experiments. Section 3.2 explains experimental setting chose present huge
data set results. Section 3.3 describes experiments variable domain abstraction IPC benchmarks, Section 3.4 discusses results domain-specific
abstractions hand-made instances certain benchmark domains amount
irrelevance controlled. Section 3.5 briefly summarizes findings abstraction
methods variable domain abstraction.
3.1 Variable Domain Abstractions
designed three different methods automatically generate variable domain abstractions. methods listed based increasingly conservative approximations
16. Note define actions triples pre, add , del , components; hence
two actions identical pre, add , del may contained set A. reflects practical planner
implementations, actions names and/or unique IDs, checks duplicate actions
usually performed.
17. similar fashion, duplicate actions may arise outcome Edelkamps (2001) pattern database
abstraction.

432

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

relevance. common relevance approximations (cf. Nebel et al., 1997), algorithmic basis is, cases, simplified backchaining goals.
1Support starts first layer planning graph contains goal facts (possibly
mutexes them). goal fact layer, selects one achiever
preceding action layer marks preconditions action new sub-goals;
process iterated created sub-goals.
AllSupports proceeds similarly 1Support except selects achievers
(sub-)goal.
AllSupportsNonMutex proceeds similarly AllSupports except starts backchaining first plan graph layer contains goals without mutexes.
three methods, selected set relevant facts R P taken set
goals sub-goals created backchaining. set facts turned
variable domain abstraction follows. First, compute partition problems
fact set P subsets P1 , . . . , Pk pairwise persistently mutex facts. take
subsets correspond underlying multiple-valued variables (e.g., position
package). perform abstraction within Pi values relevant,
i.e., Pi \ R 6= . Within subset Pi , arbitrarily choose one irrelevant fact p, i.e.,
p Pi \ R. replace irrelevant facts, i.e., q Pi \ R q 6= p,
p.
example, Logistics, 1Support abstracts away in(p, v) facts package p except vehicles v selected supportin particular, single
airplane. contrast, AllSupports mark in(p, v) relevant airplanes v unless
special case applies (e.g., p must transported within origin city only). Finally,
AllSupportsNonMutex even conservative covers special cases
AllSupports abstracts in(p, v) fact away. Note identifying positions inside
airplanes positions outside airplanes may well affect length optimal plan.
addition domain-independent, automatic variable domain abstractions, six
IPC domains designed domain-specific solution length preserving variable domain
abstractions hand. Logistics, domain-specific abstraction explained
introduction. Zenotravel, use similar abstraction exploiting irrelevant object positions. Blocksworld, on(A, B) considered irrelevant B neither initial
goal position A, B initially clear.18 Depots, combination Logistics
Blocksworld, abstraction combination two individual abstractions.
Satellite, abstraction performs simple analysis goal relevance detect directions
irrelevant satellite turn to. direction relevant satellites initial direction, goal direction, potential goal camera calibration target.
Similarly, Rovers, waypoint (location) considered relevant rover
either initial position, relevant needed rock sample/soil sample/image,
necessarily lies path rover must traverse reach relevant location.
18. last conditions necessary avoid possibility clearing block C moving
away C although actually placed third block.

433

fiDomshlak, Hoffmann, & Sabharwal

3.2 Experiment Setup Presentation
presented data generated set work stations running Linux, Pentium 4 processor running 3 GHz 1 GB RAM. used time cutoff 30 minutes.
experimented plan-length optimizing planners SATPLAN04, IPP (Koehler,
Nebel, Hoffmann, & Dimopoulos, 1997), Mips-BDD (Edelkamp & Helmert, 1999).19
choice SATPLAN04 rather SATPLAN06 arbitrary, except that, using
naive encoding (C), resolution best case SATPLAN04 improved variable domain abstractionmaking bad empirical results even significant.
Note also that, although SATPLAN06 could considered recent, contains
developments beyond SATPLAN04, switching back older version
encoding method.
test examples, took, exceptions listed below, STRIPS domains used
international planning competitions including IPC-2004. Precisely, use
(IPC-2004) Airport, Dining Philosophers, Optical Telegraph, Pipesworld NoTankage,
Pipes- world Tankage, PSR.
(IPC-2002) Depots, Driverlog, Freecell, Rovers, Satellite, Zenotravel.
(IPC-2000) Blocksworld Logistics. (Miconic-STRIPS simple version
Logistics, Freecell part IPC-2002 set.)
(IPC-1998) Grid, Mprime, Mystery. (Movie trivial, Gripper variable domain
abstraction cannot preserve solution length, Logistics part IPC-2000 set.)
measurements aimed highlighting potential abstraction principle
speeding computation information task. Concretely, given
planning task , create abstract version , run planner X it.
three possible outcomes:
(1) X finds plan , abstract plan, happens real plan (that is, plan
). record time taken find plan, along time taken X
find plan given original task .
(2) X finds plan real plan. Since planners optimize plan
length, information still gain length optimal abstract plan,
lower bound length real plan. record time taken compute
bound (for example, SATPLAN04, time taken last unsatisfiable
iteration), along time taken X compute lower bound given
original task .
(3) X runs time memory. case, one could record time taken
last lower bound proved successfully. sake readability, omit
consider cases (1) (2) above.
19. SATPLAN04 IPP optimize step-length plan, Mips-BDD optimizes sequential plan
length. However, again, performance planners stand comparative evaluation
here, refer three simply plan-length optimizing planners.

434

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Note that, spirit optimistic usefulness abstraction,
include time taken create abstract task . Note also actually obtain
several results pair X, namely one result every particular variable domain
abstraction. sake readability, include distinctions results
(the distinctions mostly inconclusive uninteresting anyway), instead present
results following best abstraction perspective. skip abstract tasks
either solved, abstract since facts considered relevant.
abstract task remains, skip instance. Otherwise, select abstract task
providing best information instance: best case abstract plan
real, else select highest lower bound.20 several abstractions providing
best information, choose one lowest runtime.
3.3 IPC Benchmarks
Due sheer size experiments3 planners multiplied 17 domainsdiscussing
entire result set neither feasible would useful. online Appendix (see JAIR
web page article) contains detailed data three optimal planners. Herein,
provide summary analysis showing main points, particular focus SATPLAN04.
Detailed data SATPLAN04 4 17 IPC domains given Table 1:
Depots Satellite selected table 2 17
domains abstraction brings somewhat significant advantage.
Logistics selected illustrative example.
PSR selected due interesting caseunusually, current optimal planners well (or badly) PSR current sub-optimal (satisficing) planners.
domain, selected 13 challenging instances, challenging
measured runtime taken original task. Note problem-instance selection
criterion presentation also optimistic point view abstraction.
instance, Table 1 first specifies whether found abstract plan real plan not.
characterizes problem instance terms cases (1) (2) explained above,
corresponding runtimes SATPLAN04 abstract real tasks given
rows ta tr . table specifies lower bound lg proved real task
planning graph (that is, F (t) first fact layer contain goal facts
mutexes them), lower bound la proved SATPLAN04 abstract task,
and, finally, actual length lr optimal plans real task. last row RelFrac
table specifies percentage facts considered relevant.
Depots, best-case data shown Table 1 scattered across four kinds
variable domain abstractions, automatic abstractions sometimes
sometimes less aggressive handmade abstraction. example, instances numbers
11 15 best case aggressive 1Support strategy. time
20. Note quality information essential. abstraction tells us plan
must least n 1 steps, real plan length n, must still prove bound n,
typically takes time bounds together.

435

fiDomshlak, Hoffmann, & Sabharwal

Index
IsReal?
ta
tr
lg , l , L
RelFrac
Abs

Index
IsReal?
ta
tr
lg , l , L
RelFrac
Abs

Index
IsReal?
ta
tr
lg , l , L
RelFrac
Abs

Index
IsReal?
ta
tr
lg , l , L
RelFrac
Abs

2

64.43
74.26
6,12,12
37%
1S

16
N
0.63
0.8
5,15,15
47%
ASnm

10

0.99
2.14
10,15,15
33%
1S

3

73.29
45.77
11,12,12
88%


4

112.30
104.05
6,10,10
91%
HM

22

64.29
71.86
7,25,25
75%
ASnm

13

91.43
75.28
9,13,13
48%
ASnm

4

429.74
472.01
12,14,14
88%


5

53.06
51.53
4,7,7
95%
HM

29

11.16
12.05
7,18,18
79%
ASnm

14

25.41
32.87
9,12,12
48%
ASnm

7

10.44
12.25
7,10,10
77%
ASnm

8

310.76
250.08
4,8,8
90%
HM

31
N
1.04
8.38
5,16,16
49%
ASnm

15
N
70.57
68.12
9,13,13
55%
ASnm

8
N
228.30
22.52
9,13,14
76%


9

34.35
40.65
4,6,6
84%
HM

33
N
0.93
1.33
5,16,16
48%
ASnm

16
N
111.66
75.79
9,13,13
47%
ASnm

10

47.77
33.11
8,10,10
87%
ASnm

Depots
11
13
N

49.73
9.07
2.13
12.42
13,10,?
9,9,9
27%
85%
1S
ASnm
Logistics
17
18
N
N
150.37
770.44
106.61
642.97
9,13,14 9,15,15
44%
42%
ASnm
HM
PSR
36
37


2.44
2.4
4.96
2.26
8,16,16 7,19,19
90%
60%
ASnm
ASnm
Satellite
10
11


168.03
84.47
176.47
160.03
4,8,8
4,8,8
85%
74%
HM
HM
12

874.84

6,14,14
76%
HM

40
N
0.9
6.39
5,14,15
48%
ASnm

19
N
684.19
672.25
9,15,15
30%
HM

14
N
118.90
12.72
9,9,?
50%
HM

13

931.17

4,13,13
76%
HM

42
N
0.73
0.87
5,16,16
53%
ASnm

20
N
820.59
721.73
9,15,15
33%
HM

15
N
56.83
4.45
10,8,?
20%
1S

14

256.16
425.44
4,8,8
78%
HM

47

17.75
17.41
4,23,23
47%
ASnm

21
N
615.01
430.95
9,14,?
48%


16

13.81
6.54
8,8,8
89%
ASnm

15

282.79
429.81
4,8,8
75%
HM

48

125.49
131.94
7,26,26
80%
ASnm

22
N
929.64
721.82
9,15,?
28%
HM

17

18.4
17.41
6,7,7
58%
ASnm

17

65.79
152.37
4,6,6
67%
HM

49
N
3.02
3.11
8,19,?
37%
1S

23
N
965.49
769.36
9,15,?
43%


19

460.75

8,10,10
92%


18

112.50
217.76
4,8,8
74%
HM

50

0.59
1.03
4,16,16
30%
ASnm

39
N
1.1
1.9
9,8,?
14%
1S

21
N
342.20
55.70
7,7,7
51%
HM

Table 1: Full results, selected domains, SATPLAN04 variable domain abstraction (best-of, see text). Notations: Index:
index (number) instance respective IPC suite; IsReal: whether abstract plan real (Y) (N); L: length
optimal plan (? known), lg : lower bound plan length proved planning graph, la lower bound proved abstract task;
ta : runtime (secs) needed prove lower bound la abstract task; tr runtime (secs) needed prove lower bound la
real task; RelFrac: fraction facts considered relevant; dashes: time-out; Abs: corresponding form variable
domain abstraction, 1Support (1S), AllSupports (AS), AllSupportsNonMutex (ASnm), handmade (HM).

436

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Domain
Airport
Blocksworld
Depots
Dining Philosophers
Driverlog
Freecell
Grid
Logistics
Mprime
Mystery
Optical Telegraph
Pipesworld NoTankage
Pipesworld Tankage
PSR
Rovers
Satellite
Zenotravel

Index
20
7
19
29
13
1
2
23
5
20
13
12
8
48
8
12
13

ta
33.9
118.3
460.8
6.2
342.0
0.8
96.4
965.5
6.2
180.7
43.6
521.7
393.8
125.5
74.6
874.8
338.4

SATPLAN04
tr
IsReal?
lg , l , l r
29.5

25,32,32
11.2

16,20,20


8,10,10
5.5

7,11,11
113.7
N
9,11,12
0.8

4,5,5
3.1
N
19,13,?
769.4
N
9,15,15
5.2

6,6,6
112.4

7,7,7
32.0
N
11,13,13
455.5

8,14,14
143.4
N
5,6,?
131.9

7,26,26
97.8

5,9,9


6,14,14
244.8
N
4,7,7

RelFrac
73%
47%
92%
71%
61%
82%
10%
43%
78%
76%
53%
86%
89%
80%
87%
76%
67%

Table 2: Results SATPLAN04 best-case variable domain abstraction
challenging successful instances domain. Notation Table 1.

runtime better original task, yet cases abstraction
brings quite significant advantage. notably, instance number 19 SATPLAN04
runs time original task, solves abstract task, finding real plan, within
minutes. Logistics, best-case data mostly, though exclusively, due
conservative AllSupportsNonMutex handmade abstractions. abstract runtime
worse three cases (nos. 10, 14, 39), slightly better. PSR, best
cases almost exclusively due conservative AllSupportsNonMutex abstraction.
runtimes, abstraction usually faster, marginally. Satellite one
17 domains abstraction brings significant (and largely consistent) runtime
advantage. best cases almost exclusively due hand-made abstraction.
abstract plans real plans, often found significantly faster original task.
unclear us results good Satellite, but, example, Logistics,
state space reduction much larger.
Next, Table 2 provides overview results SATPLAN04 17 IPC
domains. make data presentation feasible, select one instance per domainthe
challenging successful instance. successful, mean least one abstract
task instance solved (abstract plan found), abstract task indeed
abstract (not facts relevant). challenging, mean maximum runtime original
task.21
21. Another strategy would select task maximizes tr ta , time advantage given
abstraction. However, cases strategy would select trivial instance: namely, tr ta
consistently negative, maximal easiest tasks.

437

fiDomshlak, Hoffmann, & Sabharwal

Domain
Airport
Blocksworld
Depots
Dining Philosophers
Driverlog
Grid
Logistics
Mprime
Mystery
Optical Telegraph
Pipesworld NoTankage
PipesworldTankage
PSR
Rovers
Satellite
Zenotravel

Index
8
7
17
5
9
1
8
2
2
2
5
7
10
6
7
12

ta
67.9
3.1
254.4
170.4
1.1
0.1
0.2
0.6
0.4
15.7
0.0
32.2
0.0
592.5
100.3
344.3

tr
0.3
0.0
268.4
138.0
0.7
0.2
1.0
1.0
0.7
5.2
0.0
0.6
0.0
375.7
2010.7
322.4

IPP
IsReal?




N
N

N
N
N

N
N
N



lg , l , l r
25,26,26
16,20,20
6,7,7
7,11,11
7,10,10
14,7,14
9,11,11
5,4,5
5,4,5
11,13,13
4,6,6
4,5,6
5,4,5
7,12,12
4,6,6
4,6,6

RelFrac
77%
47%
58%
71%
84%
43%
49%
62%
60%
53%
88%
82%
37%
90%
87%
67%

Table 3: Similar Table 2, IPP planner.

useful discuss 17 domains groups similar behavior. Depots, Logistics,
PSR, Satellite already discussed. Airport, Dining Philosophers,
Driverlog, Mystery, Mprime, Optical Telegraph, Pipesworld NoTankage, Pipesworld
Tankage, SATPLAN04 runtimes consistently lower original tasks, exceptions mostly among easiest instances. picture less consistent qualitatively
similar Zenotravel. degree advantage varies. relatively moderate
Dining Philosophers (up 7% less runtime original task), Optical Telegraph (up
23%), Airport (up 28%), Pipesworld Tankage (up 28%), Mprime (up 36%);
much stronger Zenotravel (up 75%), Mystery (up 80%), Driverlog (up 89%),
Pipesworld NoTankage (up 92%).
Rovers, runtime results inconclusive, minor advantages abstract
real depending instance. Blocksworld, SATPLAN04 solves abstract tasks
7 blocks only, independently abstraction used; dont know causes
bad behavior. Freecell, time AllSupports AllSupportsNonMutex
abstract anything, abstractions generated 1Support, SATPLAN04 runs
time, leaving instance number 1 successful case, shown Table 2. Grid,
finally, IPC 1998 test suite contains 5 instances, become huge quickly.
SATPLAN04 solve (abstract real) instances numbers 1 2, latter
shown Table 2.
Tables 3 4 provide similar snapshot results IPP Mips.BDD,
respectively. picture IPP is, roughly, similar SATPLAN04. main
difference is, fact, IPP weaker solver SATPLAN04 many domains,
effect domains contain interesting data. Specifically, Driverlog,
Mprime, Mystery, Pipesworld NoTankage, PSR, IPP either solves instances
time, all. Like SATPLAN04, see advantage abstraction Depots
438

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Domain
Airport
Blocksworld
Depots
Dining Philosophers
Driverlog
Freecell
Grid
Logistics
Mprime
Mystery
Optical Telegraph
Pipesworld NoTankage
Pipesworld Tankage
PSR
Rovers
Satellite
Zenotravel

Index


2

ta


1.1

Mips.BDD
tr
IsReal?







lg , l , l r


7,15,15

RelFrac


81%

10

1
12



503.6

1.8
7.2












N




5,17,17

14,7,?
10,42,42



84%

43%
43%



25
7

0.7
142.6

13.5
340.6




4,9,9
5,18,18

37%
86%

11

271.0





4,14,14

66%

Table 4: Similar Tables 23, Mips.BDD planner.

Satellite, note Satellite difference consistently huge. also
see vague advantage abstraction Logistics. Mips.BDD, even domains
gave meaningful data. domains dashed Table 4, Mips.BDD runs
time even smallest instances. domains left empty, either could run
Mips.BDD technical reasons, stopped abnormally. remaining data
set 7 domains, however, abstractions (as expected) bring consistent advantage
Mips.BDD. particular, consider behavior Logistics, Rovers, Zenotravelin
domains, Mips.BDD vastly improved abstraction SATPLAN04 IPP
less inconclusive.
3.4 Constructed Benchmarks
shown use abstractionof variable domain abstraction, least
speed state art planning systems varies quite promising Mips.BDD
rather hopeless SATPLAN04. ran number focused experiments examine
subtle reasons phenomenon. experiments done three
IPC benchmarksLogistics, Rovers, Zenotravelwhere results IPC test
suites relatively bad, although possession hand-made abstractions.
wanted test happens scale instances irrelevance. respective
experiment Logistics, Figure 1, discussed introduction. Rovers, tried
large number instance size parameters, even minor modifications operators,
could find setting contained lot irrelevance challenging
SATPLAN04 IPP. short, appears Rovers domain amenable
abstraction techniques. Zenotravel, obtained picture shown Figure 2.
439

fiDomshlak, Hoffmann, & Sabharwal

10000

100

10000

100

90
1000

90

80

1000

80

70
100

70

60

100

60

50
10

50

40

10

40

30
1

30

20
Mips.BDD-abstract
Mips.BDD-real
RelFrac

0.1
2

3

4

5

6

7

8

9

10

11

1

20
IPP-abstract
IPP-real
RelFrac

10
0

12

0.1

13

2

3

4

5

6

(a)

7

8

9

10

11

12

10
0
13

(b)
10000

100
90

1000

80
70

100

60
50

10

40
30

1

20
SATPLAN-abstract
SATPLAN-real
RelFrac

0.1
2

3

4

5

6

7

8

9

10

11

12

10
0
13

(c)
Figure 2: Runtime performance Mips.BDD (a), IPP (b), SATPLAN04 (c),
(abstract) without (real) hand-made variable domain abstraction,
Zenotravel instances explicitly scaled increase amount irrelevance.
Horizontal axis scales number cities, left vertical axis shows total runtime
seconds, right vertical axis shows percentage RelFrac relevant facts.

shown Zenotravel instances always feature 2 airplanes 5 persons. number
cities scales 2 13. Logistics, generated 5 random instances per size,
show average values time-out 1800 seconds, stopping plots 2 timeouts occurred instance size. all, relative behavior abstract real
curves planner quite similar observed Figure 1 Logistics.
SATPLAN04 IPP, abstraction slight disadvantage high RelFrac,
becomes much efficient RelFrac decreases. Mips.BDD, advantage brought
abstraction much pronounced, decreasing RelFrac consistently widens
gap solving abstract real tasks. average value RelFrac IPC
2000 Zenotravel benchmarks 64%, lying 5 cities (67%) 6 cities (63%)
Figure 2, yet much gained abstraction.
440

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

summary, appears planning benchmarks (like Rovers) good
abstractions, others (like Logistics Zenotravel) enough irrelevance
IPC test suites.
important note situation may quite bad unsolvable examples. Consider IPC benchmarks Dining Philosophers Optical Telegraph (Edelkamp,
2003). Dining Philosophers extremely basic benchmark cannot abstracted
much further. contrast, Optical Telegraph essentially version Dining Philosophers
complex inner life (exchanging data two hands philosopher).
inner life affect existence solution (deadlock situation), depends exclusively outer interfaces philosophers, is, taking releasing
forks. However, inner life does, course, affect length solution, one exists.
constructed unsolvable version domain (without deadlock situation) giving
philosophers flexibility releasing forks. one would expect, setting
abstracting inner life away gives huge savings, i.e., tasks proved unsolvable
much efficiently. suggests may easier abstract unsolvable tasks,
without invalidating property interest. Exploring topic future work.
planning benchmark domains naturally contain unsolvable instances,
over-subscription planning issue may become relevant (Sanchez & Kambhampati, 2005;
Meuleau, Brafman, & Benazera, 2006).
3.5 Abstractions
discussed earlier, one cannot expect removing preconditions, goals, entire facts
preserves plan length interesting cases. are, however, certain cases
delete effects safely ignored. Specifically: Driverlog, Logistics, Mprime, Mystery,
Zenotravel, one ignore deletes load unload actions state
object longer origin location (load) respectively object
longer inside vehicle (unload); Rovers one ignore deletes actions taking
rock soil samples, namely deletes stating sample longer origin
location. ran planners respective abstracted tasks. results
summarized follows.
SATPLAN04 clear loss runtime using abstraction Driverlog (e.g.,
task number 15 solved abstract vs. real 693.0 vs. 352.3 sec).
IPP vast gain abstraction Logistics (e.g., 52.8 vs 5540.1 sec number 12),
vast loss Zenotravel (e.g., 318.5 vs 2.5 sec number 12).
Mips.BDD vast loss abstraction Driverlog, Logistics, Zenotravel (e.g.,
163.8 vs. 8.3 sec Zenotravel number 8).
results inconclusive planner/domain pairs.

4. Resolution Complexity
discussed introduction, surprised see little improvement SATPLAN experiments, despite dramatic state space reductions brought
441

fiDomshlak, Hoffmann, & Sabharwal

variable domain abstraction. shed light issue, examining resolution
complexity original vs. abstracted planning tasks. Throughout section,
consider situation plan length boundthe number time steps CNF
encodingis small, thus CNFs unsatisfiable. Note case
one SAT tests performed SATPLAN. particular, case
SAT tests SATPLAN proves optimality plan, is, non-existence
plan n 1 steps n length optimal plan. proof typically
costly, accounting large fraction runtime taken SATPLAN.
consider abstraction methods introduced Section 2.4, plus (for completeness)
hypothetical abstraction method adds new initial facts. show Section 4.1 that,
many cases, resolution complexity cannot improved delegating optimality
proof within abstraction. Section 4.2 show that, considered cases,
resolution complexity become exponentially worse. Section 4.3 briefly examines
effect post-abstraction simplifications. sake readability, herein proofs
replaced proof sketches. full proofs available Appendix A.
Recall resolution complexity defined length shortest possible resolution refutation. proofs, arguments general (unrestricted) resolution.
However, constructions affect structure resolution refutations,
hence results hold stated (except slight weakening case Lemma 4.14)
many known variants resolution, including tree-like (DPLL), regular, ordered
resolution. general, results hold variant resolution setting variables True False replacing one variable another preserves proof structure (the
slightly exceptional status Lemma 4.14 explained discuss
result).
remainder paper, P planning task abstraction,
P denote respective abstracted planning task, is, planning task
results applying P.
4.1 Resolution Complexity Become Better?
prove three main results, captured Theorems 4.14.3 below. first
result holds four SAT encodings (A)(D) listed Section 2.2; two
results apply encodings (A) (C), respectively. respective encodings
abstraction methods, results essentially say resolution complexity cannot decrease
applying abstraction. outlined introduction, catchy (if imprecise) intuition
behind results over-approximations (abstractions) result less constrained
formulas, harder refute. encoding (C), result offset effort
required recover Graphplan mutexes; get back below. theorems
follow, recall Section 2.3 RC() denotes resolution complexity , i.e.,
size smallest resolution proof unsatisfiability .
Theorem 4.1. Let P planning task. Assume use encoding methods
(A)(D). Let abstraction P consists combination of:
(a) adding initial facts;
(b) ignoring preconditions, goals, deletes;
442

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

(c) removing fact completely.
Let n length shortest plan P , let b < n. Let encodings
b-step plan existence P P , respectively. RC() RC( ).
Theorem 4.2. Let P planning task. Assume use encoding method (A). Let
abstraction P consists combination of:
(a) adding initial facts;
(b) ignoring preconditions, goals, deletes;
(c) removing fact completely;
(d) variable domain abstraction.
Let n length shortest plan P , let b < n. Let encodings
b-step plan existence P P , respectively. RC() RC( ).
Theorem 4.3. Let P planning task. Assume use encoding method (C). Let
abstraction P consists combination of:
(a) adding initial facts;
(b) ignoring preconditions, goals, deletes;
(c) removing fact completely;
(d) variable domain abstraction.
Let n length shortest plan P , let b < n. Let encodings
b-step plan existence P P , respectively. Let number resolution steps
required infer additional mutex clauses appear ,
encoding b-step plan existence P per encoding (A). RC() RC( ) + .
Note theorems n, defined length shortest plan P ,
necessarily satisfies n length shortest plan P. Hence,
n b m, satisfiable. Detecting finding P plan length b
give us information length shortest plan P. 0 b < n,
however, unsatifiable, tells us b + 1 lower bound plan length P.
Hence, theorems say this: either b n coarse disprove existence
plan length b; b < n decrease resolution complexity
disproofat least complexity deriving additional mutexes,
case Theorem 4.3.
Let us first linger bit Theorem 4.3. general intuition results
abstractions induce less constrained formulas, hence resolution complexity cannot decrease. hold encoding (A) stated Theorem 4.2 not, strict
sense (see Proposition 4.13 later section), encoding (C)? Basically, answer
intuition imprecise general formulation, devil details.
particular case, issue variable domain abstraction makes use mutex
443

fiDomshlak, Hoffmann, & Sabharwal

relations encoding (C) aware of. Sometimes, indirect mutex original
task (omitted encoding (C)) becomes direct mutex abstraction (included encoding (C)). Refuting might involve recovering mutex, refutation
need do. Hence, potential improvement resolution complexity may stem
power mutex relations. upper bound specified Theorem 4.3 shows
thing improvement due to. Proposition 4.13 provides example
mutex must recovered, hence proves analogue Theorem 4.2
hold encoding (C).
open question whether analogue Theorem 4.2 holds encoding (B),
whether analogue Theorem 4.3 holds encoding (D). discuss
below, open questions appear related intricate properties Graphplanbased encodings vs. without fact variables. know mutexes may
need recovered also encoding (D): example provided Proposition 4.13 works
encodings (C) (D). Further, establish connection two open
questions: analogue Theorem 4.2 holds encoding (B), immediately get
analogue Theorem 4.3 holds encoding (D).
consider detail. Note that, far removal goals concerned,
theorems actually trivial: four encoding methods, removes part
goals, sub-formula . abstraction methods, latter
case. treat removal goals together methods since treament
cause overhead, goal clauses need discussed anyway (the set
achievers goal may change).
proofs, need helper notion captures over-approximated planning graphs. Assume planning task P planning graph P G(P), assume
abstraction. P G(P ) typically many vertices P G(P).
captures fact P allows fewer (and often more) facts actions P
does. will, general, result many constraints propositional translation
planning task. constraints, may seem like abstraction could,
principle, make possible derive easier/shorter proof fact plan exists
within specified bound. However, closer inspection restricted facts actions
already available original planning graph reveals one often ends fewer
weaker constraints original task. introduce notations
make formal.
Definition 4.4. planning task P abstraction it, P G (P) defined
subgraph P G(P ) induced vertices P G(P). Similarly, P Gred (P) defined
subgraph P Gred (P ) induced vertices P Gred (P).
Definition 4.5. Let P planning task. abstraction called planning graph
abstraction P P G (P) P G(P) identical sets vertices following
conditions hold:
(1) Eadd (P G (P)) Eadd (P G(P)),
(2) Epre (P G (P)) Epre (P G(P)),
(3) Emutex (P G (P)) Emutex (P G(P)),
444

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

(4) (G) G,
G (G) goal states P P , respectively. abstraction called
reduced planning graph abstraction conditions hold P Gred (P) P Gred (P)
instead.
Lemma 4.6. Let P planning task. Assume use encoding method (A) (B). Let
planning graph abstraction P. Let n length shortest plan P ,
let b < n. Let encodings b-step plan existence P P , respectively.
RC() RC( ).
Proof Sketch. Say set False variables appear , i.e.,
fix value variables 0. way defined P G (P),
yields precisely propositional encoding P G (P). show that, variable
restriction, clauses surviving also present , either stronger
form (i.e., fewer literals). example, encoding (A) precondition clause
C corresponding clause C due condition (2) Definition 4.5,
states introduce new preconditions. C C due condition
(1) Definition 4.5, states preserves add effectshence set actions
achieving precondition P contains corresponding set P. similar observation
holds effect clauses encoding (B), similar arguments apply
kinds clauses. claim follows Proposition 2.1.
Lemma 4.7. Let P planning task. Assume use encoding method (C) (D). Let
reduced planning graph abstraction P. Let n length shortest plan
P , let b < n. Let encodings b-step plan existence P P ,
respectively. RC() RC( ).
Proof. argument identical proof Lemma 4.6, except underlying
planning graph encodings (C) (D) reduced planning graph, resulting potentially fewer mutex clauses encodings (A) (B), respectively. This, however,
way affect proof arguments.
Lemma 4.8. Let P planning task. Let modification P respects
following behavior:
(a) shrink list initial facts,
(b) grow set goal facts,
(c) preserves add lists unchanged,
(d) grow pre del lists.
planning graph abstraction P well reduced planning graph abstraction
P.
445

fiDomshlak, Hoffmann, & Sabharwal

Proof Sketch. proof straightforward, little tedious details. Suppose
abstraction P satisfying prerequisites. must argue P G (P) P Gred (P)
satisfy conditions Definition 4.5. Condition (4) involving goal states easily follows
property (b) . P G(P) P G (P) (as well reduced counterparts)
shown set vertices, conditions (1) (2) involving precondition
effect relations follow directly properties (c) (d). hence remains prove
facts actions available P G(P) also available P G (P) (showing (1)
(2) said), new mutex relations created facts
actions mutex P G(P) (showing (3)). proof little tedious, proceeding
inductively construction planning graph. underlying intuition, however,
simple: P G (P) layer abstracts P G(P) layer t, properties (a), (c)
(d) respected , necessarily P G (P) layer + 1 abstracts P G(P)
layer + 1. concludes argument.
following immediate consequence Lemmas 4.6, 4.7, 4.8.
Corollary 4.9. Let P planning task. Assume use encoding methods
(A)(D). Let abstraction P consists combination of:
(a) adding initial facts;
(b) ignoring preconditions, goals, deletes.
Let n length shortest plan P , let b < n. Let encodings
b-step plan existence P P , respectively. RC() RC( ).
result essentially states rather intuitive fact that, abstraction anything yields larger planning graph, resulting Graphplan-based encodings
less constrained hence higher resolution complexity (if anything).
Matters become much less intuitive consider abstractions remove entire
factsclearly, longer result over-approximated planning graphs, since remove vertices. words, condition Definition 4.5 P G (P)
P G(P) identical sets vertices hold, need slightly different line
reasoning rely strictly abstracted planning graphs. first show
harmless remove fact appear goal pre del list.
rely Corollary 4.9 reason requirement fact easily achieved.
Lemma 4.10. Let P planning task. Assume use encoding methods
(A)(D). Let p fact appear goal pre del lists,
let abstraction P removes p initial facts add lists. Let
n length shortest plan P , let b < n. Let encodings
b-step plan existence P P , respectively. RC() = RC( ).
Proof Sketch. key point that, p appear goal never required
deleted action, p completely irrelevant planning task, particular
resolution refutations consider here. Concretely, first prove every layer
planning graph, available facts mutex fact pairs remain same,
facts fact pairs involving p. is, thing lost fact layers
446

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

P G (P) p. Since p occur precondition, action layers remain exactly
same; since p occur preconditions delete effects, action mutexes
also remain exactly (they caused p).
discussion implies precondition clauses encodings
identical. Given p appear goal, true goal clauses.
Since action mutexes unchanged, follows actually identical
encodings (A) (C). encodings (B) (D), difference
contain initial state, effect, mutex clauses involving p.
However, clauses never participate resolution refutation : effect
mutex clauses contain p polarity (negative); initial state clause
positive form {p(0)}, time index different p effect mutex
clauses. Hence every variable corresponding p occurs one polarity. concludes
argument.
Corollary 4.11. Let P planning task. Assume use encoding methods
(A)(D). Let abstraction P removes fact completely. Let n length
shortest plan P , let b < n. Let encodings b-step plan existence
P P , respectively. RC() RC( ).
Proof. equivalent following two steps. First, remove p goal facts (if
present) pre del lists. Corollary 4.9, step cannot improve resolution
complexity. Second, p removed goal pre del lists,
remove p problem completely removing initial facts add lists
well. Lemma 4.10, step well cannot improve resolution complexity,
done.
Corollaries 4.9 4.11 together prove first main result, Theorem 4.1.
move variable domain abstraction, matters complicated,
interesting abstraction method enables us construct solution length
preserving abstractions exponentially smaller state spaces, many benchmarks. First
show that, original form, result holds encoding (A).
Lemma 4.12. Let P planning task. Assume use encoding method (A). Let
variable domain abstraction P. Let n length shortest plan P , let
b < n. Let encodings b-step plan existence P P , respectively.
RC() RC( ).
Proof Sketch. combines two persistently mutex facts p p0 single fact p. first
show action pair (a, a0 ) mutex P , also mutex P.
way (a, a0 ) become mutex per requires, w.l.o.g., P, p del (a) pre(a)
p0 pre(a0 ) add (a0 ). Supposing (a, a0 ) mutex P, p 6 del (a0 ) p
mutex fact pre(a0 ). then, (noop(p), a0 ) mutex P hence
(p, p0 ) persistent mutex, contradiction.
hand, derive property rather similar planning graph
abstractions given Definition 4.5. above, know abstract encoding
mutexes appear . Further, set actions achieving
fact grows applying abstraction, goal shrink.
447

fiDomshlak, Hoffmann, & Sabharwal

subtle issue regards precondition clauses. action p0 precondition P,
replaced p P , direct correspondence two. However,
lack correspondence affect precondition clause encoding (A),
takes form {a, a1 , . . . , ak }; omits actual precondition fact achieved,
matter whether fact p0 p.
Next, proof Lemma 4.6, set variables False appear
. arguments, difficult see clauses surviving
also present , either stronger form (i.e., fewer literals).
mutex clauses, obvious. goal clauses, argument exactly
proof sketch Lemma 4.6 given above. precondition clauses, observe a1 , . . . , ak
contain achievers p0 plus achievers p. claim follows
Proposition 2.1.
Corollaries 4.9 4.11 together Lemma 4.12 prove second main result, Theorem 4.2. encodings (B)(D), matters complicated.
Consider first encodings (B) (D), differ (A) also
fact variables. changes precondition clauses. action P p0
precondition, p P , longer get clause {a, a1 , . . . , ak } proof
sketch. Instead, get clause {a, p}. clause, correspondence .
particular, consider case two actions P, action precondition
p action a0 precondition p0 . gives us clauses {a, p}, {a0 , p0 }
clauses {a, p}, {a0 , p} . Now, distinguish achievers
p p0 , problem regard. fact two
clauses share literalwhich dont exploited obtain shorter resolution
refutations? open question; discuss implications little detail
end sub-section.
Consider encoding (C), differs (A) includes direct action
mutexes. invalidates different argument proof Lemma 4.12. still true
that, action pair (a, a0 ) marked mutex P , also mutex P.
However, happen (a, a0 ) mutex P due direct interference
a0 , (a, a0 ) mutex P due mutex preconditions, rather direct
interference. Since encoding (C) accounts direct interferences,
mutex appear . result improved resolution complexity
. following proposition proves formally.
Proposition 4.13. Assume use encoding method (C). exist planning task P,
variable domain abstraction P, b < n RC() > RC( ), n
length shortest plan P , encodings b-step plan existence
P P , respectively.
Proof Sketch. construct P, , b specified. key property construction
two actions, getg1 getg2 , needed achieve goal facts
g1 g2 , respectively. precisely, getg1 = ({x}, {g1 , p0 }, {x}) getg2 = ({p, y},
{g2 }, {p}). task constructed, along help actions, way
x, p, p0 pairwise persistently mutex. variable domain abstraction replaces
p0 p, b set 2. action layer directly beneath goal layer, i.e., action
448

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

layer A(1), planning graph marks getg1 getg2 mutex preconditions
mutex. Encoding (C), however, include mutex clause
direct conflict. situation changes abstraction. getg1 adds p instead p0 ,
hence direct conflict delete effect getg2 . consequence,
abstraction two resolution steps suffice: applying getg1 getg2 A(1)
option achieve goals, new mutex clause immediately excludes option.
encoding original task, required mutex must first
derived reasoning preconditions x p.
Note reason get shorter refutation variable domain abstraction turns indirect action mutex (due competing preconditions)
direct interference. so, abstraction exploits knowledge p p0
persistently mutexa fact ignored encoding (C). Hence positive result stated
Proposition 4.13 less related power abstraction power planning
graph mutexes. capture formally. follows, note using
plan length bound, CNF formula per encoding (C) sub-formula CNF
formula per encoding (A), additional clauses (A) inferred it.
Lemma 4.14. Let P planning task. Let variable domain abstraction P.
Let n length shortest plan P , let b < n. Let C
encodings b-step plan existence P per encoding (A) (C), respectively. Let C
encoding b-step plan existence P per encoding (C). Let number
resolution steps required infer C additional mutex clauses appear .
RC(C ) RC(C ) + .
Proof. Denote encoding b-step plan existence P per encoding (A).
have:
(1) preconditions lemma, RC(C ) RC(A ) + : resolution steps,
C turned , hence shortest resolution refutation
construct one C steps longer.
(2) Lemma 4.12, RC(A ) RC(A ).
(3) C sub-formula , hence RC(A ) RC(C ).
Combining observations, have:
RC(C ) RC(A ) +

observation (1)

RC(A ) +
RC(C ) +

observation (2)




observation (3)

finishes proof.
Clearly, proof argument applies also combination variable domain
abstraction abstractions. Hence Corollaries 4.9 4.11 together
Lemmas 4.12 4.14 prove third main result, Theorem 4.3. Note latter
result hold variants resolution. claim Lemma 4.14,
449

fiDomshlak, Hoffmann, & Sabharwal

number resolution steps takes derive action mutexes present original
encoding. used resolution refutation. variant resolution
consideration is, say, DPLL tree-like resolution, deriving mutex clause
enoughit must re-derived many times used tree-like resolution
refutation. Hence effective value variants resolution would larger.
Note case DPLL solver learns mutex clauses virtue
wide-spread clause learning technique.
Lemma 4.14 particularly relevant empirical results, SATPLAN04 uses
encoding (C) experiments mostly focus variable domain abstraction.
explicit empirical proof (and proof would difficult come by, requiring
deep analysis SAT solvers search spaces), seems reasonable assume that,
least extent, disappointing results SATPLAN04 due whats proved
Lemma 4.14. abstraction cannot improve resolution complexity beyond effort
required recover indirect action mutexes. Note bound given
lemma rather pessimistic. mutex (a, a0 ) needs recovered case
a0 competing needs P, replacing p0 p results direct
interference incur simplifications. Logistics domain, example,
abstraction happens actions loading package onto airplane two
different irrelevant cities. Since load actions involved redundant
solutions anyway, seems doubtful mutexes play role resolution complexity.
generally, interesting consider upper bounds Lemma 4.14.
many resolution steps take recover indirect action mutexes? general resolution, number steps polynomially bounded, since inference process conducted
planning graph simulated (for related investigation, see Brafman, 2001).
restricted variants resolution, matters complicated. particular interest
behavior DPLL+clause learning, c.f. above. far known formula DPLL+clause learning proofs provably substantially worse general
resolution proofs; would rather surprising planning graph mutexes
first. Also, Rintanen (2008) provides related investigation, showing mutexes
recovered polynomial time particular 2-step lookahead procedure, related
(but identical) clause learning.
Concluding sub-section, let us turn attention encodings (B) (D).
mentioned before, open question whether analogue Theorem 4.2 holds
encoding (B), whether analogue Theorem 4.3 holds encoding (D).
facing two problematic issues:
(I) Fact variables. encoding (B) (D), fact variables addition
action variables used encodings (A) (C).
(II) Mutexes. Like encoding (C), may happen variable domain abstraction
converts implicit mutex encoding (D) original task explicit one
abstraction.
consider first issue (II). situation exactly encoding (C), regard.
Proposition 4.13 holds stated encoding (D) well; indeed proved using
450

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

exactly example minor adaptations proof arguments.22 Similarly, Lemma 4.14 holds encoding (D), exactly proof argumentsprovided
analogue Lemma 4.12 (and hence analogue Theorem 4.2) holds encoding (B).
Namely, proof arguments Lemma 4.14 remain valid, except need refer
encoding (B) rather (A), accordingly need corresponding version
Lemma 4.12. brings us issue (I).
Variable domain abstraction perceived gluing sets facts together. Recall
example clauses {a, p}, {a0 , p0 } sharing literals, clauses
{a, p}, {a0 , p} sharing literal p; discussed explain
proof Lemma 4.12 work encodings (B) (D). k + 1 facts glued
together, groups k clauses become linked together fashion. question
is:
(*) resolution fruitfully exploit increased linkage?
issue appears related quite intricate properties Graphplan-based
encodings vs. without fact variables. encoding (A), differs encoding
(B) use fact variables, Lemma 4.12 tells us resolution cannot
exploit variable domain abstraction. Now, appears reasonable think adding fact
variables help lot, intuition being:
(**) Whatever one encoding (B), one easily simulate encoding (A).
statement (**) true, answer question (*) no, yes
answer would contradict Lemma 4.12. Hence, initial attempt prove answer,
tried prove statement (**). However, initial investigation indicated
explicitly keeping fact variables (and non-trivial constraints them) around, encoding (B)
might facilitate significantly shorter resolution derivations general, hence statement
(**) might false. Namely, appear families formulas suitably
encoded planning tasks yield exponential separation encodings (A)
(B). true, suggests reasoning presence fact variables might
powerful hence might indeed able exploit linkage gain yielded
variable domain abstraction.
Since purpose paper compare relative power various
Graphplan-based encodings (such (A) (B)), detail progress
towards disproving statement (**). Besides, note that, statement (**) indeed false,
immediate implications answer question (*). definite
answer (*) left open future research.
4.2 Resolution Complexity Become Worse?
answer title sub-section definite yes. four encodings,
abstractions consider may exponentially deteriorate resolution complexity.
Formally, following theorem.
22. include full proof (C) (D) Appendix A.

451

fiDomshlak, Hoffmann, & Sabharwal

Theorem 4.15. Assume use encoding methods (A)(D). exist
infinite sequence planning tasks P(i), abstractions (i) P(i), b(i) < n(i)
RC( (i)) exponential RC((i)) constant independent i, n(i)
length shortest plan P (i), (i) (i) encodings b(i)-step plan
existence P(i) P (i) respectively, (i) consists one of:
(a) adding initial facts;
(b) ignoring preconditions, goals, deletes;
(c) removing fact completely;
(d) variable domain abstraction.
Proof Sketch. idea construct P(i) planning task consists two separate
sub-tasks, whose overall goal achieve goals sub-tasks.
sub-tasks infeasible within given plan length bound b(i). (The
tasks bounds constructed size grows polynomially i.) However,
first sub-task constructed require exponential size resolution refutations,
second allows constant size refutations. abstraction over-approximates easy-torefute sub-task way becomes feasible within b(i) steps, resolution
refutation overall task must rely hard-to-refute sub-task. leads
exponential growth, i, resolution complexity (i), opposed constant resolution complexity (i). single one listed abstractions, feasibility
easy-to-refute sub-task accomplished simple manner, hence proving theorem.
order construct planning tasks whose CNF encodings require exponential size
resolution refutations, resort pigeon hole problem formula PHP(i). well
known resolution proof PHP(i) must size exponential (Haken, 1985).
construct simple pigeon hole planning task PP HP (i) capture problem.
show that, four encoding methods (A)(D), CNF encoding b(i) = 1
either identical PHP(i), transforms PHP(i) variable restrictions. Hence,
Proposition 2.1, resolution refutation must size exponential i. final
construction uses combination two tasks: PP HP (i) serves hard-to-refute
sub-task, PP HP (1) disjoint sets pigeon hole objects serves easy-to-refute
sub-task.
Essentially, Theorem 4.15 states intuitive fact abstractions make bad
choices, approximating away concise reason planning task cannot
solved particular number steps. illustrate significance this, consider
comparison mutex relations. analogue Theorem 4.15 hold
them: adding mutex clause CNF encoding improve resolution complexity.
sense, mutex relations considerably less risky abstractions consider
here.
pigeon hole problem used proof Theorem 4.15 may seem artificial,
indeed contained sub-problem wide-spread domains concerned
transportation. example, Gripper, available time steps serve holes
actions picking/dropping balls pigeons (for related investigation, see
452

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Hoffmann et al., 2007). also seems quite natural planning task may consist two
disconnected parts, one complex one easy prove unsolvable
given number steps. think transporting two packages, one
close many vehicles requires one step bound allows,
one already inside vehicle needs transported along single path road
connections much longer bound (a concrete example latter situation
formalized Hoffmann et al., 2007).
4.3 Note Simplifications
pointed Section 2.4, may actions abstraction obviously
simplified without altering semantics planning task. particular, abstraction
might create duplicate actions (that is, actions identical preconditions effects),
well redundant add effects (that contained respective actions precondition).
turns natural post-abstraction simplification planning task lead
lower resolution complexity.
Proposition 4.16. Assume use encoding methods (A)(D). exist
planning task P, planning task P 0 identical P except either irrelevant
add affect duplicate action removed, b < n RC() > RC(0 ),
n length shortest plan P, 0 encodings b-step plan
existence P P 0 , respectively.
Proof Sketch. show claim duplicate actions, consider task P 0 encoding
pigeon hole problem 3 pigeons 2 holes, actions put pigeon p hole h.
plan length bound 1. point proof works solvable tasks P,
extra action a, whose preconditions two goals achieves third goal,
ensures solvability two steps. three goals, one pigeon,
mutex clauses direct action interference, pair goals
achieved three them. particular, every resolution refutation must resolve
three goal clauses. obtain P adding duplicate action one pigeons
one holes. respective goal clause becomes one literal longer.
refutation must get rid literal, hence necessitating one step. construction
works four encodings.
show claim removal redundant add effects, slightly modify P 0 , replacing
effect new fact x including another action achieves third goal given
precondition x. optimal plan length 3, length bound 2.
refutation must resolve three goal clauses. If, P, give one preconditions
additional add effect, refutations become longer respective goal
clause does. Again, construction works four encodings.
easy modify constructions used proof Proposition 4.16 way
duplicate action, respectively redundancy add effect, arise
outcome variable domain abstraction. Hence, via enabling simplifications, variable
domain abstraction may improve resolution best-case behavior. duplicate actions,
also true Edelkamps (2001) pattern database abstraction. open question
453

fiDomshlak, Hoffmann, & Sabharwal

whether improvement may exponential, whether bounded polynomially.
conjecture latter case, least unrestricted resolution.
also notable examples proof Proposition 4.16 specifically
constructed include duplicate actions/redundant add effects actions relevant
solving problem form part optimal solution. well-constructed variable
domain abstraction likely happen since abstraction target facts
irrelevant solution length. Consider Logistics domain example.
actions simplifications apply loads/unloads packages to/from locations
cities packages origin destination. actions involved
redundant solutions, seems doubtful simplification affects resolution
complexity. course, simplifications might help SAT solvers anyway. This, however,
observed, least significantly, experiments.

5. Conclusion
Abstractions, used here, power allow proving certain properties within much
smaller state spaces. particular, abstraction preserves length optimal
solution, optimality proved within abstraction. designed novel abstraction method STRIPS planning suitable purpose. Surprisingly,
approach yields little benefits planning-as-satisfiability approach represented SATPLAN, even domains featuring hand-made abstractions exponentially smaller state spaces. Towards explaining this, shown that, many cases,
abstraction method (as well commonly used abstractions) lacks ability introduce shorter resolution refutationsother exploiting mutexes,
enabling certain post-abstraction simplifications. contrast, shown
abstractions may exponentially increase size shortest resolution refutations.
Several questions left open theoretical results. know whether variable domain abstraction improve resolution complexity combination encoding
(B), whether polynomial upper bound improvement variable domain
abstraction bring encoding (D), whether polynomial upper bound
improvement result simplifications. Apart answering questions, importantly remains seen extent results generalize. Bluntly
stated, intuition behind results over-approximations usually result less
constrained formulas harder refute. However, actual technicalities
results depend quite lot detailsof encoding method abstraction
hence largely unclear extent intuitive statement captures reality.
particular: hold encodings planning SAT?
would interesting, e.g., look alternative encodings described Kautz
Selman (1992), Kautz et al. (1996), Kautz Selman (1996), Ernst, Millstein,
Weld (1997). Many encodings based unit clauses initial goal state,
action clauses stating action implies effects preconditions.
structureand lack mechanism planning graph propagates
changessome properties proved herein obvious. Removing goals initial state
facts corresponds directly removing clauses; true preconditions. Removing
fact completely may cases simply correspond removing clauses mention
454

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

fact. Hence, encodings, seems proving results might indeed
comparatively easy. challenging subject may recent developments,
encodings Rintanen et al. (2006) often give substantial speed-ups
novel notions parallelity, encodings Chen et al. (2009) introduce
long-distance mutex relations, encodings Robinson et al. (2008) make use
effective operator splitting factoring methods.
generally: results hold methods employed fields? particular,
hold model-checking, abstraction (e.g., Graf & Sadi, 1997; Clarke
et al., 2003) SAT encodings (e.g., Clarke, Biere, Raimi, & Zhu, 2001; Prasad, Biere,
& Gupta, 2005) highly successful? one example actually obvious
results hold. Gupta Strichman (2005) abstract ignoring clauses CNF
encoding original transition system (the motivation much smaller CNF
formula causes less overhead SAT solver).
ambitiously: define generic framework formal notions declarative
transition systems, CNF encodings, abstractions suitable capture
results, prove generic statements? questions appear worthwhile
research challenges. Indeed, think key contribution work may lie asking
question resolution complexity vs. without abstraction.
practical perspective, see mainly four lines research. First,
important question whether observations carry modified/extended planningas-SAT systems, one Ray Ginsberg (2008) guarantees plan
optimality branching restrictions within single SAT call, rather
calling SAT solver iteratively. Second, remains open explore whether different
abstraction techniquesbased e.g. predicate abstractioncan suitably adapted
planning. Third, important note empirical results entirely negative.
Mips.BDD often substantially improved, even point where, Figure 1,
optimal sequential planner highly competitive strong optimal parallel planner
SATPLAN, highly parallel domain Logistics. direction
may well worth exploring depth. Finally, effective abstraction methods may
exist unsolvable examples, could potentially play crucial role over-subscription
planning (Sanchez & Kambhampati, 2005; Meuleau et al., 2006).

Acknowledgments
thank anonymous reviewers, whose detailed comments helped greatly improve
paper. preliminary version work appeared ICAPS06, 16th International
Conference Automated Planning Scheduling (Hoffmann, Sabharwal, & Domshlak,
2006). work Carmel Domshlak supported Israel Science Foundation (ISF)
Grants 2008100 2009580, well C. Wellner Research Fund. part
work, Jorg Hoffmann employed Max Planck Institute Computer Science,
Saarbrucken, Germany, SAP Research, Karlsruhe, Germany. work Ashish
Sabharwal supported IISI, Cornell University (AFOSR Grant FA9550-04-1-0151),
National Science Foundation (NSF) Expeditions Computing award (Computational
455

fiDomshlak, Hoffmann, & Sabharwal

Sustainability Grant 0832782), NSF IIS award (Grant 0514429), Defense Advanced
Research Projects Agency (DARPA, REAL Grant FA8750-04-2-0216).

Appendix A. Proof Details
Proof Proposition 2.1. Suppose sequence transformations consists ` restrictions, 1 , 2 , . . . , ` . Further, let 0 strengthening transformation replaces
clause | (not necessarily strict) sub-clause also clause ;
transformation exists assumptions proposition. Observe
` + 1 transformation steps together convert (not necessarily strict) sub-formula
. show ` + 1 transformation steps individually increase
resolution complexity underlying formula. Without loss generality,
prove fact single restriction transformation generic strengthening
transformation. Since individually shown increase resolution
complexity formula, applied sequence combination, number
times, without increasing resolution complexity. would prove, particular,
resolution complexity sub-formula , implying
resolution complexity (as additional initial clauses
cannot hurt resolution refutation), desired.
start single restriction transformation x y. ease notation,
assume initial formula F = {C1 , C2 , . . . , Cm } resulting simplified
0 }, m0 m. Without loss
formula transformation F 0 = {C10 , C20 , . . . , Cm
0
0
generality, assume Ci equals empty clause {} duplicate
clauses F 0 . Let = (C1 , C2 , . . . , Cm , Cm+1 , . . . , CM = {}) resolution refutation
F smallest possible size; note involves initial clauses resolution
steps. , construct resolution refutation 0 F 0 size larger
. following three steps.
Step 1. Transform = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), Ci defined
follows. application transformation x results Ci containing True
variable negation, Ci equals True; results Ci containing
False duplicate literals, Ci consists Ci False duplicate literal removed;
otherwise Ci = Ci . Note Ci contain x either True, empty
clause, non-empty (not necessarily strict) sub-clause C. key property
Ci still logical implicant Cj Ck Ci derived resolving
Cj Ck original proof . Ci may necessarily usual resolution
resolvent Cj Ck , next two steps fix.
Step 2. Transform = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), Ci equals Ci
> defined sequentially, increasing i, follows. Suppose
Ci derived resolving clauses Cj Ck , j < k < i. Assume
without loss generality already defined Cj Ck . Ci equals
True, Ci equals True well; otherwise, one two clauses Cj Ck
equals True, Ci equals clause; otherwise, Cj Ck resolved
together variable Ci resolvent two clauses. key
property here, seen easily considering sequential nature
456

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

transformation, Ci either True (not necessarily proper) subclause Ci ; particular, CM = {}. Further, Ci equal True
either resolution resolvent Cj Ck , equals {} one Cj Ck
also {}.
Step 3. Finally, transform 0 simply removing clauses equal True
occur previously clause sequence, stopping sequence soon
first empty clause encountered. construction, , , exactly
clauses 0 clauses. Further, first m0 clauses
0 exactly clauses F 0 0 resolution refutation starting
initial clauses, desired.
consider strengthening transformation 0 , essentially replaces
clause formula sub-clause (thereby strengthening clause). show
applying 0 increase resolution complexity underlying formula.
Again, ease notation, let initial formula F = {C1 , C2 , . . . , Cm } resolution
refutation = (C1 , C2 , . . . , Cm , Cm+1 , . . . , CM = {}) smallest possible size.
argument along lines simpler restriction transformations.
Transform = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), m, Ci equals
sub-clause Ci 0 maps Ci to, > m, Ci defined sequentially, increasing
i, follows. Suppose Ci derived resolving clauses Cj Ck variable x,
j < k < i. Assume without loss generality already defined Cj
Ck . x present Cj Ck , Ci simply resolution resolvent
two clauses; otherwise, x present Cj , Ci equals Cj ; otherwise x must
present Ck set Ci equal Ck . key property here, seen
easily considering sequential nature transformation, Ci (not
necessarily proper) sub-clause Ci ; particular, CM = {}. Further, Ci either Cj
Ck resolution resolvent two. transform 0 simply removing
clauses occur previously clause sequence. construction,
exactly clauses 0 clauses. Further, first m0 clauses
0 exactly clauses F 0 0 resolution refutation starting initial
clauses, desired.
Proof Lemma 4.6. Let P planning task abstraction abstracts
P G(P) applied. Let , denote propositional encodings P P , respectively,
use either action-only encoding (A) action-fact encoding (B)
. Let U denote set variables variables . Finally,
let variable restriction sets every variable U False. setting,
propositional formula | nothing CNF encoding planning graph P G (P)
(using encoding method, (A) (B), used ). particular,
clauses correspond actions facts P G(P) trivially satisfied
, contain negation variable U , set False . Call
remaining, yet unsatisfied clauses | surviving clauses. argue
surviving clause already present, perhaps stronger weaker form,
itself, showing easier prove | unsatisfiable prove
unsatisfiable.
457

fiDomshlak, Hoffmann, & Sabharwal

First consider encoding (A). conditions (2) (4) Definition 4.5, every surviving
precondition goal clause | corresponding clause itself. Further,
condition (1) concerning facts added action, precondition
goal clause | contains sub-clause corresponding clause . Finally,
surviving mutex clauses | , condition (3), also present mutex clause .
observations, follows every surviving clause | contains (possibly
non-strict) sub-clause corresponding clause . Applying Proposition 2.1, obtain
RC() RC( ), finishing proof encoding (A).
consider encoding (B). First, Definition 4.5 P G (P) P G(P)
identical sets vertices, particular fact vertices F (0) same, hence
surviving initial state clause | also present initial state clause . Further, precondition clauses binary and, condition (2) Definition 4.5, surviving
precondition clause | also present precondition clause . Similarly,
goal clause unit clause and, condition (4), surviving goal clause | also
present goal clause . similar vein, surviving mutex clause | also
present mutex clause . Finally, surviving effect clause | , condition
(1), contains sub-clause corresponding effect clause | . Hence see
every surviving clause | contains (possibly non-strict) sub-clause corresponding clause . Applying Proposition 2.1 before, obtain RC() RC( ),
finishing proof encoding (B).
Proof Lemma 4.8. Let P planning task abstraction respects
behavior specified lemma applied. show four conditions
Definition 4.5 hold P G (P) P Gred (P). Observe condition (4)
Definition 4.5 trivially holds property (b) . therefore focus showing
V (P G(P)) = V (P G (P)), V (P Gred (P)) = V (P Gred (P)), conditions (1)(3)
Definition 4.5 hold. fact, show P G (P) P Gred (P) set
vertices original (reduced) planning graph, conditions (1) (2) Definition 4.5
would immediately satisfied due properties (c)23 (d) , would
remain would condition (3), saying new mutex clauses added applying .
Hence, task reduced proving following four new properties hold
step 0, 1, . . . , b planning task:
(i) F (t) F (t),
(ii) (t) A(t),
(iii) Ef-mutex (t)|F (t) Ef -mutex (t),

(iv) Ea-mutex
(t)|A(t) Ea-mutex (t).

F (t), A(t), Ef -mutex (t), Ea-mutex (t) denote sets facts, actions (including
noops), fact mutexes, action mutexes generated P step t, -versions
denote corresponding sets P . F (t), Ef-mutex (t)| denotes {(f1 , f2 )
23. one needs add lists shrink applying . However, another argument
shortly require add lists grow either, justifying strict requirement property (c).

458

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings


Ef-mutex (t) | f1 , f2 }, subset Ef-mutex (t) restricted facts . Ea-mutex
(t)|

defined similarly (t).
order prove four properties hold, give inductive argument t,
alternating F (t) Ef -mutex (t) one hand, A(t) Ea-mutex (t)
other. base case = 0, note F (0) F (0) property (a)
Ef-mutex (0)|F (0) Ef -mutex (0) sets empty.
words, goal prove certain facts actions available P
certain step, remain available P . Similarly, two facts actions mutually
compatible P, remain mutually compatible P . seems intuitively justifiable
given properties . following argument formalizes intuition. terms
notation, use pre, del , add specify actions P pre , del , add
specify actions P .
first part inductive step, suppose F (t) F (t) Ef-mutex (t)|F (t)

Ef -mutex (t). show (t) A(t) Ea-mutex
(t)|A(t) Ea-mutex (t), inductively
proving conditions (ii) (iv).
Let A(t). pre (a) F (t) F (t). Further, f, f 0 pre (a) pre(a)
F (t), (f, f 0 ) 6 Ef -mutex (t) Ef-mutex (t)|F (t) hence (f, f 0 ) 6 Ef-mutex (t).
Therefore (t), proving (t) A(t).
let a, a0 A(t) (a, a0 ) 6 Ea-mutex (t). reduced planning graph

(t) due properties
direct mutexes, immediately (a, a0 ) 6 Ea-mutex

(c) (d) , done proving Ea-mutex (t)|A(t) Ea-mutex (t). Otherwise,
general mutexes, several things hold. First, pre (a) pre(a) F (t) F (t),
a0 . Likewise, del (a) del (a) del (a0 ) del (a0 ). Finally,
property (c), add (a) add (a). Hence (c.1) (pre (a) add (a)) del (a0 )
(pre(a)add (a))del (a0 ) = ; last equality holds (a, a0 ) 6 Ea-mutex (t). Similarly,
(c.2) (pre (a0 )add (a0 ))del (a) = . Finally, f pre (a) pre(a) F (t) F (t)
f 0 pre (a0 ) pre(a0 ) F (t) F (t), (f, f 0 ) 6 Ef -mutex (t)
Ef-mutex (t)|F (t) , implies (c.3) (f, f 0 ) 6 Ef-mutex (t). (c.1), (c.2), (c.3),


(a, a0 ) 6 Ea-mutex
(t), proving Ea-mutex
(t)|A(t) Ea-mutex (t).

second part inductive step, suppose (t) A(t) Ea-mutex
(t)|A(t)


Ea-mutex (t). show F (t+1) F (t+1) Ef -mutex (t+1)|F (t+1) Ef -mutex (t+
1), proving conditions (i) (iii).


Let f F (t + 1). f aA(t) add (a) aA (t) add (a). (Recall noop
actions included A(t), need explicitly include F (t) F (t + 1).)
follows f F (t + 1), proving F (t + 1) F (t + 1).
let f, f 0 F (t + 1) (f, f 0 ) 6 Ef -mutex (t + 1). must exist
0
a, A(t) (t) (c.1) f add (a) add (a), (c.2) f 0 add (a0 ) add (a),


(a, a0 ) 6 Ea-mutex (t) Ea-mutex
(t)|A(t) , implies (c.3) (a, a0 ) 6 Ea-mutex
(t).
0


(c.1), (c.2), (c.3), (f, f ) 6 Ef -mutex (t + 1), proving Ef -mutex (t +
1)|F (t+1) Ef -mutex (t + 1).
finishes inductive argument, showing conditions (i)(iv) outlined
hold. earlier reasoning, proves vertices P G(P) P G (P),
well reduced counterparts, (so conditions (1) (2)
Definition 4.5 follow directly properties (c) (d) ) mutex relations
P red (P), restricted facts actions P, subset mutex relations

459

fiDomshlak, Hoffmann, & Sabharwal

reduced mutex relations, respectively, P (so condition (3) Definition 4.5
holds). Hence abstracts planning graph well reduced planning graph
P.
Proof Lemma 4.10. Let abstraction removes p initial facts
add lists given planning task P p appear goal facts
pre del lists. proof Lemma 4.6, let , denote propositional
encodings P P , respectively, use one encodings (A), (B), (C),
(D) . show encodings (A) (C), fact
identical, encodings (B) (D), differ clauses cannot part
resolution proof.
end, use planning graph notation proof Lemma 4.8
begin arguing induction F (t) = F (t) \ {p}, Ef-mutex (t) = Ef -mutex (t) \ {(p, p0 ) |

p0 F (t)}, (t) = A(t), Ea-mutex
(t) = Ea-mutex (t). base case = 0,

F (0) = F (0)\{p} definition , Ef-mutex (t) = Ef -mutex (0)\{(p, p0 ) | p0 F (0)}
sets empty. first part induction, suppose
inductive conditions F Ef -mutex hold time step t. Since p pre list,
implies (t) = A(t) well. Further, since p del list, also

Ea-mutex
(t) = Ea-mutex (t). Hence conditions Ea-mutex hold time step
t. second part induction, suppose inductive conditions
Ea-mutex hold time step t. implies F (t + 1) consists F (t + 1) possibly
p. Further, Ef -mutex (t + 1) Ef-mutex (t + 1) far mutexes involving
p concerned. follows conditions F Ef -mutex hold time step + 1,
finishing induction.

summarize, shown every step, sets A, Ea-mutex , Ea-mutex
exactly same, sets F, F Ef -mutex , Ef-mutex facts
pairs facts involving p. words, new actions facts become available mutually excluded planning graph , everything involving p remains
unchanged. Given this, observe four encodings, goal precondition clauses P exactly P p appear goal

pre lists all. Similarly, Ea-mutex
(t) = Ea-mutex (t) Ef-mutex (t) = Ef -mutex (t)

implies action mutexes P , fact mutexes present encoding,
P well. Therefore, encodings (A) (C), = .
Finally, encodings (B) (D), initial state, effect, mutex clauses
get removed applying , i.e., present . However,
clauses mention propositional variables corresponding p,
variables appears one polarity throughout . Namely, initial state clause
{p(0)} clause may contain p positive polarity; effect mutex clauses
contain p time index > 0. that, clauses cannot part
resolution refutation every variable appearing resolution refutation must
eventually resolved away order derive empty clause. follows
respect resolution refutations.
Proof Lemma 4.12. Let P planning task , variable domain abstraction,
applied. combines two persistently mutex facts p p0 single fact p.
brevity, let G denote P G(P). Define G 0 graph obtained unifying p
460

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

p0 fact vertices fact layer G single vertex p layer, similarly
noop(p) noop(p0 ) vertices action layers. Finally, let G denote subgraph
P G(P ) induced vertices G 0 . show G abstracted planning
graph sense similar Definition 4.5.
begin arguing action pair (a, a0 ) mutex P , also mutex
P. see this, observe way (a, a0 ) become mutex per requires,
w.l.o.g., P, p del (a) pre(a) p0 pre(a0 ) add (a0 ). Suppose sake
contradiction (a, a0 ) already mutex P. particular, means
p 6 del (a0 ) p mutex fact pre(a0 ). This, however, implies
(noop(p), a0 ) mutex P fact pair (p, p0 ) mutex next layer
P G(P), contradiction p p0 persistently mutex. follows edges
Ea-mutex G subset G.
Since G 0 P initial facts, argument implies actions
facts available layer G 0 also available layer P G(P ). particular,
G , construction, exactly set vertices G 0 . Further, since variable
domain abstraction, edges Eadd Epre G 0 G exactly same.
Define , , U, proof Lemma 4.6. | CNF encoding (A)
planning graph G , clauses corresponding actions facts
G trivially satisfied . Call remaining clauses | surviving clauses
before.
observation edges Ea-mutex G G, surviving mutex clauses
| also mutex clauses . surviving precondition goal clauses involving
p appear unchanged . Since considering variable domain abstraction,
actions achieving p G precisely actions achieving either p p0 G. Hence,
surviving precondition goal clauses | involving p contain sub-clause
precondition goal clause itself. follows Proposition 2.1 RC()
RC( ).
Proof Proposition 4.13. example planning task, denoted P, works
encoding (C) encoding (D). Let denote variable domain abstraction applied.
example uses following six facts: facts p p0 , glued together ;
goal facts g1 g2 ; helper facts x, y. Based facts, task P defined
follows:
Initial state {p}; Goal {g1 , g2 }
Action set containing five actions:
getx = ({p}, {x}, {p}),
gety = (, {y}, ),
getg1 = ({x}, {g1 , p0 }, {x}),
getg2 = ({p, y}, {g2 }, {p}),
getp = ({p0 }, {p}, {p0 }).
plan length bound 2, makes problem infeasible: shortest (parallel)
plan requires 4 steps: h{getx, gety}, {getg1 }, {getp}, {getg2 }i. Observe pairs
actionsexcept (getg1 , getg2 ) pair involves getydirectly interfere
461

fiDomshlak, Hoffmann, & Sabharwal

therefore mutex. P G(P), get following fact action sets
step 2:
F (0) = {p}, A(0) = {noop(p), getx, gety}
F (1) = {p, x, y}, A(1) = {noop(p), noop(x), noop(y), getx, gety, getg1 , getg2 }
F (2) = {p, x, y, g2 , g1 , p0 }
easy verify, iteratively, that, P G(P): p x mutex F (1); p x
mutex F (2); p p0 mutex F (2); x p0 mutex F (2); mutexes
get also F (3), planning graph reaches fixpoint. particular,
getg1 getg2 always (indirectly) mutex preconditions x p
persistently mutex. variable domain abstraction glue p p0 , converting
conflict getg1 getg2 direct interference, thereby allowing shorter
resolution refutation.
Consider encoding (C) P G(P). contains two goal clauses: {getg1 (1)}
{getg2 (1)}. clauses clearly must used resolution refutation formula, possible achieve goal individually within given time bound,
together. Hence, shortest refutation must involve least two steps.
argue shortest refutation achieved abstracted task P
P itself.
P , get fact action sets planning graph, except F (2) =
{p, x, y, g1 , g2 }, i.e., p0 course present, A(0) A(1) contain also getp
acts similarly noop(p). corresponding encoding (C) consists exactly
clauses (plus clauses noop(p) mirrored getp), except
get additional clause {getg1 (1), getg2 (1)}. mutex clauses arises
getg1 interferes directly getg2 (rather indirectly incompatible preconditions), getg1 adds p instead p0 , p deleted getg2 . yields
trivial two-step (tree-like) resolution proof P , using two goal clauses mutex
clause (namely, resolve second goal clause mutex clause deriving {getg1 (1)},
resolve clause first goal clause). hand, original
task P, getg1 getg2 marked mutex layer A(1), dont directly
interfere. Therefore, corresponding mutex clause immediately available,
resolution proof takes two steps must reason involve x.
Encoding (D) works similarly, lets us derive new mutex clause discussed
above. goal clauses case simply {g1 (2)} {g2 (2)}. these, using
two corresponding effect clauses, derive two goal clauses encoding (C) two
steps. here, two-step refutation discussed derives empty clause. Thus,
four-step resolution refutation P encoding (D). similarly small
resolution refutation P itself, refutation must, mentioned earlier,
reason x figure getg1 (1) getg2 (1) cannot True.
Proof Theorem 4.15. construct family STRIPS tasks whose CNF encodings
similar pigeon hole problem formula PHP(i). well known
resolution proof PHP(i) must size exponential (Haken, 1985). Concretely,
PHP(i) unsatisfiable formula encoding fact way assign + 1
462

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

pigeons holes pigeon assigned least one hole hole gets
one pigeon. formula i(i + 1) variables xp,h p {1, . . . , + 1}, h
{1, . . . , i}. pigeon p, pigeon clause (xp,1 , xp,2 , . . . , xp,n ), pair
pigeons {p, q} hole h, hole clause {xp,h , xq,h }.
pigeon hole planning task PP HP (i) defined follows. pigeon p,
fact assigned (p). hole h, fact free(h). initial state contains free
facts assigned facts. goal state contains i+1 assigned facts. available
actions (other noops) put(p, h), puts pigeon p free hole h,
h longer remains free. Formally, put(p, h) = ({free(h)}, {assigned (p)}, {free(h)}).
plan length bound b(i) set 1.
Consider one four encoding methods (A)(D), let (i) encoding
PP HP (i). Restrict (i) setting noop variables False; real restrictive
implication terms planning since plan length bound 1 none goal facts
available time step 0. action-only encodings (A) (C), identifying
action variables put(p, h) PHP(i) variables xp,h immediately yields precisely
clauses PHP(i): goal clauses (i) become pigeon clauses PHP(i)
action mutex clauses become hole clauses. action-fact encodings (B) (D),
fix free fact variables time step 0 well assigned fact variables time step 1
True, identify put(p, h) action variables xp,h . yields precisely
clauses PHP(i). follows resolution hardness PHP(i) Proposition 2.1
resolution proof fact planning task PP HP (i) plan
length 1 must require size exponential i.
claim follows planning task P 0 (i) consists combination two
disconnected pigeon hole planning sub-tasks, PP HP (i) PP HP (1), two separate sets
pigeon hole objects. goal P 0 (i) naturally defined follows: put first
set + 1 pigeons first set holes put second set two pigeons
second set holes (which consists single hole). overall CNF encoding
0 (i) P 0 (i) logical conjunction encodings (i) (1) (on disjoint sets
variables) PP HP (i) PP HP (1). Observe 0 (i) proved unsatisfiable
proving unsatisfiability either two pigeon hole problems. particular,
constant size resolution refutation 0 (i) involves refuting (1) component.
hand, argue listed abstractions make one-hole
component P 0 (i) trivially satisfiable, resolution refutation abstracted
task must resort proof unsatisfiability i-hole component (i) P 0 (i),
shown requires exponential size. Hence single example P 0 (i) serves show
claim combinations abstraction method CNF encoding.
easily verified PP HP (1) becomes solvable ignoring precondition
free(1) put(1, 1) put(2, 1): put pigeons single hole.
happens ignoring delete effect free(1) put(1, 1) put(2, 1).When
ignoring goal assigned (2), inserting assigned (2) initial state,
completely removing assigned (2), one-hole component P 0 (i) requires assign
one pigeon, course possible. Finally, variable domain abstraction, note
assigned (1) assigned (2) persistently mutex PP HP (1) actions
achieving put(1, 1) put(2, 1), respectively. According Definition 2.2,
hence replace assigned (2) assigned (1). resulting planning task,
463

fiDomshlak, Hoffmann, & Sabharwal

single goal assigned (1) achieved 1 step by, example, put(1, 1) action.
concludes argument.
Proof Proposition 4.16. first consider removal duplicate actions. example planning task, denoted P 0 , works four encodings; P 0 defined follows:
Fact set {r1 , r2 , g1 , g2 , g3 }
Initial state {r1 , r2 }; Goal {g1 , g2 , g3 }
Action set containing seven actions:
1 1 = ({r1 }, {g1 }, {r1 }),
1 2 = ({r1 }, {g2 }, {r1 }),
1 3 = ({r1 }, {g3 }, {r1 }),
2 1 = ({r2 }, {g1 }, {r2 }),
2 2 = ({r2 }, {g2 }, {r2 }),
2 3 = ({r2 }, {g3 }, {r2 }),
help = ({g1 , g2 }, {g3 }, ).
planning task, actions applicable initial state consume one
two resources r1 r2 . actions achieves one goals, pair
goals reached, three them. solution perform two steps,
second help action serves accomplish g3 . set plan length
bound 1.
planning graph P G(P 0 ) step 1 mutex relations direct
mutexes actions competing resource. Hence, encoding (A) identical
encoding (C), encoding (B) identical encoding (D). properties clearly
hold also planning task P like P 0 except additional action 1 10
identical 1 1.
Consider encoding (A) P 0 . goal clauses {1 1(0), 2 1(0)}, {1 2(0), 2 2(0)},
{1 3(0), 2 3(0)}. clauses mutex clauses form {i j, k}.
difficult verify shortest resolution refutation involves 12 steps. One
derivation proceeds via deriving {2 2(0), 2 1(0)}, {2 3(0), 2 1(0)}, {2 1(0)}, {1 2(0)},
{1 3(0)}, {}; derived, sequence, 2 steps involving resolution
one mutex clause. P, thing changes clause
{1 1(0), 1 10 (0), 2 1(0)} instead {1 1(0), 2 1(0)}, plus additional mutex clauses. Now,
obviously every resolution refutation must resolve three goal clauses. end
empty clause, hence additionally need get rid literal 1 10 (0). Clearly,
way resolve literal away additional step involving
one new mutex clauses. Hence shortest possible resolution refutation
13 steps.
encoding (B), resolution proofs first need make three steps resolving goal
clauses {g1 (1)}, {g2 (1)}, {g3 (1)} respective effect clauses {g1 (1), 1 1(0), 2 1(0)},
{g2 (1), 1 2(0), 2 2(0)}, {g3 (1), 1 3(0), 2 3(0)}; thereafter, matters
before.
show claim removal redundant add effects, slightly modify example,
define P 0 follows:
464

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Fact set {r1 , r2 , g1 , g2 , g3 , x}
Initial state {r1 , r2 }; Goal {g1 , g2 , g3 }
Action set containing eight actions:
1 1 = ({r1 }, {g1 }, {r1 }),
1 2 = ({r1 }, {g2 }, {r1 }),
1 3 = ({r1 }, {g3 }, {r1 }),
2 1 = ({r2 }, {g1 }, {r2 }),
2 2 = ({r2 }, {g2 }, {r2 }),
2 3 = ({r2 }, {g3 }, {r2 }),
help1 = ({g1 , g2 }, {x}, ),
help2 = ({x}, {g3 }, ).
task, single help action replaced two help actions need
applied consecutively. set plan length bound 2. before, planning graph
P G(P 0 ) mutex relations direct mutexes actions competing
resource; encodings (A)/(C) (B)/(D) respectively identical.
properties clearly hold also planning task P like P 0 except help1
additional add effect g1 .
Consider encoding (A) P 0 . goal clauses {1 1(1), 2 1(1), noop(g1 )(1)}, {1 2(1),
2 2(1), noop(g2 )(1)}, {1 3(1), 2 3(1), noop(g3 )(1)}. Refuting involves showing
three goals cannot achieved step 1, step 0, combination
two. refutation needs resolve three clauses. before, P get
additional literal first clause, {1 1(1), 2 1(1), help1 , noop(g1 )(1)}.
Clearly, getting rid additional literal involves least one resolution step.
encoding (B), matters essentially except first need resolve goal
fact clauses respective effect clauses.

References
Ball, T., Majumdar, R., Millstein, T., & Rajamani, S. (2001). Automatic predicate abstraction C programs. PLDI2001: Programming Language Design Implementation, pp. 203213.
Beame, P., Kautz, H., & Sabharwal, A. (2004). Towards understanding harnessing
potential clause learning. Journal Artificial Intelligence Research, 22, 319351.
Beck, C., Hansen, E., Nebel, B., & Rintanen, J. (Eds.). (2008). Proceedings 18th
International Conference Automated Planning Scheduling (ICAPS-08). AAAI
Press.
Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90 (1-2), 279298.
Blum, A. L., & Furst, M. L. (1995). Fast planning planning graph analysis.
Mellish, S. (Ed.), Proceedings 14th International Joint Conference Artificial
Intelligence (IJCAI-95), pp. 16361642, Montreal, Canada. Morgan Kaufmann.
465

fiDomshlak, Hoffmann, & Sabharwal

Boddy, M., Fox, M., & Thiebaux, S. (Eds.). (2007). Proceedings 17th International
Conference Automated Planning Scheduling (ICAPS-07). AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bonet, B., & Geffner, H. (2008). Heuristics planning penalties rewards formulated logic computed circuits. Artificial Intelligence, 172 (12-13),
15791604.
Brafman, R. (2001). reachability, relevance, resolution planning satisfiability approach. Journal Artificial Intelligence Research, 14, 128.
Chaki, S., Clarke, E., Groce, A., Jha, S., & Veith, H. (2003). Modular verification software
components C. ICSE2003: Int. Conf. Software Engineering, pp. 385395.
Chen, Y., Huang, R., Xing, Z., & Zhang, W. (2009). Long-distance mutual exclusion
planning. Artificial Intelligence, 173 (2), 365391.
Clarke, E. M., Biere, A., Raimi, R., & Zhu, Y. (2001). Bounded model checking using
satisfiability solving. Formal Methods System Design, 19 (1), 734.
Clarke, E. M., Grumberg, O., Jha, S., Lu, Y., & Veith, H. (2003). Counterexample-guided
abstraction refinement symbolic model checking. Journal Association
Computing Machinery, 50 (5), 752794.
Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem proving.
Communications ACM, 5 (7), 394397.
Davis, M., & Putnam, H. (1960). computing procedure quantification theory. Journal
Association Computing Machinery, 7 (3), 201215.
Edelkamp, S. (2001). Planning pattern databases. Cesta, A., & Borrajo, D. (Eds.),
Recent Advances AI Planning. 6th European Conference Planning (ECP01),
pp. 1324, Toledo, Spain. Springer-Verlag.
Edelkamp, S. (2003). Promela planning. Ball, T., & Rajamani, S. (Eds.), Proceedings
10th International SPIN Workshop Model Checking Software (SPIN-03),
pp. 197212, Portland, OR. Springer-Verlag.
Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimize state encoding length. Biundo, S., & Fox, M. (Eds.), Recent Advances AI
Planning. 5th European Conference Planning (ECP99), Lecture Notes Artificial
Intelligence, pp. 135147, Durham, UK. Springer-Verlag.
Ernst, M., Millstein, T., & Weld, D. (1997). Automatic sat-compilation planning problems. Pollack, M. (Ed.), Proceedings 15th International Joint Conference
Artificial Intelligence (IJCAI-97), pp. 11691176, Nagoya, Japan. Morgan Kaufmann.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach application theorem
proving problem solving. Artificial Intelligence, 2 (34), 198208.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search
temporal action graphs. Journal Artificial Intelligence Research, 20, 239290.
466

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Graf, S., & Sadi, H. (1997). Construction abstract state graphs PVS. CAV1997:
Computer Aided Verification, pp. 7283.
Gupta, A., & Strichman, O. (2005). Abstraction refinement bounded model checking.
Etessami, K., & Rajamani, S. (Eds.), Proceedings 17th International Conference
Computer Aided Verification (CAV05), Lecture Notes Computer Science, pp.
112124, Edinburgh, UK. Springer-Verlag.
Haken, A. (1985). intractability resolution. Theoretical Computer Science, 39, 297
308.
Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Chien, S.,
Kambhampati, R., & Knoblock, C. (Eds.), Proceedings 5th International Conference Artificial Intelligence Planning Systems (AIPS-00), pp. 140149, Breckenridge, CO. AAAI Press, Menlo Park.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent
construction pattern database heuristics cost-optimal planning. Proceedings
Twenty-Second AAAI Conference Artificial Intelligence (AAAI-2007), pp.
10071012. AAAI Press.
Helmert, M., & Mattmuller, R. (2008). Accuracy admissible heuristic functions selected planning domains. Proceedings 23rd AAAI Conference Artificial
Intelligence, pp. 938943, Chicago, IL. AAAI Press.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimal
sequential planning.. Boddy et al. (Boddy, Fox, & Thiebaux, 2007), pp. 176183.
Henzinger, T., Jhala, R., Majumdar, R., & McMillan, K. (2004). Abstractions proofs.
POPL2004: Principles Programming Languages, pp. 232244.
Hernadvolgyi, I., & Holte, R. (1999). PSVN: vector representation production systems.
Tech. rep. 1999-07, University Ottawa.
Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. Journal
Artificial Intelligence Research, 24, 519579.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning
benchmarks. Journal Artificial Intelligence Research, 24, 685758.
Hoffmann, J., Gomes, C., & Selman, B. (2007). Structure problem hardness: Goal asymmetry dpll proofs sat-based planning. Logical Methods Computer Science,
3 (1:6).
Hoffmann, J., Sabharwal, A., & Domshlak, C. (2006). Friends foes? AI planning
perspective abstraction search.. Long, & Smith (Long & Smith, 2006), pp.
294303.
Katz, M., & Domshlak, C. (2008). Structural pattern heuristics via fork decomposition..
Beck et al. (Beck, Hansen, Nebel, & Rintanen, 2008), pp. 182189.
467

fiDomshlak, Hoffmann, & Sabharwal

Kautz, H., & Selman, B. (1999). Unifying SAT-based graph-based planning. Pollack, M. (Ed.), Proceedings 16th International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 318325, Stockholm, Sweden. Morgan Kaufmann.
Kautz, H. (2004). SATPLAN04: Planning satisfiability. Edelkamp, S., Hoffmann,
J., Littman, M., & Younes, H. (Eds.), Proceedings 4th International Planning
Competition (IPC-04), Whistler, BC, Canada.
Kautz, H., Selman, B., & Hoffmann, J. (2006). SATPLAN: Planning satisfiability.
Gerevini, A., Dimopoulos, Y., Haslum, P., Saetti, A., Bonet, B., & Givan, B. (Eds.),
Proceedings 5th International Planning Competition (IPC-06), Ambleside, UK.
Kautz, H. A., McAllester, D., & Selman, B. (1996). Encoding plans propositional logic.
Aiello, L. C., Doyle, J., & Shapiro, S. (Eds.), Principles Knowledge Representation
Reasoning: Proceedings 5th International Conference (KR-96), pp. 374384,
Cambridge, MA. Morgan Kaufmann.
Kautz, H. A., & Selman, B. (1992). Planning satisfiability. Neumann, B. (Ed.),
Proceedings 10th European Conference Artificial Intelligence (ECAI-92),
pp. 359363, Vienna, Austria. Wiley.
Kautz, H. A., & Selman, B. (1996). Pushing envelope: Planning, propositional logic,
stochastic search. Proceedings 13th National Conference American
Association Artificial Intelligence (AAAI-96), pp. 11941201, Portland, OR. MIT
Press.
Knoblock, C. A. (1990). Learning abstraction hierarchies problem solving. Proceedings
8th National Conference American Association Artificial Intelligence
(AAAI-90), pp. 923928, Boston, MA. MIT Press.
Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs
ADL subset.. Steel, & Alami (Steel & Alami, 1997), pp. 273285.
Koehler, J., & Hoffmann, J. (2000). reasonable forced goal orderings use
agenda-driven planning algorithm. Journal Artificial Intelligence Research,
12, 338386.
Long, D., Kautz, H. A., Selman, B., Bonet, B., Geffner, H., Koehler, J., Brenner, M.,
Hoffmann, J., Rittinger, F., Anderson, C. R., Weld, D. S., Smith, D. E., & Fox, M.
(2000). aips-98 planning competition. AI Magazine, 21 (2), 1333.
Long, D., & Smith, S. (Eds.)., ICAPS-06 (2006). Proceedings 16th International Conference Automated Planning Scheduling (ICAPS-06), Ambleside, UK. Morgan
Kaufmann.
McDermott, D. (1999). Using regression-match graphs control search planning. Artificial Intelligence, 109 (1-2), 111159.
Meuleau, N., Brafman, R., & Benazera, E. (2006). Stochastic over-subscription planning
using hierarchies MDPs.. Long, & Smith (Long & Smith, 2006), pp. 121130.
Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts operators
plan generation.. Steel, & Alami (Steel & Alami, 1997), pp. 338350.
468

fiFriends Foes? Planning Satisfiability Abstract CNF Encodings

Prasad, M. R., Biere, A., & Gupta, A. (2005). survey recent advances sat-based
formal verification. International Journal Software Tools Technlogy Transfer,
7 (2), 156173.
Ray, K., & Ginsberg, M. L. (2008). complexity optimal planning efficient
method finding solutions.. Beck et al. (Beck et al., 2008), pp. 280287.
Rintanen, J. (2004). Evaluation strategies planning satisfiability. Saitta, L. (Ed.),
Proceedings 16th European Conference Artificial Intelligence (ECAI-04), pp.
682687, Valencia, Spain. Wiley.
Rintanen, J. (2008). Planning graphs propositional clause-learning. Brewka, G., &
Doherty, P. (Eds.), Principles Knowledge Representation Reasoning: Proceedings 11th International Conference (KR-08), pp. 535543, Sydney, Australia.
AAAI Press.
Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning satisfiability: parallel plans
algorithms plan search, artificial intelligence. Artificial Intelligence, 170 (12-13),
10311080.
Robinson, J. A. (1965). machine oriented logic based resolution principle. Journal
Association Computing Machinery, 12 (1), 2341.
Robinson, N., Gretton, C., Pham, D.-N., & Sattar, A. (2008). compact efficient sat
encoding planning.. Beck et al. (Beck et al., 2008), pp. 296303.
Sacerdoti, E. (1973). Planning hierarchy abstraction spaces. Proceedings
3rd International Joint Conference Artificial Intelligence (IJCAI-73), pp. 412422,
Stanford, CA. William Kaufmann.
Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics selecting objectives over-subscription planning problems. Biundo, S., Myers, K., & Rajan, K.
(Eds.), Proceedings 15th International Conference Automated Planning
Scheduling (ICAPS-05), pp. 192201, Monterey, CA, USA. Morgan Kaufmann.
Steel, S., & Alami, R. (Eds.). (1997). Recent Advances AI Planning. 4th European Conference Planning (ECP97), Vol. 1348 Lecture Notes Artificial Intelligence,
Toulouse, France. Springer-Verlag.
Streeter, M., & Smith, S. (2007). Using decision procedures efficiently optimization..
Boddy et al. (Boddy et al., 2007), pp. 312319.

469

fiJournal Artificial Intelligence Research 36 (2009) 307-340

Submitted 06/09; published 11/09

Cross-lingual Annotation Projection Semantic Roles
Sebastian Pado

pado@ims.uni-stuttgart.de

Institut fur maschinelle Sprachverarbeitung
Universitat Stuttgart, 70174 Stuttgart, Germany

Mirella Lapata

mlap@inf.ed.ac.uk

School Informatics, University Edinburgh
10 Crichton Street, Edinburgh EH8 10 AB, UK

Abstract
article considers task automatically inducing role-semantic annotations
FrameNet paradigm new languages. propose general framework based
annotation projection, phrased graph optimization problem. relatively inexpensive potential reduce human effort involved creating role-semantic
resources. Within framework, present projection models exploit lexical
syntactic information. provide experimental evaluation English-German parallel corpus demonstrates feasibility inducing high-precision German semantic
role annotation manually automatically annotated English data.

1. Introduction
Semantic roles play prominent role linguistic theory (Fillmore, 1968; Jackendoff, 1990;
Dowty, 1991). describe relations hold predicate arguments,
abstracting surface syntactic configurations. example, consider sentences (1a)
(1b) butter uniformly assigned semantic role Undergoer (since undergoes physical change) even though syntactically realized object verb
melt (1a) subject (1b):
(1)

a.
b.

[Bob]Agent melted [the butter]Undergoer .
[The butter]Undergoer melted.

intermediate representation seems promising first step towards text understanding,
ultimately benefit many natural language processing tasks require broad coverage semantic processing.
Methods automatic identification labeling semantic roles, often referred
shallow semantic parsing (Gildea & Jurafsky, 2002), important prerequisite
widespread use semantic role information large-scale applications. development
shallow semantic parsers1 greatly facilitated availability resources like
FrameNet (Fillmore, Johnson, & Petruck, 2003) PropBank (Palmer, Gildea, & Kingsbury, 2005), document possible surface realization semantic roles. Indeed, semantic
1. Approaches building shallow semantic parsers numerous list. refer interested reader
proceedings 2005 CoNLL shared task (Carreras & Marquez, 2005) 2008 Computational Linguistics Special Issue Semantic Role Labeling (Marquez, Carreras, Litkowski, & Stevenson,
2008) overview state-of-the-art.
c
2009
AI Access Foundation. rights reserved.

fiPado & Lapata

Departing
object (the Theme) moves away Source.
Theme

officer left house.
plane leaves seven.
departure delayed.

Source

departed New York.
retreated opponent.
woman left house.

FEEs

abandon.v, desert.v, depart.v, departure.n,
emerge.v, emigrate.v, emigration.n, escape.v,
escape.n, leave.v, quit.v, retreat.v, retreat.n,
split.v, withdraw.v, withdrawal.n

Table 1: Abbreviated FrameNet entry Departing frame
roles recently found use applications ranging information extraction (Surdeanu,
Harabagiu, Williams, & Aarseth, 2003) modeling textual entailment relations (Tatu
& Moldovan, 2005; Burchardt & Frank, 2006), text categorization (Moschitti, 2008), question answering (Narayanan & Harabagiu, 2004; Frank, Krieger, Xu, Uszkoreit, Crysmann,
Jorg, & Schafer, 2007; Moschitti, Quarteroni, Basili, & Manandhar, 2007; Shen & Lapata,
2007), machine translation (Wu & Fung, 2009a, 2009b) evaluation (Gimenez &
Marquez, 2007).
FrameNet paradigm, meaning predicate (usually verb, noun, adjective) represented reference frame, prototypical representation situation
predicate describes (Fillmore, 1982). semantic roles, called frame elements, correspond entities present situation, therefore frame-specific.
frame, English FrameNet database2 lists predicates evoke (called
frame-evoking elements FEEs), gives possible syntactic realizations semantic
roles, provides annotated examples British National Corpus (Burnard, 2000).
abbreviated example definition Departing frame shown Table 1. semantic roles illustrated example sentences FEEs shown bottom
table (e.g., abandon, desert, depart). PropBank corpus, second major semantic role resource English, provides role realization information verbs similar
manner Wall Street Journal portion Penn Treebank. uses index-based role
names (Arg0Argn), Arg0 Arg1 correspond Dowtys (1991) proto-agent
proto-patient. Higher indices defined verb-by-verb basis.
Unfortunately, resources FrameNet PropBank largely absent almost
languages except English, main reason role-semantic annotation
expensive time-consuming process. current English FrameNet (Version 1.3)
developed past twelve years. contains roughly 800 frames covering
2. Available http://framenet.icsi.berkeley.edu.

308

fiCross-lingual Annotation Projection Semantic Roles

around 150,000 annotated tokens 7,000 frame-evoking elements. Although FrameNets
constructed German, Spanish, Japanese, resources considerably
smaller. true PropBank-style resources, developed
Korean3 , Chinese (Xue & Palmer, 2009), Spanish Catalan (Taule, Mart, & Recasens,
2008). Compared English PropBank, covers 113,000 predicate-argument structures, resources languages two three times smaller (e.g., Korean
PropBank provides 33,000 annotations).
Given data requirements supervised learning algorithms (Fleischman & Hovy,
2003) current paucity data, unsupervised methods could potentially enable
creation annotated data new languages reduce human effort involved.
However, unsupervised approaches shallow semantic parsing still early stage,
mostly applicable resources FrameNet (Swier & Stevenson, 2004, 2005;
Grenager & Manning, 2006). article, propose method employs parallel
corpora acquiring frame elements syntactic realizations new languages (see
upper half Table 1). approach leverages existing English FrameNet overcome resource shortage languages exploiting translational equivalences
present aligned data. Specifically, uses annotation projection (Yarowsky, Ngai, & Wicentowski, 2001; Yarowsky & Ngai, 2001; Hwa, Resnik, Weinberg, & Kolak, 2002; Hi &
Hwa, 2005) transfer semantic roles English less resource-rich languages. key
idea projection summarized follows: (1) given pair sentences E (English)
L (new language) translations other, annotate E semantic roles;
(2) project roles onto L using word alignment information. manner,
induce semantic structure L side parallel text, serve
data training shallow semantic parser L independent parallel corpus.
annotation projection paradigm faces least two challenges considering semantic roles. Firstly, semantic structure projected must shared two
sentences. Clearly, role-semantic analysis source sentence E inappropriate
target sentence L, simple projection produce valid semantic role annotations.
Secondly, even two sentences demonstrate semantic parallelism, semantic role annotations
pertain potentially arbitrarily long word spans rather individual words. Recovering word span semantic roles target language challenging given
automatic alignment methods often produce noisy incomplete alignments.
address first challenge showing that, two languages exhibit substantial
degree semantic correspondence, annotation projection feasible. Using EnglishGerman parallel corpus test bed, assess whether English semantic role annotations
transferred successfully onto German. find two languages exhibit
degree semantic correspondence substantial enough warrant projection. tackle
second challenge presenting framework projection semantic role annotations
goes beyond single word alignments. Specifically, construct semantic alignments
constituents source target sentences formalize search best
semantic alignment optimization problem bipartite graph. argue bipartite
graphs offer flexible intuitive framework modeling semantic alignments able
deal noise represent translational divergences. present different classes
3. Korean PropBank available LDC (http://www.ldc.upenn.edu/).

309

fiPado & Lapata

models varying assumptions regarding admissible correspondences source
target constituents. Experimental results demonstrate constituent-based models
outperform word-based alternatives large margin.4
remainder article organized follows. Section 2 discusses annotation
projection general presents annotation study examining degree semantic
parallelism English-German corpus. Section 3, formalize semantic alignments
present modeling framework. experiments detailed Section 4. review
related work Section 5 conclude article discussion future work (Section 6).

2. Annotation Projection Semantic Correspondence
recent years, interest grown parallel corpora multilingual cross-lingual
natural language processing. Beyond machine translation, parallel corpora exploited
relieve effort involved creating annotations new languages. One important
paradigm, annotation projection, creates new monolingual resources transferring annotations English (or resource-rich languages) onto resource-scarce languages
use word alignments. resulting (noisy) annotations used
conjunction robust learning algorithms obtain NLP tools taggers
chunkers relatively cheaply. projection approach successfully used transfer wide range linguistic annotations languages. Examples include parts
speech (Yarowsky et al., 2001; Hi & Hwa, 2005), chunks (Yarowsky et al., 2001), dependencies (Hwa et al., 2002), word senses (Diab & Resnik, 2002; Bentivogli & Pianta, 2005),
information extraction markup (Riloff, Schafer, & Yarowsky, 2002), coreference chains (Postolache, Cristea, & Orasan, 2006), temporal information (Spreyer & Frank, 2008), LFG
f-structures (Tokarczyk & Frank, 2009).
important assumption underlying annotation projection linguistic analyses
one language also valid another language. however unrealistic expect
two languages, even family, perfect correspondence.
many well-studied systematic differences across languages often referred translational
divergences (van Leuven-Zwart, 1989; Dorr, 1995). structural,
semantic content source target language realized using different
structures, semantic, content undergoes change translation. Translational divergences (in conjunction poor alignments) major stumbling block
towards achieving accurate projections. Yarowsky Ngai (2001) find parts speech
transferred directly English onto French contain considerable noise, even
cases inaccurate automatic alignments manually corrected (accuracies vary
69% 78% depending tagset granularity). syntax, Hwa, Resnik, Weinberg, Cabezas, Kolak (2005) find 37% English dependency relations
direct counterparts Chinese, 38% Spanish. problem commonly addressed
filtering mechanisms, act post-processing step projection output.
example, Yarowsky Ngai (2001) exclude infrequent projections poor alignments
4. preliminary version work published proceedings EMNLP 2005 COLING/ACL
2006. current article contains detailed description approach, presents several novel
experiments, comprehensive error analysis.

310

fiCross-lingual Annotation Projection Semantic Roles

Hwa et al. (2005) apply transformation rules encode linguistic knowledge
target language.
case semantic frames reason optimism. definition, frames
based conceptual structure (Fillmore, 1982). latter constitute generalizations
surface structure therefore ought less prone syntactic variation. Indeed, efforts
develop FrameNets manually German, Japanese, Spanish reveal large
number English frames re-used directly describe predicates arguments
languages (Ohara, Fujii, Saito, Ishizaki, Ohori, & Suzuki, 2003; Subirats & Petruck,
2003; Burchardt, Erk, Frank, Kowalski, Pado, & Pinkal, 2009). Boas (2005) even suggests
frame semantics interlingual meaning representation.
Computational studies projection parallel corpora also obtained good results
semantic annotation. Fung Chen (2004) induce FrameNet-style annotations Chinese mapping English FrameNet entries directly onto concepts listed HowNet5 ,
on-line ontology Chinese, without using parallel texts. experiment, transfer
semantic roles English Chinese accuracy 68%. Basili, Cao, Croce, Coppola, Moschitti (2009) use gold standard annotations transfer semantic roles
English Italian 73% accuracy. Bentivogli Pianta (2005) project EuroWordNet
sense tags, represent fine-grained semantic information FrameNet, also
English Italian. obtain precision 88% recall 71%, without applying filtering. Fung, Wu, Yang, Wu (2006, 2007) analyse automatically annotated
EnglishChinese parallel corpus find high cross-lingual agreement PropBank roles
(in range 75%95%, depending role).
provide sound empirical justification projection-based approach, conducted manual annotation study parallel English-German corpus. identified
semantic role information bi-sentences assessed degree frames semantic roles agree diverge English German. degree divergence provides
natural upper bound accuracy attainable annotation projection.
2.1 Sample Selection
English-German bi-sentences drawn second release Europarl (Koehn, 2005),
corpus professionally translated proceedings European Parliament. Europarl
aligned document sentence level available 11 languages. English
German section contains 25 million words sides. Even though restricted
genre (transcriptions spoken text), Europarl fairly open-domain, covering wide range
topics foreign politics, cultural economic affairs, procedural matters.
naive sampling strategy would involve randomly selecting bi-sentences Europarl
contain FrameNet predicate English side aligned word
German side. two caveats here. First, alignment two predicates
may wrong, leading us assign wrong frame German predicate. Secondly, even
alignment accurate, possible randomly chosen English predicate evokes
frame yet covered FrameNet. example, FrameNet 1.3 documents
receive sense verb accept (as sentence Mary accepted gift),
entry admit sense predicate (e.g., accept problem
5. See http://www.keenage.com/zhiwang/e_zhiwang.html.

311

fiPado & Lapata

Measure
Frame Match
Role Match
Span Match

English
89.7
94.9
84.4

German
86.7
95.2
83.0


88.2
95.0
83.7

Table 2: Monolingual inter-annotator agreement calibration set

EU ) relatively frequent Europarl. Indeed, pilot study, inspected
small random sample consisting 100 bi-sentences, using publicly available GIZA++
software (Och & Ney, 2003) induce English-German word alignments. found 25%
English predicates readings documented FrameNet, additional
9% predicate pairs instances wrong alignments. order obtain cleaner
sample, final sampling procedure informed English FrameNet SALSA,
FrameNet-compatible database, German (Erk, Kowalski, Pado, & Pinkal, 2003).
gathered GermanEnglish sentences corpus least one pair
GIZA++-aligned predicates (we , wg ), listed FrameNet wg SALSA,
intersection two frame lists wg non-empty. corpus
contains 83 frame types, 696 lemma pairs, 265 unique English 178 unique German
lemmas. Sentence pairs grouped three bands according frame frequency
(High, Medium, Low). randomly selected 380 pairs band annotation.
total sample consisted 1,140 bi-sentences. semantic annotation took place,
constituency parses corpus obtained Collins (1997) parser English
Dubeys (2005) German. automatic parses corrected manually, following
annotation guidelines Penn Treebank (English) TIGER corpus (German).
2.2 Annotation
syntactic correction, two annotators native-level proficiency German English annotated bi-sentence frames evoked wg semantic
roles (i.e., one frame per monolingual sentence). every predicate, task involved two
steps: (a) selecting appropriate frame (b) assigning instantiated semantic roles
sentence constituents. Annotators provided detailed guidelines explained
task multiple examples.
annotation took place gold standard parsed corpus proceeded three
phases: training phase (40 bi-sentences), calibration phase (100 bi-sentences),
production mode phase (1000 bi-sentences). training, annotators acquainted
annotation style. calibration phase, bi-sentence doubly annotated
assess inter-annotator agreement. Finally, production mode, 1000 bi-sentences
main dataset split half randomly assigned one coders
single annotation. thus ensured annotator saw parts bi-sentence
avoid language bias role assignment (annotators may prone label
English sentence similar German translation vice versa). coder annotated
approximately amount data English German access
FrameNet SALSA resources.
312

fiCross-lingual Annotation Projection Semantic Roles

Measure
Frame Match
Role Match

Precision
71.6
90.5

Recall
71.6
92.3

F1-Score
71.6
91.4

Table 3: Semantic parallelism English German
results inter-annotator agreement study given Table 2. widely used
Kappa statistic directly applicable task requires fixed set items
classified fixed set categories. case, however, fixed items, since
span frame elements length. addition, categories (i.e., frames
roles) predicate-specific, vary item item (for discussion issue, see also
work Miltsakaki et al., 2004). Instead, compute three different agreement measures
defined as: ratio common frames two sentences (Frame Match), ratio
common roles (Role Match), ratio roles identical spans (Span Match).
shown Table 2, annotators tend agree frame assignment; disagreements mainly
due fuzzy distinctions closely related frames (e.g., Awareness
Certainty). Annotators also agree roles assign identifying role spans.
Overall, obtain high agreement aspects annotation, indicates
task well-defined. aware published agreement figures English
FrameNet annotations, results comparable numbers reported Burchardt,
Erk, Frank, Kowalski, Pado, Pinkal (2006) German, viz. 85% agreement frame
assignment (Frame Match) 86% agreement role annotation.6
2.3 Evaluation
Recall main dataset consists 1,000 English-German bi-sentences annotated
FrameNet semantic roles. Since annotations language created independently, used provide estimate degree semantic parallelism
two languages. measured parallelism using precision recall, treating
German annotations gold standard. evaluation scheme directly gauges usability English source language annotation projection. Less 100% recall
means target language frames roles present English
cannot retrieved annotation projection. Conversely, imperfect precision indicates
English frames roles whose projection yields erroneous annotations target language. Frames roles counted matching occur halves
bi-sentence, regardless role spans, comparable across languages.
shown Table 3, 72% time English German evoke
frame (Frame Match). result encouraging, especially considering frame
disagreements also arise within single language demonstrated inter-annotator
study calibration set (see row Frame Match Table 2). However, also indicates
non-negligible number cases translational divergence frame level.
often cases one language chooses single predicate express situation
whereas one uses complex predication. following example, English
transitive predicate increase evokes frame Cause change scalar position (An
6. parallel corpus created available http://nlpado.de/~sebastian/srl_data.html.

313

fiPado & Lapata

agent cause increases position variable scale). German translation
fuhrt zu hoheren (leads higher) combines Causation frame evoked fuhren
inchoative Change scalar position frame introduced hoher :
(2)

increase level employment.
Dies wird zu einer hoheren Erwebsquote
fuhren.

higher
level employment lead

level semantic roles, agreement (Role Match) reaches F1-Score 91%.
means frames correspond across languages, roles agree large extent.
Role mismatches frequently cases passivization infinitival constructions leading
role elision. example below, remembered denkt evoke Memory
frame. English uses passive construction leaves deep subject position unfilled.
contrast, German uses active construction deep subject position filled
semantically light pronoun, man (one).
(3)

ask [Ireland]Content remembered.
Ich mochte
darum bitten, dass [man]Cognizer [an Irland]Content denkt.
would like
ask one
Ireland
thinks

sum, find substantial cross-lingual semantic correspondence
English German provided predicates evoke frame. enlisted
help SALSA database meet requirement. Alternatively, could used
existing bilingual dictionary (Fung & Chen, 2004), aligned frames automatically using
vector-based representation (Basili et al., 2009) inferred FrameNet-style predicate labels
German following approach proposed Pado Lapata (2005).

3. Modeling Semantic Role Projection Semantic Alignments
previous work projection relies word alignments transfer annotations
languages. surprising, since annotations interest often defined
word level (e.g., parts speech, word senses, dependencies) rarely span
one token. contrast, semantic roles cover sentential constituents arbitrary length,
simply using word alignments projection likely result wrong role spans.
example, consider bi-sentence Figure 1.7 Assume (English) source annotated semantic roles wish project onto
(German) target. Although alignments (indicated dotted lines sentence) accurate (e.g., promised versprach, zu), others noisy incomplete
(e.g., time punktlich instead time punktlich). Based alignments,
Message role would projected German onto (incorrect) word span punktlich zu
instead punktlich zu kommen, since kommen aligned English word.
course possible devise heuristics amending alignment errors. However,
process scale well: different heuristics need created different errors,
7. literal gloss German sentence Kim promises timely come.

314

fiCross-lingual Annotation Projection Semantic Roles



NP SPEAKER


VP

NP SPEAKER

VP
MESSAGE

MESSAGE
Kim promised time

Kim versprach, pnktlich zu kommen

Figure 1: Bilingual projection semantic role information semantic alignments constituents.

process repeated new language pair. Instead, projection model
alleviates problem principled manner taking constituency information
account. Specifically, induce semantic alignments source target sentences
relying syntactic constituents introduce bias towards linguistically meaningful
spans. constituent aligned correctly, sufficient subset yield
correctly word-aligned. So, Figure 1, align time punktlich zu
kommen project role Message accurately, despite fact kommen
aligned other. following, describe detail semantic alignments
computed subsequently guide projection onto target language.
3.1 Framework Formalization
bi-sentence represented set linguistic units. distinguished
source (us Us ) target (ut Ut ) units words, chunks, constituents,
groupings. semantic roles source sentence modeled labeling function
: R 2Us maps roles sets source units. view projection construction
similar role labeling function target sentence, : R 2Ut . Without loss
generality, limit one frame per sentence, FrameNet.8
semantic alignment Us Ut subset Cartesian product
source target units:
Us Ut
(4)
alignment link us Us ut Ut implies us ut semantically
equivalent. Provided role assignment function source sentence, ,
projection consists simply transferring source labels r onto union target
units semantically aligned source units bearing label r:
(r) = {ut k us (r) : (us , ut ) A}

(5)

8. entails cannot take advantage potentially beneficial dependencies arguments different predicates within one sentence, shown improve semantic role
labeling (Carreras, Marquez, & Chrupala, 2004).

315

fiPado & Lapata

phrase search semantic alignment optimization problem. Specifically,
seek alignment maximizes product bilingual similarities sim
source target units:

sim(us , ut )
(6)
= argmax
AA

(us ,ut )A

several well-established methods literature computing semantic similarity within one language (see work Weeds, 2003, Budanitsky & Hirst, 2006,
overviews). Measuring semantic similarity across languages well studied
less consensus appropriate methods are. article, employ
simple method, using automatic word alignments proxy semantic equivalence;
however, similarity measures used (see discussion Section 6). Following
general convention, assume sim function ranging 0 (minimal similarity)
1 (maximal similarity).
wealth optimization methods used solve (6). article, treat
constituent alignment bipartite graph optimization problem. Bipartite graphs provide
simple intuitive modeling framework alignment problems optimization algorithms well-understood computationally moderate. importantly,
imposing constraints bipartite graph, bias model linguistically
implausible alignments, example alignments map multiple English roles onto single German constituent. Different graph topologies correspond different constraints
set admissible alignments A. instance, may want ensure source
target units aligned, restrict alignment one-to-one matches (see Section 3.3
details).
formally, weighted bipartite graph graph G = (V, E) whose node set V
partitioned two nonempty sets V1 V2 way every edge E joins
node V1 node V2 labeled weight. projection application,
two partitions sets linguistic units Us Ut , source target sentence,
respectively. assume G complete, is, source node connected
target nodes vice versa.9 Edge weights model (dis-)similarity pair
source target units.
optimization problem Equation (6) identifies alignment maximizes
product link similarities equivalent edges bipartite graph. Finding
optimal alignment amounts identifying minimum-weight subgraph (Cormen, Leiserson,
& Rivest, 1990) subgraph G0 G satisfies certain structural constraints (see
discussion below) incurring minimal edge cost:
= argmin
AA

X

weight(us , ut )

(7)

(us ,ut )A

minimization problem Equation (7) equivalent maximization problem (6)
setting weight(us , ut ) to:
weight(us , ut ) = log sim(us , ut )
9. Unwanted alignments excluded explicitly setting similarity zero.

316

(8)

fiCross-lingual Annotation Projection Semantic Roles

S4

S'4
VP2

NP3

NP'3

S1

Kim promised time

VP'2
S'1

Kim versprach, pnktlich zu kommen

(a) Bi-sentence word alignments

S1
VP2
NP3
S4

S01
0.58
0.45
0
0.37

VP02
0.45
0.68
0
0.55

NP03
0
0
1
0.18

S04
0.37
0.55
0.18
0.73

S1
VP2
NP3
S4

(b) Constituent Similarities

S01
0.54
0.80

1.00

VP02
0.80
0.39

0.60

NP03


0
1.70

S04
1.00
0.60
1.70
0.31

(c) Edge weights

Figure 2: Example bi-sentence represented edge weight matrix
example, consider Figure 2. shows bi-sentence Figure 1 representation edge weight matrix corresponding complete bipartite graph. nodes
graph (S1 S4 source side S01 S04 target side) model sentential constituents.
numbers Figure 2b similarity scores, corresponding edge weights shown
Figure 2c. High similarity scores correspond low edge weights. Edges zero similarity
set infinity (in practice, large number). Finally, notice alignments
high similarity scores (low edge weights) occur diagonal matrix.
order obtain complete projection models must (a) specify linguistic units
alignment takes place; (b) define appropriate similarity function; (c)
formulate alignment constraints. following, describe two models, one
uses words linguistic units one uses constituents. also present appropriate
similarity functions models detail alignment constraints.
3.2 Word-based Projection
first model linguistic units word tokens. Source target sentences represented sets words, Us = {ws1 , ws2 , . . . } Ut = {wt1 , wt2 , . . . }, respectively. Semantic
alignments links individual words. thus conveniently interpret
off-the-shelf word alignments semantic alignments. Formally, achieved
following binary similarity function, trivially turns word alignment optimal
semantic alignment.
(
1 ws wt word-aligned
sim(ws , wt ) =
(9)
0 else
Constraints admissible alignments often encoded word alignment models either
heuristically (e.g., enforcing one-to-one alignments Melamed, 2000) virtue
317

fiPado & Lapata

translation model used computation. example, IBM models introduced
seminal work Brown, Pietra, Pietra, Mercer (1993) require target word
aligned exactly one source word (which may empty word), therefore allow
one-to-many alignments one direction. experiments use alignments induced
publicly available GIZA++ software (Och & Ney, 2003). GIZA++ yields alignments
interfacing IBM models 14 (Brown et al., 1993) HMM extensions models 1
2 (Vogel, Ney, & Tillmann, 1996). particular configuration shown
outperform several heuristic statistical alignment models (Och & Ney, 2003). thus
take advantage alignment constraints already encoded GIZA++ assume
optimal semantic alignment given set GIZA++ links. resulting target
language labeling function is:
aw
(r) = {wt k ws (r) : ws wt GIZA++ word-aligned}

(10)

labeling function corresponds (implicit) labeling functions employed
word-based annotation projection models. models easily derived different
language pairs without recourse corpus-external resources. Unfortunately, discussed Section 2, automatically induced alignments often noisy, thus leading
projection errors. Cases point function words (e.g., prepositions) multi-word
expressions, systematically misaligned due high degree cross-lingual
variation.
3.3 Constituent-based Projection
second model linguistic units constituents. Source target sentences
thus represented constituent sets (Us = {c1s , c2s , . . . }) (Ut = {c1t , c2t , . . . }).
constituent-based similarity function capture extent cs projection ct express semantic content. approximate measuring word
alignment-based word overlap cs ct Jaccards coefficient.
Let yield(c) denote set words yield constituent c, al(c) set
words target language aligned yield c. word overlap
source constituent cs target constituent ct defined as:
o(cs , ct ) =

|al(cs ) yield(ct )|
|al(cs ) yield(ct )|

(11)

Jaccards coefficient asymmetric: consider well projection source
constituent al(cs ) matches target constituent ct , vice versa. order take
target-source source-target correspondences account, measure word overlap
directions use mean measure similarity:
sim(cs , ct ) = (o(cs , ct ) + o(ct , cs ))/2

(12)

addition similarity measure, constituent-based projection model must also specify
constraints characterize set admissible alignments A. paper,
consider three types alignment constraints affect number alignment links per
constituent (in graph-theoretic terms, degree nodes Us ). focus motivated
318

fiCross-lingual Annotation Projection Semantic Roles

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

5

e

5

5

6

e

6

6

r1

(a) Perfect matching

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

r1
r1
r2

(b) Edge cover

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

r1
r1
r2

r1
r2

(c) Total alignment

Figure 3: Constituent alignments role projections resulting three families alignment constraints (Us , Ut : source target constituents; r1 , r2 : semantic roles).

patterns observe gold standard corpus (cf. Section 2). English
German constituent, determined whether corresponded none, exactly one,
several constituents language, according gold standard word alignment.
majority constituents correspond exactly one constituent (67%), followed
substantial number one-to-many/many-to-one correspondences (32%), cases
constituents translated (i.e., corresponding node side
bi-sentence) rare (1%).
analysis indicates perfect data, expect best performance
model allows one-to-many alignments. However, common finding machine
learning restrictive models, even though appropriate data hand,
yield better results limiting hypothesis space. spirit, compare three families
admissible alignments range restrictive permissive,
evaluate other.
3.3.1 Alignments Perfect Matchings
first consider restrictive case, constituent exactly one adjacent
alignment edge. Semantic alignments property thought bijective
functions: source constituent mapped one target constituent, vice versa.
resulting bipartite graphs perfect matchings. example perfect bipartite matching
given Figure 3a. Note target side contains two nodes labeled (e), shorthand
empty node. Since bi-sentences often differ size, resulting graph partitions
different sizes well. cases, introduce empty nodes smaller
partition enable perfect matching. Empty nodes assigned similarity zero
nodes. Alignments empty nodes (such source nodes (3) (6))
ignored purposes projection; gives model possibility abstaining
aligning node good correspondence found.
Perfect matchings assume strong equivalence constituent structures
two languages; neither alignments Figure 3(b) 3(c) perfect matching.
319

fiPado & Lapata

Perfect matchings cannot model one-to-many matches, i.e., cases semantic material
expressed one constituent one language split two constituents
language. means perfect matchings appropriate source
target role annotations span exactly one constituent. always case,
perfect matchings also advantage expressive models: allowing
node one adjacent edge, introduce strong competition edges. result,
errors word alignment corrected extent.
Perfect matchings computed efficiently using algorithms network optimization (Fredman & Tarjan, 1987) time approximately cubic total number constituents bi-sentence (O(|Us |2 log |Us | + |Us |2 |Ut |)). Furthermore, perfect matchings
equivalent well-known linear assignment problem, many solution algorithms
developed (e.g., Jonker Volgenant 1987, time O(max(|Us |, |Ut |)3 )).
3.3.2 Alignments Edge Covers
next consider edge covers, generalization perfect matchings. Edge covers bipartite graphs source target constituent adjacent least one edge.
illustrated Figure 3b, source target nodes one adjacent edge (i.e., alignment link), nodes one (see source node (2) target node (4)). Edge
covers impose weaker correspondence assumptions perfect matchings, since allow one-to-many alignments constituents either direction.10 So, theory, edge
covers higher chance delivering correct role projection perfect matchings
syntactic structures source target sentences different. also
deal better situations semantic roles assigned one constituent
one languages (cf. source nodes (3) (4), labeled role r2 , example
graph). Notice perfect matchings shown Figure 3a also edge covers, whereas
graph Figure 3c not, target-side nodes (1) (3) adjacent edges.
Eiter Mannila (1997) develop algorithm computing optimal edge covers.
show minimum-weight edge covers reduced minimum weight perfect
matchings (see above) auxiliary bipartite graph two partitions size |Us | + |Ut |.
allows computation minimum weight edge covers time O((|Us | + |Ut |)3 ) =
O(max(|Us |, |Ut |)3 ), also cubic number constituents bi-sentence.
3.3.3 Total Alignments
last family admissible alignments consider total alignments. Here, source
constituent linked target constituent (i.e.,the alignment forms total function
source nodes). Total alignments impose constraints target nodes,
therefore linked arbitrary number source nodes, including none. Total
alignments permissive alignment class. contrast perfect matchings
edge covers, constituents target sentence left unaligned. Total alignments
10. general definition edge covers also allows many-to-many alignments. However, optimal edge covers
according Equation (7) cannot many-to-many, since weight edge covers many-to-many
alignments never minimal: many-to-many edge cover, one edge removed,
resulting edge cover lower weight.

320

fiCross-lingual Annotation Projection Semantic Roles

computed linking source node maximally similar target node:
= {(cs , ct ) | cs Us ct = argmax sim(cs , c0t )}

(13)

c0t Ut

Due independence source nodes, local optimization results globally optimal
alignment. time complexity procedure quadratic number constituents,
O(|Us ||Ut |).
example shown Figure 3c, source constituents (1) (2) correspond
target constituent (2), source constituents (3)(6) correspond (4). Target constituents (1) (3) aligned. quality total alignments relies heavily
underlying word alignment. Since little competition edges,
tendency form alignments mostly high (similarity) scoring target constituents.
practice, means potentially important, idiosyncratic, target constituents
low similarity scores left unaligned.
3.4 Noise Reduction
discussed Section 2, noise elimination techniques integral part projection
architectures. Although constituent-based model overly sensitive noise
expect syntactic information compensate alignment errors word-based
model error-prone since relies solely automatically obtained alignments
transferring semantic roles. introduce three filtering techniques either
correct discard erroneous projections.
3.4.1 Filling-the-gaps
According definition projection Equation (5), span projected role r
corresponds union target units aligned source units labeled r.
definition sufficient constituent-based projection models, roles rarely span
one constituent yield many wrong alignments word-based models
roles typically span several source units (i.e., words). Due errors gaps
word alignment, target span role often non-contiguous set words.
repair non-contiguous projections first last word projected
correctly applying simple heuristic fills gaps target role span. Let pos
index word token given sentence, extension set indices
set words. target span role r without gaps defined as:
acc
(r) = {u | min(pos(at (r))) pos(u) max(pos(at (r))}

(14)

apply heuristic word-based models article.
3.4.2 Word Filtering
technique removes words form bi-sentence prior projection according certain
criteria. apply two intuitive instantiations word filtering experiments.
first removes non-content words, i.e., words adjectives, adverbs, verbs,
nouns, source target sentences, since alignments non-content words
321

fiPado & Lapata



NP

VP


Kim versprach, pnktlich zu kommen

Figure 4: Argument filtering (predicate boldface, potential arguments boxes).
notoriously unreliable may negatively impact similarity computations.
second filter removes words remain unaligned output automatic word
alignment. filters aim distinguishing genuine word alignments noisy ones
speed computation semantic alignments.
3.4.3 Argument Filtering
last filter applies constituent-based models defined full parse trees. Previous
work shallow semantic parsing demonstrated nodes tree equally
probable semantic roles given predicate (Xue & Palmer, 2004). fact, assuming
perfect parse, set likely arguments, almost semantic roles
assigned. set likely arguments consists constituents child
ancestor predicate, provided (a) dominate predicate
(b) sentence boundary constituent predicate.
definition covers long-distance dependencies control constructions verbs,
support constructions nouns, extended accommodate coordination.
apply filter reduce size target tree. example Figure 4, tree
nodes removed except NP Kim punktlich zu kommen.
3.5 Discussion
presented framework bilingual projection semantic roles based
notion semantic alignment. discussed two broad instantiations
framework, namely word-based constituent-based models. latter case, operationalize search optimal semantic alignment graph optimization problem.
Specifically, bi-sentence conceptualized bipartite graph. nodes correspond
syntactic constituents bi-sentence, weighted edges cross-lingual
pairwise similarity constituents. Assumptions semantic correspondence
languages formalized constraints graph structure.
also discussed three families constraints. Perfect matching forces correspondences English German constituents bijective. contrast, total alignments assume looser correspondence leaving constituents target side unaligned.
Edge covers occupy middle ground, assuming constituents must aligned without strictly enforcing one-to-one alignments. perfect matching linguistically implausible, assuming structural divergence languages, overcome word
alignment errors. Total alignments model structural changes therefore linguis322

fiCross-lingual Annotation Projection Semantic Roles

LingUnit
words
constituents
constituents
constituents

Similarity
binary
overlap
overlap
overlap

Correspondence
one-to-one
one-to-one
one-to-at-least-one
one-to-many

BiGraph
n/a
perfect matching
edge cover
total

Complexity
linear
cubic
cubic
quadratic

Table 4: Framework instantiations
tically appropriate, time sensitive alignment errors. Finding
optimal alignment corresponds finding optimal subgraph consistent constraints. Efficient algorithms exist problem. Finally, introduced small
number filtering techniques reduce impact alignment errors.
models properties summarized Table 4. vary along following dimensions: linguistic units employed (LingUnit), similarity measure (Similarity), assumptions semantic correspondence (Correspondence) structure
bipartite graph entails (BiGraph). also mention complexity computation (Complexity). empirically assess performance following sections.

4. Experiments
describe evaluation framework developed Section 3. present two
experiments, consider projection semantic roles English sentences
onto German translations, evaluate German gold standard role annotation. Experiment 1 uses gold standard data syntactic semantic annotation.
oracle setting assesses potential role projection own, separating errors due translational divergence modeling assumptions incurred
preprocessing (e.g., parsing automatic alignment). Experiment 2 investigates
practical setting employs automatic tools syntactic semantic parsing, thus
approximating conditions large-scale role projection parallel corpora.
4.1 Setup
4.1.1 Data
models evaluated parallel corpus described Section 2. corpus
randomly shuffled split development test set (each 50% data).
Table 5 reports number tokens, sentences, frames, arguments development
test set English German.
Word alignments computed GIZA++ toolkit (Och & Ney, 2003).
used entire English-German Europarl bitext training data induce alignments
directions (source-target, target-source), default GIZA++ settings. Following
common practice Machine Translation, alignments symmetrized using intersection heuristic (Koehn, Och, & Marcu, 2003), known lead high-precision
alignments. also produced manual word alignments sentences corpus,
using GIZA++ alignments starting point following Blinker annotation
guidelines (Melamed, 1998).
323

fiPado & Lapata

Language
Dev-EN
Test-EN
Dev-DE
Test-DE

Tokens
11,585
12,019
11,229
11,548

Sentences
491
496
491
496

Frames
491
496
491
496

Roles
2,423
2,465
2,576
2,747

Table 5: Statistics gold standard parallel corpus broken development (Dev)
test (Test) set.

4.1.2 Method
implemented four models shown Table 4 filtering
techniques introduced Section 3.4. resulted total sixteen models,
evaluated development set. Results best-performing models next
validated test set. found practical runtime experiment dominated
input/output XML processing rather optimization problem itself.
experiments, constituent-based models compared word-based
model, treat baseline. latter model relatively simple: projection relies
exclusively word alignments, require syntactic analysis, linear time
complexity. thus represents good point comparison models take linguistic
knowledge account.
4.1.3 Evaluation Measure
measure model performance using labeled Precision, Recall, F1 Exact
Match condition, i.e., label span projected English role
match German gold standard role count true positive. also assess whether differences performance statistically significant using stratified shuffling (Noreen, 1989),
instance assumption-free approximate randomization testing (see Yeh, 2000, discussion).11 Whenever discuss changes F1, refer absolute (rather relative)
differences.
4.1.4 Upper Bound
annotation study (see Table 2, Section 2.2) obtained inter-annotator agreement
0.84 Span Match condition (annotation roles span).
number seen reasonable upper bound performance automatic
semantic role labeling system within language. difficult determine ceiling
projection task, since addition inter-annotator agreement, take
account effect bilingual divergence. annotation study provide estimate
former, latter. default method measuring bilingual agreement
spans, adopt monolingual Span Match agreement upper bound
projection experiments. Note, however, upper bound strict system
oracle able outperform it.
11. software available http://www.nlpado.de/~sebastian/sigf.html.

324

fiCross-lingual Annotation Projection Semantic Roles


Model
WordBL
PerfMatch
EdgeCover
Total
UpperBnd

Filter
Prec
52.0
75.8
71.7
68.9
85.0

Rec
46.2
57.1
61.8
61.3
83.0

F1
48.9
65.1
66.4
64.9
84.0

NA Filter
Model
Prec Rec
WordBL
52.0 46.2
PerfMatch 81.4 69.4
EdgeCover
77.9 69.3
Total
78.8 70.0
UpperBnd
85.0 83.0

F1
48.9
74.9
73.3
74.1
84.0

NC
Model
WordBL
PerfMatch
EdgeCover
Total
UpperBnd

Filter
Prec
37.1
79.4
75.0
69.7
85.0

Rec
32.0
62.2
63.0
60.1
83.0

F1
34.4
69.8
68.5
64.5
84.0

Arg Filter
Model
Prec Rec
WordBL
52.0 46.2
PerfMatch
88.8 56.2
EdgeCover 81.4 69.7
Total
81.2 69.6
UpperBnd
85.0 83.0

F1
48.9
68.8
75.1
75.0
84.0

Table 6: Model comparison development set using gold standard parses semantic
roles four filtering techniques: filtering (No Filter), removal non-aligned
words (NA Filter), removal non-content words (NC Filter), removal nonarguments (Arg Filter). Best performing models indicated boldface.

4.2 Experiment 1: Projection Gold Standard Data
Experiment 1, use manually annotated semantic roles hand-corrected syntactic
analyses constituent-based projection models. explained Section 4.1, first
discuss results development set. best model instantiations next evaluated
test set.
4.2.1 Development Set
Table 6 shows performance models (No Filter) combination
filtering techniques. Filter condition, word-based model (WordBL) yields
modest F1 48.9% application filling-the-gaps heuristic12 (see Section 3.4
details). condition, constituent-based models deliver F1 increase approximately 20% (all differences WordBL constituent-based models significant,
p < 0.01). EdgeCover model performs significantly better total alignments (Total,
p < 0.05) comparably perfect matchings (PerfMatch).
Filtering schemes generally improve resulting projections constituent-based
models. non-aligned words removed (NA Filter), F1 increases 9.8% PerfMatch, 6.9% EdgeCover, 9.2% Total. PerfMatch Total best performing models NA Filter. significantly outperform EdgeCover (p < 0.05)
condition constituent-based models Filter condition (p < 0.01).
word-based models performance remains constant. definition WordBL considers
aligned words only; thus, NA Filter impact performance.
12. Without filling-the-gaps, F1 drops 40.8%.

325

fiPado & Lapata

Moderate improvements observed constituent-based models non-content
words removed (NC filter). PerfMatch performs best condition. significantly
better PerfMatch, EdgeCover Total Filter condition (p < 0.01), significantly worse constituent-based models NA filter condition (p < 0.01).
NC filter yields significantly worse results WordBL (p < 0.01). surprising,
since word-based model cannot recover words deleted filter,
role-initial prepositions subordinating conjunctions.
Note also combinations different filtering techniques applied
constituent- word-based models. example, create constituent-based model
non-aligned content words removed well non-arguments. sake
brevity, present results filter combinations generally
improve results further. find combining filters tends remove large number
words, result, good alignments also removed.
Overall, obtain best performing models non-argument words removed
(Arg Filter). Arg Filter aggressive filtering technique, since removes constituents
likely occupy argument positions. EdgeCover Total significantly better
PerfMatch Arg Filter condition (p < 0.01), perform comparably PerfMatch NA Filter applied. Moreover, EdgeCover Total construct almost
identical alignments. indicates two latter models behave similarly
alignment space reduced removing many possible bad alignments, despite imposing
different constraints structure bipartite graph. Interestingly, strict correspondence constraints imposed PerfMatch result substantially different alignments.
Recall PerfMatch attempts construct bilingual one-to-one mapping arguments. direct correspondence identified source nodes, abstains
projecting. result, alignment produced PerfMatch shows highest precision
among models (88.8%), offset lowest recall (56.2%). results tie
earlier analysis constituent alignments (Section 3.3), found
one-third corpus correspondences one-to-many type. Consider
following example:
(15)

Charter means [NP opportunity bring EU closer people].
Die Charta bedeutet [NP eine Chance], [S die EU den Burgern naherzubringen].
Charter means [NP chance], [S EU citizens bring closer].

Ideally, English NP aligned German NP S. EdgeCover,
model one-to-many relationships, acts confidently aligns NP German
maximize overlap similarity, incurring precision recall error. PerfMatch,
hand, cannot handle one-to-many alignments acts cautiously makes
recall error aligning English NP empty node. Thus, according
evaluation criteria, analysis EdgeCover deemed worse PerfMatch, even
though former partly correct.
sum, filtering improves resulting projections making syntactic analyses
source target sentences similar other. Best results observed
NA Filter (PerfMatch) Arg Filter conditions (Total EdgeCover). Finally, note
best models obtain precision figures close upper bound.
326

fiCross-lingual Annotation Projection Semantic Roles

Intersective word
Model
Prec
WordBL
52.9
EdgeCover 86.6
PerfMatch
85.1
UpperBnd
85.0

alignment
Rec
F1
47.4 50.0
75.2 80.5
73.3 78.8
83.0 84.0

Manual word alignment
Model
Prec Rec
F1
WordBL
76.1 73.9 75.0
EdgeCover 86.0 81.8 83.8
PerfMatch
82.8 76.3 79.4
UpperBnd
85.0 83.0 84.0

Table 7: Model performance test set intersective manual alignments. EdgeCover uses Arg Filter PerfMatch uses NA Filter. Best performing models
indicated boldface.

best recall values around 70%, significantly upper bound 83%. Aside
wrongly assigned roles, recall errors due short semantic roles (e.g., pronouns),
intersective word alignment often contain alignment links,
projection cannot proceed.
4.2.2 Test Set
experiments test set focus models obtained best results
development set using specific filtering technique. particular, report performance
EdgeCover PerfMatch Arg Filter NA Filter conditions, respectively.
addition, assess effect automatic word alignment models using
intersective manual word alignments.
results summarized Table 7. intersective alignments used (lefthand side), EdgeCover performs numerically better PerfMatch, difference
statistically significant. corresponds findings development set.13
right-hand side shows results manually annotated word alignments used.
seen, performance WordBL increases sharply 50.0% 75.0% (F1).
underlines reliance word-based model clean word alignments. Despite
performance improvement, WordBL still lags behind best constituent-based model
approximately 9% F1. means errors made word-based model
corrected constituent-based models, mostly cases translation introduced
material target sentence cannot word-aligned expressions source
sentence recovered filling-the-gaps heuristic. example shown below,
translation clarification detailed explanation leads introduction two
German words, die naheren. words unaligned word level thus
form part role word-based projection used.
(16)

[Commissioner Barniers clarification]Role
[die naheren
Erlauterungen von Kommissar
Barnier]Role
[the detailed explanations Commissioner Barnier]Role

13. results test set slightly higher comparison development set. fluctuation
reflects natural randomness partitioning corpus.

327

fiPado & Lapata

Evaluation condition
predicates
Verbs

Prec
81.3
81.3

Rec
58.6
63.8

F1
68.1
71.5

Table 8: Evaluation Giuglea Moschittis (2004) shallow semantic parser English side parallel corpus (test set)

Constituent-based models generally profit less cleaner word alignments. performance increases 1%3% F1. improvement due higher recall (approximately
5% case EdgeCover) precision. words, main effect manually corrected word alignment make possible projection previously unavailable
roles. EdgeCover performs close human upper bound gold standard alignments
used. noise-free conditions able account one-to-many constituent alignments, thus models corpus better.
Aside alignment noise, errors observe output models
due translational divergences, problematic monolingual role assignments,
pronominal adverbs German. Many German verbs glauben (believe) exhibit
diathesis alternation: subcategorize either prepositional phrase (X glaubt Y,
X believes Y), embedded clause must preceded pronominal
adverb daran (X glaubt daran, dass Y, X believes Y). Even though pronominal
adverb forms part complement clause (and therefore also role assigned it),
English counterpart. contrast example (16) above, incomplete span dass
forms complete constituent. Unless removed Arg Filter prior alignment,
error cannot corrected use constituents.
4.3 Experiment 2: Projection Automatic Roles
experiment, evaluate projection models realistic setting, using
automatic tools syntactic semantic parsing.
4.3.1 Preprocessing
experiment, use uncorrected syntactic analyses bilingual corpus
provided Collins (1997) Dubeys (2005) parsers (cf. Section 2.1). automatically
assigned semantic roles using state-of-the-art semantic parser developed Giuglea
Moschitti (2004). trained parser FrameNet corpus (release 1.2) using
standard features extended set based PropBank would
required PropBank analysis entire FrameNet corpus.
applied shallow semantic parser English side parallel corpus
obtain semantic roles, treating frames given.14 task involves locating frame
elements sentence finding correct label particular frame element. Table 8
shows evaluation parsers output test set English gold standard
annotation. Giuglea Moschitti report accuracy 85.2% role classification
14. decomposition frame-semantic parsing general practice recent role labeling tasks,
e.g. Senseval-3 (Mihalcea & Edmonds, 2004).

328

fiCross-lingual Annotation Projection Semantic Roles

Model
WordBL
PerfMatch
EdgeCover
UpperBnd

Filter
NA Filter
Arg Filter

Prec
52.5
73.0
70.0
81.3

Rec
34.5
45.4
45.1
58.6

F1
41.6
56.0
54.9
68.1

Table 9: Performance best constituent-based model test set (automatic syntactic
semantic analysis, intersective word alignment)

task, using standard feature set.15 results strictly comparable theirs,
since identify role-bearing constituents addition assigning label.
performance thus expected worse, since inherit errors frame element
identification stage. Secondly, test set differs training data vocabulary
(affecting lexical features) suffers parsing errors. Since Giuglea Moschittis
(2004) implementation handle verbs, also assessed performance subset
verbal predicates (87.5% test tokens). difference complete verbs-only
data sets amounts 3.4% F1, represents unassigned roles nouns.
4.3.2 Setup
report results word-based baseline model, best projection models
Experiment 1, namely PerfMatch (NA filter) EdgeCover (Arg Filter). use
complete test set (including nouns adjectives) order make evaluation comparable Experiment 1.
4.3.3 Results
results summarized Table 9. PerfMatch (NA Filter) EdgeCover
(Arg Filter) perform comparably 5556% F1. approximately 25 points F1 worse
results obtained manual annotation (compare Table 9 Table 7). WordBLs
performance (now 41.6% F1) degrades less (around 8% F1), since affected
semantic role assignment errors. However, consistently Experiment 1, constituentbased models sill outperform WordBL 10% F1 (p < 0.01). results
underscore ability bracketing information, albeit noisy, correct extend word
alignment. Although Arg Filter performed well Experiment 1, observe less
effect here. Recall filter uses bracketing, also dominance information,
therefore particularly vulnerable misparses. Like Experiment 1, find
models yield overall high precision low recall. Precision drops 15% F1
automatic annotations used, whereas recall drops 30%; however, note drop
includes 5% nominal roles fall outside scope shallow semantic parser.
analysis showed parsing errors form large source problems Experiment 2. German verb phrases particularly problematic. Here, relatively free
word order combines morphological ambiguity produce ambiguous structures, since
15. See work Giuglea Moschitti (2006) updated version shallow semantic parser.

329

fiPado & Lapata

Band
Error 0
Error 1
Error 2+

Prec
85.1
75.9
40.7

Rec
74.1
34.6
18.5

F1
79.2
47.6
25.4

Table 10: PerfMatchs performance relation error rate automatic semantic role
labeling (Error 0: labeling errors, Error 1: one labeling error, Error 2+: two
labeling errors).

third person plural verb forms (FIN) identical infinitival forms (INF). Consider
following English sentence, (17a), two syntactic analyses German translation,
(17b)/(17c):
(17)

a.
b.
c.

recognize [that work issue]
wenn wir erkennenFIN , [dass wir [daran arbeitenINF ] mussenFIN ]
recognize [that [work it]]
wenn wir [erkennenINF , [dass wir daran arbeitenFIN ]] mussenFIN
[recognize [we work it]]

(17b) gives correct syntactic analysis, parser used produced highly
implausible (17c). result, English sentential complement work
issue cannot aligned single German constituent, combination them.
situation, PerfMatch generally align constituent thus sacrifice
recall. EdgeCover (and Total) produce (wrong) alignment sacrifice precision.
Finally, evaluated impact semantic role labeling errors projection. split
semantic parsers output three bands: (a) sentences role labeling errors
(Error 0, 35% test set), (b) sentences one labeling error (Error 1, 33%
test set), (c) sentences two labeling errors (Error 2+, 31% test set).
Table 10 shows performance best model, PerfMatch (NA filter),
bands. seen, output automatic labeler error-free, projection
attains F1 79.2%, comparable results obtained Experiment 1.16 Even though
projection clearly degrades quality semantic role input, PerfMatch still
delivers projections high precision Error 1 band. discussed above, low
recall values bands Error 1 2+ result labelers low recall.

5. Related Work
Previous work annotation projection primarily focused annotations spanning
short linguistic units. range POS tags (Yarowsky & Ngai, 2001), NP chunks
(Yarowsky & Ngai, 2001), dependencies (Hwa et al., 2002), word senses (Bentivogli &
Pianta, 2005). different strategy cross-lingual induction frame-semantic information presented Fung Chen (2004), require parallel corpus. Instead,
16. reasonable assume sentences, least relevant part syntactic analysis
correct.

330

fiCross-lingual Annotation Projection Semantic Roles

use bilingual dictionary construct mapping FrameNet entries concepts HowNet, on-line ontology Chinese.17 second step, use HowNet
knowledge identify monolingual Chinese sentences predicates instantiate
concepts, label arguments FrameNet roles. Fung Chen report high accuracy values, method relies existence resources presumably
unavailable languages (in particular, rich ontology). Recently, Basili et al. (2009)
propose approach semantic role projection word-based employ syntactic information. Using phrase-based SMT system, heuristically assemble
target role spans phrase translations, defining phrase similarity terms translation probability. method occupies middle ground word-based projection
constituent-based projection.
work described article relies parallel corpus harnessing information
semantic correspondences. Projection works creating semantic alignments
constituents. latter correspond nodes bipartite graph, search best
alignment cast optimization problem. view computing optimal alignments
graph matching relatively widespread machine translation literature (Melamed,
2000; Matusov, Zens, & Ney, 2004; Tiedemann, 2003; Taskar, Lacoste-Julien, & Klein, 2005).
Despite individual differences, approaches formalize word alignment minimumweight matching problem, pair words bi-sentence associated
score representing desirability pair. alignment bi-sentence
highest scoring matching constraints, example matchings must oneto-one. work applies graph matching level constituents compares larger
class constraints (see Section 3.3) previous approaches. example, Taskar et al.
(2005) examine solely perfect matchings Matusov et al. (2004) edge covers.
number studies addressed constituent alignment problem context
extracting translation patterns (Kaji, Kida, & Morimoto, 1992; Imamura, 2001). However,
approaches search pairs constituents perfectly word aligned,
infeasible strategy alignments obtained automatically. work focuses
constituent alignment problem, uses greedy search techniques guaranteed
find optimal solution (Matsumoto, Ishimoto, & Utsuro, 1993; Yamamoto & Matsumoto,
2000). Meyers, Yangarber, Grishman (1996) propose algorithm aligning parse
trees applicable isomorphic structures. Unfortunately, restriction limits
application structurally similar languages high-quality parse trees.
Although evaluate models semantic role projection task, believe
also show promise context SMT, especially systems use syntactic
information enhance translation quality. example, Xia McCord (2004) exploit
constituent alignment rearranging sentences source language make
word order similar target language. learn tree reordering rules aligning
constituents heuristically, using optimization procedure analogous total alignment
model presented article. similar approach described paper Collins, Koehn,
Kucerova (2005); however, rules manually specified constituent alignment step reduces inspection source-target sentence pairs. different alignment
17. information HowNet, see http://www.keenage.com/zhiwang/e_zhiwang.html.

331

fiPado & Lapata

models presented article could easily employed reordering task common
approaches.

6. Conclusions
article, argued parallel corpora show promise relieving lexical acquisition bottleneck new languages. proposed annotation projection means
obtaining FrameNet annotations automatically, using resources available English
exploiting parallel corpora. presented general framework projecting semantic
roles capitalizes use constituent information projection, modelled
computation constituent alignment search optimal subgraph
bipartite graph. formalization allows us solve search problem efficiently using
well-known graph optimization methods. experiments focused three modeling
aspects: level noise linguistic annotation, constraints alignments, noise
reduction techniques.
found constituent information yields substantial improvements word
alignments. Word-based models offer starting point low-density languages
parsers available. However, word alignments noisy fragmentary deliver
accurate projections annotations long spans semantic roles. experiments compared contrasted three constituent-based models differ
assumptions regarding cross-lingual correspondence (total alignments, edge covers, perfect matchings). Perfect matchings, restrictive alignment model enforces one-to-one
alignments, performed reliably across experimental conditions. particular,
precision surpassed models. indicates strong semantic correspondence
assumed modelling strategy, least English German parsing
tools available languages. side effect, performance constituent-based
models increases slightly manual word alignments used, means
near-optimal results obtained using automatic alignments.
far alignment noise reduction techniques concerned, find removing
non-aligned words (NA Filter) non-arguments (Arg Filter) yields best results.
filters independent language pair make weak assumptions
underlying linguistic representations question. choice best filter depends
goals projection. Removing non-aligned words relatively conservative tends
balance precision recall. contrast, aggressive filtering non-arguments
yields projections high precision low recall. Arguably, training shallow semantic
parsers target language (Johansson & Nugues, 2006), desirable
access high-quality projections. However, number options increasing
precision subsequent projection explored article. One fully automatic possibility generalization multiple occurrences predicate detect
remove suspicious projection instances, e.g. following work Dickinson Lee
(2008). Another direction postprocessing annotators, e.g., adopting annotate
automatically, correct manually methodology used provide high volume annotation
Penn Treebank project. models could also used semi-supervised setting,
e.g., provide training data unknown predicates.
332

fiCross-lingual Annotation Projection Semantic Roles

extensions improvements framework presented many varied.
Firstly, believe models developed article useful semantic
role paradigms besides FrameNet, indeed types semantic annotation. Potential applications include projection PropBank roles18 , discourse structure,
named entities. mentioned earlier, models also relevant machine translation
could used reordering constituents. results indicate syntactic
knowledge target side plays important role projecting annotations longer
spans. Unfortunately, many languages broad-coverage parsers available.
However, may necessary obtain complete parses semantic role projection
task. Two types syntactic information especially valuable here: bracketing information (which guides projection towards linguistically plausible role spans) knowledge
arguments sentence predicates. Bracketing information acquired
unsupervised fashion (Geertzen, 2003). Argument structure information could obtained
dependency parsers (e.g., McDonald, 2006) partial parsers able identify
predicate-argument relations (e.g., Hacioglu, 2004). Another interesting direction concerns
combination longer phrases, like provided phrase-based SMT systems,
constituent information obtained output parser chunker.
experiments presented article made use simple semantic similarity
measure based word alignment. sophisticated approach could combined
alignment scores information provided bilingual dictionary. Inspired crosslingual information retrieval, Widdows, Dorow, Chan (2002) propose bilingual vector
model. underlying assumption words similar co-occurrences
parallel corpus also semantically similar. Source target words represented
n-dimensional vectors whose components correspond frequent content words
source language. framework, similarity source-target word pair
computed using geometric measure cosine Euclidean distance. recent
polylingual topic models (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009) offer
probabilistic interpretation similar idea.
article, limited parallel sentences frame preserved. allows us transfer roles directly source onto target language
without acquire knowledge possible translational divergences first. generalization framework presented could adopt strategy form
mapping applied projection, akin transfer rules used machine translation.
Thus far, explored models applying identity mapping. Knowledge
possible mappings acquired manually annotated parallel corpora (Pado
& Erk, 2005). interesting avenue future work identify semantic role mappings
fully automatic fashion.
Acknowledgments
grateful three anonymous referees whose feedback helped improve
present article. Special thanks due Chris Callison-Burch linearb word alignment user interface, Ana-Maria Giuglea Alessandro Moschitti providing us
18. Note, however, cross-lingual projection PropBank roles raises question interpretation; see discussion Fung et al. (2007).

333

fiPado & Lapata

shallow semantic parser, annotators Beata Kouchnir Paloma Kreischer. acknowledge financial support DFG (Pado; grant Pi-154/9-2) EPSRC
(Lapata; grant GR/T04540/01).

References
Basili, R., Cao, D., Croce, D., Coppola, B., & Moschitti, A. (2009). Cross-language frame
semantics transfer bilingual corpora. Proceedings 10th International Conference Computational Linguistics Intelligent Text Processing, pp. 332345,
Mexico City, Mexico.
Bentivogli, L., & Pianta, E. (2005). Exploiting parallel texts creation multilingual semantically annotated resources: MultiSemCor corpus. Natural Language
Engineering, 11 (3), 247261.
Boas, H. C. (2005). Semantic frames interlingual representations multilingual lexical
databases. International Journal Lexicography, 18 (4), 445478.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). Mathematics
statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2006). Consistency coverage: Challenges exhaustive semantic annotation. 28. Jahrestagung,
Deutsche Gesellschaft fur Sprachwissenschaft. Bielefeld, Germany.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2009). Framenet
semantic analysis German: Annotation, representation automation.
Boas, H. (Ed.), Multilingual FrameNet. Mouton de Gruyter. appear.
Burchardt, A., & Frank, A. (2006). Approaching textual entailment LFG
FrameNet frames. Proceedings RTE-2 Workshop, Venice, Italy.
Burnard, L. (2000). Users Reference Guide British National Corpus (World
Edition). British National Corpus Consortium, Oxford University Computing Service.
Carreras, X., & Marquez, L. (2005). Introduction CoNLL-2005 shared task: Semantic
role labeling. Proceedings 9th Conference Computational Natural Language
Learning, pp. 152164, Ann Arbor, MI.
Carreras, X., Marquez, L., & Chrupala, G. (2004). Hierarchical recognition propositional
arguments perceptrons. Proceedings Eighth Conference Computational Natural Language Learning, pp. 106109, Boston, MA.
Collins, M. (1997). Three generative, lexicalised models statistical parsing. Proceedings
35th Annual Meeting Association Computational Linguistics, pp. 16
23, Madrid, Spain.
Collins, M., Koehn, P., & Kucerova, I. (2005). Clause restructuring statistical machine translation. Proceedings 43rd Annual Meeting Association
Computational Linguistics, pp. 531540, Ann Arbor, MI.
334

fiCross-lingual Annotation Projection Semantic Roles

Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.
Diab, M., & Resnik, P. (2002). unsupervised method word sense tagging using
parallel corpora. Proceedings 40th Annual Meeting Association
Computational Linguistics, pp. 255262, Philadelphia, PA.
Dickinson, M., & Lee, C. M. (2008). Detecting errors semantic annotation. Proceedings 6th International Conference Language Resources Evaluation,
Marrakech, Morocco.
Dorr, B. (1995). Machine translation divergences: formal description proposed solution. Computational Linguistics, 20 (4), 597633.
Dowty, D. (1991). Thematic proto-roles argument selection. Language, 67, 547619.
Dubey, A. (2005). lexicalization fails: parsing German suffix analysis
smoothing. Proceedings 43rd Annual Meeting Association
Computational Linguistics, pp. 314321, Ann Arbor, MI.
Eiter, T., & Mannila, H. (1997). Distance measures point sets computation..
Acta Informatica, 34 (2), 109133.
Erk, K., Kowalski, A., Pado, S., & Pinkal, M. (2003). Towards resource lexical semantics: large German corpus extensive semantic annotation. Proceedings
41st Annual Meeting Association Computational Linguistics, pp. 537544,
Sapporo, Japan.
Fillmore, C. J. (1968). case case. Bach, & Harms (Eds.), Universals Linguistic
Theory, pp. 188. Holt, Rinehart, Winston, New York.
Fillmore, C. J. (1982). Frame semantics. Linguistics Morning Calm, pp. 111137.
Hanshin, Seoul, Korea.
Fillmore, C. J., Johnson, C. R., & Petruck, M. R. (2003). Background FrameNet. International Journal Lexicography, 16, 235250.
Fleischman, M., & Hovy, E. (2003). Maximum entropy models FrameNet classification. Proceedings 8th Conference Empirical Methods Natural Language
Processing, pp. 4956, Sapporo, Japan.
Frank, A., Krieger, H.-U., Xu, F., Uszkoreit, H., Crysmann, B., Jorg, B., & Schafer, U.
(2007). Question answering structured knowledge sources. Journal Applied
Logic, 5 (1), 2048.
Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps uses improved network
optimization algorithms. Journal ACM, 34 (3), 596615.
Fung, P., & Chen, B. (2004). BiFrameNet: Bilingual frame semantics resources construction
cross-lingual induction. Proceedings 20th International Conference
Computational Linguistics, pp. 931935, Geneva, Switzerland.
Fung, P., Wu, Z., Yang, Y., & Wu, D. (2006). Automatic learning Chinese-English
semantic structure mapping. Proceedings IEEE/ACL Workshop Spoken
Language Technology, Aruba.
335

fiPado & Lapata

Fung, P., Wu, Z., Yang, Y., & Wu, D. (2007). Learning bilingual semantic frames: Shallow
semantic parsing vs. semantic role projection. Proceedings 11th Conference
Theoretical Methodological Issues Machine Translation, pp. 7584, Skovde,
Sweden.
Geertzen, J. (2003). String alignment grammatical inference: suffix trees do.
Masters thesis, ILK, Tilburg University, Tilburg, Netherlands.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Computational
Linguistics, 28 (3), 245288.
Gimenez, J., & Marquez, L. (2007). Linguistic features automatic evaluation heterogenous MT systems. Proceedings Second Workshop Statistical Machine
Translation, pp. 256264, Prague, Czech Republic.
Giuglea, A.-M., & Moschitti, A. (2004). Knowledge discovery using FrameNet, VerbNet
PropBank. Proceedings Workshop Ontology Knowledge Discovering
15th European Conference Machine Learning, Pisa, Italy.
Giuglea, A.-M., & Moschitti, A. (2006). Semantic role labeling via FrameNet, VerbNet
PropBank. Proceedings 44th Annual Meeting Association
Computational Linguistics, pp. 929936, Sydney, Australia.
Grenager, T., & Manning, C. (2006). Unsupervised discovery statistical verb lexicon.
Proceedings 11th Conference Empirical Methods Natural Language
Processing, pp. 18, Sydney, Australia.
Hacioglu, K. (2004). lightweight semantic chunker based tagging. Proceedings
joint Human Language Technology Conference Annual Meeting North
American Chapter Association Computational Linguistics, pp. 145148,
Boston, MA.
Hi, C., & Hwa, R. (2005). backoff model bootstrapping resources non-english
languages. Proceedings joint Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 851858, Vancouver, BC.
Hwa, R., Resnik, P., Weinberg, A., & Kolak, O. (2002). Evaluation translational correspondance using annotation projection. Proceedings 40th Annual Meeting
Association Computational Linguistics, pp. 392399, Philadelphia, PA.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers
via syntactic projection across parallel texts. Journal Natural Language Engineering, 11 (3), 311325.
Imamura, K. (2001). Hierarchical phrase alignment harmonized parsing.. Proceedings 6th Natural Language Processing Pacific Rim Symposium, pp. 377384,
Tokyo, Japan.
Jackendoff, R. S. (1990). Semantic Structures. MIT Press, Cambridge, MA.
Johansson, R., & Nugues, P. (2006). FrameNet-Based Semantic Role Labeler Swedish.
Proceedings 44th Annual Meeting Association Computational Linguistics, pp. 436443, Sydney, Australia.
336

fiCross-lingual Annotation Projection Semantic Roles

Jonker, R., & Volgenant, A. (1987). shortest augmenting path algorithm dense
sparse linear assignment problems. Computing, 38, 325340.
Kaji, H., Kida, Y., & Morimoto, Y. (1992). Learning translation templates bilingual
text. Proceedings 14th International Conference Computational Linguistics, pp. 672678, Nantes, France.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings joint Human Language Technology Conference Annual Meeting
North American Chapter Association Computational Linguistics, pp.
4854, Edmonton, AL.
Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. Proceedings MT Summit X, Phuket, Thailand.
Marquez, L., Carreras, X., Litkowski, K. C., & Stevenson, S. (2008). Semantic role labeling:
introduction special issue. Computational Linguistics, 34 (2), 145159.
Matsumoto, Y., Ishimoto, H., & Utsuro, T. (1993). Structural matching parallel texts.
Proceedings ACL 31st Annual Meeting Association Computational
Linguistics, pp. 2330, Columbus, OH.
Matusov, E., Zens, R., & Ney, H. (2004). Symmetric word alignments statistical maching
translation. Proceedings 20th International Conference Computational
Linguistics, pp. 219225, Geneva, Switzerland.
McDonald, R. (2006). Discriminative Training Spanning Tree Algorithms Dependency Parsing. Ph.D. thesis, University Pennsylvania.
Melamed, I. D. (1998). Manual annotation translational equivalence: Blinker project.
Tech. rep. IRCS TR #98-07, IRCS, University Pennsylvania.
Melamed, I. D. (2000). Models translational equivalence among words. Computational
Linguistics, 2 (23), 221249.
Meyers, A., Yangarber, R., & Grishman, R. (1996). Alignment shared forests bilingual corpora. Proceedings 16th International Conference Computational
Linguistics, pp. 460465, Copenhagen, Denmark.
Mihalcea, R., & Edmonds, P. (Eds.). (2004). Proceedings Senseval-3: 3rd International Workshop Evaluation Systems Semantic Analysis Text,
Barcelona, Spain.
Miltsakaki, E., Prasad, R., Joshi, A., & Webber, B. (2004). Annotating discourse connectives
arguments. Proceedings NAACL/HLT Workshop Frontiers
Corpus Annotation, Boston, MA.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual topic models. Proceedings 14th Conference Empirical Methods
Natural Language Processing, pp. 880889, Singapore.
Moschitti, A. (2008). Kernel methods, syntax semantics relational text categorization. Proceedings 17th ACM Conference Information Knowledge
Management, pp. 253262, Napa Valley, CA.
337

fiPado & Lapata

Moschitti, A., Quarteroni, S., Basili, R., & Manandhar, S. (2007). Exploiting syntactic
shallow semantic kernels question answer classification. Proceedings
45th Annual Meeting Association Computational Linguistics, pp. 776783,
Prague, Czech Republic.
Narayanan, S., & Harabagiu, S. (2004). Question answering based semantic structures.
Proceedings 20th International Conference Computational Linguistics, pp.
693701, Geneva, Switzerland.
Noreen, E. (1989). Computer-intensive Methods Testing Hypotheses: Introduction.
John Wiley Sons Inc.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment
models. Computational Linguistics, 29 (1), 1952.
Ohara, K. H., Fujii, S., Saito, H., Ishizaki, S., Ohori, T., & Suzuki, R. (2003). Japanese
FrameNet project: preliminary report. Proceedings 6th Meeting
Pacific Association Computational Linguistics, pp. 249254, Halifax, Nova Scotia.
Pado, S., & Erk, K. (2005). cause cause: Cross-lingual semantic matching
paraphrase modelling. Proceedings EUROLAN Workshop Cross-Linguistic
Knowledge Induction, Cluj-Napoca, Romania.
Pado, S., & Lapata, M. (2005). Cross-lingual bootstrapping semantic lexicons.
Proceedings 22nd National Conference Artificial Intelligence, pp. 10871092,
Pittsburgh, PA.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: annotated
corpus semantic roles. Computational Linguistics, 31 (1), 71106.
Postolache, O., Cristea, D., & Orasan, C. (2006). Tranferring coreference chains
word alignment. Proceedings 5th International Conference Language
Resources Evaluation, Genoa, Italy.
Riloff, E., Schafer, C., & Yarowsky, D. (2002). Inducing information extraction systems
new languages via cross-language projection. Proceedings 19th International
Conference Computational Linguistics, pp. 828834, Taipei, Taiwan.
Shen, D., & Lapata, M. (2007). Using semantic roles improve question answering.
Proceedings 12th Conference Empirical Methods Natural Language Processing, pp. 1221, Prague, Czech Republic.
Spreyer, K., & Frank, A. (2008). Projection-based acquisition temporal labeller.
Proceedings 3rd International Joint Conference Natural Language Processing,
pp. 489496, Hyderabad, India.
Subirats, C., & Petruck, M. (2003). Surprise: Spanish FrameNet. Proceedings
Workshop Frame Semantics, XVII. International Congress Linguists, Prague,
Czech Republic.
Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using predicate-argument
structures information extraction. Proceedings 41st Annual Meeting
Association Computational Linguistics, pp. 815, Sapporo, Japan.
338

fiCross-lingual Annotation Projection Semantic Roles

Swier, R. S., & Stevenson, S. (2004). Unsupervised semantic role labelling. Proceedings
Conference Empirical Methods Natural Language Processing, pp. 95102.
Bacelona, Spain.
Swier, R. S., & Stevenson, S. (2005). Exploiting verb lexicon automatic semantic
role labelling. Proceedings Joint Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 883890,
Vancouver, British Columbia.
Taskar, B., Lacoste-Julien, S., & Klein, D. (2005). discriminative matching approach
word alignment. Proceedings joint Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 7380,
Vancouver, BC.
Tatu, M., & Moldovan, D. (2005). semantic approach recognizing textual entailment.
Proceedings joint Human Language Technology Conference Conference
Empirical Methods Natural Language Processing, pp. 371378, Vancouver, BC.
Taule, M., Mart, M., & Recasens, M. (2008). Ancora: Multilevel annotated corpora
Catalan Spanish. Proceedings 6th International Conference Language
Resources Evaluation, Marrakesh, Morocco.
Tiedemann, J. (2003). Combining clues word alignment. Proceedings 16th
Meeting European Chapter Association Computational Linguistics,
pp. 339346, Budapest, Hungary.
Tokarczyk, A., & Frank, A. (2009). Cross-lingual projection LFG f-structures: Resource
induction Polish. Proceedings Lexical Functional Grammar 2009, Cambridge,
UK.
van Leuven-Zwart, K. M. (1989). Translation original: Similarities dissimilarities.
Target, 1 (2), 151181.
Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment statistical translation. Proceedings 16th International Conference Computational Linguistics, pp. 836841, Copenhagen.
Weeds, J. (2003). Measures Applications Lexical Distributional Similarity. Ph.D.
thesis, University Sussex.
Widdows, D., Dorow, B., & Chan, C.-K. (2002). Using parallel corpora enrich multilingual
lexical resources. Proceedings 3rd International Conference Language
Resources Evaluation, pp. 240245, Las Palmas, Canary Islands.
Wu, D., & Fung, P. (2009a). semantic role labeling improve SMT?. Proceedings
13th Annual Conference European Association Machine Translation,
pp. 218225, Barcelona, Spain.
Wu, D., & Fung, P. (2009b). Semantic roles SMT: hybrid two-pass model. Proceedings joint Human Language Technology Conference Annual Meeting
North American Chapter Association Computational Linguistics, pp.
1316, Boulder, CO.
339

fiPado & Lapata

Xia, F., & McCord, M. (2004). Improving statistical MT system automatically
learned rewrite patterns. Proceedings 20th International Conference
Computational Linguistics, pp. 508514, Geneva, Switzerland.
Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Proceedings
9th Conference Empirical Methods Natural Language Processing, pp. 88
94, Barcelona, Spain.
Xue, N., & Palmer, M. (2009). Adding semantic roles Chinese treebank. Natural
Language Engineering, 15 (1), 143172.
Yamamoto, K., & Matsumoto, Y. (2000). Acquisition phrase-level bilingual correspondence using dependency structure. Proceedings 18th International Conference
Computational Linguistics, pp. 933939, Saarbrucken, Germany.
Yarowsky, D., & Ngai, G. (2001). Inducing multilingual POS taggers NP bracketers via
robust projection across aligned corpora. Proceedings 2nd Annual Meeting
North American Chapter Association Computational Linguistics, pp.
200207, Pittsburgh, PA.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis
tools via robust projection across aligned corpora. Proceedings 1st Human
Language Technology Conference, pp. 161168, San Francisco, CA.
Yeh, A. (2000). accurate tests statistical significance result differences.
Proceedings 18th International Conference Computational Linguistics, pp.
947953, Saarbrucken, Germany.

340

fiJournal Artificial Intelligence Research 36 (2009) 547-556

Submitted 06/09; published 12/09

Research Note
Soft Goals Compiled Away
Emil Keyder

emil.keyder@upf.edu

Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona Spain

Hector Geffner

hector.geffner@upf.edu

ICREA & Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona Spain

Abstract
Soft goals extend classical model planning simple model preferences.
best plans ones least cost ones maximum utility,
utility plan sum utilities soft goals achieved minus
plan cost. Finding plans high utility appears involve two linked problems:
choosing subset soft goals achieve finding low-cost plan achieve them. New
search algorithms heuristics developed planning soft goals,
new track introduced International Planning Competition (IPC) test
performance. note, show however extensions needed:
soft goals increase expressive power basic model planning action
costs, easily compiled away. apply compilation problems
net-benefit track recent IPC, show optimal satisficing
cost-based planners better compiled problems optimal satisficing netbenefit planners original problems explicit soft goals. Furthermore, show
penalties, negative preferences expressing conditions avoid, also compiled
away using similar idea.

1. Models
STRIPS problem tuple P = hF, I, O, Gi F set fluents, F
G F initial state goal situation, set actions operators
precondition, add, delete lists P re(a), Add(a), Del(a) respectively,
subsets F . action sequence = ha0 , . . . , applicable P actions ai ,
= 0, . . . , n, O, exists sequence states hs0 , . . . , sn+1 i,
s0 = I, P re(ai ) si si+1 = si Add(ai ) \ Del(ai ) = 0, . . . , n. applicable
action sequence achieves fluent g g sn+1 , plan P achieves goal
g G, write |= G. classical setting, cost plan c() given
||, number actions . cost structure generalized addition
cost function operators:
c
2009
AI Access Foundation. rights reserved.

fiKeyder & Geffner

Definition 1 STRIPS problem action costs tuple Pc = hF, I, O, G, ci,
+
P = hF, I, O, Gi STRIPS problem c function c :
7 R+
0 R0 stands
non-negative reals.
cost plan problem Pc given
||
X

c() =

c(ai )

(1)

i=1

ai denotes th action . cost function c() = || obtained special
case c(o) = 1 O. Adding utilities soft goals problem formulation
results new model:
Definition 2 STRIPS problem action costs soft goals tuple Pu = hF, I, O,
G, c, ui, P = hF, I, O, G, ci STRIPS problem action costs, u partial
function u : F 7 R+ maps subset fluents (the soft goals) positive reals.
STRIPS problem soft goals Pu , utility plan given difference
total utility obtained plan cost:
u() =

X

u(p) c() .

(2)

p:|=p

plan problem soft goals Pu optimal plan 0 utility
u( 0 ) higher u(). utility optimal plan problem hard goals
never negative, empty plan non-negative utility zero cost.
recent International Planning Competition (IPC6) featured Sequential Optimal Net Benefit Optimal tracks objective find optimal plans
respect models captured Equation 1 Equation 2 respectively (Helmert, Do,
& Refanidis, 2008).1

2. Equivalence
Given problem P soft goals, equivalent problem P 0 action costs soft
goals defined whose plans encode corresponding plans P . transformation,
first introduced Keyder Geffner (2007), simple direct, yet seems escaped attention researchers area (Smith, 2004; Sanchez & Kambhampati, 2005;
Bonet & Geffner, 2008; Baier, Bacchus, & McIlraith, 2007). Also, unlike compilation
soft goals numeric variables arbitrary plan metrics (Edelkamp, 2006), proposed
transformation makes use neither requires planners ability handle
1. PDDL3, soft goals represented expressions form ( u (is-violated hprefi)) appearing
problem metric pref preference soft goal associated formula A.
single fluent, expression corresponds u(A) = u terminology used here.
competition benchmarks contain preferences form. general case arises
compound formula fluents considered Section 4.

548

fiSoft Goals Compiled Away

action costs, basic functionality required satisficing track recent IPC
(Helmert et al., 2008).2
write actions tuples form = hPre(o), Eff(o)i, effects
positive (Adds) negative (Deletes). assume soft goal fluent p, P also
contains fluent p representing negation. introduced standard way,
adding p initial state p initially true, including p Add Delete
lists actions deleting adding p respectively (Gazen & Knoblock, 1997; Nebel, 2000).
problem P 0 action costs soft goals equivalent problem P
soft goals obtained following transformation:
Definition 3 STRIPS problem action costs soft goals P = hF, I, O, G, c, ui,
compiled STRIPS problem action costs P 0 = hF 0 , 0 , O0 , G0 , c0
F 0 = F 0 (P ) S(P ) {normal-mode, end-mode}
0 = S(P ) {normal-mode}
G0 = G 0 (P )
O0 = O00 {collect(p), f orgo(p) | p SG(P )} {end}

c(o) O00
0
u(p) = forgo(p)
c (o) =

0
= collect(p) = end

SG(P ) = {p | (p F ) (u(p) > 0)}
0 (P ) = {p0 | p SG(P )}
S(P ) = {p0 | p0 0 (P )}
end = h{normal-mode}, {end-mode, normal-mode}i
collect(p) = h{end-mode, p, p0 }, {p0 , p0 }i
forgo(p) = h{end-mode, p, p0 }, {p0 , p0 }i
O00 = {hPre(o) {normal-mode}, Eff(o)i | O}
2. Edelkamps transformation associates soft goals p1 , . . . , pm numeric
variables n1 , . . . , nm ,
P
domain {0, 1}. utility plan expressed U () = n
i=1 ni u(pi ) cost(),
u(pi ) represents utility associated soft goal pi ni represents value numeric variable
final state achieved plan. transformation also eliminates soft goals, requires
place plan metric whose terms (namely, whether variables u(pi ) 1 0) state-dependent.
Current heuristics deal metrics (See Sections 3 4).

549

fiKeyder & Geffner

soft goal p P , transformation adds dummy hard goal p0 P 0
achieved two ways: action collect(p) cost 0 requires p
true, action forgo(p) cost equal utility p yet performed
p false, equivalently p true. two actions used
end action makes fluent end-mode true, actions original
problem P used fluent normal-mode true prior execution
end action. Moreover, exactly one {collect(p), forgo(p)} appear soft goal
p plan, delete shared precondition p0 , action makes true.
way make normal-mode true deleted end action,
plans 0 P 0 form 0 = h, end, 00 i, plan P 00 sequence
|S 0 (P )| collect(p) forgo(p) actions order, former appearing |= p,
latter otherwise.
two problems P P 0 equivalent sense correspondence
plans P P 0 , corresponding plans ranked way.
specifically, plan P , plan 0 P 0 extends end action
set collect forgo actions, plan cost c( 0 ) = u() + ,
constant independent 0 . Finding optimal (maximum utility)
plan P therefore equivalent finding optimal (minimum cost) plan 0 P 0 .
Proposition 1 (Correspondence plans) applicable action sequence
P , let extension 0 denote sequence obtained appending end action
followed permutation actions collect(p) forgo(p) p SG(P ),
|= p 6|= p respectively.
plan P 0 plan P 0
Proof: () new actions P 0 delete p F , hard goal achieved
remain true final state reached 0 , 0 |= G. p F
u(p) > 0, either |= p 6|= p. first case, p0 achieved collect(p),
second, forgo(p), therefore 0 |= 0 (P ). Since G0 = G 0 (P ), 0 |= G0 .
() 0 plan P 0 , hard goals G P must made true 0
end action, action collect forgo actions applied
make p F true. plan obtained removing end action collect
forgo actions must therefore achieve G thus valid plan P .
2
Proposition 2 (Correspondence utilities costs) Let 1 2 two
plans P , let 10 20 extensions 1 2 respectively. Then,
u(1 ) > u(2 ) c(10 ) < c(20 )

0
0
Proof: Let
P plan P extension . demonstrate c( ) =
u() + pSG(P ) u(p). Since summation expression constant given
problem P , assertion follows directly:

550

fiSoft Goals Compiled Away

X

c( 0 ) = c() + c0 (end) +

c0 (forgo(p)) +

forgo(p) 0

X

= c() +

X

c0 (collect(p))

collect(p) 0

c0 (forgo(p))

forgo(p) 0

= c() +

X

u(p)

p:6|=p

X

= c() +

u(p)

X

u(p)

p:|=p

pSG(P )

= u() +

X

u(p)

pSG(P )
2

Proposition 3 (Equivalence) Let plan P , 0 plan P 0 extends
. Then,
optimal plan P 0 optimal plan P 0

Proof: Direct two propositions above.

2

following section, empirically compare performance net-benefit planners
problems P explicit soft goals sequential planners problems P 0
soft goals compiled away. order improve latter, make
transformation Definition 3 effective simple trick. Recall single
plan P , many extensions 0 P 0 , containing actions
cost, differing way collect forgo actions ordered.
efficiency purposes, implementation enforces fixed arbitrary ordering p1 , . . . , pm
soft goals P adding dummy hard goal p0i precondition actions
collect(pi+1 ) forgo(pi+1 ) = 1, . . . , 1. result single possible
extension 0 every plan P , space plans search therefore reduced.
optimization used experiments reported below.

3. Experimental Results
formal results imply best plans problem P action costs
soft goals computed looking best plans compiled problem P 0
action costs soft goals, standard classical planning techniques
applied. test practical value transformation, evaluate performance
optimal satisficing planning techniques soft goals. problems test
suite contain preferences conjunctions rather single fluents. preferences
handled variant approach described above, detailed Section 4.
results shown three columns Table 1 labelled Net-benefit optimal planners
results reported organizers 2008 International Planning Competition
(IPC6) (Helmert et al., 2008). results obtained using machines
551

fiKeyder & Geffner

Domain
crewplanning(30)
elevators (30)
openstacks (30)
pegsol (30)
transport (30)
woodworking (30)
total

Net-benefit optimal planners
Gamer HSP*P Mips-XXL
4
16
8
11
5
4
7
5
2
24
0
23
12
12
9
13
11
9
71
49
55

Sequential optimal planners
Gamer HSP*F HSP*0 Mips-XXL
8
21
8
19
8
8
3
6
4
6
1
22
26
14
22
15
15
9
10
14
7
71
78
50

Table 1: Coverage optimal planners: leftmost three columns give number problems
solved planners Net Benefit Optimal track IPC6, reported
competition organizers. rightmost four columns give number compiled
problems solved Sequential Optimal versions planners. Dashes indicate
version planner could run domain.

settings used competition: Xeon Woodcrest computers clock speeds 2.33
GHz, time limit 30 minutes memory limit 2GB.
first set experiments, consider problems used Net Benefit Optimal
(NBO) track IPC6, soft goals defined terms goal-state preferences
(Gerevini & Long, 2006), compare results obtained three optimal netbenefit planners results obtained Sequential Optimal (SO) variants
compilations.3 three planners entered NBO track IPC6 Gamer,
Mips-XXL, HSP*P . planners test compiled versions NBO
problems versions Gamer (Edelkamp & Kissmann, 2008) Mips-XXL
(Edelkamp & Jabbar, 2008) two planners HSP*F HSP*0 (Haslum, 2008).4
ranked first, fifth, second, third, respectively, track (Helmert
et al., 2008). Three six domains NBO track IPC6 involve numeric
variables appear preconditions actions. version Gamer
handle numeric variables, therefore unable run Gamer problems.
Numeric variables never appear soft goals left untouched compilation.
data Table 1 show two HSP* planners track run
compiled problems well as, better than, best planner NBO track run
original problems soft goals. maximum number solved problems
domain higher NBO track planners single domain, openstacks (7 vs. 6).
domains, planners able solve larger number problems
3. compiled problems currently available http://ipc.informatik.uni-freiburg.de/Domains.
4. versions HSP* bug may cause suboptimal invalid solutions computed
domains non-monotonic numeric variables (numeric variables whose values may increase
decrease) occur preconditions actions goals (See http://ipc.informatik.uni-freiburg.
de/Planners). variables present transport domain tested, yet plans
computed HSP* versions domain turn valid (as verified VAL plan
validator, Howey & Long, 2003) optimal instances checked
costs plans computed planners.

552

fiSoft Goals Compiled Away

Domain
elevators (30)
openstacks (30)
pegsol (30)
rovers (20)
total

Net-benefit satisficing planners
SGPlan YochanPS Mips-XXL
0
0
8
2
0
2
0
5
23
8
2
1
10
7
34

Cost satisficing planners
Lama
23
28
29
17
97

Table 2: Coverage quality satisficing planners: entries indicate number problems
planner generated best quality plan.

maximum number solved NBO planner. Considering performance NBO
variants planner, compilation benefits two versions
heuristic search planner HSP* , leaving BDD planners Gamer Mips-XXL relatively
unaffected. Interestingly, HSP*0 using compilation ends solving problems
Gamer, winner NBO track (78 vs. 71). drastically better performance
versions HSP* compared net-benefit version result simple scheme
handling soft goals latter, optimal plans computed possible
subset soft goals problem (roughly), change search algorithm
IDA* A*.
second set experiments, consider three domains NBO track
IPC6 contain numeric variables preconditions actions,
domain rovers net-benefit track IPC5. Domains containing numeric variables
preconditions actions considered due lack state-of-the-art cost-based
planners able handle them. Domains rovers NB track IPC5
considered contain disjunctive, existentially qualified, universally qualified
soft goals current implementation support. satisficing net-benefit
planners test problems SGPlan (Hsu & Wah, 2008), winner net
benefit track IPC5, YochanPS (Benton, Do, & Kambhampati, 2009), received
distinguished performance award competition, satisficing variant
MIPS-XXL, also received distinguished performance award competition
competed optimal track IPC6. solve compiled versions problems
LAMA, winner sequential satisficing track IPC6. YochanPS, MIPS-XXL,
LAMA anytime planners, results discussed refer cost
best plan found end evaluation period 30 minutes.
Entries Table 2 show number problems domain plan
generated planner best plan produced. report data rather
showing graphs plan utilities absolute difference quality plans
meaningful except shortest plans (that ignore costs and/or soft goals)
problem significantly costly. results show running state-of-the-art
cost-based planner compiled problems yields best plan 98 total
110 instances, almost three times number instances best-performing
native soft goals planner, MIPS-XXL, gives best plan. Furthermore, 22 23
553

fiKeyder & Geffner

problems MIPS-XXL finds best plan pegsol domain, LAMA finds plan
quality. problems satisficing net-benefit planners outperform
LAMA run compiled problems therefore few.
results appear contradict results reported Benton et al. (2009),
native net-benefit planner, YochanPS , yields better results cost-based planner,
YochanCOST , run problems compiled according earlier version transformation
(Keyder & Geffner, 2007). discrepancy appears result non-informative
cost-based heuristic used YochanCOST , leads plans forgo soft goals,
fact make use optimization discussed end Section 2,
results unnecessary blowup state space. analysis differences
recent cost-based planners, see paper Keyder Geffner (2008).

4. Extensions
shown possible compile away positive utilities u(p) associated
single fluents p. show compilation extended deal positive
utilities defined formulas fluents negative utilities defined single
fluents formulas. Negative utilities stand conditions avoided rather
sought; example, utility u(p q) = 10 penalizes plan results state
p q true extra cost 10. compilation soft goals defined
formulas based standard compilation goal precondition formulas classical
planning (Gazen & Knoblock, 1997; Nebel, 1999).
positive utility logical formula compiled away introducing new
fluent pA achieved zero cost end state holds,
assigning utility associated pA . DNF formula D1 . . . Dn ,
suffices add n new actions a1 , . . . , ai = hDi , pA = 1, . . . , n.
CNF formula C1 . . . Cn , fluent pi introduced = 1, . . . , n, along
actions aij = hCij , pi j = 1, . . . , |Ci |, Cij stands jth fluent Ci . also
introduce action = h{p1 , . . . , pn }, pA allows addition fluent pA states
holds. newly introduced actions zero cost, must applicable
P 0 actions original problem P collect forgo actions.
best extensions plan achieves P achieve pA use collect
action achieve hard goal fluent p0A associated pA zero cost.
negative utility u(A) < 0 formula DNF CNF compiled away
two steps, first substituting positive utility u(A) negation
compiling positive utility formula utility single fluent described
above. makes use fact negation formula CNF formula
DNF vice versa.

5. Summary
shown soft goals add expressive power easily compiled
away. implies new search algorithms heuristics strictly required
handling them. practical standpoint, experiments indicate state-of-the-art
sequential planners outperform state-of-the-art net-benefit planners compiled versions
554

fiSoft Goals Compiled Away

benchmarks used recent planning competitions. Furthermore, similar transformations
used compile away positive negative utilities logical formulas DNF
CNF.

Acknowledgments
thank Malte Helmert help compiling running many IPC6 planners,
Patrik Haslum help aspects various versions HSP, J. Benton
help compiling running YochanPS . H. Geffner partially supported Grant
TIN2006-15387-C03-03 MEC, Spain.

References
Baier, J. A., Bacchus, F., & McIlraith, S. A. (2007). heuristic search approach planning
temporally extended preferences. Proc. IJCAI-07, pp. 18081815.
Benton, J., Do, M., & Kambhampati, S. (2009). Anytime heuristic search partial satisfaction planning. Artificial Intelligence, 173 (5-6), 562592.
Bonet, B., & Geffner, H. (2008). Heuristics planning penalties rewards formulated logic computed circuits. Artificial Intelligence, 172 (12-13),
15791604.
Edelkamp, S. (2006). compilation plan constraints preferences. Proc.
ICAPS-06, pp. 374377.
Edelkamp, S., & Jabbar, S. (2008). MIPS-XXL: Featuring external shortest path search
sequential optimal plans external branch-and-bound optimal net benefit.
6th. Int. Planning Competition Booklet (ICAPS-08).
Edelkamp, S., & Kissmann, P. (2008). Gamer: Bridging planning general game playing
symbolic search. 6th. Int. Planning Competition Booklet (ICAPS-08).
Gazen, B., & Knoblock, C. (1997). Combining expressiveness UCPOP
efficiency Graphplan. Steel, S., & Alami, R. (Eds.), Proc. 4th European Conf.
Planning, pp. 221233. Springer.
Gerevini, A., & Long, D. (2006). Preferences soft constraints PDDL3. Proc.
ICAPS-06 Workshop Preferences Soft Constraints Planning, pp. 4653.
Haslum, P. (2008). Additive reversed relaxed reachability heuristics revisited. 6th.
Int. Planning Competition Booklet (ICAPS-08).
Helmert, M., Do, M., & Refanidis, I. (2008). IPC 2008 deterministic competition. 6th.
Int. Planning Competition Booklet (ICAPS-08).
Howey, R., & Long, D. (2003). VALs progress: automatic validation tool PDDL2.1
used international planning competition. Proc. 2003 ICAPS Workshop
Competition: Impact, Organization, Evaluation, Benchmarks.
Hsu, C.-W., & Wah, B. W. (2008). SGPlan planning system IPC6. 6th. Int.
Planning Competition Booklet (ICAPS-08).
555

fiKeyder & Geffner

Keyder, E., & Geffner, H. (2007). Set-additive TSP heuristics planning action costs soft goals. Proc. ICAPS-06 Workshop Heuristics DomainIndependent Planning.
Keyder, E., & Geffner, H. (2008). Heuristics planning action costs revisited.
Proc. 18th European Conference Artificial Intelligence, pp. 588592.
Nebel, B. (1999). Compilation schemes: theoretical tool assessing expressive
power planning formalisms. Proc. KI-99: Advances Artificial Intelligence, pp.
183194. Springer-Verlag.
Nebel, B. (2000). compilability expressive power propositional planning.
Journal Artificial Intelligence Research, 12, 271315.
Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics selecting objectives
over-subscription planning problems. Proc. ICAPS-05, pp. 192201.
Smith, D. E. (2004). Choosing objectives over-subscription planning. Proc. ICAPS-04,
pp. 393401.

556

fiJournal Artificial Intelligence Research 36 (2009) 169

Submitted 04/09; published 10/09

DL-Lite Family Relations
Alessandro Artale
Diego Calvanese

artale@inf.unibz.it
calvanese@inf.unibz.it

KRDB Research Centre
Free University Bozen-Bolzano
Piazza Domenicani, 3 I-39100 Bolzano, Italy

Roman Kontchakov
Michael Zakharyaschev

roman@dcs.bbk.ac.uk
michael@dcs.bbk.ac.uk

Department Computer Science Information Systems
Birkbeck College
Malet Street, London WC1E 7HX, U.K.

Abstract
recently introduced series description logics common moniker DLLite attracted attention description logic semantic web communities due
low computational complexity inference, one hand, ability represent
conceptual modeling formalisms, other. main aim article carry
thorough systematic investigation inference extensions original DL-Lite
logics along five axes: (i) adding Boolean connectives (ii) number restrictions
concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry,
asymmetry, reflexivity, irreflexivity transitivity constraints, (v) adopting dropping unique name assumption. analyze combined complexity satisfiability
resulting logics, well data complexity instance checking answering
positive existential queries. approach based embedding DL-Lite logics suitable fragments one-variable first-order logic, provides useful insights
properties and, particular, computational behavior.

1. Introduction
Description Logic (cf. Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003
references therein) family knowledge representation formalisms developed
past three decades and, recent years, widely used various application areas as:
conceptual modeling (Bergamaschi & Sartori, 1992; Calvanese et al., 1998b, 1999;
McGuinness & Wright, 1998; Franconi & Ng, 2000; Borgida & Brachman, 2003; Berardi, Calvanese, & De Giacomo, 2005; Artale et al., 1996, 2007, 2007b),
information data integration (Beeri, Levy, & Rousset, 1997; Levy & Rousset,
1998; Goasdoue, Lattes, & Rousset, 2000; Calvanese et al., 1998a, 2002a, 2002b,
2008; Noy, 2004; Meyer, Lee, & Booth, 2005),
ontology-based data access (Dolby et al., 2008; Poggi et al., 2008a; Heymans et al.,
2008),
Semantic Web (Heflin & Hendler, 2001; Horrocks, Patel-Schneider, & van Harmelen, 2003).
c
2009
AI Access Foundation. rights reserved.

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Description logics (DLs, short) underlie standard Web Ontology Language OWL,1
process standardized W3C second edition, OWL 2.
widespread use DLs flexible modeling languages stems fact that,
similarly traditional modeling formalisms, structure domain interest
classes (or concepts, DL parlance) objects common properties. Properties
associated objects means binary relationships (or roles) objects.
Constraints available standard DLs also resemble used conceptual modeling
formalisms structuring information: is-a hierarchies (i.e., inclusions) disjointness
concepts roles, domain range constraints roles, mandatory participation
roles, functionality general numeric restrictions roles, covering within concept
hierarchies, etc. DL knowledge base (KB), constraints combined form
TBox asserting intensional knowledge, ABox collects extensional knowledge
individual objects, whether object instance concept, two objects
connected role. standard reasoning services DL KB include checking
consistency (or satisfiability), instance checking (whether certain individual instance
concept), logic entailment (whether certain constraint logically implied
KB). sophisticated services emerging support modular development
ontologies checking, example, whether one ontology conservative extension
another respect certain vocabulary (see, e.g., Ghilardi, Lutz, & Wolter, 2006;
Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008; Kontchakov, Wolter, & Zakharyaschev,
2008; Kontchakov, Pulina, Sattler, Schneider, Selmer, Wolter, & Zakharyaschev, 2009).
Description logics recently used provide access large amounts data
high-level conceptual interface, relevance data integration
ontology-based data access. setting, TBox constitutes conceptual, high-level
view information managed system, ABox physically stored
relational database accessed using standard relational database technology (Poggi
et al., 2008a; Calvanese et al., 2008). fundamental inference service case answering queries ABox constraints TBox taken account. kind
queries often considered first-order conjunctive queries,
correspond commonly used Select-Project-Join SQL queries. key properties
approach viable practice (i) efficiency query evaluation,
ideal target traditional database query processing, (ii) query evaluation
done leveraging relational technology already used storing data.
objectives mind, series description logicsthe DL-Lite familyhas
recently proposed investigated Calvanese, De Giacomo, Lembo, Lenzerini,
Rosati (2005, 2006, 2008a), later extended Artale, Calvanese, Kontchakov,
Zakharyaschev (2007a), Poggi, Lembo, Calvanese, De Giacomo, Lenzerini, Rosati
(2008a). logics family meet requirements and, time,
capable representing many important types constraints used conceptual modeling.
particular, inference various DL-Lite logics done efficiently size
data (data complexity) overall size KB (combined complexity):
shown KB satisfiability logics polynomial combined complexity,
answering queries AC0 data complexitywhich, roughly, means that, given
1. http://www.w3.org/2007/OWL/

2

fiThe DL-Lite Family Relations

conjunctive query KB, query TBox rewritten (independently
ABox) union conjunctive queries ABox alone. (It emphasized
data complexity measure important application context DL-Lite
logics, since one reasonably assume size data largely dominates size
TBox.) Query rewriting techniques implemented various systems
QuOnto2 (Acciarri, Calvanese, De Giacomo, Lembo, Lenzerini, Palmieri, & Rosati, 2005;
Poggi, Rodriguez, & Ruzzi, 2008b), ROWLKit (Corona, Ruzzi, & Savo, 2009), Owlgres
(Stocker & Smith, 2008) REQUIEM (Perez-Urbina, Motik, & Horrocks, 2009).
also demonstrated (Kontchakov et al., 2008) developing, analyzing re-using
DL-Lite ontologies (TBoxes) supported efficient tools capable checking various
types entailment ontologies respect given vocabularies, particular,
minimal module extraction tools (Kontchakov et al., 2009)which yet exist
richer languages.
significance DL-Lite family testified fact forms basis
OWL 2 QL, one three profiles OWL 2.3 OWL 2 profiles fragments
full OWL 2 language designed standardized specific application
requirements. According (the current version of) official W3C profiles document,
purpose OWL 2 QL language choice applications use large
amounts data query answering important reasoning task.
common denominator DL-Lite logics constructed far follows: (i) quantification roles inverses qualified (in words, concepts
form R.C must C = >) (ii) TBox axioms concept inclusions cannot represent kind disjunctive information (say, two concepts cover whole
domain). DL-Lite-related dialects designedwith aim capturing
conceptual modeling constraints, somewhat ad hoc mannerby extending
core language number constructs global functionality constraints,
role inclusions restricted Boolean operators concepts (see Section 4 details).
Although attempts made (Calvanese et al., 2006; Artale et al., 2007a;
Kontchakov & Zakharyaschev, 2008) put original DL-Lite logics general
perspective investigate extensions variety DL constructs required
conceptual modeling, resulting picture still remains rather fragmentary far
comprehensive. systematic investigation DL-Lite family relatives become
even urgent challenging view choice constructs included
specification OWL 2 QL profile4 (in particular, OWL make
unique name assumption, UNA, usually adopted DL-Lite, uses equalities
inequalities object names instead).
main aim article fill gap provide thorough comprehensive understanding interaction various DL-Lite constructs impact
computational complexity reasoning. achieve goal, consider spectrum
logics, classified according five mutually orthogonal features:
(1) presence absence role inclusions;
2. http://www.dis.uniroma1.it/quonto/
3. http://www.w3.org/TR/owl2-profiles/
4. http://www.w3.org/TR/owl2-profiles/#OWL_2_QL

3

fiArtale, Calvanese, Kontchakov & Zakharyaschev

(2) form allowed concept inclusions, consider four classes, called core,
Krom, Horn, Bool, exhibit different computational properties;
(3) form allowed numeric constraints, ranging none, global functionality
constraints only, arbitrary number restrictions;
(4) presence absence unique name assumption (and equalities inequalities object names, assumption dropped);
(5) presence absence standard role constraints disjointness, symmetry,
asymmetry, reflexivity, irreflexivity, transitivity.
resulting cases, investigate combined data complexity KB satisfiability instance checking, well data complexity query answering.
obtained tight complexity results summarized Section 3.4 (Table 2 Remark 3.1).
already mentioned, original motivation distinguishing feature logics
DL-Lite family lite-ness sense low computational complexity
reasoning tasks (query answering AC0 data complexity tractable KB satisfiability
combined complexity). broader perspective take here, logics
meet requirement, particular, Krom Bool concept inclusions.5 However,
identify another distinguishing feature regarded natural logic-based
characterization DL-Lite family: embeddability one-variable fragment firstorder logic without equality function symbols. allows us relate complexity
DL-Lite logics complexity corresponding fragments first-order logic, thus
obtain deep insight underlying logical properties DL-Lite variant.
example, upper complexity bounds established follow embedding
well-known results classical decision problem (see, e.g., Borger, Gradel, & Gurevich,
1997) descriptive complexity (see, e.g., Immerman, 1999).
One interesting findings article number restrictions, even
expressed locally, instead global role functionality, added original DL-Lite
logics (under UNA without role inclusions) free, is, without changing
computational complexity. first-order approach shows cases
also extend DL-Lite logics role constraints mentioned above, keeping
complexity. also gives framework analyze effect adopting dropping
UNA using (in)equalities object names. example, observe
equality allowed language DL-Lite (which makes sense without UNA)
query answering becomes LogSpace-complete data complexity, therefore
first-order rewritable. also turns dropping UNA results P-hardness
reasoning (for combined data complexity) presence functionality constraints (NLogSpace-hardness shown Calvanese et al., 2008), NP-hardness
arbitrary number restrictions allowed.
Another interesting finding dramatic impact role inclusions, combined
number restrictions (or even functionality constraints), computational complexity reasoning. already observed Calvanese et al. (2006), combination increases data complexity instance checking membership LogSpace
5. Note, way, logics Bool concept inclusions turn quite useful conceptual
modeling reasonably manageable computationally (Kontchakov et al., 2008).

4

fiThe DL-Lite Family Relations

NLogSpace-hardness. show situation actually even worse: data
complexity, instance checking turns P-complete case core Horn
logics coNP-complete case Krom Bool logics; moreover, KB satisfiability,
NLogSpace-complete combined complexity simplest core casei.e., efficiently tractable, role inclusions number restrictions used separatelybecomes
ExpTime-completei.e., provably intractable, used together.
retain role inclusions functionality constraints language keep
complexity within required limits, Poggi et al. (2008a) introduced another DL-Lite
dialect, called DL-LiteA , restricts interaction role inclusions functionality constraints. extend result showing DL-Lite logics
limited interaction role inclusions number restrictions still embedded one-variable fragment first-order logic, exhibit behavior
fragments role inclusions number restrictions.
article structured following way. Section 2, introduce logics
extended DL-Lite family illustrate features conceptual modeling formalisms.
Section 3, discuss reasoning services complexity measures analyzed
follows, give overview obtained complexity results. Section 4,
place introduced DL-Lite logics context original DL-Lite family,
discuss relationship OWL 2. Section 5, study combined complexity
KB satisfiability instance checking, Section 6, consider data complexity
problems. Section 7, study data complexity query answering.
Section 8, analyze impact dropping UNA adding (in)equalities
object names complexity reasoning. Section 9 concludes article.

2. Extended DL-Lite Family Description Logics
Description Logic (Baader et al., 2003) family logics studied
used knowledge representation reasoning since 1980s. DLs, elements
domain interest structured concepts (unary predicates), properties
specified means roles (binary predicates). Complex concept role expressions
(or simply concepts roles) constructed, starting set concept role
names, applying suitable constructs, set available constructs depends
specific description logic. Concepts roles used knowledge base
assert knowledge, intensional level, so-called TBox (T terminological),
extensional level, so-called ABox (A assertional). TBox typically
consists set axioms stating inclusion concepts roles. ABox, one
assert membership objects (i.e., constants) concepts, pair objects
connected role. DLs supported reasoning services, satisfiability checking
query answering, rely logic-based semantics.
2.1 Syntax Semantics Logics DL-Lite Family
introduce (extended) DL-Lite family description logics, initially
proposed aim capturing typical conceptual modeling formalisms, UML
class diagrams ER models (see Section 2.2 details), maintaining good computational properties standard DL reasoning tasks (Calvanese et al., 2005). begin
5

fiArtale, Calvanese, Kontchakov & Zakharyaschev

defining logic DL-LiteHN
bool , regarded supremum original
DL-Lite family (Calvanese et al., 2005, 2006, 2007b) lattice description logics.
HN
DL-LiteHN
bool . language DL-Litebool contains object names a0 , a1 , . . . , concept names
A0 , A1 , . . . , role names P0 , P1 , . . . . Complex roles R concepts C language
defined follows:

Pk ,

R

::=

Pk

|

B

::=



|

Ak

|

q R,

C

::=

B

|

C

|

C1 u C2 ,

q positive integer. concepts form B called basic.
DL-LiteHN
bool TBox, , finite set concept role inclusion axioms (or simply
concept role inclusions) form:
C1 v C2



R1 v R2 ,

ABox, A, finite set assertions form:
Ak (ai ),

Ak (ai ),

Pk (ai , aj )



Pk (ai , aj ).

Taken together, constitute DL-LiteHN
bool knowledge base K = (T , A).
following, denote role(K) set role names occurring A, role (K)
set {Pk , Pk | Pk role(K)}, ob(A) set object names A. role R,
set:
(
Pk , R = Pk ,
inv(R) =
Pk , R = Pk .
usual description logic, interpretation, = (I , ), consists nonempty
domain interpretation function assigns object name ai element
aIi , concept name Ak subset AIk domain, role name
Pk binary relation PkI domain. Unless otherwise stated, adopt
unique name assumption (UNA):
aIi 6= aIj



6= j.

(UNA)

However, shall always indicate results depend UNA
not, depend assumption, discuss also consequences
dropping (see also Sections 4 8).
role concept constructs interpreted standard way:
(Pk )I = {(y, x) | (x, y) PkI },






( q R)

= ,


= x | ]{y | (x, y) RI } q ,

(C)I = \ C ,


(C1 u C2 )

=

C1I



(inverse role)
(the empty set)
(at least q R-successors)
(not C)

C2I ,

(both C1 C2 )

6

fiThe DL-Lite Family Relations

]X denotes cardinality X. use standard abbreviations
C1 C2 = (C1 u C2 ),

> = ,

R = ( 1 R),

q R = ( q + 1 R).

Concepts form q R q R called number restrictions, form
R called existential concepts.
satisfaction relation |= also standard:
|= C1 v C2

iff

C1I C2I ,

|= R1 v R2

iff

R1I R2I ,

|= Ak (ai )

iff aIi AIk ,

|= Pk (ai , aj )

iff

(aIi , aIj ) PkI ,

|= Ak (ai )

iff aIi
/ AIk ,

|= Pk (ai , aj )

iff

(aIi , aIj )
/ PkI .

knowledge base K = (T , A) said satisfiable (or consistent) interpretation, I, satisfying members A. case write |= K (as well
|= |= A) say model K (and A).
languages DL-Lite family investigate article obtained restricting language DL-LiteHN
bool along three axes: (i) Boolean operators (bool )
concepts, (ii) number restrictions (N ) (iii) role inclusions, hierarchies (H).
Similarly classical logic, adopt following definitions. DL-LiteHN
bool TBox
called Krom TBox 6 concept inclusions restricted to:
B1 v B2 ,

B1 v B2



B1 v B2

(Krom)

(here Bi B basic concepts). called Horn TBox
concept inclusions restricted to:
l
Bk v B
(Horn)
k

(by definition, empty conjunction >). Finally, call core TBox
concept inclusions restricted to:
B1 v B2



B1 v B2 .

(core)

B1 v B2 equivalent B1 u B2 v , core TBoxes regarded sitting
intersection Krom Horn TBoxes.
Remark 2.1 sometimes use conjunctions
right-hand side concept includ
sions restricted languages: C v k Bk . Clearly, syntactic sugar add
extra expressive power.
HN
HN
HN
DL-LiteHN
krom , DL-Litehorn DL-Litecore . fragments DL-Litebool Krom,
HN
HN
Horn, core TBoxes denoted DL-Litekrom , DL-Litehorn DL-LiteHN
core , respectively. fragments obtained limiting use number restrictions role
inclusions.

6. Krom fragment first-order logic consists formulas prenex normal form whose quantifier-free
part conjunction binary clauses.

7

fiArtale, Calvanese, Kontchakov & Zakharyaschev

HN
DL-LiteH
. fragment DL-Lite , {core, krom, horn, bool}, without number
restrictions q R, q 2, (but role inclusions) denoted DL-LiteH
. Note
H
that, DL-Lite , still use existential concepts R (that is, 1 R).
HF
DL-LiteHF
fragment DL-LiteHN
number
. Denote DL-Lite

restrictions q R, existential concepts (with q = 1) q = 2
occur concept inclusions form 2 R v . inclusion called
global functionality constraint states role R functional (more precisely,
|= ( 2 R v ) (x, y) RI (x, z) RI , = z).
F
DL-LiteN
, DL-Lite DL-Lite . role inclusions excluded language,
{core, krom, horn, bool} obtain three fragments: DL-LiteN
(with arbitrary number restrictions), DL-LiteF
(with
functionality
constraints

existential
concepts

R), DL-Lite (without number restrictions different R).

shall see later article, logics form DL-LiteHF
DL-LiteHN

,
even = core, turn computationally rather costly interaction
role inclusions functionality constraints (or, generally, number restrictions). hand, purpose conceptual modeling one may need
constructs; cf. example Section 2.2. compromise found artificially
limiting interplay role inclusions number restrictions way similar
logic DL-LiteA proposed Poggi et al. (2008a).
TBox , let vT denote reflexive transitive closure relation


(R, R0 ), (inv(R), inv(R0 )) | R v R0
let R R0 iff R vT R0 R0 vT R. Say R0 proper sub-role R
R0 vT R R0
6 R.
(HN )

(HN )

DL-Lite . introduce logics DL-Lite ,
{core, krom, horn, bool},
HN
which, one hand, restrict logics DL-Lite limiting interaction
role inclusions number restrictions order reduce complexity reasoning, and,
hand, include additional constructs, limited qualified existential quantifiers, role disjointness, (a)symmetry (ir)reflexivity constraints, increase
expressive power logics affect computational properties.
(HN )
DL-Lite
TBoxes must satisfy conditions (A1 )(A3 ) below. (We remind
reader occurrence concept right-hand (left-hand) side concept
inclusion called negative scope odd (even) number negations ;
otherwise occurrence called positive.)
(A1 ) may contain positive occurrences qualified number restrictions q R.C,
C conjunction concepts allowed right-hand side -concept
inclusions;
(A2 ) q R.C occurs , contain negative occurrences number
restrictions q 0 R q 0 inv(R) q 0 2;
(A3 ) R proper sub-role , contain negative occurrences
q R q inv(R) q 2.
8

fiThe DL-Lite Family Relations

role
role
constraints inclusions






yes

disj.
(a)sym.
(ir)ref.
disj.
(a)sym.
(ir)ref.
tran.
a)

number
restrictions
R
R/funct.
qR
R
R/funct.
qR

concept inclusions
Krom
Horn
DL-Litekrom DL-Litehorn
DL-LiteF
DL-LiteF
krom
horn
N
DL-Litekrom DL-LiteN
horn
H
DL-LiteH
DL-Lite
krom
horn
HF
DL-Litekrom DL-LiteHF
horn
HN
DL-LiteHN
DL-Lite
krom
horn

core
DL-Litecore
DL-LiteF
core
DL-LiteN
core
DL-LiteH
core
DL-LiteHF
core
DL-LiteHN
core

(HF )

Bool
DL-Litebool
DL-LiteF
bool
DL-LiteN
bool
DL-LiteH
bool
DL-LiteHF
bool
DL-LiteHN
bool

(HF )

(HF )

)
R.C/funct.a) DL-Lite(HF
DL-Litekrom DL-Litehorn DL-Litebool
core
(HN )
(HN )
(HN )
(HN )
a)
q R.C
DL-Litecore
DL-Litekrom DL-Litehorn DL-Litebool

yes

+

)
R.C/funct.a) DL-Lite(HF
core

yes

q R.C a)

(HF )+

DL-Litekrom

+

(HN )+

)
DL-Lite(HN
DL-Litekrom
core

(HF )+

DL-Litehorn

(HN )+

DL-Litehorn

(HF )+

DL-Litebool

(HN )+

DL-Litebool

restricted (A1 )(A3 ).

Table 1: extended DL-Lite family.
(HN )

(It follows DL-Lite
TBox contain both, say, functionality constraint
2 R v occurrence q R.C, q 1.)
(HN )
Additionally, DL-Lite
TBoxes contain role constraints (or axioms) form:
Dis(R1 , R2 ),

Asym(Pk ),

Sym(Pk ),

Irr(Pk ),



Ref(Pk ).

meaning new constructs defined usual: interpretation = (I , ),


( q R.C)I = x | ]{y C | (x, y) RI } q ;
|= Dis(R1 , R2 )
|= Asym(Pk )
|= Sym(Pk )
|= Irr(Pk )
|= Ref(Pk )

iff
iff

iff
iff
iff

R1I R2I = (roles R1 R2 disjoint);
PkI (Pk )I = (role Pk asymmetric);

PkI = (Pk )I

(Pk symmetric);

(x, x)
/ PkI x
(x, x) PkI x

(Pk irreflexive);
(Pk reflexive).

emphasized extra constructs often used conceptual modeling
(HN )
introduction DL-Lite
motivated OWL 2 QL proposal. (Note
(HN )
DL-Lite
contains DL-LiteH

DL-LiteN

proper fragments.)
(HN )+

(HN )+

DL-Lite
.
{bool, horn, krom, core}, denote DL-Lite
extension
(HN )
DL-Lite
role transitivity constraints form Tra(Pk ), meaning
expected:
|= Tra(Pk )

iff

(x, y) PkI (y, z) PkI imply (x, z) PkI , x, y, z
(Pk transitive).
9

fiArtale, Calvanese, Kontchakov & Zakharyaschev

DL-Litebool

DL-LiteHN


@

@
DL-Litehorn






DL-Litekrom






@

@

DL-Litecore

PP


6
DL-LiteHF

6
DL-LiteH


DL-LiteN


(HN ) )+
DL-Lite
DL-Lite(HN


1
6
6

6
PP



1

DL-LiteF


6





iP
P
DL-Lite

)+
(HF ) DL-Lite
DL-Lite(HF



Figure 1: Language inclusions extended DL-Lite family.
remind reader standard restriction limiting use transitive roles DLs
(see, e.g., Horrocks, Sattler, & Tobies, 2000):
simple roles R allowed concepts form q R, q 2,
simple role given TBox understand role without transitive sub-roles
(including itself). particular, contains Tra(P ) P P simple,
cannot contain occurrences concepts form q P q P , q 2.
(HF )

(HF )+

(HF )

DL-Lite
DL-Lite .
also define languages DL-Lite
sub-languages
(HN )
DL-Lite ,
number restrictions form R, R.C functionality
constraints 2 R v allowedprovided, course, satisfy (A1 )(A3 );
(HF )+
particular, R.C allowed R functional. before, DL-Lite
extensions
(HF )
DL-Lite
role transitivity constraints (satisfying restriction above).
Thus, extended DL-Lite family consider article consists 40 different
logics collected Table 1. inclusions logics shown Figure 1.
obtained taking product left- right-hand parts picture,
subscript right-hand part ranges {core, krom, horn, bool}, i.e.,
subscripts left-hand part, similarly, superscript left-hand part
ranges { , F, N , H, HF, HN , (HF), (HN ), (HF)+ , (HN )+ }, i.e., superscripts
right-hand part.
position logics relative DL-Lite logics known literature
OWL 2 QL profile discussed Section 4. starting Section 5, begin
thorough investigation computational properties logics extended DLLite family, without UNA. illustrate expressive
power DL-Lite logics concrete example.
2.2 DL-Lite Conceptual Modeling
tight correspondence conceptual modeling formalisms, ER model
UML class diagrams, various description logics pointed various
papers (e.g., Calvanese et al., 1998b, 1999; Borgida & Brachman, 2003; Berardi et al.,
2005). give example showing DL-Lite logics used conceptual
modeling purposes; details see work Artale et al. (2007b).
10

fiThe DL-Lite Family Relations

1..1

Employee

1..*

empCode: Integer
salary: Integer

worksOn
boss
3..*
Project

Manager

projectName: String

1..*
1..1
{disjoint, complete}

AreaManager

manages
TopManager

1..1

Figure 2: UML class diagram.
Let us consider UML class diagram depicted Figure 2 representing (a portion
of) company information system. According diagram, managers employees partitioned area managers top managers. information
represented means following concept inclusions (where brackets specify
minimal DL-Lite language inclusion belongs to):
Manager v Employee

(DL-Litecore )

AreaManager v Manager

(DL-Litecore )

TopManager v Manager

(DL-Litecore )

AreaManager v TopManager

(DL-Litecore )

Manager v AreaManager TopManager

(DL-Litebool )

employee two functional attributes, empCode salary, integer values.
Unlike OWL, distinguish abstract objects data values. Hence
model datatype, Integer , means concept, attribute,
employees salary, means role. Thus, salary represented follows:
Employee v salary
salary



(DL-Litecore )

v Integer

(DL-Litecore )
(DL-LiteF
core )

2 salary v

functional attribute empCode values Integer represented way.
binary relationship worksOn Employee domain Project range:
worksOn v Employee
worksOn



(DL-Litecore )

v Project

(DL-Litecore )

binary relationship boss domain Employee range Manager treated analogously. employee works project exactly one boss, project must
11

fiArtale, Calvanese, Kontchakov & Zakharyaschev

involve least three employees:
Employee v worksOn

(DL-Litecore )

Employee v boss

(DL-Litecore )
(DL-LiteF
core )

2 boss v
Project v 3 worksOn

(DL-LiteN
core )

top manager manages exactly one project also works project, project
managed exactly one top manager:
manages v TopManager
manages



v Project

(DL-Litecore )

TopManager v manages
Project v manages

(DL-Litecore )
(DL-Litecore )



(DL-Litecore )

2 manages v

(DL-LiteF
core )

2 manages v

(DL-LiteF
core )
(DL-LiteH
core )

manages v worksOn

all, languages extended DL-Lite family capable representing
(HN )
UML class diagram Figure 2 DL-LiteHN
bool DL-Litebool . Note, however, except covering constraint, Manager v AreaManager TopManager , concept
inclusions DL-Lite translation UML class diagram belong variants
(HN )
core fragments DL-LiteHN
core DL-Litecore . hard imagine situation
one needs Horn concept inclusions represent integrity constraints UML class diagrams, example, express (together axioms) chief executive
officer may work five projects manager one them:
CEO u ( 5 worksOn) u manages v

(DL-LiteN
horn )

context UML class diagrams, Krom fragment DL-Litekrom (with variants)
seems useless: extends DL-Litecore concept inclusions form B1 v B2
or, equivalently, > v B1 B2 , rarely used conceptual modeling. Indeed,
would correspond partitioning whole domain interest two parts,
general useful covering constraints form B v B1 Bk require full
Bool language. hand, Krom fragments important pinpointing
borderlines various complexity classes description logics DL-Lite family
extensions; see Table 2.

3. Reasoning DL-Lite Logics
discuss reasoning problems consider article, mutual relationships, complexity measures adopt. also provide overview complexity
results DL-Lite logics obtained article.

12

fiThe DL-Lite Family Relations

3.1 Reasoning Problems
concentrate three fundamental standard reasoning tasks description
logics: satisfiability (or consistency), instance checking, query answering.
DL L extended DL-Lite family, define L-concept inclusion
concept inclusion allowed L. Similarly, define notions L-KB L-TBox.
Finally, define L-concept concept occur right-hand side
L-concept inclusion conjunction concepts.
Satisfiability. KB satisfiability problem check, given L-KB K, whether
model K. Clearly, satisfiability minimal requirement ontology.
well known DL (Baader et al., 2003), many reasoning tasks description logics
reducible satisfiability problem. Consider, example, subsumption problem:
given L-TBox L-concept inclusion C1 v C2 , decide whether |= C1 v C2 ,
is, C1I C2I , every model . reduce problem (un)satisfiability, take
fresh concept name A, fresh object name a, set K = (T 0 , A),
0 = {A v C1 , v C2 }

= {A(a)}.

easy
see |= C1 v C2 iff K satisfiable. core, Krom Horn KBs,
C2 = k Dk , Dk (possibly negated) basic concept, checking unsatisfiability
K amounts checking unsatisfiability KBs Kk = (Tk , A), Tk =
{A v C1 , v Dk } (for Horn KBs, replace v B equivalent u B v ).
concept satisfiability problemgiven L-TBox L-concept C, decide
whether C 6= model also easily reducible KB satisfiability. Indeed,
take fresh concept name A, fresh object name a, set K = (T 0 , A),
0 = {A v C}

= {A(a)}.

C satisfiable respect iff K satisfiable.
Instance checking. instance checking problem decide, given object name a,
L-concept C L-KB K = (T , A), whether K |= C(a), is, aI C , every
model K. Instance checking also reducible (un)satisfiability: object
instance L-concept C every model K = (T , A) iff KB K0 = (T 0 , A0 ),
0 = {A v C}



A0 = {A(a)},

notdsatisfiable, fresh concept name. core, Krom Horn KBs,
C = k Dk , Dk (possibly negated) basic concept, proceed
subsumption: checking unsatisfiability K0 amounts checking unsatisfiability
KB Kk0 = (Tk0 , A0 ) Tk0 = {A v Dk }.
Conversely, KB satisfiability reducible complement instance checking: K
satisfiable iff K 6|= A(a), fresh concept name fresh object a.
Query answering. positive existential query q(x1 , . . . , xn ) first-order formula
(x1 , . . . , xn ) constructed means conjunction, disjunction existential quantification starting atoms Ak (t) Pk (t1 , t2 ), Ak concept name, Pk
13

fiArtale, Calvanese, Kontchakov & Zakharyaschev

role name, t, t1 , t2 terms taken list variables y0 , y1 , . . . list
object names a0 , a1 , . . . (i.e., positive existential formula). precisely,


::=

yi

|



::=

Ak (t)

ai ,
|

Pk (t1 , t2 )

|

1 2

|

1 2

|

yi .

free variables called distinguished variables q bound ones nondistinguished variables q. write q(x1 , . . . , xn ) query distinguished variables
x1 , . . . , xn . conjunctive query positive existential query contains disjunction
(it constructed atoms means conjunction existential quantification only).
Given query q(~x) = (~x) ~x = x1 , . . . , xn n-tuple ~a object names,
write q(~a) result replacing every occurrence xi (~x) ith member
~a. Queries containing distinguished variables called ground (they also known
Boolean).
Let = (I , ) interpretation. assignment function associating
every variable element a(y) . use following notation: aI,a
= aIi

I,a = a(y). satisfaction relation positive existential formulas respect
given assignment defined inductively taking:
|=a Ak (t)

iff

tI,a AIk ,

|=a Pk (t1 , t2 )

iff

I,a

(tI,a
1 , t2 ) Pk ,

|=a 1 2

iff

|=a 1 |=a 2 ,

|=a 1 2

iff

|=a 1 |=a 2 ,

|=a yi

iff

|=b , assignment b may differ yi .

ground query q(~a), satisfaction relation depend assignment a,
write |= q(~a) instead |=a q(~a). answer query either yes
no.
KB K = (T , A), say tuple ~a object names certain answer
q(~x) respect K, write K |= q(~a), |= q(~a) whenever |= K. query
answering problem formulated follows: given L-KB K = (T , A), query q(~x),
tuple ~a object names A, decide whether K |= q(~a).
Note instance checking problem special case query answering: object
instance L-concept C respect KB K iff answer query A(a)
respect K0 yes, K0 = (T 0 , A) 0 = {C v A}, fresh
concept name. Horn-concepts B1 u u Bk , consider query A1 (a) Ak (a)
respect K0 , K0 = (T 0 , A) 0 = {B1 v A1 , . . . , Bk v Ak },
Ai fresh concept names. Similarly, deal Krom-concepts D1 u u Dk ,
Di possibly negated basic concept. core-concepts, reduction holds
conjunctions basic concepts.
3.2 Complexity Measures: Data Combined Complexity
computational complexity reasoning problems discussed analyzed
respect different complexity measures, depend parameters
14

fiThe DL-Lite Family Relations

problem regarded input (i.e., vary) regarded
fixed. satisfiability instance checking, parameters consider size
TBox size ABox A, number symbols A,
denoted |T | |A|, respectively. size |K| knowledge K = (T , A) simply given
|T | + |A|. query answering, one parameter consider would size
query. However, analysis adopt standard database assumption size
queries always bounded reasonable constant and, case, negligible
respect size TBox size ABox. Thus count
query part input.
Hence, consider reasoning problems two complexity measures. whole
KB K regarded input, deal combined complexity. If, however,
ABox counted input, TBox (and query) regarded
fixed, concern data complexity (Vardi, 1982). Combined complexity interest
still designing testing ontology. hand, data complexity
preferable cases TBox fixed size (and size query)
negligible compared size ABox, case, instance, context
ontology-based data access (Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, & Rosati,
2007) data intensive applications (Decker, Erdmann, Fensel, & Studer, 1999; Noy,
2004; Lenzerini, 2002; Calvanese et al., 2008). Since logics DL-Lite family
tailored deal large data sets stored relational databases, data complexity
instance checking query answering particular interest us.
3.3 Remarks Complexity Classes LogSpace AC0
paper, deal following complexity classes:
AC0 ( LogSpace NLogSpace P NP ExpTime.
definitions found standard textbooks (e.g., Garey & Johnson, 1979;
Papadimitriou, 1994; Vollmer, 1999; Kozen, 2006). remind reader
two smallest classes LogSpace AC0 .
problem belongs LogSpace two-tape Turing machine that,
starting input length n written read-only input tape, stops accepting rejecting state used log n cells (initially blank) read/write work
tape. LogSpace transducer three-tape Turing machine that, started
input length n written read-only input tape, writes result (of polynomial size)
write-only output tape using log n cells (initially blank) read/write
work tape. LogSpace-reduction reduction computable LogSpace transducer;
composition two LogSpace transducers also LogSpace transducer (Kozen,
2006, Lemma 5.1).
formal definition complexity class AC0 (see, e.g., Boppana & Sipser, 1990;
Vollmer, 1999 references therein) based circuit model, functions
represented directed acyclic graphs built unbounded fan-in And,
gates (i.e., gates may unbounded number incoming edges).
definition assume decision problems encoded alphabet {0, 1}
regarded Boolean functions. AC0 class problems definable using
15

fiArtale, Calvanese, Kontchakov & Zakharyaschev

family circuits constant depth polynomial size, generated
deterministic Turing machine logarithmic time (in size input); latter
condition called LogTime-uniformity. Intuitively, AC0 allows us use polynomially
many processors run-time must constant. typical example AC0 problem
evaluation first-order queries databases (or model checking first-order sentences
finite models), database (first-order model) regarded input
query (first-order sentence) assumed fixed (Abiteboul, Hull, & Vianu, 1995;
Vollmer, 1999). hand, undirected graph reachability problem known
LogSpace (Reingold, 2008) AC0 . Boolean function f : {0, 1}n {0, 1}
called AC0 -reducible (or constant-depth reducible) function g : {0, 1}n {0, 1}
(LogTime-uniform) family constant-depth circuits built And, Or,
g gates computes f . case say AC0 -reduction. Note
reductions considered Section 3.1 AC0 -reductions. Unless otherwise indicated,
follows write reduction AC0 -reduction.
3.4 Summary Complexity Results
article, aim investigate (i) combined data complexity satisfiability instance checking problems (ii) data complexity query answering
problem logics extended DL-Lite family, without UNA.
(HF )+
obtained known results first 32 logics Table 1 (the logics DL-Lite
(HN )+
DL-Lite
included) summarized Table 2 (we remind reader
satisfiability instance checking reducible complements
instance checking special case query answering). fact, results
table follow lower upper bounds marked [] [], respectively (by
taking account hierarchy languages DL-Lite family): example,
NLogSpace membership satisfiability DL-LiteN
krom Theorem 5.7 implies
N
F
upper bound DL-Litekrom , DL-Litekrom , DL-Litecore , DL-LiteF
core DL-Litecore
.
sub-languages DL-LiteN
krom
Remark 3.1 Two complexity results noted (they included
Table 2):
(i) equality object names allowed language DL-Lite,
makes sense UNA dropped, AC0 memberships Table 2 replaced LogSpace-completeness (see Section 8, Theorem 8.3 8.9); inequality
constraints affect complexity.
(ii) extend languages role transitivity constraints combined complexity satisfiability remains same, data complexity, instance
checking query answering become NLogSpace-hard (see Lemma 6.3), i.e.,
membership AC0 data complexity replaced NLogSpace-completeness,
complexity results remain same.
either case, property first-order rewritabilitythat is, possibility rewriting
given query q given TBox single first-order query q0 returning certain
answers q (T , A) every ABox A, ensures query answering problem
AC0 data complexityis lost.
16

fiThe DL-Lite Family Relations

Complexity
Languages

UNA

Combined complexity
Satisfiability

|H]
DL-Lite[core
[ |H]
DL-Litehorn
[ |H]
DL-Litekrom
[ |H]
DL-Litebool
|N |(HF )|(HN )]
DL-Lite[F
core
[F |N |(HF )|(HN )]
DL-Litehorn
[F |N |(HF )|(HN )]
DL-Litekrom
[F |N |(HF )|(HN )]
DL-Litebool
[F |(HF )]
DL-Litecore/horn
[F |(HF )]
DL-Litekrom
[F |(HF )]
DL-Litebool
[N |(HN )]
DL-Litecore/horn
[N |(HN )]
DL-Litekrom/bool
DL-LiteHF
core/horn
DL-LiteHF
krom/bool
DL-LiteHN
core/horn
HN
DL-Litekrom/bool

yes/no

yes



yes/no

Data complexity
Instance checking
0

Query answering

NLogSpace [A]

AC

AC0

P [Th.8.2] [A]

AC0

AC0 [C]

0

NLogSpace [Th.8.2]

AC

coNP [B]

NP [Th.8.2] [A]

AC0 [Th.8.3]

coNP

0

NLogSpace

AC

AC0

P [Th.5.8, 5.13]

AC0

AC0 [Th.7.1]

NLogSpace [Th.5.7,5.13]

AC0

coNP

0

NP [Th.5.6, 5.13]

AC [Cor.6.2]

coNP

P [Cor.8.8] [Th.8.7]

P [Th.8.7]

P

P [Cor.8.8]

P

coNP

NP

P [Cor.8.8]

coNP

NP [Th.8.4]

coNP [Th.8.4]

coNP

NP [Th.8.5]

coNP

coNP

ExpTime [Th.5.10]

P [Th.6.7]

P [D]

ExpTime

coNP [Th.6.5]

coNP

ExpTime

coNP [Th.6.6]

coNP

ExpTime [F]

coNP

coNP [E]

[A] complexity respective fragment propositional Boolean logic
[B] follows proof data complexity result instance checking ALE (Schaerf, 1993)
[C] (Calvanese et al., 2006)
[D] follows Horn-SHIQ (Hustadt, Motik, & Sattler, 2005; Eiter, Gottlob, Ortiz, & Simkus, 2008)
[E] follows SHIQ (Ortiz, Calvanese, & Eiter, 2006, 2008; Glimm, Horrocks, Lutz, & Sattler, 2007)
[F] follows SHIQ (Tobies, 2001)

Table 2: Complexity DL-Lite logics (all complexity bounds save AC0 tight).

1 ||n ]
means DL-Lite1 , . . . , DL-Liten
DL-Lite[

(in particular, DL-Lite[ |H] either DL-Lite DL-LiteH
).

DL-Litecore/horn means DL-Litecore DL-Litehorn (likewise DL-Litekrom/bool ).
[X] ( [X]) means upper (respectively, lower) bound follows [X].

Detailed proofs results given Sections 58. variants logics
involving number restrictions, upper bounds hold also assumption
numbers q concepts form q R given binary. (Intuitively, follows
fact proofs use numbers explicitly occur KB.)
lower bounds remain unary coding, since corresponding proofs
use numbers exceeding 4.
next section consider extended DL-Lite family general context
identifying place among DL-Lite-related logics, particular OWL 2 profiles.
17

fiArtale, Calvanese, Kontchakov & Zakharyaschev

4. Landscape DL-Lite Logics
original family DL-Lite logics created two goals mind: identify
description logics that, one hand, capable representing basic features
conceptual modeling formalisms (such UML class diagrams ER diagrams) and,
hand, computationally tractable, particular, matching AC0 data
complexity database query answering.
saw Section 2.2, represent UML class diagrams one need typical quantification constructs basic description logic ALC (Schmidt-Schau & Smolka,
1991), namely, universal restriction R.C qualified existential quantification R.C: one
always take role filler C >. Indeed, domain range restrictions
relationship P expressed concepts inclusions P v B1 P v B2 , respectively. Thus, almost concept inclusions required capturing UML class diagrams
form B1 v B2 B1 v B2 . observations motivated introduction
Calvanese et al. (2005) first DL-Lite logic, new nomenclature corresponds
DL-LiteF
core . main results polynomial-time upper bound combined
complexity KB satisfiability LogSpace upper bound data complexity
conjunctive query answering (under UNA). results extended Calvanese
H
et al. (2006) two larger languages: DL-LiteF
horn DL-Litehorn , originally
called DL-Liteu,F DL-Liteu,R , respectively. Calvanese et al. (2007b) introduced another member DL-Lite family (named DL-LiteR ), extended DL-LiteH
core
role disjointness axioms form Dis(R1 , R2 ). computational behavior new
logic turned DL-LiteH
core . may worth mentioning
DL-LiteH
covers

DL
fragment

RDFS
(Klyne
& Carroll, 2004; Hayes, 2004). Note
core
also Calvanese et al. (2006) considered variants DL-Liteu,F DL-Liteu,R
arbitrary n-ary relations (not usual binary roles) showed query answering still LogSpace data complexity. conjecture similar results
obtained DL-Lite logics introduced paper. Artale et al. (2007b)
demonstrated n-ary relations represented DL-LiteF
core means reification.
variant DL-Lite, called DL-LiteA (A attributes), introduced
Poggi et al. (2008a) aim capturing many features conceptual modeling
formalisms possible, still maintaining computational properties basic
variants DL-Lite. One features DL-LiteA , borrowed conceptual modeling
formalisms adopted also OWL, distinction (abstract) objects data
values, consequently, concepts (sets objects) datatypes (sets data
values), roles (i.e., object properties OWL, relating objects objects)
attributes (i.e., data properties OWL, relating objects data values). However,
far results paper concerned, distinction concepts
datatypes, roles attributes impact reasoning whatsoever, since
datatypes simply treated special concepts mutually disjoint also
disjoint proper concepts. Instead, relevant reasoning possibility
express DL-LiteA role inclusions functionality, i.e., DL-LiteA includes
F
HF
DL-LiteH
core DL-Litecore , DL-Litecore .
already mentioned, role inclusions functionality constraints cannot
combined unrestricted way without losing good computational properties:

18

fiThe DL-Lite Family Relations

.
b
b

DL-LiteHN
krom
b

b

b

...

SHIQ
DL-LiteHN
horn
b

DL-LiteHN
bool
b

...
b

DL-Litekrom
b

b

DL-LiteHF
core
b

(HN )

b

b

b

P

b

DL-Litecore
(HF )
DL-Litecore

DL-LiteN
core

(HN )

DL-Litehorn
(HF )
DL-Litehorn
DL-LiteN
horn
+
DL-LiteA,u
b

b

b

b

b

DL-LiteHF
horn

DL-Lite+

DL-LiteA
DL-LiteF = DL-LiteF
DL-LiteF ,u = DL-LiteF
core
horn
H
DL-LiteR = DL-Litecore
DL-LiteR,u = DL-LiteH
horn
DL-Litecore
b

b

AC0

DL-Litebool

Horn-SHIQ

coNP

.

b

DL-LiteHN
core

b

b

b

Figure 3: DL-Lite family relations.
Theorems 5.10 6.7, prove satisfiability DL-LiteHF
core KBs ExpTime-hard
combined complexity, instance checking data-hard P (NLogSpace-hardness
shown Calvanese et al., 2006). DL-LiteA , keep query answering AC0
data complexity satisfiability NLogSpace combined complexity, functional roles
(and attributes) allowed specialized, i.e., used positively right-hand
side role (and attribute) inclusion axioms. So, condition (A3 ) slight generalization
restriction. DL-LiteA also allows axioms form B v R.C non-functional
roles R, covered conditions (A1 ) (A2 ). Thus, DL-LiteA regarded
(HF )
(HN )
proper fragment DL-Litecore DL-Litehorn . show Sections 5.3 7
three languages enjoy similar computational properties UNA:
tractable satisfiability query answering AC0 .
conclude section picture Figure 3 illustrating landscape DLLite-related logics grouping according data complexity positive existential
query answering UNA. original eight DL-Lite logics, called Calvanese
et al. (2007b) DL-Lite family, shown bottom sector picture (the logics
+
DL-Lite+
DL-LiteA,u extend DL-LiteA DL-LiteA,u identification constraints,
(HN )

scope article). nearest relatives logic DL-Litehorn
fragments, AC0 well. next layer contains logics DL-LiteHF
core
DL-LiteHF
,


query
answering

data-complete

P
(no
matter
whether

horn
UNA adopted not). fact, logics fragments much expressive DL
Horn-SHIQ, shown enjoy data complexity query answering
Eiter et al. (2008). remains seen whether polynomial query answering practically
feasible; recent experiments DL EL (Lutz, Toman, & Wolter, 2008) indicate
may indeed case. Finally, distant relatives DL-Lite family comprise

19

fiArtale, Calvanese, Kontchakov & Zakharyaschev

upper layer picture, query answering data-complete coNP, is,
expressive DL SHIQ.
4.1 DL-Lite Family OWL 2
upcoming version 2 Web Ontology Language OWL7 defines three profiles,8
is, restricted versions language suit specific needs. DL-Lite family, notably
DL-LiteH
core (or original DL-LiteR ), basis one OWL 2 profiles, called
OWL 2 QL. According http://www.w3.org/TR/owl2-profiles/, OWL 2 QL aimed
applications use large volumes instance data, query answering
important reasoning task. OWL 2 QL, [. . . ] sound complete conjunctive query
answering performed LogSpace respect size data (assertions)
[and] polynomial time algorithms used implement ontology consistency
class expression subsumption reasoning problems. expressive power profile
necessarily quite limited, although include main features conceptual
models UML class diagrams ER diagrams. section, briefly discuss
results obtained article context additional constructs present
OWL 2.
important difference DL-Lite family OWL status
unique name assumption (UNA): assumption quite common data management,
hence adopted DL-Lite family, adopted OWL. Instead, OWL
syntax provides explicit means stating object names, say b, supposed
denote individual, b, interpreted differently, 6 b (in
OWL, constructs called sameAs differentFrom).
complexity results obtain logics form DL-LiteH
depend
whether UNA adopted (because every model DL-LiteH
KB without UNA
untangled model KB respecting UNA; see Lemma 8.10).
N
However, case logics DL-LiteF
DL-Lite , obvious
interaction UNA number restrictions (cf. Table 2). example,
0
UNA, instance checking DL-LiteF
core AC data complexity, whereas dropping
assumption results much higher complexity: Section 8, prove P-complete.
H
addition equality construct DL-LiteH
core DL-Litehorn slightly changes
data complexity query answering instance checking, rises membership
AC0 LogSpace-completeness; see Section 8. important, however,
case loose first-order rewritability query answering instance checking,
result cannot use standard database query engines straightforward manner.
Since OWL 2 profiles defined syntactic restrictions language without
changing basic semantic assumptions, chosen include OWL 2 QL
profile construct interferes UNA which, absence UNA,
would cause higher complexity. OWL 2 QL include number restrictions, even functionality constraints. Also, keys (the mechanism identifying objects
means values properties) supported, although impor7. http://www.w3.org/2007/OWL/
8. logic, profiles would called fragments defined placing restrictions OWL 2
syntax only.

20

fiThe DL-Lite Family Relations

tant notion conceptual modeling. Indeed, keys considered generalization
functionality constraints (Toman & Weddell, 2005, 2008; Calvanese, De Giacomo, Lembo,
Lenzerini, & Rosati, 2007a, 2008b), since asserting unary key, i.e., one involving
single role R, equivalent asserting functionality inverse R. Hence,
absence UNA, allowing keys would change computational properties.
already mentioned, standard OWL constructs, role disjointness, (a)symmetry (ir)reflexivity constraints, added DL-Lite logics
without changing computational behavior. Role transitivity constraints, Tra(R), as(HN )
serting R must interpreted transitive role, also added DL-Litehorn
leads increase data complexity reasoning problems NLogSpace,
although satisfiability remains P combined complexity. results found
Section 5.3.
constructs OWL 2 far supported DL-Lite logics
mention nominals (i.e., singleton concepts), Boolean operators roles, role chains.

5. Satisfiability: Combined Complexity
DL-LiteHN
bool clearly sub-logic description logic SHIQ, satisfiability problem
known ExpTime-complete (Tobies, 2001).
Section 5.1 show, however, satisfiability problem DL-LiteN
bool KBs
1
reducible satisfiability problem one-variable fragment, QL , first-order logic
without equality function symbols. satisfiability QL1 -formulas NP-complete
(see, e.g., Borger et al., 1997) logics consideration contain full Booleans
concepts, satisfiability DL-LiteN
bool KBs NP-complete well. shall also see
translations Horn Krom KBs QL1 belong Horn Krom fragments
QL1 , respectively, known P- NLogSpace-complete (see, e.g., Papadimitriou, 1994; Borger et al., 1997). Section 5.2, show simulate behavior
polynomial-space-bounded alternating Turing machines means DL-LiteHF
core KBs.
give (optimal) ExpTime lower bound satisfiability KBs languages
family containing unrestricted occurrences functionality constraints role
inclusions. Section 5.3, extend embedding QL1 , defined Section 5.1,
(HN )
logic DL-Litebool , thereby establishing upper bounds DL-LiteN
bool
fragments. Finally, Section 5.4 investigate impact role transitivity constraints.
5.1 DL-LiteN
bool Fragments: First-Order Perspective
aim section construct reduction satisfiability problem DL-LiteN
bool
KBs satisfiability QL1 -formulas. two steps: first present lengthy
yet quite natural transparent (yet exponential) reduction , shall see
proof reduction substantially optimized linear reduction .

Let K = (T , A) DL-LiteN
bool KB. Recall role (K) denotes set direct
inverse role names occurring K ob(A) set object names occurring A.
R role (K), let QR
set natural numbers containing 1 numbers q
concept q R occurs (recall ABox contain number
restrictions). Note |QR
| 2 contains functionality constraint R.

21

fiArtale, Calvanese, Kontchakov & Zakharyaschev

every object name ai ob(A) associate individual constant ai QL1
every concept name Ai unary predicate Ai (x) signature QL1 .
role R role (K), introduce |QR
|-many fresh unary predicates
q QR
T.

Eq R(x),

intended meaning predicates follows: role name Pk ,
Eq Pk (x) Eq Pk (x) represent sets points least q distinct Pk -successors
least q distinct Pk -predecessors, respectively. particular, E1 Pk (x)
E1 Pk (x) represent domain range Pk , respectively.
Additionally, every pair roles Pk , Pk role (K), take two fresh individual constants
dpk

dp
k
QL1 , serve representatives points domains Pk

Pk , respectively (provided empty). Let dr(K) = dr | R role (K) .
Furthermore, pair object names ai , aj ob(A) R role (K), take
fresh propositional variable Rai aj QL1 encode ABox assertion R(ai , aj ).9
1
induction construction DL-LiteN
bool concept C define QL -formula
C :
= ,

(Ai ) = Ai (x),

( q R) = Eq R(x),

(C) = C (x),

(C1 u C2 ) = C1 (x) C2 (x).

1

DL-LiteN
bool TBox corresponds QL -sentence x (x),
^

(x) =
C1 (x) C2 (x) .

(1)

C1 vC2

ABox translated following pair QL1 -sentences
^
^
1
=
Ak (ai )
Ak (ai ),


2

=

^

(2)

Ak (ai )A

Ak (ai )A

Pk ai aj



^

Pk ai aj .

(3)

Pk (ai ,aj )A

Pk (ai ,aj )A

every role R role (K), need two QL1 -formulas:
R (x) = E1 R(x) inv(E1 R)(inv(dr)),
^

R (x) =
Eq0 R(x) Eq R(x) ,

(4)
(5)

0
q,q 0 QR
, q >q
0
00
q >q >q q 00 QR


9. follows, slightly abuse notation write R(ai , aj ) indicate Pk (ai , aj )
R = Pk , Pk (aj , ai ) R = Pk .

22

fiThe DL-Lite Family Relations

(by overloading inv operator),
(
Eq Pk , R = Pk ,
inv(Eq R) =
Eq Pk , R = Pk ,



(
dp
k,
inv(dr) =
dpk ,

R = Pk ,
R = Pk .

Formula (4) says domain R empty range empty either:
contains constant inv(dr), representative domain inv(R).
also need formulas representing relationship propositional variables Rai aj
unary predicates role domain range: role R role (K), let R
following QL1 -sentence
^

^

^

q
^


Rai ajk Eq R(ai )

ai ob(A) qQR
aj1 ,...,ajq ob(A) k=1

jk 6=jk0 k6=k0



^


Rai aj inv(R)aj ai , (6)

ai ,aj ob(A)

inv(R)aj ai propositional variable Pk aj ai R = Pk Pk aj ai R = Pk .
Note first conjunct (6) part translation relies UNA.
Finally, DL-LiteN
bool knowledge base K = (T , A), set

h 1
h
^
^

2



R .
K = x (x)
R (x) R (x)
Rrole (K)

Rrole (K)

Thus, K universal sentence QL1 .
Example 5.1 Consider, example, KB K = (T , A)


= v P , P v A, v 2 P, > v 1 P , P v
= {A(a), P (a, a0 )}. obtain following first-order translation:
K = x (x) A(a) P aa0


P aa0 E1 P (a) P aa E1 P (a)


P a0 E1 P (a0 ) P a0 a0 E1 P (a0 )


P aa0 E1 P (a) P aa E1 P (a)


P a0 E1 P (a0 ) P a0 a0 E1 P (a0 )


P aa0 P aa E2 P (a) P a0 P a0 a0 E2 P (a0 )


P aa0 P aa E2 P (a) P a0 P a0 a0 E2 P (a0 )




P aa0 P a0 P a0 P aa0 P aa P aa P a0 a0 P a0 a0 .

(x) =

A(x) E1 P (x)



E1 P (x) A(x) A(x) E2 P (x)


> E2 P (x) E1 P (x) A(x)


E1 P (x) E1 P (dp ) E1 P (x) E1 P (dp)


E2 P (x) E1 P (x) E2 P (x) E1 P (x) . (7)




23

fiArtale, Calvanese, Kontchakov & Zakharyaschev

1
Theorem 5.2 DL-LiteN
bool knowledge base K = (T , A) satisfiable iff QL -sentence

K satisfiable.

Proof () K satisfiable model K whose domain consists
constants occurring K i.e., ob(A) dr(K) (say, Herbrand model K ).
denote domain interpretations (unary) predicates P , propositional
variables p constants QL1 P , pM , respectively. Thus, every
constant a, = a. Let D0 set constants a, ob(A). Without loss
generality may assume D0 6= .

construct interpretation DL-LiteN
bool based domain D0
inductively defined union

[

=



Wm ,

W0 = D0 .

m=0

interpretations object names ai given interpretations M,
namely, aIi =
W0 . set Wm+1 , 0, constructed adding Wm
new elements fresh copies certain elements \ D0 . new element
w0 copy w \ D0 write cp(w0 ) = w, w D0 let cp(w) = w.
set Wm \ Wm1 , 0, denoted Vm (for convenience, let W1 = ,
V0 = D0 ).
interpretations AIk concept names Ak defined taking


AIk = w | |= Ak [cp(w)] .
(8)
interpretation PkI role name Pk defined inductively union
PkI

=


[

Pkm ,



Pkm Wm Wm ,

m=0

along construction . First, role R role (K), define required
R-rank r(R, d) point taking

r(R, d) = max {0} { q QR
| |= Eq R[d] } .
follows (5) r(R, d) = q then, every q 0 QR
, |= Eq 0 R[d]
whenever q 0 q, |= Eq0 R[d] whenever q < q 0 . also define actual R-rank
rm (R, w) point w step taking
(
]{w0 Wm | (w, w0 ) Pkm }, R = Pk ,
rm (R, w) =
]{w0 Wm | (w0 , w) Pkm }, R = Pk .
basis induction set, role name Pk role(K),



Pk0 = (aM
, aj ) W0 W0 | |= Pk ai aj .

(9)

Observe that, (6), R role (K) w W0 ,
r0 (R, w) r(R, cp(w)).
24

(10)

fiThe DL-Lite Family Relations

Suppose Wm Pkm , 0, already defined.
rm (R, w) = r(R, cp(w)), roles R role (K) points w Wm , interpretation need would constructed. However, general case
may defects sense actual rank points smaller
required rank.
role name Pk role(K), consider following two sets defects Pkm :



w Vm | rm (Pk , w) < r(Pk , cp(w)) ,
k =



= w Vm | rm (Pk , w) < r(Pk , cp(w)) .
k
purpose of, say,
k identify defective points w Vm precisely
r(Pk , cp(w)) distinct Pk -arrows start (according M), arrows still
missing (only rm (Pk , w) many arrows exist). cure defects, extend Wm
Pkm respectively Wm+1 Pkm+1 according following rules:

(m
k ) Let w k , q = r(Pk , cp(w)) rm (Pk , w) = cp(w). |= Eq 0 Pk [d]
0
q 0 QR
q q > 0. Then, (5), |= E1 Pk [d] and, (4),


|= E1 Pk [dpk ]. case take q fresh copies w10 , . . . , wq0 dp
k (and set

0
0
cp(wi ) = dpk , 1 q), add Wm+1 add pairs (w, wi ), 1 q,
Pkm+1 .




(m
k ) Let w k , q = r(Pk , cp(w)) rm (Pk , w) = cp(w). |= Eq 0 Pk [d]

0
q 0 QR
q q > 0. So, (5), |= E1 Pk [d] and, (4),
0
0
|= E1 Pk [dpk ]. Take q fresh copies w1 , . . . , wq dpk (and set cp(wi0 ) = dpk ,
1 q), add Wm+1 add pairs (wi0 , w), 1 q, Pkm+1 .

Example 5.3 Consider KB K first-order translation K Example 5.1.
Consider also model K domain = {a, a0 , dp, dp },
= (E1 P )M = (E1 P )M = (E2 P )M = D,

(E2 P )M = ,
(P aa0 )M = (P a0 a)M = t.

begin construction interpretation K setting W0 = V0 = D0 = {a, a0 }
P 0 = {(a, a0 )}. compute required actual ranks r(R, w) r0 (R, w),
R {P, P } w V0 :
(i) r(P, a) = 2 r0 (P, a) = 1,
(iii) r(P , a) = 1 r0 (P , a) = 0,

(ii) r(P, a0 ) = 2 r0 (P, a0 ) = 0,
(iv) r(P , a0 ) = 1 r0 (P , a0 ) = 1.

next step, draw P -arrow fresh copy dp cure defect (i), draw
two P -arrows a0 two fresh copies dp order cure defects (ii), finally
take fresh copy dp connect P -arrow, thereby curing defect (iii).
One step unraveling construction shown Figure 4.
Observe following important property construction: m, m0 0, w Vm0
R role (K),


< m0 ,
0,
rm (R, w) =
(11)
q,
= m0 , q r(R, cp(w)),


r(R, cp(w)), > m0 .
25

fiArtale, Calvanese, Kontchakov & Zakharyaschev

.

V0

a0
V1



V2

dp

dp

.
Figure 4: Unraveling model (first three steps).
prove property, consider possible cases:
< m0 point w added Wm yet, i.e., w
/ Wm ,
rm (R, w) = 0.
= m0 m0 = 0 rm (R, w) r(R, cp(w)) follows (10).
= m0 m0 > 0 w added step m0 cure defect point
w0 Wm0 1 . means Pk role(K) either (w0 , w) Pkm0
(m 1)
0 1
w0
(w, w0 ) Pkm0 w0 k 0
. Consider former case.
k
.
Since
fresh
witnesses

picked
every time rule (km0 1 )
cp(w) = dp
k

applied, rm0 (Pk , w) = 1, rm0 (Pk , w) = 0 rm0 (R, w) = 0, every R 6= Pk , Pk .
0
suffices show r(Pk , dp
k ) 1. Indeed, |= Eq Pk [cp(w )]

R
0
q QT , have, (5), |= E1 Pk [cp(w )], so, (4), |= E1 Pk [dp
k ].
)

1.

latter
case

considered
analogously.
definition r, r(Pk , dp
k
= m0 + 1 then, role name Pk , defects w cured step m0 + 1
m0
0
applying rules (m
). Therefore, rm0 +1 (R, w) = r(R, cp(w)).
k ) (k
> m0 + 1 (11) follows observation new arrows involving w
added step m0 + 1, is, 0 role name Pk role(K),
Pkm+1 \ Pkm



Vm Vm+1



Vm+1 Vm .

(12)


follows that, R role (K), q QR
w , have:

|= Eq R[cp(w)]

iff

w ( q R)I .

(13)

Indeed, |= Eq R[cp(w)] then, definition, r(R, cp(w)) q. Let w Vm0 . Then,
(11), rm (R, w) = r(R, cp(w)) q, > m0 . follows definition
26

fiThe DL-Lite Family Relations

rm (R, w) RI w ( q R)I . Conversely, let w ( q R)I w Vm0 . Then,
(11), q rm (R, w) = r(R, cp(w)), > m0 . So, definition r(R, cp(w))
(5), |= Eq R[cp(w)].
induction construction concepts C K one readily see that, every
w ,
|= C [cp(w)]
iff
w CI .
(14)
Indeed, basis trivial B = follows (8) B = Ak (13)
B = q R, induction step Booleans (C = C1 C = C1 u C2 )
immediately follows induction hypothesis.
Finally, show A,
|=

iff

|= .

case = C1 v C2 follows (14); = Ak (ai ) = Ak (ai )
definition AIk . = Pk (ai , aj ) = Pk (ai , aj ), (aIi , aIj ) PkI iff, (12),
(aIi , aIj ) Pk0 iff, (9), |= Pk ai aj .
Thus, established |= K.
() Conversely, suppose |= K interpretation domain . construct

model K based . every ai ob(A), let
= ai and,
every R role (K), take ( 1 R)I ( 1 R)I 6= arbitrary element

otherwise, let drM = d. Next, every concept name Ak , let
k = Ak

= ( q R)I . Finally, every
and, every role R role (K) q QR
, set Eq R

role R role (K) every pair objects ai , aj ob(A), define (Rai aj )M true
iff |= R(ai , aj ). One readily check |= K . Details left reader.
q
first-order translation K K obviously lengthy provide us reasonably
low complexity results: |K | |K| + (2 + qT2 ) |role(K)| + 2 |role(K)| |ob(A)|qT . However,
follows proof lot information translation redundant
safely omitted.
define concise translation K K = (T , A) QL1 taking:
h
^

1
2
K = x (x)
R (x) R (x)

,
Rrole (K)
1

(x), R (x), R (x) defined means (1), (4), (5) (2),
respectively,
^
^
^
2
=
EqR,a R(a)
(Pk (ai , aj )) ,
(15)
aob(A)

Rrole (K)
a0 ob(A) R(a,a0 )A

Pk (ai ,aj )A

qR,a maximum number QR
qR,a many distinct ai
R(a, ai ) (here use UNA) (Pk (ai , aj )) = Pk (ai , aj ) >
2
otherwise. size size K linear size K,
respectively, matter whether numbers coded unary binary.
27

fiArtale, Calvanese, Kontchakov & Zakharyaschev

importantly, translation actually done LogSpace. Indeed,
1
2
trivially case (x), R (x), R (x), last conjunct .
2
first conjunct then, R role (K) ob(A), maximum qR,a
QR
qR,a many distinct ai R(a, ai ) A, computed using
log min(max QR
, |ob(A)|) + log |ob(A)| cells. Initially set q = 0, enumerate
object names ai incrementing current q time find R(a, ai ) A. stop
q = max QR
reach end object name list. resulting qR,a maximum
number QR
exceeding q.
Example 5.4 translation K KB K Example 5.1 looks follows:
K = x (x) A(a) E1 P (a) E1 P (a0 ),
(x) defined (7).
1

Corollary 5.5 DL-LiteN
bool KB K satisfiable iff QL -sentence K satisfiable.

Proof claim follows fact K satisfiable iff K satisfiable. Indeed,
|= K clearly |= K . Conversely, |= K one construct new model
M0 based domain taking:
0



k = Ak , concept names Ak ;
0

Eq RM = Eq RM , R role (K) q QR
T;
0

(Rai aj )M true iff R(ai , aj ) A;
0



= ai , ai ob(A);
0

drM = drM , R role (K).
0

claim M0 |= K . Indeed, Eq RM = Eq RM , every R role (K) q QR
.
1
2
follows M0 |= x (x) M0 |= x R (x). definition, M0 |=VA , M0 |=
q
M0 |= x R (x). remains show M0 |= R . Suppose M0 |= i=1 Raaji ,
R(a, aji ) A, distinct aj1 , . . . , ajq , q QR
. Clearly, q qR,a
|= Eq R(a) thus M0 |= Eq R(a).
q
immediate consequence Corollary 5.5, facts translation
done LogSpace, satisfiability problem QL1 -formulas NP-complete
DL-Litebool contains Booleansand encode full propositional logicwe
obtain following result:
F
Theorem 5.6 Satisfiability DL-LiteN
bool , DL-Litebool DL-Litebool knowledge bases
NP-complete combined complexity.
1

Observe K DL-LiteN
krom KB K Krom fragment QL .
F
Theorem 5.7 Satisfiability DL-LiteN
, DL-Lite DL-Lite knowledge bases,
{core, krom}, NLogSpace-complete combined complexity.

28

fiThe DL-Lite Family Relations

Proof satisfiability problem Krom formulas prefix form x (as
K ) NLogSpace-complete (see, e.g., Borger et al., 1997, Exercise 8.3.7)
LogSpace reduction, satisfiability NLogSpace logics mentioned
theorem. lower bound, suffices recall NLogSpace-hardness
satisfiability propositional Krom formulas proved reduction directed graph
reachability problem using core propositional formulas (Borger et al., 1997),
satisfiability logics NLogSpace-hard.
q
1

K DL-LiteN
horn KB K belongs universal Horn fragment QL .
F
Theorem 5.8 Satisfiability DL-LiteN
horn , DL-Litehorn DL-Litehorn KBs P-complete
combined complexity.

Proof QL1 contains function symbols K universal, satisfiability K
LogSpace-reducible satisfiability set propositional Horn formulas, namely,
formulas obtained K replacing x constants occurring
K . remains recall satisfiability problem propositional Horn formulas
P-complete (see, e.g., Papadimitriou, 1994), gives required upper bound
q
DL-LiteN
horn lower bound DL-Litehorn .
5.2 DL-LiteHF
core ExpTime-hard
Unfortunately, translation constructed previous section cannot extended
logics form DL-LiteHN
number restrictions role inclusions.

section show satisfiability problem DL-LiteHF
core KBs ExpTime-hard,
matches upper bound satisfiability DL-LiteHN
KBs
even binary coding
bool
natural numbers (Tobies, 2001).
Note first that, although intersection allowed left-hand side DL-LiteHF
core
concept inclusions, certain cases (when right-hand side consistent) simulate
using role inclusions functionality constraints. Suppose knowledge base K
contains concept inclusion form C1 u C2 v C. Define new KB K0 replacing
axiom K following set new axioms, R1 , R2 , R3 , R12 , R23 fresh
role names:
C1 v R1

C2 v R2 ,

(16)

R1 v R12 ,

R2 v R12 ,

(17)

2 R12 v ,
R1

v

(18)

R3 ,

(19)

R3 v C,

(20)

R3 v R23 ,



2 R23

R2 v R23 ,

v .

(21)
(22)

Lemma 5.9 (i) |= K0 |= K, every interpretation I.
(ii) |= K C 6= model 0 K0 domain
agrees every symbol K.
29

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Proof (i) Suppose |= K0 x C1I C2I . (16), (x, y) R1I ,
, whence
(R1 )I , z (x, z) R2I . (17), {(x, y), (x, z)} R12
= z view (18). (19), (R3 )I hence u (u, y) R3I
(x, y) RI . Finally, follows
u (R3 )I . (20), u C . (21), (u, y) R23
23
(22) u = x, x C . Thus, |= K.
(ii) Take point c C define extension 0 new role names
setting:
0

R1I = {(x, x) | x C1I },
0

R2I = {(x, x) | x C2I },
0

R3I = {(x, x) | x (C1 u C2 )I } {(c, x) | x (C1 u C2 )I },
0

0

= RI RI
R12
1
2

0



0

0

0

= RI RI .
R23
2
3

readily seen 0 satisfies axioms (16)(22), 0 |= K0 .

q

position prove following:
Theorem 5.10 Satisfiability DL-LiteHF
core KBs ExpTime-hard combined complexity
(with without UNA).
Proof prove theorem two steps. First consider logic DL-LiteHF
horn
show encode behavior polynomial-space-bounded alternating Turing machines (ATMs, short) means DL-LiteHF
horn KBs. APSpace = ExpTime,
APSpace class problems recognized polynomial-space-bounded ATMs (see, e.g.,
Kozen, 2006), establish ExpTime-hardness satisfiability DL-LiteHF
horn . Then,
using Lemma 5.9, show get rid conjunctions left-hand side
concept inclusions involved encoding ATMs thus establish ExpTime-hardness
DL-LiteHF
core .
Without loss generality, consider ATMs binary computational
trees. means that, every non-halting state q every symbol tape
alphabet, precisely two instructions form
(q, a) ;0M (q 0 , a0 , d0 )



(q, a) ;1M (q 00 , a00 , d00 ),

(23)

d0 , d00 {, } (resp., ) means move head right (resp., left) one cell.
remind reader non-halting state either and-state or-state.
Given ATM M, polynomial function p(n) every run every
input length n use p(n) tape cells, input word ~a = a1 , . . . , ,
construct DL-LiteHF
horn knowledge base KM,~a following properties: (i) size
KM,~a polynomial size M, ~a, (ii) accepts ~a iff KM,~a satisfiable.
Denote Q set states tape alphabet M.
encode instructions M, need following roles:
Sq , Sq0 , Sq1 , q Q: informally, x (Sq )I , interpretation I, means
x represents configuration state q, x (Sqk )I means
next state, according transition ;kM , q, k {0, 1};
30

fiThe DL-Lite Family Relations

Hi , Hi0 , Hi1 , p(n): x (Hi )I means x represents configuration
head scans ith cell, x (Hik )I that, according transition
;kM , k {0, 1}, next configuration head scans ith cell;
0 , C 1 , p(n) : x (C )I means x represents
Cia , Cia
ia
ia
k )I that, according
configuration ith cell contains a, x (Cia
;kM , k {0, 1}, next configuration ith cell contains a.

intended meaning encoded using following concept inclusions: every
instruction (q, a) ;kM (q 0 , a0 , ) every < p(n),

k
k
Sq u Hi u Cia
v Hi+1
u Sqk0 u Cia
0,

(24)

every instruction (q, a) ;kM (q 0 , a0 , ) every i, 1 < p(n),

k
k
Sq u Hi u Cia
v Hi1
u Sqk0 u Cia
0.

(25)

preserve symbols tape active cell, use following
concept inclusions, k {0, 1}, i, j p(n) j 6= i, :

k
Hj u Cia
v Cia
.

(26)

synchronize roles, need two (functional) roles Tk number role
inclusions added TBox: k {0, 1}, p(n), q Q, ,
k
Cia
v Cia ,

Hik v Hi ,

Sqk v Sq ,

(27)

k
Cia
v Tk ,

Hik v Tk ,

Sqk v Tk ,

(28)

2 Tk v .

(29)

remains encode acceptance conditions ~a. done help
role names Yk , k {0, 1}, concept name A:
Sq v A,

q accepting state,

Yk v Tk ,
2 Tk
Tk u
Sq u Yk
Sq

(31)

v ,
v

(32)

Yk ,

v A,

u Y0 u Y1 v A,

(30)

(33)
q or-state,

(34)

q and-state.

(35)

TBox DL-LiteHF
horn knowledge base KM,~a constructing consists
axioms (24)(35) together auxiliary axiom
u v ,

(36)

fresh concept name. ABox KM,~a comprised following
assertions, object names u:
Sq0 (u, s),

q0 initial state,

H1 (u, s),
Ciai (u, s),

(37)
(38)

p(n), ai ith symbol input tape,

(39)
(40)

D(s).
31

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Clearly, KM,~a = (T , A) DL-LiteHF
a.
horn KB size polynomial size M, ~
Lemma 5.11 ATM accepts ~a iff KB KM,~a satisfiable.
Proof () Suppose accepts ~a |= KM,~a interpretation I.
reconstruct full computation tree ~a induction following way.
Let root tree point sI . (37)(39), represents initial configuration ~a accordance intended meaning roles Sq0 , H1 Ciai
explained (it matter if, instance, also sI (H5 )I ).
Assume already found point x representing configuration
c = b1 , . . . , bi1 , (q, bi ), bi+1 , . . . , bp(n) ,

(41)

q current non-halting state head scans ith cell containing bi .
means
x (Sq )I (Hi )I


x (Cjb
) ,
j

j p(n).

Assume also contains two instructions form (23) (q, bi ), q nonhalting. (q, bi ) ;kM (q 0 , b0 , ), k = 0 1, then, (24) (26),
points ys , yh yj , j p(n),
(x, ys ) (Sqk0 )I ,

k
(x, yh ) (Hi+1
)I ,

k
(x, yi ) (Cib
0) ,

k
) ,
(x, yj ) (Cjb
j

j 6= i.

0 , C 0 C 0 , j 6= i, sub-roles functional role ,
(28)(29), Sq00 , Hi+1
k
jbj
ib0i
points ys , yh yj coincide; denote point xk . (27),
have:

(x, xk ) TkI ,



) (Cib
xk (Sq0 )I (Hi+1
0)


) ,
xk (Cjb
j

j 6= i.

Similarly, (q, bi ) ;kM (q 00 , b00 , ), k = 0 1, then, (25) (26),
point xk
(x, xk ) TkI ,



) (Cib
xk (Sq00 )I (Hi1
00 )


) ,
xk (Cjb
j

j 6= i.

Thus, k = 0, 1, xk Tk -successor x representing configuration ck
executed (q, bi ) ;kM (q 00 , b00 , d) c; case ck called k-successor c.
According (30), every point constructed computation tree ~a representing configuration accepting state AI . Suppose now, inductively,
x represents configuration c form (41), q or-state, xk represents ksuccessor c (x, xk ) TkI , k = 0, 1, one xk , say x0 , AI .
view (33), x0 (Y0 )I . T0 functional (32) Y0 sub-role
T0 (31), (x, x0 ) Y0I , so, (34), x AI . case x and-state
considered analogously help (35).
Since accepts ~a, conclude sI AI , contrary (36) (40).
() Conversely, suppose accept ~a. Consider full computation
tree (, <0 <1 ) nodes labeled configurations way root
labeled initial configuration
(q0 , a1 ), a2 , . . . , , an+1 , . . . , ap(n) ,
32

fiThe DL-Lite Family Relations

(where ai , n + 1 p(n), blank), node x tree labeled
non-halting c form (41) contains two instructions form (23),
x one <0 -successor labeled 0-successor c one <1 -successor labeled
1-successor c. (It emphasized (, <0 <1 ) tree, different
nodes may labeled configuration.)
use tree construct interpretation = (I , ) follows:
= {u}, u
/ ;
sI root uI = u;
DI = {sI };
k )I , (x, x ) (C k )I , (x, x ) (C k )I , j 6= i,
(x, xk ) (Sqk0 )I , (x, xk ) (Hi+1
k
k
jbj
ib0
iff x labeled c form (41), (q, bi ) ;kM (q 0 , b0 , ) x <k xk , k = 0, 1;
k )I , (x, x ) (C k )I , (x, x ) (C k )I , j 6= i,
(x, xk ) (Sqk0 )I , (x, xk ) (Hi1
k
k
jbj
ib0
k
iff x labeled c form (41), (q, bi ) ;M (q 0 , b0 , ) x <k xk , k = 0, 1;

(u, sI ) (Sq0 )I , (u, sI ) (H1 )I , (u, sI ) (Ciai )I , p(n) extensions
roles Sq , Hi Cia defined according (27);
TkI = <k , k = 0, 1;
Y0I , Y1I AI defined inductively:
Induction basis: x labeled accepting configuration, x AI .
Induction step: (i) x <k xk , k = 0, 1, xk AI , (x, xk ) YkI ; (ii)
x or-state (respectively, and-state) (x, xk ) YkI (respectively,
all) k {0, 1}, x AI .
follows given definition |= KM,~a . Details left reader.

q

lemma proved establishes satisfiability DL-LiteHF
horn KBs
ExpTime-hard. next aim show one eliminate conjunctions
left-hand side TBox axioms (24)(26), (33)(35). help
Lemma 5.9. applying it, check first KM,~a satisfiable satisfiable
interpretation |= KM,~a C 6= , every C occurring
axiom form C1 u C2 v C K. Consider, instance, axiom (24) assume
|= KM,~a , (Sqk0 )I = . Then, construct new interpretation 0 adding two
0
0
new points, say x y, domain I, setting (x, y) (Sqk0 )I , (x, y) (Sq0 )I ,
0
0
0
(x, y) (Tk )I . Furthermore, q 0 accepting state, also set AI (x, y) YkI .
One readily check 0 still model KM,~a . conjuncts (24)
remaining axioms considered analogously.
application Lemma 5.9 axiom form C1 uC2 v C C2 = C20 uC200
obtain, (16)(22), new KB K0 concept inclusion form C20 uC200 v R1 ,
also requires treatment means lemma. able this,
33

fiArtale, Calvanese, Kontchakov & Zakharyaschev

00

check K0 satisfiable interpretation 00 (R1 )I 6= . Suppose
0
0 |= K0 (R1 )I = . construct 00 adding two new points, say x
0
0
0 , RI 0 RI 0 .
y, domain 0 , adding x C (x, y) R1I , R12
23
3
readily seen 00 |= K0 .
noted proof depend whether UNA adopted
not.
q
immediate consequence obtain:
Corollary 5.12 Satisfiability DL-LiteHF
DL-LiteHN
KBs without UNA


ExpTime-complete combined complexity, {core, krom, horn, bool}.
5.3 Reconciling Number Restrictions Role Inclusions
seen previous section, unrestricted interaction number restrictions role inclusions allowed logics form DL-LiteHN
results high

combined complexity satisfiability. Section 6.2, shall see data complexity
instance checking query answering also becomes unacceptably high logics.
quick look proof Theorem 5.10 reveals culprit: interplay role
inclusions R1 v R, R2 v R functionality constraints 2 R v , effectively mean
R1 (x, y) R2 (x, z) = z. section study case
interplay allowed.
(HN )
Recall Section 2.1 DL-Lite
TBoxes , {core, krom, horn, bool},
satisfy following conditions:
(A1 ) may contain positive occurrences qualified number restrictions q R.C,
C conjunction concepts allowed right-hand side -concept
inclusions;
(A2 ) q R.C occurs , contain negative occurrences number
restrictions q 0 R q 0 inv(R) q 0 2;
(A3 ) R proper sub-role , contain negative occurrences
q R q inv(R) q 2.
(HN )

DL-Lite
TBoxes contain role constraints Dis(R1 , R2 ), Asym(Pk ), Sym(Pk ),
Irr(Pk ), Ref(Pk ).
main aim section prove following theorem develop technical
(HN )
tools need investigate data complexity reasoning DL-Litebool
sublogics later paper.
(HN )

Theorem 5.13 combined complexity, (i) satisfiability DL-Litebool KBs NP(HN )
complete; (ii) satisfiability DL-Litehorn KBs P-complete; (iii) satisfiability
(HN )
(HN )
DL-Litekrom DL-Litecore KBs NLogSpace-complete.

34

fiThe DL-Lite Family Relations

(HN )

Let us consider first sub-language DL-Litebool

without qualified number restric(HN )

tions role constraints mentioned above; denote DL-Litebool . sub-

(HN )

language required purely technical reasons. Section 7, also use DL-Litehorn ,
need core Krom fragments.
(HN )
Suppose given DL-Litebool
KB K = (T , A). Let Id distinguished
role name. use simulate identity relation required encoding role
constraints. assume either K contain Id satisfies following
conditions:
(Id1 ) Id(ai , aj ) iff = j, ai , aj ob(A),


Id = {1},
(Id2 ) > v Id, Id v Id , QId
= QT
(Id3 ) Id allowed role inclusions form Id v Id Id v R.
follows, without loss generality, assume
0

0
R

(Q) QR
QT whenever R vT R
0

(for case always add missing numbers QR
, e.g., introducing
fictitious concept inclusions form v q R0 ).
Now, way Section 5.1, define two translations e e K
one-variable fragment QL1 first-order logic. former translation, e , retains
information relationships ABox objects, show every model
Ke unraveled model K. define e taking:
h
Ke = x (x) R (x)

^

R (x) R (x)





Rrole (K)

h

1

2



^


R

Rrole (K)

^

Rai aj R0 ai aj



,

RvR0
ai ,aj ob(A)

1

2

(x), , , R (x), R (x) R (1)(6)
^
^

R (x) =
Eq R(x) Eq R0 (x) .
RvR0
inv(R)vinv(R0 )T

(42)

qQR


following lemma analogue Theorem 5.2:
(HN )

Lemma 5.14 DL-Litebool

KB K satisfiable iff QL1 -sentence Ke satisfiable.

Proof proof basically follows lines proof Theorem 5.2 modifications. present modified unraveling construction here; converse direction exactly
Theorem 5.2.
equivalence class [Ri ] = {Rj | Ri Rj } select single role (a representative
class) denote repT (Ri ). extending Pkm Pkm+1 , use following
modified curing rules:
35

fiArtale, Calvanese, Kontchakov & Zakharyaschev



(m
k ) Pk 6= repT (Pk ) nothing: defects cured repT (Pk ). Otherwise, let

w k , q = r(Pk , cp(w)) rm (Pk , w) = cp(w). |= Eq0 Pk [d]
q 0 q > 0. Then, (5), |= E1 Pk [d] and, (4), |= E1 Pk [dp
k ].
0 ) = dp , 1 q),
case take q fresh copies w10 , . . . , wq0 dp
(and
set
cp(w

k
k
add Wm+1

add pairs (w, wi0 ), 1 q, Pjm+1 Pk vT Pj (including
Pj = Pk );
add pairs (wi0 , w), 1 q, Pjm+1 Pk vT Pj ;
Id occurs K, add pairs (wi0 , wi0 ), 1 q, Pjm+1 Id vT Pj .


(m
k ) rule mirror image (k ): Pk dpk replaced everywhere

Pk dpk , respectively; see proof Theorem 5.2.

follows definition Id never
resulting
defects interpreted

interpretation identity relation IdI = (w, w) | w ; interpretations
roles respect role inclusions, i.e., R1I R2I whenever R1 vT R2 .
remains show constructed interpretation indeed model K.
First, (11) trivially holds Id required actual ranks equal 1. Second, (11) holds R R 6= Id R proper sub-roles: proof exactly
Theorem 5.2, taking account cure defects single role
equivalence class that, (42), R0 [R], r(R0 , cp(w)) = r(R, cp(w))
r(inv(R), cp(w)) = r(inv(R0 ), cp(w)). follows (13) holds Id role R
without proper sub-roles. However, (13) necessarily hold roles R proper
sub-roles: follows construction, actual rank may greater required
rank, case following:
|= Eq R[cp(w)]



w ( q R)I .

However, enough purposes. induction structure concepts
using (A3 ), one show |= C1 v C2 whenever |= x (C1 (x) C2 (x)),
concept inclusion C1 v C2 , therefore, |= . also |= (see proof
Theorem 5.2) thus |= K.
q
Remark 5.15 follows proofs Theorem 5.2 Lemma 5.14 that,
(HN )
DL-Litebool
KB K = (T , A), every model Ke induces model IM K
following properties:
(ABox) ai , aj ob(A), (aIi , aIj ) RIM iff R(ai , aj ) CleT (A),
CleT (A) =




R2 (ai , aj ) | R1 (ai , aj ) A, R1 vT R2 .

(forest) object names ob(A) induce partitioning IM disjoint labeled
trees Ta = (Ta , Ea , `a ) nodes Ta , edges Ea , root aIM , labeling function
`a : Ea role (K) \ {Id, Id }.
36

fiThe DL-Lite Family Relations

(copy) function cp : IM ob(A) dr(K)
cp(aIM ) = ob(A),
cp(w) = dr if, w0 Ta , (w0 , w) Ea `a (w0 , w) = inv(R).
(iso) R role (K), labeled subtrees generated elements w IM
cp(w) = dr isomorphic.
(concept) w B IM iff |= B [cp(w)], basic concept B K w IM .
fi


(role) IdIM = (w, w) fi w IM and, every role name Pk ,
PkIM =






(aIi , aIj ) | R(ai , aj ) A, R vT Pk

(w, w) | Id vT Pk

[

(w, w0 ) Ea | `a (w, w0 ) = R, R vT Pk .
aob(A)

model called untangled model K (the untangled model K induced
M, precise).
translation e generalizes thus suffers exponential blowup.
define optimized translation, e , linear size K, taking:
h
^

1
2

e ,
R (x) R (x)
Ke = x (x) R (x)
Rrole (K)
1

(x), R (x), R (x), R (x) defined (1), (42), (4), (5) (2),
respectively,
^
^
^
2
e R(a)
Ae =
EqR,a

(43)
(Pk (ai , aj ))e ,
aob(A)

Rrole (K)
a0 ob(A) R(a,a0 )CleT (A)

Pk (ai ,aj )A

e
e
qR,a
maximum number QR
qR,a many distinct ai
R(a, ai ) CleT (A) (here use UNA) (Pk (ai , aj ))e = Pk (ai , aj ) CleT (A)

> otherwise; cf. (15). note QR
= {1}, roles R role (K),
translation depend whether UNA adopted not.
following corollary proved similarly Corollary 5.5:
(HN )

Corollary 5.16 DL-Litebool

KB K satisfiable iff QL1 -sentence Ke satisfiable.

clear translation e computed NLogSpace (for combined
1
complexity). Indeed, readily seen (x), R (x), R (x), R (x), .
2
order compute Ae , need able check whether R(ai , aj ) CleT (A): test
performed non-deterministic algorithm using logarithmic space |role (K)|
(it basically standard directed graph reachability problem,
NLogSpace-complete; see, e.g., Kozen, 2006); done using N log |role (K)| +
2 log |ob(A)| cells work tape, N constant (in fact, N = 3 enough: one
37

fiArtale, Calvanese, Kontchakov & Zakharyaschev

store R, current role R0 path length graph reachability subroutine,
also bounded log |role (K)|). Therefore, translation e computed
NLogSpace transducer.
(HN )
show satisfiability DL-Litebool KBs easily reduced satisfiability
(HN )

(HN )

DL-Litebool
KBs. First, assume DL-Litebool KBs contain role symmetry
asymmetry constraints Asym(Pk ) equivalently replaced Dis(Pk , Pk )
Sym(Pk ) Pk v Pk (it noted introduction Pk v Pk
TBox violate (A3 )). following lemma allows us get rid qualified number
restrictions well role disjointness, reflexivity irreflexivity constraints:
(HN )

Lemma 5.17 every DL-Litebool
KB K = (T , A)

(HN )

KB K0 = (T 0 , A0 ), one construct DL-Litebool

every untangled model IM K model K0 , provided
R1 (ai , aj ), R2 (ai , aj ) CleT (A) Dis(R1 , R2 ) 0 ,
R(ai , ai ) CleT (A) Irr(R) 0 ;

(44)

every model 0 K0 gives rise model K based domain 0
agrees 0 symbols K0 .
(HN )

(HN )

K0 DL-Litehorn KB K DL-Litehorn

KB.

Proof First, every pair R, C q R.C occurs 0 , introduce fresh role
name RC . replace (positive) occurrence q R.C 0 q RC
add following concept role inclusions TBox:

vC
RC



RC v R.

repeat procedure occurrences qualified number restrictions eliminated. Denote 00 resulting TBox. Observe (A1 ) (A2 ) ensure 00
satisfies (A3 ). also notice C occurs right-hand side extra
axioms thus 00 belongs fragment 0 . clear that, since
q R.C occur positively, every model 00 model 0 . Conversely, every
model 0 0 , model 00 00 based domain 00 coincides
00 = {(w, u) RI 0 | u C 0 }, new role R . So,
0 symbols 0 RC
C
without loss generality may assume 0 = 00 .
Let
0
0
0
Tirref
Tdisj
,
0 = T00 Tref
0 , T0
0
Tref
irref Tdisj sets role reflexivity, irreflexivity disjointness con(HN )

straints 0 T00 remaining DL-Litebool
TBox. Let




0
T10 = > v Id, Id v Id
Id v P | Ref(P ) Tref
,


A01 = Id(ai , ai ) | ai ob(A0 ) .
(HN )

construct K modifying DL-Litebool
KB K0 = (T00 T10 , A0 A01 ) two steps:
0 , take fresh role name
Step 1. every reflexivity constraint Ref(P ) Tref
P
38

fiThe DL-Lite Family Relations

add new role inclusion SP v P TBox;
replace every basic concept B T00 B SP , defined inductively follows:
ASP = A, concept name A,
( q R)SP = q R, role R
/ {P, P },
( q P )SP = (q 1) SP ( q P )SP = (q 1) SP , q 2,
(P )SP = > (P )SP = >;
replace R(ai , aj ) A0 R 0 P SP (ai , aj ) whenever 6= j.
Intuitively, split role P irreflexive part SP Id. Note P
reflexive proper sub-role then, (A3 ), restrictions maximal number
P -successors P -predecessors, therefore SP Ref(P ) 0 . Let (T1 , A)
(HN )
resulting DL-Litebool
KB. Clearly, (T1 , A) satisfies (Id1 )(Id3 ). Observe
CleT1 (A) role(K0 ) = CleT 0 0 (A0 ),
0

1

(45)

role(K0 ) means restriction role names K0 .
Let IM untangled model (T1 , A). show IM |= T00 . Consider role P
Ref(P ) 0 . Notice SP proper sub-roles T1 IdIM disjoint
SPIM . Thus, SPIM IdIM P IM
(*) (B SP )IM B IM , B = q R q 2, whenever Ref(P ) 0 , R {P, P }
P proper sub-role 0 .
P proper sub-roles 0 (i.e., proper sub-roles T1 different SP Id)
SPIM IdIM = P IM . So, basic concepts B T00 covered (*),
B IM = (B SP )IM . follows (A3 ) IM |= T00 .
0
0 }
Step 2. Next take account set = Tdisj
{Dis(Pk , Id) | Irr(Pk ) Tirref
disjointness constraints modifying KB (T1 , A) constructed previous step.
Observe R1 v logical consequence {Dis(R1 , R2 )} whenever R1 vT R2 .
Let = T1 T2 , T2 defined taking
fi


T2 = R1 v fi R1 vT1 R2 either Dis(R1 , R2 ) Dis(R2 , R1 ) .
(role), untangled model IM (T , A) R1 , R2 role (K), IM |= Dis(R1 , R2 )
R1 (ai , aj ), R2 (ai , aj ) CleT1 (A), which, (45), means
R1 (ai , aj ), R2 (ai , aj ) CleT 0 0 (A0 ). So, (44) holds every untangled model IM
0

1

0 . IdIM identity relation,
(T , A) also model T1 thus, IM |= Tdisj
0
0
0
IM |= Tref Tirref . (45), IM |= shown above, IM |= T00 .
Therefore, IM |= K0 .
Conversely, suppose 0 model K0 . Let interpretation IdI
0
0
0
identity relation, SPI = P \ IdI , P Ref(P ) 0 , AI = AI ,
0
0
P = P aI = aI , concept, role object names A, P K0 . Clearly,
|= (T00 T10 , A0 A01 ). definition SP , |= T1 and, since |= D, obtain
|= T2 thus |= . (45), |= A, whence |= K.
q

39

fiArtale, Calvanese, Kontchakov & Zakharyaschev

(HN )

Now, follows Lemma 5.17, given DL-Lite
KB K0 , {krom, horn,

(HN )
bool}, compute DL-Litebool
KB K using LogSpace transducer (which
essentially required checking whether R 0 P ). immediately obtain Theorem 5.13
Lemma 5.14 observing that, {krom, horn, bool}, Ke belongs
respective first-order fragment condition (44) checked NLogSpace
(HN )
(computing CleT (A) requires directed graph accessibility checks). result DL-Litecore
(HN )
follows corresponding result DL-Litekrom .
5.4 Role Transitivity Constraints
(HN )+

consider languages DL-Lite
, {core, krom, horn, bool}, extend
(HN )
DL-Lite
role transitivity constraints form Tra(Pk ). remind reader
role called simple (see, e.g., Horrocks et al., 2000) transitive sub-roles
(including itself) simple roles R allowed concepts form q R,
q 2. particular, contains Tra(P ) P P simple, cannot
contain occurrences concepts form q P q P , q 2.
(HN )+
DL-Lite
KB K = (T , A), define transitive closure TraT (A)
taking


TraT (A) = P (ai1 , ) | ai2 . . . ain1 P (ai1 , aij+1 ) A, 1 j < n, Tra(P ) .
Clearly, TraT (A) computed NLogSpace: pair (ai , aj ) objects ob(A),
add P (ai , aj ) TraT (A) iff P -path length < |ob(A)| ai aj
(recall directed graph reachability problem NLogSpace-complete).
(HN )+

(HN )

Lemma 5.18 DL-Lite
KB (T , A) satisfiable iff DL-Lite
KB (T 0 , A0 )
0
satisfiable, results removing transitivity axioms
A0 = CleT (TraT (CleT (A))).
Proof Indeed, KB (T 0 , A0 ) satisfiable construct model described
proofs Lemmas 5.14 5.17 take transitive closure P every P
Tra(P ) (and update RI P vT R). P P simple, contains
axioms imposing upper bounds number P -successors predecessors,
resulting interpretation must model (T , A). converse direction trivial. q
note analogue Remark 5.15 also holds case: replace CleT (A)
CleT (TraT (CleT (A))) (ABox) take transitive closure transitive subrole (role).
Remark 5.19 noted two different reasons reduction
Lemma 5.18 NLogSpace rather LogSpace (as reduction is). First,
order compute CleT (A), pair ai , aj , one find path directed
graph induced role inclusion axioms. Second, order compute TraT (CleT (A)), one
find path graph induced ABox itself. So, concerned
data complexity, CleT (A) computed LogSpace (in fact, AC0 , shall
40

fiThe DL-Lite Family Relations

see Section 6.1) role inclusion graph (and hence size) depend
A. second reason, however, dangerous data complexity shall see
Section 6.1.
consequence Lemma 5.18 Theorem 5.13 obtain following:
(HN )+

Corollary 5.20 combined complexity, (i) satisfiability DL-Litebool
complete; (ii)
(HN )+
DL-Litekrom

(HN )+
satisfiability DL-Litehorn KBs P-complete;
(HN )+
DL-Litecore
KBs NLogSpace-complete.

KBs NP-

(iii) satisfiability

Note KBs contain number restrictions form q R,
q 2, (as extensions DL-LiteH
languages) result depend
UNA.
Remark 5.21 noted role disjointness, symmetry, asymmetry transitivity constraints added logics DL-LiteHF
DL-LiteHN

,
{core, krom, horn, bool}, without changing combined complexity satisfiability problems (which, Corollary 5.12, ExpTime-complete). Indeed, follows
Theorem 10 Glimm et al. (2007), KB satisfiability extension SHIQ
role conjunction ExpTime length role conjunctions bounded
constant (in case, constant 2 Dis(R1 , R2 ) encoded
(R1 u R2 ).> v ; Asym(R) dealt similarly). conjecture role reflexivity irreflexivity constraints change complexity either.

6. Instance Checking: Data Complexity
far assumed whole KB K = (T , A) input satisfiability problem. According classification suggested Vardi (1982), considering
combined complexity. Two types complexity knowledge bases are:
schema (or TBox ) complexity, TBox regarded input,
ABox assumed fixed;
data (or ABox ) complexity, ABox regarded input.
easy see schema complexity satisfiability problem logics
considered coincides corresponding combined complexity. section,
analyze data complexity satisfiability instance checking.
(HN )

H
6.1 DL-LiteN
bool , DL-Litebool DL-Litebool

AC0

follows, without loss generality assume role concept names
given knowledge base K = (T , A) occur TBox write role(T ), role (T )
dr(T ) instead role(K), role (K) dr(K), respectively; set concept names
(HN )
denoted con(T ). section reduce satisfiability DL-Litebool KBs model
checking first-order logic. end, fix signature containing two unary predicates
Ak Ak , concept name Ak , two binary predicates Pk Pk , role
name Pk .
41

fiArtale, Calvanese, Kontchakov & Zakharyaschev

(HN )

Consider first case DL-Litebool
KB K. represent ABox K
first-order model AA signature. domain AA ob(A) and,
ai , aj ob(A) predicates Ak , Ak , Pk Pk signature,
AA |= Ak [ai ]

iff

Ak (ai ) A,

AA |= Pk [ai , aj ]

iff

Pk (ai , aj ) A,

AA |= Ak [ai ]

iff

Ak (ai ) A,

AA |= Pk [ai , aj ]

iff

Pk (ai , aj ) A.

construct first-order sentence signature (i) depends
depend A, (ii) AA |= iff Ke satisfiable.
simplify presentation, denote ext(T ) extension following
concept inclusions:
0
0
00
q 0 R v q R, R role (T ) q, q 0 QR
q > q q > q > q
00
R
q QT ,
0
0
q R v q R0 , q QR
R v R inv(R) v inv(R ) .
V
Clearly, (ext(T )) (x) equivalent (in first-order logic) (x)T R (x) Rrole (T ) R (x);
see (1), (5) (42).
Let Bcon(T ) set basic concepts occurring (i.e., concepts form
q R, con(T ), R role (T ) q QR
). indicate basic concepts
hold hold domain element first-order model Ke , use functions
: Bcon(T ) {>, }, called types. Denote Tp set types
(there 2|Bcon(T )| them). complex concept C, define (C) induction:
(C) = (C) (C1 u C2 ) = (C1 ) (C2 ). propositional variable-free formula
^

=
(C1 ) (C2 )
C1 vC2 ext(T )

ensures type consistent concept role inclusions .
emphasized built > using Boolean connectives therefore
depend particular domain element AA . following formula true given
1
2
element x AA type (see Ae ; (2) (43), respectively):
(x) =

^


(Ak (x) (Ak )) (Ak (x) (Ak ))



Ak con(T )

^

^

Eq RT (x) ( q R)



^



Rrole (T ) qQR



xy PkT (x, y) Pk (x, y) ,

Pk role(T )

Eq RT (x) RT (x, y), R role (T ), abbreviations defined
^
^

Eq RT (x) = y1 . . . yq
(yi 6= yj )
RT (x, yi ) ,
1i<jq

RT (x, y) =

_
Pk vT

Pk (x, y)



_
Pk vT

R

42

(46)

1iq

Pk (y, x).
R

(47)

fiThe DL-Lite Family Relations

Clearly, R(ai , aj ) CleT (A) iff AA |= RT [ai , aj ] AA |= Eq RT [a] iff
least q distinct R-successors CleT (A) (and thus every model K).
Without loss generality may assume role (T ) = {R1 , . . . , Rk } 6= . Denote
Tpk set k-tuples ~ containing type dri Tp role Ri role (T ).
set
_
~
x (x),

=
k
~
Tp


(dr1 ,...,drk )



(x)

=

_

(x)

^




dr






Tp

Ri role (T )

^



(Ri )

Ri role (T )

_



ds (Ri ) inv(dri ) (inv(Ri )) .

Srole (T )

explain meaning subformulas , assume (T , A) satisfiable. order
construct model Ke first-order model AA , specify basic
concepts contain given constant Ke . words, select type
dri dr(T ) ob(A). formula says one select k-tuple
types ~ = (dr1 , . . . , drk ) Tpk one disjuncts true AA .
k-tuple fixes witness part model M, consisting dri , determines
basic concepts dri belong to. disjunct says (having fixed
witness part model), every ob(A), type (determining basic
concepts belongs to)
consistent information (cf. (x));
also consistent concept role inclusions (cf. );
);
dr1 , . . . , drk consistent concept role inclusions (cf. dr


role Ri nonempty domain (i.e., either ds > Ri )
nonempty range, particular, inv(dri ) (inv(Ri )) = >; see also R (x) defined
(4).
Lemma 6.1 AA |= iff Ke satisfiable.
~
Proof () Fix ~ = (dr1 , . . . , drk ) Tpk AA |= x (x). Then,
~

ob(A), fix type respective disjunct (x) holds AA
denote . Define first-order model domain ob(A) dr(T ) taking:
|= B [c] iff c (B) = >, c ob(A) dr(T ) B Bcon(T )
(B unary predicate B defined p. 22). easy check |= Ke .
() Suppose Ke satisfiable. model Ke domain
ob(A) dr(T ). see AA |= , suffices take functions dri defined
by:
43

fiArtale, Calvanese, Kontchakov & Zakharyaschev

dri (B) = > iff |= B [dri ], dri dr(T ) B Bcon(T ),
(B) = > iff |= B [a], ob(A) B Bcon(T ).
Details left reader.

q

follows Lemmas 6.1 5.17 Corollary 5.16 have:
H
Corollary 6.2 satisfiability instance checking problems DL-LiteN
bool , DL-Litebool
(HN )
DL-Litebool KBs AC0 data complexity.
(HN )

H
Proof DL-LiteN
bool DL-Litebool sub-languages DL-Litebool ,
(HN )
result immediately follows Lemma 6.1 Corollary 5.16. DL-Litebool KB
(HN )

K0 = (T 0 , A0 ), Lemma 5.17, construct DL-Litebool
KB K = (T , A)
0
K satisfiable iff K satisfiable (44) holds. latter condition corresponds
following first-order sentence
^
^


0 =
xy R1T (x, y) R2T (x, y)

x PkT (x, x) ,
Dis(R1 ,R2 )T 0

Irr(Pk )T 0

evaluated AA . Therefore, K0 satisfiable iff AA |= 0 . Let = 0 0
result replacing SP (t1 , t2 ), Ref(P ) 0 , P (t1 , t2 ) (t1 6= t2 ); see
proof Lemma 5.17. remains observe AA |= iff AA0 |= 0 .
q
before, result depend UNA member DL-Lite family
number restrictions form q R, q 2 (in particular, DL-LiteH
bool
fragments).
also note transitive roles cannot included languages free
concerned data complexity:
Lemma 6.3 Satisfiability instance checking DL-Litecore KBs extended role transitivity constraints NLogSpace-hard data complexity.
Proof Suppose given directed graph. Let P role name. Define ABox
taking P (ai , aj ) iff edge (ai , aj ) graph. node
reachable node a0 iff DL-Litecore ABox {P (a0 , )} satisfiable
models transitive P . encoding immediately gives claim lemma
directed graph reachability problem NLogSpace-complete, NLogSpace closed
complement (see, e.g., Kozen, 2006) TBox {Tra(P )} depend
input.
q
hand, reduction Lemma 5.18 computable NLogSpace,
obtain following:
(HN )+

Corollary 6.4 Satisfiability instance checking DL-Litebool
complete data complexity.

KBs NLogSpace-

Proof upper bound obtained applying NLogSpace reduction Lemma 5.18
using Corollary 6.2. lower bound follows Lemma 6.3.
q

44

fiThe DL-Lite Family Relations

6.2 P- coNP-hardness Data Complexity
Let us turn data complexity instance checking DL-Lite logics
arbitrary number restrictions role inclusions. follows results Ortiz et al.
(2006) SHIQ, instance checking (and fact query answering) DL-LiteHN
bool
coNP data complexity, results Hustadt et al. (2005) Eiter et al. (2008)
Horn-SHIQ imply polynomial-time upper bound DL-LiteHF
horn .
show upper bounds optimal following sense: one
hand, instance checking DL-LiteHF
core P-hard data complexity; hand,
HN
becomes coNP-hard DL-LiteHF
krom DL-Litecore (that is, allow negated
concept names arbitrary number restrictionsin fact, 2 R enough). Note
results section depend whether adopt UNA not.
Theorem 6.5 instance checking (and query answering) problem DL-LiteHF
krom KBs
data-hard coNP (with without UNA).
Proof proof reduction unsatisfiability problem 2+2CNF,
known coNP-complete (Schaerf, 1993). Given 2+2CNF formula
=

n
^

(ak,1 ak,2 ak,3 ak,4 ),

k=1

ak,j one propositional variables a1 , . . . , , construct KB (T , )
whose TBox depend . use object names f , ck , 1 k n,
ai , 1 m, role names S, Sf Pj , Pj,t , Pj,f , 1 j 4, concept names
D.
Define set following assertions, 1 k n:
S(f, ck ),

P1 (ck , ak,1 ),

P2 (ck , ak,2 ),

P3 (ck , ak,3 ),

P4 (ck , ak,4 ),

let consist axioms
2 Pj v ,
Pj,f v Pj ,

Pj,t v Pj ,

Pj,t v Pj,f ,

Pj,f


Pj,t

v A,

P1,f u P2,f u P3,t u P4,t v

Sf ,

v A,

for1 j 4,

(48)

1 j 4,

(49)

1 j 4,

(50)

1 j 4,

(51)
(52)



2 v ,

(53)

Sf v S,

(54)

Sf v D.

(55)

Note axiom (52) belong DL-LiteHF
krom conjunctions
left-hand side. However, eliminated help Lemma 5.9. let us prove
(T , ) |= D(f ) iff satisfiable.
() Suppose satisfiable |= (T , ). Define assignment
truth values f propositional variables taking a(ai ) = iff aIi AI . false
45

fiArtale, Calvanese, Kontchakov & Zakharyaschev

a, k, 1 k n, a(ak,1 ) = a(ak,2 ) = f a(ak,3 ) = a(ak,4 ) = t.
view (50), j, 1 j 4, cIk (Pj,t )I (Pj,f )I , (49),
cIk (Pj )I . Therefore, (48) (51), cIk (Pj,t )I a(ak,j ) = cIk (Pj,f )I
a(ak,j ) = f, hence, (52), cIk (Sf )I . (53) (54), f (Sf )I ,
which, (55), f DI . follows (T , ) |= D(f ).
() Conversely, suppose satisfiable. assignment
a(ak,1 ) = a(ak,2 ) = a(ak,3 ) = f a(ak,4 ) = f, 1 k n. Define taking



= x | 1 yk | 1 k n z ,
aIi = xi , 1 m,
cIk = yk , 1 k n,
f = z,



AI = xi | a(ai ) = yk | 1 k n z ,




= (y , aI ) | 1 k n, a(a ) = (x , x ) | a(a ) = (z, z) ,
Pj,t


k k,j
k,j



= (y , aI ) | 1 k n, a(a ) = f (x , x ) | a(a ) = f ,
Pj,f


k,j
k k,j
P , 1 j 4,
PjI = Pj,t
j,f


SfI = (z, yk ) | a(ak,1 ak,2 ak,3 ak,4 ) = f = ,


= (z, yk ) | 1 k n ,


DI = z | a() = f = .

hard check |= (T , ) 6|= D(f ).

q

Theorem 6.6 instance checking (and query answering) problem DL-LiteHN
core
KBs data-hard coNP (with without UNA).
Proof proof reduction unsatisfiability problem 2+2CNF.
HF
main difference previous one DL-LiteHN
core , unlike DL-Litekrom , cannot express
covering conditions like (50). turns out, however, use number restrictions
represent constraints kind. Given 2+2CNF formula , take ABox
constructed proof Theorem 6.5. ( independent) TBox , describing
meaning representation terms , also defined way
proof, except axiom (50) replaced following set axioms:
Tj,1 v Tj ,


2 Tj

v ,

Pj v Tj,1 ,

Tj,1

u


Tj,2

Tj,2 v Tj ,

v

(56)
(57)

Pj v Tj,2 ,


Tj,3
,

2 Tj v Pj,t

Tj,3 v Tj ,

(58)
(59)

Tj,3 v Pj,f ,

(60)

Tj , Tj,1 , Tj,2 , Tj,3 fresh role names, j, 1 j 4. Note axioms (52)
(59) belong DL-LiteHN
core conjunctions left-hand side,
46

fiThe DL-Lite Family Relations

easily eliminate using Lemma 5.9. remains prove (T , ) |= D(f )
iff satisfiable.
() Suppose satisfiable |= (T , ). Define assignment
truth values f propositional variables taking a(ai ) = iff aIi AI .
false a, k, 1 k n, a(ak,1 ) = a(ak,2 ) = f, a(ak,3 ) = a(ak,4 ) = t.
j, 1 j 4, cIk (Pj )I ; (58), cIk (Tj,1 )I , (Tj,2 )I .
(cI , v ) . v 6= v cI ( 2 )I
v1 , v2 (cIk , v1 ) Tj,1
1
2
j
j,2
k 2
k

and, (60), cIk (Pj,t )I . Otherwise, v1 = v2 = v, v (Tj,3
) (59),
(56) (57), cIk (Tj,3 )I , which, (60), cIk (Pj,f )I . Therefore,
cIk (Pj,t )I (Pj,f )I , (49), cIk (Pj )I . Thus, (48) (51), cIk (Pj,t )I
a(ak,j ) = cIk (Pj,f )I a(ak,j ) = f, hence, (52), cIk (Sf )I .
(53) (54), f (Sf )I , which, (55), f DI . follows
(T , ) |= D(f ).
() Conversely, suppose satisfiable. assignment
a(ak,1 ) = a(ak,2 ) = a(ak,3 ) = f a(ak,4 ) = f, 1 k n. Define taking




= xi | 1 yk | 1 k n uk,j,1 , uk,j,2 | 1 j 4, 1 k n z ,
aIi = xi , 1 m,

cIk = yk , 1 k n,

f = z,

AI = {xi | a(ai ) = t},


= (y , aI ) | 1 k n, a(a ) = , 1 j 4,
Pj,t
k,j
k k,j


= (y , aI ) | 1 k n, a(a ) = f , 1 j 4,
Pj,f
k,j
k k,j
P , 1 j 4,
PjI = Pj,t
j,f


= (y , u
Tj,1
k k,j,1 ) | 1 k n , 1 j 4,


= (y , u
Tj,2
k k,j,2 ) | 1 k n, a(ak,j ) =


(yk , uk,j,1 ) | 1 k n, a(ak,j ) = f , 1 j 4,


= (y , u
Tj,3
k,j,1 ) | 1 k n, a(ak,j ) = f , 1 j 4,
TI ,
TjI = Tj,1
j,2


SfI = (z, yk ) | a(ak,1 ak,2 ak,3 ak,4 ) = f = ,


= (z, yk ) | 1 k n ,


DI = z | a() = f = .

hard check |= (T , ) 6|= D(f ).

q

next lower bound would follow Theorem 6, item 2 work Calvanese
et al. (2006); unfortunately, proof incorrect cannot repaired.
Theorem 6.7 instance checking (and query answering) problem DL-LiteHF
core KBs
data-hard P (with without UNA).
47

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Proof proof reduction entailment problem Horn-CNF, known
P-complete (see, e.g., Borger et al., 1997, Exercise 2.2.4). Given Horn-CNF formula
=

n
^

(ak,1 ak,2 ak,3 )



k=1

p
^

al,0 ,

l=1

ak,j al,0 one propositional variables a1 , . . . , , construct
KB (T , ) whose TBox depend . need object names c1 , . . . , cn
vk,j,i , 1 k n, 1 j 3, 1 (for variable, take one object name
possible occurrence variable non-unit clause), role names S, St
Pj , Pj,t , 1 j 3, concept name A.
Define set containing assertions:
S(v1,1,i , v1,2,i ), S(v1,2,i , v1,3,i ), S(v1,3,i , v2,1,i ), S(v2,1,i , v2,2,i ), S(v2,2,i , v2,3,i ), . . .
. . . , S(vn,2,i , vn,3,i ), S(vn,3,i , v1,1,i ),
Pj (vk,j,i , ck )

iff

A(v1,1,i )

al,0 = ai ,

iff

ak,j = ai ,

1 m,

1 m, 1 k n, 1 j 3,
1 m, 1 l p

(all objects variable organized S-cycle Pj (vk,j,i , ck ) iff
variable ai occurs kth non-unit clause jth position). Let consist
following concept role inclusions:
St v S,

(61)

2 v ,

(62)

v St ,
St

(63)

v A,

(64)

2 P1 v
P1,t v P1 ,
v P1,t ,


P1,t

u

2 P3

2 P2 v ,

(65)

P2,t v P2 ,

(66)

v P2,t ,

(67)

v ,

(68)

P3,t v P3 ,

(69)


P2,t

v


P3,t
,

(70)

P3,t v A.

(71)

before, axiom, namely (70), belong DL-LiteHF
core
conjunction left-hand side, eliminated help
Lemma 5.9. aim show (T , ) |= A(v1,1,i0 ) iff |= ai0 .
() Suppose |= ai0 . Consider arbitrary model (T , ) define
assignment truth values f propositional variables a(ai ) = iff


v1,1,i
AI , 1 m. (61)(64), i, 1 m, either vk,j,i
AI ,


k, j 1 k n, 1 j 3, vk,j,i
/ , k, j 1 k n,
1 j 3. Now, a(ak,1 ) = a(ak,2 ) = t, 1 k n then, (65)(67),




cIk (P1,t
) , (P2,t
) . (70), cIk (P3,t
) hence, (68) (69), vk,3,i
(P3,t )I ,
48

fiThe DL-Lite Family Relations

St ,

Pj,t , Pj
Pj

a1 a2 a3
y1

a2 a4 a5
y2




.

xk,j,i
.

zk,j,i
a1

a2

a3

a4

a5

Figure 5: model satisfying (T , ), = (a1 a2 a3 ) (a2 a4 a5 ).


ak,3 = ai , means, (71), vk,3,i
AI , v1,1,i
AI a(ai ) = t.

follows a() = t, hence a(ai0 ) = t, which, definition, means v1,1,i
AI .
0
conclude (T , ) |= A(v1,1,i0 ).

() Conversely, suppose 6|= ai0 . assignment a() =
a(ai0 ) = f. construct model (T , ) 6|= A(v1,1,i0 ). Define
taking



= xk,j,i , zk,j,i | 1 k n, 1 j 3, 1 yk | 1 k n ,
cIk = yk , 1 k n,

= xk,j,i , 1 k n, 1 j 3, 1 m,
vk,j,i


AI = xk,j,i | 1 k n, 1 j 3, a(ai ) = ,
[


SI =
Si , Si = (xk,1,i , xk,2,i ), (xk,2,i , xk,3,i ), (xk,3,i , xk1,1,i ) | 1 k n
1im

k 1 = k + 1 k < n, k 1 = 1 k = n,
[
StI =
Si ,
1im
a(ai )=t



PjI = (xk,j,i , yk ) | 1 k n, ai = ak,j
(xk,j,i , zk,j,i ) | 1 k n, ai 6= ak,j , 1 j 2,


P3I = (xk,3,i , yk ) | 1 k n, ai = ak,3 ,


= (x
Pj,t
k,j,i , yk ) | 1 k n, ai = ak,j , a(a
i) =
(xk,j,i , zk,j,i ) | 1 k n, ai 6= ak,j , 1 j 2,


= (x
P3,t
k,3,i , yk ) | 1 k n, ai = ak,3 , a(ai ) = .
routine check indeed |= (T , ) 6|= A(v1,1,i0 ). See Figure 5
example.
q

49

fiArtale, Calvanese, Kontchakov & Zakharyaschev

7. Query Answering: Data Complexity
positive existential query answering problem known data-complete coNP
case DL-LiteHN
bool : upper bound follows results Ortiz et al. (2006),
lower bound established DL-Litekrom Calvanese et al. (2006), Schaerf
(1993). case DL-LiteHF
horn , query answering data-complete P, follows
results Hustadt et al. (2005) Eiter et al. (2008) Horn-SHIQ,
0
DL-LiteH
horn AC (Calvanese et al., 2006).
fact, coNP upper bound holds extension DL-LiteHN
bool role disjointness (a)symmetry constraints (this follows Glimm et al., 2007, Theorem 10;
cf. Remark 5.21). conjecture result holds role (ir)reflexivity constraints.
main result section following:
Theorem 7.1 positive existential query answering problem logics DL-LiteN
horn ,
(HN )
H
0
DL-Litehorn DL-Litehorn AC data complexity.
(HN )

Proof Suppose given consistent DL-Litehorn KB K0 = (T 0 , A0 ) (with
concept role names occurring TBox 0 ) positive existential query prenex
(HN )
form q(~x) = ~y (~x, ~y ) signature K0 . Consider DL-Litehorn KB K = (T , A)
(HN )

provided Lemma 5.17 (the language DL-Litehorn

defined Section 5.3).

Lemma 7.2 every tuple ~a object names K0 , K0 |= q(~a) iff |= q(~a)
untangled models K.
Proof () Suppose K0 |= q(~a) untangled model K. Lemma 5.17
view consistency K0 , ensures (44) holds, |= K0
therefore, |= q(~a).
() Suppose 0 |= K0 . Lemma 5.17, model K domain
0
coincides 0 symbols K0 . |= q(~a), must 0 |= q(~a),
K0 |= q(~a) required.
q
Next show that, Ke Horn sentence, enough consider one special
model I0 K formulation Lemma 7.2. Let M0 minimal Herbrand model
(the universal Horn sentence) Ke . remind reader (for details consult, e.g., Apt, 1990;
Rautenberg, 2006) M0 constructed taking intersection Herbrand
models Ke , is, models based domain consists constant symbols
Ke i.e., = ob(A) dr(T ); cf. Remark 5.15. following
M0 |= B [c]

iff

Ke |= B (c),

B Bcon(T ) c .

Let I0 untangled model K induced M0 . Denote domain I0 I0 .
Property (copy) Remark 5.15 provides us function cp : I0 .
two consequences Lemma 5.14. First,
aIi 0 B I0

iff

K |= B(ai ),

B Bcon(T ) ai ob(A).

50

(72)

fiThe DL-Lite Family Relations

Second, every R role (T ), RI0 6= RI 6= , models K. Indeed,
RI0 6= M0 |= (R) [dr]. Therefore, (T {R v }, A) satisfiable, thus
RI 6= , |= K. Moreover, RI0 6=
w B I0

iff

K |= R v B,

B Bcon(T ) w I0 cp(w) = dr. (73)

Lemma 7.3 I0 |= q(~a) |= q(~a) untangled models K.
Proof Suppose |= K. q(~a) positive existential sentence, enough construct
homomorphism h : I0 I. remind reader that, (forest), domain I0
I0 partitioned disjoint trees Ta , ob(A). Define depth point w I0
length shortest path respective tree root. Denote Wm
set points depth m; particular, W0 = {aI0 | ob(A)}. construct h
union maps hm , 0, hm defined Wm following properties:
hm+1 (w) = hm (w), w Wm ,
(am ) every w Wm , w B I0 hm (w) B , B Bcon(T );
(bm ) u, v Wm , (u, v) RI0 (hm (u), hm (v)) RI , R role (T ).
basis induction, set h0 (aIi 0 ) = aIi , ai ob(A). Property (a0 ) follows
(72) (b0 ) (ABox) Remark 5.15.
induction step, suppose hm already defined Wm , 0. Set
hm+1 (w) = hm (w) w Wm . Consider arbitrary v Wm+1 \ Wm . (forest),
unique u Wm (u, v) Ea , Ta . Let `a (u, v) = S. Then,
(copy), cp(v) = inv(ds). (role), u (S)I0 and, (am ), hm (u) (S)I ,
means w (hm (u), w) . Set hm+1 (v) = w. cp(v) = inv(ds)
(inv(S))I0 6= , follows (73) v B I0 w0 B whenever
w0 (inv(S))I . w (inv(S))I , obtain (am+1 ) v. show (bm+1 ), notice
that, (role), (w, v) RI0 , w Wm+1 , two cases: either
w Wm+1 \ Wm , w = v Id vT R, w Wm , w = u vT R.
former case, (hm+1 (v), hm+1 (v)) RI IdI identity relation (role).
latter case, (u, v) I0 ; hence (hm+1 (u), hm+1 (v)) and, vT R,
(hm+1 (u), hm+1 (v)) RI .
q
Assume that, query q(~x) = ~y (~x, ~y ), ~y = y1 , . . . , yk ,
quantifier-free formula. next lemma shows case check whether I0 |= q(~a)
suffices consider points depth m0 I0 , m0
depend |A|.
Lemma 7.4 Let m0 = k + |role (T )|. I0 |= ~y (~a, ~y ) assignment a0
Wm0 (i.e., a0 (yi ) Wm0 i) I0 |=a0 (~a, ~y ).
Proof Suppose I0 |=a (~a, ~y ), assignment I0 , yi ,
1 k, a(yi )
/ Wm0 . Let minimal subset ~y contains yi
every either P (y 0 , y) P (y, 0 ) subformula , 0
role name P . Let yj > |role (T )| a(yj ) Wm
51

fiArtale, Calvanese, Kontchakov & Zakharyaschev

a(y)
/ Wm1 (for convenience, W1 = before). Clearly, exists:
a(yi )
/ Wm0 , k variables and, (forest), relations P I0 connect point
Wn \ Wn1 point Wn+1 \ Wn2 , n 1. Let w = a(yj ) point
Ta . w Wm \ Wm1 , cp(w) = dr, R role (T ).
|role (T )| distinct labels labeled tree Ta view (copy), point u
depth > |role (T )|, point u0 depth |role (T )| Ta
cp(u) = cp(u0 ); (iso), trees generated u u0 isomorphic. So,
isomorphism g labeled tree generated w (which contains a(y), )
onto labeled tree generated point depth |role (T )| Ta . Define new
assignment aY taking aY (y) = g(a(y)) aY (y) = a(y) otherwise. (copy),
(concept) (role) I0 |=aY (~a, ~y ) aY (y) Wm0 , .
aY (yj )
/ Wm0 j, repeat described construction. k iterations
shall obtain assignment a0 required lemma.
q
complete proof Theorem 7.1, encode problem K |= q(~a)? model
checking problem first-order formulas. precisely way Section 6.1,
fix signature contains unary predicates A, A, concept name A, binary
predicates P , P , role name P , represent ABox K first-order
model AA domain ob(A). define first-order formula ,q (~x)
signature (i) ,q (~x) depends q A, (ii) AA |= ,q (~a)
iff I0 |= q(~a).
begin defining formulas B (x), B Bcon(T ), describe types
elements ob(A) model I0 following sense (see also (72)):
AA |= B [ai ]

iff aIi 0 B I0 ,

B Bcon(T ) ai ob(A).

(74)

0 (x), 1 (x), . . . formulas
formulas defined fixed-points sequences B
B
one free variable,
(
A(x),
B = A,
0
B (x) =

Eq R (x), B = q R,
_

i1
i1

0
B
(x) = B
(x)
B
(x) B
(x) , 1,
1
k
B1 uuBk vBext(T )

Eq RT (x) given (46). (As Section 6.1, simplify presentation use ext(T ) instead .) clear that, B Bcon(T ),
(x) i+1 (x) (i.e., every (x) equivalent i+1 (x) first-order logic),
B
B
B
B
(x) j (x) every B Bcon(T ) j i. minimum
B
B
N (x).
exceed N = |Bcon(T )|, set B (x) = B
Next introduce sentences B,dr , B Bcon(T ) dr dr(T ), describe
types elements dr(T ) following sense (see also (73)):
AA |= B,dr

iff

w B I0 , B Bcon(T ) w I0 cp(w) = dr.

(75)

(By (concept), definition correct.) sentences defined similarly B (x).
Namely, B Bcon(T ) dr dr(T ), inductively define sequence
52

fiThe DL-Lite Family Relations

0
1
B,dr
, B,dr
, . . . taking
0
B,dr
= 0B,dr




B,dr
= iB,dr

_


i1
i1
B






, 1,
,dr
B
,dr
1
k

B1 uuBk vBext(T )

iB,dr = , 0, whenever B 6= R
0R,dr = x inv(R) (x)



iR,dr =

_

i1
inv(R),ds
,

1.

dsdr(T )
i+1

clear |role (T )|N B,dr
B,dr
, B Bcon(T )
|role (T )|N

dr dr(T ). set B,dr = B,dr
.
consider directed graph GT = (VT , ET ), VT set equivalence
classes [R], [R] = {R0 | R R0 }, R empty model , ET
set pairs ([Ri ], [Rj ])
(path) |= inv(Ri ) v q Rj



either inv(Ri ) 6vT Rj q 2,

Rj proper sub-role satisfying (path). ([Ri ], [Rj ]) ET iff,
ABox A0 , whenever minimal untangled model I0 (T , A0 ) contains copy w inv(dri0 ),
Ri0 [Ri ], w connected copy inv(drj0 ), Rj0 [Rj ], relations
Rj vT S.
Recall given query q(~x) = ~y (~x, ~y ), quantifier-free
positive formula ~y = y1 , . . . , yk . Let ,m0 set paths graph GT
length m0 . precisely,


,m0 = ([R1 ], [R2 ], . . . , [Rn ]) | 1 n m0 , ([Rj ], [Rj+1 ]) ET , 1 j < n .
R

, 0 ,m0 role R role (T ), write 0 one following three
conditions satisfied: (i) = 0 Id vT R, (ii) .[S] = 0 (iii) = 0 .[inv(S)],
role vT R.
Let kT ,m0 set k-tuples form ~ = (1 , . . . , k ), ,m0 . Intuitively,
evaluating query ~y (~x, ~y ) I0 , bound, non-distinguished, variable yi
mapped point w Wm0 . However, first-order model AA contain
points Wm0 \ W0 , represent them, use following trick. (forest),
every point w Wm0 uniquely determined pair (a, ), aI0 root
tree Ta containing w, sequence labels `a (u, v) path aI0 w.
follows unraveling procedure (path) ,m0 . So, formula
,q define assume yi range W0 represent first
component pairs (a, ), whereas second component encoded ith member
~ (these yi confused yi original query q, range
Wm0 ). order treat arbitrary terms occurring (~x, ~y ) uniform way,
set t~ = , = ob(A) = xi , t~ = , = yi (the distinguished variables xi
object names mapped W0 require second component
pairs).
Given assignment a0 Wm0 denote split(a0 ) pair (a, ~ ),
assignment AA ~ = (1 , . . . , k ) kT ,m0
53

fiArtale, Calvanese, Kontchakov & Zakharyaschev

distinguished variable xi , a(xi ) = aI0 = a0 (xi );
bound variable yi , a(yi ) = = ([R1 ], . . . , [Rn ]), n m0 , aI0
root tree containing a0 (yi ) R1 , . . . , Rn sequence
labels `a (u, v) path aI0 a0 (yi ).
every pair (a, ~ ), however, corresponds assignment Wm0 paths
~ may exist I0 : GT represents possible paths models fixed
TBox varying ABox. follows unraveling procedure, point Wm0 \ W0
corresponds ob(A) = ([R], . . . ) ,m0 iff enough R-witnesses
0
R
A, i.e., iff AA |= q
)
R [a] q R [a], q QT . Thus, every (a, ~
~ = (1 , . . . , k ), assignment a0 Wm0 split(a0 ) = (a, ~ ) iff AA |=a ~ (~y ),

^
_

0
q
(1 ,...,k ) (y1 , . . . , yk ) =
Ri (yi ) q Ri (yi )
R

1ik
6=

qQT

Ri , 1 k 6= , = ([Ri ], . . . ).
define now, every ~ kT ,m0 , concept name role name R,
(
(t),
t~ = ,
A~ (t) =
A,inv(ds) , t~ = 0 .[S], 0 ,m0 ,

~

~


R (t1 , t2 ), t1 = t2 = ,
R
R~ (t1 , t2 ) =
(t1 = t2 ),
t~1 t~2 either t~1 6= t~2 6= ,


,
otherwise,
RT (y1 , y2 ) given (47). claim that, every assignment a0 Wm0
(a, ) = split(a0 ),
I0 |=a0 A(t)

iff

AA |=a A~ (t),

I0 |=a0 R(t1 , t2 )

iff

AA |=a R~ (t1 , t2 ),

concept names terms t,
roles R terms t1 , t2 .

(76)
(77)

A(a), A(xi ) A(yi ) = claim follows (74). A(yi ) = 0 .[S],
(copy), cp(a(yi )) = inv(dr), R [S]; claim follows (75).
R(yi1 , yi2 ) i1 = i2 = , claim follows (ABox). Let us consider case
R(yi1 , yi2 ) i2 6= : a0 (yi2 )
/ W0 thus, (role), I0 |=a0 R(yi1 , yi2 ) iff
a0 (yi1 ), a0 (yi2 ) tree Ta , ob(A), i.e., AA |=a (yi1 = yi2 ),
either (a0 (yi1 ), a0 (yi2 )) Ea `a (a0 (yi1 ), a0 (yi2 )) = vT R,
(a0 (yi2 ), a0 (yi1 )) Ea `a (a0 (yi2 ), a0 (yi1 )) = inv(S) vT R,
R

a0 (yi1 ) = a0 (yi2 ) Id vT R, i.e., i1 i2 .
cases similar left reader.
Finally, let ~ (~x, ~y ) result attaching superscript ~ atom

_
,q (~x) = ~y
~ (~x, ~y ) ~ (~y ) .
~
kT ,m

0

54

fiThe DL-Lite Family Relations

follows (76)(77), every assignment a0 Wm0 , I0 |=a0 (~x, ~y ) iff
AA |=a ~ (~x, ~y ) (a, ) = split(a0 ). converse direction notice that, AA |=a ~ (~y )
assignment a0 Wm0 split(a0 ) = (a, ~ ).
Clearly, AA |= ,q (~a) iff I0 |= q(~a), every tuple ~a. also note that, every
pair tuples ~a ~b object names ob(A), ~ (~a, ~b) positive existential sentence
inequalities, domain-independent.10 also easily seen that, ~b,
~ (~b) domain-independent. follows minimality I0 ,q (~a) domainindependent, tuple ~a object names ob(A).

Finally, note resulting query contains |role (T )|k(k+|role (T )|) disjuncts. q

8. DL-Lite without Unique Name Assumption
section, unless otherwise stated, assume interpretations respect
UNA, is, may aIi = aIj distinct object names ai aj . consequence
relation |=noUNA refers class interpretations.
Description logics without UNA usually extended additional equality
inequality constraints form:
ai aj



ai 6 aj ,

ai , aj object names. semantics quite obvious: |= ai aj iff
aIi = aIj , |= ai 6 aj iff aIi 6= aIj . equality inequality constraints supposed
belong ABox part knowledge base. noted, however, reasoning
equalities LogSpace-reducible reasoning without them:
Lemma 8.1 every KB K = (T , A), one construct LogSpace size
KB K0 = (T , A0 ) without equality constraints |= K iff |= K0 , every
interpretation I.
Proof Let G = (V, E) undirected graph


V = ob(A),
E = (ai , aj ) | ai aj aj ai
[ai ] set vertices G reachable ai . Define A0 removing
equality constraints replacing every ai aj [ai ] minimal j. Note
minimal j computed LogSpace: enumerate object names aj
respect order indexes j check whether current aj reachable
ai G. remains recall reachability undirected graphs SLogSpace-complete
SLogSpace = LogSpace (Reingold, 2008).
q
mentioned Section 5.3, logics form DL-LiteH
feel whether
adopt UNA not. observation Lemmas 5.17, 5.18 8.1 hand,
obtain following result consequence Theorem 5.13:
10. query q(~
x) said domain-independent case AA |=a q(~
x) iff |=a q(~
x),
domain contains ob(A), active domain AA , AA = AAA P = P AA , concept
role names P .

55

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Theorem 8.2 without UNA, combined complexity, (i) satisfiability
H
DL-LiteH
bool KBs NP-complete; (ii) satisfiability DL-Litehorn KBs P-complete;
H
(iii) satisfiability DL-LiteH
krom DL-Litecore KBs NLogSpace-complete. results hold even KBs contain role disjointness, (a)symmetry, (ir )reflexivity transitivity constraints, equalities inequalities.
hand, Corollary 6.2 Lemmas 5.17, 5.18 8.1 derive
following:
Theorem 8.3 Without UNA, satisfiability instance checking DL-LiteH
bool KBs
0
0
AC data complexity. problems also AC KBs contain role
disjointness, (a)symmetry (ir )reflexivity constraints inequalities. However,
LogSpace-complete KBs may contain equalities, NLogSpace-complete
role transitivity constraints allowed.
also note complexity results (Corollary 5.12, Theorems 6.5, 6.6 6.7)
logics DL-LiteHF
DL-LiteHN
depend UNA.


section, analyze combined data complexity reasoning logics
(HF )
(HN )
form DL-Lite
DL-Lite
(as well fragments) without UNA.
obtained known results summarized Table 2 page 17.
(HN )

8.1 DL-Lite

: Arbitrary Number Restrictions

following theorem shows interaction number restrictions
possibility identifying objects ABox results higher complexity.
Theorem 8.4 Without UNA, satisfiability DL-LiteN
core KBs (even without equality
inequality constraints) NP-hard combined data complexity.
Proof proof reduction following variant 3SAT problemcalled monotone one-in-three 3SAT known NP-complete (Garey & Johnson, 1979):
given positive 3CNF formula
=

n
^


ak,1 ak,2 ak,3 ,

k=1

ak,j one propositional variables a1 , . . . , , decide whether
assignment variables aj exactly one variable true clauses
k
. encode problem language DL-LiteN
core , need object names ai ,
1 k n, 1 m, ck tk , 1 k n, role names P , concept
names A1 , A2 , A3 . Let ABox containing following assertions:
S(a1i , a2i ), . . . , S(ain1 , ani ), S(ani , a1i ),

1 m,

S(t1 , t2 ), . . . , S(tn1 , tn ), S(tn , t1 ),
P (ck , tk ),

1 k n,

P (ck , akk,j ), Aj (akk,j ),

1 k n, 1 j 3,

56

fiThe DL-Lite Family Relations

let TBox following axioms:
A1 v A2 ,

A2 v A3 ,

A3 v A1 ,

2 v ,

4 P v .

Clearly, (T , ) DL-LiteN
core KB depend (so cover
combined data complexity). claim answer monotone one-in-three
3SAT problem positive iff (T , ) satisfiable without UNA.
() Suppose |= (T , ). Define assignment truth values f
propositional variables taking a(ai ) = iff (a1i )I = (t1 )I . aim show
a(ak,j ) = exactly one j {1, 2, 3}, k, 1 k n. j {1, 2, 3},
(cIk , (akk,j )I ) P . Moreover, (akk,i )I 6= (akk,j )I 6= j. cIk ( 3 P )I
(cIk , (tk )I ) P , must (akk,j )I = (tk )I unique j {1, 2, 3}. follows
functionality that, 1 k n, (a1k,j )I = (t1 )I exactly one
j {1, 2, 3}.
() Let assignment satisfying monotone one-in-three 3SAT problem. Take
ai0 a(ai0 ) = (clearly, i0 exists, otherwise a() = f) construct
interpretation = (I , ) taking:



= yk , z k | 1 k n xki | a(ai ) = f, 1 m, 1 k n ,
cIk = yk (tk )I = z k , 1 k n,
(
xki , a(ai ) = f,
(aki )I =
1 m, 1 k n,
z k , a(ai ) = t,


= ((a1i )I , (a2i )I ), . . . , ((ain1 )I , (ani )I ), ((ani )I , (a1i )I ) | 1 ,


P = (cIk , (tk )I ), (cIk , (akk,1 )I ), (cIk , (akk,2 )I ), (cIk , (akk,3 )I ) | 1 k n .
readily checked |= (T , ).

q

fact, lower bound optimal:
(HN )

(HN )+

Theorem 8.5 Without UNA, satisfiability DL-LiteN
DL-Lite
, DL-Lite
KBs equality inequality constraints NP-complete combined data
complexity {core, krom, horn, bool}.
Proof lower bound immediate Theorem 8.4, matching upper bound
(HN )+
proved following non-deterministic algorithm. Given DL-Litebool
KB
K = (T , A),
guess equivalence relation ob(A);

select equivalence class ai / representative, say ai , replace every occurrence ai / ai ;
fail equalities inequalities violated resulting ABoxi.e.,
contains ai 6 ai ai aj , 6= j;
57

fiArtale, Calvanese, Kontchakov & Zakharyaschev

otherwise, remove equality inequality constraints ABox denote
result A0 ;
(HN )+

use NP satisfiability checking algorithm DL-Litebool
KB K0 = (T , A0 ) consistent UNA.

decide whether

Clearly, algorithm returns yes, 0 |= K0 , 0 respecting UNA,
construct model K (not necessarily respecting UNA) extending 0
0
following interpretation object names: aI = aIi , whenever ai representative
a/ (I coincides 0 symbols). Conversely, |= K take
equivalence relation defined ai aj iff aIi = aIj . Let 0 constructed
removing interpretations object names representatives
equivalence classes . follows 0 respects UNA 0 |= K0 , algorithm
returns yes.
q
(HF )

8.2 DL-Lite

: Functionality Constraints
(HF )+

Let us consider DL-Litebool
fragments. following lemma shows
logics reasoning without UNA reduced polynomial time size
ABox reasoning UNA.
(HF )+

Lemma 8.6 every DL-Litebool

KB K = (T , A) equality inequality con(HF )+

straints, one construct polynomial time |A| DL-Litebool
KB K0 = (T , A0 )
A0 contains equalities inequalities K satisfiable without UNA iff K0
satisfiable UNA.
Proof follows identifying aj ak mean replacing occurrence
ak aj . construct A0 first identifying aj ak , aj ak A,
removing equality A, exhaustively applying following procedure
A:
2 R v R(ai , aj ), R(ai , ak ) CleT (A), distinct aj ak ,
identify aj ak (recall functional R cannot transitive sub-roles
thus CleT (A) enough).
resulting ABox contains ai 6 ai , ai , then, clearly, K satisfiable,
add A(ai ) A(ai ) ABox, concept name A. Finally, remove
inequalities ABox denote result A0 . clear A0
computed polynomial time that, without UNA, K satisfiable iff K0
satisfiable. suffices show K0 satisfiable without UNA iff satisfiable
UNA. implication () trivial.
() Observe procedure ensures
e
qR,a
1,

R 2 v , R vT ob(A0 )
(HN )

(see page 37 definitions). Let K00 DL-Litebool
KB provided Lemma 5.17
K0 . follows property proofs Lemma 5.14 Corollary 5.16
58

fiThe DL-Lite Family Relations

K00 satisfiable without UNA (K00 )e satisfied first-order model
constants interpreted domain element. (K00 )e universal
first-order sentence containing equality, satisfiable first-order model
constants interpreted distinct elements. follows proofs Lemma 5.14
Corollary 5.16 first-order model unraveled model J K00
respecting UNA. Lemma 5.17, J model K0 .
q
reduction cannot done better P, shown next theorem:
Theorem 8.7 Without UNA, satisfiability DL-LiteF
core KBs (even without equality
inequality constraints) P-hard combined data complexity.
Proof proof reduction entailment problem Horn-CNF (as proof
Theorem 6.7). Let
=

n
^

ak,1 ak,2 ak,3



p
^



al,0

l=1

k=1

Horn-CNF formula, ak,j al,0 one propositional variables
a1 , . . . , ak,1 , ak,2 , ak,3 distinct, k, 1 k n. encode Pk
complete problem |= ai ? language DL-LiteF
core need object names t, ai ,
1 k n, 1 m, fk gk , 1 k n, role names P , Q, .
ABox contains following assertions
S(a1i , a2i ), . . . , S(an1
, ani ), S(ani , a1i ),


1 m,

P (akk,1 , fk ), P (akk,2 , gk ), Q(gk , akk,3 ), Q(fk , akk,1 ),
(t, a1l,0 ),

1 k n,

1 l p,

TBox asserts roles functional:
2 P v ,

2 Q v ,

2S v



2 v .

Clearly, K = (T , A) DL-LiteF
core KB depend . claim
|= aj iff (T , {T (t, a1j )}) satisfiable without UNA. show this, suffices
prove |= aj iff K |=noUNA (t, a1j ).
() Suppose |= aj . derive aj using following inference rules:
|= al,0 l, 1 l p;
|= ak,1 |= ak,2 , k, 1 k n, |= ak,3 .
show K |=noUNA (t, a1j ) induction length derivation aj .
basis induction trivial. assume aj = ak,3 , |= ak,1 , |= ak,2 , k,
1 k n, K |=noUNA (t, a1k,1 ) (t, a1k,2 ). Suppose also |= K. Since
0
0
functional, (a1k,1 )I = (a1k,2 )I . Since functional, (akk,1 )I = (akk,2 )I , k 0 ,
1 k 0 n, particular, k 0 = k. Then, since P functional, fkI = gkI , which,
0
0
functionality Q, (akk,3 )I = (akk,1 )I . Finally, since functional, (akk,3 )I = (akk,1 )I ,
59

fiArtale, Calvanese, Kontchakov & Zakharyaschev

k 0 , 1 k 0 n, particular, k 0 = 1. Thus, |= (t, a1j ) therefore
K |=noUNA (t, a1j ).
() Suppose 6|= aj . assignment a() =
a(aj ) = f. Construct interpretation taking



= xki | a(ai ) = f, 1 k n, 1 z k , uk , vk | 1 k n w ,
(
xki , a(ai ) = f,
(aki )I =
1 k n 1 m,
z k , a(ai ) = t,


tI = w, = (w, z 1 ) ,


= ((a1i )I , (a2i )I ), . . . , ((an1
)I , (ani )I ), ((ani )I , (a1i )I ) | 1 ,

(
vk , a(ak,2 ) = f,
fkI = uk gkI =
1 k n,
uk , a(ak,2 ) = t,


P = ((akk,1 )I , fkI ), ((akk,2 )I , gkI ) | 1 k n ,


QI = (gkI , (akk,3 )I ), (fkI , (akk,1 )I ) | 1 k n .
readily checked |= K 6|= (t, a1j ), K 6|=noUNA (t, a1j ).

q

result strengthens NLogSpace lower bound instance checking
DL-LiteF
core proved Calvanese et al. (2008).
(HF )

(HF )+

Corollary 8.8 Without UNA, satisfiability DL-LiteF
DL-Lite
, DL-Lite
KBs, {core, krom, horn}, equalities inequalities P-complete combined data complexity.
(HF )
(HF )+
Without UNA, satisfiability DL-LiteF
KBs
bool , DL-Litebool DL-Litebool
equalities inequalities NP-complete combined complexity P-complete data
complexity.

Proof upper bounds follow Lemma 8.6 corresponding upper bounds
UNA case. NP lower bound combined complexity obvious polynomial
lower bounds follow Theorem 8.7.
q
8.3 Query Answering: Data Complexity
P coNP upper bounds query answering without UNA follow results Horn-SHIQ (Hustadt et al., 2005; Eiter et al., 2008) SHIQ (Ortiz et al., 2006,
2008; Glimm et al., 2007), respectively (see discussion beginning Section 7).
present following result:
Theorem 8.9 Without UNA, positive existential query answering DL-LiteH
horn KBs
role disjointness, (a)symmetry, (ir )reflexivity constraints inequalities AC0
data complexity. problem LogSpace-complete if, additionally, equalities
allowed KBs.
60

fiThe DL-Lite Family Relations

Proof proof follows lines proof Theorem 7.1 uses observation
models without UNA give answers untangled counterparts.
precisely, let KB K0 = (T 0 , A0 ) above. Suppose consistent. Let q(~x)
positive existential query signature K0 . Given K0 , Lemma 5.17 provides
us KB K. easy see K DL-LiteH
horn KB extended inequality
constraints. following analogue Lemma 7.2, also allows us get rid
inequalities:
Lemma 8.10 every tuple ~a object names K0 , K0 |=noUNA q(~a) iff |= q(~a)
untangled models K (respecting UNA).
Proof () Suppose K0 |=noUNA q(~a) untangled model K. respects
UNA, Lemma 5.17 view satisfiability K0 , ensures (44) holds,
|= K0 therefore, |= q(~a).
() Suppose 0 |= K0 . construct interpretation J 0 respecting UNA follows.
0
0
0
0
Let J disjoint union ob(A). Define function h : J taking
0
0
h(a) = aI , ob(A), h(w) = w, w , let


0
0
0
0
0
aJ = a,
AJ = u | h(u) AI

P J = (u, v) | (h(x), h(v)) P ,
object, concept role name a, A, P . Clearly, J 0 respects UNA J 0 |= K0 .
also follows h homomorphism.
Lemma 5.17, model K domain J 0 coincides
J 0 symbols K0 . |= q(~a), must J 0 |= q(~a), since h
homomorphism, 0 |= q(~a). Therefore, K0 |=noUNA q(~a) required.
q
remaining part proof exactly Theorem 7.1 (since may assume
K DL-LiteH
horn KB containing inequality constraints).
LogSpace-completeness case equalities follows Lemma 8.1.
q

9. Conclusion
article, investigated boundaries extended DL-Lite family description
logics providing thorough comprehensive understanding interaction
various DL-Lite constructs impact computational complexity reasoning.
studied 40 different logics, classified according five mutually orthogonal features:
(1) presence absence role inclusion assertions, (2) form allowed concept
inclusion assertions, distinguishing four main logical groups called core, Krom, Horn,
Bool, (3) form allowed numeric constraints, ranging none, global functionality constraints only, arbitrary number restrictions, (4) presence absence
unique name assumption (and equalities inequalities object names,
assumption dropped), (5) presence absence standard role constraints
role disjointness, role symmetry, asymmetry, reflexivity, irreflexivity transitivity. resulting logics, studied combined data complexity KB
satisfiability instance checking, well data complexity answering positive
existential queries.
61

fiArtale, Calvanese, Kontchakov & Zakharyaschev

query answering
= instance checking

coNP
query answering

.
Legend
satisfiability
combined complexity

with/without UNA
role inclusions

F

N

ExpTime
NP
P
NLogSpace

UNA
role inclusions

F

UNA
role inclusions

coNP
P
AC0

oo
l

N

instance checking
data complexity

B

ro

K

co

H


n

.



F

N

Figure 6: Complexity basic DL-Lite logics.
obtained tight complexity results illustrated Figure 6, combined
complexity satisfiability represented height vertical dashed lines,
data complexity instance checking size color circle top lines
(recall satisfiability instance checking reducible complement
other). data complexity query answering core Horn logics, shown
left-hand side separating vertical plane, coincides data complexity
instance checking; Krom Bool logics, shown right-hand side plane,
query answering always data-complete coNP. upper layer shows complexity
logics role inclusions, case depend whether adopt
UNA not. middle lower layers deal logics without role inclusions
UNA dropped adopted, respectively. layers, twelve
languages arranged 4 3 grid: one axis shows type concepts inclusions
allowed (Horn, core, Krom, Bool), type number restrictions (none,
global functionality F arbitrary N ). observations order:
UNA without role inclusions, number restrictions increase
complexity reasoning, depends form concept inclusions allowed.
hand, without form number restrictions, logics role
inclusions insensitive UNA; again, complexity determined
shape concept inclusions only.
either cases, instance checking AC0 data complexity,
means problems first-order rewritable.

62

fiThe DL-Lite Family Relations

Without UNA adopted without either disjunctions role inclusions, functionality
leads P-completeness instance checking data complexity, suggests
reducibility Datalog.
data complexity, difference core Horn logics,
Krom Bool ones, means core Krom logics
extended conjunctions left-hand side concept inclusions free.
(HF )

(HN )

Finally, logics DL-Lite
DL-Lite
(qualified) number restrictions role inclusions, whose interaction restricted conditions (A1 )(A3 ),
complexity reasoning always coincides complexity fragments DL-LiteF

and, respectively, DL-LiteN
without role inclusions, matter whether adopt UNA
not.
Role disjointness, symmetry asymmetry constraints added
(HN )
(HF )
languages without changing complexity. fact, DL-Lite
DL-Lite
logics contain types constraints together role reflexivity irreflexivity. conjecture (ir)reflexivity constraints added logics without
affecting complexity. However, extend DL-Lite logic role transitivity
constraints, combined complexity satisfiability remains same, instance
checking query answering become data-hard NLogSpace. addition
equality object nameswhich makes sense UNA droppedleads
increase membership AC0 LogSpace-completeness data complexity;
results remain unchanged.
list DL constructs considered paper far complete.
example, would interest analyze impact nominals, role chains Boolean
operators roles computational behavior DL-Lite logics. Another interesting
practically important problem investigate depth interaction various
constructs aim pushing restrictions like (A1 )(A3 ) far possible.
One main ideas behind DL-Lite logics provide efficient access large
amounts data high-level conceptual interface. supposed achieved
representing high-level view information managed system DL-Lite
TBox , data stored relational database ABox A, rewriting positive existential queries knowledge base (T , A) standard first-order queries
database represented A. approach believed viable because, number
DL-Lite logics, query answering problem AC0 data complexity; cf. Theorems 7.1, 8.9 Figure 6. first-order rewriting technique implemented
various system, notably QuOnto (Acciarri et al., 2005; Poggi et al., 2008b),
query, relying ontology-to-relational mappings, data stored standard relational
database management system, Owlgres (Stocker & Smith, 2008), access
ABox stored Postgres database (though, best knowledge, latter
implementation incomplete conjunctive query answering). noted, however,
size rewritten query substantially larger size original
query, cause problems even efficient database query engine.
positive existential query q TBox , two major sources high
complexity first-order formula ,q proof Theorem 7.1: (i) formulas
B (x) computing whether ABox object instance concept B (and formulas
63

fiArtale, Calvanese, Kontchakov & Zakharyaschev

R,dr computing whether objects outgoing R-arrows instances B), (ii)
(HN )
disjunction paths ~ graph GT . case DL-Litecore , size
(HN )
B (x) linear |T |, DL-Litehorn become exponential (however, various
optimizations possible). size disjunction (ii) exponential number
non-distinguished variables q. One way removing source (i) would extend
given database (ABox) precomputing Horn closure ABox respect
TBox storing resulting data supplementary database. approach
advocated Lutz et al. (2008) querying databases via description logic EL.
could also promising Horn fragments expressive description logics
SHIQ (Hustadt et al., 2005; Hustadt, Motik, & Sattler, 2007)containing DL-LiteHF
horn
sub-languagefor data complexity instance checking (Hustadt et al., 2005,
2007) conjunctive query answering polynomial (Eiter et al., 2008). disadvantage
using supplementary database necessity update every time ABox
changed. would interesting investigate alternative approach DL-Lite logics
compare approach described above. Another important problem
characterize queries disjunction (ii) represented formula
polynomial size.
unique name assumption replaced OWL constructs sameAs
differentFrom (i.e., 6), challenging problem investigate possible ways
dealing equality (inequality require special treatment shown
proof Lemma 8.10). Although reasoning equality LogSpace-reducible reasoning without (cf. Lemma 8.1), lose property first-order rewritability,
computing equivalence classes may costly real-world applications.
DL-Lite logics among examples DLs usually complex
non-standard reasoning problemssuch checking whether one ontology conservative
extension another one respect given signature (Kontchakov et al., 2008),
computing minimal modules ontologies respect (Kontchakov et al., 2009)
uniform interpolants (Wang, Wang, Topor, & Pan, 2008)can supported practical
reasoning tools. However, first steps made direction,
research needed order include reasoning problems tools standard
OWL toolkit. would also interesting investigate unification problem DL-Lite
logics (Baader & Narendran, 2001).
Finally, exist certain parallels Horn logics DL-Lite family, EL,
Horn-SHIQ first-order language tuple equality generating dependencies,
TGDs EGDs, used theory databases (see, e.g., Gottlob & Nash, 2008).
investigations relationships logics may lead deeper understanding
role description logics play database framework.
Acknowledgments
research partially supported FET project TONES (Thinking ONtologiES), funded within EU 6th Framework Programme contract FP6-7603,
large-scale integrating project (IP) OntoRule (ONTOlogies meet Business RULEs
ONtologiES), funded EC ICT Call 3 FP7-ICT-2008-3, contract number FP7231875. thank referees constructive criticism, comments, suggestions.

64

fiThe DL-Lite Family Relations

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.
Acciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &
Rosati, R. (2005). QuOnto: Querying ontologies. Proc. 20th Nat. Conf.
Artificial Intelligence (AAAI 2005), pp. 16701671.
Apt, K. (1990). Logic programming. van Leeuwen, J. (Ed.), Handbook Theoretical
Computer Science, Volume B: Formal Models Sematics, pp. 493574. Elsevier
MIT Press.
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2007a). DL-Lite
light first-order logic. Proc. 22nd Nat. Conf. Artificial Intelligence
(AAAI 2007), pp. 361366.
Artale, A., Calvanese, D., Kontchakov, R., Ryzhikov, V., & Zakharyaschev, M. (2007b).
Reasoning extended ER models. Proc. 26th Int. Conf. Conceptual
Modeling (ER 2007), Vol. 4801 Lecture Notes Computer Science, pp. 277292.
Springer.
Artale, A., Cesarini, F., & Soda, G. (1996). Describing database objects concept
language environment. IEEE Trans. Knowledge Data Engineering, 8 (2), 345
351.
Artale, A., Parent, C., & Spaccapietra, S. (2007). Evolving objects temporal information
systems. Ann. Mathematics Artificial Intelligence, 50, 538.
Baader, F., & Narendran, P. (2001). Unification concepts terms description logics. J.
Symbolic Computation, 31 (3), 277305.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook: Theory, Implementation Applications.
Cambridge University Press. (2nd edition, 2007).
Beeri, C., Levy, A. Y., & Rousset, M.-C. (1997). Rewriting queries using views description
logics. Proc. 16th ACM SIGACT SIGMOD SIGART Symp. Principles
Database Systems (PODS97), pp. 99108.
Berardi, D., Calvanese, D., & De Giacomo, G. (2005). Reasoning UML class diagrams.
Artificial Intelligence, 168 (12), 70118.
Bergamaschi, S., & Sartori, C. (1992). taxonomic reasoning conceptual design. ACM
Trans. Database Systems, 17 (3), 385422.
Boppana, R., & Sipser, M. (1990). complexity finite functions. van Leeuwen, J.
(Ed.), Handbook Theoretical Computer Science, Volume A: Algorithms Complexity, pp. 757804. Elsevier MIT Press.
Borger, E., Gradel, E., & Gurevich, Y. (1997). Classical Decision Problem. Perspectives
Mathematical Logic. Springer.
Borgida, A., & Brachman, R. J. (2003). Conceptual modeling description logics.
Baader et al. (Baader et al., 2003), chap. 10, pp. 349372.

65

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., & Rosati, R. (2007).
Ontology-based database access. Proc. 15th Ital. Conf. Database Systems
(SEBD 2007), pp. 324331.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rosati, R., & Ruzzi, M.
(2008). Data integration DL-Lite ontologies. Schewe, K.-D., & Thalheim,
B. (Eds.), Revised Selected Papers 3rd Int. Workshop Semantics Data
Knowledge Bases (SDKB 2008), Vol. 4925 Lecture Notes Computer Science, pp.
2647. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite:
Tractable description logics ontologies. Proc. 20th Nat. Conf. Artificial
Intelligence (AAAI 2005), pp. 602607.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Data
complexity query answering description logics. Proc. 10th Int. Conf.
Principles Knowledge Representation Reasoning (KR 2006), pp. 260270.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007a). OWL
model football leagues?. Proc. 3rd Int. Workshop OWL: Experiences
Directions (OWLED 2007), Vol. 258 CEUR Workshop Proceedings.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007b). Tractable
reasoning efficient query answering description logics: DL-Lite family. J.
Automated Reasoning, 39 (3), 385429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2008a). Inconsistency tolerance P2P data integration: epistemic logic approach. Information
Systems, 33 (4), 360384.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2008b). Path-based
identification constraints description logics. Proc. 11th Int. Conf.
Principles Knowledge Representation Reasoning (KR 2008), pp. 231241.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002a). Description logics information
integration. Kakas, A., & Sadri, F. (Eds.), Computational Logic: Logic Programming Beyond, Essays Honour Robert A. Kowalski, Vol. 2408 Lecture Notes
Computer Science, pp. 4160. Springer.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002b). framework ontology integration. Cruz, I., Decker, S., Euzenat, J., & McGuinness, D. (Eds.), Emerging
Semantic Web Selected Papers First Semantic Web Working Symposium,
pp. 201214. IOS Press.
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998a). Description
logic framework information integration. Proc. 6th Int. Conf.
Principles Knowledge Representation Reasoning (KR98), pp. 213.
Calvanese, D., Lenzerini, M., & Nardi, D. (1998b). Description logics conceptual data
modeling. Chomicki, J., & Saake, G. (Eds.), Logics Databases Information
Systems, pp. 229264. Kluwer Academic Publishers.
Calvanese, D., Lenzerini, M., & Nardi, D. (1999). Unifying class-based representation formalisms. J. Artificial Intelligence Research, 11, 199240.
66

fiThe DL-Lite Family Relations

Corona, C., Ruzzi, M., & Savo, D. F. (2009). Filling gap OWL 2 QL
QuOnto: ROWLKit. Proc. 22nd Int. Workshop Description Logics
(DL 2009), Vol. 477 CEUR Workshop Proceedings.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse ontologies: Theory practice. J. Artificial Intelligence Research, 31, 273318.
Decker, S., Erdmann, M., Fensel, D., & Studer, R. (1999). Ontobroker: Ontology based
access distributed semi-structured information. Meersman, R., Tari, Z.,
& Stevens, S. (Eds.), Database Semantic: Semantic Issues Multimedia Systems,
chap. 20, pp. 351370. Kluwer Academic Publishers.
Dolby, J., Fokoue, A., Kalyanpur, A., Ma, L., Schonberg, E., Srinivas, K., & Sun, X. (2008).
Scalable grounded conjunctive query evaluation large expressive knowledge
bases. Proc. 7th Int. Semantic Web Conf. (ISWC 2008), Vol. 5318 Lecture
Notes Computer Science, pp. 403418. Springer.
Eiter, T., Gottlob, G., Ortiz, M., & Simkus, M. (2008). Query answering description logic Horn-SHIQ. Proc. 11th Eur. Conference Logics Artificial
Intelligence (JELIA 2008), pp. 166179.
Franconi, E., & Ng, G. (2000). i.com tool intelligent conceptual modeling. Proc.
7th Int. Workshop Knowledge Representation meets Databases (KRDB 2000),
Vol. 29 CEUR Workshop Proceedings, pp. 4553.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. W. H. Freeman.
Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. Doherty, P., Mylopoulos, J., & Welty,
C. (Eds.), Proc. 10th Int. Conf. Principles Knowledge Representation
Reasoning (KR 2006), pp. 187197.
Glimm, B., Horrocks, I., Lutz, C., & Sattler, U. (2007). Conjunctive query answering
description logic SHIQ. Proc. 20th Int. Joint Conf. Artificial Intelligence
(IJCAI 2007), pp. 399404.
Goasdoue, F., Lattes, V., & Rousset, M.-C. (2000). use CARIN language
algorithms information integration: Picsel system. Int. J. Cooperative
Information Systems, 9 (4), 383401.
Gottlob, G., & Nash, A. (2008). Efficient core computation data exchange. J.
ACM, 55 (2), 149.
Hayes, P. (2004). RDF semantics. W3C Recommendation. http://www.w3.org/TR/
rdf-mt/.
Heflin, J., & Hendler, J. (2001). portrait Semantic Web action. IEEE Intelligent
Systems, 16 (2), 5459.
Heymans, S., Ma, L., Anicic, D., Ma, Z., Steinmetz, N., Pan, Y., Mei, J., Fokoue, A.,
Kalyanpur, A., Kershenbaum, A., Schonberg, E., Srinivas, K., Feier, C., Hench, G.,
Wetzstein, B., & Keller, U. (2008). Ontology reasoning large data repositories.
Hepp, M., De Leenheer, P., de Moor, A., & Sure, Y. (Eds.), Ontology Management,
67

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Semantic Web, Semantic Web Services, Business Applications, Vol. 7 Semantic
Web Beyond Computing Human Experience, pp. 89128. Springer.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDF
OWL: making web ontology language. J. Web Semantics, 1 (1), 726.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. J. Interest Group Pure Applied Logic, 8 (3), 239264.
Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning description logics reduction
disjunctive Datalog. J. Automated Reasoning, 39 (3), 351384.
Hustadt, U., Motik, B., & Sattler, U. (2005). Data complexity reasoning expressive
description logics. Proc. 19th Int. Joint Conf. Artificial Intelligence
(IJCAI 2005), pp. 466471.
Immerman, N. (1999). Descriptive Complexity. Springer.
Klyne, G., & Carroll, J. J. (2004). Resource description framework (RDF): Concepts
abstract syntax. W3C Recommendation. http://www.w3.org/TR/rdf-concepts/.
Kontchakov, R., Pulina, L., Sattler, U., Schneider, T., Selmer, P., Wolter, F., & Zakharyaschev, M. (2009). Minimal module extraction DL-Lite ontologies using
QBF solvers. Proc. 21st Int. Joint Conf. Artificial Intelligence (IJCAI 2009), pp. 836840.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2008). tell difference
DL-Lite ontologies?. Proc. 11th Int. Conf. Principles
Knowledge Representation Reasoning (KR 2008), pp. 285295.
Kontchakov, R., & Zakharyaschev, M. (2008). DL-Lite role inclusions. Domingue, J.,
& Anutariya, C. (Eds.), Proc. 3rd Asian Semantic Web Conf. (ASWC 2008),
Vol. 5367 Lecture Notes Computer Science, pp. 1630. Springer.
Kozen, D. (2006). Theory Computation. Springer.
Lenzerini, M. (2002). Data integration: theoretical perspective. Proc. 21st ACM
SIGACT SIGMOD SIGART Symp. Principles Database Systems (PODS 2002),
pp. 233246.
Levy, A. Y., & Rousset, M.-C. (1998). Combining Horn rules description logics
CARIN. Artificial Intelligence, 104 (12), 165209.
Lutz, C., Toman, D., & Wolter, F. (2008). Conjunctive query answering EL using
database system. Proc. 5th Int. Workshop OWL: Experiences Directions (OWLED 2008).
McGuinness, D., & Wright, J. R. (1998). Conceptual modelling configuration: description logic-based approach. Artificial Intelligence Engineering Design, Analysis,
Manufacturing. Special Issue Configuration, 12, 333344.
Meyer, T., Lee, K., & Booth, R. (2005). Knowledge integration description logics.
Proc. 20th Nat. Conf. Artificial Intelligence (AAAI 2005), pp. 645650.
Noy, N. F. (2004). Semantic integration: survey ontology-based approaches. SIGMOD
Record, 33 (4), 6570.
68

fiThe DL-Lite Family Relations

Ortiz, M., Calvanese, D., & Eiter, T. (2006). Characterizing data complexity conjunctive
query answering expressive description logics. Proc. 21st Nat. Conf.
Artificial Intelligence (AAAI 2006), pp. 275280.
Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity query answering expressive description logics via tableaux. J. Automated Reasoning, 41 (1), 6198.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2009). comparison query rewriting
techniques DL-Lite. Proc. 22nd Int. Workshop Description Logics
(DL 2009), Vol. 477 CEUR Workshop Proceedings.
Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008a).
Linking data ontologies. J. Data Semantics, X, 133173.
Poggi, A., Rodriguez, M., & Ruzzi, M. (2008b). Ontology-based database access
DIG-Mastro OBDA Plugin Protege. Clark, K., & Patel-Schneider,
P. F. (Eds.), Proc. 4th Int. Workshop OWL: Experiences Directions
(OWLED 2008 DC).
Rautenberg, W. (2006). Concise Introduction Mathematical Logic. Springer.
Reingold, O. (2008). Undirected connectivity log-space. J. ACM, 55 (4).
Schaerf, A. (1993). complexity instance checking problem concept languages
existential quantification. J. Intelligent Information Systems, 2, 265278.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions complements. Artificial Intelligence, 48 (1), 126.
Stocker, M., & Smith, M. (2008). Owlgres: scalable OWL reasoner. Proc. 5th
Int. Workshop OWL: Experiences Directions (OWLED 2008).
Tobies, S. (2001). Complexity results practical algorithms logics Knowledge Representation. Ph.D. thesis, LuFG Theoretical Computer Science, RWTH-Aachen, Germany.
Toman, D., & Weddell, G. E. (2005). interaction inverse features pathfunctional dependencies description logics. Proc. 19th Int. Joint Conf.
Artificial Intelligence (IJCAI 2005), pp. 603608.
Toman, D., & Weddell, G. E. (2008). keys functional dependencies first-class
citizens description logics. J. Automated Reasoning, 40 (23), 117132.
Vardi, M. (1982). complexity relational query languages (extended abstract).
Proc. 14th ACM SIGACT Symp. Theory Computing (STOC82), pp.
137146.
Vollmer, H. (1999). Introduction Circuit Complexity: Uniform Approach. Springer.
Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2008). Forgetting concepts DL-Lite.
Bechhofer, S., Hauswirth, M., Hoffmann, J., & Koubarakis, M. (Eds.), Proc.
5th Eur. Semantic Web Conf. (ESWC 2008), Vol. 5021 Lecture Notes Computer
Science, pp. 245257. Springer.

69

fiJournal Artificial Intelligence Research 36 (2009) 387-414

Submitted 07/09; published 11/09

Approximate Strong Equilibrium Job Scheduling Games
Michal Feldman

mfeldman@huji.ac.il

School Business Administration
Center Study Rationality,
Hebrew University Jerusalem, Israel.

Tami Tamir

tami@idc.ac.il

School Computer Science,
Interdisciplinary Center, Herzliya, Israel.

Abstract
Nash Equilibrium (NE) strategy profile resilient unilateral deviations,
predominantly used analysis multiagent systems. downside NE
necessarily stable deviations coalitions. Yet, show paper,
cases, NE exhibit stability coalitional deviations, benefits
joint deviation bounded. sense, NE approximates strong equilibrium.
Coalition formation key issue multiagent systems. provide framework
quantifying stability performance various assignment policies solution
concepts face coalitional deviations. Within framework evaluate given
configuration according three measures: (i) IR min : maximal number ,
exists coalition minimal improvement ratio among coalition members , (ii) IR max : maximal number , exists coalition
maximal improvement ratio among coalition members , (iii) DR max :
maximal possible damage ratio agent outside coalition.
analyze measures job scheduling games identical machines. particular,
provide upper lower bounds three measures NE wellknown assignment rule Longest Processing Time (LPT).
results indicate LPT performs better general NE. However, LPT
best possible approximation. particular, present polynomial time approximation
scheme (PTAS) makespan minimization problem provides schedule
IR min 1 + given . respect computational complexity, show
given NE 3 identical machines 2 unrelated machines, NP-hard
determine whether given coalition deviate every member decreases cost.

1. Introduction
consider job scheduling systems, n jobs assigned identical machines
incur cost equal total load machine assigned to.1
problems widely studied recent years game theoretic perspective (Koutsoupias & Papadimitriou, 1999; Andelman, Feldman, & Mansour, 2007; Christodoulou,
Koutsoupias, & Nanavati, 2004; Czumaj & Vocking, 2002; A. Fiat & Olonetsky., 2007).
contrast traditional setting, central designer determines allocation
jobs machines participating entities assumed obey protocol, mul1. cost function characterizes systems jobs processed parallel, jobs
particular machine single pick-up time, need share resource simultaneously.

c
2009
AI Access Foundation. rights reserved.

fiFeldman & Tamir

tiagent systems populated heterogeneous, autonomous agents, often display
selfish behavior. Different machines jobs may owned different strategic entities,
typically attempt optimize objective rather global objective.
Game theoretic analysis provides us mathematical tools study situations,
indeed extensively used recently analyze multiagent systems. trend
motivated part emergence Internet, composed distributed
computer networks managed multiple administrative authorities shared users
competing interests (Papadimitriou, 2001).
game theoretic models applied job scheduling problems, well network games (e.g., Fabrikant, Luthra, Maneva, Papadimitriou, & Shenker, 2003; Albers,
Elits, Even-Dar, Mansour, & Roditty, 2006; Roughgarden & Tardos, 2002; Anshelevich,
Dasgupta, Kleinberg, Tardos, Wexler, & Roughgarden, 2004), use solution concept
Nash equilibrium (NE), strategy agent best response
strategies agents. NE powerful tool analyzing outcomes competitive environments, notion stability applies unilateral deviations.

numerous multiagent settings, selfish agents stand benefit cooperating forming
coalitions (Procaccia & Rosenschein, 2006). Therefore, even single agent profit
unilateral deviation, NE might still stable group agents coordinating joint deviation, profitable members group. stronger
notion stability exemplified strong equilibrium (SE) solution concept, coined
Aumann (1959). strong equilibrium, coalition deviate improve utility
every member coalition.

M3

3

2

5

M3

3

5

8

M2

3

2

5

M2

3

5

8

10

M1

M1

5

5
(a)

2

2

4

(b)

Figure 1: example configuration (a) Nash equilibrium resilient
coordinated deviations, since jobs load 5 2 profit deviation demonstrated (b).

example, consider configuration depicted Figure 1(a). figures,
job represented rectangle whose width corresponds jobs length. jobs
scheduled specific machine form vertical concatenation rectangles. example,
Figure 1(a) three machines, M1 processes two jobs length 5. Note
internal order jobs effect, since cost job defined load
machine assigned to. configuration NE since job reduce cost
unilateral deviation. One might think NE identical machines must
also sustainable joint deviations. Yet, already observed (Andelman

388

fiApproximate Strong Equilibrium

et al., 2007), may true.2 example, configuration resilient
coordinated deviation coalition consisting four jobs load 5
2 deviating configuration (b), jobs load 5 decrease costs 10 8,
jobs load 2 improve 5 4. Note cost two jobs load 3
(which members coalition) increases.
example above, every member coalition improves cost (multiplicative) factor 45 . much coalition improve? bound
improvement ratio? turn out, example fact extreme one
sense clarified below. Thus, NE completely stable coordinated deviations, settings, provide us notion approximate
stability coalitional deviations (or approximate strong equilibrium).
also consider subclass NE schedules, produced Longest Processing Time
(LPT) rule (Graham, 1969). LPT rule sorts jobs non-increasing order
loads greedily assigns job least loaded machine. easy verify
every configuration produced LPT rule NE (Fotakis, S. Kontogiannis, &
Spiraklis, 2002). also SE? Note instance depicted Figure 1, LPT
would produced SE. However, show, always case.
paper provide framework studying notion approximate stability
coalitional deviations. analysis, consider three different measures. first
two measure stability configuration, uses notion improvement ratio
job, defined ratio jobs costs deviation.
third measures worst possible effect non-deviating jobs, explained
below.3
1. Minimum Improvement Ratio: definition, members coalition must
reduce cost. is, improvement ratio every member coalition larger
1. Clearly, coalition members might share improvement ratio.
minimum improvement ratio particular deviation minimal improvement ratio
coalition member. minimum improvement ratio schedule s, denoted IR min (s),
maximum possible deviations originated minimal improvement
ratio deviation. words, coalitional deviation originating
every member coalition reduces cost factor greater IR min (s).
closely related notion suggested Albers (2009), defined strategy
profile -SE coalition agent improve factor
. notation, schedule -SE IR min (s) . Albers
studied notion context SE existence cost-sharing games, showed
sufficiently large , -SE always exists. justification behind concept
agents may willing deviate improve sufficiently high factor (due
to, example, overhead associated deviation).
2. statement holds 3. 2 identical machines, every NE also SE (Andelman et al.,
2007).
3. Throughout paper, define approximation multiplicative factor. Since improvement
damage ratios three measures presented constants greater one (as shown
below), additive ratios unbounded. Formally, value possible construct instances
(by scaling instances provide multiplicative ratio) cost jobs reduced,
cost jobs increased, least additive factor a.

389

fiFeldman & Tamir

three machines, show every NE 54 -SE. is, coalition
deviate every member improves factor larger 54 .
case, also provide matching lower bound (recall Figure 1 above), holds
2
3. arbitrary m, show every NE (2 m+1
)-SE. proof technique
draws connection makespan approximation approximate stability,
makespan configuration defined maximum load machine
configuration.
next consider schedules obtained
LPT rule. show = 3,

every LPT configuration ( 21 + 46 )-SE ( 1.1123), also provide matching lower
1
bound, holds 3. arbitrary m, show upper bound 43 3m
.
results indicate LPT stable NE respect coalitional
deviations. Yet, LPT best possible approximation SE. Similar notion
approximation algorithms, define SE-PTAS assignment algorithm
gets input additional parameter , specifying close SE schedule
produces (1 + )-SE time polynomial n, 1/. paper devise
SE-PTAS fixed number machines, also approximates makespan within
factor 1 + .
2. Maximum Improvement Ratio: maximum improvement ratio particular
deviation maximal improvement ratio experienced coalition member.
maximum improvement ratio schedule s, denoted IR max (s), maximum
possible deviations originated maximal improvement ratio deviation.
words, coalition deviation originating exists
member coalition reduces cost factor greater IR max (s).
notion establishes bounds much agent would gain deviating
coalition agents gain something deviation. Also, notion similar
spirit stability large total improvement. also suits environments
individuals willing obey specific player long hurt. Interestingly,
find given NE configuration, improvement ratio single agent may
arbitrarily large, 3. contrast, LPT configurations three machines,
agent improve factor 53 bound tight. Thus, respect
IR max , relative stability LPT compared NE significant respect
1
, believe tight.
IR min . arbitrary m, provide lower bound 2
3. Maximum Damage Ratio: case jobs load 3 Figure 1, jobs
might get hurt result coalitional deviation. third measure consider
worst possible effect deviation jobs members deviating
coalition. Formally, maximum damage ratio maximal ratio costs
non-coalition member deviation. Variants measure
considered distributed systems, e.g., Byzantine Generals problem (Lamport, Shostak,
& Pease, 1982), rational secret sharing (Halpern & Teague, 2004).4 Section 5,
prove maximum damage ratio less 2 NE configuration, less
4. rational secret sharing protocol, set players, holding share secret, aims jointly
reconstruct it. Viewing protocol game, players utilities typically assumed satisfy
following two basic constraints: (i) player prefers learning secret learning it, (ii)
conditioned learned secret, player prefers possible players learn it.

390

fiApproximate Strong Equilibrium

23 LPT configuration. bounds hold 3, cases
provide matching lower bounds.
summary, results Sections 3-5 (see Table 1) indicate NE configurations
approximately stable respect IR min measure. Moreover, performance
jobs outside coalition would hurt much result coalitional deviation.
IR max , results provide additional strength LPT rule, already
known possess attractive properties (with respect to, e.g., makespan approximation
stability unilateral deviations).

NE
LPT

IR min
upper bound
m3
m=3
2
5
2 m+1
4
4
3



1
3m

1
2

+


6
4

lower
bound
1
2

5
4

+

6
4

IR max
upper
lower
bound bound
unbounded
5
3 (m=3)

2

1


DR max
upper lower
bound bound
2
2
3
2

3
2

Table 1: results three measures. Unless specified otherwise, results hold
arbitrary number machines m.
Section 7, study computational complexity aspects coalitional deviations.
find NP-hard determine whether NE configuration 3 identical
machines SE. Moreover, given particular configuration set jobs, NPhard determine whether set jobs engage coalitional deviation.
unrelated machines (i.e., job incurs different load machine),
hardness results hold already = 2 machines. results might implications
coalitional deviations computationally restricted agents.
Related work: NE shown paper provide approximate stability coalitional deviations. related body work studies well NE approximates optimal
outcome competitive games. Price Anarchy defined ratio
worst-case NE optimum solution (Papadimitriou, 2001; Koutsoupias & Papadimitriou, 1999), extensively studied various settings, including job scheduling
(Koutsoupias & Papadimitriou, 1999; Christodoulou et al., 2004; Czumaj & Vocking, 2002),
network design (Albers et al., 2006; Anshelevich et al., 2004; Anshelevich, Dasgupta, Tardos, Wexler, & Roughgarden, 2003; Fabrikant et al., 2003), network routing (Roughgarden
& Tardos, 2002; Awerbuch, Azar, Richter, & Tsur, 2003; Christodoulou & Koutsoupias,
2005), more.
notion strong equilibrium (SE) (Aumann, 1959) expresses stability coordinated deviations. downside SE games admit SE, even
amongst admitting Nash equilibrium. Various recent works studied existence SE particular families games. example, shown every job
scheduling game (almost) every network creation game, SE exists (Andelman et al.,
2007). addition, several papers (Epstein, Feldman, & Mansour, 2007; Holzman & LawYone, 1997, 2003; Rozenfeld & Tennenholtz, 2006) provided topological characterization
existence SE different congestion games, including routing cost-sharing
391

fiFeldman & Tamir

connection games. vast literature SE (e.g., Holzman & Law-Yone, 1997, 2003;
Milchtaich, 1998; Bernheim, Peleg, & Whinston, 1987) concentrate pure strategies
pure deviations, case paper. job scheduling settings, shown
Andelman et al. (2007) mixed deviations allowed, often case SE
exists. SE exists, clearly, price anarchy respect SE (denoted
strong price anarchy Andelman et al., 2007) significantly better price
anarchy respect NE (Andelman et al., 2007; A. Fiat & Olonetsky., 2007; Leonardi
& Sankowski, 2007).
Following work, IR min bounds case = 4 machines provided
Chen (2009), extended bound 45 NE schedules, provided bound
( 12 +



345
30 )

1.119 LPT-based schedules.

2. Model Preliminaries
section give formal description model provide several useful observations properties deviations coalitions.
2.1 Resilience Deviations Coalitions
first present general game theoretic setting describe specific job scheduling
setting focus paper.
game denoted tuple G = hN, (Sj ), (cj )i, N = {1, . . . , n} set
players, Sj finite action space player j N , cj cost function player j.
joint action space players = ni=1 Si . joint action = (s1 , . . . , sn ) S,
denote sj actions players j 0 6= j, i.e., sj = (s1 , . . . , sj1 , sj+1 , . . . , sn ).
Similarly, set players , also called coalition, denote actions
players , respectively. cost function player j maps joint action
real number, i.e., cj : R.
joint action pure Nash Equilibrium (NE) player j N benefit
unilaterally deviating action another action, i.e., j N Sj : cj (sj , a)
cj (s). pure joint action coalition N specifies action player
coalition, i.e., s0 j Sj . joint action resilient pure deviation
coalition pure joint action s0 cj (s , s0 ) < cj (s) every
j (i.e., players coalition deviate way player strictly
reduces cost). case say deviation s0 = (s , s0 ) profitable
deviation coalition .
pure joint action resilient pure deviations coalitions
coalition N profitable deviation s.
Definition 2.1 pure strong equilibrium (SE) pure joint action resilient
pure deviations coalitions.
Clearly, strong equilibrium refinement notion Nash equilibrium (in particular, strong equilibrium, resilient deviations coalitions size 1,
coincides definition NE).

392

fiApproximate Strong Equilibrium

2.2 Job Scheduling Identical Machines
job scheduling setting identical machines characterized set machines =
{M1 , . . . , Mm }, set {1, . . . , n} jobs, job j processing time pj . assignment
method produces assignment jobs machines, sj denotes machine
job j assigned to. assignment referred schedule configuration (we use
two terms interchangeably). load machine Mi inP
schedule sum
processing times jobs assigned Mi , Li (s) = j:sj =Mi pj . set jobs

, let s() = j {sj } denote set machines members assigned
schedule s.
makespan schedule load loaded machines. social optimum
minimizes makespan, i.e., OP = mins makespan(s).
job scheduling setting define job scheduling game jobs players.
action space Sj player j N individual machines, i.e., Sj = .
joint action space = nj=1 Sj . joint action constitutes schedule. schedule
player j N selects machine sj action incurs cost cj (s),
load machine sj , i.e., cj (s) = Li (s), sj = Mi . job scheduling game,
makespan also highest cost among players. Formally, makespan(s) = maxj cj (s).
0
Let s0 two configurations. Let Pis,s
binary indicator whose value 1
1 ,i2
job j sj = Mi1 s0j = Mi2 (i.e., job chooses Mi1
Mi2 s0 ), 0 otherwise. clear context, abuse notation
0
denote Pis,s
Pi1 ,i2 . addition, denote Li (s) Li (s0 ) Li L0i , respectively.
1 ,i2
Let s0 = (s , s0 ) profitable deviation coalition . improvement
ratio job j sj = Mi1 s0j = Mi2 (i.e., job migrating machine
0
Mi1 machine Mi2 ) denoted IRs,s (j) = Li1 (s)/Li2 (s0 ). Clearly, job j ,
0
IRs,s (j) > 1. damage ratio job j 6 sj = s0j = Mi denoted
0
DRs,s (j) = Li (s0 )/Li (s).
sj 6= s0j , say job j migrates deviation. Note that, terminology,
job member profitable deviation even migrate deviation.
Yet, every job migrates deviation member deviating coalition
definition.
Definition 2.2 Given schedules s0 = (s , s0 ), minimal improvement ratio s0
0
respect IR min (s, s0 ) = minj IRs,s (j). addition, minimal improvement
ratio schedule IR min (s) = maxs0 =(s ,s0 ),N IR min (s, s0 ).
Given schedules s0 = (s , s0 ), maximal improvement ratio s0 respect
0
IR max (s, s0 ) = maxj IRs,s (j). addition, maximal improvement ratio
schedule IR max (s) = maxs0 =(s ,s0 ),N IR max (s, s0 ).
Given schedules s0 = (s , s0 ), maximal damage ratio s0 respect
0
DR max (s, s0 ) = maxjN DRs,s (j). addition, maximal damage ratio schedule
DR max (s) = maxs0 =(s ,s0 ),N DR max (s, s0 ).
particular, define notion -SE (Albers, 2009) terms minimum
improvement ratio follows:
Definition 2.3 schedule -strong equilibrium (-SE) IR min (s) .
393

fiFeldman & Tamir

next provide several useful observations claims shall use sequel.
refer profitable deviation NE-schedule NE-originated profitable deviation.
Similarly, profitable deviations schedule produced LPT rule referred
LPT-originated profitable deviation.
first observation shows NE-originated profitable deviation, job
migrates machine, job must migrate machine. Formally:
Observation 2.4 Let NE let s0 = (s , s0 ) profitable deviation. s0j = Mi
j , j 0 sj 0 = Mi s0j 0 = Mi0 i0 6= i.
obvious, since job j strictly decreases cost migrating machine
job leaves, also profitably migrates unilaterally, contradicting NE.
next define special deviation structure, called flower structure
deviations loaded machine.
Definition 2.5 Let M1 loaded machine given schedule s. say
s,s0
s,s0
deviation s0 obeys flower structure > 1, P1,i
= Pi,1
= 1 i, j > 1,
0

s,s
Pi,j
= 0 (See Figure 2).

M2

M3
M1

M5

M4

Figure 2: graph representation coalition 5 machines obeying flower structure.
0

s,s
edge Mi Mj Pi,j
= 1.

0

s,s
particular, = 3, deviation s0 obeys flower structure P1,2
=
0

0

0

0

0

s,s
s,s
s,s
s,s
s,s
P2,1
= P1,3
= P3,1
= 1 P2,3
= P3,2
= 0. Recall simplicity presentation,
0

s,s
write sequel Pi,j denote Pi,j
also write Li L0i denote Li (s)
Li (s0 ), respectively.

Claim 2.6 NE-originated profitable deviation three machines obeys flower structure.
Proof: Let NE M1 loaded machine s, let s0 profitable
deviation. first show P2,3 = P3,2 = 0. Assume first P2,3
P = P3,2P= 1.
Thus, L02 < L3 L03 < L2 . Clearly, since total load change, Li = L0i .
Therefore, must hold L01 > L1 . However, profitable deviation increase
load loaded machine. contradiction. Therefore, one P2,3 , P3,2
1. Assume w.l.o.g P2,3 = 1. Observation 2.4 job leaves M3 ,
cannot M2 . Thus, must P3,1 = 1. Similarly, job
M1 .
Pleaves P
P1,2 = 1, get L01 < L3 , L02 < L1 , L03 < L2 , contradicting Li = L0i .
394

fiApproximate Strong Equilibrium

0
0
0
P1,3 = 1
P getPthat0 L1 < L3 , L2 < L2 (no job added M2 ), L3 < L1 ,
contradicting Li = Li again. Thus, P2,3 = 0. proof P3,2 = 0 analogous.
remains show P1,2 = P1,3 = P2,1 = P3,1 = 1. know three machines
assigned jobs s0 assigned s. P2,3 = P3,2 = 0.
Observation 2.4 job leaves M2 , M3 , therefore, P2,1 = P3,1 = 1. Also,
job leaves M1 , thus least one P1,2 , P1,3 equals 1. Assume w.l.o.g P1,2 = 1.
show also P1,3 = 1. particular, show L03 > L3 , since P2,3 = 0 must
P1,3 = 1. Assume opposite, L03 L3 . already know
P2,1 = 1.
P P1,2 =P
Thus, L02 < L1 , L01 < L2 , assumption L03 L3 . is, L0i < Li .
contradiction.

known NE schedule two identical machines also SE (Andelman
et al., 2007). claim, least four jobs change machines profitable
deviation three machines. Clearly, least four jobs change machines coalition
> 3 machines. Therefore,

Corollary 2.7 Every NE-schedule stable deviations coalitions size three
less.
next two propositions characterize coalition deviation three machines.
show M1 loaded machine deviation, becomes
least loaded deviation.
Proposition 2.8 NE-originated deviation three machines, loads two
less loaded machines increasing, is, L02 > L2 L03 > L3 .
Proof: Assume contrary L02 L2 . Claim 2.6, P1,3 P
= P3,1 = P
1. Thus,
0 <
L03 < L1 , L01 < L3 , assumption L02 L2 . is,
L

Li .
0
contradiction. proof L3 > L3 analogous.

Proposition 2.9 NE-originated deviation three machines loaded machine becomes least loaded machine, is, L01 < min(L02 , L03 ).
Proof: Claim 2.6, P1,2 = 1, thus L01 < L2 . proposition, L2 < L02 .
Thus, L01 < L02 . proof L01 < L03 symmetric.


3. -Strong Equilibrium
section, stability configurations measured minimal improvement ratio
measure. first provide complete analysis (i.e. matching upper lower bounds)
three identical machines5 NE LPT. arbitrary m, provide upper
bound NE LPT, show lower bounds = 3 hold m.
Theorem 3.1 NE schedule three machines 45 -SE.
5. note unrelated machines, improvement ratio cannot bounded within finite factor
even two machines. seen simple example two jobs two machines,
load vector job 1 (1, ), load vector job 2 (, 1). job assigned machine (for
= 1, 2), resulting configuration NE, load 1 machine. However, jobs
reduce load 1 swapping.

395

fiFeldman & Tamir

Proof: Let NE-configuration three machines, let r = IR min (s). Claim
2.6, deviation obeys flower P
structure. Therefore: L01 L2 /r , L01 L3 /r , L02
L1 /r , L03 L1 /r. Let P = j pj (also = L1 + L2 + L3 ). Summing
inequalities get r (L1 + P )/(L01 + P ).
Proposition 3.2 load loaded machine half total load,
is, L1 P/2.
Proof: Let g = max(L1 L2 , L1 L3 ). flower structure, least two jobs
M1 , thus g L1 /2 - since otherwise job would benefit leaving M1 , contradicting
NE. definition g, know 2L1 L2 + L3 + 2g, since 2g L1 , get
L1 P/2.

Distinguish two cases:
1. L01 P/5: case r (L1 + P )/(L01 + P ) (3P/2)/(6P/5) = 5/4.
2. L01 < P/5: means L02 + L03 > 4P/5 (M2 M3 rest load),
is, least one L02 , L03 > 2P/5. W.l.o.g. let M2 . flower structure
job M1 migrates M2 . jobs improvement ratio L1 /L02 , which,
Proposition 3.2, less (P/2)/(2P/5) = 5/4. Thus, again, r < 5/4.

analysis tight shown Figure 1. Moreover, lower bound
extended > 3 adding 3 machines 3 heavy jobs assigned
machines. Thus,
Theorem 3.3 3, exists NE schedule IR min (s) = 54 .
LPT configurations, bound minimum improvement ratio lower.
proof following theorems appear Appendix A.
Theorem 3.4 LPT schedule three machines ( 12 +


6
4

1.1123)-SE.

Theorem 3.5 3, exists LPT schedule IR min (s) = 12 +


6
4 .

next provide upper bounds arbitrary m. analysis based drawing
connection makespan approximation SE-approximation. Assume
given schedule r-approximation minimum makespan. show
conditions original schedule, subset jobs form coalition
IR min > r, then, considering subset machines, possible get schedule
apparently better optimal one. first define set assignment rules
connection exists.
Definition 3.6 Let schedule instance = hN, i. Given , let =
hN , instance induced s, N = {j|sj }. assignment method,
A, said subset-preserving , holds sj = sj

j N , assignments produced instances I,
respectively.
396

fiApproximate Strong Equilibrium

Claim 3.7 LPT subset-preserving method.
Proof: proof induction number jobs N . show
k 6= N , first k jobs N 0 assigned machine LPT executed
Note since N sublist N , jobs N
input input I.
order N . first job scheduled first empty machine among .
job j N , induction hypothesis, j scheduled, load
machines identical load corresponding machines time j
scheduled member N . load generated jobs N come j
N . Therefore, LPT, j scheduled least loaded machine among machines
, is, sj = sj . assume LPT uses deterministic tie-breaking rule
several least loaded machines N . Therefore sj = sj also case.

Lemma 3.8 Let assignment method (i) subset-preserving, (ii) yields Nash
equilibrium, (iii) approximates minimum makespan within factor r, r 1
non-decreasing m. Then, produces r-SE.
Proof: Assume contradiction exists instance schedule
produced A, exists coalition improvement ratio every member
greater r. Let coalition minimum size. job j
migrate, set jobs \ {j} smaller coalition, contradicting minimality
; therefore, every j , holds sj 6= s0j . next show s() = s0 ().
First, s() s0 (), is, every machine s() job j migrates,
must exist job migrating it, otherwise, \ {j} smaller coalition, contradicting
minimality . Second, s0 () s(), is, every machine job j
migrates, must exist job migrating (otherwise job j improve unilaterally,
contradiction NE). Given s() = s0 (), denote set machines
, let = |M |. Finally, let N N set jobs assigned machines A,
Consider instance = hN , i. Since subset-preserving, jobs N
scheduled exactly schedule scheduled part I.
particular, scheduled A, deviation exists, every job
improves factor greater r, machines take part it.
words, pair machines i, j, Pi,j = 1, Li /L0j > r(m) r(m),
r(m) approximation ratio machines. hand, since
OP (I)

produces r(m)-approximation, machine i, Li r(m)OP (I),

minimum possible makespan machines. Therefore, Pi,j = 1 r(m) <
Li /L0j


r(m)OP (I)
.
L0j

implies machine j receives least one job,


L0j < OP (I).
However, since least one job migrated participating machines,
deviation machines assigned jobs N load
contradiction.
less OP (I).

Let NE machines. Clearly, , induced schedule
2
set machines also NE. Also, known NE provides (2 m+1
)approximation makespan (Finn & Horowitz, 1979; Schuurman & Vredeveld, 2007).
2
implies Lemma 3.8 applied r = 2 m+1
assignment yields
NE. Therefore,
397

fiFeldman & Tamir

Corollary 3.9 NE schedule identical machines (2

2
m+1 )

SE.

next result direct corollary Lemma 3.8, Claim 3.7, fact LPT
1
)-approximation makespan (Graham, 1969).
provides ( 43 3m
1
Corollary 3.10 schedule produced LPT identical machines ( 43 3m
)SE.

bounds tight, gap lower upper bounds
small constant.

4. Maximum Improvement Ratio
section, analyze maximum improvement ratio measure. provide complete analysis NE configurations 3, LPT configurations three
machines. lower bound LPT three machines extended arbitrary m.
contrast measures consider paper, NE LPT differ
small constant, turns respect maximum improvement ratio, NE
LPT significantly different. improvement ratio NE-originated deviations arbitrarily high, deviations LPT configurations, highest possible
improvement ratio participating job less 35 .
Theorem 4.1 Fix r 1. 3 machines, exists instance
machines NE IR max (s) > r.
Proof: Given r, consider NE-configuration three machines given Figure 3(a).
coalition consists {1, 1, 2r, 2r}. improved schedule given Figure 3(b).
improvement ratio jobs load 1 2r/2 = r. > 3, dummy machines jobs
added.

M3

2r-1

1

2r

M3

2r-1

2r

4r-1

M2

2r-1

1

2r

M2

2r-1

2r

4r-1

M1

2r

4r

M1 1 1

2r
(a)

2
(b)

Figure 3: NE-originated deviation jobs load 1 improvement ratio
r.

contrast NE-originated deviations, LPT-originated deviations able
bound maximum improvement ratio small constant. proof following
claim given Appendix A.
Theorem 4.2 Let LPT schedule three machines. holds IRmax (s) 53 .

398

fiApproximate Strong Equilibrium

1+

2m-2+

Mm

M2

2m-3

1+

2m-2+

M2

M1

2m-3

2

2m-1

2m-3

2

2m-1



2m-3


Mm

2



2

1+ 2m-1+

M1 1+ 1+ 1+

(a)

m+m

(b)

Figure 4: LPT-originated deviation machines job load 1 + assigned
M1 improvement ratio arbitrarily close 2

1
m.

bound tight, demonstrated Figure 4 = 3 (where im1
provement ratio 2
= 35 ). Moreover, figure shows lower bound
generalized 3. Note coalitional deviation example obeys
flower structure. believe example tight, flower structure seems
enable largest possible decrease load single machine. job load 1 +
remains M1 improves cost 2m 1 + m(1 + ), is, job, j,
1
IR(j) = 2m1+
m(1+) = 2 . Formally,
Theorem 4.3 3, exists LPT configuration IR max (s) =
1
2
arbitrarily small > 0.

5. Maximum Damage Ratio
section, provide results maximum damage profitable deviation
impose jobs take part coalition. Formally, quality configuration measured maxj6 DR(j). provide complete analysis NE LPT
configurations 3. again, find LPT provides strictly better
performance guarantee compared general NE: cost job LPT schedule
cannot increase factor 32 larger, increase factor arbitrarily close
2 NE schedules.
Theorem 5.1 m, DR max (s) < 2 every NE configuration s.
Proof: Let s0 = (s0 , ) profitable deviation, let M1 loaded machine
among machines either job migrated job migrated into. Observation 2.4, must job migrated M1 . implies must
least two jobs M1 s, since single job, could benefit
deviation. Therefore, exists job j sj = M1 pj L1 /2. Using fact
NE again, get machine 6= 1, Li L1 /2 (otherwise job j
improve unilaterally migrating Mi ).
addition, every machine job migrates, must hold L0i < L1 .
job migrated Mi left machine j load Lj L1 . Combining

399

fiFeldman & Tamir

bounds, get every job j stays machine job
0
migrates holds DRs,s (j) = L0i /Li < L1 /Li (2Li )/Li = 2.

analysis tight shown Figure 3: damage ratio jobs load
2r 1 (4r 1)/(2r), arbitrarily close 2. Formally,
Theorem 5.2 3, exists NE configuration DR max (s) =
2 arbitrarily small > 0.
LPT configurations obtain smaller bound:
Theorem 5.3 m, DR max (s) <

3
2

every LPT configuration s.

Proof: Let s0 = (s0 , ) profitable deviation, let M1 loaded machine
among machines either job migrated job migrated into. Since every
LPT configuration NE, M1 must least two jobs (following arguments
proof Theorem 5.1). Assume w.l.o.g lightest (also last) job assigned
M1 load 1, denote job 1-job. assumption valid
minimum improvement ratio invariant linear transformations. Let ` = L1 1. Since
LPT configuration, every machine i, must hold Li ` (otherwise, 1-job
would assigned different machine). addition, since every machine j
job migrates, Lj L1 , must hold every machine job
migrates L0i < ` + 1. distinguish two cases.
3
case (a): ` 2. Then, every machine Mi job migrates, L0i /Li < `+1
` 2.
case (b): ` < 2. show profitable deviation exists case. ` < 2,
M1 exactly 2 jobs, loads ` 1, since LPT assigns jobs non-increasing order.
LPT, every machine must (i) one job load least ` (and possibly
small jobs), (ii) two jobs load least 1 (and possible small jobs). Let k k 0
number machines type (i) (ii), respectively (excluding M1 ). Thus,
total k + 1 jobs load ` 2k 0 + 1 jobs load 1. deviation, machine
jobs load ` 1 together, three jobs load 1. k + 1 machines
assigned k + 1 jobs load ` deviation cannot assigned job
load x. So, end 2k 0 + 1 jobs load 1 assigned k 0 machines.
Thus, must machine least three jobs load 1. Contradiction.

M3

1+2

1+

2+3

M3

1+2

1

M2

1+2

1+

2+3

M2

1+2

1+3

M1

1+3

1

3+3

M1

1+

1

(a)

1+

1

3+2
2+5
2+2

(b)

Figure 5: LPT-originated deviation, damage ratio job load 1 + 2 M3
arbitrarily close 32 .

analysis tight shown Figure 5. Moreover, adding dummy machines
jobs extended 3. Formally,
400

fiApproximate Strong Equilibrium

Theorem 5.4 3, exists LPT configuration DR max (s) =
3
2 arbitrarily small > 0.

6. Approximation Scheme
section present polynomial time approximation scheme provides (1 + )SE. PTAS applied fixed number machines.
Definition 6.1 vector (l1 , l2 , . . . lm ) smaller (l1 , l2 , . . . lm ) lexicographically
i, li < li b < i, lb = lb . configuration lexicographically smaller
vector machine loads L(s) = (L1 (s), . . . , Lm (s)), sorted non increasing order,
smaller lexicographically L(s), sorted non increasing order.
PTAS combines lexicographically minimal assignment longest k jobs
LPT rule applied remaining jobs. value k depends desired approximation ratio (to defined later).
Formally, algorithm Ak defined follows:
1. Find lexicographically minimal assignment longest k jobs.
2. Add remaining jobs greedily using LPT rule.
particular, since lexicographically minimal assignment minimizes makespan
(given load loaded machine), algorithm PTAS
minimum makespan problem, restriction known PTAS Graham (1966).
Grahams algorithm, step 1, first k jobs scheduled way minimizes
makespan. scheme, requirement schedule long jobs strict.
particular, shown Andelman et al. (2007), schedule longest k jobs
SE sub-instance.

Given , let denote algorithm k =
. first show
subset machines , provides (1 + )-approximation makespan
subset jobs scheduled . Formally,
Lemma 6.2 Let = hN, instance job scheduling machines jobs
N . Let output I. given , let N N set jobs
scheduled s. Consider instance = hN , i. Let assignment

induced s. (1 + )-approximation makespan I.

Proof: Let LA
max (M ) denote largest completion time machine set
schedule produced , let OP (I) denote minimum makespan I. Let
denote largest completion time long job N scheduled minimal
lexicographic schedule found step 1. Since minimal lexicographic assignment,

minimum makespan long jobs N . particular, lower bound OP (I),


thus, makespan increased second step, is, Lmax (M ) = ,
Otherwise, makespan larger . Let j job
optimal I.
determining makespan (the job completes last N ). definition LPT,
implies machines busy job j started execution (otherwise

401

fiFeldman & Tamir

job j could start earlier). Since optimal schedule step 1 intended idles,

holds machines busy time interval [0; LA
max (M ) pj ]
Pn
Let P =
j=1 pj total processing time n jobs N . above,


P m(Lmax (M ) pj ). Also, since jobs sorted non-increasing order processing

times, pj pk+1 therefore P m(LA
max (M ) pk+1 ). lower bound

optimal solution schedule load machines balanced;


P /m, implies LA
thus OP (I)
max (M ) OP (I) + pk+1 .



order bound Lmax (M ) terms OP (I), need bound pk+1 terms
first bound gap OP (I) OP (I).
following assumption
OP (I).
used.
Claim 6.3 Let z job determining makespan (I). W.l.o.g., z one
k long jobs.
Proof: Assume makespan (I) determined one long jobs. Let M1
machine z scheduled. particular, M1 processes long jobs. Fix
schedule M1 repeat PTAS remaining jobs machines
value k. Repeat necessary makespan determined job assigned using
LPT rule.
Note algorithm still polynomial, PTAS might repeated
1 times, constant. approximation ratio improving subinstance: number jobs considered long, among set fewer jobs, is,
larger portion jobs scheduled optimally, therefore approximation ratio proof
valid sub-instance. Finally, merging last PTAS result schedule
machines holding long jobs only, get PTAS whole instance, since long
jobs scheduled optimally step. Moreover, load machine
lower bound makespan sub-instance considered machine
gets jobs.

+ pk+1 .
Claim 6.4 OP (I) OP (I)
Proof: Let z job determining makespan (I). Claim 6.3, z assumed

assigned step 2 (by LPT rule). z N (I) = LA
max (M ). Else, load
machine least (I) pz , since otherwise job z assigned
one machines . Therefore, even total load N balanced among
(I) pz . Since pz pk+1 , OP (I) (I), get
, OP (I)
+ pk+1 .
OP (I) (I) OP (I)


Claim 6.5 pk+1 OP (I)


k .

Proof: Consider k + 1 longest jobs. optimal schedule, machine assigned
least d(k + 1)/me 1 + bk/mc jobs. Since jobs processing
time least pk+1 , conclude OP (I) (1 + bk/mc)pk+1 , implies

402

fiApproximate Strong Equilibrium

0
pk+1 OP (I)/(1 + bk/mc). Claim 6.4, pk+1
mOP
(I)/(1 + bk/mc) (OP (I ) +

pk+1 )/(1 + bk/mc). follows pk+1 OP (I)

k .







Back
Lmax (I), conclude Lmax (I) OP (I) + pk+1
mbound


+ ).
OP (I)(1 + k ) = OP (I)(1

prove main result section, showing schedule produced
, (1 + )-SE. stability proved following theorem. running
time, fixed m, k, minimal lexicographic schedule first k jobs found
O(mk ) steps. Applying LPT rule takes additional O(nlogn). , get
running time scheme O(mm/ ), is, exponential (that assumed
constant) 1/.

Theorem 6.6 produces (1 + )-SE.
Proof: proof similar proof Lemma 3.8. Assume contradiction
exists instance machines, schedule produced
, exists coalition improvement ratio every member larger
1 + . Let coalition minimum size. every machine job
j migrates, must exists job migrating it, otherwise, \ {j} also coalition
IR min > 1 + , contradiction minimality . Let denote set
machines part coalition, let N N set jobs assigned ,
let = |M |. Consider instance = hN , i, schedule s. Lemma
coalition exists s,
6.2, (1 + )-approximation makespan I.
machines take part it. Moreover, jobs improves factor
(1 + ). words, pair machines i, j, Pi,j = 1,
Li /L0j > 1 + . hand, since (1 + )-approximation, machine i,

(I)
Therefore, Pi,j = 1 1 + < L0i (1+)OP
. words,
Li (1 + )OP (I).
0
Lj

Lj


machine j receives least one job,
< OP (I).
However, since least one job migrated participating machines,
deviation machines assigned jobs N load
contradiction.
less OP (I).

note 0, schedule produced algorithm NE. Similar
stability proof LPT (Fotakis et al., 2002), easy verify job
benefit leaving machine Mi also shortest job machine
benefit migration. However, independent whether short job, length
pj , assigned step 1 algorithm (as part minimal lexicographical schedule
long job) step 2 (by LPT), gap Li load machine
pj .
L0j

7. Computational Complexity
easy see one determine polynomial time whether given configuration
NE. Yet, SE, task involved. section, provide hardness
results coalitional deviations.
Theorem 7.1 Given NE schedule 3 identical machines, NP-hard determine SE.
403

fiFeldman & Tamir

M3

B-1

B-2

2B-3

M3

B-1

Jobs A1

2B-1

M2

B-1

B-2

2B-3

M2

B-1

Jobs A2

2B-1

2B

M1

B-2

2B-4

M1

Jobs

B-2
(b)

(a)

Figure 6: Partition induces coalition schedule identical machines.

Proof: give reduction Partition. Given set n integers a1 , . . . ,
total size 2B, question whether subset total size B, construct
schedule Figure 6(a). schedule three machines n + 4 jobs loads
a1 , . . . , , B 2, B 2, B 1, B 1. assume w.l.o.g. mini ai 3, else whole
instance scaled. Thus, schedule 6(a) NE. 3, add 3 machines
single job load 2B.
Claim 7.2 NE schedule Figure 6(a) SE partition.
Proof: partition K1 , K2 , total size B, schedule
Figure 6(b) better jobs originated partition instance two
(B 2)-jobs. partition jobs improved cost 2B cost 2B 1, (B 2)jobs improved 2B 3 2B 4.
Next, show partition initial schedule SE. Theorem 2.7, action coalition three machines, jobs must migrate M1
M2 M3 . order decrease load 2B 3, set jobs migrating M1
must set two jobs load B 2. Also, must partition jobs move
away M1 - otherwise, total load M1 least 2B 4 + 3 = 2B 1,
improvement (B 2)-jobs. implies jobs M1 split
M2 M3 . However, since partition, one two subsets total load
least B + 1. jobs join job load B 1 get total load least 2B,
improvement 2B-load initial schedule.

establishes proof Theorem.

direct corollary proof following:
Corollary 7.3 Given NE schedule coalition, NP-hard determine whether
coalition deviate.
Theorem 7.1 holds 3 identical machines. 2, configuration
NE SE (Andelman et al., 2007), therefore possible determine
whether given configuration SE polynomial time. Yet, following theorem shows
case unrelated machines, problem NP-hard already = 2.
unrelated machines environment, processing time job depends machine
assigned. every job j machine i, pi,j denotes processing time job
j processed machine i.
Theorem 7.4 Given NE schedule 2 unrelated machines, NP-hard
determine SE.
404

fiApproximate Strong Equilibrium

Proof: give reduction Partition. Given n integers a1 , . . . , total size
2B, question whether subset total size B, construct following
instance scheduling: 2 machines n + 1 jobs following loads (for
< 1/(n 1)):
pi,1 = ai + pi,2 = 2ai + , {1, . . . , n} ; pn+1,1 = B, pn+1,2 = 2B + n.
Consider schedule jobs 1, . . . , n M1 , job n + 1 M2 .
completion times machines 2B + n. NE.

M2

Jn+1

2B+n

M2

M1

J1,,Jn

2B+n

M1

(a)

2B+|A2|

Jobs A2
Jobs A1

Jn+1

2B+|A1|

(b)

Figure 7: Partition induces coalition schedule related machines.

Claim 7.5 NE schedule Figure 7(a) SE partition.
Proof: partition A1 , A2 , total size B, schedule given
Figure 7(b) better everyone. completion time M1 2B + |A1 | < 2B + n
completion time M2 2B + |A2 | < 2B + n.
Next, show partition initial schedule SE. Since
partition, partition A1 , A2 , one two subsets, w.l.o.g., A1 total
size least B + 1. A1 increase load migrating M2 even alone (bearing
load least 2B + 2 + |A1 | instead 2B + n). Therefore, A1 leave M1 .
However, A1 stays M1 , job n + 1 better-off staying M2 (since migrates,
bears load least 2B + 1 + |A1 | smaller 2B + n |A1 |
1/(n 1)).

establishes proof Theorem.

direct corollary proof following:
Corollary 7.6 Given NE schedule unrelated machines coalition, NP-hard
determine whether coalition deviate.

8. Conclusions Open Problems
paper study well NE schedules special subset obtained
outcome LPT assignment rule approximate SE job scheduling games.
using two measures IR min IR max . addition, use DR max measure
study hurtful coalitional deviations agents outside coalition. present
upper lower bounds NE LPT-based schedules, demonstrate LPTbased schedules perform better general NE schedules, gap significant
IR max measure. NE LPT, IR min bounded small constant,
405

fiFeldman & Tamir

implying notion stability coalitional deviations (assuming existence
transition cost). IR max , bounded constant LPT schedules,
universal bound NE schedules. Yet, LPT best possible approximation
SE, demonstrated SE-PTAS design, computes schedule IR min
arbitrarily close 1.
problems remain open are:
1. IR min measure, gap upper lower bounds > 4 6 .
1
2. IR max LPT-originated deviations, 3 presented lower bound 2
5
matching upper bound 3 = 3. Closing gap general left
open problem.
3. paper focuses case identical machines. would interesting study
topic approximate strong equilibrium additional job scheduling settings. particular,
setting uniformly related machines part ongoing research, already
case two machines seems rather involved. Note that, mentioned Section 7,
unrelated machines, IR min unbounded already two machines.
4. measures defined respect strong equilibrium solution concept,
profitable deviation defined one every member coalition strictly
benefits. would interesting consider measures introduce respect
additional solution concepts, coalition-proof Nash equilibrium (Bernheim et al.,
1987) (which stable profitable deviations stable
deviations sub-coalitions), also respect profitable deviations none
coalition members worse-off least one member strictly better-off.
summary, introduced three general measures stability performance
schedules coalitional deviations. believe measures used
measure stability performance various algorithms coalitional deviations
performance additional settings games. hope see work
makes use measures within framework algorithmic game theory. would
interesting study families games Nash equilibria approximate strong equilibria
defined measures introduced here.
Acknowledgments. thank Leah Epstein Alon Rosen helpful discussions.
also thank anonymous reviewers insightful remarks suggestions. work
partially supported Israel Science Foundation (grant number 1219/09).

Appendix A. Bounding IR min IR max LPT-originated Deviations
first provide several observation valid LPT-originated deviation.
observations used later analysis. Moreover, observations characterize
coalitions might exist schedules produced LPT-rule. Combined
flower structure (that characterizes NE-originated deviations three machines),
get set LPT-originated deviation limited must follow strict
structure.
Let M1 loaded machine. Assume w.l.o.g lightest (also last) job
assigned M1 load 1, denote job 1-job. assumption valid
6. paper provides tight bounds = 3 case = 4 considered Chen (2009).

406

fiApproximate Strong Equilibrium

minimum improvement ratio invariant linear transformations. = 2, 3, denote
Ki set (and also total load) jobs remain Mi . Denote Hi,j set
(and also total load) jobs migrating Mi Mj . = 1, let K1 , H1,2 , H1,3
above, excluding 1-job.
next propositions show total size jobs migrating M2 , M3 M1
remaining M2 , M3 least large last job M1 .
Proposition A.1 H2,1 , H3,1 least 1.
Proof: show H2,1 1, proof H3,1 analogous. Assume contradiction
H2,1 < 1. Since LPT schedule jobs non-increasing order, jobs composing
H2,1 assigned 1-job. Therefore, 1-job assigned, load M2
K2 least H1,2 + H1,3 + K1 (else, LPT would assign 1-job M2 ). Thus,
K2 H1,2 + H1,3 + K1 . flower structure, job migrating M1 M2 .
migration beneficial L02 < L1 . Distinguish two cases:
1. 1-job migrates M2 . case, L02 = K2 + H1,2 + 1. Therefore, K2 +
H1,2 + 1 < H1,2 + H1,3 + K1 + 1, K2 < H1,3 + K1 . However, above,
K2 H1,2 + H1,3 + K1 H1,3 + K1 . contradiction.
2. 1-job migrate M2 . case, L02 = K2 + H1,2 . Therefore, K2 +
H1,2 < H1,2 + H1,3 + K1 + 1, K2 < H1,3 + K1 + 1. However, above,
K2 H1,2 + H1,3 + K1 1 + H1,3 + K1 . contradiction. last inequality follows
fact H1,2 empty consists least one job least big
smallest job M1 .

Proposition A.2 K2 , K3 least 1.
Proof: first show K2 1. Assume K2 < 1, means 1-job assigned
M1 , load M2 composed jobs subset H2,1 only. Therefore,
LPT rule, H2,1 K1 + H1,2 + H1,3 . However, Proposition 2.8, L02 > L2 , therefore
H2,1 < H1,2 + 1. Thus, K1 + H1,3 < 1. However, H1,3 1. contradiction. show
K3 1, note K3 > 1 similar argument H3,1 H1,2 + H1,3 .
Proposition 2.8, L03 > L3 . Therefore H1,3 > H3,1 , implying K1 + H1,2 < 0. contradiction.



Theorem 3.4 LPT configuration three machines ( 21 + 46 1.1123)-SE.
Proof: Let M1 loaded machine schedule. Recall lightest (also
last) job assigned M1 1-job load 1. Let ` = L1 1. give LPT schedule
deviation s0 = (s0 , ), let r = IR min (s, s0 ).
Claim 2.6, obeys flower structure. Therefore:
(i) r L2 /L01 ; (ii) r
P
0
0
0
L3 /L1 ; (iii) r L1 /L2 ; (iv) r L1 /L3 . Let P = j pj , Clearly, P = L1 + L2 + L3 =
L01 + L02 + L03 . Summing (i) (ii), get
L01

L2 + L3
P (` + 1)
=
.
2r
2r
407

(1)

fiFeldman & Tamir

LPT, L2 , L3 `, thus P 3` + 1. Summing (iii) (iv), using Equation (1)
get
r

2L1
2(` + 1)
2(` + 1)
=

0
0
1
+ L3
P L1
P (1 2r
)+

L02

Implying,
r(3` + 1)

`+1
2r



2(` + 1)
1
(3` + 1)(1 2r
)+

`+1
2r

.

3` + 1 ` + 1
+
2` + 2
2
2

and,
r

3` + 2
.
3` + 1

(2)

Case 1: ` 3. case, Equation (2) implies r 1.1.
Case 2: ` < 3. case requires closer analysis. Let instance LPT
creates schedule deviation s0 = (s0 , ) achieving maximal IR min ` < 3.
= 2, 3, denote Hi total load jobs migrating Mi M1 , Ki
total load jobs remain Mi . flower structure, L01 H2 + H3 , therefore
H2 < K3 H3 < K2 , else would beneficial jobs composing H2 , H3
join coalition. Propositions A.1 A.2, H2 , H3 , K2 , K3 least 1.
Claim A.3 load ` M1 incurred exactly two jobs.
Proof: Clearly, since consider case ` < 3 lightest job M1 load 1,
load ` incurred two jobs. Assume contradiction ` consists single
job. Then, exactly two jobs M1 , loads ` 1. flower structure,
`-job must join coalition. W.l.o.g assume migrates M2 . migration profitable
K2 < 1, contradicting Proposition A.2.

Therefore, assume w.l.o.g instance achieving maximal IR min , M1
assigned three jobs loads 1 + , 1 + , 1, , 0.
` = 2 + + , bound Equation (2) implies
r

8 + 3( + )
3` + 2
=
.
3` + 1
7 + 3( + )

(3)

Consider first case one two big jobs M1 migrate away
M1 . show coalition deviation beneficial case. W.l.o.g, assume
job length 1 + remains M1 job length 1 + migrates M2 .
migration 1 + profitable K2 < 2 + . hand, migration
jobs migrating M2 M1 profitable K2 > H3 + 1 + 2 + .
contradiction.
Consider next case 1-job migrate away M1 . W.l.o.g,
assume job length 1 + migrates M2 job length 1 + migrates
M3 . order bound r according Equation (3), find lower bound ( + ).
Equation (1),
2r

L2 + L3
K2 + H2 + K3 + H3
K2 + K3 + 2
6++
=


.
0
L1
1 + H2 + H3
3
3
408

(4)

fiApproximate Strong Equilibrium

third inequality due fact ratio decreasing H2 + H3 ,
known least 2 Proposition A.1. last inequality since migrations
beneficial jobs leaving M1 , is, K2 < 2 + , K3 < 2 + .
Equation (4) implies 6r < 6 + + + > 6r 6. Next, apply bound
+ Equation (3) obtain
r<

18r 10
.
18r 11



6
1
implies r < 10
9 < 2 + 4 .
case analyze yet one three jobs assigned M1 migrate
away M1 deviation. Assume w.l.o.g jobs size 1+ 1 migrating
M2 job size 1 + migrating M3 . Clearly, jobs size 1 + , 1 +
migrate machine currently assigned additional load
1 K2 K3 least 1, Proposition A.2. Figure 8 shows schedule
migration (Figure 8(a)) migration (Figure 8(b)).

M3

K3

H3

M2

K2

H2

M1

1+

1+

l

1

K3+H3

M3

K2+H2

M2

K2

1+

3+ +

M1

H2

H3

K3

1+

K3+1+
1

K2+2+
H2+H 3

(b)

(a)

Figure 8: LPT coalition achieving maximal IR min .
Next, find lower bound + . Considering migration M1 M2 ,
know r (3 + + )/(2 + + K2 ). Therefore + 2r + r + K2 r 3
2r + rK2 3 (because 0). Considering migration M2 M1 , know
r (K2 + H2 )/(H2 + H3 ). Therefore, K2 H2 (r 1) + H3 r. LPT assigns 1-job
M1 load 2 + + , load M2 time K2 + H2 .
Therefore 2 + + K2 + H2 , implying H2 2 + + K2 . Also, Proposition A.1,
H3 1. use bounds H2 , H3 get improved bound K2 . Specifically,
K2 (2 + + K2 )(r 1) + r. implies K2 r 3r + r( + ) (2 + ( + )). Back
bound + , + 2r + 3r + r( + ) (2 + ( + )) 3. Thus,
+ (5r 5)/(2 r). Note (2 r) positive since Theorem 3.1, r < 5/4.
Finally, apply bound + Equation (3) obtain
r

1 + 7r
8 + 3(5r 5)/(2 r)
=
.
7 + 3(5r 5)/(2 r)
1 + 8r



implies r 12 + 46 .

bound tight. Specifically,

Theorem 3.5 3, exists LPT schedule IR min (s) = 12 + 46 .

6
1
+
2
4 , consider Figure 8,
r(3+)2
, H2 = 2 + K2 , K3 = 1
r


10+5
6,
6 6

Proof: Let r =

substitute =

K2 =

+ , H3 = 1 (the instance

409

= 0,

fiFeldman & Tamir

rounded values appears Figure 9). easy verify three jobs leaving M1
improvement ratio exactly r = 12 + 46 , holds two jobs migrating


M1 . Thus, instance IR min = 12 + 46 . Moreover, lower bound easily
extended > 3 adding dummy jobs machines. Thus,
M3

1.633

1

2.633

M3

M2

1.367

1.266

2.633

M2

1.266

M1

1.633

1

3.633

M1

1.367

1

1.633
1

1.633

3.266

1

3.266

1

2.367

(b)

(a)

Figure 9: LPT-originated
deviation three machines migrating jobs im
6
1
prove 2 + 4 .


Theorem 4.2 Let LPT schedule three machines. holds IR max (s) 53 .
Proof: Let M1 loaded machine. Recall lightest (also last) job assigned
M1 1-job load 1. = 2, 3, Ki set (and also total load) jobs
remain Mi , Hi,j set (and also total load) jobs migrating Mi
Mj . = 1, let K1 , H1,2 , H1,3 above, excluding 1-job.
1-job assigned M1 LPT, meaning load M2 M3 least
K1 + H1,2 + H1,3 time. Since load M2 , M3 could increase time
1-job assigned, get
K1 + H1,2 + H1,3 K2 + H2,1



K1 + H1,2 + H1,3 K3 + H3,1 .

(5)

Therefore (sum two):
2(K1 + H1,2 + H1,3 ) K2 + K3 + H2,1 + H3,1 .

(6)

Distinguish two cases:
(i) 1-job remains M1 . case, L1 = K1 + H1,2 + H1,3 + 1; L2 =
K2 +H2,1 ; L3 = K3 +H3,1 , coalition active L01 = K1 +H2,1 +H3,1 +1; L02 =
K2 + H1,2 ; L03 = K3 + H1,3 .
Since jobs H1,2 H1,3 part coalition, L02 + L03 < 2L1 . Deducing
H1,2 H1,3 sides get K2 + K3 < H1,2 + H1,3 + 2K1 + 2. Combining
Equation 6, get:
H1,2 + H1,3 < H2,1 + H3,1 + 2.
(7)
Proposition A.1, H2,1 , H3,1 , K2 , K3 least 1. Proposition 2.9,
improvement ratio 1-job, equals L1 /L01 , largest among coalition.
ratio bounded follows:
K1 + H1,2 + H1,3 + 1
K1 + H2,1 + H3,1 + 3
L1
5
=
<
.
0
L1
K1 + H2,1 + H3,1 + 1
K1 + H2,1 + H3,1 + 1
3
410

fiApproximate Strong Equilibrium

left inequality follows Equation 7. right one follow Proposition A.1
fact K1 might empty.
(ii) 1-job leaves M1 . assume w.l.o.g 1-job moves M2 .
case, L1 = K1 + H1,2 + H1,3 + 1; L2 = K2 + H2,1 ; L3 = K3 + H3,1 , coalition
active L01 = K1 + H2,1 + H3,1 ; L02 = K2 + H1,2 + 1; L03 = K3 + H1,3 .
Since jobs H1,2 H1,3 part coalition, L02 + L03 < 2L1 . Deducing
1, H1,2 H1,3 sides get K2 + K3 < H1,2 + H1,3 + 2K1 + 1. Combining
Equation 6, get:
H1,2 + H1,3 < H2,1 + H3,1 + 1.
(8)
Propositions A.1 A.2, H2,1 , H3,1 , K2 , K3 least 1. K1 empty
jobs K1 improvement ratio L1 /L01 is, Proposition 2.9, largest
ratio among coalition. ratio bounded follows:
K1 + H1,2 + H1,3 + 1
K1 + H2,1 + H3,1 + 2
L1
5
=

< .
0
L1
K1 + H2,1 + H3,1
K1 + H2,1 + H3,1
3
left inequality follows Equation 8. right one follows Proposition A.1,
fact K1 empty includes least one job load least 1.
K1 empty, show below, maximal improvement ratio less 3/2.
bound separately improvement ratio H1,2 , H1,3 , Hi,1 (i {1, 2}). Denote
ri,j IR jobs moving Mi Mj . addition Equations 5 8,
Propositions A.1 A.2, also use Proposition 2.8. Specifically, H2,1 < H1,2 + 1
H3,1 < H1,3 . Finally, bear mind K1 = .
r1,2 =

H1,2 + H1,3 + 1
K2 + H2,1 + 1
K2 + H2,1 + 1
3
L1
=

<
< .
0
L2
K2 + H1,2 + 1
K2 + H1,2 + 1
K2 + H2,1
2

r1,3 =

H1,2 + H1,3 + 1
K3 + H3,1 + 1
K3 + H3,1 + 1
3
L1
=

<
< .
0
L3
K3 + H1,3
K3 + H1,3
K3 + H3,1
2

ri,1 =

Ki + Hi,1
H1,2 + H1,3
H2,1 + H3,1 + 1
Li
3
=
<
<
< .
0
L1
H2,1 + H3,1
H2,1 + H3,1
H2,1 + H3,1
2


Appendix B. List Scheduling
List Scheduling (LS) greedy scheduling algorithms jobs assigned
machines arbitrary order, similar LPT, job assigned least loaded
1
machine time assignment. LS known provide (2
)-approximation
minimum makespan (Graham, 1966). LS depart qualitatively
LPT respect makespan approximation (i.e., provide constant approximation
optimal makespan), qualitatively different respect game theoretic
properties. First, LS necessarily produce NE. Moreover, next show, LS
performs poorly respect measures introduced paper.
improvement ratio job bounded even coalition consists single
job. Consider example instance 2 machines jobs lengths 1, 1, X (in
411

fiFeldman & Tamir

order) X > 1. LS produce schedule loads 1, 1 + X. job length
1 scheduled long job migrate join short job. improvement
ratio 1 + X/2 arbitrarily large.
damage ratio deviation LS schedule bounded either. Consider
instance three machines jobs lengths {1 2, 1 , 1, 2, X, 2, 3}. easy
verify resulting LS-configuration, exists coalition job
length X migrates. Since X arbitrarily large, damage ratio job
machine X migrates arbitrarily large. note damage ratio caused
deviation single job 2. see this, consider LS configuration
assume job j length pj migrates M1 M2 . Denote Bj , Aj total
load jobs M1 assigned j respectively. Aj = 0 (j last)
beneficial j migrate (Bj < L2 , else j assigned
M2 ). Else, first job j assigned M1 Bj + pj less load
time M2 . Therefore L2 Bj + pj , particular pj L2 . damage
ratio (L1 + pj )/L1 2. analysis tight exemplified instance
= 2, = {1, 1, X}.

References
A. Fiat, H. Kaplan, M. L., & Olonetsky., S. (2007). Strong price anarchy machine
load balancing. International Colloquium Automata, Languages Programming(ICALP).
Albers, S. (2009). value coordination network design. SIAM J. Comput., 38(6),
22732302.
Albers, S., Elits, S., Even-Dar, E., Mansour, Y., & Roditty, L. (2006). Nash Equilibria
Network Creation Game. Annual ACM-SIAM Symposium Discrete Algorithms
(SODA).
Andelman, N., Feldman, M., & Mansour, Y. (2007). Strong Price Anarchy. SODA07.
Anshelevich, E., Dasgupta, A., Kleinberg, J. M., Tardos, E., Wexler, T., & Roughgarden, T.
(2004). price stability network design fair cost allocation.. FOCS,
pp. 295304.
Anshelevich, E., Dasgupta, A., Tardos, E., Wexler, T., & Roughgarden, T. (2003). Nearoptimal network design selfish agents. ACM Symposium Theory Computing (STOC).
Aumann, R. (1959). Acceptable Points General Cooperative n-Person Games. Contributions Theory Games, Vol. 4.
Awerbuch, B., Azar, Y., Richter, Y., & Tsur, D. (2003). Tradeoffs Worst-Case Equilibria.
1st International Workshop Approximation Online Algorithms.
Bernheim, D. B., Peleg, B., & Whinston, M. D. (1987). Coalition-proof nash equilibria:
concepts. Journal Economic Theory, 42, 112.
Chen, B. (2009). Equilibria load balancing games. Acta Mathematica Applicata Sinica,
appear.
412

fiApproximate Strong Equilibrium

Christodoulou, G., Koutsoupias, E., & Nanavati, A. (2004). Coordination mechanisms. J.
Daz, J. Karhumaki, A. Lepisto, D. Sannella (Eds.), Automata, Languages
Pro- gramming, Volume 3142 Lecture Notes Computer Science. Berlin: Springer,
pp. 345357.
Christodoulou, G., & Koutsoupias, E. (2005). Price Anarchy Stability
Correlated Equilibria Linear Congestion Games. Annual European Symposium
Algorithms (ESA).
Czumaj, A., & Vocking, B. (2002). Tight bounds worst-case equilibria. ACM-SIAM
Symposium Discrete Algorithms (SODA), pp. 413420.
Epstein, A., Feldman, M., & Mansour, Y. (2007). Strong Equilibrium Cost Sharing
Connection Games. ACM Conference Electronic Commerce (ACMEC).
Fabrikant, A., Luthra, A., Maneva, E., Papadimitriou, C., & Shenker, S. (2003).
network creation game. ACM Symposium Principles Distriubted Computing
(PODC).
Finn, G., & Horowitz, E. (1979). linear time approximation algorithm multiprocessor
scheduling. BIT Numerical Mathematics, 19 (3), 312320.
Fotakis, D., S. Kontogiannis, M. M., & Spiraklis, P. (2002). Structure Complexity Nash Equilibria Selfish Routing Game. International Colloquium
Automata, Languages Programming (ICALP), pp. 510519.
Graham, R. (1966). Bounds certain multiprocessing anomalies. Bell Systems Technical
Journal, 45, 15631581.
Graham, R. (1969). Bounds multiprocessing timing anomalies. SIAM J. Appl. Math.,
17, 263269.
Halpern, J. Y., & Teague, V. (2004). Rational secret sharing multiparty computation.
ACM Symposium Theory Computing (STOC), pp. 623632.
Holzman, R., & Law-Yone, N. (1997). Strong equilibrium congestion games. Games
Economic Behavior, 21, 85101.
Holzman, R., & Law-Yone, N. (2003). Network structure strong equilibrium route
selection games. Mathematical Social Sciences, 46, 193205.
Koutsoupias, E., & Papadimitriou, C. H. (1999). Worst-case equilibria.. Symposium
Theoretical Aspects Computer Science (STACS), pp. 404413.
Lamport, L., Shostak, R., & Pease, M. (1982). byzantine generals problem. ACM
Trans. Prog. Lang. Sys., 4, 382401.
Leonardi, S., & Sankowski, P. (2007). Network formation games local coalitions.
ACM Symposium Principles Distriubted Computing (PODC).
Milchtaich, I. (1998). Crowding games sequentially solvable. International Journal
Game Theory, 27, 501509.
Papadimitriou, C. H. (2001). Algorithms, games, internet. ACM Symposium
Theory Computing (STOC), pp. 749753.

413

fiFeldman & Tamir

Procaccia, A. D., & Rosenschein, J. S. (2006). communication complexity coalition
formation among autonomous agents. Int. Conference Autonomous Agents
Multiagent Systems (AAMAS), pp. 505512.
Roughgarden, T., & Tardos, E. (2002). bad selfish routing?. Journal ACM,
49 (2), 236 259.
Rozenfeld, O., & Tennenholtz, M. (2006). Strong correlated strong equilibria monotone congestion games. Workshop Internet Network Economics (WINE).
Schuurman, P., & Vredeveld, T. (2007). Performance guarantees local search
multiproces- sor scheduling. INFORMS Journal Computing, 19(1), 52 63.

414

fiJournal Artificial Intelligence Research 36 (2009) 129163

Submitted 04/09; published 10/09

Content Modeling Using Latent Permutations
Harr Chen
S.R.K. Branavan
Regina Barzilay
David R. Karger

harr@csail.mit.edu
branavan@csail.mit.edu
regina@csail.mit.edu
karger@csail.mit.edu

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
32 Vassar Street, Cambridge, Massachusetts 02139 USA

Abstract
present novel Bayesian topic model learning discourse-level document structure. model leverages insights discourse theory constrain latent topic assignments way reflects underlying organization document topics. propose
global model topic selection ordering biased similar across
collection related documents. show space orderings effectively represented using distribution permutations called Generalized Mallows Model.
apply method three complementary discourse-level tasks: cross-document alignment,
document segmentation, information ordering. experiments show incorporating permutation-based model applications yields substantial improvements
performance previously proposed methods.

1. Introduction
central problem discourse analysis modeling content structure document.
structure encompasses topics addressed order topics
appear across documents single domain. Modeling content structure particularly
germane domains exhibit recurrent patterns content organization, news
encyclopedia articles. models aim induce, example, articles
cities typically contain information History, Economy, Transportation,
descriptions History usually precede Transportation.
Previous work (Barzilay & Lee, 2004; Elsner, Austerweil, & Charniak, 2007) demonstrated content models learned raw unannotated text, useful
variety text processing tasks summarization information ordering. However, expressive power approaches limited: taking Markovian view
content structure, model local constraints topic organization. shortcoming substantial since many discourse constraints described literature global
nature (Graesser, Gernsbacher, & Goldman, 2003; Schiffrin, Tannen, & Hamilton, 2001).
paper, introduce model content structure explicitly represents two
important global constraints topic selection.1 first constraint posits document follows progression coherent, nonrecurring topics (Halliday & Hasan, 1976).
Following example above, constraint captures notion single topic,
1. Throughout paper, use topic refer interchangeably discourse unit
language model views topic.

c
2009
AI Access Foundation. rights reserved.

fiChen, Branavan, Barzilay, & Karger

History, expressed contiguous block within document, rather spread
disconnected sections. second constraint states documents domain
tend present similar topics similar orders (Bartlett, 1932; Wray, 2002). constraint
guides toward selecting sequences similar topic ordering, placing History Transportation. constraints universal across genres human
discourse, applicable many important domains, ranging newspaper text
product reviews.2
present latent topic model related documents encodes discourse
constraints positing single distribution entirety documents content ordering. Specifically, represent content structure permutation topics.
naturally enforces first constraint since permutation allow topic repetition.
learn distribution permutations, employ Generalized Mallows Model
(GMM). model concentrates probability mass permutations close canonical
permutation. Permutations drawn distribution likely similar, conforming second constraint. major benefit GMM compact parameterization
using set real-valued dispersion values. dispersion parameters allow model
learn strongly bias documents topic ordering toward canonical permutation. Furthermore, number parameters grows linearly number topics,
thus sidestepping tractability problems typically associated large discrete space
permutations.
position GMM within larger hierarchical Bayesian model explains
set related documents generated. document, model posits topic
ordering drawn GMM, set topic frequencies drawn multinomial distribution. Together, draws specify documents entire topic structure,
form topic assignments textual unit. traditional topic models, words
drawn language models indexed topic. estimate model posterior,
perform Gibbs sampling topic structures GMM dispersion parameters
analytically integrating remaining hidden variables.
apply model three complex document-level tasks. First, alignment
task, aim discover paragraphs across different documents share topic.
experiments, permutation-based model outperforms Hidden Topic Markov
Model (Gruber, Rosen-Zvi, & Weiss, 2007) wide margin gap averaged 28% percentage points F-score. Second, consider segmentation task, goal
partition document sequence topically coherent segments. model yields
average Pk measure 0.231, 7.9% percentage point improvement competitive
Bayesian segmentation method take global constraints account (Eisenstein & Barzilay, 2008). Third, apply model ordering task, is, sequencing
held set textual units coherent document. previous two applications, difference model state-of-the-art baseline substantial:
model achieves average Kendalls 0.602, compared value 0.267
HMM-based content model (Barzilay & Lee, 2004).
success permutation-based model three complementary tasks demonstrates flexibility effectiveness, attests versatility general document
2. example domain first constraint violated dialogue. Texts domains follow
stack structure, allowing topics recur throughout conversation (Grosz & Sidner, 1986).

130

fiContent Modeling Using Latent Permutations

structure induced model. find encoding global ordering constraints
topic models makes suitable discourse-level analysis, contrast local
decision approaches taken previous work. Furthermore, evaluation scenarios, full model yields significantly better results simpler variants either
use fixed ordering order-agnostic.
remainder paper proceeds follows. Section 2, describe approach relates previous work topic modeling statistical discourse processing.
provide problem formulation Section 3.1 followed overview content
model Section 3.2. heart model distribution topic permutations,
provide background Section 3.3, employing formal description
models probabilistic generative story Section 3.4. Section 4 discusses estimation models posterior distribution given example documents using collapsed Gibbs
sampling procedure. Techniques applying model three tasks alignment,
segmentation, ordering explained Section 5. evaluate models performance tasks Section 6 concluding touching upon directions
future work Section 7. Code, data sets, annotations, raw outputs
experiments available http://groups.csail.mit.edu/rbg/code/mallows/.

2. Related Work
describe two areas previous work related approach. algorithmic
perspective work falls broad class topic models. earlier work topic
modeling took bag words view documents, many recent approaches expanded
topic models capture structural constraints. Section 2.1, describe extensions highlight differences model. linguistic side, work
relates research modeling text structure statistical discourse processing. summarize work Section 2.2, drawing comparisons functionality supported
model.
2.1 Topic Models
Probabilistic topic models, originally developed context language modeling,
today become popular range NLP applications, text classification document browsing. Topic models posit latent state variable controls generation
word. parameters estimated using approximate inference techniques
Gibbs sampling variational methods. traditional topic models Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004), documents
treated bags words, word receives separate topic assignment words
assigned topic drawn shared language model.
bag words representation sufficient applications, many cases
structure-unaware view limited. Previous research considered extensions
LDA models two orthogonal directions, covering intrasentential extrasentential
constraints.

131

fiChen, Branavan, Barzilay, & Karger

2.1.1 Modeling Intrasentential Constraints
One promising direction improving topic models augment constraints
topic assignments adjoining words within sentences. example, Griffiths, Steyvers,
Blei, Tenenbaum (2005) propose model jointly incorporates syntactic
semantic information unified generative framework constrains syntactic classes
adjacent words. approach, generation word controlled two hidden
variables, one specifying semantic topic specifying syntactic class.
syntactic class hidden variables chained together Markov model, whereas semantic
topic assignments assumed independent every word.
another example intrasentential constraints, Wallach (2006) proposes way
incorporate word order information, form bigrams, LDA-style model.
approach, generation word conditioned previous word
topic current word, word topics generated perdocument topic distributions LDA. formulation models text structure
level word transitions, opposed work Griffiths et al. (2005) structure
modeled level hidden syntactic class transitions.
focus modeling high-level document structure terms semantic content.
such, work complementary methods impose structure intrasentential
units; possible combine model constraints adjoining words.
2.1.2 Modeling Extrasentential Constraints
Given intuitive connection notion topic LDA notion topic
discourse analysis, natural assume LDA-like models useful discourselevel tasks segmentation topic classification. hypothesis motivated research
models topic assignment guided structural considerations (Purver, Kording,
Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly
relationships topics adjacent textual units. Depending application,
textual unit may sentence, paragraph, speaker utterance. common property
models bias topic assignments cohere within local segments text.
Models category vary terms mechanisms used encourage local topic
coherence. instance, model Purver et al. (2006) biases topic distributions
adjacent utterances (textual units) similar. model generates utterance
mixture topic language models. parameters topic mixture distribution assumed follow type Markovian transition process specifically, high
probability utterance u topic distribution previous utterance
u 1; otherwise, new topic distribution drawn u. Thus, textual units topic
distribution depends previous textual unit, controlled parameter indicating
whether new topic distribution drawn.
similar vein, Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) posits
generative process sentence (textual unit) assigned single topic,
sentences words drawn single language model. model
Purver et al., topic transitions adjacent textual units modeled Markovian
fashion specifically, sentence topic sentence 1 high probability,
receives new topic assignment drawn shared topic multinomial distribution.

132

fiContent Modeling Using Latent Permutations

HTMM model, assumption single topic per textual unit allows
sections text related across documents topic. contrast, Purver et al.s model
tailored task segmentation, utterance drawn mixture topics.
Thus, model capture utterances topically aligned across related
documents. importantly, HTMM model Purver et al. able
make local decisions regarding topic transitions, thus difficulty respecting longrange discourse constraints topic contiguity. model instead takes global view
topic assignments textual units explicitly generating entire documents topic
ordering one joint distribution. show later paper, global view yields
significant performance gains.
recent Multi-Grain Latent Dirichlet Allocation (MGLDA) model (Titov & McDonald, 2008) also studied topic assignments level sub-document textual units.
MGLDA, set local topic distributions induced sentence, dependent
window local context around sentence. Individual words drawn either
local topics document-level topics standard LDA. MGLDA represents
local context using sliding window, window frame comprises overlapping short
spans sentences. way, local topic distributions shared sentences
close proximity.
MGLDA represent complex topical dependencies models Purver
et al. Gruber et al., window incorporate much wider swath local
context two adjacent textual units. However, MGLDA unable encode longer
range constraints, contiguity ordering similarity, sentences close
proximity loosely connected series intervening window frames.
contrast, work specifically oriented toward long-range constraints, necessitating
whole-document notion topic assignment.
2.2 Modeling Ordering Constraints Statistical Discourse Analysis
global constraints encoded model closely related research discourse
information ordering applications text summarization generation (Barzilay,
Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004;
Elsner et al., 2007). emphasis body work learning ordering constraints
data, goal reordering new text domain. methods build
assumption recurring patterns topic ordering discovered analyzing
patterns word distribution. key distinction prior methods approach
existing ordering models largely driven local constraints limited ability
capture global structure. Below, describe two main classes probabilistic ordering
models studied discourse processing.
2.2.1 Discriminative Models
Discriminative approaches aim directly predict ordering given set sentences.
Modeling ordering sentences simultaneously leads complex structure prediction
problem. practice, however, computationally tractable two-step approach taken:
first, probabilistic models used estimate pairwise sentence ordering preferences; next,
local decisions combined produce consistent global ordering (Lapata, 2003;

133

fiChen, Branavan, Barzilay, & Karger

Althaus, Karamanis, & Koller, 2004). Training data pairwise models constructed
considering pairs sentences document, supervision labels based
actually ordered. Prior work demonstrated wide range features
useful classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji &
Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006). instance, Lapata (2003)
demonstrated lexical features, verb pairs input sentences, serve
proxy plausible sequences actions, thus effective predictors well-formed
orderings. second stage, local decisions integrated global order
maximizes number consistent pairwise classifications. Since finding
ordering NP-hard (Cohen, Schapire, & Singer, 1999), various approximations used
practice (Lapata, 2003; Althaus et al., 2004).
two-step discriminative approaches effectively leverage information
local transitions, provide means representing global constraints.
recent work, Barzilay Lapata (2008) demonstrated certain global properties captured discriminative framework using reranking mechanism.
set-up, system learns identify best global ordering given set n possible
candidate orderings. accuracy ranking approach greatly depends quality
selected candidates. Identifying candidates challenging task given large
search space possible alternatives.
approach presented work differs existing discriminative models two
ways. First, model represents distribution possible global orderings. Thus,
use sampling mechanisms consider whole space rather limited
subset candidates ranking models. second difference arises
generative nature model. Rather focusing ordering task, order-aware
model effectively captures layer hidden variables explain underlying structure
document content. Thus, effectively applied wider variety applications,
including sentence ordering already observed, appropriately adjusting
observed hidden components model.
2.2.2 Generative Models
work closer technique generative models treat topics hidden variables.
One instance work Hidden Markov Model (HMM)-based content model (Barzilay & Lee, 2004). model, states correspond topics state transitions represent
ordering preferences; hidden states emission distribution language model
words. Thus, similar approach, models implicitly represent patterns
level topical structure. HMM used ranking framework select
ordering highest probability.
recent work, Elsner et al. (2007) developed search procedure based simulated annealing finds high likelihood ordering. contrast ranking-based approaches, search procedure cover entire ordering space. hand,
show Section 5.3, define ordering objective maximized
efficiently possible orderings prediction model parameters
learned. Specifically, bag p paragraphs, O(pK) calculations paragraph
probabilities necessary, K number topics.

134

fiContent Modeling Using Latent Permutations

Another distinction proposed model prior work way global
ordering constraints encoded. Markovian model, possible induce global
constraints introducing additional local constraints. instance, topic contiguity
enforced selecting appropriate model topology (e.g., augmenting hidden states
record previously visited states). However, global constraints, similarity
overall ordering across documents, much challenging represent. explicitly
modeling topic permutation distribution, easily capture kind global
constraint, ultimately resulting accurate topic models orderings. show
later paper, model substantially outperforms approach Barzilay Lee
information ordering task applied HMM-based content model.

3. Model
section, describe problem formulation proposed model.
3.1 Problem Formulation
content modeling problem formalized follows. take input corpus
{d1 , . . . dD } related documents, specification number topics K.3
document comprised ordered sequence Nd paragraphs (pd,1 , . . . , pd,Nd ).
output, predict single topic assignment zd,p {1, . . . , K} paragraph p.4
z values reflect underlying content organization document
related content discussed within document, across separate documents,
receive z value.
formulation shares similarity standard LDA setup common
set topics assigned across collection documents. difference LDA
words topic assignment conditionally independent, following bag words view
documents, whereas constraints topics assigned let us connect word
distributional patterns document-level topic structure.
3.2 Model Overview
propose generative Bayesian model explains corpus documents
produced set hidden variables. high level, model first selects
frequently topic expressed document, topics ordered.
topics determine selection words paragraph. Notation used
subsequent sections summarized Figure 1.
document Nd paragraphs, separately generate bag topics td
topic ordering . unordered bag topics td , contains Nd elements, expresses
many paragraphs document assigned K topics. Equivalently,
td viewed vector occurrence counts topic, zero counts
topics appear all. Variable td constructed taking Nd samples
3. nonparametric extension model would also learn K.
4. well structured documents, paragraphs tend internally topically consistent (Halliday & Hasan,
1976), predicting one topic per paragraph sufficient. However, note approach
applied modifications levels textual granularity sentences.

135

fiChen, Branavan, Barzilay, & Karger



parameters distribution
topic counts



parameters distribution
topic orderings



vector topic counts

v

vector inversion counts



topic ordering

z

paragraph topic assignment



language model parameters
topic

Dirichlet(0 )
j = 1 . . . K 1:
j GMM0 (0 , 0 )
k = 1 . . . K:
k Dirichlet(0 )

td
vd
=
zd =

w document words

document
Multinomial()
GMM()
Compute-(vd )
Compute-z(td , )

paragraph p
word w p
w Multinomial(zd,p )

K number topics
number documents corpus
Nd number paragraphs
document
Np number words paragraph p

Algorithm: Compute-
Input: Inversion count vector v

Algorithm: Compute-z
Input: Topic counts t, permutation

Create empty list
[1] K
j = K 1 1
= K 1 v[j]
[i + 1] [i]
[v[j]] j

Create empty list z
end 1
k = K 1
= 1 t[[k]]
z[end] [k]
end end + 1

Output: Permutation

Output: Paragraph topic vector z

Figure 1: plate diagram generative process model, along table
notation reference purposes. Shaded circles figure denote observed
variables, squares denote hyperparameters. dotted arrows indicate
constructed deterministically v according algorithm Compute-,
z constructed deterministically according Compute-z.
136

fiContent Modeling Using Latent Permutations

distribution topics , multinomial representing probability topic
expressed. Sharing documents captures notion certain topics
likely across documents corpus.
topic ordering variable permutation numbers 1 K
defines order topics appear document. draw Generalized
Mallows Model, distribution permutations explain Section 3.3.
see, particular distribution biases permutation selection close single
centroid, reflecting discourse constraint preferring similar topic structures across
documents.
Together, documents bag topics td ordering determine topic assignment
zd,p paragraphs. example, corpus K = 4, seven-paragraph
document td = {1, 1, 1, 1, 2, 4, 4} = (2, 4, 3, 1) would induce topic sequence
zd = (2, 4, 4, 1, 1, 1, 1). induced topic sequence zd never assign topic
two unconnected portions document, thus satisfying constraint topic contiguity.
assume topic k associated language model k . words
paragraph assigned topic k drawn topics language model k .
portion similar standard LDA topic relates language model.
However, unlike LDA, model enforces topic coherence entire paragraph rather
viewing paragraph mixture topics.
turning formal discussion generative process, first provide
background permutation model topic ordering.
3.3 Generalized Mallows Model Permutations
central challenge approach presented modeling distribution possible topic orderings. purpose use Generalized Mallows Model (GMM) (Fligner
& Verducci, 1986; Lebanon & Lafferty, 2002; Meila, Phadnis, Patterson, & Bilmes, 2007;
Klementiev, Roth, & Small, 2008), exhibits two appealing properties context
task. First, model concentrates probability mass canonical ordering
small perturbations (permutations) ordering. characteristic matches
constraint documents domain exhibit structural similarity. Second,
parameter set scales linearly number elements ordered, making
sufficiently constrained tractable inference.
first describe standard Mallows Model orderings (Mallows, 1957).
Mallows Model takes two parameters, canonical ordering dispersion parameter .
sets probability ordering proportional ed(,) ,
d(, ) represents distance metric orderings . Frequently, metric
Kendall distance, minimum number swaps adjacent elements needed
transform ordering canonical ordering . Thus, orderings close
canonical ordering high probability, many elements
moved less probability mass.
Generalized Mallows Model, first introduced Fligner Verducci (1986), refines
standard Mallows Model adding additional set dispersion parameters.
parameters break apart distance d(, ) orderings set independent
components. component separately vary sensitivity perturbation.

137

fiChen, Branavan, Barzilay, & Karger

tease apart distance function components, GMM distribution considers
inversions required transform canonical ordering observed ordering. first
discuss inversions parameterized GMM, turn distributions
definition characteristics.
3.3.1 Inversion Representation Permutations
Typically, permutations represented directly ordered sequence elements
example, (3, 1, 2) represents permuting initial order placing third element
first, followed first element, second. GMM utilizes alternative
permutation representation defined vector (v1 , . . . , vK1 ) inversion counts
respect identity permutation (1, . . . , K). Term vj counts number times
value greater j appears j permutation. Note jth inversion
count vj take integer values 0 K j inclusive. Thus inversion count
vector K 1 elements, vK always zero. instance, given standard
form permutation (3, 1, 5, 6, 2, 4), v2 = 3 3, 5, 6 appear 2, v3 = 0
numbers appear it; entire inversion count vector would (1, 3, 0, 2, 0).
Likewise, previous example permutation (2, 4, 3, 1) maps inversion counts (3, 0, 1).
sum components entire inversion count vector simply orderings
Kendall distance canonical ordering.
significant appeal inversion representation every valid, distinct vector
inversion counts corresponds distinct permutation vice versa. see this,
note permutation straightforwardly compute inversion counts.
Conversely, given sequence inversion counts, construct unique corresponding
permutation. insert items permutation, working backwards item K.
Assume already placed items j + 1 K proper order. insert
item j, note exactly vj items j + 1 K must precede it, meaning
must inserted position vj current order (see Compute- algorithm
Figure 1). Since one place j inserted fulfills inversion
counts, induction shows exactly one permutation constructed satisfy
given inversion counts.
model, take canonical topic ordering always identity ordering
(1, . . . , K). topic numbers task completely symmetric linked
extrinsic meaning, fixing global ordering specific arbitrary value
sacrifice representational power. general case GMM, canonical ordering
parameter distribution.
3.3.2 Probability Mass Function
GMM assigns probability mass particular order based order permuted canonical ordering. precisely, associates distance every
permutation, canonical ordering distance zero permutations many
inversions respect canonical ordering larger distance. distance assignment based K 1 real-valued dispersion parameters P
(1 , . . . , K1 ). distance
permutation inversion counts v defined j j vj . GMMs probability

138

fiContent Modeling Using Latent Permutations

mass function exponential distance:
P

e j j vj
GMM(v; ) =
()
=

K1

j=1

() =

Q

j

ej vj
,
j (j )

(1)

j (j ) normalization factor value:
j (j ) =

1 e(Kj+1)j
.
1 ej

(2)

Setting j equal single value recovers standard Mallows Model Kendall
distance function. factorization GMM independent probabilities per
inversion count makes distribution particularly easy apply; use GMMj refer
jth multiplicand probability mass function, marginal distribution
vj :
GMMj (vj ; j ) =

ej vj
.
j (j )

(3)

Due exponential form distribution, requiring j > 0 constrains GMM
assign highest probability mass vj zero, i.e., distributional mode
canonical identity permutation. higher value j assigns probability mass vj
close zero, biasing j fewer inversions.
3.3.3 Conjugate Prior
major benefit GMM membership exponential family distributions;
means particularly amenable Bayesian representation, admits
natural independent conjugate prior parameter j (Fligner & Verducci, 1990):
GMM0 (j | vj,0 , 0 ) e(j vj,0 log j (j ))0 .

(4)

prior distribution takes two parameters 0 vj,0 . Intuitively, prior states
0 previous trials, total number inversions observed 0 vj,0 . distribution
easily updated observed vj derive posterior distribution.
vj different range, inconvenient set prior hyperparameters
vj,0 directly. work, instead assign common prior value parameter j ,
denote 0 . set vj,0 maximum likelihood estimate
j 0 . differentiating likelihood GMM respect j , straightforward
verify works setting:
vj,0 =

e0

K j+1
1
(Kj+1)
.
0 1
1 e

139

(5)

fiChen, Branavan, Barzilay, & Karger

3.4 Formal Generative Process
fully specify details content model, whose plate diagram appears
Figure 1. observe corpus documents, document ordered
sequence Nd paragraphs paragraph represented bag words. number
topics K assumed pre-specified. model induces set hidden variables
probabilistically explain words corpus produced. final desired
output posterior distributions paragraphs hidden topic assignment variables.
following, variables subscripted 0 fixed prior hyperparameters.
1. topic k, draw language model k Dirichlet(0 ). LDA,
topic-specific word distributions.
2. Draw topic distribution Dirichlet(0 ), expresses likely topic
appear regardless position.
3. Draw topic ordering distribution parameters j GMM0 (0 , 0 ) j = 1
K 1. parameters control rapidly probability mass decays
inversions topic. separate j every topic allows us learn
topics likely reordered others.
4. document Nd paragraphs:
(a) Draw bag topics td sampling Nd times Multinomial().
(b) Draw topic ordering , sampling vector inversion counts vd GMM(),
applying algorithm Compute- Figure 1 vd .
(c) Compute vector topic assignments zd document ds paragraphs
sorting td according , algorithm Compute-z Figure 1.5
(d) paragraph p document d:
i. Sample word w p according language model p: w
Multinomial(zd,p ).
3.5 Properties Model
section describe rationale behind using GMM represent ordering
component content model.
Representational Power GMM concentrates probability mass around one centroid permutation, reflecting preferred bias toward document structures similar topic orderings. Furthermore, parameterization GMM using vector
dispersion parameters allows flexibility strongly model biases toward
single ordering one extreme ( = ) one ordering nonzero probability, ( = 0) orderings equally likely. comprised
5. Multiple permutations contribute probability single documents topic assignments zd ,
topics appear td . result, current formulation biased toward
assignments fewer topics per document. practice, find negatively impact model
performance.

140

fiContent Modeling Using Latent Permutations

independent dispersion parameters (1 , . . . , K1 ), distribution assign different penalties displacing different topics. example, may learn middle
sections (in case Cities, sections Economy Culture) likely
vary position across documents early sections (such Introduction
History).
Computational Benefits parameterization GMM using vector dispersion parameters compact tractable. Since number parameters grows
linearly number topics, model efficiently handle longer documents
greater diversity content.
Another computational advantage model seamless integration larger
Bayesian model. Due membership exponential family existence
conjugate prior, inference become significantly complex
GMM used hierarchical context. case, entire document generative
model also accounts topic frequency words within topic.
One final beneficial effect GMM breaks symmetry topic assignments fixing distribution centroid. Specifically, topic assignments
invariant relabeling, probability underlying permutation would
change. contrast, many topic models assign probability relabeling
topic assignments. model thus sidesteps problem topic identifiability, issue model may multiple maxima likelihood due
underlying symmetry hidden variables. Non-identifiable models
standard LDA may cause sampling procedures jump maxima produce
draws difficult aggregate across runs.
Finally, show Section 6 benefits GMM extend theoretical empirical: representing permutations using GMM almost always leads
superior performance compared alternative approaches.

4. Inference
variables aim infer paragraph topic assignments z, determined bag topics ordering document. Thus, goal estimate
joint marginal distributions given document text integrating
remaining hidden parameters:
P (t, , | w).
(6)
accomplish inference task Gibbs sampling (Geman & Geman, 1984; Bishop,
2006). Gibbs sampler builds Markov chain hidden variable state space whose
stationary distribution actual posterior joint distribution. new sample
drawn distribution single variable conditioned previous samples
variables. collapse sampler integrating hidden
variables model, effect reducing state space Markov chain. Collapsed
sampling previously demonstrated effective LDA variants (Griffiths
& Steyvers, 2004; Porteous, Newman, Ihler, Asuncion, Smyth, & Welling, 2008; Titov &

141

fiChen, Branavan, Barzilay, & Karger

P (td,i = | . . .) P (td,i = | t(d,i) , 0 ) P (wd | td , , wd , zd , 0 )


N (t(d,i) , t) + 0

P (wd | z, wd , 0 ),
|t(d,i) | + K0

P (vd,j = v | . . .) P (vd,j = v | j ) P (wd | td , , wd , zd , 0 )
= GMMj (v; j ) P (wd | z, wd , 0 ),

P


vd,j + vj,0 0
P (j | . . .) = GMM0 j ;
, N + 0 ,
N + 0
Figure 2: collapsed Gibbs sampling inference procedure estimating models
posterior distribution. plate diagram, variable resampled
shown double circle Markov blanket highlighted black;
variables, impact variable resampled, grayed out.
Variables , shown dotted circles, never explicitly depended
re-estimated, marginalized sampler. diagram
accompanied conditional resampling distribution respective variable.

142

fiContent Modeling Using Latent Permutations

McDonald, 2008). typically preferred explicit Gibbs sampling hidden
variables smaller search space generally shorter mixing time.
sampler analytically integrates three sets hidden variables: bags
topics t, orderings , permutation inversion parameters . burn-in period,
treat last samples draw posterior. samples
marginalized variables necessary, estimated based topic
assignments show Section 5.3. Figure 2 summarizes Gibbs sampling steps
inference procedure.
Document Probability preliminary step, consider calculate probability
single documents words wd given documents paragraph topic assignments zd
remaining documents topic assignments. Note probability decomposable product probabilities individual paragraphs paragraphs
different topics conditionally independent word probabilities. Let wd zd indicate words topic assignments documents d, W vocabulary
size. probability words then:
K Z

P (wd | z, wd , 0 ) =
P (wd | zd , k ) P (k | z, wd , 0 ) dk

=

k=1 k
K


DCM({wd,i : zd,i = k} | {wd,i : zd,i = k}, 0 ),

(7)

k=1

DCM() refers Dirichlet compound multinomial distribution, result
integrating multinomial parameters Dirichlet prior (Bernardo & Smith, 2000).
Dirichlet prior parameters = (1 , . . . , W ), DCM assigns following
probability series observations x = {x1 , . . . , xn }:
P
W
( j j )
(N (x, i) + )
P
DCM(x; ) = Q
,
(8)
(|x| + j j )
j (j )
i=1

N (x, i) refers number times word appears x. Here, () Gamma
function, generalization factorial real numbers. algebra shows
DCMs posterior probability density function conditioned series observations =
{y1 , . . . , yn } computed updating counts often word appears
y:
DCM(x | y, ) = DCM(x; 1 + N (y, 1), . . . , W + N (y, W )).

(9)

Equations 7 9 used compute conditional distributions hidden
variables. turn individual random variable resampled.
Bag Topics First consider resample td,i , ith topic draw document
conditioned parameters fixed (note topic ith
paragraph, reorder topics using , generated separately):
P (td,i = | . . .) P (td,i = | t(d,i) , 0 ) P (wd | td , , wd , zd , 0 )


N (t(d,i) , t) + 0

P (wd | z, wd , 0 ),
|t(d,i) | + K0
143

(10)

fiChen, Branavan, Barzilay, & Karger

td updated reflect td,i = t, zd deterministically computed last step
using Compute-z Figure 1 inputs td . first step reflects application
Bayes rule factor term wd ; drop superfluous terms
conditioning. second step, former term arises DCM, updating
parameters 0 observations t(d,i) Equation 9 dropping constants.
latter document probability term computed using Equation 7. new td,i selected
sampling probability computed possible topic assignments.
Ordering parameterization permutation series inversion values vd,j
reveals natural way decompose search space Gibbs sampling. document
d, resample vd,j j = 1 K 1 independently successively according
conditional distribution:
P (vd,j = v | . . .) P (vd,j = v | j ) P (wd | td , , wd , zd , 0 )
= GMMj (v; j ) P (wd | z, wd , 0 ),

(11)

updated reflect vd,j = v, zd computed deterministically according
td . first term refers Equation 3; second computed using Equation 7.
probability computed every possible value v, ranges 0 K j,
term vd,j sampled according resulting probabilities.
GMM Parameters j = 1 K 1, resample j posterior distribution:
P


vd,j + vj,0 0
P (j | . . .) = GMM0 j ;
, N + 0 ,
(12)
N + 0
GMM0 evaluated according Equation 4. normalization constant
distribution unknown, meaning cannot directly compute invert cumulative distribution function sample distribution. However, distribution
univariate unimodal, expect MCMC technique slice
sampling (Neal, 2003) perform well. practice, Matlabs built-in slice sampler
provides robust draw distribution.6
Computational Issues inference, directly computing document probabilities
basis Equation 7 results many redundant calculations slow runtime
iteration considerably. improve computational performance proposed
inference procedure, apply memoization techniques sampling. Within
single iteration, document, Gibbs sampler requires computing documents
probability given topic assignments (Equation 7) many times, computation
frequently conditions slight variations topic assignments. nave approach
would compute probability every paragraph time document probability
desired, performing redundant calculations topic assignment sequences shared
subsequences repeatedly considered.
Instead, use lazy evaluation build three-dimensional cache, indexed tuple
(i, j, k), follows. time document probability requested, broken independent subspans paragraphs, subspan takes one contiguous topic assignment. possible due way Equation 7 factorizes independent per-topic
6. particular, use slicesample function Matlab Statistics Toolbox.

144

fiContent Modeling Using Latent Permutations

multiplicands. subspan starting paragraph i, ending paragraph j, assigned topic k, cache consulted using key (i, j, k). example, topic assignments
zd = (2, 4, 4, 1, 1, 1, 1) would result cache lookups (1, 1, 2), (2, 3, 4), (4, 7, 1).
cached value unavailable, correct probability computed using Equation 7
result stored cache location (i, j, k). Moreover, also record values every
intermediate cache location (i, l, k) l = j 1, values computed
subproblems evaluating Equation 7 (i, j, k). cache reset proceeding
next document since conditioning changes documents. document,
caching guarantees O(Nd2 K) paragraph probability calculations.
practice, individual Gibbs steps small, bound loose
caching mechanism reduces computation time several orders magnitude.
also maintain caches word-topic paragraph-topic assignment frequencies,
allowing us rapidly compute counts used equations 7 10. form caching
used Griffiths Steyvers (2004).

5. Applications
section, describe model applied three challenging discourselevel tasks: aligning paragraphs similar topical content documents, segmenting
document topically cohesive sections, ordering new unseen paragraphs
coherent document. particular, show posterior samples produced
inference procedure Section 4 used derive solution tasks.
5.1 Alignment
alignment task wish find paragraphs document topically
relate paragraphs documents. Essentially, cross-document clustering
task alignment assigns paragraph document one K topically related
groupings. instance, given set cell phone reviews, one group may represent text
fragments discuss Price, another group consists fragments Reception.
model readily employed task: view topic assignment
paragraph z cluster label. example, two documents d1 d2
topic assignments zd1 = (2, 4, 4, 1, 1, 1, 1) zd2 = (4, 4, 3, 3, 2, 2, 2), paragraph 1 d1
grouped together paragraphs 5 7 d2 , paragraphs 2 3 d1 1
2 d2 . remaining paragraphs assigned topics 1 3 form separate
per-document clusters.
Previously developed methods cross-document alignment primarily driven
similarity functions quantify lexical overlap textual units (Barzilay & Elhadad, 2003; Nelken & Shieber, 2006). methods explicitly model document
structure, specify global constraints guide search optimal
alignment. Pairs textual units considered isolation making alignment decisions. contrast, approach allows us take advantage global structure shared
language models across related textual units without requiring manual specification
matching constraints.

145

fiChen, Branavan, Barzilay, & Karger

5.2 Segmentation
Segmentation well-studied discourse task goal divide document
topically cohesive contiguous sections. Previous approaches typically relied lexical
cohesion is, similarity word choices within document subspan guide
choice segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, &
Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay,
2008). model relies notion determining language models topics,
connecting topics across documents constraining topics appear allow
better learn words indicative topic cohesion.
output samples models inference procedure map straightforwardly
segmentations contiguous spans paragraphs assigned topic number taken one segment. example, seven-paragraph document topic
assignments zd = (2, 4, 4, 1, 1, 1, 1) would segmented three sections, comprised
paragraph 1, paragraphs 2 3, paragraphs 4 7. Note segmentation ignores specific values used topic assignments, heeds paragraph
boundaries topic assignments change.
5.3 Ordering
third application model problem creating structured documents
collections unordered text segments. text ordering task important step
broader NLP tasks text summarization generation. task, assume
provided well structured documents single domain training examples;
trained, model used induce ordering previously unseen collections
paragraphs domain.
training, model learns canonical ordering topics documents within
collection, via language models associated topic. GMM
concentrates probability mass around canonical (1, . . . , K) topic ordering, expect
highly probable words language models lower -numbered topics tend appear early
document, whereas highly probable words language models higher -numbered
topics tend appear late document. Thus, structure new documents according
intuition paragraphs words tied low topic numbers placed earlier
paragraphs words relating high topic numbers.
Formally, given unseen document comprised unordered set paragraphs
{p1 , . . . , pn }, order paragraphs according following procedure. First, find
probable topic assignment zi independently paragraph pi , according
parameters learned training phase:
zi = arg max P (zi = k | pi , , )
k

= arg max P (pi | zi = k, k )P (zi = k | ).

(13)

k

Second, sort paragraphs topic assignment zi ascending order since (1 . . . K)
GMMs canonical ordering, yields likely ordering conditioned single
estimated topic assignment paragraph. Due possible ties topic assignments,
146

fiContent Modeling Using Latent Permutations

resulting document may partial ordering; full ordering required, ties
broken arbitrarily.
key advantage proposed approach closed-form computationally
efficient. Though training phase requires running inference procedure Section 4,
model parameters learned, predicting ordering new set p paragraphs
requires computing pK probability scores. contrast, previous approaches
able rank small subset possible document reorderings (Barzilay &
Lapata, 2008), performed search procedure space orderings find
optimum (Elsner et al., 2007).7
objective function Equation 13 depends posterior estimates given
training documents. Since collapsed Gibbs sampler integrates two hidden
variables, need back values known posterior samples
z. easily done computing point estimate distribution based
word-topic topic-document assignment frequencies, respectively, done Griffiths
Steyvers (2004). probability mass kw word w language model topic k
given by:
N (k, w) + 0
kw =
,
(14)
N (k) + W 0
N (k, w) total number times word w assigned topic k, N (k)
total number words assigned topic k, according posterior sample z.
derive similar estimate k , prior likelihood topic k:
k =

N (k) + 0
,
N + K0

(15)

N (k) total number paragraphs assigned topic k according sample
z, N total number paragraphs entire corpus.

6. Experiments
section, evaluate performance model three tasks presented
Section 5: cross-document alignment, document segmentation, information ordering.
first describe preliminaries common three tasks, covering data sets,
reference comparison structures, model variants, inference algorithm settings shared
evaluation. provide detailed examination model performs
individual task.
6.1 General Evaluation Setup
Data Sets experiments use five data sets, briefly described (for additional
statistics, see Table 1):
7. approach describe finding probable paragraph ordering according
data likelihood, optimal ordering derived HMM-based content model.
proposed ordering technique essentially approximates objective using per-paragraph maximum
posteriori estimate topic assignments rather full posterior topic assignment distribution.
approximation makes much faster prediction algorithm performs well empirically.

147

fiChen, Branavan, Barzilay, & Karger

Articles
Corpus
CitiesEn
CitiesEn500
CitiesFr

large cities Wikipedia
Language Documents Sections
English
100
13.2
English
500
10.5
French
100
10.4

Paragraphs
66.7
45.9
40.7

Vocabulary
42,000
95,400
31,000

Tokens
4,920
3,150
2,630

Articles chemical elements Wikipedia
Corpus
Language Documents Sections
Elements
English
118
7.7

Paragraphs
28.1

Vocabulary
18,000

Tokens
1,920

Cell phone reviews PhoneArena.com
Corpus
Language Documents Sections
Phones
English
100
6.6

Paragraphs
24.0

Vocabulary
13,500

Tokens
2,750

Table 1: Statistics data sets used evaluations. values except vocabulary
size document count per-document averages.

CitiesEn: Articles English Wikipedia worlds 100 largest cities
population. Common topics include History, Culture, Demographics. articles typically substantial size share similar content organization patterns.
CitiesEn500 : Articles English Wikipedia worlds 500 largest cities
population. collection superset CitiesEn. Many lower-ranked
cities well known English Wikipedia editors thus, compared CitiesEn
articles shorter average exhibit greater variability content selection
ordering.
CitiesFr : Articles French Wikipedia 100 cities CitiesEn.
Elements: Articles English Wikipedia chemical elements periodic table,8 including topics Biological Role, Occurrence, Isotopes.
Phones: Reviews extracted PhoneArena.com, popular cell phone review website. Topics corpus include Design, Camera, Interface. reviews
written expert reviewers employed site, opposed lay users.9
heterogeneous collection data sets allows us examine behavior
model diverse test conditions. sets vary articles generated,
language articles written, subjects discuss. result,
patterns topic organization vary greatly across domains. instance, within Phones
corpus, articles formulaic, due centralized editorial control website,
establishes consistent standards followed expert reviewers. hand,
Wikipedia articles exhibit broader structural variability due collaborative nature
8. 118 elements http://en.wikipedia.org/wiki/Periodic table, including undiscovered element 117.
9. Phones set, 35 documents short express reviews without section headings; include
input model, evaluate them.

148

fiContent Modeling Using Latent Permutations

Wikipedia editing, allows articles evolve independently. Wikipedia articles
within category often exhibit similar section orderings, many idiosyncratic
inversions. instance, CitiesEn corpus, Geography History sections
typically occur toward beginning document, History appear either
Geography across different documents.
corpus consider manually divided sections authors,
including short textual heading section. Sections 6.2.1 6.3.1, discuss
author-created sections headings used generate reference annotations
alignment segmentation tasks. Note use headings evaluation;
none heading information provided methods consideration.
tasks alignment segmentation, evaluation performed datasets presented
Table 1. ordering task, however, data used training, evaluation
performed using separate held-out set documents. details held-out dataset
given Section 6.4.1.
Model Variants evaluation, besides comparing baselines literature,
also consider two variants proposed model. particular, investigate
impact Mallows component model alternately relaxing tightening
way constrains topic orderings:
Constrained : variant, require documents follow exact canonical ordering topics. is, topic permutation inversions allowed, though
documents may skip topics before. case viewed special case
general model, Mallows inversion prior 0 approaches infinity.
implementation standpoint, simply fix inversion counts v zero
inference.10
Uniform: variant assumes uniform distribution topic permutations,
instead biasing toward small related set. Again, special case full
model, inversion prior 0 set zero, strength prior 0 approaching
infinity, thus forcing item always zero.
Note variants still enforce long-range constraint topic contiguity,
vary full model capture topic ordering similarity.
Evaluation Procedure Parameter Settings evaluation model
variants, run collapsed Gibbs sampler five random seed states, take
10,000th iteration chain sample. Results presented average
five samples.
Dirichlet prior hyperparameters bag topics 0 language models 0 set
0.1. GMM, set prior dispersion hyperparameter 0 1, effective
10. first glance, Constrained model variant appears equivalent HMM state
transition either + 1. However, case topics may appear zero times
document, resulting multiple possible transitions state. Furthermore, transition
probabilities would dependent position within document example, earlier absolute
positions within document, transitions high-index topics unlikely, would require
subsequent paragraphs high-index topic.

149

fiChen, Branavan, Barzilay, & Karger

sample size prior 0 0.1 times number documents. values minimally
tuned, similar results achieved alternative settings 0 0 . Parameters 0
0 control strength bias toward structural regularity, trading
Constrained Uniform model variants. values chosen middle ground
two extremes.
model also takes parameter K controls upper bound number
latent topics. Note algorithm select fewer K topics document,
K determine number segments document. general, higher K
results finer-grained division document different topics, may result
precise topics, may also split topics together. report results
evaluation using K = 10 20.
6.2 Alignment
first evaluate model task cross-document alignment, goal
group textual units different documents topically cohesive clusters. instance,
Cities-related domains, one cluster may include Transportation-related paragraphs. turning results first present details specific evaluation setup
targeted task.
6.2.1 Alignment Evaluation Setup
Reference Annotations generate sufficient amount reference data evaluating alignments use section headings provided authors. assume two
paragraphs aligned section headings identical. headings
constitute noisy annotations Wikipedia datasets: topical content may
labeled different section headings different articles (e.g., CitiesEn, Places
interest one article Landmarks another), call reference structure
noisy headings set.
clear priori effect noise section headings may evaluation accuracy. empirically estimate effect, also use manually annotated
alignments experiments. Specifically, CitiesEn corpus, manually annotated articles paragraphs consistent set section headings, providing us
additional reference structure evaluate against. clean headings set, found
approximately 18 topics expressed one document.
Metrics quantify alignment output compute recall precision score
candidate alignment reference alignment. Recall measures, unique
section heading reference, maximum number paragraphs heading
assigned one particular topic. final score computed summing
section heading dividing total number paragraphs. High recall indicates
paragraphs section headings generally assigned topic.
Conversely, precision measures, topic number, maximum number paragraphs topic assignment share section heading. Precision summed
topic normalized total number paragraphs. High precision means
paragraphs assigned single topic usually correspond section heading.

150

fiContent Modeling Using Latent Permutations

Recall precision trade finely grained topics tend
improve precision cost recall. extremes, perfect recall occurs every
paragraph assigned topic, perfect precision paragraph
topic.
also present one summary F-score results, harmonic mean
recall precision.
Statistical significance setup measured approximate randomization (Noreen,
1989), nonparametric test directly applied nonlinearly computed metrics
F-score. test used prior evaluations information extraction
machine translation (Chinchor, 1995; Riezler & Maxwell, 2005).
Baselines

task, compare two baselines:

Hidden Topic Markov Model (HTMM) (Gruber et al., 2007): explained Section 2, model represents topic change adjacent textual units Markovian fashion. HTMM capture local constraints, would allow topics
recur non-contiguously throughout document. use publicly available implementation,11 priors set according recommendations made original
work.
Clustering: use repeated bisection algorithm find clustering paragraphs maximizes sum pairwise cosine similarities items
cluster.12 clustering implemented using CLUTO toolkit.13 Note
approach completely structure-agnostic, treating documents bags paragraphs rather sequences paragraphs. types clustering techniques
shown deliver competitive performance cross-document alignment
tasks (Barzilay & Elhadad, 2003).
6.2.2 Alignment Results
Table 2 presents results alignment evaluation. datasets, best
performance achieved model variants, statistically significant usually
substantial margin.
comparative performance baseline methods consistent across domains
surprisingly, clustering performs better complex HTMM model. observation consistent previous work cross-document alignment multidocument
summarization, use clustering main component (Radev, Jing, & Budzikowska,
2000; Barzilay, McKeown, & Elhadad, 1999). Despite fact HTMM captures
dependencies adjacent paragraphs, sufficiently constrained. Manual examination actual topic assignments reveals HTMM often assigns topic
disconnected paragraphs within document, violating topic contiguity constraint.
one domain full GMM-based approach yields best performance compared variants. one exception Phone domain. Constrained
11. http://code.google.com/p/openhtmm/
12. particular clustering technique substantially outperforms agglomerative graph partitioningbased clustering approaches task.
13. http://glaros.dtc.umn.edu/gkhome/views/cluto/

151

fiK = 20

K = 10

K = 20

K = 10

Chen, Branavan, Barzilay, & Karger

Clustering
HTMM
Constrained
Uniform
model
Clustering
HTMM
Constrained
Uniform
model

CitiesEn
Clean headings
Recall
Prec
F-score
0.578
0.439 0.499
0.446
0.232 0.305
0.579
0.471 0.520
0.520
0.440 0.477
0.639 0.509
0.566
0.486
0.541 0.512
0.260
0.217 0.237
0.458
0.519 0.486
0.499
0.551 0.524
0.578 0.636
0.606

CitiesEn
Noisy headings
Recall
Prec
F-score
0.611
0.331 0.429
0.480
0.183 0.265
0.667
0.382 0.485
0.599
0.343 0.436
0.705 0.399
0.510
0.527
0.414 0.464
0.304
0.187 0.232
0.553
0.415 0.474
0.571
0.423 0.486
0.648 0.489
0.557

CitiesEn500
Noisy headings
Recall
Prec
F-score
0.609
0.329 0.427
0.461
0.269 0.340
0.643
0.385 0.481
0.582
0.344 0.432
0.722 0.426
0.536
0.489
0.391 0.435
0.351
0.234 0.280
0.515
0.394 0.446
0.557
0.422 0.480
0.620 0.473
0.537

Clustering
HTMM
Constrained
Uniform
model
Clustering
HTMM
Constrained
Uniform
model

CitiesFr
Noisy headings
Recall
Prec
F-score
0.588
0.283 0.382
0.338
0.190 0.244
0.652
0.356
0.460
0.587
0.310 0.406
0.657 0.360
0.464
0.453
0.317 0.373
0.253
0.195 0.221
0.584
0.379 0.459
0.571
0.373 0.451
0.633 0.431
0.513

Elements
Noisy headings
Recall
Prec
F-score
0.524
0.361 0.428
0.430
0.190 0.264
0.603
0.408 0.487
0.591
0.403 0.479
0.685 0.460
0.551
0.477
0.402 0.436
0.248
0.243 0.246
0.510
0.421 0.461
0.550
0.479 0.512
0.569 0.498
0.531

Phones
Noisy headings
Recall
Prec
F-score
0.599
0.456 0.518
0.379
0.240 0.294
0.745 0.506
0.602
0.656
0.422 0.513
0.738
0.493
0.591
0.486
0.507 0.496
0.274
0.229 0.249
0.652 0.576
0.611
0.608
0.471 0.538
0.683
0.546
0.607

Table 2: Comparison alignments produced model series baselines
model variations, 10 20 topics, evaluated clean noisy sets
section headings. Higher scores better. Within K, methods
model significantly outperforms indicated p < 0.001
p < 0.01.

152

fiContent Modeling Using Latent Permutations

baseline achieves best result K small margin. results
expected, given fact domain exhibits highly rigid topic structure across
documents. model permits permutations topic ordering, GMM,
flexible highly formulaic domains.
Finally, observe evaluations based manual noisy annotations exhibit
almost entirely consistent ranking methods consideration (see clean
noisy headings results CitiesEn Table 2). consistency indicates noisy
headings sufficient gaining insight comparative performance different
approaches.
6.3 Segmentation
Next consider task text segmentation. test whether model able
identify boundaries topically coherent text segments.
6.3.1 Segmentation Evaluation Setup
Reference Segmentations described Section 6.1, datasets used
evaluation manually divided sections authors. annotations
used create reference segmentations evaluating models output. Recall
Section 6.2.1 also built clean reference structure CitiesEn set. structure encodes clean segmentation document adjusts granularity
section headings consistent across documents. Thus, also compare
segmentation specified CitiesEn clean section headings.
Metrics Segmentation quality evaluated using standard penalty metrics Pk
WindowDiff (Beeferman, Berger, & Lafferty, 1999; Pevzner & Hearst, 2002). pass
sliding window documents compute probability words end
windows improperly segmented respect other. WindowDiff
stricter, requires number segmentation boundaries endpoints
window correct well.14
Baselines first compare BayesSeg (Eisenstein & Barzilay, 2008),15 Bayesian
segmentation approach current state-of-the-art task. Interestingly,
model reduces approach every document considered completely isolation,
topic sharing documents. Connecting topics across documents makes
much difficult inference problem one tackled Eisenstein Barzilay.
time, algorithm cannot capture structural relatedness across documents.
Since BayesSeg designed operated specification number segments,
provide baseline benefit knowing correct number segments
document, provided system. run baseline using
14. Statistical significance testing standardized usually reported segmentation task,
omit tests results.
15. evaluate corpora used work, since model relies content similarity across
documents corpus.

153

fiBayesSeg
U&I
U&I
Constrained
Uniform
model
Constrained
Uniform
model

CitiesEn
Clean headings
Pk
WD
# Segs
0.321
0.376
12.3
0.337
0.404
12.3
0.353
0.375
5.8
0.260 0.281
7.7
0.268
0.300
8.8
0.253
0.283
9.0
0.274
0.314
10.9
0.234
0.294
14.0
0.221 0.278
14.2

CitiesEn
Noisy headings
Pk
WD
# Segs
0.317
0.376
13.2
0.337
0.405
13.2
0.357
0.378
5.8
0.267
0.288
7.7
0.273
0.304
8.8
0.257 0.286
9.0
0.274
0.313
10.9
0.234
0.290
14.0
0.222 0.278
14.2

CitiesEn500
Noisy headings
Pk
WD
# Segs
0.282
0.335
10.5
0.292
0.350
10.5
0.321
0.346
5.4
0.221
0.244
6.8
0.227
0.257
7.8
0.196 0.225
8.1
0.226
0.261
9.1
0.203
0.256
12.3
0.196 0.247
12.1

BayesSeg
U&I
U&I
Constrained
Uniform
model
Constrained
Uniform
model

CitiesFr
Noisy headings
Pk
WD
# Segs
0.274
0.332
10.4
0.282
0.336
10.4
0.321
0.342
4.4
0.230
0.244
6.4
0.214 0.233
7.3
0.216 0.233
7.4
0.230
0.250
7.9
0.203
0.234
10.4
0.201 0.230
10.8

Elements
Noisy headings
Pk
WD
# Segs
0.279
0.316
7.7
0.248
0.286
7.7
0.294
0.312
4.8
0.227
0.244
5.4
0.226
0.250
6.6
0.201 0.226
6.7
0.231
0.257
6.6
0.209
0.248
8.7
0.203 0.243
8.6

Phones
Noisy headings
Pk
WD
# Segs
0.392
0.457
9.6
0.412
0.463
9.6
0.423
0.435
4.7
0.312 0.347
8.0
0.332
0.367
7.5
0.309
0.349
8.0
0.295 0.348
10.8
0.327
0.381
9.4
0.302
0.357
10.4

K = 20 K = 10

K = 20 K = 10

Chen, Branavan, Barzilay, & Karger

Table 3: Comparison segmentations produced model series baselines
model variations, 10 20 topics, evaluated clean noisy
sets section headings. Lower scores better. BayesSeg U&I given
true number segments, segments counts reflect reference structures
segmentations. contrast, U&I automatically predicts number segments.

154

fiContent Modeling Using Latent Permutations

authors publicly available implementation;16 priors set using built-in mechanism
automatically re-estimates hyperparameters.
also compare method algorithm Utiyama Isahara (2001),
commonly used point reference evaluation segmentation algorithms.
algorithm computes optimal segmentation estimating changes predicted
language models segments different partitions. used publicly available
implementation system,17 require parameter tuning held-out
development set. contrast BayesSeg, algorithm mechanism predicting
number segments, also take pre-specified number segments.
comparison, consider versions algorithm U&I denotes case
correct number segments provided model U&I denotes model
estimates optimal number segments.
6.3.2 Segmentation Results
Table 3 presents segmentation experiment results. every data set model outperforms BayesSeg U&I baselines substantial margin regardless K. result
provides strong evidence learning connected topic models related documents leads
improved segmentation performance.
best performance generally obtained full version model, three
exceptions. two cases (CitiesEn K = 10 using clean headings WindowDiff
metric, CitiesFr K = 10 Pk metric), variant performs better
full model minute margin. Furthermore, instances,
corresponding evaluation K = 20 using full model leads best overall
results respective domains.
case variant outperforms full model notable margin
Phones data set. result unexpected given formulaic nature dataset
discussed earlier.
6.4 Ordering
final task evaluate model finding coherent ordering
set textual units. Unlike previous tasks, prediction based hidden variable
distributions, ordering observed document. Moreover, GMM model uses
information inference process. Therefore, need divide data sets
training test portions.
past, ordering algorithms applied textual units various granularities, commonly sentences paragraphs. ordering experiments operate
level relatively larger unit sections. believe granularity suitable
nature model, captures patterns level topic distributions
rather local discourse constraints. ordering sentences paragraphs
studied past (Karamanis et al., 2004; Barzilay & Lapata, 2008) two types
models effectively combined induce full ordering (Elsner et al., 2007).
16. http://groups.csail.mit.edu/rbg/code/bayesseg/
17. http://www2.nict.go.jp/x/x161/members/mutiyama/software.html#textseg

155

fiChen, Branavan, Barzilay, & Karger

Corpus
CitiesEn
CitiesFr
Phones

Set
Training
Testing
Training
Testing
Training
Testing

Documents
100
65
100
68
100
64

Sections
13.2
11.2
10.4
7.7
6.6
9.6

Paragraphs
66.7
50.3
40.7
28.2
24.0
39.3

Vocabulary
42,000
42,000
31,000
31,000
13,500
13,500

Tokens
4,920
3,460
2,630
1,580
2,750
4,540

Table 4: Statistics training test sets used ordering experiments. values
except vocabulary average per document. training set statistics
reproduced Table 1 ease reference.

6.4.1 Ordering Evaluation Setup
Training Test Data Sets use CitiesEn, CitiesFr Phones data sets
training documents parameter estimation described Section 5. introduce
additional sets documents domains test sets. Table 4 provides statistics
training test set splits (note out-of-vocabulary terms test sets
discarded).18
Even though perform ordering section level, collections still pose
challenging ordering task: example, average number sections CitiesEn test
document 11.2, comparable 11.5 sentences (the unit reordering) per document
National Transportation Safety Board corpus used previous work (Barzilay & Lee,
2004; Elsner et al., 2007).
Metrics report Kendalls rank correlation coefficient ordering experiments. metric measures much ordering differs reference order
underlying assumption reasonable sentence orderings fairly similar
it. Specifically, permutation sections N -section document, ()
computed
d(, )
() = 1 2 N ,
(16)
2

d(, ) is, before, Kendall distance, number swaps adjacent textual
units necessary rearrange reference order. metric ranges -1 (inverse
orders) 1 (identical orders). Note random ordering yield zero score expectation. measure widely used evaluating information ordering (Lapata,
2003; Barzilay & Lee, 2004; Elsner et al., 2007) shown correlate human
assessments text quality (Lapata, 2006).
Baselines Model Variants ordering method compared original
HMM-based content modeling approach Barzilay Lee (2004). baseline delivers
18. Elements data set limited 118 articles, preventing us splitting reasonably sized
training test sets. Therefore consider ordering experiments. Citiesrelated sets, test documents shorter cities lesser population.
hand, Phones test set include short express reviews thus exhibits higher
average document length.

156

fiContent Modeling Using Latent Permutations

HMM-based Content Model
Constrained
K = 10
model
Constrained
K = 20
model

CitiesEn
0.245
0.587
0.571
0.583
0.575

CitiesFr
0.305
0.596
0.541
0.575
0.571

Phones
0.256
0.676
0.678
0.711
0.678

Table 5: Comparison orderings produced model series baselines
model variations, 10 20 topics, evaluated respective test sets.
Higher scores better.

state-of-the art performance number datasets similar spirit model
also aims capture patterns level topic distribution (see Section 2). Again,
use publicly available implementation19 parameters adjusted according
values used previous work. content modeling implementation provides A*
search procedure use find optimal permutation.
include comparison local coherence models (Barzilay & Lapata, 2008;
Elsner et al., 2007). models designed sentence-level analysis particular,
use syntactic information thus cannot directly applied section-level ordering.
state above, models orthogonal topic-based analysis; combining two
approaches promising direction future work.
Note Uniform model variant applicable task, since
make claims preferred underlying topic ordering. fact, document likelihood perspective, proposed paragraph order reverse order would
probability Uniform model. Thus, model variant consider
Constrained.
6.4.2 Ordering Results
Table 5 summarizes ordering results GMM- HMM-based content models. Across
data sets, model outperforms content modeling large margin. instance,
CitiesEn dataset, gap two models reaches 35%. difference
expected. previous work, content models applied short formulaic texts.
contrast, documents collection exhibit higher variability original collections.
HMM provide explicit constraints generated global orderings. may
prevent effectively learning non-local patterns topic organization.
also observe Constrained variant outperforms full model.
difference two small, fairly consistent across domains. Since
possible predict idiosyncratic variations test documents topic orderings,
constrained model better capture prevalent ordering patterns consistent
across domain.
19. http://people.csail.mit.edu/regina/code.html

157

fiChen, Branavan, Barzilay, & Karger

6.5 Discussion
experiments three separate tasks reveal common trends results.
First, observe single unified model document structure readily
successfully applied multiple discourse-level tasks, whereas previous work proposed
separate approaches task. versatility speaks power topic-driven
representation document structure. Second, within task model outperforms
state-of-the-art baselines substantial margins across wide variety evaluation scenarios. results strongly support hypothesis augmenting topic models
discourse-level constraints broadens applicability discourse-level analysis tasks.
Looking performance model across different tasks, make notes
importance individual topic constraints. Topic contiguity consistently
important constraint, allowing model variants outperform alternative baseline
approaches. cases, introducing bias toward similar topic ordering, without requiring identical orderings, provides benefits encoded model.
flexible models achieve superior performance segmentation alignment tasks.
case ordering, however, extra flexibility pay off, model distributes
probability mass away strong ordering patterns likely occur unseen data.
also identify properties dataset strongly affect performance
model. Constrained model variant performs slightly better full model
rigidly formulaic domains, achieving highest performance Phones data set.
know priori domain formulaic structure, worthwhile choose
model variant suitably enforces formulaic topic orderings. Fortunately, adaptation
achieved proposed framework using prior Generalized Mallows
Model recall Constrained variant special case full model.
However, performance model invariant respect data set characteristics. Across two languages considered, model baselines exhibit
comparative performance task. Moreover, consistency also holds
general-interest cities articles highly technical chemical elements articles. Finally, smaller CitiesEn larger CitiesEn500 data sets, observe
results consistent.

7. Conclusions Future Work
paper, shown unsupervised topic-based approach capture content
structure. resulting model constrains topic assignments way requires global
modeling entire topic sequences. showed Generalized Mallows Model
theoretically empirically appealing way capturing ordering component
topic sequence. results demonstrate importance augmenting statistical models
text analysis structural constraints motivated discourse theory. Furthermore,
success GMM suggests could applied modeling ordering
constraints NLP applications.
multiple avenues future extensions work. First, empirical results
demonstrated certain domains providing much flexibility model may
fact detrimental predictive accuracy. cases, tightly constrained
variant model yields superior performance. interesting extension current
158

fiContent Modeling Using Latent Permutations

model would allow additional flexibility prior GMM drawing
another level hyperpriors. technical perspective, form hyperparameter
re-estimation would involve defining appropriate hyperprior Generalized Mallows
Model adapting estimation present inference procedure.
Additionally, may cases assumption one canonical topic ordering
entire corpus limiting, e.g., data set consists topically related articles
multiple sources, editorial standards. model extended
allow multiple canonical orderings positing additional level hierarchy
probabilistic model, i.e., document structures generated mixture several
Generalized Mallows Models, distributional mode. case,
model would take additional burden learning topics permuted
multiple canonical orderings. change model would greatly complicate
inference re-estimating Generalized Mallows Model canonical ordering general NPhard. However, recent advances statistics produced efficient approximate algorithms
theoretically guaranteed correctness bounds (Ailon, Charikar, & Newman, 2008)
exact methods tractable typical cases (Meila et al., 2007).
generally, model presented paper assumes two specific global constraints
content structure. domains satisfy constraints plentiful,
domains modeling assumptions hold. example, dialogue well
known topics recur throughout conversation (Grosz & Sidner, 1986), thereby violating
first constraint. Nevertheless, texts domains still follow certain organizational
conventions, e.g. stack structure dialogue. results suggest explicitly incorporating domain-specific global structural constraints content model would likely
improve accuracy structure induction.
Another direction future work combine global topic structure model
local coherence constraints. previously noted, model agnostic toward
relationships sentences within single topic. contrast, models local coherence
take advantage wealth additional knowledge, syntax, make decisions
information flow across adjoining sentences. linguistically rich model would provide
powerful representation levels textual structure, could used even
greater variety applications considered here.

Bibliographic Note
Portions work previously presented conference publication (Chen, Branavan,
Barzilay, & Karger, 2009). article significantly extends previous work, notably
introducing new algorithm applying models output information ordering
task (Section 5) considering new data sets experiments vary genre,
language, size (Section 6).

Acknowledgments
authors acknowledge funding support NSF CAREER grant IIS-0448168
grant IIS-0712793, NSF Graduate Fellowship, Office Naval Research, Quanta,
Nokia, Microsoft Faculty Fellowship. thank many people offered

159

fiChen, Branavan, Barzilay, & Karger

suggestions comments work, including Michael Collins, Aria Haghighi, Yoong
Keok Lee, Marina Meila, Tahira Naseem, Christy Sauper, David Sontag, Benjamin Snyder,
Luke Zettlemoyer. especially grateful Marina Meila introducing us
Mallows model. paper also greatly benefited thoughtful feedback
anonymous reviewers. opinions, findings, conclusions, recommendations expressed
paper authors, necessarily reflect views funding
organizations.

160

fiContent Modeling Using Latent Permutations

References
Ailon, N., Charikar, M., & Newman, A. (2008). Aggregating inconsistent information:
Ranking clustering. Journal ACM, 55 (5).
Althaus, E., Karamanis, N., & Koller, A. (2004). Computing locally coherent discourses.
Proceedings ACL.
Bartlett, F. C. (1932). Remembering: study experimental social psychology. Cambridge University Press.
Barzilay, R., & Elhadad, N. (2003). Sentence alignment monolingual comparable corpora.
Proceedings EMNLP.
Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies sentence ordering
multidocument news summarization. Journal Artificial Intelligence Research,
17, 3555.
Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach.
Computational Linguistics, 34 (1), 134.
Barzilay, R., & Lee, L. (2004). Catching drift: Probabilistic content models, applications generation summarization. Proceedings NAACL/HLT.
Barzilay, R., McKeown, K., & Elhadad, M. (1999). Information fusion context
multi-document summarization. Proceedings ACL.
Beeferman, D., Berger, A., & Lafferty, J. D. (1999). Statistical models text segmentation.
Machine Learning, 34, 177210.
Bernardo, J. M., & Smith, A. F. (2000). Bayesian Theory. Wiley Series Probability
Statistics.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.
Blei, D. M., & Moreno, P. J. (2001). Topic segmentation aspect hidden markov
model. Proceedings SIGIR.
Blei, D. M., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation. Journal Machine
Learning Research, 3, 9931022.
Bollegala, D., Okazaki, N., & Ishizuka, M. (2006). bottom-up approach sentence
ordering multi-document summarization. Proceedings ACL/COLING.
Chen, H., Branavan, S., Barzilay, R., & Karger, D. R. (2009). Global models document
structure using latent permutations. Proceedings NAACL/HLT.
Chinchor, N. (1995). Statistical significance MUC-6 results. Proceedings 6th
Conference Message Understanding.
Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning order things. Journal
Artificial Intelligence Research, 10, 243270.
Eisenstein, J., & Barzilay, R. (2008). Bayesian unsupervised topic segmentation. Proceedings EMNLP.
Elsner, M., Austerweil, J., & Charniak, E. (2007). unified local global model
discourse coherence. Proceedings NAACL/HLT.
161

fiChen, Branavan, Barzilay, & Karger

Fligner, M., & Verducci, J. (1986). Distance based ranking models. Journal Royal
Statistical Society, Series B, 48 (3), 359369.
Fligner, M. A., & Verducci, J. S. (1990). Posterior probabilities consensus ordering.
Psychometrika, 55 (1), 5363.
Galley, M., McKeown, K. R., Fosler-Lussier, E., & Jing, H. (2003). Discourse segmentation
multi-party conversation. Proceedings ACL.
Geman, S., & Geman, D. (1984). Stochastic relaxation, gibbs distributions bayesian
restoration images. IEEE Transactions Pattern Analysis Machine Intelligence, 12, 609628.
Graesser, A., Gernsbacher, M., & Goldman, S. (Eds.). (2003). Handbook Discourse
Processes. Erlbaum.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings National
Academy Sciences, 101, 52285235.
Griffiths, T. L., Steyvers, M., Blei, D. M., & Tenenbaum, J. B. (2005). Integrating topics
syntax. Advances NIPS.
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions, structure discourse.
Computational Linguistics, 12 (3), 175204.
Gruber, A., Rosen-Zvi, M., & Weiss, Y. (2007). Hidden topic markov models. Proceedings
AISTATS.
Halliday, M. A. K., & Hasan, R. (1976). Cohesion English. Longman.
Hearst, M. (1994). Multi-paragraph segmentation expository text. Proceedings
ACL.
Ji, P. D., & Pulman, S. (2006). Sentence ordering manifold-based classification
multi-document summarization. Proceedings EMNLP.
Karamanis, N., Poesio, M., Mellish, C., & Oberlander, J. (2004). Evaluating centeringbased metrics coherence text structuring using reliably annotated corpus.
Proceedings ACL.
Klementiev, A., Roth, D., & Small, K. (2008). Unsupervised rank aggregation distancebased models. Proceedings ICML, pp. 472479.
Lapata, M. (2003). Probabilistic text structuring: Experiments sentence ordering.
Proceedings ACL.
Lapata, M. (2006). Automatic evaluation information ordering: Kendalls tau. Computational Linguistics, 32 (4), 471484.
Lebanon, G., & Lafferty, J. (2002). Cranking: combining rankings using conditional probability models permutations. Proceedings ICML.
Malioutov, I., & Barzilay, R. (2006). Minimum cut model spoken lecture segmentation.
Proceedings ACL.
Mallows, C. L. (1957). Non-null ranking models. Biometrika, 44, 114130.

162

fiContent Modeling Using Latent Permutations

Meila, M., Phadnis, K., Patterson, A., & Bilmes, J. (2007). Consensus ranking
exponential model. Proceedings UAI.
Neal, R. M. (2003). Slice sampling. Annals Statistics, 31, 705767.
Nelken, R., & Shieber, S. M. (2006). Towards robust context-sensitive sentence alignment
monolingual corpora. Proceedings EACL.
Noreen, E. W. (1989). Computer Intensive Methods Testing Hypotheses. Introduction. Wiley.
Pevzner, L., & Hearst, M. A. (2002). critique improvement evaluation metric
text segmentation. Computational Linguistics, 28, 1936.
Porteous, I., Newman, D., Ihler, A., Asuncion, A., Smyth, P., & Welling, M. (2008). Fast
collapsed gibbs sampling latent dirichlet allocation. Proceedings SIGKDD.
Purver, M., Kording, K., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic
modelling multi-party spoken discourse. Proceedings ACL/COLING.
Radev, D., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: Sentence extraction, utility-based evaluation user studies.
Proceedings ANLP/NAACL Summarization Workshop.
Riezler, S., & Maxwell, J. T. (2005). pitfalls automatic evaluation significance testing MT. Proceedings ACL Workshop Intrinsic Extrinsic
Evaluation Measures Machine Translation and/or Summarization.
Schiffrin, D., Tannen, D., & Hamilton, H. E. (Eds.). (2001). Handbook Discourse
Analysis. Blackwell.
Titov, I., & McDonald, R. (2008). Modeling online reviews multi-grain topic models.
Proceedings WWW.
Utiyama, M., & Isahara, H. (2001). statistical model domain-independent text segmentation. Proceedings ACL.
van Mulbregt, P., Carp, I., Gillick, L., Lowe, S., & Yamron, J. (1998). Text segmentation
topic tracking broadcast news via hidden markov model approach. Proceedings
ICSLP.
Wallach, H. M. (2006). Topic modeling: beyond bag words. Proceedings ICML.
Wray, A. (2002). Formulaic Language Lexicon. Cambridge University Press, Cambridge.

163

fiJournal Artificial Intelligence Research 36 (2009) 267-306

Submitted 06/09; published 10/09

ParamILS: Automatic Algorithm Configuration Framework
Frank Hutter
Holger H. Hoos
Kevin Leyton-Brown

HUTTER @ CS . UBC . CA
HOOS @ CS . UBC . CA
KEVINLB @ CS . UBC . CA

University British Columbia, 2366 Main Mall
Vancouver, BC, V6T1Z4, Canada

Thomas Stutzle

STUETZLE @ ULB . AC .

Universite Libre de Bruxelles, CoDE, IRIDIA
Av. F. Roosevelt 50 B-1050 Brussels, Belgium

Abstract
identification performance-optimizing parameter settings important part development application algorithms. describe automatic framework algorithm
configuration problem. formally, provide methods optimizing target algorithms
performance given class problem instances varying set ordinal and/or categorical parameters. review family local-search-based algorithm configuration procedures
present novel techniques accelerating adaptively limiting time spent evaluating individual configurations. describe results comprehensive experimental evaluation
methods, based configuration prominent complete incomplete algorithms
SAT. also present is, knowledge, first published work automatically configuring C PLEX mixed integer programming solver. algorithms considered default
parameter settings manually identified considerable effort. Nevertheless, using
automated algorithm configuration procedures, achieved substantial consistent performance
improvements.

1. Introduction
Many high-performance algorithms parameters whose settings control important aspects
behaviour. particularly case heuristic procedures used solving computationally hard problems.1 example, consider C PLEX, commercial solver mixed integer
programming problems.2 CPLEX version 10 80 parameters affect solvers search
mechanism configured user improve performance. many acknowledgements literature finding performance-optimizing parameter configurations heuristic algorithms often requires considerable effort (see, e.g., Gratch & Chien, 1996; Johnson, 2002;
Diao, Eskesen, Froehlich, Hellerstein, Spainhower & Surendra, 2003; Birattari, 2004; Adenso-Diaz
& Laguna, 2006). many cases, tedious task performed manually ad-hoc way. Automating task high practical relevance several contexts.
Development complex algorithms Setting parameters heuristic algorithm
highly labour-intensive task, indeed consume large fraction overall development
1. use term heuristic algorithm includes methods without provable performance guarantees well
methods guarantees, nevertheless make use heuristic mechanisms. latter case, use
heuristic mechanisms often results empirical performance far better bounds guaranteed rigorous
theoretical analysis.
2. http://www.ilog.com/products/cplex/
c
2009
AI Access Foundation. rights reserved.

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

time. use automated algorithm configuration methods lead significant time
savings potentially achieve better results manual, ad-hoc methods.
Empirical studies, evaluations, comparisons algorithms central question comparing heuristic algorithms whether one algorithm outperforms another fundamentally superior, developers successfully optimized parameters (Johnson, 2002). Automatic algorithm configuration methods mitigate problem unfair
comparisons thus facilitate meaningful comparative studies.
Practical use algorithms ability complex heuristic algorithms solve large
hard problem instances often depends critically use suitable parameter settings.
End users often little knowledge impact algorithms parameter
settings performance, thus simply use default settings. Even carefully
optimized standard benchmark set, default configuration may perform well
particular problem instances encountered user. Automatic algorithm configuration
methods used improve performance principled convenient way.
wide variety strategies automatic algorithm configuration explored literature. Briefly, include exhaustive enumeration, hill-climbing (Gratch & Dejong, 1992), beam
search (Minton, 1993), genetic algorithms (Terashima-Marn, Ross & Valenzuela-Rendon, 1999),
experimental design approaches (Coy, Golden, Runger & Wasil, 2001), sequential parameter optimization (Bartz-Beielstein, 2006), racing algorithms (Birattari, Stutzle, Paquete & Varrentrapp,
2002; Birattari, 2004; Balaprakash, Birattari & Stutzle, 2007), combinations fractional experimental design local search (Adenso-Diaz & Laguna, 2006). discuss
related work extensively Section 9. Here, note authors refer
optimization algorithms performance setting (typically numerical) parameters
parameter tuning, favour term algorithm configuration (or simply, configuration).
motivated fact interested methods deal potentially large number
parameters, numerical, ordinal (e.g., low, medium, high) categorical (e.g., choice heuristic). Categorical parameters used select combine discrete
building blocks algorithm (e.g., preprocessing variable ordering heuristics); consequently,
general view algorithm configuration includes automated construction heuristic algorithm building blocks. best knowledge, methods discussed article
yet general ones available configuration algorithms many categorical
parameters.
give overview follows highlight main contributions. formally stating algorithm configuration problem Section 2, Section 3 describe ParamILS
(first introduced Hutter, Hoos & Stutzle, 2007), versatile stochastic local search approach
automated algorithm configuration, two instantiations, BasicILS FocusedILS.
introduce adaptive capping algorithm runs, novel technique used
enhance search-based algorithm configuration procedures independently underlying search
strategy (Section 4). Adaptive capping based idea avoiding unnecessary runs
algorithm configured developing bounds performance measure optimized.
present trajectory-preserving variant heuristic extension technique. discussing experimental preliminaries Section 5, Section 6 present empirical evidence showing adaptive capping speeds BasicILS FocusedILS. also show BasicILS
268

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

outperforms random search simple local search, well evidence FocusedILS
outperforms BasicILS.
present extensive evidence ParamILS find substantially improved parameter configurations complex highly optimized algorithms. particular, apply automatic
algorithm configuration procedures aforementioned commercial optimization tool C PLEX,
one powerful, widely used complex optimization algorithms aware of.
stated C PLEX user manual (version 10.0, page 247), great deal algorithmic development effort devoted establishing default ILOG C PLEX parameter settings achieve
good performance wide variety MIP models. demonstrate consistent improvements
default parameter configuration wide range practically relevant instance distributions. cases, able achieve average speedup order magnitude
previously-unseen test instances (Section 7). believe first results
published automatically configuring C PLEX piece software comparable complexity.
Section 8 review wide range (separately-published) ParamILS applications. Specifically, survey work considered optimization complete incomplete heuristic
search algorithms problems propositional satisfiability (SAT), probable explanation
(MPE), protein folding, university time-tabling, algorithm configuration itself. three
cases, ParamILS integral part algorithm design process allowed exploration
large design spaces. could done effectively manual way
existing automated method. Thus, automated algorithm configuration general ParamILS
particular enables new way (semi-)automatic design algorithms components.
Section 9 presents related work and, finally, Section 10 offers discussion conclusions.
distill common patterns helped ParamILS succeed various applications. also
give advice practitioners would like apply automated algorithm configuration general
ParamILS particular, identify promising avenues research future work.

2. Problem Statement Notation
algorithm configuration problem consider work informally stated follows:
given algorithm, set parameters algorithm set input data, find parameter
values algorithm achieves best possible performance input data.
avoid potential confusion algorithms whose performance optimized algorithms used carrying optimization task, refer former target algorithms
latter configuration procedures (or simply configurators). setup illustrated
Figure 1. Different algorithm configuration problems also considered literature, including setting parameters per-instance basis adapting parameters algorithm
running. defer discussion approaches Section 9.
following, define algorithm configuration problem formally introduce
notation use throughout article. Let denote algorithm, let p1 , . . . , pk
parameters A. Denote domain possible values parameter pi . Throughout
work, assume parameter domains finite sets. assumption met
discretizing numerical parameters finite number values. Furthermore, parameters
269

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Figure 1: configuration scenario includes algorithm configured collection problem instances. configuration procedure executes target algorithm specified parameter settings
instances, receives information performance runs, uses
information decide subsequent parameter configurations evaluate.

may ordered, exploit ordering relations. Thus, effectively assume
parameters finite categorical.3
problem formulation allows us express conditional parameter dependencies (for example,
one algorithm parameter might used select among search heuristics, heuristics
behaviour controlled parameters). case, values parameters
irrelevant heuristic selected. ParamILS exploits effectively searches
space equivalence classes parameter configuration space. addition, formulation supports
constraints feasible combinations parameter values. use 1 . . . k denote
space feasible parameter configurations, A() denoting instantiation algorithm
parameter configuration .
Let denote probability distribution space problem instances, denote element . may given implicitly, random instance generator distribution
generators. also possible (and indeed common) consist finite sample
instances; case, define uniform distribution .
many ways measuring algorithms performance. example, might interested minimizing computational resources consumed given algorithm (such runtime,
memory communication bandwidth), maximizing quality solution found. Since
high-performance algorithms computationally-challenging problems often randomized,
behaviour vary significantly multiple runs. Thus, algorithm always achieve
performance, even run repeatedly fixed parameters single problem instance. overall goal must therefore choose parameter settings minimize cost
statistic algorithms performance across input data. denote statistic c().
example, might aim minimize mean runtime median solution cost.
intuition mind, define algorithm configuration problem formally.
Definition 1 (Algorithm Configuration Problem). instance algorithm configuration problem 6-tuple hA, , D, max , o, mi, where:
parameterized algorithm;
parameter configuration space A;
3. currently extending algorithm configuration procedures natively support parameter types.

270

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

distribution problem instances domain ;
max cutoff time (or captime), run terminated still running;
function measures observed cost running A() instance
captime R (examples runtime solving instance, cost solution found)
statistical population parameter (such expectation, median, variance).
parameter configuration candidate solution algorithm configuration
problem. configuration , denotes distribution costs induced function o,
applied instances drawn distribution multiple independent runs randomized
algorithms, using captime = max . cost candidate solution defined
c() := m(O ),

(1)

statistical population parameter cost distribution . optimal solution, , minimizes c():
arg min c().
(2)


algorithm configuration procedure procedure solving algorithm configuration
problem. Unfortunately, least algorithm configuration problems considered article, cannot optimize c closed form since access algebraic representation function. denote sequence runs executed configurator R =
((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , )). ith run described five values:
denotes parameter configuration evaluated;
denotes instance algorithm run;
si denotes random number seed used run (we keep track seeds able block
them, see Section 5.1.2);
denotes runs captime;
oi denotes observed cost run
Note , , s, , vary one element R next, regardless whether
elements held constant. denote ith run R R[i], subsequence
runs using parameter configuration (i.e., runs = ) R . configuration
procedures considered article compute empirical estimates c() based solely R ,
principle methods could used. compute cost estimates online,
runtime configurator, well offline, evaluation purposes.
Definition 2 (Cost Estimate). Given algorithm configuration problem hA, , D, max , o, mi,
define cost estimate cost c() based sequence runs R = ((1 , 1 , s1 , 1 , o1 ), . . . ,
(n , n , sn , n , )) c(, R) := m({oi | = }), sample statistic analogue
statistical population parameter m.
example, c() expected runtime distribution instances random number
seeds, c(, R) sample mean runtime runs R .
configuration procedures paper anytime algorithms, meaning times
keep track configuration currently believed lowest cost; refer configuration incumbent configuration, short incumbent, inc . evaluate configurators
performance time means incumbents training test performance, defined follows.
271

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Definition 3 (Training performance). time configurator performed sequence runs R = ((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , )) solve algorithm configuration problem hA, , D, max , o, mi, thereby found incumbent configuration inc ,
training performance time defined cost estimate c(inc , R).
set instances {1 , . . . , n } discussed called training set. true cost
parameter configuration cannot computed exactly, estimated using training performance. However, training performance configurator biased estimator incumbents
true cost, instances used selecting incumbent evaluating it.
order achieve unbiased estimates offline evaluation, set aside fixed set instances
{10 , . . . , T0 } (called test set) random number seeds {s01 , . . . , s0T }, unknown
configurator, use evaluation.
Definition 4 (Test performance). time t, let configurators incumbent algorithm
configuration problem hA, , D, max , o, mi inc (this found means executing sequence runs training set). Furthermore, let R0 = ((inc , 10 , s01 , max , o1 ), . . . , (inc , T0 ,
s0T , max , oT )) sequence runs instances random number seeds test set
(which performed offline evaluation purposes), configurators test performance
time defined cost estimate c(inc , R0 ).
Throughout article, aim minimize expected runtime. (See Section 5.1.1 discussion
choice.) Thus, configurators training performance mean runtime runs
performed incumbent. test performance mean runtime incumbent
test set. Note that, configurator free use max , test performance always
computed using maximal captime, max .
obvious automatic algorithm configurator choose runs order best
minimize c() within given time budget. particular, make following choices:
1. parameter configurations 0 evaluated?
2. problem instances 0 used evaluating 0 0 ,
many runs performed instance?
3. cutoff time used run?
Hutter, Hoos Leyton-Brown (2009) considered design space detail, focusing
tradeoff (fixed) number problem instances used evaluation
parameter configuration (fixed) cutoff time used run, well interaction
choices number configurations considered. contrast, here, study
adaptive approaches selecting number problem instances (Section 3.3) cutoff
time evaluation parameter configuration (Section 4); also study configurations
selected (Sections 3.1 6.2).

3. ParamILS: Iterated Local Search Parameter Configuration Space
section, address first important previously mentioned dimensions
automated algorithm configuration, search strategy, describing iterated local search
framework called ParamILS. start with, fix two dimensions, using unvarying
benchmark set instances fixed cutoff times evaluation parameter configuration. Thus, stochastic optimization problem algorithm configuration reduces simple
272

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

optimization problem, namely find parameter configuration yields lowest mean runtime given benchmark set. Then, Section 3.3, address second question many
runs performed configuration.
3.1 ParamILS framework
Consider following manual parameter optimization process:
1. begin initial parameter configuration;
2. experiment modifications single parameter values, accepting new configurations whenever result improved performance;
3. repeat step 2 single-parameter change yields improvement.
widely used procedure corresponds manually-executed local search parameter configuration space. Specifically, corresponds iterative first improvement procedure search
space consisting possible configurations, objective function quantifies performance
achieved target algorithm given configuration, neighbourhood relation based
modification one single parameter value time (i.e., one-exchange neighbourhood).
Viewing manual procedure local search algorithm advantageous suggests
automation procedure well improvement drawing ideas stochastic
local search community. example, note procedure stops soon reaches local optimum (a parameter configuration cannot improved modifying single parameter value).
sophisticated approach employ iterated local search (ILS; Lourenco, Martin & Stutzle,
2002) search performance-optimizing parameter configurations. ILS prominent stochastic
local search method builds chain local optima iterating main loop consisting
(1) solution perturbation escape local optima, (2) subsidiary local search procedure
(3) acceptance criterion decide whether keep reject newly obtained candidate solution.
ParamILS (given pseudocode Algorithm 1) ILS method searches parameter configuration space. uses combination default random settings initialization, employs
iterative first improvement subsidiary local search procedure, uses fixed number (s) random moves perturbation, always accepts better equally-good parameter configurations,
re-initializes search random probability prestart .4 Furthermore, based
one-exchange neighbourhood, is, always consider changing one parameter time.
ParamILS deals conditional parameters excluding configurations neighbourhood configuration differ conditional parameter relevant .
3.2 BasicILS Algorithm
order turn ParamILS specified Algorithm Framework 1 executable configuration
procedure, necessary instantiate function better determines two parameter settings preferred. ultimately propose several different ways this.
Here, describe simplest approach, call BasicILS. Specifically, use term
BasicILS(N ) refer ParamILS algorithm function better(1 , 2 ) implemented
shown Procedure 2: simply comparing estimates cN cost statistics c(1 ) c(2 )
based N runs each.
4. original parameter choices hr, s, prestart = h10, 3, 0.01i (from Hutter et al., 2007) somewhat arbitrary,
though expected performance quite robust respect settings. revisit issue Section 8.4.

273

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Algorithm Framework 1: ParamILS(0 , r, prestart , s)
Outline iterated local search parameter configuration space; specific variants ParamILS
study, BasicILS(N) FocusedILS, derived framework instantiating procedure
better (which compares , 0 ). BasicILS(N) uses betterN (see Procedure 2), FocusedILS
uses betterF oc (see Procedure 3). neighbourhood Nbh() configuration set
configurations differ one parameter, excluding configurations differing conditional
parameter relevant .
Input : Initial configuration 0 , algorithm parameters r, prestart , s.
Output : Best parameter configuration found.
1 = 1, . . . , r
2
random ;
3
better(, 0 ) 0 ;
4
5
6

ils IterativeFirstImprovement (0 );
TerminationCriterion()
ils ;

7

// ===== Perturbation
= 1, . . . , random 0 Nbh();

8

// ===== Basic local search
IterativeFirstImprovement ();

9
10

// ===== AcceptanceCriterion
better(, ils ) ils ;
probability prestart ils random ;

11

return overall best inc found;

12
13
14
15
16

Procedure IterativeFirstImprovement ()
repeat
0 ;
foreach 00 N bh( 0 ) randomized order
better( 00 , 0 ) 00 ; break;

17
18

0 = ;
return ;

BasicILS(N ) simple intuitive approach since evaluates every parameter configuration
running N training benchmark instances using random number seeds.
Like many related approaches (see, e.g., Minton, 1996; Coy et al., 2001; Adenso-Diaz &
Laguna, 2006), deals stochastic part optimisation problem using estimate
based fixed training set N instances. benchmark instances heterogeneous
Procedure 2: betterN (1 , 2 )
Procedure used BasicILS(N ) RandomSearch(N ) compare two parameter configurations. Procedure objective(, N ) returns user-defined objective achieved A() first N instances
keeps track incumbent solution, inc ; detailed Procedure 4 page 279.
Input
: Parameter configuration 1 , parameter configuration 2
Output
: True 1 better equal 2 first N instances; false otherwise
Side Effect : Adds runs global caches performed algorithm runs R1 R2 ; potentially
updates incumbent inc
1 cN (2 ) objective(2 , N )
2 cN (1 ) objective(1 , N )
3 return cN (1 ) cN (2 )

274

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

user identify rather small representative subset instances, approach find
good parameter configurations low computational effort.
3.3 FocusedILS: Adaptively Selecting Number Training Instances
question choose number training instances, N , BasicILS(N ) straightforward answer: optimizing performance using small training set leads good training
performance, poor generalization previously unseen test benchmarks. hand,
clearly cannot evaluate every parameter configuration enormous training setif did,
search progress would unreasonably slow.
FocusedILS variant ParamILS deals problem adaptively varying
number training samples considered one parameter configuration another. denote
number runs available estimate cost statistic c() parameter configuration
N (). performed different numbers runs using different parameter configurations,
face question comparing two parameter configurations 0 N () N ( 0 ).
One option would simply compute empirical cost statistic based available number
runs configuration. However, lead systematic biases if, example, first
instances easier average instance. Instead, compare 0 based N () runs
instances seeds. amounts blocking strategy, straight-forward
adaptation known variance reduction technique; see 5.1 detailed discussion.
approach comparison leads us concept domination. say dominates 0
least many runs conducted 0 , performance A()
first N ( 0 ) runs least good A( 0 ) runs.
Definition 5 (Domination). 1 dominates 2 N (1 ) N (2 ) cN (2 ) (1 )
cN (2 ) (2 ).
ready discuss comparison strategy encoded procedure betterF oc (1 , 2 ),
used FocusedILS algorithm (see Procedure 3). procedure first acquires one
additional sample configuration smaller N (i ), one run configurations
number runs. Then, continues performing runs way one configuration dominates other. point returns true 1 dominates 2 , false otherwise.
also keep track total number configurations evaluated since last improving step (i.e.,
since last time betterF oc returned true); denote number B. Whenever betterF oc (1 , 2 )
returns true, perform B bonus runs 1 reset B 0. mechanism ensures
perform many runs good configurations, error made every comparison two
configurations 1 2 decreases expectation.
difficult show limit, FocusedILS sample every parameter configuration
unbounded number times. proof relies fact that, instantiation ParamILS,
FocusedILS performs random restarts positive probability.
Lemma 6 (Unbounded number evaluations). Let N (J, ) denote number runs FocusedILS
performed parameter configuration end ILS iteration J estimate c(). Then,
constant K configuration (with finite ||), limJ P [N (J, ) K] = 1.
Proof. ILS iteration ParamILS, probability prestart > 0 new configuration
picked uniformly random, probability 1/||, configuration . probability
275

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Procedure 3: betterF oc (1 , 2 )
Procedure used FocusedILS compare two parameter configurations. Procedure objective(, N )
returns user-defined objective achieved A() first N instances, keeps track incumbent solution, updates R (a global cache algorithm runs performed parameter configuration ); detailed Procedure 4 page 279. , N () = length(R ). B global
counter denoting number configurations evaluated since last improvement step.
Input
: Parameter configuration 1 , parameter configuration 2
Output
: True 1 dominates 2 , false otherwise
Side Effect: Adds runs global caches performed algorithm runs R1 R2 ; updates
global counter B bonus runs, potentially incumbent inc
1 B B+1
2 N (1 ) N (2 )
3
min 1 ; max 2
4
N (1 ) = N (2 ) B B + 1
else min 2 ; max 1
repeat
N (min ) + 1
ci (max ) objective(max , i) // N (min ) = N (max ), adds new run Rmax .
ci (min ) objective(min , i) // Adds new run Rmin .
10 dominates(1 , 2 ) dominates(2 , 1 )
11 dominates(1 , 2 )
5
6
7
8
9

// ===== Perform B bonus runs.
12
13
14

cN (1 )+B (1 ) objective(1 , N (1 ) + B) // Adds B new runs R1 .
B 0
return true

15

else return false

16
17
18

Procedure dominates(1 , 2 )
N (1 ) < N (2 ) return false
return objective(1 , N (2 )) objective(2 , N (2 ))

visiting ILS iteration thus p prestart
> 0. Hence, number runs performed
||
lower-bounded binomial random
variable B(k; J, p). Then, constant k < K obtain
limJ B(k; J, p) = limJ Jk pk (1 p)Jk = 0. Thus, limJ P [N (J, ) K] = 1.
Definition 7 (Consistent estimator). cN () consistent estimator c() iff
> 0 : lim P (|cN () c()| < ) = 1.
N

cN () consistent estimator c(), cost estimates become reliable
N approaches infinity, eventually eliminating overconfidence possibility mistakes
comparing two parameter configurations. fact captured following lemma.
Lemma 8 (No mistakes N ). Let 1 , 2 two parameter configurations
c(1 ) < c(2 ). Then, consistent estimators cN , limN P (cN (1 ) cN (2 )) = 0.
Proof. Write c1 shorthand c(1 ), c2 c(2 ), c1 cN (1 ), c2 cN (2 ). Define
= 21 (c2 + c1 ) midpoint c1 c2 , = c2 = c1 > 0
distance two points. Since cN consistent estimator c, estimate
c1 comes arbitrarily close real cost c1 . is, limN P (|c1 c1 | < ) = 1. Since
276

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

|m c1 | = , estimate c1 cannot greater equal m: limN P (c1 m) = 0.
Similarly, limN P (c2 < m) = 0. Since
P (c1 c2 ) = P (c1 c2 c1 m) + P (c1 c2 c1 < m)
= P (c1 c2 c1 m) + P (c1 c2 c1 < c2 < m)
P (c1 m) + P (c2 < m),
limN P (c1 c2 ) limN (P (c1 m) + P (c2 < m)) = 0 + 0 = 0.
Combining two lemmata show limit, FocusedILS guaranteed
converge true best parameter configuration.
Theorem 9 (Convergence FocusedILS). FocusedILS optimizes cost statistic c based
consistent estimator cN , probability finds true optimal parameter configuration
approaches one number ILS iterations goes infinity.
Proof. According Lemma 6, N () grows unboundedly . 1 , 2 ,
N (1 ) N (2 ) go infinity, Lemma 8 states pairwise comparison, truly better
configuration preferred. Thus eventually, FocusedILS visits finitely many parameter
configurations prefers best one others probability arbitrarily close one.
note many practical scenarios cost estimators may consistentthat is,
may fail closely approximate true performance given parameter configuration even
large number runs target algorithm. example, finite training set, , used
configuration rather distribution problem instances, D, even large N , cN
accurately reflect cost parameter configurations training set, . small
training sets, , cost estimate based may differ substantially true cost defined
performance across entire distribution, D. larger training set, , smaller
expected difference (it vanishes training set size goes infinity). Thus, important use
large training sets (which representative distribution interest) whenever possible.

4. Adaptive Capping Algorithm Runs
consider last dimensions automated algorithm configuration, cutoff time
run target algorithm. introduce effective simple capping technique
adaptively determines cutoff time run. motivation capping technique comes
problem encountered configuration procedures considered article: often
search performance-optimizing parameter setting spends lot time evaluating parameter configuration much worse other, previously-seen configurations.
Consider, example, case parameter configuration 1 takes total 10 seconds
solve N = 100 instances (i.e., mean runtime 0.1 seconds per instance), another parameter configuration 2 takes 100 seconds solve first instances. order compare
mean runtimes 1 2 based set instances, knowing runtimes 1 ,
necessary run 2 100 instances. Instead, already terminate first run 2
10 + seconds. results lower bound 2 mean runtime 0.1 + /100 since
remaining 99 instances could take less zero time. lower bound exceeds mean
runtime 1 , already certain comparison favour 1 . insight
provides basis adaptive capping technique.
277

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

4.1 Adaptive Capping BasicILS
section, introduce adaptive capping BasicILS. first introduce trajectory-preserving
version adaptive capping (TP capping) provably change BasicILSs search trajectory lead large computational savings. modify strategy heuristically
perform aggressive adaptive capping (Aggr capping), potentially yielding even better performance practice.
4.1.1 RAJECTORY- PRESERVING C APPING
Observe comparisons parameter configurations ParamILS pairwise.
BasicILS(N ), comparisons based Procedure betterN (1 , 2 ), 2 either
best configuration encountered ILS iteration best configuration last ILS iteration. Without adaptive capping, comparisons take long time, since poor parameter
configuration easily take order magnitude longer good configurations.
case optimizing mean non-negative cost functions (such runtime solution
cost), implement bounded evaluation parameter configuration based N runs
given performance bound Procedure objective (see Procedure 4). procedure sequentially
performs runs parameter configuration run computes lower bound cN ()
based N runs performed far. Specifically, objective mean runtime
sum runtimes runs, divide sum N ; since runtimes must
nonnegative, quantity lower bounds cN (). lower bound exceeds bound passed
argument, skip remaining runs . order pass appropriate bounds
Procedure objective, need slightly modify Procedure betterN (see Procedure 2 page 274)
adaptive capping. Procedure objective bound additional third argument,
set line 1 betterN , cN (2 ) line 2.
approach results computation exactly function betterN used
original version BasicILS, modified procedure follows exactly search trajectory
would followed without capping, typically requires much less runtime. Hence, within
amount overall running time, new version BasicILS tends able search
larger part parameter configuration space. Although work focus objective
minimizing mean runtime decision algorithms, note adaptive capping approach
applied easily configuration objectives.
4.1.2 AGGRESSIVE C APPING
demonstrate Section 6.4, use trajectory-preserving adaptive capping result
substantial speedups BasicILS. However, sometimes approach still less efficient
could be. upper bound cumulative runtime used capping computed
best configuration encountered current ILS iteration (where new ILS iteration begins
perturbation), opposed overall incumbent. perturbation resulted
new parameter configuration , new iterations best configuration initialized .
frequent case new performs poorly, capping criterion apply quickly
comparison performed overall incumbent.
counteract effect, introduce aggressive capping strategy terminate
evaluation poorly-performing configuration time. heuristic extension
adaptive capping technique, bound evaluation parameter configuration per278

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Procedure 4: objective(, N, optional parameter bound)
Procedure computes cN (), either performing new runs exploiting previous cached runs.
optional third parameter specifies bound computation performed; parameter
specified, bound taken . , N () number runs performed ,
i.e., length global array R . computing runtimes, count unsuccessful runs 10
times cutoff time.
Input
: Parameter configuration , number runs, N , optional bound bound
Output
: cN () cN () bound, otherwise large constant (maxPossibleObjective) plus
number instances remain unsolved bound exceeded
Side Effect: Adds runs global cache performed algorithm runs, R ; updates global
incumbent, inc
// ===== Maintain invariant: N (inc ) N ()
1
2

=
6 inc N (inc ) < N
cN (inc ) objective(inc , N, ) // Adds N N (inc ) runs Rinc
// ===== aggressive capping, update bound.

3

Aggressive capping bound min(bound, bm cN (inc ))
// ===== Update run results tuple R .

= 1...N
sum runtime sum runtimes R [1], . . . , R [i 1] // Tuple indices starting 1.
0i max(max , N bound sum runtime)
N () (, , , oi ) R [i]
N () ((i 0i oi = unsuccessful) (i < 0i oi 6= unsuccessful))
o0i oi // Previous run longer yet unsuccessful shorter yet successful re-use result
9
else
10
o0i objective newly executed run A() instance seed si captime
4
5
6
7
8

11
12
13
14

R [i] (, , 0i , o0i )
1/N (sum runtime + o0i ) > bound return maxPossibleObjective + (N + 1)
N = N (inc ) (sum runtimes R ) < (sum runtimes Rinc ) inc
return 1/N (sum runtimes R )

formance incumbent parameter configuration multiplied factor call bound
multiplier, bm. comparison two parameter configurations 0 performed evaluations terminated preemptively, configuration solved
instances within allowed time taken better one. (This behaviour achieved
line 12 Procedure objective, keeps track number instances solved exceeding bound.) Ties broken favour moving new parameter configuration instead
staying current one.
Depending bound multiplier, use aggressive capping mechanism may change
search trajectory BasicILS. bm = heuristic method reduces trajectorypreserving method, aggressive setting bm = 1 means know parameter
configuration worse incumbent, stop evaluation. experiments set
bm = 2, meaning lower bound performance configuration exceeds twice
performance incumbent solution, evaluation terminated. (In Section 8.4, revisit
choice bm = 2, configuring parameters ParamILS itself.)
279

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

4.2 Adaptive Capping FocusedILS
main difference BasicILS FocusedILS latter adaptively varies number runs used evaluate parameter configuration. difference complicates,
prevent use adaptive capping. FocusedILS always compares pairs parameter configurations based number runs configuration, even though
number differ one comparison next.
Thus, extend adaptive capping FocusedILS using separate bounds every number
runs, N . Recall FocusedILS never moves one configuration, , neighbouring
configuration, 0 , without performing least many runs 0 performed .
Since keep track performance number runs N (), bound
evaluation 0 always available. Therefore, implement trajectory-preserving
aggressive capping BasicILS.
BasicILS, FocusedILS inner workings adaptive capping implemented
Procedure objective (see Procedure 4). need modify Procedure betterF oc (see Procedure
3 page 276) call objective right bounds. leads following changes
Procedure betterF oc . Subprocedure dominates line 16 takes bound additional
argument passes two calls objective line 18. two calls dominates
line 10 one call line 11 use bound cmax . three direct calls objective
lines 8, 9, 12 use bounds , cmax , , respectively.

5. Experimental Preliminaries
section give background information computational experiments presented
following sections. First, describe design experiments. Next, present
configuration scenarios (algorithm/benchmark data combinations) studied following section.
Finally, describe low-level details experimental setup.
5.1 Experimental Design
describe objective function methods used selecting instances seeds.
5.1.1 C ONFIGURATION BJECTIVE : P ENALIZED AVERAGE RUNTIME
Section 2, mentioned algorithm configuration problems arise context various
different cost statistics. Indeed, past work explored several them: maximizing solution
quality achieved given time, minimizing runtime required reach given solution quality,
minimizing runtime required solve single problem instance (Hutter et al., 2007).
work focus objective minimizing mean runtime instances
distribution D. optimization objective naturally occurs many practical applications.
also implies strong correlation c() amount time required obtain good
empirical estimate c(). correlation helps make adaptive capping scheme effective.
One might wonder whether means right way aggregate runtimes. preliminary
experiments, found minimizing mean runtime led parameter configurations overall good runtime performance, including rather competitive median runtimes, minimizing
median runtime yielded less robust parameter configurations timed large (but < 50%)
fraction benchmark instances. However, encounter runs terminate within
280

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

given cutoff time mean ill-defined. order penalize timeouts, define penalized
average runtime (PAR) set runs cutoff time max mean runtime
runs, unsuccessful runs counted p max penalization constant p 1.
study, use p = 10.
5.1.2 ELECTING NSTANCES EEDS
mentioned previously, often finite set instances available upon evaluate
algorithm. case experiments report here. Throughout study, configuration experiments performed training set containing half given benchmark instances.
remaining instances solely used test set evaluate found parameter configurations.
evaluations within ParamILS based N runs, selected N instances
random number seeds used following common blocking technique (see, e.g., Birattari
et al., 2002; Ridge & Kudenko, 2006). ensured whenever two parameter configurations
compared, cost estimates based exactly instances seeds.
serves avoid noise effects due differences instances use different seeds.
example, prevents us making mistake considering configuration better
configuration 0 tested easier instances.
dealing randomized target algorithms, also tradeoff number
problem instances used number independent runs performed instance.
extreme case, given sample size N , one could perform N runs single instance
single run N different instances. latter strategy known result minimal variance
estimator common optimization objectives minimization mean runtime (which
consider study) maximization mean solution quality (see, e.g., Birattari, 2004).
Consequently, performed multiple runs per instance wanted acquire
samples cost distribution instances training set.
Based considerations, configuration procedures study article
implemented take list hinstance, random number seedi pairs one inputs. Empirical
estimates cN () cost statistic c() optimized determined first N hinstance,
seedi pairs list. list hinstance, seedi pairs constructed follows. Given training
set consisting problem instances, N , drew sample N instances uniformly
random without replacement added list. wished evaluate algorithm
samples training instances, could happen case randomized
algorithms, repeatedly drew random samples size described before,
batch corresponded random permutation N training instances, added final sample
size N mod < , case N . sample drawn, paired
random number seed chosen uniformly random set possible seeds
added list hinstance, seedi pairs.
5.1.3 C OMPARISON C ONFIGURATION P ROCEDURES
Since choice instances (and degree seeds) important final outcome
optimization, experimental evaluations always performed number independent
runs configuration procedure (typically 25). created separate list instances seeds
run explained above, kth run configuration procedure uses
kth list instances seeds. (Note, however, disjoint test set used measure performance
parameter configurations identical runs.)
281

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Configuration scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Type benchmark instances & citation
Graph colouring (Gent, Hoos, Prosser & Walsh, 1999)
Graph colouring (Gent, Hoos, Prosser & Walsh, 1999)
Quasigroup completion (Gomes & Selman, 1997)
Quasigroup completion (Gomes & Selman, 1997)
Combinatorial Auctions (CATS) (Leyton-Brown, Pearson & Shoham, 2000)

Table 1: Overview five B R configuration scenarios.
Algorithm
APS
PEAR

C PLEX

Parameter type
Continuous
Categorical
Integer
Continuous
Categorical
Integer
Continuous

# parameters type
4
10
4
12
50
8
5

# values considered
7
220
58
36
27
57
35

Total # configurations, ||
2 401
8.34 1017

1.38 1037

Table 2: Parameter overview algorithms consider. information parameters algorithm given text.
detailed list parameters values considered found online appendix
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/algorithms.html.
performed paired statistical test compare final results obtained runs two
configuration procedures. paired test required since kth run procedures shared
kth list instances seeds. particular, performed two-sided paired Max-Wilcoxon
test null hypothesis difference performances, considering p-values
0.05 statistically significant. p-values reported tables derived using
test; p-values shown parentheses refer cases procedure expected perform better
actually performed worse.
5.2 Configuration Scenarios
Section 6, analyze configurators based five configuration scenarios, combining
high-performance algorithm widely-studied benchmark dataset. Table 1 gives overview
these, dub B R scenarios. algorithms benchmark instance sets used
scenarios described detail Sections 5.2.1 5.2.2, respectively. five
B R configuration scenarios, set fairly aggressive cutoff times five seconds per run
target algorithm allowed configuration procedure execute target algorithm
aggregate runtime five CPU hours. short cutoff times fairly short times algorithm
configuration deliberately chosen facilitate many configuration runs B R scenario. contrast, second set configuration scenarios (exclusively focusing C PLEX),
set much larger cutoff times allowed time configuration. defer description
scenarios Section 7.
5.2.1 TARGET LGORITHMS
three target algorithms listed Table 2 along configurable parameters.
282

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

APS first target algorithm used experiments APS, high-performance dynamic
local search algorithm SAT solving (Hutter, Tompkins & Hoos, 2002) implemented UBCSAT (Tompkins & Hoos, 2004). introduced 2002, APS state-of-the-art solver,
still performs competitively many instances. chose study algorithm
well known, relatively parameters, intimately familiar it. APSs four
continuous parameters control scaling smoothing clause weights, well probability random walk steps. original default parameters set manually based experiments
prominent benchmark instances; manual experimentation kept percentage random
steps fixed took one week development time. subsequently gained
experience APSs parameters general problem classes (Hutter, Hamadi, Hoos &
Leyton-Brown, 2006), chose promising intervals parameter, including, centered
at, original default. picked seven possible values parameter spread uniformly
across respective interval, resulting 2401 possible parameter configurations (these exactly
values used Hutter et al., 2007). starting configuration ParamILS, used
center point parameters domain.
PEAR second target algorithm considered PEAR, recent tree search algorithm
solving SAT problems. PEAR state-of-the-art SAT solver industrial instances,
appropriate parameter settings best available solver certain types hardware
software verification instances (Hutter, Babic, Hoos & Hu, 2007). Furthermore, configured
ParamILS, PEAR quantifier-free bit-vector arithmetic category 2007 Satisfiability
Modulo Theories Competition. PEAR 26 parameters, including ten categorical, four integer,
twelve continuous parameters, default values manually engineered developer. (Manual tuning required one week.) categorical parameters mainly control
heuristics variable value selection, clause sorting, resolution ordering, enable disable
optimizations, pure literal rule. continuous integer parameters mainly deal
activity, decay, elimination variables clauses, well interval randomized
restarts percentage random choices. discretized integer continuous parameters
choosing lower upper bounds reasonable values allowing three eight
discrete values spread relatively uniformly across resulting interval, including default,
served starting configuration ParamILS. number discrete values chosen according intuition importance parameter. discretization,
3.7 1018 possible parameter configurations. Exploiting fact nine parameters
conditional (i.e., relevant parameters take certain values) reduced 8.34 1017
configurations.
C PLEX third target algorithm used commercial optimization tool C PLEX 10.1.1,
massively parameterized algorithm solving mixed integer programming (MIP) problems.
159 user-specifiable parameters, identified 81 parameters affect C PLEXs search trajectory. careful omit parameters change problem formulation (e.g., changing
numerical accuracy solution). Many C PLEX parameters deal MIP strategy heuristics (such variable branching heuristics, probing, dive type subalgorithms)
amount type preprocessing performed. also nine parameters governing
frequently different type cut used (those parameters four allowable
magnitude values value choose automatically; note last value prevents parameters ordinal). considerable number parameters deal simplex
283

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

barrier optimization, various algorithm components. categorical parameters
automatic option, considered categorical values well automatic one. contrast, continuous integer parameters automatic option, chose option instead
hypothesizing values might work well. also identified numerical parameters
primarily deal numerical issues, fixed default values. numerical
parameters, chose five possible values seemed sensible, including default.
many categorical parameters automatic option, included automatic option choice
parameter, also included manual options. Finally, ended 63 configurable parameters, leading 1.78 1038 possible configurations. Exploiting fact seven
C PLEX parameters relevant conditional parameters taking certain values,
reduced 1.38 1037 distinct configurations. starting configuration configuration
procedures, used default settings, obtained careful manual configuration
broad range MIP instances.
5.2.2 B ENCHMARK NSTANCES
applied target algorithms three sets benchmark instances: SAT-encoded quasi-group
completion problems, SAT-encoded graph-colouring problems based small world graphs,
MIP-encoded winner determination problems combinatorial auctions. set consisted
2000 instances, partitioned evenly training test sets.
QCP first benchmark set contained 23 000 instances quasi-group completion problem (QCP), widely studied AI researchers. generated QCP instances
around solubility phase transition, using parameters given Gomes Selman (1997).
Specifically, order n drawn uniformly interval [26, 43], number holes
H (open entries Latin square) drawn uniformly [1.75, 2.3] n1.55 . resulting
QCP instances converted SAT CNF format. use complete solver, PEAR,
sampled 2000 SAT instances uniformly random. average 1497 variables (standard deviation: 1094) 13 331 clauses (standard deviation: 12 473), 1182
satisfiable. use incomplete solver, APS, randomly sampled 2000 instances
subset satisfiable instances (determined using complete algorithm); number
variables clauses similar used PEAR.
SW-GCP second benchmark set contained 20 000 instances graph colouring problem
(GCP) based small world (SW) graphs Gent et al. (1999). these, sampled 2000
instances uniformly random use PEAR; average 1813 variables (standard
deviation: 703) 13 902 clauses (standard deviation: 5393), 1109 satisfiable.
use APS, randomly sampled 2000 satisfiable instances (again, determined using
complete SAT algorithm), whose number variables clauses similar used
PEAR.
Regions100 third benchmark set generated 2000 instances combinatorial auction
winner determination problem, encoded mixed-integer linear programs (MILPs). used
regions generator Combinatorial Auction Test Suite (Leyton-Brown et al., 2000),
goods parameter set 100 bids parameter set 500. resulting MILP instances
contained 501 variables 193 inequalities average, standard deviation 1.7 variables
2.5 inequalities.
284

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Default
20.41
9.74
12.97
2.65
1.61

Test performance (penalized average runtime, CPU seconds)
mean stddev. 10 runs
Run best training performance
BasicILS
FocusedILS
BasicILS
FocusedILS
0.32 0.06 0.32 0.05
0.26
0.26
8.05 0.9
8.3 1.1
6.8
6.6
4.86 0.56 4.70 0.39
4.85
4.29
1.39 0.33
1.29 0.2
1.16
1.21
0.5 0.3
0.35 0.04
0.35
0.32

Fig.
2(a)
2(b)
2(c)
2(d)
2(e)

Table 3: Performance comparison default parameter configuration configurations found
BasicILS FocusedILS (both Aggr Capping bm = 2). configuration scenario, list test performance (penalized average runtime 1000 test instances, CPU seconds) algorithm default, mean stddev test performance across
25 runs BasicILS(100) & FocusedILS (run five CPU hours each), test performance run BasicILS FocusedILS best terms training performance. Boldface indicates better BasicILS FocusedILS. algorithm configurations found FocusedILSs run best training performance listed online appendix http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html. Column Fig. gives reference scatter plot comparing performance configurations
algorithm defaults.

5.3 Experimental Setup
carried experiments cluster 55 dual 3.2GHz Intel Xeon PCs 2MB
cache 2GB RAM, running OpenSuSE Linux 10.1. measured runtimes CPU time
reference machines. configuration procedures implemented Ruby scripts,
include runtime scripts configuration time. easy configuration
scenarios, algorithm runs finish milliseconds, overhead scripts
substantial. Indeed, longest configuration run observed took 24 hours execute five hours
worth target algorithm runtime. contrast, harder C PLEX scenarios described Section
7 observed virtually overhead.

6. Empirical Evaluation BasicILS, FocusedILS Adaptive Capping
section, use B R scenarios empirically study performance BasicILS(N )
FocusedILS, well effect adaptive capping. first demonstrate large speedups
ParamILS achieved default parameters study components responsible
success.
6.1 Empirical Comparison Default Optimized Parameter Configurations
section, five B R configuration scenarios, compare performance
respective algorithms default parameter configuration final configurations found
BasicILS(100) FocusedILS. Table 3 especially Figure 2 show configurators led
substantial speedups.
Table 3, report final performance achieved 25 independent runs configurator. independent configuration run, used different set training instances seeds
(constructed described Section 5.1.2). note often rather large variance
performances found different runs configurators, configuration found
285

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

4

4

10

Runtime [s], autotuned

Runtime [s], autotuned

10

3

10

2

10

1

10

0

10

1

10

2

10

2

10

1

10

0

10

1

10

2

10
2

1

10

0

10

1

10

10

2

10

3

4

10

2

10

Runtime [s], default

10

(a) P -SWGCP.
531s vs 0.15s; 499 vs timeouts
4

1

10

0

10

1

2

10

1

10

0

10

1

10

2

10

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

(c) P -QCP.
72s vs 0.17s; 149 vs 1 timeouts

4

10

4

10

4

3

10

2

10

1

10

0

10

1

10

2

10
2

3

10

10

Runtime [s], autotuned

Runtime [s], autotuned

3

10

(b) P E R -SWGCP.
33s vs 17s; 3 vs 2 timeouts

10

10

2

10

Runtime [s], default

4

10

Runtime [s], autotuned

3

10

3

10

2

10

1

10

0

10

1

10

2

10
2

10

1

10

0

10

1

10

2

10

10

Runtime [s], default

(d) P E R -QCP.
9.6s vs 0.85s; 1 vs 0 timeouts

3

4

10

2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(e) C P L E X -R E G N 100.
1.61s vs 0.32s; timeouts

Figure 2: Comparison default vs automatically-determined parameter configurations five B R
configuration scenarios. dot represents one test instance; timeouts (after one CPU hour)
denoted circles. dashed line five CPU seconds indicates cutoff time target
algorithm used configuration process. subfigure captions give mean runtimes
instances solved configurations (default vs optimized), well number
timeouts each.

run best training performance also tended yield better test performance
others. reason, used configuration result algorithm configuration. (Note
choosing configuration found run best training set performance perfectly
legitimate procedure since require knowledge test set. course, improvements thus achieved come price increased overall running time, independent runs
configurator easily performed parallel.)
Figure 2, compare performance automatically-found parameter configuration
default configuration, runs allowed last hour. speedups
obvious figure Table 3, since penalized average runtime table counts
runtimes larger five seconds fifty seconds (ten times cutoff five seconds), whereas
data figure uses much larger cutoff time. larger speedups apparent scenarios
P -SWGCP, P -QCP, P E R -QCP: corresponding speedup factors mean runtime
3540, 416 11, respectively (see Figure 2).
286

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Algorithm 5: RandomSearch(N, 0 )
Outline random search parameter configuration space; inc denotes incumbent parameter
configuration, betterN compares two configurations based first N instances training
set.
Input : Number runs use evaluating parameter configurations, N ; initial configuration
0 .
Output : Best parameter configuration inc found.
1 inc 0 ;
2 TerminationCriterion()
3
random ;
4
betterN (, inc )
5
inc ;
6

return inc

6.2 Empirical Comparison BasicILS Simple Baselines
section, evaluate effectiveness BasicILS(N ) two components:
simple random search, used BasicILS initialization (we dub RandomSearch(N )
provide pseudocode Algorithm 5);
simple local search, type iterative first improvement search used BasicILS(N )
(we dub SimpleLS(N )).
evaluate one component time, section Section 6.3 study algorithms
without adaptive capping. investigate effect adaptive capping methods Section
6.4.
sufficient structure search space, expect BasicILS outperform RandomSearch. local minima, expect BasicILS perform better simple local search.
experiments showed BasicILS indeed offer best performance.
Here, solely interested comparing effectively approaches search space
parameter configurations (and found parameter configurations generalize unseen
test instances). Thus, order reduce variance comparisons, compare configuration
methods terms performance training set.
Table 4, compare BasicILS RandomSearch B R configuration scenarios.
average, BasicILS always performed better, three five scenarios, difference
statistically significant judged paired Max-Wilcoxon test (see Section 5.1.3). Table 4 also
lists performance default parameter configuration scenarios. note
BasicILS RandomSearch consistently achieved substantial (and statistically significant)
improvements default configurations.
Next, compared BasicILS second component, SimpleLS. basic local search
identical BasicILS, stops first local minimum encountered. used order
study whether local minima pose problem simple first improvement search. Table 5 shows
three configuration scenarios BasicILS time perform multiple ILS iterations,
training set performance statistically significantly better SimpleLS. Thus,
conclude search space contains structure exploited local search algorithm
well local minima limit performance iterative improvement search.
287

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Training performance (penalized average runtime, CPU seconds)
Default RandomSearch(100)
BasicILS(100)
19.93
0.46 0.34
0.38 0.19
10.61
7.02 1.11
6.78 1.73
12.71
3.96 1.185
3.19 1.19
2.77
0.58 0.59
0.36 0.39
1.61
1.45 0.35
0.72 0.45

p-value
0.94
0.18
1.4 105
0.007
1.2 105

Table 4: Comparison RandomSearch(100) BasicILS(100), without adaptive capping. table
shows training performance (penalized average runtime N = 100 training instances, CPU
seconds). Note approaches yielded substantially better results default configuration, BasicILS performed statistically significantly better RandomSearch three
five B R configuration scenarios judged paired Max-Wilcoxon test (see Section
5.1.3).
Scenario
P -SWGCP
P -QCP
P E R -QCP

SimpleLS(100)
Performance
0.5 0.39
3.60 1.39
0.4 0.39

BasicILS(100)
Performance
Avg. # ILS iterations
0.38 0.19
2.6
3.19 1.19
5.6
0.36 0.39
1.64

p-value
9.8 104
4.4 104
0.008

Table 5: Comparison SimpleLS(100) BasicILS(100), without adaptive capping. table shows
training performance (penalized average runtime N = 100 training instances, CPU seconds). configuration scenarios P E R -SWGCP C P L E X -R E G N 100, BasicILS complete first ILS iteration 25 runs; two approaches thus identical
listed here. configuration scenarios, BasicILS found significantly better configurations
SimpleLS.

6.3 Empirical Comparison FocusedILS BasicILS
section investigate FocusedILSs performance experimentally. contrast previous comparison RandomSearch, SimpleLS, BasicILS using training performance,
compare FocusedILS BasicILS using test performance. becausein contrast BasicILS SimpleLSFocusedILS grows number target algorithm runs used evaluate
parameter configuration time. Even different runs FocusedILS (using different training sets
random seeds) use number target algorithm runs evaluate parameter configurations. However, eventually aim optimize cost statistic, c, therefore
test set performance (an unbiased estimator c) provides fairer basis comparison training performance. compare FocusedILS BasicILS, since BasicILS already outperformed
RandomSearch SimpleLS Section 6.2.
Figure 3 compares test performance FocusedILS BasicILS(N ) N = 1, 10
100. Using single target algorithm run evaluate parameter configuration, BasicILS(1)
fast, generalize well test set all. example, configuration scenario
P -SWGCP, BasicILS(1) selected parameter configuration whose test performance turned
even worse default. hand, using large number target algorithm runs
evaluation resulted slow search, eventually led parameter configurations
good test performance. FocusedILS aims achieve fast search good generalization test
set. configuration scenarios Figure 3, FocusedILS started quickly also led best
final performance.
288

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

2

2

Mean runtime [s], test

Mean runtime [s], test

10

1

10

0

10

1

10

BasicILS(1)
BasicILS(10)
BasicILS(100)
FocusedILS
0

10

2

1

0.5

0

4

10

1.5

10

BasicILS(1)
BasicILS(10)
BasicILS(100)
FocusedILS
0

10

CPU time used tuner [s]

2

4

10

10

CPU time used tuner [s]

(a) P -SWGCP

(b) C P L E X -R E G N 100

Figure 3: Comparison BasicILS(N ) N = 1, 10, 100 vs FocusedILS, without adaptive
capping. show median test performance (penalized average runtime across 1 000 test
instances) across 25 runs configurators two scenarios. Performance three
B R scenarios qualitatively similar: BasicILS(1) fastest move away
starting parameter configuration, performance robust all; BasicILS(10)
rather good compromise speed generalization performance, given enough time
outperformed BasicILS(100). FocusedILS started finding good configurations quickly
(except scenario P E R -QCP, took even longer BasicILS(100) improve
default) always amongst best approaches end configuration process.

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Test performance (penalized average runtime, CPU seconds)
Default BasicILS(100)
FocusedILS
20.41
0.59 0.28
0.32 0.08
9.74
8.13 0.95
8.40 0.92
12.97
4.87 0.34
4.69 0.40
2.65
1.32 0.34
1.35 0.20
1.61
0.72 0.45
0.33 0.03

p-value
1.4 104
(0.21)
0.042
(0.66)
1.2 105

Table 6: Comparison BasicILS(100) FocusedILS, without adaptive capping. table shows
test performance (penalized average runtime 1 000 test instances, CPU seconds).
configuration scenario, report test performance default parameter configuration, mean
stddev test performance reached 25 runs BasicILS(100) FocusedILS, pvalue paired Max-Wilcoxon test (see Section 5.1.3) difference two configurators
performance.

compare performance FocusedILS BasicILS(100) configuration scenarios Table 6. three APS C PLEX scenarios, FocusedILS performed statistically significantly better BasicILS(100). results consistent past work FocusedILS achieved statistically significantly better performance BasicILS(100) (Hutter et al.,
2007). However, found configuration scenarios involving PEAR algorithm,
BasicILS(100) actually performed better average FocusedILS, albeit statistically significantly. attribute fact complete, industrial solver PEAR, two
benchmark distributions QCP SWGCP quite heterogeneous. expect FocusedILS
problems dealing highly heterogeneous distributions, due fact frequently tries
extrapolate performance based runs per parameter configuration.
289

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

2

2.5
BasicILS, capping
BasicILS, TP capping

Mean runtime [s], train

Mean runtime [s], train

10

1

10

0

10

1

10

2

3

10

10

BasicILS, capping
BasicILS, TP capping
2

1.5

1

0.5

0 2
10

4

10

CPU time used tuner [s]

3

10

4

10

CPU time used tuner [s]

(a) P -SWGCP, significant.

(b) C P L E X -R E G N 100, significant.

Figure 4: Speedup BasicILS adaptive capping two configuration scenarios. performed 25
runs BasicILS(100) without adaptive capping TP capping. time step,
computed training performance run configurator (penalized average runtime
N = 100 training instances) plot median 25 runs.
Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Training performance (penalized average runtime)
capping
TP capping
p-value
0.38 0.19
0.24 0.05
6.1 105
6.78 1.73
6.65 1.48
0.01
3.19 1.19
2.96 1.13
9.8 104
0.361 0.39 0.356 0.44
0.66
0.67 0.35
0.47 0.26
7.3 104

Avg. # ILS iterations
capping TP capping
3
12
1
1
6
10
2
3
1
1

Table 7: Effect adaptive capping BasicILS(100). show training performance (penalized average runtime N = 100 training instances, CPU seconds). configuration scenario,
report mean stddev final training performance reached 25 runs configurator without capping TP capping, p-value paired Max-Wilcoxon test difference
(see Section 5.1.3), well average number ILS iterations performed respective
configurator.

6.4 Empirical Evaluation Adaptive Capping BasicILS FocusedILS
present experimental evidence use adaptive capping strong impact
performance BasicILS FocusedILS.
Figure 4 illustrates extent TP capping sped BasicILS two configuration scenarios. cases, capping helped improve training performance substantially; P -SWGCP,
BasicILS found solutions order magnitude faster without capping.
Table 7 quantifies speedups five B R configuration scenarios. TP capping enabled
four times many ILS iterations (in P -SWGCP) improved average performance
scenarios. improvement statistically significant scenarios, except P E R -QCP.
Aggressive capping improved BasicILS performance one scenario. scenario
P -SWGCP, increased number ILS iterations completed within configuration time
12 219, leading significant improvement performance. first ILS iteration
BasicILS, capping techniques identical (the best configuration iteration always
incumbent). Thus, observe difference configuration scenarios P E R -SWGCP
C P L E X -R E G N 100, none 25 runs configurator finished first ILS iteration.
remaining two configuration scenarios, differences insignificant.
290

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Scenario
P -SWGCP
P E R -SWGCP
P -QCP
P E R -QCP
C P L E X -R E G N 100

Number ILS iterations performed
capping
TP capping
p-value
121 12
166 15
1.2 105
37 12
43 15
0.0026
142 18
143 22
0.54
153 49
165 41
0.03
36 13
40 16
0.26

Aggr capping
244 19
47 18
156 28
213 62
54 15

p-value
1.2 105
9 105
0.016
1.2 105
1.8 105

Number runs performed incumbent parameter configuration
Scenario
capping
TP capping
p-value
Aggr capping
p-value
P -SWGCP
993 211
1258 262 4.7 104 1818 243 1.2 105
503 265
476 238
(0.58)
642 288
0.009
P E R -SWGCP
P -QCP
1575 385 1701 318
0.065
1732 340
0.084
P E R -QCP
836 509
1130 557
0.02
1215 501
0.003
761 215
795 184
0.40
866 232
0.07
C P L E X -R E G N 100

Table 8: Effect adaptive capping search progress FocusedILS, measured number ILS
iterations performed number runs performed incumbent parameter configuration.
configuration scenario, report mean stddev measures across 25
runs configurator without capping, TP capping, Aggr capping, well
p-values paired Max-Wilcoxon tests (see Section 5.1.3) differences capping
TP capping; capping Aggr capping.

evaluate usefulness capping FocusedILS. Training performance useful
quantity context comparing different versions FocusedILS, since number target
algorithm runs measure based varies widely runs configurator. Instead,
used two measures quantify search progress: number ILS iterations performed
number target algorithm runs performed incumbent parameter configuration. Table 8
shows two measures five B R configuration scenarios three capping schemes
(none, TP, Aggr). FocusedILS TP capping achieved higher values without capping
scenarios measures (although differences statistically significant).
Aggressive capping increased measures scenarios, differences
capping aggressive capping statistically significant. Figure 5 demonstrates
two configuration scenarios FocusedILS capping reached solution qualities
quickly without capping. However, finding respective configurations, FocusedILS
showed significant improvement.
Recall experiments Section 6.2 6.3 compared various configurators without adaptive capping. One might wonder comparisons change presence adaptive
capping. Indeed, adaptive capping also worked box RandomSearch enabled
evaluate 3.4 33 times many configurations without capping. improvement
significantly improved simple algorithm RandomSearch point average performance came within 1% one BasicILS two domains (S P -SWGCP P E R -SWGCP;
compare much larger differences without capping reported Table 4). P E R -QCP,
still 25% difference average performance, result significant. Finally,
P -QCP C P L E X -R E G N 100 difference still substantial significant (22% 55%
difference average performance, p-values 5.2 105 0.0013, respectively).
Adaptive capping also reduced gap BasicILS FocusedILS. particular,
P -SWGCP, where, even without adaptive capping, FocusedILS achieved best performance
encountered scenario, BasicILS caught using adaptive capping. Similarly,
291

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

2

2
FocusedILS, capping
FocusedILS, TP capping
FocusedILS, Aggr capping

Mean runtime [s], test

Mean runtime [s], test

10

1

10

0

10

1

10

2

10

3

10

4

10

FocusedILS, capping
FocusedILS, TPcapping
FocusedILS, Aggr capping
1.5

1

0.5

0 1
10

5

10

CPU time used tuner [s]

2

10

3

10

4

10

5

10

CPU time used tuner [s]

(a) P -SWGCP

(b) C P L E X -R E G N 100

Figure 5: Speedup FocusedILS adaptive capping two configuration scenarios. performed 25
runs FocusedILS without adaptive capping, TP capping Aggr capping.
time step, computed test performance run configurator (penalized average
runtime 1000 test instances) plot median 25 runs. differences
end trajectory statistically significant. However, capping time required
achieve quality lower two configuration scenarios. three scenarios,
gains due capping smaller.

C P L E X -R E G N 100, FocusedILS already performed well without adaptive capping
BasicILS not. Here, BasicILS improved based adaptive capping, still could rival
FocusedILS. scenarios, adaptive capping affect relative performance much;
compare Tables 6 (without capping) 3 (with capping) details.

7. Case Study: Configuring C PLEX Real-World Benchmarks
section, demonstrate ParamILS improve performance commercial optimization tool C PLEX variety interesting benchmark distributions. best knowledge,
first published study automatically configuring C PLEX.
use five C PLEX configuration scenarios. these, collected wide range MIP benchmarks public benchmark libraries researchers, split 50:50
disjoint training test sets; detail following.
Regions200 set almost identical Regions100 set (described Section 5.2.2
used throughout paper), instances much larger. generated 2 000 MILP
instances generator provided Combinatorial Auction Test Suite (LeytonBrown et al., 2000), based regions option goods parameter set 200
bids parameter set 1 000. instances contain average 1 002 variables 385
inequalities, respective standard deviations 1.7 3.4.
MJA set comprises 343 machine-job assignment instances encoded mixed integer
quadratically constrained programs (MIQCP). obtained Berkeley Computational Optimization Lab5 introduced Akturk, Atamturk S. Gurel (2007).
instances contain average 2 769 variables 2 255 constraints, respective
standard deviations 2 133 1 592.
5. http://www.ieor.berkeley.edu/atamturk/bcol/, set called conic.sch

292

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

CLS set comprises 100 capacitated lot-sizing instances encoded mixed integer linear
programs (MILP). also obtained Berkeley Computational Optimization Lab
introduced Atamturk Munoz (2004). 100 instances contain 181 variables
180 constraints.
MIK set 120 MILP-encoded mixed-integer knapsack instances also obtained
Berkeley Computational Optimization Lab originally introduced Atamturk
(2003). instances contain average 384 variables 151 constraints, respective standard deviations 309 127.
QP set quadratic programs originated RNA energy parameter optimization (Andronescu, Condon, Hoos, Mathews & Murphy, 2007). Mirela Andronescu generated 2 000 instances experiments. instances contain 9 3667 165 variables 9 1917 186
constraints. Since instances polynomial-time solvable quadratic programs, set
large number inconsequential C PLEX parameters concerning branch cut mechanism default values, ending 27 categorical, 2 integer 2 continuous parameters configured, discretized parameter configuration space size 3.27 1017 .
study ParamILSs behavior harder problems, set significantly longer cutoff times
C PLEX scenarios B R scenarios previous section. Specifically,
used cutoff time 300 CPU seconds run target algorithm training,
allotted two CPU days every run configurators. always, configuration
objective minimize penalized average runtime penalization constant 10.
Table 9, compare performance C PLEXs default parameter configuration
final parameter configurations found BasicILS(100) FocusedILS (both aggressive capping bm = 2). Note that, similar situation described Section 6.1, configuration
scenarios (e.g., C P L E X -CLS, C P L E X -MIK) substantial variance different runs
configurators, run best training performance yielded parameter configuration
also good test set. BasicILS outperformed FocusedILS 3 5
scenarios terms mean test performance across ten runs, FocusedILS achieved better test
performance run best training performance one scenario (in performed almost well). scenarios C P L E X -R E G N 200 C P L E X -CLS, FocusedILS performed
substantially better BasicILS.
Note C PLEX configuration scenarios considered, BasicILS FocusedILS
found parameter configurations better algorithm defaults, sometimes
order magnitude. particularly noteworthy since ILOG expended substantial effort
determine strong default C PLEX parameters. Figure 6, provide scatter plots five scenarios. C P L E X -R E G N 200, C P L E X - C N C . C H , C P L E X -CLS, C P L E X -MIK, speedups quite
consistent across instances (with average speedup factors reaching 2 C P L E X - C N C . C H
23 C P L E X -MIK). Finally, C P L E X -QP see interesting failure mode ParamILS.
optimized parameter configuration achieved good performance cutoff time used
6. configuration scenario C P L E X -MIK, nine ten runs FocusedILS yielded parameter configurations
average runtimes smaller two seconds. One run, however, demonstrated interesting failure mode FocusedILS aggressive capping. Capping aggressively caused every C PLEX run unsuccessful,
FocusedILS selected configuration manage solve single instance test set. Counting unsuccessful runs ten times cutoff time, resulted average runtime 10 300 = 3000 seconds run.
(For full details, see Section 8.1 Hutter, 2009).

293

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Scenario
C P L E X -R E G N 200
CP L E X-C N C.S C H
C P L E X -CLS
C P L E X -MIK
C P L E X -QP

Test performance (penalized average runtime, CPU seconds)
mean stddev. 10 runs
Run best training performance
Default
BasicILS
FocusedILS BasicILS
FocusedILS
72
45 24
11.4 0.9
15
10.5
5.37
2.27 0.11
2.4 0.29
2.14
2.35
712
443 294
327 860
80
23.4
64.8
20 27
301 948 6
1.72
1.19
969
755 214
827 306
528
525

Fig.
6(a)
6(b)
6(c)
6(d)
6(e)

Table 9: Experimental results C PLEX configuration scenarios.

configuration scenario, list test performance (penalized average runtime test instances) algorithm default, mean stddev test performance across ten runs BasicILS(100)
& FocusedILS (run two CPU days each), test performance run
BasicILS FocusedILS best terms training performance. Boldface indicates better BasicILS FocusedILS. algorithm configurations found
FocusedILSs run best training performance listed online appendix
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html.
Column
Fig. gives reference scatter plot comparing performance configurations
algorithm defaults.

configuration process (300 CPU seconds, see Figure 6(f)), performance carry
higher cutoff time used tests (3600 CPU seconds, see Figure 6(e)). Thus, parameter configuration found FocusedILS generalize well previously unseen test data,
larger cutoff times.

8. Review ParamILS Applications
section, review number applications ParamILSsome dating back
earlier stages development, others recentthat demonstrate utility versatility.
8.1 Configuration SAPS, GLS+ SAT4J
Hutter et al. (2007), first publication ParamILS, reported experiments three target algorithms demonstrate effectiveness approach: SAT algorithm SAPS (which
4 numerical parameters), local search algorithm GLS+ solving probable explanation (MPE) problem Bayesian networks (which 5 numerical parameters; Hutter, Hoos &
Stutzle, 2005), tree search SAT solver SAT4J (which 4 categorical 7 numerical
parameters; http://www.sat4j.org). compared respective algorithms default performance,
performance CALIBRA system (Adenso-Diaz & Laguna, 2006), performance
BasicILS FocusedILS. four configuration scenarios studied, FocusedILS significantly outperformed CALIBRA two performed better average third. fourth
one (configuring SAT4J), CALIBRA applicable due categorical parameters,
FocusedILS significantly outperformed BasicILS.
Overall, automated parameter optimization using ParamILS achieved substantial improvements
previous default settings: GLS+ sped factor > 360 (tuned parameters found
solutions better quality 10 seconds default found one hour), SAPS factors 8
130 SAPS-QWH SAPS-SW, respectively, SAT4J factor 11.
294

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

4

4

2

10

1

10

0

10

1

10

2

10

Runtime [s], autotuned

3

10

10

3

10

2

10

1

10

0

10

1

10

2

10
2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

1

10

0

10

1

10

2

10

3

4

10

Runtime [s], default

1

10

0

10

1

10

2

10

10

1

10

0

10

1

10

2

10

10

Runtime [s], default

(d) C P L E X -MIK.
28s vs 1.2s; timeouts

3

4

10

2
1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(c) C P L E X -CLS.
309s vs 21.5s; timeouts
4

10

3

10

2

10

1

10

0

10

1

10

2

3

10

2

10

1

10

0

10

1

10

2

10
2

1

10

2

Runtime [s], autotuned

Runtime [s], autotuned

2

10

0

10

10

4

3

1

10

10

10

10

2

10

4

(b) C P L E X - C N C . C H .
5.37s vs 2.39.5s; timeouts

10

3

10

10
2

10

(a) C P L E X -R E G N 200.
72s vs 10.5s; timeouts

Runtime [s], autotuned

4

10

Runtime [s], autotuned

Runtime [s], autotuned

10

10
2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

(e) C P L E X -QP.
296s vs 234s; 0 vs 21 timeouts

4

2

10

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(f) C P L E X -QP, test cutoff 300
seconds.
81s vs 44s; 305 vs 150 timeouts

Figure 6: Comparison default vs automatically-determined parameter configuration five C PLEX
configuration scenarios. dot represents one test instance; time-outs (after one CPU hour)
denoted red circles. blue dashed line 300 CPU seconds indicates cutoff time
target algorithm used configuration process. subfigure captions give mean
runtimes instances solved configurations (default vs optimized), well
number timeouts each.

8.2 Configuration Spear Industrial Verification Problems
Hutter et al. (2007) applied ParamILS specific real-world application domain: configuring
26 parameters tree-search DPLL solver PEAR minimize mean runtime set
practical verification instances. particular, considered two sets industrial problem
instances, bounded model-checking (BMC) instances Zarpas (2005) software verification
(SWV) instances generated C ALYSTO static checker (Babic & Hu, 2007).
instances problem distributions exhibited large spread hardness PEAR.
SWV instances, default configuration solved many instances milliseconds failed
solve others days. despite fact PEAR specifically developed type
instances, developer generated problem instances (and thus intimate
domain knowledge), week manual performance tuning expended order
optimize solvers performance.
PEAR first configured good general performance industrial SAT instances
previous SAT competitions. already led substantial improvements default perfor295

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

mance 2007 SAT competition.7 PEAR default solved 82 instances ranked 17th
first round competition, automatically configured version solved 93 instances
ranked 8th, optimized version solved 99 instances, ranking 5th (above MiniSAT).
speedup factors due general optimization 20 1.3 SWV BMC datasets,
respectively.
Optimizing specific instance sets yielded further, much larger improvements (a factor
500 SWV 4.5 BMC). encouragingly, best parameter configuration found
software verification instances take longer 20 seconds solve SWV
problem instances (compared multiple timeouts CPU day original default values).
Key good performance application perform multiple independent runs FocusedILS, select found configuration best training performance (as also done
Sections 6.1 7 article).
8.3 Configuration SATenstein
KhudaBukhsh, Xu, Hoos Leyton-Brown (2009 ) used ParamILS perform automatic algorithm
design context stochastic local search algorithms SAT. Specifically, introduced
new framework local search SAT solvers, called SATenstein, used ParamILS choose
good instantiations framework given instance distributions. SATenstein spans three broad
categories SLS-based SAT solvers: WalkSAT-based algorithms, dynamic local search algorithms
G2 WSAT variants. combined highly parameterized framework solver
total 41 parameters 4.82 1012 unique instantiations.
FocusedILS used configure SATenstein six different problem distributions,
resulting solvers compared eleven state-of-the-art SLS-based SAT solvers. results
showed automatically configured versions SATenstein outperformed eleven
state-of-the-art solvers six categories, sometimes large margin.
SAT ENSTEIN work clearly demonstrated automated algorithm configuration methods
used construct new algorithms combining wide range components existing algorithms novel ways, thereby go beyond simple parameter tuning. Due low
level manual work required approach, believe automated design algorithms
components become mainstream technique development algorithms hard
combinatorial problems.
Key successful application FocusedILS configuring SAT ENSTEIN careful
selection homogeneous instance distributions, instances could solved within
comparably low cutoff time 10 seconds per run. Again, configuration best training
quality selected ten parallel independent runs FocusedILS per scenario.
8.4 Self-Configuration ParamILS
heuristic optimization procedure, ParamILS controlled number parameters:
number random configurations, r, sampled beginning search; perturbation
strength, s; probability random restarts, prestart . Furthermore, aggressive capping
mechanism makes use additional parameter: bound multiplier, bm. Throughout article,
used manually-determined default values hr, s, prestart , bmi = h10, 3, 0.01, 2i.
7. See http://www.cril.univ-artois.fr/SAT07. PEAR allowed participate second round
competition since source code publicly available.

296

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

recent work (see Section 8.2 Hutter, 2009), evaluated whether FocusedILSs performance could improved using ParamILS automatically find better parameter configuration.
self-configuration task, configuration scenarios play role instances, configurator optimized plays role target algorithm. avoid confusion, refer
configurator target configurator. Here, set fairly short configuration times one CPU
hour target configurator. However, still significantly longer cutoff times
used experiments, parallelization turned crucial finish experiment reasonable amount time. BasicILS easier parallelize
FocusedILS, chose BasicILS(100) meta-configurator.
Although notion algorithm configurator configure intriguing,
case, turned yield small improvements. Average performance improved four
five scenarios degraded remaining one. However, none differences
statistically significant.
8.5 Applications ParamILS
Thachuk, Shmygelska Hoos (2007 ) used BasicILS order determine performance-optimizing
parameter settings new replica-exchange Monte Carlo algorithm protein folding 2DHP 3D-HP models.8 Even though algorithm four parameters (two categorical
two continuous), BasicILS achieved substantial performance improvements. manuallyselected configurations biased favour either short long protein sequences, BasicILS
found configuration consistently yielded good mean runtimes types sequences.
average, speedup factor achieved approximately 1.5, certain classes protein
sequences 3. manually-selected configurations performed worse previous
state-of-the-art algorithm problem instances, robust parameter configurations
selected BasicILS yielded uniformly better performance.
recent work, Fawcett, Hoos Chiarandini (2009) used several variants ParamILS
(including version slightly extended beyond ones presented here) design
modular stochastic local search algorithm post-enrollment course timetabling problem.
followed design approach used automated algorithm configuration order explore
large design space modular highly parameterised stochastic local search algorithms.
quickly led solver placed third Track 2 2nd International Timetabling Competition
(ITC2007) subsequently produced improved solver shown achieve consistently
better performance top-ranked solver competition.

9. Related Work
Many researchers us dissatisfied manual algorithm configuration, various
fields developed approaches automatic parameter tuning. start section
closely-related workapproaches employ direct search find good parameter
configurationsand describe methods. Finally, discuss work related problems,
finding best parameter configuration algorithm per-instance basis, approaches
adapt parameters algorithms execution (see also Hoos, 2008, related
work automated algorithm design).
8. BasicILS used, FocusedILS yet developed study conducted.

297

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

9.1 Direct Search Methods Algorithm Configuration
Approaches automated algorithm configuration go back early 1990s, number
systems developed adaptive problem solving. One systems Composer (Gratch
& Dejong, 1992), performs hill-climbing search configuration space, taking moves
enough evidence gathered render neighbouring configuration statistically significantly
better current configuration. Composer successfully applied improving five
parameters algorithm scheduling communication collection ground-based
antennas spacecrafts (Gratch & Chien, 1996).
Around time, MULTI-TAC system introduced Minton (1993, 1996). MULTITAC takes input generic heuristics, specific problem domain, distribution problem instances. adapts generic heuristics problem domain automatically generates
domain-specific LISP programs implementing them. beam search used choose best
LISP program program evaluated running fixed set problem instances
sampled given distribution.
Another search-based approach uses fixed training set introduced Coy et al. (2001).
approach works two stages. First, finds good parameter configuration instance Ii training set combination experimental design (full factorial fractional
factorial) gradient descent. Next, combines parameter configurations 1 , . . . , N thus determined setting parameter average values taken them. Note
averaging step restricts applicability method algorithms numerical parameters.
similar approach, also based combination experimental design gradient descent,
using fixed training set evaluation, implemented CALIBRA system Adenso-Diaz
Laguna (2006). CALIBRA starts evaluating parameter configuration full factorial
design two values per parameter. iteratively homes good regions parameter
configuration space employing fractional experimental designs evaluate nine configurations
around best performing configuration found far. grid experimental design
refined iteration. local optimum found, search restarted (with coarser
grid). Experiments showed CALIBRAs ability find parameter settings six target algorithms
matched outperformed respective originally-proposed parameter configurations. main
drawback limitation tuning numerical ordinal parameters, maximum five
parameters. first introduced ParamILS, performed experiments comparing performance CALIBRA (Hutter et al., 2007). experiments reviewed Section 8.1.
Terashima-Marn et al. (1999) introduced genetic algorithm configuring constraint satisfaction algorithm large-scale university exam scheduling. constructed configured
algorithm works two stages seven configurable categorical parameters. optimized choices genetic algorithm 12 problem instances,
found configuration improved performance modified Brelaz algorithm. However, note
performed optimization separately instance. paper quantify
long optimizations took, stated Issues time delivering solutions
method still matter research.
Work automated parameter tuning also found numerical optimization literature. particular, Audet Orban (2006) proposed mesh adaptive direct search algorithm.
Designed purely continuous parameter configuration spaces, algorithm guaranteed converge local optimum cost function. Parameter configurations evaluated fixed
298

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

set large unconstrained regular problems CUTEr collection, using optimization objectives runtime number function evaluations required solving given problem instance.
Performance improvements around 25% classical configuration four continuous parameters interior point methods reported.
Algorithm configuration stochastic optimization problem, exists large body
algorithms designed problems (see, e.g., Spall, 2003). However, many algorithms
stochastic optimization literature require explicit gradient information thus inapplicable
algorithm configuration. algorithms approximate gradient function evaluations
(e.g., finite differences), provably converge local minimum cost function
mild conditions, continuity. Still, methods primarily designed deal
numerical parameters find local minima. aware applications general
purpose algorithms stochastic optimization algorithm configuration.
9.2 Methods Algorithm Configuration
Sequential parameter optimization (SPO) (Bartz-Beielstein, 2006) model-based parameter optimization approach based Design Analysis Computer Experiments (DACE; see, e.g.,
Santner, Williams & Notz, 2003), prominent approach statistics blackbox function optimization. SPO starts running target algorithm parameter configurations Latin
hypercube design number training instances. builds response surface model based
Gaussian process regression uses models predictions predictive uncertainties determine next parameter configuration evaluate. metric underlying choice promising
parameter configurations expected improvement criterion used Jones, Schonlau Welch
(1998). algorithm run, response surface refitted, new parameter configuration determined based updated model. contrast previously-mentioned methods,
SPO use fixed training set. Instead, starts small training set doubles size
whenever parameter configuration determined incumbent already incumbent
previous iteration. recent improved mechanism resulted robust version, SPO+ (Hutter, Hoos, Leyton-Brown & Murphy, 2009). main drawbacks SPO variants,
fact entire DACE approach, limitation continuous parameters optimizing
performance single problem instances, well cubic runtime scaling number data
points.
Another approach based adaptations racing algorithms machine learning (Maron &
Moore, 1994) algorithm configuration problem. Birattari et al. (2002; 2004) developed procedure dubbed F-Race used configure various stochastic local search algorithms. F-Race
takes input algorithm A, finite set algorithm configurations , instance distribution D. iteratively runs target algorithm surviving parameter configurations
number instances sampled (in simplest case, iteration runs surviving configurations one instance). configuration eliminated race soon enough statistical
evidence gathered it. iteration, non-parametric Friedman test used check
whether significant differences among configurations. case, inferior
configurations eliminated using series pairwise tests. process iterated
one configuration survives given cutoff time reached. Various applications F-Race
demonstrated good performance (for overview, see Birattari, 2004). However, since
start procedure candidate configurations evaluated, approach limited situations
space candidate configurations practically enumerated. fact, published ex299

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

periments F-Race limited applications around 1200 configurations.
recent extension presented Balaprakash et al. (2007) iteratively performs F-Race subsets
parameter configurations. approach scales better large configuration spaces, version
described Balaprakash et al. (2007) handles algorithms numerical parameters.
9.3 Related Algorithm Configuration Problems
point, focused problem finding best algorithm configuration
entire set (or distribution) problem instances. Related approaches attempt find best
configuration algorithm per-instance basis, adapt algorithm parameters
execution algorithm. Approaches setting parameters per-instance basis
described Patterson Kautz (2001), Cavazos OBoyle (2006), Hutter et al. (2006).
Furthermore, approaches attempt select best algorithm per-instance basis
studied Leyton-Brown, Nudelman Shoham (2002), Carchrae Beck (2005), Gebruers,
Hnich, Bridge Freuder (2005), Gagliolo Schmidhuber (2006), Xu, Hutter, Hoos
Leyton-Brown (2008). related work, decisions restart algorithm made
online, run algorithm (Horvitz, Ruan, Gomes, Kautz, Selman & Chickering, 2001;
Kautz, Horvitz, Ruan, Gomes & Selman, 2002; Gagliolo & Schmidhuber, 2007). So-called reactive
search methods perform online parameter modifications (Battiti, Brunato & Mascia, 2008).
last strategy seen complementary work: even reactive search methods tend
parameters remain fixed search hence configured using offline approaches
ParamILS.
9.4 Relation Local Search Methods
Since ParamILS performs iterated local search one-exchange neighbourhood,
similar spirit local search methods problems, SAT (Selman, Levesque &
Mitchell, 1992; Hoos & Stutzle, 2005), CSP (Minton, Johnston, Philips & Laird, 1992), MPE
(Kask & Dechter, 1999; Hutter et al., 2005). Since ParamILS local search method, existing
theoretical frameworks (see, e.g., Hoos, 2002; Mengshoel, 2008), could principle used
analysis. main factor distinguishing problem ones faced standard local
search algorithms stochastic nature optimization problem (for discussion local
search stochastic optimization, see, e.g., Spall, 2003). Furthermore, exists compact
representation objective function could used guide search. illustrate this,
consider local search SAT, candidate variables flipped limited
occurring currently-unsatisfied clauses. general algorithm configuration, hand,
mechanism cannot used, information available target algorithm
performance runs executed far. While, obviously, (stochastic) local search
methods could used basis algorithm configuration procedures, chose iterated local
search, mainly conceptual simplicity flexibility.

10. Discussion, Conclusions Future work
work, studied problem automatically configuring parameters complex,
heuristic algorithms order optimize performance given set benchmark instances.
extended earlier algorithm configuration procedure, ParamILS, new capping mechanism
300

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

obtained excellent results applying resulting enhanced version ParamILS two
high-performance SAT algorithms well C PLEX wide range benchmark sets.
Compared carefully-chosen default configurations target algorithms, parameter configurations found ParamILS almost always performed much better evaluated
sets previously unseen test instances, configuration scenarios much two orders
magnitude. improvements C PLEXs default parameter configuration particularly
noteworthy, though claim found new parameter configuration C PLEX
uniformly better default. Rather, given somewhat homogeneous instance set, find
configuration specific set typically outperforms default, sometimes factor high
20. Note achieved results even though intimately familiar C PLEX
parameters; chose parameters optimize well values consider based
single person-day studying C PLEX user manual. success automated algorithm
configuration even extreme conditions demonstrates potential approach.
ParamILS source code executable freely available
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/,
along quickstart guide data configuration scenarios studied article.9
order apply ParamILS, automated algorithm configuration methods, practitioner must supply following ingredients.
parameterized algorithm must possible set configurable parameters externally, e.g., command line call. Often, search hard-coded parameters hidden
algorithms source code lead large number additional parameters exposed.
Domains parameters Algorithm configurators must provided allowable
values parameter. Depending configurator, may possible include additional knowledge dependencies parameters, conditional parameters
supported ParamILS. use ParamILS, numerical parameters must discretized
finite number choices. Depending type parameter, uniform spacing
values spacing, uniform log scale, typically reasonable.
set problem instances homogeneous problem set interest is, better
expect algorithm configuration procedure perform it. possible
configure algorithm good performance rather heterogeneous instance sets (e.g.,
industrial SAT instances, PEAR reported Section 8.2), results
homogeneous subsets interest improve configure instances subset. Whenever possible, set instances split disjoint training test sets
order safeguard over-tuning. configuring small and/or heterogeneous
benchmark set, ParamILS (or configuration procedure) might find configurations perform well independent test set.
objective function used median performance first study ParamILS
(Hutter et al., 2007), since found cases optimizing median performance led
parameter configurations good median poor overall performance. cases,
optimizing mean performance yielded robust parameter configurations. However,
optimizing mean performance one define cost unsuccessful runs.
article, penalized runs counting ten times cutoff time.
deal unsuccessful runs principled manner open research question.
9. ParamILS continues actively developed; currently maintained Chris Fawcett.

301

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

cutoff time unsuccessful runs smaller cutoff time run target
algorithm chosen, quickly configuration procedure able explore
configuration space. However, choosing small cutoff risks failure mode experienced C P L E X -QP scenario. Recall there, choosing 300 seconds timeout
yielded parameter configuration good judged cutoff time (see
Figure 6(f)), performed poorly longer cutoffs (see Figure 6(e)). experiments, parameter configurations performing well low cutoff times turned scale
well harder problem instances well. many configuration scenarios, fact, noticed
automatically-found parameter configurations showed much better scaling behaviour
default configuration. attribute use mean runtime configuration
objective. mean often dominated hardest instances distribution. However,
manual tuning, algorithm developers typically pay attention easier instances, simply
repeated profiling hard instances takes long. contrast, patient automatic
configurator achieve better results avoids bias.
Computational resources amount (computational) time required application
automated algorithm configuration clearly depends target application. target
algorithm takes seconds solve instances homogeneous benchmark set interest,
experience single five-hour configuration run suffice yield good results
domains achieved good results configuration times short half
hour. contrast, runs target algorithm slow performance large
cutoff time expected yield good results instances interest, time
requirements automated algorithm configuration grow. also regularly perform multiple
parallel configuration runs pick one best training performance order deal
variance across configuration runs.
Overall, firmly believe automated algorithm configuration methods ParamILS
play increasingly prominent role development high-performance algorithms
applications. study methods rich fruitful research area many interesting questions remaining explored.
ongoing work, currently developing methods adaptively adjust domains
integer-valued continuous parameters configuration process. Similarly, plan
enhance ParamILS dedicated methods dealing continuous parameters require discretization user. Another direction development concerns strategic
selection problem instances used evaluation configurations instance-specific cutoff times used context. heuristically preventing configuration procedure spending inordinate amounts time trying evaluate poor parameter settings hard problem
instances, possible improve scalability.
believe significant room combining aspects methods studied
concepts related work similar algorithm configuration problems. particular,
believe would fruitful integrate statistical testing methodsas used, e.g., F-Race
ParamILS. Furthermore, see much potential use response surface models
active learning, believe combined approach. Finally, algorithm
configuration problem studied article significant practical importance, also much
gained studying methods related problems, particular, instance-specific algorithm
configuration online adjustment parameters run algorithm.
302

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Acknowledgments
thank Kevin Murphy many helpful discussions regarding work. also thank Domagoj
Babic, author PEAR, Dave Tompkins, author UBCSAT APS implementation
used experiments. thank researchers provided instances instance
generators used work, particular Gent et al. (1999), Gomes Selman (1997), LeytonBrown et al. (2000), Babic Hu (2007), Zarpas (2005), Le Berre Simon (2004), Akturk
et al. (2007), Atamturk Munoz (2004), Atamturk (2003), Andronescu et al. (2007). Lin
Xu created specific sets QCP SWGCP instances used. Thanks also Chris Fawcett
Ashique KhudaBukhsh comments draft article. Finally, thank
anonymous reviewers well Rina Dechter Adele Howe valuable feedback. Thomas
Stutzle acknowledges support F.R.S.-FNRS, Research Associate. Holger
Hoos acknowledges support NSERC Discovery Grant 238788.

References
Adenso-Diaz, B. & Laguna, M. (2006). Fine-tuning algorithms using fractional experimental design
local search. Operations Research, 54(1), 99114.
Akturk, S. M., Atamturk, A., & Gurel, S. (2007). strong conic quadratic reformulation machine-job
assignment controllable processing times. Research Report BCOL.07.01, University CaliforniaBerkeley.
Andronescu, M., Condon, A., Hoos, H. H., Mathews, D. H., & Murphy, K. P. (2007). Efficient parameter
estimation RNA secondary structure prediction. Bioinformatics, 23, i19i28.
Atamturk, A. (2003). facets mixedinteger knapsack polyhedron. Mathematical Programming,
98, 145175.
Atamturk, A. & Munoz, J. C. (2004). study lot-sizing polytope. Mathematical Programming, 99,
443465.
Audet, C. & Orban, D. (2006). Finding optimal algorithmic parameters using mesh adaptive direct search
algorithm. SIAM Journal Optimization, 17(3), 642664.
Babic, D. & Hu, A. J. (2007). Structural Abstraction Software Verification Conditions. W. Damm, H. H.
(Ed.), Computer Aided Verification: 19th International Conference, CAV 2007, volume 4590 Lecture
Notes Computer Science, (pp. 366378). Springer Verlag, Berlin, Germany.
Balaprakash, P., Birattari, M., & Stutzle, T. (2007). Improvement strategies F-Race algorithm: Sampling design iterative refinement. Bartz-Beielstein, T., Aguilera, M. J. B., Blum, C., Naujoks,
B., Roli, A., Rudolph, G., & Sampels, M. (Eds.), 4th International Workshop Hybrid Metaheuristics (MH07), (pp. 108122).
Bartz-Beielstein, T. (2006). Experimental Research Evolutionary Computation: New Experimentalism. Natural Computing Series. Springer Verlag, Berlin, Germany.
Battiti, R., Brunato, M., & Mascia, F. (2008). Reactive Search Intelligent Optimization, volume 45
Operations research/Computer Science Interfaces. Springer Verlag. Available online http://reactivesearch.org/thebook.
Birattari, M. (2004). Problem Tuning Metaheuristics Seen Machine Learning Perspective.
PhD thesis, Universite Libre de Bruxelles, Brussels, Belgium.
Birattari, M., Stutzle, T., Paquete, L., & Varrentrapp, K. (2002). racing algorithm configuring metaheuristics. Langdon, W. B., Cantu-Paz, E., Mathias, K., Roy, R., Davis, D., Poli, R., Balakrishnan, K.,
Honavar, V., Rudolph, G., Wegener, J., Bull, L., Potter, M. A., Schultz, A. C., Miller, J. F., Burke, E.,
& Jonoska, N. (Eds.), Proceedings Genetic Evolutionary Computation Conference (GECCO2002), (pp. 1118). Morgan Kaufmann Publishers, San Francisco, CA, USA.
303

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

Carchrae, T. & Beck, J. C. (2005). Applying machine learning low-knowledge control optimization
algorithms. Computational Intelligence, 21(4), 372387.
Cavazos, J. & OBoyle, M. F. P. (2006). Method-specific dynamic compilation using logistic regression.
Cook, W. R. (Ed.), Proceedings ACM SIGPLAN International Conference Object-Oriented Programming, Systems, Languages, Applications (OOPSLA-06), (pp. 229240)., New York, NY, USA.
ACM Press.
Coy, S. P., Golden, B. L., Runger, G. C., & Wasil, E. A. (2001). Using experimental design find effective
parameter settings heuristics. Journal Heuristics, 7(1), 7797.
Diao, Y., Eskesen, F., Froehlich, S., Hellerstein, J. L., Spainhower, L., & Surendra, M. (2003). Generic online
optimization multiple configuration parameters application database server. Brunner, M. &
Keller, A. (Eds.), 14th IFIP/IEEE International Workshop Distributed Systems: Operations Management (DSOM-03), volume 2867 Lecture Notes Computer Science, (pp. 315). Springer Verlag,
Berlin, Germany.
Fawcett, C., Hoos, H. H., & Chiarandini, M. (2009). automatically configured modular algorithm post
enrollment course timetabling. Technical Report TR-2009-15, University British Columbia, Department
Computer Science.
Gagliolo, M. & Schmidhuber, J. (2006). Dynamic algorithm portfolios. Amato, C., Bernstein, D., & Zilberstein, S. (Eds.), Ninth International Symposium Artificial Intelligence Mathematics (AI-MATH-06).
Gagliolo, M. & Schmidhuber, J. (2007). Learning restart strategies. Veloso, M. M. (Ed.), Proceedings
Twentieth International Joint Conference Artificial Intelligence (IJCAI07), volume 1, (pp. 792
797). Morgan Kaufmann Publishers, San Francisco, CA, USA.
Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR select solution strategies constraint programming. Munoz-Avila, H. & Ricci, F. (Eds.), Proceedings 6th International Conference Case Based Reasoning (ICCBR05), volume 3620 Lecture Notes Computer Science, (pp.
222236). Springer Verlag, Berlin, Germany.
Gent, I. P., Hoos, H. H., Prosser, P., & Walsh, T. (1999). Morphing: Combining structure randomness.
Hendler, J. & Subramanian, D. (Eds.), Proceedings Sixteenth National Conference Artificial
Intelligence (AAAI99), (pp. 654660)., Orlando, Florida. AAAI Press / MIT Press, Menlo Park, CA,
USA.
Gomes, C. P. & Selman, B. (1997). Problem structure presence perturbations. Kuipers, B. &
Webber, B. (Eds.), Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97),
(pp. 221226). AAAI Press / MIT Press, Menlo Park, CA, USA.
Gratch, J. & Chien, S. A. (1996). Adaptive problem-solving large-scale scheduling problems: case
study. Journal Artificial Intelligence Research, 4, 365396.
Gratch, J. & Dejong, G. (1992). Composer: probabilistic solution utility problem speed-up
learning. Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings Tenth National Conference
Artificial Intelligence (AAAI92), (pp. 235240). AAAI Press / MIT Press, Menlo Park, CA, USA.
Hoos, H. H. (2002). mixture-model behaviour SLS algorithms SAT. Proceedings
Eighteenth National Conference Artificial Intelligence (AAAI-02), (pp. 661667)., Edmonton, Alberta,
Canada.
Hoos, H. H. (2008). Computer-aided design high-performance algorithms. Technical Report TR-2008-16,
University British Columbia, Department Computer Science.
Hoos, H. H. & Stutzle, T. (2005). Stochastic Local Search Foundations & Applications. Morgan Kaufmann
Publishers, San Francisco, CA, USA.
Horvitz, E., Ruan, Y., Gomes, C. P., Kautz, H., Selman, B., & Chickering, D. M. (2001). Bayesian
approach tackling hard computational problems. Breese, J. S. & Koller, D. (Eds.), Proceedings
Seventeenth Conference Uncertainty Artificial Intelligence (UAI01), (pp. 235244). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
304

fiPARAM ILS: N AUTOMATIC LGORITHM C ONFIGURATION F RAMEWORK

Hutter, F. (2009). Automated Configuration Algorithms Solving Hard Computational Problems. PhD
thesis, University British Columbia, Department Computer Science, Vancouver, Canada.
Hutter, F., Babic, D., Hoos, H. H., & Hu, A. J. (2007). Boosting Verification Automatic Tuning Decision
Procedures. Proceedings Formal Methods Computer Aided Design (FMCAD07), (pp. 2734).,
Washington, DC, USA. IEEE Computer Society.
Hutter, F., Hamadi, Y., Hoos, H. H., & Leyton-Brown, K. (2006). Performance prediction automated
tuning randomized parametric algorithms. Benhamou, F. (Ed.), Principles Practice Constraint Programming CP 2006: Twelfth International Conference, volume 4204 Lecture Notes
Computer Science, (pp. 213228). Springer Verlag, Berlin, Germany.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2009). Tradeoffs empirical evaluation competing algorithm designs. Technical Report TR-2009-21, University British Columbia, Department Computer
Science.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Murphy, K. P. (2009). experimental investigation
model-based parameter optimisation: SPO beyond. Proceedings Genetic Evolutionary
Computation Conference (GECCO-2009), (pp. 271278).
Hutter, F., Hoos, H. H., & Stutzle, T. (2005). Efficient stochastic local search MPE solving. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI05), (pp. 169174).
Hutter, F., Hoos, H. H., & Stutzle, T. (2007). Automatic algorithm configuration based local search.
Howe, A. & Holte, R. C. (Eds.), Proceedings Twenty-second National Conference Artificial
Intelligence (AAAI07), (pp. 11521157). AAAI Press / MIT Press, Menlo Park, CA, USA.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient dynamic
local search SAT. Hentenryck, P. V. (Ed.), Principles Practice Constraint Programming
CP 2002: Eighth International Conference, volume 2470 Lecture Notes Computer Science, (pp.
233248). Springer Verlag, Berlin, Germany.
Johnson, D. S. (2002). theoreticians guide experimental analysis algorithms. Goldwasser,
M. H., Johnson, D. S., & McGeoch, C. C. (Eds.), Data Structures, Near Neighbor Searches, Methodology: Fifth Sixth DIMACS Implementation Challenges, (pp. 215250). American Mathematical Society, Providence, RI, USA.
Jones, D. R., Schonlau, M., & Welch, W. J. (1998). Efficient global optimization expensive black box
functions. Journal Global Optimization, 13, 455492.
Kask, K. & Dechter, R. (1999). Stochastic local search Bayesian networks. Seventh International
Workshop Artificial Intelligence Statistics (AISTATS99).
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C. P., & Selman, B. (2002). Dynamic restart policies. Dechter,
R., Kearns, M., & Sutton, R. (Eds.), Proceedings Eighteenth National Conference Artificial
Intelligence (AAAI02), (pp. 674681). AAAI Press / MIT Press, Menlo Park, CA, USA.
KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building local search sat solvers components. Proceedings Twenty-first International Joint Conference
Artificial Intelligence (IJCAI09), (pp. 517524).
Le Berre, D. & Simon, L. (2004). Fifty-five solvers Vancouver: SAT 2004 competition. Hoos, H. H.
& Mitchell, D. G. (Eds.), Theory Applications Satisfiability Testing: Proceedings Seventh
International Conference (SAT04), volume 3542 Lecture Notes Computer Science, (pp. 321344).
Springer Verlag.
Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning empirical hardness optimization
problems: case combinatorial auctions. Hentenryck, P. V. (Ed.), Principles Practice
Constraint Programming CP 2002: Eighth International Conference, volume 2470 Lecture Notes
Computer Science, (pp. 556572). Springer Verlag, Berlin, Germany.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards universal test suite combinatorial
auction algorithms. Jhingran, A., Mason, J. M., & Tygar, D. (Eds.), EC 00: Proceedings 2nd
305

fiH UTTER , H OOS , L EYTON -B ROWN & UTZLE

ACM conference Electronic commerce, (pp. 6676)., New York, NY, USA. ACM.
Lourenco, H. R., Martin, O., & Stutzle, T. (2002). Iterated local search. F. Glover & G. Kochenberger
(Eds.), Handbook Metaheuristics (pp. 321353). Kluwer Academic Publishers, Norwell, MA, USA.
Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search classification
function approximation. Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances Neural
Information Processing Systems 7 (NIPS-94), volume 6, (pp. 5966). Morgan Kaufmann Publishers, San
Francisco, CA, USA.
Mengshoel, O. J. (2008). Understanding role noise stochastic local search: Analysis experiments. Artificial Intelligence, 172(8-9), 955990.
Minton, S. (1993). analytic learning system specializing heuristics. Bajcsy, R. (Ed.), Proceedings
Thirteenth International Joint Conference Artificial Intelligence (IJCAI93), (pp. 922929). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Minton, S. (1996). Automatically configuring constraint satisfaction programs: case study. Constraints,
1(1), 140.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: heuristic repair
method constraint-satisfaction scheduling problems. Artificial Intelligence, 58(1), 161205.
Patterson, D. J. & Kautz, H. (2001). Auto-WalkSAT: self-tuning implementation WalkSAT. Electronic
Notes Discrete Mathematics (ENDM), 9.
Ridge, E. & Kudenko, D. (2006). Sequential experiment designs screening tuning parameters
stochastic heuristics. Paquete, L., Chiarandini, M., & Basso, D. (Eds.), Workshop Empirical Methods
Analysis Algorithms Ninth International Conference Parallel Problem Solving
Nature (PPSN), (pp. 2734).
Santner, T. J., Williams, B. J., & Notz, W. I. (2003). Design Analysis Computer Experiments.
Springer Verlag, New York.
Selman, B., Levesque, H. J., & Mitchell, D. (1992). new method solving hard satisfiability problems.
Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings Tenth National Conference Artificial
Intelligence (AAAI92), (pp. 440446). AAAI Press / MIT Press, Menlo Park, CA, USA.
Spall, J. C. (2003). Introduction Stochastic Search Optimization. New York, NY, USA: John Wiley &
Sons, Inc.
Terashima-Marn, H., Ross, P., & Valenzuela-Rendon, M. (1999). Evolution constraint satisfaction strategies examination timetabling. Proceedings Genetic Evolutionary Computation Conference
(GECCO-1999), (pp. 635642). Morgan Kaufmann.
Thachuk, C., Shmygelska, A., & Hoos, H. H. (2007). replica exchange monte carlo algorithm protein
folding hp model. BMC Bioinformatics, 8, 342342.
Tompkins, D. A. D. & Hoos, H. H. (2004). UBCSAT: implementation experimentation environment
SLS algorithms SAT & MAX-SAT. Theory Applications Satisfiability Testing: Proceedings Seventh International Conference (SAT04), volume 3542, (pp. 306320). Springer Verlag,
Berlin, Germany.
Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: portfolio-based algorithm selection
SAT. Journal Artificial Intelligence Research, 32, 565606.
Zarpas, E. (2005). Benchmarking SAT Solvers Bounded Model Checking. Bacchus, F. & Walsh, T.
(Eds.), Theory Applications Satisfiability Testing: Proceedings Eighth International Conference (SAT05), volume 3569 Lecture Notes Computer Science, (pp. 340354). Springer Verlag.

306

fiJournal Artificial Intelligence Research 36 (2009) 513-546

Submitted 08/09; published 12/09

RoxyBot-06: Stochastic Prediction Optimization
TAC Travel
Amy Greenwald

amy@cs.brown.edu

Department Computer Science, Brown University
Providence, RI 02912 USA

Seong Jae Lee

seongjae@u.washington.edu

Computer Science Engineering, University Washington
Seattle, WA 98195 USA

Victor Naroditskiy

vnarodit@cs.brown.edu

Department Computer Science, Brown University
Providence, RI 02912 USA

Abstract
paper, describe autonomous bidding agent, RoxyBot, emerged victorious travel division 2006 Trading Agent Competition photo finish.
high level, design many successful trading agents summarized follows:
(i) price prediction: build model market prices; (ii) optimization: solve
approximately optimal set bids, given model. predict, RoxyBot builds stochastic model market prices simulating simultaneous ascending auctions. optimize,
RoxyBot relies sample average approximation method, stochastic optimization
technique.

1. Introduction
annual Trading Agent Competition (TAC) challenges entrants design build
autonomous agents capable effective trading online travel1 shopping game. first
TAC, held Boston 2000, attracted 16 entrants six countries North America,
Europe, Asia. Excitement generated event led refinement game
rules, continuation regular tournaments increasing levels competition
next six years. Year-by-year, entrants improved designs, developing new ideas
building previously successful techniques. Since TACs inception, lead author
entered successive modifications autonomous trading agent, RoxyBot. paper
reports RoxyBot-06, latest incarnation top scorer TAC-06 tournament.
key feature captured TAC travel game goods highly interdependent
(e.g., flights hotels must coordinated), yet markets goods operate
independently. second important feature TAC agents trade via three different
kinds market mechanisms, presents distinct challenges. Flights traded
posted-price environment, designated party sets price parties
1. four divisions TAC: Travel, Supply Chain Management (SCM), CAT (TAC backwards),
Ad Auctions (AA). paper concerned first; description others, see
papers Arunachalam Sadeh (2005), Cai et al. (2009), Jordan Wellman (2009), respectively.
paper, say TAC, mean TAC Travel.

c
2009
AI Access Foundation. rights reserved.

fiGreenwald, Lee, & Naroditskiy

must take leave. Hotels traded simultaneous ascending auctions, like FCC
spectrum auctions. Entertainment tickets traded continuous double auctions, like
New York Stock Exchange. grappling three mechanisms constructing
agent strategies, participants confronted number interesting problems.
success autonomous trading agent TAC agent often hinges upon
solutions two key problems: (i) price prediction, agent builds model
market prices; (ii) optimization, agent solves approximately
optimal set bids, given model. example, core RoxyBots 2000 architecture (Greenwald & Boyan, 2005) deterministic optimization problem, namely
bid given price predictions form point estimates. spite effectiveness
TAC-00 tournament, weakness 2000 design RoxyBot could explicitly
reason variance within prices. years since 2000, recast key challenges
faced TAC agents several different stochastic bidding problems (see, example,
paper Greenwald & Boyan, 2004), whose solutions exploit price predictions form
distributions. spite perseverance, RoxyBot fared unimpressively tournament
conditions year year, 2006. Half decade laboratory spent searching
bidding heuristics exploit stochastic information reasonable computational expense finally bore fruit, RoxyBot emerged victorious TAC-06. nutshell, secret
RoxyBot-06s success is: (hotel) price prediction simulating simultaneous ascending
auctions, optimization based sample average approximation method. Details
approach subject present article.
Overview paper organized follows. Starting Section 2, summarize
TAC market game. Next, Section 3, present high-level view RoxyBots 2006
architecture. Section 4, describe RoxyBots price prediction techniques flights, hotels, entertainment, turn. Perhaps greatest interest hotel price prediction
method. Following Wellman et al. (2005), predict hotel prices computing approximate competitive equilibrium prices. Only, instead computing prices running
tatonnement process, simulate simultaneous ascending auctions. procedure
simpler implement tatonnement, yet achieves comparable performance, runs
sufficiently fast. Section 5, describe RoxyBots optimization technique: sample average
approximation. argue approach optimal pseudo-auctions, abstract
model auctions. Section 6.1, describe simulation experiments controlled testing environment show combined approachsimultaneous ascending auctions
hotel price prediction sample average approximation bid optimizationperforms
well practice comparison reasonable bidding heuristics. Section 6.2,
detail results TAC-06 tournament, validating success RoxyBot-06s
strategy, reporting statistics shed light bidding strategies participating agents. Finally, Section 7, evaluate collective behavior autonomous
agents TAC finals since 2002. find accuracy competitive equilibrium
calculations varied year year highly dependent particular agent
pool. Still, generally speaking, collective appears moving toward competitive
equilibrium behavior.

514

fiRoxyBot-06

2. TAC Market Game: Brief Summary
section, summarize TAC game. details, see http://www.sics.se/
tac/.
Eight agents play TAC game. simulated travel agent whose task
organize itineraries clients travel TACTown five day (four
night) period. time allotted (nine minutes), agents objective procure
travel goods inexpensively possible, trading fact goods
ultimately compiled feasible trips satisfy client preferences greatest
extent possible. agents know preferences eight clients only,
56.
Travel goods sold simultaneous auctions follows:
Flight tickets sold TACAir dynamic posted-pricing environments.
flights TACTown applicable day. resale flight
tickets agents permitted.
Flight price quotes broadcast TAC server every ten seconds.
Hotel reservations sold TAC seller multi-unit ascending call markets.
Specifically, 16 hotel reservations sold hotel auction 16 highest
bidders 16th highest price. TAC seller runs eight hotel auctions, one per
night-hotel combination (recall travel takes place four night period;
moreover, two hotels: good one bad one). resale hotel
reservations agents permitted. bid withdrawal allowed.
specifically, eight hotel auctions clear minute exactly one auction
closing minutes one eight. (The precise auction close chosen
random, open auctions equally likely selected.) auction
closes, TAC server broadcasts final closing price, informs agent
winnings. others, TAC server reports current ask price, informs
agent hypothetical quantity (HQW).
Agents allocated initial endowment entertainment tickets, trade
among continuous double auctions (CDAs). three entertainment events scheduled day.
Although event auctions clear continuously, price quotes broadcast every
30 seconds.
One primary challenges posed TAC design build autonomous agents
bid effectively interdependent (i.e., complementary substitutable) goods
sold separate markets. Flight tickets hotel reservations complementary
flights useful client without corresponding hotel reservations, vice
versa. Tickets entertainment events (e.g., Boston Red Sox Boston Symphony
Orchestra) substitutable client cannot attend multiple events simultaneously.

515

fiGreenwald, Lee, & Naroditskiy

REPEAT
{start bid interval }
0. Download current prices winnings server
1. predict: build stochastic models
a. flights: Bayesian updating/learning
b. hotels: simultaneous ascending auctions
c. entertainment: sample historical data
2. optimize: sample average approximation
3. Upload current bids server
(three separate threads)
{end bid interval }
game

Table 1: high-level view RoxyBot-06s architecture.

3. RoxyBot-06s Architecture: High-Level View
approach problem bidding interdependent goods separate TAC
markets, adopt simplifying assumptions. Rather tackle game-theoretic
problem characterizing strategic equilibria, focus single agents (decision-theoretic)
problem optimizing bidding behavior, assuming agents strategies
fixed. addition, assume environment modeled terms agents
predictions market clearing prices. prices serve summarize relevant information hidden agents bidding strategies. two assumptionsfixed otheragent behaviors market information encapsulated pricessupport modular design RoxyBot-06 many successful TAC agents, consists two key stages:
(i) price prediction; (ii) optimization.
optimization problem faced TAC agents dynamic one incorporates
aspects sequentiality well simultaneity auctions. markets operate simultaneously, addition, prices discovered incrementally time. principle,
clairvoyant agentone knowledge future clearing pricescould justifiably employ
open-loop strategy: could solve TAC optimization problem start
game place bids accordingly, never reconsidering decisions. practical alternative (and usual approach taken TAC2 ), incorporate agents
architecture closed loop, bidding cycle, enabling agent condition behavior
evolution prices. price information revealed, agent improves price
predictions, reoptimizes bidding decisions, repeatedly.
One distinguishing feature RoxyBot-06 builds stochastic models market
clearing prices, rather predicting clearing prices point estimates. Given stochastic
price predictions, stochastic optimization lies heart RoxyBot-06. Assuming time
2. exception livingagents (Fritschi & Dorer, 2002), winner TAC 2001.

516

fiRoxyBot-06

discretized stages, bid intervals, iteration bidding cycle, RoxyBot-06
faces n-stage stochastic optimization problem, n number stages remaining
game. key input optimization problem sequence n 1 stochastic
models future prices, one joint probability distribution goods conditioned
past prices past hotel closings. solution optimization problem,
output iteration bidding cycle, vector bids, one per good (or auction).
Table 1 presents high-level view RoxyBot-06s architecture, emphasizing bidding
cycle. start bid interval, current prices winnings downloaded
TAC server. Next, key prediction optimization routines run. prediction module, stochastic models flight, hotel, entertainment prices built.
optimization module, bids constructed approximate solution n-stage
stochastic optimization problem. Prior end bid interval, agents bids
uploaded TAC server using three separate threads: (i) flight thread bids
flight price near predicted minimum; (ii) hotel thread bids open
hotels moments end minute; (iii) entertainment thread
places bids immediately.
discuss details RoxyBot-06s price prediction module first, optimization
module second.

4. Price Prediction
section, describe RoxyBot-06 builds stochastic models flight, hotel,
event prices. model discrete probability distribution, represented set scenarios. scenario vector future pricesprices goods bought
sold current stage. flights, price prediction model stochastic:
future buy price simply RoxyBot-06s prediction expected minimum price
current stage. hotels, future buy prices predicted Monte Carlo simulations
simultaneous ascending auctions approximate competitive equilibrium prices.
current buy prices hotels. entertainment, RoxyBot-06 predicts future buy
sell prices based historical data. Details price prediction methods focus
section.
4.1 Flights
Efforts deliberate flight purchasing start understanding TAC model
flight price evolution.
4.1.1 TAC Flight Prices Stochastic Process
Flight prices follow biased random walk. initialized uniformly range
[250, 400], constrained remain range [150, 800]. start TAC
game instance, bound z final perturbation value selected flight.
bounds revealed agents. revealed agents sequence
random flight prices. Every ten seconds, TACAir perturbs price flight
random value depends hidden parameter z current time follows:
given constants c, R > 0, (intermediate) bound perturbation value

517

fiGreenwald, Lee, & Naroditskiy

linear function t:


(z c)
(1)

perturbation value time drawn uniformly one following ranges (see
Algorithm 1):
x(t, z) = c +

U [c, x(t, z)], x(t, z) > 0
U [c, +c], x(t, z) = 0
U [x(t, z), +c], x(t, z) < 0
Observe expected perturbation value case simply average
corresponding upper lower bounds. particular,
x(t, z) > c, expected perturbation positive;
x(t, z) (0, c), expected perturbation negative;
x(t, z) (c, 0), expected perturbation positive;
otherwise, x(t, z) {c, 0, c}, expected perturbation zero.
Moreover, using Equation 1, compute expected perturbation value conditioned
z:
z [0, c], x(t, z) [0, c], prices expected increase;
z [c, c + d], x(t, z) [c, c + d], prices expected decrease;
z [c, 0], x(t, z) [c, c], prices expected increase
cT
.
expected decrease cz

cT
cz

TAC parameters set follows: c = 10, = 30, = 540, z uniformly
distributed range [c, d]. Based discussion, note following:
given information z, TAC flight prices expected increase (i.e.,
expected perturbation positive); however, conditioned z, TAC flight prices may
increase decrease (i.e., expected perturbation positive negative).
4.1.2 RoxyBot-06s Flight Prices Prediction Method
Although value hidden parameter z never revealed agents, recall
agents observe sample flight prices, say y1 , . . . , yt , depend value.
information used model probability distribution Pt [z] P [z | y1 , . . . , yt ].
probability distribution estimated using Bayesian updating. RoxyBot06, agents Walverine (Cheng et al., 2005) Mertacor (Toulis et al., 2006) took approach.
Walverine uses Bayesian updating compute next expected price perturbation
compares value threshold, postponing flight purchases prices expected
increase threshold. Mertacor uses Bayesian updating estimate
time flight prices reach minimum value. RoxyBot uses Bayesian updating
compute expected minimum price, describe.
518

fiRoxyBot-06

Algorithm 1 getRange(c, t, z)
compute x(t, z) {Equation 1}
x(t, z) > 0
= c; b = x(t, z)
else x(t, z) < 0
= x(t, z); b = +c
else
= c; b = +c
end
return [a, b] {range}
RoxyBot-06s implementation Bayesian updating presented Algorithm 2. Letting

Q0 [z] =

1
c+d

= P [z], algorithm estimates Pt+1 [z] = P [z | y1 , . . . , yt+1 ] usual:
P [y1 , . . . , yt | z]P [z]



z P [y1 , . . . , yt | z ]P [z ] dz



P [z | y1 , . . . , yt ] = P
P [y1 , . . . , yt | z] =
=




i=1



(2)

P [yi | y1 , . . . , yi1 , z]

(3)

P [yi | z]

(4)

i=1

Equation 4 follows fact future observations independent past observations; observations depend hidden parameter z.
thing left explain set values P [yi | z], = 1, . . . , t.
described pseudocode, done follows: yt+1 within appropriate range
time, probability set uniformly within bounds range; otherwise,
set 0. Presumably, Walverines Mertacors implementations Bayesian updating
different one.3 However, alluded above, agents make
use ensuing estimated probability distributions differ.
RoxyBot-06 predicts flights price expected minimum price. value
computed follows (see Algorithm 3): possible value hidden parameter z,
RoxyBot simulates expected random walk, selects minimum price along walk,
outputs prediction expectation minima, averaging according
Pt [z]. call random walk expected, since perturbation value expectation
(i.e., = ba
2 ) instead sample (i.e., U [a, b]). carrying computation,
RoxyBot generates flight price predictions point estimates. implicit decision
make RoxyBot-06s hotel event price predictions stochastic made based
intuitive sense time vs. accuracy tradeoffs RoxyBots optimization module,
hence warrants study.
3. provide details here, corresponding details agents seem publicly
available.

519

fiGreenwald, Lee, & Naroditskiy

Algorithm 2 Flight Prediction(c, d, t, yt+1 , Qt )
z {c, c + 1, . . . , d}
[a, b] = getRange(c, t, z)
yt+1 [a, b]
1
P [yt+1 | z] = ba
else
P [yt+1 | z] = 0
end
Qt+1 [z] = P [yt+1 | z]Qt [z]
end for{update probabilities}
z {c, c + 1, . . . , d}
t+1 [z]
Pt+1 [z] = P Q


z Qt+1 [z ] dz
end for{normalize probabilities}
return Pt+1 {probabilities}
Algorithm 3 Expected Minimum Price(c, t, , pt , Pt )
z R
min[z] = +
= + 1, . . . ,
[a, b] = getRange(c, , z)
= ba
2 {expected perturbation}
p = p 1 + {perturb price}
p = max(150, min(800, p ))
p < min[z]
min[z] = p
end
end
end forP
return z Pt [z] min[z] dz
4.2 Hotels
competitive market individuals effect prices negligible, equilibrium prices prices supply equals demand, assuming producers profitmaximizing consumers utility-maximizing. RoxyBot-06 predicts hotel prices
simulating simultaneous ascending auctions (SimAA) (Cramton, 2006), attempt
approximate competitive equilibrium (CE) prices. approach inspired Walverines (Cheng et al., 2005), tatonnement method (Walras, 1874) used
purpose.
4.2.1 Simultaneous Ascending Auctions
Let p~ denote vector prices. ~y (~
p) denotes cumulative supply producers,
~x(~
p) denotes cumulative demand consumers, ~z(~
p) = ~x(~
p)~y (~
p) denotes

520

fiRoxyBot-06

excess demand market. tatonnement process adjusts price vector iteration
n + 1, given price vector iteration n adjustment rate n follows: p~n+1 =
p~n + n~z(~
pn ). SimAA adjusts price vector follows: p~n+1 = p~n + max{~z(~
pn ), 0},
fixed value . processes continue excess demand non-positive:
i.e., supply exceeds demand.
Although competitive equilibrium prices guaranteed exist TAC markets (Cheng et al., 2003), SimAA adjustment process, still guaranteed converge:
prices increase, demand decreases supply increases; hence, supply eventually exceeds demand. parameter SimAA method magnitude price
adjustment. smaller value, accurate approximation (assuming CE
prices exist), value chosen lowest value time permits.
tatonnement process, hand, difficult apply
guaranteed converge. Walverine team dealt convergence issue decaying
initial value . However, careful optimization required ensure convergence
reasonable solution reasonable amount time. fact, Walverine found helpful
set initial prices certain non-zero values. complexity present using
simultaneous ascending auctions approximate competitive equilibrium prices.
4.2.2 Prediction Quality
TAC, cumulative supply fixed. Hence, key computing excess demand
compute cumulative demand. TAC agent knows preferences clients,
must estimate demand others. Walverine computes single hotel price prediction (a
point estimate) considering clients demands together 56 expected
clients. Briefly, utility expected client average across travel dates hotel
types augmented fixed entertainment bonuses favor longer trips (see paper
Cheng et al., 2005, details). contrast, RoxyBot-06 builds stochastic model
hotel prices consisting scenarios considering clients demands together
random samples 56 clients. (random expected) clients demand simply
quantity good optimal package, given current prices. cumulative demand
sum total clients individual demands.
Figure 1, present two scatter plots depict quality various hotel price
predictions beginning TAC 2002 final games. price predictions evaluated using two metrics: Euclidean distance expected value perfect prediction
(EVPP). Euclidean distance measure difference two vectors, case
actual predicted prices. value perfect prediction (VPP) client
difference surplus (value preferred package less price) based actual
predicted prices. EVPP VPP averaged distribution client preferences.4
left, plot predictions generated using competitive equilibrium ap1
proximation methods, tatonnement SimAA, fixed = 24
, making expected,
random, exact predictions. exact predictions computed based actual
clients games, client distribution; hence, serve lower bound
performance techniques data set. metrics,
expected random, SimAAs predictions outperform tatonnements.
4. See paper Wellman et al. (2004) details.

521

fiGreenwald, Lee, & Naroditskiy

44

70
livingagents
PackaTAC
Southampton
RoxyBot UMBCTAC
whitebear
SICS_baseline
ATTac

Expected Value Perfect Prediction

Expected Value Perfect Prediction

65
tatonnement, random

42

SimAA, random
40

tatonnement, expected
38

36

SimAA, expected

SimAA, exact
tatonnement, exact

60
55
50
harami

cuhk

45
kavayaH

SimAA, random

40
Walverine
35

34
180

190

200

210

220

30
180

230

Euclidean Distance

ATTac01
190

200

210

220

230

240

250

260

Euclidean Distance

Figure 1: EVPP Euclidean Distance CE price prediction methods (tatonnement
1
; expected, random, exact) TAC 2002 agents
SimAA = 24
predictions 2002 finals (60 games). plot left shows SimAAs
predictions better tatonnements expecteds better
randoms. RoxyBot-06s method hotel price prediction (SimAA, Random)
plotted right. Note differences scales two plots.

Since fixed, tatonnement guaranteed converge condition,
outcome entirely surprising. interesting, though, SimAA expected
performs comparably Walverine (see right plot).5 interesting SimAA
fewer parameter settings tatonnementonly single value compared
initial value together decay scheduleand moreover, optimize
parameter setting. Walverines parameter settings, hand, highly
optimized.
interpret prediction generated using randomly sampled clients sample
scenario, set scenarios represents draws probability distribution
CE prices. corresponding vector predicted prices evaluated actually
average multiple (40) predictions; is, evaluate estimate mean
probability distribution. predictions generated using sets random clients
good predictions expected clients (see Figure 1 left), although
40 sets random clients, results might improve. Still, predictions random
clients comprise RoxyBot-06s stochastic model hotel prices, key bidding
strategy. Moreover, using random clients helps RoxyBot-06 make better interim predictions
later game explain next.
4.2.3 Prediction Quality Time: Interim Price Prediction
graphs depicted Figure 1 pertain hotel price predictions made beginning
game, hotel auctions open. CE computations, prices initialized
0. hotel auctions close, RoxyBot-06 updates predicted prices hotel auctions
5. exception RoxyBot-06 data point (i.e., SimAA random), plot produced
Walverine team (Wellman et al., 2004).

522

fi22

140

tatonnement, expected clients
SimAA, expected clients
tatonnement, random clients
SimAA, random clients
tatonnement, random clients, distribution
SimAA, random clients, distribution

20
18
16

tatonnement, expected clients
SimAA, expected clients
tatonnement, random clients
SimAA, random clients
tatonnement, random clients, distribution
SimAA, random clients, distribution

120
Euclidean Distance per Hotel

Expected Value Perfect Prediction per Hotel

RoxyBot-06

14
12
10
8
6
4

100
80
60
40
20

2
0

0
0

1

2

3

4

5

6

7

0

Minute

1

2

3

4

5

6

7

Minute

Figure 2: EVPP Euclidean Distance TAC 2006 finals (165 games) CE price
prediction methods without distribution game progresses. Distribution improves prediction quality.

remain open. experimented two ways constructing interim price predictions.
first initialize lower bound prices hotel markets closing
(for closed auctions) current ask (for open auctions) prices computing competitive
equilibrium prices.6 second differs treatment closed auctions: simulate
process distributing goods closed auctions clients want most,
exclude closed markets (i.e., fix prices ) computations
competitive equilibrium prices.
Regarding second methodthe distribution methodwe determine distribute goods computing competitive equilibrium prices again. explained Algorithm 4, hotels (in open closed auctions) distributed random clients
determining willing pay competitive equilibrium prices what.
immediately obvious distribute goods expected clients; hence, enhanced
prediction methods random clients distribution.
Figure 2, depicts prediction quality time, shows prediction methods
enhanced distribution better predictions obtained merely initializing
prices closed hotel auctions closing prices. Hotels close early tend
sell less hotels close late; hence, prediction quality method
makes decent initial predictions bound deteriorate predictions remain relatively
constant throughout game.
4.2.4 Run Time
Table 2 shows run times CE prediction methods TAC 2002 (60 games)
TAC 2006 (165 games) finals data set minute 0, well run times
6. first blush, may seem sensible fix prices closed hotels closing prices, rather
merely lower bound (i.e., allow increase). hotel closed artificially low
price, however, price permitted increase, predicted prices hotels
complementing hotel question would artificially high.

523

fiGreenwald, Lee, & Naroditskiy

Algorithm 4 Distribute
1: hotel auctions h
2:
initialize price 0
3:
initialize supply 16
4: end
5: compute competitive equilibrium prices {Tatonnement SimAA}
6: closed hotel auctions h
7:
distribute units h demand computed competitive equilibrium prices
8:
distribute leftover units h uniformly random
9: end
minutes 17 TAC 2006 finals data set. numbers table convey
SimAAs run time, even distribution, reasonable. example, minute 0,
SimAA sample takes order 0.1 seconds. minutes 1-7, method without
distribution runs even faster. speed increase occurs CE prices bounded
current ask prices maximum price client willing pay
hotel, current ask prices increase time, correspondingly reducing size
search space. SimAA sample distribution minutes 1-7 takes twice long SimAA
sample without distribution minute 0 time takes distribute goods,
run time still (roughly) 0.2 seconds. implementation tatonnement runs
slowly fixed instead optimizing tradeoff convergence rate
accuracy, process converge, instead ran maximum number
iterations (10,000). summary, SimAA simpler tatonnement implement, yet
performs comparably optimized version tatonnement (i.e., Walverine), runs
sufficiently fast.

2002, minute 0
2006, minute 0
2006, average 17

Exp Tat
2213
2252
2248

Exp SimAA
507
508
347

Sam Tat
1345
1105
1138

Sam SimAA
157
130
97

Dist Tat

1111
2249

Dist SimAA

128
212

Table 2: Run times CE price prediction methods, milliseconds. Experiments
run AMD Athlon(tm) 64 bit 3800+ dual core processors 2M RAM.

4.2.5 Summary
simulation methods discussed sectionthe tatonnement process simultaneous ascending auctionswere employed predict hotel prices only. (In simulations,
flight prices fixed expected minima, entertainment prices fixed 80.)
principle, competitive equilibrium (CE) prices could serve predictions TAC markets. However, CE prices unlikely good predictors flight prices, since flight
prices determined exogenously. regard entertainment tickets, CE prices might
524

fiRoxyBot-06

predictive power; however, incorporating entertainment tickets tatonnement
SimAA calculations would expensive. (In simulations, following Wellman et al., 2004, client utilities simply augmented fixed entertainment bonuses
favor longer trips.) Nonetheless, future work, could interest evaluate
success related methods predicting CDA clearing prices.
Finally, note refer methods computing excess demand clientbased compute demands client individual basis. contrast,
one could employ agent-based method, whereby demands agents, clients,
would calculated. Determining agents demands involves solving so-called completion, deterministic (prices known) optimization problem heart RoxyBot-00s
architecture (Greenwald & Boyan, 2005). TAC completion NP-hard, agent-based
method predicting hotel prices expensive included RoxyBot-06s inner
loop. designing RoxyBot-06, reasoned architecture based stochastic pricing model generated using client-based method randomly sampled clients would
outperform one based point estimate pricing model generated using agent-based
method form expected clients, verify reasoning empirically.
4.3 Entertainment
bid interval, RoxyBot-06 predicts current future buy sell prices tickets
entertainment events. price predictions optimistic: agent assumes
buy (or sell) goods least (or most) expensive prices expects see
end game. specifically, current price prediction best predicted
price current bid interval.
RoxyBot-06s estimates entertainment ticket prices based historical data
past 40 games. generate scenario, sample game drawn random
collection, sequences entertainment bid, ask, transaction prices extracted.
Given history, auction a, let trade ai denote price last trade
time transacted; value initialized 200 buying 0 selling.
addition, let bid ai denote bid price time i, let ask ai denote ask price time
i.
RoxyBot-06 predicts future buy price auction time follows:
future buy =

min

i=t+1,...,T

min{trade ai , ask ai }

(5)

words, future buy price time = + 1, . . . , minimum ask price
time recent trade price. future buy price time minimum
across future buy prices later times. future sell price time predicted
analogously:
(6)
future sell = max max{trade ai , bid ai }
i=t+1,...,T

Arguably, RoxyBot-06s entertainment predictions made simplest possible way:
past data future predictions. likely one could improve upon naive approach
using generalization technique capable learning distribution data,
sampling learned distribution.

525

fiGreenwald, Lee, & Naroditskiy

4.4 Summary
section, described RoxyBot-06s price prediction methods. key ideas,
may transferable beyond TAC, least TAC agents, follows:
1. RoxyBot makes stochastic price predictions. generating set so-called
scenarios, scenario vector future prices.
2. flight, RoxyBot uses Bayesian updating predict expected minimum price.
3. hotels, RoxyBot-uses method inspired Walverines: approximates competitive
equilibrium prices simulating simultaneous ascending auctions, rather
usual tatonnement process.

5. Optimization
Next, characterize RoxyBot-06s optimization routine. (i) stochastic, (ii) global,
(iii) dynamic. takes input stochastic price predictions; considers flight, hotel,
entertainment bidding decisions unison; simultaneously reasons bids
placed current future stages game.
5.1 Abstract Auction Model
Recall treatment bidding decision-theoretic, rather game-theoretic.
particular, focus single agents problem optimizing bidding behavior, assuming agents strategies fixed. keeping basic agent architecture,
assume environment modeled terms agents predictions
market clearing prices. introduce term pseudo-auction refer market
mechanism defined two assumptionsfixed other-agent behaviors market information encapsulated prices. optimization problem RoxyBot solves one
bidding pseudo-auctions, (true) auctions. section, formally develop
abstract auction model relate TAC auctions; next, define propose
heuristics solve various pseudo-auction bidding problems.
5.1.1 Basic Formalism
section, formalize basic concepts needed precisely formulate bidding uncertainty optimization problem, including: packagessets goods, possibly
multiple units each; function describes much agent values package; pricelinesdata structures store prices unit good;
bidspairs vectors corresponding buy sell offers.
Packages Let G denote ordered set n distinct goods let N Nn represent
multiset goods marketplace, Ng denoting number units
good g G. package collection goods, is, submultiset N . write
N whenever Mg Ng g G.
instructive interpret notation TAC domain. flights, hotel rooms,
entertainment events auction TAC comprise ordered set 28 distinct

526

fiRoxyBot-06

goods. principle, multiset goods TAC marketplace is:
N TAC = h, . . . , , 16, . . . , 16, 8, . . . , 8i N28
| {z } | {z } | {z }
8 flights

8 hotels

12 events

practice, however, since agent works satisfy preferences eight clients,
suffices consider multiset goods:
N TAC8 = h8 . . . , 8, 8, . . . , 8, 8, . . . , 8i N TAC
| {z } | {z } | {z }
8 flights

8 hotels

12 events

trip corresponds package, specifically N TAC8 satisfies TAC
feasibility constraints.
Given A, B N , rely two basic operations, , defined follows:
g G,
(A B)g Ag + Bg
(A B)g Ag Bg
example, G = {, , } N = h1, 2, 3i, = h0, 1, 2i N B = h1, 1, 1i
N . Moreover, (A B) = 1, (A B) = 2, (A B) = 3; (A B) = 1,
(A B) = 0, (A B) = 1.
Value Let N denote set submultisets N : i.e., packages comprised goods
N . denote v : N R function describes value bidding agent attributes
viable package.
TAC, agents objective compile packages = 8 individual clients.
such, agents value function takes special form. client c characterized
value function vc : N R, agents value collection packages sum
~ = (X1 , . . . , Xm ),
clients respective values packages: given vector packages X
~ =
v(X)


X

vc (Xc ).

(7)

c=1

N

Pricelines buyer priceline good g vector p~g R+g , kth component,
pgk , stores marginal cost agent acquiring kth unit good g. example,
agent currently holds four units good g, four additional units g
available costs $25, $40, $65, $100, corresponding buyer priceline (a
vector length 8) given p~g = h0, 0, 0, 0, 25, 40, 65, 100i. leading zeros indicate
four goods agent holds may acquired cost.
assume buyer pricelines nondecreasing. Note assumption WLOG,
since optimizing agent buys cheaper goods expensive ones.
Given set buyer pricelines P = {~
pg | g G}, define costs additively, is,
cost goods multiset N given by:
g,

Costg (Y, P ) =

Yg
X

pgk ,

X

Costg (Y, P ).

k=1

Cost(Y, P ) =

gG

527

(8)

fiGreenwald, Lee, & Naroditskiy

N

seller priceline good g vector ~g R+g . Much like buyer priceline, kth
component seller priceline g stores marginal revenue agent could earn
kth unit sells. example, market demands four units good g,
sold prices $20, $15, $10, $5, corresponding seller priceline given
~g = h20, 15, 10, 5, 0, 0, 0, 0i. Analogously buyer pricelines, tail zero revenues
indicates market demands four units.
assume seller pricelines nonincreasing. Note assumption WLOG,
since optimizing agent sells expensive goods cheaper ones.
Given set seller pricelines = {~g | g G}, define revenue additively, is,
revenue associated multiset Z N given by:
g,

Revenueg (Z, ) =

Zg
X

gk ,

X

Revenueg (Z, ).

(9)

k=1

Revenue(Z, ) =

(10)

gG

priceline constant, say prices linear. refer constant value
unit price. linear prices, cost acquiring k units good g k times
unit price good g.
Bids agent submits bid expressing offers buy sell various units goods
marketplace. divide two components h~b, ~ai, good g bid
consists buy offer, ~bg = hbg1 , . . . , bgNg i, sell offer, ~ag = hag1 , . . . , agNg i. bid
price bgk R+ (resp. agk R+ ) represents offer buy (sell) kth unit good g
price.
definition, agent cannot buy (sell) kth unit unless also buys (sells) units
1, . . . , k 1. accommodate fact, impose following constraint: Buy offers must
nonincreasing k, sell offers nondecreasing. addition, agent may offer
sell good less price willing buy good: i.e., bg1 < ag1 .
Otherwise, would simultaneously buy sell good g. refer restrictions
bid monotonicity constraints.
5.1.2 Pseudo-Auction Rules
Equipped formalism, specify rules govern pseudo-auctions.
true auction, outcome pseudo-auction dictates quantity good
exchange, prices, conditional agents bid. quantity issue resolved
winner determination rule whereas price issue resolved payment rule.
Definition 5.1 [Pseudo-Auction Winner Determination Rule] Given buyer seller pricelines P , bid = h~b, ~ai, agent buys multiset goods Buy(, P ) sells
multiset goods Sell(, ),
Buyg (, P ) = max k bgk pgk
k

Sellg (, ) = max k agk gk
k

528

fiRoxyBot-06

Note monotonicity restrictions bids ensure agents offer better
equal price every unit exchanges, agent simultaneously
buy sell good.
least two alternative payment rules agent may face. first-price
pseudo-auction, agent pays bid price (for buy offers, paid bid price sell
offers) good wins. second-price pseudo-auction, agent pays (or paid)
prevailing prices, specified realized buyer seller pricelines. terminology
derives analogy standard first- second-price sealed bid auctions (Krishna,
2002; Vickrey, 1961). mechanisms, high bidder single item pays bid (the
first price), highest losing bid (the second price), respectively. salient property
first-price pseudo-auctions, price set bid winner, whereas
second-price pseudo-auctions agents bid price determines whether wins
price pays.
paper, focus second-price model. is, basic problem definitions
presume second-price auctions; however, bidding heuristics tailored
case. true auctions, adopting second-price model pseudo-auctions simplifies
problem bidder. also provides reasonable approximation situation faced
TAC agents, argue:
TAC entertainment auctions, agents submit bids (i.e., buy sell offers)
form specified above. interpret agents buyer seller pricelines
current order book (not including agents bid), agents immediate winnings determined winner determination rule, payments
according second-price rule (i.e., order-book prices prevail).
TAC hotel auctions, buy bids allowed. Assuming order
book reflects outstanding bids agents own, accurate buyer
priceline would indicate agent win k units good paysfor
k unitsa price (17 k)th existing (other-agent) offer. actual
price pays 16th-highest unit offer (including offer). Since
agents bid may affect price,7 situation lies first-
second-price characterizations pseudo-auctions described above.
TAC flight auctions, agents may buy number units posted price.
situation given time modeled exactly second-price pseudo-auction
abstraction.
5.2 Bidding Problems
ready discuss optimization module repeatedly employed RoxyBot-06
within bidding cycle construct bids. key bidding decisions are: goods
bid on, price, when?
7. two ways. First, agent may submit 16th-highest unit offer, case sets
price. Second, bids multiple units, number wins determines price-setting unit,
thus affecting price winning units. Note second effect would present even
auction cleared 17th-highest price.

529

fiGreenwald, Lee, & Naroditskiy

Although RoxyBot technically faces n-stage stochastic optimization problem, solves
problem collapsing n stages two relevant stages, current
future, necessitating one stochastic model future prices (current prices known).
simplification achieved ignoring potentially useful information hotel
auctions close one one random, unspecified order, instead operating (like
TAC agents) assumption hotel auctions close end current
stage. Hence, one model hotel prices: stochastic model future prices.
Moreover, pressing decisions regarding hotels goods bid
price. need reason timing hotel bid placement.
contrast, since flight entertainment auctions clear continuously, trading agent
reason relevant tradeoffs timing placement bids goods.
Still, assumption hotel auctions close end current stage,
future stages, hotel prices, hence hotel winnings, known, remaining
decisions flight entertainment tickets buy. rational agent time
bids markets capitalize best prices. (The best prices minima
buying maxima selling.) Hence, suffices agents model future prices
markets predict best prices (conditioned current prices). is,
suffices consider one stochastic pricing model. information necessary.
established suffices RoxyBot pose solve two-stage, rather
n-stage, stochastic optimization problem, proceed define abstract series
problems designed capture essence bidding uncertainty
TAC-like hybrid markets incorporate aspects simultaneous sequential, one-shot
continuously-clearing, auctions. specifically, formulate problems twostage stochastic programs integer recourse (see book Birge & Louveaux, 1997,
introduction stochastic programming).
two-stage stochastic program, two decision-making stages, hence two
sets variables: first- second-stage variables. objective maximize sum
first-stage objectives (which depend first-stage variables) expected
value ensuing second-stage objectives (which depend first- secondstage variables). objective value second stage called recourse value,
second-stage variables integer-valued, stochastic program said
integer recourse.
high-level, bidding problem formulated two-stage stochastic program
follows: first stage, current prices known future prices uncertain,
bids selected; second stage, uncertainty resolved, goods exchanged.
objective maximize expected value second-stage objective, namely
sum inherent value final holdings profits earned, less first-stage costs.
Since second stage involves integer-valued decisions (the bidder decides goods
buy sell known prices), bidding problem one integer recourse.
section, formulate series bidding problems two-stage stochastic programs integer recourse, one tailored different type auction mechanism,
illustrating different type bidding decision. mechanisms study, inspired
TAC, one-shot continuously-clearing variants second-price pseudo-auctions.
former, bids placed first stage; latter, opportunity

530

fiRoxyBot-06

recourse. Ultimately, combine decision problems one unified problem
captures mean bidding uncertainty.
formal problem statements, rely following notation:
Variables:
Q1 multiset goods buy
Q2 multiset goods buy later
R1 multiset goods sell
R2 multiset goods sell later
Constants:
P 1 set current buyer pricelines
P 2 set future buyer pricelines
1 set current seller pricelines
2 set future seller pricelines
Note P 1 1 always known, whereas P 2 2 uncertain first stage
uncertainty resolved second stage.
Flight Bidding Problem agents task bidding flight auctions decide
many flights buy current prices later lowest future prices, given (known)
current prices stochastic model future prices. Although TAC units
flight sell price one time, state flight bidding problem
generally: allow different prices different units flight.
Definition 5.2 [Continuously-Clearing, Buying] Given set current buyer pricelines P 1
probability distribution f future buyer pricelines P 2 ,
FLT(f ) = max
EP 2 f
1
n
Q Z




max
v(Q1 Q2 ) Cost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 )
2
n

Q Z



(11)

Note two cost terms referring future pricelines (Cost(, P 2 )). first
terms adds total cost goods bought first second stages.
second term subtracts cost goods bought first stage. construction
ensures that, agent buys k units good now, later purchases good incur
charges units (k + 1, k + 2, ...) goods future priceline.
Entertainment Bidding Problem Abstractly, entertainment buying problem
flight bidding problem. agent must decide many entertainment tickets
buy current prices later lowest future prices. entertainment selling
problem opposite buying problem. agent must decide many tickets
sell current prices later highest future prices.

531

fiGreenwald, Lee, & Naroditskiy

Definition 5.3 [Continuously-Clearing, Buying Selling] Given set current buyer
seller pricelines (P, )1 probability distribution f future buyer seller
pricelines (P, )2 ,

ENT(f ) = max E(P,)2 f
max v((Q1 Q2 ) (R1 R2 ))
Q1 ,R1 Zn
Q2 ,R2 Zn

Cost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 )

+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2 ) Revenue(R1 , 2 )
(12)

subject Q1 R1 Q1 Q2 R1 R2 , (P, )2 .

constraints ensure agent sell units good buys.
Hotel Bidding Problem Hotel auctions close fixed times, unknown order.
Hence, iteration agents bidding cycle, one-shot auctions approximate
auctions well. Unlike continuous setup, decisions made
first second stages, one-shot setup, bids placed first stage;
second stage, winnings determined evaluated.
Definition 5.4 [One-Shot, Buying] Given probability distribution f future buyer
pricelines P 2 ,


(13)
HOT(f ) = max EP 2 f v(Buy( 1 , P 2 )) Cost(Buy( 1 , P 2 ), P 2 )
1 =h~b,0i

Hotel Bidding Problem, Selling Although possible agents sell
TAC hotel auctions, one could imagine analogous auction setup possible
sell goods well buy them.
Definition 5.5 [One-Shot, Buying Selling] Given probability distribution f
future buyer seller pricelines (P, )2 ,


max E(P,)2 f v(Buy( 1 , P 2 ) Sell( 1 , 2 )) Cost(Buy( 1 , P 2 ), P 2 ) + Revenue(Sell( 1 , 2 ), 2 )

1 =h~b,~
ai

(14)

1

2

1

2

2

subject Buy( , P ) Sell( , ), (P, ) .

Bidding Problem Finally, present (a slight generalization of) TAC bidding problem combining four previous stochastic optimization problems one. abstract
problem models bidding buy sell goods via continuously-clearing one-shot
second-price pseudo-auctions, follows:
Definition 5.6 [Bidding Uncertainty] Given set current buyer seller pricelines (P, )1 probability distribution f future buyer seller pricelines (P, )2 ,
BID(f ) =
max

Q1 ,R1 Zn , 1 =h~b,~
ai

E(P,)2 f



max

Q2 ,R2 Zn

v((Q1 Q2 ) (R1 R2 ) Buy( 1 , P 2 ) Sell( 1 , P 2 ))


Cost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 ) + Cost(Buy( 1 , P 2 ), P 2 )


+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2 ) Revenue(R1 , 2 ) + Revenue(Sell( 1 , 2 ), 2 )
(15)
532

fiRoxyBot-06

subject Q1 R1 Q1 Q2 R1 R2 Buy( 1 , P 2 ) Sell( 1 , 2 ), (P, )2 .

again, bidding problem (i) stochastic: takes input stochastic model
future prices; (ii) global: seamlessly integrates flight, hotel, entertainment bidding
decisions; (iii) dynamic: facilitates simultaneous reasoning current future
stages game.
Next, describe various heuristic approaches solving problem bidding
uncertainty.
5.3 Bidding Heuristics
section, discuss two heuristic solutions bidding problem: specifically,
expected value method (EVM), approach collapses stochastic information,
sample average approximation (SAA), approach exploits stochastic information
characterizes RoxyBot-06.
5.3.1 Expected Value Method
expected value method (Birge & Louveaux, 1997) standard way approximating
solution stochastic optimization problem. First, given distribution collapsed
point estimate (e.g., mean); then, solution corresponding deterministic optimization problem output approximate solution original stochastic
optimization problem. Applying idea problem bidding uncertainty
yields:
Definition 5.7 [Expected Value Method] Given probability distribution f buyer
seller pricelines, expected values P 2 2 , respectively,
BID EVM(P 2 , 2 ) =
max

Q1 ,R1 Zn , 1 =h~b,~
ai,Q2 ,R2 Zn

v((Q1 Q2 ) (R1 R2 ) (Buy( 1 , P 2 ) Sell( 1 , P 2 ))


Cost(Q1 , P 1 ) + Cost(Q1 Q2 , P 2 ) Cost(Q1 , P 2 ) + Cost(Buy( 1 , P 2 ), P 2 )


+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2 ) Revenue(R1 , 2 ) + Revenue(Sell( 1 , 2 ), 2 )
(16)
subject Q1 R1 Q1 Q2 R1 R2 .

practice, without full knowledge distribution f , cannot implement
expected value method; particular, cannot compute P 2 2 cannot solve
BID EVM(P 2 , 2 ) exactly. can, however, solve approximation problem
expected buyer seller pricelines P 2 2 replaced average scenario
(P 2 , 2 ) (i.e., average buyer seller pricelines), defined follows:
P 2 =


1X 2
Pi ,


2 =

i=1


1X 2
.

i=1

533

fiGreenwald, Lee, & Naroditskiy

Algorithm 5 EVM(G, N, f, S)
1: sample scenarios (P, )21 , . . . , (P, )2S f
P

PS

2
2,
2: BID EVM

P
i=1
i=1
3: return
5.3.2 Sample Average Approximation
Like expected value method, sample average approximation intuitive way approximating solution stochastic optimization problem. idea simple: (i) generate
set sample scenarios, (ii) solve approximation problem incorporates
sample scenarios. Applying SAA heuristic (see Algorithm 6) involves solving
following approximation bidding problem:
Definition 5.8 [Sample Average Approximation] Given set scenarios,
(P, )21 , . . . , (P, )2S f ,
BID SAA((P, )21 , . . . , (P, )2S ) =
max


X

max

v((Q1 Q2 ) (R1 R2 ) (Buy( 1 , Pi2 ) Sell( 1 , Pi2 ))

2
2
n
Q1 ,R1 Zn , 1 =h~b,~
ai i=1 Q ,R Z
1
1
1


Cost(Q , P ) + Cost(Q Q2 , Pi2 ) Cost(Q1 , Pi2 ) + Cost(Buy( 1 , Pi2 ), Pi2 )


+ Revenue(R1 , 1 ) + Revenue(R1 R2 , 2i ) Revenue(R1 , 2i ) + Revenue(Sell( 1 , 2i ), 2i )
(17)
subject Q1 R1 Q1 Q2 R1 R2 .

Algorithm 6 SAA(G, N, f, S)
1: sample scenarios (P, )21 , . . . , (P, )2S f
2: BID SAA((P, )21 , . . . , (P, )2S )
3: return
Using theory large deviations, Ahmed Shapiro (2002) establish following
result: , probability optimal solution sample average approximation stochastic program integer recourse optimal solution original
stochastic optimization problem approaches 1 exponentially fast. Given hard time space
constraints, however, always possible sample sufficiently many scenarios infer
reasonable guarantees quality solution sample average approximation. Hence, propose modified SAA heuristic, SAA fed tailor-made
important scenarios, apply idea bidding problem.
5.3.3 Modified Sample Average Approximation
bids SAA places sample prices appear scenarios. SAA never bids
higher good highest sampled price, far knows, bidding
price enough win good scenarios. However, chance
534

fiRoxyBot-06

highest sampled price falls clearing price. Let us compute probability
case single-unit auction, uniform-price multi-unit auction: i.e., one
units good auctioned clear price.
Let F denote cumulative distribution function predicted prices, let f denote
corresponding density function, let G denote cumulative distribution function
clearing prices. Using notation, term 1 G(x) probability
clearing price greater x. Further, let X random variable represents
highest value among sample price predictions. P (X x) = F (x)S
probability samples (and hence highest among them) less x;
P (X = x) = (F (x)S ) = S(F (x))S1 f (x) probability highest value among
samples equals x. Putting two terms togethernamely, probability
highest sample price prediction exactly x, probability clearing price greater
xwe express probability highest SAAs sample price predictions less
clearing price follows:
Z

S(F (x))S1 f (x)(1 G(x))dx
(18)



Assuming perfect prediction (so G = F ), complex expression simplies follows:
Z
S(F (x))S1 f (x)(1 F (x))dx

Z
Z
S1
(F (x))S f (x)dx
(F (x))
f (x)dx
=






(F (x))S
(F (x))S+1
=


S+1


1
=
S+1
Hence, probability SAAs sample price predictions less clearing
price 1/(S + 1). particular, assuming perfect prediction clearing prices
TAC hotel auctions independent, probability SAA agent 49 scenarios
bidding TAC Travel chance winning eight hotels (i.e., probability
sample price least one scenarios greater clearing price)
8

1
= 0.988 0.85.
1 49+1
remedy situation, designed implemented simple variant SAA
RoxyBot-06. SAA* heuristic (see Algorithm 7) close cousin SAA, difference
arising respective scenario sets.
P Whereas SAA samples scenarios, SAA* samples
|N | scenarios, |N | = g Ng . SAA* creates additional |N | scenarios
follows: unit k good g G, sets price kth unit good g
upper limit range possible prices and, conditioning price setting, sets
prices goods mean values. Next, describe experiments
test suite bidding heuristics, including SAA SAA*, controlled testing environment.

535

fiGreenwald, Lee, & Naroditskiy

Algorithm 7 SAA(G, N, f, S)
Require: |N |
1: hard-code |N | scenarios (P, )21 , . . . , (P, )2|N |
2: sample |N | scenarios (P, )2|N |+1 , . . . , (P, )2S f
3: BID SAA((P, )21 , . . . , (P, )2S )
4: return
Agent
SMU
AMU

Predictions
Average scenario
scenarios

TMU

TMU*

Average scenario
scenarios
Average scenario

BE*

scenarios

Bids
Marginal utilities
Calculates marginal utilities scenario
Bids average marginal utilities across scenarios
Marginal utilities
Best TMU solutions
Marginal utilities, assuming goods
target set available
Best TMU*solutions


goods
goods
Goods target set
Goods target set
Goods target set
Goods target set

Table 3: Marginal-utility-based agents. marginal utility good defined
incremental utility achieved winning good, relative
utility set goods already held.

5.4 Summary
section, developed series bidding problems, heuristics solutions
problems, captures essence bidding one-shot continuously-clearing
auctions characterize TAC. bulk presentation deliberately abstract,
suggest problems solutions applicable well beyond realm
TAC: e.g., bidding interdependent goods separate eBay auctions. Still, remains
validate approach application domains.

6. Experiments
close paper two sets experimental results, first controlled testing
environment, second results final round 2006 TAC Travel competition. combined strategy hotel price prediction via SimAA bid optimization
via SAA emerged victorious settings.
6.1 Controlled Experiments
extent least, approach bidding validated success
RoxyBot-06 TAC-06. Nonetheless, ran simulations controlled testing environment
validate approach. results reported Lee (2007) Greenwald et
al. (2008), summarize well.

536

fiRoxyBot-06

built test suite agents, predict using RoxyBot-06s SimAA random
mechanism distribution. agents differ bidding strategies; possibilities
include SAA,8 SAA*, six marginal-utility-based heuristics studied Wellman et
al. (2007), summarized Table 3.
experiments conducted TAC Travel-like setting, modified remove
aspects game would obscure controlled study bidding. Specifically,
eliminated flight entertainment trading, endowed agents eight flights
eight flights day. Further, assumed hotels closed one round
bidding (i.e., hotel auctions one-shot, ensuing bid optimization problem
adheres Definition 5.4).
designed two sets experiments: one decision-theoretic one game-theoretic.
former, hotel clearing prices outcome simulation simultaneous ascending
auctions, depend actual clients game, random sampling. (Our
simulator informed individual agents.) latter, hotel clearing prices
determined bids agents submit using mechanism TAC Travel:
clearing price 16th highest bid (or zero, fewer 16 bids submitted).
first ran experiments 8 agents per game, found hotel prices
often zero: i.e., insufficient competition. changed setup include
random number agents drawn binomial distribution n = 32 p = 0.5,
requisite number agents sampled uniformly replacement set possible
agents. agents first sample number competitors binomial distribution,
generate scenarios assuming sampled number competitors.
game-theoretic nature TAC, individual agents performance
depend heavily agents included agent pool. experiments,
attempted mitigate artificial effects specific agents chose include
pool sampling agents pool play game, replacement. Thus,
agents average score games measure agents performance
various combinations opponents.
Figures 3(a) 3(b), plot mean scores obtained agent type
setting, along 95% confidence intervals. averages computed based 1000
independent observations, obtained playing 1000 games. Scores averaged across
agent types game account game dependencies. SAAB SAAT9
best performing agents game-theoretic experiments among best
decision-theoretic setting.
6.2 TAC 2006 Competition Results
Table 4 lists agents entered TAC-06 Table 5 summarizes outcome.
TAC-06 finals comprised 165 games three days, 80 games last day
weighted 1.5 times much 85 first two days. first day finals,
RoxyBot finished third, behind Mertacor Walverinethe top scorers 2005. happens,
RoxyBots optimization routine, designed stochastic hotel entertainment
8. particular implementation details explaining RoxyBot-06 applied SAA TAC domain
relegated Appendix A.
9. SAAB SAA, SAAT slight variant SAA*. See paper Greenwald et al. (2008)
details.

537

fiGreenwald, Lee, & Naroditskiy

1

0.85

0.95
Score (thousands)

Score (thousands)

0.8
0.75
0.7
0.65
0.6

0.9
0.85
0.8
0.75
0.7

0.55
0.65
0.5
0.6
SAAT SAAB

TMU

TMU*

Agent

BE*

AMU

SMU

(a) Decision-theoretic setting

SAAT SAAB

TMU

TMU*

Agent

BE*

AMU

SMU

(b) Game-theoretic setting

Figure 3: Mean scores confidence intervals.

price predictions, accidentally fed deterministic predictions (i.e., point price estimates)
entertainment. Moreover, predictions fixed, rather adapted based
recent game history.
days 2 3, RoxyBot ran properly, basing bidding auctions stochastic
information. Moreover, agent upgraded day 1 bid flights once,
twice, minute. enabled agent delay bidding somewhat
end game flights whose prices decreasing. doubt minor modification
enabled RoxyBot emerge victorious 2006, edging Walverine whisker,
integer precision reported Table 5. actual margin 0.22a mere 22 parts
400,000. Adjusting control variates (Ross, 2002) spreads top two finishers bit
further.10
Agent
006
kin agent
L-Agent
Mertacor
RoxyBot
UTTA
Walverine
WhiteDolphin

Affiliation
Swedish Inst Comp Sci
U Macau
Carnegie Mellon U
Aristotle U Thessaloniki
Brown U
U Tehran
U Michigan
U Southampton

Reference
Aurell et al., 2002
Sardinha et al., 2005
Toulis et al., 2006; Kehagias et al., 2006
Greenwald et al., 2003, 2004, 2005; Lee et al., 2007
Cheng et al., 2005; Wellman et al., 2005
& Jennings, 2002; Vetsikas & Selman, 2002

Table 4: TAC-06 participants.

10. Kevin Lochner computed adjustment factors using method described Wellman et al. (2007,
ch. 8).

538

fiRoxyBot-06

Agent
RoxyBot
Walverine
WhiteDolphin
006
Mertacor
L-Agent
kin agent
UTTA

Finals
4032
4032
3936
3902
3880
3860
3725
2680

Adjustment Factor
5
17
2
27
16
7
0
14

Table 5: TAC-06 final scores, adjustment factors based control variates.

Mean scores, utilities, costs (with 95% confidence intervals) last day
TAC-06 finals (80 games) plotted Figure 4 detailed statistics tabulated
Table 6. single metric low hotel flight costs responsible
RoxyBots success. Rather success derives right balance contradictory goals.
particular, RoxyBot incurs high hotel mid-range flight costs achieving mid-range
trip penalty high event profit.11
Let us compare RoxyBot two closest rivals: Walverine WhiteDolphin. Comparing
Walverine first, Walverine bids lower prices (by 55) fewer hotels (49 less), yet wins (0.8)
wastes less (0.42). would appear Walverines hotel bidding strategy outperforms
RoxyBots, except RoxyBot earns higher hotel bonus (15 more). RoxyBot also gains
advantage spending 40 less flights earning 24 total entertainment profit.
different competition takes place RoxyBot WhiteDolphin. WhiteDolphin
bids lower prices (120 less) hotels (by 52) RoxyBot. RoxyBot spends much
(220) hotels WhiteDolphin makes earning higher hotel bonus (by
96) lower trip penalty (by 153). seems WhiteDolphins strategy minimize
costs even means sacrificing utility.
6.3 Summary
already noted, TAC Travel bidding, viewed optimization problem, n-stage
decision problem. solve n-stage decision problem sequence 2-stage decision
problems. controlled experiments reported section establish bidding
strategy, SAA, best test suite setting designed,
2 stages. TAC competition results establish strategy also effective
n-stage setting.

7. Collective Behavior
hotel price prediction techniques described Section 4.2 designed compute (or
least approximate) competitive equilibrium prices without full knowledge client pop11. agent suffers trip penalties extent assigns clients packages differ
preferred.

539

fiGreenwald, Lee, & Naroditskiy

# Hotel Bids
Average Hotel Bids
# Hotels
Hotel Costs
# Unused Hotels
Hotel Bonus
Trip Penalty
Flight Costs
Event Profits
Event Bonus
Total Event Profits
Average Utility
Average Cost
Average Score

Rox
130
170
15.99
1102
2.24
613
296
4615
110
1470
1580
9787
5608
4179

Wal
81
115
16.79
1065
1.82
598
281
4655
26
1530
1556
9847
5693
4154

Whi
182
50
23.21
882
9.48
517
449
4592
6
1529
1535
9597
5468
4130

SIC
33
513
13.68
1031
0.49
617
340
4729
-6
1498
1492
9775
5765
4010

Mer
94
147
18.44
902
4.86
590
380
4834
123
1369
1492
9579
5628
3951

L-A
58
88
14.89
987
1.89
592
388
4525
-93
1399
1306
9604
5605
3999

kin
15
356
15.05
1185
0.00
601
145
4867
-162
1619
1457
10075
6213
3862

UTT
24
498
9.39
786
0.48
424
213
3199
-4
996
992
6607
3989
2618

Table 6: 2006 Finals, Last day. Tabulated Statistics. omit first two days
agents vary across days, cannot vary within. Presumably, entries
last day teams preferred versions agents.
2006 Finals, Last Day

2006 Finals, Last Day

2006 Finals, Last Day

4.5

6.5

10

6

3.5
3

9
8
7

2.5

6

2

5

Rox Wal Whi SIC Mer LA kin UTT
Agent

Cost (thousands)

Utility (thousands)

Score (thousands)

4

5.5
5
4.5
4
3.5

Rox Wal Whi SIC Mer LA kin UTT
Agent

3

Rox Wal Whi SIC Mer LA kin UTT
Agent

Figure 4: 2006 Finals, Last day. Mean scores, utilities, costs, 95% confidence
intervals.

ulation. section, assume knowledge view output tatonnement
SimAA calculations predictions ground truth. compare actual
prices final games ground truth respective years since 2002 determine
whether TAC market prices resemble CE prices. find depicted Figure 5.
nature methods, calculations pertain hotel prices only.
results highly correlated metrics (Euclidean distance EVPP).
observe accuracy CE price calculations varied year year. 2003
year TAC Supply Chain Management (SCM) introduced. Many
participants diverted attention away Travel towards SCM year, perhaps
leading degraded performance Travel. Things seem improve 2004 2005.

540

fiRoxyBot-06

cannot explain setback 2006, except noting performance highly dependent
particular agent pool, 2006 fewer agents pool.
260

45

tatonnement, exact
simAA, exact
Expected Value Perfect Prediction

240

Euclidean Distance

220
200
180
160
140
120
100
2002

2003

2004
Year

2005

2006

tatonnement, exact
simAA, expact

40

35

30

25

20
2002

2003

2004
Year

2005

2006

Figure 5: comparison actual (hotel) prices output competitive equilibrium
price calculations final games since 2002. label exact means: full
knowledge client population.

8. Conclusion
foremost aim trading agent research develop body techniques effective
design analysis trading agents. Contributions trading agent design include
invention trading strategies, together models algorithms realizing
computation methods measure evaluate performance agents characterized
strategies. Researchers seek specific solutions particular trading problems
general principles guide development trading agents across market scenarios.
paper purports contribute research agenda. described design
implementation RoxyBot-06, able trading agent demonstrated performance
TAC-06.
Although automated trading electronic markets yet fully taken hold,
trend well underway. TAC, trading agent community demonstrating
potential autonomous bidders make pivotal trading decisions effective way.
agents offer potential accelerate automation trading broadly,
thus shape future commerce.

Acknowledgments
paper extends work Lee et al. (2007). material Section 5.1 based
book Wellman et al. (2007). grateful several anonymous reviewers whose
constructive criticisms enhanced quality work. research supported
NSF Career Grant #IIS-0133689.

541

fiGreenwald, Lee, & Naroditskiy

Appendix A. TAC Bidding Problem: SAA
problem bidding simultaneous auctions characterize TAC formulated two-stage stochastic program. appendix, present implementation
details integer linear program (ILP) encoded RoxyBot-06 approximates
optimal solution stochastic program.12
formulate ILP assuming current prices known, future prices uncertain first stage revealed second stage. Note whenever prices
known, suffices agent make decisions quantity good buy,
rather bid amounts, since choosing bid amount greater
equal price good equivalent decision buy good.
Unlike main body paper, ILP formulation bidding TAC assumes
linear prices. Table 7 lists price constants decision variables auction type.
hotels, decisions pertain buy offers; flights, agent decides many
tickets buy many buy later; entertainment events, agent chooses
sell quantities well buy quantities.
Hotels
bid

Price
Yas

Flights Events
buy
buy later
Events
sell
sell later

Variable (bid)
apq
Price

Yas

Price
Na
Zas

Variable (qty)



Variable (qty)



Table 7: Auction types associated price constants decision variables.

A.1 Index Sets
indexes set goods, auctions.
af Af indexes set flight auctions.
ah Ah indexes set hotel auctions.
ae Ae indexes set event auctions.
c C indexes set clients.
p P indexes set prices.
12. precise formulation RoxyBot-06s bidding ILP appears paper Lee et al. (2007).
formulation slightly simplified, expect would perform comparably TAC. key
differences flight entertainment bidding.

542

fiRoxyBot-06

q Q indexes set quantities
(i.e., units good auction).
indexes set scenarios.
indexes set trips.
A.2 Constants
Gat indicates quantity good required complete trip t.
indicates current buy price af , ae .
Na indicates current sell price ae .
Yas indicates future buy price af , ah , ae scenario s.
Zas indicates future sell price ae scenario s.
Ha indicates hypothetical quantity hotel ah .
Oa indicates quantity good agent owns.
Uct indicates client cs value trip t.
A.3 Decision Variables
= {cst } set boolean variables indicating whether client c allocated
trip scenario s.
= {apq } set boolean variables indicating whether bid price p qth
unit ah .
= {a } set integer variables indicating many units af , ae buy now.
N = {a } set integer variables indicating many units ae sell now.
= {as } set integer variables indicating many units af , ae buy later
scenario s.
Z = {as } set integer variables indicating many units ae sell later
scenario s.
A.4 Objective Function


flight cost

current}| future {
hotel cost
z }| {
z X }|
{
X z }| { z }| {
X
X


+ Yas
Uct cts
Yas apq +
max


,,M,N,Y,Z
Af
Ah ,Q,pYas
C,T
trip value

z

543

(19)

fiGreenwald, Lee, & Naroditskiy


event revenue
event cost
}|
{ z
z
}|
{
current
future
future
current

z
}|
{
z
z
}|
{
}|
{
}| {
z
X


Na + Zas Yas


Ae

A.5 Constraints
X

cst 1 c C,

(20)



allocation

buy

z }| { z}|{
z }| {
X
cst Gat Oa + (a + )

Af ,

(21)

Ah ,

(22)

C,T

buy

allocation


z }| { z}|{
z X}|
{
X
cst Gat Oa +
apq
C,T

Q,pYas

allocation


sell

buy
z }| { z}|{
z }| {
X
z }| {
cst Gat Oa + + +
C,T

Ae ,

X

apq Ha

(23)

Ah

(24)

apq 1 Ah , q Q

(25)

P,Q

X
P

Equation (20) limits client one trip scenario. Equation (21) prevents
agent allocating flights buy. Equation (22) prevents agent
allocating hotels buy. Equation (23) prevents agent
allocating event tickets buy sell. Equation (24) ensures
agent bids least HQW units hotel auction. Equation (25) prevents agent
placing one buy offer per unit hotel auction.
agent might also constrained place sell offers units good
owns, and/or place buy (sell) offers units good
market supplies (demands).
Note need explicitly enforce bid monotonicity constraints
ILP formulation:
Buy offers must nonincreasing k, sell offers nondecreasing.
ILP need constraint prices assumed linear.
effect, decisions ILP makes many units good bid
on. Hence, bids (10, 15, 20) (20, 15, 10) equivalent.
agent may offer sell less price willing buy.
544

fiRoxyBot-06

ILP would choose place buy offer sell offer good
buy price good exceeds sell price, would unprofitable.

References
Ahmed, S., & Shapiro, A. (2002). sample average approximation method
stochastic programs integer recourse. Optimization Online, http://www.
optimization-online.org.
Arunachalam, R., & Sadeh, N. M. (2005). supply chain trading agent competition.
Electronic Commerce Research Applications, 4 (1), 6684.
Aurell, E., Boman, M., Carlsson, M., Eriksson, J., Finne, N., Janson, S., Kreuger, P., &
Rasmusson, L. (2002). trading agent built constraint programming. Eighth
International Conference Society Computational Economics: Computing
Economics Finance, Aix-en-Provence.
Birge, J., & Louveaux, F. (1997). Introduction Stochastic Programming. Springer, New
York.
Cai, K., Gerding, E., McBurney, P., Niu, J., Parsons, S., & S.Phelps (2009). Overview
CAT: market design competition. Tech. rep. ULCS-09-005, University Liverpool.
Cheng, S., Leung, E., Lochner, K., K.OMalley, Reeves, D., Schvartzman, L., & Wellman,
M. (2003). Walverine: Walrasian trading agent. Proceedings Second
International Joint Conference Autonomous Agents Multi-Agent Systems, pp.
465472.
Cheng, S., Leung, E., Lochner, K., K.OMalley, Reeves, D., Schvartzman, L., & Wellman,
M. (2005). Walverine: Walrasian trading agent. Decision Support Systems, 39 (2),
169184.
Cramton, P. (2006). Simultaneous ascending auctions. Cramton, P., Shoham, Y., &
Steinberg, R. (Eds.), Combinatorial Auctions. MIT Press.
Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering successful TAC participation. Proceedings First International Joint Conference Autonomous
Agents Multiagent Systems, pp. 4546.
Greenwald, A. (2003). Bidding marginal utility simultaneous auctions. Workshop
Trading Agent Design Analysis.
Greenwald, A., & Boyan, J. (2004). Bidding uncertainty: Theory experiments.
Proceedings 20th Conference Uncertainty Artificial Intelligence, pp.
209216.
Greenwald, A., Naroditskiy, V., & Lee, S. (2008). Bidding heuristics simultaneous
auctions: Lessons tac travel. Workshop Trading Agent Design Analysis.
Greenwald, A., & Boyan, J. (2005). Bidding algorithms simultaneous auctions: case
study. Journal Autonomous Agents Multiagent Systems, 10 (1), 6789.
He, M., & Jennings, N. (2002). SouthamptonTAC: Designing successful trading agent.
Proceedings Fifteenth European Conference Artificial Intelligence, pp. 812.
545

fiGreenwald, Lee, & Naroditskiy

Jordan, P. R., & Wellman, M. P. (2009). Designing ad auctions game trading
agent competition. Workshop Trading Agent Design Analysis.
Kehagias, D., Toulis, P., & Mitkas, P. (2006). long-term profit seeking strategy continuous double auctions trading agent competition. Fourth Hellenic Conference
Artificial Intelligence, Heraklion.
Krishna, V. (2002). Auction Theory. Academic Press.
Lee, S. J. (2007). Comparison bidding algorithms simultaneous auctions. B.S. honors thesis, Brown University, http://list.cs.brown.edu/publications/theses/
ugrad/.
Lee, S., Greenwald, A., & Naroditskiy, V. (2007). Roxybot-06: (SAA)2 TAC travel agent.
Proceedings 20th International Joint Conference Artificial Intelligence,
pp. 13781383.
Ross, S. M. (2002). Simulation (Third edition). Academic Press.
Sardinha, J. A. R. P., Milidiu, R. L., Paranhos, P. M., Cunha, P. M., & de Lucena, C.
J. P. (2005). agent based architecture highly competitive electronic markets.
Proceedings Eighteenth International Florida Artificial Intelligence Research
Society Conference, Clearwater Beach, Florida, USA, pp. 326332.
Toulis, P., Kehagias, D., & Mitkas, P. (2006). Mertacor: successful autonomous trading
agent. Fifth International Joint Conference Autonomous Agents Multiagent
Systems, pp. 11911198, Hakodate.
Vetsikas, I., & Selman, B. (2002). WhiteBear: empirical study design tradeoffs autonomous trading agents. Workshop Game-Theoretic Decision-Theoretic Agents.
Vickrey, W. (1961). Counterspeculation, auctions, competitive sealed tenders. Journal
Finance, 16, 837.
Walras, L. (1874). Elements deconomie politique pure. L. Corbaz, Lausanne.
Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: Strategies
Lessons Trading Agent Competition. MIT Press.
Wellman, M. P., Reeves, D. M., Lochner, K. M., & Suri, R. (2005). Searching Walverine
2005. Workshop Trading Agent Design Analysis, No. 3937 Lecture Notes
Artificial Intelligence, pp. 157170. Springer.
Wellman, M., Reeves, D., Lochner, K., & Vorobeychik, Y. (2004). Price prediction
Trading Agent Competition. Artificial Intelligence Research, 21, 1936.

546

fiJournal Artificial Intelligence Research 36 (2009) 341-385

Submitted 05/09; published 11/09

Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches
Tahira Naseem
Benjamin Snyder
Jacob Eisenstein
Regina Barzilay

TAHIRA @ CSAIL . MIT. EDU
BSNYDER @ CSAIL . MIT. EDU
JACOBE @ CSAIL . MIT. EDU
REGINA @ CSAIL . MIT. EDU

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
77 Massachusetts Avenue, Cambridge 02139

Abstract
demonstrate effectiveness multilingual learning unsupervised part-of-speech tagging. central assumption work combining cues multiple languages,
structure becomes apparent. consider two ways applying intuition
problem unsupervised part-of-speech tagging: model directly merges tag structures
pair languages single sequence second model instead incorporates multilingual context using latent variables. approaches formulated hierarchical Bayesian
models, using Markov Chain Monte Carlo sampling techniques inference. results demonstrate incorporating multilingual evidence achieve impressive performance gains
across range scenarios. also found performance improves steadily number
available languages increases.

1. Introduction
paper, explore application multilingual learning part-of-speech tagging
annotation available.1 fundamental idea upon work based patterns
ambiguity inherent part-of-speech tag assignments differ across languages. lexical level,
word part-of-speech tag ambiguity one language may correspond unambiguous word
language. example, word English may function auxiliary verb,
noun, regular verb. However, many languages likely express different
senses three distinct lexemes. Languages also differ patterns structural ambiguity.
example, presence article English greatly reduces ambiguity succeeding
tag. languages without articles, however, constraint obviously absent. key idea
multilingual learning combining natural cues multiple languages, structure
becomes apparent.
Even expressing meaning, languages take different syntactic routes, leading
cross-lingual variation part-of-speech patterns. Therefore, effective multilingual model must
accurately represent common linguistic structure, yet remain flexible idiosyncrasies
language. tension becomes stronger additional languages added mix. Thus,
key challenge multilingual learning capture cross-lingual correlations preserving
individual language tagsets, tag selections, tag orderings.
1. Code,
data
sets,


raw
outputs
http://groups.csail.mit.edu/rbg/code/multiling pos.

c
2009
AI Access Foundation. rights reserved.





experiments



available



fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

paper, explore two different approaches modeling cross-lingual correlations.
first approach directly merges pairs tag sequences single bilingual sequence, employing
joint distributions aligned tag-pairs; unaligned tags, language-specific distributions still
used. second approach models multilingual context using latent variables instead explicit
node merging. group aligned words, multilingual context encapsulated value
corresponding latent variable. Conditioned latent variable, tagging decisions
language remain independent. contrast first model, architecture hidden variable
model allows scale gracefully number languages increases.
approaches formulated hierarchical Bayesian models underlying trigram
HMM substructure language. first model operates simple directed graphical
model one additional coupling parameter beyond transition emission parameters
used monolingual HMMs. latent variable model, hand, formulated
non-parametric model; viewed performing multilingual clustering aligned sets
tag variables. latent variable value indexes separate distribution tags language,
appropriate given context. models, perform inference using Markov Chain Monte
Carlo sampling techniques.
evaluate models parallel corpus eight languages: Bulgarian, Czech, English,
Estonian, Hungarian, Romanian, Serbian, Slovene. consider range scenarios vary
combinations bilingual models single model jointly trained eight languages. results show consistent robust improvements monolingual baseline
almost combinations languages. complete tag lexicon available latent variable model trained using eight languages, average performance increases 91.1% accuracy
95%, halving gap unsupervised supervised performance. realistic cases, lexicon restricted frequently occurring words, see even larger
gaps monolingual multilingual performance. one scenario, average multilingual performance increases 82.8% monolingual baseline 74.8%. language
pairs, improvement especially noteworthy; instance, complete lexicon scenario, Serbian
improves 84.5% 94.5% paired English.
find scenarios latent variable model achieves higher performance
merged structure model, even restricted pairs languages. Moreover hidden
variable model effectively accommodate large numbers languages makes
desirable framework multilingual learning. However, observe latent variable model
somewhat sensitive lexicon coverage. performance merged structure model,
hand, robust respect. case drastically reduced lexicon (with 100
words only), performance clearly better hidden variable model. indicates
merged structure model might better choice languages lack lexicon resources.
surprising discovery experiments marked variation level improvement
across language pairs. best pairing language chosen oracle, average bilingual
performance reaches 95.4%, compared average performance 93.1% across pairs.
experiments demonstrate variability influenced cross-lingual links languages
well model consideration. identify several factors contribute
success language pairings, none uniquely predict supplementary language
helpful. results suggest multi-parallel corpora available, model
simultaneously exploits languages latent variable model proposed

342

fiM ULTILINGUAL PART- -S PEECH TAGGING

preferable strategy selects one bilingual models. found performance tends
improves steadily number available languages increases.
realistic scenarios, tagging resources number languages may already available.
models easily exploit amount tagged data subset available languages.
experiments show, annotation added, performance increases even languages
lacking resources.
remainder paper structured follows. Section 2 compares approach
previous work multilingual learning unsupervised part-of-speech tagging. Section 3 presents
two approaches modeling multilingual tag sequences, along inference procedures
implementation details. Section 4 describes corpora used experiments, preprocessing steps
various evaluation scenarios. results experiments analysis given
Sections 5, 6. summarize contributions consider directions future work
Section 7.

2. Related Work
identify two broad areas related work: multilingual learning inducing part-of-speech tags
without labeled data. discussion multilingual learning focuses unsupervised approaches
incorporate two languages. describe related work unsupervised semisupervised models part-of-speech tagging.
2.1 Multilingual Learning
potential multilingual data rich source linguistic knowledge recognized since
early days empirical natural language processing. patterns ambiguity vary greatly
across languages, unannotated multilingual data serve learning signal unsupervised
setting. especially interested methods leverage two languages jointly,
compare approach relevant prior work.
Multilingual learning may also applied semi-supervised setting, typically projecting
annotations across parallel corpus another language resources exist (e.g.,
Yarowsky, Ngai, & Wicentowski, 2000; Diab & Resnik, 2002; Pado & Lapata, 2006; Xi & Hwa,
2005). primary focus unsupervised induction cross-linguistic structures,
address area.
2.1.1 B ILINGUAL L EARNING
Word sense disambiguation (WSD) among first successful applications automated multilingual learning (Dagan et al., 1991; Brown et al., 1991). Lexical ambiguity differs across languages
sense polysemous word one language may translate distinct counterpart another
language. makes possible use aligned foreign-language words source noisy supervision. Bilingual data leveraged way variety WSD models (Brown et al.,
1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002;
Bhattacharya, Getoor, & Bengio, 2004), quality supervision provided multilingual
data closely approximates manual annotation (Ng et al., 2003). Polysemy one source
ambiguity part-of-speech tagging; thus model implicitly leverages multilingual WSD
context higher-level syntactic analysis.

343

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Multilingual learning previously applied syntactic analysis; pioneering effort
inversion transduction grammar Wu (1995). method trained unannotated parallel
corpus using probabilistic bilingual lexicon deterministic constraints bilingual tree structures. inside-outside algorithm (Baker, 1979) used learn parameters manually specified
bilingual grammar. ideas extended subsequent work synchronous grammar induction hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005).
One characteristic family methods designed inherently multilingual tasks machine translation lexicon induction. share goal learning
multilingual data, seek induce monolingual syntactic structures applied even
multilingual data unavailable.
respect, approach closer unsupervised multilingual grammar induction work
Kuhn (2004). Starting hypothesis trees induced parallel sentences
exhibit cross-lingual structural similarities, Kuhn uses word-level alignments constrain set
plausible syntactic constituents. constraints implemented hand-crafted deterministic rules, incorporated expectation-maximization grammar induction assign zero
likelihood illegal bracketings. probabilities productions estimated separately
language, applied monolingual data directly. Kuhn shows form
multilingual training yields better monolingual parsing performance.
methods incorporate cross-lingual information fundamentally different manner. Rather
using hand-crafted deterministic rules may require modification language
pair estimate probabilistic multilingual patterns directly data. Moreover, estimation
multilingual patterns incorporated directly tagging model itself.
Finally, multilingual learning recently applied unsupervised morphological segmentation (Snyder & Barzilay, 2008). research related, moving morphological
syntactic analysis imposes new challenges. One key difference Snyder & Barzilay model
morphemes unigrams, ignoring transitions morphemes. syntactic analysis, transition information provides crucial constraint, requiring fundamentally different model structure.
2.1.2 B EYOND B ILINGUAL L EARNING
work multilingual learning focuses bilingual analysis, models operate
one pair languages. instance, Genzel (2005) describes method inducing
multilingual lexicon group related languages. work first induces bilingual models
pair languages combines them. take different approach simultaneously
learning languages, rather combining bilingual results.
related thread research multi-source machine translation (Och & Ney, 2001; Utiyama
& Isahara, 2006; Cohn & Lapata, 2007; Chen, Eisele, & Kay, 2008; Bertoldi, Barbaiani, Federico,
& Cattoni, 2008) goal translate multiple source languages single target
language. using multi-source corpora, systems alleviate sparseness increase translation coverage, thereby improving overall translation accuracy. Typically, multi-source translation
systems build separate bilingual models select final translation output. instance, method developed Och Ney (2001) generates several alternative translations
source sentences expressed different languages selects likely candidate. Cohn
Lapata (2007) consider different generative model: rather combining alternative sentence
translations post-processing step, model estimates target phrase translation distribu-

344

fiM ULTILINGUAL PART- -S PEECH TAGGING

tion marginalizing multiple translations various source languages. model
combines multilingual information phrase level, core estimates phrase tables
obtained using bilingual models.
contrast, present approach unsupervised multilingual learning builds single
joint model across languages. makes maximal use unlabeled data sidesteps
difficult problem combining output multiple bilingual systems without supervision.
2.2 Unsupervised Part-of-Speech Tagging
Unsupervised part-of-speech tagging involves predicting tags words, without annotations
correct tags word tokens. Generally speaking, unsupervised setting permit
use declarative knowledge relationship tags word types, form
dictionary permissible tags common words. setup referred semisupervised Toutanova Johnson (2008), considered unsupervised papers topic (e.g., Goldwater & Griffiths, 2007). evaluation considers tag dictionaries
varying levels coverage.
Since work Merialdo (1994), hidden Markov model (HMM) common representation2 unsupervised tagging (Banko & Moore, 2004). Part-of-speech tags
encoded linear chain hidden variables, words treated emitted observations. Recent
advances include use fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007),
places prior distributions tag transition word-emission probabilities. Bayesian
priors permit integration parameter settings, yielding models perform well across range
settings. particularly important case small datasets, many counts
used maximum-likelihood parameter estimation sparse. Bayesian setting also facilitates integration data sources, thus serves departure point work.
Several recent papers explored development alternative training procedures
model structures effort incorporate expressive features permitted generative HMM. Smith Eisner (2005) maintain HMM structure, incorporate large number
overlapping features conditional log-linear formulation. Contrastive estimation used
provide training criterion maximizes probability observed sentences compared
set similar sentences created perturbing word order. use large set features
discriminative training procedure led strong performance gains.
Toutanova Johnson (2008) propose LDA-style model unsupervised part-of-speech
tagging, grouping words latent layer ambiguity classes. ambiguity class corresponds set permissible tags; many languages set tightly constrained morphological features, thus allowing incomplete tagging lexicon expanded. Haghighi Klein
(2006) also use variety morphological features, learning undirected Markov Random Field
permits overlapping features. propagate information small number labeled prototype examples using distributional similarity prototype non-prototype words.
focus effectively incorporate multilingual evidence, require simple model
easily applied multiple languages widely varying structural properties. view
direction orthogonal refining monolingual tagging models particular language.
2. addition basic HMM architecture, part-of-speech tagging approaches explored (Brill, 1995;
Mihalcea, 2004)

345

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

(

(



b

)

)



l



v

e

f





h





r



e
l

J

(

c





e











l

n



r



v

e

f

l
J







h



e

p





e











n

p



)

l



l



v

e

f





h

J









r

e



p

e











n

p



n





h

e

v





g







u

j

h

e





c

h

c

h

l









n



h





Figure 1: Example graphical structures (a) two standard monolingual HMMs, (b) merged
node model, (c) latent variable model three superlingual variables.

3. Models
motivating hypothesis work patterns ambiguity part-of-speech level differ
across languages systematic ways. considering multiple languages simultaneously, total
inherent ambiguity reduced language. potential advantages leveraging
multilingual information comes challenge respecting language-specific characteristics
tag inventory, selection order. end, develop models jointly tag parallel streams
text multiple languages, maintaining language-specific tag sets parameters
transitions emissions.

346

fiM ULTILINGUAL PART- -S PEECH TAGGING

Part-of-speech tags reflect syntactic semantic function tagged words. Across
languages, pairs word tokens known share semantic syntactic function
tags related systematic ways. word alignment task machine translation
identify pairs words parallel sentences. Aligned word pairs serve cross-lingual
anchors model, allowing information shared via joint tagging decisions. Research
machine translation produced robust tools identifying word alignments; use tool
black box treat output fixed, observed property parallel data.
Given set parallel sentences, posit hidden Markov model (HMM) language,
hidden states represent tags emissions words. unsupervised
monolingual setting, inference part-of-speech tags performed jointly estimation
parameters governing relationship tags words (the emission probabilities) consecutive tags (the transition probabilities). multilingual models built upon
structural foundation, emission transition parameters retain identical interpretation monolingual setting. Thus, parameters learned parallel text
later applied monolingual data.
consider two alternative approaches incorporating cross-lingual information. first
model, tags aligned words merged single bi-tag nodes; second, latent variable
model, additional layer hidden superlingual tags instead exerts influence tags clusters
aligned words. first model primarily designed bilingual data, second model
operates number languages. Figure 1 provides graphical model representation
monolingual, merged node, latent variable models instantiated single parallel sentence.
merged node latent variable approaches formalized hierarchical Bayesian
models. provides principled probabilistic framework integrating multiple sources
information, offers well-studied inference techniques. Table 1 summarizes mathematical
notation used throughout section. describe model depth.
3.1 Bilingual Unsupervised Tagging: Merged Node Model
bilingual merged node model, cross-lingual context incorporated creating joint bi-tag
nodes aligned words. would strong insist aligned words identical
tag; indeed, may even appropriate assume two languages share identical tag sets.
However, two words aligned, want choose tags jointly. enable this,
allow values bi-tag nodes range possible tag pairs ht, ,
represent tagsets language.
tags need identical, believe systematically related.
modeled using coupling distribution , multinomial tag pairs.
parameter combined standard transition distribution product-of-experts model.

Thus, aligned tag pair hyi , yj conditioned predecessors yi1 yj1
, well

3
coupling parameter (yi , yj ). coupled bi-tag nodes serve bilingual anchors due
Markov dependency structure, even unaligned words may benefit cross-lingual information
propagates nodes.
3. describing merged node model, consider two languages , use simplified notation

write hy, mean hy , i. Similar abbreviations used language-indexed parameters.

347

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Notation used models
xi
yi

a,











0



0



ith word token sentence language .
ith tag sentence language .
word alignments language pair h, i.
transition distribution (over tags), conditioned tag language . describe bigram transition model, though implementation uses trigrams (without bigram interpolations); extension trivial.
emission distribution (over words), conditioned tag
language .
parameter symmetric Dirichlet prior transition
distributions.
parameter symmetric Dirichlet prior emission
distributions.

Notation used merged node model




0
Ab




coupling parameter assigns probability mass pair
aligned tags.
Dirichlet prior coupling parameter.
Distribution bilingual alignments.

Notation used latent variable model






zj



z = hz1 , z2 , . . . , zn



G0







multinomial superlingual tags z.
concentration parameter , controlling much probability mass allocated first values.
setting j th superlingual tag, ranging set integers, indexing distribution set .
z th set distributions tags languages 1
n .
base distribution z drawn, whose form
set n symmetric Dirichlet distributions parameter
0 .
Distribution multilingual alignments.

Table 1: Summary notation used description models. sentence treated
isolation (conditioned parameters), sentence indexing left implicit.

348

fiM ULTILINGUAL PART- -S PEECH TAGGING

present generative account words sentence parameters
model produced. generative story forms basis sampling-based inference
procedure.
3.1.1 ERGED N ODE ODEL : G ENERATIVE TORY
generative story assumes existence two tagsets, , two vocabularies W
W one language. ease exposition, formulate model bigram
tag dependencies. However, experiments used trigram model (without bigram
interpolation), trivial extension described model.
1. Transition Emission Parameters. tag , draw transition distribution
tags , emission distribution words W . transition emission
distributions multinomial, drawn conjugate prior, Dirichlet (Gelman, Carlin, Stern, & Rubin, 2004). use symmetric Dirichlet priors, encode
expectation uniformity induced multinomials, encode preferences
specific words tags.
tag , draw transition distribution tags , emission distribution
words W , symmetric Dirichlet priors.
2. Coupling Parameter. Draw bilingual coupling distribution tag pairs pairs .
distribution multinomial dimension |T | |T |, drawn symmetric
Dirichlet prior 0 tag pairs.
3. Data. bilingual parallel sentence:
(a) Draw alignment bilingual alignment distribution Ab . following paragraph defines Ab formally.
(b) Draw bilingual sequence part-of-speech tags (y1 , ..., ym ), (y1 , ..., yn ) according to:
P ((y1 , ..., ym ), (y1 , ..., yn )|a, , , ).4 joint distribution thus conditions
alignment structure, transition probabilities languages, coupling
distribution; formal definition given Formula 1.
(c) part-of-speech tag yi first language, emit word vocabulary W :
xi yi ,
(d) part-of-speech tag yj second language, emit word vocabulary
W : xj .
j

completes outline generative story. provide detail alignments handled, distribution coupled part-of-speech tag sequences.
Alignments alignment defines bipartite graph words x x two parallel
sentences . particular, represent set integer pairs, indicating word indices.
Crossing edges permitted, would lead cycles resulting graphical model;
thus, existence edge (i, j) precludes additional edges (i + a, j b) (i a, j + b),
4. use special end state, rather explicitly modeling sentence length. Thus values n determined
stochastically.

349

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

a, b 0. linguistic perspective, assume edge (i, j) indicates words
xi xj share syntactic and/or semantic role bilingual parallel sentences.
perspective generative story, alignments treated draws distribution Ab . Since alignments always observed, remain agnostic distribution
Ab , except require assign zero probability alignments either: (i) align single
index one language multiple indices language (ii) contain crossing edges.
resulting alignments thus one-to-one, contain crossing edges, may sparse even
possibly empty. technique obtaining alignments display properties described
Section 4.2.
Generating Tag Sequences standard hidden Markov model part-of-speech tagging,
tags drawn Markov process transition distribution. permits probability
tag sequence factor across time steps. model employs similar factorization:
tags unaligned words drawn predecessors transition distribution, joined tag
nodes drawn product involving coupling parameter transition distributions
languages.
formally, given alignment sets transition parameters , factor
conditional probability bilingual tag sequence (y1 , ..., ym ), (y1 , ..., yn ) transition probabilities unaligned tags, joint probabilities aligned tag pairs:


P ((y1 , ..., ym ), (y1 , ..., yn )|a, , , ) =
yi1 (yi )
(yj )
j1

unaligned



unaligned j


P (yi , yj |yi1 , yj1
, , , ).

(1)

(i,j)a

alignment contains crossing edges, still model tags generated
sequentially stochastic process. define distribution aligned tag pairs product
languages transition probability coupling probability:

P (yi , yj |yi1 , yj1
, , , )

=

yi1 (yi )

j1

(yj )(yi , yj )

Z

normalization constant defined as:
X
yi1 (y)
Z=

j1

.

(2)

(y ) (y, ).

y,y

factorization allows language-specific transition probabilities shared across aligned
unaligned tags.
Another way view probability distribution product three experts: two transition parameters coupling parameter. Product-of-expert models (Hinton, 1999) allow
information source exercise strong negative influence probability tags
consider inappropriate, compared additive models. ideal setting,
prevents coupling distribution causing model generate tag unacceptable
perspective monolingual transition distribution. preliminary experiments found
multiplicative approach strongly preferable additive models.

350

fiM ULTILINGUAL PART- -S PEECH TAGGING

3.1.2 ERGED N ODE ODEL : NFERENCE
goal inference procedure obtain transition emission parameters
applied monolingual test data. Ideally would choose parameters highest
marginal probability, conditioned observed words x alignments a,
Z
, = arg max P (, , y, |x, a, 0 , 0 , 0 )dyd.
,

structure model permits us decompose joint probability, possible analytically marginalize hidden variables. resort standard Monte Carlo
approximation, marginalization performed sampling. repeatedly sampling
individual hidden variables according appropriate distributions, obtain Markov chain
guaranteed converge stationary distribution centered desired posterior. Thus,
initial burn-in phase, use samples approximate marginal distribution
desired parameter (Gilks, Richardson, & Spiegelhalter, 1996).
core element inference procedure Gibbs sampling (Geman & Geman, 1984). Gibbs
sampling begins randomly initializing unobserved random variables; iteration,
random variable ui sampled conditional distribution P (ui |ui ), ui refers
variables ui . Eventually, distribution samples drawn process
converge unconditional joint distribution P (u) unobserved variables. possible,
avoid explicitly sampling variables direct interest, rather integrate
them. technique known collapsed sampling; guaranteed never increase sampling
variance, often reduce (Liu, 1994).
merged node model, need sample part-of-speech tags priors.
able exactly marginalize emission parameters approximately marginalize transition
coupling parameters (the approximations required due re-normalized product
experts see details). draw repeated samples part-of-speech tags,
construct sample-based estimate underlying tag sequence. sampling, construct
maximum posteriori estimates parameters interest language, .
Sampling Unaligned Tags unaligned part-of-speech tags, conditional sampling equations
identical monolingual Bayesian hidden Markov model. probability tag decomposes three factors:
P (yi |yi , , x, x , 0 , 0 ) P (xi |yi , yi , xi , 0 )P (yi |yi1 , yi , 0 )P (yi+1 |yi , yi , 0 ), (3)
follows chain rule conditional independencies model. first factor
emission xi remaining two transitions. derive form
factor, marginalizing parameters .
emission factor, exactly marginalize emission distribution , whose prior
Dirichlet hyperparameter 0 . resulting distribution ratio counts, prior acts
pseudo-count:
Z
n(yi , xi ) + 0
yi (xi )P (yi |y, xi , 0 )dyi =
P (xi |y, xi , 0 ) =
.
(4)
n(yi ) + |Wyi |0
yi
Here, n(yi ) number occurrences tag yi yi , n(yi , xi ) number occurrences tag-word pair (yi , xi ) (yi , xi ), Wyi set word types vocabulary
351

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

W take tag yi . integral tractable due Dirichlet-multinomial conjugacy,
identical marginalization applied monolingual Bayesian HMM Goldwater Griffiths (2007).
unaligned tags, also possible exactly marginalize parameter governing transitions. transition 1 i,
Z
n(yi1 , yi ) + 0
yi1 (yi )P (yi |yi , 0 )dyi1 =
P (yi |yi1 , yi , 0 ) =
.
(5)
n(y
i1 ) + |T |0
yi1
factors similar emission probability: n(yi ) number occurrences
tag yi yi , n(yi1 , yi ) number occurrences tag sequence (yi1 , yi ),
tagset. probability transition + 1 analogous.
Jointly Sampling Aligned Tags situation tags aligned words complex.
sample tags jointly, considering |T | possibilities. begin decomposing
probability three factors:
P (yi , yj |yi , yj , x, x , a, 0 , 0 , , , ) P (xi |y, xi , 0 )P (xj |y , xj , 0 )P (yi , yj |yi , yj , a, , , ).
first two factors emissions, handled identically unaligned case (Formula 4). expansion final, joint factor depends alignment succeeding tags.
neither successors (in either language) aligned, product bilingual
coupling probability four transition probabilities:

P (yi , yj |yi , yj
, , , ) (yi , yj )yi1 (yi )yi (yi+1 )y

j1


(yj )y (yj+1
).
j

Whenever one succeeding words aligned, sampling formulas must account
effect sampled tag joint probability succeeding tags, longer
simple multinomial transition probability. give formula one casewhen
sampling joint tag pair (yi , yj ), whose succeeding words (xi+1 , xj+1 ) also aligned one
another:
"
#

yi (yi+1 ) (yj+1
)
j
P (yi , yj |yi , yj , a, , , ) (yi , yj )yi1 (yi ) (yj ) P
(t ) (t, ) . (6)
j1

(t)





t,t

j


= ,
Intuitively, puts probability mass single assignment yi+1 = t, yj+1
transitions + 1 j j + 1 irrelevant, final factor goes one.
Conversely, indifferent assigns equal probability pairs ht, i, final fac
tor becomes proportional yi (yi+1 )y (yj+1
), xi+1 xj+1
j
aligned. general, entropy increases, transition succeeding nodes exerts
greater influence yi yj . Similar equations derived cases succeeding tags
aligned other, one aligned another tag, e.g., xi+1 aligned xj+2 .
before, would like marginalize parameters , , . parameters
interact product-of-experts model, marginalizations approximations. form
marginalizations identical Formula 5. coupling distribution,

P (yi , yj |yi , yj
, 0 )

352

n(yi , yj ) + 0
,
N (a) + |T |0

(7)

fiM ULTILINGUAL PART- -S PEECH TAGGING

n(yi , yj ) number times tags yi yj aligned, excluding j, N (a)
total number alignments. above, prior 0 appears smoothing factor;
denominator multiplied dimensionality , size cross-product
two tagsets. Intuitively, approximation would exactly correct aligned tag
generated twice transition parameter coupling parameter instead
single time product experts.
alternative approximately marginalizing parameters would sample using
Metropolis-Hastings scheme work Snyder, Naseem, Eisenstein, Barzilay (2008).
use approximate marginalizations represents bias-variance tradeoff, decreased
sampling variance justifies bias introduced approximations, practical numbers
samples.
3.2 Multilingual Unsupervised Tagging: Latent Variable Model
model described previous section designed bilingual aligned data; see
Section 5, exploits data effectively. However, many resources contain
two languages: example, Europarl contains eleven, Multext-East corpus contains eight.
raises question best exploit available resources multi-aligned data
available.
One possibility would train separate bilingual models combine output test
time, either voting heuristic. However, believe cross-lingual information
reduces ambiguity training time, would preferable learn multiple languages jointly
training. Indeed, results Section 5 demonstrate joint training outperforms
voting scheme.
Another alternative would try extend bilingual model developed previous
section. extension possible principle, merged node model scale well
case multi-aligned data across two languages. Recall use merged nodes
represent tags aligned words; state space nodes grows |T |L , exponential
number languages L. Similarly, coupling parameter dimension,
counts required estimation become sparse number languages increases.
Moreover, bi-tag model required removing crossing edges word-alignment, avoid
cycles. unproblematic pairs aligned sentences, usually requiring removal less
5% edges (see Table 16 Appendix A). However, number languages grows,
increasing number alignments discarded.
Instead, propose new architecture specifically designed multilingual setting.
before, maintain HMM substructures language, learned parameters
easily applied monolingual data. However, rather merging tag nodes aligned words,
introduce layer superlingual tags. role latent nodes capture cross-lingual
patterns. Essentially perform non-parametric clustering sets aligned tags, encouraging
multilingual patterns occur elsewhere corpus.
concretely, every set aligned words, add superlingual tag outgoing edges
relevant part-of-speech nodes. example configuration shown Figure 1c. superlingual tags generated independently, influence selection part-of-speech
tags connected. before, use product-of-experts model combine
cross-lingual cues standard HMM transition model.

353

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

setup scales well. Crossing many-to-many alignments may used without creating
cycles, cross-lingual information emanates hidden superlingual tags. Furthermore,
size model parameter space scale linearly number languages.
describe role superlingual tags detail.
3.2.1 P ROPAGATING C ROSS - LINGUAL PATTERNS UPERLINGUAL TAGS
superlingual tag specifies set distributions one languages part-of-speech
tagset. order learn repeated cross-lingual patterns, need constrain number values
superlingual tags take thus number distributions provide. example,
might allow superlingual tags take integer values 1 K, integer
value indexing separate set tag distributions. set distributions correspond
discovered cross-lingual pattern data. example, one set distributions might favor nouns
language another might favor verbs, though heterogenous distributions (e.g., favoring
determiners one language prepositions others) also possible.
Rather fixing number superlingual tag values arbitrary size K, leave unbounded, using non-parametric Bayesian model. encourage desired multilingual clustering
behavior, use Dirichlet process prior (Ferguson, 1973). prior, high posterior probability obtained small number values used repeatedly. actual number
sampled values thus dictated data.
draw infinite sequence distribution sets 1 , 2 , . . . base distribution G0 .
()
set distributions tags, one distribution per language, written .
weight sets distributions, draw infinite sequence mixture weights 1 , 2 , . . .
stick-breaking process, defines distribution integers probability mass
placed initial set values. pair sequences 1 , 2 , . . . 1 , 2 , . . . define
distribution superlingual tags associated distributions parts-of-speech.
superlingual tag z N drawn probability z , associated set multinomials

hz , z , . . .i.
merged node model, distribution aligned part-of-speech tags governed
product experts. case, incoming edges superlingual tags (if any)
predecessor tag. combine distributions via normalized product. Assuming tag
position language connected superlingual tags, part-of-speech tag yi drawn
according to,
Q

yi1 (yi )
m=1 zm (yi )
yi
,
(8)
Z

yi1 indicates transition distribution, zm value mth connected superlingual
tag, zm (yi ) indicates tag distribution language given zm . normalization Z
obtained summing product possible values yi .
parameterization allows relatively simple parameter space. also leads desirable
property: tag high probability, incoming distributions must allow it. is,
expert veto potential tag assigning low probability, generally leading consensus
decisions.
formalize description giving stochastic generative process observed
data (raw parallel text alignments), according multilingual model.

354

fiM ULTILINGUAL PART- -S PEECH TAGGING

3.2.2 L ATENT VARIABLE ODEL : G ENERATIVE TORY
n languages, assume existence n tagsets 1 , . . . , n vocabularies, W 1 , . . . , W n ,
one language. Table 1 summarizes relevant parameters. clarity generative
process described using bigram transition dependencies, experiments use trigram
model, without bigram interpolations.
1. Transition Emission Parameters. language = 1, ..., n tag
, draw transition distribution tags emission distribution
words W , symmetric Dirichlet priors appropriate dimension.
2. Superlingual Tag Parameters. Draw infinite sequence sets distributions tags
1 , 2 , . . ., set n multinomials hi1 , i2 , . . . i, one n
languages. multinomial distribution tagset , drawn
symmetric Dirichlet prior; priors together comprise base distribution G0 ,
drawn.
time, draw infinite sequence mixture weights GEM (),
GEM () indicates stick-breaking distribution (Sethuraman, 1994) concentration
parameter = 1. parameters define distribution superlingual tags, equivalently part-of-speech distributions index:
P
k k=z
(9)
z
Pk
(10)

k k =k

=k defined one = k zero otherwise. Formula 10,
say set multinomials drawn Dirichlet process, conventionally written
DP (, G0 ).
3. Data. multilingual parallel sentence:
(a) Draw alignment multilingual alignment distribution . alignment
specifies sets aligned indices across languages; set may consist indices
subset languages.
(b) set indices a, draw superlingual tag value z according Formula 9.
(c) language , = 1, . . . (until end-tag reached):
i. Draw part-of-speech tag yi according Formula 8.
ii. Draw word wi W according emission distribution yi .

One important difference merged node model generative story distribution
multilingual alignments unconstrained: generate crossing many-to-one alignments needed. perform Bayesian inference model use Gibbs sampling,
marginalizing parameters whenever possible.

355

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

3.2.3 L ATENT VARIABLE ODEL : NFERENCE
section 3.1.2, employ sampling-based inference procedure. Again, standard closed forms
used analytically marginalize emission parameters , approximate marginalizations
applied transition parameters , superlingual tag distributions ; similar techniques
used marginalize superlingual tag mixture weights . before, approximations would
exact parameters numerator Formula 8 solely responsible
sampled tags.
still must sample part-of-speech tags superlingual tags z. remainder
section describes sampling equations variables.
Sampling Part-of-speech Tags
draw from:

sample part-of-speech tag language position


|yi , y(,i) , a, z)P (yi |y(,i) , a, z)
P (yi |y(,i) , x, a, z) P (xi |xi , )P (yi+1

(11)

y(,i) refers tags except yi . first factor handles emissions, latter two
factors generative probabilities (i) current tag given previous tag superlingual
tags, (ii) next tag given current tag superlingual tags. two quantities similar
equation 8, except integrate transition parameter yi1 superlingual tag
parameters z . end product integrals, compute closed form.
Terms involving transition distributions emission distributions identical
bilingual case, described Section 3.1.2. closed form integrating parameter
superlingual tag value z given by:
Z
n(z, yi , ) + 0
z (yi )P (z |0 )dz =
n(z, ) + 0
n(z, yi , ) number times tag yi observed together superlingual tag z
language , n(z, ) total number times superlingual tag z appears edge
language , 0 symmetric Dirichlet prior tags language .
Sampling Superlingual Tags set aligned words observed alignment need
sample superlingual tag z. Recall z index infinite sequence
h11 , . . . , 1n i, h21 , . . . , 2n i, . . . ,
z distribution tagset . generative distribution z given
Formula 9. sampling scheme, however, integrate possible settings mixture
weights using standard Chinese Restaurant Process closed form (Escobar & West, 1995):
(
1
fi
fi


n(zi ) zi zi
P yi fizi , zi , y(,i) k+
P zi fizi ,
(12)

otherwise
k+

first group factors product closed form probabilities tags connected
superlingual tag, conditioned zi . factors calculated manner
equation 11 above. final factor standard Chinese Restaurant Process closed form
posterior sampling Dirichlet process prior. factor, k total number sampled
superlingual tags, n(zi ) total number times value zi occurs sampled superlingual
tags, Dirichlet process concentration parameter (see Step 2 Section 3.2.2).
356

fiM ULTILINGUAL PART- -S PEECH TAGGING

3.3 Implementation
section describes implementation details necessary reproduce experiments.
present details merged node latent variable models, well monolingual baseline.
3.3.1 NITIALIZATION
initialization phase required generate initial settings word tags hyperparameters,
superlingual tags latent variable model. initialization follows:
Monolingual Model
Tags: Random, uniform probability among tag dictionary entries emitted
word.
Hyperparameters 0 , 0 : Initialized 1.0
Merged Node Model
Tags: Random, uniform probability among tag dictionary entries emitted
word. joined tag nodes, slot selected tag dictionary emitted
word appropriate language.
Hyperparameters 0 , 0 , 0 : Initialized 1.0
Latent Variable Model
Tags: Set final estimate monolingual model.
Superlingual Tags: Initially set 14 superlingual tag values assumed value
corresponds one part-of-speech tag. alignment assigned one 14 values
based common initial part-of-speech tag words alignment.
Hyperparameters 0 , 0 : Initialized 1.0
Base Distribution G0 : Set symmetric Dirichlet distribution parameter value
fixed 1.0
Concentration Parameter : Set 1.0 remains fixed throughout.
3.3.2 H YPERPARAMETER E STIMATION
models symmetric Dirichlet priors 0 0 , emission transition distributions respectively. merged node model also symmetric Dirichlet prior 0 coupling
parameter. re-estimate priors inference, based non-informative hyperpriors.
Hyperparameter re-estimation applies Metropolis-Hastings algorithm full epoch
sampling tags. addition, run initial 200 iterations speed convergence. MetropolisHastings sampling technique draws new value u proposal distribution, makes
stochastic decision whether accept new sample (Gelman et al., 2004). decision
based proposal distribution joint probability u observed sampled
variables x .
assume improper prior P (u) assigns uniform probability mass positive reals,
use Gaussian proposal distribution mean set previous value parameter
357

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

variance set one-tenth mean.5 non-pathological proposal distributions, MetropolisHastings algorithm guaranteed converge limit stationary Markov chain centered
desired joint distribution. observe acceptance rate approximately 1/6, line
standard recommendations rapid convergence (Gelman et al., 2004).
3.3.3 F INAL PARAMETER E STIMATES
ultimate goal training learn models applied unaligned monolingual data.
Thus, need construct estimates transition emission parameters .
sampling procedure focuses tags y. construct maximum posteriori estimates y, indicating likely tag sequences aligned training corpus. predicted tags
combined priors 0 0 construct maximum posteriori estimates transition
emission parameters. learned parameters applied monolingual test data find
highest probability tag sequences using Viterbi algorithm.
monolingual merged node models, perform 200 iterations sampling, select
modal tag settings slot. sampling found produce different results.
latent variable model, perform 1000 iterations sampling, select modal tag values
last 100 samples.

4. Experimental Setup
perform series empirical evaluations quantify contribution bilingual multilingual information unsupervised part-of-speech tagging. first evaluation follows standard
procedures established unsupervised part-of-speech tagging: given tag dictionary (i.e., set
possible tags word type), model selects appropriate tag token occurring
text. also evaluate tagger performance available dictionaries incomplete (Smith
& Eisner, 2005; Goldwater & Griffiths, 2007). scenarios, model trained using
untagged text.
section, first describe parallel data part-of-speech annotations used system
evaluation. Next describe monolingual baseline inference procedure used testing.
4.1 Data
source parallel data, use Orwells novel Nineteen Eighty Four original English
well translation seven languages Bulgarian, Czech, Estonian, Hungarian, Slovene,
Serbian Romanian.6 translation produced different translator published
print separately different publishers.
dataset representatives four language families Slavic, Romance, Ugric
Germanic. data distributed part publicly available Multext-East corpus, Version 3
(Erjavec, 2004). corpus provides detailed morphological annotation token level, including
part-of-speech tags. addition, lexicon language provided.
5. proposal identical parameter re-estimation applied emission transition priors Goldwater
Griffiths (2007).
6. initial publication (Snyder et al., 2008), used subset data, including sentences
one-to-one alignments four languages considered paper. current set-up makes use
sentences available corpus.

358

fiM ULTILINGUAL PART- -S PEECH TAGGING

Percentage Aligned
Bulgarian (BG)
Czech (CS)
English (EN)
Estonian (ET)
Hungarian (HU)
Romanian (RO)
Slovene (SL)
Serbian (SR)

Sentences
6681
6750
6736
6477
6767
6519
6688
6676

Words
101175
102834
118426
94900
98428
118330
116908
112131

BG

CS

EN

ET

HU

RO

SL

SR

41.0
43.2
35.7
32.2
35.5
39.3
41.4

41.7
36.4
42.4
32.0
27.5
49.4
44.4

50.5
41.9
42.9
39.6
42.5
45.2
43.2

33.5
39.1
34.4
32.6
23.4
36.4
33.6

31.3
30.7
32.9
33.8
22.4
29.1
26.6

41.5
31.7
42.5
29.2
26.9
31.2
33.9

45.4
56.2
44.6
44.8
34.6
30.8
53.4

45.9
48.4
40.9
39.7
30.3
32.1
51.2
-

Table 2: Percentage words row language alignments paired
column language.

corpus consists 118,426 English words 6,736 sentences (see Table 3). sentences, first 75% used training, taking advantage multilingual alignments.
remaining 25% used evaluation. evaluation, monolingual information made
available model, simulate performance non-parallel data.
4.2 Alignments
experiments use sentence-level alignments provided Multext-East corpus. Wordlevel alignments computed language pair using G IZA ++ (Och & Ney, 2003).
procedures handling alignments different merged node latent variable models.
4.2.1 ERGED N ODE ODEL
obtain 28 parallel bilingual corpora considering pairings eight languages.
generate one-to-one alignments word level, intersect one-to-many alignments going
direction. process results alignment half tokens bilingual parallel
corpus. automatically remove crossing alignment edges, would induce cycles
graphical model. employ simple heuristic: crossing alignment edges removed based
order appear left right; step eliminates average 3.62%
edges. Table 2 shows number aligned words language pair removing crossing
edges. detailed statistics total number alignments provided Appendix A.
4.2.2 L ATENT VARIABLE ODEL
previous setting, run GIZA ++ 28 pairings 8 languages, taking intersection alignments direction. Since want latent superlingual variable span
many languages possible, aggregate pairwise lexical alignments larger sets densely
aligned words assign single latent superlingual variable set. Specifically,
word token, consider set word word tokens aligned.
pairwise alignments occur 2/3 token pairs set, considered densely

359

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY




















Figure 2: example multilingual alignment configuration. Nodes correspond words tokens, labeled language. Edges indicate pairwise alignments produced
GIZA ++. Boxes indicate alignment sets, though set C1 subsumed C2
eventually discarded, described text.

connected admitted alignment set. Otherwise, increasingly smaller subsets considered one densely connected found. procedure repeated word tokens
corpus least one alignment. Finally, alignment sets pruned removing
subsets larger alignment sets. remaining sets considered site
latent superlingual variable.
process illustrated example. sentence know you, eyes seemed
say, see you, appears original English version corpus. English word
token seemed aligned word tokens Serbian (cinilo), Estonian (nais), Slovenian (zdelo).
Estonian Slovenian tokens aligned other. Finally, Serbian token aligned
Hungarian word token (mintha), aligned tokens. configuration
shown Figure 2, nodes labeled two-letter language abbreviations.
construct alignment sets words.
Hungarian word, one aligned word, Serbian, alignment
set consists pair (C1 figure).
Serbian word aligned partners Hungarian English; overall set
two pairwise alignments possible three, English Hungarian words
aligned. Still, since 2/3 possible alignments present, alignment set (C2)
formed. C1 subsumed C2, eliminated.
English word aligned tokens Serbian, Estonian, Slovenian; four six possible
links present, alignment set (C3) formed. Note Estonian Slovenian
words aligned would three six links, set
360

fiM ULTILINGUAL PART- -S PEECH TAGGING

would densely connected definition; would remove member
alignment set.
Estonian token aligned words Slovenian English; three pairwise alignments
present, alignment set (C4) formed. identical alignment set formed
starting Slovenian word, one superlingual tag created.
Thus, five word tokens, total three overlapping alignment sets created.
entire corpus, process results 284,581 alignment sets, covering 76% word tokens.
tokens, 61% occur exactly one alignment set, 29% occur exactly two alignment sets,
remaining 10% occur three alignment sets. alignment sets, 32% include
words two languages, 26% include words exactly three languages, remaining 42%
include words four languages. sets remain fixed sampling treated
model observed data.

Bulgarian (BG)
Czech (CS)
English (EN)
Estonian (ET)
Hungarian (HU)
Romanian (RO)
Slovene (SL)
Serbian (SR)

Number
Tokens
101175
102834
118426
94900
98428
118330
116908
112131

Tags per token lexicon contains ...
words count > 5 count > 10 top 100 words
1.39
4.61
5.48
7.33
1.35
5.27
6.37
8.24
1.49
3.11
3.81
6.21
1.36
4.91
5.82
7.34
1.29
5.42
6.41
7.85
1.55
4.49
5.53
8.54
1.33
4.59
5.49
7.23
1.38
4.76
5.73
7.61

Trigram
Entropy
1.63
1.64
1.51
1.61
1.62
1.73
1.64
1.73

Table 3: Corpus size tag/token ratio language set. last column shows
trigram entropy language based annotations provided corpus.

4.3 Tagset
Multext-East corpus manually annotated detailed morphosyntactic information.
experiments, focus main syntactic category encoded first letter provided
labels. annotation distinguishes 14 parts-of-speech, 11 common
languages experiments. Appendix B lists tag repository eight languages.
first experiment, assume complete tag lexicon available, set
possible parts-of-speech word known advance. use tag dictionaries provided
Multext-East corpus. average number possible tags per token 1.39. also experimented incomplete tag dictionaries, entries available words appearing
five ten times corpus. words, entire tagset 14 tags considered.
two scenarios, average per-token tag ambiguity 4.65 5.58, respectively. Finally
also considered case lexicon entries available 100 frequent words.
case average tags per token ambiguity 7.54. Table 3 shows specific tag/token ratio
language scenarios.
361

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Multext-East corpus, punctuation marks annotated part-of-speech tags.
expand tag repository defining separate tag punctuation marks. allows model
make use transition coupling patterns involving punctuation marks. However,
consider punctuation tokens computing model accuracy.
4.4 Monolingual Comparisons
monolingual baseline use unsupervised Bayesian hidden Markov model (HMM)
Goldwater Griffiths (2007). model, call BHMM1, modifies standard HMM
adding priors performing Bayesian inference. performance par state-ofthe-art unsupervised models. Bayesian HMM particularly informative baseline
model reduces baseline alignments data. implies
performance gain baseline attributed multilingual aspect model.
used implementation verifying performance Penn Treebank corpus
identical reported Goldwater Griffiths.
provide additional point comparison, use supervised hidden Markov model trained
using annotated corpus. apply standard maximum-likelihood estimation perform inference using Viterbi decoding pseudo-count smoothing unknown words (Rabiner, 1989).
Appendix C also report supervised results using Stanford Tagger, version 1.67 . Although results slightly lower supervised HMM implementation, note
system directly comparable set-up, allow use tag dictionary
constrain part-of-speech selections.
4.5 Test Set Inference
use procedure apply models (the monolingual model, bilingual merged
node model, latent variable model) test data. training, trigram transition word
emission probabilities computed, using counts tags assigned final training iteration.
Similarly, final sampled values hyperparameters selected smoothing parameters.
apply Viterbi decoding identify highest probability tag sequences monolingual
test set. report results multilingual monolingual experiments averaged five runs
bilingual experiments averaged three runs. average standard-deviation accuracy
multiple runs less 0.25 except lexicon limited 100 frequent words.
case standard deviation 1.11 monolingual model, 0.85 merged node model
1.40 latent variable model.

5. Results
section, first report performance two models full reduced lexicon
cases. Next, report results semi-supervised experiment, subset languages
annotated text training time. Finally, investigate sensitivity models hyperparameter values provide run time statistics latent variable model increasing numbers
languages.
7. http://nlp.stanford.edu/software/tagger.shtml

362

fiM ULTILINGUAL PART- -S PEECH TAGGING

1.
2.
3.
4.
5.
6.
7.

Random
Monolingual
ERGED N ODE: average
L ATENT VARIABLE
Supervised
ERGED N ODE: voting
ERGED N ODE: best pair

Avg
83.3
91.2
93.2
95.0
97.3
93.0
95.4

BG

CS

EN

ET

HU

RO

SL

SR

82.5
88.7
91.3
92.6
96.8
91.6
94.7

86.9
93.9
96.9
98.2
98.6
97.4
97.8

80.7
95.8
95.9
95.0
97.2
96.1
96.1

84.0
92.7
93.3
94.6
97.0
94.3
94.2

85.7
95.3
96.7
96.7
97.8
96.8
96.9

78.2
91.1
91.9
95.1
97.7
91.6
94.1

84.5
87.4
89.3
95.8
97.0
87.9
94.8

83.5
84.5
90.2
92.3
96.6
88.2
94.5

Table 4: Tagging accuracy complete tag dictionaries. first column reports average results
across languages (see Table 3 language name abbreviations). latent variable
model trained using eight languages, whereas merged node models trained
language pairs. latter case, results given averaging pairings (line 3),
bilingual models vote tag prediction (line 6), oracle
select best pairing target language (line 7). differences L ATENTVARIABLE, ERGED N ODE: voting, Monolingual (lines 2, 4, 6) statistically
significant p < 0.05 according sign test.

5.1 Full Lexicon Experiments
experiments show merged node latent variable models substantially improve
tagging accuracy. Since merged node model restricted pairs languages, provide
average results possible pairings. addition, also consider two methods combining
predictions multiple bilingual pairings: one using voting scheme employing
oracle select best pairings (see additional details).
shown Line 4 Table 4, merged node model achieves, average, 93.2% accuracy,
two percentage point improvement monolingual baseline.8 latent variable model
trained eight languages achieves 95% accuracy, nearly two percentage points higher
bilingual merged node model. two results correspond error reductions 23%
43% respectively, reduce gap unsupervised supervised performance
30% 60%.
mentioned above, also employ voting scheme combine information multiple
languages using merged node model. scheme, train bilingual merged node models
language pair. Then, making tag predictions particular language e.g., Romanian consider preferences bilingual model trained Romanian second
language. tag preferred plurality models selected. results method
shown line 6 Table 4, differ significantly average bilingual performance.
Thus, simple method combining information multiple language measure
joint multilingual model performance.
8. accuracy monolingual English tagger relatively high compared 87% reported Goldwater
Griffiths (2007) WSJ corpus. attribute discrepancy differences tag inventory used
data-set. example, Particles Prepositions merged WSJ corpus (as happen
tag inventory corpus), performance Goldwaters model WSJ similar report here.

363

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

improvement random

25

20

Monolingual
Merged Node
Latent Variable

15

10

5

Full Lexicon

Counts > 5

Counts > 10

Top 100

Figure 3: Summary model performance full reduced lexicon conditions. Improvement
random baseline indicated monolingual baseline, merged node
model (average performance possible bilingual pairings), latent variable
model (trained eight languages). Counts > x indicates words
counts greater x kept lexicon; Top 100 keeps 100 common words.

use sign test assess whether statistically significant differences accuracy tag predictions made monolingual baseline (line 2 Table 4), latent variable
model (line 4), voting-based merged node model (line 6). differences rows
found statistically significant p < 0.05. Note cannot use sign test compare
average performance bilingual model (line 3), since result aggregate accuracies
every language pair.
5.2 Reduced Lexicon Experiments
realistic application scenarios, may tag dictionary coverage across entire lexicon. consider three reduced lexicons: removing words counts five less;
removing words counts ten less; keeping top 100 frequent words.
Words removed lexicon take tag, increasing overall difficulty
task. results shown Table 5 graphically summarized Figure 3. cases,
monolingual model less robust reduction lexicon coverage multilingual models.
case 100 word lexicon, latent variable model achieves accuracy 57.9%, compared
53.8% monolingual baseline. merged node model, hand, achieves slightly
higher average performance 59.5%. two scenarios, latent variable model trained
eight languages outperforms bilingual merged node model, even oracle selects
best bilingual pairing target language. example, using lexicon words
appear greater five times, monolingual baseline achieves 74.7% accuracy, merged node
model using best possible pairings achieves 81.7% accuracy, full latent variable model
achieves accuracy 82.8%.

364

fiTop 100

Counts > 10

Counts > 5

ULTILINGUAL PART- -S PEECH TAGGING

Random
Monolingual
ERGED N ODE: average
L ATENT VARIABLE
ERGED N ODE: voting
ERGED N ODE: best pair
Random
Monolingual
ERGED N ODE: average
L ATENT VARIABLE
ERGED N ODE: voting
ERGED N ODE: best pair
Random
Monolingual
ERGED N ODE: average
L ATENT VARIABLE
ERGED N ODE: voting
ERGED N ODE: best pair

Avg
63.6
74.8
80.1
82.8
80.4
81.7
57.9
70.9
77.2
79.7
77.5
79.0
37.3
53.8
59.6
57.9
62.4
63.6

BG

CS

EN

ET

HU

RO

SL

SR

62.9
73.5
80.2
81.3
80.4
82.7
57.5
71.9
77.8
78.8
78.4
80.2
36.7
60.9
60.1
65.5
61.5
64.7

62
72.2
79
83.0
78.5
79.7
54.7
66.7
75.3
79.4
75.3
76.7
32.1
44.1
52.5
49.3
55.4
55.3

71.8
87.3
90.4
88.1
90.7
90.7
68.3
84.4
88.8
86.1
89.2
89.4
48.9
69.0
73.5
71.6
74.8
77.4

61.6
72.5
76.5
80.6
76.4
77.5
56
68.3
72.9
77.9
73.1
74.9
36.6
54.8
59.5
54.3
62.2
61.5

61.3
73.5
77.3
80.8
76.8
78
55.1
69.0
73.8
76.4
73.3
75.2
36.4
56.8
59.4
51.0
60.9
60.2

62.8
77.1
82.7
86.1
84.0
84.4
57.2
73.0
80.5
83.1
81.7
82.1
33.7
51.4
61.4
57.5
64.3
69.3

64.8
75.7
78.7
83.6
79.7
80.9
59.2
70.4
76.1
80.0
76.1
77.6
39.8
49.4
56.6
53.9
62.3
63.1

61.8
66.3
75.9
78.8
76.4
79.4
55.5
63.7
72.4
75.9
73.1
76.1
33.8
44.0
53.4
60.4
57.5
56.9

Table 5: Tagging accuracy reduced lexicon conditions. Counts > x indicates words
counts greater x kept lexicon; Top 100 keeps 100
common words. latent variable model trained using eight languages, whereas
merged node models trained language pairs. latter case, results given
averaging pairings, bilingual models vote tag prediction,
oracle select best pairing target language.
three pairs results marked , , , differences monolingual,
L ATENT VARIABLE, ERGED N ODE: voting statistically significant p <
0.05 according sign test.

Next consider performance bilingual merged node model lexicon
reduced one two languages. condition may occur dealing two languages asymmetric resources, terms unannotated text. shown Table 6, merged
models average scores 5.7 points higher monolingual model tag dictionaries reduced, 14.3 points higher partner language full tag dictionary.
suggests bilingual models effectively transfer additional lexical information available
resource-rich language resource-poor language, yielding substantial performance improvements.
Perhaps surprising result resource-rich language gains much average
pairing resource-poor partner language would gained pairing
language full lexicon. cases, average accuracy 93.2% achieved, compared
91.1% monolingual baseline.

365

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Monolingual
Reduced Full
60.9
88.7
44.1
93.9
69.0
95.8
54.8
92.7
56.8
95.3
51.4
91.1
49.4
87.4
44.0
84.5
53.8
91.2

BG
CS
EN
ET
HU
RO
SL
SR

Avg.

Bilingual (Merged Node)
Reduced language Unreduced language
71.3
91.6
66.7
97.1
82.4
95.8
65.6
93.3
63.0
96.7
69.3
91.5
63.3
89.1
63.6
90.3
68.1
93.2

reduced
60.1
52.5
73.5
59.5
59.4
61.4
56.6
53.4
59.5

full
91.3
96.9
95.9
93.3
96.7
91.9
89.3
90.2
93.2

Table 6: Various scenarios reducing tag dictionary 100 frequent terms.

5.3 Indirect Supervision
Although main focus paper unsupervised learning, also provide results indicating multilingual learning applied scenarios varying amounts annotated
data. scenarios fact quite realistic, previously trained highly accurate taggers
usually available least languages parallel corpus. apply latent
variable model scenarios simply treating tags annotated data (in subset
languages) fixed observed throughout sampling procedure. strictly probabilistic
perspective correct approach. However, note that, practice, heuristics objective functions place greater emphasis supervised portion data may yield better
results. explore possibility here.
supervised language(s)...
BG

accuracy for...

BG
CS
EN
ET
HU
RO
SL
SR

Avg

50.8
62.6
57.2
50.3
62.8
55.0
64.9
57.7

CS

EN

ET

HU

RO

SL

SR

others

None

69.1

68.0
52.2

65.9
50.2
68.1

60.4
51.2
61.8
56.1

67.1
51.0
61.9
56.4
51.1

73.9
56.6
80.6
59.8
49.8
62.9

69.6
53.1
69.5
57.1
50.0
59.2
56.2

76.2
76.6
82.8
72.5
62.3
74.9
77.7
72.5
74.4

65.5
49.3
71.6
54.3
51.0
57.5
53.9
60.4
57.9

70.5
58.0
50.0
61.6
56.8
65.9
61.7

57.7
53.1
61.3
55.6
64.1
58.9

51.4
57.8
53.2
63.5
58.6

58.5
54.4
61.6
57.7

54.7
63.4
57.9

69.9
64.8

59.2

Table 7: Performance latent variable model eight languages supervised annotations others frequent 100 words lexicon.
first eight columns report results one eight languages supervised.
penultimate column reports results one languages supervised.
final column reports results supervision available (repeated Table 5
convenience).

366

fiM ULTILINGUAL PART- -S PEECH TAGGING

Table 7 gives results two scenarios indirect supervision: one eight
languages annotated data, one languages annotated data.
cases, unsupervised languages provided 100 word lexicon, eight languages
trained together. one eight languages supervised, results vary depending
choice supervised language. one Bulgarian, Hungarian, Romanian supervised,
improvement seen, average, seven languages. However, Slovene supervised, improvement seen languages fairly substantial, average accuracy
rising 64.8%, 57.9% unsupervised latent variable model 53.8% monolingual baseline. Perhaps unsurprisingly, results impressive one
languages supervised. case, average accuracy lone unsupervised language rises
74.4%. Taken together, results indicate mixture supervised resources may
added mix simple straightforward way, often yielding substantial improvements
languages.
5.4 Hyperarameter Sensitivity Runtime Statistics
models employ hyperparameters emission transition distribution priors (0 0
respectively) merged node model employs additional hyperparameter coupling
distribution prior (0 ). hyperparameters updated throughout inference procedure.
latent variable model uses two additional hyperparameters remained fixed: concentration parameter Dirichlet process () parameter base distribution superlingual tags (0 ). experiments described used initialization values listed
Section 3.3.1. investigate sensitivity models different initializations 0 ,
0 , 0 , different fixed values 0 . Tables 8 9 show results obtained
merged node latent variable models, respectively, using full lexicon. observe
across wide range values, models yield similar results. addition, note
final sampled hyperparameter values transition emission distributions always fall one,
indicating sparse priors preferred.
mentioned Section 3.2 one key theoretical benefits latent variable approach
size model parameter space scale linearly number languages.
provide empirical confirmation running latent variable model possible subsets
eight languages, recording time elapsed run9 . Figure 4 shows average running
time number languages increased (averaged subsets size). see
model running time indeed scales linearly languages added, per-language running
time increases slowly: eight languages included, time taken roughly double
eight monolingual models run serially. models scale well tagset size
number examples. time dependence former cubic, use trigram models
employ Viterbi decoding find optimal sequences test-time. training time, however,
time scales linearly tagset size latent variable model quadratically
merged node model. due use Gibbs sampling isolates individual sampling
decision tags (for latent variable model) tag-pairs (for merged node model).
dependence number training examples also linear reason.
9. experiments single-threaded run using Intel Xeon 3.0 GHz processor

367

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

ERGED N ODE: hyperparameter initializations

0
0
0
BG
CS
EN
ET
HU
RO
SL
SR

Avg

1.0
1.0
1.0
91.3
96.9
95.9
93.3
96.7
91.9
89.3
90.2
93.2

0.1
1.0
1.0
91.3
97.0
95.9
93.4
96.7
91.8
89.3
90.2
93.2

0.01
1.0
1.0
91.3
97.0
95.9
93.3
96.7
91.8
89.3
90.2
93.2

1.0
0.1
1.0
91.3
96.9
95.9
93.4
96.7
91.9
89.3
90.2
93.2

1.0
0.01
1.0
91.2
96.8
95.9
93.2
96.7
91.8
89.4
90.2
93.2

1.0
1.0
0.1
91.1
96.5
95.9
93.4
96.7
91.8
89.3
90.2
93.1

1.0
1.0
0.01
91.3
97.1
95.9
93.2
96.8
91.8
89.3
90.2
93.2

Table 8: Results different initializations hyperparameters merged node model. 0 ,
0 0 hyperparameters transition, emission coupling multinomials
respectively. results language averaged possible pairings
languages.

L ATENT VARIABLE : hyperparameter initializations & settings


0
0
0
BG
CS
EN
ET
HU
RO
SL
SR

Avg

1.0
1.0
1.0
1.0
92.6
98.2
95.0
94.6
96.7
95.1
95.8
92.3
95.0

0.1
1.0
1.0
1.0
92.6
98.1
95.0
95.0
96.7
95.0
95.8
92.3
95.1

10
1.0
1.0
1.0
92.6
98.2
94.9
95.0
96.7
95.1
95.8
92.3
95.1

100
1.0
1.0
1.0
92.6
98.2
94.8
94.9
96.7
95.1
95.8
92.3
95.0

1.0
0.1
1.0
1.0
92.6
98.1
95.1
94.2
96.7
95.2
95.8
92.4
95.0

1.0
0.01
1.0
1.0
92.7
98.1
95.2
94.8
96.6
95.1
95.8
92.4
95.1

1.0
1.0
0.1
1.0
92.6
98.2
95.0
95.0
96.7
95.0
95.8
92.3
95.1

1.0
1.0
0.01
1.0
92.6
98.1
94.9
94.9
96.7
94.9
95.8
92.3
95.0

1.0
1.0
1.0
0.1
92.6
98.2
94.9
94.9
96.7
95.1
95.8
92.3
95.1

1.0
1.0
1.0
0.01
92.6
98.1
95.0
94.5
96.7
95.0
95.8
92.3
95.0

Table 9: Results different initializations settings hyperparameters latent variable
model. 0 0 hyperparameters transition emission multinomials respectively updated throughout inference. 0 concentration parameter
base distribution parameter, respectively, Dirichlet process, remain fixed.

6. Analysis
section provide analysis of: (i) factors influence effectiveness language
pairings bilingual models, (ii) incremental value adding languages latent vari368

fiM ULTILINGUAL PART- -S PEECH TAGGING

Figure 4: Average running time 1000 iterations latent variable model. Results averaged possible language subsets size. top line shows average
running time entire subset, bottom line shows running time divided
number languages.

able model, (iii) superlingual tags corresponding cross-lingual patterns learned
latent variable model, (iv) whether multilingual data helpful additional monolingual data. focus full lexicon scenario, though expect analysis extend
various reduced lexicon cases considered well.
6.1 Predicting Effective Language Pairings
first analyze cross-lingual variation performance different bilingual language pairings.
shown Table 10, performance merged node model target language varies
substantially across pairings. addition, identity optimally helpful language pairing
also depends heavily target language question. instance, Slovene, achieves large
improvement paired Serbian (+7.4), closely related Slavic language, minor improvement coupled English (+1.8). hand, Bulgarian, best
performance achieved coupling English (+6) rather closely related Slavic
languages (+2.4 +0). Thus, optimal pairings correspond simply language relatedness.
note applying multilingual learning morphological segmentation best results
obtained related languages, incorporating declarative knowledge
lower-level phonological relations using prior encourages phonologically close aligned
morphemes (Snyder & Barzilay, 2008). too, complex model models lower-level
morphological relatedness (such case) may yield better outcomes closely related languages.
upper bound merged node model performance, line 7 Table 10 shows results
selecting (with help oracle) best partner language. average accuracy using oracle 95.4%, substantially higher average bilingual pairing accuracy
93.2%, even somewhat higher latent variable model performance 95%. gap

369

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

performance motivates closer examination relationship languages constitute
effective pairings.
ERGED N ODE ODEL

coupled with...

accuracy for...

BG
CS
EN
ET
HU
RO
SL
SR

Avg
91.3
96.9
95.9
93.3
96.7
91.9
89.3
90.2

BG

95.3
96.1
93.0
96.8
94.1
88.5
88.5

CS

EN

ET

HU

RO

SL

SR

90.2

94.7
97.5

92.3
97.8
95.8

90.6
96.3
95.8
92.2

91.2
96.4
95.8
93.0
96.8

91.1
97.4
96.1
94.2
96.5
91.3

88.7
97.4
96.0
93.9
96.7
93.9
94.8

95.9
94.0
96.6
90.6
88.1
88.2

92.9
96.8
92.0
89.2
94.5

96.9
91.3
89.8
94.2

90.3
87.5
89.5

87.5
85.0

91.4

Table 10: Merged node model accuracy language pairs. row corresponds performance one language, column indicates language performance
achieved. best result language indicated bold. results
marked significantly higher monolingual baseline p < 0.05
according sign test.

6.1.1 C ROSS - LINGUAL E NTROPY
previous publication (Snyder et al., 2008) proposed using cross-lingual entropy posthoc explanation variation coupling performance. measure calculates entropy
tagging decision one language given identity aligned tag language.
cross-lingual entropy seemed correlate well relative performance four languages
considered publication, find correlate strongly eight languages
considered here. computed Pearson correlation coefficient (Myers & Well, 2002)
relative bilingual performance cross-lingual entropy. target language, rank
remaining seven languages based two measures: well paired language contributes
improved performance target, cross-lingual entropy target language given
coupled language. compute Pearson correlation coefficient two rankings
assess degree overlap. See Table 19 Appendix complete list results.
average, coefficient 0.29, indicating weak positive correlation relative bilingual
performance cross-lingual entropy.
6.1.2 LIGNMENT ENSITY
note even cross-lingual entropy exhibited higher correlation performance,
would little practical utility unsupervised scenario since estimation requires tagged
corpus. Next consider density pairwise lexical alignments language pairs
predictive measure coupled performance. Since alignments constitute multilingual
anchors models, practical matter greater alignment density yield greater opportunities cross-lingual transfer. linguistic viewpoint, measure may also indirectly
370

fiM ULTILINGUAL PART- -S PEECH TAGGING

capture correspondence two languages. Moreover, measure benefit computable untagged corpus, using automatically obtained GIZA ++ alignments.
before, target language, rank languages relative bilingual performance,
well percentage words target language provide alignments.
find average Pearson coefficient 0.42, indicating mild positive correlation. fact,
use alignment density criterion selecting optimal pairing decisions target language,
obtain average accuracy 94.67% higher average bilingual performance, still
somewhat performance multilingual model.
6.1.3 ODEL C HOICE
choice model may also contribute patterns variability observe across language
pairs. test hypothesis, ran latent variable model pairs languages. results
experiment shown Table 11. case merged node model, performance
target language depends heavily choice partner. However, exact patterns
variability differ case observed merged node model. measure variability, compare pairing preferences language two models.
specifically, target language rank remaining seven languages contribution
two models, compute Pearson coefficient two rankings.
seen last column Table 19 Appendix, find coefficient 0.49 two
rankings, indicating positive, though far perfect, correlation.
L ATENT VARIABLE ODEL

coupled with...

accuracy for...

BG
CS
EN
ET
HU
RO
SL
SR

Avg
91.9
97.2
95.7
93.9
96.8
93.2
90.5
91.6

BG

97.5
95.7
94.8
97.0
94.6
88.6
94.7

CS

EN

ET

HU

RO

SL

SR

92.2

91.9
97.5

91.6
97.6
95.7

91.6
97.4
95.6
92.3

92.1
97.4
95.7
93.9
96.8

92.3
96.5
95.7
94.5
96.6
94.4

91.8
96.8
95.8
94.1
96.8
94.7
94.6

95.7
94.3
96.8
92.1
87.7
88.5

93.4
96.7
92.4
92.4
94.5

96.7
92.3
95.2
94.5

92.1
87.5
89.7

87.6
88.0

91.1

Table 11: Accuracy latent variable model run language pairs. row corresponds
performance one language, column indicates language
performance achieved. best result language indicated bold.
results marked significantly higher monolingual
baseline p < 0.05 according sign test.

6.1.4 U TILITY



L ANGUAGE B ILINGUAL PARTNER

also analyze overall helpfulness language. before, target language,
rank remaining seven languages degree contribute increased target language performance paired bilingual model. ask whether helpfulness
371

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

rankings provided eight languages correlated one another words,
whether languages tend universally helpful (or unhelpful) whether helpfulness depends
heavily identity target language. consider pairs target languages, compute Pearson rank correlation rankings six supplementary languages
common (excluding two target languages themselves). average pairwise rank correlations obtain coefficient 0.20 merged node model 0.21
latent variable model. low correlations indicate language helpfulness depends crucially
target language question. Nevertheless, still compute average helpfulness
language (across target languages) obtain something like universal helpfulness ranking. See Table 20 appendix ranking. ask whether ranking correlates
language properties might predictive general helpfulness. compare universal helpfulness rankings10 language rankings induced tag-per-token ambiguity (the average
number tags allowed dictionary per token corpus) well trigram entropy (the
entropy tag distribution given previous two tags). cases assign highest
rank language lowest value, expect lower entropy ambiguity correlate
greater helpfulness. Contrary expectations, ranking induced tag-per-token ambiguity actually correlates negatively universal helpfulness rankings small amounts (-0.28
merged node model -0.23 latent variable model). models, Hungarian,
lowest tag-per-token ambiguity eight languages, worst universal helpfulness
ranking. correlations trigram entropy little predictable. case
latent variable model, correlation trigram entropy universal
helpfulness (-0.01). case merged node model, however, moderate positive
correlation (0.43).
6.2 Adding Languages Latent Variable Model
bilingual performance depends heavily choice language pair, latent variable
model easily incorporate available languages, obviating need choice. test
performance number languages increases, ran latent variable model possible subsets eight languages full lexicon well three reduced lexicon scenarios.
Figures 5, 6, 7, 8 plot average accuracy number available languages varies
four lexicon scenarios (in decreasing order lexicon size). comparison, monolingual
average bilingual baseline results given. scenarios, latent variable model steadily
gains accuracy number available languages increases, scenarios sees
appreciable uptick going seven eight languages. full lexicon case, gap supervised unsupervised performance cut nearly two thirds unsupervised
latent variable model eight languages.
Interestingly, lexicon reduced size, performance bilingual merged node
model gains relative latent variable model pairs. full lexicon case, latent variable
model clearly superior, whereas two moderately reduced lexicon cases, performance
pairs less two models. case drastically reduced lexicon
10. note universal helpfulness rankings obtained two multilingual models match
roughly: correlation coefficient one another 0.50. addition, universal context refers
eight languages consideration rankings could well change wider multilingual context.

372

fiM ULTILINGUAL PART- -S PEECH TAGGING

Figure 5: performance latent variable model number languages varies (averaged
subsets eight languages size). LEFT: Average performance across
languages. Scores monolingual bilingual merged node models given
comparison. RIGHT: Performance individual language number
available languages varies.

(100 words), merged node model clear winner. Thus, seems two models,
performance gains latent variable model sensitive size lexicon.
four figures (5, 6, 7, 8) also show multilingual performance broken
language. languages except English tend increase accuracy additional languages
added mix. Indeed, two cases moderately reduced lexicons (Figures 6 7) languages except English show steady large gains actually increase size going
seven full set eight languages. full lexicon case (Figure 5), Estonian, Romanian,
Slovene display steady increases end. Hungarian peaks two languages, Bulgarian
three languages, Czech Serbian seven languages. drastic reduced lexicon
case (Figure 8), performance across languages less consistent gains languages
added less stable. languages report gains going one two languages,
half increase steadily eight languages. Two languages seem trend downward
two three languages, two show mixed behavior.
full lexicon case (Figure 5), English language fails improve.
scenarios, English gains initially gains partially eroded languages
added. possible English outlier since significantly lower tag transition entropy
languages (see Table 3). Thus may internal tag transitions simply
informative English information gleaned multilingual context.

373

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Figure 6: performance latent variable model reduced lexicon scenario (Counts >
5), number languages varies (averaged subsets eight languages
size). LEFT: Average performance across languages. Scores monolingual
bilingual merged node models given comparison. RIGHT: Performance
individual language number available languages varies.

Figure 7: performance latent variable model reduced lexicon scenario (Counts >
10), number languages varies (averaged subsets eight languages
size). LEFT: Average performance across languages. Scores monolingual
bilingual merged node models given comparison. RIGHT: Performance
individual language number available languages varies.

374

fiM ULTILINGUAL PART- -S PEECH TAGGING

Figure 8: performance latent variable model reduced lexicon scenario (100
words), number languages varies (averaged subsets eight languages size). LEFT: Average performance across languages. Scores
monolingual bilingual merged node models given comparison. RIGHT:
Performance individual language number available languages varies.

6.3 Analysis Superlingual Tag Values
section analyze superlingual tags corresponding part-of-speech distributions,
learned latent variable model. Recall superlingual tag intuitively represents
discovered multilingual context tags multilingual information
propagated. formally, superlingual tag provides complete distribution partsof-speech language, allowing encoding primary secondary preferences
separately language. preferences interact language-specific context
(i.e. surrounding parts-of-speech corresponding word). place Dirichlet process
prior superlingual tags, number sampled values dictated complexity
data. fact, shown Table 12, number sampled superlingual tags steadily increases
number languages. multilingual contexts becomes complex diverse, additional
superlingual tags needed.
Number languages
Number superlingual tag values

2
11.07

3
12.57

4
13.87

5
15.07

6
15.79

7
16.13

8
16.50

Table 12: Average number sampled superlingual tag values number languages increases.

Next analyze part-of-speech tag distributions associated superlingual tag values.
superlingual tag values correspond low entropy tag distributions, single dominant
part-of-speech tag across languages. See, example, distributions associated superlin375

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

gual tag value 6 Table 13, favor nouns large margins. Similar sets distributions
occur favoring verbs, adjectives, primary part-of-speech categories. fact, among
seventeen sampled superlingual tag values, nine belong type, cover 80% actual
superlingual tag instances. remaining superlingual tags correspond complex crosslingual patterns. associated tag distributions cases favor different part-of-speech tags
various languages tend higher entropy, probability mass spread evenly
two three tags. One example set distributions associated superlingual
tag value 14 Table 13, seems mixed noun/verb class. six eight languages
favored tag verb, strong secondary choice cases noun. However,
Estonian Hungarian, preference reversed, nouns given higher probability.
superlingual tag may captured phenomenon light verbs, whereby verbs one
language correspond combination noun verb another language. example English verb whisper/V, translated Urdu, becomes collocation whisper/N do/V.
cases, verbs nouns often aligned one another, requiring complex superlingual
tag. analysis examples shows superlingual tags effectively learns simple
complex cross-lingual patterns

EN
ET
HU
RO
SL
SR

P (N ) = 0.91,
P (N ) = 0.92,
P (N ) = 0.97,
P (N ) = 0.91,
P (N ) = 0.85,
P (N ) = 0.90,
P (N ) = 0.94,
P (N ) = 0.92,

P (A) = 0.04,
P (A) = 0.03,
P (V ) = 0.00,
P (V ) = 0.03,
P (A) = 0.06,
P (A) = 0.04,
P (A) = 0.03,
P (A) = 0.03,

...
...
...
...
...
...
...
...

BG

14

CS

CS

TAG VALUE

TAG VALUE

6

BG

EN
ET
HU
RO
SL
SR

P (V ) = 0.66,
P (V ) = 0.60,
P (V ) = 0.55,
P (N ) = 0.52,
P (N ) = 0.44,
P (V ) = 0.45,
P (V ) = 0.55,
P (V ) = 0.49,

P (N ) = 0.21,
P (N ) = 0.22,
P (N ) = 0.25,
P (V ) = 0.29,
P (V ) = 0.34,
P (N ) = 0.33,
P (N ) = 0.24,
P (N ) = 0.26,

...
...
...
...
...
...
...
...

Table 13: Part-of-speech tag distributions associated two superlingual latent tag values. Probabilities two probable tags language shown.

6.3.1 P ERFORMANCE R EDUCED DATA
One potential objection claims made section improved results may due
merely addition data, multilingual aspect model may irrelevant.
test idea evaluating monolingual, merged node, latent variable systems training sets number examples reduced half. multilingual models setting
access exactly half much data monolingual model original experiment.
shown Table 14, monolingual baseline models quite insensitive drop
data. fact, models, trained half corpus, still outperform monolingual model trained entire corpus. indicates performance gains demonstrated
multilingual learning cannot explained merely addition data.

376

fiM ULTILINGUAL PART- -S PEECH TAGGING

ONOLINGUAL: full data
ONOLINGUAL: half data
ERGED N ODE: (avg.) full data
ERGED N ODE: (avg.) half data
L ATENT VARIABLE: full data
L ATENT VARIABLE: half data

Avg
91.2
91.0
93.2
93.0
95.0
94.7

BG

CS

EN

ET

HU

RO

SL

SR

88.7
88.8
91.3
91.1
92.6
92.6

93.9
93.8
96.9
96.6
98.2
97.8

95.8
95.7
95.9
95.7
95.0
94.7

92.7
92.6
93.3
92.7
94.6
93.9

95.3
95.3
96.7
96.7
96.7
96.7

91.1
90.2
91.9
92.0
95.1
94.4

87.4
87.5
89.3
88.9
95.8
95.4

84.5
84.5
90.2
89.9
92.3
92.2

Table 14: Tagging accuracy reduced training dataset, complete tag dictionaries; results
full training dataset repeated comparison. first column reports
average results across languages (see Table 3 language name abbreviations).

7. Conclusions
key hypothesis multilingual learning combining cues multiple languages,
structure becomes apparent. considered two ways applying intuition
problem unsupervised part-of-speech tagging: model directly merges tag structures
pair languages single sequence second model instead incorporates multilingual
context using latent variables.
results demonstrate incorporating multilingual evidence achieve impressive
performance gains across range scenarios. full lexicon available, two models
cut gap unsupervised supervised performance nearly one third (merged node
model, averaged pairs) two thirds (latent variable model, using eight languages).
one language, observe performance gains additional languages added. sole
exception English, gains additional languages reduced lexicon settings.
scenarios, latent variable model achieves better performance merged node
model, additional advantage scaling gracefully number languages.
observations suggest non-parametric latent variable structure provides flexible paradigm
incorporating multilingual cues. However, benefit latent variable model relative
merged node model (even running models pairs languages) seems decrease
size lexicon. Thus, practical scenarios small lexicon lexicon
available, merged node model may represent better choice.
experiments shown performance vary greatly depending choice
additional languages. difficult predict priori languages constitute good combinations.
particular, language relatedness cannot used consistent predictor sometimes
closely related languages constitute beneficial couplings sometimes unrelated languages
helpful. identify number features correlate bilingual performance, though
observe features interact complex ways. Fortunately, latent variable model
allows us bypass question simply using available languages.
models, lexical alignments play crucial role determine typology
model sentence. fact, observed positive correlation alignment
density bilingual performance, indicating importance high quality alignments.
experiments, considered alignment structure observed variable, produced standard MT

377

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

tools operate pairs languages. interesting alternative would incorporate
alignment structure model itself, find alignments best tuned tagging accuracy based
evidence multiple languages rather pairs.
Another limitation two models consider one-to-one lexical alignments.
pairing isolating synthetic languages11 would beneficial align short analytical
phrases consisting multiple words single morpheme-rich words language.
would involve flexibly aligning chunking parallel sentences throughout learning
process.
important direction future work incorporate even sources multilingual
information, additional languages declarative knowledge typological properties
(Comrie, 1989). paper showed performance improves number languages
increases. limited corpus eight languages, envision future work
massively parallel corpora involving dozens languages well learning languages
non-parallel data.

Bibliographic Note
Portions work previously presented two conference publications (Snyder, Naseem,
Eisenstein, & Barzilay, 2008, 2009). current article extends work several ways,
notably: present new inference procedure merged node model yields improved
results (Section 3.1.2) conduct extensive new empirical analyses multilingual results.
specifically, analyze properties language combinations contribute successful
multilingual learning, show adding multilingual data provides much greater benefit
increasing quantity monolingual data, investigate additional scenarios lexicon reduction, investigate scalability function number languages, experiment
semi-supervised settings (Sections 5 6).

Acknowledgments
authors acknowledge support National Science Foundation (CAREER grant IIS0448168 grants IIS-0835445, IIS-0904684) Microsoft Research Faculty Fellowship. Thanks Michael Collins, Tommi Jaakkola, Amir Globerson, Fernando Pereira, Lillian Lee,
Yoong Keok Lee, Maria Polinsky anonymous reviewers helpful comments suggestions. opinions, findings, conclusions recommendations expressed
authors necessarily reflect views NSF.
11. Isolating languages morpheme word ratio close one, synthetic languages
allow multiple morphemes easily combined single words. English example isolating language,
whereas Hungarian synthetic language.

378

fiM ULTILINGUAL PART- -S PEECH TAGGING

Appendix A. Alignment Statistics
BG
BG
CS
EN
ET
HU
RO
SL
SR

42163
51098
33849
31673
42017
45969
46434

CS

EN

ET

HU

RO

SL

SR

42163

51098
43067

33849
40207
40746

31673
31537
39012
32056

42017
32559
50289
27709
26455

45969
57789
52869
42499
34072
36442

46434
49740
48394
37681
29797
38004
59865

43067
40207
31537
32559
57789
49740

40746
39012
50289
52869
48394

32056
27709
42499
37681

26455
34072
29797

36442
38004

59865

Table 15: Number alignments per language pair

BG
BG
CS
EN
ET
HU
RO
SL
SR

2.77
6.13
3.36
4.04
4.52
2.95
3.48

CS

EN

ET

HU

RO

SL

SR

2.77

6.13
3.67

3.36
1.92
4.35

4.04
2.73
6.12
2.88

4.52
3.61
5.59
3.88
4.13

2.95
2.59
3.54
2.44
3.09
3.78

3.48
2.64
3.86
2.21
3.06
3.92
4.11

3.67
1.92
2.73
3.61
2.59
2.64

4.35
6.12
5.59
3.54
3.86

2.88
3.88
2.44
2.21

4.13
3.09
3.06

3.78
3.92

4.11

Avg.
3.89
2.85
4.75
3.01
3.72
4.20
3.22
3.33

Table 16: Percentage alignments removed per language pair

379

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Appendix B. Tag Repository

Adjective
Conjunction
Determiner
Interjection
Numeral
Noun
Pronoun
Particle
Adverb
Adposition
Article
Verb
Residual
Abbreviation

BG

CS

EN

ET

HU

RO

SL

SR

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

Table 17: Tag repository language

Appendix C. Stanford Tagger Performance
Language
BG
CS
EN
ET
HU
RO
SL
SR

Avg.

Accuracy
96.1
97.2
97.6
97.1
96.3
97.6
96.6
95.5
96.7

Table 18: Performance (supervised) Stanford tagger full lexicon scenario

380

fiM ULTILINGUAL PART- -S PEECH TAGGING

Appendix D. Rank Correlation

Language
BG
CS
EN
ET
HU
RO
SL
SR

Avg.
Language
BG
CS
EN
ET
HU
RO
SL
SR

Avg.

Performance correlates ERGED N ODE model
Cross-lingual entropy Alignment density L ATENT VARIABLE performance
-0.29
0.09
-0.09
0.39
0.34
0.24
0.28
0.77
0.42
0.46
0.56
0.56
0.31
-0.02
0.29
0.34
0.83
0.89
0.59
0.66
0.95
0.21
0.13
0.63
0.29
0.42
0.49
Performance correlates L ATENT VARIABLE model
Cross-lingual entropy Alignment density
ERGED N ODE performance
0.58
0.44
-0.09
-0.40
-0.44
0.24
0.67
0.41
0.42
0.14
0.32
0.56
-0.14
-0.72
0.29
0.04
0.68
0.89
0.57
0.54
0.95
0.18
0.10
0.68
0.21
0.17
0.49

Table 19: Pearson correlation coefficients bilingual performance target language
various rankings supplementary language. models target language, obtain ranking supplementary languages based bilingual
performance target language. rankings correlated characteristics bilingual pairing: cross-lingual entropy (the entropy tag distributions
target language given aligned tags supplementary language); alignment
density (the percentage words target language aligned words auxiliary
language); performance alternative model (target language performance
paired supplementary language alternative model).

381

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Appendix E. Universal Helpfulness
ERGED N ODE model
ET
2.43
EN
2.57
SL
3.14
BG
3.43
SR
3.43
RO
4.71
CS
5.00
HU
5.71

L ATENT VARIABLE model
BG
1.86
SR
3.00
ET
3.14
CS
3.71
EN
3.71
SL
3.71
RO
4.14
HU
6.00

Table 20: Average helpfulness rank language two models

382

fiM ULTILINGUAL PART- -S PEECH TAGGING

References
Baker, J. (1979). Trainable grammars speech recognition. Proceedings Acoustical
Society America.
Banko, M., & Moore, R. C. (2004). Part-of-speech tagging context. Proceedings
COLING, pp. 556561.
Bertoldi, N., Barbaiani, M., Federico, M., & Cattoni, R. (2008). Phrase-based statistical machine
translation pivot languages. International Workshop Spoken Language Translation
Evaluation Campaign Spoken Language Translation (IWSLT), pp. 143149.
Bhattacharya, I., Getoor, L., & Bengio, Y. (2004). Unsupervised sense disambiguation using bilingual probabilistic models. ACL 04: Proceedings 42nd Annual Meeting Association Computational Linguistics, p. 287, Morristown, NJ, USA. Association Computational Linguistics.
Brill, E. (1995). Transformation-based error-driven learning natural language processing:
case study part-of-speech tagging. Computational Linguistics, 21(4), 543565.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1991). Word-sense disambiguation
using statistical methods. Proceedings ACL, pp. 264270.
Chen, Y., Eisele, A., & Kay, M. (2008). Improving statistical machine translation efficiency
triangulation. Proceedings LREC.
Chiang, D. (2005). hierarchical phrase-based model statistical machine translation. Proceedings ACL, pp. 263270.
Cohn, T., & Lapata, M. (2007). Machine translation triangulation: Making effective use multiparallel corpora. Proceedings ACL.
Comrie, B. (1989). Language universals linguistic typology: Syntax morphology. Oxford:
Blackwell.
Dagan, I., Itai, A., & Schwall, U. (1991). Two languages informative one. Proceedings ACL, pp. 130137.
Diab, M., & Resnik, P. (2002). unsupervised method word sense tagging using parallel
corpora. Proceedings ACL, pp. 255262.
Erjavec, T. (2004). MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons corpora. Fourth International Conference Language Resources Evaluation, LREC, Vol. 4, pp. 15351538.
Escobar, M., & West, M. (1995). Bayesian density estimation inference using mixtures. Journal
american statistical association, 90(230), 577588.
Ferguson, T. (1973). bayesian analysis nonparametric problems. annals statistics,
1, 209230.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004). Bayesian data analysis. Chapman
Hall/CRC.
Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, Bayesian
restoration images. IEEE Transactions Pattern Analysis Machine Intelligence,
6, 721741.
383

fiNASEEM , NYDER , E ISENSTEIN & BARZILAY

Genzel, D. (2005). Inducing multilingual dictionary parallel multitext related languages.
Proceedings HLT/EMNLP, pp. 875882.
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo practice.
Chapman & Hall/CRC.
Goldwater, S., & Griffiths, T. L. (2007). fully Bayesian approach unsupervised part-of-speech
tagging. Proceedings ACL, pp. 744751.
Haghighi, A., & Klein, D. (2006). Prototype-driven learning sequence models. Proceedings
HLT-NAACL, pp. 320327.
Hinton, G. E. (1999). Products experts. Proceedings Ninth International Conference
Artificial Neural Networks, Vol. 1, pp. 16.
Johnson, M. (2007). doesnt EM find good HMM POS-taggers?.
EMNLP/CoNLL, pp. 296305.

Proceedings

Kuhn, J. (2004). Experiments parallel-text based grammar induction. Proceedings ACL,
p. 470.
Li, C., & Li, H. (2002). Word translation disambiguation using bilingual bootstrapping. Proceedings ACL, pp. 343351.
Liu, J. S. (1994). collapsed Gibbs sampler Bayesian computations applications
gene regulation problem. Journal American Statistical Association, 89(427), 958966.
Merialdo, B. (1994). Tagging english text probabilistic model. Computational Linguistics,
20(2), 155171.
Mihalcea, R. (2004). Current Issues Linguistic Theory: Recent Advances Natural Language
Processing, chap. Unsupervised Natural Language Disambiguation Using Non-Ambiguous
Words. John Benjamins Publisher.
Myers, J. L., & Well, A. D. (2002). Research Design Statistical Analysis (2nd edition).
Lawrence Erlbaum.
Ng, H. T., Wang, B., & Chan, Y. S. (2003). Exploiting parallel texts word sense disambiguation:
empirical study. Proceedings ACL, pp. 455462.
Och, F. J., & Ney, H. (2001). Statistical multi-source translation. MT Summit 2001, pp. 253258.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment models.
Computational Linguistics, 29(1), 1951.
Pado, S., & Lapata, M. (2006). Optimal constituent alignment edge covers semantic projection. Proceedings ACL, pp. 1161 1168.
Rabiner, L. R. (1989). tutorial hidden markov models selected applications speech
recognition. Proceedings IEEE, 77(2), 257286.
Resnik, P., & Yarowsky, D. (1997). perspective word sense disambiguation methods
evaluation. Proceedings ACL SIGLEX Workshop Tagging Text Lexical
Semantics: Why, What, How?, pp. 7986.
Sethuraman, J. (1994). constructive definition Dirichlet priors. Statistica Sinica, 4(2), 639650.

384

fiM ULTILINGUAL PART- -S PEECH TAGGING

Smith, N. A., & Eisner, J. (2005). Contrastive estimation: Training log-linear models unlabeled
data. Proceedings ACL, pp. 354362.
Snyder, B., & Barzilay, R. (2008). Unsupervised multilingual learning morphological segmentation. Proceedings ACL/HLT, pp. 737745.
Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2008). Unsupervised multilingual learning
pos tagging. Proceedings EMNLP.
Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2009). Adding languages improves
unsupervised multilingual part-of-speech tagging: bayesian non-parametric approach.
Proceedings NAACL/HLT.
Toutanova, K., & Johnson, M. (2008). Bayesian LDA-based model semi-supervised part-ofspeech tagging. Advances Neural Information Processing Systems 20, pp. 15211528.
MIT Press.
Utiyama, M., & Isahara, H. (2006). comparison pivot methods phrase-based statistical
machine translation. Proceedings NAACL/HLT, pp. 484491.
Wu, D. (1995). Stochastic inversion transduction grammars, application segmentation,
bracketing, alignment parallel corpora. IJCAI, pp. 13281337.
Wu, D., & Wong, H. (1998). Machine translation stochastic grammatical channel. Proceedings ACL/COLING, pp. 14081415.
Xi, C., & Hwa, R. (2005). backoff model bootstrapping resources non-English languages.
Proceedings EMNLP, pp. 851 858.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2000). Inducing multilingual text analysis tools via
robust projection across aligned corpora. Proceedings HLT, pp. 161168.

385

fiJournal Artificial Intelligence Research 36 (2009) 165228

Submitted 03/09; published 10/09

Hypertableau Reasoning Description Logics
Boris Motik
Rob Shearer
Ian Horrocks

boris.motik@comlab.ox.ac.uk
rob.shearer@comlab.ox.ac.uk
ian.horrocks@comlab.ox.ac.uk

Computing Laboratory, University Oxford
Wolfson Building
Parks Road
Oxford OX1 3QD
United Kingdom

Abstract
present novel reasoning calculus description logic SHOIQ+ knowledge
representation formalism applications areas Semantic Web. Unnecessary
nondeterminism construction large models two primary sources inefficiency
tableau-based reasoning calculi used state-of-the-art reasoners. order reduce
nondeterminism, base calculus hypertableau hyperresolution calculi,
extend blocking condition ensure termination. order reduce size
constructed models, introduce anywhere pairwise blocking. also present improved
nominal introduction rule ensures termination presence nominals, inverse
roles, number restrictionsa combination DL constructs proven notoriously
difficult handle. implementation shows significant performance improvements
state-of-the-art reasoners several well-known ontologies.

1. Introduction
Description Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2007)
family knowledge representation formalisms well-understood formal properties.
DLs applied numerous problems computer science information
integration metadata management. Recent interest DLs spurred
application Semantic Web: DL SHOIQ provides logical underpinning
Web Ontology Language (OWL) (Patel-Schneider, Hayes, & Horrocks, 2004),
DL SROIQ (Kutz, Horrocks, & Sattler, 2006) used OWL 2an extension OWL
currently standardized World Wide Web Consortium.
central component DL applications efficient scalable reasoner. Modern reasoners, Pellet (Parsia & Sirin, 2004), FaCT++ (Tsarkov & Horrocks, 2006),
RACER (Haarslev & Moller, 2001), typically based tableau calculi (Baader &
Nutt, 2007), demonstrate (un)satisfiability knowledge base K via constructive search abstraction model K. Numerous optimizations developed
effort reduce size search space (Horrocks, 2007). Despite major advances
tableau reasoning algorithms, however, ontologies still encountered practice
cannot handled existing DL reasoners. Two main sources complexity tableau
calculi identified literature (Donini, 2007).
first source complexity known or-branching: given disjunctive assertion

c
2009
AI Access Foundation. rights reserved.

fiMotik, Shearer, & Horrocks

(C D)(s), tableau algorithm nondeterministically guesses either C(s) D(s) holds.
show unsatisfiability K, every possible guess must lead contradiction:
assuming C(s) holds leads contradiction, algorithm must backtrack assume
D(s) holds, give rise exponential behavior. General concept inclusions
(GCIs)implications form C v Dare main source disjunctions: ensure
C v holds, tableau algorithm adds disjunction (C D)(s) individual
model. Various absorption optimizations (Horrocks, 1998; Tsarkov & Horrocks, 2004;
Hudek & Weddell, 2006; Horrocks, 2007) developed reduce nondeterminism
tableau calculi.
second source complexity tableau calculi known and-branching:
expansion model due existential quantifiers generate large models. Apart
memory consumption problems, and-branching increase or-branching increasing
number individuals GCIs applied.
paper, present reasoning calculus addresses sources complexity. focus DL SHOIQ+ , obtained extending SHOIQ local
reflexivity disjoint, reflexive, irreflexive, symmetric, asymmetric roles. SROIQ
extends SHOIQ+ generalized role inclusions form R1 . . . Rn v R.
Generalized role inclusions encoded using standard GCIs proposed Demri
de Nivelle (2005); thus, adding suitable preprocessing phase, results
paper allow us handle SROIQ (and hence OWL 2) well.
algorithm viewed hybrid resolution tableau, related
hypertableau (Baumgartner, Furbach, & Niemela, 1996) hyperresolution (Robinson,
1965) calculi. first preprocesses SHOIQ+ knowledge base set DL-clauses
universally quantified implications containing DL concepts roles predicates.
main derivation rule DL-clauses hyperresolution: atom consequent
DL-clause derived atoms DL-clause antecedent matched
already derived consequences. Hyperresolution effective restricting or-branching.
Consider, example, following example:
(1)

R(x, y1 ) S(x, y2 ) A(x) B(y1 ) C(y2 )

DL-clause derives disjunction applied assertions form R(a, b)
S(c, d) = c. presence variables (1) allows us simultaneously work
individuals a, b, c d, check whether = c. contrast, derivation rules
tableau algorithms consider pairs individuals; consequently, absorption
technique aware localize nondeterminism individuals satisfy
mentioned constraints. discuss detail Section 3.3.1, calculus generalizes
known absorption variants. Furthermore, contrast absorption techniques,
algorithm guaranteed exhibit nondeterminism Horn knowledge bases (Hustadt,
Motik, & Sattler, 2005) GALEN, NCI, SNOMED CT (see Section 7). Finally,
calculus provides uniform proof-theoretic framework handle several useful
extensions commonly used DLs (see Section 4.1.3).
Hyperresolution decides many fragments first-order logic (e.g., Fermuller, Leitsch,
Hustadt, & Tammet, 2001; Fermuller, Tammet, Zamov, & Leitsch, 1993), well description modal logics (e.g., Georgieva, Hustadt, & Schmidt, 2003; Hustadt & Schmidt,
1999). Unlike fragments, SHOIQ+ allows cyclic GCIs form
166

fiHypertableau Reasoning Description Logics

C v R.C, hyperresolution generate infinite paths successors. ensure
termination, use pairwise blocking technique (Horrocks, Sattler, & Tobies, 2000b)
detect cyclic computations. Due hyper-inferences, soundness correctness
proofs Horrocks et al. (2000b) carry immediately calculus; fact,
certain simpler blocking conditions applicable weaker DLs cannot straightforwardly
transferred setting. limit and-branching, extend blocking condition
Horrocks et al. anywhere pairwise blocking: individual blocked another
individual necessarily ancestor, reduce sizes constructed
models. Anywhere blocking already used single blocking (Buchheit, Donini,
& Schaerf, 1993; Baader, Buchheit, & Hollunder, 1996; Donini & Massacci, 2000; Donini,
Lenzerini, Nardi, & Schaerf, 1998); however, best knowledge,
neither used sophisticated pairwise blocking tested practice.
Ensuring termination tableau decision procedure DLs nominals, inverse
roles, number restrictions proven notoriously difficult. problem finally
solved Horrocks Sattler (2007) extending tableau calculus nominal
introduction rule. certain situations, rule guesses introduces new nominals,
thus potential source inefficiency practice. paper, present variant
rule simpler efficient.
implemented calculus new reasoner called HermiT.1 Even rather
nave implementation, deterministic treatment GCIs significantly reduces classification times several real-world ontologies. Furthermore, pairwise anywhere blocking seems
effective limiting model sizes allows HermiT classify several ontologies
that, best knowledge, reasoner handle.

2. Preliminaries
define syntax semantics description logic SHOIQ+ . signature
triple = (NR , NC , NI ) consisting mutually disjoint sets atomic roles NR , atomic
concepts NC , individuals NI . set roles NR {R | R NR }. function inv() defined set roles follows, R atomic role: inv(R) = R
inv(R ) = R. RBox R finite set axioms form R1 v R2 (role inclusion),
Dis(S1 , S2 ) (role disjointness), Ref(R) (reflexivity), Irr(S) (irreflexivity), Sym(R) (symmetry), Asy(S) (asymmetry), Tra(R) (transitivity), R, R1 , R2 roles,
S, S1 , S2 simple roles, defined next. Let vR reflexive-transitive closure
following relation: {(R1 , R2 ) | R1 v R2 R inv(R1 ) v inv(R2 ) R}. role R
transitive R role R0 exists R0 vR R, R vR R0 , either Tra(R0 ) R
Tra(inv(R0 )) R. role simple transitive role R exists R vR S.
set concepts smallest set containing > (the top concept), (the bottom concept),
(atomic concept), {a} (nominal ), C (negation), C u (conjunction), C (disjunction), R.C (existential restriction), R.C (universal restriction), S.Self (local reflexivity),
n S.C (at-least restriction), n S.C (at-most restriction), atomic concept,
individual, C concepts, R role, simple role, n nonnegative integer.
TBox finite set general concept inclusions (GCIs) C v C concepts. ABox finite set assertions form C(a) (concept assertion), R(a, b)
1. http://www.hermit-reasoner.com/

167

fiMotik, Shearer, & Horrocks

Table 1: Model-Theoretic Semantics SHOIQ+
Interpretation Concepts Roles
= 4I
=
= {sI }
(C)I = 4I \ C


=C
(C D)I = C DI

= {hy, xi | hx, yi R }
(S.Self)I = {x | hx, xi }
= {x | : hx, yi RI C }
= {x | : hx, yi RI C }
= {x | ]{y | hx, yi C } n}
= {x | ]{y | hx, yi C } n}
Satisfaction Axioms Interpretation

|= C v iff C DI
|= R1 v R2 iff R1I R2I


|= Ref(R) iff x 4 : hx, xi R
|= Irr(S)
iff x 4I : hx, xi 6
|= Sym(R) iff RI (inv(R))I
|= Asy(S)
iff (inv(S))I =

+

|= Tra(R) iff (R ) R
|= Dis(S1 , S2 ) iff S1I S2I =
|= C(a)
iff aI C
|= R(a, b)
iff haI , bI RI


|= b
iff = b
|= 6 b
iff aI 6= bI
+
Note: ]N number elements N , R transitive closure R.
>I
{s}I
(C u D)I
(R )I
(R.C)I
(R.C)I
( n S.C)I
( n S.C)I

(role assertion), b (equality assertion), 6 b (inequality assertion), C
concept, R role, b individuals. SHOIQ+ knowledge base K triple
(R, , A). |K| denote size Kthat is, number symbols required
encode K input tape Turing machine (numbers coded binary).
interpretation K tuple = (4I , ), 4I nonempty set,
assigns element aI 4I individual a, set AI 4I atomic concept A,
relation RI 4I 4I atomic role R. function extended concepts
roles shown upper part Table 1. model K, written |= K,
satisfies axioms K shown lower part Table 1. basic inference problem
SHOIQ+ checking whether K satisfiablethat is, checking whether model K
exists. concept C subsumes concept D, written K |= C v D, C DI model
K. easy see K |= C v K {(C u D)(a)} unsatisfiable,
individual occur K (Baader & Nutt, 2007).
negation-normal form nnf(C) concept C concept obtained C
using de Morgans laws, dualities existential universal restrictions,
dualities at-least at-most restrictions push negations inwards
occur front atomic concepts, nominals, local reflexivity concepts.
concept nnf(C) logically equivalent C, computed C time linear
size C (Baader & Nutt, 2007). use C
denote nnf(C).
mentioned Section 1, extending SHOIQ+ general role inclusions would yield
SROIQ (Kutz et al., 2006)the DL underpins OWL 2. ALCHOIQ+ obtained
SHOIQ+ disallowing transitivity axioms. SHIQ+ obtained SHOIQ+
disallowing nominals. SHOQ+ obtained SHOIQ+ disallowing inverse roles.
SHOIQ SHIQ obtained SHOIQ+ SHIQ+ , respectively, disallow168

fiHypertableau Reasoning Description Logics

ing local reflexivity, role disjointness, reflexivity, irreflexivity, symmetry, asymmetry
axioms. Finally, SHOI obtained SHOIQ disallowing at-least at-most
restrictions.

3. Motivation Algorithm Overview
section, present overview main aspects algorithm. explain
Section 3.1 root causes scalability problems encountered tableau algorithms,
Section 3.2 outline address them. Finally, Section 3.3 discuss
relationship algorithm related approaches.
3.1 Causes Scalability Problems Tableau Algorithms
show knowledge base K = (R, , A) satisfiable, tableau algorithm constructs
derivationa sequence ABoxes A0 , A1 , . . . , A0 = Ai obtained
Ai1 application one derivation rule.2 derivation rules make information implicit axioms R explicit, thus evolve ABox towards
(representation a) model K. algorithm terminates either derivation rule
applicable , case represents model K, contains
obvious contradiction, case model construction failed. following
derivation rules commonly used DL tableau calculi.
t-rule: Given (C1 C2 )(s), derive either C1 (s) C2 (s).
u-rule: Given (C1 u C2 )(s), derive C1 (s) C2 (s).
-rule: Given (R.C)(s), derive R(s, t) C(t) fresh individual.
-rule: Given (R.C)(s) R(s, t), derive C(t).
v-rule: Given GCI C v individual s, derive (C D)(s).
t-rule nondeterministic: (C1 C2 )(s) true, C1 (s) C2 (s) true.
Therefore, tableau calculi make nondeterministic guess choose either C1 C2 ;
one choice leads contradiction, algorithm must backtrack try choice.
Thus, K unsatisfiable choices lead contradiction. next discuss two
sources complexity inherent tableau derivation rules.
3.1.1 Or-Branching
Handing disjunctions reasoning case often called or-branching. v-rule
adds disjunction GCI individual ABox thus major source
or-branching inefficiency (Horrocks, 2007). Consider, example, knowledge base
K1 = (, T1 , A1 ), T1 A1 specified follows:
(2)

T1 = {R.A v A}
A1 = {A(a0 ), R(a0 , b1 ), R(b1 , a1 ), . . . , R(an1 , bn ), R(bn , ), A(an )}

2. formalizations tableau algorithms work completion graphs (Horrocks & Sattler, 2007),
natural correspondence ABoxes.

169

fiMotik, Shearer, & Horrocks

a0
(i)
(ii)
(iii)
(iv)


R.A
R.A

R

R

b1
R.A

R.A

a1

an1

R.A
R.A


R.A
R.A


R

bn
R.A

R.A

R



R.A
R.A


Figure 1: Or-Branching Example

ABox A1 graphically shown Figure 1. individuals occurring ABox
represented black dots, assertion form A(a0 ) represented placing next
individual a0 , assertion form R(a0 , b1 ) represented R-labeled
arrow a0 b1 . Initially, A1 contains concept assertions shown line (i ).
satisfy GCI T1 , tableau algorithm applies v-rule, thus adding assertions shown line (ii ) Figure 1. Tableau algorithms usually free choose order
process assertions ABox; fact, finding order exhibits good
performance practice requires advanced heuristics (Tsarkov & Horrocks, 2005b). Let us
assume algorithm chooses process assertions ai bj . Hence,
applying derivation rules ai , tableau algorithm derives assertions shown
line (iii ) Figure 1; that, applying derivation rules bi , algorithm
derives assertions shown line (iv ) Figure 1. ABox contains A(an )
A(an ), contradiction. Thus, algorithm needs backtrack
recent choice, flips guess bn1 A(bn1 ). generates contradiction
bn1 , algorithm backtracks guesses bi , changes guess A(an ),
repeats work bi . also leads contradiction, algorithm must
revise guess an1 ; then, two guesses possible . general,
revising guess ai , possibilities aj , < j n, must reexamined, results
exponential behavior. None standard backtracking optimizations (Horrocks, 2007)
helpful: problem arises order individuals processed
makes guesses ai independent guesses aj 6= j.
GCI R.A v A, however, inherently nondeterministic: equivalent
Horn clause x, : [R(x, y) A(y) A(x)], applied bottom-up derive assertions A(bn ), A(an1 ), . . . , A(a0 ) eventually reveal contradiction a0 .
inferences deterministic,3 conclude K1 unsatisfiable without
backtracking. example suggests processing GCIs tableau algorithms
unnecessarily nondeterministic. Hustadt et al. (2005) identified class
knowledge bases without unnecessary nondeterminism: knowledge bases expressed
description logic Horn-SHIQ always translated Horn clauses, suggesting
reasoning without nondeterminism possible principle. Ideally, practical DL
reasoning procedure exhibit nondeterminism Horn knowledge bases.
3. precisely, inference deterministic, order inferences performed
dont-care nondeterministic.

170

fiHypertableau Reasoning Description Logics














(a) Ancestor Blocking

(b) Anywhere Blocking

Figure 2: And-Branching Example

context tableau calculi, various absorption optimizations (Horrocks, 2007)
developed control nondeterminism arising application GCIs.
discuss optimizations depth Section 3.3.1.
3.1.2 And-Branching
introduction new individuals -rule often called and-branching,
another major source inefficiency tableau algorithms (Donini, 2007). Consider,
example, (satisfiable) knowledge base K2 = (, T2 , A2 ), T2 A2 specified
follows (where n integers):

(3)

T2 = { A1 v 2 S.A2 , . . . , An1 v 2 S.An , v A1 ,
Ai v (B1 C1 ) u . . . u (Bm Cm ) 1 n }
A2 = { A1 (a) }

At-least restrictions dealt tableau algorithms -rule, quite
similar -rule: ( n R.C)(s), -rule derives R(s, ti ) C(ti ) 1 n,
ti 6 tj 1 < j n. Thus, assertion A1 (a) implies existence least two
individuals A2 , imply existence least two individuals A3 , on.
Given K2 , tableau algorithm thus constructs binary tree, shown Figure 2a,
individual labeled Ai element = {B1 , C1 } . . . {Bm , Cm }.
individuals tree depth n instances ; GCI v A1 ,
individuals must instances A1 well, repeat whole construction
generate even deeper tree. Clearly, nave application tableau rules
terminate TBox contains existential quantifiers cycles.
ensure termination cases, tableau algorithms employ blocking (Baader & Nutt,
2007), based important observation shape ABoxes
derived input ABox A. individuals called named (shown black
circles), connected role assertions arbitrary way. individuals
introduced - -rules called blockable (shown white circles). example,
R.C(a) expanded R(a, s) C(s), called blockable individual
R-successor a. difficult see that, knowledge base contain
171

fiMotik, Shearer, & Horrocks
bloc
ks
t0

s0

u0
u

Figure 3: Forest-Like Shape ABoxes

nominals, tableau derivation rule connect arbitrary named individual:
individual participate inferences derive assertion form D(s)
concept, create new successor s, connect existing predecessor successor,
or, presence (local) reflexivity, connect itself. Hence, ABox A0 obtained
seen forest form shown Figure 3: named individual
arbitrarily connected named individuals tree blockable successors.
concept label LA (s) defined set concepts C C(s) A,
edge label LA (s, s0 ) set atomic roles R(s, s0 ) A.
forest-like structure ABoxes enables blocking. Description logics SHIQ+
SHOIQ+ allow inverse roles number restrictions, handled
literature ancestor pairwise blocking (Horrocks et al., 2000b): individuals
s, s0 , t, t0 occurring ABox shown Figure 3, blocks (shown
double border s) LA (s) = LA (t), LA (s0 ) = LA (t0 ), LA (s, s0 ) = LA (t, t0 ),
LA (s0 , s) = LA (t0 , t).4 tableau algorithms, - -rules applicable
nonblocked individuals, ensures termination: number different concept
edge labels exponential |K|, exponentially long branch forest-like ABox must
contain blocked individual, thus limiting length branch ABox. Let
ABox Figure 3 tableau derivation rule applicable,
blocked t. construct model unravelingthat is, replicating
fragment infinitely often. Intuitively, blocking ensures part
ABox s0 behaves like part t0 , unraveling
indeed generates model. logic able connect blockable individuals nontree-like way, unraveling would generate model; fact, notion ancestors,
descendants, blocking would ill-defined.
Consider unlucky run tableau algorithm ancestor pairwise blocking
K2 . number elements exponential |K2 |, happen blocking
comes effect algorithm constructs exponentially deep tree; since
tree binary, doubly exponential total. lucky run, algorithm always
pick Bj instead Cj ; then, algorithm constructs polynomially deep binary tree,
4. blocking definition must include edge labels directions because, unlike
tableau formalizations, edge labels include atomic roles.

172

fiHypertableau Reasoning Description Logics

tree exponential total. Thus, and-branching caused - -rules
lead unnecessary generation ABox doubly exponential size
input, limits scalability tableau algorithms practice.
3.2 Hypertableau Algorithm Glance
section present informal overview hypertableau algorithm addresses
problems due or- and-branching outlined Section 3.1. formalize
algorithm Section 4.
3.2.1 Derivation Rules
hyperresolution calculus (Robinson, 1965) often
V used forWfirst-order theorem proving. works clausesimplications V
form ni=1 Ui
j=1 Vj Ui
n
Vj first-order
atoms. conjunction i=1 Ui called antecedent,
W
V

called consequent; sometimes omit antecedent
disjunction
j
j=1
empty. Di possibly empty disjunction literals general unifier
(A1 , B1 ), . . . , (Am , Bm ), hyperresolution derivation rule defined follows (assuming
unifier exists):5
A1 1

...

Dm
B 1 . . . B C1 . . . Ck
D1 . . . Dm C1 . . . Ck

make calculus refutationally complete first-order logic, one additionally needs
factoring derivation rule, discuss further.
hypertableau calculus (Baumgartner et al., 1996) based observation that,
literals C1 . . . Cn share variables, replace clause
nondeterministically chosen atom Ci assume true. assume
clauses safe (i.e., variable occurring clause also occurs clauses
antecedent), Ai Di C1 . . . Cn always ground, always
nondeterministically split atoms. hypertableau inference written
A1

...


B 1 . . . B C1 . . . Ck
C1 | . . . | Ck

general unifier (A1 , B1 ), . . . , (Am , Bm ) | represents or-branching.
Horn clauses, inference deterministic,6 calculus exhibits minimal
amount dont-known nondeterminism general clauses.
hypertableau calculus Baumgartner et al. (1996) easily applied DLs:
GCIs translated first-order formulae (Borgida, 1996), converted clauses, shown following example.
v R.B

x : [A(x) : R(x, y) B(y)]

A(x) B(f (x))
A(x) R(x, f (x))

5. usual resolution theorem proving assume notation Ai Di imply Ai
left-most disjunct disjunction, follow convention.
6. mentioned before, order inferences applied nevertheless dont-care nondeterministic.

173

fiMotik, Shearer, & Horrocks

Let ABox containing assertions A(a), R(a, b), B(b). GCI v R.B
clearly satisfied A, need perform inference. clauses obtained
skolemization, however, satisfied A, hypertableau calculus derives
R(a, f (a)) B(f (a)). Hence, skolemization may make calculus perform unnecessary
inferences, may inefficient.
Therefore, instead working skolemized clauses, calculus first preprocesses
SHOIQ+ knowledge base K pair (K) = (T R (K), (K)),
(K) ABox
Vn W
R (K) set DL-clausesimplications form i=1 Ui
j=1 Vj , Ui
form R(x, y) A(x), Vj form R(x, y), A(x), R.C(x), n R.C(x),
x y. preprocessing step introduced formally Section 4.1. DL-clauses
R (K) used Hyp-rule, inspired hypertableau derivation rule.
example, GCI R.A v B translated DL-clause R(x, y) B(x) A(y); then,
ABox contains R(a, b), Hyp-rule derives either B(a) A(b).
At-most restrictions translated approach DL-clauses containing equalities;
example, axiom v 2 R.B translated DL-clause
A(x) R(x, y1 ) B(y1 ) R(x, y2 ) B(y2 ) R(x, y3 ) B(y3 ) y1 y2 y1 y3 y2 y3 .
concept form n R.B encoded using O(log n) bits, corresponding
DL-clause contains O(n2 ) literals; thus, translation incurs exponential blowup.
believe, however, issue particular approach: tableau algorithms
deal at-most restrictions using specialized -rule whose application requires O(n)
space; thus, translation merely makes exponential space requirement explicit. Consequently, (hyper)tableau algorithms unlikely able handle large numbers
number restrictions, specialized algorithms, one proposed Faddoul,
Farsinia, Haarslev, Moller (2008), may required.
translation described previous paragraph, Hyp-rule derive
equalities form t. dealt using -rule: whenever
6= t, -rule replaces vice versa assertions A; usually
called merging.
Apart Hyp- -rule, calculus contains -rule tableau
calculus deals existential quantifiers, -rule detects obvious contradictions (which form 6 s, A(s) A(s)), NI -rule ensures
termination presence nominals, number restrictions, inverse roles. discuss
NI -rule detail Section 3.2.4.
rules algorithm formalized Definition 7 page 193 Table 5 page
196, reader may find useful briefly examine definitions continuing.
3.2.2 Anywhere Pairwise Blocking
employ pairwise blocking Section 3.1.2 ensure termination calculus;
curb and-branching, however, extend anywhere pairwise blocking. key idea
extend set potential blockers beyond ancestors s. so,
must avoid cyclic blocks: allowed block block s, neither
guaranteed successors constructed, would render calculus incomplete. Therefore, parameterize algorithm strict ordering individuals
174

fiHypertableau Reasoning Description Logics

R


R.>

R

R
b



R.>

R

b

R

R
c

R.>



R

c


R.>

Figure 4: Yo-Yo Example

contains ancestor relation. allow block if, addition conditions
mentioned Section 3.1.2, s. version blocking formalized Definition 7 page 193. Note that, coincides ancestor relation, anywhere
blocking becomes equivalent ancestor blocking.
Anywhere blocking reduce and-branching practice. Consider knowledge
base K2 Section 3.1.2. exhaust exponentially many members ,
subsequently created individuals blocked. best case, always choose Bj
instead Cj , create polynomial path tree use individuals
path block siblings, shown Figure 2b. Hence, derivation K2
anywhere blocking constructed polynomial time.
3.2.3 Problems Due Merging
Merging easily lead termination problems even simple DLs, shown
following example. simplicity, present TBox set DL-clauses C3 .
(4)

A3 = { A(a), R.>(a), R(a, b), R(a, a) }
C3 = { R(x, y1 ) R(x, y2 ) y1 y2 ,
A(x) R(x, y) R.>(y) }

Consider derivation calculus A3 C3 illustrated Figure 4:
second DL-clause, Hyp-rule derives R.>(b), -rule expands R(b, c); then,
first DL-clause, Hyp-rule derives b a, -rule merges b a. Clearly,
resulting ABox isomorphic original one (that c blockable b named
individual relevant here), repeat sequence inferences,
leads nontermination. best knowledge, problem first identified
Baader Sattler (2001), commonly known yo-yo.
problem arises because, due merging, unbounded number
blockable R-successors: blockable individual c created R-successor b,
merging b makes c blockable R-successor a. This, turn, allows us apply
DL-clauses C3 arbitrary number times, leads nontermination.
problem solved always merging descendant ancestor t,
pruning mergingthat is, removing assertions containing blockable descendant thus ensuring inherit new successors.7 Pruning formally
defined Definition 7 page 193.
7. Horrocks et al. (2000b) physically remove successors, mark present setting
relevant edge labels . exactly effect pruning.

175

fiMotik, Shearer, & Horrocks

B
R.C


R.B


R

s1

R

C
S.{c}


R.B

s2





B
R.C
R

s1

c
b

R


R.B

s3
B
R.C

R

s4



b

R.B

C
S.{c}

R

s3

C
S.{c}
R

s2

c

R

B
R.C

Figure 5: Non-Tree-Like Structures Due Merging

Thus, merging b example, prune bthat is, remove
assertion R(b, c). Merging produces ABox represents model A3 C3 ,
algorithm terminates. Note pruning well-defined ABoxes
forest-shaped, cf. Figure 3: connections individuals arbitrary and,
particular, cyclic, would clear part ABox pruned.
3.2.4 Nominals
nominals, possible derive ABoxes forest-like, following
simple example demonstrates. presentation purposes, use concept R.{c}
DL-clauses even though concepts would decomposed algorithm.
(5)

A4 = { A(a), A(b) }
C4 = { A(x) (R.B)(x), B(x) (R.C)(x), C(x) (S.{c})(x) }

Successive applications Hyp- -rules A4 C4 produce ABox
A14 shown left-hand side Figure 5. ABox clearly forest-shaped:
two paths role atoms A14 start named individuals b end named
individual c. Nevertheless, role relations blockable individuals remain forestlike, termination derivation ensured using blocking. DLs include
nominals produce extended forest-like ABoxes (Horrocks & Sattler, 2001).
DL includes inverse roles, number restrictions, nominals, shape ABox
becomes much involved. end, assume extend C4 DL-clause
S(y1 , x) S(y2 , x) y1 y2 (which axiomatizes inverse-functional effectively
introduces number restrictions). A14 , Hyp-rule derives s2 s4 . Note
s2 s4 blockable individuals; furthermore, neither individual ancestor
other, merge, say, s4 s2 . produces ABox A24 shown righthand side Figure 5, assertion R(s3 , s2 ) makes A24 forest-shaped.
extending example, possible use nominals, inverse roles, number restrictions
arrange blockable individuals cycles. derived ABoxes thus forest-shaped,
makes defining suitable notions pruning unraveling difficult prevents us
using blocking ensure termination calculus.
176

fiHypertableau Reasoning Description Logics



s1

s2



s1

s2

c
b

s3

s4



s1
s2

c
b

s3

s4

b

c

s3

Figure 6: Introduction Root Individuals

solve problem, need extend arbitrarily interconnected part A24
changing status s2 blockable root individual is, individual similar named ones arbitrarily interconnected. extended forest-like
ABoxes thus consist set arbitrarily interconnected root individuals
root tree (ignoring reflexive connections connections back root individuals) otherwise consists entirely blockable individuals (see Figure 3 page 172).
Named individuals subset root individuals occur input ABox.
talk individuals, mean either root blockable ones (see Definition 7
page 193 formal definition).
Returning example, changing status s2 blockable root
individual, s1 s3 blockable A24 , ABox extended forest-like
shape apply blocking pruning usual. schematically shown
Figure 6. generally, apply following preliminary version NI -rule,
denote (*) easier reference:
change root individual whenever contains assertions R(s, a)
A(s) root named individual, blockable individual
successor a, must satisfy at-most restriction n R .A.
Note that, successor a, part ABox involving forestshaped, NI -rule need applicable.
solution, however, introduces another problem: number root individuals
grow arbitrarily, shown following example.

(6)

A5 =
{ A(b) }

A(x) (R.A)(x),
A(x) (S.{a})(x),
C5 =
S(y1 , x) S(y2 , x) S(y3 , x) y1 y2 y2 y3 y1 y3

A5 C5 , calculus produce ABox A15 shown left-hand side Figure
7. ABox A15 explicitly contain at-most restriction concepts, precondition
(*) cannot checked directly; shall discuss issue shortly. moment,
however, please note last DL-clause C5 corresponds axiom > v 2 .>,
individuals c seen satisfying precondition (*); therefore, change
root individuals. Furthermore, third DL-clause C5 satisfied,
Hyp-rule derives c b, -rule merge c b. Since blockable
individual, cannot prune it, obtain ABox A25 shown middle Figure 7.8
8. reduce clutter, repeat labels individuals.

177

fiMotik, Shearer, & Horrocks

R
b

R.A
S.{a}

b


R
c
R

R



R


b








R



R








e


Figure 7: Yo-Yo Root Individuals

Since R.A(d) satisfied, extend A25 R(d, e), A(e), R.A(e), S.{a}(e),
S(e, a) produce ABox A35 shown right-hand side Figure 7. Individual
e seen satisfying precondition (*), changed root individual.
ABox isomorphic A15 , repeat inferences forever.
solve problem NI -rule refines (*). Assume contains
individual satisfies precondition (*)that is, contains assertions R(s, a)
A(s), root named individual, blockable individual
successor a, must satisfy at-most restriction n R .A. model A,
n different individuals bi participate assertions form R(bi , a)
A(bi ). Hence, associate set n fresh root individuals {b1 , . . . , bn }
represent R -neighbors a. turn root individual nondeterministically
choosing bj set merging bj . way, number new root
individuals introduced result at-most restriction n R .A
limited n. complete definition NI -rule given Table 5 page 196.
example Figure 7, NI -rule introduces two fresh root individuals.
NI -rule applied third time, instead introducing e, one previously
introduced root individuals reused, ensures termination calculus.
formulating NI -rule, faced technical problem: at-most restriction concepts translated calculus DL-clauses, makes testing
condition previous paragraph difficult. example, application Hyprule third DL-clause (6) (obtained axiom > v 2 .>) produce
equality c b; equality alone reflect fact must satisfy
at-most restriction 2 .>. enable application NI -rule, introduce
annotated equalities annotations establish association at-most
restriction. third DL-clause (6) thus represented algorithm follows:
(7)

S(y1 , x) S(y2 , x) S(y3 , x)
y1 y2 @x2 .> y2 y3 @x2 .> y1 y3 @x2 .>

Hyp-rule derives c b @a2 .> , meaning c b; however,
annotation says that, since must satisfy at-most restriction 2 .>, b
c must also merged one (two) individuals reserved -neighbors a.
178

fiHypertableau Reasoning Description Logics

R.B



R
B
R.C

b



C
S.D
R.B


c

C
S.D
R.B


c

R.B




R

R

B
R.C

R





B
R.C

b

c

e

c

C
S.D

C
S.D

C
S.D

(a) Nonterminating Variant



R

R
b

R

R

R.B

C
S.D



B
R.C

(b) Terminating Variant

Figure 8: Caterpillar Example

3.2.5 Nominals Merging
introduction NI -rule leads another problem: repeated merging root
individuals lead nontermination caterpillar derivation. Consider, example,
application hypertableau calculus following knowledge base:


S(a, a), R.B(a)
A6 =


B(x) R.C(x),
C(x) S.D(x),
(8)
C6 =
D(x) x a,
S(y1 , x) S(y2 , x) y1 y2 @x1 .>
ABox first DL-clause cause introduction two new blockable individuals b c; next two DL-clauses connect c role S; last DL-clause
produces c c @x1 .> ; application NI -rule assertion causes c
become root individual. ABox A16 resulting inferences shown
left-hand side Figure 8a. Since inverse-functional, individuals c must
merged. individual c root, longer descendant a, choose
merge c. blockable individual b pruned (in order avoid problems
outlined Section 3.2.3), resulting ABox shown middle part Figure
8a. existential restriction R.B c, however, satisfied, similar sequence
rule applications constructs ABox A26 shown right-hand side Figure 8a.
ABox isomorphic A16 , inferences repeated forever.
problem intuitively explained following observation. NI -rule
introduces fresh root individuals neighbors existing root individual; thus,
179

fiMotik, Shearer, & Horrocks

root individual ABox seen part chain showing individual
caused introduction root individual. chain initially anchored
named individual: individuals occur input ABox introduced
NI -rule. length path blockable individuals used limit length
chains root individuals. allow chain anchors removed ABox,
chains remain limited length given ABox; however, course
derivation, one end chain extended indefinitely end shortened.
solve problem allowing named individuals merged
named individuals, specified postcondition -rule Table 5 page
196. ensures chain root individuals always remains anchored named
individual. example, instead merging c, merge c a, results
ABox shown Figure 8b. derivation rule applicable ABox,
algorithm terminates.
3.2.6 NI -Rule Unraveling
NI -rule required ensure ABoxes forest shaped, also
enable application blocking unraveling. Consider, example, knowledge
base shown (9), omit annotations equalities sake clarity.
Intuitively, axioms knowledge base state individual R neighbors, infinite chain individuals -neighbor
a.

(9)

A7 = {
A(a), (R.B)(a), }

A(x)

R(y,
x)

,
B(x)

(R.B)(x),
B(x)

(S.{a})(x),






R(y1 , x) R(y2 , x) y1 y2 ,
C7 =
S(y1 , x) S(y2 , x) S(y3 , x) S(y4 , x)






y1 y2 y1 y3 y1 y4 y2 y3 y2 y4 y3 y4 ,

Without NI -rule, application calculus A7 C7 might produce
ABox A17 shown Figure 9a. individual blocked A17 individual c,
derivation terminates. Note last DL-clause C7 (which corresponds
axiom > v 3 .>) satisfied: individual A17 -neighbors
two neighbors. construct model A17 , unravel blocked parts
ABoxthat is, construct infinite path extends past duplicating
fragment model c infinite number times. This, however,
creates additional -neighbors a, invalidates last DL-clause C7 ; thus,
unraveled ABox define model A7 C7 .
NI -rule elegantly solves problem. Since must satisfy at-most restriction
form 3 .>, soon S(b, a), S(c, a), S(d, a) derived, NI -rule applied
turn b, c, root individuals. corrects problems unraveling: root
individuals become blocked, introduce another fresh blockable individual e.
individual merged another -neighbor a, producing individual two
R -neighbors, illustrated Figure 9b. R inverse-functional, however, neighbors
merged. Merging continues b merged a, causing become

180

fiHypertableau Reasoning Description Logics


R.B


R

R

b
B
R.B
S.{a}


c
B
R.B
S.{a}

R


B
R.B
S.{a}

(a) Premature Blocking


R.B


R

b
B
R.B
S.{a}


R.B





R



R

c
B
R.B
S.{a}

R




R


B
R.B
S.{a}

e

b

B
R.B
S.{a}

B
R.B
S.{a}





R

c
B
R.B
S.{a}

R

R

B
R.B
S.{a}

(b) Correct Derivation

Figure 9: NI -rule Unraveling
R-neighbor, point algorithm correctly determines knowledge base
represented A7 C7 unsatisfiable.
3.3 Related Work
3.3.1 Hypertableau vs. Absorption
Absorption extensively used tableau calculi address problems orbranching outlined Section 3.1.1 (Horrocks, 2007). basic absorption algorithm tries
rewrite GCIs form v C atomic concept. preprocessing,
instead deriving C individual ABox, C(s) derived ABox
contains A(s); thus, nondeterminism introduced absorbed GCIs localized.
basic technique refined extended several ways. Negative absorption rewrites
GCIs form v C atomic concept; then, C(s) derived
ABox contains A(s) (Horrocks, 2007). Role absorption rewrites GCIs form
R.> v C; then, C(s) derived ABox contains R(s, t) (Tsarkov & Horrocks,

181

fiMotik, Shearer, & Horrocks

2004). Binary absorption rewrites GCIs form A1 u A2 v C; then, C(s) derived
ABox contains A1 (s) A2 (s) (Hudek & Weddell, 2006).
techniques proven indispensable practice; however, analysis shows potential improvement. example, axiom R.A v (2) cannot absorbed directly, applying role absorption (2) produces axiom R.> v R.A
containing disjunction consequent. Binary absorption directly applicable
(2) since axiom contain two concepts left-hand side v, algorithm Hudek Weddell (2006) additionally transforms (2) absorbable axiom
v R .A. Consider, however, following axiom:
> v R.C S.D

(10)

binary absorption algorithm process two disjuncts (10) two ways.
R.C processed S.D, (10) transformed axioms shown (11),
applied deterministically tableau algorithm. If, however, S.D
processed R.C, (10) transformed axioms shown (12). first
axiom absorbable, second not, tableau algorithm nondeterministic.
(11)
(12)

C v R .Q1

Q1 v S.D
> v .Q2

Q2 v R.C

Heuristics used practice find good absorption (see, e.g., Wu & Haarslev, 2008),
guarantees result incur least amount nondeterminism;
even Horn knowledge bases, reasoning without nondeterminism
possible principle (Hustadt et al., 2005). contrast, algorithm guaranteed
preprocesses Horn knowledge base Horn DL-clauses always result
deterministic derivations. example, (10) transformed Horn DL-clause (13).
(13)

R(x, y1 ) C(y1 ) S(x, y2 ) D(y2 )

Even case inherently nondeterministic knowledge bases, absorption
optimized. Consider axiom (14), translated DL-clause (15):
(14)
(15)

> v R.B S.C
R(x, y1 ) S(x, y2 ) A(x) B(y1 ) C(y2 )

binary absorption algorithm transforms (14) following axioms:
(16)

Q1 u Q2 v

(17)

> v B R .Q1

(18)

> v C .Q2

Axiom (16) absorbable; however, (17) (18) not, application introduces
nondeterministic choice point individual occurring ABox. problem
ameliorated using role absorption transforming (17) (18) (19) (20):
(19)

R .> v B R .Q1
182

fiHypertableau Reasoning Description Logics

(20)

.> v C .Q2

(19) used derive (B R .Q1 )(b) R(a, b), (20) used
derive (C .Q2 )(d) S(c, d); however, two disjunctions derived even
6= c. contrast, DL-clause (15) derives disjunction = c; thus, literals
R(x, y1 ) S(x, y2 ) (15) act guards. presence variables antecedent
(the shared variable x example) makes guards selective guard
applied isolation. Furthermore, = c, derive disjunction A(a) B(b) C(d),
involves three different individuals (a, b, case); contrast, consequences
tableau algorithms typically involve one individual. Thus, usage
variables, DL-clauses global effect tableau rules.
best knowledge, known absorption technique localize effects
axioms number restrictions, (21).
(21)

2 R.B v

order ensure instances B counted, tableau algorithms need include
choose-rule that, assertion R(a, b), nondeterministically derives B(b) B(b).
hypertableau setting, however, (21) translated following DL-clause:
(22)

R(x, y1 ) R(x, y2 ) B(y1 ) B(y2 ) A(x) y1 y2

choose-rule needed, DL-clause simply applied assertions form
R(a, b), B(b), R(a, c), B(c); furthermore, conclusion tautology whenever b = c.
presence guard atoms antecedent (22) thus significantly reduces
nondeterminism introduced number restrictions. Furthermore, Horn knowledge
bases number restrictions (which includes common case functional roles),
calculus exhibits nondeterminism; contrast, tableau calculi still need choose-rule,
introduces nondeterminism even GCIs fully absorbed.
hypertableau calculus presented paper generalize negative absorption directly; example, negatively absorbed axiom (23) translated
DL-clause (24) applied individuals ABox.
(23)
(24)

v B
A(x) B(x)

Negative absorption can, however, easily applied setting: negatively absorb
atomic concept A, simply replace input ABox DL-clauses occurrences
A0 A0 fresh concept, move literals involving A0
appropriate side DL-clauses. example, (24) would thus converted (25),
applied deterministically.
(25)

A0 (x) B(x)

Note transform DL-clause A(x) B(x) A0 (x) B(a); however,
similar situation arises tableau calculi, applying negative absorption v B
means v B cannot absorbed.
183

fiMotik, Shearer, & Horrocks

summarize, unlike various absorption techniques guided primarily heuristics, hypertableau calculus provides framework captures variants absorption
aware of, guarantees deterministic behavior whenever input knowledge base
Horn, eliminates need nondeterministic choose-rule, allows powerful use guard atoms localize remaining nondeterminism. Furthermore,
Section 4.1.3 show calculus provides proof-theoretic framework DLs
uniformly handle certain useful extensions SHOIQ+ .
3.3.2 Relationship Caching
Various caching optimizations used reduce sizes models constructed
knowledge base classification (Ding & Haarslev, 2006; Horrocks, 2007). proposed approaches, caching used parallel blockingthat is, caching alone
guarantee termination calculus, caching must carefully integrated
blocking order affect soundness and/or completeness. integration particularly problematic presence inverse roles. contrast, anywhere blocking alone
sufficient guarantee termination calculus. Furthermore, Section 6.2 present
optimization anywhere blocking seen simple effective form
general caching. Finally, discuss Section 7, efficient implementation anywhere
blocking obtained using simple techniques. Thus, anywhere blocking achieves
many effects caching without much added complexity.
Donini Massacci (2000) used anywhere blocking caching unsatisfiable
concepts obtain tableau algorithm DL ALC runs single exponential time.
Gore Nguyen (2007) presented algorithm DL SHI also runs exponential time achieves termination solely caching satisfiable unsatisfiable
concepts. algorithms, however, seem incompatible absorption variants,
latter essential making tableau algorithms practical. Furthermore,
unclear extend algorithms DLs provide number restrictions, nominals,
inverse roles, SHOIQ+ .
3.3.3 Relationship First-Order Calculi
original hypertableau calculus first-order logic subsequently extended
equality implemented KRHyper theorem prover (Baumgartner, Furbach, & Pelzer, 2008). calculus used finite model generation, decides
function-free clause logic.
Hyperresolution splitting used decide several description modal
logics (Georgieva et al., 2003; Hustadt & Schmidt, 1999). approaches, however, rely
skolemization, which, discussed previously, inefficient practice.
Furthermore, approaches deal logics much weaker SHOIQ+ ;
particular, aware hyperresolution-based decision procedure handle
inverse roles, number restrictions, nominals.
hypertableau calculus related Extended Positive (EP) tableau calculus
first-order logic Bry Torge (1998). Instead relying skolemization, EP satisfies
existential quantifiers introducing new constants, done way makes
calculus complete finite satisfiability. EP is, however, unlikely practical due

184

fiHypertableau Reasoning Description Logics

high degree nondeterminism. Furthermore, EP provide decision procedure
DLs SHOIQ+ enjoy finite model property (Baader & Nutt,
2007). Consider, example, knowledge base whose TBox contains axioms (26)
(27), whose ABox contains assertion (28):
(26)

v R.A

(27)

> v 1 R .>

(28)

(A u R.A)(a)

EP try satisfy existential quantifier reusing athat is, adding
assertions R(a, a) A(a). leads contradiction, EP backtrack, introduce
fresh individual b, add assertions R(a, b) A(b); satisfy (26), also
add R.A(b). satisfy existential quantifier latter assertion, EP try
reuse a; fail, try reuse b adding assertion R(b, b). Due
(27), however, b merged a, results contradiction; therefore, EP
backtrack, introduce yet another fresh individual c add assertions R(b, c), A(c),
R.A(c). repeating argument, easy see EP generate ever larger
models terminate. unsurprising since knowledge base satisfied
infinite models. achieve termination knowledge bases, EP would need
extended blocking techniques ones described paper.
Baumgartner Schmidt (2006) developed so-called blocking transformation firstorder clauses, improve performance bottom-up model generation methods.
Roughly speaking, clauses modified way makes bottom-up calculus derive
6 term subterm t; then, application paramodulation
achieves effect analogous reusing instead EP tableau
calculus. transformation, however, ensure termination DLs
finite model property. example, reasons explained previous paragraph, hyperresolution splitting terminate clauses obtained
application blocking transformation (the clauses corresponding to) (26)(28).
Furthermore, even DLs enjoy finite model property, unlucky sequence
applications derivation rules prevent bottom-up model generation method
blocking terminating (please refer Section 3.2.3 details).

4. Satisfiability Checking Algorithm
present hypertableau algorithm used check satisfiability
SHOIQ+ knowledge base K. algorithm consists two phases: preprocessing
phase described Section 4.1, hypertableau phase described Section 4.2.
4.1 Preprocessing
goal preprocessing phase transform SHOIQ+ knowledge base K
ABox (K) set DL-clauses R (K) equisatisfiable K.
Definition 1 (DL-Clause). concepts >, , concepts form
atomic concept called literal concepts. Let NV set variables disjoint
185

fiMotik, Shearer, & Horrocks

Table 2: Satisfaction DL-Clauses Interpretation
I, |= C(s)
I, |= R(s, t)
I, |=
Wn
V
I, |=
j=1 Vj
i=1 Ui

iff
iff
iff
iff

Wn
V
|=
j=1 Vj
i=1 Ui
|= C

iff
iff

sI, C
hsI, , tI, RI
sI, = tI,
I, |= Ui 1 implies
I, |= Vj 1 j n
Wn
V
I, |=
j=1 Vj mappings
i=1 Ui
|= r DL-clause r C

set individuals NI . atom expression form B(s), n S.B(s), R(s, t),
t, individuals variables, B literal concept, R atomic role, (not
necessarily atomic) role, n positive integer. DL-clause expression form
U1 . . . Um V1 . . . Vn
Ui Vj atoms, 0, n 0. conjunction U1 . . . Um called
antecedent, disjunction V1 . . . Vn called consequent. empty antecedent
empty consequent DL-clause written > , respectively.
Let = (4I , ) interpretation : NV 4I mapping variables elements interpretation domain. Let aI, = aI individual xI, = (x)
variable x. Satisfaction atom, DL-clause, set DL-clauses C
defined Table 2.

4.1.1 Elimination Transitivity Axioms
Transitivity axioms handled tableau algorithms + -rule: R transitive
ABox contains R.C(s) R(s, t), + -rule derives R.C(t). algorithm,
however, concepts form R.C translated DL-clauses, + -rule cannot
applied. Therefore, instead handling transitivity directly, encode SHOIQ+
knowledge base K equisatisfiable ALCHOIQ+ knowledge base (K). encoding
eliminates transitivity axioms, simulates effects using additional GCIs.
Definition 2. Given SHOIQ+ knowledge base K = (R, , A), concept closure K
smallest set concepts clos(K)
C v , nnf(C D) clos(K),
C(a) A, nnf(C) clos(K),
C clos(K) syntactically occurs C, clos(K),
n R.C clos(K), C
clos(K),
R.C clos(K), vR R, Tra(S) R, S.C clos(K).

186

fiHypertableau Reasoning Description Logics

-encoding K ALCHOIQ+ knowledge base (K) = (R0 , 0 , A) R0
obtained R removing transitivity axioms
0 = {R.C v S.(S.C) | R.C clos(K), vR R, Tra(S) R}.
Similar encodings known various description (Tobies, 2001) modal (Schmidt
& Hustadt, 2003) logics. Note that, order guarantee decidability (Horrocks, Sattler,
& Tobies, 2000a), number restrictions local reflexivity allowed SHOIQ+
simple rolesthat is, roles transitive subroles; similar reasons, role
disjointness, irreflexivity, asymmetry axioms also allowed simple roles.
Lemma 1. SHOIQ+ knowledge base K satisfiable (K) satisfiable,
(K) computed time polynomial |K|.
full proof analogous result DL SHIQ given Motik (2006) Theorem 5.2.3, generalization result SHOIQ+ straightforward; therefore,
omit proof Lemma 1 sake brevity. elimination transitivity
axioms, distinction simple complex roles. Hence, rest
paper assume roles simple unless otherwise stated and, without loss
generality, treat R.B syntactic shortcut 1 R.B.
4.1.2 Normalization
translation set DL-clauses, knowledge base first brought
normalized form. done order make negations explicit, ensure
resulting DL-clauses compatible blocking.
understand first issue, consider axiom v (R.R.R.B). Converting
axiom DL-clauses straightforward implicit negations; example,
concept seemingly negated but, due negation implicit implication,
actually occurs positively axiom. Therefore, replace axiom following
equivalent axiom. makes negations explicit, result easily translated
DL-clause.
(29)

> v R.R.R.B

R(x, y1 ) R(y1 , y2 ) R(y2 , y3 ) B(y3 ) A(x)

understand second issue, consider knowledge base K8 , consisting ABox
A8 TBox corresponds set DL-clauses C8 .
(30)

A8 = { A(a), B(a) }
C8 = { R(x, y1 ) R(y1 , y2 ) R(y2 , y3 ) B(y3 ) A(x),

B(x) R.B(x) }

applying rules Section 3.2, algorithm constructs K8 ABox shown
Figure 10. According definition blocking introduced Definition 7,9 c
blocked b; furthermore, rule applicable ABox, algorithm terminates,
leading us believe K8 satisfiable. ABox, however, represent model
K8 : expand R.B(c) R(c, d) B(d), first DL-clause C8 derive
9. version blocking introduced Definition 7 differs one presented Section 3.1.2
concept label LA (s) individual consists atomic concepts A(s) A.

187

fiMotik, Shearer, & Horrocks


B
R.B


R

b
B
R.B

R

c
B
R.B

Figure 10: Incorrect Blocking due Lack Normalization

A(a), contradicts A(a). problem arises antecedent
first DL-clause C8 checks path three R-successors, whereas pairwise blocking
condition ensures paths length two fully constructed. Intuitively,
antecedents DL-clause check paths fit fully constructed
model fragments. ensure renaming complex concepts simpler ones.
Thus, transform culprit DL-clause following ones, check
paths length one.
(31)

> v R.Q1

R(x, y) Q1 (y) A(x)

(32)

> v Q1 R.Q2

R(x, y) Q2 (y) Q1 (x)

(33)

> v Q2 R.B

R(x, y) B(y) Q2 (x)

application DL-clauses ABox shown Figure 10 would additionally
derive Q2 (a), Q2 (b), Q1 (a), c would blocked. calculus would expand
R.B(c) discover contradiction.
formalize ideas, define normalized form DL knowledge bases.
F
Definition 3 (Normalized Form). GCI normalized form > v ni=1 Ci ,
Ci form B, {a}, R.B, R.Self, R.Self, n R.B, n R.B,
B literal concept, R role, n nonnegative integer.
TBox normalized GCI normalized. ABox normalized
concept assertion contains literal concept, role assertion contains
atomic role, contains least one assertion. ALCHOIQ+ knowledge
base K = (R, , A) normalized normalized.
following transformation used normalize knowledge base.
Definition 4 (Normalization). ALCHOIQ+ knowledge base K, knowledge base
(K) computed shown Table 3.
Normalization seen variant well-known structural transformation
(Plaisted & Greenbaum, 1986; Nonnengart & Weidenbach, 2001). application
structural transformation (29) would replace complex subconcept positive
atomic concept, eventually producing > v R.Q1 . axiom cannot translated
Horn DL-clause, whereas (29) can; thus, structural transformation destroy
188

fiHypertableau Reasoning Description Logics

Table 3: Functions Used Normalization

()
(> v nnf(C1 C2 ))
RA
C1 vC2

(> v C C 0 ) = (> v C C 0 )
(> v
C 0 Ci )
(K) = {>(a)}



1in

C 0 form C 0 = C1 u . . . u Cn n 2
(> v C R.D) = (> v C R.D ) (> v
D)
(> v C n R.D) = (> v C n R.D ) (> v
D)
(> v C n R.D) =
(> v C n R.



) (> v
D)

C empty,
(> v C {s}) =
(C(s)) otherwise.
(D(s)) = {D (s)} (> v
nnf(D))
(R (s, t)) = {R(t, s)}
() = {} axiom

QC
pos(C) = true
C =
, QC fresh atomic concept unique C
QC pos(C) = false
pos(>) = false
pos() = false
pos(A) = true
pos(A) = false
pos({s}) = true
pos({s}) = false
pos(R.Self) = true
pos(R.Self) = false
pos(C1 u C2 ) = pos(C1 ) pos(C2 )
pos(C1 C2 ) = pos(C
1 ) pos(C2 )
pos(C
1 ) n = 0
pos(R.C1 ) = pos(C1 )
pos( n R.C1 ) =
pos( n R.C1 ) = true
true
otherwise
Note: atomic concept, C(i) arbitrary concepts, C possibly empty
disjunction arbitrary concepts, literal concept, fresh individual.
Note commutative, C 0 C C 0 necessarily right-most disjunct.

Horn-ness. prevent this, introduce function pos(C) (c.f. Table 3) returns
false clausification C require adding atoms consequent DLclause. replace occurrence concept C concept negative literal
concept QC pos(C) = false, positive literal concept QC pos(C) = true.
Special care must taken replacing concept concept n R.D: since
occurs n R.D implicit negation, replace

order preserve

Horn-ness. Horn knowledge base K (Hustadt et al., 2005), normalization performs
replacements one presented Hustadt et al., (K) Horn knowledge
base well.
Lemma 2. following properties hold ALCHOIQ+ knowledge base K
corresponding knowledge base (K):
K satisfiable (K) satisfiable;
(K) normalized;
189

fiMotik, Shearer, & Horrocks

(K) computed time polynomial |K|.
Proof. (Sketch) Since transformation seen syntactic variant structural
transformation, proof K (K) equisatisfiable completely analogous
ones Plaisted Greenbaum (1986) Nonnengart Weidenbach (2001),
omit sake ofFbrevity. second claim, note essentially rewrites
GCI form > v ni=1 Ci keeps replacing nested subconcepts Ci
GCI becomes normalized; adds >(a) ABox ABox empty;
replaces inverse role assertions equivalent assertions atomic roles. Thus,
(K) normalized. Finally, occurrence concept K replaced new
atomic concept once, necessary syntactic transformations performed
polynomial time, (K) computed polynomial time.
4.1.3 Translation DL-Clauses
introduce notion HT-clausessyntactically restricted DL-clauses
hypertableau calculus guaranteed terminate. rest paper, often
use function ar, which, given role R variables constants t, returns
atom semantically equivalent R(s, t) contains atomic role; is,

R(s, t) R atomic role
ar(R, s, t) =
.
S(t, s) R inverse role R =
Definition 5 (HT-Clause). assume that, individual a, set atomic concepts NC contains unique nominal guard concept denote Oa ; furthermore,
assume nominal guard concepts occur input knowledge base.
annotated equality atom form @un S.B , s, t, u
constants variables, n nonnegative integer, role, B literal concept;
part @un S.B atom called annotation. atom semantically equivalent
t.10
HT-clause DL-clause r following form, 0 n 0:
(34)

U1 . . . Um V1 . . . Vn

Furthermore, must possible separate variables center variable x, set
branch variables yi , set nominal variables zj following properties
hold, atomic concept, B literal concept containing nominal guard concept,
Oa nominal guard concept, R atomic role, role.
atom antecedent r form A(x), R(x, x), R(x, yi ), R(yi , x),
A(yi ), A(zj ).
atom consequent r form B(x), h S.B(x), B(yi ), R(x, x),
R(x, yi ), R(yi , x), R(x, zj ), R(zj , x), x zj , yi yj @xh S.B .
yi occurs antecedent r atom form R(x, yi ) R(yi , x).
10. explained Section 3.2.4, annotations used ensure termination hypertableau phase.

190

fiHypertableau Reasoning Description Logics

zj occurs antecedent r atom form Oa (zj ).
equality yi yj @xh S.A consequent r occurs subclause r
form (35) 1 , . . . , h+1 branch variables k 1 k h + 1
occurs elsewhere r.
(35)

...

h+1
^

_

k=1

1k<`h+1

[ar(S, x, k ) A(y k )] . . . . . .

k ` @xh S.A . . .

equality yi yj @xh S.A consequent r occurs subclause r
form (36) 1 , . . . , h+1 branch variables k 1 k h + 1
occurs elsewhere r.
(36)

...

h+1
^

ar(S, x, k ) . . . . . .

k=1

h+1
_

A(y k )

k=1

_

k ` @xh S.A . . .

1k<`h+1

HT-clauses general strictly needed capture ALCHOIQ+ knowledge bases. example, HT-clauses form R(x, y) A(y) S(x, y) express form
relativized role inclusions, HT-clauses form R(x, y) S(y, x) U (x, y) (y, x)
capture safe role expressions (Tobies, 2001).
show transform normalized ALCHOIQ+ knowledge base set
HT-clauses, explain need nominal guard concepts.
Definition 6 (Clausification). clausification normalized ALCHOIQ+ knowledge
base K = (R, , A) pair (K) = (T R (K), (K)) R (K) set DLclauses (K) ABox, obtained shown Table 4.
Definition 3, concepts form {a} converted ABox assertions
normalization, Table 4 need handle them. Positive nominal concepts naturally translated equalities containing constants; example, > v {a} corresponds A(x) x a. DL-clauses impractical: given equality assertion
b, -rule would need replace occurrences b assertions, DL-clauses well; thus, mentioned DL-clause replaced
A(x) x b. avoid need changing set DL-clauses derivation,
extract constants ABox; example, > v {a} transformed
DL-clause A(x) Oa (z{a} ) x z{a} assertion Oa (a). constants thus
pushed assertions, -rule perform replacements ABox.
Lemma 3. Let K normalized ALCHIQ knowledge base. Then, K equisatisfiable
(K) = (T R (K), (K)), R (K) contains HT-clauses.
Proof. inspecting Table 4, R (KB) clearly contains HT-clauses. following
equivalences DL concepts first-order formulae well known (Borgida, 1996):
R.B(x) : R(x, y) B(y)
V
n R.B(x) y1 , . . . , yn+1 :
[R(x, yi ) B(yi )]
1in+1

{a}(x) x
191

W
1i<jn+1

yi yj

fiMotik, Shearer, & Horrocks

Table 4: Translation Normalized Knowledge Base HT-Clauses
(T ) = {

n
V

lhs(Ci )

i=1

n
W

n
F

rhs(Ci ) | > v

i=1

Ci }

i=1

R (R) = {ar(R, x, y) ar(S, x, y) | R v R}
{ar(S1 , x, y) ar(S2 , x, y) | Dis(S1 , S2 ) R}
{> ar(R, x, x) | Ref(R) R}
{ar(S, x, x) | Irr(S) R}
{ar(R, x, y) ar(R, y, x) | Sym(R) R}
{ar(S, x, y) ar(S, y, x) | Asy(S) R}
R (K) = (T ) R (R)
(K) = {Oa (a) | {a} occurring K}
Note: Whenever lhs(Ci ) rhs(Ci ) undefined, omitted HT-clause.
C
lhs(C)
rhs(C)

A(x)


A(x)

{a}

Oa (zC )

x zC

n R.A

n R.A(x)

n R.A

n R.A(x)

R.Self

ar(R, x, x)

R.Self

ar(R, x, x)

R.A

ar(R, x, yC )

R.A
n R.A

A(yC )

ar(R, x, yC ) A(yC )
n+1
V

W

i=1

1i<jn+1

) A(y )]
[ar(R, x, yC
C
n+1
V

n R.A

i=1

)
ar(R, x, yC

n+1
W
i=1

)
A(yC

j @x
yC
C n R.A

W
1i<jn+1

j @x
yC
C n R.A

(i)

Note: yC zC fresh variable unique C (and i).
Let 0T R (K) set HT-clauses defined like R (K), difference
lhs({a}) = > rhs({a}) = x a. Then, (0T R (K), (K)) obtained K replacing concepts form R.B, n R.B {a} equivalent first-order formulae,
K (0T R (K), (K)) clearly equisatisfiable. show (0T R (K), (K))
equisatisfiable (T R (K), (K)).
() model 0 (0T R (K), (K)) extended model (T R (K), (K))
0
setting OaI = {aI } nominal guard concept Oa .

192

fiHypertableau Reasoning Description Logics

() model (K) model (0T R (K), (K)): 0T R (K),
R (K) Oak (ak ) (K), form shown below.
V
W
W
= V Ui V Vj nk=1 xk aWk
W
=
Ui nk=1 Oak (z{ak } ) Vj nk=1 xk z{ak }
W
disjunction nk=1 xk ak true values
x1 , . . . , xn , clearly would true values x1 , . . . , xn .
4.2 Hypertableau Calculus HT-Clauses
present hypertableau calculus deciding satisfiability ABox
set HT-clauses C. explained Section 3, algorithm uses several types
individuals. individual either root blockable summarized next; refer
simply individual, mean either root blockable one.
Root individuals either occur input ABox, introduced
NI -rule. important characteristic connected arbitrary,
tree-like, ways.
Root individuals occur input ABox called named individuals.
Root individuals introduced NI -rule defined finite strings
form a.1 . . . . .n named individual, ` form
hR.B.ii, n 0. Root individuals introduced applying NI -rule
assertion @un R.B form u.hR.B.ii 1 n.
Blockable individuals introduced -rule, make tree-like parts
model. set blockable individuals disjoint set root individuals.
Blockable individuals defined finite strings form s.i1 .i2 . . . . .in
root individual, i` integer, n 1. string representation
naturally induces parentchild relationship individuals; example, s.2
second child individual s, either blockable root.
introduce algorithm.
Definition 7 (Hypertableau Algorithm).
Individuals. Given set named individuals NI , set root individuals
smallest set NI and, x , x.hR, B, ii role
R, literal concept B, positive integer i. set individuals NA smallest
set NA and, x NA , x.i NA positive integer i.
individuals NA \ blockable individuals. blockable individual x.i successor
x, x predecessor x.i. Descendant ancestor transitive closures
successor predecessor, respectively.
ABoxes. hypertableau algorithm operates ABoxes obtained extending
standard definition Section 2 follows.
addition assertions Section 2, ABox contain annotated equality
assertions special assertion false interpretations. Furthermore,
assertions refer individuals NA NI .
193

fiMotik, Shearer, & Horrocks

(in)equality (s 6 t) also stands symmetric (in)equality (t 6 s).
true annotated equalities.
ABox contain renamings form 7 b b root individuals. Let 7 reflexive-transitive closure 7 A. individual b
canonical name root individual A, written b = kakA , b individual 7 b exists individual c 6= b b 7 c;
individual exists, kakA = a.11
input ABox ABox containing named individuals, annotated equalities,
renamings, concepts literal roles atomic.
Satisfaction ABoxes interpretation obtained straightforward generalization definitions Section 2: individuals interpreted elements
interpretation domain 4I , |= 7 b iff aI = bI .
Pairwise Anywhere Blocking. labels individual individual
pair hs, ti ABox defined follows:
LA (s) = { | A(s) atomic concept }
LA (s, t) = { R | R(s, t) }
Let strict ordering (i.e., transitive irreflexive relation) NA containing
ancestor relationthat is, s0 ancestor s, s0 s. induction ,
assign individual status follows:
blockable individual directly blocked blockable individual
following conditions satisfied, s0 t0 predecessors t, respectively:
blocked,
s,
LA (s) = LA (t) LA (s0 ) = LA (t0 ),
LA (s, s0 ) = LA (t, t0 ) LA (s0 , s) = LA (t0 , t);
indirectly blocked iff predecessor blocked;
blocked iff either directly indirectly blocked.
Pruning. ABox pruneA (s) obtained removing assertions containing
descendant s.
Merging. ABox mergeA (s t) obtained pruneA (s) replacing individual individual assertions annotations (but renamings)
and, root individuals, adding renaming 7 t.
Derivation Rules. Table 5 specifies derivation rules that, given ABox set
HT-clauses C, derive one ABoxes A1 , . . . , . Hyp-rule, mapping
11. show Lemma 4, derivation rules calculus ensure 7 functional acyclic
relation, individual b satisfying definition always exists. second part definition
kakA thus technical aid necessary make definition complete.

194

fiHypertableau Reasoning Description Logics

set variables NV individuals occurring assertions A, (U )
result replacing variable x atom U (x).
Rule Precedence. -rule applied (possibly annotated) equality
ABox contain equality @un R.B NI-rule
applicable (with t).
Clash. ABox contains clash iff A; otherwise, clash-free.
Derivation. set HT-clauses C input ABox A, derivation pair
(T, ) finitely branching tree function labels nodes
ABoxes following properties hold node :
(t) = root ;
leaf (t) derivation rule applicable (t) C;
children t1 , . . . , tn (t1 ), . . . , (tn ) exactly results applying
one (arbitrarily chosen, respecting rule precedence) applicable rule (t)
C cases.
stress several important aspects Definition 7. preconditions NI -rule
satisfied annotated equality @un R.B , rule must applied even
= t; hence, equality plays role derivation even though logical
tautology. Furthermore, even though NI -rule applied @un R.B u
blockable individual, equality cannot eagerly simplified u
subsequently merged root individual annotation might become important.
Finally, C obtained normalization DL knowledge base
use nominals, inverse roles, number restrictions, precondition NI -rule
never satisfied, need keep track annotations all.
Renamings used keep track root individuals merged root
individuals, necessary make NI -rule sound. example, root individual
a.hR, B, 2i merged named individual b, NI -rule must use b instead
a.hR, B, 2i future inferences.
proof Lemma 6 shows assertions containing least one indirectly blocked
individual used construct model ABox labeling leaf derivation.
derivation rules therefore applicable individuals either directly blocked
blocked, sufficient completeness. Since rules sound, however, one
may choose disregard restriction makes implementation easier.
next introduce notion HT-ABoxes, formalizes idea forest-shaped
ABoxes introduced Section 3.1.2.
Definition 8 (HT-ABoxes). ABox HT-ABox satisfies following conditions, R atomic role, role, B literal concept containing nominal guard
concept, Oa nominal guard concept, s, t, u NA , , b NI , i, j integers.
1. role assertion form R(a, s), R(s, a), R(s, s.i), R(s.i, s), R(s, s).

195

fiMotik, Shearer, & Horrocks

Table 5: Derivation Rules Hypertableau Calculus

Hyp-rule

-rule

-rule

-rule

NI -rule

1. r C, r = U1 . . . Um V1 . . . Vn ,
2. mapping variables r individuals exists

2.1 x NV (x) indirectly blocked,
2.2 (Ui ) 1 m,
2.3 (Vj ) 6 1 j n,
A1 := {} n = 0;
Aj := {(Vj )} 1 j n otherwise.
1. n R.B(s) A,
2. blocked A,
3. contain individuals u1 , . . . , un
3.1 {ar(R, s, ui ), B(ui ) | 1 n} {ui 6 uj | 1 < j n} A,
3.2 1 n, either ui successor ui blocked A,
A1 := {ar(R, s, ti ), B(ti ) | 1 n} {ti 6 tj | 1 < j n}
t1 , . . . , tn fresh distinct successors s.
1. (the equality possibly annotated),
2. 6= t,
3. neither indirectly blocked
A1 := mergeA (s t) named individual, root individual
named individual, descendant t;
A1 := mergeA (t s) otherwise.

6 {A(s), A(s)} indirectly blocked
A1 := {}.
1. @un R.B (the symmetry applies usual),
2. u root individual,
3. blockable individual successor u,
4. blockable individual,
5. neither indirectly blocked
Ai := mergeA (s ku.hR, B, iikA ) 1 n.

2. equality either form @an R.B blockable individual
successor blockable individual, possibly annotated
equality form s.i s.j, s.i s, s.i.j s, s, a. (The symmetry
applies cases usual.)
3. concept assertion form B(s), n S.B(s), Oa (b).
4. contains @un R.B , also contains ar(R, u, s) ar(R, u, t).
5. contains blockable individual s.i assertion, must contain
assertion form R(s, s.i) R(s.i, s).
6. contains least one assertion.

196

fiHypertableau Reasoning Description Logics

Table 6: Cases Application Hyp-Rule Role Assertions
ar(R, u, s)
ar(R, v, a)
ar(R, v, a)
ar(R, v, a)
ar(R, v.n, a)

ar(R, u, t)
ar(R, v, b)
ar(R, v, v.n)
ar(R, v, v)
ar(R, v.n, v)

@uk R.B
b @vk R.B
v.n @vk R.B
v @vk R.B
v @v.n
k R.B

ar(R, v, v.m)
ar(R, v, v.m)
ar(R, v.n, v.n.m)

ar(R, v, v.n)
ar(R, v, v)
ar(R, v.n, v)

v.m v.n @vk R.B
v.m v @vk R.B
v.n.m v @v.n
k R.B

ar(R, v, v)
ar(R, v.n, v.n)

ar(R, v, v)
ar(R, v.n, v)

v v @vk R.B
v.n v @v.n
k R.B

ar(R, v.n, v)

ar(R, v.n, v)

v v @v.n
k R.B

7. relation 7 acyclic, contains one renaming 7 b
individual a, and, contains 7 b, occur assertion A.
Clearly, input ABox HT-ABox. prove that, given HT-ABox,
calculus produces HT-ABoxes.
Lemma 4 (HT-Preservation). C set HT-clauses HT-ABox, ABox
A0 obtained applying derivation rule C HT-ABox.
Proof. Let C, A, A0 stated lemma. analyze derivation rule
Table 5 show A0 satisfies remaining conditions HT-ABoxes.
(Hyp-rule) Consider application Hyp-rule HT-clause r type (34)
mapping , deriving assertion (V ).
Assume V form yi yj @xk R.B , (V ) form @uk R.B .
Definition 5, antecedent r contains atoms form ar(R, x, yi ) ar(R, x, yj )
so, precondition Hyp-rule, contains assertions ar(R, u, s) ar(R, u, t).
u root individual either blockable individual successor
u, (V ) clearly satisfies Property (2) HT-ABoxes. Otherwise, since satisfies
Property (1) HT-ABoxes, possibilities shown Table 6, v blockable
individual, b root individuals. brevity, omit symmetric combinations
roles ar(R, u, s) ar(R, u, t) exchanged. Clearly, (V ) satisfies Property
(2) HT-ABoxes. Finally, (V ) obviously satisfies Property (4) HT-ABoxes.
Assume V form x zj , (V ) form t. Definition 5,
antecedent r contains atom Oa (zj ), either Oa (s) Oa (t) A.
Property (3) HT-ABoxes, either named individual, (V ) satisfies Property
(2) HT-ABoxes.
Assume V form R(x, x). Then, (V ) form R(s, s), satisfies
Property (1) HT-ABoxes.
197

fiMotik, Shearer, & Horrocks

Assume V form R(x, yi ) R(yi , x), (V ) form R(s, t).
Definition 5, antecedent r contains atom form S(x, yi ) S(yi , x),
either S(s, t) S(t, s) A; assertions satisfy Property (1) HT-ABoxes,
R(s, t) satisfies well.
Assume V form R(x, zj ) R(zj , x), (V ) form R(s, t).
Definition 5, antecedent r contains atom form Oa (zj ) Oa nominal
guard concept, either Oa (s) Oa (t) A; Property (3) HT-ABoxes, either
named individual, R(s, t) satisfies Property (1) HT-ABoxes.
Assume V form B(x), n S.B(x), B(yi ), (V ) form B(s)
n S.B(s). Definition 5, B literal nominal guard concept, (V )
satisfies Property (3) HT-ABoxes.
(-rule) Consider application -rule assertion n R.B(s). Property
(3) HT-ABoxes, B nominal guard concept, assertions B(ti ) introduced
rule satisfy Property (3) HT-ABoxes. Furthermore, ti introduced rule
fresh blockable successors s, role assertions introduced rule form
R(s, ti ) R(ti , s), satisfy Properties (1) (5) HT-ABoxes. inequalities
introduced rule trivially satisfy properties HT-ABoxes.
(-rule) Consider application -rule possibly annotated equality t,
merged (the annotation equality plays role here). conditions
7 relation A, ABox contains renaming t, renaming 7
renaming A0 , adding renaming introduce cycle
7. Merging replaces occurrences A, assertion A0 contains s. Hence,
7 relation A0 satisfies Property (7) HT-ABoxes.
NI -rule applicable rule precedence, so, preconditions
NI -rule Property (2) HT-ABoxes, form v a, v.i v.j,
v.i v, v.i.j v v NA ; denote property (*). Since pruning
replacements applied assertions uniformly, A0 clearly satisfies Property
(4) HT-ABoxes. Furthermore, pruning removes successors s, A0 satisfies Property
(5) HT-ABoxes. next consider types assertions change
merged t.
Consider role assertion R(s, u) changed R(t, u) A0 . either
u root individual, R(t, u) clearly satisfies Property (1) HT-ABoxes, assume
u blockable individuals. Then, u successor s, since -rule
prunes assertions contain descendant merged individual. then, (*)
since R(s, u) satisfies Property (1) HT-ABoxes, possibilities shown
Table 7. cases R(u, s) changed R(u, t) A0 merging analogous.
consider form equalities derived equalities via
merging. equality u v @sn R.C changed u v @tn R.C , resulting
equality always satisfies Property (2) HT-ABoxes. Furthermore, root individual,
u @an R.C changed u @an R.C , changed a;
however, cases, resulting equality satisfies Property (2) HT-ABoxes.
remaining cases, assume possibly annotated equality u changed possibly
annotated equality u. root individual, root individual well (the
-rule never merges root individual blockable one), u satisfies Property (2)
198

fiHypertableau Reasoning Description Logics

Table 7: Cases Application -Rule Role Assertions
R(s, u)
R(v.i, v)
R(v.i, v)
R(t.j.i, t.j)
R(v.i, v.i)
R(v.i, v.i)
R(t.j.i, t.j.i)

st
v.i v.j
v.i v
t.j.i
v.i v.j
v.i v
t.j.i

R(t, u)
R(v.j, v)
R(v, v)
R(t, t.j)
R(v.j, v.j)
R(v, v)
R(t, t)

Table 8: Cases Application -Rule Equalities
su
v.i v.k
v.i v
u.k.i u
v.i v.k
v.i v
u.k.i u
t.j.i t.j.k
t.j.i t.j
t.j.i

st
v.i v.j
v.i v.j
u.k.i u.k.j
v.i v
v.i v
u.k.i u.k
t.j.i
t.j.i
t.j.i

tu
v.j v.k
v.j v
u.k.j u
v v.k
vv
u.k u
t.j.k
t.j
tt

HT-ABoxes. Assume blockable individual. Since -rule prunes assertions
contain descendant merged individual, u successor s. (*),
Property (2) HT-ABoxes, fact NI -rule applicable A,
possibilities shown Table 8. cases, resulting assertion satisfies Property (2)
HT-ABoxes. Furthermore, replacing results A0 , A0
satisfies Property (6) HT-ABoxes.
Consider assertion C(s) changed C(t) A0 . nontrivial case
C nominal guard concept Oa . Property (3) HT-ABoxes, named
individual. -rule replaces named individuals named individuals,
named individual well. Thus, C(t) satisfies Property (3) HT-ABoxes.
(NI -rule) Consider application NI -rule equality @un R.B merges
root individual ku.hR, B, iikA . individual blockable, renaming added
7 relation A0 satisfies Property (7) HT-ABoxes. Since replaced
root individual role equality assertions, resulting assertions satisfy Properties
(1) (2) HT-ABoxes. Since named individual, assertion involving
nominal guard concept affected merging, A0 satisfies Property (3). Since pruning
replacements applied assertions uniformly, A0 clearly satisfies Property
(4) HT-ABoxes. Pruning removes successors s, A0 satisfies Property (5)

199

fiMotik, Shearer, & Horrocks

HT-ABoxes. Finally, A0 clearly empty, satisfies Property (6).
next prove soundness completeness calculus. use notions
customary resolution-based theorem proving: calculus sound derivation rules
preserve satisfiability theory, complete if, whenever calculus terminates
without detecting contradiction, theory indeed satisfiable.
Lemma 5 (Soundness). Let C set HT-clauses input ABox
(C, A) satisfiable. Then, derivation C contains branch (t)
clash-free node branch.
Proof. say model ABox A0 NI-compatible A0 following
conditions satisfied:
root individual occurring A0 , concept n R.B, 4I
aI ( n R.B)I , haI , RI , B , = (a.hR, B, ii)I
1 n.12
@un R.B A0 , huI , sI RI , huI , tI RI , sI B , tI B ,
uI ( n R.B)I .13
prove lemma, first show following property (*): (C, A0 ) satisfiable
model NI -compatible A0 A1 , . . . , ABoxes obtained applying
derivation rule C A0 , (C, Ai ) satisfiable model NI -compatible
Ai . Let model (C, A0 ) NI -compatible A0 , consider possible
derivation rules derive A1 , . . . , A0 C.
(Hyp-rule) Consider application Hyp-rule HT-clause r form (34).
Since (Ui ) A0 , |= (Ui ) 1 m. then, |= (Vj )
1 j n. Since Aj := A0 {(Vj )}, |= (C, Aj ).
|= (Vj ) atom Vj form = yk y` @xh R.B , clearly NI compatible Aj . Furthermore, Vj form , clearly h(x)I , (yk )I RI ,
h(x)I , (y` )I RI , (yk )I B , (y` )I B . Let (**) denote two properties.
Assume NI -compatible Aj 1 j n. (**), 6|= (Vj )
Vj form , (x)I 6 ( h R.B)I Vj form . Let
: NV 4I variable mapping (x) = (x)I (yk ) = (yk )I
branch variable yk occurring atom form ; furthermore, set
branch variables y1 , . . . , yh+1 occurring atom form , set (y1 ), . . . , (yh+1 )
arbitrarily chosen domain elements verify (x)I 6 ( h R.B)I . Clearly, I, 6|= Vj
Vj occurring subset (35) (36) r; furthermore, definition ,
I, 6|= Vj Vj occurring subset (35) (36) r. then,
conclude I, 6|= (C, A0 ), contradiction.
(-rule) Since n R.B(s) A0 , |= n R.B(s), implies domain
elements 1 , . . . , n 4I exist hsI , RI B 1 n, 6= j
12. Intuitively, condition ensures root individual a.hR, B, ii interpreted appropriate
neighbor aI .
13. Intuitively, condition ensures u, s, interpreted accordance annotation.

200

fiHypertableau Reasoning Description Logics

0

1 < j n. Let 0 interpretation obtained setting tIi = . Clearly,
0 |= ar(R, s, ti ), 0 |= B(ti ), 0 |= ti 6 tj 6= j, 0 |= (C, A1 ). individuals ti
root individuals, 0 NI -compatible A1 .
(-rule) Assume -rule applied assertion A0 merged
t. Since |= t, sI = tI . Pruning removes assertions, model
pruned ABox monotonicity. Merging simply replaces individual synonym,
|= (C, A1 ). Furthermore, Property (7) HT-ABoxes, contain renamings
t, kskA1 = t; hence, NI -compatible A1 .
(-rule) rule never applicable (C, A0 ) satisfiable.
(NI -rule) Assume NI -rule applied @un R.B A0
merged root individual. Since NI -compatible A0 , uI ( n R.B)I ,
huI , sI RI , sI B , sI = (u.hR, B, ii)I 1 n. Let vi = ku.hR, B, iikA0 ;
since NI -compatible, (u.hR, B, ii)I = viI . Thus, NI -rule replaces
synonym vi , |= (C, Ai ) like case -rule. vi occur
A0 , interpretation may NI -compatible Ai interpret
vi .hS, C, `i correctly. extend 0 follows. m, S, C
viI ( S.C)I , let 1 , . . . , k elements 4I hviI , j j C ;
0
clearly, k m. set (vi .hS, C, `i)I = ` 1 ` k. Since none vi .hS, C, `i
occurs Ai , 0 |= (C, Ai ), 0 NI -compatible Aj .
completes proof (*). prove main claim lemma, let
input ABox. Similarly NI -rule proof (*), extend model 0
(C, A). Since contain annotated equalities, 0 NI -compatible A.
claim lemma follows straightforward inductive application (*).
Lemma 6 (Completeness). derivation set HT-clauses C input ABox
exists leaf node labeled clash-free ABox A0 , (C, A) satisfiable.
Proof. prove lemma constructing A0 model (C, A). Since logic
finite model property, obtain model unraveling A0 intuitively
explained Section 3.1.2. usual, elements unraveled model paths (Horrocks
& Sattler, 2001, 2007), defined next.
Given individual directly blocked A0 , let blocker arbitrarily
chosen fixed individual directly blocked t.
path finite sequence pairs individuals p = [ ss00 , . . . , ssn0 ]. Let tail(p) = sn
tail0 (p) = s0n . Furthermore, let q = [p |

sn+1
]
s0n+1

n

0

path [ ss00 , . . . , ssn0 , ssn+1
]; say q
0
0

n

n+1

successor p, p predecessor q. set paths P(A0 ) defined inductively
follows:
[ aa ] P(A0 ) root individual occurring A0 ;
0

[p | ss0 ] P(A0 ) p P(A0 ), s0 successor tail(p), s0 occurs A0 , s0
blocked A0 ;
[p | ss0 ] P(A0 ) p P(A0 ), s0 successor tail(p), s0 occurs A0 , s0 directly
blocked A0 , blocker s0 A0 .
201

fiMotik, Shearer, & Horrocks

Table 9: Construction Interpretation A0

4I
aI
aI
AI
RI

=
=
=
=
=

P(A0 )
[ aa ] root individual occurs assertion A0
bI 6= b kakA0 = b
{p 4I | A(tail(p)) A0 }
{h[ aa ], pi 4I 4I
| root individual R(a, tail(p)) A0 }



{hp, [ ]i 4 4
| root individual R(tail(p), a) A0 }

{hp, [p | s0 ]i 4I 4I | R(tail(p), s0 ) A0 }
{h[p | ss0 ], pi 4I 4I | R(s0 , tail(p)) A0 }
{hp, pi 4I 4I
| R(tail(p), tail(p)) A0 }

Let interpretation constructed A0 shown Table 9. A0 HT-ABox,
4I empty. show that, ps form [ ss0 ] [qs | ss0 ]
individual w, following claims hold (*):
R(s, s) A0 (resp. A(s) A0 ) iff hps , ps RI (resp. ps AI ): Immediate
definition I.
B(w) A0 LA0 (w) = LA0 (s0 ) B literal concept, ps B : proof
immediate B atomic. B = A, since -rule applicable A0 ,
A(w) 6 A0 ; then, A(s0 ) 6 A0 A(s) 6 A0 , case
atomic concepts implies ps 6 AI .
n R.B(s) A0 , ps ( n R.B)I : definition paths, blocked;
since -rule applicable n R.B(s), individuals u1 , . . . , un exist
ar(R, s, ui ) A0 B(ui ) A0 1 n, ui 6 uj A0 1 < j n.
assertion ar(R, s, ui ) satisfies Property (1) HT-ABoxes, ui
one following forms.
ui = s. Let pui = ps . then, previous two cases conclude
ar(R, s, ui ) A0 B(ui ) A0 imply hps , pui RI pui B .
ui successor s. ui directly blocked blocker vi , let pui = [ps | uvii ];
otherwise, ui blocked blocked, let pui = [ps | uuii ].
Either way, ar(R, tail(ps ), ui ) A0 , which, definition I, implies hps , pui RI . Furthermore, B(ui ) A0 LA0 (ui ) = LA0 (tail(pui )) imply
pui B .
ui blockable predecessor s. Since blockable, ps = [qs | ss0 ];
hence, let pui = qs . s0 blocked, = s0 tail(pui ) = ui ,
ar(R, s0 , tail(pui )) A0 . s0 blocked blocker s, definition
pairwise blocking LA0 (tail(pui ), s0 ) = LA0 (ui , s) LA0 (s0 , tail(pui )) = LA0 (s, ui ),
ar(R, s0 , tail(pui )) A0 . Either way, hps , pui RI

202

fiHypertableau Reasoning Description Logics

definition I. Furthermore, B(ui ) A0 LA0 (ui ) = LA0 (tail(pui )) imply
pui B .
ui satisfy previous three conditions. blockable
individual, ui root individual, let pui = [ uuii ]. root individual,
ui blocked A0 Condition 3.2 -rule, pui 4I
exists form pui = [p | uuii ]. Either way, ar(R, s, ui ) A0
B(ui ) A0 , imply hps , pui RI pui B .
Consider 1 < j n. tail0 (pui ) 6 tail0 (puj ) A0 , since 6 A0
-rule applicable, tail0 (pui ) 6= tail0 (puj ), pui 6= puj . Furthermore,
tail0 (pui ) 6 tail0 (puj )
/ A0 , tail0 (pui ) 6= ui , possible s0
directly blocked blocker ui = ui blockable predecessor s. Note,
however, one blockable predecessor,
one ui ui = s. Therefore, ui 6= uj , implies pui 6= puj ,
conclude ps ( n R.B)I .
assertion 0 A0 form b 6 b b named individuals,
straightforward see |= 0 . Furthermore, 0 form R(a, b) B(a),
n R.B(a) named individual, (*) implies |= 0 . Consider A.
induction application derivation rules, straightforward show that,
6 A0 , A0 contains renamings that, applied , produce assertion 0 A0 .
then, since |= 0 , |= definition I.
remains shown |= C. Consider HT-clause r C containing atoms
form Ai (x), Uk (x, x), ar(Ri , x, yi ), Bi (yi ), Cj (zj ) antecedent. Furthermore,
consider variable mapping antecedent r true is,
px AIi , hpx , px UkI , hpx , pyi RiI , pyi BiI , pzj CjI px = (x), pyi = (yi ),
pzj = (zj ). Let = tail(px ), s0 = tail0 (px ), t0i = tail0 (pyi ). definition
fact LA0 (s0i ) = LA0 (si ), Ai (s) A0 , Uk (s, s) A0 , Bi (t0i ) A0 .
Depending relationship px pyi , define ti follows.
pyi successor px pyi = px . Let ti = t0i . Clearly, Bi (ti ) A0 ; furthermore,
definition hpx , pyi RiI imply ar(Ri , s, t0i ) A0 , ar(Ri , s, ti ) A0 .
pyi predecessor px . following cases.
directly blocks s0 . Let ti predecessor s; ti exists since
blockable. definition hpx , pyi RiI imply ar(Ri , s0 , tail(pyi )) A0
B(tail(pyi )) A0 , definition pairwise blocking conclude
ar(Ri , s, ti ) A0 Bi (ti ) A0 .
s0 blocked. Let ti = t0i . definition I, Bi (ti ) A0
ar(Ri , s, ti ) A0 .
pyi px match conditions mentioned thus far. definition
I, either px pyi form [ aa ]. Let ti = tail(pyi ). hpx , pyi RiI
definition I, conclude Bi (ti ) A0 ar(Ri , s, ti ) A0 .

203

fiMotik, Shearer, & Horrocks

Definition 5, antecedent r contains atom form Oa (zj ) nominal
variable zj . Thus, definition Property (3) HT-ABoxes, pzj
u
form [ ujj ] uj named individual; furthermore, Cj (uj ) A0 .
Let mapping (x) = s, (yi ) = ti , (zj ) = uj . Clearly, neither
ti indirectly blocked, (Uj ) A0 atom Uj antecedent r.
Hyp-rule applicable r, A0 , , r contains atom Vi consequent
(Vi ) A0 . Depending type Vi , following possibilities.
Assume Vi form yi yj @xk S.B ; thus, ti tj @sk S.B A0 . Since
-rule applicable A0 , ti = tj . Definition 5, r contains subclause
form (35) (36), antecedent r contains atoms ar(S, x, yi ) ar(S, x, yj );
therefore, hpx , pyi hpx , pyj . NI -rule applicable ti tj @sk S.B
so, preconditions NI -rule, root individual, ti (tj ) either root
individual successor s. rules possibility px form [ aa ]
pyi (pyj ) neither successor px form [ bb ]. Hence, construction I,
pyi (pyj ) either successor px , equal px , predecessor px ,
form [ aa ]. consider following cases (w.l.o.g. omit symmetric cases
obtained swapping pyi pyj ):
pyi form [ aa ]. Then, ti = tj implies pyi = pyj definition paths.
pyi successor px . Then, pyi = [px | utii ] ui = ti ti blocked ui
blocker ti . Either way, ti different predecessor (if latter
exists). following possibilities pyj :
u

pyj successor px . Then, pyj = [px | tjj ], ti = tj clearly implies pyi = pyj .
pyj = px pyj predecessor px . tj = tj predecessor
s, contradicts fact ti 6= tj .
pyi = px . ti = s. nontrivial case pyj predecessor px ;
then, tj 6= s, contradicts fact ti 6= tj .
pyi predecessor px . remaining possibility pyj predecessor px . Since px one predecessor, pyi = pyj .
Thus, conclude I, |= r.
Assume Vi form x zj ; thus, uj A. Since -rule
applicable A0 , = uj . Since uj named individual, cannot block
individuals, s0 = s, implies px = pzj . Thus, I, |= r.
Assume Vi form Ti (x, x); thus, Ti (s, s) A0 . (*),
hpx , px RiI . Thus, I, |= r.
Assume Vi form Di (x) Di literal concept form n T.B;
thus, Di (s) A0 . (*), px DiI . Thus, I, |= r.
Assume Vi form Ei (yi ) Ei literal concept; thus, Ei (ti ) A0 .
already established LA0 (ti ) = LA0 (t0i ); (*), pyi EiI . Thus,
I, |= r.

204

fiHypertableau Reasoning Description Logics

Assume Vi form ar(Si , x, yi ), ar(Si , s, ti ) A0 . definition
blocking, ar(Si , s0 , t0i ) A0 . Finally, definition I, hpx , pyi SiI .
Thus, I, |= r.
Assume Vi form ar(Sj , x, zj ), ar(Sj , s, uj ) A0 . Since uj named
individual, definition hpx , pzj SjI . Thus, I, |= r.
next prove termination hypertableau calculus.
Lemma 7 (Termination). set HT-clauses C input ABox A, let |C, A|
sum size A, number concepts roles C, dlog ne
integer n occurring C atom form n R.B yi yj @xn R.B . total
number individuals introduced path derivation C
doubly exponential |C, A|, derivation C finite.
Proof. prove claim showing (i) derivation rule applied
fixed set individuals derivation path, (ii) number new individuals
introduced derivation path doubly exponential |C, A|. supply
blockable individuals infinite, assume blockable individual introduced
twice derivation path. Furthermore, root individual removed ABox
A0 due merging, renaming added A0 ensures kskA0 6= s. renaming
added A0 , ABoxes occurring A0 derivation contain renaming
well, subsequent application NI -rule reintroduce s.
Next, prove (i) considering derivation rule.
application Hyp-rule HT-clause r form (34) mapping
introduces assertion (Vi ), prevents subsequent reapplication
Hyp-rule r . Merging pruning remove (Vi ) subsequent
derivation steps, also removes least one individual occurring
set potential premises Hyp-rule, thus preventing reuse
future application Hyp-rule r.
application -rule assertion n R.B(s) introduces t1 , . . . , tn fresh
successors assertions B(ti ), ar(R, s, ti ), ti 6 tj 1 < j n.
Thus, individuals u1 , . . . , un Condition 3 -rule matched
t1 , . . . , tn . Furthermore, root individual, none ti become blocked
Condition 3.2 always satisfied ti ; moreover, blockable, Condition 3.2
trivially satisfied ti . ti merged another individual v, B(v),
ar(R, s, v), v 6 tj added ABox, ABox still contains individuals
matched Condition 3 -rule. Finally, ti becomes indirectly
blocked, blocked -rule applicable s.
application -rule removes either t, rule cannot
reapplied
application -rule produces ABox labels derivation leaf.
application NI -rule equality @un R.B removes s, rule
cannot reapplied n R.B, u.
205

fiMotik, Shearer, & Horrocks

Next, prove (ii)that is, total number individuals introduced derivation path doubly exponential |C, A|. path length n individuals
ABox A0 sequence individuals u0 , u1 , . . . , un u0 = s, un = t,
and, 0 n 1, either R(ui , ui+1 ) A0 R(ui+1 , ui ) A0 R atomic role.
root path root individual ABox A0 path named
individual intermediate individuals ui , 1 n 1, root individuals.
level lev(t) length shortest root path t. Thus, lev(t) = 0
named individual.
depth dep(t) individual number ancestors t. Thus, dep(t) = 0
root individual. Due Property (5) HT-ABoxes, individual occurs
ABox A0 , A0 contains path length dep(t) root individual
individuals ui , 0 n 1, ancestors t; since individual
one predecessor, ui also ancestors t.
show maximum level root individual maximum depth
every individual exponential size C A.
application derivation rule never increases level individual.
named individual never pruned merged another named
individual,14 root individual merged another root individual.
rule applications make root path shorter, longer.
Let number atomic concepts n number atomic roles occur
C, let = 22m+2n + 1, let A0 ABox labeling node derivation
C. next show (1) dep(t) individual occurring A0 , (2)
root individual, lev(t) .
(Claim 1) pair individuals occurring A0 , 2m different
possible labels LA0 (s) 2n different possible labels LA0 (s, t). Thus, A0 contains least
= 2m 2m 2n 2n + 1 predecessor-successor pairs blockable individuals, A0 must
contain two pairs hs, s.ii ht, t.ji following conditions satisfied:
LA0 (s.i) = LA0 (t.j)
LA0 (s, s.i) = LA0 (t, t.j)

LA0 (s) = LA0 (t)
LA0 (s.i, s) = LA0 (t.j, t)

Since contains ancestor relation, path A0 containing blockable individuals
must include least one blocked individual, blockable individual depth must
blocked. -rule applied individuals blocked, rule cannot
introduce individual u dep(u) > .
(Claim 2) show following stronger claim (*) holds root individual
occurring assertion A0 (the symmetry applies usual):
1. lev(s) ;
2. R(s, t) A0 R(t, s) A0 u @sn R.B A0 blockable nonsuccessor
s, lev(s) + dep(t) ;
3. A0 blockable nonsuccessor (where equality annotated), lev(s) + dep(t) + 1.
14. derivation rule replaced named individual individual named, levels
root individuals could increase.

206

fiHypertableau Reasoning Description Logics

claim clearly true input ABox labeling root derivation,
contains named individuals. assume (*) holds ABox A0
consider possible derivation rules applied A0 .
Assume Hyp-rule derives assertion R(s, t) R(t, s), root
individual blockable nonsuccessor s. Let R(x, y) R(y, x) atom
consequent HT-clause r instantiated derivation rule.
following two possibilities antecedent r.
antecedent r contains atom form S(x, y) S(y, x)
matched assertion form S(s, t) S(t, s) A0 . Since A0 satisfies (*),
resulting ABox satisfies (*) well.
antecedent r contains atom form Oa (x) Oa (y) matched
assertion form Oa (s) A0 (since blockable, A0 cannot contain
Oa (t) Property 3 HT-ABoxes). dep(t) lev(s) = 0,
resulting ABox satisfies (*) well.
Assume Hyp-rule derives assertion u @sn R.B , root individual blockable nonsuccessor s. Definition 5, antecedent
HT-clause contains atoms form ar(R, x, yi ) ar(R, x, yj )
matched assertions ar(R, s, t) ar(R, s, u) A0 . Since A0 satisfies (*),
lev(s) + dep(t) , resulting equality satisfies Item 2 (*). show
u @sn R.B satisfies Item 3 (*), assume u root individual
nonsuccessor u. Since A0 contains ar(R, s, u), lev(u) lev(s) + 1;
then, lev(u) + dep(t) + 1, required.
Hyp-rule derives assertion t, root individual
blockable nonsuccessor s, remaining possibility consequent
HT-clause contains equality x zj . Definition 5, antecedent
contains Oa (zj ) matched assertion Oa (s) A0 , named
individual. dep(t) lev(s) = 0, resulting ABox satisfies (*).
Assume -rule introduces assertion form R(s, t) R(t, s)
fresh individual. Individual always successor s, resulting ABox
trivially satisfies (*).
Assume -rule applied assertion form u u
merged s. definition merging, dep(u) dep(s) u
pruned. blockable individual, u blockable well, resulting
ABox satisfies (*) u replaced individual equal smaller depth.
Therefore, assume root individual consider types assertions
added A0 result merging.
R(u, u) changed R(s, s), resulting ABox clearly satisfies (*).
Assume R(u, t) root individual changed R(s, t).
inference make root paths shorter longer,
levels decrease rather increase. Thus, resulting
ABox satisfies Item 1 (*).
207

fiMotik, Shearer, & Horrocks

Assume R(u, t), predecessor u, changed R(s, t);
nontrivial case blockable nonsuccessor s. Since
predecessor u, dep(t) + 1 = dep(u); since A0 satisfies (*),
lev(s) + dep(u) + 1; then, lev(s) + dep(t) required.
cases R(t, u) changed R(t, s) analogous.
Assume possibly annotated equality v u changed v s.
nontrivial case v blockable nonsuccessor s. u root
individual, level merging bounded min(lev(s), lev(u))
merging, (*) preserved. u v blockable individuals,
Property (2) HT-ABoxes, either u ancestor v, u v
siblings, v ancestor u. u ancestor v, pruning u
removes v u A0 . v sibling ancestor u, u must
nonsuccessor s, lev(s) + dep(u) + 1; then, dep(v) dep(u),
lev(s) + dep(v) + 1 (*) preserved.
Assume v v 0 @un R.B changed v v 0 @sn R.B v @sn R.B .
nontrivial case v blockable nonsuccessor s. Since u pruned
merging, Properties (2) (4) HT-ABoxes v must predecessor u, dep(v) + 1 = dep(u). Furthermore, properties u
must blockable nonsuccessor s, lev(s) + dep(u) + 1. then,
lev(s) + dep(v) , required.
application -rule trivially preserves (*).
Assume NI -rule applied assertion @un R.B replacing
root individual v = ku.hR, B, iikA0 . v already occurs assertion A0 , v
satisfies Item 1 (*). If, however, v fresh, Property (4) HT-ABoxes v
connected u role assertion, lev(v) lev(u) + 1. Furthermore, since
blockable nonsuccessor u, lev(u) + dep(s) . Finally, since blockable,
dep(s) 1, lev(u) 1. consequence, conclude lev(v) ,
proves Item 1 (*). proof assertions introduced merging satisfy
(*) analogous case -rule.
complete proof claim (ii)that is, total number individuals
introduced derivation rules doubly exponential |C, A|.
named individuals level 0 never introduced derivation rules.
application NI -rule root individual u level ` introduce n root
individuals level ` + 1 concept n R.B occurs C. Thus, named
individual, derivation rules create tree root individuals. maximum depth
tree , exponential |C, A|. Furthermore, maximum branching factor
b equal sum numbers occurring C atoms form yi yj @xn R.B .
Clearly, b exponential |C, A|, tree doubly exponential |C, A|.15
Similarly, root individual become root tree blockable individuals
depth . blockable individual introduced applying -rule predecessor.
15. numbers coded unary, branching factor would polynomial, tree
would still doubly exponential |C, A|.

208

fiHypertableau Reasoning Description Logics

Furthermore, -rule applied individual concept
form n R.B. Thus, branching factor exponential assuming binary coding
numbers, tree doubly exponential |C, A|.
Thus, total number individuals appearing derivation doubly exponential |C, A|. Since branching factor derivation exponentially bounded
|C, A|, derivation finite.
state main theorem section.
Theorem 1. satisfiability SHOIQ+ knowledge base K decided computing
K0 = ((K)) checking whether derivation (K0 ) contains leaf node
labeled clash-free ABox. algorithm implemented runs
2NExpTime |K|.
Proof. first part theorem follows immediately Lemmas 1, 2, 5, 6.
Lemma 7, total number individuals doubly exponential |A (K0 ), R (K0 )|.
Since structural transformation polynomial, total number individuals doubly
exponential |K|. Thus, existence leaf derivation node labeled clash-free
ABox checked nondeterministically applying hypertableau derivation rules
construct ABox doubly exponential |K|.

5. Discussion
section discuss possibilities optimizing blocking condition single
subset blocking; furthermore, argue modifying algorithm make optimal
w.r.t. worst-case complexity might difficult.
5.1 Single Blocking
DLs SHOQ+ provide inverse roles, pairwise blocking
weakened atomic single blocking, defined follows.
Definition 9 (Atomic Single Blocking). Atomic single blocking obtained pairwise
blocking (see Definition 7) changing notion direct blocking: blockable individual
directly blocked blockable individual blocked, s,
LA (s) = LA (t) LA (s) Definition 7.16
cases, simpler blocking condition make hypertableau algorithm
construct smaller ABoxes, lead increased efficiency. next formalize
notion HT-clauses atomic single blocking applicable.
Definition 10 (Simple HT-Clause). HT-clause r simple satisfies following
restrictions, x center variable, yi branch variable, zj nominal variable, B literal
concept, R atomic role:
atom antecedent r form A(x), R(x, x), R(x, yi ), A(yi ),
A(zj ).
16. name atomic reflects fact LA (s) contains atomic concepts.

209

fiMotik, Shearer, & Horrocks

atom consequent r form B(x), h R.B(x), B(yi ), R(x, x),
R(x, yi ), R(x, zj ), x zj , yi yj .
straightforward see that, K SHOQ+ knowledge base, R (K) contains
simple HT-clauses. completeness hypertableau algorithm atomic single
blocking simple HT-clauses straightforward show.
Lemma 8. Let C set simple HT-clauses, input ABox. derivation
atomic single blocking C exists leaf node labeled clash-free
ABox A0 , (C, A) satisfiable.
Proof. slightly modifying proof Lemma 4, possible show following
property (*): atom A0 involving atomic role form R(s, a), R(s, s),
R(s, s.i), named individual individual.
Let model constructed way Lemma 6, using single
blocking. Due (*), whenever hp1 , p2 RI , p2 either form [ aa ] named
individual, successor p1 , p2 = p1 . proof model (C, A)
straightforward consequence following observations proof Lemma 6:
proof n R.B(s) A0 implies ps ( n R.B)I , individual ui never
blockable predecessor s. Thus, labels LA0 (s, ui ), LA0 (ui , s), LA0 (ui )
never relevant.
proof |= C, possible pyi predecessor px . Thus, labels
LA0 (s0 , tail(pyi )), LA0 (tail(pyi ), s0 ), LA0 (tail(pyi )) never relevant.
proof model (A, C) thus requires LA0 (s) = LA0 (t) hold
blocked blocker t; hence, model (A, C) even atomic single blocking
used.
following variant single blocking also applied DLs inverse roles
number restrictions, SHOI.
Definition 11 (Full Single Blocking). Full single blocking obtained atomic single
blocking (see Definition 9) changing definition LA (s) follows:
LA (s) = { C | C(s) C form 1 R.B
atomic B literal concept }
directly block atomic single blocking, suffices occur
atomic concepts A. Intuitively, model construction
Lemma 6 copies nonatomic concepts s; hence, assertions form C(s)
C atomic relevant. contrast, full single blocking, must
occur exactly concepts (apart negated atomic concepts). Intuitively,
given clash-free ABox A0 derivation rule applicable, model (A, C)
constructed A0 replacing t; result model, two individuals
must occur exactly concepts.

210

fiHypertableau Reasoning Description Logics


T.C



b
C
R.D

R

c

.C




C
R.D

Figure 11: Problems Single Blocking

Full single blocking must applied care hypertableau setting. Consider
following knowledge base K9 , consisting ABox A9 set HT-clauses C9 .
(37)

A9 = { T.C(a) }
C9 = {C(x) R.D(x), D(x) .C(x), R(x, y1 ) S(x, y2 ) }

K9 , hypertableau algorithm full single blocking produces ABox shown
Figure 11. individual blocked b, algorithm terminates; expansion
R.D(d), however, would reveal K9 unsatisfiable. problem arises
HT-clause R(x, y1 ) S(x, y2 ) contains two role atoms, allows HT-clause
examine successor predecessor x. Full single blocking, however,
ensure predecessors successors x fully built. correct
problem requiring normalized GCIs contain one R.C concept.
example, replace HT-clause R(x, y1 ) Q(x) Q(x) S(x, y2 ) ,
first HT-clause would additionally derive Q(b), would blocked b.
apply full single blocking DL SHOI provided HT-clause contains one role atom antecedent. always ensure suitably
renaming complex concepts atomic ones.
Lemma 9. Let ABox C set HT-clauses that, r C, ( i) r
contains atoms form R(x, x), ( ii) antecedent r contains one role
atom, ( iii) at-least restriction concepts form 1 S.B role B
literal concept. derivation full single blocking C exists leaf
node labeled clash-free ABox A0 , (C, A) satisfiable.
Proof. Let A00 obtained A0 removing assertion containing indirectly
blocked individual. Since derivation rule applicable indirectly blocked individuals,
derivation rule applicable A00 C. individual occurring A00 , let
[s]A00 = blocked A00 , let [s]A00 = s0 blocked A00 blocker s0 .
Note following useful property (*): A(s) A00 , A(s) 6 A00 since -rule
applicable A00 ; then, A([s]A00 ) 6 A00 well.
construct interpretation A00 follows.
4I
sI
AI
RI

=
=
=
=

{s | occurs A00 blocked A00 }
[s]A00 individual occurring A00
{[s]A00 | A(s) A00 }
{h[s]A00 , [t]A00 | R(s, t) A00 }
211

fiMotik, Shearer, & Horrocks

straightforward see |= A00 . Consider HT-clause r C contains
antecedent one atom form R(x, y), well atoms form Ai (x), Bi (y),
Ci (zi ). Let mapping variables r individuals A00
|= (Ui ) atom Ui antecedent r. definition I, individuals
exist R(s, t) A00 , (x) = [s]A00 , (y) = [t]A00 . definition
full single blocking, Ai (s) A00 Bi (t) A00 well. Furthermore, since
zi occurs nominal guard concept, (zi ) named individual. Let 0
0 (x) = s, 0 (y) = t, 0 (zi ) = (zi ). Since Hyp-rule applicable C A00
0 , 0 (Vj ) A00 atom Vj consequent r. Consider
possible forms Vj have.
Vj = S(x, y), |= S((x), (y)) definition I. case Vj = S(y, x)
analogous.
Vj = A(x) atomic concept, A([s]A00 ) A00 definition full
single blocking; then, |= A((x)) definition I. case Vj = A(y)
analogous.
Vj = A(x), A([s]A00 ) 6 A00 (*); then, definition
|= A((x)). case Vj = A(y) analogous.
Vj = D(x) = 1 R.B, D([s]A00 ) A00 definition full single
blocking. Since -rule applicable [s]A00 , individual exists
ar(R, s, t) A00 B atomic, B(t) A00 , B = A, A(t) 6 A00 .
definition full single blocking, B atomic, B([t]A00 ) A00 ,
B = A, A([t]A00 ) 6 A00 . definition I, h[s]A00 , [t]A00 RI ,
[t]A00 B ; therefore, |= D((x)). case Vj = D(y) analogous.
Vj = x zi , 0 (x) 0 (zi ) A00 ; since -rule applicable A00 ,
0 (x) = 0 (zi ). then, since named individuals cannot block individuals,
(x) = 0 (x); hence, |= (x) (zi ).
Thus, cases |= (Vj ). case r contain role atom R(x, y)
antecedent analogous, |= (A, C).
5.2 Subset Blocking
tableau algorithms DLs without inverse roles, full single blocking condition
Definition 11 weakened full subset blocking (Baader et al., 1996).
Definition 12 (Full Subset Blocking). Full subset blocking obtained full single
blocking (see Definition 11) changing notion direct blocking: blockable individual
directly blocked individual blocked, s, LA (s) LA (t).
Full subset blocking problematic hypertableau setting. Consider knowledge
base consists ABox A10 TBox corresponding HT-clauses C10 .
(38)

A10 =
{ T.C(a) }

C(x) R.C(x),
C(x) S.D(x),
C10 =
S(x, y) D(y) E(x), R(x, y) E(y)
212

fiHypertableau Reasoning Description Logics

T.C



C
R.C
S.D
E
b

R

c



C
R.C
S.D

Figure 12: Problems Full Subset Blocking

K10 , algorithm produce ABox shown Figure 12, blocked
b. If, however, expand S.D(d) S(d, e) D(e), derive E(d); together
R(b, d) HT-clause R(x, y) E(y) , get contradiction.
problem arises because, hypertableau setting, syntactic distinction atomic inverse roles lost: atom R (x, y) transformed (by function
ar) semantically equivalent atom R(y, x). HT-clause S(x, y) D(y) E(x)
seen including implicit inverse role, examines successor x
antecedent order derive new information x consequent, thus mimicking
behavior tableau algorithms semantically equivalent GCI v .E.
semantically equivalent inverse-free GCI S.D v E would, hypertableau
algorithm, transformed exactly HT-clause. tableau setting, however, GCI would treated differently: would result v-rule deriving (E S.D)(s) individuals s. similar effect could achieved hypertableau setting translating S.D v E two HT-clauses: > E(x) Q(x)
Q(x) S(x, y) D(y) . introduces nondeterminism, solves problem
full subset blocking deriving either E(c) Q(c), first leads immediate
contradiction, second delays blocking.
general, easy see full subset blocking could used hypertableau
setting modifying preprocessing phase ensure HT-clauses include
implicit inverses. clear, however, would useful: would result
(possibly) smaller ABoxes, cost (possibly) larger derivation trees.
5.3 Number Blockable Individuals
Buchheit et al. (1993) presented tableau algorithm DL ALCN R which, due
anywhere blocking, runs NExpTime instead 2NExpTime, Donini et al. (1998)
presented similar result basic DL ALC. interesting compare algorithms
see whether anywhere blocking improve worst-case complexity
algorithm K SHIQ+ knowledge base. case, HT-clause (K)
contains nominal guard concept, prevents derivation assertions satisfying
preconditions NI -rule; hence, new root individuals introduced derivation,
eliminates significant source complexity.
following example shows that, unfortunately, anywhere blocking improve worst-case complexity; fact, identify tension and- or213

fiMotik, Shearer, & Horrocks

branching. example, use well-known encoding binary numbers concepts B0 , B1 , . . . , Bk1 : assign individual ABox binary number
`A (s) = bk1 . . . b1 b0 bi = 1 Bi (s) A. Using k concepts,
thus encode 2k different binary numbers. Furthermore, atomic role R, using
well-known R-successor counting formula (Tobies, 2000), ensure that, whenever
individual R-successor A, `A (t) = (`A (s) + 1) mod 2k ; omit
formula sake brevity. Let K11 following knowledge base. sake
brevity, omit HT-clauses corresponding axioms K11 .
(39)

C(a)

(40)

C v L.C u R.C

(41)

(The R-successor formula B0 , . . . , Bk1 )

(42)

(The L-successor formula B0 , . . . , Bk1 )

(43)

B0 u . . . u Bk1 v

(44)

L.A u R.A v

Figure 13 schematically presents derivation K11 doubly exponential
number blockable individuals introduced.17 simplicity presentation, use
single anywhere blocking. Due (39)(42), algorithm create individuals a.1, a.2,
a.1.1, a.1.2, a.1.1.1, a.1.1.2, on, s.1 L-successor s, s.2
k
k
R-successor s. creating individuals form a.12 1 .1 a.12 1 .2
k
12 1 string 2k 1 ones, individual x.1 blocks x.2 (c.f. Figure 13a). then, due
k
k
k
(43), a.12 1 .1 a.12 1 .2 become instances A. (44), a.12 1 made instance
k
k
well, block sibling a.12 2 .2 more; hence, a.12 2 .2
expanded exponential depth (c.f. Figure 13b). repeating process, algorithm
k
k
derives a.12 2 instance A, block sibling a.12 3 .2 (c.f.
Figure 13d). Eventually, algorithm constructs binary tree exponential depth, thus
creating doubly-exponential number blockable nodes total (c.f. Figure 13d).
Buchheit et al. Donini et al. obtained nondeterministic exponential behavior
applying u-, t-, -, v-rules exhaustively applying -rule.
strategy ensures label individual fully constructed introducing
successor s, prevents individuals indirectly blocked. K11 ,
means GCI (44) applied individual introducing successors.
Thus, existentials expanded, assertion (L.A R.A A)(s)
introduced one disjunct chosen nondeterministically. choices (L.A)(s)
(R.A)(s) lead clash, algorithm eventually derives A(s), expands
existentials introduces s.1 s.2. Thus, generating exponential
models, algorithm incurs massive amount nondeterminism.
Nondeterministic exponential behavior guaranteed hypertableau algorithm
nondeterministically fixing label individual applying -rule it.
technique similar one used Tobies (2001) order obtain PSpace
17. Initially, suggested informally algorithm run NExpTime SHIQ (Motik, Shearer,
& Horrocks, 2007). example shows, case.

214

fiHypertableau Reasoning Description Logics


a.2

a.1

a.2

|2 k



3|

a.1



x

x


x.1

x.2

x.1

(a) exponential path constructed
blockable individual blocking
sibling. individual contains
label.

(b) Adding label x.1 unblocks
x.2.


a.1

x.2


a.2


x




x.1

x.2

(c) Adding label x.2 makes x.2
blocked, forces addition
label x. unblocks sibling
x another subtree created.

(d) Derivation terminates exponential number
unblocked individuals, doubly-exponential
number indirectly blocked individuals.

Figure 13: Creation Exponentially Deep Binary Tree Blockable Individuals

215

fiMotik, Shearer, & Horrocks

decision procedure concept satisfiability DL inverse roles without GCIs.
performance results Section 7, however, seem suggest might
beneficial practice. Still, might worth exploring whether nondeterministically adding
concepts labels individuals used optimization would detect early
blocks thus prevent construction large models.
5.4 Number Root Individuals
SHOIQ NExpTime-complete (Tobies, 2000), straightforward extend
result SHOIQ+ . Thus, one might wonder whether complexity result Theorem 1
sharpened obtain worst-case optimal decision procedure. This, unfortunately,
case: present example algorithm generates doubly-exponential
number root individuals. construct K12 extending K11 (axioms (39)(44))
following two axioms:
(45)

B0 u . . . u Bk1 v {b}

(46)

v 2 L .> u 2 R .>

shown Section 5.3, axioms K11 cause algorithm construct
binary tree blockable individuals exponential depth. Axiom (45) K12 , however,
merges leaves tree single named individual b, axiom (46) ensures
N I-rule applied remaining blockable individuals, beginning
neighbors b. If, application N I-rule, always merge blockable individuals
root individuals shown Figure 14a, algorithm constructs ABox shown
Figure 14b, contains two binary trees root individuals depth 2k/2 . Unlike
case K11 , fully constructing individual labels avoid double-exponential
behavior, since promotion blockable individuals root individuals prevents blocking.

6. Algorithm Optimizations
DL reasoning algorithms often used practice compute classification knowledge
base Kthat is, determine whether K |= v B pair atomic concepts
B occurring K. nave classification algorithm would involve quadratic number calls
subsumption checking algorithm, potentially highly expensive.
obtain acceptable levels performance, various optimizations developed
reduce number subsumption checks time required check (Baader,
Hollunder, Nebel, Profitlich, & Franconi, 1994). well-known dependency-directed backtracking optimization (Horrocks, 2007) readily used hypertableau calculus.
Furthermore, developed two simple optimizations that, best knowledge, considered previously literature.
6.1 Reading Classification Relationships Concept Labels
Let (A, C) ABox set HT-clauses obtained clausifying knowledge base
K, let B atomic concepts want check whether K |= v B;
since B atomic, case (A0 , C) unsatisfiable
216

fiHypertableau Reasoning Description Logics



L

L

L

R

R

L

b.hL, >, 1i.hL, >, 1i

R

R

L

b.hL, >, 1i.hL, >, 2i

L

R

R

L

b.hL, >, 2i.hL, >, 1i

b.hL, >, 1i

R

b.hL, >, 2i.hL, >, 2i

b.hL, >, 2i

b
(a) root introduction strategy N I-rule


L
L

R

R

L

R

|2k/2 1|

|2k/2 1|

b
(b) resulting tree, containing doubly-exponential
number root individuals

A0 = {A(a), B(a)} fresh individual. Let A1 clash-free ABox labeling
leaf derivation (A0 , C). use A1 learn following things
subsumption K. proofs claims straightforward.
1. C(a) A1 concept C derivation C(a) depend
nondeterministic choice, K |= v C.
217

fiMotik, Shearer, & Horrocks

2. A1 obtained A0 deterministically, K |= v C C(a) A1 .
3. C(b) A1 D(b) 6 A1 C concepts b individual
blocked, K 6|= C v D.
Thus, K deterministic, classify using linear number calls hypertableau algorithm: atomic concept A, check satisfiability (A {A(a)}, C);
algorithm produces clash-free ABox A1 , set subsumers contained
LA1 (a). optimizations applicable case tableau algorithms well;
however, might less effective due increased or-branching.
6.2 Caching Blocking Labels
Let R SHIQ+ TBox RBox, respectively, let C = R (T R); since
contain nominals, assertions involving nominal guard concepts needed.
Furthermore, assume classification R involves n calls hypertableau
algorithm ({Ai (ai ), Bi (ai )}, C). Then, derivation ({Ai (ai ), Bi (ai )}, C) contains
leaf node labeled clash-free ABox Ai , use nonblocked individuals
Ai blockers subsequent satisfiability checks ({Aj (aj ), Bj (aj )}, C) j > i.
simple consequence following fact. Let I1 I2 two models R
4I1 4I2 = ; furthermore, let defined 4I = 4I1 4I2 , AI = AI1 AI2 ,
RI = RI1 RI2 , atomic concept atomic role R. Then, simple
induction structure axioms R, trivial show |= R.
property hold presence nominals, impose bound
number elements interpretation concept; bound could satisfied I1
I2 individually, violated I.
optimization correct because, instead ({Ai (ai ), Bi (ai )}, C), check
satisfiability (Ai {Ai (ai ), Bi (ai )}, C), use individuals
Ai potential blockers due anywhere blocking. optimization seen
simple form model caching (Horrocks, 2007), key obtaining results
present Section 7. example, GALEN one subsumption test costly
computes substantial part model TBox; subsequent subsumption
tests reuse large parts model.
practice, need keep entire ABox Ai around; rather, nonblocked blockable individual predecessor t0 , simply need retain sets LAi (t),
LAi (t0 ), LAi (t, t0 ), LAi (t0 , t).

7. Implementation Evaluation
Based calculus Section 4, implemented prototype DL reasoner called
HermiT. order estimate well calculus performs practice, compared
HermiT two state-of-the-art tableau reasoners several practical problems.
objective evaluation establish superiority HermiT, compare
behavior calculus tableau calculi used many existing systems,
demonstrate usefulness calculus realistic problems.
important understand HermiT prototype, always
outperform well-established reasoners. particular, HermiT may uncompetitive
218

fiHypertableau Reasoning Description Logics

ontologies specialized optimizations needed good performance. example, HermiT cannot process SNOMED CT ontology due large number
concepts, many reasoners classify ontology easily. reasoners, however, employ techniques quite different standard tableau algorithm;
example, EL++ ontology SNOMED CT, Pellet uses reasoning algorithm
Baader, Brandt, Lutz (2005), reasoners employ specialized techniques
well (Haarslev, Moller, & Wandelt, 2008). Similarly, artificial test problems
used TANCS comparison Tableaux98 conference (Balsiger & Heuerding, 1998;
Balsiger, Heuerding, & Schwendimann, 2000) DL98 workshop (Horrocks & PatelSchneider, 1998b) often either easy reasoners employing particular optimizations
difficult due fact encode large propositional satisfiability problems
(Horrocks & Patel-Schneider, 1998a). Since goal demonstrate usefulness
hypertableau calculus realistic problems, chosen ignore ontologies
test problems, mainly test specialized calculi optimizations applicable various sublanguages SHOIQ+ . Instead, focus evaluation practical
ontologies main difficulty due nontrivial reasoning problems encountered
classification.
addition hypertableau calculus described Section 4, HermiT also implements
optimizations Section 6 well-known dependency directed backtracking
optimization (Horrocks, 2007). Thus, HermiT fully supports SHOIQ+ perform
satisfiability subsumption testing well knowledge base classification.
extensive discussion implementation techniques beyond scope paper;
comment briefly implementation anywhere blocking. DL community,
commonly understood anywhere blocking costly ancestor blocking
because, determine blocking status individual, one may need examine
individuals ABox individuals ancestors. implementation avoids
problem maintaining hash table individuals indexed four
blocking labels. table created scanning individuals increasing
sequence ordering . individual A, parent blocked,
indirectly blocked; otherwise, algorithm queries hash table individual
whose blocking labels equal s. hash table contains individual
t, directly blocked t; otherwise, blocked added
hash table. blocking status individuals thus determined
linear number hash table lookups.
used Pellet 2.0.0rc4 (Parsia & Sirin, 2004) FaCT++ 1.2.2 (Tsarkov & Horrocks,
2006) reference implementations SHOIQ tableau algorithm (Horrocks & Sattler,
2007). Pellet employs ancestor blocking, FaCT++ recently extended
anywhere blocking. time writing, however, implementation anywhere
blocking FaCT++ known incorrect,18 switched feature used
FaCT++ ancestor blocking well. measure effects ancestor vs. anywhere
blocking, also used HermiT-Anca version HermiT ancestor blocking.
used collection 392 test ontologies assembled three independent
sources.
18. Personal communication Dmitry Tsarkov.

219

fiMotik, Shearer, & Horrocks

Gardiner ontology suite (Gardiner, Horrocks, & Tsarkov, 2006) collection
OWL ontologies gathered Web includes many commonlyused OWL ontologies.
Open Biological Ontologies (OBO) Foundry19 collection biology life
science ontologies.
GALEN (Rector & Rogers, 2006) large complex biomedical ontology
proven notoriously difficult classify existing reasoners.
preprocessed ontologies resolve ontology imports eliminate trivial
syntactic errors. Thus, test ontology parsed single file using OWL
API. test ontologies available online.20
measured time needed classify test ontology using mentioned reasoners. tests performed 2.2 GHz MacBook Pro 2 GB physical memory.
classification attempt aborted exhausted available memory (Java tools
allowed use 1 GB heap space), exceeded timeout 30 minutes.
three reasoners exhibited negligible differences performance test
ontologies. Therefore, discuss next test results interesting ontologies
is, ontologies classified least one tested reasoners,
either trivial tested reasoners exhibited significant difference
performance. include several ontologies OBO corpus (Molecule Role, XP
Uber Anatomy, XP Plant Anatomy, Cellular Component, Gazetteer, CHEBI), two versions
National Cancer Institute (NCI) Thesaurus (Hartel et al., 2005), two versions
GALEN medical terminology ontology, two versions Foundational Model Anatomy
(FMA) (Golbreich et al., 2006), Wine ontology OWL Guide,21 two SWEET
ontologies developed NASA,22 version DOLCE ontology developed
Institute Cognitive Science Technology Italian National Research Council.23
Basic statistical information ontologies summarized Table 10.
noticed that, three reasoners, classification times may vary run run.
Pellet HermiT, due Javas collection library: order iteration
collections often depends objects hash codes, may vary run run;
that, turn, may change order derivation rules applied,
orders may better others. conjecture FaCT++ susceptible similar
variations. times may vary, noticed case ontology might
successfully classified one run, another. Therefore, Table 11 present
classification times interesting ontologies obtained one particular
run; times taken typical. identified four groups ontologies,
delineate Tables 10 11 horizontal lines.
19.
20.
21.
22.
23.

http://obofoundry.org/
http://hermit-reasoner.com/2009/JAIR_benchmarks/
http://www.w3.org/TR/owl-guide/
http://sweet.jpl.nasa.gov/ontology/
http://www.loa-cnr.it/DOLCE.html

220

fiHypertableau Reasoning Description Logics

Table 10: Statistics Interesting Ontologies

Ontology Name

Classes

Roles

Individuals

Molecule Role
XP Uber Anatomy
XP Plant Anatomy
XP Regulators
Cellular Component
NCI-1
Gazetteer
GALEN-doctored
GALEN-undoctored
CHEBI
FMA-Lite
SWEET Phenomena
SWEET Numerics
Wine
DOLCE-Plans
NCI-2
FMA-Constitutional

8849
11427
19145
25520
27889
27652
150979
2748
2748
20977
75141
1728
1506
138
118
70576
41648

2
82
82
4
4
70
2
413
413
9
2
145
177
17
264
189
168

128056
88955
86099
155169
163244
0
214804
0
0
243972
46225
171
113
206
27
0
85

Number Axioms
TBox RBox
ABox
9243
14669
35770
42896
47345
46800
167349
3937
4179
38375
119558
2419
2184
355
265
100304
122695

1
80
87
3
3
140
2
799
800
2
3
239
305
40
948
290
395

128056
88955
86099
155169
163244
0
214804
0
0
243972
46225
491
340
494
68
0
86

Expressivity
ALE+
ALEHIF +
SHIF
SH
SH
ALE
ALE+
ALEHIF +
ALEHIF +
ALE+
ALEI+
SHOIN (D)
SHOIN (D)
SHOIN (D)
SHOIN (D)
ALCH(D)
ALCOIF(D)

ontologies first group, HermiT performs similarly HermiT-Anc,
suggests little impact anywhere blocking performance. Consequently, believe
HermiT outperforms reasoners mainly due reduced nondeterminism
hypertableau calculus. shown Table 10, Molecule Role, XP Uber Anatomy,
NCI-1 use disjunctions, HermiT classifies deterministically using linear
number calls hypertableau algorithm. FaCT++ outperforms HermiT NCI-1
ontology classified using completely defined concepts optimization
(Tsarkov & Horrocks, 2005a), FaCT++ implements HermiT not.
optimization enables FaCT++ use simpler structural reasoning techniques ontologies
satisfy certain syntactic constraints.
ontologies second group, HermiT-Anc significantly slower HermiT.
suggests anywhere blocking significantly improves performance since prevents construction large models. Pellet runs memory ontologies
group; furthermore, FaCT++ cannot process two significantly slower
HermiT CHEBI. FaCT++, however, faster HermiT-Anc CHEBI GALENdoctored, conjecture mainly due ordering heuristics (Tsarkov &
Horrocks, 2005b) used FaCT++. superior performance HermiT ontologies
group mainly due fact ontologies classified deterministically using linear number concept satisfiability tests. Furthermore, HermiTs
classification time cases dominated first test, caching
blocking labels described Section 6.2 makes subsequent tests easy.
ontologies third group, HermiT significantly slower
reasoners. Table 10 shows, ontologies group contain nominals, prevents
HermiT caching blocking labels. Furthermore, due nominals, ABox must
221

fiMotik, Shearer, & Horrocks

Table 11: Results Performance Evaluation
Classification Times (seconds)
HermiT
HermiT-Anc
Pellet
FaCT++
Molecule Role
3.3
3.4
25.7
304.5
XP Uber Anatomy
5.4
4.9

86.0
XP Plant Anatomy
12.8
11.2
87.2
22.9
XP Regulators
14.1
17.1
35.4
66.6
Celular Component
18.6
18.0
40.5
76.7
NCI-1
14.1
14.4
23.2
3.0
Gazetteer
131.9
132.3


GALEN-doctored
8.8
456.3

15.9
GALEN-undoctored
126.3



CHEBI
24.2


397.0
FMA-Lite
107.2



SWEET Phenomena
13.5
11.2

0.2
SWEET Numerics
76.7
72.6
3.7
0.2
Wine
343.7
524.6
19.5
162.1
DOLCE-Plans
1075.1

105.1

NCI-2


172.0
60.7
FMA-Constitutional



616.7
Note: entry means reasoner unable classify ontology either
due time memory exhaustion.
Ontology Name

taken account classification, HermiT currently reapplies hypertableau
rules entire ABox run. Effectively, HermiT reuse computation
different hypertableau runs. two reasoners, however, use completion
graph caching optimization (Sirin, Cuenca Grau, & Parsia, 2006), tableau rules
first applied entire ABox, resulting completion graph used starting
point subsequent run.
HermiT unable classify two ontologies fourth group. NCI-2
FMA-Constitutional use disjunctions, cannot classified using linear number
concept satisfiability tests; instead, HermiT uses classification algorithm Baader
et al. (1994). classification tests straightforward (each test takes less 50 ms);
however, resulting taxonomy rather shallow, HermiT makes almost quadratic
number tests. Pellet FaCT++, however, use optimized versions
classification algorithm reduce number tests need performed.
summarize, although HermiT better Pellet FaCT++ ontologies,
results clearly demonstrate practical potential reduced nondeterminism anywhere blocking. fact, anywhere blocking mean difference
success failure complex ontologies, suggests and-branching significant source inefficiency practice or-branching. Anywhere blocking applicable

222

fiHypertableau Reasoning Description Logics

tableau calculi well (as mentioned earlier, FaCT++ already contains preliminary
version it), believe results used improve performance
tableau reasoners well without need major redesign. Conversely,
optimizations used tableau reasoners used hypertableau algorithm,
incorporating HermiT would make HermiT competitive Pellet FaCT++
cases HermiT currently slower.

8. Conclusion
paper presented novel reasoning algorithm DLs. algorithm based
hyperresolution anywhere blocking, reduces nondeterminism due GCIs
sizes generated models. Furthermore, algorithm uses novel refinement
NI -rule reduce amount nondeterminism introduced order handle
interaction nominals, inverse roles, number restrictions (Horrocks & Sattler,
2007). refined version NI -rule equally applicable tableau algorithms.
implemented calculus conducted extensive performance comparison. results show combination new calculus novel optimizations
significantly increases performance DL reasoning practice: reasoner currently
one classify several complex ontologies.
Despite advance performance, still ontologies, full
version GALEN,24 defeat HermiT (and state-of-the-art tableau reasoners).
large number cyclic axioms ontologies cause HermiT construct
extremely large ABoxes eventually exhaust available memory. alleviate
problem, developed reasoning technique -rule modified nondeterministically reuse individuals ABox generated thus far. Initial experiments
technique shown promising results (Motik & Horrocks, 2008).
Finally, plan extend technique DL SROIQ, extends SHOIQ
expressive role inclusion axioms allow us express, example,
brother persons father also persons uncle. logic considerable interest
underpins OWL 2the extension OWL currently standardized W3C.

Acknowledgments
extended version paper published CADE 2007 (Motik et al., 2007).
thank anonymous reviewer numerous comments contributed quality
paper.

References
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL Envelope. Kaelbling, L. P., &
Saffiotti, A. (Eds.), Proc. 19th Int. Joint Conference Artificial Intelligence
(IJCAI 2005), pp. 364369, Edinburgh, UK. Morgan Kaufmann Publishers.
Baader, F., Buchheit, M., & Hollunder, B. (1996). Cardinality Restrictions Concepts.
Artificial Intelligence, 88 (12), 195213.
24. http://www.co-ode.org/galen/

223

fiMotik, Shearer, & Horrocks

Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2007). Description Logic Handbook: Theory, Implementation Applications
(2nd edition). Cambridge University Press.
Baader, F., Hollunder, B., Nebel, B., Profitlich, H.-J., & Franconi, E. (1994). Empirical
Analysis Optimization Techniques Terminological Representation systems or:
Making KRIS Get Move on. Applied Intelligence, 4 (2), 109132.
Baader, F., & Nutt, W. (2007). Basic Description Logics. Baader, F., Calvanese, D.,
McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), Description Logic
Handbook: Theory, Implementation Applications (2nd edition)., pp. 47104. Cambridge University Press.
Baader, F., & Sattler, U. (2001). Overview Tableau Algorithms Description Logics.
Studia Logica, 69, 540.
Balsiger, P., & Heuerding, A. (1998). Comparison Theorem Provers Modal Logics
Introduction Summary. de Swart, H. (Ed.), Proc. 2nd Int. Conf.
Analytic Tableaux Related Methods (TABLEAUX98), Vol. 1397 LNAI, pp.
2526. Springer.
Balsiger, P., Heuerding, A., & Schwendimann, S. (2000). Benchmark Method
Propositional Modal Logics K, KT, S4. Journal Automated Reasoning, 24 (3), 297
317.
Baumgartner, P., Furbach, U., & Niemela, I. (1996). Hyper Tableaux. Proc.
European Workshop Logics Artificial Intelligence (JELIA 96), No. 1126
LNAI, pp. 117, Evora, Portugal. Springer.
Baumgartner, P., Furbach, U., & Pelzer, B. (2008). Hyper Tableaux Calculus
Equality Application Finite Model Computation. Journal Logic
Computation.
Baumgartner, P., & Schmidt, R. A. (2006). Blocking Enhancements BottomUp Model Generation Methods. Furbach, U., & Shankar, N. (Eds.), Proc.
3rd Int. Joint Conf. Automated Reasoning (IJCAR 2006), Vol. 4130 LNCS, pp.
125139, Seattle, WA, USA. Springer.
Borgida, A. (1996). Relative Expressiveness Description Logics Predicate
Logics. Artificial Intelligence, 82 (12), 353367.
Bry, F., & Torge, S. (1998). Deduction Method Complete Refutation Finite
Satisfiability. Dix, J., del Cerro, L. F., & Furbach, U. (Eds.), Proc. European
Workshop Logics Artificial Intelligence (JELIA 98), Vol. 1489 LNCS, pp.
122138, Dagstuhl, Germany. Springer.
Buchheit, M., Donini, F. M., & Schaerf, A. (1993). Decidable Reasoning Terminological
Knowledge Representation Systems. Journal Artificial Intelligence Research, 1,
109138.
Demri, S., & de Nivelle, H. (2005). Deciding Regular Grammar Logics Converse
First-Order Logic. Journal Logic, Language Information, 14 (3), 289
329.
224

fiHypertableau Reasoning Description Logics

Ding, Y., & Haarslev, V. (2006). Tableau Caching Description Logics Inverse
Transitive Roles. Parsia, B., Sattler, U., & Toman, D. (Eds.), Proc. 2006 Int.
Workshop Description Logics (DL 2006), Vol. 189 CEUR Workshop Proceedings,
Windermere, UK.
Donini, F. M. (2007). Complexity Reasoning. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), Description Logic Handbook:
Theory, Implementation Applications (2nd edition)., pp. 105148. Cambridge
University Press.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1998). AL-log: Integrating Datalog
Description Logics. Journal Intelligent Information Systems, 10 (3), 227252.
Donini, F. M., & Massacci, F. (2000). EXPTIME tableaux ALC. Artificial Intelligence,
124 (1), 87138.
Faddoul, J., Farsinia, N., Haarslev, V., & Moller, R. (2008). Hybrid Tableau Algorithm
ALCQ. Ghallab, M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.),
Proc. 18th European Conf. Artificial Intelligence (ECAI 2008), Vol. 178
Frontiers Artificial Intelligence Applications, pp. 725726, Patras, Greece. IOS
Press.
Fermuller, C., Tammet, T., Zamov, N., & Leitsch, A. (1993). Resolution Methods
Decision Problem, Vol. 679 LNAI. Springer.
Fermuller, C. G., Leitsch, A., Hustadt, U., & Tammet, T. (2001). Resolution Decision Procedures. Robinson, A., & Voronkov, A. (Eds.), Handbook Automated Reasoning,
Vol. II, chap. 25, pp. 17911849. Elsevier Science.
Gardiner, T., Horrocks, I., & Tsarkov, D. (2006). Automated Benchmarking Description
Logic Reasoners. Proc. 2006 Description Logic Workshop (DL 2006), Vol.
189 CEUR Workshop Proceedings.
Georgieva, L., Hustadt, U., & Schmidt, R. A. (2003). Hyperresolution Guarded Formulae. Journal Symbolic Computation, 36 (12), 163192.
Golbreich, C., Zhang, S., & Bodenreider, O. (2006). Foundational Model Anatomy
OWL: Experience Perspectives. Journal Web Semantics, 4 (3), 181195.
Gore, R., & Nguyen, L. A. (2007). EXPTIME Tableaux Global Caching Description
Logics Transitive Roles, Inverse Roles Role Hierarchies. Proc. 16th
Int. Conf. Automated Reasoning Tableaux Related Methods (TABLEAUX
2007), Vol. 4548 LNCS, pp. 133148, Aix en Provence, France. Springer.
Haarslev, V., & Moller, R. (2001). RACER System Description. Gore, R., Leitsch, A., &
Nipkow, T. (Eds.), Proc. 1st Int. Joint Conf. Automated Reasoning (IJCAR
2001), Vol. 2083 LNAI, pp. 701706, Siena, Italy. Springer.
Haarslev, V., Moller, R., & Wandelt, S. (2008). Revival Structural Subsumption
Tableau-Based Description Logic Reasoners. Baader, F., Lutz, C., & Motik, B.
(Eds.), Proc. 21st Int. Workshop Description Logics (DL 2008), Vol. 353
CEUR Workshop Proceedings, Dresden, Germany.

225

fiMotik, Shearer, & Horrocks

Hartel, F. W., de Coronado, S., Dionne, R., Fragoso, G., & Golbeck, J. (2005). Modeling
description logic vocabulary cancer research. Journal Biomedical Informatics,
38 (2), 114129.
Horrocks, I. (1998). Using Expressive Description Logic: FaCT Fiction?. Cohn,
A. G., Schubert, L., & Shapiro, S. C. (Eds.), Proc. 6th Int. Conf. Principles Knowledge Representation Reasoning (KR 98), pp. 636647, Trento,
Italy. Morgan Kaufmann Publishers.
Horrocks, I. (2007). Implementation Optimization Techniques. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), Description Logic Handbook: Theory, Implementation Applications (2nd edition)., pp.
329373. Cambridge University Press.
Horrocks, I., & Patel-Schneider, P. F. (1998a). Comparing Subsumption Optimizations.
Franconi, E., De Giacomo, G., MacGregor, R. M., Nutt, W., & Welty, C. A. (Eds.),
Proc. 1998 Description Logic Workshop (DL98), Vol. 11 CEUR Workshop
Proceedings, pp. 9094, PovoTrento, Italy.
Horrocks, I., & Patel-Schneider, P. F. (1998b). DL Systems Comparison. Franconi,
E., De Giacomo, G., MacGregor, R. M., Nutt, W., & Welty, C. A. (Eds.), Proc.
1998 Int. Workshop Description Logic (DL98), Vol. 11 CEUR Workshop
Proceedings, pp. 5557, PovoTrento, Italy.
Horrocks, I., & Sattler, U. (2001). Ontology Reasoning SHOQ(D) Description Logic.
Nebel, B. (Ed.), Proc. 7th Int. Joint Conf. Artificial Intelligence (IJCAI
2001), pp. 199204, Seattle, WA, USA. Morgan Kaufmann Publishers.
Horrocks, I., & Sattler, U. (2007). Tableau Decision Procedure SHOIQ. Journal
Automated Reasoning, 39 (3), 249276.
Horrocks, I., Sattler, U., & Tobies, S. (2000a). Practical Reasoning Expressive
Description Logics. Logic Journal IGPL, 8 (3), 239263.
Horrocks, I., Sattler, U., & Tobies, S. (2000b). Reasoning Individuals Description Logic SHIQ. MacAllester, D. (Ed.), Proc. 17th Int. Conf. Automated
Deduction (CADE-17), Vol. 1831 LNAI, pp. 482496, Pittsburgh, USA. Springer.
Hudek, A. K., & Weddell, G. (2006). Binary Absorption Tableaux-Based Reasoning
Description Logics. Parsia, B., Sattler, U., & Toman, D. (Eds.), Proc.
2006 Int. Workshop Description Logics (DL 2006), Vol. 189 CEUR Workshop
Proceedings, Windermere, UK.
Hustadt, U., Motik, B., & Sattler, U. (2005). Data Complexity Reasoning Expressive Description Logics. Proc. 19th Int. Joint Conf. Artificial Intelligence
(IJCAI 2005), pp. 466471, Edinburgh, UK. Morgan Kaufmann Publishers.
Hustadt, U., & Schmidt, R. A. (1999). Issues Decidability Description Logics
Framework Resolution. Caferra, R., & Salzer, G. (Eds.), Selected Papers
Automated Deduction Classical Non-Classical Logics, Vol. 1761 LNAI, pp.
191205. Springer.
Kutz, O., Horrocks, I., & Sattler, U. (2006). Even Irresistible SROIQ.
Doherty, P., Mylopoulos, J., & Welty, C. A. (Eds.), Proc. 10th Int. Conf.
226

fiHypertableau Reasoning Description Logics

Principles Knowledge Representation Reasoning (KR 2006), pp. 6878, Lake
District, UK. AAAI Press.
Motik, B. (2006). Reasoning Description Logics using Resolution Deductive
Databases. Ph.D. thesis, Univesitat Karlsruhe, Germany.
Motik, B., & Horrocks, I. (2008). Individual Reuse Description Logic Reasoning.
Proc. 4th Int. Joint Conf. Automated Reasoning (IJCAR 2008), Sydney,
Australia. Springer. appear.
Motik, B., Shearer, R., & Horrocks, I. (2007). Optimized Reasoning Description Logics
using Hypertableaux. Pfenning, F. (Ed.), Proc. 21st Conference Automated Deduction (CADE-21), Vol. 4603 LNAI, pp. 6783, Bremen, Germany.
Springer.
Nonnengart, A., & Weidenbach, C. (2001). Computing Small Clause Normal Forms.
Robinson, A., & Voronkov, A. (Eds.), Handbook Automated Reasoning, Vol. I,
chap. 6, pp. 335367. Elsevier Science.
Parsia, B., & Sirin, E. (2004). Pellet: OWL-DL Reasoner. Poster 3rd Int. Semantic
Web Conference (ISWC 2004).
Patel-Schneider, P. F., Hayes, P., & Horrocks, I. (2004). OWL Web Ontology Language:
Semantics Abstract Syntax, W3C Recommendation..
http://www.w3.org/TR/owl-semantics/.
Plaisted, D. A., & Greenbaum, S. (1986). Structure-Preserving Clause Form Translation.
Journal Symbolic Logic Computation, 2 (3), 293304.
Rector, A. L., & Rogers, J. (2006). Ontological Practical Issues Using Description
Logic Represent Medical Concept Systems: Experience GALEN. Barahona,
P., Bry, F., Franconi, E., Henze, N., & Sattler, U. (Eds.), Tutorial Lectures 2nd
Int. Summer School 2006, Vol. 4126 LNCS, pp. 197231, Lisbon, Portugal. Springer.
Robinson, A. (1965). Automatic Deduction Hyper-Resolution. Int. Journal Computer Mathematics, 1, 227234.
Schmidt, R. A., & Hustadt, U. (2003). Principle Incorporating Axioms FirstOrder Translation Modal Formulae. Baader, F. (Ed.), Proc. 19th Int.
Conf. Automated Deduction (CADE-19), Vol. 2741 LNAI, pp. 412426, Miami
Beach, FL, USA. Springer.
Sirin, E., Cuenca Grau, B., & Parsia, B. (2006). Wine Water: Optimizing Description Logic Reasoning Nominals. Doherty, P., Mylopoulos, J., & Welty, C. A.
(Eds.), Proc. 10th Int. Conf. Principles Knowledge Representation
Reasoning (KR 2006), pp. 9099, Lake District, UK. AAAI Press.
Tobies, S. (2000). Complexity Reasoning Cardinality Restrictions Nominals
Expressive Description Logics. Journal Artificial Intelligence Research, 12, 199
217.
Tobies, S. (2001). Complexity Results Practical Algorithms Logics Knowledge
Representation. Ph.D. thesis, RWTH Aachen, Germany.

227

fiMotik, Shearer, & Horrocks

Tsarkov, D., & Horrocks, I. (2004). Efficient Reasoning Range Domain Constraints.
Haarslev, V., & Moller, R. (Eds.), Proc. 2004 Int. Workshop Description
Logics (DL 2004), Vol. 104 CEUR Workshop Proceedings, Whistler, BC, Canada.
Tsarkov, D., & Horrocks, I. (2005a). Optimised Classification Taxonomic Knowledge
Bases. Horrocks, I., Sattler, U., & Wolter, F. (Eds.), Proc. 2005 Int. Workshop
Description Logics (DL 2005), Edinburgh, Scotland, UK.
Tsarkov, D., & Horrocks, I. (2005b). Ordering Heuristics Description Logic Reasoning.
Kaelbling, L. P., & Saffiotti, A. (Eds.), Proc. 19th Int. Joint Conf. Artificial Intelligence (IJCAI 2005), pp. 609614, Edinburgh, UK. Morgan Kaufmann
Publishers.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ Description Logic Reasoner: System Description. Proc. 3rd Int. Joint Conf. Automated Reasoning (IJCAR 2006),
Vol. 4130 LNAI, pp. 292297, Seattle, WA, USA. Springer.
Wu, J., & Haarslev, V. (2008). Planning Axiom Absorption. Baader, F., Lutz, C., &
Motik, B. (Eds.), Proc. 21st Int. Workshop Description Logics (DL 2008),
Vol. 353 CEUR Workshop Proceedings, Dresden, Germany.

228

fiJournal Artificial Intelligence Research 36 (2009) 71

Submitted 12/08; published 10/09

Prime Implicates Prime Implicants:
Propositional Modal Logic
Meghyn Bienvenu

meghyn@informatik.uni-bremen.de

Department Mathematics Computer Science
University Bremen, Germany

Abstract
Prime implicates prime implicants proven relevant number areas
artificial intelligence, notably abductive reasoning knowledge compilation.
purpose paper examine notions might appropriately extended
propositional logic modal logic K. begin paper considering number
potential definitions clauses terms K. different definitions evaluated
respect set syntactic, semantic, complexity-theoretic properties characteristic
propositional definition. compare definitions respect properties
notions prime implicates prime implicants induce.
definition perfectly generalizes propositional notions, show
exist one definition satisfies many desirable properties propositional
case. second half paper, consider computational properties
selected definition. end, provide sound complete algorithms generating
recognizing prime implicates, show prime implicate recognition task
Pspace-complete. also prove upper lower bounds size number prime
implicates. paper focuses logic K, results hold equally well
multi-modal K concept expressions description logic ALC.

1. Introduction
Prime implicates prime implicants important notions artificial intelligence.
given rise significant body work automated reasoning applied
number different sub-areas AI. Traditionally, concepts studied
context propositional logic, also considered many-valued
(Ramesh & Murray, 1994) first-order logic (Marquis, 1991a, 1991b). much
known, however, prime implicates prime implicants logics. particular,
definition prime implicate prime implicant ever proposed modal
description logic, shown reasonable definition provided.
Given increasing interest modal description logics knowledge representation
languages, one naturally wonders whether notions suitably generalized
expressive logics.
recall propositional logic prime implicates formula defined
logically strongest clausal consequences. restriction clauses made order
reduce redundant elements formulas set consequences: use keeping
around consequence b one already consequences b. decision
consider logically strongest clausal consequences motivated desire
eliminate irrelevant weaker consequences: already consequence a,
c
2009
AI Access Foundation. rights reserved.

fiBienvenu

point retaining consequences b b. Prime implicates thus provide
complete yet compact representation set logical consequences formula.
particularly nice representation makes many computational tasks
simpler: satisfiability, tautology, entailment, equivalence queries conditioning
forgetting transformations tractable formulae represented prime
implicates (Darwiche & Marquis, 2002). prime implicates considered
interesting target language knowledge compilation (Cadoli & Donini, 1997; Darwiche
& Marquis, 2002). Prime implicates also proved relevant sub-areas AI,
like distributed reasoning (Adjiman, Chatalic, Goasdoue, Rousset, & Simon, 2006), belief
revision (Bittencourt, 2007; Pagnucco, 2006), non-monotonic reasoning (cf. Przymusinski,
1989), characterizations relevance (Lakemeyer, 1995; Lang, Liberatore, & Marquis,
2003).
dual notion prime implicates prime implicants, defined
logically weakest terms (= conjunctions literals) imply given formula. main
application domain prime implicants abduction diagnosis. recall
abduction, one given background theory observation, objective
find explanation observation. logical terms, explanation formula
logically entails observation taken together background theory.
set explanations abduction problem large, important question
select representative subset explanations. One common approach
use prime implicants: relevant explanations observation respect
background theory taken prime implicants (de Kleer, Mackworth,
& Reiter, 1992; Eiter & Makino, 2002).
many applications AI, expressive power propositional logic proves insufficient. First-order logic provides much greater level expressivity, price
undecidability. Modal description logics offer interesting trade-off expressivity complexity, generally expressive propositional logic yet
better-behaved computationally first-order logic. explains growing trend
towards using languages knowledge representation.
prototypical description logic ALC, extends propositional logic restricted
forms universal existential quantification. example expression ALC
F emale hasChild.F emale hasChild.(Doctor P rof essor) hasP et.Dog
describes set individuals female, least one daughter one
pet dog, children either doctors professors.
concept expression represented equally well modal logic K2 formula:
F emale 31 F emale 21 (Doctor P rof essor) 32 Dog
Schild (1991) proved general result showed description logic ALC n
binary relations fact notational variant multi-modal logic Kn . means
results concerning Kn transferred ALC, vice-versa.
paper, investigate notions prime implicates prime implicants
modal logic K = K1 , actually results hold formulae Kn , hence
also concept expressions ALC. decision present results terms K
72

fiPrime Implicates Prime Implicants Modal Logic

rather terms Kn ALC motivated solely desire simplify notation
increase readability proofs.
question notions prime implicates prime implicants suitably
defined logic K clearly interest theoretical point view. argue,
however, question also practically relevant. support claim, briefly
discuss two application areas study prime implicates prime implicants
K might prove useful.
first domain application consider abductive reasoning K. noted
above, one key foundational issues abductive reasoning selection
interesting subset explanations. issue especially crucial logics like K
allow infinite number non-equivalent formulae, since means number
non-equivalent explanations abduction problem large fact infinite,
making simply impossible enumerate entire set explanations. prime implicants
widely-accepted means characterizing relevant explanations propositional logic,
reasonable starting point research abductive reasoning logic K study
different possible definitions prime implicant K properties.
investigation prime implicates K also relevant development knowledge compilation procedures K. recall knowledge compilation (cf. Darwiche
& Marquis, 2002) general technique coping intractability reasoning
consists off-line phase knowledge base rewritten equivalent
knowledge base allows tractable reasoning, followed online phase
reasoning performed compiled knowledge base. idea initial cost
compiling knowledge base offset computational savings later queries.
Currently, work knowledge compilation restricted propositional logic, even
though technique could prove highly relevant modal description logics,
generally suffer even higher computational complexity propositional logic.
prime implicates one better-known mechanisms compiling formulae propositional logic, certainly makes sense investigate whether approach knowledge
compilation fruitfully extended logics like K.
paper organized follows. preliminaries, consider appropriately generalize notions clauses terms K. obvious definition,
enumerate list syntactic, semantic, complexity-theoretic properties propositional definitions, use compare different candidate definitions.
next consider different definitions light notions prime implicate prime
implicant induce. again, list basic properties propositional
case would like satisfy, see different definitions measure up.
second half paper, investigate computational properties
satisfactory definition prime implicates. consider problems prime implicate
generation recognition, provide sound complete algorithms tasks.
also study complexity prime implicate recognition problem, showing
Pspace-complete thus complexity satisfiability deduction K.
conclude paper discussion relevance results two application
areas cited directions future research. order enhance readability paper, proofs omitted body text. Full proofs
found appendix.
73

fiBienvenu

2. Preliminary Definitions Notation
briefly recall basics modal logic K (refer Blackburn, de Rijke, & Venema,
2001; Blackburn, van Benthem, & Wolter, 2006, good introductions modal logic).
Formulae K built set propositional variables V, standard logical
connectives (, , ), modal operators 2 3. call formula
form 2 (resp. 3) 2-formula (resp. 3-formula). convenient use
abbreviation . adopt shorthand 2k (resp. 3k ) refer
formula consisting preceded k copies 2 (resp. 3), convention
20 = 30 = . use var() refer set propositional variables appearing
formula . modal depth formula , written (), defined maximal
number nested modal operators appearing , e.g. (3(a 2a) a) = 2. define
length formula , written ||, number occurrences propositional variables,
logical connectives, modal operators . example, would |(a b)| = 4
|3(a b) 2a| = 8.
Negation normal form (NNF) defined propositional logic: formula said
NNF negation appears directly propositional variables. Every formula
K transformed equivalent formula NNF using recursive procedure
Nnf defined follows:
Nnf (l)=l (for propositional literals l)
Nnf (1 2 )=Nnf (1 )Nnf (2 )
Nnf ((1 2 ))=Nnf (1 )Nnf (2 )
Nnf (1 2 )=Nnf (1 )Nnf (2 )
Nnf ((1 2 ))=Nnf (1 )Nnf (2 )

Nnf (2)=2Nnf ()
Nnf (2)=3Nnf ()
Nnf (3)=3Nnf ()
Nnf (3)=2Nnf ()
Nnf ()=Nnf ()

example, applying Nnf formula 2(a 3(b c)) results formula 3(a
2(b c)) NNF. transformation Nnf takes linear time, yields formula
double size original formula modal depth
propositional variables original.
model K tuple = hW, R, vi, W non-empty set possible worlds,
R W W binary relation worlds, v : W V {true, f alse} valuation
propositional variables world. Models understood labelled directed
graphs, vertices correspond elements W, directed edges represent
binary relation R, vertices labeled propositional valuations specify
propositional variables true corresponding possible world.
Satisfaction formula model world w (written M, w |= ) defined
inductively follows:
M, w |= v(w, a) = true
M, w |= M, w 6|=
M, w |= M, w |= M, w |=
M, w |= M, w |= M, w |=
M, w |= 2 M, w |= w wRw
74

fiPrime Implicates Prime Implicants Modal Logic

M, w |= 3 M, w |= w wRw
think models labeled directed graphs, determining satisfaction
formula 2 vertex w consists evaluating vertices reached
w via edge; 2 satisfied w case holds
successor vertices. Similarly, order decide whether formula 3 holds vertex w,
consider successors w graph check whether least one
vertices satisfies .
formula said tautology, written |= , M, w |= every model
world w. formula satisfiable model world w
M, w |= . w M, w |= , called unsatisfiable,
write |= .
Ladner (1977) showed satisfiability unsatisfiability K Pspace-complete.
Pspace membership, Ladner exhibited polynomial space tableaux-style algorithm
deciding satisfiability K formulae. Pspace-hardness proven means reduction
QBF validity (the canonical Pspace-complete problem).
modal logic, notion logical consequence (or entailment) defined one
two ways:
formula global consequence whenever M, w |= every world w
model M, M, w |= every world w
formula local consequence M, w |= implies M, w |= every model
world w
paper, consider notion local consequence, take |=
mean local consequence . |= , say entails .
Two formulae called equivalent, written , |= |= .
formula said logically stronger |= 6|= .
highlight basic properties logical consequence equivalence K
play important role proofs results.
Theorem 1. Let , 1 , ..., , , 1 , ..., n formulae K, let propositional
formula.
1. |= |= |=
2. |= 3 |= 3 2 |= 2
3. 31 ...3m 21 ...2n |= ( |= 1 ...n |= i)
4. |= 31 ... 3m 21 ... 2n (|= |= 1 ... i)
5. 2 |= 21 ... 2n |=
6. 31 ... 3m 21 ... 2n
31 ... 3m 2(1 1 ... ) ... 2(n 1 ... )
75

fiBienvenu

Statement 1 Theorem 1 shows three reasoning tasks entailment, unsatisfiability, tautology-testing rephrased terms one another. Statement 2 tells us
entailment two 2- 3-formulae reduced entailment
formulae first modality removed. Statements 3 4 define conditions
conjunction (resp. disjunction) propositional literals 2- 3-formulae
unsatisfiable (resp. tautology). Statement 5 gives us conditions
2-formula implies disjunction 2-formulae. Statement 6 demonstrates interaction
2- 3-formulae disjunction.
Theorem 2. Let disjunction propositional literals 2- 3-formulae.
following statements holds:
1. |= non-tautological propositional clause , every disjunct
either propositional literal unsatisfiable 3-formula
2. |= 31 ... 3n , every disjunct 3-formula
3. |= 21 ... 2n 6|= 21 ... 2n , every disjunct either
2-formula unsatisfiable 3-formula
Theorem 3. Let = 31 ... 3m 21 ... 2n = 31 ... 3p
21 ... 2q formulae K. propositional 6|= ,

|=
1 ... |= 1 ... p
|=

every j |= 1 ... p j
Theorems 2 3 concern entailment relations formulae disjunctions
propositional literals 2- 3-formulae. Theorem 2 tells us kinds formulae
type entail propositional clause, disjunction 3-formulae, disjunction
2-formulae, Theorem 3 outlines conditions two formulae
type related entailment relation. illustrate Theorem 3
small example.
Example 4. Consider formula = b 3(a 3c) 3(d 2a) 2(c d).
according Theorem 3, have:
|= b 3(a d) 2c, since b |= b (a 3c) (d 2a) |=
c |= c (a d)
6|= 3c, since b 6|=
6|= b 3(a c), since (a 3c) (d 2a) 6|= c
6|= b 3(a 2a) 2c, since c 6|= c (a 2a)
76

fiPrime Implicates Prime Implicants Modal Logic

3. Literals, Clauses, Terms K
seen introduction, notions prime implicates implicants
straightforwardly defined using notions clauses terms. Thus, aim provide
suitable definitions prime implicates implicants logic K, first need decide
upon suitable definition clauses terms K. Unfortunately, whereas clauses
terms standard notions propositional first-order logic1 , generally
accepted definition clauses terms K. Indeed, several quite different notions
clauses terms proposed literature different purposes.
Instead blindly picking definition hoping appropriate, prefer
list number characteristics literals, clauses, terms propositional logic, giving
us principled means comparing different candidate definitions. properties
describes something literal, clause, term propositional logic.
Although list cannot considered exhaustive, believe covers principal
syntactic, semantic, complexity-theoretic properties propositional definition.
P1 Literals, clauses, terms negation normal form.
P2 Clauses contain , terms contain , literals contain neither .
P3 Clauses (resp. terms) disjunctions (resp. conjunctions) literals.
P4 negation literal equivalent another literal. Negations clauses (resp.
terms) equivalent terms (resp. clauses).
P5 Every formula equivalent finite conjunction clauses. Likewise, every formula
equivalent finite disjunction terms.
P6 task deciding whether given formula literal, term, clause accomplished polynomial-time.
P7 task deciding whether clause (resp. term) entails another clause (resp. term)
accomplished polynomial-time.
One may wonder whether exist definitions literals, clauses, terms K
satisfying properties. Unfortunately, show impossible.
Theorem 5. definition literals, clause, terms K satisfies properties P1
P2 cannot satisfy P5.
proof Theorem 5 makes use fact distribute 3
distribute 2, means impossibility result holds equally
well standard modal description logics.
consider variety possible definitions evaluate respect
criteria. first definition consider proposed Cialdea
1. One might wonder simply translate formulae K first-order formulae
put clausal form. reason simple: looking define clauses terms within
language K, clauses obtain passing first-order logic generally expressible K.
Moreover, define clauses K first-order clauses representable K,
would obtain set clauses containing 3 modalities, thereby losing much expressivity K.

77

fiBienvenu

Mayer Pirri (1995) paper abductive reasoning modal logic. define terms
formulae constructed propositional literals using , 2,
3. Modal clauses literals used paper defined analogously,
yielding following definition2 :
D1

L ::= | | 2L | 3L
C ::= | | 2C | 3C | C C
::= | | 2T | 3T |

easy see inspection definition satisfies properties P1-P2, P4, P6.
Property P3 satisfied, however, since clauses disjunctions
literals take instance 2(a b). Theorem 5 fact P1 P2
satisfied, conclude property P5 cannot hold. first glance, may seem
entailment clauses terms could accomplished polynomial time,
case. fact, show problem NP-complete. proof relies
strong resemblance terms D1 concept expressions description
logic ALE (for unsatisfiability deduction known NP-complete).
using slightly different definition, gain P3:
D2

L ::= | | 2L | 3L
C ::= L | C C
::= L |

easily verified definition D2 satisfies properties P1-P4 P6. definition
D1 satisfy P5, definition D2 even less expressive, follows D2
satisfy P5 either. reduced expressiveness however improve computational
complexity: property P7 still satisfied show entailment
clauses terms NP-complete using reduction used definition D1.
fact even extremely inexpressive definition like D2 allow polynomial
entailment clauses terms suggests property P7 cannot satisfied
reasonable definition clauses terms K.
Let us consider expressive options. begin following definition
clauses proposed Enjalbert Farinas del Cerro (1989) purpose
modal resolution:
D3
C ::= | | 2C | 3ConjC | C C
ConjC ::= C | ConjC ConjC
definition clauses extended definition terms literals satisfies
P3 P4, extension satisfies properties. Let us first consider
one possible extensions satisfies P4 maximal subset P1-P7:
D3a

L ::= | | 2L | 3L
C ::= | | 2C | 3ConjC | C C
ConjC ::= C | ConjC ConjC
::= | | 2DisjT | 3T |
DisjT ::= | DisjT DisjT

2. Note follows, let range propositional variables L, C, range
sets literals, clauses, terms, respectively.

78

fiPrime Implicates Prime Implicants Modal Logic

definition satisfies P1 P4-P6 (satisfaction P5 shown Enjalbert & Farinas
del Cerro, 1989). satisfy P3 clauses disjunctions
literals take example 2(a b). Given definition D3a strictly expressive
definitions D1 D2, follows entailment clauses terms must
NP-hard, means D3a satisfy P7. fact, show entailment
clauses terms definition D3a Pspace-complete. so, modify
polynomial translation QBF K used prove Pspace-hardness K
translated formula conjunction clauses respect D3a. notice
formula unsatisfiable 3 entails 3(a a). thus reduce QBF
validity entailment clauses, making task Pspace-hard, hence (being
subproblem entailment K) Pspace-complete. idea used show Pspacecompleteness definitions D3b D5 below.
instead extend D3 enforce property P3, obtain following definition:
L ::= | | 2C | 3ConjC
C ::= | | 2C | 3ConjC | C C
ConjC ::= C | ConjC ConjC
::= L |
definition satisfies properties except P2, P4, P7. Property P4 fails
hold negation literal 3(a b) equivalent literal. proof
P5 holds constructive: use standard logical equivalences rewrite formulae
equivalent conjunctions clauses disjunctions terms (this also
definitions D4 D5 below).
consider two rather simple definitions satisfy properties P3, P4, P5.
first definition, inspired notion modal atom proposed Giunchiglia
Sebastiani (1996), defines literals set formulae NNF cannot decomposed propositionally.
D3b

L ::= | | 2F | 3F
C ::= L | C C
::= L |
F ::= | | F F | F F | 2F | 3F
D4 satisfies properties except P2 P7. P7, note arbitrary formula NNF unsatisfiable (a Pspace-complete problem) 3 |= 3(a a).
Definition D4 liberal, imposing structure formulae behind modal operators. define literals formulae NNF cannot decomposed modally
(instead propositionally), obtain much stricter definition satisfies exactly
properties D4.
D4

D5

L ::= | | 2C | 3T
C ::= L | C C
::= L |

summary analysis different definitions respect properties P1-P7
provided following table.
Theorem 6. results Figure 1 hold.
79

fiBienvenu

P1
P2
P3
P4
P5
P6
P7

D1
D2
yes
yes
yes
yes

yes
yes
yes


yes
yes
(unless P=NP)

D3a D3b D4 D5
yes
yes
yes yes





yes
yes yes
yes

yes yes
yes
yes
yes yes
yes
yes
yes yes
(unless P=Pspace)

Figure 1: Properties different definitions literals, clauses, terms.

Clearly deciding different candidate definitions complicated counting number properties definitions satisfy, simple reason
properties important others. Take instance property P5
requires clauses terms expressive enough represent formulae K.
use standard propositional definition clauses terms (thereby disregarding
modal operators), find satisfies every property except P5, hence
properties definitions considered section, yet would
hard-pressed find someone considers propositional definition appropriate definition K. demonstrates expressiveness particularly important property,
important fact willing sacrifice properties P2 P7 keep it.
Among definitions satisfy P5, prefer definitions D4 D5 definitions D3a
D3b, latter definitions less common propositional definition
present advantages D4 D5.
course, comes it, choice definition must depend
particular application mind. may well circumstances less
expressive less elegant definition may prove suitable. paper
using clauses terms define prime implicates prime implicants, us
important criteria choosing definition quality notions prime
implicates prime implicants definition induces.

4. Prime Implicates/Implicants K
definition clauses terms fixed, define prime implicates
prime implicants exactly manner propositional logic:
Definition 7. clause implicate formula |= . prime
implicate if:
1. implicate
2. implicate |= , |=
Definition 8. term implicant formula |= . prime
implicant if:
1. implicant
80

fiPrime Implicates Prime Implicants Modal Logic

2. implicant |= , |=
course, notion prime implicate (resp. implicant) get determined definition clause (resp. term) chosen. compare different definitions using following well-known properties prime implicates/implicants
propositional logic:
Finiteness number prime implicates (resp. prime implicants) formula finite
modulo logical equivalence.
Covering Every implicate formula entailed prime implicate formula.
Conversely, every implicant formula entails prime implicant formula.
Equivalence model model model prime
implicates model prime implicant 3 .
Implicant-Implicate Duality Every prime implicant formula equivalent
negation prime implicate negated formula. Conversely, every prime
implicate formula equivalent negation prime implicant negated
formula.
Distribution prime implicate 1 ... n , exist prime implicates
1 , ..., n 1 , ..., n 1 ... n . Likewise, prime implicant
1 ... n , exist prime implicants 1 , ..., n 1 , ..., n
1 ... n
Finiteness ensures prime implicates/implicants formula finitely
represented, Covering means prime implicates provide complete representation
formulas implicates. Equivalence guarantees information lost replacing formula prime implicates/implicants, whereas Implicant-Implicate Duality
allows us transfer results algorithms prime implicates prime implicants,
vice-versa. Finally, Distribution relates prime implicates/implicants formula
prime implicates/implicants sub-formulae. property play key role
prime implicate generation algorithm presented next section.
show definition D4 satisfies five properties. Finiteness Covering, first demonstrate every implicate formula entailed implicate
var( ) var() depth () + 1 (and similarly implicants). finitely many non-equivalent formulae finite language
bounded depth, follows finitely many prime implicates/implicants
given formula, infinite chains increasingly stronger implicates (or increasingly weaker implicants). Equivalence follows directly Covering
property P5 previous section: use P5 rewrite conjunction
clauses, implied prime implicate Covering.
property Implicant-Implicate Duality immediate consequence duality
3. property Equivalence commonly taken mean formula equivalent conjunction prime implicates disjunction prime implicants. chosen model-theoretic
formulation order allow possibility set prime implicates/implicants infinite.

81

fiBienvenu

clauses terms (P4). Distribution shown using Covering plus
fact disjunction clauses clause conjunction terms term (P3).
Theorem 9. notions prime implicates prime implicants induced definition
D4 satisfy Finiteness, Covering, Equivalence, Implicant-Implicate Duality,
Distribution.
remark way contrast first-order logic notion prime implicate induced standard definition clauses shown falsify Finiteness, Covering,
Equivalence (Marquis, 1991a, 1991b).
show definition D4 one definitions satisfy five
properties. definitions D1 D2, show Equivalence hold.
fairly straightforward consequence fact definitions satisfy
property P5.
Theorem 10. notions prime implicates prime implicants induced definitions
D1 D2 satisfy Equivalence.
notions prime implicates induced definitions D3a, D3b, D5, show
appendix clause 2 3k 3(a b 2k a) prime implicate 2(a b)
every k 14 . thereby demonstrate definitions admit formulae
infinitely many distinct prime implicates also allow seemingly irrelevant
clauses counted prime implicates. gives us strong grounds dismissing
definitions much utility prime implicates applications comes ability
eliminate irrelevant consequences.
Theorem 11. notions prime implicates prime implicants induced D3a, D3b,
D5 falsify Finiteness.
comparison last section suggested D5 least suitable
D4 definition clauses terms, results section rule D5 suitable
definition prime implicates prime implicants. remainder paper,
concentrate attention notions prime implicates prime implicants induced
definition D4, shown satisfactory generalizations
propositional case.

5. Prime Implicate Generation Recognition
section, investigate computational aspects modal prime implicates.
primarily focusing notion prime implicate induced definition D4,
remainder paper use words clause, term, prime implicate
mean clause, term, prime implicate respect definition D4, except
explicitly stated otherwise.
remark that, without loss generality, restrict attention prime implicates since Implicant-Implicate Duality (Theorem 9) algorithm generating
recognizing prime implicates easily adapted algorithm generating
recognizing prime implicants.
4. D4, prime implicate 2(a b) itself.

82

fiPrime Implicates Prime Implicants Modal Logic

Function Dnf -4()
Input: formula
Output: set D4-terms
Return set terms output Iter-Dnf-4({Nnf ()}).
Function Iter-Dnf -4(S)
Input: set formulae NNF
Output: set D4-terms, output one-by-one
= { } , Iter-Dnf -4(S {} {})
Else = { } , Iter-Dnf -4(S {}), Iter-Dnf -4(S {})
V
Else output consistent (and nothing otherwise)
Figure 2: Helper functions Dnf -4 Iter-Dnf -4.

5.1 Generating Prime Implicates
start considering problem generating set prime implicates given
formula. task important want produce abductive explanations, want
compile formula set prime implicates.
generation algorithm, require means transforming input formula
equivalent disjunction simpler formulae. end, introduce Figure 2
helper function Dnf -4() returns set satisfiable terms respect D4
whose disjunction equivalent . function Dnf -4 defined terms another
function Iter-Dnf -4 takes input set formulae NNF returns
iterative fashion set satisfiable terms whose disjunction equivalent S. following
lemmas highlight important properties functions.
Lemma 12. Iter-Dnf-4 terminates requires polynomial space size
input.
Lemma 13. output Dnf-4 input set satisfiable terms respect
D4 whose disjunction equivalent .
Lemma 14. 2|| terms Dnf-4(). terms length
2||, depth (), contains propositional variables appearing
var().
present Figure 3 algorithm GenPI computes set prime implicates
given formula. algorithm works follows: Step 1, check whether
unsatisfiable, outputting contradictory clause case. satisfiable ,
set equal set satisfiable terms whose disjunction equivalent .
Distribution, know every prime implicate equivalent disjunction
prime implicates terms . Step 2, set (T ) equal propositional
83

fiBienvenu

Function GenPI()
Input: formula
Output: set clauses
(1) unsatisfiable, return {3(a a)}. Otherwise, set = Dnf -4().
(2) : let LT set propositional literals let DT
set formulae 3 . literals
form 2 , set (T ) = LT {3 | DT }. Otherwise, set (T ) =
LT {2T } {3( ) | DT }
conjunction
formulae 2 .
W
(3) Set Candidates = { | (T )}.
(4) j Candidates: remove j Candidates k |= j
k Candidates k < j, j |= k j 6|= k k > j.
(5) Return Candidates.
Figure 3: Algorithm prime implicate generation.

literals (LT ) plus strongest 2-literal implied (2T ) plus strongest 3literals implied ({3( ) | DT }). hard see every prime
implicate must equivalent one elements (T ). means
Step 3 guaranteed every prime implicate input formula equivalent
candidate prime implicate Candidates. comparison phase Step 4,
non-prime candidates eliminated, exactly one prime implicate equivalence
class retained.
illustrate behavior GenPI example:
Example 15. run GenPI input = ((3(b c) 3b) (3b 3(c d) 2e 2f )).
Step 1: satisfiable, call function Dnf -4 , returns two terms
T1 = 3(b c) 3b T2 = 3b 3(c d) 2e 2f .
Step 2: LT1 = {a}, DT1 = {b c, b}, 2-literals T1 ,
get (T1 ) = {a, 3(b c), 3b}. T2 , LT2 = {a}, DT2 = {b, c d},
T2 = e f , giving us (T2 ) = {a, 2(e f ), 3(b e f ), 3((c d) e f )}.
Step 3: set Candidates contain different possible disjunctions elements
(T1 ) elements (T2 ), 12: aa, a2(ef ), a3(bef ),
3((c d) e f ), 3(b c) a, 3(b c) 2(e f ), 3(b c) 3(b e f ),
3(bc)3((cd)ef ), 3ba, 3b2(ef ), 3b3(bef ), 3b3((cd)ef ).
Step 4: remove Candidates clauses 2(e f ), 3(b e f ),
3((c d) e f ), 3(b c) a, 3b since strictly weaker
a. also eliminate clauses 3b 2(e f ), 3b 3(b e f ),
84

fiPrime Implicates Prime Implicants Modal Logic

3b 3((c d) e f ) since weaker clauses 3(b c) 2(e f ),
3(b c) 3(b e f ), 3(b c) 3((c d) e f ).
Step 5: GenPI return four remaining clauses Candidates, a,
3(b c) 2(e f ), 3(b c) 3(b e f ), 3(b c) 3((c d) e f ).
algorithm shown sound complete procedure generating prime
implicates.
Theorem 16. algorithm GenPI always terminates outputs exactly set
prime implicates input formula.
examining prime implicates produced algorithm, place upper
bound length formulas prime implicates.
Theorem 17. length smallest clausal representation prime implicate
formula single exponential length formula.
upper bound optimal find formulae exponentially large prime
implicates. situation contrasts propositional logic, length prime
implicates linearly bounded number propositional variables formula.
Theorem 18. length smallest clausal representation prime implicate
formula exponential length formula.
interesting note formula used proof Theorem 18 depth
1, means cannot avoid worst-case spatial complexity restricting
attention formulae shallow depth. escape exponential worst-case
spatial complexity dropping one less expressive notions prime implicates
examined previous section, following theorem attests.
Theorem 19. prime implicates defined using either D1 D2, length
smallest clausal representation prime implicate formula exponential
length formula.
examination set candidate prime implicates constructed algorithm
allows us place bound maximal number non-equivalent prime implicates
formula possess.
Theorem 20. number non-equivalent prime implicates formula
double exponential length formula.
bound also shown optimal. situation contrasts propositional
logic, single exponentially many non-equivalent prime implicates
given formula.
Theorem 21. number non-equivalent prime implicates formula may double
exponential length formula.
Again, worst-case result robust improved neither restricting depth formulae, using less expressive notions prime implicate,
following theorem demonstrates.
85

fiBienvenu

Theorem 22. prime implicates defined using either D1 D2, number
non-equivalent prime implicates formula may double exponential length
formula.
Theorems 19 22 together suggest definitions D1 D2 yield especially interesting approximate notions prime implicate, induce significant loss
expressivity without improvement size number prime implicates
worst-case.
generation algorithm GenPI corresponds simplest possible implementation
distribution property, quite clear represent practicable
way producing prime implicates. One major source inefficiency large number
clauses generated, want design efficient algorithm, need
find ways generate fewer candidate clauses. couple different techniques
could used. One simple method could yield smaller number clauses
eliminate (T ) elements prime implicates , thereby
decreasing cardinalities (T ) hence Candidates. this, simply
test whether tautology (and remove is) compare 3-literals
(T ), discarding weaker elements. apply technique Example 15, would
remove 3b (T1 ), thereby reducing cardinality Candidates 12 8.
substantial savings could achieved using technique developed framework propositional logic (cf. Marquis, 2000) consists calculating prime implicates T1 , prime implicates T1 T2 , T1 T2 T3 ,
get prime implicates full disjunction terms. interleaving comparison
construction, eliminate early partial clause cannot give rise prime
implicates instead producing extensions partial clause deleting
one one comparison phase. example, two terms,
imagine third term T3 . applying technique, would
first produce 4 prime implicates T1 T2 would compare 4|(T3 )|
candidate clauses T1 T2 T3 . Compare current algorithm generates
compares 12|(T3 )| candidate clauses.
Given number elements Candidates double exponential
length input, cutting size input GenPI could yield significant
savings. One obvious idea would break conjunctions formulae conjuncts,
calculate prime implicates conjuncts. Unfortunately, however,
cannot apply method every formula prime implicates conjuncts
necessarily prime implicates full conjunction. One solution proposed
context approximation description logic concepts (cf. Brandt & Turhan, 2002)
identify simple syntactic conditions guarantee get result
break formula conjuncts. instance, one possible condition
conjuncts share propositional variables. formula example satisfies
condition since variables ((3(b c) 3b) (3b 3(c d) 2e 2f ))
disjoint. generating prime implicates conjuncts separately, directly
identify prime implicate a, 6 candidate clauses ((3(b c) 3b)
(3b 3(c d) 2e 2f )) compare. also remove weaker elements (Ti )
86

fiPrime Implicates Prime Implicants Modal Logic

suggested above, get 3 candidate clauses ((3(bc)3b)(3b3(cd)2e2f )),
prime implicates .
Another important source inefficiency algorithm comparison phase
compare candidate clauses one-by-one order identify strongest ones.
problem course worst-case double exponential
number candidate clauses, simply may double exponentially many
distinct prime implicates, prime implicate must equivalent candidate
clause. Keeping double exponentially many clauses memory generally
feasible. Fortunately, however, necessary keep candidate clauses
memory since generate demand sets (T ). Indeed,
demonstrate appendix, implementing algorithm clever fashion,
obtain algorithm outputs prime implicates iteratively requiring
single-exponential space (the output algorithm could course double exponentially
large Theorem 21).
Theorem 23. exists algorithm runs single-exponential space size
input incrementally outputs, without duplicates, set prime implicates
input formula.
Although modified algorithm much better spatial complexity original,
still yield practicable means generating prime implicates. reason
still need compare candidate clauses candidate clauses
order decide whether candidate prime implicate not. Given set
candidate clauses may double exponential number, means algorithm may
need perform double exponentially many entailment tests producing even single
prime implicate. much promising approach would test directly whether
candidate clause prime implicate without considering candidate clauses.
order implement approach, must course come procedure
determining whether given clause prime implicate. objective
following section.
5.2 Recognizing Prime Implicates
focus section problem recognizing prime implicates, is, problem
deciding whether clause prime implicate formula . discussed
previous subsection, problem central importance, algorithm
generating prime implicates must contain (implicitly explicitly) mechanism
ensuring generated clauses indeed prime implicates.
propositional logic, prime implicate recognition BH2 -complete (Marquis, 2000),
hard satisfiability deduction. K, satisfiability unsatisfiability
Pspace-complete, cannot hope find prime implicate recognition algorithm
complexity less Pspace.
Theorem 24. Prime implicate recognition Pspace-hard.
order obtain first upper bound, exploit Theorem 17 tells us
exists polynomial function f every prime implicate formula
87

fiBienvenu

equivalent clauses length 2f (||) . leads simple procedure
determining clause prime implicate formula . simply check every
clause length 2f (||) whether implicate implies
implied . case, prime implicate (we found logically
stronger implicate ), otherwise, exists stronger implicate, prime
implicate. hard see algorithm carried exponential
space, gives us Expspace upper bound.
course, problem naive approach take
account structure , end comparing huge amount irrelevant clauses,
exactly hoping avoid. algorithm propose later
section avoids problem exploiting information input formula clause
order cut number clauses test. key algorithm
following theorem shows general problem prime implicate recognition
reduced specialized tasks prime implicate recognition propositional
formulae, 2-formulae, 3-formulae. simplify presentation theorem,
let () refer set prime implicates , use notation \ {l1 , ..., ln }
refer clause obtained removing literals li . example
(a b 3c) \ {a, 3c} refers clause b.
Theorem 25. Let formula K, let = 1 ... k 31 ... 3n 21 ...
2m (j propositional literals) non-tautologous clause (a) 1 ...n
i, (b) literal l \ {l}. ()
following conditions hold:
1. 1 ... k ( ( \ {1 , ..., k }))
2. 2(i 1 ... n ) ( ( \ {2i })) every
3. 3(1 ... n ) ( ( \ {31 , ..., 3n }))
remark restriction clauses 1 ...
6 \ {l} l required. drop first requirement,
non-prime implicates satisfy three conditions, drop second,
prime implicates fail satisfy one conditions5 . restrictions
without loss generality however since every clause transformed equivalent
clause satisfying them. first condition, replace 2i 2(i 1 ... ),
thereby transforming clause 1 ... k 31 ...3m 21 ... 2n equivalent
1 ... k 31 ...3m 2(1 1 ... ) ... 2(n 1 ... ). make
clause satisfy second condition, simply remove literals
\ {l} literal remains.
Theorem 25 shows prime implicate recognition split three specialized sub-tasks, tell us carry tasks. Thus, order turn
5. first restriction, consider formula = 3(abc)2a clause = 3(ab)2(ab).
easily shown implicate , prime implicate since exist
stronger implicates (e.g. itself). Nonetheless, verified 2(a b (a b))
( ( \ {2(a b)})) 3(a b) ( ( \ {3(a b)})). second restriction, consider
formula 2a clause 2a2(ab). 2(ab) 6 (2a(2a)) even though 2a2(ab) 2a
prime implicate 2a.

88

fiPrime Implicates Prime Implicants Modal Logic

theorem algorithm prime implicate recognition, need figure
test whether propositional clause, 2-formula, 3-formula prime implicate
formula.
Determining whether propositional clause prime implicate formula K
conceptually difficult determining whether propositional clause prime
implicate propositional formula. first ensure clause implicate
formula make sure literals appearing clause necessary.
Theorem 26. Let formula K, let non-tautologous propositional clause
|= literal l \ {l}. ()
6|= \ {l} l .
move problem deciding whether clause form 2 prime
implicate formula . remark 2 implied , must also implied
terms Ti Dnf -4(). Ti |= 2, Theorem 1, must
case conjunction 2-literals Ti implies 2. means formula
21 ... 2n (where conjunction formulae 2 Ti )
implicate implies 2, moreover strongest implicate. follows
2 prime implicate case 2 |= 21 ... 2n ,
true |= (by Theorem 1). Thus, comparing formula
formulae associated terms , decide whether 2
prime implicate .
Theorem 27. Let formula K, let = 2 non-tautologous clause
|= . () exists term Dnf-4()
|= , conjunction formulae 2 .
Finally let us turn problem deciding whether clause 3 prime implicate
formula . know Covering 3 implicate , must
prime implicate implies 3. follows Theorem 2 must

W disjunction 3-literals, Theorem 16 equivalent disjunction
Dnf -4() 3dT 3dT element (T ) every (refer back Figure 3
definition (TW
)). According Definition 7, 3 aWprime implicate
case 3 |= Dnf -4() 3dT , equivalently |= Dnf -4() dT . Thus, 3
prime implicate W
case isWa choice 3dT (T )
6|= Dnf -4() dT .
Dnf -4() Dnf -4() dT |= W
Testing directly whether entails formula Dnf -4() dT could take exponential
space worst case since may exponentially many terms Dnf -4(). Luckily,
W however, get around problem exploiting structure formula
Dnf -4() dT . remark way (T ) defined formula dT must
conjunction formulae 2 3 appears Nnf () outside scope
modal operators use X W
denote set formulae satisfying condition.
show appendix
6|= Dnf -4() dT implies existence subset X
W
(a) 6|= (b) every dT least W
one conjunct set S. Conversely,
existence subset X implies 6|= Dnf -4() dT . observation
basis algorithm Test3PI given Figure 4. basic idea behind algorithm
try different subsets X order see whether subset satisfies
89

fiBienvenu

Function Test3PI(3, )
Input: clause 3 formula |= 3
Output: yes
(1) |= , return yes |= otherwise.
(2) Set X equal set formulae 2 3 appears Nnf () outside
scope modal operators.
(3) W
X , test whether following two conditions hold:
(a) 6|=
(b) Ti Dnf -4(), exists conjuncts 3i , 2i,1 , ..., 2i,k(i) Ti
that:
(i) {i , i,1 , ..., i,k(i) } 6=
(ii) 3(i i,1 ... i,k(i) ) |= 3
Return satisfies conditions, yes otherwise.
Figure 4: Algorithm identifying prime implicates form 3.

aforementioned conditions. find suitable subset, proves 3 prime
implicate, subset exists, sure stronger implicate
3. algorithm shown run polynomial space since
|| elements X , consider terms Dnf -4() one time.
Theorem 28. Let formula, let 3 implicate . algorithm
Test3PI returns yes input (3, ) 3 prime implicate .
Theorem 29. algorithm Test3PI runs polynomial space.
illustrate algorithm Test3PI two examples.
Example 30. use Test3PI test whether clause = 3(ab) prime implicate
= (2(b c) 2(e f )) 3(a b).
Step 1: satisfiable, pass directly Step 2.
Step 2: set X equal set formulae 2 3 appears Nnf ()
outside scope modal operators. case, set X = {b c, e f, b}
since =Nnf () b c, e f , b formulae satisfying
requirements.
Step 3: examine different subsets X determine whether satisfy
conditions (a) (b). particular, consider subset = {b c, e f }.
remark subset satisfies condition (a) since b 6|= (b c) (e f ). order
check condition (b), first call function Dnf -4 returns two
terms T1 = 2(b c) 3(a b) T2 = 2(e f ) 3(a b). notice
conjuncts 3(a b) 2(b c) T1 satisfy conditions (i) (ii) since b c
3(a b (b c)) |= . notice conjuncts 3(a b) 2(e f )
T2 also satisfy conditions (i) (ii) since e f 3(a b (e f )) |= .
90

fiPrime Implicates Prime Implicants Modal Logic

Function TestPI(, )
Input: clause formula
Output: yes
(1) 6|= , return no.
(2) |= , return yes |= not.
|= , return yes |= otherwise.
(3) li = l1 ... ln , test \ {li } , so, remove li .
Let = {31 , ..., 3m } set 3-literals . non-empty, replace
disjunct 2 literal 2( 1 ... ).
(4) Let P set propositional literals disjuncts .
l P, check whether |= \ {l}, return so.
(5) Let B set 2-formulae appearing disjuncts . Check 2
B whether Dnf -4( ( \ {2})) formula
2( 1 ... k ) implies conjunction 2-literals , return
not.
W
(6) empty, return yes, otherwise return Test3PI(3(
i=1 ), ( \ D)).
Figure 5: Algorithm identifying prime implicates.

means found subset X satisfies conditions (a) (b),
algorithm returns no. correct output since 3(a b ((b c) (e f )))
implicate strictly stronger .
Example 31. use Test3PI test whether clause = 3(a b c) prime
implicate = (2(b c) 2(e f )) 3(a b) 2(e f (a b c)).
Step 1: proceed directly Step 2 since satisfiable.
Step 2: set X = {b c, e f, b, e f (a b c))} since Nnf ()=a (2(b
c) 2(e f )) 3(a b) 3(e f (a b c)).
Step 3: check whether subset X satisfying conditions (a) (b).
claim subset. see why, notice 2(b c) 3(a b)
3(e f (a b c)) term Dnf -4(). Moreover, one
set conjuncts term implies 3(a b c), namely {3(a b), 2(b c)}.
means must contain either b b c order satisfy condition
(b)(i). b c implies b b c, guaranteed b c
imply disjunction elements S, thereby falsifying condition (a). follows
subset X satisfying necessary conditions, Test3PI returns
yes, desired result.
Figure 5, present algorithm testing whether clause prime implicate
formula . first two steps algorithm treat limit cases
implicate one tautology contradiction. Step 3,
91

fiBienvenu

apply equivalence-preserving transformations make satisfy requirements
Theorem 25. Steps 4, 5, 6 use procedures Theorems 26, 27,
28 test whether three conditions Theorem 25 verified. three tests
succeed, Theorem 25, clause prime implicate, return yes.
test fails, return clause shown prime implicate.
Theorem 32. algorithm TestPI always terminates, returns yes input (,
) prime implicate .
demonstrate use TestPI example.
Example 33. use TestPI test clauses 1 = b, 2 = 2b 2(e f ), 3 = 3c,
4 = 3(a b), 5 = 3(a b c) 3(a b c f ) 2(e f ) prime implicates
= (2(b c) 2(e f )) 3(a b).
1 : output Step 1 since 6|= 1 .
2 : skip Steps 1 2 since |= 2 neither |= |= 2 . Step 3,
make changes 2 since contains redundant literals 3-literals.
skip Step 4 since 2 propositional disjuncts. Step 5, return since
Dnf -4( (2 \ {2b})) = {a 2(b c) 3(a b) 3(e f )} 2b 6|= 2(b c).
3 : proceed directly Step 3 since |= 3 , 6|= , 6|= 3 . modifications
made 3 Step 3 contain redundant literals 2-literals.
Step 4, test whether |= 3 \ {a}. 6|= 3c, proceed Step
5, directly Step 6 since 3 contains 2-literals. Step 6, call
Test3PI(3c, (3 \ {3c})), outputs since (3 \ {3c}) |=
c 6|= .
4 : Steps 1-5 inapplicable, skip directly Step 6. step, call
Test3PI input clause 3(a b) formula (4 \{3(a b)}) = .
already seen Example 30 Test3PI returns input,
means TestPI also returns no.
5 : proceed directly Step 3, delete redundant literal 3(a b c f )
modify literal 2(ef ). end step, 5 = 3(abc)
2((ef )(abc)). Step 4 applicable since propositional disjuncts
5 . Step 5, continue since Dnf -4( (5 \ {2((e f (a b c))})) =
{a2(ef )3(ab)2(abc)}, 2(((ef (abc))(abc)) |=
2(e f ) 2(a b c). Step 6, return yes since call Test3PI input
(3(a b c), (5 \ {3(a b c)})), previously shown Example
31 Test3PI returns yes input.
show appendix algorithm TestPI runs polynomial space.
already shown TestPI decides prime implicate recognition, follows
problem Pspace:
Theorem 34. Prime implicate recognition Pspace.
92

fiPrime Implicates Prime Implicants Modal Logic

putting together Theorems 24 34, obtain tight complexity bound
prime implicate recognition task.
Corollary 35. Prime implicate recognition Pspace-complete.

6. Conclusion Future Work
first contribution work detailed comparison several different possible
definitions clauses, terms, prime implicates, prime implicants modal logic K.
results investigation largely positive: although shown
perfect definition exists, exhibit simple definition (D4) satisfies
desirable properties propositional case. second contribution work
thorough investigation computational aspects selected definition D4.
end, presented sound complete algorithm generating prime implicates, well
number optimizations improve efficiency algorithm. examination
structure prime implicates generated algorithm allowed us place upper
bounds length prime implicates number prime implicates formula
possess. showed bounds optimal exhibiting matching lower bounds,
proved lower bounds hold even much less expressive notions prime implicates. Finally, constructed polynomial-space algorithm deciding
prime implicate recognition, thereby showing problem Pspace-complete,
lowest complexity could reasonably expected. Although focus paper
logic K, results easily lifted multi-modal K concept
expressions well-known description logic ALC.
mentioned introduction, one main applications prime implicants
propositional logic area abductive reasoning, prime implicants play
role abductive explanations. results paper directly applied
problem abduction K: notion prime implicants used definition
abductive explanations K, prime implicate generation algorithm provides means
producing abductive explanations given abduction problem. Moreover,
notion term underlying definition abductive explanations
expressive used Cialdea Mayer Pirri (1995), able find explanations
overlooked method. instance, look explanation
observation c given background information 2(a b) c, obtain 2(a b), whereas
framework yields 2a 2b. argument favor approach since
generally abduction one looking find weakest conditions guaranteeing truth
observation given background information.
Also interest results size number prime implicates,
yield corresponding lower bounds size number abductive explanations.
particular, results imply abductive explanations Cialdea Mayer Pirri
(1995) exponential size double exponentially many number worst
case, thus behave better respects notion abductive explanation
induced preferred definition D4. Moreover, fact lower bounds hold
even case extremely inexpressive notion abductive explanations induced
definition D2 suggests high worst-case complexity results really cannot
93

fiBienvenu

avoided. light intractability results, interesting question future research
would study problem generating single prime implicate, since applications may prove sufficient produce single minimal explanation observation.
Another interesting subject future work relevant point view abduction investigation notion prime implicate fixed vocabulary.
development generation algorithms refined notion prime implicate would
allow one generate abductive explanations built given set
propositional variables.
second domain application mentioned introduction
area knowledge compilation. propositional logic, one well-known target language
knowledge compilation prime implicate normal form, formula represented
conjunction prime implicates. natural idea would use selected
definition prime implicate define analogous manner notion prime implicate
normal form K formulae. Unfortunately, normal form obtain satisfies
nice properties propositional case. instance, find entailment two
formulae prime implicate normal form easier arbitrary K formulae.
see why, consider pair formulae negation normal form. formulae
3 3 prime implicates hence prime implicate normal form
according naive definition. |= case 3 |= 3, reduce
entailment arbitrary K formulae NNF entailment formulae prime
implicate normal form. former problem known Pspace-complete, follows
latter Pspace-complete well.
first sight, appears quite disappointing result one would hope
computational difficulty representing formula prime implicates would offset
good computational properties resulting formula. turns out, however,
problem lies definition prime implicates rather naive way
defining prime implicate normal form. Indeed, continuation present work (Bienvenu, 2008), proposed sophisticated definition prime implicate normal form,
specify many different clausal representations prime implicate
used. normal form shown enjoy number desirable properties
make interesting viewpoint knowledge compilation. notably,
proven entailment formulae K prime implicate normal form
carried polynomial time using simple structural comparison algorithm
reminiscent structural subsumption algorithms used subpropositional description
logics. noted proof results Bienvenu (2008) make
ample use material presented current paper.
work, studied prime implicates respect local consequence relation,
natural direction future work would investigation prime implicates respect global consequence relation. question particularly interesting given
global consequence type consequence used description logic ontologies. Unfortunately, preliminary investigations suggest defining generating prime implicates
respect global consequence relation likely prove difficult
local consequence relation. one thing, use definition clause reasonably
94

fiPrime Implicates Prime Implicants Modal Logic

expressive, notion prime implicate obtain satisfy Covering since
construct infinite sequences stronger stronger implicates. Take instance
formula (a b) (b 3b) implies (using global consequence relation)
increasingly stronger clauses infinite sequence 3b, 3(b 3b),
3(b 3(b 3b)), ... familiar situation description logic practitioners since
infinite sequences responsible non-existence specific concepts
many common DLs (cf. Kusters & Molitor, 2002) lack uniform interpolation
ALC TBoxes (Ghilardi, Lutz, & Wolter, 2006). standard solution problem
simply place bound depth formulae considered, effectively blocking
problematic infinite sequences. allow us regain Covering, give
us weaker version property, sufficient applications.
development generation algorithms global consequence relation may also prove
challenging, since unclear point whether able draw inspiration
pre-existing methods. Despite potential difficulties, feel subject worth
exploring since could contribute development flexible ways accessing
structuring information description logic ontologies.
Finally, another natural direction future research would extend investigation prime implicates prime implicants popular modal description logics.
Particularly interest modal logics knowledge belief expressive description
logics used semantic web. confident experience gained
investigation prime implicates prime implicants K prove valuable asset
exploration modal description logics.

Acknowledgments
paper corrects significantly extends earlier conference publication (Bienvenu,
2007). paper written author PhD student working IRIT,
Universite Paul Sabatier, France. author would like thank thesis supervisors
Andreas Herzig, Jerome Lang, Jerome Mengin, well anonymous reviewers
helpful feedback.

Appendix A. Proofs
Theorem 1 Let , 1 , ..., , , 1 , ..., n formulae K, let propositional
formula.
1. |= |= |=
2. |= 3 |= 3 2 |= 2
3. 31 ...3m 21 ...2n |= ( |= 1 ...n |= i)
4. |= 31 ... 3m 21 ... 2n (|= |= 1 ... i)
5. 2 |= 21 ... 2n |=
95

fiBienvenu

6. 31 ... 3m 21 ... 2n
31 ... 3m 2(1 1 ... ) ... 2(n 1 ... )
Proof. first statement well-known property local consequence, prove
completeness:
|=






M, w |= implies M, w |= M, w
M, w 6|= M, w |= M, w
M, w |= M, w |= M, w
|=
M, w 6|= M, w
|=

second statement, 6|= , M, w M, w |= .
Create new model adding new world w placing single arc
w w. , w |= 3 2, means 3 2 satisfiable hence
3 6|= 3 (since 2 3). direction, suppose 3 6|= 3.
exists M, w M, w |= 3 3 3 2. means
w , hence 6|= . complete proof, use following chain
equivalences: 2 |= 2 2 |= 2 3 |= 3 |= |= .
3, suppose 31 ... 3m 21 ... 2n 6|= . exist M, w
M, w |= 31 ...3m 21 ... 2n . M, w |= , cannot
|= , 1 ... n |= since w
M, w |= 1 ... n . direction suppose
1 ... n satisfiable. propositional model w ,
i, find Mi , wi Mi , wi |= 1 ... n . construct new
Kripke structure contains models Mi world w arcs
going w wi . hard see new model Mnew
Mnew , w |= 31 ...3m 21 ...2n , 31 ...3m 21 ... 2n 6|= .
Statement 4 follows easily third statement. simply notice 31
... 3m 21 ... 2n tautology case negation 31
... 3n 21 ... 2m unsatisfiable.
5, use statements 1 4 get following chain equivalences:
2 |= 21 ... 2n
|= 3 21 ... 2n
|=
|=
first implication equivalence 6 immediate since 31 ... 3m |=
31 ... 3m 2i |= 2(i 1 ... ) i. direction, remark
using statements 1 3, get following equivalences:
2(i 1 ... ) |= 2i 31 ... 3m
2(i 1 ... ) (2i 31 ... 3m ) |=
2(i 1 ... ) 3i 21 ... 2m |=
(i 1 ... ) 1 ... |=
96

fiPrime Implicates Prime Implicants Modal Logic

(i 1 ... ) 1 ... clearly unsatisfiable, follows
2(i 1 ... ) |= 2i 31 ... 3m every hence 31 ... 3m
2(1 1 ... ) ... 2(n 1 ... ) |= 31 ... 3m 21 ... 2n ,
completing proof.
Theorem 2 Let disjunction propositional literals 2- 3-formulae.
following statements holds:
1. |= non-tautological propositional clause , every disjunct
either propositional literal unsatisfiable 3-formula
2. |= 31 ... 3n , every disjunct 3-formula
3. |= 21 ... 2n 6|= 21 ... 2n , every disjunct either
2-formula unsatisfiable 3-formula
Proof. (1), let non-tautologous propositional clause |= , suppose
contradiction contains disjunct 2 disjunct 3 6|= .
first case, 2 |= , hence |= 3 . follows Theorem 1
|= , contradicting assumption tautology. second case,
3 |= , thus |= 2 . Theorem 1, either |= |= . cases,
reach contradiction since assumed 6|= 6|= . follows
cannot 2-formulae satisfiable 3-formulae disjuncts.
proofs (2) (3) proceed similarly.
Theorem 3 Let = 31 ... 3m 21 ... 2n = 31 ... 3p
21 ... 2q formulae K. propositional 6|= ,

|=

1 ... |= 1 ... p
|=

every j |= 1 ... p j
Proof. Since 6|= , know 6|= 6|= 1 ... p i. Using
information together Theorem 1, get following equivalences:
|=


31 ... 3m |=



2i |=



|= 31 ... 3p 21 ... 2m
|=
|=
3(1 ... ) |=
|= 31 ... 3p 2(1 ... ) 21 ... 2q
|= 1 ... p (1 ... )
1 ... |= 1 ... p
|= 3(1 ... p ) 21 ... 2q
j |= 1 ... p j
j |= 1 ... p j

complete proof, use fact |= |= , 31 ... 3m |= ,
2i |= every i.
97

fiBienvenu

Theorem 5 definition literals, clause, terms K satisfies properties P1
P2 cannot satisfy P5.
Proof. remark set clauses (resp. terms) respect definition D1
precisely set formulae NNF contain (resp. ), i.e. D1
expressive definition satisfying P1 P2. Thus, show result, suffices
show D1 satisfy P5.
Suppose contradiction D1 satisfy P5. must exist clauses 1 , ..., n
3(a b) 1 ... n . clauses disjunction li,1 .... li,pi .
distributing , obtain following:
n
^

_

3(a b)

li,ji

(j1 ,...,jn){1,...,p1 }...{1,...,pn} i=1

infer (j1 , ..., jn ) {1, ..., p1 } ... {1, ..., pn }
n
^

li,ji |= 3(a b)

i=1

Vn

Consider (j1 , ..., jn ) i=1 li,ji consistent (there must least one
tuple, otherwise would 3(a b) ). formulae li,ji eitherVpropositional
literals formulae form 2 3 clause . follows ni=1 li,ji must
following form:
1 ... k 31 ... 3m 21 ... 2n
1 , ..., k propositional
literals 1 , ...,
V , 1 , ..., n clauses respect
V
D1. know ni=1 li,ji |= 3(a b) ni=1 li,ji 6|= , Theorem 1, must
3q
3q 21 ... 2n |= 3(a b)
show 3q 6|= 3(a b) (and hence 6|= 1 ... n ). Suppose
contradiction case. must q |= q |= b.
Theorem 1, every disjunct q (which recall D1-clause) must either unsatisfiable
equal b. latter
V impossible, follows q |= ,
contradiction since assumed ni=1 li,ji satisfiable. follows order
get 3q 21 ... 2n |= 3(a b), must r tautology.
let us consider formula
_
=
2j1 ,...,jn
{(j1 ,...,jn)|

Vn

i=1 li,ji 6}

V
2j1 ,...,jn non-tautological 2-formula appearing ni=1 li,ji (we shown
formula must exist). Clearly must case
n
^

_

(j1 ,...,jn){1,...,p1 }...{1,...,pn} i=1

98

li,ji |=

fiPrime Implicates Prime Implicants Modal Logic

get:
3(a b) |=
according Theorem 2, satisfiable 3-formula cannot imply disjunction 2formulae unless disjunction tautology, must |= . However,
impossible since would imply (Theorem 1) j1 ,...,jn tautology,
contradicting earlier assumption contrary. thus conclude
set clauses 1 , ..., n respect D1 3(a b) 1 ... n , hence
definition satisfies P1 P2 cannot satisfy P5.
order prove Theorem 6, make use following lemmas:
Lemma 6.1 Definition D5 satisfies P5.
Proof. demonstrate formula K NNF equivalent conjunction
clauses respect definition D5. restriction formulae NNF without loss
generality every formula equivalent formula NNF. proof proceeds
induction structural complexity formulae. base case propositional literals,
already conjunctions clauses since every propositional literal clause
respect D5. suppose statement holds formulae 1 2 show
holds complex formulae.
first consider = 1 2 . assumption, find clauses j
1 1 ... n 2 1 ... . Thus, equivalent formula
1 ... n 1 ... , conjunction clauses respect D5.
Next consider = 1 2 . induction hypothesis, 1 1 ... n
2 1 ... clauses j . Thus, (1 ... n ) (1 ... ),
written equivalently (i,j){1,...,n}{1,...,m}(i j ). Since union
two clauses produces another clause, j clauses, completing proof.
consider case = 21 . assumption, 1 1 ... n ,
clause. 2(1 ...n ). also know 2(1 ...n ) 21 ...2n .
follows equivalent 21 ... 2n , conjunction clauses since
2i clauses.
Finally, consider = 31 . Using induction hypothesis, 3(1
... n ) clauses . since clauses, disjunction literals
li,1 ... li,pi . distributing 3, find equivalent
formula
_
3(l1,j1 l2,j2 ... ln,jn )
(j1 ,...,jn){1,...,p1 }...{1,...,pn}

clause respect D5.
proof every formula equivalent disjunction terms respect D5
proceeds analogously.
Lemma 6.2 Every clause (resp. term) respect D5 clause (resp. term)
respect definitions D3a, D3b, D4.
Proof. show induction structural complexity formulae that:
99

fiBienvenu

1. every clause C respect D5 clause respect definitions D3a, D3b,
D4 disjunction terms respect D3a
2. every term respect D5 term respect definitions D3a, D3b,
D4 conjunction clauses respect D3a D3b
require stronger formulation statement prove sub-cases.
base case induction propositional literals, clauses
terms respect D5. easy see (1) (2) verified since propositional
literals clauses terms respect definitions D3a, D3b, D4 (and
hence also disjunctions terms respect D3a conjunctions clauses
respect D3a D3b).
induction step, show statements hold arbitrary clauses
terms respect D5 assumption statments hold
proper sub-clauses sub-terms.
begin clauses. Let C D5-clause proper sub-clauses subterms C satisfy (1) (2). since C clause respect D5, either
propositional literal formula form C1 C2 clauses C1 C2 , 2C1
clause C1 , 3T1 term T1 . case C propositional literal
already treated base case. Let us thus consider case C = C1 C2 .
first part (1) holds since induction hypothesis C1 C2 clauses
respect definitions D3a, D3b, D4, three definitions disjunction
two clauses clause. second half (1) also verified since C1 C2
disjunctions terms respect D3a, thus disjunction C1 C2 .
next consider case C = 2C1 clause C1 respect D5. first
part (1) follows easily know C1 must also clause respect D3a,
D3b, D4, definitions putting 2 clause yields another
clause. second part (1) holds well since C1 disjunction terms respect
D3a thus 2C1 term respect definition. suppose
C = 3T1 term T1 respect D5. definitions D3a D3b, know
induction hypothesis T1 conjunction clauses respect D3a
D3b hence 3T1 clause respect definitions. D4, result
obviously holds since allowed put formula NNF behind 3. second part
(1) holds since induction hypothesis T1 term respect D3a hence
3T1 also term respect definition.
next consider terms. Let D5-term proper sub-clauses subterms satisfy (1) (2). must either propositional literal formula
form T1 T2 terms T1 T2 , 2C1 clause C1 , 3T1 term T1 .
= T1 T2 , first half (2) holds since know T1 T2 terms respect
D3a, D3b, D4, conjunctions terms also terms three definitions.
second half also verified since T1 T2 assumed conjunctions clauses
respect D3a D3b, means also conjunction clauses
respect definitions. Next suppose = 2C1 . definitions D3b D4,
easy see literal hence term. D3a, induction hypothesis tells us
C1 disjunction terms, deduce 2C1 term. Moreover,
since C1 known clause respect D3a D3b, 2C1 must also
100

fiPrime Implicates Prime Implicants Modal Logic

clause respect definitions, conjunction clauses respect
D3a D3b. Finally, treat case = 3T1 . D3a, use fact
T1 term respect D3a, means 3T1 must also term. D3b,
use supposition T1 conjunction clauses respect D3b,
get 3T1 literal hence term. first part (2) clearly also holds D4
since formula behind 3 yields literal thus term. second half (2) follows
fact induction hypothesis T1 conjunction clauses respect
D3a D3b, 3T1 clause (and hence conjunction clauses) respect
definitions.

U ,S = 1,1 ... 1,m
i,j defined inductively follows

3i+1,j , either n, ui Sj , > n uin Sj
i,j =
2i+1,j , either n, ui 6 Sj , > n uin 6 Sj
{1, ..., 2n} 2n+1,j = , = 2...2
| {z } .
2n

Figure 6: formula U ,S codes instance U = {u1 , ..., un }, = {S1 , ..., Sm }
exact cover problem.

Lemma 6.3 Entailment terms clauses NP-complete definitions D1
D2.
Proof. proofs NP-membership NP-hardness, exploit relationship terms respect definitions D1 D2 concept expressions
description logic ALE (cf. Baader, McGuiness, Nardi, & Patel-Schneider, 2003).
recall concept expressions logic constructed follows (we use modal logic
syntax assume single modal operator order facilitate comparison
formalisms):
::= | | | | | 2 | 3
semantics symbols one would expect: M, w |= M, w 6|=
every model world w. semantics atomic literals, conjunctions, universal
existential modalities exactly K.
hard see every term respect D1 D2 concept expression
ALE. entailment ALE expressions decidable nondeterministic polynomial
time (cf. Donini, Lenzerini, Nardi, Hollunder, Nutt, & Marchetti Spaccamela, 1992),
follows deciding entailment terms respect either D1 D2 also
accomplished nondeterministic polynomial time, i.e. problems belong NP.
remains shown problems NP-hard. prove this, show
polynomial-time reduction Donini (2003) (adapted original NP-hardness
proof Donini et al., 1992) NP-complete exact cover (XC) problem (Garey &
101

fiBienvenu

Johnson, 1979) unsatisfiability ALE modified give polynomial-time
reduction XC entailment terms respect D1 D2.
exact cover problem following: given set U = {u1 , ..., un } set =
{S1 , ..., Sm } subsets U, determine whether exists
Sqan exact cover, is, subset
{Si1 , ..., Siq } Sih Sik = h 6= k k=1 Sik = U. Donini proven
(2003) U, exact cover formula U ,S pictured Figure 6
unsatisfiable. Notice U ,S term respect either D1 D2 uses
symbols . would like find similar formula term respect
definitions satisfiable U ,S is. Consider formula
U ,S = 1,1 ... 1,m
i,j defined exactly like i,j except replace
a. easy verify U ,S indeed term respect D1
D2. Moreover, hard see 1,1 ... 1,m |= 32n
1,1 ... 1,m |= 32n hence U ,S U ,S equisatisfiable. U,
exact cover U ,S unsatisfiable, U ,S unsatisfiable case
U ,S is, follows U, exact cover U ,S unsatisfiable. Moreover,
U ,S produced linear time U ,S , polynomial-time reduction
XC unsatisfiability terms D1 D2. formula unsatisfiable
case entails term a. So, XC polynomially-reduced entailment
terms respect either D1 D2, making problems NP-hard hence
NP-complete.
order show NP-completeness clausal entailment, remark
definitions D1 D2, function Nnf transforms negations clauses terms
negations terms clauses. means test whether clause entails
clause testing whether term Nnf ( ) entails term Nnf (). Likewise,
test whether term entails another term testing whether clause Nnf ( )
entails clause Nnf (). NNF transformation polynomial, follows
entailment clauses exactly difficult entailment terms, clausal
entailment NP-complete.
Lemma 6.4 definition D5, entailment clauses terms Pspace-complete.
Proof. Membership Pspace immediate since entailment arbitrary formulae
K decided polynomial space. prove Pspace-hardness, adapt existing
proof Pspace-hardness K.
Figure 7 presents encoding QBF = Q1 p1 ...Qm pm K-formula f ()
used section 6.7 (Blackburn et al., 2001) demonstrate Pspace-hardness
K. formula f () property satisfiable case
QBF-validity. formula f () generated polynomial-time ,
QBF-validity problem known Pspace-hard, follows satisfiability formulae
K Pspace-hard well.
Figure 8, show modified encoding. claim following:
(1) f () f () logically equivalent
102

fiPrime Implicates Prime Implicants Modal Logic

(i) q0
Vm
(ii) i=0 ((qi j6=i qj ) 2(qi j6=i qj ) ... 2m (qi j6=i qj ))
Vm
(iiia) i=0 ((qi 3qi+1 ) 2(qi 3qi+1 ) ... 2m (qi 3qi+1 ))
V
(iiib) {i|Qi =} 2i (qi (3(qi+1 pi+1 ) 3(qi+1 pi+1 )))
V
Vm1 j
(iv) m1
i=1 ( j=i 2 ((pi 2pi ) (pi 2pi )))

(v) 2m (qm )

Figure 7: formula f () conjunction formulae.
(i) q0
Vm V
(ii) i=0 ( j6=i ((qi qj ) 2(qi qj ) ... 2m (qi qj )))
Vm
(iiia) i=0 ((qi 3qi+1 ) 2(qi 3qi+1 ) ... 2m (qi 3qi+1 ))
V
(iiib) {i|Qi =} 2i (qi 3(qi+1 pi+1 )) 2i (qi 3(qi+1 pi+1 ))
Vm1 Vm1
(iv) i=1 ( j=i (2j (pi 2pi ) 2j (pi 2pi )))

(v) 2m (qm 1 ) .... 2m (qm l )

Figure 8: formula f () conjunction formulae, formulae
(v) propositional clauses 1 ... l .

(2) CNF, f () conjunction clauses respect D5
(3) CNF, f () generated polynomial time f ()
show (1), suffices show (i)(i), (ii)(ii), (iiia)(iiia), (iiib)(iiib), (iv)(iv),
(v)(v). first equivalence immediate since (i) (i) identical. (ii)(ii)
follows fact 2k (qi j6=i qj ) j6=i 2k (qi qj ). (iiia)(iiia) holds
since (iiia) (iiia) qi 3qi+1 replaced qi 3qi+1 . (iiib)(iiib)
since 2i (qi (3(qi+1 pi+1 ) 3(qi+1 pi+1 ))) 2i (qi 3(qi+1 pi+1 )) 2i (qi
3(qi+1 pi+1 )). equivalence (iv)(iv) holds 2j ((pi 2pi ) (pi 2pi ))
2j (pi 2pi ) 2j (pi 2pi ). Finally, (v)(v) since 1 ... l . Thus, f ()
f () logically equivalent.
prove (2), show component formulae f () conjunction
clauses respect D5, provided CNF. Clearly case (i)
(i) propositional literal. formula (ii) also conjunction clauses
respect D5 since conjunction formulae form 2k (qi qj ). Similarly,
(iiia), (iiib), (iv) conjunctions clauses since formulae 2k (qi 3qi+1 ),
2i (qi 3(qi+1 pi+1 )), 2i (qi 3(qi+1 pi+1 )), 2k (pi 2pi ), 2k (pi 2pi )
clauses respect D5. formula (v) must also conjunction clauses since
assumed propositional clauses, making 2m (qm ) clause
respect D5, (v) conjunction clauses respect D5.
103

fiBienvenu

(3), clear transform (i), (iiia), (iiib), (iv) (i), (iiia), (iiib),
(iv) polynomial time transformations involve simple syntactic operations
resulting formulae twice large. transformation (ii) (ii)
slightly involved, hard see resulting formula
times large original (and greater length f ()).
step could potentially result exponential blow-up transformation
(v) (v), put CNF. assumption already CNF,
transformation executed polynomial time space,
separate conjuncts rewrite (qm ) (qm ).
let = Q1 p1 ...Qm pm QBF = 1 ... l propositional
clauses . Let f () formula defined Figure 8. (2) above, know
f () = 1 ... p clauses respect D5. consider following
formula
= 3(21 ... 2p 32(a a))
show f () satisfiable satisfiable follows:
unsatisfiable
21 ... 2p 32(a a) unsatisfiable
1 ... p 2(a a) unsatisfiable
1 ... p unsatisfiable
f () unsatisfiable
also know (1) f () f (), (Blackburn et al., 2001)
f () satisfiable case QBF validity. also easy see
satisfiable entail contradiction 3(a a). Putting
altogether, find valid case entail 3(a a).
3(a a) clauses terms respect D5, shown
QBF-validity problem QBF propositional formulae CNF reduced
problems entailment clauses terms respect D5. Moreover,
polynomial time reduction since follows (3) transformation
accomplished polynomial time. suffices show Pspace-hardness, since
well-known QBF-validity remains Pspace-hard even restrict propositional
part formula CNF (cf. Papadimitriou, 1994).
Theorem 6 results Figure 1 hold.
Proof. satisfaction dissatisfaction properties P1 P2 immediately
determined inspection definitions, satisfaction P3 definitions D2,
D3b, D4, D5. Counterexamples P3 definitions D1 D3a provided
body paper: formula 2(a b) clause disjunction literals
respect definitions.
order show definition D3b satisfy P4, remark negation
literal 3(a b) equivalent 2(a b) cannot expressed literal
D3b. definitions, shown (by straightforward inductive
proof) Nnf (L) literal whenever L literal, Nnf (C) term whenever
104

fiPrime Implicates Prime Implicants Modal Logic

C clause, Nnf (T ) clause whenever term. enough prove
definitions satisfy P4 since Nnf () equivalent .
Since know definitions D1 D2 satisfy properties P1 P2, follows
Theorem 5 definitions satisfy P5. seen Lemma 6.1
definition D5 satisfy P5, i.e. every formula equivalent conjunction
clauses respect D5 disjunction terms respect D5. every
clause (resp. term) D5 also clause (resp. term) respect definitions D3a, D3b,
D4 (by Lemma 6.2), follows every formula equivalent conjunction
clauses disjunction terms respect definitions, means
satisfy P5.
easy see property P6 satisfied definitions since
definitions context-free grammars, well-known deciding membership
context-free grammars accomplished polynomial time (cf. Younger, 1967).
Lemma 6.3, know deciding entailment clauses terms
respect either D1 D2 NP-complete (and hence P, unless P=NP). Entailment
clauses/terms Pspace-complete D5 (Lemma 6.4). every clause (resp.
term) D5 also clause (resp. term) respect definitions D3a, D3b, D4
(from Lemma 6.2), follows entailment clauses terms Pspace-hard
definitions. Membership Pspace immediate since entailment arbitary
K formulae Pspace.
prove Theorem 9 several steps:
Lemma 9.1 notions prime implicates prime implicants induced D4 satisfy
Implicant-Implicate Duality.
Proof. Suppose contradiction prime implicant formula
equivalent negation prime implicate . Let clause
equivalent (there must exist clause property P4, cf. Theorem
6). clause implicate since |= . assumed
prime implicate, must implicate |=
6|= . let term equivalent (here use P4). must
implicant since |= . Moreover, strictly weaker since |=
6|= . means cannot prime implicant,
contradicting earlier assumption. Hence, conclude every prime implicant
formula equivalent negation prime implicate . proof
every prime implicate formula equivalent negation prime implicant
proceeds analogously.
Lemma 9.2 clauses terms defined according definition D4, every implicate formula entailed implicate var( ) var()
depth () + 1, every implicant entails implicant
var( ) var() depth () + 1.
Proof. intend show following statement holds: formula
implicate , exists clause |= |= var( ) var()
105

fiBienvenu

() () + 1. let arbitrary formula, let implicate .
tautology, set = (where var()). , set
= 3(a a) (where var()), clause verifies necessary conditions.
consider case neither tautology falsehood,
show construct clause . first thing use Dnf-4 rewrite
disjunction satisfiable terms Ti respect D4 Ti contain
variables appearing depth ():
= T1 ... Tz
|= , must case Ti |= every Ti . aim find clause
terms Ti Ti |= |= var(i ) var(Ti ) (i ) (Ti ).
consider Ti . Since Ti term, form 1 ... k 31 ... 3m 21
... 2n , 1 , ..., k propositional literals. clause, must form
1 ... p 31 ... 3q 21 ... 2r , 1 , ..., p propositional literals.
Ti |= , must case formula
1 ... k 31 ... 3m 21 ... 2n
1 ... p 21 ... 2q 31 ... 3r
unsatisfiable. Theorem 1, one following must hold:
(a) exists u v u v
(b) exists u u 1 ... n 1 ... q |=
(c) exists u u 1 ... n 1 ... q |=
(a) holds, set = u since Ti |= u |= , (u ) = 0 (Ti ), var(u )
var(Ti ). (b) holds, must case
u 1 ... n |= 1 ... q
hence
3(u 1 ... n ) |= 31 ... 3q |=
set = 3(u 1 ... n ), since Ti |= 3(u 1 ... n ) |= , (3(u 1
... n )) (Ti ), var(3(u 1 ... n )) var(Ti ). Finally, (c) holds,
must case
1 ... n |= 1 ... q u
hence
2(1 ... n ) |= 31 ... 3q 2u |=
set = 2(1 ... n ), Ti |= 2(1 ... n ) |= , (2(1 ... n )) (Ti ),
var(2(1 ... n )) var(Ti ). Thus, shown every Ti ,
Ti |= |= var(i ) var(Ti ) (i ) (Ti ). 1 ... z
clause implied every Ti , hence , var(i ) var(Ti ) var()
(i ) maxi (Ti ) ().
let implicant , let formula Nnf (). know
NNF transformation equivalence-preserving, hence , straightforward
106

fiPrime Implicates Prime Implicants Modal Logic

show must clause respect D4. implicate ,
must clause var( ) var() = var() depth
() + 1 = () + 1 |= |= . Let Nnf ( ). easily verified
term. Moreover, properties NNF transformation, ,
var( ) = var( ) = var( ), ( ) = ( ) = ( ). term
var( ) var(), ( ) () + 1, |= |= .
Lemma 9.3 notions prime implicates prime implicants induced D4 satisfy
Finiteness.
Proof. Consider arbitrary formula . Lemma 9.2, know prime
implicate , must implicate containing propositional
atoms appearing ( ) () + 1 |= . since prime
implicate, must also |= hence . Thus, every prime implicate
equivalent clause built finite set propositional symbols
depth () + 1. finitely many non-equivalent formulae
finite alphabet fixed depth, follows finitely many
distinct prime implicates. Lemma 9.1, every prime implicant equivalent
negation prime implicate . follows every formula
finitely many distinct prime implicants.
Lemma 9.4 notions prime implicates prime implicants induced D4 satisfy
Covering.
Proof. Let arbitrary formula. Lemma 9.2, know every implicate
entailed implicate whose propositional variables contained var()
whose depth () + 1. consider following set
= { | |= , clause, var() var(), () () + 1}
define another set follows:
= { | 6 . |= 6|= }
words, set logically strongest implicates depth
() + 1 built propositional letters . claim following:
(1) every prime implicate
(2) every implicate , |=
begin proving (1). Suppose (1) hold, is,
prime implicate . Since definition implicate , follows
must implicate |= 6|= . Lemma 9.2,
implicate ( ) () + 1, var( ) var(), |= .
means element implies implied , contradicting
assumption . thus conclude every element must
prime implicate .
107

fiBienvenu

(2): let implicate . Lemma 9.2, exists clause
|= . , done. Otherwise, must exist
|= 6|= . , done, otherwise, find another stronger
member . finitely many elements modulo equivalence, finite number
steps, find element implies . Since
seen members prime implicates , follows every implicate
implied prime implicate .
second part Covering, let implicant , let clause
equivalent (there must one D4 satisfies P4). since |= , must
also |= . According shown, must prime
implicate |= |= . Lemma 9.1, must equivalent
negation prime implicant . since |= ,
follows |= , completing proof.
Lemma 9.5 notions prime implicates prime implicants induced D4 satisfy
Equivalence.
Proof. Let formula K, suppose model every prime implicate
. D4 known satisfy property P5 (by Theorem 6), find conjunction
clauses equivalent . Covering (Lemma 9.3), clauses implied
prime implicate , must model clauses. follows
model . direction, simply note definition
prime implicates model , must also model every prime implicate
. thus shown model model every prime
implicate . Using similar argument, show model
model prime implicant .
Lemma 9.6 notions prime implicates prime implicants induced D4 satisfy
Distribution.
Proof. Let prime implicate 1 ... n . , must |= .
Covering, know must exist prime implicate
|= . means formula 1 ... n (which clause
disjunction clauses) entails . since prime implicate, must also case
|= 1 ... n , hence 1 ... n . proof prime implicants entirely
similar.
Theorem 9 notions prime implicates prime implicants induced definition
D4 satisfy Finiteness, Covering, Equivalence, Implicant-Implicate Duality,
Distribution.
Proof. Follows directly Lemmas 9.1-9.6.
Theorem 10 notions prime implicates prime implicants induced definitions
D1 D2 satisfy Equivalence.
108

fiPrime Implicates Prime Implicants Modal Logic

Proof. proof definitions. Suppose Equivalence holds.
every formula , set prime implicates equivalent . means
set {} inconsistent, hence compactness K (cf. Blackburn et al.,
2001, p. 86) finite subset {} inconsistent. 6 ,
know set must contain set prime implicates
cannot inconsistent. conjunction elements \ {} conjunction
clauses equivalent . follows every formula equivalent
conjunction clauses. shown earlier proof Theorem 5
formulae equivalent conjunction clauses respect D1 D2,
follows Equivalence cannot hold definitions.
Theorem 11 notions prime implicates prime implicants induced definitions
D3a, D3b, D5 satisfy Finiteness.
Proof. Suppose clauses defined respect definition D3a, D3b, D5 (the
proof three definitions). Consider formula = 2(a b). follows
Theorem 3 implies k = 2(3k a) 3(a b 2k a) every k 1.
formulae k clauses (with respect D3a, D3b, D5), k implicates
. complete proof, show every k prime implicate . Since k
mutually non-equivalent (because 2p 6|= 2q whenever p 6= q), follows
infinitely many prime implicates modulo equivalence.
Consider k implicate = 31 ... 3m 21 ... 2n
implies (by Theorem 2 cannot propositional literals ). Using Theorem 3
fact |= |= k , get following:
(a) b |= ...
(b) |= (3k a) (a b 2k a) every
(c) 1 ... |= b 2k
Let b |= ... . remark must satisfiable since
otherwise combine (a) (c) get b |= b 2k a. (b), know
|= (3k a) (a b 2k a) hence (2k a) (a b 3ka) inconsistent.
follows (2k a) (2k a) b inconsistent. Using Theorem 1,
find either |= 3k |= b. satisfiable clause respect
definitions D3a, D3b, D5, cannot imply b, must |= 3k a.
putting (a) (c) together, find
b |= 1 ... |= b 2k
follows |= 2k a, i.e. 3k |= . thus 3k 1 ...
b 2k a. 3k |= b 2k |= 1 ... , Theorem 3 get
2(3k a) 3(a b 2k a) |= 2i 3i ... 3m |= hence k . thus
shown implicate implies k must equivalent k . means
k prime implicate , completing proof.
109

fiBienvenu

Lemmas 12, 13, 14 follow easily known properties disjunctive normal
form transformation propositional logic (cf. Bienvenu, 2009, ch. 2).
proof Theorem 16, make use following lemmas:
Lemma 16.1 algorithm GenPI always terminates.
Proof. know Lemma 12 algorithm Dnf -4 always terminates returns
finite set formulae. means finitely many terms consider.
, set (T ) contains finitely many elements (this immediate given
definition (T )), means set Candidates also finite cardinality.
final step, compare pair elements Candidates.
comparison always terminates, finitely many pairs check, follows
algorithm GenPI terminates.
Lemma 16.2 algorithm GenPI outputs exactly set prime implicates input
formula.
Proof. first prove every prime implicate satisfiable term equivalent
element (T ). Let = 1 ... k 31 ... 3m 21 ... 2n
satisfiable term, let = 1 ... p 31 ... 3q 21 ... 2r one
prime implicates. restrict attention interesting case
non-tautologous. |= , must case
1 ... k 31 ... 3m 21 ... 2n
1 ... p 21 ... 2q 31 ... 3r
unsatisfiable. Theorem 1, one following must hold:
(a) exists u v u v
(b) exists u u 1 ... n |= 1 ... q
(c) exists u 1 ... n |= u 1 ... q
(a) holds, u |= , must equivalent u else would found
stronger implicate, contradicting assumption prime implicate .
result holds since u (T ). (b) holds, formula 3(u 1 ... n )
implicate implies , 3(u 1 ... n ). done since
3(u 1 ... r ) member (T ). Finally consider case (c) holds.
case, 2(1 ... n ) implicate implies , equivalent (as
prime implicate). desired result since 2(1 ... n ) one
elements (T ). Thus conclude every prime implicate term
equivalent element (T ). Lemma 13, elements Dnf -4() terms,
disjunction equivalent . D4 satisfies Distribution, follows every
prime implicate input equivalent element Candidates. means
element Candidates prime implicate , prime
implicate implies implied , hence j Candidates
j |= 6|= j . Thus, comparison phase, clause
removed Candidates. suppose clause prime implicate .
110

fiPrime Implicates Prime Implicants Modal Logic

know must Candidates , moreover,
choose j j < j |= . final step
compare clauses j j 6= i, never find j |= j < i,
j |= 6|= j j > i, otherwise would prime implicate. follows
remains set Candidates returned algorithm.
thus shown set formulae output GenPI input precisely set
prime implicates .
Theorem 16 algorithm GenPI always terminates outputs exactly set prime
implicates input formula.
Proof. Follows directly Lemmas 16.1 16.2.
Theorem 17 length smallest clausal representation prime implicate
formula single exponential length formula.
Proof. Prime implicates generated GenPI 2|| disjuncts
2|| terms Dnf -4() Lemma 14. Moreover, disjunct length
2|| (also Lemma 14). gives us total 2|| 2|| symbols, must
add 2|| 1 disjunction symbols connecting disjuncts. thus find
length smallest representation prime implicate formula
2|| 2|| + (2|| 1).
Theorem 18 length smallest clausal representation prime implicate
formula exponential length formula.
Proof. Consider formula
=

n
^

(2ai,1 2ai,2 )

i=1

clause
=

_

(j1 ,...,jn

2(a1,j1 a2,j2 ... an,jn )

){1,2}n

ak,l 6= am,p whenever k 6= l 6= p. difficult see
equivalent, means must prime implicate . remains
shown clause equivalent must length least ||. yields result
since clearly size exponential n, whereas length linear n.
Let shortest clause equivalent . equivalent , follows
Theorem 2 disjunction 2-literals inconsistent 3-literals.
since assumed shortest representation , cannot contain inconsistent
3-literals redundant 2-literals, since could remove find equivalent
shorter clause. must form 21 ... 2m , l 6|= k whenever l 6= k.
since |= , every disjunct 2p must also imply . disjunction 2-literals,
follows Theorem 3 every disjunct 2p implies disjunct 2q .
means every 2p must length least 2n + 1, since p satisfiable
formula implies conjunction n distinct propositional variables. also know
every disjunct 2q implies disjunct 2p since |= . wish
111

fiBienvenu

show two disjuncts imply disjunct . Suppose
case, is, distinct disjuncts 21 22 disjunct 2p
21 |= 2p 22 |= 2p . since 21 22 distinct disjuncts,
must 21 |= ai,1 22 |= ai,2 21 |= ai,2 22 |= ai,1 .
know 2p |= 2q q , every q implies either ai1 ai2 , either
2p |= 2ai1 2p |= 2ai2 . know 2q imply either 2ai,1 2ai,2
both, one 21 22 must imply 2p . contradicts earlier
assumption 21 |= 2p 22 |= 2p , disjunct must imply distinct
disjunct . thus demonstrated contains many disjuncts .
already shown disjuncts shorter disjuncts ,
follows | | ||, hence | | = ||. conclude every clause equivalent
length least ||, completing proof.
Theorem 19, prove following clause
_
=
2q1 ...qn c
(q1 ,...,qn){3,2}n

prime implicate (with respect D1 D2) formula


=

( 23(b0 b1 ) 22(b0 b1 ) )

n
^

( 2i 3bi 2i 2bi )

i=2



n1
^

2i+1 ( (bi1 bi ) 2bi ) 2n+1 ( (bn1 bn ) c )

i=1

moreover shorter way represent .
proof Theorem 19 makes use following lemmas.
Lemma 19.1 Let l1 ... lm D1-clause implies q1 ...qn a, qi {2, 3}
propositional variable. l1 ... lm q1 ...qn a.
Proof. proof, make use fact every D1-clause satisfiable.
straightforwardly shown structural induction. base case propositional
literals, clearly satisfiable. induction step, consider D1-clause
proper sub-clauses satisfiable. three possibilities: either
form 2 3 satisfiable D1-clause, disjunction 1 2
satisfiable D1-clauses 1 2 . three cases, find must also satisfiable.
proof lemma induction n. n = 0, l1 ... lm |= a.
According Theorem 2, every disjunct l1 ... lm must either unsatisfiable
formula. shown previous paragraph every D1-clause satisfiable,
l1 ... lm a.
suppose result holds whenever n k, suppose l1 ... lm |=
q1 ...qk+1 a. every li , must li |= q1 ...qk+1 a, hence |= li q1 ...qk+1 a. Using
Theorem 1, arrive following four possibilities:
112

fiPrime Implicates Prime Implicants Modal Logic

(a) |= q1 ...qk+1
(b) li
(c) q1 = 3 li 3li li |= q2 ...qk+1
(d) q1 = 2 li 2li li |= q2 ...qk+1
eliminate case (a) since 6|= q1 ...qk+1 every string modalities q1 ...qk+1 .
also eliminate (b) since li must satisfiable D1-clauses. remark
(c) holds, according induction hypothesis, li 3q2 ...qk+1 a. Similarly,
(d) holds, li 2q2 ...qk+1 a. follows li equivalent q1 ...qk+1 a,
l1 ... lm q1 ...qk+1 a.
V
n , let = 2q (b b ) ( n
k
Lemma
19.2
Fix
(q
,
...,
q
)

{2,
3}
1
n
1
0
1
k=2 2 qk bk )
Vn1 k+1
( (bk1 bk ) 2bk ) 2n+1 ( (bn1 bn ) c ). |= 2r1 ...rn c
k=1 2
rk = qk 1 k n.
Proof. begin showing 1 n 1 formula
bi1 bi (

n
^

n1
^

2ki1 qk bk ) (

k=i+1

2ki ((bk1 bk ) 2 bk ) ) 2ni ((bn1 bn ) c)

k=i

entails formula ri+1 ...rn c case qi+1 ...qn = ri+1 ...rn .
proof induction i. base case = n 1.
bn2 bn1 qn bn ((bn2 bn1 ) 2bn1 ) 2((bn1 bn ) c) |= rn c

(1)


bn2 bn1 qn bn 2bn1 2((bn1 bn ) c) |= rn c
(Theorem 1) either
qn = 3 rn = 2 bn1 ((bn1 bn ) c) |= c

qn = rn bn1 bn ((bn1 bn ) c) |= c
bn1 ((bn1 bn ) c) 6|= c, cannot first alternative. follows
Equation (1) holds, second alternative must hold, case get qn = rn ,
desired. direction, simply note bn1 bn ((bn1 bn ) c) |= c
valid entailment, means qn = rn implies Equation (1).
Next let us suppose statement holds 1 < j n 1, let us
prove statement holds = j 1.
bj2 bj1 (

n
^

kj

2

qk bk ) (

k=j

n1
^

2kj+1 (bk1 bk 2bk ) )

k=j1

2nj+1 ((bn1 bn ) c)

|= rj ...rn c
113

(2)

fiBienvenu

one following holds:
(a) qj = 3 rj = 2
bj1 (

n
^

n1
^

2kj1 qk bk ) (

k=j+1

2kj ((bk1 bk ) 2bk ) ) 2nj ((bn1 bn ) c)

k=j

|= rj+1 ...rn c

(b) qj = rj
bj1 bj (

n
^

n1
^

2kj1 qk bk ) (

k=j+1

2kj ((bk1 bk ) 2bk )) 2nj ((bn1 bn ) c)

k=j

|= rj+1 ...rn c

first show entailment (a) hold. Consider model =
hW, R, vi defined follows:
W = {wj , ..., wn }
R = {(wj , wj+1 ), ..., (wn1 , wn )}
v(c, w) = f alse w W
w 6= wj : v(bk , w) = true w = wk
v(bk , wj ) = true k = j 1
Notice since world (excepting wn ) exactly one successor, 2- 3quantifiers behaviour (except wn ). easily verified M, wj
satisfies left-hand side aboveV
entailment tuple qj+1 ...qn : M, wj |=
bj1 definition, M, wj |= nk=j+1 2kj1 qk bk M, wk |= bk k 6= j,
Vn1 kj
M, wj |= k=j
2 ((bk1 bk ) 2bk ) ) since M, wj 6|= bj M, wk 6|= bk1
k 6= j, finally M, wj |= 2nj ((bn1 bn ) c) since wn 6|= bn1 . However,
right-hand side rj+1 ...rn c satisfied wj : world accessible wj n j
steps wn satisfy c.
shown case (a) cannot hold, means Equation (2) holds
(b) does. apply induction hypothesis entailment (b),
find holds case qj+1 ...qn = rj+1 ...rn . follows Equation (2)
qj ...qn = rj ...rn , desired. completes proof statement.
proceed proof lemma. Theorem 1,
2q1 (b0 b1 ) (

n
^

n1
^

2k qk bk ) (

k=2

2k+1 ( (bk1 bk ) 2bk ) 2n+1 ( (bn1 bn ) c )

k=1

|= 2r1 ...rn c

holds case
q1 (b0 b1 ) (

n
^

k=2

2k1 qk bk )

n1
^

2k ( (bk1 bk ) 2bk ) 2n ( (bn1 bn ) c )

k=1

|= r1 ...rn c
114

fiPrime Implicates Prime Implicants Modal Logic

turn holds one following statements holds:
(i) q1 = 3 r1 = 2
(

n
^

n1
^

2k2 qk bk ) (

k=2

2k1 ((bk1 bk ) 2bk ) ) 2n1 ((bn1 bn ) c) |= r2 ...rn c

k=1

(ii) q1 = r1
b0 b1 (

n
^

k=2

n1
^

2k2 qk bk ) (

2k1 ((bk1 bk ) 2bk ) ) 2n1 ((bn1 bn ) c)

k=1

|= r2 ...rn c

remark set j = 1 (a) above, left-hand side entailment (i)
logically weaker (a), right-hand side matches (a).
already shown entailment (a) hold, follows entailment
(i) cannot hold either. Thus, find desired entailment relation statement
lemma holds (ii) does. completes proof since already
shown induction entailment (ii) holds q2 ...qn = r2 ...rn ,
i.e. (ii) true case q1 ...qn = r1 = rn .
Lemma 19.3 D1-clause equivalent strictly smaller size .
Proof. Let D1-clause equivalent . Suppose furthermore
shortest clause. non-tautologous contains 2-literals disjuncts,
follows every disjunct must either unsatisfiable 2-literal (cf. Theorem 2).
D1-clauses always satisfiable (cf. proof Lemma 19.1), must contain
2-literals.
Since |= , every disjunct 2l must imply disjunct 2q1 ...qn c . Also,
every disjunct 2l must implied disjunct 2q1 ...qn c , since otherwise
could remove 2l preserving equivalence .
follows disjunct implied disjunct implies
disjunct . since disjuncts imply (because Lemma
19.1), follows disjunct equivalent disjunct , moreover
every disjunct equivalent disjunct .
completes proof since clear disjuncts 2q1 ...qn c cannot
compactly represented.
proof works equally well D2, since every D2-clause also D1-clause.
Theorem 19 prime implicates defined using either D1 D2, length
smallest clausal representation prime implicate formula exponential
length formula.
Proof. begin definition D1. Let defined page 112. begin
distributing order transform equivalent disjunction D4-terms:
_

Tq1 ,...,qn
(q1 ,...,qn){2,3}n

115

fiBienvenu

Tq1 ,...,qn equal
2q1 (b0 b1 ) (

n
^

2 q bi )

n1
^

2i+1 ( (bi1 bi ) 2bi ) 2n+1 ( (bn1 bn ) c )

i=1

i=2

Lemma 19.2, Tq1 ,...,qn |= 2q1 ...qn c, hence Tq1 ,...,qn |= . thus |= .
show stronger clause respect D1 implied
. Let D1-clause |= |= . non-tautologous disjunction
2-literals, know Lemma 2 every disjunct must form 2l
l D1-clause l |= r1 ...rn c quantifier string r1 ...rn . according
Lemma 19.1, l |= r1 ...rn c, l equivalent r1 ...rn c. follows equivalent
clause disjuncts forms 2r1 ...rn c.
|= , must case terms Tq1 ,...,qn implies , equivalently
Tq1 ,...,qn |= . shown disjuncts 2-literals,
follows Theorem 1 term implies disjunct . Moreover, know
preceding paragraph disjuncts equivalent formula
form 2r1 ...rn c. Lemma 19.2, formula type implied
Tq1 ,...,qn formula 2q1 ...qn c. means every tuple quantifiers (q1 , ..., qn ),
disjunct equivalent 2q1 ...qn c. follows every disjunct
equivalent disjunct , giving us |= . thus conclude
prime implicate .
completes proof, since already shown Lemma 19.3
shorter D1-clause equivalent itself.
proof also works definition D2 since every D2-clause also D1-clause.
particular means D2-clause prime implicate respect D1
also prime implicate respect D2, D2-clause shortest
among equivalent D1-clauses also shortest among D2-clauses.
Theorem 20 number non-equivalent prime implicates formula double
exponential length formula.
Proof. know Theorem 16 every prime implicateWof equivalent
clause returned GenPI. Every clause form Dnf -4()
(T ). 2|| terms Dnf -4() Lemma 14, clauses
2|| disjuncts. Moreover, 2|| choices disjunct
since cardinality (T ) bounded size , know Lemma
||
1.3 2||. follows (2||)2 clauses returned
||
GenPI, hence (2||)2 non-equivalent prime implicates .
Theorem 21 number non-equivalent prime implicates formula may double
exponential length formula.
Proof. Let n natural number, let a11 , a12 , ..., an1 , an2 , b11 , b12 , b12 , ..., bn1 ,
bn2 4n distinct propositional variables. Consider formula defined
n
^

((3ai1 2bi1 ) (3ai2 2bi2 ))

i=1

116

fiPrime Implicates Prime Implicants Modal Logic

hard see 2n terms Dnf -4(), corresponding 2n ways
deciding {1, ..., n} whether take first second disjunct. term
Dnf -4() form
n
^

(3ai f (i,T ) 2bi f (i,T ) )

i=1

f (i, ) {1, 2} i. , denote D(T ) set formulae {3(a f (i,T )
b1 f (1,T ) ... bn f (n,T ) )) | 1 n}. consider set clauses C defined
{

_

dT | dT D(T )}

Dnf -4()
n

Notice n2 clauses C since clause corresponds choice one
n elements D(T ) 2n terms Dnf -4(). number double
exponential || since length linear n. order complete proof,
show (i) clauses C prime implicates (ii) clauses C
mutually non-equivalent.
begin showing 1 6|= 2 every pair distinct elements 1 2 C.
immediately gives us (ii) prove useful proof (i). Let 1 2
distinct clauses C. 1 2 distinct, must term Dnf -4()
1 2 choose different elements D(T ). Let d1 element
D(T ) appearing disjunct 1 , let d2 element D(T ) disjunct
2 , let aj,k a-literal appears d2 (and hence d1 ). Consider
formula = 2(aj,k b1,k1 ... bn,kn ), tuple (k1 , ..., kn ) like tuple
associated except 1s 2s inversed. Clearly d1 consistent,
since variables appear d1 . inconsistent every disjunct
2 , since construction every disjunct 2 contains literal whose negation appears
. follows 2 |= 1 6|= , hence 1 6|= 2 .
prove (i). Let clause C, let prime implicate
implies . Theorem 16, know must equivalent one clauses output
GenPI, specifically clause output GenPI disjunction
3-literals (because Theorem 2). remark set C composed exactly
candidate clauses disjunctions 3-literals, must equivalent
clause C. shown element C implies itself.
follows , means prime implicate .
Theorem 22 prime implicates defined using either D1 D2, number
non-equivalent prime implicates formula may double exponential length
formula.
Proof. Let defined page 112. Set equal formula obtained
replacing c last conjunct c d. Set equal set clauses
obtained replacing zero occurrences c d. example, n = 1,
n
= {23c 22c, 23d 22c, 23c 22d, 23d 22d}. 22 elements
since choose 2n disjuncts whether change c d. intend
117

fiBienvenu

show clauses pairwise non-equivalent prime implicates .
proof every element indeed prime implicate (with respect D1
D2) proceeds quite similarly proof prime implicate (see proof
Theorem 19), repeat here. Instead show elements
pairwise non-equivalent. so, consider two distinct elements
. Since distinct, must string quantifiers q1 ...qn
disjunct 2q1 ...qn ( {c, d}) disjunct . |= ,
would 2q1 ...qn |= , hence 2q1 ...qn |= 2r1 ...rn disjunct r1 ...rn .
using Lemma 19.1, see happen r1 ...rn = q1 ...qn = ,
i.e. 2q1 ...qn disjunct . contradiction, must 6|= . follows
elements pairwise non-equivalent, hence possesses double
exponential number prime implicates.
Theorem 23 exists algorithm runs single-exponential space size
input incrementally outputs, without duplicates, set prime implicates
input formula.
Proof. Let sets Candidates function defined Figure 3.
assume ordered: = {T1 , ..., Tn }. Ti , let max denote
number elements (Ti ), assume ordering elements (Ti ):
(Ti ) = {i,1 , ..., i,max }. Notice tuples {1, .., max 1 } ... {1, ..., max n }
ordered using standard lexicographic ordering <lex : (a1 , ..., ) <lex (b1 , ..., bn )
1 j n aj < bj ak bk 1 k j 1.
set maxindex = ni=1 maxi , let f : {1, .., max 1 } ... {1, ..., max n } {1, ..., maxindex }
bijection defined follows: f (a1 , ..., ) = (a1 , ..., ) m-th
tuple lexicographic ordering {1, .., max 1 } ... {1, ..., max n }. denote
unique clause form 1,a1 ... n,an f (a1 , ..., ) = m. remark
given index {1, ..., maxindex } sets (T1 ), ..., (Tn ), possible
generate polynomial space (in size sets (T1 ), ..., (Tn )) clause .
make use fact modified version algorithm GenPI, defined follows:
Function IterGenPI()
(1) GenPI.
(2) GenPI.
(3) = 1 maxindex : j 6|= j < either j 6|= |= j
every < j maxindex , output .
proofs termination, correctness, completeness IterGenPI similar corresponding results GenPI (Theorem 16), omit details.
instead focus spatial complexity IterGenPI. first step IterGenPI
clearly runs single-exponential space ||, since deciding satisfiability takes
polynomial space ||, generating elements Dnf -4() takes
single-exponential space || (refer Lemma 14). Step 2 also uses singleexponential space ||, since sets (T ) associated term Ti
polynomial size Ti . Finally, Step 3, use observation generation given index done polynomial space size sets
118

fiPrime Implicates Prime Implicants Modal Logic

(T1 ), ..., (Tn ), hence single-exponential space ||. sufficient since
comparisons Step 3, need keep two candidate clauses memory
one time, deciding whether one candidate clause entails another accomplished
single-exponential space (since clauses single-exponential size ||).
Theorem 24 Prime implicate recognition Pspace-hard.
Proof. reduction simple: formula unsatisfiable 3(a a)
prime implicate . suffices problem checking unsatisfiability formulae
K known Pspace-complete.
need following two lemmas Theorem 25:
Lemma 25.1 Let formula K, let = 1 ... k 31 ... 3m
21 ... 2n (j propositional literals) non-tautologous clause. Suppose furthermore
literal l \ {l}. (), 1 ... k
( ( \ {1 , ..., k })) 3(1 ... n ) ( ( \ {31 , ..., 3m })) every
i, 2(i 1 ... ) ( ( \ {2i })).
Proof. prove contrapositive: 1 ... k 6 ( ( \ {1 , ..., k }))
3(1 ... n ) 6 ( ( \ {31 , ..., 3m })) 2(i 1
... ) 6 ( ( \ {2i })), 6 (). consider case
|= 6|= immediately get 6 ().
Let us first suppose 1 ...k 6 ((\{1 , ..., k })). Since |= , must also
(\{1 , ..., k }) |= 1 ...k , 1 ...k implicate (\{1 , ..., k }).
1 ... k known prime implicate ( \ {1 , ..., k }), follows
must clause ( \ {1 , ..., k }) |= |= 1 ... k 6|= .
consider clause = 31 ... 3m 21 ... 2n . know |= since
( \ {1 , ..., k }) |= , |= |= 1 ... k . also 6|=
since must equivalent propositional clause (by Theorem 2) propositional
part (namely 1 ... k ) imply . follows |= |= 6|= ,
6 ().
Next suppose 3(1 ... n ) 6 ( ( \ {31 , ..., 3m })). 3(1 ... n )
must implicate ( \ {31 , ..., 3m }) since assumed |= .
3(1 ... n ) prime implicate ( \ {31 , ..., 3m }), follows
( \ {31 , ..., 3m }) |= |= 3(1 ... n ) 6|= . Let
= 1 ...k 21 ...2n . Theorem 2, know disjunction
3-literals, according Theorem 3 must 6|= since 3(1 ... n ) 6|= .
also know |= since ( \ {31 , ..., 3m }) |= |= since
|= 3(1 ... n ). means |= |= 6|= , 6 ().
Finally consider case 2(i 1 ... ) 6
( ( \ {2i })). know |= hence ( \ {2i }) |= 2i .
Moreover, since ( \ {2i }) |= 3j j, ( \ {2i }) |= 2(i
1 ... ). Thus, 2(i 1 ... ) 6 ( ( \ {2i })), must mean
( \ {2i }) |= |= 2(i 1 ... ) 6|= .
assumption, tautology, 2(i 1 ... ) cannot tautology
119

fiBienvenu

either. |= 2(i 1 ... ) 2(i 1 ... ) tautology,
follows Theorem 2 equivalent formula 21 ... 2p . Let
= 1 ... k 31 ... 3m 21 ... 2i1 (21 ... 2p ) 2i+1 ... 2n .
( \ {2i }) |= 21 ... 2p , must case |= . Also, know
j |= j 1 ... otherwise would
1 ... |= j hence 2(i 1 ... ) |= 21 ... 2p . Similarly,
k 6= 2i |= 2(k 1 ... ) would mean
\ {2i }, contradicting assumption superfluous disjuncts
. follows Theorem 3 6|= . Thus, |= |= 6|= , means
6 ().
Lemma 25.2 Let formula K, let = 1 ...k 31 ...3m 21 ...2n
(j propositional literals) non-tautologous clause. Suppose furthermore
literal l \ {l}. 6 (), either 1 ... k 6 ( ( \
{1 , ..., k })) 3(1 ... ) 6 ( (1 ... k 2(1 1 ... ) ... 2(n
1 ... ))) 2(i 1 ... ) 6 ( ( \ {2i })) i.
Proof. consider case |= 6|= immediately
get result. Suppose 6 () |= . Definition 7, must
= 1 ...o 31 ... 3p 21 ... 2q |= |= 6|= . Since 6|= ,
Proposition 3 know either 1 ... k 6|= 1 ... 1 ... 6|= 1 ... p
6|= j 1 ... p j.
begin case 1 ... k 6|= 1 ... . |= , Theorem 3,
1 ...p |= 1 ...m every j |= 1 ...m j .
follows (also Theorem 3) |= |= 1 ...o 31 ...3m 21 ...2n ,
hence (\{1 , ..., k }) |= 1 ...o . 1 ...o |= 1 ...k 6|= 1 ...o ,
found implicate ( \ {1 , ..., k }) stronger 1 ... k ,
1 ... k 6 ( ( \ {1 , ..., k })).
Next suppose 1 ...m 6|= 1 ...p . |= , follows Theorem 3

1 ...o |= 1 ...k every j |= 1 ...m j .
thereby obtain |= |= 1 ... k 31 ... 3p 2(1 1 ... ) ...
2(n 1 ... ). this, infer (1 ... k 2(1 1 ...
) ... 2(n 1 ... )) |= 31 ... 3p |= 31 ... 3m 6|= 31 ... 3p .
31 ... 3m 3(1 ... ), follows 3(1 ... ) 6 ( (1 ...
k 2(1 1 ... ) ... 2(n 1 ... ))).
Finally suppose 6|= j 1 ... p j furthermore 1 ... |=

1 ... p (we already shown result holds 1 ... 6|= 1 ...
p ). 2(i 1 ... ) implicate ( \ {2i })) show
2(i 1 ... ) prime implicate ( \ {2i })), must find
stronger implicate. Consider set = {s {1, ..., q} : |= 1 ... 6|=
k 1 ...m k 6= i}. note must least one element
assumed 6|= \ {2i }. since 1 ... |= 1 ... k , 1 ... p |= 1 ... m,
... , |= S,
every 6 r 6= sW|= r 1 W

get |= W
|= 1 ...k 31 ...3m ( j6=i 2j )( W
sS 2s ). follows (\

{2i }) |= sS 2(s 1 ...m ), means sS 2(s 1 ...m )
120

fiPrime Implicates Prime Implicants Modal Logic

W
implicate (\{2i }). Moreover, sS 2(s 1 ...m ) |= 2(i 1 ...m )
since construction |= 1 ... every S.
W
remains shown 2(i 1 ... ) 6|= sS 2(s 1 ... ).
Suppose
contradiction contrary holds. 2(i 1 ... ) |=
W

sS 2(s 1 ... ), Theorem 1, must
1 ... |= 1 ... . |= 1 ... , thus
|= 1 ... p since assumed 1 ... |= 1 ... p . contradicts
earlier assumption W
6|= j 1 ... p j. Thus, shown
2(i 1 ... ) 6|= sS 2(s 1 ... ), 2(i 1 ... ) 6
( ( \ {2i })).
Theorem 25 Let formula K, let = 1 ...k 31 ...3n 21 ...2m
(j propositional literals) non-tautologous clause (a) 1 ... n
i, (b) literal l \ {l}. ()
following conditions hold:
1. 1 ... k ( ( \ {1 , ..., k }))
2. 2(i 1 ... n ) ( ( \ {2i })) every
3. 3(1 ... n ) ( ( \ {31 , ..., 3n }))
Proof. forward direction shown Lemma 25.1. direction follows
Lemma 25.2 together hypothesis 1 ... n (which
ensures (1 ... k 2(1 1 ... ) ... 2(n 1 ... ))
( \ {31 , ..., 3n })).
Theorem 26 Let formula K, let non-tautologous propositional clause
|= literal l \ {l}. ()
6|= \ {l} l .
Proof. Consider formula non-tautologous propositional clause |=
literal l \ {l}. Suppose |= \ {l}
l . know 6 \ {l}, follows \ {l} implicate
strictly stronger , prime implicate .
direction, suppose 6 (). must case clause
|= |= 6|= . Since |= , follows Theorem 2 literal
propositional literal inconsistent. literals inconsistent,
must inconsistent, clearly |= \ {l} every l . Otherwise,
equivalent propositional clause, specifically propositional clause
containing literals appearing (since |= ). strictly stronger ,
must literal l appear . means |= \ {l}
|= \ {l}, completing proof.
Theorem 27 Let formula K, let = 2 non-tautologous clause
|= . () exists term Dnf-4()
|= , conjunction formulae 2 .
121

fiBienvenu

Proof. Let formula, let = 2 non-tautologous clause |= .
first direction, suppose term Dnf -4() |= ,
conjunction formulae 2 . two cases: either
terms Dnf -4() unsatisfiable, terms none satisfy
condition. first case, 2 prime implicate , since contradictory
clause
W
(e.g. 3(a a)) stronger. second case, consider clause = 2T ,
conjunction formulae 2 . every must
W
2T |= 2, otherwise would 6|= 2, hence
W 6|= 2. Moreover, |= 2T
since |= 2T every . Theorem 1, 2 6|= 2T since 6|= .
|= |= 6|= , means prime implicate .
direction, suppose 2 prime implicate 6|= .

must |= 2 Dnf -4(),
W Dnf -4() non-empty. |= 2, W
2T also implies 2. show
. let
W 2T primeWimplicate W
implicate implies 2T . since |= 2T 2T
non-tautologous, follows Theorem 2 21 ... 2n formulae .
|= , must |= 21 ... 2n W
Dnf -4().
Wbe
case 2T |= 21 ... 2n , W
means 2T |= 21 ... 2n . 2T
implies every implicate W
implies it, 2T must prime implicate .
means 2 6|= 2T , since assumed 2 prime implicate
. follows Theorem 1 6|= Dnf -4().
order show Theorem 28 need following lemmas:
Lemma 28.1 3 implicate prime implicate, algorithm
Test3PI returns input (3, ).
Proof. Suppose 3 prime implicate . unsatisfiable, must
satisfiable, return first step. satisfiable, since
assumed 3 implicate , must clause |= |= 3
3 6|= . |= 3, follows Theorem 2 equivalent disjunction
3-formulae, hence clause 3 .
know Lemma 13 equivalent disjunction terms Dnf -4().
must thus case Ti |= 3 Ti Dnf -4(). Since Ti satisfiable
conjunction propositional literals 2- 3-formulae, follows exists set
{3i , 2i,1 , ..., 2i,k(i) } conjuncts Ti 3(i i,1 ...i,k(i) ) |= 3 , otherwise
Ti would fail imply 3 . Moreover, elements {3i , 2i,1 , ..., 2i,k(i) } must
appear NNF outside modal operators, formulae , i,1 , ..., i,k(i) must
elements set X . immediate
3

_
(i i,1 ... i,k(i) ) |= 3 |= 3



3 6|= 3

_

(i i,1 ... i,k(i) )



122

(3)

fiPrime Implicates Prime Implicants Modal Logic

W
latter implies formula 3 (3 (i i,1 ... i,k(i) )) must consistent,
means
_
^
( (i i,1 ... i,k(i) )) (i i,1 ... i,k(i) )




must consistent well. itVmust case select
{i , i,1 , ..., i,k(i) } consistent. Let set . set
satisfies condition algorithm since:
SX
6|=

W



(because know

V



consistent)

Ti Dnf -4(), conjuncts 3i , 2i,1 , ..., 2i,k(i) Ti that:
{i , i,1 , ..., i,k(i) } 6= (since contains {i , i,1 , ..., i,k(i) })
3(i i,1 ... i,k(i) ) |= 3 (follows (3) above)
Since exists set X satisfying conditions, algorithm returns no.
Lemma 28.2 algorithm Test3PI returns input (3, ), 3
prime implicate .
Proof. Suppose Test3PI returns input (3, ). happens first
step, must case unsatisfiable 3 unsatisfiable, case 3
prime implicate . possibility algorithm returns Step 3,
means W
must X satisfying:
(a) 6|=
(b) Ti Dnf -4(), exist conjuncts 3i , 2i,1 , ..., 2i,k(i) Ti
that:
(i) {i , i,1 , ..., i,k(i) } 6=
(ii) 3(i i,1 ... i,k(i) ) |= 3
W
Let clause 3(i i,1 W
... i,k(i)
W ). remark Ti , Ti |=
3(i i,1 ...i,k(i) ), hence

|=


3(i i,1 ...i,k(i) ).
W
W definition
Dnf -4(), also Ti . immediately follows |= 3(i i,1 ...i,k(i) )
hence W
|= . 2 (b) (ii), 3(i i,1 ... i,k(i) ) |= 3 every i,
hence 3(i i,1 ... i,k(i)) |= 3 yields |= 3. 2 (b) (i),
{i , i,1 , ..., i,k(i) } 6= hence every
W

W
i,1 ... i,k(i)W|= . infer 3(i i,1 ... i,k(i) ) |= W
3,
hence |= 3 . know 2 (a) Theorem 1 3 6|= 3 .
follows 3 6|= . Putting together, see exists clause
|= |= 3 3 6|= , hence 3 prime implicate .
Theorem 28 Let formula, let 3 implicate . algorithm
Test3PI returns yes input (3, ) 3 prime implicate .
123

fiBienvenu

Proof. clear Test3PI terminates since unsatisfiability testing NNF transformation always terminate, finitely many Ti . Lemmas 28.1
28.2 show us algorithm always gives correct response.
Theorem 29 algorithm Test3PI runs polynomial space.
Proof. remark sum lengths elements X bounded
length formula Nnf(), hence Lemma 14 sum theW lengths
elements particular X cannot exceed 2||. Testing whether 6|= thus
accomplished polynomial V
space length involves testing
satisfiability formula whose length clearly polynomial .
let us turn Step 3 (b). notice necessary keep Ti
memory once, since generate terms Ti one time using polynomial
space Lemma 12. Lemma 14, length Ti Dnf -4() 2||.
follows checking whether {i , i,1 , ..., i,k(i) } 6= , whether 3(i i,1 ...
i,k(i) ) |= 3 accomplished polynomial space length .
conclude algorithm Test3PI runs polynomial space.
order show Theorem 32, use following lemmas:
Lemma 32.1 clause prime implicate , TestPI outputs
input.
Proof. Let us begin considering formula clause prime
implicate . two possible reasons this: either implicate ,
implicate exists stronger implicate. first case, TestPI returns
Step 1, desired. focus case implicate
prime implicate. begin treating limit cases one
tautology contradiction. Given know non-prime implicate ,
two possible scenarios: either 6|= |= , |= 6|= . cases,
algorithm returns Step 2.
implicate , neither tautology contradiction,
algorithm continue Step 3. step, redundant literals deleted
, contains 3-literals, add extra disjunct 2-literals
satisfies syntactic requirements Theorem 25. Let 1 ...k 31 ... 3m 21
... 2n clause end Step 3 modifications made.
transformations Step 3 equivalence-preserving (Theorem 1), modified
equivalent original, still non-tautologous non-prime implicate .
means satisfy conditions Theorem 25. follows one
following holds:
(a) 1 ... k 6 ( ( \ {1 , ..., k })
(b) 2(i 1 ... n ) 6 ( ( \ {2i }))
(c) 3(1 ... n ) 6 ( ( \ {31 , ..., 3n }))
124

fiPrime Implicates Prime Implicants Modal Logic

Suppose (a) holds. 1 ... k non-tautologous propositional clause implied
( \ {1 , ..., k }) contains redundant literals. means ( \
{1 , ..., k }) 1 ...k satisfy conditions Theorem 26. According theorem,
1 ... k 6 ( ( \ {1 , ..., k }), must j ( \
{1 , ..., k }) |= 1 ... j1 j+1 ... k . means |= \ {j }, algorithm
returns Step 4.
Suppose next (b) holds, let 2(i 1 ... n ) 6 (
( \ {2i })). Theorem 27, means Dnf -4()
2(i 1 ... n ) entails conjunction 2-formulae conjuncts . follows
algorithm returns Step 5.
Finally consider case
Step 6,
Wm neither (a) (b) holds (c)Wdoes.

call Test3PI(3( i=1 ), ( \ {31 , ..., 3m })). 3( i=1 ) prime
implicate (\{31 , ..., 3m })) shown Test3PI correct (Theorem
28), Test3PI return no, TestPI return well. covered
possible cases, conclude clause prime implicate ,
TestPI outputs no.
Lemma 32.2 TestPI outputs input (, ) clause,
prime implicate .
Proof. 5 different ways TestPI return (these occur Steps 1, 2, 4, 5,
6). Let us consider turn. first way algorithm return
Step 1 find 6|= . correct since cannot prime implicate
consequence . Step 2, return unsatisfiable not,
tautology not. also correct since cases cannot prime
implicate since exist stronger implicates (any contradictory clause ,
non-tautologous implicate ). Step 3, may modify , resulting
formula equivalent original, prime implicate case
original clause was. Let 1 ...k 31 ... 3m 21 ... 2n clause end
Step 3. Step 4, return find propositional literal l
|= \{l}. since Step 3, removed redundant literals , sure
\ {l} strictly stronger . |= \ {l} |= 6|= \ {l},
means prime implicate . consider Step 5 TestPI. step,
return disjunct 2i term Dnf -4((\{2i }))
2(i 1 ... ) entails conjunction 2-literals . According Theorem 27,
means 2(i 1 ... ) prime implicate ( \ {2i }),
means prime implicate
W Theorem 25. . step, return
Test3PI returns input (3( ki=1 ), ( \ {31 , ..., 3m })). Theorem 28,
W
know happens case 3( ki=1 ) prime implicate
( \ {31 , ..., 3m }). follows Theorem 25 prime implicate
.
Theorem 32 algorithm TestPI always terminates, returns yes input (,
) prime implicate .
125

fiBienvenu

Proof. algorithm TestPI clearly terminates Steps 1 5 involve finite number
syntactic operations finite number entailment checks. Moreover, call
Test3PI Step 6 known terminate (Theorem 28). Correctness completeness
already shown Lemmas 32.1 32.2.
make use following lemma proof Theorem 34:
Lemma 34.1 algorithm TestPI provided Figure 5 runs polynomial space
length input.
Proof. clear steps 1 5 carried polynomial space length
input, since simply involve testing satisfiability formulae whose lengths
polynomial ||+||. Step 6 also
W carried polynomial space since Theorem
29 deciding whether formula 3(W
i=1 ) prime implicate ( \ {1 , ..., }))
takes polynomial space |3(
i=1 )| + | ( \ {31 , ..., 3m }))|, hence
|| + ||. thus conclude algorithm TestPI runs polynomial space
length input.
Theorem 34 Prime implicate recognition Pspace.
Proof. show Theorem 32 TestPI always terminates returns yes whenever clause prime implicate otherwise. means TestPI decision
procedure prime implicate recognition. Since algorithm shown run
polynomial space (Lemma 34.1), conclude prime implicate recognition
Pspace.
Corollary 35 Prime implicate recognition Pspace-complete.
Proof. Follows directly Theorems 24 34.

References
Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2006). Distributed
reasoning peer-to-peer setting: Application semantic web. Journal
Artificial Intelligence Research, 25, 269314.
Baader, F., McGuiness, D. L., Nardi, D., & Patel-Schneider, P. (Eds.). (2003). Description Logic Handbook. Cambridge University Press.
Bienvenu, M. (2007). Prime implicates prime implicants modal logic. Proceedings
Twenty-Second Conference Artificial Intelligence (AAAI07), pp. 397384.
Bienvenu, M. (2008). Prime implicate normal form ALC concepts. Proceedings
Twenty-Third Conference Artificial Intelligence (AAAI08), pp. 412417.
Bienvenu, M. (2009). Consequence Finding Modal Logic. Ph.D. thesis, Universite de
Toulouse.
Bittencourt, G. (2007). Combining syntax semantics prime form representation. Journal Logic Computation, 18 (1), 1333.
126

fiPrime Implicates Prime Implicants Modal Logic

Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal logic. Cambridge University
Press.
Blackburn, P., van Benthem, J., & Wolter, F. (Eds.). (2006). Handbook Modal Logic.
Elsevier.
Brandt, S., & Turhan, A. (2002). approach optimized approximation. Proceedings
KI-2002 Workshop Applications Description Logics (KIDLWS01).
Cadoli, M., & Donini, F. M. (1997). survey knowledge compilation. AI Communications, 10 (3-4), 137150.
Cialdea Mayer, M., & Pirri, F. (1995). Propositional abduction modal logic. Logic
Journal IGPL, 3 (6), 907919.
Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial
Intelligence Research, 17, 229264.
de Kleer, J., Mackworth, A. K., & Reiter, R. (1992). Characterizing diagnoses systems.
Artificial Intelligence, 56, 197222.
Donini, F. M. (2003). Description Logic Handbook, chap. Complexity Reasoning.
Cambridge University Press.
Donini, F. M., Lenzerini, M., Nardi, D., Hollunder, B., Nutt, W., & Marchetti Spaccamela,
A. (1992). complexity existential qualification concept languages. Artificial
Intelligence, 53, 309327.
Eiter, T., & Makino, K. (2002). computing abductive explanations. Proceedings
Eighteenth National Conference Artificial Intelligence (AAAI02), pp. 6267.
Enjalbert, P., & Farinas del Cerro, L. (1989). Modal resolution clausal form. Theoretical
Computer Science, 65 (1), 133.
Garey, M. R., & Johnson, D. S. (1979). Computers intractability. guide theory
NP-completeness. W. H. Freeman.
Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. Proceedings Tenth International
Conference Principles Knowledge Representation Reasoning (KR06), pp.
187197.
Giunchiglia, F., & Sebastiani, R. (1996). SAT-based decision procedure ALC.
Proceedings Fifth International Conference Principles Knowledge Representation Reasoning (KR96), pp. 304314.
Kusters, R., & Molitor, R. (2002). Approximating specific concepts logics
existential restrictions. AI Communications, 15 (1), 4759.
Ladner, R. (1977). computational complexity provability systems modal propositional logic. SIAM Journal Computing, 6 (3), 467480.
Lakemeyer, G. (1995). logical account relevance. Proceedings Fourteenth
International Joint Conference Artificial Intelligence (IJCAI95), pp. 853861.
127

fiBienvenu

Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence: Formulavariable independence forgetting. Journal Artificial Intelligence Research, 18,
391443.
Marquis, P. (1991a). Contribution letude des methodes de construction dhypotheses en
intelligence artificielle. french, Universite de Nancy I.
Marquis, P. (1991b). Extending abduction propositional first-order logic. Proceedings Fundamentals Artificial Intelligence Research Workshop, pp. 141155.
Marquis, P. (2000). Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5, chap. Consequence Finding Algorithms, pp. 41145. Kluwer.
Pagnucco, M. (2006). Knowledge compilation belief change. Proceedings
Nineteenth Australian Conference Artificial Intelligence (AI06), pp. 9099.
Papadimitriou, C. (1994). Computational Complexity. Addison Welsey.
Przymusinski, T. (1989). algorithm compute circumscription. Artificial Intelligence,
38 (1), 4973.
Ramesh, A., & Murray, N. (1994). Computing prime implicants/implicates regular logics.
Proceedings Twenty-Fourth IEEE International Symposium MultipleValued Logic, pp. 115123.
Schild, K. (1991). correspondence theory terminological logics: Preliminary report.
Proceedings Twelth International Joint Conference Artificial Intelligence
(IJCAI91), pp. 466471.
Younger, D. H. (1967). Recognition parsing context-free languages time n3 .
Information Control, 10 (2), 189208.

128

fi
