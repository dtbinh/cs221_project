Journal Artificial Intelligence Research 19 (2003) 469-512Submitted 12/02; published 10/03Temporal Decision Trees:Model-based Diagnosis Dynamic Systems On-BoardLuca ConsoleClaudia Picardiluca.console@di.unito.itclaudia.picardi@di.unito.itDipartimento di Informatica, Universita di Torino,Corso Svizzera 185, I-10149, Torino, ItalyDaniele Theseider Dupredtd@mfn.unipmn.itDipartimento di Informatica, Universita del Piemonte OrientaleSpalto Marengo 33, I-15100, Alessandria, ItalyAbstractautomatic generation decision trees based o-line reasoning modelsdomain reasonable compromise advantages using model-based approach technical domains constraints imposed embedded applications.paper extend approach deal temporal information. introduce notiontemporal decision tree, designed make use relevant information longacquired, present algorithm compiling trees model-basedreasoning system.1. Introductionembedding software components inside physical systems became widespreadlast decades due convenience including electronic control systems themselves. phenomenon occurs several industrial sectors, ranging large-scale products cars much expensive systems like aircraft spacecrafts.case automotive systems paradigmatic. fact, number complexityvehicle subsystems managed software control increased signicantly sincemid 80s increase next decades (see Foresight-Vehicle, 2002), duepossibility introducing, costs acceptable wide scale products,exibility systems, e.g. increased performance safety, reducedemissions. Systems fuel injection control, ABS (to prevent blockage wheelsbraking), ASR (to avoid slipping wheels), ESP (controlling stability vehicle),would possible feasible costs without electronic control.software modules usually installed dedicated Electronic Control Units (ECUs)play important role since complete control subsystem: human control becomes simply input control system, together inputsappropriate sensors. example, position accelerator pedal inputECU controls fuel delivery injectors.serious problem systems software must behave properly alsopresence faults must guarantee high levels availability safety controlledsystem vehicle. controlled systems, fact, many cases safetycritical: braking system obvious example. means monitoring systemsc2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiConsole, Picardi, & Theseider Duprebehaviour, detecting isolating failures, performing appropriate recovery actionscritical task must performed control software. problem detectedsuspected software must react, modifying way system controlled,primary goal guaranteeing safety availability. According recent estimates,75% ECU software deals detecting problems performing recovery actions,tasks diagnosis repair (see Foresight-Vehicle, 2002).Thus design diagnostic software critical time consuming activity, currently performed manually expert engineers use knowledgeperform Failure Mode Eect Analysis (FMEA) 1 dene diagnosticrecovery strategies.problem complex critical per-se, made even dicultnumber issues constraints taken account:resources available on-board must limited, terms memorycomputing power, keep costs low. combined problemnear real time performance needed, especially situations may safetycritical. example, direct injection fuel delivery systems, fuel maintanedhigh pressure (more 1000 bar) cases system mustreact problems within rotation engine (e.g. 15 milliseconds 4000 rpm),prevent serious damage engine danger passengers. fact, fuelleakage dangerous comes high pressure line. caseimportant distinguish whether loss pressure due leak, orderactivate emergency action (for example, stop engine),failure simply signalled user.order keep costs acceptable large scale product, set sensors availableboard usually limited necessary controlling systemscorrect behaviour; thus, always easy gure impact faults mayquantities monitored sensors, whose physical, logical temporalrelation faults may straightforward.devices diagnosed complex behavioural point view:dynamic time-varying behaviour; embedded complex systemsinteract subsystems; cases control system automaticallycompensates deviations nominal behaviour.aspects make design software modules control diagnosis challenging also expensive time consuming. signicant needimproving activity, making reliable, complete ecient useautomated systems support complement experience engineers, order meetgrowing standards required monitoring, diagnosis repair strategies.Model-based reasoning (MBR) proved interesting opportunity automotiveapplications indeed applications real systems experimented1. result FMEA table lists, possible fault component system,eect faults component system whole possible strategy detectfaults. table compiled manually engineers, based experience knowledge blueprintsystem.470fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board90s (e.g., Cascio & Sanseverino, 1997; Mosterman, Biswas, & Manders, 1998; Sachenbacher,Malik, & Struss, 1998; Sachenbacher, Struss, & Weber, 2000). type models adoptedMBR conceptually far away adopted engineers. particular,component oriented approach typical MBR ts quite well problem dealingseveral variants systems, assembled starting set basic components.thorough discussion advantages MBR approach, see ConsoleDressler (1999).applications developed far, however, concentrated o-board diagnosis,diagnosis workshop, on-board diagnosis. case on-boardsystems seems complicated since, due restrictions hardwareput on-board, still questionable diagnostic systems designed reasonrst-principle models on-board. reason approaches developedorder exploit advantages MBR also on-board applications. particular,compilation-based scheme design on-board diagnostic systems vehiclesexperimented Vehicle Model Based Diagnosis (VMBD) BRITE-Euram Project (199699), applied Common-rail fuel injection system (Cascio, Console, Guagliumi, Osella,Sottano, & Theseider, 1999). approach model-based diagnostic system usedgenerate compact on-board diagnostic system form decision tree. Similarly,automated FMEA reports generated Model-Based Reasoning Autosteve systemused generate diagnostic decision trees (Price, 1999). Yet similar ideaproposed Darwiche (1999), diagnostic rules generated model ordermeet resource constraints.approaches interesting advantages. one hand, sharebenets model-based systems, relying comprehensive representationsystem behaviour well dened characterization diagnosis.hand, decision trees compact representations make sense representing onboard diagnostic strategies, ecient space time. Furthermore, algorithmssynthesizing decision trees examples well established machine learningcommunity. specic case examples solutions (diagnoses recoveryactions) generated model-based system.However, basic notion decision tree approaches learning treesexamples major limitation kind applications: copeproperly temporal behaviour systems diagnosed, and, particular,fact incremental discrimination possible faults, leading nal decisionaction taken on-board, based observations acquired across time,thus taking account temporal patterns.reason, work described paper introduce new notiondecision tree, temporal decision tree, takes account temporal dimension,introduce algorithm synthesizing temporal decision trees.Temporal decision trees extend traditional decision trees fact nodestemporal label species condition checked order select onebranches make decision. shall see, allows taking accountcases order delay observable measures inuencesdecision made thus provides important power improve decision process.Waiting, however, always possible thus generation trees includes471fiConsole, Picardi, & Theseider Duprenotion deadline possible decision. Thus, temporal decision process supportspossibility selecting best decision, exploiting observations temporallocations (patterns) taking account cases point decisiontaken anyway prevent serious problems.rest paper organized follows. section 2 summarize basicideas model-based diagnosis (MBD), use decision trees conjunction it,temporal dimension MBD decision trees. section 3 provide basicformal denitions decision trees, form basis extension temporaldecision trees section 4. describe (section 5) problem synthesizing temporaldecision trees solution (section 6).2. Model-based Diagnosis Decision Treessection briey recall basic notions model-based diagnosis discussdecision trees used diagnostic purposes, focusingused VMBD conjunction model-based approach (Cascio et al., 1999).2.1 Atemporal CaseFirst let us sketch atemporal case traditional use diagnostic decisiontrees.2.1.1 Atemporal model-based diagnosisstarting point model-based diagnosis model structure behaviourdevice diagnosed. specically, assume component centered approachwhich:model provided component type; component characterizedset variables (with distinguished set interface variables);set modes, including ok mode (correct behaviour) possibly setfault modes.set relations involving component variables modes, describing behaviour component mode. relations may model correctbehaviour device and, cases, behaviour presence faults(faulty behaviour).model device given list component instancesconnections (connections interface variables).Articial Intelligence approach, models usually qualitative, domainvariable nite set values. abstraction proven usefuldiagnostic purposes.model used simulating behaviour system computingdiagnoses. fact, given set observations system behaviour, diagnosesdetermined comparing behaviour predicted model (in normal conditionspresence single multiple faults) observed behaviour.472fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardorder model useful on-board diagnosis, fault mode F (orcombination fault modes) model must include recovery action control softwareperform case F occurs. general actions cost, mainly relatedresulting reduction functionality system. Moreover, two actions a1 a2related following sense:a1 , recovery action associated fault F1 , carries operations includeperformed a2 , recovery action associated F2 ;a1 used recovery action also F2 occurs; may however carryunneeded operations, thus reducing system functionality strictly necessary.However, case cannot discriminate F1 F2 , applying a1 rationalchoice.section 4.3 present model actions formalizes relation.Thus main goal on-board diagnostic procedure decide best actionperformed, given observed malfunction. type procedure ecientlyrepresented using decision trees.2.1.2 Decision treesDecision trees used implement classication problem solving thus formdiagnostic procedure. node tree corresponds observable. on-boarddiagnosis, observables correspond either direct sensor readings, results computations carried ECUs available measurements. followingword sensor denote types observables; worth pointing lattermay require time performed. paper mainly assume sensorreading takes time; however apporach propose deals also casesensor reading time consuming, pointed section 5.1. node manydescendants number qualitative values associated sensor. leavestree correspond actions performed board. Thus, given availablesensor readings, tree easily used make decision recovery actionperformed.decision trees generated automatically set examples cases.example mean possible assignment values observables corresponding diagnosis, possible alternative diagnoses, selected recovery actionappropriate set suspect diagnoses. set produced using modelbased diagnostic systems, which, given set observables compute diagnosesrecovery actions.atemporal case, nite qualitative domains, number possible combinations observations nite, usually small, therefore considering cases exhaustively(and sample) feasible two equivalent ways buildingexhaustive set cases:1. Simulation approach: fault F , run model-based system predictobservations corresponding F .473fiConsole, Picardi, & Theseider Dupre2. Diagnostic approach: run diagnosis engine combinations (all relevant combinations) observations, compute candidate diagnoses onecases.either case, resulting decision tree contains information set cases;if, sensors placed system, observations cost, decisiontree way save space respect table, speed lookup information.way advantages model-based approach use compact decision trees combined: model-based engine produces diagnoses based reusablecomponent models used diagnoser o-board; compact decision trees,synthesized cases classied model-based engine, installed on-board.2.2 Towards Temporal Decision Treessection briey recall basic notions temporal model-based diagnosis (seeBrusoni, Console, Terenziani, & Theseider Dupre, 1998 general discussion temporaldiagnosis), informally introduce temporal decision trees.2.2.1 Temporal MBDbasic denition MBD conceptually similar atemporal case. Let us considermain dierences.regards model component type consider type variable:state variables used model dynamic behaviour component. set relationsdescribing behavior component (for mode) augmented temporalinformation (constraints); make specic assumptions model time, eventhough, shall see following, impact casesconsidered tree generation. example, constraints may specify delayinput output change state component.regards recovery actions, deadline performing action must specied;represents maximum time elapse fault detection recovery action; amount time available control software perform discrimination.piece information specic component instance, rather componenttype, action deadline related potential unacceptable eectsfault could overall system; fault component typecould dangerous one instance tolerable another.Diagnosis started observations indicate system behaving correctly.Observations correspond (possibly qualitative) values variables across time. general,temporal case diagnosis assignment mode behaviour componentinstances across time observed behaviour explained assignment givenmodel. details dierent ways dening explanation case see Brusoni etal. (1998). purposes paper interested fact that, givenset observables, diagnosis (or set candidate diagnoses complete discriminationpossible) computed recovery action determined.means starting point approach table containing resultsrunning model-based diagnostic system set cases, (almost) independentlymodel-based diagnostic system used generating table.474fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardalready mentioned static case, nite qualitative domains, exhaustive set cases considered. temporal case, model time purelyqualitative, table temporal information cannot built prediction,built running diagnosis engine set cases quantitative information: diagnosesmake qualitative predictions inconsistent quantitative informationruled out. course, cannot general done exhaustively, even observations assumed acquired discrete times; not, decision tree generationactually learning examples.Thus simulation approach used case temporal constraintsmodel precise enough predict least partial information temporal locationobservations, e.g., case model includes quantitative temporal constraints.diagnostic approach used also case weaker (qualitative) temporal constraints model.regards observations, consider general case set snapshotsavailable; snapshot labelled time observation reports valuesensors (observables) time. makes approach suitable dierent notionstime underlying model observations (see discussion Brusoniet al., 1998).Example 1 starting point generating temporal decision tree table likeone Figure 1.sit1sit2sit3sit4sit5sit6sit7sit80nhnnhnhh1nhnnhnhh2nhnnhnhhs13 4 5 6 7 0n h hlhn h h h h lh h h hlhhh h hlh h hlh h hh1vnllnvlh2vnllnvnns23 4 5 6 7 0v v vnnl v v v v nl l l vnnnz z znn l vnn l ln1nnnnnnnn2nnnhnnnns33 4 5 6 7 Act Dll l l5b2l l l v v b7h h h hc6lc3l l v5l l vb5l v zc5Figure 1: example set cases learning temporal decision trees.row table corresponds situation (case example terminologymachine learning) reports:sensor si values observed snapshots (in example8 snapshots, labelled 0 7); n, l, h v correspond qualitativevalues sensor measurements; n normal, l low, h high, v lowz zero.recovery action Act performed situation.deadline Dl performing action.table one example represents set situations mayencountered case faults and, noticed above, generated using either475fiConsole, Picardi, & Theseider Duprediagnostic simulation approach. next section shall introduce notiontemporal decision trees show pieces information sensor histories liketable exploited generation trees.2.2.2 Introduction temporal decision treesTraditional decision trees include notion time, i.e., fact data mayobservable dierent times dierent faults may distinguishedtemporal patterns data. Thus, neglecting notion time may lead limitationsdecision process.reason work introduce notion temporal decision tree. Let usanalyse intuition behind temporal decision trees decision process support.Formal denitions provided later paper.Let us consider, example, fault situations sit3 sit4 Figure 1, letus assume, sake simplicity, available sensor s2 . two faultsituations distinguished control software require dierentrecovery actions. detected fact s2 shows low value.Moreover, situation s2 starts showing low value.way discriminate two situations make use temporal information,exploit fact sit3 value v shows 4 time units fault detection,sit4 value shows 6 time units.order take account decision tree, include timetree. examples, best decision procedure wait observing thst s2 = l(that is, dectecting fault occurred). 4 time units makedecision, depending whether s2 = v not. corresponds procedure describedtree Figure 2.s2......ls2 4lsit4vsit3Figure 2: simple example temporal decision treeObviously, waiting always solution always possible. many cases,fact, safety constraints may impose deadlines performing recovery actions.reected generation decision procedure. Suppose, exampleabove, deadline sit3 3 rather 6: case two situations wouldindistinguishable, beacause would infeasible wait 4 time units.476fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-BoardThus, essential idea generating small decision trees temporal casetake advantage fact cases nothing better waiting2 , orderget good discrimination, provided safety integrity physical systemkept account, deadlines recovery actions always met.generally, one exploit information temporal patterns observables deadline, like ones Figure 1, produce optimal diagnosticprocedure.Another idea use approach integration incremental discrimination,basis generation traversal decision tree, incrementalacquisition information across time.atemporal case, decision tree generated order guide incremental acquisition information: dierent subtrees node relative dierent setsfaults therefore may involve dierent measurements: subtree performmeasurements useful discriminating faults compatible measurements made us reach subtree T, starting root. o-board diagnosis,allows reducing average number measurements get decision (i.e. averagedepth tree), useful measurements cost - e.g. time operator take system; on-board diagnosis, even case measurementssimply sensor readings, cost sensor made partsystem, interested generating small decision trees save space.temporal case issue: incremental acquisition informationnaturally constrained ow time. want store sensor values acrosstime seems natural choice since memory constraints information mustacquired available possible read sensors choicewaiting made. issue taken account generation temporaldecision trees.3. Basic Notions Decision Treesmoving formal denition temporal decision trees, section brieyrecall denitions algorithms atemporal case. particular, recallstandard ID3 algorithm (Quinlan, 1986), basis algorithmtemporal case. denitions section standard ones (see textbookArticial Intelligence, e.g., Russel & Norvig, 1995).3.1 Decision Treesadopt following formal denition decision tree, extended section4.1 temporal decision trees.Definition 1 Let us consider decision process P set available decisions,set tests performed external environment, out(oi ) =2. dierent approach would weighing amount elapsed time agains possibilitybetter discriminating faults; approach something considering future worktemporal decision trees, outlined section 7.477fiConsole, Picardi, & Theseider Dupre{v1 , . . . , vki } possible outcomes test oi O. decision tree P labelledtree structure = r, N, E, L where:r, N, E tree structure root r, set nodes N set edges E N N ;N partitioned set internal nodes NI set leaves NL .L labelling function dened N E.n NI , L(n) O; words internal node labelled nametest.(n, c) E L((n, c)) out(L(n)); is, edge directed n clabelled possible outcome test associated n.Moreover, (n, c1 ), (n, c2 ) E L((n, c1 )) = L((n, c2 )) c1 = c2 ,v out(L(n)) c(n, c) E L((n, c)) = v; is, n exactly one outgoing edge possibleoutcome test L(n).l NL , L(l) A; words leaf labelled decision.decision-making agent uses tree, starts root. Every time reachesinner node n, agent performs test L(n), observes outcome v follows vlabelled edge. agent reaches leaf l, makes decision = L(l).3.2 Building Decision TreesFigure 3 shows generic recursive algorithm used build decision treestarting set Examples set Tests.Recursion ends either remaining examples need discriminationcorrespond decision, available observables used,values observed match cases dierent decisions. latter case observablesenough getting proper decision agent actually using tree,take account this.case terminating condition holds, algorithm chooses observable variabletest become root label subtree T. Depending ChooseTest implemented get dierent specic algorithms dierent decision trees.subtree built possible outcome value test recursive callBuildTree, sets Tests Update SubExamples inputs. Tests Update obtained removing test set tests, order avoid using again. SubExamplessubset Examples containing examples value outcometest.mentioned before, many specic algorithms, and, general, results,implementations ChooseTest. general desirable generate treeminimum average depth, two reasons:Minimizing average depth means minimizing average number tests thusspeeding decision process.machine learning, small number tests also means higher degree generalization particular examples used building tree.478fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board12345678910111213141516171819202122function BuildTree (set Examples, set Tests)returns decision tree = root, Nodes, Edges, Labelsbeginex Examples correspond decisionreturn BuildLeaf(Examples);Tests emptyreturn BuildLeaf(Examples);test ChooseTest (Tests, Examples);root new node;Nodes {root}; Edges ; Labels(root) test;root, Nodes, Edges, Labels;Tests Update Tests \ {test};possible outcome value test beginSubExamples {ex Examples | test outcome value ex};SubExamples empty beginSubTree BuildTree (SubExamples, Tests Update);Append(T, root, SubTree);Labels((root, Root(SubTree))) value;end;end;return T;end.Figure 3: Generic algorithm building decision tree3.3 ID3Unfortunately, nding decision tree minimum average depth intractable problem; however, exists good best-rst heuristic choosing tests order producetrees deep. heuristic proposed ID3 algorithm (Quinlan,1986), base concept entropy information theory. followingrecall approach detail also order introduce notationused rest paper.Definition 2 Given (discrete) probability distribution P = {p1 , . . . , pn } entropy E(P)is:(1)E(P) =npi log2 pii=1Entropy measures degree disorder. choose test, want splitexamples lowest degree disorder respect decisions associatedthem.Given set examples E introduce sets:E |a = {e E | decision associated example e a}.set available decisions = {a1 , . . . , } E |A = {E |a1 , . . . , E |an } partition E.479fiConsole, Picardi, & Theseider DupreDefinition 3 ai A, = 1, . . . , n, dene probability ai respectE follows:|(E |ai )|P (ai ; E) =|E|worth noting that, examples endowed priori probabilities,redene P (ai ; E) order take account. basic formulation ID3 assumeshowever examples equiprobable computes probability distributionfrequencies examples.entropy E is:E(E) =(2)nP (ai ; E) log2 P (ai ; E).i=1decisions equiprobable, get E(E) = log2 n, maximum degreedisorder n decisions. decisions one probability equal 0, E(E) = 0:degree disorder minimum.Entropy used follows test selection. test possible outcomes v1 , . . . , vk ,splits set examples into:E |ovi = {e E | test value vi e}.E |o = {E |ov1 , . . . , E |ovk } partition E. particular, buildingtree choose test o, E |ovj subset examples use building subtreechild corresponding vj . lowest degree disorder E |ovj , closerleaf. Therefore, following equation (2):E(E |ovj ) =nP (ai ; E |ovj ) log2 P (ai ; E |ovi ).i=1Finally, dene entropy test average entropy possible outcomes:Definition 4 entropy test respect set examples E is:E(o; E) =(3)kP (o vj )E(E |ovj ),j=1where3 P (o vj ) =|(E |ovj )||E|.ID3 algorithm simply consists choosing test lowest entropy. Figure 4 showsimplementation ChooseTest yields ID3.3. Again, examples endowed priori probabilities, denition changed ordertake account480fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board123456789101112131415function ID3ChooseTest (set Tests, set Examples)returns test best testbeginbest test element Tests;min entropy log2 |Examples|;test Tests beginpart Partition({Examples}, test);ent Entropy(part);(ent < min entropy) beginmin entropy ent;best test test;end;end;return best test;end.Figure 4: ID3 implementation ChooseTest4. Extending Decision Treessection formally introduce notion temporal decision tree, showtiming information added set examples used tree building. Moreoverintroduce model recovery action expresses information needed algorithm.4.1 Temporal Decision Treessection 2.2.2 motivated monotonicity requirement temporal decision trees,traversal requires information relative increasing time informationstored.discuss temporal information actually included tree matchedtemporal information observations. tree used abnormal value detected sensor (fault detection). intend time faultdetection reference time temporal labels observations tree. lookexample data shown Figure 1, see that, every fault situation,always least one sensor whose value time 0 dierent nnormal. reasonthat, fault situation, associate 0 time label rst snapshotsensor showing deviation nominal behaviour.following denition provides extension temporal dimension decisiontrees.Definition 5 temporal decision tree decision tree r, NI , NL , E, L endowedtime-labelling function that:(1) : NI IR+ ; call (n) time label;(2) n NI exist n (n, n ) E (in words, n child n),(n ) (n).481fiConsole, Picardi, & Theseider DupreSince assume store information, rather use information traversing tree dictated tree itself, rst branching discrimination provideddepending sensor provided value. assumedierent temporal decision trees, one sensor could possibly provide rstabnormal value, or, alternatively, root node time label, edgesroot labelled dierent values single observable, dierentsensors could provide rst abnormal observation. tree, subtreesecond alternative, generated independently ones, using examples sensor providing fault detection same. generationdescribed rest paper.Let us tree temporal decision tree (or forest, case multiple detectingsensors) exploited on-board diagnostic agent order choose recoveryaction. rst abnormal value detected, agent activates time counterstarts visiting appropriate tree root. reaches inner node n,agent retrieves associated test = L(n) time label = (n).waits time counter reaches t, performs test chooses one child nodesdepending outcome. agent reaches leaf, performs correspondingrecovery action.respect atemporal case, agent option wait.point view agent may seem pointless wait could look sensor values,since reading sensor values cost. However, point view tree thingsquite dierent: want add test makes tree deepertime necessary.Condition (2) states agent move forward time. correspondsassumption sensor readings stored, discussed section 2.2.2.Example 2 Let us consider diagnostic setting described example 1. Figure 5 showstemporal decision tree setting, temporal decision tree uses sensorsrecovery actions mentioned Figure 1. tree run fault situationstable, proper recovery action selected within deadline.4.2 Adding Timing Information Set Examplesorder generate temporal decision trees, need temporal information examples.already introduced informally notion set examples (or fault situations)temporal information describing table Figure 1. following denitionformalizes notion.Definition 6 temporal example-set (te-set short) E collection fault situationssit1 , . . . , sitn characterized number sensors sens1 , . . . , sensm ascending sequence time labels t1 < . . . < tlast representing instants time sensorreadings available. context call observation pair sensi , tj . te-setorganized table follows:(1) table n rows, one fault situation.482fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardlowhighs1,0normalAction: bAction:normals2,1Actions: b,clowlowhighs2,0s2,3s2,1Action: clownormalzeroAction:highAction: bs3,2highAction: cFigure 5: temporal decision tree situations described Figure 1.(2) table last observation columns containing outcomes dierentobservations fault situation. denote Val(sith , sensi , tj ) valuemeasured sensor sensi time tj fault situation sith .(3) table distinguished column Act containing recovery action associatedfault situation. denote Act(sith ) recovery action associatedsith .(4) table second distinguished column Dl containing deadlinefault situation. denote Dl(sith ) deadline sith ,Dl(sith ) {t1 , . . . , tlast } h = 1, . . . , n. dene global deadlinete-set Dl(S) = min{Dl(sit) | sit S}.4 P (sit; E) associated sit E ,moreover assume probabilitysitE P (sit; E) = 1. every E E every sit E introduce followingnotation: P (E ; E) = sitE P (sit; E) P (sit; E ) = PP (sit;E)(E ;E) .4.3 Model Recovery Actionsalgorithms shall introduce require detailed model recovery actions.particular want better characterize happens possible uniquelyidentify appropriate recovery action. Moreover, want quantify lossincur happens.start formal denition:Definition 7 basic model recovery actions triple A, , where:(1) = {a1 , . . . , aK } nite set symbols denoting basic recovery actions.4. P (sit; E) computed frequency, P (sit; E) = 1/n, n number faultsituations, known priori added set examples.483fiConsole, Picardi, & Theseider Dupre(2) partial strict order relation A. say ai weakeraj , written ai aj , aj produces recovery eects ai , senseaj could used place ai (but vice versa). therefore assumedrawbacks actions, action performedtime negative consequences, apart cost action (seebelow). clearly limitation something tackled future work (seediscussion section 7).(3) : IR+ cost function, ai aj (ai ) < (aj ).associates cost basic recovery action, expressing possible drawbacksaction itself. Recovery actions performed on-board usually imply performancelimitation abortion ongoing activity; costs meant estimatemonetary losses inconveniences users resulting these. requirement monotonicity respect stems following consideration:ai aj (ai ) (aj ) would make sense ever perform ai , since ajcould performed eects (or lower) cost. moreoverassume costs independent fault situation (a consequenceno-drawbacks assumption mentioned previous point).Example 3 Let us consider four recovery actions a, b, c, appear teset Figure 1. Figure 6 shows basic action model them. graph expressesoredering relation , costs shown next action names.- : 100b - : 20c - : 50- : 10Figure 6: basic action model.seen previous section fault situation associatedrecovery action; usually association depends fault, may also dependoperating conditions fault occurs.happens cannot discriminate multiple fault situations? section 3.2,outlining generic algorithm atemporal case, referred solutiondecision-making agent. case want precise.Definition 8 Let A, , basic model recovery actions. dene functionmerge : 2A 2A follows:(4)merge(S) = {ai | aj ai aj }484fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardmoreover dene:(5)merge-set(A) = {merge(S) | 2A } 2Amerge-set(A) set compound recovery actions includes basic recovery actionsform singletons.intuition behind merge cannot discriminate multiple fault situationssimply merge corresponding recovery actions. means collect recoveryactions, remove unnecessary ones (equation (4)). action set becomesunnecessary set contains stronger action. Thus given te-set E, dene:(6)Act(E) = merge({Act(sit) | sit E}).take account compound actions, extend notion model recoveryactions follows:Definition 9 Let = A, , basic model recovery actions. extended modeltriple A, ext , ext where:(1) ext merge-set(A) merge-set(A) given A, merge-set(A), extevery either exists . Notice{ai } ext {aj } ai aj .actions(2) ext : merge-set(A) IR+ cost function compoundevery merge-set(A), maxaA (a) ext (A) aA . Moreover, extmust hold ext (A) < ext (A ).ext uniquely determined , hold ext : reason one extended model basic model. requirementmaxaA (a) ext (A) motivated follows: existed ext (A)(a) would make sense never perform a, substituting A. fact, {a} extwould lower cost. also ask ext - basic models monotonic respect ext .following shall consider extended models recovery actions, thusshall drop ext prex .Example 4 Figure 7 shows possible extension basic model Figure 6.case cost compound action {b, c} given sum individual costs bc.5. Problem Building Temporal Decision Treessection outline peculiarities building temporal decision trees, showingdierences respect traditional case.485fiConsole, Picardi, & Theseider Dupre{a} - : 100{b, c} - : 70{b} - : 20{c} - : 50{d} - : 10Figure 7: extended action model.5.1 Challenge Temporal Decision Treesmakes generation temporal decision trees dicult standard onesrequirement time labels decrease moving root leaves:corresponds assuming sensor values cannot stored; decision-makingagent decides wait gives using values sensors show waiting.release restriction actually generate temporal decision treesminor variation ID3, essentially considering pair formed sensor timelabel individual test. words ID3ChooseTest selects sensor timelabel reading time allows maximum discrimination among examples.However systems ones considering, low-memory real-time systems,possibility performing diagnostic task without discarding dynamics alsowithout store sensor values across time serious issue take account.reason denition temporal decision tree includes requirement timelabels decreasing root-leaf paths.Figure 8 shows generic algorithm building temporal decision trees help usoutline diculties task. Line 8 shows minor modication aimed takingaccount deadlines: observation used given set examples timelabel exceed global deadline. Violating condition would result treeselects recovery action deadline corresponding fault situationexpired.major change respect standard algorithm however shown line15: select observation pair sensor, tlabel must remove setobservations pairs whose time label lower tlabel5 .consequence operations - ruling invalid observations discardingpast - set observations available building child nodedierent one used parent one way:5. Actually assumes reading sensor moving downwards tree accordingly doneswiftly qualitative sensor values time change meanwhile.case, one choose remove also pairs time labels equal tlabel, generallytime labels lower tlabel + k k time needed diagnostic agent carrytree operations. sake simplicity, however, following assume k 0, sincechoice k aect approach propose.486fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board12345678910111213141516171819202122232425function BuildTemporalTree (te-set Examples, set Obs, action model ActModel)returns temporal decision tree = root, Nodes, Edges, Labels, TLabelsbeginsit Examples Act(sit)return BuildLeaf(Examples, ActModel);deadline Dl(Examples);UsefulObs {o Obs | s1, s2 Examples s.t. Val(s1, o) = Val(s2, o)};ValidObs {sensor, tlabel UsefulObs | tlabel deadline};ValidObs emptyreturn BuildLeaf(Examples, ActModel);sensor, tlabel ChooseObs (ValidObs, Examples, ActModel);root new node;Nodes {root}; Edges ; Labels(root) sensor; TLabels(root) tlabel;root, Nodes, Edges, Labels, TLabels;Obs Update {sens, tst UsefulObs | tst tlabel};possible measure value sensor beginSubExamples {sit Examples | Val(sit, sensor, tlabel) = value};SubExamples empty beginSubTree BuildTemporalTree (SubExamples, Obs Update, ActModel);Append(T, root, SubTree);Labels((root, Root(SubTree))) value;end;end;return T;end.2627282930313233function BuildLeaf (te-set Examples, action model ActModel)returns loose temporal decision tree = leaf, {leaf}, , Labels,beginact {Act(sit) | sit Examples};comp act merge(all act);leaf new node; Labels(leaf) comp act; leaf, {leaf}, , Labels, ;return T;end.Figure 8: Generic algorithm building temporal decision treeobservations invalid parent valid child. recursivecall child works smaller set examples; therefore global deadline mayahead time, allowing observations used.observations become unavailable child timelabel lower used parent.course problematic issue latter: observations lost, amongmay information necessary properly selecting recovery action.Let us consider example te-set Figure 9, four fault situations, two timelabels (0 1) one sensor (s). fault situation characterized dierentrecovery action, te-set obviously allows discriminate them. Howeverentropy criterion would rst select observation s, 1, discriminating.487fiConsole, Picardi, & Theseider Dupresit1sit2sit3sit4s, 0xxs, 1xzActbcFigure 9: te-set causing problems standard ID3 algorithm used temporaldecision trees.observation s, 0 would become unavailable, resulting tree could neverdiscriminate sit2 sit3 .shows relevant dierence building standard decision treesbuilding temporal decision trees. Let us look generic algorithm standarddecision trees presented Figure 3: particular strategy implemented ChooseTestaect capability tree selecting proper recovery action,size tree. Essentially tree contains information set examples least concerns association observations recovery actions.say tree always discriminating power set examples,meaning case tree capable deciding two recoveryactions set examples contains two fault situations identical observationsdierent actions.consider algorithm Figure 8 see order observationsselected - is, particular implementation ChooseObs - aect discriminating power tree, size. Since one recursive call followingobservations discarded, obtain tree less discriminating informationoriginal set examples. primary task avoid situation,build tree small, sacrice relevant information. consequence, cannot exploit strategy simply selecting observation minimumentropy.next sections shall formalize new requirements output tree,propose implementation ChooseObs meets them.5.2 Tree Costprevious section informally introduced notion discriminating power.section shall introduce general notion expected cost temporal decisiontree. Intuitively, expected cost associated temporal decision tree expectedcost recovery action selected tree, respect probability distributionfault situations.Expected cost stronger notion discriminating power: one hand treediscriminates better another, also lower expected cost (we shall soonprove statement). hand expected cost adds something notiondiscriminating power, since two trees comparable point view cost,may point view discrimination carry out.488fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boarddening expected cost, need introduce preliminary denitions.shall make use function, named examples, given initial set examplesE tree associates node tree subset E. understand meaningfunction formally dening it, let us imagine run tree Ecertain point decision process reach node n: examples tells ussubset fault situations yet discarded.Definition 10 Let E te-set sensors s1 , . . . , sm , time labels t1 , . . . , tlast actionsmodel A, , . Moreover let = r, N, E, L, temporal decision treeevery internal node n N L(n) {s1 , . . . , sm } (n) {t1 , . . . , tlast }.dene function examples(; E) : N 2E follows:(7) examples(r; E) = Er root T;examples(n; E) = {sit examples(p; E) | Val(sit, L(p), (p)) = L((p, n))}n N , n = r (p, n) E.examples well dened since n N dierent root exists exactlyone p N (p, n) E.Notice that, E set examples used building T, examples(n; E) correspondssubset examples used creating node n.Example 5 Let us consider Figure 10: shows tree Figure 5, everynode n also see set fault situations examples(n; E), E te-setFigure 1.lowhighAction: bsit7lows1,0sit1,sit3,sit4sit6,sit7normalsit1sit6highnormalActions: b,clows2,3s2,2sit1,sit2,sit3,sit4sit5,sit6,sit7,sit8s2,1sit1,sit3sit4,sit6s2,1sit2,sit5sit8highAction: csit2,sit5sit8lowzeronormals3,2highsit3,sit4Action:Action:Action: bAction: csit1sit6sit3sit4Figure 10: temporal decision tree showing value example(n, E).489fiConsole, Picardi, & Theseider Duprefollowing using function examples shall omit second argument,denoting initial te-set, ambiguity te-set considered.every tree used given set examples: actually need compatibility two, characterized following denition.Definition 11 Let E previous denition. say temporal decision tree= r, N, E, L, compatible E if:every internal node n N , L(n) {s1 , . . . , sm }, (n) {t1 , . . . , tlast } (n)Dl(examples(n; E)).every leaf l N , L(l) merge(A) L(l) = merge(Act(examples(l; E))).straightforward see tree compatible set examples used buildingit.following property6 :Proposition 12 Let = r, N, E, L, temporal decision tree compatible teset E. Let l1 , . . . , lf N denote leaves . examples(l1 ), . . . , examples(lf )partition E.sit E denote leafT (sit) unique leaf l sitexamples(l). ready formalize notion discriminating power.Definition 13 Let = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU denote two temporal decision trees compatible te-set E. Let moreover A, ,recovery action model used building U. say discriminatingU respect E if:(1) every sit E either LT (leafT (sit)) LU (leafU (sit)) LT (leafT (sit)) =LU (leafU (sit));(2) exists sit E LT (leafT (sit)) LU (leafU (sit)).Notice second condition makes sure two trees equal (incase would equally discriminating), something rst condition alone cannotguarantee.Example 6 Let us consider tree Figure 10 tree Figure 11 below.former discriminating latter. fact, two trees associateactions sit1 , sit2 , sit3 , sit4 , sit5 sit6 . However former associates action b sit7action c sit8 , latter associates sit7 sit8 compound action{b, c}.Unfortunately cannot easily use discriminating power - dened - preference criterion decision trees. reason dene total orderdecision trees, partial one. following situations may arise:6. sake readability, proofs collected separate appendix end paper.490fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardsit, LT (leafT (sit)) LU (leafU (sit)); sit, LU (leafU (sit))LT (leafT (sit)).given sit, LT (leafT (sit)) LU (leafU (sit)), LU (leafU (sit)) LT (leafT (sit)).point view discriminating power alone, reasonable Ucomparable cases. Nonetheless, may reason preferring oneother, reason cost. example consider second situation, evenLT (sit) LU (sit) directly comparable point view strength,one two may cheaper thus preferable.therefore introduce notion expected cost tree.Definition 14 Let = r, N, E, L, temporal decision tree compatible teset E action model = A, , . inductively dene expected cost functionXE,A : N IR+ tree nodes follows:l N leaf;(L(l))(8)XE,A (n) =P (L(n) L((n, c))) XE,A (c) n N inner node.c:(n,c)EP (L(n) L((n, c))) probability sensor L(n) showing value v = L((n, c))given by:P (L(n) L((n, c))) =P (examples(c); E)= P (examples(c); examples(n)).P (examples(n); E)expected cost respect E A, denoted XE,A (T) , dened as:XE,A (T) = XE,A (r) r root(9)denition states that:expected cost tree leaf l simply cost recovery action L(l);expected cost inner node n given weighted sum childrensexpected costs; weight child c given probability P (L(n) L((n, c))).expected cost temporal decision tree expected cost root.following proposition states weighted sum computing expected costroot performed directly tree leaves.Proposition 15 Let = r, N, E, L, denote temporal decision tree, let l1 , . . . , luleaves.(10)XE,A (T) =u(L(li )) P (examples(l); E)i=1491fiConsole, Picardi, & Theseider Duprelowlows2,5sit1sit6s2,0lowsit1,sit2,sit3,sit4sit5,sit6,sit7,sit8highzerolows3,6sit3sit4highAction:Action:Actions: bAction: csit1sit6sit3sit4Actions: bsit2,sit5,sit7,sit8Figure 11: temporal decision tree less discriminating one Figure 10.next proposition shows expected cost monotonic respect betterdiscrimination relation, therefore good preference criterion temporal decisiontrees, since tree lowest possible expected cost discriminating one,moreover cheapest among equally discriminating trees.Proposition 16 Let = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU two temporal decision trees compatible te-set E actions model A.discriminating U XE,A (T) < XE,A (U).Example 7 Let us compute expected cost tree T1 Figure 10 tree T2Figure 11, respect te-set E Figure 1 action model Figure 7.shall assume fault situations equiprobable, probability1/8. exploiting proposition 15 obtain:XE,A (T1 ) = P (sit1 )(a) + P (sit7 )(b) + P (sit6 )(d) + P (sit3 )(b) + P (sit4 )(c) ++P ({sit2 , sit5 })({b, c}) + P (sit8 )(c) =1111111100 + 20 + 10 + 20 + 50 + 70 + 50 ==8888848= 12.5 + 2.5 + 1.25 + 2.5 + 6.25 + 17.5 + 6.25 = 48.75XE,A (T2 ) = P (sit1 )(a) + P (sit6 )(d) + P (sit3 )(b) + P (sit4 )(c) ++P ({sit2 , sit5 , sit7 , sit8 })({b, c}) =11111100 + 10 + 20 + 50 + 70 ==88882= 12.5 + 1.25 + 2.5 + 6.25 + 35 = 57.5see less discriminating tree, T2 , higher expected cost.5.3 Restating Problemprevious section introduced expected cost preference criterion decisiontrees. Given notion, restate problem building temporal decision tree492fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardbuilding tree minimum possible expected cost. section formally showsnotion minimum possible expected cost well dened, preciselycorresponds cost tree exploits observations te-set. goalexpressed nding reasonably small tree among whose expected costminimum.section, well formalizing mentioned notions, introduceformal machinery useful proving correctness algorithm.Definition 17 Let E denote te-set. Moreover, let t1 , . . . , tlast denote time labelsE. say siti , sitj E pairwise indistinguishable, write siti sitj ,ti < min{Dl(siti ), Dl(sitj )} sensors Val(siti , s, ti ) =Val(sitj , s, ti ).relation, obviously reexive symmetric, transitive. considersitk particularly strict deadline, might well siti sitk , sitk sitj ,siti sitj . introduce new relation transitive closure .Definition 18 Let E denote te-set. say siti , sitj E indistinguishable,write siti sitj , exists nite sequence sitk1 , . . . , sitku Esitk1 = siti ;sitku = sitj ;every g = 1, . . . , u 1, sitkg sitkg+1 .equivalence relation E, denote E/ corresponding quotient set.following denition:Definition 19 Let E te-set, actions model A. expected cost E, denotedX E,A , dened as:X E,A =(merge({Act(sit) | sit })) P (; E)(11)E/Example 8 Let us consider te-set E Figure 1 action model Figure 7.two indistinguishable fault situations E sit2 sit5 . Thus have:X E,A = P (sit1 )(a) + P ({sit2 , sit5 })({b, c}) + P (sit3 )(b) + P (sit4 )(c) ++P (sit6 )(d) + P (sit7 )(b) + P (sit8 )(c) =1111111100 + 70 + 20 + 50 + 10 + 20 + 50 ==8488888= 12.5 + 17.5 + 2.5 + 6.25 + 1.25 + 2.5 + 6.25 = 48.75Notice tree Figure 10 cost te-set, thus cost minimumpossible, show below. course may still able build another smaller treecost.493fiConsole, Picardi, & Theseider Dupreneed show X E,A actually minimum possible expected costtemporal decision tree compatible E.Theorem 20 Let E te-set actions model A. that:(i) exists decision tree compatible E XE,A (T) = X E,A .(ii) every temporal decision tree compatible E, X E,A XE,A (T).state precisely problem building temporal decision tree:Given te-set E actions model A, want build temporal decisiontree E, XE,A (T) = X E,A . Moreover, want keep treereasonably small exploiting entropy.6. Algorithmsection describe detail proposal building temporal decision treesgiven te-set action model. also discuss complexity algorithm introduce,give example algorithm works.6.1 Preconditionsgoal dene implementation function ChooseObs that,plugged function BuildTemporalTree, yields solution problem buildingtemporal decision trees stated section 5.3. First however shall analyze properties BuildTemporalTree dened Figure 8: lead us smoothlysolution help us prove formally correctness. order accomplish taskneed introduce notation allows us speak algorithm properties.Let E te-set fault situations {sit1 , . . . , sitn }, sensors {s1 , . . . , sm }, time labels{t1 , . . . , tlast } action model A. aim computing tree executing:(12)BuildTemporalTree({sit1 , . . . , sitn }, {s1 , . . . , sm } {t1 , . . . , tlast }, A)execution BuildTemporalTree comprises several recursive callsfunction; given two recursive calls c, c shall write c c c occurs immediatelyinside c . Moreover shall denote c0 initial call. Finally, shall call terminalrecursive call inner call.given call c, shall denote [[Example]]c , [[Obs]]c , [[ActModel]]c actual valuesformal parameters c. slight abuse notation, shall also write [[var]]cdenote value variables var c that, set, never change value(deadline, UsefulObs, ValidObs, sensor, tlabel, Obs Update). Finally, shall denote[[T]]c tree returned call c.recursive call c works dierent te-set, dened [[Examples]]c[[Obs]]c . actions model however always same, since c c[[ActModel]]c = [[ActModel]]c . shall denote Ec te-set used call c,determined input parameters.494fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardfollowing proposition critical proving correctness approach.states obtain tree minimum expected cost guaranteeincrease expected cost te-set passing setobservations Obs set Obs Update:Proposition 21 Let us consider execution BuildTemporalTree startingmain call c0 . initial te-set, want build tree over, E = Ec0= [[ActModel]]c0 . recursive call c, let us denote Ec te-set determined[[Examples]]c [[Obs Update]]c . Then:(1) XE,A ([[T]]c0 ) X E,A(2) XE,A ([[T]]c0 ) = X E,A every non terminal7 recursive call c generatedc0 holds X Ec ,A = X Ec ,A6.2 Implementing ChooseObsProposition 21 suggests need provide implementation ChooseObsrecursive call c, X Ec ,A = X Ec ,A . Let us examine detail relations[[Obs]]c [[Obs Update]]c .rst step, [[UsefulObs]]c obtained removing [[Obs]]c observationshelp discriminating fault situations. eect expected costte-set, since aect relation indistinguishability.[[Obs Update]]c obtained [[UsefulObs]]c removing observationswhose time label precedes chosen one. expected cost resulting te-set thusdepends time label selected function ChooseObs. following properties:Proposition 22 Let c, denote two independent calls BuildTemporalTreeinput arguments dierent implementations ChooseObs. [[tlabel]]c[[tlabel]]d X Ec ,A X Ed ,A .Proposition 23 Let c call BuildTemporalTree. [[tlabel]]c = tminc = min{t |t, [[ValidObs]]c } X Ec ,A X Ec ,A .dene notion safe time label:Definition 24 Let c denote call BuildTemporalTree. time label saidsafe respect c [[tlabel]]c = implies X Ec ,A = X Ec ,A .immediate consequence propositions 22 23 following:Proposition 25 call c BuildTemporalTree exist time label tmaxcsafe time labels tminc tmaxc , tmincproposition 23.7. exclude terminal calls even compute Obs Update.495fiConsole, Picardi, & Theseider DupreFigure 12 describes ID3ChooseSafeObs, implementation ChooseObs propose. exploits properties proved previous section orderachieve desired task ecient way. Let us examine detail.ID3ChooseSafeObs (Figure 12) computes set safe observations (line 4)chooses among one minimum entropy (line 5). provednow, implementation yields temporal decision tree minimum expectedcost, time exploits entropy order keep tree small.Let us see FindSafeObs (also Figure 12) computes set safe observations. Proposition 23 shows notion safeness tied time labels ratherindividual observations. First FindSafeObs determines range valid timelabels current set examples (line 12); lower bound tlow lowest time labelObs, stored variable low, upper bound tup given globaldeadline Examples, stored variable up.idea nd maximum safe label tmax (variable max) allows useasily build set safe observations (line 21).order accomplish task following steps performed:Given initial te-set E, dened Examples, Obs ActModel, compute X E,A .time label range delimited tlow tup , consider te-set Etdened Examples observations time label equal greatert. compute X Et ,A .soon nd time label X Et ,A > X E,A , know tmax timelabel immediately preceding t.critical operation (in terms eciency) computing expectedcost Et , involves nding quotient set Et /. fact, orderobtain quotient set, need repeatedly partition te-set respectobservations available it.QuotientSet function (Figure 12) performs precisely task. takes inputcurrent time label tlabel, initial partition (possibly made single blockentire te-set) set observations, select valid ones.First partitions input te-set respect observations currenttime label (lines 2831). executes iteratively following operations:partition block checks whether deadline moved time(lines 3638).so, partitions block stores resulting sub-blocksexamination (lines 3941).not, block part Final partition returned (line 42).order simplify task, introduce data type extended partition,partition block stored together highest time label used building it. wayeasily check deadline block allows us exploit observationsnot. Using extended partitions instead standard ones need dene new function,ExtPartition, works way Partition function used Figure 4,also records block highest time label used it.496fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board1234567891011121314151617181920212223242526272829303132333435363738394041424344454647function ID3ChooseSafeObs (set Obs, te-set Examples, action model ActModel)returns observation = sensor, tlabelbeginSafeObs FindSafeObs(Obs, Examples, ActModel);ID3ChooseTest(SafeObs, Examples);return o;end.function FindSafeObs (set Obs, te-set Examples, action model ActModel)returns set observations SafeObsbegincost sitExamples Act(sit);Dl(Examples); low min{t | s, Obs};max up; part {Examples, up};every time label tx starting low beginpart QuotientSet(part, Obs, tx);newcost ExpectedCost(part);newcost < cost begincost newcost; max tx;end;end;SafeObs {s, | max};return SafeObs;end.function QuotientSet (partition Initial, set Obs, time label tlabel)returns rened partition Finalbeginpart Initial;s, = tlabel beginpart ExtPartition(part, s, t);ObsCurr ObsCurr {s, t};end;Final ;part =tmp part ;block, ty part beginnewdl Dl(block);single {block, ty};newdl > ty begins, ty < newdlsingle ExtPartition(single, s, t);tmp part tmp part single;end else Final Final start;end;part tmp part;end;return Final;end.Figure 12: ID3ChooseSafeObs implementation ChooseObs yielding treeminimum expected cost497fiConsole, Picardi, & Theseider DupreNotice QuotientSet needs whole set observations (and valid ones)properly compute result; therefore BuildTemporalTree calls ID3ChooseSafeObsmust pass rst argument UsefulObs instead ValidObs.FindSafeObs exploits QuotientSet nd quotient sets Et ,using ecient approach call backward strategy.First all, notice order observations consideredmatter building quotient set. Moreover, < , Et / renement Et /;words obtain Et / simply rening partition additionalobservations.Thus, compute quotient sets time compute E/.FindSafeObs exactly so: computes quotient sets expected coststarting last time label tup . quotient set built scratch,renement previous one. reason QuotientSet (and ExtPartitionwell) takes rst argument single set, partition. way, quotient setscomputed operations8 needed build E/. next section analyzesdetail complexity issues.6.3 Complexitysection aim showing additional computations needed buildingtemporal decision trees lead higher asymptotical complexitywould get using standard ID3 algorithm set examples (we discussedsection 5.1 circumstances could make approach feasible).Essentially dierence two cases lies presence FindSafeObs function. Wherever BuildTree calls ID3ChooseTest, BuildTemporalTree callsID3ChooseSafeObs, turn calls FindSafeObs ID3ChooseTest.Let us compare FindSafeObs ID3ChooseTest, similar many ways.former repeatedly partitions input te-set respect every available observation;computes entropy partition built way. FindSafeObs buildsone partition exploiting available observations; words instead usingobservation partition initial te-set, exploits order rene existingpartition te-set. Moreover, time label computes expected costpartition built far. Essentially, denote NS number sensorsNT number time labels initial partition, roughly followingcomparison:NS NT (number observations) entropy computations ID3ChooseTest vs.NT expected cost computations FindSafeObs.NS NT partitions initial te-set ID3ChooseTest vs. NS NT renementsexisting partitions initial te-set FindSafeObs.Entropy expected cost computed roughly eort: requireretrieving information element partition block, combineinformation quite straightforward way. complexity task dependsoverall number elements, distributed dierent8. slight overhead due need nd observations used step.498fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardblock partition. even expected cost computed times nerpartitions entropy, thing matters partitionsset, thus involving elements.let us examine problem creating partition. involves retrieving valueelement block initial partition (which dependsnumber elements, number blocks initial partition) properlyassign element new partition block depending original block newvalue. main dierence case starting whole te-set (creation)initial partition (renement), size new blocks created,smaller second case. Dependent implement partition datatype, may make dierence, may take less time renement case. However,never happens renement (corresponding FindSafeObs function) requirestime creation (corresponding ID3ChooseTest function) partition.Therefore claim FindSafeObs function asymptotic complexity function ID3ChooseTest. Thus also ID3ChooseSafeObs asymptotic complexity ID3ChooseTest, conclude BuildTreeasymptotic complexity BuildTemporalTree.6.4 Examplesection shall show algorithm operates te-set Figure 1respect action model Figure 7.Let us summarize information algorithm receives. Eight fault situationsinvolved; moreover exploit three sensors, show dierentqualitative values: h - high, n - normal, l - low, v - low, z - zero. Time labelscorrespond natural numbers ranging 0 7, assume correspondtimes measured internal clock started time fault detection.four basic recovery actions a, b, c, d, b c a. setcompound recovery actions thus = {{a}, {b}, {c}, {b, c}, {d}}; ordering relationpictured 7, together action costs.BuildTemporalTree rst called whole te-set. None two terminating conditions met (notice however two observations useful, since discriminate: s3 , 0 s3 , 1). main function callsID3ChooseSafeObs consequently FindSafeObs. Since global deadline 2must check time labels 0, 1 2, starting last one.Exploiting observations time label 2 obtain following partition:{{sit1 , sit6 }, {sit2 , sit5 , sit7 , sit8 }, {sit3 }, {sit4 }}However order nd expected cost still check partition blockdeadline changed; happens {sit1 , sit6 } well {sit3 } {sit4 }.last two blocks change anything - already contain one element.rst block, deadline 5 thus possible split partition.Therefore obtain partition time label 2 is:Pt=2 = {{sit1 }, {sit6 }, {sit2 , sit5 , sit7 , sit8 }, {sit3 }, {sit4 }}499fiConsole, Picardi, & Theseider Duprending partition, algorithm computes expected cost, turnsbe:1+ (Act(sit6 ))81+ (Act(sit3 )) + (Act(sit4 ))81111= 100 + 10 + 70 + 208828= 57.5XE,t=2 = (Act(sit1 ))11+ (Act({sit2 , sit5 , sit7 , sit8 }))82181+ 508algorithm moves time label 1; starting Pt=2 adds observationstime label 1, obtaining new partition:Pt=1 = {{sit1 }, {sit6 }, {sit2 , sit5 }, {sit7 }, {sit8 }, {sit3 }, {sit4 }}Deadlines move {sit7 } {sit8 }, since singletons new observationscannot split partition. expected cost now:1+ (Act(sit6 ))81+ (Act(sit8 )) + (Act(sit3 ))81111= 100 + 10 + 70 + 208848= 48.75XE,t=1 = (Act(sit1 ))1+ (Act({sit2 , sit5 }))811+ (Act(sit4 ))8811+ +50 + 20 + 508811+ (Act(sit7 ))4818Since XE,t=1 < XE,t=2 conclude observations time label 2 safe.move time label 0, immediately realize new observationschange partition. Thus XE,t=0 = XE,t=1 , safe observations time labelequal either 0 1.algorithm calls function ID3ChooseTest selects observationminimum entropy. Figure 13 shows entropies dierent observations stage,deduce best choice s2 , 1.Entropies {sit1 , sit2 , sit3 , sit4 , sit5 , sit6 , sit7 , sit8 }s1 , 0 1.5s1 , 1 1.5s2 , 0 1.451s2 , 1 0.844Figure 13: Entropies safe observations initial callFigure 15.(a) shows tree point; function BuildTemporalTree recursivelyinvokes four times, yielding:call c1 Ev = {sit1 , sit6 };call c2 En = {sit2 , sit5 };call c3 El = {sit3 , sit4 , sit7 };call c4 Eh = {sit8 }.500fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-BoardLet us focus call c1 : again, none terminating conditions met, thereforealgorithm invokes ID3ChooseSafeObs thus FindSafeObs. Notice howeversubset observations UsefulObs: s1 , 3, s2 , 3, s2 , 4, s2 , 5s3 , 5. global deadline 5.First nd partition (and expected cost) time label 5:Pt=5 = {{sit1 }, {sit6 }}1111XEv ,t=5 = (Act(sit1 )) + (Act(sit6 )) = 100 + 10 = 552222additional observations split partition lower cost; thereforend valid observations also safe. moreover obvious observations entropy, 0. Therefore algorithm non-deterministicallyselect one them; reasonable criterion would select earliest ones,example s1 , 3.Since initial te-set call c1 split two, two recursive calls.However notice BuildTemporalTree called te-set singleelement, rst terminating condition trivially met (all fault situationsrecovery action). function simply returns tree leaf name properrecovery action. Figure 15.(b) shows tree c1 completed.let us examine c2 : algorithm eliminates non-discriminating observations,nds set useful observations empty. Thus builds leaf recoveryaction {b, c}. Let us pass call c3 . case none terminating conditions met:Entropies {sit3 , sit4 , sit7 }s1 , 1 0.667 s1 , 2 0.667 s1 , 3 0.667s2 , 2 0.667 s2 , 3 0.667 s2 , 4 0.667s2 , 50s3 , 10s3 , 20s3 , 30s3 , 40Figure 14: Entropies safe observations call c3algorithm must look safe observations. global deadline 5, startexamining time label 5, nd:Pt=5 = {{sit3 }, {sit4 }, {sit7 }}111XEl ,t=5 = (Act(sit3 )) + (Act(sit4 )) + (Act(sit7 ))333111= 20 + 50 + 20 = 30333Much happened c1 , additional observation split partition,conclude valid observations also safe. Figure 14 shows entropy validobservations; earliest one minimum entropy s3 , 2 algorithm selects.two recursive sub-calls generated immediately terminate: {sit4 } singleton,{sit3 , sit7 } fault situations correspond recovery action.last recursive call, c4 , input singleton thus immediately terminates.nal decision tree pictured Figure 15.(c).501fiConsole, Picardi, & Theseider Duprelows2,1sit1,sit2,sit3,sit4sit5,sit6,sit7,sit8lowlowlownormalnormalhigh?s2,1sit1,sit2,sit3,sit4sit5,sit6,sit7,sit8sit1,sit6s2,3normal?sit3,sit4,sit7??sit2,sit5sit8highsit1sit6high?sit1sit6(a)?sit3,sit4,sit7sit2,sit5?sit8(b)lows2,1sit1,sit2,sit3,sit4sit5,sit6,sit7,sit8normalnormals2,3sit1sit6lowhighhighnormals3,2sit3,sit4highsit7sit1sit6b,ccsit2,sit5sit8bcsit3,sit7sit4(c)Figure 15: output tree dierent stages. (c) shows nal tree.Let us check expected cost really equal expected cost E. Figure 16shows tree leaf l corresponding set fault situations examples(l), probabilityP (examples(l); E) cost (L(l)). Leaves numbered 1 6 going left rightFigure 15.(c).leafl1l2l3l4l5l6examplessit1sit6sit2 , sit5sit8sit3 , sit7sit4P1/81/81/41/81/41/81001070502050Figure 16: Fault situations, probabilities costs leaves tree Figure 15.(c)502fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-BoardThus expected cost tree is:XE,A (T) =6(L(li )) P (examples(l); E)i=1111111100 + 10 + 70 + 50 + 20 + 50884848= 48.75=look back example 8 see XE,A = 48.75, thus minimum possibleexpected cost. Moreover, compare tree T1 example 5. Also T1minimum possible expected cost, compact.7. Conclusionspaper introduced new notion diagnostic decision tree takes accounttemporal information observations temporal constraints recovery actionsperformed. way take advantage discriminatory poweravailable model dynamic system. presented algorithm generatestemporal diagnostic decision trees set examples, discussing also optimaltree generated.automatic compilation decision trees seems promising approach reconciling advantages model-based reasoning constraints imposed on-boardhardware software environment. worth noting trueautomotive domain indeed idea compiling diagnostic rules modelinvestigated also approaches (see e.g., Darwiche, 1999; Dvorak & Kuipers, 1989).Darwiche (1999), particular, discusses rules generated platformsconstrained resources allow direct use model-based diagnostic system.new approach possibility compiling also information concerningsystem temporal behaviour, obtaining way accurate decision procedures.best knowledge, temporal decision trees new notion diagnosticliterature. However, works elds relation ours, sinceaimed learnig rules associations take account time.Geurts Whenkel (1998) propose notion temporal tree used earlyprediction faults. topic closely related diagnosis, albeit dierentways: idea device examination failed yet, observingbehaviour possible predict fault occur. Geurts Wehenkelpropose learn relation observed behavioural patterns consequent failuresinducing temporal tree.notion temporal tree introduced Geurts Wehenkel dierenttemporal decision trees, reecting dierent purpose introduced for. Rathersensor readings, consider general notion test, treespecies time wait performing tests, rather agent running treesupposed wait one tests associated tree node becomes true.Also notion optimality quite dierent: situation described GeurtsWehenkel size resulting tree concern. tree-building algorithms aims503fiConsole, Picardi, & Theseider Dupreminimizing time nal decision taken. algorithm, sizeprimary concern, point view time suces diagnosis carriedwithin certain deadlines. point view time alone, apporach GeurtsWehenkel probably general ours; problem considering trade-odiagnostic capability time needed diagnosis one major extensionsconsidering future work topic (see below).Finally, algorithm proposed Geurts Wehenkel works quite dierentway ours: rst builds tree greedily, using evaluation function weighsdiscriminability power agains time needed reach result, selecting steptexts optimizes function. prunes tree order avoid overtting.hand, approach aim optimizing tree point view cost,time tries keep tree small entropy heuristic. think that, sinceoptimization carried additional cost9 respect minimizationentropy, approach obtain better results, least cases one denenotion deadline.process learning association rules involving time also studiedareas, machine learning (see example Bischof & Caelli, 2001, authorspropose technique learn movements) data mining. specic diagnostictailoring approach makes dicult compare generic learning algorithms, connections data mining may stronger. proposal fact aimsessentially extracting series observations patterns time allowcorrectly diagnose fault: process regarded form temporal classication.preliminary investigation papers area (see Antunes & Oliveira, 2001overview) seems suggest that, whereas analysis temporal sequences datareceived much interest last years, much work done directiondata classication, temporal decision trees could exploited.suggests interesting development work, particular concernsapplicability areas. However, believe algorithm presented needsextended order exploited contexts. particular investigatingfollowing extensions:Deadlines could turned hard soft. Soft deadlines met,rather dene cost associated meeting them. Thus meeting deadlinebecomes option taken account less expensiveperforming recovery action diagnosis complete. One could evendene cost increases time passes expiration deadline.extension would allow model also trade-o discriminabilitypower time needed decision process, believe key makingwork applicable areas.Actions could assumed dierent cost depending fault situation;example action associated fault could become dangerous thus extremelyexpensive performed presence another fault.9. point view asymptotical complexity.504fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardlong term, future work topic aimed widening areasapplicability, investigating deeper details connections elds,fault prevention data mining.8. Acknowledgementswork partially supported EU grant GRD-1999-0058, project IDD(Integrated Diagnosis Design), whose partners are: Centro Ricerche Fiat, DaimlerChrysler, Magneti Marelli, OCCM, PSA, Renault, Technische Universitat Munchen, Universite Paris XIII, Universita di Torino.Appendix A. Proofssection contains proofs propositions, lemmas theorems paper.Proposition 12. Let = r, N, E, L, temporal decision tree compatiblete-set E. Let l1 , . . . , lf N denote leaves . examples(l1 ), . . . , examples(lf )partition E.Proof. Follows immediately denition examples (10), noticingn1 , . . . , nk children p {examples(n1 ), . . . , examples(nk )} partitionexamples(p).Proposition 15 Let = r, N, E, L, denote temporal decision tree, letl1 , . . . , lu leaves.XE,A (T) =u(L(li )) P (examples(l); E)i=1Proof. induction depth T. depth 0 consists singleleaf l 10 holds trivially since examples(l) must equal E P (E; E) = 1.depth > 0 let T1 , . . . , Tk denote direct subtrees c1 , . . . , ck denoteroots. regard Ti autonomous temporal decision tree compatiblete-set Ei = examples(ci ). induction hypothesis that:(13)P (examples(l); E).(L(l)) P (examples(l); Ei ) =(L(l))XEi ,A (Ti ) =P (Ei ; E)l leaf Til leaf TiMoreover denition expected cost:(14) XE,A (T) =kP (L(r) L((r, ci ))) XEi ,A (Ti )i=1P (L(r) L((r, ci ))) = P (examples(ci ); examples(r)) = P (Ei ; E).505fiConsole, Picardi, & Theseider Dupre(13) (14) thus obtain:(15) XE,A (T) =kP (Ei ; E) (L(l))i=1 l leaf Ti=P (examples(l); E)P (Ei ; E)k(L(l)) P (examples(l); E)i=1 l leaf TiSince leaves leaves T1 , . . . , Tk , (15) equivalentthesis.Proposition 16 Let = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU twotemporal decision trees compatible te-set E actions model A.discriminating U XE,A (T) < XE,A (U).Proof. Rewriting equation (10) obtain:(16) XE,A (T) =r(LT (li )) P (examples(l); E) =(17) XE,A (U) =(LT (leafT (sit)))P (sit; E)sitEi=1(LU (leafU (sit)))P (sit; E).sitESince discriminating U, sit E:LT (leafT (sit)) LU (leafU (sit))LT (leafT (sit)) = LU (leafU (sit))least one sit satisfying rst relation. denition follows sit:(LT (leafT (sit))) (LU (leafU (sit)))least one sit:(LT (leafT (sit))) = (LU (leafU (sit)))Therefore compare individual elements two sums (16) (17) observeexists least one sit which:(LT (leafT (sit)))P (sit; E) < (LU (leafU (sit)))P (sit; E)sit(LT (leafT (sit)))P (sit; E) (LU (leafU (sit)))P (sit; E)concludes proof.Theorem 20 Let E te-set actions model A. that:(i) exists decision tree compatible E XE,A (T) = X E,A .(ii) every temporal decision tree compatible E, X E,A XE,A (T).506fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Boardorder prove theorem introduce lemmas.Lemma 26 Let temporal decision tree compatible te-set E. siti sitjimplies leafT (siti ) = leafT (sitj ).Proof.prove siti sitj implies leafT (siti ) = leafT (sitj ),lemma easily follows. Let us suppose leafT (siti ) = leafT (sitj ). meanscommon ancestor n two leaves siti , sitj examples(n)Val(siti , L(n), (n)) = Val(sitj , L(n), (n)). Since siti sitj possible(n) > min{Dl(siti ), Dl(sitj )}. since compatible E must hold (n)Dl(examples(n)) min{Dl(siti ), Dl(sitj )}, contradicts previous statement.Lemma 27 Let E te-set sensors s1 , . . . , sm , time labels t1 , . . . , tlast actionsmodel A. exists temporal decision tree = r, N , E, L, XE,A (T) =X E,A .Proof. order prove thesis construct tree expected costte-set.Let us dene total order observations E follows: s, < , either <= precedes lexicographic ordering. Let us denote o1 , . . . , omaxordered sequence observations thus obtained. shall dene level level (startingroot, level 1) giving value L nodes level h.maximum max + 1 levels, max number observations. Newlevels added nodes level leaves (which shall see happenslevel max + 1). Let thus n node level h , let sih , tih = oh h max.have:leafh = max + 1, Dl(examples(n)) < tih ;ninternal node otherwise.merge({Act(sit | sit examples(n)}) n leaf;L(n) =n internal node.sih(n) = tihn internal node.decision-making agent running tree would essentially take account sensormeasurement time labels either available observationsmust perform recovery action deadline expire.need show XE,A (T) = X E,A .Let l1 , . . . , lu denote leaves T. shall rst prove siti sitjleafT (siti ) = leafT (sitj ), equivalently{examples(l1 ), . . . , examples(lu )} = E/.This, together equations (10) (11) yields thesis.already know lemma 26 siti sitj implies leafT (siti ) = leafT (sitj );need show opposite also true. Let us thus assume siti , sitj examples(l)507fiConsole, Picardi, & Theseider Duprel {l1 , . . . , lu }. Let r = n1 , n2 , . . . , nH , nH+1 = l path rootl. know denition h H, L(nh ), (nh ) = oh ,o1 , . . . , oH observations s, E Dl(examples(l)). Moreover sincesiti , sitj examples(l) h = 1, . . . , H, Val(siti , oh ) = Val(sitj , oh ).two possibilities: either Dl(examples(l)) = min{Dl(siti ), Dl(sitj )},Dl(examples(l)) < min{Dl(siti ), Dl(sitj )}.rst case immediately obtain siti sitj thus siti sitj .second case, must sitk examples(l) Dl(examples(l)) =Dl(sitk ). Moreover, Dl(sitk ) = min{Dl(siti ), Dl(sitk )} = min{Dl(sitj ), Dl(sitk )}. Sinceconsiderations apply also sitk thus siti sitk sitk sitj ; thereforetransitivity siti sitj .Lemma 28 Let = r, N, E, L, decision tree compatible te-set Eactions model A. X E,A XE,A (T ).Proof. Let dened proof lemma 27. order prove thesissuces show either equally10 discriminating (see proposition16). Actually shall show given sit E either L(leafT (sit)) L(leafT (sit))L(leafT (sit)) = L(leafT (sit)).know examples(leafT (sit)) examples(leafT (sit)). fact, let sitelement examples(leafT (sit)) dierent sit itself: constructionsit sit , lemma 26 follows leafT (sit) = leafT (sit ).Let:= {Act(s) | examples(leafT (sit))}, = {Act(s) | examples(leafT (sit))}.Since A, denition merge:merge(A) merge(A)merge(A) = merge(A)Thus L(leafT (sit)) = merge(A) L(leafT (sit)) = merge(A) obtain eitheraction selected weaker selected T, same.prove theorem 20.Proof. Point (i) proved lemma 27, point (ii) corresponds lemma 28.Proposition 21 Let us consider execution BuildTemporalTree startingmain call c0 . initial te-set, want build tree over, E = Ec0= [[ActModel]]c0 . recursive call c, let us denote Ec te-set determined[[Examples]]c [[Obs Update]]c . Then:(1) XE,A ([[T]]c0 ) X E,A(2) XE,A ([[T]]c0 ) = X E,A every non terminal11 recursive call c generatedc0 holds X Ec ,A = X Ec ,A10. Rather intuitively, two trees equally discriminating associate fault situationrecovery action.11. exclude terminal calls even compute Obs Update.508fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-BoardProof. shall prove (1) (2) every recursive call c (rather c0 ).proof induction depth recursion starting c.depth = 0. c terminal, prove XE,A ([[T]]c ) = X E,A .two reasons c may terminal: either (i) fault situations [[Examples]]cassociated recovery action A, (ii) [[ValidObs]]c = .(i) denition 19 have:X E,A =(merge({Act(sit) | sit })) P (; Ec )Ec /=(A) P (; Ec )Ec /= (A)P (; Ec ) = (A).Ec /Since [[T]]c made single leaf l L(l) = also XEc ,A ([[T]]c ) =(A), proves thesis.(ii) [[ValidObs]]c = s, [[Examples]]c , > Dl([[Examples]]c ).Let sit Ec Dl(sit) = Dl([[Examples]]c ). denitionindistinguishability sit Ec sit sit . provesEc / made single equivalence class coincides Ec itself. ThusX Ec ,A = (merge({Act(sit) | sit Ec })). Since [[T]]c made single leaf lL(l) = merge({Act(sit) | sit Ec }), follows XEc ,A ([[T]]c ) = X Ec ,A .depth > 0. c terminal ChooseObs selects observation = s, t.Let v1 , . . . , vk possible values o: c k inner recursive callsBuildTemporalTree, shall denote respectively c1 , . . . , ck .{Ec1 , . . . , Eck } partition Ec .denition expected cost (14) that:XEc ,A ([[T]]c ) =kP (Ec |ovi ; Ec ) XEc |ovi=1,A ([[T]]ci )Ec |ovi Eci dier set observations, [[Obs]]c former[[Obs Update]]c latter. However that:P (Ec |ovi ; Ec ) = P (Eci ; Ec ) since probabilities depend fault situations te-set, observations.XEc |ov ,A ([[T]]ci ) = XEci ,A ([[T]]ci ): expected cost depends also observaitions, [[T]]ci construction contain labels observationsEci .Moreover, since also Ec Ec dier observations P (Eci ; Ec ) = P (Eci ; Ec ).Therefore write:XEc ,A ([[T]]c ) =kP (Eci ; Ec ) XEci ,A ([[T]]ci )i=1509fiConsole, Picardi, & Theseider Dupreorder prove (1), apply induction hypothesis XEci ,A ([[T]]ci ) X Eci ,A )obtain:XEc ,A ([[T]]c )(18)kP (Eci ; Ec ) X Eci ,Ai=1let us work right-hand side expression 18:kP (Eci ; Ec ) X Eci ,A =i=1kP (Eci ; Ec )i=1=(Act()) P (; Eci )Eci /k(Act()) P (; Eci ) P (Eci ; Ec )i=1 Eci /=k(Act()) P (; Ec )i=1 Eci /Notice however {Eci /} partition Ec /; words Ec /belongs exactly one set Eci /. fact, splitting examples according valueone observation cannot split class undistinguishable observations. Thusequality becomes:kP (Eci ; Ec ) X Eci ,A =i=1(Act()) P (; Ec ) = X Ec ,AEc /This, together 18, yields:(19)XEc ,A ([[T]]c ) X Ec ,Amentioned above, dierence Ec Ec formerfewer observations. implies that, sit sit Ec , sit sit Ec well.means Ec / sub-partition12 Ec / following sense:partition every Ec / () = {1 , . . . , k } j existsexactly one j Ec / containing exactly fault situations j . yields:X Ec ,A =(Act()) P (; Ec )Ec /Since j fault situations corresponding j , necessarilyj containing it, have:X Ec ,A =(Act( )) P (; Ec )Ec / ()12. Ec / sub-partition Ec / ordinary sense setobservations.510fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board() fault situations subset ; thus (Act( ))(Act()). obtain:X Ec ,A(Act()) P ( ; Ec )Ec / ()=(Act())Ec /=P ( ; Ec )()(Act()) P (; Ec ) = X Ec ,AEc /Together equation 19, proves (1):XEc ,A ([[T]]c ) X Ec ,Alet us prove (2). induction hypothesis changes 18, thus 19, equalities, thus yielding:(20)XEc ,A ([[T]]c ) = X Ec ,ASince hypothesis (2) X Ec ,A = X Ec ,A , immediately obtain:XEc ,A ([[T]]c ) = X Ec ,Aconcludes proof.Proposition 22 Let c, denote two independent calls BuildTemporalTreeinput arguments dierent implementations ChooseObs.[[tlabel]]c [[tlabel]]d X Ec ,A X Ed ,A .Proof. Follows immediately [[Obs Update]]c [[Obs Update]]d .Proposition 23 Let c call BuildTemporalTree. [[tlabel]]c = tminc =min{t | t, [[ValidObs]]c } X Ec ,A X Ec ,A .Proof. case [[Obs Update]]c = [[UsefulObs]]c , thus removed observations non discriminating ones.Proposition 25 call c BuildTemporalTree exist time label tmaxcsafe time labels tminc tmaxc , tmincproposition 23.Proof. Straightforward.ReferencesAntunes, C., & Oliveira, A. (2001). Temporal data mining: overview. KDD WorkshopTemporal Data Mining, San Francisco.Bischof, W., & Caelli, T. (2001). Learning spatio-temporal relational structures. JournalApplied Intelligence, 15, 707722.Brusoni, V., Console, L., Terenziani, P., & Theseider Dupre, D. (1998). spectrumdenitions temporal model-based diagnosis. Articial Intelligence, 102 (1), 3979.511fiConsole, Picardi, & Theseider DupreCascio, F., Console, L., Guagliumi, M., Osella, M., Sottano, S., & Theseider, D. (1999).Generating on-board diagnostics dynamic automotive systems based qualitativedeviations.. AI Communications, 12 (1), 3344.Cascio, F., & Sanseverino, M. (1997). IDEA (Integrated Diagnostic Expert Assistant)model-based diagnosis car repair centers. IEEE Expert, 12 (6).Console, L., & Dressler, O. (1999). Model-based diagnosis real world: lessons learnedchallenges remaining. Proc. 16th IJCAI, pp. 13931400, Stockholm.Darwiche, A. (1999). compiling system descriptions diagnostic rules. Proc. 10thInt. Work. Principles Diagnosis, pp. 5967.Dvorak, D., & Kuipers, B. (1989). Model-based monitoring dynamic systems. Proc.11th IJCAI, pp. 12381243, Detroit.Foresight-Vehicle (2002).Foresight vehicle automotive roadmap,technologyresearch directions future road vehicles.Tech. rep.,http://www.foresightvehicle.org.uk/initiatives/init01/init01-report.asp.Geurts, P., & Wehenkel, L. (1998). Early prediction electric power system blackoutstemporal machine learning. Proceedings ICML98/AAAI98 WorkshopPredicting future: AI Approaches time series analysis, Madison, July 24-26.Mosterman, P., Biswas, G., & Manders, E. (1998). comprehensive framework modelbased diagnosis. Proc. 9th Int. Work. Principles Diagnosis, pp. 8693.Price, C. (1999). Computer-Based Diagnostic Systems. Springer.Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.Russel, S., & Norvig, P. (1995). Articial Intelligence: Modern Approach. Prentice Hall.Sachenbacher, M., Malik, A., & Struss, P. (1998). electrics emissions: experiencesapplying model-based diagnosis real problems real cars. Proc. 9th Int.Work. Principles Diagnosis, pp. 246253.Sachenbacher, M., Struss, P., & Weber, R. (2000). Advances design implementationobd functions diesel injection systems based qualitative approach diagnosis.SAE 2000 World Congress.512fiJournal Artificial Intelligence Research 19 (2003) 631-657Submitted 10/02; published 12/03AltAltp : Online Parallelization PlansHeuristic State SearchRomeo Sanchez NigendaSubbarao Kambhampatirsanchez@asu.edurao@asu.eduDepartment Computer Science Engineering,Arizona State University, Tempe AZ 85287-5406AbstractDespite near dominance, heuristic state search planners still lag behind disjunctiveplanners generation parallel plans classical planning. reason directlysearching parallel solutions state space planners would require planners branchpossible subsets parallel actions, thus increasing branching factor exponentially.present variant heuristic state search planner AltAlt called AltAltpgenerates parallel plans using greedy online parallelization partial plans. greedyapproach significantly informed use novel distance heuristics AltAltpderives graphplan-style planning graph problem. approachguaranteed provide optimal parallel plans, empirical results show AltAltpcapable generating good quality parallel plans fraction cost incurreddisjunctive planners.1. IntroductionHeuristic state space search planning proved one efficient planningframeworks solving large deterministic planning problems (Bonet, Loerincs, & Geffner,1997; Bonet & Geffner, 1999; Bacchus, 2001). Despite near dominance, one achillesheel remains generation parallel plans (Haslum & Geffner, 2000). Parallel plans allowconcurrent execution multiple actions time step. concurrency likelyimportant progress temporal domains. disjunctive plannersGraphplan (Blum & Furst, 1997) SATPLAN (Kautz & Selman, 1996) GPCSP (Do & Kambhampati, 2000) seem trouble generating parallel plans,planners search space states overwhelmed task. main reasonstraightforward methods generation parallel plans would involve progressionregression sets actions. increases branching factor search spaceexponentially. Given n actions, branching factor simple progression regressionsearch bounded n, progression regression search parallel plansbounded 2n .inability state search planners producing parallel plans notedliterature previously. Past attempts overcome limitation successful. Indeed, Haslum Geffner (2000) consider problem generating parallelplans using regression search space states. note resulting planner, HSP*p, scales significantly worse Graphplan. present TP4 (Haslum &Geffner, 2001), addition aimed actions durations, also improvesc2003AI Access Foundation. rights reserved.fiSanchez & Kambhampatibranching scheme HSP*p, making incremental along lines Graphplan.Empirical studies reported Haslum Geffner (2001), however indicate evennew approach, unfortunately, scales quite poorly compared Graphplan variants. Informally, achilles heel heuristic state search planners interpreted sortlast stand disjunctive planners capable generating parallel plansefficiently.Given way efficiently generating optimal parallel plans involves usingdisjunctive planners, might want consider ways generating near-optimal parallelplans using state search planners. One obvious approach post-process sequentialplans generated state search planners make parallel. easilydone - using approaches explored Backstrom (1998), one drawbackapproaches limited transforming sequential plan given input.Parallelization sequential plans often results plans close optimal parallelplans.1alternative, explore paper, involves incremental online parallelization.Specifically, planner AltAltp , variant AltAlt planner (Sanchez, Nguyen,& Kambhampati, 2000; Nguyen, Kambhampati, & Sanchez, 2002), starts searchspace regression single actions. promising single action regressselected, AltAltp attempts parallelize (fatten) selected search branchindependent actions. parallelization done greedy incremental fashion actions considered addition current search branch based heuristic costsubgoals promise achieve. parallelization continues next stepstate resulting addition new action better heuristic cost.sub-optimality introduced greedy nature parallelization offsetextent plan-compression procedure called Pushup tries rearrange evolvingparallel plans pushing actions higher levels search branch (i.e. later stagesexecution) plan.Despite seeming simplicity approach, proven quite robustpractice. fact, experimental comparison five competing planners - STAN (Long& Fox, 1999), LPG (Gerevini & Serina, 2002), Blackbox (Kautz & Selman, 1996), SAPA (Do& Kambhampati, 2001) TP4 (Haslum & Geffner, 2001) - shows AltAltp viablescalable alternative generating parallel plans several domains. many problems,AltAltp able generate parallel plans close optimal makespan. alsoseems retain efficiency advantages heuristic state search disjunctive planners,producing plans fraction time taken disjunctive planners many cases.AltAltp also found superior post-processing approaches. Specifically,compared AltAltp approach involves post-processing sequential plansgenerated AltAlt using techniques Backstrom (1998). found AltAltpable generate shorter parallel plans many cases. Finally, show AltAltp incurslittle additional overhead compared AltAlt.rest paper, discuss implementation evaluation approachgenerate parallel plans AltAltp . Section 2 starts providing reviewAltAlt planning system, AltAltp based. Section 3 describes generation1. empirically demonstrate later; curious readers may refer plots Figure 15.632fiOnline Parallelization Plans Heuristic State SearchAction TemplatesSerial PlanningGraphGraphplanPlan Extension Phase(based STAN)Problem Spec(Init, Goal state)ExtractionHeuristicsActionsLast LevelAltAltHeuristicsRegression Planner(based HSP-R)Solution PlanFigure 1: Architecture AltAltparallel plans AltAltp . Section 4 presents extensive empirical evaluation AltAltp .evaluation includes comparison ablation studies. Finally, Section 5 discussesrelated work classical well metric temporal planning. Section 6 summarizescontributions.2. AltAlt Background Architecture HeuristicsAltAlt planning system based combination Graphplan (Blum & Furst, 1997;Long & Fox, 1999; Kautz & Selman, 1999) heuristic state space search (Bonet et al.,1997; Bonet & Geffner, 1999; McDermott, 1999) technology. AltAlt extracts powerfulheuristics planning graph data structure guide regression search spacestates. high level architecture AltAlt shown Figure 1. problem specificationaction template description first fed Graphplan-style planner (in case,STAN Long & Fox, 1999), constructs planning graph problempolynomial time (we assume reader familiar Graphplan algorithm Blum& Furst, 1997). planning graph structure fed heuristic extractor modulecapable extracting variety effective heuristics (Nguyen & Kambhampati,2000; Nguyen et al., 2002). heuristics, along problem specification,set ground actions final action level planning graph structure fedregression state-search planner.explain operation AltAlt detailed level, need providebackground various components. shall start regression searchmodule. Regression search process searching space potential plan suffixes. suffixes generated starting goal state regressingset relevant action instances domain. resulting states (nondeterministically) regressed relevant action instances, process repeatedreach state (set subgoals) satisfied initial state. stateframework set (conjunction of) literals seen subgoals needmade true way achieving top level goals. action instance consideredrelevant state effects give least one element delete633fiSanchez & Kambhampatielement S. result regressing (S\ef f (a)) prec(a) -essentially set goals still need achieved application a,everything would achieved applied. relevant actiona, separate search branch generated, result regressing actionnew fringe branch. Search terminates success node every literalstate corresponding node present initial state problem.crux controlling regression search involves providing heuristic functionestimate relative goodness states fringe current search treeguide search promising directions. heuristic function needs evaluatecost achieving set subgoals (comprising regressed state) initialstate. words, heuristic computes length plan needed achievesubgoals initial state. discuss heuristic computedplanning graph, which, provides optimistic reachability estimates.Normally, planning graph data structure supports parallel plans - i.e., plansstep one action may executed simultaneously. Since want planning graph provide heuristics regression search module AltAlt, generatessequential solutions, first make modification algorithm generatesserial planning graph. serial planning graph planning graph which, additionnormal mutex relations, every pair non-noop actions level markedmutex. additional action mutexes propagate give additional propositional mutexes. Finally, planning graph said level change action,proposition mutex lists two consecutive levels.assume given problem, Graphplan module AltAlt usedgenerate expand serial planning graph levels off. discussed Sanchezet al. (2000), relax requirement growing planning graph level-off,tolerate graded loss informedness heuristics derived planning graph.start notion level set propositions:Definition 1 (Level) Given set propositions, lev(S) index first levelleveled serial planning graph propositions appear non-mutexone another. singleton, lev(S) index first levelsingleton element occurs. level exists, lev(S) = planning graphgrown level-off.intuition behind definition level literal p serial planninggraph provides lower bound length plan (which, serial planning graph,equal number actions plan) achieve p initial state. Usinginsight, simple way estimating cost set subgoals sum levels.Heuristic 1 (Sum heuristic) hsum (S) :=PpSlev({p})sum heuristic similar greedy regression heuristic used UNPOP (McDermott, 1999) heuristic used HSP planner (Bonet et al., 1997). mainlimitation heuristic makes implicit assumption subgoals (elementsS) independent. hsum heuristic neither admissible particularly informedignores interactions subgoals. develop effective heuristics,634fiOnline Parallelization Plans Heuristic State Searchneed consider positive negative interactions among subgoals limitedfashion.(Nguyen et al., 2002), discuss variety ways using planning graphincorporate negative positive interactions heuristic estimate, discussrelative tradeoffs. One best heuristics according analysis heuristic calledhAdjSum2M . adopted heuristic default heuristic AltAlt. basic ideahAdjSum2M adjust sum heuristic take positive negative interactionsaccount. heuristic approximates cost achieving subgoals setsum cost achieving S, considering positive interactions ignoring negativeinteractions, plus penalty ignoring negative interactions. first componentRP (S) computed length relaxed plan supporting S,extracted ignoring mutex relations. approximate penalty inducednegative interactions alone, proceed following argument. Consider pairsubgoals p, q S. negative interactions p q, lev({p, q}),level p q present together, exactly maximum lev(p) lev(q).degree negative interaction p q thus quantified by:(p, q) = lev({p, q}) max (lev(p), lev(q))want use - values characterize amount negative interactionspresent among subgoals given set S. subgoals pair-wise independent,clearly, values zero, otherwise pair subgoals differentvalue. largest value among pair subgoals used measurenegative interactions present heuristic hAdjSum2M . summary,Heuristic 2 (Adjusted 2M) hAdjSum2M (S) := length(RP (S)) + maxp,qS (p, q)analysis Nguyen et al. (2002) shows one robust heuristicsterms solution time quality. thus default heuristic used AltAlt(as well AltAltp ; see below).3. Generation Parallel Plans Using AltAltpobvious way make AltAlt produce parallel plans would involve regressing subsets(non interfering) actions. Unfortunately, increases branching factor exponentiallyinfeasible practice. Instead, AltAltp uses greedy depth-first approach makesuse heuristics regress single actions, incrementally parallelizes partial planstep, rearranging partial plan later necessary.high level architecture AltAltp shown Figure 2. Notice heuristicextraction phase AltAltp similar AltAlt, one important modification. contrast AltAlt uses serial planning graph basisheuristic (see Section 2), AltAltp uses standard parallel planning graph. makessense given AltAltp interested parallel plans AltAlt aimed generatingsequential plans. regression state-search engine AltAltp also differentsearch module AltAlt. AltAltp augments search engine AltAlt 1) fatteningstep 2) plan compression procedure (Pushup). details proceduresdiscussed below.635fiSanchez & KambhampatiAction TemplatesParallelPlanningGraphGraphplanPlan Extension Phase(based STAN)ExtractionHeuristicsActionsLast LevelAltAltpHeuristicsProblem Spec(Init, Goal state)NodeExpansion(Fattening)Node OrderingSelectionPlanCompressionAlgorithm(PushUp)Solution PlanFigure 2: Architecture AltAltpA={a 1 ,a 2 ,...,a p ,...a }a1a2S1S2ap. ..SpFigure 3: AltAltp Notation636. ..SmfiOnline Parallelization Plans Heuristic State Searchparexpand(S)get set applicable actions current stateforall aiSi Regress(S,ai )CHILDREN(S) CHILDREN(S) + SiSp state among Children(S) minimumhadjsum2M valueap action regresses Sp/**Fattening process{ ap }forall g ranked decreasing order level(g)Find action ag supporting g ag 6ai pairwise independent action O.multiple actions, pick oneminimum hadjsum (Regress(S, + ag )) among aghadjsum2M (S, + ai ) < hadjsum2M (S, O)+ agSpar Regress(S, O)CHILDREN(S) CHILDREN(S) + Sparreturn CHILDRENEND;Figure 4: Node Expansion Proceduregeneral idea AltAltp select fringe action ap among actionsused regress particular state stage search (see Figure 3). Then,pivot branch given action ap fattened adding actions A,generating new state consequence regression multiple parallel actions.candidate actions used fattening pivot branch must (a) come siblingbranches pivot branch (b) pairwise independent actionscurrently pivot branch. use standard definition action independence: twoactions a1 a2 considered independent state resulting regressingactions simultaneously obtained applying a1 a2 sequentiallypossible linearizations. sufficient condition preconditionseffects actions interfere:((|prec(a1 )| |ef f (a1 )|) (|prec(a2 )| |ef f (a2 )|)) =|L| refers non-negated versions literals set L. discussdetails pivot branch selected first place, branchincrementally fattened.Selecting Pivot Branch: Figure 4 shows procedure used select parallelizepivot branch. procedure first identifies set regressable actionscurrent node S, regresses them, computing new children states. Next,action leading child state lowest heuristic cost among new childrenselected pivot action ap , corresponding branch becomes pivot branch.637fiSanchez & KambhampatiAt(pack1,ASU)At(pack2,ASU)At(pack3,ASU)At(pack4, Home)H=23am: Unload(pack4,airp2,Home)ap: Unload(pack1,airp1,ASU)a1: Unload(pack1,airp2,ASU)SpH=21PivotUnload(pack2,airp1,ASU)Unload(pack2,airp2,ASU)Unload(pack3,airp1,ASU)Unload(pack3,airp2,ASU)Unload(pack4,airp2,HOME)S1H=21...SmH=22Possible PairwiseIndependent ActionsFigure 5: regression state, identify P ivot related setpairwise independent actions.heuristic cost states computed hadjsum2M heuristic AltAlt,based parallel planning graph. Specifically, context discussionhadjsum2M heuristic end Section 2, compute (p, q) values, turndepend level(p), level(q) level(p, q) terms levels parallel planninggraph rather serial planning graph. easy show level setconditions parallel planning graph less equal level serialplanning graph. length relaxed plan still computed terms numberactions. show later (see Figure 19(a)) change improve qualityparallel plans produced AltAltp .search algorithm used AltAltp similar used HSPr (Bonet & Geffner,1999) - hybrid greedy depth first weighted A* search. goes depthfirst long heuristic cost children states lowercurrent state. Otherwise, algorithm resorts weighted A* search select nextnode expand. latter case, evaluation function used rank nodesf (S) = g(S) + w h(S) g(S) length current partial plan termsnumber steps, h(S) estimated cost given heuristic function (e.g. hAdjSum2M ),w weight given heuristic function. w set 5 based empiricalexperience.2Breaking Ties: case tie selecting pivot branch, i.e., one branchleads state lowest heuristic cost, break tie choosing action2. role w Best-First search see (Korf, 1993).638fiOnline Parallelization Plans Heuristic State Searchsupports subgoals harder achieve. Here, hardness literal l measuredterms level planning graph l first appears. standard rationaledecision (c.f. Kambhampati & Sanchez, 2000) want fail fasterconsidering difficult subgoals first. additional justification case,also know subgoal higher level value requires steps actionsachievement appeared later planning graph. So, supportingfirst, may able achieve easier subgoals along way thereby reducenumber parallel steps partial plan.Fattening Pivot Branch: Next procedure needs decide subsetsibling actions pivot action ap used fatten pivot branch.obvious first idea would fatten pivot branch maximally adding pairwiseindependent actions found search stage. problem ideamay add redundant heuristically inferior actions branch, satisfyingpreconditions may lead increase number parallel steps.So, order avoid fattening pivot branch irrelevant actions,adding action O, require heuristic cost state resultsregressing + strictly lower S. additionrequirement pairwise independent current set actions O.simple check also ensures add one action supportingset subgoals S.overall procedure fattening pivot branch thus involves picking nexthardest subgoal g (with hardness measured terms level subgoalplanning graph), finding action ag achieving g, pair-wise independentactions which, added used regress S, leads statelowest heuristic cost, consequence lower cost S.found, ag added O, procedure repeated. oneaction ag , break ties considering degree overlappreconditions action ag set actions currently O. degree preconditionoverlap defined |prec(a) {oO prec(o)}|. action higherdegree overlap preferred reduce amount additional work needestablish preconditions. Notice fattening process, searchnode may multiple actions leading parent, multiple actions leadingchildren.Example: Figure 5 illustrates use node expansion procedure problemlogistics domain (Bacchus, 2001). example four packages pack1,pack2, pack3 pack4. goal place first three ASUremaining one home. two planes airp1 airp2 carry plans.figure shows first level search regressed. also shows pivotaction ap given unload(pack1,airp1,ASU), candidate set pairwise independentactions respect ap . Finally, see Figure 6 generation parallelbranch. Notice node seen partial regressed plan. describedparagraphs above, actions regressing lower heuristic estimates considered aparfatten pivot branch. Notice action unload(pack4,airp2,Home)discarded leads state higher cost, even though inconsistent639fiSanchez & KambhampatiAt(pack1,ASU)At(pack2,ASU)At(pack3,ASU)At(pack4, Home)apar: Unload(pack1,airp1,ASU)Unload(pack2,airp1,ASU)Unload(pack3,airp1,ASU)H=23am: Unload(pack4,airp2,Home)a1: Unload(pack1,airp2,ASU)ap: Unload(pack1,airp1,ASU)SparH=19SpS1H=21H=21Pivot...SmH=22Unload(pack1,airp1,ASU)Unload(pack2,airp1,ASU)Unload(pack2,airp2,ASU)Unload(pack3,airp1,ASU)Unload(pack3,airp2,ASU)Unload(pack4,airp2,HOME)Figure 6: Spar result incrementally fattening P ivot branch pairwiseindependent actionsrest actions chosen fatten pivot branch. Furthermore, alsosee preferred actions using plane airp1, since overlappivot action ap .Offsetting Greediness Fattening: fattening procedure greedy, sinceinsists state resulting fattening strictly better heuristic value.useful avoiding addition irrelevant actions plan, procedure alsosometimes preclude actions ultimately relevant discardedheuristic perfect. actions may become part plan later stagessearch (i.e., earlier parts execution eventual solution plan; sincesearching space plan suffixes). happens, length parallel planlikely greater, since steps may needed support preconditionsactions would forced come even later stages search (earlier partsplan). action allowed partial plan earlier search (i.e., closerend eventual solution plan), preconditions could probably achievedparallel subgoals plan, thus improving number steps.order offset negative effect greediness, AltAltp re-arranges partial planpromote actions higher search branch (i.e., later parts executioneventual solution plan). Specifically, expanding given node S, AltAltp checkssee actions leading parent node (i.e., Figure 6 showsApar leads Spar ) pushed higher levels search branch. online640fiOnline Parallelization Plans Heuristic State SearchpushUP(S)get actions leadingforallx0Sx get parent node/** Getting highest ancestor actionLoopAx get actions leading Sx(parallel(a, Ax ))x x+1Sx get parent node Sx1Elseaj get action conflicting Ax(Secondary Optimizations)Remove aj branchInclude anew necessaryElseAx1 Ax1 +breakEnd Loop/**Adjusting partial planSx get highest ancestor x historycreateN ewBranchF rom(Sx )x > 0Snew regress Sx Ax1Sx Snewx x1END;Figure 7: Pushup Procedurere-arrangement plan done Pushup procedure, shown Figure 7.Pushup procedure called time node gets expanded, trycompress partial plan. actions find highest ancestor nodeSx search branch action applied (i.e., gives literalSx without deleting literals Sx , pairwise independent actionsAx currently leading Sx , words condition parallel(a, Ax ) satisfied).Sx found, removed set actions leading introducedset actions leading Sx (to child current search branch). Next,states search branch Sx adjusted reflect change. adjustmentinvolves recomputing regressions search nodes Sx . first glance,might seem like transformation questionable utility since preconditions (andregressions) become part descendants Sx , necessarilyreduce length plan. however expect length reduction actionssupporting preconditions get pushed eventually later expansions.641fiSanchez & KambhampatiAt(pack1,ASU)At(pack2,ASU)At(pack3,ASU)At(pack4, Home)Unload(pack1,airp1,ASU)Unload(pack2,airp1,ASU)Unload(pack3,airp1,ASU)H=23Unload(pack4,airp2,Home)Unload(pack1,airp2,ASU)Unload(pack1,airp1,ASU)SparSpH=19...S1H=21H=21SmH=22Unload(pack4,airp2,Home)fly(airp1,LocX,ASU)SparSp...SmH=18PivotUnload(pack4,airp2,Home)fly(airp1,LocX,ASU)H=18H=16(a) Finding highest ancestor node actionpushed up.Unload(pack1,airp1,ASU)Unload(pack2,airp1,ASU)Unload(pack3,airp1,ASU)Unload(pack4,airp2,HOME)At(pack1,ASU)At(pack2,ASU)At(pack3,ASU)At(pack4, Home)H=23Unload(pack4,airp2,Home)Unload(pack1,airp1,ASU)Unload(pack2,airp1,ASU)Unload(pack3,airp1,ASU)Unload(pack1,airp2,ASU)Unload(pack1,airp1,ASU)SparSnewSpH=19S1H=21SmH=21fly(airp1,LocX,ASU)fly(airp1,LocX,ASU)...SnewSparSpH=16fly(airp1,LocX,ASU)Unload(pack4,airp2,Home)H=18PivotH=22Unload(pack4,airp2,Home)...SmH=18H=16(b) Pushup procedure generates new search branch.Figure 8: Rearranging Partial Plan642fiOnline Parallelization Plans Heuristic State SearchRather doctor existing branch, current implementation, addnew branch Sx reflects changes made Pushup procedure.3 newbranch becomes active search branch, leaf node expanded next.Aggressive Variation Pushup: Pushup procedure, described above,expensive affects current search branch, operations involvedrecomputing regressions branch. course, possible aggressivemanipulating search branch. example, applying action ancestorSx set literals child state, say Snew changes, thus additional actions maybecome relevant expanding Snew . principle, could re-expand Snew lightnew information. decided go re-expansion option, typicallyseem worth cost. Section 4.3, compare default version Pushupprocedure variant re-expands nodes search branch, resultsstudies support decision avoid re-expansion. Finally, although introducedPushup procedure add-on fattening step, also used independentlatter, case net effect would incremental parallelization sequentialplan.Example: Figure 8(a), two actions leading node Spar (at depth two),two actions Unload(pack4,airp2,Home) fly(airp1,LocX,ASU). So,expanding Spar check two actions leading pushed up.second action pushable since interacts actions ancestor node, firstone is. find highest ancestor partial plan interacts pushableaction. example root node ancestor. So, insert pushableaction Unload(pack4,airp2,Home) directly root node. re-adjuststate Spar Snew depth 1, shown Figure 8(b), adding new branch, reflectingchanges states below. Notice action Unload(pack4,airp2,Home)initially discarded greediness fattening procedure (see Figure 6),offset negative effect plan compression algorithm. see alsore-expanded state Snew depth 1, made adjustmentspartial plan using actions already presented search trace.44. Evaluating Performance AltAltpimplemented AltAltp top AltAlt. tested implementation suiteproblems used 2000 2002 AIPS competition (Bacchus, 2001; Long& Fox, 2002), well benchmark problems (McDermott, 2000). experimentsbroadly divided three sets, aimed comparing performance AltAltpdifferent scenarios:1. Comparing performance AltAltp planning systems capable producingparallel plans.3. way data structures set up, adding new branch turns robustoption manipulating existing search branch.4. Instead, aggressive Pushup modification would expand Snew depth 1, generating similar statesgenerated expansion Spar depth.643fiSanchez & Kambhampati80706035Gripper AIPS-98AltAlt-pSTANTP4BlackboxLPG 2ndElevator AIPS-00AltAlt-pSTAN30BlackboxLPG 2nd2550Steps20Steps40153010205100012345Problems6718357911 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49Problems(b)(a)Figure 9: Performance Gripper (AIPS-98) Elevator (AIPS-00) Domains.121400AltAlt-pSTANBlackbox1012001000Steps8Time80066004400220000147 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 701Problems4710 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70Problems(b)(a)Figure 10: Performance Schedule domain (AIPS-00)2. Comparing incremental parallelization technique AltAlt + Post-Processing.3. Ablation studies analyze effect different parts AltAltp approachoverall performance.experiments done Sun Blade-100 workstation, running SunOS 5.81GB RAM. Unless noted otherwise, AltAltp run hadjsum2M heuristic644fiOnline Parallelization Plans Heuristic State Search90450Altalt-pSTANTP4BlackboxLPG 2nd807035030050TimeSteps60400250402003015020100501000135719 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 614710 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61ProblemsProblems(a)(b)Figure 11: Performance Logistics domain(AIPS-00)35800AltAlt-pSTANTP4BlackboxLPG 2ndSapa3025700600400TimeSteps5002015300102005100012345678910111213140151Problems234(a)56789Problems101112131415(b)Figure 12: Performance DriverLog domain(AIPS-02)described section 2 paper, parallel planning graph grown firstlevel top-level goals present without mutex. times seconds.4.1 Comparing AltAltp Competing Approachesfirst set experiments compared performance plannerresults obtained running STAN (Long & Fox, 1999), Blackbox (Kautz & Selman,645fiSanchez & Kambhampati1999), TP4 (Haslum & Geffner, 2001), LPG (Gerevini & Serina, 2002) SAPA (Do &Kambhampati, 2001). Unless noted otherwise, every planner run defaultsettings. planners could run domains due parsing problemsmemory allocation errors. cases, omit planner considerationparticular domains.4.1.1 Planners Used Comparison StudiesSTAN disjunctive planner, optimized version Graphplan algorithmreasons invariants symmetries reduce size search space. Blackboxalso based Graphplan algorithm works converting planning problemsspecified STRIPS (Fikes & Nilsson, 1971) notation boolean satisfiability problemssolving using SAT solver (the version used defaults SATZ).5 LPG (Gerevini& Serina, 2002) judged best performing planner 3rd International PlanningCompetition (Long & Fox, 2002), planner based planning graphs localsearch inspired Walksat approach. LPG run default heuristicssettings. Since LPG employs iterative improvement algorithm, quality plansproduced improved running multiple iterations (thus increasingrunning time). make comparisons meaningful, decided run LPG twoiterations (n=2), since beyond that, running time LPG generally worseAltAltp . Finally, also chosen two metric temporal planners, ablerepresent parallel plans representation time durative actions.TP4 (Haslum & Geffner, 2001) temporal planner based HSP*p (Haslum & Geffner,2000), optimal parallel state space planner IDA* search algorithm.last planner list SAPA (Do & Kambhampati, 2001). SAPA powerfuldomain-independent heuristic forward chaining planner metric temporal domainsemploys distance-based heuristics (Kambhampati & Sanchez, 2000) control search.4.1.2 Comparison Results Different Domainsrun planners Gripper domain International PlanningScheduling competition 1998 (McDermott, 2000), well three different domains(Logistics, Scheduling, Elevator-miconic-strips) 2000 (Bacchus, 2001), three2002 competition (Long & Fox, 2002) - DriverLog, ZenoTravel, Satellite.cases multiple versions domain, used STRIPS Untypedversions.6 . discuss results domains below.Gripper: Figure 9(a), compare performance AltAltp Gripper domain (McDermott, 2000) rest planners excluding SAPA. plot showsresults terms number (parallel) steps. see even simplistic domain, AltAltp LPG planners capable scaling generating parallel5. chosen IPP (Koehler, 1999), also optimized Graphplan planning systemresults reported Haslum Geffner (2001) show already less efficient STAN.6. Since SAPA read STRIPS file format, run SAPA planner equivalent problemsunit-duration actions Long Fox (2002).646fiOnline Parallelization Plans Heuristic State Search500040AltAlt-pSTANLPG 2ndSapa3545004000303500253000StepsTime20152500200015001010005500001234567891011121314115Problems23456789101112131415Problems(b)(a)Figure 13: Performance ZenoTravel domain (AIPS-02)plans. None approaches able solve four problems.7 AltAltpable scale without difficulty problems involving 30 balls. Furthermore, AltAltpreturns better plans LPG.Elevator: Figure 9(b), compare AltAltp STAN, Blackbox LPG Elevatordomain (Miconic Strips) (Bacchus, 2001).8 AltAltp approached quality solutionsproduced optimal approaches (e.g. Blackbox STAN). Notice Blackboxsolve around half problems solved AltAltp domain.Scheduling: Results Scheduling domain shown Figure 10. BlackboxSTAN considered comparison.9 AltAltp seems reasonably approximateoptimal parallel plans many problems (around 50 them), produce significantlysuboptimal plans some. However, able solve problemstwo approaches fraction time.Logistics: plots corresponding Logistics domain Bacchus (2001) shownFigure 11.10 difficult problems AltAltp outputs lower qualitysolutions optimal approaches. However, AltAltp LPG able scalecomplex problems, easily see AltAltp provides better qualitysolutions LPG. AltAltp also seems efficient approaches.7. Although STAN supposed able generate optimal step-length plans, handful casesseems produced nonoptimal solutions Gripper Domain. explanationbehavior, informed authors code.8. include traces TP4 pre-processor planner able readdomain.9. TP4 pre-processor cannot read domain, LPG runs memory, SAPA parsingproblems.10. SAPA excluded due parsing problems.647fiSanchez & Kambhampati500060AltAlt-pSTANTP4Blackbox5045004000LPG 2ndSapa4035003000StepsTime30250020002015001000105000012345678910 11 12 13 14 15 16 17 18 1912345678910111213141516171819ProblemsProblems(b)(a)Figure 14: Performance Satellite domain(AIPS-02)LPG solutions problems 49 61 obtained one iteration, since LPGable complete second iteration reasonable amount time. explainslow time taken LPG, lower quality solutions.DriverLog: see Figure 12(a) AltAltp reasonably well terms qualityrespect approaches DriverLog domain. Every planner consideredtime. AltAltp one two planners able scale up. Figure 12(b) shows alsoAltAltp efficient planners.Zeno-Travel: AltAltp , SAPA, LPG able solve problemsdomain.11 AltAltp solves efficiently (Figure 13(b)) providing good solutionquality (Figure 13(a)) compared temporal metric planners.Satellite: results Satellite domain shown Figure 14. Although everyplanner considered, AltAltp , SAPA, LPG solve problems. SAPAsolves problems produces lower quality solutions many them. AltAltp producesbetter solution quality SAPA, also efficient. However, AltAltp produceslower quality solutions LPG four problems. LPG cannot solve one problemsproduces lower quality solutions 5 them.Summary: summary, note AltAltp significantly superior elevatorgripper domains. also performs well DriverLog, ZenoTravel, Satellitedomains 2002 competition (Long & Fox, 2002). performance plannerssimilar Schedule domain. Logistics domain, quality AltAltp planssecond Blackbox problems optimal planner solve.However, scales along LPG bigger size problems, returning good step11. Blackbox TP4 able parse domain.648fiOnline Parallelization Plans Heuristic State Search250900AltAlt-PostProcAltAlt-pAltAlt800200700600150TimeSteps5004001003002005010000147 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 6114710 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61ProblemsProblems(a)(b)Figure 15: AltAlt Post-Processing vs. AltAltp (Logistics domain)45060AltAltAltAlt-PostProcAltAlt-p50400350300Time40Steps302502001502010010500012345678Problems9101112131411523456789101112131415Problems(a)(b)Figure 16: AltAlt Post-Processing vs. AltAltp (Zenotravel domain)length quality plans. TP4, heuristic state search regression planner capableproducing parallel plans able scale domains. SAPA, heuristicsearch progression planner, competitive, still outperformed AltAltp planningtime solution quality.649fiSanchez & KambhampatiSOLUTION: solution found (length = 9)Time 1: load-truck(obj13,tru1,pos1) Level: 1Time 1: load-truck(obj12,tru1,pos1) Level: 1Time 1: load-truck(obj11,tru1,pos1) Level: 1Time 2: drive-truck(tru1,pos1,apt1,cit1) Level: 1Time 3: unload-truck(obj12,tru1,apt1) Level: 3Time 3: fly-airplane(apn1,apt2,apt1) Level: 1Time 3: unload-truck(obj11,tru1,apt1) Level: 3Time 4: load-airplane(obj12,apn1,apt1) Level: 4Time 4: load-airplane(obj11,apn1,apt1) Level: 4Time 5: load-truck(obj21,tru2,pos2) Level: 1Time 5: fly-airplane(apn1,apt1,apt2) Level: 2Time 6: drive-truck(tru2,pos2,apt2,cit2) Level: 1Time 6: unload-airplane(obj11,apn1,apt2) Level: 6Time 7: load-truck(obj11,tru2,apt2) Level: 7Time 7: unload-truck(obj21,tru2,apt2) Level: 3Time 8: drive-truck(tru2,apt2,pos2,cit2) Level: 2Time 9: unload-airplane(obj12,apn1,apt2) Level: 6Time 9: unload-truck(obj13,tru1,apt1) Level: 3Time 9: unload-truck(obj11,tru2,pos2) Level: 9Total Number actions Plan: 19POST PROCESSED PLAN ...Time: 1 : load-truck(obj13,tru1,pos1)Time: 1 : load-truck(obj12,tru1,pos1)Time: 1 : load-truck(obj11,tru1,pos1)Time: 1 : fly-airplane(apn1,apt2,apt1)Time: 1 : load-truck(obj21,tru2,pos2)Time: 2 : drive-truck(tru1,pos1,apt1,cit1)Time: 2 : drive-truck(tru2,pos2,apt2,cit2)Time: 3 : unload-truck(obj12,tru1,apt1)Time: 3 : unload-truck(obj11,tru1,apt1)Time: 3 : unload-truck(obj21,tru2,apt2)Time: 3 : unload-truck(obj13,tru1,apt1)Time: 4 : load-airplane(obj12,apn1,apt1)Time: 4 : load-airplane(obj11,apn1,apt1)Time: 5 : fly-airplane(apn1,apt1,apt2)Time: 6 : unload-airplane(obj11,apn1,apt2)Time: 6 : unload-airplane(obj12,apn1,apt2)Time: 7 : load-truck(obj11,tru2,apt2)Time: 8 : drive-truck(tru2,apt2,pos2,cit2)Time: 9 : unload-truck(obj11,tru2,pos2)END POST PROCESSING: Actions= 19 Length: 9(a) AltAltp Solution(b) AltAltp plus Post-processingFigure 17: Plots showing AltAltp solutions cannot improved anymore Postprocessing.4.2 Comparison Post-Processing Approachesmentioned earlier (see Section 1), one way producing parallel plansstudied previously literature post-process sequential plans (Backstrom, 1998).compare online parallelization post-processing, implemented Backstrom (1998)sMinimal De-ordering Algorithm, used post-process sequential plans producedAltAlt (running default heuristic hAdjSum2M using serial planning graph).section compare online parallelization procedure offline method.first set experiments Logistics domain (Bacchus, 2001). resultsshown Figure 15. expected, original AltAlt longest plans since allowsone action per time step. plot shows post-processing techniques helpreducing makespan plans generated AltAlt. However, also noticeAltAltp outputs plans better makespan either AltAlt AltAlt followed postprocessing. shows online parallelization better approach post-processingsequential plans. Moreover, plot Figure 15(b) shows time taken AltAltplargely comparable taken two approaches. fact, muchadditional cost overhead procedure.Figure 16 repeats experiments ZenoTravel domain (Long & Fox, 2002).again, see AltAltp produces better makespan post-processing sequentialplans AltAlt. Notice time, AltAlt plus post-processing clearly less efficient650fiOnline Parallelization Plans Heuristic State Search140Logistics AIPS-00600AltAlt-pAltAlt-p NoPushAltAlt-p AGR120Logistics AIPS-00500100TimeSteps40080300602004010020001471 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 6110 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61ProblemsProblems(a)100(b)Satellite AIPS-02250Satellite AIPS-0290AltAlt-pAltAlt-NoPushAltAlt-p AGR8020060150TimeSteps7050401003020501000123456789 10 11 12 13 14 15 16 17 18 19 20Problems1234567891011121314151617181920Problems(c)(d)Figure 18: Analyzing effect Pushup procedureeither two approaches. summary, results section demonstrateAltAltp superior AltAlt plus post-processing.One might wonder plans generated AltAltp also benefit postprocessing phase. investigated issue found specific post-processingroutines used produce improvements. main reasonbehavior Pushup procedure already tries exploit opportunityshortening plan length promoting actions partial plan. illustrativeexample, show, Figure 17, parallel plan output AltAltp problemlogistics domain (logistics-4-1 Bacchus, 2001), result post-processing651fiSanchez & Kambhampati80140Logistics AIPS-00BlocksWorld AIPS-00Serial PGParallel PG7012060AltAlt-pAltAlt10050TimeSteps80406030402020100014710 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61135791113Problems15171921Problems23252729313335(b) Solving Serial domain(a) Utility using Parallel Planning GraphsFigure 19: Plots showing utility using parallel planning graphs computingheuristics, characterizing overhead incurred AltAltp serial domains.solution. Although two solutions differ terms step contents, noticestep length. difference step contents explained factde-ordering algorithm relaxes ordering relations plan, allowingactions come earlier, Pushup always moves actions towards end plan.run comprehensive studies three different domains (Logistics, SatelliteZenotravel), found case step length plan produced AltAltpimproved post-processing routine (we omit comparison plots since essentiallyshow curves corresponding AltAltp AltAltp post-processing coincident).124.3 Ablation Studiessection attempts analyze impact different parts AltAltp performance.Utility Pushup Procedure: Figure 18 shows effects running AltAltpwithout Pushup procedure (but fattening procedure), well runningaggressive version Pushup, described Section 3, re-expandsnodes search branch, action pushed up. see runningAltAltp Pushup fattening procedures better latter. Comparisonresults Figure 15(a) Figure 18(a) shows even fattening procedureperforms better original AltAlt. Figure 18(b) see althoughPushup procedure add much overhead, aggressive version Pushup getquite expensive. also notice around 20 problems solved within time limits12. also verified least one problem domains.652fiOnline Parallelization Plans Heuristic State Searchaggressive Pushup. plots Figure 18(c) Figure 18(d) show resultsexperiments Satellite domain. see situation quite similardomain. conclude Pushup procedure, used offset greedinessalgorithm, achieves purpose.Utility basing heuristics Parallel Planning Graphs: see Figure 19(a) using parallel planning graph basis deriving heuristic estimatesAltAltp winning idea. serial planning graph overestimates heuristic valuesterms steps, producing somewhat longer parallel solutions. fact versionusing serial planning graph runs time many problems also demonstratesrunning times also improved use parallel planning graphs.Comparison AltAlt: One final concern would much extra computationalhit taken AltAltp algorithm serial domains (e.g. Blocks-World Bacchus,2001). expect negligible confirm intuitions, ran AltAltp setproblems sequential Blocks-World domain. see plot 19(b)time performance AltAlt AltAltp equivalent almost problems.5. Related workidea partial exploration parallelizable sets actions new (Kabanza, 1997;Godefroid & Kabanza, 1991; & Kambhampati, 2001). studied areaconcurrent reactive planning, one main goals approximate optimalparallelism. However, research focused forward chainingplanners (Kabanza, 1997), state world completely known.implied backward-search methods suitable kind analysis (Godefroid& Kabanza, 1991) search nodes correspond partial states. shownbackward-search methods also used approximate parallel plans contextclassical planning.Optimization plans according different criteria (e.g. execution time, quality, etc)also done post-processing step. post-processing computation givenplan maximize parallelism discussed Backstrom (1998). Reorderingde-ordering techniques used maximize parallelism plan. de-orderingtechniques ordering relations removed, added. reordering, arbitrarymodifications plan allowed. general case problem NP-Harddifficult approximate (Backstrom, 1998). Furthermore, discussed Section 14, post-processing techniques concerned modifying orderexisting actions given plan. approach considers modifying orderingsalso inserting new actions online minimize possible number parallelsteps overall problem.already discussed Graphplan based planners (Long & Fox, 1999; Kautz &Selman, 1999), return optimal plans based number time steps. Graphplanuses IDA* include greatest number parallel actions time stepsearch. However, iterative procedure time consuming provideguarantee number actions final plans. attemptsminimize number actions planners (Huang, Selman, & Kautz, 1999)653fiSanchez & Kambhampatiusing domain control knowledge based generation rules specificplanning domain. Graphplan algorithm tries maximize parallelism satisfyingsubgoals time step, search fails backtracks reducesset parallel actions considered one level before. AltAltp opposite,tries guess initial parallel nodes given heuristics, iteratively adds actionsnodes possible Pushup procedure later search.recently, work generalizing forward state search handle action concurrency metric temporal domains. particular relevance workTemporal TLPlan (Bacchus & Ady, 2001) SAPA (Do & Kambhampati, 2001).planners designed specifically handling metric temporal domains, usesimilar search strategies. main difference Temporal TLPlandepends hand-coded search control knowledge guide search, SAPA (likeAltAltp ) uses heuristics derived (temporal) planning graphs. such,planners co-opted produce parallel plans classical domains. planners forward chaining search, like AltAltp , achieve concurrencyincrementally, without projecting sets actions, following way. Normal forwardsearch planners start initial state S0 , corresponding time t0 , consider actionsapply S0 , choose one, say a1 apply S0 , getting S1 . simultaneouslyprogress system clock t0 t1 . order allow concurrency, plannersBacchus Ady (2001), Kambhampati (2001) essentially decoupleaction application clock progression. every point search, nondeterministic choice - progressing clock, applying (additional) actionscurrent time point. point view planners, AltAltp seen providing heuristic guidance non-deterministic choice (modulo difference AltAltpregression search). results empirical comparisons AltAltp SAPA,show AltAltp outperforms SAPA, suggest heuristic strategies employedAltAltp including incremental fattening, pushup procedure, gainfullyadapted planners increase concurrency solution plans. Finally, HSP*,TP4, extension temporal domains, heuristic state search planners usingregression capable producing parallel plans (Haslum & Geffner, 2000). TP4seen regression version approach used SAPA temporal TLPlan.experiments however demonstrate neither planners scales well comparisonAltAltp .Pushup procedure seen plan compression procedure. such, similarplan compression procedures double-back optimization (Crawford, 1996).One difference double-back used context local search, Pushupused context systematic search. Double-back could also appliedfinished plan schedule, post-processing approach outcome woulddepend highly plan given input.6. Concluding RemarksMotivated acknowledged inability heuristic search planners generate parallelplans, developed presented approach generate parallel planscontext AltAlt, heuristic state space planner. challenging problem654fiOnline Parallelization Plans Heuristic State Searchexponential branching factor incurred naive methods. approach tries avoidbranching factor blow greedy online parallelization evolving partialplans. plan compression procedure called Pushup used offset ill effectsgreedy search. empirical results show comparison planners capableproducing parallel plans, AltAltp able provide reasonable quality parallel plansfraction time competing approaches. approach also seems provide betterquality plans achieved post-processing sequential plans. results showAltAltp provides attractive tradeoff quality efficiency generationparallel plans. future, plan adapt AltAltp approach metric temporaldomains, need concurrency pressing. One idea adaptsources strength AltAltp SAPA, metric temporal planner developedgroup (Do & Kambhampati, 2001).Acknowledgmentsthank Minh B. XuanLong Nguyen helpful discussions feedback. alsothank David Smith JAIR reviewers many constructive comments. researchsupported part NASA grants NAG2-1461 NCC-1225, NSF grantIRI-9801676.ReferencesBacchus, F. (2001). AIPS00 planning competition. AI Magazine, 22 (3), 4756.Bacchus, F., & Ady, M. (2001). Planning resources concurrency: forwardchaining approach. Proceedings IJCAI-01, pp. 417424.Backstrom, C. (1998). Computational aspects reordering plans. Journal ArtificialIntelligence Research, 9, 99137.Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90, 281300.Bonet, B., & Geffner, H. (1999). Planning heuristic search: new results. ProceedingsECP-99.Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanismplanning. Proceedings AAAI-97, pp. 714719. AAAI Press.Crawford, J. (1996). approach resource-constrained project scheduling. Proceedings1996 Artificial Intelligence Manufacturing Research Planning Workshop.AAAI Press.Do, M. B., & Kambhampati, S. (2000). Solving planning graph compiling CSP.Proceedings AIPS-00, pp. 8291.Do, M. B., & Kambhampati, S. (2001). SAPA: domain-independent heuristic metrictemporal planner. Proceedings ECP-01.655fiSanchez & KambhampatiFikes, R., & Nilsson, N. (1971). Strips: new approach application theoremproving problem solving. Artificial Intelligence, 2 (3-4), 189208.Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphs.Proceedings AIPS-02. AAAI Press.Godefroid, P., & Kabanza, F. (1991). efficient reactive planner synthesizing reactiveplans. Proceedings AAAI-91, Vol. 2, pp. 640645. AAAI Press/MIT Press.Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. ProceedingsAIPS-00, pp. 140149.Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. ProceedingsECP-01. Springer.Huang, Y., Selman, B., & Kautz, H. (1999). Control knowledge planning: benefitstradeoffs. Proceedings AAAI/IAAI-99, pp. 511517.Kabanza, F. (1997). Planning verifying reactive plans (position paper). ProceedingsAAAI-97 Workshop Immobots: Theories Action, Planning Control.Kambhampati, S., & Sanchez, R. (2000). Distance based goal ordering heuristics graphplan. Proceedings AIPS-00, pp. 315322.Kautz, H., & Selman, B. (1996). Pushing envelope: planning, propositional logic,stochastic search. Proceedings AAAI-96, pp. 11941201. AAAI Press.Kautz, H., & Selman, B. (1999). Blackbox: unifying sat-based graph-based planning.Proceedings IJCAI-99.Koehler, J. (1999). RIFO within IPP. Tech. rep. 126, University Freiburg.Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.Long, D., & Fox, M. (1999). Efficient implementation plan graph STAN. JournalArtificial Intelligence Research, 10, 87115.Long, D., & Fox, M. (2002). 3rd international planning competition: resultsanalysis. appear JAIR.McDermott, D. (1999). Using regression-match graphs control search planning. Artificial Intelligence, 109 (1-2), 111159.McDermott, D. (2000). 1998 AI planning systems competition. AI Magazine, 21 (2),3555.Nguyen, X., & Kambhampati, S. (2000). Extracting effective admissible heuristicsplanning graph. Proceedings AAAI/IAAI-00, pp. 798805.Nguyen, X., Kambhampati, S., & Sanchez, R. (2002). Planning graph basis deriving heuristics plan synthesis state space CSP search. Artificial Intelligence,135 (1-2), 73123.656fiOnline Parallelization Plans Heuristic State SearchSanchez, R., Nguyen, X., & Kambhampati, S. (2000). AltAlt: combining advantagesgraphplan heuristics state search. Proceedings KBCS-00. Bombay, India.657fifffi! #"$ %'&)( *,+.-//0213( 0*54'( 6789:;< =>*2?/-!@A: %&=CB2?/0DFEHGJILKNMPO3ERQLSUTRS>VXW2EHIY[Z$\]^ _a`Pb,cd^ea^'`gfhjilknmpoqrsRhjtvuwxay{z}|lr~yr$Lc\]Z$#jcv`tvknm}sRhjtvuwxay{z}|lr~yr$}jU 'XR j<22 9'v2nn9U 'UvC)2{v>a3v5.g.2<<! 9nga{)a5<gaHja{.XpCa.a~ff 9a~~2Ua!UvR#9>vaU55a!g52!{vnd,!#dj2a5dj5aa<apa{9R.55{~Xj 5a{>ff93.~59 52!R !5{~a59va<{#U}.)9v29!a2R255a{nC{J9a5aa5a{9a$<.>a95{<2ag>5a{ap>2n599X5n<{!aja>aa#$9{ffH25g.a595)5<{ 9555ffan5Lvaa9{5a.9J 5vaH95a{!C!59!!5a,5R !C5593{.a9p<<L25a<a>93Jn <,jvHff}$XJdpC$XlffU$ ,fffi$ v p) 2$lp$>Jl$ll Jdpp>2p ,ffv,Jd >J 2 2J2$)ff"!ld$2#l$ 2%ff 2&pdp<'2$})('2*+<dl-,X$.!/10#v*23354+6798-." "$ $*.p:vXp+ 2}2!2pR 2 2$Jld$ *+ 2$Rp C$!$ 2$jv'RJ)$ RJ2JllJ22; 2)p ,"d$2 2 $~J 2 2 $$2l> >R?; 2#@Xj$; 2?!$2$; 2$*< Rp=)2p292 pRff$l2~ff$$p2 !@2 p)pU2 %AB$J$,! $2 $,$)XpX!&)$ff2D$C E(F0 G/IHB$p2*J23KL+MdN lX)$*O233P)6 $($$2,>$*23L+L.0,UffV*23VKW276XRWC$*$ff$$p2J2p$,ff$<Q+ROS$2}! 2 Z([\p2}=HT$*]2333^M_ $$JR$*`aaa=6-$2!b2 $@>c 2 p)$V$9$3p"!ff2b! l! $ff$ ef = 9; g)$2 JE$C v}$2$*2 $ ; J2 pd$) "\Jdp$$&2 $v$2}l$$; 9$2 l<#932X$" *V)$9@.l)$2 92 $)bChVpW:)J2! l22?!ff$$i2 $>C$l!jp$Xj$,YX.$$<JX"2 + Xk^$+2 $2< $R$.2pX! ffl$Jdp]el= 9; ffl$2p2 g $k^p*2 pCJ2 pd$]! $,l2Bm)pnC$*$;2 l* 2 $ff"! l~$2}l$RpC $$ $oX>aVp4 $aVrq^ < p>J2 pd$Ol"$$-d$gg2 p,lff"2 $2 gfi.l2V2 pYp>"*5R$X ,jX.lps(Fg2u/vff$X*`aa2WH6 jp$X:X.lps(fiw +! =H$*J`aa`67;2 x,gff $iJ$p* p ,v* bX.$l!:2 $)ff"! ld$2ll2 %jff2 &pdp\2pyp>"$22 $)R8 z Dp2ff9p$Wbd$z2 p ; +2l?$!$X$p,gX.l$)E2 pLlff"2 $2 zF.$2GR$zCzX.$$R{2 pJff"! ?$EXp9$|p@2 pXj}2 p|>c $ ;~ 9$pb2 $j>ppb^{~2?$uff$XE(`aa`67-//0 v %% !=5=}C2Y3!fi{;9v:! %&% a&2%l%?2 =fi oxorlzGir$!p2&2-'pJlgd2lff <$ff"!$$vp#o('C$ 6 e52?!!JOtg$!+ #$pv$22X$pZ*Y}"pdpb ? '( =6bH 2?!Dpdp$+$2&2 $! + ;lff I(fi67*g'( W6 < ,ff"! $:pv$J$+ 2p& 2$!+ ;lff I'( 6HBp2 pp\lff2.!dpff"! lMx2 p2p-pdp$2D+h2 $Rlff -! } oF$ef 2$ff" 2}>C$2+ <H 2pvL|X.l$l! 2)$L$<" 2L2ff;#$ 2X yXX$2l! 2?| 2p$2- 2$ U2}?$; ,gpdpX "& yX.$| 2pff"!?$@ 2!= LLX22$l)! +2 p2 phX.$$$ff2s)$2< p^p"p2.$$?! ?$; 9e 22$$2d.p2Jl2W:$$C~ff"!lx.+ 23$="!Jue JX" 2= 2$~ &2 $ @$$! $ff$$2Wd2 E 9.$k^pff2p @. "$$p$2-w L$2X@ $>WvHffn2 pX.$$2l! 2$! 2 $lC ;w py 2pU2"l$ <'p 22 ""* p$C''^l"bpXLff$$p2 $k^pX~$2l! 2ld!b2 $$X!b.$ff2Xx2 p?2efsNv 2E`D,l?\2 $b! pj2 pvYX.$$2}! 2 *OR$ p2$XJ2! $2 $$uvN 2 \^ielndN 2 D4b>2pW>2 pjX$dpU%2 p$! 2 $WL$lD*)$2<|vN 2 &qj2 pU2"$2 ff2pW-b2U2"2 }p>"d'?$$*nvN 2 nP^*.>$$$$$$2>X22$lRl2 lx:fi "$22ff9\g j{ XL3$>pG[ff"WZp >" $Cps+{ 2pX = 2.(TT69 pX$p+ 2pdp- < pl2ll$ & |C$h 2pgp ,ffJ" 2; 2 C$2X" 2$(Ti6i>(T 60lff\)pPp+ 2; 2pffff!$R$2ll$(f2W6W)pv$u('pp ^T67*>2$Z!p 2$L2k^$2 2pff$ 22-+lXCB 2pd$= 2$+; 2tg )lH$Cp 2p 2 H>lff g$vp*J U * hX@ 2$y$vp- 2$ lff922p2;C%2 pjp>"Ym)pD 2*.$" 2$*V 2pj.$] 2$)$vpCX + 22$+2;pff"! $%pdp* R$dl$!&2 $ ffff! $%pv$R$2 2$\2 $.$E(fi)$9}2 p@[ ff"WD}U967UxW?+ ;ff(`6fix |lr.u>k i.$impo rNvi#?$; 2$i$!p:2 e gJp Op2ff"*pW>* 2$ ?pCpg 2$!+ glff9 2>)2+lXgd2 p'pdp* F$* l2p'2 p>lff ff2V*. Op#?$$p< pjX^u}(F$!$2|2 67}H "=$l$2:2 p2;'2 p2 p"x2 $ -2 p2~'2 p2 g"2 l*i?'uu F$ l* U2Xp! $dRJ2 2 lNV$$X-2 l Y>jVpWFLlffff2 $ffC ;2 J!]2 plff9 pv$*' ? *2 pJ 2 !2pl$$2 $X$; $.29! $\ U C;( 5 ?6'?6( F$F$ F$(6< $gd$22O 2}x J U2l}?,"< 2px[ff"Wyl. ]2px.$X<$G F$ < +lx 2pL "$]^$@Xff2L$pX$$= C' 2p 2; R' 2p@p ,ff$uXg.$$2 p:^v}$2 $p!&2 $"$2:^=2 $RJ2 l$;2J2pdopN ff9$3* $X>&>$GVpW2 pp$; $.2@2 $lff pdp*J>$g2 pff"! $X$ll9( 6i( F$ ?6 ( ?6? U F$ F$(fi4+68B" ".$ $*H!p)> pp h^p 2p $; 9$ 2 ( U6$ 2p2G>.pJ ( 67|w *#$W>*<; 2p" 2 l2X" 2} 2pffff!$$"$2 *$J$p$X,$,xX.l$iC;2' i$$; 2$*php$XX.$2|Xj$h+J.2 !( 6i5V+ ? 1?6^x( U6( 7 F$? U F$ F$(q6.$$@ 2p$" + 2@ U ? x( ?6al}( ?6y2 < p$" $ 2? ?x( U6 2X$$' u?=X2UlffJ 9-(F.$$i 2p!$$; + 276lb2py>;n'( p2B2 pJ5p$)6 ff"! $]ffp9 mB^d$2$X* $9x( ?6(?}62p$$Xx$222l$}|2pBffp.*.!p(?6C..^p:$D>$Wg 5p~ WR3X22$lR$" $2 $mjp2!l$~ 9k^$2 \Jq2J}$ff)$2! 9!G(6 $2}R$2 Ujffffl:w $2 2ffk+l2uCl>W>:X.$$ J ( 68 . pR>$$$lp+2 X&x( ?6>$! +$+ E2 pk^$2\c} "p$Xb2 $ >2. J Zp$X$W>$$s2 pff"! ?$~l2 p%$vp]2 $bl%RW >>}$,.2 $O^$W).! ow U="2 $!p2 l,}v2 lffff! $^pv$O>}$pff9$2U. ]2 p2Y2 lff2U".l2? XmB+d$$*+Rpp>jC$2.J. h$$>n%)p2%% ? ( *uX>"p2j2 lx( U6'}( T6Y ( 6(P6? ?$2ff$2$W>x$ < pHk+l 2$Uff2jp 2$?!JB 2lV L+ + 2x( ?6B)${>W$lpJ{ 2ph #$lEXk+l 2sq^ < pX22$ )ff2"l 2'UR9p C $ !&`-)V $" + 2fi)T 'TfFffT ffTf9iJr 9T 9FWTx;ffTY7j;"Tr'ffiff 5)lJpTff=fi oxorlzGir$!p2`^ < pff2 R$x( ( - 6$; 2p :Ix(aa=6Jl$ 2$!2$2p $; 9$ 2Dp9$~ =6 < p ^v h 2p WCl } < p$ff"Llp2p pW 2 $^9 sX2; s^ $l!ff9Ly#DX.$l>)$; +2 )?h2 ppffUl2! ?L! $2lu%6 < $>2ff$$2X$dpp < pnl" ?p2p 2 plp&)p2u2 $ufi$26 U$;2 + :2 $Clffff2 $ffU$l( / G29 ( - 6}( ( - Cefz2 $B>Wg*2G+\$$HJ ff*3$2l! b \*2p2^yC$$?!- 2!= X$$%9 2p)ffff!$ < $$2d2bXU2X &jl 2<ff"! lpv$},l2 u.+2 3+"! $LC2$"!$#&%\('*)l\nZ,+-+ ^'`.)bC^'`gcvZ$\efD U; 2$lff$ffU$2!!*9k^$ 2nqDXv$20/i13 2 64 55 455)pj 2pjnff9lUff2pCp41a87:9D4=; <(TL61x( U 6(K6(36U6' ( V ?6( F 5F$ F $( 69?>?@ ( ( B A=D A=? 6 ?6>; ?C @C ( 6EBF UDG(f2a=6)p@ ( DA= ?6\ 2IH> 2$" 22$G$vpoX 21pdp2 2$ ?PFff2k^$< ^$} 2pl&$>] 2$ J9 2X$h E UW* 2pW)UK9[$ ; 2p$; 9= 2g(%)$o,l@`-x\U67 < pp?!% 2p2 )CL9fffix |lr.u>k i.$impo rR! $}2 p2JgX 2 < $l; + 2$ :x( ?6>2p$hX; U22ff=Up$~uX$9 N9[$ ; +2k^$9!$Ux( U6x2C$Ux( ?6}C2(f22W62 4 22X$$ ( F67 < p@p! 2$( 6C\.lD+< p5d@$)1O$2?! C 132 4 x 2$QP 2$gF.$ 2"!RTSVU ^ + K Wc8XZY~Z,+ K Wce!p2!&pp2* / *% 2$ lff92 2$2;x 2p|p ,"+G > 2ppp$* ( l- w \>+ J. nX.$$ 2pff"!$ ( / {2W67 Nd$D>pEp|^p2 pl; $ 2 Wo 2plff opdp*Y,lu $2pC2$l$"$2 $}#j9k^$2 @q^ < $2',>= v.l%2 $ilffJ 9Ox( ( - 6 < $'!]pWs HJ 92 p>$@$$5p@lF2 pXfi.l2 @(G29 ( - 6Vx( ( - 6(- /ef!p2Eff` >2pW ?12 p2 $Jl2$2 p l(fi2 p ff"! ^96 )$912 p$"$2 x( ( - C6 x(aa=U6 J}$2 $y! $n+p?+$}2 $g2 p22 p-n?pw l* $W>*'2 pff9J '(! |$vp 6)$$ ( (6 $( - 6D2 pJ$l< pJ2; 9 )2 pJl{$!pbff` hFp"2 $3* 2$$;2 $n'( 29k^$2 \P6wx( ( G2W69x( ( {2[A - 6ix(;2a=6.\x(;22W6}lplpX$p+ l; + 2*))$9Fff2pW:( ( G2W6(f2`6$!p2`fflp? 2p^mB^d$2$*^$$?! 2lR?9 2\ 2pJl\)$9G}( ( - 6RWH2; 92l,2p :|!p2-`5Jefb2 pH2JC.!$2>R$WR$p|lffLp*)l2X$2$)l$')p2-2 pQP2 $Bfi$2,$;2 +H ;2 $$ff9$ff$2! !j v)pJ$ $2 "$ffb2 pH5d@$$&2 pR$$2 $ifi.l2 b2 ^$iX.$l!)2 pff"! ?$( / {2W6`cb~g]_^oi< $$2}DwedEnD`X0fff"}$ ff"Gp >"E>|Cl; p p& 2pXU@>|R'$|2p9p2fE(hgG69ji+U)B"kRlT-mgn7kl "kl5omgqp(f26)po"kRl#g 2$$; y?xz{ F$ $ p$ !bpdp < $9p$ 22p;2 lB"kl 6gs*i)l J2L2Lp $ 2pJ. 2 2Z 2Jzex>$*Jl* g` 62 p)5e !y! h$!p2b2 2ff$!llff"|pdpd* $$2 pBC.!$2 L!p2)2 ->$@X>?$$p0fj'C$2?! g` 2 ]2 pUC.!p92 $$!p&2V$&F2 $:ppW:= &fj*. xp:$!$2&2 V}* 2?$)2 $)lff92X$p?ff"! $@( p!$!$2&2 67ief2 p-,lfEh( g{C6 }2 p<?}Y)$9{UuVu F U9pCff"! Y2 $rg*^,$,H$2?! :pdp) 9>$222$ X.$$ffffff(sfi oxorlzGirJf)'w(TT6$ (F6$?$$ 2~Y 2$ pR$|~2X 2$$X.$Wz$ " 2-," 2+)$\ {X.$$"} 29(p!$!p2|2$67Yef+ ; - tP+)l2ll$ 2 l*.2 p@X.$$ff"!l "l22.$@XJ 2J}$G+ ff$.! Wb2 pff"! ?$j*l2lB>$W$|X.$$UlDp2 p2Jn$wj)lJ -X.l$9UJ})$9| 2phXB$L}ff2ffRJJffbARE2$$g> $Wuv!$#jw0x c8y]acv\Zp]n^Rzc{%HZ$\]pjVUZ|}fZ>CX"9$CJU 2'; %,U 'py 2pfi9P$ ;' 2p #l2lD*22l& 2$pX$$$$?L$ 2p J X.$$jR$Z%) ?px # $l+d$2g U < p)+$@Xj}'( pk^$$ 6< p)^$X>Jffffl>$; 9= 2)Uk^$~` ~(f24+6~~?Fm)$ 2p #$2lBpu 2$ff*,@ "h Jl2WX.$$} 2$ Hff2$CpXjR$U 2lffIH< pJ}ff zC.!$2b2$*$; 2l*gp+ 2* B 2p|$ X)pGX.ln>J$ LW 2H" 2 lJ$ ) 2p1 Xk+l 2 s3n $ 2 ff$$$ bffp < $E>J | 2pop$$X$ W>X.$$}: 2l | ^$d!b 2p #$l )C5d$~$$$~'e2p@p.$hffpJl2Wx 2$gX.$*^,; 2$ *p 2$")j>llp$2C2X u.+2 X=ff! $C2pJiel$)2$ 2l9>< $$ 2 $$v.$pC$|X.l:X.! =ff! vB2 p22 $ J$WJ+Rfi p$ 92Ux2$p9= iel%2 $Upl})$$*V2 p$! 9$2 $U; }uv!R '+ K g]aZp]^'}`#Z,W '*+ L WcYg^]#$2l<$-| 2p2!l$ " "$$J& 2p)$ 9 "p2 < pJ 2 2$}Jlp$"$$2lUU2p :h &XX$^$(Fd$l$*O23L367YcY. )J2ff 2= 2$*VJ;C$92 $2l92 pj^$@Cg2 $xp$>) LpffCo2 pB^$@X>ffffl))2 p^$@X)j$" +2 * R$$)2y( < v$J*`aa`67jef9$ 2l$2?! 2 JplX"9J*R$)2JX2 u2 $H2! $2pJ=2Y2 pJ9 $< $22 $X J.2 2 $<Jld$ n$d! pj # $lOc?= *V)p2 92 p)^$X>g$; 9=2 < p "$lff2 2 z2 J pX$$Rpd$\2 p lff"2 lffRl2lD* Hu! p9 #$2}jp. lX2 $$2$$}ffffl$$; ?=2 ff2J2$}< $H2pl2+ J2 pdD 2 lgRlff :)$2 ZU)<J2ffU2$~"n Lp|\V>XpR$X >) # $2}XpX]2 pXpl#$p9%2 p,YX.$%)B2 $,?b2 $ 2 *+,Hp@pXp| $.! :2 pH _9 $&pCp:2 pQP2 $F.$2 p* +d$$$:nff";2Xk+$ 2 E2aV < pg o1 *2 $R)$2 < p22*>X2 J2 2 $Jlp$- "'?=|p3$2! p$;W k+l"d$Z2 p$! $2 l =ff! J2$$$;2iel2 pjpV>22 $* p ,*>$p292 $9 lRo2 pBl2,#@$l$p-2 $>2 *^,H2ffff&2 $ Y)p2 pB # $2lYp2 l* p< l$C)WR+$" +2 )p$$* lWv.!b2 p$%! ;2! 2X.$$ffWfix |lr.u>k i.$impo r1Mean0.5Exact|S|=1mar|S|=2mar| Smar | = 300.51/w1w5010015020001Ring position$!p2^-Hc>$ ;~.9$p9!J`aapdp<?$$ 2~) 2b 2p22$$<$b>$!+ 2R2$W:u2 p W>Hlff"'2 p@C.!p2idHJ$Hff2} ; ->g-2 p2"$$2x2 poX.$l2l! 2 $! 2 $D < p^$@:pdp $$$#$$ h2 p! $J^ * ~`< b! RJ?= "$ 2. x 2pX"9$] 2$$!$ 2$ ,2 L!u(pff"!v#6J`aagl?$ff"ypv$%Rdpjff2R.pjy2 p-,c ;~.$$)$ 2Jj>$!+! !o2 pL!$ p v$$o2 p22${)w $! +2 $ z2 p2p$Rff2ynff"d2p : 2poX;$!$2h^zpb2 $2J}$2}>J 2 $p2.! |pdpff"! $< p2ff2l ; L p?$!p2h^Gw 92 poX.$$2}! 2$! 2 $2 $2x2 J*Rp2x>xnffy2 p5p@lI+$@X<$vp<?$$p;2*x;` lp;v^bef <,p| >lff9 pdp2pj ,op$! +9,< ^$}2 pj2 $22X$h f ` 0 ,* f ` 7 ${f ` 6H >>ffY2H$!p2)@2 p2Jl;j$2&C $p+$$i}2 p(P$x2popv$upJff"! op! 2 $o,$! +2 <* p ,*92 $9ppL"= uHH )!ff"! }ff ff$J$2 -2 $2ff$$ Bel)2 pLJ2 2 z2 C|$l -!$# ^ Y~c%Z'}^'`]aeZl`. '`z}cv\()}cv`cel RUX2$l) p$g 2$CdX+' 2pgX.$$2l! 2$!$ 2l $$^ 2,c ;~ .! $,B22! :n$:C>$! +2 )( ,6 $$-> 2p2p$B(h+67#2! @pv$ff"! lff2 k+lg"$$>" B[ 2p o2 $R;d J "$X)lp,BX.$$U\2 p 2! g$vpR2p$DXp+2 < $)J}}2 $ -B2 py%5 -Yffiff TlJpTj:=LLT r7ZffWTff7;fffi oxorlzGir1110.10.010.81.60.50.50.991.41.2thresholdthreshold0.91.8100.80.990.70.60.99900.9990.90.40.90.60.30.50.50.20.40.10.2120.510weight1120210weight12$!p2j4 < pJ2"l$ )nc, ;~. 9$pJ9!U ff"l$ 9ff"\^$@xpdpbHH2p22p$RlG,$!+ 2$W& 2pJ2Jbffpgefz 2pfi R}p 2py!ff 2p :,Z2 php$X $W>X.lx2 pJ2 phpdp < p$!9$ 2$="! >dXp=2 $ 2 pC $X.$$?!-ffp,jv,$jj)$!)2 p$;2 l$2 ;dJ. 2 yffpH$ff2 $% 8.*J)$2 )2 p< +l+$@XU<$ 2 $< plff 2p :D2 $$! +UlpCdX+ 2$gW)?! $$})=J"( 6'5.1+( ( ( 6Vx( ( ( 6(f2q6.$$x 2p$; ?= 2x( ( 6xx( ( x6x( ( 6x( ( 6i6( (( ( 6ix( ( 6xJ ( ( 6i( 6( T6(f2P6( 6( T6(f2K6(f2WL6(f236$ff9$b: 2pW>:X.l < p$$ 2$$2l}$ D9k^$ 22qJx!$u+( ( ( 6id@(h\ )(\ )( 6(`a=6p2 2pJk^$ 2$p pb 2pbCd?= )( 6H$ ( 67be j "p7$H *p ,*X 2$ J$ L 2b p ?p 2$Cd X=2pDplL$ ,bX.$2pJp* p$( 2W6 C ( C 2W6j$ ( 2W6 C ( C 2W6ic} 2$,$}pHX)D2 p$$l)"$$2 Ux2! = "p)$+"!$ Xefn$!pj4 &2 p $IHX2$gX>\2 pp$X$W>:X.lu 2pJ$))2p :&nff9$9p)2 pD>$! +$2 $22pJHH &>2WX2D2 $! + &X.$$LX.2$2}>$! +2 $2J)$ff"! *YLX2$2 $D>$! +2 *XRp2|$! 2 $>$! =2 j2"$?L2 pjXv*."2 pV#T 9$v+X.l$Oelj2$$XJ=2 p* p ,W*2$ '2 p2>"$$2 222l@ j2 $pU2 pU2;<$; x("f U2 lDr?`6ff(fix |lr.u>k i.$impo rx 2p)$l2l! 2$! 2$D < pBX.$$>2&XH$2Wo^$ff"!! 2p$" x>2W$!p^- b2 $BC.vCX+ 92 p)X.$$2l! 2!$ 2$ +"!dXp= 2$ < p$"2$) &2 p;dJ. 2 )ffpDX);*)p2t 2pg^$@XU$ 9 2$$aPRD^$@X-X>{~ 2$$L?$$2 .!&2 p+"! $X < p jp*^2 p2 ,:2 p$! $2 $ +"! XelD$!$2)4{C2$W:hx2 p2Jj>$! +2$G2 p2p$geffj2 $ff"! ,$! +2 l.$L2W>+"! $< l))$>h! $$ll2 92 l)ff2 p ?$;2 H2 p :D2 p+"! $G v ``~~<< 22i 2p:k^$$2pxX.$ $2}! 2$! 2$IjJ29$!?!jp >"d*=>J.\X.l$Cc W2Ep ,ffZ(TNv 2Eq^2W67* 2l HJ$$nl?ff" 2C$2+22 $g2>$2}DivN $$$*^>H2$W12 $ Y2 p-X.l$2}! 2J2pdz$C$P=2 $! = hX.$l&ff"! ze 2!s! (TvN 2 q^r` $C$$3* EvN 2 Eq^r^*O>2p 2"$2 )hl$# lff"2 $! fflpg2 p2&p>"d*O>b)$.l2$2X.$$l! 2 22 )2 E2 p$; @nff2 2 pv(Fff$X/w! $ff *`aa`6x)$9>p%2 pjX; )ff$$2Wd2J2 pd$}! p! ff}$w pp)X2$l*^,2pW>2 ppRff"! ?$"!$#jw0xcWZl\+c]'l\nfhHHffp >" 0 $$$2Z$ >")l 2$2+ 2$ 2$R2C< pcC2p >"Ee CL$!$$ p2$X+c>l Jx(f23K36LLz$ >"J$! l2 =2 + $2$ff2he $2; 2:LDpdpx ,* 2p2p" 2$H2#p! \ ppJ2 2 $Belz$!p2bq&2 $@! ff}$Jv$gH2p :m)p:2 ;h>: p pj2 p! gpdpffff! $?l$+vp$X$R9pdpz2 ppff"! ?$)l2ll$2 b$2 "2 ff2 2p :P$$~ +2 Hp< p$zX;92 pL22 .! ff2.$z2 p2LpH$$ 2 pyp$X$W>jX.$$2X2$|p2 $jp,"{>|$2fE(`q A"aaa=6 7 &eY>|2 l$*]>2p2 $|2 p?$[ ff"l,2! BpdpHff"! $< $H$! 9$2 $ ="!X2$?+w 2b2 $ )2 pbX.$$Hz2 p ,fiffpff2bff2 D2 $Jdgffp@w! $+ "$$2 GR={2 $$ff$X$umBl2"&2 $ ypdpL $2 : lG2 p2Rglff9xpdp-2- bP^ < +l,222 $X,o2 p2Bpdp>$2 pR;,2pbp,"ff22 p2! Gpdp&L+*R$J$)2 l B2 $B$"2 +$$pX$$Rz2 p.$ff2 =ipdpL+ < p ; Hlff$X$x |Xgff2X'( $2p-2 $RH&! n'76$D2 $2g2 pff"! ?$X.$$Y:pdpg2jP ff2j"h2 $! +.$$~?g2 pYp$X3$! +#ff$* @2 p,2 pl$ff* ff2Xk^$$ Xv < $~glff"2 $< $iX.p- y2 pH$$2$J2 p.p2 lX2 $2Hl2 $B^$@>g;2 C9O2 p)pdp>$ef @ll2 Z2 $ $ ff"! ;2 lJ2EJ2>$; 9 < p22*'>.pUJ )$$2 C@l$2 l-pv$*+R$! p$$Y y>X.$$W*5K 5ffX;7t W;J5 - TT ffr7t]lfi7c]ffpT7Bff,h,$Rh$(xF"TJpT_F 9 77 W ffrpr lT;ffT57 7 xff W5 [ 7 JpT7f5FTT%"T'5 7. BF"Tr(fiff T7JT x7TTxf75 xr(Yrffc7 l]f5FTlTfff[fi oxorlzGir21. pulmembolus12. minvolset34. pap19. fio213. ventmach14. venttube11. disconnect22. shunt9. intubation35. press10. kinkedtube16. ventalv15. ventlung33. minvol17. artco232. expco28. errlowoutput27. hr29. errcauter28. hrbp30. hrekg31. hrsat20. pvsat24. anaphylaxis23. sao218. insuffanesth1. lvfailure3. hypovolemia25. tpr26. catechol2. history4. lvedvolume7. strokevolume5. cvp6. pcwp36. co37. bp$!p2q^ < pp Hff"!$@(fi 2$"$$~+ 2g$76>$u 2$gX.$$( 2!76xB 2pffp,"ovN b$$Hff2D2 $! + j2 $ g2 $J22! jpjv2$l@w{fE(`$q A"aaa=6ff(fix |lr.u>k i.$impo r0.650.5100.415200.3250.2300.13540510152025303540$!p2P^ < pBl$o) 2z(Fp$XC^$,W>x$ 6gupdp@4a5V4aye2!!2pW-^j2p9}"Vp2#2 p,ffk+$ff < pjp ff"!$$$ ]XjJ<"$9ffff! Yp,ff < p>p2p s2 $XX.$lff"))p2Y2 pYl$@) 2-aV29w $>2{fE(`qaa=6Nv$l$* 2p,d$2ll$ B 2lff2>l-2$ gl$$$% 2pY$$2}! 2$! 2 $D < $$* J$2l}$2 >ff2g &~2$p2C%)$ >x! p< pCffC2JUlff"2 <2 pxp ,ff@ 2$ ff$ff< "pJ. D(F$gp22ff$JX2$l6YR$z$"l$$WJ2Jp$J; 7$"p2L$ff"! |'( vUR6 ;2 lym)2 plff"2 GXyX.$p\"z,T)b2 l j2 $)p>"$2$&XX$p"ff"! jp,"o2 $C+ 2 lB+pJ2 pd?n2 $*O2 p@X.l$l! 2$! $2 l lnXy$z hC$2ff)2ff$$2N ln2 p J2 pd ldRJ 2 2 $*-2 pd"2 $ff"! "p2.$$.!,p"$)J; :$plw"!RTS() 8ye^`)q\n^b>Z$\ }cL2 n ># $J$$Je2.!Jdp*R$)2 2!$ff-!u)$ 2V#2 lpX>3l$vp 2$ Rff2 $2 $$!=Xp9iw y$|!4a&^h4allff"gpdp < pCX = 2]>2p9:2 ).$$[$; 2jX ,b~$ pef+ ; })$ 2cC2hXp ,ffvg(FAR*233`69 2pRl2ll$ L$; 9$ 2L x 2p.$JX&e 2!g! 9XpC2 $&p$~ *^ X2 $jlY$$2k+plX}2 p$! 2 $Dff(fi oxorlzGir1010$!p2@L+-Hl$#lff" 2$ Dp ,") >+ pdpJ 9 Wj9lpdpE(fiX ;)6 $:2 p2}ff2= 2Ru 2$g $9}ff2= Bpdp$)d 2$n 2p2$p2Jj0 2ll$ -2 l>2lp$H$$2 $~ :2F:.$$$; 9$2 Jdp,b2 p-; W>ff2HJXb L"2 ?;2 < pX.$< p)2lp|p$2l! 2$! 2 $ n)$2 8f(`qaa=)6 l+"! nL $$xp9v$2 $b2 $"pp$~ +2 <$HpWI2 pdff"! $*2 $b $GX;%2$22! ffg2 pplU$W>X.$$}.$J2Ye 2!zp! $LJ 2 !z2 ppff"! ly 2l*X JJv b 2p!)ff"!- 2lX B`q|^n`q& 2pd $!9$ 2$').p 2pvXp+ 2g!2pJ2 2 $,lp$ < p$$2}! 2 !$ 2$D*s 2p 2pJ$$*$$ pX$lL2 pd; ff$"$2Z'( X2 p2~)2 pz[ ff"l2pdp 6$2 +$2 $ff$nR$2 {2 po+$@Xpdphw 2 4a5V4a\5e 2?!u! E)$2ff"pdp<2ff$X)$Lb2 $x$l2l! 2! $2 $Dpi2 l'p>"2pp$! $2 $ ,$2k+l2"! ffl<R" -` 7( 2O+$@X*)$2X.$$l! 2 +"! X@L^2 ?+ G)$9{2 JJ$$H2pff"! $> H'2PaaopdpefE$!p2|Po>2$W2 p|4a5^4an! G)pb2 pblff^p}2 p"k^$ff22X$G2pl$n)292 p2?! pdp ff"! $< $:l$n)2 )pC pB2 p $IHX2$X >2 $|p$X$W>@X.lp| \2 p&'2 $ ff"! l<$2l}$2 "lp*2 p >Dl$\).2 $)2 pyl$ffffupp2$Hff2Jp+2 Bw J ff$JJ2! ly2 p ;2 @(fi2 pl",ffV 76i)$yX.$$2}! 2$J2J)$$$$2[ ;UO2 pffff! $* p ,}* ff2)$pok+$-><J2)2 $nLq l@}$o)222$uaV2"!uw0x%\nZ$ xcJ^ HZ$\]^]acHl$#lff" 2$ o!ffl L\$ >" $; 2!: , WL-pdp*)$2 2$o W$lp $Z2 phX; W&v2$l < p$$ .p 2$J? 2pDp ,ff ff2D2 2pn( }ff2= ypdp76j z 2$X ; n('9$spdp767ENV$ ff$ "p2ff$Xff"2J}*2.\)2 ' $Ypdp$U = 9 2lb J. & 2pff"! ?$Jp2 $)pv$$Jl2+ H }# lff"2 $! 9ffl Xu.$$$h( X+2! p :=Cff$67H 2+<J}C2 px$C 9;]2 $ h( $2d.$lX"2 762>H+ (f23336])$2 $22 Xpv$ < $x )$ffg[ $)2l^c( J6Jp>"j2N +,Li(f233V2W6! vvpJlyl2 Gl# }ff"2 $! 9ffl)=2 $\2 p2y>2fix |lr.u>k i.$impo r9001508007006000.1 <0.01 < < 0.10 < < 0.0110050040030050200100000.20.4$!p2K^-H0.6l#}ff" 2$ Gp >"p$DC$22=0.80010.20.4Z 2p$2$1pdp"s 2llpdp0.60.81DC2 F$d#< pfi J$; !2pWRb 2pl$ER 2$.lE@ 2p2.!|pdpffff!$)9$JXEpdp < p$$2l! 2$! $2 l =ff!. -Lq ^ U$\$}fE(`qaa=6! +$"! 2p )2 $22 $X22]2 $ff$$2Wd2 D)2 $o2 p< p$$$J!! -HH ff$$2Wd2 $:,2yX>z2 pyp$XR$,)X.$*$2p! D2 lUp$22ff|2 p2$$*2333^M][\p2l+ $*'233367 < p22,J$WJpz D$ 2pl#lffff 2$ @p ,ffff9$$"p2 & ; 2 p$$2l! 2$!$ 2lw 2 }# lff"2 $ hp,"dLRk^$}+lX(fillff"{n?p 6pv$ZX 2Wgj9 $\$vp $Hd2 \2 plff=2 $lff2+RX+2 B p2 $\2 p2lp2J < $R.p2 $Y,2$Rlp$ < pH$$$2 $ll2ll$ 2 ljc( S.Q 76pl!|2$ l2ll$92 $L9$\! $ $2 Hlff=2 )C$$2 $~ uR$2 \$$9$p^$XxX>D~ 2$p< $HlC$2l}$ &2 l}2 plff=2 x>2l$2 $~2ff$ef$!p2GLz,2p {l$# lff"2 $,ff)$2 ,+ $vp W*})$9;2Hp! J 2 p pff"! ?$< p ff2W)2pW 2 p lff2+ f# l22$< "2 $ $ dploCJ$= (fi2 p2$p$vp67Zw 92 pX.$$2l! 2 $! 2 $ R$2 -fE(`qaa=6 l29p =ff! $L l$\$"gpdpi>,pWs2 pjff"! ?$W<$2 >R;2 < pi2 $ffpJ2 pjpff"! $* 2 p$|X;J2 pR22! ff2.$$jl$ :2 $-p$Xjl,YX.$JX2 $$el<ff'2 $ 'J;<2 pxpv$2 pUJ yX.$lffC2j2 $! = '2 l %2 $@! $J;2ppffp&$"{pdpn( p. g ,+6g2 pb! ffX,sp$X$,$22$pj2 $2 pj)p!,"2p :?n$!p2bL )2~p! z oX2 p2 $ < $)$lB$< $@ppW2 p uX.l$}! 2 p))$2 n2 ppff"! $< p$$2}! 2$! 2 $D}* $W>3* \Xln@$9ff"! Hff9$$ "$2 < $2>2 l#lff"2@! ff} ->J$\X2* :2 $-2 J@R$2 2 $$2$zpdp9 WH-!>$= dp$"E2 $pdpE2 $X; W< p|X.l$2}! 2Jff$$29pb slff$$2Wd 2 $k^pb& 2$bp ,ff( +s/fi oxorlzGir$! 2$$2!tfE(`qaa=6+"!LX iLqH^ %ely$!p2xKff)>CpW$; !92p@l$\) 2l(Fp$XR+l) ,BX.$ 6Y.$nB 2pL2!pdpffff!$:.$JXo$vpXCff$*2p(P] 2$ff"!l}"h 2$!+ x$$Cff2g.$H $2 pH! pJ.2 2 $ff22$}gj2 $)p>"~* pJ ;2 ]$2Lffl$2^#2J2 pd$) uC$"2Y2 p2.! bpv$Lff"! $< $2*J,J9{2 p$" &nff92J2 $v>c( ,)L6 p2$X(Fff$X/ w! $ff *`aa`67po2 p$" :p$u^8Lg* ,2Jl| vJpdp2 -:)$ +2 #pC$Y$"y.$@pdpX>UJ y2 $>2 $CX2$22 pUff$$p2%2 $C!dpff"p! ?$V)$2 $?2 p:$$H! + "nT* ppb^0< ^$K|1l$ffl$2^#2Jk+l. 2 $R ,X$* $2ff$| 2x92 $-p$X'X.$J]HH $2 p! |2 p2g$+d$2s2 pff$l2pff"! $2p$G$2$2 $&X.$$.! ?= "ff*$ g "$ffp@2 pG,hH l;! C'2 p2 $X2$2 $pW: E$!pK5J*)p@>2l$ Bp\2 $@F$#$;! + o2 p2 lff"2 X* 2!& "22!l$GR2 $eff"ffly2 $ j2 p2lffLY2 p$"! pvgpjffff"$E! $2po2 $! + "p22 poX.$$uRH $$p2 $@ $GE2 $$$*% @LXl$).2 $Y2 p2ff2ff$$2Wd2 $U &2 p!m)$ff"!p2 l ->ff2 Jlff?! ff$}x ff9}* 2?$g>2$$2W@2 p2"l$2X2 J$! $2 l*)$>$pff'R$Ljff2Jlff}< $"2 $ffp*O$g$p+ =2 GlffG p $R$ 2 pv )2 pX; @RH $$p2$GX.$l! J2 $v$)X2 $Wb2 p :GXpC.2 gw $2+$!p2xK5y )! $U;<JC= "l$2X2 pC "$^$@9Oef! $V,.$@2 $ff$$p2 $Hff2y$"$$R$2 $D2 $@X.$l! + "ff*)p2 2 z2 JL:.2! l$u2 pJ2J < $Rpv~* p ,W*]pb2 pbX.l$R22ff= ym)z2 pJ= ffff*pH$&$22 pX.$l!)n?p>gff$2 yRp2 p,ffl$2p2 $ff2-! d,j2 $ )}$C p$+ "ffw_dffpjffffU$U2$W: 2$ %X.$l2l! 2LRJl,$!$ 2$)$ 2 "p$2?!$!vv 2ff$$ 2el >X",p J 2 2lCvlff" 2C] 2pjp >"$olx ff2$)$;#2+ 2} 2$ }$?!,!ief 2lY>W$ CCffllB jp$XC$W>xX.$$,+ff"!?$gl2ll$,2X$vpH\ 2pL= 92lbp>"@}p2=2 $h>pp X.lp; 2$|)$9|p ,ff$2X" 2jff2RX$2$lY2 p:2 $! + "p2,J2 p-X.$$.$ef$!pBP^*>l; 2$*V,2WF2$l<O>H2"$2 ,LJ2 $! + XX.$l]el^v$ 2$ px}y$Lpl2<$2 ip! =Xp$dff* 2$}X.$$2}! 2X"dg 2 2$Hn 2p@p,ff w L$Wpp* pW>*)+o2 $$$ff2U ' 2pd 2>x.l@2$D]w x 9y )C$L222 yR$2 2 pU2$~ C2 px>$! +2(29$ 2!j 2pBp >"X +2 X >c $ ;~ .$; 9$2 36 Y)$2 |2 pRd.ff$" 22pp,ff'( lD! 2);2 67*ll?D2 pk^$$2 p$$UD 92pgk^$+2 $2 iR2g>X$RXuk+$;2lJ}d2 p)$! 2 $ $$Lp ?pb^2 $);2 R2l< $)J2 2 $2$>}ff ipdppx$*)$L2 pU$! 2 $ $"$$2 }$d$.$,p"dU"$9x,c ;~ .9$p'R"2 p*2 p2>Lff"!! gff9$$ "p)${X.$l2l! 2 2\Xp$hNV$9p,ffv@ vl$W$fix |lr.u>k i.$impo r^$XU.$ 2$@pv$&)$9(P@ 2p $} 2p[ff"WE} 2p!u &Xj 2lg2p$$2l! 2$!$ 2$uw jC= } $2$2jpJX"2 +,$ffJ2 $BX.l$l! 2$! $ 2l>$p $p2)X2elG 2$Hffff 2y,y.$px? )+z9k^$ 22^ py 2$$j2plff $vpY)l$C}XH?2 $)$$! =Xp9pd''w $*$p ,W*$F2{p 9p2Z ? elvN 2 >`Z>;2 >2 $"=$l$2n2 p! $2 $ 92 lb2 $ {UnVn F$2p! p pJ.22 $yH 2b! p*O;2 "=$l$2 B2 $ g>2pl9pvo?"$9n2 l (TV;U6 nXJ+2 $X* 2$g2 lR2 pk+$+2 $ D,yp'(9k^$2 u3679e]2 pgp>"; ff$"$2$2p:2 pffJlff D^u?)u)9$ ;P".$ 2 2})$2 H2p! @ 2}).2 *W>>LJ X2 $>l$$2 $$2l}$2$x2po+$@X$vp$2^Z?J" ff"! \$Lff2 ff$ "p22 plff9RD2 $RgX2X$! Ym)$u2 $.<&p,"$;2 J!D^$X2 ltP".$2 \ j2 $ff2p{ 9 2 pg^+lXxpdphmjp2plGXCff2}2 pJX"2 +$IHX2$bX>s2 pJ2 pd .2 pp2$D2 p2 pv<$$$2 l!D(F#0 ff9*]23KK67*)$9pvpR.2l9w $|p2k^$2D2 $lff"j2 $np>"2 $J2 pu2 plff {'( ,pdp&p? *BF$6Y &X) 2 l< pX$2l} p+)$! $2 l bC$"$2 llff 9l* 2lj2 pgpdp"l\Uff2j! pl2D)$$$$X2 $<ff"2 }>C$} 2LBJx,l<.pff$2! ! < $< $DlC ,ff2 $;2 p}!bH-+ J$WJ+2 # $d! $! 2 $$2$$$92 p>2"l$2 gl2+ ?2 $#ff"2 ]w l! pY2 $J2Uff$# J2 pd$$&$C. 'I2 p'92 $ 92 p) $9 &Xk+l2 u2jlff'H}>;$$}2 p+ ff2:~ 2 < jXJ$22O2 p)l; +2 jl.$+Llffff2 $ffX%ff2Up2 $&`-%j2 )y2 p 09)2 p2 $&`- U X$V#T~ 2+ *+2 +$<ld!2}2 C 2[ WX* 2 pgpV#T~ J= Uff2p$$*V2 p2~ 2X$2.!|F "p2y# g* 2?$y2 p$! $2 lj DXJ"$2 l}ffJ.2!u(FRH \/[! $p*J233a=67,8` b]}< $C2ff9)"p$X" h^h 2p < .p!up.$$ 28 Q* *lffll2$pd2$! J% 2p@[$" "jpjH H3& $ D2 pj 9.p !GlHH*]A@$*i/ [!$$*%A@}(f233a=67{0ff,?pff$!.!sCvffp2$?l; 2= 2JellJqL 45^qK`^$$2J;fffifi !""#$&%(')fi*!,+.-/1023+.04+*c>$*Je$*NpJ$. *$*iC$~*$*/ >X*<x(f23K367 < $HBtOHZ-[ J$ !"v; uH L; "$.D)$2 ,$2}l" 22$g 9.$k^p-BXp >"def.vN $.! ;# <!*c>657-80++9;:<%=!>+@? A7-*+9BC')D%2+2-+20+<FE=GAHI+J"08KL+3Mfi oxorlzGirA57-804+J+9BK;:B7&%7!>+ON@ff!>=ff!,+.-4B!"PBQR')B8%2+2-+20+B(EG-4!" S02PQ1NA+.-Q1NA+2!"T-UD #M^V* < $*X/$*9[-(f233367VMff 2$$2ll; 22$$ 2p61W9X Q$ >" YB-4QZ&%=E=-.!" S[02PQ1ff!,+2QKQ\]:+.04+G^_+48+9-0.=> *)`b5a *`3V27^``^H= J*n+(f233367V02d.$ 2~vdX" 2ef* <O2*l$JJ27^P^dff$X*@$*O/vw!$ff *wX(`aa`67|HpW,$ 22pvJ%ef29pJbg 2pl; jnff9 2cEd 10+4@FNA+.-Q18%2-4fi/B!""Be57-80+.J;:$#.!,+.fiA7*#M<TJ24*pJell22d$9$$*t9(f23L367HX$Vp$!$ 2l[??pff$2!!('?0-$22 67fff<UgQhE=U b+2fiiKPjN<Ukff#B^ *@?b?Dl *@2a3W 2a3P^ 9.!2 $2 2-Nv v n[ 2p 2d*#<M ?$J@`aV*O23V2#ff2354t~2* [$*/dff$X*@%(`aa2W67jH 2$!+ )X.$D-!9ffl$#Jv$=NA+.-Q[')fim!3 !"" *Z`Bn (367*`V24=3W^`V2WL^2t~2*[$*J/dff$X* @i(`aa`67[$*322 2$$z$$-elc ff>*)N$*/ $pl*x,j(F,$ 67*7* <O24$* lJ4=qqW=4=P`^*U$9!*[\H [ne < 02; 9* <~$*Ed B0+.opNA+.-Qq8%.B-fi/B!"Pr57-80+.J4K;:ffg.!,+.fiA[\p2l+*%d&$*]w *%_$*i/ l*%[(f233367st~v^EXU$2}! 2ZJff$$p?$OHB Jl9<; "$|el* <X2q^*#$J4=PL=4+Lq^tsu04+.-.!3ff!"cIE=-4!" S08"Qu!,+.QQvh:;+20+#EG-4!" S02PQff!,+.QQvh:;+20+*wbx*L^27 22^0ff**.O(f23KK67y5y-&;z9bz2KQ\{.!""0A-9+9.#;:(KoK!,+.QQvh:;+2ff!O44!,+.fiA*$$J`a45^`V2aVx[ff!ndU#.u09.l2$*=ef$O|h8} a5#354=PV2 #lL #lL+AR*V(f233`67 C.p 2$; #ff7$!^$ >"v0 *X)$*'/>$"vHB$p9*x(f23KL67H JC$s 2p"* * 33qW 223^A')fiQ~+9$44!,+.fiA u`ffff$?!!$ 2$y$p0jUffV* < %(f23KV2W67>+"!$J$l$ 22p Q=ROS k^$ 2\) 2pLCl$ #!2!2lh!2)Jv$* *]23L^27 23LK^7Y-4Q&%=5[>;g4"02=EHIB!>+2fi !""0QZ1:;+.L+.-Q Z` wNd$T*+ti$*+V* < $*/$*[EJ(f233P679[hC$ 2p"U2$!|XJp >"d=* * PV27VLP^Y*-4QZ%=E=-.!" S08"BQff!,+.QQvh:;+20+^q+42+J-0.>Nd+,*[$*[$l * cB$*R"J* $*Rp*[$*5R"d$ ;~*$*t~$* @$*/1CX*y)(f233V2W670 2}l"2 $!p2|$2?!$ 2j 2p Q QVp)! g}* 5*`54.27^`qq^|HI+8!>8 A%=8%2-fi/B!"PcHI+J"08KL+ n;a0J*Kgx(f23LL67ZNv 2* ](67*q3W^Pa2< pl2* $*'HB$p*]0%$*'/!2$[57>bKQ\K.ff>;"0QRHI8:bg.1+ n;w| W8;jnlJdpjRlHB!>+2fi/B!""0Q57-9: -&fiifii;:*ub`*< v$J*^[(`aa`67 < $R+&' 2Ug$ffj$2!9!4.2WL=4=P^w+$!+ *[E$* ^V* <$*/w;^* H@(`aa`67Hp2H9$$X-X.$lz 2psu04+.-.!3ff!"cE=-4!" S08"Q!,+.QQvh:;+20+*#M<O2K^!lff" 2$ 2hfi.l 2el_$lV*.$*V$2*Vw$*/w,X$.!* A@$*/0*6Ed 10+4*=_yJ(`aaa=67jHp~Xl2l! 2]elcN<+2-&QL8%.B-fi/B!"Po5y-&804+4;:F#4!,+2fiA7*#M<]2^*$$JPK3W^P3q^(f23354+67jH2Jlffl$2\ DcC2Gp ,ffJ 2 2$:ef5y-&D0+J+J;:@&%G!>+/1+.ff!>o')B Pj')B8%2+2-+20+E=-.!" S[02PQff!,+2QKQ\h:;+.10+*$J2WL^272WLK^fiJournal Artificial Intelligence Research 19 (2003) 315-354Submitted 12//02; published 10/03Learning Training Data Costly: Effect ClassDistribution Tree InductionGary M. WeissGMWEISS@ATT.COMAT&T Labs, 30 Knightsbridge RoadPiscataway, NJ 08854 USAFoster ProvostFPROVOST@STERN.NYU.EDUNew York University, Stern School Business44 W. 4th St., New York, NY 10012 USAAbstractlarge, real-world inductive learning problems, number training examples oftenmust limited due costs associated procuring, preparing, storing trainingexamples and/or computational costs associated learning them. circumstances, one question practical importance is: n training examples selected,proportion classes represented? article help answerquestion analyzing, fixed training-set size, relationship class distribution training data performance classification trees induced data.study twenty-six data sets and, each, determine best class distribution learning.naturally occurring class distribution shown generally perform well classifierperformance evaluated using undifferentiated error rate (0/1 loss). However, areaROC curve used evaluate classifier performance, balanced distributionshown perform well. Since neither choices class distribution always generatesbest-performing classifier, introduce budget-sensitive progressive sampling algorithm selecting training examples based class associated example.empirical analysis algorithm shows class distribution resulting trainingset yields classifiers good (nearly-optimal) classification performance.1. Introductionmany real-world situations number training examples must limited obtainingexamples form suitable learning may costly and/or learning examples maycostly. costs include cost obtaining raw data, cleaning data, storingdata, transforming data representation suitable learning, well costcomputer hardware, cost associated time takes learn data, opportunity cost associated suboptimal learning extremely large data sets due limitedcomputational resources (Turney, 2000). costs make necessary limit amounttraining data, important question is: proportion classes representedtraining data? answering question, article makes two main contributions. addresses (for classification-tree induction) practical problem select class distribution training data amount training data must limited, and, providingdetailed empirical study effect class distribution classifier performance, providesbetter understanding role class distribution learning.2003 AI Access Foundation Morgan Kaufmann Publishers. Rights Reserved.fiWeiss & Provostpractitioners believe naturally occurring marginal class distributionused learning, new examples classified using model built underlying distribution. practitioners believe training set contain increasedpercentage minority-class examples, otherwise induced classifier classifyminority-class examples well. latter viewpoint expressed statement, samplesize fixed, balanced sample usually produce accurate predictions unbalanced 5%/95% split (SAS, 2001). However, aware thorough prior empirical studyrelationship class distribution training data classifier performance,neither views validated choice class distribution often madearbitrarilyand little understanding consequences. article provide thorough study relationship class distribution classifier performance provideguidelinesas well progressive sampling algorithmfor determining good class distribution use learning.two situations research described article direct practical use.training data must limited due cost learning data, resultsand guidelines establishcan help determine class distributionused training data. case, guidelines determine many examplesclass omit training set cost learning acceptable. secondscenario training examples costly procure number training examplesmust limited. case research presented article used determineproportion training examples belonging class procured ordermaximize classifier performance. Note assumes one select examples belongingspecific class. situation occurs variety situations, examplesbelonging class produced stored separately main cost due transforming raw data form suitable learning rather cost obtaining raw,labeled, data.Fraud detection (Fawcett & Provost, 1997) provides one example training instances belonging class come different sources may procured independently class.Typically, bill paid, transactions credited fraudulent storedseparately legitimate transactions. Furthermore transactions credited customerfraudulent may fact legitimate, transactions must undergo verificationprocess used training data.situations, labeled raw data obtained cheaply, processforming usable training examples raw data expensive. example, considerphone data set, one twenty-six data sets analyzed article. data set usedlearn classify whether phone line associated business residential customer.data set constructed low-level call-detail records describe phone call,record includes originating terminating phone numbers, time call made,day week duration call. may hundreds even thousands call-detailrecords associated given phone line, must summarized single training example. Billions call-detail records, covering hundreds millions phone lines, potentially available learning. effort associated loading data dozenscomputer tapes, disk-space limitations enormous processing time required summarize raw data, feasible construct data set using available raw data. Consequently, number usable training examples must limited. case donebased class associated phone linewhich known. phone data set316fiLearning Training Data Costly: Effect Class Distribution Tree Inductionlimited include approximately 650,000 training examples, generated approximately 600 million call-detail records. huge transaction-oriented databasesroutinely used learning, expect number training examples alsoneed limited many cases.remainder article organized follows. Section 2 introduces terminologyused throughout article. Section 3 describes adjust classifier compensatechanges made class distribution training set, generated classifierimproperly biased. experimental methodology twenty-six benchmark data sets analyzed article described Section 4. Section 5 performance classifiersinduced twenty-six naturally unbalanced data sets analyzed, order showclass distribution affects behavior performance induced classifiers. Section 6,includes main empirical results, analyzes varying class distributiontraining data affects classifier performance. Section 7 describes progressive samplingalgorithm selecting training examples, resulting class distribution yields classifiers perform well. Related research described Section 8 limitations researchfuture research directions discussed Section 9. main lessons learnedresearch summarized Section 10.2. Background NotationLet x instance drawn fixed distribution D. Every instance x mapped (perhapsprobabilistically) class C {p, n} function c, c represents true, unknown, classification function.1 /HW EH WKH PDUJLQDO SUREDELOLW\ RI PHPEHUVKLS RI xpositive class 1 WKH PDUJLQDO SURbability membership negative class.marginal probabilities sometimes referred class priors base rate.classifier mapping instances x classes {p, n} approximation c.notational convenience, let t(x) {P, N} always clear whether class valueactual (lower case) predicted (upper case) value. expected accuracy classifier t, t,GHILQHGDV = Pr(t(x) = c(x)), or, equivalently as:Pr(t(x) = P | c(x) = p) + (1 fiPr(t(x) = N | c(x) = n)[1]Many classifiers produce classification, also estimates probability xtake class value. Let Postt(x) classifier ts estimated (posterior) probabilityinstance x, c(x) = p. Classifiers produce class-membership probabilities produce classification applying numeric threshold posterior probabilities. example, thresholdvalue .5 may used t(x) = P iff Postt (x) > .5; otherwise t(x) = N.variety classifiers function partitioning input space set L disjoint regions(a region defined set potential instances). example, classification tree,regions described conjoining conditions leading leaves tree. regionL L ZLOOFRQWDLQVRPHQXPEHURIWUDLQLQJLQVWDQFHV L/HW LpDQG Ln numberspositiYHDQGQHJDWLYHWUDLQLQJLQVWDQFHVLQUHJLRQ/VXFKWKDW L Lp + Ln. classifiers1. paper addresses binary classification; positive class always corresponds minority class negative class majority class.317fiWeiss & Provostoften estimate Postt(x | x /fiDV Lpff Lp+ Ln) assign classification instances x Lbased estimate numeric threshold, described earlier. Now, let LP LNsets regions predict positive negative classes, respectively, LP LN = L.region L L, tKDVDQDVVRFLDWHGDFFXUDF\ L = Pr(c(x) = t(x) | x Lfi/HW LP represent expected accuracy x LPDQG LN expected accuracy x LN.2 shall seeLQ6HFWLRQZHH[SHFW LP LN .5.3. Correcting Changes Class Distribution Training SetMany classifier induction algorithms assume training test data drawnfixed, underlying, distribution D. particular, algorithms assume rtrain rtest,fractions positive examples training test sets, approximDWH WKH WUXH SULRUprobability encountering positive example. induction algorithms use estimatedclass priors based rtrain, either implicitly explicitly, construct model assign classifications. estimated value class priors accurate, posterior probabilities model improperly biased. Specifically, increasing prior probabilityclass increases posterior probability class, moving classification boundaryclass cases classified class (SAS, 2001). Thus, training-set dataselected rtrain GRHVQRWDSSUR[LPDWH WKHQWKHSRVWHULRUSUREDELOLWLHVVKRXOGEHDdjusted based differences beWZHHQ DQGrtrain. correction performed,resulting bias cause classifier classify preferentially sampled class accurately, overall accuracy classifier almost always suffer (we discussSection 4 provide supporting evidence Appendix A).3majority experiments described article class distribution training setpurposefully altered rtrainGRHVQRWDSSUR[LPDWH 7KHSXUSRVHIRUPRGLIying classdistribution training set evaluate change affects overall performanceclassifierand whether produce better-performing classifiers. However,want biased posterior probability estimates affect results. section describemethod adjusting posterior probabilities account difference rtrainDQGmethod (Weiss & Provost, 2001) justified informally, using simple, intuitive, argument.Elkan (2001) presents equivalent method adjusting posterior probabilities, includingformal derivation.learning classification trees, differences rtrainDQG QRUPDOO\UHVXOWLQEiasedposterior class-probability estimates leaves. remove bias, adjust probabilityestimates take differences account. Two simple, common probability estimationIRUPXODV DUH OLVWHG LQ 7DEOH )RU HDFK OHW Lp ff Ln) represent number minority-class(majority-class) training examples leaf L decision tree (or, generally, withinregion L). uncorrected estimates, based assumption trainingtest sets drawn approximate , estimate probability seeing minority-class(positive) example L. uncorrected frequency-based estimate straightforward requires explanation. However, estimate perform well sample size,Lp Ln, smalland even defined sample size 0. reasons2. notational convenience treat LP LN union sets instances corresponding regions.3. situations costly misclassify minority-class examples majority-class examples, practitioners sometimes introduce bias purpose.318fiLearning Training Data Costly: Effect Class Distribution Tree InductionLaplace estimate often used instead. consider version based Laplace law succession (Good, 1965). probability estimate always closer 0.5 frequencybased estimate, difference two estimates negligible large samplesizes.Estimate NameFrequency-BasedLaplace (law succession)UncorrectedLp/( Lp+ Ln)ff Lp+1)/( Lp+ Ln+2)CorrectedLpff Lp+o Ln)ff Lpfiff Lp+o Ln+2)Table 1: Probability Estimates Observing Minority-Class Examplecorrected versions estimates Table 1 account differences rtrainDQGfactoring over-sampling ratio o, measures degree minority classover-sampled training set relative naturally occurring distribution. valuecomputed ratio minority-class examples majority-class examples training setdivided ratio naturally occurring class distribution. ratio minoritymajority examples 1:2 training set 1:6 naturally occurring distribution,would 3. learner account properly differences rtrainDQG E\XVLQJcorrected estimates calculate posterior probabilities L.example, ratio minority-class examples majority-class examples naturally occurring class distribution 1:5 training distribution modified ratio1:1, 1.0/0.2, 5. L labeled minority class probability mustgreater 0.5, so, using corrected frequency-EDVHG HVWLPDWH Lpff Lp Ln) > 0.5, or,Lp! Ln. Thus, L labeled minority class covers times many minorityclass examples majority-class examples. Note calculating use classratios fraction examples belonging minority class (if mistakenly usedlatter example, would one-half divided one-sixth, 3). Using classratios substantially simplifies formulas leads easily understood estimates. Elkan(2001) provides complex, equivalent, formula uses fractions instead ratios.discussion assume good approximation true base rate known.real-world situations true different methods required compensatechanges training set (Provost & Fawcett, 2001; Saerens et al., 2002).order demonstrate importance using corrected estimates, Appendix presentsresults comparing decision trees labeled using uncorrected frequency-based estimatetrees using corrected frequency-based estimate. comparison shows particularmodification class distribution training sets (they modified classesbalanced), using corrected estimates yields classifiers substantially outperform classifierslabeled using uncorrected estimate. particular, twenty-six data sets usedstudy, corrected frequency-based estimate yields relative reduction error rate 10.6%.Furthermore, one twenty-six data sets corrected estimate perform worse.Consequently critical take differences class distributions accountlabeling leaves. Previous work modifying class distribution training set (Catlett,1991; Chan & Stolfo, 1998; Japkowicz, 2002) take differences accountundoubtedly affected results.319fiWeiss & Provost4. Experimental Setupsection describe data sets analyzed article, sampling strategy usedalter class distribution training data, classifier induction program used, and, finally,metrics evaluating performance induced classifiers.4.1 Data Sets Method Generating Training Datatwenty-six data sets used throughout article described Table 2. collectionincludes twenty data sets UCI repository (Blake & Merz, 1998), five data sets, identified +, previously published work researchers AT&T (Cohen & Singer,1999), one new data set, phone data set, generated authors. data sets Table2 listed order decreasing class imbalance, convention used throughout article.Datasetletter-a*pendigits*abalone*sick-euthyroidconnect-4*optdigits*covertype*solar-flare*phoneletter-vowel*contraceptive*adultsplice-junction*% Minority DatasetExamplesSize# Dataset3.920,000 14 network28.313,821 15 yeast*8.74,177 16 network1+9.33,163 17 car*9.511,258 18 german9.95,620 19 breast-wisc14.8581,102 20 blackjack+15.71,389 21 weather+18.2652,557 22 bands19.420,000 23 market1+22.61,473 24 crx23.948,842 25 kr-vs-kp24.13,175 26 move+% Minority DatasetExamplesSize27.93,82628.91,48429.23,57730.01,72830.01,00034.569935.615,00040.15,59742.253843.03,18144.569047.83,19649.93,029Table 2: Description Data Setsorder simplify presentation analysis results, data setstwo classes mapped two-class problems. accomplished designating oneoriginal classes, typically least frequently occurring class, minority classmapping remaining classes majority class. data sets originally contained2 classes identified asterisk (*) Table 2. letter-a data set created letter-recognition data set assigning examples labeled letterminority class; letter-vowel data set created assigning examples labeledvowel minority class.generated training sets different class distributions follows. experimental run, first test set formed randomly selecting 25% minority-class examples25% majority-class examples original data set, without replacement (the resultingtest set therefore conforms original class distribution). remaining data availabletraining. ensure experiments given data set training-set sizematter class distribution training setthe training-set size, S, made equaltotal number minority-class examples still available training (i.e., 75% original320fiLearning Training Data Costly: Effect Class Distribution Tree Inductionnumber). makes possible, without replicating examples, generate class distribution training-set size S. training set formed random samplingremaining data, without replacement, desired class distribution achieved.experiments described article, class distribution training set variedminority class accounts 2% 95% training data.4.2 C4.5 Pruningexperiments article use C4.5, program inducing classification trees labeledexamples (Quinlan, 1993). C4.5 uses uncorrected frequency-based estimate labelleaves decision tree, since assumes training data approximate true, underlying distribution. Given modify class distribution training set, essentialuse corrected estimates re-label leaves induced tree. results presentedbody article based use corrected versions frequency-basedLaplace estimates (described Table 1), using probability threshold .5 label leavesinduced decision trees.C4.5 factor differences class distributions training testsetswe adjust post-processing step. C4.5s pruning strategy, attemptsminimize error rate, allowed execute, would prune based false assumption (viz.,test distribution matches training distribution). Since may negatively affectgenerated classifier, except otherwise indicated results based C4.5 without pruning. decision supported recent research, indicates target misclassification costs (or class distributions) unknown standard pruning improve,may degrade, generalization performance (Provost & Domingos, 2001; Zadrozny & Elkan, 2001;Bradford et al., 1998; Bauer & Kohavi, 1999). Indeed, Bradford et al. (1998) found evenpruning strategy adapted take misclassification costs class distribution account,generally improve performance classifier. Nonetheless, order justifyusing C4.5 without pruning, also present results C4.5 pruning trainingset uses natural distribution. situation C4.5s assumption rtrainDSSUR[LPDWLQJvalid hence pruning strategy perform properly. Looking ahead, results showC4.5 without pruning indeed performs competitively C4.5 pruning.4.3 Evaluating Classifier Performancevariety metrics assessing classifier performance based terms listed confusion matrix shown below.c(x) Actual PositiveActual Negativet(x)Positive Prediction Negative Predictiontp (true positive)fn (false negative)fp (false positive)tn (true negative)Table 3 summarizes eight metrics. metrics described first two rows measureability classifier classify positive negative examples correctly, metricsdescribed last two rows measure effectiveness predictions made classifier.example, positive predictive value (PPV), precision, classifier measures fraction positive predictions correctly classified. metrics described last two321fiWeiss & Provostrows Table 3 used throughout article evaluate various training-set class distributions affect predictions made induced classifiers. Finally, metrics secondcolumn Table 3 complements corresponding metrics first column,alternatively computed subtracting value first column 1. specifically,proceeding row 1 4, metrics column 1 (column 2) represent: 1) accuracy(error rate) classifying positive/minority examples, 2) accuracy (error rate) classifying negative/minority examples, 3) accuracy (error rate) positive/minority predictions, 4) accuracy (error rate) negative/majority predictions.tptp + fnTrue Positive Rate FN = Pr(N|p)(recall sensitivity)fntp + fnFalse Negative RateTN = Pr(N|n)tntn + fpTrue Negative Rate FP = Pr(P|n)(specificity)fptn + fpFalse Positive RatePPV = Pr(p|P)tptp + fpNPV = Pr(n|N)tntn + fnTP = Pr(P|p)fpPositive Predictive Value PPV = Pr(n|P)(precision)tp + fpNegative Predictive Value NPV =Pr(y|N)fntn + fnTable 3: Classifier Performance Metricsuse two performance measures gauge overall performance classifier: classification accuracy area ROC curve (Bradley, 1997). Classification accuracy (tp +fp)/(tp + fp + tn + fn). formula, represents fraction examples correctlyFODVVLILHGLVDQHVWLPDWHRIWKHH[SHFWHGDFFXUDF\ t, defined earlier equation 1. Throughoutarticle specify classification accuracy terms error rate, 1 accuracy.consider classification accuracy part common evaluation metricmachine-learning research. However, using accuracy performance measure assumestarget (marginal) class distribution known unchanging and, importantly,error coststhe costs false positive false negativeare equal. assumptionsunrealistic many domains (Provost et al., 1998). Furthermore, highly unbalanced data setstypically highly non-uniform error costs favor minority class, which, casemedical diagnosis fraud detection, class primary interest. use accuracycases particularly suspect since, discuss Section 5.2, heavily biased favormajority class therefore sometimes generate classifiers never predict minority class. cases, Receiver Operating Characteristic (ROC) analysis appropriate(Swets et al., 2000; Bradley, 1997; Provost & Fawcett, 2001). producing ROC curvesuse Laplace estimate estimate probabilities leaves, since shownyield consistent improvements (Provost & Domingos, 2001). assess overall qualityclassifier measure fraction total area falls ROC curve (AUC),equivalent several statistical measures evaluating classification ranking models(Hand, 1997). Larger AUC values indicate generally better classifier performance and, particular, indicate better ability rank cases likelihood class membership.322fiLearning Training Data Costly: Effect Class Distribution Tree Induction5. Learning Unbalanced Data Setsanalyze classifiers induced twenty-six naturally unbalanced data sets described Table 2, focusing differences performance minority majorityclasses. alter class distribution training data section, classifiers need adjusted using method described Section 3. However, experiments consistent Section 6 use natural distribution, sizetraining set reduced, described Section 4.1.addressing differences, important discuss issue may lead confusion left untreated. Practitioners noted learning performance often unsatisfactorylearning data sets minority class substantially underrepresented. particular, observe large error rate minority class. clearTable 3 associated discussion, two different notions error rateminority class: minority-class predictions could high error rate (large PPV )minority-class test examples could high error rate (large FN). practitioners observeerror rate unsatisfactory minority class, usually referring factminority-class examples high error rate (large FN). analysis sectionshow error rate associated minority-class predictions ( PPV ) minority-class test examples (FN) much larger majority-class counterparts ( NPVFP, respectively). discuss several explanations observed differences.5.1 Experimental Resultsperformances classifiers induced twenty-six unbalanced data sets described Table 4. table warrants explanation. first column specifies data setname second column, convenience copied Table 2, specifiespercentage minority-class examples natural class distribution. third columnspecifies percentage total test errors attributed test examplesbelong minority class. comparing values columns two three seecases disproportionately large percentage errors come minority-class examples. instance, minority-class examples make 3.9% letter-a data set contribute 58.3% errors. Furthermore, 22 26 data sets majority errorsattributed minority-class examples.fourth column specifies number leaves labeled minority majorityclasses shows two cases fewer leaves labeled minority classmajority class. fifth column, Coverage, specifies average numbertraining examples minority-labeled majority-labeled leaf classifies (covers).results indicate leaves labeled minority class formed far fewer trainingexamples labeled majority class.Prediction ER column specifies error rates associated minority-classmajority-class predictions, based performance predictions classifying testexamples. Actuals ER column specifies classification error rates minoritymajority class examples, based test set. last two columns also labeledusing terms defined Section 2 ( PPV , NPV , FN, FP). example, columnsshow letter-a data set minority-labeled predictions error rate 32.5%majority-labeled predictions error rate 1.7%, minorityclass test examples classification error rate 41.5% majority-class test exam323fiWeiss & Provostples error rate 1.2%. last two columns underline highererror rate.Datasetletter-apendigitsabalonesick-euthyroidconnect-4optdigitscovertypesolar-flarephoneletter-vowelcontraceptiveadultsplice-junctionnetwork2yeastnetwork1cargermanbreast-wiscblackjackweatherbandsmarket1crxkr-vs-kpmoveAverageMedian% Minority % ErrorsExamples Min.3.98.38.79.39.59.914.815.718.219.422.623.924.127.928.929.230.030.034.535.640.142.243.044.547.849.925.826.058.332.468.951.251.473.016.764.464.461.848.757.558.957.158.957.158.655.445.781.550.791.250.351.054.061.456.957.3LeavesMin. Maj.CoverageMin. Maj.Prediction ERMin.Maj.Actuals ERMin.Maj.116544715350121008233316272650842383451313452872823235120332.24.316.8 109.32.8 35.57.1 26.91.75.82.92.427.3 123.21.73.113.0 62.72.40.91.82.83.11.65.59.64.0 10.314.4 26.15.1 12.83.16.62.02.012.6 26.057.7 188.05.07.21.40.35.12.73.92.124.0 41.22.40.68.8 27.43.96.9(PPV)32.525.869.822.555.818.023.167.830.827.069.834.315.148.245.646.214.057.111.428.941.017.830.923.21.224.433.929.9(FN)41.514.384.424.757.636.75.778.944.637.568.341.520.355.555.053.918.662.49.864.441.769.831.224.11.433.941.441.513888912817344648122025477041184661124942815191423892276515102542667(NPV)1.71.37.72.56.03.91.013.79.58.720.112.66.320.420.921.07.725.45.127.927.734.823.418.91.329.913.811.1(FP)1.22.73.62.45.71.54.98.15.55.621.19.64.516.215.616.75.621.56.18.127.14.923.318.51.121.210.15.9Table 4: Behavior Classifiers Induced Unbalanced Data Setsresults Table 4 clearly demonstrate minority-class predictions perform muchworse majority-class predictions minority-class examples misclassifiedmuch frequently majority-class examples. twenty-six data sets, minoritypredictions average error rate ( PPV ) 33.9% majority-class predictionsaverage error rate ( NPV ) 13.8%. Furthermore, three twenty-six datasets majority-class predictions higher error rateand three data setsclass distributions slightly unbalanced. Table 4 also shows us average error rateminority-class test examples (FN) 41.4% whereas majority-class test exampleserror rate (FP) 10.1%. every one twenty-six cases minority-class testexamples higher error rate majority-class test examples.324fiLearning Training Data Costly: Effect Class Distribution Tree Induction5.2 Discussionminority-class predictions higher error rate ( PPV ) majority-classpredictions ( NPV )? least two reasons. First, consider classifier trandompartitions L chosen randomly assignment L L LP LN also maderandomly (recall LP LN represent regions labeled positive negativeclasses). two-class learning problem WKH H[SHFWHG RYHUDOO DFFXUDF\ t, randomlygenerated labeled classifier must 0.5. However, expected accuracy regionspositive partiWLRQ LPZLOOEH ZKLOHWKHH[SHFWHGDFFXUDF\RIWKHUHJLRQVLQWKHQHJDWLYHpartition, LN, 1 )RUDKLJKO\XQEDODQFHGFODVVGLVWULEXWLRQZKHUH LP = .01DQG LN = .99. Thus, scenario negative/majority predictions muchaccurate. test distribution effect small well-learned conceptlow Bayes error rate (and non-existent perfectly learned concept Bayes error rate0), many learning problems quite hard high Bayes error rates.4results Table 4 suggest second explanation minority-class predictionserror prone. According coverage results, minority-labeled predictions tend formedfewer training examples majority-labeled predictions. Small disjuncts,components disjunctive concepts (i.e., classification rules, decision-tree leaves, etc.) covertraining examples, shown much higher error rate large disjuncts(Holte, et al., 1989; Weiss & Hirsh, 2000). Consequently, rules/leaves labeled minority class higher error rate partly suffer problem smalldisjuncts.Next, minority-class examples classified incorrectly much often majorityclass examples (FN > FP)a phenomenon also observed others (Japkowicz &Stephen, 2002)? Consider estimated accuracy, at, classifier t, test set drawntrue, underlying distribution D:= TP rtest + TN (1 rtest)[2]Since positive class corresponds minority class, rtest < .5, highly unbalanceddata sets rtest << .5. Therefore, false-positive errors damaging classification accuracyfalse negative errors are. classifier induced using induction algorithm gearedtoward maximizing accuracy therefore prefer false-negative errors false-positiveerrors. cause negative/majority examples predicted often hencelead higher error rate minority-class examples. One straightforward examplelearning algorithms exhibit behavior provided common-sense rule:evidence favoring one classification another, predict majority class. generally, induction algorithms maximize accuracy biased perform better classifying majority-class examples minority-class examples, since former componentweighted heavily calculating accuracy. also explains why, learningdata sets high degree class imbalance, classifiers rarely predict minority class.second reason minority-class examples misclassified often majorityclass examples fewer minority-class examples likely sampled distribu4. (optimal) Bayes error rate, using terminology Section 2, occurs t(.)=c(.). c(.) mayprobabilistic (e.g., noise present), Bayes error rate well-learned concept may always low.test distribution effect small concept well learned Bayes error rate low.325fiWeiss & Provosttion D. Therefore, training data less likely include (enough) instancesminority-class subconcepts concept space, learner may opportunityrepresent truly positive regions LP. this, minority-class test examplesmistakenly classified belonging majority class.Finally, worth noting PPV > NPV imply FN > FP. is,error-prone minority predictions imply minority-class examplesmisclassified often majority-class examples. Indeed, higher error rate minoritypredictions means majority-class test examples misclassified. reason generally observe lower error rate majority-class test examples (FN > FP)majority class predicted far often minority class.6. Effect Training-Set Class Distribution Classifier Performanceturn central questions study: different training-set class distributions affect performance induced classifiers class distributions leadbest classifiers? begin describing methodology determining class distribution performs best. Then, next two sections, evaluate analyze classifier performance twenty-six data sets using variety class distributions. use error rateperformance metric Section 6.2 AUC performance metric Section 6.3.6.1 Methodology Determining Optimum Training Class Distribution(s)order evaluate effect class distribution classifier performance, vary training-set class distributions twenty-six data sets using methodology described Section4.1. evaluate following twelve class distributions (expressed percentage minority-class examples): 2%, 5%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 95%.data set also evaluate performance using naturally occurring class distribution.try determine best class distribution training set, several issues must addressed. First, evaluate every possible class distribution,determine best distribution among 13 evaluated distributions. Beyondconcern, however, issue statistical significance and, generate classifiers13 training distributions, issue multiple comparisons (Jensen & Cohen, 2000).issues cannot always conclude distribution yields best performing classifiers truly best one training.take several steps address issues statistical significance multiple comparisons. enhance ability identify true differences classifier performance respectchanges class distribution, results presented section based 30 runs, rather10 runs employed Section 5. Also, rather trying determine best class distribution, adopt conservative approach, instead identify optimal range classdistributionsa range confident best distribution lies. identify optimal range class distributions, begin identifying, data set, class distributionyields classifiers perform best 30 runs. perform t-tests compareperformance 30 classifiers 30 classifiers generated usingtwelve class distributions (i.e., 12 t-tests n=30 data points). t-test yields probability .10 conclude best distribution different distribution(i.e., least 90% confident this); otherwise cannot conclude class distributions truly perform differently therefore group distributions together. grouped326fiLearning Training Data Costly: Effect Class Distribution Tree Inductiondistributions collectively form optimal range class distributions. Tables 5 6show, 50 52 cases optimal ranges contiguous, assuaging concerns conclusions due problems multiple comparisons.6.2 Relationship Class Distribution Classification Error RateTable 5 displays error rates classifiers induced twenty-six data sets.first column Table 5 specifies name data set next two columns specifyerror rates result using natural distribution, without pruning.next 12 columns present error rate values 12 fixed class distributions (without pruning). data set, best distribution (i.e., one lowest error rate) highlighted underlining displaying boldface. relative position naturaldistribution within range evaluated class distributions denoted use verticalbar columns. example, letter-a data set vertical bar indicatesnatural distribution falls 2% 5% distributions (from Table 2 see 3.9%).Error Rate using Specified Training Distribution(training distribution expressed % minority)DatasetNat-Prune Natletter-a2510203040506070Relative %Improvement809095 best vs. nat best vs. bal2.80 x 2.782.86 2.75 2.59 3.03 3.79 4.53 5.38 6.48 8.51 12.37 18.10 26.146.851.9pendigits3.65 + 3.745.77 3.95 3.63 3.45 3.70 3.64 4.02 4.48 4.98 5.73 8.83 13.367.814.2abalone10.68 x 10.469.04 9.61 10.64 13.19 15.33 20.76 22.97 24.09 26.44 27.70 27.73 33.9113.660.64.46 x 4.105.78 4.82 4.69 4.25 5.79 6.54 6.85 9.73 12.89 17.28 28.84 40.340.040.110.68 x 10.567.65 8.66 10.80 15.09 19.31 23.18 27.57 33.09 39.45 47.24 59.73 72.0827.672.34.94 x 4.688.91 7.01 4.05 3.05 2.83 2.79 3.41 3.87 5.15 5.75 9.72 12.8740.418.25.12 x 5.0322.6sick-euthyroidconnect-4optdigits5.54 5.04 5.00 5.26 5.64 5.95 6.46 7.23 8.50 10.18 13.03 16.270.6solar-flare19.16 + 19.98 16.54 17.52 18.96 21.45 23.03 25.49 29.12 30.73 33.74 38.31 44.72 52.2217.243.2phone12.63 x 12.62 13.45 12.87 12.32 12.68 13.25 13.94 14.81 15.97 17.32 18.73 20.24 21.072.416.8letter-vowel11.76 x 11.63 15.87 14.24 12.53 11.67 12.00 12.69 14.16 16.00 18.68 23.47 32.20 41.810.017.9contraceptive31.71 x 30.47 24.09 24.57 25.94 30.03 32.43 35.45 39.65 43.20 47.57 54.44 62.31 67.0720.939.2adult17.42 x 17.25 18.47 17.26 16.85 17.09 17.78 18.85 20.05 21.79 24.08 27.11 33.00 39.752.316.08.30 + 8.37 20.00 13.95 10.72 8.68 8.50 8.15 8.74 9.86 9.85 12.08 16.25 21.182.66.827.13 x 26.67 27.37 25.91 25.71 25.66 26.94 28.65 29.96 32.27 34.25 37.73 40.76 37.723.814.4yeast26.98 x 26.59 29.08 28.61 27.51 26.35 26.93 27.10 28.80 29.82 30.91 35.42 35.79 36.330.98.5network127.57 + 27.59 27.90 27.43 26.78 26.58 27.45 28.61 30.99 32.65 34.26 37.30 39.39 41.093.714.2covertypesplice-junctionnetwork2cargerman9.51 x 8.85 23.22 18.58 14.90 10.94 8.63 8.31 7.92 7.35 7.79 8.78 10.18 12.8616.97.233.76 x 33.41 30.17 30.39 31.01 32.59 33.08 34.15 37.09 40.55 44.04 48.36 55.07 60.999.718.70.07.41 x 6.82 20.65 14.04 11.00 8.12 7.49 6.82 6.74 7.30 6.94 7.53 10.02 10.561.2blackjack28.14 + 28.40 30.74 30.66 29.81 28.67 28.56 28.45 28.71 28.91 29.78 31.02 32.67 33.870.01.1weather33.68 + 33.69 38.41 36.89 35.25 33.68 33.11 33.43 34.61 36.69 38.36 41.68 47.23 51.691.74.3breast-wiscbands32.26 + 32.53 38.72 35.87 35.71 34.76 33.33 32.16 32.68 33.91 34.64 39.88 40.98 40.801.11.6market126.71 x 26.16 34.26 32.50 29.54 26.95 26.13 26.05 25.77 26.86 29.53 31.69 36.72 39.901.50.0crx20.99 x 20.39 35.99 30.86 27.68 23.61 20.84 20.82 21.48 21.64 22.20 23.98 28.09 32.850.05.11.25 + 1.39 12.18 6.50 3.20 2.33 1.73 1.16 1.22 1.34 1.53 2.55 3.66 6.0416.54.927.54 + 28.57 46.13 42.10 38.34 33.48 30.80 28.36 28.24 29.33 30.21 31.80 36.08 40.951.20.0kr-vs-kpmoveTable 5: Effect Training Set Class Distribution Error Rate327fiWeiss & Provosterror rate values significantly different, statistically, lowest errorrate (i.e., comparison yields t-test value > .10) shaded. Thus, letter-a data set,optimum range includes class distributions include 2% 10% minorityclass exampleswhich includes natural distribution. last two columns Table 5 showrelative improvement error rate achieved using best distribution instead natural balanced distributions. improvement statistically significant (i.e., associated t-test value .10) value displayed bold.results Table 5 show 9 26 data sets confident natural distribution within optimal range. 9 data sets, using best distributionrather natural distribution yields remarkably large relative reduction error rate.feel sufficient evidence conclude accuracy, training-set size mustlimited, appropriate simply assume natural distribution used.Inspection error-rate results Table 5 also shows best distribution differnatural distribution consistent mannersometimes includes minorityclass examples (e.g., optdigits, car) sometimes fewer (e.g., connect-4, solar-flare). However,clear data sets substantial amount class imbalance (the ones top halftable), balanced class distribution also best class distribution training,minimize undifferentiated error rate. specifically, none top-12 skewed datasets balanced class distribution within respective optimal ranges, datasets relative improvements balanced distributions striking.Let us consider error-rate values remaining 17 data sets t-test results permit us conclude best observed distribution truly outperforms natural distribution. cases see error rate values 12 training-set classdistributions usually form unimodal, nearly unimodal, distribution. distributionone would expect accuracy classifier progressively degrades deviatesbest distribution. suggests adjacent class distributions may indeed produceclassifiers perform differently, statistical testing sufficiently sensitiveidentify differences. Based this, suspect many observed improvementsshown last column Table 5 deemed significant statistically nonetheless meaningful.Figure 1 shows behavior learned classifiers adult, phone, covertype, letter-a data sets graphical form. figure natural distribution denoted Xtick mark associated error rate noted marker. error rate bestdistribution underlined displayed corresponding data point (for four datasets best distribution happens include 10% minority-class examples). Two curvesassociated data sets (adult, phone) >90% confident best distribution performs better natural distribution, two curves (covertype,letter-a) not. Note four curves perfectly unimodal. also clear neardistribution minimizes error rate, changes class distribution yield modest changeserror ratefar dramatic changes occur elsewhere. also evident datasets Table 5. convenient property given common goal minimizing error rate.property would far less evident correction described Section 3 performed, since classifiers induced class distributions deviating naturally occurring distribution would improperly biased.328fiLearning Training Data Costly: Effect Class Distribution Tree Induction25adultError Rate (%)2017.2516.851512.6212.32phone10covertype5.0352.75 5.002.59letter-a001020304050607080% Minority ClassFigure 1: Effect Class Distribution Error Rate Select Data SetsFinally, assess whether pruning would improved performance, consider secondcolumn Table 5, displays error rates result using C4.5 pruningnatural distribution (recall Section 4.2 case C4.5s pruning strategy give unbiased results). +/x second column indicates C4.5 pruningoutperforms/underperforms C4.5 without pruning, learning natural distribution.Note C4.5 pruning underperforms C4.5 without pruning 17 26 data sets,leads us conclude C4.5 without pruning reasonable learner. Furthermore,case C4.5 pruning generate classifier within optimal range C4.5 withoutpruning also generate classifier within range.6.3 Relationship Class Distribution AUCperformance induced classifiers, using AUC performance measure, displayedTable 6. viewing results, recall AUC larger values indicate improvedperformance. relative improvement classifier performance specified lasttwo columns, relative improvement performance calculated terms areaROC curve (i.e., 1 AUC). use area ROC curve betterreflects relative improvementjust Table 5 relative improvement specified termschange error rate instead change accuracy. before, relative improvements shown bold 90% confident reflect true improvement performance (i.e., t-test value figeneral, optimum ranges appear centered near, slightly right, balanced class distribution. 12 26 data sets optimum range include natural distribution (i.e., third column shaded). Note data sets,exception solar-flare data set, class distributions within optimal range containminority-class examples natural class distribution. Based results concludeeven strongly AUC (i.e., cost-sensitive classification ranking) accu329fiWeiss & Provostracy appropriate simply choose natural class distribution training. Table 6also shows that, unlike accuracy, balanced class distribution generally performs well,although always perform optimally. particular, see 19 26 datasets balanced distribution within optimal range. result surprising sinceAUC, unlike error rate, unaffected class distribution test set, effectively factors classifier performance class distributions.AUC using Specified Training Distribution(training distribution expressed % minority)DatasetNat-prune Nat251020304050607080Relative %Improv. (1-AUC)9095 best vs. nat best vs. balletter-a.500 x.772.711 .799 .865 .891 .911 .938 .937 .944 .951 .954 .952 .94079.8pendigits.962 x.967.892 .958 .971 .976 .978 .979 .979 .978 .977 .976 .966 .95736.427.00.0abalone.590 x.711.572 .667 .710 .751 .771 .775 .776 .778 .768 .733 .694 .68725.80.9sick-euthyroid.937 x.940.892 .908 .933 .943 .944 .949 .952 .951 .955 .945 .942 .92125.06.3connect-4.658 x.731.664 .702 .724 .759 .763 .777 .783 .793 .793 .789 .772 .73023.14.6optdigits.659 x.803.599 .653 .833 .900 .924 .943 .948 .959 .967 .965 .970 .96584.842.320.0covertype.982 x.984.970 .980 .984 .984 .983 .982 .980 .978 .976 .973 .968 .9600.0solar-flare.515 x.627.614 .611 .646 .627 .635 .636 .632 .650 .662 .652 .653 .6239.48.2phone.850 x.851.843 .850 .852 .851 .850 .850 .849 .848 .848 .850 .853 .8501.32.6letter-vowel.806 +.793.635 .673 .744 .799 .819 .842 .849 .861 .868 .868 .858 .83336.212.6contraceptive.539 x.611.567 .613 .617 .616 .622 .640 .635 .635 .640 .641 .627 .6137.71.6adult.853 +.839.816 .821 .829 .836 .842 .846 .851 .854 .858 .861 .861 .85513.76.7splice-junction.932 +.905.814 .820 .852 .908 .915 .925 .936 .938 .944 .950 .944 .94447.421.9network2.712 +.708.634 .696 .703 .708 .705 .704 .705 .702 .706 .710 .719 .6833.84.7yeast.702 x.705.547 .588 .650 .696 .727 .714 .720 .723 .715 .699 .659 .62110.92.5network1.707 +.705.626 .676 .697 .709 .709 .706 .702 .704 .708 .713 .709 .6962.73.7car.931 +.879.754 .757 .787 .851 .884 .892 .916 .932 .931 .936 .930 .91547.123.8german.660 +.646.573 .600 .632 .615 .635 .654 .645 .640 .650 .645 .643 .6132.32.5breast-wisc.951 x.958.876 .916 .940 .958 .963 .968 .966 .963 .963 .964 .949 .94823.85.9blackjack.682 x.700.593 .596 .628 .678 .688 .712 .713 .715 .700 .678 .604 .5585.00.71.5weather.748 +.736.694 .715 .728 .737 .738 .740 .736 .730 .736 .722 .718 .7021.5bands.604 x.623.522 .559 .564 .575 .599 .620 .618 .604 .601 .530 .526 .5360.01.3market1.815 +.811.724 .767 .785 .801 .810 .808 .816 .817 .812 .805 .795 .7813.20.5crx.889 +.852.804 .799 .805 .817 .834 .843 .853 .845 .857 .848 .853 .8669.58.8kr-vs-kp.996 x.997.937 .970 .991 .994 .997 .998 .998 .998 .997 .994 .988 .98233.30.0move.762 +.734.574 .606 .632 .671 .698 .726 .735 .738 .742 .736 .711 .6723.02.6Table 6: Effect Training Set Class Distribution AUClook results pruning, see 15 26 data sets C4.5 pruningunderperforms C4.5 without pruning. Thus, respect AUC, C4.5 without pruningreasonable learner. However, note car data set natural distribution pruningfalls optimum range, whereas without pruning not.Figure 2 shows class distribution affects AUC adult, covertype, letter-a datasets (the phone data set displayed Figure 1 would obscure adultdata set). Again, natural distribution denoted X tick mark. AUC bestdistribution underlined displayed corresponding data point. case alsosee near optimal class distribution AUC curves tend flatter, hence less sensitive changes class distribution.330fiLearning Training Data Costly: Effect Class Distribution Tree Induction1.0covertype.984.984.984.954letter-aAUC0.9.839.861.8618090adult0.8.7720.7010203040506070% Minority ClassFigure 2: Effect Class Distribution AUC Select Data SetsFigure 3 shows several ROC curves associated letter-vowel data set. curvesgenerated single run C4.5 (which AUC values exactly matchvalues Table 6). ROC space, point (0,0) corresponds strategy never makingpositive/minority prediction point (1,1) always predicting positive/minority class.Points northwest indicate improved performance.1.050% MinorityTrue Positive Rate0.80.690% Minority19% Minority (Natural)% Minority2%19%50%90%0.40.22% MinorityAUC.599.795.862.8550.00.00.20.40.60.8False Positive RateFigure 3: ROC Curves Letter-Vowel Data set3311.0fiWeiss & ProvostObserve different training distributions perform better different areas ROC space.Specifically note classifier trained 90% minority-class examples performs substantially better classifier trained natural distribution high true-positive ratesclassifier training 2% minority-class examples performs fairly well low truepositive rates. Why? small sample minority-class examples (2%) classifieridentify minority-labeled rules high confidence. However, much largersample minority-class examples (90%) identify many minority-labeled rules.However, data set balanced distribution largest AUC performs best overall.Note curve generated using balanced class distribution almost always outperformscurve associated natural distribution (for low false-positive rates natural distribution performs slightly better).7. Forming Good Class Distribution Sensitivity Procurement Costsresults previous section demonstrate marginal class distributions yieldclassifiers perform substantially better classifiers produced training distributions. Unfortunately, order determine best class distribution training, formingthirteen training sets size n, different class distribution, requires nearly 2n examples. costly obtain training examples form suitable learning, approach self-defeating. Ideally, given budget allows n training examples, one wouldselect total n training examples would used final training setandassociated class distribution would yield classifiers perform better generatedclass distribution (given n training examples). section describe evaluateheuristic, budget-sensitive, progressive sampling algorithm approximates ideal.order evaluate progressive sampling algorithm, necessary measure classdistribution affects classifier performance variety different training-set sizes.measurements summarized Section 7.1 (the detailed results included Appendix B).algorithm selecting training data described Section 7.2 performanceevaluated Section 7.3, using measurements included Appendix B.7.1Effect Class Distribution Training-Set Size Classifier PerformanceExperiments run establish relationship class distribution, training-set sizeclassifier performance. order ensure training sets contain sufficient numbertraining examples provide meaningful results training-set size dramatically reduced, data sets yield relatively large training sets used (this determined basedsize data set fraction minority-class examples data set). Basedcriterion, following seven data sets selected analysis: phone, adult, covertype,blackjack, kr-vs-kp, letter-a, weather. detailed results associated experimentscontained Appendix B.results one data sets, adult data set, shown graphically Figure 4Figure 5, show classifier performance using error rate AUC, respectively.nine performance curves figures associated specific training-set size,contains 1/128 training data available learning (using methodologydescribed Section 4.1). performance curves always improve increasing dataset size, curves corresponding smallest largest training-set sizes explicitlylabeled.332fiLearning Training Data Costly: Effect Class Distribution Tree Induction1/1281/641/321/161/81/41/23/41Error Rate3530Natural1/12825201 (all available training data)1501020304050607080% Minority Examples Training SetFigure 4: Effect Class Distribution Training-set Size Error Rate (Adult Data Set)0.91 (all available training data)AUC0.80.71/1280.6Natural0.50102030405060708090100% Minority Class Examples Training SetFigure 5: Effect Class Distribution Training-set Size AUC (Adult Data Set)333fiWeiss & ProvostFigure 4 Figure 5 show several important things. First, change training-set sizeshifts performance curves, relative rank point performance curve remainsroughly same. Thus, class distribution yields best performance occasionally varies training-set size, variations relatively rare occur,small. example, Figure 5 (and supporting details Appendix B) indicatesadult data set, class distribution yields best AUC typically contains 80% minorityclass examples, although occasionally small deviation (with 1/8 training data 70%minority-class examples best). gives support notion may bestmarginal class distribution learning task suggests progressive sampling algorithmmay useful locating class distribution yields best, nearly best, classifier performance.results also indicate that, fixed class distribution, increasing size training set always leads improved classifier performance. Also note performance curvestend flatten size data set grows, indicating choice class distribution may become less important training-set size grows. Nonetheless, evenavailable training data used, choice class distribution make difference.significant plateau reached (i.e., learning stopped), wouldpossible reduce size training set without degrading classifier performance.case would necessary select class distribution training data carefully.results Figure 4 Figure 5 also show carefully selecting class distribution, one sometimes achieve improved performance using fewer training examples.see this, consider dashed horizontal line Figure 4, intersects curve associatedtraining data lowest error rate, class distribution includes 10% minority-class examples. horizontal line curve associated availabletraining data, training set data outperforms full training set.case see training data 10% class distribution outperforms natural classdistribution using available training data. two horizontal lines Figure 5 highlightcases one achieve improved AUC using fewer training data (becauselarger AUC values indicate improved performance, compare horizontal lines curveslie them). example, Figure 5 shows training set class distributioncontains 80% minority-class examples 1/128th total training data outperformstraining set twice training data class distribution contains less equal40% minority-class examples (and outperforms training set four times data classdistribution contains less equal 10% minority-class examples). results Appendix B show trends noted adult data set hold data setsone often achieve improved performance using less training data.7.2Budget-Sensitive Progressive sampling Algorithm Selecting Training Datadiscussed above, size training set sometimes must limited due costs associatedprocuring usable training examples. simplicity, assume budget n,permits one procure exactly n training examples. assume number trainingexamples potentially procured sufficiently large training set size nformed desired marginal class distribution. would like sampling strategyselects x minority-class examples majority-class examples, x + = n,334fiLearning Training Data Costly: Effect Class Distribution Tree Inductionresulting class distribution yields best possible classification performance training setsize n.sampling strategy relies several assumptions. First, assume cost executing learning algorithm negligible compared cost procuring examples,learning algorithm may run multiple times. certainly true training datacostly. assume cost procuring examples classhence budget n represents number examples procured well totalcost. assumption hold many, all, domains. example, phone dataset described Section 1 cost procuring business consumer examples equal,telephone fraud domain cost procuring fraudulent examples may substantially higher cost procuring non-fraudulent examples. algorithm describedsection extended handle non-uniform procurement costs.sampling algorithm selects minority-class majority-class training examplesresulting class distribution yield classifiers tend perform well. algorithmbegins small amount training data progressively adds training examples usinggeometric sampling schedule (Provost, Jensen & Oates, 1999). proportion minority-classexamples majority-class examples added iteration algorithm determinedempirically forming several class distributions currently available training data,evaluating classification performance resulting classifiers, determiningclass distribution performs best. algorithm implements beam-search spacepossible class distributions, beam narrows budget exhausted.say sampling algorithm budget-efficient examples selected iteration algorithm used final training set, heuristically determinedclass distribution. key constrain search space class distributionsbudget-efficiency either guaranteed, likely. show, algorithm described section guaranteed budget-efficient. Note, however, class distribution final training set, heuristically determined, guaranteed bestclass distribution; however, show, performs well practice.algorithm outlined Table 7, using pseudo-code, followed line-by-line explanation (a complete example provided Appendix C). algorithm takes three user-specifiedLQSXWSDUDPHWHUV WKHJHRPHWULFIDFWRUXVHGWRGHWHUPLQHWKHUDWHDWZKLFKWKHWUDLQLQJ-set sizegrows; n, budget; cmin, minimum fraction minority-class examples majorityclass examples assumed appear final training set order budgetefficiency guarantee hold.5)RUWKHUHVXOWVSUHVHQWHGLQWKLVVHFWLRQ LVVHWWRVRWKDWWKHtraining-set size doubles every iteration algorithm, cmin set 1/32.algorithm begins initializing values minority majority variables,represent total number minority-class examples majority-class examples requestedalgorithm. Then, line 2, number iterations algorithm determined,initial training-set size, subsequently set line 5, cmin n.allow possible class distributions formed using cmin minority-class examples cmin majority-FODVVH[DPSOHV)RUH[DPSOHJLYHQWKDW LVDQGcmin 1/32, line 2variable K set 5 line 5 initial training-set size set 1/32 n.5. Consider degenerate case algorithm determines best class distribution contains minorityclass examples majority-class examples. algorithm begins even single example class,budget-efficient.335fiWeiss & Provost1. minority = majority = 0;2.K = log1c min# current number minority/majority examples hand;# number iterations K+13. (j = 0; j .M Mfi4. {5.size = nff fiK-j6.7.8.9.10.11.12.# iteration (e.g., j = 0, 1,2,3,4,5)# set training-set size iteration j(j = 0)beam_bottom = 0; beam_top = 1;# initialize beam first iterationelse (j = K)beam_bottom = best; beam_top = best; # last iteration evaluate previous bestelsemin(best,1 best )beam_radius =+1 1beam_bottom = best beam_radius; beam_top = best + beam_radius;13.14.min_needed = size beam_top;# number minority examples neededmaj_needed = size (1.0 beam_bottom); # number majority examples needed15.16.17.18.(min_needed > minority)request (min_needed - minority) additional minority-class examples;(maj_needed > majority)request (maj_needed - majority) additional majority-class examples;19.20.evaluate(beam_bottom, beam_top, size); # evaluate distributions beam; set best}Table 7: Algorithm Selecting Training DataNext, lines 6-12, algorithm determines class distributions considerediteration setting boundaries beam. first iteration, class distributionsconsidered (i.e., fraction minority-class examples training set may vary 01) last iteration, best-performing class distribution previous iteration evaluated. iterations, beam centered class distributionperformed best previous iteration radius beam set (in line 11)ratio beam_top/beam_bottom wiOOHTXDO )RUH[DPSOHLI LVDQGbest .15,beam_radius .05 beam span .10 .20which difIHUE\DIDFWRURIffLH filines 13 14 algorithm computes number minority-class examples majority-class examples needed form class distributions fall within beam. valuesdetermined class distributions boundaries beam. lines 15-18 additional examples requested, required. line 19 evaluation procedure called form336fiLearning Training Data Costly: Effect Class Distribution Tree Inductionclass distributions within beam induce evaluate classifiers.minimum procedure evaluate class distributions endpoints midpointbeam; however, procedure may implemented evaluate additional class distributions within beam. procedure set variable best class distribution performs best. best performance achieved several class distributions, resolutionprocedure needed. example, class distribution surrounding class distributions perform best may chosen; still yield unique value, bestperforming class distribution closest center beam may chosen. event,last iteration, one class distribution evaluatedthe previous best. ensure budgetefficiency, one class distribution evaluated final iteration.algorithm guaranteed request examples subsequently used finaltraining set, heuristically determined class distribution. guaranteeverified inductively. First, base case. calculation K line 2 ensures initial training set contain cmin n training examples. Since assume final trainingset least cmin minority-class examples cmin majority-class examples, examples used form initial training set guaranteed included final training set.Note cmin may set arbitrarily smallthe smaller cmin larger K smallersize initial training set.inductive step based observation radius beam line 11sHWVRWKDWWKHEHDPVSDQVDWPRVWDIDFWRURI DOOH[DPSOHVUHTXHVWHGLQHDFKLWHUDWLRQDUHJXDranteed used final training set. see this, work backward finaliteration, rather working forward case inductive proofs. Assumeresult algorithm fraction minority-class examples final training set p,p n minority-class examples final training set. means pbest distribution previous iteration. Since p must fall somewhere within beamprevious iteration beam must span factor ZHFDQVD\WKHIROORZLQJWKHIUDFWLRQminority-class examples previous iteration could range p ffLIp topWKH SUHYLRXV EHDPfi WR p (if p bottom previous beam). Since previousiteration contains n/ H[DPSOHVGXHWRWKHJHRPHWULFVDPSOLQJVFKHPHWKHQWKHSUHYLRXVLWHUaWLRQKDVDWPRVWff p) n RUp n, minority-class examples. Thus, possible casesminority-class examples previous iteration used final interaction.argument applies similarly majority-class examples extended backwardsprevious iterations.6 Thus, bound initial training-set size restrictionwidth beam exceed geometULFIDFWRU WKHDOJRULWKPJXDUDQWHHVWKDWDOOexamples requested execution algorithm used final training set.complete, detailed, iteration-by-iteration example describing sampling algorithmapplied phone data set provided Appendix C, Table C1. example error rateused evaluate classifier performance. description specifies class distributionsevaluated execution algorithm. trajectory graphically depictedFigure 6, narrowing final class distribution. iteration, algorithm considersbeam class distributions bounded two curves.6. exception first iteration algorithm, since situation beam unconditionally setspan class distributions. reason cmin value required provide efficiency guarantee.337fiWeiss & Provost% Minority Class Examples1008060beam_top4010% (best)beam_bottom200012345IterationFigure 6: Trajectory Algorithm Space Class Distributions.7.3 Results Sampling Algorithmbudget-sensitive progressive sampling algorithm applied phone, adult, covertype,kr-vs-kp, weather, letter-a blackjack data sets using error rate AUC measure classifier performance. However, method setting beam (described lines 6-12 Table 7)modified results experiments described Section 7.1 (and detailedAppendix B), evaluate 13 listed class distributions, could used. Specifically,iteration low end (high end) beam set class distribution specifiedAppendix B (above) best performing class distribution prior iteration. example, one iteration best performing class distribution contains 30% minority-class examples, next iteration bottom beam set include 20%minority-class examples top beam include 40% minority-class examples (of13 sampled class distributions, closest 30% class distribution). AlthoughsomeWLPHVDOORZWKHEHDPWRVSDQDUDQJHJUHDWHUWKDQ fffiLQSUDFWLFHWKLVGRHVQRWUHVXOWproblemfor seven data sets examples requested algorithm includedfinal training set. addition, slight improvement made algorithm. Specifically,iteration, number examples already hand (procured previous iterations) sufficient evaluate additional class distributions, beam widened include additional class distributions (this happen first iteration beam setwide).performance sampling algorithm summarized Table 8, along performance two strategies selecting class distribution training data. firsttwo additional strategies, Pick Natural/Balanced Strategy, based guidelinessuggested empirical results Section 6. strategy selects natural distributionerror rate performance metric balanced class distribution AUC338fiLearning Training Data Costly: Effect Class Distribution Tree Inductionperformance metric. Pick Best strategy selects class distribution performs best13 evaluated class distributions (see Tables 5 6). Given consider13 class distributions, strategy always yield best results but, shall see,costly strategies. value representing best, budget-efficient performance(lowest error rate, highest AUC) underlined data set. detailed iteration-by-iterationdescription algorithm, seven data sets, provided Appendix C, Table C3.Table 8 also specifies cost strategy, based number training examples requested algorithm. cost expressed respect budget n (each strategy yieldsfinal training set n examples). Pick Natural/Balanced strategy always requires exactly n examples selected therefore cost n budget-efficient. PickBest strategy total cost 1.93n hence budget-efficient (because evaluatesclass distributions 2% 95% minority-class examples, requires .95n minorityclass examples .98n majority-class examples). cost sampling algorithm dependsperformance induced classifiers: changes algorithm describedsection, longer guaranteed budget-efficient. Nonetheless, casesforerror rate AUCthe sampling algorithm cost exactly n hence turnsbudget-efficient.Data Setphoneadultcovertypekr-vs-kpweatherletter-ablackjackSampling Algorithm Pick Natural/BalancedERAUC CostERAUC Cost12.3%17.1%5.0%1.2%33.1%2.8%28.4%.851.861.984.998.740.954.715nnnnnnn12.6%17.3%5.0%1.4%33.7%2.8%28.4%.849.851.980.998.736.937.713nnnnnnnERPick BestAUC Cost12.3%16.9%5.0%1.2%33.1%2.6%28.4%.853.861.984.998.740.954.7151.93n1.93n1.93n1.93n1.93n1.93n1.93nTable 8: Comparative Performance Sampling Algorithmresults Table 8 show using budget-sensitive progressive sampling algorithm choose training data possible achieve results good betterstrategy always using natural distribution error rate balanced distributionAUCwithout requiring extra examples procured. particular, comparing two strategies, progressive sampling strategy win-tie-loss record 10-4-0.cases wins lead large improvements performance, cases(e.g., kr-vs-kp data set sampling strategy yields relative reduction errorrate 17%). results Table 8 also show sampling algorithm performs nearlywell Pick Best strategy (it performs well 11 14 cases), almost twicecostly. progressive sampling strategy performs nearly well Pick Beststrategy, conclude progressive sampling strategy substantially outperform Pick Natural/Balanced strategy, sampling strategy cannot identify good (i.e., near-optimal) class distribution learning, rather optimal classdistribution happens near natural (balanced) distribution error rate (AUC). Notedata sets (optdigits, contraceptive, solar-flare, car)case hence Pick Natural/Balanced strategy perform poorly. Unfortunately, data sets would yield relatively small training sets, progressive sampling algorithm could run them.339fiWeiss & Provostsummary, sampling algorithm introduced section leads near-optimal resultsresults outperform straw-man strategy using natural distribution minimize errorrate balanced distribution maximize AUC. Based results, budgetsensitive progressive sampling algorithm attractiveit incurs minimum possible costterms procuring examples permitting class distribution training selectedusing intelligence.8. Related WorkSeveral researchers considered question class distribution use fixedtraining-set size, and/or, generally, class distribution affects classifier performance.Catlett (1991) Chan & Stolfo (1998) study relationship (marginal) trainingclass distribution classifier performance training-set size held fixed, focusattention issues. studies also analyze data sets,makes impossible draw general conclusions relationship class distributionclassifier performance. Nonetheless, based results three data sets, Chan & Stolfo(1998) show accuracy performance metric, training set uses naturalclass distribution yields best results. results agree partially resultsalthoughshow natural distribution always maximize accuracy, show optimal distribution generally close natural distribution. Chan & Stolfo also showactual costs factored (i.e., cost false positive false negative), natural distribution perform best; rather training distribution closer balanced distribution performs best. also observe, did, increasing percentageminority-class examples training set, induced classifier performs better classifyingminority examples. important note, however, neither Chan & Stolfo Catlett adjusted induced classifiers compensate changes made class distributiontraining set. means results biased favor natural distribution (whenmeasuring classification accuracy) could improve classification performanceminority class examples simply changing (implicitly) decision threshold. resultsAppendix show, compensating changed class distribution affect performanceclassifier significantly.Several researchers looked general question reduce need labeledtraining data selecting data intelligently, without explicitly considering class distribution. example, Cohn et al. (1994) Lewis Catlett (1994) use active learningadd examples training set classifier least certain classification.Saar-Tsechansky Provost (2001, 2003) provide overview methods also extendcover AUC non-accuracy based performance metrics. settingmethods applicable different setting consider. particular, methodsassume either arbitrary examples labeled descriptions pool unlabeled examples available critical cost associated labeling (so algorithms select examples intelligently rather randomly). typical setting, costprocuring descriptions examplesthe labels known beforehand.also prior work progressive sampling strategies. John Langley(1996) show one use extrapolation learning curves determine classifierperformance using subset available training data comes close performance wouldachieved using full data set. Provost et al. (1999) suggest using geometric sampling340fiLearning Training Data Costly: Effect Class Distribution Tree Inductionschedule show often efficient using available training data.techniques described John Langley (1996) Provost et al. (1999) changedistribution examples training set, rather rely taking random samplesavailable training data. progressive sampling routine extends methods stratifyingsampling class, using information acquired process select goodfinal class distribution.considerable amount research build good classifiers classdistribution data highly unbalanced costly misclassify minority-class examples (Japkowicz et al., 2000). research related work frequent approachlearning highly skewed data sets modify class distribution training set.conditions, classifiers optimize accuracy especially inappropriate tend generate trivial models almost always predict majority class. common approach dealing highly unbalanced data sets reduce amount classimbalance training set. tends produce classifiers perform better minority class original distribution used. Note situation training-set sizefixed motivation changing distribution simply produce better classifiernot reduce, minimize, training-set size.two basic methods reducing class imbalance training data under-samplingover-sampling. Under-sampling eliminates examples majority class over-samplingreplicates examples minority class (Breiman, et al., 1984; Kubat & Matwin, 1997; Japkowicz & Stephen, 2001). Neither approach consistently outperforms specific under-sampling over-sampling rate consistently yield best results. EstabrooksJapkowicz (2001) address issue showing mixture-of-experts approach, combines classifiers built using under-sampling over-sampling methods various samplingrates, produce consistently good results.under-sampling over-sampling known drawbacks. Under-sampling throwspotentially useful data over-sampling increases size training set hencetime build classifier. Furthermore, since over-sampling methods make exact copiesminority class examples, overfitting likely occurclassification rules may inducedcover single replicated example.7 Recent research focused improving basic methods. Kubat Matwin (1997) employ under-sampling strategy intelligently removesmajority examples removing majority examples redundant border minority examplesfiguring may result noise. Chawla et al. (2000) combine under-sampling over-sampling methods, and, avoid overfitting problem, form newminority class examples interpolating minority-class examples lie close together.Chan Stolfo (1998) take somewhat different, innovative, approach. first run preliminary experiments determine best class distribution learning generate multiple training sets class distribution. typically accomplished includingminority-class examples majority-class examples training set.apply learning algorithm training set combine generated classifiers formcomposite learner. method ensures available training data used, sincemajority-class example found least one training sets.7. especially true methods C4.5, stops splitting based counting examples leavestree.341fiWeiss & Provostresearch article could properly viewed research under-samplingeffect classifier performance. However, given perspective, research performs undersampling order reduce training-set size, whereas research relating skewed datasets primary motivation improve classifier performance. example, Kubat Matwin (1997) motivate use under-sampling handle skewed data sets saying addingexamples majority class training set detrimental effect learnersbehavior: noisy otherwise unreliable examples majority class overwhelmminority class (p. 179). consequence different motivations experimentsunder-sample minority and/or majority classes, research concernedlearning skewed distributions majority class under-sampled.use under-sampling reducing training-set size (and thereby reducing cost) maypractically useful perspective. Reducing class imbalance training set effectively causes learner impose greater cost misclassifying minority-class examples(Breiman et al., 1984). Thus, cost acquiring learning dataissue, cost-sensitive probabilistic learning methods direct arguably appropriate way dealing class imbalance, problems, notedearlier, associated under-sampling over-sampling. approachesshown outperform under-sampling over-sampling (Japkowicz & Stephen, 2002). quoteDrummond Holte (2000) data available used produce tree, thusthrowing away information, learning speed degraded due duplicate instances (p.239).9. Limitations Future ResearchOne limitation research described article results baseduse decision-tree learner, conclusions may hold class learners. However, reasons believe conclusions hold learners well.Namely, since role class distribution plays learningand reasons, discussedSection 5.2, classifier perform worse minority classare specificdecision-tree learners, one would expect learners behave similarly. One class learnersmay especially warrant attention, however, learners form disjunctive concepts. learners suffer way problem smalldisjuncts, results indicate partially responsible minority-class predictions higher error rate majority-class predictions.8 Thus, would informative extendstudy include classes learners, determine results indeed generalize.program inducing decision trees used throughout article, C4.5, considersclass distribution training data generating decision tree. differencesclass distribution training data test data accounted post-processingstep re-computing probability estimates leaves using estimates re-labeltree. induction program knowledge target (i.e., test) distributiontree-building process, different decision tree might constructed. However, researchindicates serious limitation. particular, Drummond Holte (2000) showedsplitting criteria completely insensitive class distribution8. However, many learners form disjunctive concepts something quite close. example, Van den Bosch et al.(1997) showed instance-based learners viewed forming disjunctive concepts.342fiLearning Training Data Costly: Effect Class Distribution Tree Inductionsplitting criteria perform well better methods factor class distribution.showed C4.5s splitting criterion relatively insensitive class distributionand therefore changes class distribution.employed C4.5 without pruning study pruning sensitive class distribution C4.5s pruning strategy take changes made class distributiontraining data account. justify choice showed C4.5 without pruning performscompetitively C4.5 pruning (Sections 6.2 6.3). Moreover, research (Bradford et al., 1998) indicates classifier performance generally improve pruningtakes class distribution costs account. Nevertheless would worthwhile seecost/distribution-sensitive pruning strategy would affect results. knowpublished pruning method attempts maximize AUC.article introduced budget-sensitive algorithm selecting training datacostly obtain usable training examples. would interesting consider casecostly procure examples belonging one class another.10. Conclusionarticle analyze, fixed training-set size, relationship class distribution training data classifier performance respect accuracy AUC. analysisuseful applications data procurement costly data procured independently class, costs associated learning training data sufficientrequire size training set reduced. results indicate accuracyperformance measure, best class distribution learning tends near natural classdistribution, AUC performance measure, best class distribution learningtends near balanced class distribution. general guidelinesguidelinesand particular data set different class distribution may lead substantialimprovements classifier performance. Nonetheless, additional information providedclass distribution must chosen without experimentation, results showaccuracy AUC maximization, natural distribution balanced distribution (respectively) reasonable default training distributions.possible interleave data procurement learning, show budget-sensitiveprogressive sampling strategy improve upon default strategy using natural distribution maximize accuracy balanced distribution maximize area ROCcurvein experiments budget-sensitive sampling strategy never worse. Furthermore,experiments sampling strategy performs nearly well strategy evaluatesmany different class distributions chooses best-performing one (which optimal termsclassification performance inefficient terms number examples required).results presented article also indicate many data sets class distributionyields best-performing classifiers remains relatively constant different training-setsizes, supporting notion often best marginal class distribution. resultsshow amount training data increases differences performancedifferent class distributions lessen (for error rate AUC), indicating databecomes available, choice marginal class distribution becomes less less importantespecially neighborhood optimal distribution.article also provides comprehensive understanding class distribution affects learning suggests answers fundamental questions, classifiers almost always perform worse classifying minority-class examples. method adjusting343fiWeiss & Provostclassifier compensate changes made class distribution training set describedadjustment shown substantially improve classifier accuracy (see Appendix A).consider particularly significant previous research effect class distribution learning employed this, other, adjustment (Catlett, 1991; Chan & Stolfo,1998; Japkowicz & Stephen, 2002).Practitioners often make changes class distribution training data, especiallyclasses highly unbalanced. changes seldom done principled mannerreasons changing distributionand consequencesare often fully understood.hope article helps researchers practitioners better understand relationship class distribution classifier performance permits learn effectivelyneed limit amount training data.Acknowledgmentswould like thank Haym Hirsh comments feedback provided throughoutresearch. would also like thank anonymous reviewers helpful comments, IBMFaculty Partnership Award.Appendix A: Impact Class Distribution Correction Classifier PerformanceTable A1 compares performance decision trees labeled using uncorrected frequencybased estimate (FB) labeled using corrected frequency-based estimate (CT-FB).Datasetletter-apendigitsabalonesick-euthyroidconnect-4optdigitscovertypesolar-flarephoneletter-vowelcontraceptiveadultsplice-junctionnetwork2yeastnetwork1cargermanbreast-wiscblackjackweatherbandsmarket1crxkr-vs-kpmoveAverageError RateFBCT-FB9.795.384.094.0230.4522.979.826.8530.2127.576.173.416.626.4636.2029.1217.8514.8118.8914.1640.7739.6522.6920.059.028.7430.8029.9634.0128.8031.9930.998.267.9238.3737.096.766.7433.0228.7134.6234.6132.6832.6825.7725.7720.8421.481.221.2228.2428.2421.8919.90% Rel.Improv.45.01.724.630.28.744.72.419.617.025.02.711.63.12.715.33.14.13.30.313.10.00.00.0-3.10.00.010.6% Labels % Errors Min.ChangedFBCT-FB39.02.77.23.25.67.85.68.519.16.78.814.614.78.510.442.56.021.22.47.08.520.419.330.73.225.244.444.115.930.211.120.627.630.719.636.814.120.128.41.232.940.14.629.447.01.332.938.25.325.933.816.130.835.80.438.538.717.142.976.20.040.540.50.690.290.223.946.048.617.246.251.40.258.558.520.852.660.713.328.336.4Table A1: Impact Probability Estimates Error Rate344fiLearning Training Data Costly: Effect Class Distribution Tree Inductionresults main body article based use corrected frequencybased estimate label leaves induced decision trees, decision treesimproperly biased changes made class distribution training set. Thus,comparison Table A1 evaluates significance correcting changes class distribution training data. comparison based situation class distributiontraining set altered contain equal number minority-class majority-class examples (the test set still contain natural class distribution). results based 30runs data sets listed order decreasing class imbalance.error rate estimates displayed second third columns Table A1, and,data set, lowest error rate underlined. fourth column specifies relativeimprovement results using corrected frequency-based estimate. fifth columnspecifies percentage leaves decision tree assigned different class labelcorrected estimate used. last two columns specify, estimate, percentage total errors contributed minority-class test examples.Table A1 shows employing corrected frequency-based estimate instead uncorrected frequency-based estimate, is, average, relative 10.6% reduction error rate.Furthermore, one case uncorrected frequency-based estimate outperformcorrected frequency-based estimate. correction tends yield larger reductionhighly unbalanced data setsin cases plays larger role. restrictfirst 13 data sets listed Table 2, minority class makes less 25%examples, relative improvement data sets 18.2%. Notescenario minority class over-sampled training set, corrected frequency-basedestimate cause minority-labeled leaves labeled majority-class. Consequently, last column table demonstrates, corrected version estimatecause errors come minority-class test examples.Appendix B: Effect Training-Set Size Class Distribution LearningExperiments run establish joint impact class distribution training-set sizeclassifier performance. Classifier performance reported thirteen classdistributions analyzed Section 6 nine different training set sizes. ninetraining set sizes generated omitting portion available training data (recall that,described Section 4.1, amount available training data equals number minority-class examples). experiments training set sizes varied containfollowing fractions total available training data: 1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 3/41. order ensure training sets contain sufficient number training examplesprovide meaningful results, original data set must relatively large and/or contain highproportion minority-class examples. reason, following seven data setsselected analysis: phone, adult, covertype, kr-vs-kp, weather, letter-a blackjack.last four data sets list yield smaller number training examples first three,data sets two smallest training-set sizes (1/128 1/64) evaluated.experimental results summarized Tables B1a B1b. asterisk used denotenatural class distribution data set and, training-set size, class distributionyields best performance displayed bold underlined.345fiWeiss & ProvostData SetPHONE|||||||||||||||||Size Metric251018.2*20304050601/128.641 .737 .784 .793 .792 .791 .791 .789 .7881/64.707 .777 .784 .803 .803 .803 .801 .802 .8011/32.762 .794 .809 .812 .812 .811 .811 .811 .8101/16.784 .813 .816 .823 .823 .824 .818 .821 .8221/8AUC .801 .823 .828 .830 .830 .830 .830 .829 .8301/4.819 .835 .837 .839 .839 .837 .837 .836 .8361/2.832 .843 .846 .846 .845 .845 .843 .843 .8433/4.838 .847 .849 .849 .849 .848 .846 .847 .8461.843 .850 .852 .851 .851 .850 .850 .849 .8481/12817.47 16.42 15.71 16.10 16.25 17.52 18.81 21.21 22.871/6417.01 15.75 15.21 15.12 15.20 16.39 17.59 19.60 22.111/3216.22 15.02 14.52 14.50 14.75 15.41 16.81 18.12 20.021/16 Error 15.78 14.59 14.01 14.02 14.18 14.70 16.09 17.50 18.681/8Rate 15.17 14.08 13.46 13.61 13.71 14.27 15.30 16.51 17.661/414.44 13.55 13.12 13.23 13.27 13.85 14.78 15.85 17.091/213.84 13.18 12.81 12.83 12.95 13.47 14.38 15.30 16.433/413.75 13.03 12.60 12.70 12.74 13.35 14.12 15.01 16.17113.45 12.87 12.32 12.62 12.68 13.25 13.94 14.81 15.97ADULT|||||||||||||||||1/1281/641/321/161/81/41/23/411/1281/641/321/161/81/41/23/41COVERTYPE|||||||||||||||||1/1281/641/321/161/81/41/23/411/1281/641/321/161/81/41/23/41AUCErrorRateAUCErrorRate2.571.621.638.690.735.774.795.811.81623.8023.3222.9522.6621.6520.5619.5118.8218.475.586.630.674.721.753.779.803.814.82123.6422.6822.0921.3420.1519.0818.1017.7017.2610.633.657.711.733.768.793.812.823.82923.1022.2121.1220.2919.1318.2017.5417.1716.8520.674.702.735.760.785.804.822.830.83623.4421.7720.7719.9018.8718.4217.5417.3217.0923.9*.680.714.742.762.787.809.825.833.83923.6821.8020.9720.0719.3018.7017.8517.4617.2570.786.798.812.821.829.836.843.847.84826.4024.8021.7720.7019.6618.9417.8817.3317.3280.785.799.811.822.831.838.844.848.85030.4327.3424.8622.4621.2620.4319.5718.8218.7390.774.788.805.817.828.836.846.851.85333.2630.2125.3124.1523.2322.2821.6820.4320.2495.731.744.778.805.818.832.844.848.85037.2726.8628.7424.5223.3322.9021.6821.2421.0730.694.711.751.778.793.813.829.837.84223.9023.0821.1120.3719.6719.1218.3918.0717.7840.701.722.755.787.799.820.834.843.84625.2224.3822.3721.4320.8620.1019.3818.9618.8550.704.732.766.791.809.827.838.845.85126.9426.2924.4123.1822.3321.3920.8320.4020.0560.723.739.762.794.812.831.841.849.85429.5028.0727.0825.2724.5623.4822.8122.1321.7970.727.746.765.787.816.832.847.853.85833.0831.4530.2728.6727.1425.7824.8824.3224.0880.728.755.772.785.813.834.849.856.86137.8536.4134.0433.4131.0629.5428.1527.5927.1190.722.752.766.780.803.824.847.855.86146.1343.6442.4040.6538.3536.1734.7133.9233.0095.708.732.759.771.797.811.834.848.85548.3447.5247.2046.6845.8343.9341.2440.4739.75251014.8*2030.767 .852 .898 .909 .916 .913.836 .900 .924 .932 .937 .935.886 .925 .942 .947 .950 .947.920 .944 .953 .957 .959 .959.941 .955 .963 .965 .967 .968.953 .965 .970 .973 .975 .976.963 .972 .979 .981 .981 .980.968 .976 .982 .982 .983 .982.970 .980 .984 .984 .984 .98310.44 10.56 10.96 11.86 13.50 16.169.67 9.29 10.23 11.04 12.29 14.558.87 8.66 9.44 10.35 11.29 13.598.19 7.92 8.93 9.67 10.37 11.937.59 7.32 7.87 8.65 9.26 10.316.87 6.44 7.04 7.49 8.01 9.056.04 5.71 5.97 6.45 6.66 7.145.81 5.31 5.48 5.75 5.87 6.255.54 5.04 5.00 5.03 5.26 5.6440.916.936.948.959.969.975.978.980.98218.2616.5215.3413.5111.639.867.536.575.9550.916.932.948.957.968.973.977.979.98020.5018.5817.3015.3513.0610.568.036.896.4660.909.928.944.955.967.972.975.976.97823.4421.4019.3117.4214.6811.458.807.587.2370.901.922.939.951.963.970.972.975.97626.9524.7821.8219.4016.3912.289.948.728.5080.882.913.930.945.957.965.970.971.97331.3927.6524.8622.3018.2814.3611.4410.6910.1890.854.885.908.929.948.956.961.966.96837.9234.1228.3725.7422.5018.0514.8513.9213.0395.817.851.876.906.929.943.953.958.96044.5441.6733.9128.3626.8722.5918.3716.2916.27Table B1a: Effect Training-Set Size Class Distribution Classifier Performance346fiLearning Training Data Costly: Effect Class Distribution Tree InductionData SetKR-VS-KP|||||||||||||WEATHER|||||||||||||Size1/321/161/81/41/23/411/321/161/81/41/23/411/321/161/81/41/23/411/321/161/81/41/23/41LETTER-A|||||||||||||1/321/161/81/41/23/411/321/161/81/41/23/41BLACKJACK|||||||||||||1/321/161/81/41/23/411/321/161/81/41/23/41MetricAUCErrorRateAUCErrorRateAUCErrorRateAUCErrorRate2.567.618.647.768.886.922.93742.6137.9935.1626.3317.1113.3712.185.637.681.809.888.946.966.97036.3533.0222.7315.7411.077.496.501020304047.8*5060708090.680 .742 .803 .852 .894 .894 .897 .854 .797 .695.800 .888 .920 .942 .951 .952 .951 .945 .929 .839.893 .947 .960 .976 .976 .976 .975 .974 .967 .936.938 .980 .984 .987 .989 .989 .989 .985 .982 .973.981 .992 .994 .995 .995 .995 .995 .994 .990 .982.987 .994 .995 .996 .995 .996 .996 .995 .994 .986.991 .994 .997 .998 .997 .998 .998 .997 .994 .98833.49 27.44 21.92 17.82 14.08 14.06 17.17 21.18 26.31 33.1022.76 15.49 12.66 10.46 10.14 9.74 10.08 11.53 13.97 22.1415.30 10.51 8.66 7.10 6.45 6.63 6.91 7.44 9.24 13.2111.26 6.16 5.46 4.59 4.24 4.32 4.23 5.27 5.97 8.546.00 3.71 2.72 2.38 2.05 2.11 2.32 2.66 4.16 5.614.10 2.75 2.12 1.60 1.64 1.55 1.55 1.93 2.88 5.053.20 2.33 1.73 1.16 1.39 1.22 1.34 1.53 2.55 3.6695.637.724.807.947.974.980.98238.8230.9523.9712.458.667.036.042.535.535.535.563.578.582.69440.7639.5639.2739.0038.6238.5638.415.535.533.565.606.626.657.71540.7639.5638.7038.1137.6637.2336.8910.535.562.591.627.682.698.72840.7638.8537.9536.7235.8935.3835.2595.529.540.555.600.629.642.70253.7753.5553.4653.3253.1952.5351.692.532.552.603.637.677.702.7117.865.194.604.383.633.222.863.9*.532.601.622.654.724.745.7727.866.044.584.363.493.082.785.532.601.642.692.734.776.7997.866.044.844.773.473.072.752.545.556.579.584.587.593.59334.2634.0932.8331.8431.1130.8030.745.575.589.592.594.596.596.59633.4832.9631.9030.7830.7030.6830.6610.593.603.604.612.621.622.62832.4331.2730.7030.6030.3029.9329.8120.557.588.617.680.690.700.73741.0638.8736.4535.4035.3235.2333.6830.559.593.632.678.705.715.73841.5539.9637.0135.8434.3934.1433.1140.571.595.651.670.712.720.74041.9139.8137.6836.9835.6234.2533.4340.1*.570.595.651.670.712.720.73641.9139.8137.6836.9835.6235.2133.6950.570.600.642.671.707.713.73641.9141.1939.2937.7936.4736.0834.6160.563.617.619.672.700.711.73045.4041.0841.4938.3737.6237.3536.6970.536.603.617.675.690.699.73649.7343.2542.8440.4740.0739.9138.3680.556.597.615.644.679.700.72248.5946.4246.3245.4744.1143.5541.6890.529.562.583.615.664.661.71852.7752.7351.3450.6849.8049.4647.23102030405060708090.558 .637 .699 .724 .775 .765 .769 .745 .747.639 .704 .726 .798 .804 .828 .833 .830 .799.645 .758 .798 .826 .841 .860 .861 .871 .854.743 .793 .845 .865 .878 .893 .899 .904 .900.790 .868 .893 .912 .916 .921 .926 .933 .927.841 .890 .908 .917 .930 .935 .941 .948 .939.865 .891 .911 .938 .937 .944 .951 .954 .9528.81 11.11 12.58 12.31 15.72 19.66 22.55 32.06 42.387.38 8.05 9.23 10.48 14.44 16.40 20.84 27.38 40.645.22 6.76 8.19 10.03 12.32 13.67 16.74 24.00 35.445.25 6.12 6.87 7.90 9.66 12.21 14.33 18.69 30.223.97 4.27 5.32 6.08 7.03 9.02 10.33 15.65 22.763.05 3.60 4.04 5.23 5.99 7.31 9.86 12.93 20.602.59 3.03 3.79 4.53 5.38 6.48 8.51 12.37 18.1020.607.613.639.652.675.675.67832.3030.4129.6329.6128.9628.7328.6730.620.629.651.672.688.688.68831.9730.5729.7129.2528.7328.5628.5635.6*.621.636.657.673.692.699.70032.4430.9130.0229.3428.6028.4428.4040.624.643.657.677.697.703.71232.8430.9730.3029.6429.0328.5028.4550.619.651.665.686.703.710.71333.4831.8230.6629.6229.3328.7728.7160.618.648.665.686.704.710.71534.8932.1231.3430.4029.3228.9928.9170.609.634.659.680.690.699.70036.0533.6132.0530.8630.1029.9529.7880.600.622.630.650.670.677.67838.0435.5532.4431.3331.3231.1731.0290.580.594.603.603.603.604.60438.3138.1935.1133.0232.8032.7532.6795.724.780.824.876.910.927.94048.5247.6145.0943.1235.9329.6226.1495.532.551.553.554.556.558.55843.6537.8637.7335.0934.4634.1833.87Table B1b: Effect Training-Set Size Class Distribution Classifier Performance347fiWeiss & Provostbenefit selecting class distribution training data demonstrated using several examples. Table B1a highlights six cases (by using line connect pairs data points)competitive improved performance achieved fewer training examples.six cases, data point corresponding smaller data-set size performs wellbetter data point corresponds larger data-set size (the latter eithernatural distribution balanced one).Appendix C: Detailed Results Budget-Sensitive Sampling Algorithmappendix describes execution progressive sampling algorithm describedTable 7. execution algorithm evaluated using detailed results AppendixB. First, Table C1, detailed iteration-by-iteration description sampling algorithmpresented applied phone data set using error rate measure classifier performance.Table C2 provides compact version description, reporting keyvariables change value iteration iteration. Finally, Table C3a Table C3b,compact description used describe execution sampling algorithmphone, adult, covertype, kr-vs-kp, weather blackjack data sets, using error rate AUCmeasure performance. Note tables, column labeled budget refersbudget used, cost incurredand case budget exceeded, meansexamples requested execution algorithm used final trainingset, heuristically-determined class distribution (i.e., algorithm budget-efficient).results described appendix, consistent results presented Section7, baVHGRQDJHRPHWULFIDFWRU RIDQGDYDOXHRIcmin 1/32. total budget available procuring training examples n. Based values, value K, determines number iterations algorithm computed line 2 Table 7, set 5.Note value n different data set and, given methodology alteringclass distribution specified Section 4.1, training set size Table 2 fractionminority-class examples f, n = Sf.description sampling algorithm, applied phone data seterror rate performance measure:j = 0 Training-set size = 1/32 n. Form 13 data sets, contain 2% 95%minority-class examples. requires .0297n (95% 1/32 n) minority-class examples.0306n (100%-2% = 98% 1/32 n) majority-class examples. Induceevaluate resulting classifiers. Based results Table 7, natural distribution, contains 18.2% minority-class examples, performs best. Total Budget:.0603n (.0297n minority, .0306n majority).j=1Training-set size = 1/16 n. Form data sets corresponding best-performing classdistribution form previous iteration (18.2% minority) adjoining class distributions used beam search, contain 10% 20% minority-class examples.requires .0250n (20% 1/16 n) minority-class examples .0563n (90% 1/16n) majority-class examples. Since .0297n minority-class examples previously obtained, class distributions containing 30% 40% minority-class examples also348fiLearning Training Data Costly: Effect Class Distribution Tree Inductionformed without requesting additional examples. iteration requires .0257n additionalmajority-class examples. best-performing distribution contains 10% minority-classexamples. Total Budget: .0860 n (.0297n minority, .0563n majority).j=2Training-set size = 1/8 n. Since 10% distribution performed best, beam searchevaluates 5%, 10%, 18.2% minority-class distributions. 20% class distribution also evaluated since requires .0250n .0297n previously obtainedminority-class examples. total .1188n (95% 1/8 n) majority-class examplesrequired. best performing distribution contains 10% minority-class examples.iteration requires .0625n additional majority-class examples. Total Budget: .1485n(.0297n minority, .1188n majority).j=3Training-set size = 1/4 n. distributions evaluated 5%, 10%, 18.2%.extra minority-class examples available evaluate additional class distributions. iteration requires .0455n (18.2% 1/4 n) minority-class examples.2375n (95% 1/4 n) majority-class examples. best-performing class distributioncontains 10% minority-class examples. Total Budget: .2830n (.0455n minority, .2375nmajority)j=4Training-set size = 1/2 n. 5%, 10%, 18.2% class distributions evaluated.iteration requires .0910n (18.2% 1/2 n) minority-class examples .4750n (95%1/2 n) majority-class examples. best-performing distribution contains 10% minority-class examples. Total Budget: .5660n (.0910n minority, .4750n majority).j=5Training-set size = n. last iteration best class distribution previous iteration evaluated. Thus, data set size n formed, containing .1n minorityclass examples .9n majority-class examples. Thus .0090n additional minority-classexamples .4250n additional majority-class examples required. Since previously obtained examples used, waste budget exceeded.Total Budget: 1.0n (.1000n minority, .9000n majority)Table C1: Detailed Example Sampling Algorithm (Phone Data Set using Error Rate)j sizeclass-distr0 1/32 n1 1/16 n 10, 18.2 , 20, 30, 402 1/8 n 5, 10 , 18.2, 203 1/4 n 5, 10 , 18.24 1/2 n 5, 10 , 18.21 n 105Expressed fraction nbest min-need maj-need minority majority budget18.2%.0297.0306.0297.0306 .060310%.0250.0563.0297.0563 .086010%.0250.1188.0297.1188 .148510%.0455.2375.0455.2375 .283010%.0910.4750.0910.4750 .5660.1000.9000.1000.9000 1.0000Table C2: Compact Description Results Table B1a Phone Data Set349fiWeiss & ProvostData setPhoneMetric jER012345PhoneAUC012345AdultER012345AdultAUC012345Covertype ER012345Covertype AUC012345Kr-vs-kp ER012345Kr-vs-kp AUC012345size1/32 n1/16 n1/8 n1/4 n1/2 n1n1/32 n1/16 n1/8 n1/4 n1/2 n1n1/32 n1/16 n1/8 n1/4 n1/2 n1n1/32 n1/16 n1/8 n1/4 n1/2 n1n1/32 n1/16 n1/8 n1/4 n1/2 n1n1/32 n1/16 n1/8 n1/4 n1/2 n1n1/32 n1/16 n1/8 n1/4 n1/2 n1n1/32 n1/16 n1/8 n1/4 n1/2 n1nclass-distr10, 18.2 , 20, 30, 405, 10 , 18.2, 205, 10 , 18.25, 10 , 18.21018.2, 20 , 30, 4020, 30 , 4020, 3 0, 4018.2, 20 , 3018.210, 20 , 23.9, 30, 4010, 20 , 23.910, 20 , 23.95, 10 , 202060, 70, 80 , 9060, 70 , 8060, 70 , 8070, 80 , 90802, 5 , 10, 20, 30, 402, 5 , 10, 202, 5 , 102, 5 , 10514.8, 20 , 30, 4020, 30 , 4030, 40 , 5020, 30 , 402047.8, 50, 6047.8, 50, 6040, 47.8 , 5040, 47.8 , 505050, 60, 7047.8, 50, 6047.8, 50, 6047.8, 50, 6050Expressed fraction nbest min-need maj-need minority majority budget18.2.0297.0306.0297.0306 .060310.0250.0563.0297.0563 .086010.0250.1188.0297.1188 .148510.0455.2375.0455.2375 .283010.0910.4750.0910.4750 .5660.1000.9000.1000.90001.020.0297.0306.0297.0306 .060330.0250.0511.0297.0511 .080830.0500.1000.0500.1000 .150020.1000.2000.1000.2000 .300018.2.1500.4090.1500.4090 .5590.1820.8180.1820.81801.020.0297.0306.0297.0306 .060320.0250.0563.0297.0563 .086020.0299.1125.0299.1125 .142410.0598.2250.0598.2250 .284820.1000.4750.1000.4750 .5750.2000.8000.2000.80001.080.0297.0306.0297.0306 .060370.0563.0250.0563.0306 .086970.1000.0500.1000.0500 .150080.2000.1000.2000.1000 .300080.4500.1500.4500.1500 .6000.8000.2000.8000.20001.05.0297.0306.0297.0306 .06035.0250.0613.0297.0613 .09105.0250.1225.0297.1225 .15225.0250.2450.0297.2450 .27475.0500.4900.0500.4900 .5400.0500.9500.0500.95001.020.0297.0306.0297.0306 .060330.0250.0533.0297.0533 .083040.0500.1000.0500.1000 .150030.1250.1750.1250.1750 .300020.2000.4000.2000.4000 .6000.2000.8000.2000.80001.050.0297.0306.0297.0306 .060350.0375.0327.0375.0327 .070247.8.0750.0653.0750.0653 .140347.8.1250.1500.1250.1500 .275050.2500.3000.2500.3000 .5500.5000.5000.5000.50001.060.0297.0306.0297.0306 .060350.0438.0313.0438.0313 .075150.0750.0653.0750.0653 .140350.1500.1305.1500.1305 .280550.3000.2610.3000.2610 .5610.5000.5000.5000.50001.0Table C3a: Summary Results Sampling Algorithm (phone, adult, covertype, kr-vs-kp)350fiLearning Training Data Costly: Effect Class Distribution Tree InductionData setWeatherMetric j sizeER0 1/32 n1 1/16 n2 1/8 n3 1/4 n4 1/2 n5 1nWeather AUC0 1/32 n1 1/16 n2 1/8 n3 1/4 n4 1/2 n5 1nLetter-aER0 1/32 n1 1/16 n2 1/8 n3 1/4 n4 1/2 n5 1nLetter-aAUC0 1/32 n1 1/16 n2 1/8 n3 1/4 n4 1/2 n5 1nBlackjack ER0 1/32 n1 1/16 n2 1/8 n3 1/4 n4 1/2 n5 1nBlackjack AUC0 1/32 n1 1/16 n2 1/8 n3 1/4 n4 1/2 n5 1nclass-distr2,5 ,10,20,30,40, 40.15, 10, 2010, 20 , 3010, 20 , 303030, 40 , 5040, 50 , 6030, 40, 5040, 50 , 60402, 3.9 , 5, 10, 20, 30, 402 , 3.9, 5, 10, 202, 3.9 , 5, 102, 3.9, 5540, 50 , 6050, 60 , 7060, 70 , 8070, 80 , 9020, 30, 35.6, 4010, 20, 3010, 20, 3020, 30, 35.635.620, 35.6, 40, 5040, 50, 6040, 50, 6040, 50, 6060Expressed fraction nbest min-need maj-need minority majority budget5.0297.0306.0297.0316 .061310.0250.0613.0297.0613 .091020.0250.1188.0297.1188 .148520.0750.2250.0750.2250 .300030.1500.4500.1500.4500 .6000.3000.7000.3000.70001.040.0297.0306.0297.0316 .061350.0313.0438.0313.0438 .075140.0750.0750.0750.0750 .150050.1250.1750.1250.1750 .300040.3000.3000.3000.3000 .6000.4000.6000.4000.60001.03.9.0297.0306.0297.0306 .06032.0250.0613.0297.0613 .09103.9.0250.1125.0297.1225 .15223.9.0250.2450.0297.2450 .27475.0250.4900.0250.4900 .5150.0500.9500.0500.95001.050.0297.0306.0297.0306 .060360.0375.0375.0375.0375 .075070.0875.0625.0875.0625 .150080.2000.1000.2000.1000 .300080.4500.1500.4500.1500 .6000.8000.2000.8000.20001.030.0297.0306.0297.0316 .061320.0250.0500.0297.0500 .079720.0375.1125.0375.1125 .150030.0750.2250.0750.2250 .300035.6.1780.4000.1780.4000 .5780.3560.6440.3560.64401.040.0297.0306.0297.0316 .061350.0313.0403.0313.0403 .071650.0750.0750.0750.0750 .150050.1500.1500.1500.1500 .300060.3000.3000.3000.3000 .6000.6000.4000.6000.40001.0Table C3b: Summary Results Sampling Algorithm (weather, letter-a, blackjack)ReferencesBauer, E., & Kohavi, R. (1999). empirical comparison voting classification algorithms:bagging, boosting, variants. Machine Learning, 36, 105-139.Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification Regression Trees. Belmont, CA: Wadsworth International Group.Blake, C., & Merz, C. (1998).UCI Repository Machine Learning Databases,(http://www.ics.uci.edu/~mlearn/MLRepository.html), Department Computer Science,University California.Bradley, A. (1997). use area ROC curve evaluation machine learning algorithms. Pattern Recognition, 30(7), 1145-1159.351fiWeiss & ProvostBradford, J.P., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. E. (1998). Pruning decision treesmisclassification costs. Proceedings European Conference MachineLearning, pp. 131-136.Catlett, J. (1991). Megainduction: machine learning large databases. Ph.D. thesis, Department Computer Science, University Sydney.Chan, P., & Stolfo, S. (1998). Toward scalable learning non-uniform class cost distributions: case study credit card fraud detection. Proceedings Fourth InternationalConference Knowledge Discovery Data Mining, pp. 164-168, Menlo Park, CA:AAAI Press.Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, W. P. (2000). SMOTE: synthetic minorityover-sampling technique. International Conference Knowledge Based Computer Systems.Cohen, W., & Singer, Y. (1999). simple, fast, effective rule learner. ProceedingsSixteenth National Conference Artificial Intelligence, pp. 335-342, Menlo Park, CA:AAAI Press.Cohn, D., Atlas, L. Ladner, R. (1994). Improved generalization active learning. Machine Learning, 15:201-221.Drummond, C., & Holte, R.C. (2000). Exploiting cost (in)sensitivity decision tree splittingcriteria. Proceedings Seventeenth International Conference Machine Learning,pp. 239-246.Elkan, C. (2001). foundations cost-sensitive learning. Proceedings SeventeenthInternational Joint Conference Artificial Intelligence, pp. 973-978.Estabrooks, A., & Japkowicz, N. (2001). Mixture-of-Experts Framework ConceptLearning Imbalanced Data Sets. Proceedings 2001 Intelligent Data AnalysisConference.Fawcett, T. & Provost, F. (1997). Adaptive Fraud Detection. Data Mining Knowledge Discovery 1(3): 291-316.Good, I. J. (1965). Estimation Probabilities. Cambridge, MA: M.I.T. Press.Hand, D. J. (1997). Construction Assessment Classification Rules. Chichester, UK: JohnWiley Sons.Holte, R. C., Acker, L. E., & Porter, B. W. (1989). Concept learning problem smalldisjuncts. Proceedings Eleventh International Joint Conference Artificial Intelligence, pp. 813-818. San Mateo, CA: Morgan Kaufmann.Japkowicz, N. & Stephen, S. (2002). Class Imbalance Problem: Systematic Study. Intelligent Data Analysis Journal, 6(5).Japkowicz, N., Holte, R. C., Ling, C. X., & Matwin S. (Eds.) (2000). Papers AAAIWorkshop Learning Imbalanced Data Sets. Tech, rep. WS-00-05, Menlo Park, CA:AAAI Press.352fiLearning Training Data Costly: Effect Class Distribution Tree InductionJensen, D. D., & Cohen, P. R. (2000). Multiple comparisons induction algorithms. MachineLearning, 38(3): 309-338.John, G. H., & Langley, P. (1996). Static versus dynamic sampling data mining. Proceedings Second International Conference Knowledge Discovery Data Mining, pp. 367-370. Menlo Park, CA. AAAI Press.Kubat, M., & Matwin, S. (1997). Addressing curse imbalanced training sets: one-sidedselection. Proceedings Fourteenth International Conference Machine Learning, pp. 179-186.Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling supervised learning.Proceedings Eleventh International Conference Machine Learning, pp.148-156.Provost, F., Fawcett, T., & Kohavi, R. (1998). case accuracy estimation comparing classifiers. Proceedings Fifteenth International Conference MachineLearning. San Francisco, CA: Morgan Kaufmann.Provost, F., Jensen, D., & Oates, T. (1999). Efficient progressive sampling. ProceedingsFifth International Conference Knowledge Discovery Data Mining. ACM Press.Provost, F., & Fawcett, (2001). Robust classification imprecise environments. MachineLearning, 42, 203-231.Provost, F., & Domingos, P. (2001). Well-trained PETs: improving probability estimation trees.CeDER Working Paper #IS-00-04, Stern School Business, New York University, NewYork, NY.Quinlan, J.R. (1993). C4.5: Programs Machine Learning. San Mateo, CA: Morgan Kaufmann.Saar-Tsechansky, M., & Provost, F. (2001). Active learning class probability estimationranking. Proceedings Seventeenth International Joint Conference Artificial Intelligence, Seattle, WA.Saar-Tsechansky, M. F. Provost (2003). Active Sampling Class Probability EstimationRanking. appear Machine Learning.SAS Institute (2001). Getting Started SAS Enterprise Miner. Cary, NC: SAS Institute Inc.Saerens, M., Latinne, P., & Decaestecker, C. (2002). Adjusting outputs classifier newpriori probabilities: simple procedure. Neural Computation, 14:21-41.Swets, J., Dawes, R., & Monahan, J. (2000). Better decisions science. Scientific American, October 2000: 82-87.Turney P. (2000). Types cost inductive learning. Workshop Cost-Sensitive LearningSeventeenth International Conference Machine Learning, 15-21, Stanford, CA.Van den Bosch A., Weijters, A., Van den Herik, H.J., & Daelemans, W. (1997). smalldisjuncts abound, try lazy learning: case study. Proceedings Seventh BelgianDutch Conference Machine Learning, 109-118.353fiWeiss & ProvostWeiss, G.M., & Hirsh, H. (2000). quantitative study small disjuncts, ProceedingsSeventeenth National Conference Artificial Intelligence, 665-670. Menlo Park, CA:AAAI Press.Weiss, G. M., & Provost, F (2001). effect class distribution classifier learning:empirical study. Tech rep. ML-TR-44, Department Computer Science, Rutgers University, New Jersey.Zadrozny, B., & Elkan, C. (2001). Learning making decisions costs probabilitiesunknown. Tech. rep. CS2001-0664, Department Computer Science Engineering, University California, San Diego.354fiJournal Artificial Intelligence Research 19 (2003) 513-567Submitted 1/03; published 11/03Decentralized Supply Chain Formation: Market ProtocolCompetitive Equilibrium AnalysisWilliam E. WalshWWALSH 1@ US . IBM . COMIBM T. J. Watson Research Center19 Skyline DriveHawthorne, NY 10532 USAMichael P. WellmanWELLMAN @ UMICH . EDUUniversity Michigan AI Laboratory1101 Beal AvenueAnn Arbor, MI 48109-2110 USAAbstractSupply chain formation process determining structure terms exchange relationships enable multilevel, multiagent production activity. present simple modelsupply chains, highlighting two characteristic features: hierarchical subtask decomposition,resource contention. decentralize formation process, introduce market price systemresources produced along chain. competitive equilibrium system, agentschoose locally optimal allocations respect prices, outcomes optimal overall. determine prices, define market protocol based distributed, progressive auctions, myopic,non-strategic agent bidding policies. presence resource contention, protocol producesbetter solutions greedy protocols common artificial intelligence multiagent systems literature. protocol often converges high-value supply chains, competitiveequilibria exist, typically approximate competitive equilibria. However, complementaritiesagent production technologies cause protocol wastefully allocate inputs agentsproduce outputs. subsequent decommitment phase recovers significant fractionlost surplus.1. IntroductionElectronic commerce technology provide significant improvements existing modes commercial interaction, increased speed, convenience, quality, reduced costs. Yetproposed radical visions business may transformed. Exponential increasescommunications bandwidth computational ability potential qualitatively decreasefriction business interactions. premise, Malone Laubauchers treatiseemerging E-Lance Economy (1998) puts forth view that, not-too-distant future, businessrelationships lose much current persistent character. Indeed, Malone Laubaucherpropose large companies know cease exist, rather dynamically formedelectronically connected freelancers (e-lancers) purpose producing particular goodsservices, dissolved projects completed. Others employ evocative termvirtual corporation (Davidow, 1992) describe groups agile organizations forming temporaryconfederations ad hoc purposes.c2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiWALSH & W ELLMANWhether one accepts full extent vision virtual corporations, several businesstrends provide evidence moving direction. Software companies time-shiftingdevelopment U.S. India, Sun Microsystems allows freelance programmers bid fix customers software problems (Borenstein & Saloner, 2001). Large, traditionalmanufacturing companies, exemplified major automotive manufacturers, increasingly outsourceproduction various components. Ford General Motors (GM) spun parts manufacturing separate companies (Lucking-Reily & Spulber, 2001). Start-ups smallcompanies form partnerships compete larger, established companies. Applicationservice providers supplant in-house provision standard operations, information, technologyservices.study phenomenon guise supply chains, common form coordinated commercial interaction. purposes, supply chain network production exchangerelationships spans multiple levels production task decomposition. Wheneverproducer buys inputs sells outputs, supply chain. Although typically used refermulti-business structures manufacturing industries, service contracting relationshipspans multiple levels viewed supply chain.Supply chain formation process determining participants supply chain,exchange whom, terms exchanges. Traditionally, supply chainsformed maintained long periods time means extensive human interactions.acceleration commercial decision making creating need advanced support.Companies ranging auto makers computer manufacturers basing business modelsrapid development, build-to-order, customized products satisfy ever-changing consumerdemand. fluctuations resource costs availability mean companies must respondrapidly maintain production capabilities profits. changes increasingly occurspeeds, scales, complexity unmanageable humans, need automated supply chainformation becomes acute.agents autonomous electronic commerce setting, must generally assume specialized knowledge capabilities limited knowledgeindividuals large-scale structure problem. agents self-interested,participate goal maximizing benefit. Additionally, maycause control allocation resource individually if, instance, global optimizationinfeasible one entity global allocative authority. environments information, decision making, control inherently decentralized, seek engineer processbottom-up supply chain formation. problem complicated structure resourcecontention precludes use simple greedy allocation strategies.present decentralized, asynchronous market protocol supply chain formationconditions resource scarcity. protocol allows agents negotiate formation supplychains bottom-up fashion, requiring local knowledge communication. marketprotocol, agents decisions coordinated price system, price resourcedetermined ascending auction.remainder paper describes market protocol, characterizes behavior theoretically empirically.1 begin Section 2 formal definition supply chainformation problem, illustrating application automotive industry. Section 3,1. details may found first authors dissertation (Walsh, 2001).514fiD ECENTRALIZED UPPLY C HAIN F ORMATIONshow typical greedy top-down approaches supply chain formation fail presenceresource contention. define price system analyze static properties price equilibriaSection 4. Section 5, introduce price-based market protocol supply chain formationanalyze convergence properties. present results empirical study protocolSection 6. Section 7, discuss relevant results issues price-based analysis auctiontheory, well related work supply chain formation. conclude Section 8suggest extensions future work. Throughout, defer proofs Appendix A.2. Supply Chain Formation ProblemAgents supply chain characterized terms capabilities perform tasks,interests tasks accomplished. central feature model problem hierarchicaltask decomposition: order perform particular task, agent may need achievesubtasks, may delegated agents. may turn subtasks maydelegated, forming supply chain decomposition task achievement. Constraintstask assignment arise resource contention, agents require common resource (e.g.,task achievement, something tangible piece equipment) accomplish tasks.Tasks performed behalf particular agents; two agents need task wouldperformed twice satisfy both. way, tasks discrete, rivalresource. Hence, make distinction model, use term good refer taskresource provided needed agents. assumption goods cannot shared reused(i.e., limited available quantities) necessary much analysis. Goodsreplicated little marginal cost, software information, provide many interestingchallenges economic analysis (Shapiro & Varian, 1999), addressed work.2.1 Example: Automotive Supply Chain Formationillustrate model supply chain formation application stylized, hypotheticalexample automotive industry. Traditionally, automotive supply chains span many tiers,formed maintained long periods time extensive human negotiations.automation emerging, example Covisint2 , company formed GM, Ford,DaimlerChrysler mediate negotiation exchange parts, well supply chaininteractions. Currently focus efforts particular exchange relationship withinsingle level production. consider broader problem assembling combinations relationships across multiple levels form complete, feasible supply chains.example presented Figure 1, Ford GM need acquire contracts transmissionsorder produce particular models cars. Ford produce transmissions factories acquire independent transmission producer. GM currentlycapacity produce desired transmissions, must outsource. independent transmissionproducer capacity provide transmissions either Ford GM, both. Fordindependent factory require services job shop metal-working tasks, job shopcapacity serve simultaneously. Contracts job shop twotransmission factories scarce goods allocated.2. http://www.covisint.com515fiWALSH & W ELLMANFord AutoAssemblyFordDistributionJob ShopIndependentTransmissionFactoryFordTransmissionFactoryGM AutoAssemblyGMDistributionFigure 1: automotive supply chain formation problem.limited capacity job shop entails certain constraints feasible supply chains. Fordcannot acquire transmissions independent factory, job shop cannot serveindependent factory Ford simultaneously. Additionally, Ford GM cannot simultaneouslysatisfied.2.2 Problem Specificationprovide formal description supply chain formation problem terms bipartite graphs.two types nodes represent goods agents, respectively. task dependency networkdirected, acyclic graph, (V, E), vertices V = G A, where:G = set goods,= C , set agents,C = set consumers,= set producers,set edges E connecting agents goods use produce. exists edgehg, ai g G agent make use one unit g, edge ha, giprovide one unit g. agent requires multiple units good input, treatunit separate edge, distinguishing subscripts. (Edges without explicit subscriptsinterpreted implicitly subscripted 1.) instance, agent requires two units ginput, input edges hg, ai1 hg, ai2 .various agent types characterized position task dependency network.consumer, c C, wishes acquire one unit one good set consumable goods,Gc G, hg, ci E iff g Gc .producer produce single unit output good conditional acquiring inputgoods. producer associate:1. input set, G, g iff edges hg, ik E one k,516fiD ECENTRALIZED UPPLY C HAIN F ORMATIONWorkedMetalFord TransmissionSubcontractorFordCarsFordDistribution$25,000FordTransmissions$20Job Shop$20Ford AutoAssembler$20,000FordTransmissionFactory$100IndependentTransmissionFactory$50GM TransmissionSubcontractor$60GMTrasmissionsGM AutoAssembler$25,000GMCarsGMDistribution$30,000Figure 2: Network auto: task dependency network automotive supply chain depictedFigure 1.2. single output, g G \ , h, g E.producers input goods complementary agent must acquire orderproduce output; cannot accomplish anything partial set. Alternate producersoutput indicate different ways good produced.Task dependency networks constrained acyclic, is, agent produces goodscould used assemble inputs chain production. Although might broadlyview global commerce one large cycle production consumption, practice, negotiations tend clustered within limited scopes concern, often referred industries.resulting supply chains typically acyclic.Figure 2 shows example task dependency network automotive supply chain problemFigure 1. goods indicated circles, agents boxes. Producers inputsrepresented curved boxes. numbers agent boxes represent production costsconsumption values, explained below. arrow agent good indicates agentprovide good, arrow good agent indicates agent make usegood. instance, producer labeled Ford Auto Assembly requires Worked Metal FordTransmissions order produce cars. Since transmissions produced Ford TransmissionFactory used Ford, need distinguish Ford GM transmissions separategoods. turn requires introduce Ford GM Transmission Subcontractor producersmodel fact Independent Transmission Factory used produce either type.allocation subgraph (V 0 , E 0 ) (V, E). g G, edge ha, gi E 0 meansagent provides g, hg, ai E 0 means acquires g. allocations vertices agentsgoods incident edges:1. agent allocation graph iff acquires provides good:A, V 0 iff hg, ai E 0 ha, gi E 0 .517fiWALSH & W ELLMANWorkedMetalFord TransmissionSubcontractorFordCarsFordDistribution$25,000FordTransmissions$20Job Shop$20Ford AutoAssembler$20,000FordTransmissionFactory$100IndependentTransmissionFactory$50GM TransmissionSubcontractor$60GMTrasmissionsGM AutoAssembler$25,000GMCarsGMDistribution$30,000Figure 3: solution Network auto.2. good allocation graph iff acquired provided:g G, g V 0 iff hg, ai E 0 ha, gi E 0 .producer active iff provides output. producer feasible iff inactiveacquires inputs. Consumers always feasible.Good g material balance (V 0 , E 0 ) iff number edges equals number out:fififi fifi{(a, k) | ha, gik E 0 }fi = fi{(a, k) | hg, aik E 0 }fi .allocation feasible iff agents feasible goods material balance.solution feasible allocation forms partial ordering feasible production, culminatingconsumption. is, consumer acquires good desires:exists hg, ci E 0 c C V 0 .solution may involve multiple consumers. consumer c solution (V 0 , E 0 ) say(V 0 , E 0 ) solution c.Figure 3 shows solution allocation task dependency network Figure 2. Shadedagents solid arrows part solution, unshaded agents dashed arrows indicatingelements problem part solution. Note Ford Auto Assembler wins input,inactive. However, recall inactive producers feasible, hence solution propertiesmet. refer configuration inactive producer acquiring input allocationdead end.producer production cost providing unit output. cost mightrepresent value could obtain engaging activity (i.e., opportunity cost),direct cost incurred producing output (but including input costs). Since producerprovides one unit one good, total production cost , output g, allocationE 0 , h, gi E 0 0 otherwise.assume consumer preferences different possible goods, wishes obtainsingle unit one good. Thus, consumer c obtains value vc (g) obtaining single unit518fiD ECENTRALIZED UPPLY C HAIN F ORMATIONgood g, and, allocation E 0 , obtains value vc ((V 0 , E 0 )) maxhg,ciE 0 vc (g). depicting taskdependency networks, display costs values corresponding agent boxes.Definition 1 (value allocation) value allocation (V 0 , E 0 ) is:value((V 0 , E 0 ))vc ((V 0, E 0)) ((V 0, E 0)).cCDefinition 2 (efficient allocations) set efficient allocations contains feasible allocations(V , E ) that:value((V , E )) =max(value((V 0 , E 0 )) | (V 0 , E 0 ) feasible).(V 0 ,E 0 )(V,E)Task dependency networks describe supply chain formation problem global perspective. decentralized approach formation, would generally assume agent,entity, perfect complete knowledge entire network. generally assumeagents perfect knowledge costs, values, goods interest. mediatorsfacilitate negotiations goods (as protocols described below), agent knows relevantmediators goods interest. knowledge includes rules enforced mediators.Likewise, mediators know existence agents interested respective goods. Beyondthat, mediator knows agents reveal communication negotiation.mediator know agents true costs valuations, aware agents preferencesgoods outside direct scope facilitation. address detail agentsmediators achieve mutual awareness (i.e., connections originate), assumeaccomplished via unspecified search, notification, broadcast protocol.3. Resource ContentionOne natural candidate approach supply chain formation CONTRACT NET protocol (Davis& Smith, 1983), widely studied algorithm forming task performance relations amongdistributed agents. C ONTRACT NET indeed apply framework, employs local negotiation achieve hierarchical task decomposition. Although definitive characterization difficultdue many variants CONTRACT NET literature (Baker, 1996; Davis & Smith, 1983;Dellarocas et al., 2000; Sandholm, 1993), fair say that, generally, request quotes proceed top root task (right-to-left consumers, network terminology),contracting proceeds bottom-up (left-to-right towards consumers), selecting level amongcandidate bids received. (Variants protocol primarily distinguished form bidsselection criteria employed.) consequence, choices made greedily, without reflectingramifications upstream evolving chain.approach form satisficing supply chains sufficient resources support greedy selection. However, basic CONTRACT NET protocol explicitly addressresource scarcity contention among multiple agents. Producers accept bids inputsestablished whether might cause infeasibility upstream. Without lookaheadbacktracking, CONTRACT NET might construct infeasible supply chains limited resources.instance, greedy protocol would produce solution network shown Figure 4.Here, producers bid according common function monotone cost, output bid519fiWALSH & W ELLMANa151a22a5051a331a4a706cons15a6041Figure 4: Network greedy-bad: network greedy protocols produce infeasibleallocations.producer a6 would preferred a5, a6 acquire inputs cheaper. sincea7 must acquire one available unit good 4 feasibly participate solution, a6 cannotpart solution.issue resource contention motivates adoption market-based approach. keyidea prices signal resource value scarcity chain, enabling localdecision making avoiding pitfalls greedy one-pass selection communication globalstructure information.4. Price Systemsprice system p assigns good g, nonnegative number p(g) price. Prices anonymous (i.e., agent dependent) linear quantity goods. Intuitively, prices indicaterelative value goods, agents use prices guide local decision making.assume agents quasilinear utility functions, defined money holdings plusvalue (or minus cost) associated allocation goods. Agents wish maximize surplusrespect prevailing prices.Definition 3 (surplus) surplus, (a, (V 0 , E 0 ), p), agent allocation (V 0 , E 0 ) pricesp, given by:va ((V 0 , E 0 )) hg,aiE 0 p(g), Cha,giE 0 p(g) hg,aiE 0 p(g) ((V 0 , E 0 )), .4.1 Price EquilibriumGenerally, allocation (V 0 , E 0 ) competitive equilibrium prices p (V 0 , E 0 ) feasibleassigns agent allocation optimizes agents surplus p. model, meansspecifically:520fiD ECENTRALIZED UPPLY C HAIN F ORMATIONproducers optimal choice either active feasible, acquire goods. Hence,producer allocation obtains nonnegative surplus active, producerallocation would obtain nonpositive surplus active.V 0 ,p(g)\V 0 ,p(g)p(g) 0p(g) 0hg,iEh,giEh,giEhg,iEconsumer receives value obtaining one good, consumers optimalchoice obtain good gives maximum nonnegative surplus, obtaingoods positive price. Furthermore, consumer allocation (i.e., obtaininggoods) would obtain nonpositive surplus good.c C V 0 , hg, ci E 0 , g = arg maxvc (g0 ) p(g0 )0g Gvc (g) p(g) 0hg0 , Ei, g0 6= g, p(g0 ) = 0c C \V 0 , g G,vc (g) p(g) 0Figure 5 shows example competitive equilibrium Network greedy-bad. pricesshown respective goods.a151a2211a3310a44165a5057a706cons1415a60Figure 5: competitive equilibrium Network greedy-bad.competitive equilibrium allocation stable sense agent would want differentallocation equilibrium prices. Moreover, equilibrium way reallocate resources (including money transfers) agent greater surplus, without degradingagents surplus. absence gains trade referred Pareto optimality.Given quasilinear utility, price equilibria shown efficient fairly general conditions (Bikhchandani & Mamer, 1997; Gul & Stacchetti, 1999; Ygge, 1998). also holdsparticular case task dependency networks, stated Corollary 4.521fiWALSH & W ELLMANp(1) >_5a15a21a31a411p(2) >_12p(3) <_13p(4) >_4a509_> p(6) >_ 10p(5) >_65a706cons9a604Figure 6: Network greedy-bad costs values support competitive equilibrium.4.2 Existence Competitive Equilibriumtask dependency networks competitive equilibria. Consider Network greedybad vcons = 9, shown Figure 6. allocation shown efficient allocation,hence equilibrium must support it. Recall equilibrium, active agents must obtain nonnegative surplus, inactive producers must able obtain positive surplus. price inequalities goods follow constraints surplus associated agent activity. lowerbounds prices goods 1, 2, 5 ensure producers a1, a2, a5, respectively receiv5enonnegative surplus. upper bound 3 ensures a3 could obtain positive surplus.lower bound 4 ensures a6 would receive nonpositive surplus. Propagating bounds6, see p(6) 10 give a7 positive surplus, also p(6) 9 give cons nonnegativesurplus. Since impossible, competitive equilibrium cannot exist.Technically, non-existence equilibrium due complementarity inputs producersdiscrete-quantity goods. fact, complementarities necessary preclude competitive equilibrium task dependency networks. network input complementarities producersone input.Theorem 1 Competitive equilibria exist network input complementarities.defer proof subsequent theorems Appendix A.Consider Figure 6. multiple undirected paths 1 4 give rise lowerbound price good 6. turns undirected cycles also necessary precludecompetitive equilibrium.polytree graph one undirected path vertex another.Recall task dependency networks, producer uses multiple units good, unitrepresented separate edge. follows allocation polytree iff one unitgood used produce another given good, used multiple ways produce good.Theorem 2 Competitive equilibria exist polytree.522fiD ECENTRALIZED UPPLY C HAIN F ORMATION4.3 Approximate Price Equilibriumgenerally expect market protocols based discrete price adjustments (suchSAMP-SB protocol describe Section 5) would overshoot exact equilibria least smallamount. Therefore, analysis emphasizes approximate equilibrium concepts (Demange et al.,1986; Wellman et al., 2001a). introduce particular type approximation, --competitiveequilibrium, defined terms parameters bound degree agents acquire suboptimal surplus. Intuitively, b bounds suboptimality consumers surplus, boundssuboptimality producers surplus attributable output, g bounds suboptimalityproducer surplus attributable input g. described Section 5, parameters alsospecial interpretation market protocol applied task dependency networks.Denote Ha (p) maximum surplus agent obtain (V, E), prices p, subjectfeasibility. is,Ha (p)max (a, (V 0 , E 0 ), p)(V 0 ,E 0 )(V,E)feasible (V 0 , E 0 ).Definition 4 (--competitive equilibrium) Given parameters:b , 0,g G,gallocation (V 0 , E 0 ) --competitive equilibrium prices p iff:1. A, (a, (V 0 , E 0 ), p) 0.2. c C, (c, (V 0 , E 0 ), p) Hc (p) b .3. , (, (V 0 , E 0 ), p) H (p) (hg,iE g + ), feasible (V 0 , E 0 ).4. goods material balance.Consider Network greedy-bad prices shown Figure 5 except p(5) = 8.constitute exact competitive equilibrium a6, though inactive, could makepositive profit. However, 2a6 + 3a6 + 4a6 + 1, since Ha6 (p) = 1, a6 obeys Condition 3allocation --competitive equilibrium specified prices.Theorem 3 (V 0 , E 0 ) --competitive equilibrium (V, E) prices p, (V 0 , E 0 )feasible allocation nonnegative value differs value efficient allocation[hg,iE g + ] + |C|b .--competitive equilibrium corresponds standard notion competitive equilibriumgb = = 0, = 0 g.Corollary 4 (to Theorem 3) competitive equilibrium allocation efficient.noted Section 4.1, consistent previously established results.523fiWALSH & W ELLMAN4.4 Valid Solutionsfollowing sections show --competitive equilibria useful concept analyzing decentralized market protocols. However, protocols always reach --competitiveequilibria networks. Hence also consider weaker constraints prices, consistentlesser degree agent optimization solution allocation.say solution (V 0 , E 0 ) valid respect prices p if:1. consumer solution pays value single good. is,c C V 0 , exists single hg, ci E 0p(g) vc (g),p(g0 ) = 0 g0 6= g hg0 , ci E 0 .2. None active producers unprofitable. V 0 h, g E 0(, (V 0 , E 0 ), p) 0. Note solution validity preclude inactive producerunprofitable (i.e., admits dead ends).Note (1) effectively states consumers obtain negative utility, weakercompetitive equilibrium conditions require consumers receive optimalallocation. Similarly, (2) require producers optimize, competitive equilibrium,requires nonnegative utility active producers.a151a2212a3311a44155a5058a706cons1315a60Figure 7: valid solution Network greedy-bad.Figure 7 shows example valid solution, underlying costs valuesFigure 5. allows dead ends, validity directly provide useful boundsinefficiency allocation.5. SAMP-SB Protocolpreceding section introduces static properties price configurations allocations.address problem prices might obtained. compute prices allocations,must elicit information bearing relative value goods, systematic communication process. Mechanisms determine market-based exchanges based messagesagents called auctions (McAfee & McMillan, 1987).524fiD ECENTRALIZED UPPLY C HAIN F ORMATIONagents bidding policies represent strategies interacting auctions. Whereasauction mechanism may designed central authority, bidding policies generally determined individual agents. understand implications auction design requiresanalysis market protocol arises combination auction mechanismagent bidding policies.space potential auctions expansive (Wurman et al., 2001), definitive theoreticalresults currently known fairly limited classes problems (Bikhchandani & Mamer,1997; Demange & Gale, 1985; Gul & Stacchetti, 2000; Klemperer, 1999; McAfee & McMillan,1987). Complementarities discrete goods, cause nonexistence price equilibria,also greatly complicate auction design analysis auctions (Milgrom, 2000).supply-chain domain, investigated particular protocol, called SAMP-SB (Simultaneous Ascending (M+1)st Price Simple Bidding). demonstrated below, SAMP-SBproduce good allocations which, cases, consistent competitive price equilibrium theory.5.1 Auction MechanismSAMP-SB mechanism comprises set auctions, one good. Auctions run simultaneously, asynchronously, independently, without direct coordination. Agents interactauctions submitting bids goods wish buy sell. bid form: ((q1 p1 ) . . . (qnpn )). pair (qi pi ) indicates offer buy sell good, qi indicating quantityoffer pi indicating price. qi > 0, offer buy qi units goodpi per unit, refer buy offer. qi < 0, offer sell qi units lesspi per unit, refer sell offer. agent buys sells goodtask dependency network, bid contains either positive negative quantity offers. Bidspossess sometimes called additive-OR semanticsthe offers treated exactlycame separate bids, hence auction match individual offers independently.Without loss generality, henceforth impose restriction |qi | = 1 offers bids,continuing allow agents may submit multiple offers bid.auction receives new bid, sends bidders price quote specifyingprice would result auction ended current bid state. Price quotes issuedinitial bids received, subsequently issued immediately receipt new bids.offers may tied current price, information alone sufficientagent tell whether winning offer placed price. clarify ambiguity, pricequote also reports bidder quantity would buy sell current state.prices sent bidders, reported winning state specific recipient. Agents maychoose revise bids response notifications (if agent wish changebid, inaction leaves previous bid standing auction).assume communication reliable asynchronous.3 is, messages sent eventually reach recipients, although impose bound delays. Agents auctions usemessage IDs ensure handle messages appropriate order. Note even auctions agents deterministic behaviors, overall run SAMP-SB may nondeterministicdue asynchrony.3. Technically, adopt model asynchronous reliable message passing systems (Fagin et al., 1995).525fiWALSH & W ELLMANasynchrony, helpful auction send ID recent bid receivedagent price quote. agent responds price quote reflectsrecent bid sent. Without device, agent difficulty establishing feasibility,understanding input output bid states may based nonuniformly delayed reports.Bidding continues quiescence, state messages received, agentchooses revise bids, auction changes prices, ask prices, allocation. point,auctions clear; bidder notified final prices many units transactsgood. Note quiescent system necessarily solution state (approximate) equilibriumstate.Although detecting quiescence straightforward centralized system, decentralized,asynchronous system need perform operation using local message passing. previous work (Wellman & Walsh, 2000), described protocol detecting quiescence generaldistributed negotiations, based well-known termination-detection algorithm.auction runs according (M+1)st-price rules (Satterthwaite & Williams, 1989, 1993;Wurman et al., 1998). (M+1)st price auction variant (second-price) Vickrey auction (Vickrey, 1961), generalized allow exchange multiple units good. Given setoffers including units offered sale, (M+1)st-price auction sets price equal price(M+1)st highest offer offers. price said separate winnerslosers, winners include sell offers strictly price buy offersstrictly price. agents offer (M+1)st price also win; case ties, offerssubmitted earlier precedence. Winning buy sell offers matched one-to-one, pay(or get paid) (M+1)st price.issuing price quotes, auction reports price (i.e., current going price,(M+1)st price), p(g) ask price, (g) good g. ask price specifies amountbuyer would offer order buy good, given current set offers. askprice determined price Mth highest offers auction, hence (g) p(g).instance, buy bids 12, 10, 6 sell bids 15, 11, 8, p(g) = 10, (g) = 11,auction quiescence, buy bids 12 10 would match sell bids 15 11trade p(g) = 10.producer complementary inputs, ensuring feasibility challenging problem,requiring careful design. auctions run simultaneously, auction requires pricesagents successive buy offers increase less (generally small) positive numberb prices successive sell offers increase less .4 auction enforceascending rule simply rejecting agents offer price increase b .constraining direction price changes, design gives producers accurate indicationrelative prices inputs outputs prices allowed fluctuate directions.ascending bid restriction ensures ascending auction prices, one technicality. Dueasynchrony immediate issuance price quotes, initial bid agent arriveshigher bid, price quote could decrease. handled simply auction issuingprice quotes specified period time auction opens. first price quoteissued, auction accepts new bids agents previously placed bids.common auction literature practice place ascending restriction buy-offerprices. may seem counterintuitiveand fact atypicalto place restriction4. rules differ typical simultaneous ascending auction (Demange et al., 1986; Milgrom,2000), specify agents must submit offer prices least increment current price.526fiD ECENTRALIZED UPPLY C HAIN F ORMATIONsell-offer prices. However, ascending offer price restriction ensures price quotes risemonotonically auctions progress. Section 5.4 shows ascendingoffer-price restriction buy sell offers serves key role establishing relationships systemquiescence solution convergence system.5.2 Bidding PoliciesAlthough designers negotiation mechanisms generally control agents behaviors, conclusions outcome mechanism must based assumptionsbehaviors. typical assumption economics agents rational sense,example play policies form Bayes-Nash equilibrium. However, discussedSection 7.1, complexity supply chain formation markets beyond current state-of-the-artanalyzing Bayes-Nash equilibria simultaneous ascending auctions. Instead, analysis assumes agents follow simple, non-strategic bidding policy, described section.variations may reasonable, perhaps better respects policies describe.Rather explore range possibilities, chose work investigate particular setpolicies depth. chosen policies obey ascending offer restriction enforced auction,respect locality information require knowledge agents system,myopic use information provided current price quotes, withoutforecasting future prices.Recall consumer wishes acquire single good maximizes surplus givenprices. assume consumer initially offers zero good interest. longwinning good, change offer. Whenever winning good, offers p(g ) + bgood g = arg maxgG (vc (g) p(g) b ) vc (g ) p(g ) b 0, otherwise stops bidding.producers objective much complex, namely maximize differenceprice receives output total price pays inputs, remaining feasible.assume producer initially offers zero input goods, gradually increasesoffers ensure feasibility. raises offer price input good b pricequotes indicate losing good winning output.assume producer bids output good g effort recover production costperceived costs inputs. producer places first output offer receivingfirst price quotes inputs, subsequently updates output offer whenever receivesnew price quote input. simplicity, consider case one offer (eachquantity one) input. currently winning input g, perceived cost, p (g) gsimply p(g). currently winning g particular offer, p (g) = max((g), p(g) +b ). price previous offer made g , perceived costs increase,offers max( + , hg,iE p (g)) output g . multiple offers good g,assumes separate perceived cost respect offer, bids output accordingly.Figure 8 shows producer would bid next function current prices currentoffers, b = 1 2.Note throughout negotiation, producer places bids output goodsreceived commitments input goods. Producers counteract potential risk continually updating bids based price changes feasibility status. producer reduces exposure deadends incrementing offer prices inputs minimal amounts necessary.527fiWALSH & W ELLMANCurrent offer price = 2p(A) = 1(A) = 2C3Bp(B) = 2(B) = 4GoodNext OfferPriceBChold 225Current offer price C = 3Current offer price B = 1Figure 8: producers next offers, according SAMP-SB, b = 1 2. dashedarrow good B indicates producer currently losing B. solid arrowsindicate producer currently winning goods C.5.3 Bidding General Preferencestask dependency network model represents fairly simple production capabilities consumerutility. discuss natural potential extensions bidding policies broader classcapabilities preferences.producer capable variable-unit production could bid exactly multiple identicalproducers. producer would maintain separate offers bids unit, updateseparate offers independently. Similarly, consumer additive value multiple goods,multiple units good, could bid unit good separate consumer.producer alternatives input, independent inputs, switch biddingcurrently cheapest option. Subtle issues arise producer alternative input sets,particularly tentatively winning parts sets. One option would focus biddingset lowest perceived cost, may include premium goods tentativelywinning set. Alternatively, producer could assume definitely win tentativelygoods effectively treat sunk costs. Fractional accounting sunk costs may alsoreasonable. Similar considerations arise extensions presenting complex consumption choices.5.4 Properties SAMP-SBsection describe number theoretical properties SAMP-SB. Section 5.4.1describe properties relating convergence quiesence, Section 5.4.2 present properties relating efficiency convergence price equilibrium, Section 5.4.3 present propertiesrelating solution convergence.5.4.1 C ONVERGENCEQ UIESCENCESAMP-SB auctions bidding policies guarantee system always reach quiescence.Theorem 5 SAMP-SB reaches quiescence finite number bids placed.However, convergence take long time.Observation 6 asynchronous environment, possible run protocol may requirenumber bids exponential network size, function consumer value.528fiD ECENTRALIZED UPPLY C HAIN F ORMATION1-12-11-A001-3start201-B3-A012-301-23-12-A023-302-22-B003-23consumer13-B0Figure 9: Network exponential: network may require exponential number bidsreach quiescence.Figure 9 shows Network exponential, illustrates observation. agent namedstart places one-time bid sell one unit good 0 $2. Since (0) = 2, producers 1-11-2 initially losing input bids, agents offer price 2 output goods.Producer 1-3 receive new price quotes goods 1-A 1-B asynchronously, hence mayupdate bid good 1 twice, offering price 2 first time price 4 secondtime. Continuing process, see producer 3-3 updates bid good 3 eighttimes. extend network maintain labeling consistent Figure 9, producer n-3would place O(2n ) bids good n. Note however, bids price quotes propagatedsynchronously, exponential growth would occur.example above, bids actually superfluous meaningfullyaffect outcome protocol. appears often true situations exhibiting worst-casebehavior described. capture distinction relevant irrelevant bidding, introducenotion quasi-quiescence, persistent state subsequent bids effectivelymatter solution convergence. SAMP-SB convergence quasi-quiescence requires numbermeaningful bids bounded size network value maximumconsumer value.Definition 5 (quasi-quiescent) run SAMP-SB quasi-quiescent state when, consumer active producer , bids received would change bidsresponse price quotes already received transmitted auctions.Clearly, requirements quasi-quiescence subset requirements quiescence.Observation 7 quiescent state quasi-quiescent state.Theorem 8 run SAMP-SB reaches quasi-quiescent state, remains quasiquiescent state. Furthermore, neither allocation prices p subsequently change.theorem means that, quasi-quiescence reached, subsequent bids effectivelymatter terms equilibrium solution convergence.Corollary 9 (to Theorem 8) quiescent state SAMP-SB --equilibrium valid solutioniff first quasi-quiescent state reached --equilibrium valid solution, respectively.following theorem establishes bound number relevant bids necessary reachquasi-quiescence.529fiWALSH & W ELLMANTheorem 10 SAMP-SB reaches quasi-quiescent state number bids bounded polynomial size network value maximum consumer value placedconsumers active producers.previously mentioned quiescence-detection protocol (Wellman & Walsh, 2000) also detectquasi-quiescence, thus terminate negotiations reached.5.4.2 E FFICIENCYC ONVERGENCEP RICE E QUILIBRIUMintentionally use b , parametrize SAMP-SB concept --competitiveequilibrium. interpretation g terms prices ask prices, specify necessary sufficient conditions result SAMP-SB corresponds --competitiveequilibrium.Theorem 11 prices allocation determined quiescence SAMP-SB protocol--competitive equilibrium, g = max((g) p(g), b ), iff inactive producer buyspositive-price input.Theorems 3 11, establish bounds inefficiency --competitive equilibrium, parametrized g = max((g) p(g), b ) good. cases, difference(g) p(g) may quite high. However, actually establish tighter bound.Theorem 12 (V 0 , E 0 ) --competitive equilibrium computed SAMP-SB, (V 0 , E 0 )nonnegative value differs value efficient allocation (|{hg,E}| b + ) + |C|b .Note theorem replaces Theorem 3 b bound.network tree polytree one consumer.gTheorem 13 quiescent state SAMP-SB --competitive equilibrium tree.unaware general network structures SAMP-SB guaranteed converge --competitive equilibrium. However, Theorem 11 implies improve allocations modify SAMP-SB avoid dead ends. say bidding policy safe producerproducer cannot obtain negative surplus quiescence. clear protocol safeproducers, converge --competitive equilibrium.SAMP-SB assumed producer updates buy sell offers simultaneouslyresponse price quotes. policy safe, even single-input producers,producer bids input based state standing offer output, rather offerplace. producer would get negative surplus win new output offergets stuck winning new input offer. However, slight variant bidding policy,call safe SAMP-SB, safe single-input producer. protocol, producer updatesinput bids would update, currently winning, recent output offer.Clearly, safe SAMP-SB static properties SAMP-SB, hence Theorem 12 appliessafe SAMP-SB.Theorem 14 quiescent state safe SAMP-SB --competitive equilibrium networkinput complementarities.530fiD ECENTRALIZED UPPLY C HAIN F ORMATIONSafe SAMP-SB guaranteed safe producers multiple inputs arbitrarynetworks, know safe producer bidding policy ensures safety producersarbitrary network (other degenerate policies bidding).Safe SAMP-SB may take longer reach quiescence regular SAMP-SB. safe SAMP-SB,producer must always wait notification results pending output offers increasing input offers. producer win output offer may require propagations many messagesvarious paths network buyers output good would increase buy offerprices good. resulting delay would greater local delay communicatingoutput good auction.non---competitive equilibrium runs SAMP-SB result dead ends suggests potentialsource significant efficiency loss. example, Figure 7 shows result run SAMP-SBNetwork greedy-bad. valid solution dead end producer a6. Since producer a3incurs cost $1 provide good 3 a6, contribute value system,dead end pure waste global efficiency perspective. allocation undesirable directlyproducer a6 committed pay $1 input cannot use. large networkscosts, dead ends result significant efficiency losses negative profits individual agents.propose contract decommitment protocol remove dead ends SAMP-SB reachesquiescence. According decommitment protocol, inactive producer decommitcontracts inputs would pay positive price. protocol applied recursivelyproducers lose outputs due decommitment. decommitment processterminates, agents exchange goods specified remaining contracts. refer SAMP-SBdecommitment SAMP-SB-D.Figure 7, producer a6 would decommit contract a3. Clearly, Theorem 11 impliesagent decommits iff SAMP-SB produced --competitive equilibrium. Moreover,remove consideration producers decommit, remaining agents --competitiveequilibrium.Decommitment benefit that, whereas producers lose money SAMP-SBprotocol, agent receives negative surplus participating SAMP-SB-D. However,achieved making auction allocations non-binding, undesirable producerslose output sales decommitments. also begs question enforcerequirement inactive producers agents decommit.addition dead ends, efficiency also lost SAMP-SB fails find solutionpositive value solution exists, SAMP-SB forms solution value inferior efficient solution (dead ends necessarily mutually exclusive two cases). Section 6describe experimental analysis efficiency, source inefficiency, equilibriumattainment SAMP-SB set networks.5.4.3 OLUTION C ONVERGENCERecall SAMP-SB always converges valid solution (specifically --competitive equilibrium) networks tree structures, safe variant converges networks inputcomplementarities. following theorem shows that, sufficiently high consumer value, regular SAMP-SB always converge (possibly non-equilibrium) valid solution polytrees.531fiWALSH & W ELLMANTheorem 15 (V, E) polytree solution assigns good g consumer c, givencosts values, exists value vc (g) SAMP-SB guaranteed convergevalid solution (V 0 , E 0 ) c.dead ends may result, cannot usefully bound inefficiency solution reachedSAMP-SB polytree.general network structures, prices sell offers consumers goods could risevalues, case system necessarily reach quasi-quiescence nonsolution state. If, however, quasi-quiescence reached price consumers goodreaches value good, valid solution.Theorem 16 SAMP-SB reaches quasi-quiescence p(g) < vc (g) hg, ci E, c C,systems state represents valid solution.next theorem establishes conditions valid solution state immediatelylead quasi-quiescence.Theorem 17 run SAMP-SB (V, E) valid solution state that:consumer c either winning offer p(g) + b > vc (g) hg, ci E,agents correct beliefs goods currently winning,bids consumers active producers received response currentprice quotes,sell offers lost due tie breaking,subsequent price quote auction, system quasi-quiescentstate valid solution.Although SAMP-SB guaranteed converge solution, fact problem finding solution NP-Complete (Walsh et al., 2003) lead us expect problemsSAMP-SB would converge solution exponential number meaningfulbids. Since number meaningful bids bounded polynomial maximum consumervalue, expect exist networks SAMP-SB convergesolution exponential consumer values. practice find construct problems consumer value must exponential order SAMP-SB convergesolution (Walsh et al., 2003). However, run many simulations required valuemuch reasonable (Walsh et al., 2003).networks, costs, values, SAMP-SB cannot converge valid solutionvalues b , matter high consumer value. One example (the simplestable construct) Network no-converge, shown Figure 10. Observe solutionmust include agent a8, cannot include a7. Agent a6 always offers price least p(2) + 20good 4, hence a8 cannot win two units good 4 less p(2) + 20 each. Thus agent a8always offer price least 2p(2) + 40 good 5. Since agent a7 never offer price2p(2) + 22a7 good 5, agent a8 could win good 5 2a7 20. But, occur,must b 20. thorough analysis, taking account dynamics SAMP-SB,shows must b 40 = 0 obtain valid solution quiescence,certain patterns asynchrony.532fiD ECENTRALIZED UPPLY C HAIN F ORMATIONa2a1102a7200a350a63consa8400a4a52020Figure 10: Network no-converge: network SAMP-SB cannot converge solutioncertain values b .6. Empirical Performance SAMP-SBWhereas analytic results provide insight SAMP-SB variants, support comprehensive characterization performance, except certain special-case network structures. order gain understanding effectiveness SAMP-SB SAMP-SB-D,performed empirical study based protocol simulations sample task dependency networks.6.1 Setupinvestigation focuses small set networks exhibiting variety structural properties:simple (Figure 11), unbalanced (Figure 12), two-cons (Figure 13), bigger (Figure 14),many-cons (Figure 15). also also studied Network greedy-bad (Figure 4).a11a33c11.216a22a4Figure 11: Network simple.ran experiments multiple instances network. instance randomlychose producer costs uniformly [0, 1], consumer network, calculatedfixed value that, excluding consumers, exists positive-surplus solutionconsumer 0.9 probability. determined consumer values via simulation, assumingspecified distributions producer costs. discarded instances whose efficient solutionsvalue zero. set b = = .01.test effect competitive equilibrium existence performance protocols,generated instances unbalanced, two-cons, greedy-bad costs admitcompetitive equilibrium costs not. simple many-cons polytrees,533fiWALSH & W ELLMAN1a1a88a2a92a13a339a10a44a55a66a7715c13.73a14a11a15a1210Figure 12: Network unbalanced.a3a131c11.23a44c22.17a22a5Figure 13: Network two-cons.a11a17a9a22a33a10a18a1110a12a44a55a13a66a14a77a15a25139a19a26a2714a28a201711a21a2915a22a30a23a311612a88a16a24a32Figure 14: Network bigger.534c13.51fiD ECENTRALIZED UPPLY C HAIN F ORMATIONa11a22a33a44a55a66a1313a1919c14.42a1414a20a151520a773.89a16a88a99a1010a1111a1212c216a21a1717a2221c34.44a1818Figure 15: Network many-cons.know Theorem 2 instances thereof competitive equilibria. ablegenerate no-equilibrium instances bigger given cost distributions.generate instance desired type cost structure (equilibrium no-equilibrium)repeatedly chose sets producer costs randomly uniform distribution desiredproperty met. experiments, determined whether competitive equilibrium existedgiven complete information network structure, values, costsusing followingprocedure. Recall competitive equilibrium always efficient (Corollary 4). Hence, givenoptimal allocation (V , E ), attempt solve system linear equations characterizecompetitive equilibrium, described Section 4.1. solution equations exists,resulting prices constitute competitive equilibrium, otherwise equilibrium exists. usedCPLEX, commercial mixed-integer-linear programming package, find efficient allocationsolve corresponding equilibrium equations.type cost structure network, tested 100 random instances, exception simple, tested 3220 instances.5 instance protocol, measured efficiencythe fraction efficient valueattained SAMP-SB SAMP-SB-D.also measured percentage available surplus (i.e., percentage value optimalsolutions) obtained producers.6.2 Resultsclassify efficiency run protocols one four ways: Negative, Zero, Suboptimal(but positive), Optimal efficiency. Table 1 shows distribution efficiency classesexperiments. Note SAMP-SB-D cannot produce negative efficiency, construction.5. tested instances simple part broader study (Walsh et al., 2000).535fiWALSH & W ELLMANNetworksimpleunbalanced, case:equilibrium existsequilibrium existstwo-cons, case:equilibrium existsequilibrium existsbiggermany-consgreedy-bad, case:equilibrium existsequilibrium existsSAMP-SB% instancesNeg Zero Sub Opt0.00.30.0 99.7SAMP-SB-D% instancesZero SubOpt0.30.099.75.0100.01.00.07.00.087.00.01.0100.01.00.098.00.011.018.00.027.00.00.00.00.06.078.04.056.083.04.096.017.00.01.00.00.03.095.00.02.097.04.0100.098.04.0100.00.00.021.00.075.00.01.0100.00.00.099.00.0Table 1: Distribution efficiency classes SAMP-SB SAMP-SB-D. Efficiency classes:Negative (Neg), Zero, Suboptimal (Sub), Optimal (Opt).Recall (from Section 5.4.2) efficiency loss SAMP-SB attributable three,necessarily exclusive, causes: dead ends, failure form solution positive-valuedsolution exists, finding suboptimal solution. infer percentage instances exhibiting dead-end suboptimality SAMP-SB examining differences SAMP-SB-DSAMP-SB totaled Negative, Zero, Suboptimal columns Table 1. Decommitmentaffect contribution no-solution suboptimal-solution losses, helps revealeliminating dead-end suboptimality. Hence, infer percentage instances exhibiting no-solution suboptimal-solution suboptimality SAMP-SB examining ZeroSuboptimal columns SAMP-SB-D, respectively.Table 2 shows average efficiency attained protocols, factored network equilibrium existence (where relevant). see, difference SAMP-SB-DSAMP-SB columns, dead ends significant source inefficiency. Additionally, existencecompetitive equilibrium significant effect performance protocols. networks, SAMP-SB-D produces nearly perfect efficiency competitive equilibrium exists (recallstudied instances simple, bigger, many-cons equilibria), much lesseffective equilibrium exist, fact failing find solutions no-equilibriumcases unbalanced greedy-bad.check whether differences performance significant, performed Students tTests protocol, comparing mean efficiencies instances admit competitive equilibrium means instances admit competitive equilibrium. Table 3 showsresults, indicating p-values means equilibrium no-equilibrium instances cameunderlying population. typical analyses, null hypothesis meansequal rejected p-value 0.05. threshold, seems safely rejecthypothesis mean efficiencies equilibrium non-equilibrium instances536fiD ECENTRALIZED UPPLY C HAIN F ORMATIONNetworksimpleunbalancedequilibrium existsequilibrium existstwo-cons, case:equilibrium existsequilibrium existsbiggermany-consgreedy-bad, case:equilibrium exists:equilibrium exists:SAMP-SB0.997SAMP-SB-D0.9970.86720.0800.9900.0000.7330.2681.0000.1200.9860.6861.0000.9965.32018.2300.9900.000Table 2: Average efficiency network protocols.Networkunbalancedtwo-consgreedy-badSAMP-SBSAMP-SB-D6.27 10305.15 1071.41 1018.23 101011.43 10228.04 10101Table 3: P-values computed Students t-Test. t-Test compared means efficienciesinstances admit competitive equilibrium admit competitiveequilibrium.537fiWALSH & W ELLMANNetworks unbalanced greedy-bad. Inspection data supports conclusion, SAMP-SB-D essentially always produces zero efficiency, produces perfect efficiencymany instances admit competitive equilibrium.face it, high SAMP-SB/greedy-bad p-value suggests cannot safelyreject hypothesis mean efficiencies differ equilibrium no-equilibriuminstances network. However, inspection data indicates high probability results one outlying equilibrium instance large negative efficiency. Indeed, factSAMP-SB-D always produces essentially optimal results instances admit competitive equilibrium, predominantly produces suboptimal results instances without equilibria,suggests unlikely equilibrium no-equilibrium means SAMP-SBNetwork greedy-bad.Networksimpleunbalancedequilibrium existsequilibrium existstwo-cons, case:equilibrium existsequilibrium existsbiggermany-consgreedy-bad, case:equilibrium existsequilibrium exists% --CompetitiveEquilibrium1008808329617750Table 4: Percentage instances SAMP-SB attained --competitive equilibrium.Table 4 shows percentage instances SAMP-SB attained --competitive equilibrium network. straightforward determine whether --competitive equilibriumattained observing whether dead ends (Theorem 11). Again, see strongconnection existence competitive equilibrium. One notable exception many-cons(which always admits competitive equilibrium), SAMP-SB frequently produced deadends. see --competitive equilibria form small percentage no-equilibriumtwo-cons instances, although prevalent phenomenon b parameterschose.Table 5 shows average efficiency, factored --competitive equilibrium attainment(SAMP-SB SAMP-SB-D produce results --competitive equilibrium attained). must careful drawing conclusions statistics because, givennetwork case, relatively many --competitive equilibrium instances (Table 4).Still, note certain salient trends. --competitive equilibrium runs produce near perfect efficiency, smaller degrees inefficiency specified bounds Theorem 12.allocation produced SAMP-SB --competitive equilibrium iff dead ends,expect significant portion efficiency loss non---competitive equilibrium pro538fiD ECENTRALIZED UPPLY C HAIN F ORMATIONNetworksimpleunbalancedequilibrium existsequilibrium existstwo-cons, case:equilibrium existsequilibrium existsbiggermany-consgreedy-bad, case:equilibrium existsequilibrium exists--EquilibriumFoundSAMP-SB SAMP-SB-DN/AN/A--EquilibriumFoundSAMP-SB/ SAMP-SB-D0.9970.24820.080.9890.0000.998N/A0.5700.1300.9970.0600.9200.7071.0000.9951.0001.0001.0001.00024.2818.220.9600.0001.000N/ATable 5: Average efficiency network protocols, factored --competitive equilibrium attainment.ducing runs SAMP-SB would attributable negative surplus incurred dead ends.significant differences efficiency SAMP-SB-D SAMP-SB shown non--competitive equilibrium column provides evidence hypothesis. Indeed, appearssurplus lost dead ends (as opposed suboptimal solution attainment) dominant causeinefficiency --competitive equilibrium attained. instances, improvementdecommitment greater difference efficiency SAMP-SB-D optimalefficiency.Table 6 shows average fraction available surplus obtained producers, respectively,network. Perhaps surprisingly, networks producers gain significant surplusSAMP-SB-D protocol, even though bidding obtain zero surplus. reasonproducers output offer indicates minimum amount willing acceptexchange output. rising buy offers cause price rise producers outputoffer. could happen cases necessary block certain agents feasibleallocation quiescence. Note however, decommitment step needed producersobtain high average surplus. Without decommitment, average producer surplus highlynegative, shown SAMP-SB column.7. Related Literaturesection discuss literature related present work. Section 7.1 discuss relatedliterature price-based analysis auction theory, Section 7.2 discuss related literaturesupply chain formation.539fiWALSH & W ELLMANNetworksimpleunbalanced, case:equilibrium existsequilibrium existstwo-cons, case:equilibrium existsequilibrium existsbiggermany-consgreedy-bad, case:equilibrium existsequilibrium existsSAMP-SB0.000SAMP-SB-D0.0000.04120.090.0820.0000.2100.1370.0010.5170.4640.5550.0010.3596.0818.110.1370.000Table 6: Average fraction available surplus obtained producers network protocols.7.1 Price-Based Analysis Auction Theoryshown special cases competitive equilibria exist task dependency networks (polytree single-input-producer networks), SAMP-SB always finds --competitiveequilibrium trees, minor variant always finds --competitive equilibria single-inputproducer networks. review results price equilibrium auction theory revealslimited positive results typical.well-known given arbitrarily divisible goods convex utility, cost, production functions, competitive equilibrium prices exist. additionally, gross substitutes condition(which generalization no-complementarities) met, classic tatonnement procedure findscompetitive equilibrium distributed manner.6goods discrete, competitive equilibria exist exchange (non-production) economiesgross substitutes conditions met (Bikhchandani & Mamer, 1997; Gul & Stacchetti, 1999;Kelso & Crawford, 1982). Milgrom (2000) showed existence single complementaritysufficient preclude equilibrium exchange economies. Bickhchandani Mamer (1997)also show existence variety conditions, appear natural interpretations task dependency networks. exchange economies, gross substitutes conditionalso ensures convergence (approximately) competitive equilibria simultaneous ascendingauctions (Demange et al., 1986; Gul & Stacchetti, 2000).distributed price-based auction protocols leave agents undesired goodspreferences complementary (e.g., dead ends task dependency network), widely recognized problem. alternative approach use combinatorial auction, mediates negotiation single location, performing global matching combinations goods based indivisiblebids. general approach received much attention AI community late, motivated6. reader may consult standard microeconomic textbook (Mas-Colell et al., 1995) details results.540fiD ECENTRALIZED UPPLY C HAIN F ORMATIONpart techniques quickly performing necessary global optimization (Andersson et al.,2000; Leyton-Brown et al., 2000; Sandholm & Suri, 2000).Currently, results combinatorial equilibria auctions established onesided (i.e., buyer only) bidding. Bikhchandani Ostroy (2002) Wurman Wellman (2000),using different combinatorial frameworks, provide positive results equilibrium existence,properties thereof. Wurman Wellman describe combinatorial auction framework.Parkes Ungar (2000) describe combinatorial auction guaranteed convergeefficient allocation agents follow myopic best-response strategies. adding extend-andadjust phase, authors able obtain allocation ex post Nash equilibrium (Parkes& Ungar, 2002). Ausubel Milgrom (2002) present proxy-auction mechanism obtainsefficient allocations straightforward bidding equilibrium goods substitutes.present work consider simple, local, myopic bidding policies. policiesnon-strategic, agents reason effect negotiations attemptextract greater surplus. assumption non-strategic behavior plausiblelarge number agents. networks many agents bidding individual goods, many parallelbranches, many agents sequence, potential contribution one agent valuesolution relatively small little gain strategic behavior.experiments shown that, even producers bid obtain zero surplus specified policy, obtain positive surplus networks. Nevertheless, smaller networks,potential strategic improvement pronounced, non-strategic assumption becomes less plausible. widely studied concept used analyzing strategic behavior Bayes-Nashequilibrium.7 Informally, set strategies constitutes Bayes-Nash equilibrium single agentincentive deviate strategy, given agents play Bayes-Nash equilibrium strategies. McAfee McMillan (1987) Klemperer (1999) survey state knowledge strategic analysis auctions exchange economies. Milgrom (2000) provides insightsfundamental challenges understanding agent behavior complementarypreferences. However, definitive results known quite restrictive market structures,encompass two-sided markets complementarities, never mind multi-level characteristic negotiation task dependency networks. problem even specifying informationstructure extensive form game simultaneous ascending (M+1)st price auctions task dependency networks, prerequisite computing Bayes-Nash equilibria, well beyond currentstate art game-theoretic analysis.auction theory currently fails provide satisfactory guidance understanding strategicbehavior even moderately complicated domains, used tournaments frameworkdeveloping evaluating candidate agent strategies. Santa Fe Double Auction Tournament (Rust et al., 1994) provided unexpected insights effective strategies continuousdouble auctions, recent TAC series trading agent competitions (Wellman et al., 2001b,2003) encouraged development sophisticated agent strategies (Greenwald, 2003; Stone &Greenwald, 2000) complex market game.Vickrey-Clarke-Groves mechanism (Clarke, 1971; Groves, 1973; Vickrey, 1961), alsocalled Generalized Vickrey Auction (GVA) (MacKie-Mason & Varian, 1994), direct revelation approach, agents report valuations allocations, auction computeslump-sum payment. GVA, solution optimal allocation based reports,7. foundational reference, Chapter 7 Fudenberg Tiroles game theory text (1998) provides formal treatment strategic issues auction mechanism design analysis.541fiWALSH & W ELLMANpayment function dominant strategy agents report true utility.incentive compatibility perfect efficiency, GVA may seem idealeconomic perspective (although computation intractable). However, GVA budget balancedwhen buyers sellers bid, GVA pay money takesin. Unfortunately unavoidable, impossible simultaneously ensure efficiency, budgetbalance, individual rationality (no agent achieves negative surplus) (Myerson & Satterthwaite,1983). Recently, Babaioff et al. (Babaioff & Nisan, 2001; Babaioff & Walsh, 2003) describeddistributed auction mechanisms, based McAfees trade reduction auction (1992), obtain incentive compatibility budget balance linear supply chains, expense perfect efficiency.Recent work Parkes, Kalagnanam, Eso (2001) explores methods minimize deviationefficiency maintaining budget balance two-sided GVA-like mechanisms.7.2 Supply Chain FormationSupply chain managementthe problem accurately forecasting planning production deliveries meet demand minimize inventoryis active field study operations research.problem management differs supply chain formation exchange partnerssupply chain pre-established, assumed information gatheredagents effectively optimize global production across supply chain. contrast, workapproached problem automating process determining supply chain participants dynamically, assumption information decision making decentralized. Readersinterested supply chain management may refer Kjenstad (1998) extensive review.Relatively less effort explicitly devoted problem cast supply chain formation, despite rhetorical appeals decentralized dynamic relation-building commonplacepopular literature. Nevertheless, point Section 3, venerable AI methodsinparticular widely-known CONTRACT NET protocolcan principle applied supply chainformation. discussed Section 3, standard CONTRACT NET mechanismsresolve nontrivial resource contention, precluding systematic comparison SAMP-SB general network structures. can, however, compare protocols network structuresresource contention mechanism necessary CONTRACT NET. clear agentsbid true costs, CONTRACT NET greedy allocation converge optimal allocations trees. holds tree structures relaxed allow multiple-unit input bids.shown, SAMP-SB guaranteed converge approximately efficient allocations trees.However, shown may converge good solutions multiple unit input bidsallowed. latter case, competitive equilibrium may exist, observedequilibrium non-existence substantially hurt efficiency SAMP-SB allocations. contrast,producers may receive different prices good CONTRACT NET. discriminatory pricing mechanism makes CONTRACT NET robust presence complementarities,class network structures.Sandholm (1993) examines specialization CONTRACT NET generalization Task Oriented Domains (TODs) (Rosenschein & Zlotkin, 1994). Agents begin initial allocationtasks negotiate task exchanges mutually beneficial trades. Sandholms modelallows local constraints task achievement agent perform certain combinations tasks. However, dependency structurean agent rely agentstask achievement order accomplish tasks. Thus, every locally feasible trade results542fiD ECENTRALIZED UPPLY C HAIN F ORMATIONglobally feasible allocation, executed immediately independently trades.cannot generally apply incremental trading protocol task allocation model subtaskdependencies. local exchange may require reallocation throughout entire network maintainproduction feasibility.Andersson Sandholm (1998) find decommitment protocols increase qualityresulting allocations variants TODs. incremental trading, decommitment gives agentsopportunity engage cost-effective contracts. Andersson Sandholm also consider decommitment penalties provide friction reallocation compensate agents whosecontracts broken. expect penalties would appropriate extensionSAMP-SB-D protocol.Veeramani et al. (Veeramani et al., 1999; Joshi et al., 1999) consider issues arising simultaneous negotiation multiple subtasking issues various levels supply chain.asynchronous model, agents may opportunity finalize contract negotiationsstill pending. uncertainty induces complex decision problem agents wishoverextend commitments.Hunsberger Grosz (2000) study problem assigning task performance roles agentsSharedPlans collaborative planning framework. model based recipes, describe precedence constraints execution time across various sub-tasks constitutecomplex task. Contention shared resources modeled explicitly recipe, individual agents may additional cost, timing, constraints, potentially arisingindividual resource limitations. Hunsberger Grosz use combinatorial auction assign tasksagents, given constraints, produce high-valued shared plan. find limiting task assignment certain combinations roles effect tradeoff computational allocativeefficiency.work (Walsh et al., 2000; Walsh, 2001), studied strategic behavior agentsbidding particular one-shot combinatorial auction within task dependency network model.empirically compared performance SAMP-SB, SAMP-SB-D, combinatorial auction (with strategic bidding). combinatorial auction eliminates problem dead ends allocating inputs outputs producers all-or-nothing basis. advantage notwithstanding,combinatorial auctions may always appropriate mechanism. Since finding feasiblesupply chain solution NP-hard (Walsh et al., 2003), sufficiently large problems intractable,even advanced optimization procedures. Even computation tractable, social factorsmay limit authority one entity compute allocations entire supply chain.8. Extensions Future Worktask dependency network model propose provides basis beginning understandautomation supply chain formation. discussed ways extend bidding policies market protocol accommodate general production capabilities consumerpreferences. extensions model capabilities preferences multi-attributegoods (e.g., goods multiple features quality delivery time, addition pricequantity) simply representing configuration distinct good network. However,clear explode number goods attributes. effectively handlegreater numbers attributes would require multiattribute auctions (Bichler, 2001), multipleinseparable features exchange negotiated simultaneously.543fiWALSH & W ELLMANrealistic scenarios, producers may also solve complex internal scheduling, planning, forecasting, complex problems order evaluate costs feasible options.types extensions would increase fidelity model, would implicationsagent bidding policies computation convergence speed market protocols. Despitebest efforts agents forecast plan, agents cannot predict certainty operationwithin formed supply chain. Sophisticated agents would employ probabilistic reasoning techniques evaluating options negotiation. unexpected events occur impairoperation formed supply chain, agents would need protocols repairing reformingsupply chain.assumed simple set non-strategic, myopic bidding policies simultaneous ascending auction. agents must coordinate input output bids dynamic auction mechanism,understanding strategic bidding behavior challenging unsolved, albeit important problemfuture work. seems likely significant developments game-theoretic methodology wouldnecessary analytically solve, even realistically specify, extensive form games incompleteinformation corresponding asynchronous iterative auctions. meantime, make progressunderstanding performance auctions, consider alternate approaches developinggood bidding policies. Tournaments proven effective ways encourage smartpeople design smart trading policies evaluate relative qualities (Rust et al., 1994;Wellman et al., 2001b). Axelrod (1987) used evolutionary approach evaluate populationsstrategies, fixed types, iterated prisoners dilemma. major challenge applyingevolutionary approach supply chain formation problem develop sufficiently rich, yetreasonably searchable set agent bidding policies.suggested decommitment solution problem dead ends SAMP-SB,strategic analysis protocol would take phase account. producerscould lose money decommitment allowed, expect producers wouldwilling participate, would also aggressive bidding. Allowing decommitment begs question enforce producers decommit deadends, also address fact unilateral decisions decommitment potentiallybreak (possibly desirable) contracts many downstream producers. reduce aggressivebidding mitigate potential problems, could charge penalties producers initiatedecommitment (Andersson & Sandholm, 1998), perhaps paid producers whose output contracts get decommitted. would reduce spurious decommitments still allowingproducers stuck costly dead ends.Finally, note market configuration studied hereseparate auctions goodrepresents one possible partition scope negotiations supply chain.extreme, production activity could mediated one combinatorial auction mechanism coveringentire supply chain (Walsh et al., 2000). avoids coordination pitfalls separateauction approach, imposes disadvantages associated imposing mechanismglobal scope. Intermediate configurations, involving multiple auctions clusters highly relatedgoods, represent promising alternative investigation.544fiD ECENTRALIZED UPPLY C HAIN F ORMATIONACKNOWLEDGMENTSpaper includes material previously presented Sixteenth International Joint ConferenceArtificial Intelligence (IJCAI-99) (Walsh & Wellman, 1999). work supported partNSF grant IIS-0205435.Appendix A. Proofsappendix provides proofs theorems. convenience, restate theoremsproofs.proofs, sometimes useful index position producer network. Clevel producer output g maximum distance consumer, formally statedfollows: one producer, consumer, g input, k + 1 maximum levelproducer input g k. S-level producer defined similarly, respectdistance producer input, basis zero producersinputs themselves. C-level S-level well defined, acyclicity.A.1 Proof Theorem 1Let (V, E) network input complementarities, producers oneinput, let (V , E ) optimal allocation (V, E). convenience, partition producerssets 1 , producers single input, 0 , producers inputs.Procedure Input Complementarities Equilibrium constructs prices supportcompetitive equilibrium (V , E ).Input Complementarities Equilibrium:1. Initialize prices zero.2. Perform following price changes made:(a) c C \V , vc (g) > p(g), hg, ci E \ E ,p(g) vc (g).(b) c C V , vc (g0 ) p(g0 ) > vc (g) p(g) 0,hg, ci E hg0 , ci E \ E ,p(g0 ) vc (g0 ) (vc (g) p(g)).(c) 0 V , p(g ) < , h, g E ,p(g ) .(d) 1 V , p(g ) < p(g) +h, g E hg, E ,p(g ) p(g) + .(e) 1 \V , p(g ) > p(g) + ,h, g E \ E hg, E \ E ,p(g) p(g ) .network (V, E) (with input complementarities) prices p, closed, reverse-surplus sequence directly connected sequence agents goods every agent would betterreversing allocation. Formally, sequence (n1 , . . . , nk ) vertices ni V , that:545fiWALSH & W ELLMAN1. hni , ni+1 E hni+1 , ni E 1 k 1.2. nk G.3. n1 (C \V ) (0 V ).(a) n1 C \ V , hn2 , n1 E \ E n1 would obtain nonnegative surplus pobtaining n2 . 1 = k 1, n1 would obtain strictly positive surplus pobtaining n2 .(b) n1 0 V , hn1 , n2 E n1 would obtain nonpositive surplus pactive. k = 2, n1 would obtain strictly negative surplus pactive.4. 2, ni ni 1 (C V ).(a) ni 1 V , hni1 , ni E , hni , ni+1 E , ni would obtain nonpositivesurplus p active. = k 1, ni would obtain strictly negative surplusp active.(b) ni 1 \ V , hni+1 , ni E , hni , ni1 E , ni would obtain nonnegativesurplus p active. = k 1, ni would obtain strictly positive surplusp active.(c) ni C V , hni1 , ni E , hni+1 , ni E \E , ni would obtain less surplusni+1 ni1 p. = k 1, ni would obtain strictly better surplusni+1 ni1 .open reverse-surplus sequence closed, reverse-surplus sequence exceptthat, instead Condition 3, n1 G n2 1 (C V ) Condition 4. Clearlyclosed, reverse-surplus sequence length greater two contains open, reverse-surplussequence.Lemma 18 Procedure Input Complementarities Equilibrium reach stateopen, reverse-surplus sequence K = (n1 , . . . , nk ) constituting cycle n1 = nkk 3.Proof. Assume, contrary wish prove, cycle K prices p.Moreover, let cycle smallest, contains cycle.show create alternate, feasible solution (V 0 , E 0 ) higher value (V , E ),giving us contradiction. Initialize (V 0 , E 0 ) = (V , E ). n j , 1 j < k, hn j , n j+1E , remove edge E 0 , edge E \ E , add edge E 0 . Also, add removevertices necessary consistent added removed edges.producer (V 0 , E 0 ) feasible feasible (V , E ) input,either input output added, removed, neither changed. Consider goodn j G, 1 < j < k. Since j 1 > 1, must agents n j1 n j+1 1 (C V ).inspecting Conditions 4(a)4(b) definition closed, reverse-surplus sequence (whichalso apply open reverse-surplus sequence), see edges incident n j addedremoved way n j material balance. Similarly, considering agents nk1 , n2 ,546fiD ECENTRALIZED UPPLY C HAIN F ORMATIONgood n1 = nk , material balance good n1 = nk . Since goods material balanceproducers feasible, (V 0 , E 0 ) feasible.surpluses agents K unaffected transformation. definition open,reverse-surplus sequence, every agent K obtains lower surplus p transformation,agent nk1 obtains strictly higher surplus p. value feasible allocationsum agent surpluses particular prices (Lemma 22), must value((V 0 , E 0 )) >value((V , E )). contradicts optimality (V , E ), assumption K existsmust false. 2Lemma 19 price good nk increases Procedure Input ComplementaritiesEquilibrium, exists finite closed, reverse-surplus sequence (n1 , . . . , nk ) pricesp price increase.Proof.show construct desired closed, reverse-surplus sequence, referringconditions definition, steps Procedure Input ComplementaritiesEquilibrium. price increase nk occurred one Steps 2(a)2(e), triggered agentnk1 . Since step triggered, nk1 would obtain strictly better surplus reversing allocation p, specified conditions 3(a), 3(b), 4(a)4(c). price nk increased Step 2(a) 2(c), desired closed, reverse-surplus sequence,nk1 (C \V ) (0 V ) k 1 = 1. Otherwise, price nk increased Step 2(b),2(d), 2(e), nk1 1 (C V ) k 1 > 2. case, let nk2 good alsomatched condition step.price nk2 increased, Procedure Input Complementarities Equilibriumensures find agent nk3 matching one Conditions 3(a), 3(b), 4(a)4(c). If,hand, p(nk2 ) = 0, producers positive costs consumers positivevalues, also find agent nk3 . find agent corresponds condition 3(a)3(b), k 3 = 1 done. Otherwise, find good nk4 , nk2 ,continue manner.Clearly, process constructs open, reverse-surplus sequence. Now, must showprocess selecting vertices eventually selects element n1 (C \V ) (0 V ). Since (V, E)finite, since Lemma 18 cycles open, reverse-surplus sequence,must eventually find n1 (C \V ) (0 V ) give us closed, reverse-surplus sequence. 2Lemma 20 Procedure Input Complementarities Equilibrium terminates.Proof. Assume, contrary wish prove, procedure terminateprice good g increases infinite number times. Consider cycle K = (n1 = g, . . . , nk = g)vertices ni V , k 3 that:1. hni , ni+1 E hni+1 , ni E {1, . . . , k 1}.2. {2, . . . , k 1}, ni 6= g.3. {3, . . . , k}, ni G, price increase good ni occurred one Steps 2(b),2(d), 2(e) procedure, agent ni1 good ni2 also matched conditionstep. Furthermore, price increase ni2 , triggered agent ni3 good ni4 , causedneed price increase good ni .547fiWALSH & W ELLMANprice g increases infinite number times, cycle must exist.Let p prices p(n1 ) n1 agent n2 triggered price increasen3 , ni G 1 < < k, p(ni ) increased, triggeredagent ni1 good ni2 . price goods arbitrary nonnegative number.way constructed p, way prices increased procedure, K mustopen, reverse-surplus sequence. Lemma 18, K cannot exist. Therefore,procedure terminates. 2Theorem 1 Competitive equilibria exist network input complementarities.Proof. show Procedure Input Complementarities Equilibrium terminatesprices p every agent obtaining maximum surplus according (V , E ) . Since (V , E )efficient, also feasible, giving us competitive equilibrium prices p.Lemma 20, procedure terminates. Clearly, procedure terminates, agents1 (0 V ) (C \ V ) optimize according (V , E ). remains show (CV ) (0 \V ). Assume, contrary wish prove, (C V ) (0 \V )optimize according (V , E ).Consider case (C V ) hg, ai E . Since algorithm guaranteesprefer good g0 g prices p, must p(g) > vc (g). Let p0prices immediately price g rose vc (g) p00 prices immediately after.Lemma 19, closed, reverse-surplus sequence (n1 , . . . , nk = g) prices p0 . p00 ,conditions closed, reverse-surplus sequence hold, except surplus condition 4(a),4(b), 4(c) applies nk1 becomes non-strict. However, obtains strictly negative surplusp00 . Denote nk+1 .create alternate, feasible solution (V 0 , E 0 ) proof Lemma 18 addingedges hni , ni+1 E \ E , removing edges E , {1, . . . , k}.surpluses agents K unaffected transformation. Every agent (a1 , . . . , nk1 )obtains lower surplus p00 transformation. Agent = nk obtains zero surplustransformation, higher negative surplus before. valuefeasible allocation sum agent surpluses particular prices (Lemma 22), mustvalue((V 0 , E 0 )) > value((V , E )). contradicts optimality (V , E ), mustp(g) vc (g) obtaining maximum surplus p (V , E ).If, hand, (0 \V ), ha, gi E. must < p(g). useline proof case C V show (V , E ) suboptimal value, providingcontradiction. Thus must optimize according (V , E ) p.Thus shown algorithm terminates agents optimizing according(V , E ) p. Thus p supports competitive equilibrium allocation (V , E ). 2A.2 Proof Theorem 2Given polytree (V, E) efficient allocation (V , E ), present Procedure PolytreeEquilibrium constructs lower bounds p (g) upper bounds p (g) pricesgoods g, turn uses bounds construct prices p goods. proveresulting prices fact competitive equilibrium prices support (V , E ).548fiD ECENTRALIZED UPPLY C HAIN F ORMATIONObserve that, purposes competitive equilibrium pricing, treat consumer cwishes obtain one good set Gc consumer desires single good gcvalue vc (gc ) = vc = maxgGc vc (g), along additional producers. g Gc createproducer output gc , input g, = vc vc (g). Thus, without loss generality,consider consumers preferences single goods. denote gc good consumerc desires denote vc value c gc .refer n0 V either hn, n0 E hn0 , ni E neighbors vertexn V . use refer null vertex neighbor vertex.Polytree Equilibrium:1. g G, p (g) 0 p (g) .2. connected subgraph (V , E) (V, E), select g G V arbitrarily:Perform Set Bounds(g, ).p(g) p (g).Set Bounds recursively visits vertices, updating price bounds postorder (i.e.,recursion unwinds) setting prices either lower upper bounds. (V, E)polytree, procedure sets price good exactly once.Set Bounds(n, r), n A, r G procedure either updates p (r) p (r)bounds neighbors n, r, fixed. updates p (r),way n/ V n, active, would get nonpositive surplus p(r) p (r),given bounds neighbors n, n V n, active, would get nonnegative surplus p(r) p (r), given bounds neighbors n. Since p (r)increases (Steps 2, 4(b), 5(c)), property maintained. Similarly, Set Bounds(n, r)updates p (r), way n/ V n, active, would get nonpositivesurplus p(r) p (r), given bounds neighbors n, n Vn, active, would get nonnegative surplus p(r) p (r), given boundsneighbors n. Since p (r) decreases (Steps 3, 4(c), 5(b)), property maintained.Set Bounds(n, r):1. neighbor z n z 6= r, perform Set Bounds(z, n).2. n C \V ,p (r) max(vn , p (r)).3. Else n C V ,p (r) min(vn , p (r)).4. Else n \V then,(a) neighbor g n g 6= rg input np(g) p (g).Else g output n,p(g) p (g).549fiWALSH & W ELLMAN(b) r input n,output, gn , np (r) max(p (r), p (gn ) hg,niE, g6=r p (g) n ).(c) Else r output n,p (r) min(p (r), hg,niE p (g) + n ).5. Else n V then,(a) neighbor g n g 6= r,g input n,p(g) p (g).g output n,p(g) p (g).(b) r input n,output, gn , n),p (r) min(p (r), p (gn ) hg,niE, g6=r p (g) n ).(c) Else r output n,p (r) max(p (r), hg,niE p (g) + n )Lemma 21 Procedure Polytree Equilibrium computes price bounds p (g) p (g)goods g G.Proof. Assume, contrary wish prove, state g Gp (g) > p (g). Assume g first good visited.say agent constrained p (g) Set Bounds(a, g) last change p (g).Similarly, say agent constrained p (g) Set Bounds(a, g) last changep (g).Recall Lemma 22 value feasible allocation equal sum agentsurpluses particular prices. show transform (V , E ) alternate feasibleallocation (V 0 , E 0 ) compute alternate prices p show sum surpluses (V 0 , E 0 )greater (V , E ).First, initialize (V 0 , E 0 ) = (V , E ) good g G initialize p(g) = 0. Next, set p(g) =p (g). recursively change prices allocation portion subtree rootedg. Perform Lower Bound(a, g) agent constrained p (g) perform UpperBound(a, g) agent constrained p (g).Throughout transformation, perform Lower Bound(a, g) iff visit g agentconstrained p (g). Similarly, perform Upper Bound(a, g) iff visit g agent constrained p (g). following describes portions transformation.Lower Bound(a, g):1. \V , must g input (because constrained p (g)).neighbor g0 6= g a:550fiD ECENTRALIZED UPPLY C HAIN F ORMATION(a) g0 input a,p(g0 ) p (g0 ),perform Upper Bound(a0 , g0 ) agent a0 constrained p (g0 ).(b) Else (g0 output a),p(g0 ) p (g0 ),perform Lower Bound(a0 , g0 ) agent a0 constrained p (g0 ).2. Else V , must g output (because constrained p (g)).input g0 a:p(g0 ) p (g0 ),perform Lower Bound(a0 , g0 ) agent a0 constrained p (g0 ).3. V ,remove incident edges (V 0 , E 0 ).4. Else V \V ,add incident edges (V 0 , E 0 ).Upper Bound(a, g):1. \V , must g output (because constrained p (g)).input g0 a:p(g0 ) p (g0 ),perform Upper Bound(a0 , g0 ) agent a0 constrained p (g0 ).2. V , must g input (because constrained p (g)).neighbor g0 6= g a:(a) g0 input a,p(g0 ) p (g0 ),perform Lower Bound(a0 , g0 ) agent a0 constrained p (g0 ).(b) Else (g0 output a),p(g0 ) p (g0 ),perform Upper Bound(a0 , g0 ) agent a0 constrained p (g0 ).3. V ,remove incident edges (V 0 , E 0 ).4. Else V \V ,add incident edges (V 0 , E 0 ).Observe that, (V, E) polytree, vertex visited either UpperBound Lower Bound.show (V 0 , E 0 ) feasible. Consumers always feasible. Producers feasibleadd remove incident edges add remove producer, respectively.prove every g G material balance (V 0 , E 0 ).551fiWALSH & W ELLMANConsider good g p (g) > p (g). Lower Bound(a, g) performed agentconstrained p (g), occurred 2, 4(b), 5(c) Set Bounds(a, g). Therefore LowerBound(a, g) either adds hg, ai E \ E else removes ha, gi E . Upper Bound(a, g)performed constrained p (g), occurred 3, 4(c), 5(b) Set Bounds(a, g).Therefore Upper Bound(a, g) either adds ha, gi E \ E else removes hg, ai E .possible combination, material balance maintained g.consider good g 6= g. g visited Lower Bound(a, g), p(g) setp (g) one following ways, immediately prior:1. p(g) set p (g) 1(b) Lower Bound(a, g), agent \Vgood g. case, g output ha, gi E \ E added (V 0 , E 0 ) 4Lower Bound(a, g).2. p(g) set p (g) 2 Lower Bound(a, g), agent Vgood g. case g input hg, ai E removed (V 0 , E 0 ) 3Lower Bound(a, g).3. p(g) set p (g) 2(a) Upper Bound(a, g), agent Vgood g. case case g input hg, ai E removed (V 0 , E 0 )3 Upper Bound(a, g).One following operations occurred Lower Bound(a, g):1. constrained p (g) 2 4(b) Set Bounds(a, g), hg, ai E \ E added(V 0 , E 0 ) 4 Lower Bound(a, g).2. agent constrained p (g) 5(c) Set Bounds(a, g), ha, gi E removed(V 0 , E 0 ) 3 Lower Bound(a, g).possible combination additions removals edges incident g prior to, LowerBound(a, g), material balance maintained g. show similar result g visitedUpper Bound(a, g). Hence established feasibility (V 0 , E 0 ).show agent A, (a, (V 0 , E 0 ), p) (a, (V , E ), p),agent a0 (a0 , (V 0 , E 0 ), p) > (a0 , (V , E ), p).agent visited construction (V 0 , E 0 ), (a, (V 0 , E 0 ), p) = (a, (V , E ), p),allocation (V , E ).Consider a0 visited Upper Bound(a0 , g). a0 thus visited, a0 constrainedp (g). Upper Bound(a0 , g) sets prices neighbor goods g 6= g prices usedcompute p (g) Set Bounds(a0 , g). prices neighboring goods computeda0 V , a0 would get negative surplus price g p (g) a0 V \Vwould get positive surplus price g p (g). But, alternate pricescomputed, p(g) = p (g), assume p (g) > p (g). Since a0 V 0V , (a0 , (V 0 , E 0 ), p) > (a0 , (V , E ), p).consider A, 6= a0 , visited construction (V 0 , E 0 ). visitedUpper Bound(a, g), p(g) = p (g) must constrained p (g). C,Set Bounds(a, g) set p (g) va p (g) = 0. , Upper Bound(a, g) setsprices goods neighboring prices used compute p (g) Set Bounds(a, g).neighboring prices active feasible, would get zero surplus552fiD ECENTRALIZED UPPLY C HAIN F ORMATIONp(g) = p (p). Thus (a, (V 0 , E 0 ), p) = (a, (V , E ), p). If, hand, visited LowerBound, p(g) = p (g) must constrained p (g). similar argument usedUpper Bound, gets zero surplus p(g) = p (g). Again, gives (a, (V 0 , E 0 ), p) =(a, (V , E ), p).shown agent A, (a, (V 0 , E 0 ), p) (a, (V , E ), p),agent a0 (a0 , (V 0 , E 0 ), p) > (a0 , (V , E ), p). value((V 0 , E 0 )) >value((V , E )), contradiction. Hence, initial assumption p (g) > p (g) mustfalse. p (g) p (g) goods g.2Theorem 2 Competitive equilibria exist polytree.Proof. show agents optimize according (V , E ) prices p computed procedure Polytree Equilibrium. Since (V , E ) feasible definition, resulting pricesallocation constitute competitive equilibrium (V, E).construction p (g) ensures never decreases, Step 2 Set Bounds ensures every consumer c C \V optimizes p(gc ) p (gc ). p (gc ) p(c) p (gc )(by construction p Lemma 21), c optimizes according (V , E ). similar argument, every c C V optimizes according (V , E ).Consider producer \ V , visited Set Bounds(, g). good g input ,4(a) Set Bounds(, g) sets price every neighbor good g0 6= g pricebounds used compute p (g) Step 4(b) Set Bounds(, g). Moreover, p (g) setsmallest price could get maximum surplus zero, given specified boundsneighbor goods. Since p (g) could increase subsequently, since p (g) p(g) p(g)(by construction p Lemma 21), since price good set(because (V, E) polytree) cannot get positive surplus prices set Set Bounds(,g). similar argument, g output , Step 4(c) p (g) set largest pricewould get maximum surplus zero, given prices set neighbor goods.Since again, p (g) p(g) p (g), p (g) increases subsequently, price goodset once, must get zero surplus. Thus optimizes according(V , E ). Symmetrically, see every V optimizes according (V , E ).shown agents optimize according (V , E ) p, hence showncompetitive equilibrium exists polytree (V, E). 2A.3 Proof Theorem 3Lemma 22 value feasible allocation (V 0 , E 0 ), prices p, expressed as:value((V 0 , E 0 )) =(a, (V 0, E 0), p).aA553(1)fiWALSH & W ELLMANProof. Equation (1) expands to:value((V 0 , E 0 )) =vc ((V 0 , E 0 ))+p(g)p(g) ((V , E )) .hg,ciE 0cCp(g)h,giE 0!0hg,iE 00!Since goods material balance feasible allocation, price terms cancel out.leftvc ((V 0, E 0)) ((V 0 , E 0)),cCoriginal formula value solution (Definition 1). 2Theorem 3 (V 0 , E 0 ) --competitive equilibrium (V, E) prices p, (V 0 , E 0 )feasible allocation nonnegative value differs value efficient allocation[hg,iE g + ] + |C|b .Proof. refer four conditions --competitive equilibrium (Definition 4). Let (V , E )efficient allocation (V, E).Conditions (3) (4) imply (V 0 , E 0 ) feasible. Recall formula valuefeasible allocation Equation (1). Since (V 0 , E 0 ) (V , E ) feasible, expressvaluesvalue((V 0 , E 0 )) =(a, (V 0 , E 0), p),(2)(a, (V , E ), p).(3)aAvalue((V , E )) =aAc C, Condition (2), (c, (V 0 , E 0 ), p) Hc (p) b . allocationbetter agent optimal allocation, (c, (V , E ), p) Hc (p). Thus,(c, (V , E ), p) (c, (V 0 , E 0 ), p) b .(4), Condition (3), (, (V 0 , E 0 ), p) H (p) (hg,iE g + ).allocation better agent optimal allocation, (, (V , E ), p) H (p). Thus,(, (V , E ), p) (, (V 0 , E 0 ), p)g + .(5)hg,iEEquations (2)(5) together imply value((V , E )) value((V 0 , E 0 )) [hg,iE +] + |C|b . Condition (1) implies sum term Equation (2) nonnegative, hencevalue((V 0 , E 0 )) 0. noted, (V 0 , E 0 ) feasible.2g554fiD ECENTRALIZED UPPLY C HAIN F ORMATIONA.4 Proof Theorem 5proving theorem, refer C-level S-level producers network, definedbeginning Section A.task dependency network (V, E) characterized following parameters:: maximum C-level producer network,: maximum number input goods producer,R: maximum consumer value, maxhg,ciE|cC vc (g).Lemma 23 run SAMP-SB network (V, E), agent places buy offer R + 2b .Proof. Consumers never offer valuation, bounded R. prove inductionproducer C-level producer C-level k places buy offer R + 2kb .Suppose producer C-level one places offer buy input g price > R + 2b .Since always increments buy offers b , means previous time submitted buyoffer g price R + b < 0 R + 2b . time, must losing g, else wouldbidding. ask quote g must greater R, offered greaterR output. Since consumer offer buy output producer C-levelone, must lose output. offers nondecreasing, situation permanent, hencenever raises input offer, contrary supposition. Thus C-level one producernever place buy offer R + 2b .inductive step, assume producer C-level i, < k, places buyoffer R + 2ib . Thus, producer C-level k win output offer R +2(k 1)b . Applying reasoning analogous base case (C-level one), see producerC-level k places buy offer R + 2kb . k producers, lemma followsimmediately. 2Lemma 24 agent places (R + 2b )/b + buy offers.Proof. Since consumers offer R increase offers least b , place offersR/b times. producer initially places buy offers inputs. AccordingLemma 23 producer bidding policy, producer subsequently offers higher R + 2bincrements b maximum inputs. 2Theorem 5 SAMP-SB reaches quiescence finite number bids placed.Proof. Lemma 24, finite number buy offers placed. need show producers place finite number sell (output) offers establish finite number total bidsplaced.producer change output offer if: 1) price input changes, 2) askprice input changes, 2) loses offer good previously winning.unchanged input offer switch winning losing without pricechanging. Similarly, unchanged input offer switch winning state without askprice changing. Hence, sufficient show price ask price producers555fiWALSH & W ELLMANinput goods change finite number times. prove induction producer S-levelprice ask quotes input good producer S-level k changes finite numbertimes.producer input places output offer input good g producer S-levelone, producers input place one offer each. Hence, price ask price gchange response change buy offer g. Lemma 24, number buy offerchanges g finite.assume producers S-levels less k place finite number output offers.good g input producer S-level k, number output offer changesfinite. number input offers g must finite. Since number input outputoffers g finite, places finite number output offers. 2A.5 Proof Theorem 8proving theorem, refer C-level producers network, defined beginningSection A. reference, quasi-quiescence described Definition 5.Lemma 25 run SAMP-SB quasi-quiescent state time interval [t, 0 ]inactive producer changes offer input good time interval [t, 0 + ],smallest period time agent requires update bid response price quote.Proof. definition quasi-quiescence, interval [t, 0 ], consumer active producerchanges offer. Thus, simple induction C-level inactive producers showsproducer inactive time would win output [t,t 0 ], hence inactive producersremain inactive interval. producer inactive [t, 0 ] would changeinput offer [t, 0 + ]. 2Lemma 26 run SAMP-SB quasi-quiescent time interval [t, 0 ],quasi-quiescent time interval [t, 0 + ], smallest period time agentrequires update bid response price quote.Proof. Assume, contrary wish prove, run SAMP-SB quasi-quiescent[t, 0 ] time [t 0 ,t 0 + ]. Let consumer active producer change offer[t 0 , 0 + ].consumer, would change offer lost offer previouslywinning quasi-quiescence. producer, must feasible, otherwise would changeinput offer (because active) violating quasi-quiescence. Since feasible, would changeoffer loses input previously winning, price one inputs increases.cases, either loses buy offer previously winning, price onebuy offers increased. one occur, must time 00 [t,t 0 ], agenteither 1) changed winning output offer 2) changed input offer. definitionquasi-quiescence precludes #1, Lemma 25 definition quasi-quiescence preclude #2.gives us contradiction, proving lemma. 2Lemma 27 run SAMP-SB quasi-quiescent state time t, quasi-quiescenttimes 0 > t.556fiD ECENTRALIZED UPPLY C HAIN F ORMATIONProof. Lemma 26 conclude quasi-quiescence interval [t, + ],extend interval , indefinitely. 2Lemma 28 run SAMP-SB quasi-quiescent time t, producer inactivetime inactive time + , producer active time also inactive time + ,smallest period time agent requires update bid response price quote.Furthermore, p(g) change good g time +Proof. Since agent cannot lower offers, way inactive producer becomeactive agent raise buy offer. Lemma 27 definition quasiquiescence, inactive producers change offers t, Lemma 25 inactiveproducer change input offers. remains inactive.Since offers decrease, active producer become inactive increasingoffer output. prices inputs increase. Sincequasi-quiescent state, happen inactive producer 0 changes offer outputg. since 0 inactive, change offer g cause (g) change. Since activeproducers feasible (otherwise would want change bids, violating quasi-quiescence),losing buy offer g time t. Therefore, respond changes (g), hencechange output offer remain active. 2Theorem 8 run SAMP-SB reaches quasi-quiescent state, remains quasiquiescent state. Furthermore, neither allocation prices p subsequently change.Proof. theorem follows directly Lemmas 27 28. 2A.6 Proof Theorem 10proving theorem, refer C-level producers network, defined beginningSection A.given run SAMP-SB network (V, E) characterized following parameters:: maximum C-level producer network,: maximum number input goods producer,R: maximum consumer value, maxhg,ciE|cC vc (g).Theorem 10 SAMP-SB reaches quasi-quiescent state number bids bounded polynomial size network value maximum consumer value placedconsumers active producers.Proof. SAMP-SB guaranteed reach quasi-quiescent state (Theorem 5 Observation 7).Lemma 24, number buy offers bounded polynomial value R, henceneed concerned number sell offers placed. Since prices buy offers increaseleast b , producers perceived cost good must rise least b , increase557fiWALSH & W ELLMANsell offer less b . Also, producer increase sell offer less , required auction. Hence, Lemma 23 implies active producer become permanentlyinactive places (R + 2b )/[max(b , )] output offers. 2A.7 Proof Theorem 11proving theorem, refer conditions --competitive equilibrium (Definition 4).Lemma 29 SAMP-SB reaches quiescence network (V, E) consumer obeys--competitive equilibrium conditions (Conditions (1) (2)).Proof. Since consumer maintains single winning offer good gives nonnegative surplus, obeys Condition (1).Let final prices allocation p (V 0 , E 0 ), respectively. Assume, contrary Condition (2), (c, (V 0 , E 0 ), p) < Hc (p) b consumer c. Let g surplus-maximizinggood c p.c buy good, p(g ) + b < vc (g ) (otherwise would placedoffer g ) (c, (V 0 , E 0 ), p) = 0. Noting also Hc (p) = vc (g ) p(g ), algebraic manipulation gives us (c, (V 0 , E 0 ), p) > Hc (p) b , contradiction.Thus, c buys one good g0vc (g0 ) p(g0 ) < vc (g ) p(g ) b .(6)Let p(g ) p(g0 ) prices g g0 c placed final offer g0 . Since c offersp(g0 ) + b g0 , since c offer p(g0 ),p(g0 ) + b p(g0 ).(7)p(g ) p(g ).(8)Since prices decrease,Substituting Equations (7) (8) left right sides, respectively, Equation (6) gives usvc (g0 ) ( p(g0 ) + b ) < vc (g ) ( p(g ) + b ).consumer bidding policy specifies c would bid g , rather g0 prices p,contradiction. Thus consumer obeys Condition (2). 2Lemma 30 SAMP-SB reaches quiescence network (V, E) inactive producer buyspositive-price input, producer obeys --competitive equilibrium conditions (Congditions (1) (3)), = max((g) p(g), b ).Proof. bidding policy ensures producer sells output g nonnegativesurplus, lemma conditions directly imply zero surplus sell . Thusobeys Condition (1).producer bidding policy guarantees feasible quiescence.558fiD ECENTRALIZED UPPLY C HAIN F ORMATIONLet final prices p allocation (V 0 , E 0 ). H (p) > hg,iE + quiescence,H (p) = p(g ) hg,iE p(g) > hg,iE g + . Thus p(g ) > hg,iE p(g) + hg,iE g +. producer bidding policy ensures offers hg,iE p(g) + hg,iE g +g , must winning g profit. Thus (, (V 0 , E 0 ), p) = H (p) Condition (3) holds.instead H (p) hg,iE g + , since (, (V 0 , E 0 ), p) 0, Condition (3) holds. 2gTheorem 11 prices allocation determined quiescence SAMP-SB protocol--competitive equilibrium, g = max((g) p(g), b ), iff inactive producer buyspositive-price input.Proof. Case if: Condition (1) --competitive equilibrium (Definition 4) fails inactive producer buys positive-price input.Case if: Lemmas 29 30 show consumers producers, respectively, obey -competitive equilibrium conditions (Conditions (1)(3)). (M+1)st-price auction rules ensureCondition (4). conditions --competitive equilibrium met. 2A.8 Proof Theorem 12proving theorem, refer conditions --competitive equilibrium (Definition 4).Lemma 31 (g) p(g) > b good g quiescent state SAMP-SB network (V, E),agent wins offer g.Proof. Assume, contrary wish prove, (g) p(g) > b agentwinning offer g, quiescence SAMP-SB. Either buy offer sell offer sets (g).Case 1: agent sets (g) buy offer. According SAMP-SB bidding policies,agent increase buy offer losing offer. agent win offer gprice p(g). producer increases buy offer increments b consumer offersp(g) + b . either case, agent place buy offer higher p(g) + b g.(g) p(g) + b , contradiction.Case 2: agent sets (g) sell offer. Case 1, buy offers higherp(g) + b , hence every buy offer strictly (g). Recall that, sell offers,Mth highest offer determines (g). since buy offers (g), mustsell offers (g). sell offers strictly buy offersagent wins offer g, contradiction.Since case gives us contradiction, must case agent wins offer g(g) p(g) > b . 2Lemma 32 (V 0 , E 0 ) --competitive equilibrium prices p, quiescence SAMP-SBnetwork (V, E), exist prices p0 (V 0 , E 0 ) also --competitive equilibriump0 , g = b producers goods g.Proof. specify p0 follows: (g) > p(g) + b , p0 (g) = (g), otherwise p0 (g) = p(g).show conditions --competitive equilibrium hold g = b .considering allocation, goods still material balance Condition 4 still holdsp0 .559fiWALSH & W ELLMANConsider agent p0 (g) = p(g) adjacent goods g. Clearly, Ha (p0 ) = Ha (p)(a, (V 0 , E 0 ), p0 ) = (a, (V 0 , E 0 ), p). (a, (V 0 , E 0 ), p0 ) 0 (Condition 1),surplus bound met consumers (Condition 2) since hold p. producer,input g, (g) p(g) b since p0 (g) = p(g). Hence, following boundperceived cost g: pa (g) p(g) + b . result, producer bidding policies imply(a, (V 0 , E 0 ), p) H (p) (hg,iE b + ). Therefore (a, (V 0 , E 0 ), p0 ) H (p0 ) (hg,aiE ga +0) producer surplus bound (Condition 3) holds ga inputs g0 .consider agent adjacent good g p0 (g) = (g). Lemma 31, winoffer g, (a, (V 0 , E 0 ), p0 ) = (a, (V 0 , E 0 ), p), implying (a, (V 0 , E 0 ), p0 ) 0 (Condition 1).consumer, since p0 (g) p(g), since win g, Ha (p0 ) = Ha (p),surplus bound met consumers (Condition 2).producer, since win g, must good (according--competitive equilibrium conditions Theorem 11), implying (a, (V 0 , E 0 ), p0 ) = 0.producer bidding policy specifies offered price = + hg0 ,aiE max((g0 ), p(g0 )+b ) + output ga . Since win ga , must (ga ) . wayp0 constructed, p0 (ga ) (ga ) p0 (g0 ) + b max((g0 ), p(g0 ) + b ), giving us p0 (ga )(ga ) + hg0 ,aiE (p0 (g0 ) + b ) + . would optimize p0 active,Ha (p0 ) = p0 (ga ) hg0 ,aiE p0 (g0 ) hg0 ,aiE b + . since (a, (V 0 , E 0 ), p0 ) = 0 follows(a, (V 0 , E 0 ), p0 ) Ha (p0 ) (hg0 ,aiE b + ). If, hand, would optimize p0inactive at, (a, (V 0 , E 0 ), p0 ) = Ha (p0 ). either case, surplus bound met producers0(Condition 3) ga = b inputs g0 . 2Theorem 12 (V 0 , E 0 ) --competitive equilibrium computed SAMP-SB (V 0 , E 0 )nonnegative value differs value efficient allocation (|{hg,E}| b + ) + |C|b .Proof. Lemma 32, --competitive equilibrium (V 0 , E 0 ) = b progducers goods g. b substituted equation Theorem 3, provedpresent theorem. 2gA.9 Proof Theorem 13proving theorem, refer S-level producers network, defined beginningSection A.Theorem 13 quiescent state SAMP-SB --competitive equilibrium tree.Proof. prove, induction S-level producers, producer changes initialoutput offer. Since buy offers never decrease, follows that, producer winning output,lose output successive state run protocol. Since producer bidsinputs winning output, inactive producer buy positive-price outputpresent theorem follows Theorem 11.Basis case: bidding policy specifies producer S-level zero never changes initialoutput offer.560fiD ECENTRALIZED UPPLY C HAIN F ORMATIONInductive case: Assume producer S-level less k changes initial output offershow producer S-level k never changes initial output offer. Consider input good gsell offers lowest sell offer . Since network tree, agent placesbuy offers g. Producer initially offers zero g, long offers less losesoffer, p (g) = (g). holds, (g), defined Mth highest price, lowestsell offer, hence p (g) = . soon offers greater g win offer,p (g) = p(g). holds, p(g), defined + 1st highest price, lowest sell offer,hence p (g) = . conclude p (g) never changes input g, hence never changesinitial output offer.proven producer changes initial output offer, argument above,theorem proven. 2A.10 Proof Theorem 14Theorem 14 quiescent state safe SAMP-SB --competitive equilibrium networkinput complementarities.Proof. show inactive producer buys input positive price quiescencesafe SAMP-SB. Since properties safe SAMP-SB quiescence SAMP-SB,present theorem follows Theorem 11. Assume, contrary, that, quiescence,producer wins input g positive price loses offer output g .Let price final offer g, p(g) > 0 final price g, p (g)final perceived cost g . Since wins g quiescence, p (g) = p(g). Let 0 pricesecond last offer . Immediately places offer , let p0 (g) perceived priceg p0 (g) price component price quote g. According biddingpolicy, = 0 + b . Since offers loses g offer 0 , must 0 p0 (g), hencep0 (g) + b . Furthermore, since loses offer 0 , p0 (g) p0 (g) + b .assume wins g quiescence, must p(g) , hence p(g) p0 (g) + b . followsthat, since p (g) = p(g) p0 (g) p0 (g) + b , p (g) p0 (g).According safe SAMP-SB bidding policies, offers g first winning goffer price p0 (g). Since p (g) p0 (g), offer g quiescenceplaced g. since offers agent decrease, must continue win final offerg , contradicting assumption loses g quiescence. Thus, win inputpositive price inactive, quiescent state safe SAMP-SB --competitiveequilibrium. 2A.11 Proof Theorem 15proving theorem, refer C-level S-level producers network, definedbeginning Section A.Theorem 15 (V, E) polytree solution assigns good g consumer c, givencosts values, exists value vc (g) SAMP-SB guaranteed convergevalid solution (V 0 , E 0 ) c.561fiWALSH & W ELLMANProof. convenience, denote max V 0 , maxc0 C, hg0 ,c0 i6=hg,ci vc0 (g0 ) . showtheorem holds for:vc (g) = [ + (2b + )||] || + b .need show SAMP-SB cannot reach state p(g) > vc (g)b c winningg, c would stop bidding g desired solution would form.First, observe consumer c0 good g0 hg0 , ai =6 hg, ci, c0 offer0g , construction.Now, consider producer directed path g output. show, induction C-level producers, producer offers higher+ b , C-level , one inputs. basis case, producerC-level one cannot win output offer (by definition ). increases input offersincrements b , offer 0 > + b , input g0 , must first offer , < + binput. offer 0 losing winning output offer. losing, must p(g0 ) , must offering output. cannotwinning output, hence would offer 0 g. Thus C-level one offer+ b input, establishing base case. Now, assume property holds everyproducer C-level less k show holds producer C-level k. Given inductiveassumption, must cannot win output + b (k 1). argumentsimilar basis case, offer + b k input, proving inductive case.Since ||, producer offers higher + b || input.producer V 0 , denote maximum number producers, ,subgraph (V, E), rooted . show induction S-level, producerdirected path g offers [ + b (|| + ) + (d 1)]I + output,S-level . basis case, consider producer S-level one, offering buyg0 . consumer offers g0 . (V, E) polytree, producer 0offers buy g0 directed path g, hence offers + b || buy g0 .producer offers sell g0 must inputs, hence offers g0 . Hence 0successfully buy g0 offer higher + b (|| + 1), thus offer higheramount g0 . Since number inputs equal , offer( + b (|| + 1))I + output, basis case proven. Now, assume propertyholds every producer S-level less k prove holds producer offeringbuy g0 S-level k. inductive assumption, producer offers sell g0( + b (|| + k 1) + (k 2))I + . basis case, consumer offersg0 producer offer + b || buy g0 . Hence, offer( + b (|| + k 1) + (k 2))I + + b buy g0 , output offer"h,g0 ipE | hg0 ,iE#( + b (|| + k 1) + (k 2))I + + b +[ + b (|| + k) + s(k 1)]I + ,proving inductive case. Since || ||, producer offers higher[ + 2b || + (|| 1)]|| + [ + (2b + )||] || = vc (g) b .shown agent 6= c places buy offer high vc (g) b g producerdirected path g places sell offer high vc (g) b g. Hence c agent562fiD ECENTRALIZED UPPLY C HAIN F ORMATIONcould possibly offer high vc (g) b g. c offer high, necessary, win g,win g offers vc (g) b higher. follows c win g price vc (g)quiescence. Observation 7 Theorem 16, state must valid solution. 2A.12 Proof Theorem 16Theorem 16 SAMP-SB reaches quasi-quiescence p(g) < vc (g) hg, ci E, c C,systems state represents valid solution.Proof. definition quasi-quiescence requires active producers changebids, must feasible. agents feasible definition. price activeproducers output good must less total price input goods, otherwise wouldincrease output offer, violating quasi-quiescence.p(g) < vc (g), consumer c must offer g. consumer bids waywins one unit one good, consumers change bids quasi-quiescence.Finally, auction guarantees one-to-one mapping successful buy offerssuccessful sell offers good, ensuring material balance.Thus, constraints valid solution satisfied. 2A.13 Proof Theorem 17Theorem 17 run SAMP-SB (V, E) valid solution state that:consumer c either winning offer p(g) + b > vc (g) hg, ci E,agents correct beliefs goods currently winning,bids consumers active producers received response currentprice quotes,sell offers lost due tie breaking,subsequent price quote auction, system quasi-quiescentstate valid solution.Proof. Let current prices p. consumer bidding policy dictates consumerschange offers specified conditions. valid solution, producer feasible thus raise buy offers inputs. Therefore, agent changesbuy offers.active producer feasible valid solution. Since winning inputs,raises offer output g p(g) changes inputs g, place offeroutput price higher sum input good prices. definition valid solution,active, current price output good less sum current pricesinputs. since offer g , must offered higher p(g ) g .previous offer price g higher p(g ), sum currentprices inputs higher p(g), offer higher p(g ) g .563fiWALSH & W ELLMANestablished agent changes buy offers, currently active producerplaces sell offer p(g) good g. show implies that, next price quotesprices p0 , p0 (g) = p(g). Assume contrary. Since offers decrease, p0 (g) > p(g).Since buy offer winning sell offer changed, price increase due updated losingsell offer price , = p0 (g). agent losing previous offer price0 , must 0 least high (M + 1)st highest offer. Thus , higher,must strictly higher (M + 1)st highest offer, hence cannot raise price g. Hencep0 (g) = p(g).Since prices change, temporal-precedence tie breaking ensures set winningbuy offers change. Additionally, since winning seller offers p(g) selloffers currently lost tie breaking, set winning sell offers change. Since pricesallocations change, consumer active producer change bids. Furthermore,system valid solution state based current price quotes, must validsolution state based next price quotes. 2note temporal-precedence tie-breaking (without requirement tied selloffers lost) sufficient ensure allocation sellers change. tiedsell offers lost, possible active producer could increase next sell offer priceprice output good. occurs, producer would lose tie breakingoutput next quote, system would quasi-quiescence.ReferencesAndersson, A., Tenhunen, M., & Ygge, F. (2000). Integer programming combinatorial auctionwinner determination. Fourth International Conference Multi-Agent Systems, pp. 3946.Andersson, M. R., & Sandholm, T. W. (1998). Leveled commitment contracting among myopicindividually rational agents. Third International Conference Multi-Agent Systems, pp.2633.Ausubel, L. M., & Milgrom, P. R. (2002). Ascending auctions package bidding. FrontiersTheoretical Economics, 1(1).Axelrod, R. (1987). evolution strategies iterated prisoners dilemma. Davis, L.(Ed.), Genetic Algorithms Simulated Annealing, chap. 3, pp. 3241. Morgan Kaufmann.Babaioff, M., & Nisan, N. (2001). Concurrent auctions across supply chain. Third ACMConference Electronic Commerce, pp. 110.Babaioff, M., & Walsh, W. E. (2003). Incentive-compatible, budget-balanced, yet highly efficientauctions supply chain formation. Fourth ACM Conference Electronic Commerce,pp. 6475.Baker, A. D. (1996). Metaphor reality: case study agents bid actual costsschedule factory. Clearwater (Clearwater, 1996).Bichler, M. (2001). Future e-Markets: Multidimensional Market Mechanisms. CambridgeUniversity Press.Bikhchandani, S., & Mamer, J. W. (1997). Competitive equilibrium exchange economyindivisibilities. Journal Economic Theory, 74, 385413.564fiD ECENTRALIZED UPPLY C HAIN F ORMATIONBikhchandani, S., & Ostroy, J. M. (2002). package assignment model. Journal EconomicTheory, 107, 377406.Borenstein, S., & Saloner, G. (2001). Economics electronic commerce. Journal EconomicPerspectives, 15(1), 312.Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 11, 1733.Clearwater, S. (Ed.). (1996). Market-Based Control: Paradigm Distributed Resource Allocation. World Scientific.Davidow, W. H. (1992). Virtual Corporation: Structuring Revitalizing Corporation21st Century. HarperCollins Publishers.Davis, R., & Smith, R. G. (1983). Negotiation metaphor distributed problem solving.Artificial Intelligence, 20, 63109.Dellarocas, C., Klein, M., & Rodriguez-Aguilar, J. A. (2000). exception-handling architectureopen electronic marketplaces Contract Net software agents. Second ACM ConferenceElectronic Commerce, pp. 225232.Demange, G., & Gale, D. (1985). strategy structure two-sided matching markets. Econometrica, 53(4), 873888.Demange, G., Gale, D., & Sotomayor, M. (1986). Multi-item auctions. Journal Political Economy, 94(4), 863872.Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge. MITPress.Friedman, D., & Rust, J. (Eds.). (1993). Double Auction Market: Institutions, Theories,Evidence. Addison-Wesley.Fudenberg, D., & Tirole, J. (1998). Game Theory. MIT Press.Greenwald, A. (2003). 2002 trading agent competition: overview agent strategies. AIMagazine, 24(1), 7782.Groves, T. (1973). Incentives teams. Econometrica, 41(4), 617631.Gul, F., & Stacchetti, E. (2000). English double auctions differentiated commodities.Journal Economic Theory, 92, 6695.Gul, F., & Stacchetti, E. (1999). Walrasian equilibrium gross substitutes. Journal EconomicTheory, 87, 95124.Hunsberger, L., & Grosz, B. J. (2000). combinatorial auction collaborative planning.Fourth International Conference MultiAgent Systems, pp. 151158.Joshi, P., Oke, M., Sharma, V., & Veeramani, D. (1999). Issues dynamic highly-distributedconfiguration supply webs. First IAC Workshop Internet-Based Negotiation Technologies.Kelso, A. S., & Crawford, V. P. (1982). Job matching, coalition formation, gross substitutes.Econometrica, 50(6), 14831504.Kjenstad, D. (1998). Coordinated Supply Chain Scheduling. Ph.D. thesis, Norwegian UniversityScience Technology.565fiWALSH & W ELLMANKlemperer, P. (1999). Auction theory: guide literature. Journal Economic Surveys, 13(3),227286.Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards universal test suite combinatorial auction algorithrms. Second ACM Conference Electronic Commerce, pp. 6676.Lucking-Reily, D., & Spulber, D. F. (2001). Business-to-business electronic commerce. JournalEconomic Perspectives, 15(1), 5568.MacKie-Mason, J. K., & Varian, H. R. (1994). Generalized Vickrey auctions. Tech. rep., Dept.Economics, Univ. Michigan.Malone, T. W., & Laubacher, R. J. (1998). dawn e-lance economy. Harvard BusinessReview, 145152.Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford UniversityPress, New York.McAfee, R. P. (1992). dominant strategy double auction. Journal Economic Theory, 56,434450.McAfee, R. P., & McMillan, J. (1987). Auctions bidding. Journal Economic Literature, 25,699738.Milgrom, P. (2000). Putting auction theory work: simultaneous ascending auction.Journal Political Economy, 108(2), 245272.Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms bilateral trading. JournalEconomic Theory, 29, 265281.Parkes, D. C., Kalagnanam, J., & Eso, M. (2001). Achieving budget-balance Vickrey-basedpayment schemes exchanges. Seventeenth International Joint Conference ArtificialIntelligence, pp. 11611168.Parkes, D. C., & Ungar, L. H. (2000). Iterative combinatorial auctions: Theory practice.Seventeenth National Conference Artificial Intelligence, pp. 7481.Parkes, D. C., & Ungar, L. H. (2002). ascending-price generalized Vickrey auction. StanfordInstitute Theoretical Economics Summer Workshop Economics Internet.Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter. MIT Press.Rust, J., Miller, J. H., & Palmer, R. (1994). Characterizing effective trading strategies: Insightscomputerized double auction tournament. Journal Economic Dynamics Control, 18,6196.Sandholm, T., & Suri, S. (2000). Improved algorithms optimal winner determination combinatorial auctions generalizations. Seventeenth National Conference ArtificialIntelligence, pp. 9097.Sandholm, T. W. (1993). implementation CONTRACT NET protocol based marginalcost calculations. Eleventh National Conference Artificial Intelligence, pp. 256262.Satterthwaite, M. A., & Williams, S. R. (1989). Bilateral trade sealed bid k-double auction:Existence efficiency. Journal Economic Theory, 48, 107133.Satterthwaite, M. A., & Williams, S. R. (1993). Bayesian theory k-double auction.Friedman, & Rust (Friedman & Rust, 1993), chap. 4, pp. 99123.566fiD ECENTRALIZED UPPLY C HAIN F ORMATIONShapiro, C., & Varian, H. R. (1999). Information Rules. Harvard Business School Press.Stone, P., & Greenwald, A. (2000). first international trading agent competition: Autonomousbidding agents. Journal Electronic Commerce Research, appear.Veeramani, D., Joshi, P., & Sharma, V. (1999). Critical research issues agent-based manufacturingsupply webs. Agents-99 Workshop Agents Electronic Commerce ManagingInternet-Enabled Supply Chain.Vickrey, W. (1961). Counterspeculation, auctions, competitive sealed tenders. Journal Finance, 16, 837.Walsh, W. E. (2001). Market Protocols Decentralized Supply Chain Formation. Ph.D. thesis,University Michigan.Walsh, W. E., & Wellman, M. P. (1999). Efficiency equilibrium task allocation economieshierarchical dependencies. Sixteenth International Joint Conference Artificial Intelligence, pp. 520526.Walsh, W. E., Wellman, M. P., & Ygge, F. (2000). Combinatorial auctions supply chain formation. Second ACM Conference Electronic Commerce, pp. 260269.Walsh, W. E., Yokoo, M., Hirayama, K., & Wellman, M. P. (2003). market-inspired approachespropositional satisfiability. Artificial Intelligence, 144, 125156.Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003). 2001 trading agent competition. Electronic Markets, 13(1), 412.Wellman, M. P., & Walsh, W. E. (2000). Distributed quiescence detection multiagent negotiation.Fourth International Conference Multi-Agent Systems, pp. 317324.Wellman, M. P., Walsh, W. E., Wurman, P. R., & MacKie-Mason, J. K. (2001a). Auction protocolsdecentralized scheduling. Games Economic Behavior, 35(1/2), 271303.Wellman, M. P., Wurman, P. R., OMalley, K., Bangera, R., Lin, S.-d., Reeves, D., & Walsh, W. E.(2001b). Designing market game trading agent competition. IEEE Internet Computing, 5(2), 4351.Wurman, P. R., Walsh, W. E., & Wellman, M. P. (1998). Flexible double auctions electroniccommerce: Theory implementation. Decision Support Systems, 24, 1727.Wurman, P. R., & Wellman, M. P. (2000). AkBA: progressive, anonymous-price combinatorialauction. Second ACM Conference Electronic Commerce, pp. 2129.Wurman, P. R., Wellman, M. P., & Walsh, W. E. (2001). parametrization auction designspace. Games Economic Behavior, 35(1/2), 304338.Ygge, F. (1998). Market-Oriented Programming Application Power Load Management.Ph.D. thesis, Lund University.567fiJournal Artificial Intelligence Research 19 (2003) 279-314Submitted 09/02; published 10/03Compiling Causal Theories Successor State AxiomsSTRIPS-Like SystemsFangzhen Linflin@cs.ust.hkDepartment Computer ScienceHong Kong University Science TechnologyClear Water Bay, Kowloon, Hong KongAbstractdescribe system specifying effects actions. Unlike commonlyused AI planning, system uses action description language allows onespecify effects actions using domain rules, state constraintsentail new action effects old ones. Declaratively, action domain languagecorresponds nonmonotonic causal theory situation calculus. Procedurally,action domain compiled set logical theories, one action domain,fully instantiated successor state-like axioms STRIPS-like systemsgenerated. expect system useful tool knowledge engineers writing actionspecifications classical AI planning systems, GOLOG systems, systemsformal specifications actions needed.1. Introductiondescribe system generating action effect specifications set domain rulesdirect action effect axioms, among things. expect system usefultool knowledge engineers writing action specifications classical AI planning systems,GOLOG systems (Levesque et al., 1997), systems formal specificationsactions needed.motivate, consider language used STRIPS (Fikes & Nilsson, 1971) describing effects actions. Briefly speaking, action described languagefirst-order formula, called precondition describes conditionaction executable, add list enumerates propositions action maketrue successfully executed situation, delete list enumerates propositions action make false successfully executed situation.original STRIPS allowed precondition elements two lists complexformulas, STRIPS actions refer whose precondition given conjunction atomic formulas whose add delete lists lists atomic formulas.widely acknowledged language inadequate describing actionsreal world. One limitations, one address paper,language, one enumerate possible effects action, difficult impossibletask complex domains. example, given large C program, hard figureeffects changing value pointer values pointers program.However, underlying principle simple: value pointer changes,values pointers point memory location change well. Putanother way, direct effect action changing value pointer xc2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiLinvalue pointer x. indirect side effects action derivedconstraint says two pointers point common location,values must same.idea specifying effects actions using domain constraints like engineeringfirst principle, many advantages. First all, constraints action independent, work actions. Secondly, effects actions derived domainconstraints agree ones expectation, good indication oneaxiomatized domain correctly. Finally, domain constraints used purposes well. instance, used check consistency initial situationdatabase. general, set sentences violates domain constraint, knowlegal situation satisfy set sentences. idea used planningprune impossible states. Recently, even efforts reverse engineeringdomain constraints STRIPS-like systems speed planners (e.g. Zhang & Foo,1997; Gerevini & Schubert, 1998; Fox & Long, 1998, others).appealing use domain constraints derive indirect effects actions,making idea work formally turned challenge. problem commonlyknown ramification problem, various proposals made solve it.recently, however, proposals best theoretical interesthigh computational complexity. situation since changed substantially dueuse causality representing domain constraints (Lin, 1995, 1996; McCain & Turner,1995, 1997; Thielscher, 1995, 1997; Baral, 1995; Lifschitz, 1997, others).describe paper implemented system builds recent work causalitybased approaches ramification problem. Specifically, system takes inputaction domain description actions described precondition axiomsdirect effect axioms, domain constraints represented call domain rules.system returns output complete action specification STRIPS-like formatset fully instantiated successor state axioms (Reiter, 1991).paper organized follows. begin introducing action description language. propose procedure compile action domain specified languagecomplete set successor state axioms STRIPS-like descriptionextracted. show soundness procedure respect translationaction domain descriptions situation calculus causal theories Lin (1995). nextdescribe implementation procedure, present experimental results.one see, one limitations system essentially propositional.effect axioms domain rules variables, need fully instantiatedcompilation process. partially overcome limitation, show resultsallow one generalize propositional output first-order case certain classesaction domain descriptions. discuss related work, conclude paperpointers future work.2. Action Description Languageassume first-order language equality. shall call predicates whose extensions may changed actions fluents, whose extensions changedactions static relations. also call unary static relations types. fluent atoms280fiFrom Causal Theories STRIPS-Like Systemsmean atomic formulas formed fluents. equality atom one form u = v,u v variables constants, inequality constraint one formu 6= v. Actions represented functions, assumed functionspositive arities language.action description language includes following components.2.1 Type Definitionstype definition specified expressions following form:Domain(p, {a1 , ..., }),p type, a1 , ..., constants. intuitive meaning expressiondomain (extension) type p set {a1 , ..., }. instance, blocksworld, may type called block, have, say, five blocks named numerically:Domain(block, {1, 2, 3, 4, 5}). logistics domain, may type called loclocations, have, say, 3 locations l1 , l2 , l3 : Domain(loc, {l1 , l2 , l3 }).2.2 Primitive Fluent DefinitionsPrimitive fluents defined expressions following form:Fluent(f (x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),f n-ary fluent, pi , 1 n, type, ei , 1 m,inequality constraint form xj 6= xk , 1 j < k n. intuitive meaningexpression f (x1 , ..., xn ) legal fluent atom second argument true.instance, blocks world, given type definition Domain(block, {1, 2, 3}),following fluent specification:Fluent(on(x, y), block(x) block(y) x 6= y)would generate following six legal fluent atoms:on(1, 2), on(1, 3), on(2, 1), on(2, 3), on(3, 1), on(3, 2).Clearly, exactly one fluent definition fluent.2.3 Complex Fluent DefinitionsGiven set primitive fluents, one may want define new ones. instance,blocks world, given primitive fluent on, define clear terms as:(x)clear(x) (y)on(y, x).specify complex fluents like clear, first define fluent formulas follows:t1 = t2 fluent formula, t1 t2 terms, i.e. either constantdomain type variable.f (t1 , ..., tn ) fluent formula, t1 , ..., tn terms, f either n-aryprimitive fluent, complex fluent, static relation.281fiLin0 fluent formula, , 0 , 0 , 0 , 0 alsofluent formulas.fluent formula, x variable, p type, (x, p) (for x type p,holds) (x, p) (for x type p, holds) fluent formulas. Noticerequire types finite domains, quantifications really shorthands:domain p {a1 , ..., }, (x, p) stands(x/a1 ) (x/an ),(x, p) stands(x/a1 ) (x/an ).complex fluent specified language pair expressions followingform:Complex(f (x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),Defined(f (x1 , ..., xn ), ),pi ei primitive fluent definitions, fluent formulamention complex fluents whose free variables among x1 , ..., xn .first expression specifies syntax second semantics complex fluent.instance, complex fluent clear blocks world specified as:Complex(clear(x), block(x)),Defined(clear(x), (y, block)on(y, x)).mentioned above, quantifiers shorthands type mustfinite domain. instance, given following specification:Domain(block, {1, 2, 3}),Fluent(on(x, y), block(x) block(y))fluent definition clear expanded to:Defined(clear(1), (on(1, 1) on(2, 1) on(3, 1))),Defined(clear(2), (on(1, 2) on(2, 2) on(3, 2))),Defined(clear(3), (on(1, 3) on(2, 3) on(3, 3))).2.4 Static Relation Definitionsmentioned, static relation one changed action domain.instance, robot navigation domain, may proposition connected(d, r1, r2)meaning door connects rooms r1 r2. truth value proposition cannotchanged navigating robot rolls room room.language, static relation defined expression following form:Static(g(x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),g n-ary predicate, pi ei primitive fluent definitions.meaning expression similar fluent definition, exactlyone definition static relation.282fiFrom Causal Theories STRIPS-Like Systems2.5 Domain AxiomsDomain axioms constraints static relations. instance, static propositionconnected(d, r1, r2), may want impose following constraint: connected(d, r1, r2)connected(d, r2, r1). language, domain axioms specified expressionsform:Axiom(),fluent formula mention fluents, i.e. mentions staticrelations equality. instance, constraint connected written as:Axiom((d, door)(r1 , room)(r2 , room)connected(d, r1 , r2 )connected(d, r2 , r1 )),door room types.2.6 Action DefinitionsActions defined expressions following form:Action(a(x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),n-ary action, pi ei primitive fluent definitions.instance, blocks world, given type definition Domain(block, {1, 2, 3}),following action specification:Action(stack(x, y), block(x) block(y) x 6= y)would generate following six action instances:stack(1, 2), stack(1, 3), stack(2, 1), stack(2, 3), stack(3, 1), stack(3, 2).exactly one action definition action.2.7 Action Precondition DefinitionsAction precondition definitions specified expressions following form:P recond(a(x1 , ..., xn ), ),n-ary action, fluent formula whose free variables among x1 , ..., xn .exactly one precondition definition action. instance,blocks world, may have:P recond(stack(x, y), clear(x) clear(y) ontable(x)),says action stack(x, y) executable situation, clear(x), clear(y),ontable(x) must true it.283fiLin2.8 Action Effect SpecificationsAction effects specified expressions following form:Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),form:Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),fluent formula, f primitive fluent. intuitive meaningexpressions true initial situation, action a(x1 , ..., xn ) causef (y1 , ..., yk ) true (false). instance, blocks world, action stack(x, y) causesx y:Effect(stack(x, y), true, on(x, y)).example context dependent effect, consider action drop(x) breaks objectfragile:Effect(drop(x), f ragile(x), broken(x)).Notice fluent formula action effect specifications variables x1 , ..., xn , y1 , ..., yk . Informally, variables supposeduniversally quantified. precisely, expressions instantiated, onesubstitute objects variables, provided resulting formulas well-formed.instance, given action effect specification Effect(move(x), g(x1 ) q(x1 , x2 ), f (y)),one instantiate to: Effect(move(a), g(b) q(b, c), f (d)), long move(a) legalaction (according action definition move) g(b), q(b, c), f (d) legalfluent atoms (according fluent definitions g, q, f ).2.9 Domain RulesDomain rules specified expressions following form:Causes(, f (x1 , ..., xn )),following form:Causes(, f (x1 , ..., xn )),fluent formula, f primitive fluent. Like action effect specifications,variables x1 , ..., xn . intuitive meaning domain rulesituation, holds, fluent atom f (x1 , ..., xn ) causedtrue. domain rule stronger material implication. formal semantics givenmapping causal rule Lin (1995) (see Section 4), thus name causes.instance, blocks world, block one block:Causes(on(x, y) 6= z, on(x, z)).logistics domain, one may want say package inside trucklocation l, package location l well:Causes(in(x, y) at(y, l), at(x, l)).284fiFrom Causal Theories STRIPS-Like Systems2.10 Action Domain Descriptionsfollowing definition sums action description language:Definition 1 action domain description set type definitions, primitive fluentdefinitions, complex fluent definitions, static proposition definitions, domain axioms, actiondefinitions, action precondition definitions, action effect specifications, domain rules.Example 1 following action domain description defines blocks world threeblocks:Domain(block, {1, 2, 3}),Fluent(on(x, y), block(x) block(y)),Fluent(ontable(x), block(x)),Complex(clear(x), block(x)),Defined(clear(x), (y, block)on(y, x)),Causes(on(x, y) x 6= z, on(z, y)),Causes(on(x, y) 6= z, on(x, z)),Causes(on(x, y), ontable(x)),Causes(ontable(x), on(x, y)),Action(stack(x, y), block(x) block(y) x 6= y),P recond(stack(x, y), ontable(x) clear(x) clear(y)),Effect(stack(x, y), true, on(x, y)),Action(unstack(x, y), block(x) block(y) x 6= y),P recond(unstack(x, y), clear(x) on(x, y)),Effect(unstack(x, y), true, ontable(x)),Action(move(x, y, z), block(x) block(y) block(z) x 6= x 6= z 6= z),P recond(move(x, y, z), on(x, y) clear(x) clear(z)),Effect(move(x, y, z), true, on(x, z)).3. Procedural SemanticsGiven action domain description D, use following procedure called CCP (CausalCompletion Procedure) generate complete action effect specification:1. Use primitive complex fluent definitions generate legal fluent atoms.following let F set fluent atoms generated.2. Use action definitions generate legal action instances, actioninstance following.285fiLin2.1. primitive fluent atom F F, collect ground instances1 positiveeffects:Effect(A, 1 , F ), , Effect(A, n , F ),ground instances negative effects:Effect(A, 1 , F ), , Effect(A, , F ),ground instances positive domain rules:Causes(01 , F ), , Causes(0k , F ),ground instances negative domain rules:Causes(01 , F ), , Causes(0l , F ),generate following pseudo successor state axiom;succ(F ) init(1 ) init(n ) succ(01 ) succ(0l )init(F ) [init(1 ) init(m )succ(01 ) succ(0k )],(1)fluent formula , init() formula obtained replacing every fluent atom f init(f ), similarly succ() formulaobtained replacing every fluent atom f succ(f ). Intuitively,init(f ) means f true initial situation, succ(f ) f truesuccessor situation performing action initial situation.2.2. Let Succ set pseudo successor state axioms, one primitivefluent F , generated last step, Succ1 following set axioms:Succ1 = {succ(F ) succ() | Defined(F, ) complex fluent definition},Init following set axioms:Init = { | Axiom() domain axiom}{init() init(F ) | Causes(, F ) domain rule}{init() init(F ) | Causes(, F ) domain rule}{init(F ) init() | Defined(F, ) complex fluent definition}{init(A ) | P recond(A, ) precondition definition A}.fluent atom F , formula FInit Succ Succ1 |= succ(F ) F ,F mention propositions form succ(f ), outputaxiomsucc(F ) F .1. generating ground instances, shorthands like (x, p) expanded. See definition fluentformulas last section.286fiFrom Causal Theories STRIPS-Like SystemsOtherwise, action effect F indeterminate. case, output twoaxioms:succ(F ) F ,F succ(F ),F strongest formula satisfying first implication, F weakestformula satisfying second implication. following, explicitaction computing effects, write axiomsSuccA , InitA , Succ1A .Conceptually, Step 2.1 procedure significant. next section,shall prove step provably correct translation situation calculuscausal theories Lin (1995). Computationally, Step 2.2 expensive. shalldescribe strategies system uses implement Section 5.procedure work properly, action domain description satisfyfollowing conditions.1. require fluent atoms Init, Succ, Succ1 among generatedStep 1. would rule cases likeFluent(on(x, y), block(x) block(y) x 6= y)together Defined(clear(x), (y, block)on(y, x)), latter would generatefluent atoms form on(x, x) ruled fluent definition on.one could either drop inequality constraint definition changecomplex fluent definition Defined(clear(x), (y, block)(on(y, x) x 6= y)).could built test procedure reject action domaindescription incoherent fluent definitions like this. One easy way making surehappen use inequality constraints definition fluents.2. mentioned above, action exactly one action precondition captures exactly conditions action executable.action precondition given explicitly like this, one needs carefulwriting action effect axioms domain rules contradictory effectswould generated. instance, given P recond(A, true), action effect axiomsEffect(A, true, F ) Effect(A, true, F ) clearly realizable simultaneously.Similarly, Causes(true, F ) given domain rule, one writeeffect axiom Effect(A, true, F ). insisted always executable,could simply conclude executable effect axioms contradiction effect axioms contradict domain rules. remains futurework extend procedure allow automatic generation implicitlygiven action preconditions. now, shall assume given action domainspecification consistent sense action instance generatedStep 1, following theoryInit Succ Succ1 {init() succ(F ) |Effect(A, , F ) ground instance effect axiom}287fiLinconsistent.3. related point, procedure assumes information initial situationgiven Init. particular, action effect axioms entail information initial situation. instance, given Causes(q, p), Effect(A, true, p),P recond(A, true), must initial situation, q cannot true,otherwise, persist next situation, causing p false, contradicts action effect. Formally, means given set atomsform init(f ), f primitive fluent atom, Init consistent,SuccSucc1 also consistent, , complement I, followingset:{init(f ) | init(f ) 6f primitive fluent atom generated Step 1}Notice similar reason, Reiter needed called consistency assumption order completion procedure sound complete generatingsuccessor state axioms (Reiter, 1991).action domain descriptions clearly targeted specifying deterministic actions,indeterminate effects sometimes arise cyclic domain rules. instance,consider following action domain description:Causes(p, p),P recond(A, true)action A, Init tautology, Succ1 empty, Succ consists following pseudosuccessor state axiom p:succ(p) succ(p) init(p),equivalent init(p) succ(p). initially p true, performed,know p continue true. p initially false, performed,know p true not.Example 2 Consider blocks world description Example 1. set fluent atomsgenerated Step 1 is:F = {on(1, 1), on(1, 2), on(1, 3), on(2, 1), on(2, 2), on(2, 3),on(3, 1), on(3, 2), on(3, 3), clear(1), clear(2), clear(3),ontable(1), ontable(2), ontable(3)}.Step 2 generates following action instances:stack(1, 2), stack(1, 3), stack(2, 1), stack(2, 3), stack(3, 1), stack(3, 2),unstack(1, 2), unstack(1, 3), unstack(2, 1), unstack(2, 3), unstack(3, 1),unstack(3, 2), move(1, 2, 3), move(1, 3, 2), move(2, 1, 3), move(2, 3, 1),move(3, 1, 2), move(3, 2, 1)288fiFrom Causal Theories STRIPS-Like Systemsaction instances, need go Steps 2.1 2.2. instance,stack(1, 2), one effect axiom on(1, 2):Effect(stack(1, 2), true, on(1, 2)),following causal rules on(1, 2):Causes(on(1, 1), on(1, 2)),Causes(on(1, 3), on(1, 2)),Causes(on(2, 2), on(1, 2)),Causes(on(3, 2), on(1, 2)),Causes(ontable(1), on(1, 2)).Therefore Step 2.1 generates following pseudo-successor state axiom on(1, 2):succ(on(1, 2)) trueinit(on(1, 2)) [succ(on(1, 1)) succ(on(1, 3))succ(on(2, 2)) succ(on(3, 2) succ(ontable(1))].similarly generate following pseudo-successor state axioms primitivefluent atoms:succ(on(1, 1)) init(on(1, 1)) [succ(on(1, 2)) succ(on(1, 3))succ(on(2, 1)) succ(on(3, 1) succ(ontable(1))],succ(on(1, 3)) init(on(1, 3)) [succ(on(1, 1)) succ(on(1, 2))succ(on(2, 3)) succ(on(3, 3)) succ(ontable(1))],succ(on(2, 1)) init(on(2, 1)) [succ(on(1, 1)) succ(on(3, 1))succ(on(2, 2)) succ(on(2, 3)) succ(ontable(2))],succ(on(2, 2)) init(on(2, 2)) [succ(on(1, 2)) succ(on(1, 3))succ(on(2, 1) succ(on(2, 3)) succ(ontable(2))],succ(on(2, 3)) init(on(2, 3)) [succ(on(1, 3)) succ(on(3, 3))succ(on(2, 1) succ(on(2, 2)) succ(ontable(2))],succ(on(3, 1)) init(on(3, 1)) [succ(on(3, 2)) succ(on(3, 3))succ(on(1, 1) succ(on(1, 3)) succ(ontable(1))],succ(on(3, 2)) init(on(3, 2)) [succ(on(3, 1)) succ(on(3, 3))succ(on(1, 2) succ(on(2, 2)) succ(ontable(2))],succ(on(3, 3)) init(on(3, 3)) [succ(on(3, 1)) succ(on(3, 2))succ(on(1, 3) succ(on(2, 3)) succ(ontable(3))],succ(ontable(1))init(ontable(1)) [succ(on(1, 2)) succ(on(1, 1)) succ(on(1, 3))],succ(ontable(2))init(ontable(2)) [succ(on(2, 1)) succ(on(2, 2)) succ(on(2, 3))],289fiLinsucc(ontable(3))init(ontable(3)) [succ(on(3, 1)) succ(on(3, 2)) succ(on(3, 3))].complex fluent clear, definition yields following axioms:succ(clear(1)) succ(on(1, 1)) succ(on(2, 1)) succ(on(3, 1)),succ(clear(2)) succ(on(1, 2)) succ(on(2, 2)) succ(on(3, 2)),succ(clear(3)) succ(on(1, 3)) succ(on(2, 3)) succ(on(3, 3)).solve pseudo-successor state axioms generate following successor state axioms:succ(on(1, 1)) f alsesucc(on(1, 3)) f alsesucc(on(2, 2)) f alsesucc(on(3, 1)) f alsesucc(on(3, 3)) init(on(3, 3))succ(ontable(2)) init(ontable(2))succ(clear(1)) init(clear(1))succ(clear(3)) init(clear(3))succ(on(1, 2)) truesucc(on(2, 1)) f alsesucc(on(2, 3)) init(on(2, 3))succ(on(3, 2)) f alsesucc(ontable(1)) f alsesucc(ontable(3)) init(ontable(3))succ(clear(2)) f alseset fully instantiated successor state axioms, generateSTRIPS-like descriptions like following:stack(1, 2)Preconditions:Add list:Delete list:Cond. effects:Indet. effects:ontable(1), clear(1), clear(2).on(1, 2).ontable(1), clear(2).none.none.stack(1, 3)Preconditions:Add list:Delete list:Cond. effects:Indet. effects:ontable(1), clear(1), clear(3).on(1,3).ontable(1), clear(3).none.none.290fiFrom Causal Theories STRIPS-Like Systemsfollowing remarks:Although generate axiom succ(on(1, 3)) f alse stack(1, 2), puton(1, 3) delete list. deduce init(on(1, 3)) f alseInit well. fluent atom put add delete list actionfluent atoms truth value definitely changed action. See Section 5details STRIPS-like description generated successor stateaxioms.one see, CCP procedure crucially depends fact typefinite domain reasoning done propositional logic.limitation current system, limitation bad one might think.First all, typical planning problems assume finite domains, changingdomain type action description easy - one needs changecorresponding type definition. significantly, generic action domain descriptionoften obtained one assumes finite domain. blocks worldexample, numbers 1, 2, 3 generic names, replacedparameters. instance, replace 1 x 2 STRIPSlike description stack(1, 2), get STRIPS-like description stack(x, y)works x y. found strategy often worksplanning domains.4. Formal Semanticsformal semantics action domain description defined translatingsituation calculus causal theory Lin (1995). shall show procedure CCP givensound semantics.section mainly interested nonmonotonic action theories.interested using action description language describing actiondomains, section safely skipped.first briefly review language situation calculus.4.1 Situation Calculuslanguage situation calculus many sorted first-order language. assumefollowing sorts: situation situations, action actions, fluent propositional fluents,truth-value truth values true f alse, object everything else.use following domain independent predicates functions:Binary function - action situation s, do(a, s) situationresulting performing s.Binary predicate H - p situation s, H(p, s) true p holds s.Binary predicate P oss - action situation s, P oss(a, s) truepossible (executable) s.Ternary predicate Caused - fluent atom p, truth value v, situations, Caused(p, v, s) true fluent atom p caused (by something unspecified)truth value v situation s.291fiLinlast section, introduced fluent formulas. extend H formulas:fluent formula situation s, H(, s) defined follows:H(t1 = t2 , s) t1 = t2 .P static proposition, H(P, s) P .inductively, H(, s) H(, s), H( 0 , s) H(, s) H(0 , s), similarlyconnectives.inductively, H((x, p), s) x.[p(x) H(, s)] H((x, p), s) x.[p(x)H(, s)].According definition, H(, s) expanded situation calculus formulaH applied fluents.4.2 Translation Situation CalculusGiven first-order language L writing action domain descriptions, assumecorresponding language L0 situation calculus constants Lconstants sort object L0 , types L types (unary predicates) L0 ,static relations predicates arities L0 , fluents L functionssort f luent L0 , actions L functions sort action L0 .conventions, following translation map action domain description situationcalculus theory.Let action domain description. translation situation calculustheory defined follows:type definition Domain(p, {a1 , ..., ak }) translated to:(x).p(x) (x = a1 x = ak ),a1 6= a2 6= 6= ak .primitive fluent definitionFluent(f (x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em )translated(x1 , ..., xn ).Fluent(f (x1 , ..., xn )) p1 (x1 ) pn (xn ) e1 em .complex fluent definitionComplex(f (x1 , ..., xn ), p1 (x1 ) pn (x) e1 em ),Defined(f (x1 , ..., xn ), ),translated(x1 , ..., xn ).Fluent(f (x1 , ..., xn )) p1 (x1 ) pn (x) e1 em ,(x1 , ..., xn , s).Fluent(f (x1 , ..., xn )) [H(f (x1 , ..., xn ), s) H(, s)].292fiFrom Causal Theories STRIPS-Like Systemsdomain axiom static propositions:Axiom()translated quantifiers treated shorthands:(x, p) = x.p(x) ,(x, p) = x.p(x) .action definitionAction(a(x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em )translated(x1 , ..., xn ).Action(a(x1 , ..., xn )) p1 (x1 ) pn (xn ) e1 em .assume domain description one action definitionaction.action precondition axiomP recond(a(x1 , ..., xn ), )translated~ s).Action(a(x1 , ..., xn )) [P oss(a(x1 , ..., xn ), s) H(, s)],(,~ list free variables a(x1 , ..., xn ) . mentioned earlierone limitations current system action preconditionsgiven explicitly. reflected translation.action effect axiom:Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),translated~ s).Action(a(x1 , ..., xn )) Fluent(f (y1 , ..., yk )) P oss(a(x1 , ..., xn ), s)(,{H(, s) Caused(f (y1 , ..., yk ), true, do(a(x1 , ..., xn ), s))},~ list free variables a(x1 , ..., xn ), f (y1 , ..., yk ), .Similarly, effect axiomEffect(a(x1 , ..., xn ), , f (y1 , ..., yk )),translated~ s).Action(a(x1 , ..., xn )) Fluent(f (y1 , ..., yk )) P oss(a(x1 , ..., xn ), s)(,{H(, s) Caused(f (y1 , ..., yk ), f alse, do(a(x1 , ..., xn ), s))}.293fiLindomain rule formCauses(, f (x1 , ..., xn ))translated~().Fluent(f(x1 , ..., xn )) (s){H(, s) Caused(f (x1 , ..., xn ), true, s)},~ list free variables f (x1 , ..., xn ) . Similarly, domain ruleformCauses(, f (x1 , ..., xn ))translated~().Fluent(f(x1 , ..., xn )) (s){H(, s) Caused(f (x1 , ..., xn ), f alse, s)}.given action domain description D, let translation situationcalculus. semantics determined completion comp(T ) definedset following sentences:1. circumscription Caused predicates fixed.2. following basic axioms Caused says fluent atom causedtrue (false), true (false):Caused(p, true, s) Holds(p, s),(2)Caused(p, f alse, s) Holds(p, s).(3)3. truth values, following unique names domain closure axiom:true 6= f alse (v)(v = true v = f alse).(4)4. unique names assumptions fluents actions. Specifically, F1 ,..., Fnfluents, have:Fi (~x) 6= Fj (~y ), j different,Fi (~x) = Fi (~y ) ~x = ~y .Similarly actions.5. primitive fluent atom F , following generic successor state axiom:a, s.P oss(a, s) H(F, do(a, s))(5)[Caused(F, true, do(a, s)) H(F, s) Caused(F, f alse, do(a, s))].6. foundational axioms (Lin & Reiter, 1994b) discrete situation calculus.axioms characterize structure space situations. purposepaper, enough mention include following unique namesaxioms situations:6= do(a, s),do(a, s) = do(a0 , s0 ) (a = a0 = s0 ).294fiFrom Causal Theories STRIPS-Like Systemsfollowing theorem shows procedural semantics given previous sectionsound respect semantics given here.Theorem 1 Let action domain description, translation situationcalculus. Let ground action instance, situation variable, (s) situationcalculus formula satisfies following two conditions (1) contains twosituation terms do(A, s); (2) mention predicate H,equality, static relations. Let obtained replacing H(f, s)init(f ), H(f, do(A, s)) succ(f ).comp(T ) |= s.P oss(A, s) (s)(6)Init Succ Succ1 |= ,(7)Init, Succ, Succ1 sets axioms generated according procedure Section 3Proof: Suppose situation model comp(T ) {P oss(A, S)}. ConstructMS,A follows:domain MS,A object domain .interpretations non-situational function predicate symbols MS,A.fluent atom f , MS,A |= init(f ) iff |= H(f, S) MS,A |= succ(f ) iff|= H(f, do(A, S)).Clearly, |= (S) iff MS,A |= . show MS,A model left handside (7). this, see (7), (6).Notice first ground fluent atom F generated procedure CCP iff Fluent(F )true . Notice also fluent atoms Init Succ Succ1 must generatedprocedure.show first MS,A model Init:1. Axiom() domain axiom, . Thus satisfies . Sincefluent symbols it, MS,A satisfies too.2. Causes(, f (x1 , ..., xn )) domain rule,~().Fluent(f(x1 , ..., xn )) (s){H(, s) Caused(f (x1 , ..., xn ), true, s)}. Thus satisfies~().Fluent(f(x1 , ..., xn )) (s){H(, s) H(f (x1 , ..., xn ), s)}.Thus init( 0 ) init(F ) corresponding formula Init, 0 Fground instantiation f (x1 , ..., xn ), respectively, Fluent(F ) musttrue (otherwise formula would Init), satisfiesH( 0 , S) H(F, S). construction MS,A , satisfies init( 0 ) init(F ).case Causes(, f ) similar.295fiLin3. Suppose Defined(F, ) instantiation complex fluent definitionF Init. Init, Fluent(F ) must true. Thus must satisfyH(F, S) H(, S). construction MS,A , satisfies init(F ) init(, S).4. Suppose P recond(A, ) precondition axiom A. Since satisfies P oss(A, S)Action(A) (because one action instances generated procedure),thus satisfies H(A , S). MS,A satisfies init(A ).show MS,A model Succ, is, primitive fluent atom Fgenerated procedure Step 1, pseudo-successor state axiom (1) holds. Referringnotation axiom, need show satisfies following formula:H(F, do(A, S)) H(1 , S) H(n , S)H(01 , do(A, S)) H(0l , do(A, S))H(F, S) [H(1 , S) H(m , S)H(01 , do(A, S)) H(0k , do(A, S))].First all, instantiating generic successor state axiom (5) S, get:P oss(A, S) H(F, do(A, S))[Caused(F, true, do(A, S)) H(F, S) Caused(F, f alse, do(A, S))].Since model P oss(A, S),H(F, do(A, S))(8)[Caused(F, true, do(A, S)) H(F, S) Caused(F, f alse, do(A, S))].consider circumscription Caused predicates fixed. Noticeaxioms Caused form W Caused(x, y, z), W formulamention Caused. Therefore circumscription Caused equivalentpredicate completion Caused. Suppose F f (t), axiomsCaused(f (x), v, s) follows:W1 Caused(f (x), v, s), , Wi Caused(f (x), v, s).unique names axioms fluents, result predicate completionCaused entail:Caused(f (x), v, s) W1 Wi .W1 ,...,Wi action effect axioms domain rules f . way(1) generated, noting Action(A), Fluent(F ), P oss(A, S) true, onesee equivalence instantiated replacing x, true v,s, getCaused(F, true, do(A, S)) H(1 , S) H(n , S)H(01 , do(A, S)) H(0l , do(A, S)).296fiFrom Causal Theories STRIPS-Like SystemsSimilarly, following axiom Caused(F, f alse, do(A, S)):Caused(F, f alse, do(A, S)) H(1 , S) H(m , S)H(01 , do(A, S)) H(0k , do(A, S)).two axioms (8), get:H(F, do(A, S)) H(1 , S) H(n , S)H(01 , do(A, S)) H(0l , do(A, S))H(F, S) [H(1 , S) H(m , S)H(01 , do(A, S)) H(0k , do(A, S))].Since model Comp(T ), satisfies formula. constructionMS,A , satisfies pseudo-successor state axiom (1).Finally, fact MS,A model Succ1 apparent.2general, (6) imply (7). several reasons:mentioned procedure CCP, assume informationinitial situation given Init.procedure works actions one time. situation calculus theorycaptures effects actions single theory. possible bad specification action causes entire theory become inconsistent. instance,Causes(true, p), P recond(A, true), Effect(A, f alse, p), corresponding situation calculus theory inconsistent action A.procedure, generate inconsistent theory A.5. ImplementationExcept Step 2.2, procedure CCP Section 3 straightforward implement.section describes strategy system uses implementing Step 2.2. mainidea comes work Lin (2001a) strongest necessary weakest sufficientconditions.Given propositional theory , proposition q, set B propositions, formulasaid sufficient condition q B consists propositions B|= q. said weakest sufficient condition sufficientcondition 0 , |= 0 . Similarly, formula said necessarycondition q B consists propositions B |= q . saidstrongest necessary condition necessary condition 0 ,|= 0 .easy see weakest sufficient condition strongest necessary condition unique logical equivalence background theory. shown (Lin,2001a) two notions closely related, computed using techniqueforgetting (Lin & Reiter, 1994a). particular, action theories, effective strategy first compute strongest necessary condition, add background theory,297fiLincompute weakest sufficient condition new theory. strategyjustified following proposition Lin (2001a):Proposition 1 Let theory, q proposition, B set propositions.necessary condition q B , weakest sufficient condition q B{}, weakest sufficient condition q B .describe strategy implementing Step 2.2 procedure CCP.following, given action instance A, Step 2.2, let Succ set pseudosuccessor state axioms primitive fluent atoms, Succ1 set pseudo-successor stateaxioms complex fluent atoms, Init set initial situation axioms derivedaction precondition axiom A, domain axioms, domain rules, complex fluentdefinitions. Also following, succ-proposition one form succ(f ),init-proposition one form init(f ).1. Transform Init clausal form derive set unit clauses U nit.2. Use U nit simplify axioms Succ resulting axiom it:succ(f ) f ,(9)f mention succ-propositions, delete Succ, outputreplace succ(f ) rest axioms f .3. fluent atom f whose pseudo-successor state axiom (9) Succ, fform init(f ) ... (a candidate frame axiom), check see succ(f )derived Succ, U nit, init(f ) unit resolution. so, delete Succ,output succ(f ) init(f ), replace succ(f ) Succ init(f ).4. fluent atom f whose pseudo-successor state axiom (9) Succ, computestrongest necessary condition f succ(f ) init-propositionstheory Init Succ, weakest sufficient condition f succ(f ) initpropositions theory {f } Init Succ. f tautology, delete(9) Succ, output succ(f ) f , replace succ(f ) Succ f .tautology, output succ(f ) f f f succ(f ), delete (9)Succ. correctness step follows Proposition 1.5. previous steps solve equations Succ, generate appropriate outputprimitive fluent atoms. complex fluent atom F :Defined(F, ),every primitive fluent atom successor state axiom, following:(a) primitive fluent atoms changed action, complexfluent atom changed action either, output succ(F ) init(F );(b) otherwise, output succ(F ) , obtained succ() replacingevery succ-proposition right side successor state axiom.298fiFrom Causal Theories STRIPS-Like SystemsOtherwise, primitive fluent atoms successor stateaxiom, means action may indeterminate effect them,action may indeterminate effect F well. Compute strongest necessary weakest sufficient conditions succ(F ) Init Succ Succ1last step, output them.6. step try generate STRIPS-like description action instancebased results Steps 4 5. fluent atom F , according onefollowing cases:(a) successor state axiom succ(F ) true, put F add list unlessinit(F ) entailed Init;(b) successor state axiom succ(F ) f alse, put F delete listunless init(F ) entailed Init;(c) successor state axiom succ(F ) , true, f alse,init(F ), put F conditional effect list output successor stateaxiom.(d) F successor state axiom, put list indeterminateeffects.Clearly, F put lists, truth value affected A.Steps 4 5 procedure bottleneck worst case, computingstrongest necessary condition proposition coNP-hard. However,experience action context-free effect fluent atom F , successorstate axiom computed without going Step 4.implemented procedure CCP using strategy SWI-Prolog 3.2.92 .url system follows:http://www.cs.ust.hk/~flin/ccp.htmlUsing system, encoded action description language many planningdomains come original release PDDL (McDermott, 1998), compiledSTRIPS-like specifications. encodings domains results returnedsystem included online appendix. following, illustrateinteresting features system using following two domains: blocks worldmonkey bananas domain.5.1 Blocks Worldused blocks world running example. shall give alternative specification domain using following better known set actions: stack,unstack, pickup, putdown. shall use domain show changing slightlyprecondition one actions result different action specification.2. SWI-Prolog developed Jan Wielemaker University Amsterdam299fiLinbegin description corresponds standard STRIPS encodingdomain.Fluent(on(x, y), block(x) block(y)),Fluent(ontable(x), block(x)),Fluent(holding(x), block(x)),Complex(clear(x), block(x),Defined(clear(x), ((y, block)on(y, x)) holding(x)),Complex(handempty, true),Defined(handempty, (x, block)holding(x)),Causes(on(x, y) x 6= z, on(z, y)),Causes(on(x, y) 6= z, on(x, z)),Causes(on(x, y), ontable(x)),Causes(ontable(x), on(x, y)),Causes(on(x, y), holding(x)),Causes(on(x, y), holding(y)),Causes(holding(x), ontable(x)),Causes(holding(x), on(x, y)),Causes(holding(x), on(y, x)),Causes(holding(x) 6= x, holding(y)),Action(stack(x, y), block(x) block(y) x 6= y),P recond(stack(x, y), holding(x) clear(y)),Effect(stack(x, y), true, on(x, y)),Action(unstack(x, y), block(x) block(y) x 6= y),P recond(unstack(x, y), clear(x) on(x, y) handempty),Effect(unstack(x, y), true, holding(x)),Action(putdown(x), block(x)),P recond(putdown(x), holding(x)),Effect(putdown(x), true, ontable(x)),Action(pickup(x), block(x)),P recond(pickup(x), handempty ontable(x) clear(x)),Effect(pickup(x), true, holding(x)).Notice compared description Example 1, two fluents, holdinghandempty here. Thus domain rules them, definitionclear changed take account block held, consideredclear.300fiFrom Causal Theories STRIPS-Like Systemsassuming domain three blocks Domain(block, {1, 2, 3}), systemgenerate 19 fluent atoms, 18 action instances. action instance, returnscomplete set successor state axioms STRIPS-like representation. totalcomputation time actions 835K inferences 0.5 seconds.3 pure STRIPSdomain, i.e. actions context free. type domains, mentioned earlier,Step 4 implementation procedure needed, Step 5 easy.results expected. instance, action pickup(1), STRIPS-like representation returned system looks like following: track 1 track 2), STRIPSlike representation looks like:pickup(1):Preconditions: clear(1), handempty, ontable(1)Add list: holding(1)Delete list: ontable(1), clear(1), handemptyConditional effects:Indeterminate effects:complete output given online appendix. let us consider happendrop ontable(x) precondition pickup(x):P recond(pickup(x), handempty clear(x)).means long block clear, picked up. new precondition,system returns following STRIPS-like representation action pickup(1);pickup(1):Preconditions: clear(1), handemptyAdd list: holding(1)Delete list: clear(1), handempty, on(1, 2), on(1, 3), ontable(1)Conditional effects:succ(clear(2))<-> - (init(on(2, 2))\/init(on(3, 2)))succ(clear(3))<-> - (init(on(2, 3))\/init(on(3, 3)))Indeterminate effects:- negation, \/ disjunction. ADL-like description actionwould something like following:pickup(x):Preconditions: clear(x), handemptyAdd list: holding(x),clear(y) on(x,y)Delete list: clear(x), handempty,on(x,y) on(x,y)ontable(x) ontable(x)3. times paper refer CPU times Pentium III 1GHz machine 512MB RAM runningSWI-Prolog 3.2.9 Linux. number inferences one reported SWI-Prolog,roughly corresponds number resolution steps carried Prolog interpreter,machine independent.301fiLin5.2 Monkey Bananas Domaindomain adapted McDermotts PDDL library planning domains,attributes University Washingtons UCPOP collection action domains,turn attributes Prodigy. action effects generated systemcontext-dependent, context-free systems. shall elaboratedifference later.domain, two types, loc locations (we assume three locationshere), object things like monkey, banana, box, etc.:Domain(loc, {1, 2, 3}),Domain(object, {monkey, box, banana, knif e, glass, f ountain}).following fluent definitions:Fluent(onF loor),Fluent(at(M, X), object(M ) loc(X)),Fluent(hasknif e),Fluent(onbox(X), loc(X)),Fluent(hasbanana),Fluent(haswater),Fluent(hasglass).following domain rules fluents:Causes(onbox(X), at(monkey, X)),(10)Causes(onbox(X), at(box, X)),(11)Causes(onbox(X), onF loor),(12)Causes(onF loor, onbox(X)),(13)Causes(at(M, X) X 6= Y, at(M, )),(14)Causes(hasglass at(monkey, X), at(glass, X)),(15)Causes(hasknif e at(monkey, X), at(knif e, X)),(16)Causes(hasbanana at(monkey, X), at(banana, X)).(17)following action definitions along respective preconditions effectaxioms:goto(x, y) - monkey goes x y:Action(goto(X, ), loc(X) loc(Y ) X 6= ),P recond(goto(X, ), at(monkey, ) onF loor),Effect(goto(X, ), true, at(monkey, X)).302fiFrom Causal Theories STRIPS-Like Systemsclimb(X) - monkey climbs onto box location X:Action(climb(X), loc(X)),P recond(climb(X), at(box, X) onF loor at(monkey, X)),Effect(climb(X), true, onbox(X)).pushbox(X, ) - monkey pushes box X.Action(pushbox(X, ), loc(X) loc(Y ) X 6= ),P recond(pushbox(X, ), at(monkey, ) at(box, )) onF loor),Effect(pushbox(X, ), true, at(monkey, X)),Effect(pushbox(X, ), true, at(box, X)).getknif e(X) - get knife location X.Action(getknif e(X), loc(X)),P recond(getknif e(X), at(knif e, X) at(monkey, X) hasknif e),Effect(getknif e(X), true, hasknif e).getbanana(X) - grab banana loc X, provided monkey box.Action(getbanana(X), loc(X)),P recond(getbanana(X), onbox(X) at(banana, X) hasbanana),Effect(getbanana(X), true, hasbanana).pickglass(X) - pick glass loc X.Action(pickglass(X), loc(X)),P recond(pickglass(X), at(glass, X) at(monkey, X) hasglass),Effect(pickglass(X), true, hasglass).getwater(X) - get water fountain loc X, provided monkey box,glass hand.Action(getwater(X), loc(X)),P recond(getwater(X), at(f ountain, X) onbox(X) hasglass haswater),Effect(getwater(X), true, haswater).domain 27 actions 26 fluent atoms. Again, action, systemgenerates complete set fully instantiated successor state axioms STRIPSlike representation. instance, action goto(1, 2), following STRIPS-likerepresentation generated system:303fiLinAction goto(1, 2)Preconditions: at(monkey, 2), onFloorAdd list: at(monkey, 1)Delete list: at(monkey, 2)Conditional effects:succ(at(banana, 1)) <-> init(hasbanana) \/ init(at(banana, 1))succ(at(knife, 1)) <-> init(hasknife) \/ init(at(knife, 1))succ(at(glass, 1)) <-> init(hasglass) \/ init(at(glass, 1))succ(at(banana, 2)) <-> - init(hasbanana) & init(at(banana, 2))succ(at(knife, 2)) <-> - init(hasknife) & init(at(knife, 2))succ(at(glass, 2)) <-> - init(hasglass) & init(at(glass, 2))total running time actions 8 seconds performing 20 million inferences.90 percent time spent Step 4, i.e. computing strongest necessaryweakest sufficient conditions fluent atoms given action contextdependent effects. instance, action goto(1, 2) above, majority time spentgenerating 6 conditional effects.action, actually actions domain, could use ADL-likedescription (Pednault, 1989) conditional effects:Add list: at(banana,1) hasbananaat(knife,1) hasknifeat(glass,1) hasglassDelete list: at(banana,2) hasbananaat(knife,2) hasknifeat(glass,2) hasglassHowever, clear whether always done general case.mentioned earlier specifications domain given McDermotts collection well others context-free. instance, following specificationaction goto PDDL McDermotts collection:(:action GO-TO:parameters (?x ?y):precondition (and (location ?x) (location ?y)(not (= ?y ?x)) (on-floor) (at monkey ?y)):effect (and (at monkey ?x) (not (at monkey ?y))))corresponds context-free action change fluent except at.clear design action take account domain rules (15) - (17).specification, initially banana location 1, goal bananalocation 2 would achievable.304fiFrom Causal Theories STRIPS-Like Systems5.3 Summarydomains experimented including scheduling domain includesPednaults dictionary paycheck domain special case, rocket domain, SRIrobot domain, machine shop assembling domain, ferry domain, grid domain,sokoban domain, gear domain. included online appendix.summarize common features domains:domains tried, quite straightforward decide effectsaction encoded direct effects (those given predicate Effect)effects indirect effects (those derived domain rules).common domain rules functional dependency constraints. instance,blocks world, fluent atom on(x, y) functional arguments;monkey banana domain, fluent atom at(object, loc) functionalsecond argument (each object one location). makes sensewould special shorthand domain rules, perhaps specialprocedure handling well. significantly, given prevalencefunctional dependency constraints action domains, worthwhileinvestigate possibility general purpose planner making good useconstraints.mentioned earlier, system propositional. generated successor stateaxioms STRIPS-like systems fully instantiated. However, often easyuser generalize propositional specifications first-order ones.shall investigate generality observation next.6. Generalizing Propositional STRIPS-Like Systems OnesParametersmentioned, many action domain descriptions, successor state axiomsSTRIPS-like systems generated specific domain generalized arbitrary ones.precisely, let domain description,Domain(p1 , Dp1 ), , Domain(pk , Dpk )type specification. Suppose action InitA SuccA |= .suppose D0 another domain description like except differenttype specification:Domain(p1 , Dp0 1 ), , Domain(pk , Dp0 k ).question interested this: given one-to-one mapping typespecification D0 , InitA0 SuccA0 |= 0 true D0 ? A0 (resp. 0 )result replacing objects (resp. ) according mapping.instance, true blocks world, generalize resultsdomain description Example 1 follows. shown, action stack(1, 2),succ(on(1, 2)) succ(on(1, 3)) true. change type specificationDomain(block, {a, b, c, d, e}), map 1 a, 2 c, 3 e, new domain305fiLinspecification, action stack(a, c), succ(on(a, c)) succ(on(a, e))true. Furthermore, changing mapping 3, see x differentc (the mapping needs one-to-one), succ(on(a, x)) true.Obviously, expected blocks world. proceed showgeneral classes domain descriptions, well. first make precisemapping one type specification another.Definition 2 Given two type specifications O:Domain(p1 , Dp1 ), , Domain(pk , Dpk ),O0 :Domain(p1 , Dp0 1 ), , Domain(pk , Dp0 k ),embedding O0 one-to-one mapping Dp1 Dpk Dp0 1 Dp0 k1 k, Dpi , f (a) Dp0 .Clearly, embedding O0 , type p, size domainp O0 must least size domain p O. Given embedding, expression (actions, propositions, formulas) action domain descriptiontype specification mapped () language D0 : one simplyreplaces object (a), D0 differs uses O0type specification. Notice objects (those domain type)replaced, constants may occur effect axioms domain rules.Definition 3 action domain description belongs simple-I class mentionfunction positive arity, mention complex fluents except complex fluentdefinitions, satisfies following conditions:1. P recond(A, ) action precondition definition, form(x, p)...(y, q)W , W fluent formula quantifiers.2. Effect(A, , F ) Effect(A, , F ) action effect axiom,quantifiers, variables F among A. is, onecannot something likeEffect(explodeAt(x), nearby(y, x), dead(y)).3. Causes(, F ) Causes(, F ) domain rule, quantifiers, variables must F .Theorem 2 Let simple-I action domain description, action instanceD. Let D0 like except type specification. formulamention complex fluent quantifiers, embeddingtype specification D0 , InitA SuccA |= D.Init (A) Succ (A) |= () D0 .306fiFrom Causal Theories STRIPS-Like SystemsProof: Suppose Init (A) Succ (A) |= () true, M1 truth assignmentlanguage D0 satisfies Init (A) Succ (A) (). construct truthassignment M2 language follows: proposition P languagemention complex fluent, M2 |= P iff M1 |= (P ) (P really eitherstatic proposition, succ(F ),or init(F ), F primitive fluent atom). truthvalues complex fluent atoms M2 defined according definitions. Clearly,M2 |= . need show M2 also satisfies InitA SuccA . InitA ,three cases:1. M2 |= init(F ) init() Defined(F, ) complex fluent definition.follows construction M2 .2. M2 |= init(A ) P recond(A, ) precondition definition A.assumption, form (x, p)...(y, q).W , W formula withoutquantifiers. Without loss generality, let us assume (x, p)W . formulaequivalent_W (x/a)aDpD, Dp domain type p D. M2 |= (x, p)W iffM2 |=_W (x/a)aDpiffM1 |=_W (x/ (a)),aDptrue since M1 |= (x, p)W .3. formulas InitA mention complex fluents quantifiers.true M2 corresponding ones true M1 .SuccA , suppose F primitive fluent atom, pseudo-successor state axiom Fconstructed according procedure CCP given Section 3 follows:succ(F ) init(1 ) init(n ) succ(01 ) succ(0l )init(F ) [init(1 ) init(m ) succ(01 ) succ(0k )].following properties D:effect axiom Effect(A, , F ) Effect(A, , F ) propertyquantifier, variables also F ;domain rule form Causes(, F ) Causes(, F ) propertyquantifier, variables also F ;pseudo-successor state axiom (succ(F )) D0 (F ). Thus M2 |= Fsince M1 |= (F ). proves M2 model SuccA , thus theorem.2307fiLinHowever, examples paper belong simple-Iclass, two reasons: action preconditions, like blocks world, mentioncomplex fluents; negative domain rules Causes(, F ) mayvariables F . first problem problem principle complex fluentsreplaced definitions. second problem serious, leadsnew type simple action theories.Definition 4 action domain description belongs simple-II class mentionfunction positive arity, mention complex fluents except complex fluentdefinitions, satisfies following conditions:1. P recond(A, ) action precondition definition, form(x, p)...(y, q)W , W fluent formula quantifiers.2. Effect(A, , F ) Effect(A, , F ) action effect axiom,quantifiers, variables F among A.3. positive domain rules form Causes(, F ).4. Causes(, F ) domain rule, must form 1 2 , 1formula mention fluents 2 fluent atom. Noticerestriction variables 2 .Simple-II class action domain descriptions seem limitedpositive domain rules, negative domain rules allowed binary. Nevertheless,still capture many context-free action domains. instance, blocks worldmeet-and-pass domains paper belong class: blocks world, noticeuses complex fluent clear action precondition definitions,P recond(stack(x, y), ontable(x) clear(x) clear(y)), definitions reformulatedfollows using clears definition:P recond(stack(x, y),ontable(x) (x1 , block)(y1 , block)(on(x1 , x) on(y1 , y))).satisfy condition P recond definition simple-IIaction domain descriptions. verified formally, seemscontext-free action domains McDermotts PDDL library action domains, includinglogistics domain, belong simple-II class.Theorem 3 Let simple-II action domain description, action instanceD. Let D0 like except type specification. formulamention complex fluent quantifiers, embeddingtype specification D0 , InitA SuccA |=Init (A) Succ (A) |= () D0 .Proof: Suppose Init (A) Succ (A) |= () true, M1 truth assignmentlanguage D0 satisfies Init (A) Succ (A) (). construct truth308fiFrom Causal Theories STRIPS-Like Systemsassignment M2 language follows: proposition P languagemention complex fluent, M2 |= P iff M1 |= (P ) (P really eitherstatic proposition, succ(F ),or init(F ), F primitive fluent atom). truthvalues complex fluent atoms M2 defined according definitions. Clearly,M2 |= . need show M2 also satisfies InitA SuccA . InitA ,three cases:1. M2 |= init(F ) init() Defined(F, ) complex fluent definition.follows construction M2 .2. M2 |= init(A ) P recond(A, ) precondition definition A.assumption, form (x, p)...(y, q).W , W formula withoutquantifiers. Without loss generality, let us assume (x, p1 )W .formula equivalentW (x/a11 ) W (x/a1n1 )D. M2 |= (x, p1 )W iffM2 |= W (x/a11 ) W (x/a1n1 )iffM1 |= W (x/ (a11 )) W (x/ (a1n1 )),true since M1 |= (x, p1 )W .3. formulas InitA mention complex fluents quantifiers.true M2 corresponding ones true M1 .SuccA , suppose F primitive fluent atom. Since positive domain ruleform Causes(, F ), pseudo-successor state axiom F constructed accordingprocedure CCP given Section 3 must following form:succ(F ) init(1 ) init(n )init(F ) [init(1 ) init(m ) succ(01 ) succ(0k )],1 k, Causes(0i , F ) instance domain rule D.D0 , effect axiom Effect(, F ) Effect(, F ) propertyquantifier, variables also F , pseudo-successorstate axiom (succ(F )) D0 must form:succ( (F )) init( (1 )) init( (n ))init( (F )) [init( (1 )) init( (m ))succ( (01 ))succ( (0k ))(18)succ()],disjunction disjunct must Causes(, (F ))instance D0 fluent atom contains object (A) (F ).two cases:309fiLinSuppose M2 |= succ(F ). M1 |= succ( (F )). Since M1 model Succ (A) ,M1 satisfies axiom succ( (F )). Therefore M1 satisfies followingformula:init( (1 )) init( (n ))init( (F )) [init( (1 )) init( (m ))succ( (01 )) succ( (0k ))].Since formula mention complex fluents quantifiers,M2 satisfies corresponding formula:init(1 ) init(n )(19)init(F ) [init(1 ) init(m )succ(01 )succ(0k )],right side equivalence pseudo-successor state axiomsucc(F ) SuccA .suppose M2 satisfies (19). Well show M1 satisfies right side (18),thus M1 |= succ( (F )) M2 |= succ(F ). two cases:M2 satisfies following formula:init(1 ) init(n ).(20)case, since formula mention complex fluentsquantifier, M1 satisfies following corresponding formula:init( (1 )) init( (n )).(21)Thus M1 satisfies right side (18).M2 satisfy (20) satisfies following formula:init(F ) [init(1 ) init(m ) succ(01 ) succ(0k )].Thus M1 satisfies following formula:init( (F )) [init( (1 )) init( (m ))succ( (01 )) succ( (0k ))].show right side equivalence (18) satisfied M1 ,need show M1 |= succ(). Recall disjunctiondisjunct must correspond domain rule form Causes(, (F )),form 1 G 1 mention fluents, Gfluent atom mentions object occur (A). Noteinit() init( (F )) axiom Succ (A) , satisfied M1 . ThusM1 |= init(). means either 1 init(G) false M1 . 1false, succ() false since succ(1 ) 1 . Supposeinit(G) false M1 . Notice since positive domain rules,310fiFrom Causal Theories STRIPS-Like SystemsG object (A) (F ), pseudo-successor state axiomG Succ (A) must form succ(G) init(G) . ThereforeM1 |= init(G) get M1 |= succ(). Since disjunct ,proved M1 |= succ(). Therefore M1 |= succ( (F )). Thus M2 |= succ(F ).27. Related Workplanning, closely related work causal reasoning module Wilkinss SIPEsystem (Wilkins, 1988). Wilkins writes (page 85, Wilkins, 1988): Use STRIPSassumptions made operators unacceptably difficult describe previous classicalplanners... One primary reasons effects action must explicitly stated... Deductive causal theories one important mechanisms usedSIPE alleviate problems operator representation caused STRIPS assumption. certainly one motivations system well. SIPE, domain rulestriggers, preconditions, conditions, effects. Informally, triggers becometrue new situation, SIPE would check sequence see preconditionstrue old situation, conditions true new situation.conditions true, deduce effects. instance, SIPE causal ruleon(x, y) blocks world would look like:Causal-rule: Not-onArguments: x, y, z;Trigger: on(x,y);Precondition: on(x,z);Effects: on(x,z);comparison, domain rules much simpler. instance, corresponding ruleSIPE rule simply: Causes(on(x, y) 6= z, on(x, z)). needprocedural directives like triggers. large degree, see system rationalreconstruction causal reasoning module SIPE. shown Theorem 1,procedure used system sound translation causal theoriessituation calculus. Wilkins also gave translation causal rules formulassituation calculus, specify underlying logic reason formulas.fact, shown Lin (1995), translations would work.familiar PDDL, original version McDermott AIPS-98Planning Competition Committee allows domain axioms stratified theories. Accordingmanual PDDL 1.2 (McDermott, 1998), axioms logical formulas assertrelationships among propositions hold within situation. format writingaxioms PDDL follows:(:axiom:vars (?x ?y ...):context W:implies P)311fiLinW formula P literal. Axioms treated directionally, W P .following rule intention using axioms according manual:rule action definitions allowed effects mentionpredicates occur :implies field axiom. intentionaction definitions mention primitive predicates like on, changestruth value derived predicates like occur axioms. Withoutaxioms, action definitions describe changes predicatesmight affected action, leads complex software engineering(or domain engineering) problem.clear quotation axioms PDDL intended defining derivedpredicates. similar complex fluent definitions. New versions PDDLextended original version allowing actions durations continuous changes.considered using axioms derive changes primitive predicates likedone domain rules.action domain description language, different syntaxstrongly influenced Prolog syntax, shares much ideas behind action languages(Gelfond & Lifschitz, 1999). However, unlike action languages, provide facilities expressing truth value fluent atom particular situation like initialsituation. Rather, aimed specifying generic effects actions. hand,facilities specifying types static relations. importantly, date, actionlanguages either implemented directly mapped nonmonotonic logic programmingsystem rather compilation monotonic system action effects givenexplicitly, done here. instance, new SAT-based planning method wouldimplemented (e.g. McCain & Turner, 1998) action languages. comparison,action domain description compiled STRIPS-like description, existing planningsystems Blackbox (Selman & Kautz, 1999) System R (Lin, 2001b) directlycalled.8. Concluding Remarksdescribed system generating effects actions direct action effectaxioms domain rules, among things. shown soundness procedure used system tested successfully many benchmark action domains usedcurrent AI planners. future work, considering generalize simpleaction theories Section 6 include context-dependent action domain descriptions likemonkey bananas domain.Acknowledgmentsextended abstract part paper appeared Proceedings AAAI-2000. wouldlike thank anonymous reviewers JAIR AAAI2000 well associateeditor charge paper JAIR insightful comments earlier versionspaper. work supported part Research Grants Council Hong KongCompetitive Earmarked Research Grant HKUST6061/00E.312fiFrom Causal Theories STRIPS-Like SystemsReferencesBaral, C. (1995). Reasoning actions: nondeterministic effects, constraints, qualification. Proceedings Fourteenth International Joint Conference ArtificialIntelligence (IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo,CA., pp. 20172023.Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach theorem provingproblem solving. Artificial Intelligence, 2, 189208.Fox, M., & Long, D. (1998). automatic inference state invariants TIM. JournalArtificial Intelligence Research, 9, 367421.Gelfond, M., & Lifschitz, V. (1999). Action languages. Electronic Transactions ArtificialIntelligence, http://www.ep.liu.se/ea/cis, Vol 3, nr 016.Gerevini, A., & Schubert, L. (1998). Inferring state constraints domain-independentplanning. Proceedings 15th National Conference Artificial Intelligence(AAAI98), AAAI Press, Menlo Park, CA.Levesque, H., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. (1997). GOLOG: logicprogramming language dynamic domains. Journal Logic Programming, Specialissue Reasoning Action Change, 31, 5984.Lifschitz, V. (1997). logic causal explanation. Artificial Intelligence, 96, 451465.Lin, F. (1995). Embracing causality specifying indirect effects actions. Proceedings Fourteenth International Joint Conference Artificial Intelligence(IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 19851993.Lin, F. (1996). Embracing causality specifying indeterminate effects actions.Proceedings 13th National Conference Artificial Intelligence (AAAI96),AAAI Press, Menlo Park, CA., pp. 670676.Lin, F. (2001a). strongest necessary weakest sufficient conditions. Artificial Intelligence, 128(1-2), 143159.Lin, F. (2001b). planner called R. AI Magazine, 22(3), 7376.Lin, F., & Reiter, R. (1994a). Forget it!. Greiner, R., & Subramanian, D. (Eds.),Working Notes AAAI Fall Symposium Relevance, pp. 154159. American Association Artificial Intelligence, Menlo Park, CA. Also availablehttp://www.cs.toronto.edu/cogrobo/forgetting.ps.Z.Lin, F., & Reiter, R. (1994b). State constraints revisited. Journal Logic Computation,Special Issue Actions Processes, 4(5), 655678.McCain, N., & Turner, H. (1995). causal theory ramifications qualifications.Proceedings Fourteenth International Joint Conference Artificial Intelligence(IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 19781984.McCain, N., & Turner, H. (1997). Causal theories action change. Proceedings14th National Conference Artificial Intelligence (AAAI97), AAAI Press,Menlo Park, CA., pp. 460465.313fiLinMcCain, N., & Turner, H. (1998). Satisfiability planning causal theories. ProceedingsSixth International Conference Principles Knowledge RepresentationReasoning (KR98), pp. 212221.McDermott, D. (1998). PDDL planning domain definition language. Tech. rep. TR98-003/DCS TR-1165, Yale Center Computational Vision Control.Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situation calculus. Proceedings First International Conference PrinciplesKnowledge Representation Reasoning (KR89), pp. 324332. Morgan KaufmannPublishers, Inc.Reiter, R. (1991). frame problem situation calculus: simple solution (sometimes) completeness result goal regression. Lifschitz, V. (Ed.), ArtificialIntelligence Mathematical Theory Computation: Papers Honor John McCarthy, pp. 418420. Academic Press, San Diego, CA.Selman, B., & Kautz, H. (1999). Unifying SAT-based graph-based planning. Proceedings Sixteenth International Joint Conference Artificial Intelligence (IJCAI99), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 318325.Thielscher, M. (1995). Computing ramification post-processing. ProceedingsFourteenth International Joint Conference Artificial Intelligence (IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 19942000.Thielscher, M. (1997). Ramification causality. Artificial Intelligence, 89, 317364.Wilkins, D. (1988). Practical planning: extending classical AI planning paradigm. Morgan Kaufmann, San Mateo, CA.Zhang, Y., & Foo, N. (1997). Deriving invariants constraints action theories.Fundamenta Informaticae, 30(1), 109123.314fiJournal Artificial Intelligence Research 19 (2003) 205-208Submitted 12/02; published 9/03Research NotePotential-Based Shaping Q-Value InitializationEquivalentEric Wiewiorawiewiora@cs.ucsd.eduDepartment Computer Science EngineeringUniversity California, San DiegoLa Jolla, CA 92093 0114AbstractShaping proven powerful precarious means improving reinforcementlearning performance. Ng, Harada, Russell (1999) proposed potential-based shapingalgorithm adding shaping rewards way guarantees learner learn optimalbehavior.note, prove certain similarities shaping algorithm initialization step required several reinforcement learning algorithms. specifically,prove reinforcement learner initial Q-values based shaping algorithms potential function make updates throughout learning learner receivingpotential-based shaping rewards. prove broad category policies,behavior two learners indistinguishable. comparison provides intuitiontheoretical properties shaping algorithm well suggestion simplermethod capturing algorithms benefit. addition, equivalence raises previouslyunaddressed issues concerning efficiency learning potential-based shaping.1. Potential-Based ShapingShaping common technique improving learning performance reinforcement learning tasks. idea shaping provide learner supplemental rewardsencourage progress towards highly rewarding states environment. shapingrewards applied arbitrarily, run risk distracting learner intendedgoals environment. case, learner converges policy optimalpresence shaping rewards, suboptimal terms original task.Ng, Harada, Russell (1999) proposed method adding shaping rewards wayguarantees optimal policy maintains optimality. model reinforcementlearning task Markov Decision Process (MDP), learner tries find policymaximizes discounted future reward (Sutton & Barto, 1998). define potentialfunction () states. shaping reward transitioning state s0defined terms as:F (s, s0 ) = (s0 ) (s),MDPs discount rate. shaping reward added environmentalreward every state transition learner experiences. potential functionviewed defining topography state space. shaping reward transitioning one state another therefore discounted change state potential.Potential-based shaping guarantees cycle sequence states yields netc2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiWiewiorabenefit shaping. fact, standard conditions Ng et al. prove policyoptimal MDP augmented potential-based shaping reward alsooptimal unaugmented MDP.2. New ResultsMany reinforcement learning algorithms learn optimal policy maintaining Q-values.Q-values estimates expected future reward taking given action givenstate. show effects potential-based shaping achieved initializinglearners Q-values state potential function. prove resultQ-learning algorithm, results extend Sarsa TD algorithms well.define two reinforcement learners, L L0 , experience changesQ-values throughout learning. Let initial values Ls Q-table Q(s, a) = Q0 (s, a).potential-based shaping reward F based potential function appliedlearning. learner, L0 , Q-table initialized Q00 (s, a) = Q0 (s, a) + (s).learner receive shaping rewards.Let experience 4-tuple hs, a, r, s0 i, representing learner taking action states, transitioning state s0 receiving reward r. learners Q-values updatedbased experience using standard update rule Q-learning. Q(s, a) updatedpotential-based shaping reward, Q0 (s, a) updated without shapingreward:Q(s, a) Q(s, a) + r + F (s, s0 ) + maxQ(s0 , a0 ) Q(s, a) ,0|{z}0000Q(s,a)0Q (s, a) Q (s, a) + r + maxQ (s , ) Q0 (s, a) .0|{z}Q0 (s,a)equations interpreted updating Q-values error termscaled , learning rate (assume learners). refer errorterms Q(s, a) Q0 (s, a). also track total change Q Q0 learning.difference original current values Q(s, a) Q0 (s, a) referredQ(s, a) Q0 (s, a), respectively. Q-values learners representedinitial values plus change values resulted updates:Q(s, a) = Q0 (s, a) + Q(s, a)Q0 (s, a) = Q0 (s, a) + (s) + Q0 (s, a).Theorem 1 Given sequence experiences learning, Q(s, a) alwaysequals Q0 (s, a).Proof: Proof induction. base case Q-table entries s0 stillinitial values. theorem holds case, entries Q Q0uniformly zero.inductive case, assume entries Q(s, a) = Q0 (s, a) a.show response experience hs, a, r, s0 i, error terms Q(s, a) Q0 (s, a)206fiPotential-Based Shaping Q-Value Initializationequal. First examine update performed Q(s, a) presence potentialbased shaping reward:Q(s, a) = r + F (s, s0 ) + maxQ(s0 , a0 ) Q(s, a)00 00 0= r + (s0 ) (s) + maxQ(s,)+Q(s,)Q0 (s, a) Q(s, a)00examine update performed Q0 :Q0 (s0 , a0 ) Q0 (s, a)Q0 (s, a) = r + max00 000 0= r + maxQ(s,)+(s)+Q(s,)Q0 (s, a) (s) Q(s, a)0a0= r + (s0 ) (s) + maxQ0 (s0 , a0 ) + Q(s0 , a0 ) Q0 (s, a) Q(s, a)0= Q(s, a)Q-tables updated value, thus Q() Q0 () equal. 2implications proof appreciated consider learner choosesactions. policies defined terms learners Q-values. define advantagebased policy policy chooses action given state probabilitydetermined differences Q-values state, absolute magnitude.Thus, constant added Q-values, probability distributionnext action change.Theorem 2 L L0 learned sequence experiences useadvantage-based policy, identical probability distribution nextaction.Proof: Recall Q-values defined:Q(s, a) = Q0 (s, a) + Q(s, a)Q0 (s, a) = Q0 (s, a) + (s) + Q0 (s, a)proved Q(s, a) Q0 (s, a) equal updatedexperiences. Therefore, difference two Q-tables additionstate potentials Q0 . addition uniform across actions given state,affect policy.2turns almost policies used reinforcement learning advantage-based.important policy greedy policy. two popular exploratorypolicies, -greedy Boltzmann soft-max, also advantage-based.policies, difference learning initialization describedpotential-based shaping.207fiWiewiora3. Shaping Goal-Directed Tasksshown initial Q-values large influence efficiencyreinforcement learning goal directed tasks (Koenig & Simmons, 1996). problemscharacterized state-space goal region. agents task find policyreaches goal region quickly possible. Clearly agent must find goal stateleast exploration optimal policy found. Q-valuesinitialized optimal value, agent may require learning time exponentialstate action space order find goal state. However, deterministic environments,optimistic initialization Q-values requires learning time polynomial stateaction space goal found. See Bertsekas Tsitsiklis (1996) analysisreinforcement learning algorithms various initializations. potential-basedshaping equivalent Q-value initialization, care must taken choosing potentialfunction lead poor learning performance.4. Conclusionshown effects potential-based shaping captured particularinitialization Q-values agents using Q-learning. results extend SarsaTD methods. addition, results extend versions algorithmsaugmented eligibility traces.discrete-state environment, results imply one simply initializelearners Q-values potential function rather alter learning algorithmincorporate shaping rewards. case continuous state-spaces, potential-basedshaping may still offer benefit. continuous potential function state-spacewould analogous continuous initialization state values. potential-basedshaping allows function defined state space used potential function,method may beneficial agent restricted representation state. carefulanalysis case would fruitful avenue future research.Acknowledgementsresearch supported grant Matsushita Electric Industrial Co., Ltd.ReferencesBertsekas, D. P., & Tsitsiklis, J. T. (1996). Neuro-dynamic Programming. Athena Scientific.Koenig, S., & Simmons, R. (1996). effect representation knowledge goaldirected exploration reinforcement-learning algorithms. Machine Learning,22 (1/3), 227 250.Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance reward transformations:theory application reward shaping. Machine Learning, ProceedingsSixteenth International Conference, pp. 278287. Morgan Kaufmann.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MITPress.208fiJournal Artificial Intelligence Research 19 (2003) 399-468Submitted 1/02; published 10/03Efficient Solution Algorithms Factored MDPsCarlos Guestringuestrin@cs.stanford.eduComputer Science Dept., Stanford UniversityDaphne Kollerkoller@cs.stanford.eduComputer Science Dept., Stanford UniversityRonald Parrparr@cs.duke.eduComputer Science Dept., Duke UniversityShobha Venkataramanshobha@cs.cmu.eduComputer Science Dept., Carnegie Mellon UniversityAbstractpaper addresses problem planning uncertainty large Markov DecisionProcesses (MDPs). Factored MDPs represent complex state space using state variablestransition model using dynamic Bayesian network. representation often allowsexponential reduction representation size structured MDPs, complexity exactsolution algorithms MDPs grow exponentially representation size. paper,present two approximate solution algorithms exploit structure factored MDPs.use approximate value function represented linear combination basis functions,basis function involves small subset domain variables. key contributionpaper shows basic operations algorithms performed efficientlyclosed form, exploiting additive context-specific structure factored MDP.central element algorithms novel linear program decomposition technique, analogousvariable elimination Bayesian networks, reduces exponentially large LP provablyequivalent, polynomial-sized one. One algorithm uses approximate linear programming,second approximate dynamic programming. dynamic programming algorithm noveluses approximation based max-norm, technique directly minimizes termsappear error bounds approximate MDP algorithms. provide experimental resultsproblems 1040 states, demonstrating promising indication scalabilityapproach, compare algorithm existing state-of-the-art approach, showing,problems, exponential gains computation time.1. Introductionlast years, Markov Decision Processes (MDPs) used basicsemantics optimal planning decision theoretic agents stochastic environments.MDP framework, system modeled via set states evolve stochastically.main problem representation that, virtually real-life domain,state space quite large. However, many large MDPs significant internal structure,modeled compactly structure exploited representation.Factored MDPs (Boutilier, Dearden, & Goldszmidt, 2000) one approach representing large, structured MDPs compactly. framework, state implicitly describedassignment set state variables. dynamic Bayesian network (DBN) (Dean& Kanazawa, 1989) allow compact representation transition model,exploiting fact transition variable often depends small numberc2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiGuestrin, Koller, Parr & Venkataramanvariables. Furthermore, momentary rewards often also decomposedsum rewards related individual variables small clusters variables.two main types structure simultaneously exploited factoredMDPs: additive context-specific structure. Additive structure captures facttypical large-scale systems often decomposed combination locally interacting components. example, consider management large factory manyproduction cells. course, long run, cell positioned early production linegenerates faulty parts, whole factory may affected. However, qualityparts cell generates depends directly state cell qualityparts receives neighboring cells. additive structure also presentreward function. example, cost running factory depends, among things,sum costs maintaining local cell.Context-specific structure encodes different type locality influence: Althoughpart large system may, general, influenced state every partsystem, given point time small number parts may influence directly.factory example, cell responsible anodization may receive parts directlycell factory. However, work order cylindrical part may restrictdependency cells lathe. Thus, context producing cylindricalparts, quality anodized parts depends directly state cellslathe.Even large MDP represented compactly, example, using factoredrepresentation, solving exactly may still intractable: Typical exact MDP solution algorithms require manipulation value function, whose representation linearnumber states, exponential number state variables. One approachapproximate solution using approximate value function compact representation. common choice use linear value functions approximation valuefunctions linear combination potentially non-linear basis functions (Bellman,Kalaba, & Kotkin, 1963; Sutton, 1988; Tsitsiklis & Van Roy, 1996b). work buildsideas Koller Parr (1999, 2000), using factored (linear) value functions,basis function restricted small subset domain variables.paper presents two new algorithms computing linear value function approximations factored MDPs: one uses approximate dynamic programming anotheruses approximate linear programming. algorithms based use factored linear value functions, highly expressive function approximation method.representation allows algorithms take advantage additive context-specificstructure, order produce high-quality approximate solutions efficiently. capability exploit types structure distinguishes algorithms differ earlierapproaches (Boutilier et al., 2000), exploit context-specific structure. providedetailed discussion differences Section 10.show that, factored MDP factored value functions, various critical operations planning algorithms implemented closed form without necessarilyenumerating entire state space. particular, new algorithms build uponnovel linear programming decomposition technique. technique reduces structured LPsexponentially many constraints equivalent, polynomially-sized ones. decomposition follows procedure analogous variable elimination applies additively400fiEfficient Solution Algorithms Factored MDPsstructured value functions (Bertele & Brioschi, 1972) value functions also exploit context-specific structure (Zhang & Poole, 1999). Using basic operations,planning algorithms implemented efficiently, even though size state spacegrows exponentially number variables.first method based approximate linear programming algorithm (Schweitzer& Seidmann, 1985). algorithm generates linear, approximate value functionsolving single linear program. Unfortunately, number constraints LP proposedSchweitzer Seidmann grows exponentially number variables. Using LPdecomposition technique, exploit structure factored MDPs represent exactlyoptimization problem exponentially fewer constraints.terms approximate dynamic programming, paper makes twofold contribution.First, provide new approach approximately solving MDPs using linear valuefunction. Previous approaches linear function approximation typically utilizedleast squares (L2 -norm) approximation value function. Least squares approximationsincompatible convergence analyses MDPs, based max-norm.provide first MDP solution algorithms value iteration policy iterationuse linear max-norm projection approximate value function, thereby directlyoptimizing quantity appears provided error bounds. Second, showexploit structure problem apply technique factored MDPs,leveraging LP decomposition technique.Although approximate dynamic programming currently possesses stronger theoreticalguarantees, experimental results suggest approximate linear programminggood alternative. Whereas former tends generate better policies setbasis functions, due simplicity computational advantages approximate linearprogramming, add basis functions, obtaining better policy still requiringless computation approximate dynamic programming approach.Finally, present experimental results comparing approach work Boutilieret al. (2000), illustrating tradeoffs two methods. particular,problems significant context-specific structure value function, approachfaster due efficient handling value function representation. However,cases significant context-specific structure problem, rathervalue function, algorithm requires exponentially large value functionrepresentation. classes problems, demonstrate using value function exploits additive context-specific structure, algorithm obtainpolynomial-time near-optimal approximation true value function.paper starts presentation factored MDPs approximate solution algorithms MDPs. Section 4, describe basic operations used algorithms,including LP decomposition technique. Section 5, present first twoalgorithms: approximate linear programming algorithm factored MDPs. secondalgorithm, approximate policy iteration max-norm projection, presented Section 6.Section 7 describes approach efficiently computing bounds policy quality basedBellman error. Section 8 shows extend methods deal context-specificstructure. paper concludes empirical evaluation Section 9 discussionrelated work Section 10.401fiGuestrin, Koller, Parr & Venkataramanpaper greatly expanded version work published Guestrinet al. (2001a), work presented Guestrin et al. (2001b, 2002).2. Factored Markov Decision ProcessesMarkov decision process (MDP) mathematical framework sequential decisionproblems stochastic domains. thus provides underlying semantics taskplanning uncertainty. begin concise overview MDP framework,describe representation factored MDPs.2.1 Markov Decision Processesbriefly review MDP framework, referring reader books BertsekasTsitsiklis (1996) Puterman (1994) in-depth review. Markov Decision Process(MDP) defined 4-tuple (X, A, R, P ) where: X finite set |X| = N states;finite set actions; R reward function R : X 7 R, R(x, a) representsreward obtained agent state x taking action a; P Markoviantransition model P (x0 | x, a) represents probability going state x statex0 action a. assume rewards bounded, is, exists RmaxRmax |R(x, a)| , x, a.Example 2.1 Consider problem optimizing behavior system administrator(SysAdmin) maintaining network computers. network, machineconnected subset machines. Various possible network topologiesdefined manner (see Figure 1 examples). one simple network, mightconnect machines ring, machine connected machines + 1 1. (Inexample, assume addition subtraction performed modulo m.)machine associated binary random variable Xi , representing whetherworking failed. every time step, SysAdmin receives certain amountmoney (reward) working machine. job SysAdmin decidemachine reboot; thus, + 1 possible actions time step: reboot onemachines nothing (only one machine rebooted per time step). machinerebooted, working high probability next time step. Every machinesmall probability failing time step. However, neighboring machine fails,probability increases dramatically. failure probabilities define transition modelP (x0 | x, a), x particular assignment describing machines workingfailed current time step, SysAdmins choice machine reboot x0resulting state next time step.assume MDP infinite horizon future rewards discountedexponentially discount factor [0, 1). stationary policy MDPmapping : X 7 A, (x) action agent takes state x. computernetwork problem, possible configuration working failing machines, policywould tell SysAdmin machine reboot. policy associated valuefunction V RN , V (x) discounted cumulative value agent getsstarts state x follows policy . precisely, value V state x402fiEfficient Solution Algorithms Factored MDPsServerServerStarBidirectional RingRing StarServer3 LegsRing RingsFigure 1: Network topologies tested; status machine influence statusparent network.policy given by:V (x) = E"X(t)(t)R X , (Xt=0#(0)) X = x ,X(t) random variable representing state system steps.running example, value function represents much money SysAdmin expectscollect starts acting according network state x. value functionfixed policy fixed point set linear equations define valuestate terms value possible successor states. formally, define:Definition 2.2 DP operator, , stationary policy is:V(x) = R(x, (x)) +XP (x0 | x, (x))V(x0 ).x0value function policy , V , fixed point operator: V = V .optimal value function V describes optimal value agent achievestarting state. V also defined set non-linear equations. case,value state must maximal expected value achievable policy startingstate. precisely, define:Definition 2.3 Bellman operator, , is:V(x) = max[R(x, a) +XP (x0 | x, a)V(x0 )].x0optimal value function V fixed point : V = V .value function V, define policy obtained acting greedily relativeV. words, state, agent takes action maximizes one-step403fiGuestrin, Koller, Parr & Venkataramanutility, assuming V represents long-term utility achieved next state.precisely, define:Greedy(V)(x) = arg max[R(x, a) +XP (x0 | x, a)V(x0 )].(1)x0greedy policy relative optimal value function V optimal policy =Greedy(V ).2.2 Factored MDPsFactored MDPs representation language allows us exploit problem structurerepresent exponentially large MDPs compactly. idea representing largeMDP using factored model first proposed Boutilier et al. (1995).factored MDP, set states described via set random variables X ={X1 , . . . , Xn }, Xi takes values finite domain Dom(Xi ). state xdefines value xi Dom(Xi ) variable Xi . general, use upper case letters(e.g., X) denote random variables, lower case (e.g., x) denote values.use boldface denote vectors variables (e.g., X) values (x). instantiationDom(Y) subset variables Z Y, use y[Z] denote valuevariables Z instantiation y.factored MDP, define state transition model using dynamic Bayesiannetwork (DBN) (Dean & Kanazawa, 1989). Let Xi denote variable Xi currenttime Xi0 , variable next step. transition graph DBNtwo-layer directed acyclic graph G whose nodes {X1 , . . . , Xn , X10 , . . . , Xn0 }. denoteparents Xi0 graph Parents (Xi0 ). simplicity exposition, assumeParents (Xi0 ) X; thus, arcs DBN variables consecutivetime slices. (This assumption used expository purposes only; intra-time slice arcshandled small modification presented Section 4.1.) node Xi0 associatedconditional probability distribution (CPD) P (Xi0 | Parents (Xi0 )). transitionprobability P (x0 | x) defined be:P (x0 | x) =P (x0i | ui ) ,ui value x variables Parents (Xi0 ).Example 2.4 Consider instance SysAdmin problem four computers, labelledM1 , . . . , M4 , unidirectional ring topology shown Figure 2(a). first taskmodeling problem factored MDP define state space X. machineassociated binary random variable Xi , representing whether workingfailed. Thus, state space represented four random variables: {X1 , X2 , X3 , X4 }.next task define transition model, represented DBN. parentsnext time step variables Xi0 depend network topology. Specifically, probabilitymachine fail next time step depends whether working currenttime step status direct neighbors (parents topology) networkcurrent time step. shown Figure 2(b), parents Xi0 example XiXi1 . CPD Xi0 Xi = false, Xi0 = false high probability;404fiEfficient Solution Algorithms Factored MDPsX1X1RX2M1RM4M2M3(a)2X3RX4R13P (Xi0 = | Xi , Xi1 , A):h1h2X2X3h3X4h4(b)Action reboot:machine machine4Xi1XiXi1XiXi1XiXi1Xi=f=f=f=t=t=f=t=t10.023810.47510.047510.95(c)Figure 2: Factored MDP example: network topology (a) obtain factoredMDP representation (b) CPDs described (c).is, failures tend persist. Xi = true, Xi0 noisy parents (inunidirectional ring topology Xi0 one parent Xi1 ); is, failureneighbors independently cause machine fail.described represent factored Markovian transition dynamics arisingMDP DBN, directly addressed representation actions.Generally, define transition dynamics MDP defining separate DBNmodel = hGa , Pa action a.Example 2.5 system administrator example, action ai rebootingone machines, default action nothing. transition modeldescribed corresponds nothing action. transition model aidifferent transition model variable Xi0 , Xi0 = trueprobability one, regardless status neighboring machines. Figure 2(c) showsactual CPD P (Xi0 = W orking | Xi , Xi1 , A), one entry assignmentstate variables Xi Xi1 , action A.fully specify MDP, also need provide compact representation rewardfunction. assume reward function factored additively set localizedreward functions, depends small set variables. example,might reward function associated machine i, depends Xi .is, SysAdmin paid per-machine basis: every time step, receives moneymachine working. formalize concept localized functions:Definition 2.6 function f scope Scope[f ] = C X f : Dom(C) 7 R.f scope Z, use f (z) shorthand f (y) partinstantiation z corresponds variables Y.405fiGuestrin, Koller, Parr & Venkataramancharacterize concept local rewards. Let R1a , . . . , Rra setfunctions, scope Ria restricted variable cluster Uai {X1 , . . . , Xn }.Preward taking action state x defined Ra (x) = ri=1 Ria (Uai ) R.example, reward function Ri associated machine i, dependsXi , depend action choice. local rewards representeddiamonds Figure 2(b), usual notation influence diagrams (Howard &Matheson, 1984).3. Approximate Solution Algorithmsseveral algorithms compute optimal policy MDP. threecommonly used value iteration, policy iteration, linear programming. key component three algorithms computation value functions, defined Section 2.1.Recall value function defines value state x state space.explicit representation value function vector values different states,solution algorithms implemented series simple algebraic steps. Thus,case, three implemented efficiently.Unfortunately, case factored MDPs, state space exponential numbervariables domain. SysAdmin problem, example, state x systemassignment describing machines working failed; is, state xassignment random variable Xi . Thus, number states exponentialnumber machines network (|X| = N = 2m ). Hence, even representingexplicit value function problems ten machines infeasible. Onemight tempted believe factored transition dynamics rewards would resultfactored value function, thereby represented compactly. Unfortunately, eventrivial factored MDPs, guarantee structure model preservedvalue function (Koller & Parr, 1999).section, discuss use approximate value function, admitscompact representation. also describe approximate versions exact algorithms,use approximate value functions. description section somewhat abstract,specify basic operations required algorithms performedexplicitly. later sections, elaborate issues, describe algorithmsdetail. brevity, choose focus policy iteration linear programming;techniques easily extend value iteration.3.1 Linear Value Functionspopular choice approximating value functions using linear regression, firstproposed Bellman et al. (1963). Here, define space allowable value functionsV H RN via set basis functions:Definition 3.1 linear value function set basis functions H = {h1 , . . . , hk }Pfunction V written V(x) = kj=1 wj hj (x) coefficients w =(w1 , . . . , wk )0 .define H linear subspace RN spanned basis functions H.useful define N k matrix H whose columns k basis functions viewed406fiEfficient Solution Algorithms Factored MDPsvectors. compact notation, approximate value function representedHw.expressive power linear representation equivalent, example,single layer neural network features corresponding basis functions definingH. features defined, must optimize coefficients w order obtaingood approximation true value function. view approach separatingproblem defining reasonable space features induced space H,problem searching within space. former problem typically purviewdomain experts, latter focus analysis algorithmic design. Clearly,feature selection important issue essentially areas learning approximation.offer simple methods selecting good features MDPs Section 11,goal address large important topic paper.chosen linear value function representation set basis functions,problem becomes one finding values weights w Hw yieldgood approximation true value function. paper, consider twoapproaches: approximate dynamic programming using policy iteration approximatelinear programming. section, present two approaches. Section 4,show exploit problem structure transform approaches practicalalgorithms deal exponentially large state spaces.3.2 Policy Iteration3.2.1 Exact Algorithmexact policy iteration algorithm iterates policies, producing improved policyiteration. Starting initial policy (0) , iteration consists two phases.Value determination computes, policy (t) , value function V(t) , findingfixed point equation T(t) V(t) = V(t) , is, unique solution set linearequations:XP (x0 | x, (t) (x))V(t) (x0 ), x.V(t) (x) = R(x, (t) (x)) +x0policy improvement step defines next policy(t+1) = Greedy(V(t) ).shown process converges optimal policy (Bertsekas & Tsitsiklis,1996). Furthermore, practice, convergence optimal policy often quick.3.2.2 Approximate Policy Iterationsteps policy iteration algorithm require manipulation value functionspolicies, often cannot represented explicitly large MDPs. defineversion policy iteration algorithm uses approximate value functions, usefollowing basic idea: restrict algorithm using value functions withinprovided H; whenever algorithm takes step results value function Voutside space, project result back space finding value functionwithin space closest V. precisely:407fiGuestrin, Koller, Parr & VenkataramanDefinition 3.2 projection operator mapping : RN H. saidprojection w.r.t. norm kk V = Hw w arg minw kHw Vk.is, V linear combination basis functions, closest V respectchosen norm.approximate policy iteration algorithm performs policy improvement step exactly. value determination step, value function value acting accordingcurrent policy (t) approximated linear combination basis functions.consider problem value determination policy (t) . point,useful introduce notation: Although rewards function stateaction choice, policy fixed, rewards become function stateonly, denote R(t) , R(t) (x) = R(x, (t) (x)). Similarly, transitionmodel: P(t) (x0 | x) = P (x0 | x, (t) (x)). rewrite value determination stepterms matrices vectors. view V(t) R(t) N -vectors, P(t)N N matrix, equations:V(t) = R(t) + P(t) V(t) .system linear equations one equation state,solved exactly relatively small N . goal provide approximate solution, withinH. precisely, want find:w(t) = arg min kHw (R(t) + P(t) Hw)k ;w= arg min (H P(t) H) w(t) R(t) .wThus, approximate policy iteration alternates two steps:w(t) = arg min kHw (R(t) + P(t) Hw)k ;w(t+1) = Greedy(Hw(t) ).(2)(3)3.2.3 Max-norm Projectionapproach along lines described used various papers, severalrecent theoretical algorithmic results (Schweitzer & Seidmann, 1985; Tsitsiklis & VanRoy, 1996b; Van Roy, 1998; Koller & Parr, 1999, 2000). However, approaches sufferproblem might call norm incompatibility. computing projection,utilize standard Euclidean projection operator respect L2 normweighted L2 norm.1 hand, convergence error analyses MDPalgorithms utilize max-norm (L ). incompatibility made difficult provideerror guarantees.tie projection operator closely error bounds useprojection operator L norm. problem minimizing L normstudied optimization literature problem finding Chebyshev solution21. Weighted L2 norm projections stable meaningful error bounds weights correspondstationary distribution fixed policy evaluation (value determination) (Van Roy, 1998),stable combined . Averagers (Gordon, 1995) stable non-expansiveL , require mixture weights determined priori. Thus, not, general,minimize L error.2. Chebyshev norm also referred max, supremum L norms minimax solution.408fiEfficient Solution Algorithms Factored MDPsoverdetermined linear system equations (Cheney, 1982). problem definedfinding w that:w arg min kCw bk .(4)wuse algorithm due Stiefel (1960), solves problem linear programming:Variables: w1 , . . . , wk , ;Minimize: ;P(5)Subject to: kj=1 cij wj biPkbi j=1 cij wj , = 1...N.Pconstraints linear program imply kj=1 cij wj bi i,equivalently, kCw bk . objective LP minimize . Thus,solution (w , ) linear program, w solution Equation (4) Lprojection error.use L projection context approximate policy iterationobvious way. implementing projection operation Equation (2), useL projection (as Equation (4)), C = (H P(t) H) b = R(t) .minimization solved using linear program (5).key point LP k + 1 variables. However, 2N constraints,makes impractical large state spaces. SysAdmin problem, example,number constraints LP exponential number machines network(a total 2 2m constraints machines). Section 4, show that, factored MDPslinear value functions, 2N constraints represented efficiently, leadingtractable algorithm.3.2.4 Error Analysismotivated use max-norm projection within approximate policy iterationalgorithm via compatibility standard error analysis techniques MDP algorithms.provide careful analysis impact L error introduced projection step. analysis provides motivation use projection step directlyminimizes quantity. acknowledge, however, main impact analysismotivational. practice, cannot provide priori guarantees L projectionoutperform methods.goal analyze approximate policy iteration terms amount errorintroduced step projection operation. error zero,performing exact value determination, error accrue. error small,get approximation accurate. result follows analysis below.precisely, define projection error error resulting approximatevalue determination step:(t) = Hw(t) R(t) + P(t) Hw(t) .Note that, using max-norm projection, finding set weights w(t)exactly minimizes one-step projection error (t) . is, choosing best409fiGuestrin, Koller, Parr & Venkataramanpossible weights respect error measure. Furthermore, exactly errormeasure going appear bounds theorem. Thus, makebounds step tight possible.first show projection error accrued step bounded:Lemma 3.3 value determination error bounded: exists constant P RmaxP (t) iterations algorithm.Proof: See Appendix A.1.Due contraction property Bellman operator, overall accumulated errordecaying average projection error incurred throughout iterations:Definition 3.4 discounted value determination error iteration defined as:(t1)(0)(t) +; = 0.(t)=Lemma 3.3 implies accumulated error remains bounded approximate policy(t)(1 )iteration: P 1. bound loss incurred acting accordingpolicy generated approximate policy iteration algorithm, opposedoptimal policy:Theorem 3.5 approximate policy iteration algorithm, let (t) policy generatediteration t. Furthermore, let V(t) actual value acting according policy.loss incurred using policy (t) opposed optimal policy value Vbounded by:(t)2kV V(t) k kV V(0) k +.(6)(1 )2Proof: See Appendix A.2.words, Equation (6) shows difference approximation iterationoptimal value function bounded sum two terms. first termpresent standard policy iteration goes zero exponentially fast. seconddiscounted accumulated projection error and, Lemma 3.3 shows, bounded. secondterm minimized choosing w(t) one minimizes:Hw(t) R(t) + P(t) Hw(t),exactly computation performed max-norm projection. Therefore,theorem motivates use max-norm projections minimize error term appearsbound.bounds provided far may seem fairly trivial, providedstrong priori bound (t) . Fortunately, several factors make bounds interesting despite lack priori guarantees. approximate policy iteration converges,b policyoccurred experiments, obtain much tighter bound:convergence, then:V V 2b,b(1 )b one-step max-norm projection error associated estimating valueb . Since max-norm projection operation provides b, easily obtain410fiEfficient Solution Algorithms Factored MDPsposteriori bound part policy iteration procedure. details providedSection 7.One could rewrite bound Theorem 3.5 terms worst case projection error P , worst projection error cycle policies, approximate policy iterationgets stuck cycle. formulations would closer analysis BertsekasTsitsiklis (1996, Proposition 6.2, p.276). However, consider case policies(or policies final cycle) low projection error, policiescannot approximated well using projection operation, largeone-step projection error. worst-case bound would loose, woulddictated error difficult policy approximate. hand, usingdiscounted accumulated error formulation, errors introduced policies hardapproximate decay rapidly. Thus, error bound represents average caseanalysis: decaying average projection errors policies encountered successive iterations algorithm. convergent case, bound computedeasily part policy iteration procedure max-norm projection used.practical benefit posteriori bounds give meaningful feedbackimpact choice value function approximation architecture.explicitly addressing difficult general problem feature selection paper,error bounds motivate algorithms aim minimize error given approximationarchitecture provide feedback could useful future efforts automaticallydiscover improve approximation architectures.3.3 Approximate Linear Programming3.3.1 Exact AlgorithmLinear programming provides alternative method solving MDPs. formulatesproblem finding value function linear program (LP). LP variablesV1 , . . . , VN , Vi represents V(xi ): value starting ith state system.LP given by:Variables: V1 , . . . , VN ;PMinimize:xi (xi ) Vi ;PSubject to: Vi [R(xi , a) + j P (xj | xi , a)Vj ] xi X, A,(7)state relevance weights positive. Note that, exact case, solutionobtained positive weight vector. interesting note stepssimplex algorithm correspond policy changes single states, steps policyiteration involve policy changes multiple states. practice, policy iteration tendsfaster linear programming approach (Puterman, 1994).3.3.2 Approximate Linear Programapproximate formulation LP approach, first proposed Schweitzer Seidmann (1985), restricts space allowable value functions linear space spannedbasis functions. approximate formulation, variables w1 , . . . , wk :weights basis functions. LP given by:411fiGuestrin, Koller, Parr & VenkataramanVariables: w1 , . . . , wk ;PPMinimize:(x) wi hi (x) ;PxPP00Subject to:wi hi (x) [R(x, a) +x0 P (x | x, a)wi hi (x )] x X, A.(8)words, formulation takes LP (7) substitutes explicit statePvalue function linear value function representation wi hi (x), or, compactnotation, V replaced Hw. linear program guaranteed feasible constantfunction function constant value states included setbasis functions.approximate linear programming formulation, choice state relevance weights,, becomes important. Intuitively, constraints LP binding; is,constraints tighter states others. state x, relevanceweight (x) indicates relative importance tight constraint. Therefore, unlikeexact case, solution obtained may differ different choices positive weight vector. Furthermore, is, general, guarantee quality greedy policygenerated approximation Hw. However, recent work de Farias VanRoy (2001a) provides analysis error relative best possible approximation subspace, guidance selecting improve qualityapproximation. particular, analysis shows LP provides bestapproximation Hw optimal value function V weighted L1 sense subjectconstraint Hw Hw , weights L1 norm state relevanceweights .transformation exact approximate problem formulation effect reducing number free variables LP k (one basis functioncoefficient), number constraints remains N |A|. SysAdmin problem,example, number constraints LP (8) (m + 1) 2m , numbermachines network. Thus, process generating constraints solvingLP still seems unmanageable machines. next section, discussuse structure factored MDP provide compact representationefficient solution LP.4. Factored Value Functionslinear value function approach, algorithms described Section 3, applychoice basis functions. context factored MDPs, Koller Parr (1999) suggestparticular type basis function, particularly compatible structurefactored MDP. suggest that, although value function typically structured,many cases might close structured. is, might wellapproximated using linear combination functions refers smallnumber variables. precisely, define:Definition 4.1 factored (linear) value function linear function basis seth1 , . . . , hk , scope hi restricted subset variables Ci .Value functions type long history area multi-attribute utility theory (Keeney & Raiffa, 1976). example, might basis function hi412fiEfficient Solution Algorithms Factored MDPsmachine, indicating whether working not. basis function scope restrictedXi . represented diamonds next time step Figure 2(b).Factored value functions provide key performing efficient computationsexponential-sized state spaces factored MDPs. main insight restricted scope functions (including basis functions) allow certain basic operationsimplemented efficiently. remainder section, show structurefactored MDPs exploited perform two crucial operations efficiently: one-steplookahead (backprojection), representation exponentially many constraintsLPs. Then, use basic building blocks formulate efficient approximation algorithms factored MDPs, presented self-contained section:approximate linear programming factored MDPs Section 5, approximate policyiteration max-norm projection Section 6.4.1 One-step Lookaheadkey step algorithms computation one-step lookahead valueaction a. necessary, example, computing greedy policyEquation (1). Lets consider computation Q function, Qa (x), representsexpected value agent obtains taking action current time step receivinglong-term value V thereafter. Q function computed by:Qa (x) = R(x, a) +XP (x0 | x, a)V(x).(9)x0is, Qa (x) given current reward plus discounted expected future value.Using notation, express greedy policy as: Greedy(V)(x) = maxa Qa (x).Recall estimating long-term value policy using set basisPfunctions: V(x) = wi hi (x). Thus, rewrite Equation (9) as:Qa (x) = R(x, a) +XP (x0 | x, a)Xx0wi hi (x).(10)Psize state space exponential, computing expectation x0 P (x0 |Px, a) wi hi (x) seems infeasible. Fortunately, discussed Koller Parr (1999),expectation operation, backprojection, performed efficiently transitionmodel value function factored appropriately. linearity valuefunction permits linear decomposition, summand expectationviewed independent value function updated manner similar valueiteration procedure used Boutilier et al. (2000). recap construction briefly,first defining:Ga (x) =Xx0P (x0 | x, a)Xwi hi (x0 ) =XwiXP (x0 | x, a)hi (x0 ).x0Thus, compute expectation basis function separately:gia (x) =XP (x0 | x, a)hi (x0 ),x0413fiGuestrin, Koller, Parr & VenkataramanPweight wi obtain total expectation Ga (x) = wi gia (x).intermediate function gia called backprojection basis function hitransition model Pa , denote gia = Pa hi . Note that, factored MDPs,transition model Pa factored (represented DBN) basis functions hiscope restricted small set variables. two important properties allow uscompute backprojections efficiently.show restricted scope function h (such basis functions)backprojected transition model P represented DBN .h scope restricted Y; goal compute g = P h. define backprojected scope set parents Y0 transition graph G ;(Y0 ) = Yi0 Y0 Parents (Yi0 ). intra-time slice arcs included, Parents (Xi0 ){X1 , . . . , Xn , X10 , . . . , Xn0 }, change algorithm definition backprojected scope . definition includes direct parents 0 ,also variables {X1 , . . . , Xn } ancestors 0 :(Y0 ) = {Xj | exist directed path Xj Xi0 Y0 }.Thus, backprojected scope may become larger, functions still factored.show that, h scope restricted Y, backprojection gscope restricted parents Y0 , i.e., (Y0 ). Furthermore, backprojectioncomputed enumerating settings variables (Y0 ), rather settingsvariables X:g(x) = (P h)(x);=XP (x0 | x)h(x0 );x0=XP (x0 | x)h(y0 );x0=Xy0=XXP (y0 | x)h(y0 )P (u0 | x);u0 (x0 y0 )P (y0 | z)h(y0 );y0= g(z);Pz value (Y0 ) x term u0 (x0 y0 ) P (u0 | x) = 1sum probability distribution complete domain. Therefore, see (P h)function whose scope restricted (Y0 ). Note cost computation dependslinearly |Dom( (Y0 ))|, depends (the scope h) complexityprocess dynamics. backprojection procedure summarized Figure 3.Returning example, consider basis function hi indicator variable Xi :takes value 1 ith machine working 0 otherwise. hi scope restrictedXi0 , thus, backprojection gi scope restricted Parents (Xi0 ): (Xi0 ) = {Xi1 , Xi }.4.2 Representing Exponentially Many Constraintsseen Section 3, approximation algorithms require solution linear programs: LP (5) approximate policy iteration, LP (8) approximate414fiEfficient Solution Algorithms Factored MDPsBackproja (h)basis function h scope C.Define scope backprojection: (C0 ) = Xi0 C0 Parentsa (Xi0 ).0assignmentPQ (C ):0 0g (y) = c0 C0 i|X 0 C0 Pa (c [Xi ] | y)h(c0 ).Return g .Figure 3: Backprojection basis function h.linear programming algorithm. LPs common characteristics:small number free variables (for k basis functions k + 1 free variables approximate policy iteration k approximate linear programming), numberconstraints still exponential number state variables. However, factored MDPs,LP constraints another useful property: functionals constraintsrestricted scope. key observation allows us represent constraintscompactly.First, observe constraints linear programs form:Xwi ci (x) b(x), x,(11)w1 , . . . , wk free variables LP x ranges states.general form represents type constraint max-norm projection LP (5)approximate linear programming formulation (8).3first insight construction replace entire set constraintsEquation (11) one equivalent non-linear constraint:maxxXwi ci (x) b(x).(12)second insight new non-linear constraint implemented setlinear constraints using construction follows structure variable eliminationcost networks. insight allows us exploit structure factored MDPs representconstraint compactly.tackle problem representing constraint Equation (12) two steps:first, computing maximum assignment fixed set weights; then, representingnon-linear constraint small set linear constraints, using construction callfactored LP.4.2.1 Maximizing State Spacekey computation algorithms represent non-linear constraint formEquation (12) efficiently small set linear constraints. presenting construction, lets first consider simpler problem: Given fixed weights wi , wouldPlike compute maximization: = maxx wi ci (x) b(x), is, state x,P3. complementary constraints (5), b(x) wi ci (x), formulated using analogousconstruction one present section changing sign ci (x) b(x). approximatelinear programming constraints (8) also formulated form, show Section 5.415fiGuestrin, Koller, Parr & VenkataramanPdifference wi ci (x) b(x) maximal. However, cannot explicitly enumerate exponential number states compute difference. Fortunately,structure factored MDPs allows us compute maximum efficiently.case factored MDPs, state space set vectors x assignments state variables X = {X1 , . . . , Xn }. view Cw b functionsstate variables, hence also difference. Thus, define functionPF w (X1 , . . . , Xn ) F w (x) = wi ci (x) b(x). Note executedrepresentation shift; viewing F w function variables X, parameterized w. Recall size state space exponential numbervariables. Hence, goal section compute maxx F w (x) without explicitlyconsidering exponentially many states. solution use fact F wPfactored representation. precisely, Cw form wi ci (Zi ), Zisubset X. example, might c1 (X1 , X2 ) takes value 1 statesX1 = true X2 = false 0 otherwise. Similarly, vector b case also sumPrestricted scope functions. Thus, express F w sum j fjw (Zj ), fjw maymay depend w. future, sometimes drop superscript wclear context.PUsing compact notation, goal simply compute maxx wi ci (x)b(x) = maxx F w (x), is, find state x F w maximized. RecallPwFw =j=1 fj (Zj ). maximize function, F , without enumerating every stateusing non-serial dynamic programming (Bertele & Brioschi, 1972). idea virtuallyidentical variable elimination Bayesian network. review construction here,central component solution LP.goal computeXmaxfj (x[Zj ]).x1 ,...,xnjmain idea that, rather summing functions maximization,maximize variables one time. maximizing xl , summandsinvolving xl participate maximization.Example 4.2 AssumeF = f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).therefore wish compute:maxx1 ,x2 ,x3 ,x4f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).first compute maximum x4 ; functions f1 f2 irrelevant,push out. getmax f1 (x1 , x2 ) + f2 (x1 , x3 ) + max[f3 (x2 , x4 ) + f4 (x3 , x4 )].x1 ,x2 ,x3x4result internal maximization depends values x2 , x3 ; thus, introduce new function e1 (X2 , X3 ) whose value point x2 , x3 value internalmax expression. problem reduces computingmax f1 (x1 , x2 ) + f2 (x1 , x3 ) + e1 (x2 , x3 ),x1 ,x2 ,x3416fiEfficient Solution Algorithms Factored MDPsVariableElimination (F, O)//F = {f1 , . . . , fm } set functions maximized;//O stores elimination order.= 1 number variables://Select next variable eliminated.Let l = O(i) ;//Select relevant functions.Let e1 , . . . , eL functions F whose scope contains Xl .//Maximize current variable Xl .Define new function e = maxxlLj=1 Scope[ej ] {Xl }.PLj=1 ej; note Scope[e] =//Update set functions.Update set functions F = F {e} \ {e1 , . . . , eL }.//Now, functions empty scopeP sum maximum value f1 + + fm .Return maximum valueei Fei .Figure 4: Variable elimination procedure computing maximum value f1 + + fm ,fi restricted scope function.one fewer variable. Next, eliminate another variable, say X3 , resultingexpression reducing to:max f1 (x1 , x2 ) + e2 (x1 , x2 ),x1 ,x2e2 (x1 , x2 ) = max[f2 (x1 , x3 ) + e1 (x2 , x3 )].x3Finally, definee3 = max f1 (x1 , x2 ) + e2 (x1 , x2 ).x1 ,x2result point number, desired maximum x1 , . . . , x4 .naive approach enumerating states requires 63 arithmetic operations variablesbinary, using variable elimination need perform 23 operations.general variable elimination algorithm described Figure 4. inputsalgorithm functions maximized F = {f1 , . . . , fm } eliminationordering variables, O(i) returns ith variable eliminated.example above, variable Xl eliminated, select relevant functionse1 , . . . , eL , whose scope contains Xl . functions removed set FPintroduce new function e = maxxl Lj=1 ej . point, scope functionsF longer depends Xl , is, Xl eliminated. procedure repeatedvariables eliminated. remaining functions F thus emptyscope. desired maximum therefore given sum remaining functions.computational cost algorithm linear number new functionvalues introduced elimination process. precisely, consider computationnew function e whose scope Z. compute function, need compute |Dom[Z]|different values. cost algorithm linear overall number values,introduced throughout execution. shown Dechter (1999), cost exponential417fiGuestrin, Koller, Parr & Venkataramaninduced width cost network, undirected graph defined variablesX1 , . . . , Xn , edge Xl Xm appear together one originalfunctions fj . complexity algorithm is, course, dependent variableelimination order problem structure. Computing optimal elimination orderNP-hard problem (Arnborg, Corneil, & Proskurowski, 1987) elimination ordersyielding low induced tree width exist problems. issuesconfronted successfully large variety practical problems Bayesian networkcommunity, benefited large variety good heuristicsdeveloped variable elimination ordering problem (Bertele & Brioschi, 1972; Kjaerulff,1990; Reed, 1992; Becker & Geiger, 2001).4.2.2 Factored LPsection, present centerpiece planning algorithms: new, generalapproach compactly representing exponentially large sets LP constraints problemsfactored structure functions constraints decomposedsum restricted scope functions. Consider original problem representingnon-linear constraint Equation (12) compactly. Recall wish representPnon-linear constraint maxx wi ci (x) b(x), equivalently, maxx F w (x),without generating one constraint state Equation (11). new, key insightnon-linear constraint implemented using construction followsstructure variable elimination cost networks.Consider function e used within F (including original fi s), let Z scope.assignment z Z, introduce variable uez , whose value represents ez ,linear program. initial functions fiw , include constraint ufzi = fiw (z).fiw linear w, constraint linear LP variables. Now, consider new functione introduced F eliminating variable Xl . Let e1 , . . . , eL functions extractedF, let Z scope resulting e. introduce set constraints:uezLXejj=1u(z,xl )[Zj ]xl .(13)Let en last function generated elimination, recall scope empty.Hence, single variable uen . introduce additional constraint uen .complete algorithm, presented Figure 5, divided three parts: First,generate equality constraints functions depend weights wi (basis functions).second part, add equality constraints functions dependweights (target functions). equality constraints let us abstract away differencestwo types functions manage unified fashion thirdpart algorithm. third part follows procedure similar variable eliminationdescribed Figure 4. However, unlike standard variable elimination would inPtroduce new function e, e = maxxl Lj=1 ej , factored LP procedureintroduce new LP variables uez . enforce definition e maximum XlPLj=1 ej , introduce new LP constraints Equation (13).Example 4.3 understand construction, consider simple example above,assume want express fact maxx F w (x). first introduce set418fiEfficient Solution Algorithms Factored MDPsFactoredLP (C, b,O)// C = {c1 , . . . , ck } set basis functions.// b = {b1 , . . . , bm } set target functions.//O stores elimination order.PP//Return (polynomial) set constraints equivalent wi ci (x) + j bj (x), x .//Data structure constraints factored LP.Let = {} .//Data structure intermediate functions generated variable elimination.Let F = {} .//Generate equality constraint abstract away basis functions.ci C:Let Z = Scope[ci ].assignment z Z, create new LP variable ufzi addconstraint :ufzi = wi ci (z).Store new function fi use variable elimination step: F = F {fi }.//Generate equality constraint abstract away target functions.bj b:Let Z = Scope[bj ].fassignment z Z, create new LP variable uzj addconstraint :fuzj = bj (z).Store new function fj use variable elimination step: F = F {fj }.//Now, F contains functions involved LP, constraints become:Pe (x), x , represent compactly using variable elimination procedure.e F= 1 number variables://Select next variable eliminated.Let l = O(i) ;//Select relevant functions.Let e1 , . . . , eL functions F whose scope contains Xl , letZj = Scope[ej ].//Introduce linear constraints maximum current variable Xl .Define new function e scope Z = Lj=1 Zj {Xl } representPLmaxxl j=1 ej .Add constraints enforce maximum: assignment z Z:uezLXeju(z,xl )[Zj ]xl .j=1//Update set functions.Update set functions F = F {e} \ {e1 , . . . , eL }.//Now, variables eliminated functions empty scope.Add last constraint :Xei .ei FReturn .Figure 5: Factored LP algorithm compact representation exponential setPPconstraints wi ci (x) + j bj (x), x.419fiGuestrin, Koller, Parr & Venkataramanvariables ufx11 ,x2 every instantiation values x1 , x2 variables X1 , X2 . Thus,X1 X2 binary, four variables. introduce constraintdefining value ufx11 ,x2 appropriately. example, f1 above, uft,t1 = 0uft,f1 = w1 . similar variables constraints fj value zZj . Note constraints simple equality constraint involving numericalconstants perhaps weight variables w.Next, introduce variables intermediate expressions generated variable elimination. example, eliminating X4 , introduce set LP variablesuex12 ,x3 ; them, set constraintsuex12 ,x3 ufx32 ,x4 + ufx43 ,x4one value x4 X4 . similar set constraint uex21 ,x2 termsufx21 ,x3 uex12 ,x3 . Note constraint simple linear inequality.prove factored LP construction represents constraintnon-linear constraint Equation (12):Theorem 4.4 constraints generated factored LP construction equivalentnon-linear constraint Equation (12). is, assignment (, w) satisfiesfactored LP constraints satisfies constraint Equation (12).Proof: See Appendix A.3.PReturning original formulation, j fjw Cw b originalset constraints. Hence new set constraints equivalent original set:Pmaxx wi ci (x) b(x) Equation (12), turn equivalent exponentialPset constraints wi ci (x) b(x), x Equation (11). Thus, representexponential set constraints new set constraints LP variables. sizenew set, variable elimination, exponential induced width costnetwork, rather total number variables.section, presented new, general approach compactly representing exponentially-large sets LP constraints problems factored structure. remainderpaper, exploit construction design efficient planning algorithms factoredMDPs.4.2.3 Factored Max-norm Projectionuse procedure representing exponential number constraintsEquation (11) compactly compute efficient max-norm projections, Equation (4):w arg min kCw bk .wmax-norm projection computed linear program (5). two setsPPconstraints LP: kj=1 cij wj bi , bi kj=1 cij wj , i.sets instance constraints Equation (11), addressedprevious section. Thus, k basis functions C restricted scopefunction target function b sum restricted scope functions,use factored LP technique represent constraints max-norm projection LPcompactly. correctness algorithm corollary Theorem 4.4:420fiEfficient Solution Algorithms Factored MDPsCorollary 4.5 solution ( , w ) linear program minimizes subjectconstraints FactoredLP(C, b,O) FactoredLP(C, b,O), eliminationorder satisfies:w arg min kCw bk ,w= min kCw bk .woriginal max-norm projection LP k + 1 variables two constraintsstate x; thus, number constraints exponential number state variables.hand, new factored max-norm projection LP variables,exponentially fewer constraints. number variables constraints new factoredLP exponential number state variables largest factor costnetwork, rather exponential total number state variables. showSection 9, exponential gain allows us compute max-norm projections efficientlysolving large factored MDPs.5. Approximate Linear Programmingbegin simplest approximate MDP solution algorithms, basedapproximate linear programming formulation Section 3.3. Using basic operationsdescribed Section 4, formulate algorithm simple efficient.5.1 Algorithmdiscussed Section 3.3, approximate linear program formulation based linearprogramming approach solving MDPs presented Section 3.3. However, approximate version, restrict space value functions linear space definedbasis functions. precisely, approximate LP formulation, variablesw1 , . . . , wk weights basis functions. LP given by:Variables: w1 , . . . , wk ;PPMinimize:(x) wi hi (x) ;xPPP00Subject to:wi hi (x) [R(x, a) +x0 P (x | x, a)wi hi (x )] x X, A.(14)words, formulation takes LP (7) substitutes explicit state valuePfunction linear value function representation wi hi (x). transformationexact approximate problem formulation effect reducing numberfree variables LP k (one basis function coefficient), numberconstraints remains |X| |A|. SysAdmin problem, example, numberconstraints LP (14) (m + 1) 2m , number machinesnetwork. However, using algorithm representing exponentially large constraint setscompactly able compute solution approximate linear programmingalgorithm closed form exponentially smaller LP, Section 4.2.PPFirst, consider objective function x (x) wi hi (x) LP (14). Naivelyrepresenting objective function requires summation exponentially large statespace. However, rewrite objective obtain compact representation.first reorder terms:421fiGuestrin, Koller, Parr & VenkataramanFactoredALP (P , R, , H, O, )//P factored transition model.//R set factored reward functions.// discount factor.//H set basis functions H = {h1 , . . . , hk }.//O stores elimination order.// state relevance weights.//Return basis function weights w computed approximate linear programming.//Cache backprojections basis functions.basis function hi H; action a:Let gia = Backproja (hi ).//Compute factored state relevance weights.basis function hi , compute factored state relevance weightsEquation (15) .//Generate approximate linear programming constraintsLet = {}.action a:}, Ra , O).Let = FactoredLP({g1a h1 , . . . , gka hkPP//So far, constraints guarantee R(x, a) + x0 P (x0 | x, a) wi hi (x0 )Pw hi (x); satisfy approximate linear programming solution (14) must addfinal constraint.Let = { = 0}.//We obtain solution weights solving LP.Let w solution linear program: minimizeconstraints .Return w.Pwi , subjectFigure 6: Factored approximate linear programming algorithm.422fiEfficient Solution Algorithms Factored MDPsX(x)Xxwi hi (x) =XXwi(x) hi (x).xNow, consider state relevance weights (x) distribution states, (x) > 0Px (x) = 1. backprojections, write:=XX(x) hi (x) =x(ci ) hi (ci );(15)ci Ci(ci ) represents marginal state relevance weights domainDom[Ci ] basis function hi . example, use uniform state relevance weights1experiments (x) = |X|marginals become (ci ) = |C1i | . Thus,Prewrite objective function wi , basis weight computed shownEquation (15). state relevance weights represented marginals, costcomputing depends exponentially size scope Ci only, ratherexponentially number state variables. hand, state relevanceweights represented arbitrary distributions, need obtain marginalsCi s, may efficient computation. Thus, greatest efficiency achievedusing compact representation, Bayesian network, state relevance weights.Second, note right side constraints LP (14) correspond Qafunctions:XXQa (x) = Ra (x) +P (x0 | x, a)wi hi (x0 ).x0Using efficient backprojection operation factored MDPs described Section 4.1rewrite Qa functions as:Qa (x) = Ra (x) +Xwi gia (x);giabackprojection basis function hi transition model Pa .discussed, hi scope restricted Ci , gia restricted scope function (C0i ).precompute backprojections gia basis relevance weights .approximate linear programming LP (14) written as:Variables: w1 , . . . , wk ;PMinimize:w ;PiPSubject to:whi (x) [R (x) +wi gi (x)] x X, A.(16)Finally, rewrite LP use constraints form one Equation (12):Variables: w1 , . . . , wk ;PMinimize:wi ;PSubject to: 0 maxx {Ra (x) + wi [gia (x) hi (x)]} A.(17)use factored LP construction Section 4.2 represent non-linearconstraints compactly. Basically, one set factored LP constraints actiona. Specifically, write non-linear constraint form Equation (12) expressing functions C as: ci (x) = hi (x)gia (x). ci (x) restricted423fiGuestrin, Koller, Parr & Venkataramanscope function; is, hi (x) scope restricted Ci , gia (x) scope restricted(C0i ), means ci (x) scope restricted Ci (C0i ). Next, targetfunction b becomes reward function Ra (x) which, assumption, factored. Finally,constraint Equation (12), free variable. hand, LP (17)maximum right hand side must less zero. final conditionachieved adding constraint = 0. Thus, algorithm generates set factoredLP constraints, one action. total number constraints variablesnew LP linear number actions |A| exponential induced widthcost network, rather total number variables. complete factoredapproximate linear programming algorithm outlined Figure 6.5.2 Examplepresent complete example operations required approximate LP algorithm solve factored MDP shown Figure 2(a). presentation follows four steps:problem representation, basis function selection, backprojections LP construction.Problem Representation: First, must fully specify factored MDP modelproblem. structure DBN shown Figure 2(b). structure maintainedaction choices. Next, must define transition probabilities action.5 actions problem: nothing, reboot one 4 machinesnetwork. CPDs actions shown Figure 2(c). Finally, must definereward function. decompose global reward sum 4 local reward functions,one machine, reward machine working. Specifically,Ri (Xi = true) = 1 Ri (Xi = false) = 0, breaking symmetry setting R4 (X4 = true) =2. use discount factor = 0.9.simple example, use five simple basis functions.Basis Function Selection:First, include constant function h0 = 1. Next, add indicators machinetake value 1 machine working: hi (Xi = true) = 1 hi (Xi = false) = 0.Backprojections:first algorithmic step computing backprojectionbasis functions, defined Section 4.1. backprojection constant basissimple:g0a =XPa (x0 | x)h0 ;x0=XPa (x0 | x) 1 ;x0= 1.Next, must backproject indicator basis functions hi :gia =Xx0=Pa (x0 | x)hi (x0i ) ;XPa (x0j | xj1 , xj )hi (x0i ) ;x01 ,x02 ,x03 ,x04 j424fiEfficient Solution Algorithms Factored MDPs=Xx0i=XXPa (x0i | xi1 , xi )hi (x0i )Pa (x0j | xj1 , xj ) ;x0 [X0 {Xi0 }] j6=iPa (x0i | xi1 , xi )hi (x0i ) ;x0i= Pa (Xi0 = true | xi1 , xi ) 1 + Pa (Xi0 = false | xi1 , xi ) 0 ;= Pa (Xi0 = true | xi1 , xi ) .Thus, gia restricted scope function {Xi1 , Xi }. use CPDs Figure 2(c) specify gia :reboot =(Xi1 , Xi ) =reboot 6=(Xi1 , Xi ) =gigiXi = true Xi = falseXi1 = true11;Xi1 = false11Xi1 = trueXi1 = falseXi = true Xi = false0.90.09.0.50.05LP Construction:illustrate factored LPs constructed algorithms,define constraints approximate linear programming approach presented above.First, define functions cai = gia hi , shown Equation (17). example,functions ca0 = 1 = 0.1 constant basis, indicator bases:reboot =(Xi1 , Xi ) =reboot 6=(Xi1 , Xi ) =ciciXi = true Xi = falseXi1 = true0.10.9;Xi1 = false0.10.9Xi1 = trueXi1 = falseXi = true Xi = false0.190.081.0.550.045Using definition cai , approximate linear programming constraints given by:0 maxxXRi +Xwj caj , .(18)jpresent LP construction one 5 actions: reboot = 1. Analogous constructionsmade actions.first set constraints, abstract away difference rewards basisfunctions introducing LP variables u equality constraints. begin rewardfunctions:R11uRx1 = 1 , ux1 = 0 ;R22uRx2 = 1 , ux2 = 0 ;R33uRx3 = 1 , ux3 = 0 ;R44uRx4 = 2 , ux4 = 0 .represent equality constraints caj functions reboot = 1 action. Noteappropriate basis function weight Equation (18) appears constraints:425fiGuestrin, Koller, Parr & Venkataramanuc0 = 0.1 w0 ;ucx11 ,x4 = 0.9 w1 ,ucx11 ,x4 = 0.1 w1 ,ucx11 ,x4 = 0.9 w1 ;ucx11 ,x4 = 0.1 w1 ,cccux21 ,x2 = 0.19 w2 , ux21 ,x2 = 0.55 w2 , ux21 ,x2 = 0.081 w2 , ucx21 ,x2 = 0.045 w2 ;ucx32 ,x3 = 0.19 w3 , ucx32 ,x3 = 0.55 w3 , ucx32 ,x3 = 0.081 w3 , ucx32 ,x3 = 0.045 w3 ;ucx43 ,x4 = 0.19 w4 , ucx43 ,x4 = 0.55 w4 , ucx43 ,x4 = 0.081 w4 , ucx43 ,x4 = 0.045 w4 .Using new LP variables, LP constraint Equation (18) reboot = 1 actionbecomes:0maxX1 ,X2 ,X3 ,X444XXcc0uR+u+uXjj1 ,Xj .Xii=1j=1ready variable elimination process. illustrate eliminationvariable X4 :0maxX1 ,X2 ,X333hXXcRic1c4c04uXi + u +uXjj1 ,Xj + max uRX4 + uX1 ,X4 + uX3 ,X4 .i=1X4j=2hc1c44represent term maxX4 uRX4 + uX1 ,X4 + uX3 ,X4 set linear constraints,one assignment X1 X3 , using new LP variables ueX11 ,X3 representmaximum:uex11 ,x3c1c44uRx4 + ux1 ,x4 + ux3 ,x4 ;uex11 ,x3c1c44uRx4 + ux1 ,x4 + ux3 ,x4 ;uex11 ,x3c1c44uRx4 + ux1 ,x4 + ux3 ,x4 ;uex11 ,x3c1c44uRx4 + ux1 ,x4 + ux3 ,x4 ;uex11 ,x3c4c14uRx4 + ux1 ,x4 + ux3 ,x4 ;uex11 ,x3c1c44uRx4 + ux1 ,x4 + ux3 ,x4 ;uex11 ,x3c1c44uRx4 + ux1 ,x4 + ux3 ,x4 ;uex11 ,x3c1c44uRx4 + ux1 ,x4 + ux3 ,x4 .eliminated variable X4 global non-linear constraint becomes:033XXcc0uR+u+uXjj1 ,Xj + ueX11 ,X3 .XiX1 ,X2 ,X3maxj=2i=1Next, eliminate variable X3 . new LP constraints variables form:c3e13ueX21 ,X2 uRX3 + uX2 ,X3 + uX1 ,X3 , X1 , X2 , X3 ;thus, removing X3 global non-linear constraint:2Xc2e2c0uRXi + u + uX1 ,X2 + uX1 ,X2 .X1 ,X20 maxi=1426fiEfficient Solution Algorithms Factored MDPs250000Number LP constraints200000# explicit constraints =(n+1) 2 nExplicit LPFactored LP15000010000050000# factored constraints =12n2 + 5n - 800246810Number machines ring121416Figure 7: Number constraints LP generated explicit state representationversus factored LP construction solution ring problembasis functions single variables approximate linear programmingsolution algorithm.eliminate X2 , generating linear constraints:c2e22ueX31 uRX2 + uX1 ,X2 + uX1 ,X2 , X1 , X2 .Now, global non-linear constraint involves X1 :e3c010 max uRX1 + u + uX1 .X1X1 last variable eliminated, scope new LP variable emptylinear constraints given by:e31u e4 u RX1 + uX1 , X1 .state variables eliminated, turning global non-linear constraintsimple linear constraint:0 uc0 + ue4 ,completes LP description approximate linear programming solutionproblem Figure 2.small example four state variables, factored LP technique generatestotal 89 equality constraints, 115 inequality constraints 149 LP variables,explicit state representation Equation (8) generates 80 inequality constraints5 LP variables. However, problem size increases, number constraintsLP variables factored LP approach grow O(n2 ), explicit state approachgrows exponentially, O(n2n ). scaling effect illustrated Figure 7.6. Approximate Policy Iteration Max-norm Projectionfactored approximate linear programming approach described previous sectionelegant easy implement. However, cannot, general, provide strong427fiGuestrin, Koller, Parr & Venkataramanguarantees error achieves. alternative use approximate policyiteration described Section 3.2, offer certain bounds error. However,shall see, algorithm significantly complicated, requires placeadditional restrictions factored MDP.particular, approximate policy iteration requires representation policyiteration. order obtain compact policy representation, must make additionalassumption: action affects small number state variables. first stateassumption formally. Then, show obtain compact representation greedypolicy respect factored value function, assumption. Finally, describefactored approximate policy iteration algorithm using max-norm projections.6.1 Default Action ModelSection 2.2, presented factored MDP model, action associatedfactored transition model represented DBN factored rewardfunction. However, different actions often similar transition dynamics, differing effect small set variables. particular, many cases variabledefault evolution model, changes action affects directly (Boutilieret al., 2000).type structure turns useful compactly representing policies, property important approximate policy iteration algorithm. Thus, sectionpaper, restrict attention factored MDPs defined using default transition model = hGd , Pd (Koller & Parr, 2000). action a, define Effects[a] X0variables next state whose local probability model different , i.e.,variables Xi0 Pa (Xi0 | Parentsa (Xi0 )) 6= Pd (Xi0 | Parentsd (Xi0 )).Example 6.1 system administrator example, action ai rebootingone machines, default action nothing. transition modeldescribed corresponds nothing action, also default transitionmodel. transition model ai different transition modelvariable Xi0 , Xi0 = true probability one, regardless statusneighboring machines. Thus, example, Effects[ai ] = Xi0 .transition dynamics, also define notion default reward model.Pcase, set reward functions ri=1 Ri (Ui ) associated default actiond. addition, action reward function Ra (Ua ). Here, extra rewardaction scope restricted Rewards[a] = Uai {X1 , . . . , Xn }. Thus, total rewardPassociated action given Ra + ri=1 Ri . Note Ra also factoredlinear combination smaller terms even compact representation.build additional assumption define complete algorithm.Recall approximate policy iteration algorithm iterates two steps: policyimprovement approximate value determination. discuss steps.6.2 Computing Greedy Policiespolicy improvement step computes greedy policy relative value function V (t1) :(t) = Greedy(V (t1) ).428fiEfficient Solution Algorithms Factored MDPsRecall value function estimates linear form Hw. describedSection 4.1, greedy policy type value function given by:Greedy(Hw)(x) = arg max Qa (x),PQa represented by: Qa (x) = R(x, a) + wi gia (x).attempt represent policy naively, faced problemexponentially large state spaces. Fortunately, shown Koller Parr (2000),greedy policy relative factored value function form decision list.precisely, policy written form ht1 , a1 i, ht2 , a2 i, . . . , htL , aL i, tiassignment values small subset Ti variables, ai action.greedy action take state x action aj corresponding first event tjlist x consistent. completeness, review constructiondecision-list policy.critical assumption allows us represent policy compact decision listdefault action assumption described Section 6.1. assumption, Qafunctions written as:Qa (x) = R (x) +rXRi (x) +i=1Xwi gia (x),Ra scope restricted Ua . Q function default action just:PPQd (x) = ri=1 Ri (x) + wi gid (x).set linear Q-functions implicitly describes policy .immediately obvious Q functions result compactly expressible policy.important insight components weighted combinationidentical, gia equal gid i. Intuitively, component gia correspondingbackprojection basis function hi (Ci ) different action influencesone variables Ci . formally, assume Effects[a] Ci = . case,variables Ci transition model . Thus,gia (x) = gid (x); words, ith component Qa function irrelevantdeciding whether action better default action d. definecomponents actually relevant: let Ia set indices Effects[a] Ci 6= .indices basis functions whose backprojection differs Pa Pd .example DBN Figure 2, actions basis functions involve single variables,Iai = i.Let us consider impact taking action default action d.define impact difference value as:(x) = Qa (x) Qd (x);= Ra (x) +Xhwi gia (x) gid (x) .(19)iIaanalysis shows (x) function whose scope restrictedTa = Ua iIa (C0i ) .429(20)fiGuestrin, Koller, Parr & VenkataramanDecisionListPolicy (Qa )//Qa set Q-functions, one action;//Return decision list policy .//Initialize decision list.Let = {}.//Compute bonus functions.action a, default action d:Compute bonus taking action a,(x) = Qa (x) Qd (x);Equation (19). Note scope restricted Ta ,Equation (20).//Add states positive bonuses (unsorted) decision list.assignment Ta :(t) > 0, add branch decision list:= {ht, a, (t)i}.//Add default action (unsorted) decision list.Let = {h, d, 0i}.//Sort decision list obtain final policy.Sort decision list decreasing order element ht, a, i.Return .Figure 8: Method computing decision list policy factored representationQa functions.example DBN, Ta2 = {X1 , X2 }.Intuitively, situation baseline value function Qd (x)defines value state x. action changes baseline addingsubtracting amount state. point amount depends Ta ,states variables Ta take values.define greedy policy relative Q functions. action a, defineset conditionals ht, a, i, assignment values variables Ta ,(t). Now, sort conditionals actions order decreasing :ht1 , a1 , 1 i, ht2 , a2 , 2 i, . . . , htL , aL , L i.Consider optimal action state x. would like get largest possible bonusdefault value. x consistent t1 , clearly take action a1 ,gives us bonus 1 . not, try get 2 ; thus, check xconsistent t2 , so, take a2 . Using procedure, compute decisionlist policy associated linear estimate value function. complete algorithmcomputing decision list policy summarized Figure 8.PNote number conditionals list |Dom(Ta )|; Ta , turn, dependsset basis function clusters intersect effects a. Thus, sizepolicy depends natural way interaction structure430fiEfficient Solution Algorithms Factored MDPsprocess description structure basis functions. problems actionsmodify large number variables, policy representation could become unwieldy.approximate linear programming approach Section 5 appropriate cases,require explicit representation policy.6.3 Value Determinationapproximate value determination step algorithm computes:w(t) = arg min kHw (R(t) + P(t) Hw)k .wrearranging expression, get:w(t) = arg min k(H P(t) H) w R(t) k .wequation instance optimization Equation (4). P(t) factored,conclude C = (H P(t) H) also matrix whose columns correspond restrictedscope functions. specifically:(t)ci (x) = hi (x) gi (x),(t)gi backprojection basis function hi transition model P(t) ,described Section 4.1. target b = R(t) corresponds reward function,moment assumed factored. Thus, apply factored LPSection 4.2.3 estimate value policy (t) .Unfortunately, transition model P(t) factored, decision list representation policy (t) will, general, induce transition model P(t) cannotrepresented compact DBN. Nonetheless, still generate compact LP exploiting decision list structure policy. basic idea introduce cost networkscorresponding branch decision list, ensuring, additionally, statesconsistent branch considered cost network maximization. Specifically,factored LP construction branch hti , ai i. ith cost networkconsiders subset states consistent ith branch decision list.Let Si set states x ti first event decision list xconsistent. is, state x Si , x consistent ti , consistenttj j < i.Recall that, Equation (11), LP construction defines set constraintsPimply wi ci (x) b(x) state x. Instead, separate setconstraints states subset Si . state Si , know action aitaken. Hence, apply construction using Pai transition modelfactored assumption place non-factored P(t) . Similarly, reward functionPbecomes Rai (x) + ri=1 Ri (x) subset states.issue guarantee cost network constraints derived transition model applied states Si . Specifically, must guaranteeapplied states consistent ti , states consistenttj j < i. guarantee first condition, simply instantiate variables Titake values specified ti . is, cost network considers variables431fiGuestrin, Koller, Parr & VenkataramanFactoredAPI (P , R, , H, O, , tmax )//P factored transition model.//R set factored reward functions.// discount factor.//H set basis functions H = {h1 , . . . , hk }.//O stores elimination order.// Bellman error precision.//tmax maximum number iterations.//Return basis function weights w computed approximate policy iteration.//Initialize weightsLet w(0) = 0.//Cache backprojections basis functions.basis function hi H; action a:Let gia = Backproja (hi ).//Main approximate policy iteration loop.Let = 0.Repeat//Policy improvement part loop.//Compute decision list policy iteration weights.Let (t) = DecisionListPolicy(Ra +P(t)wi gia ).//Value determination part loop.//Initialize constraints max-norm projection LP.Let + = {} = {}.//Initialize indicators.Let = {}.//For every branch decision list policy, generate relevant set constraints,update indicators constraint state space future branches.branch htj , aj decision list policy (t) ://Instantiate variables Tj assignment given tj .Instantiate set functions {h1 g1 j , . . . , hk gk j }partial state assignment tj store C.Instantiate target functions Raj partial state assignment tj store b.Instantiate indicator functions partial state assignment tj store 0 .//Generate factored LP constraints current decision list branch.Let + = + FactoredLP(C, b + 0 , O).Let = FactoredLP(C, b + 0 , O).//Update indicator functions.Let Ij (x) = 1(x = tj ) update indicators = Ij .//We obtain new set weights solving LP, correspondsmax-norm projection.Let w(t+1) solution linear program: minimize , subjectconstraints {+ , }.Let = + 1.BellmanErr(Hw(t) ) tmax w(t1) = w(t) .Return w(t) .Figure 9: Factored approximate policy iteration max-norm projection algorithm.432fiEfficient Solution Algorithms Factored MDPs{X1 , . . . , Xn }Ti , computes maximum states consistent Ti = ti .guarantee second condition, ensure impose constraintsstates associated previous decisions. achieved adding indicators Ijprevious decision tj , weight . specifically, Ij function takes valuestates consistent tj zero assignments Tj . constraintsith branch form:R(x, ai ) +Xwl (gl (x, ai ) h(x)) +X1(x = tj ),x [ti ],(21)j<ilx [ti ] defines assignments X consistent ti . introductionindicators causes constraints associated ti trivially satisfied states Sjj < i. Note indicators restricted-scope function Tjhandled fashion terms factored LP. Thus, decisionlist size L, factored LP contains constraints 2L cost networks. completeapproximate policy iteration max-norm projection algorithm outlined Figure 9.6.4 Comparisonsinstructive compare max-norm policy iteration algorithm L2 -projectionpolicy iteration algorithm Koller Parr (2000) terms computational costs periteration implementation complexity. Computing L2 projection requires (amongthings) series dot product operations basis functions backprojectedbasis functions hhi gj i. expressions easy compute P refers transitionmodel particular action a. However, policy represented decision list,result policy improvement step, step becomes much complicated.particular, every branch decision list, every pair basis functions j,assignment variables Scope[hi ] Scope[gja ], requires solutioncounting problem ]P -complete general. Although Koller Parr showcomputation performed using Bayesian network (BN) inference, algorithmstill requires BN inference one assignments branch decisionlist. makes algorithm difficult implement efficiently practice.max-norm projection, hand, relies solving linear program everyiteration. size linear program depends cost networks generated.discuss, two cost networks needed point decision list. complexitycost networks approximately one BN inferencescounting problem L2 projection. Overall, branch decisionlist, total two inferences, opposed one assignmentScope[hi ] Scope[gja ] every pair basis functions j. Thus, max-norm policyiteration algorithm substantially less complex computationally approach basedL2 -projection. Furthermore, use linear programming allows us rely existingLP packages (such CPLEX), highly optimized.also interesting compare approximate policy iteration algorithm approximate linear programming algorithm presented Section 5. approximatelinear programming algorithm, never need compute decision list policy.policy always represented implicitly Qa functions. Thus, algorithm433fiGuestrin, Koller, Parr & Venkataramanrequire explicit computation manipulation greedy policy. difference twoimportant consequences: one computational terms generality.First, compute consider decision lists makes approximate linearprogramming faster easier implement. algorithm, generate single LPone cost network action never need compute decision list policy.hand, iteration, approximate policy iteration needs generate two LPsevery branch decision list size L, usually significantly longer |A|,total 2L cost networks. terms representation, require policiescompact; thus, need make default action assumption. Therefore,approximate linear programming algorithm deal general class problems,action independent DBN transition model. hand,described Section 3.2, approximate policy iteration stronger guarantees termserror bounds. differences highlighted experimental resultspresented Section 9.7. Computing Bounds Policy Qualitypresented two algorithms computing approximate solutions factored MDPs.b wbalgorithms generate linear value functions denoted Hw,resulting basis function weights. practice, agent define behaviorb One issue remainsb = Greedy(Hw).acting according greedy policyb compares true optimal policy ; is, actual value Vbpolicypolicyb compares V .Section 3, showed priori bounds quality policy. Anotherpossible procedure compute posteriori bound. is, given resulting weightsb compute bound loss acting according greedy policyb ratherw,optimal policy. achieved using Bellman error analysis WilliamsBaird (1993).Bellman error defined BellmanErr(V) = kT V Vk . Given greedyb = Greedy(V), analysis provides bound:policyV V 2BellmanErr(V) .b1(22)b evaluate quality resultingThus, use Bellman error BellmanErr(Hw)greedy policy.Note computing Bellman error involves maximization state space.Thus, complexity computation grows exponentially number statevariables. Koller Parr (2000) suggested structure factored MDPexploited compute Bellman error efficiently. Here, show error boundcomputed set cost networks using similar construction one maxb representednorm projection algorithms. technique useddecision list depend algorithm used determine policy. Thus,apply technique solutions determined approximate linear programmingaction descriptions permit decision list representation policy.b Bellman error given by:set weights w,434fiEfficient Solution Algorithms Factored MDPsbFactoredBellmanErr (P , R, , H, O, w)//P factored transition model.//R set factored reward functions.// discount factor.//H set basis functions H = {h1 , . . . , hk }.//O stores elimination order.//wb weights linear value function.//Return Bellman error value function Hw.b//Cache backprojections basis functions.basis function hi H; action a:Let gia = Backproja (hi ).//Compute decision list policy value functionHw.bb = DecisionListPolicy(Ra + P wLetbi gi ).//Initialize indicators.Let = {}.//Initialize Bellman error.Let = 0.//For every branch decision list policy, generate relevant cost networks, solvevariable elimination, update indicators constraint state space future branches.bbranch htj , aj decision list policy ://Instantiate variables Tj assignment given tj .Instantiate set functions {wb1 (h1 g1 j ), . . . , wbk (hk gk j )}partial state assignment tj store C.Instantiate target functions Raj partial state assignmenttj store b.Instantiate indicator functions partial state assignmenttj store 0 .//Use variable elimination solve first cost network, update Bellman error, errorbranch larger.Let = max (, VariableElimination(C b + 0 , O)).//Use variable elimination solve second cost network, update Bellman error, errorbranch larger.Let = max (, VariableElimination(C + b + 0 , O)).//Update indicator functions.Let Ij (x) = 1(x = tj ) update indicators = Ij .Return .bFigure 10: Algorithm computing Bellman error factored value function Hw.435fiGuestrin, Koller, Parr & Venkataramanbb Hwkb ;BellmanErr(Hw)= kT Hw= maxPPPmaxx wi hi (x) Rb (x) x0 Pb (x0 | x) j wj hj (x0 ) ,PPPmaxx Rb (x) + x0 Pb (x0 | x) j wj hj (x0 ) wi hi (x)!rewards Rb transition model Pb factored appropriately,compute one two maximizations (maxx ) using variable elimination costb decision list policynetwork described Section 4.2.1. However,induce factored transition model. Fortunately, approximate policy iterationalgorithm Section 6, exploit structure decision list performmaximization efficiently. particular, approximate policy iteration, generatetwo cost networks branch decision list. guarantee maximizationperformed states branch relevant, include typeindicator functions, force irrelevant states value , thus guaranteeing point decision list policy obtain corresponding statemaximum error. state overall largest Bellman error maximumones generated point decision list policy. complete factoredalgorithm computing Bellman error outlined Figure 10.One last interesting note concerns approximate policy iteration algorithm maxnorm projection Section 6. experiments, algorithm converged,w(t) = w(t+1) iterations. convergence occurs, objective function(t+1) linear program last iteration equal Bellman error finalpolicy:Lemma 7.1 approximate policy iteration max-norm projection converges,w(t) = w(t+1) iteration t, max-norm projection error (t+1) lastb = Hw(t) :iteration equal Bellman error final value function estimate Hwb = (t+1) .BellmanErr(Hw)Proof: See Appendix A.4.Thus, bound loss acting according final policy (t+1) substituting(t+1)Bellman error bound:Corollary 7.2 approximate policy iteration max-norm projection convergesb associated greedy policyb =iterations final value function estimate Hwbb instead optimal policyGreedy(Hw),loss acting accordingbounded by:(t+1)V V 2,b1b.Vb actual value policyTherefore, approximate policy iteration converges obtain boundquality resulting policy without needing compute Bellman error explicitly.436.fiEfficient Solution Algorithms Factored MDPs8. Exploiting Context-specific StructureThus far, presented suite algorithms exploit additive structurereward basis functions sparse connectivity DBN representing transitionmodel. However, exists another important type structure alsoexploited efficient decision making: context-specific independence (CSI). example,consider agent responsible building maintaining house, painting taskcompleted plumbing electrical wiring installed,probability painting done 0, contexts plumbing electricitydone, independently agents action. representation used farpaper would use table represent type function. table exponentiallylarge number variables scope function, ignores context-specificstructure inherent problem definition.Boutilier et al. (Boutilier et al., 1995; Dearden & Boutilier, 1997; Boutilier, Dean, &Hanks, 1999; Boutilier et al., 2000) developed set algorithms exploit CSItransition reward models perform efficient (approximate) planning. Althoughapproach often successful problems value function contains sufficientcontext-specific structure, approach able exploit additive structurealso often present real-world problems.section, extend factored MDP model include context-specific structure.present simple, yet effective extension algorithms exploit CSIadditive structure obtain efficient approximations factored MDPs. first extendfactored MDP representation include context-specific structure showbasic operations Section 4 required algorithms performed efficientlynew representation.8.1 Factored MDPs Context-specific Additive Structureseveral representations context-specific functions. commondecision trees (Boutilier et al., 1995), algebraic decision diagrams (ADDs) (Hoey, St-Aubin,Hu, & Boutilier, 1999), rules (Zhang & Poole, 1999). choose use rulesbasic representation, two main reasons. First, rule-based representation allowsfairly simple algorithm variable elimination, key operation framework.Second, rules required mutually exclusive exhaustive, requirementrestrictive want exploit additive independence, functionsrepresented linear combination set non-mutually exclusive functions.begin describing rule-based representation (along lines ZhangPooles presentation (1999)) probabilistic transition model, particular, CPDsDBN model. Roughly speaking, rule corresponds set CPD entriesassociated particular probability value. entriesvalue referred consistent contexts:Definition 8.1 Let C {X, X0 } c Dom(C). say c consistentb Dom(B), B {X, X0 }, c b assignment variablesC B.probability consistent contexts represented probability rules:437fiGuestrin, Koller, Parr & VenkataramanElectricalElectricalDonedoneDonedonePlumbingP(Painting) = 0PlumbingP(Painting) = 0doneDonedonePaintingP(Painting) = 0DonedoneP(Painting) = 0P(Painting) = 0.95P(Painting) = 0(a)DoneP(Painting) = 0.9(b)4 = hElectrical : 0i5 = hElectrical Plumbing : 0i6 = hElectrical Plumbing Painting : 0i7 = hElectrical Plumbing Painting : 0.9i(d)1 = hElectrical : 0i2 = hElectrical Plumbing : 0i3 = hElectrical Plumbing : 0.95i(c)Figure 11: Example CPDs variable Painting = true represented decision trees:(a) action paint; (b) action paint. CPDsrepresented probability rules shown (c) (d), respectively.Definition 8.2 probability rule = hc : pi function : {X, X0 } 7 [0, 1],context c Dom(C) C {X, X0 } p [0, 1], (x, x0 ) = p (x, x0 )consistent c equal 1 otherwise.case, convenient require rules mutually exclusive exhaustive, CPD entry uniquely defined association single rule.Definition 8.3 rule-based conditional probability distribution (rule CPD) Pa function Pa : ({Xi0 } X) 7 [0, 1], composed set probability rules {1 , 2 , . . . , } whosecontexts mutually exclusive exhaustive. define:Pa (x0i | x) = j (x, x0 ),j unique rule Pa cj consistent (x0i , x). require that,x,XPa (x0i | x) = 1.x0idefine Parentsa (Xi0 ) union contexts rules Pa (Xi0 | X).example CPD represented set probability rules shown Figure 11.Rules also used represent additive functions, reward basis functions.represent context specific value dependencies using value rules:438fiEfficient Solution Algorithms Factored MDPsDefinition 8.4 value rule = hc : vi function : X 7 R (x) = vx consistent c 0 otherwise.Note value rule hc : vi scope C.important note value rules required mutually exclusiveexhaustive. value rule represents (weighted) indicator function, takesvalue v states consistent context c, 0 states. given state,values zero rules consistent state simply added together.Example 8.5 construction example, might set rules:1 = hPlumbing = done : 100i;2 = hElectricity = done : 100i;3 = hPainting = done : 100i;4 = hAction = plumb : 10i;...which, summed together, define reward function R = 1 + 2 + 3 + 4 + .general, reward function Ra represented rule-based function:Definition 8.6 rule-based function f : X 7 R composed set rules {1 , . . . , n }Pf (x) = ni=1 (x).manner, one basis functions hj represented rule-basedfunction.notion rule-based function related tree-structure functions usedBoutilier et al. (2000), substantially general. tree-structure value functions, rules corresponding different leaves mutually exclusive exhaustive.Thus, total number different values represented tree equal numberleaves (or rules). rule-based function representation, rules mutuallyexclusive, values added form overall function value different settingsvariables. Different rules added different settings, and, fact, k rules,one easily generate 2k different possible values, demonstrated Section 9. Thus,rule-based functions provide compact representation much richer classvalue functions.Using rule-based representation, exploit CSI additive independencerepresentation factored MDP basis functions. show basicoperations Section 4 adapted exploit rule-based representation.8.2 Adding, Multiplying Maximizing Consistent Rulestable-based algorithms, relied standard sum product operators appliedtables. order exploit CSI using rule-based representation, must redefinestandard operations. particular, algorithms need add multiply rulesascribe values overlapping sets states.start defining operations rules context:439fiGuestrin, Koller, Parr & VenkataramanDefinition 8.7 Let 1 = hc : v1 2 = hc : v2 two rules context c. Definerule product 1 2 = hc : v1 v2 i, rule sum 1 + 2 = hc : v1 + v2 i.Note definition restricted rules context. addressissue moment. First, introduce additional operation maximizesvariable set rules, otherwise share common context:Definition 8.8 Let variable Dom[Y ] = {y1 , . . . , yk }, let , =1, . . . , k, rule form = hc = yi : vi i. rule-based functionf = 1 + + k , define rule maximization maxY f = hc : maxi vi .operation, maximized scope function f .three operations described applied sets rulessatisfy stringent conditions. make set rules amenable applicationoperations, might need refine rules. therefore definefollowing operation:Definition 8.9 Let = hc : vi rule, variable. Define rule splitSplit(6 ) variable follows: Scope[C], Split(6 ) = {};otherwise,Split(6 ) = {hc = yi : vi | yi Dom[Y ]} .Thus, split rule variable scope context ,generate new set rules, one assignment domain .general, purpose rule splitting extend context c one rule coincidecontext c0 another consistent rule 0 . Naively, might take variablesScope[C0 ] Scope[C] split recursively one them. However, processcreates unnecessarily many rules: variable Scope[C0 ] Scope[C] split, one |Dom[Y ]| new rules generated remain consistent 0 :one assignment one c0 . Thus, consistent ruleneeds split further. define recursive splitting procedure achievesparsimonious representation:Definition 8.10 Let = hc : vi rule, b context b Dom[B].Define recursive rule split Split(6 b) context b follows:1. {}, c consistent b; else,2. {}, Scope[B] Scope[C]; else,3. {Split(i 6 b) | Split(6 )}, variable Scope[B] Scope[C] .definition, variable Scope[B] Scope[C] leads generation k =|Dom(Y )| rules step split. However, one k rules usednext recursive step one consistent b. Therefore, sizePsplit set simply 1 + Scope[B]Scope[C] (|Dom(Y )| 1). size independentorder variables split within operation.440fiEfficient Solution Algorithms Factored MDPsNote one rules Split(6 b) consistent b: one contextc b. Thus, want add two consistent rules 1 = hc1 : v1 2 = hc2 : v2 i,need replace rules set:Split(1 6 c2 ) Split(2 6 c1 ),simply replace resulting rules hc1 c2 : v1 hc2 c1 : v2 sumhc1 c2 : v1 + v2 i. Multiplication performed analogous manner.Example 8.11 Consider adding following set consistent rules:1 = ha b : 5i,2 = ha c : 3i.rules, context c1 1 b, context c2 2 c d.Rules 1 2 consistent, therefore, must split perform additionoperation:ha b c : 5i,ha b c : 5i,Split(1 6 c2 ) =ha b c : 5i.Likewise,(Split(2 6c1 ) =ha b c : 3i,ha b c : 3i.result adding rules 1 2ha b c : 5i,ha b c : 5i,ha b c : 8i,ha b c : 3i.8.3 Rule-based One-step LookaheadUsing compact rule-based representation, able compute one-step lookaheadplan efficiently models significant context-specific additive independence.Section 4.1 table-based case, rule-based Qa function representedsum reward function discounted expected value next state.Due linear approximation value function, expectation term is, turn,represented linear combination backprojections basis functions.exploit CSI, representing rewards basis functions rule-based functions.represent Qa rule-based function, sufficient us show representbackprojection gj basis function hj rule-based function.P (h )hj rule-basedfunction,written hj (x) = j (x),E(h )(h )(h )j form ci j : vi j . rule restricted scope function; thus,simplify backprojection as:441fiGuestrin, Koller, Parr & VenkataramanRuleBackproja () ,given hc : vi, c Dom[C].Let g = {}.Select set P relevant probability rules:P = {j P (Xi0 | Parents(Xi0 )) | Xi0 C c consistent cj }.Remove X0 assignments context rules P.// Multiply consistent rules:two consistent rules 1 = hc1 : p1 2 = hc2 : p2 i:c1 = c2 , replace two rules hc1 : p1 p2 i;Else replace two rules set: Split(1 6 c2 ) Split(2 6 c1 ).// Generate value rules:rule P:Update backprojection g = g {hci : pi vi}.Return g.Figure 12: Rule-based backprojection.gja (x) =XPa (x0 | x)hj (x0 ) ;x0=XPa (x0 | x)x0=XX=X (hj )(x0 );(hj )0Pa (x | x)i(x0 );x0X (hj )vi(hj )Pa (ci| x);(h )(h )term vi j Pa (ci j | x) written rule function. denote back(h )projection operation RuleBackproja (i j ).backprojection procedure, described Figure 12, follows three steps. First,relevant rules selected: CPDs variables appear context ,select rules consistent context, rules play rolebackprojection computation. Second, multiply consistent probability rulesform local set mutually-exclusive rules. procedure analogous additionprocedure described Section 8.2. represented probabilitiesaffect mutually-exclusive set, simply represent backprojectionproduct probabilities value . is, backprojectionrule-based function one rule one mutually-exclusive probability rules. context new value rule , value productprobability value .Example 8.12 example, consider backprojection simple rule,= h Painting = done : 100i,CPD Figure 11(c) paint action:RuleBackprojpaint () =XPpaint (x0 | x)(x0 );x0442fiEfficient Solution Algorithms Factored MDPsX=Ppaint (Painting0 | x)(Painting0 );Painting0= 1003(Painting = done, x) .i=1Note product simple rules equivalent decision tree CPD shownFigure 11(a). Hence, product equal 0 contexts, example, electricitydone time t. product non-zero one context: context associatedrule 3 . Thus, express result backprojection operation rule-basedfunction single rule:RuleBackprojpaint () = hPlumbing Electrical : 95i.Similarly, backprojection action paint also representedsingle rule:RuleBackprojpaint () = hPlumbing Electrical Painting : 90i.Using algorithm, write backprojection rule-based basis function hj as:gja (x) =X(hj )RuleBackproja (i),(23)gja sum rule-based functions, therefore also rule-based function.simplicity notation, use gja = RuleBackproja (hj ) refer definition backproPjection. Using notation, write Qa (x) = Ra (x) + j wj gja (x),rule-based function.8.4 Rule-based Maximization State Spacesecond key operation required extend planning algorithms exploit CSImodify variable elimination algorithm Section 4.2.1 handle rule-based representation. Section 4.2.1, showed maximization linear combinationtable-based functions restricted scope performed efficiently using non-serialdynamic programming (Bertele & Brioschi, 1972), variable elimination. exploit structure rules, use algorithm similar variable elimination Bayesian networkcontext-specific independence (Zhang & Poole, 1999).Intuitively, algorithm operates selecting value rules relevant variablemaximized current iteration. Then, local maximization performedsubset rules, generating new set rules without current variable.procedure repeated recursively variables eliminated.precisely, algorithm eliminates variables one one, elimination process performs maximization step variables domain. Supposeeliminating Xi , whose collected value rules lead rule function f , f involvesadditional variables set B, f scope B {Xi }. need computemaximum value Xi choice b Dom[B]. use MaxOut (f, Xi ) denote procedure takes rule function f (B, Xi ) returns rule function g(B)443fiGuestrin, Koller, Parr & VenkataramanMaxOut (f, B)Let g = {}.Add completing rules f : hB = bi : 0i, = 1, . . . , k.// Summing consistent rules:two consistent rules 1 = hc1 : v1 2 = hc2 : v2 i:c1 = c2 , replace two rules hc1 : v1 + v2 i;Else replace two rules set: Split(1 6 c2 ) Split(2 6 c1 ).// Maximizing variable B:Repeat f empty:rules hc B = bi : vi i, bi Dom(B) :remove rules f add rule hc : maxi vi g;Else select two rules: = hci B = bi : vi j = hcj B = bj : vjci consistent cj , identical, replaceSplit(i 6 cj ) Split(j 6 ci ) .Return g.Figure 13: Maximizing variable B rule function f .that: g(b) = maxxi f (b, xi ). procedure extension variable eliminationalgorithm Zhang Poole (Zhang & Poole, 1999).rule-based variable elimination algorithm maintains set F value rules, initiallycontaining set rules maximized. algorithm repeats following stepsvariable Xi variables eliminated:1. Collect rules depend Xi fi fi = {hc : vi F | Xi C}remove rules F.2. Perform local maximization step Xi : gi = MaxOut (fi , Xi );3. Add rules gi F; now, Xi eliminated.cost algorithm polynomial number new rules generatedmaximization operation MaxOut (fi , Xi ). number rules never larger manycases exponentially smaller complexity bounds table-based maximizationSection 4.2.1, which, turn, exponential induced width cost networkgraph (Dechter, 1999). However, computational costs involved managing sets rulesusually imply computational advantage rule-based approach tablebased one significant problems possess fair amount context-specificstructure.remainder section, present algorithm computing localmaximization MaxOut (fi , Xi ). next section, show ideas appliedextending algorithm Section 4.2.2 exploit CSI LP representationplanning factored MDPs.procedure, presented Figure 13, divided two parts: first, consistentrules added together described Section 8.2; then, variable B maximized.maximization performed generating set rules, one assignment B, whosecontexts assignment variables except B, Definition 8.8.set substituted single rule without B assignment context valueequal maximum values rules original set. Note that, simplify444fiEfficient Solution Algorithms Factored MDPsalgorithm, initially need add set value rules 0 value, guaranteerule function f complete (i.e., least one rule consistent everycontext).correctness procedure follows directly correctness rule-basedvariable elimination procedure described Zhang Poole, merely replacing summations product max, products products sums. concludesection small example illustrate algorithm:Example 8.13 Suppose maximizing following set rules:1234= ha : 1i,= ha b : 2i,= ha b c : 3i,= ha b : 1i.add completing rules, get:5 = ha : 0i,6 = ha : 0i.first part algorithm, need add consistent rules: add 5 1 (whichremains unchanged), combine 1 4 , 6 2 , split 6 context3 , get following inconsistent set rules:23789= ha b : 2i,= ha b c : 3i,= ha b : 2i,(from adding 4 consistent rule Split(1 6 b))= ha b : 1i,(from Split(1 6 b))= ha b c : 0i,(from Split(6 6 b c)).Note several rules value 0 also generated, shownadded rules consistent contexts. move second stage (repeat loop)MaxOut. remove 2 , 8 , maximize them, give:10 = hb : 2i.select rules 3 7 split 7 c (3 split empty setchanged),11 = ha b c : 2i,12 = ha b c : 2i.Maximizing rules 12 3 , get:13 = hb c : 3i.left 11 , maximized counterpart 9 gives12 = hb c : 2i.Notice that, throughout maximization, split variable C b ci ,giving us 6 distinct rules final result. possible table-basedrepresentation, since functions would 3 variables a,b,c, thereforemust 8 entries.445fiGuestrin, Koller, Parr & Venkataraman8.5 Rule-based Factored LPSection 4.2.2, showed LPs used algorithms exponentially manyPconstraints form: wi ci (x) b(x), x, substituted single,Pequivalent, non-linear constraint: maxx wi ci (x) b(x). showed that, usingvariable elimination, represent non-linear constraint equivalent setlinear constraints construction called factored LP. number constraintsfactored LP linear size largest table generated variable eliminationprocedure. table-based algorithm exploit additive independence.extend algorithm Section 4.2.2 exploit additive context-specific structure,using rule-based variable elimination described previous section.Suppose wish enforce general constraint 0 maxy F w (y), F w (y) =P wj fj (y) fj rule. table-based version, superscript w meansfj might depend w. Specifically, fj comes basis function hi , multipliedweight wi ; fj rule reward function, not.rule-based factored linear program, generate LP variables associatedcontexts; call LP rules. LP rule form hc : ui; associatedcontext c variable u linear program. begin transforming originalrules fjw LP rules follows: rule fj form hcj : vj comes basisfunction hi , introduce LP rule ej = hcj : uj equality constraint uj = wi vj .fj form comes reward function, introduce LP ruleform, equality constraint becomes uj = vj .PNow, LP rules need represent constraint: 0 maxy j ej (y).represent constraint, follow algorithm similar variable elimination procedure Section 8.4. main difference occurs MaxOut (f, B) operationFigure 13. Instead generating new value rules, generate new LP rules, associatednew variables new constraints. simplest case occurs computing splitadding two LP rules. example, add two value rules original algorithm,instead perform following operation associated LP rules: LP ruleshc : ui hc : uj i, replace new rule hc : uk i, associated new LPvariable uk context c, whose value ui + uj . enforce value constraint,simply add additional constraint LP: uk = ui + uj . similar procedurefollowed computing split.interesting constraints generated perform maximization.rule-based variable elimination algorithm Figure 13, maximization occursreplace set rules:hc B = bi : vi i, bi Dom(B),new rulec : max vi .Following process LP rule summation above, maximizingei = hc B = bi : ui i, bi Dom(B),generate new LP variable uk associated rule ek = hc : uk i. However,cannot add nonlinear constraint uk = maxi ui , add set equivalent linear446fiEfficient Solution Algorithms Factored MDPsconstraintsuk ui , i.Therefore, using simple operations, exploit structure rule functionsPrepresent nonlinear constraint en maxy j ej (y), en last LPrule generate. final constraint un = implies representing exactlyconstraints Equation (12), without enumerate every state.correctness rule-based factored LP construction corollary Theorem 4.4correctness rule-based variable elimination algorithm (Zhang & Poole,1999) .Corollary 8.14 constraints generated rule-based factored LP constructionequivalent non-linear constraint Equation (12). is, assignment (, w)satisfies rule-based factored LP constraints satisfies constraintEquation (12).number variables constraints rule-based factored LP linearnumber rules generated variable elimination process. turn, number ruleslarger, often exponentially smaller, number entries table-basedapproach.illustrate generation LP constraints described, present smallexample:Example 8.15 Let e1 , e2 , e3 , e4 set LP rules depend variableb maximized. Here, rule ei associated LP variable ui :e1e2e3e4= ha b : u1 i,= ha b c : u2 i,= ha b : u3 i,= ha b c : u4 i.set, note rules e1 e2 consistent. combine generatefollowing rules:e5 = ha b c : u5 i,e6 = ha b c : u1 i.constraint u1 + u2 = u5 . Similarly, e6 e4 may combined, resulting in:e7 = ha b c : u6 i.constraint u6 = u1 + u4 . Now, following three inconsistent rulesmaximization:e3 = ha b : u3 i,e5 = ha b c : u5 i,e7 = ha b c : u6 i.Following maximization procedure, since pair rules eliminated right away,split e3 e5 generate following rules:e8 = ha b c : u3 i,e9 = ha b c : u3 i,e5 = ha b c : u5 i.447fiGuestrin, Koller, Parr & Venkataramanmaximize b e8 e5 , resulting following rule constraintsrespectively:e10 = ha c : u7 i,u7 u5 ,u7 u3 .Likewise, maximizing b e9 e6 , get:e11 = ha c : u8 i,u8 u3 ,u8 u6 ;completes elimination variable b rule-based factored LP.presented algorithm exploiting additive context-specific structure LP construction steps planning algorithms. rule-based factored LPapproach applied directly approximate linear programming approximate policy iteration algorithms, presented Sections 5 6.additional modification required concerns manipulation decisionlist policies presented Section 6.2. Although approximate linear programmingrequire explicit policy representation (or default action model), approximate policy iteration require us represent policy. Fortunately, major modificationsrequired rule-based case. particular, conditionals hti , ai , decisionlist policies already context-specific rules. Thus, policy representation algorithmSection 6.2 applied directly new rule-based representation. Therefore,complete framework exploiting additive context-specific structureefficient planning factored MDPs.9. Experimental Resultsfactored representation value function appropriate certain typessystems: Systems involve many variables, strong interactionsvariables fairly sparse, decoupling influence variablesinduce unacceptable loss accuracy. argued Herbert Simon (1981)Architecture Complexity, many complex systems nearly decomposable,hierarchical structure, subsystems interacting weakly themselves.evaluate algorithm, selected problems believe exhibit type structure.section, perform various experiments intended explore performancealgorithms. First, compare factored approximate linear programming (LP)approximate policy iteration (PI) algorithms. also compare L2 -projectionalgorithm Koller Parr (2000). second evaluation compares table-based implementation rule-based implementation exploit CSI. Finally, presentcomparisons approach algorithms Boutilier et al. (2000).9.1 Approximate LP Approximate PIorder compare approximate LP approximate PI algorithms, testedSysAdmin problem described detail Section 2.1. problem relates system448fiEfficient Solution Algorithms Factored MDPsadministrator maintain network computers; experimented variousnetwork architectures, shown Figure 1. Machines fail randomly, faulty machineincreases probability neighboring machines fail. every time step,SysAdmin go one machine reboot it, causing working next timestep high probability. Recall state space problem grows exponentiallynumber machines network, is, problem machines 2m states.machine receives reward 1 working (except ring, one machinereceives reward 2, introduce asymmetry), zero reward given faultymachines, discount factor = 0.95. optimal strategy rebooting machinesdepend upon topology, discount factor, status machinesnetwork. machine machine j faulty, benefit rebooting mustweighed expected discounted impact delaying rebooting j js successors.topologies rings, policy may function status every singlemachine network.basis functions used included independent indicators machine, value1 working zero otherwise (i.e., one restricted scope function singlevariable), constant basis, whose value 1 states. selected straightforwardvariable elimination orders: Star Three Legs topologies, first eliminatedvariables corresponding computers legs, center computer (server)eliminated last; Ring, started arbitrary computer followed ringorder; Ring Star, ring machines eliminated first center one;finally, Ring Rings topology, eliminated computers outer ringsfirst ones inner ring.implemented factored policy iteration linear programming algorithmsMatlab, using CPLEX LP solver. Experiments performed Sun UltraSPARCII, 359 MHz 256MB RAM. evaluate complexity approximate policyiteration max-norm projection algorithm, tests performed increasingnumber states, is, increasing number machines network. Figure 14 showsrunning time increasing problem sizes, various architectures. simplest oneStar, backprojection basis function scope restricted twovariables largest factor cost network scope restricted two variables.difficult one Bidirectional Ring, factors contain five variables.Note number states growing exponentially (indicated log scaleFigure 14), running times increase logarithmically number states,polynomially number variables. illustrate behavior Figure 14(d),fit 3rd order polynomial running times unidirectional ring. Notesize problem description grows quadratically number variables: addingmachine network also adds possible action fixing machine.problem,computationcost factored algorithm empirically grows approximately(n |A|)1.5 , problem n variables, opposed exponential complexitypoly (2n , |A|) explicit algorithm.evaluation, measured error approximate value function relativetrue optimal value function V . Note possible compute V smallproblems; case, able go 10 machines. comparison,also evaluated error approximate value function produced L2 -projection449fiGuestrin, Koller, Parr & Venkataraman500400Ring3 Legs300Ring Rings300Total Time (minutes)Total Time (minutes)400Star200Ring Star200100100001E+001E+021E+041E+06 1E+08 1E+10number states1E+1211E+14100100001000000number states(a)12005001000Fitting polynomial:800time = 0.0184|X| - 0.6655|X| +9.2499|X| - 31.9223Ring:Total Time (minutes)Total Time (minutes)1E+10(b)600400100000000UnidirectionalBidirectional30020022Quality fit: R = 0.99960040010020001E+001E+021E+041E+061E+081E+101E+121E+1400number state(c)10203040number variables |X|5060(d)Figure 14: (a)(c) Running times policy iteration max-norm projection variantsSysAdmin problem; (d) Fitting polynomial running timeRing topology.algorithm Koller Parr (2000). discussed Section 6.4, L2 projectionsfactored MDPs Koller Parr difficult time consuming; hence,able compare two algorithms smaller problems, equivalent L2 -projectionimplemented using explicit state space formulation. Results algorithmspresented Figure 15(a), showing relative error approximate solutionstrue value function increasing problem sizes. results indicate that, largerproblems, max-norm formulation generates better approximation true optimalvalue function V L2 -projection. Here, used two types basis functions:single variable functions, pairwise basis functions. pairwise basis functionscontain indicators neighboring pairs machines (i.e., functions two variables).expected, use pairwise basis functions resulted better approximations.450fiEfficient Solution Algorithms Factored MDPs0.4Max norm, single basisL2, single basis0.3Bellman Error / RmaxMax norm, pair basisL2, pair basisRelative error:0.20.10345678910number variables0.30.2Ring3 Legs0.101E+00Star1E+021E+041E+061E+081E+101E+121E+14numbe r sta tes(a)(b)Figure 15: (a) Relative error optimal value function V comparison L2 projectionRing; (b) large models, measuring Bellman error convergence.small problems, also compare actual value policy generatedalgorithm value optimal policy. Here, value policy generatedalgorithm much closer value optimal policy error implieddifference approximate value function V . example, Stararchitecture one server 6 clients, approximation single variablebasis functions relative error 12%, policy generated valueoptimal policy. case, true policy generated L2projection. Unidirectional Ring 8 machines pairwise basis, relativeerror approximation V 10%, resulting policy6% loss optimal policy. problem, L2 approximation valuefunction error 12%, true policy loss 9%. words, methods inducepolicies lower errors errors approximate value function (at leastsmall problems). However, algorithm continues outperform L2 algorithm,even respect actual policy loss.large models, longer compute correct value function, cannotevaluate results computing kV Hwk . Fortunately, discussed Section 7,Bellman error used provide bound approximation errorcomputed efficiently exploiting problem-specific structure. Figure 15(b) showsBellman error increases slowly number states.also valuable look actual decision-list policies generated experiments.First, noted lists tended short, length final decision list policygrew approximately linearly number machines. Furthermore, policyoften fairly intuitive. Ring Star architecture, example, decision listsays: server faulty, fix server; else, another machine faulty, fix it.Thus far, presented scaling results running times approximation errorapproximate PI approach. compare algorithm simpler approximate451fiGuestrin, Koller, Parr & Venkataraman400200PI single basisPI single basis160LP single basis140LP pair basis120LP triple basisDiscounted reward final policy(averaged 50 trials 100 steps)Total running time (minutes)18010080604020005101520253035numbe r machineLP single basisLP pair basis300LP triple basis2001000010203040numbe r machine(a)(b)Figure 16: Approximate LP versus approximate PI SysAdmin problem Ringtopology: (a) running time; (b) estimated value policy.LP approach Section 5. shown Figure 16(a), approximate LP algorithmfactored MDPs significantly faster approximate PI algorithm. fact, approximate PI single-variable basis functions variables costly computationallyLP approach using basis functions consecutive triples variables. shownFigure 16(b), singleton basis functions, approximate PI policy obtains slightly betterperformance problem sizes. However, increase number basis functionsapproximate LP formulation, value resulting policy much better. Thus,problem, factored approximate linear programming formulation allows us usebasis functions obtain resulting policy higher value, still maintainingfaster running time. results, along simpler implementation, suggestpractice one may first try apply approximate linear programming algorithmdeciding move elaborate approximate policy iteration approach.9.2 Comparing Table-based Rule-based Implementationsnext evaluation compares table-based representation, exploits additiveindependence, rule-based representation presented Section 8, exploitadditive context-specific independence. experiments, implementedfactored approximate linear programming algorithm table-based rule-basedrepresentations C++, using CPLEX LP solver. Experiments performedSun UltraSPARC-II, 400 MHz 1GB RAM.evaluate compare algorithms, utilized complex extensionSysAdmin problem. problem, dubbed Process-SysAdmin problem, contains threestate variables machine network: Loadi , Statusi Selectori . computer runs processes receives rewards processes terminate. processesrepresented Loadi variable, takes values {Idle, Loaded, Success},computer receives reward assignment Loadi Success. Statusi variable,452fitotal running time (minutes)Efficient Solution Algorithms Factored MDPs200Table-based, single+ basisRule-based, single+ basis150Table-based, pair basis100Rule-based, pair basis5001E+001E+071E+141E+211E+281E+351E+42number statestotal running time (minutes)(a)250200Table-based, single+ basisRule-based, single+ basis150Table-based, pair basis100Rule-based, pair basis5001E+00 1E+04 1E+08 1E+12 1E+16 1E+20 1E+24 1E+28number states(b)total running time (minutes)6002(x-1)(x-1)= 7E- 17 x * 18 + 2E- 06 x * 18+ 0.11242R = 0.995500Table-based, single+ basisRule-based, single+ basis40030032= 0.2294x - 4.5415x +30.974x - 67.851R 2= 0.999520010000510number machines1520(c)Figure 17: Running time Process-SysAdmin problem various topologies: (a) Star;(b) Ring; (c) Reverse star (with fit function).453fiGuestrin, Koller, Parr & VenkataramanCPLEX time / Total time10.8Table-based, single+ basis0.6Rule-based, single+ basis0.40.2005101520number machinesFigure 18: Fraction total running time spent CPLEX Process-SysAdmin problem Ring topology.representing status machine i, takes values {Good, Faulty, Dead}; valueFaulty, processes smaller probability terminating value Dead,running process lost Loadi becomes Idle. status machine become Faulty eventually Dead random; however, machine receives packetdead machine, probability Statusi becomes Faulty Dead increases.Selectori variable represents communication selecting one neighborsuniformly random every time step. SysAdmin select one computerreboot every time step. computer rebooted, status becomes Goodprobability 1, running process lost, i.e., Loadi variable becomes Idle.Thus, problem, SysAdmin must balance several conflicting goals: rebootingmachine kills processes, rebooting machine may cause cascading faults network.Furthermore, SysAdmin choose one machine reboot, imposes additional tradeoff selecting one (potentially many) faulty dead machinesnetwork reboot.experimented two types basis functions: single+ includes indicatorsjoint assignments Loadi , Statusi Selectori , pair which, addition,includes set indicators Statusi , Statusj , Selectori = j, neighbor jmachine network. discount factor = 0.95. variable eliminationorder eliminated Loadi variables first, followed patternssimple SysAdmin problem, eliminating first Statusi Selectori machineeliminated.Figure 17 compares running times table-based implementation onesrule-based representation three topologies: Star, Ring, Reverse star.Reverse star topology reverses direction influences Star: rathercentral machine influencing machines topology, machines influencecentral one. three topologies demonstrate three different levels CSI:454fiEfficient Solution Algorithms Factored MDPsStar topology, factors generated variable elimination small. Thus, althoughrunning times polynomial number state variables methods, tablebased representation significantly faster rule-based one, due overheadmanaging rules. Ring topology illustrates intermediate behavior: single+basis functions induce relatively small variable elimination factors, thus table-basedapproach faster. However, pair basis factors larger rule-basedapproach starts demonstrate faster running times larger problems. Finally, Reverse star topology represents worst-case scenario table-based approach. Here,scope backprojection basis function central machine involvecomputers network, machines potentially influence central onenext time step. Thus, size factors table-based variable elimination approach exponential number machines network, illustratedexponential growth Figure 17(c). rule-based approach exploit CSIproblem; example, status central machine Status0 depends machinej value selector j, i.e., Selector0 = j. exploiting CSI, solveproblem polynomial time number state variables, seen second curveFigure 17(c).also instructive compare portion total running time spent CPLEXtable-based compared rule-based approach. Figure 18 illustratescomparison. Note amount time spent CPLEX significantly highertable-based approach. two reasons difference: first, due CSI, LPsgenerated rule-based approach smaller table-based ones; second, rulebased variable elimination complex table-based one, due overheadintroduced rule management. Interestingly, proportion CPLEX time increasesproblem size increases, indicating asymptotic complexity LP solutionhigher variable elimination, thus suggesting that, larger problems, additionallarge-scale LP optimization procedures, constraint generation, may helpful.9.3 Comparison Apricoddclosely related work line research began workBoutilier et al. (1995). particular, approximate Apricodd algorithm Hoey etal. (1999), uses analytic decision diagrams (ADDs) represent value functionstrong alternative approach solving factored MDPs. discussed detail Section 10, Apricodd algorithm successfully exploit context-specific structurevalue function, representing set mutually-exclusive exhaustive branchesADD. hand, approach exploit additive context-specificstructure problem, using linear combination non-mutually-exclusive rules.better understand difference, evaluated rule-based approximate linearprogramming algorithm Apricodd two problems, Linear Expon, designedBoutilier et al. (2000) illustrate respectively best-case worst-case behavioralgorithm. experiments, used web-distributed version Apricodd (Hoey, St-Aubin, Hu, & Boutilier, 2002), running locally Linux Pentium III700MHz 1GB RAM.455fiGuestrin, Koller, Parr & Venkataraman500Rule-based403032= 0.1473x - 0.8595x + 2.5006x - 1.59642R = 0.999720Apricodd2= 0.0254x + 0.0363x+ 0.072510Apricodd400Time (in seconds)Time (in seconds)50x2x= 3E-05 * 2 - 0.0026 * 2 + 5.6737R2 = 0.9999300200Rule-based= 5.275x3 - 29.95x2 +53.915x - 28.83R2 = 11002R = 0.9983006810121416Number variables1862081012Number variables(a)(b)Figure 19: Comparing Apricodd rule-based approximate linear programming (a)Linear (b) Expon problems.two problems involve n binary variables X1 , . . . , Xn n deterministic actionsa1 , . . . , . reward 1 variables Xk true, 0 otherwise. problemdiscounted factor = 0.99. difference Linear Exponproblems transition probabilities. Linear problem, action ak setsvariable Xk true makes succeeding variables, Xi > k, false. state spaceLinear problem seen binary number, optimal policy set repeatedlylargest bit (Xk variable) preceding bits set true. Using ADD, optimalvalue function problem represented linear space, n+1 leaves (Boutilieret al., 2000). best-case Apricodd, algorithm compute valuefunction quite efficiently. Figure 19(a) compares running time Apricoddone algorithms indicator basis functions pairs consecutive variables.Note algorithms obtain policy polynomial time numbervariables. However, structured problems, efficient implementation ADDpackage used Apricodd makes faster problem.hand, Expon problem illustrates worst-case Apricodd.problem, action ak sets variable Xk true, preceding variables, Xi < k,true, makes preceding variables false. state space seen binary number,optimal policy goes binary numbers sequence, repeatedly settinglargest bit (Xk variable) preceding bits set true. Due discounting,noptimal value function assigns value 2 j1 jth binary number,value function contains exponentially many different values. Using ADD, optimalvalue function problem requires exponential number leaves (Boutilier et al.,2000), illustrated exponential running time Figure 19(b). However,value function approximated compactly factored linear valuefunction using n + 1 basis functions: indicator variable Xk constantbase. shown Figure 19(b), using representation, factored approximate linearprogramming algorithm computes value function polynomial time. Furthermore,456fiEfficient Solution Algorithms Factored MDPs3060Running time (minutes)Discounted value policy(avg. 50 runs 100 steps)Rule-based LP50Apricodd403020100Rule-based LP25Apricodd2015105002468Number machines101202468Number machines(a)121012(b)503045Rule-based LPRule-based LP40Discounted value policy(avg. 50 runs 100 steps)Running time (minutes)10Apricodd35302520151025Apricodd201510550002468Number machines1012(c)02468Number machines(d)Figure 20: Comparing Apricodd rule-based approximate linear programming single+ basis functions Process-SysAdmin problem Ring topology(a) running time (b) value resulting policy; Star topology(c) running time (d) value resulting policy.policy obtained approach optimal problem. Thus, problem,ability exploit additive independence allows efficient polynomial time solution.also compared Apricodd rule-based approximate linear programmingalgorithm Process-SysAdmin problem. problem significant additive structure reward function factorization transition model. Although typestructure exploited directly Apricodd, ADD approximation steps performedalgorithm can, principle, allow Apricodd find approximate solutions problem. spent significant amount time attempting find best set parametersApricodd problems.4 settled sift method variable reorderinground approximation method size (maximum ADD size) criteria.4. grateful Jesse Hoey Robert St-Aubin assistance selecting parameters.457fiGuestrin, Koller, Parr & Venkataramanallow value function representation scale problem size, set maximumADD size 4000 + 400n network n machines. (We experimented varietydifferent growth rates maximum ADD size; here, parameters,selected choice gave best results Apricodd.) compared Apricoddparameters rule-based approximate linear programming algorithmsingle+ basis functions Pentium III 700MHz 1GB RAM. resultssummarized Figure 20.small problems (up 45 machines), performance two algorithmsfairly similar terms running time quality policies generated.However, problem size grows, running time Apricodd increases rapidly,becomes significantly higher algorithm . Furthermore, problem sizeincreases, quality policies generated Apricodd also deteriorates. differencepolicy quality caused different value function representation used twoalgorithms. ADDs used Apricodd represent k different values k leaves; thus,forced agglomerate many different states represent using single value.smaller problems, agglomeration still represent good policies. Unfortunately,problem size increases state space grows exponentially, Apricodds policyrepresentation becomes inadequate, quality policies decreases.hand, linear value functions represent exponentially many values k basisfunctions, allows approach scale significantly larger problems.10. Related Workclosely related work line research began workBoutilier et al. (1995). address comparison separately below, beginsection broader background references.10.1 Approximate MDP Solutionsfield MDPs, popularly known, formalized Bellman (1957)1950s. importance value function approximation recognized early stageBellman (1963). early 1990s MDP framework recognized AIresearchers formal framework could used address problem planninguncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993).Within AI community, value function approximation developed concomitantlynotion value function representations Markov chains. Suttons seminal papertemporal difference learning (1988), addressed use value functions predictionplanning, assumed general representation value function notedconnection general function approximators neural networks. However,stability combination directly addressed time.Several important developments gave AI community deeper insight relationship function approximation dynamic programming. Tsitsiklis VanRoy (1996a) and, independently, Gordon (1995) popularized analysis approximateMDP methods via contraction properties dynamic programming operatorfunction approximator. Tsitsiklis Van Roy (1996b) later established general convergence result linear value function approximators D(), Bertsekas458fiEfficient Solution Algorithms Factored MDPsTsitsiklis (1996) unified large body work approximate dynamic programmingname Neuro-dynamic Programming, also providing many novel general erroranalyses.Approximate linear programming MDPs using linear value function approximationintroduced Schweitzer Seidmann (1985), although approach somewhatdeprecated fairly recently due lack compelling error analyses lackeffective method handling large number constraints. Recent work de FariasVan Roy (2001a, 2001b) started address concerns new error boundsconstraint sampling methods. approach, rather sampling constraints, utilizesstructure model value function represent constraints compactly.10.2 Factored ApproachesTatman Shachter (1990) considered additive decomposition value nodes influence diagrams. number approaches factoring general MDPs exploredliterature. Techniques exploiting reward functions decompose additivelystudied Meuleau et al. (1998), Singh Cohn (1998).use factored representations dynamic Bayesian networks pioneeredBoutilier et al. (1995) developed steadily recent years. methods relyuse context-specific structures decision trees analytic decision diagrams(ADDs) (Hoey et al., 1999) represent transition dynamics DBNvalue function. algorithms use dynamic programming partition state space,representing partition using tree-like structure branches state variablesassigns values leaves. tree grown dynamically part dynamic programming process algorithm creates new leaves needed: leaf splitapplication DP operator two states associated leaf turndifferent values backprojected value function. process also interpretedform model minimization (Dean & Givan, 1997).number leaves tree used represent value function determines computational complexity algorithm. also limits number distinct valuesassigned states: since leaves represent partitioning state space, every statemaps exactly one leaf. However, recognized early on, trivial MDPsrequire exponentially large value functions. observation led line approximationalgorithms aimed limiting tree size (Boutilier & Dearden, 1996) and, later, limitingADD size (St-Aubin, Hoey, & Boutilier, 2001). Kim Dean (2001) also exploredtechniques discovering tree-structured value functions factored MDPs.methods permit good approximate solutions large MDPs, complexity stilldetermined number leaves representation number distinct valuesassigned states still limited well.Tadepalli Ok (1996) first apply linear value function approximationFactored MDPs. Linear value function approximation potentially expressiveapproximation method assign unique values every state MDP withoutrequiring storage space exponential number state variables. expressivepower tree k leaves captured linear function approximator k basisfunctions basis function hi indicator function tests state belongs459fiGuestrin, Koller, Parr & Venkataramanpartition leaf i. Thus, set value functions representedtree k leaves subset set value functions representedvalue function k basis functions. experimental results Section 9.3 highlightdifference showing example problem requires exponentially many leavesvalue function, approximated well using linear value function.main advantage tree-based value functions structure determineddynamically solution MDP. principle, value function representation derived automatically model description, approach requires less insightuser. problems value function well approximated relatively small number values, approach provides excellent solution problem.method linear value function approximation aims address believecommon case, large range distinct values required achieve goodapproximation.Finally, note Schuurmans Patrascu (2001), based earlier workmax-norm projection using cost networks linear programs, independently developedalternative approach approximate linear programming using cost network.method embeds cost network inside single linear program. contrast, methodbased constraint generation approach, using cost network detect constraintviolations. constraint violations found, new constraint added, repeatedlygenerating attempting solve LPs feasible solution found. Interestingly,approach Schuurmans Patrascu uses multiple calls variable eliminationorder speed LP solution step, successful time spentsolving LP significantly larger time required variable elimination.suggested Section 9.2, LP solution time larger table-based approach. Thus,Schuurmans Patrascus constraint generation method probably successfultable-based problems rule-based ones.11. Conclusionspaper, presented new algorithms approximate linear programming approximate dynamic programming (value policy iteration) factored MDPs.algorithms leverage novel LP decomposition technique, analogous variable elimination cost networks, reduces exponentially large LP provablyequivalent, polynomial-sized one.approximate dynamic programming algorithms motivated error analysesshowing importance minimizing L error. algorithms efficientsubstantially easier implement previous algorithms based L2 -projection.experimental results suggest also perform better practice.approximate linear programming algorithm factored MDPs simpler, easierimplement general dynamic programming approaches. Unlike policyiteration algorithm, rely default action assumption, statesactions affect small number state variables. Although algorithmtheoretical guarantees max-norm projection approaches, empirically seemsfavorable option. experiments suggest approximate policy iteration tendsgenerate better policies set basis functions. However, due computa460fiEfficient Solution Algorithms Factored MDPstional advantages, add basis functions approximate linear programmingalgorithm, obtaining better policy still maintaining much faster running timeapproximate policy iteration.Unlike previous approaches, algorithms exploit additive contextspecific structure factored MDP model. Typical real-world systems possesstypes structure. thus, feature algorithms increase applicability factored MDPs practical problems. demonstrated exploitingcontext-specific independence, using rule-based representation instead standardtable-based one, yield exponential improvements computational time problem significant amounts CSI. However, overhead managing sets rules makeless well-suited simpler problems. also compared approach workBoutilier et al. (2000), exploits context-specific structure. problemssignificant context-specific structure value function, approach faster dueefficient handling ADD representation. However, problemssignificant context-specific structure problem representation, rather valuefunction, require exponentially large ADDs. problems, demonstrated using linear value function algorithm obtain polynomial-timenear-optimal approximation true value function.success algorithm depends ability capture importantstructure value function using linear, factored approximation. ability, turn,depends choice basis functions properties domain.algorithms currently require designer specify factored basis functions.limitation compared algorithms Boutilier et al. (2000), fully automated.However, experiments suggest simple rules quite successful designing basis. First, ensure reward function representable basis.simple basis that, addition, contained separate set indicators variable oftenquite well. also add indicators pairs variables; simply, chooseaccording DBN transition model, indicator added variablesXi one variables Parents(Xi ), thus representing one-step influences.procedure extended, adding basis functions represent influencesrequired. Thus, structure DBN gives us indications choose basisfunctions. sources prior knowledge also included specifyingbasis.Nonetheless, general algorithm choosing good factored basis functions stillexist. However, potential approaches: First, problems CSI, onecould apply algorithms Boutilier et al. iterations generate partial treestructured solutions. Indicators defined variables backprojection leavescould, turn, used generate basis set problems. Second, Bellmanerror computation, performed efficiently shown Section 7,provide bound quality policy, also actual state errorlargest. knowledge used create mechanism incrementally increasebasis set, adding new basis functions tackle states high Bellman error.many possible extensions work. already pursued extensions collaborative multiagent systems, multiple agents act simultaneouslymaximize global reward (Guestrin et al., 2001b), factored POMDPs,461fiGuestrin, Koller, Parr & Venkataramanfull state observed directly, indirectly observation variables (Guestrin,Koller, & Parr, 2001c). However, settings remain explored.particular, hope address problem learning factored MDP planningcompetitive multiagent system.Additionally, paper tackled problems induced width costnetwork sufficiently low possess sufficient context-specific structure allowexact solution factored LPs. Unfortunately, practical problems mayprohibitively large induced width. plan leverage ideas loopy belief propagation algorithms approximate inference Bayesian networks (Pearl, 1988; Yedidia,Freeman, & Weiss, 2001) address issue.believe methods described herein significantly extend efficiency,applicability general usability factored models value functions controlpractical dynamic systems.Acknowledgementsgrateful Craig Boutilier, Dirk Ormoneit Uri Lerner many usefuldiscussions, anonymous reviewers detailed thorough comments.also would like thank Jesse Hoey, Robert St-Aubin, Alan Hu, Craig Boutilierdistributing algorithm useful assistance using Apricoddselecting parameters. work supported DoD MURI program, administered Office Naval Research Grant N00014-00-1-0637, Air Force contractF30602-00-2-0598 DARPAs TASK program, Sloan Foundation. firstauthor also supported Siebel Scholarship.Appendix A. ProofsA.1 Proof Lemma 3.3exists setting weights zero setting yields bounded maxnorm projection error P policy (P Rmax ). max-norm projection operatorchooses set weights minimizes projection error (t) policy (t) . Thus,projection error (t) must least low one given zero weights P(which bounded). Thus, error remains bounded iterations.A.2 Proof Theorem 3.5First, need bound approximation V(t) :V(t) Hw(t)T(t) Hw(t) Hw(t)+ V(t) T(t) Hw(t)(t)(t)T(t) Hw Hw + V(t) Hw(t); (triangle inequality;); (T(t) contraction.)Moving second term right hand side dividing 1 , obtain:V(t) Hw(t)1(t).T(t) Hw(t) Hw(t) =11462(24)fiEfficient Solution Algorithms Factored MDPsnext part proof, adapt lemma Bertsekas Tsitsiklis (1996, Lemma6.2, p.277) fit framework. manipulation, lemma reformulated as:kV V(t+1) k kV V(t) k +2V(t) Hw(t) .1(25)proof concluded substituting Equation (24) Equation (25) and, finally, induction t.A.3 Proof Theorem 4.4First, note equality constraints represent simple change variable. Thus,rewrite Equation (12) terms new LP variables ufzii as:maxXxufzii ,(26)assignment weights w implies assignment ufzii . stage,LP variables.remains show factored LP construction equivalent constraintEquation (26). system n variables {X1 , . . . , Xn }, assume, without lossgenerality, variables eliminated starting Xn X1 . proveequivalence induction number variables.base case n = 0, functions ci (x) b(x) Equation (12)empty scope. case, Equation (26) written as:Xuei .(27)case, transformation done constraint, equivalence immediate.Now, assume result holds systems i1 variables prove equivalencesystem variables. system, maximization decomposedtwo terms: one factors depend Xi , irrelevantmaximization Xi , another term factors depend Xi . Usingdecomposition, write Equation (26) as:maxX ejx1 ,...,xiuzj ;jmaxx1 ,...,xi1XXuezll + maxxil : xi 6zleuzjj .(28)j : xi zjpoint define new LP variables uez corresponding second termright hand side constraint. new LP variables must satisfy followingconstraint:uez maxxiX ejj=1463u(z,xi )[Zj ] .(29)fiGuestrin, Koller, Parr & Venkataramannew non-linear constraint represented factored LP constructionset equivalent linear constraints:uezX ejj=1u(z,xi )[Zj ] , z, xi .(30)equivalence non-linear constraint Equation (29) set linear constraints Equation (30) shown considering binding constraints. newLP variable created uez , |Xi | new constraints created, one value xi Xi .assignment LP variables right hand side constraint EquaPejtion (30), one |Xi | constraints relevant. is, one `j=1 u(z,x)[Zj ]maximal, corresponds maximum Xi . Again, value zone assignment Xi achieves maximum, (and only) constraintscorresponding maximizing assignments could binding. Thus, Equation (29)Equation (30) equivalent.Substituting new LP variables uez Equation (28), get:maxx1 ,...,xi1Xuezll + uez ,l : xi 6zldepend Xi anymore. Thus, equivalent system i1 variables,concluding induction step proof.A.4 Proof Lemma 7.1First note iteration + 1 objective function (t+1) max-norm projectionLP given by:(t+1) = Hw(t+1) R(t+1) + P(t+1) Hw(t+1) .However, convergence value function estimates equal iterations:w(t+1) = w(t) .that:(t+1) = Hw(t) R(t+1) + P(t+1) Hw(t) .operator notation, term equivalent to:(t+1) = Hw(t) T(t+1) Hw(t) .Note that, (t+1) = Greedy(Hw(t) ) definition. Thus, that:T(t+1) Hw(t) = Hw(t) .Finally, substituting previous expression, obtain result:(t+1) = Hw(t) Hw(t) .464fiEfficient Solution Algorithms Factored MDPsReferencesArnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity finding embeddingsK-tree. SIAM Journal Algebraic Discrete Methods, 8 (2), 277 284.Becker, A., & Geiger, D. (2001). sufficiently fast algorithm finding close optimalclique trees. Artificial Intelligence, 125 (1-2), 317.Bellman, R., Kalaba, R., & Kotkin, B. (1963). Polynomial approximation new computational technique dynamic programming. Math. Comp., 17 (8), 155161.Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, NewJersey.Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press, NewYork.Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific,Belmont, Massachusetts.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research, 11, 194.Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamicprogramming. Proc. ICML, pp. 5462.Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proc. IJCAI, pp. 11041111.Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programmingfactored representations. Artificial Intelligence, 121 (1-2), 49107.Cheney, E. W. (1982). Approximation Theory (2nd edition). Chelsea Publishing Co., NewYork, NY.de Farias, D., & Van Roy, B. (2001a). linear programming approach approximatedynamic programming. Submitted Operations Research.de Farias, D., & Van Roy, B. (2001b). constraint sampling linear programming approach approximate dynamic programming. appear MathematicsOperations Research.Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning deadlinesstochastic domains. Proceedings Eleventh National Conference ArtificialIntelligence (AAAI-93), pp. 574579, Washington, D.C. AAAI Press.Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.Computational Intelligence, 5 (3), 142150.Dean, T., & Givan, R. (1997). Model minimization Markov decision processes.Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97), pp. 106111, Providence, Rhode Island, Oregon. AAAI Press.Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.465fiGuestrin, Koller, Parr & VenkataramanDechter, R. (1999). Bucket elimination: unifying framework reasoning. ArtificialIntelligence, 113 (12), 4185.Gordon, G. (1995). Stable function approximation dynamic programming. ProceedingsTwelfth International Conference Machine Learning, pp. 261268, TahoeCity, CA. Morgan Kaufmann.Guestrin, C. E., Koller, D., & Parr, R. (2001a). Max-norm projections factored MDPs.Proceedings Seventeenth International Joint Conference Artificial Intelligence (IJCAI-01), pp. 673 680, Seattle, Washington. Morgan Kaufmann.Guestrin, C. E., Koller, D., & Parr, R. (2001b). Multiagent planning factored MDPs.14th Neural Information Processing Systems (NIPS-14), pp. 15231530, Vancouver,Canada.Guestrin, C. E., Koller, D., & Parr, R. (2001c). Solving factored POMDPs linear valuefunctions. Seventeenth International Joint Conference Artificial Intelligence(IJCAI-01) workshop Planning Uncertainty Incomplete Information,pp. 67 75, Seattle, Washington.Guestrin, C. E., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination planning factored MDPs. Eighteenth National ConferenceArtificial Intelligence (AAAI-2002), pp. 253259, Edmonton, Canada.Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision diagrams. Proceedings Fifteenth Conference UncertaintyArtificial Intelligence (UAI-99), pp. 279288, Stockholm, Sweden. Morgan Kaufmann.Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (2002). Stochastic planning using decisiondiagrams C implementation. http://www.cs.ubc.ca/spider/staubin/Spudd/.Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Howard, R. A., & Matheson, J. E. (Eds.), Readings Principles Applications Decision Analysis,pp. 721762. Strategic Decisions Group, Menlo Park, California.Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: PreferencesValue Tradeoffs. Wiley, New York.Kim, K.-E., & Dean, T. (2001). Solving factored Mdps using non-homogeneous partitioning. Proceedings Seventeenth International Joint Conference ArtificialIntelligence (IJCAI-01), pp. 683 689, Seattle, Washington. Morgan Kaufmann.Kjaerulff, U. (1990). Triangulation graphs algorithms giving small total state space.Tech. rep. TR R 90-09, Department Mathematics Computer Science, Strandvejen, Aalborg, Denmark.Koller, D., & Parr, R. (1999). Computing factored value functions policies structuredMDPs. Proceedings Sixteenth International Joint Conference ArtificialIntelligence (IJCAI-99), pp. 1332 1339. Morgan Kaufmann.Koller, D., & Parr, R. (2000). Policy iteration factored MDPs. ProceedingsSixteenth Conference Uncertainty Artificial Intelligence (UAI-00), pp. 326334, Stanford, California. Morgan Kaufmann.466fiEfficient Solution Algorithms Factored MDPsMeuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.(1998). Solving large weakly-coupled Markov decision processes. Proceedings15th National Conference Artificial Intelligence, pp. 165172, Madison, WI.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann, San Mateo, California.Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley, New York.Reed, B. (1992). Finding approximate separators computing tree-width quickly.24th Annual Symposium Theory Computing, pp. 221228. ACM.Schuurmans, D., & Patrascu, R. (2001). Direct value-approximation factored MDPs.Advances Neural Information Processing Systems (NIPS-14), pp. 15791586,Vancouver, Canada.Schweitzer, P., & Seidmann, A. (1985). Generalized polynomial approximations Markovian decision processes. Journal Mathematical Analysis Applications, 110, 568582.Simon, H. A. (1981). Sciences Artificial (second edition). MIT Press, Cambridge,Massachusetts.Singh, S., & Cohn, D. (1998). dynamically merge Markov decision processes.Jordan, M. I., Kearns, M. J., & Solla, S. A. (Eds.), Advances Neural InformationProcessing Systems, Vol. 10. MIT Press.St-Aubin, R., Hoey, J., & Boutilier, C. (2001). APRICODD: Approximate policy construction using decision diagrams. Advances Neural Information Processing Systems13: Proceedings 2000 Conference, pp. 10891095, Denver, Colorado. MIT Press.Stiefel, E. (1960). Note Jordan elimination, linear programming Tchebycheff approximation. Numerische Mathematik, 2, 1 17.Sutton, R. S. (1988). Learning predict methods temporal differences. MachineLearning, 3, 944.Tadepalli, P., & Ok, D. (1996). Scaling average reward reinforcmeent learning approximating domain models value function. Proceedings ThirteenthInternational Conference Machine Learning, Bari, Italy. Morgan Kaufmann.Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming influence diagrams.IEEE Transactions Systems, Man Cybernetics, 20 (2), 365379.Tsitsiklis, J. N., & Van Roy, B. (1996a). Feature-based methods large scale dynamicprogramming. Machine Learning, 22, 5994.Tsitsiklis, J. N., & Van Roy, B. (1996b). analysis temporal-difference learningfunction approximation. Technical report LIDS-P-2322, Laboratory InformationDecision Systems, Massachusetts Institute Technology.Van Roy, B. (1998). Learning Value Function Approximation Complex DecisionProcesses. Ph.D. thesis, Massachusetts Institute Technology.467fiGuestrin, Koller, Parr & VenkataramanWilliams, R. J., & Baird, L. C. I. (1993). Tight performance bounds greedy policies basedimperfect value functions. Tech. rep., College Computer Science, NortheasternUniversity, Boston, Massachusetts.Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. AdvancesNeural Information Processing Systems 13: Proceedings 2000 Conference,pp. 689695, Denver, Colorado. MIT Press.Zhang, N., & Poole, D. (1999). role context-specific independence probabilisticreasoning. Proceedings Sixteenth International Joint Conference ArtificialIntelligence (IJCAI-99), pp. 12881293. Morgan Kaufmann.468fiJournal Artificial Intelligence Research 19 (2003) 1-10Submitted 10/02; published 7/03Research NoteNew Polynomial Classes Logic-Based AbductionBruno Zanuttinizanutti@info.unicaen.frGREYC, Universite de Caen, Boulevard du Marechal Juin14032 Caen Cedex, FranceAbstractaddress problem propositional logic-based abduction, i.e., problemsearching best explanation given propositional observation according givenpropositional knowledge base. give general algorithm, based notion projection; study restrictions representations knowledge basequery, find new polynomial classes abduction problems.1. IntroductionAbduction consists searching plausible explanation given observation.instance, p |= q p plausible explanation observation q. generally,abduction process searching set facts (the explanation, p) that,conjointly given knowledge base (here p q), imply given query (q). processalso constrained set hypotheses among explanations chosen,preference criterion among them.problem abduction proved practical interest many domains. instance,used formalize text interpretation (Hobbs et al., 1993), system (Coste-Marquis& Marquis, 1998; Stumptner & Wotawa, 2001) medical diagnosis (Bylander et al., 1989,Section 6). also closely related configuration problems (Amilhastre et al., 2002),ATMS/CMS (Reiter & de Kleer, 1987), default reasoning (Selman & Levesque,1990) even induction (Goebel, 1997).interested complexity propositional logic-based abduction, i.e.,assume knowledge base query represented propositional formulas.Even framework, many different formalizations proposed literature,mainly differing definition hypothesis best explanation (Eiter& Gottlob, 1995). assume hypotheses conjunctions literalsformed upon distinguished subset variables involved, best explanationone proper subconjunction explanation (subset-minimality criterion).purpose exhibit new polynomial classes abduction problems. givegeneral algorithm finding best explanation framework defined above, independently syntactic form formulas representing knowledge basequery. explore syntactic forms allow polynomial running timealgorithm. find new polynomial classes abduction problems, among onerestricting knowledge base given Horn DNF query positive CNF,one restricting knowledge base given affine formula querydisjunction linear equations. algorithm also unifies several previous results.c2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBruno Zanuttininote organized follows. first recall useful notions propositionallogic (Section 2), formalize problem (Section 3) briefly survey previous workcomplexity abduction (Section 4). give algorithm (Section 5)explore polynomial classes (Section 6). Finally, discuss results perspectives(Section 7). lack space cannot detail proofs, longer version work,containing detailed proofs examples, available (Zanuttini, 2003).2. Preliminariesassume countable number propositional variables x1 , x2 . . . standard connectives , , , , , . literal either variable xi (positive literal) negationxi (negative literal). propositional formula well-formed formula built finitenumber variables connectives; V ar() denotes set variables occurpropositional formula . clause finite disjunction literals, propositional formula Conjunctive Normal Form (CNF) written finite conjunctionclauses. instance, = (x1 x2 ) (x1 x2 x3 ) CNF. dual notionsclause CNF notions term (finite conjunction literals) DisjunctiveNormal Form (DNF) (finite disjunction terms).assignment set variables V set literals contains exactly oneliteral per variable V , model propositional formula assignmentV ar() satisfies usual way, assigns 1 xi iff xi m; alsowrite tuple, e.g., 0010 {x1 , x2 , x3 , x4 }. write m[i] value assignedxi m, M() set models propositional formula ;said satisfiable M() 6= . formula said imply propositional formula 0(written |= 0 ) M() M(0 ). generally, identify sets models Booleanfunctions, use notations (negation), M0 (disjunction) on.notion projection important rest paper. assignmentset variables V V , write SelectA (m) set literalsformed upon A, e.g., Select{x1 ,x2 } (0110) = 01. Projecting set assignments onto subsetvariables intuitively consists replacing assignment SelectA (m);sake simplicity however, define projection set models built uponset variables M. yields following definition.Definition 1 (projection) Let V = {x1 , . . . , xn } set variables, set assignments V V . projection onto set assignments VM|A = {m | m0 M, SelectA (m0 ) = SelectA (m)}.instance, let = {0001, 0010, 0111, 1100, 1101} set assignments V ={x1 , x2 , x3 , x4 }, let = {x1 , x2 }. easily seenM|A = {0000, 0001, 0010, 0011} {0100, 0101, 0110, 0111} {1100, 1101, 1110, 1111}since {SelectA (m) | M} = {00, 01, 11}.Remark projection set models formula onto set variablesset models general consequence independentvariables A. Note also projection M() onto set modelsformula obtained forgetting variables occurring A. details2fiLogic-Based Abductionvariable forgetting independence refer reader work Lang etal. (Lang et al., 2002).useful note straightforward properties projection. Let M, M0 denotetwo sets assignments set variables V , let V . First, projectiondistributive disjunction, i.e., (M M0 )|A = M|A M0 |A . distributiveconjunction depend variables M0 depends on, i.e., existA, A0 V , A0 = M|A = (M depend V \A) M0 |A0 = M0 ,(M M0 )|A = M|A M0 |A holds; note true general case. Note finallygeneral (M)|A M|A .3. Model Abductionformalize model; sake simplicity, first define abduction problemsnotions hypothesis explanation.Definition 2 (abduction problem) triple = (, , A) called abduction problem satisfiable propositional formulas set variablesV ar(), V ar(); called knowledge base , query setabducibles.Definition 3 (hypothesis,explanation) Let = (, , A) abduction problem.hypothesis set literals formed upon (seen conjunction),hypothesis E explanation E satisfiable E |= .proper subconjunction E explanation , E called best explanation .Note framework allow one specify variable must occur unnegated(resp. negated) explanation. think prohibiting restriction, sinceabducibles intuitively meant represent variables whose values be, e.g., modified, observed repaired, matter sign explanation. noterestriction, general framework defined abduciblesliterals hypotheses, conjunctions abducibles (Marquis, 2000).interested computational complexity computing best explanationgiven abduction problem, asserting none all. Following usual model,establish complexities respect size representationsnumber abducibles; hardness results, following associated decision problemusually considered: least one explanation ? Obviously, latter problemhard, function problem also is.4. Previous Workmain general complexity results propositional logic-based abduction subsetminimality preference stated Eiter Gottlob (1995). authors showdeciding whether given abduction problem solution P2 -complete problem,even V ar() = V ar() CNF. stated well Selman Levesque(1990), also establish problem becomes NP-complete Horn,even acyclic Horn. Note SAT deduction polynomialproblem obviously NP.3fiBruno Zanuttinifact, classes abduction problems known polynomialsearch explanations. far know, classes definedfollowing restrictions (once refer reader references definitions):2CNF 2DNF (Marquis, 2000, Section 4.2)given monotone CNF clause (Marquis, 2000, Section 4.2)given definite Horn CNF conjunction positive literals (Selman& Levesque, 1990; Eiter & Gottlob, 1995)given acyclic Horn CNF pseudo-completion unit-refutablevariable (Eshghi, 1993)bounded induced kernel width given literal (del Val, 2000)represented set characteristics models (with respect particular basis)variable (Khardon & Roth, 1996); note set characteristic modelspropositional formula, result however similar onesrepresented set models, or, equivalently, DNF every variableoccurring term, propositional formula.first two classes proved polynomial general method solving abductionproblems notion prime implicants, last one obvious since informationexplicitely given input, four others exhibited ad hoc algorithms.Let us also mention Amilhastre et al. (2002) study related problemsgeneral framework multivalued theories instead propositional formulas, i.e.,domain variables restricted {0, 1}. authors mainly show,far note concerned, deciding whether exists explanation stillP2 -complete problem (Amilhastre et al., 2002, Table 1).Note results stated exact framework papers citedabove, still hold it. Let us also mention problem enumeratingbest explanations given abduction problem great interest; Eiter Makino(2002) provide discussion first results it, mainly caseknowledge base Horn.5. General Algorithmgive principle algorithm. Let us stress first that, well as, e.g., Marquisconstruction (Marquis, 2000, Section 4.2), outline matches point point definitionbest explanation; ideas Marquis anyway rather close.first interested hypotheses every abducible x occurs (eithernegated unnegated); let us call full hypotheses. Note indeed every explanationE abduction problem subconjunction full explanation F ; indeed, since Edefinition E satisfiable implies , suffices let F SelectA (m)model E . Minimization F discussed later on.4fiLogic-Based AbductionProposition 1 Let = (, , A) abduction problem, F full hypothesis .F explanation exists assignment V ar()F = SelectA (m) M() (M( ))|A .Proof Assume first F explanation . (i) exists assignmentV ar() |= F , thus F = SelectA (m) M(), (ii) F |= , i.e.,F unsatisfiable, thus F/ {SelectA (m) | M( )}, thus/ (M( ))|A ,thus (M( ))|A . Conversely, M() (M( ))|A let F = SelectA (m).(i) since M(), F satisfiable, (ii) since/ (M( ))|A ,00M( ) SelectA (m ) = F , thus F unsatisfiable, thusF |= .Thus characterized full explanations given abduction problem.minimizing explanation F problem, since following greedy procedure,given Selman Levesque (1990) reduces F best explanation :every literal ` FF \{`} |= F F \{`} endif;Endfor;Note depending order literals ` F considered result maydifferent, cases best explanation .Finally, give general algorithm computing best explanation givenabduction problem = (, , A); correctness follows directly Proposition 1:0 propositional formula M(0 ) = M() (M( ))|A ;0 unsatisfiable return explanation;Elsemodel 0 ;F SelectA (m);minimize F ;return F ;Endif;6. Polynomial Classesexplore new polynomial classes abduction problems algorithm allowsexhibit. Throughout section, n denotes number variables V ar().6.1 Affine Formulaspropositional formula said affine (or XOR-CNF ) (Schaefer, 1978; Kavvadias &Sideri, 1998; Zanuttini, 2002) written finite conjunction linear equationstwo-element field, e.g., = (x1 x3 = 1) (x1 x2 x4 = 0). seen, equationsplay role affine formulas clauses CNFs; roughly, affine formulas representconjunctions parity equivalence constraints. class proves interesting knowledgerepresentation, since one hand tractable common reasoning tasks,5fiBruno Zanuttinihand affine approximations knowledge base made smallefficiently learnable (Zanuttini, 2002). show projecting affine formula ontosubset variables quite easy too, enabling algorithm run polynomial time.proof following lemma easily obtained gaussian elimination (Curtis, 1984):triangulate variables put rightmost, keep equationsformed upon A; full details given technical report version (Zanuttini, 2003).Lemma 1 Let affine formula containing k equations, V ar().affine formula M() = (M())|A containing k equationscomputed time O(k 2 |V ar()|).Proposition 2 represented affine formula containing k equationsdisjunction k 0 linear equations, subset V ar(), searching bestexplanation = (, , A) done time O((k + k 0 )((k + 1)2 + |A|(k + k 0 ))n).Sketch proof easily seen affine formula (containing k 0 + k equationsn variables) computed time linear size ; formulaprojected onto time O((k + k 0 )2 n), straightforwardly get disjunctionk + k 0 linear equations (M( ))|A . use distributivitysolving satisfiability problem algorithm; recall SAT solved timeO(k 2 n) affine formula k equations n variables elimination methodGauss (Curtis, 1984). remaining operations straightforward.Note variables, literals clauses special cases disjunctions linear equations.6.2 DNFsThough class DNF formulas good computational properties, abductionremains hard problem whole, even additional restrictions. RecallTAUTOLOGY problem one deciding whether given DNF formula representsidentically true function, problem coNP-complete.Proposition 3 Deciding whether least one explanation given abductionproblem (, , A) NP-complete given DNF, even variable{} = V ar().Sketch proof Membership NP obvious, since deduction DNFs polynomial;easily seen tautological abduction problem ((x), x, V ar()) explanation, x variable occuring (see DNF(x) implication x); (x) DNF, get result.However, represented DNF projecting onto easy; indeed, properties projection show suffices cancel literals formed upon A.Consequently, DNF containing k terms, DNF M() = (M())|Acontaining k terms computed time O(k|V ar()|).Thus show subclasses class DNFs allow polynomialabduction. state first result quite generally, note assumptionssatisfied natural classes DNFs: e.g., Horn DNFs, i.e., DNFs6fiLogic-Based Abductionone positive literal per term; similarly, Horn-renamable DNFs, i.e.,turned Horn DNF replacing variables negation,simplifying double negations, everywhere formula; 2DNFs, DNFstwo literals per term. omit proof following proposition, since essentiallyProposition 2 (simply follow execution algorithm).Proposition 4 Let class DNFs stable removal occurrencesliterals TAUTOLOGY problem polynomial. restricted belongD, clause subset V ar(), searching best explanation= (, , A) done polynomial time.Thus establish abduction tractable (among others) Horn-renamableDNF (including Horn reverse Horn cases) 2DNF, clause.Finally, let us point similar proof obtain polynomialityproblems obtained strengthening restriction Proposition 4 ,weakening .Proposition 5 represented Horn (resp. reverse Horn) DNF k termspositive (resp. negative) CNF k 0 clauses, subset V ar(),searching best explanation = (, , A) done time O((k + |A|)kk 0 n).holds represented positive (resp. negative) DNF k termsHorn (resp. reverse Horn) CNF k 0 clauses.note variables, literals terms special cases (reverse) HornCNFs, variables, positive (resp. negative) clauses positive (resp. negative)terms special cases positive (resp. negative) CNFs.7. Discussion Perspectivesgeneral algorithm presented note allows us derive new polynomial restrictionsabduction problems; even discussed here, lack space, also allowsunify previously known restrictions (such 2CNF 2DNF,monotone CNF given clause). following list summarizes main newpolynomial restrictions:given affine formula disjunction linear equations (Proposition 2)Horn-renamable DNF given clause (Proposition 4)2DNF given clause (Proposition 4)Horn (reverse Horn) DNF positive (negative) CNF (Proposition 5)negative (positive) DNF reverse Horn (Horn) CNF (Proposition 5).Moreover, even guarantee efficiency general case presentationalgorithm depend syntactic form , uses standardoperations Boolean functions (projection, conjunction, negation).7fiBruno ZanuttiniAnother interesting feature algorithm minimization computesexplanations intentionnally. Consequently, full explanations enumeratedroughly delay models formula representing (0 ). However,course, guarantee two would minimizedbest explanation, prevents concluding algorithm enumeratebest explanations; trying extend direction would interesting problem.details enumeration refer reader Eiter Makinos work (Eiter& Makino, 2002).identified Selman Levesque (1990), central task notion projection set variables, algorithm isolates subtask. However, notionprojection concerns variables, literals, prevents imposing signliterals hypotheses formed upon, contrariwise general formalizationsproposed abduction, Marquis (Marquis, 2000). Even think prohibiting restriction, would interesting try fix weakness algorithmpreserving polynomial classes.Another problem interest behaviour algorithmpropositional formulas, generally multivalued theories, domainvariables restricted {0, 1}: e.g., signed formulas (Beckert et al., 1999).framework used, instance, configuration problems Amilhastre et al. (2002).easily seen algorithm still correct framework; however, still leftstudy cases running time polynomial.Finally, problems great interest deciding relevance necessityabducible (Eiter & Gottlob, 1995). abducible x said relevant abductionproblem least one best explanation containing x x, necessarybest explanations contain x x. easily seen x necessary= (, , A) 0 = (, , A\{x}) explanation, hence showingpolynomial restrictions search explanations polynomial well decidingnecessity hypothesis soon stable substitution A\{x}A, case restrictions considered note. Contrastingly,know relation relevance, study problem would alsogreat interest.Acknowledgmentsauthor wishes thank anonymous referees version previousone (Proc. JNPC02, French), well Jean-Jacques Hebrard, valuableconstructive comments.ReferencesAmilhastre, J., Fargier, H., & Marquis, P. (2002). Consistency restoration explanationsdynamic CSPs application configuration. Artificial Intelligence, 135 (12),199234.8fiLogic-Based AbductionBeckert, B., Hahnle, R., & Manya, F. (1999). Transformations signed classical clause logic. Proc. 29th International Symposium Multiple-Valued Logics(ISMVL99), pp. 248255. IEEE Computer Society Press.Bylander, T., Allemang, D., Tanner, M., & Josephson, J. (1989). results concerningcomputational complexity abduction. Proc. 1st International ConferencePrinciples Knowledge Representation Reasoning (KR89), pp. 4454. MorganKaufmann.Coste-Marquis, S., & Marquis, P. (1998). Characterizing consistency-based diagnoses.Proc. 5th International Symposium Artificial Intelligence Mathematics(AIMATH98).Curtis, C. (1984). Linear algebra. introductory approach. Springer Verlag.del Val, A. (2000). complexity restricted consequence finding abduction.Proc. 17th National Conference Artificial Intelligence (AAAI00), pp. 337342.AAAI Press/MIT Press.Eiter, T., & Gottlob, G. (1995). complexity logic-based abduction. JournalACM, 42 (1), 342.Eiter, T., & Makino, K. (2002). computing abductive explanations. Proc. 18thNational Conference Artificial Intelligence (AAAI02), pp. 6267. AAAI Press.Eshghi, K. (1993). tractable class abduction problems. Proc. 13th InternationalJoint Conference Artificial Intelligence (IJCAI93), pp. 38. Morgan Kaufmann.Goebel, R. (1997). Abduction relation constrained induction. Proc. IJCAI97workshop abduction induction AI.Hobbs, J., Stickel, M., Appelt, D., & Martin, P. (1993). Interpretation abduction. Artificial Intelligence, 63, 69142.Kavvadias, D., & Sideri, M. (1998). inverse satisfiability problem. SIAM JournalComputing, 28 (1), 152163.Khardon, R., & Roth, D. (1996). Reasoning models. Artificial Intelligence, 87, 187213.Lang, J., Liberatore, P., & Marquis, P. (2002). Conditional independence propositionallogic. Artificial Intelligence, 141, 79121.Marquis, P. (2000). Consequence finding algorithms. Handbook Defeasible ReasoningUncertainty Management Systems (DRUMS), Vol. 5, pp. 41145. Kluwer Academic.Reiter, R., & de Kleer, J. (1987). Foundations assumption-based truth maintenance systems: preliminary report. Proc. 6th National Conference Artificial Intelligence(AAAI87), pp. 183188. AAAI Press/MIT Press.Schaefer, T. (1978). complexity satisfiability problems. Proc. 10th Annual ACMSymposium Theory Computing (STOC78), pp. 216226. ACM Press.Selman, B., & Levesque, H. (1990). Abductive default reasoning: computational core.Proc. 8th National Conference Artificial Intelligence (AAAI90), pp. 343348.AAAI Press.9fiBruno ZanuttiniStumptner, M., & Wotawa, F. (2001). Diagnosing tree-structured systems. Artificial Intelligence, 127, 129.Zanuttini, B. (2002). Approximating propositional knowledge affine formulas.Proc. 15th European Conference Artificial Intelligence (ECAI02), pp. 287291.IOS Press.Zanuttini, B. (2003). New polynomial classes logic-based abduction. Tech. rep., Universite de Caen, France.10fiJournal Artificial Intelligence Research 19 (2003) 569-629Submitted 05/01; published 12/03Accelerating Reinforcement LearningImplicit ImitationBob Priceprice@cs.ubc.caDepartment Computer ScienceUniversity British ColumbiaVancouver, B.C., Canada V6T 1Z4Craig Boutiliercebly@cs.toronto.eduDepartment Computer ScienceUniversity TorontoToronto, ON, Canada M5S 3H5AbstractImitation viewed means enhancing learning multiagent environments.augments agents ability learn useful behaviors making intelligent useknowledge implicit behaviors demonstrated cooperative teachers experienced agents. propose study formal model implicit imitationaccelerate reinforcement learning dramatically certain cases. Roughly, observingmentor, reinforcement-learning agent extract information capabilitiesin, relative value of, unvisited parts state space. study two specificinstantiations model, one learning agent mentor identicalabilities, one designed deal agents mentors different action sets.illustrate benefits implicit imitation integrating prioritized sweeping,demonstrating improved performance convergence observation singlemultiple mentors. Though make stringent assumptions regarding observabilitypossible interactions, briefly comment extensions model relaxrestricitions.1. Introductionapplication reinforcement learning multiagent systems offers unique opportunitieschallenges. agents viewed independently trying achieve ends,interesting issues interaction agent policies (Littman, 1994) must resolved (e.g.,appeal equilibrium concepts). However, fact agents may share informationmutual gain (Tan, 1993) distribute search optimal policies communicate reinforcement signals one another (Mataric, 1998) offers intriguing possibilitiesaccelerating reinforcement learning enhancing agent performance.Another way individual agent performance improvednovice agent learn reasonable behavior expert mentor. type learningbrought explicit teaching demonstration (Atkeson & Schaal, 1997;Lin, 1992; Whitehead, 1991a), sharing privileged information (Mataric, 1998),explicit cognitive representation imitation (Bakker & Kuniyoshi, 1996).imitation, agents exploration used ground observations agentsc2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiPrice & Boutilierbehaviors capabilities resolve ambiguities observations arisingpartial observability noise. common thread work use mentorguide exploration observer. Typically, guidance achieved formexplicit communication mentor observer. less direct form teachinginvolves observer extracting information mentor without mentor makingexplicit attempt demonstrate specific behavior interest (Mitchell, Mahadevan, &Steinberg, 1985).paper develop imitation model call implicit imitation allowsagent accelerate reinforcement learning process observation expertmentor (or mentors). agent observes state transitions induced mentorsactions uses information gleaned observations update estimatedvalue states actions. distinguish two settings implicitimitation occur: homogeneous settings, learning agent mentoridentical actions; heterogeneous settings, capabilities may differ.homogeneous setting, learner use observed mentor transitions directlyupdate estimated model actions, update value function. addition,mentor provide hints observer parts state spacemay worth focusing attention. observers attention area might take formadditional exploration area additional computation brought bear agentsprior beliefs area. heterogeneous setting, similar benefits accrue,potential agent misled mentor possesses abilities differentown. case, learner needs mechanism detect situationsmake efforts temper influence observations.derive several new techniques support implicit imitation largely independent specific reinforcement learning algorithm, though best suited usemodel-based methods. include model extraction, augmented backups, feasibilitytesting, k-step repair. first describe implicit imitation homogeneous domains,describe extension heterogeneous settings. illustrate effectivenessempirically incorporating Moore Atkesons (1993) prioritized sweeping algorithm.implicit imitation model several advantages direct forms imitationteaching. require agent explicitly play role mentor teacher.Observers learn simply watching behavior agents; observed mentorshares certain subtasks observer, observed behavior incorporated (indirectly) observer improve estimate value function. importantmany situations observer learn mentorunwilling unable alter behavior accommodate observer, even communicateinformation it. example, common communication protocols may unavailableagents designed different developers (e.g., Internet agents); agents may findcompetitive situation disincentive share information skills;may simply incentive one agent provide information another.1Another key advantage approachwhich arises formalizing imitationreinforcement learning contextis fact observer constrained directly1. reasons consistency, use term mentor describe agent observerlearn, even mentor unwilling unwitting participant.570fiImplicit Imitationimitate (i.e., duplicate actions of) mentor. learner decide whetherexplicit imitation worthwhile. Implicit imitation thus seen blendingadvantages explicit teaching explicit knowledge transfer independentlearning. addition, agent learns observation, exploit existencemultiple mentors, essentially distributing search. Finally, assumeobserver knows actual actions taken mentor, mentor sharesreward function (or goals) mentor. Again, stands sharp contrast manyexisting models teaching, imitation, behavior learning observation. makestrict assumptions paper respect observability, complete knowledgereward functions, existence mappings agent state spaces, modelgeneralized interesting ways. elaborate generalizations nearend paper.remainder paper structured follows. provide necessary background Markov decision processes reinforcement learning developmentimplicit imitation model Section 2. Section 3, describe general formal frameworkstudy implicit imitation reinforcement learning. Two specific instantiationsframework developed. Section 4, model homogeneous agentsdeveloped. model extraction technique explained augmented Bellman backupproposed mechanism incorporating observations model-based reinforcementlearning algorithms. Model confidence testing introduced ensure misleadinginformation undue influence learners exploration policy. usementor observations focus attention interesting parts state space alsointroduced. Section 5 develops model heterogeneous agents. model extendshomogeneous model feasibility testing, device learner detectwhether mentors abilities similar own, k-step repair, whereby learnerattempt mimic trajectory mentor cannot duplicated exactly.techniques prove crucial heterogeneous settings. effectiveness modelsdemonstrated number carefully chosen navigation problems. Section 6 examinesconditions implicit imitation work well. Section 7 describesseveral promising extensions model. Section 8 examines implicit imitation modelcontext related work Section 9 considers future work drawinggeneral conclusions implicit imitation field computational imitationbroadly.2. Reinforcement Learningaim provide formal model implicit imitation, whereby agent learnact optimally combining experience observations behaviorexpert mentor. so, describe section standard modelreinforcement learning used artificial intelligence. model build singleagent view learning act. begin reviewing Markov decision processes,provide model sequential decision making uncertainty, movedescribe reinforcement learning, emphasis model-based methods.571fiPrice & Boutilier2.1 Markov Decision ProcessesMarkov decision processes (MDPs) proven useful modeling stochastic sequential decision problems, widely used decision-theoretic planning modeldomains agents actions uncertain effects, agents knowledge environment uncertain, agent multiple, possibly conflicting objectives.section, describe basic MDP model consider one classical solution procedure.consider action costs formulation MDPs, though pose specialcomplications. Finally, make assumption full observability. Partially observableMDPs (POMDPs) (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Smallwood &Sondik, 1973) much computationally demanding fully observable MDPs.imitation model based fully observable model, though generalizations model mentioned concluding section build POMDPs. referreader Bertsekas (1987); Boutilier, Dean Hanks (1999); Puterman (1994)material MDPs.MDP viewed stochastic automaton actions induce transitionsstates, rewards obtained depending states visited agent.Formally, MDP defined tuple hS, A, T, Ri, finite set statespossible worlds, finite set actions, state transition function, R rewardfunction. agent control state system extent performing actionscause state transitions, movement current state new state.Actions stochastic actual transition caused cannot generally predictedcertainty. transition function : (S) describes effectsaction state. (si , a) probability distribution S; specifically, (si , a)(sj )probability ending state sj action performed state si .denote quantity Pr(si , a, sj ). require 0 Pr(si , a, sj ) 1Psi , sj , si , sj Pr(si , a, sj ) = 1. components S, determinedynamics system controlled. assumption system fullyobservable means agent knows true state time (once stagereached), decisions based solely knowledge. Thus, uncertainty liesprediction actions effects, determining actual effectexecution.(deterministic, stationary, Markovian) policy : describes course actionadopted agent controlling system. agent adopting policy performsaction (s) whenever finds state s. Policies form Markovian sinceaction choice state depend system history, stationary sinceaction choice depend stage decision problem. problemsconsider, optimal stationary Markovian policies always exist.assume bounded, real-valued reward function R : <. R(s) instantaneous reward agent receives occupying state s. number optimality criteriaadopted measure value policy , measuring way rewardaccumulated agent traverses state space execution .work, focus discounted infinite-horizon problems: current value reward received stages future discounted factor (0 < 1). allows simpler572fiImplicit Imitationcomputational methods used, discounted total reward finite. Discountingjustified (e.g., economic) grounds many situations well.value function V : < reflects value policy state s;simply expected sum discounted future rewards obtained executing beginnings. policy optimal if, policies , V (s) V (s).guaranteed optimal (stationary) policies exist setting (Puterman,1994). (optimal) value state V (s) value V (s) optimal policy .solving MDP, refer problem constructing optimal policy. Valueiteration (Bellman, 1957) simple iterative approximation algorithm optimal policyconstruction. Given arbitrary estimate V 0 true value function V , iterativelyimprove estimate follows:V n (si ) = R(si ) + max{aAXPr(si , a, sj )V n1 (sj )}(1)sjcomputation V n (s) given V n1 known Bellman backup. sequence valuefunctions V n produced value iteration converges linearly V . iteration valueiteration requires O(|S|2 |A|) computation time, number iterations polynomial|S|.finite n, actions maximize right-hand side Equation 1 formoptimal policy, V n approximates value. Various termination criteria applied;example, one might terminate algorithmkV i+1 V k(1 )2(2)(where kXk = max{|x| : x X} denotes supremum norm). ensures resultingvalue function V i+1 within 2 optimal function V state, inducedpolicy -optimal (i.e., value within V ) (Puterman, 1994).concept useful later Q-function. Given arbitrary valuefunction V , define QVa (si )QVa (si ) = R(si ) +XPr(si , a, sj )V (sj )(3)sjIntuitively, QVa (s) denotes value performing action state actingmanner value V (Watkins & Dayan, 1992). particular, define QaQ-function defined respect V , Qna Q-function defined respectV n1 . manner, rewrite Equation 1 as:V n (s) = max{Qna (s)}aA(4)define ergodic MDP MDP every state reachablestate finite number steps non-zero probability.573fiPrice & Boutilier2.2 Model-based Reinforcement LearningOne difficulty use MDPs construction optimal policy requiresagent know exact transition probabilities Pr reward model R. specification decision problem, requirements, especially detailed specificationdomains dynamics, impose undue burden agents designer. Reinforcementlearning viewed solving MDP full details model, particular Pr R, known agent. Instead, agent learns act optimallyexperience environment. provide brief overview reinforcementlearning section (with emphasis model-based approaches). details,please refer texts Sutton Barto (1998) Bertsekas Tsitsiklis (1996),survey Kaelbling, Littman Moore (1996).general model, assume agent controlling MDP hS, A, T, Riinitially knows state action spaces, A, transition modelreward function R. agent acts environment, stage processmakes transition hs, a, r, ti; is, takes action state s, receives reward rmoves state t. Based repeated experiences type determine optimalpolicy one two ways: (a) model-based reinforcement learning, experiencesused learn true nature R, MDP solved using standardmethods (e.g., value iteration); (b) model-free reinforcement learning, experiencesused directly update estimate optimal value function Q-function.Probably simplest model-based reinforcement learning scheme certainty equivalence approach. Intuitively, learning agent assumed current estimatedctransition model Tb environment consisting estimated probabilities Pr(s,a, t)bestimated rewards model R(s). experience hs, a, r, ti agent updates esc obtain policyb would optimaltimated models, solves estimated MDPestimated models correct, acts according policy.make certainty equivalence approach precise, specific form estimated modelupdate procedure must adopted. common approach used empirical distribution observed state transitions rewards estimated model. instance,action attempted C(s, a) times state s, C(s, a, t) occasionscstate reached, estimate Pr(s,a, t) = C(s, a, t)/C(s, a). C(s, a) = 0,prior estimate used (e.g., one might assume state transitions equiprobable).Bayesian approach (Dearden, Friedman, & Andre, 1999) uses explicit prior distributionparameters transition distribution Pr(s, a, ), updatesexperienced transition. instance, might assume Dirichlet (Generalized Beta)distribution (DeGroot, 1975) parameters n(s, a, t) associated possible successor state t. Dirichlet parameters equal experience-based counts C(s, a, t)plus prior count P (s, a, t) representing agents prior beliefs distribution(i.e., n(s, a, t) = C(s, a, t) + P (s, a, t)). expected transition probability Pr(s, a, t)Pc solvedn(s, a, t)/ t0 n(s, a, t0 ). Assuming parameter independence, MDPusing expected values. Furthermore, model updated ease, simplyincreasing n(s, a, t) one observation hs, a, r, ti. model advantagecounter-based approach allowing flexible prior model generally574fiImplicit Imitationassign probability zero unobserved transitions. adopt Bayesian perspectiveimitation model.One difficulty certainty equivalence approach computational burden rec update models Tb Rb (i.e., experience). Onesolving MDPcould circumvent extent batching experiences updating (and re-solving)model periodically. Alternatively, one could use computational effort judiciouslyapply Bellman backups states whose values (or Q-values) likely changegiven change model. Moore Atkesons (1993) prioritized sweepingcalgorithm this. Tb updated changing Pr(s,a, t), Bellman backupbb a). Supposeapplied update estimated value V , well Q-value Q(s,bbmagnitude change V (s) given V (s). predecessor w, Q-valuesbcQ(w,a0 )hence values Vb (w)can change Pr(w,a0 , s) > 0. magnitude changecbounded Pr(w,a0 , s)Vb (s). predecessors w placed priority0cqueue Pr(w, , s)Vb (s) serving priority. fixed number Bellman backupsapplied states order appear queue. backup,change value cause new predecessors inserted queue. way,computational effort focused states Bellman backup greatestimpact due model change. Furthermore, backups applied subsetstates, generally applied fixed number times. way contrast,certainty equivalence approach, backups applied convergence. Thus prioritizedsweeping viewed specific form asynchronous value iteration, appealingcomputational properties (Moore & Atkeson, 1993).certainty equivalence, agent acts current approximation modelcorrect, even though model likely inaccurate early learning process.optimal policy inaccurate model prevents agent exploring transitions form part optimal policy true model, agent failfind optimal policy. reason, explicit exploration policies invariably usedensure action tried state sufficiently often. acting randomly (assuming ergodic MDP), agent assured sampling action state infinitelyoften limit. Unfortunately, actions agent fail exploit (in fact,completely uninfluenced by) knowledge optimal policy. explorationexploitation tradeoff refers tension trying new actions order findenvironment executing actions believed optimal basiscurrent estimated model.common method exploration greedy method agentchooses random action fraction time, 0 < < 1. Typically, decayedtime increase agents exploitation knowledge. Boltzmann approach,action selected probability proportional value:Prs (a) = PeQ(s,a)/ea0Q(s,a0 )/(5)proportionality adjusted nonlinearly temperature parameter .0 probability selecting action highest value tends 1. Typically,started high actions randomly explored early stages learning.agent gains knowledge effects actions value effects,575fiPrice & Boutilierparameter decayed agent spends time exploiting actions knownvaluable less time randomly exploring actions.sophisticated methods attempt use information model confidencevalue magnitudes plan utility-maximizing exploration plan. early approximationscheme found interval estimation method (Kaelbling, 1993). Bayesianmethods also used calculate expected value information gainedexploration (Meuleau & Bourgine, 1999; Dearden et al., 1999).concentrate paper model-based approaches reinforcement learning.However, point model-free methodsthose estimateoptimal value function Q-function learned directly, without recourse domainmodelhave attracted much attention. example, TD-methods (Sutton, 1988)Q-learning (Watkins & Dayan, 1992) proven among popularmethods reinforcement learning. methods modified deal model-freeapproaches, discuss concluding section. also focus so-called tablebased (or explicit) representations models value functions. state actionspaces large, table-based approaches become unwieldy, associated algorithmsgenerally intractable. situations, approximators often used estimatevalues states. discuss ways techniques extended allowfunction approximation concluding section.3. Formal Framework Implicit Imitationmodel influence mentor agent decision process learningbehavior observer, must extend single-agent decision model MDPs accountactions objectives multiple agents. section, introduce formalframework studying implicit imitation. begin introducing general modelstochastic games (Shapley, 1953; Myerson, 1991), impose various assumptionsrestrictions general model allow us focus key aspects implicitimitation. note framework proposed useful study formsknowledge transfer multiagent systems, briefly point various extensionsframework would permit implicit imitation, forms knowledge transfer,general settings.3.1 Non-Interacting Stochastic GamesStochastic games viewed multiagent extension Markov decision processes.Though Shapleys (1953) original formulation stochastic games involved zero-sum (fullycompetitive) assumption, various generalizations model proposed allowingarbitrary relationships agents utility functions (Myerson, 1991).2 Formally,n-agent stochastic game hS, {Ai : n}, T, {Ri : n}i comprises set n agents(1 n), set states S, set actions Ai agent i, state transition function, reward function Ri agent i. Unlike MDP, individual agent actionsdetermine state transitions; rather joint action taken collection agentsdetermines system evolves point time. Let = A12. example, see fully cooperative multiagent MDP model proposed Boutilier (1999).576fiImplicit Imitationset joint actions; : (S), (si , a)(sj ) = Pr(si , a, sj ) denotingprobability ending state sj joint action performed state si .convenience, introduce notation Ai denote set joint actions A1Ai1 Ai+1 involving agents except i. use ai ai denote(full) joint action obtained conjoining ai Ai ai Ai .interests individual agents may odds, strategic reasoningnotions equilibrium generally involved solution stochastic games.aim study reinforcement agent might learn observing behaviorexpert mentor, wish restrict model way strategic interactions needconsidered: want focus settings actions observermentor interact. Furthermore, want assume reward functionsagents conflict way requires strategic reasoning.define noninteracting stochastic games appealing notion agent projection function used extract agents local state underlying game.games, agents local state determines aspects global state relevantdecision making process, projection function determines global statesidentical agents local perspective. Formally, agent i, assume localstate space Si , projection function Li : Si . s, S, writeiff Li (s) = Li (t). equivalence relation partitions set equivalence classeselements within specific class (i.e., L1(s) Si ) needdistinguished agent purposes individual decision making. say stochasticgame noninteracting exists local state space Si projection function Liagent that:1. t, ai Ai , ai Ai , wi SiX{Pr(s, ai ai , w) : w L1(wi )} =X{Pr(t, ai ai , w) : w L1(wi )}2. Ri (s) = Ri (t)Intuitively, condition 1 imposes two distinct requirements gameperspective agent i. First, ignore existence agents, provides notionstate space abstraction suitable agent i. Specifically, Li clusters together statesstate equivalence class identical dynamics respectabstraction induced Li . type abstraction form bisimulationtype studied automaton minimization (Hartmanis & Stearns, 1966; Lee & Yannakakis,1992) automatic abstraction methods developed MDPs (Dearden & Boutilier, 1997;Dean & Givan, 1997). hard showignoring presence agentsthatunderlying system Markovian respect abstraction (or equivalently, w.r.t.Si ) condition 1 met. quantification ai imposes strong noninteractionrequirement, namely, dynamics game perspective agentindependent strategies agents. Condition 2 simply requiresstates within given equivalence class agent reward agent i.means states within class need distinguishedeach local state viewedatomic.577fiPrice & Boutiliernoninteracting game induces MDP Mi agent Mi = hSi , Ai , Pri , RiPri given condition (1) above. Specifically, si , ti Si :Pri (si , ai , ti ) =P{Pr(s, ai .ai , t) : L1(ti )}state L1(si ) ai element Ai . Let : Sa Aioptimal policy Mi . extend strategy iG : Ai underlyingstochastic game simply applying (si ) every state Li (s) = si .following proposition shows term noninteracting indeed provides appropriatedescription game.Proposition 1 Let G noninteracting stochastic game, Mi induced MDP agenti, optimal policy Mi . strategy iG extending G dominantagent i.Thus agent solve noninteracting game abstracting away irrelevant aspects state space, ignoring agent actions, solving personal MDPMi .Given arbitrary stochastic game, generally quite difficult discover whethernoninteracting, requiring construction appropriate projection functions.follows, simply assume underlying multiagent system noninteractinggame. Rather specifying game projection functions, specify individual MDPs Mi themselves. noninteracting game induced set individualMDPs simply cross product individual MDPs. view often quitenatural. Consider example three robots moving two-dimensional office domain. able neglect possibility interactionfor example, robotsoccupy 2-D position (at suitable level granularity) requireresources achieve tasksthen might specify individual MDProbot. local state might determined robots x, y-position, orientation,status tasks. global state space would cross product S1 S2 S3local spaces. individual components joint action would affectlocal state, agent would care (through reward function Ri ) localstate.note projection function Li viewed equivalent observation function. assume agent distinguish elements Sifact, observations agents states crucial imitation. Rather existenceLi simply means that, point view decision making known model,agent need worry distinctions made Li . Assumingcomputational limitations, agent need solve Mi , may use observationsagents order improve knowledge Mi dynamics.33.2 Implicit ImitationDespite independent nature agent subprocesses noninteracting multiagent system, circumstances behavior one agent may relevant3. elaborate condition computational limitations below.578fiImplicit Imitationanother. keep discussion simple, assume existence expert mentor agentm, implementing stationary (and presumably optimal) policylocal MDP Mm = hSm , , Prm , Rm i. also assume second agent o, observer,local MDP Mo = hSo , Ao , Pro , Ro i. nothing mentors behavior relevantobserver knows MDP (and solve without computational difficulty),situation quite different reinforcement learner without complete knowledge model Mo . may well observed behavior mentor providesvaluable information observer quest learn act optimally within Mo .take extreme case, mentors MDP identical observers, mentorexpert (in sense acting optimally), behavior mentor indicatesexactly observer do. Even mentor acting optimally,mentor observer different reward functions, mentor state transitions observedlearner provide valuable information dynamics domain.Thus see one agent learning act, behavior anotherpotentially relevant learner, even underlying multiagent system noninteracting. Similar remarks, course, apply case observer knows MDPMo , computational restrictions make solving difficultobserved mentor transitionsmight provide valuable information focus computational effort.4 mainmotivation underlying model implicit imitation behavior expertmentor provide hints appropriate courses action reinforcement learningagent.Intuitively, implicit imitation mechanism learning agent attemptsincorporate observed experience expert mentor agent learning process.Like classical forms learning imitation, learner considers effectsmentors action (or action sequence) context. Unlike direct imitation, however,assume learner must physically attempt duplicate mentorsbehavior, assume mentors behavior necessarily appropriateobserver. Instead, influence mentor agents transition modelestimate value various states actions. elaborate points below.follows, assume mentor associated MDP Mm , learnerobserver associated MDP Mo , described above. MDPs fully observable.focus reinforcement learning problem faced agent o. extension multiplementors straightforward discussed below, clarity assume onementor description abstract framework. clear certain conditions mustmet observer extract useful information mentor. list numberassumptions make different points development model.Observability: must assume learner observe certain aspects mentors behavior. work, assume state mentors MDP fullyobservable learner. Equivalently, interpret full observabilityunderlying noninteracting game, together knowledge mentors projection4. instance, algorithms like asynchronous dynamic programming prioritized sweeping benefitguidance. Indeed, distinction reinforcement learning solving MDPs viewedrather blurry (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996). focuscase unknown model (i.e., classical reinforcement learning problem) opposed onecomputational issues key.579fiPrice & Boutilierfunction Lm . general partially observable model would require specification observation signal set Z observation function : Sm (Z),O(so , sm )(z) denotes probability observer obtains signal zlocal states observer mentor sm , respectively.pursue model here. important note assumeobserver access action taken point time. Since actionsstochastic, state (even fully observable) results mentor invokingspecific control signal generally insufficient determine signal. Thus seemsmuch reasonable assume states (and transitions) observableactions gave rise them.Analogy: observer mentor acting different local state spaces, clearobservations made mentors state transitions offer useful informationobserver unless relationship two state spaces.several ways relationship specified. Dautenhahn Nehaniv(1998) use homomorphism define relationship mentor observerspecific family trajectories (see Section 8 discussion).slightly different notion might involve use analogical mapping h : Smobserved state transition provides informationobserver dynamics value state h(s) . certain circumstances,might require mapping h homomorphic respect Pr(, a, ) (for some,all, a), perhaps even respect R. discuss issues detailbelow. order simplify model avoid undue attention (admittedlyimportant) topic constructing suitable analogical mappings, simply assumementor observer identical state spaces; is, Smsense isomorphic. precise sense spaces isomorphicorcases, presumed isomorphic proven otherwiseis elaborateddiscuss relationship agent abilities. Thus pointsimply refer state without distinguishing mentors local space Smobservers .Abilities: Even mapping states, observations mentors state transitionstell observer something mentors abilities, own. mustassume observer way duplicate actions taken mentorinduce analogous transitions local state space. words, mustpresumption mentor observer similar abilities.sense analogical mapping state spaces takenhomomorphism. Specifically, might assume mentor observeractions available (i.e., = Ao = A) h : Smhomomorphic respect Pr(, a, ) A. requirementweakened substantially, without diminishing utility, requiringobserver able implement actions actually taken mentor givenstate s. Finally, might observer assumes duplicateactions taken mentor finds evidence contrary. case,presumed homomorphism state spaces. follows,distinguish implicit imitation homogeneous action settingsdomains580fiImplicit Imitationanalogical mapping indeed homomorphicand heterogeneous actionsettingswhere mapping may homomorphism.general ways defining similarity ability, example, assumingobserver may able move state space similar fashionmentor without following trajectories (Nehaniv & Dautenhahn, 1998).instance, mentor may way moving directly key locations statespace, observer may able move analogous locations lessdirect fashion. case, analogy states may determinedsingle actions, rather sequences actions local policies. suggestways dealing restricted forms analogy type Section 5.Objectives: Even observer mentor similar identical abilities,value observer information gleaned mentor may dependactual policy implemented mentor. might supposeclosely related mentors policy optimal policy observer,useful information be. Thus, extent, expectclosely aligned objectives mentor observer are, valuableguidance provided mentor. Unlike existing teaching models,suppose mentor making explicit efforts instruct observer.objectives may identical, force observer(attempt to) explicitly imitate behavior mentor. general, makeexplicit assumptions relationship objectives mentorobserver. However, see that, extent, closer are,utility derived implicit imitation.Finally, remark important assumption make throughout remainderpaper: observer knows reward function Ro ; is, state s,observer evaluate Ro (s) without visited state s. view consistentview reinforcement learning automatic programming. user may easilyspecify reward function (e.g., form set predicates evaluatedstate) prior learning. may difficult specify domain modeloptimal policy. setting, unknown component MDP Motransition function Pro . believe approach reinforcement learning is,fact, common practice approach reward function mustsampled.reiterate, aim describe mechanism observer acceleratelearning; emphasize position implicit imitationin contrast explicitimitationis merely replicating behaviors (or state trajectories) observed anotheragent, even attempting reach similar states. believe agent must learncapabilities adapt information contained observed behaviorthese. Agents must also explore appropriate application (if any) observed behaviors,integrating own, appropriate, achieve ends. thereforesee imitation interactive process behavior one agent used guidelearning another.581fiPrice & BoutilierGiven setting, list possible ways observer mentor (andcannot) interact, contrasting along way perspective assumptionsexisting models literature.5 First, observer could attempt directly inferpolicy observations mentor state-action pairs. model conceptualsimplicity intuitive appeal, forms basis behavioral cloning paradigm(Sammut, Hurst, Kedzier, & Michie, 1992; Urbancic & Bratko, 1994). However, assumesobserver mentor share reward function action capabilities.also assumes complete unambiguous trajectories (including action choices)observed. related approach attempts deduce constraints value functioninferred action preferences mentor agent (Utgoff & Clouse, 1991; Suc & Bratko,1997). Again, however, approach assumes congruity objectives. model alsodistinct models explicit teaching (Lin, 1992; Whitehead, 1991b): assumementor incentive move environment way explicitlyguides learner explore environment action space effectively.Instead trying directly learn policy, observer could attempt use observedstate transitions agents improve environment model Pro (s, a, t).accurate model reward function, observer could calculateaccurate values states. state values could used guide agent towardsdistant rewards reduce need random exploration. insight forms coreimplicit imitation model. approach developed literature,appropriate conditions listed above, specifically, conditionsmentors actions unobservable, mentor observer different rewardfunctions objectives. Thus, approach applicable general conditionsmany existing models imitation learning teaching.addition model information, mentors may also communicate informationrelevance irrelevance regions state space certain classes reward functions.observer use set states visited mentor heuristic guidanceperform backup computations state space.next two sections, develop specific algorithms insightsagents use observations others improve models assessrelevance regions within state spaces. first focus homogeneous actioncase, extend model deal heterogeneous actions.4. Implicit Imitation Homogeneous Settingsbegin describing implicit imitation homogeneous action settingsthe extensionheterogeneous settings build insights developed section. developtechnique called implicit imitation observations mentor usedaccelerate reinforcement learning. First, define homogeneous setting.develop implicit imitation algorithm. Finally, demonstrate implicit imitationworks number simple problems designed illustrate role various mechanisms describe.5. describe models detail Section 8.582fiImplicit Imitation4.1 Homogeneous Actionshomogeneous action setting defined follows. assume single mentorobserver o, individual MDPs Mm = hS, , Prm , Rm Mo = hS, Ao , Pro , Ro i,respectively. Note agents share state space (more precisely, assumetrivial isomorphic mapping allows us identify local states). also assumementor executing stationary policy . often treat policydeterministic, remarks apply stochastic policies well. Let supportset Supp(m , s) state set actions accorded nonzero probabilitystate s. assume observer abilities mentorfollowing sense: s, S, Supp(m , s), exists action ao AoPro (s, ao , t) = Prm (s, , t). words, observer able duplicate (in senseinducing distribution successor states) actual behavior mentor;equivalently, agents local state spaces isomorphic respect actionsactually taken mentor subset states actions might taken.much weaker requiring full homomorphism Sm . course,existence full homomorphism sufficient perspective; resultsrequire this.4.2 Implicit Imitation Algorithmimplicit imitation algorithm understood terms component processes.First, extract action models mentor. integrate informationobservers value estimates augmenting usual Bellman backup mentoraction models. confidence testing procedure ensures use augmentedmodel observers model mentor reliable observers modelbehavior. also extract occupancy information observations mentortrajectories order focus observers computational effort (to extent) specificparts state space. Finally, augment action selection process choose actionsexplore high-value regions revealed mentor. remainder sectionexpands upon processes fit together.4.2.1 Model Extractioninformation available observer quest learn act optimallydivided two categories. First, action takes, receives experience tuplehs, a, r, ti; fact, often ignore sampled reward r, since assume rewardfunction R known advance. standard model-based learning, experienceused update transition model Pro (s, a, ).Second, mentor transition, observer obtains experience tuple hs, ti.Note observer direct access action taken mentor,induced state transition. Assume mentor implementing deterministic,stationary policy , (s) denoting mentors choice action state s.policy induces Markov chain Prm (, ) S, Prm (s, t) = Pr(s, (s), t) denoting583fiPrice & Boutilierprobability transition t.6 Since learner observes mentors statec chain: Prc (s, t) simply estimatedtransitions, construct estimate Prrelative observed frequency mentor transitions (w.r.t. transitions takens). observer prior possible mentor transitions, standard Bayesianupdate techniques used instead. use term model extraction processestimating mentors Markov chain.4.2.2 Augmented Bellman Backupsc mentors Markov chain.Suppose observer constructed estimate Prhomogeneity assumption, action (s) replicated exactly observerstate s. Thus, policy can, principle, duplicated observer (were ableidentify actual actions used). such, define value mentors policyobservers perspective:Vm (s) = Ro (s) +XPrm (s, t)Vm (t)(6)tSNotice Equation 6 uses mentors dynamics observers reward function.Letting V denote optimal (observers) value function, clearly V (s) Vm (s), Vmprovides lower bound observers value function.importantly, terms making Vm (s) integrated directly Bellman equation observers MDP, forming augmented Bellman equation:((V (s) = Ro (s) + max maxaAoX)Pro (s, a, t)V (t) ,tSX)Prm (s, t)V (t)(7)tSusual Bellman equation extra term added, namely, secondPsummation, tS Prm (s, t)V (t) denoting expected value duplicating mentorsaction . Since (unknown) action identical one observers actions,term redundant augmented value equation valid. course, observer usingaugmented backup operation must rely estimates quantities. observerexploration policy ensures state visited infinitely often, estimates Proterms converge true values. mentors policy ergodic state space S,Prm also converge true value. mentors policy restricted subsetstates 0 (those forming basis Markov chain), estimates Prmsubset converge correctly respect 0 chain ergodic. states0 remain unvisited estimates remain uninformed data. Sincementors policy control observer, way observerinfluence distribution samples attained Prm . observer must thereforeable reason accuracy estimated model Prm restrictapplication augmented equation states Prm known sufficientaccuracy.6. somewhat imprecise, since initial distribution Markov chain unknown.purposes, dynamics relevant observer, transition probabilitiesused.584fiImplicit ImitationPrm cannot used indiscriminately, argue highly informativeearly learning process. Assuming mentor pursuing optimal policy (orleast behaving way tends visit certain states frequently),many states observer much accurate estimates Prm (s, t)Pro (s, a, t) specific a. Since observer learning, must explorestate spacecausing less frequent visits sand action spacethus spreadingexperience actions a. generally ensures sample size uponPrm based greater Pro action forms part mentorspolicy. Apart accurate, use Prm (s, t) often give informedvalue estimates state s, since prior action models often flat uniform,become distinguishable given state observer sufficient experience states.note reasoning holds even mentor implementing (stationary) stochastic policy (since expected value stochastic policy fully-observableMDP cannot greater optimal deterministic policy). direction offered mentor implementing deterministic policy tends focused,empirically found mentors offer broader guidance moderately stochasticenvironments implement stochastic policies, since tend visitstate space. note extension multiple mentors straightforwardeachmentor model incorporated augmented Bellman equation without difficulty.4.2.3 Model Confidencementors Markov chain ergodic, mixing rate7 sufficiently low,mentor may visit certain state relatively infrequently. estimated mentor transitionmodel corresponding state rarely (or never) visited mentor may providemisleading estimatebased small sample prior mentors chainofvalue mentors (unknown) action s; since mentors policycontrol observer, misleading value may persist extended period. Sinceaugmented Bellman equation consider relative reliability mentorobserver models, value state may overestimated;8 is, observertricked overvaluing mentors (unknown) action, consequently overestimatingvalue state s.overcome this, incorporate estimate model confidence augmentedbackups. mentors Markov chain observers action transitions,assume Dirichlet prior parameters multinomial distributions(DeGroot, 1975). reflect observers initial uncertainty possible transition probabilities. sample counts mentor observer transitions, updatedistributions. information, could attempt perform optimal Bayesianestimation value function; sample counts small (and normal approximations appropriate), simple, closed form expression resultantdistributions values. could attempt employ sampling methods, in7. mixing rate refers quickly Markov chain approaches stationary distribution.8. Note underestimates based considerations problematic, since augmented Bellmanequation reduces usual Bellman equation.585fiPrice & BoutilierVVO-VO VMFigure 1: Lower bounds action values incorporate uncertainty penaltyterest simplicity employed approximate method combining informationsources inspired Kaelblings (1993) interval estimation method.Let V denote current estimated augmented value function, Pro Prm denote2 denote varianceestimated observer mentor transition models. let o2model parameters.augmented Bellman backup respect V using confidence testing proceedsfollows. first compute observers optimal action ao based estimatedaugmented values observers actions. Let Q(ao , s) = Vo (s) denote value.best action, use model uncertainty encoded Dirichlet distributionconstruct lower bound Vo (s) value state observer using model(at state s) derived behavior (i.e., ignoring observations mentor).employ transition counts (s, a, t) nm (s, t) denote number timesobserver made transition state state action performed,number times mentor observed making transition statet, respectively. counts, estimate uncertainty model usingPvariance Dirichlet distribution. Let = (s, a, t) = t0 St (s, a, t0 ).model variance is:2model(s, a, t) =( +)2++ ( + + 1)(8)variance Q-value action due uncertainty local modelfound simple application rule combining linear combinations variances, V ar(cX + dY ) = c2 V ar(X) + d2 V ar(Y ) expression Bellman backup,PV ar(R(s) + P r(t|s, a)V (t). result is:2 (s, a) = 2X2model(s, a, t)v(t)2(9)Using Chebychevs inequality,9 obtain confidence level even though Dirichletdistributions small sample counts highly non-normal. lower boundVo (s) = Vo (s)co (s, ao ) suitable constant c. One may interpret penalizing9. Chebychevs inequality states 1 k12 probability mass arbitrary distributionwithin k standard deviations mean.586fiImplicit Imitation22FUNCTION augmentedBackup( V ,Pro ,omodel,Prm ,mmodel,s,c)= arg maxaAoPtSPr(s, a, t)V (t)PVo (s) = Ro (s) + PtS Pro (s, , t)V (t)Vm (s) = Ro (s) + tS Prm (s, t)V (t)P22o2 (s, ) =(t)2P tS2 omodel (s, , t)V222(s) =(s, t)V (t)tS mmodelVo (s) = Vo (s) c (s, )Vm(s) = Vm (s) c (s)Vo (s) > Vm (s)V (s) = Vo (s)ELSEV (s) = Vm (s)ENDTable 1: Implicit Backupvalue state subtracting uncertainty (see Figure 1).10 valueVm (s) mentors action (s) estimated similarly analogous lower boundVm (s) also constructed. Vo (s) > Vm (s), say Vo (s) supersedesVm (s) write Vo (s) Vm (s). Vo (s) Vm (s) either mentor-inspiredmodel has, fact, lower expected value (within specified degree confidence)uses nonoptimal action (from observers perspective), mentor-inspired modellower confidence. either case, reject information provided mentoruse standard Bellman backup using action model derived solely observersexperience (thus suppressing augmented backup)the backed value Vo (s)case.algorithm computing augmented backup using confidence test shownTable 1. algorithm parameters include current estimate augmented value2function V , current estimated model Pro associated local variance omodel,2model mentors Markov chain Prm associated variance mmodel .calculates lower bounds returns mean value, Vo Vm , greatest lowerbound. parameter c determines width confidence interval used mentorrejection test.4.2.4 Focusingaugmented Bellman backups improves accuracy observers model. secondway observer exploit observations mentor focus attentionstates visited mentor. model-based approach, specific focusing mecha10. Ideally, would like take uncertainty model current state account,also uncertainty future states well (Meuleau & Bourgine, 1999).587fiPrice & Boutiliernism adopt require observer perform (possibly augmented) Bellman backupstate whenever mentor makes transition s. three effects. First,mentor tends visit interesting regions space (e.g., shares certain reward structure observer), significant values backed mentor-visited statesbias observers exploration towards regions. Second, computational effortc (s, t) changes,concentrated toward parts state space estimated model Prhence estimated value one observers actions may change. Third,computation focused model likely accurate (as discussed above).4.2.5 Action Selectionintegration exploration techniques action selection policy importantreinforcement learning algorithm guarantee convergence. implicit imitation, playssecond, crucial role helping agent exploit information extracted mentor.improved convergence results rely greedy quality exploration strategybias observer towards higher-valued trajectories revealed mentor.expediency, adopted -greedy action selection method, using exploration rate decays time. could easily employed semi-greedymethods Boltzmann exploration. presence mentor, greedy action selection becomes complex. observer examines actions state usualway obtains best action ao corresponding value Vo (s). value alsocalculated mentors action Vm (s). Vo (s) Vm (s), observers actionmodel used greedy action defined exactly mentor present.If, however, Vm (s) Vo (s) would like define greedy action actiondictated mentors policy state s. Unfortunately, observer knowaction is, define greedy action observers action closestmentors action according observers current model estimates s. precisely,action similar mentors state s, denoted (s), whose outcomedistribution minimum Kullback-Leibler divergence mentors action outcomedistribution:((s) = argminaX)Pro (s, a, t) log Prm (s, t)(10)observers experience-based action models poor early training,chance closest action computation select wrong action. relyexploration policy ensure observers actions sampled appropriatelylong run.11present work assumed state space large agenttherefore able completely update Q-function whole space. (Theintractability updating entire state space one motivations using imitationtechniques). absence information states true values, would likebias value states along mentors trajectories look worthwhileexplore. assuming bounds reward function setting initial Qvalues entire space bound. simple examples, rewards strictly11. mentor executing stochastic policy, test based KL-divergence mislead learner.588fiImplicit Imitationpositive set bounds zero. mentor trajectories intersect states valuedobserving agent, backups cause states trajectories highervalue surrounding states. causes greedy step exploration methodprefer actions lead mentor-visited states actions agentinformation.4.2.6 Model Extraction Specific Reinforcement Learning AlgorithmsModel extraction, augmented backups, focusing mechanism, extended notiongreedy action selection, integrated model-based reinforcement learning algorithms relative ease. Generically, implicit imitation algorithm requires that: (a)c (s, t) Markov chain induced mentorsobserver maintain estimate Prpolicythis estimate updated every observed transition; (b) backupsperformed estimate value function use augmented backup (Equation 7) confic (s, a, t)dence testing. course, backups implemented using estimated models PrcPrm (s, t). addition, focusing mechanism requires augmented backupperformed state visited mentor.demonstrate generality mechanisms combining wellknown efficient prioritized sweeping algorithm (Moore & Atkeson, 1993). outlinedcSection 2.2, prioritized sweeping works maintaining estimated transition model Prb Whenever experience tuple hs, a, r, ti sampled, estimatedreward model R.model state change; Bellman backup performed incorporate revisedmodel (usually fixed) number additional backups performed selectedstates. States selected using priority estimates potential change valuesbased changes precipitated earlier backups. Effectively, computational resources(backups) focused states benefit backups.Incorporating ideas prioritized sweeping simply requires following changes:c (s, a, t) transition hs, a, ti observer takes, estimated model Prdated augmented backup performed state s. Augmented backupsperformed fixed number states using usual priority queue implementation.c (s, t) updatedobserved mentor transition hs, ti, estimated model Praugmented backup performed s. Augmented backups performedfixed number states using usual priority queue implementation.Keeping samples mentor behavior implements model extraction. Augmented backupsintegrate information observers value function, performing augmentedbackups observed transitions (in addition experienced transitions) incorporatesfocusing mechanism. observer forced follow otherwise mimic actionsmentor directly. back value information along mentors trajectoryhad. Ultimately, observer must move states discover actionsused; meantime, important value information propagatedguide exploration.Implicit imitation alter long run theoretical convergence propertiesunderlying reinforcement learning algorithm. implicit imitation framework orthogonal -greedy exploration, alters definition greedy action,589fiPrice & Boutiliergreedy action taken. Given theoretically appropriate decay factor, -greedystrategy thus ensure distributions action models statesampled infinitely often limit converge true values. Since extractedmodel mentor corresponds one observers actions, effectvalue function calculations different effect observers sampledaction models. confidence mechanism ensures model sampleseventually come dominate is, fact, better. therefore sure convergence properties reinforcement learning implicit imitation identicalunderlying reinforcement learning algorithm.benefit implicit imitation lies way models extractedmentor allow observer calculate lower bound value function uselower bound choose greedy actions move agent towards higher-valued regionsstate space. result quicker convergence optimal policies better short-termpractical performance respect accumulated discounted reward learning.4.2.7 Extensionsimplicit imitation model easily extended extract model informationmultiple mentors, mixing matching pieces extracted mentor achieve goodresults. searching, state, set mentors knows findmentor highest value estimate. value estimate best mentorcompared using confidence test described observers value estimate.formal expression algorithm given multi-augmented Bellman equation:((V (s) = Ro (s) + max maxaAomaxmMXX)Pro (s, a, t)V (t),tS)Prm (s, t)V (t)(11)tSset candidate mentors. Ideally, confidence estimates takenaccount comparing mentor estimates other, may get mentorhigh mean value estimate large variance. observer experiencestate all, mentor likely rejected poorer quality informationobserver already experience. observer mightbetter picking mentor lower mean confident estimate wouldsucceeded test observers model. interests simplicity,however, investigate multiple mentor combination without confidence testing.now, assumed action costs (i.e., agents rewards dependstate action selected state); however, use generalreward functions (e.g., reward form R(s, a)). difficulty lies backingaction costs mentors chosen action unknown. Section 4.2.5 definedclosest action function . function used choose appropriate reward.augmented Bellman equation generalized rewards takes following form:((V (s) = max max Ro (s, a) +aAoXtS590)Pro (s, a, t)V (t),fiImplicit ImitationRo (s, (s)) +X)Prm (s, t)V (t)tSnote Bayesian methods could used could used estimate action costsmentors chain well. case, generalized reward augmented equationreadily amended use confidence estimates similar fashion transition model.4.3 Empirical Demonstrationsfollowing empirical tests incorporate model extraction focusing mechanismprioritized sweeping. results illustrate types problems scenariosimplicit imitation provide advantages reinforcement learning agent.experiments, expert mentor introduced experiment serve modelobserver. case, mentor following -greedy policy small (onorder 0.01). tends cause mentors trajectories lie within clustersurrounding optimal trajectories (and reflect good optimal policies). Evensmall amount exploration environment stochasticity, mentors generallycover entire state space, confidence testing important.experiments, prioritized sweeping used fixed number backups per observed experienced sample.12 -greedy exploration used decaying .Observer agents given uniform Dirichlet priors Q-values initialized zero. Observer agents compared control agents benefit mentors experience,otherwise identical (implementing prioritized sweeping similar parametersexploration policies). tests performed stochastic grid world domains, sincemake clear extent observers mentors optimal policies overlap (orfail to). Figure 2, simple 10 10 example shows start end state grid.typical optimal mentor trajectory illustrated solid line startend states. dotted line shows typical mentor-influenced trajectory quitesimilar observed mentor trajectory. assume eight-connectivity cellsstate grid nine neighbors including itself, agents fourpossible actions. experiments, four actions move agent compassdirections (North, South, East West), although agent initially knowaction which. focus primarily whether imitation improves performancelearning, since learner converge optimal policy whether uses imitationnot.4.3.1 Experiment 1: Imitation Effectfirst experiment compare performance observer using model extractionexpert mentor performance control agent using independent reinforcement learning. Given uniform nature grid world lack intermediaterewards, confidence testing required. agents attempt learn policymaximizes discounted return 10 10 grid world. start upper-left cornerseek goal value 1.0 lower-right corner. Upon reaching goal, agents12. Generally, number backups set roughly equal length optimal noise-freepath.591fiPrice & BoutilierXFigure 2: simple grid world start state goal state X50ObsFA SeriesCtrlAverage Reward per 1000 Steps40302010Delta010050010001500200025003000350040004500Simulation StepsFigure 3: Basic observer control agent comparisonsrestarted upper-left corner. Generally mentor follow similar identical trajectory run, mentors trained using greedy strategy leavesone path slightly highly valued rest. Action dynamics noisy,intended direction realized 90% time, one directions takenotherwise (uniformly). discount factor 0.9. Figure 3, plot cumulativenumber goals obtained previous 1000 time steps observer Obscontrol Ctrl agents (results averaged ten runs). observer able quicklyincorporate policy learned mentor value estimates. resultssteeper learning curve. contrast, control agent slowly explores space buildmodel first. Delta curve shows difference performance agents.agents converge optimal value function.592fiImplicit Imitation3025Average Reward per 1000 Steps20BasicScale1510Stoch5050100020003000400050006000Simulation StepsFigure 4: Delta curves showing influence domain size noise4.3.2 Experiment 2: Scaling Noisenext experiment illustrates sensitivity imitation size state spaceaction noise level. Again, observer uses model-extraction confidence testing.Figure 4, plot Delta curves (i.e., difference performance observercontrol agents) Basic scenario described, Scale scenariostate space size increased 69 percent (to 13 13 grid), Stoch scenarionoise level increased 40 percent (results averaged ten runs).total gain represented area curves observer non-imitatingprioritized sweeping agent increases state space size. reflects Whiteheads(1991a) observation grid worlds, exploration requirements increase quicklystate space size, optimal path length increases linearly. seeguidance mentor help larger state spaces.Increasing noise level reduces observers ability act upon informationreceived mentor therefore erodes advantage control agent.note, however, benefit imitation degrades gracefully increased noisepresent even relatively extreme noise level.4.3.3 Experiment 3: Confidence TestingSometimes observers prior beliefs transition probabilities mentormislead observer cause generate inappropriate values. confidence mechanism proposed previous section prevent observer fooledmisleading priors mentors transition probabilities. demonstrate roleconfidence mechanism implicit imitation, designed experiment based scenario illustrated Figure 5. Again, agents task navigate top-left cornerbottom-right corner 10 10 grid order attain reward +1. cre593fiPrice & Boutilier+5+5+5+5Figure 5: environment misleading priorsated pathological scenario islands high reward (+5) enclosed obstacles.Since observers priors reflect eight-connectivity uniform, high-valued cellsmiddle island believed reachable states diagonally adjacentsmall prior probability. reality, however, agents action set precludesagent therefore never able realize value. four islandsscenario thus create fairly large region center space high estimatedvalue, could potentially trap observer persisted prior beliefs.Notice standard reinforcement learner quickly learn none actionstake rewarding islands; contrast, implicit imitator using augmented backupscould fooled prior mentor model. mentor visit states neighboringisland, observer evidence upon change prior beliefmentor actions equally likely take one eight possible directions.imitator may falsely conclude basis mentor action model actionexist would allow access islands value. observer therefore needsconfidence mechanism detect mentor model less reliable model.test confidence mechanism, mentor follows path around outsideobstacles path cannot lead observer trap (i.e., providesevidence observer diagonal moves islands feasible).combination high initial exploration rate ability prioritized sweepingspread value across large distances virtually guarantees observer ledtrap. Given scenario, ran two observer agents control. first observerused confidence interval width given 5, which, according Chebychev rule,cover approximately 96 percent arbitrary distribution. second observergiven 0 interval, effectively disables confidence testing. observerconfidence testing consistently became stuck. Examination value function revealedconsistent peaks within trap region, inspection agent state trajectories showedstuck trap. observer confidence testing consistently escapedtrap. Observation value function time shows trap formed, fadedaway observer gained enough experience actions allow ignore594fiImplicit Imitation45CR Series40Ctrl35Average Reward per 1000 StepsObs3025201510505Delta020004000600080001000012000Simulation StepsFigure 6: Misleading priors may degrade performanceovercome erroneous priors mentor actions. Figure 6, performanceobserver confidence testing shown performance control agent (resultsaveraged 10 runs). see observers performance slightly degradedunaugmented control agent even pathological case.4.3.4 Experiment 4: Qualitative Difficultynext experiment demonstrates potential gains imitation increase(qualitative) difficulty problem. observer employs model extractionconfidence testing, though confidence testing play significant role here.13maze scenario, introduce obstacles order increase difficulty learningproblem. maze set 25 25 grid (Figure 7) 286 obstacles complicatingagents journey top-left bottom-right corner. optimal solution takesform snaking 133-step path, distracting paths (up length 22) branchingsolution path necessitating frequent backtracking. discount factor 0.98.10 percent noise, optimal goal-attainment rate six goals per 1000 steps.graph Figure 8 (with results averaged ten runs), see controlagent takes order 200,000 steps build decent value function reliably leadsgoal. point, achieving four goals per 1000 steps average,exploration rate still reasonably high (unfortunately, decreasing exploration quicklyleads slower value function formation). imitation agent able take advantagementors expertise build reliable value function 20,000 steps. Sincecontrol agent unable reach goal first 20,000 steps, Deltacontrol imitator simply equal imitators performance.13. mentor provide evidence path choices problem,intermediate rewards would cause observer make use misleading mentor priorsstates.595fiPrice & BoutilierFigure 7: complex maze7CMB Series6Average Reward per 1000 Steps543Obs2Delta100Ctrl0.511.522.55x 10Simulation StepsFigure 8: Imitation complex spaceimitator quickly achieve optimal goal attainment rate six goals per 1000 steps,exploration rate decays much quickly.4.3.5 Experiment 5: Improving Suboptimal Policies Imitationaugmented backup rule require reward structure mentorobserver identical. many useful scenarios rewards dissimilarvalue functions policies induced share structure. experiment,demonstrate one interesting scenario relatively easy find suboptimalsolution, difficult find optimal solution. observer finds suboptimalpath, however, able exploit observations mentor see596fiImplicit Imitation14******************235Figure 9: maze perilous shortcutshortcut significantly shortens path goal. structure scenarioshown Figure 9. suboptimal solution lies path location 1 aroundscenic route location 2 goal location 3. mentor takesvertical path location 4 location 5 shortcut.14 discourageuse shortcut novice agents, lined cells (marked *)agent immediately jumps back start state. therefore difficult novice agentexecuting random exploratory moves make way end shortcutobtain value would reinforce future use. observer controltherefore generally find scenic route first.Figure 10, performance (measured using goals reached previous 1000steps) control observer compared (averaged ten runs), indicatingvalue observations. see observer control agent find longerscenic route, though control agent takes longer find it. observer goes findshortcut increases return almost double goal rate. experiment showsmentors improve observer policies even observers goalsmentors path.4.3.6 Experiment 6: Multiple Mentorsfinal experiment illustrates model extraction readily extendedobserver extract models multiple mentors exploit valuable partseach. Again, observer employs model extraction confidence testing. Figure 11,learner must move start location 1 goal location 4. Two expert agentsdifferent start goal states serve potential mentors. One mentor repeatedly moveslocation 3 location 5 along dotted line, second mentor departs location2 ends location 4 along dashed line. experiment, observer must14. mentor proceeding 5 4 would provide guidance without prior knowledge actionsreversible.597fiPrice & Boutilier35CSB Series30Average Reward per 1000 Steps25Obs20Delta1510Ctrl5000.511.522.534x 10Simulation StepsFigure 10: Transfer non-identical rewards13254Figure 11: Multiple mentors scenariocombine information examples provided two mentors independentexploration order solve problem.Figure 12, see observer successfully pulls together informationsources order learn much quickly control agent (results averaged10 runs). see use value-based technique allows observer choosementors influence use state-by-state basis order get best solutionproblem.598fiImplicit Imitation60ObsCMM SeriesAverage Reward per 1000 Steps5040Ctrl3020Delta1000100020003000400050006000Simulation StepsFigure 12: Learning multiple mentors5. Implicit Imitation Heterogeneous Settingshomogeneity assumption violated, implicit imitation framework describedcause learners convergence rate slow dramatically and, cases,cause learner become stuck small neighborhood state space. particular,learner unable make state transition (or transitionprobability) mentor given state, may drastically overestimate valuestate. inflated value estimate causes learner return repeatedly stateeven though exploration never produce feasible action attains inflatedestimated value. mechanism removing influence mentors Markovchain value estimatesthe observer extremely (and correctly) confidentobservations mentors model. problem lies fact augmentedBellman backup justified assumption observer duplicate every mentoraction. is, state s, Pro (s, a, t) = Prm (s, t)t. equivalent action exist, guarantee valuecalculated using mentor action model can, fact, achieved.5.1 Feasibility Testingheterogeneous settings, prevent lock-up poor convergenceuse explicit action feasibility test: augmented backup performed s,observer tests whether mentors action differs actions s, givencurrent estimated models. so, augmented backup suppressed standardBellman backup used update value function. 15 default, mentor actions15. decision binary; could envision smoother decision criterion measures extentmentors action duplicated.599fiPrice & Boutilierassumed feasible observer; however, observer reasonably confidentinfeasible state s, augmented backups suppressed s.Recall uncertainty agents true transition probabilities capturedDirichlet distribution derived sampled transitions. Comparing ao effecteddifference means test respect corresponding Dirichlets. complicatedfact Dirichlets highly non-normal small parameter values transitiondistributions multinomial. deal non-normality requiring minimumnumber samples using robust Chebychev bounds pooled variancedistributions compared. Conceptually, evaluate Equation 12:r| Pro (s, ao , t) Prm (s, t)|22(s,ao ,t)omodel(s,ao ,t)+nm (s,t)mmodel(s,t)(s,ao ,t)+nm (s,t)> Z/2(12)Z/2 critical value test. parameter significance test,probability falsely reject two actions differentactually same. Given highly non-normal distributions early training process,appropriate Z value given computed Chebychevs bound solving2 = 1 Z12 Z/2 .samples accurate test, persist augmentedbackups (embodying default assumption homogeneity). value estimateinflated backups, agent biased obtain additional samples,allow agent perform required feasibility test. assumption thereforeself-correcting. deal multivariate complications performing Bonferronitest (Seber, 1984), shown give good results practice (Mi & Sampson,1993), efficient compute, known robust dependence variables.Bonferroni hypothesis test obtained conjoining several single variable tests. Supposeactions ao result r possible successor states, s1 , , sr (i.e., r transitionprobabilities compare). si , hypothesis Ei denotes aotransition probability successor state si ; Pr(s, , si ) = Pr(s, ao , si ). letEi denote complementary hypothesis (i.e., transition probabilities differ).Bonferroni inequality states:"Prr\#Ei 1i=1rXPr Eii=1Thus test joint hypothesis ri=1 Ei two action models samebytesting r complementary hypotheses Ei confidence level /r. rejecthypotheses reject notion two actions equal confidenceleast . mentor action deemed infeasible every observer action ao ,multivariate Bonferroni test described rejects hypothesis actionmentors.Pseudo-code Bonferroni component feasibility test appears Table 2.assumes sufficient number samples. efficiency reasons, cache resultsfeasibility testing. duplication mentors action state first determinedinfeasible, set flag state effect.600fiImplicit ImitationFUNCTION feasible(m,s) : BooleanAoallSuccessorProbsSimilar = truesuccessors(s)= |P roq(s, a, t) P rm (s, t)|(s,a,t) 2(s,a,t)+nm (s,t) 2omodelz = /(s,a,t)+nm (s,t)z > z/2rallSuccessorProbsSimilar = falseENDallSuccessorProbsSimilarreturn trueENDRETURN falsemmodel(s,t)Table 2: Action Feasibility Testing5.2 k-step Similarity RepairAction feasibility testing essentially makes strict decision whether agentduplicate mentors action specific state: decided mentors actioninfeasible, augmented backups suppressed potential guidance offered eliminatedstate. Unfortunately, strictness test results somewhat impoverishednotion similarity mentor observer. This, turn, unnecessarily limitstransfer mentor observer. propose mechanism whereby mentorsinfluence may persist even specific action chooses feasible mentor;instead rely possibility observer may approximately duplicate mentorstrajectory instead exactly duplicating it.Suppose observer previously constructed estimated value function using augmented backups. Using mentor action model (i.e., mentors chain Prm (s, t)), highvalue calculated state s. Subsequently, suppose mentors action statejudged infeasible. illustrated Figure 13, estimated value stateoriginally due mentors action (s), sake illustration moveshigh probability state t, lead highly-rewarding regionstate space. number experiences state s, however, learner concludesaction (s)and associated high probability transition tis feasible.point, one two things must occur: either (a) value calculated statepredecessors collapse exploration towards highly-valued regionsbeyond state ceases; (b) estimated value drops slightly exploration continuestowards highly-valued regions. latter case may arise follows. observerpreviously explored vicinity state s, observers action model maysufficiently developed still connect higher value-regions beyond state stateBellman backups. example, learner sufficient experiencelearned highly-valued region reached alternative trajectoryu v w, newly discovered infeasibility mentors transitiondeleterious effect value estimate s. highly-valued, likely statesclose mentors trajectory explored degree. case, state601fiPrice & BoutilierInfeasible TransitionHigh-valueStatew"Bridge"uvFigure 13: alternative path bridge value backups around infeasible pathshighly-valued using mentors action model, stillvalued highly enough likely guide exploration toward area. callalternative (in case u v w) mentors action bridge, allowsvalue higher value regions flow infeasible mentor transition.bridge formed without intention agent, call process spontaneousbridging.spontaneous bridge exist, observers action models generally undeveloped (e.g., close uniform prior distributions). Typically,undeveloped models assign small probability every possible outcome therefore diffuse value higher valued regions lead poor value estimate state s.result often dramatic drop value state predecessors;exploration towards highly-valued region neighborhood state ceases.example, could occur observers transition models state assign lowprobability (e.g., close prior probability) moving state u due lack experience(or similarly surrounding states, u v, insufficiently explored).spontaneous bridging effect motivates broader notion similarity.observer find short sequence actions bridges infeasible actionmentors trajectory, mentors example still provide extremely useful guidance.moment, assume short path path length greater giveninteger k. say observer k-step similar mentor state observerduplicate k fewer steps mentors nominal transition state sufficientlyhigh probability.Given notion similarity, observer test whether spontaneous bridgeexists determine whether observer danger value function collapseconcomitant loss guidance decides suppress augmented backup state s.this, observer initiates reachability analysis starting state using actionmodel Pro (s, a, t) determine sequence actions leads sufficientlyhigh probability state state mentors trajectory downstreaminfeasible action.16 k-step bridge already exists, augmented backups safelysuppressed state s. efficiency, maintain flag state mark bridged.state known bridged, k-step reachability analysis need repeated.spontaneous bridge cannot found, might still possible intentionally setbuild one. build bridge, observer must explore state k-steps away,hoping make contact mentors trajectory downstream infeasible mentor16. general state space ergodicity lacking, agent must consider predecessors statek steps guarantee k-step paths checked.602fiImplicit Imitationaction. implement single search attempt k2 -step random walk, resulttrajectory average k steps away long ergodicity local connectivityassumptions satisfied. order search occur, must motivate observerreturn state engage repeated exploration. could provide motivationobserver asking observer assume infeasible action repairable.observer therefore continue augmented backups support high-value estimatesstate observer repeatedly engage exploration point.danger, course, may fact bridge, case observerrepeat search bridge indefinitely. therefore need mechanism terminaterepair process k-step repair infeasible. could attempt explicitly keeptrack possible paths open observer paths explicitly triedobserver determine repair possibilities exhausted. Instead, electfollow probabilistic search eliminates need bookkeeping: bridge cannotconstructed within n attempts k-step random walk, repairability assumptionjudged falsified, augmented backup state suppressed observers biasexplore vicinity state eliminated. bridge found state s, flag usedmark state irreparable.approach is, course, nave heuristic strategy; illustrates basicimport bridging. systematic strategies could used, involving explicit planningfind bridge using, say, local search (Alissandrakis, Nehaniv, & Dautenhahn, 2000).Another aspect problem address persistence searchbridges. specific domain, number unsuccessful attempts find bridges,learner may conclude unable reconstruct mentors behavior, casesearch bridges may abandoned. involves simple, higher-level inference,notion (or prior beliefs about) similarity capabilities. notions could alsoused automatically determine parameter settings (discussed below).parameters k n must tuned empirically, estimated given knowledge connectivity domain prior beliefs similar (in termslength average repair) trajectories mentor observer be. instance,n > 8k 4 seems suitable 8-connected grid world low noise, based numbertrajectories required cover perimeter states k-step rectangle around state.note large values n reduce performance non-imitatingagents results temporary lock up.Feasibility k-step repair easily integrated homogeneous implicit imitation framework. Essentially, simply elaborate conditions augmentedbackup employed. course, additional representation introducedkeep track whether state feasible, bridged, repairable, many repair attempts made. action selection mechanism also overriddenbridge-building algorithm required order search bridge. Bridge buildingalways terminates n attempts, however, cannot affect long run convergence.aspects algorithm, however, exploration policy, unchanged.complete elaborated decision procedure used determine augmented backupsemployed state respect mentor appears Table 3. usesinternal state make decisions. original model, first check seeobservers experience-based calculation value state supersedes mentor603fiPrice & BoutilierFUNCTION use augmented?(s,m) : BooleanVo (s) Vm (s) RETURN falseELSE f easible(s, m) RETURN trueELSE bridged(s, m) RETURN falseELSE reachable(s, m)bridged(s,m) := trueRETURN falseELSE repairable(s, m) return falseELSE % searching0 < search steps(s, m) < k % search progressreturn truesearch steps(s, m) > k % search failedattempts(s) > nrepairable(s) = falseRETURN falseELSEreset search(s,m)attempts(s) := attempts(s) + 1RETURN trueattempts(s) :=1 % initiate first attempt searchinitiate-search(s)RETURN trueTable 3: Elaborated augmented backup testbased calculation; so, observer uses experience-based calculation.mentors action feasible, accept value calculated using observationbased value function. action infeasible check see state bridged.first time test requested, reachability analysis performed, resultsdrawn cache subsequent requests. state bridged, suppressaugmented backups, confident cause value function collapse. statebridged, ask repairable. first n requests, agent attemptk-step repair. repair succeeds, state marked bridged. cannot repairinfeasible transition, mark not-repairable suppress augmented backups.may wish employ implicit imitation feasibility testing multiple-mentorscenario. key change implicit imitation without feasibility testingobserver imitate feasible actions. observer searches setmentors one action results highest value estimate, observermust consider mentors whose actions still considered feasible (or assumedrepairable).5.3 Empirical Demonstrationssection, empirically demonstrate utility feasibility testing k-step repairshow techniques used surmount differences actionsagents small local differences state-space topology. problems604fiImplicit Imitationchosen specifically demonstrate necessity utility feasibility testingk-step repair.5.3.1 Experiment 1: Necessity Feasibility Testingfirst experiment shows importance feasibility testing implicit imitationagents heterogeneous actions. scenario, agents must navigate acrossobstacle-free, 10 10 grid world upper-left corner goal location lowerright. agent reset upper-left corner. first agent mentorNEWS action set (North, South, East, West movement actions). mentorgiven optimal stationary policy problem. study performance threelearners, Skew action set (N, S, NE, SW) unable duplicate mentorexactly (e.g., duplicating mentors E-move requires learner move NE followedS, move SE N). Due nature grid world, control imitationagents actually execute actions get goal mentoroptimal goal rate control imitator therefore lowermentor. first learner employs implicit imitation feasibility testing, second usesimitation without feasibility testing, third control agent uses imitation (i.e.,standard reinforcement learning agent). agents experience limited stochasticityform 5% chance action randomly perturbed. last section,agents use model-based reinforcement learning prioritized sweeping. set k = 3n = 20.effectiveness feasibility testing implicit imitation seen Figure 14.horizontal axis represents time simulation steps vertical axis representsaverage number goals achieved per 1000 time steps (averaged 10 runs). seeimitation agent feasibility testing converges much quickly optimalgoal-attainment rate agents. agent without feasibility testing achievessporadic success early on, frequently locks due repeated attempts duplicateinfeasible mentor actions. agent still manages reach goal time time,stochastic actions permit agent become permanently stuck obstaclefree scenario. control agent without form imitation demonstrates significantdelay convergence relative imitation agents due lack form guidance,easily surpasses agent without feasibility testing long run. gradualslope control agent due higher variance control agents discovery timeoptimal path, feasibility-testing imitator control agent convergeoptimal solutions. shown comparison two imitation agents, feasibilitytesting necessary adapt implicit imitation contexts involving heterogeneous actions.5.3.2 Experiment 2: Changes State Spacedeveloped feasibility testing bridging primarily deal problem adaptingagents heterogeneous actions. techniques, however, appliedagents differences state-space connectivity (ultimately, equivalentnotions). test this, constructed domain agents NEWSaction set, alter environment learners introducing obstacles arentpresent mentor. Figure 15, learners find mentors path obstructed605fiPrice & Boutilier40Feas35FS SeriesAverage Reward per 1000 Steps3025Ctrl2015NoFeas105005001000150020002500Simulation Steps3000350040004500Figure 14: Utility feasibility testingXFigure 15: Obstacle map mentor pathobstacles. Movement toward obstacle causes learner remain current state.sense, action different effect mentors.Figure 16, see results qualitatively similar previous experiment.contrast previous experiment, imitator control use NEWS action settherefore shortest path length mentor. Consequently,optimal goal rate imitators control higher previous experiment.observer without feasibility testing difficulty maze, value functionaugmented mentor observations consistently leads observer states whose pathgoal directly blocked. agent feasibility testing quickly discoversmentors influence inappropriate states. conclude local differencesstate well handled feasibility testing.Next, demonstrate feasibility testing completely generalize mentorstrajectory. Here, mentor follows path completely infeasible imitatingagent. fix mentors path runs give imitating agent maze shown606fiImplicit Imitation50FeasFO Series45Average Reward per 1000 Steps4035Ctrl302520151050NoFeas05001000150020002500Simulation Steps3000350040004500Figure 16: Interpolating around obstaclesObserverMentorXFigure 17: Parallel generalizationFigure 17 two states mentor visits blocked obstacle.imitating agent able use mentors trajectory guidance buildsparallel trajectory completely disjoint mentors.results Figure 18 show gain imitator feasibility testingcontrol agent diminishes, still exists marginally imitator forced generalizecompletely infeasible mentor trajectory. agent without feasibility testingpoorly, even compared control agent. gets stuck arounddoorway. high value gradient backed along mentors path becomes accessibleagents doorway. imitation agent feasibility conclude cannotproceed south doorway (into wall) try different strategy.imitator without feasibility testing never explores far enough away doorwaysetup independent value gradient guide goal. slower decayschedule exploration, imitator without feasibility testing would find goal,607fiPrice & Boutilier60Feas50FP SeriesAverage Reward per 1000 StepsCtrl40302010NoFeas005001000150020002500Simulation Steps3000350040004500Figure 18: Parallel generalization resultswould still reduce performance imitator feasibility testing.imitator feasibility testing makes use prior beliefs follow mentorbackup value perpendicular mentors path. value gradient therefore formparallel infeasible mentor path imitator follow along side infeasiblepath towards doorway makes necessary feasibility test proceedsgoal.explained earlier, simple problems good chance informal effectsprior value leakage stochastic exploration may form bridges feasibility testingcuts value propagation guides exploration. difficult problemsagent spends lot time exploring, accumulate sufficient samples concludementors actions infeasible long agent constructed bridge.imitators performance would drop unaugmented reinforcementlearner.demonstrate bridging, devised domain agents must navigateupper-left corner bottom-right corner, across river three steps wideexacts penalty 0.2 per step (see Figure 19). goal state worth 1.0. figure,path mentor shown starting top corner, proceeding along edgeriver crossing river goal. mentor employs NEWS actionset. observer uses Skew action set (N, NE, S, SW) attempts reproducementor trajectory. fail reproduce critical transition borderriver (because East action infeasible Skew agent). mentor actionlonger used backup value rewarding state alternativepaths river blocks greedy exploration region. Without bridgingoptimistic lengthly exploration phase, observer agents quickly discover negativestates river curtail exploration direction actually making across.608fiImplicit ImitationFigure 19: River scenarioexamine value function estimate (after 1000 steps) imitator feasibilitytesting repair capabilities, see that, due suppression feasibility testing,darkly shaded high-value states Figure 19 (backed goal) terminate abruptlyinfeasible transition without making across river. fact, dominatedlighter grey circles showing negative values. experiment, show bridgingprolong exploration phase right way. employ k-step repairprocedure k = 3.Examining graph Figure 20, see imitation agents experience earlynegative dip guided deep river mentors influence. agentwithout repair eventually decides mentors action infeasible, thereafter avoidsriver (and possibility finding goal). imitator repair also discoversmentors action infeasible, immediately dispense mentorsguidance. keeps exploring area mentors trajectory using random walk,accumulating negative reward suddenly finds bridge rapidlyconverges optimal solution.17 control agent discovers goalten runs.6. Applicabilitysimple experiments presented demonstrate major qualitative issues confronting implicit imitation agent specific mechanisms implicit imitationaddress issues. section, examine assumptions mechanisms presented previous sections determine types problems suitableimplicit imitation. present several dimensions prove useful predictingperformance implicit imitation types problems.17. repair steps take place area negative reward scenario, need case.Repair doesnt imply short-term negative return.609fiPrice & Boutilier15Average Reward per 1000 Steps10FB Series5CtrlCtrl0NoRepairRepair5NoRepair101520Repair0100020003000Simulation Steps400050006000Figure 20: Utility bridgingalready identified number assumptions implicit imitationapplicablesome assumptions models imitation teaching cannotapplied, assumptions restrict applicability model. include:lack explicit communication mentors observer; independent objectivesmentors observer; full observability mentors observer; unobservability mentorsactions; (bounded) heterogeneity. Assumptions full observability necessarymodelas formulatedto work (though discuss extension partially observable case Section 7). Assumptions lack communication unobservable actionsextend applicability implicit imitation beyond models literature;conditions hold, simpler form explicit communication may preferable. Finally,assumptions bounded heterogeneity independent objectives also ensure implicitimitation applied widely. However, degree rewardsactions homogeneous impact utility (i.e., acceleration learning offered by) implicit imitation. turn attention predicting performanceimplicit imitation function certain domain characteristics.6.1 Predicting Performancesection examine two questions: first, given implicit imitation applicable,implicit imitation bias agent suboptimal solution; second,performance implicit imitation vary structural characteristics domainsone might want apply to? show analysis internal structure state spaceused motivate metric (roughly) predicts implicit imitation performance.conclude analysis problem space understood termsdistinct regions playing different roles within imitation context.610fiImplicit Imitationimplicit imitation model, use observations agents improve observers knowledge environment rely sensible exploration policyexploit additional knowledge. clear understanding knowledge environment affects exploration therefore central understanding implicit imitationperform domain.Within implicit imitation framework, agents know reward functions, knowledge environment consists solely knowledge agents action models.general, models take form. simplicity, restrictedmodels decomposed local models possible combination systemstate agent action.local models state-action pairs allow prediction j-step successor statedistribution given initial state sequence actions local policy. qualityj-step state predictions function every action model encounteredinitial state states time j 1. Unfortunately, quality j-stepestimate drastically altered quality even single intermediate state-actionmodel. suggests connected regions state space, statesfairly accurate models, allow reasonably accurate future state predictions.Since estimated value state based immediate rewardreward expected received subsequent states, quality value estimatealso depend quality action models states connected s. Now,since greedy exploration methods bias exploration according estimated valueactions, exploratory choices agent state also dependentconnectivity reliable action models states reachable s. analysisimplicit imitation performance respect domain characteristics therefore organizedaround idea state space connectivity regions connectivity defines.6.1.1 Imitation Regions FrameworkSince connected regions play important role implicit imitation, introduce classification different regions within state space shown graphically Figure 21.follows, describe regions affect imitation performance model.first observe many tasks carried agent small subsetstates within state space defined problem. precisely, many MDPs,optimal policy ensure agent remains small subspace state space.leads us definition first regional distinction: relevant vs. irrelevant regions.relevant region set states non-zero probability occupancy optimalpolicy.18 -relevant region natural generalization optimal policy keepssystem within region fraction 1 time.Within relevant region, distinguish three additional subregions. exploredregion contains states observer formulated reliable action modelsbasis experience. augmented region contains states observerlacks reliable action models improved value estimates due mentor observations.18. One often assumes system starts one small set states. Markov chain inducedoptimal policy ergodic, irrelevant region nonempty. Otherwiseempty.611fiPrice & BoutilierObserverExploredRegionBlindRegionMentorAugmentedRegionIrrelevantRegionRewardFigure 21: Classification regions state spaceNote explored augmented regions created result observationsmade learner (of either transitions mentor). regionstherefore significant connected components; is, contiguous regions state spacereliable action mentor models available. Finally, blind region designatesstates observer neither (significant) personal experience benefitmentor observations. information states within blind region come(largely) agents prior beliefs.19ask regions interact imitation agent. First considerimpact relevance. Implicit imitation makes assumption accurate dynamicsmodels allow observer make better decisions will, turn, result higher returnssooner learning process. However, model information equally helpful:imitator needs enough information irrelevant region able avoid it.Since action choices influenced relative value actions, irrelevant regionavoided looks worse relevant region. Given diffuse priors actionmodels, none actions open agent initially appear particularly attractive.However, mentor provides observations within relevant region quickly makerelevant region look much promising method achieving higher returnstherefore constrain exploration significantly. Therefore, considering problemspoint view relevance, problem small relevant region relative entire spacecombined mentor operates within relevant region result maximumadvantage imitation agent non-imitating agent.explored region, observer sufficiently accurate models compute goodpolicy respect rewards within explored region. Additional observations19. partitioning states explored, blind augmented regions bears resemblance KearnsSinghs (1998) partitioning state space known unknown regions. Unlike Kearns Singh,however, use partitions analysis. implicit imitation algorithm explicitlymaintain partitions use way compute policy.612fiImplicit Imitationstates within explored region provided mentor still improve performancesomewhat significant evidence required accurately discriminate expectedvalue two actions. Hence, mentor observations explored region help,result dramatic speedups convergence.Now, consider augmented region observers Q-valuesaugmented observations mentor. experiments previous sections,seen observer entering augmented region experience significant speedupsconvergence due information inherent augmented value functionlocation rewards region. Characteristics augmented zone, however,affect degree augmentation improves convergence speed.Since observer receives observations mentors state, actions,observer improved value estimates states augmented region, policy.observer must therefore infer actions taken duplicate mentorsbehavior. observer prior beliefs effects actions, may ableperform immediate inference mentors actual choice action (perhaps usingKL-divergence maximum likelihood). observers prior model uninformative,observer explore local action space. exploring local action space,however, agent must take action action effect. Sinceguarantee agent took action duplicates mentors action, may endsomewhere different mentor. action causes observer fall outsideaugmented region, observer lose guidance augmented value functionprovides fall back performance level non-imitating agent.important consideration, then, probability observer remainaugmented regions continue receive guidance. One quality augmented regionaffects observers probability staying within boundaries relative coveragestate space. policy mentor may sparse complete. relativelydeterministic domain defined begin end states, sparse policy covering statesmay adequate. highly stochastic domain many start end states, agentmay need complete policy (i.e., covering every state). Implicit imitation provideguidance agent domains stochastic require completepolicies, since policy cover larger part state space.important completeness policy predicting guidance, mustalso take account probability transitions augmented region.actions domain largely invertible (directly, effectively so), agentchance re-entering augmented region. ergodicity lacking, however,agent may wait process undergoes form resetopportunity gather additional evidence regarding identity mentors actionsaugmented region. reset places agent back explored region,make way frontier last explored. lack ergodicitywould reduce agents ability make progress towards high-value regions resets,agent still guided attempt augmented region. Effectively,agent concentrate exploration boundary explored regionmentor augmented region.utility mentor observations depend probability augmentedexplored regions overlapping course agents exploration. explored613fiPrice & Boutilierregions, accurate action models allow agent move quickly possible highvalue regions. augmented regions, augmented Q-values inform agents stateslead highly-valued outcomes. augmented region abuts explored region,improved value estimates augmented region rapidly communicated acrossexplored region accurate action models. observer use resultant improvedvalue estimates explored region, together accurate action modelsexplored region, rapidly move towards promising states frontierexplored region. states, observer explore outward thereby eventuallyexpand explored region encompass augmented region.case explored region augmented region overlap,blind region. Since observer information beyond priors blind region,observer reduced random exploration. non-imitation context, statesexplored blind. However, imitation context, blind area reducedeffective size augmented area. Hence, implicit imitation effectively shrinks sizesearch space problem even overlap exploredaugmented spaces.challenging case implicit imitation transfer occurs region augmented mentor observations fails connect observer explored regionregions significant reward values. case, augmented region initiallyprovide guidance. observer independently located rewarding states,augmented regions used highlight shortcuts. shortcuts represent improvements agents policy. domains feasible solution easy find,optimal solutions difficult, implicit imitation used convert feasible solutionincreasingly optimal solution.6.1.2 Cross regional texturesseen distinctive regions used provide certain level insightimitation perform various domains. also analyze imitation performanceterms properties cut across state space. analysis model informationimpacts imitation performance, saw regions connected accurate action modelsallowed observer use mentor observations learn promising directionexploration. see, then, set mentor observations usefulconcentrated connected region less useful dispersed statespace unconnected components. fortunate completely observable environmentsobservations mentors tend capture continuous trajectories, thereby providingcontinuous regions augmented states. partially observable environments, occlusionnoise could lessen value mentor observations absence model predictmentors state.effects heterogeneity, whether due differences action capabilitiesmentor observer due differences environment two agents, alsounderstood terms connectivity action models. Value propagate alongchains action models hit state mentor observer differentaction capabilities. state, may possible achieve mentors valuetherefore, value propagation blocked. Again, sequential decision making aspect614fiImplicit Imitationreinforcement learning leads conclusion many scattered differencesmentor observer create discontinuity throughout problem space, whereascontiguous region differences mentor observer cause discontinuityregion, leave large regions fully connected. Hence, distribution patterndifferences mentor observer capabilities important prevalencedifference. explore pattern next section.6.2 Fracture Metrictry characterize connectivity form metric. Since differences reward structure, environment dynamics action models affect connectivity wouldmanifest differences policies mentor observer, designedmetric based differences agents optimal policies. call metric fracture.Essentially, computes average minimum distance state mentorobserver disagree policy state mentor observer agree policy. measure roughly captures difficulty observer faces profitably exploitingmentor observations reduce exploration demands.formally, let mentors optimal policy observers. Letstate space Sm 6=o set disputed states mentor observerdifferent optimal actions. set neighboring disputed states constitutes disputedregion. set Sm 6=o called undisputed states. Let distancemetric space S. metric corresponds number transitions alongminimal length path states (i.e., shortest path using nonzero probabilityobserver transitions).20 standard grid world, correspond Manhattandistance. define fracture (S) state space average minimal distancedisputed state closest undisputed state:(S) =1X|Sm 6=o | sS6=omintSSm 6=o(s, t).(13)things equal, lower fracture value tend increase propagationvalue information across state space, potentially resulting less explorationrequired. test metric, applied number scenarios varying fracturecoefficients. difficult construct scenarios vary fracture coefficient yetexpected value. scenarios Figure 22 constructedlength possible paths start state goal state xscenario. scenario, however, upper path lower path. mentortrained scenario penalizes lower path mentor learns takeupper path. imitator trained scenario upper path penalizedtherefore take lower path. equalized difficulty problemsfollows: using generic -greedy learning agent fixed exploration schedule (i.e.,fixed initial rate decay) one scenario, tuned magnitude penaltiesexact placement along loops scenarios learner usingexploration policy would converge optimal policy roughly numbersteps each.20. expected distance would give accurate estimate fracture, difficult calculate.615fiPrice & BoutilierXX(a) = 0.5X(b) = 1.7(c) = 3.5X(d) = 6.0Figure 22: Fracture metric scenarios0.51.73.56.05 10260%1 10270%Observer Initial Exploration Rate5 103 1 103 5 104 1 10490%0%80%90%90 %30%100 %30 %70 %5 1051 105100 %100 %Figure 23: Percentage runs (of ten) converging optimal policy given fractureinitial exploration rateFigure 22(a), mentor takes top loop optimal run, imitatorwould take bottom loop. Since loops short length commonpath long, average fracture low. compare Figure 22(d), seeloops longthe majority states scenario loops.states loop distance nearest state observer mentorpolicies agree, namely, state loop. scenario therefore high averagefracture coefficient.Since loops various scenarios differ length, penalties inserted loopsvary respect distance goal state therefore affect total discounted expected reward different ways. penalties may also cause agentbecome stuck local minimum order avoid penalties exploration ratelow. set experiments, therefore compare observer agents basislikely converge optimal solution given mentor example.Figure 23 presents percentage runs (out ten) imitator convergedoptimal solution (i.e., taking lower loops) function exploration ratescenario fracture.21 see distinct diagonal trend table illustratingincreasing fracture requires imitator increase levels exploration order find21. reasons computational expediency, entries near diagonal computed. Sampling entries confirms trend.616fiImplicit Imitationoptimal policy. suggests fracture reflects feature RL domains mayimportant predicting efficacy implicit imitation.6.3 Suboptimality BiasImplicit imitation fundamentally biasing exploration observer. such,worthwhile ask positive effect observer performance. shortanswer mentor following optimal policy observer cause observerexplore neighborhood optimal policy generally bias observertowards finding optimal policy.detailed answer requires looking explicitly exploration reinforcement learning. theory, -greedy exploration policy suitable rate decay causeimplicit imitators eventually converge optimal solution unassistedcounterparts. However, practice, exploration rate typically decayed quicklyorder improve early exploitation mentor input. Given practical, theoreticallyunsound exploration rates, observer may settle mentor strategy feasible,non-optimal. easily imagine examples: consider situation agentobserving mentor following policy. Early learning process, valuepolicy followed mentor may look better estimated value alternativepolicies available observer. could case mentors policy actuallyoptimal policy. hand, may case one alternativepolicies, observer neither personal experience, observationsmentor, actually superior. Given lack information, aggressive exploitation policy might lead observer falsely conclude mentors policy optimal.implicit imitation bias agent suboptimal policy, reason expectagent learning domain sufficiently challenging warrant use imitationwould discovered better alternative. emphasize even mentors policysuboptimal, still provides feasible solution preferable solutionmany practical problems.regard, see classic exploration/exploitation tradeoff additionalinterpretation implicit imitation setting. component exploration ratecorrespond observers belief sufficiency mentors policy.paradigm, then, seems somewhat misleading think terms decision whetherfollow specific mentor not. question much explorationperform addition required reconstruct mentors policy.6.4 Specific Applicationssee applications implicit imitation variety contexts. emerging electroniccommerce information infrastructure driving development vast networksmulti-agent systems. networks used competitive purposes trade, implicitimitation used RL agent learn buying strategies informationfiltering policies agents order improve behavior.control, implicit imitation could used transfer knowledge existinglearned controller already adapted clients new learning controllercompletely different architecture. Many modern products elevator controllers617fiPrice & Boutilier(Crites & Barto, 1998), cell traffic routers (Singh & Bertsekas, 1997) automotive fuelinjection systems use adaptive controllers optimize performance systemspecific user profiles. upgrading technology underlying system, quitepossible sensors, actuators internal representation new systemincompatible old system. Implicit imitation provides method transferringvaluable user information systems without explicit communication.traditional application imitation-like technologies lies area bootstrappingintelligent artifacts using traces human behavior. Research within behavioral cloningparadigm investigated transfer applications piloting aircraft (Sammut et al.,1992) controlling loading cranes (Suc & Bratko, 1997). researchers investigated use imitation simplify programming robots (Kuniyoshi, Inaba, &Inoue, 1994). ability imitation transfer complex, nonlinear dynamic behaviorsexisting human agents makes particularly attractive control problems.7. Extensionsmodel implicit imitation presented makes certain restrictive assumptions regarding structure decision problem solved (e.g., full observability, knowledgereward function, discrete state action space). simplifying assumptionsaided detailed development model, believe basic intuitions muchtechnical development extended richer problem classes. suggest severalpossible extensions section, provides interesting avenuefuture research.7.1 Unknown Reward Functionscurrent paradigm assumes observer knows reward function.assumption consistent view RL form automatic programming.can, however, relax constraint assuming ability generalize observed rewards.Suppose expected reward expressed terms probability distributionfeatures observers state, Pr(r|f (so )). model-based RL, distributionlearned agent experience. featuresapplied mentors state sm , observer use learnedreward distribution estimate expected reward mentor states well. extendsparadigm domains rewards unknown, preserves abilityobserver evaluate mentor experiences terms.Imitation techniques designed around assumption observer mentorshare identical rewards, Utgoffs (1991), would course work absencereward function. notion inverse reinforcement learning (Ng & Russell, 2000) couldadapted case well. challenge future research would explore synthesisimplicit imitation reward inversion approaches handle observers priorbeliefs intermediate level correlation reward function observermentor.618fiImplicit Imitation7.2 Interaction agentscast general imitation model framework stochastic games, restriction model presented thus far noninteracting games essentially meansstandard issues associated multiagent interaction arise. are, course,many tasks require interactions agents; cases, implicit imitation offerspotential accelerate learning. general solution requires integration imitationgeneral models multiagent RL based stochastic Markov games (Littman,1994; Hu & Wellman, 1998; Bowling & Veloso, 2001). would doubt ratherchallenging, yet rewarding endeavor.take simple example, simple coordination problems (e.g., two mobile agentstrying avoid carrying related tasks) might imagine imitatorlearning mentor reversing roles roles consideringobserved state transition influenced joint action. generalsettings, learning typically requires great care, since agents learning nonstationaryenvironment may converge (say, equilibrium). Again, imitation techniques offercertain advantages: instance, mentor expertise suggest means coordinatingagents (e.g., providing focal point equilibrium selection, making clearspecific convention always passing right avoid collision).challenges opportunities present imitation used multiagent settings. example, competitive educational domains, agentschoose actions maximize information exploration returns exploitation;must also reason actions communicate information agents.competitive setting, one agent may wish disguise intentions, contextteaching, mentor may wish choose actions whose purpose abundantly clear.considerations must become part action selection process.7.3 Partially Observable Domainsextension model partially observable domains critical, since unrealisticmany settings suppose learner constantly monitor activities mentor.central idea implicit imitation extract model information observationsmentor, rather duplicating mentor behavior. means mentors internalbelief state policy (directly) relevant learner. take somewhatbehaviorist stance concern mentors observed behaviorstell us possibilities inherent environment. observer keepbelief state mentors current state, done using estimatedworld model observer uses update belief state.Preliminary investigation model suggests dealing partial observabilityviable. derived update rules augmented partially observable updates.updates based Bayesian formulation implicit imitation is, turn, basedBayesian RL (Dearden et al., 1999). fully observable contexts, seeneffective exploration using mentor observations possible fully observable domainsBayesian model imitation used (Price & Boutilier, 2003). extensionmodel cases mentors state partially observable reasonably straightforward.anticipate updates performed using belief state mentors state619fiPrice & Boutilieraction help alleviate fracture could caused incomplete observationbehavior.interesting dealing additional factor usual exploration-exploitationtradeoff: determining whether worthwhile take actions render mentorvisible (e.g., ensuring mentor remains view source information remainsavailable learning).7.4 Continuous Model-Free Learningmany realistic domains, continuous attributes large state action spaces prohibituse explicit table-based representations. Reinforcement learning domainstypically modified make use function approximators estimate Q-functionpoints direct evidence received. Two important approachesparameter-based models (e.g., neural networks) (Bertsekas & Tsitsiklis, 1996)memory-based approaches (Atkeson, Moore, & Schaal, 1997). approaches,model-free learning generally employed. is, agent keeps value function usesenvironment implicit model perform backups using sampling distributionprovided environment observations.One straightforward approach casting implicit imitation continuous setting wouldemploy model-free learning paradigm (Watkins & Dayan, 1992). First, recall augmented Bellman backup function used implicit imitation:((V (s) = Ro (s) + max maxaAoX)Pro (s, a, t)V (t) ,tS)XPrm (s, t)V (t)(14)tSexamine augmented backup equation, see convertedmodel-free form much way ordinary Bellman backup. use standardQ-function observer actions, add one additional action correspondsaction taken mentor.22 imagine observer state ,took action ao ended state s0o . time, mentor made transitionstate sm s0m . write:Q(so , ao ) = (1 )Q(so , ao ) + (Ro (so , ao ) + maxmaxa0 AoQ(s0o , a0 ) ,Q(sm , ) = (1 )Q(sm , ) + (Ro (sm , ) + max max0AoQ(s0o , )Q(s0m , a0 ) ,(15)Q(s0m , )discussed earlier, relative quality mentor observer estimates Qfunction specific states may vary. Again, order avoid inaccurate prior beliefsmentors action models bias exploration, need employ confidence measuredecide apply augmented equations. feel natural settingkind tests memory-based approaches function approximation. Memorybased approaches, locally-weighted regression (Atkeson et al., 1997), provide estimates functions points previously unvisited, also maintain evidence22. doesnt imply observer knows actions corresponds .620fiImplicit Imitationset used generate estimates. note implicit bias memory-based approaches assumes smoothness points unless additional data proves otherwise.basis bias, propose compare average squared distance queryexemplars used estimate mentors Q-value average squared distancequery exemplars used observer-based estimate heuristically decideagent reliable Q-value.approach suggested benefit prioritized sweeping. Prioritizedsweeping, however, adapted continuous settings (Forbes & Andre, 2000).feel reasonably efficient technique could made work.8. Related WorkResearch imitation spans broad range dimensions, ethological studies,abstract algebraic formulations, industrial control algorithms. fields crossfertilized informed other, come stronger conceptual definitionsbetter understanding limits capabilities imitation. Many computationalmodels proposed exploit specialized niches variety control paradigms,imitation techniques applied variety real-world control problems.conceptual foundations imitation clarified work natural imitation. work apes (Russon & Galdikas, 1993), octopi (Fiorito & Scotto, 1992),animals, know socially facilitated learning widespread throughout animal kingdom. number researchers pointed out, however, social facilitationtake many forms (Conte, 2000; Noble & Todd, 1999). instance, mentors attentionobject draw observers attention thereby lead observer manipulate object independently model provided mentor. True imitationtherefore typically defined restrictive fashion. Visalberghi Fragazy (1990)cite Mitchells definition:1. something C (the copy behavior) produced organism2. C similar something else (the Model behavior)3. observation necessary production C (above baseline levels Coccurring spontaneously)4. C designed similar5. behavior C must novel behavior already organized precise wayorganisms repertoire.definition perhaps presupposes cognitive stance towards imitationagent explicitly reasons behaviors agents behaviors relateaction capabilities goals.Imitation analyzed terms type correspondence demonstratedmentors behavior observers acquired behavior (Nehaniv & Dautenhahn,1998; Byrne & Russon, 1998). Correspondence types distinguished level.action level, correspondence actions. program level, actions621fiPrice & Boutiliermay completely different correspondence may found subgoals.effect level, agent plans set actions achieve effect demonstratedbehavior direct correspondence subcomponents observersactions mentors actions. term abstract imitation proposedcase agents imitate behaviors come imitating mental stateagents (Demiris & Hayes, 1997).study specific computational models imitation yielded insightsnature observer-mentor relationship affects acquisition behaviorsobservers. instance, related field behavioral cloning, observedmentors implement conservative policies generally yield reliable clones (Urbancic& Bratko, 1994). Highly-trained mentors following optimal policy small coveragestate space yield less reliable clones make mistakes (Sammut et al.,1992). partially observable problems, learning perfect oracles disastrous,may choose policies based perceptions available observer. observertherefore incorrectly biased away less risky policies require additionalperceptual capabilities (Scheffer, Greiner, & Darken, 1997). Finally, observedsuccessful clones would often outperform original mentor due cleanup effect(Sammut et al., 1992).One original goals behavioral cloning (Michie, 1993) extract knowledgehumans speed design controllers. extracted knowledgeuseful, argued rule-based systems offer best chance intelligibility(van Lent & Laird, 1999). become clear, however, symbolic representationscomplete answer. Representational capacity also issue. Humans often organizecontrol tasks time, typically lacking state perception-based approachescontrol. Humans also naturally break tasks independent componentssubgoals (Urbancic & Bratko, 1994). Studies also demonstrated humansgive verbal descriptions control policies match actual actions(Urbancic & Bratko, 1994). potential saving time acquisition borneone study explicitly compared time extract rules time requiredprogram controller (van Lent & Laird, 1999).addition traditionally considered imitation, agent may also faceproblem learning imitate finding correspondence actionsstates observer mentor (Nehaniv & Dautenhahn, 1998). fully credible approachlearning observation absence communication protocols dealissue.theoretical developments imitation research accompanied numberpractical implementations. implementations take advantage properties different control paradigms demonstrate various aspects imitation. Early behavioral cloningresearch took advantage supervised learning techniques decision trees (Sammutet al., 1992). decision tree used learn human operator mapped perceptions actions. Perceptions encoded discrete values. time delay insertedorder synchronize perceptions actions trigger. Learning apprentice systems(Mitchell et al., 1985) also attempted extract useful knowledge watching users,goal apprentices independently solve problems. Learning apprentices closelyrelated programming demonstration systems (Lieberman, 1993). Later efforts used622fiImplicit Imitationsophisticated techniques extract actions visual perceptions abstractactions future use (Kuniyoshi et al., 1994). Work associative recurrent learning models allowed work area extended learning temporal sequences(Billard & Hayes, 1999). Associative learning used together innate followingbehaviors acquire navigation expertise agents (Billard & Hayes, 1997).related slightly different form imitation studied multi-agentreinforcement learning community. early precursor imitation found worksharing perceptions agents (Tan, 1993). Closer imitation ideareplaying perceptions actions one agent second agent (Lin, 1991; Whitehead,1991a). Here, transfer one agent another, contrast behavioral cloningstransfer human agent. representation also different. Reinforcement learningprovides agents ability reason effects current actions expectedfuture utility agents integrate knowledge knowledge extractedagents comparing relative utility actions suggested knowledgesource. seeding approaches closely related. Trajectories recorded humansubjects used initialize planner subsequently optimizes plan orderaccount differences human effector robotic effector (Atkeson &Schaal, 1997). technique extended handle notion subgoals withintask (Atkeson & Schaal, 1997). Subgoals also addressed others (Suc & Bratko,1997). work based idea agent extracting model mentorusing model information place bounds value actions usingreward function. Agents therefore learn mentors reward functions differentown.Another approach family based assumption mentor rational(i.e., follows optimal policy), reward function observer choosesset actions. Given assumptions, conclude actionchosen mentor particular state must higher value mentoralternatives open mentor (Utgoff & Clouse, 1991) therefore higher valueobserver alternative. system Utgoff Clouse therefore iteratively adjustsvalues actions constraint satisfied model. related approachuses methodology linear-quadratic control ( Suc & Bratko, 1997). First modelsystem constructed. inverse control problem solved find cost matrixwould result observed controller behavior given environment model. Recentwork inverse reinforcement learning takes related approach reconstructing rewardfunctions observed behavior (Ng & Russell, 2000). similar inversionquadratic control approach, formulated discrete domains.Several researchers picked idea common representations perceptual functions action planning. One approach using representationperception control based PID controller model. PID controller representsbehavior. output compared observed behaviors order select actionclosest observed behavior (Demiris & Hayes, 1999). Explicit motor actionschema also investigated dual role perceptual motor representations(Mataric, Williamson, Demiris, & Mohan, 1998).Imitation techniques applied diverse collection applications. Classical control applications include control systems robot arms (Kuniyoshi et al., 1994;623fiPrice & BoutilierFriedrich, Munch, Dillmann, Bocionek, & Sassin, 1996), aeration plants (Scheffer et al.,1997), container loading cranes (Suc & Bratko, 1997; Urbancic & Bratko, 1994). Imitation learning also applied acceleration generic reinforcement learning (Lin,1991; Whitehead, 1991a). Less traditional applications include transfer musical style(Canamero, Arcos, & de Mantaras, 1999) support social atmosphere (Billard, Hayes, & Dautenhahn, 1999; Breazeal, 1999; Scassellati, 1999). Imitation alsoinvestigated route language acquisition transmission (Billard et al., 1999;Oliphant, 1999).9. Concluding Remarksdescribed formal principled approach imitation called implicit imitation.stochastic problems explicit forms communication possible,underlying model-based framework combined model extraction provides alternativeimitation learning-by-observation systems. new approach makes usemodel compute actions imitator take without requiring observerduplicate mentors actions exactly. shown implicit imitation offer significanttransfer capability several test problems, proves robust facenoise, capable integrating subskills multiple mentors, able provide benefitsincrease difficulty problem.seen feasibility testing extends implicit imitation principled mannerdeal situations homogeneous action assumption invalid. Addingbridging capabilities preserves extends mentors guidance presence infeasible actions, whether due differences action capabilities local differences statespaces. approach also relates idea following sense imitatoruses local search model repair discontinuities augmented value function acting world. process applying imitation various domains,learned properties. particular developed fracture metriccharacterize effectiveness mentor given observer specific domain.also made considerable progress extending imitation new problem classes. modeldeveloped rather flexible extended several ways: example,Bayesian approach imitation building work shows great potential (2003);initial formulations promising approaches extending implicit imitation multiagent problems, partially observable domains domains reward functionspecified priori.number challenges remain field imitation. Bakker Kuniyoshi (1996)describe number these. Among intriguing problems unique imitation are:evaluation expected payoff observing mentor; inferring useful statereward mappings domains mentors observers; repairinglocally searching order fit observed behaviors observers capabilitiesgoals. also raised possibility agents attempting reasoninformation revealed actions addition whatever concrete value actionsagent.Model-based reinforcement applied numerous problems. Since implicit imitation added model-based reinforcement learning relatively little effort,624fiImplicit Imitationexpect applied many problems. basis simpleelegant theory Markov decision processes makes easy describe analyze. Thoughfocused simple examples designed illustrate different mechanismsrequired implicit imitation, expect variations approach provide interesting directions future research.AcknowledgmentsThanks anonymous referees suggestions comments earlier versionswork Michael Littman editorial suggestions. Price supported NCE IRISIII Project BAC. Boutilier supported NSERC Research Grant OGP0121843,NCE IRIS-III Project BAC. parts paper presented Implicit Imitation Reinforcement Learning, Proceedings Sixteenth International ConferenceMachine Learning (ICML-99), Bled, Slovenia, pp.325334 (1999) ImitationReinforcement Learning Agents Heterogeneous Actions, Proceedings FourteenthBiennial Conference Canadian Society Computational Studies Intelligence (AI2001), Ottawa, pp.111120 (2001).ReferencesAlissandrakis, A., Nehaniv, C. L., & Dautenhahn, K. (2000). Learning thingsimitation. Bauer, M., & Rich, C. (Eds.), AAAI Fall Symposium LearningThings, pp. 16 Cape Cod, MA.Atkeson, C. G., & Schaal, S. (1997). Robot learning demonstration. ProceedingsFourteenth International Conference Machine Learning, pp. 1220 Nashville, TN.Atkeson, C. G., Moore, A. W., & Schaal, S. (1997). Locally weighted learning control. ArtificialIntelligence Review, 11 (1-5), 75113.Bakker, P., & Kuniyoshi, Y. (1996). Robot see, robot do: overview robot imitation. AISB96Workshop Learning Robots Animals, pp. 311 Brighton,UK.Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton.Bertsekas, D. P. (1987). Dynamic Programming: Deterministic Stochastic Models. Prentice-Hall,Englewood Cliffs.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena, Belmont, MA.Billard, A., & Hayes, G. (1997). Learning communicate imitation autonomous robots.Proceedings Seventh International Conference Artificial Neural Networks, pp.76368 Lausanne, Switzerland.Billard, A., & Hayes, G. (1999). Drama, connectionist architecturefor control learningautonomous robots. Adaptive Behavior Journal, 7, 3564.Billard, A., Hayes, G., & Dautenhahn, K. (1999). Imitation skills means enhance learningsynthetic proto-language autonomous robot. Proceedings AISB99 SymposiumImitation Animals Artifacts, pp. 8895 Edinburgh.Boutilier, C. (1999). Sequential optimality coordination multiagent systems. ProceedingsSixteenth International Joint Conference Artificial Intelligence, pp. 478485 Stockholm.625fiPrice & BoutilierBoutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptionscomputational leverage. Journal Artificial Intelligence Research, 11, 194.Bowling, M., & Veloso, M. (2001). Rational convergent learning stochastic games. Proceedings Seventeenth International Joint Conference Artificial Intelligence, pp. 10211026Seattle.Breazeal, C. (1999). Imitation social exchange humans robot. ProceedingsAISB99 Symposium Imitation Animals Artifacts, pp. 96104 Edinburgh.Byrne, R. W., & Russon, A. E. (1998). Learning imitation: hierarchical approach. BehavioralBrain Sciences, 21, 667721.Canamero, D., Arcos, J. L., & de Mantaras, R. L. (1999). Imitating human performances automatically generate expressive jazz ballads. Proceedings AISB99 SymposiumImitation Animals Artifacts, pp. 11520 Edinburgh.Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally partially observable stochastic domains. Proceedings Twelfth National Conference ArtificialIntelligence, pp. 10231028 Seattle.Conte, R. (2000). Intelligent social learning. Proceedings AISB00 Symposium StartingSociety: Applications Social Analogies Computational Systems Birmingham.Crites, R., & Barto, A. G. (1998). Elevator group control using multiple reinforcement learningagents. Machine-Learning, 33 (23), 23562.Dean, T., & Givan, R. (1997). Model minimization Markov decision processes. ProceedingsFourteenth National Conference Artificial Intelligence, pp. 106111 Providence.Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning.Artificial Intelligence, 89, 219283.Dearden, R., Friedman, N., & Andre, D. (1999). Model-based bayesian exploration. ProceedingsFifteenth Conference Uncertainty Artificial Intelligence, pp. 150159 Stockholm.DeGroot, M. H. (1975). Probability statistics. Addison-Wesley, Reading, MA.Demiris, J., & Hayes, G. (1997). robots ape?. Proceedings AAAI Fall SymposiumSocially Intelligent Agents, pp. 2831 Cambridge, MA.Demiris, J., & Hayes, G. (1999). Active passive routes imitation. ProceedingsAISB99 Symposium Imitation Animals Artifacts, pp. 8187 Edinburgh.Fiorito, G., & Scotto, P. (1992). Observational learning octopus vulgaris. Science, 256, 54547.Forbes, J., & Andre, D. (2000). Practical reinforcement learning continuous domains. Tech. rep.UCB/CSD-00-1109, Computer Science Division, University California, Berkeley.Friedrich, H., Munch, S., Dillmann, R., Bocionek, S., & Sassin, M. (1996). Robot programmingdemonstration (RPD): Support induction human interaction. Machine Learning, 23,163189.Hartmanis, J., & Stearns, R. E. (1966). Algebraic Structure Theory Sequential Machines. PrenticeHall, Englewood Cliffs.Hu, J., & Wellman, M. P. (1998). Multiagent reinforcement learning: Theoretical frameworkalgorithm. Proceedings Fifthteenth International Conference Machine Learning,pp. 242250 Madison, WI.Kaelbling, L. P. (1993). Learning Embedded Systems. MIT Press, Cambridge,MA.626fiImplicit ImitationKaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: survey. JournalArtificial Intelligence Research, 4, 237285.Kearns, M., & Singh, S. (1998). Near-optimal reinforcement learning polynomial time. Proceedings Fifthteenth International Conference Machine Learning, pp. 260268 Madison,WI.Kuniyoshi, Y., Inaba, M., & Inoue, H. (1994). Learning watching: Extracting reusable taskknowledge visual observation human performance. IEEE Transactions RoboticsAutomation, 10 (6), 799822.Lee, D., & Yannakakis, M. (1992). Online miminization transition systems. Proceedings24th Annual ACM Symposium Theory Computing (STOC-92), pp. 264274 Victoria,BC.Lieberman, H. (1993). Mondrian: teachable graphical editor. Cypher, A. (Ed.), WatchDo: Programming Demonstration, pp. 340358. MIT Press, Cambridge, MA.Lin, L.-J. (1991). Self-improvement based reinforcement learning, planning teaching. MachineLearning: Proceedings Eighth International Workshop (ML91), 8, 32327.Lin, L.-J. (1992). Self-improving reactive agents based reinforcement learning, planningteaching. Machine Learning, 8, 293321.Littman, M. L. (1994). Markov games framework multi-agent reinforcement learning.Proceedings Eleventh International Conference Machine Learning, pp. 157163 NewBrunswick, NJ.Lovejoy, W. S. (1991). survey algorithmic methods partially observed Markov decisionprocesses. Annals Operations Research, 28, 4766.Mataric, M. J. (1998). Using communication reduce locality distributed multi-agent learning.Journal Experimental Theoretical Artificial Intelligence, 10 (3), 357369.Mataric, M. J., Williamson, M., Demiris, J., & Mohan, A. (1998). Behaviour-based primitivesarticulated control. R. Pfiefer, B. Blumberg, J.-A. M. . S. W. W. (Ed.), Fifth Internationalconference simulation adaptive behavior SAB98, pp. 165170 Zurich. MIT Press.Meuleau, N., & Bourgine, P. (1999). Exploration multi-state environments: Local mesuresback-propagation uncertainty. Machine Learning, 32 (2), 117154.Mi, J., & Sampson, A. R. (1993). comparison Bonferroni Scheffe bounds. JournalStatistical Planning Inference, 36, 101105.Michie, D. (1993). Knowledge, learning machine intelligence. Sterling, L. (Ed.), IntelligentSystems. Plenum Press, New York.Mitchell, T. M., Mahadevan, S., & Steinberg, L. (1985). LEAP: learning apprentice VLSIdesign. Proceedings Ninth International Joint Conference Artificial Intelligence,pp. 573580 Los Altos, California. Morgan Kaufmann Publishers, Inc.Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning lessdata less real time. Machine Learning, 13 (1), 10330.Myerson, R. B. (1991). Game Theory: Analysis Conflict. Harvard University Press, Cambridge.Nehaniv, C., & Dautenhahn, K. (1998). Mapping dissimilar bodies: Affordancesalgebraic foundations imitation. Proceedings Seventh European WorkshopLearning Robots, pp. 6472 Edinburgh.627fiPrice & BoutilierNg, A. Y., & Russell, S. (2000). Algorithms inverse reinforcement learning. ProceedingsSeventeenth International Conference Machine Learning, pp. 663670 Stanford.Noble, J., & Todd, P. M. (1999). really imitation? review simple mechanisms socialinformation gathering. Proceedings AISB99 Symposium Imitation AnimalsArtifacts, pp. 6573 Edinburgh.Oliphant, M. (1999). Cultural transmission communications systems: Comparing observationalreinforcement learning models. Proceedings AISB99 Symposium ImitationAnimals Artifacts, pp. 4754 Edinburgh.Price, B., & Boutilier, C. (2003). Bayesian approach imitation reinforcement learning. Proceedings Eighteenth International Joint Conference Artificial Intelligence Acapulco.appear.Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.John Wiley Sons, Inc., New York.Russon, A., & Galdikas, B. (1993). Imitation free-ranging rehabilitant orangutans (pongopygmaeus). Journal Comparative Psychology, 107 (2), 147161.Sammut, C., Hurst, S., Kedzier, D., & Michie, D. (1992). Learning fly. ProceedingsNinth International Conference Machine Learning, pp. 385393 Aberdeen, UK.Scassellati, B. (1999). Knowing imitate knowing succeed. ProceedingsAISB99 Symposium Imitation Animals Artifacts, pp. 105113 Edinburgh.Scheffer, T., Greiner, R., & Darken, C. (1997). experimentation better perfectguidance. Proceedings Fourteenth International Conference Machine Learning,pp. 331339 Nashville.Seber, G. A. F. (1984). Multivariate Observations. Wiley, New York.Shapley, L. S. (1953). Stochastic games. Proceedings National Academy Sciences, 39,327332.Singh, S. P., & Bertsekas, D. (1997). Reinforcement learning dynamic channel allocationcellular telephone systems. Advances Neural information processing systems, pp. 974980 Cambridge, MA. MIT Press.Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable Markovprocesses finite horizon. Operations Research, 21, 10711088.Suc, D., & Bratko, I. (1997). Skill reconstruction induction LQ controllers subgoals.Proceedings Fifteenth International Joint Conference Artificial Intelligence, pp.914919 Nagoya.Sutton, R. S. (1988). Learning predict method temporal differences. Machine Learning,3, 944.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,Cambridge, MA.Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. ICML93, pp. 33037.Urbancic, T., & Bratko, I. (1994). Reconstruction human skill machine learning. EleventhEuropean Conference Artificial Intelligence, pp. 498502 Amsterdam.628fiImplicit ImitationUtgoff, P. E., & Clouse, J. A. (1991). Two kinds training information evaluation functionlearning. Proceedings Ninth National Conference Artificial Intelligence, pp. 596600 Anaheim, CA.van Lent, M., & Laird, J. (1999). Learning hierarchical performance knowledge observation.Proceedings Sixteenth International Conference Machine Learning, pp. 229238Bled, Slovenia.Visalberghi, E., & Fragazy, D. (1990). monkeys ape?. Parker, S., & Gibson, K. (Eds.),Language Intelligence Monkeys Apes, pp. 247273. Cambridge University Press,Cambridge.Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.Whitehead, S. D. (1991a). Complexity analysis cooperative mechanisms reinforcement learning. Proceedings Ninth National Conference Artificial Intelligence, pp. 607613Anaheim.Whitehead, S. D. (1991b). Complexity cooperation q-learning. Machine Learning. Proceedings Eighth International Workshop (ML91), pp. 363367.629fifffi ffffff!"# %$'&(*),+--.0/1+2.3+0456789:;<&-0=-+0>?8ff#$;@&-0=-.ACBEDGFIH"JKGLNMPORQTSGFISGJUVJVHWJVXZY[]\_^`\badcfehgiaWk:j lRmonqp%rs1tWuWv@wyx'z{|q}~'i,]00_01h*_WI00*0_d~W0b_]0]0,0_0iy00!0E 00]W0]0fifi!qqfifiqfi"`\b:Pen p Z\ba lq~W0bI_'0qq}s11|qv@wy{wyW|"1}}{t"~W0_Ry00yiy00hfifi!fiqVfifififififififififififi1IVfi0'1V1I1'ifiG011V]1Tb11oV0o1107]fffi10E0d 11Vyy 1fiV*1 fi7oV01fi1@701WV07V V7d01fi7fi fiV1 7 ooVfi01o E fi 7fifi1 E0 "!$#% %V7o&71')(* E71 7 0+ 7077@0V7fi V 1%1 +0, 00-'h+070V:fifi/. 1 710R710"!V0'11107123fi1Vfi7 fiqE'41@fiV,10R17V,V 0* /fi5671'V,fi 71 17V1,7VW0fi0*1&fi+11 0789!0+ 1/:1fi 07Vh7,017V7fi 0 0Z99!;.d 01fi'0010+ V10 1117 0'<=7 V07 +0 Vfi 0fi>#ff!"?@(A.d +0V1, 7V 0o 189! V01Z17V,V 1V6<=7<V0@V1Z10fi 01,70d7 1h71]Vh7]'0,7fi '1'7ff!B?7Vfi 0T1,0C +01"+0fiDV14/:EV1,7V0fi,701VV701C77111,81I+07h9! 0ff!B?-FDG HI fiJLK9MNOJPQ/RSRATVUWYX%Z[X%\VW4Z]^_W4R/^W4U_W4Z`]aU'b/W4^'Q/ZCcfiW4^_] XVd Z]e[X%fgTVQ/]hXjiTSkffXVd ZlQ/Ud ZS\mXR/^_TVfCX%f*d2n2d ]eoi/d U]a^d f/Q/p] TVZCq*r&]@U_TSkW'RgTSd Z`] sU_b/W;^_WcfiWd tVW4U&U_TSkW;Z/W4uvd Z*wxTV^k5X%] TVZmX%fgTVQ/]9]ab/WhiVTSkffXVd ZCq-yBTuzU_b/TVQCn2i{U_b/WQ/RBi/X%]aW6b/W4^ji/d U_]a^d f/QS] TVZ|d Z}]ab/W~n2d \Vb]TSwC]abCd Ud ZCwTV^kffX%] TVZC*S//>d Ufe[wX%^]ab/WlkTVU_]cfiTSkffkTVZ>kW4]abSTgijd Zmc4X%UW']abSWhd ZCwTV^kffX%] TVZ>cfiTSkW4U{d Z]ab/WhwxTV^kTSw1X%ZW4tVW4Z] qyBTu9W4tVW4^s)]ab/W4^_W6X%^_WZQCkW4^_TVQ/U&u@Wn2n pZ/T uZlW41XVkRCn W4U0Ub/T u8d ZS\6]abCX%]0ZCXVd tVWjcfiTVZCi/d ] TVZCd Z/\lc4X%Zn WXVi6]aT5R/^_TVfCn WkUqLW\Sd tVWQ/U_]&]u9T5TSwS]abSWkvb/W4^_WqeAff b/W~~/;77),6affTVU_]aWn2n W4^s"VVVVEtVTVUYP/XatX%Z] sBVVVVahPQ/R/RgTVU_W8]abCX%]n/ p \[WeVTVQC ^_W TVZX\SXVkW U_b/TuX%ZCi5\Sd tVW4ZX5cOb/TSd2cfiWhTSwg]ab/^_W4WiT`TV^UqCW4bCd ZCi5TVZ/Wjd U6XYc4X%^/fgW4bCd ZCiff]ab/WTV]ab/W4^_UX%^_W\VTSX%]aUqTVQ5RCd2cOiVT`TV^;Vq&CWwTV^_W0TVRgW4ZCd ZS\5iT`TV^'VsS5TVZ`]ey;XVn2n2s]ab/WbSTVU_]0ub/T{Z/TuUubCX%]jd UfgW4bCd ZCijWXVcObziTTV^-TVRgW4Z/UjiT`TV^VsDu0bCd2cbobCX%UYX6\VTSX%] qmy"W5]ab/W4ZX%U_`U8eVTVQzd2wCeVTVQoU_] d2n2nuX%Z`]{]aT] X%VWYubCX%] U{fgW4bCd Z*iliVT`TV^5VsDTV^{]aT] X%VW5ub*X%] U{fgW4bCd ZCimiTTV^5d Z/U_]aWXVi/qPbSTVQCn2ijeVTVQU_u8d ] cObCYr;UU_QCkffd Z/\h]abCX%] sEd ZCd ] d2XVn2n es]ab/Wjc4X%^uX%U;WQ*XVn2n emn2d VWn e]aTfffgWfgW4bCd ZCi WXVcOb~TSwS]ab/W5iTTV^_UsZCXVd tVWhcfiTVZCi/d ] TVZCd Z/\ U_Q/\V\VW4U]aU9]abCX%] s\Sd tVW4Z]abCX%]'d ]d U"Z/TV]9fgW4bCd ZCiffiTTV^'Vs7d ]d U9WQ*XVn2n e~n2d VWn eff]aT8fgWfgW4bCd ZCiiTTV^8ffX%ZCiiVT`TV^8Vq b`QSUs`]ab/W4^_WYd UZST6^_WX%U_TVZ]aTffUu8d ] cbCq;yBTu9W4tVW4^sEX%Z/TV]ab/W4^ X%^_\VQCkW4Z]U_Q/\V\VW4U]aUBeVTVQ6U_b/TVQCn2i;U_u ] cbC@d2wSX@\VTSX%];d UfgW4bCd Z*i8iTTV^0ubCd2cOb6bCX%R/RgW4Z/ULu8d ]abhR/^_TVfCX%f*d2n2d ]eVVVas+--.b##7; ;@I 9R 9 9b8ff#$#fiffff0$V#"#;Vfi " |qxq}t~"}s1|U_u8d ] cObCd Z/\jb/Wn R/U*d2wXjc4X%^6d U'fgW4bCd ZCiiVT`TV^6Yu0bCd2cblbCX%R/RgW4Z/U;u8d ]abRS^_TVfCX%fCd2n2d ]eoVVVas7U_u8d ] cb*d Z/\bQ/^_]aUq*bCd2cb[X%^_\VQCkW4Z]8d U^d \Vb`]e @ bSW', 7%,/ %E7)+"a@X%^py;d2n2n Wn/XVn 1sVVVVS{X%^iZ/W4^sVVVV/5TVU_]aWn2n W4^sn/ p \[WVVVVa{{wA]abS^_W4W8RS^d U_TVZ/W4^_Uh`s*sEX%ZCils/]u9T>X%^_W8]aTYfgW8W4`WcfiQ/]aWiSsSfSQ/]8iTW4U;ZSTV];Z/T uubCd2cObCqbQ/UsC ]abCd Z/U0]abCX%]']ab/WRS^_TVfCX%fCd2n2d ]e]abCX%]0-u8d2n2n/fgWW4WcfiQ/]aWimd UhVVjwxTV^'[V`44Vq8y"WUXaeU]aT]ab/WaXVd2n W4^s %PSd ZCcfiWWd ]ab/W4^TV^jd UcfiW4^_] XVd ZCn e\VTSd Z/\>]aTfgWjW4`WcfiQ/]aWiSsBeVTVQu d2n2n*\Sd tVWokWZ/TZCwTV^kffX%] TVZoX%fgTVQ/]k0ejT uZ[cbCX%ZCcfiW4Uhd2w/eVTVQ~\Sd tVWjkW]ab/WZ*XVkWTSw7TVZ/W5kffX%ZCsWd ]ab/W4^h9TV^8sub/TU@\VTSd Z/\8]aT8fgWW4`WcfiQS]aWi/q2>CQ/]"]abSW4ZCsZSTYkffX%]a]aW4^-ubCX%]B]abSW0aXVd2n W4^@UXaeUsZ*XVd tVWhcfiTVZCiSd ] TVZCd Z/\n WXViU]aTfffgWn2d W4tVW]abCX%]b*d U8cbCX%Z*cfiW{TSw7W4`WcfiQ/] TVZu9W4Z] iVT uZ}w^_TSkVV{]aTVVVqbSW4^_W>X%^_WZ`Q*kW4^_TVQ/UhTV]ab/W4^ffu@Wn2n pZ/T uZW4gXVkR*n W4Uffub/W4^_WZCXVd tVW}cfiTVZ*i/d ] TVZCd Z/\\Sd tVW4UYubCX%]U_W4WkUj]aT}fgW[X%Zd ZCX%R/RS^_TVR/^d2X%]aW[X%Z/U_u@W4^sd ZCc4n QCi/d ZS\>]ab/W+fi [A,+a{X%^iZ/W4^sVVVVtVTVU6P/XatX%Z`] s*VVVVsLVV%S;X%Z*i6]ab/W,/fi,7),a^_W4Q/ZCiSsDVVVVLPbCXVwW4^sDVVVVLy;XVn RgW4^_Z[Q/]a] n WsEVVVVaq &b`eiTW4U{ZCXVd tVWlcfiTVZCi/d ] TVZCd Z/\~\Sd tVW5]ab/Wffu0^_TVZ/\}X%Z/U_u@W4^Yd Z>UQCcb>W41XVkRCn W4Ur'UYX%^_\VQSWif`ey'XVn RAW4^Z|X%ZCi Q/]a] n WmaVVVV8X%ZCi}PbCXVwW4^jaVVVVas*]ab/Wff^WXVn1R/^_TVf*n Wkd U]abCX%]8u9WlX%^_WffZ/TV]cfiTVZCi/d p] TVZCd Z/\md Z~]ab/W8^d \Vb]0U_RCXVcfiWq94w7u9W8u9TV^_>d Z[X5n2X%^_\VW4^~U_TVR/bCd U] d2c4X%]aWi/8U_RCXVcfiWsu0b/W4^_W{u@W] X%VW ]ab/WR/^_TV]aT1cfiTSnQ/U_Wife~ffTVZ]elad ZmDgXVkR*n W Vq2V@X%ZCi]ab/W;XVd2n W4^ad Z11XVkRCn W8Vq2V-d Z]aTjXVc4cfiTVQ/Z] sgcfiTVZ/pi/d ] TVZCd ZS\~iT`W4UiVWn2d tVW4^0]ab/W^d \Vb]X%Z/U_u@W4^q&BTVQ/\VbCn ejU_RgWX%gd Z/\Ss]abSWU_TVR/bCd U_] d2c4X%]aWi6U_RCXVcfiW5cfiTVZ/Ud U]aUTSwXVn2n]abSW{RgTVU_Ud fCn W{UWQ/W4Z*cfiW4U;TSw7W4tVW4Z]aU']ab*X%] cfiTVQCn2iffbCX%R/RgW4ZoawTV^;W41XVkRCn Ws/ubCX%]85TVZ`]eu@TVQCn2iUX_e|d ZoWXVcObc4d ^cfiQCkU] X%ZCcfiWs*TV^8u0bCX%]8]abSW5aXVd2n W4^ u@TVQCn2i~UXaezd ZoWXVcObc4d ^cfiQCkU_] X%Z*cfiWasLu8d ]ab]ab/Wd ^R/^_TVf*X%fCd2n2d ]eq + y"T u@W4tVW4^s)u@TV^_1d Z/\d Zj]ab/W'U_TVR/bCd U] d2c4X%]aWi U_RCXVcfiW'bCX%U-R/^_TVfCn WkU@]aTTSq&TV^&TVZ/W']abCd Z/\Ss]d UhZ/TV]XVn uX_e`Uc4n WX%^6u0bCX%]8]abSWj^Wn W4t)X%Z]hR/^_TVfCX%fCd2n2d ] W4U~d Z[]ab/WU_TVRSbCd U_] d2c4X%]aWilU_RCXVcfiW>X%^_WqVTV^W41XVkRCn WsubCX%];d U9]ab/WR/^_TVf*X%fCd2n2d ]e6]abCX%]9]ab/WaXVd2n W4^9UXaeU&d2wg&X%ZCiff0X%^_W]aTfgWW4WcfiQ/]aWi/5fiZCiW4Wi/sZ5UTSkW c4X%U_W4UsAd ]d U"Z/TV]9W4tVW4Zmc4n WX%^-ub*X%]B]ab/W0Wn WkW4Z`]aU-TSw]ab/W8n2X%^\VW4^@URCXVcfiW X%^_Wq&5TV^_W4TtVW4^s`W4tVW4Zub/W4Z]abSWffWn WkW4Z]aUX%ZCij]abSWff^_Wn W4tX%Z`]8R/^_TVfCX%fCd2n2d ] W4UX%^_WffZ/T u0ZCsg]ab/W6Ud 4WYTSwD]abSWffU_TVR/bCd U] d2c4X%]aWiU_RCXVcfiWjkffXaefgWcfiTSkWYX%Zod UU_Q/Ws1X%U0]abSWYwxTSn2n Tu8d Z/\ffW41XVkRCn WU_b/T u0UqeC PQ/RSRATVUW;]abCX%]Xu9TV^n2iiVW4Ucfi^d fgW4UubCd2cObTSwCVV{RgW4TVRCn WbCXatVWjX6cfiW4^_] XVd Zi/d UWX%U_Wqn/ p \[Wru9TV^n2ioc4X%ZofAWlcbCX%^XVcfi]aW4^d 4Wilf`eX6]aQSRCn WffTSw-VV%UX%ZCi>%UsEu0b/W4^_Wff]ab/W5O]abzcfiTSkRATVZSW4Z`]Yd UZCi/d t1d2iQCXVnAbCX%U]ab/Wi/d U_WX%UWq b/W4^_W~X%^_W~ &-- RATVUUd fCn W6u@TV^n2iUqQ/^]ab/W4^{U_Q/RSRATVUW ]ab*X%]{]ab/W%X%\VW4Z] ZvQSW4U_] TVZvd UX>cfiTSkRSQ/]aW4^YUe`U_]aWk5qfiZCd ] d2XVn2n es-]abSWX%\VW4Z]YbCX%UYZ/Td Z*wxTV^k5X%] TVZCsX%ZCicfiTVZ/Ud2iVW4^_U;XVn2nS &-- u9TV^n2iVU"W QCXVn2n en2d VWn eq b/WX%\VW4Z]9]abSW4Zff^_WcfiWd tVW4U{d ZCwTV^kffX%] TVZ5]ab*X%];d U'X%U_UQCkWi]aTfgWh]a^_Q/W~X%fgTVQ/]ubCd2cOb>u9TV^n2id U{]ab/W~XVcfi]aQCXVngu@TV^n2i/q bCd U5d Z*wxTV^k5X%] TVZ|cfiTSkW4UYd Z>]abSW~wxTV^kTSwU_] X%]aWkW4Z]aU{n2d VW%d ZCiSd tgd2iQ*XVn@d U9Ud2chTV^'d Z*i/d tgd2iVQCXVn6d U"b/WXVn ]abegTV^6%X%]n WX%U_]'RAW4TVR*n WbCXatVW;]ab/Wi/d U_WX%UWVq"XVcObU_QCcbjU_] X%]aWkW4Z]c4X%ZfgWhd2iW4Z] /Wi8u8d ]abX;U_W4]9TSwRATVUUd fCn W'u9TV^n2iVUq-TV^&W4gXVkRCn Ws]ab/W U_] X%]aWkW4Z]l%X%]ffn WX%U_]ffRgW4TVRCn W bCX_tVW ]abSWi/d UWX%U_Wc4X%ZlfgWd2iW4Z] /WiYu8d ]ab]abSW8U_W4]TSwg]aQSRCn W4Uu8d ]abX%]5n WX%U_]5%UqYTV^Ud2kRCn2d2c4d ]es"X%U_U_Q*kWh]abCX%]]ab/WX%\VW4Z`]jd U\Sd tVW4Zd ZCwTV^kffX%] TVZUX_egd ZS\]ab/WXVcfi]aQCXVnEu9TV^n2i}d Ud Z}U_W4]Vs-wxTV^8t)X%^d TVQ/UU_W4]aUqmPQSR/RgTVU_W~X%]U_TSkW5RgTSd Z`]8]ab/WX%\VW4Z]8b*X%U8fgW4W4Z%D`,0-@, fffifixO E7 fffifix7fi fffi fi x!#"%$&('*),+ -V.x0/%21ff/ , 3 4A6 57x! 8 9ff:-; x=<>?, ".@6fi AVCBED44F#"$G&('*),+H8 :-ff fi /fi ?ff 6 ?-a2fi V8 fi xJ /8 K1 L/MO, -8 NNOP?@%fi ,fi Qfi Vfi /fi x0R :@ H ff R 6 %ff :- C fi V,S%4 T1A% x, / DC<2? /U ?fi AVO W<( Xfi ;?fifi ARU IVY:Z96 [#1fi fi ,.S,\T1 ]x ^ ff x,C:-X 1, x;; @/0x ffx 9/%RD+6 _D+8 CRfi`7a7afibc edbgfb]fbz}zhcyz1s1t yz|qu]aTSn2ih]abCX%]-]ab/WffXVcfi]aQCXVnu9TV^n2id Ud Z} & ^i^i^i 4Fj7q b/W4ZCsgXVw]aW4^iTSd Z/\~cfiTVZCiSd ] TVZCd Z/\Ss]ab/WffX%\VW4Z]bCX%UXQ/ZCd2wTV^kR/^TVfCX%fCd2n2d ]eTVZo &Rk i^i^i k jSqCQ/]bST uiVT`W4U]abSWYX%\VW4Z`];VW4W4R~]a^XVcOTSw7]ab/W{u@TV^n2iU8d ]8cfiTVZSUd2iW4^_URgTVU_Ud fCn WY]8cfiW4^] XVd ZCn eu8d2n2nZ/TV]W4RCn2d2c4d ] n en2d U_]{]ab/Wk51]ab/W4^_W~X%^W6Ud2kRCn e]aT`T}kffX%Zeq-Z/WhRgTVU_Ud f*d2n2d ]ed U{]abCX%]5d ]VW4W4R/U8]a^XVcTSw7ubCX%]8d ]0bCX%UfgW4W4Z~]aTSn2i/]ab/W{RgTVU_Ud fCn Wu9TV^n2iVU8X%^_W{]ab/W4Z]ab/W{TVZ/W4UhcfiTVZ/Ud U_]aW4Z]0u ]abubCX%]8d ]0bCX%UfgW4W4Zm]aTSn2i/qYCQ/]]abCd U5n WXViVU]aT]u@TTVf`t1d TVQ/U{R/^TVfCn WkU8cb/WcOgd Z/\wxTV^5cfiTVZ/Ud U_]aW4ZCcfielu8d ]abmubCX%]ffd ]bCX%U&fgW4W4Z]aTSn2ikffX_eYfgWbCX%^iSsAX%Z*id2wCd ]&bCX%U&fgW4W4Z]aTSn2mlm]abCd ZS\VUwxTV^8n2X%^\V.W l"s^_WkWk0fAW4^d Z/\h]ab/WkXVn2nBkffX_efgWd ZCwWX%Ud fCn Wq8fiZUd ]aQCX%] TVZ/U{u0b/W4^_Wh]ab/W4U_W6]u9TRS^_TVfCn WkU5X%^d UWs"X%ZX%\VW4Z]YkffXaemZ/TV]fgWX%fCn W{]aTlcfiTVZCi/d ] TVZX%RSR/^_TVR/^d2X%]aWn eq11XVkRCn W~Vq2 RS^_T t1d2iW4U{U_TSkW~kTV] t)X%] TVZzwxTV^u@TV^_1d Z/\}d Z>]ab/WhUk5XVn2n W4^s"kTV^W Z*XVd tVWffU_RCXVcfiWq1pXVkRCn W4U6Vq2jX%ZCi~Vq2U_b/Tu]abCX%];]abCd U6d U0Z/TV]hXVn uX_e`U5X%R/RS^_TVR/^d2X%]aWq bQ/UsEX%ZTVftgd TVQSUh Q/W4U_] TVZd Uub/W4Zd ]5d U5X%R/RS^_TVR/^d2X%]aWq{]]aQ/^_ZSU'TVQS]']ab*X%]']ab*d U5Q/W4U] TVZ|d UbCd \VbCn e^_Wn W4t)X%Z]5d Z]ab/WhU_] X%] U_] d2c4XVnX%^_WX%U@TSwD +o n +8fia,,YjX%ZCqph,/q&^d \Sd ZCXVn2n e6U_]aQCiSd Wiu8d ]abCd ZY]ab/W4U_W8cfiTVZ]aW4`]aUaQSfCd ZCs*VVVVKrXau8d2isrd2cVW4es@VVVVasBd ]uX%U5n2X%]aW4^5wxTVQ/Z*iff]abCX%]ffd ]ffXVn U_TRCn2X_e`U5XwQ/ZCi/XVkW4Z] XVn^_TSn W{d Z6]ab/W-U_] X%] U_] d2c4XVnVu9TV^_8TVZl) n n )/'u t{n Wd ZfCX%QCkffsSVVVVaq&EQ*d2n2i/d Z/\TVZ6R/^_W4t1d TVQ/UX%R/pR/^_TSXVcOb/W4UsAyBWd ]aaX%Z>X%ZCi5BQ/fCd ZaVVVVCR/^W4U_W4Z`]aWijX&ZSWcfiW4U_UX%^_e~X%ZCi{U_]Q vc4d W4Z]'cfiTVZ*i/d ] TVZmwTV^@ub/W4ZcfiTVZCi/d ] TVZ*d Z/\Yd Z6]ab/WjZCXVd tVW;U_RCXVcfiW{d U'X%R/R/^_TVR/^d2X%]aW;q w"T uXVi/X_e`U9]abCd UBU_TVpc4XVn2n Wi}x0y{z* ,, //Vy"/|p0}cfiTVZ*i/d ] TVZd U6X%ZmW4U_] X%f*n2d U_b/WiY]aT`TSnLd ZmU_Q/^tgd tXVnCX%Z*XVn e`Ud Uq awTV^T tVW4^_t1d W4uUs1U_W4Wad2n2n2stX%Z~iW4V^ ~EXVX%ZCsBTVfCd Z/Us)VVVVEw;d Wn U_W4Z*s/VVVVaq2W&W41XVkffd Z/W&]abCd U0cfi^d ]aW4^d TVZ~d Z TVQS^T uZCs^X%]ab/W4^i/d AW4^_W4Z];cfiTVZ`]aW4] sAX%ZCiU_b/Tu]abCX%];d ]'X%R/RCn2d W4U"^X%]ab/W4^B^X%^_Wn eq&PVRAWc4d *c4XVn2n esVu@WU_b/Tu]abCX%]B]ab/W4^W8X%^_W^_WXVn2d U_] d2c9U_W4]a] ZS\VU&ub/W4^_W;]ab/W;UXVkRCn W0URCXVcfiWhd U-U_]a^QCcfi]aQ/^_WiYd ZjUQCcbXuXae5]abCX%]d ]{d Ud2kRgTVU_Ud fCn W;]aTUX%] Uwe Lr60sCX%ZCihu@WR/^_Ttgd2iVWjX5cfi^d ]aW4^d TVZ]aTjb/Wn R[iW4]aW4^kffd Z/W8ub/W4]ab/W4^0TV^;Z/TV];]abCd U U']ab/Wc4X%U_WqW~XVn U_T\Sd tVW~X6A,)ficObCX%^XVcfi]aW4^d X%] TVZTSwD]abSW rh cfiTVZ*i/d ] TVZCsgfe~\Sd t1d Z/\}X ^X%ZCiVTSkffd 4WiXVn \VTV^d ]abCk]abCX%]{\VW4Z/W4^X%]aW4UjXVn2nBX%ZCijTVZ*n eiSd U_]a^d f/Q/] TVZSU5wxTV^{u0bCd2cb Lr6bSTSn2iUsg]ab/W4^W4f`elU_TSn t1d Z/\X%ZTVRgW4ZR/^TVfCn WkvRgTVU_Wi fed2n2n)W4]8XVn2qDaVVVVaqW ]ab/W4Z~Ub/T u]abCX%]0]ab/W8Ud ]aQCX%] TVZd U0u@TV^_U_Wd2wS]abSWjd ZCwTV^kffX%] TVZiTW4U0Z/TV]8cfiTSkWd Z]ab/WYwTV^kTSwDX%ZjW4tVW4Z`] q0TV^-]abCX%]c4X%UWsUW4tVW4^XVn`\VW4Z/W4^XVn2d X%] TVZ/UTSw1cfiTVZ*i/d ] TVZCd Z/\hbCX_tVW;fgW4W4ZjR/^_TVRgTVU_Wi/;q 7W4^_pbCX%R/U0]ab/W{fgW4U_]&Z/T uZ[X%^_W fi YS//u 4W4A^_W4esEVVVV'aXVn U_T5Z/TuZX%=U `L%)`X%ZCim~/o ph4pyB + n W9/fi7zyVU}``S~u t&QCn2n fCXVc1s*VVVV2LUd U_2X% ^s*VVVVPb/TV^WY4TVbSZ/U_TVZCs-VVVVhaXVn U_Tm`ZST uZX%U fi,/47aq 4W4A^_W4e|cfiTVZCi/d ] TVZ*d Z/\[d UX5\VW4Z/W4^XVn2d X%] TVZTSwTV^i/d Z*X%^_emcfiTVZ*i/d ] TVZCd Z/\SDl0}Q/RBi/X%] Z/\d U8X\VW4Z/W4^XVn2d X%] TVZmTSRw 4W4A^_W4emcfiTVZ*i/d ] TVZCd Z/\SqWYU_b/T u]abCX%q] W47^W4ecfiTVZ*i/d ] TVZCd Z/\SsEub/W4Z|X%RSRCn2d2c4X%fCn Ws@c4X%ZofgW6Q/U_] /WiQSZCiW4^5X%ZX%RSR/^_TVpR/^d2X%]aW\VW4ZSW4^XVn2d X%] TVZzTSwB]ab/W rhcfiTVZ*i/d ] TVZCqr6n ]ab/TVQ/\Vbd ]6bCX%U5fAW4W4ZX%^\VQ/Wi/sQSUd Z/\kTVU_] n eX%1d TSkffX%] d2c5cObCX%^XVcfi]aW4^d X%] TVZ/Us*]abCX%]Yl0QSRiSX%] Z/\>aX%ZCib/W4ZCcfiW~XVn U4W4A^_W4ecfiTVZCi/d ] TVZCd Z/\S8d Usub/W4ZX%RSRCn2d2c4X%fCn Ws1]abSW~/^_WX%U_TVZ*X%fCn W6uX_em]aTQ/RBi/X%]aWhR/^_TVfCX%fCd2n2d ]e[UW4Ws1Wq \Sq2s@u LUd U_2X% ^s"VVVVPb/TV^WTVb/Z/U_TVZ*sVVVVaas9d ]5d U{u@Wn2nAZ/T uZ]ab*X%]]ab/W4^_W~X%^_W6Ud ]aQ*X%] TVZ/U{ub/W4^W~X%R/RCn e1d Z/\>;n WXViUh]aTmRCX%^XViTgd2c4XVn2sBbCd \VbCn e|cfiTVQSZ`]aW4^d Z]aQCd ] tVW^_W4UQCn ]aUayBQ/Z]aW4^sVVVVPWd2iVW4ZCwxWn2iSs@VVVV9tX%Z^XVX%UU_W4ZCsDVVVVaqW ).- 8 |ph8R/^_TVfCn WkvtX%Z^XVX%U_U_W4Z*s/VVVVa4QCie5d Un TVU_]d ZXeS LTVZ/Ud2iW4^C]abSn/ p \[W^_W4\Sd TVZ8]abCX%]9d U@iSd tgd2iWid Z`]aT]u9T;bCXVn tVW4Us)-n Q/W;X%ZCiWi-]aW4^_^d ]aTV^_esWXVcb TSwub*d2cbjd U@wxQ/^]ab/W4^9i/d t1d2iWiZ]aToyBWXViSQCX%^]aW4^_qU LTSkRCX%Z`eX%^_WXX%ZCi>PWcfiTVZCLTSkRCX%Ze[X%^WXVq6rR/^d TV^d2sH4QCie[cfiTVZ/Ud2iW4^UYd ]W QCXVn2n en2d VWn eff]abCX%]9U_b/Whd Ud ZmX%Z`e6TSw]ab/W4UWhwTVQ/^' QCXVi^X%Z`]aUq-PbSW8cfiTVZ`] XVcfi]aU&bSW4^9TuZ5b/WXViSQCX%^]aW4^_Ufe8^XViSd TSsSX%ZCi6d UB]aTSn2im-c4X%Z* ]BfgW&U_Q/^_W-ub/W4^W&eVTVQX%^_Wq*4wVeVTVQlX%^Wd ZWi]aW4^^d ]aTV^_esV]abSWTgiSiU0X%^_WV2&]ab*X%]"eVTVQX%^_W Zm0LTSkRCX%ZeX%^_WXq2q2q2jr&]@]abCd U"RgTSd Z]B]ab/W;^XVi/d T{\Sd tVW4U-TVQ/] q&l0~Q/RBi/X%] Z/\`7a7fi " |qxq}t~"}s1|TVZ]abCd Ud Z*wxTV^k5X%] TVZon WXViU0]aT~X6iSd U_]a^d f/Q/] TVZu0b/W4^_W]ab/WRgTVU_]aW4^d TV^&RS^_TVfCX%fCd2n2d ]ejTSwSfgWd Z/\d Z}-n Q/W]aW4^_^d ]aTV^ed U0\V^_WX%]aW4^;]ab*X%ZoVVVqBfiZCiW4WiSsAd2wLQzbCXVihUXVd2iow7eVTVQ[X%^_W5d ZBWih]aW4^_^d ]aTV^_es/]ab/WT1i/iUX%^_W "6]ab*X%]8eVTVQX%^_Wld ZQcfiTSkR*X%Z`eX%^_WXlqq4q4Vs*]ab/W4ZzwxTV^XVn2;n Vs-XVc4cfiTV^i/d Z/\l]aT;Q/RBi/X%] Z/\Ss]abSWRATVU]aW4^d TV^0R/^TVfCX%fCd2n2d ]ejTSw7fgWd Z/\~d Zo-n Q/W{]aW4^_^d ]aTV^_e>d U8XVn uX_e`U'\V^_WX%]aW4^']abCX%ZoVVVq-^T tVWX%ZCizy'XVn RAW4^Z aVVVV RS^_T t1d2iWXzUTVR/bCd U_] d2c4X%]aWi[URCXVcfiWlub/W4^_W[cfiTVZCi/d ] TVZCd ZS\\Sd tVW4UubCX%]d UX%^_\VQCX%fCn e>]abSWmkTV^Wmd Z]aQCd ] tVW}X%Z/U_u@W4^d Z]abSWQCieCW4Z/XVk5d ZRS^_TVfCn WkffsCZCXVkWn e}]abCX%]d2wQU_W4ZCiUXkW4U_UX%\VWTSwL]ab/WwxTV^k %d2wBeVTVQX%^Wmd ZWi]aW4^_^d ]aTV^_esL]ab/W4Z]ab/WjTgi/iVUX%^_eW -]abCX%]@eVTVQmX%^Whd Zm0cfiTSkRCX%ZeX%^_WXV0]ab/W4eZ 4QCie1 U"RgTVU_]aW4^d TV^9R/^TVfCX%fCd2n2d ]e5TSwfAWd ZS\Yd ZYWXVcObTSw)]ab/W]u@TQCXViV^X%Z`]aUd Zl-n Q/W^WkffXVd Z/U'X%]V%Sq&PWd2iW4Z*wxWn2iYaVVVVasU_]a^_W4Z/\V]ab/W4Z*d Z/\{^_W4U_Q*n ]aU"TSw1^d WiSkffX%ZX%ZCiYPbCd2kTVZe~aVVVVasU_b/Tu9Wi]abCX%]@]ab/W4^_W U/{U_TVR/b*d U_] d2c4X%]aWi8U_RCXVcfiW6d Z5ubCd2cObmcfiTVZ*i/d ] TVZCd Z/\ u8d2n2n\Sd tVW~]abSWUXVkW}X%Z/U_u@W4^X%Ul0d Z]abCd Uc4X%UWqaPW4W}XVn U_T|u rX_u8d2i/sVVVVaswxTV^5Ud2kffd2n2X%^ff^_W4UQCn ]aUXVn TVZ/\l]ab/W4U_Wn2d ZSW4Uq2WYU]a^_W4Z/\V]ab/W4Zo]ab/W4U_W5^W4U_QCn ]aUfemU_b/Tu8d Z/\~]ab*X%] sEW4tVW4Zd ZXc4n2X%U_U TSw&kQCcObUd2kR*n W4^Ud ]aQCX%] TVZ/U6ub/W4^WW 4W4A^_W4e>cfiTVZCi/d ] TVZCd Z/\lc4X%Z/Z/TV]fgW5X%R/RCn2d WiSas)Q/Ud Z/\~l0d Z]ab/WZCXVd tVWU_RCXVcfiW cfiTV^_^_W4U_RgTVZCiU]aTjcfiTVZCi/d ] TVZCd Z/\jd Zff]ab/WU_TVRSbCd U_] d2c4X%]aWiURCXVcfiW8d ZffW4U_UW4Z`] d2XVn2n effTVZ*n e8]a^d t1d2XVnSc4X%UW4UqS7 /~}/ n `,7b/W4UW5^_W4U_Q*n ]aU ] X%VW4Z]aTV\VW4]abSW4^6U_b/T u]abCX%]~ / fimO`|j>o nY47fi0+o +j~ n 5ph+//, %aq bCX%]6d U]ab/WkffXVd ZkW4U_UX%\VW{TSw7]abCd URCX%RgW4^qWff^WkffX%^_l]abCX%] sBXVn ]ab/TVQ/\Vbo]ab/W4^_W~X%^_W~cfiW4^] XVd Z}Ud2kffd2n2X%^d ] W4Us1TVQ/^^W4U_QCn ]aUYX%^_W~ QCd ]aW~i/d AW4^_W4Z]ZU_R*d ^d ]{wx^_TSk ]ab/Wu9Wn2n pZ/TuZ~^_W4UQCn ]aU&TSwrd2XVcfiTVZCd U6X%ZCiDX%fgWn2n1aVVVVaq b/W4emcfiTVZ/Ud2iW4^_Wi6ub/W4ZXRgTVU_]aW4^d TV^YR/^_TVfCX%f*d2n2d ]ecfiTVQCn2ifgWt1d W4u9Wi|X%U5]ab/W~^_W4U_QCn ]ffTSwcfiTVZCiSd ] TVZCd Z/\|XRS^d TV^ffR/^_TVf*X%fCd2n2d ]eTVZU_TSkWn2X%^_\VW4^U_RCXVcfiWq CeuX_e|TSw cfiTVZ]a^X%U_] s'u@WbCXatVW|XlS`Win2X%^\VW4^U_RCXVcfiWd Zk5d ZCiz]ab/WU_TVR/b*d U_] d2c4X%]aWiffURCXVcfiWVasCX%Z*iX%^_WYd Z]aW4^_W4U_]aWid Z~ub/W4Z}cfiTVZ*i/d ] TVZCd Z/\md Z]ab/W{ZCXVd tVW8U_RCXVcfiWYX%Z*ih]ab/WU_TVR/b*d U_] d2c4X%]aWi6U_R*XVcfiWYX%\V^_W4Wqfi]5d UYXVn U_Tu@TV^_]ab>U]a^_W4U_Ud Z/\]abCX%]]ab/W~i/d U_] ZCcfi] TVZ}fAW4]u9W4W4Zo]ab/W6ZCXVd tVWX%Z*ij]ab/W6U_TVR/bCd U] d2c4X%]aWiU_RCXVcfiWd U"W4Z`] ^_Wn e Q/Z/^Wn2X%]aWi]aT]ab/W&R/bCd2n TVUTVR/bCd2c4XVnt1d W4u]abCX%]TVZ/W&bCX%UTSwR/^_TVf*X%fCd2n2d ]eX%ZCi'b/TuTVZ/WU_b/TVQ*n2iiTYR/^_TVfCX%f*d2n2d U_] d2chd ZCwxW4^W4ZCcfiWqTV^W41XVkRCn Ws7]ab/W R/^_TVf*X%fCd2n2d ] W4Uffd Zl]abSW5TVZ`]ey'XVn2n7R/Q/4n Wc4X%ZjfAW0tgd W4u@WiYX%U9]ab/W0R*X%^_] d2c4d RCX%Z] U@UQ/fDWcfi] tVW0R/^_TVf*X%fCd2n2d ] W4UX%fgTVQ/]9]ab/W8n T1c4X%] TVZTSw]ab/W c4X%^X%ZCiX%fgTVQ/]-ubCX%]{5TVZ`]eYu8d2n2nUXae5QSZCiW4^-ubCX%]{c4d ^cfiQCkU_] X%ZCcfiW4U1XVn ]aW4^_Z*X%] tVWn es/]ab/W4emc4X%ZfgW;t1d W4u@WiX%U%w^_WQSW4Z`] U_] 5R/^_TVf*X%fCd2n2d ] W4Us&d ZCwW4^_^_Wiowx^_TSkuX%] cObCd Z/\l]ab/WffTVZ]e|y'XVn2nEU_b/T uTVZ]aWn W4tgd Ud TVZwTV^kffX%ZeYu@W4W4U X%ZCih]abSW4ZU_W4]a] Z/\ff]abSWR/^_TVfCX%fCd2n2d ] W4U;WQ*XVn)]aTffTVf/U_W4^tVWiwx^WQ/W4Z*c4d W4Uq b/WRS^_TVfCn Wku@WYXVi/i^_W4UU&Tgc4cfiQ/^UfgTV]abowx^TSkXffw^_WQSW4Z`] U_]X%Z*iwx^TSkXU_Q/fEWcfi] tVW{U] X%ZCcfiWqbSW ^W4U_]TSw1]abCd URCX%RgW4^ffd UTV^_\SX%ZCd 4Wi}X%UYwxTSn2n TuUq{ZPWcfi] TVZu@W~wTV^kffXVn2d 4W6]ab/WhZ/TV] TVZ>TSwZCXVd tVWX%Z*iU_TVR/bCd U] d2c4X%]aWiU_RCXVcfiW4UqhZPVWcfi] TVZVs1u@WcfiTVZ/Ud2iW4^{]ab/W~c4X%U_WYub/W4^_Wh]ab/Wld ZCwxTV^kffX%] TVZcfiTSkW4Ud Zj]ab/W6wxTV^kTSwDX%ZW4tVW4Z] qLWffiW4Ucfi^d fgW0]abSmW Lr6cfiTVZCi/d ] TVZ>X%ZCiU_b/Tuz]abCX%]d ]{d U@tgd TSn2X%]aWiZX \VW4Z/W4^XVn1U_W4]a] Z/\TSwEubCd2cOb]ab/W~ffTVZ]ey'XVn2nBX%ZCij]ab/^W4W4pR/^d U_TVZ/W4^UR/Q/4n W~X%^_W6U_RgWc4d2XVnBc4X%UW4UqfiZPVWcfi] TVZju@W6\Sd tVW6U_W4tVW4^XVncObCX%^XVcfi]aW4^d X%] TVZSU8TS!w rh;qSW6UQ/R/RCn e}cfiTVZCiSd ] TVZ/UQ/ZCiW4^'ubCd2cOb]ffd U\VQCX%^X%Z]aW4Wi]aTb/TSn2imX%ZCiY\VQCX%^X%Z]aW4WijZ/TV]]aTjbSTSn2i/sLX%ZCiffu@Wh\Sd tVW~X8^X%ZCiTSkffd 4WimXVn \VTV^d ]ab*k]abCX%]8\VW4Z/W4^X%]aW4UXVn2n9X%Z*iTVZCn ei/d U]a^d f/Q/] TVZ/UjwxTV^ ubCd2cOb Lr6b/TSn2iUqYfiZ|PWcfi] TVZ6u@WlcfiTVZ/Ud2iW4^]ab/Wc4X%U_WubSW4^_W{]ab/Wd ZCwxTV^kffX%] TVZ[d U;Z/TV] Z]ab/WjwTV^k TSw*X%ZlW4tVW4Z] q@W8/^U_]8cfiTVZ/Ud2iVW4^0Ud ]aQCX%] TVZSUub/W4^W 4W4A^_W4e>cfiTVZCi/d ] TVZCd ZS\mc4X%ZfAWjX%R/RCn2d Wi/q9WUb/T u]abCX%] W47^W4ecfiTVZ*i/d ] TVZCd Z/\md Z~]ab/W8ZCXVd tVWU_RCXVcfiW5\Sd tVW4U]ab/W~X%R/RS^_TVR/^d2X%]aW~X%Z/Uu9W4^Yd Xh\VW4Z/W4^XVn2d 4WLr6$cfiTVZCiSd ] TVZ}b/TSn2iUqW5]ab/W4Z>U_b/Tu]abCX%] s`]eRCd2c4XVn2n esDX%RSRCn egd ZS\l0d Z]ab/W'ZCXVd tVWU_RCXVcfiW6iTW4U@ZSTV]9\Sd tVW]ab/W6X%R/R/^TVR/^d2X%]aW6X%Z/U_u@W4^q*WcfiTVZCc4n QCiVW{u8d ]abU_TSkWYi/d UcfiQ/UUd TVZTSw7]ab/W5d2kRCn2d2c4X%] TVZlTSw7]ab/W4U_W{^_W4UQCn ]aU8d ZoPWcfi] TVZ[Vq`7a7fib9Gc edbgfb]fbz}zhcyz1s1t yz|quC KG FJ K"NNM/K ?qgN-Q/^'wxTV^k5XVnSkT1iWn/d UX&U_RgWc4d2XVnSc4X%UW0TSw`]ab/W k0QCn ] pX%\VW4Z]-U_e`U]aWkU'w^XVkW4u@TV^_ay;XVn RgW4^_Zm/X%\Sd Z*sVVVVasubCd2cObmd U@W4U_U_W4Z] d2XVn2n eff]ab/W'UXVkWhX%U9]ab*X%]9Q/UWifeV^d Wi/kffX%Z>X%ZCiYy;XVn RgW4^_ZmaVVVVE]aTkT1iWnfgWn2d Ww^_W4t1d Ud TVZCqW5X%U_U_QCkW']abCX%]&]ab/W4^W6d U&UTSkWW4`]aW4^ZCXVn`u9TV^n2id Z}X;U_W4] vsgX%ZCiX%Z}X%\VW4Z]&ub/TkffX%VW4UhTVf/U_W4^t)X%] TVZ/U TV^h\VW4]aUd ZCwxTV^kffX%] TVZzX%fgTVQ/]8]abCX%]u@TV^n2i/qWmc4X%ZiW4Ucfi^d fgWY]ab/W5Ud ]aQ*X%] TVZfe[XhR*XVd ^j ^ asEub/W4^_WU8]ab/W~XVcfi]aQCXVnDu9TV^n2iSs"X%ZC&d U]ab/WX%\VW4Z] U+"sEubCd2cObW4U_U_W4Z] d2XVn2n ecb*X%^XVcfi]aW4^d 4W4Ujb/W4^~d ZCwTV^kffX%] TVZCqUffubCX%]6u@W}c4XVn2n Wi]abSWZCXVd tVWU_RCXVcfiW}d Z]ab/WZ]a^_TgiVQCcfi] TVZCqTV^{]abSWffR/Q/^_RgTVU_W4UTSwE]abCd U{RCX%RgW4^sAu@WX%U_UQCkW6]abCX%W] CbCX%U{]abSW~wxTV^k & ^ i^i^i^ jgasub/W4^#W %-d UL]ab/W9TVfSU_W4^_tX%] TVZ6]abCX%]L]ab/WX%\VW4Z]0kffX%VW4U0X%]*] d2kW-s V^ i^i^i 6 l"q b*d UC^_W4RS^_W4U_W4Z] X%] TVZd2kRCn2d2c4d ] n eX%U_U_Q*kW4U]abCX%]]ab/WX%\VW4Z]^_WkWk0fAW4^UW4tVW4^_e`]ab*d Z/\U_b/W bCX%UTVf/U_W4^tVWiUd ZCcfiW6b/W4^5n T1c4XVnU_] X%]aWlW4ZCcfiT1iW4UlXVn2n"]ab/WR/^W4tgd TVQ/UYTVf/U_W4^_tX%] TVZ/Uaq b`Q/UsBu@Wd \VZ/TV^WkWkTV^ed U_U_Q/W4UYb/W4^_Wq|WXVn U_T|d \VZSTV^_W}cfiTSkR/Q/] X%] TVZ*XVn0d U_U_QSW4UsQ/U_]6U_TX%U5]aT>fgWX%f*n W]aT|wxT1cfiQ/U6TVZu0b/W4ZcfiTVZCi/d ] TVZCd ZS\|d UX%R/R/^TVR/^d2X%]aWqrvRCXVd ^8 h& ^ i^i^i^ %jga0d U8c4XVn2n WiX^_Q/ZCqCr^_Q/Z}kffX_ejfgWt1d W4u9WiX%U8XffcfiTSkR*n W4]aWjiW4Ucfi^d R/] TVZTSwgub*X%];bCX%RSRAW4ZSU0T tVW4^] d2kWd ZmTVZ/W RgTVU_Ud fCn W8W4`WcfiQS] TVZTSw1]ab/W U_eU_]aWkffq TV^Ud2kRCn2d2c4d ]esd Zm]abCd URCX%RgW4^su@WX%U_UQCkW@]abCX%]C]abSW9U_] X%]aW-TSw]abSW9u@TV^n2ihiTW4U*ZSTV]-cObCX%Z/\VW-T tVW4^] d2kWq b/WffU_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiW5d U0]ab/WU_W4]0TSw*XVn2nRATVUUd fCn W^_Q/ZSUqfiZ]ab/W55TVZ`]e}y;XVn2n)RSQ/4n Ws]ab/WZCXVd tVW{URCXVcfiW{bCX%U&]ab/^W4Wu9TV^n2iVUs^_W4R/^_W4UW4Z`] Z/\6]abSW]ab/^_W4WRgTVU_pUd fCn Wn T1c4X%] TVZ/U{TSwD]ab/W~c4X%^q bSW8U_TVRSbCd U_] d2c4X%]aWiU_R*XVcfiW~iW4Ucfi^d fgW4UubCX%]ff5TVZ`]eu@TVQCn2ijbCX_tVW6UXVd2iZ>XVn2ngc4d ^cfiQCkU_] X%Z*cfiW4Uad2q Wq2s15TVZ]e1 U;7fi0X%U&u@Wn2ngX%U-ub/W4^_W;]ab/Whc4X%^d Uq b/W0]abS^_W4W4pR/^d U_TVZSW4^_UR/Q/4n W>d U6]a^_WX%]aWid ZiW4] XVd2n&d ZDgXVkR*n WVq2jfgWn Tu8ql|b*d2n Wd Z]abSW4U_W>c4X%U_W4U6]ab/WU_TVR/bCd U] d2c4X%]aWiU_RCXVcfiW[d U5U_] d2n2n^_Wn2X%] tVWn eUd2kRCn WsB]ab*d Uld U5ZST|n TVZ/\VW4^j]ab/Woc4X%U_W[wxTV^Y]ab/W 4Q*ieCW4Z/aXVkffd ZRSQ/4n Wqr6n ]ab/TVQ/\Vb]ab/WhZCXVd tVWhU_RCXVcfiW6b*X%U'TVZ*n ewTVQ/^Wn WkW4Z`]aUsBcfiTVZ/U_]a^_Q*cfi] Z/\]ab/WhU_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiWd Z/ptVTSn tVW4UffcfiTVZ/Ud2iW4^d Z/\lXVn2n/]ab/W]abCd ZS\VU0]abCX%]h0cfiTVQCn2i6b*XatVW UXVd2i/su0bCd2cb[d UhwX%^ w^_TSk c4n WX%^sCX%ZCiff]ab/WcfiTVZCi/d ] TVZSUQ/ZCiVW4^ubCd2cb0UXaeUjX%ZemRCX%^_] d2cfiQCn2X%^{]ab*d Z/\Sq~-^T tVWlX%Z*i>y;XVn RgW4^_ZaVVVV8i/d UcfiQ/U_U]ab/WYiSd vcfiQCn ] W4U8d ZocfiTVZSU_]a^_QCcfi] Z/\5U_QCcbXU_TVR/bCd U] d2c4X%]aWiffU_R*XVcfiWqfiZ\VW4Z/W4^XVn2s"Z/TV]6TVZCn ezd Ud ]6Z/TV]c4n WX%^5ub*X%] ]abSWU_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiW}d Us"f/Q/] ]ab/WZ/W4WiwTV^X~U_TVR/bCd U_] d2c4X%]aWiURCXVcfiWX%ZCi>]abSW[wTV^k ]mk0Q/U_]j] X%VWkffXae|fgWcfiTSkW[c4n WX%^TVZ*n eXVwx]aW4^]abSW[wXVcfi] qTV^W41XVkRCn Wsd Zm]ab/W 4QCiVe}EW4ZSXVkffd ZmR/^TVfCn WkffsSfAWwTV^_WcfiTVZ`] XVcfi] ZS\b/WXVi/ QCX%^_]aW4^_UQCieu@TVQCn2iXVn2kTVU_]~cfiW4^] XVd ZCn e}Z/TV]hbCX_tVWbCXViX5UTVR/bCd U_] d2c4X%]aWimURCXVcfiW>d Zk5d ZCioW4tVW4ZX%U_U_QCkffd ZS\U_bSWYuX%U~X%ZW4RAW4^]d ZjR/^_TVfCX%f*d2n2d ]e1as1X%ZCiYcfiTVQ*n2iZSTV]@b*XatVW`ZST uZj]ab/WffwxTV^kd ]-u@TVQCn2i8bCXatVW]aTh] X%VWQ/Z`] d2nAXVw]aW4^b/WX%^d ZS\6b/WXVi/ QCX%^_]aW4^ U;^_W4U_RgTVZ/U_WqfiZzX%Ze|c4X%U_Ws0d2w*]ab/WmX%\VW4Z]6bCX%UXffRS^d TV^8RS^_TVfCX%fCd2n2d ]e>TVZ[]ab/WYUW4] TSwLRgTVU_Ud f*n W5^_Q/ZSUjd Z[]ab/WU_TVR/b*d U_] d2c4X%]aWi[U_RCXVcfiWs XVw]aW4^b/WX%^d Z/\[TV^TVf/UW4^_tgd ZS\ & ^ i^i^i^ %_asUb/Wc4X%Z$cfiTVZCi/d ] TVZCs;]aT[\VW4]oXRgTVU_]aW4^d TV^'TVZ mqTV^kffXVn2n esA]ab/WjX%\VW4Z]ffd UhcfiTVZ*i/d ] TVZCd Z/\b/W4^;R/^d TV^'TVZ]ab/W U_W4]'TSwA^_Q/Z/U;ub/W4^_Wb/W4^n T1c4XVnU_] X%]aWYX%]0] d2kqWU & ^ i^i^i^ _aqn WX%^n e8]ab/W{X%\VW4Z] UBR/^_TVf*X%fCd2n2d ]e 1^LTVZ ZCiVQCcfiW4UX@R/^_TVfCX%fCd2n2d ]qe D,^ TVZ fejkffX%^_\Sd Z*XVn2d X%p] TVZCqEWX%^W'd Z]aW4^_W4U_]aWihd ZubSW4]ab/W4^E]ab/WX%\VW4Z`]&c4X%ZcfiTSkR/Q/]aW9b/W4^ERgTVU_]aW4^d TV^CTVZ XVw]aW4^*TVf/U_W4^_t1d Z/\& ^ i^i^i^ %4&d ZoX^_Wn2X%] tVWn eUd2kRCn WuXaesu ]ab/TVQ/]bCX_tgd ZS\6]aT5u@TV^_md Z~]ab/W{U_TVR/b*d U_] d2c4X%]aWi6U_R*XVcfiWqTVZ/Ud2iW4^&]ab/W{]ab/^W4W4pR/^d U_TVZ/W4^UR/Q/4n W5d ZokTV^_WYiW4] XVd2n2q;yBW4^_W]abSW{ZCXVd tVW{U_R*XVcfiWYd Up \[We/ L`6464Vs7ub/W4^_Wd U]ab/Wu@TV^n2iYub/W4^_Wd U'ZSTV];W4WcfiQ/]aWi/q0WX%^W8TVZCn ed Z`]aW4^_W4U]aWiZ^_Q/Z/UYTSw{n W4Z/\V]ab Vs-U_Tl Vq b/W~UW4]TSw-TVfSU_W4^_tX%] TVZ/U>ubCX%]X%\VW4Z`]>c4X%ZzfgW]aTSn2i/d UV 6% V4 6 VVq&y"W4^_W% 64 V{cfiTV^_^_W4URATVZ*iUB]aT{]ab/W;TVf/U_W4^_tX%] TVZ5]ab*X%]"Wd ]ab/W4^0TV^*u8d2n2n/EfgW&W4WcfiQ/]aWijad2q Wq2s]ab/W0XVd2n W4^9UXae1d Z/\%*u d2n2nfgW&W4`WcfiQ/]aWiSVaUd2kffd2n2X%^n esA 6 {cfiTV^_^W4U_RgTVZCiUn/`7a_fi " |qxq}t~"}s1|]aTff]ab/WXVd2n W4^;UXae1d Z/\[%@u8d2n2nfAWW4WcfiQ/]aWi/Vq b/WU_TVR/b*d U_] d2c4X%]aWiffURCXVcfiWYcfiTVZ/Ud U]aUTSw7]ab/WYwTVQ/^&^_Q/Z/U``6aa4)64aa4`64aa4`64auiw"TV]aW']ab*X%]9]ab/W4^Whd U-Z/T8^_Q/Z5u ]abjTVfSU_W4^_tX%] TVZ464asUd ZCcfiW;]abSW0aXVd2n W4^&u8d2n2nZSTV]"]aWn2ng]abCX%]@b/Wu8d2n2n)fgWW4WcfiQ/]aWi/qr6c4cfiTV^i/d Z/\h]aTh]ab/WU_]aTV^_es]abSW;R/^d TV^.D^, Zj]abSW'Z*XVd tVW'URCXVcfiWbCX%UZ1^8h VVhwTV^[vqZ D^TVZ]ab/W{^Q/Z/U{d U0Z/TV]cfiTSkRCn W4]aWn eU_RgWc4d /WihfeY]abSW{U_]aTV^_eqLfiZRCX%^_] d2cfiQ*n2X%^sb/W5wQCn2n1i/d U]a^d f/Q/] TVu@WX%^W9Z/TV]L]aTSn2i;]ab/W-R/^_TVf*X%fCd2n2d ]e{u ]ab u0bCd2cbh]ab/W-aXVd2n W4^*u d2n2n UXaej9X%ZCih"d2w/&u8d2n2nZ/TV]CfgW@W4`WcfiQS]aWi/qW^W4]aQ/^_Z]aTff]abCd U0RATSd Z]8d ZoDgXVkRCn W5Vq2VqJ9G K eK"NNJrvRCX%^_] d2cfiQCn2X%^n eUd2kRCn W{UW4]a] Z/\md U;ub/W4^_W]ab/WYX%\VW4Z];TVf/UW4^_tVW4U0TV^6n WX%^_Z/U;]abCX%]0]ab/WW4`]aW4^_Z*XVnu@TV^n2iUd Z[U_TSkWU_W4]vqVTV^6Ud2kRCn2d2c4d ]esu@WX%UU_QCkWj]ab/^_TVQ/\VbSTVQ/]8]ab*d U6RCX%RgW4^8]abCX%]h]ab/WX%\VW4Z]kffX%VW4U{TVZ*n e~TVZ/WhTVf/U_W4^t)X%] TVZCs"X%ZCimkffX%VW4UYd ]ffX%]]ab/Wh/^_U_]'U_]aW4RTSwD]ab/Wh^_Q/ZCq bQ/Us7]ab/WhU_W4=] vTSwRgTVU_Ud fCn W TVf/U_W4^t)X%] TVZ/U5cfiTVZSUd U_]aUTSw1Z/TVZSWkR/]e~U_Q/f/U_W4]aUTS!w vq bQ/Us*X%Z`e^_Q/Z lc4X%ZmfgW8u0^d ]a]aW4ZX%Ua*ub/W4^_0W U-]ab/WhXVcfi]aQ*XVnu@TV^n2iX%ZCij U{XZ/TVZSWkR/]e5U_Q/f/U_W4]9TSCw vq-yBTu9W4tVW4^FiTW4U&Z/TV]&Z/WcfiW4U_UX%^d2n emcfiTVZSUd U_]&TSwg]ab/WZ/TVZ/WkR/]eYU_Q/f/U_W4]aU&TSw vq-PTSkWU_Q/fSU_W4]aUkffXaejZ/W4tVW4^fgWTVf/U_W4^tVWi/q-TV^9W41XVkRCn WsAd ZDgXVkR*n W8Vq2Vs7hd U9Z/W4tVW4^"]aTSn2i]abCX%]Bb/W0u8d2n2nfgW&W4`WcfiQS]aWi/sUTY 46UZ/TV]&TVfSU_W4^_tVWi/q*WYX%UU_QCkW]abCX%]]abSW6X%\VW4Z] U'TVf/U_W4^_tX%] TVZ/U8X%^_W5XVc4cfiQS^X%]aWsDd Z]abCX%]d2wS]ab/WffX%\VW4Z]TVf/U_W4^tVW4U $d ZX^_Q/Z /s]ab/W4Z]abSWYXVcfi]aQCXVnu@TV^n2idZ U Zoq bCX%]d Us)u@WYX%U_U_Q*kW]abCX%]8XVn2n/^_Q/Z/UX%^_WYTSwE]abSWwxTV^ka0u0b/W4^_W q5Zz11XVkRCn WVq2Vs-XVc4cfiQ/^XVcfied U W4ZCwTV^cfiWifem]ab/W^_W QCd ^_WkW4Z]]abCX%]0^_Q/Z/U-bCX_tVW]ab/W5wTV^k6 `aaqbSW8TVf/UW4^_t)X%] TVZTV^ffd ZCwTV^kffX%] TVZTVfS] XVd Z/WimiT`W4UZ/TV]bCX_tVWh]aTjfgW W4gXVcfi] n elTSwg]ab/WwTV^k ]ab/WXVcfi]aQCXVn/u9TV^n2id U Zo8VqBfi]U_Q vcfiW4U]abCX%]hd ]8d U0W QCd tXVn W4Z`];]aT5U_QCcObXU] X%]aWkW4Z`] q b*d U8d U0]ab/Wjc4X%U_WZfgTV]abm]ab/WffTVZ]ey'XVn2nAR/Q/4n WX%ZCiY]ab/W6]ab/^_W4W4pRS^d U_TVZ/W4^_UR/QS4n Wq5TV^{W41XVkRCn Ws"d Zl]abSW ]abS^_W4W4pR/^d UTVZ/W4^_U R/Q/4n Ws*fgWd Z/\~]aTSn2i]abCX%]u8d2n2nEfgW5W4WcfiQ/]aWid U W4U_U_W4Z] d2XVn2n eoW QCd tXVn W4Z`]h]aTlTVf/U_W4^_t1d Z/\6 5Wd ]ab/W4^ 8TV^8@u8d2n2nZSTV]fgWW4`WcfiQ/]aWiSaqfiZz]abCd UU_W4]a] Z/\Ss0u9Wc4X%Z$X%U_|ub/W4]ab/W4^sXVw]aW4^TVf/U_W4^_t1d Z/\s]ab/WX%\VW4Z`]}c4X%Z$cfiTSkRSQ/]aWmb/W4^RgTVU_]aW4^d TV^TVZ f`ecfiTVZCi/d ] TVZCd Z/\TVZq TVQ/\Vb*n eU_RgWX%gd ZS\Ss/]abCd UffXVkTVQSZ`]aU]aTX%U_gd ZS\ju0b/W4]ab/W4^TVf/U_W4^tgd Z/\ffd U]ab/W&UXVkWX%U;iSd UcfiT tVW4^d ZS\{]abCX%]0d UB]a^Q/Wq bCd U0kffXae Z/TV]fgW-]ab/W{c4X%U_W8d Z6\VW4Z/W4^XVnTVf/U_W4^tgd Z/\~TV^fgWd Z/\]aTSn2i kffX_ec4X%^_^_ekTV^_Wd Z*wxTV^k5X%] TVZ}]abCX%ZoQ/U]{]ab/WwXVcfi]]abCX%]j U8]a^_Q/WqTV^W41XVkRCn WsLd2wBwTV^;U_TSkW8^_WX%UTVZ[hZ/TuU;]ab*X%];]ab/W aXVd2n W4^'u@TVQCn2i5Z/W4tVW4^UXaed2wAb/WcfiTVQCn2i5b/Wn R]U_T]ab*X%] s&d ZoRCX%^] d2cfiQCn2X%^s&d2w&ffX%ZCiu8d2n2nEfgW5W4WcfiQ/]aWi/s*]ab/W4Zb/WYu8d2n2n9iVW4/ZCd ]aWn e>UX_e|asC]ab/W4Zb/WX%^d ZS\m8ad2q Wq2sgTVf/U_W4^_t1d Z/\ 6 4V9]aWn2n U5k0QCcbkTV^W8]abCX%Zm]ab/WwXVcfi]']ab*X%]']abSW8]a^_QSW8u@TV^n2imd UTVZ/WTSgw TV0^ %q]&UX_e`U;]abCX%]&]ab/W{]a^Q/Wu9TV^n2i~kQ/U]fgQW awTV^ d2wS]ab/W]a^_Q/Wu@TV^n2ihu@W4^_ZW s)]ab/WaXVd2n W4^0u@TVQCn2ihbCX_tVWUXVd2iaqfiZ8]abSW9^_Wk5XVd ZCiW4^*TSw]abCd ULRCX%RgW4^Eu@WX%U_UQCkW"]ab*X%[] UL/Z*d ]aWq9TV^LW4tVW4^_e8UcfiW4ZCX%^d T'u@WcfiTVZ/Ud2iW4^u@W5iW4/Z/W5X'U_W4]&TSwSRgTVU_Ud fCn WTVfSU_W4^_tX%] TVZ/U s1cfiTVZSUd U_] Z/\ffTSwSZ/TVZ/WkR/]ejU_Q/f/UW4]aU&TSw vq&TV^;\Sd tVW4ZX%ZC8s]ab/W{U_W4]&TSw7^_Q/ZS[U U0]abSW4ZoiW4/Z/Wi ]aT5fAW]abSWU_W4]V aF06>8i{d tVW4ZTVQ/^X%U_U_QCkR/] TVZ/U]abCX%]&]abSW{U_] X%]aWYiTW4UZ/TV]cObCX%Z/\VW{TtVW4^0] d2kWjX%ZCih]abCX%]&]ab/W5X%\VW4Z] 5k X%VW4UTVZCn e|TVZ/WTVf/U_W4^_tX%] TVZCs;]ab/WmU_W4] TSw&^_Q/ZSUc4X%ZfgWmt1d W4u9WiX%UX~UQ/f/U_W4]TSw8qbCd2n W`7a7fibc edbgfb]fbz}zhcyz1s1t yz|quQ/U_]] X%1d Z/\ ]aTmfgWmX5U_Q/f/UW4]TSwu@TVQCn2iUn2d \Vb] n e}Ud2kR*n2d2wxe>]abSWYR/^W4U_W4Z`] X%] TVZbSW4^_Ws&d Z\VW4Z/W4^XVn2s1u9W~cfiW4^_] XVd Z*n euX%Z`]]aT}XVn2n Tu$U_W Q/W4ZCcfiW4UTSwETVf/U_W4^t)X%] TVZ/Uq5uLTVZ/Ud2iW4^swTV^W4gXVkR*n WsBX%Zl1piTTV^0tVW4^_Ud TVZTSwA]ab/WjffTVZ]eoy'XVn2nSR/^_TVfCn Wkffsub/W4^_WffTVZ]eTVRgW4Z/U XU_W Q/W4ZCcfiW8TSwiT`TV^Uq2 bCd Uw^XVkW4u@TV^_YW4]aW4ZCiU;ZCX%]aQ/^XVn2n ej]aT5]ab*X%]U_W4]a] Z/\Sqb/W4Z/W4tVW4^@u9W0U_RgWX% TSwgX8i/d U_]a^d f/QS] TVZ 1^9TVWZ lsu9W d2kRCn2d2c4d ] n eX%U_UQCkW&]abCX%]9]ab/WR/^_TVf*X%fCd2n2d ]eTSw1X%Ze6U_W4]9TVZYubCd2cObYu@W cfiTVZCiSd ] TVZd U@U_]a^d2cfi] n e5\V^_WX%]aW4^-]abCX%ZlV[q ~/W4H] X%ZCZWfAW;]u@T8^X%ZCiTSktX%^d2X%fCn W4U6TVZ ls*ub/W4^W W Uh]ab/WXVcfi]aQCXVnCu9TV^n2i[X%ZCU ]abSWYTVf/UW4^_tVWiW4tVW4Z`] q bQ/Us-wTV^aaCW / X%ZCW8qh{d tVW4ZXYiSd U_]a^d f/Q/] TVZ 1^TVZm^_Q/ZSU ls7u@WiVW4Z/TV]aWfe D,^ ]ab/W}kffX%^_\Sd ZCXVn0i/d U_]a^d f/QS] TVZTS!w W>s;X%ZCilfe D^ ]abSW}kffX%^_\Sd ZCXVniSd U_]a^d f/Q/] TVZTSKw qTV^'W4gXVkRCn WsCwTV^ C4{vC18^ }u 8;d U;U_bSTV^_]8wTVm^ D^ W0X%ZC1,^ >u fi;d U;Ub/TV^_]8wTV^1^>8aq~/W4.] D^-fAW6X;R/^d TV^-TVZ X%ZCin W4Z] 1,^ D^u 8EfgW']ab/WRgTVU_]aW4^d TV^{XVwx]aW4^&TVfSU_W4^_t1d Z/\qb/W5k5XVd ZoQSW4U_] TVZ~u@WYX%U_d Z]abCd U;RCX%RgW4^d UQ/ZCiW4^&u0bCX%]cfiTVZCi/d ] TVZ/U;u9WbCXatVW1^ uD^,>uaVwTV^0XVn2n?vq bCX%]d Usu@W&uX%Z`]]aTZ/T uQSZCiW4^*ubCX%]cfiTVZCi/d ] TVZ/U"]ab/W-RATVU]aW4^d TV^ Z*iQCcfiWi;f`e1^ gc4X%ZfffgWcfiTSkR/Q/]aWi6w^_TSk]ab/W&R/^d TV^TVZ fecfiTVZCiSd ] TVZCd Z/\TVZ6]ab/W&TVf/UW4^_t)X%] TVZ*q@aDgXVkRCn WVq2fgWn u\Sd tVW4UhXffcfiTVZ*cfi^_W4]aWjc4X%U_Wq2WU]a^_W4U_U0]ab*X%]1^8X%Z*i1^,X%^_Wji/d U_]a^d f/QS] TVZ/UTVZms)ubCd2n WD^,X%ZCZD^ X%^_W;iSd U_]a^d f/Q/] TVZSUDTVZ TVf/] XVd Z/Wif`e6kffX%^_\Sd ZCXVn2d X%] TVZw^_TSk D^9X%ZCZD^ ^_W4U_RgWcfi] tVWn e1aqw"TV]aW]abCX%]8aV0d U0W QCd t)XVn W4Z] n eU_] X%]aWiX%UD^ W81^W W>wTV^8XVn2n]>iaVaVW QCd tXVn W4Z`] n esLaVad Uhc4XVn2n Wi6]ab/WmrhcfiTVZCi/d ] TVZCVqB]8c4X%Z~fAW5cObCX%^XVcfi]aW4^d 4WimX%U8wTSn2n u0U .e& z8@D @oK||gG}Lo>7|+1^[:+/fffi6,)on/\b"a\/h*zO%}9q1^ # ; 1^W D^ W W 8! ff>mzu}* 'ff n /\W 9/_` /*G'ffn S\ n #W>mff&X >Oz u}D^ W 1^ W 8R 6?|)~V1^W!hzO%}D ^ W h1 ^ W # m!h6 ,)^W h!/qD ^ W 2TV^'cfiTSkRCn W4]aW4Z/W4U_UaX%ZCi'fgWc4X%Q/U_Wd ]0d UQ/UWwxQCn)wTV^TVQ/^0n2X%]aW4^ b/W4TV^_Wk Vq2VasVu@W&R/^_Ttgd2iVWX@R/^_TTSwTSw b/W4TV^_Wk Vq2ffd Z]ab/WYX%RSRAW4Z*i/d gqbSW8/^U_] cfiTVZCiSd ] TVZd Z b/W4TV^_WkVq2d U'Q/U_]haVaq b/W ]abCd ^ilX%ZCimwxTVQS^_]ab[cfiTVZ*i/d ] TVZ/UQSU_] d2wxe]ab/WZ*XVkW%cfiTSX%^_U_W4Z*d Z/\X%]"^X%ZCiTSkffVqCfiZ`]aQ*d ] tVWn es/^_U_]U_TSkWu9TV^n2.U"^_WXVn2d 4Wi/sgX%ZCi]ab/W4Z%CGff!Dfi1 Y4VfiYR!fififi ?-%7RxXfi ;4gfiFXxfiX9AAK Ix[ VfiE/%fix% 18D`_[ , ff x ,?<h7 X5XA _ <( %ff@A _[?O 2K^x fi _[ x ff9!4V fi2! Xx e<hA ^5XA_[ E9o-7/ Q fi U8 4-_ ff x ff J 8NN P?9:; fix 0 <h %ff@AV x x %'D _[ fi 44-, 6: F ff Hff 4D 8DM1T 8 fi:% x J VI ?A X5 _ P? Y/ fi 2%@ x K V fi ?D 8m: E fiV , ff ff@, 6: UI E fiK J 8NN 8P J VI F , @ _P R V fi ff 2D fiX ox 6 #x 1 6 x %fiff,`7afi " |qxq}t~"}s1|U_TSkW%cfiTSX%^_U_W4Z*d Z/\kWcb*X%ZCd UkffliWc4d2iW4U6ub*d2cb[W4tVW4Z`] U_Q*cb[]abCX%] vd U6^W4tVWXVn Wi]aT]ab/WX%\VW4Z`] q b/WW4tVW4Z]5 Uffc4XVn2n WiX%cfiTSX%^U_W4ZCd Z/\S8TSwUhq b/W ]abCd ^ilX%ZCiwTVQ/^_]abcfiTVZCi/d ] TVZSUW4AWcfi] tVWn eYUX_eff]abCX%]@]ab/W'R/^_TVfCX%fCd2n2d ]eff]abCX%;] $d UcfiTSX%^U_W4Z/Wi8]aTvd U-]ab/W;UXVkW6wxTV^XVn2n| >8q bCd UkWX%Z/U;]abCX%]0]abSW%cfiTSX%^_U_W4Z*d Z/\lkWcObCX%ZCd Uk55d U'U_QCcb]abCX%]0]abSW{R/^_TVfCX%f*d2n2d ]eTSwATVf/U_W4^tgd Z/\ld U'Z/TV]X%AWcfi]aWifffeY]abSW{U_RgWc4d Cc@tXVn Q/W{TSw v>]abCX%]uX%U0^_WXVn2d 4WiSqfiZ|]abSW^_Wk5XVd ZCiW4^jTSw-]abCd URCX%RgW4^s@ub/W4Zu@WUX_e 1^UX%] U/W4U Lrh;VsBu@WkWX%Zz]abCX%] 1^UX%] U_SW4U5cfiTVZCi/d ] TVZ|aXV@TSw b/W4TV^_Wk Vq2TV^s1WQCd tXVn W4Z] n es"X%Ze~TSw1]abSW6TV]ab/W4^]ab/^_W4WcfiTVZCi/d ] TVZSU58q bQ/Us 1^ UX%] U_/W4U Lr60~kWX%Z/U8]ab*X%]cfiTVZCiSd ] TVZCd Z/\d Z]ab/WYZCXVd tVWYURCXVcfiWcfiTSd ZCc4d2iW4Uu8d ]abvcfiTVZCi/d ] TVZCd ZS\zd Z|]ab/W~U_TVRSbCd U_] d2c4X%]aWioU_RCXVcfiW u8d ]abR/^_TVf*X%fCd2n2d ]eVq bSW Lr6cfiTVZCi/d ] TVZYW4`R*n2XVd Z/U"ubecfiTVZCi/d ] TVZCd ZS\Yd Z5]ab/W0ZCXVd tVW0U_R*XVcfiW8d U"Z/TV]'X%R/R/^_TVR/^d2X%]aW8d Z5]ab/W ffTVZ]e~y'XVn2nR/Q/4n WTV^6]abSW]ab/^_W4W4pR/^d UTVZ/W4^_U6R/Q/4n Wq}WocfiTVZ/Ud2iW4^6]ab/W]ab/^_W4W4pR/^d U_TVZ/W4^_U6R/QS4n W}d ZiW4] XVd2n2{XUd2kffd2n2X%^hX%ZCXVn eUd U X%R/R*n2d W4U]aT5TVZ`]e}y;XVn2n2qeE fiZ}]ab/W5]ab/^_W4W4pR/^d UTVZ/W4^_URSQ/4n WsDu0bCX%]Yd U` U8R/^d TV^Yi/d U]a^d f/Q/] TVZ1^TVZlzfiZn/ p \11XVkRCn W6Vq2;u9WffX%U_UQCkWi]abCX%]9]abSW6k5X%^_\Sd ZCXVnAi/d U_]a^d f/QS] TVZeD^ TVZ U@QSZCd2wxTV^kffqDr'RCX%^_]w^_TSk]abCd URD^6d UQ/ZSU_RgWc4d /Wi/q wBTu U_Q/R/RgTVU_W{]ab*X%]66TVf/U_W4^_tVW4U5 6 ]abSW aXVd2n W4^UX_e`U5Vaqw'XVd tVWcfiTVZCi/d ] TVZ*d Z/\>u@TVQCn2in WXVi~]aT|XViTVRS]6]ab/W}i/d U_]a^d fSQ/] TVZ 1,^ }u fi 6 Vaq bCd Ui/d U_]a^d f/Q/] TVZUX%] U_SW4U 1,^ } fi 6 V VVVqPTVR/bCd U_] d2c4X%]aWi|cfiTVZCi/d ] TVZCd ZS\n WXViUl~]aT|XViTVR/]6]abSW}i/d U_p]a^d f/QS] TVZ 1,^ 1^ u 6 VaqCelR*X%^_]5ai/@TSw b/W4TV^_Wk Vq2VsDZCXVd tVWmcfiTVZCi/d ] TVZCd ZS\od UX%R/R/^TVR/^d2X%]aW>ad2q Wq20D^ 1,^ }u fi 6 VaTVZ*n ed2w]abSWjaXVd2n W4^d U6W QCXVn2n en2d VWn e[]aTUX_ejd ZfgTV]ab>u@TV^n2iZU X%ZCqP/d Z*cfiW6]ab/WffaXVd2n W4^jk0Q/U_]UX_e]abCX%]Y;u8d2n2ngfgW6W4WcfiQ/]aWi}d Z>u@TV^n2s9d ]wTSn2n uU{]ab*X%] D^ W `64 Vq b`Q/Us*cfiTVZ*i/d ] TVZCd Z/\}d UffX%R/R/^TVR/^d2X%]aWhTVZCn ed2w]ab/WjXVd2n W4^ U RS^_TV]aTgcfiTSn-d U U_QCcObo]abCX%]hb/WmiW4/ZCd ]aWn e}UXaeUffdZ s&d2q Wq2sLW4tVW4Zd2w*fgTV]abzYX%ZCioYX%^_WW4WcfiQ/]aWi/q&CQ/];d2w`]abCd Ud U"]ab/W8c4X%UWsubSW4Zff]ab/WaXVd2n W4^"UX_e`UsScfiTVZCiSd ] TVZCd Z/q\ D,^ TVZl 6 4{d USX%R/R/^TVR/^d2X%]aWsVUd ZCcfiW0]ab/W4Zm'`ZST uU9]abCX%]"b/W0u8d2n2nVfgWW4WcfiQ/]aWi/q bSWu9TV^n2i5c4X%Z/Z/TV]9fgVW s7wxTV^9]ab/W4Z]ab/W'XVd2n W4^-u@TVQCn2i8bCXatVWUXVd2ijq b/W4^_WwTV^_Ws1/Wph"+B7fi,EasAcfiTVZCi/d ] TVZ*d Z/\Z]ab/WZCXVd tVWU_R*XVcfiWYc4X%Z/Z/TV]8cfiTSd Z*c4d2iWu8d ]ab[cfiTVZCi/d ] TVZCd ZS\d Z]ab/W{UTVR/bCd U_] d2c4X%]aWi5U_RCXVcfiWYwTV^0fgTV]abTSwbCd U^W4U_RgTVZ/U_W4Uqb WwTSn2n u8d Z/\W41XVkRCn W6Ub/T uU]abCX%]5d Zm\VW4Z/W4^XVn2sBd ZmU_W4]a] Z/\VU{TSwD]ab/Wh]eRAWX%^d Ud Z/\>d Zl]abSW5TVZ]e/y'XVn2n7X%ZCi]ab/W;]ab/^_W4W4pR/^d UTVZ/W4^_U"RSQ/4n Ws`]ab/WLr6zcfiTVZCi/d ] TVZc4X%ZTVZCn e6fgWUX%] U_/Wijd ZjtVW4^_e6U_RgWc4d2XVnc4X%U_W4Ueg PQ/RSRATVUW&]abCX%]# V & 4 + Vs7X%Z*ifgTV]abl & X%ZCiY + X%^_W;TVf/U_W4^_tVWiu8d ]abjRgTVUd ] tVWn/ p \R/^_TVf*X%fCd2n2d ]eq bCd UYd U{]ab/W~c4X%UWwxTV^fATV]abffTVZ]ey'XVn2n"X%ZCij]ab/W6]abS^_W4W4pR/^d U_TVZSW4^_U{R/Q/4n Wq2 b/W4Z]ab/W LrhcfiTVZCi/d ] TVZ b/W4TV^Wk Vq2Vac4ac4X%Z/Z/TV]Yb/TSn2iwTV^5fgTV]abv & X%ZCi| + QSZCn W4U_UD^ W&k + hd U6Wd ]ab/W4^YTV^~VqTV^5U_Q/R/RgTVU_WY]abCX%]1^ & Vs1^ + Vs;X%ZCiD^ W &Kk +Vq8vd ]ab/TVQS]Yn TVU_U{TSwE\VW4Z/W4^XVn2d ]esE]abSW4^_W~d U{U_TSkWW & & + X%ZCi+ [ &!k + U_QCcbl]abCX%=] 1^ W &jX%ZC1^ W + [Vq P/d ZCcfiWTVfSU_W4^_tX%] TVZ/U5X%^_WXVc4cfiQ/^X%]aWsAu@WkQ/U];bCX_tVW D^ W & & Vq42w Lr6|b/TSn2iU6wxTV^ff & s7]ab/W4Zlu9Wk0Q/U_]bCX_tVW 1^ & W + VqCQ/] ]ab/W4Z 1^ + W + VqCQ/]hUd ZCcfiW1^ + [VsLd ]6wTSn2n u0U]abCX%];]abSW4^_Wd UU_TSkW . + U_QCcObm]abCX%m] D^ W . [X%ZCi1^ + W . 2 Vq bCd U cfiTVZ`]a^XViSd2cfi]aU;]ab/WW rhcfiTVZCi/d ] TVZCqPTYub/W4ZiVT`W4U Lr6bSTSn2i/ b/WR/^W4tgd TVQ/U'W41XVkRCn WW4bCd fCd ]aWimXYcfiTSkfCd Z*X%] TVZTS2w X%Z*wTV^ubCd2cObLr6c4X%ZTVZCn ejfAW'UX%] U_SWid Z%iW4\VW4Z/W4^X%]aW5c4X%U_W4UqLfiZ]ab/WZ/W4]&U_Wcfi] TVZCs)u@WU_bCXVn2nU_]aQCiVe]abCd U8 Q/W4U_] TVZ[wxTV^ X%^_fCd ]a^X%^_ecfiTSk0fCd ZCX%] TVZ/U'TSw X%ZCvq`7fibc edbgfb]fbz}zhcyz1s1t yz|qu9G 1 "NN efiZo]abCd U U_Wcfi] TVZCsu9WjR/^_Ttgd2iWYU_TSkWcbCX%^XVcfi]aW4^d X%] TVZ/UffTSwLu0b/W4Z}]ab/WeLr6cfiTVZCi/d ] TVZ[b/TSn2iUs-wTV^/ZCd ]aW X%ZCivq@&Q/^B^_W4U_Q*n ]aU"W4]aW4ZCi{WX%^n2d W4^9^_W4UQCn ]aU"TSw1{d2n2n2stX%ZliW4^#~EXVX%ZCsAX%ZCiffBTVfCd Z/U'aVVVVaqWS^_U_]W4bCd fCd ]8X{Ud2kRCn WUd ]aQCX%] TVZd Z~ub*d2cb rhd U0\VQCX%^X%Z`]aW4Wi5]aT5b/TSn2iSsEX%ZCihu@WUb/T u]abCX%]]abCd Ud UB]ab/WTVZCn ehUd ]aQ*X%] TVZmd Zffu0bCd2cbd ]d UB\VQCX%^X%Z]aW4Wi]aTb/TSn2i/qEW0]abSW4Z5U_b/Tu]abCX%] sAwxTV^'X%^_fCd ]a^X%^_eX%ZCvs7u9Wc4X%ZcfiTVZ/U_]a^_Q*cfi]6X%p VtXVn Q/Wi>kffX%]a^d owx^TSk$u0bCd2cbX8U_]a^_TVZ/\Z/WcfiW4UUX%^_eocfiTVZCiSd ] TVZwTV^ Lr6v]aT>b/TSn2ic4X%ZfAWTVf/] XVd ZSWi/q]6]aQS^_Z/U6TVQ/]6]abCX%] s'd ZU_TSkW}c4X%U_W4U5TSw'd Z`]aW4^W4U_] #Lrh U^_TVQ/\Vb*n eU_RgWX%1d Z/\S@\VQCX%^X%Z]aW4WiS]aTb/TSn2iW41cfiW4R/]Yd Z%iW4\VW4Z/W4^X%]aW6Ud ]aQCX%] TVZSUqLd ZCXVn2n es1u@WZ]a^_TgiVQCcfiW5X'Z/W4uR/^_T1cfiWiQ/^XVn26cbCX%^XVcfi]aW4^d X%] TVZlTSw Lr60u@WR/^_Ttgd2iW5X6kWcbCX%ZCd UkvU_QCcOb]abCX%]X i/d U_]a^d f/QS] TVZ 1^c4X%ZjfAW;]ab/TVQ/\Vb]9TSw1X%UX%^d Ud ZS\w^_TSk]ab/W kWcObCX%ZCd Ukd2w1X%ZCi{TVZCn ed2Uw D^"UX%] U_SW4ULr60q:e\ l!#" gkn nql ^`\b\ r^`p%rW /^_U_]8cfiTVZSUd2iW4^']ab/W8TVZ*n eUd ]aQCX%] TVZmub/W4^_WqLrhvd U'\VQCX%^X%Z]aW4WiY]aTYb/TSn2iSEd2w7]ab/W U_W4]aUhd ZX%^_WRCXVd ^_u U_W5i/d U_TSd Z`] q:e/ * x0ySE,, )/QD^ff/{6;`,5(/-,_'6[<a"ZW " ^ " l//+5bCX%]bCX%R/RgW4Z/U6d2w1]abSW6U_W4]aU5dZ X%^_WhZ/TV]RCXVd ^u8d U_Wi/d U_TSd Z`] r'^_Wh]ab/W4^_WhU_] d2n2nc4X%UW4UYacfiTSkf*d pZCX%] TVZ/U;TSw8sUvs1X%ZCii/d U_]a^d f/Q/] TVZ/U&TVZmLub/W4Zrhb/TSn2iU b/W4^_WffX%^_Wsf/Q/]&]ab/W4eX%^_WY QCd ]aWU_RgWc4d2XVn2q:e\ l!#$n&%p%rWZ/T uzR/^W4U_W4Z`]X n WkffkffX0]abCX%]-R/^_T t1d2iW4UX;Z/W4uvcObCX%^XVcfi]aW4^d X%] TVZTSwCLr6d ZY]aW4^kU-TSwDXUd2kRCn WVV%pkffX%]a^d 1q b/Wn WkffkffXXVn2n TuUQ/U*]aT6iVW4]aW4^kffd Z/WwxTV^kffX%ZeYcfiTSk0fCd ZCX%] TVZ/U"TSw]X%ZCisu0b/W4]ab/W4^Xffi/d U]a^d f/Q/] TVZTVZ W41d U_]aU0]abCX%]0UX%] U_/W4mU LrhX%ZCih\Sd tVW4UhcfiW4^_] XVd Zu@TV^n2iU0RATVUd ] tVW{R/^_TVfCX%f*d2n2d ]eqLd ~X&UW4!] TSw^Q/Z/Usu0b/TVU_Wu@TV^n2iUX%^_W8d Z5UTSkW0/Z*d ]aWU_W4Q] X%ZCi{ub/TVU_W0TVf/U_W4^_tX%] TVZ/UcfiTSkWw^_TSkU_TSkWSZCd ]aW-U_W40] V & ^ i^i^i4 j VqCWUX_eh]abCX%(] ' U0X%WZ fi|p^Wn2X%] tVW0]aWX%ZCi*fisEX%ZCi 5|W 9d2)w 'b*X%U0]ab/WYwTV^k &Rk i^i^i k _jSs/ub/W4^_WWXVcOb &*-d U;Wd ]abSW4^ +**TV^ (''q,#q ~/W4(] . /' & ^ i^i^i '10fgW{]abSW{U_W4]0TSgw YpX%]aTSkU;^Wn2X%] tVW ]aX%ZC8qBWc4X%Z~]abCd Z/TS2w . X%U5XR*X%^_] ] TVZTSw1]ab/W u@TV^n2iU5XVc4cfiTV^i/d ZS\j]aTu0bCX%]hc4X%ZfgW TVf/U_W4^_tVWiSq u9Tu@TV^n2i.U & X%ZCi+ X%^_Wd Zl]abSW8UXVkW UW43] '1*@4. d2w1]abSW4^_WX%^W ZSTYTVf/UW4^_t)X%] TVZSU]abCX%]ffi/d U_] Z/\VQCd Ubl]abSWkffA]abCX%]ffd Us]ab/W4^_Wd U'Z/TYTVfSU_W4^_tX%] TVZ$UQCcbl]abCX%#] & X%ZCW+ [q r"W4SZ/W]ab/6W 5lkffX%]a^d 87u8d ]abW4Z]a^d W4:U 9*(X%U wxTSn2n TuU9*(2d w)'*RTV]ab/W4^_u U_WqaVW6c4XVn2n;7~]ab/WxQyB %,/ph,awTV^#vX%ZCivaq;w"TV]aW']abCX%]9WXVcObY^T u-d Z<7}cfiTV^_^W4U_RgTVZCiU]aTXQ/Z*d2Q/W X%]aTSkdZ .u@Whc4XVn2n`]abCd U@]ab/WhX%]aTSk ,%`SS8]aT8^T u,q bCd UkffX%]a^d ~aXVcfi]aQCXVn2n es]aU0]a^X%Z/U_RgTVU_W*uX%U0/^_U]d Z]a^_T1iQCcfiWifSQ/]wxTV^ Xffi/d AW4^_W4Z`]0R/Q/^_RgTVU_WCfem{d2n2n/W4]8XVn2qDaVVVVaq`7&=fi " |qxq}t~"}s1|n/W4]aQS^_ZCd Z/\6]aTDgXVkRCn WYVq2Vs)]ab/WqLrh;XVcfi]aW4^d Z/\mkffX%]a^d >d U\Sd tVW4Z~fee/p \ub/W4^W ]abSW~cfiTSn QCkZ/UYcfiTV^^_W4U_RgTVZCiY]aTo & X%ZCi} + X%ZCij]ab/W6^_TuUYcfiTV^_^W4U_RgTVZCiY]aT]ab/W6]ab/^_W4W~X%]aTSkU& + 4 & k + X%ZCi + & qTV^ffW41XVkRCn WsB]ab/W>wXVcfi]5]abCX%]hW4Z`]a^_e>9 .0& TSw"]abCd U~kffX%]a^d zd UZCi/d2c4X%]aW4U;]abCX%]8 & c4X%Z/Z/TV]fgWTVf/U_W4^tVWid2w7]ab/W5XVcfi]aQCXVnu@TV^n2im U8d Zo +? & qfiZ]ab/WYwTSn2n u8d ZS\ln WkffkffXVs?AC@ B iW4ZSTV]aW4U0]ab/W{]a^X%Z/U_RgTVU_WTSwS]ab/WY^_Tu8tVWcfi]aTV^4A @ sDX%Z*i ff@ iW4Z/TV]aW4U;]ab/W^_TutVWcfi]aTV^hcfiTVZSUd U_] Z/\YTSwCXVn2nD%Uq"G ,)/'|n | , n /0E gn eExQyB ,/ph%o;./Dh\/j%Q*/jF7,8zO%} E ;D^qffSj,, )}|n [/~17 ,ffp ,Y / j + /fi|p7&,%`SS}}|pG'1^W H'' JILK-/ln ,@ & ^i^i^i jS , / 1^ FW h WD^W zF#0SK 5x0yV N7 2A @ B @ B,ff_ 9 2i^i^iul;#9D^ff,M-zu} E 7 ,6p ,ff//ffGff4 , @Ghfffi{667HS2O QP RTS 6ff9G,, )/{|n #, `%@,%`SSYU7 WV ffoQP RTS / XgXg>Y''K Z'$,%`Shjfiz[7@ ) 17 A_@ B @ B 9 ]? {XVn2n`XE aO QP R&S "9ff fij7 n , \A^@ ] ~ff Y>%o1^l|n Z e1^, EX zffophS-GD^mXE})z^}1^,MK xQy/zu}D^ FW F 6Sq1^>!w"TV]aW]abCX%]0fCd UBW4UU_W4Z`] d2XVn2n eXcfiTVZtVW4^_U_WTSwAaXVaq*rZCX%]aQ/^XVn Q/W4U_] TVZ6]aTYX%Ud UubSW4]ab/W4^0fCAu@TVQCn2iU_] d2n2nb/TSn2i d2wu@W9^_W4R*n2XVcfiWi~%wxTV^XVn2nbXg !O QP R&S ]abSW4^_W9W41d U_]aUD ^CUX%] Uwxe1d Z/\L rhlu ]abD ^8 XE>fe%wTV^5XVn2ni/d U_]a^d f/QS] TVZ/UfiXgtVW4^]ab/W4^_WhW4gd U]aU1 ^UX%] Uwe1d Z/\L r6u8d ]ab1 ^, Xg&q2 b/WX%Z/U_u@W4^8d U;Z/TSU_W4WYDgXVkR*n W{Sq2VfCaad2d2aq~/WkffkffXYSq2jUXaeU6]abCX%]Xmi/d U_]a^d fSQ/] TVZ1^h]abCX%]6UX%] U_SW4ULrh X%Z*iX%]6]abSWjUXVkW] d2kWbCX%U1^ c''YwxTV^ 5 iSd 7W4^_W4Z]6X%]aTSkfiU ' c4X%ZlW41d U_]6d2wX%ZCi6TVZ*n eod2wX5cfiW4^_] XVd ZmU_W4]'TSw_5 n2d Z/WX%^W QCX%] TVZ/U'd WZ lQ/Z/Z/T uZSU*b*X%U'X@U_TSn Q/] TVZ*q*Z~kffX%ZehUd ]aQCX%] TVZ/U9TSw7d Z]aW4^_W4U_]5 ] l}Z/TV]aW0]abCX%]e5jkffX_ejfgW5X%Uhn2X%^_\VWYX%U Vaq w"TV]U_QS^_R/^d Ud ZS\Sn e5]ab/W4ZCsEd ZU_QCcObUd ]aQCX%] TVZ/U;]ab/W4^_W{TSw]aW4Zc4X%ZfgW5Si/d U_]a^d f/Q/] TVZ D^]abCX%]0UX%] U_/W4U rh;sDX%U0u@W{U_b/Tud Z]abSW{Z/W4]U_Q/f/UWcfi] TVZCq0-Z~]ab/W{TV]ab/W4^;bCX%ZCi/s@ iTW4U9bCX_tVW X0U_TSn Q/] TVZd gd2w]ab/W;U_W4]"TSwW QCX%] TVZ/fU 7 AC@ B 8Z @ s]ab/W4Zj]ab/W;U_W4]"TSw1XVn2nU_TSn Q/] TVZ/U{wTV^kUj X%ZCi~]ab/W5RgTVUd ] tVWjTV^_]abCX%Z]]ab/Wmd Z`]aW4^_UWcfi] TVZTSwX%ZzX v5Z/W5U_QSf/U_RCXVcfiWad2q WqX6beRAW4^RCn2X%Z/W&TSwjh V q b/W4U_W"UTSn Q/] TVZ/U&X%^_W"Q/U_]D]abSW'cfiTVZCi/d ] TVZ*XVnR/^TVfCX%fCd2n2d ] W4[U 1^;W}FCwxTV^-XVn2ni/d U_]a^d f/Q/] TVZ/U&wxTV^*ubCd2cOb rhb/TSn2iU*]abCX%]*bCXatVW@U_Q/RSRATV^]9cfiTV^_^_W4U_RgTVZCiSd Z/\]a37 q b/W4UWcfiTVZCiSd ] TVZCXVnR/^_TVf*X%fCd2n2d ] W4U5kffXae]ab/W4ZmfgW8W4]aW4ZCiWiY]aT>Xi/d U_]a^d f/QS] TVZlTtVW4.^ feU_W4]a] Z/\ 1^ XE wTV^ffX%ZX%^_fCd ]a^X%^_eli/d U]a^d f/Q/] TV^Z XET tVW4^;]ab/Wu9TV^n2iVUd Z}X%]aTSkU8cfiTV^_^_W4URATVZ*i/d Z/\h]a[7 1XVn2Un 1^cfiTVZ/U_]a^QCcfi]aWiZ]abCd U0uX_eUX%] Uwee rh;q`77`fibc edbgfb]fbz}zhcyz1s1t yz|quPQ*kffkffX%^d Z/\Ss/u9W8bCXatVW ]ab/W8^_WkffX%^_X%fCn WjwXVcfi]0]ab*X%]hwTV^ X%Ze\Sd tVW4ZmU_W4]0TSwX%]aTSkU. ]ab/W4^_WX%^_WTVZCn e8]u@TRATVUUd fCd2n2d ] W4UEWd ]ab/W4^/6i/d U_]a^d f/QS] TVZ W41d U_]aUBu0bCd2cb6bCX%UD^ W <''!wTV^0XVn2nj'.X%ZCiUX%] U_/W4U Lr60sDTV0^ j'iSd U_]a^d f/Q/] TVZSU XE TtVW4^ u@TV^n2iUcfiTV^_^_W4URATVZ*i/d Z/\]aTX%]aTSkUd Zg''s]ab/W4^_W W41d U_]aUffXYi/d U]a^d f/Q/] TVZlUX%] Uwegd ZS\ Lr6|u8d ]abkffX%^_\Sd ZCXVn*i/d U_]a^d fSQ/] TVZTtVW4^u9TV^n2iU;W QCXVnS]aTXE>q:ek\ l!#" gkn nql ^`\b\ rml^`f p%rWjZ/T uR/^_W4U_W4Z]jXff]ab/W4TV^_Wk ]abCX%]\Sd tVW4Uh]u9TmW4RCn2d2c4d ]X%Z*iWX%U_ep]aTVpcb/WcOoUQ]vc4d W4Z]jcfiTVZCi/d ] TVZSUQ/ZCiVW4^ubCd2cOb Lr6c4X%ZSZ/TV]0b/TSn2i5Q/ZCn W4UU]ab/WRS^_TVfCX%fCd2n2d ] W4U'TSwAU_TSkWX%]aTSkUhX%Z*i/%TV^0TVf/UW4^_t)X%] TVZSUX%^_WVq b/W]abSW4TV^_Wk U'R/^_TtVWifffeU_b/Tu8d Z/\5]ab*X%];]ab/WcfiTVZCi/d ] TVZlTS2w ~Wkffk5XSq2VaXVc4X%Z/Z/TV];b/TSn2iQ/ZCiVW4^&]ab/WU_] X%]aWilcfiTVZCi/d ] TVZ/UqW9f/^d W n/e;^_Wc4XVn2n4U_TSkWU_] X%ZCi/X%^iiW4/ZCd ] TVZ/U@wx^_TSkn2d ZSWX%^@XVn \VW4f/^XVqEr[U_W4]DTSw tVWcfi]aTV^_:U @ & ^ i^i^ijo@ 0Ulc4XVn2n Wi+/,%//d2w9]ab/W4^_W~W4gd U_]mcfiT`^W vlc4d W4Z`]aqU p & ^ i^i^i pT0 Z/TV]mXVn2nL4W4^_TSU_QCcb]abCX%]0 p&*t@ * @ 7]ab/WtVWcfi]aTV^_U5X%^Wv uS +a`//{d2wA]abSW4^_W8W41d U_]6cfiT`^W vlc4d W4Z`]aUU p & ^ i^i^i p&0Z/TV]*sr &0 p&*w@ * '0XVn2n4W4^_TS7U_QCcOb8]abCX%]^ @ U&c4XVn2n Wi6X%Zv u/'|pm/*sr & @ X%ZCi *r & p&* VqCrtVWcfi]aTVx0 p&*t@ * @ X%ZCi 0 p&* VqTS(w @ & ^ i^i^iz@ 0 d2w/]ab/W4^_WW41d U_]{cfiT`^W vc4d W4Z]a6U p & ^ i^i^i p&0zU_QCcOb]abCX%]*r &*sr &696 h,)/n |, n /Z\b"a\:e*{E U/+ e7h~xQyB ,/=ph%H#/qV & ^i^i^i 4Fj7/%ZzO%}c|S `~' fi7, l4 ?}Gj6^7vSln , ^y @ & ^i^i^i j&vu/6p S>G6ff{G6} )j ] ff}VV^i^i^i6l"/ ~ j|hp 8jffVV^i^i^i 6"l * fiff/%o)1^l,MK-Yx0v,H1 ^ ~K/q1 ^W ^''KV , >|p ',%`SSYa}zu}9Yj7 ,84 , 1} GYjfihGfi7;/,,+_` /0 )&/0vu/ +a`/ S1ff fi0B/%o )1 ^' 1MK& 'xQ)E1 ^W''!{fi|p ',fi O`//jjfiz[}Oz u}@n } } S/G#l/,+~/_` / /;6637v/l~% )qXE,1XE>Y''HQ '%`//a}9 fi5{)/ ),, )mXg ) j eD^%o ) /xQy^W '' XE>Y'' 8|p ',,fi O`//fia}B 1 ^8 X a,fi]8d U'u9Wn2n/`Z/TuZ~]ab*X%] ZoX%ZN5 ql kffX%]a^d gsCX%] kTVU_]0>l ^_TuU c4X%ZfAWjn2d Z/WX%^n e}d ZCiW4RgW4ZCiVW4Z`] qfiZkffX%Zec4X%UW4U"TSwgd Z`]aW4^W4U_]'ac4wqSDgXVkR*n WSq2-fgWn u8as]ab/WZQCk0fgW4^TSwAX%]aTSkU5 Un2X%^_\VW4^"]abCX%ZY]ab/WZQCkfgW4^-TSwSTVf/U_W4^_tX%] TVZ/VU l"sU_T6]ab*X%]&]ab/W4^_W5k0Q/U_]-W4gd U]&U_Q/f/U_W4]aQU }TSw/^_T u0UTSw 7l]ab*X%]X%^_W5n2d Z/WX%^n eiW4RgW4ZCiW4Z] q bQ/UsR*X%^_] fCBTSw b/W4TV^_Wk$Sq jR/Q/]aU0ZSTVZ`]a^d t1d2XVn*cfiTVZ/U_]a^XVd Z]aUTVZ]ab/Wi/d U_]a^d fSQ/] TVZ/U]abCX%]UX%] Uwxe Lr60qbSW8^_W QCd ^_WkW4Z`]ffd ZmRCX%^_]6aXVkffXaeU_W4WkU_TSkW4ub*X%]'TVfSUcfiQ/^_W f/Q/]hd ]6c4X%ZfgW8WX%Ud2n e[cOb/WcVWiX%ZCiX%R/R*n2d Wi>d Z|X ZQCkfgW4^TSwEUd ]aQCX%] TVZ/Us"X%UYd2n2n Q/U]a^X%]aWi}d Z|DgXVkRCn W6Sq2X%ZCijSq2hfgWn Tu8q LX%^_]ac4UXaeU0]abCX%]8d ZkffX%Z`ejTV]ab/W4^hc4X%U_W4U;TSw*d Z]aW4^_W4U_]0u0b/W4^_WZ/Wd ]ab/W4^0RCX%^_]8aXVLZSTV^8fCX%R/R*n2d W4Us)W4tVW4Zd2wX6i/d U_]a^d f/Q/] TVZTVZ W41d U_]aUUX%] Uwe1d Z/\ Lr60s`]ab/WR/^_TVfCX%f*d2n2d ] W4UTSwEkffX%1d Z/\6]ab/WTVf/UW4^_t)X%] TVZSU8X%^_WcfiTSkRCn W4]aWn eiVW4]aW4^kffd Z/Wifel]ab/W6R/^_TVfCX%f*d2n2d ]emTSwCt)X%^d TVQSU{W4tVW4Z`]aUd Z>]ab/Wffu@TV^n2iT1c4cfiQ/^_^d Z/\SsgubCd2cObU_W4WkU;^X%]ab/W4^QSZ/^_WX%U_TVZCX%f*n Wq`7fi " |qxq}t~"}s1|W Lrh;XVcfi]aW4^d Z/\k5X%]a^d YTSw*11XVkRCn WSq2Vq0wBTV] d2cfiW{]abSW4^_WW4gd U]aU8X%Zn/ p \eE LTVZ/Ud2iVW4^@]abSW@ X%ZCi b*X%U0Z/T6Z/W4\SX%] tVWcfiTSkRgTVZ/W4Z]aUX vffZ/W5cfiTSk0fCd ZCX%] TVZlTSw7]ab/W/^_U]&]u@T5^_TuU]abCX%]8d U0Z/TV] ffVV@ X%ZCiYbCX%UZ/TP d2kffd2n2X%^n esA]ab/W4^_W W4gd U_]aU5X%ZXv5Z/WcfiTSk0fCd ZCX%] TVZ}TSwg]ab/Wn2X%U_]]u@T^_T uU]ab*X%]6d UZ/TV] j/Z/W4\SX%] tVW8cfiTSkRATVZSW4Z`]aUqCfi]wxTSn2n TuU'w^_TSk b/W4TV^_WkSq SaXVD]abCX%]L]ab/W4^_Wd UZ/Tffi/d U]a^d f/Q/] TVZhUX%] Uwe1d Z/\Lr6]abCX%]\Sd tVW4U{fgTV]abTSwD]ab/W TVf/U_W4^_tX%] TVZ/U. & X%ZCi + RgTVUd ] tVW6R/^TVfCX%fCd2n2d ]e[X%ZCiWd ]ab/W4^aXV&\Sd tVW4UhfgTV]ab W &1 + X%ZCW &;k + RgTVUd ] tVWYR/^_TVfCX%fCd2n2d ]eTV^jfC&\Sd tVW4UfgTV]ab v + & X%ZCv & k + RgTVUd ] tVWR/^_TVf*X%fCd2n2d ]eqm4wLfgTV]ab[TVf/U_W4^_tX%] TVZ/U6bCX_tVWRgTVUd ] tVW R/^_TVfCX%fCd2n2d ]es/]abSW4Z Lr6c4X%Zb/TSn2i5TVZCn e}d2w7]ab/W8R/^_TVfCX%fCd2n2d ]eTSw &Rk + U'Wd ]ab/W4^6{TV^hVqa11XVkRCn WjVq2ffXVn ^_WXVieU_b/TuU]abCd U;Q/Ud Z/\~X6kTV^_WYi/d ^_Wcfi] X%^_\VQCkW4Z] q2bSW6Z/W4`]W41XVkRCn W~wQ/^_]abSW4^5d2n2n Q/U_]a^X%]aW4U8]abCX%]5d Z>\VW4Z/W4^XVn2s9d ]5c4X%Z}fgWhtVW4^_ei/d vcfiQCn ]]aTUX%] UwxeLr60q] V & 4 + 4 . VsX%Z*iXVn2nE]abS^_W4WTVf/U_W4^_tX%] TVZ/Uc4X%ZfgWkffXViWn/ p \e& PQ/R/RgTVU_WY]abCX%u8d ]abRgTVUd ] tVWhR/^_TVf*X%fCd2n2d ]eq]]aQ/^Z/U;TVQS]']ab*X%]5d Zl]ab*d UUd ]aQCX%] TVZ>]ab/WLr6 cfiTVZCi/d ] TVZc4X%Zb/TSn2i/sf/Q/] TVZCn e|d2w;aXVm1^ W v &k +Zk . lad2q Wq2s'XVn2nETSw; & + s0X%ZCi[ . kQ/U]8b/TSn2iSasfC1^ W >aa &gk + . haa +Rk . & & haa &gk . + ad2q Wq2s`W41XVcfi] n e5]u@T TSw1 &+ s)X%ZCih . k0Q/U_]*b/TSn2i/asac4R1^ W >a &/ + ; . 0a + & 0 . ;a . + 0 & aaad2q Wq2s1W41XVcfi] n elTVZ/WhTSw9 & + sATV^ff . kQSU_]b/TSn2i/asATV^5ai/9TVZ/WhTSw"a &e + .la +#k .+ & 6 . a&6a & k . ETV^a . & 6 + ab6a & k + DbCX%U9RS^_TVfCX%fCd2n2d ]e8Wd ]abSW4^@W41XVcfi] n eTVZ/W{TSw* & s1 + sTV^ . bSTSn2iUs)TV^]abSW{^_WkffXVd ZCd ZS\6]u@T5fgTV]abb/TSn2i/aqWm/^_U]lcbSWc]ab*X%] Lr6 c4X%Zzb/TSn2i|d Z XVn2n9]ab/W4U_Wc4X%U_W4Uq]YU_b/TVQCn2i}fgWc4n WX%^]abCX%e] Lr6c4X%Zb/TSn2i[d Zc4X%U_W}aXVaq5TV^_W4TtVW4^sB]abSW4^_WmX%^WjZSTcfiTVZSU_]a^XVd Z]aUffTVZ 1^ +*CW hwTV^> &Rk +Vk . W4gcfiW4RS] s/feY]abSW Lr6cfiTVZCiSd ] TVZCsEwTV^0WXVcOb/Wih,s]ab/W{R/^_TVf*X%fCd2n2d ]e}k0Q/U_]fgW]ab/W{UXVkW5wxTV^ XVn2?n &Rk +Vk . s1X%ZCih]abSW{]ab/^_W4WR/^TVfCX%fCd2n2d ] W4U k0Q/U_]0U_QCk]aTVaqTV^ c4X%U_WYfCas1n W4Q] '1*DfgW]ab/WffX%]aTSk u0b/W4^_WW4gXVcfi] n e]u9T6TSw* & sg + s1X%Z*ij . b/TSn2i/s1X%ZCi*"iTW4UZ/TV]b/TSn2iSs"wTV^{ V4V4VqPQ/R/RgTVU_Wh]abCX%W] D^ g' & 4' + 4' . Vq wBTV]aW5]abCX%] sDUd ZCcfiW~XVn2n]ab/^_W4W5TVf/U_W4^_tX%] TVZ/Ujc4X%Z}fAW~k5XViWffu8d ]ab>RgTVUd ] tVW5R/^_TVfCX%fCd2n2d ]es9X%]jn WX%U_]]u9TTS1w ' &' + s"X%ZC<' .k0Q/U_]{bCX_tVW5RgTVUd ] tVWYR/^_TVfCX%fCd2n2d ]eqy"W4ZCcfiWYu9Wmc4X%Zi/d U_] ZS\VQCd U_b>fgW4]u@W4W4Z]u@T~U_Q/fBc4X%U_W4Uffad2&TVZCn e]u@T5TSwS]ab/Wk bCXatVW{RgTVUd ] tVW8R/^_TVfCX%f*d2n2d ]esDX%ZCi~ad2d2XVn2n]ab/^W4WbCXatVW8RATVUd ] tVW{R/^_TVfCX%f*d2n2d ]eqTV^8U_Q/fBc4X%U_W~ad2asEU_Q/R/RgTVU_Whu8d ]abSTVQ/]5n TVU_UTSwC\VW4Z/W4^XVn2d ]e]abCX%]{TVZ*ne ' & X%ZC<' + bCXatVWYRgTVUd ] tVWR/^_TVf*X%fCd2n2d ]eq b/W4Z>d ]d2kffkWi/d2X%]aWn e>wTSn2n uU8w^_TSk]ab/=W Lr6cfiTVZCiSd ] TVZ]abCX%]&]ab/W4^_Wffk0Q/U_]-fgWU_TSkWu8d ]ab~U_QCcOb]abCX%e] 1^ . W' & c' + U_QCcObCswTV^mXVn2Kn k]abCX%e] 1^ WhVqb/QU#1^WwV^VXn2Kn k' + U_QCcOb&]abCX%] 1^ WVs0X%ZCD^ + W wTV^XVn2n' & U_QCcOb[]abCX%]1^ W K VqPQSfc4X%UWad2d2Bd U'kTV^W8d Z`]aW4^W4U_] Z/\Sq b/W&^T uU9TSw]abSW Lrh;XVcfi]aW4^d Z/\kffX%]a^d x7}cfiTV^_^W4U_RgTVZCi/d Z/\]aq' & F' + sLX%ZC4' . X%^_WaYVasajVasX%ZCimajVasA^W4U_RgWcfi] tVWn eWq w"T u ~Wk5kffX{Sq2VaXV&]aWn2n UQ/U]abCX%]6d2Kw D^;UX%] U/W4WU Lr60sS]ab/W4Zmu9Wk0Q/U_]'bCXatV[W 7A_@ B @ B wxTV^U_TSkW @ & + . "u8d ]ab*CW}+*fiaq b/W4U_W]ab/^_W4W5n2d Z/WX%^;WQCX%] TVZSU0bCX_tVWU_TSn Q/] TVZ* D^ +&A+ A.`77afibc edbgfb]fbz}zhcyz1s1t yz|quP/d ZCcfiW]abCd UU_TSn Q/] TVZd UQ/Z*d2Q/WsEd ]8wxTSn2n TuU'f`ee~Wkffk5XSq2VfC]ab*X%]6BiSd U_]a^d f/Q/] TVZSU]abCX%]UX%] UwxeLr6 kQSU_]8b*XatVW>cfiTVZCi/d ] TVZCXVn*RS^_TVfCX%fCd2n2d ] W4U 1^ +*CW v+*fi VVVs;X%ZCi~]abCX%]h]ab/Wd ^kffX%^_\Sd Z*XVnCi/d U]a^d f/Q/] TVZ/U'TVZc4X%ZmfgWjX%^_fCd ]a^X%^_eq bCd U wQCn2n eocObCX%^XVcfi]aW4^d 4W4U{]abSW8U_W4]'TSwBi/d U]a^d f/Q/p] TVZ/U 1^wTV^8u0bCd2cb Lr6b/TSn2iUd Z[]abCd Uc4X%U_Wq wBTV]aW]abCX%]wTV^ V4V4VsUd ZCcfiWYu@Wc4X%Zu^d ]aW*fia21^ W o+*fiu@W{bCXatVW 1^ +* 1^ }* *`VV{U_Tff]abCX%] sCd Z* 1^ +cfiTVZ]a^X%U_]]aT]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/] TVZlT tVW4^ vsS]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/] TVZlT tVW4^ c4X%Z/ZSTV]'fgWcbSTVU_W4ZX%^_f*d ]a^X%^d2n eqfiZ~c4X%U_W8ac4as7d ]U_b/TVQCn2i6XVn U_TfgW{c4n WX%^"]ab*X%]Lrh|c4X%Zffb/TSn2i/q@5TV^_W4T tVW4^sE1^ - +*CW hU-Wd ]ab/W4^{;TV^VsgiW4RgW4ZCiSd Z/\{TVZub/W4]ab/W4^ +*q&Ld ZCXVn2n es1wTV^c4X%U_Wffai/as`U_QSR/RgTVU_W0]abCX%].1^W& +Qk . VWq rhzbSTSn2iU5d }]ab/W4^W W41d U_]aU>U_QCcOb]abCX%]=1^ + WX%ZC1^ W . h wxTV^ffXVn2n [ + k . U_QCcOb]abCX%] 1^ h[vVq8a{wcfiTVQ/^_UWC1^ & W h ffwTV^8XVn24n > & U_QCcb~]abCX%] 1^ W KVq2w"T uu@W&U_b/Tu]abCX%] Lr6c4X%Z/Z/TV]bSTSn2i6d Z~X%Z`e TV]ab/W4^0c4X%U_Wq&Ld ^_U_]UQ/R/RgTVU_W@]abCX%]0{1^Wff& k +_k . eVq bQ/Us ]ab/W4^_WkQ/U]EfgW;X%]&n WX%U_]LTVZ/W"TV]ab/W4^&X%]aTSkZ'lU_QCcOb8]abCX%][1^ W q''2Vqb/W"^_T uzcfiTV^_^_W4U_RgTVZCi/d ZS\&]aT0]ab/W;X%]aTSkv &^k +k . U@a;0Vaq@PQ/R/RgTVU_WRd UC]ab/W"^_TucfiTV^_^W4U_RgTVZCi/d Z/\]aT>]ab/W~TV]ab/W4^X%]aTSk ';qP/d Z*cfi8W 7 UX>%pokffX%]a^d 1sB]abSWtVWcfi]aTV^ma}V m\Sd tVW4Uld UlX%ZXvffZ/WcfiTSk0fCd ZCX%] TVZTSwa>V5X%ZCm]abCX%]d UffZ/TVZS4W4^_TX%ZCimbCX%U5Z/TVZ/Z/W4\SX%] tVW[cfiTSkRgTVZ/W4Z`]aUq]6Z/TuwTSn2n uU;f`e b/W4TV^WkvSq ff]abCX%] rhc4X%Z/Z/TV]b/TSn2i~d Z]abCd U c4X%UWqP d2k5d2n2X%^~X%^_\VQCkW4Z]aU6\Sd tVW}XcfiTVZ`]a^XVi/d2cfi] TVZd ZXVn2n*]ab/WTV]ab/W4^~c4X%U_W4U"u@W>n WXatVWoiVW4] XVd2n Uff]aT]ab/W/^_WXViW4^q:ec"k ` " l4!#"\ba%^` %" l)_e{&"\_^"\_f" fiW^` " l)fiZTVZ/WTSw7]ab/Wd ^8kffXVd Z~]ab/W4TV^_WkUsE{d2n2n2s)tX%ZiW4^~CXVX%ZCsDX%ZCiBTVfCd Z/UaVVVVs*PWcfi] TVZVU_b/T u]abCX%]]ab/W Lr6 X%U_U_QCkR/] TVZd U6Q/Z`]aW4U] X%fCn W}wx^TSk TVfSU_W4^_tX%] TVZ/U6TS2w XVn TVZ/Ws;d Z]abSWjUW4Z/U_Wj]abCX%]6]ab/WX%U_U_Q*kR/] TVZ 1^UX%] U_/W4WU rh;Yd2kRgTVU_W4U'Z/Tj^_W4U_]a^d2cfi] TVZSU5X%]hXVn2n7TVZm]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/] TVZ1^ TVZ qffTV^WR/^_Wc4d U_Wn es@]abSW4eoU_bST u]abCX%]lwxTV^YW4tVW4^_e[/ZCd ]aWU_W4]TSw"u9TV^n2iUs"W4tVW4^_e[U_W4]vTSwETVf/U_W4^_tX%] TVZ/Us"X%ZCiYW4tVW4^_e[iSd U_]a^d f/Q/] TVZ X TVZ sg]abSW4^_Wd U5Xi/d U_]a^d f/QS] TVZ 1^ TVeZ U_QCcOb]abCX%] 1^ ]ab/WmkffX%^_\Sd ZCXVnDTSw 1^ TVZ {d U WQ*XVng]aXE X%ZC1^ UX%] U/W4qU Lr60q b/W~X%Q/]abSTV^_UU_QCk5kffX%^d 4W]abCd UhX%U Lr6d UW4tVW4^e`]abCd ZS\SVqW|k0Q/U_]fgWc4X%^_WwQCnd Zd Z]aW4^_R/^_W4] Z/\]ab*d U^_W4U_QCn ] q b/W4TV^Wk Sq U_b/TuU]abCX%] shwxTV^kffX%Z`ecfiTSk0fCd ZCX%] TVZ/UTSw2X%ZCivsLrhvc4X%Zlb/TSn2i/+lwxTV^hi/d U_]a^d f/QS] TVZ/UmD^0u8d ]ab1^ W c''wTV^8U_TSkWlX%]aTSk\U ''q>Z]ab/WYR/^_W4t1d TVQ/UU_Wcfi] TVZSUs*u9Wc4XVn2n WiU_QCcObzi/d U]a^d f/Q/] TVZ/U>%iW4\VW4Z/W4^X%]aWVq2fiZ>TVQ/^{t1d W4u8s1]ab*d UUX_e`U8]abCX%]Yd Z>U_TSkWc4X%U_W4U;Lr6W4AWcfi] tVWn ec4X%Z/Z/TV]b/TSn2i/q TU_W4Wffu0b`esD/^_U_]U_Q/RSRATVUWYu@W>X%^_W\Sd tVW4ZXjU_W4]TSwBu9TV^n2iU~X%ZCi[XjU_W4] TSwTVfSU_W4^_tX%] TVZ/Uq w"T u u9W}kffX_ewW4WncfiTVZ/CiVW4Z`]Y57,%-]ab*X%]'UTSkW~ -X%ZCiYU_TSkW -c4X%Z/Z/TV]T1c4cfiQ/^5d ZR/^XVcfi] d2cfiWq8fiZm]abCd Uc4X%U_Wsu9W X%^_W&u8d2n2n2d Z/\{]aTcfiTVZ/Ud2iW4^"TVZCn ei/d U_]a^d f/QS] TVZ/U 1^BTVZ ]abCX%]"bCXatVW D^ - Vs1^ W - Vq aTV^W4gXVkR*n WZkffXaefAWXR/^_T1iQCcfi]5U_RCXVcfiWX%ZCi|d ]U `Z/TuZ]abCX%]8UTSkWmcfiTSk0fCd ZCX%] TVZ lX%Z*YdZ 5c4X%ZZSW4tVW4^ T1c4cfiQ/^ ]aTV\VW4]ab/W4^B]ab/W4Z1^ 6 4a Vq2r9W4/Z/mW ]aTfgW]abSW0U_Q/fSU_W4]BTSYw vcfiTVZ/Ud U_] Z/\8TSw1XVn2nS]abCX%]"u@W c4X%ZSZ/TV]X-R/^d TV^d^_QCn W0TVQ/] Ud2kffd2n2X%^n eEZ-d U9]ab/WU_QSf/U_W4]BTSYw cfiTVZ/Ud U_] ZS\TSw1XVn2n ]abCX%]9u9W8c4X%ZSZ/TV]'X-RS^d TV^d^_QCn W9TVQ/] q@Ee b/W4TV^_WkSq Ssd ]-d U*U_] d2n2n RATVUUd fCn WB]abCX%[] X%ZCX%^_W@U_QCcOb]abCX%] sW4tVW4Zd2wu@W"^_W4U]a^d2cfi]]aT^_Q/ZSU'u0b/W4^_WhTVZCn elTVf/U_W4^_tX%] TVZ/UYdZ 6X%^_W~k5XViW!Lr6 c4X%Z>TVZCn eb/TSn2id2;w 1^ W''wTV^@UTSkWhX%]aTSkUZSTVZ/WkR/]effU_Q/f/UW4]aU' q bCd UkWX%Z/U@]abCX%Z] Lr6k5XaelwTV^cfiW'Q/U@]aTX%UUd \VZ`77fi " |qxq}t~"}s1|R/^_TVf*X%fCd2n2d ]em0]aT6UTSkWW4tVW4Z`]aU]ab*X%] s17%,su9W4^W6cfiTVZSUd2iW4^_Wi RgTVU_Ud f*n Wq@DgXVkRCn W4UVq26X%ZCi8Sq2d2n2n Q/U_]a^X%]aW]abCd URSb/W4Z/TSkW4Z/TVZ*q*WYkffXaeU_Q*kffkffX%^d 4W]abCd UhX%UU_TSkW4] d2kW4UmLr6d U;Z/TV]abCd Z/\SVq{d tVW4Z]ab/W4^_WwTV^_W]abCX%] Lr6d2kRgTVU_W4U&UQCcbU]a^_TVZ/\cfiTVZCi/d ] TVZ/Us)]ab/W^_WXViW4^k5Xaeju9TVZCiVW4^&ub`e]ab/W4^_Wd UffU_T|k0QCcbU_]aQCieTSw9]ab/W Lr6 cfiTVZCiSd ] TVZd Z]ab/W~U_] X%] U_] d2cfiUn2d ]aW4^X%]aQ/^_Wq b/W^_WX%U_TVZvd U]abCX%]6U_TSkWTSwB]abSWU_RgWc4d2XVn*Ud ]aQCX%] TVZ/Umd ZubCd2cb Lrh b/TSn2iU6TSw]aW4ZX%^d U_Wd Zkffd U_Ud Z/\iSX%] X>X%ZCiU_Q/^tgd tXVnX%ZCXVn eUd U"R/^TVfCn WkUq@yBW4^W8d U'X%ZffW41XVkRCn W-PQSR/RgTVU_W9]ab*X%]B]ab/WU_W4]"TSwTVf/UW4^_t)X%] TVZSU'c4X%Z5fgW*fisubSW4^_W;WXVcOb *"d UX0RCX%^_] ] TVZTSCw ]abCX%]d UsAX0U_W4]@TSw)RCXVd ^_u U_WhiSd U_TSd Z]u^d ]a]aW4Z>X%ZU *r&U_Q/fSU_W4]aU;TSKw ub/TVU_W8Q/ZCd TVZdU vaq8Q/^_]ab/W4^'U_Q/RSRATVUW]abCX%]'TVf/U_W4^_tX%] TVZ/UffX%^_W \VW4Z/W4^X%]aWiYfe]ab/WwTSn2n u8d Z/\mR/^TgcfiW4U_UsEu0bCd2cb[u@Wmc4XVn2n {:f)z*q}PVTSkWYfgW4]u@W4W4ZX%Z*Ucb/TVU_W4ZXVc4cfiTV^iSd Z/\]aTU_TSkWX%^_fCd ]a^X%^eoi/d U_]a^d fSQ/] TVcZ X - ZCiW4RgW4ZCiVW4Z`] n eU6cOb/TVU_W4ZXVc4cfiTV^i/d ZS\j]aqXE>q b/WX%\VW4Z]]ab/W4ZTVf/U_W4^_tVW4U0]ab/WQ/ZCd2 Q/Wh * U_Q*cb]abCX%[] >q*fiZ`]aQ*d ] tVWn es)]ab/WRCX%^_] ] TVZ/U * kffXae^_W4R/^W4U_W4Z`]-]ab/W'TVf/U_W4^t)X%] TVZ/U&]ab*X%]c4X%ZfgW6kffXViW'u8d ]ab>X;RCX%^_] d2cfiQCn2X%^&U_W4ZSU_TV^q bQ/UCX - iW4]aW4^k5d Z/W4U]ab/WR/^_TVf*X%fCd2n2d ]eo]abCX%]X5R*X%^_] d2cfiQCn2X%^6U_W4Z/UTV^d U~cOb/TVU_W4ZCX iW4]aW4^kffd Z/W4Uh]ab/WR/^TVfCX%fCd2n2d ]eo]abCX%]~XRCX%^_] d2cfiQ*n2X%^u@TV^n2i}d UYcOb/TVU_W4ZCq b/W6UW4Z/U_TV^YX%ZCi]ab/Wffu@TV^n2i]aTV\VW4]ab/W4^iW4]aW4^k5d Z/Wff]ab/WffTVfSU_W4^_tX%] TVZ]abCX%]6d UffkffXViWq0]6d UWX%U_e~]aTjU_W4W ]abCX%]]abCd UffkWcbCX%Z*d Uk ZCiQCcfiW4U6XYiSd U_]a^d f/Q/] TVZlTVZ wxTV^'ubCd2cObLr6b/TSn2iVUqb & + &vVs`X%ZCi + V h$v&cfiTV^_^_W4U_RgTVZCiVU1]aT XbSWU_RgWc4d2XVnc4X%U_WBu8d ]aUd2kR*n Whkffd UUd Z/\i/X%] X;R/^_TVfCn Wka11XVkRCn WSq2;fgWn u8aq*fiZ]aQCd ] tVWn es)Wd ]ab/W4^cfiTSkR*n W4]aW5d ZCwxTV^kffX%] TVZUff\Sd tVW4ZCs"TV^ff]ab/W4^W}d UffZ/Ti/X%] X>X%]~XVn2n2qfiZ]abCd UcfiTVZ]aW4`] #Lr6d UffTSw]aW4Zc4XVn2n WizxQyV#p ,S{fi/|pqfiZkTV^_W^_WXVn2d U_] d2c5rh R/^_TVfCn WkUs*u9WkffXaeoTVf/U_W4^_tVW>XYtVWcfi]aTV^ffu8d ]ab[U_TSkWjTSw;d ]aUcfiTSkRgTVZ/W4Z]aU5kffd U_Ud ZS\SqfiZlUQCcbc4X%U_W4U]abSW Lrh cfiTVZ*i/d ] TVZ>U_TSkW4] d2kW4U{U_] d2n2nAb/TSn2iUqZmR/^XVcfi] d2c4XVnkffd U_Ud Z/\i/X%] XLR/^TVfCn WkUs]ab/WB\VTSXVnd UETSwx]aW4Z8]aT8d ZCwW4^D]ab/W0i/d U_]a^d f/Q/] TVZ D^1TVZ^_Q/ZSEU w^_TSk[U_QCc4cfiW4UUd tVWTVf/U_W4^t)X%] TVZ/U6TS!w W&q bCX%]d UsLTVZ/WjTVf/U_W4^tVW4U~X5UXVkRCn W> )&/ 4 ),+0/ ^ i^i^i4 ) j / sLub/W4^_W ) * / qe`RCd2c4XVn2n esV]ab/W ) * / X%^_W{X%U_U_Q*kWi]aT'fgWX%Zd2q2d2q2i/q@ad ZCiW4RgW4ZCiVW4Z`] n eYd2iVW4Z`] d2c4XVn2n ei/d U_]a^d f/QS]aWi//UXVkRCn WTSw-TVQ/] cfiTSkW4UTSw W&q b/WcfiTV^_^_W4U_RgTVZCiSd Z/\ u@TV^n2iU& 6 + ^ i^i^iTVQ/] cfiTSkW4UTS[w X%^_WSTVf/U_W4^tVWi/[q r9W4RAW4Z*i/d Z/\hTVZ]ab/WUd ]aQCX%] TVZCUD^{kffXaejfgWhcfiTSkR*n W4]aWn ejQSZ/`ZST uZjTV^d UX%U_U_QCkWi]aT6fgWXkWk0fgW4^TSwCU_TSkW5RCX%^XVkW4]a^d2cYwXVk5d2n elTSw-i/d U]a^d f/Q/] TVZ/Uq64wD]ab/W5Z`QCk0fgW4^{TSwCTVf/U_W4^_tX%] TVZ/U ld Un2X%^_\VWs]ab/W4Zc4n WX%^n e8]ab/W9UXVkRCn W' )&/ 4 ),+0/ ^ i^i^i4 ) j / c4X%ZhfAW"Q/U_Wi0]aT;TVf/] XVd ZX"^_WX%U_TVZCX%f*n W9W4U_] d2k5X%]aWTS2w 1^ sS]ab/WkffX%^_\Sd ZCXVn*i/d U_]a^d fSQ/] TVZ~TVZ q{CQ/]0TVZSWd U6d Z`]aW4^W4U_]aWimd Z]ab/WwxQ*n2nEi/d U_]a^d f/Q/] TVZ D^qbCX%] i/d U_]a^d f/QS] TVZQ/U_QCXVn2n ec4X%Z/Z/TV]0fAW5d Z*wxW4^_^Wihu8d ]ab/TVQ/]k5X%gd Z/\lXVi/i/d ] TVZCXVnCX%U_U_QCkRS] TVZ/UsU_QCcObX%U0]abSWW LrhX%U_U_QCkR/] TVZCqrX%ZCd Wn Us6 BTVfCd Z/Us VVVVa~PVQ/R/RgTVU_Wl]abCX%]oXn/ p \es@ aXVi/X%R/]aWiwx^_TSk aPScbCX%^wU_]aWd ZCmkWi/d2c4XVn"U_]aQCiVezd UcfiTVZCiQCcfi]aWi>]aTo]aW4U]5]ab/WW47Wcfi]jTSwXZ/W4u i^_Q/\Sq b/Wi^_Q/\|d UmXVi/kffd ZCd U]aW4^_Wi]aToXh\V^TVQ/RTSwCRCX%] W4Z`]aU8TVZ|Xhu@W4W4gn emfCX%Ud UqCWwxTV^_W5]ab/WffW4RAW4^d2kW4Z`]5d UU_] X%^_]aWi}X%Z*iXVwx]aW4^d ]Yd U/ZCd Ub/Wi/sU_TSkW cbCX%^XVcfi]aW4^d U_] d2cUXaes]ab/WfCn TT1i'R/^_W4U_U_QS^_WATSw`]ab/W&RCX%] W4Z]aUd UkWX%U_Q/^_WiSq bSWi/X%] XX%^_W]abQ/Ui/d 7W4^W4ZCcfiW4U'd Zfff*n T`T1iR/^_W4UU_Q/^_WwTV^'d ZCiSd tgd2iQ*XVnRCX%] W4Z`]aU@fgWwxTV^WX%ZCi5XVw]aW4^"]ab/W0]a^_WX%] kW4Z] qfiZR/^XVcfi] d2c4XVngU_]aQ*i/d W4UTSw1]abCd Ugd ZCiSsSTSw]aW4Z>U_W4tVW4^XVngTSw1]ab/W6R*X%] W4Z`]aUYiV^_TVRTVQ/]TSwD]abSW6W4`RgW4^d2kW4Z] qTV^;U_QCcb~RCX%] W4Z`]aU']ab/W4^_Wjd U]ab/W4ZZ/Tli/X%] XVqLWjkTgiWn]ab*d U X%U wTSn2n u0UU]ab/WU_W4]TSwSRATVUUd fCn WtXVn Q/W4ULTSw]ab/WcObCX%^XVcfi]aW4^d U] d2c*u9WX%^_Wd Z`]aW4^_W4U]aWi ZWq \Sq2sfCn TT1iR/^W4U_U_Q/^_W'i/d 7W4^W4ZCcfiWaq & +u8d ]ab &vVs@X%ZCi + V hvX%UjX%fgT tVWqVTV^>%cfiTSkR*n2d W4^_UlRCX%] W4Z`]aUh]abCX%]i/d2iZ/TV]5i^TVRTVQ/] as1u@WffTVf/U_W4^tVW VsEub/W4^W U{]ab/W6tXVn Q/WffTSwE]ab/W~cObCX%^XVcfi]aW4^d U_] d2cu@W6uX%Z]]aTokWX%UQ/^_WqTV^i^_TVRgTVQ/]aUs1u9WffTVfSU_W4^_tV=W W]abCX%]jd UsDu9W5TVf/U_W4^_tVWffZSTV]abCd Z/\}X%]YXVn2n2aq5W]abQ/U{bCX_tVWs@wTV^W41XVkRCn Ws@XhU_W Q/W4ZCcfiW5TSwETVfSU_W4^_tX%] TVZ/Uj & & V4 + + V4 . l4 22 V4 l^ i^i^i 4 j j7Vq4wC]abCd U UXVkRCn Wmd Un2X%^\VWYW4Z/TVQS\VbCsDu@Wc4X%ZoQ/UWld ]]aTlTVf/] XVd ZX^_WX%U_TVZ*X%fCn WW4U_] d2kffX%]aW{TSw/]ab/WR/^_TVf*X%fCd2n2d ]eY]abCX%]X'RCX%] W4Z]i^_TVR/U-TVQ/]]ab/W^X%] T5TSwTVQ/] cfiTSkW4U0u8d ]ab`77fibc edbgfb]fbz}zhcyz1s1t yz|qu+* ]aT]abSW6]aTV] XVn1ZQCkfgW4^TSwCTVQ/] cfiTSkW4UaqhWlc4X%Z|XVn UT\VW4]jX ^_WX%UTVZCX%fCn WffW4U_] d2k5X%]aW5TSwC]ab/Wi/d U_]a^d f/Q/] TVZTSw?W wTV^&]ab/WffcfiTSkRCn e1d Z/\6RCX%] W4Z]aUq TV\VW4]ab/W4^0]ab/W4U_W]u@T~i/d U_]a^d f/Q/] TVZ/UiW4]aW4^kffd Z/W]ab/WYiSd U_]a^d f/Q/] TVZTSwg qWX%^_Wjd Z]aW4^_W4U_]aWimd Zl]ab/WW4AWcfi]'TSwA]ab/Wji^Q/\~d Z]abSW\VW4Z/W4^XVnSRgTVR/QCn2X%] TVZCq "ZCwTV^_]aQ/ZCX%]aWn es*d ]k X_efgW"]ab/Wc4X%U_W-]abCX%]L]ab/W@W47Wcfi]TVZi^TVRATVQS]aU@d Ui/d AW4^_W4Z`]w^_TSk]ab/W9W4AWcfi]TVZcfiTSkRCn2d W4^_Uq@aPScbCX%^wpffU_]aWd ZCs4rX%ZCd Wn UsX%Z*iTVf*d Z/U@aVVVVLi/d UcfiQSU_U@X%ZXVcfi]aQCXVnkWi/d2c4XVn4U]aQCieffd Z8u0bCd2cb8R/b`eUd2c4d2X%Z/U*QCi\VWi]ab/W6W4AWcfi]{TVZ|i^TVRATVQS]aU']aTfgWhtVW4^_ei/d AW4^_W4Z`]5w^_TSk]ab/W6W4AWcfi]{TSw@cfiTSkRCn2d W4^_Uq2 b/W4Zmu9W~c4X%ZSZ/TV]ZCwW4^]ab/Wmi/d U_]a^d fSQ/] TVZ}TVZwx^_TSk]ab/W5TVfSU_W4^_tX%] TVZ/U & 4 + ^ i^i^i@XVn TVZ/WYu ]ab/TVQ/]YkffX%1d Z/\[XVi/i/d p] TVZCXVn7X%U_U_QCkRS] TVZ/U0X%fgTVQ/]Bb/Tu]ab/W8i/d U]a^d f/Q/] TVZ~wTV^'i^TVRATVQS]aU0d UB^_Wn2X%]aWi]aT{]ab/Wi/d U_]a^d f/Q/] TVZwTV^cfiTSkRCn2d W4^Uq SW4^bCX%R/UB]ab/W0Ud2kRCn W4U_]UQCcbX%UU_QCkR/] TVZ5]abCX%]BTVZ/Wc4X%ZmkffX%VW8d UB]abCX%]"]ab/W8i/d U_]a^d f/Q/] TVZTSYw W wTV^6iV^_TVRgTVQ/]aUff5d ZwXVcfi]]ab/W UXVkWX%U]ab/WiSd U_]a^d f/Q/] TVZmTSYw W wTV^ffcfiTSkRCn2d W4^_U]ab/Wi/X%] XX%^_Wz%k5d U_Ud Z/\|X%]5^X%ZCiTSkffVq{w;cfiTVQS^_U_Ws"]abCd UlX%U_UQCkR/] TVZd U5Q/U]6]ab/W Lr6 X%U_UQCkR/] TVZCqEerhb/TSn2iU8d wxTV^hXVn24nb/W4TV^WkVq2VaXVaD^W v D^ W Wv D^ WubCd2cOb>kWX%Z/U&Q/U_]@]abCX%]&]ab/Wffi/d U_]a^d fSQ/] TVZTSwF U{d ZCiVW4RAW4Z*iW4Z`]@TSwu0b/W4]ab/W4^{X;RCX%] W4Z`]iV^_TVR/U-TVQ/]vETV^-Z/TV] q bQ/Us1 Lr6c4X%ZfgWhX%UU_QCkWi/s]ab/W4Zu@W6c4X%Z>d ZCwW4^-]ab/Wffi/d U_]a^d fSQ/] TVZYTVZubCd2cObod U0u0bCX%]&u9WjX%^_W{^_WXVn2n e>d Z]aW4^_W4U_]aWi~d Z*aqlX%Z`effRS^_TVfCn WkUd Zmk5d U_Ud Z/\i/X%] X8X%Z*i{U_QS^_tgd tXVnSX%Z*XVn e`Ud U{X%^_W;TSw)]ab/W;gd ZCijd2n2n Q/U_]a^X%]aWijX%fgTtVW b/WX%ZCXVn eUd U@u9TVQCn2ifgW\V^_WX%] n eYUd2kRCn2d /WiYd2Uw Lr6}bSTSn2iUsVfSQ/]Bub/W4]abSW4^"TV^@Z/TV]9]ab*d U'd U@U_Tjd U@Z/TV]c4n WX%^qfi]jd U ]ab/W4^_WwTV^_W5TSw*TVftgd TVQSUd Z]aW4^_W4U_] ]aTd ZtVW4U_] \SX%]aWub/W4]abSW4^s9w^_TSkTVf/U_W4^_t1d Z/\~]abSW[%cfiTSX%^_U_W4Z/WiSi/X%] X6 )&/ 4 )+0/ ^ i^i^i 4 ) j / XVn TVZ/Ws1d ]kffX_emXVn ^_WXViVe5fgWRATVUUd fCn W']aT6]aW4U_]&]ab/WffX%U_U_QCkR/] TVZ]abCX%Z] Lr6b/TSn2iUq;TV^'W41XVkRCn WsTVZSWYkffd \Vb] d2kffX%\Sd Z/W8]abCX%]0]abSW4^_WYX%^_Wi/d U_]a^d f/QS] TVZ/UTVZ wxTV^;ubCd2cb Lr6Ud2kR*n ec4X%Z/Z/TV]&b/TSn2iSq*w)]abSW'WkRCd ^d2c4XVn1i/d U_]a^d f/Q/] TVZTSw]ab/Wff*1u@W4^_W%c4n TVU_Wffad Z]ab/WffX%R/R/^TVR/^d2X%]aWU_W4Z/UW@]aT}Xi/d U_]a^d f/QS] TVZ]abCX%]^_QCn W4UTVQ/W] rh;sS]abSW6U_] X%] U_] d2c4d2X%Zkffd \Vb`]5d Z*wxW4^]abCX%q] 1^ffiTW4U{Z/TV]UX%] Uwe rh;;q "ZCwTV^_]aQ/ZCX%]aWn esgd2Cw U-/ZCd ]aWs`]ab/W4Zj]ab/W^_W4U_Q*n ]9TSwDd2n2n2st)X%Z>iW4.^ ~EXVX%ZCsgX%ZCiBTVfCd Z/UaVVVVsPWcfi] TVZV"^_WwxW4^^_Wi6]aTX%];]abSWfgW4\Sd Z/ZCd Z/\YTSwA]abCd U'U_Wcfi] TVZmU_b/T u0U;]abCX%]'u@Wjc4X%ZmZ/W4tVW4^'^_QCn WTVQ/] rhd Z]abCd UuXaeqW6X%^_W6d Z`]aW4^_W4U]aWijd Zj]ab/WhQ/W4U] TVZjTSwu0b/W4]ab/W4^Qrhzc4X%Zb/TSn2i5d ZXZ/TVZCiW4\VW4Z/W4^X%]aW0UW4Z/U_Ws\Sd tVW4Z X%Z*ivqffV^_TSk]abCd URgTSd Z]'TSwDtgd W4u sA]ab/WhUn TV\SX%ZvU_TSkW4] d2kW4UqLr6 UZ/TV]abCd ZS\Sjk5X%VW4UU_W4Z/UWqfiZcfiTVZ]a^X%U_] s{d2n2nW4]mXVn2q;aVVVV8u@W4^_Wd Z]aW4^_W4U_]aWid Z]ab/W[QSW4U_] TVZ|u0b/W4]ab/W4^Lr6 c4X%ZfgWY]aW4U]aWiwx^TSkTVf/U_W4^_tX%] TVZ/U6TS2w XVn TVZ/Wq^_TSk]ab*X%]6RATSd Z]hTSwt1d W4u8s]abSWjUn TV\SX%Z Lr6d UW4tVW4^_e]abCd Z/\Sffk5X%VW4URgW4^wxWcfi]-U_W4ZSU_WqLfiZwXVcfi] sD{d2n2n2st)X%Z}iW4^ ~CXVX%ZCs1X%ZCiBTVfCd Z/U&u@W4^_WffQCd ]aW5X_uX%^WsX%ZCiW4RCn2d2c4d ] n e6U_] X%]aWiSs]abCX%Q] Lrh|d2kRgTVU_W4UBtVW4^_ehU_]a^TVZ/\5X%U_U_QCkR/] TVZ/UBTVZY]ab/Wi/d U_]a^d fSQ/] TVZ 1^qEfiZX n2X%]aW4^&RCX%RgW4^sSd ]@uX%U-W4tVW4Z}d2kR*n2d2c4d ] n e5U_] X%]aWi8]abCX%]d ZU_TSkW6c4X%U_W4U Lr6wTV^cfiW4ZU 1^ W q''wTV^U_TSkW{X%]aTSkU '|aTVfCd Z/Us/TV]aZCd ]a4esSPScbCX%^wU_]aWd ZCsVVVVsAPWcfi] TVZ~Vq2Vaq&-QS^&cfiTVZ]a^d f/QS] TVZd U]aT6R/^_Ttgd2iVW;]ab/WR/^Wc4d U_W5cfiTVZCi/d ] TVZ/Uu ~/WkffkffX;Sq26X%ZCi b/W4TV^WkSq S*Q/ZCiW4^@ubCd2cOb]abCd U-bCX%R/RgW4Z/UqBTVfCd Z/Us;T1iZCd ]a4es;X%ZCiP/cbCX%^wxU_]aWd ZaVVVV5XVn U_T|d Z]a^_T1iQCcfiWiX-X_eVW4Ud2X%ZvkW4]ab/Tgian2X%]aW4^W4]aW4ZCiWife[PScbCX%^wU_]aWd Z>W4]jXVn2qBaVVVVa]ab*X%]YXVn2n uU8TVZ/Wff]aTU_RgWc4d2we[X6R/^d TV^5i/d U_]a^d f/Q/] TVZ>TtVW4^XhRCX%^XVkW4]aW4q^ oubCd2cObd ZCiSd2c4X%]aW4Us9d ZXhR/^_Wc4d U_W5U_W4Z/U_WsDb/T u kQCcOb D^YiW4t1d2X%]aW4Ujw^_TSk Lr60qVTV^W41XVkRCn W.}cfiTV^_^_W4URATVZ*iUff]aT}]abSW~U_W4]YTSwi/d U_]a^d f/QS] TVZ/U D^5UX%] Uwe1d Z/\ Lr60q b/W~RS^_Wc4d U_WcfiTVZ/Z/Wcfi] TVZlfgW4]u@W4W4Z~]abCd U0u9TV^_X%ZCihTVQS^_UZ/W4WiU wxQ/^]ab/W4^d Z`tVW4U] \SX%] TVZCq`7_fi " |qxq}t~"}s1|:e $ T\ nql" Wagi\ l \bafiZ b/W4TV^_Wk Vq2X%ZCi~ Wk5kffX8Sq2n ^ " l)c" `^0a "s:k ^ " ln ^ "v%2"%lg!u9W~iW4Ucfi^d fgWrh ZX%ZXVn \VW4f/^XVd2c'uX_es"X%U5XcfiTSn2n Wcfi] TVZoTSwR/^_TVf*X%fCd2n2d ] W4U&UX%] Uwegd ZS\~cfiW4^_] XVd ZW QCXVn2d ] W4UqLfiU&]ab/W4^_WffXhkTV^_WR/^_T1cfiWiQ/^XVn20uXaejTSw^W4R/^_W4U_W4Z] Z/\Lr60~ZRCX%^_] d2cfiQCn2X%^shiTW4U]ab/W4^W>W4gd U]oXmUd Z/\Sn WkWcbCX%Z*d Uk ]abCX%]~\Sd tVW4U^d UW]aLr6 U_QCcOb]abCX%]o7 n %c4X%U_WmTSZw Lr6 c4X%ZzfAWtgd W4u@WiX%U>XU_RgWc4d2XVn;c4X%U_WTSw&]abCd U>kWcbCX%Z*d Ukff WbCX_tVWXVn ^_WXVieW4ZCcfiTVQ/Z]aW4^_WimX8RATVUUd fCn Wjc4X%Z*i/d2i/X%]aWwxTV^U_Q*cbXYkWcbCX%ZCd Ukff-]ab/W {:f)zR/^_T1cfiWiQ/^_WqfiZjPWcfi] TVZ Sq 0u@W'iW4Ucfi^d fgWi-]abCd U@kWcbCX%ZCd UkX%ZCid ZCi/d2c4X%]aWi0]abCX%]@d ]C\VW4Z/W4^X%]aW4U*TVZCn e5i/d U_]a^d fSQ/] TVZ/U]abCX%]*UX%] UwWe Lr60dq "ZCwTV^_]aQ/ZCX%]aWn esX%ULu@W"Z/TuoUb/T u8s]ab/W4^W9W41d U_;] rh[i/d U_]a^d f/Q/] TVZ/U*]abCX%]@c4X%ZSZ/TV]fgW5d Z`]aW4^R/^_W4]aWiX%U;fAWd ZS\6\VW4Z/W4^X%]aWi6fNe {:f*q-QS^EW41XVkRCn W'd UCfCX%U_Wi0TVZjX%Z W4gXVkRCn W"\Sd tVW4Zhfe6d2n2n2st)X%ZiW4;^ ~CXVX%ZCs)X%ZCiBTVfCd Z/U@aVVVVasVub/Tu@W4^_WXVcfi]aQCXVn2n e]abSW8/^U_];]aT>cfiTVZ/Ud2iW4^ub/W4]ab/W4^]ab/W4^_W W41d U_fi] ZCX%]aQ/^XVn2LkWcObCX%ZCd UkU]ab*X%];\VW4Z/W4^X%]aWXVn2nLX%ZCiYTVZCn eiSd U_]a^d f/Q/] TVZSU'UX%] Uwxe1d Z/\ Lr60q b/W4eUb/T u ]abCX%]ffd ZmU_W4tVW4^XVnARS^_TVfCn WkU'TSw1U_QS^_tgd tXVnX%ZCXVn eUd UsLTVfSU_W4^_tX%] TVZ/UX%^_W\VW4Z/W4^X%]aWiXVc4cfiTV^i/d ZS\l]aTlubCX%] ]ab/W4ezc4XVn2n@Xfi/|ph,phS/,, //Y 7 ph4q b/W4e>XVn U_T5Ub/T u]abCX%]]ab/Wd ^;^X%ZCiTSkffd 4WiffUcbSWkW{\VW4Z/W4^X%]aW4UTVZCn e>i/d U]a^d f/Q/p] TVZ/U']abCX%]'UX%] Uwe Lrh;qfiZwXVcfi] s/]abSW^X%ZCiVTSkffd 4WikTVZ/TV]aTVZ/WjcfiTSX%^U_W4ZCd Z/\jUcbSWkW8]aQS^_Z/UTVQ/];]aTfgWffX'U_RgWc4d2XVn1c4X%U_W{TSw {:fzLs1XVn ]ab/TVQ/\Vbu@W5iT6Z/TV]-R/^_TtVW{]abCd U&b/W4^Wq{d2n2n2s)tX%Z}iW4^ ~EXVX%Z*sDX%ZCiTVf*d Z/U"U_bST ufehW4gXVkRCn W0]abCX%]9]ab/W^X%Z*iTSkffd 4Wi5cfiTSX%^U_W4ZCd Z/\8Ucb/WkW4UiTZ/TV]BU_]Q vlcfiW]aT\VW4Z/W4^X%]aWXVn2Un Lr6i/d U]a^d f/Q/] TVZ/Uq@DU_U_W4Z] d2XVn2n e]ab/WUXVkWW41XVkRCn WU_bST uU&]abCX%{] {:fzoiVT`W4U&Z/TV]&Wd ]abSW4^qn/ p \eE LTVZ/Ud2iW4^-U_QSfc4X%UWhad2d2*TSwC11XVkRCn WSq26X%\SXVd ZCqV~/W4] & 4 + sg . X%ZCi!' & sC' + X%ZCi' . fgW6X%U8d Z]abCX%]&W41XVkRCn Ws1X%ZCiX%UU_QCkWffwTV^&Ud2kRCn2d2c4d ]ej]ab*X%]. ' & U' + !' . q b/WW4gXVkRCn WU_b/Tu9Wij]ab*X%]]ab/W4^_WhW4gd U]aU5i/d U_]a^d f/QS] TVZ/=U 1^UX%] Uwe1d Z/\ Lrh$d Zl]ab*d U5c4X%U_W6u8d ]ab1^Y'1*V$wTV^l VV4V4VVshXVn2n"bCXat1d Z/\cfiTVZCi/d ] TVZCXVn9R/^TVfCX%fCd2n2d ] W4U 1^ +*UWVV[wxTV^XVn2n| * qq n WX%^n es" & 4 + X%ZCi . c4X%Z/Z/TV]fgWh\V^_TVQ/RgWiY]aTV\VW4]ab/W4^]aT}wTV^k X8U_W4]TSwERCX%^_] ] TVZ/U{TSwvq&PTSs)W4tVW4Z]ab/TVQ/\Vb Lr6bSTSn2iU8wTV^ 1^ CQ6f)zc4X%ZSZ/TV]fgWQ/U_Wih]aT5Ud2kQ*n2X%]aqW 1^ qbCd2n W{d2n2n2s0t)X%Z iW4^ ~EXVX%Z*sX%ZCiTVfCd ZSUaVVVVshPWcfi] TVZ$VX%U_|ub/W4]ab/W4^j]ab/W4^_WmW41d U_]aU}X\VW4Z/W4^XVn9kWcb*X%ZCd UkwTV^\VW4Z/W4^X%] Z/\oXVn2n"X%Z*iTVZCn eLrhi/d U_]a^d fSQ/] TVZ/UsD]ab/W4e[iVTZ/TV]YkffX%VW5]abCd UQ/W4U_] TVZ}kffX%]ab/Wk5X%] d2c4XVn2n ejRS^_Wc4d U_Wq*r'U&Z/TV]aWi8f`em{d2n2n2st)X%Z>iW4Z^ ~CXVX%ZCsgX%ZCiTVfCd Z/Us]abSW'RS^_TVfCn Wkb/W4^_Wjd U]abCX%]0u ]ab/TVQ/]X%ZecfiTVZSU_]a^XVd Z]0]aT5u0bCX%]cfiTVZ/U_] ]aQ/]aW4U6XtXVn2d2i~kWcbCX%ZCd Ukffs)]ab/W4^_Wjd U c4n WX%^n eX-]a^d t1d2XVnVUTSn Q/] TVZ5]aT8]ab/WR/^_TVf*n Wkff9d tVW4ZmX{iSd U_]a^d f/Q/] TVZ 1^BUX%] Uwxe1d Z/q\ rh;su9W;Ud2kRCn ei^X_uXu@TV^n2$XVc4cfiTV^i/d Z/\h]a18^ sgX%ZCi]ab/W4Zi^Xau |U_QCcOb]abCX%H] >vXVc4cfiTV^i/d ZS\ ]aTh]ab/W6i/d U_]a^d f/Q/] TVZ1^W aq bCd Ud U0TVftgd TVQSUn emcOb/WX%] Z/\md ZU_TSkWU_W4ZSU_WqLfiZ`]aQCd ] tVWn esCd ]U_W4WkU0]abCX%]X ^_WX%U_TVZCX%fCn W&kWcbCX%ZCd Uk Ub/TVQCn2i~Z/TV]hfAWmXVn2n u@Wim]aTcb/TTVU_W> XVc4cfiTV^i/d ZS\l]aTXmi/d U_]a^d f/Q/] TVZNiW4RgW4ZCi/d ZS\{TVZ q*fi]iTW4U-Z/TV]@bCXatVW]abCX%]-1d ZCi{TSwEcfiTVZ`]a^_TSn`TtVW4^&]ab/W;TVfSU_W4^_tX%] TVZ/U&]abCX%]X%^_WffkffXViWqPTmub*X%]cfiTVQ/Z]aU~X%U~X^_WX%U_TVZCX%f*n W>kWcb*X%ZCd Ukff$Z]aQCd ] tVWn es"]ab/W>kWcb*X%ZCd UkU_b/TVQ*n2i~fgWX%fCn W{]aTmcfiTVZ`]a^TSnTVZCn eubCX%]c4X%ZfgW5cfiTVZ`]a^_TSn2n Wild ZX%Z~W4`RgW4^d2kW4Z] XVnUW4]aQ/RCqLWc4X%Z~]abCd Z/jTSw7]ab/WkWcObCX%ZCd UkX%U X%Z%X%\VW4Z`] ]abCX%]&Q/U_W4UX'UW4]&TSw/U_W4ZSU_TV^_U&]aTffTVf/] XVd Zd ZCwxTV^kffX%] TVZoX%fgTVQ/]-]ab/Wu@TV^n2i/q^ XE>q|bCd2n W]abSWmX%\VW4Z]~kffXaezcfiW4^_] XVd ZCn ecb/TTVU_WubCd2cObb/WX%\VW4Z]~iT`W4UhZ/TV]hbCX_tVW>cfiTVZ`]a^_TSn*TtVW4aU_W4Z/UTV^']aTQ/UWs*d ]ffd UZ/TV]'^_WX%U_TVZCX%f*n W ]aT>X%U_UQCkW ]abCX%]U_b/Wc4X%ZcfiTVZ`]a^_TSn7]ab/Wd ^TVQS]aR/Q/]6TV^'W41XVcfi] n eubCX%]]ab/W4e[c4X%ZU_W4Z/U_Waq{Z*iW4Wi/sA\Sd tVW4Z|X8u@TV^n2qsA]ab/W6TVfSU_W4^_tX%] TVZ^_W4]aQ/^_Z/WiYfe~]ab/WhU_W4ZSU_TV^5d UwQCn2n eliW4]aW4^kffd Z/Wi/q bCd Ud U&W41XVcfi] n ej]ab/WU_W4]aQ/R}d2kRCn WkW4Z`]aWid Z]ab/6W {:f)zUcOb/WkW5i/d UcfiQSU_U_WiZPWcfi] TVZSq Ss*ubCd2cObu@Wj]ab/W4^_WwTV^_WY^W4\SX%^i[X%UXln W4\Sd ] d2kffX%]aWokWcObCX%ZCd Uk5qWjZ/T ud Z]a^_T1iQCcfiWXhR/^TgcfiWiQS^_!W {:f s1ub*d2cb}W4]aW4ZCiU {:f*s9X%Z*i]aQ/^_Z/UTVQ/]{]aT~\VW4ZSW4^X%]aWmXVn2n"X%ZCiTVZCn ei/d U_]a^d f/Q/] TVZ/U{UX%] Uwxe1d Z/\ Lrh;!q 4QSU_]5n2d V\W {:f)z*e{:f X%U_U_QCkW4U]abCX%]{]abSW4^_Wd UjXcfiTSn pn Wcfi] TVZTSwAU_W4Z/UTV^_Us*X%ZCi~d ]6cfiTVZ/U_QCn ]aU6X\Sd tVW4ZlUW4Z/U_TV^'u8d ]ab[XYcfiW4^_] XVd ZmR/^_WiW4]aW4^kffd Z/Wi5R/^_TVfCX%fCd2n2d ]eq`77fibc edbgfb]fbz}zhcyz1s1t yz|quy"T u@W4tVW4^sAQ/ZCn2d VUW {:f*2Q6f)z kffXaed \VZ/TV^_WX{U_W4ZSU_TV^;^WXVi/d Z/\Sq b/WjiWc4d Ud TVZlu0b/W4]ab/W4^TV^ZSTV]YX6U_W4ZSU_TV^^_WXViSd Z/\od Ujd \VZSTV^_Wi}d UXVn2n u@Wi]aT[iW4RgW4ZCiTVZCn emTVZo]ab/WffU_W4Z/UTV^]abCX%]8bCX%UfgW4W4ZcbSTVU_W4Zfej]ab/WjX%\VW4Z] sEX%ZCiffTVZ~]ab/WTVf/U_W4^_tX%] TVZ\VW4ZSW4^X%]aWifffej]abCX%]0U_W4ZSU_TV^qfi]8d U;Z/TV]8XVn2n u@WiY]aTiW4RgW4ZCi8TVZ~]abSW5XVcfi]aQCXVnu@TV^n2i/sUd Z*cfiW]ab/WYX%\VW4Z] kffX_eYZ/TV]0`Z/TuubCX%]&]abSWYXVcfi]aQCXVn)u@TV^n2id Uq0-ZCcfiWX%\SXVd ZCs7]ab/W8R/^_T1cfiWiQ/^_WYd U~^_WX%U_TVZ*X%fCn Wd Z]abCX%]']ab/WX%\VW4Z`]ffd U'TVZCn e}XVn2n u@Wij]aTmcfiTVZ]a^_TSn7ubCX%]hc4X%ZfgW5cfiTVZ`]a^TSn2n Wid ZoX%Z~W4`RgW4^d2kW4Z] XVn)U_W4]aQSRCqW Z/T uvR/^W4U_W4Z`Q] {:f s]ab/W4ZX%^_\VQ/W ]abCX%]hd ]hd U~^_WX%U_TVZCX%f*n Wjd Zl]ab/W8U_W4Z/U_WX%fATtVWsX%ZCi]abCX%]8d ]0\VW4Z/W4^X%]aW4U6XVn2nDX%ZCihTVZCn ee Lr6i/d U]a^d f/Q/] TVZ/Uq[<a21\ rk a\{:f)zVqZD^_W4RCX%^X%] TVZ*Ld >X%ZoX%^_f*d ]a^X%^_e>i/d U_]a^d fSQ/] TVZ^XgLd >XU_W4](OTVZvqTSw/R*X%^_] ] TVZ/U0TSwRvs1X%ZCihSlX%ZX%^fCd ]a^X%^_ei/d U_]a^d fSQ/] TVZ^X[TVZ[O5qb/T`TVUW6Z`QCk0fgW4^_Ufiff h V4V{X%ZCi<w_ h V4;wxTV^WXVcOb}RCXVd ^ja" @U_QCcb}]abCX%]X%ZCi UX%] Uwe1d Z/\>]ab/W[wxTSn2n Tu8d Z/\cfiTVZSU_]a^XVd Z] swxTV^YWXVcb U_QCcb]abCX%]X hKVX;_) P / F P `Vq{&W4Z/W4^X%] TVZCVq2b/T`TVUW.vVq2b/T`TVUWVq2mxXVc4cfiTV^iSd Z/\ff]aTaXE>qXVc4cfiTV^i/d Z/\5]aTaXq[~/W4]8fgW]ab/WQ/ZCd2 Q/WU_W4]8d Z]ab~RS^_TVfCX%fCd2n2d ]eU]aW4RVq2Vq_s)^W4]aQ/^_Z} 480X%ZCi6bCXVn ] qLU_QCcOb]abCX%]Vv>q]ab~R/^_TVf*X%fCd2n2d ]eq_s\VTff]aTfi]hd UWX%U_e~]aTjUW4W8]abCX%6] {:f)zd U']abSW8U_RgWc4d2XVnLc4X%U_W TSw){:f)zFub/W4^_W! _ YwTV^ffXVn2na" aq8rhn2n Tu8d Z/\^w_ \Sd tVW4U8Q/UffXn2d ]a] n WkTV^_W3nSW4gd fCd2n2d ]eq TQ/ZCiVW4^_U_] X%ZCiY]ab/Wh^_TSn W6TSwE]ab/WcfiTVZ/U_]a^XVd Z`]YSas1Z/TV]aW6]abCX%]w_ U{]abSW RS^_TVfCX%fCd2n2d ]e]abCX%]]abSW~XVn \VTV^d ]abCkiTW4UZ/TV]]aW4^k5d ZCX%]aW~X%]U_]aW4RoVq2Vs\Sd tVW4Z]abCX%]$X%Z*i X%^_W5cOb/TVU_W4ZoX%]&U]aW4RoVq2VqLfi]wxTSn2n TuU0]abCX%]&]abSWR/^_TVfCX%fCd2n2d ]e4l]abCX%]XRCXVd ^ 4;d UZ/TV]TVQS]aR/Q/]X%]U_]aW4RVq25wTV^U_TSkWYd U`) P / F PX _b Q/UsAS*UXaeU&]abCX%]-]ab/WR/^_TVf*X%fCd2n2d ]e4]abCX%]{X0R*XVd ^&ub/TVU_W'/^_U_]cfiTSkRgTVZ/W4Z]d U[d U&Z/TV]9TVQS]aR/Q/]X%]U_]aW4R[Vq2ffd U]ab/WUXVkWYwTV^8XVn2n4 vq{:f)z c4X%Z\VW4Z/W4^X%]aW>]ab/W Lrh i/d U_]a^d fSQ/] TVZ$d Z11XVkRCn W}Sq2Vs'ubCd2cbcfiTVQCn2i[Z/TV]fgW\VW4Z/W4^X%]aWif`^e {:fzLq TU_W4W6]abCd UsAQSUd Z/\]ab/W6UXVkWhZ/TV] X%] TVZzX%U5d Z]ab/W6W41XVkRCn WsBcfiTVZ/Ud2iW4^]ab/WmU_W4]TSw&R*X%^_] ] TVZ/xU & + . u8d ]ab * V * ' * Vq ~W4q] X & X +X;- . VVV`/ ) VsCX%ZCN) Vq&fi] UWX%U_e~]aTYtVW4^d2we]abCX%]6wxTV^6XVn2gn $vsu@W8bCX_tVW]abCX%]P F P ` X _ VVVsU_Tl]abCX%]h]ab/WcfiTVZ/U_]a^XVd Z]~Shd U UX%] U_/Wi/q5TV^_W4T tVW4^si/d ^_Wcfi]c4XVn2cfiQCn2X%] TVZU_bST uUj]abCX%] swTV^mX%^_f*d ]a^X%^_e XE>s@]ab/WiSd U_]a^d f/Q/] TV{Z 1^ TVZ^_QSZ/U5\VW4Z/W4^X%]aWifae Q6f)zF-u8d ]ab~]abCd Uhcb/TSd2cfiW TSw7RCX%^XVkW4]aW4^U U'R/^_Wc4d U_Wn e]ab/W8Q/ZCd2 Q/W5i/d U_]a^d fSQ/] TVZUX%] Uwe1d Z/\Lr6d Z]ab*d U8c4X%U_Wq`7fi " |qxq}t~"}s1|PT~ub`edU {:fz`'^_WX%U_TVZCX%f*n W$1tVW4Zo]ab/TVQ/\Vbou9Wffb*XatVWjZ/TV]{\Sd tVW4ZX~wTV^kffXVnBiW4SZCd ] TVZTSw'^WX%U_TVZCX%fCn WaXVn ]abSTVQ/\Vb|d ]5c4X%Z>fgWiTVZ/Wd Z]ab/W6^_Q/ZSUhw^XVkW4u@TV^__W4U_U_W4Z] d2XVn2n esDWXVcOb>U_]aW4R>TSw]ab/WoXVn \VTV^d ]abCk c4X%ZviW4RgW4ZCilTVZCn eTVZd ZCwTV^kffX%] TVZvXatXVd2n2X%fCn W~]aTo]ab/WW4`RgW4^d2kW4Z`]aW4^s9ubSW4^_W]ab/W%d ZCwTV^kffX%] TVZCd UhW4ZCcfiT1iWi[d Z[]ab/WTVf/U_W4^_tX%] TVZ/U~kffXViVWYfe>]ab/WW4RAW4^d2kW4Z`]aW4^~d Z[]ab/WcfiTVQ/^_UWjTSw^_Q/ZSZCd Z/\h]ab/W5XVn \VTV^d ]abCk5asCd ]8d U0Z/TV]bCX%^i ]aTYU_W4W{]abCX%{] {:fz UX%] U_/W4U;TVQ/^8d Z]aQCd ] tVWjiW4Ud2iW4^_pX%] XVq b/WjVW4e}RgTSd Z]d Uh]abCX%]XVn2nC]ab/Wj^_Wn W4tX%Z`]6U_]aW4RSUd Z[]ab/WXVn \VTV^d ]abCk c4X%ZfgWmc4X%^_^d WilTVQ/] f`eX%ZmW4`RgW4^d2kW4Z`]aW4^q bSWRCX%^XVkW4]aW4^_fiU 5X%ZC4_ wxTV^OX%ZCiNX%^_WcbSTVU_W4ZmfAWwTV^_W ]ab/WXVn \VTV^d ]abCk fAW4\Sd ZSUg]abCd U5c4X%ZcfiW4^_] XVd Z*n efgWiTVZ/WhfeX%ZW4RAW4^d2kW4Z`]aW4^q5P/d2kffd2n2X%^n esBd ]ffd UU_]a^XVd \Vb]apwTV^_uX%^i5]aTcbSWcl]abCX%]]ab/W8W QCX%] TVZS9b/TSn2iUffwTV^;WXVcObvq;r'U6wTV^']ab/WXVn \VTV^d ]abCk ]aU_Wn2ws]ab/WYW4`RgW4^d2kW4Z]aW4^ bCX%UZ/T[cfiTVZ`]a^TSnDTtVW4^6]ab/WlcOb/TSd2cfiWYTSw hL]abCd Ud UjcOb/TVU_W4Z[f`emZCX%]aQS^_WlXVc4cfiTV^iSd Z/\]aUliSd U_]a^d f/Q/] TVZ*(X qy"T u@W4tVW4^s-]ab/WW4RgW4^d2kW4Z]aW4^lc4X%ZRgW4^wTV^kU_]aW4RSUVq2}X%ZCiVq2Vs@]ab*X%]d UcbST`TVUd ZS\ !OXVc4cfiTV^iSd Z/\6]aTh]ab/WR/^_TVfCX%fCd2n2d ]emiSd U_]a^d f/Q/] TVqZ X;&sAX%Z*i^W4Wcfi] Z/\6]abSW'TVfSU_W4^_tX%] TVZu8d ]abR/^_TVf*X%fCd2n2d ]4e _ s`Ud ZCcfiW]ab/WW4RAW4^d2kW4Z`]aW4^-Z/T u0U&fATV]abj]ab/WU_W4Z/UTV^cOb/TVU_W4Z}ad2q Wq2s -X%ZCi]ab/W{TVfSU_W4^_tX%] TVZaaq] {:fFiTW4UW4gXVcfi] n eu0bCX%]&u9WuX%Z] qbSW5wxTSn2n Tu8d Z/\5]abSW4TV^_Wk U_bST uU]ab*X%f\b"a\eE @ n Y, g G8%)/|n 'jR G8%{/jY, R G8| n S,:1^@% )Y. C,MK-xQy ;//;'@ /;G'"`fi|ph %z ,aGM(} {:fz`0) 8ff &7 /mE1^ a5 W>/ h4 /V'7fi| | +j`{:fz`8fi ),S 48,9G6 zEJKMqWNOJ ~JFfiZ]abSWR/^_W4t1d TVQ/UhU_Wcfi] TVZCs"u9W>X%U_U_Q*kWil]abCX%]6]ab/W>d ZCwTV^kffX%] TVZ^_WcfiWd tVWi>uX%U6TSwB]ab/W}wTV^k ]ab/WXVcfi]aQCXVnLu@TV^n2id U~d ZVq}ZCwTV^kffX%] TVZkQ/U] fgW>d Z]abCd U~wTV^k]aT|X%RSRCn ecfiTVZCiSd ] TVZCd Z/\SqzCQ/]~d Z\VW4Z/W4^XVn2s*d ZCwTV^kffX%] TVZiTW4U;Z/TV]hXVn uX_e`UffcfiTSkWd Z~UQCcblZCd2cfiWR*XVcX%\VW4Uq&fiZ~]abCd U'U_Wcfi] TVZmu@WU_]aQCiVekTV^_W\VW4Z/W4^XVn)]e`RgW4UTSw7TVf/U_W4^t)X%] TVZ/UsDn WXVi/d Z/\ff]aT5\VW4Z/W4^XVn2d X%] TVZ/UTSwLcfiTVZCi/d ] TVZCd ZS\Sq:e\&Za\%-lr)" ^ " )l "l)7W4^_bCX%R/UE]ab/WUd2kRCn W4U]DRgTVU_Ud fCn W\VW4Z/W4^XVn2d X%] TVZd UE]aT8X%U_U_QCkW]abCX%]D]ab/W4^_W0d U-X*RCX%^_] ] TVZV & ^i^i^i 4 jTSwX%ZCi]ab/WX%\VW4Z]TVfSU_W4^_tVW4Uq & & ^i^i^i^gjAFj7s1ub/W4^W &^^ ^^Xgj Vq bCd Ujd U]aT~fgWZ]aW4^_R/^_W4]aWiX%UX%Z[TVf/U_W4^_tX%] TVZ]abCX%]n WXViU6]ab/W>X%\VW4Z]6]aTmfAWn2d W4tVW> u8d ]ab[R/^_TVfCX%fCd2n2d ]eE s&wTV^V^ i^i^i 6 l"qLr6c4cfiTV^i/d ZS\ff]a4W4A^_W4ecfiTVZCi/d ] TVZCd ZS\Ss\Sd tVW4ZX5i/d U_]a^d f/QS] TV^Z XgTVZ vsXE>u6& Xg>ufi & ^^ Ej;XEufiFjAui4W4A^_W4ecfiTVZ*i/d ] TVZCd Z/\d U>iW4/Z/Wi>TVZCn ed2w.C*q od2kR*n2d W4Uj]ab*X%]4XE>a+* V d2w.C* X%ZCiXE>a+*fi Vs]abSW4Z;*Xg}ufi**d U] X%VW4Z5]aTfAW{Vqn WX%^n e8TV^i/d ZCX%^ejcfiTVZCi/d ] TVZ*d Z/\5d U]ab/W-U_RgWc4d2XVnc4X%U_W8TSwW47^W4e}cfiTVZCi/d ] TVZCd Z/\Yub/W4^_WWC* YwxTV^;U_TSkW{@U_TSsDX%U6d U0U] X%ZCi/X%^i/su9WYiVWn2d fAW4^X%]aWn eQ/U_W]ab/W{UXVkWZ/TV] X%] TVZwxTV^QSRiSX%] Z/\hQ/Ud Z/\ W47^_W4e>cfiTVZCiSd ] TVZCd Z/\X%ZCi6TV^i/d ZCX%^_emcfiTVZCi/d ] TVZCd ZS\SqWffZST u uX%Z`]]aTiW4]aW4^kffd Z/W6u0b/W4ZQ/RBi/X%] Z/\d Z>]ab/W6ZCXVd tVW5U_RCXVcfiW6Q/Ud ZS\e W47^W4e[cfiTVZCiSd ] TVZ/p&& ^i^i^i ^EjSFj7Z/\[d UX%R/RS^_TVR/^d2X%]aWq bQ/UsEu@WlX%U_U_Q*kW5]abCX%]8]ab/WlX%\VW4Z] U6TVf/U_W4^t)X%] TVZ/U Z/TubCX_tVWY]abSWlwTV^kTSw& & ^ i^i^i ^ j j wxTV^&UTSkWRCX%^_] ] TVZoV & ^i^i^i4 j ;TSwFvq&urd 7W4^W4Z`]TVfSU_W4^_tX%] TVZ/U8kffX_esDd Z\VW4Z/pW4^XVn2s-QSU_Wi/d AW4^_W4Z]YR*X%^_] ] TVZ/Uq2 4Q/U_]mX%Uu9Wi/d2iwTV^j]ab/W[c4X%UW]abCX%]jTVf/U_W4^t)X%] TVZ/U>X%^_WlW4tVW4Z]aUaPWcfi] TVZVsS/^_U_];RCX%^X%\V^X%RSbCas/u@W TVZCcfiWX%\SXVd ZX%U_UQCkW]ab*X%];]ab/WX%\VW4Z] UTVf/U_W4^_tX%] TVZ/UffX%^_WXVc4cfiQ/p^X%]aWqjbCX%]jiVT`W4U ]abCX%]jkWX%Zzd Zo]ab/W5R/^W4U_W4Z`]jcfiTVZ`]aW4] WYUd2kRCn em^_WQ*d ^_W5]abCX%] s-cfiTVZCiSd ] TVZCXVn`7fibc edbgfb]fbz}zhcyz1s1t yz|quTVZkffX%1d Z/\]ab/WhTVf/U_W4^_tX%] TVZCsg]abSW RS^_TVfCX%fCd2n2d ]e~TSw"*^_WXVn2n ed UWC*0wTV^'V^ i^i^i 6 l"su@W{bCXatVWD^W>*CV^i^i^i6l"q bCX%]ffd UswTV^& ^i^i^i ^EjSFj7 C*GiaVbCd Uc4n WX%^n ej\VW4Z/W4^XVn2d 4W4U]abSW;^_W QCd ^_WkW4Z`]@TSwDXVc4cfiQ/^XVcfieY\Sd tVW4Z}d Z]ab/Whc4X%UW']ab*X%]@]abSW;TVf/UW4^_t)X%] TVZSUX%^_W{W4tVW4Z]aUqw"TV]&U_Q/^_RS^d Ud Z/\Sn es`]ab/W4^_W5d UX'\VW4ZSW4^XVn2d X%] TVZTSwS]ab/WW Lr6cfiTVZCiSd ] TVZ]abCX%]d U&Z/W4WiWi ]aT5\VQCX%^_pX%Z]aW4W]abCX%] W47^W4emcfiTVZCiSd ] TVZCd Z/\lc4X%Z~fAW5X%RSRCn2d Wih]aTff]ab/WZCXVd tVW{U_RCXVcfiWq&A|,| +W1^heL`%V & ^i^i^i4Fj7YGZ*/{A|,| +\b"a\:e L& ^ i^i^i^gj)ffR & ^^ gj E ) 8 | , n & & ^i^i^i^gj7FjE#Lp@VV^i^i^i 6"l W* lK+Sfffi6, )n +Oz %}9=D^ 8V &e1 ^W 8 ^86 & & ^i^i^i^gj7Fj7R>*fizu}D ^ W ^ W +*fi 8?[+*)D ^ W!hLX%^_]fffC"TSw b/W4TV^WkVq2d U6X%ZCXVn TV\VTVQ/U{]aTjRCX%^_]6ac49TSw b/W4TV^WkVq2Vq bSW4^_WX%^WXZ`Q*kfgW4^TSwEcfiTVZ*i/d ] TVZ/U-WQ*d t)XVn W4Z]&]aTfCC]abCX%]-u9WffcfiTVQCn2i8bCX_tVWU_] X%]aWi/sUd2kffd2n2X%^d ZU_R*d ^d ]9]aTh]ab/W6cfiTVZCi/d ] TVZSUZ b/W4TV^_Wk Vq2Vq w"TV]aW6]abCX%]]ab/W4U_WX%^_WhW4tVW4ZkTV^_W U_]a^d Z/\VW4Z]ffcfiTVZCi/d ] TVZ/U]ab*X%ZX%^W ^WQCd ^WiwTV^TV^i/d Z*X%^_emcfiTVZ*i/d ] TVZCd Z/\ff]aT5fAW5X%RSR/^_TVR/^d2X%]aWqDgXVkR*n W4UhVq2jX%ZCi6Sq2YXVn ^_WXVieUQ/\V\VW4U_]0]ab*X%]0]ab/W4^_WX%^_WZSTV]0]aTTmkffX%ZeZ/TVZ]a^d t1d2XVnUcfiW4ZCX%^d TVUub/W4^W6X%RSRCn egd ZS\ 4W4A^_W4emcfiTVZ*i/d ] TVZCd Z/\ff]aT6]abSW{ZCXVd tVWU_RCXVcfiWjd U8X%R/R/^_TVRS^d2X%]aWq&yBTu9W4tVW4^s/Q/U_]{X%U8wTV^]ab/WhTV^d \Sd ZCXV2n rh cfiTVZ*i/d ] TVZCsA]ab/W4^_WiTW41d U_]U_RgWc4d2XVn7Ud ]aQCX%] TVZ/Ujd Zlu0bCd2cb\VW4Z/W4^XVn2d 4WLr6 UX6^WXVn2d U_] d2c5X%U_UQCkR/] TVZCqlTV^ TV^i/d ZCX%^e Lr60s1u9WmkW4Z`] TVZSWi]ab/\W {:fzkWcObCX%ZCd Uk aPWcfip] TVZmSq2Vaq6TVW^ 4W4A^_W4eocfiTVZ*i/d ] TVZCd Z/\SsXUd2kffd2n2X%^ffkWcObCX%ZCd Uk kffX_e~fgWjX^_WXVn2d U_] d2c6kT1iWn*d ZmU_TSkWUd ]aQCX%] TVZSUub/W4^W~XVn2ngTVf/U_W4^_tX%] TVZ/U8^_WwxW4^]aT]ab/WffUXVkWffRCX%^_] ] TVZzV & ^ i^i^i4j7hTS;w vq W5Z/TuiW4Ucfi^d fAWX"UcfiW4ZCX%^d TffwTV^*UQCcbX9Ud ]aQCX%] TVZCq-PQSR/RgTVU_0W cfiTVZ/Ud U]aUTS]w ="TVf/U_W4^_tX%] TVZ/(U & ^ i^i^i 2u8d ]ab `* C* & & ^ i^i^i ^ C*(jSFjU_QCcOb>]abCX%]5XVn2!n C*(Vq w"T u8sD/laX%^_f*d ]a^X%^_e1{cfiTVZCi/d ] TVZCXVn"i/d U_p]a^d f/QS] TVZ/WU 18^ sg V^ i^i^i6 l"sgTVZ vq{fiZ]aQCd ] tVWn eHD,^ ffdU D,^ >u fiFaqq TVZ/Ud2iW4^]ab/WwTSn2n u8d ZS\kWcObCX%ZCd Ukff6/^_U_]X%Z[TVf/U_W4^_tX%] TVZ `*{d Ucb/TVUW4ZaXVc4cfiTV^i/d Z/\]aTmU_TSkWi/d U_]a^d fSQ/] TVJZ X TVZ]ab/W4ZX{UW4] UffcbSTVU_W4Zu ]abR/^TVfCX%fCd2n2d ]e *( ad2q Wq2sLXVc4cfiTV^i/d Z/\]aTj]ab/Wji/d U]a^d f/Q/] TVZd ZCiQCcfiWifff`e`*fia)/ZCXVn2n esDXu9TV^n2v>U8cOb/TVU_W4Z[XVc4cfiTV^i/d Z/\ff]a18^ q4w*]ab/WjTVf/U_W4^_tX%] TVJZ `*X%ZCiu@TV^n2X%^_Wj\VW4Z/W4^X%]aWi]abCd UhuX_esL]ab/W4Z]ab/W5\VW4ZSW4^XVn2d 4WLr6cfiTVZCi/d ] TVZb/TSn2iUs]ab*X%]8d UsDcfiTVZCiSd ] TVZCd Z/\d Z]abSWU_TVR/bCd U_] d2c4X%]aWi5U_RCXVcfiWjcfiTSd ZCc4d2iW4U0u ]ab 4W4A^_W4ecfiTVZ/pi/d ] TVZCd ZS\S:e *S {`%lV & ^i^i^i 4 j YGZ /j5BGj| , n /8|n7S67n ,% )qX \X Y`*fi # 6/'|VV^i^i^i^&7,;5,, )1^h ,ffeX 1^ z7X 'phfi /D6#D^h0}1^5 K- {6 fi,} x0y/z8*ffpogzu}8} & ^i^i^i 4jY[<a"ZW " ^ " lD^_TVRgTVUd ] TVZvVq2[iWkTVZ/U_]a^X%]aW4Uj]abCX%] s&W4tVW4Z]ab/TVQ/\Vb]ab/WX%ZCXVn TV\VQ/WmTSw@]ab/WLrh cfiTVZCiSd ] TVZW4R/^_W4U_U_Wijd Z b/W4TV^_WkVq2hd U9b*X%^i{]aThUX%] Uwxed Zj\VW4Z/W4^XVn2s1X%]n WX%U_]{d2w)]ab/W'U_W4]V & ^i^i^i4 j U-]ab/WUXVkW6wTV^XVn2n`TVf/UW4^_t)X%] TVZSUs]ab/W4ZwxTV^-W4tVW4^_eYU_QCcbU_W4]-TSw)TVfSU_W4^_tX%] TVZ/U-]ab/W4^_WW4gd U],|ph'R/^d TV^_UZ1^TVZ wTV^ubCd2cOb>]ab/W rhBpX%ZCXVn TV\VQ/W}UX%] U/Wi}wTV^jXVn2nDTVf/U_W4^_tX%] TVZ/Uq5r;U8u9WYU_b/Tu Z/W4] s9wTV^l0QSRiSX%] Z/\Ss]abCd U8d U;Z/T~n TVZ/\VW4^0]abSWYc4X%U_Wq`7&=fi " |qxq}t~"}s1|:e $Z"%l"k*\ p%n ^"\l ^`a"%r:n ^ %" l)bCX%]@X%fgTVQ/]9c4X%UW4UEub/W4^WB]ab/W;cfiTVZ/U]a^XVd Z`]aU&X%^_W9Z/TV]9d Z]abSWBU_RgWc4d2XVnwTV^k[ubSW4^_W04W4A^_W4e1 U@cfiTVZCiSd ] TVZ/pZ/\c4X%ZfgWhX%R/R*n2d Wi/7W4^_bCX%RSU@]abSW6kTVU_]cfiTSk5kTVZ}X%R/R/^_TSXVcOb>d Zj]ab*d Uc4X%U_Wffd U@]aThQ/UWhl09qAd tVW4ZXcfiTVZSU_]a^XVd Z]ubSW4^_W8XcfiTVZSU_]a^XVd Z]d U@Ud2kRCn eXU_W4]"TSwR/^TVfCX%fCd2n2d ]eli/d U_]a^d f/QS] TVZ/U Z]aQCd ] tVWn es`]ab/Wi/d U_]a^d f/Q/] TVZ/U UX%] Uwegd ZS\]ab/WcfiTVZ/U_]a^XVd Z`] hX%ZCiXffRS^d TV^iSd U_]a^d f/Q/] TV>Z XE TVZ vsC]ab/Wld2iWXmd U ]aTRCd2cOgsgXVkTVZ/\XVn2nAi/d U_]a^d fSQ/] TVZ/U@UX%] Uwegd Z/\h]ab/W6cfiTVZ/U_]a^XVd Z`] s]ab/W'TVZ/W']abCX%]{d UY%c4n TVU_W4U_] ']aT ]ab/WR/^d TV^i/d U_]a^d f/Q/] TVZCsAubSW4^_Wh]ab/W%c4n TVU_W4Z/W4U_UhTSw X ]a^XE UYkWX%U_Q/^WijQ/Ud Z/\^_Wn2X%] tVWffW4Z]a^_TVReq b/Wfi +o nh /fi7=, , xX /3XE u t-QCn2n fCXVcOl ~Wd f*n W4^sgVVVVCLT tVW4^ bSTSkffX%UsAVVVVd UiW4/ZSWiX%UXX hn TV\Xg}FXbSW0n TV\SX%^d ]abCkb/W4^_W0d UD] X%VW4Z8]aT]ab/WBfCX%UW0Vd2wTX L]ab/W4ZUX hn TV\gYX ha/XgoU'] X%VW4Z]aTYfgWYVq bCd U U^_WX%UTVZCX%fCn W Ud ZCcfiWjn2d2k - n TV\g*V Yd2wQvVq2 b/W^_Wn2X%] tVW6W4Z/p]a^_TVRezd U5/ZCd ]aWR/^_Ttgd2iVWil]abCX%]aX Um| ,+oS/))5u8d ]ab^W4U_RgWcfi]6]aTX s0d Z]abCX%]~d2wXE> Vs)]ab/W48Z X VsDwTV^8XVn2n4 vq&-]ab/W4^_u U_Ws1d ]8d U iVW4/Z/Wi ]aTfffgW5d Z//Z*d ]aWqbSWhcfiTVZ/U]a^XVd Z`]aU&u@WffcfiTVZ/Ud2iW4^-b/W4^WhX%^_W6XVn2ngc4n TVU_WiX%ZCicfiTVZtVW45U_W4]aU&TSw)RS^_TVfCX%fCd2n2d ]elkWX%U_QS^_W4UqfiZo]abCd Uc4X%U_Ws&d ]d U `ZST uZ[]abCX%] ]ab/W4^_Wd UXffQ/Z*d2Q/Wli/d U_]a^d f/QS] TVZ]ab*X%]8UX%] U/W4Uh]ab/WmcfiTVZSU_]a^XVd Z]aUX%ZCizkffd ZCd2k5d 4W4Uj]abSWl^Wn2X%] tVWW4Z`]a^_TVReq {d tVW4Z$XZ/TVZ/WkR/]ecfiTVZSU_]a^XVd Z^] X%Z*iX~R/^_TVf*X%fCd2n2d ]ei/d U_]a^d f/Q/] TVaZ XETVZ vs/n W4] XE>u 8BiW4Z/TV]aW]abSW8i/d U_]a^d fSQ/] TVZ6]abCX%];kffd Z*d2kffd 4W4U"^_Wn2X%] tVW;W4Z`]a^TVR`eu8d ]ab^W4U_RgWcfi]&]aNX q4w*]ab/WcfiTVZ/U_]a^XVd Z]aUhbCXatVW]ab/WwxTV^k ]aTmubCd2cOb W47^_W4e1 UQCn Wd UX%RSRCn2d2c4X%fCn WsL]abCX%]d Us-d2wL]ab/W4ebCX_tVW~]ab/W[wxTV^k /XX a+* ;*fifi V^ i^i^i 6 l"}wTV^5UTSkW~RCX%^] ] TVZvV & ^ i^i^i4FjAVs9]ab/W4Z]d UYu@Wn2n9`Z/TuZ]abCX%]Y]abSWi/d U_]a^d f/Q/] TVZ]abCX%]mk5d ZCd2kffd 4W4UW4Z`]a^TVR`e^Wn2X%] tVWl]aTX~R/^d TV<^ Xg UXE>u 6 & & ^ i^i^i ^ EjSjS0UW4Ws/Wq \Sq2s*u rd2XVcfiTVZCd U5DX%fgWn2n2sCVVVVaaq bQ/UsDl0oQ/RBi/X%] Z/\6\VW4ZSW4^_pXVn2d 4W4U 4W4A^_W4emcfiTVZ*i/d ] TVZCd Z/\maX%ZCihb/W4ZCcfiW5XVn UT5U_] X%ZCiSX%^icfiTVZCi/d ] TVZCd ZS\SaqT~U_]aQ*ie[l0Q/RBi/X%] Z/\}d Z}TVQ/^jw^XVkW4u@TV^_1s1u@WlX%U_U_Q*kW6]abCX%]]abSWffTVf/U_W4^_tX%] TVZ/UX%^_WffZ/TuX%^_fCd ]a^X%^_elc4n TVU_WicfiTVZtVW4>cfiTVZ/U_]a^XVd Z]aUTVZ]ab/WR/^TVfCX%fCd2n2d ]ekWX%U_QS^_Wq*r;\SXVd Z*su9WffX%U_U_Q*kW]abCX%]&]ab/WTVf/U_W4^t)X%] TVZ/UX%^_WXVc4cfiQS^X%]aWmd Zo]abCX%] s@cfiTVZCi/d ] TVZCXVnDTVZkffX%1d Z/\~]abSWffTVf/U_W4^_tX%] TVZCsC]ab/WcfiTVZSU_]a^XVd Z]aUb/TSn2i/qTV^ZST u8sEu9WlwxT1cfiQ/U{TVZo]ab/WffUd2kR*n W4U_]{RgTVU_Ud fCn W~c4X%UW5]abCX%]c4X%Z/Z/TV]{fgW6bCX%ZCiSn Wif`e W47^_W4eQ/RBi/X%] Z/\SqZ]abCd U5c4X%UWsBcfiTVZ/U_]a^XVd Z]aUYTVf/UW4^_t)X%] TVZSU-U_] d2n2nAbCX_tVW6]ab/WwxTV^k & & ^ i^i^i^ gj7Fj7sgf/Q/]Z/Tu]ab/W+*fi U6iT5Z/TV]'bCX_tVW8]aTwxTV^k X{RCX%^_] ] TVZ]ab/W4eokffX_eT tVW4^n2X%RX%Z*i/%TV^;ZSTV]hcfiTtVW4W^ ;X%ZCi]ab/W * iTZ/TV]bCX_tVW6]aTU_QCk]aT>Vq5PQ*cbX%ZTVf/U_W4^t)X%] TVZd U5XVc4cfiQ/^X%]aWd2w"d ]UX%] U_SW4UYaVasgQ/U_]ffX%UfgWwxTV^WqW~c4X%ZmZ/Tu X%U~]ab/W UXVkWQSW4U_] TVZ/U]abCX%]u@WX%U_VWijfgWwTV^_WX%fATVQS];TV^iSd ZCX%^_e}cfiTVZCi/d ] TVZ*d Z/\X%ZCW47^W4emcfiTVZCiSd ] TVZCd Z/\d Z]abSW{ZCXVd tVW{U_R*XVcfiWqVq-fiU&]ab/W4^_WffX%Z}XVn ]aW4^_Z*X%] tVWYcbCX%^XVcfi]aW4^d X%] TVZTSw/]ab/W5cfiTVZCi/d ] TVZSU&Q/ZCiW4^@ubCd2cOb};mQ/RBi/X%] Z/\cfiTSd Z*c4d2iW4U{u8d ]abcfiTVZCi/d ] TVZ*d Z/\}d Zm]ab/WhU_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiW bCX%]ffd UsX%^_Wh]ab/W4^_WX%ZCXVn TV\VQSW4UTSw bSW4TV^_WkVq2ffX%ZCi b/W4TV^_WkVq2YwxTV^ ;Q/RBi/X%] Z/\SVq-r'^_W ]ab/W4^_WcfiTSkfCd Z*X%] TVZ/UTSw!X%ZCiwxTV^'ubCd2cOb[d ]6d U'Z/TV];W4tVW4ZmRgTVU_Ud f*n W]abCX%]6;c4X%ZcfiTSd Z*c4d2iWu8d ]abcfiTVZCi/d ] TVZCd Z/\ld Z]ab/W{U_TVRSbCd U_] d2c4X%]aWi6URCXVcfiW]ab~^_W4\SX%^i6]aTQSW4U_] TVZVsDd ] UWX%U_e]aTffR/^_Ttgd2iVWYXffcfiTVQ/Z]aW4^_W41XVkRCn WUb/T u8d ZS\6]abCX%]]ab/W4^_Wjd UZ/TTVftgd TVQ/U&X%ZCXVn TV\VQ/W9]aT bSW4TV^_WkVq2'wxTV^-;"q b/W4^_W;d U-X0cfiTVZ/U_]a^XVd Z`]>U_Q*cb]ab*X%]D]ab/W'cfiTVZ*i/d ] TVZ8TSw`77`fibc edbgfb]fbz}zhcyz1s1t yz|quRCX%^_]aXVDTSw b/W4TV^_Wk$Vq20b/TSn2iUwxTV^l0Q/RBi/X%] Z/\{u0b/W4^_WX%U"R*X%^_]'fC9iTW4U9Z/TV]9b/TSn2i/q-W'TSkffd ]]ab/W8iVW4] XVd2n U"b/W4^_Wq2h{wAcfiTVQ/^_UWsSd ];d U9RATVUUd fCn W&]abCX%]B]abSW4^_W8X%^_WUTSkWQ*d ]aW8i/d AW4^_W4Z`]'cfiTVZCi/d ] TVZ/U9]abCX%]cb*X%^XVcfi]aW4^d 4Wub/W4Zl0|Q/RBi/X%] Z/\[cfiTSd ZCc4d2iW4U6u8d ]abcfiTVZCi/d ] TVZCd ZS\[d Z[]ab/WYU_TVR/bCd U_] d2c4X%]aWilURCXVcfiWqy"T u@W4tVW4^s/W4tVW4Z[d2w7]ab/W4eW4gd U_] sU_QCcb[cfiTVZCi/d ] TVZ/UhkffXaefgWQ/ZCd Z]aW4^_W4U_] ZS\d Z]abCX%];]ab/W4emk5XaebCX%^iSn eW4tVW4^X%R/RCn eqfiZCiW4Wi/sX%UXRCX%^_] d2XVn;X%Z/U_u@W4^Y]aTzQSW4U_] TVZvVs@u@WlZ/T u Z`]a^_T1iQCcfiW[XtVW4^_eUd2kRCn WU_W4]a] Z/\d Z~u0bCd2cb[;}QSRiSX%] Z/\6Z/WcfiW4U_UX%^d2n eon WXViU']aTlX^W4U_QCn ] iSd 7W4^_W4Z] w^_TSk cfiTVZCi/d ] TVZCd Z/\md Z]ab/W{UTVR/bCd U_] d2c4X%]aWiffU_RCXVcfiWq~/W4]' & X%ZCi5 + fAW0]u@T8U_QSf/U_W4]aUBTSwU U_Q*cb5]ab*X%]Q & & + sg + + & sg . & k +%X ZCi 2 & & + *X%^_W'XVn2n Z/TVZ/WkR/]eqLTVZ/Ud2iW4^@X'cfiTVZ/U_]a^XVd Z]*TSw]ab/WwTV^km & & ^ + +ub/W4^W. & ^ + X%^W&fATV]ab~d ZlaV4Vaq*W8d Z`tVW4U] \SX%]aW;ub*X%]bCX%R/RgW4Z/U&d2wu@W&Q/U_W{l0QSRiSX%] Z/\TVZ[qP/d ZCcfiWff & X%ZCi + TtVW4^n2X%RX%Z*ijiVT ZSTV]cfiT tVW4^]ab/WURCXVcfiWs1d Z\VW4Z/W4^XVnU4W4A^_W4emcfiTVZCi/d ] TVZCd Z/\~c4X%ZSZ/TV]fgWX%RSRCn2d Wij]aTQ/RBi/X%]aWhTVZq b/W4^_WX%^_WhU_TSkW6Ud ]aQ*X%] TVZ/U{ub/W4^Ws*iVW4U_RCd ]aW6]ab/WhT tVW4^n2X%RCsKW47^_W4ecfiTVZCi/d ] TVZ*d Z/\jc4X%ZYW4U_U_W4Z] d2XVn2n e5fgWX%R/RCn2d Wi/qCW'UXaeh]abCX%]9TVf/U_W4^_tX%] TVZ< & & ^ + + U`L+o sXVwx]aW4^5l0Q/RBi/X%] ZS\5TVZTVZ/W TSw1]ab/WcfiTVZ/U]a^XVd Z`]aWU & & TV=^ + + sS]abSW TV]abSW4^5cfiTVZ/U_]a^XVd Z]b/TSn2iU6X%U'u9Wn2n2q bCX%]hd UU 4W4A^_W4epn2d VWu ]ab^_W4URAWcfi]']a<X ;d2wAWd ]ab/W4fi^ X + 6 & & +TV\^ Xg}a & 6 + + & qoPVQ/R/RgTVU_Wff]abCX%\] XE>a + 6 & & + ]abSW4Zd ]d UhWX%U_e}]aTmU_bST u]abCX%]XE>u 6 & & XE>u 6 & & ^ + + aqfiZ`]aQ*d ] tVWn es d2w&]ab/W%c4n TVU_W4U_] i/d U]a^d f/Q/] TVZX ]aTJXE ]abCX%]UX%] U_/W4U^X & & XVn U_TUX%] U_SW4U X + + s]abSW4ZxX U*]abSWc4n TVU_W4U]0i/d U_]a^d f/QS] TVZ8]aTfiXE$]ab*X%]CUX%] U/W4U]ab/WcfiTVZ/U_]a^XVd Z]& & ^ + + q.w"TV]aW8]ab*X%]hl0oQ/RBi/X%] Z/\5TVZ* U;W QCd tXVn W4Z`]']aTW47^_W4e}cfiTVZ*i/d ] TVZCd Z/\jTVZ*4a Cau aq bQ/Us/d2w_d UQ4W4A^_W4epn2d VWs]abSW4Z5Q/RBi/X%] Z/\u8d ]ab4d UBW QCd tXVn W4Z`]9]aTqW47^_W4eQ/RBi/X%] Z/\Sqz Gm%S/, / & + Gm| n S,ff:e' @;* & & ^C* + + R V4ZD^e,o% ) ,) * 1^ & ,1^ + H9/1^,} D^W hHv0 ?$ E HD^ 1^u** ,"/~KD^ * ff=phSLGD^ * 96f & Q + {/fi fiffB6//-|n 1^ 1^8u`*,\ Z, V4\b"a\`*TV^ /Wi} & X%ZCi} + sDu@Wlc4X%Zd2iW4Z] d2wxeX%ZTVf/U_W4^_tX%] TVZ & & ^ + + u8d ]ab}]ab/WYRCXVd ^ju & ^ + {aV4V + qBZ*iW4^ffTVQ/^mcfiTVZCi/d ] TVZ/UYTVZ & X%ZCi + s9]ab/WU_W4]ffTSwXVn2nW47^_W4epn2d VWlTVf/U_W4^_tX%] TVZ/Uld UXU_Q/fSU_W4]TSw9u~/W4fgW4U_\VQ/WkWX%U_QS^_W6TSwD]ab*d U{U_W4] q bQ/Usg]ab/W6UW4]TSwDTVf/UW4^_t)X%] TVZSUjwTV^ubCd2cb;cfiTVZCi/d ] TVZ*d Z/\YcfiTV^_^_W4URATVZ*iU]aTYcfiTVZCi/d ] TVZ*d Z/\Yd Z6]ab/WUTVR/bCd U_] d2c4X%]aWi{U_RCXVcfiW8d U;Xu~/W4fgW4U_\VQ/WBkWX%UQ/^_W&UW4]'d Z5]abSW0U_RCXVcfiW;TSw`RATVUUd fCn WTVf/UW4^_t)X%] TVZSU[q wBTV]aW'b/Tu9W4tVW4^s`]abCX%]9]abCd U"UW4]'iW4RgW4ZCiU"TVZ5]ab/W;R/^d TV^X tVW4^ vqr^W4U_QCn ]jUd2kffd2n2X%^]aT b/W4TV^_Wk Vq2uX%UR/^T tVWiof`ePWd2iW4ZCwWn2i|aVVVVaX%Z*icfiTVZSUd2iW4^X%fCn e\VW4Z/W4^XVn2d 4WilferXau8d2i[aVVVVaaqPWd2iVW4ZCwxWn2iU_b/TuU8]ab*X%] s*Q/ZCiW4^8tVW4^_e>u@WX%|cfiTVZCi/d ] TVZ/Us&;Q/RBi/X%] Z/\c4X%Z/Z/TV]{cfiTSd ZCc4d2iVWu8d ]abU_TVR/bCd U] d2c4X%]aWicfiTVZCi/d ] TVZCd ZS\~d2w]ab/WTVf/U_W4^t)X%] TVZ/U&b*XatVW]ab/WffwTV^k]ab/W|cfiTVZ*i/d ] TVZCXVn-R/^TVfCX%fCd2n2d ]ezTSw \Sd tVW4sZU *aX%U[d U]ab/Wc4X%U_W|d Z]ab/W 4QCiVeCW4Z/aXVkffd ZR/^_TVf*n Wkffaq b/W4TV^_Wk Vq2U_bST uU6]abCX%]6]ab*d Ud Ud2kRgTVU_Ud f*n WjW4tVW4ZwxTV^6TVf/UW4^_t)X%] TVZSUffTSwB]abSW}kQCcObUd2kR*n W4^}wxTV^k & & ^ + + s&QSZCn W4U_Uu@Wc4X%Z^_WiQCcfiWl]ab/WR/^_TVf*n Wk ]aW47^W4ecfiTVZCi/d ] TVZ*d Z/\ad ZubCd2cOboc4X%U_W b/W4TV^_WkVq25X%R/RCn2d W4Uaq`7fi " |qxq}t~"}s1|8n,`W4tVW4Z]W4tVW4Z]RS^_TVfCX%fCd2n2d ]etVWcfi]aTV^RS^_TVfCX%fCd2n2d ]etVWcfi]aTV^CG{| , n /[RCXVd ^_u8d UW5i/d U_TSd Z`]%X ^_fCd ]a^X%^eU_W4]TSwW4tVW4Z`]aUR/^_TVfCX%f*d2n2d ] W4UTSwRCX%^_] ] TVZR/^_TVfCX%f*d2n2d ] W4UTSw]u@TtVW4^n2X%RSRCd Z/\ffU_W4]aUp;7+ 47+,+h`ff%)ZCXVd tVWcfiTVZCi/d ] TVZ*d Z/\ZCXVd tVWcfiTVZCi/d ] TVZ*d Z/\4W4A^_W4ecfiTVZCi/d ] TVZ*d Z/\l0m/ ,7,S//XVn uX_e`U6u 1^_TVRgTVUd ] TVZSq2Vrhb/TSn2iUb/W4TV^_Wk Vq2V\VW4Z/W4^XVn2d X%] TVZ TSwLrhb/TSn2iU8 bSW4TV^_Wk Vq2Vd2w/fgTV]abTVf/U_W4^t)X%] TVZ/mU 4W4A^_W4epn2d VWj b/W4TV^WkVq2VLd \VQ/^_WYVLTVZCi/d ] TVZ/U;Q/ZCiW4^0ubCd2cbQ/RBi/X%] Z/\~d Z]ab/W{ZCXVd tVW8U_RCXVcfiWjcfiTSd Z*c4d2iW4U;u ]abcfiTVZCi/d ] TVZ*d Z/\Z]ab/W{U_TVRSbCd U_] d2c4X%]aWi6URCXVcfiWq9G6NMfiNJW&bCX_tVW&U_]aQCi/d Wi']ab/W{c4d ^cfiQ*kU_] X%ZCcfiW4UQ/Z*iW4^LubCd2cbhTV^iSd ZCX%^_ejcfiTVZ*i/d ] TVZCd Z/\Ss]W47^W4ejcfiTVZCi/d ] TVZ*d Z/\SsX%ZCiY;~QSRiSX%] Z/\Yd ZlX0ZCXVd tVW;U_RCXVcfiW6c4X%ZYfgWQ/U_] /WiSsVub/W4^WQSU_] /Wi/8wxTV^@Q/U'kWX%Z/U6%X%\V^_W4W4Uu8d ]abvcfiTVZCi/d ] TVZCd Z/\d Z]ab/WUTVR/bCd U_] d2c4X%]aWi}U_RCXVcfiWVq b/W}kffXVd ZvkW4U_UX%\VWTSw9]abCd UffRCX%RgW4^d UY]abCX%]W41cfiW4R/] wTV^ QCd ]aWURAWc4d2XVnCc4X%U_W4Us/]ab/W]ab/^W4WYkW4]ab/T1iU c4X%ZSZ/TV]0fgWQ/U_] /Wi/q'Ld \VQ/^_WU_QCkffk5X%^d 4W4U]ab/WYk5XVd Zod Z/Ud \Vb]aU0TSwS]abCd URCX%RgW4^8d ZkTV^_W5iW4] XVd2n2qr'Uu@WYkW4Z] TVZ/Wid Z]ab/W5d Z]a^_T1iQCcfi] TVZCs]ab/W5d2iWXTSw*cfiTSkRCX%^d ZS\~X%ZQ/RBi/X%]aW^_QCn Wffd ZoX~Z*XVd tVWU_RCXVcfiW~u8d ]abvcfiTVZCi/d ] TVZCd Z/\d Z XU_TVR/b*d U_] d2c4X%]aWioU_RCXVcfiWd UYZ/TV]YZ/W4u88d ]mX%R/RgWX%^_Umd Z]ab/W Lr6n2d ]aW4^X%]aQ/^WX%ZCij]ab/W~l0n2d ]aW4^X%]aQ/^_WaX%U8u9Wn2nX%UYd ZR*X%RAW4^UU_QCcbX%Ujay;XVn RgW4^_Z| Q/]a] n Ws9VVVVX%ZCi u rXau8d2ihr'd2cOVW4esSVVVVaaqLfiZjXViSi/d ] TVZh]aT;f/^d Z/\Sd Z/\0]abSW4U_W9]u9T'U_]a^X%Z*iUCTSw^W4U_WX%^cbh]aTV\VW4]ab/W4^sTVQ/^@T uZ>cfiTVZ]a^d f/Q/] TVZSUX%^W;]ab/W6wTSn2n u8d Z/\S0aXVEu@W'U_b/T uz]abCX%]@]ab/W Lr6wx^XVkW4u9TV^_lc4X%ZjfgW;Q/U_WiX%UXff\VW4Z/W4^XVnD]aTTSnD]aTc4n2X%^d2we|kffX%ZeTSw*]ab/Wju9Wn2n pZ/T u0ZR*X%^XViT W4U TSwcfiTVZCi/d ] TVZCXVnER/^_TVf*X%fCd2n2d ]e1fC*u@W\Sd tVWjX'\VW4Z/W4^XVnDcbCX%^XVcfi]aW4^d X%] TVZmTSw Lr6d Z]aW4^kUTSw*X'fCd ZCX%^_ept)XVn QSWikffX%]a^d 1s)U_b/Tu8d Z/\]abCX%]d ZkffX%Z`e[^_WXVn2d U] d2c6UcfiW4ZCX%^d TVUs9]ab/W Lr6 cfiTVZCiSd ] TVZ //b/TSn2i b/W4TV^_WkSq Saac4u@WiW4/ZSWX}kWcbCX%Z*d Uk {:f)z ]abCX%]ff\VW4ZSW4^X%]aW4UlXVn2nX%Z*imTVZCn ei/d U_]a^d f/QS] TVZ/U6UX%] Uwegd ZS\ Lr6b/W4TV^WkSq2Va1ai/Du9W;U_b/Tu|]abCX%]9]ab/W rhzcfiTVZCi/d ] TVZbCX%UX&ZCX%]aQS^XVnVW4]aW4Z/Ud TVZ]aTc4X%U_W4U9u0b/W4^_W4W4A^_W4evcfiTVZCi/d ] TVZCd Z/\c4X%ZfgWX%R/RCn2d Wi bSW4TV^_Wk Vq2Va5X%ZCizW u@WU_b/Tu]ab*X%]Z/LrhBpn2d VWcfiTVZCi/d ] TVZc4X%Zb/TSn2id Z\VW4Z/W4^XVnDwxTV^ c4X%U_W4Uub/W4^W'TVZ*n eml0|aX%ZCihZ/TV] W47^_W4e1*Q/RBi/X%] Z/\c4X%ZfgWX%R/RCn2d Wi~ b/W4TV^_Wk Vq2Vaq-QS^9^_W4UQCn ]aU9UQ/\V\VW4U_]-]abCX%]@u9TV^_1d Z/\d ZY]abSW;ZCXVd tVWU_R*XVcfiWhd U-^X%]ab/W4^@R/^_TVf*n WkffX%] d2c4q-Zj]ab/W'TV]ab/W4^bCX%ZCiSsX%Uu@W-TVf/U_W4^_tVWi6d Z6]abSWd Z]a^_T1iQCcfi] TVZCsu@TV^_1d Z/\ffd Z6]ab/W@U_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiWW4tVW4Z~X%U_U_Q*kffd Z/\]8c4X%ZfAWffcfiTVZ/U]a^_QCcfi]aWi/d U;R/^_TVfCn Wk5X%] d2c-]aT`TSq0PTffubCX%]X%^W{]ab/W5XVn ]aW4^_ZCX%] tVW4UTV^~TVZ/W]abCd Z/\Ss ]}d Uu9TV^]abTVf/U_W4^_t1d Z/\[]abCX%]}l0vQ/RBi/X%] ZS\d UZSTV]}XVn uX_e`UUTfCXVi/q fiZkffX%ZeU_QCc4cfiW4U_UwxQCn/R/^XVcfi] d2c4XVn*X%R/RCn2d2c4X%] TVZ/UsS]abSW%cfiTVZ/U_]a^XVd Z`] 8TVZlubCd2cbl]aT5QSRiSX%]aWYd U;TSwA]abSWjwTV^kjW l"s1ub/W4^_W *'d U]ab/W6]ab>TVQ/] cfiTSkWYTSw9Xh^X%ZCiTSktX%^d2X%fCn =W TVZj & *r & * wxTV^{UTSkWn2X%^_\V=vq b*X%]hd Us7u@W8TVfSU_W4^_tVWX%ZlWkRCd ^d2c4XVnLXatVW4^X%\VW6TSw1TVQ/] cfiTSkW4UTSYw [qfiZmU_QCcbXjc4X%U_WsA]ab/W;i/d U_]a^d f/Q/] TVZd Ul%c4n TVU_Wad Z]ab/WX%R/R/^_TVR/^d2X%]aW~i/d U_] X%ZCcfiW~kWX%UQ/^_W9]aT]ab/W~i/d U_]a^d f/Q/] TVZmu9W~X%^^d tVW`77afibc edbgfb]fbz}zhcyz1s1t yz|quX%]'f`eUTVR/bCd U_] d2c4X%]aWicfiTVZCiSd ] TVZCd Z/\Sq bCX%]6d UsCd2wK1^ D^8u[ asD^, D^ ujj iVW4Z/TV]aW4U{]ab/Wml1pwTSn2i5RS^_TgiVQCcfi];TSw9X R/^_TVfCX%f*d2n2d ]eiSd U_]a^d f/Q/] TVZ8sA]ab/W4Zj & *r & * > aasX%ZC<j8 u 1,^ j tX%ZXVkRAW4ZSb/TVQ/]ff LTtVW4^s-VVVVwTV^U_]Q vlc4d W4Z`] n en2X%^_\VW l"s1u@WffbCXatVWj]abCX%]Yu D,^-^ QS Z`uXVn2i/s1VVVVEPV`e^kUsgVVVV!vffZSgsgVVVVaq b`QSUsgd ZU_QCcbc4X%U_W4U l0aXVn2kTVU_] 0cfiTSd ZCc4d2iVW4Uu8d ]ab>UTVR/bCd U_] d2c4X%]aWicfiTVZCi/d ] TVZCd ZS\oXVwx]aW4^XVn2n2q~aPVW4Wu rXau8d2i/s@VVVV wxTV^Xi/d UcfiQ/UUd TVZ>TSwEb/T u]abCd U^_W4U_Q*n ]c4X%Z~fgW^_WcfiTVZCc4d2n Wiffu8d ]ab]ab/W{^W4U_QCn ]aUTSw*PWcfi] TVZVq2CQ/]Lub/W4Zh]abCd UURAWc4d2XVnUd ]aQ*X%] TVZiTW4U*ZSTV]&X%RSRCn esd ]0d Uu9TV^]abX%Ugd Z/\u0b/W4]ab/W4^L]ab/W4^_W-W41d U_]aU0X%Z%X R/R/^TSXVcbowTV^Q/RBi/X%] Z/\d Z]ab/WZCXVd tVWU_RCXVcfiW{]ab*X%]c4X%Z~fgWWX%Ud2n e>X%R/RCn2d Wid ZR/^XVcfi] d2c4XVnUd ]aQCX%] TVZSUseVW4];n WXViU"]aTq _sSd ZffU_TSkWwTV^kffXVn2n e R/^_Tt)X%fCn W&UW4Z/U_WsQ/RBi/X%]aWihi/d U]a^d f/Q/] TVZ/U]ab*X%Z6]ab/WkW4]ab/T1iUu@WBbCXatVW'cfiTVZ/Ud2iW4^_WiS rtVW4^ehd Z]aW4^_W4U_] Z/\hc4X%ZCi/d2i/X%]aWs TSw]aW4Zjd ZCwTV^kffXVn2n effX%R/RCn2d Wi&f`e'b`Q*kffX%Z5X%\VW4Z`]aUsU{]aTUd2kRCn e /5]ab/WX_t)XVd2n2X%fCn WYW4`]a^Xd ZCwTV^kffX%] TVZCq8fi]]aQ/^_Z/UTVQ/]]abCX%]]abSW4^_W~X%^_W6Ud ]aQCX%] TVZSUub/W4^W{]abCd U0QSRiSX%]aW{^_QCn W{fgW4bCX_tVW4U;fgW4]a]aW4^sCd Z[XR/^_Wc4d U_W8U_W4Z/U_Ws/]abCX%Z]abSW{]ab/^_W4WkW4]ab/T1iU0u@WbCX_tVWcfiTVZ/Ud2iVW4^_Wi/q b*d U&u8d2n2n)fgWW4`R*n TV^_Wid ZowQ/]aQ/^_Wu@TV^_1qr Z/TV]ab/W4^Yd UU_Q/W6]abCX%]Z/W4WiVUYwxQS^_]ab/W4^W4RCn TV^X%] TVZd U]ab/W6U_Q/f/] n W~iSd U_] ZCcfi] TVZ>fgW4]u@W4W4Z u@WX%1'X%ZCiU_]a^_TVZ/\S0Lrh;subCd2cObuX%UCfS^_TVQ/\Vb]D]aT0TVQ/^-X%]a]aW4Z`] TVZhfeffX%ZCw^_WiX%W4\VW4^q b/W"]aW4^kffd Z/TSn TV\VeU~iQ/Wj]aXVkW4U~TVfCd Z/UsLub/TlTVZCn e}tVW4^e}^_WcfiW4Z] n ei/d UcfiT tVW4^Wim]ab/W>i/d U_] ZCcfi] TVZ*qfi] ]aQS^_Z/UhTVQ/]]abCX%]]ab/W6ZSTV] TVZ>TSHw rhu9W6Q/UWd Z}]abCd URCX%RgW4^5 u9WX%1rh;d UUn2d \Vb`] n ei/d AW4^_W4Z]5wx^TSk]ab/WU_]a^_TVZS\S/tVW4^_Ud TVZlTS2w rh|Q/U_Wi5fe}{d2n2n7W4]hXVn2q*aVVVVaq b/W4et1d W4u]ab/W Lr6vX%U_U_QCkRS] TVZ[ QCd ]aWn2d ]aW4^XVn2n2n eX%UX%Z|X%U_U_QCkR/] TVZ|X%fgTVQ/]5X}%cfiTSX%^_U_W4Z*d Z/\~R/^TgcfiW4U_UVqhrhiVQ/U]aWij]aTTVQ/^{Z/TV] X%] TVZ*sD]ab/W4eu^d ]aWja{d2n2nW4]8XVn2q2sCVVVVsRCX%\VWjVVVah%Ld ^_U_] n e]abSW^X%ZCiTSkvtX%^d2X%fCnW W TSw*d Z`]aW4^_W4U] U^_WXVn2d 4WiSU_WcfiTVZCiSn es1X5/,a7)+5 fi SLR/^TgcfiW4U_UQSU_QCXVn2n elX%UU_Tgc4d2X%]aWi6u8d ]ab}wxWX%]aQS^_W4U&TSwCkWX%U_Q/^_WkW4Z]TV^TVf/U_W4^_tX%] TVZCXVn9^_W4U_]a^d2cfi] TVZ/Us&^X%]ab/W4^]abCX%Zz]ab/WUc4d W4Z`] CcYR/bSW4Z/TSkW4Z/TVZQ/ZCiW4^YU_]aQCied ]aUWn2waas\Sd tVW4Z~]abSWt)XVn Q/.W ] X%VW4Z~fe W}s^_W4RCn2XVcfiW4U0]ab*d U&t)XVn Q/Wf`emXU_W4]UQCcb]abCX%] >8q2 bQ/Usgd Z]ab/Wd ^Dt1d W4u8s ]ab/W;i/d U]a^d f/Q/] TVZ{TVZ U9cfiTVZSU_]a^_QCcfi]aWi8wx^_TSkXi/d U_]a^d fSQ/] TVZTVqZ X%ZCiXacfiTVZ*cfiW4R/]aQCXVn2n eQ/Z/^Wn2X%]aWi/DUW4]@TSwEcfiTVZCi/d ] TVZCXVngi/d U_]a^d f/Q/] TVZ/.U 1^ W hasTVZ/WhwTV^-WXVcb evq bCd Ud2kRCn2d W4UY]abCX%] D^ W ffd UmXu@Wn2n piW4/Z/WimZQCk0fgW4^6W4tVW4Zd2#w 1^ W h Vq{d2n2n2sDt)X%Z|iVW4^ ~EXVX%ZCs"X%ZCi>TVf*d Z/U5aVVVV]abSW4ZL K-S6]abSW Lr6 cfiTVZCi/d ] TVZzX%Um%wxTV^jXVn2nB1^WZ Vq bCd Umd UjQ/U_]YRCX%^_]mai/{TSw b/W4TV^_Wk Vq2VsUcfiTVZ/U_] X%Z]dubCd2cObZ/T uvbCX%U]aTbSTSn2i5W4tVW4Zd2!w 1^ h Vq b`QSUs7]ab/WhU_W4]TSw"i/d U_]a^d f/Q/] TVZ/UUX%] Uwe1d Z/\U_]a^_TVZS\ Lr6 U6XUQ/f/U_W4];TSwg]ab/W U_W4];UX%] Uwxe1d Z/\u@WX%Lr60qCTVfCd Z/UffX%ZCX%W4\VW4^Ub/T u]abCX%]]ab/WZCc4n Q/Ud TVZc4X%ZfgWU_]a^d2cfi]lX%ZCim]ab*X%]ff]abCd Ulc4X%Z|bCXatVWU_Q/f/U] X%Z`] d2XVncfiTVZSU_WQSW4ZCcfiW4Uq b/W4^_WwTV^_Ws"TVQ/^cb*X%^XVcfi]aW4^d X%] TVZ/UjTS0w rhd ZPVWcfi] TVZX%R/RCn e>TVZ*n eo]aTu@WX%rh;s@X%Z*iwxQ/^]ab/W4^h^_W4U_WX%^cObd UZ/W4WiWi6]aTffU_W4W{]ab/WW4`]aW4Z]]aT5u0bCd2cb]abSW4emXVn U_TmX%R/RCn ej]aTffU_]a^_TVZS\ Lrh;q-QS^Yi/d UcfiQ/UUd TVZ}b/W4^WffbCX%UwxT1cfiQ/U_Wi>cfiTSkRCn W4]aWn e>TVZ]ab/WffR/^_TVf*X%fCd2n2d U_] d2c5c4X%UWqyBTu9W4tVW4^s*]abSW4U_WQ/W4U_] TVZ/UhXVn U_TlkffX%VW8U_W4Z/U_WjwxTV^;TV]ab/W4^0^_W4RS^_W4U_W4Z] X%] TVZ/U;TSw7Q/Z*cfiW4^_] XVd Z`]eq9Z]aW4^_W4U_] Z/\Sn es*^d WiSkffX%ZX%ZCily;XVn RgW4^_ZaVVVV@U_bST uv]abCX%]']abCX%]r{ffpU]e1n WfffgWn2d Wwg^_W4t1d Ud TVZrhn2cOb/TVQ/^_C^ TV Z*s*QX%^iW4ZCwTV^_Us*lX%gd Z/UTVZCsLVVVVc4X%ZlfgW8^_W4R/^_W4U_W4Z]aWimd Zm]aW4^kUTSwBcfiTVZCi/d ] TVZ*d Z/\YQSUd Z/\XY QCXVn2d ] X%] tVW6^_W4R/^W4U_W4Z/p] X%] TVZTSwQ/ZCcfiW4^] XVd Z`]elc4XVn2n WiX'7,o +p ,)fi4]aTiT]ab*d Us`]ab/W0RCn2X%QSUd fCd2n2d ]emkWX%U_Q/^W k0Q/U_]UX%] Uwe[]ab/WoX%ZCXVn TV\VQSW~TSw b/W4TV^_Wk Vq2VaXVas-U_T>]abCX%]5TVf/U_W4^_tX%] TVZ/Uc4X%^_^_eZ/T|kTV^_Wd ZCwxTV^kffX%] TVZ]abCX%Z[]ab/WwXVcfi]h]abCX%] ]ab/W4ezX%^_WY]a^Q/Wq wBLr6pn2d VW>cfiTVZCi/d ] TVZd Uh\Sd tVW4Z]aTm\VQCX%^X%Z]aW4W]abCX%]h]abCd UcfiTVZCi/d ] TVZbSTSn2iU5wxTV^RCn2X%QSUd fCd2n2d ]ekWX%U_QS^_W4U]ab/TVQ/\VbCqfi]u9TVQ*n2i5fgWd Z]aW4^_W4U_] Z/\]aTZ/T u d2wD]ab/W4^_WX%^_W5X%ZCXVn TV\VQSW4U0]aLr6wTV^&TV]ab/W4^^_W4R/^W4U_W4Z`] X%] TVZSUTSw/QSZCcfiW4^_] XVd Z]es)U_Q*cb}X%U;` +p ,)fiu r9Q/fgTSd U8D^XViWsDVVVVBTVm^! )/ / aPVbCXVwxW4^s1VVVVaq`77fi " |qxq}t~"}s1|J;\/Kfir R/^Wn2d2kffd ZCX%^_etVW4^_Ud TVZlTSwA]abCd U;RCX%RgW4^ X%R/RgWX%^_Uhd Z 1/,/x0, K-a//ff 9,/G.9/~*8 fi S4s7VVVVqVXVkW4U;TVfCd ZSU;X%ZCi6lX%ZCwx^Wi=X%W4\VW4^cfiTSkffkW4Z]aWiTVZ}X%ZWX%^n2d W4^ i^XVw]@TSwS]abCd UR*X%RAW4^qEWu9TVQCn2i tVW4^emk0QCcb}n2d VW]aT6]abCX%Z/j]ab/Wk wxTV^&]abSWd ^8d Z/Ud \Vb]apwQCn`^_WkffX%^_Us`ubCd2cOb}n Wi ]aT6U_W4tVW4^XVn)Ud \VZ*d Cc4X%Z`]d2kR/^_T tVWkW4Z`]aUhd Zj]abSWRCX%RgW4^qCWYXVn U_T6]abCX%Z/Y]ab/W^_WwW4^_W4W4UhTSw]ab/<W "r;{UQ/fCkffd U_Ud TVZzX%ZCi~]ab/W \xQuyUQ/fCkffd U_Ud TVZzwTV^h]ab/Wd ^hRAW4^cfiW4R/] tVW>cfiTSkffkW4Z]aUqW wBW4]abSW4^n2X%ZCiU{-^_\SX%Z*d X%] TVZb/W;/^_U_]X%Q/]ab/TV^-uX%U@UQ/R/RgTV^_]aWifeX0]a^XatVWn\V^X%Z`]{XauX%^iWi8feff]ab/wTV^P/c4d W4Z] CcW4UWX%^cbu w"{aq b/WYU_WcfiTVZCi[X%Q/]ab/TV^ uX%UhU_QSR/RgTV^_]aWi}d ZRCX%^_]8f`e w'P/[QSZCiW4^\V^X%Z]aUB_4PpVVVV%S{X%ZCmLpVVVVVVVsfejw;mQSZCiW4^*\V^X%Z`]aQU w;VVV%VpV%p%pV%SFw;VVV%VpV%p%pVVVVs1X%ZC=w'VVV%VpV%p%p%SVsSf`eh]ab/W r"rffQCn ] d2iSd Uc4d RCn2d ZCX%^_[e "ZCd tVW4^_Ud ]e~W4UWX%^cbjZ*d ] d2X%] tVW;BBR/^_TV\V^XVk XVi/kffd ZCd U_]aW4^Wi6fej]abSWYZw'Q/Z*iW4^&\V^X%Z]amU w'VVV%VpV%pVVV~X%ZCw;VVV%VpV%p%pVVVVsLX%ZCihfe>Xff-Q/\V\VW4Z/bSWd2k Wn2n TuU_bCd R*sCXffVQCn f/^d \Vb]8Wn2n TuU_bCd RCs*X%ZCiX{\V^X%Z]8wx^TSk w"{qP/X%f/f*X%] d2c4XVn*U_Q/R/RgTV^_]Yw^_TSk LjX%ZCi~]ab/W>y"W4f/^_W4u BZ*d tVW4^_Ud ]eTSw W4^_Q/UXVn Wk U~XVn U_Tm\V^X%]aWwQCn2n eXVcZ/Tu8n Wi\VWi/qeK[K"NWG J*J+fiZ]abCd UU_Wcfi] TVZCsDu9W6R/^T t1d2iWh]ab/W6R/^_TTSwxUTSw@XVn2nA]ab/W6^_W4U_Q*n ]aU5d Z]ab/W6RCX%RgW4^q5VTV^YcfiTVZ`tVW4Z*d W4ZCcfiWs1u@W^_W4U_] X%]aW8]ab/W^_W4U_Q*n ]aUb/W4^_WqL87|+1^ /55**/ fi88 )n + /zO%}9q1^ # ;1 ^WD^ W W 8! ff>mzu}*'ff n /\W9/_` /*G'ff n S\ n #W>mff&X >zOu}D ^ W 1 ^ W 8R 6?|)~V1 ^W!hzO%}D ^ W h1 ^ W # m!h6 ,)^W h!/qD ^ W 2\b"a\e7PQ/RSRATVUW6aXVBb/TSn2iUq"WuX%Z]0]aT5Ub/T u]abCX%]VW X%ZCi X%^Wjd ZCiW4RgW4ZCiVW4Z`]w TV^XVn2n\v>q&Ld Wv>q*4wCD^ W ]ab/W4Z]ab/W'W4tVW4Z]aU8X%^_W']a^d tgd2XVn2n emd ZCiW4RgW4ZCiW4Z] q-PTU_Q/RSRATVUW']ab*X%]1^ 2Vq0n WX%^n e[<aR1^ Wk W>8 D^ WUd ZCcfiWTVf/U_W4^_t1d Z/\~d2kRCn2d W4U]abCX%]0]ab/W{]a^_Q/Wu@TV^n2id Uhd Z8'aqDCeYRCX%^]8aXVas1^ W1^bQ/Us}ui^W k W > D^W W>8a1U_b/Tu8d Z/\6]abCX%]W U ZCiW4RgW4ZCiVW4Z`]-TSw s\Sd tVW4ZW8q`77fibc edbgfb]fbz}zhcyz1s1t yz|quw"W4`]'U_Q/RSRATVUW]abCX%] fCb/TSn2iUsCX%ZCi$d U'U_QCcOb~]abCX%]m1^ W ;Vq^_TSk$RCX%^_]hfC]'d Ud2kffkWiSd2X%]aW0]abCX%]#1^ W k W>8 1^ W >aq&5TV^_W4T tVW4^sUd ZCcfiW$o8sCc4n WX%^n e1^ W k W D^ W aq.LX%^_]6ac4Z/TuwxTSn2n TuUqn WX%^n eaiSBwxTSn2n TuU'd2k5kWi/d2X%]aWn e~w^_TSk ac4aq bQ/Us/d ]"^_WkffXVd Z/UB]aTU_b/T u]abCX%];aXV"wTSn2n u0U'w^_TSkai/aqCWiT']abCd UfeU_b/Tu8d Z/\']abCX%]0ai/Ld2kRCn2d W4U0ac4X%Z*i0]ab*X%]&ac4d2kRCn2d W4U0aXVaq-PTU_Q/R/RgTVU_W9]abCX%]0ai/b/TSn2iUq;PQSR/RgTVU_W]abCX%] D^W wTV^ XVn2?n oUQCcb]ab*X%] 1^ W KVq^_TSk ]ab/W5iW4/Z*d ] TVZTSwLcfiTVZCi/d ] TVZCXVn/R/^_TVfCX%fCd2n2d ]eD^W>8F b) rb /_- 1 ^ k W a21^W >8a21^WF b) rb /_- 1 ^ W ?1^W hH^Wh21^W>8F b) rb /_>bQ/Us1ac4wTSn2n u0U w^_TSkai/aqLd ZCXVn2n esg]aTU_W4W ]abCX%]ffaXVwTSn2n uU5w^_TSk ac4asAU_Q/R/RgTVU_W8]abCX%]6ac4@b/TSn2iUq0wU UU_QCcObm]abCX%]1^ W h VsA]ab/W4ZaXVd Uffd2kffkWiSd2X%]aWsgU_TU_Q/R/RgTVU_W8]abCX%]=1^W 0 Vq Sb W4ZCs/QSUd Z/\ac4X%ZCi6]ab/WYwXVcfi]]abCX%V] W>8s)u@W{bCXatVW]abCX%]1^1^1^W1^1^1^W1^WW ?D^W ha21^>?D^ ha21^Wk W >?D^W ha21^W >?D^?D ^W ha21^W >?D^ 8a2D ^W}W >8a8X%U iVW4Ud ^_Wi/q*lxQy$/+K h*,, )/1^q 5/l/+S{G'`,ff( /-4 6[<aR Ld ^_U_]{U_QSR/RgTVU_W6]abCX%]{]ab/W5U_W4]aUjd Z X%^_W5RCXVd ^_u8d U_Wli/d U_TSd Z`] q b/W4Z|wTV^{WXVcbR/^_TVf*X%fCd2n2d ]ei/d U_]a^d f/Q/] TVZ1^TVZelsAWXVcOb|sX%ZCiYWXVcOb>u9TV^n2iq|vU_QCcObl]ab*X%]D ^ VVsd ]k0Q/U_]&fgW]ab/WYc4X%U_W]abCX%]1^ W Vq bQ/Us`RCX%^_]8aiS*TSw b/W4TV^_WkVq2ffX%RSRCn2d W4UqTV^0]ab/W5cfiTVZ`tVW4^U_WsU_QSR/RgTVU_W;]ab*X%]&]ab/WU_W4]aU8d ZX%^_WZ/TV]-RCXVd ^_u8d UW5i/d U_TSd Z`] q b/W4Z]ab/W4^_WW41d U_]U_W4]aUh"4U_QCcOb~]abCX%]0fATV]ab X%ZCi k X%^_W{Z/TVZSWkR/]eq0~ W4]V - > k qVn WX%^n e]ab/W4^_W&W41d U_]aU;Xi/d U]a^d f/Q/] TVZD^TVZ U_QCcObff]abCX%]1 ^ 2Vs1 ^ 2Vs]1 ^W- W V^1 ^ - W QVqCQ/]]abSW4Z 1 ^ - z#Vq[<a"ZW " ^ " lbQ/U:eS1^-W01^->aX%ZCih]abSWWLrhcfiTVZCi/d ] TVZ[RCX%^_]8aXVLTSw bSW4TV^_WkVq2V0d U0tgd TSn2X%]aWi/qjj ;GY,)/ |n 6| nn :eL E~xQyB %,/ph%H# /Dh\`7_S%+@/m7,fi " |qxq}t~"}s1|zO%} E ;D^qffSj,, )}|n [/~17 ,ffp ,Y / j + /fi|p7&,%`SS}}|pG'1^W H'' JILK-/ln ,@ & ^i^i^i jS , / 1^ FW h WD^W zF#0SK Yx0y a7 _A @ B @ B,ff_ 9 V^i^i^i 6l;.9D^ff,Mzu} E 7 ,6p ,ff//ffGff4 , @Ghfffi{66H72O QP R 6ff9G,, )/{|n #, `%@,%`SSYU7 V ffoQP R / XgXg>Y''K Z'$,%`Shjfiz[7@ ) 17 A_@ B @ B 9 ]? {XVn2n`XE aO QP R&S "9ff fij7 n , \A^@ ] ~ff Y>%o1^l|n Z e1^, EX zffophS-GD^mXE})z^}1^,MK xQy/zu}D^ FW F 6Sq1^>!TV^RCX%^]haXVasgUQ/R/RgTVU_W]ab*X%]D^ffd U6Xi/d U]a^d f/Q/] TVZmTVZ]abCX%]UX%] U_SW4UWLrh;q~/W4]=5fgW]ab/W'Z`Q*kfgW4^@TSw)^_T u0Ud Zq7 sgX%ZCijn W4].C* 1^W q'*fiasgwxTV^- V^i^i^i ^s`ub/W4^W:'1*Bd U-]ab/W6X%]aTSkcfiTV^_^_W4URATVZ*i/d Z/\h]aTff]ab/W{]ab~^_TuTSFw 7 q wBTV]aW8]abCX%] C*5wxTV^ V^ i^i^i ^q n WX%^n es[<aR1^ F ;W q'*aVfi]&WX%Ud2n e}wxTSn2n TuU w^_TSkv]abSWLr6cfiTVZCi/d ] TVZ]abCX%]D^ F W q'1*fi 1^ FHW>FwTV^8XVn2n'1*F sUTaVd U0W QCd tXVn W4Z`]0]aT1^ ;W}FaVVd2kRCn2d W4U]ab*X%]jwTV^ V^i^i^i ^q=~/W4]q9@ *fAW ]ab/Wh^_Tu Z7 cfiTV^_^W4U_RgTVZCi/d Z/\]aTq'*fiqhP/d ZCcfiW9@ *bCX%UffXjjX%U5d ]aU ]abcfiTSkRgTVZ/W4Z]6d2w'1*H 6X%ZCimXY8TV]ab/W4^_u U_WsLd ]6wTSn2n u0U]abCX%]9@ *gA @ B ffX%ZCi bSW4ZCcfiUW 7 2A @ B @ B qTV^hRCX%^]YfCas-n W4] fgWff]ab/W5ZQCk0fgW4^TSw*^_TuUjd >Z 7 s@n W4c] 9 @ & ^ i^i^i j98@ jfgWff]ab/WY^_T u0U8TSw7 s-X%ZCin W4N] ' & ^ i^i^i ';fAW~]ab/WocfiTV^_^_W4URATVZ*i/d Z/\X%]aTSkUqLd Xg OQP REs;X%ZCi>U_W4] C* Xg}Y '*ffwTV^V^ i^i^i ^ Vq ~/W4] 1^fgW]ab/W{QSZCd2QSW6iSd U_]a^d f/Q/] TVZ~TVZ UQCcb]ab*X%]D^Wq'1*D^Wq''D^ FW <'1*C*4wTV^0V^ i^i^i ^ /ffd2w)'|x. /' & ^i^i^i ';Vw '*L>sd2)TV]ab/W4^_u U_Wqw"TV]aW]abCX%]1^md Uld ZCiW4Wi|XR/^TVfCX%fCd2n2d ]eiSd U_]a^d f/Q/] TVZ|TVZ ms9Ud ZCcfiW 1^W''@1^ q' * !6wxTV^; V^i^i^ ^ sEX%ZCi/sUd ZCcfiWu9WYX%^WYX%U_U_QCk5d Z/\6]abCX%]7 2A_@ B Bj^ W q' * 9 @ * _ @ B V1r&`77aVVsfibc edbgfb]fbz}zhcyz1s1t yz|quq fi]{^_Wk5XVd Z/U{]aTU_bST u ]abCX%]q1 ^{UX%] U/W4UqLr6$X%Z*ij]abCX%]V^i^i^i ^qn WX%^n eD^8 Xg>D^ HW oFaq0{d tVW4ZlhoVV^i^i^i 6l"Vs/U_Q/R/RgTVU_W]abCX%]]abSW4^_W{W41d U_] X%]aTSkU3'1*' *cfiTV^_^_W4URATVZ*i/d Z/\h]aTff^_TuUa9@ *9X%Z*i9 @ *S TSwF7 U_QCcOb]abCX%]'1*' *S >F q b/W4Z^ \ W <'1* 1 ^ F_ W q' * 7ifi]"Z/TuwTSn2n uU@f`e b/W4TV^_Wk$Vq2Vac4E]abCX%]#1^"UX%] U_/W4U@]ab/WL r6cfiTVZCi/d ] TVZwxTV^ & ^i^i^i 47j q-5TV^_W4ptVW4^s b/W4TV^WkVq2Vai/asEd ] kQ/U]&fAW]abSW5c4X%U_W]abCX%]1^ ;W}F qwTV^{bSWffR/^_TTSwCTSw b/W4TV^_Wk Sq fSQCd2n2iU{TVZ~Wkffk5X6Sq2lX%ZCi]ab/WmwxTSn2n Tu8d Z/\lR/^_TVRgTVUd ] TVZCsEubCd2cObU_b/TuU-]abCX%]-]ab/WffcfiTVZCi/d ] TVZTSw)R*X%^_]{fCDTSw b/W4TV^_WkSq UXVcfi]aQCXVn2n ejU_]a^_TVZ/\VW4^&]ab*X%Zj]abSWhcfiTVZCiSd ] TVZTSw7RCX%^_]aXVaqB]d U]abSW4^_WwxTV^W{Z/TV]U_QS^_R/^d Ud ZS\ ]ab*X%]8d ]n WXViU0]aTlXU_]a^_TVZ/\VW4^hcfiTVZCc4n Q/Ud TVZ*q,5,_(}G~fiY67$ff+/%+la`/ SZ )eC 9 fi~7'vu/l_` / /' = ffg>p 3' %`//mmfiv^} /mA/VV^i^i^i6l"9 6' F~M@j7,n Ny @ {>u/5,|pm /o6ffY8},) ]h>VV^ i^i^i6 l"/ ~[<a"ZW " ^ " gl[<aR PQ/R/RgTVU_Wh]abCX%]{]abSW4^_WffW41d U_]aUjXhUQ/f/U_W4]}TSwE^_T u0UTSw17]abCX%]Yd Ujn2d Z/WX%^n eiW4RgW4ZCiW4Z]f/Q/]Z/TV]ffXvffZ/Wn eiW4RgW4ZCiW4Z] q ]ab/TVQ/]5n TVU_UTSw1\VW4Z/W4^XVn2d ]es9n W4]8o @ & ^i^i^izo @ 5fgW8]ab/Wh^_TuUYd Z}q b/W4^_W*r p * X%Z*i *r p * @ * VqW6/^U_];Ub/T u ]abCX%]ffd ZwXVcfi]W41d U_] p & ^ i^i^i p\ffU_QCcbm]abCX%]&&W4tVW4^_e^T Gu o@ dcZ }d UhX%ZX vffZSWjcfiTSk0fCd ZCX%] TVZTSwg]ab/W8TV]ab/W4^^_T u0UqLd U_TSkW8YVV^ i^i^i^ Vq ~/W4]*r * ffX%ZCip\ *r & p&* *r4 p&*@X%ZCin W4] * p&*@wTV^#q b/W4Z&p&*to @ *p&*to @ @ ffiw* @ **r*r&&&TV^ V^i^i^i^sLn W4] * *fi/Sq b/W4Z*r & * jX%ZCi *r & * @ * @ qwBTu d2we'1*K F ~wTV^BU_TSkW V^i^i^i ^ffX%Z*iU_TSkWj V^i^i^i 6l"s]ab/W4Zo @ * bCX%U;X{{X%U'd ]aU"b]ablcfiTSkRgTVZ/W4Z`] qCr6n U_TSso @ *U8X%Z}XvffZSW5cfiTSkfCd Z*X%] TVZ~TSwS]ab/W^_T u0UTSw|} u8d ]ab~Z/ThZ/W4\SX%] tVWjcfiTSkRgTVZ/W4Z]aUs)UTo @ *@d U]ab/WffiW4Ud ^_Wi*rtVWcfi]aTV^q{YG {%)/0n '|n\b"a\e9LE/+ e7h~xQyB ,/=ph%H#/qV & ^i^i^i 4FjA5/5,+0zO%}c|S `~' fi7, l4 ?}Gj6^7vSln , ^y @ & ^i^i^i jA&vu/6p S>G6ff{G6} )j ] ff}VV^i^i^i6l"/ ~ j|hp 8jffVV^i^i^i 6"l * fiff/%o)1^l,MK-Yx0v,H1 ^ W ~K/q1 ^ ^''KV , >|p ',%`SSYa}zu}9Yj7 ,84 , 1} GYjfihGfi7;/,,+_` /0 )&/0vu/ +a`/ S1ff fi0B/%o )1 ^' 1MK& 'xQ)E1 ^''!{fi|p ',fi O`//jjfiz[}Oz u}@n } } S/G#l/,+~/_` / /;6637v/l~% )qXE,1X Y''HQ '%`//a}9 fi5{)/ ),, )mX ) j eD^%o ) /xQy^W '' XE>Y'' 8|p ',,fi O`//fia}B 1 ^8 X a,`7fi " |qxq}t~"}s1|VTV^RCX%^_]aXVasU_Q/RSRATVUW>]abCX%]} cfiTVZ/Ud U]aUlTSw^o @ & ^i^i^i jo @ s5cfiTV^^_W4U_RgTVZCi/d Z/\|]aTvX%]aTSkU' & ^ i^i^i'HqCelX%UU_QCkR/] TVZ*s]abSW4^_WW4gd U]cfiT`W^vc4d W4Z]aU6p & ^i^i^i p_U_Q*cb]abCX%] *sr & p&* Vs1X%ZCiX*sr p&*t@ B* U_QCcbm]abCX%]W4tVW4^_ecfiTSkRgTVZ/W4Z];TSw3@ UZ/TVZ/Z/W4\SX%] tVWqYPQ/R/RgTVU_WsSfeuX_eTSwtVWcfi]aTV^y @&cfiTVZ]a^XVi/d2cfi] TVZCs]abCX%].1^@UX%] U_SW4U.rhzX%ZCi]abCX%].;* 1^Wq'1* !8wTV^9-VV^i^i^i^Vq0Ee[<aR~Wk5kffX'Sq2VaXVas/u@W{bCX_tVW@ 2 @*r&pT*wo @ *2A @&p *fio @ *YA @&p **r &s* r &VaVub/W4^WA @ UffiW4/Z/WilX%Uffd Z~WkffkffXSq2VqhTV^ V^i^i^i6l"sCd2wK1^W [ {]abSW4ZD^WF k W 1^ F. X%Z*i1^W . VsEU_T = Vq~CeX%U_U_QCkR/] TVZCsXVn2n]abSWcfiTSkRgTVZ/W4Z]aUYTSw[y @ X%ZCiHA @ X%^_WZ/TVZ/Z/W4\SX%] tVWq b/W4^_WwTV^_Wsd2w9]ab/W4^_W~W41d U_]aUjb UQCcb|]abCX%]1^ F ~2 X%ZCi ~0Vs]abSW4Zmy @ bA @ Vq bCd UcfiTVZ`]a^XViSd2cfi]aUaVas1X%ZCi{R*X%^_]aXV@d U-R/^_TtVWi/qTV^RCX%^_]6fCasAU_Q/R/RgTVU_W8]abCX%]]ab/W4^_WhW41d U_]aU5X8U_Q/f/U_W4fi] }TSw1^_TuUTSw 7]abCX%]ffd U5n2d Z/WX%^n eiW4RgW4Z/piW4Z];fSQ/];ZSTV]hX vffZSWn eoiW4RgW4ZCiW4Z] q8PVQ/R/RgTVU_WsfeuXaeTSw"cfiTVZ]a^XVi/d2cfi] TVZCsg]ab*X%m] D^'UX%] U_/W4=U Lr6X%ZCi~]abCX%] 1^ W';mwTV^XVn2n-X%]aTSk\U 'cfiTV^^_W4U_RgTVZCi/d Z/\]aTXff^_Tud gZ }q "d2cO|X%ZX%]aTSk'ffcfiTV^_^_W4URATVZ*i/d Z/\~]aTlU_QCcObzX5^_Tu8q}Ce D^_TVRgTVUd ] TVZor6q2mX%ZCi b/W4TV^_Wk Sq SaXVasu@WYb*XatVWj]abCX%]1^ F ~ ffwTV^8XVn2n) U_QCcOb]abCX%] ' >~qCQ/]]abSW4Z 1^ W q' VsDX%ZCihu@W{bCX_tVWX%^_^d tVWiX%]8X6cfiTVZ]a^XVi/d2cfi] TVZCqTV^RCX%^_]>ac4as'U_Q/RSRATVUW]abCX%] } cfiTVZSUd U_]aUTSw0]ab/W^_TuU @ & ^ i^i^i jo@ jq ~W4] 7 fgW]ab/eWU_Q/f*kffX%]a^d lTSw 7cfiTVZ/Ud U] Z/\~TSwC]ab/Wff^T uU8TS1w }qlP/d ZCcfiW5]ab/W4U_W5^T uUX%^_Wn2d Z/WX%^n ed Z*iW4RgW4ZCiW4Z] s"XU_] X%ZCiSX%^iY^_W4U_QCn ]TSw9n2d Z/WX%^YXVn \VW4f/^X UXaeU{]abCX%] 7 UYd Z`tVW4^] fCn Wq ~/W4W] 1^fgWXi/d U_]a^d f/QS] TVZTVeZ@@&UX%] Uwe1d Z/\ Lrh;qgEe ~WkffkffX;Sq2VaXVa7 @ B q bQ/U1@ 7 q&TV^& V^ i^i^i 6 llu9Wffk0Q/U_]bCX_tVW - 2D^ W |asgubSW4^_W D^aq{d tVW4Z D,^ >Y ';wTV^{WXVcObX%]aTSk ''su@WYc4X%Zc4n WX%^n eU_TSn tVWwxTV^0]ab/W Uq\b"a\eL @ n 6 G%)/0|n ;ffF G{,+'/ff5 6| n S,:1^o% ) 6MK& xQy fi}S>G`fi|ph ,m{:f)z )] h? m!D^aW} h4 / V87fi|`{:fF8fi )%/ 48,]abCX%]}d2w1^}d U}XlR/^_TVf*X%fCd2n2d ]eTVZ U_QCcOb]abCX%] shwxTV^UTSkWU_W4]a] Z/\TSw[<aR Ld ^_U_]u@WU_b/Tu]ab/WRCX%^XVkW4]aW4^_U6TSw1{:f)z sVD^ a[W/ 4 / Vhd U6]ab/WjR/^_TVf*X%fCd2n2d ]eo]abCX%]{:f)zFB^_W4]aQ/^_Z/U{h4as)]ab/W4ZD ^@UX%] U_/W4UL r60qgCe bSW4TV^_WkVq2VsDd ]-U_Q]v cfiW4U-]aT6U_b/Tuz]abCX%]wTV^WXVcbffU_W4]0X%ZCi'u@TV^n2iU2 & 6 + >U_QCcb6]ab*X%]1 ^ & 2X%ZCim1 ^ + 2Vsu9W'bCXatVW1^ W & 1 ^ W + aq@PVTU_Q/RSRATVUW]abCX%]K & 6 + }s^ W & Vs{X%ZCiD ^ W + Vq/~ W4]1` X& aa _ aqfiZ`]aQCd ] tVWn es. Uff]ab/W~R/^_TVfCX%fCd2n2d ]e[]abCX%]ff]ab/WoXVn \VTV^d ]abCk]aW4^kffd ZCX%]aW4Uld2k5kWi/d2X%]aWn eX%]ffU_]aW4RvVq2u8d ]abm 49cfiTVZCi/d ] TVZCXVnTVZ5U_TSkWVv}fgWd Z/\5cOb/TVU_W4ZlX%]9U_]aW4RlVq2Vq["w TV] d2cfiW6wxTV^wxQ/]aQS^_W-^_WwxW4^W4ZCcfiW]abCX%] sDwTV^8XVn2n])_X) P / P F _aa_aVVub/W4^Wd U&iW4/Z/Wi-fe5SaqCr;U*W4RCn2XVd Z/Wi Z]ab/WkffXVd Z8]aW4] swTV^EfgTV]ab V4Vsjd UE]abSW"R/^_TVf*X%fCd2n2d ]e]abCX%]8]ab/WXVn \VTV^d ]abCk iTW4UZSTV]{]aW4^kffd ZCX%]aWmX%]U_]aW4RVq26\Sd tVW4Z]abCX%](*d UjcbSTVU_W4Zzd ZoU_]aW4RVq2Vqjfi]`_Yfibc edbgfb]fbz}zhcyz1s1t yz|quWX%Ud2n e}wTSn2n uU;]abCX%]]ab/WR/^_TVfCX%fCd2n2d ]eY]ab*X%]8(*4d UTVQS]aR/Q/]X%]0U]aW4RoVq2ffd UX * u+*^^ X u Va uibQ/UsUD^ W (* k 8 Xg}(*fiu Vaaq?"Ud Z/\~aVVas/u9WbCXatVW8]abCX%]1^ W (*1^W (* k EX >( *EX >(*ui__Ld ZCXVn2n esSu9W8bCXatVW8]abCX%]D^ W (*fi Va asEwTV^0 V4Vq bQ/UsCD^UX%] U_SW4U]ab/WrhcfiTVZCi/d ] TVZCqTV^]abSWcfiTVZtVW4^_U_WsgUQ/R/RgTVU_W{]abCX%]=D^;UX%] U/W4U]ab/WLrh cfiTVZ*i/d ] TVZCqm~W4] V & ^i^i^i 4 j VqWcb/TTVU_W5]abSWffRCX%^XVkW4]aW4^UwTVfi^ {:f)z X%UwTSn2n uUqPVW4]UXg} D^W hX%ZCion W4]*aq; ]ab/TVQ/]ffn TVU_UTSwg\VW4Z/W4^XVn2d ]esgu@WX%U_U_QCkW8]abCX%] *; jTV]abSW4^_u8d U_Ws7] X%VW* 1^]aTlcfiTVZSUd U_]0TSwA]abSTVU_WU_W4]aU']abCX%]hX%^_WTVf/UW4^_tVWi6u8d ]abRgTVUd ] tVW8RS^_TVfCX%fCd2n2d ]esCX%ZCiiTff]ab/W8R/^_TTSw/QSUd Z/\aq*fiVq~PW4U] X; *fi D^ +*fi *X%ZCq; ) VqTV^ V^ i^i^i 6 l"s@n W4] * V* +b Q/Us*]ab/WjU_W4] (*8d UXVn uXaeU6^_W4Wcfi]aWi/sLQ/Z*n W4U_U (* F q2PSd ZCcfiWD^W vF ] 1^jfeX%U_U_Q*kR/] TVZCsBd ]5k0Q/U_]fgWh]ab/W~c4X%U_Wff]ab*X%] kffd Z r & 1^ QVqqw"T u$U_W4]w ) 2D^ W }+*fiaqW/^U_]"U_bST uz]abCX%] su8d ]abj]ab/W4U_WRCX%^XVkW4]aW4^&U_W4]a] Z/\VUsu@Whc4X%Z}cOb/TTVU_Wfi&U_QCcOb]abCX%]{cfiTVZ/U_]a^XVd Z]S&d U-UX%] U_/WiS[q ~W4{]b vU_QCcObj]ab*X%f] XE> h!P ) P F X; t_ "q&TV^WXVcOVs)u@W{bCX_tVWX Yw_j*r P F P) `P X; *fiY _ )&* F X ** ) ; X * L )b/W0n2X%U]DWQ*XVn2d ]e5wTSn2n uUCfAWc4X%QSU_W V * * Vq b`QSUswTV^9XL/Wi-,sF P ) X * _UWd ]ab/W4^:X *fiY /L d2w >+fi* sTV^6X;&*Y ; ) d2wg *q]8wTSn2n u0U0]abCX%]/ *a 21^W +*a* e /L *EV* ) D^*aa21^W +*a*e /L D^j*r * )1/^ * * F D^ +*U W* >*&h Ud ZCcfiWW1^UX%] U_/W4Urhe1^+C*Wh*F,bQ/Us d2wXg} aXEo Vs@U_T>]abSW4U_W~RCX%^XVkW4]aW4^YUW4]a] Z/\VUmX%^WX%R/R/^TVR/^d2X%]aWowTV^{:f)zF] X%1d Z/\N wxTV^8X%ZeUQCcb]ab*X%]Xg} !Vaq;ffTV^W4T tVW4^s qWZ/Tu U_b/T u ]abCX%] s0u8d ]abz]ab/W4U_WmRCX%^XVkW4]aW4^U_W4]a] Z/\VU1^ Wk jd U]ab/WR/^_TVf*X%fCd2n2d ]ez]abCX%]<Q6f)zFYb*XVn ]aUu8d ]abh4asffwTV^oXVn2n[%X ZCi q n WX%^n e d2w1^ W h Vs]abCd Uhd U0]a^_Q/WsUd ZCcfiW{]ab/W4Z 1^ W k VsEX%ZCi6]ab/W{R/^_TVf*X%fCd2n2d ]e]abCX%N] {:f)zFhb*XVn ]aUu8d ]abzTVQ/]aR/Q/]m 4d U>X%]>kTVU_<] XE> D^WVqPTU_Q/RSRATVUW0]abCX%] D^ W HVq b/W4Z>d ]-U_]Q vlcfiW4U@]aT6Ub/T uz]abCX%] D^ +*UW -d U&]ab/W`_=fi " |qxq}t~"}s1|R/^_TVf*X%fCd2n2d ]e ]abCX%]0 4*ad UTVQ/]aR/Q/] sV\Sd tVW4Z5]abCX%]2vd U;cb/TVUW4ZX%]]ab/W&/^_U]*U]aW4RCq-EQS]]ab/W{X%^_\VQCkW4Z]TSw7]ab/W/^_U]&bCXVn2w/TSw7]ab/WRS^_T`TSwSU_b/TuU]abCX%]0]abCd UR/^_TVf*X%fCd2n2d ]e>d UQ/U_]x qCQ/]X%U iVW4Ud ^_Wi/q&b)T_Y)1^&)/)&//bh Ud ZCcfiWU/b+*fia21^W >+*fih Ud ZCcfiWD^UX%] U_/W4ULrhe1^ +*CW h{;7fi|1^8ZeC'`%V & ^i^i^i4FjAh6QD/'A|,| +\b"a\:eS *& ^ i^i^i^gj)ffR & ^^ gj E ) 8 | , n & & ^i^i^i^gj7FjE#Lp@VV^i^i^i 6"l W* lK+Sfffi6, )n +Oz %}9=D^ 8V &e1 ^W 8 ^86 & & ^i^i^i^gj7Fj7R>*fizu}D ^ W ^ W +*fi 8?[+*)D ^ W!hbSW~R/^_TTSwd UYUd2kffd2n2X%^md ZU_RCd ^d ]ff]aTo]ab*X%]YTSw bSW4TV^_Wk Vq2VqPQ/R/RgTVU_W]abCX%]maXV{bSTSn2iUs>+*fis1X%ZCiD^W h!Vq bSW4ZD^W^W?D^ 8a21^Wjj^6^^^^?1^Wa21^8& &a2D^8C4* 1^,>fi+fi* ?1^C*41^8218^*[<aRP/d2kffd2n2X%^n esD^ W >+*fiD^ W >*Y ?D^CS 1 ^ 6 & & ^i^i^i ^ jfi+*?D^CS C *\D ^8a+*fiC*_1 ^ 8a21 ^8}^WbQ/UsCD^WKVqa2D^ W >*j ?1^W a2D^}+*fi8a21 ^W* 0wTV^XVn2n$o**U_QCcOb~]abCX%]m1^TV^]ab/WcfiTVZtVW4^_U_Ws-U_QSR/RgTVU_W]abCX%]fCbSTSn2iUmX%Z*i^ Vq${d tVW4Z *fisd2w1^ W Vs1]ab/W4ZaXV-]a^d t1d2XVn2n emb/TSn2iUsAU_TUQ/R/RgTVU_W ]abCX%]W1^/W h0VqPQ/R/RgTVU_W]abCX%] *qn WX%^n e D^6 & & ^i^i^i^gjAFj7 ;*_1^8fi+*aqwBTu8s*Q/Ud Z/\[fCas*u@WYbCX_tVW]abCX%]D^ WD^ WD^ WD^ W +*CC*\D ^8fi+*?D^ ha21^W>+*fi?1^W a2D^ 88?1^W a2D^W }+*fih Q/Ud ZS\~aVLq`_u`fibc edbgfb]fbz}zhcyz1s1t yz|qubQ/Us1aXVLbSTSn2iUq6| , n /:eS */,0'`,V & ^i^i^i 47j 6G0 /66EG#/ & ^i^i^i2` * * & & ^i^i^i^ *j j ,) &C *( S87n %Y% )X aX Y`*fi0. 6SVV^i^i^i ^; fi7, %o1^q ,Xg 1^, zff6Xg {p /*6mD^5}Y/D^5,MK-85 fi,xQy/z `%z}66*ffpo%}; { & ^i^i^i 4jETSw7u@TV^n2iUsEXU_W4] / & ^i^i^i 2`TSwSTVfSU_W4^_tX%] TVZ/U;u ]abi/d U_]a^d f/Q/] TVZ[<aR {d tVW4Z[XU_W4]X UX%] Uwxe1d Z/\X Y`* mwxTV^65 VV^i^i^i ^ Vs0X%ZCi[X%^_f*d ]a^X%^_e|i/d U]a^d f/Q/] TVZ/UD ^8 TVZFV^i^i^ 6"l sAu@W6W4RCn2d2c4d ] n ecfiTVZ/U_]a^_QCcfi]5X8R/^d TV^W1 ^TVZe]abCX%]UX%] U_/W4UL r6zU_Q*cb]abCX%]fiXE 1 ^,&ub/W4^WW1^ U0]abSW5kffX%^_\Sd ZCXVn)TSwRD ^TVZ$X%Z*i1 ^, 1 ^8ufi aq{d tVW4Zv}F s1iVW4/Z/W^ a5/ `*fi6W>/ hV X Y`*u; *(7D ^, huiay"T uv]ab/W{R/^TVfCX%fCd2n2d ]e}d U'U_RCn2d ];Q/RTtVW4^6XVn2n/]ab/W^Q/Z/UV6U_Q*cb~]ab*X%] / `*-X%ZCiW}U{d ^_^_Wn W4tX%Z`] q2{]-^_Wk5XVd Z/U-]aTcOb/Wcj]abCX%]Z1^{d U{Xhi/d U_]a^d fSQ/] TVZTVZq X%ZCi8]abCX%]{d ]@UX%] U_/W4U{XVn2n]ab/W[<a"ZW " ^ " l^_W QCd ^_WkW4Z]aUqLfi]d U0WX%Uej]aTlcb/WcO]abCX%]j1^ `*X `*fiuC*(7D^,r & Ffi ]8wxTSn2n TuU0]ab*X%]*r & 1^ `*fi]ab/WYk5X%^_\Sd ZCXVn)TSw1^TVZqL4w]v>X `*uisU_bST u8d Z/\6]ab*X%]1^8d U XR/^_TVfCX%f*d2n2d ]e>kWX%U_Q/^WYX%ZCixXVs]ab/W4ZdU1^,>fiF D^,> a2D^aFb fiW) / _ ) /ff_fi") /_) / b fi)CS)_ ) // ff^ ui1b ) _S //_"fi ) /ZCXVn2n esgZ/TV]aWh]abCX%] swTV^VV^i^i^i6l"VswTV^ffXVn2nEF'U_QCcObm]abCX%]=1^WL]abCX%]D^ `*CW hfi") / b_) /b fiW) / b ) /fi") /_b fi ) /fi") / b_) /b fiW) / b ) 1 /rb ) b fi ) 1 / /D^ `*CW>VsAu9WhbCX_tVWVU_Tff]abSW\VW4Z/W4^XVn2d 4WiLrhcfiTVZCi/d ] TVZb/TSn2iU8wTV^8V & ^i^i^i4Fj7VqTR/^_TtVW b/W4TV^_Wk Vq2VsBu@W/^_U_]hZ/W4WiU_TSkWfCXVcO`\V^_TVQSZCimTVZkffd ZCd2k0QCk^_Wn2X%] tVWW4Z`]a^TVR`ei/d U_]a^d f/Q/] TVZ/UqLd U_TSkW{U_R*XVcfiW X%ZCi~n W4]8 & ^i^i^i4Fj5fgW{U_Q/fSU_W4]aUTSwq0~W4] fAW]ab/WU_W4]0TSw`_Yfi " |qxq}t~"}s1|u & ^i^i^i ^EjS-wxTV^-ub*d2cb]ab/W4^_W'W41d U_]aUU_TSkWffi/d U]a^d f/Q/] TVZ<Xgu ]ab<XE>a*fi ;*9wxTV^-%X ZCicXE> = mwTV^~XVn2n {vqw"T u n W4][XE fgWmXmi/d U_]a^d f/Q/] TVZu8d ]abJXE> =@ ^i^i^i jA j s1n W4]vq&{d tVW4ZXtVWcfi]aTV^&V^ i^i^i 6 lmwTV^~XVn2n) /"!#!#!) /X h%$ $ XE>ub/W4^W Uh]ab/Wd ZCi/d2c4X%]aTV^~wQ/ZCcfi] TVZCs&d2q Wq h ld2wR v X%ZCiYTV]ab/W4^_u8d UWs&X%ZCi)7) /&"!#!#! %$ $ ) / gX } jd UlXZSTV^kffXVn2d X%] TVZ wXVcfi]aTV^q~W4];* X a*fiYwxTV^YV^i^i^i 6l"q0EeuLUd U2X% ^sDVVVVs b/W4TV^_WkU Vq2ffX%ZCiVq2VasCd ]8wxTSn2n TuU0]ab*X%]Xg}u6&& ^i^i^i^Ej7jSX'jaVV@ ^i^i^i j 0TV^_W4T tVW4^s@wTV^WXVcb}tVWcfi]aTV^u & ^i^i^i^ j 0(ffsg]abSW4^_W~d U5X tVWcfi]aTV^5U_QCcOb&]abCX%]aVV*b/TSn2iUqaTV^X%Z>d ZCwTV^kffXVn7X%ZCi8WX%U_eiW4^d tX%] TVZTSwDaVVasU_W4W6uLT tVW4^ bSSkffX%UsAVVVVsLbCX%RS]aW4^8Vaq2ofnq e1E ? & & ^i^i^i^EjAFj6|phYu & ^i^i^i^gj7;) E & ^i^i^i j7mn , ) Zz|%}Y. & ^i^i^i^EjU.9 * V {,|ph0@>VV^i^i^i6l"BXE>a+*C6 & & ^i^i^i ^C* & +* & ^;* R& +* R& ^i^i^i^EjAFjS ;*fiiDh\vd ]ab/TVQS]9n TVU_UETSw\VW4Z/W4^XVn2d ]esX%U_U_Q*kW]abCX%]]8wTSn2n uUhwx^TSkaVV]abCX%][<aRX 6 +U_Tff]ab*X%]&Vq X%gd Z/\* X a+*fiDwTV^DV^i^i^i 6l"sj j +* * "!#!#! % $ $ X ha& ^i^i^i ^EjSjS Xg>u6 + + ^i^i^i^ j Fj7uij jS * X%ZCi<XE>a+*U6 & & ^i^i^iXEj7jS ;*;wxTV^ V^i^i^i 6l"s7u@WV^ i^i^i 6 l"q bQ/Us+Xg>u6 + + ^i^i^i^gjAFj7 Xg}u6 & & ^i^i^i ^Ej7jSXE>u6P/d ZCcfi\W XE>a+*U6 + + ^i^i^i ^bCX_tVW8]abCX%]m;* * wxTV^'X%ZCi/s1d Z~RCX%^_] d2cfiQCn2X%^s+ ^i^i^i^&& Xg}a & 6&& ^i^i^i^Ej7jSXE>a & 6++ ^i^i^i^gjAFj7ui@on oK G~,)/j/o># / & + }G~| , n /8`* ;* &+ R V4ZD^e,o% ) ,) 1^ & ,1^ + H9/1^,}D^W* hHv0 ?$ E HD^ * 1^u*`*fi,"/~KD^ ff=hp SLGD^ 96f & Q + {/fi fiffB*6//-|n 1^ 1 ^8u `*,\ Z, V4~ W4] & & + ^ + + & ^ . &;k + s"X%Z*i 2 & + aqlP/d ZCcfiW[<aR& ^ + ^ . ^ 2 X%^_W~XVn2nX%U_UQCkWiY]aTfgW8Z/TVZSWkR/]es1u9W6bCX_tVW, aV4V + sAub/W4^W- U5iVW4/Z/WimX%UX%fgT tVWs]abCX%]'d Us.$d U"]ab/W0U_W4]'u & ^ + gU_QCcOb5]abCX%]"]ab/W4^_WW41d U_]aUX{i/d U_]a^d fSQ/] TVZNX u8d ]ab4X && Xg>+ + s2Xg>hK5wxTV^8XVn2n?vvq*4wD ^ * ^,>u `*wxTV^0 V4Vs)]abSW4ZpE1 ^,}u & pAu1 ^8}u + 1 ^,}aVV\b"a\:e*& ^C* +`_uafibc edbgfb]fbz}zhcyz1s1t yz|quub/W4^W3p D^ & aqLWR/^_TtVW]ab/W]ab/W4TV^_Wkvfe5Ub/T u8d ZS\ ]ab*X%]aVV0c4X%Z/Z/TV]&b/TSn2id2wWd ]abSW4^& TV^ + U9Z/TV]0W47^_W4epn2d VWqP/d Z*cfiWu9W;bCXatVWhX%U_U_QCkWi]abCX%]'uC* & ^;* + >aV4V + wTV^" V4Vs+ wxTV^@u@Whc4X%ZX%R/RCn e~aVVC]aTx`*BwxTV^@ V4Vq bQ/UsV]abSW4^_W X%^_W'tVWcfi]aTV^_U * & * + BV40U_QCcOb]abCX%] sDwTV^8XVn2]nvs1,^ > `*fi* D,^ > uaVV*** &D^,>u & as\D^,>u + `* * & * D^8u + as\D^,>u . `*fiu 2 * * & 1^ u 2 aq"n Q/\V\Sd ZS\]abC UYd Z]aToaVVasEu9W6TVf/] XVd Zo]ab/WaVVEd2kRCn2d W4UH1^u & `** &* D^ u . asK1^wTSn2n u8d Z/\mwxTVQS^&WQCX%] TVZSUD^8u+D^8u.D^ up /1 ^8u &&p* 1^8u +&p/+*1^,>u&2pD^8u&&1^ u2p7 +* 1 ^,>u &+p7 +*/* 1^,>u +++*+*/*^,>u. p7+7p 1^ u 2 ui+.a%SP/d ZCcfiW&u@W&bCX_tVW8X%U_U_QCkWi]abCX%]1^ !wTV^;XVn2nM evs/d ];k0Q/U_]LfgW&]ab/Wc4X%U_W]ab*X%]1^ u * 2VswTV^ff V^ i^i^i fiSq b`Q/U1^u T*fiffwXVcfi]aTV^_U5TVQ/]6TSw"]ab/W]abWQ*X%] TVZX%fgT tVWqEe]ab/W}cObCX%Z/\VW~TSwtX%^d2X%fCn W4U pA & s1 p7a + 2*(hX%ZCi UTSkW^_W4u^d ] ZS\Ssu9WU_W4W]ab*X%]a%SUW QCd tXVn W4Z`];]aT+0&&+ ++aVV&& &+ && &+ aY +0& ++ +0& ++ ui4ws wxTV^~U_TSkW,s;fgTV]abHW* & X%ZCi>v* + X%^_WZSTVZ/4W4^_TSs;]ab/W4Z]ab/W]abS^_W4WWQ*X%] TVZ/UTSwhaVVhbCX_tVW>Z/TU_TSn Q/] TVZSUhwTV^ [aV4Vaq9QCd tXVn W4Z] n esd2wwxTV^'U_TSkW8,s/fgTV]ab * & X%ZCi * + X%^_W8Z/TVZ/4W4^_TSsS]ab/W4Zl]ab/WwTVQ/^0W QCX%] TVZ/U'TSwa%SBb*XatVWZST5U_TSn Q/] TVZSU wTV^:p oaV4Vaq{PTld ];TVZCn e^_WkffXVd Z/U']aT5Ub/T u]abCX%] wTV^U_TSkW ,sSfgTV]ab * & X%ZCi * + X%^_W Z/TVZ/4W4^_TSq TjU_W4W]ab*d Us7Z/TV]aW8]ab*X%]0feoX%U_U_QCkR/] TVZ[wTV^'U_TSkW%s)`*UZSTV]q4W4A^_W4epn2d VWqCQ/]]ab/W4Zzd ]jwTSn2n uUw^_TSk ~/WkffkffXhr6q2X%fgTtVW5]abCX%]8fATV]ab * & X%ZCi * + X%^_WZ/TVZ/4W4^TSq bQ/Us]ab/W]ab/W4TV^Wk&&UR/^T tVWi/qzfiq| ?qr6n2cb/TVQ/^^CTV ZCsF0qD"q2sC{X% ^iW4ZCwTV^_UsFAq2s*$lX%1d Z/U_TVZCsr'qCaVVVVaq{-Z]ab/WYn TV\Sd2c0TSwA]ab/W4TV^_e}cb*X%Z/\VWR*X%^_] d2XVn-kW4W4]wxQ/Z*cfi] TVZ/UwxTV^cfiTVZ]a^XVcfi] TVZX%ZCi~^_W4t1d Ud TVZCq`%/-G[|/Mpm + E 4s10sVVSVVVq-X%^_py'd2n2n Wn2sq2s*$/XVn 1sL0qCaVVVVaq8PTSkW]aWX%U_W4^_U6cfiTVZCcfiW4^_Z*d Z/\mcfiTVZ*i/d ] TVZCXVnSR/^_TVfCX%fCd2n2d ] W4Uq*S)sEVV SVVVqLTtVW4^sq7lq2sAbSTSkffX%UsEqVr6qAaVVVVaq9ffp /'GZaph*%q1`_ud2n W4esUwBW4uzTV^_1qfi " |qxq}t~"}s1|LUd U2X% ^s-4q;aVVVVaq324piSd tVW4^_\VW4ZCcfiW\VW4TSkW4]a^_e|TSw-R/^_TVf*X%fCd2n2d ]eiSd U_]a^d f/Q/] TVZSUmX%ZCikffd ZCd2k5d X%] TVZRS^_TVfCn WkUq*x //+G:9 | +s54DaVasC%SSVVVqLUd U2X% ^s14qaVVVVaq6b`en WX%U_]{UQCX%^W4UYX%ZCi>kffX%1d2k0QCkW4Z]a^_TVR`e1r;ZX%1d TSkffX%] d2cjX%R/R/^_TSXVcOb>]aTZ*wxW4^_W4Z*cfiWYwxTV^ n2d Z/WX%^ ZtVW4^_U_W{RS^_TVfCn WkUq0x //+G:|/,as|E Sas*VVVSVVVVqrXau8d2iSsrhq?g qaVVVVaqSr|ZSTV]aW&TVZ~kffX%gd2k0QCkW4Z]a^_TVReX%ZCi6-X_eVW4Ud2X%ZlcfiTVZCiSd ] TVZCd Z/\Sq2q+"Z/R/QSfCn2d U_b/Wik5X%Z`Q/Ucfi^d R/] qrXau8d2iSsgrhqRgq2ssrd2cVW4es;qLqaVVVVaq~Cd VWn2d b/T`T1iX%ZCim-XaeVW4Ud2X%Zzd ZCwW4^_W4ZCcfiWwx^TSkU_Wn Wcfi] tVWn e^W4RATV^]aWii/X%] XVq0),SEG6xVhp %,4|/;x ,s[ 6EaVVVasL%SSVVVqrd2XVcfiTVZCd Us[g q2s" X%fgWn2n2s-P/q!E~ q"aVVVVaq}PTSkWmXVn ]aW4^_ZCX%] tVW4Uh]aT@X_eVW4U Uh^_QCn WqYfiZ-^_TSwkffX%ZCs@-q2s-u@W4ZCs8{qa"iUq2as3"fiff|/,S1 /on %[G|*,/|a 8n /ff*8 fi S}9+",/|hp sR/R*qAVSVVqr9Q/fgTSd Usr q2s1 ^XViWsy'qaVVVVaq r'Z$d Z`]a^_T1iQCcfi] TVZz]aTRgTVU_Ud f*d2n2d U_] d2cmX%ZCizwxQ/44evn TV\Sd2cfiUq fiZPVbCXVwxW4^s/{q2s7WX%^n2sg qa"iVUq2asgBy ,/ g %By ,//4sR/RCq)%SSVVVqAffTV^\SX%ZtX%QCwk5X%Z/ZCsgP/X%Zo^X%ZCc4d UcfiTSq^_W4QSZCi/sUq19qDaVVVVaqQ1 Q/4n W{TV^0RCX%^XViTgq0xVhp %4|/ )sR|E SasCVVVSq^d WiSkffX%ZCs_{q2sPVbCd2kTVZesr6qaVVVVaqE X_e`Z/W4UkffX%1d2kQCkW4Z`]a^TVR`eR/^W4Ucfi^d R/] TVZX%ZCi-R/^_TVf*X%fCd2n2d ]e]abSW4TV^_eqQ),SDG:|S,,)" _sF sDVVSVVVq^d WiSkffX%ZCs;w q2sC$y'XVn RAW4^ZCsF` q6qEaVVVVaq5T1iWn2d Z/\jfAWn2d Wwd Z[ieZCXVkffd2c&Ue`U_]aWkUqF9 X%^_]04DwTVQ/Z/piSX%] TVZ/Uq0x % K- Ca /+ S4sFd DaVas*VVSVVVq^d WiSkffX%ZCs4;w q2sy;XVn RgW4^_ZCs4` q46q`aVVVVaq15TgiVWn2d Z/\;fgWn2d Wwd ZieZCXVkffd2cDU_e`U]aWkUq9 X%^_]*_^_W4t1d Ud TVZX%Z*ihQ/RBi/X%]aWq),/DGmxQou2"y ,fi s2+ 0sCVVSVVVq{X%^iVZ/W4^s1lqDaVVVVaq|/,Sx|/ / K-xVhp ,- |ff6h~7hp "),/!Ion %,/aq;P/d2kTVZ} P/cOb`QSU_]aW4^sC"w W4uTV^_1q{X%^iVZ/W4^s1lqDaVVVVaq#0x |0@ +7 yV^_W4WkffX%ZL TSq{d2n2n2s6;q.'r q2s;tX%ZiW4^C~ XVX%ZCshq2s6 BTVfCd Z/Usm qaVVVVaq L TSX%^_U_W4ZCd ZS\X%]^X%ZCiTSk5 bCX%^XVcfip]aW4^d UX%] TVZ/Us&cfiTVZ/Wcfi]aQ/^_W4UX%ZCi}cfiTVQSZ`]aW4^_pW41XVkRCn W4UqjZJ"fi7VL,,|/+*8 /-,asRSRCq1VVSV%Sq-^_TtVWs7rhqq2sEy;XVn RgW4^_Z*sF qhqCaVVVVaq1 ^TVfCX%fCd2n2d ]eQSRiSX%]aW;cfiTVZCiSd ] TVZCd Z/\5tUqCcfi^_TVU_U_pW4Z]a^_TVReqfiZm"fi7*,, /z*8 fi /,sg %/mx , K-Ha / + /,z UxQ gG}RSRCq1VVSV%Sq-^QS Z`uXVn2i/sUgq1aVVVVaq;PV]a^_TVZ/\hW4Z`]a^TVR`e>cfiTVZCcfiW4Z]a^X%] TVZCs/\SXVkW{]ab/W4TV^_eX%ZCiXVn \VTV^d ]abCkffd2c-^X%Z*iTSkpZSW4U_Uq7qZ "fi7 S%/Wx0//)**8 /8o*|p;7)/ E ,//*,s`R/RCqVV SVVVqy'XVn RAW4^ZCs]qVhq2s7/X%\Sd ZCsA0qSaVVVVaqBffT1iWn2n2d Z/\8`ZST u8n Wi\VWhX%ZCi5XVcfi] TVZd ZliSd U_]a^d f/Q/]aWiU_e`U]aWkUqI%o )>*| p;7)S484ESas*VV SVVVq`_ufibc edbgfb]fbz}zhcyz1s1t yz|quy'XVn RAW4^ZCs`q/6q2sL Q/]a] n WslqC;qCaVVVVaqWt-Z/T u n Wi\VWsAR/^_TVfCX%f*d2n2d ]esLX%ZCimXVitVW4^_UX%^d W4Uq=`)%/6 hx8*s:9;0DSasCVVSVVVqy"Wd ]aX%ZCs[rq2s-Q/fCd Z*sHrq@aVVVVaql\VZ/TV^X%fCd2n2d ]e|X%Z*iocfiTSX%^_UWiSX%] XVqx/SYGx|S,as0sV%V/SVVVVqy"Q/Z`]aW4^s[r'q@aVVVVaqX%Q/UXVn2d ]eX%Z*ik5X%gd2k0QCk W4Z`]a^_TVRe}QSRiSX%] Z/\Sqa/,/S[`%/-Gx"7fi| phyB,//4<4EaVasEVV VSVVq4W4A^_W4esL0qq*aVVVVaq=1^TVfCX%fCn WZ/Tu8n Wi\VWq'fiZ~CX%)X%]aTVUsA4q*a"i/q2as!a/,/S0*X)4p\9,7mG\|/ S7* \"fi|ffp Ga /) n E 4s*RSRCqVVSVVVqV"w TV^_]ab/py"TSn2n2X%ZCi/srhkU]aW4^i/XVkffqt{n Wd ZfCX%QCkffs;rq9aVVVVaq|/)8nonKxS,8x|/ E ,//S7qmP] X%] U_] d2cfiU~d Z}]abSWly"WXVn ]abPSc4d W4ZCcfiW4UqEPVR/^d Z/\VW4^_p/=W4^n2X%\SsCBw W4uTV^_1qt-QCn2n f*XVc1sDP/q1aVVVVaq.a 8 8hp * ,%YSx|/ aqvd2n W4eqt-QCn2n f*XVc1s"P/q2sB ~ Wd fCn W4^s90qAr6qBaVVVVaql-Zd ZCwTV^kffX%] TVZX%Z*ijU_Q]vc4d W4ZCcfieqx//+6Gjph|/,as>6+6VsEVSVVq5TVU_]aWn2n W4^s&Lq"aVVVVaq*[*+ /S\"fi| +7hp ff"fi|, |m<|S)/_qjrhi/iSd U_TVZ/pW4Un W4esCBWXVi/d Z/\SsDlX%U_Uqw'd Wn U_W4ZCsP/q@aVVVVaqv*,, //l.yB/|p /8|/p )+"x0,|phaqDbCqhrqE]abSW4Ud Usr9W4RCX%^] kW4Z`];TSw b/W4TV^_W4] d2c4XVnEPV] X%] U_] d2cfiUs)BZCd tVW4^Ud ]eTSwRLTVRgW4Z/bCX%\VW4ZCqTVf*d Z/Us2q2s"TV]aZCd ]a4es1r6q2sB P/cObCX%^wU_]aWd ZCs2r qaVVVVaqPW4Z/Ud ] t1d ]eX%ZCXVn eUd UYwTV^U_Wn Wcfi] TVZofCd2X%UX%Z*i Q/ZCkWX%U_QS^_WiYcfiTVZ*wxTVQ/Z*i/d Z/\d Z}kffd U_Ud Z/\i/X%] XhX%ZCic4X%Q/UXVn1d ZCwxW4^W4ZCcfiWffkTgiVWn UqDfiZ}y'XVn2n TV^slq1"q2s1 CW4^_^esUr'q1a9iUq2as+|/,,E +1Affph+ q*m"]nfi]ph S9S*+/,;S%+as/5r?=TSn QCkWYVVVs/R/RCq1SVVqEPVR/^d Z/\VW4^_p/=W4^n2X%\SqQSfCd ZCsYrqDaVVVVaqBfiZCwW4^_W4ZCcfiW5X%ZCi~kffd U_Ud Z/\~iSX%] XVqQ-p , Vs8@+4sEVVSVVVqP/cObCX%^wU_]aWd ZCs4rq`{q2s?rX%ZCd Wn Usq2sTVfCd ZSUs\`q`lqaVVVVaqZCcfiTV^RATV^X%] Z/\0R/^d TV^CfgWn2d WwU-X%fgTVQ/]CU_W4pn Wcfi] TVZfCd2X%Ud Z`]aT ]ab/W8X%Z*XVn e`Ud U@TSw`^X%ZCiTSk5d 4Wi]a^d2XVn U@u8d ]ablk5d U_Ud Z/\{TVQS] cfiTSkW4UHq -|ph %o qTX%R/RgWX%^qPWd2iW4Z*wxWn2i/s q1aVVVVaq;DZ`]a^_TVRe>X%ZCihQ/Z*cfiW4^_] XVd Z`]eq9+,7G:|S /,4s++4sSVVSVVqPbCXVwW4^s/qSaVVVVaqHxffp ,R*%6G#Hn /,4qR1^d ZCcfiW4]aTVZN"ZCd tVW4^_Ud ]e1^_W4UUsD^d ZCcfiW4p]aTVZ*Cw;hq `qPbCXVwW4^s{q)aVVVVaqLTVZCi/d ] TVZCXVnRS^_TVfCX%fCd2n2d ]eqa/ %//|/]yB7n*s;+4EaVas7VVSVVVqPb/TV^Ws0`q"q2s; 4TVbSZ/U_TVZCs'0qL q0aVVVVaqr'1d TSkffX%] d2ciVW4^d t)X%] TVZTSw9]ab/WR/^d ZCc4d RCn WTSwkffX%1d pk0QCk$W4Z`]a^TVR`eoX%Z*i6]ab/W R/^d ZCc4d R*n W{TSwBkffd ZCd2k5d2kQCk cfi^_TVU_U_pW4Z]a^_TVRequHK S//8ph *,,Rs 7*A6+@EaVas*VSVVqPe`^kUs-q;aVVVVaqX%1d2k0QCk|SS ,48@14sDV S%SqW4Z`]a^_TVRed ZCwW4^_W4ZCcfiW[X%U>XU_RgWc4d2XVn'c4X%U_WTSwcfiTVZCi/d ] TVZCXVn2d X%] TVZCq`_7fi " |qxq}t~"}s1|KvffZ/1sqSaVVVVaq b/WcfiTVZ/U]a^XVd Z`]@^_QCn W0TSw]abSW8kffX%1d2kQCkW4Z`]a^TVR`e6R/^d ZCc4d RCn Wqe|/) ;%/x"+AGh~ %4" _s>6g,s/SSVVqtX%ZXVkRgW4Z/b/TVQ/] sq2sLLTtVW4^s qLaVVVVaqjX%1d2k0QCkW4Z]a^_TVR`e[X%ZCicfiTVZCi/d ] TVZ*XVnAR/^_TVfCX%fCd2n2d ]equHKSSSa8 p *%s7* 6gSas/SVVSVVqtX%ZV^XVX%U_U_W4ZCsg-qEq7aVVVVaqErR/^TVfCn WkwTV^-^_Wn2X%] tVW5d ZCwTV^kffX%] TVZ>kffd ZCd2k5d 4W4^_UqH-%`)%/{3"7j6 :|/ S484+6sDVV SVVVqtVTVU PSXatX%Z`] sCqDaVV%Saq.x',h~%)qPV] q1X%^_] ZSU lX%U_U lX%^_VW4]LX%RgW4^_fCXVcOgqtVTVU6P/XatX%Z`] s*lqDaX_eoVVsEVVVVaq&r;U_>lX%^d2n eZCq9fi5 /4q b/W4^_Wu9W4^_WXVn U_TlwTSn2n u0Q/RX%^] d2c4n W4UdZ "fio~/6TVZ r9Wc4q;VsVVVVslX%^cb VVsVVVVsZ4Q*n eVVsVVVVs8X%ZCicfi]aTVfAW4^hVVsDVVVVqtVTVUP/X_t)X%Z] sq&aPVW4R/] q&Vs;VVVVaqr'U_X%^d2n e`ZCqJ9m~/4s'VVqzTSn2n u0pQ/RX%^_] d2c4n W4UX%RSRAWX%^Wid cZ "ff~/&TVZ r9Wc4qEVsDVVVYRCq1VV0X%Z*iW4fCqDVVsEVVV5RCqDVVaq`_ufiJournal Artificial Intelligence Research 19 (2003) 25-71Submitted 10/02; published 08/03Answer Set Planning Action CostsThomas EiterWolfgang FaberEITER @ KR . TUWIEN . AC .FABER @ KR . TUWIEN . AC .Institut fur Informationssysteme, TU WienFavoritenstr. 9-11, A-1040 Wien, AustriaNicola LeoneLEONE @ UNICAL .Department Mathematics, University CalabriaI-87030 Rende (CS), ItalyGerald PfeiferAxel PolleresPFEIFER @ DBAI . TUWIEN . AC .POLLERES @ KR . TUWIEN . AC .Institut fur Informationssysteme, TU WienFavoritenstr. 9-11, A-1040 Wien, AustriaAbstractRecently, planning based answer set programming proposed approach towards realizing declarative planning systems. paper, present language K c ,extends declarative planning language K action costs. K c provides notion admissible optimal plans, plans whose overall action costs within given limit resp.minimum plans (i.e., cheapest plans). demonstrate, novel language allowsexpressing nontrivial planning tasks declarative way. Furthermore, utilizedrepresenting planning problems optimality criteria, computing shortest plans(with least number steps), refinement combinations cheapest fastest plans.study complexity aspects language K c provide transformation logic programs,planning problems solved via answer set programming. Furthermore, report experimental results selected problems. experience encouraging answer set planning mayvaluable approach expressive planning systems intricate planning problemsnaturally specified solved.1. IntroductionRecently, several declarative planning languages formalisms introduced, allowintuitive encoding complex planning problems involving ramifications, incomplete information, non-deterministic action effects, parallel actions (see e.g., Giunchiglia & Lifschitz, 1998;Lifschitz, 1999b; Lifschitz & Turner, 1999; McCain & Turner, 1998; Giunchiglia, 2000; Cimatti &Roveri, 2000; Eiter et al., 2000b, 2003b).systems designed generate plans accomplish planning goals,practice one often interested particular plans optimal respect objectivefunction quality (or cost) plan measured. common simple objectivefunction length plan, i.e., number time steps achieve goal. Many systemstailored compute shortest plans. example, CMBP (Cimatti & Roveri, 2000) GPT(Bonet & Geffner, 2000) compute shortest plans step consists single action,Graphplan algorithm (Blum & Furst, 1997) descendants (Smith & Weld, 1998; Weld,c2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiE ITER , FABER , L EONE , P FEIFER & P OLLERESAnderson, & Smith, 1998) compute shortest plans step actions might executedparallel.However, other, equally important objective functions consider. particular,executing actions causes cost, may desire plan minimizes overall costactions.answer set planning (Subrahmanian & Zaniolo, 1995; Dimopoulos, Nebel, & Koehler, 1997;Niemela, 1998; Lifschitz, 1999b), recent declarative approach planning plans encoded answer sets logic program, issue optimal plans objective valuefunction addressed detail far (see Section 8 details). paper,address issue present extension planning language K (Eiter et al., 2000b, 2003b),user may associate costs actions, taken account planningprocess. main contributions work follows.define syntax semantics planning language Kc , modularly extendslanguage K: Costs associated action extending action declarationsoptional cost construct describes cost executing respective action.action costs static dynamic, may depend current stage planaction considered execution. Dynamic action costs importantnatural applications, show simple variant well-known Traveling SalespersonProblem, cumbersome model solve other, similar languages.analyze computational complexity planning language K c , provide completeness results major planning tasks propositional setting, locatesuitable slots Polynomial Hierarchy classes derived it. results provide insight intrinsic computational difficulties respective planning problems,give handle efficient transformations optimal planning knowledge representation formalisms, particular logic programs.show, awareness results complexity analysis, planning actioncosts implemented transformation answer set programming, done system prototype developed. prototype, ready experiments, availablehttp://www.dlvsystem.com/K/.Finally, present applications show extended language capableeasily modeling optimal planning various criteria: computing (1) cheapest plans(which minimize overall action costs); (2) shortest plans (with least number steps);and, refinement combinations these, viz. (3) shortest plans among cheapest, (4)cheapest plans among shortest. Notice that, knowledge, task (3)addressed works far.extension K action costs provides flexible expressive tool representingvarious problems. Moreover, since Ks semantics builds states knowledge ratherstates world, deal incomplete knowledge plan quality, is,best knowledge, completely novel.experience encouraging answer set planning, based powerful logic programmingengines, allows development declarative planning systems intricate planning26fiA NSWER ET P LANNING U NDER ACTION C OSTStasks specified solved. work complements extends preliminary resultspresented previous work (Eiter et al., 2002a).remainder paper organized follows. next section, briefly reviewlanguage K informally presenting main constituents features simple planningexample. that, define Section 3 extension K action costs, considerfirst examples usage Kc . Section 4 devoted analysis complexity issues.Section 5, consider applications K c . show various types particular optimizationproblems expressed K c , also consider practical examples. Section 6,present transformation K c answer set programming, Section 7, reportprototype implementation experiments. discussion related work Section 8,conclude paper outlook ongoing future work.2. Short Review Language Ksection, give brief informal overview language K, refer (Eiter et al., 2003b)Appendix formal details. assume reader familiar basic ideasplanning action languages, particular notions actions, fluents, goals plans.illustration, shall use following planning problem running example.Problem 1 [Bridge Crossing Problem] Four persons want cross river night plankbridge, hold two persons time. lamp, must usedcrossing. pitch-dark planks missing, someone must bring lamp backothers; tricks (like throwing lamp halfway crosses, etc.) allowed.Fluents states. state K characterized truth values fluents, describing relevantproperties domain discourse. fluent may true, false, unknown state is,states K states knowledge, opposed states world fluent either truefalse (which easily enforced K, desired). Formally, state consistent set(possibly negated) legal fluent instances.action applicable precondition (a list literals fluents) holdscurrent state. execution may cause modification truth values fluents.Background knowledge. Static knowledge invariant time K planning domainspecified normal (disjunction-free) Datalog program single answer setviewed set facts. example, background knowledge specifies four persons:person(joe). person(jack). person(william). person(averell).Type declarations. fluent action must declaration ranges arguments specified. instance,crossTogether(X, Y) requires person(X), person(Y), X < Y. 1specifies arguments action crossTogether, two persons cross bridge together,across(X) requires person(X).1. < used instead inequality avoid symmetric rules.27fiE ITER , FABER , L EONE , P FEIFER & P OLLERESspecifies fluent describing specific person side river. literalsrequires must classical literals static background knowledge (like person(X)person(Y)), literals built-in predicates (such X < Y). implementation K, DLV K system (Eiter, Faber, Leone, Pfeifer, & Polleres, 2003a), currently supports built-in predicates< B, <= B, != B obvious meaning less-than, less-or-equal inequalitystrings numbers, arithmetic built-ins = B + C = B C stand integeraddition multiplication, predicate #int(X) enumerates integers (upuser-defined limit).Causation rules. Causation rules (rules brevity) syntactically similar rulesaction language C (Giunchiglia & Lifschitz, 1998; Lifschitz, 1999a; Lifschitz & Turner, 1999)basic form:caused f B A.conjunction fluent action literals, possibly including default negation, Bconjunction fluent literals, possibly including default negation, f fluent literal.Informally, rule reads: B known true current state known trueprevious state, f known true current state well. if-partafter-part allowed empty (which means true). causation rule calleddynamic, after-part empty, called static otherwise.Causation rules used express effects actions ramifications. example,caused across(X) cross(X), -across(X).caused -across(X) cross(X), across(X).describe effects single person crossing bridge either direction.Initial state constraints. Static rules apply states initial states (whichmay unique). expressed keywords always : initially : precedingsequences rules latter describes initial state constraints must satisfiedinitial state. example,initially : caused -across(X).enforces fluent across false initial state X satisfying declarationfluent across, i.e., persons. rule irrelevant subsequent states.Executability actions.expressed K explicitly. instance,executable crossTogether(X, Y) hasLamp(X).executable crossTogether(X, Y) hasLamp(Y).declares two persons jointly cross bridge one lamp. actionmay multiple executability statements. statementexecutable cross(X).empty body says cross always executable, provided type restrictions Xrespected. Dually,nonexecutable B.prohibits execution action condition B satisfied. example,nonexecutable crossTogether(X, Y) differentSides(X, Y).28fiA NSWER ET P LANNING U NDER ACTION C OSTSsays persons X cross bridge together different sides bridge.case conflicts, nonexecutable overrides executable A.Default strong negation. K supports strong negation (, also written -). Note, however, fluent f, state neither f -f needs hold. case knowledgef incomplete. addition, weak negation (not), interpreted like default negation answer setsemantics (Gelfond & Lifschitz, 1991), permitted rule bodies. allows natural modeling inertia default properties, well dealing incomplete knowledge general.example,caused hasLamp(joe) hasLamp(jack), hasLamp(william), hasLamp(averell).expresses conclusion default, joe lamp, whenever evidentpersons it.Macros.K provides number macros syntactic sugar. example,inertial across(X).informally states across(X) holds current state, across(X) held previous state,unless -across(X) explicitly known hold. macro expands rulecaused across(X) -across(X) across(X).Moreover, totalize knowledge fluent declaring total f. shortcutcaused f -f.caused -f f.intuitive meaning rules unless truth value f derived, casesf resp. -f true considered.Planning domains problems. K, planning domain PD = h, hD, Rii backgroundknowledge , action fluent declarations D, rules executability conditions R; planningproblem P = hPD, qi planning domain PD queryq = g1 , . . . , gm , gm+1 , . . . , gn ? (l)g1 , . . . , gn ground fluents l 0 plan length. instance, goal queryacross(joe), across(jack), across(william), across(averell)? (5)asks plans bring four persons across 5 steps.Plans defined using transition-based semantics, execution set actionstransforms current state new state. (optimistic) plan P sequence P = hA 1 , . . . , Alsets action instances A1 , A2 , . . . , Al trajectory = hhs0 , A1 , s1 i, hs1 , A2 , s2 i, . . . ,hsl1 , Al , sl ii legal initial state s0 state sl literals goal true.is, starting s0 , legal transition t1 = hs0 , A1 , s1 i, modeling execution actions 1(which must executable), transforms 0 state s1 . followed legal transitionsti = hsi1 , Ai , si i, = 2, 3, . . . , l (cf. Appendix details). plan sequential, |A | 1= 1, . . . , l, i.e., step consists one action; plans enforcedincluding keyword noConcurrency.Besides optimistic plans, K also support stronger secure (or conformant) plans. secureplan must guaranteed work circumstances (Eiter et al., 2000b), regardlessincomplete information initial state possible nondeterminism action effects.29fiE ITER , FABER , L EONE , P FEIFER & P OLLERESbetter readability, following always describe K planning problems Pstrictly terms sets declarations, rules executability conditions, optionally usecompact representation K programs following general form:fluents :actions :initially :always :goal :FDADIRCRq(optional) sections fluents always consist lists fluent declarations F ,action declarations AD , initial state constraints IR executability conditions causation rulesCR , respectively. Together background knowledge goal query q, specifyK planning problem P = hh, hD, Rii, qi, given F plus AD R IR plusCR . 22.1 Solving Bridge Crossing ProblemUsing constructs, K encoding Bridge Crossing Problem, assuming joeinitially carries lamp, shown Figure 1. simple five-step plans (l = 5),joe always carries lamp brings others across. One is:P = h {crossTogether(joe, jack)}, {cross(joe)}, {crossTogether(joe, william)},{cross(joe)}, {crossTogether(joe, averell)}3. Actions CostsUsing language K system prototype, DLV K , already express solveinvolved planning tasks, cf. (Eiter et al., 2003b). However, K DLV K alone offer meansfinding optimal plans objective cost function. general, different criteria planoptimality relevant, optimality wrt. action costs shown next example,slight elaboration Bridge Crossing Problem, well-known brain teasing riddle:Problem 2 [Quick Bridge Crossing Problem] persons bridge crossing scenario needdifferent times cross bridge, namely 1, 2, 5, 10 minutes, respectively. Walking twoimplies moving slower rate both. possible four persons get across within 17minutes?first thought infeasible, since seemingly optimal plan joe, fastest,keeps lamp leads others across takes 19 minutes altogether. Surprisingly,see, optimal solution indeed takes 17 minutes.order allow elegant convenient encoding optimization problems,extend K language K c one assign costs actions.3.1 Syntax KcLet act , f l , var denote (finite) sets action names, fluent names variable symbols.Furthermore, let Lact , Lf l , Ltyp denote sets action, fluent, type literals, respectively,2. also format input files system prototype, presented Section 7.30fiA NSWER ET P LANNING U NDER ACTION C OSTSactions :cross(X) requires person(X).crossTogether(X, Y) requires person(X), person(Y), X < Y.takeLamp(X) requires person(X).fluents :across(X) requires person(X).differentSides(X, Y) requires person(X), person(Y).hasLamp(X) requires person(X).initially : -across(X). hasLamp(joe).always :executable crossTogether(X, Y) hasLamp(X).executable crossTogether(X, Y) hasLamp(Y).nonexecutable crossTogether(X, Y) differentSides(X, Y).executable cross(X) hasLamp(X).executable takeLamp(X).nonexecutable takeLamp(X) hasLamp(Y), differentSides(X, Y).causedcausedcausedcausedacross(X) crossTogether(X, Y),across(Y) crossTogether(X, Y),-across(X) crossTogether(X, Y),-across(Y) crossTogether(X, Y),-across(X).-across(Y).across(X).across(Y).caused across(X) cross(X), -across(X).caused -across(X) cross(X), across(X).caused hasLamp(X) takeLamp(X).caused -hasLamp(X) takeLamp(Y), X != Y, hasLamp(X).caused differentSides(X, Y) across(X), -across(Y).caused differentSides(X, Y) -across(X), across(Y).inertial across(X).inertial -across(X).inertial hasLamp(X).noConcurrency.goal :across(joe), across(jack), across(william), across(averell)? (l)Figure 1: K encoding Bridge Crossing Problemformed action names, fluent names, predicates background knowledge (includingbuilt-in predicates), respectively, using terms nonempty (finite) set constants con .Kc extends action declarations K costs follows.Definition 3.1 action declaration K c form:p(X1 , . . . , Xn ) requires t1 , . . . , tm costs C c1 , . . . , ck .(1)(1) p act arity n 0, (2) X1 , . . . , Xn var , (3) t1 , . . . , tm , c1 , . . . , ckLtyp every Xi occurs t1 , . . . , tm , (4) C either integer constant, variableset variables occurring t1 , . . . , tm , c1 , . . . , ck (denoted var (d)), distinguishedvariable time, (5) var (d) var {time}, (6) time occur 1 , . . . tm .31fiE ITER , FABER , L EONE , P FEIFER & P OLLERES= 0, keyword requires omitted; k = 0, keyword omittedcosts C optional. Here, (1) (2) state parameters action must variables,fixed values. Informally, (3) means parameters action must typedrequires part. Condition (4) asserts cost locally defined given stageplan, referenced global variable time. Conditions (5) (6) ensurevariables known type information action parameters static, i.e., dependtime.Planning domains planning problems K c defined K.example, elaborated Bridge Crossing Problem, declaration cross(X)extended follows: suppose predicate walk(Person, Minutes) background knowledgeindicates Person takes Minutes cross. Then, may simply declarecross(X) requires person(X) costs WX walk(X, WX).3.2 Semantics KcSemantically, Kc extends K cost values actions points time. plan P =hA1 , . . . , Al i, step 1 l, actions Ai executed reach time point i.ground action p(x1 , . . . , xn ) legal action instance action declaration wrt. K cplanning domain PD = h, hD, Rii, exists ground substitution var (d){time} Xi = xi , 1 n {t1 , . . . , tm } , unique answerset background knowledge . called witness substitution p(x 1 , . . . , xn ).Informally, action instance legal, satisfies respective typing requirements. Action costsformalized follows.Definition 3.2 Let = p(x1 , . . . , xn ) legal action instance declaration form (1)let witness substitution a.costs part empty;0,cost (p(x1 , . . . , xn )) =val(C), {c1 , . . . , ck } ;undefined otherwise.unique answer set val : con defined integer valueinteger constants 0 non-integer constants.reference variable time, possible define time-dependent action costs; shall consider example Section 5.2. Using cost , introduce well-defined legal action instancesdefine action cost values follows.Definition 3.3 legal action instance = p(x 1 , . . . , xn ) well-defined iff holds (i)time point 1, witness substitution time = cost (a)integer, (ii) cost (a) = cost0 (a) holds two witness substitutions , 0 coincidetime defined costs. well-defined a, unique cost time point 1 givencosti (a) = cost (a) (i).definition, condition (i) ensures cost value exists, must integer,condition (ii) ensures value unique, i.e., two different witness substitutions0 evaluate cost part integer cost value.32fiA NSWER ET P LANNING U NDER ACTION C OSTSaction declaration well-defined, legal instances well-defined.fulfilled if, database terms, variables X1 , . . . , Xn together time (1) functionally determine value C. framework, semantics K c planning domain PD = h, hD, Riiwell-defined well-defined action declarations PD. rest paper, assumewell-definedness Kc unless stated otherwise.Using costi , define costs plans.Definition 3.4 Let P = hPD, Q ? (l)i planning problem. Then, plan P = hA 1 , . . . , AlP, cost definedcostP (P ) =Plj=1Pcost(a).jaAjplan P optimal P, costP (P ) costP (P 0 ) plan P 0 P, i.e., P least costamong plans P. cost planning problem P, denoted cost P , given costP =costP (P ), P optimal plan P.particular, costP (P ) = 0 P = hi, i.e., plan void. Note cost P definedplan P exists.3Usually one estimate upper bound plan length, know exactlength optimal plan. Although defined optimality fixed plan length l,see Section 5.1 appropriate encodings extended optimality planslength l.Besides optimal plans, also plans bounded costs interest, motivates following definition.Definition 3.5 plan P planning problem P admissible wrt. cost c, cost P (P ) c.Admissible plans impose weaker condition plan quality optimal plans.particularly relevant optimal costs crucial issue, long cost stays within givenlimit, optimal plans difficult compute. might face questions like makeairport within one hour?, enough change buy coffee? etc. amountadmissible planning problems. shall see, computing admissible plans complexity-wiseeasier computing optimal plans.3.3 Optimal Solution Quick Bridge Crossing Problemmodel Quick Bridge Crossing Problem K c , first extend background knowledgefollows, predicate walk describes time person needs cross max determinestwo persons slower:walk(joe, 1). walk(jack, 2). walk(william, 5). walk(averell, 10).max(A, B, A) :- walk( , A), walk( , B), >= B.max(A, B, B) :- walk( , A), walk( , B), B > A.Next, modify declarations cross crossTogether Figure 1 adding costs:3. following, subscripts dropped clear context.33fiE ITER , FABER , L EONE , P FEIFER & P OLLEREScross(X) requires person(X) costs WX walk(X, WX).crossTogether(X, Y) requires person(X), person(Y), X <costs Wmax walk(X, WX), walk(Y, WY), max(WX, WY, Wmax).declaration takeLamp remains unchanged, time hand lamp negligible.Using modified planning domain, 5-step plan reported Section 2.1 cost 19. Actually, optimal plan length l = 5. However, relinquish first intuitionfastest person, joe, always lamp consider problem varying plan length,find following 7-step plan:P = h {crossTogether(joe, jack)}, {cross(joe)}, {takeLamp(william)},{crossTogether(william, averell)}, {takeLamp(jack)}, {cross(jack)},{crossTogether(joe, jack)}Here, costP (P ) = 17, thus P admissible respect cost 17. means QuickBridge Crossing Problem positive answer. fact, P least cost plans lengthl = 7, thus optimal 7-step plan. Moreover, P also least cost plans emergeconsider plan lengths. Thus, P optimal solution Quick Bridge CrossingProblem arbitrary plan length.3.4 Bridge Crossing Incomplete Knowledgelanguage K well-suited model problems involve uncertainty incompleteinitial states non-deterministic action effects qualitative level. enriched language K cgracefully extends secure (conformant) plans well, must reach goal circumstances (Eiter et al., 2000b, 2003b). precisely, optimistic plan hA 1 , . . . , secure,applicable evolution system: starting legal initial state 0 , firstaction set A1 (for plan length l 1) always executed (i.e., legal transition hs 0 , A1 , s1exists), every possible state 1 , next action set A2 executed etc.,performed actions, goal always accomplished (cf. Appendix formal definition).secure plans inherit costs optimistic plans, different possibilities defineoptimality secure plans. may consider secure plan optimal, least cost eitheramong optimistic plans,among secure plans only.first alternative, might planning problems secure plans, optimalsecure plans. reason, second alternative appears appropriate.Definition 3.6 secure plan P optimal planning problem P, least cost amongsecure plans P, i.e., costP (P ) costP (P 0 ) secure plan P 0 P. secure costP, denoted costsec (P), costsec (P) = costP (P ), P optimal secure plan P.notion admissible secure plans defined analogously.example, assume known least one person bridge scenario lamp,neither exact number lamps allocation lamps persons known.four desperate persons ask plan brings safely across bridge, need(fast) secure plan works possible initial situations. K c , modeledreplacing initially-part following declarations:34fiA NSWER ET P LANNING U NDER ACTION C OSTSinitially : total hasLamp(X).caused false -hasLamp(joe), -hasLamp(jack),-hasLamp(william), -hasLamp(averell).first statement says person either lamp not, second leastone must lamp. detailed discussion use total statementmodeling incomplete knowledge non-determinism refer (Eiter et al., 2003b).easily see, optimal secure solution take least 17 minutes, since originalcase (where joe lamp) one possible initial situations, costoptimistic plan optimal plan lengths 17. However, secure planoptimal plan lengths requires least 8 steps (but higher cost): Differentoptimistic plans, need one extra step beginning makes sure onewalk first (above, joe jack) lamp, effected proper takeLamp action.example plan following cost 17:P = h {takeLamp(joe)}, {crossTogether(joe, jack)}, {cross(joe)},{takeLamp(william)}, {crossTogether(william, averell)}, {takeLamp(jack)},{cross(jack)}, {crossTogether(joe, jack)}easily check P works every possible initial situation. Thus, optimal (secure)plan plan length 8, moreover also arbitrary plan length.4. Computational Complexitysection, address computational complexity K c , complementing similar resultslanguage K (Eiter et al., 2003b).4.1 Complexity Classesassume reader familiar basic notions complexity theory, P, NP,problem reductions completeness; see e.g. (Papadimitriou, 1994) references therein.recall Polynomial Hierarchy (PH) contains classes P0 = P0 = P0 = P Pi+1 =PPNPi , Pi+1 = co-Pi+1 , Pi+1 = Pi , 0. particular, P1 = NP P2 = PNP .Note classes contain decision problems (i.e., problems answer yesno). checking well-definedness deciding plan existence problems, computingplan search problem, problem instance (possibly empty) finite set S(I)solutions exists. solve problem, (possibly nondeterministic) algorithm must computealternative solutions set computation branches, S(I) empty. precisely,search problems solved transducers, i.e., Turing machines equipped output tape.machine halts accepting state, contents output tape resultcomputation. Observe nondeterministic machine computes (partial) multi-valued function.analog NP, class NPMV contains search problems S(I) computed nondeterministic Turing machine polynomial time; precise definition, see (SelPman, 1994). analogy Pi+1 , Pi+1 MV = NPMVi , 0, denote generalizationNPMV machine access Pi oracle.Analogs classes P Pi+1 , 0, given classes FP F Pi+1 , 0,contain partial single-valued functions (that is, |S(I)| 1 problem instance35fiE ITER , FABER , L EONE , P FEIFER & P OLLERESI) computable polynomial time using resp. Pi oracle. say, abusing terminology,search problem FP (resp. FPi+1 ), partial (single-valued) function f FP(resp. f FPi+1 ) f (I) S(I) f (I) undefined iff S(I) = . example,computing satisfying assignment propositional CNF (FSAT) computing optimal tourTraveling Salesperson Problem (TSP) F P2 view, cf. (Papadimitriou, 1994).partial function f polynomial-time reducible another partial function g,polynomial-time computable functions h 1 h2 f (I) = h2 (I, g(h1 (I)))g(h1 (I)) defined whenever f (I) defined. Hardness completeness defined usual.4.2 Problem Settingfocus following questions:Checking Well-Definedness: Decide whether given action description well-defined wrt.given planning domain PD, resp. whether given planning domain PD well-defined.Admissible Planning: Decide whether planning problem P admissible (optimistic/secure)plan exists wrt. given cost value c, find plan.Optimal Planning: Find optimal (optimistic/secure) plan given planning problem.Notice (Eiter et al., 2003b) focused deciding existence optimistic/secure plans,rather actually finding plans, presented detailed study complexity taskvarious restrictions ground (propositional) planning problems. paper, confinediscussion case planning problems P = hPD, Q ? (l)i look polynomial lengthplans, i.e., problems plan length l bounded polynomial size input.shall consider mainly ground (propositional) planning, assume planningdomains well-typed unique model background knowledge computedpolynomial time. general case, well-known complexity results logic programming,cf. (Dantsin, Eiter, Gottlob, & Voronkov, 2001), already evaluating background knowledgeEXPTIME-hard, problems thus provably intractable. recall following results,appear (or directly follow from) previous work (Eiter et al., 2003b).Proposition 4.1 Deciding, given propositional planning problem P sequence P = hA 1 , . . . ,Al action sets, (i) whether given sequence = ht 1 , . . . , tl legal trajectory witnessingP optimistic plan P feasible polynomial time, (ii) whether P secure planP P2 -complete.4.3 Resultsstart considering checking well-definedness. problem, interesting investigatenon-ground case, assuming background knowledge already evaluated. wayassess intrinsic difficulty task obtaining following result.Theorem 4.2 (Complexity checking well-definedness) Given K c planning domain PD =h, hD, Rii unique model , checking (i) well-definedness given action declaration form (1) wrt. PD (ii) well-definedness PD P2 -complete.36fiA NSWER ET P LANNING U NDER ACTION C OSTSProof. Membership: (i), violated nonempty costs part legal actioninstance = p(x1 , . . . , xn ) either (1) exist witness substitutions 0time = time 0 , cost (a) = val(C) cost0 (a) = val(C 0 ), val(C) 6= val(C 0 ),(2) witness substitution cost (a) = val(C) integer.guessed checked, via witness substitution, polynomial time, alongalso 0 (1); note that, definition, variables must substituted constantsbackground knowledge (including numbers), must values time occursc1 , . . . , ck . Given a, decide (2) help NP oracle. summary, disproving welldefinedness nondeterministically possible polynomial time NP oracle. Hence,checking well-definedness co-P2 = P2 . membership part (ii) follows (i),since well-definedness PD reduces well-definedness action declarations it, P2closed conjunctions.Hardness: show hardness (i) reduction deciding whether quantified Booleanformula (QBF)Q = XY.c1 ckci = Li,1 Li,`i , = 1, . . . , k, disjunction literals L i,j atomsX = x1 , . . . , xn = xn+1 . . . , xm , true. Without loss generality, may assumeci contains three (not necessarily distinct) literals, either positive negative.construct planning domain PD follows. background knowledge, ,bool(0). bool(1).pos(1, 0, 0). pos(0, 1, 0). pos(0, 0, 1). pos(1, 1, 0). pos(1, 0, 1). pos(0, 1, 1). pos(1, 1, 1).neg(0, 0, 0). neg(1, 0, 0). neg(0, 1, 0). neg(0, 0, 1). neg(1, 1, 0). neg(1, 0, 1). neg(0, 1, 1).Here, bool declares truth values 0 1. facts pos(X 1 , X2 , X3 ) neg(X1 , X2 , X3 ) statetruth assignments X1 , X2 , X3 positive clause X1 X2 X3 resp.negative clause X1 X2 X3 satisfied.rest planning domain PD consists single action declaration formp(V1 , ..., Vn ) requires bool(V1), ..., bool(Vn) costs 0 c1 , ..., ck .ci=pos(Vi,1 , Vi,2 , Vi,3 ), ci = xi,1 xi,2 xi,3 ,neg(Vi,1 , Vi,2 , Vi,3 ), ci = xi,1 xi,2 xi,3 ,= 1, . . . , k.example, clause c = x1 x3 x6 mapped c = pos(V1 , V3 , V6 ). easy seelegal action instance = p(b1 , . . . , bn ) corresponds 1-1 truth assignment Xgiven (xi ) = bi , = 1, . . . , n. Furthermore, cost value defined (which 0) iffformula (c1 ck ) true. Thus, well-defined wrt. PD iff Q true. Since PD2efficiently constructible, proves P2 -hardness.Observe ground case, checking well-definedness much easier. Since substitutions need guessed, test proof Theorem 4.2 polynomial. Thus, assumptionefficient evaluation background program, obtain:Corollary 4.3 ground (propositional) case, checking well-definedness action description wrt. Kc planning domain PD = h, hD, Rii, resp. PD whole, possiblepolynomial time.37fiE ITER , FABER , L EONE , P FEIFER & P OLLERESremark checking well-definedness expressed planning task K, alsologic program; refer (Eiter, Faber, Leone, Pfeifer, & Polleres, 2002b) details.turn computing admissible plans.Theorem 4.4 (Complexity admissible planning) polynomial plan lengths, deciding whethergiven (well-defined) propositional planning problem hPD, qi (i) optimistic admissibleplan wrt. given integer b NP-complete, finding plan complete NPMV, (ii)deciding whether hPD, qi secure admissible plan wrt. given integer b P3 -complete,computing plan P3 MV-complete. Hardness holds cases fixed plan length.proof refer Appendix. finally address complexity computingoptimal plans.Theorem 4.5 (Complexity optimal planning) polynomial plan lengths, (i) computingoptimal optimistic plan hPD, Q ? (l)i K c FP2 -complete, (ii) computing optimalsecure plan hPD, Q ? (l)i K c FP4 -complete. Hardness holds cases even planlength l fixed.proof found Appendix.remark case unbounded plan length, complexity computing plans increases requires (at least) exponential time general, since plans might exponential lengthsize planning problem. Thus, practical terms, constructing plans infeasible,since occupy exponential space. Furthermore, follows previous results (Eiter et al.,2003b), deciding existence admissible optimistic resp. secure plan planning problem wrt. given cost PSPACE-complete resp. NEXPTIME-complete. leave detailedanalysis complexity aspects K c work.5. Applications5.1 Cost Efficient versus Time Efficient Planssection, show language K c used minimize plan length combinationminimizing costs plan. especially interesting problem settings parallelactions allowed (cf. (Kautz & Walser, 1999; Lee & Lifschitz, 2001)).domains parallel actions, Kautz Walser propose various criteria optimized, instance number actions needed, number necessary time stepsparallel actions allowed, well combinations two criteria (1999). exploitingaction costs proper modeling, solve optimization problems sort. example,single plans minimal number actions simply assigning cost 1 possibleactions.consider following optimization problems:() Find plan minimal cost (cheapest plan) given number steps.() Find plan minimal time steps (shortest plan).() Find shortest among cheapest plans.38fiA NSWER ET P LANNING U NDER ACTION C OSTS() Find cheapest among shortest plans.Problem () already defined optimal plans far. showexpress () terms optimal cost plans well, extend elaboration respectcombinations () ().5.1.1 C HEAPEST P LANSG IVEN P LAN L ENGTH ()guiding example, refer Blocks World parallel moves allowed, apartfinding shortest plans also minimizing total number moves issue. Kc encodingdomain, plans serializable, shown Figure 2. Serializability means parallelactions non-interfering executed sequentially order, i.e. parallel planarbitrarily unfolded sequential plan.fluents :on(B, L) requires block(B), location(L).blocked(B) requires block(B).moved(B) requires block(B).actions :move(B, L) requires block(B), location(L) costs 1.always :executable move(B, L) B != L.nonexecutable move(B, L) blocked(B).nonexecutable move(B, L) blocked(L).nonexecutable move(B, L) move(B1, L), B < B1, block(L).nonexecutable move(B, L) move(B, L1), L < L1.nonexecutable move(B, B1) move(B1, L).causedcausedcausedcausedon(B, L) move(B, L).blocked(B) on(B1, B).moved(B) move(B, L).on(B, L) moved(B) on(B, L).Figure 2: Kc encoding Blocks World domainplanning problem emerging initial state goal state depicted Figure 3modeled using background knowledge bw :block(1). block(2). block(3). block(4). block(5). block(6).location(table).location(B) :- block(B).extending program Figure 2 follows:initially : on(1, 2). on(2, table). on(3, 4). on(4, table). on(5, 6). on(6, table).goal :on(1, 3), on(3, table), on(2, 4), on(4, table), on(6, 5), on(5, table) ?(l)123413562465Figure 3: simple Blocks World instance39fiE ITER , FABER , L EONE , P FEIFER & P OLLERESmove penalized cost 1, results minimization total number moves.Let Pl denote planning problem plan length l.l = 2, optimal plan involves six moves, i.e. cost P2 = 6:P2 = h {move(1, table), move(3, table), move(5, table)}, {move(1, 3), move(2, 4), move(6, 5)}unfolding steps, plan gives rise similar plans length l = 3, . . . , 6 cost 6.l = 3, find among others following optimal plan, cost 5:P3 = h {move(3, table)}, {move(1, 3), move(5, table)}, {move(2, 4), move(6, 5)}plan parallelized two steps. plan length l > 3,obtain optimal plans similar P 3 , extended void steps. Thus plan cheapestplan lengths cost 5 needs three steps. Note shortest parallel plans (of length 2)expensive, explained above.5.1.2 HORTEST P LANS ()Intuitively, possible include minimization time steps cost function.describe preprocessing method which, given K planning domain PD, list Q ground literals,upper bound 0 plan length, generates planning problem P (PD, Q, i)optimal plans P correspond shortest plans reach Q PD steps, i.e.,plans hPD, Q ? (l)i l minimal. assume action costs specifiedoriginal planning domain PD, minimizing time steps target.First rewrite planning domain PD PD follows: introduce new distinct fluentgr new distinct action finish, defined follows:fluents :actions :gr.finish costs time.Intuitively, action finish represents final action, use finish plan. lateraction occurs, expensive plan assign time cost. fluent gr (goalreached) shall true remain true soon goal reached, triggeredfinish action.modeled K c adding following statements always sectionprogram:executable finish Q, gr.caused gr finish.caused gr gr.Furthermore, want finish occur exclusively want block occurrenceaction goal reached. Therefore, every action PD, addnonexecutable finish.add gr if-part executability condition A. Finally, avoid inconsistencies static dynamic effects soon goal reached, add grpart causation rule PD except nonexecutable rules remain unchanged. 4define P (PD, Q, i) = hPD , gr ?(i + 1)i. take + 1 plan length sinceneed one additional step execute finish action.4. need rewrite nonexecutable rules respective actions already switchedrewriting executability conditions.40fiA NSWER ET P LANNING U NDER ACTION C OSTSconstruction, easy see optimal plan P = hA 1 , . . . , Aj , Aj+1 , . . . , Ai+1planning problem P must Aj+1 = {finish} Aj+2 = . . . = Ai+1 =j {0, . . . , i}. thus following desired property.Proposition 5.1 optimal plans P 1-1 correspondence shortest plans reaching Q PD. precisely, P = hA1 , . . . , Aj+1 , , . . . , optimal optimistic planP (PD, Q, i) Aj+1 = {finish} P 0 = hA1 , . . . , Aj optimistic planhPD, Q ? (j)i j {0, . . . , i}, hPD, Q ? (j 0 )i optimistic plan j 0 < j.Blocks World example, using method get 2-step plans, choose 2.compute shortest plans plan lengths, set upper bound large enoughplans length l guaranteed exist. trivial bound total number legalstates general exponential number fluents.However, many typical applications inherent, much smaller bound plan length.instance, Blocks World n blocks, goal configuration reached within2n sinit sgoal steps, sinit sgoal numbers stacks initial goalstate, respectively.5 Therefore, 6 upper bound plan length simple instance.remark approach minimizing plan length efficient upper boundclose optimum known. Searching minimum length plan iteratively increasingplan length may much efficient bound known, since weak upper boundlead explosion search space (cf. benchmarks Section 7.2).5.1.3 HORTESTAMONGC HEAPEST P LANS ()previous subsection, shown calculate shortest plans K programs withoutaction costs. Combining arbitrary K c programs rewriting method described easy.want find shortest among cheapest plans, use rewriting,little change. setting costs actions except finish least highhighest possible cost finish action. obviously plan length + 1. So,simply modify action declarationsrequires B costs C D.P multiplying costs factor + 1:requires B costs C1 C1 = (i + 1) C, D.lets action costs take priority cost finish compute planssatisfying criterion (). Let P denote resultant planning problem. have:Proposition 5.2 optimal plans P 1-1 correspondence shortest amongcheapest plans reaching Q PD within steps. precisely, P = hA 1 , . . . , Aj+1 , , . . . ,optimal optimistic plan P (PD, Q, i) Aj+1 = {finish} (i) P 0 =hA1 , . . . , Aj plan Pj = hPD, Q ? (j)i, j {0, . . . , i}, (ii) P 00 = hA1 , . . . , Aj 0plan Pj 0 = hPD, Q ? (j 0 )i j 0 i, either costPj 0 (P 00 ) > costPj (P 0 )costPj 0 (P 00 ) = costPj (P 0 ) j 0 j.Figure 4 shows P Blocks World instance = 6. One optimal plan P5. One solve Blocks World problem sequentially first unstacking blocks table(n sinit steps) building goal configuration (n sgoal steps).41fiE ITER , FABER , L EONE , P FEIFER & P OLLERESfluents :on(B, L) requires block(B), location(L).blocked(B) requires block(B).moved(B) requires block(B).gr.actions :move(B, L) requires block(B), location(L) costs C C = 7 1.finish costs time.always :executable move(B, L) B != L, gr.nonexecutable move(B, L) blocked(B).nonexecutable move(B, L) blocked(L).nonexecutable move(B, L) move(B1, L), B < B1, block(L).nonexecutable move(B, L) move(B, L1), L < L1.nonexecutable move(B, B1) move(B1, L).causedcausedcausedcausedon(B, L) gr move(B, L).blocked(B) on(B1, B), gr.moved(B) gr move(B, L).on(B, L) moved(B), gr on(B, L).executable finish on(1, 3), on(3, table), on(2, 4), on(4, table),on(6, 5), on(5, table), gr.caused gr finish.caused gr gr.nonexecutable move(B, L) finish.initially : on(1, 2). on(2, table). on(3, 4). on(4, table). on(5, 6). on(6, table).goal :gr? (7)Figure 4: Computing shortest plan Blocks World instance minimum numberactionsP = h {move(3, table)}, {move(1, 3), move(5, table)},{move(2, 4), move(6, 5)}, {finish}, , , i,costP (P ) = 39. compute optimal cost wrt. optimization () subtracting cost finish dividing + 1: (39 4) (i + 1) = 35 7 = 5. Thus,need minimum 5 moves reach goal. minimal number steps obviously steps,except final finish action, i.e. 3. Thus, need least 3 steps plan five moves.5.1.4 C HEAPESTAMONGHORTEST P LANS ()Again, use rewriting optimization (). cost functions adapted similarlyprevious subsection, cost action finish takes priorityactions costs. end, sufficient set cost finish high enough, achievedmultiplying factor F higher sum action costs legal action instancessteps j = 1, . . . , + 1. Let P denote resulting planning problem. have:Proposition 5.3 optimal plans P 1-1 correspondence cheapest amongshortest plans reaching Q PD within steps. precisely, P = hA 1 , . . . , Aj+1 , , . . . ,42fiA NSWER ET P LANNING U NDER ACTION C OSTSoptimal optimistic plan P (PD, Q, i) Aj+1 = {finish} (i) P 0 =hA1 , . . . , Aj plan Pj = hPD, Q ? (j)i, j {0, . . . , i}, (ii) P 00 = hA1 , . . . , Aj 0plan Pj 0 = hPD, Q ? (j 0 )i j 0 i, either j 0 > j, j 0 = j costPj 0 (P 00 )costPj (P 0 ).example, 36 possible moves. Thus, could take F = 36 (i + 1)would set costs finish time 36 (i + 1). However, need take accountactions actually occur simultaneously. example, six blocksmoved parallel. Therefore, sufficient set F = 6 (i + 1) assign finish costtime F = time 42. Accordingly, action declarations modified follows:actions :move(B, L) requires block(B), location(L) costs 1.finish costs C C = time 42.optimal plan modified planning problem P is:P = h {move(1, table), move(3, table), move(5, table)},{move(1, 3), move(2, 4), move(6, 5)}, {finish}, , , ,costP (P ) = 132. Here, compute optimal cost wrt. optimization () simplysubtracting cost finish, i.e. 132 3 42 = 6, since finish occurs time point 3.Consequently, need minimum 6 moves shortest plan, length 3 1 = 2.indeed, seen (and how) optimization problems () ()represented Kc . remark transformations P , P , P work restrictionssecure and/or sequential plans well.5.2 Traveling Salespersonanother illustrating example optimal cost planning, introduce elaborationTraveling Salesperson Problem.Traveling Salesperson Problem (TSP). start classical Traveling Salesperson Problem (TSP), given set cities connections (e.g., roads, airways) certain costs.want know economical round trip visits cities exactly returnsstarting point (if tour exists). Figure 5 shows instance representing capitalsAustrian provinces. dashed line flight connection, connections roads;connection marked costs traveling hours.brg ... Bregenzeis ... Eisenstadtgra ... Grazibk ... Innsbruckkla ... Klagenfurtlin ... Linzsbg ... Salzburgstp ... St. Pltenvie ... Viennalinsbg22ibk25vie123222klaFigure 5: TSP Austriaeis23432stp 111brg1gra1fiE ITER , FABER , L EONE , P FEIFER & P OLLERESrepresent Kc follows. background knowledge SP defines two predicatescity(C) conn(F, T, C) representing cities connections associated costs. Connections traveled ways:conn(brg, ibk, 2). conn(ibk, sbg, 2). conn(ibk, vie, 5). conn(ibk, kla, 3).conn(sbg, kla, 2). conn(sbg, gra, 2). conn(sbg, lin, 1). conn(sbg, vie, 3).conn(kla, gra, 2). conn(lin, stp, 1). conn(lin, vie, 2). conn(lin, gra, 2).conn(gra, vie, 2). conn(gra, eis, 1). conn(stp, vie, 1). conn(eis, vie, 1).conn(stp, eis, 2). conn(vie, brg, 1).conn(B, A, C) :- conn(A, B, C).city(T) :- conn(T, , ).possible encoding TSP starting Vienna (vie) K c program Figure 6. includes twoactions traveling one city another directly returning starting pointend round trip soon cities visited.actions :travel(X, Y) requires conn(X, Y, C) costs C.return from(X) requires conn(X, vie, C) costs C.fluents :unvisited. end.in(C) requires city(C).visited(C) requires city(C).always :executable travel(X, Y) in(X).nonexecutable travel(X, Y) visited(Y).executable return from(X) in(X).nonexecutable return from(X) unvisited.caused unvisited city(C), visited(C).caused end return from(X).caused in(Y) travel(X, Y).caused visited(C) in(C).inertial visited(C).noConcurrency.initially : in(vie).goal :end? (9)Figure 6: Traveling Salespersonproblem ten optimal 9-step solutions cost 15. show first five here,others symmetrical:P1 = h {travel(vie, stp)},{travel(lin, sbg)},{return from(brg)}P2 = h {travel(vie, eis)},{travel(sbg, gra)},{return from(brg)}P3 = h {travel(vie, eis)},{travel(gra, kla)},{return from(brg)}P4 = h {travel(vie, lin)},{travel(gra, kla)},{travel(stp, eis)},{travel(sbg, kla)},{travel(eis, stp)},{travel(gra, kla)},{travel(eis, stp)},{travel(kla, sbg)},{travel(lin, stp)},{travel(kla, sbg)},{travel(eis, gra)}, {travel(gra, lin)},{travel(kla, ibk)}, {travel(ibk, brg)},{travel(stp, lin)}, {travel(lin, sbg)},{travel(kla, ibk)}, {travel(ibk, brg)},{travel(stp, lin)}, {travel(lin, gra)},{travel(sbg, ibk)}, {travel(ibk, brg)},{travel(stp, eis)}, {travel(eis, gra)},{travel(sbg, ibk)}, {travel(ibk, brg)},44fiA NSWER ET P LANNING U NDER ACTION C OSTS{return from(brg)}P5 = h {travel(vie, gra)},{travel(lin, sbg)},{return from(brg)}{travel(gra, eis)}, {travel(eis, stp)}, {travel(stp, lin)},{travel(sbg, kla)}, {travel(kla, ibk)}, {travel(ibk, brg)},TSP variable costs. Let us consider elaboration TSP, assumecosts traveling different connections may change trip. Note threefive solutions example include traveling St.Polten Eisenstadt vice versasecond day. Let us assume salesperson, starts Monday, faceexceptions might increase cost trip. instance, (i) heavy traffic jams expectedTuesdays route St.Polten Eisenstadt (ii) salesperson shall use flightconnection Vienna Bregenz Mondays expensive business class ticketsavailable connection beginning week. deal different costsrespective connections depending particular day.end, first add background knowledge SP new predicate cost(A, B, W, C)representing cost C traveling connection B weekday W take exceptionalcosts account:cost(A, B, W, C) :- conn(A, B, C), #int(W), 0 < W, W <= 7, ecost(A, B, W).ecost(A, B, W) :- conn(A, B, C), cost(A, B, W, C1), C != C1.original costs predicate conn(A, B, C) represent defaults, overriddenexplicitly adding different costs. instance, represent exceptions (i) (ii), add:cost(stp, eis, 2, 10). cost(vie, brg, 1, 10).setting exceptional costs two critical connections 10. Weekdays coded integers1 (Monday) 7 (Sunday). represent mapping time steps weekdaysfollowing rules also add SP :weekday(1, 1).weekday(D, W) :- = D1 + 1, W = W1 + 1, weekday(D1, W1), W1 < 7.weekday(D, 1) :- = D1 + 1, weekday(D1, 7).Note although modified background knowledge SP stratified (since cost definedcyclic negation), total well-founded model, thus unique answer set.Finally, change costs traveling returning K c program Figure 6:actions :travel(X, Y) requires conn(X, Y, C1) costs Cweekday(time, W), cost(X, Y, W, C).return from(X) requires conn(X, vie, C1) costs Cweekday(time, W), cost(X, vie, W, C).Since costs P1 (which includes traveling St.Polten Eisenstadt) secondday increased due exception (i), four plans remain optimal. Noteunlike default costs, exceptional costs apply bidirectionally, exceptionaffect P2 P3 . Furthermore, due exception (ii) symmetrical round trips startingflight trips Bregenz longer optimal.presented encoding proves flexible, allows adding arbitrary exceptionsconnection weekday simply adding respective facts; moreover, eveninvolved scenarios, exceptions defined rules, modeled.45fiE ITER , FABER , L EONE , P FEIFER & P OLLERES5.3 Small Example Planning Resource RestrictionsAlthough planning resources main target approach, following encodingshows action costs also used order model optimization resource consumptioncases. important resource real world planning money. instance, let us considerproblem buying selling (Lee & Lifschitz, 2001):$6 pocket. newspaper costs $1 magazine costs $3.enough money buy one newspaper two magazines?Kc , encoded compact way following background facts:item(newspaper, 1). item(magazine, 2).combined following short K c program:actions :buy(Item, Number) requires item(Item, Price), #int(Number)costs C C = Number Price.fluents :have(Item, Number) requires item(Item, Price), #int(Number).always :executable buy(Item, Number).nonexecutable buy(Item, N1) buy(Item, N2), N1 < N2.caused have(Item, Number) buy(Item, Number).goal :have(newspaper, 1), have(magazines, 2) ? (1)action buy always executable, one must buy two different amounts certainitem once. Obviously, admissible plan wrt. cost 6 exists, optimal plan problem,h{buy(newspaper, 1), buy(magazine, 2)} cost P = 7. Therefore, answer problemno.approach considers positive action costs directly allow modeling fullconsumer/producer/provider relations resources general, favor clear non-ambiguousdefinition optimality. instance, allowing negative costs one could always add produceraction make existing plan cheaper, whereas approach costs guaranteed increasemonotonically, allowing clear definition plan costs optimality.hand, encode various kinds resource restrictions using fluents represent resources. model production/consumption action effects fluentsadd restrictions constraints. allows us model even complex resource schedulingproblems; optimization, however, remains restricted action costs.6. Transformation Logic Programmingsection, describe planning action costs implemented meanstransformation answer set programming. extends previous transformation (Eiter et al.,2003a), maps ordinary K planning problems disjunctive logic programs answerset semantics (Gelfond & Lifschitz, 1991), takes advantage weak constraints, cf. (Buccafurri,Leone, & Rullo, 1997, 2000), implemented DLV system (Faber & Pfeifer, 1996; Eiter,Faber, Leone, & Pfeifer, 2000a). addition, show translation adaptedlanguage Smodels (Simons, Niemela, & Soininen, 2002).6.1 Disjunctive Logic Programs Weak ConstraintsFirst, give brief review disjunctive logic programs weak constraints.46fiA NSWER ET P LANNING U NDER ACTION C OSTSSyntaxdisjunctive rule (for short, rule) R constructa1 v v :- b1 , , bk , bk+1 , , bm .(2)ai bj classical literals function-free first-order alphabet, n 0,k 0. part left (resp. right) :- head (resp. body) R, :- omitted= 0. let H(R) = {a1 , . . ., } set head literals B(R) = B + (R) B (R)set body literals, B + (R) = {b1 ,. . . , bk } B (R) = {bk+1 , . . . , bm }. (strong)constraint rule empty head (n = 0).weak constraint construct: b1 , , bk , bk+1 , , bm . [w :](3)w integer constant variable occurring b 1 , . . . , bk bi classical literals.6B(R) defined (2).disjunctive logic program (DLPw ) (simply, program) finite set rules, constraintsweak constraints; here, superscript w indicates potential presence weak constraints.Semantics answer sets program without weak constraints defined usual (Gelfond & Lifschitz, 1991; Lifschitz, 1996). one difference, though: considerinconsistent answer sets. answer sets program weak constraints definedselection answer sets weak-constraint free part 0 optimal answer sets.weak constraint c form (3) violated, instance conjunctionsatisfied respect candidate answer set S, i.e., exists substitution mappingvariables c Herbrand base {b 1 , , bk } {bk+1 , , bm }= ; call w violation value c wrt. . 7 violation cost c wrt. S, denotedcostc (S), sum violation values violating substitutions c wrt. S; costS, denoted cost (S),Xcost (S) =costc (S),c weak constraintsi.e., sum violation costs weak constraints wrt. S. answer setselected (called optimal answer set), cost (M ) minimal answer sets .(Buccafurri et al., 2000) know given head-cycle-free disjunctive program, deciding whether query q true optimal answer set P2 -complete. respective classcomputing answer set FP2 -complete. Together results Section 4 indicates translations optimal planning problems head-cycle-free disjunctive logic programsweak constraints language Smodels feasible polynomial time.6.2 Translating Kc DLPwextend original transformation lp(P), naturally maps K planning problem Pweak-constraint free program (Eiter et al., 2003a), new translation lp w (P), optimalanswer sets lpw (P) correspond optimal cost plans K c planning problem P.6. colon [w :] stems DLV language, allows specify priority layer colon.need priority layers translation, stick DLV syntax.7. weak constraint c admissible, possible violation values candidate answer sets integers.Thus, w variable, must guarantee w bound integer.47fiE ITER , FABER , L EONE , P FEIFER & P OLLERESBasically, lp(P) fluent action literals extended additional time parameter,executability conditions well causations rules modularly translated (rule rule) corresponding program rules constraints; disjunction used guessing actionsexecuted plan point time.6.2.1 R EVIEWRANSLATION lp(P)basic steps translation K programs logic programs follows (cf. (Eiter et al.,2003a) details):Step 0 (Macro Expansion):First, replace macros K program definitions.Step 1 (Background Knowledge): background knowledge P already given logicprogram included lp(P), without modification.Step 2 (Auxiliary Predicates):represent steps, add following facts lp(P)time(0)., . . . , time(l). next(0, 1)., . . . , next(l 1, l).l plan length query q = G?(l) P hand.Step 3 (Causation Rules): Causation rules mapped rules lp(P) adding type information extending fluents actions time stamp using time next. example,caused across(X) cross(X), -across(X).leads rule across(X, T1) :- cross(X, T0), -across(X, T0), person(X), next(T0, T1 ).lp(P) T1 , T0 new variables. Here, type information person(X) across(X),-across(X), taken type declaration, added, helps avoid unsafe logic programming rules.Step 4 (Executability Conditions): Similarly, executability condition translated disjunctive rule guessing whether action occurs certain time step. running example,executable cross(X) hasLamp(X).becomes cross(X, T0) -cross(X, T0) :- hasLamp(X, T0), person(X), next(T0, T1 ).encodes guess whether time point 0 action cross(X) happen; again, type information person(X) added well next(T 0 , T1 ) ensure T0 last time point.Step 5 (Initial State Constraints): Initial state constraints transformed like static causationrules Step 3, using constant 0 instead variable 1 thus need auxiliary predicate time stamp. instance,initially : caused -across(X).becomes, adding type information -across(X, 0) :- person(X).Step 6 (Goal Query):goal :Finally, query q:g1 (t1 ), . . . , gm (tm ), gm+1 (tm+1 ), . . . , gn (tn ) ? (l).translated follows, goal reached new 0-ary predicate symbol:goal reached :- g1 (t1 , l), . . . , gm (tm , l), gm+1 (tm+1 , l), . . . , gn (tn , l).:- goal reached.48fiA NSWER ET P LANNING U NDER ACTION C OSTS6.2.2 E XTENDINGRANSLATIONACTION C OSTSextended translation lpw (P) Kc problem P first includes rules lp(Pnc ), Pncresults P stripping cost parts. Furthermore, following step added:Step 7 (Action Costs):action declaration form (1) nonempty costs-part, add:(i) new rule rd formcostp (X1 , . . . , Xn , T, C) :- p(X1 , . . . , Xn , T), t1 , . . . , tm ,c1 , . . . , ck , U = + 1.(4)costp new symbol, U new variables = {time U}. optimization,U = + 1 present U occurs elsewhere r .: costp (X1 , . . . , Xn , T, C). [C :](ii) weak constraint wcd form(5)example, cross action Quick Bridge Crossing Problem translatedcostcross(X, T, WX):- cross(X, T), person(X), walk(X, WX).: costcross(X, T, WX). [WX :]showed previous work (Eiter et al., 2003a), answer sets lp(P) correspondtrajectories optimistic plans P. following theorem states similar correspondence resultlpw (P) optimal plans P. define, consistent set ground literals S, setsASj = {a(t) | a(t, j 1) S, act } sSj = {f (t) | f (t, j) S, f (t) Lf l }, j 0.Theorem 6.1 (Answer Set Correspondence) Let P = hPD, qi (well-defined) K c planningproblem, let lpw (P) program. Then,(i) optimistic plan P = hA1 , . . . , Al P supporting trajectory = hhs 0 , A1 , s1 i,hs1 , A2 , s2 i, . . . , hsl1 , Al , sl ii P , exists answer set lp w (P)Aj = ASj j = 1, . . . , l, sj = sSj , j = 0, . . . , l costP (P ) = costlpw (P) (S);(ii) answer set lpw (P), sequence P = hA1 , . . . , Al solution P, i.e.,optimistic plan, witnessed trajectory = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . . , hsl1 , Al , sl iicostP (P ) = costlpw (P) (S), Aj = ASj sk = sSk j = 1, . . . , lk = 0, . . . , l.proof based resp. correspondence result K (Eiter et al., 2003a). details,refer Appendix.result definitions optimal cost plans optimal answer sets, concludefollowing result:Corollary 6.2 (Optimal answer set correspondence) well-defined K c planning problemP = hPD, Q ? (l)i, trajectories = hhs 0 , A1 , s1 i, . . . , hsl1 , Al , sl ii optimal plans P Pcorrespond optimal answer sets lp w (P), Aj = ASj j = 1, . . . , lsj = sSj , j = 0, . . . , l.Proof. Aj , weak constraint (5) causes violation value cost j (a). Furthermore,P onlyPcost violations. Thus, candidate answer set optimalcostlpw (P) (S) = lj=1 aAj costj (a) = costP (P ) minimal, i.e., corresponds optimalplan.2similar correspondence result also holds admissible plans:49fiE ITER , FABER , L EONE , P FEIFER & P OLLERESCorollary 6.3 (Answer set correspondence admissible plans) well-defined K c planning problem P = hPD, Q ? (l)i, trajectories = hhs 0 , A1 , s1 i, . . . , hsl1 , Al , sl ii admissible plans P P wrt. cost c correspond answer sets lp w (P) costlpw (P) (S) c,Aj = ASj j = 1, . . . , l sj = sSj , j = 0, . . . , l.secure planning, introduced technique check security optimistic plancertain planning problem instances means logic program (Eiter et al., 2003a).method carries planning action costs straightforward way, optimal resp. admissible secure plans similarly computed answer set programming.6.3 Alternative Translation SmodelsApart presented translation using weak constraints, one could also choose alternativeapproach translation answer set programming. Smodels (Simons et al., 2002) supportsanother extension pure answer set programming allowing minimize sets predicates.approach could used alternative formulation Step 7:Step 7a:action declarations nonempty costs-parts, add new rule formcost(p, X1 , . . . , Xn , 0, . . . , 0, T, C) :- t1 , . . . , tm , c1 , . . . , ck , U = + 1.(6)similar Step 7 above, two differences: (1) action name p parameter, (2) addl n parameters constant 0 X n l maximum arity actionsPD. necessary order get unique arity l + 2 predicate cost. Furthermore, addoccurs(p, X1 , . . . , Xn , 0, . . . , 0, T) :- p(X1 , . . . , Xn , T), t1 , . . . , tm ,.(7)second rule adds 0 parameters achieve unique arity l + 1 newpredicate occurs. Using Smodels syntax, compute optimal plans addingminimize[occurs(A, X1, ..., Xl , T) : cost(A, X1, ..., Xl , T, C) = C].Note Smodels support disjunction rule heads, also need modify Step 4,expressing action guess via unstratified negation Smodels choice rules.7. Implementationimplemented experimental prototype system, DLV K , solving K planning problems (Eiter et al., 2003a). improved version prototype capable optimaladmissible planning respect extended syntax K c , available experimentshttp://www.dlvsystem.com/K/ .DLVK realized frontend DLV system (Faber & Pfeifer, 1996; Eiter et al.,2000a). First, planning problem hand transformed described previous section.Then, DLV kernel invoked produce answer sets. optimistic planning (optimal,action costs defined) answer sets simply translated back suitable output userprinted.case user specified secure/conformant planning performed, systemcheck security plans computed. normal (non-optimal) planning, simply donechecking answer set returned right transforming back user output. case50fiA NSWER ET P LANNING U NDER ACTION C OSTSoptimal secure planning, hand, candidate answer set generation DLV kernelintercepted: kernel proceeds computing candidate answer sets, returning answerset minimal violation cost value, running candidates. Here, order generateoptimal secure plans, planning frontend interrupts computation, allowing answer setsrepresent secure plans considered candidates.Checking plan security done rewriting translated program wrt. candidate answerset/plan order verify whether plan secure. rewritten check program testedseparate invocation DLV kernel. details system architecture refer(Eiter et al., 2003a)7.1 UsageSuppose background knowledge program depicted Figure 1 cost extensionsSection 3.3 stored files crossing.bk crossing.plan; then, invokingprogram command linedlv FPcrossing.plancrossing.bk planlength = 7compute optimal plans solving problem seven steps. output find,supporting trajectory, following optimal plan:PLAN : crossTogether(joe, jack) : 2; cross(joe) : 1; takeLamp(william);crossTogether(william, averell) : 10; takeLamp(jack);cross(jack) : 2; crossTogether(joe, jack) : 2 COST : 17action, cost shown colon, non-zero. switch -planlength=iused set plan length; overrides plan length given query-part planingproblem. Using -planlength=5, get plans cost 19, cheaper planslength.user asked whether perform optional security check whether look(optimal) plans, respectively. switch -FPsec used instead -FP obtainsecure plans only.command line option -costbound=N effects computation admissible plansrespect cost N . example, resource problem described Section 5.3 solvedfollowing call prototype:dlv FPbuying.bkbuying.plan N = 10 planlength = 1 costbound = 6Correctly, admissible plan found. calling system without cost bound,prototype calculates following optimal cost plan:PLAN : buy(newspaper, 1) : 1, buy(magazine, 2) : 6COST : 7current prototype supports simple bounded integer arithmetics. option -N=10 usedsets upper bound N = 10 integers may used program; builtin predicate #int true integers 0 . . . N . Setting N high enough, taking accountoutcome built-in arithmetic predicates = B + C = B C, important getcorrect results. details prototype given DLV K web site http://www.dlvsystem.com/K/.51fiE ITER , FABER , L EONE , P FEIFER & P OLLERES7.2 ExperimentsPerformance experimental results DLV K (without action costs optimal planning)reported previous work (Eiter et al., 2003a). section, present encouraging experimental results planning action costs, particular parallel Blocks World TSP.experiments performed Pentium III 733MHz machine 256MB main memory running SuSE Linux 7.2. set time limit 4000 seconds tested instance exceedinglimit indicated - result tables.possible, also report results CCALC CMBP, two logic-based planningsystems whose input languages (C+ resp. AR) capabilities similar K resp. K c .CCALC. Causal Calculator (CCALC) model checker languages causal theories(McCain & Turner, 1997). translates programs action language C+ languagecausal theories turn transformed SAT problems; solved using SATsolver (McCain & Turner, 1998). current version CCALC uses mChaff (Moskewicz et al.,2001) default SAT solver. Minimal length plans generated iteratively increasing planlength upper bound. CCALC written Prolog. tests, used version 2.04bCCALC obtained <URL:http://www.cs.utexas.edu/users/tag/cc/> trial version SICStus Prolog 3.9.1. used encodings taken (Lee & Lifschitz,2001) parallel Blocks World adapted CCALC 2.0. encodings includedcurrent download version system. sequential Blocks World adapted encodingsadding C+ command noConcurrency. resembles respective K command.results CCALC include 2.30sec startup time.CMBP. Conformant Model Based Planner (CMBP) (Cimatti & Roveri, 2000) basedmodel checking paradigm exploits symbolic Boolean function representation techniquesBinary Decision Diagrams (Bryant, 1986). CMBP allows computing sequential minimallength plans, user declare upper bound plan length. input languageextension AR (Giunchiglia, Kartha, & Lifschitz, 1997). Unlike K action languagesC+ (Lee & Lifschitz, 2001), language supports propositional actions. CMBPtailored conformant planning. results reported complement previous comparisonalso shows encoding sequential Blocks World CMBP (Eiter et al., 2003a). tests,used CMBP 1.0, available <URL:http://sra.itc.it/people/roveri/cmbp/>.7.2.1 B LOCKS W ORLDTables 14 show results different Blocks World encodings Section 5.1 severalconfigurations: P0 denotes simple instance Figure 3, P1P5 instances usedprevious work (Eiter et al., 2003a; Erdem, 1999).Table 1 shows results finding shortest sequential plan. second third columnshow number blocks length shortest plan (i.e., least number moves) solvingrespective blocks world instance. execution time solving problem using shortestplan encoding P Section 5.1 shown column five, using upper bound shown fourthcolumn plan length. Column six shows execution time finding shortest planincremental plan length search starting 0, similar method used CCALC.remaining two columns show results CCALC CMBP.52fiA NSWER ET P LANNING U NDER ACTION C OSTSProblemP0P1P2P3P4P5#blocks64581111min. #moves (=#steps)5468911upper bound #steps647101616DLVKDLVKinc0.48s0.05s0.24s25.32s-0.29s0.08s0.27s2.33s8.28s12.63sCCALC4.65s3.02s4.02s10.07s27.19s32.27sCMBP21.45s0.13s8.44s-Table 1: Sequential Blocks World - shortest plansProblemP0P0P1P2P3P4P5#blocks664581111#steps(fixed)2335457min. #moves654691315DLVK0.05s0.09s0.04s0.10s0.21s0.81s327sTable 2: Parallel Blocks World - cheapest plans: Minimal number moves fixed plan length ()Table 2 shows execution times parallel blocks world fixed plan lengthnumber moves minimized, i.e. problem () Section 5.1. used encoding Figure 2,generates parallel serializable plans. CCALC CMBP allow optimizingcriteria plan length, results DLV K here.Next, Table 3 shows results finding shortest parallel plan, i.e. problem () Section 5.1. First, minimal possible number steps given. processed instance (i) usingencoding P Section 5.1, (ii) without costs iteratively increasing plan length(iii) using CCALC, iteratively increasing plan length plan found. every result,number moves first plan computed reported separately. CMBP supportssequential planning, included comparison.Finally, Table 4 shows results combined optimizations () () parallel BlocksWorld outlined Section 5.1. second column contains upper bound planupper boundP0P1P2P3P4P5647101616min. #steps235457DLVK#moves659-time0.52s0.07s0.39s-DLVKinc#moves659121826time0.09s0.08s0.21s0.43s1.54s3.45sTable 3: Parallel Blocks World - shortest plan ()53CCALC#movestime64.05s42.95s63.70s97.69s1320.45s1523.22sfiE ITER , FABER , L EONE , P FEIFER & P OLLERES()P0P1P2P3P4P5upper bound647101616steps/moves3/53/45/65/89/911/11()DLVKDLVKinc38.5s0.07s2.08s-0.18s0.11s0.21s1.57s-CCALC5.89s3.47s5.65s15.73s73.64s167ssteps/moves2/63/45/64/95/137/15DLVKDLVKinc0.26s0.08s0.78s177s-0.09s0.08s0.28s0.45s1.86s323sTable 4: Parallel Blocks World - (),()length respective instance. following three columns present results findingshortest among cheapest plans, i.e. problem () Section 5.1:DLVK refers results combined minimal encoding P described Section 5.1;DLVKinc refers results incrementally searching shortest among cheapest plans:done means -costbound=i command line option taking minimalsequential costs (i.e., shortest sequential plan length computed Table 1) uppercost limit. encodings compute serializable plans, minimal sequential lengthused cost limit special case.CCALC similar technique used CCALC encoding bound costs additive fluents (Lee & Lifschitz, 2001).Note incremental strategy (used DLV Kinc CCALC) takes advantage specific formulation parallel Blocks World problem: general, allowing parallel actionsnecessarily serializable arbitrary costs, optimal parallel cost might differoptimal sequential solution. particular, plans longer cheapest sequential plans (which, example, coincide shortest sequential plans) may needconsidered. makes incremental search solution problem () infeasible general.last test finding cheapest among shortest plans, is, problem () Section 5.1.tested integrated encoding upper bound (P ) resp. incrementally findingshortest plan. Unlike problem (), cannot derive fixed cost limit sequentialsolution here; really need optimize costs, makes encoding CCALC infeasible.Blocks World Results Blocks World experiments show DLV K solve various optimization tasks effective flexible way systems compared. hand,already stated above, minimal plan length encodings Section 5.1, solveproblems tight upper bound plan length known. Iteratively increasing planlength effective, especially upper bound much higher actual optimal solution. becomes drastically apparent execution times seem explode one instancenext, highly non-linear manner Table 1 solution P3 foundreasonable time whereas P4 P5 could solved within time limit 4000 seconds.observation also confirmed tables (instance P5 Table 2, etc.) partly explainedbehavior underlying DLV system, geared towards plan search,general purpose problem solver uses heuristics might work well cases.particular, answer set generation process DLV, distinction made actions54fiA NSWER ET P LANNING U NDER ACTION C OSTSfluents, might useful planning tasks control generation answer sets resp.plans; may part investigations.Interestingly, CCALC finds better quality parallel solutions problem () (cf. Table 3), i.e.solutions fewer moves, although significantly slower system instances.incremental encoding problem (), CCALC seems even effective system.However, CCALC offers means optimization; allows admissible optimalplanning. makes approach flexible general. stated above, could fortunatelyexploit fixed cost bound particular example CCALC, possible generalinstances problem ().Problem () also intuitively harder simply finding shortest plan cheapest amongshortest plans general: problems always solved incrementally, ()must consider plans lengths. longer plan may cheaper, cannot freeze planlength (shortest) plan incrementally found.7.2.2 TSPexperimental results TSP variable costs reported Tables 5 6. Unlikeblocks world, comparable systems available; none systems supports costoptimal planning needed solving problem. Here, plan length always givennumber cities.Table 5 shows results TSP instance Austrian province capitals Figure 5(nine cities, 18 connections), without exceptional costs Section 5.2 (with without subscript exc table). instances reported table different cost exceptions(we, lwe, rnd) described below.Results bigger TSP instances, given capitals 15 members EuropeanUnion (EU) varying connection graphs exceptional costs shown Table 6.used flight distances (km) cities connection costs. Instances TSP EU 1TSPEU 6generated randomly choosing given number connections possible connections 15 cities. Note TSP EU 1 solution; time reportedDLVK terminated, instances first optimal plan found.also tested instances practical relevance simply randomly choosingconnections: TSPEU 7 instance taken flight connections three carriers(namely, Star Alliance, Alitalia, Luxair), TSP EU 8 included direct connections 1500km. capital hopping interest small airplane limitedrange, instance.instance Tables 56 measured execution time:without exceptional costs,50% costs connections Saturdays Sundays (weekends, we)50% costs connections Fridays, Saturdays Sundays (long weekends, lwe),random cost exceptions (rnd): added number randomly generated exceptions costs 0 10 TSP Austria 0 3000 instancesEU1 EU8.55fiE ITER , FABER , L EONE , P FEIFER & P OLLERESInstanceTSPAustriaTSPAustria,excTSPAustria,weTSPAustria,lweTSPAustria,rndTSPAustria,rndTSPAustria,rndTSPAustria,rnd#cost exceptions0236541050100200cost/time15/0.31s15/0.32s12/0.34s11/0.35s14/0.30s15/0.31s23/0.35s36/0.37sTable 5: TSP Results TSPAustria varying exceptionsInstanceTSPEU 1TSPEU 1,weTSPEU 1,lweTSPEU 1,rndTSPEU 1,rndTSPEU 1,rndTSPEU 1,rndTSPEU 2TSPEU 2,weTSPEU 2,lweTSPEU 2,rndTSPEU 2,rndTSPEU 2,rndTSPEU 2,rndTSPEU 3TSPEU 3,weTSPEU 3,lweTSPEU 3,rndTSPEU 3,rndTSPEU 3,rndTSPEU 3,rndTSPEU 4TSPEU 4,weTSPEU 4,lweTSPEU 4,rndTSPEU 4,rndTSPEU 4,rndTSPEU 4,rndTSPEU 5TSPEU 5,weTSPEU 5,lweTSPEU 5,rndTSPEU 5,rndTSPEU 5,rndTSPEU 5,rndTSPEU 5,rnd#conn.303030303030303030303030303035353535353535353535353535354040404040404040#except.0609010020030040006090100200300400070105100200300400070105100200300400080120100200300400500cost/time-/9.11s-/11.93s-/13.82s-/11.52s-/12.79s-/14.64s-/16.26s16213/13.27s13195/16.41s11738/18.53s15190/15.54s13433/16.31s13829/18.34s13895/20.59s18576/24.11s15689/28.02s14589/30.39s19410/26.75s22055/29.64s18354/31.54s17285/32.66s16533/36.63s12747/41.72s11812/43.12s15553/39.17s13216/41.19s16413/43.51s13782/45.69s15716/91.83s12875/97.73s12009/100.14s13146/85.69s12162/83.44s12074/76.81s12226/82.97s13212/82.53sInstanceTSPEU 6TSPEU 6,weTSPEU 6,lweTSPEU 6,rndTSPEU 6,rndTSPEU 6,rndTSPEU 6,rndTSPEU 6,rndTSPEU 7TSPEU 7,weTSPEU 7,lweTSPEU 7,rndTSPEU 7,rndTSPEU 7,rndTSPEU 7,rndTSPEU 7,rndTSPEU 7,rndTSPEU 7,rndTSPEU 8TSPEU 8,weTSPEU 8,lweTSPEU 8,rndTSPEU 8,rndTSPEU 8,rndTSPEU 8,rndTSPEU 8,rndTSPEU 8,rndTSPEU 8,rndTSPEU 8,rnd#conn.4040404040404040555555555555555555556464646464646464646464#except.08012010020030040050001101651002003004005006007000128192100200300400500600700800cost/time17483/142.7s14336/150.3s13244/154.7s15630/142.5s14258/137.2s11754/120.5s11695/111.4s12976/120.8s15022/102.6s12917/112.2s11498/116.2s13990/104.2s12461/100.8s13838/106.9s12251/96.58s16103/109.2s14890/110.3s17070/110.7s10858/3872s9035/3685s8340/3324s10283/2603s9926/1372s10028/1621s8133/597.7s8770/573.3s8220/360.7s6787/324.6s11597/509.5sTable 6: TSP Various instances capitals 15 EU members56fiA NSWER ET P LANNING U NDER ACTION C OSTSTSP Results Instance TSPEU 8 shows limits system: given data allows manypossible tours, finding optimal one gets tricky. hand, realistic instance likeTSPEU 7 real airline connections solved rather quickly, surprising:airlines central airport (for instance Vienna Austrian Airlines) direct connectionsdestinations served. allows much fewer candidate answer sets, (asreality) number airlines consider limited. E.g., TSP EU 7 solutiontwo Star Alliance, Alitalia, Luxair allowed. course, cannot competededicated TSP solvers/algorithms, able solve much bigger TSP instancesconsidered here. However, knowledge, none solvers deal featuresincomplete knowledge, defaults, time dependent exceptional costs, etc. directly. results evenshow execution times stable yet case many exceptions. contrast, instance TSP EU 8shows exceptions also cause significant speedup. due heuristics usedunderlying DLV system, single better solutions faster costs spread evenlylike TSPEU 8 without exceptional costs.Note that, also experimented alternative Smodels translation sketched Section 6.3. refrain detailed discussion here, since (i) translation optimized DLVSmodels performance worse (around factor 10 tested TSP instances) DLV (ii)integrated planning frontend available Smodels providing high-level planning language. Nevertheless, shown approach can, minor modifications, adoptedplanning system based Smodels.8. Related Worklast years, widely recognized plan length alone one criterionoptimized planning. Several attempts made extend planners also consider actioncosts.PYRRHUS system (Williams & Hanks, 1994) extension UCPOP planningallows optimal planning resources durations. Domain-dependent knowledgeadded direct heuristic search. utility model defined planning problemused express optimization function. system supports language extensionADL (Pednault, 1989), predecessor PDDL (Ghallab et al., 1998). algorithmsynthesis branch-and-bound optimization least-commitment, plan-space planner.approaches based heuristic search include use A* strategy togetheraction costs heuristics (Ephrati, Pollack, & Mihlstein, 1996) work RefanidisVlahavas use multi-criteria heuristics obtain near-optimal plans, considering multiple criteriaapart plan length alone (Refanidis & Vlahavas, 2001). However, described heuristicsfully admissible, guarantees optimal plans certain restrictions (Haslum & Geffner,2000). fact, heuristic state-space planners able guarantee optimality.powerful approach suggested Nareyek, describes planning resourcesstructural constraint satisfaction problem (SCSP), solves problem local searchcombined global control. However, work promotes inclusion domain-dependentknowledge; general problem unlimited search space, declarative high-level language provided (Nareyek, 2001).Among related approaches, Kautz Walser generalize Planning Satisfiabilityapproach use integer optimization techniques encoding optimal planning resource pro57fiE ITER , FABER , L EONE , P FEIFER & P OLLERESduction/consumption (Kautz & Walser, 1999). First, recall integer logic programminggeneralizes SAT, SAT formula translated system inequalities. Second, extend effects preconditions actions similar STRIPS extension proposed Koehlermodeling resource consumption/production (Koehler, 1998). Kautz Walser allow arbitraryoptimization functions use non-declarative, low-level representation based algebraic modeling language AMPL (Fourer, Gay, & Kernighan, 1993). mention KoehlersSTRIPS-like formalization mapped approach. However, express nondeterminism incomplete knowledge. implementation approach called ILPPLAN, uses AMPL package (http://www.ampl.com/). Unfortunately, AMPLfreely available, could compare system approach experimentally.Lee Lifschitz describe extension C+ action language C allows intuitive encoding resources costs means called additive fluents (Lee & Lifschitz,2001). way admissible planning realized, optimization consideredframework far. implementation planner based language CCALC (McCain,1999) already described previous section. Another implementation planning system based action language C Cplan (Giunchiglia, 2000; Ferraris & Giunchiglia,2000). Cplan system mainly focuses conformant planning support advancedfeatures C+. Furthermore, code longer maintained.Son Pontelli propose translate action language B prioritized default theory answerset programming. allow express preferences actions rules object levelinterpreter part input language (Son & Pontelli, 2002). However,preferences orthogonal approach model qualitative preferences opposedoverall value function plans/trajectories.9. Conclusion Outlookwork continues research stream pursues usage answer set programmingbuilding planning systems offer declarative planning languages based action languages,planning tasks specified high level abstraction (Lifschitz, 1999a, 1999b).representation practical planning problems, languages must high expressivenessprovide convenient constructs language elements.Towards goal, presented planning language K c , extends declarativeplanning language K (Eiter et al., 2000b, 2003a) action costs taken accountgenerating optimal plans, i.e., plans least total execution cost, admissible planswrt. given cost bound, i.e., plans whose total execution cost stays within given limit. basisimplementation issues, investigated computational complexity major planning tasks language, derived complexity results sharply characterizingcomputational cost. Furthermore, presented transformation optimal admissibleplanning problems K c logic programming optimal answer set semantics (Buccafurriet al., 1997, 2000), described DLV K prototype implemented top KR toolDLV, computes semantics.shown, Kc allows representation intricate planning problems. particular,demonstrated variant Traveling Salesperson Problem (TSP), couldconveniently specified Kc . strength Kc that, via underlying language K, statesknowledge, i.e., incomplete states, suitably respected secure plans, i.e., conformant plans58fiA NSWER ET P LANNING U NDER ACTION C OSTSwork circumstances, including nondeterministic action effects. K c flexiblelanguage which, exploiting time-dependent action costs, allows representation planningvarious optimality criteria cheapest plans, shortest plans, combinations thereof.experiments shown various instances problems considered, includingrealistic instances TSP variant, could decently solved. hand, currentversion DLVK scale large problem instances general, and, unsurprisingly,compete high-end planning tools specialized algorithms particular problemTSP. see shortcoming, though, since main goal point demonstrate usefulness expressive capabilities formalism easily represent non-trivialplanning optimization tasks, especially involved viewpoint knowledgerepresentation. way, non-trivial instances problems medium size (which one mayoften encounter) solved little effort.Several issues remain work. implementation, performance improvementsmay gained via improvements underlying DLV engine, subject current work.Furthermore, alternative, efficient transformations Kc logic programming might researched, e.g. ones involve preprocessing planning problem performing means-end analysis simplify logic program constructed.Another issue language extensions. example, crucial differenceapproach resource-based approaches former hinges action costs, latterbuild fluent values, somewhat different view quality plan. possible wayencompass language allow dynamic fluent values contribute action costs;needs carefully elaborated, though: deterministic planning complete knowledge extension straightforward, non-deterministic domains incomplete knowledgewould possibly result ambiguities. Different trajectories plan possibly yield differentcosts fluent values contribute action costs. favor intuitive definition plan costsoptimality refrained extension current state.possible extension negative action costs, useful modeling producer/consumer relations among actions resources. Allowing different priorities amongactions, i.e., different cost levels, would increase flexibility allow optimizing differentcriteria once. Finally, duration actions important issue. current language,effects actions assumed materialize next state. coding techniques,may express delayed effects several states time and/or interleaving actions, constructslanguage would desirable. Investigating issues part ongoing future work.Acknowledgmentsgrateful Joohyung Lee help using CCALC Paul Walser usefulinformations ILPPLAN. Furthermore, thank Michael Gelfond interesting discussionssuggestions, anonymous reviewers detailed helpful comments.work supported FWF (Austrian Science Funds) projects P14781Z29-N04 European Commission project FET-2001-37004 WASP IST-200133570 INFOMIX.preliminary, shorter version paper presented 8th European ConferenceLogics Artificial Intelligence (JELIA02), Cosenza, Italy, September 2002.59fiE ITER , FABER , L EONE , P FEIFER & P OLLERESAppendix A. Language Kappendix contains, shortened form, definition language K translation Kanswer set programs; see (Eiter et al., 2003b, 2003a) details examples.A.1 Basic Syntaxassume act , f l , typ disjoint sets action, fluent type names, respectively, i.e.,predicate symbols arity 0, disjoint sets con var constant variable symbols.Here, f l , act describe dynamic knowledge typ describes static background knowledge.action (resp. fluent, type) atom form p(t 1 , . . . , tn ), p act (resp. f l , typ ) arity nt1 , . . . , tn con var . action (resp. fluent, type) literal l action (resp. fluent, type)atom negation a, (alternatively, ) true negation symbol. define.l = l = .l = l = a, atom. set L literals consistent,L .L = . Furthermore, L+ (resp. L ) set positive (resp. negative) literals L.set action (resp. fluent, type) literals denoted L act (resp. Lf l , Ltyp ). Furthermore, Lf l,typ+= Lf l Ltyp , Ldyn = Lf l L+act , L = Lf l,typ Lact .actions fluents must declared using statements follows.Definition A.1 (action, fluent declaration) action (resp. fluent) declaration, form:p(X1 , . . . , Xn ) requires t1 , . . . , tm(8)+var n 0 arity p, , . . . ,p L+1act (resp. p Lf l ), X1 , . . . , XnLtyp , 0, every Xi occurs t1 , . . . , tm .= 0, keyword requires may omitted. Causation rules specify dependenciesfluents fluents actions.Definition A.2 (causation rule) causation rule (rule, short) expression formcaused f b1 , . . . , bk , bk+1 , . . . , bl a1 , . . . , , am+1 , . . . ,(9)f Lf l {false}, b1 , . . . , bl Lf l,typ , a1 , . . . , L, l k 0, n 0.Rules n = 0 static rules, others dynamic rules. l = 0 (resp. n = 0), (resp.after) omitted; l = n = 0, caused optional.access parts causation rule r h(r) = {f }, post + (r) = {b1 , . . . , bk }, post (r) ={bk+1 , . . . , bl }, pre+ (r) = {a1 , . . . , }, pre (r) = {am+1 , . . . , }, lit(r) = {f, b1 , . . . , bl ,a1 , . . . , }. Intuitively, pre(r) = pre + (r) pre (r) (resp. post(r) = post+ (r) post (r))accesses state (resp. after) action(s) happen.Special static rules may specified initial states.Definition A.3 (initial state constraint) initial state constraint static rule form (9)preceded initially.language K allows conditional execution actions, several alternative executabilityconditions may specified.60fiA NSWER ET P LANNING U NDER ACTION C OSTSDefinition A.4 (executability condition) executability condition e expression formexecutable b1 , . . . , bk , bk+1 , . . . , bl(10)L+act b1 , . . . , bl L, l k 0.l = 0 (i.e., executability unconditional), skipped. parts e accessed h(e) ={a}, pre+ (e) = {b1 , . . . , bk }, pre (e) = {bk+1 , . . . , bl }, lit(e) = {a, b1 , . . . , bl }. Intuitively,pre(e) = pre+ (e) pre (e) refers state actions suitability evaluated.state action execution involved; convenience, define post+ (e) = post (e) = .causal rules executability conditions must satisfy following condition,similar safety logic programs: variable default-negated type literal must also occurliteral default-negated type literal. safety requested variables appearingliterals. reason variables appearing fluent action literals implicitly saferespective type declarations.Notation. causal rule, initial state constraint, executability condition r {post, pre, b},define (r) = + (r) (r), bs (r) = posts (r) pres (r).A.1.1 P LANNING OMAINSP LANNING P ROBLEMSDefinition A.5 (action description, planning domain) action description hD, Ri consistsfinite set action fluent declarations finite set R safe causation rules, safe initialstate constraints, safe executability conditions contain positive cyclic dependencies among actions. K planning domain pair PD = h, ADi, disjunction-freenormal Datalog program (the background knowledge) safe total well-foundedmodel (cf. (van Gelder, Ross, & Schlipf, 1991)) 8 AD action description. call PDpositive, default negation occurs AD.Definition A.6 (planning problem) planning problem P = hPD, qi pair planning domain PD query q, i.e.,g1 , . . . , gm , gm+1 , . . . , gn ? (i)(11)g1 , . . . , gn Lf l variable-free, n 0, 0 denotes plan length.A.2 Semanticsstart preliminary definition typed instantiation planning domain.similar grounding logic program, difference correctly typedfluent action literals generated.Let PD = h, hD, Rii planning domain, let (unique) answer set (Gelfond & Lifschitz, 1991). Then, (p(X 1 , . . . , Xn )) legal action (resp. fluent) instance action (resp. fluent) declaration form (8), substitution defined X1 , . . . , Xn{(t1 ), . . . , (tm )} . LPD denote set legal action fluent instances. instantiation planning domain respecting type information follows.8. total well-founded model, existing, corresponds unique answer set datalog program. Allowingmultiple answer sets would eventually lead ambiguities language.61fiE ITER , FABER , L EONE , P FEIFER & P OLLERESDefinition A.7 (typed instantiation) planning domain PD = h, hD, Rii, typed instantiation given PD = h, hD, Rii, grounding (over con )R = {(r) | r R, r }, r set substitutions variables r usingcon , lit((r)) Ldyn LPD (.LPD Lf l ).words, PD replace R ground versions, keep latterrules atoms fluent action literals agree declarations. sayPD = h, hD, Rii ground, R ground, moreover well-typed, PDPD coincide.A.2.1 TATESRANSITIONSDefinition A.8 (state, state transition) state w.r.t planning domain PD consistent setLf l (lit(PD) lit(PD) ) legal fluent instances negations. state transitiontuple = hs, A, s0 s, s0 states Lact lit(PD) set legal actioninstances PD.Observe state necessarily contain either f f legal instance ffluent, may even empty (s = ). State transitions constrained; donedefinition legal state transitions below. proceed analogy definition answer sets(Gelfond & Lifschitz, 1991), considering first positive (i.e., involving positive planning domain)general planning problems.follows, assume PD = h, hD, Rii well-typed ground planning domainunique answer set . PD, respective concepts definedtyped grounding PD.Definition A.9 (legal initial state) state 0 legal initial state positive PD, 0least set (w.r.t. ) post(c) 0 implies h(c) s0 , initial state constraintsstatic rules c R.positive PD state s, set L +act called executable action set w.r.t. s,exists executability condition e R h(e) = {a}, pre + (e)Lf l,typ sM ,+pre+ (e)L+act A, pre (e)(Lact sM ) = . Note definition allows modelingdependent actions, i.e. actions depend execution actions.Definition A.10 (legal state transition) Given positive PD, state transition = hs, A, 0called legal, executable action set w.r.t. 0 minimal consistent set satisfiescausation rules w.r.t. . is, every causation rule r R, (i) post(r) 0 ,(ii) pre(r) Lf l,typ , (iii) pre(r) Lact hold, h(r) 6= {false}h(r) s0 .extended general well-typed ground PD containing default negation usingGelfond-Lifschitz type reduction positive planning domain (Gelfond & Lifschitz, 1991).Definition A.11 (reduction) Let PD ground well-typed planning domain, let =hs, A, s0 state transition. Then, reduction PD = h, hD, Rt ii PD planningdomain Rt obtained R deleting62fiA NSWER ET P LANNING U NDER ACTION C OSTS1. r R, either post (r)(s0 ) 6= pre (r)(sAM ) 6= ,2. default literals L (L L) remaining r R.Note PD positive ground. extend definitions follows.Definition A.12 (legal initial state, executable action set, legal state transition) planningdomain PD, state s0 legal initial state, s0 legal initial state PD h,,s0 ; setexecutable action set w.r.t. state s, executable w.r.t. PD hs,A,i ; and, state transition= hs, A, s0 legal, legal PD .A.2.2 P LANSDefinition A.13 (trajectory) sequence state transitions = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . .,hsn1 , , sn ii, n 0, trajectory PD, s0 legal initial state PD hs i1 , Ai , si i,1 n, legal state transitions PD.n = 0, = hi empty s0 associated explicitly.Definition A.14 (optimistic plan) sequence action sets hA 1 , . . . , Ai i, 0, optimisticplan planning problem P = hPD, qi, trajectory = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . . ,hsi1 , Ai , si ii exists PD accomplishes goal, i.e., {g 1 , . . . gm } si {gm+1 , . . . , gn }si = .Optimistic plans amount plans, valid plans etc defined literature. termoptimistic stress credulous view definition, respect incomplete fluentinformation nondeterministic action effects. cases, execution optimistic planP might fail reach goal. thus resort secure plans.Definition A.15 (secure plans (alias conformant plans)) optimistic plan hA 1 , . . . , secure plan, every legal initial state 0 trajectory = hhs0 , A1 , s1 i, . . . , hsj1 , Aj , sj ii0 j n, holds (i) j = n accomplishes goal, (ii) j < n, j+1executable sj w.r.t. PD, i.e., legal transition hs j , Aj+1 , sj+1 exists.Note plans admit general concurrent execution actions. call plan hA 1 , . . . ,sequential (or non-concurrent), |A j | 1, 1 j n.A.3 MacrosK includes several macros shorthands frequently used concepts. Let L +act denoteaction atom, f Lf l fluent literal, B (possibly empty) sequence b 1 , . . . , bk , bk+1 , . . . ,bl bi Lf l,typ , = 1, . . . , l, (possibly empty) sequence 1 , . . . , ,am+1 , . . . , aj L, j = 1, . . . , n.Inertia allow easy representation fluent inertia, K providesinertial f B A.Defaultscaused f .f, B f, A.default value fluent expressed shortcutdefault f.caused f .f.effect unless causation rule provides evidence opposite value.63fiE ITER , FABER , L EONE , P FEIFER & P OLLERESTotalityreasoning incomplete, total knowledge K provides (f positive):total f B A.caused f f, B A.caused f f, B A.instance useful model non-deterministic action effects. discussionfull impact statement modeling planning incomplete knowledge non-determinism,refer previous paper language K (Eiter et al., 2003b).State Integrityintegrity constraints refer preceding state, K providesforbidden B A.Non-executabilitycaused false B A.specifying action executable, K providesnonexecutable B.caused false a, B.definition, nonexecutable overrides executable case conflicts.Sequential Plans exclude simultaneous execution actions, K providesnoConcurrency.caused false a1 , a2 .a1 a2 range possible actions 1 , a2 LPD Lact a1 6= a2 .macros, B (resp. A) omitted, B (resp. A) empty.Appendix B. ProofsProof Theorem 4.4: Membership (i): problems NP resp. NPMV, since l polynomial size P, optimistic plan P = hA 1 , . . . , Al P supporting trajectory= ht1 , . . . , ti P guessed and, Proposition 4.1, verified polynomial time. Furthermore, costP (P ) b efficiently checked, since costP (P ) easily computed (all costsconstants).Hardness (i): K fragment K c , K planning problem viewed problemdeciding existence resp. finding admissible plan wrt. cost 0. previously shown(Eiter et al., 2003b), deciding existence optimistic plan given K planning problemNP-hard fixed plan length l; hence, also NP-hard Kc .show finding optimistic plan hard NPMV reduction well-knownSAT problem, cf. (Papadimitriou, 1994), whose instances CNFs = c 1 ck clauses ci =Li,1 Li,mi , Li,j classical literal propositional atoms X = {x 1 , . . . , xn }.Consider following planning domain PD :fluents :actions :x1 . . . . xn . state0. state1.c1 costs 1. . . . ck costs 1.ax1 . . . . axn .initially : total x1 . . . . total xn .caused state0.always :caused state1 state0.executable c1 .L1,1 , . . . , .L1,m1 .forbidden .L1,1 , . . . , .L1,m1 , c1 .executable ck .Lk,1 , . . . , .Lk,mk .forbidden .Lk,1 , . . . , .Lk,mk , ck .executable ax1 x1 . forbidden x1 , ax1 .executable axn xn . forbidden xn , axn .64fiA NSWER ET P LANNING U NDER ACTION C OSTSfluents xi state0 total statements initially-section encode candidate truth assignments. subsequent statements force c j executed iff correspondingclause violated truth assignment encoded initial state. final pairs executableforbidden statements force actions ax executed iff corresponding fluents x hold.necessary directly extract computed truth assignments plan,since dealing function class. fluent state1 identifies state time 1.Consider planning problem P = hPD , state1?(1)i. Clearly, optimistic planP P corresponds truth assignment P X vice versa, costP (P ) numberclauses violated P . Thus, admissible optimistic plans P wrt. cost 0 correspond 1-1satisfying assignments . Clearly, constructing P efficiently possible,constructing satisfying truth assignment corresponding plan P (because actionsaxi ). concludes hardness proof.Membership (ii): Since security optimistic plan admissible wrt. cost k checked,Proposition 4.1, call P2 -oracle, membership P3 resp. P3 MV followsanalogous considerations (i) (where oracle needed).Hardness (ii): decision variant, P3 -hardness immediately inherited P3 completeness deciding existence secure plan problem language K,hardness even fixed plan length (Eiter et al., 2003b). plan computation variant, givereduction following P3 MV-complete problem: instance open QBFQ[Z] = XY [X, Y, Z]X = x1 , . . . , xl , = y1 , . . . , ym , Z = z1 , . . . , zn , respectively, [X, Y, Z](w.l.o.g.) 3CNF formula X, , Z. solutions S(I) truth assignments ZQ[Z] satisfied.Suppose [X, Y, Z] = c1 . . . ck ci = ci,1 ci,2 ci,3 . consider followingplanning domain PDQ[Z] Q[Z], variant planning domain given proofTheorem 5.5 (Eiter et al., 2003b):fluents :x1 . . . . xl . y1 . . . . ym . z1 . . . . zn . state0. state1.actions :az1 costs 0. . . . azn costs 0.initially : total x1 . . . . total xl .caused state0.always :caused state1 state0.executable az1 . executable az2 . . . . executable azn .caused x1 x1 . caused x1 x1 .caused xl xl . caused xl xl .total y1 state0. . . . total ym state0.caused z1 az1 . caused z1 az1 .caused zn azn . caused zn azn .forbidden .C1,1 , .C1,2 , .C1,3 state0.forbidden .Ck,1 , .Ck,2 , .Ck,3 state0.|X|2|X| many legal initial states s1 , . . . , s2 PDQ[Z] , correspond 1-1possible truth assignments X initial states contain state0. Starting initialstate si , executing set actions represents truth assignment variables Z. Since65fiE ITER , FABER , L EONE , P FEIFER & P OLLERESactions always executable, 2 |Z| executable action sets A1 , . . . , A2|Z| , representtruth assignments Z.|Y |pair si Aj exist 2|Y | many successor state candidates si,1 , . . . , si,2 ,contain fluents according truth assignment X represented , fluents accordingtruth assignment Z represented j , fluents according truth assignment ,fluent state1. candidate states, satisfying clauses [X, Y, Z] legal,virtue forbidden statements.hard see optimistic plan form P = hA 1 (where A1 {azi | zi Z})goal state1 exists wrt. PDQ[Z] iff assignment variables X Zformula [X, Y, Z] satisfied. Furthermore, P secure iff A1 represents assignmentvariables Z that, regardless assignment variables X chosen(corresponding legal initial state ), assignment variablesclauses [X, Y, Z] satisfied (i.e., least one state si,k reachable si executingA1 ); si,k contains state1. words, P secure iff [X, Y, Z] true. Thus,admissible secure plans PDQ[Z] wrt. cost 0, correspond 1-1 assignments ZQ[Z] true.Since PDQ[Z] constructible [X, Y, Z] polynomial time, follows computingsecure plan P = hPDQ[Z] , qi, q = state1 ? (1), P3 MV-hard.2Proof Theorem 4.5: Membership (i): Concerning membership, performing binary searchrange [0, max] (where max upper bound plan costs plan polynomiallength l given l times sum action costs) find least integer voptimistic plan P P admissible wrt. cost v exists (if optimistic plan exists);clearly, costP (P ) = v costP = v, thus plan P optimal. Since maxsingle exponential representation size P, binary search, thus computing cost P ,is, Theorem 4.4, feasible polynomial time NP oracle. Subsequently, constructoptimistic plan P costP (P ) = costP extending partial plan Pi = hA1 , . . . , Ai i,= 0, . . . , l 1 step step follows. Let = {a 1 , . . . , } set legal actioninstances. initialize Bi+1 := ask oracle whether Pi completed optimisticplan P = hA1 , . . . , Al admissible wrt. costP Ai+1 (Bi+1 \ {a1 }). answeryes, update Bi+1 := Bi+1 \ {a1 }, else leave Bi+1 unchanged. repeat testaj , j = 2, 3, . . . , m; resulting Bi+1 action set Pi+1 = hA1 , . . . , Ai , Ai+1Ai+1 = Bi+1 completed optimistic plan admissible wrt. cost P . Thus, Ai+1polynomial-time constructible NP oracle.summary, construct optimal optimistic plan polynomial time NP oracle.Thus, problem FP2 .Hardness (i): show hardness plan length l = 1 reduction problem MAX WEIGHTSAT (Papadimitriou, 1994), instance SAT instance = c 1 ck proofTheorem 4.4.(i), plus positive integer weightsPw , = 1, . . . , k. Then, S(I) containstruth assignments X wsat () = : ci =true wi maximal.end, take planning domain PD proof Theorem 4.4 modifycost ci wi , = 1, . . . , k, thus constructing new planning domain PD . Considerplanning problem PI = hPDI , state1?(1)i. Since actions cj actions nonzerocost, plan (corresponding toPtruth assignment )P associated sum weightsviolated clauses, wvio () = ( ki=1 wi ) wsat (). Since ki=1 wi constant I, minimizing66fiA NSWER ET P LANNING U NDER ACTION C OSTSwvio () equivalent maximizing wsat (). Hence, one-to-one correspondenceoptimal optimistic plans PI (for wvio () minimal) maximal truth assignments I.Furthermore, computing PI extracting MAX-WEIGHT SAT solution optimalplan P efficiently possible. proves FP2 -hardness.Membership (ii): proof similar membership proof (i), uses oracle askscompletion partial secure plan P = hA1 , . . . , Ai secure plan P = hA1 , . . . , AlAi+1 (Bi+1 \ {aj }) P admissible wrt. costP , rather partial optimistic plan.oracle is, easily seen, P3 . Thus, computing optimal secure plan F P4 .Hardness (ii): show hardness reduction following problem, F P4 complete (cf. (Krentel, 1992)): Given open QBF Q[Z] = XY [X, Y, Z] like proofTheorem 4.4.(ii), compute lexicographically first truth assignment Z Q[Z]satisfied.accomplished changing cost action az PDQ[Z] 0 2ni ,= 1, . . . , n. Let PD 0 [Q[Z]] resulting planning domain. Since cost az (i.e., assigningzi value true) greater sum costs az j + 1 j n, optimalsecure plan planning problem hPD 0 [Q[Z]], state1 ? (1)i amounts lexicographicallyfirst truth assignment Z Q[Z] satisfied. Thus, FP4 -hardness problem follows.2Proof Theorem 6.1: prove result applying well-known Splitting Set Theoremlogic programs (Lifschitz & Turner, 1994). theorem applies logic programssplit two parts one them, bottom part, refer predicates definedtop part all. answer sets bottom part extended answer setswhole program looking remaining (top) rules. Informally, splitting setset U ground literals defining bottom part bU () program. answer set SbbU () used reduce remaining rules \ b U () program eU ( \ bU (), Sb )involving classical literals occur b U (), evaluating literals b U ()wrt. Sb . answer set Se eU ( \ bU (), Sb ), set = Sb Se answer setoriginal program.Disregarding weak constraints, split program lp w (P) bottom part consistinglp(Pnc ), Pnc P cost information stripped off, top part containingremaining rules; derive correspondence optimistic plans P answer setslpw (P) similar correspondence result lp(P nc ) (Eiter et al., 2003a).detail, Theorem 3.1 (Eiter et al., 2003a) states K-planning problem P correspondence answer sets lp(P) supporting trajectories optimistic plansP = hA1 , . . . , Al items (i) (ii), costs discarded. Thus, answer set 0 lp(Pnc )corresponds trajectory 0 optimistic plan P 0 Pnc vice versa.follows, talking lp(P nc ) lpw (P), mean respective groundedlogic programs. lpw (P) augments lp(Pnc ) rules (4) weak constraints (5). Let U =lit(lp(Pnc )) set literals occurring lp(P nc ). Clearly, U splits lpw (P) defined(Lifschitz & Turner, 1994), disregard weak constraints lp w (P), since rules form(4) introduce new head literals. Consequently, get b U (lpw (P)) = lp(Pnc ). Then,answer set 0 lp(Pnc ), rule eU (lpw (P) \ bU (lpw (P)), 0 ) formcosta (x1 , . . . , xn , t, c) :- Body.67fiE ITER , FABER , L EONE , P FEIFER & P OLLERESfact rules positive, conclude respect split U ,answer set 0 lp(Pnc ) induces unique answer set 0 lpw (P). Therefore, modulocosts, correspondence supporting trajectories candidate answer sets claimedfollows directly Theorem 3.1 (Eiter et al., 2003a).remains prove costP (P ) = costlpw (P) (S) holds candidate answer sets corresponding optimistic plan P = hA 1 , . . . , Al P. correspondence shown above,action p(x1 , . . . , xn ) Aj corresponds exactly one atom p(x 1 , . . . , xn , j 1) ASj ,j {1, . . . , l}. Therefore, p(x1 , . . . , xn ) declared non-empty cost part, (4)well-definedness, modulo x1 , . . . , xn , exactly one fact costp (x1 , . . . , xn , j 1, c)model eU (lpw (P) \ bU (lpw (P)), S).Furthermore, definition (4), c = costj (p(x1 , . . . , xn )), i.e., cost actioninstance p(x1 , . . . , xn ) time j. Consequently,PlP violation value weak constraint wcwform (5) p lp (P) costwc (S) =j=1p(x1 ,...,xn )Aj costj (p(x1 , . . . , xn )). Sinceviolation values stem weak constraints (5), total cost lpw (P) (S) = costP (P ).proves result.2ReferencesBlum, A. L., & Furst, M. L. (1997). Fast Planning Planning Graph Analysis. ArtificialIntelligence, 90, 281300.Bonet, B., & Geffner, H. (2000). Planning Incomplete Information Heuristic SearchBelief Space. Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.), ProceedingsFifth International Conference Artificial Intelligence Planning Scheduling (AIPS00),pp. 5261, Breckenridge, Colorado, USA.Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, C-35(8), 677691.Buccafurri, F., Leone, N., & Rullo, P. (1997). Strong Weak Constraints Disjunctive Datalog.Dix, J., Furbach, U., & Nerode, A. (Eds.), Proceedings 4th International ConferenceLogic Programming Non-Monotonic Reasoning (LPNMR97), No. 1265 LectureNotes AI (LNAI), pp. 217, Dagstuhl, Germany. Springer Verlag.Buccafurri, F., Leone, N., & Rullo, P. (2000). Enhancing Disjunctive Datalog Constraints. IEEETransactions Knowledge Data Engineering, 12(5), 845860.Cimatti, A., & Roveri, M. (2000). Conformant Planning via Symbolic Model Checking. JournalArtificial Intelligence Research, 13, 305338.Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity Expressive PowerLogic Programming. ACM Computing Surveys, 33(3), 374425.Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding Planning Problems NonmonotonicLogic Programs. Proceedings European Conference Planning 1997 (ECP-97),pp. 169181. Springer Verlag.Eiter, T., Faber, W., Leone, N., & Pfeifer, G. (2000a). Declarative Problem-Solving UsingDLV System. Minker, J. (Ed.), Logic-Based Artificial Intelligence, pp. 79103. KluwerAcademic Publishers.68fiA NSWER ET P LANNING U NDER ACTION C OSTSEiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2000b). Planning IncompleteKnowledge. Lloyd, J., Dahl, V., Furbach, U., Kerber, M., Lau, K.-K., Palamidessi, C.,Pereira, L. M., Sagiv, Y., & Stuckey, P. J. (Eds.), Computational Logic - CL 2000, First International Conference, Proceedings, No. 1861 Lecture Notes AI (LNAI), pp. 807821,London, UK. Springer Verlag.Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2002a). Answer Set PlanningAction Costs. Flesca, S., Greco, S., Ianni, G., & Leone, N. (Eds.), Proceedings8th European Conference Artificial Intelligence (JELIA), No. 2424 Lecture NotesComputer Science, pp. 186197.Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2002b). Answer Set Planning Action Costs. Tech. rep. INFSYS RR-1843-02-13, Institut fur Informationssysteme, TechnischeUniversitat Wien.Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003a). Logic Programming ApproachKnowledge-State Planning, II: DLV K System. Artificial Intelligence, 144(12), 157211.Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003b). Logic Programming ApproachKnowledge-State Planning: Semantics Complexity. appear ACM TransactionsComputational Logic.Ephrati, E., Pollack, M. E., & Mihlstein, M. (1996). Cost-directed Planner: Preliminary Report.Proceedings Thirteenth National Conference Artificial Intelligence (AAAI-96),pp. 1223 1228. AAAI Press.Erdem, E. (1999). Applications Logic Programming Planning: Computational Experiments.Unpublished draft. http://www.cs.utexas.edu/users/esra/papers.html.Faber, W., & Pfeifer, G. (since 1996). DLV homepage.. http://www.dlvsystem.com/.Ferraris, P., & Giunchiglia, E. (2000). Planning Satisfiability Nondeterministic Domains.Proceedings Seventeenth National Conference Artificial Intelligence (AAAI00), July30 August 3, 2000, Austin, Texas USA, pp. 748753. AAAI Press / MIT Press.Fourer, R., Gay, D. M., & Kernighan, B. W. (1993). AMPL: Modeling Language MathematicalProgramming. Duxbury Press.Gelfond, M., & Lifschitz, V. (1991). Classical Negation Logic Programs DisjunctiveDatabases. New Generation Computing, 9, 365385.Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Veloso, M., Weld,D., & Wilkins, D. (1998).PDDL Planning Domain Definition language. Tech. rep., Yale Center Computational Vision Control. Availablehttp://www.cs.yale.edu/pub/mcdermott/software/pddl.tar.gz.Giunchiglia, E. (2000). Planning Satisfiability Expressive Action Languages: Concurrency,Constraints Nondeterminism. Cohn, A. G., Giunchiglia, F., & Selman, B. (Eds.), Proceedings Seventh International Conference Principles Knowledge RepresentationReasoning (KR 2000), April 12-15, Breckenridge, Colorado, USA, pp. 657666. MorganKaufmann.69fiE ITER , FABER , L EONE , P FEIFER & P OLLERESGiunchiglia, E., Kartha, G. N., & Lifschitz, V. (1997). Representing Action: IndeterminacyRamifications. Artificial Intelligence, 95, 409443.Giunchiglia, E., & Lifschitz, V. (1998). Action Language Based Causal Explanation: Preliminary Report. Proceedings Fifteenth National Conference Artificial Intelligence(AAAI 98), pp. 623630.Haslum, P., & Geffner, H. (2000). Admissible Heuristics Optimal Planning. Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.), Proceedings Fifth International ConferenceArtificial Intelligence Planning Scheduling (AIPS00), pp. 140149, Breckenridge, Colorado, USA. AAAI Press.Kautz, H., & Walser, J. P. (1999). State-space planning integer optimization. Proceedings16th National Conference Artificial Intelligence (AAAI-99), pp. 526533.Koehler, J. (1998). Planning Resource Constraints. Proceedings 13th EuropeanConference Artificial Intelligence (ECAI98), pp. 489493.Krentel, M. (1992). Generalizations Opt P Polynomial Hierarchy. Theoretical ComputerScience, 97(2), 183198.Lee, J., & Lifschitz, V. (2001). Additive Fluents. Provetti, A., & Cao, S. T. (Eds.), ProceedingsAAAI 2001 Spring Symposium Answer Set Programming: Towards Efficient ScalableKnowledge Representation Reasoning, pp. 116123, Stanford, CA. AAAI Press.Lifschitz, V., & Turner, H. (1994). Splitting Logic Program. Van Hentenryck, P. (Ed.), Proceedings 11th International Conference Logic Programming (ICLP94), pp. 2337,Santa Margherita Ligure, Italy. MIT Press.Lifschitz, V., & Turner, H. (1999). Representing Transition Systems Logic Programs. Gelfond,M., Leone, N., & Pfeifer, G. (Eds.), Proceedings 5th International Conference LogicProgramming Nonmonotonic Reasoning (LPNMR99), No. 1730 Lecture Notes AI(LNAI), pp. 92106, El Paso, Texas, USA. Springer Verlag.Lifschitz, V. (1996). Foundations Logic Programming. Brewka, G. (Ed.), Principles Knowledge Representation, pp. 69127. CSLI Publications, Stanford.Lifschitz, V. (1999a). Action Languages, Answer Sets Planning. Apt, K., Marek, V. W.,Truszczynski, M., & Warren, D. S. (Eds.), Logic Programming Paradigm 25-YearPerspective, pp. 357373. Springer Verlag.Lifschitz, V. (1999b). Answer Set Planning. Schreye, D. D. (Ed.), Proceedings 16thInternational Conference Logic Programming (ICLP99), pp. 2337, Las Cruces, NewMexico, USA. MIT Press.McCain, N. (1999). Causal Calculator Homepage.. http://www.cs.utexas.edu/users/tag/cc/.McCain, N., & Turner, H. (1997). Causal Theories Actions Change. Proceedings15th National Conference Artificial Intelligence (AAAI-97), pp. 460465.McCain, N., & Turner, H. (1998). Satisfiability Planning Causal Theories. Cohn, A. G.,Schubert, L., & Shapiro, S. C. (Eds.), Proceedings Sixth International Conference Principles Knowledge Representation Reasoning (KR98), pp. 212223. Morgan KaufmannPublishers.70fiA NSWER ET P LANNING U NDER ACTION C OSTSMoskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: EngineeringEfficient SAT Solver. Proceedings 38th Design Automation Conference, DAC 2001,Las Vegas, NV, USA, June 18-22, 2001, pp. 530535. ACM.Nareyek, A. (2001). Beyond Plan-Length Criterion. Local Search Planning Scheduling, ECAI 2000 Workshop, Vol. 2148 Lecture Notes Computer Science, pp. 5578.Springer.Niemela, I. (1998). Logic Programs Stable Model Semantics Constraint ProgrammingParadigm. Niemela, I., & Schaub, T. (Eds.), Proceedings Workshop Computational Aspects Nonmonotonic Reasoning, pp. 7279, Trento, Italy.Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.Pednault, E. P. D. (1989). Exploring Middle Ground STRIPS Situation Calculus. Proceedings 1st International Conference Principles Knowledge Representation Reasoning (KR89), pp. 324332, Toronto, Canada. Morgan Kaufmann Publishers,Inc.Refanidis, I., & Vlahavas, I. (2001). Framework Multi-Criteria Plan Evaluation HeuristicState-Space Planning. IJCAI-01 Workshop Planning Resources.Selman, A. L. (1994). Taxonomy Complexity Classes Functions. Journal ComputerSystem Sciences, 48(2), 357381.Simons, P., Niemela, I., & Soininen, T. (2002). Extending Implementing Stable ModelSemantics. Artificial Intelligence, 138, 181234.Smith, D. E., & Weld, D. S. (1998). Conformant Graphplan. Proceedings FifteenthNational Conference Artificial Intelligence, (AAAI98), pp. 889896. AAAI Press /MIT Press.Son, T. C., & Pontelli, E. (2002). Reasoning Actions Prioritized Default Theory. Flesca,S., Greco, S., Ianni, G., & Leone, N. (Eds.), Proceedings 8th European ConferenceArtificial Intelligence (JELIA), No. 2424 Lecture Notes Computer Science, pp. 369381.Subrahmanian, V., & Zaniolo, C. (1995). Relating Stable Models AI Planning Domains.Sterling, L. (Ed.), Proceedings 12 th International Conference Logic Programming,pp. 233247, Tokyo, Japan. MIT Press.van Gelder, A., Ross, K., & Schlipf, J. (1991). Well-Founded Semantics General LogicPrograms. Journal ACM, 38(3), 620650.Weld, D. S., Anderson, C. R., & Smith, D. E. (1998). Extending Graphplan Handle Uncertainty& Sensing Actions. Proceedings Fifteenth National Conference Artificial Intelligence, (AAAI98), pp. 897904. AAAI Press / MIT Press.Williams, M., & Hanks, S. (1994). Optimal Planning Goal-Directed Utility Model.Hammond, K. J. (Ed.), Proceedings Second International Conference Artificial Intelligence Planning Systems (AIPS-94), pp. 176181. AAAI Press.71fiJournal Artificial Intelligence Research 19 (2003) 155-203Submitted 1/03; published 9/03Representing Aggregating Conflicting BeliefsPedrito Maynard-Zhangmaynarp@muohio.eduDepartment Computer Science Systems AnalysisMiami UniversityOxford, Ohio 45056, USADaniel Lehmannlehmann@cs.huji.ac.ilSchool Computer Science EngineeringHebrew UniversityJerusalem 91904, IsraelAbstractconsider two-fold problem representing collective beliefs aggregatingbeliefs. propose novel representation collective beliefs uses modular,transitive relations possible worlds. allow us represent conflicting opinionsclear semantics, thus improving upon quasi-transitive relations oftenused social choice. describe way construct belief state agentinformed set sources varying degrees reliability. construction circumventsArrows Impossibility Theorem satisfactory manner accounting explicitlyencoded conflicts. give simple set-theory-based operator combining information multiple agents. show operator satisfies desirable invariantsidempotence, commutativity, associativity, and, thus, well-behaved iterated,describe computationally effective way computing resulting belief state.Finally, extend framework incorporate voting.1. Introductioninterested multi-agent setting agents informed sources varyinglevels reliability, agents iteratively combine belief states. settingintroduces three problems: (1) Finding appropriate representation collective beliefs;(2) Constructing agents belief state aggregating information informantsources, accounting relative reliability sources; and, (3) Combininginformation multiple agents manner well-behaved iteration.addressing first problem, take starting point total preorders possibleworlds (i.e., interpretations specified language) used belief revision communityrepresent individuals beliefs. relations describe opinions relative likelihoodworlds viewed encoding agents conditional beliefs, i.e.,believes now, would believe conditions. representationbased semantical work (cf. Grove, 1988; Katsuno & Mendelzon, 1991) supportingAlchourron, Gardenfors, Makinson proposal (Alchourron, Gardenfors, & Makinson,1985; Gardenfors, 1988) (known AGM theory) belief revision.social choice community dealt extensively problem representingcollective preferences (cf. Sen, 1986). However, problem formally equivalentrepresenting collective beliefs, results applicable. classical approachc2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiMaynard-Zhang & Lehmannuse quasi-transitive relations relations whose asymmetric restrictions transitiveset objects. (Total preorders special subclass relations.) However,relations distinguish group indifference group conflict,distinction crucial. Consider, example, situation membersgroup indifferent movie movie b. passerby expresses preferencea, group may well choose adopt opinion group borrow a.However, group already divided relative merits b, wouldwise hesitate choosing one new supporter appearsscene. propose representation distinction explicit. Specifically,propose modular, transitive relations argue solve unpleasantsemantical problems suffered earlier approach. (We define modularity precisely later,viewed intuitively sufficient relaxation totality requirement totalpreorders make distinction indifference conflict possible.)second problem addresses agent actually go combininginformation received set sources create belief state. mechanismfavor opinions held reliable sources, yet allow less reliable sources voice opinions higher ranked sources opinion. True, circumstances wouldadvisable opinion less reliable source override agnosticismreliable source, often better accept opinions default assumptionsbetter information available. define mechanism this, relyinggeneralized representation circumvent Arrows (1963) Impossibility Theoremsources equal reliability.motivate third problem, consider following dynamic scenario: robot controlling ship space receives number communication centers Earth informationstatus environment tasks. center receives informationgroup sources varying credibility accuracy (e.g., nearby satellites experts)aggregates it. Timeliness decision-making space often crucial, wantrobot wait center sends information central locationfirst combined forwarded robot. Instead, center sendsaggregated information directly robot. scheme reduce deadtime, also allows anytime behavior robots part: robot incorporates newinformation arrives makes best decisions whatever informationgiven point. distributed approach also robust since degradationperformance much graceful information individual centers get lostdelayed.scenario, robot needs mechanism combining fusing belief statesmultiple agents potentially arriving different times. Moreover, belief state outputmechanism invariant respect order agent arrivals.describe simple set-theoretic mechanism satisfies requirements wellcomputationally effective way computing resulting belief state.aggregation fusion mechanisms described far take account qualitysupport opinions, completely ignore quantity support. However, latter often provides sufficient information resolve apparent conflicts. Take, example,situation sources robot equal credibility exceptsmall minority suggest robot move spaceship avoid potential collision156fiRepresenting Aggregating Conflicting Beliefsoncoming asteroid. situation, often prefer resolve conflict sidingmajority. end, describe extend framework allow voting,introducing novel modular closure operation process.preliminary definitions, address topics turn.2. Preliminariesbegin defining various well-known properties binary relations1 ; usefulus throughout paper.Definition 1 Suppose relation finite set , i.e., .2 usex denote (x, y) x 6 denote (x, y) 6. relation is:1. reflexive iff x x x . irreflexive iff x 6 x x .2. symmetric iff x x x, . asymmetric iff x 6 xx, . anti-symmetric iff x x x = x, .3. asymmetric restriction relation 0 iff x x 0 60 xx, . symmetric restriction 0 iff x x 0 0 xx, .4. total iff x x x, .5. modular iff x x z z x, y, z .6. transitive iff x z x z x, y, z .7. quasi-transitive iff asymmetric restriction transitive.8. transitive closure relation 0 iff, integer n,x w0 , . . . , wn . x = w0 0 0 wn =x, . (We generally use + denote transitive closure relation.)9. acyclic iff w0 , . . . , wn . w0 < < wn implies wn 6< w0 integers n,< asymmetric restriction .10. total preorder iff total transitive. total order iff also antisymmetric.11. equivalence relation iff reflexive, symmetric, transitive.12. fully connected iff x x, . fully disconnected iff x 6x, .Proposition 11. transitive closure modular relation modular.1. use binary relations paper, refer simply relations.2. readers convenience, included Appendix B key notational symbolsused throughout paper.157fiMaynard-Zhang & Lehmann2. Every transitive relation quasi-transitive.3. (Sen, 1986) Every quasi-transitive relation acyclic.Given relation set alternatives subset alternatives, oftenwant pick subsets best elements respect relation. define setbest elements subsets choice set:Definition 2 relation finite set , < asymmetric restriction,X , choice set X respectch(X, ) = {x X :6 x0 X. x0 < x}.choice function one assigns every (non-empty) subset X (non-empty) subsetX:Definition 3 choice function finite set function f : 2 \ 2 \f (X) X every non-empty X .Now, every acyclic relation defines choice function, one assigns subsetchoice set:Proposition 2 (Sen, 1986) Given relation finite set , choice set operationch defines choice function iff acyclic.3relation acyclic, elements involved cycle said conflictcannot order them:Definition 4 Given relation < finite set , x conflict wrt < iffexist w0 , . . . , wn , z0 , . . . , zm x = w0 < < wn = = z0 < < zm = x,x, .Finally, cardinality set denoted kk.Assume given language L satisfaction relation |= L. LetW finite, non-empty set possible worlds (interpretations) L. worldw W sentence p L, w |= p iff p evaluates true w. Given sentence p,|p| = {w W | w |= p}.3. Representing Collective Beliefsrepresentation collective beliefs generalizes representation developed beliefrevision community conditional beliefs individual, briefly review it.consider implications social choice representing collective beliefs. Finally,describe proposal argue desirability.3. Sens uses slightly stronger definition choice sets, theorem still holds generalcase.158fiRepresenting Aggregating Conflicting Beliefs3.1 Belief Revision Representation Conditional BeliefsMuch belief revision field built seminal work Alchourron, Gardenfors,Makinson (Alchourron et al., 1985; Gardenfors, 1988) refered AGM theory.work sought formalize Occams razor-like principle minimal change: setbeliefs resulting revision one produced modifying original beliefsminimally accomodate new information. capture principle precisely,proposed famous AGM postulates impose restrictions belief change operators.Subsequent model-theoretic work (Grove, 1988; Katsuno & Mendelzon, 1991; MaynardReid II & Shoham, 2001) showed accepting postulates amounts assumingindividuals belief state represented total preorder W; revisionindividuals beliefs sentence p L consists computing ch(|p|, ).Kraus, Lehmann, Magidor (1990) Lehmann Magidor (1992) developedsimilar central role ordered structures semantics nonmonotonic logics,Gardenfors Makinson (1994) established relation two topics. Semantically, represents weak relative likelihood possible worlds: x means possibleworld x considered least likely possible world y.4 x x,x considered equally likely. also interpret sententially using famousRamsey Test (Ramsey, 1931): encodes set conditional beliefs, i.e.,believed (called belief set), counterfactual beliefs well (what wouldbelieved conditions case). According criteria, conditionalbelief p q holds p q sentences L q satisfied worldsch(|p|, ); write Bel(p?q). neither belief p?q belief p?q hold beliefstate, said agnostic respect p?q, written Agn(p?q). belief set inducedbelief state consists sentences q Bel(true?q) holds.3.2 Social Choice Implicationsfirst inclination, then, would use total preorders represent collective beliefssince work well individuals beliefs. Unfortunately, approach inherentlyproblematic discovered early social choice community. communitysinterest lies representing collective preferences rather collective beliefs; however,results equally relevant since classical representation individuals preferencesalso total preorder. Instead relative likelihood, relations represent relative preference;instead equal likelihood, indifference.Arrows (1963) celebrated Impossibility Theorem showed aggregation operatortotal preorders exists satisfying following small set desirable properties:Definition 5 Let f aggregation operator relations 1 , . . . , n n individuals, respectively, finite set alternatives , let = f (1 , . . . , n ).Restricted Range: range f set total preorders .Unrestricted Domain: domain f set n-tuples total preorders.4. direction relation symbol unintuitive, standard practice belief revision community.159fiMaynard-Zhang & LehmannPareto Principle: x i, x y.5Independence Irrelevant Alternatives (IIA): Suppose 0 = f (01 , . . . , 0n ). If,x, , x iff x 0i i, x iff x 0 y.Non-Dictatorship: individual that, every tuple domainf every x, , x implies x y.Proposition 3 (Arrow, 1963) aggregation operator satisfies restrictedrange, unrestricted domain, Pareto principle, independendence irrelevant alternatives,nondictatorship.impossibility theorem led researchers look weakenings Arrows frameworkwould circumvent result. One weaken restricted range condition, requiringresult aggregation satisfy totality quasi-transitivity ratherfull transitivity total preorder. weakening sufficient guarantee existenceaggregation function satisfying conditions, still producing relationsdefined choice functions (Sen, 1986). However, solution withoutproblems.First, perhaps obviously, domain range aggregation operatordifferent, violating known belief revision literature principlecategorical matching (cf. Gardenfors Rotts 1995 survey). problem closely relatedsecond total, quasi-transitive relations unsatisfactory semantics.total quasi-transitive total preorder, indifference relationtransitive:Proposition 4 Let relation finite set let symmetric restriction.total quasi-transitive transitive, transitive.much discussion whether indifference transitive.many cases one feels indifference transitive; Deb indifferentplums mangoes also indifferent mangoes peaches, would greatlysurprised profess strong preference plums peaches.6 Thus, seemstotal quasi-transitive relations total preorders cannot understood easilypreference indifference. Since existence choice function generally sufficientclassical social choice problems, issues least ignorable. However, iteratedaggregation, result aggregation must usable making decisions,must interpretable new preference relation may involved later aggregationsand, consequently, must maintain clean semantics.Third, totality assumption excessively restrictive representing aggregate preferences. general, binary relation express four possible relationshipspair alternatives b: b b 6 a, b 6 b, b b a, 6 bb 6 a. Totality reduces set first three which, interpretation5. Technically, known weak Pareto principle. strong Pareto principle states xexists x x i. Obviously, strong version implies weakversion, Arrows theorem applies well.6. However, see Luces (1956) work semiorders opposing arguments transitivityindifference debate.160fiRepresenting Aggregating Conflicting Beliefsrelations representing weak preference, correspond two strict orderings b,indifference. However, consider situation couple trying chooseItalian Indian restaurant, one strictly prefers Italian food Indian food,whereas second strictly prefers Indian Italian. couples opinions conflict,situation fit three categories. Thus, totality assumptionessentially assumption conflicts exist. This, one may argue, appropriatewant represent preferences one agent (but see Kahneman Tverskys (1979)persuasive arguments individuals often ambivalent). However, assumptioninappropriate want represent aggregate preferences since individuals almostcertainly differences opinion.3.3 Generalized Belief Statesbelief aggregation formally similar preference aggregation, also susceptible problems faced social choice community. take view muchdifficulty encountered previous attempts define acceptable aggregation policieslack explicit representations conflicts among individuals. generalizetotal preorder representation capture information conflicts. generalization opens way semantically clear aggregation policies, added benefitfocusing attention culprit sets worlds.3.3.1 Modular, Transitive Statestake strict likelihood primitive. Since strict likelihood necessarily total,possible represent agnosticism conflicting opinions structure.choice deviates authors, similar Kreps (1990, p. 19)interested representing indifference incomparability. Unlike Kreps, ratheruse asymmetric relation represent strict likelihood (e.g., asymmetric restrictionweak likelihood relation), impose less restrictive condition modularity.formally define generalized belief states:Definition 6 generalized belief state modular, transitive relation W.set possible generalized belief states W denoted B.interpret b mean reason consider strictly likely b.represent equal likelihood, also refer agnosticism, relationshipdefined x x 6 6 x. define conflict relationcorresponding , denoted ./, x ./ iff x x. describes situationsreasons consider either pair worlds strictly likelyother. fact, one easily check ./ precisely represents conflicts belief statesense Definition 4.convenience, refer generalized belief states simply belief states exceptwould cause confusion.3.3.2 DiscussionLet us consider choice representation justified. First, agree socialchoice community strict likelihood transitive.161fiMaynard-Zhang & Lehmanndiscussed above, often compelling reason agnosticism/indifferencetransitive; also adopt view. However, transitivity strict likelihoodguarantee transitivity agnosticism. simple example following:W = {a, b, c} = {(a, c)}, = {(a, b), (b, c)}. However, buy strict likelihood transitive, agnosticism transitive identically strict likelihoodalso modular:Proposition 5 Suppose relation transitive corresponding agnosticismrelation. transitive iff modular.summary, transitivity modularity necessary strict likelihood agnosticismrequired transitive.point conflicts also transitive framework. first glance,may appear undesirable: entirely possible group disagree relativelikelihood worlds b, b c, yet agree likely c. However,note transitivity follows cycle-based definition conflicts (Definition 4),belief state representation. highlights fact concernedconflicts arise simple disagreements pairs alternatives,inferred series inconsistent opinions well.Now, argue modular, transitive relations sufficient capture relative likelihood, agnosticism, conflicts among group information sources, first pointadding irreflexivity would give us class relations asymmetric restrictionstotal preorders, i.e., conflict-free. Let set total preorders W, T< , setasymmetric restrictions.Proposition 6 T< B set irreflexive relations B.Secondly, following representation theorem shows belief state partitionspossible worlds sets worlds either equally likely potentially involvedconflict, totally orders sets; worlds distinct sets relationsets.Proposition 7 B iff partition W = hW0 , . . . , Wn W that:1. every x Wi Wj , 6= j implies < j iff x y.2. Every Wi either fully connected (w w0 w, w0 Wi ) fully disconnected(w 6 w0 w, w0 Wi ).Figure 1 shows three examples belief states: one total preorder, oneasymmetric restrictions total preorder, one neither. (Each circlerepresents worlds W satisfy sentence inside. arc circlesindicates w w0 every w head circle w0 tail circle; arc indicatesw 6 w0 pairs. particular, set worlds represented circlefully connected arc circle itself, fully disconnected otherwise.)Thus, generalized belief states big change asymmetric restrictionstotal preorders. merely generalize weakening assumption setsworlds strictly ordered equally likely, allowing possibility conflicts.distinguish agnostic conflicting conditional beliefs. belief state162fiRepresenting Aggregating Conflicting BeliefsPPPPPP(a)(b)(c)Figure 1: Three examples generalized belief states: (a) total preorder, (b) asymmetric restriction total preorder, (c) neither.agnostic conditional belief p?q (i.e., Agn(p?q)) choice set worlds satisfyingp contains worlds satisfy q q fully disconnected. conflictbelief, written Con(p?q), choice set fully connected.Finally, compare representational power definitions discussedprevious section. First, companion result Proposition 6, obvious Bsubsumes class total preorders and, fact, set reflexive relations B.Proposition 8 B set reflexive relations B.Secondly, B neither subsumes subsumed set total, quasi-transitive relations,intersection two classes . Let Q set total, quasi-transitiverelations W, Q< , set asymmetric restrictions.Proposition 91. Q B = .2. B 6 Q.3. Q 6 B W least three elements.4. Q B W one two elements.modular, transitive relations represent strict preferences, probably fairercompare class asymmetric restrictions total, quasi-transitive relations.Again, neither class subsumes other, time intersection T< :Proposition 101. Q< B = T< .2. B 6 Q< .3. Q< 6 B W least three elements.4. Q< B W one two elements.Note generalized belief states described extremely rich would requireoptimization practice avoid high maintenance cost. Although issue somewhatoutside scope paper, address (in respective sections) ways minimizeexplosion complexity complications fusion votingintroduced.next section, define natural aggregation policy based new representation admits clear semantics obeys appropriately modified versions Arrowsconditions.163fiMaynard-Zhang & Lehmann4. Single-Agent Belief State ConstructionSuppose agent informed set sources, individual belief state.Suppose, further, agent ranked sources level credibility. proposeoperator constructing agents belief state aggregating belief statessources accounting credibility ranking sources.Example 1 use running example space robot domain help provideintuition definitions. robot sends earth stream telemetry data gatheredspacecraft, long receives positive feedback data received.point loses contact automatic feedback system, sends requestinformation agent earth find failure caused failurefeedback system overload data retrieval system. former case, wouldcontinue send data, latter, desist. happens, overload,computer running feedback system hung. agent consults followingthree experts, aggregates beliefs, sends results back robot:1. sp , computer programmer developed feedback program, believes nothingcould ever go wrong code, must overload problem.However, admits program crashed, problem could ripplecause overload.2. sm , manager telemetry division, unfortunately out-dated informationfeedback system working. also told engineer soldsystem overloading could never happen. idea would happenoverload feedback system crashed.3. st , technician working feedback system, knows feedback systemcrashed, doesnt know whether data-overload. familiarretrieval system, also unable speculate whether data retrieval systemwould overloaded feedback system failed.Let F propositional variables representing feedback data retrievalsystems, respectively, okay. belief states three sources shown Figure 2.FDFDFDFFFDFDFspsmstFigure 2: belief states sp , sm , st Example 1.164fiRepresenting Aggregating Conflicting Beliefs4.1 SourcesLet us begin formal development defining sources belief states:Definition 7 finite set sources. source associated belief state<s B.denote agnosticism conflict relations source ./s , respectively.possible assume belief state source conflict-free, i.e., acyclic. However,necessary allow sources suffer human malady tornpossibilities.assume agents credibility ranking sources total preorder builttotally ordered set ranks (e.g., integers).Definition 8 R totally ordered finite set ranks.Definition 9 rank : R assigns source rank. Also, S, ranks(S) denotes set {r R : S. rank(s) = r}.Definition 10 total preorder induced ordering R denotedw. is, w s0 iff rank(s) rank(s0 ); say credible s0 . restriction wdenoted wS .use denote asymmetric symmetric restrictions w, respectively.7finiteness (R) ensures maximal source (rank) always exists, necessary results. Weaker assumptions possible, price unnecessarily complicating discussion. Also observe R arbitrary totallyordered set. Thus, allow numeric ranking systems (such integers),non-numeric systems well (e.g., military ranks). Furthermore, generality allowsproposal easily accommodate applications new ranks need dynamicallyadded inconvenient impossible change rank labels existing sources(e.g., large workers union members ranked relative level qualityexperience).ready consider source aggregation problem. following, assumeagent informed set sources S. look two special casesaggregationequally ranked strictly ranked sourcesbefore considering general case.4.2 Aggregating Equally Ranked SourcesSuppose sources rank wS fully connected. Intuitively,want take offered opinions seriously, take union relations:Definition 11 S, Un(S) relation sS <s .simply taking union source belief states, may lose transitivity. However,lose modularity:Proposition 11 S, Un(S) modular necessarily transitive.7. Note that, unlike relations representing belief states, w read intuitive way, is,greater corresponds better.165fiMaynard-Zhang & LehmannThus, know Proposition 1 need take transitive closure Un(S)get belief state:Definition 12 S, AGRUn(S) relation Un(S)+ .Proposition 12 S, AGRUn(S) B.Intuitively, simply inferring opinions implied conflicts introducedaggregation. show formally consider general aggregationoperator below.surprisingly, taking opinions sources seriously, may generate manyconflicts, manifested fully connected subsets W.Example 2 Suppose three sources space robot scenario Example 1 considered equally credible, aggregate belief state fully connected relationindicating conflicts every belief.4.3 Aggregating Strictly Ranked SourcesNext, consider case sources strictly ranked, i.e., wS total order.define lexicographic operator lower ranked sources refine belief stateshigher ranked sources. is, determining ordering pair worlds, opinionshigher ranked sources generally override lower ranked sources, lower rankedsources consulted higher ranked sources agnostic:Definition 13 S, AGRRf(S) relationn0(x, y) : S. x <s s0 S. s0 x .AGRUn(S), AGRRf(S) guaranteed transitive, always modular:Proposition 13 S, AGRRf(S) modular necessarily transitive.However, case wS total order, result applying AGRRf guaranteedbelief state.Proposition 14 wS total order, AGRRf(S) B.Example 3 Suppose, space robot scenario Example 1, technician considered credible manager who, turn, considered credibleprogrammer. aggregate belief state, shown Figure 3, informs robot (correctly)feedback system crashed, shouldnt worry overload problemkeep sending data.4.4 General Aggregationgeneral case, may several ranks represented multiple sources rank.instructive first consider following seemingly natural strawman operator,AGR : First combine equally ranked sources using AGRUn, aggregate strictlyranked results using essentially AGRRf.166fiRepresenting Aggregating Conflicting BeliefsFDFDFDFDFigure 3: belief state aggregation Example 3 st sm sp .Definition 14 Let S. r R, let <r = AGRUn({s : rank(s) = r})r , corresponding agnosticism relation. AGR (S) relation(x, y) : r R. x <r r0 ranks(S). r0 > r x r0 .AGR indeed defines legitimate belief state:Proposition 15 S, AGR (S) B.Unfortunately, problem divide-and-conquer approach assumesresult aggregation independent potential interactions individual sourcesdifferent ranks. Consequently, opinions eventually get overridden may stillindirect effect final aggregation result introducing superfluous opinionsintermediate equal-rank aggregation step, following example shows:Example 4 Let W = {a, b, c}. Suppose = {s0 , s1 , s2 } belief states<s0 = {(b, a), (b, c)} <s1 =<s2 = {(a, b), (c, b)}, s2 s1 s0 . AGR (S){(a, b), (c, b), (a, c), (c, a), (a, a), (b, b), (c, c)}. sources agnostic c, yet(a, c) (c, a) result transitive closure lower rank involvingopinions ((b, c) (b, a)) actually get overridden final result.undesired effects, propose another aggregation operator circumvents problem applying refinement (as defined Definition 13) setsource belief states inferring new opinions via closure:Definition 15 rank-based aggregation set sources S, denoted AGR(S),AGRRf(S)+ .Encouragingly, AGR outputs valid belief state:Proposition 16 S, AGR(S) B.output running space robot example also reasonable:Example 5 Suppose, space robot scenario Example 1, technician still considered credible manager programmer, latter two considered167fiMaynard-Zhang & LehmannFFDFDFigure 4: belief state aggregation Example 5 st sm sp .equally credible. aggregate belief state, shown Figure 4, still gives robot correct information state system. robot also learns future referencedisagreement whether would data overloadfeedback system working.Furthermore, observe AGR, applied set sources Example 4,indeed bypass problem described extraneous opinion introduction:Example 6 Assume W, S, w Example 4; AGR(S) = {(a, b), (c, b)} desired. concerned reader may note s2 dictator sense s2 opinionsoverride opposing opinions. However, reasonable contextsources strictly lower rank.observe AGR behaves well special cases weve considered, reducingAGRUn sources equal rank, AGRRf sources totallyranked:Proposition 17 Suppose S.1. wS fully connected, AGR(S) = AGRUn(S).2. wS total order, AGR(S) = AGRRf(S).Another property AGR transitive closure part minimally extends resultAGRRf make complete (i.e., conflicts represented explicitly) sense newopinions added worlds already involved conflict:Proposition 18 Suppose S, = AGRRf(S), = AGR(S), x 6 x, W.x y, x ./ y.One small observation: AGR() = property definition, reflecting factgenerate opinions nothing.4.5 Arrow, RevisitedFinally, strong argument favor AGR satisfies Arrows conditions. Technically, setting slightly different Arrows, need modifycondition appropriate setting, yet retains intended spiritoriginal condition. Let f operator aggregates belief states <s1 , . . . , <snW n sources s1 , . . . , sn S, respectively, let = f (<s1 , . . . , <sn ), let wStotal preorder S. consider condition separately.168fiRepresenting Aggregating Conflicting BeliefsRestricted range setting, output aggregation function modular, transitive belief state rather total preorder considered Arrow.Definition 16 (modified) Restricted Range: range f B.Unrestricted domain Similarly, input aggregation function modular,transitive belief states sources rather total preorders.Definition 17 (modified) Unrestricted Domain: i, <si member B.Pareto principle Arrows setting, relations represented non-strict relative likelihood (preference, actually) asymmetric restrictions relations useddefine Pareto principle. However, setting, generalized belief states alreadyrepresent strict likelihood. Consequently, use actual input output relationsaggregation function place asymmetric restrictions define Paretoprinciple. Obviously, AGRs ability introduce conflicts, satisfyoriginal formal Pareto principle would essentially require sourcesunconflicted belief one world strictly likely another, must alsotrue aggregate belief state. Neither condition necessarily stronger other.Definition 18 (modified) Pareto Principle: x <si i, x y.Independence irrelevant alternatives Conflicts defined terms cycles,necessarily binary. allowing existence conflicts, effectively made possibleoutside worlds affect relation pair worlds, viz., involvingcycle. result, need weaken IIA say relation worldsindependent worlds unless worlds put conflict. makesintuitive sense: two worlds put conflict aggregation due cycle involvingworlds, may need access worlds able detect conflict.Definition 19 (modified) Independence Irrelevant Alternatives (IIA): Suppose00s01 , . . . , s0n si s0i i, 0 = f (<s1 , . . . , <sn ). suppose x <si0iff x <si i, x6 ./ y, x6 ./0 y. x iff x 0 y.Non-dictatorship Pareto principle definition, use actual inputoutput relations define non-dictatorship since belief states represent strict likelihood.perspective, setting requires informant sources highest rankdictators sense considered Arrow. However, setting originally consideredArrow one individuals ranked equally. Thus, make explicitnew definition non-dictatorship adding pre-condition sourcesequal rank. Now, AGR treats set equally ranked sources equally takingopinions seriously, price introducing conflicts. So, intuitively, dictators.However, Arrow account conflicts formulation, sourcesdictators definition. need modify definition non-dictatorshipsay source always push opinions without ever contested.Definition 20 (modified) Non-Dictatorship: si sj i, j,that, every combination source belief states every x, W, x <si 6<si ximplies x 6 x.169fiMaynard-Zhang & Lehmannshow AGR indeed satisfies conditions:Proposition 19 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR(S). AGRfsatisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,IIA, non-dictatorship.5. Multi-Agent Fusionfar, considered case single agent must construct updatebelief state informed set sources. Multi-agent fusion processaggregating belief states set agents, respective set informantsources. proceed formalize setting.5.1 Formalizationagent informed set sources S.8 Agent induced belief statebelief state formed aggregating belief states informant sources, i.e., AGR(S).use denote special agents informed S, respectively.Assume set agents fuse agree upon rank (and, consequently, w).9 definefusion set agent informed combination informant sources:Definition 21 Let = {A1 , . . . , } set agents agentSn Ai informedSi S. fusion A, written (A), agent informed = i=1 Si .surprisingly given set-theoretic definition, fusion idempotent, commutative,associative. properties guarantee invariance required multi-agent belief aggregation applications space robot domain.5.2 Computing Fusion Efficientlymulti-agent space robot scenario described Section 1, direct needbelief states result fusion. interested belief statesoriginal sources far want fused belief state reflect informant history.obvious question whether possible compute belief state inducedagents fusion solely initial belief states, is, without referencebelief states informant sources. highly desirable expensestoringor, case space robot example, transmittingall source beliefstates; would like represent agents knowledge compactly possible.fact, sources equal rank. simply take transitiveclosure union agents belief states:Ai , agent induced belief state,Proposition 20 Let Definition+21,inducedbelief state.wS , fully connected. = (A),Ai8. source thought primitive agent fixed belief state.9. could easily extend framework allow individual rankings, felt small gaingenerality would justify additional complexity loss perspicuity. Similarly, couldconsider agent credibility ordering informant sources. However,unclear how, example, crediblity orderings disjoint sets sources combinednew credibility ordering since union total.170fiRepresenting Aggregating Conflicting BeliefsUnfortunately, equal rank case special. sources different ranks,generally cannot compute induced belief state fusion using agent beliefstates fusion, following simple example demonstrates:Example 7 Let W = {a, b}. Suppose two agents A1 A2 informed sources s1belief state <s1 = {(a, b)} s2 belief state <s2 = {(b, a)}, respectively. A1 belief states1 A2 s2 s. s1 s2 , belief state induced(A1 , A2 ) <s1 , whereas s2 s1 , <s2 .Thus, knowing belief states fused agents sufficient computinginduced belief state. need maintain information agents informants.question whether better storing original sources.might wonder whether possible somehow compute credibility rankagent based credibility informant sources, simply apply AGRagents induced belief states. works fine if, every pair agents, informantsone credible other. However, work generalagent informants less credible another agentfollowing example demonstrates:Example 8 Let W = {a, b, c}. Suppose agent A1 informed source s1 belief state<s1 = {(a, b), (b, c), (a, c)}, suppose agent A2 informed sources s0 s2 beliefstates <s0 = {(c, b), (b, a), (c, a)} <s2 = {(b, a), (c, a)}, respectively. supposes2 s1 s0 . A1 induced belief state <s1 A2 <s0 . belief state induced(A1 , A2 ) {(b, c), (c, a), (b, a)}. otherhand, rank A1 A2 applyAGR induced belief states, get <s1 ; rank A2 A1 , get <s0 ; and,rank equally, get fully connected belief state. obviouslyincorrect.Hence, need store information source opinion. However,still better keeping sources around sources totally preorderedcredibility. enough store opinion AGRRf(S) rank highestranked source supporting it. define pedigreed belief states enrich belief statesadditional information:Definition 22 Let agent informed set sources S. pedigreed beliefstate pair (, l) = AGRRf(S) l : R l((x, y)) = max({rank(s) :x <s y, S}). user denote restriction pedigreed belief state r,is, r = {(x, y) : l((x, y)) = r}.verify pairs label is, fact, rank source used determine pairsmembership AGRRf(S), higher ranked source:Proposition 21 Let agent informed set sources pedigreedbelief state (, l).r relationn0(x, y) : S. x <s r = rank(s) s0 S. s0 x .belief state induced pedigreed belief state (, l) is, obviously, transitive closure.171fiMaynard-Zhang & LehmannNow, given pedigreed belief states set agents, computenew pedigreed belief state fusion. simply combine labeled opinions usingrefinement techniques. call operation pedigreed fusion:Definition 23 Let Definition 21, wS , total preorder, PA , setpedigreed belief states agents A. pedigreed fusion PA , written ped (PA ),(, l)1. relationnAj00(x, y) : Ai A, r R. xA,rR.r>rxjrr0W,2. l : R l((x, y)) = max({r : xr y, Ai A}).Proposition 22 Let A, PA , S, wS Definition 23. ped (PA )pedigreed belief state (A).perspective induced belief states, essentially discarding unlabeledopinions (i.e., derived closure operation) fusion. Intuitively,learning new information may need retract inferred opinions.fusion, re-apply closure complete new belief state. Interestingly, specialcase sources strictly-ranked, closure unnecessary:Proposition 23 A, PA , Definition 23, wS total order,ped (PA ) = (, l), + =.Let us return space robot scenario considered Example 1 illustratepedigreed fusion.Example 9 Suppose arrogant programmer part telemetry team, instead works company side country. robot requestinformation two separate agents, one query manager technician onequery programmer. Assume agents robot rank sources same,assigning technician rank 2 two agents rank 1, inducescredibility ordering used Example 5. agents pedigreed belief states resultfusion shown Figure 5.first agent provide information overloading second agentprovides incorrect information. However, see fusing two, robotbelief state identical computed Example 5 one agentinformed three sources (weve separated top set worlds showlabeling). Consequently, knows correct state system. And, satisfyingly,final result depend order robot receives agents reports.savings obtained required storage space scheme substantial. Suppose set agents informant sources, n = kWk, = kSk. Explicitly storing(along rank source) requires O(n2 m) amount space; worst casebound reached sources belief states fully connected relations.172fiRepresenting Aggregating Conflicting BeliefsFD1FDFD1FD21211FD1FDFD12 212FD211FFDFDA1A2(A1 , A2)Figure 5: pedigreed belief states agent A1 informed sm st agent A2informed sp , result fusion Example 9.hand, storing pedigreed belief state requires O(n2 ) space.10 Moreover,enriched representation allow us conserve space, also provides potentialsavings efficiency computing fusion since, pair worlds, needconsider opinions agents rather sources combined setinformants.Incidentally, used strawman AGR basis general aggregation,simply storing rank maximum supporting sources would give us sufficientinformation compute induced belief state fusion. demonstrate this, giveexample two pairs sources induce annotated agent belief states, yetyield different belief states fusion:Example 10 Let W, S, w Example 4. Suppose agents A1 , A2 , A01 , A02informed sets sources S1 , S2 , S10 , S20 , respectively, S1 = S2 = {s2 },S10 = {s0 , s2 }, S20 = {s1 , s2 }. AGR dictates pedigreed belief states fouragents equal <s2 opinions annotated rank(s2 ). spite indistinguishability, = ({A1 , A2 }) A0 = ({A01 , A02 }), induced belief state equals <s2 ,i.e., {(a, b), (c, b)}, whereas A0 {(a, b), (c, b), (a, c), (c, a), (a, a), (b, b), (c, c)}.Also notice Maynard-Reid II Shoham (2001) consider essentially specialcase fusing two agents informed strictly-ranked sources. show surprisingresult standard AGM belief revision modeled fusion two agents,informant informee, informants sources strictly credibleinformees. Furthermore, show that, clean set-theoretic semantics,fusion provides attractive, semantically well-behaved solution difficult problemiterated belief revision. general fusion definition satisfies examples iteratedfusion describe.10. bounds assume amount space needed store rank bounded smallconstant.173fiMaynard-Zhang & Lehmann6. Incorporating Votingpotential drawback framework described accountstrength support. example, cannot differentiate situationone thousand sources highest rank support < b one source ranksupports b < a, situation one source supports < b thousandsources support b < a. cases framework yields simple conflictb rather acknowledging overwhelming support one way other.additional information strength support often sufficient resolve wouldotherwise appeared conflict.address problem, generalize framework incorporate voting. firstdescribe family aggregation operators based voting AGR special case.process, introduce novel modular closure operator. discuss propertiesspecial members family including indiscriminate aggregation, simple majority,unanimity, well attractive properties family whole. describeextension setting accommodate ranked individuals individuals higherrank given precedence aggregation. Finally, consider fusion.6.1 Voting Functionsuse pairwise voting strategy similar well-known Condorcets method. (ForCondorcet method methods results standard votingtheory cite, see Blacks (1958) classical reference voting theory). Condorcets methodconsiders pair worlds separately, ranking world x world aggregatevotes ranking x. one worldbeats ties worlds, known Condorcet winner. deviatemethod use fixed threshold proportion support decide acceptanceopinion aggregate rather size support relativeopposite opinion. Let countS (x, y) = k{s : x <s y}k S.Definition 24 Let S. p [0, 1], voting function p, written vtp , mapsrelation{(x, y) : countS (x, y) > 0 countS (x, y)/kSk p}.definition falls class voting systems Black (1958) calls absolute majoritysystems. motivated observation relative support many times less relevantstrength support. support two possible rankings two worlds maylow neither justifiably considered part aggregate belief state. Similarly,support alternative rankings may high may reasonableintroduce create conflict rather choose one slightly higher support.strategy appropriate applications, course, many instancesappropriate. Also, method satisfies generalization Condorcetcriterion, widely accepted criteria good voting systems requires Condorcetwinner, exists, likely world aggregate belief state. method neverproduces strict ranking two worlds opposite Condorcets method, althoughcases Condorcets method ranks one world strictly likelyanother method produces agnosticism conflict. result, Condorcet174fiRepresenting Aggregating Conflicting Beliefswinner always among likely worlds. rate, aggregation resultsdepend significantly choice voting strategy; one could easily replacedifferent strategy desired.said, make observations voting functions. First, votingfunction definition requires accept opinion least p proportion sourcessupport it. However, often want specify opinions accepted strictlycutoff proportion sources support it. example, best-knownvoting function majority function accept opinion gets50% vote. easily specify majority vote function vt0.5+ (S)0 < < 0.5/kSk (e.g., = 0.25/kSk) tied opinions rejected. general,accept opinions garnering p proportion vote (for 0 p < 1),suffices use function vtp+ (S)0<<1 pkSk + bpkSkckSke.g., = (1 pkSk + bpkSkc)/(2kSk).11Second, immediately obvious aggregate relation may contain conflictsp 0.5, even original source belief states conflict-free. fact, possibleget conflicts aggregate conflict-free belief states even larger p, followingfamous example demonstrates:Example 11 Let W = {a, b, c} 1/3 sources belief state{(a, b), (b, c), (a, c)}, 1/3 {(b, c), (c, a), (b, a)}, 1/3 {(c, a), (a, b), (c, b)}.vtp (S) = {(a, b), (b, c), (c, a)}, cycle, 1/3 < p 2/3. known Condorcetparadox (cf. Brams Fishburns (2002) voting survey).Many solutions proposed resolving conflicts using Borda countsinstant runoff voting (aka single transferable vote) (cf. Center Voting Democracy,2002) two popular examples. before, attempt resolve conflicts but,instead, make explicit way supports flexibility choice resolutionmethodology allows semantically well-behaved iteration aggregation.Third, end-point members family voting functions special significance.voting function 0 equivalent union operator saw earlier takesopinions seriously, i.e., indiscriminate:Proposition 24 S, vt0 (S) = Un(S).extreme, voting function 1. case, equivalenttaking intersection sources belief states, i.e., accepting unanimous opinions:Proposition 25 S, vt1 (S) = sS <s .contrast vt0 generates many conflicts, vt1 generates lot agnosticism.Fourth, voting functions opinion-centered; is, proportion agnosticsources larger p, voting function p necessarily reflect agnosticismwould case opinion. If, example, belief states three sources11. bxc denotes floor x, i.e., largest integer less equal x.175fiMaynard-Zhang & LehmannW = {a, b} {(a, b)}, {(b, a)}, {}, respectively, voting function p = 1/3produce conflict respect b, agnosticism. However, sayabstainers impact final result. fact abstainers countedamong total number voters effect agnosticism respect pairworlds counts vote possible opinions. issue usually arisestandard voting schemes usually assume sources totally rank candidates.However, important observation members family voting operators produce belief states general. weve already shown, vt0 producesmodular relation necessarily transitive. end spectrum, vt1produces transitive relation necessarily modular:Proposition 26 Suppose S. vt1 (S) transitive necessarily modular.members family, result may neither modular transitive,Condorcet paradox Example 11 illustrates 1/3 < p 2/3. fact,construct scenario every 0 < p < 1:Proposition 27 kWk 3, every p (0, 1), exists vtp (S)neither modular transitive.Part problem voting may introduce conflicts may imply conflicts.before, need take transitive closure infer implied conflicts.Condorcet paradox example, produces fully connected belief state wouldhope. Unfortunately, closing transitivity necessarily restore modularitywell, following example demonstrates:Example 12 Let W = {a, b, c} 1/3 sources belief state{(a, b), (b, c), (a, c)}, 1/3 {(b, c), (c, a), (b, a)}, 1/3 {(b, a), (b, c)}. Then,p > 2/3, vtp (S) = vtp (S)+ = {(b, c)} modular.solve problem defining natural modular closure operation convertstransitive relation belief state. define modular-transitive closureoperation take result arbitrary voting function transformbelief state using transitive closure followed modular closure.6.2 Modular-Transitive Closurestart defining helper function returns level world relation, i.e.,length longest path (along strict edges) world member choiceset W. convenience, throughout modular-transitive closure subsectionuse denote arbitrary relation, ./ denote asymmetric symmetricrestrictions, respectively.Definition 25 level x W transitive relation W, written lev (x),(0x ch(W, )lev (x) =1 + max ({lev (y) : x}) otherwise.yW(Recall ch choice set function defined Definition 2.) following simpleproperties relating lev immediate:176fiRepresenting Aggregating Conflicting BeliefsProposition 28 Suppose transitive relation W x, W.1. x y, lev (x) < lev (y).2. x ./ y, lev (x) = lev (y).3. lev (x) < lev (y), z. lev (z) = lev (x) z y.4. lev (x) = lev (y), x iff x.define modular closure relation relation results fullyconnecting equi-level alternatives unless fully disconnected:Definition 26 modular closure MC() transitive relation W relation(x, y) MC() iff1. lev (x) < lev (y)2. lev (x) = lev (y) x0 , 0 . lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 .Intuitively, long reason doubt pair level interchangeable,doubt pairs level, then. Note definition MC similarone equivalent constructions rational closure Lehmann Magidor (1992)describe.see MC indeed makes transitive relation modular preserving transitivity:Proposition 29 transitive relation W, MC() B.MC additive process changes relation minimally achieve modularitypreserving transitivity levels worlds:Proposition 30 Suppose transitive relation W = MC().1. .2. modular, =.3. lev (x) = lev (x) x W.4. 0 B 0 lev0 (x) = lev (x) x W, 0 .define modular, transitive closure arbitrary relation MC appliedtransitive closure , show result belief state:Definition 27 modular, transitive closure MT() relation W relation MC (+ ).Proposition 31 relation W, MT() B.MT also minimally additive operator:Proposition 32 Suppose relation W = MT().1. .2. transitive, = MC().3. modular, =+ .4. modular transitive, =.5. conflicts, neither .177fiMaynard-Zhang & Lehmann6.3 Aggregation Familyfully equipped solve problem incorporating voting aggregation.First, consider special case sources rank. aggregationoperators construct aggregate belief state first applying voting, closingMT:Definition 28 p [0, 1], AGREqp (S) = MT (vtp (S)).Proposition 33 p [0, 1], AGREqp (S) B.easily generalize definition accommodate ranking sources.accept opinion enough individuals highest rank opinion support it,close MT:Definition 29 p [0, 1], AGRRf p (S) relationn0(x, y) : S. x <s (x, y) vtp ({s0 : s0 s}) s0 S. s0 x .Definition 30 p [0, 1], AGRp (S) = MT(AGRRf p (S)).Proposition 34 p [0, 1], AGRp (S) B.aggregation functions encountered far special cases generalfamily:Proposition 35 Suppose p [0, 1].1. wS fully connected, AGRp (S) = AGREqp (S).2. wS total order, AGRp (S) = AGRRf p (S) = AGRRf(S) = AGR(S).3. AGR0 (S) = AGR(S).obvious consequence last property, AGR0 satisfies modified Arrovian conditions.Corollary 35.1 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR0 (S). AGRfsatisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,IIA, non-dictatorship.6.4 FusionFusion still defined Definition 21, i.e., belief state created fusion setagents aggregate belief state agents cumulative informant sets. However,use AGRp rather AGR compute aggregate belief state.again, want compute fusion without storing belief statesinformant sources, possible. Unfortunately, possible general aggregationfunctions based voting. reason need keep track actual identitysources supporting opinion avoid double-counting sources sharedmultiple agents.178fiRepresenting Aggregating Conflicting BeliefsHowever, often better O(n2 m) space required store full sources,n = kWk number informant sources. store partsmatter. Specifically, given source, store opinions sourceone highest ranked supporting opinion corresponding worlds.effectively accomplish extending pedigreed belief state labelopinion rank highest ranking sources supporting opinioncorresponding worlds, also set unique identifiers sources supportingparticular opinion. also maintain table stores, rank representedset informant sources, set identifiers sources rank. callresulting representation support pedigreed belief state.Definition 31 Let agent informed set sources S. support pedigreedbelief state triple (l, sup, rtab)l : W W R {} l(x, y) = max({rank(s) : x 6s y, S} {})6 R < r r R,sup : W W 2S sup(x, y) = {s : rank(s) = l(x, y), x <s y},rtab : ranks(S) 2S rtab(r) = {s : rank(s) = r}.Note l symmetric: l(x, y) = l(y, x). hand, sup not. Now,easily compute agents belief state support pedigreed belief state. computeproportion support particular opinion, simply divide size supportset opinion number informant sources labeled rank.Proposition 36 Let agent informed set sources S, support pedigreed belief state (l, sup, rtab), using aggregation function AGRp p [0, 1]. beliefstate relationMT ({(x, y) : ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p})Observe that, unlike pedigreed belief states, support pedigreed belief states labelpossible opinions, appearing agents induced belief state, i.e., whosesupport falls threshhold. reason another agent may come along laterenough new votes cross threshold, case votes earlier sourcesbecome relevant. Similarly, support pedigreed belief states maintain rank information evenranks appearing labels. source particular rank may currently supportopinion, another agent may later bring sources rank supporting opinionhitherto unsupported source equal higher rank. correct computationproportion support opinion must take account earlier sources.address fusion, let us consider space required store support pedigreedbelief state (l, sup, rtab). l requires O(n2 ) space, rtab requires (m) space, and, rmaxdenotes number sources rank sources rank, suprequires O(n2 rmax ) space, total O(n2 rmax + m) space.12 Thus, best-casescenario where, example, sources strictly ranked, support pedigreed belief staterequires O(n2 + m) space since opinion one supporter. However,12. before, assume representing ranks requires constant space. assume representsource label constant space well.179fiMaynard-Zhang & Lehmannworst-case scenario where, example, sources equally ranked rmax = m,still need O(n2 m) space.Now, computing support pedigreed belief state resulting fusion straightforward. opinion, set l highest l value opinion among agentsset sup union sup sets opinion agents l value.rank represented agents rank table, set rtab unionrtab sets agents defined.Definition 32 Let Definition 21, wS , total preorder, PA , setsupport pedigreed belief states agents A. support pedigreed fusion PA ,written sup (PA ), (l, sup, rtab)1. l : R l((x, y)) = max({l0 (x, y) : (l0 , sup0 , rtab0 ) PA }),2. sup : W W 2S[sup(x, y) =sup0 (x, y),(l0 ,sup0 ,rtab0 )PA , l0 (x,y)=l(x,y)3. rtab : ranks(S) 2S[rtab(r) =rtab0 (r).(l0 ,sup0 ,rtab0 )PA , rrange(rtab0 )Proposition 37 Let A, PA , S, wS Definition 32. sup (PA )support pedigreed belief state (A).Thus, addition potential savings space gained using support pedigreedbelief states, also potentially save time needed compute fusion since, givenopinion, need consider opinions lesser ranked sources.7. Related WorkMuch work belief aggregation geared towards unbiased kinds beliefpooling. Besides work social choice described Section 3.2, recent attemptsbelief revision community (e.g. Borgida & Imielinski, 1984; Baral, Kraus, Minker, &Subrahmanian, 1992; Liberatore & Schaerf, 1995; Makinson, 1997; Revesz, 1997; Konieczny& Perez, 1998; Meyer, 2001; Benferhat, Dubois, Kaci, & Prade, 2002) sought modifyAGM theory capture fair revisions, is, revisions revisee revisersbeliefs treated equally seriously. Like proposal, Benferhat et al. Meyer accommodate iterative merging. Benferhat et al.s proposal also distinct approachproblem possibilistic logic point view. Besides restriction equally-rankedsources, fairness-based proposals differ generally syntacticnature sense sentences prioritized rather possible worlds. Meyersproposal exception; belief states epistemic states, structures styleSpohns (1988) ordinal conditional functions (aka -rankings). fact, Meyer, Ghose,180fiRepresenting Aggregating Conflicting BeliefsChopra (2001) shown number simple aggregation operators epistemicbelief states also satisfy Arrows postulates appropriately modified context(unrestricted domain, restricted range, IIA particular need modification). Unfortunately, epistemic states enriched total preorders and, thus, suffer problemsdescribed earlier, i.e., inability explicitly handle conflicts.Cantwells (1998) work also syntactic nature, allow sources differing credibility. Cantwell addresses complementary problem own: decidinginformation reject given subset informing sources rejecting information.assumes generalization credibility ordering, partial preorder sets sources.explores ways inducing partial preorder sentences based ordering,uses ordering determine subset (although all) sentences reject. Another difference work considers non-counterfactual beliefssources.not, course, first consider using lexicographic ordering aggregation purposes. Lexicographic operators long studied fields managementsocial science; Fishburn (1974) gives good survey much work. recently, researchers artificial intelligence taken interest operators; examples include Grosof (1991), Maynard-Reid II Shoham (2001) Andreka, Ryan,Schobbens (2002).Grosof uses lexicographic aggregation preorders means tackling problemdefault reasoning presence conflicting defaults. Besides general preordersaggregated, another interesting difference work although Grosofallow sources equal rank, allow sources incomparable rank, i.e.,ranking sources strict partial order. Thus, extreme case orderingcompletely disconnected, operator reduces Un operator (and, thus,necessarily preserve transitivity).Andreka et al., hand, frame work context preference aggregation. go one step Grosof allow input relations arbitrary.prove lexicographic operator one satisfies variationArrows properties unanimity, IIA, preservation transitivity, weaker versionnon-dictatorship. (We point perspective Arrows original framework, relation highest priority always dictator.) describe collectionproperties besides transitivity preserved operator. However, work,preserve totality.work derives much inspiration Maynard-Reid II Shohams work.restrict attention total preorders, create problemsassume sources totally ordered. focus, instead, strong connectionbelief aggregation iterated belief revision. show usediterated belief operator AGM-based setting, compare propertiesrepresentative sampling well-known iterated belief operator proposalsBoutiliers (1996) natural revision, Darwiche Pearls (1997) operators, Spohns (1988)ordinal conditional function revision, Lehmanns (1995) widening rank model revision,Williamss (1994) conditionalization adjustment operators. showoperator among semantically well-behaved: resultsoperators depend order iteration.181fiMaynard-Zhang & LehmannFinally, knowledge, none related approaches outside social choiceyet extended incorporate voting.8. Conclusiondescribed semantically clean representation class modular, transitiverelations collective qualitative beliefs allows us represent conflicting opinionswithout sacrificing ability make decisions. proposed intuitive operatortakes advantage representation agent combine belief statesset informant sources totally preordered credibility. showed operatorcircumvents Arrows Impossibility result satisfactory manner. also describedmechanism fusing belief states different agents iterates well extendedframework incorporate voting.assumed agents share credibility ranking sources. general,rankings vary among agents, even change time. Furthermore, agentsranking function depend context; different sources may different areasexpertise. Exploring behavior fusion general settings obvious nextstep.Note although described operators incorporate voting, condition ever side lower rank sources conflict higher ranksources, matter many disagreeing lower rank sources are. aggregation scheme behaves differently would built fundamentally differentassumptions framework.Another problem deserves study developing fuller understandingproperties Bel, Agn, Con operators interrelate.Acknowledgmentswant thank Yoav Shoham anonymous referees paper insightfulcomprehensive feedback. preliminary version paper appears ProceedingsSeventh International Conference Principles Knowledge RepresentationReasoning (KR2000) (the first authors last name Maynard-Reid II time).work partially supported National Physical Science Consortium FellowshipJean et Helene Alfassa Fund Research Artificial Intelligence.Appendix A. ProofsProposition 11. transitive closure modular relation modular.2. Every transitive relation quasi-transitive.3. (Sen, 1986) Every quasi-transitive relation acyclic.Proof:1. Suppose relation finite set modular, + transitive closure . Suppose x, y, z x + y. exist w0 , . . . , wn182fiRepresenting Aggregating Conflicting Beliefsx = w0 wn = y. Since modular w0 w1 , either w0 z z w1 .former case, x = w0 z, x + z. latter case, z w1 wn = y,z + y.2. Suppose finite set, x, y, z , transitive relation , <asymmetric restriction. Suppose x < < z. x y, 6 x, z,z 6 y. x z imply x z, z 6 x imply z 6 x,transitivity. x < z.2Proposition 2 (Sen, 1986) Given relation finite set , choice set operationch defines choice function iff acyclic.Proof:See Sens (1986) proof. 2Proposition 3 (Arrow, 1963) aggregation operator satisfies restrictedrange, unrestricted domain, (weak) Pareto principle, independendence irrelevant alternatives, nondictatorship.Proof:See Arrows (1963) proof. 2Proposition 4 Let relation finite set let symmetric restriction.total quasi-transitive transitive, transitive.Proof: Let total, quasi-transitive, non-transitive relation. Suppose xz x 6 z. totality, z x, z x. x y, z quasi-transitivity,contradiction. Thus, x y. Similarly, z, x, contradiction, z.z x, x 6 z. Therefore, transitive. 2Proposition 5 Suppose relation transitive corresponding agnosticismrelation. transitive iff modular.Proof: Suppose transitive suppose x z, x, y, z W. prove contradiction: Suppose x 6 6 z. transitivity, z 6 6 x, x z.assumption, x z, x 6 z, contradiction.Suppose, instead, modular suppose x z, x, y, z W. x 6 y,6 x, 6 z, z 6 y. modularity, x 6 z z 6 x, x z. 2Proposition 6 T< B set irreflexive relations B.Proof: Let x, y, z W. first show T< B. Let T< . existsasymmetric restriction . definition, transitive,Proposition 1, . Suppose x y. x 6 x. Since total,x z z x. Suppose x z. z, z 6 x (otherwise x transitivity,contradiction), x z. if, hand, 6 z, z totality,z y. Suppose, instead, z x. z transitivity 6 z (otherwise xtransitivity, contradiction), z y. Thus, x z z y, modular.show B T< irreflexive. T< , asymmetric,irreflexive. Suppose, instead, irreflexive. define relationship , show183fiMaynard-Zhang & Lehmannasymmetric restriction, show . Let defined x iff 6 x.first show asymmetric restriction . Suppose 0 asymmetricrestriction . x 0 y, x 6 x, x y. If, instead, x y, 6 x.totality, x y, x 0 y. next show . x 6 x. Otherwise,x y. since irreflexive, 6 x (otherwise x x transitivity), xtotal. Next, suppose x z. 6 x z 6 y. modularity, z 6 x,x z, and, thus transitive. 2Proposition 7 B iff partition W = hW0 , . . . , Wn W that:1. every x Wi Wj , 6= j implies < j iff x y.2. Every Wi either fully connected (w w0 w, w0 Wi ) fully disconnected(w 6 w0 w, w0 Wi ).Proof: refer conditions proposition conditions 1 2, respectively.prove direction proposition separately.(=) Suppose B, is, modular transitive relation W. useseries definitions lemmas show partition W exists satisfying conditions 12. first define equivalence relation partition W. Two elementsequivalent look perspective every element W:Definition 33 x iff every z W, x z iff z z x iff z y.Lemma 7.1 equivalence relation W.Proof: Suppose x W. every z W, x z iff x z z x iff z x, x x.Therefore, reflexive.Suppose x, W x y. every z W, x z iff z z x iff z y.every z W, z iff x z z iff z x. Therefore, x,symmetric.Suppose x, y, z W, x y, z. Suppose w W. definition, x w iff w w x iff w y, w iff z w w iff w z. Therefore,x w iff z w w x iff w z. Since w arbitrary, x z, transitive. 2partitions W equivalence classes. use [w] denote equivalence classcontaining w, is, set {w0 W : w w0 }. Observe two worlds conflictalways appear equivalence class:Lemma 7.2 x, W x ./ y, [x] = [y].Proof: Suppose x, W x ./ y. Since [x] equivalence class, suffices show[x], is, x y. Suppose z W. transitivity, x z, z; z,x z; z x, z y; and, z z x. Thus, x z iff z z xiff z y, since z arbitrary, x y. 2define total order equivalence classes:Definition 34 x, W, [x] [y] iff [x] = [y] x y.184fiRepresenting Aggregating Conflicting BeliefsLemma 7.3 well-defined, is, x x0 0 , x iff x0 0 ,x, x0 , y, 0 W.Proof: Suppose x x0 0 , x, x0 , y, 0 W. definition , everyz W, x z iff x0 z. particular, x iff x0 y. Also definition ,every z 0 W, z 0 iff z 0 0 . particular, x0 iff x0 0 . Therefore, x iff x0 0 .2Lemma 7.4 total order equivalence classes W defined .Proof: Suppose x, y, z W. first show total. definition , xx, [x] [y] [y] [x], respectively. Suppose x 6 6 x, supposez W. modularity , x z implies z, z implies x z, z x implies z y,z implies z x, x y. Therefore, [x] = [y], [x] [y] definition .Next, show anti-symmetric. Suppose [x] [y] [y] [x]. [x] = [y]x x. former case done, latter, result followsLemma 7.2.Finally, show transitive. Suppose [x] [y] [y] [z]. Obviously,[x] = [y] [y] = [z], [x] [z]. Suppose not. x z, x ztransitivity . Therefore, [x] [y] definition . 2name members partition W0 , . . . , Wn Wi Wj iff j,n integer. naming exists since every finite, totally ordered set isomorphicfinite prefix integers.check partition satisfies two conditions. first condition,suppose x Wi , Wj , 6= j. want show < j iff x y. Since 6= j,[x] 6= [y]. Suppose < j. j, [x] [y]. Since [x] 6= [y], x definition. suppose, instead, x y. [x] [y] definition , j. Since[x] 6= [y], 6 x Lemma 7.2. Since [x] 6= [y] 6 x, [y] 6 [x] definition ,j 6 i. Thus, < j.Finally, show Wi either fully connected fully disconnected. Supposex, y, z Wi x z. suffices show x x iff z. definition, x x iff x, x x iff x z. Suppose x x. Then, x x z, ztransitivity . Suppose now, x 6 x. Then, 6 x x 6 z, 6 z modularity.(=) Suppose W = hW0 , . . . , Wn partition W relation W satisfying given conditions. want show modular transitive. firstgive following lemma:Lemma 7.5 Suppose W partition W relation W satisfying condition 1. Wi , Wj W, x Wi , Wj , x y, j.Proof:= j, done. Suppose 6= j. Then, since x y, < j condition 1. 2show modular. Suppose x Wi , Wj , x y. jLemma 7.5. Suppose z Wk . k k j modularity . Suppose < kk < j. x z z condition 1. Otherwise = k = j, x, y, z Wi . Sincex y, Wi fully connected condition 2, x z (and z y).185fiMaynard-Zhang & LehmannFinally, show transitive. Suppose x Wi , Wj , z Wk , x y,z. Lemma 7.5, j j k, k transitivity . Suppose < k.x z condition 1. Otherwise = k = j, x, y, z Wi . Since x y, Wi fullyconnected condition 2, x z. 2(END PROPOSITION 7 PROOF)Proposition 8 B set reflexive relations B.Proof: first show B. Let x, y, z W. definition, transitive. Suppose x y. Since total, x z z x. z x, z transitivity,modular. hand, empty relation W modular transitive,total and, consequently, .show B reflexive. , total,reflexive. If, instead, reflexive, x x so, modularity, x x. Thus,total. And, since B, transitive. 2Proposition 91. Q B = .2. B 6 Q.3. Q 6 B W least three elements.4. Q B W one two elements.Proof:1. Suppose Q B. total transitive and, hence, . Suppose .definition, total. Also definition, transitive, Proposition 1,quasi-transitive and, thus, Q. Proposition 8, B and, so, Q B.2. empty relation modular transitive, total and, so, Q.3. Suppose b distinct elements W. relation W W \ {(b, a)} total,and, since asymmetric restriction {(a, b)} transitive, also quasitransitive. However, least three elements W, transitive and,so, B.4. Suppose W one element. B contains possible relations W, whereasQ contains fully connected relation W.Suppose W two elements b. B contains empty relation, fullyconnected relation, remaining eight relations contain either (a, b)(b, a), both. Q, hand, contains three reflexive relationscontaining either (a, b) (b, a).2Proposition 101. Q< B = T< .186fiRepresenting Aggregating Conflicting Beliefs2. B 6 Q< .3. Q< 6 B W least three elements.4. Q< B W one two elements.Proof:1. Suppose Q< B. Since Q< , irreflexive, since B, T<Proposition 6. Suppose, instead, T< . Proposition 6, B. Letrelation asymmetric restriction. (Obviously relation mustexist.) Proposition 9, Q, Q< . Thus, Q< B.2. fully connected relation W B, asymmetric and, so, Q< .3. Suppose b distinct elements W. W least three elements,relation {(a, b)} modular and, thus, B, yet asymmetric restrictionrelation W W \ {(b, a)} total quasi-transitive (since {(a, b)}transitive).4. Suppose W one element. B contains possible relations W, whereasQ< contains empty relation W.Suppose W two elements b. B contains empty relation, fullyconnected relation, eight remaining relations contain either (a, b)(b, a), both. Q< , hand, contains three irreflexiverelations.2Proposition 11 S, Un(S) modular necessarily transitive.Proof: Let = Un(S). Suppose x, y, z W x y.x <s y. assumption, <s modular, x <s z z <s y. definition Un(S),x z z y, modular.Suppose a, b, c W = {s1 , s2 } <s1 = {(a, b), (a, c)}< 2 = {(b, a), (c, a)}. Un(S) transitive. 2Proposition 12 S, AGRUn(S) B.Proof: transitive closure relation transitive. Since Un(S) modular,transitive closure Un(S) also modular Proposition 1. 2Proposition 13 S, AGRRf(S) modular necessarily transitive.Proof: first prove modularity. Suppose x, y, z W (x, y) AGRRf(S).0exists x <s s0 S, x y. modularity <s ,either x <s z z <s y. Since finite, implies either exists s00000x <s z s00 s0 S, x z, exists s0 <s z00s00 s0 S, z. Thus, (x, z) AGRRf(S) (z, y) AGRRf(S),AGRRf(S) modular.Suppose W = {x, y, z} = {s1 , s2 } s1 = {(x, y), (z, y)}, s2 = {(y, x), (y, z)},s1 s2 . AGRRf(S) = {(x, y), (z, y), (y, x), (y, z)} transitive. 2187fiMaynard-Zhang & LehmannProposition 14 wS total order, AGRRf(S) B.Proof: Weve already proven Proposition 13 AGRRf(S) modular. Let= AGRRf(S) suppose x, y, z W. remains show transitive. Suppose x z. exists s1 x <s1 and, every s01 S,00s01 implies x 6<s1 6<s1 x, exists s2 <s2 z and,00every s02 S, s02 implies 6<s2 z z 6<s2 y. Suppose s1 s2 (the case s2 s1similar). 6<s1 z z 6<s1 y. modularity, since x <s1 z 6<s1 y, x <s1 z. Let000s0 s0 s1 . x 6<s 6<s x. And, since s1 s2 , s0 s2 , 6<s z000z 6<s y. modularity, x 6<s z z 6<s x. Therefore, x z. 2Proposition 15 S, AGR (S) B.Proof: Proposition 12, <r B every r ranks(S). convenience, assumeexistence virtual source sr corresponding <r . Precisely, r ranks(S),assume exists source sr <sr =<r rank(sr ) = r, let 0set sources. Then,AGR (S) = (x, y) : r R. x <r r0 ranks(S). r0 > r x r0n0=(x, y) : 0 . x <s s0 0 . s0 0 x= AGRRf(S 0 ).Since one source 0 per rank r, since > total order R, wS 0 totalorder. result follows Proposition 14. 2Proposition 16 S, AGR(S) B.Proof: Proposition 13, AGRRf(S) modular. AGRRf(S)+ obviously transitive,and, Proposition 1, modular well. 2Proposition 17 Suppose S.1. wS fully connected, AGR(S) = AGRUn(S).2. wS total order, AGR(S) = AGRRf(S).Proof:1. Suppose wS fully connected. second half definition AGRRfvacuouslyAGRRf(S) simplifies {(x, y) : S. x <s y}.trueexactly sS <s , i.e., Un(S), AGR(S) = AGRRf(S)+ = Un(S)+ = AGRUn(S).2. Suppose wS total order. Proposition 14, AGRRf(S) transitive, AGR(S) =AGRRf(S)+ = AGRRf(S).2Proposition 18 Suppose S, = AGRRf(S), = AGR(S), x 6 x, W.x y, x ./ y.188fiRepresenting Aggregating Conflicting BeliefsProof:first show following lemma:Lemma 18.1 Suppose = AGRRf(S). every integer n 2, x, W,x 6 y, exist x0 , . . . , xn W x = x0 xn = y, n smallestinteger true, xn x0 .Proof: Suppose x, W, x 6 y, exist x0 , . . . , xn W x = x0xn = y, n smallest integer true. Consider triplexi1 , xi , xi+1 , 1 n 1. First, xi1 6 xi+1 , otherwise would chainshorter length n x y. Now, since xi1 xi , exists s10xi1 <s1 xi and, s0 s1 S, xi1 xi . Similarly, exists s20xi <s2 xi+1 and, s0 s2 S, xi xi+1 . Thus, sources higher rankmax(s1 , s2 ) agnostic respect xi1 xi+1 .Suppose s1 s2 . xi s1 xi+1 so, modularity, xi1 <s1 xi+1 .xi1 xi+1 , contradiction. Similarly, derive contradiction s2 s1 . Thus, s1 s2 .Now, since xi1 6 xi+1 sources rank higher s1 s2 agnosticrespect xi1 xi+1 , xi1 6<s1 xi+1 . modularity, xi+1 <s1 xi . Since s1 s2 ,sources higher rank s2 agnostic respect xi xi+1 ,xi+1 xi . Similarly, xi <s2 xi1 , xi xi1 . Since chosen arbitrarily 1n 1, xn x0 . And, fact, opinions worlds originatesources rank. 2suppose x 6 y. x y, exist x0 , . . . , xn x = x0xn = n smallest positive integer true. Then, Lemma 18.1,= xn x0 = x, x x ./ y. 2Proposition 19 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR(S). AGRfsatisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,IIA, non-dictatorship.Proof: Let = AGRf (<s1 , . . . , <sn ). = AGR(S).Restricted range: AGRf satisfies restricted range Proposition 16.Unrestricted domain: AGRf satisfies unrestricted domain Definition 7.Pareto principle: Suppose x <si si . particular, x <s maximalrank source S. Since maximal, vacuously true every s0 S, x 6<s06<s x. Therefore, x y, AGRf satisfies Pareto principle.IIA: Let 0 = {s01 , . . . , s0n }. First note AGRRf satisfies IIA:Lemma 19.1 Suppose = {s1 , . . . , sn } S, 0 = {s01 , . . . , s0n } S, si s0i i,0= AGRRf(S), 0 = AGRRf(S 0 ). If, x, W, x <si iff x <si i,x iff x 0 y.0Proof: Suppose si s0i , x <si iff x <si y, i. x iff x 0 sinceDefinition 13 relies relative ranking sources relationsx belief states determine relation x aggregatedstate. 2189fiMaynard-Zhang & LehmannThus, IIA disobeyed closure step AGR introduces new opinions.(Note IIA satisfied sources equal rank since, Proposition 17,closure step introduces new opinions conditions.)0Now, suppose x, W, x <si iff x <si i, x6 ./ y, x6 ./0 y. showx implies x 0 (the direction identical). Suppose x y. Let = AGRRf(S)0 = AGRRf(S 0 ). Since x6 ./ y, x Proposition 18. x 0Lemma 19.1, x 0 y.(END IIA SUB-PROOF)Non-dictatorship: Suppose wS fully connected suppose x <si 6<si x.Let sj <sj x. x x, si dictator. 2(END PROPOSITION 19 PROOF)Ai , agent induced belief state,Proposition 20 Let Definition21,+wS , fully connected. = (A),inducedbelief state.AiProof:use following lemma:Lemma 20.1 set relations arbitrary finite set ,[++ =[++ transitive closure .+++0=, a, b . Suppose b.Proof: Let =,exist 0 , . . . , n1 w0 , . . . , wn+= w0 +0 n1 wn = bThus, exist x00 , . . . , x0m0 , . . ., x(n1)0 , . . . , x(n1)mn1= w0 = x00 0 0 x0m0 = w1 = = wn1 = x(n1)0 n1 n1 x(n1)mn1 = wnwn = b, 0 b.suppose 0 b. exist 0 , . . . , n1 w0 , . . . , wn= w0 0 n1 wn = bObviously, implies+= w0 +0 n1 wn = bimplies=+= w0 +wn = b, b. 2190fiRepresenting Aggregating Conflicting BeliefsNow, let belief state induced (A). = AGR(S). Proposition 17,= AGRUn(S),= Un(S)+ =[!+<s=sS+[Sni=1<s =[ [+<sAi sSiSilemma,=[Ai[sSi+ +<s =[+AGRUn(Si ) =Ai[+AiAi2Proposition 21 Let agent informed set sources pedigreedbelief state (, l).r relationn0(x, y) : S. x <s r = rank(s) s0 S. s0 x .Proof: Suppose xr y. x l((x, y)) = r. Definitions 13 22,00exists x <s every s0 S, x y. particular, x <ss0 S, wS s0 , rank(s) rank(s0 ). Thus,0r = l((x, y)) = max({rank(s0 ) : x <s y, s0 S}) = rank(s).suppose exists x <s y, r = rank(s), and, every s0 S,00x y. x y. Moreover, since every s0 S, x <s implies wS s0 impliesrank(s) rank(s0 ),0l((x, y)) = max({rank(s0 ) : x <s y, s0 S}) = rank(s) = r.Therefore, xr y. 2Proposition 22 Let A, PA , S, wS Definition 23. ped (PA )pedigreed belief state (A).Proof: Let ped (PA ) = (, l), 0 = AGRRf(S), l0 :0 R l0 ((x, y)) =max({rank(s) : x <s y, S}). suffices show =0 l = l0 .Suppose x y. show x 0 y, i.e., exists x <s and,00every s0 S, x 6<s 6<s x, l0 ((x, y)) = l((x, y)). Since x y,Aj0exists Ai r xr and, every Aj r > r R, x 6r06r0j x. Since xr y, exists Si x < y, rank(s) = r, and, evs11ery s1 Si , x 6< 6< x. Si S, exists x <s y.00suppose s0 maximal rank source x <s <s x. s0 exists since x <s y. Since wS total preorder, suffices show wS s0 . Suppose00s0 Sj . Since Sj S, s0 also maximal rank source Sj x <s <s x,191fiMaynard-Zhang & LehmannjjAi y, r = rank(s) rank(s0 ), w s0 . Furx rank(s0 ) rank(s0 ) x. since x rthermore, l0 ((x, y)) = rank(s) = r = l((x, y)).suppose x 0 y. show x y, i.e., exists Ai r xrAjAj00and, every Aj r > r R, x 6r0 6r0 x, l((x, y)) = l ((x, y)).0Since x 0 y, exists x <s and, every s0 S, x 6<s06<s x. Suppose Si . Since Si S, also case every s0 Si ,00AjAj0x 6<s 6<s x, xrank(s) y. Now, let Aj r x r0 r0 x.0suffices show rank(s) r0 . Proposition 21, exists s0 Sj x <s0<s x rank(s0 ) = r0 . wS s0 , rank(s) rank(s0 ) = r0 . Furthermore,l((x, y)) = rank(s) = l0 ((x, y)). 2Proposition 23 A, PA , Definition 23, wS total order,ped (PA ) = (, l), + =.Proof: Since wS total order, AGR(S) = AGRRf(S) Proposition 17. Thus, =AGRRf(S) = AGR(S) = AGRRf(S)+ =+ . 2Proposition 24 S, vt0 (S) = Un(S).Proof: Suppose (x, y) Un(S). 6= x <s S. Thus,countS (x, y) > 0 countS (x, y)/kSk > 0, (x, y) vt0 (S). Suppose, instead, (x, y) 6Un(S). x 6<s S, countS (x, y) = 0, (x, y) 6 vt0 (S). 2Proposition 25 S, vt1 (S) = sS <s .Proof: Suppose (x, y) sS <s . 6= x <s S. Thus,count(x, y) > 0 countS (x, y)/kSk 1, (x, y) vt1 (S). Suppose, instead, (x, y) 6sS < . exists x 6< y, countS (x, y) < kSk. Thus,countS (x, y)/kSk < 1, (x, y) 6 vt1 (S). 2Proposition 26 Suppose S. vt1 (S) transitive necessarily modular.Proof: Let W = {x, y, z} = {s1 , s2 } <s1 = {(x, y), (y, z), (x, z)} <s2 ={(x, y), (z, y)}. vt1 (S) = {(x, y)} modular. 2Proposition 27 kWk 3, every p (0, 1), exists vtp (S)neither modular transitive.Proof: Note kWk = 2, every relation W either transitive modular(but necessarily both), kWk = 1 every relation W modulartransitive. Let W set worlds kWk 3 let x, y, z denote threedistinct members W. define parameterized p vtp (S) relation{(x, y), (y, z)} neither transitive modular.Let = {s1 , . . . , sn } satisfying following conditions:1. n = d1/p + 1e p 1/3, d2/(1 p) + 1e otherwise.1313. dxe denotes ceiling x, i.e., smallest integer greater equal x.192fiRepresenting Aggregating Conflicting Beliefs2. <s1 = {(w, y)|w W, w 6= y}.3. <s2 = {(y, w)|w W, w 6= y}.4. dpn1e remaining sources belief state {(x, w)|w W, w 6= x}{(w, z)|wW, w 6= z}.5. remaining sources fully disconnected belief states.clear <si B i. make two observations: First, observe pn > 1.p 1/3,pn = pd1/p + 1e 1 + p > 1.p > 1/3,pn = pd2/(1 p) + 1e 2p/(1 p) + pmonotonically increasing function,pn > 2(1/3)/(1 1/3) + 1/3 = 4/3 > 1.Second, note set described fifth condition non-empty since numbersources described conditions 2-4 2 + dpn 1e < 1 + pn less nn > 1/(1 p). true p 1/3 since valuesn = d1/p + 1e > 1/p > 1/(1 p).also true p > 1/3 sincen = d2/(1 p) + 1e > 2/(1 p) > 1/(1 p).(x, y) appears <s1 belief states described condition 3,countS (x, y) = 1 + dpn 1e 1 + pn 1 = pn(x, y) vtp (S). Similarly, (y, z) appears <s2 belief states describedcondition 3,countS (y, z) = 1 + dpn 1e 1 + pn 1 = pn(y, z) vtp (S). remains show vtp (S) members. showcount pair less pn. countS (w, w) = 0 < pn w W.countS (z, w) = countS (w, x) = 0 w 6= W. w W {x, y}, (w, y)appears <s1 , countS (w, y) = 1 < pn first observation above.w W {y, z}, (y, w) appears <s2 , countS (y, w) = 1 < pn. Finally,w W {x, y, z}, (x, z), (x, w), (w, z) appear belief states describedcondition 3,countS (x, z) = countS (x, w) = countS (w, z) = dpn 1e < pn.2Proposition 28 Suppose transitive relation W x, W.193fiMaynard-Zhang & Lehmann1. x y, lev (x) < lev (y).2. x ./ y, lev (x) = lev (y).3. lev (x) < lev (y), z. lev (z) = lev (x) z y.4. lev (x) = lev (y), x iff x.Proof:1. Suppose x y.lev (y) = 1 + max0Wlev (y 0 ) : 01 + lev (x)> lev (x).2. Suppose x ./ y. x ch(W, ) ch(W, ), lev (x) = lev (y) = 0. Suppose x 6 ch(W, ). 6 ch(W, ) lev (x) = 1 + max({lev (y 0 ) : 0 x}).0W0 one element, 0 transitivity,lev (y 00 ) : 00 lev (x).lev (y) = 1 + max00Widentical argument, lev (x) lev (y). Thus, lev (x) = lev (y).3. Suppose lev (x) < lev (y). sufficient prove following: every nonnegative integer l < lev (y), exists z lev (z) = l. proveinduction l. l = lev (y)1, must exist z lev (z) = l definition.Assume exists z 0 < l < lev (y) 1 lev (z) = l. Since l > 0,exists z 0 z lev (z 0 ) = l 1. transitivity, z 0 y.4. Suppose lev (x) = lev (y). x x 6 first part proposition,otherwise lev (x) < lev (x), x. Similarly, x 6 x, x y.2Proposition 29 transitive relation W, MC() B.Proof: Let = MC() x, y, z W. Suppose x y. lev (x) < lev (y)lev (x) = lev (y) x0 , 0 W. (lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 ).lev (x) < lev (z) lev (z) < lev (y), x z z y. Otherwise, lev (x) =lev (y) = lev (z) x0 , 0 W. (lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 ), x z.Thus, modular.also suppose z. lev (y) < lev (z) lev (y) = lev (z) 0 , z 0W. (lev (y 0 ) = lev (z 0 ) = lev (y) 0 ./ z 0 ). lev (x) < lev (y) lev (y) < lev (z),lev (x) < lev (z) transitivity <, x z. Otherwise, lev (x) = lev (y) =lev (z) x0 , 0 W. (lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 ), x z. Thus,transitive. 2Proposition 30 Suppose transitive relation W = MC().194fiRepresenting Aggregating Conflicting Beliefs1. .2. modular, =.3. lev (x) = lev (x) x W.4. 0 B 0 lev0 (x) = lev (x) x W, 0 .Proof:Let x, W.1. Suppose x y. lev (x) lev (y). lev (x) < lev (y), x y. Supposelev (x) = lev (y). x ./ y, exist x0 , 0 lev (x0 ) = lev (y 0 ) =lev (x) x0 ./ 0 , i.e., x0 = x 0 = y, x y.suppose x y. lev (x) < lev (y), x 6 x, x y.2. first part proposition, , suffices show . Supposex y.Case 1: lev (x) < lev (y). Then, Proposition 28, exists zlev (z) = lev (x) z y. modularity, z x x y. latter case,done. former case, z ./ x otherwise lev (z) 6= lev (x). Thus, xtransitivity.Case 2: lev (x) = lev (y). exist x0 , 0 lev (x0 ) = lev (y 0 ) =lev (x) x0 ./ 0 . modularity, x0 x x 0 . former case x0 ./ xProposition 28, latter x0 ./ x Proposition 28 transitivity. modularity,x0 x. applying Proposition 28 transitivity, x ./ y,x y.3. prove induction level x .Base case: lev (x) = 0. x ch(W, ). Suppose x. lev (x) =lev (y) exist x0 0 lev (x0 ) = lev (y 0 ) = lev (x) = lev (y)x0 ./ 0 , x y. Thus, x ch(W, ), lev (x) = 0 = lev (x).Inductive case: Assume lev (x0 ) = lev (x0 ) x0 lev (x0 ) <lev (x); show lev (x) = lev (x). Let z = arg max({lev (y 0 ) : 0 x});0Wlev (x) = 1 + lev (z). Also, let = arg max({lev (y 0 ) : 0 x});0Wlev (x) = 1 + lev (y). first part proposition, , x.Thus, lev (z) lev (y). Furthermore, lev (y) = lev (y) inductive hypothesis, lev (z) lev (y). lev (z) < lev (x) Definition 26 (otherwise x z, contradiction). inductive hypothesis, lev (z) = lev (z),lev (z) < lev (x) = 1 + lev (y). Since levels integral, lev (z) lev (y),lev (z) = lev (y). Thus, lev (x) = 1 + lev (z) = 1 + lev (y) = lev (x).4. Suppose 0 B, 0 , lev0 (z) = lev (z) z W. clear00 B x W, x partition corresponding level, i.e., Wlev00 (x) .Since 0 B preserve levels worlds ,must identical partitions. Suppose x, W Wi Wj partitions(for 0 ) x Wi Wj .195fiMaynard-Zhang & LehmannSuppose x y. Wi 6= Wj < j Proposition 7, x 0 y,Proposition 7. If, instead Wi = Wj , lev (x) = lev (y). Definition 26,exist x0 , 0 W lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 . Thus,x0 , 0 Wi and, since 0 , x0 ./0 0 . Proposition 7, Wi either fully connectedfully disconnected 0 , Wi must fully connected 0 . particular, x 0 y.2Proposition 31 relation W, MT() B.Proof:Since + transitive, MT() = MC (+ ) B Proposition 29. 2Proposition 32 Suppose relation W = MT().1. .2. transitive, = MC().3. modular, =+ .4. modular transitive, =.5. conflicts, neither .Proof:Let x, W.1. + and, first property Proposition 30, + MC(+ ) = , .2. Since transitive, + =. Thus, = MC(+ ) = C().3. Since modular, + modular Proposition 1 so, Proposition 30,MC(+ ) =+ . Thus, =+ .4. Since transitive, = MC() since modular, MC() = Proposition 30, =.5. first prove following lemma:Lemma 32.1 x conflict wrt + iff conflict wrt .Proof: direction obvious since transitive closure monotonicallyadditive operation. direction, suppose x conflict wrt+ . exist w0 , . . . , wn , z0 , . . . , zm Wx = w0 + + wn = = z0 + + zm = x.then, 0 n 1, exist wi0 , . . . , wipi Wwi = wi0 wipi = wi+1 .Similarly, 0 j 1, exist zj0 , . . . , zjqj Wzj = zj0 zjqj = zj+1 .Thus,x = w0 wn = = z0 zm = x,x conflict wrt . 2196fiRepresenting Aggregating Conflicting BeliefsNow, suppose x conflict wrt . x ./ since B Proposition 31. Propositions 28 30,lev+ (x) = lev (x) = lev (y) = lev+ (y)So, since x y, exist x0 , 0 W lev+ (x0 ) = lev+ (y 0 ) = lev+ (x)x0 ./+ 0 Definition 26. Thus, + conflict. lemma above, mustalso conflict.2Proposition 33 p [0, 1], AGREqp (S) B.Proof:Follows immediately definition AGREqp Proposition 31. 2Proposition 34 p [0, 1], AGRp (S) B.Proof:Again, follows immediately Proposition 31. 2Proposition 35 Suppose p [0, 1].1. wS fully connected, AGRp (S) = AGREqp (S).2. wS total order, AGRp (S) = AGRRf p (S) = AGRRf(S) = AGR(S).3. AGR0 (S) = AGR(S).Proof:Assume x, W.1. suffices show AGRRf p (S) = vtp (S) wS fully connected. Suppose(x, y) AGRRf p (S). Then, definition AGRRf p , exists(x, y) vtp ({s0 : s0 s}). Since wS fully connected, {s0 : s0 s} = S,(x, y) vtp (S).Suppose, instead, (x, y) vtp (S). definition vtp , countS (x, y) > 0exists x <s y. Pick one s. Again, {s0 : s0 s} = sincewS fully connected, (x, y) vtp ({s0 : s0 s}). Finally, s0 S. s00x holds vacuously, (x, y) AGRRf p (S).2. Suppose wS total order.AGRRf(S) = AGR(S).already shown Proposition 17Next show AGRRf p (S) = AGRRf(S). AGRRf p (S) set (x, y)exists x <s y, (x, y) vtp ({s0 : s0 s}), and,0s0 S, x y. w total order, {s0 : s0 s} = {s}. Sincex <s y, count{s} (x, y) = 1 > 0 count{s} (x, y)/k{s}k = 1 p. Consequently,(x, y) vtp ({s0 : s0 s}), proving requirement redundantwS total order. Thus, AGRRf p (S) set (x, y) x <s and,0s0 S, x y, i.e., AGRRf p (S) = AGRRf(S).Finally, AGRp (S) = MT(AGRRf p (S)) = MT(AGRRf(S)). Proposiion 14,AGRRf(S) modular transitive, AGRp (S) = AGRRf(S) Proposition 32.197fiMaynard-Zhang & Lehmann3. suffices show AGRRf 0 (S) = AGRRf(S), sinceAGR0 (S) = MT(AGRRf 0 (S)) = MT(AGRRf(S)) = AGRRf(S)+Propositions 13 32, AGR0 (S) = AGR(S).0Suppose (x, y) AGRRf 0 (S). x <s and, s0 S, x y,(x, y) AGRRf(S). Suppose, instead, (x, y) AGRRf(S). x <s and,0s0 S, x y. Let 0 = {s0 : s0 s}. Since x <s0 , countS 0 (x, y) > 0 countS 0 (x, y)/kS 0 k 0, (x, y) vt0 (S 0 ). Therefore,(x, y) AGRRf 0 (S).2Corollary 35.1 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR0 (S). AGRfsatisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,IIA, non-dictatorship.Proof:Follows immediately Propositions 35 19. 2Proposition 36 Let agent informed set sources S, support pedigreed belief state (l, sup, rtab), using aggregation function AGRp p [0, 1]. beliefstate relationMT ({(x, y) : ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p})Proof:LetR = {(x, y) : ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p}suffices show AGRRf p (S) = R. Suppose (x, y) AGRRf p (S). exists(a) x <s y, (b) (x, y) vtp ({s0 : s0 s}), (c) s0 S,0x y. (a) (c), rank(s) = max({rank(s) : x 6s y, S} {}),l(x, y) = rank(s). Thus, {s0 : s0 s} = {s0 : rank(s0 ) = l(x, y)} = rtab(l(x, y)),(x, y) vtp (rtab(l(x, y))) (b). definition vtp , countrtab(l(x,y)) (x, y) > 0countrtab(l(x,y)) (x, y)/krtab(l(x, y))k p. sup(x, y) = {s0 : rank(s0 ) =00l(x, y), x <s y} = {s0 rtab(l(x, y)) : x <s y}, ksup(x, y)k = countrtab(l(x,y)) (x, y).Thus, ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p, (x, y) R.suppose (x, y) R. (a) ksup(x, y)k > 0 (b) ksup(x, y)k/krtab(l(x, y))kp. Suppose sup(x, y); (a), least one exists. definition sup, x <s y,satisfying first condition AGRRf p , rank(s) = l(x, y). definition l(x, y),rank(s) = max({rank(s) : x 6s y, S} {}), s0 rank(s0 ) >0rank(s) (i.e., s0 s), x y. remains show (x, y) vtp ({s0 : s0 s}).Since rank(s) = l(x, y), {s0 : s0 s} = rtab(l(x, y)) showed above,vtp ({s0 : s0 s})= vtp (rtab(l(x, y)))= {(x0 , 0 ) : countrtab(l(x,y)) (x0 , 0 ) > 0, countrtab(l(x,y)) (x0 , 0 )/krtab(l(x, y))k p}.showed above, ksup(x, y)k = countrtab(l(x,y)) (x, y). Making substitution (a)(b), see (x, y) vtp ({s0 : s0 s}), (x, y) AGRRf p (S). 2198fiRepresenting Aggregating Conflicting BeliefsProposition 37 Let A, PA , S, wS Definition 32. sup (PA )support pedigreed belief state (A).Proof:Let sup (PA ) = (l, sup, rtab), l0 : W W R {}l0 ((x, y)) = max({l00 (x, y) : (l00 , sup00 , rtab00 ) PA }),sup0 : W W 2S[sup0 (x, y) =sup00 (x, y),(l00 ,sup00 ,rtab00 )PA , l00 (x,y)=l0 (x,y)rtab0 : ranks(S) Rrtab0 (r) =[rtab00 (r).(l00 ,sup00 ,rtab00 )PA , rrange(rtab00 )suffices show l = l0 , sup = sup0 , rtab = rtab0 .Suppose x, W agent Ai support pedigreed belief state (li , supi , rtabi ).l(x, y) = max({rank(s) : x <s y, S} {})[= max({rank(s) : x <s y, Si } {})Si informs Ai , Ai[= max{max ({rank(s) : x <s y, Si } {})}Si informs Ai , Ai= max({l00 (x, y) : (l00 , sup00 , rtab00 ) PA })= l0 (x, y).Also,sup(x, y) = {s : rank(s) = l(x, y), x <s y}[={s Si : rank(s) = l(x, y), x <s y}Si informs Ai , Ai[={s Si : rank(s) = l0 (x, y), x <s y}Si informs Ai , Ai[={s Si : rank(s) = li (x, y), x <s y}Si informs Ai , Ai A, li (x,y)=l0 (x,y)[=sup00 (x, y)(l00 ,sup00 ,rtab00 )PA , l00 (x,y)=l0 (x,y)0= sup (x, y).199fiMaynard-Zhang & LehmannFinally,rtab(r) = {s : rank(s) = r}[={s Si : rank(s) = r}Si informs Ai , Ai[=rtabi (r)Si informs Ai , Ai A, rrange(rtabi )[=rtab00 (r)(l00 ,sup00 ,rtab00 )PA , rrange(rtab00 )0= rtab (x, y).2Appendix B. Notation key: arbitrary finite seta, b, c, . . .: specific elements setx, y, z, . . .: arbitrary elements setA, B, C, . . .: specific subsets setX, Y, Z, . . .: arbitrary subsets set: arbitrary set relations: arbitrary relation+ : transitive closurech(X, ): choice set X wrtkXk: cardinality set XW: finite set possible worldsw, W : element, subset W, respectivelyB: set generalized belief states (modular, transitive relations): element B, strict likelihood: weak likelihood: equal likelihood, agnosticism./: conflictBel: belief conditional statementAgn: agnosticism conditional statementCon: conflict conditional statement: set total preordersT< : strict versions total preordersQ: set total, quasi-transitive relationsQ< : strict versions total, quasi-transitive relationsS: set sourcess, S: element, subset S, respectively200fiRepresenting Aggregating Conflicting Beliefs<s : belief state source: source agnosticism./s : source conflictR: set ranksr: element Rrank(s): rank sourceranks(S): set ranks sourcesw, wS : credibility ordering S, S, respectivelyUn: union set belief statesAGRUn: aggregation via unionAGRRf: aggregation via refinementAGR: general aggregationA: set agentsA: element: induced belief state(, l): pedigreed belief stater : restriction pedigreed belief state rank r: fusionped : pedigreed fusionvtp : voting function plev: level world transitive relationMC: modular closureMT: modular, transitive closureAGREqp : aggregation voting without refinementAGRRf p : aggregation voting via refinementAGRp : general aggregation voting(l, sup, rtab): support pedigreed belief statesup : support pedigreed fusionReferencesAlchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50, 510530.Andreka, H., Ryan, M., & Schobbens, P.-Y. (2002). Operators laws combiningpreference relations. Journal Logic Computation, 12 (1), 1353.Arrow, K. J. (1963). Social Choice Individual Values (2nd edition). Wiley, New York.Baral, C., Kraus, S., Minker, J., & Subrahmanian, V. S. (1992). Combining knowledgebases consisting first-order theories. Computational Intelligence, 8 (1), 4571.Benferhat, S., Dubois, D., Kaci, S., & Prade, H. (2002). Possibilistic merging distancebased fusion propositional information. Annals Mathematics Artificial In201fiMaynard-Zhang & Lehmanntelligence, 34 (13), 217252.Black, D. (1958). Theory Committees Elections. Cambridge University Press,Cambridge.Borgida, A., & Imielinski, T. (1984). Decision making committees: frameworkdealing inconsistency non-monotonicity. Proceedings WorkshopNonmonotonic Reasoning, pp. 2132.Boutilier, C. (1996). Iterated revision minimal change conditional beliefs. JournalPhilosophical Logic, 25, 263305.Brams, S. J., & Fishburn, P. C. (2002). Voting procedures. Arrow, K. J., Sen, A. K., &Suzumura, K. (Eds.), Handbook Social Choice Welfare, Vol. 1 HandbooksEconomics, chap. 4, pp. 173236. Elsevier Science.Cantwell, J. (1998). Resolving conflicting information. Journal Logic, Language,Information, 7, 191220.Center Voting Democracyhttp://www.fairvote.org/irv/.(2002).Instantrunoffvoting.Darwiche, A., & Pearl, J. (1997). logic iterated belief revision. Artificial Intelligence, 89 (12), 129.Fishburn, P. C. (1974). Lexicographic orders, utilities decision rules: survey. Management Science, 20 (11), 14421471.Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.MIT Press.Gardenfors, P., & Makinson, D. (1994). Nonmonotonic inference based expectations.Artificial Intelligence, 65 (1), 197245.Gardenfors, P., & Rott, H. (1995). Belief revision. Gabbay, D. M., Hogger, C. J., &Robinson, J. A. (Eds.), Epistemic Temporal Reasoning, Vol. 4 HandbookLogic Artificial Intelligence Logic Programming, pp. 35132. Oxford UniversityPress, Oxford.Grosof, B. (1991). Generalizing prioritization. Proceedings Second InternationalConference Principles Knowledge Representation Reasoning (KR 91), pp.289300.Grove, A. (1988). Two modellings theory change. Journal Philosophical Logic, 17,157170.Kahneman, D., & Tversky, A. (1979). Prospect theory: analysis decision risk.Econometrica, 47 (2), 263291.Katsuno, H., & Mendelzon, A. O. (1991). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.Konieczny, S., & Perez, R. P. (1998). logic merging. Proceedings SixthInternational Conference Principles Knowledge Representation Reasoning(KR 98), pp. 488498.202fiRepresenting Aggregating Conflicting BeliefsKraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferentialmodels cumulative logics. Artificial Intelligence, 44 (12), 167207. CoRR:cs.AI/0202021.Kreps, D. M. (1990). Course Microeconomic Theory. Princeton University Press.Lehmann, D. (1995). Belief revision, revised. Proceedings Fourteenth InternationalJoint Conference Artificial Intelligence (IJCAI 95), pp. 15341540.Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?.Artificial Intelligence, 55 (1), 160. CoRR: cs.AI/0202022.Liberatore, P., & Schaerf, M. (1995). Arbitration: commutative operator belief revision. Proceedings Second World Conference Fundamentals ArtificialIntelligence (WOCFAI 95), pp. 217228.Luce, D. R. (1956). Semiorders theory utility discrimination. Econometrica, 24 (2),178191.Makinson, D. C. (1997). Screened revision. Theoria, 63 (12), 1423. Special issuenon-prioritized belief revision.Maynard-Reid II, P., & Shoham, Y. (2001). Belief fusion: Aggregating pedigreed beliefstates. Journal Logic, Language, Information, 10 (2), 183209.Meyer, T. (2001). semantics combination operations. Journal Applied NonClassical Logics, 11 (12), 5984.Meyer, T., Ghose, A., & Chopra, S. (2001). Social choice, merging elections. Benferhat, S., & Besnard, P. (Eds.), Proceedings Sixth European Conference Symbolic Quantitative Approaches Reasoning Uncertainty (ECSQARU2001),pp. 466477.Ramsey, F. P. (1931). Foundations Mathematics Logical Essays. RoutledgeKegan Paul, New York.Revesz, P. Z. (1997). semantics arbitration. International Journal AlgebraComputation, 7 (2), 133160.Sen, A. (1986). Social choice theory. Arrow, K. J., & Intriligator, M. D. (Eds.), Handbook Mathematical Economics, Vol. III, chap. 22, pp. 10731181. Elsevier SciencePublishers.Spohn, W. (1988). Ordinal conditional functions: dynamic theory epistemic states.Harper, W. L., & Skyrms, B. (Eds.), Causation Decision, Belief Change,Statistics, II, pp. 105134. Kluwer Academic Publishers.Williams, M.-A. (1994). Transmutations knowledge systems. Proceedings FourthInternational Conference Principles Knowledge Representation Reasoning(KR 94), pp. 619629.203fiJournal Artificial Intelligence Research 19 (2003) 355-398Submitted 10/02; published 10/03Architectural ApproachEnsuring Consistency Hierarchical ExecutionRobert E. Wraywrayre@acm.orgSoar Technology, Inc., 3600 Green Court, Suite 600Ann Arbor, MI 48105 USAJohn E. Lairdlaird@umich.eduUniversity Michigan, 1101 Beal AvenueAnn Arbor, MI 48109 USAAbstractHierarchical task decomposition method used many agent systems organizeagent knowledge. work shows combination hierarchy persistentassertions knowledge lead difficulty maintaining logical consistency assertedknowledge. explore problematic consequences persistent assumptionsreasoning process introduce novel potential solutions. implemented onepossible solutions, Dynamic Hierarchical Justification, effectiveness demonstratedempirical analysis.1. Introductionprocess executing task dividing series hierarchically organized subtasks called hierarchical task decomposition. Hierarchical task decompositionused large number agent systems, including Adaptive Intelligent Systems architecture (Hayes-Roth, 1990), ATLANTIS (Gat, 1991a), Cypress (Wilkins et al., 1995),Entropy Reduction Engine (Bresina, Drummond, & Kedar, 1993), Procedural Reasoning System (Georgeff & Lansky, 1987), RAPS (Firby, 1987), Soar (Laird, Newell, &Rosenbloom, 1987; Laird & Rosenbloom, 1990), Theo (Mitchell, 1990; Mitchell et al.,1991), cornerstone belief-desire-intention-based agent implementations (Rao &Georgeff, 1991; Wooldridge, 2000). Hierarchical task decomposition helps agentsknowledge developer agent manage environmental complexity. example,agent may consider high-level tasks find power source fly Miamiindependent low-level subtasks go east 10 meters turn heading 135.low-level tasks chosen dynamically based currently active high leveltasks current situation; thus high-level task progressively decomposedsmaller subtasks. division labor simplifies design agents, thus reducingcost. Additional advantages hierarchical task decomposition include knowledge sharing(a low-level subtask invoked many different high-level procedures), modularity(the decomposition helps insulate subtasks interaction knowledge)naturalness representation (Simon, 1969).Without careful design, difficult ensure consistent reasoning agents employing hierarchical task decompositions. consistency, mean reasoninglead set assertions contains contradiction. Ensuring consistency becomesc 2003 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiWray & Lairdmuch difficult solve thus costly complexity agentsknowledge grows. Although problem solved careful design agentknowledge, approach requires understanding possible interactionshierarchy. Thus, correctness solution depends skill vigilanceknowledge engineer. bias seek solutions operation agents primitive memories processes structured ensure inconsistencies arise. Thus,prefer architectural solutions knowledge-based ones. Architectural solutionsguarantee consistency tasks domains, reducing brittleness due omissionstask knowledge. Further, developing architectural solution may costly,less costly repeatedly developing knowledge-based solutions different domains.following sections describe inconsistency problem introduce spacesolutions problem, including two novel solutions. theoreticalempirical analysis, one new solutions, Dynamic Hierarchical Justification, shownprovide efficient architectural solution problem ensuring reasoning consistencyhierarchical execution.2. Maintaining Reasoning Consistency Hierarchical Agentssection describes inconsistency problem greater detail. review methodsensuring consistency non-hierarchical systems discuss limitationsapproaches hierarchical systems.2.1 Consistency Non-hierarchical SystemsTruth maintenance systems (TMSs) often used maintain consistency non-hierarchical systems (Doyle, 1979; McDermott, 1991; Forbus & deKleer, 1993). inference engineuses domain knowledge create two different kinds assertions knowledge agentsknowledge base: assumptions entailments. inference engine enables assumptionsdecided treat true, without requiring assertion justified. Agents often treat environmental percepts assumptions unquestioned beliefs(Shoham, 1993). Entailments justified assertions. data structure, justification,captures reasons asserting entailment. reasons longer hold (theentailment longer justified), TMS retracts set asserted beliefs. Thus,TMS automatically manages assertion retraction entailments agentssituation changes, ensuring entailments consistent external environmentenabled assumptions.Careful construction domain knowledge required ensure enabledassumptions contradictory. example, assumption inconsistentcurrent input, agent must domain knowledge recognizes situationremoves assumption. Thus, agent utilizes TMS, problem maintainingconsistency reasoning largely one managing assumptions agents domainknowledge.Assumptions often reflect hypothetical reasoning world (hence assumptions). However, assumptions used represent persistent feature. Althoughresearchers explored structuring external environment provide persistent memory (Agre & Horswill, 1997), internal, persistent memory usually necessary agent356fiEnsuring Consistency Hierarchical ExecutionAssumptionsEntailmentsAssumptionsEntailmentsAssumptionsInferenceEngineEntailmentsAssumptionsTMSEntailmentsAssumptionsEntailmentsAssumptionsEntailmentsAssumptionsEntailmentsMemoryHierarchy MaintenanceFigure 1: hierarchical agent.domains. example, persistence required hypothetical reasoning, nonmonotonicrevisions assertions (such counting), remembering.2.2 Truth Maintenance Hierarchical AgentsTMS agent framework introduced extended hierarchical agent architectures. agent, inference engine TMS less identicalnon-hierarchical agent. agent initiates new subtask via dynamic hierarchicaltask decomposition, also creates new database contain assumptions entailments specific subtask. decomposition result stack subtasks,containing entailments assumptions specific subtask, shown Figure 1.consider creation deletion distinct databases assertions sine qua nonhierarchical architecture. architecture decomposes task identifyingrelevant subtasks, also dynamically organizing memory according currentdecomposition.new system component, hierarchy maintenance, responsible creatingdestroying subtask databases subtasks begin terminate. subtaskachieved (or determined longer worth pursuing), hierarchy maintenance respondsimmediately removing assertions associated subtask. functioncentral importance hierarchical architectures allows agent automaticallyretract assertions associated terminated subtask, requiring agent knowledgeclean remove individual assertions associated terminated subtask.hierarchy maintenance component efficiently remove assertions(conceptually) located distinct unit, subtask database.357fiWray & LairdSubtask1a11e11a12a14a13a15Subtask2e14e12e17e15e19e13e18a21e16a11 a12a22a13e21e22(a14, a15, e14, e19)PSfrag replacementsSubtask3a31e23a34a32e31e32(a12, a22, e22)Hierarchy MaintenanceFigure 2: example hierarchy maintenance. Assumptions (a) entailments (e)asserted within subtasks.agents hierarchy maintenance function employed help maintain consistency, illustrated notionally Figure 2. agent architecture identifies assertionshigher levels hierarchy led new subtask. assertions togetherform subtask support set. Figure 2, assertions 14 , a15 , e14 , e19 form supportset Subtask2 a12 , a22 , e22 support Subtask3 . support sets, effect, formjustifications subtasks hierarchy. assertion support set removed(e.g., a22 ), agent responds removing subtask (Subtask 3 ). hierarchical architectures use architectural processes create destroy subtask databases,example illustrates architectural hierarchical maintenance function realizedvia process similar justification truth maintenance.Within specific subtask, reason maintenance go before. However, hierarchical structure adds complication maintenance logical consistency. Assumptionslevel hierarchy dependent entailments assumptions higherlevels hierarchy.1 dependence relationship suggested Figure 1 curvedlines extending one subtask one it. Higher levels hierarchy formcontext reasoning local subtask.execution agents embedded dynamic domains, hierarchical context maychange almost time. changing context problematic entailments;1. Assumptions lower level subtask always least indirectly dependent higher level assertions. observation exploited Section 3.3.358fiEnsuring Consistency Hierarchical ExecutionTMS readily determine dependent context changes retract affected entailments.However, changes higher levels hierarchy (such deriving inputs)may also invalidate assumptions lower levels. Without additional architecturalmechanisms, domain knowledge required ensure consistency among assumptionshierarchical context non-hierarchical systems. domain knowledge ensuring consistency assumptions complicated necessity spanning multiple(possibly many) subtasks. refer knowledge across-level consistency knowledge. described detail below, identifying creating across-level consistencyknowledge tedious, costly, often incomplete process. Across-level knowledge mustexplicitly consider interactions different subtasks (in different levels hierarchy), rather focus solely local subtask, compromising benefithierarchical decomposition.continuing, note hierarchical architectures contrastedhierarchical task network (HTN) planners (Sacerdoti, 1975; Erol, Hendler, & Nau, 1994)execution-oriented systems use HTN representations, DECAF (Graham& Decker, 2000) RETSINA (Sycara, Decker, Pannu, Williamson, & Zeng, 1996).planning problem HTN planner represented initial task networkconsist primitive non-primitive tasks. planner uses operators find plansolve tasks. Methods allow planner match non-primitive taskstask networks describe accomplish task; thus, methods enable hierarchicaldecomposition planning problem family connected task networks.main difference HTN systems hierarchical architecturesplanner represents plan single global state. is, methods represent decomposition steps, hierarchical structure evolving plan represented blackboardlike database also reflect structure decomposition. followingsections discuss problems especially solutions depend hierarchical organization asserted knowledge execution, addition hierarchical task decomposition encoded agents task knowledge. Thus, following generallyapplicable HTN-based execution systems. However, HTN systems need addressinconsistency problem; Section 5.1.1 examines consequences global state respectinconsistency arising persistence hierarchy.2.3 Failing Respond Relevant Changes Hierarchical Contextmentioned introduction, agent fails respond relevant changehierarchical context leaves now-inconsistent assumption enabled, resultingbehavior become irrational; is, consistent knowledge. sectionexplores irrational behavior arise several illustrative examples.2.3.1 Blocks Worlduse variant blocks world illustrate inconsistency problem domainfamiliar readers. domain execution domain rather planningdomain, call Dynamic Blocks World reflect difference staticblocks world used planning. assume agent knowledge build ordered359fiWray & LairdGoal: (1 2) (2 3) (3 table)Agent Memoryput-on-table(3)put-on-table(2)put-down(2)Agent Memoryput-on-table(3)put-on-table(2)put-down(2)Agent Memoryput-on-table(3)put-on-table(2)put-down(2)emptyspaceemptyspaceemptyspace231231Actual World State231time2231entl ev 3nrExte kscknoActual World Statetime23 13 1Actual World StateFigure 3: Failing respond relevant changes hierarchical context DynamicBlocks World.tower (1-on-2-on-3) without resorting planning uses hierarchical task decompositiondetermine actions take builds tower.Figure 3, agent placing block-2 table, order reach block-3begin goal tower. put-down subtask finds empty location table.agent places empty assertion memory associated put-down subtask.figure, space immediately left gripper chosen. Whetherspace empty may directly observable may need inferred numberfacts domain stored assumption memory. Assume emptyassertion assumption. Now, assume block-3 suddenly placed underneath block-2.result inconsistency assumption (the location good place putblock-2) hierarchical context (the location longer good place putblock table).agent fails recognize block-3 moved, attempt put block-2location occupied block-3. behavior irrational, consistentagents goals knowledge (assuming agent knowledge indicatesblocks placed positions already occupied blocks). inconsistency arises agent failed recognize previously-derived assumption(empty) longer true current situation.Although example may appear contrived, specific situation arose experimental system developed explore architecture learning issues. course, possiblesimple domain reformulate task problem occur.reformulation task via changes additions knowledge exactly solutionwish avoid. is, desire architecture guarantee consistencyhierarchical context local assumptions architecture provides prioriconstraints (guidance) knowledge development process increased robustnessexecution (via consistency). conclusion returns example describe360fiEnsuring Consistency Hierarchical Executionpatrolinterceptattack(defensive)achieveproximityturntoheadingFigure 4: Decomposition behavior subtasks.architectural solution inconsistency problem solves particular problem withoutrequiring reformulation agents task knowledge.2.3.2 TacAir-SoarTacAir-Soar agents pilot virtual military aircraft complex, real-time computer simulation tactical combat (Tambe et al., 1995; Jones et al., 1999). TacAir-Soar domainindirectly accessible (each agent uses simulated aircraft sensor models perceivepilot real aircraft would sense), nondeterministic (from point viewagent, behavior agents cannot strictly predicted anticipated), nonepisodic (the decisions agent makes early simulation impact later optionscapabilities), dynamic (the world changes real time agent reasoning),continuous (individual inputs continuous values). Domains characteristicsdifficult ones create apply agents (Russell & Norvig, 1995).domain knowledge TacAir-Soar agents organized 450 subtasks;execution, resulting hierarchical task decomposition sometimes reaches depths greater10 subtasks. agent one several different mission roles, amongflying patrol mission, acting partner wing agents lead.Consider pair planes patrol, given specific instructionsengaging enemy aircraft. enemy aircraft enter patrol area, lead agent decidesintercept aircraft. lead decomposes intercept series situationdependent subtasks, may decomposed. example, Figure 4shows complex task intercepting enemy aircraft decomposeddecision turn agents aircraft specific heading. agent turns headingorder get close enough enemy agent (via achieve-proximity) launchattack.Assume three different kinds attack chosen intercept. first tactic(scare) engage attempt scare away enemy planes without using deadly force.tactic selected rules engagement specify deadly forceused, regardless number aircraft area. One remaining two tacticschosen deadly force allowed. Offensive attack appropriate friendlypatrolinterceptcount (enemy)patrolinterceptcount(friendly)patrolinterceptattack(defensive)achieveproximityturntoheadingFigure 5: Trace behavior leading intercept tactic TacAir-Soar.361fiWray & LairdFigure 6: Inconsistency due persistence.planes outnumber equal enemy planes. Defensive attack used enemy planesoutnumber friendly planes.Choosing offensive defensive attack requires counting current aircraftarea. Figure 5 shows evolution executing example decomposition.agent must count relevant enemy friendly planes. Determining planes siderelevance count often requires remembering sufficiently complex entailment count possible. instance, non-combatant aircraftcounted, requiring reasoning type aircraft. agent determinesenemy planes outnumber friendly ones, agent selects defensive-attack, leadingdecomposition.happens enemy plane flees, thus reducing actual count relevant enemy planes one? count maintained agent invalid. Standard TMSmechanisms insufficient count asserted assumption. actualnumber enemy friendly planes equal, agent switch tactic offensive attack. Continuing defensive attack consistent agentsknowledge. Additionally, friendly agents participating attack may basebehavior expectation agent pursuing offensive attack. Thus agentneeds recognize inconsistency remove current count.Figure 6 presents conceptual illustration problem. Assumptions representedsquares, entailments circles. horizontal line represents hierarchical relationshipassertions (i.e., assumptions entailments) hierarchical context (aboveline) assertions local subtask (below line). arrowed lines representdependence creation assertion. previous examples, reasoningsubtask may require persistence, leading creation assumptionassumption 1. However, persistent assertion may still depend assertions.work focuses dependent assertions higher level context, A, B, C, D,E1 figure.Suppose world changes E1 retracted memory E2 asserted.Assumption 1 remains memory. E 2 would also lead 1 (e.g., could leadnew assumption 2, shown), 1 longer justified may consistenthigher level context. Whether potential inconsistency among assertionsleads inconsistent behavior depends use assumption 1 later reasoning.362fiEnsuring Consistency Hierarchical Execution3. Solutionsgoal develop architectural solutions allow agent support persistentassumptions simultaneously avoid inconsistencies across hierarchical contextlead irrational behavior. introducing two new architectural solutions, however,examine knowledge-based approaches consequences order providerationale architectural approach.3.1 Knowledge-based SolutionsInconsistency avoided hierarchical agents creating domain knowledgerecognizes potential inconsistencies responds removing assumptions. Many planning agent systems use explicit domain knowledge represent knowledgeinteractions among assertions world. example, Entropy Reduction Engine(ERE) (Bresina et al., 1993) one agent system relies knowledge-based assumption consistency (KBAC). ERE requires domain constraints, knowledge describesphysics task domain. Domain constraints identify impossible conditions.instance, domain constraint would indicate robot cannot occupy two differentphysical locations simultaneously.ERE, domain constraints specifically used maintain consistency currentworld model state execution (Bresina et al., 1993, pp. 166). However, manyarchitectures use KBAC well (perhaps conjunction methods). KBACknowledge viewed simply domain knowledge must added systemachieve consistent behavior.KBAC always necessary maintain consistency among assumptions withinlevel hierarchy. However, order guarantee consistency assumptions distributed throughout hierarchy, possible interactions leading inconsistency mustidentified throughout hierarchy. knowledge engineering problem add significant cost agent development. knowledge designer must specify conditionsassumption asserted also conditions mustremoved. TacAir-Soar interception example, enemy plane flees, agentrequires knowledge disables assumptions depend upon number enemyairplanes. Similarly, Dynamic Blocks World, agent must knowledgerecognizes situation, subtask, cause disabling empty.cases, KBAC knowledge crosses levels hierarchy. complete KBAC solutionrequires agents knowledge capture potential dependencies assumptionslocal subtask higher levels hierarchy.Although possible encode complete across-level consistency knowledgesimple domains, experience TacAir-Soar complex agent systems convincedus KBAC requires significant investments time energy. Further,often possible enumerate conditions assumption must removed,agents also brittle, failing difficult-to-understand, difficult-to-duplicate ways.insufficiency knowledge-based solutions led us consider architectural solutionsproblem. Architectural solutions eliminate need domain knowledge encodedaddress inconsistency hierarchical context assumptions withinsubtask. Thus, cost developing individual agents reduced. addition363fiWray & Lairdgenerality, definition, architectural solutions also complete, thus ableguarantee consistency hierarchical context assumptions within subtask,times, agent task. completeness improve robustness agentsystems, especially situations explicitly anticipated designers.3.2 Assumption JustificationOne potential architectural solution inconsistency problem justify assumption hierarchy respect assertions higher levels hierarchy. AssumptionJustification extension truth maintenance approaches consistency outlinedpreviously. assumption hierarchy treated entailmentrespect dependent assertions higher hierarchy. new data structure, assumption justification, created captures reasons hierarchical contextparticular assumption. Locally, assumption treated exactly like assumptionnon-hierarchical system. However, assumption justification longer supported(indicating change dependent hierarchical context), architecture retractsassumption.Refer Figure 2. agent asserts 34 , architecture builds assumption justification assumption includes 22 a21 . agent retractsa21 , assumption justification 34 longer supported architecture alsoretracts a34 . architecture ensures reasoning consistency across hierarchy levelsassumption persists longer context assertions led creation.Assumption Justification solves inconsistency problem dependencieshierarchical context captured justification. Within subtask, domain knowledge still required ensure consistency among enabled assumptions subtask.However, across-level consistency knowledge needed. Assumption Justification stillsupports local nonmonotonic hypothetical reasoning. Thus, Assumption Justificationappears meet functional evaluation criteria. However, order assess impactperformance, implementation details must considered.3.2.1 Implementing Assumption JustificationCreating assumption justifications requires computing context dependencies assumption, similar computation justifications entailments. 2 Figure 7 outlinesprocedure computing assumption justification data structure. procedureinvoked assertion created. procedure creates assumption justificationsevery assertion local subtask; is, entailments well assumptions. approach allows architecture cache context dependencies local assertion.advantage caching architecture simply concatenate assumptionjustifications local assertions contributing directly creation assumption2. Assumption Justification procedure, presented, requires inference engine record justification every assertion course processing. Soar, architecture AssumptionJustification implemented, calculations available production rule matcherassumptions entailments. However, justification calculations assumptions may supportedarchitectures, requiring modifications underlying inference engine. Laird Rosenbloom(1995) Soar Users Manual (Laird, Congdon, & Coulter, 1999) describe specific mechanismsjustification creation Soar.364fiEnsuring Consistency Hierarchical ExecutionPROC create new assertion(. . .)assumption justification computed new assertioncreated. Thus, assumption justifications computedassumptions entailments....Ajust create justif ication(. . .)Justifications created via well-known, textbook algorithms(e.g., Forbus & deKleer, 1993; Russell & Norvig, 1995)Aaj make assumption justif ication f assertion(A)...ENDPROC make assumption justif ication f assertion(assertion A)AJ NILassertion j Ajust , justification1(Level(j) closer root Level(A))AJ append(j, AJ) (add j assumption justification)2ELSE(j level)AJ concatenate(jaj , AJ) (add assumption justification jassumption justification A)return AJ, list assertions comprising assumption justificationENDPROC Level(assertion A)Return subtask level associated assertionFigure 7: procedure building assumption justifications.(in2). chose caching option computing assumption justifications ondemand implementation would, assumption created, recursively followlocal dependencies context dependencies determined. advantagecaching implementation context dependencies assertion must computed once, even local assertion contributes creation multiple localassumptions.procedure creates assumption justification loops assertionsjustification new assertion A. assertions justification either contextlocal assertions. Context assertions,1, added assumption justification directly.However, local assertions added assumption justificationassumption justification include context dependencies. example, architecture retract local assertion reasons change hierarchicalcontext (e.g., non-monotonic reasoning step change enabled assumptionssubtask) cases, agent necessarily retract dependent assumptions.assumption justifications local assertions justification alreadycomputed (i.e., cached, described above), assumption justification365fiWray & LairdB C EB C EPSfrag replacements(a)12(b)1Figure 8: Technical problems Assumption Justification. (a), assumption replacesanother assumption nonmonotonically. (b), multiple assumption justificationsassumption must supported.local assertion j simply added assumption justification A,2.on-demand implementation, procedure would recur local assertions,context dependencies local assertions contributing identified.worst-case computational complexity algorithm polynomial numberassertions subtask. addition higher-level assertion done constanttime (a single pointer reference). However,2 must uniquely add context assertionsassumption justification local assertion. (n 1) local assumptionjustifications whenever nth assertion created. Thus, concatenation needsperformed (n1) times call assumption justification procedure.limit provides upper bound O(n) complexity assumption justificationprocedure: worst-case cost building individual assumption justification linearnumber assertions, n, level. However, architecture executes assumptionjustification procedure every assertion level. Thus, worst case cost buildingjustifications particular level O(1 + 2 . . . + n) O(n 2 ).Non-monotonic changes complicate implementation. architecture must disablereplaced assumption, rather delete it, initial assumption may needrestored. example, Figure 8 (a), assume assertion E leadsassertion 2 local subtask retraction 1 (i.e., 2 revision 1).agent retracts E, Assumption Justification retract 2, desired, must alsore-enable 1. Thus, assumption 1 must remain available memory, although disabled.Figure 8 (b) illustrates second problem. assumption multiple assumption justifications. justifications change reasoning progresses. Assumption 1initially depends assertions A, B, C higher levels. assume laterprocessing, agent removes A, normally would result retraction 1.However, meantime, context changed 1 also justified {C,D, E}. agent removes A, architecture immediately retract 1must determine 1 justified sources.implementation Assumption Justification Soar completed membersSoar research group University Michigan. Experiments using Air-Soar, flightsimulator domain (Pearson et al., 1993), showed overhead maintaining priorassumptions level produced significant negative impact agent performance.domain, Assumption Justification incurred significant computational cost, requiring366fiEnsuring Consistency Hierarchical Executionleast 100% time original Air-Soar agent. Further, number assumptionjustifications maintained within level continued grow execution, reasonsexplained above. subtasks required minutes execute aircraft performedmaneuver, leading large (and problematic) increases amount memory required.Thus, Assumption Justification failed meet efficiency requirements theoreticalempirical grounds. Although limitations Assumption Justification might improved developing solutions technical problems, abandoned explorationapproach strongly discouraging results.3.3 Dynamic Hierarchical JustificationFigure 2 introduced notion support set subtasks. Procedural ReasoningSystem (PRS) (Georgeff & Lansky, 1987) Soar (Laird et al., 1987) use architecturalmechanisms retract complete levels subtask hierarchy support set longerholds. section, consider solution leverages hierarchy maintenancefunction ensure consistency assumptions higher level context.significant disadvantage support set existing systems fixed.Soar, support set computed initiation subtask updatedreflect reasoning occurs within subtask. example, Figure 2, supposeassumption a34 depends assumptions a22 a21 (represented dashed, arrowedlines). support set include 21 ; assertion may even presentSubtask3 created. local assumption depends assertionsupport set, change assertion directly lead retractionassumption (or subtask). Thus, approaches using Fixed Hierarchical Justification(FHJ) still require knowledge-based solutions consistency. FHJ discussedSection 5.1.2.propose novel solution, Dynamic Hierarchical Justification (DHJ), similarFixed Hierarchical Justification, dynamically updates support set reasoningprogresses. Assumption justifications individual assumptions unnecessary. However,one consequence simplification subtask (and assertions within it)retracted dependent context changes. Refer Figure 2. DHJ agentasserts a34 Figure 2, architecture updates support set Subtask 3 include a21 .Assumption a22 already member support set need added again.member support set Subtask 3 changes, architecture retracts entiresubtask. Thus Dynamic Hierarchical Justification enforces reasoning consistency acrosshierarchy subtask persists long dependent context assertions.3.3.1 Implementing Dynamic Hierarchical JustificationFigure 9 outlines procedure computing support set DHJ. AssumptionJustification, architecture directly add context assertions support set1.architecture computes dependencies local assertion3, assertionmarked inspected4. Inspected assertions simply ignoredfuture2, architecture already added assertions dependenciessupport set. architecture also ignore dependent, local assumptions2dependencies assumptions already added support set.367fiWray & LairdPROC create new assertion(. . .)Whenever new assumption asserted, support set updatedinclude additional context dependencies....Ajust create justif ication(. . .)assumptionsubtask assertedSsupport set append(Ssupport set , add dependencies support set(A))...ENDPROC add dependencies support set(assertion A)assertion j Ajust , justification1{Level(j) closer root Level(A)}append(j, S) (append context dependency support set)234ELSEIF {Level(j) Level(A)j assumptionj previously inspected }append(S, add dependencies support set(j))(compute support set dependencies j add S)jinspected true(js context dependencies added support set)return S, list new dependencies support setENDPROC Level(assertion A)Return subtask level associated assertionFigure 9: procedure Dynamic Hierarchical Justification.DHJ needs inspect local assertion once, context dependenciescomputed on-demand, rather cached Assumption Justification. Condition2true whenever local entailment whose context dependencies yetcomputed. dependencies determined calling add dependencies support setrecursively. Recursive instantiations add dependencies support set receive localassertion justification uninspected entailment, j, return list comprisingcontext dependencies j. return value appended support setprior instantiation add dependencies support set.recursive call add dependencies support set3 non-constant timeoperation procedure. must made assertion j thusworst case complexity compute dependencies linear number assertionslevel, Assumption Justification. However, DHJ requires single inspectionindividual assertion, rather repeated inspections new assumption As368fiEnsuring Consistency Hierarchical Executionsumption Justification. Thus architecture needs call add dependencies support set n times subtask consisting n assertions, worst case costupdating support set level remains O(n). reduction complexity potentially makes Dynamic Hierarchical Justification efficient solution AssumptionJustification, especially number local assertions increases.Additionally, two technical problems outlined Assumption Justificationimpact DHJ. DHJ never needs restore previous assumption. dependencychanges, architecture retracts entire level. Thus, DHJ immediately deletereplaced assumptions memory. DHJ collects dependencies assumptions,need switch one justification another. Figure 8 (b), dependenciesA, B, C, D, E added support set. simplifications makesupport set overly specific reduce memory computation overhead requiredDynamic Hierarchical Justification.DHJ retractions sometimes followed regeneration subtaskre-assertion reasoning retracted. example, enemy plane fleddescribed TacAir-Soar scenario, DHJ would retract entire level associatedcounting subtask. count would need re-started beginning.Section 3.4.3 examines potential problems introduced interruption regeneration.cost incurred regeneration previously-derived assertions primarydrawback Dynamic Hierarchical Justification.3.4 Implications Dynamic Hierarchical JustificationDynamic Hierarchical Justification solves specific problem maintaining reasoningconsistency hierarchy, guaranteeing consistency utilizing efficient algorithm.heuristic DHJ employs assumes assumptions closely associatedsubtasks retracting subtasks nearly equivalent retracting individual assumptions.section explores implications heuristic, focusing task decompositions,impact agents ability use persistent assumptions, feasibility interruptingagent (with subtask retraction) midst reasoning.3.4.1 Influence Task Decompositionagents reasoning viewed knowledge search (Newell, 1990). perspective, inconsistency problem failure backtrack knowledge search. worldchanges, leading changes agent hierarchy. agent must retractknowledge previously asserted, backtrack knowledge state consistentworld state.3 solution described terms way achieves (oravoids) backtracking knowledge search. instance, KBAC leads knowledge-basedbacktracking, KBAC knowledge tells agent correct assumptionsgiven current situation.3. Obviously, world state usually different agents initial state often impossiblereturn prior state execution system. use backtrack section referretraction asserted execution knowledge remaining asserted knowledge consistentcurrently perceived world state.369fiWray & LairdB C E F G H(a)1B C E(b)212Figure 10: Examples (a) disjoint dependencies (b) intersecting assumption dependencies.Assumption Justification form dependency-directed backtracking (Stallman &Sussman, 1977). dependency-directed backtracking, regardless chronological orderarchitecture makes assertions, architecture identify retractassertions contributed failure search retain assertions. Assumption Justification, architecture retracts assumptions directly affectedchange context. Assumptions created later processing, dependentchange, unaffected. Consider examples Figure 10. (a), assumptions1 2 depend upon disjoint sets assertions. Assumption Justification, removal assertion 1s assumption justification result retraction 1; 2unchanged, even architecture asserted 2 1.Dynamic Hierarchical Justification similar backjumping (Gaschnig, 1979). Backjumping heuristically determines state current search backtrackbackjump. heuristics used backjumping based syntactic featuresproblem. instance, constraint satisfaction problems, backjumping algorithmidentifies variable assignments related variable assignments via constraints specified problem definition. violation discovered, algorithmbacktracks recent, related variable (Dechter, 1990). Intervening variable assignments discarded. DHJ, assertion hierarchy changes, systembackjumps knowledge search highest subtask hierarchy dependentchange. Figure 10 (a) dependent assertions collected support setsubtask. higher level assertions change, entire subtask removed.using DHJ, backjumping, previous knowledge search may needrepeated backtracking. Assume removal subtask Figure 10 (a)due change A. similar subtask reinitiated, assumption 2 may needregenerated. regeneration unnecessary 2 need retractedavoid inconsistency. Dynamic Hierarchical Justification, agent retractsreasoning dependent subtask (and lower levels hierarchy); assertionsdependent change context also removed. Thus, like backjumping, DHJuses syntactic feature reasoning (decomposition subtasks) choose backtrackingpoint backtracking always conservative possible.Although subtask decomposition syntactic feature knowledge search,strongly principled one, reflecting semantic analysis task knowledge designer. Hierarchical task decomposition based premise tasks brokendiscrete units little interaction units; nearly decom370fiEnsuring Consistency Hierarchical Executionposable (Simon, 1969). Thus, goal hierarchical decomposition separate mostlyindependent subtasks one another. consequence separation dependencies higher levels limited much possible (interaction subtasksminimized) dependencies among assertions particular subtaskshared (otherwise, subtask could subdivided two independent subtasks).course, often possible decompose given task many different ways.cases domain imposes minimal constraint knowledge engineer significantlatitude crafting task decomposition.situation illustrated Figure 10, (b) would complete decompositiontask knowledge engineer (a), assuming two alternatives representdecomposition task. (b), number dependent assertions necessarily grow function number assumptions local level, (a)does. Further, (a), two independent assumptions pursued. assumptionscould potentially inferred separate subtasks alternate decomposition. (b),hand, assumptions subtask closely tied together termsdependencies thus better asserted within subtask. dependencies assumptions 1 2 considerable overlap (b), Assumption Justificationpays high overhead cost track individual assumptions (most) everythinglocal subtask would removed simultaneously assertions B, C, changed. DHJ incurs overhead, DHJ better choice intersectionassumption dependencies high. Task knowledge structured like situation(b), rather (a) would lead unnecessary retractions. (b) appearsbetter reflect well-decomposed tasks, Dynamic Hierarchical Justification constrainknowledge development process improve resulting decompositions. Consequently,nearly-decomposed tasks allow DHJ avoid unnecessary regenerationsavoiding processing overhead Assumption Justification.3.4.2 Limiting Persistence DHJDHJ limits persistence subtasks, resulting assumptions persistentassumptions typical truth maintenance systems. section explores consequenceslimitations determine DHJ architectures 4 still provide persistencenecessary agent execution (Section 2.1).DHJ retract subtask potential inconsistency could impact hypotheticalrecursive reasoning like counting. Consider aircraft classification counting example.Perhaps aircrafts altitude contributes hypothetical classification aircraft(e.g., particular altitude speed combinations might suggest reconnaissance aircraft).agent would create assumptions locally depend aircrafts altitude.altitude (or altitude boundary) changes, assumption retracted.retraction required avoid inconsistency. contacts altitude longer suggestsreconnaissance aircraft, assumption depended assertionremoved. DHJ captures dependencies performs retraction. agent4. clarity, Assumption Justification already eliminated candidate solution,following discussion focuses exclusively DHJ. However, Assumption Justification limits persistencesimilarly.371fiWray & Lairdopportunity reconsider classification aircraft, pursue tasksclassification longer important.DHJ also retract subtask assumption created local subtaskpurpose remembering input (or elaboration input). example, agentneeded remember particular aircrafts altitude particular point time,assumption cannot stored local subtask. DHJ limits persistence wayremembering within local subtask generally impossible.order remember previous situations, assumptions asserted root task.assumption asserted level never retracted higher leveldependencies (assuming percepts associated top level higher inputlevel, Theo, Mitchell et al., 1991). primary drawback requirementremembering remembered items longer local subtask created them,requiring additional domain knowledge manage remembered assumptions. However,remembering already requires domain knowledge; possible remember assertionregardless dependencies also able retract architecturally.examples show Dynamic Hierarchical Justification still allows formspersistence, trades capturing dependencies nonmonotonic assumptions local subtasks remembering assumptions root task, dependencies captured.DHJ forces remembered items root task, also suggests fundamental aspect root task managing remembered assumptions. viewrequirement positive consequence DHJ, forces knowledge engineersbetter recognize reasons creating assumption (e.g., remembering vs. hypothetical) circumscribes remembering develop adopt functionaltemporal theories manage assumptions created remembering (e.g., Allen, 1991;Altmann & Gray, 2002).3.4.3 Recovery Interruption DHJDynamic Hierarchical Justification makes agent reactive environment, ensuring relevant changes environment lead retraction dependentsubtasks. DHJ imposes automatic interruption agent subtask retraction,without evaluating state system first. Although automatic interruption increasesreactivity system, lead difficulties way override it.section examine two cases uncontrolled interruption cause problems.problems arise DHJ biases system reactive; is, respond automatically changes environment without deliberation. However, cases,additional agent knowledge overcome bias make system deliberateavoid uncontrolled interruption.first problem arises sequence actions must completedwithout interruption order subgoal achieved. processing interrupted,possible, dynamics world, task cannot resumed.example, imagine aircraft nearing point launch missile target.task interrupted resumed, aircrafts position may changedenough, relative target, additional steering commands necessary372fiEnsuring Consistency Hierarchical Executionmissile launched. case, may preferable interrupt originallaunch sequence begun.Consider two possible approaches achieving capability Dynamic HierarchicalJustification architectures. first move processing root task.root task interrupted, processing interrupted. However,approach greatly restricts task hierarchically decomposed thusconsidered last resort. second approach add new reasoning taskfreezes external situation respect additional reasoning subtask.new processing initiates execution subtask creates persistent structuresroot task. persistent structures represent deliberate commitmentinterrupted. remaining processing subtask accesses structuresexecution task. Thus, persistent, even changessurrounding situation would interrupted subtask, processinginsensitive changes interruption prevented. approach also requiresadditional reasoning recognize completion uninterruptible behavior removepersistent structures built initial subtask. reasoning reflects deliberate act,signaling commitment longer holds. abstract, together additionsprovide mechanism overcoming automatic interruption. disadvantageapproach that, part system design, subgoals cannot interruptedmust identified beforehand. subtasks, additional agent knowledge mustimplemented create remove encapsulations dynamic data.critical problem DHJ Wesson Oil problem: someone cookingdinner higher-priority activity suddenly occurs (a hurt child), cook turnstove (a cleanup procedure) leaving hospital (Gat, 1991b). problemoccurs change hierarchical context level far terminallevel hierarchy. situation, similar tasks may resumed initiatedfollowing interruption. agent must therefore recognize whether cleanupexternal and/or internal states necessary, and, so, perform cleanup. EvenDHJ, agent still behave appropriately right knowledge. particular,agent must able recognize partially completed tasks (like cooking dinner)able select cleanup actions specific task state (like turning stove burner).DHJ requires remembered assumptions asserted root levelhierarchy, recognition task internal state available; need try reconstructstate external environment alone. However, require analysistask domain(s) knowledge engineer interruptible activity requiringcleanup include triggering assertions cleanup root task.work prompted desire architectural solutions inconsistency, yetmaintaining consistency efficiently lead interruptions, which, DHJ, requiresknowledge-based solutions problems arising automatic interruption. 5 However,requirements imposed DHJ positive consequences. Subtask retractionsobserved recovery development process help define must rememberedroot task cleanup, significantly different laborious process debugging5. Dynamic Hierarchical Justification could also used trigger meta-level deliberation ratherimmediate subtask retraction. would possibly provide architectural solution questiondeliberate potential inconsistency intention reconsideration (see Section 5.2).373fiWray & Lairdagent programs failing due inconsistency. theory, Dynamic HierarchicalJustification imposes requirements handling interruptions pose serious questionsoverall utility. practice, found addressing questionsproblem variety recent agent implementations using Soar-DHJ architecture (e.g.,Laird, 2001; Wray et al., 2002).4. Empirical Evaluation Dynamic Hierarchical JustificationArchitectural mechanisms like DHJ must efficient. demonstratedalgorithm efficient, question impact overall behavior generationcapability agent remains open question due interruption regeneration. Givencomplexity agent-based systems domains applied,analytical evaluations must extremely narrow scope, even require specializedtechniques (Wooldridge, 2000). section instead pursues empirical evaluationDynamic Hierarchical Justification, focusing efficiency responsiveness two domainsextremes continua agent domain characteristics. architecturalsolution inconsistency motivated cost (and incompleteness) knowledgebased solutions, knowledge development costs also estimated.4.1 Methodological IssuesDynamic Hierarchical Justification general solution, applicable wide range agenttasks. order evaluate solution, number methodological issues mustaddressed. following describes three important issues choices madeevaluation.4.1.1 Relative vs. Absolute Evaluationconstitutes good poor cost performance evaluations? general,absolute evaluation performance cost difficult task itself, additionagents knowledge architecture, determines overall cost performance results.circumvent problem making relative comparisons agents usingoriginal, Fixed Hierarchical Justification Soar architecture (FHJ agents) new agents(DHJ agents). FHJ agents provide cost performance benchmarks, obviatingneed absolute evaluations.4.1.2 Addressing Multiple Degrees Freedom Agent DesignEven architecture task fixed, many different functional agents developed. one know comparative results valid general experimentercontrol benchmarks new agents?DHJ agents compared agents previously implemented others. systemsprovide good performance targets, optimized performance,minimize bias, developed independently.FHJ systems used fixed benchmarks, modified. DHJ agents useidentical task decompositions employed FHJ agents initial knowledgebase. observed opportunities improve performance DHJ agents modifying374fiEnsuring Consistency Hierarchical Executioneither task decomposition re-designing significant portions agent knowledgebase. However, agent knowledge modified necessary correct behavior,order ensure DHJ agents remained tightly constrained FHJ counterparts,thus limiting bias evaluation.4.1.3 Choice Representative Tasksevaluation limited execution agents Dynamic Blocks Worldreduced-knowledge version TacAir-Soar (micro-TacAir-Soar). choicetasks domains considerable drawback benchmarks (Hanks, Pollack, & Cohen, 1993). Although choices motivated primarily availability domainspre-existing FHJ agents, two domains represent opposite extremes manydomain characteristics. Micro-TacAir-Soar, like TacAir-Soar, inaccessible, nondeterministic, dynamic, continuous, Dynamic Blocks World simulator usedexperiments accessible, deterministic, static discrete. primary motivationusing Dynamic Blocks World, less representative typical agent tasksMicro-TacAir-Soar, assess cost employing DHJ domain prioriappears would useful (although Section 6 suggests DHJ prove useful evenrelatively static domains). Thus, Dynamic Blocks World provide baselineactual cost deploying algorithm, even though little benefit expecteddeployment domain.4.2 Evaluation HypothesesAlthough specific expectations differ different domains, differences dimensionsknowledge cost performance anticipated comparing DHJ agentsbaseline agents. following discusses expectations metric(s) useddimension.4.2.1 Knowledge Engineering CostKnowledge engineering effort DHJ agents decrease comparison previouslydeveloped agents. Knowledge Soar represented production rules. productionrepresents single, independent knowledge unit. assume addition productions represents increase cost measure knowledge cost counting numberproductions type agent. number productions, course, providescoarse metric cost complexity individual productions varies significantly.However, productions removed DHJ agents often difficultones create. Therefore, difference number productions probably conservativemetric knowledge cost DHJ.4.2.2 Performance: Efficiency Responsivenessgeneral, overall performance change little DHJ agents, compared FHJcounterparts. Although Dynamic Hierarchical Justification add new architecturalmechanism, algorithm efficient contribute significant differences performance. Further, less domain knowledge need asserted375fiWray & Lairdacross-level consistency knowledge incorporated architecture. Thus, applying across-level KBAC knowledge represented significant expense overall costexecuting task, DHJ agents might perform better FHJ agents.two specific exceptions expectation. First, domains consistency knowledge (mostly) unnecessary task performance, FHJ agents may performbetter DHJ agents. example, Dynamic Blocks World requires little consistency knowledge DHJ architecture still update support set, even thoughinconsistency-causing context changes arise.Second, regeneration problematic, overall performance suffer. DHJ, wheneverdependent context changes, subtask retracted. change leaddifferent choice subtask, subtask necessarily regenerated. Thus,DHJ, subtask regeneration occur, and, regeneration significant, performancedegradation result.CPU execution time provides simple, single dimension gross performance. CPUtime reported individual experiments reflects time agent spends reasoninginitiating actions rather time takes execute actions environment.Decisions: Soar, subtasks correspond selection operators subgoalsimplementing operators. selection operator called decision. Soarselects operator, tries apply operator. Soar reaches impasse cannotapply newly selected operator. non-primitive operators lead generationsubgoal subsequent decision. example, Soar selects put-down operatorone decision creates subgoal implement put-down subsequent decision.Together, two steps constitute notion subtask Soar.number decisions thus used indication number subtasksundertaken task. FHJ, subtask generally never interrupted terminated(either successfully unsuccessfully). DHJ, subtasks interrupted wheneverdependent change occurs. Thus, decisions increase DHJ agents subtasksinterrupted re-started. Further, decisions increase substantially (suggestingsignificant regeneration), overall performance degrade.Production Firings: production rule fires conditions match resultapplied current situation. Production firings decrease DHJ two reasons.First, across-level consistency knowledge previously used FHJ agentslonger necessary (or represented); therefore, knowledge accessed. Second,reasoning occurred inconsistency arose FHJ agents interruptedeliminated. However, production firings increase significant regeneration necessary.4.3 Empirical Evaluation Blocks WorldAgents Dynamic Blocks World domain execution knowledge transforminitial configuration three blocks ordered tower using simulated gripper arm.table simulation width nine blocks. agents task goal alwaysbuild 1-on-2-on-3 tower. agent built tower resulting 981 unique,non-goal, initial configurations blocks. Table 1 summarizes results tasks.expected, total knowledge decreased. Overall performance improved. Decisions increased,expected, number rule firings increased well, anticipated.376fiEnsuring Consistency Hierarchical ExecutionRulesDecision Avg.Avg. Rule FiringsAvg. CPU Time (ms)FHJxs.d.18887.120.9720.3 153.5413.1 121.6DHJxs.d.175141.138.7855.6 199.6391.6 114.0Table 1: Summary knowledge performance data Blocks World. agentsperformed tower-building task 981 configurations. Task orderrandomly determined.4.3.1 Knowledge DifferencesTotal knowledge decreased 7% DHJ agent. small reduction consistentexpectation. aggregate comparison misleading knowledgeadded (16 productions) deleted (29).Removing Consistency Knowledge: Soar, subtask operator subgoalterminated separately. Soar monitors impasse-causing assertions determine subgoal(such subtask goal) removed via FHJ. However, removal subtaskoperator requires knowledge. original, FHJ architecture treats initiationoperator persistent assumption requires knowledge recognize selectedoperator interrupted terminated. knowledge categorized consistency knowledge determines time subtask terminated,even initiating conditions subtask longer hold.DHJ, effects operators persistent; assertions entailmentssituation. Thus, initiation subtask treated entailmentsubtask remains selected long initiation conditions subtask hold.change removes need knowledge terminate subtask: subtaskinitiation conditions longer true, subtask automatically retracted. Thus,termination knowledge removed subtask operators.Filling Gaps Domain Knowledge: persistence subtasks original architecture allows FHJ agents ignore large parts state space domain knowledge.example, knowledge initiates stack put-on-table subtasks assumesgripper currently holding block. tasks executed, gripper,course, grasp individual blocks. conditions initiating stack put-on-tableholding block ignored original domain knowledge.DHJ agent requires knowledge determine subtasks chooseholding blocks, subtasks interrupted agent still holds block.16 productions necessary, primarily stack put-on-table operators.important note knowledge necessary domain knowledge. FHJ agents couldsolve problem began task holding block lackeddomain knowledge states. additions thus positive consequenceDHJ. architectures enforcement consistency revealed gaps domain knowledge.377fiWray & Laird4.3.2 Performance DifferencesSomewhat surprisingly, overall performance DHJ agents (measured CPU time) improves slightly comparison FHJ agents, even though decisions productionfirings increase. Soar-specific performance metrics considered individuallybelow, overall performance improvement considered.Decisions: FHJ agents, average, made considerably fewer decisions DHJ agents.difference consistent across every task. additional decisions resultremoval subsequent regeneration subtasks. example, agent picksblock pursuit stack task, selection stack task must regenerated.knowledge DHJ agents could modified avoid testing specific configurationsblocks thus avoid many regenerations.Production Firings: number production firings also increased Blocks World.increase production firings attributed knowledge added systemregeneration subtasks made additions necessary. relative increasenumber production firings (19%) much smaller increase decisions (62%).smaller difference attributed productions removed (and thusfire).CPU Time: Generally, production firings increase Soar, increase CPUtime expected. However, CPU time DHJ decreased slightly comparison FHJeven though production firings increased. explain result, additional aspectsSoars processing must considered.match cost production constant grows linearly numbertokens, partial instantiations production (Tambe, 1991). token indicatesconditions production matched variable bindings conditions.Thus, token represents node search agents memory matchinginstantiation(s) production. specific productions conditions are,constrained search memory, thus costs less generate instantiation.new productions added DHJ Blocks World agent specificagents memory (i.e., external internal state) productions removed.Further, simply fewer total productions also reduce amount total searchmemory.6 informal inspection match time tokens several FHJDHJ runs showed number tokens decreased DHJ 10-15%. reductiontoken activity primary source improvement Dynamic Blocks World DHJagent CPU time. improvement, course, general result providesguarantee task domain cost matching increase ratherdecrease.6. RETE algorithm (Forgy, 1979) shares condition elements across different productions. Thus,removal productions decreases total search removed productions contain condition elementsappearing remaining productions. perform exhaustive analysis conditionelements determine removed productions reduce number unique condition elementsRETE network.378fiEnsuring Consistency Hierarchical Execution4.4 Empirical Evaluation TacAir-SoarConverting TacAir-Soar DHJ architecture would expensive, requiring manymonths effort. DHJ agents instead developed research instruction versionTacAir-Soar, Micro-TacAir-Soar (TAS). TAS agents use TacAir-Soar simulationenvironment (ModSAF) interface knowledge fly missions, resulting order magnitude decrease number productions agents. However,TAS uses tactics doctrine missions TacAir-Soar.TAS, team two agents (lead wing) fly patrol mission describedpreviously. engage hostile aircraft headed toward withinspecific range. lead agents primary role fly patrol route interceptenemy planes. wings responsibility fly formation lead.total knowledge significantly reduced, converting TAS DHJ agents relativelyinexpensive. However, results representative TacAir-Soar TASretains complexity dynamics TacAir-Soar.patrol mission clearly-defined task termination condition like DynamicBlocks World. address problem, agent simulation executes tenminutes simulator time. time, agent opportunity take off,fly formation partner patrol, intercept one enemy agent, return patrolintercept. actual TacAir-Soar scenario, activities would normallyseparated much larger time scales. However, agent spends much timepatrol mission simply monitoring situation (waiting), rather taking new actions.Ten minutes simulated time proved brief enough overall behaviordominated wait-states, also providing time natural flow events.running fixed period time, increase number decisionsattributed regeneration simply improvement decision cycle time. avoidpotential confusion running simulator constant cycle time. mode,simulator update represents 67 milliseconds simulated time. agentruns fixed period time fixed updates, FHJ DHJ agent executenumber decisions. problems due regeneration apparentnumber rule firings degradation responsiveness. Additionally, general resultschange significantly scenarios executed real-time mode normallyused TacAir-Soar agents. fixed cycle simply eliminates variability.Although patrol scenario designed minimize variation run run,TAS simulator inherently stochastic specific actions taken agenttime course actions varies task repeated. control variation, scenario run lead wing agents approximately 50 times. Loggingdata collection significantly impacted CPU time performance statistics.order control effect, actually ran scenario 99 times, randomly choosingone agent (lead wing) perform logging functions (and discarding performance measures). agent performed logging functions. Data logging agents usedcreate Figure 9. performance measures logging agents recordedconclusion scenario summarized Table 2.379fiWray & LairdLead AgentRulesNumber runs (n)DecisionsOutputsRule FiringsCPU Time (msec)FHJ59143x s.d.89740.0109.1 6.712438 1221683 301x8974142.820641030DHJ53953s.d.0.07.0381.1242Wing AgentFHJDHJ5915395646x s.d.x s.d.89580.089580.01704 42.7869 12.816540 3986321 10412576 8612175 389Table 2: Summary TAS run data.4.4.1 Improving Task DecompositionsTacAir-Soar DHJ agents required extensive knowledge revision. revisionunexpected. instance, unlike Dynamic Blocks World, TAS agents remember manypercepts, last known location enemy aircraft. previously described,assertions remembering must located root level hierarchy, thusrequiring knowledge revision. However, problems discovered.cases, FHJ agents took advantage inconsistency asserted knowledge. words,FHJ agent allowed inconsistency assertions actually dependedinconsistencies apply new knowledge. two major categories knowledge.Within-level consistency knowledge recognized specific inconsistencies (e.g., retractionproposal subtask) trigger actions clean subtaskstate. Complex subtasks allowed non-interruptible execution complex procedureregardless continuing acceptability subtask. cases, agent knowledgemodified remove dependence inconsistency. Appendix providesexplanation original knowledge subsequent changes. Section 4.4.3 summarizeschanges quantitatively.4.4.2 ResultsTable 2 lists average data FHJ DHJ lead wing agents patrol/interceptscenario modifications DHJ agents knowledge base completed.results domain consistent expectations: total knowledge decreases, rulefirings decrease performance improves, substantially DHJ wing agent.following sections explore results greater detail.4.4.3 Knowledge DifferencesTable 3 quantifies changes Soar production rules described above. 7 Modifications include deletions, additions changes. rule considered changedconditions changed slightly, made type computationsubtask. example, changed within-level consistency knowledge refers7. DHJ agent data generated knowledge base included changes accommodatelearning (Wray, 1998) changes included table completeness. presencerules knowledge base negligible impact performance data reported here.380fiAcross-levelConsistencyRememberingWithin-levelConsistencyComplexSubtasksLearningMiscellaneousEnsuring Consistency Hierarchical Execution4403632951021408103380240FHJ Agent:Deletions:Additions:DHJ Agent:Additional Changes:TOTALS591(111)5953965Table 3: Quantitative summary changes production rules FHJ agent knowledgebase DHJ agents.entailed structure rather one created assumption, structurelocated subtask. somewhat restrictive definition change inflatesaddition deletion accounting. many cases production deleted immediately added different subtask. example, productions manipulatemotor commands moved local subtasks highest subtask. Almostadditions deletions Remembering category attributed move,required synthesis new production knowledge.Total knowledge required DHJ agents decreased. approximately 9% reduction achieved making type modification 40% FHJ agentrules, may seem modest gain, given conversion cost. However, costartifact chosen methodology. DHJ agents constructed domainwithout previously existing FHJ agents, least 9% decrease total knowledge wouldexpected. result thus suggests reduction cost agent design.high conversion cost suggest converting much larger system, like TacAir-Soar,would probably costly. hand, modifications made evidentidentifiable regenerations architecture. Thus, 235 total changes made FHJknowledge base much easier make constructing similar number rules.4.4.4 Performance Differencesperformance results Table 2 show, DHJ agents improved performance relativeFHJ peers. However, improvements lead wing agents substantiallydifferent. Differences tasks lead wing pilots led differences relativeimprovements.Lead Wing Agents: lead wing agent share knowledge baseperform different tasks TAS scenario. 8 differences lead differences8. agents share knowledge base dynamically swap roles execution.instance, lead exhausts long-range missiles, order wing take lead role,take role wing itself.381fiWray & Laird2000DHJ LeadDHJ WingFHJ LeadFHJ WingCumulative Outputs15001000500interceptlaunch missilepatrol turnsresume patrol00100200300400500600Time (sec)Figure 11: Cumulative outputs course one ten minute scenario DHJ (black)FHJ (gray) agents. Cumulative outputs lead agents representedsolid lines, wing agents dashed lines.absolute performance. Recall leads primary responsibility fly patrol routeintercept enemy aircraft. hand, wings primary mission role followlead. different tasks require different responses agents.agents overall reasoning activity often correlated output activity;is, commands sends external environment take action it. Figure 11summarizes output activity two pairs lead wing agents (FHJ & DHJ)course ten-minute scenario. output activity leads mostly concentratedplaces course scenario (take-off, intercept, launch-missile,resuming patrol following intercept). wings concentrated outputactivity occurs leads turn new leg patrol wings must followlead 180 degree turn. remainder section, focus DHJagents contrast lead wing agent behavior. discussion performance metricsexamine differences FHJ DHJ leads wings.lead actually spends scenario waiting, short bursts reasoningoutput activity occurring tactically important junctures scenario. patrol,lead flies straight makes decision turn reaches end patrolleg. lead monitors environment searches enemy planes. search382fiEnsuring Consistency Hierarchical Execution(mostly) passive; agents radar notifies agent new entities detected.detecting classifying enemy plane potential threat, lead commitsintercept. lead immediately makes number course, speed, altitude adjustments,based tactical situation. actions evident figure pulse labeledintercept. lead spends time intercept closing distanceaircraft get within weapon range, maneuver little thusrequiring actions environment (thus relatively flat slope following intercept).agent reaches missile range enemy plane, lead executes numberactions quickly. lead steers plane launch window missile, pushesfire button, waits missile clear, determines course maintainradar contact missile flies target (at launch-missile). interceptcompleted, lead resumes patrol task. Again, issues large number outputcommands short period time. examples show leads reasoning focusesprimarily reacting discrete changes tactical situation (patrol leg ended, enemyrange, etc.) behavior generally requires little continuous adjustment.execution wings follow-leader task, hand, requires reactioncontinuous change leads position order maintain formation. Position correctionsrequire observing leads position, recognizing undesired separation formation,responding adjusting speed, course, altitude, etc. wing followinglead throughout scenario, executing position maintenance knowledge almostconstantly. lead flying straight level, patrol leg, wings taskrequire generation many outputs. Figure 11, periods little activityevident periodic flat segments wings cumulative outputs. leadbegins maneuver (e.g., turn), wing must maintain formation throughoutmaneuver. turn wing generates many motor commands follows lead.turn takes seconds complete, outputs increase graduallycourse turn, seen figure. Thus, wing periodically encountersdynamic situation requires significant reasoning motor responses. Further,response change discrete, lead, occurs continuouslycourse leads maneuver.differences tasks two agents account relatively large absolutedifferences performance metrics lead wing agents. wingsadjusting positions relative leads, issue many output commandsleads, requires many inferences determine commandsbe.Decisions: differences decisions lead wing due artifactdata collection. lead agents ran extra second wings halted orderinitiate data collection.Production Firings: lead wing agents, production firings decrease. However, wings production firings decrease 62%, lead, decrease15%. One reason large improvement DHJ wing due eliminationredundant output commands FHJ agents. FHJ wing sometimes issuesmotor command once. reason duplication specific motor command computed locally, thus available subtasks. cases,two subtasks may issue motor command. command stored locally,383fiWray & Lairdcommand may issued agent cannot recognize commandalready issued another subtask. motor commands rememberedtop subtask DHJ agents, inspected subtasks. DHJ wing thusnever issues redundant motor command. large relative decrease outputswing agent FHJ DHJ (Figure 11) attributed improvement. Production firings decrease decrease output activity reasoning activitywing concerns reacting leads maneuvers.contrast wing, leads average number outputs actually increases. Regeneration source additional outputs. situations, DHJ agentssubtask adjusting heading, speed altitude get updated repeatedly highlydynamic situation (e.g., hard turn). FHJ agent uses subtask knowledge decidecurrent output command needs updated. However, DHJ, subtask mayretracted due dependence changing value (e.g., current heading). subtask regenerated following retraction, lead may generate slightly different motorcommand. example, lead might decide turn heading 90.1 instead 90.2 .decision causes generation new output command wouldre-issued FHJ agents accounts small increase outputs. also suggestswithout self-imposed constraint methodology, knowledge base couldmodified avoid regeneration decrease production firings.Although large magnitude improvement wing primarily due remembering motor commands, agents also needed less consistency knowledge thusaccessed less knowledge performing task. agents perform tasksusing less knowledge.CPU Time: CPU time decreases DHJ lead wing agents. improvementlead (39%) half improvement wing (81%). differences dueprimarily decrease production firings. fewer production firings thusfewer instantiations generate, leading improvements CPU time. Match time alsoimproved, contributing overall performance improvement. 9 larger improvementsCPU time compared production firings improvements (39% vs. 15% lead,81% vs. 62% wing) might attributable decreases number rulefirings match time. Again, results offer guarantee match time alwaysdecrease DHJ. important note, however, two different domains DHJreduces total knowledge constrains remaining knowledge. architectureleveraged small differences improved overall performance.4.4.5 Differences ResponsivenessCPU time decreases DHJ agents, responsiveness generally improve.However, agent knowledge split several different subtasks,actions may initiated quickly would initiated FHJ agent.section, explore differences responsiveness one situations.9. Dynamic Blocks World, trends based observations data, rathersignificant analysis. particular, TAS, data number tokens generated collected.results reported consistent expectation token activity falls DHJ agents,compared FHJ agents.384fiEnsuring Consistency Hierarchical ExecutionFHJDHJAvg. In-Range Time(sec)161.816162.048Avg. Launch Time(sec)162.084162.993Reaction Time(sec).268.945n9599Table 4: comparison average reaction times launching missile TAS.enemy plane comes range, agent executes series actions, leadingfiring missile. Reaction time difference time enemyagent comes range time agent actually pushes fire button launchmissile. reaction time one measure agents responsiveness. Table 4 shows,FHJ agent able launch missile quarter second. However,DHJ agent three-and-a-half times slower FHJ agent launchingmissile, taking almost full second, average.Split subtasks, regeneration, subtask selection contribute increasereaction time. Splitting subtask n steps, may executedsingle decision previously, may take n decisions DHJ agent. actionsnecessary launching missile one would expect increase of, most,hundred milliseconds change. However, dividing subtasks separate steps,sequential series actions interrupted. particular, number regenerationsoccur launch-missile subtask agent prepares fire missile highlydynamic situation. agent sometimes chooses undertake similar actionsituation changed enough slightly different action might necessary, describedabove. result DHJ agents taking accurate aim FHJ agents,responding quickly dynamics environment. aiming,however takes time, although increase time tactically significant (i.e.,enemy planes escaping previously hit FHJ agents).additional re-engineering knowledge would improve reaction time (e.g.,described Section 3.4.3). However, decreases responsiveness difficult avoid,general. Dynamic Hierarchical Justification requires subtasks different dependencies initiated terminated separately, risk unnecessary regeneration. However,splitting complex tasks separate subtasks, individual actions delayed subtasks separate procedures, selection particularsubtask series postponed additional subtask choices available.4.5 Summary Empirical EvaluationsFigure 12 summarizes results Dynamic Blocks World TAS. domains,DHJ agents require fewer total productions, suggesting decrease knowledge cost. Performance roughly Dynamic Blocks World lead agents TAS.DHJ wing agents show much greater improvement overall performance, dueDHJ changes knowledge. results suggest Dynamic HierarchicalJustification expected reduce engineering effort degrade performancevariety domains, simple complex. However, response time situations maydecrease.385fiWray & Laird600500Productions400300200FHJ LeadFHJ WingFHJ DBW100DHJ LeadDHJ WingDHJ DBW0012341011121314CPU Time (sec)Figure 12: Mean CPU Time vs. knowledge productions FHJ (black) DHJ (gray)agents Dynamic Blocks World TAS. graph includes actualdistribution CPU time agent well mean agent.Means Dynamic Blocks World agents illustrated squares, TASlead agents triangles, TAS wing agents diamonds.5. DiscussionSolutions KBAC new solutions introduced developedinconsistency problem (Wray, 1998). briefly introduce additional solutions,also consider relationship Dynamic Hierarchical Justification intention reconsideration belief-desire-intention agents belief revision.5.1 solutions inconsistency across hierarchysection, review existing architectural solutions problem inconsistency arising persistence hierarchy assertions.5.1.1 Limiting PersistenceOne obvious approach eliminating inconsistency arising persistence disallowpersistent assumptions altogether. approach adopted Theo (Mitchell et al.,1991). reasoning Theo entailed sensors; perceptual inputs unjustified.Theo cannot reason non-monotonically particular world state; worldchange non-monotonically. Thus, Theo cannot generally remember previous inputs.386fiEnsuring Consistency Hierarchical ExecutionAnother possible limitation would restrict assumptions single memory(global state), or, equivalently, allow assumptions root level hierarchyhierarchical architecture. solution ensures hierarchical context alwaysconsistent (all assertions within associated subtask entailments) also allowspersistence. HTN execution systems RETSINA DECAF, mentionedpreviously, global state, obviously suffer inconsistencyhierarchy. However, interactions persistent assertions new informationderived sensors problem systems global state.RETSINA recently adopted rationale-based monitoring (Veloso, Pollack, & Cox,1998) identify environmental changes could impact currently executing task network (Paolucci et al., 1999). Rationale-based monitoring uses structure plan knowledge (in case, plan operators, including task networks) alleviate inconsistency. Monitors relevant world features created dynamically planning progresses identifyingpre-conditions operators instantiating via straightforward taxonomy monitor types (e.g., monitor quantified conditions). collection monitors form planrationale, reasons support planners decisions (Veloso et al., 1998). Plan rationalesthus similar justifications used truth maintenance. Monitors activatedpre-condition element world changes. inform plannerchange, planner deliberate whether change impactplan construction and, so, consider appropriate repairs.Rationale-based monitoring similar Dynamic Hierarchical Justification, especiallyleverage structures (different) underlying task representations provide consistency. However, two important differences. First,DHJ identifies specific subtask impacted change, require deliberationdetermine impact change; immediate return consistent knowledge statepossible. monitor activated rationale-based monitoring, planner mustfirst determine plan affected, require deliberation. Second,monitors trigger deliberation, rather automatically retracting reasoning,agent using rationale-based monitoring determine plan repairedhow. DHJ (as implemented) offer flexibility; retraction automatic. Automatic retraction assumes cost retrieving (or regenerating) pre-existing plan knowledgeless costly deliberation determine if/how plan revised. planmodification expensive plan generation (Nebel & Koehler, 1995), assumption reasonable. However, invoking deliberate revision process could circumventpotential problems arising recovery interruption (Section 3.4.3).5.1.2 Fixed Hierarchical Justificationmentioned previously, pre-DHJ version Soar Procedural ReasoningSystem (PRS) (Georgeff & Lansky, 1987) use Fixed Hierarchical Justification retractcomplete levels hierarchy support set longer holds. PRS, supportset consists set context elements must hold execution subtask.elements defined knowledge engineer. Fixed Hierarchical Justification offerscomplete solution inconsistency problem context references within reasoningsubtask limited support set. approach guarantees consistency. However,387fiWray & Lairdrequires knowledge designer identify potentially relevant features usedreasoning within subtask. Additionally, resulting system may overly sensitivefeatures support set features rarely impact reasoning, leadingunnecessary regeneration.Fixed Hierarchical Justification requires less explicit consistency knowledge knowledgebased solutions. However, KBAC knowledge still required access whole task hierarchy possible. Thus, agents ability make subtask-specific reactions unexpectedchanges environment limited knowledge designers ability anticipateexplicitly encode consequences changes.5.2 Intention Reconsiderationbelief-desire-intention (BDI) model agency, intention represents commitmentachieving goal (Rao & Georgeff, 1991; Wooldridge, 2000). intention thus similarinstantiation subtask hierarchical architecture.Dynamic Hierarchical Justification viewed partial implementation intention reconsideration (Schut & Wooldridge, 2000, 2001). Intention reconsiderationprocess determining agent abandon intentions (due goal achievement, recognition failure, recognition intention longer desired).Dynamic Hierarchical Justification partial implementation intention reconsideration able capture syntactic features problem solving (i.e.,identification dependencies via support set) determine reconsiderintention. Situations require deliberation determine intentionabandoned captured DHJ.10 Schut & Wooldridge (2001) describe initial attempt allow run-time determination reconsideration policies. optimal policywould maximize likelihood deliberate intention reconsideration actually leadsabandoning intention (i.e., agent reconsiders reconsideration necessary).contrast, Dynamic Hierarchical Justification offers low-cost, always available, domain general process abandoning intentions, cannot automatically identify reconsiderationsrequiring semantic analysis problem state.BDI models, agents choose execute current action plans withoutreconsidering current intentions first. Kinny Georgeff (1991) showed that,static domains, bold agents never reconsider intentions perform effectivelycautious agents always reconsider executing plan step. oppositetrue highly dynamic domains: cautious agents perform bold ones. SoarPRS described cautious via Fixed Hierarchical Justification. is,plan step, architectures determine elements support set remain assertedexecuting step. FHJ approaches are, effect, bold appear,reconsider intentions assertions changed dependentcontext, support set. Dynamic Hierarchical Justification providescautious agents, ensures agents reconsideration function takesaccount context dependencies subtask reasoning. perspective intention10. DHJ preclude deliberate reconsideration. However, Soar (as testbed explorationDHJ) provide architectural solution deliberate reconsideration. Thus, situationsaddressed knowledge deliberative processes architecture.388fiEnsuring Consistency Hierarchical Executionreconsideration, problems introduced dynamic domains prompted us explorecautious solutions.results empirical analysis somewhat consistent Kinny &Georgeff. cautious DHJ agents performed better less cautious FHJ agentshighly dynamic TacAir-Soar domain. Dynamic Blocks World, performancedifferences equivocal. comparison FHJ number new intentionsincreased Dynamic Hierarchical Justification (measured Soar decisions).slight overall performance improvement DHJ, due improvementsmatch time productions, Soar-specific measure likely generalizesystems. results suggest DHJ possibly overly cautious staticdomains. However, Dynamic Hierarchical Justification present significantperformance cost unexpectedly played constructive role agent execution evenstatic domain, DHJ seems warranted static dynamic domains.5.3 Belief RevisionBelief Revision refers process changing beliefs accommodate newly acquiredinformation. inconsistency problem example need revision assertedbeliefs: change hierarchical context (deriving ultimately perceived changesworld) leads situation currently asserted assumption would (necessarily) regenerated re-derived. Theories belief revision identify functionsused update belief set remains consistent.best known theory belief revision AGM theory (Alchourron, Gardenfors,& Makinson, 1985; Gardenfors, 1988, 1992). AGM coherence theory, meaningchanges beliefs determined based mutual coherence one another. approach contrasts foundations approach, justifications (reasons) determinewhen/how revise belief set. Obviously, Dynamic Hierarchical Justification extension foundations approach belief revision. However, foundationscoherence approaches reconciled (Doyle, 1994), section explore repercussions Dynamic Hierarchical Justification context AGM theory beliefrevision.AGM theory, new sentence presented database sentences representingcurrent knowledge state, agent faced task revising knowledge basevia one three processes: expansion (adding sentences knowledge base), contraction(removing sentences knowledge base) revision (a combination expansionscontractions). AGM theory emphasizes making minimal changes knowledge baseepistemic entrenchment, notion usefulness sentence within database.AGM theory prefers sentences high epistemic entrenchment (relativesentences) retained revision.Comparing Dynamic Hierarchical Justification Assumption Justification suggestssometimes cheaper remove subtask (and asserted beliefs associatedsubtask) compute minimal revision Assumption Justification.context belief revision, result surprising, since showncomputing minimal revision knowledge base computationally harderdeduction (Eiter & Gottlob, 1992). theoretical result led applications389fiWray & Lairdcompute belief updates via incremental derivations belief state, rather via beliefrevision (Kurien & Nayak, 2000).power heuristic approach used DHJ analytic solution followscharacteristics outlined Section 3.4.1: hierarchical structure organizationagent assertions efficiency underlying reasoning system regenerateunnecessarily removed assertions. Assumptions (persistent beliefs) associated particular subtasks hierarchical architectures. change perception (an epistemic input)leads revision. Rather determining minimal revision, DHJ uses heuristicthat, context, says persistent beliefs subtask similar epistemic entrenchment subtask/intention itself. cases, heuristic incorrect,leading regeneration, but, correct, provides much simpler mechanism revision. Gardenfors (1988) anticipates conclusions, suggesting systems possessingadditional internal structure (as compared relatively unstructured belief setsAGM theory) may provide additional constraints orderings epistemic entrenchment.6. Conclusionempirical results Dynamic Blocks World TAS domains consistent expectations: knowledge engineering cost decreased overall performanceDHJ roughly (or slightly improved) comparison independently-developedFHJ benchmarks. Development cost decreases designer freed taskcreating across-level consistency knowledge. One drawback DHJ responsivenessdegrade regeneration occurs.DHJ incorporated currently released version Soar (Soar 8)3 years experience users confirms development cost decreases.partly true developers need deeper understanding architecture realizebenefit. However, DHJ removes need encoding across-level consistencyknowledge, proven difficult understand encode many systems. DHJ alsomakes understanding role assumptions Soar systems straightforward, imposing design development constraints. instance, knowledge designer mustthink why, when, persistence used agent. knowledge designer determines functional role persistent assumption, DHJ guidesdevelopment knowledge necessary assumption. nonmonotonic hypothetical assumption, knowledge must created looks outside subtaskorder ensure consistency (i.e., across-level knowledge necessary). Assumptionsremembering must asserted root level hierarchy, knowledge mustcreated manage remembered assumption. Functions root task includemonitoring, updating, removing remembered assumptions (we developing domaingeneral methods managing remembered assumptions reduce cost). Thus,DHJ increase complexity architecture, makes design decisionsexplicit manageable previous KBAC approaches.Regeneration, seemingly one drawbacks DHJ, also contributes decreasedknowledge development costs. Regeneration serves debugging tool, allowing immediatelocalization problem areas domain knowledge (and specific decomposition).debugging aid contrasts previous knowledge development inconsistency390fiEnsuring Consistency Hierarchical Executionoften became evident irrational behavior, making often difficult determineactual source problem. Thus, addition reducing total knowledge necessarytask, Dynamic Hierarchical Justification might also reduce cost per knowledgeunit creating agent knowledge localizing problems via regeneration. However,case domain cannot decomposed nearly decomposable subunits,regeneration could debilitating.Another positive consequence DHJ agent may behave robustly novelsituations anticipated knowledge engineer. example, simple experiment,FHJ DHJ Dynamic Blocks World agents placed situation describedFigure 3. FHJ agent fails block moves lacks knowledge recognizemoving blocks; knowledge designer assumed static domain. knowledge,however, DHJ agent responds situation gracefully. specific situationFigure 3, DHJ agent immediately retracts put-on-table(3) subtask,block-3 table, thus selection subtask longer consistentcurrent situation. agent chooses stack(2,3) decomposes subtaskactions put block-2 block-3. new block (e.g., block-4) placedempty space block-2, architecture responds retracting subtask goalput-down(2) (i.e., subtask contains empty assumption). begins searchempty spaces order continue attempt put block-2 table.architecture, rather agent knowledge, ensures consistency across hierarchy, DHJagents less brittle situations explicitly anticipated agent design.DHJ also provides solution problem learning rules non-contemporaneousconstraints (Wray, Laird, & Jones, 1996). Non-contemporaneous constraints arisetemporally distinct assertions (e.g., red light, green light) collected single learnedrule via knowledge compilation. rule non-contemporaneous constraints leadinappropriate behavior rather never apply. problem makes difficultuse straightforward explanation-based learning approaches operationalize agent executionknowledge. Non-contemporaneous constraints arise architecture creates persistentassumptions become inconsistent hierarchical context (Wray et al., 1996).DHJ never allows inconsistency, solves non-contemporaneous problem.instance, agents Dynamic Blocks World TAS able learnunproblematically new architecture, no/little knowledge re-design. Wray (1998)provides additional details empirical assessment learning.Dynamic Hierarchical Justification operates higher level granularity Assumption Justification knowledge-based solution methods, trading fine-grained consistency lower computational cost. higher level abstraction introduce additional cost execution. particular, necessary regeneration led redundancyknowledge search Dynamic Blocks World TAS agents. Although overall efficiency improved, improvement due improvements averagematch cost productions, cannot guaranteed domains architectures. Further, Dynamic Hierarchical Justification requires complex subtaskssplit distinct subtasks. requirement improves knowledge decompositionreduces regeneration performance reduce responsiveness. However,straightforward compilation reasoning subtasks DHJ enables, reductionresponsiveness overcome learning (Wray, 1998).391fiWray & LairdAlthough implementation evaluation DHJ limited Soar, attemptedreduce specificity results Soar two ways. First, identified problems across-level consistency knowledge introduces knowledge-based approaches:expensive develop, degrades modularity simplicity hierarchical representation, robust knowledge designers imagination. agentsdeveloped sufficiently complex domains, expense creating knowledge growprohibitive. cost may lead additional researchers consider architectural assurancesconsistency. Second, Dynamic Hierarchical Justification gains power via structurehierarchically decomposed tasks. Although specific implementations may differagent architectures, heuristic simplifications employed DHJ transferarchitecture utilizing hierarchical organization memory task decomposition. Dynamic Hierarchical Justification efficient, architectural solution ensures reasoningconsistency across hierarchy agents employing hierarchical task decompositions.solution allows agents act reliably complex, dynamic environmentsfully realizing low cost agent development via hierarchical task decomposition.Acknowledgmentswork would possible without contributed directlydevelopment evaluation Dynamic Hierarchical Justification. Scott Huffman, JohnLaird Mark Portelli implemented Assumption Justification Soar. Ron Chong implemented precursor DHJ. Randy Jones, John Laird, Frank Koss developed TacAirSoar. Sayan Bhattacharyya, Randy Jones, Doug Pearson, Peter Wiemer-Hastings,members Soar group University Michigan contributed development Dynamic Blocks World simulator. anonymous reviewers provided valuable,constructive comments earlier versions manuscript. work supportedpart University Michigan Rackham Graduate School Pre-doctoral fellowship, contract N00014-92-K-2015 Advanced Systems Technology Office DARPANRL, contract N6600I-95-C-6013 Advanced Systems Technology OfficeDARPA Naval Command Ocean Surveillance Center, RDT&E division. Portions work presented 15 th National Conference Artificial IntelligenceMadison, Wisconsin.Appendix A: Improving Task Decompositionsappendix describes detail changes made TAS agent knowledgeDHJ.Remembering: Figure 4 showed agent computing new heading subtaskachieve-proximity subtask. calculation usually depends upon current heading.agent generates command turn, heading changes soon thereafter.situation, DHJ agent must remember already made decision turnnew heading placing assumption reflects new heading top level.places assumption local level, new current heading triggerremoval turn-to-heading regeneration subtask (if agent determinesstill needs turn new heading).392fiEnsuring Consistency Hierarchical ExecutionFHJ agents, output commands (such turn specific heading)asserted assumptions local subtask. DHJ agents knowledge changedissue output commands directly output interface (which, Soar, always parthighest subtask hierarchy). unnecessary regeneration occursagent remembers motor commands generates new one differentoutput necessary. change, course, requires consistency knowledgemotor commands unjustified thus must explicitly removed, trueremembered knowledge DHJ.Within-level Consistency Knowledge: Dynamic Hierarchical Justification, like solutions across-level consistency problem, still requires consistency knowledge withinindividual subtask. knowledge FHJ agents used remove intermediate results execution subtask. clean knowledge allows agentremove local assertions contributed terminating subtask thus avoid(mis)use assertions later reasoning.example, consider achieve-proximity subtask. subtask usednumber different situations agent needs get closer another agent.wing strays far lead, may invoke achieve-proximity get backformation lead. lead uses achieve-proximity get close enough enemyaircraft launch missile. subtask requires many local computations agentreasons heading take get closer another aircraft. specificcomputation depends information available aircraft.wing pursuing lead, may know leads heading thus calculate collisioncourse maximize rate convergence. Sometimes agents headingavailable. case, agent simply moves toward current location agent.local computations stored local subtask. achieve-proximityterminated FHJ agent, agent removes local structure. Removing structureimportant interrupts entailment local structure (e.g., calculationcurrent collision course) guarantees agent decides achieve-proximitydifferent aircraft, supporting data structures properly initialized. knowledgethus maintains consistency local subtask removing local structureachieve-proximity subtask longer selected.FHJ agent could recognize going remove subtask. terminationconditions FHJ agents acted signal within-level consistency knowledge.knowledge removes local structure achieve-proximity summarized as:achieve-proximity operator selected, initiation conditions longer hold,remove local achieve-proximity data structure. Thus, FHJ agent usesrecognition inconsistency assertions trigger activation within-levelconsistency knowledge.subtasks initiating conditions longer supported DHJ agents,selected subtask removed immediately. Thus, DHJ agent never opportunityapply FHJ agents within-level consistency knowledge. failure utilizeknowledge led number problems, including regenerations expected.solve problem, local subtask data structure created entailmentinitiation conditions subtask itself. subtask initiation conditionslonger held, subtask selection local structure immediately removed393fiWray & Lairdarchitecture, requiring additional knowledge. Thus, change obviated needwithin-level consistency knowledge. However, local data structure may needregenerated subtask temporarily displaced. instance, FHJ within-levelconsistency knowledge could determine conditions local structureremoved. DHJ solution lost flexibility.Subtasks Complex Actions: FHJ agents execute number actions rapidsuccession, regardless inconsistency local assertions. single subtask operatorinitiated situation representing conditions apply firstaction sequence, terminated last step sequence applied.intermediate step invalidates initiation conditions, subtask still executesactions.Consider process launching missile. actual missile launch requirespush button, assuming previous steps selecting targetappropriate missile accomplished beforehand. pushing fire button,pilot must fly straight level seconds missile rockets ignite launchmissile flight. missile cleared aircraft, agent supportsmissile keeping radar contact target. FHJ agents, push-fire-buttonsubtask includes act pushing fire button counting missile clearsaircraft. tasks different mutually exclusive dependencies. initiationcondition push-fire-button requires missile already launched. However,subsequent counting requires monitoring newly launched missile.DHJ agents using FHJ knowledge base always remove push-fire-button subtask soon missile perceived air, interrupting complete procedure.Regeneration push-fire-button subtask occurs agent never waitsmissile clear thus never realizes missile launched needs supported.DHJ agent unsuccessfully fires available missiles enemy plane.Pushing fire button waiting missile clear independent taskshappen arise serial order domain. enforced independencecreating new subtask, wait-for-missile-to-clear, dependsnewly launched missile air. DHJ agent pushes fire button, selectswait-for-missile-to-clear count seconds taking action,supports missile clears successfully.solution reduces regeneration improves behavior qualitynon-trivial cost. Whenever subtask split, effects subtask actions longer occurrapid succession within decision. Instead, effect first subtask occurs onedecision, effect second subtask second decision, etc. Thus, solutioncompromise responsiveness.ReferencesAgre, P. E., & Horswill, I. (1997). Lifeworld analysis. Journal Artificial IntelligenceResearch, 6, 111145.Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),510530.394fiEnsuring Consistency Hierarchical ExecutionAllen, J. F. (1991). Time time again. International Journal Intelligent Systems,6 (4), 341355.Altmann, E. M., & Gray, W. D. (2002). Forgetting remember: functional relationshipdecay interference. Psychological Science, 13, 2733.Bresina, J., Drummond, M., & Kedar, S. (1993). Reactive, integrated systems pose newproblems machine learning. Minton, S. (Ed.), Machine Learning MethodsPlanning, pp. 159195. Morgan Kaufmann, San Francisco, CA.Dechter, R. (1990). Enhancement schemes constraint processing: Backjumping, learningcutset decomposition. Artificial Intelligence, 41, 273312.Doyle, J. (1979). truth maintenance system. Artificial Intelligence, 12, 231272.Doyle, J. (1994). Reason maintenance belief revision. Gardenfors, P. (Ed.), BeliefRevision, pp. 2951. Cambridge University Press, Cambridge, UK.Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,updates, counterfactuals. Artificial Intelligence, 57, 227270.Erol, K., Hendler, J., & Nau, D. S. (1994). HTN planning: Complexity expressivity.Proceedings 12th National Conference Artificial Intelligence, pp. 11231128.Firby, R. J. (1987). investigation reactive planning complex domains. Proceedings 6th National Conference Artificial Intelligence, pp. 202206.Forbus, K. D., & deKleer, J. (1993). Building Problem Solvers. MIT Press, Cambridge,MA.Forgy, C. L. (1979). Efficient Implementation Production Systems. Ph.D. thesis,Computer Science Department, Carnegie-Mellon University.Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.MIT Press, Cambridge, MA.Gardenfors, P. (1992). Belief revision. Pettorossi, A. (Ed.), Meta-Programming Logic.Springer-Verlag, Berlin, Germany.Gaschnig, J. (1979). Performance measurement analysis certain search algorithms.Tech. rep. CMU-CS-79-124, Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.Gat, E. (1991a). Integrating planning reacting heterogeneous asynchronous architecture mobile robots. SIGART BULLETIN, 2, 7174.Gat, E. (1991b). Reliable, Goal-directed Control Autonomous Mobile Robots. Ph.D.thesis, Virginia Polytechnic Institute State University, Blacksburg, VA.Georgeff, M., & Lansky, A. L. (1987). Reactive reasoning planning. Proceedings6th National Conference Artificial Intelligence, pp. 677682.Graham, J., & Decker, K. (2000). Towards distributed, environment-centered agent framework. Wooldridge, M., & Lesperance, Y. (Eds.), Lecture Notes Artificial Intelligence: Agent Theories, Architectures, Languages VI (ATAL-99). Springer-Verlag,Berlin.395fiWray & LairdHanks, S., Pollack, M., & Cohen, P. R. (1993). Benchmarks, test beds, controlled experimentation design agent architectures. AI Magazine, 14, 1742.Hayes-Roth, B. (1990). architecture adaptive intelligent systems. WorkshopInnovative Approaches Planning, Scheduling Control, pp. 422432.Jones, R. M., Laird, J. E., Neilsen, P. E., Coulter, K. J., Kenny, P., & Koss, F. V. (1999).Automated intelligent pilots combat flight simulation. AI Magazine, 20 (1), 2741.Kinny, D., & Georgeff, M. (1991). Commitment effectiveness situated agents.Proceedings 12th International Joint Conference Artificial Intelligence, pp.8288.Kurien, J., & Nayak, P. P. (2000). Back future consistency-based trajectorytracking. Proceedings 17th National Conference Artificial Intelligence,pp. 370377.Laird, J. E. (2001). knows going do: Adding anticipation Quakebot.Proceedings 5th International Conference Autonomous Agents, pp. 385392.Laird, J. E., Congdon, C. B., & Coulter, K. J. (1999). Soar users manual version 8.2.Manual, Department Electrical Engineering Computer Science, UniversityMichigan, http://ai.eecs.umiuch.edu/soar/docs.html.Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). Soar: architecture generalintelligence. Artificial Intelligence, 33, 164.Laird, J. E., & Rosenbloom, P. S. (1990). Integrating execution, planning, learningSoar external environments. Proceedings 8 th National ConferenceArtificial Intelligence, pp. 10221029.Laird, J. E., & Rosenbloom, P. S. (1995). evolution Soar cognitive architecture.Steier, D., & Mitchell, T. (Eds.), Mind Matters: Contributions CognitiveComputer Science Honor Allen Newell. Lawrence Erlbaum Associates, Hillsdale,NJ.McDermott, D. (1991). general framework reason maintenance. Artificial Intelligence,50, 289329.Mitchell, T. M., Allen, J., Chalasani, P., Cheng, J., Etzioni, O., Ringuette, M., & Schlimmer,J. C. (1991). Theo: framework self-improving systems. VanLehn, K. (Ed.),Architectures Intelligence, chap. 12, pp. 323355. Lawrence Erlbaum Associates,Hillsdale, NJ.Mitchell, T. M. (1990). Becoming increasingly reactive. Proceedings 8 th NationalConference Artificial Intelligence, pp. 10511058.Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: theoreticalempirical analysis. Artificial Intelligence, 76, 427454.Newell, A. (1990). Unified Theories Cognition. Harvard University Press, Cambridge,MA.396fiEnsuring Consistency Hierarchical ExecutionPaolucci, M., Shehory, O., Sycara, K. P., Kalp, D., & Pannu, A. (1999). planning component RETSINA agents. Wooldridge, M., & Lesperance, Y. (Eds.), Lecture NotesArtificial Intelligence: Agent Theories, Architectures, Languages VI (ATAL99), pp. 147161, Berlin. Springer-Verlag.Pearson, D. J., Huffman, S. B., Willis, M. B., Laird, J. E., & Jones, R. M. (1993).symbolic solution intelligent real-time control. Robotics Autonomous Systems,11, 279291.Rao, A. S., & Georgeff, M. P. (1991). Modeling rational agents within BDI-architecture.Proceedings 2nd International Conference Principles Knowledge Representation Reasoning, pp. 471484.Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentice Hall,Upper Saddle River, NJ.Sacerdoti, E. D. (1975). nonlinear nature plans. Proceedings 4 th International Joint Conference Artificial Intelligence, pp. 206214.Schut, M., & Wooldridge, M. (2000). Intention reconsideration complex environments.Proceedings 4th International Conference Autonomous Agents, pp. 209216.Schut, M., & Wooldridge, M. (2001). Principles intention reconsideration. Proceedings5th International Conference Autonomous Agents, pp. 340347.Shoham, Y. (1993). Agent-oriented programming. Artificial Intelligence, 60 (1), 5192.Simon, H. A. (1969). Sciences Artificial. MIT Press, Cambridge, MA.Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning dependency-directedbacktracking system computer aided circuit analysis. Artificial Intelligence,9 (2), 135196.Sycara, K., Decker, K., Pannu, A., Williamson, M., & Zeng, D. (1996). Distributed intelligent agents. IEEE Expert, 11 (6), 3646.Tambe, M. (1991). Eliminating Combinatorics Production Match. Ph.D. thesis,Carnegie-Mellon University. (Also published Technical Report CMU-CS-91-150,Computer Science Department, Carnegie Mellon University.).Tambe, M., Johnson, W. L., Jones, R. M., Koss, F., Laird, J. E., Rosenbloom, P. S., &Schwamb, K. (1995). Intelligent agents interactive simulation environments. AIMagazine, 16 (1), 1539.Veloso, M. M., Pollack, M. E., & Cox, M. T. (1998). Rationale-based monitoring planning dynamic environments. Proceedings 4 th International ConferenceArtificial Intelligence Planning Systems, pp. 171180.Wilkins, D. E., Myers, K. L., Lowrance, J. D., & Wesley, L. P. (1995). Planning reactinguncertain dynamic environments. Journal Experimental TheoreticalArtificial Intelligence, 7 (1), 197227.Wooldridge, M. (2000). Reasoning Rational Agents. MIT Press, Cambridge, MA.Wray, R. E. (1998). Ensuring Reasoning Consistency Hierarchical Architectures. Ph.D.thesis, University Michigan. Also published University Michigan TechnicalReport CSE-TR-379-98.397fiWray & LairdWray, R. E., & Laird, J. (1998). Maintaining consistency hierarchical reasoning.Proceedings 15th National Conference Artificial Intelligence, pp. 928935.Wray, R. E., Laird, J., & Jones, R. M. (1996). Compilation non-contemporaneous constraints. Proceedings 13th National Conference Artificial Intelligence, pp.771778.Wray, R. E., Laird, J. E., Nuxoll, A., & Jones, R. M. (2002). Intelligent opponents virtualreality trainers. Proceedings Interservice/Industry Training, SimulationEducation Conference (I/ITSEC) 2002.398fiJournal Artificial Intelligence Research 19 (2003) 209-242Submitted 12/02; published 9/03Decision-Theoretic Bidding Based Learned DensityModels Simultaneous, Interacting Auctionspstone@cs.utexas.eduPeter StoneDept. Computer Sciences, University Texas Austin1 University Station C0500, Austin, Texas 78712-1188 USARobert E. Schapireschapire@cs.princeton.eduMichael L. Littmanmlittman@cs.rutgers.eduDepartment Computer Science, Princeton University35 Olden Street, Princeton, NJ 08544 USADept. Computer Science, Rutgers UniversityPiscataway, NJ 08854-8019 USAjanos@pobox.comJanos A. CsirikD. E. Shaw & Co.120 W 45th St, New York, NY 10036 USADavid McAllesterToyota Technological Institute Chicago1427 East 60th Street, Chicago IL, 60637 USAmcallester@tti-chicago.eduAbstractAuctions becoming increasingly popular method transacting business, especially Internet. article presents general approach building autonomousbidding agents bid multiple simultaneous auctions interacting goods. corecomponent approach learns model empirical price dynamics based pastdata uses model analytically calculate, greatest extent possible, optimalbids. introduce new general boosting-based algorithm conditional densityestimation problems kind, i.e., supervised learning problems goalestimate entire conditional distribution real-valued label. approach fullyimplemented ATTac-2001, top-scoring agent second Trading Agent Competition(TAC-01). present experiments demonstrating effectiveness boosting-basedprice predictor relative several reasonable alternatives.1. IntroductionAuctions increasingly popular method transacting business, especiallyInternet. auction single good, straightforward create automated biddingstrategies|an agent could keep bidding reaching target reserve price, couldmonitor auction place winning bid closing time (known sniping).bidding multiple interacting goods simultaneous auctions,hand, agents must able reason uncertainty make complex value assessments. example, agent bidding one's behalf separate auctions cameraash may end buying ash able find affordable camera.Alternatively, bidding good several auctions, may purchase two ashesone needed.c 2003 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiStone, Schapire, Littman, Csirik, & McAllesterarticle makes three main contributions. first contribution general approach building autonomous bidding agents bid multiple simultaneous auctionsinteracting goods. start observation key challenge auctionsprediction eventual prices goods: complete knowledge eventual prices,direct methods determining optimal bids place. guiding principleagent model uncertainty eventual prices and, greatest extent possible,analytically calculate optimal bids.attack price prediction problem, propose machine-learning approach: gatherexamples previous auctions prices paid them, use machine-learning methods predict prices based available features auction. Moreover,strategy, needed able model uncertainty associated predicted prices;words, needed able sample predicted distribution pricesgiven current state game. viewed conditional density estimationproblem, is, supervised learning problem goal estimate entiredistribution real-valued label given description current conditions, typicallyform feature vector. second main contribution article new algorithmsolving general problems based boosting (Freund & Schapire, 1997; Schapire &Singer, 1999).third contribution article complete description prototype implementation approach form, top-scoring agent1 second TradingAgent Competition (TAC-01) held Tampa Bay, FL October 14, 2001 (Wellman, Greenwald, Stone, & Wurman, 2003a). TAC domain main motivationinnovations reported here.builds top(Stone, Littman,Singh, & Kearns, 2001), top-scoring agent TAC-00, introduces fundamentallynew approach creating autonomous bidding agents.present detailsinstantiation underlying principlesbelieve applications wide variety bidding situations.uses predictive, data-driven approach bidding based expected marginal values availablegoods. article, present empirical results demonstrating robustness effectiveness's adaptive strategy. also report's performanceTAC-01 TAC-02 ect key issues raised competitions.remainder article organized follows. Section 2, presentgeneral approach bidding multiple interacting goods simultaneous auctions.Section 3, summarize TAC, substrate domain work. Section 4 describesboosting-based price predictor. Section 5, give details. Section 6,present empirical results including summary's performance TAC01, controlled experiments isolating successful aspects, controlledexperiments illustrating lessons learned competition. discussionsummary related work provided Sections 7 8.ATTac-2001ATTac-2001ATTac-2000ATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-20012. General Approachwide variety decision-theoretic settings, useful able evaluate hypotheticalsituations. computer chess, example, static board evaluator used heuristically1. Top-scoring one metric, second place another.210fiDecision-Theoretic Bidding Learned Density Modelsmeasure player ahead much given board situation. scenariosimilar auction domains, bidding agentuses situation evaluator,analogous static board evaluator, estimates agent's expected profithypothetical future situation. \profit predictor" wide variety uses agent.example, determine value item, agent compares predicted profitassuming item already owned predicted profit assuming itemavailable.Given prices goods, one often compute set purchases allocationmaximizes profit.2 Similarly, closing prices known, treated fixed,optimal bids computed (bid high anything want buy). So, one naturalprofit predictor simply calculate profit optimal purchases fixed predictedprices. (The predicted prices can, course, different different situations, e.g., previousclosing prices relevant predicting future closing prices.)sophisticated approach profit prediction construct model probability distribution possible future prices place bids maximize expectedprofit. approximate solution dicult optimization problem createdstochastically sampling possible prices computing profit predictionsampled price. sampling-based scheme profit prediction important modelinguncertainty value gaining information, i.e., reducing price uncertainty.Section 2.1 formalizes latter approach within simplified sequential auction model.abstraction illustrates decision-making issues full sampling-basedapproach presented Section 2.2. full setting approach addresses considerably complex abstract model, simplifying assumptions allow usfocus core challenge full scenario. guiding principle make decisiontheoretically optimal decisions given profit predictions hypothetical future situations.3ATTac-20012.1 Simplified Abstractionsimple model, n items auctioned sequence (first item 0,item 1, etc.). bidder must place bid r item i, bid, closingprice chosen corresponding item distribution specific item.bid matches exceeds closing price, r , bidder holds item i, h = 1. Otherwise,bidder hold item, h = 0. bidder's utility v(H ) functionfinal vector holdings H = (h0 ; : : : ; h 1 ) cost function holdingsvector closing prices, H . formalize problem optimal bid selectiondevelop series approximations make problem solvable.n2. problem computationally dicult general, solved effectively non-trivialTAC setting (Greenwald & Boyan, 2001; Stone et al., 2001).3. alternative approach would abstractly calculate Bayes-Nash equilibrium (Harsanyi, 1968)game play optimal strategy. dismissed approach intractabilityrealistically complex situations, including TAC. Furthermore, even able approximateequilibrium strategy, reasonable assume opponents would play optimal strategies.Thus, could gain additional advantage tuning approach opponents' actual behaviorobserved earlier rounds, essentially strategy adopted.211fiStone, Schapire, Littman, Csirik, & McAllester2.1.1 Exact Valuevalue auction, is, bidder's expected profit (utility minus cost)bidding optimally rest auction? bidder knows value, makenext bid one maximizes expected profit. value functionbidder's current holdings H current item bid on, i. expressedvalue(i; H ) = max E maxE +1 : : : max E 1 (v (G + H ) G );(1)1+1yiriyirirnyncomponents G new holdings result additional winnings g. Note H non-zero entries items already sold(8j i; H = 0) G non-zero entries items yet sold(8j < i; G = 0). Note also G fully specified g variables(for j i) bound sequentially expectation maximization operators.idea bids r r 1 chosen maximize value contextpossible closing prices .Equation 1 closely related equations defining value finite-horizon partially observable Markov decision process (Papadimitriou & Tsitsiklis, 1987) stochasticsatisfiability expression (Littman, Majercik, & Pitassi, 2001). Like problems,sequential auction problem computationally intractable suciently general representations v() (specifically, linear functions holdings expressive enoughachieve intractability arbitrary nonlinear functions are).jrjjjjjjnj2.1.2 Approximate Value Reorderingthree major sources intractability Equation 1|the alternation maximization expectation operators (allowing decisions conditioned exponentialnumber possible sets holdings), large number maximizations (forcing exponential number decisions considered), large number expectations(resulting sums exponential number variable settings).attack problem interleaved operators moving first maximizations inside expectations, resulting expression approximates value:(2)value-est(i; H ) = max E E +1 : : : E 1 max: : : max(v (G + H ) G ):1+1riyiyiynrirnchoices bids r +1 r 1 appear deeply nested bindingsclosing prices 1, cease bids altogether, instead representdecisions whether purchase goods given prices. Let G = opt(H; i; ) vectorrepresenting optimal number goods purchase prices specified vectorgiven current holdings H starting auction i. Conceptually, computedevaluatingopt(H; i; ) = argmax (v(G + H ) H ):(3)1Thus, Equation 2 written:000value-est(i; H ) = max E1 (v (opt(H ; + 1; ) + H ) opt(H ; + 1; ) ) (4)nngi ;:::;gnriyi ;:::;yn212fiDecision-Theoretic Bidding Learned Density ModelsH 0 identical H except i-th component ects whether item won|r .Note approximation made computing expectedprices (as point values) solving optimization problem. approach correspondsswapping expectations towards core equation:value-est(i; H ) = max(v(opt(H 0 ; + 1; E ) + H 0) opt(H 0; + 1; E ) E ) (5)evriE = E [y +1 ; : : : ; 1], vector expected costs goods. remainderarticle, refer methods use approximation Equation 5expected value approaches reasons come apparent shortly.technique swapping maximization expectation operators previously usedHauskrecht (1997) generate bound solving partially observable Markov decisionprocesses. decrease uncertainty decisions made makes approximationupper bound true value auction: value-est value. tightness approximations Equations 2 5 depends true distributions expected prices.example, prices known advance certainty, approximationsexact.n2.1.3 Approximate BiddingGiven vector costs , optimization problem opt(H; i; ) Equation 4 still NPhard (assuming representation utility function v() suciently complex).many representations v(), optimization problem cast integer linear program approximated using fractional relaxation instead exact optimizationproblem. precisely approach adopted ATTac (Stone et al., 2001).2.1.4 Approximation via SamplingEven assuming opt(H; i; ) solved unit time, literal interpretation Equation 4 says we'll need solve optimization problem exponential number costvectors (or even probability distributions Pr(y ) continuous). Kearns, Mansour, Ng (1999) showed values partially observable Markov decision processescould estimated accurately sampling trajectories instead exactly computing sums.Littman et al. (2001) stochastic satisfiability expressions. Applyingidea Equation 4 leads following algorithm.1. Generate set vectors closing costs according product distributionPr(y ) Pr(y 1).2. samples, calculate opt(H 0; +1; ) defined averageresults, resulting approximationXvalue-est (i; H ) = max (v(opt(H 0; + 1; ) + H 0) opt(H 0 ; + 1; ) )=jS j: (6)jnri2expression converges value-est increasing sample size.remaining challenge evaluating Equation 6 computing real-valued bid rmaximizes value. Note want buy item precisely closing prices213fiStone, Schapire, Littman, Csirik, & McAllestervalue item (minus cost) exceeds value item;maximizes profit. Thus, make positive profit, willing pay to,than, difference value item item.Formally, let H vector current holdings H holdings modifiedect winning item i. Let G (Y ) = opt(H ; i+1; ), optimal set purchases assumingitem won, G(Y ) = opt(H; i+1; ) optimal set purchases assuming otherwise(except cases ambiguity, write simply G G G (Y ) G(Y ) respectively).want select r achieve equivalenceXXr(v(G + H ) G )=jS j (v(G + H ) G )=jS j: (7)wwwwwSettingriw2w=X2([v(G + H )w2] [v(G + H )Gw]) j j(8)G =S:achieves equivalence desired Equation 7, verified substitution,therefore bidding average difference holding holding item maximizesvalue.42.2 Full ApproachLeveraging preceding analysis, define sampling-based approach profitprediction general simultaneous, multi-unit auctions interacting goods. scenario, let n simultaneous, multi-unit auctions interacting goods a0 ; : : : ; 1 .auctions might close different times times not, general, knownadvance bidders. auction closes, let us assume units availabledistributed irrevocably highest bidders, need pay price bidmth highest bidder. scenario corresponds mth price ascending auction.5Note bidder may place multiple bids auction, thereby potentiallywin multiple units. assume auction closes, bidders longeropportunity acquire additional copies goods sold auction (i.e.,aftermarket).approach based upon five assumptions. G = (g0 ; : : : ; g 1 ) 2 , let v(G) 2represent value derived agent owns g units commodity soldauction . Note v independent costs commodities. Noterepresentation allows interacting goods kinds, including complementaritysubstitutability.6 assumptions approach follows:1. Closing prices somewhat, somewhat, predictable. is, given setinput features X , auction , exists sampling rule outputsnnnIR4. Note strategy choosing ri Equation 8 exploit fact sample containsfinite set possibilities yi , might make robust inaccuracies sampling.5. large enough practically ecient + 1st auction. use mthprice model used TAC's hotel auctions.6. Goods considered complementary value package greater sum individualvalues; goods considered substitutable value package less sumindividual values.214fiDecision-Theoretic Bidding Learned Density Modelsclosing price according probability distribution predicted closing pricesa.2. Given vector holdings H = (h0 ; : : : ; h 1 ) h 2 represents quantitycommodity sold auction already owned agent,given vector fixed closing prices = (y0; : : : ; 1), exists tractableprocedure opt(H; ) determine optimal set purchases (g0 ; : : : ; g 1 )g 2 represents number goods purchased auctionv (opt(H; ) + H ) opt(H; ) v (G + H ) GG 2 . procedure corresponds optimization problem opt(H; i; )Equation 3.3. individual agent's bids appreciable effect economy (largepopulation assumption).4. agent free change existing bids auctions yet closed.5. Future decisions made presence complete price information. assumption corresponds operator reordering approximation previoussection.assumptions true general, reasonable enough approximations basis effective strategy.Assumption 3, price predictor generate predicted prices prior consideringone's bids. Thus, sample distributions produce complete sets closingprices goods.good consideration, assume next one close.different auction closes first, revise bids later (Assumption 4). Thus,would like bid exactly good's expected marginal utility us. is, biddifference expected utilities attainable without good. computeexpectations, simply average utilities gooddifferent price samples Equation 8. strategy rests Assumption 5assume bidding good's current expected marginal utility cannot adversely affectfuture actions, instance impacting future space possible bids. Notetime proceeds, price distributions change response observed price trajectories,thus causing agent continually revise bids.Table 1 shows pseudo-code entire algorithm. fully detailed descriptioninstantiation approach given Section 5.nnnn2.3 ExampleConsider camera ash interacting values agent shown Table 2.Further, consider agent estimates camera sell $40 probability25%, $70 probability 50%, $95 probability 25%. Consider questionagent bid ash (in auction a0). decision pertainingcamera would made via similar analysis.215fiStone, Schapire, Littman, Csirik, & McAllesterLet H = (h0 ; : : : ; hn 1 ) agent's current holdings n auctions.= 0 n1 (assume auction next close):{ total-diff = 0{ counter = 0{ time permits:auction aj ; j 6= i, generate predicted price sample yj . Let =(y0 ; : : : ; yi 1 ; 1; yi+1 ; : : : ; yn 1 ).Let Hw = (h0 ; : : : ; hi 1 ; hi + 1; hi+1 ; : : : ; hn 1 ), vector holdings agentwins unit auction ai .Compute Gw = opt(Hw ; ), optimal set purchases agent wins unitauction ai . Note additional units good purchased, sincei-th component 1.Compute G = opt(H; ), optimal set purchases agent never acquiresadditional units auction ai prices set .diff = [v(Gw + H ) Gw ] [v(G + H ) G ]total-diff = total-diff + diffcounter = counter + 1{ r = total-diff=counter{ Bid r auction ai .Table 1: decision-theoretic algorithm bidding simultaneous, multi-unit, interacting auctions.utility$50101000Table 2: table values combination camera ash example.camera aloneash aloneneitherFirst, agent samples distribution possible camera prices. pricecamera (sold auction a1) $70 sample:H = (0; 0); H = (1; 0); = (1; 70)G = opt(H ; ) best set purchases agent make ash,assuming camera costs $70. case, two options buyingcamera not. Buying camera yields profit 100 70 = 30. buyingcamera yields profit 10 0 = 10. Thus, G = (0; 1), [v(G + H ) G ] =v (1; 1) (0; 1) (1; 70) = 100 70.Similarly G = (0; 0) (since ash owned, buying camera yields profit50 70 = 20, buying yields profit 0 0 = 0) [v(G + H ) G ] = 0.wwww216wwfiDecision-Theoretic Bidding Learned Density Modelsval = 30 0 = 30.Similarly, camera predicted cost $40, val = 60 10 = 50; camerapredicted cost $95, val = 10 0 = 10. Thus, expect 50% camera pricesamples suggest ash value $30, 25% lead value $5025% lead value $10. Thus, agent bid :5 30 + :25 50 + :25 10 = $30ash.Notice analysis bid ash, actual closing priceash irrelevant. proper bid depends predicted price camera.determine proper bid camera, similar analysis would done usingpredicted price distribution ash.3. TACinstantiated approach entry second Trading Agent Competition (TAC),described section. Building success TAC-00 held July 2000 (Wellman,Wurman, O'Malley, Bangera, Lin, Reeves, & Walsh, 2001), TAC-01 included 19 agents9 countries (Wellman et al., 2003a). key feature TAC required autonomousbidding agents buy sell multiple interacting goods auctions different types.designed benchmark problem complex rapidly advancing domain emarketplaces, motivating researchers apply unique approaches common task.providing clear-cut objective function, TAC also allows competitors focusattention computational game-theoretic aspects problem leave asidemodeling model validation issues invariably loom large real applicationsautomated agents auctions (see Rothkopf & Harstad, 1994). Another feature TACprovides academic forum open comparison agent bidding strategiescomplex scenario, opposed complex scenarios, trading real stockmarkets, practitioners (understandably) reluctant share technologies.TAC game instance pits eight autonomous bidding agents one another.TAC agent simulated travel agent eight clients, would like travelTACtown Tampa back 5-day period. client characterizedrandom set preferences possible arrival departure dates, hotel rooms,entertainment tickets. satisfy client, agent must construct travel packageclient purchasing airline tickets TACtown securing hotel reservations;possible obtain additional bonuses providing entertainment tickets well. TACagent's score game instance difference sum clients' utilitiespackages receive agent's total expenditure. provide selected detailsgame next; full details design mechanisms TAC serverTAC game, see http://www.sics.se/tac.TAC agents buy ights, hotel rooms entertainment tickets auctions runTAC server University Michigan. game instance lasts 12 minutesincludes total 28 auctions 3 different types.Flights (8 auctions): separate auction type airline ticket: Tampa(in ights) days 1{4 Tampa (out ights) days 2{5. unlimitedsupply airline tickets, every 24{32 seconds ask price changes $10217fiStone, Schapire, Littman, Csirik, & McAllester$x. x increases linearly course game 10 y, 2 [10; 90]chosen uniformly random auction, unknown bidders.cases, tickets priced $150 $800. server receives bidask price, transaction cleared immediately ask priceresale allowed.Hotel Rooms (8): two different types hotel rooms|the Tampa Towers (TT)Shoreline Shanties (SS)|each 16 rooms available days 1{4.rooms sold 16th-price ascending (English) auction, meaning8 types hotel rooms, 16 highest bidders get rooms 16th highestprice. example, 15 bids TT day 2 $300, 2 bids $150,number lower bids, rooms sold $150 15 high bidders plusone $150 bidders (earliest received bid). ask price current 16thhighest bid transactions clear auction closes. Thus, agentsknowledge of, example, current highest bid. New bids must highercurrent ask price. bid withdrawal resale allowed, though price bidsmay lowered provided agent reduce number rooms would winauction close. One randomly chosen hotel auction closes minutes 4{1112-minute game. Ask prices changed minute.Entertainment Tickets (12): Alligator wrestling, amusement park, museum ticketssold days 1{4 continuous double auctions. Here, agents buysell tickets, transactions clearing immediately one agent places buy bidprice least high another agent's sell price. Unlike auctiontypes goods sold centralized stock, agent starts(skewed) random endowment entertainment tickets. prices sent agentsbid-ask spreads, i.e., highest current bid price lowest current ask price(due immediate clears, ask price always greater bid price). case, bidwithdrawal ticket resale permitted. agent gets blocks 4 tickets2 types, 2 tickets another 2 types, tickets 8 types.addition unpredictable market prices, sources variability game instance game instance client profiles assigned agents random initialallotment entertainment tickets. TAC agent eight clients randomly assigned travel preferences. Clients parameters ideal arrival day, IAD (1{4); idealdeparture day, IDD (2{5); hotel premium, HP ($50{$150); entertainment values, EV($0{$200) type entertainment ticket.utility obtained client determined travel package givencombination preferences. obtain non-zero utility, client must assignedfeasible travel package consisting ight arrival day AD, ightdeparture day DD, hotel rooms type (TT SS) days(days AD < DD). one entertainment ticket typeassigned, one day. Given feasible package, client's utilitydefined1000 travelPenalty + hotelBonus + funBonus218fiDecision-Theoretic Bidding Learned Density Models= 100(jAD IAD j + jDD IDD j)hotelBonus = HP client TT, 0 otherwise.funBonus = sum EVs assigned entertainment tickets.TAC agent's score sum clients' utilities optimal allocationgoods (computed TAC server) minus expenditures. client preferences,allocations, resulting utilities sample game shown Tables 3 4.Client IAD IDD HP AW AP MU1 Day 2 Day 5 73 175 34 242 Day 1 Day 3 125 113 124 573 Day 4 Day 5 73 157 12 1774 Day 1 Day 2 102 50 67 495 Day 1 Day 3 75 12 135 1106 Day 2 Day 4 86 197 8 597 Day 1 Day 5 90 56 197 1628 Day 1 Day 3 50 79 92 136Table 3:'s client preferences actual game. AW, AP, MU EVsalligator wrestling, amusement park, museum respectively.travelPenaltyATTac-2001Client AD DD HotelEnt'mentUtility1 Day 2 Day 5 SSAW411752 Day 1 Day 2 TTAW111383 Day 3 Day 5 SSMU3, AW412344 Day 1 Day 2 TTNone11025 Day 1 Day 2 TTAP111106 Day 2 Day 3 TTAW211837 Day 1 Day 5 SS AF2, AW3, MU4 14158 Day 1 Day 2 TTMU11086Table 4:'s client allocations utilities actual gameTable 3. Client 1's \4" \Ent'ment" indicates day 4.ATTac-2001rules TAC-01 largely identical TAC-00, three importantexceptions:1. TAC-00, ight prices tend increase;2. TAC-00, hotel auctions usually closed end game;3. TAC-00, entertainment tickets distributed uniformly agentsrelatively minor surface, changes significantly enriched strategiccomplexity game. Stone Greenwald (2003) detail agent strategies TAC-00.219fiStone, Schapire, Littman, Csirik, & McAllesterTAC-01 organized series four competition phases, culminatingsemifinals finals October 14, 2001 EC-01 conference Tampa, Florida. First,qualifying round, consisting 270 games per agent, served select 16agents would participate semifinals. Second, seeding round, consisting315 games per agent, used divide agents two groups eight.semifinals, morning 14th consisting 11 games group, four teamsgroup selected compete finals afternoon.finals summarized Section 6.TAC designed fully realistic sense agent TACimmediately deployable real world. one thing, unrealistic assumeagent would complete, reliable access clients' utility functions (or evenclient would!); typically, sort preference elicitation procedure would required (e.g.Boutilier, 2002). another, auction mechanisms somewhat contrivedpurposes creating interesting, yet relatively simple game. However, mechanismrepresentative class auctions used real world. dicultimagine future agents need bid decentralized, related, yet varyingauctions similarly complex packages goods.4. Hotel Price Predictiondiscussed earlier, central part strategy depends ability predict prices,particularly hotel prices, various points game. accurately possible,used machine-learning techniques would examine hotel prices actually paidprevious games predict prices future games. section discusses partstrategy detail, including new boosting-based algorithm conditional densityestimation.bound considerable uncertainty regarding hotel prices since dependmany unknown factors, time hotel room close,agents are, kind clients assigned agent, etc. Thus, exactlypredicting price hotel room hopeless. Instead, regard closing pricerandom variable need estimate, conditional current state knowledge(i.e., number minutes remaining game, ask price hotel, ight prices, etc.).might attempt predict variable's conditional expected value. However,strategy requires predict expected value, also ableestimate entire conditional distribution sample hotel prices.set learning problem, gathered set training examplespreviously played games. defined set features describing exampletogether meant comprise snap-shot relevant information availabletime prediction made. features used real valued; couplefeatures special value ? indicating \value unknown." used followingbasic features:number minutes remaining game.price hotel room, i.e., current ask price rooms closedactual selling price rooms closed.220fiDecision-Theoretic Bidding Learned Density Modelsclosing time hotel room. Note feature defined even roomsyet closed, explained below.prices ights.basic list, added number redundant variations, thought might helplearning algorithm:closing price hotel rooms closed (or ? room yet closed).current ask price hotel rooms closed (or ? room alreadyclosed).closing time hotel room minus closing time room whose pricetrying predict.number minutes current time hotel room closes.seeding rounds, impossible know play opponentswere, although information available end game, thereforetraining. semifinals finals, know identities competitors.Therefore, preparation semifinals finals, added following features:number players playing (ordinarily eight, sometimes fewer, instanceone players crashed).bit player indicating whether player participated game.trained specialized predictors predicting price type hotel room.words, one predictor specialized predicting price TT day1, another predicting SS day 2, etc. would seem require eight separatepredictors. However, tournament game naturally symmetric middlesense create equivalent game exchanging hotel rooms days 12 days 4 3 (respectively), exchanging inbound ightsdays 1, 2, 3 4 outbound ights days 5, 4, 3 2 (respectively). Thus,appropriate transformations, outer days (1 4) treated equivalently,likewise inner days (2 3), reducing number specialized predictors half.also created specialized predictors predicting first minute ight pricesquoted prior receiving hotel price information. Thus, total eightspecialized predictors built (for combination TT versus SS, inner versus outerday, first minute versus first minute).trained predictors predict actual closing price room per se,rather much price would increase, i.e., difference closing pricecurrent price. thought might easier quantity predict, and,predictor never outputs negative number trained nonnegative data,approach also ensures never predict closing price current bid.previously played games, able extract many examples.Specifically, minute game room yet closed,221fiStone, Schapire, Littman, Csirik, & McAllesterextracted values features described moment game, plusactual closing price room (which trying predict).Note training, problem extracting closing timesrooms. actual play game, know closing times roomsyet closed. However, know exact probability distribution closingtimes rooms yet closed. Therefore, sample vector hotelprices, first sample according distribution closing times, usepredictor sample hotel prices using sampled closing times.4.1 Learning Algorithmdescribed set learning problem, ready describelearning algorithm used. Brie y, solved learning problem first reducingmulticlass, multi-label classification problem (or alternatively multiple logistic regressionproblem), applying boosting techniques developed Schapire Singer (1999,2000) combined modification boosting algorithms logistic regression proposedCollins, Schapire Singer (2002). result new machine-learning algorithmsolving conditional density estimation problems, described detail remaindersection. Table 5 shows pseudo-code entire algorithm.Abstractly, given pairs (x1 ; y1 ); : : : ; (x ; ) x belongs space XR . case, x 's auction-specific feature vectors describedabove; n, X (R [f?g) . target quantity difference closingprice current price. Given new x, goal estimate conditional distributiongiven x.proceed working assumption training test examples (x; y)i.i.d. (i.e, drawn independently identical distributions). Although assumptionfalse case (since agents, including ours, changing time), seems likereasonable approximation greatly reduces diculty learning task.first step reduce estimation problem classification problem breakingrange 's bins:[b0; b1 ); [b1 ; b2 ); : : : ; [b ; b +1]breakpoints b0 < b1 < < b b +1 problem, chose k = 50.7endpoints b0 b +1 chosen smallest largest values observedtraining. choose remaining breakpoints b1; : : : ; b roughly equalnumber training labels fall bin. (More technically, breakpoints chosenentropy distribution bin frequencies maximized.)breakpoints b (j = 1; : : : ; k), learning algorithm attempts estimateprobability new (given x) least b . Given estimates pb , estimate probability bin [b ; b +1 ) p +1 p (anduse constant density within bin). thus reduced problemone estimating multiple conditional Bernoulli variables corresponding eventnkkkkkkjjjjjjjj7. experiment varying k, expect algorithm sensitive sucientlylarge values k.222fiDecision-Theoretic Bidding Learned Density Models; ym ) xi 2 X , yi 2 Rpositive integers kInput: (x1 ; y1 ); : : : ; (xb0 < b1 < < bk+1= mini yibk+1 = maxi yiPkb1 ; : : : ; bk chosen minimizeq ln qj q0 ; : : : ; qk fraction yi 'sj =0 j[b0 ; b1 ); [b1 ; b2 ); : : : ; [bk ; bk+1 ] (using dynamic programing)Compute breakpoints:b0Boosting:= 1; : : : :1compute weights Wt (i; j ) =1 + esj (yi )ft (xi ;j )sj (y ) Eq. (10)use Wt obtain base function ht : X f1; : : : ; k g ! R minimizingkXXsj (yi )ht (xi ;j )Wt (i; j )edecision rules ht considered. decision rulesi=1 j =1take form. work, use \decision stumps," simple thresholds onefeatures.Output sampling rule:let f =Xhtt=1let f 0 = (f + f )=2( ) = maxff (x; j 0 ) : j j 0 k gf (x; j )= minff (x; j 0 ) : 1 j 0 j gf x; jsample, given x 2 X1let pj =1 + e f (x;j )let p0 = 1; pk+1 = 0choose j 2 f0; : : : ; k g randomly probability pjchoose uniformly random [bj ; bj +1 ]output0pj +1Table 5: boosting-based algorithm conditional density estimation.b , this, use logistic regression algorithm based boosting techniquesdescribed Collins et al. (2002).learning algorithm constructs real-valued function f : X f1; : : : ; kg ! Rinterpretation1(9)1 + exp( f (x; j ))jestimate probability b , given x. negative log likelihoodconditional Bernoulli variable corresponding bjln 1 + e( ) (sj yi f xi ;j223j)fiStone, Schapire, Littman, Csirik, & McAllester(( ) = +11 ifif yy < bb .(10)attempt minimize quantity training examples (x ; ) breakpoints b . Specifically, try find function f minimizingsjjjjXXk=1 j =1ln 1 + e( ) (sj yi f xi ;j) :use boosting-like algorithm described Collins et al. (2002) minimizing objectivefunctions exactly form. Specifically, build function f rounds.round t, add new base function h : X f1; : : : ; kg ! R . Letft=X10 =1ht0accumulating sum. Following Collins, Schapire Singer, construct h ,first let1W (i; j ) =(1+e ) ( )set weights example-breakpoint pairs. choose h minimizesj yi ft xi ;jXX( )k=1 j =1Wt i; j e( ) (sj yi ht xi ;j(11))space \simple" base functions h . work, considered \decisionstumps" h form8>< (x)h(x; j ) =B (x) <>: C (x) =?() one features described above, , , B C real numbers.words, h simply compares one feature threshold returnsvector numbers h(x; ) depends whether (x) unknown (?),. Schapire Singer (2000) show eciently search besth possible choices , , , B C . (We also employed technique\smoothing" , B C .)computed sort iterative procedure, Collins et al. (2002) proveasymptotic convergence f minimum objective function Equation (11)linear combinations base functions. problem, fixed numberrounds = 300. Let f = f +1 final predictor.noted above, given new feature vector x, compute p Equation (9)estimate probability b , let p0 = 1 p +1 = 0.make sense, need p1 p2 p , equivalently, f (x; 1) f (x; 2) f (x; k),condition may hold learned function f . force condition, replacef reasonable (albeit heuristic) approximation f 0 nonincreasing j , namely,jjjjjjjjjjjjjjk224kfiDecision-Theoretic Bidding Learned Density Models= (f + f )=2 f (respectively, f ) pointwise minimum (respectively, maximum)nonincreasing functions g everywhere upper bound f (respectively, lower boundf ).modified function f 0, compute modified probabilities p . samplesingle point according estimated distribution R associated f 0, choosebin [b ; b +1) probability p p +1, select point bin uniformlyrandom. Expected value according distribution easily computedXb +1 + b:(p p +1)2=0Although present results using algorithm trading agent context,test performance general learning problems, comparemethods conditional density estimation, studied Stone (1994).clearly area future research.f0jjjjjkjjjjj5. ATTac-2001described hotel price prediction detail, present remaining details's algorithm. begin brief description goods allocator,used subroutine throughout algorithm. present algorithmtop-down fashion.ATTac-20015.1 Starting Pointcore subproblem TAC agents allocation problem: finding profitableallocation goods clients, G, given set owned goods prices goods.allocation problem corresponds finding opt(H; i; ) Equation 3. denotevalue G (i.e., score one would attain G) v(G ). general allocationproblem NP-complete, equivalent set-packing problem (Garey & Johnson,1979). However solved tractably TAC via integer linear programming (Stoneet al., 2001).solution integer linear program value-maximizing allocation ownedresources clients along list resources need purchased. Using linearprogramming package \LPsolve",usually able find globally optimalsolution 0.01 seconds 650 MHz Pentium II. However, since integer linearprogramming NP-complete problem, inputs lead great deal searchintegrality constraints, therefore significantly longer solution times.v (G ) needed (as opposed G itself), upper bound produced LPsolve priorsearch integrality constraints, known LP relaxation, usedestimate. LP relaxation always generated quickly.Note means possible formulation allocationproblem. Greenwald Boyan (2001) studied fast, heuristic search variant foundperformed extremely well collection large, random allocation problems. Stoneet al. (2001) used randomized greedy strategy fallback cases linearprogram took long solve.ATTac-2001225fiStone, Schapire, Littman, Csirik, & McAllester5.2 OverviewTable 6 shows high-level overviewremainder section.. italicized portions describedATTac-2001first ight quotes posted:Compute G current holdings expected pricesBuy ights G expected cost postponing commitment exceeds expectedbenefit postponing commitmentStarting 1 minute hotel close:Compute G current holdings expected pricesBuy ights G expected cost postponing commitment exceeds expectedbenefit postponing commitment (30 seconds)Bid hotel room expected marginal values given holdings, new ights, expected hotelpurchases (30 seconds)Last minute: Buy remaining ights needed Gparallel (continuously): Buy/sell entertainment tickets based expected valuesTable 6:'s high-level algorithm. italicized portions describedremainder section.ATTac-20015.3 Cost Additional Roomshotel price predictor described Section 4 assumes's bids affectultimate closing price (Assumption 3 Section 2). assumption holds largeeconomy. However TAC, hotel auction involved 8 agents competing 16 hotelrooms. Therefore, actions agent appreciable effect clearing price:hotel rooms agent attempted purchase, higher clearing price wouldbe, things equal. effect needed taken account solvingbasic allocation problem.simplified model usedassumed nth highest bid hotelauction roughly proportional c (over appropriate range n) c 1.Thus, predictor gave price p,used purchasing two hotelrooms (the \fair" share single agent 16 rooms), adjusted pricesquantities rooms using c.example,would consider cost obtaining 4 rooms 4pc2 . Onetwo rooms cost p, 3 cost pc, 4 cost pc2 , 5 cost pc3, etc. total, 2rooms cost 2p, 4 cost 4pc2 . reasoning behind procedurebuys two rooms | fair share given 16 rooms 8 agents, 16thhighest bid ('s 2 bids addition 14 others) sets price.bids additional unit, previous 15th highest bid becomes price-setting bid:price rooms sold goes p pc.constant c calculated data several hundred games seedinground. hotel auction, ratio 14th 18th highest bids (re ectingATTac-2001ATTac-2001nATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-2001226fiDecision-Theoretic Bidding Learned Density Modelsrelevant range n) taken estimate c4, (geometric) meanresulting estimates taken obtain c = 1:35.LP allocator takes price estimates account computing G assigning higher costs larger purchase volumes, thus tending spread'sdemand different hotel auctions., heuristics applied procedure improve stabilityavoid pathological behavior: prices $1 replaced $1 estimating c;c = 1 used purchasing fewer two hotel rooms; hotel rooms dividedearly closing late closing (and cheap expensive) ones, c valuescorresponding subsets auctions seeding rounds used case.ATTac-2001ATTac-20015.4 Hotel Expected Marginal ValuesUsing hotel price prediction module described Section 4, coupled modeleffect economy,equipped determine bids hotel rooms.Every minute, hotel auction still open,assumes auctionclose next computes marginal value hotel room given predictedclosing prices rooms. auction close next, assumeschance revise bids. Since predicted prices representeddistributions possible future prices,samples distributionsaverages marginal values obtain expected marginal value. Using full minuteclosing times computation (or 30 seconds still ights considertoo),divides available time among different open hotel auctionsgenerates many price samples possible hotel room. end,bids expected marginal values rooms.algorithm described precisely explanation Table 7.One additional complication regarding hotel auctions that, contrary oneassumptions Section 2.2 (Assumption 4), bids fully retractable:changed $1 current ask price. case current activebids goodslonger wants less $1 current askprice, may advantageous refrain changing bid hopes askprice surpass them: is, current bid may higher expected valuebest possible new bid. address issue,samples learned pricedistribution find average expected values current potential bids,enters new bid case potential bid better.ATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-20015.5 Expected Cost/Benefit Postponing Commitmentmakes ight bidding decisions based cost-benefit analysis: particular,computes incremental cost postponing bidding particular ight versusvalue delaying commitment. section, describe determinationcost postponing bid.Due diculties compounded sophisticated approaches,used following simple model estimating price ight ticket givenfuture time. evident formulation that|given y|the expected price increasetime 0 time nearly form t2 . also clearATTac-2001ATTac-2001ATTac-2001227fiStone, Schapire, Littman, Csirik, & McAllesterhotel (in order increasing expected price):value ith copy room mean ViRepeat time bound1. Generate random hotel closing order (only open hotels)2. Sample closing prices predicted hotel price distributions3. Given closing prices, compute V0 ; V1 ; : : : VnVi v (G ) owning hotelEstimate v (G ) LP relaxationAssume additional hotel rooms type boughthotels, assume outstanding bids sampled price already owned(i.e., cannot withdrawn).Note V0 V1 : : : Vn : values monotonically increasing sincegoods cannot worse terms possible allocations.Vi1samples.Note V1 V0 V2 V1 : : : Vn Vn 1 : value differences monotonicallydecreasing since additional room assigned client derivevalue it.Bid one room value ith copy room valueleast much current price. Due monotonicity noted step above,matter closing price, desired number rooms price purchased.Table 7: algorithm generating bids hotel rooms.long price hit artificial boundaries $150 $800, constantmust depend linearly 10. linear dependence coecient estimatedseveral hundred ight price evolutions qualifying round. Thus, constantm, expected price increase time time m(T 2 t2 )(y 10).price prediction needed, formula first used first recent actualprice observations obtain guess y, used formulaestimate future price. change predicted formula yielded price decrease.approach suffers systemic biases various kinds (mainly due factvariance price changes gets relatively smaller longer periods time),thought accurate enough use, predict whether ticketexpected get significantly expensive next minutes.practice, TAC-01,started ight-lookahead parameter set3 (i.e., cost postponing average predicted ight costs 1, 2, 3 minutesfuture). However, parameter changed 2 end finals ordercausedelay ight commitments further.ATTac-2001ATTac-20015.5.1 Expected Benefit Postponing CommitmentFundamentally, benefit postponing commitments ights additional information eventual hotel prices becomes known. Thus, benefit postponingcommitment computed sampling possible future price vectors determining,average, much better agent could bought different ight insteadone question. optimal buy ight future scenarios,value delaying commitment ight purchased immediately. However,228fiDecision-Theoretic Bidding Learned Density Modelsmany scenarios ight best one get, purchaselikely delayed.algorithm determining benefit postponing commitment similardetermining marginal value hotel rooms. detailed, explanation,Table 8.Assume we're considering buying n ights given typeRepeat time bound1. Generate random hotel closing order (open hotels)2. Sample closing prices predicted price distributions (open hotels)3. Given closing prices compute V0; V1 ; : : : VV = v (G ) forced buy ightEstimate v(G) LP relaxationAssume ights bought current priceNote V0 V1 : : : V since never worse retain extra exibility.value waiting buy copy mean V V 1 samples.price samples lead conclusion ith ight bought,V = V 1 benefit postponing commitment.Table 8: algorithm generating value postponing ight commitments.nn5.6 Entertainment Expected Valuescore's entertainment-ticket-bidding strategy calculationexpected marginal values ticket. ticket,computes expectedvalue one one fewer ticket. calculations give boundsbid ask prices willing post. actual bid ask prices linear functiontime remaining game:settles smaller smaller profit tickettransactions game goes on. Details functions bid ask price functiongame time ticket value remained unchanged(Stone et al., 2001).Details entertainment-ticket expected marginal utility calculations givenTable 9.ATTac-2001ATTac-2001ATTac-2001ATTac-20006. Resultssection presents empirical results demonstrating effectivenessstrategy. First, summarize performance 2001 2002 Trading Agent Competitions (TACs). summaries provide evidence strategy's overall effectiveness,but, due small number games competitions, anecdotal rather scientifically conclusive. present controlled experiments provide conclusiveevidence utility decision theoretic learning approaches embedded within.ATTac-2001ATTac-2001229fiStone, Schapire, Littman, Csirik, & McAllesterAssume n given ticket type currently ownedRepeat time bound1. Generate random hotel closing order (open hotels)2. Sample closing prices predicted price distributions (open hotels)3. Given closing prices compute V 1; V ; V +1V = v (G ) ticketEstimate v(G) LP relaxationAssume tickets bought soldNote V 1 V V +1 since never worse extra tickets.value buying ticket mean V +1 V samples; valueselling mean V V 1.Since tickets considered sequentially, determined buy sell bid leadsprice would clear according current quotes, assume transaction goescomputing values buying selling ticket types.Table 9: algorithm generating value entertainment tickets.nnnnnnnnnn6.1 TAC-01 Competition19 teams entered qualifying round,one eight agentsmake finals afternoon October 14th, 2001. finals consisted24 games among eight agents. Right beginning, became clear(Fritschi & Dorer, 2002) team beat finals. jumpedearly lead first two games, eight games round,135 points per game ahead closest competitor (& Jennings,2002). 16 games round, 250 points ahead two closestcompetitors ().point,, continually retraining price predictors basedrecent games, began making comeback. time last game played,average 22 points per game behind. thus needed beat514 points final game overtake it, well within margins observedindividual game instances. game completed,'s score 3979 onefirst scores posted server. agents' scores reported oneone,score left. agonizing seconds (at least us),TAC server posted final game score 4626, resulting win.competition, TAC team University Michigan conducted regression analysis effects client profiles agent scores. Using data seedingrounds, determined agents better clients had:1. fewer total preferred travel days;2. higher total entertainment values;3. higher ratio outer days (1 4) inner (2 3) preferred trip intervals.ATTac-2001livingagentsSouthamptonTAC,ATTac-2001whitebearATTac-2001livingagentslivingagentsATTac-2001livingagentslivingagents230fiDecision-Theoretic Bidding Learned Density ModelsBased significant measures, games finals could handicapped basedagents' aggregate client profiles. indicated' clientsmuch easier satisfy, givinghighest handicappedscore. final scores, well handicapped scores, shown Table 10. Completeresults aliations available http://tac.eecs.umich.edu.AgentMean Handicapped score3622415436704094351339313421390933523812307437663253367928593338Table 10: Scores finals. agent played 24 games. Southampton's scoreadversely affected game agent crashed buying manyights hotels, leading loss 3000 points. Discarding gameresults average score 3531.livingagentsATTac-2001ATTac-2001ATTac-2001livingagentswhitebearUrlaub01RetsinaCaiserSoseSouthamptonTACTacsMan6.2 TAC-02 Competitionyear TAC-01 competition,re-entered TAC-02 competitionusing models trained end TAC-01. Specifically, price predictors leftunchanged throughout (no learning). seeding round included 19 agents, playing440 games course 2 weeks.top-scoring agentround, shown Table 11. Scores seeding round weighted emphasizelater results earlier results: scores day n seeding round given weightn. practice designed encourage experimentation early round.ocial ranking competitions based mean score ignoring agent'sworst 10 results allow occasional program crashes network problems.one hand, strikingable finish strongly fieldagents presumably improved course year. hand,agents tuned, better worse,consistentthroughout. particular, toldexperimented approachlater days round, perhaps causing fall lead (by weightedscore) end. 14-game semifinal heat,, restoredlearning capability retrained data 2002 seeding round, finished6th 8 thereby failing reach finals.number possible reasons sudden failure. One relatively mundane explanation agent change computational environmentsseeding rounds finals, may bug computational resourceconstraint introduced. Another possibility due small number gamesATTac-2001ATTac-2001ATTac-2001ATTac-2001SouthamptonTACATTac-2001231fiStone, Schapire, Littman, Csirik, & McAllesterAgentMean Weighted, dropped worst 103050313131003129298031183018309129983055295230002945296627382855Table 11: Top 8 scores seeding round TAC-02. agent played 440 games,worst 10 games ignored computing rankings.ATTac-2001SouthamptonTACUMBCTAClivingagentscuhkThaliswhitebearRoxyBotsemifinals,simply got unlucky respect clients interactionopponent strategies. However, also plausible training data 2002qualifying seeding round data less representative 2002 finalstraining data 2001; and/or competing agents improved significantlyseeding roundremained unchanged. TAC team UniversityMichigan done study price predictors several 2002 TAC agents suggestsbug hypothesis plausible:predictor 2001 outperformspredictors 2002 data 2002 semifinals finals; oneagent uses 2002 data produce good predictions based data (Wellman,Reeves, Lochner, & Vorobeychik, 2003b).8ATTac-2001ATTac-2001ATTac-20016.3 Controlled Experiments's success TAC-01 competition demonstrates effectiveness completesystem. However, since competing agents differed along several dimensions, competition results cannot isolate successful approaches. section, report controlledexperiments designed test ecacy's machine-learning approach priceprediction.ATTac-2001ATTac-20016.3.1 Varying Predictorfirst set experiments, attempted determine quality'shotel price predictions affects performance. end, devised seven price predictionschemes, varying considerably sophistication inspired approaches takenTAC competitors, incorporated schemes agent. playedseven agents one another repeatedly, regular retraining described below.Following seven hotel prediction schemes used, decreasing ordersophistication:ATTac-20018. Indeed, TAC-03 competition, ATTac-2001 entered using trained models 2001,competition, suggesting failure 2002 due problem learnedmodels used finals 2002.232fiDecision-Theoretic Bidding Learned Density Models: \full-strength" agent based boosting usedtournament. (The denotes sampling.): agent samples prices empirical distribution pricespreviously played games, conditioned closing time hotel room (asubset features used). words, collects historicalhotel prices breaks time hotel closed (as wellroom type, usual). price predictor simply samples collectionprices corresponding given closing time.: agent samples prices empirical distribution pricespreviously played games, without regard closing time hotel room (butstill broken room type). uses subset features used.,,: agents predict waycorresponding predictors above, instead returning random sampleestimated distribution hotel prices, deterministically return expectedvalue distribution. (The ev denotes expected value, introduced Section 2.1.): agent uses simple predictor always predicts hotelroom close current price.every case, whenever price predictor returns price current price,replace current price (since prices cannot go down).experiments, added eighth agent, inspiredagent.usedpredict closing prices, determined optimal setpurchases, placed bids goods suciently high prices ensurewould purchased ($1001 hotel rooms,TAC-01) rightfirst ight quotes. never revised bids.agents require training, i.e., data previously played games. However,faced sort \chicken egg" problem: run agents, needfirst train agents using data games involved, getkind data, need first run agents. get around problem, ranagents phases. Phase I, consisted 126 games, used training dataseeding, semifinals finals rounds TAC-01. Phase II, lasting 157 games,retrained agents every six hours using data seeding, semifinalsfinals rounds well games played Phase II. Finally, Phase III, lasting622 games, continued retrain agents every six hours, usingdata games played Phases II, including data seeding,semifinals finals rounds.Table 12 shows agents performed phases. Muchobserve table consistent expectations. sophisticated boostingbased agents () clearly dominated agents based simplerprediction schemes. Moreover, continued training, agents improved markedlyrelative. also see performance simplest agent,,ATTac-2001sCond'lMeansATTac-2001sSimpleMeansCond'lMeansATTac-2001evCond'lMeanevSimpleMeanevCurrentBidEarlyBidderEarlyBidderlivingagentsSimpleMeanevlivingagentsATTac-2001sATTac-2001evEarlyBidderCurrentBid233fiStone, Schapire, Littman, Csirik, & McAllesterAgentATTac-2001evATTac-2001sEarlyBidderSimpleMeanevSimpleMeansCond'lMeanevCond'lMeansCurrentBidPhase105:2 49:527:8 42:1140:3 38:628:8 45:172:0 47:58:6 41:2147:5 35:633:7 52:4Relative ScorePhase II131:6 47:7 (2)86:1 44:7 (3)152:8 43:4 (1)53:9 40:1 (5)71:6 42:8 (6)3:5 37:5 (4)91:4 41:9 (7)157:1 54:8 (8)(2)(3)(1)(5)(7)(4)(8)(6)Phase III166:2 20:8122:3 19:4117:0 18:011:5 21:744:1 18:260:1 19:791:1 17:6198:8 26:0(1)(2)(3)(4)(5)(6)(7)(8)Table 12: average relative scores ( standard deviation) eight agents threephases controlled experiment hotel prediction algorithmvaried. relative score agent score minus average scoreagents game. agent's rank within phase shown parentheses.employ kind training, significantly decline relative data-drivenagents.hand, phenomena table surprisingus. surprising failure sampling help. strategy relies heavilyestimating hotel prices, also taking samples distribution hotelprices. Yet results indicate using expected hotel price, rather price samples,consistently performs better. speculate may insucient numbersamples used (due computational limitations) numbers derivedsamples high variance. Another possibility method usingsamples estimate scores consistently overestimates expected score assumesagent behave perfect knowledge individual sample|a propertyapproximation scheme. Finally, algorithm uses sampling several different points(computing hotel expected values, deciding buy ights, pricing entertainment tickets, etc.), quite possible sampling beneficial decisions detrimentalothers. example, directly comparing versions algorithm samplingused subsets decision points, data suggests sampling hoteldecisions beneficial, sampling ights entertainment tickets neutral best, possibly detrimental. result surprising given samplingapproach motivated primarily task bidding hotels.also surprisedeventually performed worseless sophisticated. One possible explanationsimpler model happens give predictions good complicatedmodel, perhaps closing time terribly informative, perhapsadjustment price based current price significant. things equal,simpler model advantage statistics based price data,regardless closing time, whereas conditional model makes prediction basedeighth data (since eight possible closing times, equally likely).addition agent performance, possible measure inaccuracy eventualpredictions, least non-sampling agents. agents, measured rootCond'lMeansSimpleMeansCond'lMeanevSimpleMeanev234fiDecision-Theoretic Bidding Learned Density Modelsmean squared error predictions made Phase III. were: 56.0,66.6, 69.871.3. Thus, seelower error predictions (according measure), higher score(correlation R = 0:88).ATTac-2001evSimpleMeanev6.3.2ATTac-2001vs.CurrentBidCond'lMeanevEarlyBiddersense, two agents finished top standings TAC-01 representedopposite ends spectrum.agent uses simple open-loop strategy, committing set desired goods right beginning game,usesclosed-loop, adaptive strategy.open-loop strategy relies agents stabilize economy createconsistent final prices. particular, eight agents open loop place highbids goods want, many prices skyrocket, evaporating potentialprofit. Thus, set open-loop agents would tend get negative scores|the open-loopstrategy parasite, manner speaking. Table 13 shows results running 27games 7 copies open-loopone. Although motivated, actuality identicalexcept usesplaces ight hotel bids immediately first ight quotes. bidshotels appear G time. hotel bids $1001.experiments, one copyincluded comparison. price predictorsPhase preceding experiments.'s high bidding strategy backfiresends overpaying significantly goods. experiments indicate,may improve even allowed train games on-goingexperiment well.livingagentsATTac-2001EarlyBidderlivingagentsATTac-2001ATTac-2001SimpleMeanevATTac-2001sEarlyBidderATTac-2001AgentATTac-2001(7)EarlyBidderScore2431 4644880 337Utility8909 2649870 34Table 13: results running7 copiescourse27 games.achieves high utility, overpays significantly, resultinglow scores.ATTac-2001EarlyBidderEarlyBidderopen-loop strategy advantage buying minimal set goods. is,never buys use. hand, susceptible unexpected pricesget stuck paying arbitrarily high prices hotel rooms decidedbuy.Notice Table 13 average utility's clients significantlygreater's clients. Thus, difference score accountedentirely cost goods.ends paying exorbitant prices,generally steers clear expensive hotels. clients' utility suffers,cost-savings well worth it.Compared open-loop strategy,'s strategy relatively stableitself. main drawback changes decision goods wantsEarlyBidderATTac-2001EarlyBidderATTac-2001ATTac-2001235fiStone, Schapire, Littman, Csirik, & McAllestermay also buy goods hedge possible price changes, end getting stuckpaying goods ultimately useless clients.Table 14 shows results 7 copiesplayingone copy. Again, training seeding round finals TAC01: agents don't adapt experiment. Included experiment threevariants, different ight-lookahead parameter (from section\cost postponing ight commitments"). three copies agentsight-lookahead set 2 3 ((2)(3), respectively),oneagent ight-lookahead set 4 ((4)).ATTac-2001EarlyBidderATTac-2001ATTac-2001ATTac-2001ATTac-2001ATTac-2001AgentEarlyBidder(2)(3)ATTac-2001(4)ATTac-2001ATTac-2001Score2869 692614 382570 392494 68Utility10079 559671 329641 329613 55Table 14: results running7 copiescourse 197 games. three different versionsdifferent ight-lookaheads.EarlyBidderslightlyATTac-2001ATTac-2001results Table 14 clearbetter committingight purchases later game ((2) opposed(4)).comparison Table 13, economy represented significantly better overall.is, many copieseconomy cause suffer.However, economy,able invade. gets significantly higher utilityclients pays slightlyagents (as computedutility minus score).9results section suggest variance closing prices largestdetermining factor effectiveness two strategies (assuming nobody elseusing open-loop strategy). speculate large price variances, closedloop strategy () better, small price variances, open-loopstrategy could better.ATTac-2001ATTac-2001ATTac-2001ATTac-2001EarlyBidderATTac-2001ATTac-20017. Discussionopen-loop closed-loop strategies previous section differ handlingprice uctuation. fundamental way taking price uctuation account place\safe bids." high bid exposes agent danger buying somethingridiculously high price. prices fact stable high bids safe. pricesuctuate, high bids, bids stable-price strategy, risky. TAC,hotel rooms sold Vickrey-style nth price action. separate auctionday hotel auctions done sequentially. Although orderauctions randomized, known agent, placing bids one9. suspect agents allowed retrain course experiments, ATTac-2001would end improving, saw Phase III previous set experiments. occur,possible EarlyBidder would longer able invade.236fiDecision-Theoretic Bidding Learned Density Modelsauctions agent assumes auction close next. assumed designagent bids one auction affect prices auctions. assumptionstrictly true, large economy one expects bids single individuallimited effect prices. Furthermore, price affected bid priceitem bid on; effect auctions seems less direct perhapslimited. Assuming bids one auction affect prices another, optimal biddingstrategy standard strategy Vickrey auction|the bid item equalutility bidder. So, place Vickrey-optimal bid, one must able estimateutility item. utility owning item simply expected final scoreassuming one owns item minus expected final score assuming oneitem. So, problem computing Vickrey-optimal bid reduced problempredicting final scores two alternative game situations. use two score predictionprocedures, call stable-price score predictor (corresponding Equation 5)unstable-price score predictor (Equation 4).Stable-Price Score Predictor. stable-price score predictor first estimatesexpected prices rest game using whatever information availablegiven game situation. computes value achieved optimal purchasesestimated prices. economy stable prices, estimate quite accurate|make optimal purchases expected price then, prices nearestimates, performance also near estimated value.Unstable-Price Score Predictor. Stable-price score prediction takeaccount ability agent react changes price game progresses. Suppose given room often cheap sometimes expensive. agent first determineprice room, plan price, agent better guessingprice ahead time sticking purchases dictated price. unstableprice predictor uses model distribution possible prices. repeatedly samplesprices distribution, computes stable-price score prediction sampledprice, takes average stable-price scores various price samples.score prediction algorithm similar algorithm used Ginsberg's (2001) quitesuccessful computer bridge program score predicted sampling possiblehands opponent and, sample, computing score optimal play caseplayers complete information (double dummy play). approachsimple intuitive motivation, clearly imperfect. unstable-price score predictorassumes future decisions made presence complete price information,agent free change existing bids auctions yet closed.assumptions approximately true best. Ways compensatingimperfections score prediction described Section 5.Buy Decide Later. trading agent must decide airline tickets buybuy them. deciding whether buy airline ticket, agent comparepredicted score situation owns airline ticket predicted scoresituation airline ticket may buy later. Airline ticketstend increase price, agent knows certain ticket needed buysoon possible. whether given ticket desirable may dependprice hotel rooms, may become clearer game progresses. airline ticketsincrease price, case TAC-00, bought237fiStone, Schapire, Littman, Csirik, & McAllesterlast possible moment (Stone et al., 2001). determine whether airline ticketbought not, one compare predicted score situation onebought ticket current price predicted score situationprice ticket somewhat higher yet bought. interesting noteone uses stable-price score predictor predictions, ticketpurchased optimal allocation current price estimate, predictedscore buying ticket always higher|increasing price ticketreduce score. However, unstable-price score predictor yield advantagedelaying purchase. advantage comes fact buying ticket mayoptimal prices optimal others. ticket yetbought, score higher sampled prices ticketbought. corresponds intuition certain cases purchasedelayed information available.guiding principle design agent was, greatest extent possible,agent analytically calculate optimal actions. key component calculationsscore predictor, based either single estimated assignment prices modelprobability distribution assignments prices. score predictors, though clearlyimperfect, seem useful. two predictors, unstable-price predictorused quantitatively estimate value postponing decision informationavailable. accuracy price estimation clearly central importance. Future researchundoubtedly focus ways improving price modeling score prediction basedprice modeling.8. Related Future WorkAlthough good deal research auction theory, especiallyperspective auction mechanisms (Klemperer, 1999), studies autonomous bidding agentsinteractions relatively recent. TAC one example. FM97.6another auction test-bed, based fishmarket auctions (Rodriguez-Aguilar, Martin,Noriega, Garcia, & Sierra, 1998). Automatic bidding agents also createddomain (Gimenez-Funes, Godo, Rodriguez-Aguiolar, & Garcia-Calves, 1998).number studies agents bidding single good multiple auctions (Ito,Fukuta, Shintani, & Sycara, 2000; Anthony, Hall, Dang, & Jennings, 2001; Preist, Bartolini,& Phillips, 2001).notable auction-based competition held prior TAC Santa FeDouble Auction Tournament (Rust, Miller, & Palmer, 1992). auction involved severalagents competing single continuous double auction similar TAC entertainmentticket auctions. analyzed Tesauro Das (2001), tournamentparasite strategy that, likedescribed Section 6.3, relied agentsfind stable price took advantage gain advantage. case,advantage gained waiting last minute bid, strategy commonly knownsniping.TAC-01 second iteration Trading Agent Competition. rules TAC01 largely identical TAC-00, three important exceptions:1. TAC-00, ight prices tend increase;livingagents238fiDecision-Theoretic Bidding Learned Density Models2. TAC-00, hotel auctions usually closed end game;3. TAC-00, entertainment tickets distributed uniformly agentsminor surface, differences significantly enriched strategic complexitygame. TAC-00, designers discovered dominant strategydefer serious bidding end game. result, focus solvingallocation problem, agents using greedy, heuristic approach. Sincehotel auctions closed end game, timing issues also important,significant advantages going agents able bid response last-second pricequotes (Stone & Greenwald, 2003). Nonetheless, many techniques developed 2000relevant 2001 competition: agent strategies put forth TAC-00 importantprecursors second year's field, instance pointed Section 5.1.Predicting hotel clearing prices perhaps interesting aspect TAC agentstrategies TAC-01, especially relation TAC-00 last-minute bidding createdessentially sealed-bid auction. indicated experiments described Section 6.3,many possible approaches hotel price estimation problem, approachchosen significant impact agent's performance. Among observedTAC-01 following (Wellman, Greenwald, Stone, & Wurman, 2002), associatedcases price-predictor variant experiments motivated it.1. use current price quote p ().2. Adjust based historic data. example, average historical differenceclearing price price time t, predicted clearing price p + .3. Predict fitting curve sequence ask prices seen current game.4. Predict based closing price data hotel past games (,).5. above, condition hotel closing time, recognizing closingsequence uence relative prices.6. above, condition full ordering hotel closings, hotelsopen closed particular point (,).7. Learn mapping features current game (including current prices) closingprices based historic data (,).8. Hand-construct rules based observations associations abstract features.demonstrated's success bidding simultaneous auctions multiple interacting goods TAC domain, extended approach applyU.S. Federal Communications Commission (FCC) spectrum auctions domain (Weber, 1997).FCC holds spectrum auctions sell radio bandwidth telecommunications companies. Licenses entitle owners use specified radio spectrum band within specifiedgeographical area, market. Typically several licenses auctioned simultaneouslybidders placing independent bids license. recent auction broughtCurrentBidSimpleMeanevCond'lMeanevATTac-2001sATTac-2001239Cond'lMeansATTac-2001evSimpleMeansfiStone, Schapire, Littman, Csirik, & McAllester$16 billion dollars. detailed simulation domain (Csirik, Littman, Singh, &Stone, 2001), discovered novel, successful bidding strategy domain allowsbidders increase profits significantly reasonable default strategy (Reitsma,Stone, Csirik, & Littman, 2002).ongoing research agenda includes applying approach similar domains.particularly expect boosting approach price prediction decision-theoreticreasoning price distributions transfer domains. candidate real-worlddomains include electricity auctions, supply chains, perhaps even travel bookingpublic e-commerce sites.Acknowledgementswork partially supported United States{Israel Binational Science Foundation (BSF), grant number 1999038. Thanks TAC team University Michiganproviding infrastructure support required run many experiments.Thanks Ronggang Yu University Texas Austin running one experiments mentioned article. research conductedauthors AT&T Labs | Research.ReferencesAnthony, P., Hall, W., Dang, V. D., & Jennings, N. R. (2001). Autonomous agentsparticipating multiple on-line auctions. Proceedings IJCAI-2001 WorkshopE-Business Intelligent Web Seattle, WA.Boutilier, C. (2002). pomdp formulation preference elicitation problems. ProceedingsEighteenth National Conference Artificial Intelligence, pp. 239{246.Collins, M., Schapire, R. E., & Singer, Y. (2002). Logistic regression, AdaBoost Bregman distances. Machine Learning, 48 (1/2/3).Csirik, J. A., Littman, M. L., Singh, S., & Stone, P. (2001). FAucS: FCC spectrumauction simulator autonomous bidding agents. Fiege, L., Muhl, G., & Wilhelm,U. (Eds.), Electronic Commerce: Proceedings Second International Workshop,pp. 139{151 Heidelberg, Germany. Springer Verlag.Freund, Y., & Schapire, R. E. (1997). decision-theoretic generalization on-line learningapplication boosting. Journal Computer System Sciences, 55 (1),119{139.Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering successful TACparticipation. First International Joint Conference Autonomous AgentsMulti-Agent Systems Bologna.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: GuideTheory NP-completeness. Freeman, San Francisco, CA.240fiDecision-Theoretic Bidding Learned Density ModelsGimenez-Funes, E., Godo, L., Rodriguez-Aguiolar, J. A., & Garcia-Calves, P. (1998). Designing bidding strategies trading agents electronic auctions. ProceedingsThird International Conference Multi-Agent Systems, pp. 136{143.Ginsberg, M. L. (2001). GIB: Imperfect information computationally challenging game.JAIR, 14, 303{358.Greenwald, A., & Boyan, J. (2001). Bidding algorithms simultaneous auctions.Proceedings Third ACM Conference E-Commerce, pp. 115{124 Tampa, FL.Harsanyi, J. (1967{1968). Games incomplete information played bayesian players.Management Science, 14, 159{182,320{334,486{502.Hauskrecht, M. (1997). Incremental methods computing bounds partially observableMarkov decision processes. Proceedings Fourteenth National ConferenceArtificial Intelligence, pp. 734{739.He, M., & Jennings, N. R. (2002). SouthamptonTAC: Designing successful trading agent.Fifteenth European Conference Artificial Intelligence Lyon, France.Ito, T., Fukuta, N., Shintani, T., & Sycara, K. (2000). Biddingbot: multiagent supportsystem cooperative bidding multiple auctions. Proceedings FourthInternational Conference MultiAgent Systems, pp. 399{400.Kearns, M., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm nearoptimal planning large Markov decision processes. Proceedings SixteenthInternational Joint Conference Artificial Intelligence (IJCAI-99), pp. 1324{1331.Klemperer, P. (1999). Auction theory: guide literature. Journal EconomicSurveys, 13 (3), 227{86.Littman, M. L., Majercik, S. M., & Pitassi, T. (2001). Stochastic Boolean satisfiability.Journal Automated Reasoning, 27 (3), 251{296.Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12 (3), 441{450.Preist, C., Bartolini, C., & Phillips, I. (2001). Algorithm design agents participate multiple simultaneous auctions. Agent Mediated Electronic Commerce III(LNAI), pp. 139{154. Springer-Verlag, Berlin.Reitsma, P. S. A., Stone, P., Csirik, J. A., & Littman, M. L. (2002). Self-enforcing strategicdemand reduction. Agent Mediated Electronic Commerce IV: Designing Mechanisms Systems, Vol. 2531 Lecture Notes Artificial Intelligence, pp. 289{306.Springer Verlag.Rodriguez-Aguilar, J. A., Martin, F. J., Noriega, P., Garcia, P., & Sierra, C. (1998). Towardstest-bed trading agents electronic auction markets. AI Communications, 11 (1),5{19.241fiStone, Schapire, Littman, Csirik, & McAllesterRothkopf, M. H., & Harstad, R. M. (1994). Modeling competitive bidding: critical essay.Management Science, 40 (3), 364{384.Rust, J., Miller, J., & Palmer, R. (1992). Behavior trading automata computerizeddouble auction market. Friedman, D., & Rust, J. (Eds.), Double AuctionMarket: Institutions, Theories, Evidence. Addison-Wesley, Redwood City, CA.Schapire, R. E., & Singer, Y. (1999). Improved boosting algorithms using confidence-ratedpredictions. Machine Learning, 37 (3), 297{336.Schapire, R. E., & Singer, Y. (2000). BoosTexter: boosting-based system text categorization. Machine Learning, 39 (2/3), 135{168.Stone, C. J. (1994). use polynomial splines tensor products multivariatefunction estimation. Annals Statistics, 22 (1), 118{184.Stone, P., & Greenwald, A. (2003). first international trading agent competition:Autonomous bidding agents. Electronic Commerce Research. appear.Stone, P., Littman, M. L., Singh, S., & Kearns, M. (2001). ATTac-2000: adaptiveautonomous bidding agent. Journal Artificial Intelligence Research, 15, 189{206.Tesauro, G., & Das, R. (2001). High-performance bidding agents continuous doubleauction. Third ACM Conference Electronic Commerce, pp. 206{209.Weber, R. J. (1997). Making less: Strategic demand reduction FCCspectrum auctions. Journal Economics Management Strategy, 6 (3), 529{548.Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2002). 2001 trading agentcompetition. Proceedings Fourteenth Innovative Applications ArtificialIntelligence Conference, pp. 935{941.Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003a). 2001 tradingagent competition. Electronic Markets, 13 (1), 4{12.Wellman, M. P., Reeves, D. M., Lochner, K. M., & Vorobeychik, Y. (2003b). Price predictiontrading agent competition. Tech. rep., University Michigan.Wellman, M. P., Wurman, P. R., O'Malley, K., Bangera, R., Lin, S.-d., Reeves, D., & Walsh,W. E. (2001). trading agent competition. IEEE Internet Computing, 5 (2), 43{51.242fiJournal Artificial Intelligence Research 19 (2003) 73-138Submitted 12/02; published 8/03Optimal Schedules Parallelizing Anytime Algorithms:Case Shared ResourcesLev FinkelsteinShaul MarkovitchEhud Rivlinlev@cs.technion.ac.ilshaulm@cs.technion.ac.ilehudr@cs.technion.ac.ilComputer Science DepartmentTechnion - Israel Institute TechnologyHaifa 32000, IsraelAbstractperformance anytime algorithms improved simultaneously solvingseveral instances algorithm-problem pairs. pairs may include different instancesproblem (such starting different initial state), different algorithms (if severalalternatives exist), several runs algorithm (for non-deterministic algorithms).paper present methodology designing optimal scheduling policy basedstatistical characteristics algorithms involved. formally analyze caseprocesses share resources (a single-processor model), provide algorithmoptimal scheduling. analyze, theoretically empirically, behaviorscheduling algorithm various distribution types. Finally, present empirical resultsapplying scheduling algorithm Latin Square problem.1. IntroductionAssume task learn concept predefined success rate, measuredgiven test set. Assume use two alternative learning algorithms, one learnsfast requires preprocessing, another works slowly requirespreprocessing. possibly benefit using learning algorithms parallelsolve one learning task single-processor machine?Another area application constraint satisfaction problems. Assumestudent tries decide two elective courses trying scheduleset compulsory courses. student try solve two setsconstraints sequentially two computations somehow interleaved?Assume crawler searches specific page site. onestarting point, process could speeded simultaneous application crawler(or all) them. However, would optimal strategy bandwidthrestricted?examples common?potential benefits gained uncertainty amountresources required solve one instance algorithmproblem pair. use different algorithms (in first example) differentproblems (in last two examples). non-deterministic algorithms, alsouse different runs algorithm.c2003AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiFinkelstein, Markovitch & Rivlinprocess executed purpose satisfying given goal predicate.task considered accomplished one runs succeeds.goal predicate satisfied time , also satisfied time > .property equivalent utility monotonicity anytime algorithms (Dean &Boddy, 1988; Horvitz, 1987), solution quality restricted Boolean values.objective provide schedule minimizes expected cost, possiblyconstraints (for example, processes may share resources). problem definitiontypical rational-bounded reasoning (Simon, 1982; Russell & Wefald, 1991). problemresembles faced contract algorithms (Russell & Zilberstein, 1991; Zilberstein, 1993).There, given allocated resources, task construct algorithm providingsolution highest quality. case, given quality requirements, taskconstruct algorithm solves problem using minimal resources.several research works deal similar problems. Simple parallelization,information exchange processes, may speed process duehigh diversity solution times. example, Knight (1993) showed using manyreactive agents employing RTA* search (Korf, 1990) beneficial using singledeliberative agent. Another example work Yokoo Kitamura (1996), usedseveral search agents parallel, agent rearrangement preallotted periods time.Janakiram, Agrawal, Mehrotra (1988) showed many common distributionssolution time, simple parallelization leads linear speedup. One exceptionfamily heavy-tailed distributions (Gomes, Selman, & Kautz, 1998) possibleobtain superlinear speedup simple parallelization.superlinear speedup also obtained access internal structureprocesses involved. example, Clearwater, Hogg, Huberman (1992) reportedsuperlinear speedup cryptarithmetic problems result information exchange processes. Another example works Kumar Rao (Rao & Kumar,1987; Kumar & Rao, 1987; Rao & Kumar, 1993), devoted parallelizing standard searchalgorithms, superlinear speedup obtained dividing search space.interesting domain-independent approach based portfolio construction (Huberman, Lukose, & Hogg, 1997; Gomes & Selman, 1997). approach, differentamount resources allotted process. reduce expected resourceconsumption variance.case non-deterministic algorithms, another way benefit solution timediversity restart algorithm attempt switch better trajectory.framework analyzed detail Luby, Sinclair, Zuckerman (1993) casesingle processor Luby Ertel (1994) multiprocessor case. particular,proven single processor, optimal strategy periodically restartalgorithm constant amount time solution found. strategysuccessfully applied combinatorial search problems Gomes, Selman, Kautz (1998).several settings, however, restart strategy optimal.goal schedule number runs single non-deterministic algorithm,number limited due nature problem (for example, robotic search),restart strategy applicable optimal. special case settingsscheduling number runs deterministic algorithm finite set available initial74fiOptimal Schedules Parallelizing Anytime Algorithmsconfigurations (inputs). Finally, case goal schedule set algorithmsdifferent scope restart strategy.goal research develop methodology designing optimal schedulingpolicy number instances algorithm-problem pairs, algorithmseither deterministic non-deterministic. present formal framework scheduling parallel anytime algorithms case processes share resources (a singleprocessor model), based statistical characteristics algorithms involved.framework assumes know probability goal condition satisfiedfunction time (a performance profile (Simon, 1955; Boddy & Dean, 1994) restrictedBoolean quality values). analyze properties optimal schedules suspendresume model, allocation resources performed mutual exclusion basis,show cases extension framework intensity control, resourcesmay allocated simultaneously proportionately multiple demands, yieldbetter schedules. also present algorithm building optimal schedules. Finally,demonstrate experimental results optimal schedules.2. Motivationstarting formal discussion, would like illustrate different schedulingstrategies affect performance system two search processes. first examplesimple setup allows us perform full analysis. second example,show quantitative results real CSP problem.2.1 Scheduling DFS Search ProcessesAssume DFS random tie-breaking applied simple search space shown Figure 1,two runs algorithm allowed 1 . large numberpaths goal, half length 10, quarter length 40, quarterlength 160. one processes finds solution, task consideredaccomplished.104010160B104010160Figure 1: simple search task: two DFS-based agents search path B. Schedulingprocesses may reduce costs.1. limit follow, example, physical constraints, problem robotic search.unlimited number runs optimal results would provided restart strategy.75fiFinkelstein, Markovitch & Rivlinconsider single-processor system, two processes cannot run simultaneously. Let us denote processes 1 A2 , L1 L2 actual path lengthsA1 A2 respectively particular run.application single processes (without loss generality, 1 ) gives us expectedexecution time 1/2 10 + 1/4 40 + 1/4 160 = 55, shown Figure 2.@1/2L1 = 10, cost = 10@@1/2L1 = 40, cost = 401/2@@@t L1 6= 10@@1/2@@@@tL1 = 160, cost = 160Figure 2: Path lengths, probabilities costs running single processimprove performance simulating simultaneous execution two processes. purpose, allow processes expand single node,switch process (without loss generality, 1 starts first). case,expected execution time 1/2 19 + 1/4 20 + 1/8 79 + 1/16 80 + 1/16 319 = 49.3125,shown Figure 3.Finally, know distribution path lengths, allow 1 open 10 nodes;A1 fails, stop allow A2 open 10 nodes; A2 fails well, allowA1 open next 30 nodes, forth. scenario, 1 A2 switch 1040 nodes (if processes fail find solution 40 nodes, guaranteedfound A1 160 nodes). scheme shown Figure 4, expected time1/2 10 + 1/4 20 + 1/8 50 + 1/16 80 + 1/16 200 = 33.75.2.2 Latin Square Exampletask Latin Square problem place N symbols N N squaresymbol appears row column. example shownFigure 5.interesting problem arises square partially filled. problemcase may solvable (see left side Figure 6) unsolvable (see right sideFigure 6). problem satisfiability partially filled Latin Square typicalconstraint-satisfaction problem. consider slight variation task. Let us assumetwo partially filled squares available, need decide whether least onesolvable. assume allocated single processor. attempt speedtime finding solution starting solve two problems two differentinitial configurations parallel.processes employs deterministic heuristic DFS First-Fail heuristic (Gomes & Selman, 1997). consider 10%-filled 20 20 Latin Squares. behaviorsingle process measured set 50,000 randomly generated samples shown76fiOptimal Schedules Parallelizing Anytime Algorithmst0@1/2L1 = 10,L2 10,cost = 19t0 = 0@@t11/2@@@t1@1/2L1 40,L2 = 10,cost = 20@L1 40,L2 10@t1 = 191/2@L1 40,@@t2 L2 40@@ 1/21/2@@L1 = 160,t3@@t3L2 40@@ 1/21/2@L1 = 160,@t4L2 = 40,@@t4cost = 80t2L1 = 40,L2 40,cost = 79t2 = 20t3 = 79L1 = 160,L2 = 160t4 = 801L1 = 160,L2 = 160,cost = 319t5A2A1...012345678910Figure 3: Path lengths, probabilities costs simulating simultaneous execution77t5 = 319fiFinkelstein, Markovitch & Rivlint0@1/2L1 = 10,L2 10,cost = 10t0 = 0@@t11/2@@@t1@1/2L1 40,L2 = 10,cost = 20t2L1 = 40,L2 40,cost = 50@L1 40,L2 10@t1 = 101/2@L1 40,@@t2 L2 40@@ 1/21/2@@L1 = 160,t3@@t3L2 40@@ 1/21/2@L1 = 160,@t4L2 = 40,@@t4cost = 80t2 = 20t3 = 50L1 = 160,L2 = 160t4 = 801L1 = 160,L2 = 160,cost = 200t5t5 = 200A2A1t0t1t2t3t4t5Figure 4: Path lengths, probabilities costs interleaved execution1352424135352414135252413Figure 5: example 5 5 Latin Square.78fiOptimal Schedules Parallelizing Anytime Algorithms1145? 4 5122311 24Figure 6: example solvable (to left) unsolvable (to right) prefilled 5 5 LatinSquares.Figure 7. Figure 7(a) shows probability finding solution function numbersearch steps, Figure 7(b) shows corresponding distribution density. Assume0.80.050.0450.70.040.60.0350.030.4f(t)F(t)0.50.0250.020.30.0150.20.010.103000.005400500600700800900030010004005006007008009001000(a)(b)Figure 7: behavior DFS First-Fail heuristic 10%-filled 20 20 Latin Squares.(a) probability finding solution function number search steps;(b) corresponding distribution density.run limited 25,000 search steps (only 88.6% problems solvablecondition). apply algorithm one available two initial configurations,average number search steps 3777. run two processes parallel (alternatingstep), obtain result 1358 steps. allow single switch optimalpoint (an analogue restart technique (Luby et al., 1993; Gomes et al., 1998) twoprocesses), get 1376 steps average (the optimal point 1311 steps). Finally,interleave processes, switching points corresponding 679, 3072, 10208total steps, average number steps 1177. results averagedtest set 25,000 pairs initial configurations.last sequence switch points optimal schedule process behaviordescribed graphs Figure 7. rest paper present algorithmderiving optimal schedules.79fiFinkelstein, Markovitch & Rivlin3. Framework Parallelization Schedulingsection formalize intuitive description parallelization scheduling. firstpart framework similar framework presented (Finkelstein & Markovitch,2001).Let set states, time variable non-negative real values,random process realization (trajectory) A(t) represents mappingR+ S. Let X0 random variable defined S. Since algorithm Alg startinginitial state S0 corresponds single trajectory (for deterministic algorithms),set trajectories associated distribution (for non-deterministic algorithms),pair hX0 , Algi, X0 stands initial state, viewed random process.Drawing trajectory process corresponds, without loss generality, twostep procedure: first initial state 0 drawn X0 , trajectory A(t) startingS0 drawn Alg. Thus, source randomness either randomnessinitial state, randomness algorithm (which come algorithmenvironment), both.Let designated set states, G : {0, 1} characteristicfunction called goal predicate. behavior trajectory A(t) respectcA (t). saygoal predicate G written G(A(t)), denote GcA (t) non-decreasing functionmonotonic G GcA (t) step functiontrajectory A(t) A. assumptions Gone discontinuity point.Let monotonic G. definitions see behaviorG trajectory A(t) described single point btA,G , first pointcA (t) = 1}. GcA (t) always 0,goal predicate true, i.e, btA,G = inf {t|Gbsay tA,G defined. Therefore, define random variable,trajectory A(t) btA,G defined, corresponds btA,G . behavior variabledescribed distribution function F (t). points F (t) differentiable,use probability density f (t) = F 0 (t).important note practice every trajectory leads goalpredicate satisfaction even infinitely large time. means set trajectories btA,G undefined necessarily measure zero. defineprobability success p probability A(t) btA,G defined2 . LatinSquare example described Section 2.2, probability success 0.886, graphsFigure 7 correspond pF (t) pf (t).Assume system n random processes 1 , . . . corresponding distribution functions F1 , . . . , Fn goal predicates G1 , . . . , Gn . distributionfunctions Fi Fj identical, refer Ai Aj F -equivalent.define schedule system set binary functions { },moment t, i-th process active (t) = 1 idle otherwise. refer schemesuspend-resume scheduling. possible generalization framework extendsuspend/resume control refined mechanism allows us determine2. Another way express possibility process reach goal state use F (t)approach 1 p . prefer use p explicitly distribution function must meetrequirement limt F (t) = 1.80fiOptimal Schedules Parallelizing Anytime Algorithmsintensity process acts. software processes, means varyingfraction CPU utilization; tasks like robot navigation implies changing speedrobots. Mathematically, using intensity control equivalent replacing binaryfunctions (t) continuous functions range zero one 3 .Note scheduling makes term time ambiguous. one hand,subjective time process, consumed process active. kindtime corresponds resource consumed process. hand,objective time measured point view external observer. distributionfunction Fi (t) process defined subjective time, cost function (seebelow) may use kinds times. Since using several processes, formulaspaper based objective time.Let us denote (t) total time process active t.definition,Z(x)dx.(t) =(1)0practice (t) provides mapping objective time subjective timei-th process, refer functions subjective schedule functions. Sinceobtained differentiation, often describe schedules { } instead{i }.processes {Ai } goal predicates {Gi } running schedules {i }Wresultnew process A, goal predicate G. G disjunction G (G(t) = Gi (t)),therefore monotonic G. denote distribution function corresponding random variable Fn (t, 1 , . . . , n ), corresponding distribution densityfn (t, 1 , . . . , n ).Assume given monotonic non-decreasing cost function u(t, 1 , . . . , tn ),depends objective time subjective times per process . also assumeu(0, t1 , . . . , tn ) = 0. Since subjective times calculated (t), actuallyu = u(t, 1 (t), . . . , n (t)).expected cost schedule {i } expressed, therefore, as4Z +Eu (1 , . . . , n ) =u(t, 1 , . . . , n )fn (t, 1 , . . . , n )dt(2)0(for sake readability, omit (t)). suspend-resume model assumptions, must differentiable (except countable set process switch points)derivatives 0 1 would ensure correct values . intensity controlassumptions, derivatives must lie 0 1.consider two alternative setups resource sharing processes:1. processes share resources mutual exclusion basis. means exactlyone process active moment, processes active oneanother goal reached one them. case sum derivatives3. special case setup using constant intensities described Huberman, Lukose,Hogg (1997).4. generalization case probability success p 1 considered endnext section.81fiFinkelstein, Markovitch & Rivlinalways one5 . case shared resources corresponds case severalprocesses running single processor.2. processes fully independent: additional constraints .case corresponds n independent processes running n processors.goal find schedule minimizes expected cost (2) correspondingconstraints. current paper devoted case shared processes. caseindependent resources studied (Finkelstein, Markovitch, & Rivlin, 2002).scheduled algorithms considered framework viewed anytime algorithms. behavior anytime algorithms usually characterized performanceprofile expected quality algorithm output function alloted resources.goal predicate G viewed quality function two possible values, thusdistribution function F (t) meets definition performance profile, time playsrole resource.4. Suspend-Resume Based Schedulingsection consider case suspend-resume based control ( continuousfunctions derivatives 0 1).Claim 1 expressions goal-time distribution F n (t, 1 , . . . , n ) expectedcost Eu (1 , . . . , n ) follows6 :Fn (t, 1 , . . . , n ) = 1Eu (1 , . . . , n ) =Z0+u0t+ni=1nX(1 Fi (i )),(3)n(4)i0 u0ii=1!i=1(1 Fi (i ))dt.Proof: Let ti time would take i-th process meet goal acted alone(if process fails reach goal, consider = ). Let time takessystem n processes reach goal. case, distributed accordingFn (t, 1 , . . . , n ), ti distributed according Fi (t). Thus, processes,given schedule, independent, obtainFn (t, 1 , . . . , n ) = P (t t) = 1 P (t > t) = 1 P (t1 > 1 (t)) . . . P (tn > n (t)) =n(1 Fi (i (t))),1 (1 F1 (1 (t))) . . . (1 Fn (n (t))) = 1i=1corresponds (3). Since F (t) distribution time, assume F (t) = 00.5. fact obvious case suspend-resume control, intensity control reflectedLemma 3.6. u0t u0i stand partial derivatives u respectively.82fiOptimal Schedules Parallelizing Anytime Algorithmsaverage cost function thereforeZ +Eu (1 , . . . , n ) =u(t, 1 , . . . , n )fn (t, 1 , . . . , n )dt =0Z +u(t, 1 , . . . , n )d(1 Fn (t, 1 , . . . , n )) =0u(t, 1 , . . . , n )(1Fn (t, 1 , . . . , n ))|0+Z+0ndu(t, 1 , . . . , n )(1 Fi (i ))dt.dti=1Since u(0, 1 , . . . , n ) = 0 Fn (, 1 , . . . , n ) = 1, first term last expression0. Besides, since full derivative u writtennXdu(t, 1 , . . . , n )i0 u0i ,= u0t +dti=1obtainEu (1 , . . . , n ) =Z+u0t +0nXi=1i0 u0i!ni=1(1 Fi (i ))dt,completes proof.Q.E.D.Note case (t) = Fi (t) = F (t) (parallel application nF -equivalent processes), obtain formula presented (Janakiram et al., 1988), i.e.,Fn (t) = 1 (1 F (t))n .rest section show formal solution (necessary conditions algorithm) framework shared resources. start two processes presentformulas algorithm, generalize solution arbitrary numberprocesses. case two processes, assume u differentiable.elaborated setup n processes, assume total cost linearcombination objective time subjective times, subjective timesweight:nX(t).(5)u(t, 1 , . . . , n ) = + bi=1Since time consumed active process, trivial caseprocesses idle may ignored, obtain (without loss generality)Eu (1 , . . . , n ) =Z0n(1 Fj (j ))dt min .(6)j=1assumption made keep expressions readable. solution processremains general form u.4.1 Necessary Conditions Optimal Solution Two ProcessesLet A1 A2 two processes sharing resource. working, one process locksresource, necessarily idle. show dependency yields83fiFinkelstein, Markovitch & Rivlinstrong constraint behavior process, allowing building effectivealgorithm solving minimization problem.suspend-resume model, therefore, two states system possible:A1 active A2 idle (S1 ); A1 idle A2 active (S2 ). ignorecase processes idle, since removing state scheduleincrease cost. Therefore, system continuously alternates two states:S1 S2 S1 S2 . . .. call time interval corresponding pair hS 1 , S2phase denote phase k k . denote process switch points , phasek corresponds [t2k2 , t2k ]. See Figure 8 illustration.2k3S2k1t2k2t2k1S1S2kt2k t2k+1S1S2k+12k+2S1t2k+3k+2Figure 8: Notations times, states phases two processesscheme, A1 active intervals [t0 , t1 ], [t2 , t3 ], . . . , [t2k , t2k+1 ], . . . ,A2 active intervals [t1 , t2 ], [t3 , t4 ], . . . , [t2k+1 , t2k+2 ], . . . .Let us denote 2k1 total time A1 active t2k1 ,2k total time A2 active t2k . phase definition, 2k1 2kcorrespond cumulative time spent phases 1 k states 1 S2 respectively.exists one-to-one correspondence sequences ti :+ i+1 = ti+1 .(7)Moreover, definition1 (t2k1 ) = 1 (t2k ) = 2k1 ,2 (t2k ) = 2 (t2k+1 ) = 2k .(8)process switch scheme defined above, subjective schedule functions 12 time intervals [t2k , t2k+1 ] (state S1 phase k+1 ) form1 (t) = t2k + 1 (t2k ) = t2k + 2k1 = 2k ,2 (t) = 2 (t2k ) = 2k .(9)Similarly, intervals [t2k+1 , t2k+2 ] (state S2 phase k+1 ), subjective schedulefunctions defined1 (t) = 1 (t2k+1 ) = 2k+1 ,2 (t) = t2k+1 + 2 (t2k+1 ) = t2k+1 + 2k = 2k+1 .Let us denotev(t1 , t2 ) = u0t (t1 + t2 , t1 , t2 ) + u01 (t1 + t2 , t1 , t2 ) + u02 (t1 + t2 , t1 , t2 )84(10)fiOptimal Schedules Parallelizing Anytime Algorithmsvi (t1 , t2 ) = u0t (t1 + t2 , t1 , t2 ) + u0i (t1 + t2 , t1 , t2 ).provide optimal solution suspend/resume model, may split (4) phasesk writeEu (1 , . . . , n ) =ZXt2kk=1 t2k2v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt.(11)last expression may rewrittenEu (1 , . . . , n ) =Z t2k+1Xv(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt+k=0 t2kZ t2k+2Xk=0 t2k+1(12)v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt.Using (9) interval [t2k , t2k+1 ], performing substitution x = 2k , using (7),obtainZt2k+1t2kZ t2k+1v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt =v1 (tt2kZ t2k+1 2kt2k 2kZ 2k+12k12k , 2k )(1 F1 (t 2k ))(1 F2 (2k ))dt =(13)v1 (x, 2k )(1 F1 (x))(1 F2 (2k ))dx =v1 (x, 2k )(1 F1 (x))(1 F2 (2k ))dx.Similarly, interval [t2k+1 , t2k+2 ]Zt2k+2t2k+1Z t2k+2v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt =v2 (2k+1 ,t2k+1Z t2k+2 2k+1t2k+1 2k+1Z 2k+22k2k+1 )(1 F1 (2k+1 ))(1 F2 (t 2k+1 ))dt =v2 (2k+1 , x)(1 F1 (2k+1 ))(1 F2 (x))dx =v2 (2k+1 , x)(1 F1 (2k+1 ))(1 F2 (x))dx.85(14)fiFinkelstein, Markovitch & RivlinSubstituting (13) (14) (12), obtain new form minimization problem:Eu (1 , . . . , n ) ="ZX(1 F2 (2k ))k=0(1 F1 (2k+1 ))Z2k+12k1v1 (x, 2k )(1 F1 (x))dx +2k+22k(15)v2 (2k+1 , x)(1 F2 (x))dx min(for sake generality, assume 1 = 0).minimization problem (15) equivalent original problem (4), dependency solutions described (9) (10). constraint newproblem follows fact processes alternating non-negative periodstime:0 = 0 < 2 . . . 2n . . .(16)1 < 3 . . . 2n+1 . . .expression (15) reaches optimal values eitherdEu= 0 k = 1, . . . , n, . . . ,dk(17)border described (16). However, two processes can, without lossgenerality, ignore border case. Indeed, assume = i+2 > 1 (oneprocesses skips turn). construct new schedule removing i+1 i+2 :1 , . . . , i1 , , i+3 , i+4 , i+5 , . . .easy see process described schedule exactly processdescribed original one, singularity point removed.Thus, step time spent processes determined (17). see2k appears three subsequent terms E u (1 , . . . , n ):. . . + (1 F1 (2k1 ))(1 F2 (2k ))Z2k+1Z2k2k2v2 (2k1 , x)(1 F2 (x))dx+v1 (x, 2k )(12k1Z 2k+2(1 F1 (2k+1 ))2kF1 (x))dx+v2 (2k+1 , x)(1 F2 (x))dx + . . . .86fiOptimal Schedules Parallelizing Anytime AlgorithmsDifferentiating (15) 2k , therefore, yieldsdEu= v2 (2k1 , 2k )(1 F1 (2k1 ))(1 F2 (2k ))d2kZ 2k+1f2 (2k )v1 (x, 2k )(1 F1 (x))dx+2k1(1 F2 (2k ))Z2k+12k1v1(x, 2k )(1 F1 (x))dxt2v2 (2k+1 , 2k )(1 F1 (2k+1 ))(1 F2 (2k )) =(1 F2 (2k ))(v2 (2k1 , 2k )(1 F1 (2k1 )) v2 (2k+1 , 2k )(1 F1 (2k+1 ))Z 2k+1f2 (2k )v1 (x, 2k )(1 F1 (x))dx+2k1(1 F2 (2k ))Z2k+12k1v1(x, 2k )(1 F1 (x))dx.t2similar expression derived differentiating (15) 2k+1 . Combiningexpressions (17) gives us following theorem:Theorem 1 (The chain theorem two processes)value i+1 2 computed given i1 using formulasf2 (2k )v2 (2k1 , 2k )(1 F1 (2k1 )) v2 (2k+1 , 2k )(1 F1 (2k+1 ))+=R 2k+11 F2 (2k )2k1 v1 (x, 2k )(1 F1 (x))dxR 2k+1 v12k1 t2 (x, 2k )(1 F1 (x))dx, = 2k + 1,R 2k+12k1 v1 (x, 2k )(1 F1 (x))dxv1 (2k , 2k+1 )(1 F2 (2k )) v1 (2k+2 , 2k+1 )(1 F2 (2k+2 ))f1 (2k+1 )+=R 2k+21 F1 (2k+1 )v(,x)(1F(x))dx222k+12kR 2k+2 v22kt1 (2k+1 , x)(1 F2 (x))dx, = 2k + 2.R 2k+2v(,x)(1F(x))dx222k+12k(18)(19)Corollary 1 linear cost function (5), value i+1 2 computedgiven i1 using formulasf2 (2k )F1 (2k+1 ) F1 (2k1 ), = 2k + 1,= R2k+11 F2 (2k )(1 F1 (x))dx(20)2k1f1 (2k+1 )F2 (2k+2 ) F2 (2k ), = 2k + 2.= R2k+21 F1 (2k+1 )(1 F2 (x))dx(21)2kproof follows immediately fact v (t1 , t2 ) = + b.Theorem 1 allows us formulate algorithm building optimal solution.algorithm presented next subsection.87fiFinkelstein, Markovitch & Rivlin4.2 Optimal Solution Two Processes: Algorithmgoal scheduling algorithm minimize expression (15)Eu (1 , . . . , n ) ="ZX(1 F2 (2k ))k=0(1 F1 (2k+1 ))constraintsZ2k+12k1v1 (x, 2k )(1 F1 (x))dx +2k+22kv2 (2k+1 , x)(1 F2 (x))dx min0 = 0 < 2 . . . 2n . . .1 < 3 . . . 2n+1 . . . .Assume A1 acts first (1 > 0). Theorem 1 see values0 = 0 1 determine set possible values 2 , values 1 2 determinepossible values 3 , on.Therefore, non-zero value 1 provides us tree possible values k .branching factor tree determined number roots (18) (19).possible sequence 1 , 2 , . . . evaluated using (15).cases total time limited discussed Section 4.5,series expression converge, e.g., process finite cost findingsolution, algorithm stops finite number points. cases, however,extremely heavy-tailed distributions, possible series diverge.ensure finite number iterations cases, set upper limit maximalexpected cost.Another limit added probability failure. Since = i1 + , probabilityruns would able find solution(1 F1 (i1 ))(1 F2 (i )).Therefore, difference(1 F1 (i1 ))(1 F2 (i )) (1 p1 )(1 p2 )becomes small enough, conclude runs failed find solution stopexecution.value 1 find best sequence using one standard searchalgorithms, Branch-and-Bound. Let us denote value best sequence1 Eu (1 ). Performing global optimization E u (1 ) 1 provides usoptimal solution case 1 acts first. Note value 1 may also 0(A2 acts first), need compare value obtained optimization 1value obtained optimization 2 1 = 0.flow algorithm illustrated Figure 9, formal scheme presentedFigure 10, description main routine (realized DFS Branch Boundmethod) Figure 11.88fiOptimal Schedules Parallelizing Anytime Algorithmsalgorithm considers two main branches, one 1 one A2 ,processed procedure minimize sequence f irst point (Figure 10). step,initialize array values, pass it, procedure build optimal sequence,recursive procedure df sbnb, represents core algorithm (Figure 11).df sbnb procedure, shown Figure 11, acts follows. obtains inputarray values, cost involved current moment, best value reached tillnow. cost exceeds value, procedure performs classical Branch-and-Boundcutoff (lines 1-2).inner loop (lines 4-19) corresponds different roots expressions (18)(19). new value corresponding k calculated procedurecalculate next zeta (line 5), cannot exceed previously found root savedlast zeta (for first iteration, last zeta initialized k2 ), lines 3 8. Lines 67 correspond case lower bound passed calculate next zeta exceedsmaximal available time, case procedure stopped.new possible value found, procedure updates current cost (line9), stopping criteria mentioned validated new array values,denoted concatenation old array new value (line 10).task accomplished, cost verified versus best known value (which updatednecessary), procedure returns (lines 10-16). Otherwise, temporarily addedarray , Branch-and-Bound procedure called recursively calculationk+1 .whole tree traversed (except cutoffs), best known cost returned(line 20). corresponding array required solution.Figure 13 shows trace single Branch-and-Bound run example shownSection 2.2 starting optimal value 1 . optimal schedule derivedrun 679, 2393, 7815, 17184 expected cost 1216.49 steps. scheduling pointsgiven subjective times. Using objective (total) time schedule written 679,3072, 10208, 25000. particular run Branch-and-Bound cutoffs duesmall number roots (18) (19).4.3 Necessary Conditions Optimal Solution n Processessection generalize solution case two processes case nprocesses.Assume n processes A1 , . . . , using shared resources. One possibleways present schedule use sequenceh(Ai1 , t1 ), (Ai2 , t2 ), . . . , (Aij , tj ), . . .i,Aij j-th active process, tj time allocated invocation ij .simplify formalization problem, however, use following alternativerepresentation. First, allow j 0, makes possible represent everyscheduleh(A1 , t1 ), (A2 , t2 ), . . . , (An , tn ), (A1 , tn+1 ), (A2 , tn+2 ), . . . , (An , t2n ), . . .i.89fiFinkelstein, Markovitch & Rivlin+xQGet optimal schedule costsQ 1 = 0 1 6= 0,QQreturnbest valueQQcorrespondingscheduleQQA1 acts firstQQQA2 acts first??...Minimization 1PQPQPPPPPQ1 trialsQPP)qPQQ+Q/wh?JJJJ^Jh/=hhx?JJJJJ^hxhxx?.??Jh@JBJB@JB @@JB@J^/R@??JBNminimization procedureRoot Branch Bound tree2 satisfying (19) k = 0Branch Bound nodes3 satisfying (18) k = 1Branch Bound nodes4 satisfying (19) k = 1.....hBranch Bound non-leaf nodesxLeaf nodes (terminating condition satisfied) cutoff nodes (expected resultworse already known). cost calculated accordance (15).Figure 9: flow algorithm constructing optimal schedules 2 processes90fiOptimal Schedules Parallelizing Anytime Algorithmsprocedure optimizeInput: F1 (t), F2 (t) (performance profiles).Output: optimal sequence value.[sequence1 , val1 ] minimize sequence f irst point(A 1 )[sequence2 , val2 ] minimize sequence f irst point(A 2 )val1 < val2return [sequence1 , val1 ]elsereturn [sequence2 , val2 ]endendprocedure minimize sequence f irst point(process)zetas[1] 0zetas[0] 0process = A2zetas[1] 0endUsing one standard minimization methods, find zetas,minimizing value function build optimal sequence(zetas),corresponding cost.endFigure 10: Procedure optimize builds optimal sequence case 1 starts, optimal sequence case A2 starts, compares results, returns bestone. Procedure minimize sequence f irst point returns optimal sequencevalue.91fiFinkelstein, Markovitch & Rivlinprocedure build optimal sequence(zetas)curr cost calculate cost(zetas)return df sbnb(zetas, curr cost, AX V ALU E)endprocedure df sbnb(zetas, curr cost, thresh)1:(curr cost thresh)// Cutoff2:return AX V ALU E3:last value zetas[length(zetas) 2]// previous time value4:repeat5:calculate next zeta(zetas, last value)6:( = last value)// Skip7:return thresh8:last value9:delta cost calculate partial cost(zetas, )10:(task accomplished([zetas || ]))// Leaf11:(curr cost + delta cost < thresh)12:optimal zetas [zetas || ]13:thresh curr cost + delta cost14:end15:return thresh16:end17:tmp result df sbnb([zetas || ], curr cost + delta cost, thresh)18:thresh = min(thresh, tmp result)19: end// repeat20: return threshendFigure 11: Procedure build optimal sequence, given prefix time sequence, restoresoptimal sequence prefix using DFS Branch Bound search algorithm,returns sequence value. [x || y] stands concatenation x y.Auxiliary functions shown Figure 12.92fiOptimal Schedules Parallelizing Anytime Algorithms1. calculate cost(zetas) computes cost sequence (or part) accordance(15),2. calculate partial cost(zetas, ) computes additional cost obtained addingsequence,3. calculate next zeta(zetas, last value) uses (18) (19) calculate valuenext greater last value. solution exists,maximal time value returned,4. task accomplished(zetas) returns true task may consideredaccomplished (e.g., either maximal possible time over, probabilityerror negligible, upper limit cost exceeded).Figure 12: Auxiliary functions used optimal schedule algorithm1679.02379.4324620.62393.07815.4u=2664.54424321.0u=1534.0622607.0u=1265.6717184.6u=1216.49Figure 13: trace single run Branch-and-Bound procedure starting optimalvalue 1 .93fiFinkelstein, Markovitch & RivlinTherefore, system alternates n states 1 S2 . . . Sn S1 . . .,state Si corresponds situation active rest processesidle. time spent k-th invocation tkn+i .case two processes, call time interval corresponding sequencestates S1 S2 . . . Sn phase denote phase k k . denote processswitch points k t1k , t2k , . . . , tnk ,tik=k1Xtnj+i.j=00Process Ai active phase k interval [t i1k , tk ], entire phase lasts ktnk . corresponding scheme shown Figure 14.n1tk1Snk1tn = t0kk1S1t1skS2t2sk...ktn1tn = t0k+1 t1k+1skskSnS1k+1Figure 14: Notations times, states phases n processessimplify following discussion, would like allow indices ik less0 greater n. purpose, denotemod n,tik = tik+bi/nc(22)index process active interval [t i1k , tk ] denote #i. mod n 6=0 obtain #i = mod n, mod n = 0 #i = n. Notation (22) claimsshift n upper index equivalent shift 1 phase number:ti+n= tik+1 .kcase two processes, denote ki total time A#i activetik . ki corresponds cumulative time spent phases 1 k state #i ,one-to-one correspondence sequences ki tik :ki k1= tik ti1k ,n1Xj=0kij = tik n.(23)(24)first equation corresponds fact time i1tik accumulatedkvalues process A#i , second equation claims switchobjective time system equal sum subjective times process.sake uniformity also denoten1= . . . = 1= 00 = 0.194fiOptimal Schedules Parallelizing Anytime Algorithmsconstruction ki see, time interval [ti1k , tk ] subjective time processAj following form:jj = 1, . . . , 1,k ,i1(t tk ) + k1 , j = i,(25)j (t) =jk1 ,j = + 1, . . . , n.subjective time functions system 3 processes illustrated Figure 15.6 (t)1 (t)21222312#11########2 (t)3 (t)13t01t11t21t31t12t22t32Figure 15: Subjective time functions system 3 processesfind optimal schedule system n processes, need minimizeexpression given (6). constraints monotonicity sequenceprocess i:ki k+1k, i.(26)Given expressions j , prove following lemma:Lemma 1 system n processes, expression expected cost (6)rewrittenZn i+n1XXkj(1 Fi (x))dx.(27)(1 F#j (k1 ))Eu (1 , . . . , n , . . .) =k1k=0 i=1 j=i+1proof given Appendix A.1.lemma makes possible prove chain theorem arbitrary numberprocesses:95fiFinkelstein, Markovitch & Rivlinl1l1 ,Theorem 2 (The chain theorem) value m+1may eithercomputed given previous 2n 2 values using formula1l )fl (ml )Fl (m=l+n1j=l+1l1Xj(1 F#j (m1))i+n1i=ln+1 j=i+1#j6=lj(1 F#j (m))l+n1j=l+1Zj(1 F#j (m))m+1(28)(1 F#i (x))dxproof theorem given Appendix A.2.4.4 Optimal Solution n Processes: Algorithmgoal presented algorithm minimize expression (27)Eu (1 , . . . , n , . . .) =n i+n1XXk=0 i=1 j=i+1(1jF#j (k1))Zkik1(1 Fi (x))dxconstraintski k+1k, i.case two processes, assume 1 acts first. Theorem 2, given 2n 2values10 , 11 , . . . , 1n , 21 , 22 , 2n3 ,determine possibilities value 2n2 (either 1n2 process skipsturn, one roots (28)). Given values 2n2 , determinevalues 2n1 , on.idea algorithm similar algorithm two processes. first 2n 2variables (including 10 = 0) determine tree possible values . Optimization2n3 first variables, therefore, provides us optimal schedule (as before, compareresults case first k < n variables 0). differencecase two processes process may skip turn. However, ignore caseprocesses skip turn, since remove loop schedule.scheme algorithm presented Figure 16, description mainroutine (realized DFS Branch Bound method) presented Figure 17.4.5 Optimal Solution Case Additional ConstraintsAssume problem additional constraints: solution time limitedprobability success i-th process p necessarily 1.possible show expressions distribution function expectedcost almost form regular framework:Claim 2 Let system solution time limited , let p probability successi-th process. expressions goal-time distribution expected cost96fiOptimal Schedules Parallelizing Anytime AlgorithmsProcedure optimize builds n optimal schedules (each process may start first), comparesresults, returns best oneprocedure optimizebest val AX V ALU Ebest sequenceloop 1 n[sequence, val] minimize sequence f irst points(i)(val < best val)best val valbest sequence sequenceendreturn [best sequence, best val]end// Procedure minimize sequence f irst points gets parameter// index process starts, returns optimal// sequence valueprocedure minimize sequence f irst points(process start)loop 0 n 1zetas[i] 0endloop 1 process start 1zetas[i] 0endUsing one standard minimization methods, find zetas,minimizing value function build optimal sequence(zetas).endFigure 16: algorithm finding optimal schedule n processes. result containsmod nvector , = 0i = bi/nc.97fiFinkelstein, Markovitch & Rivlinprocedure build optimal sequence(zetas)curr cost calculate cost(zetas)return df sbnb(zetas, curr cost, AX V ALU E, 0)endprocedure df sbnb(zetas, curr cost, thresh, nskip)(curr cost thresh)return AX V ALU E// Cutoff// previous time value current processlast value zetas[length(zetas) n]repeatcalculate next zeta(zetas, last value)( = last value)// Skipbreak looplast valuedelta cost calculate partial cost(zetas, )// Leaf(task accomplished([zetas || ]))(curr cost + delta cost < thresh)optimal zetas [zetas || ]thresh curr cost + delta costendbreak loopendtmp result df sbnb([zetas || ], curr cost + delta cost, thresh, 0)thresh = min(thresh, tmp result)end// repeat(nskip < n 1)// Skip possiblezeta zetas[length(zetas) n]tmp result df sbnb([zetas || ], curr cost, thresh, nskip + 1)thresh = min(thresh, tmp result)endreturn threshendFigure 17: Procedure build optimal sequence, given prefix time sequence, restores optimal sequence prefix using DFS Branch Bound search algorithm,returns sequence value. [x || y] stands concatenation x y.auxiliary functions used similar counterparts Figure 12, deal nprocesses instead 2.98fiOptimal Schedules Parallelizing Anytime Algorithmsfollows:Fn (t, 1 , . . . , n ) = 1Eu (1 , . . . , n ) =Z0ni=1(1 pi Fi (i )) (for ),u0t +nXi0 u0ii=1!ni=1(1 pi Fi (i ))dt.(29)(30)proof similar proof Claim 1.claim shows formulas used previous sections validcurrent settings, three differences:1. use pj Fj instead Fj pj fj instead fj .2. integrals 0 instead 0 .3. time variables limited .first two conditions may easily incorporated algorithms. last condition implies additional changes chain theorems algorithms. chaintheorem n processes becomes:jTheorem 3 value kj either k1, computed given previous2n 2 values using formula (28), calculated formulakj=Tn1Xkjl .(31)l=1first two alternatives similar Theorem 2, third one correspondsboundary condition given Equation (24). third alternative adds one branchDFS Branch Bound algorithm; rest algorithm remains unchanged.Similar changes algorithms performed case maximal allowedtime Ti per process. practice, always use limitation, settingprobability Ai reach goal Ti , pi (1 Fi (Ti )), becomes negligible.5. Process Scheduling Intensity Controlsection analyze problem optimal scheduling case intensity control, equivalent replacing binary scheduling functions (t) continuousfunctions range 0 1. paper assume linear cost functionform (5). believe, however, similar analysis applicable setupdifferentiable u.easy see formulas distribution function expected costClaim 1 still valid intensity control settings.linear cost function (5), minimization problem form! nZnXi0Eu (1 , . . . , n ) =a+b(1 Fj (j ))dt min .(32)0i=199j=1fiFinkelstein, Markovitch & RivlinWithout loss generality, assume + b = 1. leads equivalent minimization problem! nZnX0(1 Fj (j ))dt min,(33)(1 c) + cEu (1 , . . . , n ) =0i=1j=1c = b/(a + b) viewed normalized resource weight. constraints,however, complicated suspend/resume model:1. before, must continuous, (0) = i0 (0) = 0 (at beginningprocesses idle).2. assume partially-continuous derivative i0 , derivativelie 0 1. requirement follows definition intensityfact i0 = : process work negative amount time,process work intensity greater one allowed. Since considerframework shared resources, total intensity limited,additional constraint: sum derivatives i0 time point cannotexceed 1.Thus, optimization problem following boundary conditions:(0) = 0, i0 (0) = 0 = 1, . . . , n,0 i0 1 = 1, . . . , n,nX0i0 1.(34)i=1looking set functions { } provide solution minimizationproblem (33) constraints (34).Let g(t, 1 , . . . , n , 10 , . . . , n0 ) function integral sign (33):! nnX000(1 Fj (j )).(35)g(t, 1 , . . . , n , 1 , . . . , n ) = (1 c) + ci=1j=1traditional method solving problems type use Euler-Lagrange necessaryconditions: set functions 1 , . . . , n provides weak (local) minimum functionalZEu (1 , . . . , n ) =g(t, 1 , . . . , n , 10 , . . . , n0 )dt01 , . . . , n satisfy system equations formg0 k0g 0 = 0.dt k(36)prove following lemma:Lemma 2 Euler-Lagrange conditions minimization problem (33) yield two stronginvariants:100fiOptimal Schedules Parallelizing Anytime Algorithms1. processes k1 k2 k1 k2 border described (34),distribution density functions satisfyfk1 (k1 )fk2 (k2 )=.1 Fk1 (k1 )1 Fk2 (k2 )(37)2. schedules processes border described (34), eitherc = 1 fk (k ) = 0 k.proof lemma given Appendix A.3. lemma provides necessaryconditions local minimum inner points described constraints (34).conditions, however, restricting. Therefore, look general conditions,suitable boundary points well 7 .start following lemma:Lemma 3 optimal solution minimization problem (33) constraints (34)exists, exists optimal solution 1 , . . . , n , timeresources consumed, i.e.,nX(38)i0 (t) = 1.i=1case time cost zero (c 6= 1), equality necessary conditionsolution optimality.proof lemma given Appendix A.4.Corollary 2 intensity control settings, case suspend-resume settings,minimization problem (33) form (6), i.e.Eu (1 , . . . , n ) =Z0n(1 Fj (j ))dt min .j=1Lemma 3 corresponds intuition: resource available, used.Without loss generality, restrict discussion schedules satisfying (38), evencase time cost zero. leads following invariant:nX(t) = t.(39)i=1Assume two F -equivalent processes 1 A2 density functionf (t) satisfying normal distribution law mean value m. Let 1 t2cumulative time consumed processes time t, i.e., 1 (t) = t1 2 (t) = t2 .question is, process active (or active parallelpartial intensities)?7. Note also even conditions hold, necessarily provide optimal solution.Moreover, problems variation calculus necessarily minimum, since analogueWeierstrass theorem continuous functions closed set.101fiFinkelstein, Markovitch & Rivlin0.40.40.350.350.30.30.250.250.20.2f(t)f(t)Without loss generality, t1 < t2 , means first process requiredcover larger area succeed: 1 F (t 1 ) > 1 F (t2 ). supports policy timeactivates second process. policy supported 1 lower distributiondensity, f1 (t1 ) < f2 (t2 ), illustrated Figure 18(a). If, however, first processhigher density, illustrated Figure 18(b), clear two processesactivated time t. optimal policy general case 8 ? answer relies0.150.150.10.10.050.050012t13t24567890100123(a)456t17t28910(b)Figure 18: (a) Process A1 (currently t1 ) lower density larger area cover, thereforeinferior. (b) Process A1 lower density, smaller area cover, decisionunclear.heavily functions appear (37). functions, described equationhk (t) =fk (t),1 Fk (t)(40)known hazard functions, play important role following theoremdescribing necessary conditions optimal schedules.Theorem 4 Let set functions { } solution minimization problem (6)constraints (34). Let t0 point hazard functions processes h (i (t))continuous, let Ak process active t0 (k0 (t0 ) > 0),process Aihi (i (t0 )) < hk (k (t0 )).(41)t0 process k consumes resources, i.e. k0 (t0 ) = 1.proof theorem given Appendix A.5.Theorem 4 Equation (37), intensity control may useful hazardfunctions least two processes equal. However, even case equilibriumalways stable. Assume within interval [t 0 , t00 ] processes Ai Ajworking partial intensity, implies h (i (t)) = hj (j (t)). Assume8. Analysis normal distribution given Section 6.3 shows optimal policy examplegive resources process A2 cases.102fiOptimal Schedules Parallelizing Anytime Algorithmshi (t) hj (t) monotonically increasing. moment give priority oneprocesses, obtain higher value hazard function, getsubsequent resources. case stable equilibrium h (i (t)) hj (j (t))monotonically decreasing functions constants.intuitive discussion formulated following theorem:Theorem 5 active process remain active consume resources longhazard function monotonically increasing.proof given Appendix A.6.theorem imply important corollary:Corollary 3 hazard function one processes greater equalothers = 0 monotonically increasing t, processone activated.conclude extension suspend-resume model intensity controlmany cases increase power model beneficial monotonicallydecreasing hazard functions. time cost taken account (c = 1), however,intensity control permits us connect two concepts: model sharedresources model independent agents:Theorem 6 time cost taken account (c = 1), model shared resourcesintensity control settings equivalent model independent processessuspend-resume control settings. Namely, given suspend-resume solution modelindependent processes, may reconstruct intensity-based solutioncost model shared resources vice versa.proof theorem given Appendix A.7.Theorem 4 claims process maximal value h k (k (t)) active,take resources. Why, then, would always choose processhighest value hk (k (t)) active? turns strategy optimal.Let us consider two processes distribution densities shown Figure 19(a).corresponding values hazard functions shown Figure 19(b). usingstrategy, A2 would active process. Indeed, time = 0, h 2 (2 (0)) >h1 (1 (0)), would lead activation 2 . moment, A1 would remainidle hazard function remain 0. strategy would result expected time 2.If, hand, would activated 1 only, result would expectedtime 1.5. Thus, although h1 (1 (0)) < h2 (2 (0)), better give resourcesA1 beginning due superiority future.elaborate example shown Figure 20. corresponds case twoprocesses F -equivalent, one linear combination two normaldistributions, f (t) = 0.5fN (0.6,0.2) (t) + 0.5fN (4.0,2.0) (t), fN (,) (t) distributiondensity normal distribution mean value standard deviation , secondprocess uniformly distributed [1.5, 2.5]. Activating 1 results 0.5 0.6 + 0.54.0 = 2.3, activating A2 results expected time 2.0, activating 1 time1.2 followed activating A2 results (approximately) 0.6 0.5 + (1.2 + 2.0) 0.5 = 1.9.103fiFinkelstein, Markovitch & Rivlin1.25f1(t)f2(t)1h1(t)h2(t)4h(t) (truncated)f(t)0.80.6320.410.200123405012345(a)(b)Figure 19: density function hazard function two processes. Although h 1 (1 (0)) <h2 (2 (0)), better give resources A1 .best solution is, therefore, start execution activating 1 , point t0transfer control A2 . case interrupt active process greater valuehazard function, preferring idle process zero value hazard function (sinceh1 (1 (t0 )) > h2 (2 (t0 )) = 0).1.25f1(t)f2(t)1h1(t)h2(t)4h(t) (truncated)f(t)0.80.6320.410.20024680100246810(a)(b)Figure 20: density function hazard function two processes. best solutionstart A1 , point interrupt favor A2 , although latterzero hazard function.examples show straightforward use hazard functions building optimalschedules problematic. However, since suspend-resume model specificcase intensity control model, hazard functions still may useful understandingbehavior optimal schedules, used next section.104fiOptimal Schedules Parallelizing Anytime Algorithms6. Optimal Scheduling Standard Distributionssection present results optimal scheduling strategy systemprocesses whose performance profiles meet one well-known distributions: uniform,exponential, normal lognormal. show results processes bimodalmultimodal distribution functions.implemented three scheduling policies two agents:1. Sequential strategy, schedules processes one another, initiatingsecond process probability first one find solution becomesnegligible. processes F -equivalent, choose best order processinvocation.2. Simultaneous strategy, simulates simultaneous execution processes.3. Optimal strategy, implementation algorithm described Section 4.2.rest section compare three strategies, deadline given,processes stopped probability still find solution becomesnegligible.goal compare different scheduling strategies analyze behaviorprocesses. Absolute quantitative measurements, average cost, processdependent, therefore appropriate scheduling strategy evaluation. therefore would like normalize results application different scheduling methodsminimize effect process behavior. case F -equivalent processes, goodcandidate normalization coefficient expected time individual process.processes F -equivalent, however, decision straightforward,therefore use results sequential strategy normalization factor.define relative quality qref (S) strategy respect strategy refqref (S) = 1u(S),u(Sref )(42)u(S) average cost strategy S. measurement corresponds gain(maybe negative) strategy relative reference strategy. section usesequential strategy reference strategy.6.1 Uniform DistributionAssume goal-time distribution processes meets uniform lawinterval [t0 , ], i.e., distribution functions< t0 ,0(t t0 )/(T t0 ) [t0 , ],(43)F (t) =1>density functionsf (t) =06 [t0 , ],1/(T t0 ) [t0 , ].105(44)fiFinkelstein, Markovitch & Rivlindensity function process uniformly distributed [0, 1] shown Figure 21(a).hazard function uniform distribution form< t0 ,01/(T t0 )1h(t) =(45)=[t0 , ],1 (t t0 )/(T t0 )monotonically increasing function. Corollary 3, one processactive, optimal strategy equivalent sequential strategy.processes F -equivalent, problem solved choosing processminimal expected time.interesting setup involves uniformly distributed process guaranteedfind solution. case corresponds probability success p less 1.claimed Section 4.5, corresponding distribution density functionmultiplied p. result, hazard function becomesh(t) =(0p(T t0 ) p(t t0 )< t0 ,[t0 , ].(46)function still monotonically increasing t, conclusions remain same.graphs hazard functions processes uniformly distributed [0, 1] probabilitysuccess 0.5, 0.8 1 shown Figure 21(b).1.210f(t)1h(t) p = 1.0h(t) p = 0.8h(t) p = 0.58h(t) (truncated)f(t)0.80.6640.420.2000.20.40.60.8101.200.20.4(a)0.60.811.2(b)Figure 21: (a) density function process, uniformly distributed [0, 1], (b) hazard functionsprocesses uniformly distributed [0, 1] probability success 0.5, 0.8 1.6.2 Exponential Distributionexponential distribution described density function00f (t) =e> 0,106(47)fiOptimal Schedules Parallelizing Anytime Algorithmsdistribution function form00F (t) =1e> 0.(48)Substituting expressions (6) givesEu (1 , . . . , n ) =Z0n(1 Fj (j ))dt =j=1ZePnj=1j j (t)dt.0system F -equivalent processes, Lemma 3nXj j (t) =nXj (t) = t,j=1j=1thereforeEu (1 , . . . , n ) =Z0et dt =1.Thus, system F -equivalent processes schedules equivalent. interesting fact reflected also behavior hazard function, constant:h(t) .However, probability success smaller 1, hazard function becomesmonotonically decreasing function:h(t) =petp.=1 p(1 e )p + (1 p)etprocesses work simultaneously (with identical intensities F -equivalent processes, intensities maintaining equilibrium hazard functions otherwise), sinceprocess idle advantage working teammate.Figure 22(a) shows density function exponentially distributed process= 1. graphs hazard functions processes exponentially distributed= 1 probability success 0.5, 0.8 1 shown Figure 22(b).Let us consider somewhat elaborate example, involving processesF -equivalent. Assume two learning systems, exponential-likeperformance profile typical systems. also assume one systems requiresdelay preprocessing works faster. Thus, assume first systemdistribution density f1 (t) = 1 e1 , second one density f2 (t) = 2 e2 (tt2 ) ,1 < 2 (the second faster), t2 > 0 (it also delay). Assumelearning systems deterministic given set examples, may faillearn concept probability 1 p = 0.5. graphs densityhazard functions two systems shown Figure 23.applied optimal scheduling algorithm Section 4.2 values 1 = 3,2 = 10, t2 = 5. optimal schedule activate first system 1.15136 timeunits, (if found solution) activate second system 5.77652 time units.107fiFinkelstein, Markovitch & Rivlin1.2f(t)110.80.8h(t) (truncated)f(t)1.20.60.60.40.40.20.2002468010h(t) p = 1.0h(t) p = 0.8h(t) p = 0.50246(a)(b)810Figure 22: (a) density function process, exponentially distributed = 1, (b) hazardfunctions processes exponentially distributed = 1 probability success0.5, 0.8 1.53f1(t)f2(t)h1(t)h2(t)2.542f(t)h(t) (truncated)321.51100.5024680100246810(a)(b)Figure 23: (a) Density (b) hazard functions two exponentially distributed systems,different values time shift.108fiOptimal Schedules Parallelizing Anytime Algorithmsfirst system run additional 3.22276 time units, finally secondsystem run 0.53572 time units. point solution found,systems failed probability 1 10 6 each.Figure 24(a) shows relative quality simultaneous optimal schedulingstrategies function t2 p = 0.8 (for 10000 simulated examples). large valuest2 benefit switching first algorithm second decreases,reflected relative quality optimal strategy. simultaneous strategy,see, beneficial relatively small values 2 .Figure 24(b) reflects behavior strategies fixed value 2 = 5.0function probability success p. simultaneous strategy inferior, qualitydecreases p increases. Indeed, probability success 1, running secondalgorithm first one simultaneously waste time. hand,optimal strategy positive benefit, means resulting schedulestrivial.4040OptimalSimultaneous352030250Relative quality (in percent)Relative quality (in percent)OptimalSimultaneous2015105-20-40-600-5-80-10-15123456Delay second system789-1000.110(a)0.20.30.40.50.6Probability success0.70.80.91(b)Figure 24: Learning systems: Relative quality optimal simultaneous scheduling strategies(a) function t2 fixed p = 0.8, (b) function p fixed t2 = 5.6.3 Normal Distributionnormal distribution mean value deviation described densityfunction(tm)21(49)f (t) =e 22 ,2distribution functionZ(xm)21F (t) =(50)e 22 dx.2Since use t0 = 0, used truncated normal distribution distributiondensity(tm)211e 22 ,(1 )2109fiFinkelstein, Markovitch & Rivlindistribution functionZ(xm)211e 22 dx ,121=2Z0e(xm)22 2dx,large enough, may considered 0. density function normallydistributed process = 5 = 1 shown Figure 25(a).hazard function normal distribution monotonically increasing, leadsconclusions uniform distribution. However, probability successless 1 completely changes behavior hazard function: point,starts decrease. graphs hazard functions processes normally distributedmean value 5, standard deviation 1 probabilities success 0.5, 0.8 1shown Figure 25(b).0.46f(t)h(t) p = 1.0h(t) p = 0.8h(t) p = 0.50.3550.34h(t) (truncated)f(t)0.250.230.1520.110.050024680100246810(a)(b)Figure 25: (a) density function normally distributed process, = 5 = 1, (b)hazard functions normally distributed processes = 5 = 1,probabilities success 0.5, 0.8 1.previous example, consider case two processes F equivalent, running deviation = 1 probability successp. first process assumed 1 = 1, second process starteddelay m. relative quality 10000 simulated examples shown Figure 26.Figure 26(a) shows relative quality function p = 0.8; Figure 26(b) showsrelative quality function p = 2. Unlike exponential distribution, gainexample optimal strategy rather small.6.4 Lognormal Distributionrandom variable X lognormally distributed, ln X normally distributed.density function distribution function corresponding parameters110fiOptimal Schedules Parallelizing Anytime Algorithms1020Optimal scheduleSimultaneous0Relative quality (in percent)Relative quality (in percent)0-10-20-30-40-50Optimal scheduleSimultaneous-20-40-60-8001234Delay second process5-1000.160.20.3(a)0.40.50.6Probability success0.70.80.91(b)Figure 26: Normal distribution: relative quality (a) function fixed p = 0.8, (b)function p fixed = 2.written(log(t)m)21e 22 ,f (t) =2Z log(t)(xm)21e 22 dx.F (t) =2(51)(52)Lognormal distribution plays significant role AI applications since many cases searchtime distributed lognormal law. density function lognormal distribution mean value log(5.0) standard deviation 1.0 shown Figure 27(a),hazard functions different values p shown Figure 27(b). Let us considersimulated experiment similar analogue normal distribution. consider twoprocesses F -equivalent, parameters = 1 probabilitysuccess p. first process assumed 1 = 1, second process starteddelay, m2 m1 = > 0. relative quality 10000 simulatedexamples shown Figure 28. Figure 28(a) shows relative quality functionp = 0.8; Figure 28(b) shows relative quality function p = 2.graphs show small values optimal simultaneous strategysignificant benefit sequential one. However, larger values, performance optimal strategy approaches performance sequential strategy,simultaneous strategy becomes inferior.6.5 Bimodal Multimodal Density FunctionsExperiments show case F -equivalent processes unimodal distributionfunction, sequential strategy often optimal. section consider less trivialdistributions.111fiFinkelstein, Markovitch & Rivlin0.140.18f(t)h(t) p = 1.0h(t) p = 0.8h(t) p = 0.50.160.120.140.1h(t) (truncated)0.12f(t)0.080.060.10.080.060.040.040.0200.02051015202530354045050051015(a)20253035404550(b)Figure 27: (a) Density function lognormal distribution mean value log(5.0) standarddeviation 1.0 (b) hazard functions lognormally distributed processesmean value log(5.0), standard deviation 1, probabilities success 0.5,0.8 1.6040Optimal scheduleSimultaneousOptimal scheduleSimultaneous5020Relative quality (in percent)Relative quality (in percent)403020100-20-400-60-10-2000.511.522.5Delay second process33.5-800.14(a)0.20.30.40.50.6Probability success0.70.80.91(b)Figure 28: Lognormal distribution: relative quality (a) function fixed p = 0.8,(b) function p fixed = 2.112fiOptimal Schedules Parallelizing Anytime AlgorithmsAssume first non-deterministic algorithm performance profileexpressed linear combination two normal distributions deviation:f (t) = 0.5fN (1 ,) + 0.5fN (2 ,) .example density hazard functions distributions 1 = 2, 2 = 5,= 0.5 given Figure 29.0.350.9f(t)h(t) p = 1.00.80.30.70.25h(t) (truncated)0.6f(t)0.20.150.50.40.30.10.20.0500.1012345670801234(a)5678(b)Figure 29: (a) Density function (b) hazard function process distributed accordingdensity function f (t) = 0.5fN (2,0.5) + 0.5fN (5,0.5) probability successp = 0.8.Assume invoke two runs algorithm fixed values 1 = 2, = 0.5,p = 0.8, free variable 2 . Figure 30 shows relative qualityscheduling strategies influenced distance peaks, 2 1 . resultscorrespond intuitive claim larger distance peaks,attractive optimal simultaneous strategies become.25OptimalSimultaneous20Relative quality (in percent)151050-5-10-15-20-250246810121416Distance peaksFigure 30: Bimodal distribution: relative quality function distance peaks.113fiFinkelstein, Markovitch & Rivlinlet us see number peaks density function affects schedulingquality. consider case partial uniform distribution, density distributedk identical peaks length 1 placed symmetrically time interval 0 100.(Thus, density function equal 1/k belongs one peaks,0 otherwise.) experiment chosen p = 1.Figure 31 shows relative quality system function k, obtained10000 randomly generated examples. see results, simultaneousstrategy inferior, due valleys distribution function. optimal strategyreturns schedules processes switch peak, relative qualityschedules decreases number peaks increases.50OptimalSimultaneous40Relative quality (in percent)3020100-10-20-3023456Number peaks78910Figure 31: Multimodal distribution: relative quality function number peaks.7. Experiments: Using Optimal Scheduling Latin Square Problemtest performance algorithm realistic domain, applied LatinSquare problem described Section 2.2. assume given Latin Squareproblem two initial configurations, fully deterministic algorithm distributionfunction distribution density shown Figure 7.compare performance schedule produced algorithm performance sequential simultaneous strategies described Section 6. addition,test schedule runs processes one another, allowing single switchoptimal point (an analogue restart technique two processes). referschedule single-point restart schedule.Note case two initial configurations corresponds case two processesframework. general, could think set n initial configurations wouldcorrespond n processes. sufficiently large n, restart strategy restartstarts different initial configuration, becomes close optimal.experiments performed different values N , 10% square precolored. performance profile induced based run 50, 000 instances,remaining 50, 000 instances used 25, 000 testing pairs. schedules applied114fiOptimal Schedules Parallelizing Anytime Algorithmsfixed deadline , corresponds maximal allowed number generatednodes.Since results sequential strategy type problems much worseresults strategies sufficiently large values , instead usedsimultaneous strategy reference relative quality measure.30OptimalSingle-point restart25Relative quality (in percent)20151050-5-10-15050001000015000200002500030000Maximal available time35000400004500050000Figure 32: Relative quality function maximal allowed timeFigure 32 shows maximal available time (the x axis) influences qualityschedules (the axis), simultaneous strategy used reference.small values , single-point restart optimal strategy25% gain simultaneous strategy, since produce schedules closesequential one. However, available time increases, benefit parallelizationbecomes significant, simultaneous strategy overcomes single-point restartstrategy. relative quality optimal schedule also decreases increases, sinceresulting schedule contains switches two problem instancessolved.Figure 33 illustrates optimal single-point restart schedules relatesimultaneous schedule different size Latin Squares (given = 25, 000). initial gainstrategies 50%. However, problems N = 20 single-pointrestart strategy becomes worse simultaneous one. larger sizes probabilitysolving Latin Square problem time limit 25, 000 steps becomes smallersmaller, benefit optimal strategy also approaches zero.115fiFinkelstein, Markovitch & Rivlin50OptimalSingle-point restartRelative quality (in percent)403020100-105101520Lain Square size253035Figure 33: Relative quality function size Latin Square8. Combining Restart Scheduling PoliciesLuby, Sinclair, Zuckerman (1993) showed restart strategy optimalinfinite number identical runs available. number limited, restartstrategy optimal. Sometimes, however, mixed situation. Assumetwo initial states, non-deterministic algorithm, linear time cost. one hand,perform restarts run corresponding one initial states.hand, switch runs corresponding two initial states. wouldoptimal policy case?expected time run based single initial state1E(t ) =F (t )Z0(1 F (t))dt,(53)restart point F (t) distribution function. formula obtainedsimple summation geometric series coefficient 1F (t ), continuousform formula given Luby, Sinclair, Zuckerman (1993). Minimization (53)gives us optimal restart point.Assume first sequence restarts single initial state process interruptible restart points. Since probability failure successive restarts(1 F (t ))i , process exponentially distributed. Thus, problem reducedscheduling two exponentially distributed processes. According analysis Section 6.2, schedules equivalent problems corresponding two initial states116fiOptimal Schedules Parallelizing Anytime Algorithmssolvable. Otherwise, optimal policy alternate two processesrestart point.interesting case allow rescheduling time point. general,beneficial switch processes non-restart points (otherwiserescheduling points would chosen restart). rescheduling, however,beneficial cost associated restarts higher rescheduling cost 9 .Let us assume restart constant cost C. Similarly (53), writeexpected cost policy performing restarts pointE(t ) =1F (t )Z0(1 F (t))dt +1 F (t )C,F (t )2(54)second term corresponds series0 + C(1 F (t )) + 2C(1 F (t ))2 + . . .Let optimal restart points setups without associated costsrespectively. greater due restart cost.Let us consider following schedule: first process runs , secondprocess runs , first process runs (with restart) additional ,second process runs additional . first process restarts runsforth.Let us compare expected time schedule time pure restartpolicy, first process runs , second process runs ,first process restarts runs forth.Similarly (15), expected time first schedule interval [0, 2t ]writtenZ(1 F (t))dt + (1 F (t ))Z(1 F (t))dt+ZZ(1 F (t ))(1 F (t))dt + (1 F (t ))(1 F (t))dt.Esched =00hand, expected time second schedule intervalEsimple =Z(1 F (t))dt + (1 F (t ))0Z(2 F (t ))(1 F (t))dt.Z0(1 F (t))dt =09. example setup robotic search, returning robot initial positionexpensive suspending resuming robot.117fiFinkelstein, Markovitch & RivlinEsched rewrittenZZ(1 F (t))dt+(1 F (t))dt + (1 F (t ))Esched =00ZZ(1 F (t))dt =(1 F (t))dt (1 F (t ))(1 F (t ))00ZZ(1 F (t))dt =(1 F (t))dt + (2 F (t ) F (t ))F (t )00ZZ(1 F (t))dt + Esimple .(1 F (t))dt F (t )F (t )00Thus, obtainEsimple Esched = F (t )F (t )F (t )1F (t )Z0Z0(1 F (t))dt F (t )(1 F (t))dt1F (t )Z0Z0(1 F (t))dt =!(1 F (t))dt ,since provides minimum (53), last expression positive, meansscheduling improves simple restart policy.Note, claim proposed scheduling policy optimal exampleshows pure restart strategy optimal. optimalcombination interleaving restarts global level scheduling local level,finding combination left future research.9. Conclusionswork present algorithm optimal scheduling anytime algorithmsshared resources. first introduce formal framework representing analyzingscheduling strategies. begin analyzing case allowed scheduling operations suspending resuming processes. prove necessary conditionsschedule optimality present algorithm building optimal schedules basedconditions. analyze general case scheduler increasedecrease intensity scheduled processes. prove necessary conditions showintensity control rarely needed. analyze, theoretically empirically,behavior scheduling algorithm various distribution types. Finally, presentempirical results applying scheduling algorithm Latin Square problem.results show optimal strategy indeed outperforms scheduling strategies. lognormal distribution, showed improvement 50%naive sequential strategy. general, algorithm particularly beneficial heavy-taileddistributions, even exponential distribution show benefit 35%.cases, however, simple scheduling strategies yield results similar obtained algorithm. example, optimal schedule uniform distributionapply one processes switch. probability succeed within giventime limit approaches 1, simple scheduling strategy also becomes close optimal,118fiOptimal Schedules Parallelizing Anytime Algorithmsleast unimodal distributions strong skew towards zero. hand,probability success approaches zero, another simple strategy appliesprocesses simultaneously becomes close optimal.behavior meets intuition. heavy-tailed distributions, switchingruns promising chance bad trajectory high enough.correct distributions low probability success. However, probabilitybad trajectory high, best strategy switch runsfast possible, equivalent simultaneous strategy. hand,distribution skewed right, often sense switch runs,since new run pay high penalty reaches promising distributionarea. general, user certain particular application falls onecategories above, cost calculating optimal schedule saved.high complexity computation one potential weaknesses presentedalgorithm. complexity represented multiplication three factors: functionminimization, Branch-and-Bound search, solving Equations (18) (19) casetwo agents Equation (28) general case. two agents, exponential component Branch-and-Bound search. found, however, practicebranching factor, roughly number roots equations above, rathersmall, depth search tree controlled iterative-deepening strategies.arbitrary number agents, function minimization may also exponential. practice, however, depends behavior minimized function minimizationalgorithm.Since optimal schedule static applied large number probleminstances, computation beneficial even associated high cost. Moreover,applications (such robotic search) computational cost outweighedgain obtained single invocation.previous work related research restart framework (Luby et al.,1993). important difference algorithm restart policyability handle cases number runs limited, different algorithmsinvolved. one algorithm available number runs infinite,restart strategy optimal. However, shown Section 8, problems maybenefit combination two approaches.algorithm assumes availability performance profiles processes.performance profiles derived analytically using theoretical models processesempirically previous experience solving similar problems. Online learningperformance profiles, could expand applicability proposed framework,subject ongoing research.framework presented used wide range applications. introduction presented three examples. first example describes two alternative learningalgorithms working parallel. behavior algorithms usually exponential,analysis setup given Section 6.2. second example CSP problemtwo alternative initial configurations, analogous Latin Square exampleSections 2.2 7. last example includes crawling processes limited sharedbandwidth. Unlike first two examples, setup falls framework intensitycontrol described Section 5.119fiFinkelstein, Markovitch & RivlinSimilar schemes may applied elaborate setups:Scheduling system n anytime algorithms, overall cost systemdefined maximal cost components (unlike analysis Section 4,function differentiable);Scheduling non-zero process switch costs;Providing dynamic scheduling algorithms able handle changes environment;Building effective algorithms case several resources different types, e.g.,multiprocessor systems.Appendix A. Formal ProofsA.1 Proof Lemma 1claim lemma follows:system n processes, expression expected cost (6) rewrittenZXn i+n1Xkj(1 Fi (x))dx.(55)Eu (1 , . . . , n , . . .) =))(1 F#j (k1k1k=0 i=1 j=i+1Proof: Splitting whole integration range [0, ) intervals [t i1k , tk ] yieldsfollowing expression:Znn Z tiXnXk(1 Fj (j ))dt.(56)(1 Fj (j ))dt =Eu (1 , . . . , n ) =0k=0 i=1j=1ti1kj=1(25), rewrite inner integralZ tink(1 Fj (j )) =ti1kj=1Ztikti1ki1i1j=1j=i+1n))(1 Fj (kj )) (1 Fi (t ti1+ k1k(1 F#j (kj ))Ztikti1knj=i+1j(1 Fj (k1)) dt =(57)+ k1))dt.(1 Fi (t ti1kSubstituting x ti1+ k1using (23), obtainkZ tii1kj(1 F#j (k ))(1 Fi (t ti1+ k1))dt =kti1kj=i+1ni+n1j=i+1i+n1j=i+1(1jF#j (k1))(1jF#j (k1))ZZtik ti1k +k1k1kik1(1 Fi (x))dx =(1 Fi (x))dx.120(58)fiOptimal Schedules Parallelizing Anytime AlgorithmsCombining (56), (57) (58) gives us (55).Q.E.D.A.2 Proof Chain Theorem n Processeschain theorem claim follows:l1l1 , computed given previous 2n 2value m+1may eithervalues using formula1l )fl (ml )Fl (ml+n1j=l+1=l1Xj(1 F#j (m1))i+n1i=ln+1 j=i+1#j6=ll+n1j(1 F#j (m))j=l+1Zj(1 F#j (m))m+1(59)(1 F#i (x))dxProof: Lemma 1, expression want minimize described equationEu (1 , . . . , n , . . .) =Xn i+n1Xk=0 i=1 j=i+1j(1 F#j (k1))Zkik1(1 Fi (x))dx.(60)expression reaches optimal values eitherdEu= 0 j = 1, . . . , n, . . . ,dj(61)border described (26).Reaching optimal values border corresponds first alternative describedtheorem. Let us consider case derivative E u j 0.l , 0 l n 1. Let us seevariable j may presented mn+l =l participating in.summation terms (60)l may lower bound integral (60). happens k = + 11.= l. corresponding termS0 =l+n1j=l+1(1jF#j (m))Zlm+1l(1 Fl (x))dx,l+n1dS0lj=(1F())(1 F#j (m)).lldmj=l+1l may upper bound integral, happens k =2.= l. corresponding termSl =l+n1j=l+1(1jF#j (m1))121Zllm1(1 Fl (x))dx,fiFinkelstein, Markovitch & Rivlinl+n1dSljl(1 F#j (m1)).=(1F())lldmj=l+1l may participate product3. Finally,i+n1j=i+1j(1 F#j (k1)).= 1 . . . l 1, may happen k = + 1 j = l, correspondingtermZi+n1m+1j(1 Fi (x))dx,(1 F#j (m ))Si =j=i+1derivativedSil= fl (m)ldmi+n1(1jF#j (m))j=i+1,#j6=lZm+1(1 Fi (x))dx.= l + 1 . . . n, k = j = l + n. corresponding termSi =i+n1(1j=i+1jF#j (m1))Zm1(1 Fi (x))dx,derivativedSil= fl (m)ldmi+n1j=i+1,#j6=l(1jF#j (m1))Zm1(1 Fi (x))dx.l appears integral, possibility l appearSince = l,expression, thereforendEu X dSi=.lldmdmi=0right-hand side sum written follows:nXdSi=ldmi=0(1l1XlFl (m))j=l+1lfl (m)i=1nXi=l+1l+n1i+n1(1j=i+1,#j6=llfl (m)i+n1jF#j (m))j(1 F#j (m))(1l+n1j(1 F#j (m1))+ (1lFl (m))Z(1 Fi (x))dxm+1jF#j (m1))j=i+1,#j6=l122Zj=l+1m1(1 Fi (x))dx.(62)fiOptimal Schedules Parallelizing Anytime AlgorithmsHowever,nXi+n1jF#j (m1))(1i=l+1 j=i+1,#j6=l0Xi+n1(1jF#j (m))i=ln+1 j=i+1,#j6=lZm1Z(1 Fi (x))dx =m+1(1 Fi (x))dx.(63)Substituting (63) (62), obtainnXdSi=ldmi=0l(1 Fl (m))lfl (m)l1Xl+n1j=l+1j(1 F#j (m1))i+n1(1jF#j (m))i=ln+1 j=i+1,#j6=ll+n1j=l+1Zj(1 F#j (m))m+1(1 Fi (x))dx.(64)l ) 0, would mean goal reached probability1 Fl (m1, scheduling would redundant. Otherwise, expression (64) 01l )fl (ml )Fl (ml+n1j=l+1=l1X(1jF#j (m1))i+n1(1i=ln+1 j=i+1,#j6=ll+n1j=l+1jF#j (m))Zj(1 F#j (m))m+1,(1 F#i (x))dxequivalent (59).l1l+1= n(m+1)+l1 ),= n(m1)+l+1 m+1Equation (59) includes 2n 1 variables ( m1l1providing implicit dependency m+1 remaining 2n 2 variables.Q.E.D.A.3 Proof Lemma 2claim lemma follows:Euler-Lagrange conditions minimization problem (33) yield two strong invariants:1. processes k1 k2 k1 k2 border described (34),distribution density functions satisfyfk2 (k2 )fk1 (k1 )=.1 Fk1 (k1 )1 Fk2 (k2 )123(65)fiFinkelstein, Markovitch & Rivlin2. schedules processes border described (34), eitherc = 1 fk (k ) = 0 k.Proof: Let g(t, 1 , . . . , n , 10 , . . . , n0 ) function integral sign (33):g(t, 1 , . . . , n , 10 , . . . , n0 )(1 c) + c=nXi0i=1!n(1 Fj (j )).(66)j=1necessary condition Euler-Lagrange claims set functions 1 , . . . , n providesweak (local) minimum functionalEu (1 , . . . , n ) =Z0g(t, 1 , . . . , n , 10 , . . . , n0 )dtfunctions satisfy system equations formg0 k0g 0 = 0.dt kcase,g0 k= (1 c) + cnXi0i=1!fk (k )(67)j6=k(1 Fj (j )),nnX0g 0 = c(1 Fj (j )) = cl0 fl (l ) (1 Fj (j )).dt kdtj=1l=1(68)(69)j6=lSubstituting last expression (67), obtaing0 1=g0 2= ... =g0 n= cnXl0 fl (l )l=1j6=l(1 Fj (j )),(68) every k1 k2fk1 (k1 )j6=k1(1 Fj (j )) = fk2 (k2 )j6=k2(1 Fj (j )).ignore case one terms 1 F j (j ) 0. Indeed, possiblegoal reached process j probability 1, case optimizationneeded. Therefore, obtainfk1 (k1 )(1 Fk2 (k2 )) = fk2 (k2 )(1 Fk1 (k1 )),equivalent (65).124(70)fiOptimal Schedules Parallelizing Anytime AlgorithmsLet us show correctness second invariant. (69) (65), obtainnX0l0 fl (l ) (1 Fj (j )) =g 0 = cdt kcl=1nXl=1nXj6=ll0nfl (l )(1 Fj (j )) =1 Fl (l )j=1nfk (k )(1 Fj (j )) =1 Fk (k )j=1l=1!nXi0 fk (k )c(1 Fj (j )).cl0i=1j6=k(36) getg0 kg0 0 =dt k+c(1 c) + cnXi=1i0!nXi0i=1fk (k )(1 c)fk (k )!fk (k )j6=kj6=kj6=k(1 Fj (j ))(1 Fj (j )) =(1 Fj (j )) = 0.Since ignore case (1 Fj (j )) = 0, second invariant correct.Q.E.D.A.4 Proof Lemma 3claim lemma follows:optimal solution exists, exists optimal solution 1 , . . . , n ,time resources consumed, i.e.,nXi0 (t) = 1.i=1case time cost zero (c 6= 1), equality necessary conditionsolution optimality.Proof: know {i } provide minimum expression (33)! nZnXi0(1 c) + c(1 Fj (j ))dt.0i=1j=1Let us assume time interval [t 0 , t1 ], {i } satisfy lemmas constraints.However, possible use amount resources effectively. Let us consider125fiFinkelstein, Markovitch & Rivlinlinear time warp (t) = + time interval [t 0 , t1 ], satisfying (t0 ) = t0 .last condition, follows = 0 (1 ). Let t01 point (t) achieves t1 ,i.e., t01 = t0 + (t1 t0 )/. Let us consider set new objective schedule functionse (t)formt0 ,(t),(t + ),t0 t01 ,ei (t) =(t + t1 t01 ), > t01 .Thus,ei (t) behaves (t) t0 , (t) time shift t01 , linearlyspeeded version (t) interval [t0 , t01 ]. Since (t0 ) = t0 (t01 ) = t1 ,ei (t)continuous points t0 t01 .ei0 (t) equal i0 (t) within interval [t0 , t1 ], i0 (t) outside interval.contradiction assumption, meet lemma constraints [t 0 , t1 ], thustake1P> 1,=maxt[t0 ,t1 ] ni=1 i0 (t)leading valid functionsei0 (t). Usingei (t) (33), obtain! nZnX0ei (t)(1 c) + c(1 Fj (ej (t)))dt =Eu (e1 , . . . ,en ) =0Zi=1t0(1 c) + c0Zt01t0Z(1 c) + ci0 (t)i=1(1 c) + ct01nXnX!i0 (tj=1nj=1(1 Fj (j (t)))dt ++ )i=1nXi0 (ti=1+ t1!n(1 Fj (j (t + )))dt +j=1t01 )!n(1 Fj (j (t + t1 t01 )))dt.j=1substituting x = + second term last sum, x = + 1 t01third term, obtain! nZ t0nXEu (e1 , . . . ,en ) =(1 c) + ci0 (t)(1 Fj (j (t)))dt +0Zi=1t1t0Zt11c+cnXi0 (x)i=1(1 c) + cEu (1 , . . . , n )nXi0 (x)i=1Zt1t0!j=1n(1 Fj (j (x)))dx +j=1!nj=1(1 Fj (j (x)))dx =1(1 c) 1Since > 1, last term non-negative, thereforeEu (e1 , . . . ,en ) Eu (1 , . . . , n ),126n(1 Fj (j ))dt.j=1fiOptimal Schedules Parallelizing Anytime Algorithmsmeaning set {e} provides solution least quality { }. c 6= 1,contradicts optimality original schedule, c = 1, new schedulealso optimal.Q.E.D.A.5 Proof Theorem 4claim theorem follows:Let set functions {i } solution minimization problem (6) constraints (34). Let t0 point hazard functions processes h (i (t))continuous, let Ak process active t0 (k0 (t0 ) > 0),process Aihi (i (t0 )) < hk (k (t0 )).(71)t0 process k consumes resources, i.e. k0 (t0 ) = 1.Proof: First want prove theorem case two processes,generalize proof case n processes. Assume 1 (t) 2 (t) provideoptimal solution, point 0 10 (t0 ) > 0f2 (2 (t0 ))f1 (1 (t0 ))>.1 F1 (1 (t0 ))1 F2 (2 (t0 ))(72)continuity functions h (t) point t0 , follows existsneighborhood U (t0 ) t0 , two points t0 , t00 neighborhood h1 (t0 ) >h2 (t00 ), i.e.,f1 (1 (t0 ))f2 (2 (t00 ))min>max.(73)t0 U (t0 ) 1 F1 (1 (t0 ))t00 U (t0 ) 1 F2 (2 (t00 ))Let us consider interval [t0 , t1 ] U (t0 ). order make proof readable,introduce following notation (for proof only):denote 1 (t) (t). Lemma 3, 2 (t) = (t).denote (t0 ) 0 (t1 ) 1 .interval [t0 , t1 ] first process obtains 1 0 resources, second processobtains (t1 t0 )( 1 0 ) resources. Let us consider special resource distributione,first gives resources first process, second process, keepingquantity resources :(t),t0 ,0 + 0 , t0 0 + 1 0 ,e(t) =1 ,t0 + 1 0 1(t),t1 .easy see (t) continuous points 0 , t1 , t0 + 1 0 . wantshow that, unless first process consumes resources beginning, scheduleproducede outperforms schedule produced .127fiFinkelstein, Markovitch & RivlinLet = t0 + 1 0 , corresponds time first process wouldconsumed resources working maximal intensity. First, wantshow interval [t0 , ](1 F1 ((t)))(1 F2 (t (t))) (1 F1 (t t0 + 0 ))(1 F2 (t0 0 )).(74)(t) = (t t0 + 0 ) (t).(75)Letinequality (74) becomes(1F1 (tt0 + 0 (t)))(1F2 (t0 0 +(t))) (1F1 (tt0 + 0 ))(1F2 (t0 0 )). (76)Let us find value x = (t) provides minimum left-hand side (76)fixed t. Let us denoteG(x) = (1 F1 (t t0 + 0 x))(1 F2 (t0 0 + x)).Then,G0 (x) = f1 (t t0 + 0 x))(1 F2 (t0 0 + x)) f2 (t0 0 + x)(1 F1 (t t0 + 0 x)).Since valid (t) interval [t 0 , t1 ] obtains values 0 1 , (75)t0 + 0 x [ 0 , 1 ],t0 0 + x [t0 0 , t1 1 ].Therefore, exist t0 , t00 [t0 , t1 ], 1 (t0 ) = (t0 ) = t0 + 0 x 2 (t00 ) =t00 (t00 ) = t0 0 + x. (73) obtain G0 (x) > 0, meaning G(x) monotonicallyincreases. Besides, (75) x = (t) 0 (since 0 (t) 1), therefore G(x)obtains minimal value x = 0. Therefore, denote Ran(t) set validvalues (t),(1 F1 ())(1 F2 (t )) = (1 F1 (t t0 + 0 (t)))(1 F2 (t0 0 + (t)))min (1 F1 (t t0 + 0 x))(1 F2 (t0 0 + x)) =xRan(t)(1 F1 (t t0 + 0 ))(1 F2 (t0 0 )),strict equality occurs (t) = 0 + 0 . Thus,(1 F1 ())(1 F2 (t )) (1 F1 (e))(1 F2 (te))[t0 , ].Let us show correctness statement interval [t , t1 ],equivalent inequality(1 F1 ((t)))(1 F2 (t (t))) (1 F1 ( 1 ))(1 F2 (t 1 )).(77)proof similar. Let(t) = 1 (t).128(78)fiOptimal Schedules Parallelizing Anytime Algorithmsinequality (77) becomes(1 F1 ( 1 (t)))(1 F2 (t 1 + (t))) (1 F1 ( 1 ))(1 F2 (t 1 )).(79)before, find value x = (t) provides minimum left-hand side(79)G(x) = (1 F1 ( 1 x))(1 F2 (t 1 + x)).derivative G(x)G0 (x) = f1 ( 1 x))(1 F2 (t 1 + x)) f2 (t 1 + x)(1 F1 ( 1 x)),since valid (t) interval [t 0 , t1 ] obtains values 0 1 , (78)1 x [ 0 , 1 ],1 + x [t0 0 , t1 1 ].Therefore, exist t0 , t00 [t0 , t1 ], 1 (t0 ) = (t0 ) = 1 x 2 (t00 ) =t00 (t00 ) = 1 + x. (73), G0 (x) > 0, therefore G(x) monotonically increases.Since x = 1 (t) 0, G(x) G(0). Thus, [t , t1 ],(1 F1 ())(1 F2 (t )) = (1 F1 ( 1 (t)))(1 F2 (t 1 + (t)))min (1 F1 ( 1 x))(1 F2 (t 1 + x)) = (1 F1 ( 1 ))(1 F2 (t 1 )),xRan(t)strict equality occurs (t) = 1 .Combining result previous one, obtain(1 F1 ())(1 F2 (t )) (1 F1 (e))(1 F2 (te))holds every [t0 , t1 ]. Sincee(t) behaves (t) outside interval, E u () Eu (e).Besides, since equality obtainede, since E u () optimal,obtaine, therefore first process take resources interval[t0 , t1 ].proof n processes exactly same. Let { } provide optimal solution,point t0 process k, j 6= khk (k (t0 )) > hj (j (t0 )).continuity functions h (i (t)) point t0 , follows existsneighborhood U (t0 ) t0 ,min hk (k (t0 )) > max max hi (i (t00 )).i6=k t00 U (t0 )t0 U (t0 )Let us take process l 6= k, lety(t) = k (t) + l (t).129(80)fiFinkelstein, Markovitch & Rivlinrepeat proof substituting y(t) instead functionsign:k (t),y(t) y(t0 ) + k (t0 ),ek (t) =(t ),k 1k (t),y(t) y(t0 ),y(t0 ) y(t) y(t0 ) + k (t1 ) k (t0 ),y(t0 ) + k (t1 ) k (t0 ) y(t) y(t1 ),y(t) y(t1 ).substitution produces valid schedule due monotonicity y(t). restproof remains unchanged.Q.E.D.A.6 Proof Theorem 5claim theorem follows:active process remain active consume resources long hazardfunction monotonically increasing.Proof: proof contradiction. Let { j } form optimal schedule. Assumepoint t1 process Ak suspended, hazard function h k (k (t1 ))monotonically increasing t1 .Let us assume first point 2 process Ak becomes active again. Sinceconsider case making process active single point, exists > 0,Ak active intervals [t1 , t1 ] [t2 , t2 + ]. Ak stoppedpoint monotonicity hazard function, therefore, Theorem 4,intervals Ak active process. consider two alternative scenarios. firstone, allow Ak active additional time starting 1 (i.e., shifting idleperiod ), second suspend k earlier.first scenario, scheduling functions following form:k (t),t1 ,(t)+(t),t1 t1 + ,1k 1ka (t) =(t ) + = k (t1 ) + , t1 + t2 + ,kk (t),t2 + ;t1 ,j (t),j (t1 ),t1 t1 + ,ja (t) =(t),1 + t2 + ,jj (t),t2 + .(81)(82)possible see scheduling functions continuous satisfy invariant (39),makes set suitable candidate optimality.130fiOptimal Schedules Parallelizing Anytime AlgorithmsSubstituting values (6), obtainEu (1a , . . . , na )Zt1 +Z(1 Fj (j (t)))dt+j=1t2 +t1 +0Z0nt1(1 Fk (k (t1 ) + (t t1 )))t1Z=Z(1 Fk (k (t1 ) + ))nt1j=1(1 Fj (j (t)))dt +t2(1 Fk (k (t1 ) + ))t1j6=kj6=kj6=kj6=k(1 Fj (j (t1 )))dt+(1 Fj (j (t )))dt +(1 Fj (j (t1 )))Z(1 Fj (j (t)))dt +Zn(1 Fj (j (t)))dt =t2 + j=1(1 Fk (k (t1 ) + x))dx+0Znt2 + j=1(1 Fj (j (t)))dt.Subtracting Eu (1 , . . . , n ) given (6) Eu (1a , . . . , na ), getEu (1 , . . . , n ) Eu (1a , . . . , na ) =Z t2[(1 Fk (k (t))) (1 Fk (k (t1 ) + ))](1 Fj (j (t)))dt+t1Zj6=knt2 +t2(1 Fj (j (t)))dtj=1j6=k(1 Fj (j (t1 )))Z(83)(1 Fk (k (t1 ) + x))dx.0Let us consider first term last equation. Since interval [t 1 , t2 ] k (t) = k (t1 ),interval(1 Fk (k (t))) (1 Fk (k (t1 ) + )) = (1 Fk (k (t1 ))) (1 Fk (k (t1 ) + )) =ZZZhk (k (t1 ) + x)(1 Fk (k (t1 ) + x))dx.fk (k (t1 ) + x)dx =d(1 Fk (k (t1 ) + x)) =000Due monotonicity hk (k ) t1 ,(1 Fk (k (t))) (1 Fk (k (t1 ) + )) =ZZhk (k (t1 ) + x)(1 Fk (k (t1 ) + x))dx > hk (k (t1 ))0leadsZ t2t1[(1 Fk (k (t))) (1 Fk (k (t1 ) + ))]hk (k (t1 ))Z0(1 Fk (k (t1 ) + x))dx131Zt2j6=kt1 j6=k0(1 Fk (k (t1 ) + x))dx,(1 Fj (j (t)))dt >(1 Fj (j (t)))dt.(84)fiFinkelstein, Markovitch & RivlinLet us consider second term (83). Since interval [t 2 , t2 + ] Akactive, intervalj (t2 ),j 6= k,j (t) =k (t1 ) + (t t2 ), j = k.Thus,Znt2 +t2j=1(1 Fj (j (t)))dt =j6=kZ(1 Fj (j (t2 )))(1 Fk (k (t1 ) + x))dx.0(85)Substituting (84) (85) (83), obtainZEu (1a , . . . , na )(1 Fk (k (t1 ) + x))dxEu (1 , . . . , n )>0(86)Z t2hk (k (t1 ))(1 Fj (j (t)))dt +(1 Fj (j (t2 )))(1 Fj (j (t1 ))) .t1 j6=kj6=kj6=kproof second scenario, k suspended , similar.scenario, scheduling functions k (t) j (t) j 6= k represented follows:k (t),t1 ,k (t1 ) = k (t1 ) ,t1 t2 ,ki (t) =(87)(t)+(t(t))=(t)+(t),t2 2 ,22k 1k 1k (t),t2 ;j (t),t1 ,(t+),j1 t2 ,(88)ji (t) =(t ),t2 2 ,j 2j (t),t2 .before, scheduling functions continuous satisfy invariant (39).Substituting (6), obtainEu (1i , . . . , ni )Zt2t1Z0Z0nt1(1 Fj (j (t)))dt+j=1(1 Fk (k (t1 ) ))t2t2Z=Zj6=k(1 Fk (k (t1 ) + (t t2 )))nt1(1 Fj (j (t)))dt +j=1t2t1(1 Fj (j (t + )))dt+(1 Fk (k (t1 ) ))j6=kj6=kj6=k(1 Fj (j (t2 )))dt +(1 Fj (j (t2 )))(1 Fj (j (t)))dt +132ZZZ0nt2j=1(1 Fj (j (t)))dt =(1 Fk (k (t1 ) x))dx+n(1 Fj (j (t)))dt.t2 + j=1fiOptimal Schedules Parallelizing Anytime AlgorithmsSubtracting Eu (1 , . . . , n ) given (6) Eu (1i , . . . , ni ), getEu (1 , . . . , n ) Eu (1i , . . . , ni ) =Z t2[(1 Fk (k (t))) (1 Fk (k (t1 ) ))](1 Fj (j (t)))dt+t1Zj6=kt1nt1 j=1(1 Fj (j (t)))dtj6=k(1 Fj (j (t2 )))Z(89)0(1 Fk (k (t1 ) x))dx.first scenario, interval [t 1 , t2 ](1 Fk (k (t))) (1 Fk (k (t1 ) )) = (1 Fk (k (t1 ))) (1 Fk (k (t1 ) )) =Z 0Z 0fk (k (t1 ) + x)dx =d(1 Fk (k (t1 ) + x)) =Z0fk (k (t1 ) x)dx =Zhk (k (t1 ) x)(1 Fk (k (t1 ) x))dx.0Due monotonicity hk (k ) t1 ,(1 Fk (k (t))) (1 Fk (k (t1 ) )) =ZZhk (k (t1 ) x)(1 Fk (k (t1 ) x))dx > hk (k (t1 ))0leadsZ t2t1[(1 Fk (k (t))) (1 Fk (k (t1 ) ))]hk (k (t1 ))Z0(1 Fk (k (t1 ) x))dxZt2j6=kt1 j6=k0(1 Fk (k (t1 ) x))dx,(1 Fj (j (t)))dt >(90)(1 Fj (j (t)))dt.transformations second term (89) also similar previous scenario.Since interval [t1 , t1 ] Ak active, intervalj (t1 ),j 6= k,j (t) =k (t1 ) (t1 t), j = k.Thus,Zt1nt1 j=1(1 Fj (j (t)))dt =j6=k(1 Fj (j (t1 )))Z0(1 Fk (k (t1 ) x))dx.(91)Substituting (90) (91) (89), obtainZEu (1 , . . . , n ) Eu (1 , . . . , n ) >(1 Fk (k (t1 ) x))dx0Z t2(1 Fj (j (t1 ))) .(1 Fj (j (t2 )))(1 Fj (j (t)))dt +hk (k (t1 ))t1 j6=kj6=kj6=k(92)133fiFinkelstein, Markovitch & Rivlin(86) (92),sign(Eu (1 , . . . , n ) Eu (1a , . . . , na )) = sign(Eu (1 , . . . , n ) Eu (1i , . . . , ni )),(93)therefore one scenarios leads better schedule, contradicts optimality original one.proof case control return k exactlyomitted here. Informally, viewed replacing 2 formulasabove, results same. results.Q.E.D.A.7 Proof Theorem 6claim theorem follows:time cost taken account (c = 1), model shared resources intensity control settings equivalent model independent processessuspend-resume control settings. Namely, given suspend-resume solution modelindependent processes, may reconstruct intensity-based solutioncost model shared resources vice versa.Proof: Let Esharedoptimal value framework shared resources,Eindependent optimal value framework independent processes. Sincec = 1, two problems minimize expression! nZ XnEu (1 , . . . , n ) =i0(1 Fj (j ))dt min,(94)0i=1j=1set {i } satisfying resource sharing constraints automatically satisfiesprocess independence constraints, obtainEindependentEshared.Let us prove.EsharedEindependentAssume set functions 1 , 2 , . . . , n optimal solution problemindependent processes, i.e.,Eu (1 , . . . , n ) = Eindependent.want construct set functions {ei } satisfying resource sharing constrains,Eu (f1 , . . . ,fn ) = Eu (1 , . . . , n ).Let us consider set discontinuity points i0= {t|i : i0 (t ) 6= i0 (t + )}.134fiOptimal Schedules Parallelizing Anytime Algorithmsmodel set countable, write sorted sequence 0 = 0 < t1 <. . . < tk < . . .. expected schedule cost case formEu (1 , . . . , n ) =XEuj (1 , . . . , n ),j=0Euj (1 , . . . , n ) =ZnXtj+1tji=1i0!nl=1(1 Fl (l ))dt.want construct functions ei incrementally. time interval [t j , tj+1 ]define corresponding point tej set functions ei ,! nZ tgnj+1Xg(1 Fl (el ))dt = Eu (1 , . . . n ).el 0E1 , . . . ,fn ) =u (fjjtejl=1l=1Let us denote ij = (tj ) fei (tj ). beginning, fij =i0 = 0 i,0 < j,0definedje(t)definedintervalte0 = 0. Assume tfjtej ej [tej , tg[ftj 0 , t]j+1 ].j 0 +1 ]. Let us showPdefinen0definition tj , k = l=1 l (t) constant [tj , tj+1 ]. Since {i } satisfysuspend-resume constraints, exactly k 1 processes active interval,full intensity. Without loss generality, active processes 1 , A2 , . . . , Ak ,Z tj+1nEuj (1 , . . . , n ) = k(1 Fl (l ))dt =tjkkn(1 Fl (lj ))l=k+1n(1 Fl (lj ))l=k+1ZZtj+1tj0l=1kl=1(1 Fl (t tj + lj ))dt =ntj+1 tjl=1(1 Fl (x + lj ))dx.Let tgei (t) segment [tej , tgj+1 = tej + k(tj+1 tj ), let us definej+1 ] follows:0(t tej )/k + fij , > 0 [tj , tj+1 ]ei (t) =(95)fotherwise.ij ,case, segmentnXl=1ei 0 (t) = 1,means ei satisfy resource sharing constraints. definition,tgj+1 tej = k(tj+1 tj ),therefore processes active [t j , tj+1 ] obtain^fi,j+1ij =tgj+1 tej= tj+1 tj = i,j+1 ij .k135(96)fiFinkelstein, Markovitch & Rivlinprocesses idle [tj , tj+1 ] equality holds well:^fi,j+1ij = 0 = i,j+1 ij ,since ei (t) = 0 obtain invariantfij = ij .(97)average cost new schedules may represented! nZ tgnj+1Xgel 0E1 , . . . ,fn ) =(1 Fl (el ))dt =u (fjnl=k+1(1 Fl (flj ))Ztejl=1kgtj+1tejl=1l=1(1 Fl ((t tej )/k + flj ))dt.Substituting x = (t tej )/k using (95), (96) (97), obtaingE1 , . . . ,fn ) = kuj (fknnl=k+1(1 Fl (lj ))dtl=k+1Euj (1 , . . . , n ).(1 Fl (flj ))Z0ktj+1 tjl=1Z0k(tgj+1 tej )/kl=1(1 Fl (x + flj ))dx =(1 Fl (x + lj ))dx =last equation, immediately followsEu (f1 , . . . ,fn ) =Xj=0gE1 , . . . ,fn ) =uj (fXEuj (1 , . . . , n ) = Eu (1 , . . . , n ),j=0completes proof.Q.E.D.ReferencesBoddy, M., & Dean, T. (1994). Decision-theoretic deliberation scheduling problemsolving time-constrained environments. Artificial Intelligence, 67 (2), 245286.Clearwater, S. H., Hogg, T., & Huberman, B. A. (1992). Cooperative problem solving.Huberman, B. (Ed.), Computation: Micro Macro View, pp. 3370. WorldScientific, Singapore.Dean, T., & Boddy, M. (1988). analysis time-dependent planning. ProceedingsSeventh National Conference Artificial Intelligence (AAAI-88), pp. 4954,Saint Paul, Minnesota, USA. AAAI Press/MIT Press.Finkelstein, L., & Markovitch, S. (2001). Optimal schedules monitoring anytime algorithms. Artificial Intelligence, 126, 63108.136fiOptimal Schedules Parallelizing Anytime AlgorithmsFinkelstein, L., Markovitch, S., & Rivlin, E. (2002). Optimal schedules parallelizinganytime algorithms: case independent processes. Proceedings Eighteenth National Conference Artificial Intelligence, pp. 719724, Edmonton, Alberta, Canada.Gomes, C. P., & Selman, B. (1997). Algorithm portfolio design: Theory vs. practice.Proceedings UAI-97, pp. 190197, San Francisco. Morgan Kaufmann.Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search randomization. Proceedings 15th National Conference Artificial Intelligence(AAAI-98), pp. 431437, Menlo Park. AAAI Press.Horvitz, E. (1987). Reasoning beliefs actions computational resourceconstraints. Proceedings Third Workshop Uncertainty Artificial Intelligence, pp. 429444, Seattle, Washington.Huberman, B. A., Lukose, R. M., & Hogg, T. (1997). economic approach hardcomputational problems. Science, 275, 5154.Janakiram, V. K., Agrawal, D. P., & Mehrotra, R. (1988). randomized parallel backtracking algorithm. IEEE Transactions Computers, 37 (12), 16651676.Knight, K. (1993). many reactive agents better deliberative ones.Proceedings Thirteenth International Joint Conference Artificial Intelligence,pp. 432437, Chambery, France. Morgan Kaufmann.Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.Kumar, V., & Rao, V. N. (1987). Parallel depth-first search multiprocessors. part II:Analysis. International Journal Parallel Programming, 16 (6), 501519.Luby, M., & Ertel, W. (1994). Optimal parallelization Las Vegas algorithms. Proceedings Annual Symposium Theoretical Aspects Computer Science(STACS 94), pp. 463474, Berlin, Germany. Springer.Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup Las Vegas algorithms.Information Processing Letters, 47, 173180.Rao, V. N., & Kumar, V. (1987). Parallel depth-first search multiprocessors. part I:Implementation. International Journal Parallel Programming, 16 (6), 479499.Rao, V. N., & Kumar, V. (1993). efficiency parallel backtracking. IEEE Transactions Parallel Distributed Systems, 4 (4), 427437.Russell, S., & Wefald, E. (1991). Right Thing: Studies Limited Rationality.MIT Press, Cambridge, Massachusetts.Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. ProceedingsTwelfth National Joint Conference Artificial Intelligence (IJCAI-91), pp. 212217,Sydney. Morgan Kaufmann.Simon, H. A. (1982). Models Bounded Rationality. MIT Press.Simon, H. A. (1955). behavioral model rational choice. Quarterly Journal Economics,69, 99118.137fiFinkelstein, Markovitch & RivlinYokoo, M., & Kitamura, Y. (1996). Multiagent real-time-A* selection: Introducingcompetition cooperative search. Proceedings Second International Conference Multiagent Systems (ICMAS-96), pp. 409416.Zilberstein, S. (1993). Operational Rationality Compilation Anytime Algorithms.Ph.D. thesis, Computer Science Division, University California, Berkeley.138fiff fifi fi# $ % & ' (' ) * +4!", + + & - (' % * $ . / 0 ($ ' * 12 35 + - $ 16 7 % 8 $ - 4 9 9 & + % 0 :; < = >= ? @ BCDE C =F G H J H K L N P Q OH ORSUVW X Z[ \ ]^Z_ ]Z ` ZW a[YV Z_b Z_ c dX [^U_ e_ ^f Z[g ^Yhb ZZ[c\i Zf aj k g [a Zl mn p qr < > u> = = > = < vw xJ Nz { L G F | RM N ON|H K G } O{ } Q~ ]X lYh U k _ X g Y[^al _ ^_ ZZ[^_ a_ a_ ZV Z_Z]i _ ^U_a^ p p pj k g [a Zlfi fififiGHIJH K{ K K { K z S|fffifffifffi@ C E > > = w < =!" #$ %&'() *+,-. '/$012314 5$ 612/7489 4(/':&4(5' 3&9 14 &4 ;1<<) =8( ;1 ;5<< $/( $11> (:54 '&?&=5<5(@"ABfi{ H G K RK P |G } RK H| { RM R{ K |ff fiff ff> = w < =ff ffff ffw C ! > C E > " # $ %% & ' () B < * C * v w +#0 (uB C = w < =.& '%,- = + w < = " ./ # ff . ff ff 0 1 1 & '3 & ' & ' & ' & '24 545454567 8 9 :45A;fiGHIJH K.{ K K { K z S|...& '7'45%# ff ff ff## # ffff # ff ff ff # # fffi22 &7#@ - E w < = < E v> wC=w E C v wfi'4 57 1 & ' 4 54545#2 &7&##fi G } RK H| { RM R{ K |{ H G K RK P |#45457.#45.# ff ff .# ff ff .##&# ff .00 # ff ff .064 5&' & &&ff ffff ffffff..fi.u > < B > E & &' & & & %& ' 7 2 && 2 &' &1 7 7 1 (" /29 &<<@) ;1 ;5<< &4489 1 (:&( (:121 54 & '/$4(&$( !) 48': (:&( 5% (:1 $89=12 /% &'(5/$4 &0&5<&=<1 (/&31$( 54 ) (:1$ 5( 54 '/9 9 /$<@ $/;$ (:&( &$@ &31$( '&$ :&01 & 9 /4( /% !" # $ ! % & ' (() &'(5/$4"* (:12 ;&@4 /% %/29 &<565$3 (:54 '/$'1?( &21 (21&(1> 459 5<&2<@"A+fiGHIJH K{ K K { K z S|< B < vvC B & &' &7 2 &&() B< <D "&&% &&454 5.ff ff ff4 54 5fffifffiCs> " < E E <=<B>B< > B < =w C + w < = @fffifffi4 5fi{ H G K RK P |G } RK H| { RM R{ K |fffifffiC > " + < E E < = v= < = < BC + w < = > w x > < C vv C ! > =w @> B = !< > B w > C ! > =wC== < v>!><D wt>fffffifffiffffC > " ) > B D> + wr < = w<B =! @fiGHIJH K{ K K { K z S|C > " = < = C + w < =s>w x>s @00fffi0ff00fffffffffifffffi0fffffffffi{ H G K RK P |G } RK H| { RM R{ K |fffffffffififiCs> "CB>< v = < E C v * < - =< = C +w < =s>w x>s @###fi' fifiC > " ) < v = < E C v * < - =< = C+w < = s>w x>s-== < = E= ! w E > @fffifffiff.fiGHIJH K{ K K { K z S|00ffffff045fffiBfi G } RK H| { RM R{ K |{ H G K RK P |'554554 5fffifffi45# ff ff # fffiff ff#fi # ff./ # ff ff .# .#./ # ff ff./BAfiGHIJH Kfffi{ K K { K z S|4500fffififi # fffiff? = w C v x > "ff ff4 5 0 # ff # ff ff # ff# ff ff # ff ff ff# fifi # ff fi # ./ # ff ff #ff ff ./ # ff ff #% 7 %7%;>>Cw "<E -w>+ w "%C = - C w > "C=* s> B>## ff##ffffff '%u t><B>E11 8 9 : % (9 & % 7 1' ff ff& 7 2 1& 1' 7 & 1'fi . ff ff ( %(fffiBBfi{ H G K RK P |G } RK H| { RM R{ K |1 1 & ' &fffi & % &&fi . ff ff % 7 1 &' & ff ff fffffi(4 574545:&3 &&&7 & 8:8 7&8:&ff 8fi8((8: (&((8: (: (8( ( (: ( ( 7 & :' ff :B;fi