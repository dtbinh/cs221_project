Journal Artificial Intelligence Research 19 (2003) 469-512

Submitted 12/02; published 10/03

Temporal Decision Trees:
Model-based Diagnosis Dynamic Systems On-Board
Luca Console
Claudia Picardi

luca.console@di.unito.it
claudia.picardi@di.unito.it

Dipartimento di Informatica, Universita di Torino,
Corso Svizzera 185, I-10149, Torino, Italy

Daniele Theseider Dupre

dtd@mfn.unipmn.it

Dipartimento di Informatica, Universita del Piemonte Orientale
Spalto Marengo 33, I-15100, Alessandria, Italy

Abstract
automatic generation decision trees based o-line reasoning models
domain reasonable compromise advantages using model-based approach technical domains constraints imposed embedded applications.
paper extend approach deal temporal information. introduce notion
temporal decision tree, designed make use relevant information long
acquired, present algorithm compiling trees model-based
reasoning system.

1. Introduction
embedding software components inside physical systems became widespread
last decades due convenience including electronic control systems themselves. phenomenon occurs several industrial sectors, ranging large-scale products cars much expensive systems like aircraft spacecrafts.
case automotive systems paradigmatic. fact, number complexity
vehicle subsystems managed software control increased signicantly since
mid 80s increase next decades (see Foresight-Vehicle, 2002), due
possibility introducing, costs acceptable wide scale products,
exibility systems, e.g. increased performance safety, reduced
emissions. Systems fuel injection control, ABS (to prevent blockage wheels
braking), ASR (to avoid slipping wheels), ESP (controlling stability vehicle),
would possible feasible costs without electronic control.
software modules usually installed dedicated Electronic Control Units (ECUs)
play important role since complete control subsystem: human control becomes simply input control system, together inputs
appropriate sensors. example, position accelerator pedal input
ECU controls fuel delivery injectors.
serious problem systems software must behave properly also
presence faults must guarantee high levels availability safety controlled
system vehicle. controlled systems, fact, many cases safety
critical: braking system obvious example. means monitoring systems

c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiConsole, Picardi, & Theseider Dupre

behaviour, detecting isolating failures, performing appropriate recovery actions
critical task must performed control software. problem detected
suspected software must react, modifying way system controlled,
primary goal guaranteeing safety availability. According recent estimates,
75% ECU software deals detecting problems performing recovery actions,
tasks diagnosis repair (see Foresight-Vehicle, 2002).
Thus design diagnostic software critical time consuming activity, currently performed manually expert engineers use knowledge
perform Failure Mode Eect Analysis (FMEA) 1 dene diagnostic
recovery strategies.
problem complex critical per-se, made even dicult
number issues constraints taken account:
resources available on-board must limited, terms memory
computing power, keep costs low. combined problem
near real time performance needed, especially situations may safety
critical. example, direct injection fuel delivery systems, fuel maintaned
high pressure (more 1000 bar) cases system must
react problems within rotation engine (e.g. 15 milliseconds 4000 rpm),
prevent serious damage engine danger passengers. fact, fuel
leakage dangerous comes high pressure line. case
important distinguish whether loss pressure due leak, order
activate emergency action (for example, stop engine),
failure simply signalled user.
order keep costs acceptable large scale product, set sensors available
board usually limited necessary controlling systems
correct behaviour; thus, always easy gure impact faults may
quantities monitored sensors, whose physical, logical temporal
relation faults may straightforward.
devices diagnosed complex behavioural point view:
dynamic time-varying behaviour; embedded complex systems
interact subsystems; cases control system automatically
compensates deviations nominal behaviour.
aspects make design software modules control diagnosis challenging also expensive time consuming. signicant need
improving activity, making reliable, complete ecient use
automated systems support complement experience engineers, order meet
growing standards required monitoring, diagnosis repair strategies.
Model-based reasoning (MBR) proved interesting opportunity automotive
applications indeed applications real systems experimented
1. result FMEA table lists, possible fault component system,
eect faults component system whole possible strategy detect
faults. table compiled manually engineers, based experience knowledge blueprint
system.

470

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

90s (e.g., Cascio & Sanseverino, 1997; Mosterman, Biswas, & Manders, 1998; Sachenbacher,
Malik, & Struss, 1998; Sachenbacher, Struss, & Weber, 2000). type models adopted
MBR conceptually far away adopted engineers. particular,
component oriented approach typical MBR ts quite well problem dealing
several variants systems, assembled starting set basic components.
thorough discussion advantages MBR approach, see Console
Dressler (1999).
applications developed far, however, concentrated o-board diagnosis,
diagnosis workshop, on-board diagnosis. case on-board
systems seems complicated since, due restrictions hardware
put on-board, still questionable diagnostic systems designed reason
rst-principle models on-board. reason approaches developed
order exploit advantages MBR also on-board applications. particular,
compilation-based scheme design on-board diagnostic systems vehicles
experimented Vehicle Model Based Diagnosis (VMBD) BRITE-Euram Project (199699), applied Common-rail fuel injection system (Cascio, Console, Guagliumi, Osella,
Sottano, & Theseider, 1999). approach model-based diagnostic system used
generate compact on-board diagnostic system form decision tree. Similarly,
automated FMEA reports generated Model-Based Reasoning Autosteve system
used generate diagnostic decision trees (Price, 1999). Yet similar idea
proposed Darwiche (1999), diagnostic rules generated model order
meet resource constraints.
approaches interesting advantages. one hand, share
benets model-based systems, relying comprehensive representation
system behaviour well dened characterization diagnosis.
hand, decision trees compact representations make sense representing onboard diagnostic strategies, ecient space time. Furthermore, algorithms
synthesizing decision trees examples well established machine learning
community. specic case examples solutions (diagnoses recovery
actions) generated model-based system.
However, basic notion decision tree approaches learning trees
examples major limitation kind applications: cope
properly temporal behaviour systems diagnosed, and, particular,
fact incremental discrimination possible faults, leading nal decision
action taken on-board, based observations acquired across time,
thus taking account temporal patterns.
reason, work described paper introduce new notion
decision tree, temporal decision tree, takes account temporal dimension,
introduce algorithm synthesizing temporal decision trees.
Temporal decision trees extend traditional decision trees fact nodes
temporal label species condition checked order select one
branches make decision. shall see, allows taking account
cases order delay observable measures inuences
decision made thus provides important power improve decision process.
Waiting, however, always possible thus generation trees includes
471

fiConsole, Picardi, & Theseider Dupre

notion deadline possible decision. Thus, temporal decision process supports
possibility selecting best decision, exploiting observations temporal
locations (patterns) taking account cases point decision
taken anyway prevent serious problems.
rest paper organized follows. section 2 summarize basic
ideas model-based diagnosis (MBD), use decision trees conjunction it,
temporal dimension MBD decision trees. section 3 provide basic
formal denitions decision trees, form basis extension temporal
decision trees section 4. describe (section 5) problem synthesizing temporal
decision trees solution (section 6).

2. Model-based Diagnosis Decision Trees
section briey recall basic notions model-based diagnosis discuss
decision trees used diagnostic purposes, focusing
used VMBD conjunction model-based approach (Cascio et al., 1999).
2.1 Atemporal Case
First let us sketch atemporal case traditional use diagnostic decision
trees.
2.1.1 Atemporal model-based diagnosis
starting point model-based diagnosis model structure behaviour
device diagnosed. specically, assume component centered approach
which:
model provided component type; component characterized
set variables (with distinguished set interface variables);
set modes, including ok mode (correct behaviour) possibly set
fault modes.
set relations involving component variables modes, describing behaviour component mode. relations may model correct
behaviour device and, cases, behaviour presence faults
(faulty behaviour).
model device given list component instances
connections (connections interface variables).
Articial Intelligence approach, models usually qualitative, domain
variable nite set values. abstraction proven useful
diagnostic purposes.
model used simulating behaviour system computing
diagnoses. fact, given set observations system behaviour, diagnoses
determined comparing behaviour predicted model (in normal conditions
presence single multiple faults) observed behaviour.
472

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

order model useful on-board diagnosis, fault mode F (or
combination fault modes) model must include recovery action control software
perform case F occurs. general actions cost, mainly related
resulting reduction functionality system. Moreover, two actions a1 a2
related following sense:
a1 , recovery action associated fault F1 , carries operations include
performed a2 , recovery action associated F2 ;
a1 used recovery action also F2 occurs; may however carry
unneeded operations, thus reducing system functionality strictly necessary.
However, case cannot discriminate F1 F2 , applying a1 rational
choice.
section 4.3 present model actions formalizes relation.
Thus main goal on-board diagnostic procedure decide best action
performed, given observed malfunction. type procedure eciently
represented using decision trees.
2.1.2 Decision trees
Decision trees used implement classication problem solving thus form
diagnostic procedure. node tree corresponds observable. on-board
diagnosis, observables correspond either direct sensor readings, results computations carried ECUs available measurements. following
word sensor denote types observables; worth pointing latter
may require time performed. paper mainly assume sensor
reading takes time; however apporach propose deals also case
sensor reading time consuming, pointed section 5.1. node many
descendants number qualitative values associated sensor. leaves
tree correspond actions performed board. Thus, given available
sensor readings, tree easily used make decision recovery action
performed.
decision trees generated automatically set examples cases.
example mean possible assignment values observables corresponding diagnosis, possible alternative diagnoses, selected recovery action
appropriate set suspect diagnoses. set produced using modelbased diagnostic systems, which, given set observables compute diagnoses
recovery actions.
atemporal case, nite qualitative domains, number possible combinations observations nite, usually small, therefore considering cases exhaustively
(and sample) feasible two equivalent ways building
exhaustive set cases:
1. Simulation approach: fault F , run model-based system predict
observations corresponding F .
473

fiConsole, Picardi, & Theseider Dupre

2. Diagnostic approach: run diagnosis engine combinations (all relevant combinations) observations, compute candidate diagnoses one
cases.
either case, resulting decision tree contains information set cases;
if, sensors placed system, observations cost, decision
tree way save space respect table, speed lookup information.
way advantages model-based approach use compact decision trees combined: model-based engine produces diagnoses based reusable
component models used diagnoser o-board; compact decision trees,
synthesized cases classied model-based engine, installed on-board.
2.2 Towards Temporal Decision Trees
section briey recall basic notions temporal model-based diagnosis (see
Brusoni, Console, Terenziani, & Theseider Dupre, 1998 general discussion temporal
diagnosis), informally introduce temporal decision trees.
2.2.1 Temporal MBD
basic denition MBD conceptually similar atemporal case. Let us consider
main dierences.
regards model component type consider type variable:
state variables used model dynamic behaviour component. set relations
describing behavior component (for mode) augmented temporal
information (constraints); make specic assumptions model time, even
though, shall see following, impact cases
considered tree generation. example, constraints may specify delay
input output change state component.
regards recovery actions, deadline performing action must specied;
represents maximum time elapse fault detection recovery action; amount time available control software perform discrimination.
piece information specic component instance, rather component
type, action deadline related potential unacceptable eects
fault could overall system; fault component type
could dangerous one instance tolerable another.
Diagnosis started observations indicate system behaving correctly.
Observations correspond (possibly qualitative) values variables across time. general,
temporal case diagnosis assignment mode behaviour component
instances across time observed behaviour explained assignment given
model. details dierent ways dening explanation case see Brusoni et
al. (1998). purposes paper interested fact that, given
set observables, diagnosis (or set candidate diagnoses complete discrimination
possible) computed recovery action determined.
means starting point approach table containing results
running model-based diagnostic system set cases, (almost) independently
model-based diagnostic system used generating table.
474

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

already mentioned static case, nite qualitative domains, exhaustive set cases considered. temporal case, model time purely
qualitative, table temporal information cannot built prediction,
built running diagnosis engine set cases quantitative information: diagnoses
make qualitative predictions inconsistent quantitative information
ruled out. course, cannot general done exhaustively, even observations assumed acquired discrete times; not, decision tree generation
actually learning examples.
Thus simulation approach used case temporal constraints
model precise enough predict least partial information temporal location
observations, e.g., case model includes quantitative temporal constraints.
diagnostic approach used also case weaker (qualitative) temporal constraints model.
regards observations, consider general case set snapshots
available; snapshot labelled time observation reports value
sensors (observables) time. makes approach suitable dierent notions
time underlying model observations (see discussion Brusoni
et al., 1998).
Example 1 starting point generating temporal decision tree table like
one Figure 1.

sit1
sit2
sit3
sit4
sit5
sit6
sit7
sit8

0
n
h
n
n
h
n
h
h

1
n
h
n
n
h
n
h
h

2
n
h
n
n
h
n
h
h

s1
3 4 5 6 7 0
n h h
l
h
n h h h h l
h h h h
l
h
h
h h h
l
h h h
l
h h h
h

1
v
n
l
l
n
v
l
h

2
v
n
l
l
n
v
n
n

s2
3 4 5 6 7 0
v v v
n
n
l v v v v n
l l l v
n
n
n
z z z
n
n l v
n
n l l
n

1
n
n
n
n
n
n
n
n

2
n
n
n
h
n
n
n
n

s3
3 4 5 6 7 Act Dl
l l l

5
b
2
l l l v v b
7
h h h h
c
6
l
c
3
l l v

5
l l v
b
5
l v z
c
5

Figure 1: example set cases learning temporal decision trees.
row table corresponds situation (case example terminology
machine learning) reports:
sensor si values observed snapshots (in example
8 snapshots, labelled 0 7); n, l, h v correspond qualitative
values sensor measurements; n normal, l low, h high, v low
z zero.
recovery action Act performed situation.
deadline Dl performing action.



table one example represents set situations may
encountered case faults and, noticed above, generated using either
475

fiConsole, Picardi, & Theseider Dupre

diagnostic simulation approach. next section shall introduce notion
temporal decision trees show pieces information sensor histories like
table exploited generation trees.
2.2.2 Introduction temporal decision trees
Traditional decision trees include notion time, i.e., fact data may
observable dierent times dierent faults may distinguished
temporal patterns data. Thus, neglecting notion time may lead limitations
decision process.
reason work introduce notion temporal decision tree. Let us
analyse intuition behind temporal decision trees decision process support.
Formal denitions provided later paper.
Let us consider, example, fault situations sit3 sit4 Figure 1, let
us assume, sake simplicity, available sensor s2 . two fault
situations distinguished control software require dierent
recovery actions. detected fact s2 shows low value.
Moreover, situation s2 starts showing low value.
way discriminate two situations make use temporal information,
exploit fact sit3 value v shows 4 time units fault detection,
sit4 value shows 6 time units.
order take account decision tree, include time
tree. examples, best decision procedure wait observing thst s2 = l
(that is, dectecting fault occurred). 4 time units make
decision, depending whether s2 = v not. corresponds procedure described
tree Figure 2.
s2
...
...

l
s2 4
l
sit4

v
sit3

Figure 2: simple example temporal decision tree
Obviously, waiting always solution always possible. many cases,
fact, safety constraints may impose deadlines performing recovery actions.
reected generation decision procedure. Suppose, example
above, deadline sit3 3 rather 6: case two situations would
indistinguishable, beacause would infeasible wait 4 time units.

476

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

Thus, essential idea generating small decision trees temporal case
take advantage fact cases nothing better waiting2 , order
get good discrimination, provided safety integrity physical system
kept account, deadlines recovery actions always met.
generally, one exploit information temporal patterns observables deadline, like ones Figure 1, produce optimal diagnostic
procedure.
Another idea use approach integration incremental discrimination,
basis generation traversal decision tree, incremental
acquisition information across time.
atemporal case, decision tree generated order guide incremental acquisition information: dierent subtrees node relative dierent sets
faults therefore may involve dierent measurements: subtree perform
measurements useful discriminating faults compatible measurements made us reach subtree T, starting root. o-board diagnosis,
allows reducing average number measurements get decision (i.e. average
depth tree), useful measurements cost - e.g. time operator take system; on-board diagnosis, even case measurements
simply sensor readings, cost sensor made part
system, interested generating small decision trees save space.
temporal case issue: incremental acquisition information
naturally constrained ow time. want store sensor values across
time seems natural choice since memory constraints information must
acquired available possible read sensors choice
waiting made. issue taken account generation temporal
decision trees.

3. Basic Notions Decision Trees
moving formal denition temporal decision trees, section briey
recall denitions algorithms atemporal case. particular, recall
standard ID3 algorithm (Quinlan, 1986), basis algorithm
temporal case. denitions section standard ones (see textbook
Articial Intelligence, e.g., Russel & Norvig, 1995).
3.1 Decision Trees
adopt following formal denition decision tree, extended section
4.1 temporal decision trees.
Definition 1 Let us consider decision process P set available decisions,
set tests performed external environment, out(oi ) =
2. dierent approach would weighing amount elapsed time agains possibility
better discriminating faults; approach something considering future work
temporal decision trees, outlined section 7.

477

fiConsole, Picardi, & Theseider Dupre

{v1 , . . . , vki } possible outcomes test oi O. decision tree P labelled
tree structure = r, N, E, L where:
r, N, E tree structure root r, set nodes N set edges E N N ;
N partitioned set internal nodes NI set leaves NL .
L labelling function dened N E.
n NI , L(n) O; words internal node labelled name
test.
(n, c) E L((n, c)) out(L(n)); is, edge directed n c
labelled possible outcome test associated n.
Moreover, (n, c1 ), (n, c2 ) E L((n, c1 )) = L((n, c2 )) c1 = c2 ,
v out(L(n)) c
(n, c) E L((n, c)) = v; is, n exactly one outgoing edge possible
outcome test L(n).
l NL , L(l) A; words leaf labelled decision.



decision-making agent uses tree, starts root. Every time reaches
inner node n, agent performs test L(n), observes outcome v follows vlabelled edge. agent reaches leaf l, makes decision = L(l).
3.2 Building Decision Trees
Figure 3 shows generic recursive algorithm used build decision tree
starting set Examples set Tests.
Recursion ends either remaining examples need discrimination
correspond decision, available observables used,
values observed match cases dierent decisions. latter case observables
enough getting proper decision agent actually using tree,
take account this.
case terminating condition holds, algorithm chooses observable variable
test become root label subtree T. Depending ChooseTest implemented get dierent specic algorithms dierent decision trees.
subtree built possible outcome value test recursive call
BuildTree, sets Tests Update SubExamples inputs. Tests Update obtained removing test set tests, order avoid using again. SubExamples
subset Examples containing examples value outcome
test.
mentioned before, many specic algorithms, and, general, results,
implementations ChooseTest. general desirable generate tree
minimum average depth, two reasons:
Minimizing average depth means minimizing average number tests thus
speeding decision process.
machine learning, small number tests also means higher degree generalization particular examples used building tree.
478

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

function BuildTree (set Examples, set Tests)
returns decision tree = root, Nodes, Edges, Labels
begin
ex Examples correspond decision
return BuildLeaf(Examples);
Tests empty
return BuildLeaf(Examples);
test ChooseTest (Tests, Examples);
root new node;
Nodes {root}; Edges ; Labels(root) test;
root, Nodes, Edges, Labels;
Tests Update Tests \ {test};
possible outcome value test begin
SubExamples {ex Examples | test outcome value ex};
SubExamples empty begin
SubTree BuildTree (SubExamples, Tests Update);
Append(T, root, SubTree);
Labels((root, Root(SubTree))) value;
end;
end;
return T;
end.

Figure 3: Generic algorithm building decision tree
3.3 ID3
Unfortunately, nding decision tree minimum average depth intractable problem; however, exists good best-rst heuristic choosing tests order produce
trees deep. heuristic proposed ID3 algorithm (Quinlan,
1986), base concept entropy information theory. following
recall approach detail also order introduce notation
used rest paper.
Definition 2 Given (discrete) probability distribution P = {p1 , . . . , pn } entropy E(P)
is:
(1)

E(P) =

n


pi log2 pi

i=1


Entropy measures degree disorder. choose test, want split
examples lowest degree disorder respect decisions associated
them.
Given set examples E introduce sets:
E |a = {e E | decision associated example e a}.
set available decisions = {a1 , . . . , } E |A = {E |a1 , . . . , E |an } partition E.
479

fiConsole, Picardi, & Theseider Dupre

Definition 3 ai A, = 1, . . . , n, dene probability ai respect
E follows:
|(E |ai )|
P (ai ; E) =
|E|


worth noting that, examples endowed priori probabilities,
redene P (ai ; E) order take account. basic formulation ID3 assumes
however examples equiprobable computes probability distribution
frequencies examples.
entropy E is:
E(E) =

(2)

n


P (ai ; E) log2 P (ai ; E).

i=1

decisions equiprobable, get E(E) = log2 n, maximum degree
disorder n decisions. decisions one probability equal 0, E(E) = 0:
degree disorder minimum.
Entropy used follows test selection. test possible outcomes v1 , . . . , vk ,
splits set examples into:
E |ovi = {e E | test value vi e}.
E |o = {E |ov1 , . . . , E |ovk } partition E. particular, building
tree choose test o, E |ovj subset examples use building subtree
child corresponding vj . lowest degree disorder E |ovj , closer
leaf. Therefore, following equation (2):
E(E |ovj ) =

n


P (ai ; E |ovj ) log2 P (ai ; E |ovi ).

i=1

Finally, dene entropy test average entropy possible outcomes:
Definition 4 entropy test respect set examples E is:
E(o; E) =

(3)

k


P (o vj )E(E |ovj ),

j=1

where3 P (o vj ) =

|(E |ovj )|
|E|



.

ID3 algorithm simply consists choosing test lowest entropy. Figure 4 shows
implementation ChooseTest yields ID3.
3. Again, examples endowed priori probabilities, denition changed order
take account

480

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

function ID3ChooseTest (set Tests, set Examples)
returns test best test
begin
best test element Tests;
min entropy log2 |Examples|;
test Tests begin
part Partition({Examples}, test);
ent Entropy(part);
(ent < min entropy) begin
min entropy ent;
best test test;
end;
end;
return best test;
end.

Figure 4: ID3 implementation ChooseTest

4. Extending Decision Trees
section formally introduce notion temporal decision tree, show
timing information added set examples used tree building. Moreover
introduce model recovery action expresses information needed algorithm.
4.1 Temporal Decision Trees
section 2.2.2 motivated monotonicity requirement temporal decision trees,
traversal requires information relative increasing time information
stored.
discuss temporal information actually included tree matched
temporal information observations. tree used abnormal value detected sensor (fault detection). intend time fault
detection reference time temporal labels observations tree. look
example data shown Figure 1, see that, every fault situation,
always least one sensor whose value time 0 dierent nnormal. reason
that, fault situation, associate 0 time label rst snapshot
sensor showing deviation nominal behaviour.
following denition provides extension temporal dimension decision
trees.
Definition 5 temporal decision tree decision tree r, NI , NL , E, L endowed
time-labelling function that:
(1) : NI IR+ ; call (n) time label;
(2) n NI exist n (n, n ) E (in words, n child n),

(n ) (n).

481

fiConsole, Picardi, & Theseider Dupre

Since assume store information, rather use information traversing tree dictated tree itself, rst branching discrimination provided
depending sensor provided value. assume
dierent temporal decision trees, one sensor could possibly provide rst
abnormal value, or, alternatively, root node time label, edges
root labelled dierent values single observable, dierent
sensors could provide rst abnormal observation. tree, subtree
second alternative, generated independently ones, using examples sensor providing fault detection same. generation
described rest paper.
Let us tree temporal decision tree (or forest, case multiple detecting
sensors) exploited on-board diagnostic agent order choose recovery
action. rst abnormal value detected, agent activates time counter
starts visiting appropriate tree root. reaches inner node n,
agent retrieves associated test = L(n) time label = (n).
waits time counter reaches t, performs test chooses one child nodes
depending outcome. agent reaches leaf, performs corresponding
recovery action.
respect atemporal case, agent option wait.
point view agent may seem pointless wait could look sensor values,
since reading sensor values cost. However, point view tree things
quite dierent: want add test makes tree deeper
time necessary.
Condition (2) states agent move forward time. corresponds
assumption sensor readings stored, discussed section 2.2.2.
Example 2 Let us consider diagnostic setting described example 1. Figure 5 shows
temporal decision tree setting, temporal decision tree uses sensors
recovery actions mentioned Figure 1. tree run fault situations

table, proper recovery action selected within deadline.

4.2 Adding Timing Information Set Examples
order generate temporal decision trees, need temporal information examples.
already introduced informally notion set examples (or fault situations)
temporal information describing table Figure 1. following denition
formalizes notion.
Definition 6 temporal example-set (te-set short) E collection fault situations
sit1 , . . . , sitn characterized number sensors sens1 , . . . , sensm ascending sequence time labels t1 < . . . < tlast representing instants time sensor
readings available. context call observation pair sensi , tj . te-set
organized table follows:
(1) table n rows, one fault situation.
482

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

low
high

s1,0

normal

Action: b

Action:

normal

s2,1

Actions: b,c
low

low

high

s2,0

s2,3

s2,1

Action: c

low
normal

zero

Action:

high

Action: b

s3,2

high

Action: c

Figure 5: temporal decision tree situations described Figure 1.
(2) table last observation columns containing outcomes dierent
observations fault situation. denote Val(sith , sensi , tj ) value
measured sensor sensi time tj fault situation sith .
(3) table distinguished column Act containing recovery action associated
fault situation. denote Act(sith ) recovery action associated
sith .
(4) table second distinguished column Dl containing deadline
fault situation. denote Dl(sith ) deadline sith ,
Dl(sith ) {t1 , . . . , tlast } h = 1, . . . , n. dene global deadline
te-set Dl(S) = min{Dl(sit) | sit S}.
4 P (sit; E) associated sit E ,

moreover assume probability


sitE P (sit; E) = 1. every E E every sit E introduce following


notation: P (E ; E) = sitE P (sit; E) P (sit; E ) = PP (sit;E)
(E ;E) .

4.3 Model Recovery Actions
algorithms shall introduce require detailed model recovery actions.
particular want better characterize happens possible uniquely
identify appropriate recovery action. Moreover, want quantify loss
incur happens.
start formal denition:
Definition 7 basic model recovery actions triple A, , where:
(1) = {a1 , . . . , aK } nite set symbols denoting basic recovery actions.
4. P (sit; E) computed frequency, P (sit; E) = 1/n, n number fault
situations, known priori added set examples.

483

fiConsole, Picardi, & Theseider Dupre

(2) partial strict order relation A. say ai weaker
aj , written ai aj , aj produces recovery eects ai , sense
aj could used place ai (but vice versa). therefore assume
drawbacks actions, action performed
time negative consequences, apart cost action (see
below). clearly limitation something tackled future work (see
discussion section 7).
(3) : IR+ cost function, ai aj (ai ) < (aj ).
associates cost basic recovery action, expressing possible drawbacks
action itself. Recovery actions performed on-board usually imply performance
limitation abortion ongoing activity; costs meant estimate
monetary losses inconveniences users resulting these. requirement monotonicity respect stems following consideration:
ai aj (ai ) (aj ) would make sense ever perform ai , since aj
could performed eects (or lower) cost. moreover
assume costs independent fault situation (a consequence
no-drawbacks assumption mentioned previous point).

Example 3 Let us consider four recovery actions a, b, c, appear teset Figure 1. Figure 6 shows basic action model them. graph expresses

oredering relation , costs shown next action names.
- : 100

b - : 20

c - : 50

- : 10

Figure 6: basic action model.
seen previous section fault situation associated
recovery action; usually association depends fault, may also depend
operating conditions fault occurs.
happens cannot discriminate multiple fault situations? section 3.2,
outlining generic algorithm atemporal case, referred solution
decision-making agent. case want precise.
Definition 8 Let A, , basic model recovery actions. dene function
merge : 2A 2A follows:
(4)

merge(S) = {ai | aj ai aj }
484

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

moreover dene:
(5)

merge-set(A) = {merge(S) | 2A } 2A

merge-set(A) set compound recovery actions includes basic recovery actions

form singletons.
intuition behind merge cannot discriminate multiple fault situations
simply merge corresponding recovery actions. means collect recovery
actions, remove unnecessary ones (equation (4)). action set becomes
unnecessary set contains stronger action. Thus given te-set E, dene:
(6)

Act(E) = merge({Act(sit) | sit E}).

take account compound actions, extend notion model recovery
actions follows:
Definition 9 Let = A, , basic model recovery actions. extended model
triple A, ext , ext where:
(1) ext merge-set(A) merge-set(A) given A, merge-set(A), ext
every either exists . Notice
{ai } ext {aj } ai aj .
actions
(2) ext : merge-set(A) IR+ cost function compound

every merge-set(A), maxaA (a) ext (A) aA . Moreover, ext
must hold ext (A) < ext (A ).
ext uniquely determined , hold ext : reason one extended model basic model. requirement
maxaA (a) ext (A) motivated follows: existed ext (A)
(a) would make sense never perform a, substituting A. fact, {a} ext
would lower cost. also ask ext - basic models monotonic respect ext .
following shall consider extended models recovery actions, thus
shall drop ext prex .
Example 4 Figure 7 shows possible extension basic model Figure 6.
case cost compound action {b, c} given sum individual costs b

c.

5. Problem Building Temporal Decision Trees
section outline peculiarities building temporal decision trees, showing
dierences respect traditional case.

485

fiConsole, Picardi, & Theseider Dupre

{a} - : 100
{b, c} - : 70

{b} - : 20

{c} - : 50

{d} - : 10

Figure 7: extended action model.
5.1 Challenge Temporal Decision Trees
makes generation temporal decision trees dicult standard ones
requirement time labels decrease moving root leaves:
corresponds assuming sensor values cannot stored; decision-making
agent decides wait gives using values sensors show waiting.
release restriction actually generate temporal decision trees
minor variation ID3, essentially considering pair formed sensor time
label individual test. words ID3ChooseTest selects sensor time
label reading time allows maximum discrimination among examples.
However systems ones considering, low-memory real-time systems,
possibility performing diagnostic task without discarding dynamics also
without store sensor values across time serious issue take account.
reason denition temporal decision tree includes requirement time
labels decreasing root-leaf paths.
Figure 8 shows generic algorithm building temporal decision trees help us
outline diculties task. Line 8 shows minor modication aimed taking
account deadlines: observation used given set examples time
label exceed global deadline. Violating condition would result tree
selects recovery action deadline corresponding fault situation
expired.
major change respect standard algorithm however shown line
15: select observation pair sensor, tlabel must remove set
observations pairs whose time label lower tlabel5 .
consequence operations - ruling invalid observations discarding
past - set observations available building child node
dierent one used parent one way:
5. Actually assumes reading sensor moving downwards tree accordingly done
swiftly qualitative sensor values time change meanwhile.
case, one choose remove also pairs time labels equal tlabel, generally
time labels lower tlabel + k k time needed diagnostic agent carry
tree operations. sake simplicity, however, following assume k 0, since
choice k aect approach propose.

486

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

function BuildTemporalTree (te-set Examples, set Obs, action model ActModel)
returns temporal decision tree = root, Nodes, Edges, Labels, TLabels
begin
sit Examples Act(sit)
return BuildLeaf(Examples, ActModel);
deadline Dl(Examples);
UsefulObs {o Obs | s1, s2 Examples s.t. Val(s1, o) = Val(s2, o)};
ValidObs {sensor, tlabel UsefulObs | tlabel deadline};
ValidObs empty
return BuildLeaf(Examples, ActModel);
sensor, tlabel ChooseObs (ValidObs, Examples, ActModel);
root new node;
Nodes {root}; Edges ; Labels(root) sensor; TLabels(root) tlabel;
root, Nodes, Edges, Labels, TLabels;
Obs Update {sens, tst UsefulObs | tst tlabel};
possible measure value sensor begin
SubExamples {sit Examples | Val(sit, sensor, tlabel) = value};
SubExamples empty begin
SubTree BuildTemporalTree (SubExamples, Obs Update, ActModel);
Append(T, root, SubTree);
Labels((root, Root(SubTree))) value;
end;
end;
return T;
end.

26
27
28
29
30
31
32
33

function BuildLeaf (te-set Examples, action model ActModel)
returns loose temporal decision tree = leaf, {leaf}, , Labels,
begin
act {Act(sit) | sit Examples};
comp act merge(all act);
leaf new node; Labels(leaf) comp act; leaf, {leaf}, , Labels, ;
return T;
end.

Figure 8: Generic algorithm building temporal decision tree
observations invalid parent valid child. recursive
call child works smaller set examples; therefore global deadline may
ahead time, allowing observations used.
observations become unavailable child time
label lower used parent.
course problematic issue latter: observations lost, among
may information necessary properly selecting recovery action.
Let us consider example te-set Figure 9, four fault situations, two time
labels (0 1) one sensor (s). fault situation characterized dierent
recovery action, te-set obviously allows discriminate them. However
entropy criterion would rst select observation s, 1, discriminating.

487

fiConsole, Picardi, & Theseider Dupre

sit1
sit2
sit3
sit4

s, 0
x
x



s, 1
x


z

Act

b
c


Figure 9: te-set causing problems standard ID3 algorithm used temporal
decision trees.

observation s, 0 would become unavailable, resulting tree could never
discriminate sit2 sit3 .
shows relevant dierence building standard decision trees
building temporal decision trees. Let us look generic algorithm standard
decision trees presented Figure 3: particular strategy implemented ChooseTest
aect capability tree selecting proper recovery action,
size tree. Essentially tree contains information set examples least concerns association observations recovery actions.
say tree always discriminating power set examples,
meaning case tree capable deciding two recovery
actions set examples contains two fault situations identical observations
dierent actions.
consider algorithm Figure 8 see order observations
selected - is, particular implementation ChooseObs - aect discriminating power tree, size. Since one recursive call following
observations discarded, obtain tree less discriminating information
original set examples. primary task avoid situation,
build tree small, sacrice relevant information. consequence, cannot exploit strategy simply selecting observation minimum
entropy.
next sections shall formalize new requirements output tree,
propose implementation ChooseObs meets them.
5.2 Tree Cost
previous section informally introduced notion discriminating power.
section shall introduce general notion expected cost temporal decision
tree. Intuitively, expected cost associated temporal decision tree expected
cost recovery action selected tree, respect probability distribution
fault situations.
Expected cost stronger notion discriminating power: one hand tree
discriminates better another, also lower expected cost (we shall soon
prove statement). hand expected cost adds something notion
discriminating power, since two trees comparable point view cost,
may point view discrimination carry out.

488

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

dening expected cost, need introduce preliminary denitions.
shall make use function, named examples, given initial set examples
E tree associates node tree subset E. understand meaning
function formally dening it, let us imagine run tree E
certain point decision process reach node n: examples tells us
subset fault situations yet discarded.
Definition 10 Let E te-set sensors s1 , . . . , sm , time labels t1 , . . . , tlast actions
model A, , . Moreover let = r, N, E, L, temporal decision tree
every internal node n N L(n) {s1 , . . . , sm } (n) {t1 , . . . , tlast }.
dene function examples(; E) : N 2E follows:
(7) examples(r; E) = E

r root T;

examples(n; E) = {sit examples(p; E) | Val(sit, L(p), (p)) = L((p, n))}
n N , n = r (p, n) E.
examples well dened since n N dierent root exists exactly

one p N (p, n) E.
Notice that, E set examples used building T, examples(n; E) corresponds
subset examples used creating node n.
Example 5 Let us consider Figure 10: shows tree Figure 5, every
node n also see set fault situations examples(n; E), E te-set

Figure 1.

low
high

Action: b
sit7

low

s1,0
sit1,sit3,sit4
sit6,sit7

normal

sit1
sit6

high
normal

Actions: b,c

low
s2,3

s2,2
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

s2,1
sit1,sit3
sit4,sit6

s2,1
sit2,sit5
sit8

high
Action: c

sit2,sit5

sit8

low

zero

normal

s3,2

high

sit3,sit4

Action:

Action:

Action: b

Action: c

sit1

sit6

sit3

sit4

Figure 10: temporal decision tree showing value example(n, E).

489

fiConsole, Picardi, & Theseider Dupre

following using function examples shall omit second argument,
denoting initial te-set, ambiguity te-set considered.
every tree used given set examples: actually need compatibility two, characterized following denition.
Definition 11 Let E previous denition. say temporal decision tree
= r, N, E, L, compatible E if:
every internal node n N , L(n) {s1 , . . . , sm }, (n) {t1 , . . . , tlast } (n)
Dl(examples(n; E)).
every leaf l N , L(l) merge(A) L(l) = merge(Act(examples(l; E))).
straightforward see tree compatible set examples used building
it.
following property6 :
Proposition 12 Let = r, N, E, L, temporal decision tree compatible teset E. Let l1 , . . . , lf N denote leaves . examples(l1 ), . . . , examples(lf )
partition E.
sit E denote leafT (sit) unique leaf l sit
examples(l). ready formalize notion discriminating power.
Definition 13 Let = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU denote two temporal decision trees compatible te-set E. Let moreover A, ,
recovery action model used building U. say discriminating
U respect E if:
(1) every sit E either LT (leafT (sit)) LU (leafU (sit)) LT (leafT (sit)) =
LU (leafU (sit));
(2) exists sit E LT (leafT (sit)) LU (leafU (sit)).



Notice second condition makes sure two trees equal (in
case would equally discriminating), something rst condition alone cannot
guarantee.
Example 6 Let us consider tree Figure 10 tree Figure 11 below.
former discriminating latter. fact, two trees associate
actions sit1 , sit2 , sit3 , sit4 , sit5 sit6 . However former associates action b sit7
action c sit8 , latter associates sit7 sit8 compound action

{b, c}.
Unfortunately cannot easily use discriminating power - dened - preference criterion decision trees. reason dene total order
decision trees, partial one. following situations may arise:
6. sake readability, proofs collected separate appendix end paper.

490

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

sit, LT (leafT (sit)) LU (leafU (sit)); sit, LU (leafU (sit))
LT (leafT (sit)).
given sit, LT (leafT (sit)) LU (leafU (sit)), LU (leafU (sit)) LT (leafT (sit)).
point view discriminating power alone, reasonable U
comparable cases. Nonetheless, may reason preferring one
other, reason cost. example consider second situation, even
LT (sit) LU (sit) directly comparable point view strength,
one two may cheaper thus preferable.
therefore introduce notion expected cost tree.
Definition 14 Let = r, N, E, L, temporal decision tree compatible teset E action model = A, , . inductively dene expected cost function
XE,A : N IR+ tree nodes follows:


l N leaf;
(L(l))

(8)
XE,A (n) =
P (L(n) L((n, c))) XE,A (c) n N inner node.


c:(n,c)E

P (L(n) L((n, c))) probability sensor L(n) showing value v = L((n, c))
given by:
P (L(n) L((n, c))) =

P (examples(c); E)
= P (examples(c); examples(n)).
P (examples(n); E)

expected cost respect E A, denoted XE,A (T) , dened as:
XE,A (T) = XE,A (r) r root

(9)


denition states that:
expected cost tree leaf l simply cost recovery action L(l);
expected cost inner node n given weighted sum childrens
expected costs; weight child c given probability P (L(n) L((n, c))).
expected cost temporal decision tree expected cost root.
following proposition states weighted sum computing expected cost
root performed directly tree leaves.
Proposition 15 Let = r, N, E, L, denote temporal decision tree, let l1 , . . . , lu
leaves.
(10)

XE,A (T) =

u


(L(li )) P (examples(l); E)

i=1

491

fiConsole, Picardi, & Theseider Dupre

low

low

s2,5
sit1
sit6

s2,0

low

sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

high

zero

low

s3,6
sit3
sit4

high

Action:

Action:

Actions: b

Action: c

sit1

sit6

sit3

sit4

Actions: b
sit2,sit5,sit7,sit8

Figure 11: temporal decision tree less discriminating one Figure 10.
next proposition shows expected cost monotonic respect better
discrimination relation, therefore good preference criterion temporal decision
trees, since tree lowest possible expected cost discriminating one,
moreover cheapest among equally discriminating trees.
Proposition 16 Let = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU two temporal decision trees compatible te-set E actions model A.
discriminating U XE,A (T) < XE,A (U).
Example 7 Let us compute expected cost tree T1 Figure 10 tree T2
Figure 11, respect te-set E Figure 1 action model Figure 7.
shall assume fault situations equiprobable, probability
1/8. exploiting proposition 15 obtain:
XE,A (T1 ) = P (sit1 )(a) + P (sit7 )(b) + P (sit6 )(d) + P (sit3 )(b) + P (sit4 )(c) +
+P ({sit2 , sit5 })({b, c}) + P (sit8 )(c) =
1
1
1
1
1
1
1
100 + 20 + 10 + 20 + 50 + 70 + 50 =
=
8
8
8
8
8
4
8
= 12.5 + 2.5 + 1.25 + 2.5 + 6.25 + 17.5 + 6.25 = 48.75
XE,A (T2 ) = P (sit1 )(a) + P (sit6 )(d) + P (sit3 )(b) + P (sit4 )(c) +
+P ({sit2 , sit5 , sit7 , sit8 })({b, c}) =
1
1
1
1
1
100 + 10 + 20 + 50 + 70 =
=
8
8
8
8
2
= 12.5 + 1.25 + 2.5 + 6.25 + 35 = 57.5
see less discriminating tree, T2 , higher expected cost.



5.3 Restating Problem
previous section introduced expected cost preference criterion decision
trees. Given notion, restate problem building temporal decision tree
492

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

building tree minimum possible expected cost. section formally shows
notion minimum possible expected cost well dened, precisely
corresponds cost tree exploits observations te-set. goal
expressed nding reasonably small tree among whose expected cost
minimum.
section, well formalizing mentioned notions, introduce
formal machinery useful proving correctness algorithm.
Definition 17 Let E denote te-set. Moreover, let t1 , . . . , tlast denote time labels
E. say siti , sitj E pairwise indistinguishable, write siti sitj ,
ti < min{Dl(siti ), Dl(sitj )} sensors Val(siti , s, ti ) =

Val(sitj , s, ti ).
relation, obviously reexive symmetric, transitive. consider
sitk particularly strict deadline, might well siti sitk , sitk sitj ,
siti sitj . introduce new relation transitive closure .
Definition 18 Let E denote te-set. say siti , sitj E indistinguishable,
write siti sitj , exists nite sequence sitk1 , . . . , sitku E
sitk1 = siti ;
sitku = sitj ;
every g = 1, . . . , u 1, sitkg sitkg+1 .



equivalence relation E, denote E/ corresponding quotient set.
following denition:
Definition 19 Let E te-set, actions model A. expected cost E, denoted
X E,A , dened as:

X E,A =
(merge({Act(sit) | sit })) P (; E)
(11)
E/


Example 8 Let us consider te-set E Figure 1 action model Figure 7.
two indistinguishable fault situations E sit2 sit5 . Thus have:
X E,A = P (sit1 )(a) + P ({sit2 , sit5 })({b, c}) + P (sit3 )(b) + P (sit4 )(c) +
+P (sit6 )(d) + P (sit7 )(b) + P (sit8 )(c) =
1
1
1
1
1
1
1
100 + 70 + 20 + 50 + 10 + 20 + 50 =
=
8
4
8
8
8
8
8
= 12.5 + 17.5 + 2.5 + 6.25 + 1.25 + 2.5 + 6.25 = 48.75
Notice tree Figure 10 cost te-set, thus cost minimum
possible, show below. course may still able build another smaller tree

cost.
493

fiConsole, Picardi, & Theseider Dupre

need show X E,A actually minimum possible expected cost
temporal decision tree compatible E.
Theorem 20 Let E te-set actions model A. that:
(i) exists decision tree compatible E XE,A (T) = X E,A .
(ii) every temporal decision tree compatible E, X E,A XE,A (T).



state precisely problem building temporal decision tree:
Given te-set E actions model A, want build temporal decision
tree E, XE,A (T) = X E,A . Moreover, want keep tree
reasonably small exploiting entropy.

6. Algorithm
section describe detail proposal building temporal decision trees
given te-set action model. also discuss complexity algorithm introduce,
give example algorithm works.
6.1 Preconditions
goal dene implementation function ChooseObs that,
plugged function BuildTemporalTree, yields solution problem building
temporal decision trees stated section 5.3. First however shall analyze properties BuildTemporalTree dened Figure 8: lead us smoothly
solution help us prove formally correctness. order accomplish task
need introduce notation allows us speak algorithm properties.
Let E te-set fault situations {sit1 , . . . , sitn }, sensors {s1 , . . . , sm }, time labels
{t1 , . . . , tlast } action model A. aim computing tree executing:
(12)

BuildTemporalTree({sit1 , . . . , sitn }, {s1 , . . . , sm } {t1 , . . . , tlast }, A)

execution BuildTemporalTree comprises several recursive calls
function; given two recursive calls c, c shall write c c c occurs immediately
inside c . Moreover shall denote c0 initial call. Finally, shall call terminal
recursive call inner call.
given call c, shall denote [[Example]]c , [[Obs]]c , [[ActModel]]c actual values
formal parameters c. slight abuse notation, shall also write [[var]]c
denote value variables var c that, set, never change value
(deadline, UsefulObs, ValidObs, sensor, tlabel, Obs Update). Finally, shall denote
[[T]]c tree returned call c.
recursive call c works dierent te-set, dened [[Examples]]c
[[Obs]]c . actions model however always same, since c c
[[ActModel]]c = [[ActModel]]c . shall denote Ec te-set used call c,
determined input parameters.
494

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

following proposition critical proving correctness approach.
states obtain tree minimum expected cost guarantee
increase expected cost te-set passing set
observations Obs set Obs Update:
Proposition 21 Let us consider execution BuildTemporalTree starting
main call c0 . initial te-set, want build tree over, E = Ec0
= [[ActModel]]c0 . recursive call c, let us denote Ec te-set determined
[[Examples]]c [[Obs Update]]c . Then:
(1) XE,A ([[T]]c0 ) X E,A
(2) XE,A ([[T]]c0 ) = X E,A every non terminal7 recursive call c generated
c0 holds X Ec ,A = X Ec ,A
6.2 Implementing ChooseObs
Proposition 21 suggests need provide implementation ChooseObs
recursive call c, X Ec ,A = X Ec ,A . Let us examine detail relations
[[Obs]]c [[Obs Update]]c .
rst step, [[UsefulObs]]c obtained removing [[Obs]]c observations
help discriminating fault situations. eect expected cost
te-set, since aect relation indistinguishability.
[[Obs Update]]c obtained [[UsefulObs]]c removing observations
whose time label precedes chosen one. expected cost resulting te-set thus
depends time label selected function ChooseObs. following properties:
Proposition 22 Let c, denote two independent calls BuildTemporalTree
input arguments dierent implementations ChooseObs. [[tlabel]]c
[[tlabel]]d X Ec ,A X Ed ,A .
Proposition 23 Let c call BuildTemporalTree. [[tlabel]]c = tminc = min{t |
t, [[ValidObs]]c } X Ec ,A X Ec ,A .
dene notion safe time label:
Definition 24 Let c denote call BuildTemporalTree. time label said
safe respect c [[tlabel]]c = implies X Ec ,A = X Ec ,A .
immediate consequence propositions 22 23 following:
Proposition 25 call c BuildTemporalTree exist time label tmaxc
safe time labels tminc tmaxc , tminc
proposition 23.
7. exclude terminal calls even compute Obs Update.

495

fiConsole, Picardi, & Theseider Dupre

Figure 12 describes ID3ChooseSafeObs, implementation ChooseObs propose. exploits properties proved previous section order
achieve desired task ecient way. Let us examine detail.
ID3ChooseSafeObs (Figure 12) computes set safe observations (line 4)
chooses among one minimum entropy (line 5). proved
now, implementation yields temporal decision tree minimum expected
cost, time exploits entropy order keep tree small.
Let us see FindSafeObs (also Figure 12) computes set safe observations. Proposition 23 shows notion safeness tied time labels rather
individual observations. First FindSafeObs determines range valid time
labels current set examples (line 12); lower bound tlow lowest time label
Obs, stored variable low, upper bound tup given global
deadline Examples, stored variable up.
idea nd maximum safe label tmax (variable max) allows us
easily build set safe observations (line 21).
order accomplish task following steps performed:
Given initial te-set E, dened Examples, Obs ActModel, compute X E,A .
time label range delimited tlow tup , consider te-set Et
dened Examples observations time label equal greater
t. compute X Et ,A .
soon nd time label X Et ,A > X E,A , know tmax time
label immediately preceding t.
critical operation (in terms eciency) computing expected
cost Et , involves nding quotient set Et /. fact, order
obtain quotient set, need repeatedly partition te-set respect
observations available it.
QuotientSet function (Figure 12) performs precisely task. takes input
current time label tlabel, initial partition (possibly made single block
entire te-set) set observations, select valid ones.
First partitions input te-set respect observations current
time label (lines 2831). executes iteratively following operations:
partition block checks whether deadline moved time
(lines 3638).
so, partitions block stores resulting sub-blocks
examination (lines 3941).
not, block part Final partition returned (line 42).
order simplify task, introduce data type extended partition,
partition block stored together highest time label used building it. way
easily check deadline block allows us exploit observations
not. Using extended partitions instead standard ones need dene new function,
ExtPartition, works way Partition function used Figure 4,
also records block highest time label used it.
496

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47

function ID3ChooseSafeObs (set Obs, te-set Examples, action model ActModel)
returns observation = sensor, tlabel
begin
SafeObs FindSafeObs(Obs, Examples, ActModel);
ID3ChooseTest(SafeObs, Examples);
return o;
end.
function FindSafeObs (set Obs, te-set Examples, action model ActModel)
returns set observations SafeObs
begin

cost sitExamples Act(sit);
Dl(Examples); low min{t | s, Obs};
max up; part {Examples, up};
every time label tx starting low begin
part QuotientSet(part, Obs, tx);
newcost ExpectedCost(part);
newcost < cost begin
cost newcost; max tx;
end;
end;
SafeObs {s, | max};
return SafeObs;
end.
function QuotientSet (partition Initial, set Obs, time label tlabel)
returns rened partition Final
begin
part Initial;
s, = tlabel begin
part ExtPartition(part, s, t);
ObsCurr ObsCurr {s, t};
end;
Final ;
part =
tmp part ;
block, ty part begin
newdl Dl(block);
single {block, ty};
newdl > ty begin
s, ty < newdl
single ExtPartition(single, s, t);
tmp part tmp part single;
end else Final Final start;
end;
part tmp part;
end;
return Final;
end.

Figure 12: ID3ChooseSafeObs implementation ChooseObs yielding tree
minimum expected cost

497

fiConsole, Picardi, & Theseider Dupre

Notice QuotientSet needs whole set observations (and valid ones)
properly compute result; therefore BuildTemporalTree calls ID3ChooseSafeObs
must pass rst argument UsefulObs instead ValidObs.
FindSafeObs exploits QuotientSet nd quotient sets Et ,
using ecient approach call backward strategy.
First all, notice order observations considered
matter building quotient set. Moreover, < , Et / renement Et /;
words obtain Et / simply rening partition additional
observations.
Thus, compute quotient sets time compute E/.
FindSafeObs exactly so: computes quotient sets expected cost
starting last time label tup . quotient set built scratch,
renement previous one. reason QuotientSet (and ExtPartition
well) takes rst argument single set, partition. way, quotient sets
computed operations8 needed build E/. next section analyzes
detail complexity issues.
6.3 Complexity
section aim showing additional computations needed building
temporal decision trees lead higher asymptotical complexity
would get using standard ID3 algorithm set examples (we discussed
section 5.1 circumstances could make approach feasible).
Essentially dierence two cases lies presence FindSafeObs function. Wherever BuildTree calls ID3ChooseTest, BuildTemporalTree calls
ID3ChooseSafeObs, turn calls FindSafeObs ID3ChooseTest.
Let us compare FindSafeObs ID3ChooseTest, similar many ways.
former repeatedly partitions input te-set respect every available observation;
computes entropy partition built way. FindSafeObs builds
one partition exploiting available observations; words instead using
observation partition initial te-set, exploits order rene existing
partition te-set. Moreover, time label computes expected cost
partition built far. Essentially, denote NS number sensors
NT number time labels initial partition, roughly following
comparison:
NS NT (number observations) entropy computations ID3ChooseTest vs.
NT expected cost computations FindSafeObs.
NS NT partitions initial te-set ID3ChooseTest vs. NS NT renements
existing partitions initial te-set FindSafeObs.
Entropy expected cost computed roughly eort: require
retrieving information element partition block, combine
information quite straightforward way. complexity task depends
overall number elements, distributed dierent
8. slight overhead due need nd observations used step.

498

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

block partition. even expected cost computed times ner
partitions entropy, thing matters partitions
set, thus involving elements.
let us examine problem creating partition. involves retrieving value
element block initial partition (which depends
number elements, number blocks initial partition) properly
assign element new partition block depending original block new
value. main dierence case starting whole te-set (creation)
initial partition (renement), size new blocks created,
smaller second case. Dependent implement partition data
type, may make dierence, may take less time renement case. However,
never happens renement (corresponding FindSafeObs function) requires
time creation (corresponding ID3ChooseTest function) partition.
Therefore claim FindSafeObs function asymptotic complexity function ID3ChooseTest. Thus also ID3ChooseSafeObs asymptotic complexity ID3ChooseTest, conclude BuildTree
asymptotic complexity BuildTemporalTree.
6.4 Example
section shall show algorithm operates te-set Figure 1
respect action model Figure 7.
Let us summarize information algorithm receives. Eight fault situations
involved; moreover exploit three sensors, show dierent
qualitative values: h - high, n - normal, l - low, v - low, z - zero. Time labels
correspond natural numbers ranging 0 7, assume correspond
times measured internal clock started time fault detection.
four basic recovery actions a, b, c, d, b c a. set
compound recovery actions thus = {{a}, {b}, {c}, {b, c}, {d}}; ordering relation
pictured 7, together action costs.
BuildTemporalTree rst called whole te-set. None two terminating conditions met (notice however two observations useful, since discriminate: s3 , 0 s3 , 1). main function calls
ID3ChooseSafeObs consequently FindSafeObs. Since global deadline 2
must check time labels 0, 1 2, starting last one.
Exploiting observations time label 2 obtain following partition:
{{sit1 , sit6 }, {sit2 , sit5 , sit7 , sit8 }, {sit3 }, {sit4 }}
However order nd expected cost still check partition block
deadline changed; happens {sit1 , sit6 } well {sit3 } {sit4 }.
last two blocks change anything - already contain one element.
rst block, deadline 5 thus possible split partition.
Therefore obtain partition time label 2 is:
Pt=2 = {{sit1 }, {sit6 }, {sit2 , sit5 , sit7 , sit8 }, {sit3 }, {sit4 }}
499

fiConsole, Picardi, & Theseider Dupre

nding partition, algorithm computes expected cost, turns
be:
1
+ (Act(sit6 ))
8
1
+ (Act(sit3 )) + (Act(sit4 ))
8
1
1
1
1
= 100 + 10 + 70 + 20
8
8
2
8
= 57.5

XE,t=2 = (Act(sit1 ))

1
1
+ (Act({sit2 , sit5 , sit7 , sit8 }))
8
2
1
8
1
+ 50
8

algorithm moves time label 1; starting Pt=2 adds observations
time label 1, obtaining new partition:
Pt=1 = {{sit1 }, {sit6 }, {sit2 , sit5 }, {sit7 }, {sit8 }, {sit3 }, {sit4 }}
Deadlines move {sit7 } {sit8 }, since singletons new observations
cannot split partition. expected cost now:
1
+ (Act(sit6 ))
8
1
+ (Act(sit8 )) + (Act(sit3 ))
8
1
1
1
1
= 100 + 10 + 70 + 20
8
8
4
8
= 48.75

XE,t=1 = (Act(sit1 ))

1
+ (Act({sit2 , sit5 }))
8
1
1
+ (Act(sit4 ))
8
8
1
1
+ +50 + 20 + 50
8
8

1
1
+ (Act(sit7 ))
4
8
1
8

Since XE,t=1 < XE,t=2 conclude observations time label 2 safe.
move time label 0, immediately realize new observations
change partition. Thus XE,t=0 = XE,t=1 , safe observations time label
equal either 0 1.
algorithm calls function ID3ChooseTest selects observation
minimum entropy. Figure 13 shows entropies dierent observations stage,
deduce best choice s2 , 1.
Entropies {sit1 , sit2 , sit3 , sit4 , sit5 , sit6 , sit7 , sit8 }
s1 , 0 1.5
s1 , 1 1.5
s2 , 0 1.451
s2 , 1 0.844

Figure 13: Entropies safe observations initial call
Figure 15.(a) shows tree point; function BuildTemporalTree recursively
invokes four times, yielding:
call c1 Ev = {sit1 , sit6 };
call c2 En = {sit2 , sit5 };
call c3 El = {sit3 , sit4 , sit7 };
call c4 Eh = {sit8 }.
500

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

Let us focus call c1 : again, none terminating conditions met, therefore
algorithm invokes ID3ChooseSafeObs thus FindSafeObs. Notice however
subset observations UsefulObs: s1 , 3, s2 , 3, s2 , 4, s2 , 5
s3 , 5. global deadline 5.
First nd partition (and expected cost) time label 5:
Pt=5 = {{sit1 }, {sit6 }}
1
1
1
1
XEv ,t=5 = (Act(sit1 )) + (Act(sit6 )) = 100 + 10 = 55
2
2
2
2
additional observations split partition lower cost; therefore
nd valid observations also safe. moreover obvious observations entropy, 0. Therefore algorithm non-deterministically
select one them; reasonable criterion would select earliest ones,
example s1 , 3.
Since initial te-set call c1 split two, two recursive calls.
However notice BuildTemporalTree called te-set single
element, rst terminating condition trivially met (all fault situations
recovery action). function simply returns tree leaf name proper
recovery action. Figure 15.(b) shows tree c1 completed.
let us examine c2 : algorithm eliminates non-discriminating observations,
nds set useful observations empty. Thus builds leaf recovery
action {b, c}. Let us pass call c3 . case none terminating conditions met:
Entropies {sit3 , sit4 , sit7 }
s1 , 1 0.667 s1 , 2 0.667 s1 , 3 0.667
s2 , 2 0.667 s2 , 3 0.667 s2 , 4 0.667
s2 , 5
0
s3 , 1
0
s3 , 2
0
s3 , 3
0
s3 , 4
0

Figure 14: Entropies safe observations call c3
algorithm must look safe observations. global deadline 5, start
examining time label 5, nd:
Pt=5 = {{sit3 }, {sit4 }, {sit7 }}
1
1
1
XEl ,t=5 = (Act(sit3 )) + (Act(sit4 )) + (Act(sit7 ))
3
3
3
1
1
1
= 20 + 50 + 20 = 30
3
3
3
Much happened c1 , additional observation split partition,
conclude valid observations also safe. Figure 14 shows entropy valid
observations; earliest one minimum entropy s3 , 2 algorithm selects.
two recursive sub-calls generated immediately terminate: {sit4 } singleton,
{sit3 , sit7 } fault situations correspond recovery action.
last recursive call, c4 , input singleton thus immediately terminates.
nal decision tree pictured Figure 15.(c).
501

fiConsole, Picardi, & Theseider Dupre

low

s2,1
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

low

low

low

normal

normal
high

?

s2,1
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

sit1,sit6

s2,3

normal

?
sit3,sit4,sit7

?

?

sit2,sit5

sit8

high

sit1
sit6

high

?





sit1

sit6

(a)

?
sit3,sit4,sit7

sit2,sit5

?
sit8

(b)
low

s2,1
sit1,sit2,sit3,sit4
sit5,sit6,sit7,sit8

normal
normal

s2,3
sit1
sit6

low

high

high

normal

s3,2
sit3,sit4

high

sit7





sit1

sit6

b,c

c

sit2,sit5

sit8

b

c

sit3,sit7

sit4

(c)

Figure 15: output tree dierent stages. (c) shows nal tree.
Let us check expected cost really equal expected cost E. Figure 16
shows tree leaf l corresponding set fault situations examples(l), probability
P (examples(l); E) cost (L(l)). Leaves numbered 1 6 going left right
Figure 15.(c).
leaf
l1
l2
l3
l4
l5
l6

examples
sit1
sit6
sit2 , sit5
sit8
sit3 , sit7
sit4

P
1/8
1/8
1/4
1/8
1/4
1/8


100
10
70
50
20
50

Figure 16: Fault situations, probabilities costs leaves tree Figure 15.(c)

502

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

Thus expected cost tree is:
XE,A (T) =

6


(L(li )) P (examples(l); E)

i=1

1
1
1
1
1
1
100 + 10 + 70 + 50 + 20 + 50
8
8
4
8
4
8
= 48.75

=

look back example 8 see XE,A = 48.75, thus minimum possible
expected cost. Moreover, compare tree T1 example 5. Also T1
minimum possible expected cost, compact.

7. Conclusions
paper introduced new notion diagnostic decision tree takes account
temporal information observations temporal constraints recovery actions
performed. way take advantage discriminatory power
available model dynamic system. presented algorithm generates
temporal diagnostic decision trees set examples, discussing also optimal
tree generated.
automatic compilation decision trees seems promising approach reconciling advantages model-based reasoning constraints imposed on-board
hardware software environment. worth noting true
automotive domain indeed idea compiling diagnostic rules model
investigated also approaches (see e.g., Darwiche, 1999; Dvorak & Kuipers, 1989).
Darwiche (1999), particular, discusses rules generated platforms
constrained resources allow direct use model-based diagnostic system.
new approach possibility compiling also information concerning
system temporal behaviour, obtaining way accurate decision procedures.
best knowledge, temporal decision trees new notion diagnostic
literature. However, works elds relation ours, since
aimed learnig rules associations take account time.
Geurts Whenkel (1998) propose notion temporal tree used early
prediction faults. topic closely related diagnosis, albeit dierent
ways: idea device examination failed yet, observing
behaviour possible predict fault occur. Geurts Wehenkel
propose learn relation observed behavioural patterns consequent failures
inducing temporal tree.
notion temporal tree introduced Geurts Wehenkel dierent
temporal decision trees, reecting dierent purpose introduced for. Rather
sensor readings, consider general notion test, tree
species time wait performing tests, rather agent running tree
supposed wait one tests associated tree node becomes true.
Also notion optimality quite dierent: situation described Geurts
Wehenkel size resulting tree concern. tree-building algorithms aims
503

fiConsole, Picardi, & Theseider Dupre

minimizing time nal decision taken. algorithm, size
primary concern, point view time suces diagnosis carried
within certain deadlines. point view time alone, apporach Geurts
Wehenkel probably general ours; problem considering trade-o
diagnostic capability time needed diagnosis one major extensions
considering future work topic (see below).
Finally, algorithm proposed Geurts Wehenkel works quite dierent
way ours: rst builds tree greedily, using evaluation function weighs
discriminability power agains time needed reach result, selecting step
texts optimizes function. prunes tree order avoid overtting.
hand, approach aim optimizing tree point view cost,
time tries keep tree small entropy heuristic. think that, since
optimization carried additional cost9 respect minimization
entropy, approach obtain better results, least cases one dene
notion deadline.
process learning association rules involving time also studied
areas, machine learning (see example Bischof & Caelli, 2001, authors
propose technique learn movements) data mining. specic diagnostic
tailoring approach makes dicult compare generic learning algorithms, connections data mining may stronger. proposal fact aims
essentially extracting series observations patterns time allow
correctly diagnose fault: process regarded form temporal classication.
preliminary investigation papers area (see Antunes & Oliveira, 2001
overview) seems suggest that, whereas analysis temporal sequences data
received much interest last years, much work done direction
data classication, temporal decision trees could exploited.
suggests interesting development work, particular concerns
applicability areas. However, believe algorithm presented needs
extended order exploited contexts. particular investigating
following extensions:
Deadlines could turned hard soft. Soft deadlines met,
rather dene cost associated meeting them. Thus meeting deadline
becomes option taken account less expensive
performing recovery action diagnosis complete. One could even
dene cost increases time passes expiration deadline.
extension would allow model also trade-o discriminability
power time needed decision process, believe key making
work applicable areas.
Actions could assumed dierent cost depending fault situation;
example action associated fault could become dangerous thus extremely
expensive performed presence another fault.
9. point view asymptotical complexity.

504

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

long term, future work topic aimed widening areas
applicability, investigating deeper details connections elds,
fault prevention data mining.

8. Acknowledgements
work partially supported EU grant GRD-1999-0058, project IDD
(Integrated Diagnosis Design), whose partners are: Centro Ricerche Fiat, DaimlerChrysler, Magneti Marelli, OCCM, PSA, Renault, Technische Universitat Munchen, Universite Paris XIII, Universita di Torino.

Appendix A. Proofs
section contains proofs propositions, lemmas theorems paper.
Proposition 12. Let = r, N, E, L, temporal decision tree compatible
te-set E. Let l1 , . . . , lf N denote leaves . examples(l1 ), . . . , examples(lf )
partition E.
Proof. Follows immediately denition examples (10), noticing
n1 , . . . , nk children p {examples(n1 ), . . . , examples(nk )} partition

examples(p).
Proposition 15 Let = r, N, E, L, denote temporal decision tree, let
l1 , . . . , lu leaves.
XE,A (T) =

u


(L(li )) P (examples(l); E)

i=1

Proof. induction depth T. depth 0 consists single
leaf l 10 holds trivially since examples(l) must equal E P (E; E) = 1.
depth > 0 let T1 , . . . , Tk denote direct subtrees c1 , . . . , ck denote
roots. regard Ti autonomous temporal decision tree compatible
te-set Ei = examples(ci ). induction hypothesis that:
(13)


P (examples(l); E)
.
(L(l)) P (examples(l); Ei ) =
(L(l))
XEi ,A (Ti ) =
P (Ei ; E)
l leaf Ti

l leaf Ti

Moreover denition expected cost:
(14) XE,A (T) =

k


P (L(r) L((r, ci ))) XEi ,A (Ti )

i=1

P (L(r) L((r, ci ))) = P (examples(ci ); examples(r)) = P (Ei ; E).

505

fiConsole, Picardi, & Theseider Dupre

(13) (14) thus obtain:
(15) XE,A (T) =

k




P (Ei ; E) (L(l))

i=1 l leaf Ti

=

P (examples(l); E)
P (Ei ; E)
k




(L(l)) P (examples(l); E)

i=1 l leaf Ti

Since leaves leaves T1 , . . . , Tk , (15) equivalent

thesis.
Proposition 16 Let = rT , NT , ET , LT , TT , U = rU , NU , EU , LU , TU two
temporal decision trees compatible te-set E actions model A.
discriminating U XE,A (T) < XE,A (U).
Proof. Rewriting equation (10) obtain:
(16) XE,A (T) =

r


(LT (li )) P (examples(l); E) =

(17) XE,A (U) =

(LT (leafT (sit)))P (sit; E)

sitE

i=1





(LU (leafU (sit)))P (sit; E).

sitE

Since discriminating U, sit E:
LT (leafT (sit)) LU (leafU (sit))

LT (leafT (sit)) = LU (leafU (sit))

least one sit satisfying rst relation. denition follows sit:
(LT (leafT (sit))) (LU (leafU (sit)))
least one sit:
(LT (leafT (sit))) = (LU (leafU (sit)))
Therefore compare individual elements two sums (16) (17) observe
exists least one sit which:
(LT (leafT (sit)))P (sit; E) < (LU (leafU (sit)))P (sit; E)
sit
(LT (leafT (sit)))P (sit; E) (LU (leafU (sit)))P (sit; E)
concludes proof.
Theorem 20 Let E te-set actions model A. that:



(i) exists decision tree compatible E XE,A (T) = X E,A .
(ii) every temporal decision tree compatible E, X E,A XE,A (T).
506



fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

order prove theorem introduce lemmas.
Lemma 26 Let temporal decision tree compatible te-set E. siti sitj
implies leafT (siti ) = leafT (sitj ).
Proof.
prove siti sitj implies leafT (siti ) = leafT (sitj ),
lemma easily follows. Let us suppose leafT (siti ) = leafT (sitj ). means
common ancestor n two leaves siti , sitj examples(n)
Val(siti , L(n), (n)) = Val(sitj , L(n), (n)). Since siti sitj possible
(n) > min{Dl(siti ), Dl(sitj )}. since compatible E must hold (n)

Dl(examples(n)) min{Dl(siti ), Dl(sitj )}, contradicts previous statement.
Lemma 27 Let E te-set sensors s1 , . . . , sm , time labels t1 , . . . , tlast actions
model A. exists temporal decision tree = r, N , E, L, XE,A (T) =
X E,A .
Proof. order prove thesis construct tree expected cost
te-set.
Let us dene total order observations E follows: s, < , either <
= precedes lexicographic ordering. Let us denote o1 , . . . , omax
ordered sequence observations thus obtained. shall dene level level (starting
root, level 1) giving value L nodes level h.
maximum max + 1 levels, max number observations. New
levels added nodes level leaves (which shall see happens
level max + 1). Let thus n node level h , let sih , tih = oh h max.
have:

leaf
h = max + 1, Dl(examples(n)) < tih ;
n
internal node otherwise.

merge({Act(sit | sit examples(n)}) n leaf;
L(n) =
n internal node.
sih
(n) = tih

n internal node.

decision-making agent running tree would essentially take account sensor
measurement time labels either available observations
must perform recovery action deadline expire.
need show XE,A (T) = X E,A .
Let l1 , . . . , lu denote leaves T. shall rst prove siti sitj
leafT (siti ) = leafT (sitj ), equivalently
{examples(l1 ), . . . , examples(lu )} = E/.
This, together equations (10) (11) yields thesis.
already know lemma 26 siti sitj implies leafT (siti ) = leafT (sitj );
need show opposite also true. Let us thus assume siti , sitj examples(l)
507

fiConsole, Picardi, & Theseider Dupre

l {l1 , . . . , lu }. Let r = n1 , n2 , . . . , nH , nH+1 = l path root
l. know denition h H, L(nh ), (nh ) = oh ,
o1 , . . . , oH observations s, E Dl(examples(l)). Moreover since
siti , sitj examples(l) h = 1, . . . , H, Val(siti , oh ) = Val(sitj , oh ).
two possibilities: either Dl(examples(l)) = min{Dl(siti ), Dl(sitj )},
Dl(examples(l)) < min{Dl(siti ), Dl(sitj )}.
rst case immediately obtain siti sitj thus siti sitj .
second case, must sitk examples(l) Dl(examples(l)) =
Dl(sitk ). Moreover, Dl(sitk ) = min{Dl(siti ), Dl(sitk )} = min{Dl(sitj ), Dl(sitk )}. Since
considerations apply also sitk thus siti sitk sitk sitj ; therefore

transitivity siti sitj .
Lemma 28 Let = r, N, E, L, decision tree compatible te-set E
actions model A. X E,A XE,A (T ).
Proof. Let dened proof lemma 27. order prove thesis
suces show either equally10 discriminating (see proposition
16). Actually shall show given sit E either L(leafT (sit)) L(leafT (sit))
L(leafT (sit)) = L(leafT (sit)).
know examples(leafT (sit)) examples(leafT (sit)). fact, let sit
element examples(leafT (sit)) dierent sit itself: construction
sit sit , lemma 26 follows leafT (sit) = leafT (sit ).
Let:
= {Act(s) | examples(leafT (sit))}, = {Act(s) | examples(leafT (sit))}.
Since A, denition merge:
merge(A) merge(A)

merge(A) = merge(A)

Thus L(leafT (sit)) = merge(A) L(leafT (sit)) = merge(A) obtain either

action selected weaker selected T, same.
prove theorem 20.
Proof. Point (i) proved lemma 27, point (ii) corresponds lemma 28.
Proposition 21 Let us consider execution BuildTemporalTree starting
main call c0 . initial te-set, want build tree over, E = Ec0
= [[ActModel]]c0 . recursive call c, let us denote Ec te-set determined
[[Examples]]c [[Obs Update]]c . Then:
(1) XE,A ([[T]]c0 ) X E,A
(2) XE,A ([[T]]c0 ) = X E,A every non terminal11 recursive call c generated
c0 holds X Ec ,A = X Ec ,A
10. Rather intuitively, two trees equally discriminating associate fault situation
recovery action.
11. exclude terminal calls even compute Obs Update.

508

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

Proof. shall prove (1) (2) every recursive call c (rather c0 ).
proof induction depth recursion starting c.
depth = 0. c terminal, prove XE,A ([[T]]c ) = X E,A .
two reasons c may terminal: either (i) fault situations [[Examples]]c
associated recovery action A, (ii) [[ValidObs]]c = .
(i) denition 19 have:

X E,A =
(merge({Act(sit) | sit })) P (; Ec )
Ec /

=



(A) P (; Ec )

Ec /

= (A)



P (; Ec ) = (A).

Ec /

Since [[T]]c made single leaf l L(l) = also XEc ,A ([[T]]c ) =
(A), proves thesis.
(ii) [[ValidObs]]c = s, [[Examples]]c , > Dl([[Examples]]c ).
Let sit Ec Dl(sit) = Dl([[Examples]]c ). denition
indistinguishability sit Ec sit sit . proves
Ec / made single equivalence class coincides Ec itself. Thus
X Ec ,A = (merge({Act(sit) | sit Ec })). Since [[T]]c made single leaf l
L(l) = merge({Act(sit) | sit Ec }), follows XEc ,A ([[T]]c ) = X Ec ,A .
depth > 0. c terminal ChooseObs selects observation = s, t.
Let v1 , . . . , vk possible values o: c k inner recursive calls
BuildTemporalTree, shall denote respectively c1 , . . . , ck .
{Ec1 , . . . , Eck } partition Ec .
denition expected cost (14) that:
XEc ,A ([[T]]c ) =

k


P (Ec |ovi ; Ec ) XEc |ov



i=1

,A ([[T]]ci )

Ec |ovi Eci dier set observations, [[Obs]]c former
[[Obs Update]]c latter. However that:
P (Ec |ovi ; Ec ) = P (Eci ; Ec ) since probabilities depend fault situations te-set, observations.
XEc |ov ,A ([[T]]ci ) = XEci ,A ([[T]]ci ): expected cost depends also observai
tions, [[T]]ci construction contain labels observations
Eci .
Moreover, since also Ec Ec dier observations P (Eci ; Ec ) = P (Eci ; Ec ).
Therefore write:
XEc ,A ([[T]]c ) =

k


P (Eci ; Ec ) XEci ,A ([[T]]ci )

i=1

509

fiConsole, Picardi, & Theseider Dupre

order prove (1), apply induction hypothesis XEci ,A ([[T]]ci ) X Eci ,A )
obtain:

XEc ,A ([[T]]c )

(18)

k


P (Eci ; Ec ) X Eci ,A

i=1

let us work right-hand side expression 18:
k


P (Eci ; Ec ) X Eci ,A =

i=1

k


P (Eci ; Ec )

i=1

=



(Act()) P (; Eci )

Eci /

k



(Act()) P (; Eci ) P (Eci ; Ec )

i=1 Eci /

=

k



(Act()) P (; Ec )

i=1 Eci /

Notice however {Eci /} partition Ec /; words Ec /
belongs exactly one set Eci /. fact, splitting examples according value
one observation cannot split class undistinguishable observations. Thus
equality becomes:
k


P (Eci ; Ec ) X Eci ,A =

i=1



(Act()) P (; Ec ) = X Ec ,A

Ec /

This, together 18, yields:
(19)

XEc ,A ([[T]]c ) X Ec ,A

mentioned above, dierence Ec Ec former
fewer observations. implies that, sit sit Ec , sit sit Ec well.
means Ec / sub-partition12 Ec / following sense:
partition every Ec / () = {1 , . . . , k } j exists
exactly one j Ec / containing exactly fault situations j . yields:

X Ec ,A =
(Act()) P (; Ec )
Ec /

Since j fault situations corresponding j , necessarily
j containing it, have:

X Ec ,A =
(Act( )) P (; Ec )
Ec / ()

12. Ec / sub-partition Ec / ordinary sense set
observations.

510

fiTemporal Decision Trees: Model-based Diagnosis Dynamic Systems On-Board

() fault situations subset ; thus (Act( ))
(Act()). obtain:

X Ec ,A
(Act()) P ( ; Ec )
Ec / ()

=



(Act())

Ec /

=





P ( ; Ec )

()

(Act()) P (; Ec ) = X Ec ,A

Ec /

Together equation 19, proves (1):
XEc ,A ([[T]]c ) X Ec ,A
let us prove (2). induction hypothesis changes 18, thus 19, equalities, thus yielding:
(20)

XEc ,A ([[T]]c ) = X Ec ,A

Since hypothesis (2) X Ec ,A = X Ec ,A , immediately obtain:
XEc ,A ([[T]]c ) = X Ec ,A
concludes proof.

Proposition 22 Let c, denote two independent calls BuildTemporalTree
input arguments dierent implementations ChooseObs.
[[tlabel]]c [[tlabel]]d X Ec ,A X Ed ,A .

Proof. Follows immediately [[Obs Update]]c [[Obs Update]]d .
Proposition 23 Let c call BuildTemporalTree. [[tlabel]]c = tminc =
min{t | t, [[ValidObs]]c } X Ec ,A X Ec ,A .
Proof. case [[Obs Update]]c = [[UsefulObs]]c , thus removed observa
tions non discriminating ones.
Proposition 25 call c BuildTemporalTree exist time label tmaxc
safe time labels tminc tmaxc , tminc
proposition 23.

Proof. Straightforward.

References
Antunes, C., & Oliveira, A. (2001). Temporal data mining: overview. KDD Workshop
Temporal Data Mining, San Francisco.
Bischof, W., & Caelli, T. (2001). Learning spatio-temporal relational structures. Journal
Applied Intelligence, 15, 707722.
Brusoni, V., Console, L., Terenziani, P., & Theseider Dupre, D. (1998). spectrum
denitions temporal model-based diagnosis. Articial Intelligence, 102 (1), 3979.
511

fiConsole, Picardi, & Theseider Dupre

Cascio, F., Console, L., Guagliumi, M., Osella, M., Sottano, S., & Theseider, D. (1999).
Generating on-board diagnostics dynamic automotive systems based qualitative
deviations.. AI Communications, 12 (1), 3344.
Cascio, F., & Sanseverino, M. (1997). IDEA (Integrated Diagnostic Expert Assistant)
model-based diagnosis car repair centers. IEEE Expert, 12 (6).
Console, L., & Dressler, O. (1999). Model-based diagnosis real world: lessons learned
challenges remaining. Proc. 16th IJCAI, pp. 13931400, Stockholm.
Darwiche, A. (1999). compiling system descriptions diagnostic rules. Proc. 10th
Int. Work. Principles Diagnosis, pp. 5967.
Dvorak, D., & Kuipers, B. (1989). Model-based monitoring dynamic systems. Proc.
11th IJCAI, pp. 12381243, Detroit.
Foresight-Vehicle (2002).
Foresight vehicle automotive roadmap,technology
research directions future road vehicles.
Tech. rep.,
http://www.foresightvehicle.org.uk/initiatives/init01/init01-report.asp.
Geurts, P., & Wehenkel, L. (1998). Early prediction electric power system blackouts
temporal machine learning. Proceedings ICML98/AAAI98 Workshop
Predicting future: AI Approaches time series analysis, Madison, July 24-26.
Mosterman, P., Biswas, G., & Manders, E. (1998). comprehensive framework model
based diagnosis. Proc. 9th Int. Work. Principles Diagnosis, pp. 8693.
Price, C. (1999). Computer-Based Diagnostic Systems. Springer.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.
Russel, S., & Norvig, P. (1995). Articial Intelligence: Modern Approach. Prentice Hall.
Sachenbacher, M., Malik, A., & Struss, P. (1998). electrics emissions: experiences
applying model-based diagnosis real problems real cars. Proc. 9th Int.
Work. Principles Diagnosis, pp. 246253.
Sachenbacher, M., Struss, P., & Weber, R. (2000). Advances design implementation
obd functions diesel injection systems based qualitative approach diagnosis.
SAE 2000 World Congress.

512

fiJournal Artificial Intelligence Research 19 (2003) 631-657

Submitted 10/02; published 12/03

AltAltp : Online Parallelization Plans
Heuristic State Search
Romeo Sanchez Nigenda
Subbarao Kambhampati

rsanchez@asu.edu
rao@asu.edu

Department Computer Science Engineering,
Arizona State University, Tempe AZ 85287-5406

Abstract
Despite near dominance, heuristic state search planners still lag behind disjunctive
planners generation parallel plans classical planning. reason directly
searching parallel solutions state space planners would require planners branch
possible subsets parallel actions, thus increasing branching factor exponentially.
present variant heuristic state search planner AltAlt called AltAltp
generates parallel plans using greedy online parallelization partial plans. greedy
approach significantly informed use novel distance heuristics AltAltp
derives graphplan-style planning graph problem. approach
guaranteed provide optimal parallel plans, empirical results show AltAltp
capable generating good quality parallel plans fraction cost incurred
disjunctive planners.

1. Introduction
Heuristic state space search planning proved one efficient planning
frameworks solving large deterministic planning problems (Bonet, Loerincs, & Geffner,
1997; Bonet & Geffner, 1999; Bacchus, 2001). Despite near dominance, one achilles
heel remains generation parallel plans (Haslum & Geffner, 2000). Parallel plans allow
concurrent execution multiple actions time step. concurrency likely
important progress temporal domains. disjunctive planners
Graphplan (Blum & Furst, 1997) SATPLAN (Kautz & Selman, 1996) GPCSP (Do & Kambhampati, 2000) seem trouble generating parallel plans,
planners search space states overwhelmed task. main reason
straightforward methods generation parallel plans would involve progression
regression sets actions. increases branching factor search space
exponentially. Given n actions, branching factor simple progression regression
search bounded n, progression regression search parallel plans
bounded 2n .
inability state search planners producing parallel plans noted
literature previously. Past attempts overcome limitation successful. Indeed, Haslum Geffner (2000) consider problem generating parallel
plans using regression search space states. note resulting planner, HSP*p, scales significantly worse Graphplan. present TP4 (Haslum &
Geffner, 2001), addition aimed actions durations, also improves
c
2003
AI Access Foundation. rights reserved.

fiSanchez & Kambhampati

branching scheme HSP*p, making incremental along lines Graphplan.
Empirical studies reported Haslum Geffner (2001), however indicate even
new approach, unfortunately, scales quite poorly compared Graphplan variants. Informally, achilles heel heuristic state search planners interpreted sort
last stand disjunctive planners capable generating parallel plans
efficiently.
Given way efficiently generating optimal parallel plans involves using
disjunctive planners, might want consider ways generating near-optimal parallel
plans using state search planners. One obvious approach post-process sequential
plans generated state search planners make parallel. easily
done - using approaches explored Backstrom (1998), one drawback
approaches limited transforming sequential plan given input.
Parallelization sequential plans often results plans close optimal parallel
plans.1
alternative, explore paper, involves incremental online parallelization.
Specifically, planner AltAltp , variant AltAlt planner (Sanchez, Nguyen,
& Kambhampati, 2000; Nguyen, Kambhampati, & Sanchez, 2002), starts search
space regression single actions. promising single action regress
selected, AltAltp attempts parallelize (fatten) selected search branch
independent actions. parallelization done greedy incremental fashion actions considered addition current search branch based heuristic cost
subgoals promise achieve. parallelization continues next step
state resulting addition new action better heuristic cost.
sub-optimality introduced greedy nature parallelization offset
extent plan-compression procedure called Pushup tries rearrange evolving
parallel plans pushing actions higher levels search branch (i.e. later stages
execution) plan.
Despite seeming simplicity approach, proven quite robust
practice. fact, experimental comparison five competing planners - STAN (Long
& Fox, 1999), LPG (Gerevini & Serina, 2002), Blackbox (Kautz & Selman, 1996), SAPA (Do
& Kambhampati, 2001) TP4 (Haslum & Geffner, 2001) - shows AltAltp viable
scalable alternative generating parallel plans several domains. many problems,
AltAltp able generate parallel plans close optimal makespan. also
seems retain efficiency advantages heuristic state search disjunctive planners,
producing plans fraction time taken disjunctive planners many cases.
AltAltp also found superior post-processing approaches. Specifically,
compared AltAltp approach involves post-processing sequential plans
generated AltAlt using techniques Backstrom (1998). found AltAltp
able generate shorter parallel plans many cases. Finally, show AltAltp incurs
little additional overhead compared AltAlt.
rest paper, discuss implementation evaluation approach
generate parallel plans AltAltp . Section 2 starts providing review
AltAlt planning system, AltAltp based. Section 3 describes generation
1. empirically demonstrate later; curious readers may refer plots Figure 15.

632

fiOnline Parallelization Plans Heuristic State Search

Action Templates

Serial Planning
Graph

Graphplan
Plan Extension Phase
(based STAN)

Problem Spec
(Init, Goal state)

Extraction
Heuristics

Actions
Last Level

AltAlt

Heuristics

Regression Planner
(based HSP-R)

Solution Plan

Figure 1: Architecture AltAlt
parallel plans AltAltp . Section 4 presents extensive empirical evaluation AltAltp .
evaluation includes comparison ablation studies. Finally, Section 5 discusses
related work classical well metric temporal planning. Section 6 summarizes
contributions.

2. AltAlt Background Architecture Heuristics
AltAlt planning system based combination Graphplan (Blum & Furst, 1997;
Long & Fox, 1999; Kautz & Selman, 1999) heuristic state space search (Bonet et al.,
1997; Bonet & Geffner, 1999; McDermott, 1999) technology. AltAlt extracts powerful
heuristics planning graph data structure guide regression search space
states. high level architecture AltAlt shown Figure 1. problem specification
action template description first fed Graphplan-style planner (in case,
STAN Long & Fox, 1999), constructs planning graph problem
polynomial time (we assume reader familiar Graphplan algorithm Blum
& Furst, 1997). planning graph structure fed heuristic extractor module
capable extracting variety effective heuristics (Nguyen & Kambhampati,
2000; Nguyen et al., 2002). heuristics, along problem specification,
set ground actions final action level planning graph structure fed
regression state-search planner.
explain operation AltAlt detailed level, need provide
background various components. shall start regression search
module. Regression search process searching space potential plan suffixes. suffixes generated starting goal state regressing
set relevant action instances domain. resulting states (nondeterministically) regressed relevant action instances, process repeated
reach state (set subgoals) satisfied initial state. state
framework set (conjunction of) literals seen subgoals need
made true way achieving top level goals. action instance considered
relevant state effects give least one element delete
633

fiSanchez & Kambhampati

element S. result regressing (S\ef f (a)) prec(a) -
essentially set goals still need achieved application a,
everything would achieved applied. relevant action
a, separate search branch generated, result regressing action
new fringe branch. Search terminates success node every literal
state corresponding node present initial state problem.
crux controlling regression search involves providing heuristic function
estimate relative goodness states fringe current search tree
guide search promising directions. heuristic function needs evaluate
cost achieving set subgoals (comprising regressed state) initial
state. words, heuristic computes length plan needed achieve
subgoals initial state. discuss heuristic computed
planning graph, which, provides optimistic reachability estimates.
Normally, planning graph data structure supports parallel plans - i.e., plans
step one action may executed simultaneously. Since want planning graph provide heuristics regression search module AltAlt, generates
sequential solutions, first make modification algorithm generates
serial planning graph. serial planning graph planning graph which, addition
normal mutex relations, every pair non-noop actions level marked
mutex. additional action mutexes propagate give additional propositional mutexes. Finally, planning graph said level change action,
proposition mutex lists two consecutive levels.
assume given problem, Graphplan module AltAlt used
generate expand serial planning graph levels off. discussed Sanchez
et al. (2000), relax requirement growing planning graph level-off,
tolerate graded loss informedness heuristics derived planning graph.
start notion level set propositions:
Definition 1 (Level) Given set propositions, lev(S) index first level
leveled serial planning graph propositions appear non-mutex
one another. singleton, lev(S) index first level
singleton element occurs. level exists, lev(S) = planning graph
grown level-off.
intuition behind definition level literal p serial planning
graph provides lower bound length plan (which, serial planning graph,
equal number actions plan) achieve p initial state. Using
insight, simple way estimating cost set subgoals sum levels.
Heuristic 1 (Sum heuristic) hsum (S) :=

P

pS

lev({p})

sum heuristic similar greedy regression heuristic used UNPOP (McDermott, 1999) heuristic used HSP planner (Bonet et al., 1997). main
limitation heuristic makes implicit assumption subgoals (elements
S) independent. hsum heuristic neither admissible particularly informed
ignores interactions subgoals. develop effective heuristics,
634

fiOnline Parallelization Plans Heuristic State Search

need consider positive negative interactions among subgoals limited
fashion.
(Nguyen et al., 2002), discuss variety ways using planning graph
incorporate negative positive interactions heuristic estimate, discuss
relative tradeoffs. One best heuristics according analysis heuristic called
hAdjSum2M . adopted heuristic default heuristic AltAlt. basic idea
hAdjSum2M adjust sum heuristic take positive negative interactions
account. heuristic approximates cost achieving subgoals set
sum cost achieving S, considering positive interactions ignoring negative
interactions, plus penalty ignoring negative interactions. first component
RP (S) computed length relaxed plan supporting S,
extracted ignoring mutex relations. approximate penalty induced
negative interactions alone, proceed following argument. Consider pair
subgoals p, q S. negative interactions p q, lev({p, q}),
level p q present together, exactly maximum lev(p) lev(q).
degree negative interaction p q thus quantified by:
(p, q) = lev({p, q}) max (lev(p), lev(q))
want use - values characterize amount negative interactions
present among subgoals given set S. subgoals pair-wise independent,
clearly, values zero, otherwise pair subgoals different
value. largest value among pair subgoals used measure
negative interactions present heuristic hAdjSum2M . summary,
Heuristic 2 (Adjusted 2M) hAdjSum2M (S) := length(RP (S)) + maxp,qS (p, q)
analysis Nguyen et al. (2002) shows one robust heuristics
terms solution time quality. thus default heuristic used AltAlt
(as well AltAltp ; see below).

3. Generation Parallel Plans Using AltAltp
obvious way make AltAlt produce parallel plans would involve regressing subsets
(non interfering) actions. Unfortunately, increases branching factor exponentially
infeasible practice. Instead, AltAltp uses greedy depth-first approach makes
use heuristics regress single actions, incrementally parallelizes partial plan
step, rearranging partial plan later necessary.
high level architecture AltAltp shown Figure 2. Notice heuristic
extraction phase AltAltp similar AltAlt, one important modification. contrast AltAlt uses serial planning graph basis
heuristic (see Section 2), AltAltp uses standard parallel planning graph. makes
sense given AltAltp interested parallel plans AltAlt aimed generating
sequential plans. regression state-search engine AltAltp also different
search module AltAlt. AltAltp augments search engine AltAlt 1) fattening
step 2) plan compression procedure (Pushup). details procedures
discussed below.
635

fiSanchez & Kambhampati

Action Templates

Parallel
Planning
Graph

Graphplan
Plan Extension Phase
(based STAN)

Extraction
Heuristics

Actions
Last Level

AltAltp

Heuristics

Problem Spec
(Init, Goal state)

Node
Expansion
(Fattening)

Node Ordering
Selection

Plan
Compression
Algorithm
(PushUp)

Solution Plan

Figure 2: Architecture AltAltp



A={a 1 ,a 2 ,...,a p ,...a }

a1

a2

S1

S2

ap
. ..


Sp

Figure 3: AltAltp Notation

636

. ..

Sm

fiOnline Parallelization Plans Heuristic State Search

parexpand(S)
get set applicable actions current state
forall ai
Si Regress(S,ai )
CHILDREN(S) CHILDREN(S) + Si
Sp state among Children(S) minimum
hadjsum2M value
ap action regresses Sp
/**Fattening process
{ ap }
forall g ranked decreasing order level(g)
Find action ag supporting g ag 6
ai pairwise independent action O.
multiple actions, pick one
minimum hadjsum (Regress(S, + ag )) among ag
hadjsum2M (S, + ai ) < hadjsum2M (S, O)
+ ag
Spar Regress(S, O)
CHILDREN(S) CHILDREN(S) + Spar
return CHILDREN
END;
Figure 4: Node Expansion Procedure
general idea AltAltp select fringe action ap among actions
used regress particular state stage search (see Figure 3). Then,
pivot branch given action ap fattened adding actions A,
generating new state consequence regression multiple parallel actions.
candidate actions used fattening pivot branch must (a) come sibling
branches pivot branch (b) pairwise independent actions
currently pivot branch. use standard definition action independence: two
actions a1 a2 considered independent state resulting regressing
actions simultaneously obtained applying a1 a2 sequentially
possible linearizations. sufficient condition preconditions
effects actions interfere:
((|prec(a1 )| |ef f (a1 )|) (|prec(a2 )| |ef f (a2 )|)) =
|L| refers non-negated versions literals set L. discuss
details pivot branch selected first place, branch
incrementally fattened.
Selecting Pivot Branch: Figure 4 shows procedure used select parallelize
pivot branch. procedure first identifies set regressable actions
current node S, regresses them, computing new children states. Next,
action leading child state lowest heuristic cost among new children
selected pivot action ap , corresponding branch becomes pivot branch.
637

fiSanchez & Kambhampati


At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)

H=23
am: Unload(pack4,airp2,Home)

ap: Unload(pack1,airp1,ASU)

a1: Unload(pack1,airp2,ASU)

Sp
H=21
Pivot
Unload(pack2,airp1,ASU)
Unload(pack2,airp2,ASU)
Unload(pack3,airp1,ASU)
Unload(pack3,airp2,ASU)
Unload(pack4,airp2,HOME)

S1
H=21

...

Sm
H=22

Possible Pairwise
Independent Actions

Figure 5: regression state, identify P ivot related set
pairwise independent actions.

heuristic cost states computed hadjsum2M heuristic AltAlt,
based parallel planning graph. Specifically, context discussion
hadjsum2M heuristic end Section 2, compute (p, q) values, turn
depend level(p), level(q) level(p, q) terms levels parallel planning
graph rather serial planning graph. easy show level set
conditions parallel planning graph less equal level serial
planning graph. length relaxed plan still computed terms number
actions. show later (see Figure 19(a)) change improve quality
parallel plans produced AltAltp .
search algorithm used AltAltp similar used HSPr (Bonet & Geffner,
1999) - hybrid greedy depth first weighted A* search. goes depthfirst long heuristic cost children states lower
current state. Otherwise, algorithm resorts weighted A* search select next
node expand. latter case, evaluation function used rank nodes
f (S) = g(S) + w h(S) g(S) length current partial plan terms
number steps, h(S) estimated cost given heuristic function (e.g. hAdjSum2M ),
w weight given heuristic function. w set 5 based empirical
experience.2
Breaking Ties: case tie selecting pivot branch, i.e., one branch
leads state lowest heuristic cost, break tie choosing action
2. role w Best-First search see (Korf, 1993).

638

fiOnline Parallelization Plans Heuristic State Search

supports subgoals harder achieve. Here, hardness literal l measured
terms level planning graph l first appears. standard rationale
decision (c.f. Kambhampati & Sanchez, 2000) want fail faster
considering difficult subgoals first. additional justification case,
also know subgoal higher level value requires steps actions
achievement appeared later planning graph. So, supporting
first, may able achieve easier subgoals along way thereby reduce
number parallel steps partial plan.
Fattening Pivot Branch: Next procedure needs decide subset
sibling actions pivot action ap used fatten pivot branch.
obvious first idea would fatten pivot branch maximally adding pairwise
independent actions found search stage. problem idea
may add redundant heuristically inferior actions branch, satisfying
preconditions may lead increase number parallel steps.
So, order avoid fattening pivot branch irrelevant actions,
adding action O, require heuristic cost state results
regressing + strictly lower S. addition
requirement pairwise independent current set actions O.
simple check also ensures add one action supporting
set subgoals S.
overall procedure fattening pivot branch thus involves picking next
hardest subgoal g (with hardness measured terms level subgoal
planning graph), finding action ag achieving g, pair-wise independent
actions which, added used regress S, leads state
lowest heuristic cost, consequence lower cost S.
found, ag added O, procedure repeated. one
action ag , break ties considering degree overlap
preconditions action ag set actions currently O. degree precondition
overlap defined |prec(a) {oO prec(o)}|. action higher
degree overlap preferred reduce amount additional work need
establish preconditions. Notice fattening process, search
node may multiple actions leading parent, multiple actions leading
children.
Example: Figure 5 illustrates use node expansion procedure problem
logistics domain (Bacchus, 2001). example four packages pack1,
pack2, pack3 pack4. goal place first three ASU
remaining one home. two planes airp1 airp2 carry plans.
figure shows first level search regressed. also shows pivot
action ap given unload(pack1,airp1,ASU), candidate set pairwise independent
actions respect ap . Finally, see Figure 6 generation parallel
branch. Notice node seen partial regressed plan. described
paragraphs above, actions regressing lower heuristic estimates considered apar
fatten pivot branch. Notice action unload(pack4,airp2,Home)
discarded leads state higher cost, even though inconsistent
639

fiSanchez & Kambhampati


At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)
apar: Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)

H=23
am: Unload(pack4,airp2,Home)

a1: Unload(pack1,airp2,ASU)
ap: Unload(pack1,airp1,ASU)

Spar
H=19

Sp

S1
H=21

H=21
Pivot

...

Sm
H=22


Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack2,airp2,ASU)
Unload(pack3,airp1,ASU)
Unload(pack3,airp2,ASU)
Unload(pack4,airp2,HOME)

Figure 6: Spar result incrementally fattening P ivot branch pairwise
independent actions

rest actions chosen fatten pivot branch. Furthermore, also
see preferred actions using plane airp1, since overlap
pivot action ap .
Offsetting Greediness Fattening: fattening procedure greedy, since
insists state resulting fattening strictly better heuristic value.
useful avoiding addition irrelevant actions plan, procedure also
sometimes preclude actions ultimately relevant discarded
heuristic perfect. actions may become part plan later stages
search (i.e., earlier parts execution eventual solution plan; since
searching space plan suffixes). happens, length parallel plan
likely greater, since steps may needed support preconditions
actions would forced come even later stages search (earlier parts
plan). action allowed partial plan earlier search (i.e., closer
end eventual solution plan), preconditions could probably achieved
parallel subgoals plan, thus improving number steps.
order offset negative effect greediness, AltAltp re-arranges partial plan
promote actions higher search branch (i.e., later parts execution
eventual solution plan). Specifically, expanding given node S, AltAltp checks
see actions leading parent node (i.e., Figure 6 shows
Apar leads Spar ) pushed higher levels search branch. online
640

fiOnline Parallelization Plans Heuristic State Search

pushUP(S)
get actions leading
forall
x0
Sx get parent node
/** Getting highest ancestor action
Loop
Ax get actions leading Sx
(parallel(a, Ax ))
x x+1
Sx get parent node Sx1
Else
aj get action conflicting Ax
(Secondary Optimizations)
Remove aj branch
Include anew necessary
Else
Ax1 Ax1 +

break
End Loop
/**Adjusting partial plan
Sx get highest ancestor x history
createN ewBranchF rom(Sx )
x > 0
Snew regress Sx Ax1
Sx Snew
x x1
END;

Figure 7: Pushup Procedure

re-arrangement plan done Pushup procedure, shown Figure 7.
Pushup procedure called time node gets expanded, try
compress partial plan. actions find highest ancestor node
Sx search branch action applied (i.e., gives literal
Sx without deleting literals Sx , pairwise independent actions
Ax currently leading Sx , words condition parallel(a, Ax ) satisfied).
Sx found, removed set actions leading introduced
set actions leading Sx (to child current search branch). Next,
states search branch Sx adjusted reflect change. adjustment
involves recomputing regressions search nodes Sx . first glance,
might seem like transformation questionable utility since preconditions (and
regressions) become part descendants Sx , necessarily
reduce length plan. however expect length reduction actions
supporting preconditions get pushed eventually later expansions.
641

fiSanchez & Kambhampati


At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)
Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)

H=23
Unload(pack4,airp2,Home)

Unload(pack1,airp2,ASU)
Unload(pack1,airp1,ASU)

Spar

Sp

H=19

...

S1

H=21

H=21

Sm
H=22

Unload(pack4,airp2,Home)
fly(airp1,LocX,ASU)
Spar

Sp

...

Sm

H=18
Pivot

Unload(pack4,airp2,Home)
fly(airp1,LocX,ASU)

H=18

H=16

(a) Finding highest ancestor node action
pushed up.


Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)
Unload(pack4,airp2,HOME)

At(pack1,ASU)
At(pack2,ASU)
At(pack3,ASU)
At(pack4, Home)

H=23
Unload(pack4,airp2,Home)

Unload(pack1,airp1,ASU)
Unload(pack2,airp1,ASU)
Unload(pack3,airp1,ASU)

Unload(pack1,airp2,ASU)

Unload(pack1,airp1,ASU)
Spar

Snew

Sp

H=19

S1

H=21

Sm

H=21

fly(airp1,LocX,ASU)

fly(airp1,LocX,ASU)

...

Snew

Spar

Sp

H=16

fly(airp1,LocX,ASU)
Unload(pack4,airp2,Home)

H=18
Pivot

H=22

Unload(pack4,airp2,Home)

...

Sm
H=18

H=16

(b) Pushup procedure generates new search branch.

Figure 8: Rearranging Partial Plan
642

fiOnline Parallelization Plans Heuristic State Search

Rather doctor existing branch, current implementation, add
new branch Sx reflects changes made Pushup procedure.3 new
branch becomes active search branch, leaf node expanded next.
Aggressive Variation Pushup: Pushup procedure, described above,
expensive affects current search branch, operations involved
recomputing regressions branch. course, possible aggressive
manipulating search branch. example, applying action ancestor
Sx set literals child state, say Snew changes, thus additional actions may
become relevant expanding Snew . principle, could re-expand Snew light
new information. decided go re-expansion option, typically
seem worth cost. Section 4.3, compare default version Pushup
procedure variant re-expands nodes search branch, results
studies support decision avoid re-expansion. Finally, although introduced
Pushup procedure add-on fattening step, also used independent
latter, case net effect would incremental parallelization sequential
plan.
Example: Figure 8(a), two actions leading node Spar (at depth two),
two actions Unload(pack4,airp2,Home) fly(airp1,LocX,ASU). So,
expanding Spar check two actions leading pushed up.
second action pushable since interacts actions ancestor node, first
one is. find highest ancestor partial plan interacts pushable
action. example root node ancestor. So, insert pushable
action Unload(pack4,airp2,Home) directly root node. re-adjust
state Spar Snew depth 1, shown Figure 8(b), adding new branch, reflecting
changes states below. Notice action Unload(pack4,airp2,Home)
initially discarded greediness fattening procedure (see Figure 6),
offset negative effect plan compression algorithm. see also
re-expanded state Snew depth 1, made adjustments
partial plan using actions already presented search trace.4

4. Evaluating Performance AltAltp
implemented AltAltp top AltAlt. tested implementation suite
problems used 2000 2002 AIPS competition (Bacchus, 2001; Long
& Fox, 2002), well benchmark problems (McDermott, 2000). experiments
broadly divided three sets, aimed comparing performance AltAltp
different scenarios:
1. Comparing performance AltAltp planning systems capable producing
parallel plans.
3. way data structures set up, adding new branch turns robust
option manipulating existing search branch.
4. Instead, aggressive Pushup modification would expand Snew depth 1, generating similar states
generated expansion Spar depth.

643

fiSanchez & Kambhampati

80

70

60

35

Gripper AIPS-98

AltAlt-p
STAN
TP4
Blackbox
LPG 2nd

Elevator AIPS-00
AltAlt-p
STAN

30

Blackbox
LPG 2nd

25

50

Steps

20

Steps

40

15

30
10

20

5

10

0

0
1

2

3

4

5

Problems

6

7

1

8

3

5

7

9

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49

Problems

(b)

(a)

Figure 9: Performance Gripper (AIPS-98) Elevator (AIPS-00) Domains.
12

1400

AltAlt-p
STAN
Blackbox

10

1200

1000

Steps

8

Time

800

6

600

4
400

2
200

0

0

1

4

7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70

1

Problems

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70

Problems

(b)

(a)

Figure 10: Performance Schedule domain (AIPS-00)
2. Comparing incremental parallelization technique AltAlt + Post-Processing.
3. Ablation studies analyze effect different parts AltAltp approach
overall performance.
experiments done Sun Blade-100 workstation, running SunOS 5.8
1GB RAM. Unless noted otherwise, AltAltp run hadjsum2M heuristic
644

fiOnline Parallelization Plans Heuristic State Search

90

450

Altalt-p
STAN
TP4
Blackbox
LPG 2nd

80

70

350
300

50

Time

Steps

60

400

250

40

200

30

150

20

100
50

10

0

0
1

3

5

7

1

9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

Problems

Problems

(a)

(b)

Figure 11: Performance Logistics domain(AIPS-00)

35

800

AltAlt-p
STAN
TP4
Blackbox
LPG 2nd
Sapa

30

25

700

600

400

Time

Steps

500

20

15

300

10
200

5
100

0
1

2

3

4

5

6

7

8

9

10

11

12

13

14

0

15

1

Problems

2

3

4

(a)

5

6

7

8

9

Problems

10

11

12

13

14

15

(b)

Figure 12: Performance DriverLog domain(AIPS-02)
described section 2 paper, parallel planning graph grown first
level top-level goals present without mutex. times seconds.
4.1 Comparing AltAltp Competing Approaches
first set experiments compared performance planner
results obtained running STAN (Long & Fox, 1999), Blackbox (Kautz & Selman,
645

fiSanchez & Kambhampati

1999), TP4 (Haslum & Geffner, 2001), LPG (Gerevini & Serina, 2002) SAPA (Do &
Kambhampati, 2001). Unless noted otherwise, every planner run default
settings. planners could run domains due parsing problems
memory allocation errors. cases, omit planner consideration
particular domains.
4.1.1 Planners Used Comparison Studies
STAN disjunctive planner, optimized version Graphplan algorithm
reasons invariants symmetries reduce size search space. Blackbox
also based Graphplan algorithm works converting planning problems
specified STRIPS (Fikes & Nilsson, 1971) notation boolean satisfiability problems
solving using SAT solver (the version used defaults SATZ).5 LPG (Gerevini
& Serina, 2002) judged best performing planner 3rd International Planning
Competition (Long & Fox, 2002), planner based planning graphs local
search inspired Walksat approach. LPG run default heuristics
settings. Since LPG employs iterative improvement algorithm, quality plans
produced improved running multiple iterations (thus increasing
running time). make comparisons meaningful, decided run LPG two
iterations (n=2), since beyond that, running time LPG generally worse
AltAltp . Finally, also chosen two metric temporal planners, able
represent parallel plans representation time durative actions.
TP4 (Haslum & Geffner, 2001) temporal planner based HSP*p (Haslum & Geffner,
2000), optimal parallel state space planner IDA* search algorithm.
last planner list SAPA (Do & Kambhampati, 2001). SAPA powerful
domain-independent heuristic forward chaining planner metric temporal domains
employs distance-based heuristics (Kambhampati & Sanchez, 2000) control search.
4.1.2 Comparison Results Different Domains
run planners Gripper domain International Planning
Scheduling competition 1998 (McDermott, 2000), well three different domains
(Logistics, Scheduling, Elevator-miconic-strips) 2000 (Bacchus, 2001), three
2002 competition (Long & Fox, 2002) - DriverLog, ZenoTravel, Satellite.
cases multiple versions domain, used STRIPS Untyped
versions.6 . discuss results domains below.
Gripper: Figure 9(a), compare performance AltAltp Gripper domain (McDermott, 2000) rest planners excluding SAPA. plot shows
results terms number (parallel) steps. see even simplistic domain, AltAltp LPG planners capable scaling generating parallel
5. chosen IPP (Koehler, 1999), also optimized Graphplan planning system
results reported Haslum Geffner (2001) show already less efficient STAN.
6. Since SAPA read STRIPS file format, run SAPA planner equivalent problems
unit-duration actions Long Fox (2002).

646

fiOnline Parallelization Plans Heuristic State Search

5000

40

AltAlt-p
STAN
LPG 2nd
Sapa

35

4500
4000

30
3500

25

3000

Steps

Time

20

15

2500
2000
1500

10
1000

5

500
0

0
1

2

3

4

5

6

7

8

9

10

11

12

13

14

1

15

Problems

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Problems

(b)

(a)

Figure 13: Performance ZenoTravel domain (AIPS-02)
plans. None approaches able solve four problems.7 AltAltp
able scale without difficulty problems involving 30 balls. Furthermore, AltAltp
returns better plans LPG.
Elevator: Figure 9(b), compare AltAltp STAN, Blackbox LPG Elevator
domain (Miconic Strips) (Bacchus, 2001).8 AltAltp approached quality solutions
produced optimal approaches (e.g. Blackbox STAN). Notice Blackbox
solve around half problems solved AltAltp domain.
Scheduling: Results Scheduling domain shown Figure 10. Blackbox
STAN considered comparison.9 AltAltp seems reasonably approximate
optimal parallel plans many problems (around 50 them), produce significantly
suboptimal plans some. However, able solve problems
two approaches fraction time.
Logistics: plots corresponding Logistics domain Bacchus (2001) shown
Figure 11.10 difficult problems AltAltp outputs lower quality
solutions optimal approaches. However, AltAltp LPG able scale
complex problems, easily see AltAltp provides better quality
solutions LPG. AltAltp also seems efficient approaches.
7. Although STAN supposed able generate optimal step-length plans, handful cases
seems produced nonoptimal solutions Gripper Domain. explanation
behavior, informed authors code.
8. include traces TP4 pre-processor planner able read
domain.
9. TP4 pre-processor cannot read domain, LPG runs memory, SAPA parsing
problems.
10. SAPA excluded due parsing problems.

647

fiSanchez & Kambhampati

5000

60

AltAlt-p
STAN
TP4
Blackbox

50

4500
4000

LPG 2nd
Sapa

40

3500
3000

Steps

Time

30

2500
2000

20
1500
1000

10

500

0

0

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16 17 18 19

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

Problems

Problems

(b)

(a)

Figure 14: Performance Satellite domain(AIPS-02)
LPG solutions problems 49 61 obtained one iteration, since LPG
able complete second iteration reasonable amount time. explains
low time taken LPG, lower quality solutions.
DriverLog: see Figure 12(a) AltAltp reasonably well terms quality
respect approaches DriverLog domain. Every planner considered
time. AltAltp one two planners able scale up. Figure 12(b) shows also
AltAltp efficient planners.
Zeno-Travel: AltAltp , SAPA, LPG able solve problems
domain.11 AltAltp solves efficiently (Figure 13(b)) providing good solution
quality (Figure 13(a)) compared temporal metric planners.
Satellite: results Satellite domain shown Figure 14. Although every
planner considered, AltAltp , SAPA, LPG solve problems. SAPA
solves problems produces lower quality solutions many them. AltAltp produces
better solution quality SAPA, also efficient. However, AltAltp produces
lower quality solutions LPG four problems. LPG cannot solve one problems
produces lower quality solutions 5 them.
Summary: summary, note AltAltp significantly superior elevator
gripper domains. also performs well DriverLog, ZenoTravel, Satellite
domains 2002 competition (Long & Fox, 2002). performance planners
similar Schedule domain. Logistics domain, quality AltAltp plans
second Blackbox problems optimal planner solve.
However, scales along LPG bigger size problems, returning good step11. Blackbox TP4 able parse domain.

648

fiOnline Parallelization Plans Heuristic State Search

250

900

AltAlt-PostProc
AltAlt-p
AltAlt

800

200

700
600

150

Time

Steps

500
400

100
300
200

50

100
0

0

1

4

7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

1

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

Problems

Problems

(a)

(b)

Figure 15: AltAlt Post-Processing vs. AltAltp (Logistics domain)

450

60

AltAlt
AltAlt-PostProc
AltAlt-p

50

400
350
300

Time

40

Steps

30

250
200
150

20

100
10
50
0

0
1

2

3

4

5

6

7

8

Problems

9

10

11

12

13

14

1

15

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Problems

(a)

(b)

Figure 16: AltAlt Post-Processing vs. AltAltp (Zenotravel domain)

length quality plans. TP4, heuristic state search regression planner capable
producing parallel plans able scale domains. SAPA, heuristic
search progression planner, competitive, still outperformed AltAltp planning
time solution quality.
649

fiSanchez & Kambhampati

SOLUTION: solution found (length = 9)
Time 1: load-truck(obj13,tru1,pos1) Level: 1
Time 1: load-truck(obj12,tru1,pos1) Level: 1
Time 1: load-truck(obj11,tru1,pos1) Level: 1
Time 2: drive-truck(tru1,pos1,apt1,cit1) Level: 1
Time 3: unload-truck(obj12,tru1,apt1) Level: 3
Time 3: fly-airplane(apn1,apt2,apt1) Level: 1
Time 3: unload-truck(obj11,tru1,apt1) Level: 3
Time 4: load-airplane(obj12,apn1,apt1) Level: 4
Time 4: load-airplane(obj11,apn1,apt1) Level: 4
Time 5: load-truck(obj21,tru2,pos2) Level: 1
Time 5: fly-airplane(apn1,apt1,apt2) Level: 2
Time 6: drive-truck(tru2,pos2,apt2,cit2) Level: 1
Time 6: unload-airplane(obj11,apn1,apt2) Level: 6
Time 7: load-truck(obj11,tru2,apt2) Level: 7
Time 7: unload-truck(obj21,tru2,apt2) Level: 3
Time 8: drive-truck(tru2,apt2,pos2,cit2) Level: 2
Time 9: unload-airplane(obj12,apn1,apt2) Level: 6
Time 9: unload-truck(obj13,tru1,apt1) Level: 3
Time 9: unload-truck(obj11,tru2,pos2) Level: 9
Total Number actions Plan: 19

POST PROCESSED PLAN ...
Time: 1 : load-truck(obj13,tru1,pos1)
Time: 1 : load-truck(obj12,tru1,pos1)
Time: 1 : load-truck(obj11,tru1,pos1)
Time: 1 : fly-airplane(apn1,apt2,apt1)
Time: 1 : load-truck(obj21,tru2,pos2)
Time: 2 : drive-truck(tru1,pos1,apt1,cit1)
Time: 2 : drive-truck(tru2,pos2,apt2,cit2)
Time: 3 : unload-truck(obj12,tru1,apt1)
Time: 3 : unload-truck(obj11,tru1,apt1)
Time: 3 : unload-truck(obj21,tru2,apt2)
Time: 3 : unload-truck(obj13,tru1,apt1)
Time: 4 : load-airplane(obj12,apn1,apt1)
Time: 4 : load-airplane(obj11,apn1,apt1)
Time: 5 : fly-airplane(apn1,apt1,apt2)
Time: 6 : unload-airplane(obj11,apn1,apt2)
Time: 6 : unload-airplane(obj12,apn1,apt2)
Time: 7 : load-truck(obj11,tru2,apt2)
Time: 8 : drive-truck(tru2,apt2,pos2,cit2)
Time: 9 : unload-truck(obj11,tru2,pos2)
END POST PROCESSING: Actions= 19 Length: 9

(a) AltAltp Solution

(b) AltAltp plus Post-processing

Figure 17: Plots showing AltAltp solutions cannot improved anymore Postprocessing.

4.2 Comparison Post-Processing Approaches
mentioned earlier (see Section 1), one way producing parallel plans
studied previously literature post-process sequential plans (Backstrom, 1998).
compare online parallelization post-processing, implemented Backstrom (1998)s
Minimal De-ordering Algorithm, used post-process sequential plans produced
AltAlt (running default heuristic hAdjSum2M using serial planning graph).
section compare online parallelization procedure offline method.
first set experiments Logistics domain (Bacchus, 2001). results
shown Figure 15. expected, original AltAlt longest plans since allows
one action per time step. plot shows post-processing techniques help
reducing makespan plans generated AltAlt. However, also notice
AltAltp outputs plans better makespan either AltAlt AltAlt followed postprocessing. shows online parallelization better approach post-processing
sequential plans. Moreover, plot Figure 15(b) shows time taken AltAltp
largely comparable taken two approaches. fact, much
additional cost overhead procedure.
Figure 16 repeats experiments ZenoTravel domain (Long & Fox, 2002).
again, see AltAltp produces better makespan post-processing sequential
plans AltAlt. Notice time, AltAlt plus post-processing clearly less efficient
650

fiOnline Parallelization Plans Heuristic State Search

140

Logistics AIPS-00

600

AltAlt-p
AltAlt-p NoPush
AltAlt-p AGR

120

Logistics AIPS-00

500

100

Time

Steps

400

80

300

60
200

40
100

20

0

0

1

4

7

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

Problems

Problems

(a)

100

(b)

Satellite AIPS-02

250

Satellite AIPS-02

90

AltAlt-p
AltAlt-NoPush
AltAlt-p AGR

80

200

60

150

Time

Steps

70

50
40

100

30
20

50

10
0

0

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20

Problems

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

Problems

(c)

(d)

Figure 18: Analyzing effect Pushup procedure

either two approaches. summary, results section demonstrate
AltAltp superior AltAlt plus post-processing.
One might wonder plans generated AltAltp also benefit postprocessing phase. investigated issue found specific post-processing
routines used produce improvements. main reason
behavior Pushup procedure already tries exploit opportunity
shortening plan length promoting actions partial plan. illustrative
example, show, Figure 17, parallel plan output AltAltp problem
logistics domain (logistics-4-1 Bacchus, 2001), result post-processing
651

fiSanchez & Kambhampati

80

140

Logistics AIPS-00

BlocksWorld AIPS-00

Serial PG
Parallel PG

70

120

60

AltAlt-p
AltAlt

100

50

Time

Steps

80

40

60

30
40

20
20

10

0

0
1

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61

1

3

5

7

9

11

13

Problems

15

17

19

21

Problems

23

25

27

29

31

33

35

(b) Solving Serial domain

(a) Utility using Parallel Planning Graphs

Figure 19: Plots showing utility using parallel planning graphs computing
heuristics, characterizing overhead incurred AltAltp serial domains.

solution. Although two solutions differ terms step contents, notice
step length. difference step contents explained fact
de-ordering algorithm relaxes ordering relations plan, allowing
actions come earlier, Pushup always moves actions towards end plan.
run comprehensive studies three different domains (Logistics, Satellite
Zenotravel), found case step length plan produced AltAltp
improved post-processing routine (we omit comparison plots since essentially
show curves corresponding AltAltp AltAltp post-processing coincident).12
4.3 Ablation Studies
section attempts analyze impact different parts AltAltp performance.
Utility Pushup Procedure: Figure 18 shows effects running AltAltp
without Pushup procedure (but fattening procedure), well running
aggressive version Pushup, described Section 3, re-expands
nodes search branch, action pushed up. see running
AltAltp Pushup fattening procedures better latter. Comparison
results Figure 15(a) Figure 18(a) shows even fattening procedure
performs better original AltAlt. Figure 18(b) see although
Pushup procedure add much overhead, aggressive version Pushup get
quite expensive. also notice around 20 problems solved within time limits
12. also verified least one problem domains.

652

fiOnline Parallelization Plans Heuristic State Search

aggressive Pushup. plots Figure 18(c) Figure 18(d) show results
experiments Satellite domain. see situation quite similar
domain. conclude Pushup procedure, used offset greediness
algorithm, achieves purpose.
Utility basing heuristics Parallel Planning Graphs: see Figure 19(a) using parallel planning graph basis deriving heuristic estimates
AltAltp winning idea. serial planning graph overestimates heuristic values
terms steps, producing somewhat longer parallel solutions. fact version
using serial planning graph runs time many problems also demonstrates
running times also improved use parallel planning graphs.
Comparison AltAlt: One final concern would much extra computational
hit taken AltAltp algorithm serial domains (e.g. Blocks-World Bacchus,
2001). expect negligible confirm intuitions, ran AltAltp set
problems sequential Blocks-World domain. see plot 19(b)
time performance AltAlt AltAltp equivalent almost problems.

5. Related work
idea partial exploration parallelizable sets actions new (Kabanza, 1997;
Godefroid & Kabanza, 1991; & Kambhampati, 2001). studied area
concurrent reactive planning, one main goals approximate optimal
parallelism. However, research focused forward chaining
planners (Kabanza, 1997), state world completely known.
implied backward-search methods suitable kind analysis (Godefroid
& Kabanza, 1991) search nodes correspond partial states. shown
backward-search methods also used approximate parallel plans context
classical planning.
Optimization plans according different criteria (e.g. execution time, quality, etc)
also done post-processing step. post-processing computation given
plan maximize parallelism discussed Backstrom (1998). Reordering
de-ordering techniques used maximize parallelism plan. de-ordering
techniques ordering relations removed, added. reordering, arbitrary
modifications plan allowed. general case problem NP-Hard
difficult approximate (Backstrom, 1998). Furthermore, discussed Section 1
4, post-processing techniques concerned modifying order
existing actions given plan. approach considers modifying orderings
also inserting new actions online minimize possible number parallel
steps overall problem.
already discussed Graphplan based planners (Long & Fox, 1999; Kautz &
Selman, 1999), return optimal plans based number time steps. Graphplan
uses IDA* include greatest number parallel actions time step
search. However, iterative procedure time consuming provide
guarantee number actions final plans. attempts
minimize number actions planners (Huang, Selman, & Kautz, 1999)
653

fiSanchez & Kambhampati

using domain control knowledge based generation rules specific
planning domain. Graphplan algorithm tries maximize parallelism satisfying
subgoals time step, search fails backtracks reduces
set parallel actions considered one level before. AltAltp opposite,
tries guess initial parallel nodes given heuristics, iteratively adds actions
nodes possible Pushup procedure later search.
recently, work generalizing forward state search handle action concurrency metric temporal domains. particular relevance work
Temporal TLPlan (Bacchus & Ady, 2001) SAPA (Do & Kambhampati, 2001).
planners designed specifically handling metric temporal domains, use
similar search strategies. main difference Temporal TLPlan
depends hand-coded search control knowledge guide search, SAPA (like
AltAltp ) uses heuristics derived (temporal) planning graphs. such,
planners co-opted produce parallel plans classical domains. planners forward chaining search, like AltAltp , achieve concurrency
incrementally, without projecting sets actions, following way. Normal forward
search planners start initial state S0 , corresponding time t0 , consider actions
apply S0 , choose one, say a1 apply S0 , getting S1 . simultaneously
progress system clock t0 t1 . order allow concurrency, planners
Bacchus Ady (2001), Kambhampati (2001) essentially decouple
action application clock progression. every point search, nondeterministic choice - progressing clock, applying (additional) actions
current time point. point view planners, AltAltp seen providing heuristic guidance non-deterministic choice (modulo difference AltAltp
regression search). results empirical comparisons AltAltp SAPA,
show AltAltp outperforms SAPA, suggest heuristic strategies employed
AltAltp including incremental fattening, pushup procedure, gainfully
adapted planners increase concurrency solution plans. Finally, HSP*,
TP4, extension temporal domains, heuristic state search planners using
regression capable producing parallel plans (Haslum & Geffner, 2000). TP4
seen regression version approach used SAPA temporal TLPlan.
experiments however demonstrate neither planners scales well comparison
AltAltp .
Pushup procedure seen plan compression procedure. such, similar
plan compression procedures double-back optimization (Crawford, 1996).
One difference double-back used context local search, Pushup
used context systematic search. Double-back could also applied
finished plan schedule, post-processing approach outcome would
depend highly plan given input.

6. Concluding Remarks
Motivated acknowledged inability heuristic search planners generate parallel
plans, developed presented approach generate parallel plans
context AltAlt, heuristic state space planner. challenging problem
654

fiOnline Parallelization Plans Heuristic State Search

exponential branching factor incurred naive methods. approach tries avoid
branching factor blow greedy online parallelization evolving partial
plans. plan compression procedure called Pushup used offset ill effects
greedy search. empirical results show comparison planners capable
producing parallel plans, AltAltp able provide reasonable quality parallel plans
fraction time competing approaches. approach also seems provide better
quality plans achieved post-processing sequential plans. results show
AltAltp provides attractive tradeoff quality efficiency generation
parallel plans. future, plan adapt AltAltp approach metric temporal
domains, need concurrency pressing. One idea adapt
sources strength AltAltp SAPA, metric temporal planner developed
group (Do & Kambhampati, 2001).

Acknowledgments
thank Minh B. XuanLong Nguyen helpful discussions feedback. also
thank David Smith JAIR reviewers many constructive comments. research
supported part NASA grants NAG2-1461 NCC-1225, NSF grant
IRI-9801676.

References
Bacchus, F. (2001). AIPS00 planning competition. AI Magazine, 22 (3), 4756.
Bacchus, F., & Ady, M. (2001). Planning resources concurrency: forward
chaining approach. Proceedings IJCAI-01, pp. 417424.
Backstrom, C. (1998). Computational aspects reordering plans. Journal Artificial
Intelligence Research, 9, 99137.
Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90, 281300.
Bonet, B., & Geffner, H. (1999). Planning heuristic search: new results. Proceedings
ECP-99.
Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism
planning. Proceedings AAAI-97, pp. 714719. AAAI Press.
Crawford, J. (1996). approach resource-constrained project scheduling. Proceedings
1996 Artificial Intelligence Manufacturing Research Planning Workshop.
AAAI Press.
Do, M. B., & Kambhampati, S. (2000). Solving planning graph compiling CSP.
Proceedings AIPS-00, pp. 8291.
Do, M. B., & Kambhampati, S. (2001). SAPA: domain-independent heuristic metric
temporal planner. Proceedings ECP-01.
655

fiSanchez & Kambhampati

Fikes, R., & Nilsson, N. (1971). Strips: new approach application theorem
proving problem solving. Artificial Intelligence, 2 (3-4), 189208.
Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphs.
Proceedings AIPS-02. AAAI Press.
Godefroid, P., & Kabanza, F. (1991). efficient reactive planner synthesizing reactive
plans. Proceedings AAAI-91, Vol. 2, pp. 640645. AAAI Press/MIT Press.
Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Proceedings
AIPS-00, pp. 140149.
Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. Proceedings
ECP-01. Springer.
Huang, Y., Selman, B., & Kautz, H. (1999). Control knowledge planning: benefits
tradeoffs. Proceedings AAAI/IAAI-99, pp. 511517.
Kabanza, F. (1997). Planning verifying reactive plans (position paper). Proceedings
AAAI-97 Workshop Immobots: Theories Action, Planning Control.
Kambhampati, S., & Sanchez, R. (2000). Distance based goal ordering heuristics graphplan. Proceedings AIPS-00, pp. 315322.
Kautz, H., & Selman, B. (1996). Pushing envelope: planning, propositional logic,
stochastic search. Proceedings AAAI-96, pp. 11941201. AAAI Press.
Kautz, H., & Selman, B. (1999). Blackbox: unifying sat-based graph-based planning.
Proceedings IJCAI-99.
Koehler, J. (1999). RIFO within IPP. Tech. rep. 126, University Freiburg.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.
Long, D., & Fox, M. (1999). Efficient implementation plan graph STAN. Journal
Artificial Intelligence Research, 10, 87115.
Long, D., & Fox, M. (2002). 3rd international planning competition: results
analysis. appear JAIR.
McDermott, D. (1999). Using regression-match graphs control search planning. Artificial Intelligence, 109 (1-2), 111159.
McDermott, D. (2000). 1998 AI planning systems competition. AI Magazine, 21 (2),
3555.
Nguyen, X., & Kambhampati, S. (2000). Extracting effective admissible heuristics
planning graph. Proceedings AAAI/IAAI-00, pp. 798805.
Nguyen, X., Kambhampati, S., & Sanchez, R. (2002). Planning graph basis deriving heuristics plan synthesis state space CSP search. Artificial Intelligence,
135 (1-2), 73123.
656

fiOnline Parallelization Plans Heuristic State Search

Sanchez, R., Nguyen, X., & Kambhampati, S. (2000). AltAlt: combining advantages
graphplan heuristics state search. Proceedings KBCS-00. Bombay, India.

657

fi
ff
fi
! #"$ %
'&)( *,+.-//0213( 0*54'( 6
7

89:;< =>*2?
/-!@A: %&=CB2?
/0

DFEHGJILKNMPO3ERQLSUTRS>VXW2EHI

Y[Z$\]^ _a`Pb,cd^ea^'`gf

hjilknmpoqrsRhjtvuwxay{z}|lr~yr$

Lc\]Z$#jcv`

tvknm}sRhjtvuwxay{z}|lr~yr$

}
jU 'XR j<

2

2 9'v2nn9U 'UvC)2{v

>a3v
5.g.2<<! 9nga{)a5<gaHja{.XpCa

.a~ff 9a
~~2
Ua!UvR#9>vaU55a
!g52!{vnd,!#dj2a5dj5aa<apa{9R
.55{~Xj 5a{>ff93.~59 52!R !5{~a59va
<{#U}.)9v29!a2R255a{n
C{J9a5aa

5a{9a$<.
>a95{<2ag>5a{ap>2n599X
5n<{!aja>aa#$9{ffH25
g.a595)5<{ 95
55ffan5Lvaa9{5a.9J 5vaH9
5a{!C!59!!5a,5R !C5593{.a9p
<<
L25a<a>9
3Jn <,jv
Hff}$XJdpC$XlffU$ ,ff
fi$ v p) 2$lp$>
Jl

$ll Jdpp>2p ,ffv,Jd >J 2 2J
2$)ff"!ld$2#

l$ 2%
ff 2&pdp<'
2$})('2*+
<dl-,X$.!/10#v*23354+6798-.
" "$ $*
.p:

vXp+ 2}2!
2pR 2 2$Jld$ *+ 2$Rp C$!$ 2$jv


'
RJ)$ RJ2JllJ22; 2)p ,"d
$2 2 $~J 2 2 $
$2l
> >
R?
; 2#@Xj$; 2?!$2$; 2$*
< Rp=)
2p292 pRff$l2~

ff$$p
2 !@2 p)p
U
2 %AB$
J$
,
! $2 $,$)X

pX!&)
$ff2D$
C E(F
0 G/IHB$
p2*J23KL+Md
N l
X)
$*O233P)
6 $

(
$




$




2

,




>




$


*

2

3

L
+
L
.

0
,




U



ff
V


*

2

3
V
K
W
2
7
6
X


R
W


C


$







*
$
ff

$

$






p







2






J




2
p



$

,

ff


$
















<
Q+ROS
$2}
! 2 Z([\p
2}=
H
T$*]2333^M
_ $$JR
$*`aaa=6-$
2!b2 $@>
c 2 p)
$V$
9$3p
"!
ff2b
! l

! $
ff$ ef = 9; g)
$2 JE$
C v
}$
2$
*2 $ ; J2 pd$) "\
Jdp$
$&2 $v$2}
l
$
$; 9$2 l
<#

932X$
" *V)
$9@.l

)$2 92 $)
bC
hVp
W:
)J2
! l22?!
ff$$i2 $>

C$
l!jp
$Xj$
,
YX
.$
$<
JX"2 + Xk^$
+2 $2
< $R$

.2
pX
! ffl$Jdp]el
= 9; ffl$2p
2 g $
k^p
*2 pCJ2 pd$]
! $,l


2
Bm)p
nC$
*
$
;2 l
* 2 $
ff"
! l
~$2}
l
$
RpC $
$ $oX
>

aVp
4 $
aVrq^ < p>J2 pd$Ol
"$
$-
d$
gg2 p,lff"2 $2 g
fi.l
2

V2 pYp
>
"*5R
$
X ,
jX
.l
ps(Fg
2u/v
ff$X*`aa2WH
6 jp
$X:X
.l
ps(fiw +

! =H

$*J`aa`67
;2 x,
g
ff $i
J
$p

* p ,
v
* bX
.$
l!:2 $)ff"
! l
d$2l
l
2 %
jff2 &p
dp
\2
pyp
>
"
$22 $)R
8 z Dp
2ff9p$Wb
d$
z2 p ; +2

l
?$
!
$X$
p
,
gX
.l
$)E2 pLlff"2 $2 z
F.$
2
GR
$zC
zX
.$
$R{2 pJff"
! ?$

EX
p9$|p
@2 pXj
}2 p|>
c $ ;~ 9$p
b2 $j>
pp
b^{~
2?
$
u
ff$XE(`aa`67

-//0 v %% !=5
=}C2
Y3
!fi{;
9v:! %&% a&2%l%?2 =

fi oxorlzGir








$!p2&2-'pJlg
d2lff <$ff"!$$vp#o('C$ 6 e52?!!JOtg$!+ #$p
v$22X$
p
Z*Y}
"p
dpb ? '( =6bH 2?!Dpdp$+
$2
&2 $
! + ;
lff I(fi67*g'( W6 < ,
ff"
! $
:p
v$J$
+ 2p& 2$!+ ;
lff I'( 6HBp
2 pp\
lff
2.
!
dpff"
! l
Mx2 p
2
p-p
dp$
2D+h2 $R
lff -

! } oF$

ef 2$ff" 2}>C$2+ <H 2pvL|X.l$l! 2)$L$<" 2L2ff;#
$ 2X yX
X$2l! 2?| 2p$2- 2$ U2}?$; ,
g
pdpX "& yX.$| 2p
ff"!?$

@ 2!= LLX22$l

)

! +2 p
2 phX
.$
$$ff2s)
$2
< p^p
"p2.$$?! ?$; 9e 2
2$$2d.p2Jl2W:$$C~ff"!lx.+ 23$
="!Jue JX" 2= 2$
~ &2 $ @
$
$
! $
ff$$2Wd
2 E 9.$
k^p
ff2
p @. "$$p$2-w L$2X@ $>
Wv
Hffn2 pX
.$
$2l
! 2
$
! 2 $l
C ;
w py 2pU2"l$ <'p 22 "
"
* p

$C

''
^l
"bp
XL
ff$$p
2 $
k^p

X
~$2l! 2
ld
!b2 $$
X
!b.$
ff2
X
x2 p?2
efsNv 2E`D,

l?\2 $b
! p
j2 pv
YX
.$
$2}
! 2 *OR
$ p2$X

J2
! $2 $$
uv
N 2 \^ielnd
N 2 D4b>
2pW>2 pjX
$dp
U
%2 p$
! 2 $

WL
$l
D*)
$2<|v
N 2 &qj2 pU2"$
2 ff2pW-b
2U2"2 }p
>
"d
'
?$
$*
nv
N 2 nP^*.>
$
$
$$
$$
2>X22$l
Rl2 lx
:
fi "$
22ff9
\g j{ X

L3

$
>
pG[ff"WZp >" $Cps+{ 2pX = 2.(TT69 pX$p+ 2

pdp- < pl2ll$ & |C$h 2pgp ,ffJ" 2; 2 C$2X" 2$
(Ti6i>


(T 6

0lff\)pPp+ 2; 2pffff!$R$2ll$

(f2W6
W


)pv$u('pp ^T67*>2$Z!p 2$L2k^$2 2pff$ 2

2-+lX
C
B 2p

d$= 2$+; 2

tg )lH$Cp 2p 2 H
>lff g$vp*J U * hX@ 2$y$vp- 2$ lff9
2
2p2;C

%2 pjp
>
"Ym)pD 2*.
$" 2$*V 2pj.$
] 2$)$vpCX + 2
2$
+2

;
p
ff"
! $
%p
dp* R
$d
l
$!&2 $ ffff
! $
%p
v$R$2 2$

\2 $
.$
E(fi)
$9}2 p@
[ ff"
WD}
U

967
Ux



W?+ ;

ff



(`6

fix |lr.u>k i.$impo r

Nvi
#?$; 2$i$!p:2 e gJp Op2ff"*pW>* 2$ ?pCpg 2$!+ glff9 2
>)2+l
X
gd

2 p'p
dp* F$
* l
2p'2 p>lff ff2V*. Op
#
?$
$
p
< pjX
^u}(F
$!$
2|2 67}H "
=
$
l$2
:2 p2;

'2 p2 p"
x2 $ -2 p2~
'2 p
2 g"2 l*i?'uu F$ l
* U2Xp

! $d
R
J2 2 l

NV$
$X-2 l Y>
jVp
WF
Llffff2 $
ffC ;2 J
!
]2 plff9 p
v$*' ? *2 p
J 2 !
2pl
$$2 $
X$; $.2

9
! $\ U C;
( 5 ?6'


?6
( F$
F$ F$

(6

< $gd$22O 2}x J U2l}?,"< 2px[ff"Wyl. ]
2px.$

X<$G F$ < +lx 2pL "$]^$@Xff2L$pX$$= C
' 2p 2; R
' 2p@p ,ff$
uX
g
.$
$2 p:^
v
}$2 $
p!&2 $"$
2
:^
=
2 $RJ2 l$;2
J2
pdop
N ff9$3
* $
X>
&>
$
GVp
W2 pp

$; $.2
@2 $lff p
dp*J>


$
g2 pff"
! $
X$l
l


9
( 6i



( F$ ?6 ( ?6
? U F$ F$

(fi4+6

8B
" ".$ $*H!p)> pp h^p 2p $; 9$ 2 ( U6$ 2p
2G>
.p
J ( 67|w *#$W>*<; 2p" 2 l2X" 2
} 2pffff!$
$"
$2 *$
J$p
$X,$
,
xX
.l
$i
C;2
' i$$; 2$*php$X
X.$


2|X
j
$
h+
J.2 !
( 6i

5V

+ ? 1



?6^x( U6
( 7 F$
? U F$ F$

(q6

.$$@ 2p$" + 2@ U ? x( ?6al
}( ?6y2 < p$" $ 2

? ?
x( U
6 2X$
$' u?=X
2UlffJ 9-(F.$$i 2p!$$; + 276lb
2py>
;
n'( p2B2 pJ5p
$
)
6 ff"
! $
]ffp

9 mB^d$
2$X
* $
9x( ?6

(



?



}
6

2
p




$
$

X



x








$






2

2



2






l
$

}



|


2
p











B

ff




p


.
*


.







!




p








(



?



6
C
..^p
:


$
D>
$Wg 5p
~ WR3X22$l
R$" $2 $
mjp

2!l
$~ 9k^$
2 \J
q
2J}$
ff)$2
! 9!G(
6 $2}
R
$2 Ujffffl
:w $2 2ffk+l
2
uCl
>
W>
:X
.$
$ J ( 6
8 . p
R
>
$$

$
l
p+
2 X
&x( ?6>
$
! +
$+ E2 p
k^$
2
\c} "p
$Xb2 $ >
2. J Zp
$X$
W>

$
$s2 p
ff"
! ?$
~l

2 p%$
vp]2 $bl%R
W >
>
}$
,
.2 $O^$
W)
.
! ow U="2 $
!
p
2 l
,}

v2 l

ffff
! $
^p
v$O>
}
$
pff9$
2
U. ]2 p2Y2 l
ff2U".l
2

? XmB+d$
$*+R
pp
>
jC$
2.
J. h
$
$>n%
)
p2%
% ? ( *
uX
>
"p
2j2 l

x( U6'}( T6Y ( 6
(P6
? ?

$2ff$
2$W>x$ < pHk+l 2$Uff2jp 2$?!JB 2lV L+ + 2
x( ?6B)${>W

$lpJ{ 2ph #$l
EXk+l 2sq^ < pX2

2$ )ff2"l 2
'UR9p C $ !&`-
)V $" + 2

fi)T 'TfFffT ffTf9iJr 9T 9FW
Tx
;ffTY7j;"Tr'ffiff 5
)l
JpT





ff=

fi oxorlzGir




$!p2`^ < pff2 R$x( ( - 6$; 2p :Ix(aa=6Jl$ 2$!2$
2p $; 9$ 2Dp9$~ =6 < p ^v h 2p WCl } < p
$ff"
Llp
2p pW 2 $^9 sX2; s^ $l!ff9Ly#
DX
.$
l>)$
; +2 )?h2 pp
ffUl2
! ?L
! $2l
u%6 < $>2ff$
$2

X$d
pp < pnl
" ?p
2p 2 plp
&)
p2u2 $u
fi$
2

6 U$
;2 + :
2 $Clffff2 $
ffU$l

( / G29 ( - 6}( ( - C
efz2 $B>
Wg
*
2
G+\
$
$HJ ff*3$2l! b \*
2p2^yC$$?!- 2!= X$$%
9 2p)ffff!$ < $$2d2bXU2X &
jl 2
<

ff"
! l
p
v$},
l2 u.+2 3+"
! $
LC2$

"!$#

&%\('*)l\nZ,+-+ ^'`.)

bC^'`gcvZ$\

efD U
; 2$lff$ffU$2!!*9k^$ 2nqDXv$2

0/i13 2 6
4 55 4
5

5

)pj 2pjnff9lUff2pCp

4

1

a87:9D4

=; <

(TL6

1x( U 6

(K6



(36


U6' ( V ?6
( F 5
F$ F $
( 6
9?>?@ ( ( B A=D A=? 6 ?6
>
; ?

C @
C ( 6

E


BF U
DG

(f2a=6

)p
@ ( DA= ?6\ 2IH> 2$" 2
2$G$vpoX 21pdp2 2$ ?PFff2
k^$
< ^$} 2pl&$>
] 2$ J9 2X$h E UW* 2pW)U
K9[$ ; 2p
$
; 9= 2g(
%)$o,l@`-
x
\
U67 < pp?!
% 2p2 )C
L9
ff


fix |lr.u>k i.$impo r

R
! $
}2 p2Jg
X 2 < $l; + 2$ :x( ?6>2p$hX
; U22ff=U
p$~uX$9 N9[$ ; +2k^$9!
$


U

x( U6x2

C

$


U

x( ?6}

C
2

(f22W6

2 4 22X$$ ( F67 < p@p! 2$
( 6C\
.lD+
< p5d@$
)1O
$2?! C 13
2 4 x 2$QP 2$g
F.$ 2

"!RTSVU ^ + K Wc8XZY~Z,+ K Wc
e
!p2!&pp2* / *% 2$ lff9
2 2$2;
x 2p|p ,"+G > 2p
p
p
$
* ( l
- w \>+ J. nX.$$ 2pff"!$ ( / {2W67 Nd$D>
pEp
|^p
2 pl; $ 2 Wo 2plff opdp*Y,lu $2pC2$l
$"
$2 $
}#j9k^$
2 @q^ < $2
',
>= v.l
%2 $i
lffJ 9Ox( ( - 6 < $'!
]p
Ws HJ 92 p>$
@$
$
5p
@l
F
2 pX
fi.l
2 @
(
G29 ( - 6Vx( ( - 6
(- /
ef
!p
2Eff
` >
2pW ?12 p2 $Jl
2$
2 p l(fi2 p ff"
! ^
9
6 )
$912 p
$"
$2 x( ( - C
6 x(aa=U
6 J}$2 $y
! $n+
p
?+$
}2 $g2 p22 p-n?p

w l
* $W>
*'2 pff9J '(
! |$
vp 6)
$
$ ( (
6 $
( - 6
D2 pJ$l
< pJ2; 9 )2 pJl{
$!p
bff
` h
Fp
"2 $3
* 2$

$
;2 $
n'( 2
9k^$
2 \P6

w

x( ( G2W69




x( ( {2[A - 6ix(;2a=6.\x(;22W6}

l
plpX$p+ l; + 2*))$9Fff2

pW:

( ( G2W6

(f2`6

$!p2`fflp? 2p

^

mB^d$2$*^$$?! 2lR?
9 2\ 2pJl\)$9G}( ( - 6RWH2; 9
2l,
2p :
|
!p
2-`5Jefb2 pH2JC.!$
2>
R$WR$p|l
ffLp*)l2X$
2$)l$
')
p2-2 pQ
P2 $B
fi$
2
,$
;2 +
H ;2 $
$ff9$
ff$2
! !j v
)pJ$ $
2 "$
ffb2 pH5d
@$
$
&2 pR$
$

2 $i
fi.l
2 b2 ^$
iX
.$
l!)2 p
ff"
! ?$
( / {2W6
`cb~g
]_^oi

< $
$2}Dw



edE

n

D`

X

0f

ff"}$ ff"Gp >"E>|Cl; p p& 2p


XU@>|R'$|
2p

9p2

fE(hgG69ji+U)B"kRlT-mgn7



kl "kl5

omgqp

(f26

)po"kRl#g 2$$; y?xz{ F$ $ p$ !bpdp < $9p$ 2
2p;2 l
B"
kl 6gs*i)
l J2L2Lp $ 2pJ. 2 2Z 2Jze
x>
$*

Jl
* g` 6
2 p)5e !y
! h
$!p
2b
2 2ff$
!l
l
ff"|p
dpd
* $
$2 p
B
C.!$
2 L

!p
2)2 ->
$
@X
>?$
$
p0
fj'C
$2?
! g` 2 ]2 pUC.!p
92 $


$!p
&2V
$
&
F
2 $:p

pW:
= &fj*. xp
:
$!$
2&2 V}
* 2?$
)2 $)
lff9
2
X
$p?
ff"
! $
@( p!
$!$
2&2 67ief
2 p-,
l
fEh( g{C
6 }2 p
<


?}
Y)
$9{UuVu F U
9p
C
ff"
! Y2 $r
g*^,
$
,
H$2?
! :p
dp) 9>
$

222
$ X
.$
$ffff
ff(s

fi oxorlzGir

Jf)'w
(TT6$ (F6$?$$ 2~Y 2$ pR$|~2X 2$$
X
.$
Wz$ " 2-," 2+)$\ {X.$$
"} 2
9(p!$!p
2|2
$
67Yef
+ ; - tP
+)
l2l
l
$ 2 l
*.2 p@X
.$
$
ff"!l "l22
.$
@X
J 2J}$G+ ff$
.
! Wb2 pff"
! ?$
j

*l2lB>$W$
|X
.$
$Ul
Dp
2 p2Jn$

w

j)lJ -X.l$9
UJ
})$9| 2phXB
$L}ff

2ffRJJffb


AR
E
2$
$
g> $W


uv!$#jw0x c8y]acv\Zp]n^Rzc{%HZ$\]
pjVUZ|}fZ>CX"
9

$CJU 2'; %,U 'py 2pfi9P$ ;
' 2p #l2lD*
2
2l& 2$pX$$$$?L$ 2p J X.$$j
R$Z%
) ?
p
x # $l
+d$
2
g U < p)+$
@X
j
}'( p
k^$$ 6
< p)^$X>
Jffffl>
$; 9= 2)Uk^$
~

` ~

(f24+6
~
~


?







F


m)$ 2p #$2l

Bpu 2$
ff*,@ "h Jl2WX.$$} 2$ Hff2$Cp

X
jR$U 2lff

IH

< pJ}ff zC.!$2b2

$

*
$; 2l*g

p+ 2* B 2p|$ X)pGX.ln>J$ LW 2H" 2 lJ$ ) 2p

1 Xk+l 2 s3n $ 2 ff$$$ bffp < $E>J | 2pop$$X$ W>
X.$$}
: 2l | ^$d!b 2p #$l )C5d$~$$$~'e
2p@p

.$
hffp
Jl2Wx 2$gX.$*^,; 2$ *p 2$")j>llp$
2C2X u.+2 X=ff
! $
C2pJiel
$)2$ 2l9>
< $$ 2 $$v.$
pC$
|X
.l
:X
.
! =ff
! v
B2 p22 $ J$WJ+R

fi p
$ 92
Ux2
$
p
9= iel
%2 $Upl}
)
$
$*V2 p$
! 9$2 $U; }
uv!R '+ K g]aZp]^'}`#Z,W '*+ L WcYg^]
#$2l<$-| 2p2!l$ " "$$J& 2p)$ 9 "p2 < pJ 2 2$}Jlp$

"$$2lUU2p :h &XX$^$(Fd$l$*O23L367YcY. )J2ff 2= 2$*V

J;C
$92 $2l
92 pj^$
@
Cg

2 $
xp
$>) Lp
ffCo2 pB^$
@X
>

ffffl
))2 p^$
@X
)j

$
" +2 * R
$$)2y( < v$J*`aa`67jef
9$ 2

l$2
?
! 2 Jp
l
X"
9J*R
$)2JX2 u2 $H
2
! $

2pJ=2
Y2 pJ
9 $
< $2
2 $
X J.2 2 $
<Jld
$ n

$d
! p
j # $l
Oc
?= *V)
p2 92 p)^$
X
>g

$
; 9=2 < p "$

l
ff2 2 z2 J pX$
$Rpd$
\2 p lff"2 l
ffRl2l
D* H
u
! p
9 #
$2}
jp
. l

X2 $$
2$
$
}ffffl
$
$
; ?=2 ff2J2$
}
< $H

2pl2+ J2 pdD 2 l
g
Rlff :)
$2 ZU)
<

J2ffU2$~
"n L
p
|\V>
Xp
R$X >
) # $2}
X
p
X
]2 pXp
l#$
p
9
%2 p
,
YX
.$
%)
B2 $,
?b2 $ 2 *+,
Hp@p
Xp
| $.
! :2 pH _
9 $
&
pCp
:2 pQ
P2 $
F.$
2 p
* +d$
$
$:nff"
;
2Xk+$ 2 E2aV < pg o1 *2 $R
)$2 < p2
2*>

X
2 J2 2 $
Jlp
$


- "'?=

|p
3$2
! p
$
;
W k+l
"d
$Z2 p$
! $2 l =ff
! J2$
$
$
;2
iel2 pjp
V>
22 $
* p ,
*>
$p292 $9 lRo2 pBl
2,#

@
$l

$
p-2 $>2 *^,
H2ffff&2 $ Y)
p2 pB # $2l
Yp
2 l

* p

< l
$C

)W
R+
$
" +2 )p
$$
* lWv
.!b2 p$
%
! ;2
! 2X
.$
$

ffW

fix |lr.u>k i.$impo r

1

Mean

0.5

Exact
|S
|=1
mar
|S
|=2
mar
| Smar | = 3

0

0.5

1
/w

1


w

50

100

150

200

0

1

Ring position

$!p2^-Hc>$ ;~.9$p9!
J`aapdp<?$$ 2~) 2b 2p22$$<$b>$!+ 2
R2$W:
u2 p W>
Hlff"

'2 p@C.!p
2id
H
J$
Hff2} ; ->
g-2 p
2"$
$2
x2 poX
.$
l2l
! 2 $
! 2 $D < p^$
@

:p
dp $
$
$
#
$
$ h2 p
! $
J




^ * ~`


< b! RJ?= "$ 2. x 2pX"
9$
] 2$$!$ 2$ ,2 L!u(pff"!
v#
6
J`aagl
?$
ff"yp
v$%R
dpjff2R.p
jy2 p-,
c ;~.$$)$ 2Jj>$!+
! !o2 pL
!
$ p v$$o2 p22${)w $! +2 $ z2 p2p$Rff2ynff"d
2p : 2
poX
;

$!$
2h^zp
b2 $2J}$2}
>
J 2 $p

2.
! |p
dpff"
! $
< p2ff2l ; L p
?
$!p
2h^Gw 92 poX
.$

$2}
! 2
$
! 2 $2 $2x2 J*R
p2x>
xnffy2 p5p
@l
I+$
@X
<
$
vp<?$
$
p
;2*x;
` l
p;v^bef <,
p| >
lff9 p
dp
2pj ,
op
$
! +
9,
< ^$
}2 pj2 $22X$
h f ` 0 ,
* f ` 7 $
{f ` 6
H >>ffY
2
H
$!p
2)@2 p2Jl;j
$2&C $
p
+
$
$i
}2 p(P$

x2
pop
v$up
Jff"
! op

! 2 $o,
$
! +2 <
* p ,
*92 $9pp
L
"
= uH
H )
!
ff"
! }ff ff$
J$2 -2 $2ff$
$ Bel
)2 pLJ2 2 z2 C
|

$
l -

!$# ^ Y~c%Z'}^'`]aeZl`. '`z}cv\()}cv`c
el RUX2$l) p$g 2$CdX+
' 2pgX.$$2l! 2$!$ 2l $$^ 2

,
c ;
~ .

! $
,
B2
2
! :n$
:
C>
$
! +2 )( ,
6 $
$
-
> 2p2p$B(h+67
#2
! @p
v$ff"
! l
ff2 k+l
g
"$
$
>
" B
[ 2
p o2 $R;d J "
$X)l
p
,
BX
.$
$U\2 p 2
! g$
vpR2p$
DX
p+2 < $)J}}2 $ -
B2 p

y%5 -Yffiff TlJpTj:=LLT r7ZffWTff7;
ff


fi oxorlzGir

1

1
1
0.1
0.01

0.8

1.6

0.5

0.5

0.99

1.4
1.2

threshold

threshold

0.9

1.8

1

0

0.8

0.99

0.7
0.6
0.999

0
0.999

0.9

0.4
0.9

0.6

0.3

0.5

0.5

0.2

0.4

0.1

0.2
1
2

0.5

1

0
weight

1

1
2

0

2

1

0
weight

1

2





$!p2j4 < pJ2"l$ )
nc, ;~. 9$pJ9!
U ff"l$ 9ff"\^$@
xpdpbHH
2p22p$RlG,$!+ 2$W& 2pJ2Jbffpgefz 2p
fi R}p 2py!ff 2p :
,

Z2 php
$X $
W>
X
.l

x2 pJ
2 php
dp < p$!9$ 2$
="
! >d
Xp
=2 $ 2 pC $
X
.$
$?!-ffp
,jv
,$j
j)$

!
)2 p$;2 l
$2 ;d
J. 2 yffp
H$ff2 $
% 8.*J)
$2 )2 p
< +l
+$
@X
U<

$ 2 $
< plff 2p :
D2 $$
! +U
lp


CdX+ 2$g
W)?! $$}
)=J"
( 6'

5.

1

+

( ( ( 6Vx( ( ( 6

(f2q6

.$$x 2p$; ?= 2
x( ( 6x
x( ( x
6
x( ( 6x

( ( 6i
6
( (
( ( 6i

x( ( 6x

J ( ( 6i

( 6
( T6

(f2P6

( 6
( T6

(f2K6

(f2WL6
(f236

$ff9$b
: 2pW>:X.l < p$$ 2$$2l}$ D9k^$ 22qJx!$u+

( ( ( 6id@(h

\ )

(

\ )

( 6

(`a=6

p2 2pJk^$ 2$p pb 2pbCd?= )
( 6H$ ( 67be j "p7$H *
p ,
*X 2$ J$ L 2b p ?p 2$Cd X=
2pDplL$ ,bX.$
2pJp
* p$
( 2W6 C ( C 2W6j$ ( 2W6 C ( C 2W6ic} 2$,$}p
HX)
D2 p$$
l)"$
$2 Ux2
! = "p
)$
+"!$ X
efn
$!p
j
4 &2 p $IHX2$
gX
>
\2 pp
$X$
W>
:X.lu 2pJ$))2p :

&nff9$
9p
)2 pD>
$
! +
$
2 $22pJH
H &>
2WX

2D2 $
! + &X
.$
$LX

.2
$

2}>
$
! +2 $
2J)
$
ff"
! *YL
X2$2 $D>
$
! +2 *XR
p2|$

! 2 $
>$
! =2 j2"$

?L
2 pjXv*.
"2 p
V#T 9$v+X
.l
$Oelj
2$$
X
J=2 p

* p ,
W*
2$ '2 p2>"$
$2 222l
@ j2 $pU
2 pU2;<
$
; x("f U
2 l
Dr?`6
ff(

fix |lr.u>k i.$impo r


x 2p)$l2l! 2$! 2$D < pBX.$$>2&XH$2Wo^$ff"!! 2p
$
" x>
2W
$!p
^
- b2 $BC.v
C
X+ 92 p)X
.$
$2l! 2!$ 2$ +"!dXp= 2$ < p
$"2
$
) &2 p;d
J. 2 )ffp
DX
)
;
*)
p2t 2pg^$@XU
$ 9 2$
$
aPRD^$
@X
-X
>
{~ 2$
$
L?$
$2 .!&2 p+"
! $
X < p j
p
*^2 p2 ,
:2 p$
! $2 $ +"
! XelD
$!$
2)4{
C2$W:h
x2 p2Jj>
$
! +2
$
G2 p2p$ge
ffj2 $
ff"
! ,
$
! +2 l
.$
L2W>
+"
! $
< l
))
$
>h
! $

$l
l
2 92 l)
ff2 p ?$
;2 H2 p :D2 p+"
! $


G v `


`

~

~<

< 22i 2p:k^$$
2pxX.$ $2}! 2$! 2$I
jJ29$!?!jp >"d*=>
J.
\X
.l
$
C
c W
2Ep ,ffZ(TNv 2Eq^2W67* 2l HJ$$nl?ff" 2
C$2+2
2 $g

2>
$2}
Div
N $
$$*^>
H2$W12 $ Y2 p-X
.l
$2}
! 2
J2
pd
z$
C$
P=2 $
! = hX
.$
l&
ff"
! z
e 2!s
! (Tv
N 2 q^r` $

C$
$3
* Ev
N 2 Eq^r^*O>
2p 2"$
2 )
hl
$# lff"2 $
! fflp
g2 p2&p
>
"d
*O>
b)

$.
l2$
2X
.$
$l
! 2 22 )
2 E2 p$
; @nff2 2 pv(F
ff$X
/w
! $
ff *`aa`6x)
$9>p

%2 pjX
; )ff$$2Wd
2
J2 pd$}

! p

! ff}$
w pp
)X2$l
*^,
2pW>2 pp
R
ff"
! ?$


"!$#jw0x
c

WZl\
+c]'l\nf

hHHff
p >" 0 $$$2Z$ >")l 2$2+ 2$ 2$
R2C

< p
cC2p >"Ee CL$!$$ p2$X+c>l Jx(f23K36LLz$ >"

J$

! l2 =2 + $
2$ff2he $2; 2
:LDpdp
x ,* 2p2
p" 2
$
H2#p

! \ pp

J2 2 $
Belz
$!p
2bq&2 $@
! ff}$Jv$gH2p :
m)p
:2 ;h>
: p p
j2 p
! gp
dpffff
! $
?
l
$


+
vp$
X$
R9
pdpz2 pp

ff"
! ?$
)l2l
l
$2 b
$2 "2 ff2 2p :P
$$~ +2 Hp
< p
$
zX
;
92 pL22 .
! ff2.$
z2 p2Lp
H$
$ 2 pyp
$X$
W>
jX
.$
$
2X2
$|p
2 $jp
,
"{>
|$
2fE(`
q A"aaa=6 7 &e
Y>

|2 l$*]>
2
p
2 $|2 p?$

[ ff"
l
,


2
! Bp
dpHff"
! $
< $H$
! 9$2 $ ="
!

X

2$
?+
w 2b2 $ )2 pbX
.$
$Hz2 p ,

fi
ffp
ff2bff
2 D2 $Jd
gffp
@w

! $+ "$
$2 GR
={2 $$ff$X$
umBl
2"&2 $ yp
dp
L $2 : lG2 p
2Rg

lff9
xp
dp-2- bP^ < +l
,222 $
X
,
o2 p2Bp
dp>$
2 pR;,


2pbp
,
"
ff22 p2
! Gp
dp&L+*R
$J$
)2 l B2 $B$
"2 +
$
$
pX$
$Rz2 p
.$
ff2 =

ip
dpL+ < p ; Hlff$X$
x |X
gff
2X'( $
2p-2 $R
H&
! n'
76
$
D2 $2
g2 pff"
! ?$
X
.$
$Y
:p
dpg2j
P ff2j"h2 $
! +
.$
$~?g2 pYp
$X3$
! +#
ff$

* @2 p,2 pl$
ff
* ff2Xk^$
$ Xv < $~glff"2 $
< $iX
.p
- y2 pH$$
2$

J2 p.p
2 l
X2 $2Hl
2 $B^$
@
>g

;2 C9
O2 p)p
dp>$
ef @l
l
2 Z2 $ $ ff"
! ;2 lJ2E
J2>$
; 9 < p2
2*'>

.p
U
J )
$
$
2 C

@l
$2 l-p
v$*+R
$
! p
$
$Y y>

X
.$
$

W*5K 5ffX;7t W;J5 - TT ffr7t]lfi7c]ffpT7Bff
,h,$Rh$(

xF"T
JpT_F 9 7
7 W ffrpr lT;ff
T57 7 x

ff W5 [ 7 JpT
7f5FTT%"T'5 7. B
F"Tr(fiff T7JT x7TTxf75 xr(Y
rffc7 l]f5FTlTf
ff[

fi oxorlzGir

21. pulmembolus

12. minvolset

34. pap

19. fio2

13. ventmach

14. venttube

11. disconnect

22. shunt

9. intubation

35. press

10. kinkedtube

16. ventalv

15. ventlung

33. minvol

17. artco2

32. expco2

8. errlowoutput

27. hr

29. errcauter

28. hrbp

30. hrekg

31. hrsat

20. pvsat

24. anaphylaxis

23. sao2

18. insuffanesth

1. lvfailure

3. hypovolemia

25. tpr

26. catechol

2. history

4. lvedvolume

7. strokevolume

5. cvp

6. pcwp

36. co

37. bp

$!p2q^ < pp Hff"!$@(fi 2$"$$~+ 2g$76>$u 2$gX.$$( 2!76x
B 2p
ffp
,
"ov
N b
$
$Hff2D2 $
! + j2 $ g2 $J22
! jp
jv2$l
@w
{fE(`
$
q A"aaa=6

ff(

fix |lr.u>k i.$impo r

0.6
5
0.5
10
0.4

15
20

0.3

25
0.2
30
0.1

35
40
5

10

15

20

25

30

35

40

$!p2P^ < pBl$o) 2z(Fp$XC^$,W>x$ 6
gupdp@4a5V4aye
2!!
2pW-^j2
p9}
"Vp
2#
2 p,ffk+$
ff < pjp ff"!$$$ ]XjJ

<"$
9ffff
! Yp
,
ff < p>p
2p s2 $XX
.$
lff"))
p2Y2 pYl
$
@) 2
-aV29w $
>
2{fE(`qaa=6

Nv$l$* 2p,d$2ll$ B 2lff2>
l-
2$ gl$$$
% 2pY$$2}! 2
$
! 2 $D < $
$
* J


$2l
}
$2 >ff2g &~2$p
2C%)
$ >
x
! p

< pCffC2JUlff"2 <2 pxp ,ff@ 2$ ff$ff
< "p
J. D(F$
gp
22ff$
JX2$l
6YR
$z$
"l
$
$WJ2Jp$
J; 7$
"p
2L$

ff"
! |'( vUR
6 ;2 lym)2 plff"2 GX
yX
.$
p\"z,
T)
b2 l j2 $)p
>
"
$
2$&X
X
$p
"
ff"
! jp
,
"o2 $
C+ 2 l
B
+
p

J2 pd
?n2 $
*O2 p@X
.l
$l
! 2
$
! $2 l l
nX
y$
z hC$
2ff)2ff$
$2
N l

n2 p J2 pd l

dRJ 2 2 $
*-2 pd
"2 $


ff"
! "p
2.$
$.!
,
p
"
$)J; :$

p
l


w

"!RTS

() 8ye^`)q\n^

b>Z$\ }c

L2 n ># $J$$Je
2.!Jdp*R$)2 2!$ff-!u)$ 2V#

2 l
p
X
>
3l
$vp 2$ Rff2 $2 $$!=Xp9iw y$|!
4a&^h4a
llff"gpdp < pCX = 2]>2p9:
2 ).$$
[$; 2jX ,b~$ p
ef
+ ; })
$ 2cC2hX
p ,ffvg(FAR*233`69 2pRl2ll$ L$; 9$ 2L x 2p
.$
JX&
e 2!g
! 9Xp
C
2 $&p
$~ *^ X2 $jlY$
$
2k+p
l
X
}2 p
$
! 2 $D
ff(

fi oxorlzGir

1
0

1
0

$!p2@L+-Hl$#lff" 2$ Dp ,"
) >+ pdpJ 9 Wj9lpdpE(fiX ;

)
6 $:2 p2}ff2= 2Ru 2$g $9}ff2= Bpdp$)d 2$n 2p2
$p2Jj
0 2l
l
$ -2 l
>
2l
p$H
$
$2 $~ :
2F:.$
$
$; 9$2 J
dp,b2 p-
; W
>ff2HJXb L"2 ?
;2 < pX
.$

< p)2lp|p
$2l
! 2
$
! 2 $ n)
$2 8f(`qaa=)
6 l
+"
! n
L $
$xp
9
v$2 $b2 $"
p
p$~ +2 <$
HpWI2 pd

ff"
! $
*2 $b $
GX
;

%2
$22
! ffg2 pp
lU$
W>
X
.$
$}
.$
J

2Y
e 2!z
p
! $LJ 2 !z2 pp

ff"
! ly 2l*X JJv b 2p
!)ff"!- 2lX B`q|^n`q& 2pd $!9$ 2$
').p 2pvXp+ 2g!

2
pJ2 2 $
,lp
$ < p
$
$2}
! 2 !$ 2$D*s 2p 2pJ$$*
$
$ pX$
lL2 pd; ff$
"$
2Z'( X2 p2~
)2 pz
[ ff"
l
2

p
dp 6
$
2 +$
2 $
ff$nR
$2 {2 po+$
@X

p
dphw 2 4a5V4a\5e 2?!u
! E)
$2

ff"p
dp<2ff$
X
)$
Lb2 $x
$
l2l
! 2

! $2 $Dp
i2 l'p
>
"
2pp

$
! $2 $ ,
$
2k+l
2"
! ffl
<

R
" -` 7( 2O+$
@X
*)
$2
X.$
$l
! 2 +"
! X
@L^
2 ?+ G)
$9{2 J
J
$
$H
2pff"
! $
> H'2Paaop
dp
efE
$!p
2|Po>
2$W2 p|4a5^4an
! G)
pb2 pbl
ff^p

}2 p"k^$
ff22X$
G
2pl
$
n)
2
92 p2?
! p
dp ff"
! $
< $:l
$
n)
2 )pC p
B2 p $IHX2$

X >
2 $|p
$X$
W>
@X
.l
p
| \2 p&'
2 $ ff"
! l
<$2l
}
$2 "l

p
*2 p >
Dl
$
\)
.2 $
)2 pyl
$
ffffup
p
2$
Hff2Jp+2 Bw J ff$
JJ
2
! l
y2 p ;2 @(fi2 pl
"
,ffV 76
i)
$yX
.$
$2}
! 2
$J2J)$$
$
$2
[ ;U


O2 pffff
! $

* p ,
}
* ff2)
$
pok+$
->
<
J2)2 $nL
q l@}
$
o)
2
22
$uaV2

"!uw0x

%

\nZ$ x

cJ^ HZ$\]^]ac

Hl$#lff" 2$ o!ffl L\$ >" $; 2!
: , WL
-pdp*)$2 2$o W
$lp $
Z2 phX
; W&v2$l < p$$ .p 2$J? 2pDp ,ff ff2D
2 2p

n( }ff2= yp
dp76j z 2$X ; n('9$spdp767ENV$ ff$ "p2ff$Xff
"
2J}*
2.\)
2 ' $
Ypdp$
U = 9 2lb J. & 2p
ff"
! ?$
Jp
2 $)
p
v$$
Jl2+ H }
# lff"2 $
! 9ffl X
u.$
$
$
h( X+2
! p :=C
ff$67H 2+<

J}C
2 px$
C 9;
]2 $ h( $2d.$

l


X"2 76

2>H
+ (f23336])
$2 $22 Xp
v$ < $x )$
ffg
[ $)
2l
^c( J6Jp
>
"j
2
N +,

L
i(f233V2W
6
! vvp
Jly
l2 Gl
# }ff"2 $
! 9ffl
)=2 $\2 p2y>
2


fix |lr.u>k i.$impo r

900

150

800
700
600

0.1 <
0.01 < < 0.1
0 < < 0.01

100

500
400
300

50

200
100
0
0

0.2

0.4



$!p2K^-H



0.6

l#}ff" 2$ Gp >"
p$DC$22=

0.8

0
0

1

0.2

0.4





Z 2p$2$1pdp

"s 2l

lpdp



0.6

0.8

1

DC2 F$d#

< p
fi J$; !

2pWRb 2p

l$ER 2$
.lE
@ 2p2.!|pdpffff!$
)9$JXEpdp < p
$
$2l
! 2
$
! $2 l =ff
!

. -L
q ^ U$
\$
}fE(`qaa=6
! +
$"
! 2p )
2 $22 $X22

]2 $ff$$2Wd
2 D)
2 $o2 p
< p$
$
$J
!
! -H
H ff$$2Wd
2 $
:,
2yX
>
z2 pyp
$XR$
,
)X
.$
*
$2
p
! D2 lUp
$
22ff|2 p2

$
$*
2333^M][\p2l+ $*'233367 < p2
2,J$WJpz D$ 2pl#lffff 2$ @p ,ff
ff9$$
"p
2 & ; 2 p
$$2l! 2$!$ 2l
w 2 }
# lff"2 $ hp
,
"d
LR

k^$
}+l
X
(fillff"{n?p 6pv$ZX 2
W
gj
9 $\$
vp $Hd
2 \2 plff=2 $
lff2+R
X+2 B p
2 $\2 p2
lp2J < $R.p
2 $
Y,
2$Rl
p$ < pH$
$$2 $
ll2l
l
$ 2 l
jc( S.Q 76
pl!|2
$ l2l
l
$

92 $L9$\
! $ $2 Hlff=2 )C
$
$2 $~ uR
$2 \$
$
9$
p
^$
X
xX
>
D~ 2$
p
< $HlC$2l
}
$ &2 l
}
2 plff=2 x>
2l
$2 $~
2ff$
ef
$!p
2GLz,
2p {l
$# lff"2 $
,
ff)
$2 ,
+ $
vp W
*})
$9
;2
Hp

! J 2 p p

ff"
! ?$
< p ff2W)
2pW 2 p lff2+ f# l
22
$
< "2 $ $ dpl
oC
J$= (fi2 p2$p$
vp67Zw 92 p
X.$
$2l
! 2 $
! 2 $ R
$2 -fE(`qaa=6 l
29p =ff
! $

L l
$\$

"gp
dpi>
,pWs2 pjff"
! ?$
W<$2 >
R;2 < pi2 $ff
p
J2 pjp

ff"
! $
* 2 p
$
|X
;
J2 pR22
! ff2.$
$j
l
$ :2 $-p
$Xjl
,
YX
.$
JX2 $$
el
<ff'2 $ '
J;<

2 pxp
v$2 pUJ yX
.$
lffC2j2 $
! = '2 l %2 $@
! $J;
2pp
ffp
&$
"{p
dpn( p
. g ,
+6g2 pb
! ffX
,
sp
$X$
,

$

22
$
p
j2 $
2 pj)
p
!
,
"
2p :
?n
$!p
2b
L )2~p

! z oX
2 p
2 $ < $)$
l
B$

< $@p
pW2 p uX
.l
$}
! 2 p))
$2 n2 pp

ff"
! $
< p
$
$2}
! 2
$
! 2 $D}
* $W>
3
* \X
l
n
@$
9ff"
! Hff9$$ "$
2 < $2
>
2 l
#
lff"2
@
! ff} ->
J$\X

2* :2 $-2 J@R
$2 2 $$
2$
zp
dp
9 W
H-
!
>$= dp$
"E2 $p
dpE2 $X
; W
< p|X
.l
$2}
! 2
Jff$$29pb slff$$2Wd 2 $k^pb
& 2$bp ,ff( +s/



fi oxorlzGir

$! 2$

$2!tfE(`qaa=6+"!LX iLqH^ %ely$!p2xKff)>CpW

$; !9


2p@l$\) 2l(Fp$XR+l) ,BX.$ 6Y
.$n
B 2pL2!pdpffff!$:


.$JXo$vpXCff$*
2p(P

] 2$ff"!l}"h 2$!+ x$$Cff2g
.$
H $2 p
H
! p

J.2 2 $
ff2
2$}
g
j2 $)p
>
"~
* p
J ;2 ]$
2Lffl$2^#
2
J2 pd$) uC$
"2
Y2 p2.
! bp
v$Lff"
! $
< $2
*J,
J9{2 p
$
" &nff92
J2 $v>c( ,)L
6 p2$X
(F
ff$X/ w
! $
ff *`aa`67p
o2 p
$
" :p
$u^8
Lg* ,
2Jl| v
Jp
dp2 -
:)
$ +2 #pC$
Y$

"y.$
@p
dpX>
UJ y2 $>2 $CX2$2

2 pUff$$p
2
%2 $C
!
dpff"
p
! ?$
V)
$2 $?2 p:
$
$H
! + "nT
* pp
b^0
< ^$
K|1
l
$
ffl$2^#
2
J
k+l
. 2 $R ,
X
$

* $
2ff$
| 2x
92 $-p
$X'X
.$
J]H
H $2 p
! |2 p2
g$
+d$
2s2 pff$l2p
ff"
! $
2p$
G
$
2$2 $&X
.$
$.
! ?= "ff*
$ g "$
ffp
@2 pG,
hH l;
! C

'2 p2 $X2$2 $
pW: E
$!p
K5J*
)p@>
2l$ Bp
\2 $@
F$
#$;
! + o2 p2 lff"2 X
* 2
!& "2
2
!


l$
GR
2 $e
ff"ff
l
y2 $ j2 p2lffL
Y2 p$"
! pvgp
jffff"
$
E
! $
2po2 $
! + "p
2
2 poX
.$
$uR
H $$p
2 $
@ $
G
E2 $$$*% @
LXl
$

).2 $Y2 p2ff2ff$$2Wd
2 $
U &2 p
!
m)$

ff"!p
2 l ->
ff2 Jlff?
! ff$}x ff9}
* 2?$
g>

2$
$2W@2 p
2"l
$2
X
2 J$
! $2 l*)
$>$
p

ff'R
$Ljff2Jlff}
< $"2 $

ffp
*O
$
g$

p
+ =2 G
lffG p $
R
$ 2 pv )2 p
X; @R
H $$p
2
$
GX
.$
l
! J2 $v$)X
2 $Wb2 p :GX
p
C.2 gw $2+
$!p

2xK5y )
! $U
;<
JC= "l
$2
X
2 pC "$
^$
@
9Oef
! $
V,

.$
@2 $
ff$$p
2 $
Hff2y$
"$
$R
$2 $D2 $@X
.$
l
! + "ff*)
p
2 2 z2 JL:
.
2
! l$u2 pJ2J < $Rpv~
* p ,
W*]p


b2 pbX
.l
$R22ff= ym)z2 pJ= ffff*
p
H$
&$
22 pX
.$
l!)n?p
>gff$2 yR
p2 p,ffl$2p
2 $
ff2-
! d,
j2 $ )}
$
C p$
+ "ff

w

_d

ffpjffff

U$U2$W: 2$ %X.$l2l! 2LRJl,$!$ 2$

)$ 2 "p$2?!$!vv 2ff$$ 2
el >X"
,p J 2 2lCvlff" 2C
] 2pjp >"$
olx ff
2$)$;#
2+ 2} 2$ }$?!,!ief 2lY>W$ CCffllB jp
$XC$
W>
xX
.$
$,
+ff"!?$gl2ll$
,2
X$vpH\ 2pL= 92
l
bp
>
"@}p
2=2 $h>

pp X.lp; 2$|)$9|p ,ff$2X" 2jff2RX$
2$l

Y2 p:2 $
! + "p
2,
J2 p-X
.$
$

.$ef$!pBP^*
>l; 2$*V,2WF2$l<
O>
H2"$
2 ,
L
J2 $
! + XX
.$
l]el
^v$ 2$ px}y$Lpl2
<$2 ip

! =X
p
$dff
* 2$
}X
.$
$2}
! 2
X"
dg 2 2$Hn 2p@p
,
ff w L$Wp
p

* pW>
*)
+o2 $$
$
ff2U ' 2pd 2>x
.l@2
$D]w x 9y )C$
L222 yR
$2 2 pU2$~ C
2 px>
$
! +2
(29$ 2!j 2pBp >
"
X +2 X >
c $ ;~ .
$; 9$2 3
6 Y)
$2 |2 pRd.
ff$
" 2

2pp
,
ff'( lD
! 2);2 67*
l



l?D2 pk^$
$

2 p
$
$UD 9

2
pgk^$
+2 $2 iR
2g>
X$
R
Xuk+$
;2
lJ}d

2 p)$
! 2 $ $
$L
p ?p
b^2 $);2 R2l
< $)J2 2 $

2
$>}ff ip
dpp
x$
*)
$L
2 pU$
! 2 $ $
"$
$2 }

$d$
.$

,
p
"d
U"$
9x,
c ;
~ .
9$p
'R
"2 p*2 p2>Lff"
!
! g

ff9$$ "p


)
${X
.$
l2l
! 2 2\X
p$
hNV$
9p
,
ffv
@ v
l
$W$



fix |lr.u>k i.$impo r

^$X
U.$ 2$@pv$&)$9

(P

@ 2p $


} 2p[ff"WE} 2

p!u &Xj 2lg
2p$$2l! 2$!$ 2$u

w jC
= } $2$
2jp
JX"2 +,
$ff

J2 $BX
.l
$l
! 2
$
! $ 2l>$
p $p2)X
2elG 2$Hffff 2y,y
.$p
x? )+z9k^$ 22^ py 2$
$j2
plff $
vpY)
l$C

}X
H?2 $)$
$
! =X
p
9pd
''w $*$p ,W*
$
F2{
p 9p2Z ? elv
N 2 >`Z>
;2 >2 $
"
=
$
l$2

n2 p
! $2 $ 92 l
b2 $ {UnVn F$
2p

! p p

J.2
2 $
yH 2b
! p
*O
;2 "
=
$
l$2 B2 $ g>
2pl
9pvo?
"$
9n2 l (TV;U
6 nX
J
+2 $X
* 2$
g2 lR2 pk+$
+2 $ D,
yp
'(
9k^$
2 u3679e
]2 pgp
>
"
; ff$
"$
2$
2p:2 pffJlff D^u?)
u
)
9$ ;

P".$ 2 2})
$2 H2p

! @ 2})
.2 *W>
>L
J X2 $>l
$$2 $
$2l
}
$2
$
x2
po+$
@X

$
vp$
2^Z?J
" ff"
! \$
Lff2 ff$ "p
22 p
lff9
R

D2 $R
gX
2X$

! Ym)$
u2 $.
<

&p
,
"
$
;2 J
!

D^$
X

2 l
tP".$
2 \ j2 $
ff2p
{ 9 2 pg^
+l
X


xp
dphmjp
2pl
GX
C
ff2
}2 pJX"2 +
$IHX2$
bX
>
s2 pJ2 pd .2 p

p2$
D2 p2 pv<

$
$$2 l
!D(F#
0 ff9*]23KK67*)
$9pvp
R
.
2l
9w $|p

2k^$
2D2 $lff"

j2 $np
>
"2 $
J2 pu2 plff {'( ,
p
dp&p

? *
BF$6Y &X
) 2 l
< pX
$2l
} p

+)
$
! $2 l bC$

"$
2 l
lff 9l
* 2l
j2 pgp
dp
"l
\U
ff2j
! p

l2D)
$$
$
$X2 $<ff"2 }>
C$
} 2L
B

Jx,
l<
.
p
ff$2
! ! < $
< $
Dl
C ,

ff2 $
;2 p}!bH-+ J$WJ+2 # $d
! $
! 2 $
$2
$

$
$
92 p>2"l
$2 gl2+ ?2 $#ff"2 ]w l

! p
Y2 $
J2Uff$

# J2 pd$$
&
$
C. '
I2 p'
92 $ 92 p) $
9 &Xk+l
2 u2
jlff'H}>
;
$$}


2 p+ ff2:~ 2 < jX
J$22O2 p)l
; +2 jl
.$
+L
lffff2 $
ffX%

ff2Up
2 $&`-%
j
2 )
y2 p 0
9)
2 p
2 $&`- U X$
V#T~ 2+ *+2 +$
<ld!

2


}2 C 2
[ WX
* 2 pgp
V#T~ J= Uff2p

$
$*V2 p2
~ 2X
$2.!|
F "p
2y
# g
* 2?$
y2 p$
! $2 lj DX
J"$
2 l

}ff
J.2
!u(FR
H \/
[
! $p*J233a=67


,8` b]


}

< $C2ff9)"p$X" h^h 2p < .p!up.$$ 28 Q* *lffll2$pd2

$
! J
% 2p@[$" "
jpjH H3
& $ D2 pj 9.p !

G

l



HH*]A@$*i/ [!$$*%A@}(f233a=67{0ff,?pff$!.!sCv
ffp
2$?l; 2= 2Jel
lJqL 45^qK`^

$$2

J;

ff
fifi !""#$&%(')fi*!,+.-/1023+.04+*

c>$*Je$*NpJ$. *$*iC$~*$*/ >X*<x(f23K367 < $HBtOHZ-[ J$ !
"v
; uH L; "$
.D)
$2 ,
$2}
l" 2
2$g 9.$k^p-
BX
p >"d
ef
.v
N $.
! ;# <!*
c
>

657-80++9;:<%=!>+@? A7-*+9BC')D%2+2-+20+<FE=GAHI+J"08KL+


3M

fi oxorlzGir

A57-804+J+9BK;:B7&%7!>+ON@ff!>=ff!,+.-4B!"PBQR')B8%2+2-+20+
B(EG-4!" S02PQ1NA+.-Q1NA+2!"T-UD #M
^V* < $*X/$*9[-(f233367VMff 2$$2ll; 2
2$$ 2p61W9X Q
$ >" YB-4QZ&%=E=-.!" S[02PQ1ff!,+2QKQ\]:+.04+G^_+48+9-0.=
> *)`b5a *`3V27^``^

H= J*n+(f233367V02d.$ 2~
vdX" 2ef
* <O2*l$JJ27^P^

dff$X*@$*O/vw!$ff *wX(`aa`67|HpW,$ 2
2
pvJ%ef

29pJb
g 2pl; jnff9 2

cEd 10+4@FNA+.-Q18%2-4fi/B!""Be57-80+.J;:$#
.!,+.fiA7*#M<TJ24*pJell22
d$9$$*t9(f23L367HX$Vp$!$ 2l[??pff$2!!('?0-$22 67fff<UgQh

E=U b+2fiiKPjN<Ukff#B^ *@?b?Dl *@2a3W 2a3P^ 9.!2 $2 2-Nv v n[ 2p 2
d*#<M ?$J@`aV*O23V2#ff2354
t~2* [$*/dff$X*@%(`aa2W67jH 2$!+ )X.$D
-!9ffl$#Jv$=NA+.-Q[')fim
!3 !"" *Z`Bn (367*`V24=3W^`V2WL^2
t~2*[$*J/dff$X* @i(`aa`67[$*322 2$$z$$-el
c ff
>
*)N$*/ $pl*x,j(F,$ 67*
7* <O24$
* lJ4=qqW=4=P`^*U
$
9!*[\H [ne < 02



; 9* <

~$*

Ed B0+.opNA+.-Qq8%.B-fi/B!"Pr57-80+.J4K;:

ff
g.!,+.fiA

[\p2l+*%d&$*]w *%_$*i/ l*%[(f233367st~v^EX
U$2}! 2Z
Jff$$p
?
$
OHB Jl9<; "$|el
* <X2q^*#$J
4=PL=4+Lq^

tsu04+.-.!3ff!"
cIE=-4!" S08"Qu!,+.QQvh:;+20+

#EG-4!" S02PQff!,+.QQvh:;+20+*wbx*L^27 22^
0ff**.O(f23KK67y5y-&;z9bz2KQ\{.!""0A-9+9.#;:(KoK!,+.QQvh:;+2ff!O4
4!,+.fiA*$$J`a45^`V2aVx[ff!nd
U#
.u09.l2$*=ef$O|h8} a5#354=PV2 #lL #lL+
AR*
V(f233`67 C.p 2$; #ff7$!

^$ >"v

0 *X)$*'/
>
$
"v


HB$p9*x(f23KL67H J
C$s 2p"
* * 33qW 2
23^

A')fiQ~+9$4
4!,+.fiA u`

ffff$?!!$ 2$
y$p

0j
UffV* < %
(f23KV2W67>+"!$J$l$ 2
2p Q=ROS k^$ 2\
) 2pLCl$ #!2!
2lh!2)Jv$
* *]23L^27 23LK^

7Y-4Q&%=5[>;
g4"02=EHIB!>+2fi !""0QZ1:;+.L+.-Q Z` w

Nd$T*+ti$*+V* < $*/$*[EJ(f233P679[hC$ 2p"
U2$!|X
Jp >"d
=* * PV27VLP^

Y*-4QZ%=E=-.!" S08"BQff!,+.QQvh:;+20+^q+42+J-0.>

Nd+,*[$*[$l * cB$*R"J* $*Rp*[$*5R"d$ ;~*$*t~$* @$*/1CX*
y)(f233V2W67
0 2}
l
"2 $!p
2|$2?!
$ 2
j 2p Q Q

Vp
)

! g}

* 5*`54.27^`qq^

|

HI+8!>8 A%=8%2-fi/B!"PcHI+J"08KL+ n;a
0J*Kgx(f23LL67ZNv 2
* ](67*q3W^Pa2

< pl2* $*'HB$p*]0%$*'/
!2$

[57>bKQ\K.ff>;"0QRHI8:bg.1+ n;w

| W8;


jnlJdpj
Rl

HB!>+2fi/B!""0Q57-9: -&fiifii;:*ub`*

< v$J*^[(`aa`67 < $R+&
' 2U
g$ffj$2!9!
4.2WL=4=P^
w



+$!+ *[E$* ^V* <

$*/w;^* H@(`aa`67Hp

2H
9$$X-X.$lz 2p

su04+.-.!3ff!"
cE=-4!" S08"Q!,+.QQvh:;+20+*#M<O2K^

!lff" 2$ 2h
fi.l 2el

_$lV*.$*V$2*Vw$*/

w

,X$.!* A@$*/0*



6Ed 10+4

*=_yJ(`aaa=67jHp~X
l2l! 2]el

cN<+2-&QL8%.B-fi/B!"Po5y-&804+4;:F#
4!,+2fiA7*#M<]2^*$$JPK3W^P3q^
(f23354+67jH

2Jlffl$2\ DcC2Gp ,ffJ 2 2$:ef

5y-&D0+J+J;:@&%G!>+/1+.ff!>o')B Pj')B8%2+2-+20+E=-.!" S[02PQff!,+2QKQ\h:;+.10+*$J2WL^27


2WLK^

fiJournal Artificial Intelligence Research 19 (2003) 315-354

Submitted 12//02; published 10/03

Learning Training Data Costly: Effect Class
Distribution Tree Induction
Gary M. Weiss

GMWEISS@ATT.COM

AT&T Labs, 30 Knightsbridge Road
Piscataway, NJ 08854 USA

Foster Provost

FPROVOST@STERN.NYU.EDU

New York University, Stern School Business
44 W. 4th St., New York, NY 10012 USA

Abstract
large, real-world inductive learning problems, number training examples often
must limited due costs associated procuring, preparing, storing training
examples and/or computational costs associated learning them. circumstances, one question practical importance is: n training examples selected,
proportion classes represented? article help answer
question analyzing, fixed training-set size, relationship class distribution training data performance classification trees induced data.
study twenty-six data sets and, each, determine best class distribution learning.
naturally occurring class distribution shown generally perform well classifier
performance evaluated using undifferentiated error rate (0/1 loss). However, area
ROC curve used evaluate classifier performance, balanced distribution
shown perform well. Since neither choices class distribution always generates
best-performing classifier, introduce budget-sensitive progressive sampling algorithm selecting training examples based class associated example.
empirical analysis algorithm shows class distribution resulting training
set yields classifiers good (nearly-optimal) classification performance.

1. Introduction
many real-world situations number training examples must limited obtaining
examples form suitable learning may costly and/or learning examples may
costly. costs include cost obtaining raw data, cleaning data, storing
data, transforming data representation suitable learning, well cost
computer hardware, cost associated time takes learn data, opportunity cost associated suboptimal learning extremely large data sets due limited
computational resources (Turney, 2000). costs make necessary limit amount
training data, important question is: proportion classes represented
training data? answering question, article makes two main contributions. addresses (for classification-tree induction) practical problem select class distribution training data amount training data must limited, and, providing
detailed empirical study effect class distribution classifier performance, provides
better understanding role class distribution learning.
2003 AI Access Foundation Morgan Kaufmann Publishers. Rights Reserved.

fiWeiss & Provost

practitioners believe naturally occurring marginal class distribution
used learning, new examples classified using model built underlying distribution. practitioners believe training set contain increased
percentage minority-class examples, otherwise induced classifier classify
minority-class examples well. latter viewpoint expressed statement, sample
size fixed, balanced sample usually produce accurate predictions unbalanced 5%/95% split (SAS, 2001). However, aware thorough prior empirical study
relationship class distribution training data classifier performance,
neither views validated choice class distribution often made
arbitrarilyand little understanding consequences. article provide thorough study relationship class distribution classifier performance provide
guidelinesas well progressive sampling algorithmfor determining good class distribution use learning.
two situations research described article direct practical use.
training data must limited due cost learning data, resultsand guidelines establishcan help determine class distribution
used training data. case, guidelines determine many examples
class omit training set cost learning acceptable. second
scenario training examples costly procure number training examples
must limited. case research presented article used determine
proportion training examples belonging class procured order
maximize classifier performance. Note assumes one select examples belonging
specific class. situation occurs variety situations, examples
belonging class produced stored separately main cost due transforming raw data form suitable learning rather cost obtaining raw,
labeled, data.
Fraud detection (Fawcett & Provost, 1997) provides one example training instances belonging class come different sources may procured independently class.
Typically, bill paid, transactions credited fraudulent stored
separately legitimate transactions. Furthermore transactions credited customer
fraudulent may fact legitimate, transactions must undergo verification
process used training data.
situations, labeled raw data obtained cheaply, process
forming usable training examples raw data expensive. example, consider
phone data set, one twenty-six data sets analyzed article. data set used
learn classify whether phone line associated business residential customer.
data set constructed low-level call-detail records describe phone call,
record includes originating terminating phone numbers, time call made,
day week duration call. may hundreds even thousands call-detail
records associated given phone line, must summarized single training example. Billions call-detail records, covering hundreds millions phone lines, potentially available learning. effort associated loading data dozens
computer tapes, disk-space limitations enormous processing time required summarize raw data, feasible construct data set using available raw data. Consequently, number usable training examples must limited. case done
based class associated phone linewhich known. phone data set
316

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

limited include approximately 650,000 training examples, generated approximately 600 million call-detail records. huge transaction-oriented databases
routinely used learning, expect number training examples also
need limited many cases.
remainder article organized follows. Section 2 introduces terminology
used throughout article. Section 3 describes adjust classifier compensate
changes made class distribution training set, generated classifier
improperly biased. experimental methodology twenty-six benchmark data sets analyzed article described Section 4. Section 5 performance classifiers
induced twenty-six naturally unbalanced data sets analyzed, order show
class distribution affects behavior performance induced classifiers. Section 6,
includes main empirical results, analyzes varying class distribution
training data affects classifier performance. Section 7 describes progressive sampling
algorithm selecting training examples, resulting class distribution yields classifiers perform well. Related research described Section 8 limitations research
future research directions discussed Section 9. main lessons learned
research summarized Section 10.

2. Background Notation
Let x instance drawn fixed distribution D. Every instance x mapped (perhaps
probabilistically) class C {p, n} function c, c represents true, unknown, classification function.1 /HW EH WKH PDUJLQDO SUREDELOLW\ RI PHPEHUVKLS RI x
positive class 1 WKH PDUJLQDO SURbability membership negative class.
marginal probabilities sometimes referred class priors base rate.
classifier mapping instances x classes {p, n} approximation c.
notational convenience, let t(x) {P, N} always clear whether class value
actual (lower case) predicted (upper case) value. expected accuracy classifier t, t,
GHILQHGDV = Pr(t(x) = c(x)), or, equivalently as:
Pr(t(x) = P | c(x) = p) + (1 fiPr(t(x) = N | c(x) = n)

[1]

Many classifiers produce classification, also estimates probability x
take class value. Let Postt(x) classifier ts estimated (posterior) probability
instance x, c(x) = p. Classifiers produce class-membership probabilities produce classification applying numeric threshold posterior probabilities. example, threshold
value .5 may used t(x) = P iff Postt (x) > .5; otherwise t(x) = N.
variety classifiers function partitioning input space set L disjoint regions
(a region defined set potential instances). example, classification tree,
regions described conjoining conditions leading leaves tree. region
L L ZLOOFRQWDLQVRPHQXPEHURIWUDLQLQJLQVWDQFHV L/HW LpDQG Ln numbers
positiYHDQGQHJDWLYHWUDLQLQJLQVWDQFHVLQUHJLRQ/VXFKWKDW L Lp + Ln. classifiers
1. paper addresses binary classification; positive class always corresponds minority class negative class majority class.

317

fiWeiss & Provost

often estimate Postt(x | x /fiDV Lpff Lp+ Ln) assign classification instances x L
based estimate numeric threshold, described earlier. Now, let LP LN
sets regions predict positive negative classes, respectively, LP LN = L.
region L L, tKDVDQDVVRFLDWHGDFFXUDF\ L = Pr(c(x) = t(x) | x Lfi/HW LP represent expected accuracy x LPDQG LN expected accuracy x LN.2 shall see
LQ6HFWLRQZHH[SHFW LP LN .5.

3. Correcting Changes Class Distribution Training Set
Many classifier induction algorithms assume training test data drawn
fixed, underlying, distribution D. particular, algorithms assume rtrain rtest,
fractions positive examples training test sets, approximDWH WKH WUXH SULRU
probability encountering positive example. induction algorithms use estimated
class priors based rtrain, either implicitly explicitly, construct model assign classifications. estimated value class priors accurate, posterior probabilities model improperly biased. Specifically, increasing prior probability
class increases posterior probability class, moving classification boundary
class cases classified class (SAS, 2001). Thus, training-set data
selected rtrain GRHVQRWDSSUR[LPDWH WKHQWKHSRVWHULRUSUREDELOLWLHVVKRXOGEHDdjusted based differences beWZHHQ DQGrtrain. correction performed,
resulting bias cause classifier classify preferentially sampled class accurately, overall accuracy classifier almost always suffer (we discuss
Section 4 provide supporting evidence Appendix A).3
majority experiments described article class distribution training set
purposefully altered rtrainGRHVQRWDSSUR[LPDWH 7KHSXUSRVHIRUPRGLIying class
distribution training set evaluate change affects overall performance
classifierand whether produce better-performing classifiers. However,
want biased posterior probability estimates affect results. section describe
method adjusting posterior probabilities account difference rtrainDQG
method (Weiss & Provost, 2001) justified informally, using simple, intuitive, argument.
Elkan (2001) presents equivalent method adjusting posterior probabilities, including
formal derivation.
learning classification trees, differences rtrainDQG QRUPDOO\UHVXOWLQEiased
posterior class-probability estimates leaves. remove bias, adjust probability
estimates take differences account. Two simple, common probability estimation
IRUPXODV DUH OLVWHG LQ 7DEOH )RU HDFK OHW Lp ff Ln) represent number minority-class
(majority-class) training examples leaf L decision tree (or, generally, within
region L). uncorrected estimates, based assumption training
test sets drawn approximate , estimate probability seeing minority-class
(positive) example L. uncorrected frequency-based estimate straightforward requires explanation. However, estimate perform well sample size,
Lp Ln, smalland even defined sample size 0. reasons
2. notational convenience treat LP LN union sets instances corresponding regions.
3. situations costly misclassify minority-class examples majority-class examples, practitioners sometimes introduce bias purpose.

318

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

Laplace estimate often used instead. consider version based Laplace law succession (Good, 1965). probability estimate always closer 0.5 frequencybased estimate, difference two estimates negligible large sample
sizes.
Estimate Name
Frequency-Based
Laplace (law succession)

Uncorrected
Lp/( Lp+ Ln)

ff Lp+1)/( Lp+ Ln+2)

Corrected
Lpff Lp+o Ln)
ff Lpfiff Lp+o Ln+2)

Table 1: Probability Estimates Observing Minority-Class Example
corrected versions estimates Table 1 account differences rtrainDQG
factoring over-sampling ratio o, measures degree minority class
over-sampled training set relative naturally occurring distribution. value
computed ratio minority-class examples majority-class examples training set
divided ratio naturally occurring class distribution. ratio minority
majority examples 1:2 training set 1:6 naturally occurring distribution,
would 3. learner account properly differences rtrainDQG E\XVLQJ
corrected estimates calculate posterior probabilities L.
example, ratio minority-class examples majority-class examples naturally occurring class distribution 1:5 training distribution modified ratio
1:1, 1.0/0.2, 5. L labeled minority class probability must
greater 0.5, so, using corrected frequency-EDVHG HVWLPDWH Lpff Lp Ln) > 0.5, or,
Lp! Ln. Thus, L labeled minority class covers times many minorityclass examples majority-class examples. Note calculating use class
ratios fraction examples belonging minority class (if mistakenly used
latter example, would one-half divided one-sixth, 3). Using class
ratios substantially simplifies formulas leads easily understood estimates. Elkan
(2001) provides complex, equivalent, formula uses fractions instead ratios.
discussion assume good approximation true base rate known.
real-world situations true different methods required compensate
changes training set (Provost & Fawcett, 2001; Saerens et al., 2002).
order demonstrate importance using corrected estimates, Appendix presents
results comparing decision trees labeled using uncorrected frequency-based estimate
trees using corrected frequency-based estimate. comparison shows particular
modification class distribution training sets (they modified classes
balanced), using corrected estimates yields classifiers substantially outperform classifiers
labeled using uncorrected estimate. particular, twenty-six data sets used
study, corrected frequency-based estimate yields relative reduction error rate 10.6%.
Furthermore, one twenty-six data sets corrected estimate perform worse.
Consequently critical take differences class distributions account
labeling leaves. Previous work modifying class distribution training set (Catlett,
1991; Chan & Stolfo, 1998; Japkowicz, 2002) take differences account
undoubtedly affected results.
319

fiWeiss & Provost

4. Experimental Setup
section describe data sets analyzed article, sampling strategy used
alter class distribution training data, classifier induction program used, and, finally,
metrics evaluating performance induced classifiers.

4.1 Data Sets Method Generating Training Data
twenty-six data sets used throughout article described Table 2. collection
includes twenty data sets UCI repository (Blake & Merz, 1998), five data sets, identified +, previously published work researchers AT&T (Cohen & Singer,
1999), one new data set, phone data set, generated authors. data sets Table
2 listed order decreasing class imbalance, convention used throughout article.

Dataset
letter-a*
pendigits*
abalone*
sick-euthyroid
connect-4*
optdigits*
covertype*
solar-flare*
phone
letter-vowel*
contraceptive*
adult
splice-junction*

% Minority Dataset
Examples
Size
# Dataset
3.9
20,000 14 network2
8.3
13,821 15 yeast*
8.7
4,177 16 network1+
9.3
3,163 17 car*
9.5
11,258 18 german
9.9
5,620 19 breast-wisc
14.8
581,102 20 blackjack+
15.7
1,389 21 weather+
18.2
652,557 22 bands
19.4
20,000 23 market1+
22.6
1,473 24 crx
23.9
48,842 25 kr-vs-kp
24.1
3,175 26 move+

% Minority Dataset
Examples
Size
27.9
3,826
28.9
1,484
29.2
3,577
30.0
1,728
30.0
1,000
34.5
699
35.6
15,000
40.1
5,597
42.2
538
43.0
3,181
44.5
690
47.8
3,196
49.9
3,029

Table 2: Description Data Sets
order simplify presentation analysis results, data sets
two classes mapped two-class problems. accomplished designating one
original classes, typically least frequently occurring class, minority class
mapping remaining classes majority class. data sets originally contained
2 classes identified asterisk (*) Table 2. letter-a data set created letter-recognition data set assigning examples labeled letter
minority class; letter-vowel data set created assigning examples labeled
vowel minority class.
generated training sets different class distributions follows. experimental run, first test set formed randomly selecting 25% minority-class examples
25% majority-class examples original data set, without replacement (the resulting
test set therefore conforms original class distribution). remaining data available
training. ensure experiments given data set training-set size
matter class distribution training setthe training-set size, S, made equal
total number minority-class examples still available training (i.e., 75% original
320

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

number). makes possible, without replicating examples, generate class distribution training-set size S. training set formed random sampling
remaining data, without replacement, desired class distribution achieved.
experiments described article, class distribution training set varied
minority class accounts 2% 95% training data.

4.2 C4.5 Pruning
experiments article use C4.5, program inducing classification trees labeled
examples (Quinlan, 1993). C4.5 uses uncorrected frequency-based estimate label
leaves decision tree, since assumes training data approximate true, underlying distribution. Given modify class distribution training set, essential
use corrected estimates re-label leaves induced tree. results presented
body article based use corrected versions frequency-based
Laplace estimates (described Table 1), using probability threshold .5 label leaves
induced decision trees.
C4.5 factor differences class distributions training test
setswe adjust post-processing step. C4.5s pruning strategy, attempts
minimize error rate, allowed execute, would prune based false assumption (viz.,
test distribution matches training distribution). Since may negatively affect
generated classifier, except otherwise indicated results based C4.5 without pruning. decision supported recent research, indicates target misclassification costs (or class distributions) unknown standard pruning improve,
may degrade, generalization performance (Provost & Domingos, 2001; Zadrozny & Elkan, 2001;
Bradford et al., 1998; Bauer & Kohavi, 1999). Indeed, Bradford et al. (1998) found even
pruning strategy adapted take misclassification costs class distribution account,
generally improve performance classifier. Nonetheless, order justify
using C4.5 without pruning, also present results C4.5 pruning training
set uses natural distribution. situation C4.5s assumption rtrainDSSUR[LPDWLQJ
valid hence pruning strategy perform properly. Looking ahead, results show
C4.5 without pruning indeed performs competitively C4.5 pruning.

4.3 Evaluating Classifier Performance
variety metrics assessing classifier performance based terms listed confusion matrix shown below.

c(x) Actual Positive
Actual Negative

t(x)
Positive Prediction Negative Prediction
tp (true positive)
fn (false negative)
fp (false positive)
tn (true negative)

Table 3 summarizes eight metrics. metrics described first two rows measure
ability classifier classify positive negative examples correctly, metrics
described last two rows measure effectiveness predictions made classifier.
example, positive predictive value (PPV), precision, classifier measures fraction positive predictions correctly classified. metrics described last two
321

fiWeiss & Provost

rows Table 3 used throughout article evaluate various training-set class distributions affect predictions made induced classifiers. Finally, metrics second
column Table 3 complements corresponding metrics first column,
alternatively computed subtracting value first column 1. specifically,
proceeding row 1 4, metrics column 1 (column 2) represent: 1) accuracy
(error rate) classifying positive/minority examples, 2) accuracy (error rate) classifying negative/minority examples, 3) accuracy (error rate) positive/minority predictions, 4) accuracy (error rate) negative/majority predictions.


tp
tp + fn

True Positive Rate FN = Pr(N|p)
(recall sensitivity)



fn
tp + fn

False Negative Rate

TN = Pr(N|n)

tn
tn + fp

True Negative Rate FP = Pr(P|n)
(specificity)



fp
tn + fp

False Positive Rate

PPV = Pr(p|P)

tp
tp + fp

NPV = Pr(n|N)

tn
tn + fn

TP = Pr(P|p)

fp
Positive Predictive Value PPV = Pr(n|P)
(precision)
tp + fp
Negative Predictive Value NPV =Pr(y|N)

fn
tn + fn

Table 3: Classifier Performance Metrics
use two performance measures gauge overall performance classifier: classification accuracy area ROC curve (Bradley, 1997). Classification accuracy (tp +
fp)/(tp + fp + tn + fn). formula, represents fraction examples correctly
FODVVLILHGLVDQHVWLPDWHRIWKHH[SHFWHGDFFXUDF\ t, defined earlier equation 1. Throughout
article specify classification accuracy terms error rate, 1 accuracy.
consider classification accuracy part common evaluation metric
machine-learning research. However, using accuracy performance measure assumes
target (marginal) class distribution known unchanging and, importantly,
error coststhe costs false positive false negativeare equal. assumptions
unrealistic many domains (Provost et al., 1998). Furthermore, highly unbalanced data sets
typically highly non-uniform error costs favor minority class, which, case
medical diagnosis fraud detection, class primary interest. use accuracy
cases particularly suspect since, discuss Section 5.2, heavily biased favor
majority class therefore sometimes generate classifiers never predict minority class. cases, Receiver Operating Characteristic (ROC) analysis appropriate
(Swets et al., 2000; Bradley, 1997; Provost & Fawcett, 2001). producing ROC curves
use Laplace estimate estimate probabilities leaves, since shown
yield consistent improvements (Provost & Domingos, 2001). assess overall quality
classifier measure fraction total area falls ROC curve (AUC),
equivalent several statistical measures evaluating classification ranking models
(Hand, 1997). Larger AUC values indicate generally better classifier performance and, particular, indicate better ability rank cases likelihood class membership.

322

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

5. Learning Unbalanced Data Sets
analyze classifiers induced twenty-six naturally unbalanced data sets described Table 2, focusing differences performance minority majority
classes. alter class distribution training data section, classifiers need adjusted using method described Section 3. However, experiments consistent Section 6 use natural distribution, size
training set reduced, described Section 4.1.
addressing differences, important discuss issue may lead confusion left untreated. Practitioners noted learning performance often unsatisfactory
learning data sets minority class substantially underrepresented. particular, observe large error rate minority class. clear
Table 3 associated discussion, two different notions error rate
minority class: minority-class predictions could high error rate (large PPV )
minority-class test examples could high error rate (large FN). practitioners observe
error rate unsatisfactory minority class, usually referring fact
minority-class examples high error rate (large FN). analysis section
show error rate associated minority-class predictions ( PPV ) minority-class test examples (FN) much larger majority-class counterparts ( NPV
FP, respectively). discuss several explanations observed differences.

5.1 Experimental Results
performances classifiers induced twenty-six unbalanced data sets described Table 4. table warrants explanation. first column specifies data set
name second column, convenience copied Table 2, specifies
percentage minority-class examples natural class distribution. third column
specifies percentage total test errors attributed test examples
belong minority class. comparing values columns two three see
cases disproportionately large percentage errors come minority-class examples. instance, minority-class examples make 3.9% letter-a data set contribute 58.3% errors. Furthermore, 22 26 data sets majority errors
attributed minority-class examples.
fourth column specifies number leaves labeled minority majority
classes shows two cases fewer leaves labeled minority class
majority class. fifth column, Coverage, specifies average number
training examples minority-labeled majority-labeled leaf classifies (covers).
results indicate leaves labeled minority class formed far fewer training
examples labeled majority class.
Prediction ER column specifies error rates associated minority-class
majority-class predictions, based performance predictions classifying test
examples. Actuals ER column specifies classification error rates minority
majority class examples, based test set. last two columns also labeled
using terms defined Section 2 ( PPV , NPV , FN, FP). example, columns
show letter-a data set minority-labeled predictions error rate 32.5%
majority-labeled predictions error rate 1.7%, minorityclass test examples classification error rate 41.5% majority-class test exam323

fiWeiss & Provost

ples error rate 1.2%. last two columns underline higher
error rate.

Dataset
letter-a
pendigits
abalone
sick-euthyroid
connect-4
optdigits
covertype
solar-flare
phone
letter-vowel
contraceptive
adult
splice-junction
network2
yeast
network1
car
german
breast-wisc
blackjack
weather
bands
market1
crx
kr-vs-kp
move
Average
Median

% Minority % Errors
Examples Min.
3.9
8.3
8.7
9.3
9.5
9.9
14.8
15.7
18.2
19.4
22.6
23.9
24.1
27.9
28.9
29.2
30.0
30.0
34.5
35.6
40.1
42.2
43.0
44.5
47.8
49.9
25.8
26.0

58.3
32.4
68.9
51.2
51.4
73.0
16.7
64.4
64.4
61.8
48.7
57.5
58.9
57.1
58.9
57.1
58.6
55.4
45.7
81.5
50.7
91.2
50.3
51.0
54.0
61.4
56.9
57.3

Leaves
Min. Maj.

Coverage
Min. Maj.

Prediction ER
Min.
Maj.

Actuals ER
Min.
Maj.

11
6
5
4
47
15
350
12
1008
233
31
627
26
50
8
42
38
34
5
13
134
52
87
28
23
235
120
33

2.2
4.3
16.8 109.3
2.8 35.5
7.1 26.9
1.7
5.8
2.9
2.4
27.3 123.2
1.7
3.1
13.0 62.7
2.4
0.9
1.8
2.8
3.1
1.6
5.5
9.6
4.0 10.3
14.4 26.1
5.1 12.8
3.1
6.6
2.0
2.0
12.6 26.0
57.7 188.0
5.0
7.2
1.4
0.3
5.1
2.7
3.9
2.1
24.0 41.2
2.4
0.6
8.8 27.4
3.9
6.9

(PPV)
32.5
25.8
69.8
22.5
55.8
18.0
23.1
67.8
30.8
27.0
69.8
34.3
15.1
48.2
45.6
46.2
14.0
57.1
11.4
28.9
41.0
17.8
30.9
23.2
1.2
24.4
33.9
29.9

(FN)
41.5
14.3
84.4
24.7
57.6
36.7
5.7
78.9
44.6
37.5
68.3
41.5
20.3
55.5
55.0
53.9
18.6
62.4
9.8
64.4
41.7
69.8
31.2
24.1
1.4
33.9
41.4
41.5

138
8
8
9
128
173
446
48
1220
2547
70
4118
46
61
12
49
42
81
5
19
142
389
227
65
15
1025
426
67

(NPV)
1.7
1.3
7.7
2.5
6.0
3.9
1.0
13.7
9.5
8.7
20.1
12.6
6.3
20.4
20.9
21.0
7.7
25.4
5.1
27.9
27.7
34.8
23.4
18.9
1.3
29.9
13.8
11.1

(FP)
1.2
2.7
3.6
2.4
5.7
1.5
4.9
8.1
5.5
5.6
21.1
9.6
4.5
16.2
15.6
16.7
5.6
21.5
6.1
8.1
27.1
4.9
23.3
18.5
1.1
21.2
10.1
5.9

Table 4: Behavior Classifiers Induced Unbalanced Data Sets
results Table 4 clearly demonstrate minority-class predictions perform much
worse majority-class predictions minority-class examples misclassified
much frequently majority-class examples. twenty-six data sets, minority
predictions average error rate ( PPV ) 33.9% majority-class predictions
average error rate ( NPV ) 13.8%. Furthermore, three twenty-six data
sets majority-class predictions higher error rateand three data sets
class distributions slightly unbalanced. Table 4 also shows us average error rate
minority-class test examples (FN) 41.4% whereas majority-class test examples
error rate (FP) 10.1%. every one twenty-six cases minority-class test
examples higher error rate majority-class test examples.
324

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

5.2 Discussion
minority-class predictions higher error rate ( PPV ) majority-class
predictions ( NPV )? least two reasons. First, consider classifier trandom
partitions L chosen randomly assignment L L LP LN also made
randomly (recall LP LN represent regions labeled positive negative
classes). two-class learning problem WKH H[SHFWHG RYHUDOO DFFXUDF\ t, randomly
generated labeled classifier must 0.5. However, expected accuracy regions
positive partiWLRQ LPZLOOEH ZKLOHWKHH[SHFWHGDFFXUDF\RIWKHUHJLRQVLQWKHQHJDWLYH
partition, LN, 1 )RUDKLJKO\XQEDODQFHGFODVVGLVWULEXWLRQZKHUH LP = .01
DQG LN = .99. Thus, scenario negative/majority predictions much
accurate. test distribution effect small well-learned concept
low Bayes error rate (and non-existent perfectly learned concept Bayes error rate
0), many learning problems quite hard high Bayes error rates.4
results Table 4 suggest second explanation minority-class predictions
error prone. According coverage results, minority-labeled predictions tend formed
fewer training examples majority-labeled predictions. Small disjuncts,
components disjunctive concepts (i.e., classification rules, decision-tree leaves, etc.) cover
training examples, shown much higher error rate large disjuncts
(Holte, et al., 1989; Weiss & Hirsh, 2000). Consequently, rules/leaves labeled minority class higher error rate partly suffer problem small
disjuncts.
Next, minority-class examples classified incorrectly much often majorityclass examples (FN > FP)a phenomenon also observed others (Japkowicz &
Stephen, 2002)? Consider estimated accuracy, at, classifier t, test set drawn
true, underlying distribution D:
= TP rtest + TN (1 rtest)

[2]

Since positive class corresponds minority class, rtest < .5, highly unbalanced
data sets rtest << .5. Therefore, false-positive errors damaging classification accuracy
false negative errors are. classifier induced using induction algorithm geared
toward maximizing accuracy therefore prefer false-negative errors false-positive
errors. cause negative/majority examples predicted often hence
lead higher error rate minority-class examples. One straightforward example
learning algorithms exhibit behavior provided common-sense rule:
evidence favoring one classification another, predict majority class. generally, induction algorithms maximize accuracy biased perform better classifying majority-class examples minority-class examples, since former component
weighted heavily calculating accuracy. also explains why, learning
data sets high degree class imbalance, classifiers rarely predict minority class.
second reason minority-class examples misclassified often majorityclass examples fewer minority-class examples likely sampled distribu4. (optimal) Bayes error rate, using terminology Section 2, occurs t(.)=c(.). c(.) may
probabilistic (e.g., noise present), Bayes error rate well-learned concept may always low.
test distribution effect small concept well learned Bayes error rate low.

325

fiWeiss & Provost

tion D. Therefore, training data less likely include (enough) instances
minority-class subconcepts concept space, learner may opportunity
represent truly positive regions LP. this, minority-class test examples
mistakenly classified belonging majority class.
Finally, worth noting PPV > NPV imply FN > FP. is,
error-prone minority predictions imply minority-class examples
misclassified often majority-class examples. Indeed, higher error rate minority
predictions means majority-class test examples misclassified. reason generally observe lower error rate majority-class test examples (FN > FP)
majority class predicted far often minority class.

6. Effect Training-Set Class Distribution Classifier Performance
turn central questions study: different training-set class distributions affect performance induced classifiers class distributions lead
best classifiers? begin describing methodology determining class distribution performs best. Then, next two sections, evaluate analyze classifier performance twenty-six data sets using variety class distributions. use error rate
performance metric Section 6.2 AUC performance metric Section 6.3.

6.1 Methodology Determining Optimum Training Class Distribution(s)
order evaluate effect class distribution classifier performance, vary training-set class distributions twenty-six data sets using methodology described Section
4.1. evaluate following twelve class distributions (expressed percentage minority-class examples): 2%, 5%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 95%.
data set also evaluate performance using naturally occurring class distribution.
try determine best class distribution training set, several issues must addressed. First, evaluate every possible class distribution,
determine best distribution among 13 evaluated distributions. Beyond
concern, however, issue statistical significance and, generate classifiers
13 training distributions, issue multiple comparisons (Jensen & Cohen, 2000).
issues cannot always conclude distribution yields best performing classifiers truly best one training.
take several steps address issues statistical significance multiple comparisons. enhance ability identify true differences classifier performance respect
changes class distribution, results presented section based 30 runs, rather
10 runs employed Section 5. Also, rather trying determine best class distribution, adopt conservative approach, instead identify optimal range class
distributionsa range confident best distribution lies. identify optimal range class distributions, begin identifying, data set, class distribution
yields classifiers perform best 30 runs. perform t-tests compare
performance 30 classifiers 30 classifiers generated using
twelve class distributions (i.e., 12 t-tests n=30 data points). t-test yields probability .10 conclude best distribution different distribution
(i.e., least 90% confident this); otherwise cannot conclude class distributions truly perform differently therefore group distributions together. grouped
326

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

distributions collectively form optimal range class distributions. Tables 5 6
show, 50 52 cases optimal ranges contiguous, assuaging concerns conclusions due problems multiple comparisons.

6.2 Relationship Class Distribution Classification Error Rate
Table 5 displays error rates classifiers induced twenty-six data sets.
first column Table 5 specifies name data set next two columns specify
error rates result using natural distribution, without pruning.
next 12 columns present error rate values 12 fixed class distributions (without pruning). data set, best distribution (i.e., one lowest error rate) highlighted underlining displaying boldface. relative position natural
distribution within range evaluated class distributions denoted use vertical
bar columns. example, letter-a data set vertical bar indicates
natural distribution falls 2% 5% distributions (from Table 2 see 3.9%).
Error Rate using Specified Training Distribution
(training distribution expressed % minority)

Dataset
Nat-Prune Nat
letter-a

2

5

10

20

30

40

50

60

70

Relative %
Improvement
80

90

95 best vs. nat best vs. bal

2.80 x 2.78

2.86 2.75 2.59 3.03 3.79 4.53 5.38 6.48 8.51 12.37 18.10 26.14

6.8

51.9

pendigits

3.65 + 3.74

5.77 3.95 3.63 3.45 3.70 3.64 4.02 4.48 4.98 5.73 8.83 13.36

7.8

14.2

abalone

10.68 x 10.46

9.04 9.61 10.64 13.19 15.33 20.76 22.97 24.09 26.44 27.70 27.73 33.91

13.6

60.6

4.46 x 4.10

5.78 4.82 4.69 4.25 5.79 6.54 6.85 9.73 12.89 17.28 28.84 40.34

0.0

40.1

10.68 x 10.56

7.65 8.66 10.80 15.09 19.31 23.18 27.57 33.09 39.45 47.24 59.73 72.08

27.6

72.3

4.94 x 4.68

8.91 7.01 4.05 3.05 2.83 2.79 3.41 3.87 5.15 5.75 9.72 12.87

40.4

18.2

5.12 x 5.03

22.6

sick-euthyroid
connect-4
optdigits

5.54 5.04 5.00 5.26 5.64 5.95 6.46 7.23 8.50 10.18 13.03 16.27

0.6

solar-flare

19.16 + 19.98 16.54 17.52 18.96 21.45 23.03 25.49 29.12 30.73 33.74 38.31 44.72 52.22

17.2

43.2

phone

12.63 x 12.62 13.45 12.87 12.32 12.68 13.25 13.94 14.81 15.97 17.32 18.73 20.24 21.07

2.4

16.8

letter-vowel

11.76 x 11.63 15.87 14.24 12.53 11.67 12.00 12.69 14.16 16.00 18.68 23.47 32.20 41.81

0.0

17.9

contraceptive

31.71 x 30.47 24.09 24.57 25.94 30.03 32.43 35.45 39.65 43.20 47.57 54.44 62.31 67.07

20.9

39.2

adult

17.42 x 17.25 18.47 17.26 16.85 17.09 17.78 18.85 20.05 21.79 24.08 27.11 33.00 39.75

2.3

16.0

8.30 + 8.37 20.00 13.95 10.72 8.68 8.50 8.15 8.74 9.86 9.85 12.08 16.25 21.18

2.6

6.8

27.13 x 26.67 27.37 25.91 25.71 25.66 26.94 28.65 29.96 32.27 34.25 37.73 40.76 37.72

3.8

14.4

yeast

26.98 x 26.59 29.08 28.61 27.51 26.35 26.93 27.10 28.80 29.82 30.91 35.42 35.79 36.33

0.9

8.5

network1

27.57 + 27.59 27.90 27.43 26.78 26.58 27.45 28.61 30.99 32.65 34.26 37.30 39.39 41.09

3.7

14.2

covertype

splice-junction
network2

car
german

9.51 x 8.85 23.22 18.58 14.90 10.94 8.63 8.31 7.92 7.35 7.79 8.78 10.18 12.86

16.9

7.2

33.76 x 33.41 30.17 30.39 31.01 32.59 33.08 34.15 37.09 40.55 44.04 48.36 55.07 60.99

9.7

18.7
0.0

7.41 x 6.82 20.65 14.04 11.00 8.12 7.49 6.82 6.74 7.30 6.94 7.53 10.02 10.56

1.2

blackjack

28.14 + 28.40 30.74 30.66 29.81 28.67 28.56 28.45 28.71 28.91 29.78 31.02 32.67 33.87

0.0

1.1

weather

33.68 + 33.69 38.41 36.89 35.25 33.68 33.11 33.43 34.61 36.69 38.36 41.68 47.23 51.69

1.7

4.3

breast-wisc

bands

32.26 + 32.53 38.72 35.87 35.71 34.76 33.33 32.16 32.68 33.91 34.64 39.88 40.98 40.80

1.1

1.6

market1

26.71 x 26.16 34.26 32.50 29.54 26.95 26.13 26.05 25.77 26.86 29.53 31.69 36.72 39.90

1.5

0.0

crx

20.99 x 20.39 35.99 30.86 27.68 23.61 20.84 20.82 21.48 21.64 22.20 23.98 28.09 32.85

0.0

5.1

1.25 + 1.39 12.18 6.50 3.20 2.33 1.73 1.16 1.22 1.34 1.53 2.55 3.66 6.04

16.5

4.9

27.54 + 28.57 46.13 42.10 38.34 33.48 30.80 28.36 28.24 29.33 30.21 31.80 36.08 40.95

1.2

0.0

kr-vs-kp
move

Table 5: Effect Training Set Class Distribution Error Rate
327

fiWeiss & Provost

error rate values significantly different, statistically, lowest error
rate (i.e., comparison yields t-test value > .10) shaded. Thus, letter-a data set,
optimum range includes class distributions include 2% 10% minorityclass exampleswhich includes natural distribution. last two columns Table 5 show
relative improvement error rate achieved using best distribution instead natural balanced distributions. improvement statistically significant (i.e., associated t-test value .10) value displayed bold.
results Table 5 show 9 26 data sets confident natural distribution within optimal range. 9 data sets, using best distribution
rather natural distribution yields remarkably large relative reduction error rate.
feel sufficient evidence conclude accuracy, training-set size must
limited, appropriate simply assume natural distribution used.
Inspection error-rate results Table 5 also shows best distribution differ
natural distribution consistent mannersometimes includes minorityclass examples (e.g., optdigits, car) sometimes fewer (e.g., connect-4, solar-flare). However,
clear data sets substantial amount class imbalance (the ones top half
table), balanced class distribution also best class distribution training,
minimize undifferentiated error rate. specifically, none top-12 skewed data
sets balanced class distribution within respective optimal ranges, data
sets relative improvements balanced distributions striking.
Let us consider error-rate values remaining 17 data sets t-test results permit us conclude best observed distribution truly outperforms natural distribution. cases see error rate values 12 training-set class
distributions usually form unimodal, nearly unimodal, distribution. distribution
one would expect accuracy classifier progressively degrades deviates
best distribution. suggests adjacent class distributions may indeed produce
classifiers perform differently, statistical testing sufficiently sensitive
identify differences. Based this, suspect many observed improvements
shown last column Table 5 deemed significant statistically nonetheless meaningful.
Figure 1 shows behavior learned classifiers adult, phone, covertype, letter-a data sets graphical form. figure natural distribution denoted X
tick mark associated error rate noted marker. error rate best
distribution underlined displayed corresponding data point (for four data
sets best distribution happens include 10% minority-class examples). Two curves
associated data sets (adult, phone) >90% confident best distribution performs better natural distribution, two curves (covertype,
letter-a) not. Note four curves perfectly unimodal. also clear near
distribution minimizes error rate, changes class distribution yield modest changes
error ratefar dramatic changes occur elsewhere. also evident data
sets Table 5. convenient property given common goal minimizing error rate.
property would far less evident correction described Section 3 performed, since classifiers induced class distributions deviating naturally occurring distribution would improperly biased.

328

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

25

adult

Error Rate (%)

20
17.25
16.85

15
12.62
12.32

phone

10

covertype
5.03

5

2.75 5.00
2.59

letter-a

0
0

10

20

30

40

50

60

70

80

% Minority Class
Figure 1: Effect Class Distribution Error Rate Select Data Sets

Finally, assess whether pruning would improved performance, consider second
column Table 5, displays error rates result using C4.5 pruning
natural distribution (recall Section 4.2 case C4.5s pruning strategy give unbiased results). +/x second column indicates C4.5 pruning
outperforms/underperforms C4.5 without pruning, learning natural distribution.
Note C4.5 pruning underperforms C4.5 without pruning 17 26 data sets,
leads us conclude C4.5 without pruning reasonable learner. Furthermore,
case C4.5 pruning generate classifier within optimal range C4.5 without
pruning also generate classifier within range.

6.3 Relationship Class Distribution AUC
performance induced classifiers, using AUC performance measure, displayed
Table 6. viewing results, recall AUC larger values indicate improved
performance. relative improvement classifier performance specified last
two columns, relative improvement performance calculated terms area
ROC curve (i.e., 1 AUC). use area ROC curve better
reflects relative improvementjust Table 5 relative improvement specified terms
change error rate instead change accuracy. before, relative improvements shown bold 90% confident reflect true improvement performance (i.e., t-test value fi
general, optimum ranges appear centered near, slightly right, balanced class distribution. 12 26 data sets optimum range include natural distribution (i.e., third column shaded). Note data sets,
exception solar-flare data set, class distributions within optimal range contain
minority-class examples natural class distribution. Based results conclude
even strongly AUC (i.e., cost-sensitive classification ranking) accu329

fiWeiss & Provost

racy appropriate simply choose natural class distribution training. Table 6
also shows that, unlike accuracy, balanced class distribution generally performs well,
although always perform optimally. particular, see 19 26 data
sets balanced distribution within optimal range. result surprising since
AUC, unlike error rate, unaffected class distribution test set, effectively factors classifier performance class distributions.
AUC using Specified Training Distribution
(training distribution expressed % minority)

Dataset
Nat-prune Nat

2

5

10

20

30

40

50

60

70

80

Relative %
Improv. (1-AUC)
90

95 best vs. nat best vs. bal

letter-a

.500 x

.772

.711 .799 .865 .891 .911 .938 .937 .944 .951 .954 .952 .940

79.8

pendigits

.962 x

.967

.892 .958 .971 .976 .978 .979 .979 .978 .977 .976 .966 .957

36.4

27.0
0.0

abalone

.590 x

.711

.572 .667 .710 .751 .771 .775 .776 .778 .768 .733 .694 .687

25.8

0.9

sick-euthyroid

.937 x

.940

.892 .908 .933 .943 .944 .949 .952 .951 .955 .945 .942 .921

25.0

6.3

connect-4

.658 x

.731

.664 .702 .724 .759 .763 .777 .783 .793 .793 .789 .772 .730

23.1

4.6

optdigits

.659 x

.803

.599 .653 .833 .900 .924 .943 .948 .959 .967 .965 .970 .965

84.8

42.3
20.0

covertype

.982 x

.984

.970 .980 .984 .984 .983 .982 .980 .978 .976 .973 .968 .960

0.0

solar-flare

.515 x

.627

.614 .611 .646 .627 .635 .636 .632 .650 .662 .652 .653 .623

9.4

8.2

phone

.850 x

.851

.843 .850 .852 .851 .850 .850 .849 .848 .848 .850 .853 .850

1.3

2.6

letter-vowel

.806 +

.793

.635 .673 .744 .799 .819 .842 .849 .861 .868 .868 .858 .833

36.2

12.6

contraceptive

.539 x

.611

.567 .613 .617 .616 .622 .640 .635 .635 .640 .641 .627 .613

7.7

1.6

adult

.853 +

.839

.816 .821 .829 .836 .842 .846 .851 .854 .858 .861 .861 .855

13.7

6.7

splice-junction

.932 +

.905

.814 .820 .852 .908 .915 .925 .936 .938 .944 .950 .944 .944

47.4

21.9

network2

.712 +

.708

.634 .696 .703 .708 .705 .704 .705 .702 .706 .710 .719 .683

3.8

4.7

yeast

.702 x

.705

.547 .588 .650 .696 .727 .714 .720 .723 .715 .699 .659 .621

10.9

2.5

network1

.707 +

.705

.626 .676 .697 .709 .709 .706 .702 .704 .708 .713 .709 .696

2.7

3.7

car

.931 +

.879

.754 .757 .787 .851 .884 .892 .916 .932 .931 .936 .930 .915

47.1

23.8

german

.660 +

.646

.573 .600 .632 .615 .635 .654 .645 .640 .650 .645 .643 .613

2.3

2.5

breast-wisc

.951 x

.958

.876 .916 .940 .958 .963 .968 .966 .963 .963 .964 .949 .948

23.8

5.9

blackjack

.682 x

.700

.593 .596 .628 .678 .688 .712 .713 .715 .700 .678 .604 .558

5.0

0.7
1.5

weather

.748 +

.736

.694 .715 .728 .737 .738 .740 .736 .730 .736 .722 .718 .702

1.5

bands

.604 x

.623

.522 .559 .564 .575 .599 .620 .618 .604 .601 .530 .526 .536

0.0

1.3

market1

.815 +

.811

.724 .767 .785 .801 .810 .808 .816 .817 .812 .805 .795 .781

3.2

0.5

crx

.889 +

.852

.804 .799 .805 .817 .834 .843 .853 .845 .857 .848 .853 .866

9.5

8.8

kr-vs-kp

.996 x

.997

.937 .970 .991 .994 .997 .998 .998 .998 .997 .994 .988 .982

33.3

0.0

move

.762 +

.734

.574 .606 .632 .671 .698 .726 .735 .738 .742 .736 .711 .672

3.0

2.6

Table 6: Effect Training Set Class Distribution AUC
look results pruning, see 15 26 data sets C4.5 pruning
underperforms C4.5 without pruning. Thus, respect AUC, C4.5 without pruning
reasonable learner. However, note car data set natural distribution pruning
falls optimum range, whereas without pruning not.
Figure 2 shows class distribution affects AUC adult, covertype, letter-a data
sets (the phone data set displayed Figure 1 would obscure adult
data set). Again, natural distribution denoted X tick mark. AUC best
distribution underlined displayed corresponding data point. case also
see near optimal class distribution AUC curves tend flatter, hence less sensitive changes class distribution.
330

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

1.0

covertype

.984
.984

.984
.954

letter-a
AUC

0.9
.839

.861

.861

80

90

adult

0.8
.772

0.7
0

10

20

30

40

50

60

70

% Minority Class
Figure 2: Effect Class Distribution AUC Select Data Sets
Figure 3 shows several ROC curves associated letter-vowel data set. curves
generated single run C4.5 (which AUC values exactly match
values Table 6). ROC space, point (0,0) corresponds strategy never making
positive/minority prediction point (1,1) always predicting positive/minority class.
Points northwest indicate improved performance.
1.0

50% Minority
True Positive Rate

0.8

0.6

90% Minority

19% Minority (Natural)
% Minority
2%
19%
50%
90%

0.4

0.2

2% Minority

AUC
.599
.795
.862
.855

0.0
0.0

0.2

0.4

0.6

0.8

False Positive Rate
Figure 3: ROC Curves Letter-Vowel Data set
331

1.0

fiWeiss & Provost

Observe different training distributions perform better different areas ROC space.
Specifically note classifier trained 90% minority-class examples performs substantially better classifier trained natural distribution high true-positive rates
classifier training 2% minority-class examples performs fairly well low truepositive rates. Why? small sample minority-class examples (2%) classifier
identify minority-labeled rules high confidence. However, much larger
sample minority-class examples (90%) identify many minority-labeled rules.
However, data set balanced distribution largest AUC performs best overall.
Note curve generated using balanced class distribution almost always outperforms
curve associated natural distribution (for low false-positive rates natural distribution performs slightly better).

7. Forming Good Class Distribution Sensitivity Procurement Costs
results previous section demonstrate marginal class distributions yield
classifiers perform substantially better classifiers produced training distributions. Unfortunately, order determine best class distribution training, forming
thirteen training sets size n, different class distribution, requires nearly 2n examples. costly obtain training examples form suitable learning, approach self-defeating. Ideally, given budget allows n training examples, one would
select total n training examples would used final training setand
associated class distribution would yield classifiers perform better generated
class distribution (given n training examples). section describe evaluate
heuristic, budget-sensitive, progressive sampling algorithm approximates ideal.
order evaluate progressive sampling algorithm, necessary measure class
distribution affects classifier performance variety different training-set sizes.
measurements summarized Section 7.1 (the detailed results included Appendix B).
algorithm selecting training data described Section 7.2 performance
evaluated Section 7.3, using measurements included Appendix B.
7.1

Effect Class Distribution Training-Set Size Classifier Performance

Experiments run establish relationship class distribution, training-set size
classifier performance. order ensure training sets contain sufficient number
training examples provide meaningful results training-set size dramatically reduced, data sets yield relatively large training sets used (this determined based
size data set fraction minority-class examples data set). Based
criterion, following seven data sets selected analysis: phone, adult, covertype,
blackjack, kr-vs-kp, letter-a, weather. detailed results associated experiments
contained Appendix B.
results one data sets, adult data set, shown graphically Figure 4
Figure 5, show classifier performance using error rate AUC, respectively.
nine performance curves figures associated specific training-set size,
contains 1/128 training data available learning (using methodology
described Section 4.1). performance curves always improve increasing dataset size, curves corresponding smallest largest training-set sizes explicitly
labeled.
332

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1

Error Rate

35

30

Natural

1/128

25

20
1 (all available training data)
15
0

10

20

30

40

50

60

70

80

% Minority Examples Training Set
Figure 4: Effect Class Distribution Training-set Size Error Rate (Adult Data Set)

0.9
1 (all available training data)

AUC

0.8

0.7
1/128
0.6
Natural
0.5
0

10

20

30

40

50

60

70

80

90

100

% Minority Class Examples Training Set
Figure 5: Effect Class Distribution Training-set Size AUC (Adult Data Set)
333

fiWeiss & Provost

Figure 4 Figure 5 show several important things. First, change training-set size
shifts performance curves, relative rank point performance curve remains
roughly same. Thus, class distribution yields best performance occasionally varies training-set size, variations relatively rare occur,
small. example, Figure 5 (and supporting details Appendix B) indicates
adult data set, class distribution yields best AUC typically contains 80% minorityclass examples, although occasionally small deviation (with 1/8 training data 70%
minority-class examples best). gives support notion may best
marginal class distribution learning task suggests progressive sampling algorithm
may useful locating class distribution yields best, nearly best, classifier performance.
results also indicate that, fixed class distribution, increasing size training set always leads improved classifier performance. Also note performance curves
tend flatten size data set grows, indicating choice class distribution may become less important training-set size grows. Nonetheless, even
available training data used, choice class distribution make difference.
significant plateau reached (i.e., learning stopped), would
possible reduce size training set without degrading classifier performance.
case would necessary select class distribution training data carefully.
results Figure 4 Figure 5 also show carefully selecting class distribution, one sometimes achieve improved performance using fewer training examples.
see this, consider dashed horizontal line Figure 4, intersects curve associated
training data lowest error rate, class distribution includes 10% minority-class examples. horizontal line curve associated available
training data, training set data outperforms full training set.
case see training data 10% class distribution outperforms natural class
distribution using available training data. two horizontal lines Figure 5 highlight
cases one achieve improved AUC using fewer training data (because
larger AUC values indicate improved performance, compare horizontal lines curves
lie them). example, Figure 5 shows training set class distribution
contains 80% minority-class examples 1/128th total training data outperforms
training set twice training data class distribution contains less equal
40% minority-class examples (and outperforms training set four times data class
distribution contains less equal 10% minority-class examples). results Appendix B show trends noted adult data set hold data sets
one often achieve improved performance using less training data.
7.2

Budget-Sensitive Progressive sampling Algorithm Selecting Training Data

discussed above, size training set sometimes must limited due costs associated
procuring usable training examples. simplicity, assume budget n,
permits one procure exactly n training examples. assume number training
examples potentially procured sufficiently large training set size n
formed desired marginal class distribution. would like sampling strategy
selects x minority-class examples majority-class examples, x + = n,

334

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

resulting class distribution yields best possible classification performance training set
size n.
sampling strategy relies several assumptions. First, assume cost executing learning algorithm negligible compared cost procuring examples,
learning algorithm may run multiple times. certainly true training data
costly. assume cost procuring examples class
hence budget n represents number examples procured well total
cost. assumption hold many, all, domains. example, phone data
set described Section 1 cost procuring business consumer examples equal,
telephone fraud domain cost procuring fraudulent examples may substantially higher cost procuring non-fraudulent examples. algorithm described
section extended handle non-uniform procurement costs.
sampling algorithm selects minority-class majority-class training examples
resulting class distribution yield classifiers tend perform well. algorithm
begins small amount training data progressively adds training examples using
geometric sampling schedule (Provost, Jensen & Oates, 1999). proportion minority-class
examples majority-class examples added iteration algorithm determined
empirically forming several class distributions currently available training data,
evaluating classification performance resulting classifiers, determining
class distribution performs best. algorithm implements beam-search space
possible class distributions, beam narrows budget exhausted.
say sampling algorithm budget-efficient examples selected iteration algorithm used final training set, heuristically determined
class distribution. key constrain search space class distributions
budget-efficiency either guaranteed, likely. show, algorithm described section guaranteed budget-efficient. Note, however, class distribution final training set, heuristically determined, guaranteed best
class distribution; however, show, performs well practice.
algorithm outlined Table 7, using pseudo-code, followed line-by-line explanation (a complete example provided Appendix C). algorithm takes three user-specified
LQSXWSDUDPHWHUV WKHJHRPHWULFIDFWRUXVHGWRGHWHUPLQHWKHUDWHDWZKLFKWKHWUDLQLQJ-set size
grows; n, budget; cmin, minimum fraction minority-class examples majorityclass examples assumed appear final training set order budgetefficiency guarantee hold.5)RUWKHUHVXOWVSUHVHQWHGLQWKLVVHFWLRQ LVVHWWRVRWKDWWKH
training-set size doubles every iteration algorithm, cmin set 1/32.
algorithm begins initializing values minority majority variables,
represent total number minority-class examples majority-class examples requested
algorithm. Then, line 2, number iterations algorithm determined,
initial training-set size, subsequently set line 5, cmin n.
allow possible class distributions formed using cmin minority-class examples cmin majority-FODVVH[DPSOHV)RUH[DPSOHJLYHQWKDW LVDQGcmin 1/32, line 2
variable K set 5 line 5 initial training-set size set 1/32 n.
5. Consider degenerate case algorithm determines best class distribution contains minorityclass examples majority-class examples. algorithm begins even single example class,
budget-efficient.

335

fiWeiss & Provost

1. minority = majority = 0;


2.

K = log



1


c min



# current number minority/majority examples hand

;

# number iterations K+1

3. (j = 0; j .M Mfi
4. {
5.
size = nff fiK-j
6.
7.
8.
9.
10.
11.
12.

# iteration (e.g., j = 0, 1,2,3,4,5)
# set training-set size iteration j

(j = 0)
beam_bottom = 0; beam_top = 1;
# initialize beam first iteration
else (j = K)
beam_bottom = best; beam_top = best; # last iteration evaluate previous best
else
min(best,1 best )
beam_radius =
+1 1
beam_bottom = best beam_radius; beam_top = best + beam_radius;

13.
14.

min_needed = size beam_top;
# number minority examples needed
maj_needed = size (1.0 beam_bottom); # number majority examples needed

15.
16.
17.
18.

(min_needed > minority)
request (min_needed - minority) additional minority-class examples;
(maj_needed > majority)
request (maj_needed - majority) additional majority-class examples;

19.
20.

evaluate(beam_bottom, beam_top, size); # evaluate distributions beam; set best
}
Table 7: Algorithm Selecting Training Data

Next, lines 6-12, algorithm determines class distributions considered
iteration setting boundaries beam. first iteration, class distributions
considered (i.e., fraction minority-class examples training set may vary 0
1) last iteration, best-performing class distribution previous iteration evaluated. iterations, beam centered class distribution
performed best previous iteration radius beam set (in line 11)
ratio beam_top/beam_bottom wiOOHTXDO )RUH[DPSOHLI LVDQGbest .15,
beam_radius .05 beam span .10 .20which difIHUE\DIDFWRURIffLH fi
lines 13 14 algorithm computes number minority-class examples majority-class examples needed form class distributions fall within beam. values
determined class distributions boundaries beam. lines 15-18 additional examples requested, required. line 19 evaluation procedure called form
336

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

class distributions within beam induce evaluate classifiers.
minimum procedure evaluate class distributions endpoints midpoint
beam; however, procedure may implemented evaluate additional class distributions within beam. procedure set variable best class distribution performs best. best performance achieved several class distributions, resolution
procedure needed. example, class distribution surrounding class distributions perform best may chosen; still yield unique value, bestperforming class distribution closest center beam may chosen. event,
last iteration, one class distribution evaluatedthe previous best. ensure budgetefficiency, one class distribution evaluated final iteration.
algorithm guaranteed request examples subsequently used final
training set, heuristically determined class distribution. guarantee
verified inductively. First, base case. calculation K line 2 ensures initial training set contain cmin n training examples. Since assume final training
set least cmin minority-class examples cmin majority-class examples, examples used form initial training set guaranteed included final training set.
Note cmin may set arbitrarily smallthe smaller cmin larger K smaller
size initial training set.
inductive step based observation radius beam line 11
sHWVRWKDWWKHEHDPVSDQVDWPRVWDIDFWRURI DOOH[DPSOHVUHTXHVWHGLQHDFKLWHUDWLRQDUHJXDranteed used final training set. see this, work backward final
iteration, rather working forward case inductive proofs. Assume
result algorithm fraction minority-class examples final training set p,
p n minority-class examples final training set. means p
best distribution previous iteration. Since p must fall somewhere within beam
previous iteration beam must span factor ZHFDQVD\WKHIROORZLQJWKHIUDFWLRQ
minority-class examples previous iteration could range p ffLIp top
WKH SUHYLRXV EHDPfi WR p (if p bottom previous beam). Since previous
iteration contains n/ H[DPSOHVGXHWRWKHJHRPHWULFVDPSOLQJVFKHPHWKHQWKHSUHYLRXVLWHUaWLRQKDVDWPRVWff p) n RUp n, minority-class examples. Thus, possible cases
minority-class examples previous iteration used final interaction.
argument applies similarly majority-class examples extended backwards
previous iterations.6 Thus, bound initial training-set size restriction
width beam exceed geometULFIDFWRU WKHDOJRULWKPJXDUDQWHHVWKDWDOO
examples requested execution algorithm used final training set.
complete, detailed, iteration-by-iteration example describing sampling algorithm
applied phone data set provided Appendix C, Table C1. example error rate
used evaluate classifier performance. description specifies class distributions
evaluated execution algorithm. trajectory graphically depicted
Figure 6, narrowing final class distribution. iteration, algorithm considers
beam class distributions bounded two curves.

6. exception first iteration algorithm, since situation beam unconditionally set
span class distributions. reason cmin value required provide efficiency guarantee.

337

fiWeiss & Provost

% Minority Class Examples

100

80

60

beam_top

40

10% (best)
beam_bottom

20

0
0

1

2

3

4

5

Iteration

Figure 6: Trajectory Algorithm Space Class Distributions.
7.3 Results Sampling Algorithm
budget-sensitive progressive sampling algorithm applied phone, adult, covertype,
kr-vs-kp, weather, letter-a blackjack data sets using error rate AUC measure classifier performance. However, method setting beam (described lines 6-12 Table 7)
modified results experiments described Section 7.1 (and detailed
Appendix B), evaluate 13 listed class distributions, could used. Specifically,
iteration low end (high end) beam set class distribution specified
Appendix B (above) best performing class distribution prior iteration. example, one iteration best performing class distribution contains 30% minority-class examples, next iteration bottom beam set include 20%
minority-class examples top beam include 40% minority-class examples (of
13 sampled class distributions, closest 30% class distribution). Although
someWLPHVDOORZWKHEHDPWRVSDQDUDQJHJUHDWHUWKDQ fffiLQSUDFWLFHWKLVGRHVQRWUHVXOW
problemfor seven data sets examples requested algorithm included
final training set. addition, slight improvement made algorithm. Specifically,
iteration, number examples already hand (procured previous iterations) sufficient evaluate additional class distributions, beam widened include additional class distributions (this happen first iteration beam set
wide).
performance sampling algorithm summarized Table 8, along performance two strategies selecting class distribution training data. first
two additional strategies, Pick Natural/Balanced Strategy, based guidelines
suggested empirical results Section 6. strategy selects natural distribution
error rate performance metric balanced class distribution AUC
338

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

performance metric. Pick Best strategy selects class distribution performs best
13 evaluated class distributions (see Tables 5 6). Given consider
13 class distributions, strategy always yield best results but, shall see,
costly strategies. value representing best, budget-efficient performance
(lowest error rate, highest AUC) underlined data set. detailed iteration-by-iteration
description algorithm, seven data sets, provided Appendix C, Table C3.
Table 8 also specifies cost strategy, based number training examples requested algorithm. cost expressed respect budget n (each strategy yields
final training set n examples). Pick Natural/Balanced strategy always requires exactly n examples selected therefore cost n budget-efficient. Pick
Best strategy total cost 1.93n hence budget-efficient (because evaluates
class distributions 2% 95% minority-class examples, requires .95n minorityclass examples .98n majority-class examples). cost sampling algorithm depends
performance induced classifiers: changes algorithm described
section, longer guaranteed budget-efficient. Nonetheless, casesfor
error rate AUCthe sampling algorithm cost exactly n hence turns
budget-efficient.

Data Set

phone
adult
covertype
kr-vs-kp
weather
letter-a
blackjack

Sampling Algorithm Pick Natural/Balanced
ER
AUC Cost
ER
AUC Cost

12.3%
17.1%
5.0%
1.2%
33.1%
2.8%
28.4%

.851
.861
.984
.998
.740
.954
.715

n
n
n
n
n
n
n

12.6%
17.3%
5.0%
1.4%
33.7%
2.8%
28.4%

.849
.851
.980
.998
.736
.937
.713

n
n
n
n
n
n
n

ER

Pick Best
AUC Cost

12.3%
16.9%
5.0%
1.2%
33.1%
2.6%
28.4%

.853
.861
.984
.998
.740
.954
.715

1.93n
1.93n
1.93n
1.93n
1.93n
1.93n
1.93n

Table 8: Comparative Performance Sampling Algorithm
results Table 8 show using budget-sensitive progressive sampling algorithm choose training data possible achieve results good better
strategy always using natural distribution error rate balanced distribution
AUCwithout requiring extra examples procured. particular, comparing two strategies, progressive sampling strategy win-tie-loss record 10-4-0.
cases wins lead large improvements performance, cases
(e.g., kr-vs-kp data set sampling strategy yields relative reduction error
rate 17%). results Table 8 also show sampling algorithm performs nearly
well Pick Best strategy (it performs well 11 14 cases), almost twice
costly. progressive sampling strategy performs nearly well Pick Best
strategy, conclude progressive sampling strategy substantially outperform Pick Natural/Balanced strategy, sampling strategy cannot identify good (i.e., near-optimal) class distribution learning, rather optimal class
distribution happens near natural (balanced) distribution error rate (AUC). Note
data sets (optdigits, contraceptive, solar-flare, car)
case hence Pick Natural/Balanced strategy perform poorly. Unfortunately, data sets would yield relatively small training sets, progressive sampling algorithm could run them.
339

fiWeiss & Provost

summary, sampling algorithm introduced section leads near-optimal results
results outperform straw-man strategy using natural distribution minimize error
rate balanced distribution maximize AUC. Based results, budgetsensitive progressive sampling algorithm attractiveit incurs minimum possible cost
terms procuring examples permitting class distribution training selected
using intelligence.

8. Related Work
Several researchers considered question class distribution use fixed
training-set size, and/or, generally, class distribution affects classifier performance.
Catlett (1991) Chan & Stolfo (1998) study relationship (marginal) training
class distribution classifier performance training-set size held fixed, focus
attention issues. studies also analyze data sets,
makes impossible draw general conclusions relationship class distribution
classifier performance. Nonetheless, based results three data sets, Chan & Stolfo
(1998) show accuracy performance metric, training set uses natural
class distribution yields best results. results agree partially resultsalthough
show natural distribution always maximize accuracy, show optimal distribution generally close natural distribution. Chan & Stolfo also show
actual costs factored (i.e., cost false positive false negative), natural distribution perform best; rather training distribution closer balanced distribution performs best. also observe, did, increasing percentage
minority-class examples training set, induced classifier performs better classifying
minority examples. important note, however, neither Chan & Stolfo Catlett adjusted induced classifiers compensate changes made class distribution
training set. means results biased favor natural distribution (when
measuring classification accuracy) could improve classification performance
minority class examples simply changing (implicitly) decision threshold. results
Appendix show, compensating changed class distribution affect performance
classifier significantly.
Several researchers looked general question reduce need labeled
training data selecting data intelligently, without explicitly considering class distribution. example, Cohn et al. (1994) Lewis Catlett (1994) use active learning
add examples training set classifier least certain classification.
Saar-Tsechansky Provost (2001, 2003) provide overview methods also extend
cover AUC non-accuracy based performance metrics. setting
methods applicable different setting consider. particular, methods
assume either arbitrary examples labeled descriptions pool unlabeled examples available critical cost associated labeling (so algorithms select examples intelligently rather randomly). typical setting, cost
procuring descriptions examplesthe labels known beforehand.
also prior work progressive sampling strategies. John Langley
(1996) show one use extrapolation learning curves determine classifier
performance using subset available training data comes close performance would
achieved using full data set. Provost et al. (1999) suggest using geometric sampling
340

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

schedule show often efficient using available training data.
techniques described John Langley (1996) Provost et al. (1999) change
distribution examples training set, rather rely taking random samples
available training data. progressive sampling routine extends methods stratifying
sampling class, using information acquired process select good
final class distribution.
considerable amount research build good classifiers class
distribution data highly unbalanced costly misclassify minority-class examples (Japkowicz et al., 2000). research related work frequent approach
learning highly skewed data sets modify class distribution training set.
conditions, classifiers optimize accuracy especially inappropriate tend generate trivial models almost always predict majority class. common approach dealing highly unbalanced data sets reduce amount class
imbalance training set. tends produce classifiers perform better minority class original distribution used. Note situation training-set size
fixed motivation changing distribution simply produce better classifiernot reduce, minimize, training-set size.
two basic methods reducing class imbalance training data under-sampling
over-sampling. Under-sampling eliminates examples majority class over-sampling
replicates examples minority class (Breiman, et al., 1984; Kubat & Matwin, 1997; Japkowicz & Stephen, 2001). Neither approach consistently outperforms specific under-sampling over-sampling rate consistently yield best results. Estabrooks
Japkowicz (2001) address issue showing mixture-of-experts approach, combines classifiers built using under-sampling over-sampling methods various sampling
rates, produce consistently good results.
under-sampling over-sampling known drawbacks. Under-sampling throws
potentially useful data over-sampling increases size training set hence
time build classifier. Furthermore, since over-sampling methods make exact copies
minority class examples, overfitting likely occurclassification rules may induced
cover single replicated example.7 Recent research focused improving basic methods. Kubat Matwin (1997) employ under-sampling strategy intelligently removes
majority examples removing majority examples redundant border minority examplesfiguring may result noise. Chawla et al. (2000) combine under-sampling over-sampling methods, and, avoid overfitting problem, form new
minority class examples interpolating minority-class examples lie close together.
Chan Stolfo (1998) take somewhat different, innovative, approach. first run preliminary experiments determine best class distribution learning generate multiple training sets class distribution. typically accomplished including
minority-class examples majority-class examples training set.
apply learning algorithm training set combine generated classifiers form
composite learner. method ensures available training data used, since
majority-class example found least one training sets.

7. especially true methods C4.5, stops splitting based counting examples leaves
tree.

341

fiWeiss & Provost

research article could properly viewed research under-sampling
effect classifier performance. However, given perspective, research performs undersampling order reduce training-set size, whereas research relating skewed data
sets primary motivation improve classifier performance. example, Kubat Matwin (1997) motivate use under-sampling handle skewed data sets saying adding
examples majority class training set detrimental effect learners
behavior: noisy otherwise unreliable examples majority class overwhelm
minority class (p. 179). consequence different motivations experiments
under-sample minority and/or majority classes, research concerned
learning skewed distributions majority class under-sampled.
use under-sampling reducing training-set size (and thereby reducing cost) may
practically useful perspective. Reducing class imbalance training set effectively causes learner impose greater cost misclassifying minority-class examples
(Breiman et al., 1984). Thus, cost acquiring learning data
issue, cost-sensitive probabilistic learning methods direct arguably appropriate way dealing class imbalance, problems, noted
earlier, associated under-sampling over-sampling. approaches
shown outperform under-sampling over-sampling (Japkowicz & Stephen, 2002). quote
Drummond Holte (2000) data available used produce tree, thus
throwing away information, learning speed degraded due duplicate instances (p.
239).

9. Limitations Future Research
One limitation research described article results based
use decision-tree learner, conclusions may hold class learners. However, reasons believe conclusions hold learners well.
Namely, since role class distribution plays learningand reasons, discussed
Section 5.2, classifier perform worse minority classare specific
decision-tree learners, one would expect learners behave similarly. One class learners
may especially warrant attention, however, learners form disjunctive concepts. learners suffer way problem small
disjuncts, results indicate partially responsible minority-class predictions higher error rate majority-class predictions.8 Thus, would informative extend
study include classes learners, determine results indeed generalize.
program inducing decision trees used throughout article, C4.5, considers
class distribution training data generating decision tree. differences
class distribution training data test data accounted post-processing
step re-computing probability estimates leaves using estimates re-label
tree. induction program knowledge target (i.e., test) distribution
tree-building process, different decision tree might constructed. However, research
indicates serious limitation. particular, Drummond Holte (2000) showed
splitting criteria completely insensitive class distribution
8. However, many learners form disjunctive concepts something quite close. example, Van den Bosch et al.
(1997) showed instance-based learners viewed forming disjunctive concepts.

342

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

splitting criteria perform well better methods factor class distribution.
showed C4.5s splitting criterion relatively insensitive class distributionand therefore changes class distribution.
employed C4.5 without pruning study pruning sensitive class distribution C4.5s pruning strategy take changes made class distribution
training data account. justify choice showed C4.5 without pruning performs
competitively C4.5 pruning (Sections 6.2 6.3). Moreover, research (Bradford et al., 1998) indicates classifier performance generally improve pruning
takes class distribution costs account. Nevertheless would worthwhile see
cost/distribution-sensitive pruning strategy would affect results. know
published pruning method attempts maximize AUC.
article introduced budget-sensitive algorithm selecting training data
costly obtain usable training examples. would interesting consider case
costly procure examples belonging one class another.

10. Conclusion
article analyze, fixed training-set size, relationship class distribution training data classifier performance respect accuracy AUC. analysis
useful applications data procurement costly data procured independently class, costs associated learning training data sufficient
require size training set reduced. results indicate accuracy
performance measure, best class distribution learning tends near natural class
distribution, AUC performance measure, best class distribution learning
tends near balanced class distribution. general guidelines
guidelinesand particular data set different class distribution may lead substantial
improvements classifier performance. Nonetheless, additional information provided
class distribution must chosen without experimentation, results show
accuracy AUC maximization, natural distribution balanced distribution (respectively) reasonable default training distributions.
possible interleave data procurement learning, show budget-sensitive
progressive sampling strategy improve upon default strategy using natural distribution maximize accuracy balanced distribution maximize area ROC
curvein experiments budget-sensitive sampling strategy never worse. Furthermore,
experiments sampling strategy performs nearly well strategy evaluates
many different class distributions chooses best-performing one (which optimal terms
classification performance inefficient terms number examples required).
results presented article also indicate many data sets class distribution
yields best-performing classifiers remains relatively constant different training-set
sizes, supporting notion often best marginal class distribution. results
show amount training data increases differences performance
different class distributions lessen (for error rate AUC), indicating data
becomes available, choice marginal class distribution becomes less less important
especially neighborhood optimal distribution.
article also provides comprehensive understanding class distribution affects learning suggests answers fundamental questions, classifiers almost always perform worse classifying minority-class examples. method adjusting
343

fiWeiss & Provost

classifier compensate changes made class distribution training set described
adjustment shown substantially improve classifier accuracy (see Appendix A).
consider particularly significant previous research effect class distribution learning employed this, other, adjustment (Catlett, 1991; Chan & Stolfo,
1998; Japkowicz & Stephen, 2002).
Practitioners often make changes class distribution training data, especially
classes highly unbalanced. changes seldom done principled manner
reasons changing distributionand consequencesare often fully understood.
hope article helps researchers practitioners better understand relationship class distribution classifier performance permits learn effectively
need limit amount training data.

Acknowledgments
would like thank Haym Hirsh comments feedback provided throughout
research. would also like thank anonymous reviewers helpful comments, IBM
Faculty Partnership Award.

Appendix A: Impact Class Distribution Correction Classifier Performance
Table A1 compares performance decision trees labeled using uncorrected frequencybased estimate (FB) labeled using corrected frequency-based estimate (CT-FB).
Dataset
letter-a
pendigits
abalone
sick-euthyroid
connect-4
optdigits
covertype
solar-flare
phone
letter-vowel
contraceptive
adult
splice-junction
network2
yeast
network1
car
german
breast-wisc
blackjack
weather
bands
market1
crx
kr-vs-kp
move
Average

Error Rate
FB
CT-FB
9.79
5.38
4.09
4.02
30.45
22.97
9.82
6.85
30.21
27.57
6.17
3.41
6.62
6.46
36.20
29.12
17.85
14.81
18.89
14.16
40.77
39.65
22.69
20.05
9.02
8.74
30.80
29.96
34.01
28.80
31.99
30.99
8.26
7.92
38.37
37.09
6.76
6.74
33.02
28.71
34.62
34.61
32.68
32.68
25.77
25.77
20.84
21.48
1.22
1.22
28.24
28.24
21.89
19.90

% Rel.
Improv.
45.0
1.7
24.6
30.2
8.7
44.7
2.4
19.6
17.0
25.0
2.7
11.6
3.1
2.7
15.3
3.1
4.1
3.3
0.3
13.1
0.0
0.0
0.0
-3.1
0.0
0.0
10.6

% Labels % Errors Min.
Changed
FB
CT-FB
39.0
2.7
7.2
3.2
5.6
7.8
5.6
8.5
19.1
6.7
8.8
14.6
14.7
8.5
10.4
42.5
6.0
21.2
2.4
7.0
8.5
20.4
19.3
30.7
3.2
25.2
44.4
44.1
15.9
30.2
11.1
20.6
27.6
30.7
19.6
36.8
14.1
20.1
28.4
1.2
32.9
40.1
4.6
29.4
47.0
1.3
32.9
38.2
5.3
25.9
33.8
16.1
30.8
35.8
0.4
38.5
38.7
17.1
42.9
76.2
0.0
40.5
40.5
0.6
90.2
90.2
23.9
46.0
48.6
17.2
46.2
51.4
0.2
58.5
58.5
20.8
52.6
60.7
13.3
28.3
36.4

Table A1: Impact Probability Estimates Error Rate
344

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

results main body article based use corrected frequencybased estimate label leaves induced decision trees, decision trees
improperly biased changes made class distribution training set. Thus,
comparison Table A1 evaluates significance correcting changes class distribution training data. comparison based situation class distribution
training set altered contain equal number minority-class majority-class examples (the test set still contain natural class distribution). results based 30
runs data sets listed order decreasing class imbalance.
error rate estimates displayed second third columns Table A1, and,
data set, lowest error rate underlined. fourth column specifies relative
improvement results using corrected frequency-based estimate. fifth column
specifies percentage leaves decision tree assigned different class label
corrected estimate used. last two columns specify, estimate, percentage total errors contributed minority-class test examples.
Table A1 shows employing corrected frequency-based estimate instead uncorrected frequency-based estimate, is, average, relative 10.6% reduction error rate.
Furthermore, one case uncorrected frequency-based estimate outperform
corrected frequency-based estimate. correction tends yield larger reduction
highly unbalanced data setsin cases plays larger role. restrict
first 13 data sets listed Table 2, minority class makes less 25%
examples, relative improvement data sets 18.2%. Note
scenario minority class over-sampled training set, corrected frequency-based
estimate cause minority-labeled leaves labeled majority-class. Consequently, last column table demonstrates, corrected version estimate
cause errors come minority-class test examples.

Appendix B: Effect Training-Set Size Class Distribution Learning
Experiments run establish joint impact class distribution training-set size
classifier performance. Classifier performance reported thirteen class
distributions analyzed Section 6 nine different training set sizes. nine
training set sizes generated omitting portion available training data (recall that,
described Section 4.1, amount available training data equals number minority-class examples). experiments training set sizes varied contain
following fractions total available training data: 1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 3/4
1. order ensure training sets contain sufficient number training examples
provide meaningful results, original data set must relatively large and/or contain high
proportion minority-class examples. reason, following seven data sets
selected analysis: phone, adult, covertype, kr-vs-kp, weather, letter-a blackjack.
last four data sets list yield smaller number training examples first three,
data sets two smallest training-set sizes (1/128 1/64) evaluated.
experimental results summarized Tables B1a B1b. asterisk used denote
natural class distribution data set and, training-set size, class distribution
yields best performance displayed bold underlined.

345

fiWeiss & Provost

Data Set
PHONE
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Size Metric
2
5
10
18.2*
20
30
40
50
60
1/128
.641 .737 .784 .793 .792 .791 .791 .789 .788
1/64
.707 .777 .784 .803 .803 .803 .801 .802 .801
1/32
.762 .794 .809 .812 .812 .811 .811 .811 .810
1/16
.784 .813 .816 .823 .823 .824 .818 .821 .822
1/8
AUC .801 .823 .828 .830 .830 .830 .830 .829 .830
1/4
.819 .835 .837 .839 .839 .837 .837 .836 .836
1/2
.832 .843 .846 .846 .845 .845 .843 .843 .843
3/4
.838 .847 .849 .849 .849 .848 .846 .847 .846
1
.843 .850 .852 .851 .851 .850 .850 .849 .848
1/128
17.47 16.42 15.71 16.10 16.25 17.52 18.81 21.21 22.87
1/64
17.01 15.75 15.21 15.12 15.20 16.39 17.59 19.60 22.11
1/32
16.22 15.02 14.52 14.50 14.75 15.41 16.81 18.12 20.02
1/16 Error 15.78 14.59 14.01 14.02 14.18 14.70 16.09 17.50 18.68
1/8
Rate 15.17 14.08 13.46 13.61 13.71 14.27 15.30 16.51 17.66
1/4
14.44 13.55 13.12 13.23 13.27 13.85 14.78 15.85 17.09
1/2
13.84 13.18 12.81 12.83 12.95 13.47 14.38 15.30 16.43
3/4
13.75 13.03 12.60 12.70 12.74 13.35 14.12 15.01 16.17
1
13.45 12.87 12.32 12.62 12.68 13.25 13.94 14.81 15.97

ADULT
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1
1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1

COVERTYPE
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1
1/128
1/64
1/32
1/16
1/8
1/4
1/2
3/4
1

AUC

Error
Rate

AUC

Error
Rate

2
.571
.621
.638
.690
.735
.774
.795
.811
.816
23.80
23.32
22.95
22.66
21.65
20.56
19.51
18.82
18.47

5
.586
.630
.674
.721
.753
.779
.803
.814
.821
23.64
22.68
22.09
21.34
20.15
19.08
18.10
17.70
17.26

10
.633
.657
.711
.733
.768
.793
.812
.823
.829
23.10
22.21
21.12
20.29
19.13
18.20
17.54
17.17
16.85

20
.674
.702
.735
.760
.785
.804
.822
.830
.836
23.44
21.77
20.77
19.90
18.87
18.42
17.54
17.32
17.09

23.9*
.680
.714
.742
.762
.787
.809
.825
.833
.839
23.68
21.80
20.97
20.07
19.30
18.70
17.85
17.46
17.25

70
.786
.798
.812
.821
.829
.836
.843
.847
.848
26.40
24.80
21.77
20.70
19.66
18.94
17.88
17.33
17.32

80
.785
.799
.811
.822
.831
.838
.844
.848
.850
30.43
27.34
24.86
22.46
21.26
20.43
19.57
18.82
18.73

90
.774
.788
.805
.817
.828
.836
.846
.851
.853
33.26
30.21
25.31
24.15
23.23
22.28
21.68
20.43
20.24

95
.731
.744
.778
.805
.818
.832
.844
.848
.850
37.27
26.86
28.74
24.52
23.33
22.90
21.68
21.24
21.07

30
.694
.711
.751
.778
.793
.813
.829
.837
.842
23.90
23.08
21.11
20.37
19.67
19.12
18.39
18.07
17.78

40
.701
.722
.755
.787
.799
.820
.834
.843
.846
25.22
24.38
22.37
21.43
20.86
20.10
19.38
18.96
18.85

50
.704
.732
.766
.791
.809
.827
.838
.845
.851
26.94
26.29
24.41
23.18
22.33
21.39
20.83
20.40
20.05

60
.723
.739
.762
.794
.812
.831
.841
.849
.854
29.50
28.07
27.08
25.27
24.56
23.48
22.81
22.13
21.79

70
.727
.746
.765
.787
.816
.832
.847
.853
.858
33.08
31.45
30.27
28.67
27.14
25.78
24.88
24.32
24.08

80
.728
.755
.772
.785
.813
.834
.849
.856
.861
37.85
36.41
34.04
33.41
31.06
29.54
28.15
27.59
27.11

90
.722
.752
.766
.780
.803
.824
.847
.855
.861
46.13
43.64
42.40
40.65
38.35
36.17
34.71
33.92
33.00

95
.708
.732
.759
.771
.797
.811
.834
.848
.855
48.34
47.52
47.20
46.68
45.83
43.93
41.24
40.47
39.75

2
5
10
14.8*
20
30
.767 .852 .898 .909 .916 .913
.836 .900 .924 .932 .937 .935
.886 .925 .942 .947 .950 .947
.920 .944 .953 .957 .959 .959
.941 .955 .963 .965 .967 .968
.953 .965 .970 .973 .975 .976
.963 .972 .979 .981 .981 .980
.968 .976 .982 .982 .983 .982
.970 .980 .984 .984 .984 .983
10.44 10.56 10.96 11.86 13.50 16.16
9.67 9.29 10.23 11.04 12.29 14.55
8.87 8.66 9.44 10.35 11.29 13.59
8.19 7.92 8.93 9.67 10.37 11.93
7.59 7.32 7.87 8.65 9.26 10.31
6.87 6.44 7.04 7.49 8.01 9.05
6.04 5.71 5.97 6.45 6.66 7.14
5.81 5.31 5.48 5.75 5.87 6.25
5.54 5.04 5.00 5.03 5.26 5.64

40
.916
.936
.948
.959
.969
.975
.978
.980
.982
18.26
16.52
15.34
13.51
11.63
9.86
7.53
6.57
5.95

50
.916
.932
.948
.957
.968
.973
.977
.979
.980
20.50
18.58
17.30
15.35
13.06
10.56
8.03
6.89
6.46

60
.909
.928
.944
.955
.967
.972
.975
.976
.978
23.44
21.40
19.31
17.42
14.68
11.45
8.80
7.58
7.23

70
.901
.922
.939
.951
.963
.970
.972
.975
.976
26.95
24.78
21.82
19.40
16.39
12.28
9.94
8.72
8.50

80
.882
.913
.930
.945
.957
.965
.970
.971
.973
31.39
27.65
24.86
22.30
18.28
14.36
11.44
10.69
10.18

90
.854
.885
.908
.929
.948
.956
.961
.966
.968
37.92
34.12
28.37
25.74
22.50
18.05
14.85
13.92
13.03

95
.817
.851
.876
.906
.929
.943
.953
.958
.960
44.54
41.67
33.91
28.36
26.87
22.59
18.37
16.29
16.27

Table B1a: Effect Training-Set Size Class Distribution Classifier Performance

346

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

Data Set
KR-VS-KP
|
|
|
|
|
|
|
|
|
|
|
|
|
WEATHER
|
|
|
|
|
|
|
|
|
|
|
|
|

Size
1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1

LETTER-A
|
|
|
|
|
|
|
|
|
|
|
|
|

1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1

BLACKJACK
|
|
|
|
|
|
|
|
|
|
|
|
|

1/32
1/16
1/8
1/4
1/2
3/4
1
1/32
1/16
1/8
1/4
1/2
3/4
1

Metric

AUC

Error
Rate

AUC

Error
Rate

AUC

Error
Rate

AUC

Error
Rate

2
.567
.618
.647
.768
.886
.922
.937
42.61
37.99
35.16
26.33
17.11
13.37
12.18

5
.637
.681
.809
.888
.946
.966
.970
36.35
33.02
22.73
15.74
11.07
7.49
6.50

10
20
30
40
47.8*
50
60
70
80
90
.680 .742 .803 .852 .894 .894 .897 .854 .797 .695
.800 .888 .920 .942 .951 .952 .951 .945 .929 .839
.893 .947 .960 .976 .976 .976 .975 .974 .967 .936
.938 .980 .984 .987 .989 .989 .989 .985 .982 .973
.981 .992 .994 .995 .995 .995 .995 .994 .990 .982
.987 .994 .995 .996 .995 .996 .996 .995 .994 .986
.991 .994 .997 .998 .997 .998 .998 .997 .994 .988
33.49 27.44 21.92 17.82 14.08 14.06 17.17 21.18 26.31 33.10
22.76 15.49 12.66 10.46 10.14 9.74 10.08 11.53 13.97 22.14
15.30 10.51 8.66 7.10 6.45 6.63 6.91 7.44 9.24 13.21
11.26 6.16 5.46 4.59 4.24 4.32 4.23 5.27 5.97 8.54
6.00 3.71 2.72 2.38 2.05 2.11 2.32 2.66 4.16 5.61
4.10 2.75 2.12 1.60 1.64 1.55 1.55 1.93 2.88 5.05
3.20 2.33 1.73 1.16 1.39 1.22 1.34 1.53 2.55 3.66

95
.637
.724
.807
.947
.974
.980
.982
38.82
30.95
23.97
12.45
8.66
7.03
6.04

2
.535
.535
.535
.563
.578
.582
.694
40.76
39.56
39.27
39.00
38.62
38.56
38.41

5
.535
.533
.565
.606
.626
.657
.715
40.76
39.56
38.70
38.11
37.66
37.23
36.89

10
.535
.562
.591
.627
.682
.698
.728
40.76
38.85
37.95
36.72
35.89
35.38
35.25

95
.529
.540
.555
.600
.629
.642
.702
53.77
53.55
53.46
53.32
53.19
52.53
51.69

2
.532
.552
.603
.637
.677
.702
.711
7.86
5.19
4.60
4.38
3.63
3.22
2.86

3.9*
.532
.601
.622
.654
.724
.745
.772
7.86
6.04
4.58
4.36
3.49
3.08
2.78

5
.532
.601
.642
.692
.734
.776
.799
7.86
6.04
4.84
4.77
3.47
3.07
2.75

2
.545
.556
.579
.584
.587
.593
.593
34.26
34.09
32.83
31.84
31.11
30.80
30.74

5
.575
.589
.592
.594
.596
.596
.596
33.48
32.96
31.90
30.78
30.70
30.68
30.66

10
.593
.603
.604
.612
.621
.622
.628
32.43
31.27
30.70
30.60
30.30
29.93
29.81

20
.557
.588
.617
.680
.690
.700
.737
41.06
38.87
36.45
35.40
35.32
35.23
33.68

30
.559
.593
.632
.678
.705
.715
.738
41.55
39.96
37.01
35.84
34.39
34.14
33.11

40
.571
.595
.651
.670
.712
.720
.740
41.91
39.81
37.68
36.98
35.62
34.25
33.43

40.1*
.570
.595
.651
.670
.712
.720
.736
41.91
39.81
37.68
36.98
35.62
35.21
33.69

50
.570
.600
.642
.671
.707
.713
.736
41.91
41.19
39.29
37.79
36.47
36.08
34.61

60
.563
.617
.619
.672
.700
.711
.730
45.40
41.08
41.49
38.37
37.62
37.35
36.69

70
.536
.603
.617
.675
.690
.699
.736
49.73
43.25
42.84
40.47
40.07
39.91
38.36

80
.556
.597
.615
.644
.679
.700
.722
48.59
46.42
46.32
45.47
44.11
43.55
41.68

90
.529
.562
.583
.615
.664
.661
.718
52.77
52.73
51.34
50.68
49.80
49.46
47.23

10
20
30
40
50
60
70
80
90
.558 .637 .699 .724 .775 .765 .769 .745 .747
.639 .704 .726 .798 .804 .828 .833 .830 .799
.645 .758 .798 .826 .841 .860 .861 .871 .854
.743 .793 .845 .865 .878 .893 .899 .904 .900
.790 .868 .893 .912 .916 .921 .926 .933 .927
.841 .890 .908 .917 .930 .935 .941 .948 .939
.865 .891 .911 .938 .937 .944 .951 .954 .952
8.81 11.11 12.58 12.31 15.72 19.66 22.55 32.06 42.38
7.38 8.05 9.23 10.48 14.44 16.40 20.84 27.38 40.64
5.22 6.76 8.19 10.03 12.32 13.67 16.74 24.00 35.44
5.25 6.12 6.87 7.90 9.66 12.21 14.33 18.69 30.22
3.97 4.27 5.32 6.08 7.03 9.02 10.33 15.65 22.76
3.05 3.60 4.04 5.23 5.99 7.31 9.86 12.93 20.60
2.59 3.03 3.79 4.53 5.38 6.48 8.51 12.37 18.10
20
.607
.613
.639
.652
.675
.675
.678
32.30
30.41
29.63
29.61
28.96
28.73
28.67

30
.620
.629
.651
.672
.688
.688
.688
31.97
30.57
29.71
29.25
28.73
28.56
28.56

35.6*
.621
.636
.657
.673
.692
.699
.700
32.44
30.91
30.02
29.34
28.60
28.44
28.40

40
.624
.643
.657
.677
.697
.703
.712
32.84
30.97
30.30
29.64
29.03
28.50
28.45

50
.619
.651
.665
.686
.703
.710
.713
33.48
31.82
30.66
29.62
29.33
28.77
28.71

60
.618
.648
.665
.686
.704
.710
.715
34.89
32.12
31.34
30.40
29.32
28.99
28.91

70
.609
.634
.659
.680
.690
.699
.700
36.05
33.61
32.05
30.86
30.10
29.95
29.78

80
.600
.622
.630
.650
.670
.677
.678
38.04
35.55
32.44
31.33
31.32
31.17
31.02

90
.580
.594
.603
.603
.603
.604
.604
38.31
38.19
35.11
33.02
32.80
32.75
32.67

95
.724
.780
.824
.876
.910
.927
.940
48.52
47.61
45.09
43.12
35.93
29.62
26.14
95
.532
.551
.553
.554
.556
.558
.558
43.65
37.86
37.73
35.09
34.46
34.18
33.87

Table B1b: Effect Training-Set Size Class Distribution Classifier Performance

347

fiWeiss & Provost

benefit selecting class distribution training data demonstrated using several examples. Table B1a highlights six cases (by using line connect pairs data points)
competitive improved performance achieved fewer training examples.
six cases, data point corresponding smaller data-set size performs well
better data point corresponds larger data-set size (the latter either
natural distribution balanced one).

Appendix C: Detailed Results Budget-Sensitive Sampling Algorithm
appendix describes execution progressive sampling algorithm described
Table 7. execution algorithm evaluated using detailed results Appendix
B. First, Table C1, detailed iteration-by-iteration description sampling algorithm
presented applied phone data set using error rate measure classifier performance.
Table C2 provides compact version description, reporting key
variables change value iteration iteration. Finally, Table C3a Table C3b,
compact description used describe execution sampling algorithm
phone, adult, covertype, kr-vs-kp, weather blackjack data sets, using error rate AUC
measure performance. Note tables, column labeled budget refers
budget used, cost incurredand case budget exceeded, means
examples requested execution algorithm used final training
set, heuristically-determined class distribution (i.e., algorithm budget-efficient).
results described appendix, consistent results presented Section
7, baVHGRQDJHRPHWULFIDFWRU RIDQGDYDOXHRIcmin 1/32. total budget available procuring training examples n. Based values, value K, determines number iterations algorithm computed line 2 Table 7, set 5.
Note value n different data set and, given methodology altering
class distribution specified Section 4.1, training set size Table 2 fraction
minority-class examples f, n = Sf.
description sampling algorithm, applied phone data set
error rate performance measure:

j = 0 Training-set size = 1/32 n. Form 13 data sets, contain 2% 95%
minority-class examples. requires .0297n (95% 1/32 n) minority-class examples
.0306n (100%-2% = 98% 1/32 n) majority-class examples. Induce
evaluate resulting classifiers. Based results Table 7, natural distribution, contains 18.2% minority-class examples, performs best. Total Budget:
.0603n (.0297n minority, .0306n majority).
j=1

Training-set size = 1/16 n. Form data sets corresponding best-performing class
distribution form previous iteration (18.2% minority) adjoining class distributions used beam search, contain 10% 20% minority-class examples.
requires .0250n (20% 1/16 n) minority-class examples .0563n (90% 1/16
n) majority-class examples. Since .0297n minority-class examples previously obtained, class distributions containing 30% 40% minority-class examples also
348

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

formed without requesting additional examples. iteration requires .0257n additional
majority-class examples. best-performing distribution contains 10% minority-class
examples. Total Budget: .0860 n (.0297n minority, .0563n majority).
j=2

Training-set size = 1/8 n. Since 10% distribution performed best, beam search
evaluates 5%, 10%, 18.2% minority-class distributions. 20% class distribution also evaluated since requires .0250n .0297n previously obtained
minority-class examples. total .1188n (95% 1/8 n) majority-class examples
required. best performing distribution contains 10% minority-class examples.
iteration requires .0625n additional majority-class examples. Total Budget: .1485n
(.0297n minority, .1188n majority).

j=3

Training-set size = 1/4 n. distributions evaluated 5%, 10%, 18.2%.
extra minority-class examples available evaluate additional class distributions. iteration requires .0455n (18.2% 1/4 n) minority-class examples
.2375n (95% 1/4 n) majority-class examples. best-performing class distribution
contains 10% minority-class examples. Total Budget: .2830n (.0455n minority, .2375n
majority)

j=4

Training-set size = 1/2 n. 5%, 10%, 18.2% class distributions evaluated.
iteration requires .0910n (18.2% 1/2 n) minority-class examples .4750n (95%
1/2 n) majority-class examples. best-performing distribution contains 10% minority-class examples. Total Budget: .5660n (.0910n minority, .4750n majority).

j=5

Training-set size = n. last iteration best class distribution previous iteration evaluated. Thus, data set size n formed, containing .1n minorityclass examples .9n majority-class examples. Thus .0090n additional minority-class
examples .4250n additional majority-class examples required. Since previously obtained examples used, waste budget exceeded.
Total Budget: 1.0n (.1000n minority, .9000n majority)

Table C1: Detailed Example Sampling Algorithm (Phone Data Set using Error Rate)

j size
class-distr
0 1/32 n
1 1/16 n 10, 18.2 , 20, 30, 40
2 1/8 n 5, 10 , 18.2, 20
3 1/4 n 5, 10 , 18.2
4 1/2 n 5, 10 , 18.2
1 n 10
5

Expressed fraction n
best min-need maj-need minority majority budget
18.2%
.0297
.0306
.0297
.0306 .0603
10%
.0250
.0563
.0297
.0563 .0860
10%
.0250
.1188
.0297
.1188 .1485
10%
.0455
.2375
.0455
.2375 .2830
10%
.0910
.4750
.0910
.4750 .5660
.1000
.9000
.1000
.9000 1.0000

Table C2: Compact Description Results Table B1a Phone Data Set

349

fiWeiss & Provost

Data set
Phone

Metric j
ER
0
1
2
3
4
5
Phone
AUC
0
1
2
3
4
5
Adult
ER
0
1
2
3
4
5
Adult
AUC
0
1
2
3
4
5
Covertype ER
0
1
2
3
4
5
Covertype AUC
0
1
2
3
4
5
Kr-vs-kp ER
0
1
2
3
4
5
Kr-vs-kp AUC
0
1
2
3
4
5

size
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n
1/32 n
1/16 n
1/8 n
1/4 n
1/2 n
1n

class-distr

10, 18.2 , 20, 30, 40
5, 10 , 18.2, 20
5, 10 , 18.2
5, 10 , 18.2
10

18.2, 20 , 30, 40
20, 30 , 40
20, 3 0, 40
18.2, 20 , 30
18.2

10, 20 , 23.9, 30, 40
10, 20 , 23.9
10, 20 , 23.9
5, 10 , 20
20

60, 70, 80 , 90
60, 70 , 80
60, 70 , 80
70, 80 , 90
80

2, 5 , 10, 20, 30, 40
2, 5 , 10, 20
2, 5 , 10
2, 5 , 10
5

14.8, 20 , 30, 40
20, 30 , 40
30, 40 , 50
20, 30 , 40
20

47.8, 50, 60
47.8, 50, 60
40, 47.8 , 50
40, 47.8 , 50
50

50, 60, 70
47.8, 50, 60
47.8, 50, 60
47.8, 50, 60
50

Expressed fraction n
best min-need maj-need minority majority budget
18.2
.0297
.0306
.0297
.0306 .0603
10
.0250
.0563
.0297
.0563 .0860
10
.0250
.1188
.0297
.1188 .1485
10
.0455
.2375
.0455
.2375 .2830
10
.0910
.4750
.0910
.4750 .5660
.1000
.9000
.1000
.9000
1.0
20
.0297
.0306
.0297
.0306 .0603
30
.0250
.0511
.0297
.0511 .0808
30
.0500
.1000
.0500
.1000 .1500
20
.1000
.2000
.1000
.2000 .3000
18.2
.1500
.4090
.1500
.4090 .5590
.1820
.8180
.1820
.8180
1.0
20
.0297
.0306
.0297
.0306 .0603
20
.0250
.0563
.0297
.0563 .0860
20
.0299
.1125
.0299
.1125 .1424
10
.0598
.2250
.0598
.2250 .2848
20
.1000
.4750
.1000
.4750 .5750
.2000
.8000
.2000
.8000
1.0
80
.0297
.0306
.0297
.0306 .0603
70
.0563
.0250
.0563
.0306 .0869
70
.1000
.0500
.1000
.0500 .1500
80
.2000
.1000
.2000
.1000 .3000
80
.4500
.1500
.4500
.1500 .6000
.8000
.2000
.8000
.2000
1.0
5
.0297
.0306
.0297
.0306 .0603
5
.0250
.0613
.0297
.0613 .0910
5
.0250
.1225
.0297
.1225 .1522
5
.0250
.2450
.0297
.2450 .2747
5
.0500
.4900
.0500
.4900 .5400
.0500
.9500
.0500
.9500
1.0
20
.0297
.0306
.0297
.0306 .0603
30
.0250
.0533
.0297
.0533 .0830
40
.0500
.1000
.0500
.1000 .1500
30
.1250
.1750
.1250
.1750 .3000
20
.2000
.4000
.2000
.4000 .6000
.2000
.8000
.2000
.8000
1.0
50
.0297
.0306
.0297
.0306 .0603
50
.0375
.0327
.0375
.0327 .0702
47.8
.0750
.0653
.0750
.0653 .1403
47.8
.1250
.1500
.1250
.1500 .2750
50
.2500
.3000
.2500
.3000 .5500
.5000
.5000
.5000
.5000
1.0
60
.0297
.0306
.0297
.0306 .0603
50
.0438
.0313
.0438
.0313 .0751
50
.0750
.0653
.0750
.0653 .1403
50
.1500
.1305
.1500
.1305 .2805
50
.3000
.2610
.3000
.2610 .5610
.5000
.5000
.5000
.5000
1.0

Table C3a: Summary Results Sampling Algorithm (phone, adult, covertype, kr-vs-kp)

350

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

Data set
Weather

Metric j size
ER
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Weather AUC
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Letter-a
ER
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Letter-a
AUC
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Blackjack ER
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n
Blackjack AUC
0 1/32 n
1 1/16 n
2 1/8 n
3 1/4 n
4 1/2 n
5 1n

class-distr

2,5 ,10,20,30,40, 40.1
5, 10, 20
10, 20 , 30
10, 20 , 30
30

30, 40 , 50
40, 50 , 60
30, 40, 50
40, 50 , 60
40

2, 3.9 , 5, 10, 20, 30, 40
2 , 3.9, 5, 10, 20
2, 3.9 , 5, 10
2, 3.9, 5
5

40, 50 , 60
50, 60 , 70
60, 70 , 80
70, 80 , 90

20, 30, 35.6, 40
10, 20, 30
10, 20, 30
20, 30, 35.6
35.6

20, 35.6, 40, 50
40, 50, 60
40, 50, 60
40, 50, 60
60

Expressed fraction n
best min-need maj-need minority majority budget
5
.0297
.0306
.0297
.0316 .0613
10
.0250
.0613
.0297
.0613 .0910
20
.0250
.1188
.0297
.1188 .1485
20
.0750
.2250
.0750
.2250 .3000
30
.1500
.4500
.1500
.4500 .6000
.3000
.7000
.3000
.7000
1.0
40
.0297
.0306
.0297
.0316 .0613
50
.0313
.0438
.0313
.0438 .0751
40
.0750
.0750
.0750
.0750 .1500
50
.1250
.1750
.1250
.1750 .3000
40
.3000
.3000
.3000
.3000 .6000
.4000
.6000
.4000
.6000
1.0
3.9
.0297
.0306
.0297
.0306 .0603
2
.0250
.0613
.0297
.0613 .0910
3.9
.0250
.1125
.0297
.1225 .1522
3.9
.0250
.2450
.0297
.2450 .2747
5
.0250
.4900
.0250
.4900 .5150
.0500
.9500
.0500
.9500
1.0
50
.0297
.0306
.0297
.0306 .0603
60
.0375
.0375
.0375
.0375 .0750
70
.0875
.0625
.0875
.0625 .1500
80
.2000
.1000
.2000
.1000 .3000
80
.4500
.1500
.4500
.1500 .6000
.8000
.2000
.8000
.2000
1.0
30
.0297
.0306
.0297
.0316 .0613
20
.0250
.0500
.0297
.0500 .0797
20
.0375
.1125
.0375
.1125 .1500
30
.0750
.2250
.0750
.2250 .3000
35.6
.1780
.4000
.1780
.4000 .5780
.3560
.6440
.3560
.6440
1.0
40
.0297
.0306
.0297
.0316 .0613
50
.0313
.0403
.0313
.0403 .0716
50
.0750
.0750
.0750
.0750 .1500
50
.1500
.1500
.1500
.1500 .3000
60
.3000
.3000
.3000
.3000 .6000
.6000
.4000
.6000
.4000
1.0

Table C3b: Summary Results Sampling Algorithm (weather, letter-a, blackjack)

References
Bauer, E., & Kohavi, R. (1999). empirical comparison voting classification algorithms:
bagging, boosting, variants. Machine Learning, 36, 105-139.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification Regression Trees. Belmont, CA: Wadsworth International Group.
Blake, C., & Merz, C. (1998).
UCI Repository Machine Learning Databases,
(http://www.ics.uci.edu/~mlearn/MLRepository.html), Department Computer Science,
University California.
Bradley, A. (1997). use area ROC curve evaluation machine learning algorithms. Pattern Recognition, 30(7), 1145-1159.
351

fiWeiss & Provost

Bradford, J.P., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. E. (1998). Pruning decision trees
misclassification costs. Proceedings European Conference Machine
Learning, pp. 131-136.
Catlett, J. (1991). Megainduction: machine learning large databases. Ph.D. thesis, Department Computer Science, University Sydney.
Chan, P., & Stolfo, S. (1998). Toward scalable learning non-uniform class cost distributions: case study credit card fraud detection. Proceedings Fourth International
Conference Knowledge Discovery Data Mining, pp. 164-168, Menlo Park, CA:
AAAI Press.
Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, W. P. (2000). SMOTE: synthetic minority
over-sampling technique. International Conference Knowledge Based Computer Systems.
Cohen, W., & Singer, Y. (1999). simple, fast, effective rule learner. Proceedings
Sixteenth National Conference Artificial Intelligence, pp. 335-342, Menlo Park, CA:
AAAI Press.
Cohn, D., Atlas, L. Ladner, R. (1994). Improved generalization active learning. Machine Learning, 15:201-221.
Drummond, C., & Holte, R.C. (2000). Exploiting cost (in)sensitivity decision tree splitting
criteria. Proceedings Seventeenth International Conference Machine Learning,
pp. 239-246.
Elkan, C. (2001). foundations cost-sensitive learning. Proceedings Seventeenth
International Joint Conference Artificial Intelligence, pp. 973-978.
Estabrooks, A., & Japkowicz, N. (2001). Mixture-of-Experts Framework ConceptLearning Imbalanced Data Sets. Proceedings 2001 Intelligent Data Analysis
Conference.
Fawcett, T. & Provost, F. (1997). Adaptive Fraud Detection. Data Mining Knowledge Discovery 1(3): 291-316.
Good, I. J. (1965). Estimation Probabilities. Cambridge, MA: M.I.T. Press.
Hand, D. J. (1997). Construction Assessment Classification Rules. Chichester, UK: John
Wiley Sons.
Holte, R. C., Acker, L. E., & Porter, B. W. (1989). Concept learning problem small
disjuncts. Proceedings Eleventh International Joint Conference Artificial Intelligence, pp. 813-818. San Mateo, CA: Morgan Kaufmann.
Japkowicz, N. & Stephen, S. (2002). Class Imbalance Problem: Systematic Study. Intelligent Data Analysis Journal, 6(5).
Japkowicz, N., Holte, R. C., Ling, C. X., & Matwin S. (Eds.) (2000). Papers AAAI
Workshop Learning Imbalanced Data Sets. Tech, rep. WS-00-05, Menlo Park, CA:
AAAI Press.

352

fiLearning Training Data Costly: Effect Class Distribution Tree Induction

Jensen, D. D., & Cohen, P. R. (2000). Multiple comparisons induction algorithms. Machine
Learning, 38(3): 309-338.
John, G. H., & Langley, P. (1996). Static versus dynamic sampling data mining. Proceedings Second International Conference Knowledge Discovery Data Mining, pp. 367-370. Menlo Park, CA. AAAI Press.
Kubat, M., & Matwin, S. (1997). Addressing curse imbalanced training sets: one-sided
selection. Proceedings Fourteenth International Conference Machine Learning, pp. 179-186.
Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling supervised learning.
Proceedings Eleventh International Conference Machine Learning, pp.148-156.
Provost, F., Fawcett, T., & Kohavi, R. (1998). case accuracy estimation comparing classifiers. Proceedings Fifteenth International Conference Machine
Learning. San Francisco, CA: Morgan Kaufmann.
Provost, F., Jensen, D., & Oates, T. (1999). Efficient progressive sampling. Proceedings
Fifth International Conference Knowledge Discovery Data Mining. ACM Press.
Provost, F., & Fawcett, (2001). Robust classification imprecise environments. Machine
Learning, 42, 203-231.
Provost, F., & Domingos, P. (2001). Well-trained PETs: improving probability estimation trees.
CeDER Working Paper #IS-00-04, Stern School Business, New York University, New
York, NY.
Quinlan, J.R. (1993). C4.5: Programs Machine Learning. San Mateo, CA: Morgan Kaufmann.
Saar-Tsechansky, M., & Provost, F. (2001). Active learning class probability estimation
ranking. Proceedings Seventeenth International Joint Conference Artificial Intelligence, Seattle, WA.
Saar-Tsechansky, M. F. Provost (2003). Active Sampling Class Probability Estimation
Ranking. appear Machine Learning.
SAS Institute (2001). Getting Started SAS Enterprise Miner. Cary, NC: SAS Institute Inc.
Saerens, M., Latinne, P., & Decaestecker, C. (2002). Adjusting outputs classifier new
priori probabilities: simple procedure. Neural Computation, 14:21-41.
Swets, J., Dawes, R., & Monahan, J. (2000). Better decisions science. Scientific American, October 2000: 82-87.
Turney P. (2000). Types cost inductive learning. Workshop Cost-Sensitive Learning
Seventeenth International Conference Machine Learning, 15-21, Stanford, CA.
Van den Bosch A., Weijters, A., Van den Herik, H.J., & Daelemans, W. (1997). small
disjuncts abound, try lazy learning: case study. Proceedings Seventh BelgianDutch Conference Machine Learning, 109-118.

353

fiWeiss & Provost

Weiss, G.M., & Hirsh, H. (2000). quantitative study small disjuncts, Proceedings
Seventeenth National Conference Artificial Intelligence, 665-670. Menlo Park, CA:
AAAI Press.
Weiss, G. M., & Provost, F (2001). effect class distribution classifier learning:
empirical study. Tech rep. ML-TR-44, Department Computer Science, Rutgers University, New Jersey.
Zadrozny, B., & Elkan, C. (2001). Learning making decisions costs probabilities
unknown. Tech. rep. CS2001-0664, Department Computer Science Engineering, University California, San Diego.

354

fiJournal Artificial Intelligence Research 19 (2003) 513-567

Submitted 1/03; published 11/03

Decentralized Supply Chain Formation: Market Protocol
Competitive Equilibrium Analysis
William E. Walsh

WWALSH 1@ US . IBM . COM

IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532 USA

Michael P. Wellman

WELLMAN @ UMICH . EDU

University Michigan AI Laboratory
1101 Beal Avenue
Ann Arbor, MI 48109-2110 USA

Abstract
Supply chain formation process determining structure terms exchange relationships enable multilevel, multiagent production activity. present simple model
supply chains, highlighting two characteristic features: hierarchical subtask decomposition,
resource contention. decentralize formation process, introduce market price system
resources produced along chain. competitive equilibrium system, agents
choose locally optimal allocations respect prices, outcomes optimal overall. determine prices, define market protocol based distributed, progressive auctions, myopic,
non-strategic agent bidding policies. presence resource contention, protocol produces
better solutions greedy protocols common artificial intelligence multiagent systems literature. protocol often converges high-value supply chains, competitive
equilibria exist, typically approximate competitive equilibria. However, complementarities
agent production technologies cause protocol wastefully allocate inputs agents
produce outputs. subsequent decommitment phase recovers significant fraction
lost surplus.

1. Introduction
Electronic commerce technology provide significant improvements existing modes commercial interaction, increased speed, convenience, quality, reduced costs. Yet
proposed radical visions business may transformed. Exponential increases
communications bandwidth computational ability potential qualitatively decrease
friction business interactions. premise, Malone Laubauchers treatise
emerging E-Lance Economy (1998) puts forth view that, not-too-distant future, business
relationships lose much current persistent character. Indeed, Malone Laubaucher
propose large companies know cease exist, rather dynamically formed
electronically connected freelancers (e-lancers) purpose producing particular goods
services, dissolved projects completed. Others employ evocative term
virtual corporation (Davidow, 1992) describe groups agile organizations forming temporary
confederations ad hoc purposes.
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWALSH & W ELLMAN

Whether one accepts full extent vision virtual corporations, several business
trends provide evidence moving direction. Software companies time-shifting
development U.S. India, Sun Microsystems allows freelance programmers bid fix customers software problems (Borenstein & Saloner, 2001). Large, traditional
manufacturing companies, exemplified major automotive manufacturers, increasingly outsource
production various components. Ford General Motors (GM) spun parts manufacturing separate companies (Lucking-Reily & Spulber, 2001). Start-ups small
companies form partnerships compete larger, established companies. Application
service providers supplant in-house provision standard operations, information, technology
services.
study phenomenon guise supply chains, common form coordinated commercial interaction. purposes, supply chain network production exchange
relationships spans multiple levels production task decomposition. Whenever
producer buys inputs sells outputs, supply chain. Although typically used refer
multi-business structures manufacturing industries, service contracting relationship
spans multiple levels viewed supply chain.
Supply chain formation process determining participants supply chain,
exchange whom, terms exchanges. Traditionally, supply chains
formed maintained long periods time means extensive human interactions.
acceleration commercial decision making creating need advanced support.
Companies ranging auto makers computer manufacturers basing business models
rapid development, build-to-order, customized products satisfy ever-changing consumer
demand. fluctuations resource costs availability mean companies must respond
rapidly maintain production capabilities profits. changes increasingly occur
speeds, scales, complexity unmanageable humans, need automated supply chain
formation becomes acute.
agents autonomous electronic commerce setting, must generally assume specialized knowledge capabilities limited knowledge
individuals large-scale structure problem. agents self-interested,
participate goal maximizing benefit. Additionally, may
cause control allocation resource individually if, instance, global optimization
infeasible one entity global allocative authority. environments information, decision making, control inherently decentralized, seek engineer process
bottom-up supply chain formation. problem complicated structure resource
contention precludes use simple greedy allocation strategies.
present decentralized, asynchronous market protocol supply chain formation
conditions resource scarcity. protocol allows agents negotiate formation supply
chains bottom-up fashion, requiring local knowledge communication. market
protocol, agents decisions coordinated price system, price resource
determined ascending auction.
remainder paper describes market protocol, characterizes behavior theoretically empirically.1 begin Section 2 formal definition supply chain
formation problem, illustrating application automotive industry. Section 3,
1. details may found first authors dissertation (Walsh, 2001).

514

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

show typical greedy top-down approaches supply chain formation fail presence
resource contention. define price system analyze static properties price equilibria
Section 4. Section 5, introduce price-based market protocol supply chain formation
analyze convergence properties. present results empirical study protocol
Section 6. Section 7, discuss relevant results issues price-based analysis auction
theory, well related work supply chain formation. conclude Section 8
suggest extensions future work. Throughout, defer proofs Appendix A.

2. Supply Chain Formation Problem
Agents supply chain characterized terms capabilities perform tasks,
interests tasks accomplished. central feature model problem hierarchical
task decomposition: order perform particular task, agent may need achieve
subtasks, may delegated agents. may turn subtasks may
delegated, forming supply chain decomposition task achievement. Constraints
task assignment arise resource contention, agents require common resource (e.g.,
task achievement, something tangible piece equipment) accomplish tasks.
Tasks performed behalf particular agents; two agents need task would
performed twice satisfy both. way, tasks discrete, rival
resource. Hence, make distinction model, use term good refer task
resource provided needed agents. assumption goods cannot shared reused
(i.e., limited available quantities) necessary much analysis. Goods
replicated little marginal cost, software information, provide many interesting
challenges economic analysis (Shapiro & Varian, 1999), addressed work.
2.1 Example: Automotive Supply Chain Formation
illustrate model supply chain formation application stylized, hypothetical
example automotive industry. Traditionally, automotive supply chains span many tiers,
formed maintained long periods time extensive human negotiations.
automation emerging, example Covisint2 , company formed GM, Ford,
DaimlerChrysler mediate negotiation exchange parts, well supply chain
interactions. Currently focus efforts particular exchange relationship within
single level production. consider broader problem assembling combinations relationships across multiple levels form complete, feasible supply chains.
example presented Figure 1, Ford GM need acquire contracts transmissions
order produce particular models cars. Ford produce transmissions factories acquire independent transmission producer. GM currently
capacity produce desired transmissions, must outsource. independent transmission
producer capacity provide transmissions either Ford GM, both. Ford
independent factory require services job shop metal-working tasks, job shop
capacity serve simultaneously. Contracts job shop two
transmission factories scarce goods allocated.
2. http://www.covisint.com

515

fiWALSH & W ELLMAN

Ford Auto
Assembly

Ford
Distribution

Job Shop

Independent
Transmission
Factory

Ford
Transmission
Factory

GM Auto
Assembly

GM
Distribution

Figure 1: automotive supply chain formation problem.
limited capacity job shop entails certain constraints feasible supply chains. Ford
cannot acquire transmissions independent factory, job shop cannot serve
independent factory Ford simultaneously. Additionally, Ford GM cannot simultaneously
satisfied.
2.2 Problem Specification
provide formal description supply chain formation problem terms bipartite graphs.
two types nodes represent goods agents, respectively. task dependency network
directed, acyclic graph, (V, E), vertices V = G A, where:
G = set goods,
= C , set agents,
C = set consumers,
= set producers,
set edges E connecting agents goods use produce. exists edge
hg, ai g G agent make use one unit g, edge ha, gi
provide one unit g. agent requires multiple units good input, treat
unit separate edge, distinguishing subscripts. (Edges without explicit subscripts
interpreted implicitly subscripted 1.) instance, agent requires two units g
input, input edges hg, ai1 hg, ai2 .
various agent types characterized position task dependency network.
consumer, c C, wishes acquire one unit one good set consumable goods,
Gc G, hg, ci E iff g Gc .
producer produce single unit output good conditional acquiring input
goods. producer associate:
1. input set, G, g iff edges hg, ik E one k,
516

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Worked
Metal

Ford Transmission
Subcontractor

Ford
Cars

Ford
Distribution
$25,000

Ford
Transmissions

$20
Job Shop
$20

Ford Auto
Assembler
$20,000

Ford
Transmission
Factory
$100

Independent
Transmission
Factory
$50
GM Transmission
Subcontractor
$60

GM
Trasmissions

GM Auto
Assembler
$25,000

GM
Cars

GM
Distribution
$30,000

Figure 2: Network auto: task dependency network automotive supply chain depicted
Figure 1.

2. single output, g G \ , h, g E.
producers input goods complementary agent must acquire order
produce output; cannot accomplish anything partial set. Alternate producers
output indicate different ways good produced.
Task dependency networks constrained acyclic, is, agent produces goods
could used assemble inputs chain production. Although might broadly
view global commerce one large cycle production consumption, practice, negotiations tend clustered within limited scopes concern, often referred industries.
resulting supply chains typically acyclic.
Figure 2 shows example task dependency network automotive supply chain problem
Figure 1. goods indicated circles, agents boxes. Producers inputs
represented curved boxes. numbers agent boxes represent production costs
consumption values, explained below. arrow agent good indicates agent
provide good, arrow good agent indicates agent make use
good. instance, producer labeled Ford Auto Assembly requires Worked Metal Ford
Transmissions order produce cars. Since transmissions produced Ford Transmission
Factory used Ford, need distinguish Ford GM transmissions separate
goods. turn requires introduce Ford GM Transmission Subcontractor producers
model fact Independent Transmission Factory used produce either type.
allocation subgraph (V 0 , E 0 ) (V, E). g G, edge ha, gi E 0 means
agent provides g, hg, ai E 0 means acquires g. allocations vertices agents
goods incident edges:
1. agent allocation graph iff acquires provides good:
A, V 0 iff hg, ai E 0 ha, gi E 0 .
517

fiWALSH & W ELLMAN

Worked
Metal

Ford Transmission
Subcontractor

Ford
Cars

Ford
Distribution
$25,000

Ford
Transmissions

$20
Job Shop
$20

Ford Auto
Assembler
$20,000

Ford
Transmission
Factory
$100

Independent
Transmission
Factory
$50
GM Transmission
Subcontractor
$60

GM
Trasmissions

GM Auto
Assembler
$25,000

GM
Cars

GM
Distribution
$30,000

Figure 3: solution Network auto.
2. good allocation graph iff acquired provided:
g G, g V 0 iff hg, ai E 0 ha, gi E 0 .
producer active iff provides output. producer feasible iff inactive
acquires inputs. Consumers always feasible.
Good g material balance (V 0 , E 0 ) iff number edges equals number out:
fi
fi
fi fi
fi{(a, k) | ha, gik E 0 }fi = fi{(a, k) | hg, aik E 0 }fi .

allocation feasible iff agents feasible goods material balance.
solution feasible allocation forms partial ordering feasible production, culminating
consumption. is, consumer acquires good desires:
exists hg, ci E 0 c C V 0 .

solution may involve multiple consumers. consumer c solution (V 0 , E 0 ) say
(V 0 , E 0 ) solution c.
Figure 3 shows solution allocation task dependency network Figure 2. Shaded
agents solid arrows part solution, unshaded agents dashed arrows indicating
elements problem part solution. Note Ford Auto Assembler wins input,
inactive. However, recall inactive producers feasible, hence solution properties
met. refer configuration inactive producer acquiring input allocation
dead end.
producer production cost providing unit output. cost might
represent value could obtain engaging activity (i.e., opportunity cost),
direct cost incurred producing output (but including input costs). Since producer
provides one unit one good, total production cost , output g, allocation
E 0 , h, gi E 0 0 otherwise.
assume consumer preferences different possible goods, wishes obtain
single unit one good. Thus, consumer c obtains value vc (g) obtaining single unit
518

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

good g, and, allocation E 0 , obtains value vc ((V 0 , E 0 )) maxhg,ciE 0 vc (g). depicting task
dependency networks, display costs values corresponding agent boxes.
Definition 1 (value allocation) value allocation (V 0 , E 0 ) is:
value((V 0 , E 0 ))

vc ((V 0, E 0)) ((V 0, E 0)).


cC

Definition 2 (efficient allocations) set efficient allocations contains feasible allocations
(V , E ) that:
value((V , E )) =

max

(value((V 0 , E 0 )) | (V 0 , E 0 ) feasible).

(V 0 ,E 0 )(V,E)

Task dependency networks describe supply chain formation problem global perspective. decentralized approach formation, would generally assume agent,
entity, perfect complete knowledge entire network. generally assume
agents perfect knowledge costs, values, goods interest. mediators
facilitate negotiations goods (as protocols described below), agent knows relevant
mediators goods interest. knowledge includes rules enforced mediators.
Likewise, mediators know existence agents interested respective goods. Beyond
that, mediator knows agents reveal communication negotiation.
mediator know agents true costs valuations, aware agents preferences
goods outside direct scope facilitation. address detail agents
mediators achieve mutual awareness (i.e., connections originate), assume
accomplished via unspecified search, notification, broadcast protocol.

3. Resource Contention
One natural candidate approach supply chain formation CONTRACT NET protocol (Davis
& Smith, 1983), widely studied algorithm forming task performance relations among
distributed agents. C ONTRACT NET indeed apply framework, employs local negotiation achieve hierarchical task decomposition. Although definitive characterization difficult
due many variants CONTRACT NET literature (Baker, 1996; Davis & Smith, 1983;
Dellarocas et al., 2000; Sandholm, 1993), fair say that, generally, request quotes proceed top root task (right-to-left consumers, network terminology),
contracting proceeds bottom-up (left-to-right towards consumers), selecting level among
candidate bids received. (Variants protocol primarily distinguished form bids
selection criteria employed.) consequence, choices made greedily, without reflecting
ramifications upstream evolving chain.
approach form satisficing supply chains sufficient resources support greedy selection. However, basic CONTRACT NET protocol explicitly address
resource scarcity contention among multiple agents. Producers accept bids inputs
established whether might cause infeasibility upstream. Without lookahead
backtracking, CONTRACT NET might construct infeasible supply chains limited resources.
instance, greedy protocol would produce solution network shown Figure 4.
Here, producers bid according common function monotone cost, output bid
519

fiWALSH & W ELLMAN

a1
5

1

a2

2

a5
0
5

1
a3

3

1
a4

a7
0

6

cons
15

a6
0

4

1

Figure 4: Network greedy-bad: network greedy protocols produce infeasible
allocations.

producer a6 would preferred a5, a6 acquire inputs cheaper. since
a7 must acquire one available unit good 4 feasibly participate solution, a6 cannot
part solution.
issue resource contention motivates adoption market-based approach. key
idea prices signal resource value scarcity chain, enabling local
decision making avoiding pitfalls greedy one-pass selection communication global
structure information.

4. Price Systems
price system p assigns good g, nonnegative number p(g) price. Prices anonymous (i.e., agent dependent) linear quantity goods. Intuitively, prices indicate
relative value goods, agents use prices guide local decision making.
assume agents quasilinear utility functions, defined money holdings plus
value (or minus cost) associated allocation goods. Agents wish maximize surplus
respect prevailing prices.
Definition 3 (surplus) surplus, (a, (V 0 , E 0 ), p), agent allocation (V 0 , E 0 ) prices
p, given by:
va ((V 0 , E 0 )) hg,aiE 0 p(g), C
ha,giE 0 p(g) hg,aiE 0 p(g) ((V 0 , E 0 )), .
4.1 Price Equilibrium
Generally, allocation (V 0 , E 0 ) competitive equilibrium prices p (V 0 , E 0 ) feasible
assigns agent allocation optimizes agents surplus p. model, means
specifically:
520

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

producers optimal choice either active feasible, acquire goods. Hence,
producer allocation obtains nonnegative surplus active, producer
allocation would obtain nonpositive surplus active.
V 0 ,



p(g)

\V 0 ,



p(g)



p(g) 0



p(g) 0

hg,iE

h,giE

h,giE

hg,iE

consumer receives value obtaining one good, consumers optimal
choice obtain good gives maximum nonnegative surplus, obtain
goods positive price. Furthermore, consumer allocation (i.e., obtaining
goods) would obtain nonpositive surplus good.
c C V 0 , hg, ci E 0 , g = arg max
vc (g0 ) p(g0 )
0
g G

vc (g) p(g) 0
hg0 , Ei, g0 6= g, p(g0 ) = 0
c C \V 0 , g G,
vc (g) p(g) 0
Figure 5 shows example competitive equilibrium Network greedy-bad. prices
shown respective goods.
a1
5

1

a2

2

1

1

a3

3

1

0

a4

4

1

6

5

a5
0
5
7

a7
0

6

cons

14

15

a6
0

Figure 5: competitive equilibrium Network greedy-bad.

competitive equilibrium allocation stable sense agent would want different
allocation equilibrium prices. Moreover, equilibrium way reallocate resources (including money transfers) agent greater surplus, without degrading
agents surplus. absence gains trade referred Pareto optimality.
Given quasilinear utility, price equilibria shown efficient fairly general conditions (Bikhchandani & Mamer, 1997; Gul & Stacchetti, 1999; Ygge, 1998). also holds
particular case task dependency networks, stated Corollary 4.
521

fiWALSH & W ELLMAN

p(1) >
_5
a1
5
a2
1
a3
1
a4
1

1
p(2) >
_1
2
p(3) <
_1
3
p(4) >
_4

a5
0

9_
> p(6) >
_ 10

p(5) >
_6
5

a7
0

6

cons
9

a6
0

4

Figure 6: Network greedy-bad costs values support competitive equilibrium.

4.2 Existence Competitive Equilibrium
task dependency networks competitive equilibria. Consider Network greedybad vcons = 9, shown Figure 6. allocation shown efficient allocation,
hence equilibrium must support it. Recall equilibrium, active agents must obtain nonnegative surplus, inactive producers must able obtain positive surplus. price inequalities goods follow constraints surplus associated agent activity. lower
bounds prices goods 1, 2, 5 ensure producers a1, a2, a5, respectively receiv5e
nonnegative surplus. upper bound 3 ensures a3 could obtain positive surplus.
lower bound 4 ensures a6 would receive nonpositive surplus. Propagating bounds
6, see p(6) 10 give a7 positive surplus, also p(6) 9 give cons nonnegative
surplus. Since impossible, competitive equilibrium cannot exist.
Technically, non-existence equilibrium due complementarity inputs producers
discrete-quantity goods. fact, complementarities necessary preclude competitive equilibrium task dependency networks. network input complementarities producers
one input.
Theorem 1 Competitive equilibria exist network input complementarities.
defer proof subsequent theorems Appendix A.
Consider Figure 6. multiple undirected paths 1 4 give rise lower
bound price good 6. turns undirected cycles also necessary preclude
competitive equilibrium.
polytree graph one undirected path vertex another.
Recall task dependency networks, producer uses multiple units good, unit
represented separate edge. follows allocation polytree iff one unit
good used produce another given good, used multiple ways produce good.
Theorem 2 Competitive equilibria exist polytree.
522

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

4.3 Approximate Price Equilibrium
generally expect market protocols based discrete price adjustments (such
SAMP-SB protocol describe Section 5) would overshoot exact equilibria least small
amount. Therefore, analysis emphasizes approximate equilibrium concepts (Demange et al.,
1986; Wellman et al., 2001a). introduce particular type approximation, --competitive
equilibrium, defined terms parameters bound degree agents acquire suboptimal surplus. Intuitively, b bounds suboptimality consumers surplus, bounds
suboptimality producers surplus attributable output, g bounds suboptimality
producer surplus attributable input g. described Section 5, parameters also
special interpretation market protocol applied task dependency networks.
Denote Ha (p) maximum surplus agent obtain (V, E), prices p, subject
feasibility. is,
Ha (p)
max (a, (V 0 , E 0 ), p)
(V 0 ,E 0 )(V,E)

feasible (V 0 , E 0 ).
Definition 4 (--competitive equilibrium) Given parameters:
b , 0,
g G,
g

allocation (V 0 , E 0 ) --competitive equilibrium prices p iff:
1. A, (a, (V 0 , E 0 ), p) 0.
2. c C, (c, (V 0 , E 0 ), p) Hc (p) b .
3. , (, (V 0 , E 0 ), p) H (p) (hg,iE g + ), feasible (V 0 , E 0 ).
4. goods material balance.
Consider Network greedy-bad prices shown Figure 5 except p(5) = 8.
constitute exact competitive equilibrium a6, though inactive, could make
positive profit. However, 2a6 + 3a6 + 4a6 + 1, since Ha6 (p) = 1, a6 obeys Condition 3
allocation --competitive equilibrium specified prices.
Theorem 3 (V 0 , E 0 ) --competitive equilibrium (V, E) prices p, (V 0 , E 0 )
feasible allocation nonnegative value differs value efficient allocation
[hg,iE g + ] + |C|b .
--competitive equilibrium corresponds standard notion competitive equilibrium
g
b = = 0, = 0 g.
Corollary 4 (to Theorem 3) competitive equilibrium allocation efficient.
noted Section 4.1, consistent previously established results.

523

fiWALSH & W ELLMAN

4.4 Valid Solutions
following sections show --competitive equilibria useful concept analyzing decentralized market protocols. However, protocols always reach --competitive
equilibria networks. Hence also consider weaker constraints prices, consistent
lesser degree agent optimization solution allocation.
say solution (V 0 , E 0 ) valid respect prices p if:
1. consumer solution pays value single good. is,
c C V 0 , exists single hg, ci E 0
p(g) vc (g),
p(g0 ) = 0 g0 6= g hg0 , ci E 0 .
2. None active producers unprofitable. V 0 h, g E 0
(, (V 0 , E 0 ), p) 0. Note solution validity preclude inactive producer
unprofitable (i.e., admits dead ends).
Note (1) effectively states consumers obtain negative utility, weaker
competitive equilibrium conditions require consumers receive optimal
allocation. Similarly, (2) require producers optimize, competitive equilibrium,
requires nonnegative utility active producers.
a1
5

1

a2

2

1

2

a3

3

1

1

a4

4

1

5

5

a5
0
5
8

a7
0

6

cons

13

15

a6
0

Figure 7: valid solution Network greedy-bad.
Figure 7 shows example valid solution, underlying costs values
Figure 5. allows dead ends, validity directly provide useful bounds
inefficiency allocation.

5. SAMP-SB Protocol
preceding section introduces static properties price configurations allocations.
address problem prices might obtained. compute prices allocations,
must elicit information bearing relative value goods, systematic communication process. Mechanisms determine market-based exchanges based messages
agents called auctions (McAfee & McMillan, 1987).
524

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

agents bidding policies represent strategies interacting auctions. Whereas
auction mechanism may designed central authority, bidding policies generally determined individual agents. understand implications auction design requires
analysis market protocol arises combination auction mechanism
agent bidding policies.
space potential auctions expansive (Wurman et al., 2001), definitive theoretical
results currently known fairly limited classes problems (Bikhchandani & Mamer,
1997; Demange & Gale, 1985; Gul & Stacchetti, 2000; Klemperer, 1999; McAfee & McMillan,
1987). Complementarities discrete goods, cause nonexistence price equilibria,
also greatly complicate auction design analysis auctions (Milgrom, 2000).
supply-chain domain, investigated particular protocol, called SAMP-SB (Simultaneous Ascending (M+1)st Price Simple Bidding). demonstrated below, SAMP-SB
produce good allocations which, cases, consistent competitive price equilibrium theory.
5.1 Auction Mechanism
SAMP-SB mechanism comprises set auctions, one good. Auctions run simultaneously, asynchronously, independently, without direct coordination. Agents interact
auctions submitting bids goods wish buy sell. bid form: ((q1 p1 ) . . . (qn
pn )). pair (qi pi ) indicates offer buy sell good, qi indicating quantity
offer pi indicating price. qi > 0, offer buy qi units good
pi per unit, refer buy offer. qi < 0, offer sell qi units less
pi per unit, refer sell offer. agent buys sells good
task dependency network, bid contains either positive negative quantity offers. Bids
possess sometimes called additive-OR semanticsthe offers treated exactly
came separate bids, hence auction match individual offers independently.
Without loss generality, henceforth impose restriction |qi | = 1 offers bids,
continuing allow agents may submit multiple offers bid.
auction receives new bid, sends bidders price quote specifying
price would result auction ended current bid state. Price quotes issued
initial bids received, subsequently issued immediately receipt new bids.
offers may tied current price, information alone sufficient
agent tell whether winning offer placed price. clarify ambiguity, price
quote also reports bidder quantity would buy sell current state.
prices sent bidders, reported winning state specific recipient. Agents may
choose revise bids response notifications (if agent wish change
bid, inaction leaves previous bid standing auction).
assume communication reliable asynchronous.3 is, messages sent eventually reach recipients, although impose bound delays. Agents auctions use
message IDs ensure handle messages appropriate order. Note even auctions agents deterministic behaviors, overall run SAMP-SB may nondeterministic
due asynchrony.
3. Technically, adopt model asynchronous reliable message passing systems (Fagin et al., 1995).

525

fiWALSH & W ELLMAN

asynchrony, helpful auction send ID recent bid received
agent price quote. agent responds price quote reflects
recent bid sent. Without device, agent difficulty establishing feasibility,
understanding input output bid states may based nonuniformly delayed reports.
Bidding continues quiescence, state messages received, agent
chooses revise bids, auction changes prices, ask prices, allocation. point,
auctions clear; bidder notified final prices many units transacts
good. Note quiescent system necessarily solution state (approximate) equilibrium
state.
Although detecting quiescence straightforward centralized system, decentralized,
asynchronous system need perform operation using local message passing. previous work (Wellman & Walsh, 2000), described protocol detecting quiescence general
distributed negotiations, based well-known termination-detection algorithm.
auction runs according (M+1)st-price rules (Satterthwaite & Williams, 1989, 1993;
Wurman et al., 1998). (M+1)st price auction variant (second-price) Vickrey auction (Vickrey, 1961), generalized allow exchange multiple units good. Given set
offers including units offered sale, (M+1)st-price auction sets price equal price
(M+1)st highest offer offers. price said separate winners
losers, winners include sell offers strictly price buy offers
strictly price. agents offer (M+1)st price also win; case ties, offers
submitted earlier precedence. Winning buy sell offers matched one-to-one, pay
(or get paid) (M+1)st price.
issuing price quotes, auction reports price (i.e., current going price,
(M+1)st price), p(g) ask price, (g) good g. ask price specifies amount
buyer would offer order buy good, given current set offers. ask
price determined price Mth highest offers auction, hence (g) p(g).
instance, buy bids 12, 10, 6 sell bids 15, 11, 8, p(g) = 10, (g) = 11,
auction quiescence, buy bids 12 10 would match sell bids 15 11
trade p(g) = 10.
producer complementary inputs, ensuring feasibility challenging problem,
requiring careful design. auctions run simultaneously, auction requires prices
agents successive buy offers increase less (generally small) positive number
b prices successive sell offers increase less .4 auction enforce
ascending rule simply rejecting agents offer price increase b .
constraining direction price changes, design gives producers accurate indication
relative prices inputs outputs prices allowed fluctuate directions.
ascending bid restriction ensures ascending auction prices, one technicality. Due
asynchrony immediate issuance price quotes, initial bid agent arrives
higher bid, price quote could decrease. handled simply auction issuing
price quotes specified period time auction opens. first price quote
issued, auction accepts new bids agents previously placed bids.
common auction literature practice place ascending restriction buy-offer
prices. may seem counterintuitiveand fact atypicalto place restriction
4. rules differ typical simultaneous ascending auction (Demange et al., 1986; Milgrom,
2000), specify agents must submit offer prices least increment current price.

526

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

sell-offer prices. However, ascending offer price restriction ensures price quotes rise
monotonically auctions progress. Section 5.4 shows ascendingoffer-price restriction buy sell offers serves key role establishing relationships system
quiescence solution convergence system.
5.2 Bidding Policies
Although designers negotiation mechanisms generally control agents behaviors, conclusions outcome mechanism must based assumptions
behaviors. typical assumption economics agents rational sense,
example play policies form Bayes-Nash equilibrium. However, discussed
Section 7.1, complexity supply chain formation markets beyond current state-of-the-art
analyzing Bayes-Nash equilibria simultaneous ascending auctions. Instead, analysis assumes agents follow simple, non-strategic bidding policy, described section.
variations may reasonable, perhaps better respects policies describe.
Rather explore range possibilities, chose work investigate particular set
policies depth. chosen policies obey ascending offer restriction enforced auction,
respect locality information require knowledge agents system,
myopic use information provided current price quotes, without
forecasting future prices.
Recall consumer wishes acquire single good maximizes surplus given
prices. assume consumer initially offers zero good interest. long
winning good, change offer. Whenever winning good, offers p(g ) + b
good g = arg maxgG (vc (g) p(g) b ) vc (g ) p(g ) b 0, otherwise stops bidding.
producers objective much complex, namely maximize difference
price receives output total price pays inputs, remaining feasible.
assume producer initially offers zero input goods, gradually increases
offers ensure feasibility. raises offer price input good b price
quotes indicate losing good winning output.
assume producer bids output good g effort recover production cost
perceived costs inputs. producer places first output offer receiving
first price quotes inputs, subsequently updates output offer whenever receives
new price quote input. simplicity, consider case one offer (each
quantity one) input. currently winning input g, perceived cost, p (g) g
simply p(g). currently winning g particular offer, p (g) = max((g), p(g) +
b ). price previous offer made g , perceived costs increase,
offers max( + , hg,iE p (g)) output g . multiple offers good g,
assumes separate perceived cost respect offer, bids output accordingly.
Figure 8 shows producer would bid next function current prices current
offers, b = 1 2.
Note throughout negotiation, producer places bids output goods
received commitments input goods. Producers counteract potential risk continually updating bids based price changes feasibility status. producer reduces exposure dead
ends incrementing offer prices inputs minimal amounts necessary.
527

fiWALSH & W ELLMAN

Current offer price = 2

p(A) = 1
(A) = 2

C
3

B
p(B) = 2
(B) = 4

Good

Next Offer
Price


B
C

hold 2
2
5

Current offer price C = 3

Current offer price B = 1

Figure 8: producers next offers, according SAMP-SB, b = 1 2. dashed
arrow good B indicates producer currently losing B. solid arrows
indicate producer currently winning goods C.

5.3 Bidding General Preferences
task dependency network model represents fairly simple production capabilities consumer
utility. discuss natural potential extensions bidding policies broader class
capabilities preferences.
producer capable variable-unit production could bid exactly multiple identical
producers. producer would maintain separate offers bids unit, update
separate offers independently. Similarly, consumer additive value multiple goods,
multiple units good, could bid unit good separate consumer.
producer alternatives input, independent inputs, switch bidding
currently cheapest option. Subtle issues arise producer alternative input sets,
particularly tentatively winning parts sets. One option would focus bidding
set lowest perceived cost, may include premium goods tentatively
winning set. Alternatively, producer could assume definitely win tentatively
goods effectively treat sunk costs. Fractional accounting sunk costs may also
reasonable. Similar considerations arise extensions presenting complex consumption choices.
5.4 Properties SAMP-SB
section describe number theoretical properties SAMP-SB. Section 5.4.1
describe properties relating convergence quiesence, Section 5.4.2 present properties relating efficiency convergence price equilibrium, Section 5.4.3 present properties
relating solution convergence.
5.4.1 C ONVERGENCE



Q UIESCENCE

SAMP-SB auctions bidding policies guarantee system always reach quiescence.
Theorem 5 SAMP-SB reaches quiescence finite number bids placed.
However, convergence take long time.
Observation 6 asynchronous environment, possible run protocol may require
number bids exponential network size, function consumer value.
528

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

1-1

2-1

1-A

0
0

1-3

start
2
0

1-B

3-A

0

1

2-3

0
1-2

3-1

2-A

0
2

3-3

0
2-2

2-B

0

0
3-2

3
consumer
1

3-B

0

Figure 9: Network exponential: network may require exponential number bids
reach quiescence.

Figure 9 shows Network exponential, illustrates observation. agent named
start places one-time bid sell one unit good 0 $2. Since (0) = 2, producers 1-1
1-2 initially losing input bids, agents offer price 2 output goods.
Producer 1-3 receive new price quotes goods 1-A 1-B asynchronously, hence may
update bid good 1 twice, offering price 2 first time price 4 second
time. Continuing process, see producer 3-3 updates bid good 3 eight
times. extend network maintain labeling consistent Figure 9, producer n-3
would place O(2n ) bids good n. Note however, bids price quotes propagated
synchronously, exponential growth would occur.
example above, bids actually superfluous meaningfully
affect outcome protocol. appears often true situations exhibiting worst-case
behavior described. capture distinction relevant irrelevant bidding, introduce
notion quasi-quiescence, persistent state subsequent bids effectively
matter solution convergence. SAMP-SB convergence quasi-quiescence requires number
meaningful bids bounded size network value maximum
consumer value.
Definition 5 (quasi-quiescent) run SAMP-SB quasi-quiescent state when, consumer active producer , bids received would change bids
response price quotes already received transmitted auctions.
Clearly, requirements quasi-quiescence subset requirements quiescence.
Observation 7 quiescent state quasi-quiescent state.
Theorem 8 run SAMP-SB reaches quasi-quiescent state, remains quasiquiescent state. Furthermore, neither allocation prices p subsequently change.
theorem means that, quasi-quiescence reached, subsequent bids effectively
matter terms equilibrium solution convergence.
Corollary 9 (to Theorem 8) quiescent state SAMP-SB --equilibrium valid solution
iff first quasi-quiescent state reached --equilibrium valid solution, respectively.
following theorem establishes bound number relevant bids necessary reach
quasi-quiescence.
529

fiWALSH & W ELLMAN

Theorem 10 SAMP-SB reaches quasi-quiescent state number bids bounded polynomial size network value maximum consumer value placed
consumers active producers.
previously mentioned quiescence-detection protocol (Wellman & Walsh, 2000) also detect
quasi-quiescence, thus terminate negotiations reached.
5.4.2 E FFICIENCY



C ONVERGENCE



P RICE E QUILIBRIUM

intentionally use b , parametrize SAMP-SB concept --competitive
equilibrium. interpretation g terms prices ask prices, specify necessary sufficient conditions result SAMP-SB corresponds --competitive
equilibrium.
Theorem 11 prices allocation determined quiescence SAMP-SB protocol
--competitive equilibrium, g = max((g) p(g), b ), iff inactive producer buys
positive-price input.
Theorems 3 11, establish bounds inefficiency --competitive equilibrium, parametrized g = max((g) p(g), b ) good. cases, difference
(g) p(g) may quite high. However, actually establish tighter bound.
Theorem 12 (V 0 , E 0 ) --competitive equilibrium computed SAMP-SB, (V 0 , E 0 )
nonnegative value differs value efficient allocation (|{hg,
E}| b + ) + |C|b .
Note theorem replaces Theorem 3 b bound.
network tree polytree one consumer.
g

Theorem 13 quiescent state SAMP-SB --competitive equilibrium tree.
unaware general network structures SAMP-SB guaranteed converge --competitive equilibrium. However, Theorem 11 implies improve allocations modify SAMP-SB avoid dead ends. say bidding policy safe producer
producer cannot obtain negative surplus quiescence. clear protocol safe
producers, converge --competitive equilibrium.
SAMP-SB assumed producer updates buy sell offers simultaneously
response price quotes. policy safe, even single-input producers,
producer bids input based state standing offer output, rather offer
place. producer would get negative surplus win new output offer
gets stuck winning new input offer. However, slight variant bidding policy,
call safe SAMP-SB, safe single-input producer. protocol, producer updates
input bids would update, currently winning, recent output offer.
Clearly, safe SAMP-SB static properties SAMP-SB, hence Theorem 12 applies
safe SAMP-SB.
Theorem 14 quiescent state safe SAMP-SB --competitive equilibrium network
input complementarities.
530

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Safe SAMP-SB guaranteed safe producers multiple inputs arbitrary
networks, know safe producer bidding policy ensures safety producers
arbitrary network (other degenerate policies bidding).
Safe SAMP-SB may take longer reach quiescence regular SAMP-SB. safe SAMP-SB,
producer must always wait notification results pending output offers increasing input offers. producer win output offer may require propagations many messages
various paths network buyers output good would increase buy offer
prices good. resulting delay would greater local delay communicating
output good auction.
non---competitive equilibrium runs SAMP-SB result dead ends suggests potential
source significant efficiency loss. example, Figure 7 shows result run SAMP-SB
Network greedy-bad. valid solution dead end producer a6. Since producer a3
incurs cost $1 provide good 3 a6, contribute value system,
dead end pure waste global efficiency perspective. allocation undesirable directly
producer a6 committed pay $1 input cannot use. large networks
costs, dead ends result significant efficiency losses negative profits individual agents.
propose contract decommitment protocol remove dead ends SAMP-SB reaches
quiescence. According decommitment protocol, inactive producer decommit
contracts inputs would pay positive price. protocol applied recursively
producers lose outputs due decommitment. decommitment process
terminates, agents exchange goods specified remaining contracts. refer SAMP-SB
decommitment SAMP-SB-D.
Figure 7, producer a6 would decommit contract a3. Clearly, Theorem 11 implies
agent decommits iff SAMP-SB produced --competitive equilibrium. Moreover,
remove consideration producers decommit, remaining agents --competitive
equilibrium.
Decommitment benefit that, whereas producers lose money SAMP-SB
protocol, agent receives negative surplus participating SAMP-SB-D. However,
achieved making auction allocations non-binding, undesirable producers
lose output sales decommitments. also begs question enforce
requirement inactive producers agents decommit.
addition dead ends, efficiency also lost SAMP-SB fails find solution
positive value solution exists, SAMP-SB forms solution value inferior efficient solution (dead ends necessarily mutually exclusive two cases). Section 6
describe experimental analysis efficiency, source inefficiency, equilibrium
attainment SAMP-SB set networks.
5.4.3 OLUTION C ONVERGENCE
Recall SAMP-SB always converges valid solution (specifically --competitive equilibrium) networks tree structures, safe variant converges networks input
complementarities. following theorem shows that, sufficiently high consumer value, regular SAMP-SB always converge (possibly non-equilibrium) valid solution polytrees.

531

fiWALSH & W ELLMAN

Theorem 15 (V, E) polytree solution assigns good g consumer c, given
costs values, exists value vc (g) SAMP-SB guaranteed converge
valid solution (V 0 , E 0 ) c.
dead ends may result, cannot usefully bound inefficiency solution reached
SAMP-SB polytree.
general network structures, prices sell offers consumers goods could rise
values, case system necessarily reach quasi-quiescence nonsolution state. If, however, quasi-quiescence reached price consumers good
reaches value good, valid solution.
Theorem 16 SAMP-SB reaches quasi-quiescence p(g) < vc (g) hg, ci E, c C,
systems state represents valid solution.
next theorem establishes conditions valid solution state immediately
lead quasi-quiescence.
Theorem 17 run SAMP-SB (V, E) valid solution state that:
consumer c either winning offer p(g) + b > vc (g) hg, ci E,
agents correct beliefs goods currently winning,
bids consumers active producers received response current
price quotes,
sell offers lost due tie breaking,
subsequent price quote auction, system quasi-quiescent
state valid solution.
Although SAMP-SB guaranteed converge solution, fact problem finding solution NP-Complete (Walsh et al., 2003) lead us expect problems
SAMP-SB would converge solution exponential number meaningful
bids. Since number meaningful bids bounded polynomial maximum consumer
value, expect exist networks SAMP-SB converge
solution exponential consumer values. practice find construct problems consumer value must exponential order SAMP-SB converge
solution (Walsh et al., 2003). However, run many simulations required value
much reasonable (Walsh et al., 2003).
networks, costs, values, SAMP-SB cannot converge valid solution
values b , matter high consumer value. One example (the simplest
able construct) Network no-converge, shown Figure 10. Observe solution
must include agent a8, cannot include a7. Agent a6 always offers price least p(2) + 20
good 4, hence a8 cannot win two units good 4 less p(2) + 20 each. Thus agent a8
always offer price least 2p(2) + 40 good 5. Since agent a7 never offer price
2p(2) + 22a7 good 5, agent a8 could win good 5 2a7 20. But, occur,
must b 20. thorough analysis, taking account dynamics SAMP-SB,
shows must b 40 = 0 obtain valid solution quiescence,
certain patterns asynchrony.
532

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

a2
a1

1

0

2

a7

20

0

a3

5

0
a6
3

cons

a8

4

0

0

a4

a5

20

20

Figure 10: Network no-converge: network SAMP-SB cannot converge solution
certain values b .

6. Empirical Performance SAMP-SB
Whereas analytic results provide insight SAMP-SB variants, support comprehensive characterization performance, except certain special-case network structures. order gain understanding effectiveness SAMP-SB SAMP-SB-D,
performed empirical study based protocol simulations sample task dependency networks.
6.1 Setup
investigation focuses small set networks exhibiting variety structural properties:
simple (Figure 11), unbalanced (Figure 12), two-cons (Figure 13), bigger (Figure 14),
many-cons (Figure 15). also also studied Network greedy-bad (Figure 4).
a1

1

a3

3

c1
1.216

a2

2

a4

Figure 11: Network simple.
ran experiments multiple instances network. instance randomly
chose producer costs uniformly [0, 1], consumer network, calculated
fixed value that, excluding consumers, exists positive-surplus solution
consumer 0.9 probability. determined consumer values via simulation, assuming
specified distributions producer costs. discarded instances whose efficient solutions
value zero. set b = = .01.
test effect competitive equilibrium existence performance protocols,
generated instances unbalanced, two-cons, greedy-bad costs admit
competitive equilibrium costs not. simple many-cons polytrees,
533

fiWALSH & W ELLMAN

1

a1

a8
8

a2

a9

2

a13
a3

3
9

a10
a4

4

a5

5

a6

6

a7

7

15

c1
3.73

a14

a11

a15
a12

10

Figure 12: Network unbalanced.

a3
a1

3

1

c1
1.23

a4
4

c2
2.17

a2

2
a5

Figure 13: Network two-cons.

a1

1

a17

a9

a2

2

a3

3

a10

a18

a11
10
a12

a4

4

a5

5

a13

a6

6

a14

a7

7

a15

a25
13

9

a19

a26
a27
14
a28

a20

17
11

a21

a29
15

a22

a30

a23

a31
16

12
a8

8

a16

a24

a32

Figure 14: Network bigger.

534

c1
3.51

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

a1

1

a2

2

a3

3

a4

4

a5

5

a6

6

a13

13
a19

19

c1
4.42

a14

14

a20

a15

15
20

a7

7

3.89
a16

a8

8

a9

9

a10

10

a11

11

a12

12

c2

16

a21
a17

17
a22

21

c3
4.44

a18

18

Figure 15: Network many-cons.
know Theorem 2 instances thereof competitive equilibria. able
generate no-equilibrium instances bigger given cost distributions.
generate instance desired type cost structure (equilibrium no-equilibrium)
repeatedly chose sets producer costs randomly uniform distribution desired
property met. experiments, determined whether competitive equilibrium existed
given complete information network structure, values, costsusing following
procedure. Recall competitive equilibrium always efficient (Corollary 4). Hence, given
optimal allocation (V , E ), attempt solve system linear equations characterize
competitive equilibrium, described Section 4.1. solution equations exists,
resulting prices constitute competitive equilibrium, otherwise equilibrium exists. used
CPLEX, commercial mixed-integer-linear programming package, find efficient allocation
solve corresponding equilibrium equations.
type cost structure network, tested 100 random instances, exception simple, tested 3220 instances.5 instance protocol, measured efficiencythe fraction efficient valueattained SAMP-SB SAMP-SB-D.
also measured percentage available surplus (i.e., percentage value optimal
solutions) obtained producers.
6.2 Results
classify efficiency run protocols one four ways: Negative, Zero, Suboptimal
(but positive), Optimal efficiency. Table 1 shows distribution efficiency classes
experiments. Note SAMP-SB-D cannot produce negative efficiency, construction.
5. tested instances simple part broader study (Walsh et al., 2000).

535

fiWALSH & W ELLMAN

Network
simple
unbalanced, case:
equilibrium exists
equilibrium exists
two-cons, case:
equilibrium exists
equilibrium exists
bigger
many-cons
greedy-bad, case:
equilibrium exists
equilibrium exists

SAMP-SB
% instances
Neg Zero Sub Opt
0.0
0.3
0.0 99.7

SAMP-SB-D
% instances
Zero Sub
Opt
0.3
0.0
99.7

5.0
100.0

1.0
0.0

7.0
0.0

87.0
0.0

1.0
100.0

1.0
0.0

98.0
0.0

11.0
18.0
0.0
27.0

0.0
0.0
0.0
0.0

6.0
78.0
4.0
56.0

83.0
4.0
96.0
17.0

0.0
1.0
0.0
0.0

3.0
95.0
0.0
2.0

97.0
4.0
100.0
98.0

4.0
100.0

0.0
0.0

21.0
0.0

75.0
0.0

1.0
100.0

0.0
0.0

99.0
0.0

Table 1: Distribution efficiency classes SAMP-SB SAMP-SB-D. Efficiency classes:
Negative (Neg), Zero, Suboptimal (Sub), Optimal (Opt).

Recall (from Section 5.4.2) efficiency loss SAMP-SB attributable three,
necessarily exclusive, causes: dead ends, failure form solution positive-valued
solution exists, finding suboptimal solution. infer percentage instances exhibiting dead-end suboptimality SAMP-SB examining differences SAMP-SB-D
SAMP-SB totaled Negative, Zero, Suboptimal columns Table 1. Decommitment
affect contribution no-solution suboptimal-solution losses, helps reveal
eliminating dead-end suboptimality. Hence, infer percentage instances exhibiting no-solution suboptimal-solution suboptimality SAMP-SB examining Zero
Suboptimal columns SAMP-SB-D, respectively.
Table 2 shows average efficiency attained protocols, factored network equilibrium existence (where relevant). see, difference SAMP-SB-D
SAMP-SB columns, dead ends significant source inefficiency. Additionally, existence
competitive equilibrium significant effect performance protocols. networks, SAMP-SB-D produces nearly perfect efficiency competitive equilibrium exists (recall
studied instances simple, bigger, many-cons equilibria), much less
effective equilibrium exist, fact failing find solutions no-equilibrium
cases unbalanced greedy-bad.
check whether differences performance significant, performed Students tTests protocol, comparing mean efficiencies instances admit competitive equilibrium means instances admit competitive equilibrium. Table 3 shows
results, indicating p-values means equilibrium no-equilibrium instances came
underlying population. typical analyses, null hypothesis means
equal rejected p-value 0.05. threshold, seems safely reject
hypothesis mean efficiencies equilibrium non-equilibrium instances
536

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Network
simple
unbalanced
equilibrium exists
equilibrium exists
two-cons, case:
equilibrium exists
equilibrium exists
bigger
many-cons
greedy-bad, case:
equilibrium exists:
equilibrium exists:

SAMP-SB
0.997

SAMP-SB-D
0.997

0.867
20.080

0.990
0.000

0.733
0.268
1.000
0.120

0.986
0.686
1.000
0.996

5.320
18.230

0.990
0.000

Table 2: Average efficiency network protocols.

Network
unbalanced
two-cons
greedy-bad

SAMP-SB

SAMP-SB-D

6.27 1030
5.15 107
1.41 101

8.23 10101
1.43 1022
8.04 10101

Table 3: P-values computed Students t-Test. t-Test compared means efficiencies
instances admit competitive equilibrium admit competitive
equilibrium.

537

fiWALSH & W ELLMAN

Networks unbalanced greedy-bad. Inspection data supports conclusion, SAMP-SB-D essentially always produces zero efficiency, produces perfect efficiency
many instances admit competitive equilibrium.
face it, high SAMP-SB/greedy-bad p-value suggests cannot safely
reject hypothesis mean efficiencies differ equilibrium no-equilibrium
instances network. However, inspection data indicates high probability results one outlying equilibrium instance large negative efficiency. Indeed, fact
SAMP-SB-D always produces essentially optimal results instances admit competitive equilibrium, predominantly produces suboptimal results instances without equilibria,
suggests unlikely equilibrium no-equilibrium means SAMP-SB
Network greedy-bad.
Network
simple
unbalanced
equilibrium exists
equilibrium exists
two-cons, case:
equilibrium exists
equilibrium exists
bigger
many-cons
greedy-bad, case:
equilibrium exists
equilibrium exists

% --Competitive
Equilibrium
100
88
0
83
2
96
17
75
0

Table 4: Percentage instances SAMP-SB attained --competitive equilibrium.
Table 4 shows percentage instances SAMP-SB attained --competitive equilibrium network. straightforward determine whether --competitive equilibrium
attained observing whether dead ends (Theorem 11). Again, see strong
connection existence competitive equilibrium. One notable exception many-cons
(which always admits competitive equilibrium), SAMP-SB frequently produced dead
ends. see --competitive equilibria form small percentage no-equilibrium
two-cons instances, although prevalent phenomenon b parameters
chose.
Table 5 shows average efficiency, factored --competitive equilibrium attainment
(SAMP-SB SAMP-SB-D produce results --competitive equilibrium attained). must careful drawing conclusions statistics because, given
network case, relatively many --competitive equilibrium instances (Table 4).
Still, note certain salient trends. --competitive equilibrium runs produce near perfect efficiency, smaller degrees inefficiency specified bounds Theorem 12.
allocation produced SAMP-SB --competitive equilibrium iff dead ends,
expect significant portion efficiency loss non---competitive equilibrium pro538

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Network
simple
unbalanced
equilibrium exists
equilibrium exists
two-cons, case:
equilibrium exists
equilibrium exists
bigger
many-cons
greedy-bad, case:
equilibrium exists
equilibrium exists

--Equilibrium
Found
SAMP-SB SAMP-SB-D
N/A
N/A

--Equilibrium
Found
SAMP-SB/ SAMP-SB-D
0.997

0.248
20.08

0.989
0.000

0.998
N/A

0.570
0.130
0.997
0.060

0.920
0.707
1.000
0.995

1.000
1.000
1.000
1.000

24.28
18.22

0.960
0.000

1.000
N/A

Table 5: Average efficiency network protocols, factored --competitive equilibrium attainment.

ducing runs SAMP-SB would attributable negative surplus incurred dead ends.
significant differences efficiency SAMP-SB-D SAMP-SB shown non--competitive equilibrium column provides evidence hypothesis. Indeed, appears
surplus lost dead ends (as opposed suboptimal solution attainment) dominant cause
inefficiency --competitive equilibrium attained. instances, improvement
decommitment greater difference efficiency SAMP-SB-D optimal
efficiency.
Table 6 shows average fraction available surplus obtained producers, respectively,
network. Perhaps surprisingly, networks producers gain significant surplus
SAMP-SB-D protocol, even though bidding obtain zero surplus. reason
producers output offer indicates minimum amount willing accept
exchange output. rising buy offers cause price rise producers output
offer. could happen cases necessary block certain agents feasible
allocation quiescence. Note however, decommitment step needed producers
obtain high average surplus. Without decommitment, average producer surplus highly
negative, shown SAMP-SB column.

7. Related Literature
section discuss literature related present work. Section 7.1 discuss related
literature price-based analysis auction theory, Section 7.2 discuss related literature
supply chain formation.
539

fiWALSH & W ELLMAN

Network
simple
unbalanced, case:
equilibrium exists
equilibrium exists
two-cons, case:
equilibrium exists
equilibrium exists
bigger
many-cons
greedy-bad, case:
equilibrium exists
equilibrium exists

SAMP-SB
0.000

SAMP-SB-D
0.000

0.041
20.09

0.082
0.000

0.210
0.137
0.001
0.517

0.464
0.555
0.001
0.359

6.08
18.11

0.137
0.000

Table 6: Average fraction available surplus obtained producers network protocols.

7.1 Price-Based Analysis Auction Theory
shown special cases competitive equilibria exist task dependency networks (polytree single-input-producer networks), SAMP-SB always finds --competitive
equilibrium trees, minor variant always finds --competitive equilibria single-inputproducer networks. review results price equilibrium auction theory reveals
limited positive results typical.
well-known given arbitrarily divisible goods convex utility, cost, production functions, competitive equilibrium prices exist. additionally, gross substitutes condition
(which generalization no-complementarities) met, classic tatonnement procedure finds
competitive equilibrium distributed manner.6
goods discrete, competitive equilibria exist exchange (non-production) economies
gross substitutes conditions met (Bikhchandani & Mamer, 1997; Gul & Stacchetti, 1999;
Kelso & Crawford, 1982). Milgrom (2000) showed existence single complementarity
sufficient preclude equilibrium exchange economies. Bickhchandani Mamer (1997)
also show existence variety conditions, appear natural interpretations task dependency networks. exchange economies, gross substitutes condition
also ensures convergence (approximately) competitive equilibria simultaneous ascending
auctions (Demange et al., 1986; Gul & Stacchetti, 2000).
distributed price-based auction protocols leave agents undesired goods
preferences complementary (e.g., dead ends task dependency network), widely recognized problem. alternative approach use combinatorial auction, mediates negotiation single location, performing global matching combinations goods based indivisible
bids. general approach received much attention AI community late, motivated
6. reader may consult standard microeconomic textbook (Mas-Colell et al., 1995) details results.

540

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

part techniques quickly performing necessary global optimization (Andersson et al.,
2000; Leyton-Brown et al., 2000; Sandholm & Suri, 2000).
Currently, results combinatorial equilibria auctions established onesided (i.e., buyer only) bidding. Bikhchandani Ostroy (2002) Wurman Wellman (2000),
using different combinatorial frameworks, provide positive results equilibrium existence,
properties thereof. Wurman Wellman describe combinatorial auction framework.
Parkes Ungar (2000) describe combinatorial auction guaranteed converge
efficient allocation agents follow myopic best-response strategies. adding extend-andadjust phase, authors able obtain allocation ex post Nash equilibrium (Parkes
& Ungar, 2002). Ausubel Milgrom (2002) present proxy-auction mechanism obtains
efficient allocations straightforward bidding equilibrium goods substitutes.
present work consider simple, local, myopic bidding policies. policies
non-strategic, agents reason effect negotiations attempt
extract greater surplus. assumption non-strategic behavior plausible
large number agents. networks many agents bidding individual goods, many parallel
branches, many agents sequence, potential contribution one agent value
solution relatively small little gain strategic behavior.
experiments shown that, even producers bid obtain zero surplus specified policy, obtain positive surplus networks. Nevertheless, smaller networks,
potential strategic improvement pronounced, non-strategic assumption becomes less plausible. widely studied concept used analyzing strategic behavior Bayes-Nash
equilibrium.7 Informally, set strategies constitutes Bayes-Nash equilibrium single agent
incentive deviate strategy, given agents play Bayes-Nash equilibrium strategies. McAfee McMillan (1987) Klemperer (1999) survey state knowledge strategic analysis auctions exchange economies. Milgrom (2000) provides insights
fundamental challenges understanding agent behavior complementary
preferences. However, definitive results known quite restrictive market structures,
encompass two-sided markets complementarities, never mind multi-level characteristic negotiation task dependency networks. problem even specifying information
structure extensive form game simultaneous ascending (M+1)st price auctions task dependency networks, prerequisite computing Bayes-Nash equilibria, well beyond current
state art game-theoretic analysis.
auction theory currently fails provide satisfactory guidance understanding strategic
behavior even moderately complicated domains, used tournaments framework
developing evaluating candidate agent strategies. Santa Fe Double Auction Tournament (Rust et al., 1994) provided unexpected insights effective strategies continuous
double auctions, recent TAC series trading agent competitions (Wellman et al., 2001b,
2003) encouraged development sophisticated agent strategies (Greenwald, 2003; Stone &
Greenwald, 2000) complex market game.
Vickrey-Clarke-Groves mechanism (Clarke, 1971; Groves, 1973; Vickrey, 1961), also
called Generalized Vickrey Auction (GVA) (MacKie-Mason & Varian, 1994), direct revelation approach, agents report valuations allocations, auction computes
lump-sum payment. GVA, solution optimal allocation based reports,
7. foundational reference, Chapter 7 Fudenberg Tiroles game theory text (1998) provides formal treatment strategic issues auction mechanism design analysis.

541

fiWALSH & W ELLMAN

payment function dominant strategy agents report true utility.
incentive compatibility perfect efficiency, GVA may seem ideal
economic perspective (although computation intractable). However, GVA budget balancedwhen buyers sellers bid, GVA pay money takes
in. Unfortunately unavoidable, impossible simultaneously ensure efficiency, budget
balance, individual rationality (no agent achieves negative surplus) (Myerson & Satterthwaite,
1983). Recently, Babaioff et al. (Babaioff & Nisan, 2001; Babaioff & Walsh, 2003) described
distributed auction mechanisms, based McAfees trade reduction auction (1992), obtain incentive compatibility budget balance linear supply chains, expense perfect efficiency.
Recent work Parkes, Kalagnanam, Eso (2001) explores methods minimize deviation
efficiency maintaining budget balance two-sided GVA-like mechanisms.
7.2 Supply Chain Formation
Supply chain managementthe problem accurately forecasting planning production deliveries meet demand minimize inventoryis active field study operations research.
problem management differs supply chain formation exchange partners
supply chain pre-established, assumed information gathered
agents effectively optimize global production across supply chain. contrast, work
approached problem automating process determining supply chain participants dynamically, assumption information decision making decentralized. Readers
interested supply chain management may refer Kjenstad (1998) extensive review.
Relatively less effort explicitly devoted problem cast supply chain formation, despite rhetorical appeals decentralized dynamic relation-building commonplace
popular literature. Nevertheless, point Section 3, venerable AI methodsin
particular widely-known CONTRACT NET protocolcan principle applied supply chain
formation. discussed Section 3, standard CONTRACT NET mechanisms
resolve nontrivial resource contention, precluding systematic comparison SAMP-SB general network structures. can, however, compare protocols network structures
resource contention mechanism necessary CONTRACT NET. clear agents
bid true costs, CONTRACT NET greedy allocation converge optimal allocations trees. holds tree structures relaxed allow multiple-unit input bids.
shown, SAMP-SB guaranteed converge approximately efficient allocations trees.
However, shown may converge good solutions multiple unit input bids
allowed. latter case, competitive equilibrium may exist, observed
equilibrium non-existence substantially hurt efficiency SAMP-SB allocations. contrast,
producers may receive different prices good CONTRACT NET. discriminatory pricing mechanism makes CONTRACT NET robust presence complementarities,
class network structures.
Sandholm (1993) examines specialization CONTRACT NET generalization Task Oriented Domains (TODs) (Rosenschein & Zlotkin, 1994). Agents begin initial allocation
tasks negotiate task exchanges mutually beneficial trades. Sandholms model
allows local constraints task achievement agent perform certain combinations tasks. However, dependency structurean agent rely agents
task achievement order accomplish tasks. Thus, every locally feasible trade results
542

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

globally feasible allocation, executed immediately independently trades.
cannot generally apply incremental trading protocol task allocation model subtask
dependencies. local exchange may require reallocation throughout entire network maintain
production feasibility.
Andersson Sandholm (1998) find decommitment protocols increase quality
resulting allocations variants TODs. incremental trading, decommitment gives agents
opportunity engage cost-effective contracts. Andersson Sandholm also consider decommitment penalties provide friction reallocation compensate agents whose
contracts broken. expect penalties would appropriate extension
SAMP-SB-D protocol.
Veeramani et al. (Veeramani et al., 1999; Joshi et al., 1999) consider issues arising simultaneous negotiation multiple subtasking issues various levels supply chain.
asynchronous model, agents may opportunity finalize contract negotiations
still pending. uncertainty induces complex decision problem agents wish
overextend commitments.
Hunsberger Grosz (2000) study problem assigning task performance roles agents
SharedPlans collaborative planning framework. model based recipes, describe precedence constraints execution time across various sub-tasks constitute
complex task. Contention shared resources modeled explicitly recipe, individual agents may additional cost, timing, constraints, potentially arising
individual resource limitations. Hunsberger Grosz use combinatorial auction assign tasks
agents, given constraints, produce high-valued shared plan. find limiting task assignment certain combinations roles effect tradeoff computational allocative
efficiency.
work (Walsh et al., 2000; Walsh, 2001), studied strategic behavior agents
bidding particular one-shot combinatorial auction within task dependency network model.
empirically compared performance SAMP-SB, SAMP-SB-D, combinatorial auction (with strategic bidding). combinatorial auction eliminates problem dead ends allocating inputs outputs producers all-or-nothing basis. advantage notwithstanding,
combinatorial auctions may always appropriate mechanism. Since finding feasible
supply chain solution NP-hard (Walsh et al., 2003), sufficiently large problems intractable,
even advanced optimization procedures. Even computation tractable, social factors
may limit authority one entity compute allocations entire supply chain.

8. Extensions Future Work
task dependency network model propose provides basis beginning understand
automation supply chain formation. discussed ways extend bidding policies market protocol accommodate general production capabilities consumer
preferences. extensions model capabilities preferences multi-attribute
goods (e.g., goods multiple features quality delivery time, addition price
quantity) simply representing configuration distinct good network. However,
clear explode number goods attributes. effectively handle
greater numbers attributes would require multiattribute auctions (Bichler, 2001), multiple
inseparable features exchange negotiated simultaneously.
543

fiWALSH & W ELLMAN

realistic scenarios, producers may also solve complex internal scheduling, planning, forecasting, complex problems order evaluate costs feasible options.
types extensions would increase fidelity model, would implications
agent bidding policies computation convergence speed market protocols. Despite
best efforts agents forecast plan, agents cannot predict certainty operation
within formed supply chain. Sophisticated agents would employ probabilistic reasoning techniques evaluating options negotiation. unexpected events occur impair
operation formed supply chain, agents would need protocols repairing reforming
supply chain.

assumed simple set non-strategic, myopic bidding policies simultaneous ascending auction. agents must coordinate input output bids dynamic auction mechanism,
understanding strategic bidding behavior challenging unsolved, albeit important problem
future work. seems likely significant developments game-theoretic methodology would
necessary analytically solve, even realistically specify, extensive form games incomplete
information corresponding asynchronous iterative auctions. meantime, make progress
understanding performance auctions, consider alternate approaches developing
good bidding policies. Tournaments proven effective ways encourage smart
people design smart trading policies evaluate relative qualities (Rust et al., 1994;
Wellman et al., 2001b). Axelrod (1987) used evolutionary approach evaluate populations
strategies, fixed types, iterated prisoners dilemma. major challenge applying
evolutionary approach supply chain formation problem develop sufficiently rich, yet
reasonably searchable set agent bidding policies.

suggested decommitment solution problem dead ends SAMP-SB,
strategic analysis protocol would take phase account. producers
could lose money decommitment allowed, expect producers would
willing participate, would also aggressive bidding. Allowing decommitment begs question enforce producers decommit dead
ends, also address fact unilateral decisions decommitment potentially
break (possibly desirable) contracts many downstream producers. reduce aggressive
bidding mitigate potential problems, could charge penalties producers initiate
decommitment (Andersson & Sandholm, 1998), perhaps paid producers whose output contracts get decommitted. would reduce spurious decommitments still allowing
producers stuck costly dead ends.

Finally, note market configuration studied hereseparate auctions good
represents one possible partition scope negotiations supply chain.
extreme, production activity could mediated one combinatorial auction mechanism covering
entire supply chain (Walsh et al., 2000). avoids coordination pitfalls separate
auction approach, imposes disadvantages associated imposing mechanism
global scope. Intermediate configurations, involving multiple auctions clusters highly related
goods, represent promising alternative investigation.
544

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

ACKNOWLEDGMENTS
paper includes material previously presented Sixteenth International Joint Conference
Artificial Intelligence (IJCAI-99) (Walsh & Wellman, 1999). work supported part
NSF grant IIS-0205435.

Appendix A. Proofs
appendix provides proofs theorems. convenience, restate theorems
proofs.
proofs, sometimes useful index position producer network. Clevel producer output g maximum distance consumer, formally stated
follows: one producer, consumer, g input, k + 1 maximum level
producer input g k. S-level producer defined similarly, respect
distance producer input, basis zero producers
inputs themselves. C-level S-level well defined, acyclicity.
A.1 Proof Theorem 1
Let (V, E) network input complementarities, producers one
input, let (V , E ) optimal allocation (V, E). convenience, partition producers
sets 1 , producers single input, 0 , producers inputs.
Procedure Input Complementarities Equilibrium constructs prices support
competitive equilibrium (V , E ).
Input Complementarities Equilibrium:
1. Initialize prices zero.
2. Perform following price changes made:
(a) c C \V , vc (g) > p(g), hg, ci E \ E ,
p(g) vc (g).
(b) c C V , vc (g0 ) p(g0 ) > vc (g) p(g) 0,
hg, ci E hg0 , ci E \ E ,
p(g0 ) vc (g0 ) (vc (g) p(g)).
(c) 0 V , p(g ) < , h, g E ,
p(g ) .
(d) 1 V , p(g ) < p(g) +
h, g E hg, E ,
p(g ) p(g) + .
(e) 1 \V , p(g ) > p(g) + ,
h, g E \ E hg, E \ E ,
p(g) p(g ) .
network (V, E) (with input complementarities) prices p, closed, reverse-surplus sequence directly connected sequence agents goods every agent would better
reversing allocation. Formally, sequence (n1 , . . . , nk ) vertices ni V , that:
545

fiWALSH & W ELLMAN

1. hni , ni+1 E hni+1 , ni E 1 k 1.
2. nk G.
3. n1 (C \V ) (0 V ).
(a) n1 C \ V , hn2 , n1 E \ E n1 would obtain nonnegative surplus p
obtaining n2 . 1 = k 1, n1 would obtain strictly positive surplus p
obtaining n2 .
(b) n1 0 V , hn1 , n2 E n1 would obtain nonpositive surplus p
active. k = 2, n1 would obtain strictly negative surplus p
active.
4. 2, ni ni 1 (C V ).
(a) ni 1 V , hni1 , ni E , hni , ni+1 E , ni would obtain nonpositive
surplus p active. = k 1, ni would obtain strictly negative surplus
p active.
(b) ni 1 \ V , hni+1 , ni E , hni , ni1 E , ni would obtain nonnegative
surplus p active. = k 1, ni would obtain strictly positive surplus
p active.
(c) ni C V , hni1 , ni E , hni+1 , ni E \E , ni would obtain less surplus
ni+1 ni1 p. = k 1, ni would obtain strictly better surplus
ni+1 ni1 .
open reverse-surplus sequence closed, reverse-surplus sequence except
that, instead Condition 3, n1 G n2 1 (C V ) Condition 4. Clearly
closed, reverse-surplus sequence length greater two contains open, reverse-surplus
sequence.
Lemma 18 Procedure Input Complementarities Equilibrium reach state
open, reverse-surplus sequence K = (n1 , . . . , nk ) constituting cycle n1 = nk
k 3.
Proof. Assume, contrary wish prove, cycle K prices p.
Moreover, let cycle smallest, contains cycle.
show create alternate, feasible solution (V 0 , E 0 ) higher value (V , E ),
giving us contradiction. Initialize (V 0 , E 0 ) = (V , E ). n j , 1 j < k, hn j , n j+1
E , remove edge E 0 , edge E \ E , add edge E 0 . Also, add remove
vertices necessary consistent added removed edges.
producer (V 0 , E 0 ) feasible feasible (V , E ) input,
either input output added, removed, neither changed. Consider good
n j G, 1 < j < k. Since j 1 > 1, must agents n j1 n j+1 1 (C V ).
inspecting Conditions 4(a)4(b) definition closed, reverse-surplus sequence (which
also apply open reverse-surplus sequence), see edges incident n j added
removed way n j material balance. Similarly, considering agents nk1 , n2 ,
546

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

good n1 = nk , material balance good n1 = nk . Since goods material balance
producers feasible, (V 0 , E 0 ) feasible.
surpluses agents K unaffected transformation. definition open,
reverse-surplus sequence, every agent K obtains lower surplus p transformation,
agent nk1 obtains strictly higher surplus p. value feasible allocation
sum agent surpluses particular prices (Lemma 22), must value((V 0 , E 0 )) >
value((V , E )). contradicts optimality (V , E ), assumption K exists
must false. 2
Lemma 19 price good nk increases Procedure Input Complementarities
Equilibrium, exists finite closed, reverse-surplus sequence (n1 , . . . , nk ) prices
p price increase.
Proof.
show construct desired closed, reverse-surplus sequence, referring
conditions definition, steps Procedure Input Complementarities
Equilibrium. price increase nk occurred one Steps 2(a)2(e), triggered agent
nk1 . Since step triggered, nk1 would obtain strictly better surplus reversing allocation p, specified conditions 3(a), 3(b), 4(a)4(c). price nk increased Step 2(a) 2(c), desired closed, reverse-surplus sequence,
nk1 (C \V ) (0 V ) k 1 = 1. Otherwise, price nk increased Step 2(b),
2(d), 2(e), nk1 1 (C V ) k 1 > 2. case, let nk2 good also
matched condition step.
price nk2 increased, Procedure Input Complementarities Equilibrium
ensures find agent nk3 matching one Conditions 3(a), 3(b), 4(a)4(c). If,
hand, p(nk2 ) = 0, producers positive costs consumers positive
values, also find agent nk3 . find agent corresponds condition 3(a)
3(b), k 3 = 1 done. Otherwise, find good nk4 , nk2 ,
continue manner.
Clearly, process constructs open, reverse-surplus sequence. Now, must show
process selecting vertices eventually selects element n1 (C \V ) (0 V ). Since (V, E)
finite, since Lemma 18 cycles open, reverse-surplus sequence,
must eventually find n1 (C \V ) (0 V ) give us closed, reverse-surplus sequence. 2
Lemma 20 Procedure Input Complementarities Equilibrium terminates.
Proof. Assume, contrary wish prove, procedure terminate
price good g increases infinite number times. Consider cycle K = (n1 = g, . . . , nk = g)
vertices ni V , k 3 that:
1. hni , ni+1 E hni+1 , ni E {1, . . . , k 1}.
2. {2, . . . , k 1}, ni 6= g.
3. {3, . . . , k}, ni G, price increase good ni occurred one Steps 2(b),
2(d), 2(e) procedure, agent ni1 good ni2 also matched condition
step. Furthermore, price increase ni2 , triggered agent ni3 good ni4 , caused
need price increase good ni .
547

fiWALSH & W ELLMAN

price g increases infinite number times, cycle must exist.
Let p prices p(n1 ) n1 agent n2 triggered price increase
n3 , ni G 1 < < k, p(ni ) increased, triggered
agent ni1 good ni2 . price goods arbitrary nonnegative number.
way constructed p, way prices increased procedure, K must
open, reverse-surplus sequence. Lemma 18, K cannot exist. Therefore,
procedure terminates. 2
Theorem 1 Competitive equilibria exist network input complementarities.
Proof. show Procedure Input Complementarities Equilibrium terminates
prices p every agent obtaining maximum surplus according (V , E ) . Since (V , E )
efficient, also feasible, giving us competitive equilibrium prices p.
Lemma 20, procedure terminates. Clearly, procedure terminates, agents
1 (0 V ) (C \ V ) optimize according (V , E ). remains show (C
V ) (0 \V ). Assume, contrary wish prove, (C V ) (0 \V )
optimize according (V , E ).
Consider case (C V ) hg, ai E . Since algorithm guarantees
prefer good g0 g prices p, must p(g) > vc (g). Let p0
prices immediately price g rose vc (g) p00 prices immediately after.
Lemma 19, closed, reverse-surplus sequence (n1 , . . . , nk = g) prices p0 . p00 ,
conditions closed, reverse-surplus sequence hold, except surplus condition 4(a),
4(b), 4(c) applies nk1 becomes non-strict. However, obtains strictly negative surplus
p00 . Denote nk+1 .
create alternate, feasible solution (V 0 , E 0 ) proof Lemma 18 adding
edges hni , ni+1 E \ E , removing edges E , {1, . . . , k}.
surpluses agents K unaffected transformation. Every agent (a1 , . . . , nk1 )
obtains lower surplus p00 transformation. Agent = nk obtains zero surplus
transformation, higher negative surplus before. value
feasible allocation sum agent surpluses particular prices (Lemma 22), must
value((V 0 , E 0 )) > value((V , E )). contradicts optimality (V , E ), must
p(g) vc (g) obtaining maximum surplus p (V , E ).
If, hand, (0 \V ), ha, gi E. must < p(g). use
line proof case C V show (V , E ) suboptimal value, providing
contradiction. Thus must optimize according (V , E ) p.
Thus shown algorithm terminates agents optimizing according
(V , E ) p. Thus p supports competitive equilibrium allocation (V , E ). 2
A.2 Proof Theorem 2
Given polytree (V, E) efficient allocation (V , E ), present Procedure Polytree
Equilibrium constructs lower bounds p (g) upper bounds p (g) prices
goods g, turn uses bounds construct prices p goods. prove
resulting prices fact competitive equilibrium prices support (V , E ).
548

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Observe that, purposes competitive equilibrium pricing, treat consumer c
wishes obtain one good set Gc consumer desires single good gc
value vc (gc ) = vc = maxgGc vc (g), along additional producers. g Gc create
producer output gc , input g, = vc vc (g). Thus, without loss generality,
consider consumers preferences single goods. denote gc good consumer
c desires denote vc value c gc .
refer n0 V either hn, n0 E hn0 , ni E neighbors vertex
n V . use refer null vertex neighbor vertex.
Polytree Equilibrium:
1. g G, p (g) 0 p (g) .
2. connected subgraph (V , E) (V, E), select g G V arbitrarily:
Perform Set Bounds(g, ).
p(g) p (g).
Set Bounds recursively visits vertices, updating price bounds postorder (i.e.,
recursion unwinds) setting prices either lower upper bounds. (V, E)
polytree, procedure sets price good exactly once.
Set Bounds(n, r), n A, r G procedure either updates p (r) p (r)
bounds neighbors n, r, fixed. updates p (r),
way n
/ V n, active, would get nonpositive surplus p(r) p (r),
given bounds neighbors n, n V n, active, would get nonnegative surplus p(r) p (r), given bounds neighbors n. Since p (r)
increases (Steps 2, 4(b), 5(c)), property maintained. Similarly, Set Bounds(n, r)
updates p (r), way n
/ V n, active, would get nonpositive
surplus p(r) p (r), given bounds neighbors n, n V
n, active, would get nonnegative surplus p(r) p (r), given bounds
neighbors n. Since p (r) decreases (Steps 3, 4(c), 5(b)), property maintained.
Set Bounds(n, r):
1. neighbor z n z 6= r, perform Set Bounds(z, n).
2. n C \V ,
p (r) max(vn , p (r)).
3. Else n C V ,
p (r) min(vn , p (r)).
4. Else n \V then,
(a) neighbor g n g 6= r
g input n
p(g) p (g).
Else g output n,
p(g) p (g).
549

fiWALSH & W ELLMAN

(b) r input n,
output, gn , n
p (r) max(p (r), p (gn ) hg,niE, g6=r p (g) n ).
(c) Else r output n,
p (r) min(p (r), hg,niE p (g) + n ).
5. Else n V then,
(a) neighbor g n g 6= r,
g input n,
p(g) p (g).
g output n,
p(g) p (g).
(b) r input n,
output, gn , n),
p (r) min(p (r), p (gn ) hg,niE, g6=r p (g) n ).
(c) Else r output n,
p (r) max(p (r), hg,niE p (g) + n )

Lemma 21 Procedure Polytree Equilibrium computes price bounds p (g) p (g)
goods g G.
Proof. Assume, contrary wish prove, state g G
p (g) > p (g). Assume g first good visited.
say agent constrained p (g) Set Bounds(a, g) last change p (g).
Similarly, say agent constrained p (g) Set Bounds(a, g) last change
p (g).
Recall Lemma 22 value feasible allocation equal sum agent
surpluses particular prices. show transform (V , E ) alternate feasible
allocation (V 0 , E 0 ) compute alternate prices p show sum surpluses (V 0 , E 0 )
greater (V , E ).
First, initialize (V 0 , E 0 ) = (V , E ) good g G initialize p(g) = 0. Next, set p(g) =
p (g). recursively change prices allocation portion subtree rooted
g. Perform Lower Bound(a, g) agent constrained p (g) perform Upper
Bound(a, g) agent constrained p (g).
Throughout transformation, perform Lower Bound(a, g) iff visit g agent
constrained p (g). Similarly, perform Upper Bound(a, g) iff visit g agent constrained p (g). following describes portions transformation.
Lower Bound(a, g):
1. \V , must g input (because constrained p (g)).
neighbor g0 6= g a:
550

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

(a) g0 input a,
p(g0 ) p (g0 ),
perform Upper Bound(a0 , g0 ) agent a0 constrained p (g0 ).
(b) Else (g0 output a),
p(g0 ) p (g0 ),
perform Lower Bound(a0 , g0 ) agent a0 constrained p (g0 ).
2. Else V , must g output (because constrained p (g)).
input g0 a:
p(g0 ) p (g0 ),
perform Lower Bound(a0 , g0 ) agent a0 constrained p (g0 ).
3. V ,
remove incident edges (V 0 , E 0 ).
4. Else V \V ,
add incident edges (V 0 , E 0 ).
Upper Bound(a, g):
1. \V , must g output (because constrained p (g)).
input g0 a:
p(g0 ) p (g0 ),
perform Upper Bound(a0 , g0 ) agent a0 constrained p (g0 ).
2. V , must g input (because constrained p (g)).
neighbor g0 6= g a:
(a) g0 input a,
p(g0 ) p (g0 ),
perform Lower Bound(a0 , g0 ) agent a0 constrained p (g0 ).
(b) Else (g0 output a),
p(g0 ) p (g0 ),
perform Upper Bound(a0 , g0 ) agent a0 constrained p (g0 ).
3. V ,
remove incident edges (V 0 , E 0 ).
4. Else V \V ,
add incident edges (V 0 , E 0 ).
Observe that, (V, E) polytree, vertex visited either Upper
Bound Lower Bound.
show (V 0 , E 0 ) feasible. Consumers always feasible. Producers feasible
add remove incident edges add remove producer, respectively.
prove every g G material balance (V 0 , E 0 ).
551

fiWALSH & W ELLMAN

Consider good g p (g) > p (g). Lower Bound(a, g) performed agent
constrained p (g), occurred 2, 4(b), 5(c) Set Bounds(a, g). Therefore Lower
Bound(a, g) either adds hg, ai E \ E else removes ha, gi E . Upper Bound(a, g)
performed constrained p (g), occurred 3, 4(c), 5(b) Set Bounds(a, g).
Therefore Upper Bound(a, g) either adds ha, gi E \ E else removes hg, ai E .
possible combination, material balance maintained g.
consider good g 6= g. g visited Lower Bound(a, g), p(g) set
p (g) one following ways, immediately prior:
1. p(g) set p (g) 1(b) Lower Bound(a, g), agent \V
good g. case, g output ha, gi E \ E added (V 0 , E 0 ) 4
Lower Bound(a, g).
2. p(g) set p (g) 2 Lower Bound(a, g), agent V
good g. case g input hg, ai E removed (V 0 , E 0 ) 3
Lower Bound(a, g).
3. p(g) set p (g) 2(a) Upper Bound(a, g), agent V
good g. case case g input hg, ai E removed (V 0 , E 0 )
3 Upper Bound(a, g).
One following operations occurred Lower Bound(a, g):
1. constrained p (g) 2 4(b) Set Bounds(a, g), hg, ai E \ E added
(V 0 , E 0 ) 4 Lower Bound(a, g).
2. agent constrained p (g) 5(c) Set Bounds(a, g), ha, gi E removed
(V 0 , E 0 ) 3 Lower Bound(a, g).
possible combination additions removals edges incident g prior to, Lower
Bound(a, g), material balance maintained g. show similar result g visited
Upper Bound(a, g). Hence established feasibility (V 0 , E 0 ).
show agent A, (a, (V 0 , E 0 ), p) (a, (V , E ), p),
agent a0 (a0 , (V 0 , E 0 ), p) > (a0 , (V , E ), p).
agent visited construction (V 0 , E 0 ), (a, (V 0 , E 0 ), p) = (a, (V , E ), p),
allocation (V , E ).
Consider a0 visited Upper Bound(a0 , g). a0 thus visited, a0 constrained

p (g). Upper Bound(a0 , g) sets prices neighbor goods g 6= g prices used
compute p (g) Set Bounds(a0 , g). prices neighboring goods computed
a0 V , a0 would get negative surplus price g p (g) a0 V \V
would get positive surplus price g p (g). But, alternate prices
computed, p(g) = p (g), assume p (g) > p (g). Since a0 V 0
V , (a0 , (V 0 , E 0 ), p) > (a0 , (V , E ), p).
consider A, 6= a0 , visited construction (V 0 , E 0 ). visited
Upper Bound(a, g), p(g) = p (g) must constrained p (g). C,
Set Bounds(a, g) set p (g) va p (g) = 0. , Upper Bound(a, g) sets
prices goods neighboring prices used compute p (g) Set Bounds(a, g).
neighboring prices active feasible, would get zero surplus
552

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

p(g) = p (p). Thus (a, (V 0 , E 0 ), p) = (a, (V , E ), p). If, hand, visited Lower
Bound, p(g) = p (g) must constrained p (g). similar argument used
Upper Bound, gets zero surplus p(g) = p (g). Again, gives (a, (V 0 , E 0 ), p) =
(a, (V , E ), p).
shown agent A, (a, (V 0 , E 0 ), p) (a, (V , E ), p),
agent a0 (a0 , (V 0 , E 0 ), p) > (a0 , (V , E ), p). value((V 0 , E 0 )) >
value((V , E )), contradiction. Hence, initial assumption p (g) > p (g) must
false. p (g) p (g) goods g.
2

Theorem 2 Competitive equilibria exist polytree.
Proof. show agents optimize according (V , E ) prices p computed procedure Polytree Equilibrium. Since (V , E ) feasible definition, resulting prices
allocation constitute competitive equilibrium (V, E).
construction p (g) ensures never decreases, Step 2 Set Bounds ensures every consumer c C \V optimizes p(gc ) p (gc ). p (gc ) p(c) p (gc )
(by construction p Lemma 21), c optimizes according (V , E ). similar argument, every c C V optimizes according (V , E ).
Consider producer \ V , visited Set Bounds(, g). good g input ,
4(a) Set Bounds(, g) sets price every neighbor good g0 6= g price
bounds used compute p (g) Step 4(b) Set Bounds(, g). Moreover, p (g) set
smallest price could get maximum surplus zero, given specified bounds
neighbor goods. Since p (g) could increase subsequently, since p (g) p(g) p(g)
(by construction p Lemma 21), since price good set
(because (V, E) polytree) cannot get positive surplus prices set Set Bounds(,
g). similar argument, g output , Step 4(c) p (g) set largest price
would get maximum surplus zero, given prices set neighbor goods.
Since again, p (g) p(g) p (g), p (g) increases subsequently, price good
set once, must get zero surplus. Thus optimizes according
(V , E ). Symmetrically, see every V optimizes according (V , E ).
shown agents optimize according (V , E ) p, hence shown
competitive equilibrium exists polytree (V, E). 2

A.3 Proof Theorem 3
Lemma 22 value feasible allocation (V 0 , E 0 ), prices p, expressed as:

value((V 0 , E 0 )) =

(a, (V 0, E 0), p).

aA

553

(1)

fiWALSH & W ELLMAN

Proof. Equation (1) expands to:
value((V 0 , E 0 )) =



vc ((V 0 , E 0 ))

+







p(g)



p(g) ((V , E )) .

hg,ciE 0

cC



p(g)

h,giE 0

!
0

hg,iE 0

0

!

Since goods material balance feasible allocation, price terms cancel out.
left

vc ((V 0, E 0)) ((V 0 , E 0)),


cC

original formula value solution (Definition 1). 2
Theorem 3 (V 0 , E 0 ) --competitive equilibrium (V, E) prices p, (V 0 , E 0 )
feasible allocation nonnegative value differs value efficient allocation
[hg,iE g + ] + |C|b .
Proof. refer four conditions --competitive equilibrium (Definition 4). Let (V , E )
efficient allocation (V, E).
Conditions (3) (4) imply (V 0 , E 0 ) feasible. Recall formula value
feasible allocation Equation (1). Since (V 0 , E 0 ) (V , E ) feasible, express
values
value((V 0 , E 0 )) =

(a, (V 0 , E 0), p),

(2)

(a, (V , E ), p).

(3)

aA

value((V , E )) =

aA

c C, Condition (2), (c, (V 0 , E 0 ), p) Hc (p) b . allocation
better agent optimal allocation, (c, (V , E ), p) Hc (p). Thus,
(c, (V , E ), p) (c, (V 0 , E 0 ), p) b .

(4)

, Condition (3), (, (V 0 , E 0 ), p) H (p) (hg,iE g + ).
allocation better agent optimal allocation, (, (V , E ), p) H (p). Thus,
(, (V , E ), p) (, (V 0 , E 0 ), p)



g + .

(5)

hg,iE

Equations (2)(5) together imply value((V , E )) value((V 0 , E 0 )) [hg,iE +
] + |C|b . Condition (1) implies sum term Equation (2) nonnegative, hence
value((V 0 , E 0 )) 0. noted, (V 0 , E 0 ) feasible.
2
g

554

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

A.4 Proof Theorem 5
proving theorem, refer C-level S-level producers network, defined
beginning Section A.
task dependency network (V, E) characterized following parameters:
: maximum C-level producer network,
: maximum number input goods producer,
R: maximum consumer value, maxhg,ciE|cC vc (g).
Lemma 23 run SAMP-SB network (V, E), agent places buy offer R + 2b .
Proof. Consumers never offer valuation, bounded R. prove induction
producer C-level producer C-level k places buy offer R + 2kb .
Suppose producer C-level one places offer buy input g price > R + 2b .
Since always increments buy offers b , means previous time submitted buy
offer g price R + b < 0 R + 2b . time, must losing g, else would
bidding. ask quote g must greater R, offered greater
R output. Since consumer offer buy output producer C-level
one, must lose output. offers nondecreasing, situation permanent, hence
never raises input offer, contrary supposition. Thus C-level one producer
never place buy offer R + 2b .
inductive step, assume producer C-level i, < k, places buy
offer R + 2ib . Thus, producer C-level k win output offer R +
2(k 1)b . Applying reasoning analogous base case (C-level one), see producer
C-level k places buy offer R + 2kb . k producers, lemma follows
immediately. 2
Lemma 24 agent places (R + 2b )/b + buy offers.
Proof. Since consumers offer R increase offers least b , place offers
R/b times. producer initially places buy offers inputs. According
Lemma 23 producer bidding policy, producer subsequently offers higher R + 2b
increments b maximum inputs. 2
Theorem 5 SAMP-SB reaches quiescence finite number bids placed.
Proof. Lemma 24, finite number buy offers placed. need show producers place finite number sell (output) offers establish finite number total bids
placed.
producer change output offer if: 1) price input changes, 2) ask
price input changes, 2) loses offer good previously winning.
unchanged input offer switch winning losing without price
changing. Similarly, unchanged input offer switch winning state without ask
price changing. Hence, sufficient show price ask price producers
555

fiWALSH & W ELLMAN

input goods change finite number times. prove induction producer S-level
price ask quotes input good producer S-level k changes finite number
times.
producer input places output offer input good g producer S-level
one, producers input place one offer each. Hence, price ask price g
change response change buy offer g. Lemma 24, number buy offer
changes g finite.
assume producers S-levels less k place finite number output offers.
good g input producer S-level k, number output offer changes
finite. number input offers g must finite. Since number input output
offers g finite, places finite number output offers. 2
A.5 Proof Theorem 8
proving theorem, refer C-level producers network, defined beginning
Section A. reference, quasi-quiescence described Definition 5.
Lemma 25 run SAMP-SB quasi-quiescent state time interval [t, 0 ]
inactive producer changes offer input good time interval [t, 0 + ],
smallest period time agent requires update bid response price quote.
Proof. definition quasi-quiescence, interval [t, 0 ], consumer active producer
changes offer. Thus, simple induction C-level inactive producers shows
producer inactive time would win output [t,t 0 ], hence inactive producers
remain inactive interval. producer inactive [t, 0 ] would change
input offer [t, 0 + ]. 2
Lemma 26 run SAMP-SB quasi-quiescent time interval [t, 0 ],
quasi-quiescent time interval [t, 0 + ], smallest period time agent
requires update bid response price quote.
Proof. Assume, contrary wish prove, run SAMP-SB quasi-quiescent
[t, 0 ] time [t 0 ,t 0 + ]. Let consumer active producer change offer
[t 0 , 0 + ].
consumer, would change offer lost offer previously
winning quasi-quiescence. producer, must feasible, otherwise would change
input offer (because active) violating quasi-quiescence. Since feasible, would change
offer loses input previously winning, price one inputs increases.
cases, either loses buy offer previously winning, price one
buy offers increased. one occur, must time 00 [t,t 0 ], agent
either 1) changed winning output offer 2) changed input offer. definition
quasi-quiescence precludes #1, Lemma 25 definition quasi-quiescence preclude #2.
gives us contradiction, proving lemma. 2
Lemma 27 run SAMP-SB quasi-quiescent state time t, quasi-quiescent
times 0 > t.
556

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Proof. Lemma 26 conclude quasi-quiescence interval [t, + ],
extend interval , indefinitely. 2
Lemma 28 run SAMP-SB quasi-quiescent time t, producer inactive
time inactive time + , producer active time also inactive time + ,
smallest period time agent requires update bid response price quote.
Furthermore, p(g) change good g time +
Proof. Since agent cannot lower offers, way inactive producer become
active agent raise buy offer. Lemma 27 definition quasiquiescence, inactive producers change offers t, Lemma 25 inactive
producer change input offers. remains inactive.
Since offers decrease, active producer become inactive increasing
offer output. prices inputs increase. Since
quasi-quiescent state, happen inactive producer 0 changes offer output
g. since 0 inactive, change offer g cause (g) change. Since active
producers feasible (otherwise would want change bids, violating quasi-quiescence),
losing buy offer g time t. Therefore, respond changes (g), hence
change output offer remain active. 2
Theorem 8 run SAMP-SB reaches quasi-quiescent state, remains quasiquiescent state. Furthermore, neither allocation prices p subsequently change.
Proof. theorem follows directly Lemmas 27 28. 2
A.6 Proof Theorem 10
proving theorem, refer C-level producers network, defined beginning
Section A.
given run SAMP-SB network (V, E) characterized following parameters:
: maximum C-level producer network,
: maximum number input goods producer,
R: maximum consumer value, maxhg,ciE|cC vc (g).
Theorem 10 SAMP-SB reaches quasi-quiescent state number bids bounded polynomial size network value maximum consumer value placed
consumers active producers.
Proof. SAMP-SB guaranteed reach quasi-quiescent state (Theorem 5 Observation 7).
Lemma 24, number buy offers bounded polynomial value R, hence
need concerned number sell offers placed. Since prices buy offers increase
least b , producers perceived cost good must rise least b , increase
557

fiWALSH & W ELLMAN

sell offer less b . Also, producer increase sell offer less , required auction. Hence, Lemma 23 implies active producer become permanently
inactive places (R + 2b )/[max(b , )] output offers. 2
A.7 Proof Theorem 11
proving theorem, refer conditions --competitive equilibrium (Definition 4).
Lemma 29 SAMP-SB reaches quiescence network (V, E) consumer obeys
--competitive equilibrium conditions (Conditions (1) (2)).
Proof. Since consumer maintains single winning offer good gives nonnegative surplus, obeys Condition (1).
Let final prices allocation p (V 0 , E 0 ), respectively. Assume, contrary Condition (2), (c, (V 0 , E 0 ), p) < Hc (p) b consumer c. Let g surplus-maximizing
good c p.
c buy good, p(g ) + b < vc (g ) (otherwise would placed
offer g ) (c, (V 0 , E 0 ), p) = 0. Noting also Hc (p) = vc (g ) p(g ), algebraic manipulation gives us (c, (V 0 , E 0 ), p) > Hc (p) b , contradiction.
Thus, c buys one good g0
vc (g0 ) p(g0 ) < vc (g ) p(g ) b .

(6)

Let p(g ) p(g0 ) prices g g0 c placed final offer g0 . Since c offers
p(g0 ) + b g0 , since c offer p(g0 ),
p(g0 ) + b p(g0 ).

(7)

p(g ) p(g ).

(8)

Since prices decrease,

Substituting Equations (7) (8) left right sides, respectively, Equation (6) gives us
vc (g0 ) ( p(g0 ) + b ) < vc (g ) ( p(g ) + b ).
consumer bidding policy specifies c would bid g , rather g0 prices p,
contradiction. Thus consumer obeys Condition (2). 2
Lemma 30 SAMP-SB reaches quiescence network (V, E) inactive producer buys
positive-price input, producer obeys --competitive equilibrium conditions (Cong
ditions (1) (3)), = max((g) p(g), b ).
Proof. bidding policy ensures producer sells output g nonnegative
surplus, lemma conditions directly imply zero surplus sell . Thus
obeys Condition (1).
producer bidding policy guarantees feasible quiescence.
558

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Let final prices p allocation (V 0 , E 0 ). H (p) > hg,iE + quiescence,
H (p) = p(g ) hg,iE p(g) > hg,iE g + . Thus p(g ) > hg,iE p(g) + hg,iE g +
. producer bidding policy ensures offers hg,iE p(g) + hg,iE g +
g , must winning g profit. Thus (, (V 0 , E 0 ), p) = H (p) Condition (3) holds.
instead H (p) hg,iE g + , since (, (V 0 , E 0 ), p) 0, Condition (3) holds. 2
g

Theorem 11 prices allocation determined quiescence SAMP-SB protocol
--competitive equilibrium, g = max((g) p(g), b ), iff inactive producer buys
positive-price input.
Proof. Case if: Condition (1) --competitive equilibrium (Definition 4) fails inactive producer buys positive-price input.
Case if: Lemmas 29 30 show consumers producers, respectively, obey -competitive equilibrium conditions (Conditions (1)(3)). (M+1)st-price auction rules ensure
Condition (4). conditions --competitive equilibrium met. 2
A.8 Proof Theorem 12
proving theorem, refer conditions --competitive equilibrium (Definition 4).
Lemma 31 (g) p(g) > b good g quiescent state SAMP-SB network (V, E),
agent wins offer g.
Proof. Assume, contrary wish prove, (g) p(g) > b agent
winning offer g, quiescence SAMP-SB. Either buy offer sell offer sets (g).
Case 1: agent sets (g) buy offer. According SAMP-SB bidding policies,
agent increase buy offer losing offer. agent win offer g
price p(g). producer increases buy offer increments b consumer offers
p(g) + b . either case, agent place buy offer higher p(g) + b g.
(g) p(g) + b , contradiction.
Case 2: agent sets (g) sell offer. Case 1, buy offers higher
p(g) + b , hence every buy offer strictly (g). Recall that, sell offers,
Mth highest offer determines (g). since buy offers (g), must
sell offers (g). sell offers strictly buy offers
agent wins offer g, contradiction.
Since case gives us contradiction, must case agent wins offer g
(g) p(g) > b . 2
Lemma 32 (V 0 , E 0 ) --competitive equilibrium prices p, quiescence SAMP-SB
network (V, E), exist prices p0 (V 0 , E 0 ) also --competitive equilibrium
p0 , g = b producers goods g.
Proof. specify p0 follows: (g) > p(g) + b , p0 (g) = (g), otherwise p0 (g) = p(g).
show conditions --competitive equilibrium hold g = b .
considering allocation, goods still material balance Condition 4 still holds
p0 .
559

fiWALSH & W ELLMAN

Consider agent p0 (g) = p(g) adjacent goods g. Clearly, Ha (p0 ) = Ha (p)
(a, (V 0 , E 0 ), p0 ) = (a, (V 0 , E 0 ), p). (a, (V 0 , E 0 ), p0 ) 0 (Condition 1),
surplus bound met consumers (Condition 2) since hold p. producer,
input g, (g) p(g) b since p0 (g) = p(g). Hence, following bound
perceived cost g: pa (g) p(g) + b . result, producer bidding policies imply
(a, (V 0 , E 0 ), p) H (p) (hg,iE b + ). Therefore (a, (V 0 , E 0 ), p0 ) H (p0 ) (hg,aiE ga +
0

) producer surplus bound (Condition 3) holds ga inputs g0 .
consider agent adjacent good g p0 (g) = (g). Lemma 31, win
offer g, (a, (V 0 , E 0 ), p0 ) = (a, (V 0 , E 0 ), p), implying (a, (V 0 , E 0 ), p0 ) 0 (Condition 1).
consumer, since p0 (g) p(g), since win g, Ha (p0 ) = Ha (p),
surplus bound met consumers (Condition 2).
producer, since win g, must good (according
--competitive equilibrium conditions Theorem 11), implying (a, (V 0 , E 0 ), p0 ) = 0.
producer bidding policy specifies offered price = + hg0 ,aiE max((g0 ), p(g0 )+
b ) + output ga . Since win ga , must (ga ) . way
p0 constructed, p0 (ga ) (ga ) p0 (g0 ) + b max((g0 ), p(g0 ) + b ), giving us p0 (ga )
(ga ) + hg0 ,aiE (p0 (g0 ) + b ) + . would optimize p0 active,
Ha (p0 ) = p0 (ga ) hg0 ,aiE p0 (g0 ) hg0 ,aiE b + . since (a, (V 0 , E 0 ), p0 ) = 0 follows
(a, (V 0 , E 0 ), p0 ) Ha (p0 ) (hg0 ,aiE b + ). If, hand, would optimize p0
inactive at, (a, (V 0 , E 0 ), p0 ) = Ha (p0 ). either case, surplus bound met producers
0
(Condition 3) ga = b inputs g0 . 2
Theorem 12 (V 0 , E 0 ) --competitive equilibrium computed SAMP-SB (V 0 , E 0 )
nonnegative value differs value efficient allocation (|{hg,
E}| b + ) + |C|b .
Proof. Lemma 32, --competitive equilibrium (V 0 , E 0 ) = b prog
ducers goods g. b substituted equation Theorem 3, proved
present theorem. 2
g

A.9 Proof Theorem 13
proving theorem, refer S-level producers network, defined beginning
Section A.
Theorem 13 quiescent state SAMP-SB --competitive equilibrium tree.
Proof. prove, induction S-level producers, producer changes initial
output offer. Since buy offers never decrease, follows that, producer winning output,
lose output successive state run protocol. Since producer bids
inputs winning output, inactive producer buy positive-price output
present theorem follows Theorem 11.
Basis case: bidding policy specifies producer S-level zero never changes initial
output offer.
560

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Inductive case: Assume producer S-level less k changes initial output offer
show producer S-level k never changes initial output offer. Consider input good g
sell offers lowest sell offer . Since network tree, agent places
buy offers g. Producer initially offers zero g, long offers less loses
offer, p (g) = (g). holds, (g), defined Mth highest price, lowest
sell offer, hence p (g) = . soon offers greater g win offer,
p (g) = p(g). holds, p(g), defined + 1st highest price, lowest sell offer,
hence p (g) = . conclude p (g) never changes input g, hence never changes
initial output offer.
proven producer changes initial output offer, argument above,
theorem proven. 2
A.10 Proof Theorem 14
Theorem 14 quiescent state safe SAMP-SB --competitive equilibrium network
input complementarities.
Proof. show inactive producer buys input positive price quiescence
safe SAMP-SB. Since properties safe SAMP-SB quiescence SAMP-SB,
present theorem follows Theorem 11. Assume, contrary, that, quiescence,
producer wins input g positive price loses offer output g .
Let price final offer g, p(g) > 0 final price g, p (g)
final perceived cost g . Since wins g quiescence, p (g) = p(g). Let 0 price
second last offer . Immediately places offer , let p0 (g) perceived price
g p0 (g) price component price quote g. According bidding
policy, = 0 + b . Since offers loses g offer 0 , must 0 p0 (g), hence
p0 (g) + b . Furthermore, since loses offer 0 , p0 (g) p0 (g) + b .
assume wins g quiescence, must p(g) , hence p(g) p0 (g) + b . follows
that, since p (g) = p(g) p0 (g) p0 (g) + b , p (g) p0 (g).
According safe SAMP-SB bidding policies, offers g first winning g
offer price p0 (g). Since p (g) p0 (g), offer g quiescence
placed g. since offers agent decrease, must continue win final offer
g , contradicting assumption loses g quiescence. Thus, win input
positive price inactive, quiescent state safe SAMP-SB --competitive
equilibrium. 2
A.11 Proof Theorem 15
proving theorem, refer C-level S-level producers network, defined
beginning Section A.
Theorem 15 (V, E) polytree solution assigns good g consumer c, given
costs values, exists value vc (g) SAMP-SB guaranteed converge
valid solution (V 0 , E 0 ) c.

561

fiWALSH & W ELLMAN


Proof. convenience, denote max V 0 , maxc0 C, hg0 ,c0 i6=hg,ci vc0 (g0 ) . show
theorem holds for:
vc (g) = [ + (2b + )||] || + b .
need show SAMP-SB cannot reach state p(g) > vc (g)b c winning
g, c would stop bidding g desired solution would form.
First, observe consumer c0 good g0 hg0 , ai =
6 hg, ci, c0 offer
0
g , construction.
Now, consider producer directed path g output
. show, induction C-level producers, producer offers higher
+ b , C-level , one inputs. basis case, producer
C-level one cannot win output offer (by definition ). increases input offers
increments b , offer 0 > + b , input g0 , must first offer , < + b
input. offer 0 losing winning output offer. losing
, must p(g0 ) , must offering output. cannot
winning output, hence would offer 0 g. Thus C-level one offer
+ b input, establishing base case. Now, assume property holds every
producer C-level less k show holds producer C-level k. Given inductive
assumption, must cannot win output + b (k 1). argument
similar basis case, offer + b k input, proving inductive case.
Since ||, producer offers higher + b || input.
producer V 0 , denote maximum number producers, ,
subgraph (V, E), rooted . show induction S-level, producer
directed path g offers [ + b (|| + ) + (d 1)]I + output,
S-level . basis case, consider producer S-level one, offering buy
g0 . consumer offers g0 . (V, E) polytree, producer 0
offers buy g0 directed path g, hence offers + b || buy g0 .
producer offers sell g0 must inputs, hence offers g0 . Hence 0
successfully buy g0 offer higher + b (|| + 1), thus offer higher
amount g0 . Since number inputs equal , offer
( + b (|| + 1))I + output, basis case proven. Now, assume property
holds every producer S-level less k prove holds producer offering
buy g0 S-level k. inductive assumption, producer offers sell g0
( + b (|| + k 1) + (k 2))I + . basis case, consumer offers
g0 producer offer + b || buy g0 . Hence, offer
( + b (|| + k 1) + (k 2))I + + b buy g0 , output offer
"



h,g0 ipE | hg0 ,iE

#

( + b (|| + k 1) + (k 2))I + + b +
[ + b (|| + k) + s(k 1)]I + ,

proving inductive case. Since || ||, producer offers higher
[ + 2b || + (|| 1)]|| + [ + (2b + )||] || = vc (g) b .
shown agent 6= c places buy offer high vc (g) b g producer
directed path g places sell offer high vc (g) b g. Hence c agent
562

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

could possibly offer high vc (g) b g. c offer high, necessary, win g,
win g offers vc (g) b higher. follows c win g price vc (g)
quiescence. Observation 7 Theorem 16, state must valid solution. 2
A.12 Proof Theorem 16
Theorem 16 SAMP-SB reaches quasi-quiescence p(g) < vc (g) hg, ci E, c C,
systems state represents valid solution.
Proof. definition quasi-quiescence requires active producers change
bids, must feasible. agents feasible definition. price active
producers output good must less total price input goods, otherwise would
increase output offer, violating quasi-quiescence.
p(g) < vc (g), consumer c must offer g. consumer bids way
wins one unit one good, consumers change bids quasi-quiescence.
Finally, auction guarantees one-to-one mapping successful buy offers
successful sell offers good, ensuring material balance.
Thus, constraints valid solution satisfied. 2
A.13 Proof Theorem 17
Theorem 17 run SAMP-SB (V, E) valid solution state that:
consumer c either winning offer p(g) + b > vc (g) hg, ci E,
agents correct beliefs goods currently winning,
bids consumers active producers received response current
price quotes,
sell offers lost due tie breaking,
subsequent price quote auction, system quasi-quiescent
state valid solution.
Proof. Let current prices p. consumer bidding policy dictates consumers
change offers specified conditions. valid solution, producer feasible thus raise buy offers inputs. Therefore, agent changes
buy offers.
active producer feasible valid solution. Since winning inputs,
raises offer output g p(g) changes inputs g, place offer
output price higher sum input good prices. definition valid solution,
active, current price output good less sum current prices
inputs. since offer g , must offered higher p(g ) g .
previous offer price g higher p(g ), sum current
prices inputs higher p(g), offer higher p(g ) g .
563

fiWALSH & W ELLMAN

established agent changes buy offers, currently active producer
places sell offer p(g) good g. show implies that, next price quotes
prices p0 , p0 (g) = p(g). Assume contrary. Since offers decrease, p0 (g) > p(g).
Since buy offer winning sell offer changed, price increase due updated losing
sell offer price , = p0 (g). agent losing previous offer price
0 , must 0 least high (M + 1)st highest offer. Thus , higher,
must strictly higher (M + 1)st highest offer, hence cannot raise price g. Hence
p0 (g) = p(g).
Since prices change, temporal-precedence tie breaking ensures set winning
buy offers change. Additionally, since winning seller offers p(g) sell
offers currently lost tie breaking, set winning sell offers change. Since prices
allocations change, consumer active producer change bids. Furthermore,
system valid solution state based current price quotes, must valid
solution state based next price quotes. 2
note temporal-precedence tie-breaking (without requirement tied sell
offers lost) sufficient ensure allocation sellers change. tied
sell offers lost, possible active producer could increase next sell offer price
price output good. occurs, producer would lose tie breaking
output next quote, system would quasi-quiescence.

References
Andersson, A., Tenhunen, M., & Ygge, F. (2000). Integer programming combinatorial auction
winner determination. Fourth International Conference Multi-Agent Systems, pp. 39
46.
Andersson, M. R., & Sandholm, T. W. (1998). Leveled commitment contracting among myopic
individually rational agents. Third International Conference Multi-Agent Systems, pp.
2633.
Ausubel, L. M., & Milgrom, P. R. (2002). Ascending auctions package bidding. Frontiers
Theoretical Economics, 1(1).
Axelrod, R. (1987). evolution strategies iterated prisoners dilemma. Davis, L.
(Ed.), Genetic Algorithms Simulated Annealing, chap. 3, pp. 3241. Morgan Kaufmann.
Babaioff, M., & Nisan, N. (2001). Concurrent auctions across supply chain. Third ACM
Conference Electronic Commerce, pp. 110.
Babaioff, M., & Walsh, W. E. (2003). Incentive-compatible, budget-balanced, yet highly efficient
auctions supply chain formation. Fourth ACM Conference Electronic Commerce,
pp. 6475.
Baker, A. D. (1996). Metaphor reality: case study agents bid actual costs
schedule factory. Clearwater (Clearwater, 1996).
Bichler, M. (2001). Future e-Markets: Multidimensional Market Mechanisms. Cambridge
University Press.
Bikhchandani, S., & Mamer, J. W. (1997). Competitive equilibrium exchange economy
indivisibilities. Journal Economic Theory, 74, 385413.
564

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Bikhchandani, S., & Ostroy, J. M. (2002). package assignment model. Journal Economic
Theory, 107, 377406.
Borenstein, S., & Saloner, G. (2001). Economics electronic commerce. Journal Economic
Perspectives, 15(1), 312.
Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 11, 1733.
Clearwater, S. (Ed.). (1996). Market-Based Control: Paradigm Distributed Resource Allocation. World Scientific.
Davidow, W. H. (1992). Virtual Corporation: Structuring Revitalizing Corporation
21st Century. HarperCollins Publishers.
Davis, R., & Smith, R. G. (1983). Negotiation metaphor distributed problem solving.
Artificial Intelligence, 20, 63109.
Dellarocas, C., Klein, M., & Rodriguez-Aguilar, J. A. (2000). exception-handling architecture
open electronic marketplaces Contract Net software agents. Second ACM Conference
Electronic Commerce, pp. 225232.
Demange, G., & Gale, D. (1985). strategy structure two-sided matching markets. Econometrica, 53(4), 873888.
Demange, G., Gale, D., & Sotomayor, M. (1986). Multi-item auctions. Journal Political Economy, 94(4), 863872.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge. MIT
Press.
Friedman, D., & Rust, J. (Eds.). (1993). Double Auction Market: Institutions, Theories,
Evidence. Addison-Wesley.
Fudenberg, D., & Tirole, J. (1998). Game Theory. MIT Press.
Greenwald, A. (2003). 2002 trading agent competition: overview agent strategies. AI
Magazine, 24(1), 7782.
Groves, T. (1973). Incentives teams. Econometrica, 41(4), 617631.
Gul, F., & Stacchetti, E. (2000). English double auctions differentiated commodities.
Journal Economic Theory, 92, 6695.
Gul, F., & Stacchetti, E. (1999). Walrasian equilibrium gross substitutes. Journal Economic
Theory, 87, 95124.
Hunsberger, L., & Grosz, B. J. (2000). combinatorial auction collaborative planning.
Fourth International Conference MultiAgent Systems, pp. 151158.
Joshi, P., Oke, M., Sharma, V., & Veeramani, D. (1999). Issues dynamic highly-distributed
configuration supply webs. First IAC Workshop Internet-Based Negotiation Technologies.
Kelso, A. S., & Crawford, V. P. (1982). Job matching, coalition formation, gross substitutes.
Econometrica, 50(6), 14831504.
Kjenstad, D. (1998). Coordinated Supply Chain Scheduling. Ph.D. thesis, Norwegian University
Science Technology.
565

fiWALSH & W ELLMAN

Klemperer, P. (1999). Auction theory: guide literature. Journal Economic Surveys, 13(3),
227286.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards universal test suite combinatorial auction algorithrms. Second ACM Conference Electronic Commerce, pp. 6676.
Lucking-Reily, D., & Spulber, D. F. (2001). Business-to-business electronic commerce. Journal
Economic Perspectives, 15(1), 5568.
MacKie-Mason, J. K., & Varian, H. R. (1994). Generalized Vickrey auctions. Tech. rep., Dept.
Economics, Univ. Michigan.
Malone, T. W., & Laubacher, R. J. (1998). dawn e-lance economy. Harvard Business
Review, 145152.
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford University
Press, New York.
McAfee, R. P. (1992). dominant strategy double auction. Journal Economic Theory, 56,
434450.
McAfee, R. P., & McMillan, J. (1987). Auctions bidding. Journal Economic Literature, 25,
699738.
Milgrom, P. (2000). Putting auction theory work: simultaneous ascending auction.
Journal Political Economy, 108(2), 245272.
Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms bilateral trading. Journal
Economic Theory, 29, 265281.
Parkes, D. C., Kalagnanam, J., & Eso, M. (2001). Achieving budget-balance Vickrey-based
payment schemes exchanges. Seventeenth International Joint Conference Artificial
Intelligence, pp. 11611168.
Parkes, D. C., & Ungar, L. H. (2000). Iterative combinatorial auctions: Theory practice.
Seventeenth National Conference Artificial Intelligence, pp. 7481.
Parkes, D. C., & Ungar, L. H. (2002). ascending-price generalized Vickrey auction. Stanford
Institute Theoretical Economics Summer Workshop Economics Internet.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter. MIT Press.
Rust, J., Miller, J. H., & Palmer, R. (1994). Characterizing effective trading strategies: Insights
computerized double auction tournament. Journal Economic Dynamics Control, 18,
6196.
Sandholm, T., & Suri, S. (2000). Improved algorithms optimal winner determination combinatorial auctions generalizations. Seventeenth National Conference Artificial
Intelligence, pp. 9097.
Sandholm, T. W. (1993). implementation CONTRACT NET protocol based marginal
cost calculations. Eleventh National Conference Artificial Intelligence, pp. 256262.
Satterthwaite, M. A., & Williams, S. R. (1989). Bilateral trade sealed bid k-double auction:
Existence efficiency. Journal Economic Theory, 48, 107133.
Satterthwaite, M. A., & Williams, S. R. (1993). Bayesian theory k-double auction.
Friedman, & Rust (Friedman & Rust, 1993), chap. 4, pp. 99123.
566

fiD ECENTRALIZED UPPLY C HAIN F ORMATION

Shapiro, C., & Varian, H. R. (1999). Information Rules. Harvard Business School Press.
Stone, P., & Greenwald, A. (2000). first international trading agent competition: Autonomous
bidding agents. Journal Electronic Commerce Research, appear.
Veeramani, D., Joshi, P., & Sharma, V. (1999). Critical research issues agent-based manufacturing
supply webs. Agents-99 Workshop Agents Electronic Commerce Managing
Internet-Enabled Supply Chain.
Vickrey, W. (1961). Counterspeculation, auctions, competitive sealed tenders. Journal Finance, 16, 837.
Walsh, W. E. (2001). Market Protocols Decentralized Supply Chain Formation. Ph.D. thesis,
University Michigan.
Walsh, W. E., & Wellman, M. P. (1999). Efficiency equilibrium task allocation economies
hierarchical dependencies. Sixteenth International Joint Conference Artificial Intelligence, pp. 520526.
Walsh, W. E., Wellman, M. P., & Ygge, F. (2000). Combinatorial auctions supply chain formation. Second ACM Conference Electronic Commerce, pp. 260269.
Walsh, W. E., Yokoo, M., Hirayama, K., & Wellman, M. P. (2003). market-inspired approaches
propositional satisfiability. Artificial Intelligence, 144, 125156.
Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003). 2001 trading agent competition. Electronic Markets, 13(1), 412.
Wellman, M. P., & Walsh, W. E. (2000). Distributed quiescence detection multiagent negotiation.
Fourth International Conference Multi-Agent Systems, pp. 317324.
Wellman, M. P., Walsh, W. E., Wurman, P. R., & MacKie-Mason, J. K. (2001a). Auction protocols
decentralized scheduling. Games Economic Behavior, 35(1/2), 271303.
Wellman, M. P., Wurman, P. R., OMalley, K., Bangera, R., Lin, S.-d., Reeves, D., & Walsh, W. E.
(2001b). Designing market game trading agent competition. IEEE Internet Computing, 5(2), 4351.
Wurman, P. R., Walsh, W. E., & Wellman, M. P. (1998). Flexible double auctions electronic
commerce: Theory implementation. Decision Support Systems, 24, 1727.
Wurman, P. R., & Wellman, M. P. (2000). AkBA: progressive, anonymous-price combinatorial
auction. Second ACM Conference Electronic Commerce, pp. 2129.
Wurman, P. R., Wellman, M. P., & Walsh, W. E. (2001). parametrization auction design
space. Games Economic Behavior, 35(1/2), 304338.
Ygge, F. (1998). Market-Oriented Programming Application Power Load Management.
Ph.D. thesis, Lund University.

567

fiJournal Artificial Intelligence Research 19 (2003) 279-314

Submitted 09/02; published 10/03

Compiling Causal Theories Successor State Axioms
STRIPS-Like Systems
Fangzhen Lin

flin@cs.ust.hk

Department Computer Science
Hong Kong University Science Technology
Clear Water Bay, Kowloon, Hong Kong

Abstract
describe system specifying effects actions. Unlike commonly
used AI planning, system uses action description language allows one
specify effects actions using domain rules, state constraints
entail new action effects old ones. Declaratively, action domain language
corresponds nonmonotonic causal theory situation calculus. Procedurally,
action domain compiled set logical theories, one action domain,
fully instantiated successor state-like axioms STRIPS-like systems
generated. expect system useful tool knowledge engineers writing action
specifications classical AI planning systems, GOLOG systems, systems
formal specifications actions needed.

1. Introduction
describe system generating action effect specifications set domain rules
direct action effect axioms, among things. expect system useful
tool knowledge engineers writing action specifications classical AI planning systems,
GOLOG systems (Levesque et al., 1997), systems formal specifications
actions needed.
motivate, consider language used STRIPS (Fikes & Nilsson, 1971) describing effects actions. Briefly speaking, action described language
first-order formula, called precondition describes condition
action executable, add list enumerates propositions action make
true successfully executed situation, delete list enumerates propositions action make false successfully executed situation.
original STRIPS allowed precondition elements two lists complex
formulas, STRIPS actions refer whose precondition given conjunction atomic formulas whose add delete lists lists atomic formulas.
widely acknowledged language inadequate describing actions
real world. One limitations, one address paper,
language, one enumerate possible effects action, difficult impossible
task complex domains. example, given large C program, hard figure
effects changing value pointer values pointers program.
However, underlying principle simple: value pointer changes,
values pointers point memory location change well. Put
another way, direct effect action changing value pointer x
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiLin

value pointer x. indirect side effects action derived
constraint says two pointers point common location,
values must same.
idea specifying effects actions using domain constraints like engineering
first principle, many advantages. First all, constraints action independent, work actions. Secondly, effects actions derived domain
constraints agree ones expectation, good indication one
axiomatized domain correctly. Finally, domain constraints used purposes well. instance, used check consistency initial situation
database. general, set sentences violates domain constraint, know
legal situation satisfy set sentences. idea used planning
prune impossible states. Recently, even efforts reverse engineering
domain constraints STRIPS-like systems speed planners (e.g. Zhang & Foo,
1997; Gerevini & Schubert, 1998; Fox & Long, 1998, others).
appealing use domain constraints derive indirect effects actions,
making idea work formally turned challenge. problem commonly
known ramification problem, various proposals made solve it.
recently, however, proposals best theoretical interest
high computational complexity. situation since changed substantially due
use causality representing domain constraints (Lin, 1995, 1996; McCain & Turner,
1995, 1997; Thielscher, 1995, 1997; Baral, 1995; Lifschitz, 1997, others).
describe paper implemented system builds recent work causalitybased approaches ramification problem. Specifically, system takes input
action domain description actions described precondition axioms
direct effect axioms, domain constraints represented call domain rules.
system returns output complete action specification STRIPS-like format
set fully instantiated successor state axioms (Reiter, 1991).
paper organized follows. begin introducing action description language. propose procedure compile action domain specified language
complete set successor state axioms STRIPS-like description
extracted. show soundness procedure respect translation
action domain descriptions situation calculus causal theories Lin (1995). next
describe implementation procedure, present experimental results.
one see, one limitations system essentially propositional.
effect axioms domain rules variables, need fully instantiated
compilation process. partially overcome limitation, show results
allow one generalize propositional output first-order case certain classes
action domain descriptions. discuss related work, conclude paper
pointers future work.

2. Action Description Language
assume first-order language equality. shall call predicates whose extensions may changed actions fluents, whose extensions changed
actions static relations. also call unary static relations types. fluent atoms
280

fiFrom Causal Theories STRIPS-Like Systems

mean atomic formulas formed fluents. equality atom one form u = v,
u v variables constants, inequality constraint one form
u 6= v. Actions represented functions, assumed functions
positive arities language.
action description language includes following components.
2.1 Type Definitions
type definition specified expressions following form:
Domain(p, {a1 , ..., }),
p type, a1 , ..., constants. intuitive meaning expression
domain (extension) type p set {a1 , ..., }. instance, blocks
world, may type called block, have, say, five blocks named numerically:
Domain(block, {1, 2, 3, 4, 5}). logistics domain, may type called loc
locations, have, say, 3 locations l1 , l2 , l3 : Domain(loc, {l1 , l2 , l3 }).
2.2 Primitive Fluent Definitions
Primitive fluents defined expressions following form:
Fluent(f (x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),
f n-ary fluent, pi , 1 n, type, ei , 1 m,
inequality constraint form xj 6= xk , 1 j < k n. intuitive meaning
expression f (x1 , ..., xn ) legal fluent atom second argument true.
instance, blocks world, given type definition Domain(block, {1, 2, 3}),
following fluent specification:
Fluent(on(x, y), block(x) block(y) x 6= y)
would generate following six legal fluent atoms:
on(1, 2), on(1, 3), on(2, 1), on(2, 3), on(3, 1), on(3, 2).
Clearly, exactly one fluent definition fluent.
2.3 Complex Fluent Definitions
Given set primitive fluents, one may want define new ones. instance,
blocks world, given primitive fluent on, define clear terms as:
(x)clear(x) (y)on(y, x).
specify complex fluents like clear, first define fluent formulas follows:
t1 = t2 fluent formula, t1 t2 terms, i.e. either constant
domain type variable.
f (t1 , ..., tn ) fluent formula, t1 , ..., tn terms, f either n-ary
primitive fluent, complex fluent, static relation.
281

fiLin

0 fluent formula, , 0 , 0 , 0 , 0 also
fluent formulas.
fluent formula, x variable, p type, (x, p) (for x type p,
holds) (x, p) (for x type p, holds) fluent formulas. Notice
require types finite domains, quantifications really shorthands:
domain p {a1 , ..., }, (x, p) stands
(x/a1 ) (x/an ),
(x, p) stands
(x/a1 ) (x/an ).
complex fluent specified language pair expressions following
form:
Complex(f (x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),
Defined(f (x1 , ..., xn ), ),
pi ei primitive fluent definitions, fluent formula
mention complex fluents whose free variables among x1 , ..., xn .
first expression specifies syntax second semantics complex fluent.
instance, complex fluent clear blocks world specified as:
Complex(clear(x), block(x)),
Defined(clear(x), (y, block)on(y, x)).
mentioned above, quantifiers shorthands type must
finite domain. instance, given following specification:
Domain(block, {1, 2, 3}),
Fluent(on(x, y), block(x) block(y))
fluent definition clear expanded to:
Defined(clear(1), (on(1, 1) on(2, 1) on(3, 1))),
Defined(clear(2), (on(1, 2) on(2, 2) on(3, 2))),
Defined(clear(3), (on(1, 3) on(2, 3) on(3, 3))).
2.4 Static Relation Definitions
mentioned, static relation one changed action domain.
instance, robot navigation domain, may proposition connected(d, r1, r2)
meaning door connects rooms r1 r2. truth value proposition cannot
changed navigating robot rolls room room.
language, static relation defined expression following form:
Static(g(x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),
g n-ary predicate, pi ei primitive fluent definitions.
meaning expression similar fluent definition, exactly
one definition static relation.
282

fiFrom Causal Theories STRIPS-Like Systems

2.5 Domain Axioms
Domain axioms constraints static relations. instance, static proposition
connected(d, r1, r2), may want impose following constraint: connected(d, r1, r2)
connected(d, r2, r1). language, domain axioms specified expressions
form:
Axiom(),
fluent formula mention fluents, i.e. mentions static
relations equality. instance, constraint connected written as:
Axiom((d, door)(r1 , room)(r2 , room)connected(d, r1 , r2 )
connected(d, r2 , r1 )),
door room types.
2.6 Action Definitions
Actions defined expressions following form:
Action(a(x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em ),
n-ary action, pi ei primitive fluent definitions.
instance, blocks world, given type definition Domain(block, {1, 2, 3}),
following action specification:
Action(stack(x, y), block(x) block(y) x 6= y)
would generate following six action instances:
stack(1, 2), stack(1, 3), stack(2, 1), stack(2, 3), stack(3, 1), stack(3, 2).
exactly one action definition action.
2.7 Action Precondition Definitions
Action precondition definitions specified expressions following form:
P recond(a(x1 , ..., xn ), ),
n-ary action, fluent formula whose free variables among x1 , ..., xn .
exactly one precondition definition action. instance,
blocks world, may have:
P recond(stack(x, y), clear(x) clear(y) ontable(x)),
says action stack(x, y) executable situation, clear(x), clear(y),
ontable(x) must true it.
283

fiLin

2.8 Action Effect Specifications
Action effects specified expressions following form:
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
form:
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
fluent formula, f primitive fluent. intuitive meaning
expressions true initial situation, action a(x1 , ..., xn ) cause
f (y1 , ..., yk ) true (false). instance, blocks world, action stack(x, y) causes
x y:
Effect(stack(x, y), true, on(x, y)).
example context dependent effect, consider action drop(x) breaks object
fragile:
Effect(drop(x), f ragile(x), broken(x)).
Notice fluent formula action effect specifications variables x1 , ..., xn , y1 , ..., yk . Informally, variables supposed
universally quantified. precisely, expressions instantiated, one
substitute objects variables, provided resulting formulas well-formed.
instance, given action effect specification Effect(move(x), g(x1 ) q(x1 , x2 ), f (y)),
one instantiate to: Effect(move(a), g(b) q(b, c), f (d)), long move(a) legal
action (according action definition move) g(b), q(b, c), f (d) legal
fluent atoms (according fluent definitions g, q, f ).
2.9 Domain Rules
Domain rules specified expressions following form:
Causes(, f (x1 , ..., xn )),
following form:
Causes(, f (x1 , ..., xn )),
fluent formula, f primitive fluent. Like action effect specifications,
variables x1 , ..., xn . intuitive meaning domain rule
situation, holds, fluent atom f (x1 , ..., xn ) caused
true. domain rule stronger material implication. formal semantics given
mapping causal rule Lin (1995) (see Section 4), thus name causes.
instance, blocks world, block one block:
Causes(on(x, y) 6= z, on(x, z)).
logistics domain, one may want say package inside truck
location l, package location l well:
Causes(in(x, y) at(y, l), at(x, l)).
284

fiFrom Causal Theories STRIPS-Like Systems

2.10 Action Domain Descriptions
following definition sums action description language:
Definition 1 action domain description set type definitions, primitive fluent
definitions, complex fluent definitions, static proposition definitions, domain axioms, action
definitions, action precondition definitions, action effect specifications, domain rules.
Example 1 following action domain description defines blocks world three
blocks:
Domain(block, {1, 2, 3}),
Fluent(on(x, y), block(x) block(y)),
Fluent(ontable(x), block(x)),
Complex(clear(x), block(x)),
Defined(clear(x), (y, block)on(y, x)),
Causes(on(x, y) x 6= z, on(z, y)),
Causes(on(x, y) 6= z, on(x, z)),
Causes(on(x, y), ontable(x)),
Causes(ontable(x), on(x, y)),
Action(stack(x, y), block(x) block(y) x 6= y),
P recond(stack(x, y), ontable(x) clear(x) clear(y)),
Effect(stack(x, y), true, on(x, y)),
Action(unstack(x, y), block(x) block(y) x 6= y),
P recond(unstack(x, y), clear(x) on(x, y)),
Effect(unstack(x, y), true, ontable(x)),
Action(move(x, y, z), block(x) block(y) block(z) x 6= x 6= z 6= z),
P recond(move(x, y, z), on(x, y) clear(x) clear(z)),
Effect(move(x, y, z), true, on(x, z)).

3. Procedural Semantics
Given action domain description D, use following procedure called CCP (Causal
Completion Procedure) generate complete action effect specification:
1. Use primitive complex fluent definitions generate legal fluent atoms.
following let F set fluent atoms generated.
2. Use action definitions generate legal action instances, action
instance following.
285

fiLin

2.1. primitive fluent atom F F, collect ground instances1 positive
effects:
Effect(A, 1 , F ), , Effect(A, n , F ),
ground instances negative effects:
Effect(A, 1 , F ), , Effect(A, , F ),
ground instances positive domain rules:
Causes(01 , F ), , Causes(0k , F ),
ground instances negative domain rules:
Causes(01 , F ), , Causes(0l , F ),
generate following pseudo successor state axiom;
succ(F ) init(1 ) init(n ) succ(01 ) succ(0l )
init(F ) [init(1 ) init(m )
succ(01 ) succ(0k )],

(1)

fluent formula , init() formula obtained replacing every fluent atom f init(f ), similarly succ() formula
obtained replacing every fluent atom f succ(f ). Intuitively,
init(f ) means f true initial situation, succ(f ) f true
successor situation performing action initial situation.
2.2. Let Succ set pseudo successor state axioms, one primitive
fluent F , generated last step, Succ1 following set axioms:
Succ1 = {succ(F ) succ() | Defined(F, ) complex fluent definition},
Init following set axioms:
Init = { | Axiom() domain axiom}
{init() init(F ) | Causes(, F ) domain rule}
{init() init(F ) | Causes(, F ) domain rule}
{init(F ) init() | Defined(F, ) complex fluent definition}
{init(A ) | P recond(A, ) precondition definition A}.
fluent atom F , formula F
Init Succ Succ1 |= succ(F ) F ,
F mention propositions form succ(f ), output
axiom
succ(F ) F .
1. generating ground instances, shorthands like (x, p) expanded. See definition fluent
formulas last section.

286

fiFrom Causal Theories STRIPS-Like Systems

Otherwise, action effect F indeterminate. case, output two
axioms:
succ(F ) F ,
F succ(F ),
F strongest formula satisfying first implication, F weakest
formula satisfying second implication. following, explicit
action computing effects, write axioms
SuccA , InitA , Succ1A .
Conceptually, Step 2.1 procedure significant. next section,
shall prove step provably correct translation situation calculus
causal theories Lin (1995). Computationally, Step 2.2 expensive. shall
describe strategies system uses implement Section 5.
procedure work properly, action domain description satisfy
following conditions.
1. require fluent atoms Init, Succ, Succ1 among generated
Step 1. would rule cases like
Fluent(on(x, y), block(x) block(y) x 6= y)
together Defined(clear(x), (y, block)on(y, x)), latter would generate
fluent atoms form on(x, x) ruled fluent definition on.
one could either drop inequality constraint definition change
complex fluent definition Defined(clear(x), (y, block)(on(y, x) x 6= y)).
could built test procedure reject action domain
description incoherent fluent definitions like this. One easy way making sure
happen use inequality constraints definition fluents.
2. mentioned above, action exactly one action precondition captures exactly conditions action executable.
action precondition given explicitly like this, one needs careful
writing action effect axioms domain rules contradictory effects
would generated. instance, given P recond(A, true), action effect axioms
Effect(A, true, F ) Effect(A, true, F ) clearly realizable simultaneously.
Similarly, Causes(true, F ) given domain rule, one write
effect axiom Effect(A, true, F ). insisted always executable,
could simply conclude executable effect axioms contradiction effect axioms contradict domain rules. remains future
work extend procedure allow automatic generation implicitly
given action preconditions. now, shall assume given action domain
specification consistent sense action instance generated
Step 1, following theory
Init Succ Succ1 {init() succ(F ) |
Effect(A, , F ) ground instance effect axiom}
287

fiLin

consistent.
3. related point, procedure assumes information initial situation
given Init. particular, action effect axioms entail information initial situation. instance, given Causes(q, p), Effect(A, true, p),
P recond(A, true), must initial situation, q cannot true,
otherwise, persist next situation, causing p false, contradicts action effect. Formally, means given set atoms
form init(f ), f primitive fluent atom, Init consistent,
SuccSucc1 also consistent, , complement I, following
set:
{init(f ) | init(f ) 6
f primitive fluent atom generated Step 1}
Notice similar reason, Reiter needed called consistency assumption order completion procedure sound complete generating
successor state axioms (Reiter, 1991).
action domain descriptions clearly targeted specifying deterministic actions,
indeterminate effects sometimes arise cyclic domain rules. instance,
consider following action domain description:
Causes(p, p),
P recond(A, true)
action A, Init tautology, Succ1 empty, Succ consists following pseudosuccessor state axiom p:
succ(p) succ(p) init(p),
equivalent init(p) succ(p). initially p true, performed,
know p continue true. p initially false, performed,
know p true not.
Example 2 Consider blocks world description Example 1. set fluent atoms
generated Step 1 is:
F = {on(1, 1), on(1, 2), on(1, 3), on(2, 1), on(2, 2), on(2, 3),
on(3, 1), on(3, 2), on(3, 3), clear(1), clear(2), clear(3),
ontable(1), ontable(2), ontable(3)}.
Step 2 generates following action instances:
stack(1, 2), stack(1, 3), stack(2, 1), stack(2, 3), stack(3, 1), stack(3, 2),
unstack(1, 2), unstack(1, 3), unstack(2, 1), unstack(2, 3), unstack(3, 1),
unstack(3, 2), move(1, 2, 3), move(1, 3, 2), move(2, 1, 3), move(2, 3, 1),
move(3, 1, 2), move(3, 2, 1)
288

fiFrom Causal Theories STRIPS-Like Systems

action instances, need go Steps 2.1 2.2. instance,
stack(1, 2), one effect axiom on(1, 2):
Effect(stack(1, 2), true, on(1, 2)),
following causal rules on(1, 2):
Causes(on(1, 1), on(1, 2)),
Causes(on(1, 3), on(1, 2)),
Causes(on(2, 2), on(1, 2)),
Causes(on(3, 2), on(1, 2)),
Causes(ontable(1), on(1, 2)).
Therefore Step 2.1 generates following pseudo-successor state axiom on(1, 2):
succ(on(1, 2)) true
init(on(1, 2)) [succ(on(1, 1)) succ(on(1, 3))
succ(on(2, 2)) succ(on(3, 2) succ(ontable(1))].
similarly generate following pseudo-successor state axioms primitive
fluent atoms:
succ(on(1, 1)) init(on(1, 1)) [succ(on(1, 2)) succ(on(1, 3))
succ(on(2, 1)) succ(on(3, 1) succ(ontable(1))],
succ(on(1, 3)) init(on(1, 3)) [succ(on(1, 1)) succ(on(1, 2))
succ(on(2, 3)) succ(on(3, 3)) succ(ontable(1))],
succ(on(2, 1)) init(on(2, 1)) [succ(on(1, 1)) succ(on(3, 1))
succ(on(2, 2)) succ(on(2, 3)) succ(ontable(2))],
succ(on(2, 2)) init(on(2, 2)) [succ(on(1, 2)) succ(on(1, 3))
succ(on(2, 1) succ(on(2, 3)) succ(ontable(2))],
succ(on(2, 3)) init(on(2, 3)) [succ(on(1, 3)) succ(on(3, 3))
succ(on(2, 1) succ(on(2, 2)) succ(ontable(2))],
succ(on(3, 1)) init(on(3, 1)) [succ(on(3, 2)) succ(on(3, 3))
succ(on(1, 1) succ(on(1, 3)) succ(ontable(1))],
succ(on(3, 2)) init(on(3, 2)) [succ(on(3, 1)) succ(on(3, 3))
succ(on(1, 2) succ(on(2, 2)) succ(ontable(2))],
succ(on(3, 3)) init(on(3, 3)) [succ(on(3, 1)) succ(on(3, 2))
succ(on(1, 3) succ(on(2, 3)) succ(ontable(3))],
succ(ontable(1))
init(ontable(1)) [succ(on(1, 2)) succ(on(1, 1)) succ(on(1, 3))],
succ(ontable(2))
init(ontable(2)) [succ(on(2, 1)) succ(on(2, 2)) succ(on(2, 3))],
289

fiLin

succ(ontable(3))
init(ontable(3)) [succ(on(3, 1)) succ(on(3, 2)) succ(on(3, 3))].
complex fluent clear, definition yields following axioms:
succ(clear(1)) succ(on(1, 1)) succ(on(2, 1)) succ(on(3, 1)),
succ(clear(2)) succ(on(1, 2)) succ(on(2, 2)) succ(on(3, 2)),
succ(clear(3)) succ(on(1, 3)) succ(on(2, 3)) succ(on(3, 3)).
solve pseudo-successor state axioms generate following successor state axioms:
succ(on(1, 1)) f alse
succ(on(1, 3)) f alse
succ(on(2, 2)) f alse
succ(on(3, 1)) f alse
succ(on(3, 3)) init(on(3, 3))
succ(ontable(2)) init(ontable(2))
succ(clear(1)) init(clear(1))
succ(clear(3)) init(clear(3))

succ(on(1, 2)) true
succ(on(2, 1)) f alse
succ(on(2, 3)) init(on(2, 3))
succ(on(3, 2)) f alse
succ(ontable(1)) f alse
succ(ontable(3)) init(ontable(3))
succ(clear(2)) f alse

set fully instantiated successor state axioms, generate
STRIPS-like descriptions like following:
stack(1, 2)
Preconditions:
Add list:
Delete list:
Cond. effects:
Indet. effects:

ontable(1), clear(1), clear(2).
on(1, 2).
ontable(1), clear(2).
none.
none.

stack(1, 3)
Preconditions:
Add list:
Delete list:
Cond. effects:
Indet. effects:

ontable(1), clear(1), clear(3).
on(1,3).
ontable(1), clear(3).
none.
none.

290

fiFrom Causal Theories STRIPS-Like Systems

following remarks:
Although generate axiom succ(on(1, 3)) f alse stack(1, 2), put
on(1, 3) delete list. deduce init(on(1, 3)) f alse
Init well. fluent atom put add delete list action
fluent atoms truth value definitely changed action. See Section 5
details STRIPS-like description generated successor state
axioms.
one see, CCP procedure crucially depends fact type
finite domain reasoning done propositional logic.
limitation current system, limitation bad one might think.
First all, typical planning problems assume finite domains, changing
domain type action description easy - one needs change
corresponding type definition. significantly, generic action domain description
often obtained one assumes finite domain. blocks world
example, numbers 1, 2, 3 generic names, replaced
parameters. instance, replace 1 x 2 STRIPSlike description stack(1, 2), get STRIPS-like description stack(x, y)
works x y. found strategy often works
planning domains.

4. Formal Semantics
formal semantics action domain description defined translating
situation calculus causal theory Lin (1995). shall show procedure CCP given
sound semantics.
section mainly interested nonmonotonic action theories.
interested using action description language describing action
domains, section safely skipped.
first briefly review language situation calculus.
4.1 Situation Calculus
language situation calculus many sorted first-order language. assume
following sorts: situation situations, action actions, fluent propositional fluents,
truth-value truth values true f alse, object everything else.
use following domain independent predicates functions:
Binary function - action situation s, do(a, s) situation
resulting performing s.
Binary predicate H - p situation s, H(p, s) true p holds s.
Binary predicate P oss - action situation s, P oss(a, s) true
possible (executable) s.
Ternary predicate Caused - fluent atom p, truth value v, situation
s, Caused(p, v, s) true fluent atom p caused (by something unspecified)
truth value v situation s.
291

fiLin

last section, introduced fluent formulas. extend H formulas:
fluent formula situation s, H(, s) defined follows:
H(t1 = t2 , s) t1 = t2 .
P static proposition, H(P, s) P .
inductively, H(, s) H(, s), H( 0 , s) H(, s) H(0 , s), similarly
connectives.
inductively, H((x, p), s) x.[p(x) H(, s)] H((x, p), s) x.[p(x)
H(, s)].
According definition, H(, s) expanded situation calculus formula
H applied fluents.
4.2 Translation Situation Calculus
Given first-order language L writing action domain descriptions, assume
corresponding language L0 situation calculus constants L
constants sort object L0 , types L types (unary predicates) L0 ,
static relations predicates arities L0 , fluents L functions
sort f luent L0 , actions L functions sort action L0 .
conventions, following translation map action domain description situation
calculus theory.
Let action domain description. translation situation calculus
theory defined follows:
type definition Domain(p, {a1 , ..., ak }) translated to:
(x).p(x) (x = a1 x = ak ),
a1 6= a2 6= 6= ak .
primitive fluent definition
Fluent(f (x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em )
translated
(x1 , ..., xn ).Fluent(f (x1 , ..., xn )) p1 (x1 ) pn (xn ) e1 em .
complex fluent definition
Complex(f (x1 , ..., xn ), p1 (x1 ) pn (x) e1 em ),
Defined(f (x1 , ..., xn ), ),
translated
(x1 , ..., xn ).Fluent(f (x1 , ..., xn )) p1 (x1 ) pn (x) e1 em ,
(x1 , ..., xn , s).Fluent(f (x1 , ..., xn )) [H(f (x1 , ..., xn ), s) H(, s)].
292

fiFrom Causal Theories STRIPS-Like Systems

domain axiom static propositions:
Axiom()
translated quantifiers treated shorthands:
(x, p) = x.p(x) ,
(x, p) = x.p(x) .
action definition
Action(a(x1 , ..., xn ), p1 (x1 ) pn (xn ) e1 em )
translated
(x1 , ..., xn ).Action(a(x1 , ..., xn )) p1 (x1 ) pn (xn ) e1 em .
assume domain description one action definition
action.
action precondition axiom
P recond(a(x1 , ..., xn ), )
translated
~ s).Action(a(x1 , ..., xn )) [P oss(a(x1 , ..., xn ), s) H(, s)],
(,
~ list free variables a(x1 , ..., xn ) . mentioned earlier
one limitations current system action preconditions
given explicitly. reflected translation.
action effect axiom:
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
translated
~ s).Action(a(x1 , ..., xn )) Fluent(f (y1 , ..., yk )) P oss(a(x1 , ..., xn ), s)
(,
{H(, s) Caused(f (y1 , ..., yk ), true, do(a(x1 , ..., xn ), s))},
~ list free variables a(x1 , ..., xn ), f (y1 , ..., yk ), .
Similarly, effect axiom
Effect(a(x1 , ..., xn ), , f (y1 , ..., yk )),
translated
~ s).Action(a(x1 , ..., xn )) Fluent(f (y1 , ..., yk )) P oss(a(x1 , ..., xn ), s)
(,
{H(, s) Caused(f (y1 , ..., yk ), f alse, do(a(x1 , ..., xn ), s))}.
293

fiLin

domain rule form
Causes(, f (x1 , ..., xn ))
translated
~
().Fluent(f
(x1 , ..., xn )) (s){H(, s) Caused(f (x1 , ..., xn ), true, s)},
~ list free variables f (x1 , ..., xn ) . Similarly, domain rule
form
Causes(, f (x1 , ..., xn ))
translated
~
().Fluent(f
(x1 , ..., xn )) (s){H(, s) Caused(f (x1 , ..., xn ), f alse, s)}.
given action domain description D, let translation situation
calculus. semantics determined completion comp(T ) defined
set following sentences:
1. circumscription Caused predicates fixed.
2. following basic axioms Caused says fluent atom caused
true (false), true (false):
Caused(p, true, s) Holds(p, s),

(2)

Caused(p, f alse, s) Holds(p, s).

(3)

3. truth values, following unique names domain closure axiom:
true 6= f alse (v)(v = true v = f alse).

(4)

4. unique names assumptions fluents actions. Specifically, F1 ,..., Fn
fluents, have:
Fi (~x) 6= Fj (~y ), j different,
Fi (~x) = Fi (~y ) ~x = ~y .
Similarly actions.
5. primitive fluent atom F , following generic successor state axiom:
a, s.P oss(a, s) H(F, do(a, s))

(5)

[Caused(F, true, do(a, s)) H(F, s) Caused(F, f alse, do(a, s))].
6. foundational axioms (Lin & Reiter, 1994b) discrete situation calculus.
axioms characterize structure space situations. purpose
paper, enough mention include following unique names
axioms situations:
6= do(a, s),
do(a, s) = do(a0 , s0 ) (a = a0 = s0 ).
294

fiFrom Causal Theories STRIPS-Like Systems

following theorem shows procedural semantics given previous section
sound respect semantics given here.
Theorem 1 Let action domain description, translation situation
calculus. Let ground action instance, situation variable, (s) situation
calculus formula satisfies following two conditions (1) contains two
situation terms do(A, s); (2) mention predicate H,
equality, static relations. Let obtained replacing H(f, s)
init(f ), H(f, do(A, s)) succ(f ).
comp(T ) |= s.P oss(A, s) (s)

(6)

Init Succ Succ1 |= ,

(7)


Init, Succ, Succ1 sets axioms generated according procedure Section 3
Proof: Suppose situation model comp(T ) {P oss(A, S)}. Construct
MS,A follows:
domain MS,A object domain .
interpretations non-situational function predicate symbols MS,A
.
fluent atom f , MS,A |= init(f ) iff |= H(f, S) MS,A |= succ(f ) iff
|= H(f, do(A, S)).
Clearly, |= (S) iff MS,A |= . show MS,A model left hand
side (7). this, see (7), (6).
Notice first ground fluent atom F generated procedure CCP iff Fluent(F )
true . Notice also fluent atoms Init Succ Succ1 must generated
procedure.
show first MS,A model Init:
1. Axiom() domain axiom, . Thus satisfies . Since
fluent symbols it, MS,A satisfies too.
2. Causes(, f (x1 , ..., xn )) domain rule,
~
().Fluent(f
(x1 , ..., xn )) (s){H(, s) Caused(f (x1 , ..., xn ), true, s)}
. Thus satisfies
~
().Fluent(f
(x1 , ..., xn )) (s){H(, s) H(f (x1 , ..., xn ), s)}.
Thus init( 0 ) init(F ) corresponding formula Init, 0 F
ground instantiation f (x1 , ..., xn ), respectively, Fluent(F ) must
true (otherwise formula would Init), satisfies
H( 0 , S) H(F, S). construction MS,A , satisfies init( 0 ) init(F ).
case Causes(, f ) similar.
295

fiLin

3. Suppose Defined(F, ) instantiation complex fluent definition
F Init. Init, Fluent(F ) must true. Thus must satisfy
H(F, S) H(, S). construction MS,A , satisfies init(F ) init(, S).
4. Suppose P recond(A, ) precondition axiom A. Since satisfies P oss(A, S)
Action(A) (because one action instances generated procedure),
thus satisfies H(A , S). MS,A satisfies init(A ).
show MS,A model Succ, is, primitive fluent atom F
generated procedure Step 1, pseudo-successor state axiom (1) holds. Referring
notation axiom, need show satisfies following formula:
H(F, do(A, S)) H(1 , S) H(n , S)
H(01 , do(A, S)) H(0l , do(A, S))
H(F, S) [H(1 , S) H(m , S)
H(01 , do(A, S)) H(0k , do(A, S))].
First all, instantiating generic successor state axiom (5) S, get:
P oss(A, S) H(F, do(A, S))
[Caused(F, true, do(A, S)) H(F, S) Caused(F, f alse, do(A, S))].
Since model P oss(A, S),
H(F, do(A, S))

(8)

[Caused(F, true, do(A, S)) H(F, S) Caused(F, f alse, do(A, S))].
consider circumscription Caused predicates fixed. Notice
axioms Caused form W Caused(x, y, z), W formula
mention Caused. Therefore circumscription Caused equivalent
predicate completion Caused. Suppose F f (t), axioms
Caused(f (x), v, s) follows:
W1 Caused(f (x), v, s), , Wi Caused(f (x), v, s).
unique names axioms fluents, result predicate completion
Caused entail:
Caused(f (x), v, s) W1 Wi .
W1 ,...,Wi action effect axioms domain rules f . way
(1) generated, noting Action(A), Fluent(F ), P oss(A, S) true, one
see equivalence instantiated replacing x, true v,
s, get
Caused(F, true, do(A, S)) H(1 , S) H(n , S)
H(01 , do(A, S)) H(0l , do(A, S)).
296

fiFrom Causal Theories STRIPS-Like Systems

Similarly, following axiom Caused(F, f alse, do(A, S)):
Caused(F, f alse, do(A, S)) H(1 , S) H(m , S)
H(01 , do(A, S)) H(0k , do(A, S)).
two axioms (8), get:
H(F, do(A, S)) H(1 , S) H(n , S)
H(01 , do(A, S)) H(0l , do(A, S))
H(F, S) [H(1 , S) H(m , S)
H(01 , do(A, S)) H(0k , do(A, S))].
Since model Comp(T ), satisfies formula. construction
MS,A , satisfies pseudo-successor state axiom (1).
Finally, fact MS,A model Succ1 apparent.
2
general, (6) imply (7). several reasons:
mentioned procedure CCP, assume information
initial situation given Init.
procedure works actions one time. situation calculus theory
captures effects actions single theory. possible bad specification action causes entire theory become inconsistent. instance,
Causes(true, p), P recond(A, true), Effect(A, f alse, p), corresponding situation calculus theory inconsistent action A.
procedure, generate inconsistent theory A.

5. Implementation
Except Step 2.2, procedure CCP Section 3 straightforward implement.
section describes strategy system uses implementing Step 2.2. main
idea comes work Lin (2001a) strongest necessary weakest sufficient
conditions.
Given propositional theory , proposition q, set B propositions, formula
said sufficient condition q B consists propositions B
|= q. said weakest sufficient condition sufficient
condition 0 , |= 0 . Similarly, formula said necessary
condition q B consists propositions B |= q . said
strongest necessary condition necessary condition 0 ,
|= 0 .
easy see weakest sufficient condition strongest necessary condition unique logical equivalence background theory. shown (Lin,
2001a) two notions closely related, computed using technique
forgetting (Lin & Reiter, 1994a). particular, action theories, effective strategy first compute strongest necessary condition, add background theory,
297

fiLin

compute weakest sufficient condition new theory. strategy
justified following proposition Lin (2001a):
Proposition 1 Let theory, q proposition, B set propositions.
necessary condition q B , weakest sufficient condition q B
{}, weakest sufficient condition q B .
describe strategy implementing Step 2.2 procedure CCP.
following, given action instance A, Step 2.2, let Succ set pseudosuccessor state axioms primitive fluent atoms, Succ1 set pseudo-successor state
axioms complex fluent atoms, Init set initial situation axioms derived
action precondition axiom A, domain axioms, domain rules, complex fluent
definitions. Also following, succ-proposition one form succ(f ),
init-proposition one form init(f ).
1. Transform Init clausal form derive set unit clauses U nit.
2. Use U nit simplify axioms Succ resulting axiom it:
succ(f ) f ,

(9)

f mention succ-propositions, delete Succ, output
replace succ(f ) rest axioms f .
3. fluent atom f whose pseudo-successor state axiom (9) Succ, f
form init(f ) ... (a candidate frame axiom), check see succ(f )
derived Succ, U nit, init(f ) unit resolution. so, delete Succ,
output succ(f ) init(f ), replace succ(f ) Succ init(f ).
4. fluent atom f whose pseudo-successor state axiom (9) Succ, compute
strongest necessary condition f succ(f ) init-propositions
theory Init Succ, weakest sufficient condition f succ(f ) initpropositions theory {f } Init Succ. f tautology, delete
(9) Succ, output succ(f ) f , replace succ(f ) Succ f .
tautology, output succ(f ) f f f succ(f ), delete (9)
Succ. correctness step follows Proposition 1.
5. previous steps solve equations Succ, generate appropriate output
primitive fluent atoms. complex fluent atom F :
Defined(F, ),
every primitive fluent atom successor state axiom, following:
(a) primitive fluent atoms changed action, complex
fluent atom changed action either, output succ(F ) init(F );
(b) otherwise, output succ(F ) , obtained succ() replacing
every succ-proposition right side successor state axiom.
298

fiFrom Causal Theories STRIPS-Like Systems

Otherwise, primitive fluent atoms successor state
axiom, means action may indeterminate effect them,
action may indeterminate effect F well. Compute strongest necessary weakest sufficient conditions succ(F ) Init Succ Succ1
last step, output them.
6. step try generate STRIPS-like description action instance
based results Steps 4 5. fluent atom F , according one
following cases:
(a) successor state axiom succ(F ) true, put F add list unless
init(F ) entailed Init;
(b) successor state axiom succ(F ) f alse, put F delete list
unless init(F ) entailed Init;
(c) successor state axiom succ(F ) , true, f alse,
init(F ), put F conditional effect list output successor state
axiom.
(d) F successor state axiom, put list indeterminate
effects.
Clearly, F put lists, truth value affected A.
Steps 4 5 procedure bottleneck worst case, computing
strongest necessary condition proposition coNP-hard. However,
experience action context-free effect fluent atom F , successor
state axiom computed without going Step 4.
implemented procedure CCP using strategy SWI-Prolog 3.2.92 .
url system follows:
http://www.cs.ust.hk/~flin/ccp.html
Using system, encoded action description language many planning
domains come original release PDDL (McDermott, 1998), compiled
STRIPS-like specifications. encodings domains results returned
system included online appendix. following, illustrate
interesting features system using following two domains: blocks world
monkey bananas domain.
5.1 Blocks World
used blocks world running example. shall give alternative specification domain using following better known set actions: stack,
unstack, pickup, putdown. shall use domain show changing slightly
precondition one actions result different action specification.
2. SWI-Prolog developed Jan Wielemaker University Amsterdam

299

fiLin

begin description corresponds standard STRIPS encoding
domain.
Fluent(on(x, y), block(x) block(y)),
Fluent(ontable(x), block(x)),
Fluent(holding(x), block(x)),
Complex(clear(x), block(x),
Defined(clear(x), ((y, block)on(y, x)) holding(x)),
Complex(handempty, true),
Defined(handempty, (x, block)holding(x)),
Causes(on(x, y) x 6= z, on(z, y)),
Causes(on(x, y) 6= z, on(x, z)),
Causes(on(x, y), ontable(x)),
Causes(ontable(x), on(x, y)),
Causes(on(x, y), holding(x)),
Causes(on(x, y), holding(y)),
Causes(holding(x), ontable(x)),
Causes(holding(x), on(x, y)),
Causes(holding(x), on(y, x)),
Causes(holding(x) 6= x, holding(y)),
Action(stack(x, y), block(x) block(y) x 6= y),
P recond(stack(x, y), holding(x) clear(y)),
Effect(stack(x, y), true, on(x, y)),
Action(unstack(x, y), block(x) block(y) x 6= y),
P recond(unstack(x, y), clear(x) on(x, y) handempty),
Effect(unstack(x, y), true, holding(x)),
Action(putdown(x), block(x)),
P recond(putdown(x), holding(x)),
Effect(putdown(x), true, ontable(x)),
Action(pickup(x), block(x)),
P recond(pickup(x), handempty ontable(x) clear(x)),
Effect(pickup(x), true, holding(x)).
Notice compared description Example 1, two fluents, holding
handempty here. Thus domain rules them, definition
clear changed take account block held, considered
clear.
300

fiFrom Causal Theories STRIPS-Like Systems

assuming domain three blocks Domain(block, {1, 2, 3}), system
generate 19 fluent atoms, 18 action instances. action instance, returns
complete set successor state axioms STRIPS-like representation. total
computation time actions 835K inferences 0.5 seconds.3 pure STRIPS
domain, i.e. actions context free. type domains, mentioned earlier,
Step 4 implementation procedure needed, Step 5 easy.
results expected. instance, action pickup(1), STRIPS-like representation returned system looks like following: track 1 track 2), STRIPSlike representation looks like:
pickup(1):
Preconditions: clear(1), handempty, ontable(1)
Add list: holding(1)
Delete list: ontable(1), clear(1), handempty
Conditional effects:
Indeterminate effects:
complete output given online appendix. let us consider happen
drop ontable(x) precondition pickup(x):
P recond(pickup(x), handempty clear(x)).
means long block clear, picked up. new precondition,
system returns following STRIPS-like representation action pickup(1);
pickup(1):
Preconditions: clear(1), handempty
Add list: holding(1)
Delete list: clear(1), handempty, on(1, 2), on(1, 3), ontable(1)
Conditional effects:
succ(clear(2))<-> - (init(on(2, 2))\/init(on(3, 2)))
succ(clear(3))<-> - (init(on(2, 3))\/init(on(3, 3)))
Indeterminate effects:
- negation, \/ disjunction. ADL-like description action
would something like following:
pickup(x):
Preconditions: clear(x), handempty
Add list: holding(x),
clear(y) on(x,y)
Delete list: clear(x), handempty,
on(x,y) on(x,y)
ontable(x) ontable(x)
3. times paper refer CPU times Pentium III 1GHz machine 512MB RAM running
SWI-Prolog 3.2.9 Linux. number inferences one reported SWI-Prolog,
roughly corresponds number resolution steps carried Prolog interpreter,
machine independent.

301

fiLin

5.2 Monkey Bananas Domain
domain adapted McDermotts PDDL library planning domains,
attributes University Washingtons UCPOP collection action domains,
turn attributes Prodigy. action effects generated system
context-dependent, context-free systems. shall elaborate
difference later.
domain, two types, loc locations (we assume three locations
here), object things like monkey, banana, box, etc.:
Domain(loc, {1, 2, 3}),
Domain(object, {monkey, box, banana, knif e, glass, f ountain}).
following fluent definitions:
Fluent(onF loor),
Fluent(at(M, X), object(M ) loc(X)),
Fluent(hasknif e),
Fluent(onbox(X), loc(X)),
Fluent(hasbanana),
Fluent(haswater),
Fluent(hasglass).
following domain rules fluents:
Causes(onbox(X), at(monkey, X)),

(10)

Causes(onbox(X), at(box, X)),

(11)

Causes(onbox(X), onF loor),

(12)

Causes(onF loor, onbox(X)),

(13)

Causes(at(M, X) X 6= Y, at(M, )),

(14)

Causes(hasglass at(monkey, X), at(glass, X)),

(15)

Causes(hasknif e at(monkey, X), at(knif e, X)),

(16)

Causes(hasbanana at(monkey, X), at(banana, X)).

(17)

following action definitions along respective preconditions effect
axioms:
goto(x, y) - monkey goes x y:
Action(goto(X, ), loc(X) loc(Y ) X 6= ),
P recond(goto(X, ), at(monkey, ) onF loor),
Effect(goto(X, ), true, at(monkey, X)).
302

fiFrom Causal Theories STRIPS-Like Systems

climb(X) - monkey climbs onto box location X:
Action(climb(X), loc(X)),
P recond(climb(X), at(box, X) onF loor at(monkey, X)),
Effect(climb(X), true, onbox(X)).
pushbox(X, ) - monkey pushes box X.
Action(pushbox(X, ), loc(X) loc(Y ) X 6= ),
P recond(pushbox(X, ), at(monkey, ) at(box, )) onF loor),
Effect(pushbox(X, ), true, at(monkey, X)),
Effect(pushbox(X, ), true, at(box, X)).
getknif e(X) - get knife location X.
Action(getknif e(X), loc(X)),
P recond(getknif e(X), at(knif e, X) at(monkey, X) hasknif e),
Effect(getknif e(X), true, hasknif e).
getbanana(X) - grab banana loc X, provided monkey box.
Action(getbanana(X), loc(X)),
P recond(getbanana(X), onbox(X) at(banana, X) hasbanana),
Effect(getbanana(X), true, hasbanana).
pickglass(X) - pick glass loc X.
Action(pickglass(X), loc(X)),
P recond(pickglass(X), at(glass, X) at(monkey, X) hasglass),
Effect(pickglass(X), true, hasglass).
getwater(X) - get water fountain loc X, provided monkey box,
glass hand.
Action(getwater(X), loc(X)),
P recond(getwater(X), at(f ountain, X) onbox(X) hasglass haswater),
Effect(getwater(X), true, haswater).
domain 27 actions 26 fluent atoms. Again, action, system
generates complete set fully instantiated successor state axioms STRIPSlike representation. instance, action goto(1, 2), following STRIPS-like
representation generated system:
303

fiLin

Action goto(1, 2)
Preconditions: at(monkey, 2), onFloor
Add list: at(monkey, 1)
Delete list: at(monkey, 2)
Conditional effects:
succ(at(banana, 1)) <-> init(hasbanana) \/ init(at(banana, 1))
succ(at(knife, 1)) <-> init(hasknife) \/ init(at(knife, 1))
succ(at(glass, 1)) <-> init(hasglass) \/ init(at(glass, 1))
succ(at(banana, 2)) <-> - init(hasbanana) & init(at(banana, 2))
succ(at(knife, 2)) <-> - init(hasknife) & init(at(knife, 2))
succ(at(glass, 2)) <-> - init(hasglass) & init(at(glass, 2))
total running time actions 8 seconds performing 20 million inferences.
90 percent time spent Step 4, i.e. computing strongest necessary
weakest sufficient conditions fluent atoms given action contextdependent effects. instance, action goto(1, 2) above, majority time spent
generating 6 conditional effects.
action, actually actions domain, could use ADL-like
description (Pednault, 1989) conditional effects:
Add list: at(banana,1) hasbanana
at(knife,1) hasknife
at(glass,1) hasglass
Delete list: at(banana,2) hasbanana
at(knife,2) hasknife
at(glass,2) hasglass
However, clear whether always done general case.
mentioned earlier specifications domain given McDermotts collection well others context-free. instance, following specification
action goto PDDL McDermotts collection:
(:action GO-TO
:parameters (?x ?y)
:precondition (and (location ?x) (location ?y)
(not (= ?y ?x)) (on-floor) (at monkey ?y))
:effect (and (at monkey ?x) (not (at monkey ?y))))
corresponds context-free action change fluent except at.
clear design action take account domain rules (15) - (17).
specification, initially banana location 1, goal banana
location 2 would achievable.
304

fiFrom Causal Theories STRIPS-Like Systems

5.3 Summary
domains experimented including scheduling domain includes
Pednaults dictionary paycheck domain special case, rocket domain, SRI
robot domain, machine shop assembling domain, ferry domain, grid domain,
sokoban domain, gear domain. included online appendix.
summarize common features domains:
domains tried, quite straightforward decide effects
action encoded direct effects (those given predicate Effect)
effects indirect effects (those derived domain rules).
common domain rules functional dependency constraints. instance,
blocks world, fluent atom on(x, y) functional arguments;
monkey banana domain, fluent atom at(object, loc) functional
second argument (each object one location). makes sense
would special shorthand domain rules, perhaps special
procedure handling well. significantly, given prevalence
functional dependency constraints action domains, worthwhile
investigate possibility general purpose planner making good use
constraints.
mentioned earlier, system propositional. generated successor state
axioms STRIPS-like systems fully instantiated. However, often easy
user generalize propositional specifications first-order ones.
shall investigate generality observation next.

6. Generalizing Propositional STRIPS-Like Systems Ones
Parameters
mentioned, many action domain descriptions, successor state axioms
STRIPS-like systems generated specific domain generalized arbitrary ones.
precisely, let domain description,
Domain(p1 , Dp1 ), , Domain(pk , Dpk )
type specification. Suppose action InitA SuccA |= .
suppose D0 another domain description like except different
type specification:
Domain(p1 , Dp0 1 ), , Domain(pk , Dp0 k ).
question interested this: given one-to-one mapping type
specification D0 , InitA0 SuccA0 |= 0 true D0 ? A0 (resp. 0 )
result replacing objects (resp. ) according mapping.
instance, true blocks world, generalize results
domain description Example 1 follows. shown, action stack(1, 2),
succ(on(1, 2)) succ(on(1, 3)) true. change type specification
Domain(block, {a, b, c, d, e}), map 1 a, 2 c, 3 e, new domain
305

fiLin

specification, action stack(a, c), succ(on(a, c)) succ(on(a, e))
true. Furthermore, changing mapping 3, see x different
c (the mapping needs one-to-one), succ(on(a, x)) true.
Obviously, expected blocks world. proceed show
general classes domain descriptions, well. first make precise
mapping one type specification another.
Definition 2 Given two type specifications O:
Domain(p1 , Dp1 ), , Domain(pk , Dpk ),
O0 :
Domain(p1 , Dp0 1 ), , Domain(pk , Dp0 k ),
embedding O0 one-to-one mapping Dp1 Dpk Dp0 1 Dp0 k
1 k, Dpi , f (a) Dp0 .
Clearly, embedding O0 , type p, size domain
p O0 must least size domain p O. Given embedding
, expression (actions, propositions, formulas) action domain description
type specification mapped () language D0 : one simply
replaces object (a), D0 differs uses O0
type specification. Notice objects (those domain type)
replaced, constants may occur effect axioms domain rules.
Definition 3 action domain description belongs simple-I class mention
function positive arity, mention complex fluents except complex fluent
definitions, satisfies following conditions:
1. P recond(A, ) action precondition definition, form
(x, p)...(y, q)W , W fluent formula quantifiers.
2. Effect(A, , F ) Effect(A, , F ) action effect axiom,
quantifiers, variables F among A. is, one
cannot something like
Effect(explodeAt(x), nearby(y, x), dead(y)).
3. Causes(, F ) Causes(, F ) domain rule, quantifiers, variables must F .
Theorem 2 Let simple-I action domain description, action instance
D. Let D0 like except type specification. formula
mention complex fluent quantifiers, embedding
type specification D0 , InitA SuccA |= D.
Init (A) Succ (A) |= () D0 .
306

fiFrom Causal Theories STRIPS-Like Systems

Proof: Suppose Init (A) Succ (A) |= () true, M1 truth assignment
language D0 satisfies Init (A) Succ (A) (). construct truth
assignment M2 language follows: proposition P language
mention complex fluent, M2 |= P iff M1 |= (P ) (P really either
static proposition, succ(F ),or init(F ), F primitive fluent atom). truth
values complex fluent atoms M2 defined according definitions. Clearly,
M2 |= . need show M2 also satisfies InitA SuccA . InitA ,
three cases:
1. M2 |= init(F ) init() Defined(F, ) complex fluent definition.
follows construction M2 .
2. M2 |= init(A ) P recond(A, ) precondition definition A.
assumption, form (x, p)...(y, q).W , W formula without
quantifiers. Without loss generality, let us assume (x, p)W . formula
equivalent
_
W (x/a)
aDp

D, Dp domain type p D. M2 |= (x, p)W iff
M2 |=

_

W (x/a)

aDp

iff
M1 |=

_

W (x/ (a)),

aDp

true since M1 |= (x, p)W .
3. formulas InitA mention complex fluents quantifiers.
true M2 corresponding ones true M1 .
SuccA , suppose F primitive fluent atom, pseudo-successor state axiom F
constructed according procedure CCP given Section 3 follows:
succ(F ) init(1 ) init(n ) succ(01 ) succ(0l )
init(F ) [init(1 ) init(m ) succ(01 ) succ(0k )].
following properties D:
effect axiom Effect(A, , F ) Effect(A, , F ) property
quantifier, variables also F ;
domain rule form Causes(, F ) Causes(, F ) property
quantifier, variables also F ;
pseudo-successor state axiom (succ(F )) D0 (F ). Thus M2 |= F
since M1 |= (F ). proves M2 model SuccA , thus theorem.
2

307

fiLin

However, examples paper belong simple-I
class, two reasons: action preconditions, like blocks world, mention
complex fluents; negative domain rules Causes(, F ) may
variables F . first problem problem principle complex fluents
replaced definitions. second problem serious, leads
new type simple action theories.
Definition 4 action domain description belongs simple-II class mention
function positive arity, mention complex fluents except complex fluent
definitions, satisfies following conditions:
1. P recond(A, ) action precondition definition, form
(x, p)...(y, q)W , W fluent formula quantifiers.
2. Effect(A, , F ) Effect(A, , F ) action effect axiom,
quantifiers, variables F among A.
3. positive domain rules form Causes(, F ).
4. Causes(, F ) domain rule, must form 1 2 , 1
formula mention fluents 2 fluent atom. Notice
restriction variables 2 .
Simple-II class action domain descriptions seem limited
positive domain rules, negative domain rules allowed binary. Nevertheless,
still capture many context-free action domains. instance, blocks world
meet-and-pass domains paper belong class: blocks world, notice
uses complex fluent clear action precondition definitions,
P recond(stack(x, y), ontable(x) clear(x) clear(y)), definitions reformulated
follows using clears definition:
P recond(stack(x, y),
ontable(x) (x1 , block)(y1 , block)(on(x1 , x) on(y1 , y))).
satisfy condition P recond definition simple-II
action domain descriptions. verified formally, seems
context-free action domains McDermotts PDDL library action domains, including
logistics domain, belong simple-II class.
Theorem 3 Let simple-II action domain description, action instance
D. Let D0 like except type specification. formula
mention complex fluent quantifiers, embedding
type specification D0 , InitA SuccA |=
Init (A) Succ (A) |= () D0 .
Proof: Suppose Init (A) Succ (A) |= () true, M1 truth assignment
language D0 satisfies Init (A) Succ (A) (). construct truth
308

fiFrom Causal Theories STRIPS-Like Systems

assignment M2 language follows: proposition P language
mention complex fluent, M2 |= P iff M1 |= (P ) (P really either
static proposition, succ(F ),or init(F ), F primitive fluent atom). truth
values complex fluent atoms M2 defined according definitions. Clearly,
M2 |= . need show M2 also satisfies InitA SuccA . InitA ,
three cases:
1. M2 |= init(F ) init() Defined(F, ) complex fluent definition.
follows construction M2 .
2. M2 |= init(A ) P recond(A, ) precondition definition A.
assumption, form (x, p)...(y, q).W , W formula without
quantifiers. Without loss generality, let us assume (x, p1 )W .
formula equivalent
W (x/a11 ) W (x/a1n1 )
D. M2 |= (x, p1 )W iff
M2 |= W (x/a11 ) W (x/a1n1 )
iff
M1 |= W (x/ (a11 )) W (x/ (a1n1 )),
true since M1 |= (x, p1 )W .
3. formulas InitA mention complex fluents quantifiers.
true M2 corresponding ones true M1 .
SuccA , suppose F primitive fluent atom. Since positive domain rule
form Causes(, F ), pseudo-successor state axiom F constructed according
procedure CCP given Section 3 must following form:
succ(F ) init(1 ) init(n )
init(F ) [init(1 ) init(m ) succ(01 ) succ(0k )],
1 k, Causes(0i , F ) instance domain rule D.
D0 , effect axiom Effect(, F ) Effect(, F ) property
quantifier, variables also F , pseudo-successor
state axiom (succ(F )) D0 must form:
succ( (F )) init( (1 )) init( (n ))
init( (F )) [init( (1 )) init( (m ))
succ( (01 ))



succ( (0k ))

(18)

succ()],

disjunction disjunct must Causes(, (F ))
instance D0 fluent atom contains object (A) (F ).
two cases:
309

fiLin

Suppose M2 |= succ(F ). M1 |= succ( (F )). Since M1 model Succ (A) ,
M1 satisfies axiom succ( (F )). Therefore M1 satisfies following
formula:
init( (1 )) init( (n ))
init( (F )) [init( (1 )) init( (m ))
succ( (01 )) succ( (0k ))].
Since formula mention complex fluents quantifiers,
M2 satisfies corresponding formula:
init(1 ) init(n )

(19)

init(F ) [init(1 ) init(m )

succ(01 )



succ(0k )],

right side equivalence pseudo-successor state axiom
succ(F ) SuccA .
suppose M2 satisfies (19). Well show M1 satisfies right side (18),
thus M1 |= succ( (F )) M2 |= succ(F ). two cases:
M2 satisfies following formula:
init(1 ) init(n ).

(20)

case, since formula mention complex fluents
quantifier, M1 satisfies following corresponding formula:
init( (1 )) init( (n )).

(21)

Thus M1 satisfies right side (18).
M2 satisfy (20) satisfies following formula:
init(F ) [init(1 ) init(m ) succ(01 ) succ(0k )].
Thus M1 satisfies following formula:
init( (F )) [init( (1 )) init( (m ))
succ( (01 )) succ( (0k ))].
show right side equivalence (18) satisfied M1 ,
need show M1 |= succ(). Recall disjunction
disjunct must correspond domain rule form Causes(, (F )),
form 1 G 1 mention fluents, G
fluent atom mentions object occur (A). Note
init() init( (F )) axiom Succ (A) , satisfied M1 . Thus
M1 |= init(). means either 1 init(G) false M1 . 1
false, succ() false since succ(1 ) 1 . Suppose
init(G) false M1 . Notice since positive domain rules,
310

fiFrom Causal Theories STRIPS-Like Systems

G object (A) (F ), pseudo-successor state axiom
G Succ (A) must form succ(G) init(G) . Therefore
M1 |= init(G) get M1 |= succ(). Since disjunct ,
proved M1 |= succ(). Therefore M1 |= succ( (F )). Thus M2 |= succ(F ).
2

7. Related Work
planning, closely related work causal reasoning module Wilkinss SIPE
system (Wilkins, 1988). Wilkins writes (page 85, Wilkins, 1988): Use STRIPS
assumptions made operators unacceptably difficult describe previous classical
planners... One primary reasons effects action must explicitly stated... Deductive causal theories one important mechanisms used
SIPE alleviate problems operator representation caused STRIPS assumption. certainly one motivations system well. SIPE, domain rules
triggers, preconditions, conditions, effects. Informally, triggers become
true new situation, SIPE would check sequence see preconditions
true old situation, conditions true new situation.
conditions true, deduce effects. instance, SIPE causal rule
on(x, y) blocks world would look like:
Causal-rule: Not-on
Arguments: x, y, z;
Trigger: on(x,y);
Precondition: on(x,z);
Effects: on(x,z);
comparison, domain rules much simpler. instance, corresponding rule
SIPE rule simply: Causes(on(x, y) 6= z, on(x, z)). need
procedural directives like triggers. large degree, see system rational
reconstruction causal reasoning module SIPE. shown Theorem 1,
procedure used system sound translation causal theories
situation calculus. Wilkins also gave translation causal rules formulas
situation calculus, specify underlying logic reason formulas.
fact, shown Lin (1995), translations would work.
familiar PDDL, original version McDermott AIPS-98
Planning Competition Committee allows domain axioms stratified theories. According
manual PDDL 1.2 (McDermott, 1998), axioms logical formulas assert
relationships among propositions hold within situation. format writing
axioms PDDL follows:
(:axiom
:vars (?x ?y ...)
:context W
:implies P)
311

fiLin

W formula P literal. Axioms treated directionally, W P .
following rule intention using axioms according manual:
rule action definitions allowed effects mention
predicates occur :implies field axiom. intention
action definitions mention primitive predicates like on, changes
truth value derived predicates like occur axioms. Without
axioms, action definitions describe changes predicates
might affected action, leads complex software engineering
(or domain engineering) problem.
clear quotation axioms PDDL intended defining derived
predicates. similar complex fluent definitions. New versions PDDL
extended original version allowing actions durations continuous changes.
considered using axioms derive changes primitive predicates like
done domain rules.
action domain description language, different syntax
strongly influenced Prolog syntax, shares much ideas behind action languages
(Gelfond & Lifschitz, 1999). However, unlike action languages, provide facilities expressing truth value fluent atom particular situation like initial
situation. Rather, aimed specifying generic effects actions. hand,
facilities specifying types static relations. importantly, date, action
languages either implemented directly mapped nonmonotonic logic programming
system rather compilation monotonic system action effects given
explicitly, done here. instance, new SAT-based planning method would
implemented (e.g. McCain & Turner, 1998) action languages. comparison,
action domain description compiled STRIPS-like description, existing planning
systems Blackbox (Selman & Kautz, 1999) System R (Lin, 2001b) directly
called.

8. Concluding Remarks
described system generating effects actions direct action effect
axioms domain rules, among things. shown soundness procedure used system tested successfully many benchmark action domains used
current AI planners. future work, considering generalize simple
action theories Section 6 include context-dependent action domain descriptions like
monkey bananas domain.

Acknowledgments
extended abstract part paper appeared Proceedings AAAI-2000. would
like thank anonymous reviewers JAIR AAAI2000 well associate
editor charge paper JAIR insightful comments earlier versions
paper. work supported part Research Grants Council Hong Kong
Competitive Earmarked Research Grant HKUST6061/00E.
312

fiFrom Causal Theories STRIPS-Like Systems

References
Baral, C. (1995). Reasoning actions: nondeterministic effects, constraints, qualification. Proceedings Fourteenth International Joint Conference Artificial
Intelligence (IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo,
CA., pp. 20172023.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach theorem proving
problem solving. Artificial Intelligence, 2, 189208.
Fox, M., & Long, D. (1998). automatic inference state invariants TIM. Journal
Artificial Intelligence Research, 9, 367421.
Gelfond, M., & Lifschitz, V. (1999). Action languages. Electronic Transactions Artificial
Intelligence, http://www.ep.liu.se/ea/cis, Vol 3, nr 016.
Gerevini, A., & Schubert, L. (1998). Inferring state constraints domain-independent
planning. Proceedings 15th National Conference Artificial Intelligence
(AAAI98), AAAI Press, Menlo Park, CA.
Levesque, H., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. (1997). GOLOG: logic
programming language dynamic domains. Journal Logic Programming, Special
issue Reasoning Action Change, 31, 5984.
Lifschitz, V. (1997). logic causal explanation. Artificial Intelligence, 96, 451465.
Lin, F. (1995). Embracing causality specifying indirect effects actions. Proceedings Fourteenth International Joint Conference Artificial Intelligence
(IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 1985
1993.
Lin, F. (1996). Embracing causality specifying indeterminate effects actions.
Proceedings 13th National Conference Artificial Intelligence (AAAI96),
AAAI Press, Menlo Park, CA., pp. 670676.
Lin, F. (2001a). strongest necessary weakest sufficient conditions. Artificial Intelligence, 128(1-2), 143159.
Lin, F. (2001b). planner called R. AI Magazine, 22(3), 7376.
Lin, F., & Reiter, R. (1994a). Forget it!. Greiner, R., & Subramanian, D. (Eds.),
Working Notes AAAI Fall Symposium Relevance, pp. 154159. American Association Artificial Intelligence, Menlo Park, CA. Also available
http://www.cs.toronto.edu/cogrobo/forgetting.ps.Z.
Lin, F., & Reiter, R. (1994b). State constraints revisited. Journal Logic Computation,
Special Issue Actions Processes, 4(5), 655678.
McCain, N., & Turner, H. (1995). causal theory ramifications qualifications.
Proceedings Fourteenth International Joint Conference Artificial Intelligence
(IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 1978
1984.
McCain, N., & Turner, H. (1997). Causal theories action change. Proceedings
14th National Conference Artificial Intelligence (AAAI97), AAAI Press,
Menlo Park, CA., pp. 460465.
313

fiLin

McCain, N., & Turner, H. (1998). Satisfiability planning causal theories. Proceedings
Sixth International Conference Principles Knowledge Representation
Reasoning (KR98), pp. 212221.
McDermott, D. (1998). PDDL planning domain definition language. Tech. rep. TR98-003/DCS TR-1165, Yale Center Computational Vision Control.
Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situation calculus. Proceedings First International Conference Principles
Knowledge Representation Reasoning (KR89), pp. 324332. Morgan Kaufmann
Publishers, Inc.
Reiter, R. (1991). frame problem situation calculus: simple solution (sometimes) completeness result goal regression. Lifschitz, V. (Ed.), Artificial
Intelligence Mathematical Theory Computation: Papers Honor John McCarthy, pp. 418420. Academic Press, San Diego, CA.
Selman, B., & Kautz, H. (1999). Unifying SAT-based graph-based planning. Proceedings Sixteenth International Joint Conference Artificial Intelligence (IJCAI
99), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 318325.
Thielscher, M. (1995). Computing ramification post-processing. Proceedings
Fourteenth International Joint Conference Artificial Intelligence (IJCAI95), IJCAI Inc. Distributed Morgan Kaufmann, San Mateo, CA., pp. 19942000.
Thielscher, M. (1997). Ramification causality. Artificial Intelligence, 89, 317364.
Wilkins, D. (1988). Practical planning: extending classical AI planning paradigm. Morgan Kaufmann, San Mateo, CA.
Zhang, Y., & Foo, N. (1997). Deriving invariants constraints action theories.
Fundamenta Informaticae, 30(1), 109123.

314

fiJournal Artificial Intelligence Research 19 (2003) 205-208

Submitted 12/02; published 9/03

Research Note
Potential-Based Shaping Q-Value Initialization
Equivalent
Eric Wiewiora

wiewiora@cs.ucsd.edu

Department Computer Science Engineering
University California, San Diego
La Jolla, CA 92093 0114

Abstract
Shaping proven powerful precarious means improving reinforcement
learning performance. Ng, Harada, Russell (1999) proposed potential-based shaping
algorithm adding shaping rewards way guarantees learner learn optimal
behavior.
note, prove certain similarities shaping algorithm initialization step required several reinforcement learning algorithms. specifically,
prove reinforcement learner initial Q-values based shaping algorithms potential function make updates throughout learning learner receiving
potential-based shaping rewards. prove broad category policies,
behavior two learners indistinguishable. comparison provides intuition
theoretical properties shaping algorithm well suggestion simpler
method capturing algorithms benefit. addition, equivalence raises previously
unaddressed issues concerning efficiency learning potential-based shaping.

1. Potential-Based Shaping
Shaping common technique improving learning performance reinforcement learning tasks. idea shaping provide learner supplemental rewards
encourage progress towards highly rewarding states environment. shaping
rewards applied arbitrarily, run risk distracting learner intended
goals environment. case, learner converges policy optimal
presence shaping rewards, suboptimal terms original task.
Ng, Harada, Russell (1999) proposed method adding shaping rewards way
guarantees optimal policy maintains optimality. model reinforcement
learning task Markov Decision Process (MDP), learner tries find policy
maximizes discounted future reward (Sutton & Barto, 1998). define potential
function () states. shaping reward transitioning state s0
defined terms as:
F (s, s0 ) = (s0 ) (s),
MDPs discount rate. shaping reward added environmental
reward every state transition learner experiences. potential function
viewed defining topography state space. shaping reward transitioning one state another therefore discounted change state potential.
Potential-based shaping guarantees cycle sequence states yields net
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWiewiora

benefit shaping. fact, standard conditions Ng et al. prove policy
optimal MDP augmented potential-based shaping reward also
optimal unaugmented MDP.

2. New Results
Many reinforcement learning algorithms learn optimal policy maintaining Q-values.
Q-values estimates expected future reward taking given action given
state. show effects potential-based shaping achieved initializing
learners Q-values state potential function. prove result
Q-learning algorithm, results extend Sarsa TD algorithms well.
define two reinforcement learners, L L0 , experience changes
Q-values throughout learning. Let initial values Ls Q-table Q(s, a) = Q0 (s, a).
potential-based shaping reward F based potential function applied
learning. learner, L0 , Q-table initialized Q00 (s, a) = Q0 (s, a) + (s).
learner receive shaping rewards.
Let experience 4-tuple hs, a, r, s0 i, representing learner taking action state
s, transitioning state s0 receiving reward r. learners Q-values updated
based experience using standard update rule Q-learning. Q(s, a) updated
potential-based shaping reward, Q0 (s, a) updated without shaping
reward:

Q(s, a) Q(s, a) + r + F (s, s0 ) + max
Q(s0 , a0 ) Q(s, a) ,
0

|
{z
}
0

0

0

0

Q(s,a)
0


Q (s, a) Q (s, a) + r + max
Q (s , ) Q0 (s, a) .
0

|
{z
}
Q0 (s,a)

equations interpreted updating Q-values error term
scaled , learning rate (assume learners). refer error
terms Q(s, a) Q0 (s, a). also track total change Q Q0 learning.
difference original current values Q(s, a) Q0 (s, a) referred
Q(s, a) Q0 (s, a), respectively. Q-values learners represented
initial values plus change values resulted updates:
Q(s, a) = Q0 (s, a) + Q(s, a)
Q0 (s, a) = Q0 (s, a) + (s) + Q0 (s, a).
Theorem 1 Given sequence experiences learning, Q(s, a) always
equals Q0 (s, a).
Proof: Proof induction. base case Q-table entries s0 still
initial values. theorem holds case, entries Q Q0
uniformly zero.
inductive case, assume entries Q(s, a) = Q0 (s, a) a.
show response experience hs, a, r, s0 i, error terms Q(s, a) Q0 (s, a)
206

fiPotential-Based Shaping Q-Value Initialization

equal. First examine update performed Q(s, a) presence potentialbased shaping reward:

Q(s, a) = r + F (s, s0 ) + max
Q(s0 , a0 ) Q(s, a)
0



0 0
0 0
= r + (s0 ) (s) + max
Q
(s
,

)
+
Q(s
,

)
Q0 (s, a) Q(s, a)
0
0


examine update performed Q0 :
Q0 (s0 , a0 ) Q0 (s, a)
Q0 (s, a) = r + max
0



0 0
0
0 0
= r + max
Q
(s
,

)
+
(s
)
+
Q(s
,

)
Q0 (s, a) (s) Q(s, a)
0
a0

= r + (s0 ) (s) + max
Q0 (s0 , a0 ) + Q(s0 , a0 ) Q0 (s, a) Q(s, a)
0


= Q(s, a)
Q-tables updated value, thus Q() Q0 () equal. 2
implications proof appreciated consider learner chooses
actions. policies defined terms learners Q-values. define advantagebased policy policy chooses action given state probability
determined differences Q-values state, absolute magnitude.
Thus, constant added Q-values, probability distribution
next action change.
Theorem 2 L L0 learned sequence experiences use
advantage-based policy, identical probability distribution next
action.
Proof: Recall Q-values defined:
Q(s, a) = Q0 (s, a) + Q(s, a)
Q0 (s, a) = Q0 (s, a) + (s) + Q0 (s, a)
proved Q(s, a) Q0 (s, a) equal updated
experiences. Therefore, difference two Q-tables addition
state potentials Q0 . addition uniform across actions given state,
affect policy.
2
turns almost policies used reinforcement learning advantage-based.
important policy greedy policy. two popular exploratory
policies, -greedy Boltzmann soft-max, also advantage-based.
policies, difference learning initialization described
potential-based shaping.
207

fiWiewiora

3. Shaping Goal-Directed Tasks
shown initial Q-values large influence efficiency
reinforcement learning goal directed tasks (Koenig & Simmons, 1996). problems
characterized state-space goal region. agents task find policy
reaches goal region quickly possible. Clearly agent must find goal state
least exploration optimal policy found. Q-values
initialized optimal value, agent may require learning time exponential
state action space order find goal state. However, deterministic environments,
optimistic initialization Q-values requires learning time polynomial stateaction space goal found. See Bertsekas Tsitsiklis (1996) analysis
reinforcement learning algorithms various initializations. potential-based
shaping equivalent Q-value initialization, care must taken choosing potential
function lead poor learning performance.

4. Conclusion
shown effects potential-based shaping captured particular
initialization Q-values agents using Q-learning. results extend Sarsa
TD methods. addition, results extend versions algorithms
augmented eligibility traces.
discrete-state environment, results imply one simply initialize
learners Q-values potential function rather alter learning algorithm
incorporate shaping rewards. case continuous state-spaces, potential-based
shaping may still offer benefit. continuous potential function state-space
would analogous continuous initialization state values. potential-based
shaping allows function defined state space used potential function,
method may beneficial agent restricted representation state. careful
analysis case would fruitful avenue future research.

Acknowledgements
research supported grant Matsushita Electric Industrial Co., Ltd.

References
Bertsekas, D. P., & Tsitsiklis, J. T. (1996). Neuro-dynamic Programming. Athena Scientific.
Koenig, S., & Simmons, R. (1996). effect representation knowledge goaldirected exploration reinforcement-learning algorithms. Machine Learning,
22 (1/3), 227 250.
Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance reward transformations:
theory application reward shaping. Machine Learning, Proceedings
Sixteenth International Conference, pp. 278287. Morgan Kaufmann.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT
Press.

208

fiJournal Artificial Intelligence Research 19 (2003) 399-468

Submitted 1/02; published 10/03

Efficient Solution Algorithms Factored MDPs
Carlos Guestrin

guestrin@cs.stanford.edu

Computer Science Dept., Stanford University

Daphne Koller

koller@cs.stanford.edu

Computer Science Dept., Stanford University

Ronald Parr

parr@cs.duke.edu

Computer Science Dept., Duke University

Shobha Venkataraman

shobha@cs.cmu.edu

Computer Science Dept., Carnegie Mellon University

Abstract
paper addresses problem planning uncertainty large Markov Decision
Processes (MDPs). Factored MDPs represent complex state space using state variables
transition model using dynamic Bayesian network. representation often allows
exponential reduction representation size structured MDPs, complexity exact
solution algorithms MDPs grow exponentially representation size. paper,
present two approximate solution algorithms exploit structure factored MDPs.
use approximate value function represented linear combination basis functions,
basis function involves small subset domain variables. key contribution
paper shows basic operations algorithms performed efficiently
closed form, exploiting additive context-specific structure factored MDP.
central element algorithms novel linear program decomposition technique, analogous
variable elimination Bayesian networks, reduces exponentially large LP provably
equivalent, polynomial-sized one. One algorithm uses approximate linear programming,
second approximate dynamic programming. dynamic programming algorithm novel
uses approximation based max-norm, technique directly minimizes terms
appear error bounds approximate MDP algorithms. provide experimental results
problems 1040 states, demonstrating promising indication scalability
approach, compare algorithm existing state-of-the-art approach, showing,
problems, exponential gains computation time.

1. Introduction
last years, Markov Decision Processes (MDPs) used basic
semantics optimal planning decision theoretic agents stochastic environments.
MDP framework, system modeled via set states evolve stochastically.
main problem representation that, virtually real-life domain,
state space quite large. However, many large MDPs significant internal structure,
modeled compactly structure exploited representation.
Factored MDPs (Boutilier, Dearden, & Goldszmidt, 2000) one approach representing large, structured MDPs compactly. framework, state implicitly described
assignment set state variables. dynamic Bayesian network (DBN) (Dean
& Kanazawa, 1989) allow compact representation transition model,
exploiting fact transition variable often depends small number
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiGuestrin, Koller, Parr & Venkataraman

variables. Furthermore, momentary rewards often also decomposed
sum rewards related individual variables small clusters variables.
two main types structure simultaneously exploited factored
MDPs: additive context-specific structure. Additive structure captures fact
typical large-scale systems often decomposed combination locally interacting components. example, consider management large factory many
production cells. course, long run, cell positioned early production line
generates faulty parts, whole factory may affected. However, quality
parts cell generates depends directly state cell quality
parts receives neighboring cells. additive structure also present
reward function. example, cost running factory depends, among things,
sum costs maintaining local cell.
Context-specific structure encodes different type locality influence: Although
part large system may, general, influenced state every part
system, given point time small number parts may influence directly.
factory example, cell responsible anodization may receive parts directly
cell factory. However, work order cylindrical part may restrict
dependency cells lathe. Thus, context producing cylindrical
parts, quality anodized parts depends directly state cells
lathe.
Even large MDP represented compactly, example, using factored
representation, solving exactly may still intractable: Typical exact MDP solution algorithms require manipulation value function, whose representation linear
number states, exponential number state variables. One approach
approximate solution using approximate value function compact representation. common choice use linear value functions approximation value
functions linear combination potentially non-linear basis functions (Bellman,
Kalaba, & Kotkin, 1963; Sutton, 1988; Tsitsiklis & Van Roy, 1996b). work builds
ideas Koller Parr (1999, 2000), using factored (linear) value functions,
basis function restricted small subset domain variables.
paper presents two new algorithms computing linear value function approximations factored MDPs: one uses approximate dynamic programming another
uses approximate linear programming. algorithms based use factored linear value functions, highly expressive function approximation method.
representation allows algorithms take advantage additive context-specific
structure, order produce high-quality approximate solutions efficiently. capability exploit types structure distinguishes algorithms differ earlier
approaches (Boutilier et al., 2000), exploit context-specific structure. provide
detailed discussion differences Section 10.
show that, factored MDP factored value functions, various critical operations planning algorithms implemented closed form without necessarily
enumerating entire state space. particular, new algorithms build upon
novel linear programming decomposition technique. technique reduces structured LPs
exponentially many constraints equivalent, polynomially-sized ones. decomposition follows procedure analogous variable elimination applies additively
400

fiEfficient Solution Algorithms Factored MDPs

structured value functions (Bertele & Brioschi, 1972) value functions also exploit context-specific structure (Zhang & Poole, 1999). Using basic operations,
planning algorithms implemented efficiently, even though size state space
grows exponentially number variables.
first method based approximate linear programming algorithm (Schweitzer
& Seidmann, 1985). algorithm generates linear, approximate value function
solving single linear program. Unfortunately, number constraints LP proposed
Schweitzer Seidmann grows exponentially number variables. Using LP
decomposition technique, exploit structure factored MDPs represent exactly
optimization problem exponentially fewer constraints.
terms approximate dynamic programming, paper makes twofold contribution.
First, provide new approach approximately solving MDPs using linear value
function. Previous approaches linear function approximation typically utilized
least squares (L2 -norm) approximation value function. Least squares approximations
incompatible convergence analyses MDPs, based max-norm.
provide first MDP solution algorithms value iteration policy iteration
use linear max-norm projection approximate value function, thereby directly
optimizing quantity appears provided error bounds. Second, show
exploit structure problem apply technique factored MDPs,
leveraging LP decomposition technique.
Although approximate dynamic programming currently possesses stronger theoretical
guarantees, experimental results suggest approximate linear programming
good alternative. Whereas former tends generate better policies set
basis functions, due simplicity computational advantages approximate linear
programming, add basis functions, obtaining better policy still requiring
less computation approximate dynamic programming approach.
Finally, present experimental results comparing approach work Boutilier
et al. (2000), illustrating tradeoffs two methods. particular,
problems significant context-specific structure value function, approach
faster due efficient handling value function representation. However,
cases significant context-specific structure problem, rather
value function, algorithm requires exponentially large value function
representation. classes problems, demonstrate using value function exploits additive context-specific structure, algorithm obtain
polynomial-time near-optimal approximation true value function.
paper starts presentation factored MDPs approximate solution algorithms MDPs. Section 4, describe basic operations used algorithms,
including LP decomposition technique. Section 5, present first two
algorithms: approximate linear programming algorithm factored MDPs. second
algorithm, approximate policy iteration max-norm projection, presented Section 6.
Section 7 describes approach efficiently computing bounds policy quality based
Bellman error. Section 8 shows extend methods deal context-specific
structure. paper concludes empirical evaluation Section 9 discussion
related work Section 10.
401

fiGuestrin, Koller, Parr & Venkataraman

paper greatly expanded version work published Guestrin
et al. (2001a), work presented Guestrin et al. (2001b, 2002).

2. Factored Markov Decision Processes
Markov decision process (MDP) mathematical framework sequential decision
problems stochastic domains. thus provides underlying semantics task
planning uncertainty. begin concise overview MDP framework,
describe representation factored MDPs.
2.1 Markov Decision Processes
briefly review MDP framework, referring reader books Bertsekas
Tsitsiklis (1996) Puterman (1994) in-depth review. Markov Decision Process
(MDP) defined 4-tuple (X, A, R, P ) where: X finite set |X| = N states;
finite set actions; R reward function R : X 7 R, R(x, a) represents
reward obtained agent state x taking action a; P Markovian
transition model P (x0 | x, a) represents probability going state x state
x0 action a. assume rewards bounded, is, exists Rmax
Rmax |R(x, a)| , x, a.
Example 2.1 Consider problem optimizing behavior system administrator
(SysAdmin) maintaining network computers. network, machine
connected subset machines. Various possible network topologies
defined manner (see Figure 1 examples). one simple network, might
connect machines ring, machine connected machines + 1 1. (In
example, assume addition subtraction performed modulo m.)
machine associated binary random variable Xi , representing whether
working failed. every time step, SysAdmin receives certain amount
money (reward) working machine. job SysAdmin decide
machine reboot; thus, + 1 possible actions time step: reboot one
machines nothing (only one machine rebooted per time step). machine
rebooted, working high probability next time step. Every machine
small probability failing time step. However, neighboring machine fails,
probability increases dramatically. failure probabilities define transition model
P (x0 | x, a), x particular assignment describing machines working
failed current time step, SysAdmins choice machine reboot x0
resulting state next time step.
assume MDP infinite horizon future rewards discounted
exponentially discount factor [0, 1). stationary policy MDP
mapping : X 7 A, (x) action agent takes state x. computer
network problem, possible configuration working failing machines, policy
would tell SysAdmin machine reboot. policy associated value
function V RN , V (x) discounted cumulative value agent gets
starts state x follows policy . precisely, value V state x
402

fiEfficient Solution Algorithms Factored MDPs

Server

Server

Star

Bidirectional Ring

Ring Star

Server

3 Legs

Ring Rings

Figure 1: Network topologies tested; status machine influence status
parent network.

policy given by:
V (x) = E

"
X





(t)

(t)

R X , (X

t=0


#

(0)
) X = x ,


X(t) random variable representing state system steps.
running example, value function represents much money SysAdmin expects
collect starts acting according network state x. value function
fixed policy fixed point set linear equations define value
state terms value possible successor states. formally, define:
Definition 2.2 DP operator, , stationary policy is:
V(x) = R(x, (x)) +

X

P (x0 | x, (x))V(x0 ).

x0

value function policy , V , fixed point operator: V = V .
optimal value function V describes optimal value agent achieve
starting state. V also defined set non-linear equations. case,
value state must maximal expected value achievable policy starting
state. precisely, define:
Definition 2.3 Bellman operator, , is:
V(x) = max[R(x, a) +


X

P (x0 | x, a)V(x0 )].

x0

optimal value function V fixed point : V = V .
value function V, define policy obtained acting greedily relative
V. words, state, agent takes action maximizes one-step
403

fiGuestrin, Koller, Parr & Venkataraman

utility, assuming V represents long-term utility achieved next state.
precisely, define:
Greedy(V)(x) = arg max[R(x, a) +


X

P (x0 | x, a)V(x0 )].

(1)

x0

greedy policy relative optimal value function V optimal policy =
Greedy(V ).
2.2 Factored MDPs
Factored MDPs representation language allows us exploit problem structure
represent exponentially large MDPs compactly. idea representing large
MDP using factored model first proposed Boutilier et al. (1995).
factored MDP, set states described via set random variables X =
{X1 , . . . , Xn }, Xi takes values finite domain Dom(Xi ). state x
defines value xi Dom(Xi ) variable Xi . general, use upper case letters
(e.g., X) denote random variables, lower case (e.g., x) denote values.
use boldface denote vectors variables (e.g., X) values (x). instantiation
Dom(Y) subset variables Z Y, use y[Z] denote value
variables Z instantiation y.
factored MDP, define state transition model using dynamic Bayesian
network (DBN) (Dean & Kanazawa, 1989). Let Xi denote variable Xi current
time Xi0 , variable next step. transition graph DBN
two-layer directed acyclic graph G whose nodes {X1 , . . . , Xn , X10 , . . . , Xn0 }. denote
parents Xi0 graph Parents (Xi0 ). simplicity exposition, assume
Parents (Xi0 ) X; thus, arcs DBN variables consecutive
time slices. (This assumption used expository purposes only; intra-time slice arcs
handled small modification presented Section 4.1.) node Xi0 associated
conditional probability distribution (CPD) P (Xi0 | Parents (Xi0 )). transition
probability P (x0 | x) defined be:
P (x0 | x) =



P (x0i | ui ) ,



ui value x variables Parents (Xi0 ).
Example 2.4 Consider instance SysAdmin problem four computers, labelled
M1 , . . . , M4 , unidirectional ring topology shown Figure 2(a). first task
modeling problem factored MDP define state space X. machine
associated binary random variable Xi , representing whether working
failed. Thus, state space represented four random variables: {X1 , X2 , X3 , X4 }.
next task define transition model, represented DBN. parents
next time step variables Xi0 depend network topology. Specifically, probability
machine fail next time step depends whether working current
time step status direct neighbors (parents topology) network
current time step. shown Figure 2(b), parents Xi0 example Xi
Xi1 . CPD Xi0 Xi = false, Xi0 = false high probability;
404

fiEfficient Solution Algorithms Factored MDPs

X1

X1
R

X2

M1

R

M4

M2

M3

(a)

2

X3
R

X4
R

1

3

P (Xi0 = | Xi , Xi1 , A):
h
1

h

2

X2

X3
h

3

X4
h

4

(b)

Action reboot:
machine machine

4

Xi1
Xi
Xi1
Xi
Xi1
Xi
Xi1
Xi

=f
=f
=f
=t
=t
=f
=t
=t






1

0.0238

1

0.475

1

0.0475

1

0.95

(c)

Figure 2: Factored MDP example: network topology (a) obtain factored
MDP representation (b) CPDs described (c).

is, failures tend persist. Xi = true, Xi0 noisy parents (in
unidirectional ring topology Xi0 one parent Xi1 ); is, failure
neighbors independently cause machine fail.
described represent factored Markovian transition dynamics arising
MDP DBN, directly addressed representation actions.
Generally, define transition dynamics MDP defining separate DBN
model = hGa , Pa action a.
Example 2.5 system administrator example, action ai rebooting
one machines, default action nothing. transition model
described corresponds nothing action. transition model ai
different transition model variable Xi0 , Xi0 = true
probability one, regardless status neighboring machines. Figure 2(c) shows
actual CPD P (Xi0 = W orking | Xi , Xi1 , A), one entry assignment
state variables Xi Xi1 , action A.
fully specify MDP, also need provide compact representation reward
function. assume reward function factored additively set localized
reward functions, depends small set variables. example,
might reward function associated machine i, depends Xi .
is, SysAdmin paid per-machine basis: every time step, receives money
machine working. formalize concept localized functions:
Definition 2.6 function f scope Scope[f ] = C X f : Dom(C) 7 R.
f scope Z, use f (z) shorthand f (y) part
instantiation z corresponds variables Y.
405

fiGuestrin, Koller, Parr & Venkataraman

characterize concept local rewards. Let R1a , . . . , Rra set
functions, scope Ria restricted variable cluster Uai {X1 , . . . , Xn }.
P
reward taking action state x defined Ra (x) = ri=1 Ria (Uai ) R.
example, reward function Ri associated machine i, depends
Xi , depend action choice. local rewards represented
diamonds Figure 2(b), usual notation influence diagrams (Howard &
Matheson, 1984).

3. Approximate Solution Algorithms
several algorithms compute optimal policy MDP. three
commonly used value iteration, policy iteration, linear programming. key component three algorithms computation value functions, defined Section 2.1.
Recall value function defines value state x state space.
explicit representation value function vector values different states,
solution algorithms implemented series simple algebraic steps. Thus,
case, three implemented efficiently.
Unfortunately, case factored MDPs, state space exponential number
variables domain. SysAdmin problem, example, state x system
assignment describing machines working failed; is, state x
assignment random variable Xi . Thus, number states exponential
number machines network (|X| = N = 2m ). Hence, even representing
explicit value function problems ten machines infeasible. One
might tempted believe factored transition dynamics rewards would result
factored value function, thereby represented compactly. Unfortunately, even
trivial factored MDPs, guarantee structure model preserved
value function (Koller & Parr, 1999).
section, discuss use approximate value function, admits
compact representation. also describe approximate versions exact algorithms,
use approximate value functions. description section somewhat abstract,
specify basic operations required algorithms performed
explicitly. later sections, elaborate issues, describe algorithms
detail. brevity, choose focus policy iteration linear programming;
techniques easily extend value iteration.
3.1 Linear Value Functions
popular choice approximating value functions using linear regression, first
proposed Bellman et al. (1963). Here, define space allowable value functions
V H RN via set basis functions:
Definition 3.1 linear value function set basis functions H = {h1 , . . . , hk }
P
function V written V(x) = kj=1 wj hj (x) coefficients w =
(w1 , . . . , wk )0 .
define H linear subspace RN spanned basis functions H.
useful define N k matrix H whose columns k basis functions viewed
406

fiEfficient Solution Algorithms Factored MDPs

vectors. compact notation, approximate value function represented
Hw.
expressive power linear representation equivalent, example,
single layer neural network features corresponding basis functions defining
H. features defined, must optimize coefficients w order obtain
good approximation true value function. view approach separating
problem defining reasonable space features induced space H,
problem searching within space. former problem typically purview
domain experts, latter focus analysis algorithmic design. Clearly,
feature selection important issue essentially areas learning approximation.
offer simple methods selecting good features MDPs Section 11,
goal address large important topic paper.
chosen linear value function representation set basis functions,
problem becomes one finding values weights w Hw yield
good approximation true value function. paper, consider two
approaches: approximate dynamic programming using policy iteration approximate
linear programming. section, present two approaches. Section 4,
show exploit problem structure transform approaches practical
algorithms deal exponentially large state spaces.
3.2 Policy Iteration
3.2.1 Exact Algorithm
exact policy iteration algorithm iterates policies, producing improved policy
iteration. Starting initial policy (0) , iteration consists two phases.
Value determination computes, policy (t) , value function V(t) , finding
fixed point equation T(t) V(t) = V(t) , is, unique solution set linear
equations:
X
P (x0 | x, (t) (x))V(t) (x0 ), x.
V(t) (x) = R(x, (t) (x)) +
x0

policy improvement step defines next policy
(t+1) = Greedy(V(t) ).
shown process converges optimal policy (Bertsekas & Tsitsiklis,
1996). Furthermore, practice, convergence optimal policy often quick.
3.2.2 Approximate Policy Iteration
steps policy iteration algorithm require manipulation value functions
policies, often cannot represented explicitly large MDPs. define
version policy iteration algorithm uses approximate value functions, use
following basic idea: restrict algorithm using value functions within
provided H; whenever algorithm takes step results value function V
outside space, project result back space finding value function
within space closest V. precisely:
407

fiGuestrin, Koller, Parr & Venkataraman

Definition 3.2 projection operator mapping : RN H. said
projection w.r.t. norm kk V = Hw w arg minw kHw Vk.
is, V linear combination basis functions, closest V respect
chosen norm.
approximate policy iteration algorithm performs policy improvement step exactly. value determination step, value function value acting according
current policy (t) approximated linear combination basis functions.
consider problem value determination policy (t) . point,
useful introduce notation: Although rewards function state
action choice, policy fixed, rewards become function state
only, denote R(t) , R(t) (x) = R(x, (t) (x)). Similarly, transition
model: P(t) (x0 | x) = P (x0 | x, (t) (x)). rewrite value determination step
terms matrices vectors. view V(t) R(t) N -vectors, P(t)
N N matrix, equations:
V(t) = R(t) + P(t) V(t) .
system linear equations one equation state,
solved exactly relatively small N . goal provide approximate solution, within
H. precisely, want find:
w(t) = arg min kHw (R(t) + P(t) Hw)k ;
w







= arg min (H P(t) H) w(t) R(t) .
w

Thus, approximate policy iteration alternates two steps:
w(t) = arg min kHw (R(t) + P(t) Hw)k ;
w

(t+1) = Greedy(Hw(t) ).

(2)
(3)

3.2.3 Max-norm Projection
approach along lines described used various papers, several
recent theoretical algorithmic results (Schweitzer & Seidmann, 1985; Tsitsiklis & Van
Roy, 1996b; Van Roy, 1998; Koller & Parr, 1999, 2000). However, approaches suffer
problem might call norm incompatibility. computing projection,
utilize standard Euclidean projection operator respect L2 norm
weighted L2 norm.1 hand, convergence error analyses MDP
algorithms utilize max-norm (L ). incompatibility made difficult provide
error guarantees.
tie projection operator closely error bounds use
projection operator L norm. problem minimizing L norm
studied optimization literature problem finding Chebyshev solution2
1. Weighted L2 norm projections stable meaningful error bounds weights correspond
stationary distribution fixed policy evaluation (value determination) (Van Roy, 1998),
stable combined . Averagers (Gordon, 1995) stable non-expansive
L , require mixture weights determined priori. Thus, not, general,
minimize L error.
2. Chebyshev norm also referred max, supremum L norms minimax solution.

408

fiEfficient Solution Algorithms Factored MDPs

overdetermined linear system equations (Cheney, 1982). problem defined
finding w that:
w arg min kCw bk .
(4)
w

use algorithm due Stiefel (1960), solves problem linear programming:
Variables: w1 , . . . , wk , ;
Minimize: ;
P
(5)
Subject to: kj=1 cij wj bi
Pk
bi j=1 cij wj , = 1...N.



P


constraints linear program imply kj=1 cij wj bi i,
equivalently, kCw bk . objective LP minimize . Thus,
solution (w , ) linear program, w solution Equation (4) L
projection error.
use L projection context approximate policy iteration
obvious way. implementing projection operation Equation (2), use
L projection (as Equation (4)), C = (H P(t) H) b = R(t) .
minimization solved using linear program (5).
key point LP k + 1 variables. However, 2N constraints,
makes impractical large state spaces. SysAdmin problem, example,
number constraints LP exponential number machines network
(a total 2 2m constraints machines). Section 4, show that, factored MDPs
linear value functions, 2N constraints represented efficiently, leading
tractable algorithm.
3.2.4 Error Analysis
motivated use max-norm projection within approximate policy iteration
algorithm via compatibility standard error analysis techniques MDP algorithms.
provide careful analysis impact L error introduced projection step. analysis provides motivation use projection step directly
minimizes quantity. acknowledge, however, main impact analysis
motivational. practice, cannot provide priori guarantees L projection
outperform methods.
goal analyze approximate policy iteration terms amount error
introduced step projection operation. error zero,
performing exact value determination, error accrue. error small,
get approximation accurate. result follows analysis below.
precisely, define projection error error resulting approximate
value determination step:








(t) = Hw(t) R(t) + P(t) Hw(t) .


Note that, using max-norm projection, finding set weights w(t)
exactly minimizes one-step projection error (t) . is, choosing best
409

fiGuestrin, Koller, Parr & Venkataraman

possible weights respect error measure. Furthermore, exactly error
measure going appear bounds theorem. Thus, make
bounds step tight possible.
first show projection error accrued step bounded:
Lemma 3.3 value determination error bounded: exists constant P Rmax
P (t) iterations algorithm.
Proof: See Appendix A.1.
Due contraction property Bellman operator, overall accumulated error
decaying average projection error incurred throughout iterations:
Definition 3.4 discounted value determination error iteration defined as:
(t1)
(0)
(t) +
; = 0.

(t)

=

Lemma 3.3 implies accumulated error remains bounded approximate policy
(t)
(1 )
iteration: P 1
. bound loss incurred acting according
policy generated approximate policy iteration algorithm, opposed
optimal policy:
Theorem 3.5 approximate policy iteration algorithm, let (t) policy generated
iteration t. Furthermore, let V(t) actual value acting according policy.
loss incurred using policy (t) opposed optimal policy value V
bounded by:
(t)
2



kV V(t) k kV V(0) k +
.
(6)
(1 )2
Proof: See Appendix A.2.
words, Equation (6) shows difference approximation iteration
optimal value function bounded sum two terms. first term
present standard policy iteration goes zero exponentially fast. second
discounted accumulated projection error and, Lemma 3.3 shows, bounded. second
term minimized choosing w(t) one minimizes:





Hw(t) R(t) + P(t) Hw(t)



,

exactly computation performed max-norm projection. Therefore,
theorem motivates use max-norm projections minimize error term appears
bound.
bounds provided far may seem fairly trivial, provided
strong priori bound (t) . Fortunately, several factors make bounds interesting despite lack priori guarantees. approximate policy iteration converges,
b policy
occurred experiments, obtain much tighter bound:
convergence, then:



V V 2b
,
b

(1 )
b one-step max-norm projection error associated estimating value
b . Since max-norm projection operation provides b

, easily obtain
410

fiEfficient Solution Algorithms Factored MDPs

posteriori bound part policy iteration procedure. details provided
Section 7.
One could rewrite bound Theorem 3.5 terms worst case projection error P , worst projection error cycle policies, approximate policy iteration
gets stuck cycle. formulations would closer analysis Bertsekas
Tsitsiklis (1996, Proposition 6.2, p.276). However, consider case policies
(or policies final cycle) low projection error, policies
cannot approximated well using projection operation, large
one-step projection error. worst-case bound would loose, would
dictated error difficult policy approximate. hand, using
discounted accumulated error formulation, errors introduced policies hard
approximate decay rapidly. Thus, error bound represents average case
analysis: decaying average projection errors policies encountered successive iterations algorithm. convergent case, bound computed
easily part policy iteration procedure max-norm projection used.
practical benefit posteriori bounds give meaningful feedback
impact choice value function approximation architecture.
explicitly addressing difficult general problem feature selection paper,
error bounds motivate algorithms aim minimize error given approximation
architecture provide feedback could useful future efforts automatically
discover improve approximation architectures.
3.3 Approximate Linear Programming
3.3.1 Exact Algorithm
Linear programming provides alternative method solving MDPs. formulates
problem finding value function linear program (LP). LP variables
V1 , . . . , VN , Vi represents V(xi ): value starting ith state system.
LP given by:
Variables: V1 , . . . , VN ;
P
Minimize:
xi (xi ) Vi ;
P
Subject to: Vi [R(xi , a) + j P (xj | xi , a)Vj ] xi X, A,

(7)

state relevance weights positive. Note that, exact case, solution
obtained positive weight vector. interesting note steps
simplex algorithm correspond policy changes single states, steps policy
iteration involve policy changes multiple states. practice, policy iteration tends
faster linear programming approach (Puterman, 1994).
3.3.2 Approximate Linear Program
approximate formulation LP approach, first proposed Schweitzer Seidmann (1985), restricts space allowable value functions linear space spanned
basis functions. approximate formulation, variables w1 , . . . , wk :
weights basis functions. LP given by:
411

fiGuestrin, Koller, Parr & Venkataraman

Variables: w1 , . . . , wk ;
P
P
Minimize:
(x) wi hi (x) ;
Px
P
P
0
0
Subject to:
wi hi (x) [R(x, a) +
x0 P (x | x, a)
wi hi (x )] x X, A.
(8)
words, formulation takes LP (7) substitutes explicit state
P
value function linear value function representation wi hi (x), or, compact
notation, V replaced Hw. linear program guaranteed feasible constant
function function constant value states included set
basis functions.
approximate linear programming formulation, choice state relevance weights,
, becomes important. Intuitively, constraints LP binding; is,
constraints tighter states others. state x, relevance
weight (x) indicates relative importance tight constraint. Therefore, unlike
exact case, solution obtained may differ different choices positive weight vector
. Furthermore, is, general, guarantee quality greedy policy
generated approximation Hw. However, recent work de Farias Van
Roy (2001a) provides analysis error relative best possible approximation subspace, guidance selecting improve quality
approximation. particular, analysis shows LP provides best
approximation Hw optimal value function V weighted L1 sense subject
constraint Hw Hw , weights L1 norm state relevance
weights .
transformation exact approximate problem formulation effect reducing number free variables LP k (one basis function
coefficient), number constraints remains N |A|. SysAdmin problem,
example, number constraints LP (8) (m + 1) 2m , number
machines network. Thus, process generating constraints solving
LP still seems unmanageable machines. next section, discuss
use structure factored MDP provide compact representation
efficient solution LP.

4. Factored Value Functions
linear value function approach, algorithms described Section 3, apply
choice basis functions. context factored MDPs, Koller Parr (1999) suggest
particular type basis function, particularly compatible structure
factored MDP. suggest that, although value function typically structured,
many cases might close structured. is, might wellapproximated using linear combination functions refers small
number variables. precisely, define:
Definition 4.1 factored (linear) value function linear function basis set
h1 , . . . , hk , scope hi restricted subset variables Ci .
Value functions type long history area multi-attribute utility theory (Keeney & Raiffa, 1976). example, might basis function hi
412

fiEfficient Solution Algorithms Factored MDPs

machine, indicating whether working not. basis function scope restricted
Xi . represented diamonds next time step Figure 2(b).
Factored value functions provide key performing efficient computations
exponential-sized state spaces factored MDPs. main insight restricted scope functions (including basis functions) allow certain basic operations
implemented efficiently. remainder section, show structure
factored MDPs exploited perform two crucial operations efficiently: one-step
lookahead (backprojection), representation exponentially many constraints
LPs. Then, use basic building blocks formulate efficient approximation algorithms factored MDPs, presented self-contained section:
approximate linear programming factored MDPs Section 5, approximate policy
iteration max-norm projection Section 6.
4.1 One-step Lookahead
key step algorithms computation one-step lookahead value
action a. necessary, example, computing greedy policy
Equation (1). Lets consider computation Q function, Qa (x), represents
expected value agent obtains taking action current time step receiving
long-term value V thereafter. Q function computed by:
Qa (x) = R(x, a) +

X

P (x0 | x, a)V(x).

(9)

x0

is, Qa (x) given current reward plus discounted expected future value.
Using notation, express greedy policy as: Greedy(V)(x) = maxa Qa (x).
Recall estimating long-term value policy using set basis
P
functions: V(x) = wi hi (x). Thus, rewrite Equation (9) as:
Qa (x) = R(x, a) +

X

P (x0 | x, a)

X

x0

wi hi (x).

(10)



P

size state space exponential, computing expectation x0 P (x0 |
P
x, a) wi hi (x) seems infeasible. Fortunately, discussed Koller Parr (1999),
expectation operation, backprojection, performed efficiently transition
model value function factored appropriately. linearity value
function permits linear decomposition, summand expectation
viewed independent value function updated manner similar value
iteration procedure used Boutilier et al. (2000). recap construction briefly,
first defining:
Ga (x) =

X
x0

P (x0 | x, a)

X

wi hi (x0 ) =



X


wi

X

P (x0 | x, a)hi (x0 ).

x0

Thus, compute expectation basis function separately:
gia (x) =

X

P (x0 | x, a)hi (x0 ),

x0

413

fiGuestrin, Koller, Parr & Venkataraman

P

weight wi obtain total expectation Ga (x) = wi gia (x).
intermediate function gia called backprojection basis function hi
transition model Pa , denote gia = Pa hi . Note that, factored MDPs,
transition model Pa factored (represented DBN) basis functions hi
scope restricted small set variables. two important properties allow us
compute backprojections efficiently.
show restricted scope function h (such basis functions)
backprojected transition model P represented DBN .
h scope restricted Y; goal compute g = P h. define backprojected scope set parents Y0 transition graph G ;
(Y0 ) = Yi0 Y0 Parents (Yi0 ). intra-time slice arcs included, Parents (Xi0 )
{X1 , . . . , Xn , X10 , . . . , Xn0 }, change algorithm definition backprojected scope . definition includes direct parents 0 ,
also variables {X1 , . . . , Xn } ancestors 0 :
(Y0 ) = {Xj | exist directed path Xj Xi0 Y0 }.
Thus, backprojected scope may become larger, functions still factored.
show that, h scope restricted Y, backprojection g
scope restricted parents Y0 , i.e., (Y0 ). Furthermore, backprojection
computed enumerating settings variables (Y0 ), rather settings
variables X:
g(x) = (P h)(x);
=

X

P (x0 | x)h(x0 );

x0

=

X

P (x0 | x)h(y0 );

x0

=

X
y0

=

X

X

P (y0 | x)h(y0 )

P (u0 | x);

u0 (x0 y0 )

P (y0 | z)h(y0 );

y0

= g(z);
P

z value (Y0 ) x term u0 (x0 y0 ) P (u0 | x) = 1
sum probability distribution complete domain. Therefore, see (P h)
function whose scope restricted (Y0 ). Note cost computation depends
linearly |Dom( (Y0 ))|, depends (the scope h) complexity
process dynamics. backprojection procedure summarized Figure 3.
Returning example, consider basis function hi indicator variable Xi :
takes value 1 ith machine working 0 otherwise. hi scope restricted
Xi0 , thus, backprojection gi scope restricted Parents (Xi0 ): (Xi0 ) = {Xi1 , Xi }.
4.2 Representing Exponentially Many Constraints
seen Section 3, approximation algorithms require solution linear programs: LP (5) approximate policy iteration, LP (8) approximate
414

fiEfficient Solution Algorithms Factored MDPs

Backproja (h)

basis function h scope C.

Define scope backprojection: (C0 ) = Xi0 C0 Parentsa (Xi0 ).
0
assignment
P
Q (C ):0 0

g (y) = c0 C0 i|X 0 C0 Pa (c [Xi ] | y)h(c0 ).


Return g .

Figure 3: Backprojection basis function h.
linear programming algorithm. LPs common characteristics:
small number free variables (for k basis functions k + 1 free variables approximate policy iteration k approximate linear programming), number
constraints still exponential number state variables. However, factored MDPs,
LP constraints another useful property: functionals constraints
restricted scope. key observation allows us represent constraints
compactly.
First, observe constraints linear programs form:


X

wi ci (x) b(x), x,

(11)



w1 , . . . , wk free variables LP x ranges states.
general form represents type constraint max-norm projection LP (5)
approximate linear programming formulation (8).3
first insight construction replace entire set constraints
Equation (11) one equivalent non-linear constraint:
max
x

X

wi ci (x) b(x).

(12)



second insight new non-linear constraint implemented set
linear constraints using construction follows structure variable elimination
cost networks. insight allows us exploit structure factored MDPs represent
constraint compactly.
tackle problem representing constraint Equation (12) two steps:
first, computing maximum assignment fixed set weights; then, representing
non-linear constraint small set linear constraints, using construction call
factored LP.
4.2.1 Maximizing State Space
key computation algorithms represent non-linear constraint form
Equation (12) efficiently small set linear constraints. presenting construction, lets first consider simpler problem: Given fixed weights wi , would
P
like compute maximization: = maxx wi ci (x) b(x), is, state x,
P

3. complementary constraints (5), b(x) wi ci (x), formulated using analogous
construction one present section changing sign ci (x) b(x). approximate
linear programming constraints (8) also formulated form, show Section 5.

415

fiGuestrin, Koller, Parr & Venkataraman

P

difference wi ci (x) b(x) maximal. However, cannot explicitly enumerate exponential number states compute difference. Fortunately,
structure factored MDPs allows us compute maximum efficiently.
case factored MDPs, state space set vectors x assignments state variables X = {X1 , . . . , Xn }. view Cw b functions
state variables, hence also difference. Thus, define function
P
F w (X1 , . . . , Xn ) F w (x) = wi ci (x) b(x). Note executed
representation shift; viewing F w function variables X, parameterized w. Recall size state space exponential number
variables. Hence, goal section compute maxx F w (x) without explicitly
considering exponentially many states. solution use fact F w
P
factored representation. precisely, Cw form wi ci (Zi ), Zi
subset X. example, might c1 (X1 , X2 ) takes value 1 states
X1 = true X2 = false 0 otherwise. Similarly, vector b case also sum
P
restricted scope functions. Thus, express F w sum j fjw (Zj ), fjw may
may depend w. future, sometimes drop superscript w
clear context.
P
Using compact notation, goal simply compute maxx wi ci (x)
b(x) = maxx F w (x), is, find state x F w maximized. Recall
P
w
Fw =
j=1 fj (Zj ). maximize function, F , without enumerating every state
using non-serial dynamic programming (Bertele & Brioschi, 1972). idea virtually
identical variable elimination Bayesian network. review construction here,
central component solution LP.
goal compute
X
max
fj (x[Zj ]).
x1 ,...,xn

j

main idea that, rather summing functions maximization,
maximize variables one time. maximizing xl , summands
involving xl participate maximization.
Example 4.2 Assume
F = f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).
therefore wish compute:
max

x1 ,x2 ,x3 ,x4

f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).

first compute maximum x4 ; functions f1 f2 irrelevant,
push out. get
max f1 (x1 , x2 ) + f2 (x1 , x3 ) + max[f3 (x2 , x4 ) + f4 (x3 , x4 )].

x1 ,x2 ,x3

x4

result internal maximization depends values x2 , x3 ; thus, introduce new function e1 (X2 , X3 ) whose value point x2 , x3 value internal
max expression. problem reduces computing
max f1 (x1 , x2 ) + f2 (x1 , x3 ) + e1 (x2 , x3 ),

x1 ,x2 ,x3

416

fiEfficient Solution Algorithms Factored MDPs

VariableElimination (F, O)
//F = {f1 , . . . , fm } set functions maximized;
//O stores elimination order.

= 1 number variables:
//Select next variable eliminated.

Let l = O(i) ;
//Select relevant functions.

Let e1 , . . . , eL functions F whose scope contains Xl .
//Maximize current variable Xl .

Define new function e = maxxl
L
j=1 Scope[ej ] {Xl }.

PL

j=1 ej

; note Scope[e] =

//Update set functions.

Update set functions F = F {e} \ {e1 , . . . , eL }.
//Now, functions empty scope
P sum maximum value f1 + + fm .

Return maximum value

ei F

ei .

Figure 4: Variable elimination procedure computing maximum value f1 + + fm ,
fi restricted scope function.

one fewer variable. Next, eliminate another variable, say X3 , resulting
expression reducing to:
max f1 (x1 , x2 ) + e2 (x1 , x2 ),
x1 ,x2



e2 (x1 , x2 ) = max[f2 (x1 , x3 ) + e1 (x2 , x3 )].
x3

Finally, define
e3 = max f1 (x1 , x2 ) + e2 (x1 , x2 ).
x1 ,x2

result point number, desired maximum x1 , . . . , x4 .
naive approach enumerating states requires 63 arithmetic operations variables
binary, using variable elimination need perform 23 operations.
general variable elimination algorithm described Figure 4. inputs
algorithm functions maximized F = {f1 , . . . , fm } elimination
ordering variables, O(i) returns ith variable eliminated.
example above, variable Xl eliminated, select relevant functions
e1 , . . . , eL , whose scope contains Xl . functions removed set F
P
introduce new function e = maxxl L
j=1 ej . point, scope functions
F longer depends Xl , is, Xl eliminated. procedure repeated
variables eliminated. remaining functions F thus empty
scope. desired maximum therefore given sum remaining functions.
computational cost algorithm linear number new function
values introduced elimination process. precisely, consider computation
new function e whose scope Z. compute function, need compute |Dom[Z]|
different values. cost algorithm linear overall number values,
introduced throughout execution. shown Dechter (1999), cost exponential
417

fiGuestrin, Koller, Parr & Venkataraman

induced width cost network, undirected graph defined variables
X1 , . . . , Xn , edge Xl Xm appear together one original
functions fj . complexity algorithm is, course, dependent variable
elimination order problem structure. Computing optimal elimination order
NP-hard problem (Arnborg, Corneil, & Proskurowski, 1987) elimination orders
yielding low induced tree width exist problems. issues
confronted successfully large variety practical problems Bayesian network
community, benefited large variety good heuristics
developed variable elimination ordering problem (Bertele & Brioschi, 1972; Kjaerulff,
1990; Reed, 1992; Becker & Geiger, 2001).
4.2.2 Factored LP
section, present centerpiece planning algorithms: new, general
approach compactly representing exponentially large sets LP constraints problems
factored structure functions constraints decomposed
sum restricted scope functions. Consider original problem representing
non-linear constraint Equation (12) compactly. Recall wish represent
P
non-linear constraint maxx wi ci (x) b(x), equivalently, maxx F w (x),
without generating one constraint state Equation (11). new, key insight
non-linear constraint implemented using construction follows
structure variable elimination cost networks.
Consider function e used within F (including original fi s), let Z scope.
assignment z Z, introduce variable uez , whose value represents ez ,
linear program. initial functions fiw , include constraint ufzi = fiw (z).
fiw linear w, constraint linear LP variables. Now, consider new function
e introduced F eliminating variable Xl . Let e1 , . . . , eL functions extracted
F, let Z scope resulting e. introduce set constraints:
uez



L
X
ej
j=1

u(z,xl )[Zj ]

xl .

(13)

Let en last function generated elimination, recall scope empty.
Hence, single variable uen . introduce additional constraint uen .
complete algorithm, presented Figure 5, divided three parts: First,
generate equality constraints functions depend weights wi (basis functions).
second part, add equality constraints functions depend
weights (target functions). equality constraints let us abstract away differences
two types functions manage unified fashion third
part algorithm. third part follows procedure similar variable elimination
described Figure 4. However, unlike standard variable elimination would inP
troduce new function e, e = maxxl L
j=1 ej , factored LP procedure
introduce new LP variables uez . enforce definition e maximum Xl
PL
j=1 ej , introduce new LP constraints Equation (13).
Example 4.3 understand construction, consider simple example above,
assume want express fact maxx F w (x). first introduce set
418

fiEfficient Solution Algorithms Factored MDPs

FactoredLP (C, b,O)
// C = {c1 , . . . , ck } set basis functions.
// b = {b1 , . . . , bm } set target functions.
//O stores elimination order.
P
P
//Return (polynomial) set constraints equivalent wi ci (x) + j bj (x), x .
//Data structure constraints factored LP.

Let = {} .
//Data structure intermediate functions generated variable elimination.

Let F = {} .
//Generate equality constraint abstract away basis functions.

ci C:
Let Z = Scope[ci ].
assignment z Z, create new LP variable ufzi add
constraint :
ufzi = wi ci (z).
Store new function fi use variable elimination step: F = F {fi }.
//Generate equality constraint abstract away target functions.

bj b:
Let Z = Scope[bj ].
f
assignment z Z, create new LP variable uzj add
constraint :
f
uzj = bj (z).
Store new function fj use variable elimination step: F = F {fj }.
//Now, F contains functions involved LP, constraints become:
P
e (x), x , represent compactly using variable elimination procedure.
e F


= 1 number variables:

//Select next variable eliminated.

Let l = O(i) ;
//Select relevant functions.

Let e1 , . . . , eL functions F whose scope contains Xl , let
Zj = Scope[ej ].
//Introduce linear constraints maximum current variable Xl .

Define new function e scope Z = L
j=1 Zj {Xl } represent
PL
maxxl j=1 ej .
Add constraints enforce maximum: assignment z Z:
uez

L
X

e

j
u(z,x
l )[Zj ]

xl .

j=1

//Update set functions.

Update set functions F = F {e} \ {e1 , . . . , eL }.
//Now, variables eliminated functions empty scope.

Add last constraint :


X

ei .

ei F

Return .

Figure 5: Factored LP algorithm compact representation exponential set
P
P
constraints wi ci (x) + j bj (x), x.
419

fiGuestrin, Koller, Parr & Venkataraman

variables ufx11 ,x2 every instantiation values x1 , x2 variables X1 , X2 . Thus,
X1 X2 binary, four variables. introduce constraint
defining value ufx11 ,x2 appropriately. example, f1 above, uft,t1 = 0
uft,f1 = w1 . similar variables constraints fj value z
Zj . Note constraints simple equality constraint involving numerical
constants perhaps weight variables w.
Next, introduce variables intermediate expressions generated variable elimination. example, eliminating X4 , introduce set LP variables
uex12 ,x3 ; them, set constraints
uex12 ,x3 ufx32 ,x4 + ufx43 ,x4
one value x4 X4 . similar set constraint uex21 ,x2 terms
ufx21 ,x3 uex12 ,x3 . Note constraint simple linear inequality.
prove factored LP construction represents constraint
non-linear constraint Equation (12):
Theorem 4.4 constraints generated factored LP construction equivalent
non-linear constraint Equation (12). is, assignment (, w) satisfies
factored LP constraints satisfies constraint Equation (12).
Proof: See Appendix A.3.
P
Returning original formulation, j fjw Cw b original
set constraints. Hence new set constraints equivalent original set:
P
maxx wi ci (x) b(x) Equation (12), turn equivalent exponential
P
set constraints wi ci (x) b(x), x Equation (11). Thus, represent
exponential set constraints new set constraints LP variables. size
new set, variable elimination, exponential induced width cost
network, rather total number variables.
section, presented new, general approach compactly representing exponentially-large sets LP constraints problems factored structure. remainder
paper, exploit construction design efficient planning algorithms factored
MDPs.
4.2.3 Factored Max-norm Projection
use procedure representing exponential number constraints
Equation (11) compactly compute efficient max-norm projections, Equation (4):
w arg min kCw bk .
w

max-norm projection computed linear program (5). two sets
P
P
constraints LP: kj=1 cij wj bi , bi kj=1 cij wj , i.
sets instance constraints Equation (11), addressed
previous section. Thus, k basis functions C restricted scope
function target function b sum restricted scope functions,
use factored LP technique represent constraints max-norm projection LP
compactly. correctness algorithm corollary Theorem 4.4:
420

fiEfficient Solution Algorithms Factored MDPs

Corollary 4.5 solution ( , w ) linear program minimizes subject
constraints FactoredLP(C, b,O) FactoredLP(C, b,O), elimination
order satisfies:
w arg min kCw bk ,
w



= min kCw bk .
w

original max-norm projection LP k + 1 variables two constraints
state x; thus, number constraints exponential number state variables.
hand, new factored max-norm projection LP variables,
exponentially fewer constraints. number variables constraints new factored
LP exponential number state variables largest factor cost
network, rather exponential total number state variables. show
Section 9, exponential gain allows us compute max-norm projections efficiently
solving large factored MDPs.

5. Approximate Linear Programming
begin simplest approximate MDP solution algorithms, based
approximate linear programming formulation Section 3.3. Using basic operations
described Section 4, formulate algorithm simple efficient.
5.1 Algorithm
discussed Section 3.3, approximate linear program formulation based linear
programming approach solving MDPs presented Section 3.3. However, approximate version, restrict space value functions linear space defined
basis functions. precisely, approximate LP formulation, variables
w1 , . . . , wk weights basis functions. LP given by:
Variables: w1 , . . . , wk ;
P
P
Minimize:
(x) wi hi (x) ;
x
P
P
P
0
0
Subject to:
wi hi (x) [R(x, a) +
x0 P (x | x, a)
wi hi (x )] x X, A.
(14)
words, formulation takes LP (7) substitutes explicit state value
P
function linear value function representation wi hi (x). transformation
exact approximate problem formulation effect reducing number
free variables LP k (one basis function coefficient), number
constraints remains |X| |A|. SysAdmin problem, example, number
constraints LP (14) (m + 1) 2m , number machines
network. However, using algorithm representing exponentially large constraint sets
compactly able compute solution approximate linear programming
algorithm closed form exponentially smaller LP, Section 4.2.
P
P
First, consider objective function x (x) wi hi (x) LP (14). Naively
representing objective function requires summation exponentially large state
space. However, rewrite objective obtain compact representation.
first reorder terms:
421

fiGuestrin, Koller, Parr & Venkataraman

FactoredALP (P , R, , H, O, )
//P factored transition model.
//R set factored reward functions.
// discount factor.
//H set basis functions H = {h1 , . . . , hk }.
//O stores elimination order.
// state relevance weights.
//Return basis function weights w computed approximate linear programming.
//Cache backprojections basis functions.

basis function hi H; action a:
Let gia = Backproja (hi ).
//Compute factored state relevance weights.

basis function hi , compute factored state relevance weights
Equation (15) .
//Generate approximate linear programming constraints

Let = {}.
action a:
}, Ra , O).
Let = FactoredLP({g1a h1 , . . . , gka hkP

P

//So far, constraints guarantee R(x, a) + x0 P (x0 | x, a) wi hi (x0 )
P
w hi (x); satisfy approximate linear programming solution (14) must add

final constraint.

Let = { = 0}.
//We obtain solution weights solving LP.

Let w solution linear program: minimize
constraints .
Return w.

P


wi , subject

Figure 6: Factored approximate linear programming algorithm.

422

fiEfficient Solution Algorithms Factored MDPs

X

(x)

X

x

wi hi (x) =

X



X

wi

(x) hi (x).

x



Now, consider state relevance weights (x) distribution states, (x) > 0
P
x (x) = 1. backprojections, write:
=

X

X

(x) hi (x) =

x

(ci ) hi (ci );

(15)

ci Ci

(ci ) represents marginal state relevance weights domain
Dom[Ci ] basis function hi . example, use uniform state relevance weights
1
experiments (x) = |X|
marginals become (ci ) = |C1i | . Thus,
P
rewrite objective function wi , basis weight computed shown
Equation (15). state relevance weights represented marginals, cost
computing depends exponentially size scope Ci only, rather
exponentially number state variables. hand, state relevance
weights represented arbitrary distributions, need obtain marginals
Ci s, may efficient computation. Thus, greatest efficiency achieved
using compact representation, Bayesian network, state relevance weights.
Second, note right side constraints LP (14) correspond Qa
functions:
X
X
Qa (x) = Ra (x) +
P (x0 | x, a)
wi hi (x0 ).
x0



Using efficient backprojection operation factored MDPs described Section 4.1
rewrite Qa functions as:
Qa (x) = Ra (x) +

X

wi gia (x);



gia


backprojection basis function hi transition model Pa .
discussed, hi scope restricted Ci , gia restricted scope function (C0i ).
precompute backprojections gia basis relevance weights .
approximate linear programming LP (14) written as:
Variables: w1 , . . . , wk ;
P
Minimize:
w ;
Pi
P


Subject to:
w
hi (x) [R (x) +
wi gi (x)] x X, A.

(16)

Finally, rewrite LP use constraints form one Equation (12):
Variables: w1 , . . . , wk ;
P
Minimize:
wi ;
P
Subject to: 0 maxx {Ra (x) + wi [gia (x) hi (x)]} A.

(17)

use factored LP construction Section 4.2 represent non-linear
constraints compactly. Basically, one set factored LP constraints action
a. Specifically, write non-linear constraint form Equation (12) expressing functions C as: ci (x) = hi (x)gia (x). ci (x) restricted
423

fiGuestrin, Koller, Parr & Venkataraman

scope function; is, hi (x) scope restricted Ci , gia (x) scope restricted
(C0i ), means ci (x) scope restricted Ci (C0i ). Next, target
function b becomes reward function Ra (x) which, assumption, factored. Finally,
constraint Equation (12), free variable. hand, LP (17)
maximum right hand side must less zero. final condition
achieved adding constraint = 0. Thus, algorithm generates set factored
LP constraints, one action. total number constraints variables
new LP linear number actions |A| exponential induced width
cost network, rather total number variables. complete factored
approximate linear programming algorithm outlined Figure 6.
5.2 Example
present complete example operations required approximate LP algorithm solve factored MDP shown Figure 2(a). presentation follows four steps:
problem representation, basis function selection, backprojections LP construction.
Problem Representation: First, must fully specify factored MDP model
problem. structure DBN shown Figure 2(b). structure maintained
action choices. Next, must define transition probabilities action.
5 actions problem: nothing, reboot one 4 machines
network. CPDs actions shown Figure 2(c). Finally, must define
reward function. decompose global reward sum 4 local reward functions,
one machine, reward machine working. Specifically,
Ri (Xi = true) = 1 Ri (Xi = false) = 0, breaking symmetry setting R4 (X4 = true) =
2. use discount factor = 0.9.
simple example, use five simple basis functions.
Basis Function Selection:
First, include constant function h0 = 1. Next, add indicators machine
take value 1 machine working: hi (Xi = true) = 1 hi (Xi = false) = 0.
Backprojections:
first algorithmic step computing backprojection
basis functions, defined Section 4.1. backprojection constant basis
simple:
g0a =

X

Pa (x0 | x)h0 ;

x0

=

X

Pa (x0 | x) 1 ;

x0

= 1.
Next, must backproject indicator basis functions hi :
gia =

X
x0

=

Pa (x0 | x)hi (x0i ) ;

X



Pa (x0j | xj1 , xj )hi (x0i ) ;

x01 ,x02 ,x03 ,x04 j

424

fiEfficient Solution Algorithms Factored MDPs

=

X
x0i

=

X

X

Pa (x0i | xi1 , xi )hi (x0i )



Pa (x0j | xj1 , xj ) ;

x0 [X0 {Xi0 }] j6=i

Pa (x0i | xi1 , xi )hi (x0i ) ;

x0i

= Pa (Xi0 = true | xi1 , xi ) 1 + Pa (Xi0 = false | xi1 , xi ) 0 ;
= Pa (Xi0 = true | xi1 , xi ) .
Thus, gia restricted scope function {Xi1 , Xi }. use CPDs Figure 2(c) specify gia :
reboot =

(Xi1 , Xi ) =

reboot 6=

(Xi1 , Xi ) =

gi

gi

Xi = true Xi = false
Xi1 = true
1
1
;
Xi1 = false
1
1
Xi1 = true
Xi1 = false

Xi = true Xi = false
0.9
0.09
.
0.5
0.05

LP Construction:
illustrate factored LPs constructed algorithms,
define constraints approximate linear programming approach presented above.
First, define functions cai = gia hi , shown Equation (17). example,
functions ca0 = 1 = 0.1 constant basis, indicator bases:
reboot =

(Xi1 , Xi ) =

reboot 6=

(Xi1 , Xi ) =

ci

ci

Xi = true Xi = false
Xi1 = true
0.1
0.9
;
Xi1 = false
0.1
0.9
Xi1 = true
Xi1 = false

Xi = true Xi = false
0.19
0.081
.
0.55
0.045

Using definition cai , approximate linear programming constraints given by:
0 max
x

X

Ri +

X



wj caj , .

(18)

j

present LP construction one 5 actions: reboot = 1. Analogous constructions
made actions.
first set constraints, abstract away difference rewards basis
functions introducing LP variables u equality constraints. begin reward
functions:
R1
1
uR
x1 = 1 , ux1 = 0 ;

R2
2
uR
x2 = 1 , ux2 = 0 ;

R3
3
uR
x3 = 1 , ux3 = 0 ;

R4
4
uR
x4 = 2 , ux4 = 0 .

represent equality constraints caj functions reboot = 1 action. Note
appropriate basis function weight Equation (18) appears constraints:

425

fiGuestrin, Koller, Parr & Venkataraman

uc0 = 0.1 w0 ;
ucx11 ,x4 = 0.9 w1 ,
ucx11 ,x4 = 0.1 w1 ,
ucx11 ,x4 = 0.9 w1 ;
ucx11 ,x4 = 0.1 w1 ,
c
c
c
ux21 ,x2 = 0.19 w2 , ux21 ,x2 = 0.55 w2 , ux21 ,x2 = 0.081 w2 , ucx21 ,x2 = 0.045 w2 ;
ucx32 ,x3 = 0.19 w3 , ucx32 ,x3 = 0.55 w3 , ucx32 ,x3 = 0.081 w3 , ucx32 ,x3 = 0.045 w3 ;
ucx43 ,x4 = 0.19 w4 , ucx43 ,x4 = 0.55 w4 , ucx43 ,x4 = 0.081 w4 , ucx43 ,x4 = 0.045 w4 .
Using new LP variables, LP constraint Equation (18) reboot = 1 action
becomes:
0

max

X1 ,X2 ,X3 ,X4

4
4
X
X
c
c0

uR
+
u
+
uXjj1 ,Xj .
Xi
i=1

j=1

ready variable elimination process. illustrate elimination
variable X4 :
0

max

X1 ,X2 ,X3

3
3
h

X
X
c
Ri
c1
c4
c0
4
uXi + u +
uXjj1 ,Xj + max uR
X4 + uX1 ,X4 + uX3 ,X4 .
i=1

X4

j=2

h



c1
c4
4
represent term maxX4 uR
X4 + uX1 ,X4 + uX3 ,X4 set linear constraints,
one assignment X1 X3 , using new LP variables ueX11 ,X3 represent
maximum:

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c4
c1
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 .

eliminated variable X4 global non-linear constraint becomes:
0

3
3
X
X
c
c0

uR
+
u
+
uXjj1 ,Xj + ueX11 ,X3 .
Xi
X1 ,X2 ,X3

max

j=2

i=1

Next, eliminate variable X3 . new LP constraints variables form:
c3
e1
3
ueX21 ,X2 uR
X3 + uX2 ,X3 + uX1 ,X3 , X1 , X2 , X3 ;

thus, removing X3 global non-linear constraint:
2
X
c2
e2
c0

uR
Xi + u + uX1 ,X2 + uX1 ,X2 .
X1 ,X2

0 max

i=1

426

fiEfficient Solution Algorithms Factored MDPs

250000

Number LP constraints

200000

# explicit constraints =
(n+1) 2 n

Explicit LP
Factored LP
150000

100000

50000

# factored constraints =
12n2 + 5n - 8
0
0

2

4

6
8
10
Number machines ring

12

14

16

Figure 7: Number constraints LP generated explicit state representation
versus factored LP construction solution ring problem
basis functions single variables approximate linear programming
solution algorithm.

eliminate X2 , generating linear constraints:
c2
e2
2
ueX31 uR
X2 + uX1 ,X2 + uX1 ,X2 , X1 , X2 .

Now, global non-linear constraint involves X1 :
e3
c0
1
0 max uR
X1 + u + uX1 .
X1

X1 last variable eliminated, scope new LP variable empty
linear constraints given by:
e3
1
u e4 u R
X1 + uX1 , X1 .

state variables eliminated, turning global non-linear constraint
simple linear constraint:
0 uc0 + ue4 ,
completes LP description approximate linear programming solution
problem Figure 2.
small example four state variables, factored LP technique generates
total 89 equality constraints, 115 inequality constraints 149 LP variables,
explicit state representation Equation (8) generates 80 inequality constraints
5 LP variables. However, problem size increases, number constraints
LP variables factored LP approach grow O(n2 ), explicit state approach
grows exponentially, O(n2n ). scaling effect illustrated Figure 7.

6. Approximate Policy Iteration Max-norm Projection
factored approximate linear programming approach described previous section
elegant easy implement. However, cannot, general, provide strong
427

fiGuestrin, Koller, Parr & Venkataraman

guarantees error achieves. alternative use approximate policy
iteration described Section 3.2, offer certain bounds error. However,
shall see, algorithm significantly complicated, requires place
additional restrictions factored MDP.
particular, approximate policy iteration requires representation policy
iteration. order obtain compact policy representation, must make additional
assumption: action affects small number state variables. first state
assumption formally. Then, show obtain compact representation greedy
policy respect factored value function, assumption. Finally, describe
factored approximate policy iteration algorithm using max-norm projections.
6.1 Default Action Model
Section 2.2, presented factored MDP model, action associated
factored transition model represented DBN factored reward
function. However, different actions often similar transition dynamics, differing effect small set variables. particular, many cases variable
default evolution model, changes action affects directly (Boutilier
et al., 2000).
type structure turns useful compactly representing policies, property important approximate policy iteration algorithm. Thus, section
paper, restrict attention factored MDPs defined using default transition model = hGd , Pd (Koller & Parr, 2000). action a, define Effects[a] X0
variables next state whose local probability model different , i.e.,
variables Xi0 Pa (Xi0 | Parentsa (Xi0 )) 6= Pd (Xi0 | Parentsd (Xi0 )).
Example 6.1 system administrator example, action ai rebooting
one machines, default action nothing. transition model
described corresponds nothing action, also default transition
model. transition model ai different transition model
variable Xi0 , Xi0 = true probability one, regardless status
neighboring machines. Thus, example, Effects[ai ] = Xi0 .
transition dynamics, also define notion default reward model.
P
case, set reward functions ri=1 Ri (Ui ) associated default action
d. addition, action reward function Ra (Ua ). Here, extra reward
action scope restricted Rewards[a] = Uai {X1 , . . . , Xn }. Thus, total reward
P
associated action given Ra + ri=1 Ri . Note Ra also factored
linear combination smaller terms even compact representation.
build additional assumption define complete algorithm.
Recall approximate policy iteration algorithm iterates two steps: policy
improvement approximate value determination. discuss steps.
6.2 Computing Greedy Policies
policy improvement step computes greedy policy relative value function V (t1) :
(t) = Greedy(V (t1) ).
428

fiEfficient Solution Algorithms Factored MDPs

Recall value function estimates linear form Hw. described
Section 4.1, greedy policy type value function given by:
Greedy(Hw)(x) = arg max Qa (x),


P

Qa represented by: Qa (x) = R(x, a) + wi gia (x).
attempt represent policy naively, faced problem
exponentially large state spaces. Fortunately, shown Koller Parr (2000),
greedy policy relative factored value function form decision list.
precisely, policy written form ht1 , a1 i, ht2 , a2 i, . . . , htL , aL i, ti
assignment values small subset Ti variables, ai action.
greedy action take state x action aj corresponding first event tj
list x consistent. completeness, review construction
decision-list policy.
critical assumption allows us represent policy compact decision list
default action assumption described Section 6.1. assumption, Qa
functions written as:


Qa (x) = R (x) +

r
X

Ri (x) +

i=1

X

wi gia (x),



Ra scope restricted Ua . Q function default action just:
P
P
Qd (x) = ri=1 Ri (x) + wi gid (x).
set linear Q-functions implicitly describes policy .
immediately obvious Q functions result compactly expressible policy.
important insight components weighted combination
identical, gia equal gid i. Intuitively, component gia corresponding
backprojection basis function hi (Ci ) different action influences
one variables Ci . formally, assume Effects[a] Ci = . case,
variables Ci transition model . Thus,
gia (x) = gid (x); words, ith component Qa function irrelevant
deciding whether action better default action d. define
components actually relevant: let Ia set indices Effects[a] Ci 6= .
indices basis functions whose backprojection differs Pa Pd .
example DBN Figure 2, actions basis functions involve single variables,
Iai = i.
Let us consider impact taking action default action d.
define impact difference value as:
(x) = Qa (x) Qd (x);
= Ra (x) +

X

h



wi gia (x) gid (x) .

(19)

iIa

analysis shows (x) function whose scope restricted




Ta = Ua iIa (C0i ) .
429

(20)

fiGuestrin, Koller, Parr & Venkataraman

DecisionListPolicy (Qa )
//Qa set Q-functions, one action;
//Return decision list policy .
//Initialize decision list.

Let = {}.
//Compute bonus functions.

action a, default action d:
Compute bonus taking action a,
(x) = Qa (x) Qd (x);
Equation (19). Note scope restricted Ta ,
Equation (20).
//Add states positive bonuses (unsorted) decision list.

assignment Ta :
(t) > 0, add branch decision list:
= {ht, a, (t)i}.
//Add default action (unsorted) decision list.

Let = {h, d, 0i}.
//Sort decision list obtain final policy.

Sort decision list decreasing order element ht, a, i.
Return .

Figure 8: Method computing decision list policy factored representation
Qa functions.

example DBN, Ta2 = {X1 , X2 }.
Intuitively, situation baseline value function Qd (x)
defines value state x. action changes baseline adding
subtracting amount state. point amount depends Ta ,
states variables Ta take values.
define greedy policy relative Q functions. action a, define
set conditionals ht, a, i, assignment values variables Ta ,
(t). Now, sort conditionals actions order decreasing :
ht1 , a1 , 1 i, ht2 , a2 , 2 i, . . . , htL , aL , L i.
Consider optimal action state x. would like get largest possible bonus
default value. x consistent t1 , clearly take action a1 ,
gives us bonus 1 . not, try get 2 ; thus, check x
consistent t2 , so, take a2 . Using procedure, compute decisionlist policy associated linear estimate value function. complete algorithm
computing decision list policy summarized Figure 8.
P
Note number conditionals list |Dom(Ta )|; Ta , turn, depends
set basis function clusters intersect effects a. Thus, size
policy depends natural way interaction structure
430

fiEfficient Solution Algorithms Factored MDPs

process description structure basis functions. problems actions
modify large number variables, policy representation could become unwieldy.
approximate linear programming approach Section 5 appropriate cases,
require explicit representation policy.
6.3 Value Determination
approximate value determination step algorithm computes:
w(t) = arg min kHw (R(t) + P(t) Hw)k .
w

rearranging expression, get:
w(t) = arg min k(H P(t) H) w R(t) k .
w

equation instance optimization Equation (4). P(t) factored,
conclude C = (H P(t) H) also matrix whose columns correspond restrictedscope functions. specifically:
(t)

ci (x) = hi (x) gi (x),
(t)

gi backprojection basis function hi transition model P(t) ,
described Section 4.1. target b = R(t) corresponds reward function,
moment assumed factored. Thus, apply factored LP
Section 4.2.3 estimate value policy (t) .
Unfortunately, transition model P(t) factored, decision list representation policy (t) will, general, induce transition model P(t) cannot
represented compact DBN. Nonetheless, still generate compact LP exploiting decision list structure policy. basic idea introduce cost networks
corresponding branch decision list, ensuring, additionally, states
consistent branch considered cost network maximization. Specifically,
factored LP construction branch hti , ai i. ith cost network
considers subset states consistent ith branch decision list.
Let Si set states x ti first event decision list x
consistent. is, state x Si , x consistent ti , consistent
tj j < i.
Recall that, Equation (11), LP construction defines set constraints
P
imply wi ci (x) b(x) state x. Instead, separate set
constraints states subset Si . state Si , know action ai
taken. Hence, apply construction using Pai transition model
factored assumption place non-factored P(t) . Similarly, reward function
P
becomes Rai (x) + ri=1 Ri (x) subset states.
issue guarantee cost network constraints derived transition model applied states Si . Specifically, must guarantee
applied states consistent ti , states consistent
tj j < i. guarantee first condition, simply instantiate variables Ti
take values specified ti . is, cost network considers variables
431

fiGuestrin, Koller, Parr & Venkataraman

FactoredAPI (P , R, , H, O, , tmax )
//P factored transition model.
//R set factored reward functions.
// discount factor.
//H set basis functions H = {h1 , . . . , hk }.
//O stores elimination order.
// Bellman error precision.
//tmax maximum number iterations.
//Return basis function weights w computed approximate policy iteration.
//Initialize weights

Let w(0) = 0.
//Cache backprojections basis functions.

basis function hi H; action a:
Let gia = Backproja (hi ).
//Main approximate policy iteration loop.

Let = 0.
Repeat
//Policy improvement part loop.
//Compute decision list policy iteration weights.

Let (t) = DecisionListPolicy(Ra +

P



(t)

wi gia ).

//Value determination part loop.
//Initialize constraints max-norm projection LP.

Let + = {} = {}.
//Initialize indicators.

Let = {}.
//For every branch decision list policy, generate relevant set constraints,
update indicators constraint state space future branches.

branch htj , aj decision list policy (t) :
//Instantiate variables Tj assignment given tj .




Instantiate set functions {h1 g1 j , . . . , hk gk j }
partial state assignment tj store C.
Instantiate target functions Raj partial state assignment tj store b.
Instantiate indicator functions partial state assignment tj store 0 .
//Generate factored LP constraints current decision list branch.

Let + = + FactoredLP(C, b + 0 , O).
Let = FactoredLP(C, b + 0 , O).
//Update indicator functions.

Let Ij (x) = 1(x = tj ) update indicators = Ij .
//We obtain new set weights solving LP, corresponds
max-norm projection.

Let w(t+1) solution linear program: minimize , subject
constraints {+ , }.
Let = + 1.
BellmanErr(Hw(t) ) tmax w(t1) = w(t) .
Return w(t) .

Figure 9: Factored approximate policy iteration max-norm projection algorithm.

432

fiEfficient Solution Algorithms Factored MDPs

{X1 , . . . , Xn }Ti , computes maximum states consistent Ti = ti .
guarantee second condition, ensure impose constraints
states associated previous decisions. achieved adding indicators Ij
previous decision tj , weight . specifically, Ij function takes value
states consistent tj zero assignments Tj . constraints
ith branch form:
R(x, ai ) +

X

wl (gl (x, ai ) h(x)) +

X

1(x = tj ),

x [ti ],

(21)

j<i

l

x [ti ] defines assignments X consistent ti . introduction
indicators causes constraints associated ti trivially satisfied states Sj
j < i. Note indicators restricted-scope function Tj
handled fashion terms factored LP. Thus, decision
list size L, factored LP contains constraints 2L cost networks. complete
approximate policy iteration max-norm projection algorithm outlined Figure 9.
6.4 Comparisons
instructive compare max-norm policy iteration algorithm L2 -projection
policy iteration algorithm Koller Parr (2000) terms computational costs per
iteration implementation complexity. Computing L2 projection requires (among
things) series dot product operations basis functions backprojected
basis functions hhi gj i. expressions easy compute P refers transition
model particular action a. However, policy represented decision list,
result policy improvement step, step becomes much complicated.
particular, every branch decision list, every pair basis functions j,
assignment variables Scope[hi ] Scope[gja ], requires solution
counting problem ]P -complete general. Although Koller Parr show
computation performed using Bayesian network (BN) inference, algorithm
still requires BN inference one assignments branch decision
list. makes algorithm difficult implement efficiently practice.
max-norm projection, hand, relies solving linear program every
iteration. size linear program depends cost networks generated.
discuss, two cost networks needed point decision list. complexity
cost networks approximately one BN inferences
counting problem L2 projection. Overall, branch decision
list, total two inferences, opposed one assignment
Scope[hi ] Scope[gja ] every pair basis functions j. Thus, max-norm policy
iteration algorithm substantially less complex computationally approach based
L2 -projection. Furthermore, use linear programming allows us rely existing
LP packages (such CPLEX), highly optimized.
also interesting compare approximate policy iteration algorithm approximate linear programming algorithm presented Section 5. approximate
linear programming algorithm, never need compute decision list policy.
policy always represented implicitly Qa functions. Thus, algorithm
433

fiGuestrin, Koller, Parr & Venkataraman

require explicit computation manipulation greedy policy. difference two
important consequences: one computational terms generality.
First, compute consider decision lists makes approximate linear
programming faster easier implement. algorithm, generate single LP
one cost network action never need compute decision list policy.
hand, iteration, approximate policy iteration needs generate two LPs
every branch decision list size L, usually significantly longer |A|,
total 2L cost networks. terms representation, require policies
compact; thus, need make default action assumption. Therefore,
approximate linear programming algorithm deal general class problems,
action independent DBN transition model. hand,
described Section 3.2, approximate policy iteration stronger guarantees terms
error bounds. differences highlighted experimental results
presented Section 9.

7. Computing Bounds Policy Quality
presented two algorithms computing approximate solutions factored MDPs.
b w
b
algorithms generate linear value functions denoted Hw,
resulting basis function weights. practice, agent define behavior
b One issue remains
b = Greedy(Hw).
acting according greedy policy

b compares true optimal policy ; is, actual value Vb
policy
policy
b compares V .

Section 3, showed priori bounds quality policy. Another
possible procedure compute posteriori bound. is, given resulting weights
b compute bound loss acting according greedy policy
b rather
w,
optimal policy. achieved using Bellman error analysis Williams
Baird (1993).
Bellman error defined BellmanErr(V) = kT V Vk . Given greedy
b = Greedy(V), analysis provides bound:
policy


V V 2BellmanErr(V) .
b

1

(22)

b evaluate quality resulting
Thus, use Bellman error BellmanErr(Hw)
greedy policy.
Note computing Bellman error involves maximization state space.
Thus, complexity computation grows exponentially number state
variables. Koller Parr (2000) suggested structure factored MDP
exploited compute Bellman error efficiently. Here, show error bound
computed set cost networks using similar construction one maxb represented
norm projection algorithms. technique used
decision list depend algorithm used determine policy. Thus,
apply technique solutions determined approximate linear programming
action descriptions permit decision list representation policy.
b Bellman error given by:
set weights w,
434

fiEfficient Solution Algorithms Factored MDPs

b
FactoredBellmanErr (P , R, , H, O, w)
//P factored transition model.
//R set factored reward functions.
// discount factor.
//H set basis functions H = {h1 , . . . , hk }.
//O stores elimination order.
//w
b weights linear value function.
//Return Bellman error value function Hw.
b
//Cache backprojections basis functions.

basis function hi H; action a:
Let gia = Backproja (hi ).
//Compute decision list policy value function
Hw.
b

b = DecisionListPolicy(Ra + P w
Let
bi gi ).
//Initialize indicators.

Let = {}.
//Initialize Bellman error.

Let = 0.
//For every branch decision list policy, generate relevant cost networks, solve
variable elimination, update indicators constraint state space future branches.

b
branch htj , aj decision list policy :

//Instantiate variables Tj assignment given tj .




Instantiate set functions {w
b1 (h1 g1 j ), . . . , w
bk (hk gk j )}
partial state assignment tj store C.
Instantiate target functions Raj partial state assignment
tj store b.
Instantiate indicator functions partial state assignment
tj store 0 .
//Use variable elimination solve first cost network, update Bellman error, error
branch larger.

Let = max (, VariableElimination(C b + 0 , O)).
//Use variable elimination solve second cost network, update Bellman error, error
branch larger.

Let = max (, VariableElimination(C + b + 0 , O)).
//Update indicator functions.

Let Ij (x) = 1(x = tj ) update indicators = Ij .
Return .
b
Figure 10: Algorithm computing Bellman error factored value function Hw.

435

fiGuestrin, Koller, Parr & Venkataraman

b
b Hwk
b ;
BellmanErr(Hw)
= kT Hw


= max

P

P

P

maxx wi hi (x) Rb (x) x0 Pb (x0 | x) j wj hj (x0 ) ,
P
P
P
maxx Rb (x) + x0 Pb (x0 | x) j wj hj (x0 ) wi hi (x)

!

rewards Rb transition model Pb factored appropriately,
compute one two maximizations (maxx ) using variable elimination cost
b decision list policy
network described Section 4.2.1. However,
induce factored transition model. Fortunately, approximate policy iteration
algorithm Section 6, exploit structure decision list perform
maximization efficiently. particular, approximate policy iteration, generate
two cost networks branch decision list. guarantee maximization
performed states branch relevant, include type
indicator functions, force irrelevant states value , thus guaranteeing point decision list policy obtain corresponding state
maximum error. state overall largest Bellman error maximum
ones generated point decision list policy. complete factored
algorithm computing Bellman error outlined Figure 10.
One last interesting note concerns approximate policy iteration algorithm maxnorm projection Section 6. experiments, algorithm converged,
w(t) = w(t+1) iterations. convergence occurs, objective function
(t+1) linear program last iteration equal Bellman error final
policy:
Lemma 7.1 approximate policy iteration max-norm projection converges,
w(t) = w(t+1) iteration t, max-norm projection error (t+1) last
b = Hw(t) :
iteration equal Bellman error final value function estimate Hw
b = (t+1) .
BellmanErr(Hw)

Proof: See Appendix A.4.
Thus, bound loss acting according final policy (t+1) substituting
(t+1)

Bellman error bound:
Corollary 7.2 approximate policy iteration max-norm projection converges
b associated greedy policy
b =
iterations final value function estimate Hw
b
b instead optimal policy
Greedy(Hw),
loss acting according
bounded by:
(t+1)


V V 2
,
b

1

b.
Vb actual value policy

Therefore, approximate policy iteration converges obtain bound
quality resulting policy without needing compute Bellman error explicitly.
436

.

fiEfficient Solution Algorithms Factored MDPs

8. Exploiting Context-specific Structure
Thus far, presented suite algorithms exploit additive structure
reward basis functions sparse connectivity DBN representing transition
model. However, exists another important type structure also
exploited efficient decision making: context-specific independence (CSI). example,
consider agent responsible building maintaining house, painting task
completed plumbing electrical wiring installed,
probability painting done 0, contexts plumbing electricity
done, independently agents action. representation used far
paper would use table represent type function. table exponentially
large number variables scope function, ignores context-specific
structure inherent problem definition.
Boutilier et al. (Boutilier et al., 1995; Dearden & Boutilier, 1997; Boutilier, Dean, &
Hanks, 1999; Boutilier et al., 2000) developed set algorithms exploit CSI
transition reward models perform efficient (approximate) planning. Although
approach often successful problems value function contains sufficient
context-specific structure, approach able exploit additive structure
also often present real-world problems.
section, extend factored MDP model include context-specific structure.
present simple, yet effective extension algorithms exploit CSI
additive structure obtain efficient approximations factored MDPs. first extend
factored MDP representation include context-specific structure show
basic operations Section 4 required algorithms performed efficiently
new representation.
8.1 Factored MDPs Context-specific Additive Structure
several representations context-specific functions. common
decision trees (Boutilier et al., 1995), algebraic decision diagrams (ADDs) (Hoey, St-Aubin,
Hu, & Boutilier, 1999), rules (Zhang & Poole, 1999). choose use rules
basic representation, two main reasons. First, rule-based representation allows
fairly simple algorithm variable elimination, key operation framework.
Second, rules required mutually exclusive exhaustive, requirement
restrictive want exploit additive independence, functions
represented linear combination set non-mutually exclusive functions.
begin describing rule-based representation (along lines Zhang
Pooles presentation (1999)) probabilistic transition model, particular, CPDs
DBN model. Roughly speaking, rule corresponds set CPD entries
associated particular probability value. entries
value referred consistent contexts:
Definition 8.1 Let C {X, X0 } c Dom(C). say c consistent
b Dom(B), B {X, X0 }, c b assignment variables
C B.
probability consistent contexts represented probability rules:
437

fiGuestrin, Koller, Parr & Venkataraman

Electrical

Electrical

Done

done

Done

done

Plumbing

P(Painting) = 0

Plumbing

P(Painting) = 0
done

Done

done

Painting

P(Painting) = 0

Done

done

P(Painting) = 0

P(Painting) = 0.95

P(Painting) = 0

(a)

Done
P(Painting) = 0.9

(b)

4 = hElectrical : 0i
5 = hElectrical Plumbing : 0i
6 = hElectrical Plumbing Painting : 0i
7 = hElectrical Plumbing Painting : 0.9i
(d)

1 = hElectrical : 0i
2 = hElectrical Plumbing : 0i
3 = hElectrical Plumbing : 0.95i
(c)

Figure 11: Example CPDs variable Painting = true represented decision trees:
(a) action paint; (b) action paint. CPDs
represented probability rules shown (c) (d), respectively.

Definition 8.2 probability rule = hc : pi function : {X, X0 } 7 [0, 1],
context c Dom(C) C {X, X0 } p [0, 1], (x, x0 ) = p (x, x0 )
consistent c equal 1 otherwise.
case, convenient require rules mutually exclusive exhaustive, CPD entry uniquely defined association single rule.
Definition 8.3 rule-based conditional probability distribution (rule CPD) Pa function Pa : ({Xi0 } X) 7 [0, 1], composed set probability rules {1 , 2 , . . . , } whose
contexts mutually exclusive exhaustive. define:
Pa (x0i | x) = j (x, x0 ),
j unique rule Pa cj consistent (x0i , x). require that,
x,
X
Pa (x0i | x) = 1.
x0i

define Parentsa (Xi0 ) union contexts rules Pa (Xi0 | X).
example CPD represented set probability rules shown Figure 11.
Rules also used represent additive functions, reward basis functions.
represent context specific value dependencies using value rules:
438

fiEfficient Solution Algorithms Factored MDPs

Definition 8.4 value rule = hc : vi function : X 7 R (x) = v
x consistent c 0 otherwise.
Note value rule hc : vi scope C.
important note value rules required mutually exclusive
exhaustive. value rule represents (weighted) indicator function, takes
value v states consistent context c, 0 states. given state,
values zero rules consistent state simply added together.
Example 8.5 construction example, might set rules:
1 = hPlumbing = done : 100i;
2 = hElectricity = done : 100i;
3 = hPainting = done : 100i;
4 = hAction = plumb : 10i;
..
.
which, summed together, define reward function R = 1 + 2 + 3 + 4 + .
general, reward function Ra represented rule-based function:
Definition 8.6 rule-based function f : X 7 R composed set rules {1 , . . . , n }
P
f (x) = ni=1 (x).
manner, one basis functions hj represented rule-based
function.
notion rule-based function related tree-structure functions used
Boutilier et al. (2000), substantially general. tree-structure value functions, rules corresponding different leaves mutually exclusive exhaustive.
Thus, total number different values represented tree equal number
leaves (or rules). rule-based function representation, rules mutually
exclusive, values added form overall function value different settings
variables. Different rules added different settings, and, fact, k rules,
one easily generate 2k different possible values, demonstrated Section 9. Thus,
rule-based functions provide compact representation much richer class
value functions.
Using rule-based representation, exploit CSI additive independence
representation factored MDP basis functions. show basic
operations Section 4 adapted exploit rule-based representation.
8.2 Adding, Multiplying Maximizing Consistent Rules
table-based algorithms, relied standard sum product operators applied
tables. order exploit CSI using rule-based representation, must redefine
standard operations. particular, algorithms need add multiply rules
ascribe values overlapping sets states.
start defining operations rules context:
439

fiGuestrin, Koller, Parr & Venkataraman

Definition 8.7 Let 1 = hc : v1 2 = hc : v2 two rules context c. Define
rule product 1 2 = hc : v1 v2 i, rule sum 1 + 2 = hc : v1 + v2 i.
Note definition restricted rules context. address
issue moment. First, introduce additional operation maximizes
variable set rules, otherwise share common context:
Definition 8.8 Let variable Dom[Y ] = {y1 , . . . , yk }, let , =
1, . . . , k, rule form = hc = yi : vi i. rule-based function
f = 1 + + k , define rule maximization maxY f = hc : maxi vi .
operation, maximized scope function f .
three operations described applied sets rules
satisfy stringent conditions. make set rules amenable application
operations, might need refine rules. therefore define
following operation:
Definition 8.9 Let = hc : vi rule, variable. Define rule split
Split(6 ) variable follows: Scope[C], Split(6 ) = {};
otherwise,
Split(6 ) = {hc = yi : vi | yi Dom[Y ]} .
Thus, split rule variable scope context ,
generate new set rules, one assignment domain .
general, purpose rule splitting extend context c one rule coincide
context c0 another consistent rule 0 . Naively, might take variables
Scope[C0 ] Scope[C] split recursively one them. However, process
creates unnecessarily many rules: variable Scope[C0 ] Scope[C] split
, one |Dom[Y ]| new rules generated remain consistent 0 :
one assignment one c0 . Thus, consistent rule
needs split further. define recursive splitting procedure achieves
parsimonious representation:
Definition 8.10 Let = hc : vi rule, b context b Dom[B].
Define recursive rule split Split(6 b) context b follows:
1. {}, c consistent b; else,
2. {}, Scope[B] Scope[C]; else,
3. {Split(i 6 b) | Split(6 )}, variable Scope[B] Scope[C] .

definition, variable Scope[B] Scope[C] leads generation k =
|Dom(Y )| rules step split. However, one k rules used
next recursive step one consistent b. Therefore, size
P
split set simply 1 + Scope[B]Scope[C] (|Dom(Y )| 1). size independent
order variables split within operation.
440

fiEfficient Solution Algorithms Factored MDPs

Note one rules Split(6 b) consistent b: one context
c b. Thus, want add two consistent rules 1 = hc1 : v1 2 = hc2 : v2 i,
need replace rules set:
Split(1 6 c2 ) Split(2 6 c1 ),
simply replace resulting rules hc1 c2 : v1 hc2 c1 : v2 sum
hc1 c2 : v1 + v2 i. Multiplication performed analogous manner.
Example 8.11 Consider adding following set consistent rules:
1 = ha b : 5i,
2 = ha c : 3i.
rules, context c1 1 b, context c2 2 c d.
Rules 1 2 consistent, therefore, must split perform addition
operation:


ha b c : 5i,
ha b c : 5i,
Split(1 6 c2 ) =

ha b c : 5i.
Likewise,

(

Split(2 6

c1 ) =

ha b c : 3i,
ha b c : 3i.

result adding rules 1 2
ha b c : 5i,
ha b c : 5i,
ha b c : 8i,
ha b c : 3i.

8.3 Rule-based One-step Lookahead
Using compact rule-based representation, able compute one-step lookahead
plan efficiently models significant context-specific additive independence.
Section 4.1 table-based case, rule-based Qa function represented
sum reward function discounted expected value next state.
Due linear approximation value function, expectation term is, turn,
represented linear combination backprojections basis functions.
exploit CSI, representing rewards basis functions rule-based functions.
represent Qa rule-based function, sufficient us show represent
backprojection gj basis function hj rule-based function.
P (h )
hj rule-based
function,
written hj (x) = j (x),

E
(h )
(h )
(h )
j form ci j : vi j . rule restricted scope function; thus,
simplify backprojection as:
441

fiGuestrin, Koller, Parr & Venkataraman

RuleBackproja () ,

given hc : vi, c Dom[C].

Let g = {}.
Select set P relevant probability rules:
P = {j P (Xi0 | Parents(Xi0 )) | Xi0 C c consistent cj }.
Remove X0 assignments context rules P.
// Multiply consistent rules:
two consistent rules 1 = hc1 : p1 2 = hc2 : p2 i:
c1 = c2 , replace two rules hc1 : p1 p2 i;
Else replace two rules set: Split(1 6 c2 ) Split(2 6 c1 ).
// Generate value rules:
rule P:
Update backprojection g = g {hci : pi vi}.
Return g.

Figure 12: Rule-based backprojection.
gja (x) =

X

Pa (x0 | x)hj (x0 ) ;

x0

=

X

Pa (x0 | x)

x0

=

XX


=

X (hj )



(x0 );


(hj )

0

Pa (x | x)i

(x0 );

x0

X (hj )

vi

(hj )

Pa (ci

| x);


(h )

(h )

term vi j Pa (ci j | x) written rule function. denote back(h )
projection operation RuleBackproja (i j ).
backprojection procedure, described Figure 12, follows three steps. First,
relevant rules selected: CPDs variables appear context ,
select rules consistent context, rules play role
backprojection computation. Second, multiply consistent probability rules
form local set mutually-exclusive rules. procedure analogous addition
procedure described Section 8.2. represented probabilities
affect mutually-exclusive set, simply represent backprojection
product probabilities value . is, backprojection
rule-based function one rule one mutually-exclusive probability rules
. context new value rule , value product
probability value .
Example 8.12 example, consider backprojection simple rule,
= h Painting = done : 100i,
CPD Figure 11(c) paint action:
RuleBackprojpaint () =

X

Ppaint (x0 | x)(x0 );

x0

442

fiEfficient Solution Algorithms Factored MDPs

X

=

Ppaint (Painting0 | x)(Painting0 );

Painting0

= 100

3


(Painting = done, x) .

i=1

Note product simple rules equivalent decision tree CPD shown
Figure 11(a). Hence, product equal 0 contexts, example, electricity
done time t. product non-zero one context: context associated
rule 3 . Thus, express result backprojection operation rule-based
function single rule:
RuleBackprojpaint () = hPlumbing Electrical : 95i.
Similarly, backprojection action paint also represented
single rule:
RuleBackprojpaint () = hPlumbing Electrical Painting : 90i.
Using algorithm, write backprojection rule-based basis function hj as:
gja (x) =

X

(hj )

RuleBackproja (i

),

(23)



gja sum rule-based functions, therefore also rule-based function.
simplicity notation, use gja = RuleBackproja (hj ) refer definition backproP
jection. Using notation, write Qa (x) = Ra (x) + j wj gja (x),
rule-based function.
8.4 Rule-based Maximization State Space
second key operation required extend planning algorithms exploit CSI
modify variable elimination algorithm Section 4.2.1 handle rule-based representation. Section 4.2.1, showed maximization linear combination
table-based functions restricted scope performed efficiently using non-serial
dynamic programming (Bertele & Brioschi, 1972), variable elimination. exploit structure rules, use algorithm similar variable elimination Bayesian network
context-specific independence (Zhang & Poole, 1999).
Intuitively, algorithm operates selecting value rules relevant variable
maximized current iteration. Then, local maximization performed
subset rules, generating new set rules without current variable.
procedure repeated recursively variables eliminated.
precisely, algorithm eliminates variables one one, elimination process performs maximization step variables domain. Suppose
eliminating Xi , whose collected value rules lead rule function f , f involves
additional variables set B, f scope B {Xi }. need compute
maximum value Xi choice b Dom[B]. use MaxOut (f, Xi ) denote procedure takes rule function f (B, Xi ) returns rule function g(B)
443

fiGuestrin, Koller, Parr & Venkataraman

MaxOut (f, B)
Let g = {}.
Add completing rules f : hB = bi : 0i, = 1, . . . , k.
// Summing consistent rules:
two consistent rules 1 = hc1 : v1 2 = hc2 : v2 i:
c1 = c2 , replace two rules hc1 : v1 + v2 i;
Else replace two rules set: Split(1 6 c2 ) Split(2 6 c1 ).
// Maximizing variable B:
Repeat f empty:
rules hc B = bi : vi i, bi Dom(B) :
remove rules f add rule hc : maxi vi g;
Else select two rules: = hci B = bi : vi j = hcj B = bj : vj
ci consistent cj , identical, replace
Split(i 6 cj ) Split(j 6 ci ) .
Return g.

Figure 13: Maximizing variable B rule function f .
that: g(b) = maxxi f (b, xi ). procedure extension variable elimination
algorithm Zhang Poole (Zhang & Poole, 1999).
rule-based variable elimination algorithm maintains set F value rules, initially
containing set rules maximized. algorithm repeats following steps
variable Xi variables eliminated:
1. Collect rules depend Xi fi fi = {hc : vi F | Xi C}
remove rules F.
2. Perform local maximization step Xi : gi = MaxOut (fi , Xi );
3. Add rules gi F; now, Xi eliminated.
cost algorithm polynomial number new rules generated
maximization operation MaxOut (fi , Xi ). number rules never larger many
cases exponentially smaller complexity bounds table-based maximization
Section 4.2.1, which, turn, exponential induced width cost network
graph (Dechter, 1999). However, computational costs involved managing sets rules
usually imply computational advantage rule-based approach tablebased one significant problems possess fair amount context-specific
structure.
remainder section, present algorithm computing local
maximization MaxOut (fi , Xi ). next section, show ideas applied
extending algorithm Section 4.2.2 exploit CSI LP representation
planning factored MDPs.
procedure, presented Figure 13, divided two parts: first, consistent
rules added together described Section 8.2; then, variable B maximized.
maximization performed generating set rules, one assignment B, whose
contexts assignment variables except B, Definition 8.8.
set substituted single rule without B assignment context value
equal maximum values rules original set. Note that, simplify
444

fiEfficient Solution Algorithms Factored MDPs

algorithm, initially need add set value rules 0 value, guarantee
rule function f complete (i.e., least one rule consistent every
context).
correctness procedure follows directly correctness rule-based
variable elimination procedure described Zhang Poole, merely replacing summations product max, products products sums. conclude
section small example illustrate algorithm:
Example 8.13 Suppose maximizing following set rules:
1
2
3
4

= ha : 1i,
= ha b : 2i,
= ha b c : 3i,
= ha b : 1i.

add completing rules, get:
5 = ha : 0i,
6 = ha : 0i.
first part algorithm, need add consistent rules: add 5 1 (which
remains unchanged), combine 1 4 , 6 2 , split 6 context
3 , get following inconsistent set rules:
2
3
7
8
9

= ha b : 2i,
= ha b c : 3i,
= ha b : 2i,
(from adding 4 consistent rule Split(1 6 b))
= ha b : 1i,
(from Split(1 6 b))
= ha b c : 0i,
(from Split(6 6 b c)).

Note several rules value 0 also generated, shown
added rules consistent contexts. move second stage (repeat loop)
MaxOut. remove 2 , 8 , maximize them, give:
10 = hb : 2i.
select rules 3 7 split 7 c (3 split empty set
changed),
11 = ha b c : 2i,
12 = ha b c : 2i.
Maximizing rules 12 3 , get:
13 = hb c : 3i.
left 11 , maximized counterpart 9 gives
12 = hb c : 2i.
Notice that, throughout maximization, split variable C b ci ,
giving us 6 distinct rules final result. possible table-based
representation, since functions would 3 variables a,b,c, therefore
must 8 entries.
445

fiGuestrin, Koller, Parr & Venkataraman

8.5 Rule-based Factored LP
Section 4.2.2, showed LPs used algorithms exponentially many
P
constraints form: wi ci (x) b(x), x, substituted single,
P
equivalent, non-linear constraint: maxx wi ci (x) b(x). showed that, using
variable elimination, represent non-linear constraint equivalent set
linear constraints construction called factored LP. number constraints
factored LP linear size largest table generated variable elimination
procedure. table-based algorithm exploit additive independence.
extend algorithm Section 4.2.2 exploit additive context-specific structure,
using rule-based variable elimination described previous section.
Suppose wish enforce general constraint 0 maxy F w (y), F w (y) =
P w
j fj (y) fj rule. table-based version, superscript w means
fj might depend w. Specifically, fj comes basis function hi , multiplied
weight wi ; fj rule reward function, not.
rule-based factored linear program, generate LP variables associated
contexts; call LP rules. LP rule form hc : ui; associated
context c variable u linear program. begin transforming original
rules fjw LP rules follows: rule fj form hcj : vj comes basis
function hi , introduce LP rule ej = hcj : uj equality constraint uj = wi vj .
fj form comes reward function, introduce LP rule
form, equality constraint becomes uj = vj .
P
Now, LP rules need represent constraint: 0 maxy j ej (y).
represent constraint, follow algorithm similar variable elimination procedure Section 8.4. main difference occurs MaxOut (f, B) operation
Figure 13. Instead generating new value rules, generate new LP rules, associated
new variables new constraints. simplest case occurs computing split
adding two LP rules. example, add two value rules original algorithm,
instead perform following operation associated LP rules: LP rules
hc : ui hc : uj i, replace new rule hc : uk i, associated new LP
variable uk context c, whose value ui + uj . enforce value constraint,
simply add additional constraint LP: uk = ui + uj . similar procedure
followed computing split.
interesting constraints generated perform maximization.
rule-based variable elimination algorithm Figure 13, maximization occurs
replace set rules:
hc B = bi : vi i, bi Dom(B),
new rule





c : max vi .


Following process LP rule summation above, maximizing
ei = hc B = bi : ui i, bi Dom(B),
generate new LP variable uk associated rule ek = hc : uk i. However,
cannot add nonlinear constraint uk = maxi ui , add set equivalent linear
446

fiEfficient Solution Algorithms Factored MDPs

constraints
uk ui , i.
Therefore, using simple operations, exploit structure rule functions
P
represent nonlinear constraint en maxy j ej (y), en last LP
rule generate. final constraint un = implies representing exactly
constraints Equation (12), without enumerate every state.
correctness rule-based factored LP construction corollary Theorem 4.4
correctness rule-based variable elimination algorithm (Zhang & Poole,
1999) .
Corollary 8.14 constraints generated rule-based factored LP construction
equivalent non-linear constraint Equation (12). is, assignment (, w)
satisfies rule-based factored LP constraints satisfies constraint
Equation (12).
number variables constraints rule-based factored LP linear
number rules generated variable elimination process. turn, number rules
larger, often exponentially smaller, number entries table-based
approach.
illustrate generation LP constraints described, present small
example:
Example 8.15 Let e1 , e2 , e3 , e4 set LP rules depend variable
b maximized. Here, rule ei associated LP variable ui :
e1
e2
e3
e4

= ha b : u1 i,
= ha b c : u2 i,
= ha b : u3 i,
= ha b c : u4 i.

set, note rules e1 e2 consistent. combine generate
following rules:
e5 = ha b c : u5 i,
e6 = ha b c : u1 i.
constraint u1 + u2 = u5 . Similarly, e6 e4 may combined, resulting in:
e7 = ha b c : u6 i.
constraint u6 = u1 + u4 . Now, following three inconsistent rules
maximization:
e3 = ha b : u3 i,
e5 = ha b c : u5 i,
e7 = ha b c : u6 i.
Following maximization procedure, since pair rules eliminated right away,
split e3 e5 generate following rules:
e8 = ha b c : u3 i,
e9 = ha b c : u3 i,
e5 = ha b c : u5 i.
447

fiGuestrin, Koller, Parr & Venkataraman

maximize b e8 e5 , resulting following rule constraints
respectively:
e10 = ha c : u7 i,
u7 u5 ,
u7 u3 .
Likewise, maximizing b e9 e6 , get:
e11 = ha c : u8 i,
u8 u3 ,
u8 u6 ;
completes elimination variable b rule-based factored LP.
presented algorithm exploiting additive context-specific structure LP construction steps planning algorithms. rule-based factored LP
approach applied directly approximate linear programming approximate policy iteration algorithms, presented Sections 5 6.
additional modification required concerns manipulation decision
list policies presented Section 6.2. Although approximate linear programming
require explicit policy representation (or default action model), approximate policy iteration require us represent policy. Fortunately, major modifications
required rule-based case. particular, conditionals hti , ai , decision
list policies already context-specific rules. Thus, policy representation algorithm
Section 6.2 applied directly new rule-based representation. Therefore,
complete framework exploiting additive context-specific structure
efficient planning factored MDPs.

9. Experimental Results
factored representation value function appropriate certain types
systems: Systems involve many variables, strong interactions
variables fairly sparse, decoupling influence variables
induce unacceptable loss accuracy. argued Herbert Simon (1981)
Architecture Complexity, many complex systems nearly decomposable,
hierarchical structure, subsystems interacting weakly themselves.
evaluate algorithm, selected problems believe exhibit type structure.
section, perform various experiments intended explore performance
algorithms. First, compare factored approximate linear programming (LP)
approximate policy iteration (PI) algorithms. also compare L2 -projection
algorithm Koller Parr (2000). second evaluation compares table-based implementation rule-based implementation exploit CSI. Finally, present
comparisons approach algorithms Boutilier et al. (2000).
9.1 Approximate LP Approximate PI
order compare approximate LP approximate PI algorithms, tested
SysAdmin problem described detail Section 2.1. problem relates system
448

fiEfficient Solution Algorithms Factored MDPs

administrator maintain network computers; experimented various
network architectures, shown Figure 1. Machines fail randomly, faulty machine
increases probability neighboring machines fail. every time step,
SysAdmin go one machine reboot it, causing working next time
step high probability. Recall state space problem grows exponentially
number machines network, is, problem machines 2m states.
machine receives reward 1 working (except ring, one machine
receives reward 2, introduce asymmetry), zero reward given faulty
machines, discount factor = 0.95. optimal strategy rebooting machines
depend upon topology, discount factor, status machines
network. machine machine j faulty, benefit rebooting must
weighed expected discounted impact delaying rebooting j js successors.
topologies rings, policy may function status every single
machine network.
basis functions used included independent indicators machine, value
1 working zero otherwise (i.e., one restricted scope function single
variable), constant basis, whose value 1 states. selected straightforward
variable elimination orders: Star Three Legs topologies, first eliminated
variables corresponding computers legs, center computer (server)
eliminated last; Ring, started arbitrary computer followed ring
order; Ring Star, ring machines eliminated first center one;
finally, Ring Rings topology, eliminated computers outer rings
first ones inner ring.
implemented factored policy iteration linear programming algorithms
Matlab, using CPLEX LP solver. Experiments performed Sun UltraSPARCII, 359 MHz 256MB RAM. evaluate complexity approximate policy
iteration max-norm projection algorithm, tests performed increasing
number states, is, increasing number machines network. Figure 14 shows
running time increasing problem sizes, various architectures. simplest one
Star, backprojection basis function scope restricted two
variables largest factor cost network scope restricted two variables.
difficult one Bidirectional Ring, factors contain five variables.
Note number states growing exponentially (indicated log scale
Figure 14), running times increase logarithmically number states,
polynomially number variables. illustrate behavior Figure 14(d),
fit 3rd order polynomial running times unidirectional ring. Note
size problem description grows quadratically number variables: adding
machine network also adds possible action fixing machine.
problem,
computation
cost factored algorithm empirically grows approximately


(n |A|)1.5 , problem n variables, opposed exponential complexity
poly (2n , |A|) explicit algorithm.
evaluation, measured error approximate value function relative
true optimal value function V . Note possible compute V small
problems; case, able go 10 machines. comparison,
also evaluated error approximate value function produced L2 -projection
449

fiGuestrin, Koller, Parr & Venkataraman

500

400

Ring
3 Legs

300

Ring Rings

300
Total Time (minutes)

Total Time (minutes)

400

Star

200

Ring Star
200

100

100

0

0
1E+00

1E+02

1E+04

1E+06 1E+08 1E+10
number states

1E+12

1

1E+14

100

10000
1000000
number states

(a)
1200

500

1000

Fitting polynomial:

800

time = 0.0184|X| - 0.6655|X| +
9.2499|X| - 31.922

3

Ring:

Total Time (minutes)

Total Time (minutes)

1E+10

(b)

600

400

100000000

Unidirectional
Bidirectional

300
200

2

2

Quality fit: R = 0.999
600

400

100

200
0
1E+00

1E+02

1E+04

1E+06

1E+08

1E+10

1E+12

1E+14

0
0

number state

(c)

10

20
30
40
number variables |X|

50

60

(d)

Figure 14: (a)(c) Running times policy iteration max-norm projection variants
SysAdmin problem; (d) Fitting polynomial running time
Ring topology.

algorithm Koller Parr (2000). discussed Section 6.4, L2 projections
factored MDPs Koller Parr difficult time consuming; hence,
able compare two algorithms smaller problems, equivalent L2 -projection
implemented using explicit state space formulation. Results algorithms
presented Figure 15(a), showing relative error approximate solutions
true value function increasing problem sizes. results indicate that, larger
problems, max-norm formulation generates better approximation true optimal
value function V L2 -projection. Here, used two types basis functions:
single variable functions, pairwise basis functions. pairwise basis functions
contain indicators neighboring pairs machines (i.e., functions two variables).
expected, use pairwise basis functions resulted better approximations.
450

fiEfficient Solution Algorithms Factored MDPs

0.4

Max norm, single basis
L2, single basis

0.3

Bellman Error / Rmax

Max norm, pair basis
L2, pair basis

Relative error:

0.2

0.1

0
3

4

5

6

7

8

9

10

number variables

0.3

0.2

Ring
3 Legs

0.1

0
1E+00

Star

1E+02

1E+04

1E+06

1E+08

1E+10

1E+12

1E+14

numbe r sta tes

(a)

(b)

Figure 15: (a) Relative error optimal value function V comparison L2 projection
Ring; (b) large models, measuring Bellman error convergence.

small problems, also compare actual value policy generated
algorithm value optimal policy. Here, value policy generated
algorithm much closer value optimal policy error implied
difference approximate value function V . example, Star
architecture one server 6 clients, approximation single variable
basis functions relative error 12%, policy generated value
optimal policy. case, true policy generated L2
projection. Unidirectional Ring 8 machines pairwise basis, relative
error approximation V 10%, resulting policy
6% loss optimal policy. problem, L2 approximation value
function error 12%, true policy loss 9%. words, methods induce
policies lower errors errors approximate value function (at least
small problems). However, algorithm continues outperform L2 algorithm,
even respect actual policy loss.
large models, longer compute correct value function, cannot
evaluate results computing kV Hwk . Fortunately, discussed Section 7,
Bellman error used provide bound approximation error
computed efficiently exploiting problem-specific structure. Figure 15(b) shows
Bellman error increases slowly number states.
also valuable look actual decision-list policies generated experiments.
First, noted lists tended short, length final decision list policy
grew approximately linearly number machines. Furthermore, policy
often fairly intuitive. Ring Star architecture, example, decision list
says: server faulty, fix server; else, another machine faulty, fix it.
Thus far, presented scaling results running times approximation error
approximate PI approach. compare algorithm simpler approximate
451

fiGuestrin, Koller, Parr & Venkataraman

400

200

PI single basis
PI single basis

160

LP single basis

140

LP pair basis

120

LP triple basis

Discounted reward final policy
(averaged 50 trials 100 steps)

Total running time (minutes)

180

100
80
60
40
20
0
0

5

10

15

20

25

30

35

numbe r machine

LP single basis
LP pair basis

300

LP triple basis

200

100

0
0

10

20

30

40

numbe r machine

(a)

(b)

Figure 16: Approximate LP versus approximate PI SysAdmin problem Ring
topology: (a) running time; (b) estimated value policy.

LP approach Section 5. shown Figure 16(a), approximate LP algorithm
factored MDPs significantly faster approximate PI algorithm. fact, approximate PI single-variable basis functions variables costly computationally
LP approach using basis functions consecutive triples variables. shown
Figure 16(b), singleton basis functions, approximate PI policy obtains slightly better
performance problem sizes. However, increase number basis functions
approximate LP formulation, value resulting policy much better. Thus,
problem, factored approximate linear programming formulation allows us use
basis functions obtain resulting policy higher value, still maintaining
faster running time. results, along simpler implementation, suggest
practice one may first try apply approximate linear programming algorithm
deciding move elaborate approximate policy iteration approach.
9.2 Comparing Table-based Rule-based Implementations
next evaluation compares table-based representation, exploits additive
independence, rule-based representation presented Section 8, exploit
additive context-specific independence. experiments, implemented
factored approximate linear programming algorithm table-based rule-based
representations C++, using CPLEX LP solver. Experiments performed
Sun UltraSPARC-II, 400 MHz 1GB RAM.
evaluate compare algorithms, utilized complex extension
SysAdmin problem. problem, dubbed Process-SysAdmin problem, contains three
state variables machine network: Loadi , Statusi Selectori . computer runs processes receives rewards processes terminate. processes
represented Loadi variable, takes values {Idle, Loaded, Success},
computer receives reward assignment Loadi Success. Statusi variable,
452

fitotal running time (minutes)

Efficient Solution Algorithms Factored MDPs

200
Table-based, single+ basis
Rule-based, single+ basis

150

Table-based, pair basis
100

Rule-based, pair basis

50
0
1E+00

1E+07

1E+14

1E+21

1E+28

1E+35

1E+42

number states

total running time (minutes)

(a)
250
200

Table-based, single+ basis
Rule-based, single+ basis

150

Table-based, pair basis
100
Rule-based, pair basis
50
0
1E+00 1E+04 1E+08 1E+12 1E+16 1E+20 1E+24 1E+28
number states

(b)

total running time (minutes)

600
2
(x-1)

(x-1)

= 7E- 17 x * 18 + 2E- 06 x * 18
+ 0.1124
2
R = 0.995

500

Table-based, single+ basis
Rule-based, single+ basis

400
300

3

2

= 0.2294x - 4.5415x +
30.974x - 67.851
R 2= 0.9995

200
100
0
0

5

10
number machines

15

20

(c)
Figure 17: Running time Process-SysAdmin problem various topologies: (a) Star;
(b) Ring; (c) Reverse star (with fit function).

453

fiGuestrin, Koller, Parr & Venkataraman

CPLEX time / Total time

1
0.8
Table-based, single+ basis
0.6
Rule-based, single+ basis
0.4
0.2
0
0

5

10

15

20

number machines

Figure 18: Fraction total running time spent CPLEX Process-SysAdmin problem Ring topology.

representing status machine i, takes values {Good, Faulty, Dead}; value
Faulty, processes smaller probability terminating value Dead,
running process lost Loadi becomes Idle. status machine become Faulty eventually Dead random; however, machine receives packet
dead machine, probability Statusi becomes Faulty Dead increases.
Selectori variable represents communication selecting one neighbors
uniformly random every time step. SysAdmin select one computer
reboot every time step. computer rebooted, status becomes Good
probability 1, running process lost, i.e., Loadi variable becomes Idle.
Thus, problem, SysAdmin must balance several conflicting goals: rebooting
machine kills processes, rebooting machine may cause cascading faults network.
Furthermore, SysAdmin choose one machine reboot, imposes additional tradeoff selecting one (potentially many) faulty dead machines
network reboot.
experimented two types basis functions: single+ includes indicators
joint assignments Loadi , Statusi Selectori , pair which, addition,
includes set indicators Statusi , Statusj , Selectori = j, neighbor j
machine network. discount factor = 0.95. variable elimination
order eliminated Loadi variables first, followed patterns
simple SysAdmin problem, eliminating first Statusi Selectori machine
eliminated.
Figure 17 compares running times table-based implementation ones
rule-based representation three topologies: Star, Ring, Reverse star.
Reverse star topology reverses direction influences Star: rather
central machine influencing machines topology, machines influence
central one. three topologies demonstrate three different levels CSI:
454

fiEfficient Solution Algorithms Factored MDPs

Star topology, factors generated variable elimination small. Thus, although
running times polynomial number state variables methods, tablebased representation significantly faster rule-based one, due overhead
managing rules. Ring topology illustrates intermediate behavior: single+
basis functions induce relatively small variable elimination factors, thus table-based
approach faster. However, pair basis factors larger rule-based
approach starts demonstrate faster running times larger problems. Finally, Reverse star topology represents worst-case scenario table-based approach. Here,
scope backprojection basis function central machine involve
computers network, machines potentially influence central one
next time step. Thus, size factors table-based variable elimination approach exponential number machines network, illustrated
exponential growth Figure 17(c). rule-based approach exploit CSI
problem; example, status central machine Status0 depends machine
j value selector j, i.e., Selector0 = j. exploiting CSI, solve
problem polynomial time number state variables, seen second curve
Figure 17(c).
also instructive compare portion total running time spent CPLEX
table-based compared rule-based approach. Figure 18 illustrates
comparison. Note amount time spent CPLEX significantly higher
table-based approach. two reasons difference: first, due CSI, LPs
generated rule-based approach smaller table-based ones; second, rulebased variable elimination complex table-based one, due overhead
introduced rule management. Interestingly, proportion CPLEX time increases
problem size increases, indicating asymptotic complexity LP solution
higher variable elimination, thus suggesting that, larger problems, additional
large-scale LP optimization procedures, constraint generation, may helpful.
9.3 Comparison Apricodd
closely related work line research began work
Boutilier et al. (1995). particular, approximate Apricodd algorithm Hoey et
al. (1999), uses analytic decision diagrams (ADDs) represent value function
strong alternative approach solving factored MDPs. discussed detail Section 10, Apricodd algorithm successfully exploit context-specific structure
value function, representing set mutually-exclusive exhaustive branches
ADD. hand, approach exploit additive context-specific
structure problem, using linear combination non-mutually-exclusive rules.
better understand difference, evaluated rule-based approximate linear
programming algorithm Apricodd two problems, Linear Expon, designed
Boutilier et al. (2000) illustrate respectively best-case worst-case behavior
algorithm. experiments, used web-distributed version Apricodd (Hoey, St-Aubin, Hu, & Boutilier, 2002), running locally Linux Pentium III
700MHz 1GB RAM.
455

fiGuestrin, Koller, Parr & Venkataraman

500

Rule-based

40
30

3

2

= 0.1473x - 0.8595x + 2.5006x - 1.5964
2

R = 0.9997

20

Apricodd
2

= 0.0254x + 0.0363x
+ 0.0725

10

Apricodd

400
Time (in seconds)

Time (in seconds)

50

x

2

x

= 3E-05 * 2 - 0.0026 * 2 + 5.6737
R2 = 0.9999

300
200

Rule-based
= 5.275x3 - 29.95x2 +
53.915x - 28.83
R2 = 1

100

2

R = 0.9983

0

0

6

8

10
12
14
16
Number variables

18

6

20

8

10

12

Number variables

(a)

(b)

Figure 19: Comparing Apricodd rule-based approximate linear programming (a)
Linear (b) Expon problems.

two problems involve n binary variables X1 , . . . , Xn n deterministic actions
a1 , . . . , . reward 1 variables Xk true, 0 otherwise. problem
discounted factor = 0.99. difference Linear Expon
problems transition probabilities. Linear problem, action ak sets
variable Xk true makes succeeding variables, Xi > k, false. state space
Linear problem seen binary number, optimal policy set repeatedly
largest bit (Xk variable) preceding bits set true. Using ADD, optimal
value function problem represented linear space, n+1 leaves (Boutilier
et al., 2000). best-case Apricodd, algorithm compute value
function quite efficiently. Figure 19(a) compares running time Apricodd
one algorithms indicator basis functions pairs consecutive variables.
Note algorithms obtain policy polynomial time number
variables. However, structured problems, efficient implementation ADD
package used Apricodd makes faster problem.
hand, Expon problem illustrates worst-case Apricodd.
problem, action ak sets variable Xk true, preceding variables, Xi < k,
true, makes preceding variables false. state space seen binary number,
optimal policy goes binary numbers sequence, repeatedly setting
largest bit (Xk variable) preceding bits set true. Due discounting,
n
optimal value function assigns value 2 j1 jth binary number,
value function contains exponentially many different values. Using ADD, optimal
value function problem requires exponential number leaves (Boutilier et al.,
2000), illustrated exponential running time Figure 19(b). However,
value function approximated compactly factored linear value
function using n + 1 basis functions: indicator variable Xk constant
base. shown Figure 19(b), using representation, factored approximate linear
programming algorithm computes value function polynomial time. Furthermore,
456

fiEfficient Solution Algorithms Factored MDPs

30

60

Running time (minutes)

Discounted value policy
(avg. 50 runs 100 steps)

Rule-based LP

50

Apricodd
40
30
20
10
0

Rule-based LP

25

Apricodd
20
15
10
5
0

0

2

4
6
8
Number machines

10

12

0

2

4
6
8
Number machines

(a)

12

10

12

(b)

50

30

45

Rule-based LP

Rule-based LP

40

Discounted value policy
(avg. 50 runs 100 steps)

Running time (minutes)

10

Apricodd

35
30
25
20
15
10

25

Apricodd

20
15
10
5

5
0

0
0

2

4
6
8
Number machines

10

12

(c)

0

2

4
6
8
Number machines

(d)

Figure 20: Comparing Apricodd rule-based approximate linear programming single+ basis functions Process-SysAdmin problem Ring topology
(a) running time (b) value resulting policy; Star topology
(c) running time (d) value resulting policy.

policy obtained approach optimal problem. Thus, problem,
ability exploit additive independence allows efficient polynomial time solution.
also compared Apricodd rule-based approximate linear programming
algorithm Process-SysAdmin problem. problem significant additive structure reward function factorization transition model. Although type
structure exploited directly Apricodd, ADD approximation steps performed
algorithm can, principle, allow Apricodd find approximate solutions problem. spent significant amount time attempting find best set parameters
Apricodd problems.4 settled sift method variable reordering
round approximation method size (maximum ADD size) criteria.
4. grateful Jesse Hoey Robert St-Aubin assistance selecting parameters.

457

fiGuestrin, Koller, Parr & Venkataraman

allow value function representation scale problem size, set maximum
ADD size 4000 + 400n network n machines. (We experimented variety
different growth rates maximum ADD size; here, parameters,
selected choice gave best results Apricodd.) compared Apricodd
parameters rule-based approximate linear programming algorithm
single+ basis functions Pentium III 700MHz 1GB RAM. results
summarized Figure 20.
small problems (up 45 machines), performance two algorithms
fairly similar terms running time quality policies generated.
However, problem size grows, running time Apricodd increases rapidly,
becomes significantly higher algorithm . Furthermore, problem size
increases, quality policies generated Apricodd also deteriorates. difference
policy quality caused different value function representation used two
algorithms. ADDs used Apricodd represent k different values k leaves; thus,
forced agglomerate many different states represent using single value.
smaller problems, agglomeration still represent good policies. Unfortunately,
problem size increases state space grows exponentially, Apricodds policy
representation becomes inadequate, quality policies decreases.
hand, linear value functions represent exponentially many values k basis
functions, allows approach scale significantly larger problems.

10. Related Work
closely related work line research began work
Boutilier et al. (1995). address comparison separately below, begin
section broader background references.
10.1 Approximate MDP Solutions
field MDPs, popularly known, formalized Bellman (1957)
1950s. importance value function approximation recognized early stage
Bellman (1963). early 1990s MDP framework recognized AI
researchers formal framework could used address problem planning
uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993).
Within AI community, value function approximation developed concomitantly
notion value function representations Markov chains. Suttons seminal paper
temporal difference learning (1988), addressed use value functions prediction
planning, assumed general representation value function noted
connection general function approximators neural networks. However,
stability combination directly addressed time.
Several important developments gave AI community deeper insight relationship function approximation dynamic programming. Tsitsiklis Van
Roy (1996a) and, independently, Gordon (1995) popularized analysis approximate
MDP methods via contraction properties dynamic programming operator
function approximator. Tsitsiklis Van Roy (1996b) later established general convergence result linear value function approximators D(), Bertsekas
458

fiEfficient Solution Algorithms Factored MDPs

Tsitsiklis (1996) unified large body work approximate dynamic programming
name Neuro-dynamic Programming, also providing many novel general error
analyses.
Approximate linear programming MDPs using linear value function approximation
introduced Schweitzer Seidmann (1985), although approach somewhat
deprecated fairly recently due lack compelling error analyses lack
effective method handling large number constraints. Recent work de Farias
Van Roy (2001a, 2001b) started address concerns new error bounds
constraint sampling methods. approach, rather sampling constraints, utilizes
structure model value function represent constraints compactly.
10.2 Factored Approaches
Tatman Shachter (1990) considered additive decomposition value nodes influence diagrams. number approaches factoring general MDPs explored
literature. Techniques exploiting reward functions decompose additively
studied Meuleau et al. (1998), Singh Cohn (1998).
use factored representations dynamic Bayesian networks pioneered
Boutilier et al. (1995) developed steadily recent years. methods rely
use context-specific structures decision trees analytic decision diagrams
(ADDs) (Hoey et al., 1999) represent transition dynamics DBN
value function. algorithms use dynamic programming partition state space,
representing partition using tree-like structure branches state variables
assigns values leaves. tree grown dynamically part dynamic programming process algorithm creates new leaves needed: leaf split
application DP operator two states associated leaf turn
different values backprojected value function. process also interpreted
form model minimization (Dean & Givan, 1997).
number leaves tree used represent value function determines computational complexity algorithm. also limits number distinct values
assigned states: since leaves represent partitioning state space, every state
maps exactly one leaf. However, recognized early on, trivial MDPs
require exponentially large value functions. observation led line approximation
algorithms aimed limiting tree size (Boutilier & Dearden, 1996) and, later, limiting
ADD size (St-Aubin, Hoey, & Boutilier, 2001). Kim Dean (2001) also explored
techniques discovering tree-structured value functions factored MDPs.
methods permit good approximate solutions large MDPs, complexity still
determined number leaves representation number distinct values
assigned states still limited well.
Tadepalli Ok (1996) first apply linear value function approximation
Factored MDPs. Linear value function approximation potentially expressive
approximation method assign unique values every state MDP without
requiring storage space exponential number state variables. expressive
power tree k leaves captured linear function approximator k basis
functions basis function hi indicator function tests state belongs
459

fiGuestrin, Koller, Parr & Venkataraman

partition leaf i. Thus, set value functions represented
tree k leaves subset set value functions represented
value function k basis functions. experimental results Section 9.3 highlight
difference showing example problem requires exponentially many leaves
value function, approximated well using linear value function.
main advantage tree-based value functions structure determined
dynamically solution MDP. principle, value function representation derived automatically model description, approach requires less insight
user. problems value function well approximated relatively small number values, approach provides excellent solution problem.
method linear value function approximation aims address believe
common case, large range distinct values required achieve good
approximation.
Finally, note Schuurmans Patrascu (2001), based earlier work
max-norm projection using cost networks linear programs, independently developed
alternative approach approximate linear programming using cost network.
method embeds cost network inside single linear program. contrast, method
based constraint generation approach, using cost network detect constraint
violations. constraint violations found, new constraint added, repeatedly
generating attempting solve LPs feasible solution found. Interestingly,
approach Schuurmans Patrascu uses multiple calls variable elimination
order speed LP solution step, successful time spent
solving LP significantly larger time required variable elimination.
suggested Section 9.2, LP solution time larger table-based approach. Thus,
Schuurmans Patrascus constraint generation method probably successful
table-based problems rule-based ones.

11. Conclusions
paper, presented new algorithms approximate linear programming approximate dynamic programming (value policy iteration) factored MDPs.
algorithms leverage novel LP decomposition technique, analogous variable elimination cost networks, reduces exponentially large LP provably
equivalent, polynomial-sized one.
approximate dynamic programming algorithms motivated error analyses
showing importance minimizing L error. algorithms efficient
substantially easier implement previous algorithms based L2 -projection.
experimental results suggest also perform better practice.
approximate linear programming algorithm factored MDPs simpler, easier
implement general dynamic programming approaches. Unlike policy
iteration algorithm, rely default action assumption, states
actions affect small number state variables. Although algorithm
theoretical guarantees max-norm projection approaches, empirically seems
favorable option. experiments suggest approximate policy iteration tends
generate better policies set basis functions. However, due computa460

fiEfficient Solution Algorithms Factored MDPs

tional advantages, add basis functions approximate linear programming
algorithm, obtaining better policy still maintaining much faster running time
approximate policy iteration.
Unlike previous approaches, algorithms exploit additive contextspecific structure factored MDP model. Typical real-world systems possess
types structure. thus, feature algorithms increase applicability factored MDPs practical problems. demonstrated exploiting
context-specific independence, using rule-based representation instead standard
table-based one, yield exponential improvements computational time problem significant amounts CSI. However, overhead managing sets rules make
less well-suited simpler problems. also compared approach work
Boutilier et al. (2000), exploits context-specific structure. problems
significant context-specific structure value function, approach faster due
efficient handling ADD representation. However, problems
significant context-specific structure problem representation, rather value
function, require exponentially large ADDs. problems, demonstrated using linear value function algorithm obtain polynomial-time
near-optimal approximation true value function.
success algorithm depends ability capture important
structure value function using linear, factored approximation. ability, turn,
depends choice basis functions properties domain.
algorithms currently require designer specify factored basis functions.
limitation compared algorithms Boutilier et al. (2000), fully automated.
However, experiments suggest simple rules quite successful designing basis. First, ensure reward function representable basis.
simple basis that, addition, contained separate set indicators variable often
quite well. also add indicators pairs variables; simply, choose
according DBN transition model, indicator added variables
Xi one variables Parents(Xi ), thus representing one-step influences.
procedure extended, adding basis functions represent influences
required. Thus, structure DBN gives us indications choose basis
functions. sources prior knowledge also included specifying
basis.
Nonetheless, general algorithm choosing good factored basis functions still
exist. However, potential approaches: First, problems CSI, one
could apply algorithms Boutilier et al. iterations generate partial treestructured solutions. Indicators defined variables backprojection leaves
could, turn, used generate basis set problems. Second, Bellman
error computation, performed efficiently shown Section 7,
provide bound quality policy, also actual state error
largest. knowledge used create mechanism incrementally increase
basis set, adding new basis functions tackle states high Bellman error.
many possible extensions work. already pursued extensions collaborative multiagent systems, multiple agents act simultaneously
maximize global reward (Guestrin et al., 2001b), factored POMDPs,
461

fiGuestrin, Koller, Parr & Venkataraman

full state observed directly, indirectly observation variables (Guestrin,
Koller, & Parr, 2001c). However, settings remain explored.
particular, hope address problem learning factored MDP planning
competitive multiagent system.
Additionally, paper tackled problems induced width cost
network sufficiently low possess sufficient context-specific structure allow
exact solution factored LPs. Unfortunately, practical problems may
prohibitively large induced width. plan leverage ideas loopy belief propagation algorithms approximate inference Bayesian networks (Pearl, 1988; Yedidia,
Freeman, & Weiss, 2001) address issue.
believe methods described herein significantly extend efficiency,
applicability general usability factored models value functions control
practical dynamic systems.

Acknowledgements
grateful Craig Boutilier, Dirk Ormoneit Uri Lerner many useful
discussions, anonymous reviewers detailed thorough comments.
also would like thank Jesse Hoey, Robert St-Aubin, Alan Hu, Craig Boutilier
distributing algorithm useful assistance using Apricodd
selecting parameters. work supported DoD MURI program, administered Office Naval Research Grant N00014-00-1-0637, Air Force contract
F30602-00-2-0598 DARPAs TASK program, Sloan Foundation. first
author also supported Siebel Scholarship.

Appendix A. Proofs
A.1 Proof Lemma 3.3
exists setting weights zero setting yields bounded maxnorm projection error P policy (P Rmax ). max-norm projection operator
chooses set weights minimizes projection error (t) policy (t) . Thus,
projection error (t) must least low one given zero weights P
(which bounded). Thus, error remains bounded iterations.
A.2 Proof Theorem 3.5
First, need bound approximation V(t) :




V(t) Hw(t)












T(t) Hw(t) Hw(t)




+ V(t) T(t) Hw(t)








(t)
(t)
T(t) Hw Hw + V(t) Hw(t)






; (triangle inequality;)

; (T(t) contraction.)

Moving second term right hand side dividing 1 , obtain:




V(t) Hw(t)






1
(t)


.
T(t) Hw(t) Hw(t) =

1
1
462

(24)

fiEfficient Solution Algorithms Factored MDPs

next part proof, adapt lemma Bertsekas Tsitsiklis (1996, Lemma
6.2, p.277) fit framework. manipulation, lemma reformulated as:
kV V(t+1) k kV V(t) k +


2


V(t) Hw(t) .

1

(25)

proof concluded substituting Equation (24) Equation (25) and, finally, induction t.
A.3 Proof Theorem 4.4
First, note equality constraints represent simple change variable. Thus,
rewrite Equation (12) terms new LP variables ufzii as:
max

X

x

ufzii ,

(26)



assignment weights w implies assignment ufzii . stage,
LP variables.
remains show factored LP construction equivalent constraint
Equation (26). system n variables {X1 , . . . , Xn }, assume, without loss
generality, variables eliminated starting Xn X1 . prove
equivalence induction number variables.
base case n = 0, functions ci (x) b(x) Equation (12)
empty scope. case, Equation (26) written as:


X

uei .

(27)



case, transformation done constraint, equivalence immediate.
Now, assume result holds systems i1 variables prove equivalence
system variables. system, maximization decomposed
two terms: one factors depend Xi , irrelevant
maximization Xi , another term factors depend Xi . Using
decomposition, write Equation (26) as:




max

X ej

x1 ,...,xi

uzj ;

j

max

x1 ,...,xi1




X

X

uezll + max
xi

l : xi 6zl

e
uzjj .

(28)

j : xi zj

point define new LP variables uez corresponding second term
right hand side constraint. new LP variables must satisfy following
constraint:
uez max
xi

X ej
j=1

463

u(z,xi )[Zj ] .

(29)

fiGuestrin, Koller, Parr & Venkataraman

new non-linear constraint represented factored LP construction
set equivalent linear constraints:
uez

X ej
j=1

u(z,xi )[Zj ] , z, xi .

(30)

equivalence non-linear constraint Equation (29) set linear constraints Equation (30) shown considering binding constraints. new
LP variable created uez , |Xi | new constraints created, one value xi Xi .
assignment LP variables right hand side constraint EquaP
ej
tion (30), one |Xi | constraints relevant. is, one `j=1 u(z,x
)[Zj ]
maximal, corresponds maximum Xi . Again, value z
one assignment Xi achieves maximum, (and only) constraints
corresponding maximizing assignments could binding. Thus, Equation (29)
Equation (30) equivalent.
Substituting new LP variables uez Equation (28), get:


max

x1 ,...,xi1

X

uezll + uez ,

l : xi 6zl

depend Xi anymore. Thus, equivalent system i1 variables,
concluding induction step proof.
A.4 Proof Lemma 7.1
First note iteration + 1 objective function (t+1) max-norm projection
LP given by:








(t+1) = Hw(t+1) R(t+1) + P(t+1) Hw(t+1) .


However, convergence value function estimates equal iterations:
w(t+1) = w(t) .
that:








(t+1) = Hw(t) R(t+1) + P(t+1) Hw(t) .


operator notation, term equivalent to:






(t+1) = Hw(t) T(t+1) Hw(t) .


Note that, (t+1) = Greedy(Hw(t) ) definition. Thus, that:
T(t+1) Hw(t) = Hw(t) .
Finally, substituting previous expression, obtain result:






(t+1) = Hw(t) Hw(t) .


464

fiEfficient Solution Algorithms Factored MDPs

References
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity finding embeddings
K-tree. SIAM Journal Algebraic Discrete Methods, 8 (2), 277 284.
Becker, A., & Geiger, D. (2001). sufficiently fast algorithm finding close optimal
clique trees. Artificial Intelligence, 125 (1-2), 317.
Bellman, R., Kalaba, R., & Kotkin, B. (1963). Polynomial approximation new computational technique dynamic programming. Math. Comp., 17 (8), 155161.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, New
Jersey.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press, New
York.
Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, Massachusetts.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research, 11, 1
94.
Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamic
programming. Proc. ICML, pp. 5462.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proc. IJCAI, pp. 11041111.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121 (1-2), 49107.
Cheney, E. W. (1982). Approximation Theory (2nd edition). Chelsea Publishing Co., New
York, NY.
de Farias, D., & Van Roy, B. (2001a). linear programming approach approximate
dynamic programming. Submitted Operations Research.
de Farias, D., & Van Roy, B. (2001b). constraint sampling linear programming approach approximate dynamic programming. appear Mathematics
Operations Research.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning deadlines
stochastic domains. Proceedings Eleventh National Conference Artificial
Intelligence (AAAI-93), pp. 574579, Washington, D.C. AAAI Press.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5 (3), 142150.
Dean, T., & Givan, R. (1997). Model minimization Markov decision processes.
Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97), pp. 106111, Providence, Rhode Island, Oregon. AAAI Press.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.
465

fiGuestrin, Koller, Parr & Venkataraman

Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (12), 4185.
Gordon, G. (1995). Stable function approximation dynamic programming. Proceedings
Twelfth International Conference Machine Learning, pp. 261268, Tahoe
City, CA. Morgan Kaufmann.
Guestrin, C. E., Koller, D., & Parr, R. (2001a). Max-norm projections factored MDPs.
Proceedings Seventeenth International Joint Conference Artificial Intelligence (IJCAI-01), pp. 673 680, Seattle, Washington. Morgan Kaufmann.
Guestrin, C. E., Koller, D., & Parr, R. (2001b). Multiagent planning factored MDPs.
14th Neural Information Processing Systems (NIPS-14), pp. 15231530, Vancouver,
Canada.
Guestrin, C. E., Koller, D., & Parr, R. (2001c). Solving factored POMDPs linear value
functions. Seventeenth International Joint Conference Artificial Intelligence
(IJCAI-01) workshop Planning Uncertainty Incomplete Information,
pp. 67 75, Seattle, Washington.
Guestrin, C. E., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination planning factored MDPs. Eighteenth National Conference
Artificial Intelligence (AAAI-2002), pp. 253259, Edmonton, Canada.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision diagrams. Proceedings Fifteenth Conference Uncertainty
Artificial Intelligence (UAI-99), pp. 279288, Stockholm, Sweden. Morgan Kaufmann.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (2002). Stochastic planning using decision
diagrams C implementation. http://www.cs.ubc.ca/spider/staubin/Spudd/.
Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Howard, R. A., & Matheson, J. E. (Eds.), Readings Principles Applications Decision Analysis,
pp. 721762. Strategic Decisions Group, Menlo Park, California.
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences
Value Tradeoffs. Wiley, New York.
Kim, K.-E., & Dean, T. (2001). Solving factored Mdps using non-homogeneous partitioning. Proceedings Seventeenth International Joint Conference Artificial
Intelligence (IJCAI-01), pp. 683 689, Seattle, Washington. Morgan Kaufmann.
Kjaerulff, U. (1990). Triangulation graphs algorithms giving small total state space.
Tech. rep. TR R 90-09, Department Mathematics Computer Science, Strandvejen, Aalborg, Denmark.
Koller, D., & Parr, R. (1999). Computing factored value functions policies structured
MDPs. Proceedings Sixteenth International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 1332 1339. Morgan Kaufmann.
Koller, D., & Parr, R. (2000). Policy iteration factored MDPs. Proceedings
Sixteenth Conference Uncertainty Artificial Intelligence (UAI-00), pp. 326
334, Stanford, California. Morgan Kaufmann.
466

fiEfficient Solution Algorithms Factored MDPs

Meuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving large weakly-coupled Markov decision processes. Proceedings
15th National Conference Artificial Intelligence, pp. 165172, Madison, WI.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann, San Mateo, California.
Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley, New York.
Reed, B. (1992). Finding approximate separators computing tree-width quickly.
24th Annual Symposium Theory Computing, pp. 221228. ACM.
Schuurmans, D., & Patrascu, R. (2001). Direct value-approximation factored MDPs.
Advances Neural Information Processing Systems (NIPS-14), pp. 15791586,
Vancouver, Canada.
Schweitzer, P., & Seidmann, A. (1985). Generalized polynomial approximations Markovian decision processes. Journal Mathematical Analysis Applications, 110, 568
582.
Simon, H. A. (1981). Sciences Artificial (second edition). MIT Press, Cambridge,
Massachusetts.
Singh, S., & Cohn, D. (1998). dynamically merge Markov decision processes.
Jordan, M. I., Kearns, M. J., & Solla, S. A. (Eds.), Advances Neural Information
Processing Systems, Vol. 10. MIT Press.
St-Aubin, R., Hoey, J., & Boutilier, C. (2001). APRICODD: Approximate policy construction using decision diagrams. Advances Neural Information Processing Systems
13: Proceedings 2000 Conference, pp. 10891095, Denver, Colorado. MIT Press.
Stiefel, E. (1960). Note Jordan elimination, linear programming Tchebycheff approximation. Numerische Mathematik, 2, 1 17.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine
Learning, 3, 944.
Tadepalli, P., & Ok, D. (1996). Scaling average reward reinforcmeent learning approximating domain models value function. Proceedings Thirteenth
International Conference Machine Learning, Bari, Italy. Morgan Kaufmann.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming influence diagrams.
IEEE Transactions Systems, Man Cybernetics, 20 (2), 365379.
Tsitsiklis, J. N., & Van Roy, B. (1996a). Feature-based methods large scale dynamic
programming. Machine Learning, 22, 5994.
Tsitsiklis, J. N., & Van Roy, B. (1996b). analysis temporal-difference learning
function approximation. Technical report LIDS-P-2322, Laboratory Information
Decision Systems, Massachusetts Institute Technology.
Van Roy, B. (1998). Learning Value Function Approximation Complex Decision
Processes. Ph.D. thesis, Massachusetts Institute Technology.
467

fiGuestrin, Koller, Parr & Venkataraman

Williams, R. J., & Baird, L. C. I. (1993). Tight performance bounds greedy policies based
imperfect value functions. Tech. rep., College Computer Science, Northeastern
University, Boston, Massachusetts.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. Advances
Neural Information Processing Systems 13: Proceedings 2000 Conference,
pp. 689695, Denver, Colorado. MIT Press.
Zhang, N., & Poole, D. (1999). role context-specific independence probabilistic
reasoning. Proceedings Sixteenth International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 12881293. Morgan Kaufmann.

468

fiJournal Artificial Intelligence Research 19 (2003) 1-10

Submitted 10/02; published 7/03

Research Note
New Polynomial Classes Logic-Based Abduction
Bruno Zanuttini

zanutti@info.unicaen.fr

GREYC, Universite de Caen, Boulevard du Marechal Juin
14032 Caen Cedex, France

Abstract
address problem propositional logic-based abduction, i.e., problem
searching best explanation given propositional observation according given
propositional knowledge base. give general algorithm, based notion projection; study restrictions representations knowledge base
query, find new polynomial classes abduction problems.

1. Introduction
Abduction consists searching plausible explanation given observation.
instance, p |= q p plausible explanation observation q. generally,
abduction process searching set facts (the explanation, p) that,
conjointly given knowledge base (here p q), imply given query (q). process
also constrained set hypotheses among explanations chosen,
preference criterion among them.
problem abduction proved practical interest many domains. instance,
used formalize text interpretation (Hobbs et al., 1993), system (Coste-Marquis
& Marquis, 1998; Stumptner & Wotawa, 2001) medical diagnosis (Bylander et al., 1989,
Section 6). also closely related configuration problems (Amilhastre et al., 2002),
ATMS/CMS (Reiter & de Kleer, 1987), default reasoning (Selman & Levesque,
1990) even induction (Goebel, 1997).
interested complexity propositional logic-based abduction, i.e.,
assume knowledge base query represented propositional formulas.
Even framework, many different formalizations proposed literature,
mainly differing definition hypothesis best explanation (Eiter
& Gottlob, 1995). assume hypotheses conjunctions literals
formed upon distinguished subset variables involved, best explanation
one proper subconjunction explanation (subset-minimality criterion).
purpose exhibit new polynomial classes abduction problems. give
general algorithm finding best explanation framework defined above, independently syntactic form formulas representing knowledge base
query. explore syntactic forms allow polynomial running time
algorithm. find new polynomial classes abduction problems, among one
restricting knowledge base given Horn DNF query positive CNF,
one restricting knowledge base given affine formula query
disjunction linear equations. algorithm also unifies several previous results.
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBruno Zanuttini

note organized follows. first recall useful notions propositional
logic (Section 2), formalize problem (Section 3) briefly survey previous work
complexity abduction (Section 4). give algorithm (Section 5)
explore polynomial classes (Section 6). Finally, discuss results perspectives
(Section 7). lack space cannot detail proofs, longer version work,
containing detailed proofs examples, available (Zanuttini, 2003).

2. Preliminaries
assume countable number propositional variables x1 , x2 . . . standard connectives , , , , , . literal either variable xi (positive literal) negation
xi (negative literal). propositional formula well-formed formula built finite
number variables connectives; V ar() denotes set variables occur
propositional formula . clause finite disjunction literals, propositional formula Conjunctive Normal Form (CNF) written finite conjunction
clauses. instance, = (x1 x2 ) (x1 x2 x3 ) CNF. dual notions
clause CNF notions term (finite conjunction literals) Disjunctive
Normal Form (DNF) (finite disjunction terms).
assignment set variables V set literals contains exactly one
literal per variable V , model propositional formula assignment
V ar() satisfies usual way, assigns 1 xi iff xi m; also
write tuple, e.g., 0010 {x1 , x2 , x3 , x4 }. write m[i] value assigned
xi m, M() set models propositional formula ;
said satisfiable M() 6= . formula said imply propositional formula 0
(written |= 0 ) M() M(0 ). generally, identify sets models Boolean
functions, use notations (negation), M0 (disjunction) on.
notion projection important rest paper. assignment
set variables V V , write SelectA (m) set literals
formed upon A, e.g., Select{x1 ,x2 } (0110) = 01. Projecting set assignments onto subset
variables intuitively consists replacing assignment SelectA (m);
sake simplicity however, define projection set models built upon
set variables M. yields following definition.
Definition 1 (projection) Let V = {x1 , . . . , xn } set variables, set assignments V V . projection onto set assignments V
M|A = {m | m0 M, SelectA (m0 ) = SelectA (m)}.
instance, let = {0001, 0010, 0111, 1100, 1101} set assignments V =
{x1 , x2 , x3 , x4 }, let = {x1 , x2 }. easily seen
M|A = {0000, 0001, 0010, 0011} {0100, 0101, 0110, 0111} {1100, 1101, 1110, 1111}
since {SelectA (m) | M} = {00, 01, 11}.
Remark projection set models formula onto set variables
set models general consequence independent
variables A. Note also projection M() onto set models
formula obtained forgetting variables occurring A. details
2

fiLogic-Based Abduction

variable forgetting independence refer reader work Lang et
al. (Lang et al., 2002).
useful note straightforward properties projection. Let M, M0 denote
two sets assignments set variables V , let V . First, projection
distributive disjunction, i.e., (M M0 )|A = M|A M0 |A . distributive
conjunction depend variables M0 depends on, i.e., exist
A, A0 V , A0 = M|A = (M depend V \A) M0 |A0 = M0 ,
(M M0 )|A = M|A M0 |A holds; note true general case. Note finally
general (M)|A M|A .

3. Model Abduction
formalize model; sake simplicity, first define abduction problems
notions hypothesis explanation.
Definition 2 (abduction problem) triple = (, , A) called abduction problem satisfiable propositional formulas set variables
V ar(), V ar(); called knowledge base , query set
abducibles.
Definition 3 (hypothesis,explanation) Let = (, , A) abduction problem.
hypothesis set literals formed upon (seen conjunction),
hypothesis E explanation E satisfiable E |= .
proper subconjunction E explanation , E called best explanation .
Note framework allow one specify variable must occur unnegated
(resp. negated) explanation. think prohibiting restriction, since
abducibles intuitively meant represent variables whose values be, e.g., modified, observed repaired, matter sign explanation. note
restriction, general framework defined abducibles
literals hypotheses, conjunctions abducibles (Marquis, 2000).
interested computational complexity computing best explanation
given abduction problem, asserting none all. Following usual model,
establish complexities respect size representations
number abducibles; hardness results, following associated decision problem
usually considered: least one explanation ? Obviously, latter problem
hard, function problem also is.

4. Previous Work
main general complexity results propositional logic-based abduction subsetminimality preference stated Eiter Gottlob (1995). authors show
deciding whether given abduction problem solution P2 -complete problem,
even V ar() = V ar() CNF. stated well Selman Levesque
(1990), also establish problem becomes NP-complete Horn,
even acyclic Horn. Note SAT deduction polynomial
problem obviously NP.
3

fiBruno Zanuttini

fact, classes abduction problems known polynomial
search explanations. far know, classes defined
following restrictions (once refer reader references definitions):
2CNF 2DNF (Marquis, 2000, Section 4.2)
given monotone CNF clause (Marquis, 2000, Section 4.2)
given definite Horn CNF conjunction positive literals (Selman
& Levesque, 1990; Eiter & Gottlob, 1995)
given acyclic Horn CNF pseudo-completion unit-refutable
variable (Eshghi, 1993)
bounded induced kernel width given literal (del Val, 2000)
represented set characteristics models (with respect particular basis)
variable (Khardon & Roth, 1996); note set characteristic models
propositional formula, result however similar ones
represented set models, or, equivalently, DNF every variable
occurring term, propositional formula.
first two classes proved polynomial general method solving abduction
problems notion prime implicants, last one obvious since information
explicitely given input, four others exhibited ad hoc algorithms.
Let us also mention Amilhastre et al. (2002) study related problems
general framework multivalued theories instead propositional formulas, i.e.,
domain variables restricted {0, 1}. authors mainly show,
far note concerned, deciding whether exists explanation still
P2 -complete problem (Amilhastre et al., 2002, Table 1).
Note results stated exact framework papers cited
above, still hold it. Let us also mention problem enumerating
best explanations given abduction problem great interest; Eiter Makino
(2002) provide discussion first results it, mainly case
knowledge base Horn.

5. General Algorithm
give principle algorithm. Let us stress first that, well as, e.g., Marquis
construction (Marquis, 2000, Section 4.2), outline matches point point definition
best explanation; ideas Marquis anyway rather close.
first interested hypotheses every abducible x occurs (either
negated unnegated); let us call full hypotheses. Note indeed every explanation
E abduction problem subconjunction full explanation F ; indeed, since E
definition E satisfiable implies , suffices let F SelectA (m)
model E . Minimization F discussed later on.
4

fiLogic-Based Abduction

Proposition 1 Let = (, , A) abduction problem, F full hypothesis .
F explanation exists assignment V ar()
F = SelectA (m) M() (M( ))|A .
Proof Assume first F explanation . (i) exists assignment
V ar() |= F , thus F = SelectA (m) M(), (ii) F |= , i.e.,
F unsatisfiable, thus F
/ {SelectA (m) | M( )}, thus
/ (M( ))|A ,
thus (M( ))|A . Conversely, M() (M( ))|A let F = SelectA (m).
(i) since M(), F satisfiable, (ii) since
/ (M( ))|A ,
0
0
M( ) SelectA (m ) = F , thus F unsatisfiable, thus
F |= .

Thus characterized full explanations given abduction problem.
minimizing explanation F problem, since following greedy procedure,
given Selman Levesque (1990) reduces F best explanation :
every literal ` F
F \{`} |= F F \{`} endif;
Endfor;
Note depending order literals ` F considered result may
different, cases best explanation .
Finally, give general algorithm computing best explanation given
abduction problem = (, , A); correctness follows directly Proposition 1:
0 propositional formula M(0 ) = M() (M( ))|A ;
0 unsatisfiable return explanation;
Else
model 0 ;
F SelectA (m);
minimize F ;
return F ;
Endif;

6. Polynomial Classes
explore new polynomial classes abduction problems algorithm allows
exhibit. Throughout section, n denotes number variables V ar().
6.1 Affine Formulas
propositional formula said affine (or XOR-CNF ) (Schaefer, 1978; Kavvadias &
Sideri, 1998; Zanuttini, 2002) written finite conjunction linear equations
two-element field, e.g., = (x1 x3 = 1) (x1 x2 x4 = 0). seen, equations
play role affine formulas clauses CNFs; roughly, affine formulas represent
conjunctions parity equivalence constraints. class proves interesting knowledge
representation, since one hand tractable common reasoning tasks,
5

fiBruno Zanuttini

hand affine approximations knowledge base made small
efficiently learnable (Zanuttini, 2002). show projecting affine formula onto
subset variables quite easy too, enabling algorithm run polynomial time.
proof following lemma easily obtained gaussian elimination (Curtis, 1984):
triangulate variables put rightmost, keep equations
formed upon A; full details given technical report version (Zanuttini, 2003).
Lemma 1 Let affine formula containing k equations, V ar().
affine formula M() = (M())|A containing k equations
computed time O(k 2 |V ar()|).
Proposition 2 represented affine formula containing k equations
disjunction k 0 linear equations, subset V ar(), searching best
explanation = (, , A) done time O((k + k 0 )((k + 1)2 + |A|(k + k 0 ))n).
Sketch proof easily seen affine formula (containing k 0 + k equations
n variables) computed time linear size ; formula
projected onto time O((k + k 0 )2 n), straightforwardly get disjunction
k + k 0 linear equations (M( ))|A . use distributivity
solving satisfiability problem algorithm; recall SAT solved time
O(k 2 n) affine formula k equations n variables elimination method
Gauss (Curtis, 1984). remaining operations straightforward.

Note variables, literals clauses special cases disjunctions linear equations.
6.2 DNFs
Though class DNF formulas good computational properties, abduction
remains hard problem whole, even additional restrictions. Recall
TAUTOLOGY problem one deciding whether given DNF formula represents
identically true function, problem coNP-complete.
Proposition 3 Deciding whether least one explanation given abduction
problem (, , A) NP-complete given DNF, even variable
{} = V ar().
Sketch proof Membership NP obvious, since deduction DNFs polynomial;
easily seen tautological abduction problem (
(x), x, V ar()) explanation, x variable occuring (see DNF
(x) implication x); (x) DNF, get result.

However, represented DNF projecting onto easy; indeed, properties projection show suffices cancel literals formed upon A.
Consequently, DNF containing k terms, DNF M() = (M())|A
containing k terms computed time O(k|V ar()|).
Thus show subclasses class DNFs allow polynomial
abduction. state first result quite generally, note assumptions
satisfied natural classes DNFs: e.g., Horn DNFs, i.e., DNFs
6

fiLogic-Based Abduction

one positive literal per term; similarly, Horn-renamable DNFs, i.e.,
turned Horn DNF replacing variables negation,
simplifying double negations, everywhere formula; 2DNFs, DNFs
two literals per term. omit proof following proposition, since essentially
Proposition 2 (simply follow execution algorithm).
Proposition 4 Let class DNFs stable removal occurrences
literals TAUTOLOGY problem polynomial. restricted belong
D, clause subset V ar(), searching best explanation
= (, , A) done polynomial time.
Thus establish abduction tractable (among others) Horn-renamable
DNF (including Horn reverse Horn cases) 2DNF, clause.
Finally, let us point similar proof obtain polynomiality
problems obtained strengthening restriction Proposition 4 ,
weakening .
Proposition 5 represented Horn (resp. reverse Horn) DNF k terms
positive (resp. negative) CNF k 0 clauses, subset V ar(),
searching best explanation = (, , A) done time O((k + |A|)kk 0 n).
holds represented positive (resp. negative) DNF k terms
Horn (resp. reverse Horn) CNF k 0 clauses.
note variables, literals terms special cases (reverse) Horn
CNFs, variables, positive (resp. negative) clauses positive (resp. negative)
terms special cases positive (resp. negative) CNFs.

7. Discussion Perspectives
general algorithm presented note allows us derive new polynomial restrictions
abduction problems; even discussed here, lack space, also allows
unify previously known restrictions (such 2CNF 2DNF,
monotone CNF given clause). following list summarizes main new
polynomial restrictions:
given affine formula disjunction linear equations (Proposition 2)
Horn-renamable DNF given clause (Proposition 4)
2DNF given clause (Proposition 4)
Horn (reverse Horn) DNF positive (negative) CNF (Proposition 5)
negative (positive) DNF reverse Horn (Horn) CNF (Proposition 5).
Moreover, even guarantee efficiency general case presentation
algorithm depend syntactic form , uses standard
operations Boolean functions (projection, conjunction, negation).
7

fiBruno Zanuttini

Another interesting feature algorithm minimization computes
explanations intentionnally. Consequently, full explanations enumerated
roughly delay models formula representing (0 ). However,
course, guarantee two would minimized
best explanation, prevents concluding algorithm enumerate
best explanations; trying extend direction would interesting problem.
details enumeration refer reader Eiter Makinos work (Eiter
& Makino, 2002).
identified Selman Levesque (1990), central task notion projection set variables, algorithm isolates subtask. However, notion
projection concerns variables, literals, prevents imposing sign
literals hypotheses formed upon, contrariwise general formalizations
proposed abduction, Marquis (Marquis, 2000). Even think prohibiting restriction, would interesting try fix weakness algorithm
preserving polynomial classes.
Another problem interest behaviour algorithm
propositional formulas, generally multivalued theories, domain
variables restricted {0, 1}: e.g., signed formulas (Beckert et al., 1999).
framework used, instance, configuration problems Amilhastre et al. (2002).
easily seen algorithm still correct framework; however, still left
study cases running time polynomial.
Finally, problems great interest deciding relevance necessity
abducible (Eiter & Gottlob, 1995). abducible x said relevant abduction
problem least one best explanation containing x x, necessary
best explanations contain x x. easily seen x necessary
= (, , A) 0 = (, , A\{x}) explanation, hence showing
polynomial restrictions search explanations polynomial well deciding
necessity hypothesis soon stable substitution A\{x}
A, case restrictions considered note. Contrastingly,
know relation relevance, study problem would also
great interest.

Acknowledgments
author wishes thank anonymous referees version previous
one (Proc. JNPC02, French), well Jean-Jacques Hebrard, valuable
constructive comments.

References
Amilhastre, J., Fargier, H., & Marquis, P. (2002). Consistency restoration explanations
dynamic CSPs application configuration. Artificial Intelligence, 135 (12),
199234.
8

fiLogic-Based Abduction

Beckert, B., Hahnle, R., & Manya, F. (1999). Transformations signed classical clause logic. Proc. 29th International Symposium Multiple-Valued Logics
(ISMVL99), pp. 248255. IEEE Computer Society Press.
Bylander, T., Allemang, D., Tanner, M., & Josephson, J. (1989). results concerning
computational complexity abduction. Proc. 1st International Conference
Principles Knowledge Representation Reasoning (KR89), pp. 4454. Morgan
Kaufmann.
Coste-Marquis, S., & Marquis, P. (1998). Characterizing consistency-based diagnoses.
Proc. 5th International Symposium Artificial Intelligence Mathematics
(AIMATH98).
Curtis, C. (1984). Linear algebra. introductory approach. Springer Verlag.
del Val, A. (2000). complexity restricted consequence finding abduction.
Proc. 17th National Conference Artificial Intelligence (AAAI00), pp. 337342.
AAAI Press/MIT Press.
Eiter, T., & Gottlob, G. (1995). complexity logic-based abduction. Journal
ACM, 42 (1), 342.
Eiter, T., & Makino, K. (2002). computing abductive explanations. Proc. 18th
National Conference Artificial Intelligence (AAAI02), pp. 6267. AAAI Press.
Eshghi, K. (1993). tractable class abduction problems. Proc. 13th International
Joint Conference Artificial Intelligence (IJCAI93), pp. 38. Morgan Kaufmann.
Goebel, R. (1997). Abduction relation constrained induction. Proc. IJCAI97
workshop abduction induction AI.
Hobbs, J., Stickel, M., Appelt, D., & Martin, P. (1993). Interpretation abduction. Artificial Intelligence, 63, 69142.
Kavvadias, D., & Sideri, M. (1998). inverse satisfiability problem. SIAM Journal
Computing, 28 (1), 152163.
Khardon, R., & Roth, D. (1996). Reasoning models. Artificial Intelligence, 87, 187213.
Lang, J., Liberatore, P., & Marquis, P. (2002). Conditional independence propositional
logic. Artificial Intelligence, 141, 79121.
Marquis, P. (2000). Consequence finding algorithms. Handbook Defeasible Reasoning
Uncertainty Management Systems (DRUMS), Vol. 5, pp. 41145. Kluwer Academic.
Reiter, R., & de Kleer, J. (1987). Foundations assumption-based truth maintenance systems: preliminary report. Proc. 6th National Conference Artificial Intelligence
(AAAI87), pp. 183188. AAAI Press/MIT Press.
Schaefer, T. (1978). complexity satisfiability problems. Proc. 10th Annual ACM
Symposium Theory Computing (STOC78), pp. 216226. ACM Press.
Selman, B., & Levesque, H. (1990). Abductive default reasoning: computational core.
Proc. 8th National Conference Artificial Intelligence (AAAI90), pp. 343348.
AAAI Press.
9

fiBruno Zanuttini

Stumptner, M., & Wotawa, F. (2001). Diagnosing tree-structured systems. Artificial Intelligence, 127, 129.
Zanuttini, B. (2002). Approximating propositional knowledge affine formulas.
Proc. 15th European Conference Artificial Intelligence (ECAI02), pp. 287291.
IOS Press.
Zanuttini, B. (2003). New polynomial classes logic-based abduction. Tech. rep., Universite de Caen, France.

10

fiJournal Artificial Intelligence Research 19 (2003) 569-629

Submitted 05/01; published 12/03

Accelerating Reinforcement Learning
Implicit Imitation
Bob Price

price@cs.ubc.ca

Department Computer Science
University British Columbia
Vancouver, B.C., Canada V6T 1Z4

Craig Boutilier

cebly@cs.toronto.edu

Department Computer Science
University Toronto
Toronto, ON, Canada M5S 3H5

Abstract
Imitation viewed means enhancing learning multiagent environments.
augments agents ability learn useful behaviors making intelligent use
knowledge implicit behaviors demonstrated cooperative teachers experienced agents. propose study formal model implicit imitation
accelerate reinforcement learning dramatically certain cases. Roughly, observing
mentor, reinforcement-learning agent extract information capabilities
in, relative value of, unvisited parts state space. study two specific
instantiations model, one learning agent mentor identical
abilities, one designed deal agents mentors different action sets.
illustrate benefits implicit imitation integrating prioritized sweeping,
demonstrating improved performance convergence observation single
multiple mentors. Though make stringent assumptions regarding observability
possible interactions, briefly comment extensions model relax
restricitions.

1. Introduction
application reinforcement learning multiagent systems offers unique opportunities
challenges. agents viewed independently trying achieve ends,
interesting issues interaction agent policies (Littman, 1994) must resolved (e.g.,
appeal equilibrium concepts). However, fact agents may share information
mutual gain (Tan, 1993) distribute search optimal policies communicate reinforcement signals one another (Mataric, 1998) offers intriguing possibilities
accelerating reinforcement learning enhancing agent performance.
Another way individual agent performance improved
novice agent learn reasonable behavior expert mentor. type learning
brought explicit teaching demonstration (Atkeson & Schaal, 1997;
Lin, 1992; Whitehead, 1991a), sharing privileged information (Mataric, 1998),
explicit cognitive representation imitation (Bakker & Kuniyoshi, 1996).
imitation, agents exploration used ground observations agents
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiPrice & Boutilier

behaviors capabilities resolve ambiguities observations arising
partial observability noise. common thread work use mentor
guide exploration observer. Typically, guidance achieved form
explicit communication mentor observer. less direct form teaching
involves observer extracting information mentor without mentor making
explicit attempt demonstrate specific behavior interest (Mitchell, Mahadevan, &
Steinberg, 1985).
paper develop imitation model call implicit imitation allows
agent accelerate reinforcement learning process observation expert
mentor (or mentors). agent observes state transitions induced mentors
actions uses information gleaned observations update estimated
value states actions. distinguish two settings implicit
imitation occur: homogeneous settings, learning agent mentor
identical actions; heterogeneous settings, capabilities may differ.
homogeneous setting, learner use observed mentor transitions directly
update estimated model actions, update value function. addition,
mentor provide hints observer parts state space
may worth focusing attention. observers attention area might take form
additional exploration area additional computation brought bear agents
prior beliefs area. heterogeneous setting, similar benefits accrue,
potential agent misled mentor possesses abilities different
own. case, learner needs mechanism detect situations
make efforts temper influence observations.
derive several new techniques support implicit imitation largely independent specific reinforcement learning algorithm, though best suited use
model-based methods. include model extraction, augmented backups, feasibility
testing, k-step repair. first describe implicit imitation homogeneous domains,
describe extension heterogeneous settings. illustrate effectiveness
empirically incorporating Moore Atkesons (1993) prioritized sweeping algorithm.
implicit imitation model several advantages direct forms imitation
teaching. require agent explicitly play role mentor teacher.
Observers learn simply watching behavior agents; observed mentor
shares certain subtasks observer, observed behavior incorporated (indirectly) observer improve estimate value function. important
many situations observer learn mentor
unwilling unable alter behavior accommodate observer, even communicate
information it. example, common communication protocols may unavailable
agents designed different developers (e.g., Internet agents); agents may find
competitive situation disincentive share information skills;
may simply incentive one agent provide information another.1
Another key advantage approachwhich arises formalizing imitation
reinforcement learning contextis fact observer constrained directly
1. reasons consistency, use term mentor describe agent observer
learn, even mentor unwilling unwitting participant.

570

fiImplicit Imitation

imitate (i.e., duplicate actions of) mentor. learner decide whether
explicit imitation worthwhile. Implicit imitation thus seen blending
advantages explicit teaching explicit knowledge transfer independent
learning. addition, agent learns observation, exploit existence
multiple mentors, essentially distributing search. Finally, assume
observer knows actual actions taken mentor, mentor shares
reward function (or goals) mentor. Again, stands sharp contrast many
existing models teaching, imitation, behavior learning observation. make
strict assumptions paper respect observability, complete knowledge
reward functions, existence mappings agent state spaces, model
generalized interesting ways. elaborate generalizations near
end paper.
remainder paper structured follows. provide necessary background Markov decision processes reinforcement learning development
implicit imitation model Section 2. Section 3, describe general formal framework
study implicit imitation reinforcement learning. Two specific instantiations
framework developed. Section 4, model homogeneous agents
developed. model extraction technique explained augmented Bellman backup
proposed mechanism incorporating observations model-based reinforcement
learning algorithms. Model confidence testing introduced ensure misleading
information undue influence learners exploration policy. use
mentor observations focus attention interesting parts state space also
introduced. Section 5 develops model heterogeneous agents. model extends
homogeneous model feasibility testing, device learner detect
whether mentors abilities similar own, k-step repair, whereby learner
attempt mimic trajectory mentor cannot duplicated exactly.
techniques prove crucial heterogeneous settings. effectiveness models
demonstrated number carefully chosen navigation problems. Section 6 examines
conditions implicit imitation work well. Section 7 describes
several promising extensions model. Section 8 examines implicit imitation model
context related work Section 9 considers future work drawing
general conclusions implicit imitation field computational imitation
broadly.

2. Reinforcement Learning
aim provide formal model implicit imitation, whereby agent learn
act optimally combining experience observations behavior
expert mentor. so, describe section standard model
reinforcement learning used artificial intelligence. model build singleagent view learning act. begin reviewing Markov decision processes,
provide model sequential decision making uncertainty, move
describe reinforcement learning, emphasis model-based methods.
571

fiPrice & Boutilier

2.1 Markov Decision Processes
Markov decision processes (MDPs) proven useful modeling stochastic sequential decision problems, widely used decision-theoretic planning model
domains agents actions uncertain effects, agents knowledge environment uncertain, agent multiple, possibly conflicting objectives.
section, describe basic MDP model consider one classical solution procedure.
consider action costs formulation MDPs, though pose special
complications. Finally, make assumption full observability. Partially observable
MDPs (POMDPs) (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Smallwood &
Sondik, 1973) much computationally demanding fully observable MDPs.
imitation model based fully observable model, though generalizations model mentioned concluding section build POMDPs. refer
reader Bertsekas (1987); Boutilier, Dean Hanks (1999); Puterman (1994)
material MDPs.
MDP viewed stochastic automaton actions induce transitions
states, rewards obtained depending states visited agent.
Formally, MDP defined tuple hS, A, T, Ri, finite set states
possible worlds, finite set actions, state transition function, R reward
function. agent control state system extent performing actions
cause state transitions, movement current state new state.
Actions stochastic actual transition caused cannot generally predicted
certainty. transition function : (S) describes effects
action state. (si , a) probability distribution S; specifically, (si , a)(sj )
probability ending state sj action performed state si .
denote quantity Pr(si , a, sj ). require 0 Pr(si , a, sj ) 1
P
si , sj , si , sj Pr(si , a, sj ) = 1. components S, determine
dynamics system controlled. assumption system fully
observable means agent knows true state time (once stage
reached), decisions based solely knowledge. Thus, uncertainty lies
prediction actions effects, determining actual effect
execution.
(deterministic, stationary, Markovian) policy : describes course action
adopted agent controlling system. agent adopting policy performs
action (s) whenever finds state s. Policies form Markovian since
action choice state depend system history, stationary since
action choice depend stage decision problem. problems
consider, optimal stationary Markovian policies always exist.
assume bounded, real-valued reward function R : <. R(s) instantaneous reward agent receives occupying state s. number optimality criteria
adopted measure value policy , measuring way reward
accumulated agent traverses state space execution .
work, focus discounted infinite-horizon problems: current value reward received stages future discounted factor (0 < 1). allows simpler
572

fiImplicit Imitation

computational methods used, discounted total reward finite. Discounting
justified (e.g., economic) grounds many situations well.
value function V : < reflects value policy state s;
simply expected sum discounted future rewards obtained executing beginning
s. policy optimal if, policies , V (s) V (s).
guaranteed optimal (stationary) policies exist setting (Puterman,
1994). (optimal) value state V (s) value V (s) optimal policy .
solving MDP, refer problem constructing optimal policy. Value
iteration (Bellman, 1957) simple iterative approximation algorithm optimal policy
construction. Given arbitrary estimate V 0 true value function V , iteratively
improve estimate follows:
V n (si ) = R(si ) + max{
aA

X

Pr(si , a, sj )V n1 (sj )}

(1)

sj

computation V n (s) given V n1 known Bellman backup. sequence value
functions V n produced value iteration converges linearly V . iteration value
iteration requires O(|S|2 |A|) computation time, number iterations polynomial
|S|.
finite n, actions maximize right-hand side Equation 1 form
optimal policy, V n approximates value. Various termination criteria applied;
example, one might terminate algorithm
kV i+1 V k

(1 )
2

(2)

(where kXk = max{|x| : x X} denotes supremum norm). ensures resulting
value function V i+1 within 2 optimal function V state, induced
policy -optimal (i.e., value within V ) (Puterman, 1994).
concept useful later Q-function. Given arbitrary value
function V , define QVa (si )
QVa (si ) = R(si ) +

X

Pr(si , a, sj )V (sj )

(3)

sj

Intuitively, QVa (s) denotes value performing action state acting
manner value V (Watkins & Dayan, 1992). particular, define Qa
Q-function defined respect V , Qna Q-function defined respect
V n1 . manner, rewrite Equation 1 as:
V n (s) = max{Qna (s)}
aA

(4)

define ergodic MDP MDP every state reachable
state finite number steps non-zero probability.
573

fiPrice & Boutilier

2.2 Model-based Reinforcement Learning
One difficulty use MDPs construction optimal policy requires
agent know exact transition probabilities Pr reward model R. specification decision problem, requirements, especially detailed specification
domains dynamics, impose undue burden agents designer. Reinforcement
learning viewed solving MDP full details model, particular Pr R, known agent. Instead, agent learns act optimally
experience environment. provide brief overview reinforcement
learning section (with emphasis model-based approaches). details,
please refer texts Sutton Barto (1998) Bertsekas Tsitsiklis (1996),
survey Kaelbling, Littman Moore (1996).
general model, assume agent controlling MDP hS, A, T, Ri
initially knows state action spaces, A, transition model
reward function R. agent acts environment, stage process
makes transition hs, a, r, ti; is, takes action state s, receives reward r
moves state t. Based repeated experiences type determine optimal
policy one two ways: (a) model-based reinforcement learning, experiences
used learn true nature R, MDP solved using standard
methods (e.g., value iteration); (b) model-free reinforcement learning, experiences
used directly update estimate optimal value function Q-function.
Probably simplest model-based reinforcement learning scheme certainty equivalence approach. Intuitively, learning agent assumed current estimated
c
transition model Tb environment consisting estimated probabilities Pr(s,
a, t)
b
estimated rewards model R(s). experience hs, a, r, ti agent updates esc obtain policy
b would optimal
timated models, solves estimated MDP
estimated models correct, acts according policy.
make certainty equivalence approach precise, specific form estimated model
update procedure must adopted. common approach used empirical distribution observed state transitions rewards estimated model. instance,
action attempted C(s, a) times state s, C(s, a, t) occasions
c
state reached, estimate Pr(s,
a, t) = C(s, a, t)/C(s, a). C(s, a) = 0,
prior estimate used (e.g., one might assume state transitions equiprobable).
Bayesian approach (Dearden, Friedman, & Andre, 1999) uses explicit prior distribution
parameters transition distribution Pr(s, a, ), updates
experienced transition. instance, might assume Dirichlet (Generalized Beta)
distribution (DeGroot, 1975) parameters n(s, a, t) associated possible successor state t. Dirichlet parameters equal experience-based counts C(s, a, t)
plus prior count P (s, a, t) representing agents prior beliefs distribution
(i.e., n(s, a, t) = C(s, a, t) + P (s, a, t)). expected transition probability Pr(s, a, t)
P
c solved
n(s, a, t)/ t0 n(s, a, t0 ). Assuming parameter independence, MDP
using expected values. Furthermore, model updated ease, simply
increasing n(s, a, t) one observation hs, a, r, ti. model advantage
counter-based approach allowing flexible prior model generally
574

fiImplicit Imitation

assign probability zero unobserved transitions. adopt Bayesian perspective
imitation model.
One difficulty certainty equivalence approach computational burden rec update models Tb R
b (i.e., experience). One
solving MDP
could circumvent extent batching experiences updating (and re-solving)
model periodically. Alternatively, one could use computational effort judiciously
apply Bellman backups states whose values (or Q-values) likely change
given change model. Moore Atkesons (1993) prioritized sweeping
c
algorithm this. Tb updated changing Pr(s,
a, t), Bellman backup
b
b a). Suppose
applied update estimated value V , well Q-value Q(s,
b
b
magnitude change V (s) given V (s). predecessor w, Q-values
b
c
Q(w,
a0 )hence values Vb (w)can change Pr(w,
a0 , s) > 0. magnitude change
c
bounded Pr(w,
a0 , s)Vb (s). predecessors w placed priority
0
c
queue Pr(w, , s)Vb (s) serving priority. fixed number Bellman backups
applied states order appear queue. backup,
change value cause new predecessors inserted queue. way,
computational effort focused states Bellman backup greatest
impact due model change. Furthermore, backups applied subset
states, generally applied fixed number times. way contrast,
certainty equivalence approach, backups applied convergence. Thus prioritized
sweeping viewed specific form asynchronous value iteration, appealing
computational properties (Moore & Atkeson, 1993).
certainty equivalence, agent acts current approximation model
correct, even though model likely inaccurate early learning process.
optimal policy inaccurate model prevents agent exploring transitions form part optimal policy true model, agent fail
find optimal policy. reason, explicit exploration policies invariably used
ensure action tried state sufficiently often. acting randomly (assuming ergodic MDP), agent assured sampling action state infinitely
often limit. Unfortunately, actions agent fail exploit (in fact,
completely uninfluenced by) knowledge optimal policy. explorationexploitation tradeoff refers tension trying new actions order find
environment executing actions believed optimal basis
current estimated model.
common method exploration greedy method agent
chooses random action fraction time, 0 < < 1. Typically, decayed
time increase agents exploitation knowledge. Boltzmann approach,
action selected probability proportional value:
Prs (a) = P

eQ(s,a)/
e
a0

Q(s,a0 )/

(5)

proportionality adjusted nonlinearly temperature parameter .
0 probability selecting action highest value tends 1. Typically,
started high actions randomly explored early stages learning.
agent gains knowledge effects actions value effects,
575

fiPrice & Boutilier

parameter decayed agent spends time exploiting actions known
valuable less time randomly exploring actions.
sophisticated methods attempt use information model confidence
value magnitudes plan utility-maximizing exploration plan. early approximation
scheme found interval estimation method (Kaelbling, 1993). Bayesian
methods also used calculate expected value information gained
exploration (Meuleau & Bourgine, 1999; Dearden et al., 1999).
concentrate paper model-based approaches reinforcement learning.
However, point model-free methodsthose estimate
optimal value function Q-function learned directly, without recourse domain
modelhave attracted much attention. example, TD-methods (Sutton, 1988)
Q-learning (Watkins & Dayan, 1992) proven among popular
methods reinforcement learning. methods modified deal model-free
approaches, discuss concluding section. also focus so-called tablebased (or explicit) representations models value functions. state action
spaces large, table-based approaches become unwieldy, associated algorithms
generally intractable. situations, approximators often used estimate
values states. discuss ways techniques extended allow
function approximation concluding section.

3. Formal Framework Implicit Imitation
model influence mentor agent decision process learning
behavior observer, must extend single-agent decision model MDPs account
actions objectives multiple agents. section, introduce formal
framework studying implicit imitation. begin introducing general model
stochastic games (Shapley, 1953; Myerson, 1991), impose various assumptions
restrictions general model allow us focus key aspects implicit
imitation. note framework proposed useful study forms
knowledge transfer multiagent systems, briefly point various extensions
framework would permit implicit imitation, forms knowledge transfer,
general settings.
3.1 Non-Interacting Stochastic Games
Stochastic games viewed multiagent extension Markov decision processes.
Though Shapleys (1953) original formulation stochastic games involved zero-sum (fully
competitive) assumption, various generalizations model proposed allowing
arbitrary relationships agents utility functions (Myerson, 1991).2 Formally,
n-agent stochastic game hS, {Ai : n}, T, {Ri : n}i comprises set n agents
(1 n), set states S, set actions Ai agent i, state transition function
, reward function Ri agent i. Unlike MDP, individual agent actions
determine state transitions; rather joint action taken collection agents
determines system evolves point time. Let = A1
2. example, see fully cooperative multiagent MDP model proposed Boutilier (1999).

576

fiImplicit Imitation

set joint actions; : (S), (si , a)(sj ) = Pr(si , a, sj ) denoting
probability ending state sj joint action performed state si .
convenience, introduce notation Ai denote set joint actions A1
Ai1 Ai+1 involving agents except i. use ai ai denote
(full) joint action obtained conjoining ai Ai ai Ai .
interests individual agents may odds, strategic reasoning
notions equilibrium generally involved solution stochastic games.
aim study reinforcement agent might learn observing behavior
expert mentor, wish restrict model way strategic interactions need
considered: want focus settings actions observer
mentor interact. Furthermore, want assume reward functions
agents conflict way requires strategic reasoning.
define noninteracting stochastic games appealing notion agent projection function used extract agents local state underlying game.
games, agents local state determines aspects global state relevant
decision making process, projection function determines global states
identical agents local perspective. Formally, agent i, assume local
state space Si , projection function Li : Si . s, S, write
iff Li (s) = Li (t). equivalence relation partitions set equivalence classes
elements within specific class (i.e., L1
(s) Si ) need
distinguished agent purposes individual decision making. say stochastic
game noninteracting exists local state space Si projection function Li
agent that:
1. t, ai Ai , ai Ai , wi Si
X

{Pr(s, ai ai , w) : w L1
(wi )} =

X

{Pr(t, ai ai , w) : w L1
(wi )}

2. Ri (s) = Ri (t)
Intuitively, condition 1 imposes two distinct requirements game
perspective agent i. First, ignore existence agents, provides notion
state space abstraction suitable agent i. Specifically, Li clusters together states
state equivalence class identical dynamics respect
abstraction induced Li . type abstraction form bisimulation
type studied automaton minimization (Hartmanis & Stearns, 1966; Lee & Yannakakis,
1992) automatic abstraction methods developed MDPs (Dearden & Boutilier, 1997;
Dean & Givan, 1997). hard showignoring presence agentsthat
underlying system Markovian respect abstraction (or equivalently, w.r.t.
Si ) condition 1 met. quantification ai imposes strong noninteraction
requirement, namely, dynamics game perspective agent
independent strategies agents. Condition 2 simply requires
states within given equivalence class agent reward agent i.
means states within class need distinguishedeach local state viewed
atomic.
577

fiPrice & Boutilier

noninteracting game induces MDP Mi agent Mi = hSi , Ai , Pri , Ri
Pri given condition (1) above. Specifically, si , ti Si :
Pri (si , ai , ti ) =

P

{Pr(s, ai .ai , t) : L1
(ti )}

state L1
(si ) ai element Ai . Let : Sa Ai
optimal policy Mi . extend strategy iG : Ai underlying
stochastic game simply applying (si ) every state Li (s) = si .
following proposition shows term noninteracting indeed provides appropriate
description game.
Proposition 1 Let G noninteracting stochastic game, Mi induced MDP agent
i, optimal policy Mi . strategy iG extending G dominant
agent i.
Thus agent solve noninteracting game abstracting away irrelevant aspects state space, ignoring agent actions, solving personal MDP
Mi .
Given arbitrary stochastic game, generally quite difficult discover whether
noninteracting, requiring construction appropriate projection functions.
follows, simply assume underlying multiagent system noninteracting
game. Rather specifying game projection functions, specify individual MDPs Mi themselves. noninteracting game induced set individual
MDPs simply cross product individual MDPs. view often quite
natural. Consider example three robots moving two-dimensional office domain. able neglect possibility interactionfor example, robots
occupy 2-D position (at suitable level granularity) require
resources achieve tasksthen might specify individual MDP
robot. local state might determined robots x, y-position, orientation,
status tasks. global state space would cross product S1 S2 S3
local spaces. individual components joint action would affect
local state, agent would care (through reward function Ri ) local
state.
note projection function Li viewed equivalent observation function. assume agent distinguish elements Si
fact, observations agents states crucial imitation. Rather existence
Li simply means that, point view decision making known model,
agent need worry distinctions made Li . Assuming
computational limitations, agent need solve Mi , may use observations
agents order improve knowledge Mi dynamics.3
3.2 Implicit Imitation
Despite independent nature agent subprocesses noninteracting multiagent system, circumstances behavior one agent may relevant
3. elaborate condition computational limitations below.

578

fiImplicit Imitation

another. keep discussion simple, assume existence expert mentor agent
m, implementing stationary (and presumably optimal) policy
local MDP Mm = hSm , , Prm , Rm i. also assume second agent o, observer,
local MDP Mo = hSo , Ao , Pro , Ro i. nothing mentors behavior relevant
observer knows MDP (and solve without computational difficulty),
situation quite different reinforcement learner without complete knowledge model Mo . may well observed behavior mentor provides
valuable information observer quest learn act optimally within Mo .
take extreme case, mentors MDP identical observers, mentor
expert (in sense acting optimally), behavior mentor indicates
exactly observer do. Even mentor acting optimally,
mentor observer different reward functions, mentor state transitions observed
learner provide valuable information dynamics domain.
Thus see one agent learning act, behavior another
potentially relevant learner, even underlying multiagent system noninteracting. Similar remarks, course, apply case observer knows MDP
Mo , computational restrictions make solving difficultobserved mentor transitions
might provide valuable information focus computational effort.4 main
motivation underlying model implicit imitation behavior expert
mentor provide hints appropriate courses action reinforcement learning
agent.
Intuitively, implicit imitation mechanism learning agent attempts
incorporate observed experience expert mentor agent learning process.
Like classical forms learning imitation, learner considers effects
mentors action (or action sequence) context. Unlike direct imitation, however,
assume learner must physically attempt duplicate mentors
behavior, assume mentors behavior necessarily appropriate
observer. Instead, influence mentor agents transition model
estimate value various states actions. elaborate points below.
follows, assume mentor associated MDP Mm , learner
observer associated MDP Mo , described above. MDPs fully observable.
focus reinforcement learning problem faced agent o. extension multiple
mentors straightforward discussed below, clarity assume one
mentor description abstract framework. clear certain conditions must
met observer extract useful information mentor. list number
assumptions make different points development model.
Observability: must assume learner observe certain aspects mentors behavior. work, assume state mentors MDP fully
observable learner. Equivalently, interpret full observability
underlying noninteracting game, together knowledge mentors projection
4. instance, algorithms like asynchronous dynamic programming prioritized sweeping benefit
guidance. Indeed, distinction reinforcement learning solving MDPs viewed
rather blurry (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996). focus
case unknown model (i.e., classical reinforcement learning problem) opposed one
computational issues key.

579

fiPrice & Boutilier

function Lm . general partially observable model would require specification observation signal set Z observation function : Sm (Z),
O(so , sm )(z) denotes probability observer obtains signal z
local states observer mentor sm , respectively.
pursue model here. important note assume
observer access action taken point time. Since actions
stochastic, state (even fully observable) results mentor invoking
specific control signal generally insufficient determine signal. Thus seems
much reasonable assume states (and transitions) observable
actions gave rise them.
Analogy: observer mentor acting different local state spaces, clear
observations made mentors state transitions offer useful information
observer unless relationship two state spaces.
several ways relationship specified. Dautenhahn Nehaniv
(1998) use homomorphism define relationship mentor observer
specific family trajectories (see Section 8 discussion).
slightly different notion might involve use analogical mapping h : Sm
observed state transition provides information
observer dynamics value state h(s) . certain circumstances,
might require mapping h homomorphic respect Pr(, a, ) (for some,
all, a), perhaps even respect R. discuss issues detail
below. order simplify model avoid undue attention (admittedly
important) topic constructing suitable analogical mappings, simply assume
mentor observer identical state spaces; is, Sm
sense isomorphic. precise sense spaces isomorphicor
cases, presumed isomorphic proven otherwiseis elaborated
discuss relationship agent abilities. Thus point
simply refer state without distinguishing mentors local space Sm
observers .
Abilities: Even mapping states, observations mentors state transitions
tell observer something mentors abilities, own. must
assume observer way duplicate actions taken mentor
induce analogous transitions local state space. words, must
presumption mentor observer similar abilities.
sense analogical mapping state spaces taken
homomorphism. Specifically, might assume mentor observer
actions available (i.e., = Ao = A) h : Sm
homomorphic respect Pr(, a, ) A. requirement
weakened substantially, without diminishing utility, requiring
observer able implement actions actually taken mentor given
state s. Finally, might observer assumes duplicate
actions taken mentor finds evidence contrary. case,
presumed homomorphism state spaces. follows,
distinguish implicit imitation homogeneous action settingsdomains
580

fiImplicit Imitation

analogical mapping indeed homomorphicand heterogeneous action
settingswhere mapping may homomorphism.
general ways defining similarity ability, example, assuming
observer may able move state space similar fashion
mentor without following trajectories (Nehaniv & Dautenhahn, 1998).
instance, mentor may way moving directly key locations state
space, observer may able move analogous locations less
direct fashion. case, analogy states may determined
single actions, rather sequences actions local policies. suggest
ways dealing restricted forms analogy type Section 5.
Objectives: Even observer mentor similar identical abilities,
value observer information gleaned mentor may depend
actual policy implemented mentor. might suppose
closely related mentors policy optimal policy observer,
useful information be. Thus, extent, expect
closely aligned objectives mentor observer are, valuable
guidance provided mentor. Unlike existing teaching models,
suppose mentor making explicit efforts instruct observer.
objectives may identical, force observer
(attempt to) explicitly imitate behavior mentor. general, make
explicit assumptions relationship objectives mentor
observer. However, see that, extent, closer are,
utility derived implicit imitation.
Finally, remark important assumption make throughout remainder
paper: observer knows reward function Ro ; is, state s,
observer evaluate Ro (s) without visited state s. view consistent
view reinforcement learning automatic programming. user may easily
specify reward function (e.g., form set predicates evaluated
state) prior learning. may difficult specify domain model
optimal policy. setting, unknown component MDP Mo
transition function Pro . believe approach reinforcement learning is,
fact, common practice approach reward function must
sampled.
reiterate, aim describe mechanism observer accelerate
learning; emphasize position implicit imitationin contrast explicit
imitationis merely replicating behaviors (or state trajectories) observed another
agent, even attempting reach similar states. believe agent must learn
capabilities adapt information contained observed behavior
these. Agents must also explore appropriate application (if any) observed behaviors,
integrating own, appropriate, achieve ends. therefore
see imitation interactive process behavior one agent used guide
learning another.
581

fiPrice & Boutilier

Given setting, list possible ways observer mentor (and
cannot) interact, contrasting along way perspective assumptions
existing models literature.5 First, observer could attempt directly infer
policy observations mentor state-action pairs. model conceptual
simplicity intuitive appeal, forms basis behavioral cloning paradigm
(Sammut, Hurst, Kedzier, & Michie, 1992; Urbancic & Bratko, 1994). However, assumes
observer mentor share reward function action capabilities.
also assumes complete unambiguous trajectories (including action choices)
observed. related approach attempts deduce constraints value function
inferred action preferences mentor agent (Utgoff & Clouse, 1991; Suc & Bratko,
1997). Again, however, approach assumes congruity objectives. model also
distinct models explicit teaching (Lin, 1992; Whitehead, 1991b): assume
mentor incentive move environment way explicitly
guides learner explore environment action space effectively.
Instead trying directly learn policy, observer could attempt use observed
state transitions agents improve environment model Pro (s, a, t).
accurate model reward function, observer could calculate
accurate values states. state values could used guide agent towards
distant rewards reduce need random exploration. insight forms core
implicit imitation model. approach developed literature,
appropriate conditions listed above, specifically, conditions
mentors actions unobservable, mentor observer different reward
functions objectives. Thus, approach applicable general conditions
many existing models imitation learning teaching.
addition model information, mentors may also communicate information
relevance irrelevance regions state space certain classes reward functions.
observer use set states visited mentor heuristic guidance
perform backup computations state space.
next two sections, develop specific algorithms insights
agents use observations others improve models assess
relevance regions within state spaces. first focus homogeneous action
case, extend model deal heterogeneous actions.

4. Implicit Imitation Homogeneous Settings
begin describing implicit imitation homogeneous action settingsthe extension
heterogeneous settings build insights developed section. develop
technique called implicit imitation observations mentor used
accelerate reinforcement learning. First, define homogeneous setting.
develop implicit imitation algorithm. Finally, demonstrate implicit imitation
works number simple problems designed illustrate role various mechanisms describe.
5. describe models detail Section 8.

582

fiImplicit Imitation

4.1 Homogeneous Actions
homogeneous action setting defined follows. assume single mentor
observer o, individual MDPs Mm = hS, , Prm , Rm Mo = hS, Ao , Pro , Ro i,
respectively. Note agents share state space (more precisely, assume
trivial isomorphic mapping allows us identify local states). also assume
mentor executing stationary policy . often treat policy
deterministic, remarks apply stochastic policies well. Let support
set Supp(m , s) state set actions accorded nonzero probability
state s. assume observer abilities mentor
following sense: s, S, Supp(m , s), exists action ao Ao
Pro (s, ao , t) = Prm (s, , t). words, observer able duplicate (in sense
inducing distribution successor states) actual behavior mentor;
equivalently, agents local state spaces isomorphic respect actions
actually taken mentor subset states actions might taken.
much weaker requiring full homomorphism Sm . course,
existence full homomorphism sufficient perspective; results
require this.
4.2 Implicit Imitation Algorithm
implicit imitation algorithm understood terms component processes.
First, extract action models mentor. integrate information
observers value estimates augmenting usual Bellman backup mentor
action models. confidence testing procedure ensures use augmented
model observers model mentor reliable observers model
behavior. also extract occupancy information observations mentor
trajectories order focus observers computational effort (to extent) specific
parts state space. Finally, augment action selection process choose actions
explore high-value regions revealed mentor. remainder section
expands upon processes fit together.
4.2.1 Model Extraction
information available observer quest learn act optimally
divided two categories. First, action takes, receives experience tuple
hs, a, r, ti; fact, often ignore sampled reward r, since assume reward
function R known advance. standard model-based learning, experience
used update transition model Pro (s, a, ).
Second, mentor transition, observer obtains experience tuple hs, ti.
Note observer direct access action taken mentor,
induced state transition. Assume mentor implementing deterministic,
stationary policy , (s) denoting mentors choice action state s.
policy induces Markov chain Prm (, ) S, Prm (s, t) = Pr(s, (s), t) denoting
583

fiPrice & Boutilier

probability transition t.6 Since learner observes mentors state
c chain: Pr
c (s, t) simply estimated
transitions, construct estimate Pr
relative observed frequency mentor transitions (w.r.t. transitions taken
s). observer prior possible mentor transitions, standard Bayesian
update techniques used instead. use term model extraction process
estimating mentors Markov chain.
4.2.2 Augmented Bellman Backups
c mentors Markov chain.
Suppose observer constructed estimate Pr
homogeneity assumption, action (s) replicated exactly observer
state s. Thus, policy can, principle, duplicated observer (were able
identify actual actions used). such, define value mentors policy
observers perspective:

Vm (s) = Ro (s) +

X

Prm (s, t)Vm (t)

(6)

tS

Notice Equation 6 uses mentors dynamics observers reward function.
Letting V denote optimal (observers) value function, clearly V (s) Vm (s), Vm
provides lower bound observers value function.
importantly, terms making Vm (s) integrated directly Bellman equation observers MDP, forming augmented Bellman equation:
(

(

V (s) = Ro (s) + max max
aAo

X

)

Pro (s, a, t)V (t) ,

tS

X

)

Prm (s, t)V (t)

(7)

tS

usual Bellman equation extra term added, namely, second
P
summation, tS Prm (s, t)V (t) denoting expected value duplicating mentors
action . Since (unknown) action identical one observers actions,
term redundant augmented value equation valid. course, observer using
augmented backup operation must rely estimates quantities. observer
exploration policy ensures state visited infinitely often, estimates Pro
terms converge true values. mentors policy ergodic state space S,
Prm also converge true value. mentors policy restricted subset
states 0 (those forming basis Markov chain), estimates Prm
subset converge correctly respect 0 chain ergodic. states
0 remain unvisited estimates remain uninformed data. Since
mentors policy control observer, way observer
influence distribution samples attained Prm . observer must therefore
able reason accuracy estimated model Prm restrict
application augmented equation states Prm known sufficient
accuracy.
6. somewhat imprecise, since initial distribution Markov chain unknown.
purposes, dynamics relevant observer, transition probabilities
used.

584

fiImplicit Imitation

Prm cannot used indiscriminately, argue highly informative
early learning process. Assuming mentor pursuing optimal policy (or
least behaving way tends visit certain states frequently),
many states observer much accurate estimates Prm (s, t)
Pro (s, a, t) specific a. Since observer learning, must explore
state spacecausing less frequent visits sand action spacethus spreading
experience actions a. generally ensures sample size upon
Prm based greater Pro action forms part mentors
policy. Apart accurate, use Prm (s, t) often give informed
value estimates state s, since prior action models often flat uniform,
become distinguishable given state observer sufficient experience state
s.
note reasoning holds even mentor implementing (stationary) stochastic policy (since expected value stochastic policy fully-observable
MDP cannot greater optimal deterministic policy). direction offered mentor implementing deterministic policy tends focused,
empirically found mentors offer broader guidance moderately stochastic
environments implement stochastic policies, since tend visit
state space. note extension multiple mentors straightforwardeach
mentor model incorporated augmented Bellman equation without difficulty.
4.2.3 Model Confidence
mentors Markov chain ergodic, mixing rate7 sufficiently low,
mentor may visit certain state relatively infrequently. estimated mentor transition
model corresponding state rarely (or never) visited mentor may provide
misleading estimatebased small sample prior mentors chainof
value mentors (unknown) action s; since mentors policy
control observer, misleading value may persist extended period. Since
augmented Bellman equation consider relative reliability mentor
observer models, value state may overestimated;8 is, observer
tricked overvaluing mentors (unknown) action, consequently overestimating
value state s.
overcome this, incorporate estimate model confidence augmented
backups. mentors Markov chain observers action transitions,
assume Dirichlet prior parameters multinomial distributions
(DeGroot, 1975). reflect observers initial uncertainty possible transition probabilities. sample counts mentor observer transitions, update
distributions. information, could attempt perform optimal Bayesian
estimation value function; sample counts small (and normal approximations appropriate), simple, closed form expression resultant
distributions values. could attempt employ sampling methods, in7. mixing rate refers quickly Markov chain approaches stationary distribution.
8. Note underestimates based considerations problematic, since augmented Bellman
equation reduces usual Bellman equation.

585

fiPrice & Boutilier

V


VO-

VO VM

Figure 1: Lower bounds action values incorporate uncertainty penalty
terest simplicity employed approximate method combining information
sources inspired Kaelblings (1993) interval estimation method.
Let V denote current estimated augmented value function, Pro Prm denote
2 denote variance
estimated observer mentor transition models. let o2
model parameters.
augmented Bellman backup respect V using confidence testing proceeds
follows. first compute observers optimal action ao based estimated
augmented values observers actions. Let Q(ao , s) = Vo (s) denote value.
best action, use model uncertainty encoded Dirichlet distribution
construct lower bound Vo (s) value state observer using model
(at state s) derived behavior (i.e., ignoring observations mentor).
employ transition counts (s, a, t) nm (s, t) denote number times
observer made transition state state action performed,
number times mentor observed making transition state
t, respectively. counts, estimate uncertainty model using
P
variance Dirichlet distribution. Let = (s, a, t) = t0 St (s, a, t0 ).
model variance is:
2
model
(s, a, t) =

( +

)2

+
+ ( + + 1)

(8)

variance Q-value action due uncertainty local model
found simple application rule combining linear combinations variances, V ar(cX + dY ) = c2 V ar(X) + d2 V ar(Y ) expression Bellman backup,
P
V ar(R(s) + P r(t|s, a)V (t). result is:
2 (s, a) = 2

X


2
model
(s, a, t)v(t)2

(9)

Using Chebychevs inequality,9 obtain confidence level even though Dirichlet
distributions small sample counts highly non-normal. lower bound
Vo (s) = Vo (s)co (s, ao ) suitable constant c. One may interpret penalizing
9. Chebychevs inequality states 1 k12 probability mass arbitrary distribution
within k standard deviations mean.

586

fiImplicit Imitation

2
2
FUNCTION augmentedBackup( V ,Pro ,omodel
,Prm ,mmodel
,s,c)

= arg maxaAo

P
tS

Pr(s, a, t)V (t)

P

Vo (s) = Ro (s) + PtS Pro (s, , t)V (t)
Vm (s) = Ro (s) + tS Prm (s, t)V (t)

P

2
2

o2 (s, ) =
(t)2
P tS2 omodel (s, , t)V
2
2
2
(s) =

(s, t)V (t)
tS mmodel

Vo (s) = Vo (s) c (s, )

Vm
(s) = Vm (s) c (s)
Vo (s) > Vm (s)
V (s) = Vo (s)
ELSE
V (s) = Vm (s)
END

Table 1: Implicit Backup
value state subtracting uncertainty (see Figure 1).10 value
Vm (s) mentors action (s) estimated similarly analogous lower bound
Vm (s) also constructed. Vo (s) > Vm (s), say Vo (s) supersedes
Vm (s) write Vo (s) Vm (s). Vo (s) Vm (s) either mentor-inspired
model has, fact, lower expected value (within specified degree confidence)
uses nonoptimal action (from observers perspective), mentor-inspired model
lower confidence. either case, reject information provided mentor
use standard Bellman backup using action model derived solely observers
experience (thus suppressing augmented backup)the backed value Vo (s)
case.
algorithm computing augmented backup using confidence test shown
Table 1. algorithm parameters include current estimate augmented value
2
function V , current estimated model Pro associated local variance omodel
,
2
model mentors Markov chain Prm associated variance mmodel .
calculates lower bounds returns mean value, Vo Vm , greatest lower
bound. parameter c determines width confidence interval used mentor
rejection test.
4.2.4 Focusing
augmented Bellman backups improves accuracy observers model. second
way observer exploit observations mentor focus attention
states visited mentor. model-based approach, specific focusing mecha10. Ideally, would like take uncertainty model current state account,
also uncertainty future states well (Meuleau & Bourgine, 1999).

587

fiPrice & Boutilier

nism adopt require observer perform (possibly augmented) Bellman backup
state whenever mentor makes transition s. three effects. First,
mentor tends visit interesting regions space (e.g., shares certain reward structure observer), significant values backed mentor-visited states
bias observers exploration towards regions. Second, computational effort
c (s, t) changes,
concentrated toward parts state space estimated model Pr
hence estimated value one observers actions may change. Third,
computation focused model likely accurate (as discussed above).
4.2.5 Action Selection
integration exploration techniques action selection policy important
reinforcement learning algorithm guarantee convergence. implicit imitation, plays
second, crucial role helping agent exploit information extracted mentor.
improved convergence results rely greedy quality exploration strategy
bias observer towards higher-valued trajectories revealed mentor.
expediency, adopted -greedy action selection method, using exploration rate decays time. could easily employed semi-greedy
methods Boltzmann exploration. presence mentor, greedy action selection becomes complex. observer examines actions state usual
way obtains best action ao corresponding value Vo (s). value also
calculated mentors action Vm (s). Vo (s) Vm (s), observers action
model used greedy action defined exactly mentor present.
If, however, Vm (s) Vo (s) would like define greedy action action
dictated mentors policy state s. Unfortunately, observer know
action is, define greedy action observers action closest
mentors action according observers current model estimates s. precisely,
action similar mentors state s, denoted (s), whose outcome
distribution minimum Kullback-Leibler divergence mentors action outcome
distribution:
(

(s) = argmina

X

)

Pro (s, a, t) log Prm (s, t)

(10)



observers experience-based action models poor early training,
chance closest action computation select wrong action. rely
exploration policy ensure observers actions sampled appropriately
long run.11
present work assumed state space large agent
therefore able completely update Q-function whole space. (The
intractability updating entire state space one motivations using imitation
techniques). absence information states true values, would like
bias value states along mentors trajectories look worthwhile
explore. assuming bounds reward function setting initial Qvalues entire space bound. simple examples, rewards strictly
11. mentor executing stochastic policy, test based KL-divergence mislead learner.

588

fiImplicit Imitation

positive set bounds zero. mentor trajectories intersect states valued
observing agent, backups cause states trajectories higher
value surrounding states. causes greedy step exploration method
prefer actions lead mentor-visited states actions agent
information.
4.2.6 Model Extraction Specific Reinforcement Learning Algorithms
Model extraction, augmented backups, focusing mechanism, extended notion
greedy action selection, integrated model-based reinforcement learning algorithms relative ease. Generically, implicit imitation algorithm requires that: (a)
c (s, t) Markov chain induced mentors
observer maintain estimate Pr
policythis estimate updated every observed transition; (b) backups
performed estimate value function use augmented backup (Equation 7) confic (s, a, t)
dence testing. course, backups implemented using estimated models Pr
c
Prm (s, t). addition, focusing mechanism requires augmented backup
performed state visited mentor.
demonstrate generality mechanisms combining wellknown efficient prioritized sweeping algorithm (Moore & Atkeson, 1993). outlined
c
Section 2.2, prioritized sweeping works maintaining estimated transition model Pr
b Whenever experience tuple hs, a, r, ti sampled, estimated
reward model R.
model state change; Bellman backup performed incorporate revised
model (usually fixed) number additional backups performed selected
states. States selected using priority estimates potential change values
based changes precipitated earlier backups. Effectively, computational resources
(backups) focused states benefit backups.
Incorporating ideas prioritized sweeping simply requires following changes:
c (s, a, t) transition hs, a, ti observer takes, estimated model Pr
dated augmented backup performed state s. Augmented backups
performed fixed number states using usual priority queue implementation.
c (s, t) updated
observed mentor transition hs, ti, estimated model Pr
augmented backup performed s. Augmented backups performed
fixed number states using usual priority queue implementation.

Keeping samples mentor behavior implements model extraction. Augmented backups
integrate information observers value function, performing augmented
backups observed transitions (in addition experienced transitions) incorporates
focusing mechanism. observer forced follow otherwise mimic actions
mentor directly. back value information along mentors trajectory
had. Ultimately, observer must move states discover actions
used; meantime, important value information propagated
guide exploration.
Implicit imitation alter long run theoretical convergence properties
underlying reinforcement learning algorithm. implicit imitation framework orthogonal -greedy exploration, alters definition greedy action,
589

fiPrice & Boutilier

greedy action taken. Given theoretically appropriate decay factor, -greedy
strategy thus ensure distributions action models state
sampled infinitely often limit converge true values. Since extracted
model mentor corresponds one observers actions, effect
value function calculations different effect observers sampled
action models. confidence mechanism ensures model samples
eventually come dominate is, fact, better. therefore sure convergence properties reinforcement learning implicit imitation identical
underlying reinforcement learning algorithm.
benefit implicit imitation lies way models extracted
mentor allow observer calculate lower bound value function use
lower bound choose greedy actions move agent towards higher-valued regions
state space. result quicker convergence optimal policies better short-term
practical performance respect accumulated discounted reward learning.
4.2.7 Extensions
implicit imitation model easily extended extract model information
multiple mentors, mixing matching pieces extracted mentor achieve good
results. searching, state, set mentors knows find
mentor highest value estimate. value estimate best mentor
compared using confidence test described observers value estimate.
formal expression algorithm given multi-augmented Bellman equation:
(

(

V (s) = Ro (s) + max max
aAo

max

mM

X

X

)

Pro (s, a, t)V (t)

,

tS

)

Prm (s, t)V (t)

(11)

tS

set candidate mentors. Ideally, confidence estimates taken
account comparing mentor estimates other, may get mentor
high mean value estimate large variance. observer experience
state all, mentor likely rejected poorer quality information
observer already experience. observer might
better picking mentor lower mean confident estimate would
succeeded test observers model. interests simplicity,
however, investigate multiple mentor combination without confidence testing.
now, assumed action costs (i.e., agents rewards depend
state action selected state); however, use general
reward functions (e.g., reward form R(s, a)). difficulty lies backing
action costs mentors chosen action unknown. Section 4.2.5 defined
closest action function . function used choose appropriate reward.
augmented Bellman equation generalized rewards takes following form:
(

(

V (s) = max max Ro (s, a) +
aAo

X
tS

590

)

Pro (s, a, t)V (t)

,

fiImplicit Imitation

Ro (s, (s)) +

X

)

Prm (s, t)V (t)

tS

note Bayesian methods could used could used estimate action costs
mentors chain well. case, generalized reward augmented equation
readily amended use confidence estimates similar fashion transition model.
4.3 Empirical Demonstrations
following empirical tests incorporate model extraction focusing mechanism
prioritized sweeping. results illustrate types problems scenarios
implicit imitation provide advantages reinforcement learning agent.
experiments, expert mentor introduced experiment serve model
observer. case, mentor following -greedy policy small (on
order 0.01). tends cause mentors trajectories lie within cluster
surrounding optimal trajectories (and reflect good optimal policies). Even
small amount exploration environment stochasticity, mentors generally
cover entire state space, confidence testing important.
experiments, prioritized sweeping used fixed number backups per observed experienced sample.12 -greedy exploration used decaying .
Observer agents given uniform Dirichlet priors Q-values initialized zero. Observer agents compared control agents benefit mentors experience,
otherwise identical (implementing prioritized sweeping similar parameters
exploration policies). tests performed stochastic grid world domains, since
make clear extent observers mentors optimal policies overlap (or
fail to). Figure 2, simple 10 10 example shows start end state grid.
typical optimal mentor trajectory illustrated solid line start
end states. dotted line shows typical mentor-influenced trajectory quite
similar observed mentor trajectory. assume eight-connectivity cells
state grid nine neighbors including itself, agents four
possible actions. experiments, four actions move agent compass
directions (North, South, East West), although agent initially know
action which. focus primarily whether imitation improves performance
learning, since learner converge optimal policy whether uses imitation
not.
4.3.1 Experiment 1: Imitation Effect
first experiment compare performance observer using model extraction
expert mentor performance control agent using independent reinforcement learning. Given uniform nature grid world lack intermediate
rewards, confidence testing required. agents attempt learn policy
maximizes discounted return 10 10 grid world. start upper-left corner
seek goal value 1.0 lower-right corner. Upon reaching goal, agents
12. Generally, number backups set roughly equal length optimal noise-free
path.

591

fiPrice & Boutilier



X

Figure 2: simple grid world start state goal state X

50
Obs
FA Series
Ctrl

Average Reward per 1000 Steps

40

30

20

10

Delta

0

10

0

500

1000

1500

2000

2500

3000

3500

4000

4500

Simulation Steps

Figure 3: Basic observer control agent comparisons

restarted upper-left corner. Generally mentor follow similar identical trajectory run, mentors trained using greedy strategy leaves
one path slightly highly valued rest. Action dynamics noisy,
intended direction realized 90% time, one directions taken
otherwise (uniformly). discount factor 0.9. Figure 3, plot cumulative
number goals obtained previous 1000 time steps observer Obs
control Ctrl agents (results averaged ten runs). observer able quickly
incorporate policy learned mentor value estimates. results
steeper learning curve. contrast, control agent slowly explores space build
model first. Delta curve shows difference performance agents.
agents converge optimal value function.
592

fiImplicit Imitation

30

25

Average Reward per 1000 Steps

20

Basic
Scale

15

10
Stoch
5

0

5

0

1000

2000

3000

4000

5000

6000

Simulation Steps

Figure 4: Delta curves showing influence domain size noise

4.3.2 Experiment 2: Scaling Noise
next experiment illustrates sensitivity imitation size state space
action noise level. Again, observer uses model-extraction confidence testing.
Figure 4, plot Delta curves (i.e., difference performance observer
control agents) Basic scenario described, Scale scenario
state space size increased 69 percent (to 13 13 grid), Stoch scenario
noise level increased 40 percent (results averaged ten runs).
total gain represented area curves observer non-imitating
prioritized sweeping agent increases state space size. reflects Whiteheads
(1991a) observation grid worlds, exploration requirements increase quickly
state space size, optimal path length increases linearly. see
guidance mentor help larger state spaces.
Increasing noise level reduces observers ability act upon information
received mentor therefore erodes advantage control agent.
note, however, benefit imitation degrades gracefully increased noise
present even relatively extreme noise level.
4.3.3 Experiment 3: Confidence Testing
Sometimes observers prior beliefs transition probabilities mentor
mislead observer cause generate inappropriate values. confidence mechanism proposed previous section prevent observer fooled
misleading priors mentors transition probabilities. demonstrate role
confidence mechanism implicit imitation, designed experiment based scenario illustrated Figure 5. Again, agents task navigate top-left corner
bottom-right corner 10 10 grid order attain reward +1. cre593

fiPrice & Boutilier

+5

+5

+5

+5

Figure 5: environment misleading priors
ated pathological scenario islands high reward (+5) enclosed obstacles.
Since observers priors reflect eight-connectivity uniform, high-valued cells
middle island believed reachable states diagonally adjacent
small prior probability. reality, however, agents action set precludes
agent therefore never able realize value. four islands
scenario thus create fairly large region center space high estimated
value, could potentially trap observer persisted prior beliefs.
Notice standard reinforcement learner quickly learn none actions
take rewarding islands; contrast, implicit imitator using augmented backups
could fooled prior mentor model. mentor visit states neighboring
island, observer evidence upon change prior belief
mentor actions equally likely take one eight possible directions.
imitator may falsely conclude basis mentor action model action
exist would allow access islands value. observer therefore needs
confidence mechanism detect mentor model less reliable model.
test confidence mechanism, mentor follows path around outside
obstacles path cannot lead observer trap (i.e., provides
evidence observer diagonal moves islands feasible).
combination high initial exploration rate ability prioritized sweeping
spread value across large distances virtually guarantees observer led
trap. Given scenario, ran two observer agents control. first observer
used confidence interval width given 5, which, according Chebychev rule,
cover approximately 96 percent arbitrary distribution. second observer
given 0 interval, effectively disables confidence testing. observer
confidence testing consistently became stuck. Examination value function revealed
consistent peaks within trap region, inspection agent state trajectories showed
stuck trap. observer confidence testing consistently escaped
trap. Observation value function time shows trap formed, faded
away observer gained enough experience actions allow ignore
594

fiImplicit Imitation

45
CR Series

40

Ctrl

35

Average Reward per 1000 Steps

Obs
30

25

20

15

10

5

0

5

Delta

0

2000

4000

6000

8000

10000

12000

Simulation Steps

Figure 6: Misleading priors may degrade performance
overcome erroneous priors mentor actions. Figure 6, performance
observer confidence testing shown performance control agent (results
averaged 10 runs). see observers performance slightly degraded
unaugmented control agent even pathological case.
4.3.4 Experiment 4: Qualitative Difficulty
next experiment demonstrates potential gains imitation increase
(qualitative) difficulty problem. observer employs model extraction
confidence testing, though confidence testing play significant role here.13
maze scenario, introduce obstacles order increase difficulty learning
problem. maze set 25 25 grid (Figure 7) 286 obstacles complicating
agents journey top-left bottom-right corner. optimal solution takes
form snaking 133-step path, distracting paths (up length 22) branching
solution path necessitating frequent backtracking. discount factor 0.98.
10 percent noise, optimal goal-attainment rate six goals per 1000 steps.
graph Figure 8 (with results averaged ten runs), see control
agent takes order 200,000 steps build decent value function reliably leads
goal. point, achieving four goals per 1000 steps average,
exploration rate still reasonably high (unfortunately, decreasing exploration quickly
leads slower value function formation). imitation agent able take advantage
mentors expertise build reliable value function 20,000 steps. Since
control agent unable reach goal first 20,000 steps, Delta
control imitator simply equal imitators performance.
13. mentor provide evidence path choices problem,
intermediate rewards would cause observer make use misleading mentor priors
states.

595

fiPrice & Boutilier

Figure 7: complex maze
7

CMB Series
6

Average Reward per 1000 Steps

5

4

3

Obs

2

Delta

1

0

0

Ctrl

0.5

1

1.5

2

2.5
5

x 10

Simulation Steps

Figure 8: Imitation complex space
imitator quickly achieve optimal goal attainment rate six goals per 1000 steps,
exploration rate decays much quickly.
4.3.5 Experiment 5: Improving Suboptimal Policies Imitation
augmented backup rule require reward structure mentor
observer identical. many useful scenarios rewards dissimilar
value functions policies induced share structure. experiment,
demonstrate one interesting scenario relatively easy find suboptimal
solution, difficult find optimal solution. observer finds suboptimal
path, however, able exploit observations mentor see
596

fiImplicit Imitation

1

4

*
*
*
*
*
*
*
*
*

*
*
*
*
*
*
*
*
*

2

3
5

Figure 9: maze perilous shortcut

shortcut significantly shortens path goal. structure scenario
shown Figure 9. suboptimal solution lies path location 1 around
scenic route location 2 goal location 3. mentor takes
vertical path location 4 location 5 shortcut.14 discourage
use shortcut novice agents, lined cells (marked *)
agent immediately jumps back start state. therefore difficult novice agent
executing random exploratory moves make way end shortcut
obtain value would reinforce future use. observer control
therefore generally find scenic route first.
Figure 10, performance (measured using goals reached previous 1000
steps) control observer compared (averaged ten runs), indicating
value observations. see observer control agent find longer
scenic route, though control agent takes longer find it. observer goes find
shortcut increases return almost double goal rate. experiment shows
mentors improve observer policies even observers goals
mentors path.
4.3.6 Experiment 6: Multiple Mentors
final experiment illustrates model extraction readily extended
observer extract models multiple mentors exploit valuable parts
each. Again, observer employs model extraction confidence testing. Figure 11,
learner must move start location 1 goal location 4. Two expert agents
different start goal states serve potential mentors. One mentor repeatedly moves
location 3 location 5 along dotted line, second mentor departs location
2 ends location 4 along dashed line. experiment, observer must
14. mentor proceeding 5 4 would provide guidance without prior knowledge actions
reversible.

597

fiPrice & Boutilier

35

CSB Series
30

Average Reward per 1000 Steps

25

Obs

20
Delta
15

10
Ctrl
5

0

0

0.5

1

1.5

2

2.5

3
4

x 10

Simulation Steps

Figure 10: Transfer non-identical rewards

1

3

2

5

4

Figure 11: Multiple mentors scenario

combine information examples provided two mentors independent
exploration order solve problem.
Figure 12, see observer successfully pulls together information
sources order learn much quickly control agent (results averaged
10 runs). see use value-based technique allows observer choose
mentors influence use state-by-state basis order get best solution
problem.
598

fiImplicit Imitation

60

Obs
CMM Series

Average Reward per 1000 Steps

50

40

Ctrl
30

20
Delta

10

0

0

1000

2000

3000

4000

5000

6000

Simulation Steps

Figure 12: Learning multiple mentors

5. Implicit Imitation Heterogeneous Settings
homogeneity assumption violated, implicit imitation framework described
cause learners convergence rate slow dramatically and, cases,
cause learner become stuck small neighborhood state space. particular,
learner unable make state transition (or transition
probability) mentor given state, may drastically overestimate value
state. inflated value estimate causes learner return repeatedly state
even though exploration never produce feasible action attains inflated
estimated value. mechanism removing influence mentors Markov
chain value estimatesthe observer extremely (and correctly) confident
observations mentors model. problem lies fact augmented
Bellman backup justified assumption observer duplicate every mentor
action. is, state s, Pro (s, a, t) = Prm (s, t)
t. equivalent action exist, guarantee value
calculated using mentor action model can, fact, achieved.
5.1 Feasibility Testing
heterogeneous settings, prevent lock-up poor convergence
use explicit action feasibility test: augmented backup performed s,
observer tests whether mentors action differs actions s, given
current estimated models. so, augmented backup suppressed standard
Bellman backup used update value function. 15 default, mentor actions
15. decision binary; could envision smoother decision criterion measures extent
mentors action duplicated.

599

fiPrice & Boutilier

assumed feasible observer; however, observer reasonably confident
infeasible state s, augmented backups suppressed s.
Recall uncertainty agents true transition probabilities captured
Dirichlet distribution derived sampled transitions. Comparing ao effected
difference means test respect corresponding Dirichlets. complicated
fact Dirichlets highly non-normal small parameter values transition
distributions multinomial. deal non-normality requiring minimum
number samples using robust Chebychev bounds pooled variance
distributions compared. Conceptually, evaluate Equation 12:
r

| Pro (s, ao , t) Prm (s, t)|
2
2
(s,ao ,t)omodel
(s,ao ,t)+nm (s,t)mmodel
(s,t)
(s,ao ,t)+nm (s,t)

> Z/2

(12)

Z/2 critical value test. parameter significance test,
probability falsely reject two actions different
actually same. Given highly non-normal distributions early training process,
appropriate Z value given computed Chebychevs bound solving
2 = 1 Z12 Z/2 .
samples accurate test, persist augmented
backups (embodying default assumption homogeneity). value estimate
inflated backups, agent biased obtain additional samples,
allow agent perform required feasibility test. assumption therefore
self-correcting. deal multivariate complications performing Bonferroni
test (Seber, 1984), shown give good results practice (Mi & Sampson,
1993), efficient compute, known robust dependence variables.
Bonferroni hypothesis test obtained conjoining several single variable tests. Suppose
actions ao result r possible successor states, s1 , , sr (i.e., r transition
probabilities compare). si , hypothesis Ei denotes ao
transition probability successor state si ; Pr(s, , si ) = Pr(s, ao , si ). let
Ei denote complementary hypothesis (i.e., transition probabilities differ).
Bonferroni inequality states:
"

Pr

r
\

#

Ei 1

i=1

r
X



Pr Ei



i=1



Thus test joint hypothesis ri=1 Ei two action models sameby
testing r complementary hypotheses Ei confidence level /r. reject
hypotheses reject notion two actions equal confidence
least . mentor action deemed infeasible every observer action ao ,
multivariate Bonferroni test described rejects hypothesis action
mentors.
Pseudo-code Bonferroni component feasibility test appears Table 2.
assumes sufficient number samples. efficiency reasons, cache results
feasibility testing. duplication mentors action state first determined
infeasible, set flag state effect.
600

fiImplicit Imitation

FUNCTION feasible(m,s) : Boolean
Ao
allSuccessorProbsSimilar = true
successors(s)
= |P roq
(s, a, t) P rm (s, t)|
(s,a,t) 2

(s,a,t)+nm (s,t) 2

omodel
z = /
(s,a,t)+nm (s,t)
z > z/2r
allSuccessorProbsSimilar = false
END
allSuccessorProbsSimilar
return true
END
RETURN false

mmodel

(s,t)

Table 2: Action Feasibility Testing
5.2 k-step Similarity Repair
Action feasibility testing essentially makes strict decision whether agent
duplicate mentors action specific state: decided mentors action
infeasible, augmented backups suppressed potential guidance offered eliminated
state. Unfortunately, strictness test results somewhat impoverished
notion similarity mentor observer. This, turn, unnecessarily limits
transfer mentor observer. propose mechanism whereby mentors
influence may persist even specific action chooses feasible mentor;
instead rely possibility observer may approximately duplicate mentors
trajectory instead exactly duplicating it.
Suppose observer previously constructed estimated value function using augmented backups. Using mentor action model (i.e., mentors chain Prm (s, t)), high
value calculated state s. Subsequently, suppose mentors action state
judged infeasible. illustrated Figure 13, estimated value state
originally due mentors action (s), sake illustration moves
high probability state t, lead highly-rewarding region
state space. number experiences state s, however, learner concludes
action (s)and associated high probability transition tis feasible.
point, one two things must occur: either (a) value calculated state
predecessors collapse exploration towards highly-valued regions
beyond state ceases; (b) estimated value drops slightly exploration continues
towards highly-valued regions. latter case may arise follows. observer
previously explored vicinity state s, observers action model may
sufficiently developed still connect higher value-regions beyond state state
Bellman backups. example, learner sufficient experience
learned highly-valued region reached alternative trajectory
u v w, newly discovered infeasibility mentors transition
deleterious effect value estimate s. highly-valued, likely states
close mentors trajectory explored degree. case, state
601

fiPrice & Boutilier

Infeasible Transition



High-value
State



w
"Bridge"

u

v

Figure 13: alternative path bridge value backups around infeasible paths
highly-valued using mentors action model, still
valued highly enough likely guide exploration toward area. call
alternative (in case u v w) mentors action bridge, allows
value higher value regions flow infeasible mentor transition.
bridge formed without intention agent, call process spontaneous
bridging.
spontaneous bridge exist, observers action models generally undeveloped (e.g., close uniform prior distributions). Typically,
undeveloped models assign small probability every possible outcome therefore diffuse value higher valued regions lead poor value estimate state s.
result often dramatic drop value state predecessors;
exploration towards highly-valued region neighborhood state ceases.
example, could occur observers transition models state assign low
probability (e.g., close prior probability) moving state u due lack experience
(or similarly surrounding states, u v, insufficiently explored).
spontaneous bridging effect motivates broader notion similarity.
observer find short sequence actions bridges infeasible action
mentors trajectory, mentors example still provide extremely useful guidance.
moment, assume short path path length greater given
integer k. say observer k-step similar mentor state observer
duplicate k fewer steps mentors nominal transition state sufficiently
high probability.
Given notion similarity, observer test whether spontaneous bridge
exists determine whether observer danger value function collapse
concomitant loss guidance decides suppress augmented backup state s.
this, observer initiates reachability analysis starting state using action
model Pro (s, a, t) determine sequence actions leads sufficiently
high probability state state mentors trajectory downstream
infeasible action.16 k-step bridge already exists, augmented backups safely
suppressed state s. efficiency, maintain flag state mark bridged.
state known bridged, k-step reachability analysis need repeated.
spontaneous bridge cannot found, might still possible intentionally set
build one. build bridge, observer must explore state k-steps away,
hoping make contact mentors trajectory downstream infeasible mentor
16. general state space ergodicity lacking, agent must consider predecessors state
k steps guarantee k-step paths checked.

602

fiImplicit Imitation

action. implement single search attempt k2 -step random walk, result
trajectory average k steps away long ergodicity local connectivity
assumptions satisfied. order search occur, must motivate observer
return state engage repeated exploration. could provide motivation
observer asking observer assume infeasible action repairable.
observer therefore continue augmented backups support high-value estimates
state observer repeatedly engage exploration point.
danger, course, may fact bridge, case observer
repeat search bridge indefinitely. therefore need mechanism terminate
repair process k-step repair infeasible. could attempt explicitly keep
track possible paths open observer paths explicitly tried
observer determine repair possibilities exhausted. Instead, elect
follow probabilistic search eliminates need bookkeeping: bridge cannot
constructed within n attempts k-step random walk, repairability assumption
judged falsified, augmented backup state suppressed observers bias
explore vicinity state eliminated. bridge found state s, flag used
mark state irreparable.
approach is, course, nave heuristic strategy; illustrates basic
import bridging. systematic strategies could used, involving explicit planning
find bridge using, say, local search (Alissandrakis, Nehaniv, & Dautenhahn, 2000).
Another aspect problem address persistence search
bridges. specific domain, number unsuccessful attempts find bridges,
learner may conclude unable reconstruct mentors behavior, case
search bridges may abandoned. involves simple, higher-level inference,
notion (or prior beliefs about) similarity capabilities. notions could also
used automatically determine parameter settings (discussed below).
parameters k n must tuned empirically, estimated given knowledge connectivity domain prior beliefs similar (in terms
length average repair) trajectories mentor observer be. instance,
n > 8k 4 seems suitable 8-connected grid world low noise, based number
trajectories required cover perimeter states k-step rectangle around state.
note large values n reduce performance non-imitating
agents results temporary lock up.
Feasibility k-step repair easily integrated homogeneous implicit imitation framework. Essentially, simply elaborate conditions augmented
backup employed. course, additional representation introduced
keep track whether state feasible, bridged, repairable, many repair attempts made. action selection mechanism also overridden
bridge-building algorithm required order search bridge. Bridge building
always terminates n attempts, however, cannot affect long run convergence.
aspects algorithm, however, exploration policy, unchanged.
complete elaborated decision procedure used determine augmented backups
employed state respect mentor appears Table 3. uses
internal state make decisions. original model, first check see
observers experience-based calculation value state supersedes mentor603

fiPrice & Boutilier

FUNCTION use augmented?(s,m) : Boolean
Vo (s) Vm (s) RETURN false
ELSE f easible(s, m) RETURN true
ELSE bridged(s, m) RETURN false
ELSE reachable(s, m)
bridged(s,m) := true
RETURN false
ELSE repairable(s, m) return false
ELSE % searching
0 < search steps(s, m) < k % search progress
return true
search steps(s, m) > k % search failed
attempts(s) > n
repairable(s) = false
RETURN false
ELSE
reset search(s,m)
attempts(s) := attempts(s) + 1
RETURN true
attempts(s) :=1 % initiate first attempt search
initiate-search(s)
RETURN true

Table 3: Elaborated augmented backup test

based calculation; so, observer uses experience-based calculation.
mentors action feasible, accept value calculated using observationbased value function. action infeasible check see state bridged.
first time test requested, reachability analysis performed, results
drawn cache subsequent requests. state bridged, suppress
augmented backups, confident cause value function collapse. state
bridged, ask repairable. first n requests, agent attempt
k-step repair. repair succeeds, state marked bridged. cannot repair
infeasible transition, mark not-repairable suppress augmented backups.
may wish employ implicit imitation feasibility testing multiple-mentor
scenario. key change implicit imitation without feasibility testing
observer imitate feasible actions. observer searches set
mentors one action results highest value estimate, observer
must consider mentors whose actions still considered feasible (or assumed
repairable).
5.3 Empirical Demonstrations
section, empirically demonstrate utility feasibility testing k-step repair
show techniques used surmount differences actions
agents small local differences state-space topology. problems
604

fiImplicit Imitation

chosen specifically demonstrate necessity utility feasibility testing
k-step repair.
5.3.1 Experiment 1: Necessity Feasibility Testing
first experiment shows importance feasibility testing implicit imitation
agents heterogeneous actions. scenario, agents must navigate across
obstacle-free, 10 10 grid world upper-left corner goal location lowerright. agent reset upper-left corner. first agent mentor
NEWS action set (North, South, East, West movement actions). mentor
given optimal stationary policy problem. study performance three
learners, Skew action set (N, S, NE, SW) unable duplicate mentor
exactly (e.g., duplicating mentors E-move requires learner move NE followed
S, move SE N). Due nature grid world, control imitation
agents actually execute actions get goal mentor
optimal goal rate control imitator therefore lower
mentor. first learner employs implicit imitation feasibility testing, second uses
imitation without feasibility testing, third control agent uses imitation (i.e.,
standard reinforcement learning agent). agents experience limited stochasticity
form 5% chance action randomly perturbed. last section,
agents use model-based reinforcement learning prioritized sweeping. set k = 3
n = 20.
effectiveness feasibility testing implicit imitation seen Figure 14.
horizontal axis represents time simulation steps vertical axis represents
average number goals achieved per 1000 time steps (averaged 10 runs). see
imitation agent feasibility testing converges much quickly optimal
goal-attainment rate agents. agent without feasibility testing achieves
sporadic success early on, frequently locks due repeated attempts duplicate
infeasible mentor actions. agent still manages reach goal time time,
stochastic actions permit agent become permanently stuck obstaclefree scenario. control agent without form imitation demonstrates significant
delay convergence relative imitation agents due lack form guidance,
easily surpasses agent without feasibility testing long run. gradual
slope control agent due higher variance control agents discovery time
optimal path, feasibility-testing imitator control agent converge
optimal solutions. shown comparison two imitation agents, feasibility
testing necessary adapt implicit imitation contexts involving heterogeneous actions.
5.3.2 Experiment 2: Changes State Space
developed feasibility testing bridging primarily deal problem adapting
agents heterogeneous actions. techniques, however, applied
agents differences state-space connectivity (ultimately, equivalent
notions). test this, constructed domain agents NEWS
action set, alter environment learners introducing obstacles arent
present mentor. Figure 15, learners find mentors path obstructed
605

fiPrice & Boutilier

40

Feas

35
FS Series

Average Reward per 1000 Steps

30

25

Ctrl

20

15
NoFeas
10

5

0

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 14: Utility feasibility testing


X

Figure 15: Obstacle map mentor path
obstacles. Movement toward obstacle causes learner remain current state.
sense, action different effect mentors.
Figure 16, see results qualitatively similar previous experiment.
contrast previous experiment, imitator control use NEWS action set
therefore shortest path length mentor. Consequently,
optimal goal rate imitators control higher previous experiment.
observer without feasibility testing difficulty maze, value function
augmented mentor observations consistently leads observer states whose path
goal directly blocked. agent feasibility testing quickly discovers
mentors influence inappropriate states. conclude local differences
state well handled feasibility testing.
Next, demonstrate feasibility testing completely generalize mentors
trajectory. Here, mentor follows path completely infeasible imitating
agent. fix mentors path runs give imitating agent maze shown
606

fiImplicit Imitation

50
Feas
FO Series

45

Average Reward per 1000 Steps

40

35

Ctrl

30

25

20

15

10

5

0

NoFeas

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 16: Interpolating around obstacles



Observer
Mentor
X

Figure 17: Parallel generalization
Figure 17 two states mentor visits blocked obstacle.
imitating agent able use mentors trajectory guidance builds
parallel trajectory completely disjoint mentors.
results Figure 18 show gain imitator feasibility testing
control agent diminishes, still exists marginally imitator forced generalize
completely infeasible mentor trajectory. agent without feasibility testing
poorly, even compared control agent. gets stuck around
doorway. high value gradient backed along mentors path becomes accessible
agents doorway. imitation agent feasibility conclude cannot
proceed south doorway (into wall) try different strategy.
imitator without feasibility testing never explores far enough away doorway
setup independent value gradient guide goal. slower decay
schedule exploration, imitator without feasibility testing would find goal,
607

fiPrice & Boutilier

60

Feas
50

FP Series

Average Reward per 1000 Steps

Ctrl

40

30

20

10

NoFeas
0

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 18: Parallel generalization results

would still reduce performance imitator feasibility testing.
imitator feasibility testing makes use prior beliefs follow mentor
backup value perpendicular mentors path. value gradient therefore form
parallel infeasible mentor path imitator follow along side infeasible
path towards doorway makes necessary feasibility test proceeds
goal.
explained earlier, simple problems good chance informal effects
prior value leakage stochastic exploration may form bridges feasibility testing
cuts value propagation guides exploration. difficult problems
agent spends lot time exploring, accumulate sufficient samples conclude
mentors actions infeasible long agent constructed bridge.
imitators performance would drop unaugmented reinforcement
learner.
demonstrate bridging, devised domain agents must navigate
upper-left corner bottom-right corner, across river three steps wide
exacts penalty 0.2 per step (see Figure 19). goal state worth 1.0. figure,
path mentor shown starting top corner, proceeding along edge
river crossing river goal. mentor employs NEWS action
set. observer uses Skew action set (N, NE, S, SW) attempts reproduce
mentor trajectory. fail reproduce critical transition border
river (because East action infeasible Skew agent). mentor action
longer used backup value rewarding state alternative
paths river blocks greedy exploration region. Without bridging
optimistic lengthly exploration phase, observer agents quickly discover negative
states river curtail exploration direction actually making across.
608

fiImplicit Imitation

Figure 19: River scenario
examine value function estimate (after 1000 steps) imitator feasibility
testing repair capabilities, see that, due suppression feasibility testing,
darkly shaded high-value states Figure 19 (backed goal) terminate abruptly
infeasible transition without making across river. fact, dominated
lighter grey circles showing negative values. experiment, show bridging
prolong exploration phase right way. employ k-step repair
procedure k = 3.
Examining graph Figure 20, see imitation agents experience early
negative dip guided deep river mentors influence. agent
without repair eventually decides mentors action infeasible, thereafter avoids
river (and possibility finding goal). imitator repair also discovers
mentors action infeasible, immediately dispense mentors
guidance. keeps exploring area mentors trajectory using random walk,
accumulating negative reward suddenly finds bridge rapidly
converges optimal solution.17 control agent discovers goal
ten runs.

6. Applicability
simple experiments presented demonstrate major qualitative issues confronting implicit imitation agent specific mechanisms implicit imitation
address issues. section, examine assumptions mechanisms presented previous sections determine types problems suitable
implicit imitation. present several dimensions prove useful predicting
performance implicit imitation types problems.
17. repair steps take place area negative reward scenario, need case.
Repair doesnt imply short-term negative return.

609

fiPrice & Boutilier

15

Average Reward per 1000 Steps

10

FB Series

5
Ctrl
Ctrl
0
NoRepair

Repair
5

NoRepair
10

15

20

Repair

0

1000

2000

3000
Simulation Steps

4000

5000

6000

Figure 20: Utility bridging
already identified number assumptions implicit imitation
applicablesome assumptions models imitation teaching cannot
applied, assumptions restrict applicability model. include:
lack explicit communication mentors observer; independent objectives
mentors observer; full observability mentors observer; unobservability mentors
actions; (bounded) heterogeneity. Assumptions full observability necessary
modelas formulatedto work (though discuss extension partially observable case Section 7). Assumptions lack communication unobservable actions
extend applicability implicit imitation beyond models literature;
conditions hold, simpler form explicit communication may preferable. Finally,
assumptions bounded heterogeneity independent objectives also ensure implicit
imitation applied widely. However, degree rewards
actions homogeneous impact utility (i.e., acceleration learning offered by) implicit imitation. turn attention predicting performance
implicit imitation function certain domain characteristics.
6.1 Predicting Performance
section examine two questions: first, given implicit imitation applicable,
implicit imitation bias agent suboptimal solution; second,
performance implicit imitation vary structural characteristics domains
one might want apply to? show analysis internal structure state space
used motivate metric (roughly) predicts implicit imitation performance.
conclude analysis problem space understood terms
distinct regions playing different roles within imitation context.
610

fiImplicit Imitation

implicit imitation model, use observations agents improve observers knowledge environment rely sensible exploration policy
exploit additional knowledge. clear understanding knowledge environment affects exploration therefore central understanding implicit imitation
perform domain.
Within implicit imitation framework, agents know reward functions, knowledge environment consists solely knowledge agents action models.
general, models take form. simplicity, restricted
models decomposed local models possible combination system
state agent action.
local models state-action pairs allow prediction j-step successor state
distribution given initial state sequence actions local policy. quality
j-step state predictions function every action model encountered
initial state states time j 1. Unfortunately, quality j-step
estimate drastically altered quality even single intermediate state-action
model. suggests connected regions state space, states
fairly accurate models, allow reasonably accurate future state predictions.
Since estimated value state based immediate reward
reward expected received subsequent states, quality value estimate
also depend quality action models states connected s. Now,
since greedy exploration methods bias exploration according estimated value
actions, exploratory choices agent state also dependent
connectivity reliable action models states reachable s. analysis
implicit imitation performance respect domain characteristics therefore organized
around idea state space connectivity regions connectivity defines.
6.1.1 Imitation Regions Framework
Since connected regions play important role implicit imitation, introduce classification different regions within state space shown graphically Figure 21.
follows, describe regions affect imitation performance model.
first observe many tasks carried agent small subset
states within state space defined problem. precisely, many MDPs,
optimal policy ensure agent remains small subspace state space.
leads us definition first regional distinction: relevant vs. irrelevant regions.
relevant region set states non-zero probability occupancy optimal
policy.18 -relevant region natural generalization optimal policy keeps
system within region fraction 1 time.
Within relevant region, distinguish three additional subregions. explored
region contains states observer formulated reliable action models
basis experience. augmented region contains states observer
lacks reliable action models improved value estimates due mentor observations.
18. One often assumes system starts one small set states. Markov chain induced
optimal policy ergodic, irrelevant region nonempty. Otherwise
empty.

611

fiPrice & Boutilier

Observer
Explored
Region
Blind
Region

Mentor
Augmented
Region

Irrelevant
Region

Reward
Figure 21: Classification regions state space
Note explored augmented regions created result observations
made learner (of either transitions mentor). regions
therefore significant connected components; is, contiguous regions state space
reliable action mentor models available. Finally, blind region designates
states observer neither (significant) personal experience benefit
mentor observations. information states within blind region come
(largely) agents prior beliefs.19
ask regions interact imitation agent. First consider
impact relevance. Implicit imitation makes assumption accurate dynamics
models allow observer make better decisions will, turn, result higher returns
sooner learning process. However, model information equally helpful:
imitator needs enough information irrelevant region able avoid it.
Since action choices influenced relative value actions, irrelevant region
avoided looks worse relevant region. Given diffuse priors action
models, none actions open agent initially appear particularly attractive.
However, mentor provides observations within relevant region quickly make
relevant region look much promising method achieving higher returns
therefore constrain exploration significantly. Therefore, considering problems
point view relevance, problem small relevant region relative entire space
combined mentor operates within relevant region result maximum
advantage imitation agent non-imitating agent.
explored region, observer sufficiently accurate models compute good
policy respect rewards within explored region. Additional observations
19. partitioning states explored, blind augmented regions bears resemblance Kearns
Singhs (1998) partitioning state space known unknown regions. Unlike Kearns Singh,
however, use partitions analysis. implicit imitation algorithm explicitly
maintain partitions use way compute policy.

612

fiImplicit Imitation

states within explored region provided mentor still improve performance
somewhat significant evidence required accurately discriminate expected
value two actions. Hence, mentor observations explored region help,
result dramatic speedups convergence.
Now, consider augmented region observers Q-values
augmented observations mentor. experiments previous sections,
seen observer entering augmented region experience significant speedups
convergence due information inherent augmented value function
location rewards region. Characteristics augmented zone, however,
affect degree augmentation improves convergence speed.
Since observer receives observations mentors state, actions,
observer improved value estimates states augmented region, policy.
observer must therefore infer actions taken duplicate mentors
behavior. observer prior beliefs effects actions, may able
perform immediate inference mentors actual choice action (perhaps using
KL-divergence maximum likelihood). observers prior model uninformative,
observer explore local action space. exploring local action space,
however, agent must take action action effect. Since
guarantee agent took action duplicates mentors action, may end
somewhere different mentor. action causes observer fall outside
augmented region, observer lose guidance augmented value function
provides fall back performance level non-imitating agent.
important consideration, then, probability observer remain
augmented regions continue receive guidance. One quality augmented region
affects observers probability staying within boundaries relative coverage
state space. policy mentor may sparse complete. relatively
deterministic domain defined begin end states, sparse policy covering states
may adequate. highly stochastic domain many start end states, agent
may need complete policy (i.e., covering every state). Implicit imitation provide
guidance agent domains stochastic require complete
policies, since policy cover larger part state space.
important completeness policy predicting guidance, must
also take account probability transitions augmented region.
actions domain largely invertible (directly, effectively so), agent
chance re-entering augmented region. ergodicity lacking, however,
agent may wait process undergoes form reset
opportunity gather additional evidence regarding identity mentors actions
augmented region. reset places agent back explored region,
make way frontier last explored. lack ergodicity
would reduce agents ability make progress towards high-value regions resets,
agent still guided attempt augmented region. Effectively,
agent concentrate exploration boundary explored region
mentor augmented region.
utility mentor observations depend probability augmented
explored regions overlapping course agents exploration. explored
613

fiPrice & Boutilier

regions, accurate action models allow agent move quickly possible high
value regions. augmented regions, augmented Q-values inform agents states
lead highly-valued outcomes. augmented region abuts explored region,
improved value estimates augmented region rapidly communicated across
explored region accurate action models. observer use resultant improved
value estimates explored region, together accurate action models
explored region, rapidly move towards promising states frontier
explored region. states, observer explore outward thereby eventually
expand explored region encompass augmented region.
case explored region augmented region overlap,
blind region. Since observer information beyond priors blind region,
observer reduced random exploration. non-imitation context, states
explored blind. However, imitation context, blind area reduced
effective size augmented area. Hence, implicit imitation effectively shrinks size
search space problem even overlap explored
augmented spaces.
challenging case implicit imitation transfer occurs region augmented mentor observations fails connect observer explored region
regions significant reward values. case, augmented region initially
provide guidance. observer independently located rewarding states,
augmented regions used highlight shortcuts. shortcuts represent improvements agents policy. domains feasible solution easy find,
optimal solutions difficult, implicit imitation used convert feasible solution
increasingly optimal solution.
6.1.2 Cross regional textures
seen distinctive regions used provide certain level insight
imitation perform various domains. also analyze imitation performance
terms properties cut across state space. analysis model information
impacts imitation performance, saw regions connected accurate action models
allowed observer use mentor observations learn promising direction
exploration. see, then, set mentor observations useful
concentrated connected region less useful dispersed state
space unconnected components. fortunate completely observable environments
observations mentors tend capture continuous trajectories, thereby providing
continuous regions augmented states. partially observable environments, occlusion
noise could lessen value mentor observations absence model predict
mentors state.
effects heterogeneity, whether due differences action capabilities
mentor observer due differences environment two agents, also
understood terms connectivity action models. Value propagate along
chains action models hit state mentor observer different
action capabilities. state, may possible achieve mentors value
therefore, value propagation blocked. Again, sequential decision making aspect
614

fiImplicit Imitation

reinforcement learning leads conclusion many scattered differences
mentor observer create discontinuity throughout problem space, whereas
contiguous region differences mentor observer cause discontinuity
region, leave large regions fully connected. Hence, distribution pattern
differences mentor observer capabilities important prevalence
difference. explore pattern next section.
6.2 Fracture Metric
try characterize connectivity form metric. Since differences reward structure, environment dynamics action models affect connectivity would
manifest differences policies mentor observer, designed
metric based differences agents optimal policies. call metric fracture.
Essentially, computes average minimum distance state mentor
observer disagree policy state mentor observer agree policy. measure roughly captures difficulty observer faces profitably exploiting
mentor observations reduce exploration demands.
formally, let mentors optimal policy observers. Let
state space Sm 6=o set disputed states mentor observer
different optimal actions. set neighboring disputed states constitutes disputed
region. set Sm 6=o called undisputed states. Let distance
metric space S. metric corresponds number transitions along
minimal length path states (i.e., shortest path using nonzero probability
observer transitions).20 standard grid world, correspond Manhattan
distance. define fracture (S) state space average minimal distance
disputed state closest undisputed state:
(S) =

1

X

|Sm 6=o | sS

6=o

min

tSSm 6=o

(s, t).

(13)

things equal, lower fracture value tend increase propagation
value information across state space, potentially resulting less exploration
required. test metric, applied number scenarios varying fracture
coefficients. difficult construct scenarios vary fracture coefficient yet
expected value. scenarios Figure 22 constructed
length possible paths start state goal state x
scenario. scenario, however, upper path lower path. mentor
trained scenario penalizes lower path mentor learns take
upper path. imitator trained scenario upper path penalized
therefore take lower path. equalized difficulty problems
follows: using generic -greedy learning agent fixed exploration schedule (i.e.,
fixed initial rate decay) one scenario, tuned magnitude penalties
exact placement along loops scenarios learner using
exploration policy would converge optimal policy roughly number
steps each.
20. expected distance would give accurate estimate fracture, difficult calculate.

615

fiPrice & Boutilier

X

X





(a) = 0.5

X



(b) = 1.7



(c) = 3.5

X

(d) = 6.0

Figure 22: Fracture metric scenarios


0.5
1.7
3.5
6.0

5 102
60%

1 102
70%

Observer Initial Exploration Rate
5 103 1 103 5 104 1 104
90%
0%
80%
90%
90 %
30%
100 %
30 %
70 %

5 105

1 105

100 %

100 %

Figure 23: Percentage runs (of ten) converging optimal policy given fracture
initial exploration rate

Figure 22(a), mentor takes top loop optimal run, imitator
would take bottom loop. Since loops short length common
path long, average fracture low. compare Figure 22(d), see
loops longthe majority states scenario loops.
states loop distance nearest state observer mentor
policies agree, namely, state loop. scenario therefore high average
fracture coefficient.
Since loops various scenarios differ length, penalties inserted loops
vary respect distance goal state therefore affect total discounted expected reward different ways. penalties may also cause agent
become stuck local minimum order avoid penalties exploration rate
low. set experiments, therefore compare observer agents basis
likely converge optimal solution given mentor example.
Figure 23 presents percentage runs (out ten) imitator converged
optimal solution (i.e., taking lower loops) function exploration rate
scenario fracture.21 see distinct diagonal trend table illustrating
increasing fracture requires imitator increase levels exploration order find
21. reasons computational expediency, entries near diagonal computed. Sampling entries confirms trend.

616

fiImplicit Imitation

optimal policy. suggests fracture reflects feature RL domains may
important predicting efficacy implicit imitation.
6.3 Suboptimality Bias
Implicit imitation fundamentally biasing exploration observer. such,
worthwhile ask positive effect observer performance. short
answer mentor following optimal policy observer cause observer
explore neighborhood optimal policy generally bias observer
towards finding optimal policy.
detailed answer requires looking explicitly exploration reinforcement learning. theory, -greedy exploration policy suitable rate decay cause
implicit imitators eventually converge optimal solution unassisted
counterparts. However, practice, exploration rate typically decayed quickly
order improve early exploitation mentor input. Given practical, theoretically
unsound exploration rates, observer may settle mentor strategy feasible,
non-optimal. easily imagine examples: consider situation agent
observing mentor following policy. Early learning process, value
policy followed mentor may look better estimated value alternative
policies available observer. could case mentors policy actually
optimal policy. hand, may case one alternative
policies, observer neither personal experience, observations
mentor, actually superior. Given lack information, aggressive exploitation policy might lead observer falsely conclude mentors policy optimal.
implicit imitation bias agent suboptimal policy, reason expect
agent learning domain sufficiently challenging warrant use imitation
would discovered better alternative. emphasize even mentors policy
suboptimal, still provides feasible solution preferable solution
many practical problems.
regard, see classic exploration/exploitation tradeoff additional
interpretation implicit imitation setting. component exploration rate
correspond observers belief sufficiency mentors policy.
paradigm, then, seems somewhat misleading think terms decision whether
follow specific mentor not. question much exploration
perform addition required reconstruct mentors policy.
6.4 Specific Applications
see applications implicit imitation variety contexts. emerging electronic
commerce information infrastructure driving development vast networks
multi-agent systems. networks used competitive purposes trade, implicit
imitation used RL agent learn buying strategies information
filtering policies agents order improve behavior.
control, implicit imitation could used transfer knowledge existing
learned controller already adapted clients new learning controller
completely different architecture. Many modern products elevator controllers
617

fiPrice & Boutilier

(Crites & Barto, 1998), cell traffic routers (Singh & Bertsekas, 1997) automotive fuel
injection systems use adaptive controllers optimize performance system
specific user profiles. upgrading technology underlying system, quite
possible sensors, actuators internal representation new system
incompatible old system. Implicit imitation provides method transferring
valuable user information systems without explicit communication.
traditional application imitation-like technologies lies area bootstrapping
intelligent artifacts using traces human behavior. Research within behavioral cloning
paradigm investigated transfer applications piloting aircraft (Sammut et al.,
1992) controlling loading cranes (Suc & Bratko, 1997). researchers investigated use imitation simplify programming robots (Kuniyoshi, Inaba, &
Inoue, 1994). ability imitation transfer complex, nonlinear dynamic behaviors
existing human agents makes particularly attractive control problems.

7. Extensions
model implicit imitation presented makes certain restrictive assumptions regarding structure decision problem solved (e.g., full observability, knowledge
reward function, discrete state action space). simplifying assumptions
aided detailed development model, believe basic intuitions much
technical development extended richer problem classes. suggest several
possible extensions section, provides interesting avenue
future research.
7.1 Unknown Reward Functions
current paradigm assumes observer knows reward function.
assumption consistent view RL form automatic programming.
can, however, relax constraint assuming ability generalize observed rewards.
Suppose expected reward expressed terms probability distribution
features observers state, Pr(r|f (so )). model-based RL, distribution
learned agent experience. features
applied mentors state sm , observer use learned
reward distribution estimate expected reward mentor states well. extends
paradigm domains rewards unknown, preserves ability
observer evaluate mentor experiences terms.
Imitation techniques designed around assumption observer mentor
share identical rewards, Utgoffs (1991), would course work absence
reward function. notion inverse reinforcement learning (Ng & Russell, 2000) could
adapted case well. challenge future research would explore synthesis
implicit imitation reward inversion approaches handle observers prior
beliefs intermediate level correlation reward function observer
mentor.
618

fiImplicit Imitation

7.2 Interaction agents
cast general imitation model framework stochastic games, restriction model presented thus far noninteracting games essentially means
standard issues associated multiagent interaction arise. are, course,
many tasks require interactions agents; cases, implicit imitation offers
potential accelerate learning. general solution requires integration imitation
general models multiagent RL based stochastic Markov games (Littman,
1994; Hu & Wellman, 1998; Bowling & Veloso, 2001). would doubt rather
challenging, yet rewarding endeavor.
take simple example, simple coordination problems (e.g., two mobile agents
trying avoid carrying related tasks) might imagine imitator
learning mentor reversing roles roles considering
observed state transition influenced joint action. general
settings, learning typically requires great care, since agents learning nonstationary
environment may converge (say, equilibrium). Again, imitation techniques offer
certain advantages: instance, mentor expertise suggest means coordinating
agents (e.g., providing focal point equilibrium selection, making clear
specific convention always passing right avoid collision).
challenges opportunities present imitation used multiagent settings. example, competitive educational domains, agents
choose actions maximize information exploration returns exploitation;
must also reason actions communicate information agents.
competitive setting, one agent may wish disguise intentions, context
teaching, mentor may wish choose actions whose purpose abundantly clear.
considerations must become part action selection process.
7.3 Partially Observable Domains
extension model partially observable domains critical, since unrealistic
many settings suppose learner constantly monitor activities mentor.
central idea implicit imitation extract model information observations
mentor, rather duplicating mentor behavior. means mentors internal
belief state policy (directly) relevant learner. take somewhat
behaviorist stance concern mentors observed behaviors
tell us possibilities inherent environment. observer keep
belief state mentors current state, done using estimated
world model observer uses update belief state.
Preliminary investigation model suggests dealing partial observability
viable. derived update rules augmented partially observable updates.
updates based Bayesian formulation implicit imitation is, turn, based
Bayesian RL (Dearden et al., 1999). fully observable contexts, seen
effective exploration using mentor observations possible fully observable domains
Bayesian model imitation used (Price & Boutilier, 2003). extension
model cases mentors state partially observable reasonably straightforward.
anticipate updates performed using belief state mentors state
619

fiPrice & Boutilier

action help alleviate fracture could caused incomplete observation
behavior.
interesting dealing additional factor usual exploration-exploitation
tradeoff: determining whether worthwhile take actions render mentor
visible (e.g., ensuring mentor remains view source information remains
available learning).
7.4 Continuous Model-Free Learning
many realistic domains, continuous attributes large state action spaces prohibit
use explicit table-based representations. Reinforcement learning domains
typically modified make use function approximators estimate Q-function
points direct evidence received. Two important approaches
parameter-based models (e.g., neural networks) (Bertsekas & Tsitsiklis, 1996)
memory-based approaches (Atkeson, Moore, & Schaal, 1997). approaches,
model-free learning generally employed. is, agent keeps value function uses
environment implicit model perform backups using sampling distribution
provided environment observations.
One straightforward approach casting implicit imitation continuous setting would
employ model-free learning paradigm (Watkins & Dayan, 1992). First, recall augmented Bellman backup function used implicit imitation:
(

(

V (s) = Ro (s) + max max
aAo

X

)

Pro (s, a, t)V (t) ,

tS

)

X

Prm (s, t)V (t)

(14)

tS

examine augmented backup equation, see converted
model-free form much way ordinary Bellman backup. use standard
Q-function observer actions, add one additional action corresponds
action taken mentor.22 imagine observer state ,
took action ao ended state s0o . time, mentor made transition
state sm s0m . write:


Q(so , ao ) = (1 )Q(so , ao ) + (Ro (so , ao ) + max

max

a0 Ao






Q(s0o , a0 ) ,

Q(sm , ) = (1 )Q(sm , ) + (Ro (sm , ) + max max
0

Ao





Q(s0o , )


Q(s0m , a0 ) ,

(15)


Q(s0m , )

discussed earlier, relative quality mentor observer estimates Qfunction specific states may vary. Again, order avoid inaccurate prior beliefs
mentors action models bias exploration, need employ confidence measure
decide apply augmented equations. feel natural setting
kind tests memory-based approaches function approximation. Memorybased approaches, locally-weighted regression (Atkeson et al., 1997), provide estimates functions points previously unvisited, also maintain evidence
22. doesnt imply observer knows actions corresponds .

620

fiImplicit Imitation

set used generate estimates. note implicit bias memory-based approaches assumes smoothness points unless additional data proves otherwise.
basis bias, propose compare average squared distance query
exemplars used estimate mentors Q-value average squared distance
query exemplars used observer-based estimate heuristically decide
agent reliable Q-value.
approach suggested benefit prioritized sweeping. Prioritizedsweeping, however, adapted continuous settings (Forbes & Andre, 2000).
feel reasonably efficient technique could made work.

8. Related Work
Research imitation spans broad range dimensions, ethological studies,
abstract algebraic formulations, industrial control algorithms. fields crossfertilized informed other, come stronger conceptual definitions
better understanding limits capabilities imitation. Many computational
models proposed exploit specialized niches variety control paradigms,
imitation techniques applied variety real-world control problems.
conceptual foundations imitation clarified work natural imitation. work apes (Russon & Galdikas, 1993), octopi (Fiorito & Scotto, 1992),
animals, know socially facilitated learning widespread throughout animal kingdom. number researchers pointed out, however, social facilitation
take many forms (Conte, 2000; Noble & Todd, 1999). instance, mentors attention
object draw observers attention thereby lead observer manipulate object independently model provided mentor. True imitation
therefore typically defined restrictive fashion. Visalberghi Fragazy (1990)
cite Mitchells definition:
1. something C (the copy behavior) produced organism
2. C similar something else (the Model behavior)
3. observation necessary production C (above baseline levels C
occurring spontaneously)
4. C designed similar
5. behavior C must novel behavior already organized precise way
organisms repertoire.
definition perhaps presupposes cognitive stance towards imitation
agent explicitly reasons behaviors agents behaviors relate
action capabilities goals.
Imitation analyzed terms type correspondence demonstrated
mentors behavior observers acquired behavior (Nehaniv & Dautenhahn,
1998; Byrne & Russon, 1998). Correspondence types distinguished level.
action level, correspondence actions. program level, actions
621

fiPrice & Boutilier

may completely different correspondence may found subgoals.
effect level, agent plans set actions achieve effect demonstrated
behavior direct correspondence subcomponents observers
actions mentors actions. term abstract imitation proposed
case agents imitate behaviors come imitating mental state
agents (Demiris & Hayes, 1997).
study specific computational models imitation yielded insights
nature observer-mentor relationship affects acquisition behaviors
observers. instance, related field behavioral cloning, observed
mentors implement conservative policies generally yield reliable clones (Urbancic
& Bratko, 1994). Highly-trained mentors following optimal policy small coverage
state space yield less reliable clones make mistakes (Sammut et al.,
1992). partially observable problems, learning perfect oracles disastrous,
may choose policies based perceptions available observer. observer
therefore incorrectly biased away less risky policies require additional
perceptual capabilities (Scheffer, Greiner, & Darken, 1997). Finally, observed
successful clones would often outperform original mentor due cleanup effect
(Sammut et al., 1992).
One original goals behavioral cloning (Michie, 1993) extract knowledge
humans speed design controllers. extracted knowledge
useful, argued rule-based systems offer best chance intelligibility
(van Lent & Laird, 1999). become clear, however, symbolic representations
complete answer. Representational capacity also issue. Humans often organize
control tasks time, typically lacking state perception-based approaches
control. Humans also naturally break tasks independent components
subgoals (Urbancic & Bratko, 1994). Studies also demonstrated humans
give verbal descriptions control policies match actual actions
(Urbancic & Bratko, 1994). potential saving time acquisition borne
one study explicitly compared time extract rules time required
program controller (van Lent & Laird, 1999).
addition traditionally considered imitation, agent may also face
problem learning imitate finding correspondence actions
states observer mentor (Nehaniv & Dautenhahn, 1998). fully credible approach
learning observation absence communication protocols deal
issue.
theoretical developments imitation research accompanied number
practical implementations. implementations take advantage properties different control paradigms demonstrate various aspects imitation. Early behavioral cloning
research took advantage supervised learning techniques decision trees (Sammut
et al., 1992). decision tree used learn human operator mapped perceptions actions. Perceptions encoded discrete values. time delay inserted
order synchronize perceptions actions trigger. Learning apprentice systems
(Mitchell et al., 1985) also attempted extract useful knowledge watching users,
goal apprentices independently solve problems. Learning apprentices closely
related programming demonstration systems (Lieberman, 1993). Later efforts used
622

fiImplicit Imitation

sophisticated techniques extract actions visual perceptions abstract
actions future use (Kuniyoshi et al., 1994). Work associative recurrent learning models allowed work area extended learning temporal sequences
(Billard & Hayes, 1999). Associative learning used together innate following
behaviors acquire navigation expertise agents (Billard & Hayes, 1997).
related slightly different form imitation studied multi-agent
reinforcement learning community. early precursor imitation found work
sharing perceptions agents (Tan, 1993). Closer imitation idea
replaying perceptions actions one agent second agent (Lin, 1991; Whitehead,
1991a). Here, transfer one agent another, contrast behavioral clonings
transfer human agent. representation also different. Reinforcement learning
provides agents ability reason effects current actions expected
future utility agents integrate knowledge knowledge extracted
agents comparing relative utility actions suggested knowledge
source. seeding approaches closely related. Trajectories recorded human
subjects used initialize planner subsequently optimizes plan order
account differences human effector robotic effector (Atkeson &
Schaal, 1997). technique extended handle notion subgoals within
task (Atkeson & Schaal, 1997). Subgoals also addressed others (Suc & Bratko,
1997). work based idea agent extracting model mentor
using model information place bounds value actions using
reward function. Agents therefore learn mentors reward functions different
own.
Another approach family based assumption mentor rational
(i.e., follows optimal policy), reward function observer chooses
set actions. Given assumptions, conclude action
chosen mentor particular state must higher value mentor
alternatives open mentor (Utgoff & Clouse, 1991) therefore higher value
observer alternative. system Utgoff Clouse therefore iteratively adjusts
values actions constraint satisfied model. related approach
uses methodology linear-quadratic control ( Suc & Bratko, 1997). First model
system constructed. inverse control problem solved find cost matrix
would result observed controller behavior given environment model. Recent
work inverse reinforcement learning takes related approach reconstructing reward
functions observed behavior (Ng & Russell, 2000). similar inversion
quadratic control approach, formulated discrete domains.
Several researchers picked idea common representations perceptual functions action planning. One approach using representation
perception control based PID controller model. PID controller represents
behavior. output compared observed behaviors order select action
closest observed behavior (Demiris & Hayes, 1999). Explicit motor action
schema also investigated dual role perceptual motor representations
(Mataric, Williamson, Demiris, & Mohan, 1998).
Imitation techniques applied diverse collection applications. Classical control applications include control systems robot arms (Kuniyoshi et al., 1994;
623

fiPrice & Boutilier

Friedrich, Munch, Dillmann, Bocionek, & Sassin, 1996), aeration plants (Scheffer et al.,
1997), container loading cranes (Suc & Bratko, 1997; Urbancic & Bratko, 1994). Imitation learning also applied acceleration generic reinforcement learning (Lin,
1991; Whitehead, 1991a). Less traditional applications include transfer musical style
(Canamero, Arcos, & de Mantaras, 1999) support social atmosphere (Billard, Hayes, & Dautenhahn, 1999; Breazeal, 1999; Scassellati, 1999). Imitation also
investigated route language acquisition transmission (Billard et al., 1999;
Oliphant, 1999).

9. Concluding Remarks
described formal principled approach imitation called implicit imitation.
stochastic problems explicit forms communication possible,
underlying model-based framework combined model extraction provides alternative
imitation learning-by-observation systems. new approach makes use
model compute actions imitator take without requiring observer
duplicate mentors actions exactly. shown implicit imitation offer significant
transfer capability several test problems, proves robust face
noise, capable integrating subskills multiple mentors, able provide benefits
increase difficulty problem.
seen feasibility testing extends implicit imitation principled manner
deal situations homogeneous action assumption invalid. Adding
bridging capabilities preserves extends mentors guidance presence infeasible actions, whether due differences action capabilities local differences state
spaces. approach also relates idea following sense imitator
uses local search model repair discontinuities augmented value function acting world. process applying imitation various domains,
learned properties. particular developed fracture metric
characterize effectiveness mentor given observer specific domain.
also made considerable progress extending imitation new problem classes. model
developed rather flexible extended several ways: example,
Bayesian approach imitation building work shows great potential (2003);
initial formulations promising approaches extending implicit imitation multiagent problems, partially observable domains domains reward function
specified priori.
number challenges remain field imitation. Bakker Kuniyoshi (1996)
describe number these. Among intriguing problems unique imitation are:
evaluation expected payoff observing mentor; inferring useful state
reward mappings domains mentors observers; repairing
locally searching order fit observed behaviors observers capabilities
goals. also raised possibility agents attempting reason
information revealed actions addition whatever concrete value actions
agent.
Model-based reinforcement applied numerous problems. Since implicit imitation added model-based reinforcement learning relatively little effort,
624

fiImplicit Imitation

expect applied many problems. basis simple
elegant theory Markov decision processes makes easy describe analyze. Though
focused simple examples designed illustrate different mechanisms
required implicit imitation, expect variations approach provide interesting directions future research.

Acknowledgments
Thanks anonymous referees suggestions comments earlier versions
work Michael Littman editorial suggestions. Price supported NCE IRISIII Project BAC. Boutilier supported NSERC Research Grant OGP0121843,
NCE IRIS-III Project BAC. parts paper presented Implicit Imitation Reinforcement Learning, Proceedings Sixteenth International Conference
Machine Learning (ICML-99), Bled, Slovenia, pp.325334 (1999) Imitation
Reinforcement Learning Agents Heterogeneous Actions, Proceedings Fourteenth
Biennial Conference Canadian Society Computational Studies Intelligence (AI
2001), Ottawa, pp.111120 (2001).

References
Alissandrakis, A., Nehaniv, C. L., & Dautenhahn, K. (2000). Learning things
imitation. Bauer, M., & Rich, C. (Eds.), AAAI Fall Symposium Learning
Things, pp. 16 Cape Cod, MA.
Atkeson, C. G., & Schaal, S. (1997). Robot learning demonstration. Proceedings
Fourteenth International Conference Machine Learning, pp. 1220 Nashville, TN.
Atkeson, C. G., Moore, A. W., & Schaal, S. (1997). Locally weighted learning control. Artificial
Intelligence Review, 11 (1-5), 75113.
Bakker, P., & Kuniyoshi, Y. (1996). Robot see, robot do: overview robot imitation. AISB96
Workshop Learning Robots Animals, pp. 311 Brighton,UK.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton.
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic Stochastic Models. Prentice-Hall,
Englewood Cliffs.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena, Belmont, MA.
Billard, A., & Hayes, G. (1997). Learning communicate imitation autonomous robots.
Proceedings Seventh International Conference Artificial Neural Networks, pp.
76368 Lausanne, Switzerland.
Billard, A., & Hayes, G. (1999). Drama, connectionist architecturefor control learning
autonomous robots. Adaptive Behavior Journal, 7, 3564.
Billard, A., Hayes, G., & Dautenhahn, K. (1999). Imitation skills means enhance learning
synthetic proto-language autonomous robot. Proceedings AISB99 Symposium
Imitation Animals Artifacts, pp. 8895 Edinburgh.
Boutilier, C. (1999). Sequential optimality coordination multiagent systems. Proceedings
Sixteenth International Joint Conference Artificial Intelligence, pp. 478485 Stockholm.
625

fiPrice & Boutilier

Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions
computational leverage. Journal Artificial Intelligence Research, 11, 194.
Bowling, M., & Veloso, M. (2001). Rational convergent learning stochastic games. Proceedings Seventeenth International Joint Conference Artificial Intelligence, pp. 10211026
Seattle.
Breazeal, C. (1999). Imitation social exchange humans robot. Proceedings
AISB99 Symposium Imitation Animals Artifacts, pp. 96104 Edinburgh.
Byrne, R. W., & Russon, A. E. (1998). Learning imitation: hierarchical approach. Behavioral
Brain Sciences, 21, 667721.
Canamero, D., Arcos, J. L., & de Mantaras, R. L. (1999). Imitating human performances automatically generate expressive jazz ballads. Proceedings AISB99 Symposium
Imitation Animals Artifacts, pp. 11520 Edinburgh.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally partially observable stochastic domains. Proceedings Twelfth National Conference Artificial
Intelligence, pp. 10231028 Seattle.
Conte, R. (2000). Intelligent social learning. Proceedings AISB00 Symposium Starting
Society: Applications Social Analogies Computational Systems Birmingham.
Crites, R., & Barto, A. G. (1998). Elevator group control using multiple reinforcement learning
agents. Machine-Learning, 33 (23), 23562.
Dean, T., & Givan, R. (1997). Model minimization Markov decision processes. Proceedings
Fourteenth National Conference Artificial Intelligence, pp. 106111 Providence.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning.
Artificial Intelligence, 89, 219283.
Dearden, R., Friedman, N., & Andre, D. (1999). Model-based bayesian exploration. Proceedings
Fifteenth Conference Uncertainty Artificial Intelligence, pp. 150159 Stockholm.
DeGroot, M. H. (1975). Probability statistics. Addison-Wesley, Reading, MA.
Demiris, J., & Hayes, G. (1997). robots ape?. Proceedings AAAI Fall Symposium
Socially Intelligent Agents, pp. 2831 Cambridge, MA.
Demiris, J., & Hayes, G. (1999). Active passive routes imitation. Proceedings
AISB99 Symposium Imitation Animals Artifacts, pp. 8187 Edinburgh.
Fiorito, G., & Scotto, P. (1992). Observational learning octopus vulgaris. Science, 256, 54547.
Forbes, J., & Andre, D. (2000). Practical reinforcement learning continuous domains. Tech. rep.
UCB/CSD-00-1109, Computer Science Division, University California, Berkeley.
Friedrich, H., Munch, S., Dillmann, R., Bocionek, S., & Sassin, M. (1996). Robot programming
demonstration (RPD): Support induction human interaction. Machine Learning, 23,
163189.
Hartmanis, J., & Stearns, R. E. (1966). Algebraic Structure Theory Sequential Machines. PrenticeHall, Englewood Cliffs.
Hu, J., & Wellman, M. P. (1998). Multiagent reinforcement learning: Theoretical framework
algorithm. Proceedings Fifthteenth International Conference Machine Learning,
pp. 242250 Madison, WI.
Kaelbling, L. P. (1993). Learning Embedded Systems. MIT Press, Cambridge,MA.
626

fiImplicit Imitation

Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research, 4, 237285.
Kearns, M., & Singh, S. (1998). Near-optimal reinforcement learning polynomial time. Proceedings Fifthteenth International Conference Machine Learning, pp. 260268 Madison,
WI.
Kuniyoshi, Y., Inaba, M., & Inoue, H. (1994). Learning watching: Extracting reusable task
knowledge visual observation human performance. IEEE Transactions Robotics
Automation, 10 (6), 799822.
Lee, D., & Yannakakis, M. (1992). Online miminization transition systems. Proceedings
24th Annual ACM Symposium Theory Computing (STOC-92), pp. 264274 Victoria,
BC.
Lieberman, H. (1993). Mondrian: teachable graphical editor. Cypher, A. (Ed.), Watch
Do: Programming Demonstration, pp. 340358. MIT Press, Cambridge, MA.
Lin, L.-J. (1991). Self-improvement based reinforcement learning, planning teaching. Machine
Learning: Proceedings Eighth International Workshop (ML91), 8, 32327.
Lin, L.-J. (1992). Self-improving reactive agents based reinforcement learning, planning
teaching. Machine Learning, 8, 293321.
Littman, M. L. (1994). Markov games framework multi-agent reinforcement learning.
Proceedings Eleventh International Conference Machine Learning, pp. 157163 New
Brunswick, NJ.
Lovejoy, W. S. (1991). survey algorithmic methods partially observed Markov decision
processes. Annals Operations Research, 28, 4766.
Mataric, M. J. (1998). Using communication reduce locality distributed multi-agent learning.
Journal Experimental Theoretical Artificial Intelligence, 10 (3), 357369.
Mataric, M. J., Williamson, M., Demiris, J., & Mohan, A. (1998). Behaviour-based primitives
articulated control. R. Pfiefer, B. Blumberg, J.-A. M. . S. W. W. (Ed.), Fifth International
conference simulation adaptive behavior SAB98, pp. 165170 Zurich. MIT Press.
Meuleau, N., & Bourgine, P. (1999). Exploration multi-state environments: Local mesures
back-propagation uncertainty. Machine Learning, 32 (2), 117154.
Mi, J., & Sampson, A. R. (1993). comparison Bonferroni Scheffe bounds. Journal
Statistical Planning Inference, 36, 101105.
Michie, D. (1993). Knowledge, learning machine intelligence. Sterling, L. (Ed.), Intelligent
Systems. Plenum Press, New York.
Mitchell, T. M., Mahadevan, S., & Steinberg, L. (1985). LEAP: learning apprentice VLSI
design. Proceedings Ninth International Joint Conference Artificial Intelligence,
pp. 573580 Los Altos, California. Morgan Kaufmann Publishers, Inc.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning less
data less real time. Machine Learning, 13 (1), 10330.
Myerson, R. B. (1991). Game Theory: Analysis Conflict. Harvard University Press, Cambridge.
Nehaniv, C., & Dautenhahn, K. (1998). Mapping dissimilar bodies: Affordances
algebraic foundations imitation. Proceedings Seventh European Workshop
Learning Robots, pp. 6472 Edinburgh.
627

fiPrice & Boutilier

Ng, A. Y., & Russell, S. (2000). Algorithms inverse reinforcement learning. Proceedings
Seventeenth International Conference Machine Learning, pp. 663670 Stanford.
Noble, J., & Todd, P. M. (1999). really imitation? review simple mechanisms social
information gathering. Proceedings AISB99 Symposium Imitation Animals
Artifacts, pp. 6573 Edinburgh.
Oliphant, M. (1999). Cultural transmission communications systems: Comparing observational
reinforcement learning models. Proceedings AISB99 Symposium Imitation
Animals Artifacts, pp. 4754 Edinburgh.
Price, B., & Boutilier, C. (2003). Bayesian approach imitation reinforcement learning. Proceedings Eighteenth International Joint Conference Artificial Intelligence Acapulco.
appear.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley Sons, Inc., New York.
Russon, A., & Galdikas, B. (1993). Imitation free-ranging rehabilitant orangutans (pongopygmaeus). Journal Comparative Psychology, 107 (2), 147161.
Sammut, C., Hurst, S., Kedzier, D., & Michie, D. (1992). Learning fly. Proceedings
Ninth International Conference Machine Learning, pp. 385393 Aberdeen, UK.
Scassellati, B. (1999). Knowing imitate knowing succeed. Proceedings
AISB99 Symposium Imitation Animals Artifacts, pp. 105113 Edinburgh.
Scheffer, T., Greiner, R., & Darken, C. (1997). experimentation better perfect
guidance. Proceedings Fourteenth International Conference Machine Learning,
pp. 331339 Nashville.
Seber, G. A. F. (1984). Multivariate Observations. Wiley, New York.
Shapley, L. S. (1953). Stochastic games. Proceedings National Academy Sciences, 39,
327332.
Singh, S. P., & Bertsekas, D. (1997). Reinforcement learning dynamic channel allocation
cellular telephone systems. Advances Neural information processing systems, pp. 974
980 Cambridge, MA. MIT Press.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable Markov
processes finite horizon. Operations Research, 21, 10711088.
Suc, D., & Bratko, I. (1997). Skill reconstruction induction LQ controllers subgoals.
Proceedings Fifteenth International Joint Conference Artificial Intelligence, pp.
914919 Nagoya.
Sutton, R. S. (1988). Learning predict method temporal differences. Machine Learning,
3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. ICML93, pp. 33037.
Urbancic, T., & Bratko, I. (1994). Reconstruction human skill machine learning. Eleventh
European Conference Artificial Intelligence, pp. 498502 Amsterdam.
628

fiImplicit Imitation

Utgoff, P. E., & Clouse, J. A. (1991). Two kinds training information evaluation function
learning. Proceedings Ninth National Conference Artificial Intelligence, pp. 596
600 Anaheim, CA.
van Lent, M., & Laird, J. (1999). Learning hierarchical performance knowledge observation.
Proceedings Sixteenth International Conference Machine Learning, pp. 229238
Bled, Slovenia.
Visalberghi, E., & Fragazy, D. (1990). monkeys ape?. Parker, S., & Gibson, K. (Eds.),
Language Intelligence Monkeys Apes, pp. 247273. Cambridge University Press,
Cambridge.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Whitehead, S. D. (1991a). Complexity analysis cooperative mechanisms reinforcement learning. Proceedings Ninth National Conference Artificial Intelligence, pp. 607613
Anaheim.
Whitehead, S. D. (1991b). Complexity cooperation q-learning. Machine Learning. Proceedings Eighth International Workshop (ML91), pp. 363367.

629

fi
fffi ffffff
!"# %$'&(*),+--.0/1+2.3+045

6789:;<&-0=-+0>
?8ff#$;@&-0=-.

ACBEDGFIH"JKGLNMPORQTSGFISGJUVJVHWJVXZY

[]\_^`\badcfehgiaWk:
j lRmonqp%r
s1tWuWv@wyx'z{|q}
~'i,]00_01h
*
_W
I00

*0_d~W0b

_]0]
0
,0_0
iy00!0E 00]W0]
0
fifi!qqfifiqfi
"`\b:Pe

n p Z\ba l
q
~W0bI_
'0


qq}s11|qv@wy{wyW|"1}}{t"

~W0_R

y00yiy00h
fifi!fiqVfififififififififififi

fi1
IVfi0'1V1I1'ifiG011V]1Tb11oV0o1107]

fffi
10E0d 1
1Vyy 1fiV*1 fi7oV01fi1@7
01
WV0
7V V
7d01fi7
fi fi


V1 7 ooVfi01o E fi 7fi
fi1 E0 "
!$#% %V
7

o&
7
1'
)(* E71 7 0+ 7
07
7@
0V7
fi V 1
%1 +0, 0
0

-'h
+070V:
fifi
/. 1 710R710
"
!V
0'1
11
071

2
3fi1Vfi7 fiq
E'
4

1@
fiV,
1
0R1
7V,
V 0
* /
fi5

6
7
1'
V,



fi 71 17
V1,
7VW0fi0
*1&
fi+11 07


8
9
!0+ 1
/:

1fi 0
7V
h
7,
01
7V7
fi 0 0Z
99
!;.d 01fi

'
0
01
0+ V
10 1

1
17 0'
<
=7 V0




7 +0 Vfi 0fi>
#ff
!"?@(A.d +0
V1, 7
V 0o 18
9
! V
01Z1
7V,
V 1V6
<
=7<V
0
@V
1Z
1
0
fi 01,
70
d7 1h71
]V
h7]'
0,
7fi '1'7

ff
!B?
7Vfi 0T1,
0
C +01"
+0fiD
V14
/:
EV1,
7
V0
fi,
701VV7

01C
771
1
1,
81I+
07
h
9
! 0
ff
!B?-

FDG HI fiJLK9MNOJ
PQ/RSRATVUWYX%Z[X%\VW4Z]^_W4R/^W4U_W4Z`]aU'b/W4^'Q/ZCcfiW4^_] XVd Z]e[X%fgTVQ/]hXjiTSkffXVd ZlQ/Ud ZS\mXR/^_TVfCX%f*d2n2d ]eoi/d U]a^d f/Q/p
] TVZCq*r&]@U_TSkW'RgTSd Z`] sU_b/W;^_WcfiWd tVW4U&U_TSkW;Z/W4uvd Z*wxTV^k5X%] TVZmX%fgTVQ/]9]ab/WhiVTSkffXVd ZCq-yBTuzU_b/TVQCn2i{U_b/W
Q/RBi/X%]aW6b/W4^ji/d U_]a^d f/QS] TVZ|d Z}]ab/W~n2d \Vb]TSwC]abCd Ud ZCwTV^kffX%] TVZC*S//>d Ufe[wX%^]ab/WlkTVU_]
cfiTSkffkTVZ>kW4]abSTgijd Zmc4X%UW']abSWhd ZCwTV^kffX%] TVZ>cfiTSkW4U{d Z]ab/WhwxTV^kTSw1X%ZW4tVW4Z] qyBTu9W4tVW4^s)]ab/W4^_W6X%^_W
ZQCkW4^_TVQ/U&u@Wn2n pZ/T uZlW41XVkRCn W4U0Ub/T u8d ZS\6]abCX%]0ZCXVd tVWjcfiTVZCi/d ] TVZCd Z/\lc4X%Zn WXVi6]aT5R/^_TVfCn WkUqLW
\Sd tVWQ/U_]&]u9T5TSwS]abSWkvb/W4^_Wq
eAff b/W~~/;77),
6affTVU_]aWn2n W4^s"VVVVEtVTVUYP/XatX%Z] sBVVVVahPQ/R/RgTVU_W8]abCX%]
n/ p \[W

eVTVQC ^_W TVZX\SXVkW U_b/TuX%ZCi5\Sd tVW4ZX5cOb/TSd2cfiWhTSwg]ab/^_W4WiT`TV^UqCW4bCd ZCi5TVZ/Wjd U6XYc4X%^/fgW4bCd ZCiff]ab/W
TV]ab/W4^_UX%^_W\VTSX%]aUqTVQ5RCd2cOiVT`TV^;Vq&CWwTV^_W0TVRgW4ZCd ZS\5iT`TV^'VsS5TVZ`]ey;XVn2n2s]ab/WbSTVU_]0ub/T{Z/TuU
ubCX%]jd UfgW4bCd ZCijWXVcObziTTV^-TVRgW4Z/UjiT`TV^VsDu0bCd2cbobCX%UYX6\VTSX%] qmy"W5]ab/W4ZX%U_`U8eVTVQzd2wCeVTVQoU_] d2n2n
uX%Z`]{]aT] X%VWYubCX%] U{fgW4bCd Z*iliVT`TV^5VsDTV^{]aT] X%VW5ub*X%] U{fgW4bCd ZCimiTTV^5d Z/U_]aWXVi/qPbSTVQCn2ijeVTVQ
U_u8d ] cObCYr;UU_QCkffd Z/\h]abCX%] sEd ZCd ] d2XVn2n es]ab/Wjc4X%^uX%U;WQ*XVn2n emn2d VWn e]aTfffgWfgW4bCd ZCi WXVcOb~TSwS]ab/W5iTTV^_Us
ZCXVd tVWhcfiTVZCi/d ] TVZCd Z/\ U_Q/\V\VW4U]aU9]abCX%] s\Sd tVW4Z]abCX%]'d ]d U"Z/TV]9fgW4bCd ZCiffiTTV^'Vs7d ]d U9WQ*XVn2n e~n2d VWn eff]aT8fgW
fgW4bCd ZCiiTTV^8ffX%ZCiiVT`TV^8Vq b`QSUs`]ab/W4^_WYd UZST6^_WX%U_TVZ]aTffUu8d ] cbCq;yBTu9W4tVW4^sEX%Z/TV]ab/W4^ X%^_\VQCkW4Z]
U_Q/\V\VW4U]aUBeVTVQ6U_b/TVQCn2i;U_u ] cbC@d2wSX@\VTSX%];d UfgW4bCd Z*i8iTTV^0ubCd2cOb6bCX%R/RgW4Z/ULu8d ]abhR/^_TVfCX%f*d2n2d ]eVVVas
+--.b##7
; ;@I 9R 9 9b8
ff#$#fiffff0$V#"#;V

fi " |qxq}t~"}s1|

U_u8d ] cObCd Z/\jb/Wn R/U*d2wXjc4X%^6d U'fgW4bCd ZCiiVT`TV^6Yu0bCd2cblbCX%R/RgW4Z/U;u8d ]abRS^_TVfCX%fCd2n2d ]eoVVVas7U_u8d ] cb*d Z/\
bQ/^_]aUq*bCd2cb[X%^_\VQCkW4Z]8d U^d \Vb`]
e @ bSW', 7%,/ %E7)+"a@X%^py;d2n2n Wn/XVn 1sVVVVS{X%^iZ/W4^sVVVV/5TVU_]aWn2n W4^s
n/ p \[W
VVVVa{{wA]abS^_W4W8RS^d U_TVZ/W4^_Uh`s*sEX%ZCils/]u9T>X%^_W8]aTYfgW8W4`WcfiQ/]aWiSsSfSQ/]8iTW4U;ZSTV];Z/T uubCd2cObCq
bQ/UsC ]abCd Z/U0]abCX%]']ab/WRS^_TVfCX%fCd2n2d ]e]abCX%]0-u8d2n2n/fgWW4WcfiQ/]aWimd UhVVjwxTV^'[V`44Vq8y"WUXaeU
]aT]ab/WaXVd2n W4^s %PSd ZCcfiWWd ]ab/W4^TV^jd UcfiW4^_] XVd ZCn e\VTSd Z/\>]aTfgWjW4`WcfiQ/]aWiSsBeVTVQu d2n2n*\Sd tVWokWZ/T
ZCwTV^kffX%] TVZoX%fgTVQ/]k0ejT uZ[cbCX%ZCcfiW4Uhd2w/eVTVQ~\Sd tVWjkW]ab/WZ*XVkWTSw7TVZ/W5kffX%ZCsWd ]ab/W4^h9TV^8sub/T
U@\VTSd Z/\8]aT8fgWW4`WcfiQS]aWi/q2>CQ/]"]abSW4ZCsZSTYkffX%]a]aW4^-ubCX%]B]abSW0aXVd2n W4^@UXaeUsZ*XVd tVWhcfiTVZCiSd ] TVZCd Z/\n WXViU
]aTfffgWn2d W4tVW]abCX%]b*d U8cbCX%Z*cfiW{TSw7W4`WcfiQ/] TVZu9W4Z] iVT uZ}w^_TSkVV{]aTVVVq
bSW4^_W>X%^_WZ`Q*kW4^_TVQ/UhTV]ab/W4^ffu@Wn2n pZ/T uZW4gXVkR*n W4Uffub/W4^_WZCXVd tVW}cfiTVZ*i/d ] TVZCd Z/\\Sd tVW4UYubCX%]
U_W4WkUj]aT}fgW[X%Zd ZCX%R/RS^_TVR/^d2X%]aW[X%Z/U_u@W4^sd ZCc4n QCi/d ZS\>]ab/W+fi [A,+a{X%^iZ/W4^sVVVV
tVTVU6P/XatX%Z`] s*VVVVsLVV%S;X%Z*i6]ab/W,/fi,7),
a^_W4Q/ZCiSsDVVVVLPbCXVwW4^sDVVVVLy;XVn RgW4^_Z[
Q/]a] n WsEVVVVaq &
b`eiTW4U{ZCXVd tVWlcfiTVZCi/d ] TVZCd Z/\~\Sd tVW5]ab/Wffu0^_TVZ/\}X%Z/U_u@W4^Yd Z>UQCcb>W41XVkRCn W4Ur'UYX%^_\VQSWif`e
y'XVn RAW4^Z|X%ZCi Q/]a] n WmaVVVV8X%ZCi}PbCXVwW4^jaVVVVas*]ab/Wff^WXVn1R/^_TVf*n Wkd U]abCX%]8u9WlX%^_WffZ/TV]cfiTVZCi/d p
] TVZCd Z/\md Z~]ab/W8^d \Vb]0U_RCXVcfiWq94w7u9W8u9TV^_>d Z[X5n2X%^_\VW4^~U_TVR/bCd U] d2c4X%]aWi/8U_RCXVcfiWsu0b/W4^_W{u@W] X%VW ]ab/W
R/^_TV]aT1cfiTSnQ/U_Wife~ffTVZ]elad ZmDgXVkR*n W Vq2V@X%ZCi]ab/W;XVd2n W4^ad Z11XVkRCn W8Vq2V-d Z]aTjXVc4cfiTVQ/Z] sgcfiTVZ/p
i/d ] TVZCd ZS\~iT`W4UiVWn2d tVW4^0]ab/W^d \Vb]X%Z/U_u@W4^q&BTVQ/\VbCn ejU_RgWX%gd Z/\Ss]abSWU_TVR/bCd U_] d2c4X%]aWi6U_RCXVcfiW5cfiTVZ/Ud U]aU
TSwXVn2n]abSW{RgTVU_Ud fCn W{UWQ/W4Z*cfiW4U;TSw7W4tVW4Z]aU']ab*X%] cfiTVQCn2iffbCX%R/RgW4ZoawTV^;W41XVkRCn Ws/ubCX%]85TVZ`]eu@TVQCn2i
UX_e|d ZoWXVcObc4d ^cfiQCkU] X%ZCcfiWs*TV^8u0bCX%]8]abSW5aXVd2n W4^ u@TVQCn2i~UXaezd ZoWXVcObc4d ^cfiQCkU_] X%Z*cfiWasLu8d ]ab]ab/Wd ^
R/^_TVf*X%fCd2n2d ]eq + y"T u@W4tVW4^s)u@TV^_1d Z/\d Zj]ab/W'U_TVR/bCd U] d2c4X%]aWi U_RCXVcfiW'bCX%U-R/^_TVfCn WkU@]aTTSq&TV^&TVZ/W']abCd Z/\Ss
]d UhZ/TV]XVn uX_e`Uc4n WX%^6u0bCX%]8]abSWj^Wn W4t)X%Z]hR/^_TVfCX%fCd2n2d ] W4U~d Z[]ab/WU_TVRSbCd U_] d2c4X%]aWilU_RCXVcfiW>X%^_WqVTV^
W41XVkRCn WsubCX%];d U9]ab/WR/^_TVf*X%fCd2n2d ]e6]abCX%]9]ab/WaXVd2n W4^9UXaeU&d2wg&X%ZCiff0X%^_W]aTfgWW4WcfiQ/]aWi/5fiZCiW4Wi/s
Z5UTSkW c4X%U_W4UsAd ]d U"Z/TV]9W4tVW4Zmc4n WX%^-ub*X%]B]ab/W0Wn WkW4Z`]aU-TSw]ab/W8n2X%^\VW4^@URCXVcfiW X%^_Wq&5TV^_W4TtVW4^s`W4tVW4Z
ub/W4Z]abSWffWn WkW4Z]aUX%ZCij]abSWff^_Wn W4tX%Z`]8R/^_TVfCX%fCd2n2d ] W4UX%^_WffZ/T u0ZCsg]ab/W6Ud 4WYTSwD]abSWffU_TVR/bCd U] d2c4X%]aWi
U_RCXVcfiWjkffXaefgWcfiTSkWYX%Zod UU_Q/Ws1X%U0]abSWYwxTSn2n Tu8d Z/\ffW41XVkRCn WU_b/T u0Uq
e
C PQ/RSRATVUW;]abCX%]Xu9TV^n2iiVW4Ucfi^d fgW4UubCd2cObTSwCVV{RgW4TVRCn WbCXatVWjX6cfiW4^_] XVd Zi/d UWX%U_Wq
n/ p \[W
ru9TV^n2ioc4X%ZofAWlcbCX%^XVcfi]aW4^d 4Wilf`eX6]aQSRCn WffTSw-VV%UX%ZCi>%UsEu0b/W4^_Wff]ab/W5O]abzcfiTSkRATVZSW4Z`]Yd U
ZCi/d t1d2iQCXVnAbCX%U]ab/Wi/d U_WX%UWq b/W4^_W~X%^_W~ &-- RATVUUd fCn W6u@TV^n2iUqQ/^]ab/W4^{U_Q/RSRATVUW ]ab*X%]{]ab/W
%X%\VW4Z] ZvQSW4U_] TVZvd UX>cfiTSkRSQ/]aW4^YUe`U_]aWk5qfiZCd ] d2XVn2n es-]abSWX%\VW4Z]YbCX%UYZ/Td Z*wxTV^k5X%] TVZCsX%ZCi
cfiTVZ/Ud2iVW4^_U;XVn2nS &-- u9TV^n2iVU"W QCXVn2n en2d VWn eq b/WX%\VW4Z]9]abSW4Zff^_WcfiWd tVW4U{d ZCwTV^kffX%] TVZ5]ab*X%];d U'X%U_UQCkWi
]aTfgWh]a^_Q/W~X%fgTVQ/]ubCd2cOb>u9TV^n2id U{]ab/W~XVcfi]aQCXVngu@TV^n2i/q bCd U5d Z*wxTV^k5X%] TVZ|cfiTSkW4UYd Z>]abSW~wxTV^kTSw
U_] X%]aWkW4Z]aU{n2d VW%d ZCiSd tgd2iQ*XVn@d U9Ud2chTV^'d Z*i/d tgd2iVQCXVn6d U"b/WXVn ]abegTV^6%X%]n WX%U_]'RAW4TVR*n WbCXatVW;]ab/W
i/d U_WX%UWVq"XVcObU_QCcbjU_] X%]aWkW4Z]c4X%ZfgWhd2iW4Z] /Wi8u8d ]abX;U_W4]9TSwRATVUUd fCn W'u9TV^n2iVUq-TV^&W4gXVkRCn Ws
]ab/W U_] X%]aWkW4Z]l%X%]ffn WX%U_]ffRgW4TVRCn W bCX_tVW ]abSWi/d UWX%U_Wc4X%ZlfgWd2iW4Z] /WiYu8d ]ab]abSW8U_W4]TSwg]aQSRCn W4U
u8d ]abX%]5n WX%U_]5%UqYTV^Ud2kRCn2d2c4d ]es"X%U_U_Q*kWh]abCX%]]ab/WX%\VW4Z`]jd U\Sd tVW4Zd ZCwTV^kffX%] TVZUX_egd ZS\]ab/W
XVcfi]aQCXVnEu9TV^n2i}d Ud Z}U_W4]Vs-wxTV^8t)X%^d TVQ/UU_W4]aUqmPQSR/RgTVU_W~X%]U_TSkW5RgTSd Z`]8]ab/WX%\VW4Z]8b*X%U8fgW4W4Z
%D`,0-@, ff
fi
fixO E7 ff
fi
fix7fi fffi fi x!#"%$&('*),+ -V.x0/%21ff
/ , 3 4A6 57x! 8 9ff:-; x=
<>?, ".
@
6fi AVC
BED44F#"$G&('*),+H8 :-ff fi /fi ?ff 6 ?-a2
fi V8 fi x
J /8 K1 L/MO, -8 NNOP?@%fi ,fi Qfi Vfi /fi x0R :@ H ff R 6 %ff :- C fi V,
S%4 T1A% x, / DC<2? /U ?fi AVO W<( Xfi ;?fifi ARU IVY
:Z9
6 [#
1fi fi ,.
S,\
T1 ]x ^ ff x,
C:-X 1, x;; @/
0
x ffx 9/%R
D+6 _D+8 CR
fi

`7a7a

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

]aTSn2ih]abCX%]-]ab/WffXVcfi]aQCXVnu9TV^n2id Ud Z} & ^i^i^i 4Fj7q b/W4ZCsgXVw]aW4^iTSd Z/\~cfiTVZCiSd ] TVZCd Z/\Ss]ab/WffX%\VW4Z]bCX%UX
Q/ZCd2wTV^kR/^TVfCX%fCd2n2d ]eTVZo &Rk i^i^i k jSq
CQ/]bST uiVT`W4U]abSWYX%\VW4Z`];VW4W4R~]a^XVcOTSw7]ab/W{u@TV^n2iU8d ]8cfiTVZSUd2iW4^_URgTVU_Ud fCn WY]8cfiW4^] XVd ZCn eu8d2n2n
Z/TV]W4RCn2d2c4d ] n en2d U_]{]ab/Wk51]ab/W4^_W~X%^W6Ud2kRCn e]aT`T}kffX%Zeq-Z/WhRgTVU_Ud f*d2n2d ]ed U{]abCX%]5d ]VW4W4R/U8]a^XVc
TSw7ubCX%]8d ]0bCX%UfgW4W4Z~]aTSn2i/]ab/W{RgTVU_Ud fCn Wu9TV^n2iVU8X%^_W{]ab/W4Z]ab/W{TVZ/W4UhcfiTVZ/Ud U_]aW4Z]0u ]abubCX%]8d ]0bCX%U
fgW4W4Zm]aTSn2i/qYCQ/]]abCd U5n WXViVU]aT]u@TTVf`t1d TVQ/U{R/^TVfCn WkU8cb/WcOgd Z/\wxTV^5cfiTVZ/Ud U_]aW4ZCcfielu8d ]abmubCX%]ffd ]
bCX%U&fgW4W4Z]aTSn2ikffX_eYfgWbCX%^iSsAX%Z*id2wCd ]&bCX%U&fgW4W4Z]aTSn2m
lm]abCd ZS\VUwxTV^8n2X%^\V.
W l"s^_WkWk0fAW4^d Z/\h]ab/Wk
XVn2nBkffX_efgWd ZCwWX%Ud fCn Wq8fiZUd ]aQCX%] TVZ/U{u0b/W4^_Wh]ab/W4U_W6]u9TRS^_TVfCn WkU5X%^d UWs"X%ZX%\VW4Z]YkffXaemZ/TV]fgW
X%fCn W{]aTlcfiTVZCi/d ] TVZX%RSR/^_TVR/^d2X%]aWn eq
11XVkRCn W~Vq2 RS^_T t1d2iW4U{U_TSkW~kTV] t)X%] TVZzwxTV^u@TV^_1d Z/\}d Z>]ab/WhUk5XVn2n W4^s"kTV^W Z*XVd tVWffU_RCXVcfiWq1p
XVkRCn W4U6Vq2jX%ZCi~Vq2U_b/Tu]abCX%];]abCd U6d U0Z/TV]hXVn uX_e`U5X%R/RS^_TVR/^d2X%]aWq bQ/UsEX%ZTVftgd TVQSUh Q/W4U_] TVZd U
ub/W4Zd ]5d U5X%R/RS^_TVR/^d2X%]aWq{]]aQ/^_ZSU'TVQS]']ab*X%]']ab*d U5Q/W4U] TVZ|d UbCd \VbCn e^_Wn W4t)X%Z]5d Z]ab/WhU_] X%] U_] d2c4XVn
X%^_WX%U@TSwD +o n +8fia,,YjX%ZCq
ph,/q&^d \Sd ZCXVn2n e6U_]aQCiSd Wiu8d ]abCd ZY]ab/W4U_W8cfiTVZ]aW4`]aU
aQSfCd ZCs*VVVVK
rXau8d2is
rd2cVW4es@VVVVasBd ]uX%U5n2X%]aW4^5wxTVQ/Z*iff]abCX%]ffd ]ffXVn U_TRCn2X_e`U5XwQ/ZCi/XVkW4Z] XVn
^_TSn W{d Z6]ab/W-U_] X%] U_] d2c4XVnVu9TV^_8TVZl) n n )/
'u t{n Wd ZfCX%QCkffsSVVVVaq&EQ*d2n2i/d Z/\TVZ6R/^_W4t1d TVQ/UX%R/p
R/^_TSXVcOb/W4UsAyBWd ]aaX%Z>X%ZCi5BQ/fCd ZaVVVVCR/^W4U_W4Z`]aWijX&ZSWcfiW4U_UX%^_e~X%ZCi{U_]
Q vc4d W4Z]'cfiTVZ*i/d ] TVZmwTV^@ub/W4Z
cfiTVZCi/d ] TVZ*d Z/\Yd Z6]ab/WjZCXVd tVW;U_RCXVcfiW{d U'X%R/R/^_TVR/^d2X%]aW;
q w"T uXVi/X_e`U9]abCd UBU_TVpc4XVn2n Wi}
x0y{z* ,, //
V
y"/|
p0}cfiTVZ*i/d ] TVZd U6X%ZmW4U_] X%f*n2d U_b/WiY]aT`TSnLd ZmU_Q/^tgd tXVnCX%Z*XVn e`Ud Uq awTV^T tVW4^_t1d W4uUs1U_W4Wad2n2n2s
tX%Z~iW4V
^ ~EXVX%ZCsBTVfCd Z/Us)VVVVE
w;d Wn U_W4Z*s/VVVVaq2W&W41XVkffd Z/W&]abCd U0cfi^d ]aW4^d TVZ~d Z TVQS^T uZCs^X%]ab/W4^
i/d AW4^_W4Z];cfiTVZ`]aW4] sAX%ZCiU_b/Tu]abCX%];d ]'X%R/RCn2d W4U"^X%]ab/W4^B^X%^_Wn eq&PVRAWc4d *c4XVn2n esVu@WU_b/Tu]abCX%]B]ab/W4^W8X%^_W
^_WXVn2d U_] d2c9U_W4]a] ZS\VU&ub/W4^_W;]ab/W;UXVkRCn W0URCXVcfiWhd U-U_]a^QCcfi]aQ/^_WiYd ZjUQCcbXuXae5]abCX%]d ]{d Ud2kRgTVU_Ud fCn W;]aT
UX%] Uw
e Lr60sCX%ZCihu@WR/^_Ttgd2iVWjX5cfi^d ]aW4^d TVZ]aTjb/Wn R[iW4]aW4^kffd Z/W8ub/W4]ab/W4^0TV^;Z/TV];]abCd U U']ab/Wc4X%U_Wq
W~XVn U_T\Sd tVW~X6A,)ficObCX%^XVcfi]aW4^d X%] TVZTSwD]abS
W rh cfiTVZ*i/d ] TVZCsgfe~\Sd t1d Z/\}X ^X%ZCiVTSkffd 4Wi
XVn \VTV^d ]abCk]abCX%]{\VW4Z/W4^X%]aW4UjXVn2nBX%ZCijTVZ*n eiSd U_]a^d f/Q/] TVZSU5wxTV^{u0bCd2c
b Lr6bSTSn2iUsg]ab/W4^W4f`elU_TSn t1d Z/\
X%ZTVRgW4ZR/^TVfCn WkvRgTVU_Wi fed2n2n)W4]8XVn2qDaVVVVaq
W ]ab/W4Z~Ub/T u]abCX%]0]ab/W8Ud ]aQCX%] TVZd U0u@TV^_U_Wd2wS]abSWjd ZCwTV^kffX%] TVZiTW4U0Z/TV]8cfiTSkWd Z]ab/WYwTV^k
TSwDX%ZjW4tVW4Z`] q0TV^-]abCX%]c4X%UWsUW4tVW4^XVn`\VW4Z/W4^XVn2d X%] TVZ/UTSw1cfiTVZ*i/d ] TVZCd Z/\hbCX_tVW;fgW4W4ZjR/^_TVRgTVU_Wi/;
q 7W4^_p
bCX%R/U0]ab/W{fgW4U_]&Z/T uZ[X%^_
W fi YS//u 4W4A^_W4esEVVVV'aXVn U_T5Z/TuZX%=
U `L
%)
`
X%ZCim~/o ph4
pyB + n W
9/fi7
z
yVU}``S~u t&QCn2n fCXVc1s*VVVV2
LUd U_2
X% ^s*VVVVPb/TV^WY
4TVbSZ/U_TVZCs-VVVVhaXVn U_Tm`ZST uZX%U fi,/47a
q 4W4A^_W4e|cfiTVZCi/d ] TVZ*d Z/\[d UX5\VW4Z/W4^XVn2d X%] TVZTSw
TV^i/d Z*X%^_emcfiTVZ*i/d ] TVZCd Z/\SDl0}Q/RBi/X%] Z/\d U8X\VW4Z/W4^XVn2d X%] TVZmTSR
w 4W4A^_W4emcfiTVZ*i/d ] TVZCd Z/\Sq
WYU_b/T u]abCX%q
] W47^W4ecfiTVZ*i/d ] TVZCd Z/\SsEub/W4Z|X%RSRCn2d2c4X%fCn Ws@c4X%ZofgW6Q/U_] /WiQSZCiW4^5X%ZX%RSR/^_TVp
R/^d2X%]aW\VW4ZSW4^XVn2d X%] TVZzTSwB]ab/
W rhcfiTVZ*i/d ] TVZCqr6n ]ab/TVQ/\Vbd ]6bCX%U5fAW4W4ZX%^\VQ/Wi/sQSUd Z/\kTVU_] n e
X%1d TSkffX%] d2c5cObCX%^XVcfi]aW4^d X%] TVZ/Us*]abCX%]Yl0QSRiSX%] Z/\>aX%ZCib/W4ZCcfiW~XVn U
4W4A^_W4ecfiTVZCi/d ] TVZCd Z/\S8d Us
ub/W4ZX%RSRCn2d2c4X%fCn Ws1]abSW~/
^_WX%U_TVZ*X%fCn W6uX_em]aTQ/RBi/X%]aWhR/^_TVfCX%fCd2n2d ]e[UW4Ws1Wq \Sq2s@u LUd U_2
X% ^s"VVVV
Pb/TV^W
TVb/Z/U_TVZ*sVVVVaas9d ]5d U{u@Wn2nAZ/T uZ]ab*X%]]ab/W4^_W~X%^_W6Ud ]aQ*X%] TVZ/U{ub/W4^W~X%R/RCn e1d Z/\>;
n WXViUh]aTmRCX%^XViTgd2c4XVn2sBbCd \VbCn e|cfiTVQSZ`]aW4^d Z]aQCd ] tVW^_W4UQCn ]aUayBQ/Z]aW4^sVVVVPWd2iVW4ZCwxWn2iSs@VVVV9tX%Z
^XVX%UU_W4ZCsDVVVVaq
W ).
- 8 |
ph8R/^_TVfCn WkvtX%Z^XVX%U_U_W4Z*s/VVVVa4QCie5d Un TVU_]d ZX
eS LTVZ/Ud2iW4^C]abS
n/ p \[W

^_W4\Sd TVZ8]abCX%]9d U@iSd tgd2iWid Z`]aT]u9T;bCXVn tVW4Us)-n Q/W;X%ZCiWi-]aW4^_^d ]aTV^_esWXVcb TSwub*d2cbjd U@wxQ/^]ab/W4^9i/d t1d2iWi
Z]aToyBWXViSQCX%^]aW4^_q
U LTSkRCX%Z`eX%^_WXX%ZCi>PWcfiTVZC
LTSkRCX%Ze[X%^WXVq6rR/^d TV^d2sH4QCie[cfiTVZ/Ud2iW4^UYd ]
W QCXVn2n en2d VWn eff]abCX%]9U_b/Whd Ud ZmX%Z`e6TSw]ab/W4UWhwTVQ/^' QCXVi^X%Z`]aUq-PbSW8cfiTVZ`] XVcfi]aU&bSW4^9TuZ5b/WXViSQCX%^]aW4^_U
fe8^XViSd TSsSX%ZCi6d UB]aTSn2im-c4X%Z* ]BfgW&U_Q/^_W-ub/W4^W&eVTVQX%^_Wq*4wVeVTVQlX%^Wd ZWi]aW4^^d ]aTV^_esV]abSWTgiSiU0X%^_W
V2&]ab*X%]"eVTVQX%^_W Zm0
LTSkRCX%ZeX%^_WXq2q2q2jr&]@]abCd U"RgTSd Z]B]ab/W;^XVi/d T{\Sd tVW4U-TVQ/] q&l0~Q/RBi/X%] Z/\

`7a7

fi " |qxq}t~"}s1|

TVZ]abCd Ud Z*wxTV^k5X%] TVZon WXViU0]aT~X6iSd U_]a^d f/Q/] TVZu0b/W4^_W]ab/WRgTVU_]aW4^d TV^&RS^_TVfCX%fCd2n2d ]ejTSwSfgWd Z/\d Z}-n Q/W
]aW4^_^d ]aTV^ed U0\V^_WX%]aW4^;]ab*X%ZoVVVqBfiZCiW4WiSsAd2wLQ
zbCXVihUXVd2iow7eVTVQ[X%^_W5d ZBWih]aW4^_^d ]aTV^_es/]ab/WT1i/iU
X%^_
W "6]ab*X%]8eVTVQX%^_Wld ZQ
cfiTSkR*X%Z`eX%^_WXlqq4q4Vs*]ab/W4ZzwxTV^XVn2;
n Vs-XVc4cfiTV^i/d Z/\l]aT;
Q/RBi/X%] Z/\Ss]abSWRATVU]aW4^d TV^0R/^TVfCX%fCd2n2d ]ejTSw7fgWd Z/\~d Zo-n Q/W{]aW4^_^d ]aTV^_e>d U8XVn uX_e`U'\V^_WX%]aW4^']abCX%ZoVVVq
-^T tVWX%ZCizy'XVn RAW4^Z aVVVV RS^_T t1d2iWXzUTVR/bCd U_] d2c4X%]aWi[URCXVcfiWlub/W4^_W[cfiTVZCi/d ] TVZCd ZS\\Sd tVW4U
ubCX%]d UX%^_\VQCX%fCn e>]abSWmkTV^Wmd Z]aQCd ] tVW}X%Z/U_u@W4^d Z]abSWQCieCW4Z/XVk5d ZRS^_TVfCn WkffsCZCXVkWn e}]abCX%]
d2wQ
U_W4ZCiUXkW4U_UX%\VWTSwL]ab/WwxTV^k %d2wBeVTVQX%^Wmd ZWi]aW4^_^d ]aTV^_esL]ab/W4Z]ab/WjTgi/iVUX%^_e
W -
]abCX%]@eVTVQmX%^Whd Zm0
cfiTSkRCX%ZeX%^_WXV0]ab/W4e
Z 4QCie1 U"RgTVU_]aW4^d TV^9R/^TVfCX%fCd2n2d ]e5TSwfAWd ZS\Yd ZYWXVcObTSw)]ab/W
]u@TQCXViV^X%Z`]aUd Zl-n Q/W^WkffXVd Z/U'X%]V%Sq&PWd2iW4Z*wxWn2iYaVVVVasU_]a^_W4Z/\V]ab/W4Z*d Z/\{^_W4U_Q*n ]aU"TSw1^d WiSkffX%Z
X%ZCiYPbCd2kTVZe~aVVVVasU_b/Tu9Wi]abCX%]@]ab/W4^_W U/{U_TVR/b*d U_] d2c4X%]aWi8U_RCXVcfiW6d Z5ubCd2cObmcfiTVZ*i/d ] TVZCd Z/\ u8d2n2n
\Sd tVW~]abSWUXVkW}X%Z/U_u@W4^X%Ul0d Z]abCd Uc4X%UWqaPW4W}XVn U_T|u rX_u8d2i/sVVVVaswxTV^5Ud2kffd2n2X%^ff^_W4UQCn ]aU
XVn TVZ/\l]ab/W4U_Wn2d ZSW4Uq2WYU]a^_W4Z/\V]ab/W4Zo]ab/W4U_W5^W4U_QCn ]aUfemU_b/Tu8d Z/\~]ab*X%] sEW4tVW4Zd ZXc4n2X%U_U TSw&kQCcOb
Ud2kR*n W4^Ud ]aQCX%] TVZ/U6ub/W4^W
W 4W4A^_W4e>cfiTVZCi/d ] TVZCd Z/\lc4X%Z/Z/TV]fgW5X%R/RCn2d WiSas)Q/Ud Z/\~l0d Z]ab/WZCXVd tVW
U_RCXVcfiW cfiTV^_^_W4U_RgTVZCiU]aTjcfiTVZCi/d ] TVZCd Z/\jd Zff]ab/WU_TVRSbCd U_] d2c4X%]aWiURCXVcfiW8d ZffW4U_UW4Z`] d2XVn2n effTVZ*n e8]a^d t1d2XVnSc4X%UW4Uq
S7 /~}/ n `,7
b/W4UW5^_W4U_Q*n ]aU ] X%VW4Z]aTV\VW4]abSW4^6U_b/T u]abCX%]~ / fi
mO`|

j>o nY47fi
0+o +j~ n 5

ph+//, %aq bCX%]6d U]ab/WkffXVd Z
kW4U_UX%\VW{TSw7]abCd URCX%RgW4^q
Wff^WkffX%^_l]abCX%] sBXVn ]ab/TVQ/\Vbo]ab/W4^_W~X%^_W~cfiW4^] XVd Z}Ud2kffd2n2X%^d ] W4Us1TVQ/^^W4U_QCn ]aUYX%^_W~ QCd ]aW~i/d AW4^_W4Z]
ZU_R*d ^d ]{wx^_TSk ]ab/Wu9Wn2n pZ/TuZ~^_W4UQCn ]aU&TSwrd2XVcfiTVZCd U6X%ZCiDX%fgWn2n1aVVVVaq b/W4emcfiTVZ/Ud2iW4^_Wi6ub/W4Z
XRgTVU_]aW4^d TV^YR/^_TVfCX%f*d2n2d ]ecfiTVQCn2ifgWt1d W4u9Wi|X%U5]ab/W~^_W4U_QCn ]ffTSwcfiTVZCiSd ] TVZCd Z/\|XRS^d TV^ffR/^_TVf*X%fCd2n2d ]e
TVZU_TSkWn2X%^_\VW4^U_RCXVcfiWq CeuX_e|TSw cfiTVZ]a^X%U_] s'u@WbCXatVW|XlS`Win2X%^\VW4^U_RCXVcfiWd Zk5d ZCiz]ab/W
U_TVR/b*d U_] d2c4X%]aWiffURCXVcfiWVasCX%Z*iX%^_WYd Z]aW4^_W4U_]aWid Z~ub/W4Z}cfiTVZ*i/d ] TVZCd Z/\md Z]ab/W{ZCXVd tVW8U_RCXVcfiWYX%Z*ih]ab/W
U_TVR/b*d U_] d2c4X%]aWi6U_R*XVcfiWYX%\V^_W4Wq
fi]5d UYXVn U_Tu@TV^_]ab>U]a^_W4U_Ud Z/\]abCX%]]ab/W~i/d U_] ZCcfi] TVZ}fAW4]u9W4W4Zo]ab/W6ZCXVd tVWX%Z*ij]ab/W6U_TVR/bCd U] d2c4X%]aWi
U_RCXVcfiWd U"W4Z`] ^_Wn e Q/Z/^Wn2X%]aWi]aT]ab/W&R/bCd2n TVUTVR/bCd2c4XVnt1d W4u]abCX%]TVZ/W&bCX%UTSwR/^_TVf*X%fCd2n2d ]eX%ZCi'b/TuTVZ/W
U_b/TVQ*n2iiTYR/^_TVfCX%f*d2n2d U_] d2chd ZCwxW4^W4ZCcfiWqTV^W41XVkRCn Ws7]ab/W R/^_TVf*X%fCd2n2d ] W4Uffd Zl]abSW5TVZ`]ey'XVn2n7R/Q/4n W
c4X%ZjfAW0tgd W4u@WiYX%U9]ab/W0R*X%^_] d2c4d RCX%Z] U@UQ/fDWcfi] tVW0R/^_TVf*X%fCd2n2d ] W4UX%fgTVQ/]9]ab/W8n T1c4X%] TVZTSw]ab/W c4X%^X%ZCi
X%fgTVQ/]-ubCX%]{5TVZ`]eYu8d2n2nUXae5QSZCiW4^-ubCX%]{c4d ^cfiQCkU_] X%ZCcfiW4U1XVn ]aW4^_Z*X%] tVWn es/]ab/W4emc4X%ZfgW;t1d W4u@WiX%U
%w^_WQSW4Z`] U_] 5R/^_TVf*X%fCd2n2d ] W4Us&d ZCwW4^_^_Wiowx^_TSkuX%] cObCd Z/\l]ab/WffTVZ]e|y'XVn2nEU_b/T uTVZ]aWn W4tgd Ud TVZwTV^
kffX%ZeYu@W4W4U X%ZCih]abSW4ZU_W4]a] Z/\ff]abSWR/^_TVfCX%fCd2n2d ] W4U;WQ*XVn)]aTffTVf/U_W4^tVWiwx^WQ/W4Z*c4d W4Uq b/WRS^_TVfCn Wk
u@WYXVi/i^_W4UU&Tgc4cfiQ/^UfgTV]abowx^TSkXffw^_WQSW4Z`] U_]X%Z*iwx^TSkXU_Q/fEWcfi] tVW{U] X%ZCcfiWq
bSW ^W4U_]TSw1]abCd URCX%RgW4^ffd UTV^_\SX%ZCd 4Wi}X%UYwxTSn2n TuUq{ZPWcfi] TVZu@W~wTV^kffXVn2d 4W6]ab/WhZ/TV] TVZ>TSw
ZCXVd tVWX%Z*iU_TVR/bCd U] d2c4X%]aWiU_RCXVcfiW4UqhZPVWcfi] TVZVs1u@WcfiTVZ/Ud2iW4^{]ab/W~c4X%U_WYub/W4^_Wh]ab/Wld ZCwxTV^kffX%] TVZ
cfiTSkW4Ud Zj]ab/W6wxTV^kTSwDX%ZW4tVW4Z] qLWffiW4Ucfi^d fgW0]abSm
W Lr6cfiTVZCi/d ] TVZ>X%ZCiU_b/Tuz]abCX%]d ]{d U@tgd TSn2X%]aWi
ZX \VW4Z/W4^XVn1U_W4]a] Z/\TSwEubCd2cOb]ab/W~ffTVZ]ey'XVn2nBX%ZCij]ab/^W4W4pR/^d U_TVZ/W4^UR/Q/4n W~X%^_W6U_RgWc4d2XVnBc4X%UW4Uq
fiZPVWcfi] TVZju@W6\Sd tVW6U_W4tVW4^XVncObCX%^XVcfi]aW4^d X%] TVZSU8TS!
w rh;qSW6UQ/R/RCn e}cfiTVZCiSd ] TVZ/UQ/ZCiW4^'ubCd2cOb
]ffd U\VQCX%^X%Z]aW4Wi]aTb/TSn2imX%ZCiY\VQCX%^X%Z]aW4WijZ/TV]]aTjbSTSn2i/sLX%ZCiffu@Wh\Sd tVW~X8^X%ZCiTSkffd 4WimXVn \VTV^d ]ab*k
]abCX%]8\VW4Z/W4^X%]aW4UXVn2n9X%Z*iTVZCn ei/d U]a^d f/Q/] TVZ/UjwxTV^ ubCd2cO
b Lr6b/TSn2iUqYfiZ|PWcfi] TVZ6u@WlcfiTVZ/Ud2iW4^
]ab/Wc4X%U_WubSW4^_W{]ab/Wd ZCwxTV^kffX%] TVZ[d U;Z/TV] Z]ab/WjwTV^k TSw*X%ZlW4tVW4Z] q@W8/^U_]8cfiTVZ/Ud2iVW4^0Ud ]aQCX%] TVZSU
ub/W4^
W 4W4A^_W4e>cfiTVZCi/d ] TVZCd ZS\mc4X%ZfAWjX%R/RCn2d Wi/q9WUb/T u]abCX%
] W47^W4ecfiTVZ*i/d ] TVZCd Z/\md Z~]ab/W8ZCXVd tVW
U_RCXVcfiW5\Sd tVW4U]ab/W~X%R/RS^_TVR/^d2X%]aW~X%Z/Uu9W4^Yd Xh\VW4Z/W4^XVn2d 4W
Lr6$cfiTVZCiSd ] TVZ}b/TSn2iUqW5]ab/W4Z>U_b/Tu
]abCX%] s`]eRCd2c4XVn2n esDX%RSRCn egd ZS\l0d Z]ab/W'ZCXVd tVWU_RCXVcfiW6iTW4U@ZSTV]9\Sd tVW]ab/W6X%R/R/^TVR/^d2X%]aW6X%Z/U_u@W4^q*W
cfiTVZCc4n QCiVW{u8d ]abU_TSkWYi/d UcfiQ/UUd TVZTSw7]ab/W5d2kRCn2d2c4X%] TVZlTSw7]ab/W4U_W{^_W4UQCn ]aU8d ZoPWcfi] TVZ[Vq

`7a7

fib
9G

c edbgfb]fbz}zhcyz1

s1t yz|qu

C KG FJ K"NNM/K ?q

gN

-Q/^'wxTV^k5XVnSkT1iWn/d UX&U_RgWc4d2XVnSc4X%UW0TSw`]ab/W k0QCn ] pX%\VW4Z]-U_e`U]aWkU'w^XVkW4u@TV^_ay;XVn RgW4^_Zm/X%\Sd Z*s
VVVVasubCd2cObmd U@W4U_U_W4Z] d2XVn2n eff]ab/W'UXVkWhX%U9]ab*X%]9Q/UWifeV^d Wi/kffX%Z>X%ZCiYy;XVn RgW4^_ZmaVVVVE]aTkT1iWn
fgWn2d Ww^_W4t1d Ud TVZCqW5X%U_U_QCkW']abCX%]&]ab/W4^W6d U&UTSkWW4`]aW4^ZCXVn`u9TV^n2id Z}X;U_W4
] vsgX%ZCiX%Z}X%\VW4Z]&ub/T
kffX%VW4UhTVf/U_W4^t)X%] TVZ/U TV^h\VW4]aUd ZCwxTV^kffX%] TVZzX%fgTVQ/]8]abCX%]u@TV^n2i/qWmc4X%ZiW4Ucfi^d fgWY]ab/W5Ud ]aQ*X%] TVZ
fe[XhR*XVd ^j ^ asEub/W4^_
W

U8]ab/W~XVcfi]aQCXVnDu9TV^n2iSs"X%ZC
&d U]ab/WX%\VW4Z] U+"sEubCd2cOb
W4U_U_W4Z] d2XVn2n ecb*X%^XVcfi]aW4^d 4W4Ujb/W4^~d ZCwTV^kffX%] TVZC
q
UffubCX%]6u@W}c4XVn2n Wi]abSWZCXVd tVWU_RCXVcfiW}d Z]ab/W
Z]a^_TgiVQCcfi] TVZCqTV^{]abSWffR/Q/^_RgTVU_W4UTSwE]abCd U{RCX%RgW4^sAu@WX%U_UQCkW6]abCX%W
] CbCX%U{]abSW~wxTV^
k & ^ i^i^i^ jgas
ub/W4^#
W %-d UL]ab/W9TVfSU_W4^_tX%] TVZ6]abCX%]L]ab/WX%\VW4Z]0kffX%VW4U0X%]*] d2kW-s V^ i^i^i 6 l"q b*d UC^_W4RS^_W4U_W4Z] X%] TVZ
d2kRCn2d2c4d ] n eX%U_U_Q*kW4U]abCX%]]ab/WX%\VW4Z]^_WkWk0fAW4^UW4tVW4^_e`]ab*d Z/\U_b/W bCX%UTVf/U_W4^tVWiUd ZCcfiW6b/W4^5n T1c4XVn
U_] X%]aWlW4ZCcfiT1iW4UlXVn2n"]ab/WR/^W4tgd TVQ/UYTVf/U_W4^_tX%] TVZ/Uaq b`Q/UsBu@Wd \VZ/TV^WkWkTV^ed U_U_Q/W4UYb/W4^_Wq|W
XVn U_T|d \VZSTV^_W}cfiTSkR/Q/] X%] TVZ*XVn0d U_U_QSW4UsQ/U_]6U_TX%U5]aT>fgWX%f*n W]aT|wxT1cfiQ/U6TVZu0b/W4ZcfiTVZCi/d ] TVZCd ZS\|d U
X%R/R/^TVR/^d2X%]aWq
rvRCXVd ^8 h
& ^ i^i^i^ %jga0d U8c4XVn2n WiX^_Q/ZCqCr^_Q/Z}kffX_ejfgWt1d W4u9WiX%U8XffcfiTSkR*n W4]aWjiW4Ucfi^d R/] TVZ
TSwgub*X%];bCX%RSRAW4ZSU0T tVW4^] d2kWd ZmTVZ/W RgTVU_Ud fCn W8W4`WcfiQS] TVZTSw1]ab/W U_eU_]aWkffq TV^Ud2kRCn2d2c4d ]esd Zm]abCd U
RCX%RgW4^su@WX%U_UQCkW@]abCX%]C]abSW9U_] X%]aW-TSw]abSW9u@TV^n2ihiTW4U*ZSTV]-cObCX%Z/\VW-T tVW4^] d2kWq b/WffU_TVR/bCd U_] d2c4X%]aWi
U_RCXVcfiW5d U0]ab/WU_W4]0TSw*XVn2nRATVUUd fCn W^_Q/ZSUq
fiZ]ab/W55TVZ`]e}y;XVn2n)RSQ/4n Ws]ab/WZCXVd tVW{URCXVcfiW{bCX%U&]ab/^W4Wu9TV^n2iVUs^_W4R/^_W4UW4Z`] Z/\6]abSW]ab/^_W4WRgTVU_p
Ud fCn Wn T1c4X%] TVZ/U{TSwD]ab/W~c4X%^q bSW8U_TVRSbCd U_] d2c4X%]aWiU_R*XVcfiW~iW4Ucfi^d fgW4UubCX%]ff5TVZ`]eu@TVQCn2ijbCX_tVW6UXVd2i
Z>XVn2ngc4d ^cfiQCkU_] X%Z*cfiW4Uad2q Wq2s15TVZ]e1 U;7fi0X%U&u@Wn2ngX%U-ub/W4^_W;]ab/Whc4X%^d Uq b/W0]abS^_W4W4pR/^d U_TVZSW4^_U
R/Q/4n W>d U6]a^_WX%]aWid ZiW4] XVd2n&d ZDgXVkR*n WVq2jfgWn Tu8ql|b*d2n Wd Z]abSW4U_W>c4X%U_W4U6]ab/WU_TVR/bCd U] d2c4X%]aWi
U_RCXVcfiW[d U5U_] d2n2n^_Wn2X%] tVWn eUd2kRCn WsB]ab*d Uld U5ZST|n TVZ/\VW4^j]ab/Woc4X%U_W[wxTV^Y]ab/
W 4Q*ieCW4Z/aXVkffd ZRSQ/4n Wq
r6n ]ab/TVQ/\Vb]ab/WhZCXVd tVWhU_RCXVcfiW6b*X%U'TVZ*n ewTVQ/^Wn WkW4Z`]aUsBcfiTVZ/U_]a^_Q*cfi] Z/\]ab/WhU_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiWd Z/p
tVTSn tVW4UffcfiTVZ/Ud2iW4^d Z/\lXVn2n/]ab/W]abCd ZS\VU0]abCX%]h0
cfiTVQCn2i6b*XatVW UXVd2i/su0bCd2cb[d UhwX%^ w^_TSk c4n WX%^sCX%ZCiff]ab/W
cfiTVZCi/d ] TVZSUQ/ZCiVW4^ubCd2cb0
UXaeUjX%ZemRCX%^_] d2cfiQCn2X%^{]ab*d Z/\Sq~-^T tVWlX%Z*i>y;XVn RgW4^_ZaVVVV8i/d UcfiQ/U_U
]ab/WYiSd vcfiQCn ] W4U8d ZocfiTVZSU_]a^_QCcfi] Z/\5U_QCcbXU_TVR/bCd U] d2c4X%]aWiffU_R*XVcfiWq
fiZ\VW4Z/W4^XVn2s"Z/TV]6TVZCn ezd Ud ]6Z/TV]c4n WX%^5ub*X%] ]abSWU_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiW}d Us"f/Q/] ]ab/WZ/W4WiwTV^
X~U_TVR/bCd U_] d2c4X%]aWiURCXVcfiWX%ZCi>]abSW[wTV^k ]mk0Q/U_]j] X%VWkffXae|fgWcfiTSkW[c4n WX%^TVZ*n eXVwx]aW4^]abSW[wXVcfi] q
TV^W41XVkRCn Wsd Zm]ab/
W 4QCiVe}EW4ZSXVkffd ZmR/^TVfCn WkffsSfAWwTV^_WcfiTVZ`] XVcfi] ZS\b/WXVi/ QCX%^_]aW4^_U
QCieu@TVQCn2i
XVn2kTVU_]~cfiW4^] XVd ZCn e}Z/TV]hbCX_tVWbCXViX5UTVR/bCd U_] d2c4X%]aWimURCXVcfiW>d Zk5d ZCioW4tVW4ZX%U_U_QCkffd ZS\U_bSWYuX%U~X%Z
W4RAW4^]d ZjR/^_TVfCX%f*d2n2d ]e1as1X%ZCiYcfiTVQ*n2iZSTV]@b*XatVW`ZST uZj]ab/WffwxTV^kd ]-u@TVQCn2i8bCXatVW]aTh] X%VWQ/Z`] d2nAXVw]aW4^
b/WX%^d ZS\6b/WXVi/ QCX%^_]aW4^ U;^_W4U_RgTVZ/U_Wq
fiZzX%Ze|c4X%U_Ws0d2w*]ab/WmX%\VW4Z]6bCX%UXffRS^d TV^8RS^_TVfCX%fCd2n2d ]e>TVZ[]ab/WYUW4
] TSwLRgTVU_Ud f*n W5^_Q/ZSUjd Z[]ab/W
U_TVR/b*d U_] d2c4X%]aWi[U_RCXVcfiWs XVw]aW4^b/WX%^d Z/\[TV^TVf/UW4^_tgd ZS
\ & ^ i^i^i^ %_asUb/Wc4X%Z$cfiTVZCi/d ] TVZCs;]aT[\VW4]oX
RgTVU_]aW4^d TV^'TV
Z mqTV^kffXVn2n esA]ab/WjX%\VW4Z]ffd UhcfiTVZ*i/d ] TVZCd Z/\b/W4^;R/^d TV^'TVZ]ab/W U_W4]'TSwA^_Q/Z/U;ub/W4^_Wb/W4^
n T1c4XVnU_] X%]aWYX%]0] d2kq
W
U & ^ i^i^i^ _aq
n WX%^n e8]ab/W{X%\VW4Z] UBR/^_TVf*X%fCd2n2d ]
e 1^LTV
Z ZCiVQCcfiW4UX@R/^_TVfCX%fCd2n2d ]q
e D,^ TV
Z fejkffX%^_\Sd Z*XVn2d X%p
] TVZCqEWX%^W'd Z]aW4^_W4U_]aWihd ZubSW4]ab/W4^E]ab/WX%\VW4Z`]&c4X%ZcfiTSkR/Q/]aW9b/W4^ERgTVU_]aW4^d TV^CTV
Z XVw]aW4^*TVf/U_W4^_t1d Z/\
& ^ i^i^i^ %4&d ZoX^_Wn2X%] tVWn eUd2kRCn WuXaesu ]ab/TVQ/]bCX_tgd ZS\6]aT5u@TV^_md Z~]ab/W{U_TVR/b*d U_] d2c4X%]aWi6U_R*XVcfiWq

TVZ/Ud2iW4^&]ab/W{]ab/^W4W4pR/^d U_TVZ/W4^UR/Q/4n W5d ZokTV^_WYiW4] XVd2n2q;yBW4^_W]abSW{ZCXVd tVW{U_R*XVcfiWYd U
p \[We/ L
`6464Vs7ub/W4^_Wd U]ab/Wu@TV^n2iYub/W4^_Wd U'ZSTV];W4WcfiQ/]aWi/q0WX%^W8TVZCn ed Z`]aW4^_W4U]aWi
Z^_Q/Z/UYTSw{n W4Z/\V]ab Vs-U_Tl Vq b/W~UW4]
TSw-TVfSU_W4^_tX%] TVZ/U>ubCX%]X%\VW4Z`]>c4X%ZzfgW]aTSn2i/d U
V 6% V4 6 VVq&y"W4^_W% 64 V{cfiTV^_^_W4URATVZ*iUB]aT{]ab/W;TVf/U_W4^_tX%] TVZ5]ab*X%]"Wd ]ab/W4^0TV^*u8d2n2n
/EfgW&W4WcfiQ/]aWijad2q Wq2s]ab/W0XVd2n W4^9UXae1d Z/\%*u d2n2nfgW&W4`WcfiQ/]aWiSVaUd2kffd2n2X%^n esA 6 {cfiTV^_^W4U_RgTVZCiU

n/

`7a_

fi " |qxq}t~"}s1|

]aTff]ab/WXVd2n W4^;UXae1d Z/\[%@u8d2n2nfAWW4WcfiQ/]aWi/Vq b/WU_TVR/b*d U_] d2c4X%]aWiffURCXVcfiWYcfiTVZ/Ud U]aUTSw7]ab/WYwTVQ/^&^_Q/Z/U
``6aa4)64aa4`64aa4`64aui

w"TV]aW']ab*X%]9]ab/W4^Whd U-Z/T8^_Q/Z5u ]abjTVfSU_W4^_tX%] TVZ464asUd ZCcfiW;]abSW0aXVd2n W4^&u8d2n2nZSTV]"]aWn2ng]abCX%]@b/W
u8d2n2n)fgWW4WcfiQ/]aWi/q
r6c4cfiTV^i/d Z/\h]aTh]ab/WU_]aTV^_es]abSW;R/^d TV^.D^, Zj]abSW'Z*XVd tVW'URCXVcfiWbCX%UZ1^8h VVhwTV^[vq
Z D^TVZ]ab/W{^Q/Z/U{d U0Z/TV]cfiTSkRCn W4]aWn eU_RgWc4d /WihfeY]abSW{U_]aTV^_eqLfiZRCX%^_] d2cfiQ*n2X%^s
b/W5wQCn2n1i/d U]a^d f/Q/] TV
u@WX%^W9Z/TV]L]aTSn2i;]ab/W-R/^_TVf*X%fCd2n2d ]e{u ]ab u0bCd2cbh]ab/W-aXVd2n W4^*u d2n2n UXaej9X%ZCih"d2w/&u8d2n2nZ/TV]CfgW@W4`WcfiQS]aWi/q
W^W4]aQ/^_Z]aTff]abCd U0RATSd Z]8d ZoDgXVkRCn W5Vq2Vq
J
9G K e

K"NNJ

rvRCX%^_] d2cfiQCn2X%^n eUd2kRCn W{UW4]a] Z/\md U;ub/W4^_W]ab/WYX%\VW4Z];TVf/UW4^_tVW4U0TV^6n WX%^_Z/U;]abCX%]0]ab/WW4`]aW4^_Z*XVnu@TV^n2i
Ud Z[U_TSkWU_W4]
vqVTV^6Ud2kRCn2d2c4d ]esu@WX%UU_QCkWj]ab/^_TVQ/\VbSTVQ/]8]ab*d U6RCX%RgW4^8]abCX%]h]ab/WX%\VW4Z]
kffX%VW4U{TVZ*n e~TVZ/WhTVf/U_W4^t)X%] TVZCs"X%ZCimkffX%VW4UYd ]ffX%]]ab/Wh/^_U_]'U_]aW4RTSwD]ab/Wh^_Q/ZCq bQ/Us7]ab/WhU_W4=
] vTSw
RgTVU_Ud fCn W TVf/U_W4^t)X%] TVZ/U5cfiTVZSUd U_]aUTSw1Z/TVZSWkR/]e~U_Q/f/U_W4]aUTS!
w vq bQ/Us*X%Z`e^_Q/
Z lc4X%ZmfgW8u0^d ]a]aW4Z
X%
U

a*ub/W4^_0
W U-]ab/WhXVcfi]aQ*XVnu@TV^n2iX%ZCij U{XZ/TVZSWkR/]e5U_Q/f/U_W4]9TSC
w vq-yBTu9W4tVW4^F

iTW4U&Z/TV]&Z/WcfiW4U_UX%^d2n emcfiTVZSUd U_]&TSwg]ab/WZ/TVZ/WkR/]eYU_Q/f/U_W4]aU&TS
w vq-PTSkWU_Q/fSU_W4]aUkffXaejZ/W4tVW4^fgW
TVf/U_W4^tVWi/q-TV^9W41XVkRCn WsAd ZDgXVkR*n W8Vq2Vs7hd U9Z/W4tVW4^"]aTSn2i]abCX%]Bb/W0u8d2n2nfgW&W4`WcfiQS]aWi/sUTY 46
UZ/TV]&TVfSU_W4^_tVWi/q*WYX%UU_QCkW]abCX%]]abSW6X%\VW4Z] U'TVf/U_W4^_tX%] TVZ/U8X%^_W5XVc4cfiQS^X%]aWsDd Z]abCX%]d2wS]ab/WffX%\VW4Z]
TVf/U_W4^tVW4U $d ZX^_Q/
Z /s]ab/W4Z]abSWYXVcfi]aQCXVnu@TV^n2id
Z U Zoq bCX%]d Us)u@WYX%U_U_Q*kW]abCX%]8XVn2n/^_Q/Z/U
X%^_WYTSwE]abSWwxTV^
k

a0u0b/W4^_
W q5Zz11XVkRCn WVq2Vs-XVc4cfiQ/^XVcfied U W4ZCwTV^cfiWifem]ab/W
^_W QCd ^_WkW4Z]]abCX%]0^_Q/Z/U-bCX_tVW]ab/W5wTV^k
6 `
aaq
bSW8TVf/UW4^_t)X%] TVZTV^ffd ZCwTV^kffX%] TVZTVfS] XVd Z/WimiT`W4UZ/TV]bCX_tVWh]aTjfgW W4gXVcfi] n elTSwg]ab/WwTV^k ]ab/W
XVcfi]aQCXVn/u9TV^n2id U Zo8VqBfi]U_
Q vcfiW4U]abCX%]hd ]8d U0W QCd tXVn W4Z`];]aT5U_QCcObXU] X%]aWkW4Z`] q b*d U8d U0]ab/Wjc4X%U_W
ZfgTV]abm]ab/WffTVZ]ey'XVn2nAR/Q/4n WX%ZCiY]ab/W6]ab/^_W4W4pRS^d U_TVZ/W4^_UR/QS4n Wq5TV^{W41XVkRCn Ws"d Zl]abSW ]abS^_W4W4p
R/^d UTVZ/W4^_U R/Q/4n Ws*fgWd Z/\~]aTSn2i]abCX%]u8d2n2nEfgW5W4WcfiQ/]aWid U W4U_U_W4Z] d2XVn2n eoW QCd tXVn W4Z`]h]aTlTVf/U_W4^_t1d Z/\
6 5Wd ]ab/W4^ 8TV^8@u8d2n2nZSTV]fgWW4`WcfiQ/]aWiSaq
fiZz]abCd UU_W4]a] Z/\Ss0u9Wc4X%Z$X%U_|ub/W4]ab/W4^sXVw]aW4^TVf/U_W4^_t1d Z/\s]ab/WX%\VW4Z`]}c4X%Z$cfiTSkRSQ/]aWmb/W4^
RgTVU_]aW4^d TV^TV
Z f`ecfiTVZCi/d ] TVZCd Z/\TVZq TVQ/\Vb*n eU_RgWX%gd ZS\Ss/]abCd UffXVkTVQSZ`]aU]aTX%U_gd ZS\ju0b/W4]ab/W4^
TVf/U_W4^tgd Z/\ffd U]ab/W&UXVkWX%U;iSd UcfiT tVW4^d ZS\{]abCX%]0d UB]a^Q/Wq bCd U0kffXae Z/TV]fgW-]ab/W{c4X%U_W8d Z6\VW4Z/W4^XVn
TVf/U_W4^tgd Z/\~TV^fgWd Z/\]aTSn2i kffX_ec4X%^_^_ekTV^_Wd Z*wxTV^k5X%] TVZ}]abCX%ZoQ/U]{]ab/WwXVcfi]]abCX%]j U8]a^_Q/Wq
TV^W41XVkRCn WsLd2wBwTV^;U_TSkW8^_WX%UTVZ[hZ/TuU;]ab*X%];]ab/W aXVd2n W4^'u@TVQCn2i5Z/W4tVW4^UXaed2wAb/WcfiTVQCn2i5b/Wn R
]U_T]ab*X%] s&d ZoRCX%^] d2cfiQCn2X%^s&d2w&ffX%ZCiu8d2n2nEfgW5W4WcfiQ/]aWi/s*]ab/W4Zb/WYu8d2n2n9iVW4/ZCd ]aWn e>UX_e|asC]ab/W4Z
b/WX%^d ZS\m8ad2q Wq2sgTVf/U_W4^_t1d Z/\ 6 4V9]aWn2n U5k0QCcbkTV^W8]abCX%Zm]ab/WwXVcfi]']ab*X%]']abSW8]a^_QSW8u@TV^n2imd U
TVZ/WTSg
w TV0
^ %q]&UX_e`U;]abCX%]&]ab/W{]a^Q/Wu9TV^n2i~kQ/U]fgQ
W awTV^ d2wS]ab/W]a^_Q/Wu@TV^n2ihu@W4^_Z
W s)]ab/W
aXVd2n W4^0u@TVQCn2ihbCX_tVWUXVd2iaq
fiZ8]abSW9^_Wk5XVd ZCiW4^*TSw]abCd ULRCX%RgW4^Eu@WX%U_UQCkW"]ab*X%[
] UL/Z*d ]aWq9TV^LW4tVW4^_e8UcfiW4ZCX%^d T'u@WcfiTVZ/Ud2iW4^
u@W5iW4/Z/W5X'U_W4]&TSwSRgTVU_Ud fCn WTVfSU_W4^_tX%] TVZ/
U s1cfiTVZSUd U_] Z/\ffTSwSZ/TVZ/WkR/]ejU_Q/f/UW4]aU&TS
w vq&TV^;\Sd tVW4Z
X%ZC
8s]ab/W{U_W4]&TSw7^_Q/ZS[
U U0]abSW4ZoiW4/Z/Wi ]aT5fAW]abSWU_W4]



V aF06

>8i

{d tVW4ZTVQ/^X%U_U_QCkR/] TVZ/U]abCX%]&]abSW{U_] X%]aWYiTW4UZ/TV]cObCX%Z/\VW{TtVW4^0] d2kWjX%ZCih]abCX%]&]ab/W5X%\VW4Z] 5
k X%VW4U
TVZCn e|TVZ/WTVf/U_W4^_tX%] TVZCs;]ab/WmU_W4] TSw&^_Q/ZSUc4X%ZfgWmt1d W4u9WiX%UX~UQ/f/U_W4]TSw
8q
bCd2n W

`7a7

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

Q/U_]] X%1d Z/\ ]aTmfgWmX5U_Q/f/UW4]TSw
u@TVQCn2iUn2d \Vb] n e}Ud2kR*n2d2wxe>]abSWYR/^W4U_W4Z`] X%] TVZbSW4^_Ws&d Z
\VW4Z/W4^XVn2s1u9W~cfiW4^_] XVd Z*n euX%Z`]]aT}XVn2n Tu$U_W Q/W4ZCcfiW4UTSwETVf/U_W4^t)X%] TVZ/Uq5uLTVZ/Ud2iW4^swTV^W4gXVkR*n WsBX%Z
l1piTTV^0tVW4^_Ud TVZTSwA]ab/WjffTVZ]eoy'XVn2nSR/^_TVfCn Wkffsub/W4^_WffTVZ]eTVRgW4Z/U XU_W Q/W4ZCcfiW8TSwiT`TV^Uq2 bCd U
w^XVkW4u@TV^_YW4]aW4ZCiU;ZCX%]aQ/^XVn2n ej]aT5]ab*X%]U_W4]a] Z/\Sq
b/W4Z/W4tVW4^@u9W0U_RgWX% TSwgX8i/d U_]a^d f/QS] TV
Z 1^9TVW
Z lsu9W d2kRCn2d2c4d ] n eX%U_UQCkW&]abCX%]9]ab/WR/^_TVf*X%fCd2n2d ]e
TSw1X%Ze6U_W4]9TVZYubCd2cObYu@W cfiTVZCiSd ] TVZd U@U_]a^d2cfi] n e5\V^_WX%]aW4^-]abCX%ZlV[
q ~/W4H
] X%ZCZ
WfAW;]u@T8^X%ZCiTSk
tX%^d2X%fCn W4U6TV
Z ls*ub/W4^
W W Uh]ab/WXVcfi]aQCXVnCu9TV^n2i[X%ZC
U ]abSWYTVf/UW4^_tVWiW4tVW4Z`] q bQ/Us-wTV^

aaC
W / X%ZCW
8qh{d tVW4ZXYiSd U_]a^d f/Q/] TV
Z 1^TVZm^_Q/ZS
U ls7u@WiVW4Z/TV]aW
f
e D,^ ]ab/W}kffX%^_\Sd ZCXVn0i/d U_]a^d f/QS] TVZTS!
w W>s;X%ZCilf
e D^ ]abSW}kffX%^_\Sd ZCXVniSd U_]a^d f/Q/] TVZTSK
w q
TV^'W4gXVkRCn WsCwTV
^ C4{
vC
18^ }u 8;d U;U_bSTV^_]8wTVm
^ D^ W
0X%ZC
1,^ >u fi;d U;Ub/TV^_]8wTV^
1^
>8aq
~/W4.
] D^-fAW6X;R/^d TV^-TV
Z X%ZCin W4Z
] 1,^ D^u 8EfgW']ab/WRgTVU_]aW4^d TV^{XVwx]aW4^&TVfSU_W4^_t1d Z/\q
b/W5k5XVd ZoQSW4U_] TVZ~u@WYX%U_d Z]abCd U;RCX%RgW4^d UQ/ZCiW4^&u0bCX%]cfiTVZCi/d ] TVZ/U;u9WbCXatVW

1^ u


D^,>u

aV

wTV^0XVn2n?vq bCX%]d Usu@W&uX%Z`]]aTZ/T uQSZCiW4^*ubCX%]cfiTVZCi/d ] TVZ/U"]ab/W-RATVU]aW4^d TV^ Z*iQCcfiWi;f`e
1^ gc4X%ZfffgWcfiTSkR/Q/]aWi6w^_TSk]ab/W&R/^d TV^TVZ fecfiTVZCiSd ] TVZCd Z/\TVZ6]ab/W&TVf/UW4^_t)X%] TVZ*q@aDgXVkRCn WVq2
fgWn u\Sd tVW4UhXffcfiTVZ*cfi^_W4]aWjc4X%U_Wq2WU]a^_W4U_U0]ab*X%]1^8X%Z*i1^,X%^_Wji/d U_]a^d f/QS] TVZ/UTVZms)ubCd2n WD^,
X%ZCZ
D^ X%^_W;iSd U_]a^d f/Q/] TVZSUDTV
Z TVf/] XVd Z/Wif`e6kffX%^_\Sd ZCXVn2d X%] TVZw^_TS
k D^9X%ZCZ
D^ ^_W4U_RgWcfi] tVWn e1aq
w"TV]aW]abCX%]8aV0d U0W QCd t)XVn W4Z] n eU_] X%]aWiX%U

D^ W

8

1^W W

>wTV^8XVn2n]>i

aV

aVW QCd tXVn W4Z`] n esLaVad Uhc4XVn2n Wi6]ab/WmrhcfiTVZCi/d ] TVZCVqB]8c4X%Z~fAW5cObCX%^XVcfi]aW4^d 4WimX%U8wTSn2n u0U .
e& z8@D @oK||gG}Lo>7|+1^[
:
+/fffi6,)on
/



\b"a\

/h*

zO%}9q1^ # ; 1^W D^ W W 8! ff
>m
zu}* 'ff n /\W 9/_` /*G'ffn S\ n #W>mff&X >
Oz u}D
^ W 1^ W 8R 6?|)~V1^W
!
h

zO%}D ^ W h
1 ^ W # m!h
6 ,)
^W h

!
/qD ^ W 2

TV^'cfiTSkRCn W4]aW4Z/W4U_UaX%ZCi'fgWc4X%Q/U_Wd ]0d UQ/UWwxQCn)wTV^TVQ/^0n2X%]aW4^ b/W4TV^_Wk Vq2VasVu@W&R/^_Ttgd2iVWX@R/^_TTSw
TSw b/W4TV^_Wk Vq2ffd Z]ab/WYX%RSRAW4Z*i/d gq
bSW8/^U_] cfiTVZCiSd ] TVZd Z b/W4TV^_WkVq2d U'Q/U_]haVaq b/W ]abCd ^ilX%ZCimwxTVQS^_]ab[cfiTVZ*i/d ] TVZ/UQSU_] d2wxe
]ab/WZ*XVkW%cfiTSX%^_U_W4Z*d Z/\X%]"^X%ZCiTSkffVqCfiZ`]aQ*d ] tVWn es/^_U_]U_TSkWu9TV^n2.

U"^_WXVn2d 4Wi/sgX%ZCi]ab/W4Z

%CGff!D
fi1 Y4VfiYR!fififi ?-%7RxXfi ;4gfiFXxfiX9AAK Ix[ VfiE/%fix% 18D`
_[ , ff x ,?<h7 X5XA _ <( %ff@A _[
?O 2K^x fi _[ x ff9!4V fi2! Xx e<hA ^5XA
_[ E9o-7/ Q fi U8 4-_ ff x ff J 8NN P?9:; fix 0 <h %ff@AV x x %'D _[ fi 44-
, 6: F ff Hff 4D 8DM1T 8 fi:% x J VI ?A X5 _ P? Y/ fi 2%@ x K V fi ?D 8m
: E fiV , ff ff@
, 6: UI E fiK J 8NN 8P J VI F , @ _
P R V fi ff 2D fiX ox 6 #x 1 6 x %fiff,
`7a

fi " |qxq}t~"}s1|

U_TSkW%cfiTSX%^_U_W4Z*d Z/\kWcb*X%ZCd UkffliWc4d2iW4U6ub*d2cb[W4tVW4Z`] U_Q*cb[]abCX%] vd U6^W4tVWXVn Wi
]aT]ab/WX%\VW4Z`] q b/WW4tVW4Z]5 Uffc4XVn2n WiX%cfiTSX%^U_W4ZCd Z/\S8TSwUhq b/W ]abCd ^ilX%ZCiwTVQ/^_]abcfiTVZCi/d ] TVZSU
W4AWcfi] tVWn eYUX_eff]abCX%]@]ab/W'R/^_TVfCX%fCd2n2d ]eff]abCX%;
] $d UcfiTSX%^U_W4Z/Wi8]aTvd U-]ab/W;UXVkW6wxTV^XVn2n| >8q bCd U
kWX%Z/U;]abCX%]0]abSW%cfiTSX%^_U_W4Z*d Z/\lkWcObCX%ZCd Uk55d U'U_QCcb]abCX%]0]abSW{R/^_TVfCX%f*d2n2d ]eTSwATVf/U_W4^tgd Z/\ld U'Z/TV]
X%AWcfi]aWifffeY]abSW{U_RgWc4d Cc@tXVn Q/W{TS
w v>]abCX%]uX%U0^_WXVn2d 4WiSq
fiZ|]abSW^_Wk5XVd ZCiW4^jTSw-]abCd URCX%RgW4^s@ub/W4Zu@WUX_e 1^UX%] U/W4
U Lrh;VsBu@WkWX%Zz]abCX%
] 1^
UX%] U_SW4U5cfiTVZCi/d ] TVZ|aXV@TSw b/W4TV^_Wk Vq2TV^s1WQCd tXVn W4Z] n es"X%Ze~TSw1]abSW6TV]ab/W4^]ab/^_W4WcfiTVZCi/d ] TVZSU
5
8q bQ/Us 1^ UX%] U_/W4
U Lr60~kWX%Z/U8]ab*X%]cfiTVZCiSd ] TVZCd Z/\d Z]ab/WYZCXVd tVWYURCXVcfi
W
cfiTSd ZCc4d2iW4Uu8d ]abvcfiTVZCi/d ] TVZCd ZS\zd Z|]ab/W~U_TVRSbCd U_] d2c4X%]aWioU_RCXVcfi
W u8d ]abR/^_TVf*X%fCd2n2d ]eVq bS
W Lr6
cfiTVZCi/d ] TVZYW4`R*n2XVd Z/U"ubecfiTVZCi/d ] TVZCd ZS\Yd Z5]ab/W0ZCXVd tVW0U_R*XVcfiW8d U"Z/TV]'X%R/R/^_TVR/^d2X%]aW8d Z5]ab/W ffTVZ]e~y'XVn2n
R/Q/4n WTV^6]abSW]ab/^_W4W4pR/^d UTVZ/W4^_U6R/Q/4n Wq}WocfiTVZ/Ud2iW4^6]ab/W]ab/^_W4W4pR/^d U_TVZ/W4^_U6R/QS4n W}d ZiW4] XVd2n2{X
Ud2kffd2n2X%^hX%ZCXVn eUd U X%R/R*n2d W4U]aT5TVZ`]e}y;XVn2n2q
eE fiZ}]ab/W5]ab/^_W4W4pR/^d UTVZ/W4^_URSQ/4n WsDu0bCX%]Yd U` U8R/^d TV^Yi/d U]a^d f/Q/] TVZ1^TVZlzfiZ
n/ p \
11XVkRCn W6Vq2;u9WffX%U_UQCkWi]abCX%]9]abSW6k5X%^_\Sd ZCXVnAi/d U_]a^d f/QS] TVZeD^ TVZ U@QSZCd2wxTV^kffqDr'RCX%^_]w^_TSk
]abCd UR
D^6d UQ/ZSU_RgWc4d /Wi/
q wBTu U_Q/R/RgTVU_W{]ab*X%]66TVf/U_W4^_tVW4U5 6 ]abSW aXVd2n W4^UX_e`U5Vaqw'XVd tVW
cfiTVZCi/d ] TVZ*d Z/\>u@TVQCn2in WXVi~]aT|XViTVRS]6]ab/W}i/d U_]a^d fSQ/] TV
Z 1,^ }u fi 6 Vaq bCd Ui/d U_]a^d f/Q/] TVZ
UX%] U_SW4
U 1,^ } fi 6 V VVVqPTVR/bCd U_] d2c4X%]aWi|cfiTVZCi/d ] TVZCd ZS\n WXViUl~]aT|XViTVR/]6]abSW}i/d U_p
]a^d f/QS] TVZ 1,^ 1^ u 6 VaqCelR*X%^_]5ai/@TSw b/W4TV^_Wk Vq2VsDZCXVd tVWmcfiTVZCi/d ] TVZCd ZS\od U
X%R/R/^TVR/^d2X%]aW>ad2q Wq20
D^ 1,^ }u fi 6 VaTVZ*n ed2w]abSWjaXVd2n W4^d U6W QCXVn2n en2d VWn e[]aTUX_ejd Z
fgTV]ab>u@TV^n2iZ
U X%ZC
qP/d Z*cfiW6]ab/WffaXVd2n W4^jk0Q/U_]UX_e]abCX%]Y;u8d2n2ngfgW6W4WcfiQ/]aWi}d Z>u@TV^n2
s9d ]
wTSn2n uU{]ab*X%
] D^ W `6
4 Vq b`Q/Us*cfiTVZ*i/d ] TVZCd Z/\}d UffX%R/R/^TVR/^d2X%]aWhTVZCn ed2w
]ab/WjXVd2n W4^ U RS^_TV]aTgcfiTSn-d U U_QCcObo]abCX%]hb/WmiW4/ZCd ]aWn e}UXaeUffd
Z s&d2q Wq2sLW4tVW4Zd2w*fgTV]abzYX%ZCioYX%^_W
W4WcfiQ/]aWi/q&CQ/];d2w`]abCd Ud U"]ab/W8c4X%UWsubSW4Zff]ab/WaXVd2n W4^"UX_e`UsScfiTVZCiSd ] TVZCd Z/q
\ D,^ TVZl 6 4{d US
X%R/R/^TVR/^d2X%]aWsVUd ZCcfiW0]ab/W4Zm'`ZST uU9]abCX%]"b/W0u8d2n2nVfgWW4WcfiQ/]aWi/q bSWu9TV^n2i5c4X%Z/Z/TV]9fgV
W s7wxTV^9]ab/W4Z
]ab/W'XVd2n W4^-u@TVQCn2i8bCXatVWUXVd2ijq b/W4^_WwTV^_Ws1/W
ph"
+
B7fi,EasAcfiTVZCi/d ] TVZ*d Z/\
Z]ab/WZCXVd tVWU_R*XVcfiWYc4X%Z/Z/TV]8cfiTSd Z*c4d2iWu8d ]ab[cfiTVZCi/d ] TVZCd ZS\d Z]ab/W{UTVR/bCd U_] d2c4X%]aWi5U_RCXVcfiWYwTV^0fgTV]abTSw
bCd U^W4U_RgTVZ/U_W4Uq
b WwTSn2n u8d Z/\W41XVkRCn W6Ub/T uU]abCX%]5d Zm\VW4Z/W4^XVn2sBd ZmU_W4]a] Z/\VU{TSwD]ab/Wh]eRAWX%^d Ud Z/\>d Zl]abSW5TVZ]e
/
y'XVn2n7X%ZCi]ab/W;]ab/^_W4W4pR/^d UTVZ/W4^_U"RSQ/4n Ws`]ab/WLr6zcfiTVZCi/d ] TVZc4X%ZTVZCn e6fgWUX%] U_/Wijd ZjtVW4^_e6U_RgWc4d2XVn
c4X%U_W4U
e
g PQ/RSRATVUW&]abCX%]# V & 4 + Vs7X%Z*ifgTV]abl & X%ZCiY + X%^_W;TVf/U_W4^_tVWiu8d ]abjRgTVUd ] tVW
n/ p \
R/^_TVf*X%fCd2n2d ]eq bCd UYd U{]ab/W~c4X%UWwxTV^fATV]abffTVZ]ey'XVn2n"X%ZCij]ab/W6]abS^_W4W4pR/^d U_TVZSW4^_U{R/Q/4n Wq2 b/W4Z
]ab/
W LrhcfiTVZCi/d ] TVZ b/W4TV^Wk Vq2Vac4ac4X%Z/Z/TV]Yb/TSn2iwTV^5fgTV]abv & X%ZCi| + QSZCn W4U_UD^ W

&k + hd U6Wd ]ab/W4^YTV^~VqTV^5U_Q/R/RgTVU_WY]abCX%]1^ & Vs1^ + Vs;X%ZCi
D^ W &Kk +

Vq8vd ]ab/TVQS]Yn TVU_U{TSwE\VW4Z/W4^XVn2d ]esE]abSW4^_W~d U{U_TSkW
W & & + X%ZCi
+ [ &!k + U_QCcbl]abCX%=
] 1^ W &
jX%ZC
1^ W + [
Vq P/d ZCcfiWTVfSU_W4^_tX%] TVZ/U5X%^_W
XVc4cfiQ/^X%]aWsAu@WkQ/U];bCX_tV
W D^ W & & Vq42
w Lr6|b/TSn2iU6wxTV^ff & s7]ab/W4Zlu9Wk0Q/U_]
bCX_tV
W 1^ & W + VqCQ/] ]ab/W4
Z 1^ + W + VqCQ/]hUd ZCcfiW
1^ + [
VsLd ]6wTSn2n u0U]abCX%];]abSW4^_Wd UU_TSk
W . + U_QCcObm]abCX%m
] D^ W . [
X%ZCi
1^ + W . 2 Vq bCd U cfiTVZ`]a^XViSd2cfi]aU;]ab/W
W rhcfiTVZCi/d ] TVZCq
PTYub/W4ZiVT`W4
U Lr6bSTSn2i/ b/WR/^W4tgd TVQ/U'W41XVkRCn WW4bCd fCd ]aWimXYcfiTSkfCd Z*X%] TVZTS2
w X%Z*

wTV^
ubCd2cObLr6c4X%ZTVZCn ejfAW'UX%] U_SWid Z%iW4\VW4Z/W4^X%]aW5c4X%U_W4UqLfiZ]ab/WZ/W4]&U_Wcfi] TVZCs)u@WU_bCXVn2nU_]aQCiVe
]abCd U8 Q/W4U_] TVZ[wxTV^ X%^_fCd ]a^X%^_ecfiTSk0fCd ZCX%] TVZ/U'TS
w X%ZC
vq

`7

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu


9G 1 "NN e

fiZo]abCd U U_Wcfi] TVZCsu9WjR/^_Ttgd2iWYU_TSkWcbCX%^XVcfi]aW4^d X%] TVZ/UffTSwLu0b/W4Z}]ab/WeLr6cfiTVZCi/d ] TVZ[b/TSn2iUs-wTV^
/ZCd ]a
W X%ZCivq@&Q/^B^_W4U_Q*n ]aU"W4]aW4ZCi{WX%^n2d W4^9^_W4UQCn ]aU"TSw1{d2n2n2stX%ZliW4^#~EXVX%ZCsAX%ZCiffBTVfCd Z/U'aVVVVaq
WS^_U_]W4bCd fCd ]8X{Ud2kRCn WUd ]aQCX%] TVZd Z~ub*d2c
b rhd U0\VQCX%^X%Z`]aW4Wi5]aT5b/TSn2iSsEX%ZCihu@WUb/T u]abCX%]
]abCd Ud UB]ab/WTVZCn ehUd ]aQ*X%] TVZmd Zffu0bCd2cbd ]d UB\VQCX%^X%Z]aW4Wi]aTb/TSn2i/qEW0]abSW4Z5U_b/Tu]abCX%] sAwxTV^'X%^_fCd ]a^X%^_e
X%ZC
vs7u9Wc4X%ZcfiTVZ/U_]a^_Q*cfi]6X%p VtXVn Q/Wi>kffX%]a^d owx^TSk$u0bCd2cbX8U_]a^_TVZ/\Z/WcfiW4UUX%^_eocfiTVZCiSd ] TVZ
wTV
^ Lr6v]aT>b/TSn2ic4X%ZfAWTVf/] XVd ZSWi/q]6]aQS^_Z/U6TVQ/]6]abCX%] s'd ZU_TSkW}c4X%U_W4U5TSw'd Z`]aW4^W4U_] #
Lrh U
^_TVQ/\Vb*n eU_RgWX%1d Z/\S@\VQCX%^X%Z]aW4WiS]aTb/TSn2iW41cfiW4R/]Yd Z%iW4\VW4Z/W4^X%]aW6Ud ]aQCX%] TVZSUqLd ZCXVn2n es1u@W
Z]a^_TgiVQCcfiW5X'Z/W4uR/^_T1cfiWiQ/^XVn26cbCX%^XVcfi]aW4^d X%] TVZlTS
w Lr60u@WR/^_Ttgd2iW5X6kWcbCX%ZCd UkvU_QCcOb]abCX%]
X i/d U_]a^d f/QS] TV
Z 1^c4X%ZjfAW;]ab/TVQ/\Vb]9TSw1X%UX%^d Ud ZS\w^_TSk]ab/W kWcObCX%ZCd Ukd2w1X%ZCi{TVZCn ed2U
w D^"UX%] U_SW4U
Lr60q

:e

\ l!#" g

kn nql ^`\b\ r

^`



p%r

W /^_U_]8cfiTVZSUd2iW4^']ab/W8TVZ*n eUd ]aQCX%] TVZmub/W4^_WqLrhvd U'\VQCX%^X%Z]aW4WiY]aTYb/TSn2iSEd2w7]ab/W U_W4]aUhd ZX%^_W
RCXVd ^_u U_W5i/d U_TSd Z`] q

:e/ * x0yS
E,, )/QD^ff
/{6;`,5(/-,_'6

[<a"ZW " ^ " l

//+5

bCX%]bCX%R/RgW4Z/U6d2w1]abSW6U_W4]aU5d
Z X%^_WhZ/TV]RCXVd ^u8d U_Wi/d U_TSd Z`] r'^_Wh]ab/W4^_WhU_] d2n2nc4X%UW4UYacfiTSkf*d p
ZCX%] TVZ/U;TSw8sUvs1X%ZCii/d U_]a^d f/Q/] TVZ/U&TVZmLub/W4Zrhb/TSn2iU b/W4^_WffX%^_Wsf/Q/]&]ab/W4eX%^_WY QCd ]aW
U_RgWc4d2XVn2q

:e

\ l!#$

n&%



p%r

WZ/T uzR/^W4U_W4Z`]X n WkffkffX0]abCX%]-R/^_T t1d2iW4UX;Z/W4uvcObCX%^XVcfi]aW4^d X%] TVZTSwCLr6d ZY]aW4^kU-TSwDXUd2kRCn W
VV%pkffX%]a^d 1q b/Wn WkffkffXXVn2n TuUQ/U*]aT6iVW4]aW4^kffd Z/WwxTV^kffX%ZeYcfiTSk0fCd ZCX%] TVZ/U"TSw]X%ZCisu0b/W4]ab/W4^
Xffi/d U]a^d f/Q/] TVZTV
Z W41d U_]aU0]abCX%]0UX%] U_/W4m
U LrhX%ZCih\Sd tVW4UhcfiW4^_] XVd Zu@TV^n2iU0RATVUd ] tVW{R/^_TVfCX%f*d2n2d ]eq
Ld ~X&UW4!
] TSw^Q/Z/Usu0b/TVU_Wu@TV^n2iUX%^_W8d Z5UTSkW0/Z*d ]aWU_W4Q
] X%ZCi{ub/TVU_W0TVf/U_W4^_tX%] TVZ/UcfiTSkW
w^_TSkU_TSkWSZCd ]aW-U_W40
] V & ^ i^i^i4 j VqCWUX_eh]abCX%(
] ' U0X%W
Z fi|
p^Wn2X%] tVW0]aW
X%ZCi
*fisEX%ZCi 5
|
W 9
d2)w 'b*X%U0]ab/WYwTV^k &Rk i^i^i k _jSs/ub/W4^_WWXVcO
b &*-d U;Wd ]abSW4^ +
**TV^ (
''q
,#
q ~/W4(
] . /
' & ^ i^i^i '10fgW{]abSW{U_W4]0TSg
w YpX%]aTSkU;^Wn2X%] tVW ]a
X%ZC
8qBWc4X%Z~]abCd Z/
TS2
w . X%U5XR*X%^_] ] TVZTSw1]ab/W u@TV^n2iU5XVc4cfiTV^i/d ZS\j]aTu0bCX%]hc4X%ZfgW TVf/U_W4^_tVWiSq u9Tu@TV^n2i.
U & X%ZCi
+ X%^_Wd Zl]abSW8UXVkW UW43
] '1*@4
. d2w1]abSW4^_WX%^W ZSTYTVf/UW4^_t)X%] TVZSU]abCX%]ffi/d U_] Z/\VQCd Ubl]abSWkffA]abCX%]ffd Us
]ab/W4^_Wd U'Z/TYTVfSU_W4^_tX%] TVZ$
UQCcbl]abCX%#
] & X%ZCW
+ [

q r"W4SZ/W]ab/6
W 5lkffX%]a^d 8
7
u8d ]abW4Z]a^d W4:
U 9*(X%U wxTSn2n TuU

9*(




2d w)'*R
TV]ab/W4^_u U_Wq

aV

W6c4XVn2n;7~]ab/WxQyB %,/ph,awTV^#vX%ZCivaq;w"TV]aW']abCX%]9WXVcObY^T u-d Z<7}cfiTV^_^W4U_RgTVZCiU
]aTXQ/Z*d2Q/W X%]aTSkd
Z .u@Whc4XVn2n`]abCd U@]ab/WhX%]aTSk ,%`SS8]aT8^T u,q bCd UkffX%]a^d ~aXVcfi]aQCXVn2n es
]aU0]a^X%Z/U_RgTVU_W*uX%U0/^_U]d Z]a^_T1iQCcfiWifSQ/]wxTV^ Xffi/d AW4^_W4Z`]0R/Q/^_RgTVU_WCfem{d2n2n/W4]8XVn2qDaVVVVaq

`7&=

fi " |qxq}t~"}s1|

n/

W4]aQS^_ZCd Z/\6]aTDgXVkRCn WYVq2Vs)]ab/WqLrh;XVcfi]aW4^d Z/\mkffX%]a^d >d U\Sd tVW4Z~fe

e/

p \














ub/W4^W ]abSW~cfiTSn QCkZ/UYcfiTV^^_W4U_RgTVZCiY]aTo & X%ZCi} + X%ZCij]ab/W6^_TuUYcfiTV^_^W4U_RgTVZCiY]aT]ab/W6]ab/^_W4W~X%]aTSkU
& + 4 & k + X%ZCi + & qTV^ffW41XVkRCn WsB]ab/W>wXVcfi]5]abCX%]hW4Z`]a^_e>9 .0& TSw"]abCd U~kffX%]a^d zd U
ZCi/d2c4X%]aW4U;]abCX%]8 & c4X%Z/Z/TV]fgWTVf/U_W4^tVWid2w7]ab/W5XVcfi]aQCXVnu@TV^n2im U8d Zo +? & q
fiZ]ab/WYwTSn2n u8d ZS\ln WkffkffXVs?AC@ B iW4ZSTV]aW4U0]ab/W{]a^X%Z/U_RgTVU_WTSwS]ab/WY^_Tu8tVWcfi]aTV^4A @ sDX%Z*i ff
@ iW4Z/TV]aW4U;]ab/W
^_TutVWcfi]aTV^hcfiTVZSUd U_] Z/\YTSwCXVn2nD%Uq
"G ,)/'|n | , n /0
E g

n eE
xQyB ,/ph%o;.
/

Dh\

/j%
Q*/j
F7,8

zO%} E ;D^qffSj,, )}|n [

/~
17 ,ffp ,Y / j + /fi|p
7&,%`SS}}|pG'1^W H'' JILK-/ln ,
@ & ^i^i^i jS , / 1^ FW h WD^W zF#0S
K 5x0yV N7 2A @ B @ B
,ff_ 9 2i^i^iul;#9D^ff,M-

zu} E 7 ,6p ,ff//ffGff4 , @Ghfffi{667HS
2O QP RTS 6ff9G
,, )/{|n #
, `%@,%`SSYU7 WV ffo

QP RTS / XgXg>Y''K Z'$,%`Shjfiz[7
@ ) 17 A_@ B @ B 9 ]? {XVn2n`XE aO QP R&S "
9ff fij7 n , \A^
@ ] ~
ff Y>%o1^l|n Z e1^, E
X zffophS-GD^m
XE})z^}1^,MK xQy/zu}D^ FW F 6S
q1^



>

!

w"TV]aW]abCX%]0fCd UBW4UU_W4Z`] d2XVn2n eXcfiTVZtVW4^_U_WTSwAaXVaq*rZCX%]aQ/^XVn Q/W4U_] TVZ6]aTYX%Ud UubSW4]ab/W4^0fCAu@TVQCn2i
U_] d2n2nb/TSn2i d2wu@W9^_W4R*n2XVcfiWi~%wxTV^XVn2nbXg !O Q

P R&S ]abSW4^_W9W41d U_]aUD ^CUX%] Uwxe1d Z/\L rhlu ]abD ^8 XE>
fe%wTV^5XVn2ni/d U_]a^d f/QS] TVZ/UfiXg
tVW4^
]ab/W4^_WhW4gd U]aU1 ^UX%] Uwe1d Z/\L r6u8d ]ab1 ^, Xg&
q2 b/W

X%Z/U_u@W4^8d U;Z/TSU_W4WYDgXVkR*n W{Sq2VfCaad2d2aq
~/WkffkffXYSq2jUXaeU6]abCX%]Xmi/d U_]a^d fSQ/] TVZ1^h]abCX%]6UX%] U_SW4ULrh X%Z*iX%]6]abSWjUXVkW] d2kWbCX%U
1^ c
''
YwxTV
^ 5 iSd 7W4^_W4Z]6X%]aTSkfi
U ' c4X%ZlW41d U_]6d2wX%ZCi6TVZ*n eod2wX5cfiW4^_] XVd ZmU_W4]'TSw_5 n2d Z/WX%^
W QCX%] TVZ/U'd W
Z lQ/Z/Z/T uZSU*b*X%U'X@U_TSn Q/] TVZ*q*Z~kffX%ZehUd ]aQCX%] TVZ/U9TSw7d Z]aW4^_W4U_]
5 ] l}Z/TV]aW0]abCX%]e5
j
kffX_ejfgW5X%Uhn2X%^_\VWYX%U Va
q w"TV]U_QS^_R/^d Ud ZS\Sn e5]ab/W4ZCsEd ZU_QCcObUd ]aQCX%] TVZ/U;]ab/W4^_W{TSw]aW4Zc4X%ZfgW5S
i/d U_]a^d f/Q/] TV
Z D^]abCX%]0UX%] U_/W4
U rh;sDX%U0u@W{U_b/Tud Z]abSW{Z/W4]U_Q/f/UWcfi] TVZCq0-Z~]ab/W{TV]ab/W4^;bCX%ZCi/s
@ iTW4U9bCX_tVW X0U_TSn Q/] TVZd g
d2w]ab/W;U_W4]"TSwW QCX%] TVZ/f
U 7 AC@ B 8
Z @ s]ab/W4Zj]ab/W;U_W4]"TSw1XVn2nU_TSn Q/] TVZ/U{wTV^kU
j X%ZCi~]ab/W5RgTVUd ] tVWjTV^_]abCX%Z]
]ab/Wmd Z`]aW4^_UWcfi] TVZTSwX%ZzX v5Z/W5U_QSf/U_RCXVcfiWad2q WqX6beRAW4^RCn2X%Z/W&TSw
j
h V q b/W4U_W"UTSn Q/] TVZ/U&X%^_W"Q/U_]D]abSW'cfiTVZCi/d ] TVZ*XVnR/^TVfCX%fCd2n2d ] W4[
U 1^
;W}F
CwxTV^-XVn2n
i/d U_]a^d f/Q/] TVZ/U&wxTV^*ubCd2cO
b rhb/TSn2iU*]abCX%]*bCXatVW@U_Q/RSRATV^]9cfiTV^_^_W4U_RgTVZCiSd Z/\]a3
7 q b/W4UWcfiTVZCiSd ] TVZCXVn
R/^_TVf*X%fCd2n2d ] W4U5kffXae]ab/W4ZmfgW8W4]aW4ZCiWiY]aT>Xi/d U_]a^d f/QS] TVZlTtVW4.
^ feU_W4]a] Z/
\ 1^ XE wTV^ffX%Z
X%^_fCd ]a^X%^_eli/d U]a^d f/Q/] TV^
Z XET tVW4^;]ab/Wu9TV^n2iVUd Z}X%]aTSkU8cfiTV^_^_W4URATVZ*i/d Z/\h]a[
7 1XVn2U
n 1^cfiTVZ/U_]a^QCcfi]aWi
Z]abCd U0uX_eUX%] Uwe
e rh;q

`77`

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

PQ*kffkffX%^d Z/\Ss/u9W8bCXatVW ]ab/W8^_WkffX%^_X%fCn WjwXVcfi]0]ab*X%]hwTV^ X%Ze\Sd tVW4ZmU_W4]0TSwX%]aTSkU. ]ab/W4^_WX%^_W
TVZCn e8]u@TRATVUUd fCd2n2d ] W4UEWd ]ab/W4^/6i/d U_]a^d f/QS] TVZ W41d U_]aUBu0bCd2cb6bCX%UD^ W <''!wTV^0XVn2nj'.
X%ZCiUX%] U_/W4
U Lr60sDTV0
^ j'iSd U_]a^d f/Q/] TVZS
U XE TtVW4^ u@TV^n2iUcfiTV^_^_W4URATVZ*i/d Z/\]aTX%]aTSkUd Zg''s
]ab/W4^_W W41d U_]aUffXYi/d U]a^d f/Q/] TVZlUX%] Uwegd ZS
\ Lr6|u8d ]abkffX%^_\Sd ZCXVn*i/d U_]a^d fSQ/] TVZTtVW4^u9TV^n2iU;W QCXVnS]aT
XE>q

:e
k

\ l!#" g

kn nql ^`\b\ rml

^`f p%r

WjZ/T uR/^_W4U_W4Z]jXff]ab/W4TV^_Wk ]abCX%]\Sd tVW4Uh]u9TmW4RCn2d2c4d ]X%Z*iWX%U_ep]aTVpcb/WcOoUQ]vc4d W4Z]jcfiTVZCi/d ] TVZSU
Q/ZCiVW4^ubCd2cO
b Lr6c4X%ZSZ/TV]0b/TSn2i5Q/ZCn W4UU]ab/WRS^_TVfCX%fCd2n2d ] W4U'TSwAU_TSkWX%]aTSkUhX%Z*i/%TV^0TVf/UW4^_t)X%] TVZSU
X%^_WVq b/W]abSW4TV^_Wk U'R/^_TtVWifffeU_b/Tu8d Z/\5]ab*X%];]ab/WcfiTVZCi/d ] TVZlTS2
w ~Wkffk5XSq2VaXVc4X%Z/Z/TV];b/TSn2i
Q/ZCiVW4^&]ab/WU_] X%]aWilcfiTVZCi/d ] TVZ/Uq
W9f/^d W n/e;^_Wc4XVn2n4U_TSkWU_] X%ZCi/X%^iiW4/ZCd ] TVZ/U@wx^_TSkn2d ZSWX%^@XVn \VW4f/^XVqEr[U_W4]DTSw tVWcfi]aTV^_:
U @ & ^ i^i^ij
o@ 0
Ulc4XVn2n Wi+/,%
//d2w9]ab/W4^_W~W4gd U_]mcfiT`^W vlc4d W4Z`]aq
U p & ^ i^i^i pT0 Z/TV]mXVn2nL4W4^_TSU_QCcb]abCX%]
0 p&*t@ * @ 7]ab/WtVWcfi]aTV^_U5X%^Wv uS +a`//{d2wA]abSW4^_W8W41d U_]6cfiT`^W vlc4d W4Z`]aU
U p & ^ i^i^i p&0Z/TV]
*sr &
0 p&*w@ * '
0
XVn2n4W4^_TS7U_QCcOb8]abCX%]
^ @ U&c4XVn2n Wi6X%Zv u/'|
pm/
*sr & @ X%ZCi *
r & p&* VqCrtVWcfi]aTVx
0 p&*t@ * @ X%ZCi 0 p&* Vq
TS(
w @ & ^ i^i^iz
@ 0 d2w/]ab/W4^_WW41d U_]{cfiT`^W vc4d W4Z]a6
U p & ^ i^i^i p&0zU_QCcOb]abCX%]
*
r &
*sr &



696 h,)/
n |
, n /Z
\b"a\
:e*{
E U
/+ e7h~xQyB ,/=ph%H#/q



V & ^i^i^i 4Fj7/%
Z

zO%}c|S `~' fi7, l4 ?}Gj6^7vSln , ^y @ & ^i^i^i j
&vu/6p S>G6ff{G6} )j ] ff}VV^i^i^i6l"
/ ~ j|h
p 8jffVV^i^i^i 6"l * fiff/%o)1^l
,MK-Yx0v
,H1 ^ ~K
/q1 ^W ^''K
V , >
|
p '
,%`SSYa}
zu}9Y
j7 ,84 , 1} GY
jfihGfi
7;
/,,+_` /0 )&/0vu/ +
a`/ S1
ff fi0B/%o )1 ^' 1MK& 'xQ
)E1 ^W

''!
{
fi|
p ',fi O`//jjfiz[}
Oz u}@
n } } S/G#
l
/,+~/_` / /;663
7v/l~% )qXE

,1XE>
Y''H
Q '%`//a}9 fi5{)/ )
,, )mXg ) j eD
^%o ) /xQy
^W '' XE>

Y'' 8|
p ',,fi O`//fia}B 1 ^
8 X a,
fi]8d U'u9Wn2n/`Z/TuZ~]ab*X%] ZoX%ZN5 q
l kffX%]a^d gsCX%] kTVU_]0>
l ^_TuU c4X%ZfAWjn2d Z/WX%^n e}d ZCiW4RgW4ZCiVW4Z`] q

fiZkffX%Zec4X%UW4U"TSwgd Z`]aW4^W4U_]'ac4wqSDgXVkR*n WSq2-fgWn u8as]ab/WZQCk0fgW4^TSwAX%]aTSkU5 Un2X%^_\VW4^"]abCX%ZY]ab/W
ZQCkfgW4^-TSwSTVf/U_W4^_tX%] TVZ/V
U l"sU_T6]ab*X%]&]ab/W4^_W5k0Q/U_]-W4gd U]&U_Q/f/U_W4]aQ
U }TSw/^_T u0UTS
w 7l]ab*X%]X%^_W5n2d Z/WX%^n e
iW4RgW4ZCiW4Z] q bQ/UsR*X%^_] fCBTSw b/W4TV^_Wk$Sq jR/Q/]aU0ZSTVZ`]a^d t1d2XVn*cfiTVZ/U_]a^XVd Z]aUTVZ]ab/Wi/d U_]a^d fSQ/] TVZ/U
]abCX%]UX%] Uwx
e Lr60q
bSW8^_W QCd ^_WkW4Z`]ffd ZmRCX%^_]6aXVkffXaeU_W4WkU_TSkW4ub*X%]'TVfSUcfiQ/^_W f/Q/]hd ]6c4X%ZfgW8WX%Ud2n e[cOb/WcVWi
X%ZCiX%R/R*n2d Wi>d Z|X ZQCkfgW4^TSwEUd ]aQCX%] TVZ/Us"X%UYd2n2n Q/U]a^X%]aWi}d Z|DgXVkRCn W6Sq2X%ZCijSq2hfgWn Tu8
q LX%^_]
ac4UXaeU0]abCX%]8d ZkffX%Z`ejTV]ab/W4^hc4X%U_W4U;TSw*d Z]aW4^_W4U_]0u0b/W4^_WZ/Wd ]ab/W4^0RCX%^_]8aXVLZSTV^8fCX%R/R*n2d W4Us)W4tVW4Zd2w
X6i/d U_]a^d f/Q/] TVZTV
Z W41d U_]aUUX%] Uwe1d Z/
\ Lr60s`]ab/WR/^_TVfCX%f*d2n2d ] W4UTSwEkffX%1d Z/\6]ab/WTVf/UW4^_t)X%] TVZSU8X%^_W
cfiTSkRCn W4]aWn eiVW4]aW4^kffd Z/Wifel]ab/W6R/^_TVfCX%f*d2n2d ]emTSwCt)X%^d TVQSU{W4tVW4Z`]aUd Z>]ab/Wffu@TV^n2iT1c4cfiQ/^_^d Z/\SsgubCd2cOb
U_W4WkU;^X%]ab/W4^QSZ/^_WX%U_TVZCX%f*n Wq

`7

fi " |qxq}t~"}s1|

W Lrh;XVcfi]aW4^d Z/\k5X%]a^d YTSw*11XVkRCn WSq2Vq0wBTV] d2cfiW{]abSW4^_WW4gd U]aU8X%Z
n/ p \eE LTVZ/Ud2iVW4^@]abSW
@ X%ZCi b*X%U0Z/T6Z/W4\SX%] tVWcfiTSkRgTVZ/W4Z]aU
X vffZ/W5cfiTSk0fCd ZCX%] TVZlTSw7]ab/W/^_U]&]u@T5^_TuU]abCX%]8d U0Z/TV] ff

V






V












@ X%ZCiYbCX%UZ/T
P d2kffd2n2X%^n esA]ab/W4^_W W4gd U_]aU5X%ZXv5Z/WcfiTSk0fCd ZCX%] TVZ}TSwg]ab/Wn2X%U_]]u@T^_T uU]ab*X%]6d UZ/TV] j
/
Z/W4\SX%] tVW8cfiTSkRATVZSW4Z`]aUqCfi]wxTSn2n TuU'w^_TSk b/W4TV^_WkSq SaXVD]abCX%]L]ab/W4^_Wd UZ/Tffi/d U]a^d f/Q/] TVZhUX%] Uwe1d Z/\
Lr6]abCX%]\Sd tVW4U{fgTV]abTSwD]ab/W TVf/U_W4^_tX%] TVZ/U. & X%ZCi + RgTVUd ] tVW6R/^TVfCX%fCd2n2d ]e[X%ZCi
Wd ]ab/W4^aXV&\Sd tVW4UhfgTV]a
b W &1 + X%ZC
W &;k + RgTVUd ] tVWYR/^_TVfCX%fCd2n2d ]eTV^jfC&\Sd tVW4U
fgTV]a
b v + & X%ZC
v & k + RgTVUd ] tVWR/^_TVf*X%fCd2n2d ]eqm4wLfgTV]ab[TVf/U_W4^_tX%] TVZ/U6bCX_tVW
RgTVUd ] tVW R/^_TVfCX%fCd2n2d ]es/]abSW4
Z Lr6c4X%Zb/TSn2i5TVZCn e}d2w7]ab/W8R/^_TVfCX%fCd2n2d ]eTSw &Rk + U'Wd ]ab/W4^6{TV^hVq
a11XVkRCn WjVq2ffXVn ^_WXVieU_b/TuU]abCd U;Q/Ud Z/\~X6kTV^_WYi/d ^_Wcfi] X%^_\VQCkW4Z] q2
bSW6Z/W4`]W41XVkRCn W~wQ/^_]abSW4^5d2n2n Q/U_]a^X%]aW4U8]abCX%]5d Z>\VW4Z/W4^XVn2s9d ]5c4X%Z}fgWhtVW4^_ei/d vcfiQCn ]]aTUX%] Uwxe
Lr60q
] V & 4 + 4 . VsX%Z*iXVn2nE]abS^_W4WTVf/U_W4^_tX%] TVZ/Uc4X%ZfgWkffXViW
n/ p \e& PQ/R/RgTVU_WY]abCX%
u8d ]abRgTVUd ] tVWhR/^_TVf*X%fCd2n2d ]eq]]aQ/^Z/U;TVQS]']ab*X%]5d Zl]ab*d UUd ]aQCX%] TVZ>]ab/WLr6 cfiTVZCi/d ] TVZc4X%Zb/TSn2i/s
f/Q/] TVZCn e|d2w;aXVm
1^ W v &k +Zk . lad2q Wq2s'XVn2nETSw; & + s0X%ZCi[ . kQ/U]8b/TSn2iSasfC
1^ W >aa &gk + . haa +Rk . & & haa &gk . + ad2q Wq2s`W41XVcfi] n e5]u@T TSw1 &
+ s)X%ZCih . k0Q/U_]*b/TSn2i/asac4R
1^ W >a &/ + ; . 0a + & 0 . ;a . + 0 & aa
ad2q Wq2s1W41XVcfi] n elTVZ/WhTSw9 & + sATV^ff . kQSU_]b/TSn2i/asATV^5ai/9TVZ/WhTSw"a &e + .
la +#k .
+ & 6 . a&
6a & k . ETV^a . & 6 + ab
6a & k + DbCX%U9RS^_TVfCX%fCd2n2d ]e8Wd ]abSW4^@W41XVcfi] n e
TVZ/W{TSw* & s1 + sTV^ . bSTSn2iUs)TV^]abSW{^_WkffXVd ZCd ZS\6]u@T5fgTV]abb/TSn2i/aq
Wm/^_U]lcbSWc]ab*X%
] Lr6 c4X%Zzb/TSn2i|d Z XVn2n9]ab/W4U_Wc4X%U_W4Uq]YU_b/TVQCn2i}fgWc4n WX%^]abCX%e
] Lr6
c4X%Zb/TSn2i[d Zc4X%U_W}aXVaq5TV^_W4TtVW4^sB]abSW4^_WmX%^WjZSTcfiTVZSU_]a^XVd Z]aUffTV
Z 1^ +
*CW hwTV^
> &Rk +Vk . W4gcfiW4RS] s/feY]abS
W Lr6cfiTVZCiSd ] TVZCsEwTV^0WXVcOb/Wih,s]ab/W{R/^_TVf*X%fCd2n2d ]e}k0Q/U_]fgW
]ab/W{UXVkW5wxTV^ XVn2?
n &Rk +Vk . s1X%ZCih]abSW{]ab/^_W4WR/^TVfCX%fCd2n2d ] W4U k0Q/U_]0U_QCk]aTVaq
TV^ c4X%U_WYfCas1n W4Q
] '1*DfgW]ab/WffX%]aTSk u0b/W4^_WW4gXVcfi] n e]u9T6TSw* & sg + s1X%Z*ij . b/TSn2i/s1X%ZCi
*"iTW4U
Z/TV]b/TSn2iSs"wTV^{ V4V4VqPQ/R/RgTVU_Wh]abCX%W
] D^ g
' & 4' + 4' . V
q wBTV]aW5]abCX%] sDUd ZCcfiW~XVn2n
]ab/^_W4W5TVf/U_W4^_tX%] TVZ/Ujc4X%Z}fAW~k5XViWffu8d ]ab>RgTVUd ] tVW5R/^_TVfCX%fCd2n2d ]es9X%]jn WX%U_]]u9TTS1
w ' &
' + s"X%ZC<
' .
k0Q/U_]{bCX_tVW5RgTVUd ] tVWYR/^_TVfCX%fCd2n2d ]eqy"W4ZCcfiWYu9Wmc4X%Zi/d U_] ZS\VQCd U_b>fgW4]u@W4W4Z]u@T~U_Q/fBc4X%U_W4Uffad2&TVZCn e
]u@T5TSwS]ab/Wk bCXatVW{RgTVUd ] tVW8R/^_TVfCX%f*d2n2d ]esDX%ZCi~ad2d2XVn2n]ab/^W4WbCXatVW8RATVUd ] tVW{R/^_TVfCX%f*d2n2d ]eq
TV^8U_Q/fBc4X%U_W~ad2asEU_Q/R/RgTVU_Whu8d ]abSTVQ/]5n TVU_UTSwC\VW4Z/W4^XVn2d ]e]abCX%]{TVZ*n
e ' & X%ZC<
' + bCXatVWYRgTVUd ] tVW
R/^_TVf*X%fCd2n2d ]eq b/W4Z>d ]d2kffkWi/d2X%]aWn e>wTSn2n uU8w^_TSk]ab/=
W Lr6cfiTVZCiSd ] TVZ]abCX%]&]ab/W4^_Wffk0Q/U_]-fgWU_TSkW
u8d ]ab
~U_QCcOb]abCX%e
] 1^ . W
' & c' + U_QCcOb
CswTV^mXVn2Kn k
]abCX%e
] 1^ W
h




V

q

b
/
Q

U

#
1


^






W







w
V


^
V
X
n2K
n k
' + U_QCcOb



&


]abCX%
] 1^ W
Vs0X%ZC
D^ + W wTV^XVn2
n
' & U_QCcOb[]abCX%]
1^ W K Vq
PQSfc4X%UWad2d2Bd U'kTV^W8d Z`]aW4^W4U_] Z/\Sq b/W&^T uU9TSw]abS
W Lrh;XVcfi]aW4^d Z/\kffX%]a^d x
7}cfiTV^_^W4U_RgTVZCi/d Z/\
]aq
' & F
' + sLX%ZC4
' . X%^_WaYVasajVasX%ZCimajVasA^W4U_RgWcfi] tVWn eW
q w"T u ~Wk5kffX{Sq2VaXV&]aWn2n U
Q/U]abCX%]6d2K
w D^;UX%] U/W4W
U Lr60sS]ab/W4Zmu9Wk0Q/U_]'bCXatV[
W 7A_@ B @ B wxTV^U_TSk
W @ & + . "u8d ]ab
*CW}+
*fiaq b/W4U_W]ab/^_W4W5n2d Z/WX%^;WQCX%] TVZSU0bCX_tVWU_TSn Q/] TVZ
* D^ +


&

A+ A.
`77a







fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

P/d ZCcfiW]abCd UU_TSn Q/] TVZd UQ/Z*d2Q/WsEd ]8wxTSn2n TuU'f`ee~Wkffk5XSq2VfC]ab*X%]6BiSd U_]a^d f/Q/] TVZSU]abCX%]UX%] Uwxe
Lr6 kQSU_]8b*XatVW>cfiTVZCi/d ] TVZCXVn*RS^_TVfCX%fCd2n2d ] W4
U 1^ +
*CW v+
*fi VVVs;X%ZCi~]abCX%]h]ab/Wd ^
kffX%^_\Sd Z*XVnCi/d U]a^d f/Q/] TVZ/U'TV
Z
c4X%ZmfgWjX%^_fCd ]a^X%^_eq bCd U wQCn2n eocObCX%^XVcfi]aW4^d 4W4U{]abSW8U_W4]'TSwBi/d U]a^d f/Q/p
] TVZ/
U 1^wTV^8u0bCd2c
b Lr6b/TSn2iUd Z[]abCd Uc4X%U_W
q wBTV]aW]abCX%]wTV^ V4V4VsUd ZCcfiWYu@Wc4X%Zu^d ]aW
*fia2
1^ W o+
*fiu@W{bCXatV
W 1^ +
* 1^ }
* *`VV{U_Tff]abCX%] sCd Z
* 1^ +
cfiTVZ]a^X%U_]]aT]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/] TVZlT tVW4
^ vsS]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/] TVZlT tVW4
^ c4X%Z/ZSTV]'fgW
cbSTVU_W4ZX%^_f*d ]a^X%^d2n eq

fiZ~c4X%U_W8ac4as7d ]U_b/TVQCn2i6XVn U_TfgW{c4n WX%^"]ab*X%]Lrh|c4X%Zffb/TSn2i/q@5TV^_W4T tVW4^sE1^ - +*CW h
U-Wd ]ab/W4^{;TV^VsgiW4RgW4ZCiSd Z/\{TVZub/W4]ab/W4^ +*q&Ld ZCXVn2n es1wTV^c4X%U_Wffai/as`U_QSR/RgTVU_W0]abCX%].1^W
& +Qk . VW
q rhzbSTSn2iU5d }]ab/W4^W W41d U_]aU>U_QCcOb]abCX%]=1^ + W
X%ZC
1^ W . h wxTV^ffXVn2
n [ + k . U_QCcOb]abCX%
] 1^ h[
vVq8a{w
cfiTVQ/^_UWC
1^ & W h ffwTV^8XVn24
n > & U_QCcb~]abCX%
] 1^ W K
Vq2

w"T uu@W&U_b/Tu]abCX%
] Lr6c4X%Z/Z/TV]bSTSn2i6d Z~X%Z`e TV]ab/W4^0c4X%U_Wq&Ld ^_U_]UQ/R/RgTVU_W@]abCX%]0{1^W
ff& k +_k . eVq bQ/Us ]ab/W4^_WkQ/U]EfgW;X%]&n WX%U_]LTVZ/W"TV]ab/W4^&X%]aTSkZ'lU_QCcOb8]abCX%][1^ W q''2Vq
b/W"^_T uzcfiTV^_^_W4U_RgTVZCi/d ZS\&]aT0]ab/W;X%]aTSkv &^k +k . U@a;0Vaq@PQ/R/RgTVU_WRd UC]ab/W"^_TucfiTV^_^W4U_RgTVZCi/d Z/\
]aT>]ab/W~TV]ab/W4^X%]aTS
k ';qP/d Z*cfi8
W 7 UX>%pokffX%]a^d 1sB]abSWtVWcfi]aTV^ma}V m\Sd tVW4Uld UlX%ZXvffZ/W
cfiTSk0fCd ZCX%] TVZTSwa>V5X%ZC
m]abCX%]d UffZ/TVZS4W4^_TX%ZCimbCX%U5Z/TVZ/Z/W4\SX%] tVW[cfiTSkRgTVZ/W4Z`]aUq]6Z/Tu
wTSn2n uU;f`e b/W4TV^WkvSq ff]abCX%
] rhc4X%Z/Z/TV]b/TSn2i~d Z]abCd U c4X%UWq
P d2k5d2n2X%^~X%^_\VQCkW4Z]aU6\Sd tVW}XcfiTVZ`]a^XVi/d2cfi] TVZd ZXVn2n*]ab/WTV]ab/W4^~c4X%U_W4U"u@W>n WXatVWoiVW4] XVd2n Uff]aT]ab/W
/
^_WXViW4^q

:e
c

"

k ` " l

4


!#"

\ba

%

^` %" l)

_e{&"


\_^

"

\_

f" fi

W^` " l)

fiZTVZ/WTSw7]ab/Wd ^8kffXVd Z~]ab/W4TV^_WkUsE{d2n2n2s)tX%ZiW4^~CXVX%ZCsDX%ZCiBTVfCd Z/UaVVVVs*PWcfi] TVZVU_b/T u]abCX%]
]ab/
W Lr6 X%U_U_QCkR/] TVZd U6Q/Z`]aW4U] X%fCn W}wx^TSk TVfSU_W4^_tX%] TVZ/U6TS2
w XVn TVZ/Ws;d Z]abSWjUW4Z/U_Wj]abCX%]6]ab/W
X%U_U_Q*kR/] TVZ 1^UX%] U_/W4W
U rh;Yd2kRgTVU_W4U'Z/Tj^_W4U_]a^d2cfi] TVZSU5X%]hXVn2n7TVZm]ab/WkffX%^_\Sd ZCXVnLi/d U_]a^d f/Q/] TVZ
1^ TVZ qffTV^WR/^_Wc4d U_Wn es@]abSW4eoU_bST u]abCX%]lwxTV^YW4tVW4^_e[/ZCd ]aWU_W4
]
TSw"u9TV^n2iUs"W4tVW4^_e[U_W4]
vTSwETVf/U_W4^_tX%] TVZ/Us"X%ZCiYW4tVW4^_e[iSd U_]a^d f/Q/] TV
Z X TV
Z sg]abSW4^_Wd U5Xi/d U_]a^d f/QS] TV
Z 1^ TVe
Z U_QCcOb
]abCX%
] 1^ ]ab/WmkffX%^_\Sd ZCXVnDTS
w 1^ TV
Z {d U WQ*XVng]a
XE X%ZC
1^ UX%] U/W4q
U Lr60q b/W~X%Q/]abSTV^_U
U_QCk5kffX%^d 4W]abCd UhX%U Lr6d UW4tVW4^e`]abCd ZS\SVq
W|k0Q/U_]fgWc4X%^_WwQCnd Zd Z]aW4^_R/^_W4] Z/\]ab*d U^_W4U_QCn ] q b/W4TV^Wk Sq U_b/TuU]abCX%] shwxTV^kffX%Z`e

cfiTSk0fCd ZCX%] TVZ/UTSw2X%ZCivsLrhvc4X%Zlb/TSn2i/+lwxTV^hi/d U_]a^d f/QS] TVZ/UmD^0u8d ]ab1^ W c''
wTV^8U_TSkWlX%]aTSk\
U ''q>Z]ab/WYR/^_W4t1d TVQ/UU_Wcfi] TVZSUs*u9Wc4XVn2n WiU_QCcObzi/d U]a^d f/Q/] TVZ/U>%iW4\VW4Z/W4^X%]aWVq2
fiZ>TVQ/^{t1d W4u8s1]ab*d UUX_e`U8]abCX%]Yd Z>U_TSkWc4X%U_W4U;
Lr6W4AWcfi] tVWn ec4X%Z/Z/TV]b/TSn2i/q TU_W4Wffu0b`esD/^_U_]
U_Q/RSRATVUWYu@W>X%^_W\Sd tVW4ZXjU_W4
]
TSwBu9TV^n2iU~X%ZCi[XjU_W4
] TSwTVfSU_W4^_tX%] TVZ/U
q w"T u u9W}kffX_ewW4Wn
cfiTVZ/CiVW4Z`]Y57,%-]ab*X%]'UTSkW~ -
X%ZCiYU_TSk
W -

c4X%Z/Z/TV]T1c4cfiQ/^5d ZR/^XVcfi] d2cfiWq8fiZm]abCd U
c4X%U_Wsu9W X%^_W&u8d2n2n2d Z/\{]aTcfiTVZ/Ud2iW4^"TVZCn ei/d U_]a^d f/QS] TVZ/
U 1^BTV
Z ]abCX%]"bCXatV
W D^ - Vs
1^ W - Vq aTV^W4gXVkR*n WZ

kffXaefAWXR/^_T1iQCcfi]5U_RCXVcfi
W
X%ZCi|d ]
U `Z/TuZ]abCX%]8UTSkWmcfiTSk0fCd ZCX%] TV
Z l
X%Z*
Yd
Z 5c4X%ZZSW4tVW4^ T1c4cfiQ/^ ]aTV\VW4]ab/W4^B]ab/W4Z
1^ 6 4a Vq2
r9W4/Z/m
W ]aTfgW]abSW0U_Q/fSU_W4]BTSY
w vcfiTVZ/Ud U_] Z/\8TSw1XVn2nS]abCX%]"u@W c4X%ZSZ/TV]
X-R/^d TV^d^_QCn W0TVQ/] Ud2kffd2n2X%^n eE
Z-d U9]ab/WU_QSf/U_W4]BTSY
w cfiTVZ/Ud U_] ZS\TSw1XVn2
n ]abCX%]9u9W8c4X%ZSZ/TV]'X-RS^d TV^d
^_QCn W9TVQ/] q@Ee b/W4TV^_WkSq Ssd ]-d U*U_] d2n2n RATVUUd fCn WB]abCX%[
] X%ZC
X%^_W@U_QCcOb]abCX%] sW4tVW4Zd2wu@W"^_W4U]a^d2cfi]
]aT^_Q/ZSU'u0b/W4^_WhTVZCn elTVf/U_W4^_tX%] TVZ/UYd
Z 6X%^_W~k5XViW!
Lr6 c4X%Z>TVZCn eb/TSn2id2;
w 1^ W
''
wTV^@UTSkWhX%]aTSkUZSTVZ/WkR/]effU_Q/f/UW4]aU
' q bCd UkWX%Z/U@]abCX%Z
] Lr6k5XaelwTV^cfiW'Q/U@]aTX%UUd \VZ

`77

fi " |qxq}t~"}s1|

R/^_TVf*X%fCd2n2d ]em0]aT6UTSkWW4tVW4Z`]aU]ab*X%] s17%,su9W4^W6cfiTVZSUd2iW4^_Wi RgTVU_Ud f*n Wq@DgXVkRCn W4UVq26X%ZCi8Sq2
d2n2n Q/U_]a^X%]aW]abCd URSb/W4Z/TSkW4Z/TVZ*q*WYkffXaeU_Q*kffkffX%^d 4W]abCd UhX%UU_TSkW4] d2kW4UmLr6d U;Z/TV]abCd Z/\SVq
{d tVW4Z]ab/W4^_WwTV^_W]abCX%
] Lr6d2kRgTVU_W4U&UQCcbU]a^_TVZ/\cfiTVZCi/d ] TVZ/Us)]ab/W^_WXViW4^k5Xaeju9TVZCiVW4^&ub`e
]ab/W4^_Wd UffU_T|k0QCcbU_]aQCieTSw9]ab/
W Lr6 cfiTVZCiSd ] TVZd Z]ab/W~U_] X%] U_] d2cfiUn2d ]aW4^X%]aQ/^_Wq b/W^_WX%U_TVZvd U
]abCX%]6U_TSkWTSwB]abSWU_RgWc4d2XVn*Ud ]aQCX%] TVZ/Umd ZubCd2c
b Lrh b/TSn2iU6TSw]aW4ZX%^d U_Wd Zkffd U_Ud Z/\iSX%] X>X%ZCi
U_Q/^tgd tXVnX%ZCXVn eUd U"R/^TVfCn WkUq@yBW4^W8d U'X%ZffW41XVkRCn W-PQSR/RgTVU_W9]ab*X%]B]ab/WU_W4]"TSwTVf/UW4^_t)X%] TVZSU'c4X%Z5fgW
*fisubSW4^_W;WXVcOb *"d UX0RCX%^_] ] TVZTSCw ]abCX%]d UsAX0U_W4]@TSw)RCXVd ^_u U_WhiSd U_TSd Z]
u^d ]a]aW4Z>X%Z
U *
r
&

U_Q/fSU_W4]aU;TSK
w ub/TVU_W8Q/ZCd TVZd
U vaq8Q/^_]ab/W4^'U_Q/RSRATVUW]abCX%]'TVf/U_W4^_tX%] TVZ/UffX%^_W \VW4Z/W4^X%]aWiYfe]ab/W
wTSn2n u8d Z/\mR/^TgcfiW4U_UsEu0bCd2cb[u@Wmc4XVn2
n {:f)z*q}PVTSkWYfgW4]u@W4W4ZX%Z*
Ucb/TVU_W4ZXVc4cfiTV^iSd Z/\
]aTU_TSkWX%^_fCd ]a^X%^eoi/d U_]a^d fSQ/] TVc
Z X - ZCiW4RgW4ZCiVW4Z`] n e


U6cOb/TVU_W4ZXVc4cfiTV^i/d ZS\j]aq
XE>q b/W
X%\VW4Z]]ab/W4ZTVf/U_W4^_tVW4U0]ab/WQ/ZCd2 Q/Wh * U_Q*cb]abCX%[
] >q*fiZ`]aQ*d ] tVWn es)]ab/WRCX%^_] ] TVZ/U * kffXae
^_W4R/^W4U_W4Z`]-]ab/W'TVf/U_W4^t)X%] TVZ/U&]ab*X%]c4X%ZfgW6kffXViW'u8d ]ab>X;RCX%^_] d2cfiQCn2X%^&U_W4ZSU_TV^q bQ/UC
X - iW4]aW4^k5d Z/W4U
]ab/WR/^_TVf*X%fCd2n2d ]eo]abCX%]X5R*X%^_] d2cfiQCn2X%^6U_W4Z/UTV^d U~cOb/TVU_W4ZC
X iW4]aW4^kffd Z/W4Uh]ab/WR/^TVfCX%fCd2n2d ]eo]abCX%]~X
RCX%^_] d2cfiQ*n2X%^u@TV^n2i}d UYcOb/TVU_W4ZCq b/W6UW4Z/U_TV^YX%ZCi]ab/Wffu@TV^n2i]aTV\VW4]ab/W4^iW4]aW4^k5d Z/Wff]ab/WffTVfSU_W4^_tX%] TVZ
]abCX%]6d UffkffXViWq0]6d UWX%U_e~]aTjU_W4W ]abCX%]]abCd UffkWcbCX%Z*d Uk ZCiQCcfiW4U6XYiSd U_]a^d f/Q/] TVZlTV
Z wxTV^'ubCd2cOb
Lr6b/TSn2iVUq
b & + &
vVs`X%ZCi + V h
$
v&cfiTV^_^_W4U_RgTVZCiVU1]aT X
bSWU_RgWc4d2XVnc4X%U_WBu8d ]a
Ud2kR*n Whkffd UUd Z/\i/X%] X;R/^_TVfCn Wka11XVkRCn WSq2;fgWn u8aq*fiZ]aQCd ] tVWn es)Wd ]ab/W4^cfiTSkR*n W4]aW5d ZCwxTV^kffX%] TVZ
Uff\Sd tVW4ZCs"TV^ff]ab/W4^W}d UffZ/Ti/X%] X>X%]~XVn2n2qfiZ]abCd UcfiTVZ]aW4`] #
Lr6d UffTSw]aW4Zc4XVn2n Wiz
xQyV#p ,S
{fi/|
pqfiZkTV^_W^_WXVn2d U_] d2c5rh R/^_TVfCn WkUs*u9WkffXaeoTVf/U_W4^_tVW>XYtVWcfi]aTV^ffu8d ]ab[U_TSkWjTSw;d ]aU
cfiTSkRgTVZ/W4Z]aU5kffd U_Ud ZS\SqfiZlUQCcbc4X%U_W4U]abS
W Lrh cfiTVZ*i/d ] TVZ>U_TSkW4] d2kW4U{U_] d2n2nAb/TSn2iUqZmR/^XVcfi] d2c4XVn
kffd U_Ud Z/\i/X%] XLR/^TVfCn WkUs]ab/WB\VTSXVnd UETSwx]aW4Z8]aT8d ZCwW4^D]ab/W0i/d U_]a^d f/Q/] TV
Z D^1TVZ^_Q/ZSE
U w^_TSk[U_QCc4cfiW4UUd tVW
TVf/U_W4^t)X%] TVZ/U6TS!
w W&q bCX%]d UsLTVZ/WjTVf/U_W4^tVW4U~X5UXVkRCn W> )&/ 4 ),+0/ ^ i^i^i4 ) j / sLub/W4^_W ) * / q
e`RCd2c4XVn2n esV]ab/W ) * / X%^_W{X%U_U_Q*kWi]aT'fgWX%Zd2q2d2q2i/q@ad ZCiW4RgW4ZCiVW4Z`] n eYd2iVW4Z`] d2c4XVn2n ei/d U_]a^d f/QS]aWi//UXVkRCn W
TSw-TVQ/] cfiTSkW4UTS
w W&q b/WcfiTV^_^_W4U_RgTVZCiSd Z/\ u@TV^n2iU
& 6 + ^ i^i^iTVQ/] cfiTSkW4UTS[
w X%^_WS
TVf/U_W4^tVWi/[
q r9W4RAW4Z*i/d Z/\hTVZ]ab/WUd ]aQCX%] TVZCU
D^{kffXaejfgWhcfiTSkR*n W4]aWn ejQSZ/`ZST uZjTV^d UX%U_U_QCkWi]aT6fgW
XkWk0fgW4^TSwCU_TSkW5RCX%^XVkW4]a^d2cYwXVk5d2n elTSw-i/d U]a^d f/Q/] TVZ/Uq64wD]ab/W5Z`QCk0fgW4^{TSwCTVf/U_W4^_tX%] TVZ/
U ld U
n2X%^_\VWs]ab/W4Zc4n WX%^n e8]ab/W9UXVkRCn W' )&/ 4 ),+0/ ^ i^i^i4 ) j / c4X%ZhfAW"Q/U_Wi0]aT;TVf/] XVd ZX"^_WX%U_TVZCX%f*n W9W4U_] d2k5X%]aW
TS2
w 1^ sS]ab/WkffX%^_\Sd ZCXVn*i/d U_]a^d fSQ/] TVZ~TV
Z q{CQ/]0TVZSWd U6d Z`]aW4^W4U_]aWimd Z]ab/WwxQ*n2nEi/d U_]a^d f/Q/] TV
Z D^q
bCX%] i/d U_]a^d f/QS] TVZQ/U_QCXVn2n ec4X%Z/Z/TV]0fAW5d Z*wxW4^_^Wihu8d ]ab/TVQ/]k5X%gd Z/\lXVi/i/d ] TVZCXVnCX%U_U_QCkRS] TVZ/UsU_QCcOb
X%U0]abSW
W LrhX%U_U_QCkR/] TVZCq
rX%ZCd Wn Us6 BTVfCd Z/Us VVVVa~PVQ/R/RgTVU_Wl]abCX%]oX
n/ p \es@ aXVi/X%R/]aWiwx^_TSk aPScbCX%^wU_]aWd ZCm
kWi/d2c4XVn"U_]aQCiVezd UcfiTVZCiQCcfi]aWi>]aTo]aW4U]5]ab/WW47Wcfi]jTSwXZ/W4u i^_Q/\Sq b/Wi^_Q/\|d UmXVi/kffd ZCd U]aW4^_Wi
]aToXh\V^TVQ/RTSwCRCX%] W4Z`]aU8TVZ|Xhu@W4W4gn emfCX%Ud UqCWwxTV^_W5]ab/WffW4RAW4^d2kW4Z`]5d UU_] X%^_]aWi}X%Z*iXVwx]aW4^d ]Yd U
/ZCd Ub/Wi/sU_TSkW cbCX%^XVcfi]aW4^d U_] d2cUXaes]ab/WfCn TT1i'R/^_W4U_U_QS^_WATSw`]ab/W&RCX%] W4Z]aUd UkWX%U_Q/^_WiSq bSWi/X%] X
X%^_W]abQ/Ui/d 7W4^W4ZCcfiW4U'd Zfff*n T`T1iR/^_W4UU_Q/^_WwTV^'d ZCiSd tgd2iQ*XVnRCX%] W4Z`]aU@fgWwxTV^WX%ZCi5XVw]aW4^"]ab/W0]a^_WX%] kW4Z] q
fiZR/^XVcfi] d2c4XVngU_]aQ*i/d W4UTSw1]abCd Ugd ZCiSsSTSw]aW4Z>U_W4tVW4^XVngTSw1]ab/W6R*X%] W4Z`]aUYiV^_TVRTVQ/]TSwD]abSW6W4`RgW4^d2kW4Z] q
TV^;U_QCcb~RCX%] W4Z`]aU']ab/W4^_Wjd U]ab/W4ZZ/Tli/X%] XVqLWjkTgiWn]ab*d U X%U wTSn2n u0U
U]ab/WU_W4]TSwSRATVUUd fCn W
tXVn Q/W4ULTSw]ab/WcObCX%^XVcfi]aW4^d U] d2c*u9WX%^_Wd Z`]aW4^_W4U]aWi ZWq \Sq2sfCn TT1iR/^W4U_U_Q/^_W'i/d 7W4^W4ZCcfiWa
q & +
u8d ]ab &
vVs@X%ZCi + V h

vX%UjX%fgT tVWqVTV^>%cfiTSkR*n2d W4^_UlRCX%] W4Z`]aUh]abCX%]i/d2i
Z/TV]5i^TVRTVQ/] as1u@WffTVf/U_W4^tV
W VsEub/W4^
W U{]ab/W6tXVn Q/WffTSwE]ab/W~cObCX%^XVcfi]aW4^d U_] d2cu@W6uX%Z]
]aTokWX%UQ/^_WqTV^i^_TVRgTVQ/]aUs1u9WffTVfSU_W4^_tV=
W W
]abCX%]jd UsDu9W5TVf/U_W4^_tVWffZSTV]abCd Z/\}X%]YXVn2n2aq5W
]abQ/U{bCX_tVWs@wTV^W41XVkRCn Ws@XhU_W Q/W4ZCcfiW5TSwETVfSU_W4^_tX%] TVZ/Uj & & V4 + + V4 . l4 2
2 V4 l^ i^i^i 4 j j7Vq4wC]abCd U UXVkRCn Wmd Un2X%^\VWYW4Z/TVQS\VbCsDu@Wc4X%ZoQ/UWld ]]aTlTVf/] XVd ZX
^_WX%U_TVZ*X%fCn WW4U_] d2kffX%]aW{TSw/]ab/WR/^_TVf*X%fCd2n2d ]eY]abCX%]X'RCX%] W4Z]i^_TVR/U-TVQ/]]ab/W^X%] T5TSwTVQ/] cfiTSkW4U0u8d ]ab

`77

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

+* ]aT]abSW6]aTV] XVn1ZQCkfgW4^TSwCTVQ/] cfiTSkW4UaqhWlc4X%Z|XVn UT\VW4]jX ^_WX%UTVZCX%fCn WffW4U_] d2k5X%]aW5TSwC]ab/W
i/d U_]a^d f/Q/] TVZTSw?W wTV^&]ab/WffcfiTSkRCn e1d Z/\6RCX%] W4Z]aUq TV\VW4]ab/W4^0]ab/W4U_W]u@T~i/d U_]a^d f/Q/] TVZ/UiW4]aW4^kffd Z/W
]ab/WYiSd U_]a^d f/Q/] TVZTSwg q

WX%^_Wjd Z]aW4^_W4U_]aWimd Zl]ab/WW4AWcfi]'TSwA]ab/Wji^Q/\~d Z]abSW\VW4Z/W4^XVnSRgTVR/QCn2X%] TVZC
q "ZCwTV^_]aQ/ZCX%]aWn es*d ]
k X_efgW"]ab/Wc4X%U_W-]abCX%]L]ab/W@W47Wcfi]TVZi^TVRATVQS]aU@d Ui/d AW4^_W4Z`]w^_TSk]ab/W9W4AWcfi]TVZcfiTSkRCn2d W4^_Uq@aPScbCX%^wp
ff
U_]aWd ZCs4rX%ZCd Wn UsX%Z*iTVf*d Z/U@aVVVVLi/d UcfiQSU_U@X%ZXVcfi]aQCXVnkWi/d2c4XVn4U]aQCieffd Z8u0bCd2cb8R/b`eUd2c4d2X%Z/U*QCi\VWi
]ab/W6W4AWcfi]{TVZ|i^TVRATVQS]aU']aTfgWhtVW4^_ei/d AW4^_W4Z`]5w^_TSk]ab/W6W4AWcfi]{TSw@cfiTSkRCn2d W4^_Uq2 b/W4Zmu9W~c4X%ZSZ/TV]
ZCwW4^]ab/Wmi/d U_]a^d fSQ/] TVZ}TV
Z
wx^_TSk]ab/W5TVfSU_W4^_tX%] TVZ/U & 4 + ^ i^i^i@XVn TVZ/WYu ]ab/TVQ/]YkffX%1d Z/\[XVi/i/d p
] TVZCXVn7X%U_U_QCkRS] TVZ/U0X%fgTVQ/]Bb/Tu]ab/W8i/d U]a^d f/Q/] TVZ~wTV^'i^TVRATVQS]aU0d UB^_Wn2X%]aWi]aT{]ab/Wi/d U_]a^d f/Q/] TVZwTV^
cfiTSkRCn2d W4^U
q SW4^bCX%R/UB]ab/W0Ud2kRCn W4U_]UQCcbX%UU_QCkR/] TVZ5]abCX%]BTVZ/Wc4X%ZmkffX%VW8d UB]abCX%]"]ab/W8i/d U_]a^d f/Q/] TVZ
TSY
w W wTV^6iV^_TVRgTVQ/]aUff5d ZwXVcfi]]ab/W UXVkWX%U]ab/WiSd U_]a^d f/Q/] TVZmTSY
w W wTV^ffcfiTSkRCn2d W4^_U]ab/Wi/X%] X
X%^_Wz%k5d U_Ud Z/\|X%]5^X%ZCiTSkffVq{w;cfiTVQS^_U_Ws"]abCd UlX%U_UQCkR/] TVZd U5Q/U]6]ab/
W Lr6 X%U_UQCkR/] TVZCqEe
rhb/TSn2iU8d wxTV^hXVn24
n

b/W4TV^WkVq2VaXVa

D^W v D^ W W
v D^ W



ubCd2cOb>kWX%Z/U&Q/U_]@]abCX%]&]ab/Wffi/d U_]a^d fSQ/] TVZTSwF U{d ZCiVW4RAW4Z*iW4Z`]@TSwu0b/W4]ab/W4^{X;RCX%] W4Z`]iV^_TVR/U-TVQ/]
vETV^-Z/TV] q bQ/Us1 Lr6c4X%ZfgWhX%UU_QCkWi/s]ab/W4Zu@W6c4X%Z>d ZCwW4^-]ab/Wffi/d U_]a^d fSQ/] TVZYTVZ
ubCd2cObod U0u0bCX%]&u9WjX%^_W{^_WXVn2n e>d Z]aW4^_W4U_]aWi~d Z*aq
lX%Z`effRS^_TVfCn WkUd Zmk5d U_Ud Z/\i/X%] X8X%Z*i{U_QS^_tgd tXVnSX%Z*XVn e`Ud U{X%^_W;TSw)]ab/W;gd ZCijd2n2n Q/U_]a^X%]aWijX%fgTtVW b/W
X%ZCXVn eUd U@u9TVQCn2ifgW\V^_WX%] n eYUd2kRCn2d /WiYd2U
w Lr6}bSTSn2iUsVfSQ/]Bub/W4]abSW4^"TV^@Z/TV]9]ab*d U'd U@U_Tjd U@Z/TV]c4n WX%^q
fi]jd U ]ab/W4^_WwTV^_W5TSw*TVftgd TVQSUd Z]aW4^_W4U_] ]aTd ZtVW4U_] \SX%]aWub/W4]abSW4^s9w^_TSkTVf/U_W4^_t1d Z/\~]abSW[%cfiTSX%^_U_W4Z/WiS
i/X%] X6 )&/ 4 )+0/ ^ i^i^i 4 ) j / XVn TVZ/Ws1d ]kffX_emXVn ^_WXViVe5fgWRATVUUd fCn W']aT6]aW4U_]&]ab/WffX%U_U_QCkR/] TVZ]abCX%Z
] Lr6
b/TSn2iUq;TV^'W41XVkRCn WsTVZSWYkffd \Vb] d2kffX%\Sd Z/W8]abCX%]0]abSW4^_WYX%^_Wi/d U_]a^d f/QS] TVZ/UTV
Z wxTV^;ubCd2c
b Lr6
Ud2kR*n ec4X%Z/Z/TV]&b/TSn2iSq*w)]abSW'WkRCd ^d2c4XVn1i/d U_]a^d f/Q/] TVZTSw]ab/Wff
*1u@W4^_W%c4n TVU_Wffad Z]ab/WffX%R/R/^TVR/^d2X%]aW
U_W4Z/UW@]aT}Xi/d U_]a^d f/QS] TVZ]abCX%]^_QCn W4UTVQ/W
] rh;sS]abSW6U_] X%] U_] d2c4d2X%Zkffd \Vb`]5d Z*wxW4^]abCX%q
] 1^ffiTW4U{Z/TV]
UX%] Uw
e rh;;
q "ZCwTV^_]aQ/ZCX%]aWn esgd2C
w U-/ZCd ]aWs`]ab/W4Zj]ab/W^_W4U_Q*n ]9TSwDd2n2n2st)X%Z>iW4.
^ ~EXVX%ZCsgX%ZCiBTVfCd Z/U
aVVVVsPWcfi] TVZV"^_WwxW4^^_Wi6]aTX%];]abSWfgW4\Sd Z/ZCd Z/\YTSwA]abCd U'U_Wcfi] TVZmU_b/T u0U;]abCX%]'u@Wjc4X%ZmZ/W4tVW4^'^_QCn W
TVQ/
] rhd Z]abCd UuXaeq

W6X%^_W6d Z`]aW4^_W4U]aWijd Zj]ab/WhQ/W4U] TVZjTSwu0b/W4]ab/W4^Qrhzc4X%Zb/TSn2i5d ZXZ/TVZCiW4\VW4Z/W4^X%]aW0UW4Z/U_Ws
\Sd tVW4Z X%Z*ivqffV^_TSk]abCd URgTSd Z]'TSwDtgd W4u sA]ab/WhUn TV\SX%ZvU_TSkW4] d2kW4UqLr6 UZ/TV]abCd ZS\Sjk5X%VW4U
U_W4Z/UWqfiZcfiTVZ]a^X%U_] s{d2n2nW4]mXVn2q;aVVVV8u@W4^_Wd Z]aW4^_W4U_]aWid Z]ab/W[QSW4U_] TVZ|u0b/W4]ab/W4^Lr6 c4X%Z
fgWY]aW4U]aWiwx^TSkTVf/U_W4^_tX%] TVZ/U6TS2
w XVn TVZ/Wq^_TSk]ab*X%]6RATSd Z]hTSwt1d W4u8s]abSWjUn TV\SX%Z Lr6d U
W4tVW4^_e]abCd Z/\Sffk5X%VW4URgW4^wxWcfi]-U_W4ZSU_WqLfiZwXVcfi] sD{d2n2n2st)X%Z}iW4
^ ~CXVX%ZCs1X%ZCiBTVfCd Z/U&u@W4^_WffQCd ]aW5X_uX%^Ws
X%ZCiW4RCn2d2c4d ] n e6U_] X%]aWiSs]abCX%Q
] Lrh|d2kRgTVU_W4UBtVW4^_ehU_]a^TVZ/\5X%U_U_QCkR/] TVZ/UBTVZY]ab/Wi/d U_]a^d fSQ/] TV
Z 1^qEfiZ
X n2X%]aW4^&RCX%RgW4^sSd ]@uX%U-W4tVW4Z}d2kR*n2d2c4d ] n e5U_] X%]aWi8]abCX%]d ZU_TSkW6c4X%U_W4
U Lr6wTV^cfiW4Z
U 1^ W q
''
wTV^U_TSkW{X%]aTSk
U '|aTVfCd Z/Us/TV]aZCd ]a4esSPScbCX%^wU_]aWd ZCsVVVVsAPWcfi] TVZ~Vq2Vaq&-QS^&cfiTVZ]a^d f/QS] TVZd U
]aT6R/^_Ttgd2iVW;]ab/WR/^Wc4d U_W5cfiTVZCi/d ] TVZ/Uu ~/WkffkffX;Sq26X%ZCi b/W4TV^WkSq S*Q/ZCiW4^@ubCd2cOb]abCd U-bCX%R/RgW4Z/Uq
BTVfCd Z/Us;T1iZCd ]a4es;X%ZCiP/cbCX%^wxU_]aWd ZaVVVV5XVn U_T|d Z]a^_T1iQCcfiWiX-X_eVW4Ud2X%ZvkW4]ab/Tgian2X%]aW4^
W4]aW4ZCiWife[PScbCX%^wU_]aWd Z>W4]jXVn2qBaVVVVa]ab*X%]YXVn2n uU8TVZ/Wff]aTU_RgWc4d2we[X6R/^d TV^5i/d U_]a^d f/Q/] TVZ>TtVW4^
XhRCX%^XVkW4]aW4q
^ oubCd2cObd ZCiSd2c4X%]aW4Us9d ZXhR/^_Wc4d U_W5U_W4Z/U_WsDb/T u kQCcO
b D^YiW4t1d2X%]aW4Ujw^_TSk Lr60qVTV^
W41XVkRCn W.
}cfiTV^_^_W4URATVZ*iUff]aT}]abSW~U_W4]YTSwi/d U_]a^d f/QS] TVZ/
U D^5UX%] Uwe1d Z/
\ Lr60q b/W~RS^_Wc4d U_W
cfiTVZ/Z/Wcfi] TVZlfgW4]u@W4W4Z~]abCd U0u9TV^_X%ZCihTVQS^_UZ/W4WiU wxQ/^]ab/W4^d Z`tVW4U] \SX%] TVZCq

`7_

fi " |qxq}t~"}s1|

:e $ T\ nql" Wagi\ l \ba
fiZ b/W4TV^_Wk Vq2X%ZCi
~ Wk5kffX8Sq2

n ^ " l)

c


" `^0a "s:k ^ " l

n ^ "

v%2"%lg!

u9W~iW4Ucfi^d fgW
rh ZX%ZXVn \VW4f/^XVd2c'uX_es"X%U5XcfiTSn2n Wcfi] TVZoTSw
R/^_TVf*X%fCd2n2d ] W4U&UX%] Uwegd ZS\~cfiW4^_] XVd ZW QCXVn2d ] W4UqLfiU&]ab/W4^_WffXhkTV^_WR/^_T1cfiWiQ/^XVn20uXaejTSw^W4R/^_W4U_W4Z] Z/\
Lr60~ZRCX%^_] d2cfiQCn2X%^shiTW4U]ab/W4^W>W4gd U]oXmUd Z/\Sn WkWcbCX%Z*d Uk ]abCX%]~\Sd tVW4U^d UW]a
Lr6 U_QCcOb
]abCX%]o7 n %c4X%U_WmTSZ
w Lr6 c4X%ZzfAWtgd W4u@WiX%U>XU_RgWc4d2XVn;c4X%U_WTSw&]abCd U>kWcbCX%Z*d Ukff WbCX_tVW
XVn ^_WXVieW4ZCcfiTVQ/Z]aW4^_WimX8RATVUUd fCn Wjc4X%Z*i/d2i/X%]aWwxTV^U_Q*cbXYkWcbCX%ZCd Ukff-]ab/
W {:f)zR/^_T1cfiWiQ/^_Wq
fiZjPWcfi] TVZ Sq 0u@W'iW4Ucfi^d fgWi-]abCd U@kWcbCX%ZCd UkX%ZCid ZCi/d2c4X%]aWi0]abCX%]@d ]C\VW4Z/W4^X%]aW4U*TVZCn e5i/d U_]a^d fSQ/] TVZ/U
]abCX%]*UX%] UwW
e Lr60d
q "ZCwTV^_]aQ/ZCX%]aWn esX%ULu@W"Z/TuoUb/T u8s]ab/W4^W9W41d U_;
] rh[i/d U_]a^d f/Q/] TVZ/U*]abCX%]@c4X%ZSZ/TV]
fgW5d Z`]aW4^R/^_W4]aWiX%U;fAWd ZS\6\VW4Z/W4^X%]aWi6fN
e {:f*q
-QS^EW41XVkRCn W'd UCfCX%U_Wi0TVZjX%Z W4gXVkRCn W"\Sd tVW4Zhfe6d2n2n2st)X%ZiW4;
^ ~CXVX%ZCs)X%ZCiBTVfCd Z/U@aVVVVasVub/T
u@W4^_WXVcfi]aQCXVn2n e]abSW8/^U_];]aT>cfiTVZ/Ud2iW4^ub/W4]ab/W4^]ab/W4^_W W41d U_fi
] ZCX%]aQ/^XVn2LkWcObCX%ZCd UkU]ab*X%];\VW4Z/W4^X%]aW
XVn2nLX%ZCiYTVZCn eiSd U_]a^d f/Q/] TVZSU'UX%] Uwxe1d Z/
\ Lr60q b/W4eUb/T u ]abCX%]ffd ZmU_W4tVW4^XVnARS^_TVfCn WkU'TSw1U_QS^_tgd tXVn
X%ZCXVn eUd UsLTVfSU_W4^_tX%] TVZ/UX%^_W\VW4Z/W4^X%]aWiXVc4cfiTV^i/d ZS\l]aTlubCX%] ]ab/W4ezc4XVn2n@Xfi/|
ph,
phS/
,, //Y 7 ph4q b/W4e>XVn U_T5Ub/T u]abCX%]]ab/Wd ^;^X%ZCiTSkffd 4WiffUcbSWkW{\VW4Z/W4^X%]aW4UTVZCn e>i/d U]a^d f/Q/p
] TVZ/U']abCX%]'UX%] Uw
e Lrh;qfiZwXVcfi] s/]abSW^X%ZCiVTSkffd 4WikTVZ/TV]aTVZ/WjcfiTSX%^U_W4ZCd Z/\jUcbSWkW8]aQS^_Z/UTVQ/];]aT
fgWffX'U_RgWc4d2XVn1c4X%U_W{TS
w {:fzLs1XVn ]ab/TVQ/\Vbu@W5iT6Z/TV]-R/^_TtVW{]abCd U&b/W4^Wq{d2n2n2s)tX%Z}iW4
^ ~EXVX%Z*sDX%ZCi
TVf*d Z/U"U_bST ufehW4gXVkRCn W0]abCX%]9]ab/W^X%Z*iTSkffd 4Wi5cfiTSX%^U_W4ZCd Z/\8Ucb/WkW4UiTZ/TV]BU_]
Q vlcfiW]aT\VW4Z/W4^X%]aW
XVn2U
n Lr6i/d U]a^d f/Q/] TVZ/Uq@DU_U_W4Z] d2XVn2n e]ab/WUXVkWW41XVkRCn WU_bST uU&]abCX%{
] {:fzoiVT`W4U&Z/TV]&Wd ]abSW4^q
n/ p \eE LTVZ/Ud2iW4^-U_QSfc4X%UWhad2d2*TSwC11XVkRCn WSq26X%\SXVd ZCqV~/W4] & 4 + sg . X%ZCi!' & sC' + X%ZCi
' . fgW6X%U8d Z]abCX%]&W41XVkRCn Ws1X%ZCiX%UU_QCkWffwTV^&Ud2kRCn2d2c4d ]ej]ab*X%]. ' & U' + !' . q b/WW4gXVkRCn W
U_b/Tu9Wij]ab*X%]]ab/W4^_WhW4gd U]aU5i/d U_]a^d f/QS] TVZ/=
U 1^UX%] Uwe1d Z/
\ Lrh$d Zl]ab*d U5c4X%U_W6u8d ]ab1^Y'1*V$wTV^
l VV4V4VVshXVn2n"bCXat1d Z/\cfiTVZCi/d ] TVZCXVn9R/^TVfCX%fCd2n2d ] W4
U 1^ +
*UW
VV[wxTV^XVn2n
| * q
q n WX%^n es" & 4 + X%ZCi . c4X%Z/Z/TV]fgWh\V^_TVQ/RgWiY]aTV\VW4]ab/W4^]aT}wTV^k X8U_W4]TSwERCX%^_] ] TVZ/U{TSw
vq&PTSs)W4tVW4Z]ab/TVQ/\V
b Lr6bSTSn2iU8wTV
^ 1^ C
Q6f)zc4X%ZSZ/TV]fgWQ/U_Wih]aT5Ud2kQ*n2X%]aq
W 1^ q

bCd2n W{d2n2n2s0t)X%Z iW4
^ ~EXVX%Z*sX%ZCiTVfCd ZSUaVVVVshPWcfi] TVZ$VX%U_|ub/W4]ab/W4^j]ab/W4^_WmW41d U_]aU}X
\VW4Z/W4^XVn9kWcb*X%ZCd UkwTV^\VW4Z/W4^X%] Z/\oXVn2n"X%Z*iTVZCn eLrhi/d U_]a^d fSQ/] TVZ/UsD]ab/W4e[iVTZ/TV]YkffX%VW5]abCd U
Q/W4U_] TVZ}kffX%]ab/Wk5X%] d2c4XVn2n ejRS^_Wc4d U_Wq*r'U&Z/TV]aWi8f`em{d2n2n2st)X%Z>iW4Z
^ ~CXVX%ZCsgX%ZCiTVfCd Z/Us]abSW'RS^_TVfCn Wk
b/W4^_Wjd U]abCX%]0u ]ab/TVQ/]X%ZecfiTVZSU_]a^XVd Z]0]aT5u0bCX%]cfiTVZ/U_] ]aQ/]aW4U6XtXVn2d2i~kWcbCX%ZCd Ukffs)]ab/W4^_Wjd U c4n WX%^n e
X-]a^d t1d2XVnVUTSn Q/] TVZ5]aT8]ab/WR/^_TVf*n Wkff9d tVW4ZmX{iSd U_]a^d f/Q/] TV
Z 1^BUX%] Uwxe1d Z/q
\ rh;su9W;Ud2kRCn ei^X_uX
u@TV^n2
$XVc4cfiTV^i/d Z/\h]a
18^ sgX%ZCi]ab/W4Zi^Xau |U_QCcOb]abCX%H
] >vXVc4cfiTV^i/d ZS\ ]aTh]ab/W6i/d U_]a^d f/Q/] TVZ
1^
W aq bCd Ud U0TVftgd TVQSUn emcOb/WX%] Z/\md ZU_TSkWU_W4ZSU_WqLfiZ`]aQCd ] tVWn esCd ]U_W4WkU0]abCX%]
X ^_WX%U_TVZCX%fCn W&kWcbCX%ZCd Uk Ub/TVQCn2i~Z/TV]hfAWmXVn2n u@Wim]aTcb/TTVU_W> XVc4cfiTV^i/d ZS\l]aTXmi/d U_]a^d f/Q/] TVZ
N
iW4RgW4ZCi/d ZS\{TV
Z q*fi]iTW4U-Z/TV]@bCXatVW]abCX%]-1d ZCi{TSwEcfiTVZ`]a^_TSn`TtVW4^&]ab/W;TVfSU_W4^_tX%] TVZ/U&]abCX%]X%^_WffkffXViWq
PTmub*X%]cfiTVQ/Z]aU~X%U~X^_WX%U_TVZCX%f*n W>kWcb*X%ZCd Ukff$Z]aQCd ] tVWn es"]ab/W>kWcb*X%ZCd UkU_b/TVQ*n2i~fgW
X%fCn W{]aTmcfiTVZ`]a^TSnTVZCn eubCX%]c4X%ZfgW5cfiTVZ`]a^_TSn2n Wild ZX%Z~W4`RgW4^d2kW4Z] XVnUW4]aQ/RCqLWc4X%Z~]abCd Z/jTSw7]ab/W
kWcObCX%ZCd UkX%U X%Z%X%\VW4Z`] ]abCX%]&Q/U_W4UX'UW4]&TSw/U_W4ZSU_TV^_U&]aTffTVf/] XVd Zd ZCwxTV^kffX%] TVZoX%fgTVQ/]-]ab/Wu@TV^n2i/q
^ XE>q|bCd2n W]abSWmX%\VW4Z]~kffXaezcfiW4^_] XVd ZCn ecb/TTVU_WubCd2cOb
b/WX%\VW4Z]~iT`W4UhZ/TV]hbCX_tVW>cfiTVZ`]a^_TSn*TtVW4a
U_W4Z/UTV^']aTQ/UWs*d ]ffd UZ/TV]'^_WX%U_TVZCX%f*n W ]aT>X%U_UQCkW ]abCX%]U_b/Wc4X%ZcfiTVZ`]a^_TSn7]ab/Wd ^TVQS]aR/Q/]6TV^'W41XVcfi] n e
ubCX%]]ab/W4e[c4X%ZU_W4Z/U_Waq{Z*iW4Wi/sA\Sd tVW4Z|X8u@TV^n2q
sA]ab/W6TVfSU_W4^_tX%] TVZ^_W4]aQ/^_Z/WiYfe~]ab/WhU_W4ZSU_TV^5d U
wQCn2n eliW4]aW4^kffd Z/Wi/q bCd Ud U&W41XVcfi] n ej]ab/WU_W4]aQ/R}d2kRCn WkW4Z`]aWid Z]ab/6
W {:f)zUcOb/WkW5i/d UcfiQSU_U_Wi
ZPWcfi] TVZSq Ss*ubCd2cObu@Wj]ab/W4^_WwTV^_WY^W4\SX%^i[X%UXln W4\Sd ] d2kffX%]aWokWcObCX%ZCd Uk5qWjZ/T ud Z]a^_T1iQCcfiW
XhR/^TgcfiWiQS^_!
W {:f s1ub*d2cb}W4]aW4ZCi
U {:f*s9X%Z*i]aQ/^_Z/UTVQ/]{]aT~\VW4ZSW4^X%]aWmXVn2n"X%ZCiTVZCn e
i/d U_]a^d f/Q/] TVZ/U{UX%] Uwxe1d Z/
\ Lrh;!
q 4QSU_]5n2d V\
W {:f)z*e
{:f X%U_U_QCkW4U]abCX%]{]abSW4^_Wd UjXcfiTSn p
n Wcfi] TVZTSwAU_W4Z/UTV^_Us*X%ZCi~d ]6cfiTVZ/U_QCn ]aU6X\Sd tVW4ZlUW4Z/U_TV^'u8d ]ab[XYcfiW4^_] XVd ZmR/^_WiW4]aW4^kffd Z/Wi5R/^_TVfCX%fCd2n2d ]eq

`77

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

y"T u@W4tVW4^sAQ/ZCn2d VU
W {:f*2
Q6f)z kffXaed \VZ/TV^_WX{U_W4ZSU_TV^;^WXVi/d Z/\Sq b/WjiWc4d Ud TVZlu0b/W4]ab/W4^
TV^ZSTV]YX6U_W4ZSU_TV^^_WXViSd Z/\od Ujd \VZSTV^_Wi}d UXVn2n u@Wi]aT[iW4RgW4ZCiTVZCn emTVZo]ab/WffU_W4Z/UTV^]abCX%]8bCX%UfgW4W4Z
cbSTVU_W4Zfej]ab/WjX%\VW4Z] sEX%ZCiffTVZ~]ab/WTVf/U_W4^_tX%] TVZ\VW4ZSW4^X%]aWifffej]abCX%]0U_W4ZSU_TV^qfi]8d U;Z/TV]8XVn2n u@WiY]aT
iW4RgW4ZCi8TVZ~]abSW5XVcfi]aQCXVnu@TV^n2i/sUd Z*cfiW]ab/WYX%\VW4Z] kffX_eYZ/TV]0`Z/TuubCX%]&]abSWYXVcfi]aQCXVn)u@TV^n2id Uq0-ZCcfiW
X%\SXVd ZCs7]ab/W8R/^_T1cfiWiQ/^_WYd U~^_WX%U_TVZ*X%fCn Wd Z]abCX%]']ab/WX%\VW4Z`]ffd U'TVZCn e}XVn2n u@Wij]aTmcfiTVZ]a^_TSn7ubCX%]hc4X%Z
fgW5cfiTVZ`]a^TSn2n Wid ZoX%Z~W4`RgW4^d2kW4Z] XVn)U_W4]aQSRCq
W Z/T uvR/^W4U_W4Z`Q
] {:f s]ab/W4ZX%^_\VQ/W ]abCX%]hd ]hd U~^_WX%U_TVZCX%f*n Wjd Zl]ab/W8U_W4Z/U_WX%fATtVWsX%ZCi
]abCX%]8d ]0\VW4Z/W4^X%]aW4U6XVn2nDX%ZCihTVZCn e
e Lr6i/d U]a^d f/Q/] TVZ/Uq
[<a21\ rk a\

{:f)z

VqZD^_W4RCX%^X%] TVZ*
Ld >X%ZoX%^_f*d ]a^X%^_e>i/d U_]a^d fSQ/] TVZ^Xg




Ld >XU_W4](O

TVZvq

TSw/R*X%^_] ] TVZ/U0TSwRvs1X%ZCihSlX%ZX%^fCd ]a^X%^_ei/d U_]a^d fSQ/] TVZ^X[TVZ[O5q

b/T`TVUW6Z`QCk0fgW4^_Ufiff h V4V{X%ZCi<w_ h V4;wxTV^WXVcOb}RCXVd ^ja" @U_QCcb}]abCX%]

X%ZCi UX%] Uwe1d Z/\>]ab/W[wxTSn2n Tu8d Z/\cfiTVZSU_]a^XVd Z] swxTV^YWXVcb U_QCcb]abCX%]
X hKV

X;
_

) P / F P `
Vq{&W4Z/W4^X%] TVZC
Vq2b/T`TVUW.v
Vq2b/T`TVUW
Vq2m


x

XVc4cfiTV^iSd Z/\ff]aTaXE>q

XVc4cfiTV^i/d Z/\5]aTaXq[~/W4]8fgW]ab/WQ/ZCd2 Q/WU_W4]8d Z

]ab~RS^_TVfCX%fCd2n2d ]e
U]aW4RVq2Vq

_

s)^W4]aQ/^_Z} 480X%ZCi6bCXVn ] qL



U_QCcOb]abCX%]Vv>q

]ab~R/^_TVf*X%fCd2n2d ]eq

_

s\VTff]aT

fi]hd UWX%U_e~]aTjUW4W8]abCX%6
] {:f)zd U']abSW8U_RgWc4d2XVnLc4X%U_W TSw){:f)zFub/W4^_W! _ YwTV^ffXVn2n
a" aq8rhn2n Tu8d Z/\^w_ \Sd tVW4U8Q/UffXn2d ]a] n WkTV^_W3nSW4gd fCd2n2d ]eq TQ/ZCiVW4^_U_] X%ZCiY]ab/Wh^_TSn W6TSwE]ab/W
cfiTVZ/U_]a^XVd Z`]YSas1Z/TV]aW6]abCX%]w_ U{]abSW RS^_TVfCX%fCd2n2d ]e]abCX%]]abSW~XVn \VTV^d ]abCkiTW4UZ/TV]]aW4^k5d ZCX%]aW~X%]
U_]aW4RoVq2Vs\Sd tVW4Z]abCX%]$X%Z*i X%^_W5cOb/TVU_W4ZoX%]&U]aW4RoVq2VqLfi]wxTSn2n TuU0]abCX%]&]abSWR/^_TVfCX%fCd2n2d ]e4l]abCX%]
XRCXVd ^ 4;d UZ/TV]TVQS]aR/Q/]X%]U_]aW4RVq25wTV^U_TSkWYd U




`
) P / F P

X _

b Q/UsAS*UXaeU&]abCX%]-]ab/WR/^_TVf*X%fCd2n2d ]e4]abCX%]{X0R*XVd ^&ub/TVU_W'/^_U_]cfiTSkRgTVZ/W4Z]d U[d U&Z/TV]9TVQS]aR/Q/]

X%]U_]aW4R[Vq2ffd U]ab/WUXVkWYwTV^8XVn2n4 vq
{:f)z c4X%Z\VW4Z/W4^X%]aW>]ab/
W Lrh i/d U_]a^d fSQ/] TVZ$d Z11XVkRCn W}Sq2Vs'ubCd2cbcfiTVQCn2i[Z/TV]fgW
\VW4Z/W4^X%]aWif`^
e {:fzLq TU_W4W6]abCd UsAQSUd Z/\]ab/W6UXVkWhZ/TV] X%] TVZzX%U5d Z]ab/W6W41XVkRCn WsBcfiTVZ/Ud2iW4^
]ab/WmU_W4]TSw&R*X%^_] ] TVZ/x
U & + . u8d ]ab * V * ' * Vq ~W4q
] X & X +
X;- . VVV`
/ ) VsCX%ZCN
) Vq&fi] UWX%U_e~]aTYtVW4^d2we]abCX%]6wxTV^6XVn2g
n $
vsu@W8bCX_tVW

]abCX%]
P F P ` X _ VVVsU_Tl]abCX%]h]ab/WcfiTVZ/U_]a^XVd Z]~Shd U UX%] U_/Wi/q5TV^_W4T tVW4^s
i/d ^_Wcfi]c4XVn2cfiQCn2X%] TVZU_bST uUj]abCX%] swTV^mX%^_f*d ]a^X%^_e XE>s@]ab/WiSd U_]a^d f/Q/] TV{
Z 1^ TVZ^_QSZ/U5\VW4Z/W4^X%]aWi
fa
e Q6f)zF-u8d ]ab~]abCd Uhcb/TSd2cfiW TSw7RCX%^XVkW4]aW4^U U'R/^_Wc4d U_Wn e]ab/W8Q/ZCd2 Q/W5i/d U_]a^d fSQ/] TVZUX%] Uwe1d Z/\
Lr6d Z]ab*d U8c4X%U_Wq

`7

fi " |qxq}t~"}s1|

PT~ub`ed
U {:fz`'^_WX%U_TVZCX%f*n W$1tVW4Zo]ab/TVQ/\Vbou9Wffb*XatVWjZ/TV]{\Sd tVW4ZX~wTV^kffXVnBiW4SZCd ] TVZ
TSw'^WX%U_TVZCX%fCn WaXVn ]abSTVQ/\Vb|d ]5c4X%Z>fgWiTVZ/Wd Z]ab/W6^_Q/ZSUhw^XVkW4u@TV^__W4U_U_W4Z] d2XVn2n esDWXVcOb>U_]aW4R>TSw
]ab/WoXVn \VTV^d ]abCk c4X%ZviW4RgW4ZCilTVZCn eTVZd ZCwTV^kffX%] TVZvXatXVd2n2X%fCn W~]aTo]ab/WW4`RgW4^d2kW4Z`]aW4^s9ubSW4^_W]ab/W
%d ZCwTV^kffX%] TVZCd UhW4ZCcfiT1iWi[d Z[]ab/WTVf/U_W4^_tX%] TVZ/U~kffXViVWYfe>]ab/WW4RAW4^d2kW4Z`]aW4^~d Z[]ab/WcfiTVQ/^_UWjTSw
^_Q/ZSZCd Z/\h]ab/W5XVn \VTV^d ]abCk5asCd ]8d U0Z/TV]bCX%^i ]aTYU_W4W{]abCX%{
] {:fz UX%] U_/W4U;TVQ/^8d Z]aQCd ] tVWjiW4Ud2iW4^_p
X%] XVq b/WjVW4e}RgTSd Z]d Uh]abCX%]XVn2nC]ab/Wj^_Wn W4tX%Z`]6U_]aW4RSUd Z[]ab/WXVn \VTV^d ]abCk c4X%ZfgWmc4X%^_^d WilTVQ/] f`e
X%ZmW4`RgW4^d2kW4Z`]aW4^q bSWRCX%^XVkW4]aW4^_fi
U 5X%ZC4
_ wxTV^
OX%ZCi
N
X%^_WcbSTVU_W4ZmfAWwTV^_W ]ab/W
XVn \VTV^d ]abCk fAW4\Sd ZSUg]abCd U5c4X%ZcfiW4^_] XVd Z*n efgWiTVZ/WhfeX%ZW4RAW4^d2kW4Z`]aW4^q5P/d2kffd2n2X%^n esBd ]ffd UU_]a^XVd \Vb]ap
wTV^_uX%^i5]aTcbSWcl]abCX%]]ab/W8W QCX%] TVZS9b/TSn2iUffwTV^;WXVcO
b
vq;r'U6wTV^']ab/WXVn \VTV^d ]abCk ]aU_Wn2ws
]ab/WYW4`RgW4^d2kW4Z]aW4^ bCX%UZ/T[cfiTVZ`]a^TSnDTtVW4^6]ab/WlcOb/TSd2cfiWYTS
w hL]abCd Ud UjcOb/TVU_W4Z[f`emZCX%]aQS^_WlXVc4cfiTV^iSd Z/\
]aUliSd U_]a^d f/Q/] TVZ*(
X qy"T u@W4tVW4^s-]ab/WW4RgW4^d2kW4Z]aW4^lc4X%ZRgW4^wTV^kU_]aW4RSUVq2}X%ZCiVq2Vs@]ab*X%]d U
cbST`TVUd ZS\ !
OXVc4cfiTV^iSd Z/\6]aTh]ab/WR/^_TVfCX%fCd2n2d ]emiSd U_]a^d f/Q/] TVq
Z X;&sAX%Z*i^W4Wcfi] Z/\6]abSW'TVfSU_W4^_tX%] TVZ
u8d ]abR/^_TVf*X%fCd2n2d ]4
e _ s`Ud ZCcfiW]ab/WW4RAW4^d2kW4Z`]aW4^-Z/T u0U&fATV]abj]ab/WU_W4Z/UTV^cOb/TVU_W4Z}ad2q Wq2s -X%ZCi
]ab/W{TVfSU_W4^_tX%] TVZaaq
] {:fFiTW4UW4gXVcfi] n eu0bCX%]&u9WuX%Z] q
bSW5wxTSn2n Tu8d Z/\5]abSW4TV^_Wk U_bST uU]ab*X%f
\b"a\
eE @ n Y, g G8%)/|n 'jR G8%
{/jY, R G8| n S,
:
1^@% )Y. C,MK-xQy ;//
;'@ /;G'"`fi|ph %
z ,a
GM(
} {:fz`0) 8ff &7 /mE1^ a5 W>/ h4 /
V'7fi| | +j`{:fz`8fi ),S 48,



9G6 zEJ

KMqWNOJ ~JF

fiZ]abSWR/^_W4t1d TVQ/UhU_Wcfi] TVZCs"u9W>X%U_U_Q*kWil]abCX%]6]ab/W>d ZCwTV^kffX%] TVZ^_WcfiWd tVWi>uX%U6TSwB]ab/W}wTV^k ]ab/W
XVcfi]aQCXVnLu@TV^n2id U~d ZVq}ZCwTV^kffX%] TVZkQ/U] fgW>d Z]abCd U~wTV^k]aT|X%RSRCn ecfiTVZCiSd ] TVZCd Z/\SqzCQ/]~d Z
\VW4Z/W4^XVn2s*d ZCwTV^kffX%] TVZiTW4U;Z/TV]hXVn uX_e`UffcfiTSkWd Z~UQCcblZCd2cfiWR*XVcX%\VW4Uq&fiZ~]abCd U'U_Wcfi] TVZmu@WU_]aQCiVe
kTV^_W\VW4Z/W4^XVn)]e`RgW4UTSw7TVf/U_W4^t)X%] TVZ/UsDn WXVi/d Z/\ff]aT5\VW4Z/W4^XVn2d X%] TVZ/UTSwLcfiTVZCi/d ] TVZCd ZS\Sq

:e

\&Za\

%-

lr)" ^ " )
l "l)

7W4^_bCX%R/UE]ab/WUd2kRCn W4U]DRgTVU_Ud fCn W\VW4Z/W4^XVn2d X%] TVZd UE]aT8X%U_U_QCkW]abCX%]D]ab/W4^_W0d U-X*RCX%^_] ] TVZV & ^i^i^i 4 j
TSw
X%ZCi]ab/WX%\VW4Z]TVfSU_W4^_tVW4Uq & & ^i^i^i^gjAFj7s1ub/W4^W &
^^ ^^Xgj Vq bCd Ujd U]aT~fgW
Z]aW4^_R/^_W4]aWiX%UX%Z[TVf/U_W4^_tX%] TVZ]abCX%]n WXViU6]ab/W>X%\VW4Z]6]aTmfAWn2d W4tVW> u8d ]ab[R/^_TVfCX%fCd2n2d ]eE s&wTV^
V^ i^i^i 6 l"qLr6c4cfiTV^i/d ZS\ff]a
4W4A^_W4ecfiTVZCi/d ] TVZCd ZS\Ss\Sd tVW4ZX5i/d U_]a^d f/QS] TV^
Z XgTV
Z vs
XE>u6

& Xg>ufi & ^^ Ej;XEufiFjAui
4W4A^_W4ecfiTVZ*i/d ] TVZCd Z/\d U>iW4/Z/Wi>TVZCn ed2w.C*q od2kR*n2d W4Uj]ab*X%]4XE>a+* V d2w.C* X%ZCi
XE>a+*fi Vs]abSW4Z;*Xg}ufi**d U] X%VW4Z5]aTfAW{Vq
n WX%^n e8TV^i/d ZCX%^ejcfiTVZCi/d ] TVZ*d Z/\5d U]ab/W-U_RgWc4d2XVn
c4X%U_W8TSwW47^W4e}cfiTVZCi/d ] TVZCd Z/\Yub/W4^_WWC
* YwxTV^;U_TSkW{@U_TSsDX%U6d U0U] X%ZCi/X%^i/su9WYiVWn2d fAW4^X%]aWn eQ/U_W
]ab/W{UXVkWZ/TV] X%] TVZwxTV^QSRiSX%] Z/\hQ/Ud Z/\ W47^_W4e>cfiTVZCiSd ] TVZCd Z/\X%ZCi6TV^i/d ZCX%^_emcfiTVZCi/d ] TVZCd ZS\Sq
WffZST u uX%Z`]]aTiW4]aW4^kffd Z/W6u0b/W4ZQ/RBi/X%] Z/\d Z>]ab/W6ZCXVd tVW5U_RCXVcfiW6Q/Ud ZS\e W47^W4e[cfiTVZCiSd ] TVZ/p
&

& ^i^i^i ^EjSFj7

Z/\[d UX%R/RS^_TVR/^d2X%]aWq bQ/UsEu@WlX%U_U_Q*kW5]abCX%]8]ab/WlX%\VW4Z] U6TVf/U_W4^t)X%] TVZ/U Z/TubCX_tVWY]abSWlwTV^kTSw
& & ^ i^i^i ^ j j wxTV^&UTSkWRCX%^_] ] TVZoV & ^i^i^i4 j ;TSwFvq&urd 7W4^W4Z`]TVfSU_W4^_tX%] TVZ/U8kffX_esDd Z\VW4Z/p
W4^XVn2s-QSU_Wi/d AW4^_W4Z]YR*X%^_] ] TVZ/Uq2 4Q/U_]mX%Uu9Wi/d2iwTV^j]ab/W[c4X%UW]abCX%]jTVf/U_W4^t)X%] TVZ/U>X%^_WlW4tVW4Z]aU
aPWcfi] TVZVsS/^_U_];RCX%^X%\V^X%RSbCas/u@W TVZCcfiWX%\SXVd ZX%U_UQCkW]ab*X%];]ab/WX%\VW4Z] UTVf/U_W4^_tX%] TVZ/UffX%^_WXVc4cfiQ/p
^X%]aWqjbCX%]jiVT`W4U ]abCX%]jkWX%Zzd Zo]ab/W5R/^W4U_W4Z`]jcfiTVZ`]aW4] WYUd2kRCn em^_WQ*d ^_W5]abCX%] s-cfiTVZCiSd ] TVZCXVn

`7

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

TVZkffX%1d Z/\]ab/WhTVf/U_W4^_tX%] TVZCsg]abSW RS^_TVfCX%fCd2n2d ]e~TSw"*^_WXVn2n ed UWC*0wTV^'
V^ i^i^i 6 l"su@W{bCXatVW

D^W>*C



V^i^i^i6l"q bCX%]ffd UswTV^

& ^i^i^i ^EjSFj7 C*Gi
aV
bCd Uc4n WX%^n ej\VW4Z/W4^XVn2d 4W4U]abSW;^_W QCd ^_WkW4Z`]@TSwDXVc4cfiQ/^XVcfieY\Sd tVW4Z}d Z]ab/Whc4X%UW']ab*X%]@]abSW;TVf/UW4^_t)X%] TVZSU
X%^_W{W4tVW4Z]aUq
w"TV]&U_Q/^_RS^d Ud Z/\Sn es`]ab/W4^_W5d UX'\VW4ZSW4^XVn2d X%] TVZTSwS]ab/W
W Lr6cfiTVZCiSd ] TVZ]abCX%]d U&Z/W4WiWi ]aT5\VQCX%^_p
X%Z]aW4W]abCX%
] W47^W4emcfiTVZCiSd ] TVZCd Z/\lc4X%Z~fAW5X%RSRCn2d Wih]aTff]ab/WZCXVd tVW{U_RCXVcfiWq
&

A|,| +W1^heL`%V & ^i^i^i4Fj7YGZ*/{A|,| +
\b"a\
:e L
& ^ i^i^i^gj)ffR & ^^ gj E ) 8 | , n & & ^i^i^i^gj7FjE#Lp
@VV^i^i^i 6"
l W* lK+Sfffi6, )n +
Oz %}9=D
^ 8V &
e1 ^W 8 ^8
6 & & ^i^i^i^gj7Fj7R
>*fi
zu}D ^ W ^ W +*fi 8?
[+*)D ^ W
!
h

LX%^_]fffC"TSw b/W4TV^WkVq2d U6X%ZCXVn TV\VTVQ/U{]aTjRCX%^_]6ac49TSw b/W4TV^WkVq2Vq bSW4^_WX%^WXZ`Q*kfgW4^


TSwEcfiTVZ*i/d ] TVZ/U-WQ*d t)XVn W4Z]&]aTfCC]abCX%]-u9WffcfiTVQCn2i8bCX_tVWU_] X%]aWi/sUd2kffd2n2X%^d ZU_R*d ^d ]9]aTh]ab/W6cfiTVZCi/d ] TVZSU
Z b/W4TV^_Wk Vq2V
q w"TV]aW6]abCX%]]ab/W4U_WX%^_WhW4tVW4ZkTV^_W U_]a^d Z/\VW4Z]ffcfiTVZCi/d ] TVZ/U]ab*X%ZX%^W ^WQCd ^WiwTV^
TV^i/d Z*X%^_emcfiTVZ*i/d ] TVZCd Z/\ff]aT5fAW5X%RSR/^_TVR/^d2X%]aWq
DgXVkR*n W4UhVq2jX%ZCi6Sq2YXVn ^_WXVieUQ/\V\VW4U_]0]ab*X%]0]ab/W4^_WX%^_WZSTV]0]aTTmkffX%ZeZ/TVZ]a^d t1d2XVnUcfiW4ZCX%^d TVU
ub/W4^W6X%RSRCn egd ZS
\ 4W4A^_W4emcfiTVZ*i/d ] TVZCd Z/\ff]aT6]abSW{ZCXVd tVWU_RCXVcfiWjd U8X%R/R/^_TVRS^d2X%]aWq&yBTu9W4tVW4^s/Q/U_]{X%U8wTV^
]ab/WhTV^d \Sd ZCXV2
n rh cfiTVZ*i/d ] TVZCsA]ab/W4^_WiTW41d U_]U_RgWc4d2XVn7Ud ]aQCX%] TVZ/Ujd Zlu0bCd2cb\VW4Z/W4^XVn2d 4W
Lr6 U
X6^WXVn2d U_] d2c5X%U_UQCkR/] TVZCqlTV^ TV^i/d ZCX%^
e Lr60s1u9WmkW4Z`] TVZSWi]ab/\
W {:fzkWcObCX%ZCd Uk aPWcfip
] TVZmSq2Vaq6TVW
^ 4W4A^_W4eocfiTVZ*i/d ] TVZCd Z/\SsXUd2kffd2n2X%^ffkWcObCX%ZCd Uk kffX_e~fgWjX^_WXVn2d U_] d2c6kT1iWn*d ZmU_TSkW
Ud ]aQCX%] TVZSUub/W4^W~XVn2ngTVf/U_W4^_tX%] TVZ/U8^_WwxW4^]aT]ab/WffUXVkWffRCX%^_] ] TVZzV & ^ i^i^i4
j7hTS;
w vq W5Z/Tu
iW4Ucfi^d fAWX"UcfiW4ZCX%^d TffwTV^*UQCcbX9Ud ]aQCX%] TVZCq-PQSR/RgTVU_0
W cfiTVZ/Ud U]aUTS]
w ="TVf/U_W4^_tX%] TVZ/(
U & ^ i^i^i 2
u8d ]a
b `* C* & & ^ i^i^i ^ C*(jSF
jU_QCcOb>]abCX%]5XVn2!
n C*(V
q w"T u8sD/
laX%^_f*d ]a^X%^_e1{cfiTVZCi/d ] TVZCXVn"i/d U_p
]a^d f/QS] TVZ/W
U 18^ sg V^ i^i^i6 l"sgTV
Z vq{fiZ]aQCd ] tVWn eH
D,^ ffd
U D,^ >u fiF
aq
q TVZ/Ud2iW4^]ab/WwTSn2n u8d ZS\
kWcObCX%ZCd Ukff6/^_U_]X%Z[TVf/U_W4^_tX%] TVZ `*{d Ucb/TVUW4ZaXVc4cfiTV^i/d Z/\]aTmU_TSkWi/d U_]a^d fSQ/] TVJ
Z X TV
Z
]ab/W4ZX{UW4] UffcbSTVU_W4Zu ]abR/^TVfCX%fCd2n2d ]
e *( ad2q Wq2sLXVc4cfiTV^i/d Z/\]aTj]ab/Wji/d U]a^d f/Q/] TVZd ZCiQCcfiWifff`e
`*fia)/ZCXVn2n esDXu9TV^n2
v>
U8cOb/TVU_W4Z[XVc4cfiTV^i/d Z/\ff]a
18^ q
4w*]ab/WjTVf/U_W4^_tX%] TVJ
Z `*X%ZCiu@TV^n2
X%^_Wj\VW4Z/W4^X%]aWi]abCd UhuX_esL]ab/W4Z]ab/W5\VW4ZSW4^XVn2d 4W
Lr6
cfiTVZCi/d ] TVZb/TSn2iUs]ab*X%]8d UsDcfiTVZCiSd ] TVZCd Z/\d Z]abSWU_TVR/bCd U_] d2c4X%]aWi5U_RCXVcfiWjcfiTSd ZCc4d2iW4U0u ]a
b 4W4A^_W4ecfiTVZ/p
i/d ] TVZCd ZS\S

:e *S {`%lV & ^i^i^i 4 j YGZ /j5BGj| , n /
8|n7S67n ,% )qX \X Y`*fi # 6/'|VV^i^i^i^&

7,;5,, )1^h ,ffeX 1^ z7X 'phfi /D6#D^h0}
1^5 K- {6 fi
,} x0y/z8*ffpogzu}8} & ^i^i^i 4jY

[<a"ZW " ^ " l



D^_TVRgTVUd ] TVZvVq2[iWkTVZ/U_]a^X%]aW4Uj]abCX%] s&W4tVW4Z]ab/TVQ/\Vb]ab/WX%ZCXVn TV\VQ/WmTSw@]ab/WLrh cfiTVZCiSd ] TVZ
W4R/^_W4U_U_Wijd Z b/W4TV^_WkVq2hd U9b*X%^i{]aThUX%] Uwxed Zj\VW4Z/W4^XVn2s1X%]n WX%U_]{d2w)]ab/W'U_W4]V & ^i^i^i4 j U-]ab/W
UXVkW6wTV^XVn2n`TVf/UW4^_t)X%] TVZSUs]ab/W4ZwxTV^-W4tVW4^_eYU_QCcbU_W4]-TSw)TVfSU_W4^_tX%] TVZ/U-]ab/W4^_WW4gd U],|ph'R/^d TV^_UZ1^
TV
Z wTV^ubCd2cOb>]ab/
W rhBpX%ZCXVn TV\VQ/W}UX%] U/Wi}wTV^jXVn2nDTVf/U_W4^_tX%] TVZ/Uq5r;U8u9WYU_b/Tu Z/W4] s9wTV^
l0QSRiSX%] Z/\Ss]abCd U8d U;Z/T~n TVZ/\VW4^0]abSWYc4X%U_Wq
`7&=

fi " |qxq}t~"}s1|

:e $Z"%l"k*

\ p%n ^

"

\

l ^`a"

%

r:n ^ %" l)

bCX%]@X%fgTVQ/]9c4X%UW4UEub/W4^WB]ab/W;cfiTVZ/U]a^XVd Z`]aU&X%^_W9Z/TV]9d Z]abSWBU_RgWc4d2XVnwTV^k[ubSW4^_W04W4A^_W4e1 U@cfiTVZCiSd ] TVZ/p
Z/\c4X%ZfgWhX%R/R*n2d Wi/
7W4^_bCX%RSU@]abSW6kTVU_]cfiTSk5kTVZ}X%R/R/^_TSXVcOb>d Zj]ab*d Uc4X%U_Wffd U@]aThQ/UWhl09qAd tVW4Z
XcfiTVZSU_]a^XVd Z]ubSW4^_W8XcfiTVZSU_]a^XVd Z]d U@Ud2kRCn eXU_W4]"TSwR/^TVfCX%fCd2n2d ]eli/d U_]a^d f/QS] TVZ/U Z]aQCd ] tVWn es`]ab/W
i/d U_]a^d f/Q/] TVZ/U UX%] Uwegd ZS\]ab/WcfiTVZ/U_]a^XVd Z`] hX%ZCiXffRS^d TV^iSd U_]a^d f/Q/] TV>
Z XE TV
Z vsC]ab/Wld2iWXmd U ]aT
RCd2cOgsgXVkTVZ/\XVn2nAi/d U_]a^d fSQ/] TVZ/U@UX%] Uwegd Z/\h]ab/W6cfiTVZ/U_]a^XVd Z`] s]ab/W'TVZ/W']abCX%]{d UY%c4n TVU_W4U_] ']aT ]ab/WR/^d TV^
i/d U_]a^d f/Q/] TVZCsAubSW4^_Wh]ab/W%c4n TVU_W4Z/W4U_UhTS
w X ]a^
XE UYkWX%U_Q/^WijQ/Ud Z/\^_Wn2X%] tVWffW4Z]a^_TVReq b/W
fi +o nh /fi7=
, , x
X /3
XE u t-QCn2n fCXVcOl ~Wd f*n W4^sgVVVVC
LT tVW4^ bSTSkffX%UsAVVVVd U
iW4/ZSWiX%U
X
X hn TV\

Xg}
FX
bSW0n TV\SX%^d ]abCkb/W4^_W0d UD] X%VW4Z8]aT]ab/WBfCX%UW0Vd2wTX L]ab/W4ZUX hn TV\gYX ha/Xgo
U'] X%VW4Z]aTYfgWYVq bCd U U^_WX%UTVZCX%fCn W Ud ZCcfiWjn2d2k - n TV\g*V Yd2wQvVq2 b/W^_Wn2X%] tVW6W4Z/p
]a^_TVRezd U5/ZCd ]aWR/^_Ttgd2iVWil]abCX%]aX Um| ,+
oS/))5u8d ]ab^W4U_RgWcfi]6]aTX s0d Z]abCX%]~d2w
XE> Vs)]ab/W48
Z X VsDwTV^8XVn2n4 vq&-]ab/W4^_u U_Ws1d ]8d U iVW4/Z/Wi ]aTfffgW5d Z//Z*d ]aWq
bSWhcfiTVZ/U]a^XVd Z`]aU&u@WffcfiTVZ/Ud2iW4^-b/W4^WhX%^_W6XVn2ngc4n TVU_WiX%ZCicfiTVZtVW45U_W4]aU&TSw)RS^_TVfCX%fCd2n2d ]elkWX%U_QS^_W4Uq
fiZo]abCd Uc4X%U_Ws&d ]d U `ZST uZ[]abCX%] ]ab/W4^_Wd UXffQ/Z*d2Q/Wli/d U_]a^d f/QS] TVZ]ab*X%]8UX%] U/W4Uh]ab/WmcfiTVZSU_]a^XVd Z]aU
X%ZCizkffd ZCd2k5d 4W4Uj]abSWl^Wn2X%] tVWW4Z`]a^_TVReq {d tVW4Z$XZ/TVZ/WkR/]ecfiTVZSU_]a^XVd Z^
] X%Z*iX~R/^_TVf*X%fCd2n2d ]e
i/d U_]a^d f/Q/] TVa
Z XETV
Z vs/n W4
] XE>u 8BiW4Z/TV]aW]abSW8i/d U_]a^d fSQ/] TVZ6]abCX%];kffd Z*d2kffd 4W4U"^_Wn2X%] tVW;W4Z`]a^TVR`e
u8d ]ab^W4U_RgWcfi]&]aN
X q
4w*]ab/WcfiTVZ/U_]a^XVd Z]aUhbCXatVW]ab/WwxTV^k ]aTmubCd2cO
b W47^_W4e1 UQCn Wd UX%RSRCn2d2c4X%fCn WsL]abCX%]d Us-d2wL]ab/W4e
bCX_tVW~]ab/W[wxTV^k /
X
X a+
* ;*fifi V^ i^i^i 6 l"}wTV^5UTSkW~RCX%^] ] TVZvV & ^ i^i^i4F
jAVs9]ab/W4Z
]d UYu@Wn2n9`Z/TuZ]abCX%]Y]abSWi/d U_]a^d f/Q/] TVZ]abCX%]mk5d ZCd2kffd 4W4UW4Z`]a^TVR`e^Wn2X%] tVWl]aTX~R/^d TV<
^ Xg U
XE>u 6 & & ^ i^i^i ^ EjS
jS0UW4Ws/Wq \Sq2s*u rd2XVcfiTVZCd U5
DX%fgWn2n2sCVVVVaaq bQ/UsDl0oQ/RBi/X%] Z/\6\VW4ZSW4^_p
XVn2d 4W4
U 4W4A^_W4emcfiTVZ*i/d ] TVZCd Z/\maX%ZCihb/W4ZCcfiW5XVn UT5U_] X%ZCiSX%^icfiTVZCi/d ] TVZCd ZS\Saq
T~U_]aQ*ie[l0Q/RBi/X%] Z/\}d Z}TVQ/^jw^XVkW4u@TV^_1s1u@WlX%U_U_Q*kW6]abCX%]]abSWffTVf/U_W4^_tX%] TVZ/UX%^_WffZ/Tu
X%^_fCd ]a^X%^_elc4n TVU_WicfiTVZtVW4>cfiTVZ/U_]a^XVd Z]aUTVZ]ab/WR/^TVfCX%fCd2n2d ]ekWX%U_QS^_Wq*r;\SXVd Z*su9WffX%U_U_Q*kW]abCX%]&]ab/W
TVf/U_W4^t)X%] TVZ/UX%^_WXVc4cfiQS^X%]aWmd Zo]abCX%] s@cfiTVZCi/d ] TVZCXVnDTVZkffX%1d Z/\~]abSWffTVf/U_W4^_tX%] TVZCsC]ab/WcfiTVZSU_]a^XVd Z]aU
b/TSn2i/qTV^ZST u8sEu9WlwxT1cfiQ/U{TVZo]ab/WffUd2kR*n W4U_]{RgTVU_Ud fCn W~c4X%UW5]abCX%]c4X%Z/Z/TV]{fgW6bCX%ZCiSn Wif`
e W47^_W4e
Q/RBi/X%] Z/\SqZ]abCd U5c4X%UWsBcfiTVZ/U_]a^XVd Z]aUYTVf/UW4^_t)X%] TVZSU-U_] d2n2nAbCX_tVW6]ab/WwxTV^k & & ^ i^i^i^ gj7F
j7sgf/Q/]
Z/Tu]ab/W+
*fi U6iT5Z/TV]'bCX_tVW8]aTwxTV^k X{RCX%^_] ] TVZ]ab/W4eokffX_eT tVW4^n2X%RX%Z*i/%TV^;ZSTV]hcfiTtVW4W
^ ;X%ZCi
]ab/
W * iTZ/TV]bCX_tVW6]aTU_QCk]aT>Vq5PQ*cbX%ZTVf/U_W4^t)X%] TVZd U5XVc4cfiQ/^X%]aWd2w"d ]UX%] U_SW4UYaVasgQ/U_]ffX%U
fgWwxTV^Wq
W~c4X%ZmZ/Tu X%U~]ab/W UXVkWQSW4U_] TVZ/U]abCX%]u@WX%U_VWijfgWwTV^_WX%fATVQS];TV^iSd ZCX%^_e}cfiTVZCi/d ] TVZ*d Z/\
X%ZC
W47^W4emcfiTVZCiSd ] TVZCd Z/\d Z]abSW{ZCXVd tVW{U_R*XVcfiWq
Vq-fiU&]ab/W4^_WffX%Z}XVn ]aW4^_Z*X%] tVWYcbCX%^XVcfi]aW4^d X%] TVZTSw/]ab/W5cfiTVZCi/d ] TVZSU&Q/ZCiW4^@ubCd2cOb};mQ/RBi/X%] Z/\
cfiTSd Z*c4d2iW4U{u8d ]abcfiTVZCi/d ] TVZ*d Z/\}d Zm]ab/WhU_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiW bCX%]ffd UsX%^_Wh]ab/W4^_WX%ZCXVn TV\VQSW4U
TSw bSW4TV^_WkVq2ffX%ZCi b/W4TV^_WkVq2YwxTV^ ;Q/RBi/X%] Z/\S
Vq-r'^_W ]ab/W4^_WcfiTSkfCd Z*X%] TVZ/UTSw!X%ZCi
wxTV^'ubCd2cOb[d ]6d U'Z/TV];W4tVW4ZmRgTVU_Ud f*n W]abCX%]6;c4X%Z
cfiTSd Z*c4d2iWu8d ]abcfiTVZCi/d ] TVZCd Z/\ld Z]ab/W{U_TVRSbCd U_] d2c4X%]aWi6URCXVcfiW


]ab~^_W4\SX%^i6]aTQSW4U_] TVZVsDd ] UWX%U_e]aTffR/^_Ttgd2iVWYXffcfiTVQ/Z]aW4^_W41XVkRCn WUb/T u8d ZS\6]abCX%]]ab/W4^_Wjd UZ/T
TVftgd TVQ/U&X%ZCXVn TV\VQ/W9]aT bSW4TV^_WkVq2'wxTV^-;"q b/W4^_W;d U-X0cfiTVZ/U_]a^XVd Z`]>U_Q*cb]ab*X%]D]ab/W'cfiTVZ*i/d ] TVZ8TSw

`77`

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

RCX%^_]aXVDTSw b/W4TV^_Wk$Vq20b/TSn2iUwxTV^l0Q/RBi/X%] Z/\{u0b/W4^_WX%U"R*X%^_]'fC9iTW4U9Z/TV]9b/TSn2i/q-W'TSkffd ]
]ab/W8iVW4] XVd2n U"b/W4^_Wq2h{wAcfiTVQ/^_UWsSd ];d U9RATVUUd fCn W&]abCX%]B]abSW4^_W8X%^_WUTSkWQ*d ]aW8i/d AW4^_W4Z`]'cfiTVZCi/d ] TVZ/U9]abCX%]
cb*X%^XVcfi]aW4^d 4Wub/W4Zl0|Q/RBi/X%] Z/\[cfiTSd ZCc4d2iW4U6u8d ]abcfiTVZCi/d ] TVZCd ZS\[d Z[]ab/WYU_TVR/bCd U_] d2c4X%]aWilURCXVcfiWq
y"T u@W4tVW4^s/W4tVW4Z[d2w7]ab/W4eW4gd U_] sU_QCcb[cfiTVZCi/d ] TVZ/UhkffXaefgWQ/ZCd Z]aW4^_W4U_] ZS\d Z]abCX%];]ab/W4emk5XaebCX%^iSn e
W4tVW4^X%R/RCn eqfiZCiW4Wi/sX%UXRCX%^_] d2XVn;X%Z/U_u@W4^Y]aTzQSW4U_] TVZvVs@u@WlZ/T u Z`]a^_T1iQCcfiW[XtVW4^_eUd2kRCn W
U_W4]a] Z/\d Z~u0bCd2cb[;}QSRiSX%] Z/\6Z/WcfiW4U_UX%^d2n eon WXViU']aTlX^W4U_QCn ] iSd 7W4^_W4Z] w^_TSk cfiTVZCi/d ] TVZCd Z/\md Z
]ab/W{UTVR/bCd U_] d2c4X%]aWiffU_RCXVcfiWq

~/W4]' & X%ZCi5 + fAW0]u@T8U_QSf/U_W4]aUBTSwU U_Q*cb5]ab*X%]Q & & + sg + + & sg . & k +
%X ZCi 2 & & + *X%^_W'XVn2n Z/TVZ/WkR/]eqLTVZ/Ud2iW4^@X'cfiTVZ/U_]a^XVd Z]*TSw]ab/WwTV^km & & ^ + +
ub/W4^W. & ^ + X%^W&fATV]ab~d ZlaV4Vaq*W8d Z`tVW4U] \SX%]aW;ub*X%]bCX%R/RgW4Z/U&d2wu@W&Q/U_W{l0QSRiSX%] Z/\TVZ[q
P/d ZCcfiWff & X%ZCi + TtVW4^n2X%RX%Z*ijiVT ZSTV]cfiT tVW4^]ab/WURCXVcfiWs1d Z\VW4Z/W4^XVnU4W4A^_W4emcfiTVZCi/d ] TVZCd Z/\~c4X%ZSZ/TV]
fgWX%RSRCn2d Wij]aTQ/RBi/X%]aWhTVZq b/W4^_WX%^_WhU_TSkW6Ud ]aQ*X%] TVZ/U{ub/W4^Ws*iVW4U_RCd ]aW6]ab/WhT tVW4^n2X%RCsKW47^_W4e
cfiTVZCi/d ] TVZ*d Z/\jc4X%ZYW4U_U_W4Z] d2XVn2n e5fgWX%R/RCn2d Wi/qCW'UXaeh]abCX%]9TVf/U_W4^_tX%] TVZ< & & ^ + + U`L
+o sXVwx]aW4^5l0Q/RBi/X%] ZS\5TVZTVZ/W TSw1]ab/WcfiTVZ/U]a^XVd Z`]aW
U & & TV=
^ + + sS]abSW TV]abSW4^5cfiTVZ/U_]a^XVd Z]
b/TSn2iU6X%U'u9Wn2n2q bCX%]hd U

U 4W4A^_W4epn2d VWu ]ab^_W4URAWcfi]']a<
X ;d2wAWd ]ab/W4fi
^ X + 6 & & +
TV\
^ Xg}a & 6 + + & qoPVQ/R/RgTVU_Wff]abCX%\
] XE>a + 6 & & + ]abSW4Zd ]d UhWX%U_e}]aTmU_bST u]abCX%]
XE>u 6 & & XE>u 6 & & ^ + + aq
fiZ`]aQ*d ] tVWn es d2w&]ab/W%c4n TVU_W4U_] i/d U]a^d f/Q/] TVZX ]aTJXE ]abCX%]UX%] U_/W4U^X & & XVn U_T
UX%] U_SW4
U X + + s]abSW4ZxX U*]abSWc4n TVU_W4U]0i/d U_]a^d f/QS] TVZ8]aTfiXE$]ab*X%]CUX%] U/W4U]ab/WcfiTVZ/U_]a^XVd Z]
& & ^ + + q.w"TV]aW8]ab*X%]hl0oQ/RBi/X%] Z/\5TVZ* U;W QCd tXVn W4Z`]']aTW47^_W4e}cfiTVZ*i/d ] TVZCd Z/\jTVZ
*4a Cau aq bQ/Us/d2w_d UQ4W4A^_W4epn2d VWs]abSW4Z5Q/RBi/X%] Z/\u8d ]ab4d UBW QCd tXVn W4Z`]9]aTqW47^_W4e
Q/RBi/X%] Z/\Sq



z Gm
%S/, / & + Gm| n S,ff
:e
' @
;* & & ^C* + + R V4
ZD^e,o% ) ,) * 1^ & ,
1^ + H
9/1^,} D^W hHv0 ?$ E HD^ 1^u
*
* ,"
/~
KD^ * ff=phSLGD^ * 96f & Q + {/fi fi
ffB
6//-|n 1^ 1^8u`*,\ Z, V4
\b"a\

`*

TV^ /Wi} & X%ZCi} + sDu@Wlc4X%Zd2iW4Z] d2wxeX%ZTVf/U_W4^_tX%] TVZ & & ^ + + u8d ]ab}]ab/WYRCXVd ^ju & ^ + {
aV4V + qBZ*iW4^ffTVQ/^mcfiTVZCi/d ] TVZ/UYTVZ & X%ZCi + s9]ab/WU_W4]ffTSwXVn2nW47^_W4epn2d VWlTVf/U_W4^_tX%] TVZ/Uld UX
U_Q/fSU_W4]TSw9u~/W4fgW4U_\VQ/WkWX%U_QS^_W6TSwD]ab*d U{U_W4] q bQ/Usg]ab/W6UW4]TSwDTVf/UW4^_t)X%] TVZSUjwTV^ubCd2cb;
cfiTVZCi/d ] TVZ*d Z/\YcfiTV^_^_W4URATVZ*iU]aTYcfiTVZCi/d ] TVZ*d Z/\Yd Z6]ab/WUTVR/bCd U_] d2c4X%]aWi{U_RCXVcfiW8d U;Xu~/W4fgW4U_\VQ/WBkWX%UQ/^_W
&UW4]'d Z5]abSW0U_RCXVcfiW;TSw`RATVUUd fCn WTVf/UW4^_t)X%] TVZSU[
q wBTV]aW'b/Tu9W4tVW4^s`]abCX%]9]abCd U"UW4]'iW4RgW4ZCiU"TVZ5]ab/W;R/^d TV^
X tVW4
^ vq
r^W4U_QCn ]jUd2kffd2n2X%^]aT b/W4TV^_Wk Vq2uX%UR/^T tVWiof`ePWd2iW4ZCwWn2i|aVVVVaX%Z*icfiTVZSUd2iW4^X%fCn e
\VW4Z/W4^XVn2d 4WilferXau8d2i[aVVVVaaqPWd2iVW4ZCwxWn2iU_b/TuU8]ab*X%] s*Q/ZCiW4^8tVW4^_e>u@WX%|cfiTVZCi/d ] TVZ/Us&;
Q/RBi/X%] Z/\c4X%Z/Z/TV]{cfiTSd ZCc4d2iVWu8d ]abU_TVR/bCd U] d2c4X%]aWicfiTVZCi/d ] TVZCd ZS\~d2w]ab/WTVf/U_W4^t)X%] TVZ/U&b*XatVW]ab/WffwTV^k
]ab/W|cfiTVZ*i/d ] TVZCXVn-R/^TVfCX%fCd2n2d ]ezTSw \Sd tVW4s
Z
U *aX%U[d U]ab/Wc4X%U_W|d Z]ab/
W 4QCiVeCW4Z/aXVkffd Z
R/^_TVf*n Wkffaq b/W4TV^_Wk Vq2U_bST uU6]abCX%]6]ab*d Ud Ud2kRgTVU_Ud f*n WjW4tVW4ZwxTV^6TVf/UW4^_t)X%] TVZSUffTSwB]abSW}kQCcOb
Ud2kR*n W4^}wxTV^
k & & ^ + + s&QSZCn W4U_Uu@Wc4X%Z^_WiQCcfiWl]ab/WR/^_TVf*n Wk ]a
W47^W4ecfiTVZCi/d ] TVZ*d Z/\ad Z
ubCd2cOboc4X%U_W b/W4TV^_WkVq25X%R/RCn2d W4Uaq

`7

fi " |qxq}t~"}s1|

8n
,`
W4tVW4Z]
W4tVW4Z]
RS^_TVfCX%fCd2n2d ]e
tVWcfi]aTV^
RS^_TVfCX%fCd2n2d ]e
tVWcfi]aTV^

CG{| , n /[
RCXVd ^_u8d UW5i/d U_TSd Z`]
%X ^_fCd ]a^X%^e
U_W4]
TSw
W4tVW4Z`]aU
R/^_TVfCX%f*d2n2d ] W4U
TSw
RCX%^_] ] TVZ
R/^_TVfCX%f*d2n2d ] W4UTSw]u@T
tVW4^n2X%RSRCd Z/\ffU_W4]aU

p;7+ 47+,
+h`ff%)

ZCXVd tVW
cfiTVZCi/d ] TVZ*d Z/\
ZCXVd tVW
cfiTVZCi/d ] TVZ*d Z/\
4W4A^_W4e
cfiTVZCi/d ] TVZ*d Z/\
l0

m/ ,
7,S//
XVn uX_e`U6u 1^_TVRgTVUd ] TVZSq2V
rhb/TSn2iU
b/W4TV^_Wk Vq2V
\VW4Z/W4^XVn2d X%] TVZ TSwLrh
b/TSn2iU8 bSW4TV^_Wk Vq2V
d2w/fgTV]abTVf/U_W4^t)X%] TVZ/m
U 4W4A^_W4ep
n2d VWj b/W4TV^WkVq2V

Ld \VQ/^_WYVLTVZCi/d ] TVZ/U;Q/ZCiW4^0ubCd2cbQ/RBi/X%] Z/\~d Z]ab/W{ZCXVd tVW8U_RCXVcfiWjcfiTSd Z*c4d2iW4U;u ]abcfiTVZCi/d ] TVZ*d Z/\
Z]ab/W{U_TVRSbCd U_] d2c4X%]aWi6URCXVcfiWq

9G6

NMfiNJ

W&bCX_tVW&U_]aQCi/d Wi']ab/W{c4d ^cfiQ*kU_] X%ZCcfiW4UQ/Z*iW4^LubCd2cbhTV^iSd ZCX%^_ejcfiTVZ*i/d ] TVZCd Z/\Ss]W47^W4ejcfiTVZCi/d ] TVZ*d Z/\Ss
X%ZCiY;~QSRiSX%] Z/\Yd ZlX0ZCXVd tVW;U_RCXVcfiW6c4X%ZYfgWQ/U_] /WiSsVub/W4^WQSU_] /Wi/8wxTV^@Q/U'kWX%Z/U6%X%\V^_W4W4U
u8d ]abvcfiTVZCi/d ] TVZCd Z/\d Z]ab/WUTVR/bCd U_] d2c4X%]aWi}U_RCXVcfiWVq b/W}kffXVd ZvkW4U_UX%\VWTSw9]abCd UffRCX%RgW4^d UY]abCX%]
W41cfiW4R/] wTV^ QCd ]aWURAWc4d2XVnCc4X%U_W4Us/]ab/W]ab/^W4WYkW4]ab/T1iU c4X%ZSZ/TV]0fgWQ/U_] /Wi/q'Ld \VQ/^_WU_QCkffk5X%^d 4W4U
]ab/WYk5XVd Zod Z/Ud \Vb]aU0TSwS]abCd URCX%RgW4^8d ZkTV^_W5iW4] XVd2n2q
r'Uu@WYkW4Z] TVZ/Wid Z]ab/W5d Z]a^_T1iQCcfi] TVZCs]ab/W5d2iWXTSw*cfiTSkRCX%^d ZS\~X%ZQ/RBi/X%]aW^_QCn Wffd ZoX~Z*XVd tVW
U_RCXVcfiW~u8d ]abvcfiTVZCi/d ] TVZCd Z/\d Z XU_TVR/b*d U_] d2c4X%]aWioU_RCXVcfiWd UYZ/TV]YZ/W4u88d ]mX%R/RgWX%^_Umd Z]ab/
W Lr6
n2d ]aW4^X%]aQ/^WX%ZCij]ab/W~l0n2d ]aW4^X%]aQ/^_WaX%U8u9Wn2nX%UYd ZR*X%RAW4^UU_QCcbX%Ujay;XVn RgW4^_Z| Q/]a] n Ws9VVVV
X%ZCi u rXau8d2ih
r'd2cOVW4esSVVVVaaqLfiZjXViSi/d ] TVZh]aT;f/^d Z/\Sd Z/\0]abSW4U_W9]u9T'U_]a^X%Z*iUCTSw^W4U_WX%^cbh]aTV\VW4]ab/W4^s
TVQ/^@T uZ>cfiTVZ]a^d f/Q/] TVZSUX%^W;]ab/W6wTSn2n u8d Z/\S0aXVEu@W'U_b/T uz]abCX%]@]ab/
W Lr6wx^XVkW4u9TV^_lc4X%ZjfgW;Q/U_Wi
X%UXff\VW4Z/W4^XVnD]aTTSnD]aTc4n2X%^d2we|kffX%ZeTSw*]ab/Wju9Wn2n pZ/T u0ZR*X%^XViT W4U TSwcfiTVZCi/d ] TVZCXVnER/^_TVf*X%fCd2n2d ]e1
fC*u@W\Sd tVWjX'\VW4Z/W4^XVnDcbCX%^XVcfi]aW4^d X%] TVZmTS
w Lr6d Z]aW4^kUTSw*X'fCd ZCX%^_ept)XVn QSWikffX%]a^d 1s)U_b/Tu8d Z/\
]abCX%]d ZkffX%Z`e[^_WXVn2d U] d2c6UcfiW4ZCX%^d TVUs9]ab/
W Lr6 cfiTVZCiSd ] TVZ //b/TSn2i b/W4TV^_WkSq Saac4u@W
iW4/ZSWX}kWcbCX%Z*d U
k {:f)z ]abCX%]ff\VW4ZSW4^X%]aW4UlXVn2nX%Z*imTVZCn ei/d U_]a^d f/QS] TVZ/U6UX%] Uwegd ZS\ Lr6
b/W4TV^WkSq2Va1ai/Du9W;U_b/Tu|]abCX%]9]ab/
W rhzcfiTVZCi/d ] TVZbCX%UX&ZCX%]aQS^XVnVW4]aW4Z/Ud TVZ]aTc4X%U_W4U9u0b/W4^_W
4W4A^_W4evcfiTVZCi/d ] TVZCd Z/\c4X%ZfgWX%R/RCn2d Wi bSW4TV^_Wk Vq2Va5X%ZCizW u@WU_b/Tu]ab*X%]Z/
LrhBpn2d VW
cfiTVZCi/d ] TVZc4X%Zb/TSn2id Z\VW4Z/W4^XVnDwxTV^ c4X%U_W4Uub/W4^W'TVZ*n eml0|aX%ZCihZ/TV
] W47^_W4e1*Q/RBi/X%] Z/\c4X%ZfgW
X%R/RCn2d Wi~ b/W4TV^_Wk Vq2Vaq
-QS^9^_W4UQCn ]aU9UQ/\V\VW4U_]-]abCX%]@u9TV^_1d Z/\d ZY]abSW;ZCXVd tVWU_R*XVcfiWhd U-^X%]ab/W4^@R/^_TVf*n WkffX%] d2c4q-Zj]ab/W'TV]ab/W4^
bCX%ZCiSsX%Uu@W-TVf/U_W4^_tVWi6d Z6]abSWd Z]a^_T1iQCcfi] TVZCsu@TV^_1d Z/\ffd Z6]ab/W@U_TVR/bCd U_] d2c4X%]aWiU_RCXVcfiWW4tVW4Z~X%U_U_Q*kffd Z/\
]8c4X%ZfAWffcfiTVZ/U]a^_QCcfi]aWi/d U;R/^_TVfCn Wk5X%] d2c-]aT`TSq0PTffubCX%]X%^W{]ab/W5XVn ]aW4^_ZCX%] tVW4U
TV^~TVZ/W]abCd Z/\Ss ]}d Uu9TV^]abTVf/U_W4^_t1d Z/\[]abCX%]}l0vQ/RBi/X%] ZS\d UZSTV]}XVn uX_e`UUTfCXVi/q fiZ
kffX%ZeU_QCc4cfiW4U_UwxQCn/R/^XVcfi] d2c4XVn*X%R/RCn2d2c4X%] TVZ/UsS]abSW%cfiTVZ/U_]a^XVd Z`] 8TVZlubCd2cbl]aT5QSRiSX%]aWYd U;TSwA]abSWjwTV^k
j
W l"s1ub/W4^_
W *'d U]ab/W6]ab>TVQ/] cfiTSkWYTSw9Xh^X%ZCiTSktX%^d2X%fCn =
W TVZ
j & *
r & * wxTV^{UTSkWn2X%^_\V=
vq b*X%]hd Us7u@W8TVfSU_W4^_tVWX%ZlWkRCd ^d2c4XVnLXatVW4^X%\VW6TSw1TVQ/] cfiTSkW4UTSYw [qfiZmU_QCcbXjc4X%U_WsA]ab/W;
i/d U_]a^d f/Q/] TVZd Ul%c4n TVU_Wad Z]ab/WX%R/R/^_TVR/^d2X%]aW~i/d U_] X%ZCcfiW~kWX%UQ/^_W9]aT]ab/W~i/d U_]a^d f/Q/] TVZmu9W~X%^^d tVW

`77a

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

X%]'f`eUTVR/bCd U_] d2c4X%]aWicfiTVZCiSd ] TVZCd Z/\Sq bCX%]6d UsCd2wK1^ D^8u[ asD^, D^ u
j
j iVW4Z/TV]aW4U{]ab/Wml1pwTSn2i5RS^_TgiVQCcfi];TSw9X R/^_TVfCX%f*d2n2d ]eiSd U_]a^d f/Q/] TVZ8sA]ab/W4Z

j & *
r & * > aasX%ZC<
j8 u 1,^ j tX%ZXVkRAW4ZSb/TVQ/]ff LTtVW4^s-VVVV
wTV^U_]
Q vlc4d W4Z`] n en2X%^_\V
W l"s1u@WffbCXatVWj]abCX%]Yu D,^
-
^ QS Z`uXVn2i/s1VVVVEPV`e^kUsgVVVV
!vffZSgsgVVVVaq b`QSUsgd ZU_QCcbc4X%U_W4U l0aXVn2kTVU_] 0cfiTSd ZCc4d2iVW4U
u8d ]ab>UTVR/bCd U_] d2c4X%]aWicfiTVZCi/d ] TVZCd ZS\oXVwx]aW4^XVn2n2q~aPVW4Wu rXau8d2i/s@VVVV wxTV^Xi/d UcfiQ/UUd TVZ>TSwEb/T u]abCd U
^_W4U_Q*n ]c4X%Z~fgW^_WcfiTVZCc4d2n Wiffu8d ]ab]ab/W{^W4U_QCn ]aUTSw*PWcfi] TVZVq2
CQ/]Lub/W4Zh]abCd UURAWc4d2XVnUd ]aQ*X%] TVZiTW4U*ZSTV]&X%RSRCn esd ]0d Uu9TV^]abX%Ugd Z/\u0b/W4]ab/W4^L]ab/W4^_W-W41d U_]aU0X%Z
%X R/R/^TSXVcbowTV^Q/RBi/X%] Z/\d Z]ab/WZCXVd tVWU_RCXVcfiW{]ab*X%]c4X%Z~fgWWX%Ud2n e>X%R/RCn2d Wid ZR/^XVcfi] d2c4XVnUd ]aQCX%] TVZSUs
eVW4];n WXViU"]aTq _sSd ZffU_TSkWwTV^kffXVn2n e R/^_Tt)X%fCn W&UW4Z/U_WsQ/RBi/X%]aWihi/d U]a^d f/Q/] TVZ/U]ab*X%Z6]ab/WkW4]ab/T1iU
u@WBbCXatVW'cfiTVZ/Ud2iW4^_WiS rtVW4^ehd Z]aW4^_W4U_] Z/\hc4X%ZCi/d2i/X%]aWs TSw]aW4Zjd ZCwTV^kffXVn2n effX%R/RCn2d Wi&f`e'b`Q*kffX%Z5X%\VW4Z`]aUs
U{]aTUd2kRCn e /5]ab/WX_t)XVd2n2X%fCn WYW4`]a^Xd ZCwTV^kffX%] TVZCq8fi]]aQ/^_Z/UTVQ/]]abCX%]]abSW4^_W~X%^_W6Ud ]aQCX%] TVZSU
ub/W4^W{]abCd U0QSRiSX%]aW{^_QCn W{fgW4bCX_tVW4U;fgW4]a]aW4^sCd Z[XR/^_Wc4d U_W8U_W4Z/U_Ws/]abCX%Z]abSW{]ab/^_W4WkW4]ab/T1iU0u@WbCX_tVW
cfiTVZ/Ud2iVW4^_Wi/q b*d U&u8d2n2n)fgWW4`R*n TV^_Wid ZowQ/]aQ/^_Wu@TV^_1q
r Z/TV]ab/W4^Yd UU_Q/W6]abCX%]Z/W4WiVUYwxQS^_]ab/W4^W4RCn TV^X%] TVZd U]ab/W6U_Q/f/] n W~iSd U_] ZCcfi] TVZ>fgW4]u@W4W4Z u@WX%1
'
X%ZCiU_]a^_TVZ/\S0Lrh;subCd2cObuX%UCfS^_TVQ/\Vb]D]aT0TVQ/^-X%]a]aW4Z`] TVZhfeffX%ZCw^_WiX%W4\VW4^q b/W"]aW4^kffd Z/TSn TV\Ve
U~iQ/Wj]a
XVkW4U~TVfCd Z/UsLub/TlTVZCn e}tVW4^e}^_WcfiW4Z] n ei/d UcfiT tVW4^Wim]ab/W>i/d U_] ZCcfi] TVZ*qfi] ]aQS^_Z/UhTVQ/]
]abCX%]]ab/W6ZSTV] TVZ>TSH
w rhu9W6Q/UWd Z}]abCd URCX%RgW4^5 u9WX%1
rh;d UUn2d \Vb`] n ei/d AW4^_W4Z]5wx^TSk]ab/W
U_]a^_TVZS\S/tVW4^_Ud TVZlTS2
w rh|Q/U_Wi5fe}{d2n2n7W4]hXVn2q*aVVVVaq b/W4et1d W4u]ab/
W Lr6vX%U_U_QCkRS] TVZ[ QCd ]aW
n2d ]aW4^XVn2n2n eX%UX%Z|X%U_U_QCkR/] TVZ|X%fgTVQ/]5X}%cfiTSX%^_U_W4Z*d Z/\~R/^TgcfiW4U_UVqhrhiVQ/U]aWij]aTTVQ/^{Z/TV] X%] TVZ*sD]ab/W4e
u^d ]aWja{d2n2nW4]8XVn2q2sCVVVVsRCX%\VWjVVVah%Ld ^_U_] n e]abSW^X%ZCiTSkvtX%^d2X%fCn
W W TSw*d Z`]aW4^_W4U] U^_WXVn2d 4WiS
U_WcfiTVZCiSn es1X5/,a7)+5 fi SLR/^TgcfiW4U_UQSU_QCXVn2n elX%UU_Tgc4d2X%]aWi6u8d ]ab}wxWX%]aQS^_W4U&TSwCkWX%U_Q/^_WkW4Z]
TV^TVf/U_W4^_tX%] TVZCXVn9^_W4U_]a^d2cfi] TVZ/Us&^X%]ab/W4^]abCX%Zz]ab/WUc4d W4Z`] CcYR/bSW4Z/TSkW4Z/TVZQ/ZCiW4^YU_]aQCied ]aUWn2waas
\Sd tVW4Z~]abSWt)XVn Q/.
W ] X%VW4Z~f
e W}s^_W4RCn2XVcfiW4U0]ab*d U&t)XVn Q/Wf`emXU_W4]UQCcb]abCX%
] >8q2 bQ/Usgd Z
]ab/Wd ^Dt1d W4u8s ]ab/W;i/d U]a^d f/Q/] TVZ{TV
Z U9cfiTVZSU_]a^_QCcfi]aWi8wx^_TSkXi/d U_]a^d fSQ/] TVZTVq
Z X%ZCiXacfiTVZ*cfiW4R/]aQCXVn2n e
Q/Z/^Wn2X%]aWi/DUW4]@TSwEcfiTVZCi/d ] TVZCXVngi/d U_]a^d f/Q/] TVZ/.
U 1^ W hasTVZ/WhwTV^-WXVc
b e
vq bCd U
d2kRCn2d W4UY]abCX%
] D^ W ffd UmXu@Wn2n piW4/Z/WimZQCk0fgW4^6W4tVW4Zd2#
w 1^ W h Vq
{d2n2n2sDt)X%Z|iVW4
^ ~EXVX%ZCs"X%ZCi>TVf*d Z/U5aVVVV]abSW4ZL K-S6]abS
W Lr6 cfiTVZCi/d ] TVZzX%Um%wxTV^jXVn2nB

1^
W
Z Vq bCd Umd UjQ/U_]YRCX%^_]mai/{TSw b/W4TV^_Wk Vq2Vs
UcfiTVZ/U_] X%Z]d
ubCd2cObZ/T uvbCX%U]aTbSTSn2i5W4tVW4Zd2!
w 1^ h Vq b`QSUs7]ab/WhU_W4]TSw"i/d U_]a^d f/Q/] TVZ/UUX%] Uwe1d Z/\
U_]a^_TVZS
\ Lr6 U6XUQ/f/U_W4];TSwg]ab/W U_W4];UX%] Uwxe1d Z/\u@WX%
Lr60qCTVfCd Z/UffX%ZC
X%W4\VW4^Ub/T u]abCX%]]ab/W
ZCc4n Q/Ud TVZc4X%ZfgWU_]a^d2cfi]lX%ZCim]ab*X%]ff]abCd Ulc4X%Z|bCXatVWU_Q/f/U] X%Z`] d2XVncfiTVZSU_WQSW4ZCcfiW4Uq b/W4^_WwTV^_Ws"TVQ/^
cb*X%^XVcfi]aW4^d X%] TVZ/UjTS0
w rhd ZPVWcfi] TVZX%R/RCn e>TVZ*n eo]aTu@WX%
rh;s@X%Z*iwxQ/^]ab/W4^h^_W4U_WX%^cObd U
Z/W4WiWi6]aTffU_W4W{]ab/WW4`]aW4Z]]aT5u0bCd2cb]abSW4emXVn U_TmX%R/RCn ej]aTffU_]a^_TVZS
\ Lrh;q
-QS^Yi/d UcfiQ/UUd TVZ}b/W4^WffbCX%UwxT1cfiQ/U_Wi>cfiTSkRCn W4]aWn e>TVZ]ab/WffR/^_TVf*X%fCd2n2d U_] d2c5c4X%UWqyBTu9W4tVW4^s*]abSW4U_W
Q/W4U_] TVZ/UhXVn U_TlkffX%VW8U_W4Z/U_WjwxTV^;TV]ab/W4^0^_W4RS^_W4U_W4Z] X%] TVZ/U;TSw7Q/Z*cfiW4^_] XVd Z`]eq9Z]aW4^_W4U_] Z/\Sn es*^d WiSkffX%Z
X%ZCily;XVn RgW4^_ZaVVVV@U_bST uv]abCX%]']abCX%]r{ffpU]e1n WfffgWn2d Wwg^_W4t1d Ud TVZrhn2cOb/TVQ/^_C
^ TV Z*s*Q
X%^iW4ZCwTV^_Us*
lX%gd Z/UTVZCsLVVVVc4X%ZlfgW8^_W4R/^_W4U_W4Z]aWimd Zm]aW4^kUTSwBcfiTVZCi/d ] TVZ*d Z/\YQSUd Z/\XY QCXVn2d ] X%] tVW6^_W4R/^W4U_W4Z/p
] X%] TVZTSwQ/ZCcfiW4^] XVd Z`]elc4XVn2n WiX'7
,o +
p ,)fi4]aTiT]ab*d Us`]ab/W0RCn2X%QSUd fCd2n2d ]emkWX%U_Q/^W k0Q/U_]
UX%] Uwe[]ab/WoX%ZCXVn TV\VQSW~TSw b/W4TV^_Wk Vq2VaXVas-U_T>]abCX%]5TVf/U_W4^_tX%] TVZ/Uc4X%^_^_eZ/T|kTV^_Wd ZCwxTV^kffX%] TVZ
]abCX%Z[]ab/WwXVcfi]h]abCX%] ]ab/W4ezX%^_WY]a^Q/W
q wB
Lr6pn2d VW>cfiTVZCi/d ] TVZd Uh\Sd tVW4Z]aTm\VQCX%^X%Z]aW4W]abCX%]h]abCd U
cfiTVZCi/d ] TVZbSTSn2iU5wxTV^RCn2X%QSUd fCd2n2d ]ekWX%U_QS^_W4U]ab/TVQ/\VbCqfi]u9TVQ*n2i5fgWd Z]aW4^_W4U_] Z/\]aTZ/T u d2wD]ab/W4^_W
X%^_W5X%ZCXVn TV\VQSW4U0]a
Lr6wTV^&TV]ab/W4^^_W4R/^W4U_W4Z`] X%] TVZSUTSw/QSZCcfiW4^_] XVd Z]es)U_Q*cb}X%U;` +
p ,)fi
u r9Q/fgTSd U8
D^XViWsDVVVVBTVm
^
! )/ / aPVbCXVwxW4^s1VVVVaq

`77

fi " |qxq}t~"}s1|



J;\/K



fi

r R/^Wn2d2kffd ZCX%^_etVW4^_Ud TVZlTSwA]abCd U;RCX%RgW4^ X%R/RgWX%^_Uhd Z 1/,/
x0, K-
a/
/ff 9
,/G.9/~*8 fi S4s7VVVVqVXVkW4U;TVfCd ZSU;X%ZCi6lX%ZCwx^Wi=X%W4\VW4^cfiTSkffkW4Z]aWi
TVZ}X%ZWX%^n2d W4^ i^XVw]@TSwS]abCd UR*X%RAW4^qEWu9TVQCn2i tVW4^emk0QCcb}n2d VW]aT6]abCX%Z/j]ab/Wk wxTV^&]abSWd ^8d Z/Ud \Vb]ap
wQCn`^_WkffX%^_Us`ubCd2cOb}n Wi ]aT6U_W4tVW4^XVn)Ud \VZ*d Cc4X%Z`]d2kR/^_T tVWkW4Z`]aUhd Zj]abSWRCX%RgW4^qCWYXVn U_T6]abCX%Z/Y]ab/W
^_WwW4^_W4W4UhTSw]ab/<
W "r;{UQ/fCkffd U_Ud TVZzX%ZCi~]ab/
W \xQuyUQ/fCkffd U_Ud TVZzwTV^h]ab/Wd ^hRAW4^cfiW4R/] tVW>cfiTSkffkW4Z]aUq
W wBW4]abSW4^n2X%ZCiU{-^_\SX%Z*d X%] TVZ
b/W;/^_U_]X%Q/]ab/TV^-uX%U@UQ/R/RgTV^_]aWifeX0]a^XatVWn\V^X%Z`]{XauX%^iWi8feff]ab/
wTV^P/c4d W4Z] CcW4UWX%^cbu w"{aq b/WYU_WcfiTVZCi[X%Q/]ab/TV^ uX%UhU_QSR/RgTV^_]aWi}d ZRCX%^_]8f`e w'P/[QSZCiW4^
\V^X%Z]aUB_4PpVVVV%S{X%ZCm
LpVVVVVVVsfej
w;mQSZCiW4^*\V^X%Z`]aQ
U w;VVV%VpV%p%pV%SF
w;VVV%VpV%p
%pVVVVs1X%ZC=
w'VVV%VpV%p%p%SVsSf`eh]ab/
W r"
rffQCn ] d2iSd Uc4d RCn2d ZCX%^_[
e "ZCd tVW4^_Ud ]e~W4UWX%^cbjZ*d ] d2X%] tVW

;BBR/^_TV\V^XVk XVi/kffd ZCd U_]aW4^Wi6fej]abSWYZ
w'Q/Z*iW4^&\V^X%Z]am
U w'VVV%VpV%pVVV~X%ZC
w;VVV%VpV%p
%pVVVVsLX%ZCihfe>Xff-Q/\V\VW4Z/bSWd2k Wn2n TuU_bCd R*sCXffVQCn f/^d \Vb]8Wn2n TuU_bCd RCs*X%ZCiX{\V^X%Z]8wx^TSk w"{q
P/X%f/f*X%] d2c4XVn*U_Q/R/RgTV^_]Yw^_TSk LjX%ZCi~]ab/W>y"W4f/^_W4
u BZ*d tVW4^_Ud ]eTS
w W4^_Q/UXVn Wk U~XVn U_Tm\V^X%]aWwQCn2n e
XVcZ/Tu8n Wi\VWi/q

eK[


K"NW


G J*J+

fiZ]abCd UU_Wcfi] TVZCsDu9W6R/^T t1d2iWh]ab/W6R/^_TTSwxUTSw@XVn2nA]ab/W6^_W4U_Q*n ]aU5d Z]ab/W6RCX%RgW4^q5VTV^YcfiTVZ`tVW4Z*d W4ZCcfiWs1u@W
^_W4U_] X%]aW8]ab/W^_W4U_Q*n ]aUb/W4^_Wq

L87|+1^ /55**
/ fi88 )n + /
zO%}9q1^ # ;
1 ^W
D^ W W 8! ff
>m
zu}*'ff n /\W
9/_` /*G'
ff n S\ n #W
>mff&X >
zOu}D ^ W 1 ^ W 8R 6?
|)~V1 ^W
!
h

zO%}D ^ W h
1 ^ W # m!h
6 ,)
^W h

!
/qD ^ W 2



\b"a\



e7

PQ/RSRATVUW6aXVBb/TSn2iUq"WuX%Z]0]aT5Ub/T u]abCX%]VW X%ZCi X%^Wjd ZCiW4RgW4ZCiVW4Z`]
w TV^XVn2n\v>q&Ld Wv>q*4wCD^ W ]ab/W4Z]ab/W'W4tVW4Z]aU8X%^_W']a^d tgd2XVn2n emd ZCiW4RgW4ZCiW4Z] q-PT
U_Q/RSRATVUW']ab*X%]1^ 2Vq0n WX%^n e
[<aR

1^ W


k W>8 D^ W



Ud ZCcfiWTVf/U_W4^_t1d Z/\~d2kRCn2d W4U]abCX%]0]ab/W{]a^_Q/Wu@TV^n2id Uhd Z8'aqDCeYRCX%^]8aXVas

1^ W



1^

bQ/Us

}ui

^W k W > D^W W>8a
1
U_b/Tu8d Z/\6]abCX%]W U ZCiW4RgW4ZCiVW4Z`]-TSw s\Sd tVW4ZW8q
`77

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

w"W4`]'U_Q/RSRATVUW]abCX%] fCb/TSn2iUsCX%ZCi$d U'U_QCcOb~]abCX%]m1^ W ;Vq^_TSk$RCX%^_]hfC
]'d Ud2kffkWiSd2X%]aW0]abCX%]#1^ W k W>8 1^ W >aq&5TV^_W4T tVW4^s
Ud ZCcfiW$o8sCc4n WX%^n e1^ W k W D^ W aq.LX%^_]6ac4
Z/TuwxTSn2n TuUq
n WX%^n eaiSBwxTSn2n TuU'd2k5kWi/d2X%]aWn e~w^_TSk ac4aq bQ/Us/d ]"^_WkffXVd Z/UB]aTU_b/T u]abCX%];aXV"wTSn2n u0U'w^_TSk
ai/aqCWiT']abCd UfeU_b/Tu8d Z/\']abCX%]0ai/Ld2kRCn2d W4U0ac4X%Z*i0]ab*X%]&ac4d2kRCn2d W4U0aXVaq-PTU_Q/R/RgTVU_W9]abCX%]0ai/
b/TSn2iUq;PQSR/RgTVU_W]abCX%
] D^
W wTV^ XVn2?
n oUQCcb]ab*X%
] 1^ W K
Vq
^_TSk ]ab/W5iW4/Z*d ] TVZTSwLcfiTVZCi/d ] TVZCXVn/R/^_TVfCX%fCd2n2d ]e
D^
W>8
F b) rb /_- 1 ^ k W a21^W >8

a21^W
F b) rb /_- 1 ^ W ?1^W h

H




^


W


h



2

1


^


W


>

8




F b) rb /_





>



bQ/Us1ac4wTSn2n u0U w^_TSkai/aq
Ld ZCXVn2n esg]aTU_W4W ]abCX%]ffaXVwTSn2n uU5w^_TSk ac4asAU_Q/R/RgTVU_W8]abCX%]6ac4@b/TSn2iUq0wU UU_QCcObm]abCX%]
1^ W h VsA]ab/W4ZaXVd Uffd2kffkWiSd2X%]aWsgU_TU_Q/R/RgTVU_W8]abCX%]=1^W 0 Vq Sb W4ZCs/QSUd Z/\
ac4X%ZCi6]ab/WYwXVcfi]]abCX%V
] W
>8s)u@W{bCXatVW]abCX%]








1^
1^
1^W
1^
1^
1^W
1^

W

W ?D^W ha21^
>?D^ ha21^W
k W >?D^W ha21^W >?D^
?D ^W ha21^W >?D^ 8
a2D ^W}
W >8a

8

X%U iVW4Ud ^_Wi/q

*lxQy$/+K h*,, )/1^q 5/l/+
S{G'`,ff( /-4 6
[<aR Ld ^_U_]{U_QSR/RgTVU_W6]abCX%]{]ab/W5U_W4]aUjd Z X%^_W5RCXVd ^_u8d U_Wli/d U_TSd Z`] q b/W4Z|wTV^{WXVcbR/^_TVf*X%fCd2n2d ]e
i/d U_]a^d f/Q/] TVZ1^TVZel
sAWXVcOb|
sX%ZCiYWXVcOb>u9TV^n2iq
|vU_QCcObl]ab*X%]D ^ V
Vsd ]
k0Q/U_]&fgW]ab/WYc4X%U_W]abCX%]1
^ W Vq bQ/Us`RCX%^_]8aiS*TSw b/W4TV^_WkVq2ffX%RSRCn2d W4Uq
TV^0]ab/W5cfiTVZ`tVW4^U_WsU_QSR/RgTVU_W;]ab*X%]&]ab/WU_W4]aU8d Z
X%^_WZ/TV]-RCXVd ^_u8d UW5i/d U_TSd Z`] q b/W4Z]ab/W4^_WW41d U_]
U_W4]aUh"4
U_QCcOb~]abCX%]0fATV]ab X%ZCi k X%^_W{Z/TVZSWkR/]eq0~ W4]V - > k qV
n WX%^n e
]ab/W4^_W&W41d U_]aU;Xi/d U]a^d f/Q/] TVZD
^TVZ U_QCcObff]abCX%]1 ^ 2
Vs1 ^ 2
Vs]1 ^W
- W V^1 ^ - W Q
VqCQ/]]abSW4Z 1 ^ - z#
Vq
[<a"ZW " ^ " l

bQ/U

:eS

1^

-

W

0

1^

-



>a

X%ZCih]abSWWLrhcfiTVZCi/d ] TVZ[RCX%^_]8aXVLTSw bSW4TV^_WkVq2V0d U0tgd TSn2X%]aWi/q
jj ;GY,)/ |n 6| n

n :e
L E
~xQyB %,/ph%H# /

Dh\

`7_

S%+@/m
7,

fi " |qxq}t~"}s1|

zO%} E ;D^qffSj,, )}|n [

/~
17 ,ffp ,Y / j + /fi|p
7&,%`SS}}|pG'1^W H'' JILK-/ln ,
@ & ^i^i^i jS , / 1^ FW h WD^W zF#0S
K Yx0y a7 _A @ B @ B
,ff_ 9 V^i^i^i 6l;.9D^ff,Mzu} E 7 ,6p ,ff//ffGff4 , @Ghfffi{66H7
2O QP R 6ff9G
,, )/{|n #
, `%@,%`SSYU7 V ffo

QP R / XgXg>Y''K Z'$,%`Shjfiz[7
@ ) 17 A_@ B @ B 9 ]? {XVn2n`XE aO QP R&S "
9ff fij7 n , \A^
@ ] ~
ff Y>%o1^l|n Z e1^, E
X zffophS-GD^m
XE})z^}1^,MK xQy/zu}D^ FW F 6S
q1^



>

!

TV^RCX%^]haXVasgUQ/R/RgTVU_W]ab*X%]D^ffd U6Xi/d U]a^d f/Q/] TVZmTVZ]abCX%]UX%] U_SW4UWLrh;q~/W4]=5fgW
]ab/W'Z`Q*kfgW4^@TSw)^_T u0Ud Zq7 sgX%ZCijn W4].C* 1^W q'*fiasgwxTV^- V^i^i^i ^s`ub/W4^W:'1*Bd U-]ab/W6X%]aTSk
cfiTV^_^_W4URATVZ*i/d Z/\h]aTff]ab/W{]ab~^_TuTSF
w 7 q wBTV]aW8]abCX%
] C*5wxTV^ V^ i^i^i ^
q n WX%^n es
[<aR







1^ F ;W q
'*


aV

fi]&WX%Ud2n e}wxTSn2n TuU w^_TSkv]abSWLr6cfiTVZCi/d ] TVZ]abCX%]

D^ F W q
'1*fi 1^ F
HW

>F

wTV^8XVn2n'1*F sUTaVd U0W QCd tXVn W4Z`]0]aT






1^ ;W}F

aV

Vd2kRCn2d W4U]ab*X%]
jwTV^ V^i^i^i ^q=~/W4]q9@ *fAW ]ab/Wh^_Tu Z7 cfiTV^_^W4U_RgTVZCi/d Z/\
]aTq'*fiqhP/d ZCcfiW9@ *bCX%UffXjjX%U5d ]aU ]abcfiTSkRgTVZ/W4Z]6d2w'1*H 6X%ZCimXY8TV]ab/W4^_u U_WsLd ]6wTSn2n u0U]abCX%]
9@ *gA @ B ffX%ZCi bSW4ZCcfiU
W 7 2A @ B @ B q
TV^hRCX%^]YfCas-n W4
] fgWff]ab/W5ZQCk0fgW4^TSw*^_TuUjd >
Z 7 s@n W4c
] 9 @ & ^ i^i^i j
98@ jfgWff]ab/WY^_T u0U8TSw7 s-X%ZCi
n W4N
] ' & ^ i^i^i ';fAW~]ab/WocfiTV^_^_W4URATVZ*i/d Z/\X%]aTSkUqLd Xg OQP REs;X%ZCi>U_W4
] C* Xg}Y '*ffwTV^
V^ i^i^i ^ V
q ~/W4
] 1^fgW]ab/W{QSZCd2QSW6iSd U_]a^d f/Q/] TVZ~TV
Z UQCcb]ab*X%]

D^Wq'1*

D^W
q''

D^ FW <
'1*

C*4wTV^0

V^ i^i^i ^ /
ffd2w)'|x. /' & ^i^i^i ';V
w '*L>s
d2)

TV]ab/W4^_u U_Wq

w"TV]aW]abCX%]1^md Uld ZCiW4Wi|XR/^TVfCX%fCd2n2d ]eiSd U_]a^d f/Q/] TVZ|TVZ ms9Ud ZCcfiW 1^W
''
@
1^ q' * !
6wxTV^; V^i^i^ ^ sEX%ZCi/sUd ZCcfiWu9WYX%^WYX%U_U_QCk5d Z/\6]abCX%]7 2A_@ B B
j
^ W q' * 9 @ * _ @ B V
1
r
&

`77

aV

Vs

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

q fi]{^_Wk5XVd Z/U{]aTU_bST u ]abCX%]q1 ^{UX%] U/W4UqLr6$X%Z*ij]abCX%]
V^i^i^i ^qn WX%^n eD^8 Xg>
D^ HW oFaq0{d tVW4ZlhoVV^i^i^i 6l"Vs/U_Q/R/RgTVU_W]abCX%]]abSW4^_W{W41d U_] X%]aTSkU3'1*' *
cfiTV^_^_W4URATVZ*i/d Z/\h]aTff^_TuUa9@ *9X%Z*i9 @ *S TSwF7 U_QCcOb]abCX%]'1*' *S >F q b/W4Z
^ \ W <'1* 1 ^ F_ W q' * 7i

fi]"Z/TuwTSn2n uU@f`e b/W4TV^_Wk$Vq2Vac4E]abCX%]#1
^"UX%] U_/W4U@]ab/WL r6cfiTVZCi/d ] TVZwxTV^ & ^i^i^i 47j q-5TV^_W4p
tVW4^s b/W4TV^WkVq2Vai/asEd ] kQ/U]&fAW]abSW5c4X%U_W]abCX%]1
^ ;W
}F q
wTV^{

bSWffR/^_TTSwCTSw b/W4TV^_Wk Sq fSQCd2n2iU{TVZ~Wkffk5X6Sq2lX%ZCi]ab/WmwxTSn2n Tu8d Z/\lR/^_TVRgTVUd ] TVZCsEubCd2cOb
U_b/TuU-]abCX%]-]ab/WffcfiTVZCi/d ] TVZTSw)R*X%^_]{fCDTSw b/W4TV^_WkSq UXVcfi]aQCXVn2n ejU_]a^_TVZ/\VW4^&]ab*X%Zj]abSWhcfiTVZCiSd ] TVZ
TSw7RCX%^_]aXVaqB]d U]abSW4^_WwxTV^W{Z/TV]U_QS^_R/^d Ud ZS\ ]ab*X%]8d ]n WXViU0]aTlXU_]a^_TVZ/\VW4^hcfiTVZCc4n Q/Ud TVZ*q
,5,_
(
}G~fiY67$ff+/%+la`/ SZ )
eC 9 fi~7
'vu/
l_` / /' = ffg>p 3' %`//mmfiv^} /mA
/
VV^i^i^i6l"9 6' F~M@j7,n Ny @ {>u/5,|pm /o6ffY8
},) ]
h>VV^ i^i^i6 l"/ ~

[<a"ZW " ^ " g
l

[<aR PQ/R/RgTVU_Wh]abCX%]{]abSW4^_WffW41d U_]aUjXhUQ/f/U_W4]}TSwE^_T u0UTSw17]abCX%]Yd Ujn2d Z/WX%^n eiW4RgW4ZCiW4Z]f/Q/]
Z/TV]ffXvffZ/Wn eiW4RgW4ZCiW4Z] q ]ab/TVQ/]5n TVU_UTSw1\VW4Z/W4^XVn2d ]es9n W4]8o @ & ^i^i^izo @ 5fgW8]ab/Wh^_TuUYd Z}q b/W4^_W
*
r p * X%Z*i *
r p * @ * VqW6/^U_];Ub/T u ]abCX%]ffd ZwXVcfi]
W41d U_
] p & ^ i^i^i p\ffU_QCcbm]abCX%
]


&
&
W4tVW4^_e^T G
u o@ dc
Z }d UhX%ZX vffZSWjcfiTSk0fCd ZCX%] TVZTSwg]ab/W8TV]ab/W4^^_T u0UqLd U_TSkW8YVV^ i^i^i^ V
q ~/W4]
*
r * ffX%ZCi
p\ *
r & p&* *
r4 p&*@X%ZCin W4] * p&*@wTV^#
q b/W4Z

&







p&*to @ *
p&*to @ @ ffi
w* @ *
*


r


*
r
&
&
&


TV^ V^i^i^i^sLn W4] * *fi/S
q b/W4Z
*
r & * jX%ZCi *
r & * @ * @ qwBTu d2we'1*K F ~
wTV^BU_TSkW V^i^i^i ^ffX%Z*iU_TSkWj V^i^i^i 6l"s]ab/W4Zo @ * bCX%U;X{{X%U'd ]aU"b]ablcfiTSkRgTVZ/W4Z`] qCr6n U_TSso @ *
U8X%Z}XvffZSW5cfiTSkfCd Z*X%] TVZ~TSwS]ab/W^_T u0UTSw|
} u8d ]ab~Z/ThZ/W4\SX%] tVWjcfiTSkRgTVZ/W4Z]aUs)UTo @ *@d U]ab/WffiW4Ud ^_Wi
*
r

tVWcfi]aTV^q



{YG {%)/0
n '|
n
\b"a\
e9L
E
/+ e7h~xQyB ,/=ph%H#/q



V & ^i^i^i 4FjA5/5,+0

zO%}c|S `~' fi7, l4 ?}Gj6^7vSln , ^y @ & ^i^i^i jA
&vu/6p S>G6ff{G6} )j ] ff}VV^i^i^i6l"
/ ~ j|h
p 8jffVV^i^i^i 6"l * fiff/%o)1^l
,MK-Yx0v
,H1 ^ W ~K
/q1 ^ ^''K
V , >
|
p '
,%`SSYa}
zu}9Y
j7 ,84 , 1} GY
jfihGfi
7;
/,,+_` /0 )&/0vu/ +
a`/ S1
ff fi0B/%o )1 ^' 1MK& 'xQ
)E1 ^
''!
{
fi|
p ',fi O`//jjfiz[}
Oz u}@
n } } S/G#
l
/,+~/_` / /;663
7v/l~% )qXE

,1X Y''H
Q '%`//a}9 fi5{)/ )
,, )mX ) j eD
^%o ) /xQy
^W '' XE>

Y'' 8|
p ',,fi O`//fia}B 1 ^
8 X a,
`7

fi " |qxq}t~"}s1|

VTV^RCX%^_]aXVasU_Q/RSRATVUW>]abCX%]} cfiTVZ/Ud U]aUlTSw^o @ & ^i^i^i jo @ s5cfiTV^^_W4U_RgTVZCi/d Z/\|]aTvX%]aTSkU

' & ^ i^i^i'HqCelX%UU_QCkR/] TVZ*s]abSW4^_WW4gd U]cfiT`W^vc4d W4Z]aU6p & ^i^i^i p_U_Q*cb]abCX%] *sr & p&* Vs1X%ZCiX
*sr p&*t@ B* U_QCcbm]abCX%]W4tVW4^_ecfiTSkRgTVZ/W4Z];TSw3@ UZ/TVZ/Z/W4\SX%] tVWqYPQ/R/RgTVU_WsSfeuX_eTSw
tVWcfi]aTV^y @


&
cfiTVZ]a^XVi/d2cfi] TVZCs]abCX%].1^@UX%] U_SW4U.rhzX%ZCi]abCX%].;* 1^Wq'1* !8wTV^9-VV^i^i^i^Vq0Ee
[<aR

~Wk5kffX'Sq2VaXVas/u@W{bCX_tVW

@ 2 @


*
r
&

pT*wo @ *





2A @
&p *fio @ *YA @
&p *
*
r &
s* r &

V

aV

ub/W4^WA @ UffiW4/Z/WilX%Uffd Z~WkffkffXSq2VqhTV^ V^i^i^i6l"sCd2wK1^W [ {]abSW4ZD^W
F k W 1^ F. X%Z*i1^W . VsEU_T = Vq~CeX%U_U_QCkR/] TVZCs
XVn2n]abSWcfiTSkRgTVZ/W4Z]aUYTSw[y @ X%ZCiHA @ X%^_WZ/TVZ/Z/W4\SX%] tVWq b/W4^_WwTV^_Wsd2w9]ab/W4^_W~W41d U_]aUjb UQCcb|]abCX%]
1^ F ~2 X%ZCi ~0Vs]abSW4Zmy @ bA @ Vq bCd UcfiTVZ`]a^XViSd2cfi]aUaVas1X%ZCi{R*X%^_]aXV@d U-R/^_TtVWi/q
TV^RCX%^_]6fCasAU_Q/R/RgTVU_W8]abCX%]]ab/W4^_WhW41d U_]aU5X8U_Q/f/U_W4fi
] }TSw1^_TuUTS
w 7]abCX%]ffd U5n2d Z/WX%^n eiW4RgW4Z/p
iW4Z];fSQ/];ZSTV]hX vffZSWn eoiW4RgW4ZCiW4Z] q8PVQ/R/RgTVU_WsfeuXaeTSw"cfiTVZ]a^XVi/d2cfi] TVZCsg]ab*X%m
] D^'UX%] U_/W4=
U Lr6
X%ZCi~]abCX%
] 1^ W
';
mwTV^XVn2n-X%]aTSk\
U 'cfiTV^^_W4U_RgTVZCi/d Z/\]aTXff^_Tud g
Z }
q "d2cO|X%ZX%]aTSk
'ffcfiTV^_^_W4URATVZ*i/d Z/\~]aTlU_QCcObzX5^_Tu8q}Ce D^_TVRgTVUd ] TVZor6q2mX%ZCi b/W4TV^_Wk Sq SaXVasu@WYb*XatVWj]abCX%]
1^ F ~ ffwTV^8XVn2n) U_QCcOb]abCX%
] ' >
~qCQ/]]abSW4
Z 1^ W q
' VsDX%ZCihu@W{bCX_tVW
X%^_^d tVWiX%]8X6cfiTVZ]a^XVi/d2cfi] TVZCq
TV^RCX%^_]>ac4as'U_Q/RSRATVUW]abCX%
] } cfiTVZSUd U_]aUTSw0]ab/W^_TuU @ & ^ i^i^i j
o@ j
q ~W4
] 7 fgW]ab/e
W
U_Q/f*kffX%]a^d lTS
w 7cfiTVZ/Ud U] Z/\~TSwC]ab/Wff^T uU8TS1
w }qlP/d ZCcfiW5]ab/W4U_W5^T uUX%^_Wn2d Z/WX%^n ed Z*iW4RgW4ZCiW4Z] s"X
U_] X%ZCiSX%^iY^_W4U_QCn ]TSw9n2d Z/WX%^YXVn \VW4f/^X UXaeU{]abCX%
] 7 UYd Z`tVW4^] fCn W
q ~/W4W
] 1^fgWXi/d U_]a^d f/QS] TVZTVe
Z
@
@
&
UX%] Uwe1d Z/
\ Lrh;qgE
e ~WkffkffX;Sq2VaXVa
7 @ B q bQ/U1
@ 7 q&TV^& V^ i^i^i 6 llu9Wffk0Q/U_]
bCX_tVW - 2
D^ W |
asgubSW4^_W D^
aq{d tVW4
Z D,^ >Y ';wTV^{WXVcObX%]aTS
k ''s
u@WYc4X%Zc4n WX%^n eU_TSn tVWwxTV^0]ab/W Uq
\b"a\
eL @ n 6 G%)/0|n ;ffF G{,+'/ff5 6| n S,
:
1^o% ) 6MK& xQy fi}S>G`fi|ph ,m
{:f)z )] h? m!D^aW} h4 / V8
7fi|
`{:fF8fi )%/ 48,
]abCX%]}d2w1
^}d U}XlR/^_TVf*X%fCd2n2d ]eTVZ U_QCcOb]abCX%] shwxTV^UTSkWU_W4]a] Z/\TSw
[<aR Ld ^_U_]u@WU_b/Tu
]ab/WRCX%^XVkW4]aW4^_U6TSw1{:f)z sVD
^ a[
W
/ 4 / Vhd U6]ab/WjR/^_TVf*X%fCd2n2d ]eo]abCX%]
{:f)zFB^_W4]aQ/^_Z/U{h
4as)]ab/W4ZD ^@UX%] U_/W4UL r60qgCe bSW4TV^_WkVq2VsDd ]-U_Q]
v cfiW4U-]aT6U_b/Tuz]abCX%]
wTV^WXVcbffU_W4]0
X%ZCi'u@TV^n2iU2 & 6 + >U_QCcb6]ab*X%]1 ^ & 2
X%ZCim1 ^ + 2
Vsu9W'bCXatVW1
^ W & 1 ^ W + aq@PVTU_Q/RSRATVUW]abCX%]K & 6 + }s
^ W & Vs{X%ZCiD ^ W + Vq/~ W4]
1
` X& aa _ aq
fiZ`]aQCd ] tVWn es. Uff]ab/W~R/^_TVfCX%fCd2n2d ]e[]abCX%]ff]ab/WoXVn \VTV^d ]abCk]aW4^kffd ZCX%]aW4Uld2k5kWi/d2X%]aWn eX%]ffU_]aW4RvVq2
u8d ]abm 49cfiTVZCi/d ] TVZCXVnTVZ5U_TSkWVv
}fgWd Z/\5cOb/TVU_W4ZlX%]9U_]aW4RlVq2Vq["w TV] d2cfiW6wxTV^wxQ/]aQS^_W-^_WwxW4^W4ZCcfiW
]abCX%] sDwTV^8XVn2n]





)
_


X
) P / P F _

aa

_







aVV

ub/W4^Wd U&iW4/Z/Wi-fe5SaqCr;U*W4RCn2XVd Z/Wi Z]ab/WkffXVd Z8]aW4] swTV^EfgTV]ab V4Vsjd UE]abSW"R/^_TVf*X%fCd2n2d ]e
]abCX%]8]ab/WXVn \VTV^d ]abCk iTW4UZSTV]{]aW4^kffd ZCX%]aWmX%]U_]aW4RVq26\Sd tVW4Z]abCX%](*d UjcbSTVU_W4Zzd ZoU_]aW4RVq2Vqjfi]

`_Y

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

WX%Ud2n e}wTSn2n uU;]abCX%]]ab/WR/^_TVfCX%fCd2n2d ]eY]ab*X%]8(*4d UTVQS]aR/Q/]X%]0U]aW4RoVq2ffd U

X * u



+
*
^^ X u Va ui
bQ/UsUD^ W (* k 8 Xg}(*fiu Va
aq?"Ud Z/\~aVVas/u9WbCXatVW8]abCX%]
1^ W (
*
1^W (* k EX >( *
EX >(*ui
_
_




Ld ZCXVn2n esSu9W8bCXatVW8]abCX%]D^ W (*fi Va asEwTV^0 V4Vq bQ/UsCD^UX%] U_SW4U
]ab/WrhcfiTVZCi/d ] TVZCq
TV^]abSWcfiTVZtVW4^_U_WsgUQ/R/RgTVU_W{]abCX%]=D^;UX%] U/W4U]ab/WLrh cfiTVZ*i/d ] TVZCqm~W4] V & ^i^i^i 4 j Vq
Wcb/TTVU_W5]abSWffRCX%^XVkW4]aW4^UwTVfi
^ {:f)z X%UwTSn2n uUqPVW4]UXg} D^W hX%ZCion W4]
*aq; ]ab/TVQ/]ffn TVU_UTSwg\VW4Z/W4^XVn2d ]esgu@WX%U_U_QCkW8]abCX%] *; jTV]abSW4^_u8d U_Ws7] X%VW
* 1^
]aTlcfiTVZSUd U_]0TSwA]abSTVU_WU_W4]aU']abCX%]hX%^_WTVf/UW4^_tVWi6u8d ]abRgTVUd ] tVW8RS^_TVfCX%fCd2n2d ]esCX%ZCiiTff]ab/W8R/^_TTSw/QSUd Z/\
aq
*fiVq~PW4U
] X; *fi D^ +
*fi *X%ZCq
; ) Vq
TV^ V^ i^i^i 6 l"s@n W4] * V
* +

b Q/Us*]ab/WjU_W4] (*8d UXVn uXaeU6^_W4Wcfi]aWi/sLQ/Z*n W4U_U (* F q2PSd ZCcfiWD^W vF ] 1^
j

feX%U_U_Q*kR/] TVZCsBd ]5k0Q/U_]fgWh]ab/W~c4X%U_Wff]ab*X%] kffd Z r & 1^ QVqqw"T u$U_W4]
w ) 2
D^ W }+
*fiaq
W/^U_]"U_bST uz]abCX%] su8d ]abj]ab/W4U_WRCX%^XVkW4]aW4^&U_W4]a] Z/\VUsu@Whc4X%Z}cOb/TTVU_Wfi&U_QCcOb]abCX%]{cfiTVZ/U_]a^XVd Z]

S&d U-UX%] U_/WiS[
q ~W4{
]
b v
U_QCcObj]ab*X%f
] XE> h!

P ) P F X; t_ "q&TV^WXVcO
Vs)u@W{bCX_tVW









X Yw_
j*
r P F P) `P X; *fiY _ )


&

* F X *
* ) ; X * L )

b/W0n2X%U]DWQ*XVn2d ]e5wTSn2n uUCfAWc4X%QSU_W V * * Vq b`QSUswTV^9XL/Wi-,s
F P ) X * _
UWd ]ab/W4^:X *fiY /L d2w >+fi* sTV^6X;&
*Y ; ) d2wg *q]8wTSn2n u0U0]abCX%]


















/ *a 21^W +*a
* e /L *EV
* ) D^
*aa
21^W +*a
*
e /L D^
j*
r * )1/^ * * F D^ +*U W* >



*




&


h Ud ZCcfiWW1^UX%] U_/W4Urhe

1


^



+

C
*


W


h








*
F







,



bQ/Us d2wXg} aXEo Vs@U_T>]abSW4U_W~RCX%^XVkW4]aW4^YUW4]a] Z/\VUmX%^WX%R/R/^TVR/^d2X%]aWowTV^
{:f)zF] X%1d Z/\N wxTV^8X%ZeUQCcb]ab*X%]Xg} !Vaq;ffTV^W4T tVW4^s q

WZ/Tu U_b/T u ]abCX%] s0u8d ]abz]ab/W4U_WmRCX%^XVkW4]aW4^U_W4]a] Z/\VU
1^ W
k jd U]ab/W
R/^_TVf*X%fCd2n2d ]ez]abCX%]<Q6f)zFYb*XVn ]aUu8d ]abh4asffwTV^oXVn2n[
%X ZCi q n WX%^n e d2w
1^ W h Vs]abCd Uhd U0]a^_Q/WsUd ZCcfiW{]ab/W4
Z 1^ W k VsEX%ZCi6]ab/W{R/^_TVf*X%fCd2n2d ]e
]abCX%N
] {:f)zFhb*XVn ]aUu8d ]abzTVQ/]aR/Q/]m 4d U>X%]>kTVU_<
] XE> D^W
VqPT
U_Q/RSRATVUW0]abCX%
] D^ W H
Vq b/W4Z>d ]-U_]
Q vlcfiW4U@]aT6Ub/T uz]abCX%
] D^ +*UW -d U&]ab/W

`_=

fi " |qxq}t~"}s1|

R/^_TVf*X%fCd2n2d ]e ]abCX%]0 4*ad UTVQ/]aR/Q/] sV\Sd tVW4Z5]abCX%]2vd U;cb/TVUW4ZX%]]ab/W&/^_U]*U]aW4RCq-EQS]]ab/W{X%^_\VQCkW4Z]

TSw7]ab/W/^_U]&bCXVn2w/TSw7]ab/WRS^_T`TSwSU_b/TuU]abCX%]0]abCd UR/^_TVf*X%fCd2n2d ]e>d UQ/U_]x qCQ/]






X%U iVW4Ud ^_Wi/q

&
b



)
T_Y)

1^

&






)



/)&

//

b


h Ud ZCcfiWU
/



b



+*fia21^W >+*fi
h Ud ZCcfiWD^UX%] U_/W4ULrhe
1^ +*CW h



{
;7fi|
1^8ZeC'`%V & ^i^i^i4FjAh6QD/'A|,| +
\b"a\
:eS *
& ^ i^i^i^gj)ffR & ^^ gj E ) 8 | , n & & ^i^i^i^gj7FjE#Lp
@VV^i^i^i 6"
l W* lK+Sfffi6, )n +
Oz %}9=D
^ 8V &
e1 ^W 8 ^8
6 & & ^i^i^i^gj7Fj7R
>*fi
zu}D ^ W ^ W +*fi 8?
[+*)D ^ W
!
h



bSW~R/^_TTSwd UYUd2kffd2n2X%^md ZU_RCd ^d ]ff]aTo]ab*X%]YTSw bSW4TV^_Wk Vq2VqPQ/R/RgTVU_W]abCX%]maXV{bSTSn2iUs
>+*fis1X%ZCiD^W h!Vq bSW4Z
D^W




^


W


?D^ 8
a21^W



j
j


^




6



^

^

^



^



?

1


^


W


a21^

8
& &
a2D^8
C4* 1^,>fi+fi* ?1^
C

*
4
1


^




8


2

1

8
^






*





[<aR

P/d2kffd2n2X%^n es

D^ W >+*fi
D^ W >*Y ?D^
CS 1 ^ 6 & & ^i^i^i ^ j

fi+*?D^
CS C *\D ^8

a+*fi
C*_1 ^ 8a21 ^8}
^W
bQ/UsCD^W
K
Vq

a2D^ W >*

j ?1^W a2D^
}+*fi
8a21 ^W





* 0wTV^

XVn2n$o

*



*

U_QCcOb~]abCX%]m1^



TV^]ab/WcfiTVZtVW4^_U_Ws-U_QSR/RgTVU_W]abCX%]fCbSTSn2iUmX%Z*i
^ Vq${d tVW4Z *fisd2w
1^ W Vs1]ab/W4ZaXV-]a^d t1d2XVn2n emb/TSn2iUsAU_TUQ/R/RgTVU_W ]abCX%]W1^/W h0VqPQ/R/RgTVU_W
]abCX%] *qn WX%^n e D^6 & & ^i^i^i^gjAFj7 ;*_1^8fi+*aqwBTu8s*Q/Ud Z/\[fCas*u@WYbCX_tVW
]abCX%]









D^ W
D^ W
D^ W
D^ W +*C
C*\D ^8
fi+*




?D^ ha21^W
>+*fi?1
^W a2D^ 8
8

?


1
^W a2D^W }+*fi


h Q/Ud ZS\~aVLq

`_u`

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

bQ/Us1aXVLbSTSn2iUq

6| , n /
:eS */,0'`,V & ^i^i^i 47j 6G0 /66EG#
/ & ^i^i^i2` * * & & ^i^i^i^ *j j ,) &C *( S87n %Y% )
X aX Y`*fi0. 6SVV^i^i^i ^; fi7, %o1^q ,
Xg 1^, zff6Xg {p /*6mD^5}Y/D^5,MK-85 fi
,xQy
/z `%z}66*ffpo%}; { & ^i^i^i 4jE
TSw7u@TV^n2iUsEXU_W4] / & ^i^i^i 2`
TSwSTVfSU_W4^_tX%] TVZ/U;u ]abi/d U_]a^d f/Q/] TVZ
[<aR {d tVW4Z[XU_W4]
X UX%] Uwxe1d Z/\X Y`* mwxTV^65 VV^i^i^i ^ Vs0X%ZCi[X%^_f*d ]a^X%^_e|i/d U]a^d f/Q/] TVZ/UD ^8 TVZF
V^i^i^ 6"
l sAu@W6W4RCn2d2c4d ] n ecfiTVZ/U_]a^_QCcfi]5X8R/^d TV^W1 ^TVZe
]abCX%]UX%] U_/W4UL r6zU_Q*cb]abCX%]fiXE 1 ^,&

ub/W4^WW1
^ U0]abSW5kffX%^_\Sd ZCXVn)TSwRD ^TVZ$
X%Z*i1 ^, 1 ^8
ufi aq
{d tVW4Zv
}F s1iVW4/Z/W
^ a5


/ `*fi6W>
/ h
V X Y`*u; *(7D ^, h
ui
ay"T uv]ab/W{R/^TVfCX%fCd2n2d ]e}d U'U_RCn2d ];Q/RTtVW4^6XVn2n/]ab/W^Q/Z/UV6
U_Q*cb~]ab*X%] / `*-X%ZCiW}

U{d ^_^_Wn W4tX%Z`] q2{]-^_Wk5XVd Z/U-]aTcOb/Wcj]abCX%]Z1
^{d U{Xhi/d U_]a^d fSQ/] TVZTVZq X%ZCi8]abCX%]{d ]@UX%] U_/W4U{XVn2n]ab/W
[<a"ZW " ^ " l

^_W QCd ^_WkW4Z]aUqLfi]d U0WX%Uej]aTlcb/WcO]abCX%]



j
1^ `*
X `*fiuC*(7D^,
r & F

fi ]8wxTSn2n TuU0]ab*X%]
*
r & 1^ `*fi
]ab/WYk5X%^_\Sd ZCXVn)TSw1^TVZqL4w]v>







X `*ui

sU_bST u8d Z/\6]ab*X%]1^8d U XR/^_TVfCX%f*d2n2d ]e>kWX%U_Q/^WYX%ZCixX
V
s]ab/W4Z



dU



1^,>fiF D^,> a2D^aF
b fiW) / _ ) /




ff


_fi") /
_) / b fi)
CS

)
_ ) // ff

^ ui
1

b ) _S /
/

_"
fi ) /

ZCXVn2n esgZ/TV]aWh]abCX%] swTV^VV^i^i^i6l"VswTV^ffXVn2nEF'U_QCcObm]abCX%]=1^W
L
]abCX%]
D^ `*CW h
fi") / b
_
) /


b fiW) / b ) /

fi") /
_

b fi ) /

fi") / b
_
) /


b fiW) / b ) 1 /


r
b ) b fi ) 1 / /
D^ `*CW>

VsAu9WhbCX_tVW
V

U_Tff]abSW\VW4Z/W4^XVn2d 4WiLrhcfiTVZCi/d ] TVZb/TSn2iU8wTV^8V & ^i^i^i4Fj7Vq

TR/^_TtVW b/W4TV^_Wk Vq2VsBu@W/^_U_]hZ/W4WiU_TSkWfCXVcO`\V^_TVQSZCimTVZkffd ZCd2k0QCk^_Wn2X%] tVWW4Z`]a^TVR`e
i/d U_]a^d f/Q/] TVZ/UqLd U_TSkW{U_R*XVcfiW X%ZCi~n W4]8 & ^i^i^i4Fj5fgW{U_Q/fSU_W4]aUTSwq0~W4] fAW]ab/WU_W4]0TSw

`_Y

fi " |qxq}t~"}s1|

u & ^i^i^i ^EjS-wxTV^-ub*d2cb]ab/W4^_W'W41d U_]aUU_TSkWffi/d U]a^d f/Q/] TVZ<Xgu ]ab<XE>a*fi ;*9wxTV^-
%X ZCicXE> = mwTV^~XVn2n {vqw"T u n W4][XE fgWmXmi/d U_]a^d f/Q/] TVZu8d ]abJXE> =
@ ^i^i^i jA j s1n W4]

vq&{d tVW4ZXtVWcfi]aTV^
&



V^ i^i^i 6 l
mwTV^~XVn2n


) /"!#!#!
) /
X h
%$ $ XE>



ub/W4^W Uh]ab/Wd ZCi/d2c4X%]aTV^~wQ/ZCcfi] TVZCs&d2q Wq h ld2wR v X%ZCiYTV]ab/W4^_u8d UWs&X%ZCi


)7

) /&"!#!#! %$ $ ) / gX } jd UlXZSTV^kffXVn2d X%] TVZ wXVcfi]aTV^q~W4];* X a*fiYwxTV^Y
V^i^i^i 6l"q0EeuLUd U2X% ^sDVVVVs b/W4TV^_WkU Vq2ffX%ZCiVq2VasCd ]8wxTSn2n TuU0]ab*X%]

Xg}u6
&

& ^i^i^i^Ej7jS

X
'



j

aVV

@ ^i^i^i j 0
TV^_W4T tVW4^s@wTV^WXVcb}tVWcfi]aTV^u & ^i^i^i^ j 0(ffsg]abSW4^_W~d U5X tVWcfi]aTV^
5

U_QCcOb
&
]abCX%]aVV*b/TSn2iUqaTV^X%Z>d ZCwTV^kffXVn7X%ZCi8WX%U_eiW4^d tX%] TVZTSwDaVVasU_W4W6uLT tVW4^ bSS
kffX%UsAVVVVs
LbCX%RS]aW4^8Vaq2
ofnq e1E ? & & ^i^i^i^EjAFj6|phYu & ^i^i^i^gj7;) E & ^i^i^i j7m
n , ) Zz|%}Y
. & ^i^i^i^EjU.9 * V {,|ph0@>VV^i^i^i6l"B
XE>
a+*C6 & & ^i^i^i ^C* & +* & ^;* R& +* R& ^i^i^i^EjAFjS ;*fii

Dh\



vd ]ab/TVQS]9n TVU_UETSw\VW4Z/W4^XVn2d ]esX%U_U_Q*kW]abCX%]
]8wTSn2n uUhwx^TSkaVV]abCX%]

[<aR

X 6 +
U_Tff]ab*X%]

&



Vq X%gd Z/\

* X a+*fiDwTV^D

V^i^i^i 6l"s

j j +* * "!#!#! % $ $ X ha



& ^i^i^i ^EjSjS Xg>u6 + + ^i^i^i^ j Fj7ui
j jS * X%ZCi<XE>a+*U6 & & ^i^i^iXEj7jS ;*;wxTV^ V^i^i^i 6l"s7u@W
V^ i^i^i 6 l"q bQ/Us+Xg>u6 + + ^i^i^i^gjAFj7 Xg}u6 & & ^i^i^i ^Ej7jS

XE>u6

P/d ZCcfi\
W XE>a+
*U6 + + ^i^i^i ^
bCX_tVW8]abCX%]m;* * wxTV^'
X%ZCi/s1d Z~RCX%^_] d2cfiQCn2X%^s

+ ^i^i^i^



&

& Xg}a & 6
&

& ^i^i^i^Ej7jS

XE>a & 6
+

+ ^i^i^i^gjAFj7ui

@on oK G~,)/j/o># / & + }G~| , n /8
`* ;* &
+ R V4
ZD^e,o% ) ,) 1^ & ,
1^ + H
9/1
^,}
D^W* hHv0 ?$ E HD^ * 1^u
*
`*fi,"/~
KD^ ff=h
p SLGD^ 96f & Q + {/fi fi
ffB
*
6//-|n 1
^ 1 ^8
u `*,\ Z, V4
~ W4] & & + ^ + + & ^ . &;k + s"X%Z*i 2 & + aqlP/d ZCcfiW
[<aR
& ^ + ^ . ^ 2 X%^_W~XVn2nX%U_UQCkWiY]aTfgW8Z/TVZSWkR/]es1u9W6bCX_tVW, aV4V + sAub/W4^W- U5iVW4/Z/WimX%U
X%fgT tVWs]abCX%]'d Us.$d U"]ab/W0U_W4]'u & ^ + gU_QCcOb5]abCX%]"]ab/W4^_WW41d U_]aUX{i/d U_]a^d fSQ/] TVZNX u8d ]ab4X &
& Xg>
+ + s2Xg>
h
K
5wxTV^8XVn2n?v
v
q*4wD ^ * ^,>
u `*wxTV^0 V4Vs)]abSW4Z
pE1 ^,}
u & pAu1 ^8}
u + 1 ^,}

aVV


\b"a\



:e
*
& ^C* +

`_ua

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

ub/W4^W3p D^ & aqLWR/^_TtVW]ab/W]ab/W4TV^_Wkvfe5Ub/T u8d ZS\ ]ab*X%]aVV0c4X%Z/Z/TV]&b/TSn2id2wWd ]abSW4^
& TV
^ + U9Z/TV]0W47^_W4epn2d VWqP/d Z*cfiWu9W;bCXatVWhX%U_U_QCkWi]abCX%]'uC* & ^;* + >aV4V + wTV^" V4Vs
+ wxTV^@
u@Whc4X%ZX%R/RCn e~aVVC]aTx`*BwxTV^@ V4Vq bQ/UsV]abSW4^_W X%^_W'tVWcfi]aTV^_U * & * + B
V40U_QCcOb
]abCX%] sDwTV^8XVn2]
n
vs
1,^ > `*fi

* D,^ > u
aVV

*

*

* &
D^,>u & as\D^,>u + `* * & * D^8u + as\D^,>u . `*fi

u 2 * * & 1
^ u 2 aq"n Q/\V\Sd ZS\]abC UYd Z]aToaVVasEu9W6TVf/] XVd Zo]ab/W

aVVEd2kRCn2d W4UH1^u & `*
* &
* D^ u . asK1^

wTSn2n u8d Z/\mwxTVQS^&WQCX%] TVZSU

D^8u
+





D^8u
.





D^ u

p /

1 ^8u &
&
p
* 1^8u +

&
p
/
+*
1^,>u
&

2





p

D^8u





&





&

1^ u

2





p7 +* 1 ^,>u &
+
p7 +*/* 1
^,>u +
+
+*
+*/*
^,>u
. p7
+

7
p 1^ u 2 ui

+


.


a%S

P/d ZCcfiW&u@W&bCX_tVW8X%U_U_QCkWi]abCX%]1^ !wTV^;XVn2nM evs/d ];k0Q/U_]LfgW&]ab/Wc4X%U_W]ab*X%]1^ u * 2Vs
wTV^ff V^ i^i^i fiSq b`Q/U
1^u T*fiffwXVcfi]aTV^_U5TVQ/]6TSw"]ab/W]abWQ*X%] TVZX%fgT tVWqEe]ab/W}cObCX%Z/\VW~TSw

tX%^d2X%fCn W4U pA & s1 p7a + 2
*(
hX%ZCi UTSkW^_W4u^d ] ZS\Ssu9WU_W4W]ab*X%]a%S

UW QCd tXVn W4Z`];]aT

+0&

&+ ++

aVV
&& &+ && &+ aY +0& ++ +0& ++ ui
4ws wxTV^~U_TSkW,s;fgTV]abHW* & X%ZCi>v* + X%^_WZSTVZ/4W4^_TSs;]ab/W4Z]ab/W]abS^_W4WWQ*X%] TVZ/UTSwhaVVhbCX_tVW>Z/T
U_TSn Q/] TVZSUhwTV^ [aV4Vaq9QCd tXVn W4Z] n esd2wwxTV^'U_TSkW8,s/fgTV]ab * & X%ZCi * + X%^_W8Z/TVZ/4W4^_TSsS]ab/W4Zl]ab/W
wTVQ/^0W QCX%] TVZ/U'TSwa%SBb*XatVWZST5U_TSn Q/] TVZSU wTV^:
p oaV4Vaq{PTld ];TVZCn e^_WkffXVd Z/U']aT5Ub/T u]abCX%] wTV^
U_TSkW ,sSfgTV]ab * & X%ZCi * + X%^_W Z/TVZ/4W4^_TSq TjU_W4W]ab*d Us7Z/TV]aW8]ab*X%]0feoX%U_U_QCkR/] TVZ[wTV^'U_TSkW%s)`*
UZSTV]q4W4A^_W4epn2d VWqCQ/]]ab/W4Zzd ]jwTSn2n uUw^_TSk ~/WkffkffXhr6q2X%fgTtVW5]abCX%]8fATV]ab * & X%ZCi * + X%^_W






Z/TVZ/4W4^TSq bQ/Us]ab/W]ab/W4TV^Wk

&&





UR/^T tVWi/q

zfiq| ?q
r6n2cb/TVQ/^^CTV ZCsF0qD"q2sC{X% ^iW4ZCwTV^_UsFAq2s*$lX%1d Z/U_TVZCsr'qCaVVVVaq{-Z]ab/WYn TV\Sd2c0TSwA]ab/W4TV^_e}cb*X%Z/\VW
R*X%^_] d2XVn-kW4W4]wxQ/Z*cfi] TVZ/UwxTV^cfiTVZ]a^XVcfi] TVZX%ZCi~^_W4t1d Ud TVZCq`
%/-G[
|/Mpm + E 4s10s
VVSVVVq

-X%^_py'd2n2n Wn2sq2s*$/XVn 1sL0qCaVVVVaq8PTSkW]aWX%U_W4^_U6cfiTVZCcfiW4^_Z*d Z/\mcfiTVZ*i/d ] TVZCXVnSR/^_TVfCX%fCd2n2d ] W4Uq*
S)
sEVV SVVVq

LTtVW4^s

q7lq2sA

bSTSkffX%UsEqVr6qAaVVVVaq9
ffp /'GZaph*%q1

`_u

d2n W4esUwBW4uzTV^_1q

fi " |qxq}t~"}s1|

LUd U2X% ^s-4q;aVVVVaq324piSd tVW4^_\VW4ZCcfiW\VW4TSkW4]a^_e|TSw-R/^_TVf*X%fCd2n2d ]eiSd U_]a^d f/Q/] TVZSUmX%ZCikffd ZCd2k5d X%] TVZ
RS^_TVfCn WkUq*

x //+G:
9 | +s54DaVasC%SSVVVq
LUd U2X% ^s14qaVVVVaq6b`en WX%U_]{UQCX%^W4UYX%ZCi>kffX%1d2k0QCkW4Z]a^_TVR`e1r;ZX%1d TSkffX%] d2cjX%R/R/^_TSXVcOb>]aT
Z*wxW4^_W4Z*cfiWYwxTV^ n2d Z/WX%^ ZtVW4^_U_W{RS^_TVfCn WkUq0
x //+G:
|/,as|E Sas*VVVSVVVVq
rXau8d2iSsrhq?g qaVVVVaqSr|ZSTV]aW&TVZ~kffX%gd2k0QCkW4Z]a^_TVReX%ZCi6-X_eVW4Ud2X%ZlcfiTVZCiSd ] TVZCd Z/\Sq2q+"Z/R/QSfCn2d U_b/Wi
k5X%Z`Q/Ucfi^d R/] q

rXau8d2iSsgrhqRgq2ssrd2cVW4es;qLqaVVVVaq~Cd VWn2d b/T`T1iX%ZCim-XaeVW4Ud2X%Zzd ZCwW4^_W4ZCcfiWwx^TSkU_Wn Wcfi] tVWn e
^W4RATV^]aWii/X%] XVq0
),SEG6
xVh
p %,4|/;
x ,s[ 6EaVVVasL%SSVVVq
rd2XVcfiTVZCd Us[g q2s" X%fgWn2n2s-P/q!E~ q"aVVVVaq}PTSkWmXVn ]aW4^_ZCX%] tVW4Uh]aT@X_eVW4U Uh^_QCn WqYfiZ-^_TSwkffX%ZCs@-q2s
-u@W4ZCs8{qa"iUq2as3"fiff
|/,S1 /on %[G|
*
,/|a 8n /ff
*8 fi S}
9+"
,/|h
p sR/R*qAVSVVq
r9Q/fgTSd Us
r q2s1 ^XViWsy'qaVVVVaq r'Z$d Z`]a^_T1iQCcfi] TVZz]aTRgTVU_Ud f*d2n2d U_] d2cmX%ZCizwxQ/44evn TV\Sd2cfiUq fiZ
PVbCXVwxW4^s/{q2s7
WX%^n2sg qa"iVUq2asgBy ,/ g %By ,//4sR/RCq)%SSVVVqAffTV^\SX%Z
tX%QCwk5X%Z/ZCsgP/X%Zo^X%ZCc4d UcfiTSq
^_W4QSZCi/sU
q19qDaVVVVaqQ1 Q/4n W{TV^0RCX%^XViTgq0xVh
p %4|/ )sR|E SasCVVVSq
^d WiSkffX%ZCs_{
q2sPVbCd2kTVZesr6qaVVVVaqE X_e`Z/W4UkffX%1d2kQCkW4Z`]a^TVR`eR/^W4Ucfi^d R/] TVZX%ZCi-R/^_TVf*X%fCd2n2d ]e
]abSW4TV^_eqQ
),SDG:
|S,,)" _sF sDVVSVVVq
^d WiSkffX%ZCs;
w q2sC$y'XVn RAW4^ZCsF` q6qEaVVVVaq5T1iWn2d Z/\jfAWn2d Wwd Z[ieZCXVkffd2c&Ue`U_]aWkUqF9 X%^_]04DwTVQ/Z/p
iSX%] TVZ/Uq0
x % K- Ca /+ S4sFd DaVas*VVSVVVq
^d WiSkffX%ZCs4;
w q2sy;XVn RgW4^_ZCs4` q46q`aVVVVaq15TgiVWn2d Z/\;fgWn2d Wwd ZieZCXVkffd2cDU_e`U]aWkUq9 X%^_]*_^_W4t1d Ud TVZ
X%Z*ihQ/RBi/X%]aWq
),/DGmxQou2"y ,fi s2+ 0sCVVSVVVq
{X%^iVZ/W4^s1lqDaVVVVaq|/,Sx|/ / K-xVh
p ,- |ff
6h
~7h
p "),
/!Ion %
,/aq;P/d2kTVZ} P/cOb`QSU_]aW4^sC"
w W4uTV^_1q
{X%^iVZ/W4^s1lqDaVVVVaq#0
x |0@ +7 yV^_W4WkffX%ZL TSq
{d2n2n2s6;q.'
r q2s;tX%ZiW4^C~ XVX%ZCshq2s6 BTVfCd Z/Usm qaVVVVaq L TSX%^_U_W4ZCd ZS\X%]^X%ZCiTSk5 bCX%^XVcfip
]aW4^d UX%] TVZ/Us&cfiTVZ/Wcfi]aQ/^_W4UX%ZCi}cfiTVQSZ`]aW4^_pW41XVkRCn W4UqjZJ"fi7VL
,,|/+*8 /
-,asRSRCq1VVSV%Sq
-^_TtVWs7rhq
q2sEy;XVn RgW4^_Z*sF qhqCaVVVVaq1 ^TVfCX%fCd2n2d ]eQSRiSX%]aW;cfiTVZCiSd ] TVZCd Z/\5tUqCcfi^_TVU_U_pW4Z]a^_TVReq
fiZm"fi7*
,, /z*8 fi /,sg %/m
x , K-Ha / + /,z UxQ gG}
RSRCq1VVSV%Sq

-^QS Z`uXVn2i/sUgq1aVVVVaq;PV]a^_TVZ/\hW4Z`]a^TVR`e>cfiTVZCcfiW4Z]a^X%] TVZCs/\SXVkW{]ab/W4TV^_eX%ZCiXVn \VTV^d ]abCkffd2c-^X%Z*iTSkp
ZSW4U_Uq7q
Z "fi7 S%/W
x0//)**8 /8o*|
p;7)/ E ,//
*,s`R/RCq
VV SVVVq
y'XVn RAW4^ZCs]qVhq2s7/X%\Sd ZCsA0qSaVVVVaqBffT1iWn2n2d Z/\8`ZST u8n Wi\VWhX%ZCi5XVcfi] TVZd ZliSd U_]a^d f/Q/]aWiU_e`U]aWkUq
I%o )>*| p;7)S48
4ESas*VV SVVVq

`_u

fib

c edbgfb]fbz}zhcyz1

s1t yz|qu

y'XVn RAW4^ZCs`q/6q2sL Q/]a] n WslqC;qCaVVVVaqWt-Z/T u n Wi\VWsAR/^_TVfCX%f*d2n2d ]esLX%ZCimXVitVW4^_UX%^d W4Uq=`)%/
6 h
x8*s:9;0DSasCVVSVVVq
y"Wd ]aX%ZCs[rq2s-Q/fCd Z*sHrq@aVVVVaql\VZ/TV^X%fCd2n2d ]e|X%Z*iocfiTSX%^_UWiSX%] XVqx/S
YGx|S,as0s
V%V/
SVVVVq
y"Q/Z`]aW4^s[r'q@aVVVVaqX%Q/UXVn2d ]eX%Z*ik5X%gd2k0QCk W4Z`]a^_TVRe}QSRiSX%] Z/\Sqa/,/S[`%/-G
x"7fi| ph
yB,//4<
4EaVasEVV VSVVq

4W4A^_W4esL0qq*aVVVVaq=1^TVfCX%fCn WZ/Tu8n Wi\VWq'fiZ~CX%)X%]aTVUsA4q*a"i/q2as!a/,/S0*
X)4p
\9
,7mG\
|/ S7* \"fi|
ffp Ga /) n E 4s*RSRCqVVSVVVqV"w TV^_]ab/p
y"TSn2n2X%ZCi/srhkU]aW4^i/XVkffq

t{n Wd ZfCX%QCkffs;rq9aVVVVaq|/)8nonKxS
,8x|/ E ,//S7qmP] X%] U_] d2cfiU~d Z}]abSWly"WXVn ]ab
PSc4d W4ZCcfiW4UqEPVR/^d Z/\VW4^_p/=W4^n2X%\SsCB
w W4uTV^_1q
t-QCn2n f*XVc1sDP/q1aVVVVaq.a 8 8h
p * ,%YSx|/ aqvd2n W4eq
t-QCn2n f*XVc1s"P/q2sB ~ Wd fCn W4^s90qAr6qBaVVVVaql-Zd ZCwTV^kffX%] TVZX%Z*ijU_Q]vc4d W4ZCcfieqx//+6Gj
ph|/,as>6+6VsEVSVVq
5TVU_]aWn2n W4^s&Lq"aVVVVaq*
[*+ /S\"fi| +7h
p ff"fi|, |
m<|S
)/_qjrhi/iSd U_TVZ/p
W4Un W4esCBWXVi/d Z/\SsDlX%U_Uq

w'd Wn U_W4ZCsP/q@aVVVVaqv*,, //l.yB/|p /8|/p )+"x0
,|phaqDbCqhrqE]abSW4Ud Us
r9W4RCX%^] kW4Z`];TSw b/W4TV^_W4] d2c4XVnEPV] X%] U_] d2cfiUs)BZCd tVW4^Ud ]eTSwRLTVRgW4Z/bCX%\VW4ZCq
TVf*d Z/Us2
q2s"TV]aZCd ]a4es1r6q2sB P/cObCX%^wU_]aWd ZCs2
r qaVVVVaqPW4Z/Ud ] t1d ]eX%ZCXVn eUd UYwTV^U_Wn Wcfi] TVZofCd2X%U
X%Z*i Q/ZCkWX%U_QS^_WiYcfiTVZ*wxTVQ/Z*i/d Z/\d Z}kffd U_Ud Z/\i/X%] XhX%ZCic4X%Q/UXVn1d ZCwxW4^W4ZCcfiWffkTgiVWn UqDfiZ}y'XVn2n TV^s
lq1"q2s1 CW4^_^esUr'q1a9iUq2as+|/,,E +1Affph+ q*m"]nfi]ph S9S
*+/,;S%+as/5r?=TSn QCkWYVVVs/R/RCq1SVVqEPVR/^d Z/\VW4^_p/=W4^n2X%\Sq

QSfCd ZCsYrqDaVVVVaqBfiZCwW4^_W4ZCcfiW5X%ZCi~kffd U_Ud Z/\~iSX%] XVqQ-p , Vs8@+4sEVVSVVVq
P/cObCX%^wU_]aWd ZCs4rq`{q2s?rX%ZCd Wn Usq2sTVfCd ZSUs\`q`lqaVVVVaqZCcfiTV^RATV^X%] Z/\0R/^d TV^CfgWn2d WwU-X%fgTVQ/]CU_W4p
n Wcfi] TVZfCd2X%Ud Z`]aT ]ab/W8X%Z*XVn e`Ud U@TSw`^X%ZCiTSk5d 4Wi]a^d2XVn U@u8d ]ablk5d U_Ud Z/\{TVQS] cfiTSkW4UH
q -|
ph %o q
TX%R/RgWX%^q
PWd2iW4Z*wxWn2i/s q1aVVVVaq;DZ`]a^_TVRe>X%ZCihQ/Z*cfiW4^_] XVd Z`]eq9+,7G:|S /,4s++4sSVVSVVq
PbCXVwW4^s/qSaVVVVaqHxffp ,R*%6G#Hn /,4qR1^d ZCcfiW4]aTVZN"ZCd tVW4^_Ud ]e1^_W4UUsD^d ZCcfiW4p
]aTVZ*C
w;hq `q
PbCXVwW4^s{q)aVVVVaqLTVZCi/d ] TVZCXVnRS^_TVfCX%fCd2n2d ]eqa/ %//|/]yB7n*s;+4EaVas7VVSVVVq
Pb/TV^Ws0`q"q2s; 4TVbSZ/U_TVZCs'0qL q0aVVVVaqr'1d TSkffX%] d2ciVW4^d t)X%] TVZTSw9]ab/WR/^d ZCc4d RCn WTSwkffX%1d p
k0QCk$W4Z`]a^TVR`eoX%Z*i6]ab/W R/^d ZCc4d R*n W{TSwBkffd ZCd2k5d2kQCk cfi^_TVU_U_pW4Z]a^_TVRequHK S//8
ph *,,Rs 7*A6+@EaVas*VSVVq
Pe`^kUs-q;aVVVVaqX%1d2k0QCk
|SS ,48
@14sDV S%Sq

W4Z`]a^_TVRed ZCwW4^_W4ZCcfiW[X%U>XU_RgWc4d2XVn'c4X%U_WTSwcfiTVZCi/d ] TVZCXVn2d X%] TVZCq

`_7

fi " |qxq}t~"}s1|

KvffZ/1sqSaVVVVaq b/WcfiTVZ/U]a^XVd Z`]@^_QCn W0TSw]abSW8kffX%1d2kQCkW4Z`]a^TVR`e6R/^d ZCc4d RCn Wqe|/) ;%
/x"
+AGh~ %4" _s>6g,s/SSVVq
tX%ZXVkRgW4Z/b/TVQ/] sq2sLLTtVW4^s qLaVVVVaqjX%1d2k0QCkW4Z]a^_TVR`e[X%ZCicfiTVZCi/d ] TVZ*XVnAR/^_TVfCX%fCd2n2d ]eq
uHKSSS
a8 p *%s7* 6gSas/SVVSVVq
tX%ZV^XVX%U_U_W4ZCsg-qEq7aVVVVaqErR/^TVfCn WkwTV^-^_Wn2X%] tVW5d ZCwTV^kffX%] TVZ>kffd ZCd2k5d 4W4^_UqH-%`)%/
{3
"
7j6 :|/ S48
4+6sDVV SVVVq
tVTVU PSXatX%Z`] sCqDaVV%Saq.x',h~%
)qPV] q1X%^_] ZSU lX%U_U lX%^_VW4]LX%RgW4^_fCXVcOgq
tVTVU6P/XatX%Z`] s*lqDaX_eoVVsEVVVVaq&r;U_>lX%^d2n eZCq9fi5 /4q b/W4^_Wu9W4^_WXVn U_TlwTSn2n u0Q/R
X%^] d2c4n W4Ud
Z "fio~/6TVZ r9Wc4q;VsVVVVslX%^cb VVsVVVVsZ4Q*n eVVsVVVVs8X%ZCi
cfi]aTVfAW4^hVVsDVVVVq
tVTVUP/X_t)X%Z] sq&aPVW4R/] q&Vs;VVVVaqr'U_X%^d2n e`ZCqJ9m~/4s'VVqzTSn2n u0pQ/RX%^_] d2c4n W4U
X%RSRAWX%^Wid c
Z "ff~/&TV
Z r9Wc4qEVsDVVVYRCq1VV0X%Z*iW4fCqDVVsEVVV5RCqDVVaq

`_u

fiJournal Artificial Intelligence Research 19 (2003) 25-71

Submitted 10/02; published 08/03

Answer Set Planning Action Costs
Thomas Eiter
Wolfgang Faber

EITER @ KR . TUWIEN . AC .
FABER @ KR . TUWIEN . AC .

Institut fur Informationssysteme, TU Wien
Favoritenstr. 9-11, A-1040 Wien, Austria

Nicola Leone

LEONE @ UNICAL .

Department Mathematics, University Calabria
I-87030 Rende (CS), Italy

Gerald Pfeifer
Axel Polleres

PFEIFER @ DBAI . TUWIEN . AC .
POLLERES @ KR . TUWIEN . AC .

Institut fur Informationssysteme, TU Wien
Favoritenstr. 9-11, A-1040 Wien, Austria

Abstract
Recently, planning based answer set programming proposed approach towards realizing declarative planning systems. paper, present language K c ,
extends declarative planning language K action costs. K c provides notion admissible optimal plans, plans whose overall action costs within given limit resp.
minimum plans (i.e., cheapest plans). demonstrate, novel language allows
expressing nontrivial planning tasks declarative way. Furthermore, utilized
representing planning problems optimality criteria, computing shortest plans
(with least number steps), refinement combinations cheapest fastest plans.
study complexity aspects language K c provide transformation logic programs,
planning problems solved via answer set programming. Furthermore, report experimental results selected problems. experience encouraging answer set planning may
valuable approach expressive planning systems intricate planning problems
naturally specified solved.

1. Introduction
Recently, several declarative planning languages formalisms introduced, allow
intuitive encoding complex planning problems involving ramifications, incomplete information, non-deterministic action effects, parallel actions (see e.g., Giunchiglia & Lifschitz, 1998;
Lifschitz, 1999b; Lifschitz & Turner, 1999; McCain & Turner, 1998; Giunchiglia, 2000; Cimatti &
Roveri, 2000; Eiter et al., 2000b, 2003b).
systems designed generate plans accomplish planning goals,
practice one often interested particular plans optimal respect objective
function quality (or cost) plan measured. common simple objective
function length plan, i.e., number time steps achieve goal. Many systems
tailored compute shortest plans. example, CMBP (Cimatti & Roveri, 2000) GPT
(Bonet & Geffner, 2000) compute shortest plans step consists single action,
Graphplan algorithm (Blum & Furst, 1997) descendants (Smith & Weld, 1998; Weld,
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Anderson, & Smith, 1998) compute shortest plans step actions might executed
parallel.
However, other, equally important objective functions consider. particular,
executing actions causes cost, may desire plan minimizes overall cost
actions.
answer set planning (Subrahmanian & Zaniolo, 1995; Dimopoulos, Nebel, & Koehler, 1997;
Niemela, 1998; Lifschitz, 1999b), recent declarative approach planning plans encoded answer sets logic program, issue optimal plans objective value
function addressed detail far (see Section 8 details). paper,
address issue present extension planning language K (Eiter et al., 2000b, 2003b),
user may associate costs actions, taken account planning
process. main contributions work follows.
define syntax semantics planning language Kc , modularly extends
language K: Costs associated action extending action declarations
optional cost construct describes cost executing respective action.
action costs static dynamic, may depend current stage plan
action considered execution. Dynamic action costs important
natural applications, show simple variant well-known Traveling Salesperson
Problem, cumbersome model solve other, similar languages.
analyze computational complexity planning language K c , provide completeness results major planning tasks propositional setting, locate
suitable slots Polynomial Hierarchy classes derived it. results provide insight intrinsic computational difficulties respective planning problems,
give handle efficient transformations optimal planning knowledge representation formalisms, particular logic programs.
show, awareness results complexity analysis, planning action
costs implemented transformation answer set programming, done system prototype developed. prototype, ready experiments, available
http://www.dlvsystem.com/K/.
Finally, present applications show extended language capable
easily modeling optimal planning various criteria: computing (1) cheapest plans
(which minimize overall action costs); (2) shortest plans (with least number steps);
and, refinement combinations these, viz. (3) shortest plans among cheapest, (4)
cheapest plans among shortest. Notice that, knowledge, task (3)
addressed works far.
extension K action costs provides flexible expressive tool representing
various problems. Moreover, since Ks semantics builds states knowledge rather
states world, deal incomplete knowledge plan quality, is,
best knowledge, completely novel.
experience encouraging answer set planning, based powerful logic programming
engines, allows development declarative planning systems intricate planning
26

fiA NSWER ET P LANNING U NDER ACTION C OSTS

tasks specified solved. work complements extends preliminary results
presented previous work (Eiter et al., 2002a).
remainder paper organized follows. next section, briefly review
language K informally presenting main constituents features simple planning
example. that, define Section 3 extension K action costs, consider
first examples usage Kc . Section 4 devoted analysis complexity issues.
Section 5, consider applications K c . show various types particular optimization
problems expressed K c , also consider practical examples. Section 6,
present transformation K c answer set programming, Section 7, report
prototype implementation experiments. discussion related work Section 8,
conclude paper outlook ongoing future work.

2. Short Review Language K
section, give brief informal overview language K, refer (Eiter et al., 2003b)
Appendix formal details. assume reader familiar basic ideas
planning action languages, particular notions actions, fluents, goals plans.
illustration, shall use following planning problem running example.
Problem 1 [Bridge Crossing Problem] Four persons want cross river night plank
bridge, hold two persons time. lamp, must used
crossing. pitch-dark planks missing, someone must bring lamp back
others; tricks (like throwing lamp halfway crosses, etc.) allowed.
Fluents states. state K characterized truth values fluents, describing relevant
properties domain discourse. fluent may true, false, unknown state is,
states K states knowledge, opposed states world fluent either true
false (which easily enforced K, desired). Formally, state consistent set
(possibly negated) legal fluent instances.
action applicable precondition (a list literals fluents) holds
current state. execution may cause modification truth values fluents.
Background knowledge. Static knowledge invariant time K planning domain
specified normal (disjunction-free) Datalog program single answer set
viewed set facts. example, background knowledge specifies four persons:
person(joe). person(jack). person(william). person(averell).

Type declarations. fluent action must declaration ranges arguments specified. instance,
crossTogether(X, Y) requires person(X), person(Y), X < Y. 1

specifies arguments action crossTogether, two persons cross bridge together,

across(X) requires person(X).
1. < used instead inequality avoid symmetric rules.

27

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

specifies fluent describing specific person side river. literals
requires must classical literals static background knowledge (like person(X)
person(Y)), literals built-in predicates (such X < Y). implementation K, DLV K system (Eiter, Faber, Leone, Pfeifer, & Polleres, 2003a), currently supports built-in predicates
< B, <= B, != B obvious meaning less-than, less-or-equal inequality
strings numbers, arithmetic built-ins = B + C = B C stand integer
addition multiplication, predicate #int(X) enumerates integers (up
user-defined limit).
Causation rules. Causation rules (rules brevity) syntactically similar rules
action language C (Giunchiglia & Lifschitz, 1998; Lifschitz, 1999a; Lifschitz & Turner, 1999)
basic form:
caused f B A.

conjunction fluent action literals, possibly including default negation, B
conjunction fluent literals, possibly including default negation, f fluent literal.
Informally, rule reads: B known true current state known true
previous state, f known true current state well. if-part
after-part allowed empty (which means true). causation rule called
dynamic, after-part empty, called static otherwise.
Causation rules used express effects actions ramifications. example,
caused across(X) cross(X), -across(X).
caused -across(X) cross(X), across(X).

describe effects single person crossing bridge either direction.
Initial state constraints. Static rules apply states initial states (which
may unique). expressed keywords always : initially : preceding
sequences rules latter describes initial state constraints must satisfied
initial state. example,
initially : caused -across(X).

enforces fluent across false initial state X satisfying declaration
fluent across, i.e., persons. rule irrelevant subsequent states.
Executability actions.

expressed K explicitly. instance,

executable crossTogether(X, Y) hasLamp(X).
executable crossTogether(X, Y) hasLamp(Y).

declares two persons jointly cross bridge one lamp. action
may multiple executability statements. statement
executable cross(X).

empty body says cross always executable, provided type restrictions X
respected. Dually,
nonexecutable B.

prohibits execution action condition B satisfied. example,
nonexecutable crossTogether(X, Y) differentSides(X, Y).

28

fiA NSWER ET P LANNING U NDER ACTION C OSTS

says persons X cross bridge together different sides bridge.
case conflicts, nonexecutable overrides executable A.
Default strong negation. K supports strong negation (, also written -). Note, however, fluent f, state neither f -f needs hold. case knowledge
f incomplete. addition, weak negation (not), interpreted like default negation answer set
semantics (Gelfond & Lifschitz, 1991), permitted rule bodies. allows natural modeling inertia default properties, well dealing incomplete knowledge general.
example,
caused hasLamp(joe) hasLamp(jack), hasLamp(william), hasLamp(averell).

expresses conclusion default, joe lamp, whenever evident
persons it.
Macros.

K provides number macros syntactic sugar. example,

inertial across(X).

informally states across(X) holds current state, across(X) held previous state,
unless -across(X) explicitly known hold. macro expands rule
caused across(X) -across(X) across(X).

Moreover, totalize knowledge fluent declaring total f. shortcut
caused f -f.

caused -f f.

intuitive meaning rules unless truth value f derived, cases
f resp. -f true considered.
Planning domains problems. K, planning domain PD = h, hD, Rii background
knowledge , action fluent declarations D, rules executability conditions R; planning
problem P = hPD, qi planning domain PD query
q = g1 , . . . , gm , gm+1 , . . . , gn ? (l)
g1 , . . . , gn ground fluents l 0 plan length. instance, goal query
across(joe), across(jack), across(william), across(averell)? (5)

asks plans bring four persons across 5 steps.
Plans defined using transition-based semantics, execution set actions
transforms current state new state. (optimistic) plan P sequence P = hA 1 , . . . , Al
sets action instances A1 , A2 , . . . , Al trajectory = hhs0 , A1 , s1 i, hs1 , A2 , s2 i, . . . ,
hsl1 , Al , sl ii legal initial state s0 state sl literals goal true.
is, starting s0 , legal transition t1 = hs0 , A1 , s1 i, modeling execution actions 1
(which must executable), transforms 0 state s1 . followed legal transitions
ti = hsi1 , Ai , si i, = 2, 3, . . . , l (cf. Appendix details). plan sequential, |A | 1
= 1, . . . , l, i.e., step consists one action; plans enforced
including keyword noConcurrency.
Besides optimistic plans, K also support stronger secure (or conformant) plans. secure
plan must guaranteed work circumstances (Eiter et al., 2000b), regardless
incomplete information initial state possible nondeterminism action effects.
29

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

better readability, following always describe K planning problems P
strictly terms sets declarations, rules executability conditions, optionally use
compact representation K programs following general form:
fluents :
actions :
initially :
always :
goal :

FD
AD
IR
CR
q

(optional) sections fluents always consist lists fluent declarations F ,
action declarations AD , initial state constraints IR executability conditions causation rules
CR , respectively. Together background knowledge goal query q, specify
K planning problem P = hh, hD, Rii, qi, given F plus AD R IR plus
CR . 2
2.1 Solving Bridge Crossing Problem
Using constructs, K encoding Bridge Crossing Problem, assuming joe
initially carries lamp, shown Figure 1. simple five-step plans (l = 5),
joe always carries lamp brings others across. One is:
P = h {crossTogether(joe, jack)}, {cross(joe)}, {crossTogether(joe, william)},
{cross(joe)}, {crossTogether(joe, averell)}

3. Actions Costs
Using language K system prototype, DLV K , already express solve
involved planning tasks, cf. (Eiter et al., 2003b). However, K DLV K alone offer means
finding optimal plans objective cost function. general, different criteria plan
optimality relevant, optimality wrt. action costs shown next example,
slight elaboration Bridge Crossing Problem, well-known brain teasing riddle:
Problem 2 [Quick Bridge Crossing Problem] persons bridge crossing scenario need
different times cross bridge, namely 1, 2, 5, 10 minutes, respectively. Walking two
implies moving slower rate both. possible four persons get across within 17
minutes?
first thought infeasible, since seemingly optimal plan joe, fastest,
keeps lamp leads others across takes 19 minutes altogether. Surprisingly,
see, optimal solution indeed takes 17 minutes.
order allow elegant convenient encoding optimization problems,
extend K language K c one assign costs actions.
3.1 Syntax Kc
Let act , f l , var denote (finite) sets action names, fluent names variable symbols.
Furthermore, let Lact , Lf l , Ltyp denote sets action, fluent, type literals, respectively,
2. also format input files system prototype, presented Section 7.

30

fiA NSWER ET P LANNING U NDER ACTION C OSTS

actions :

cross(X) requires person(X).
crossTogether(X, Y) requires person(X), person(Y), X < Y.
takeLamp(X) requires person(X).

fluents :

across(X) requires person(X).
differentSides(X, Y) requires person(X), person(Y).
hasLamp(X) requires person(X).

initially : -across(X). hasLamp(joe).
always :

executable crossTogether(X, Y) hasLamp(X).
executable crossTogether(X, Y) hasLamp(Y).
nonexecutable crossTogether(X, Y) differentSides(X, Y).
executable cross(X) hasLamp(X).
executable takeLamp(X).
nonexecutable takeLamp(X) hasLamp(Y), differentSides(X, Y).
caused
caused
caused
caused

across(X) crossTogether(X, Y),
across(Y) crossTogether(X, Y),
-across(X) crossTogether(X, Y),
-across(Y) crossTogether(X, Y),

-across(X).
-across(Y).
across(X).
across(Y).

caused across(X) cross(X), -across(X).
caused -across(X) cross(X), across(X).
caused hasLamp(X) takeLamp(X).
caused -hasLamp(X) takeLamp(Y), X != Y, hasLamp(X).
caused differentSides(X, Y) across(X), -across(Y).
caused differentSides(X, Y) -across(X), across(Y).
inertial across(X).
inertial -across(X).
inertial hasLamp(X).
noConcurrency.
goal :

across(joe), across(jack), across(william), across(averell)? (l)

Figure 1: K encoding Bridge Crossing Problem
formed action names, fluent names, predicates background knowledge (including
built-in predicates), respectively, using terms nonempty (finite) set constants con .
Kc extends action declarations K costs follows.
Definition 3.1 action declaration K c form:
p(X1 , . . . , Xn ) requires t1 , . . . , tm costs C c1 , . . . , ck .

(1)

(1) p act arity n 0, (2) X1 , . . . , Xn var , (3) t1 , . . . , tm , c1 , . . . , ck
Ltyp every Xi occurs t1 , . . . , tm , (4) C either integer constant, variable
set variables occurring t1 , . . . , tm , c1 , . . . , ck (denoted var (d)), distinguished
variable time, (5) var (d) var {time}, (6) time occur 1 , . . . tm .
31

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

= 0, keyword requires omitted; k = 0, keyword omitted
costs C optional. Here, (1) (2) state parameters action must variables,
fixed values. Informally, (3) means parameters action must typed
requires part. Condition (4) asserts cost locally defined given stage
plan, referenced global variable time. Conditions (5) (6) ensure
variables known type information action parameters static, i.e., depend
time.
Planning domains planning problems K c defined K.
example, elaborated Bridge Crossing Problem, declaration cross(X)
extended follows: suppose predicate walk(Person, Minutes) background knowledge
indicates Person takes Minutes cross. Then, may simply declare
cross(X) requires person(X) costs WX walk(X, WX).

3.2 Semantics Kc
Semantically, Kc extends K cost values actions points time. plan P =
hA1 , . . . , Al i, step 1 l, actions Ai executed reach time point i.
ground action p(x1 , . . . , xn ) legal action instance action declaration wrt. K c
planning domain PD = h, hD, Rii, exists ground substitution var (d)
{time} Xi = xi , 1 n {t1 , . . . , tm } , unique answer
set background knowledge . called witness substitution p(x 1 , . . . , xn ).
Informally, action instance legal, satisfies respective typing requirements. Action costs
formalized follows.
Definition 3.2 Let = p(x1 , . . . , xn ) legal action instance declaration form (1)
let witness substitution a.

costs part empty;
0,
cost (p(x1 , . . . , xn )) =
val(C), {c1 , . . . , ck } ;

undefined otherwise.

unique answer set val : con defined integer value
integer constants 0 non-integer constants.
reference variable time, possible define time-dependent action costs; shall consider example Section 5.2. Using cost , introduce well-defined legal action instances
define action cost values follows.
Definition 3.3 legal action instance = p(x 1 , . . . , xn ) well-defined iff holds (i)
time point 1, witness substitution time = cost (a)
integer, (ii) cost (a) = cost0 (a) holds two witness substitutions , 0 coincide
time defined costs. well-defined a, unique cost time point 1 given
costi (a) = cost (a) (i).
definition, condition (i) ensures cost value exists, must integer,
condition (ii) ensures value unique, i.e., two different witness substitutions
0 evaluate cost part integer cost value.
32

fiA NSWER ET P LANNING U NDER ACTION C OSTS

action declaration well-defined, legal instances well-defined.
fulfilled if, database terms, variables X1 , . . . , Xn together time (1) functionally determine value C. framework, semantics K c planning domain PD = h, hD, Rii
well-defined well-defined action declarations PD. rest paper, assume
well-definedness Kc unless stated otherwise.
Using costi , define costs plans.
Definition 3.4 Let P = hPD, Q ? (l)i planning problem. Then, plan P = hA 1 , . . . , Al
P, cost defined
costP (P ) =

Pl

j=1

P


cost
(a)
.
j
aAj

plan P optimal P, costP (P ) costP (P 0 ) plan P 0 P, i.e., P least cost
among plans P. cost planning problem P, denoted cost P , given costP =
costP (P ), P optimal plan P.
particular, costP (P ) = 0 P = hi, i.e., plan void. Note cost P defined
plan P exists.3
Usually one estimate upper bound plan length, know exact
length optimal plan. Although defined optimality fixed plan length l,
see Section 5.1 appropriate encodings extended optimality plans
length l.
Besides optimal plans, also plans bounded costs interest, motivates following definition.
Definition 3.5 plan P planning problem P admissible wrt. cost c, cost P (P ) c.
Admissible plans impose weaker condition plan quality optimal plans.
particularly relevant optimal costs crucial issue, long cost stays within given
limit, optimal plans difficult compute. might face questions like make
airport within one hour?, enough change buy coffee? etc. amount
admissible planning problems. shall see, computing admissible plans complexity-wise
easier computing optimal plans.
3.3 Optimal Solution Quick Bridge Crossing Problem
model Quick Bridge Crossing Problem K c , first extend background knowledge
follows, predicate walk describes time person needs cross max determines
two persons slower:
walk(joe, 1). walk(jack, 2). walk(william, 5). walk(averell, 10).
max(A, B, A) :- walk( , A), walk( , B), >= B.
max(A, B, B) :- walk( , A), walk( , B), B > A.

Next, modify declarations cross crossTogether Figure 1 adding costs:
3. following, subscripts dropped clear context.

33

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

cross(X) requires person(X) costs WX walk(X, WX).
crossTogether(X, Y) requires person(X), person(Y), X <
costs Wmax walk(X, WX), walk(Y, WY), max(WX, WY, Wmax).

declaration takeLamp remains unchanged, time hand lamp negligible.
Using modified planning domain, 5-step plan reported Section 2.1 cost 19. Actually, optimal plan length l = 5. However, relinquish first intuition
fastest person, joe, always lamp consider problem varying plan length,
find following 7-step plan:
P = h {crossTogether(joe, jack)}, {cross(joe)}, {takeLamp(william)},
{crossTogether(william, averell)}, {takeLamp(jack)}, {cross(jack)},
{crossTogether(joe, jack)}

Here, costP (P ) = 17, thus P admissible respect cost 17. means Quick
Bridge Crossing Problem positive answer. fact, P least cost plans length
l = 7, thus optimal 7-step plan. Moreover, P also least cost plans emerge
consider plan lengths. Thus, P optimal solution Quick Bridge Crossing
Problem arbitrary plan length.
3.4 Bridge Crossing Incomplete Knowledge
language K well-suited model problems involve uncertainty incomplete
initial states non-deterministic action effects qualitative level. enriched language K c
gracefully extends secure (conformant) plans well, must reach goal circumstances (Eiter et al., 2000b, 2003b). precisely, optimistic plan hA 1 , . . . , secure,
applicable evolution system: starting legal initial state 0 , first
action set A1 (for plan length l 1) always executed (i.e., legal transition hs 0 , A1 , s1
exists), every possible state 1 , next action set A2 executed etc.,
performed actions, goal always accomplished (cf. Appendix formal definition).
secure plans inherit costs optimistic plans, different possibilities define
optimality secure plans. may consider secure plan optimal, least cost either
among optimistic plans,
among secure plans only.
first alternative, might planning problems secure plans, optimal
secure plans. reason, second alternative appears appropriate.
Definition 3.6 secure plan P optimal planning problem P, least cost among
secure plans P, i.e., costP (P ) costP (P 0 ) secure plan P 0 P. secure cost
P, denoted costsec (P), costsec (P) = costP (P ), P optimal secure plan P.
notion admissible secure plans defined analogously.
example, assume known least one person bridge scenario lamp,
neither exact number lamps allocation lamps persons known.
four desperate persons ask plan brings safely across bridge, need
(fast) secure plan works possible initial situations. K c , modeled
replacing initially-part following declarations:
34

fiA NSWER ET P LANNING U NDER ACTION C OSTS

initially : total hasLamp(X).
caused false -hasLamp(joe), -hasLamp(jack),
-hasLamp(william), -hasLamp(averell).

first statement says person either lamp not, second least
one must lamp. detailed discussion use total statement
modeling incomplete knowledge non-determinism refer (Eiter et al., 2003b).
easily see, optimal secure solution take least 17 minutes, since original
case (where joe lamp) one possible initial situations, cost
optimistic plan optimal plan lengths 17. However, secure plan
optimal plan lengths requires least 8 steps (but higher cost): Different
optimistic plans, need one extra step beginning makes sure one
walk first (above, joe jack) lamp, effected proper takeLamp action.
example plan following cost 17:
P = h {takeLamp(joe)}, {crossTogether(joe, jack)}, {cross(joe)},
{takeLamp(william)}, {crossTogether(william, averell)}, {takeLamp(jack)},
{cross(jack)}, {crossTogether(joe, jack)}

easily check P works every possible initial situation. Thus, optimal (secure)
plan plan length 8, moreover also arbitrary plan length.

4. Computational Complexity
section, address computational complexity K c , complementing similar results
language K (Eiter et al., 2003b).
4.1 Complexity Classes
assume reader familiar basic notions complexity theory, P, NP,
problem reductions completeness; see e.g. (Papadimitriou, 1994) references therein.
recall Polynomial Hierarchy (PH) contains classes P0 = P0 = P0 = P Pi+1 =
P
P
NPi , Pi+1 = co-Pi+1 , Pi+1 = Pi , 0. particular, P1 = NP P2 = PNP .
Note classes contain decision problems (i.e., problems answer yes
no). checking well-definedness deciding plan existence problems, computing
plan search problem, problem instance (possibly empty) finite set S(I)
solutions exists. solve problem, (possibly nondeterministic) algorithm must compute
alternative solutions set computation branches, S(I) empty. precisely,
search problems solved transducers, i.e., Turing machines equipped output tape.
machine halts accepting state, contents output tape result
computation. Observe nondeterministic machine computes (partial) multi-valued function.
analog NP, class NPMV contains search problems S(I) computed nondeterministic Turing machine polynomial time; precise definition, see (SelP
man, 1994). analogy Pi+1 , Pi+1 MV = NPMVi , 0, denote generalization
NPMV machine access Pi oracle.
Analogs classes P Pi+1 , 0, given classes FP F Pi+1 , 0,
contain partial single-valued functions (that is, |S(I)| 1 problem instance
35

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

I) computable polynomial time using resp. Pi oracle. say, abusing terminology,
search problem FP (resp. FPi+1 ), partial (single-valued) function f FP
(resp. f FPi+1 ) f (I) S(I) f (I) undefined iff S(I) = . example,
computing satisfying assignment propositional CNF (FSAT) computing optimal tour
Traveling Salesperson Problem (TSP) F P2 view, cf. (Papadimitriou, 1994).
partial function f polynomial-time reducible another partial function g,
polynomial-time computable functions h 1 h2 f (I) = h2 (I, g(h1 (I)))
g(h1 (I)) defined whenever f (I) defined. Hardness completeness defined usual.
4.2 Problem Setting
focus following questions:
Checking Well-Definedness: Decide whether given action description well-defined wrt.
given planning domain PD, resp. whether given planning domain PD well-defined.
Admissible Planning: Decide whether planning problem P admissible (optimistic/secure)
plan exists wrt. given cost value c, find plan.
Optimal Planning: Find optimal (optimistic/secure) plan given planning problem.
Notice (Eiter et al., 2003b) focused deciding existence optimistic/secure plans,
rather actually finding plans, presented detailed study complexity task
various restrictions ground (propositional) planning problems. paper, confine
discussion case planning problems P = hPD, Q ? (l)i look polynomial length
plans, i.e., problems plan length l bounded polynomial size input.
shall consider mainly ground (propositional) planning, assume planning
domains well-typed unique model background knowledge computed
polynomial time. general case, well-known complexity results logic programming,
cf. (Dantsin, Eiter, Gottlob, & Voronkov, 2001), already evaluating background knowledge
EXPTIME-hard, problems thus provably intractable. recall following results,
appear (or directly follow from) previous work (Eiter et al., 2003b).
Proposition 4.1 Deciding, given propositional planning problem P sequence P = hA 1 , . . . ,
Al action sets, (i) whether given sequence = ht 1 , . . . , tl legal trajectory witnessing
P optimistic plan P feasible polynomial time, (ii) whether P secure plan
P P2 -complete.
4.3 Results
start considering checking well-definedness. problem, interesting investigate
non-ground case, assuming background knowledge already evaluated. way
assess intrinsic difficulty task obtaining following result.
Theorem 4.2 (Complexity checking well-definedness) Given K c planning domain PD =
h, hD, Rii unique model , checking (i) well-definedness given action declaration form (1) wrt. PD (ii) well-definedness PD P2 -complete.
36

fiA NSWER ET P LANNING U NDER ACTION C OSTS

Proof. Membership: (i), violated nonempty costs part legal action
instance = p(x1 , . . . , xn ) either (1) exist witness substitutions 0
time = time 0 , cost (a) = val(C) cost0 (a) = val(C 0 ), val(C) 6= val(C 0 ),
(2) witness substitution cost (a) = val(C) integer.
guessed checked, via witness substitution, polynomial time, along
also 0 (1); note that, definition, variables must substituted constants
background knowledge (including numbers), must values time occurs
c1 , . . . , ck . Given a, decide (2) help NP oracle. summary, disproving welldefinedness nondeterministically possible polynomial time NP oracle. Hence,
checking well-definedness co-P2 = P2 . membership part (ii) follows (i),
since well-definedness PD reduces well-definedness action declarations it, P2
closed conjunctions.
Hardness: show hardness (i) reduction deciding whether quantified Boolean
formula (QBF)
Q = XY.c1 ck
ci = Li,1 Li,`i , = 1, . . . , k, disjunction literals L i,j atoms
X = x1 , . . . , xn = xn+1 . . . , xm , true. Without loss generality, may assume
ci contains three (not necessarily distinct) literals, either positive negative.
construct planning domain PD follows. background knowledge, ,
bool(0). bool(1).
pos(1, 0, 0). pos(0, 1, 0). pos(0, 0, 1). pos(1, 1, 0). pos(1, 0, 1). pos(0, 1, 1). pos(1, 1, 1).
neg(0, 0, 0). neg(1, 0, 0). neg(0, 1, 0). neg(0, 0, 1). neg(1, 1, 0). neg(1, 0, 1). neg(0, 1, 1).

Here, bool declares truth values 0 1. facts pos(X 1 , X2 , X3 ) neg(X1 , X2 , X3 ) state
truth assignments X1 , X2 , X3 positive clause X1 X2 X3 resp.
negative clause X1 X2 X3 satisfied.
rest planning domain PD consists single action declaration form
p(V1 , ..., Vn ) requires bool(V1), ..., bool(Vn) costs 0 c1 , ..., ck .


ci

=



pos(Vi,1 , Vi,2 , Vi,3 ), ci = xi,1 xi,2 xi,3 ,
neg(Vi,1 , Vi,2 , Vi,3 ), ci = xi,1 xi,2 xi,3 ,

= 1, . . . , k.

example, clause c = x1 x3 x6 mapped c = pos(V1 , V3 , V6 ). easy see
legal action instance = p(b1 , . . . , bn ) corresponds 1-1 truth assignment X
given (xi ) = bi , = 1, . . . , n. Furthermore, cost value defined (which 0) iff
formula (c1 ck ) true. Thus, well-defined wrt. PD iff Q true. Since PD
2
efficiently constructible, proves P2 -hardness.
Observe ground case, checking well-definedness much easier. Since substitutions need guessed, test proof Theorem 4.2 polynomial. Thus, assumption
efficient evaluation background program, obtain:
Corollary 4.3 ground (propositional) case, checking well-definedness action description wrt. Kc planning domain PD = h, hD, Rii, resp. PD whole, possible
polynomial time.
37

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

remark checking well-definedness expressed planning task K, also
logic program; refer (Eiter, Faber, Leone, Pfeifer, & Polleres, 2002b) details.
turn computing admissible plans.
Theorem 4.4 (Complexity admissible planning) polynomial plan lengths, deciding whether
given (well-defined) propositional planning problem hPD, qi (i) optimistic admissible
plan wrt. given integer b NP-complete, finding plan complete NPMV, (ii)
deciding whether hPD, qi secure admissible plan wrt. given integer b P3 -complete,
computing plan P3 MV-complete. Hardness holds cases fixed plan length.
proof refer Appendix. finally address complexity computing
optimal plans.
Theorem 4.5 (Complexity optimal planning) polynomial plan lengths, (i) computing
optimal optimistic plan hPD, Q ? (l)i K c FP2 -complete, (ii) computing optimal
secure plan hPD, Q ? (l)i K c FP4 -complete. Hardness holds cases even plan
length l fixed.
proof found Appendix.
remark case unbounded plan length, complexity computing plans increases requires (at least) exponential time general, since plans might exponential length
size planning problem. Thus, practical terms, constructing plans infeasible,
since occupy exponential space. Furthermore, follows previous results (Eiter et al.,
2003b), deciding existence admissible optimistic resp. secure plan planning problem wrt. given cost PSPACE-complete resp. NEXPTIME-complete. leave detailed
analysis complexity aspects K c work.

5. Applications
5.1 Cost Efficient versus Time Efficient Plans
section, show language K c used minimize plan length combination
minimizing costs plan. especially interesting problem settings parallel
actions allowed (cf. (Kautz & Walser, 1999; Lee & Lifschitz, 2001)).
domains parallel actions, Kautz Walser propose various criteria optimized, instance number actions needed, number necessary time steps
parallel actions allowed, well combinations two criteria (1999). exploiting
action costs proper modeling, solve optimization problems sort. example,
single plans minimal number actions simply assigning cost 1 possible
actions.
consider following optimization problems:
() Find plan minimal cost (cheapest plan) given number steps.
() Find plan minimal time steps (shortest plan).
() Find shortest among cheapest plans.
38

fiA NSWER ET P LANNING U NDER ACTION C OSTS

() Find cheapest among shortest plans.
Problem () already defined optimal plans far. show
express () terms optimal cost plans well, extend elaboration respect
combinations () ().
5.1.1 C HEAPEST P LANS



G IVEN P LAN L ENGTH ()

guiding example, refer Blocks World parallel moves allowed, apart
finding shortest plans also minimizing total number moves issue. Kc encoding
domain, plans serializable, shown Figure 2. Serializability means parallel
actions non-interfering executed sequentially order, i.e. parallel plan
arbitrarily unfolded sequential plan.
fluents :

on(B, L) requires block(B), location(L).
blocked(B) requires block(B).
moved(B) requires block(B).

actions :

move(B, L) requires block(B), location(L) costs 1.

always :

executable move(B, L) B != L.
nonexecutable move(B, L) blocked(B).
nonexecutable move(B, L) blocked(L).
nonexecutable move(B, L) move(B1, L), B < B1, block(L).
nonexecutable move(B, L) move(B, L1), L < L1.
nonexecutable move(B, B1) move(B1, L).
caused
caused
caused
caused

on(B, L) move(B, L).
blocked(B) on(B1, B).
moved(B) move(B, L).
on(B, L) moved(B) on(B, L).

Figure 2: Kc encoding Blocks World domain
planning problem emerging initial state goal state depicted Figure 3
modeled using background knowledge bw :
block(1). block(2). block(3). block(4). block(5). block(6).
location(table).
location(B) :- block(B).

extending program Figure 2 follows:
initially : on(1, 2). on(2, table). on(3, 4). on(4, table). on(5, 6). on(6, table).
goal :

on(1, 3), on(3, table), on(2, 4), on(4, table), on(6, 5), on(5, table) ?(l)

1
2

3
4

1
3

5
6

2
4

6
5

Figure 3: simple Blocks World instance
39

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

move penalized cost 1, results minimization total number moves.
Let Pl denote planning problem plan length l.
l = 2, optimal plan involves six moves, i.e. cost P2 = 6:
P2 = h {move(1, table), move(3, table), move(5, table)}, {move(1, 3), move(2, 4), move(6, 5)}

unfolding steps, plan gives rise similar plans length l = 3, . . . , 6 cost 6.
l = 3, find among others following optimal plan, cost 5:
P3 = h {move(3, table)}, {move(1, 3), move(5, table)}, {move(2, 4), move(6, 5)}

plan parallelized two steps. plan length l > 3,
obtain optimal plans similar P 3 , extended void steps. Thus plan cheapest
plan lengths cost 5 needs three steps. Note shortest parallel plans (of length 2)
expensive, explained above.
5.1.2 HORTEST P LANS ()
Intuitively, possible include minimization time steps cost function.
describe preprocessing method which, given K planning domain PD, list Q ground literals,
upper bound 0 plan length, generates planning problem P (PD, Q, i)
optimal plans P correspond shortest plans reach Q PD steps, i.e.,
plans hPD, Q ? (l)i l minimal. assume action costs specified
original planning domain PD, minimizing time steps target.
First rewrite planning domain PD PD follows: introduce new distinct fluent
gr new distinct action finish, defined follows:
fluents :
actions :

gr.
finish costs time.

Intuitively, action finish represents final action, use finish plan. later
action occurs, expensive plan assign time cost. fluent gr (goal
reached) shall true remain true soon goal reached, triggered
finish action.
modeled K c adding following statements always section
program:
executable finish Q, gr.
caused gr finish.
caused gr gr.

Furthermore, want finish occur exclusively want block occurrence
action goal reached. Therefore, every action PD, add
nonexecutable finish.

add gr if-part executability condition A. Finally, avoid inconsistencies static dynamic effects soon goal reached, add gr
part causation rule PD except nonexecutable rules remain unchanged. 4
define P (PD, Q, i) = hPD , gr ?(i + 1)i. take + 1 plan length since
need one additional step execute finish action.
4. need rewrite nonexecutable rules respective actions already switched
rewriting executability conditions.

40

fiA NSWER ET P LANNING U NDER ACTION C OSTS

construction, easy see optimal plan P = hA 1 , . . . , Aj , Aj+1 , . . . , Ai+1
planning problem P must Aj+1 = {finish} Aj+2 = . . . = Ai+1 =
j {0, . . . , i}. thus following desired property.
Proposition 5.1 optimal plans P 1-1 correspondence shortest plans reaching Q PD. precisely, P = hA1 , . . . , Aj+1 , , . . . , optimal optimistic plan
P (PD, Q, i) Aj+1 = {finish} P 0 = hA1 , . . . , Aj optimistic plan
hPD, Q ? (j)i j {0, . . . , i}, hPD, Q ? (j 0 )i optimistic plan j 0 < j.
Blocks World example, using method get 2-step plans, choose 2.
compute shortest plans plan lengths, set upper bound large enough
plans length l guaranteed exist. trivial bound total number legal
states general exponential number fluents.
However, many typical applications inherent, much smaller bound plan length.
instance, Blocks World n blocks, goal configuration reached within
2n sinit sgoal steps, sinit sgoal numbers stacks initial goal
state, respectively.5 Therefore, 6 upper bound plan length simple instance.
remark approach minimizing plan length efficient upper bound
close optimum known. Searching minimum length plan iteratively increasing
plan length may much efficient bound known, since weak upper bound
lead explosion search space (cf. benchmarks Section 7.2).
5.1.3 HORTEST

AMONG

C HEAPEST P LANS ()

previous subsection, shown calculate shortest plans K programs without
action costs. Combining arbitrary K c programs rewriting method described easy.
want find shortest among cheapest plans, use rewriting,
little change. setting costs actions except finish least high
highest possible cost finish action. obviously plan length + 1. So,
simply modify action declarations
requires B costs C D.

P multiplying costs factor + 1:
requires B costs C1 C1 = (i + 1) C, D.

lets action costs take priority cost finish compute plans
satisfying criterion (). Let P denote resultant planning problem. have:
Proposition 5.2 optimal plans P 1-1 correspondence shortest among
cheapest plans reaching Q PD within steps. precisely, P = hA 1 , . . . , Aj+1 , , . . . ,
optimal optimistic plan P (PD, Q, i) Aj+1 = {finish} (i) P 0 =
hA1 , . . . , Aj plan Pj = hPD, Q ? (j)i, j {0, . . . , i}, (ii) P 00 = hA1 , . . . , Aj 0
plan Pj 0 = hPD, Q ? (j 0 )i j 0 i, either costPj 0 (P 00 ) > costPj (P 0 )
costPj 0 (P 00 ) = costPj (P 0 ) j 0 j.
Figure 4 shows P Blocks World instance = 6. One optimal plan P
5. One solve Blocks World problem sequentially first unstacking blocks table
(n sinit steps) building goal configuration (n sgoal steps).

41

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

fluents :

on(B, L) requires block(B), location(L).
blocked(B) requires block(B).
moved(B) requires block(B).
gr.

actions :

move(B, L) requires block(B), location(L) costs C C = 7 1.
finish costs time.

always :

executable move(B, L) B != L, gr.
nonexecutable move(B, L) blocked(B).
nonexecutable move(B, L) blocked(L).
nonexecutable move(B, L) move(B1, L), B < B1, block(L).
nonexecutable move(B, L) move(B, L1), L < L1.
nonexecutable move(B, B1) move(B1, L).
caused
caused
caused
caused

on(B, L) gr move(B, L).
blocked(B) on(B1, B), gr.
moved(B) gr move(B, L).
on(B, L) moved(B), gr on(B, L).

executable finish on(1, 3), on(3, table), on(2, 4), on(4, table),
on(6, 5), on(5, table), gr.
caused gr finish.
caused gr gr.
nonexecutable move(B, L) finish.
initially : on(1, 2). on(2, table). on(3, 4). on(4, table). on(5, 6). on(6, table).
goal :

gr? (7)

Figure 4: Computing shortest plan Blocks World instance minimum number
actions

P = h {move(3, table)}, {move(1, 3), move(5, table)},
{move(2, 4), move(6, 5)}, {finish}, , , i,

costP (P ) = 39. compute optimal cost wrt. optimization () subtracting cost finish dividing + 1: (39 4) (i + 1) = 35 7 = 5. Thus,
need minimum 5 moves reach goal. minimal number steps obviously steps,
except final finish action, i.e. 3. Thus, need least 3 steps plan five moves.
5.1.4 C HEAPEST

AMONG

HORTEST P LANS ()

Again, use rewriting optimization (). cost functions adapted similarly
previous subsection, cost action finish takes priority
actions costs. end, sufficient set cost finish high enough, achieved
multiplying factor F higher sum action costs legal action instances
steps j = 1, . . . , + 1. Let P denote resulting planning problem. have:
Proposition 5.3 optimal plans P 1-1 correspondence cheapest among
shortest plans reaching Q PD within steps. precisely, P = hA 1 , . . . , Aj+1 , , . . . ,
42

fiA NSWER ET P LANNING U NDER ACTION C OSTS

optimal optimistic plan P (PD, Q, i) Aj+1 = {finish} (i) P 0 =
hA1 , . . . , Aj plan Pj = hPD, Q ? (j)i, j {0, . . . , i}, (ii) P 00 = hA1 , . . . , Aj 0
plan Pj 0 = hPD, Q ? (j 0 )i j 0 i, either j 0 > j, j 0 = j costPj 0 (P 00 )
costPj (P 0 ).
example, 36 possible moves. Thus, could take F = 36 (i + 1)
would set costs finish time 36 (i + 1). However, need take account
actions actually occur simultaneously. example, six blocks
moved parallel. Therefore, sufficient set F = 6 (i + 1) assign finish cost
time F = time 42. Accordingly, action declarations modified follows:
actions :

move(B, L) requires block(B), location(L) costs 1.
finish costs C C = time 42.

optimal plan modified planning problem P is:
P = h {move(1, table), move(3, table), move(5, table)},
{move(1, 3), move(2, 4), move(6, 5)}, {finish}, , , ,

costP (P ) = 132. Here, compute optimal cost wrt. optimization () simply
subtracting cost finish, i.e. 132 3 42 = 6, since finish occurs time point 3.
Consequently, need minimum 6 moves shortest plan, length 3 1 = 2.
indeed, seen (and how) optimization problems () ()
represented Kc . remark transformations P , P , P work restrictions
secure and/or sequential plans well.
5.2 Traveling Salesperson
another illustrating example optimal cost planning, introduce elaboration
Traveling Salesperson Problem.
Traveling Salesperson Problem (TSP). start classical Traveling Salesperson Problem (TSP), given set cities connections (e.g., roads, airways) certain costs.
want know economical round trip visits cities exactly returns
starting point (if tour exists). Figure 5 shows instance representing capitals
Austrian provinces. dashed line flight connection, connections roads;
connection marked costs traveling hours.
brg ... Bregenz
eis ... Eisenstadt
gra ... Graz
ibk ... Innsbruck
kla ... Klagenfurt
lin ... Linz
sbg ... Salzburg
stp ... St. Plten
vie ... Vienna

lin
sbg

2

2

ibk

2

5

vie
1

2
3
2

2

2
kla

Figure 5: TSP Austria

eis

2

3

43

2
stp 1

1

1
brg

1

gra

1

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

represent Kc follows. background knowledge SP defines two predicates
city(C) conn(F, T, C) representing cities connections associated costs. Connections traveled ways:
conn(brg, ibk, 2). conn(ibk, sbg, 2). conn(ibk, vie, 5). conn(ibk, kla, 3).
conn(sbg, kla, 2). conn(sbg, gra, 2). conn(sbg, lin, 1). conn(sbg, vie, 3).
conn(kla, gra, 2). conn(lin, stp, 1). conn(lin, vie, 2). conn(lin, gra, 2).
conn(gra, vie, 2). conn(gra, eis, 1). conn(stp, vie, 1). conn(eis, vie, 1).
conn(stp, eis, 2). conn(vie, brg, 1).
conn(B, A, C) :- conn(A, B, C).
city(T) :- conn(T, , ).

possible encoding TSP starting Vienna (vie) K c program Figure 6. includes two
actions traveling one city another directly returning starting point
end round trip soon cities visited.
actions :

travel(X, Y) requires conn(X, Y, C) costs C.
return from(X) requires conn(X, vie, C) costs C.

fluents :

unvisited. end.
in(C) requires city(C).
visited(C) requires city(C).

always :

executable travel(X, Y) in(X).
nonexecutable travel(X, Y) visited(Y).
executable return from(X) in(X).
nonexecutable return from(X) unvisited.
caused unvisited city(C), visited(C).
caused end return from(X).
caused in(Y) travel(X, Y).
caused visited(C) in(C).
inertial visited(C).

noConcurrency.
initially : in(vie).
goal :
end? (9)

Figure 6: Traveling Salesperson
problem ten optimal 9-step solutions cost 15. show first five here,
others symmetrical:
P1 = h {travel(vie, stp)},
{travel(lin, sbg)},
{return from(brg)}
P2 = h {travel(vie, eis)},
{travel(sbg, gra)},
{return from(brg)}
P3 = h {travel(vie, eis)},
{travel(gra, kla)},
{return from(brg)}
P4 = h {travel(vie, lin)},
{travel(gra, kla)},

{travel(stp, eis)},
{travel(sbg, kla)},

{travel(eis, stp)},
{travel(gra, kla)},

{travel(eis, stp)},
{travel(kla, sbg)},

{travel(lin, stp)},
{travel(kla, sbg)},

{travel(eis, gra)}, {travel(gra, lin)},
{travel(kla, ibk)}, {travel(ibk, brg)},
{travel(stp, lin)}, {travel(lin, sbg)},
{travel(kla, ibk)}, {travel(ibk, brg)},
{travel(stp, lin)}, {travel(lin, gra)},
{travel(sbg, ibk)}, {travel(ibk, brg)},
{travel(stp, eis)}, {travel(eis, gra)},
{travel(sbg, ibk)}, {travel(ibk, brg)},

44

fiA NSWER ET P LANNING U NDER ACTION C OSTS

{return from(brg)}
P5 = h {travel(vie, gra)},
{travel(lin, sbg)},
{return from(brg)}


{travel(gra, eis)}, {travel(eis, stp)}, {travel(stp, lin)},
{travel(sbg, kla)}, {travel(kla, ibk)}, {travel(ibk, brg)},


TSP variable costs. Let us consider elaboration TSP, assume
costs traveling different connections may change trip. Note three
five solutions example include traveling St.Polten Eisenstadt vice versa
second day. Let us assume salesperson, starts Monday, face
exceptions might increase cost trip. instance, (i) heavy traffic jams expected
Tuesdays route St.Polten Eisenstadt (ii) salesperson shall use flight
connection Vienna Bregenz Mondays expensive business class tickets
available connection beginning week. deal different costs
respective connections depending particular day.
end, first add background knowledge SP new predicate cost(A, B, W, C)
representing cost C traveling connection B weekday W take exceptional
costs account:
cost(A, B, W, C) :- conn(A, B, C), #int(W), 0 < W, W <= 7, ecost(A, B, W).
ecost(A, B, W) :- conn(A, B, C), cost(A, B, W, C1), C != C1.

original costs predicate conn(A, B, C) represent defaults, overridden
explicitly adding different costs. instance, represent exceptions (i) (ii), add:
cost(stp, eis, 2, 10). cost(vie, brg, 1, 10).

setting exceptional costs two critical connections 10. Weekdays coded integers
1 (Monday) 7 (Sunday). represent mapping time steps weekdays
following rules also add SP :
weekday(1, 1).
weekday(D, W) :- = D1 + 1, W = W1 + 1, weekday(D1, W1), W1 < 7.
weekday(D, 1) :- = D1 + 1, weekday(D1, 7).

Note although modified background knowledge SP stratified (since cost defined
cyclic negation), total well-founded model, thus unique answer set.
Finally, change costs traveling returning K c program Figure 6:
actions :

travel(X, Y) requires conn(X, Y, C1) costs C
weekday(time, W), cost(X, Y, W, C).
return from(X) requires conn(X, vie, C1) costs C
weekday(time, W), cost(X, vie, W, C).

Since costs P1 (which includes traveling St.Polten Eisenstadt) second
day increased due exception (i), four plans remain optimal. Note
unlike default costs, exceptional costs apply bidirectionally, exception
affect P2 P3 . Furthermore, due exception (ii) symmetrical round trips starting
flight trips Bregenz longer optimal.
presented encoding proves flexible, allows adding arbitrary exceptions
connection weekday simply adding respective facts; moreover, even
involved scenarios, exceptions defined rules, modeled.
45

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

5.3 Small Example Planning Resource Restrictions
Although planning resources main target approach, following encoding
shows action costs also used order model optimization resource consumption
cases. important resource real world planning money. instance, let us consider
problem buying selling (Lee & Lifschitz, 2001):
$6 pocket. newspaper costs $1 magazine costs $3.
enough money buy one newspaper two magazines?
Kc , encoded compact way following background facts:
item(newspaper, 1). item(magazine, 2).

combined following short K c program:
actions :

buy(Item, Number) requires item(Item, Price), #int(Number)
costs C C = Number Price.

fluents :

have(Item, Number) requires item(Item, Price), #int(Number).

always :

executable buy(Item, Number).
nonexecutable buy(Item, N1) buy(Item, N2), N1 < N2.
caused have(Item, Number) buy(Item, Number).

goal :

have(newspaper, 1), have(magazines, 2) ? (1)

action buy always executable, one must buy two different amounts certain
item once. Obviously, admissible plan wrt. cost 6 exists, optimal plan problem,
h{buy(newspaper, 1), buy(magazine, 2)} cost P = 7. Therefore, answer problem
no.
approach considers positive action costs directly allow modeling full
consumer/producer/provider relations resources general, favor clear non-ambiguous
definition optimality. instance, allowing negative costs one could always add producer
action make existing plan cheaper, whereas approach costs guaranteed increase
monotonically, allowing clear definition plan costs optimality.
hand, encode various kinds resource restrictions using fluents represent resources. model production/consumption action effects fluents
add restrictions constraints. allows us model even complex resource scheduling
problems; optimization, however, remains restricted action costs.

6. Transformation Logic Programming
section, describe planning action costs implemented means
transformation answer set programming. extends previous transformation (Eiter et al.,
2003a), maps ordinary K planning problems disjunctive logic programs answer
set semantics (Gelfond & Lifschitz, 1991), takes advantage weak constraints, cf. (Buccafurri,
Leone, & Rullo, 1997, 2000), implemented DLV system (Faber & Pfeifer, 1996; Eiter,
Faber, Leone, & Pfeifer, 2000a). addition, show translation adapted
language Smodels (Simons, Niemela, & Soininen, 2002).
6.1 Disjunctive Logic Programs Weak Constraints
First, give brief review disjunctive logic programs weak constraints.
46

fiA NSWER ET P LANNING U NDER ACTION C OSTS

Syntax

disjunctive rule (for short, rule) R construct
a1 v v :- b1 , , bk , bk+1 , , bm .

(2)

ai bj classical literals function-free first-order alphabet, n 0,
k 0. part left (resp. right) :- head (resp. body) R, :- omitted
= 0. let H(R) = {a1 , . . ., } set head literals B(R) = B + (R) B (R)
set body literals, B + (R) = {b1 ,. . . , bk } B (R) = {bk+1 , . . . , bm }. (strong)
constraint rule empty head (n = 0).
weak constraint construct
: b1 , , bk , bk+1 , , bm . [w :]

(3)

w integer constant variable occurring b 1 , . . . , bk bi classical literals.6
B(R) defined (2).
disjunctive logic program (DLPw ) (simply, program) finite set rules, constraints
weak constraints; here, superscript w indicates potential presence weak constraints.
Semantics answer sets program without weak constraints defined usual (Gelfond & Lifschitz, 1991; Lifschitz, 1996). one difference, though: consider
inconsistent answer sets. answer sets program weak constraints defined
selection answer sets weak-constraint free part 0 optimal answer sets.
weak constraint c form (3) violated, instance conjunction
satisfied respect candidate answer set S, i.e., exists substitution mapping
variables c Herbrand base {b 1 , , bk } {bk+1 , , bm }
= ; call w violation value c wrt. . 7 violation cost c wrt. S, denoted
costc (S), sum violation values violating substitutions c wrt. S; cost
S, denoted cost (S),
X
cost (S) =
costc (S),
c weak constraints

i.e., sum violation costs weak constraints wrt. S. answer set
selected (called optimal answer set), cost (M ) minimal answer sets .
(Buccafurri et al., 2000) know given head-cycle-free disjunctive program, deciding whether query q true optimal answer set P2 -complete. respective class
computing answer set FP2 -complete. Together results Section 4 indicates translations optimal planning problems head-cycle-free disjunctive logic programs
weak constraints language Smodels feasible polynomial time.
6.2 Translating Kc DLPw
extend original transformation lp(P), naturally maps K planning problem P
weak-constraint free program (Eiter et al., 2003a), new translation lp w (P), optimal
answer sets lpw (P) correspond optimal cost plans K c planning problem P.
6. colon [w :] stems DLV language, allows specify priority layer colon.
need priority layers translation, stick DLV syntax.
7. weak constraint c admissible, possible violation values candidate answer sets integers.
Thus, w variable, must guarantee w bound integer.

47

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Basically, lp(P) fluent action literals extended additional time parameter,
executability conditions well causations rules modularly translated (rule rule) corresponding program rules constraints; disjunction used guessing actions
executed plan point time.
6.2.1 R EVIEW



RANSLATION lp(P)

basic steps translation K programs logic programs follows (cf. (Eiter et al.,
2003a) details):
Step 0 (Macro Expansion):

First, replace macros K program definitions.

Step 1 (Background Knowledge): background knowledge P already given logic
program included lp(P), without modification.
Step 2 (Auxiliary Predicates):

represent steps, add following facts lp(P)

time(0)., . . . , time(l). next(0, 1)., . . . , next(l 1, l).

l plan length query q = G?(l) P hand.
Step 3 (Causation Rules): Causation rules mapped rules lp(P) adding type information extending fluents actions time stamp using time next. example,
caused across(X) cross(X), -across(X).

leads rule across(X, T1) :- cross(X, T0), -across(X, T0), person(X), next(T0, T1 ).
lp(P) T1 , T0 new variables. Here, type information person(X) across(X),
-across(X), taken type declaration, added, helps avoid unsafe logic programming rules.
Step 4 (Executability Conditions): Similarly, executability condition translated disjunctive rule guessing whether action occurs certain time step. running example,
executable cross(X) hasLamp(X).

becomes cross(X, T0) -cross(X, T0) :- hasLamp(X, T0), person(X), next(T0, T1 ).
encodes guess whether time point 0 action cross(X) happen; again, type information person(X) added well next(T 0 , T1 ) ensure T0 last time point.
Step 5 (Initial State Constraints): Initial state constraints transformed like static causation
rules Step 3, using constant 0 instead variable 1 thus need auxiliary predicate time stamp. instance,
initially : caused -across(X).

becomes, adding type information -across(X, 0) :- person(X).
Step 6 (Goal Query):
goal :

Finally, query q:

g1 (t1 ), . . . , gm (tm ), gm+1 (tm+1 ), . . . , gn (tn ) ? (l).

translated follows, goal reached new 0-ary predicate symbol:
goal reached :- g1 (t1 , l), . . . , gm (tm , l), gm+1 (tm+1 , l), . . . , gn (tn , l).
:- goal reached.
48

fiA NSWER ET P LANNING U NDER ACTION C OSTS

6.2.2 E XTENDING



RANSLATION



ACTION C OSTS

extended translation lpw (P) Kc problem P first includes rules lp(Pnc ), Pnc
results P stripping cost parts. Furthermore, following step added:
Step 7 (Action Costs):

action declaration form (1) nonempty costs-part, add:

(i) new rule rd form

costp (X1 , . . . , Xn , T, C) :- p(X1 , . . . , Xn , T), t1 , . . . , tm ,
c1 , . . . , ck , U = + 1.

(4)

costp new symbol, U new variables = {time U}. optimization,
U = + 1 present U occurs elsewhere r .
: costp (X1 , . . . , Xn , T, C). [C :]

(ii) weak constraint wcd form

(5)

example, cross action Quick Bridge Crossing Problem translated
costcross(X, T, WX):- cross(X, T), person(X), walk(X, WX).
: costcross(X, T, WX). [WX :]

showed previous work (Eiter et al., 2003a), answer sets lp(P) correspond
trajectories optimistic plans P. following theorem states similar correspondence result
lpw (P) optimal plans P. define, consistent set ground literals S, sets
ASj = {a(t) | a(t, j 1) S, act } sSj = {f (t) | f (t, j) S, f (t) Lf l }, j 0.
Theorem 6.1 (Answer Set Correspondence) Let P = hPD, qi (well-defined) K c planning
problem, let lpw (P) program. Then,
(i) optimistic plan P = hA1 , . . . , Al P supporting trajectory = hhs 0 , A1 , s1 i,
hs1 , A2 , s2 i, . . . , hsl1 , Al , sl ii P , exists answer set lp w (P)
Aj = ASj j = 1, . . . , l, sj = sSj , j = 0, . . . , l costP (P ) = costlpw (P) (S);
(ii) answer set lpw (P), sequence P = hA1 , . . . , Al solution P, i.e.,
optimistic plan, witnessed trajectory = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . . , hsl1 , Al , sl ii
costP (P ) = costlpw (P) (S), Aj = ASj sk = sSk j = 1, . . . , l
k = 0, . . . , l.
proof based resp. correspondence result K (Eiter et al., 2003a). details,
refer Appendix.
result definitions optimal cost plans optimal answer sets, conclude
following result:
Corollary 6.2 (Optimal answer set correspondence) well-defined K c planning problem
P = hPD, Q ? (l)i, trajectories = hhs 0 , A1 , s1 i, . . . , hsl1 , Al , sl ii optimal plans P P
correspond optimal answer sets lp w (P), Aj = ASj j = 1, . . . , l
sj = sSj , j = 0, . . . , l.
Proof. Aj , weak constraint (5) causes violation value cost j (a). Furthermore,
P onlyPcost violations. Thus, candidate answer set optimal
costlpw (P) (S) = lj=1 aAj costj (a) = costP (P ) minimal, i.e., corresponds optimal
plan.
2
similar correspondence result also holds admissible plans:
49

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Corollary 6.3 (Answer set correspondence admissible plans) well-defined K c planning problem P = hPD, Q ? (l)i, trajectories = hhs 0 , A1 , s1 i, . . . , hsl1 , Al , sl ii admissible plans P P wrt. cost c correspond answer sets lp w (P) costlpw (P) (S) c,
Aj = ASj j = 1, . . . , l sj = sSj , j = 0, . . . , l.
secure planning, introduced technique check security optimistic plan
certain planning problem instances means logic program (Eiter et al., 2003a).
method carries planning action costs straightforward way, optimal resp. admissible secure plans similarly computed answer set programming.
6.3 Alternative Translation Smodels
Apart presented translation using weak constraints, one could also choose alternative
approach translation answer set programming. Smodels (Simons et al., 2002) supports
another extension pure answer set programming allowing minimize sets predicates.
approach could used alternative formulation Step 7:
Step 7a:

action declarations nonempty costs-parts, add new rule form
cost(p, X1 , . . . , Xn , 0, . . . , 0, T, C) :- t1 , . . . , tm , c1 , . . . , ck , U = + 1.

(6)

similar Step 7 above, two differences: (1) action name p parameter, (2) add
l n parameters constant 0 X n l maximum arity actions
PD. necessary order get unique arity l + 2 predicate cost. Furthermore, add
occurs(p, X1 , . . . , Xn , 0, . . . , 0, T) :- p(X1 , . . . , Xn , T), t1 , . . . , tm ,.

(7)

second rule adds 0 parameters achieve unique arity l + 1 new
predicate occurs. Using Smodels syntax, compute optimal plans adding
minimize[occurs(A, X1, ..., Xl , T) : cost(A, X1, ..., Xl , T, C) = C].

Note Smodels support disjunction rule heads, also need modify Step 4,
expressing action guess via unstratified negation Smodels choice rules.

7. Implementation
implemented experimental prototype system, DLV K , solving K planning problems (Eiter et al., 2003a). improved version prototype capable optimal
admissible planning respect extended syntax K c , available experiments
http://www.dlvsystem.com/K/ .
DLVK realized frontend DLV system (Faber & Pfeifer, 1996; Eiter et al.,
2000a). First, planning problem hand transformed described previous section.
Then, DLV kernel invoked produce answer sets. optimistic planning (optimal,
action costs defined) answer sets simply translated back suitable output user
printed.
case user specified secure/conformant planning performed, system
check security plans computed. normal (non-optimal) planning, simply done
checking answer set returned right transforming back user output. case
50

fiA NSWER ET P LANNING U NDER ACTION C OSTS

optimal secure planning, hand, candidate answer set generation DLV kernel
intercepted: kernel proceeds computing candidate answer sets, returning answer
set minimal violation cost value, running candidates. Here, order generate
optimal secure plans, planning frontend interrupts computation, allowing answer sets
represent secure plans considered candidates.
Checking plan security done rewriting translated program wrt. candidate answer
set/plan order verify whether plan secure. rewritten check program tested
separate invocation DLV kernel. details system architecture refer
(Eiter et al., 2003a)
7.1 Usage
Suppose background knowledge program depicted Figure 1 cost extensions
Section 3.3 stored files crossing.bk crossing.plan; then, invoking
program command line
dlv FPcrossing.plancrossing.bk planlength = 7

compute optimal plans solving problem seven steps. output find,
supporting trajectory, following optimal plan:
PLAN : crossTogether(joe, jack) : 2; cross(joe) : 1; takeLamp(william);
crossTogether(william, averell) : 10; takeLamp(jack);
cross(jack) : 2; crossTogether(joe, jack) : 2 COST : 17

action, cost shown colon, non-zero. switch -planlength=i
used set plan length; overrides plan length given query-part planing
problem. Using -planlength=5, get plans cost 19, cheaper plans
length.
user asked whether perform optional security check whether look
(optimal) plans, respectively. switch -FPsec used instead -FP obtain
secure plans only.
command line option -costbound=N effects computation admissible plans
respect cost N . example, resource problem described Section 5.3 solved
following call prototype:
dlv FPbuying.bkbuying.plan N = 10 planlength = 1 costbound = 6

Correctly, admissible plan found. calling system without cost bound,
prototype calculates following optimal cost plan:
PLAN : buy(newspaper, 1) : 1, buy(magazine, 2) : 6

COST : 7

current prototype supports simple bounded integer arithmetics. option -N=10 used
sets upper bound N = 10 integers may used program; builtin predicate #int true integers 0 . . . N . Setting N high enough, taking account
outcome built-in arithmetic predicates = B + C = B C, important get
correct results. details prototype given DLV K web site http://www.
dlvsystem.com/K/.
51

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

7.2 Experiments
Performance experimental results DLV K (without action costs optimal planning)
reported previous work (Eiter et al., 2003a). section, present encouraging experimental results planning action costs, particular parallel Blocks World TSP.
experiments performed Pentium III 733MHz machine 256MB main memory running SuSE Linux 7.2. set time limit 4000 seconds tested instance exceeding
limit indicated - result tables.
possible, also report results CCALC CMBP, two logic-based planning
systems whose input languages (C+ resp. AR) capabilities similar K resp. K c .
CCALC. Causal Calculator (CCALC) model checker languages causal theories
(McCain & Turner, 1997). translates programs action language C+ language
causal theories turn transformed SAT problems; solved using SAT
solver (McCain & Turner, 1998). current version CCALC uses mChaff (Moskewicz et al.,
2001) default SAT solver. Minimal length plans generated iteratively increasing plan
length upper bound. CCALC written Prolog. tests, used version 2.04b
CCALC obtained <URL:http://www.cs.utexas.edu/users/tag/cc/
> trial version SICStus Prolog 3.9.1. used encodings taken (Lee & Lifschitz,
2001) parallel Blocks World adapted CCALC 2.0. encodings included
current download version system. sequential Blocks World adapted encodings
adding C+ command noConcurrency. resembles respective K command.
results CCALC include 2.30sec startup time.
CMBP. Conformant Model Based Planner (CMBP) (Cimatti & Roveri, 2000) based
model checking paradigm exploits symbolic Boolean function representation techniques
Binary Decision Diagrams (Bryant, 1986). CMBP allows computing sequential minimal
length plans, user declare upper bound plan length. input language
extension AR (Giunchiglia, Kartha, & Lifschitz, 1997). Unlike K action languages
C+ (Lee & Lifschitz, 2001), language supports propositional actions. CMBP
tailored conformant planning. results reported complement previous comparison
also shows encoding sequential Blocks World CMBP (Eiter et al., 2003a). tests,
used CMBP 1.0, available <URL:http://sra.itc.it/people/roveri/cmbp/>.
7.2.1 B LOCKS W ORLD
Tables 14 show results different Blocks World encodings Section 5.1 several
configurations: P0 denotes simple instance Figure 3, P1P5 instances used
previous work (Eiter et al., 2003a; Erdem, 1999).
Table 1 shows results finding shortest sequential plan. second third column
show number blocks length shortest plan (i.e., least number moves) solving
respective blocks world instance. execution time solving problem using shortestplan encoding P Section 5.1 shown column five, using upper bound shown fourth
column plan length. Column six shows execution time finding shortest plan
incremental plan length search starting 0, similar method used CCALC.
remaining two columns show results CCALC CMBP.
52

fiA NSWER ET P LANNING U NDER ACTION C OSTS

Problem
P0
P1
P2
P3
P4
P5

#blocks
6
4
5
8
11
11

min. #moves (=#steps)
5
4
6
8
9
11

upper bound #steps
6
4
7
10
16
16

DLVK

DLVK
inc

0.48s
0.05s
0.24s
25.32s
-

0.29s
0.08s
0.27s
2.33s
8.28s
12.63s

CCALC
4.65s
3.02s
4.02s
10.07s
27.19s
32.27s

CMBP
21.45s
0.13s
8.44s
-

Table 1: Sequential Blocks World - shortest plans

Problem
P0
P0
P1
P2
P3
P4
P5

#blocks
6
6
4
5
8
11
11

#steps(fixed)
2
3
3
5
4
5
7

min. #moves
6
5
4
6
9
13
15

DLVK
0.05s
0.09s
0.04s
0.10s
0.21s
0.81s
327s

Table 2: Parallel Blocks World - cheapest plans: Minimal number moves fixed plan length ()

Table 2 shows execution times parallel blocks world fixed plan length
number moves minimized, i.e. problem () Section 5.1. used encoding Figure 2,
generates parallel serializable plans. CCALC CMBP allow optimizing
criteria plan length, results DLV K here.
Next, Table 3 shows results finding shortest parallel plan, i.e. problem () Section 5.1. First, minimal possible number steps given. processed instance (i) using
encoding P Section 5.1, (ii) without costs iteratively increasing plan length
(iii) using CCALC, iteratively increasing plan length plan found. every result,
number moves first plan computed reported separately. CMBP supports
sequential planning, included comparison.
Finally, Table 4 shows results combined optimizations () () parallel Blocks
World outlined Section 5.1. second column contains upper bound plan

upper bound
P0
P1
P2
P3
P4
P5

6
4
7
10
16
16

min. #steps
2
3
5
4
5
7

DLVK
#moves
6
5
9
-

time
0.52s
0.07s
0.39s
-

DLVK
inc
#moves
6
5
9
12
18
26

time
0.09s
0.08s
0.21s
0.43s
1.54s
3.45s

Table 3: Parallel Blocks World - shortest plan ()

53

CCALC
#moves
time
6
4.05s
4
2.95s
6
3.70s
9
7.69s
13
20.45s
15
23.22s

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

()
P0
P1
P2
P3
P4
P5

upper bound
6
4
7
10
16
16

steps/moves
3/5
3/4
5/6
5/8
9/9
11/11

()

DLVK

DLVK
inc

38.5s
0.07s
2.08s
-

0.18s
0.11s
0.21s
1.57s
-

CCALC
5.89s
3.47s
5.65s
15.73s
73.64s
167s

steps/moves
2/6
3/4
5/6
4/9
5/13
7/15

DLVK

DLVK
inc

0.26s
0.08s
0.78s
177s
-

0.09s
0.08s
0.28s
0.45s
1.86s
323s

Table 4: Parallel Blocks World - (),()

length respective instance. following three columns present results finding
shortest among cheapest plans, i.e. problem () Section 5.1:
DLVK refers results combined minimal encoding P described Section 5.1;
DLVK
inc refers results incrementally searching shortest among cheapest plans:

done means -costbound=i command line option taking minimal
sequential costs (i.e., shortest sequential plan length computed Table 1) upper
cost limit. encodings compute serializable plans, minimal sequential length
used cost limit special case.
CCALC similar technique used CCALC encoding bound costs additive fluents (Lee & Lifschitz, 2001).
Note incremental strategy (used DLV K
inc CCALC) takes advantage specific formulation parallel Blocks World problem: general, allowing parallel actions
necessarily serializable arbitrary costs, optimal parallel cost might differ
optimal sequential solution. particular, plans longer cheapest sequential plans (which, example, coincide shortest sequential plans) may need
considered. makes incremental search solution problem () infeasible general.
last test finding cheapest among shortest plans, is, problem () Section 5.1.
tested integrated encoding upper bound (P ) resp. incrementally finding
shortest plan. Unlike problem (), cannot derive fixed cost limit sequential
solution here; really need optimize costs, makes encoding CCALC infeasible.
Blocks World Results Blocks World experiments show DLV K solve various optimization tasks effective flexible way systems compared. hand,
already stated above, minimal plan length encodings Section 5.1, solve
problems tight upper bound plan length known. Iteratively increasing plan
length effective, especially upper bound much higher actual optimal solution. becomes drastically apparent execution times seem explode one instance
next, highly non-linear manner Table 1 solution P3 found
reasonable time whereas P4 P5 could solved within time limit 4000 seconds.
observation also confirmed tables (instance P5 Table 2, etc.) partly explained
behavior underlying DLV system, geared towards plan search,
general purpose problem solver uses heuristics might work well cases.
particular, answer set generation process DLV, distinction made actions
54

fiA NSWER ET P LANNING U NDER ACTION C OSTS

fluents, might useful planning tasks control generation answer sets resp.
plans; may part investigations.
Interestingly, CCALC finds better quality parallel solutions problem () (cf. Table 3), i.e.
solutions fewer moves, although significantly slower system instances.
incremental encoding problem (), CCALC seems even effective system.
However, CCALC offers means optimization; allows admissible optimal
planning. makes approach flexible general. stated above, could fortunately
exploit fixed cost bound particular example CCALC, possible general
instances problem ().
Problem () also intuitively harder simply finding shortest plan cheapest among
shortest plans general: problems always solved incrementally, ()
must consider plans lengths. longer plan may cheaper, cannot freeze plan
length (shortest) plan incrementally found.
7.2.2 TSP
experimental results TSP variable costs reported Tables 5 6. Unlike
blocks world, comparable systems available; none systems supports cost
optimal planning needed solving problem. Here, plan length always given
number cities.
Table 5 shows results TSP instance Austrian province capitals Figure 5
(nine cities, 18 connections), without exceptional costs Section 5.2 (with without subscript exc table). instances reported table different cost exceptions
(we, lwe, rnd) described below.
Results bigger TSP instances, given capitals 15 members European
Union (EU) varying connection graphs exceptional costs shown Table 6.
used flight distances (km) cities connection costs. Instances TSP EU 1TSPEU 6
generated randomly choosing given number connections possible connections 15 cities. Note TSP EU 1 solution; time reported
DLVK terminated, instances first optimal plan found.
also tested instances practical relevance simply randomly choosing
connections: TSPEU 7 instance taken flight connections three carriers
(namely, Star Alliance, Alitalia, Luxair), TSP EU 8 included direct connections 1500km. capital hopping interest small airplane limited
range, instance.
instance Tables 56 measured execution time:
without exceptional costs,
50% costs connections Saturdays Sundays (weekends, we)
50% costs connections Fridays, Saturdays Sundays (long weekends, lwe),
random cost exceptions (rnd): added number randomly generated exceptions costs 0 10 TSP Austria 0 3000 instances
EU1 EU8.
55

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Instance
TSPAustria
TSPAustria,exc
TSPAustria,we
TSPAustria,lwe
TSPAustria,rnd
TSPAustria,rnd
TSPAustria,rnd
TSPAustria,rnd

#cost exceptions
0
2
36
54
10
50
100
200

cost/time
15/0.31s
15/0.32s
12/0.34s
11/0.35s
14/0.30s
15/0.31s
23/0.35s
36/0.37s

Table 5: TSP Results TSPAustria varying exceptions

Instance
TSPEU 1
TSPEU 1,we
TSPEU 1,lwe
TSPEU 1,rnd
TSPEU 1,rnd
TSPEU 1,rnd
TSPEU 1,rnd
TSPEU 2
TSPEU 2,we
TSPEU 2,lwe
TSPEU 2,rnd
TSPEU 2,rnd
TSPEU 2,rnd
TSPEU 2,rnd
TSPEU 3
TSPEU 3,we
TSPEU 3,lwe
TSPEU 3,rnd
TSPEU 3,rnd
TSPEU 3,rnd
TSPEU 3,rnd
TSPEU 4
TSPEU 4,we
TSPEU 4,lwe
TSPEU 4,rnd
TSPEU 4,rnd
TSPEU 4,rnd
TSPEU 4,rnd
TSPEU 5
TSPEU 5,we
TSPEU 5,lwe
TSPEU 5,rnd
TSPEU 5,rnd
TSPEU 5,rnd
TSPEU 5,rnd
TSPEU 5,rnd

#conn.
30
30
30
30
30
30
30
30
30
30
30
30
30
30
35
35
35
35
35
35
35
35
35
35
35
35
35
35
40
40
40
40
40
40
40
40

#except.
0
60
90
100
200
300
400
0
60
90
100
200
300
400
0
70
105
100
200
300
400
0
70
105
100
200
300
400
0
80
120
100
200
300
400
500

cost/time
-/9.11s
-/11.93s
-/13.82s
-/11.52s
-/12.79s
-/14.64s
-/16.26s
16213/13.27s
13195/16.41s
11738/18.53s
15190/15.54s
13433/16.31s
13829/18.34s
13895/20.59s
18576/24.11s
15689/28.02s
14589/30.39s
19410/26.75s
22055/29.64s
18354/31.54s
17285/32.66s
16533/36.63s
12747/41.72s
11812/43.12s
15553/39.17s
13216/41.19s
16413/43.51s
13782/45.69s
15716/91.83s
12875/97.73s
12009/100.14s
13146/85.69s
12162/83.44s
12074/76.81s
12226/82.97s
13212/82.53s

Instance
TSPEU 6
TSPEU 6,we
TSPEU 6,lwe
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 6,rnd
TSPEU 7
TSPEU 7,we
TSPEU 7,lwe
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 7,rnd
TSPEU 8
TSPEU 8,we
TSPEU 8,lwe
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd
TSPEU 8,rnd

#conn.
40
40
40
40
40
40
40
40
55
55
55
55
55
55
55
55
55
55
64
64
64
64
64
64
64
64
64
64
64

#except.
0
80
120
100
200
300
400
500
0
110
165
100
200
300
400
500
600
700
0
128
192
100
200
300
400
500
600
700
800

cost/time
17483/142.7s
14336/150.3s
13244/154.7s
15630/142.5s
14258/137.2s
11754/120.5s
11695/111.4s
12976/120.8s
15022/102.6s
12917/112.2s
11498/116.2s
13990/104.2s
12461/100.8s
13838/106.9s
12251/96.58s
16103/109.2s
14890/110.3s
17070/110.7s
10858/3872s
9035/3685s
8340/3324s
10283/2603s
9926/1372s
10028/1621s
8133/597.7s
8770/573.3s
8220/360.7s
6787/324.6s
11597/509.5s

Table 6: TSP Various instances capitals 15 EU members
56

fiA NSWER ET P LANNING U NDER ACTION C OSTS

TSP Results Instance TSPEU 8 shows limits system: given data allows many
possible tours, finding optimal one gets tricky. hand, realistic instance like
TSPEU 7 real airline connections solved rather quickly, surprising:
airlines central airport (for instance Vienna Austrian Airlines) direct connections
destinations served. allows much fewer candidate answer sets, (as
reality) number airlines consider limited. E.g., TSP EU 7 solution
two Star Alliance, Alitalia, Luxair allowed. course, cannot compete
dedicated TSP solvers/algorithms, able solve much bigger TSP instances
considered here. However, knowledge, none solvers deal features
incomplete knowledge, defaults, time dependent exceptional costs, etc. directly. results even
show execution times stable yet case many exceptions. contrast, instance TSP EU 8
shows exceptions also cause significant speedup. due heuristics used
underlying DLV system, single better solutions faster costs spread evenly
like TSPEU 8 without exceptional costs.
Note that, also experimented alternative Smodels translation sketched Section 6.3. refrain detailed discussion here, since (i) translation optimized DLV
Smodels performance worse (around factor 10 tested TSP instances) DLV (ii)
integrated planning frontend available Smodels providing high-level planning language. Nevertheless, shown approach can, minor modifications, adopted
planning system based Smodels.

8. Related Work
last years, widely recognized plan length alone one criterion
optimized planning. Several attempts made extend planners also consider action
costs.
PYRRHUS system (Williams & Hanks, 1994) extension UCPOP planning
allows optimal planning resources durations. Domain-dependent knowledge
added direct heuristic search. utility model defined planning problem
used express optimization function. system supports language extension
ADL (Pednault, 1989), predecessor PDDL (Ghallab et al., 1998). algorithm
synthesis branch-and-bound optimization least-commitment, plan-space planner.
approaches based heuristic search include use A* strategy together
action costs heuristics (Ephrati, Pollack, & Mihlstein, 1996) work Refanidis
Vlahavas use multi-criteria heuristics obtain near-optimal plans, considering multiple criteria
apart plan length alone (Refanidis & Vlahavas, 2001). However, described heuristics
fully admissible, guarantees optimal plans certain restrictions (Haslum & Geffner,
2000). fact, heuristic state-space planners able guarantee optimality.
powerful approach suggested Nareyek, describes planning resources
structural constraint satisfaction problem (SCSP), solves problem local search
combined global control. However, work promotes inclusion domain-dependent
knowledge; general problem unlimited search space, declarative high-level language provided (Nareyek, 2001).
Among related approaches, Kautz Walser generalize Planning Satisfiability
approach use integer optimization techniques encoding optimal planning resource pro57

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

duction/consumption (Kautz & Walser, 1999). First, recall integer logic programming
generalizes SAT, SAT formula translated system inequalities. Second, extend effects preconditions actions similar STRIPS extension proposed Koehler
modeling resource consumption/production (Koehler, 1998). Kautz Walser allow arbitrary
optimization functions use non-declarative, low-level representation based algebraic modeling language AMPL (Fourer, Gay, & Kernighan, 1993). mention Koehlers
STRIPS-like formalization mapped approach. However, express nondeterminism incomplete knowledge. implementation approach called ILPPLAN, uses AMPL package (http://www.ampl.com/). Unfortunately, AMPL
freely available, could compare system approach experimentally.
Lee Lifschitz describe extension C+ action language C allows intuitive encoding resources costs means called additive fluents (Lee & Lifschitz,
2001). way admissible planning realized, optimization considered
framework far. implementation planner based language CCALC (McCain,
1999) already described previous section. Another implementation planning system based action language C Cplan (Giunchiglia, 2000; Ferraris & Giunchiglia,
2000). Cplan system mainly focuses conformant planning support advanced
features C+. Furthermore, code longer maintained.
Son Pontelli propose translate action language B prioritized default theory answer
set programming. allow express preferences actions rules object level
interpreter part input language (Son & Pontelli, 2002). However,
preferences orthogonal approach model qualitative preferences opposed
overall value function plans/trajectories.

9. Conclusion Outlook
work continues research stream pursues usage answer set programming
building planning systems offer declarative planning languages based action languages,
planning tasks specified high level abstraction (Lifschitz, 1999a, 1999b).
representation practical planning problems, languages must high expressiveness
provide convenient constructs language elements.
Towards goal, presented planning language K c , extends declarative
planning language K (Eiter et al., 2000b, 2003a) action costs taken account
generating optimal plans, i.e., plans least total execution cost, admissible plans
wrt. given cost bound, i.e., plans whose total execution cost stays within given limit. basis
implementation issues, investigated computational complexity major planning tasks language, derived complexity results sharply characterizing
computational cost. Furthermore, presented transformation optimal admissible
planning problems K c logic programming optimal answer set semantics (Buccafurri
et al., 1997, 2000), described DLV K prototype implemented top KR tool
DLV, computes semantics.
shown, Kc allows representation intricate planning problems. particular,
demonstrated variant Traveling Salesperson Problem (TSP), could
conveniently specified Kc . strength Kc that, via underlying language K, states
knowledge, i.e., incomplete states, suitably respected secure plans, i.e., conformant plans
58

fiA NSWER ET P LANNING U NDER ACTION C OSTS

work circumstances, including nondeterministic action effects. K c flexible
language which, exploiting time-dependent action costs, allows representation planning
various optimality criteria cheapest plans, shortest plans, combinations thereof.
experiments shown various instances problems considered, including
realistic instances TSP variant, could decently solved. hand, current
version DLVK scale large problem instances general, and, unsurprisingly,
compete high-end planning tools specialized algorithms particular problem
TSP. see shortcoming, though, since main goal point demonstrate usefulness expressive capabilities formalism easily represent non-trivial
planning optimization tasks, especially involved viewpoint knowledge
representation. way, non-trivial instances problems medium size (which one may
often encounter) solved little effort.
Several issues remain work. implementation, performance improvements
may gained via improvements underlying DLV engine, subject current work.
Furthermore, alternative, efficient transformations Kc logic programming might researched, e.g. ones involve preprocessing planning problem performing means-end analysis simplify logic program constructed.
Another issue language extensions. example, crucial difference
approach resource-based approaches former hinges action costs, latter
build fluent values, somewhat different view quality plan. possible way
encompass language allow dynamic fluent values contribute action costs;
needs carefully elaborated, though: deterministic planning complete knowledge extension straightforward, non-deterministic domains incomplete knowledge
would possibly result ambiguities. Different trajectories plan possibly yield different
costs fluent values contribute action costs. favor intuitive definition plan costs
optimality refrained extension current state.
possible extension negative action costs, useful modeling producer/consumer relations among actions resources. Allowing different priorities among
actions, i.e., different cost levels, would increase flexibility allow optimizing different
criteria once. Finally, duration actions important issue. current language,
effects actions assumed materialize next state. coding techniques,
may express delayed effects several states time and/or interleaving actions, constructs
language would desirable. Investigating issues part ongoing future work.

Acknowledgments
grateful Joohyung Lee help using CCALC Paul Walser useful
informations ILPPLAN. Furthermore, thank Michael Gelfond interesting discussions
suggestions, anonymous reviewers detailed helpful comments.
work supported FWF (Austrian Science Funds) projects P14781
Z29-N04 European Commission project FET-2001-37004 WASP IST-200133570 INFOMIX.
preliminary, shorter version paper presented 8th European Conference
Logics Artificial Intelligence (JELIA02), Cosenza, Italy, September 2002.
59

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Appendix A. Language K
appendix contains, shortened form, definition language K translation K
answer set programs; see (Eiter et al., 2003b, 2003a) details examples.
A.1 Basic Syntax
assume act , f l , typ disjoint sets action, fluent type names, respectively, i.e.,
predicate symbols arity 0, disjoint sets con var constant variable symbols.
Here, f l , act describe dynamic knowledge typ describes static background knowledge.
action (resp. fluent, type) atom form p(t 1 , . . . , tn ), p act (resp. f l , typ ) arity n
t1 , . . . , tn con var . action (resp. fluent, type) literal l action (resp. fluent, type)
atom negation a, (alternatively, ) true negation symbol. define
.l = l = .l = l = a, atom. set L literals consistent,
L .L = . Furthermore, L+ (resp. L ) set positive (resp. negative) literals L.
set action (resp. fluent, type) literals denoted L act (resp. Lf l , Ltyp ). Furthermore, Lf l,typ
+
= Lf l Ltyp , Ldyn = Lf l L+
act , L = Lf l,typ Lact .
actions fluents must declared using statements follows.
Definition A.1 (action, fluent declaration) action (resp. fluent) declaration, form:
p(X1 , . . . , Xn ) requires t1 , . . . , tm

(8)

+
var n 0 arity p, , . . . ,
p L+
1

act (resp. p Lf l ), X1 , . . . , Xn
Ltyp , 0, every Xi occurs t1 , . . . , tm .

= 0, keyword requires may omitted. Causation rules specify dependencies
fluents fluents actions.
Definition A.2 (causation rule) causation rule (rule, short) expression form
caused f b1 , . . . , bk , bk+1 , . . . , bl a1 , . . . , , am+1 , . . . ,

(9)

f Lf l {false}, b1 , . . . , bl Lf l,typ , a1 , . . . , L, l k 0, n 0.
Rules n = 0 static rules, others dynamic rules. l = 0 (resp. n = 0), (resp.
after) omitted; l = n = 0, caused optional.
access parts causation rule r h(r) = {f }, post + (r) = {b1 , . . . , bk }, post (r) =
{bk+1 , . . . , bl }, pre+ (r) = {a1 , . . . , }, pre (r) = {am+1 , . . . , }, lit(r) = {f, b1 , . . . , bl ,
a1 , . . . , }. Intuitively, pre(r) = pre + (r) pre (r) (resp. post(r) = post+ (r) post (r))
accesses state (resp. after) action(s) happen.
Special static rules may specified initial states.
Definition A.3 (initial state constraint) initial state constraint static rule form (9)
preceded initially.
language K allows conditional execution actions, several alternative executability
conditions may specified.
60

fiA NSWER ET P LANNING U NDER ACTION C OSTS

Definition A.4 (executability condition) executability condition e expression form
executable b1 , . . . , bk , bk+1 , . . . , bl

(10)

L+
act b1 , . . . , bl L, l k 0.
l = 0 (i.e., executability unconditional), skipped. parts e accessed h(e) =
{a}, pre+ (e) = {b1 , . . . , bk }, pre (e) = {bk+1 , . . . , bl }, lit(e) = {a, b1 , . . . , bl }. Intuitively,
pre(e) = pre+ (e) pre (e) refers state actions suitability evaluated.
state action execution involved; convenience, define post+ (e) = post (e) = .
causal rules executability conditions must satisfy following condition,
similar safety logic programs: variable default-negated type literal must also occur
literal default-negated type literal. safety requested variables appearing
literals. reason variables appearing fluent action literals implicitly safe
respective type declarations.
Notation. causal rule, initial state constraint, executability condition r {post, pre, b},
define (r) = + (r) (r), bs (r) = posts (r) pres (r).
A.1.1 P LANNING OMAINS



P LANNING P ROBLEMS

Definition A.5 (action description, planning domain) action description hD, Ri consists
finite set action fluent declarations finite set R safe causation rules, safe initial
state constraints, safe executability conditions contain positive cyclic dependencies among actions. K planning domain pair PD = h, ADi, disjunction-free
normal Datalog program (the background knowledge) safe total well-founded
model (cf. (van Gelder, Ross, & Schlipf, 1991)) 8 AD action description. call PD
positive, default negation occurs AD.
Definition A.6 (planning problem) planning problem P = hPD, qi pair planning domain PD query q, i.e.,
g1 , . . . , gm , gm+1 , . . . , gn ? (i)

(11)

g1 , . . . , gn Lf l variable-free, n 0, 0 denotes plan length.
A.2 Semantics
start preliminary definition typed instantiation planning domain.
similar grounding logic program, difference correctly typed
fluent action literals generated.
Let PD = h, hD, Rii planning domain, let (unique) answer set (Gelfond & Lifschitz, 1991). Then, (p(X 1 , . . . , Xn )) legal action (resp. fluent) instance action (resp. fluent) declaration form (8), substitution defined X1 , . . . , Xn
{(t1 ), . . . , (tm )} . LPD denote set legal action fluent instances. instantiation planning domain respecting type information follows.
8. total well-founded model, existing, corresponds unique answer set datalog program. Allowing
multiple answer sets would eventually lead ambiguities language.

61

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Definition A.7 (typed instantiation) planning domain PD = h, hD, Rii, typed instantiation given PD = h, hD, Rii, grounding (over con )
R = {(r) | r R, r }, r set substitutions variables r using
con , lit((r)) Ldyn LPD (.LPD L
f l ).
words, PD replace R ground versions, keep latter
rules atoms fluent action literals agree declarations. say
PD = h, hD, Rii ground, R ground, moreover well-typed, PD
PD coincide.
A.2.1 TATES



RANSITIONS

Definition A.8 (state, state transition) state w.r.t planning domain PD consistent set
Lf l (lit(PD) lit(PD) ) legal fluent instances negations. state transition
tuple = hs, A, s0 s, s0 states Lact lit(PD) set legal action
instances PD.
Observe state necessarily contain either f f legal instance f
fluent, may even empty (s = ). State transitions constrained; done
definition legal state transitions below. proceed analogy definition answer sets
(Gelfond & Lifschitz, 1991), considering first positive (i.e., involving positive planning domain)
general planning problems.
follows, assume PD = h, hD, Rii well-typed ground planning domain
unique answer set . PD, respective concepts defined
typed grounding PD.
Definition A.9 (legal initial state) state 0 legal initial state positive PD, 0
least set (w.r.t. ) post(c) 0 implies h(c) s0 , initial state constraints
static rules c R.
positive PD state s, set L +
act called executable action set w.r.t. s,
exists executability condition e R h(e) = {a}, pre + (e)Lf l,typ sM ,
+

pre+ (e)L+
act A, pre (e)(Lact sM ) = . Note definition allows modeling
dependent actions, i.e. actions depend execution actions.
Definition A.10 (legal state transition) Given positive PD, state transition = hs, A, 0
called legal, executable action set w.r.t. 0 minimal consistent set satisfies
causation rules w.r.t. . is, every causation rule r R, (i) post(r) 0 ,
(ii) pre(r) Lf l,typ , (iii) pre(r) Lact hold, h(r) 6= {false}
h(r) s0 .
extended general well-typed ground PD containing default negation using
Gelfond-Lifschitz type reduction positive planning domain (Gelfond & Lifschitz, 1991).
Definition A.11 (reduction) Let PD ground well-typed planning domain, let =
hs, A, s0 state transition. Then, reduction PD = h, hD, Rt ii PD planning
domain Rt obtained R deleting
62

fiA NSWER ET P LANNING U NDER ACTION C OSTS

1. r R, either post (r)(s0 ) 6= pre (r)(sAM ) 6= ,
2. default literals L (L L) remaining r R.
Note PD positive ground. extend definitions follows.
Definition A.12 (legal initial state, executable action set, legal state transition) planning
domain PD, state s0 legal initial state, s0 legal initial state PD h,,s0 ; set
executable action set w.r.t. state s, executable w.r.t. PD hs,A,i ; and, state transition
= hs, A, s0 legal, legal PD .
A.2.2 P LANS
Definition A.13 (trajectory) sequence state transitions = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . .,
hsn1 , , sn ii, n 0, trajectory PD, s0 legal initial state PD hs i1 , Ai , si i,
1 n, legal state transitions PD.
n = 0, = hi empty s0 associated explicitly.
Definition A.14 (optimistic plan) sequence action sets hA 1 , . . . , Ai i, 0, optimistic
plan planning problem P = hPD, qi, trajectory = hhs 0 , A1 , s1 i, hs1 , A2 , s2 i, . . . ,
hsi1 , Ai , si ii exists PD accomplishes goal, i.e., {g 1 , . . . gm } si {gm+1 , . . . , gn }
si = .
Optimistic plans amount plans, valid plans etc defined literature. term
optimistic stress credulous view definition, respect incomplete fluent
information nondeterministic action effects. cases, execution optimistic plan
P might fail reach goal. thus resort secure plans.
Definition A.15 (secure plans (alias conformant plans)) optimistic plan hA 1 , . . . , secure plan, every legal initial state 0 trajectory = hhs0 , A1 , s1 i, . . . , hsj1 , Aj , sj ii
0 j n, holds (i) j = n accomplishes goal, (ii) j < n, j+1
executable sj w.r.t. PD, i.e., legal transition hs j , Aj+1 , sj+1 exists.
Note plans admit general concurrent execution actions. call plan hA 1 , . . . ,
sequential (or non-concurrent), |A j | 1, 1 j n.
A.3 Macros
K includes several macros shorthands frequently used concepts. Let L +
act denote
action atom, f Lf l fluent literal, B (possibly empty) sequence b 1 , . . . , bk , bk+1 , . . . ,
bl bi Lf l,typ , = 1, . . . , l, (possibly empty) sequence 1 , . . . , ,
am+1 , . . . , aj L, j = 1, . . . , n.
Inertia allow easy representation fluent inertia, K provides
inertial f B A.

Defaults



caused f .f, B f, A.

default value fluent expressed shortcut
default f.



caused f .f.

effect unless causation rule provides evidence opposite value.
63

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Totality

reasoning incomplete, total knowledge K provides (f positive):
total f B A.



caused f f, B A.
caused f f, B A.

instance useful model non-deterministic action effects. discussion
full impact statement modeling planning incomplete knowledge non-determinism,
refer previous paper language K (Eiter et al., 2003b).
State Integrity

integrity constraints refer preceding state, K provides
forbidden B A.

Non-executability



caused false B A.

specifying action executable, K provides

nonexecutable B.



caused false a, B.

definition, nonexecutable overrides executable case conflicts.
Sequential Plans exclude simultaneous execution actions, K provides
noConcurrency.



caused false a1 , a2 .

a1 a2 range possible actions 1 , a2 LPD Lact a1 6= a2 .
macros, B (resp. A) omitted, B (resp. A) empty.

Appendix B. Proofs
Proof Theorem 4.4: Membership (i): problems NP resp. NPMV, since l polynomial size P, optimistic plan P = hA 1 , . . . , Al P supporting trajectory
= ht1 , . . . , ti P guessed and, Proposition 4.1, verified polynomial time. Furthermore, costP (P ) b efficiently checked, since costP (P ) easily computed (all costs
constants).
Hardness (i): K fragment K c , K planning problem viewed problem
deciding existence resp. finding admissible plan wrt. cost 0. previously shown
(Eiter et al., 2003b), deciding existence optimistic plan given K planning problem
NP-hard fixed plan length l; hence, also NP-hard Kc .
show finding optimistic plan hard NPMV reduction well-known
SAT problem, cf. (Papadimitriou, 1994), whose instances CNFs = c 1 ck clauses ci =
Li,1 Li,mi , Li,j classical literal propositional atoms X = {x 1 , . . . , xn }.
Consider following planning domain PD :
fluents :
actions :

x1 . . . . xn . state0. state1.
c1 costs 1. . . . ck costs 1.
ax1 . . . . axn .
initially : total x1 . . . . total xn .
caused state0.
always :
caused state1 state0.
executable c1 .L1,1 , . . . , .L1,m1 .
forbidden .L1,1 , . . . , .L1,m1 , c1 .

executable ck .Lk,1 , . . . , .Lk,mk .
forbidden .Lk,1 , . . . , .Lk,mk , ck .
executable ax1 x1 . forbidden x1 , ax1 .

executable axn xn . forbidden xn , axn .

64

fiA NSWER ET P LANNING U NDER ACTION C OSTS

fluents xi state0 total statements initially-section encode candidate truth assignments. subsequent statements force c j executed iff corresponding
clause violated truth assignment encoded initial state. final pairs executable
forbidden statements force actions ax executed iff corresponding fluents x hold.
necessary directly extract computed truth assignments plan,
since dealing function class. fluent state1 identifies state time 1.
Consider planning problem P = hPD , state1?(1)i. Clearly, optimistic plan
P P corresponds truth assignment P X vice versa, costP (P ) number
clauses violated P . Thus, admissible optimistic plans P wrt. cost 0 correspond 1-1
satisfying assignments . Clearly, constructing P efficiently possible,
constructing satisfying truth assignment corresponding plan P (because actions
axi ). concludes hardness proof.
Membership (ii): Since security optimistic plan admissible wrt. cost k checked,
Proposition 4.1, call P2 -oracle, membership P3 resp. P3 MV follows
analogous considerations (i) (where oracle needed).
Hardness (ii): decision variant, P3 -hardness immediately inherited P3 completeness deciding existence secure plan problem language K,
hardness even fixed plan length (Eiter et al., 2003b). plan computation variant, give
reduction following P3 MV-complete problem: instance open QBF
Q[Z] = XY [X, Y, Z]
X = x1 , . . . , xl , = y1 , . . . , ym , Z = z1 , . . . , zn , respectively, [X, Y, Z]
(w.l.o.g.) 3CNF formula X, , Z. solutions S(I) truth assignments Z
Q[Z] satisfied.
Suppose [X, Y, Z] = c1 . . . ck ci = ci,1 ci,2 ci,3 . consider following
planning domain PDQ[Z] Q[Z], variant planning domain given proof
Theorem 5.5 (Eiter et al., 2003b):
fluents :
x1 . . . . xl . y1 . . . . ym . z1 . . . . zn . state0. state1.
actions :
az1 costs 0. . . . azn costs 0.
initially : total x1 . . . . total xl .
caused state0.
always :
caused state1 state0.
executable az1 . executable az2 . . . . executable azn .
caused x1 x1 . caused x1 x1 .

caused xl xl . caused xl xl .
total y1 state0. . . . total ym state0.
caused z1 az1 . caused z1 az1 .

caused zn azn . caused zn azn .
forbidden .C1,1 , .C1,2 , .C1,3 state0.

forbidden .Ck,1 , .Ck,2 , .Ck,3 state0.
|X|

2|X| many legal initial states s1 , . . . , s2 PDQ[Z] , correspond 1-1
possible truth assignments X initial states contain state0. Starting initial
state si , executing set actions represents truth assignment variables Z. Since
65

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

actions always executable, 2 |Z| executable action sets A1 , . . . , A2|Z| , represent
truth assignments Z.
|Y |
pair si Aj exist 2|Y | many successor state candidates si,1 , . . . , si,2 ,
contain fluents according truth assignment X represented , fluents according
truth assignment Z represented j , fluents according truth assignment ,
fluent state1. candidate states, satisfying clauses [X, Y, Z] legal,
virtue forbidden statements.
hard see optimistic plan form P = hA 1 (where A1 {azi | zi Z})
goal state1 exists wrt. PDQ[Z] iff assignment variables X Z
formula [X, Y, Z] satisfied. Furthermore, P secure iff A1 represents assignment
variables Z that, regardless assignment variables X chosen
(corresponding legal initial state ), assignment variables
clauses [X, Y, Z] satisfied (i.e., least one state si,k reachable si executing
A1 ); si,k contains state1. words, P secure iff [X, Y, Z] true. Thus,
admissible secure plans PDQ[Z] wrt. cost 0, correspond 1-1 assignments Z
Q[Z] true.
Since PDQ[Z] constructible [X, Y, Z] polynomial time, follows computing
secure plan P = hPDQ[Z] , qi, q = state1 ? (1), P3 MV-hard.
2
Proof Theorem 4.5: Membership (i): Concerning membership, performing binary search
range [0, max] (where max upper bound plan costs plan polynomial
length l given l times sum action costs) find least integer v
optimistic plan P P admissible wrt. cost v exists (if optimistic plan exists);
clearly, costP (P ) = v costP = v, thus plan P optimal. Since max
single exponential representation size P, binary search, thus computing cost P ,
is, Theorem 4.4, feasible polynomial time NP oracle. Subsequently, construct
optimistic plan P costP (P ) = costP extending partial plan Pi = hA1 , . . . , Ai i,
= 0, . . . , l 1 step step follows. Let = {a 1 , . . . , } set legal action
instances. initialize Bi+1 := ask oracle whether Pi completed optimistic
plan P = hA1 , . . . , Al admissible wrt. costP Ai+1 (Bi+1 \ {a1 }). answer
yes, update Bi+1 := Bi+1 \ {a1 }, else leave Bi+1 unchanged. repeat test
aj , j = 2, 3, . . . , m; resulting Bi+1 action set Pi+1 = hA1 , . . . , Ai , Ai+1
Ai+1 = Bi+1 completed optimistic plan admissible wrt. cost P . Thus, Ai+1
polynomial-time constructible NP oracle.
summary, construct optimal optimistic plan polynomial time NP oracle.
Thus, problem FP2 .
Hardness (i): show hardness plan length l = 1 reduction problem MAX WEIGHT
SAT (Papadimitriou, 1994), instance SAT instance = c 1 ck proof
Theorem 4.4.(i), plus positive integer weightsP
w , = 1, . . . , k. Then, S(I) contains
truth assignments X wsat () = : ci =true wi maximal.
end, take planning domain PD proof Theorem 4.4 modify
cost ci wi , = 1, . . . , k, thus constructing new planning domain PD . Consider
planning problem PI = hPDI , state1?(1)i. Since actions cj actions nonzero
cost, plan (corresponding toP
truth assignment )
P associated sum weights
violated clauses, wvio () = ( ki=1 wi ) wsat (). Since ki=1 wi constant I, minimizing
66

fiA NSWER ET P LANNING U NDER ACTION C OSTS

wvio () equivalent maximizing wsat (). Hence, one-to-one correspondence
optimal optimistic plans PI (for wvio () minimal) maximal truth assignments I.
Furthermore, computing PI extracting MAX-WEIGHT SAT solution optimal
plan P efficiently possible. proves FP2 -hardness.
Membership (ii): proof similar membership proof (i), uses oracle asks
completion partial secure plan P = hA1 , . . . , Ai secure plan P = hA1 , . . . , Al
Ai+1 (Bi+1 \ {aj }) P admissible wrt. costP , rather partial optimistic plan.
oracle is, easily seen, P3 . Thus, computing optimal secure plan F P4 .
Hardness (ii): show hardness reduction following problem, F P4 complete (cf. (Krentel, 1992)): Given open QBF Q[Z] = XY [X, Y, Z] like proof
Theorem 4.4.(ii), compute lexicographically first truth assignment Z Q[Z]
satisfied.
accomplished changing cost action az PDQ[Z] 0 2ni ,
= 1, . . . , n. Let PD 0 [Q[Z]] resulting planning domain. Since cost az (i.e., assigning
zi value true) greater sum costs az j + 1 j n, optimal
secure plan planning problem hPD 0 [Q[Z]], state1 ? (1)i amounts lexicographically
first truth assignment Z Q[Z] satisfied. Thus, FP4 -hardness problem follows.
2
Proof Theorem 6.1: prove result applying well-known Splitting Set Theorem
logic programs (Lifschitz & Turner, 1994). theorem applies logic programs
split two parts one them, bottom part, refer predicates defined
top part all. answer sets bottom part extended answer sets
whole program looking remaining (top) rules. Informally, splitting set
set U ground literals defining bottom part bU () program. answer set Sb
bU () used reduce remaining rules \ b U () program eU ( \ bU (), Sb )
involving classical literals occur b U (), evaluating literals b U ()
wrt. Sb . answer set Se eU ( \ bU (), Sb ), set = Sb Se answer set
original program.
Disregarding weak constraints, split program lp w (P) bottom part consisting
lp(Pnc ), Pnc P cost information stripped off, top part containing
remaining rules; derive correspondence optimistic plans P answer sets
lpw (P) similar correspondence result lp(P nc ) (Eiter et al., 2003a).
detail, Theorem 3.1 (Eiter et al., 2003a) states K-planning problem P correspondence answer sets lp(P) supporting trajectories optimistic plans
P = hA1 , . . . , Al items (i) (ii), costs discarded. Thus, answer set 0 lp(Pnc )
corresponds trajectory 0 optimistic plan P 0 Pnc vice versa.
follows, talking lp(P nc ) lpw (P), mean respective grounded
logic programs. lpw (P) augments lp(Pnc ) rules (4) weak constraints (5). Let U =
lit(lp(Pnc )) set literals occurring lp(P nc ). Clearly, U splits lpw (P) defined
(Lifschitz & Turner, 1994), disregard weak constraints lp w (P), since rules form
(4) introduce new head literals. Consequently, get b U (lpw (P)) = lp(Pnc ). Then,
answer set 0 lp(Pnc ), rule eU (lpw (P) \ bU (lpw (P)), 0 ) form
costa (x1 , . . . , xn , t, c) :- Body.
67

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

fact rules positive, conclude respect split U ,
answer set 0 lp(Pnc ) induces unique answer set 0 lpw (P). Therefore, modulo
costs, correspondence supporting trajectories candidate answer sets claimed
follows directly Theorem 3.1 (Eiter et al., 2003a).
remains prove costP (P ) = costlpw (P) (S) holds candidate answer sets corresponding optimistic plan P = hA 1 , . . . , Al P. correspondence shown above,
action p(x1 , . . . , xn ) Aj corresponds exactly one atom p(x 1 , . . . , xn , j 1) ASj ,
j {1, . . . , l}. Therefore, p(x1 , . . . , xn ) declared non-empty cost part, (4)
well-definedness, modulo x1 , . . . , xn , exactly one fact costp (x1 , . . . , xn , j 1, c)
model eU (lpw (P) \ bU (lpw (P)), S).
Furthermore, definition (4), c = costj (p(x1 , . . . , xn )), i.e., cost action
instance p(x1 , . . . , xn ) time j. Consequently,
Pl
P violation value weak constraint wc
w
form (5) p lp (P) costwc (S) =
j=1
p(x1 ,...,xn )Aj costj (p(x1 , . . . , xn )). Since
violation values stem weak constraints (5), total cost lpw (P) (S) = costP (P ).
proves result.
2

References
Blum, A. L., & Furst, M. L. (1997). Fast Planning Planning Graph Analysis. Artificial
Intelligence, 90, 281300.
Bonet, B., & Geffner, H. (2000). Planning Incomplete Information Heuristic Search
Belief Space. Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.), Proceedings
Fifth International Conference Artificial Intelligence Planning Scheduling (AIPS00),
pp. 5261, Breckenridge, Colorado, USA.
Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEE Transactions Computers, C-35(8), 677691.
Buccafurri, F., Leone, N., & Rullo, P. (1997). Strong Weak Constraints Disjunctive Datalog.
Dix, J., Furbach, U., & Nerode, A. (Eds.), Proceedings 4th International Conference
Logic Programming Non-Monotonic Reasoning (LPNMR97), No. 1265 Lecture
Notes AI (LNAI), pp. 217, Dagstuhl, Germany. Springer Verlag.
Buccafurri, F., Leone, N., & Rullo, P. (2000). Enhancing Disjunctive Datalog Constraints. IEEE
Transactions Knowledge Data Engineering, 12(5), 845860.
Cimatti, A., & Roveri, M. (2000). Conformant Planning via Symbolic Model Checking. Journal
Artificial Intelligence Research, 13, 305338.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity Expressive Power
Logic Programming. ACM Computing Surveys, 33(3), 374425.
Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding Planning Problems Nonmonotonic
Logic Programs. Proceedings European Conference Planning 1997 (ECP-97),
pp. 169181. Springer Verlag.
Eiter, T., Faber, W., Leone, N., & Pfeifer, G. (2000a). Declarative Problem-Solving Using
DLV System. Minker, J. (Ed.), Logic-Based Artificial Intelligence, pp. 79103. Kluwer
Academic Publishers.
68

fiA NSWER ET P LANNING U NDER ACTION C OSTS

Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2000b). Planning Incomplete
Knowledge. Lloyd, J., Dahl, V., Furbach, U., Kerber, M., Lau, K.-K., Palamidessi, C.,
Pereira, L. M., Sagiv, Y., & Stuckey, P. J. (Eds.), Computational Logic - CL 2000, First International Conference, Proceedings, No. 1861 Lecture Notes AI (LNAI), pp. 807821,
London, UK. Springer Verlag.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2002a). Answer Set Planning
Action Costs. Flesca, S., Greco, S., Ianni, G., & Leone, N. (Eds.), Proceedings
8th European Conference Artificial Intelligence (JELIA), No. 2424 Lecture Notes
Computer Science, pp. 186197.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2002b). Answer Set Planning Action Costs. Tech. rep. INFSYS RR-1843-02-13, Institut fur Informationssysteme, Technische
Universitat Wien.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003a). Logic Programming Approach
Knowledge-State Planning, II: DLV K System. Artificial Intelligence, 144(12), 157
211.
Eiter, T., Faber, W., Leone, N., Pfeifer, G., & Polleres, A. (2003b). Logic Programming Approach
Knowledge-State Planning: Semantics Complexity. appear ACM Transactions
Computational Logic.
Ephrati, E., Pollack, M. E., & Mihlstein, M. (1996). Cost-directed Planner: Preliminary Report.
Proceedings Thirteenth National Conference Artificial Intelligence (AAAI-96),
pp. 1223 1228. AAAI Press.
Erdem, E. (1999). Applications Logic Programming Planning: Computational Experiments.
Unpublished draft. http://www.cs.utexas.edu/users/esra/papers.html.
Faber, W., & Pfeifer, G. (since 1996). DLV homepage.. http://www.dlvsystem.com/.
Ferraris, P., & Giunchiglia, E. (2000). Planning Satisfiability Nondeterministic Domains.
Proceedings Seventeenth National Conference Artificial Intelligence (AAAI00), July
30 August 3, 2000, Austin, Texas USA, pp. 748753. AAAI Press / MIT Press.
Fourer, R., Gay, D. M., & Kernighan, B. W. (1993). AMPL: Modeling Language Mathematical
Programming. Duxbury Press.
Gelfond, M., & Lifschitz, V. (1991). Classical Negation Logic Programs Disjunctive
Databases. New Generation Computing, 9, 365385.
Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Veloso, M., Weld,
D., & Wilkins, D. (1998).
PDDL Planning Domain Definition language. Tech. rep., Yale Center Computational Vision Control. Available
http://www.cs.yale.edu/pub/mcdermott/software/pddl.tar.gz.
Giunchiglia, E. (2000). Planning Satisfiability Expressive Action Languages: Concurrency,
Constraints Nondeterminism. Cohn, A. G., Giunchiglia, F., & Selman, B. (Eds.), Proceedings Seventh International Conference Principles Knowledge Representation
Reasoning (KR 2000), April 12-15, Breckenridge, Colorado, USA, pp. 657666. Morgan
Kaufmann.
69

fiE ITER , FABER , L EONE , P FEIFER & P OLLERES

Giunchiglia, E., Kartha, G. N., & Lifschitz, V. (1997). Representing Action: Indeterminacy
Ramifications. Artificial Intelligence, 95, 409443.
Giunchiglia, E., & Lifschitz, V. (1998). Action Language Based Causal Explanation: Preliminary Report. Proceedings Fifteenth National Conference Artificial Intelligence
(AAAI 98), pp. 623630.
Haslum, P., & Geffner, H. (2000). Admissible Heuristics Optimal Planning. Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.), Proceedings Fifth International Conference
Artificial Intelligence Planning Scheduling (AIPS00), pp. 140149, Breckenridge, Colorado, USA. AAAI Press.
Kautz, H., & Walser, J. P. (1999). State-space planning integer optimization. Proceedings
16th National Conference Artificial Intelligence (AAAI-99), pp. 526533.
Koehler, J. (1998). Planning Resource Constraints. Proceedings 13th European
Conference Artificial Intelligence (ECAI98), pp. 489493.
Krentel, M. (1992). Generalizations Opt P Polynomial Hierarchy. Theoretical Computer
Science, 97(2), 183198.
Lee, J., & Lifschitz, V. (2001). Additive Fluents. Provetti, A., & Cao, S. T. (Eds.), Proceedings
AAAI 2001 Spring Symposium Answer Set Programming: Towards Efficient Scalable
Knowledge Representation Reasoning, pp. 116123, Stanford, CA. AAAI Press.
Lifschitz, V., & Turner, H. (1994). Splitting Logic Program. Van Hentenryck, P. (Ed.), Proceedings 11th International Conference Logic Programming (ICLP94), pp. 2337,
Santa Margherita Ligure, Italy. MIT Press.
Lifschitz, V., & Turner, H. (1999). Representing Transition Systems Logic Programs. Gelfond,
M., Leone, N., & Pfeifer, G. (Eds.), Proceedings 5th International Conference Logic
Programming Nonmonotonic Reasoning (LPNMR99), No. 1730 Lecture Notes AI
(LNAI), pp. 92106, El Paso, Texas, USA. Springer Verlag.
Lifschitz, V. (1996). Foundations Logic Programming. Brewka, G. (Ed.), Principles Knowledge Representation, pp. 69127. CSLI Publications, Stanford.
Lifschitz, V. (1999a). Action Languages, Answer Sets Planning. Apt, K., Marek, V. W.,
Truszczynski, M., & Warren, D. S. (Eds.), Logic Programming Paradigm 25-Year
Perspective, pp. 357373. Springer Verlag.
Lifschitz, V. (1999b). Answer Set Planning. Schreye, D. D. (Ed.), Proceedings 16th
International Conference Logic Programming (ICLP99), pp. 2337, Las Cruces, New
Mexico, USA. MIT Press.
McCain, N. (1999). Causal Calculator Homepage.. http://www.cs.utexas.edu/
users/tag/cc/.
McCain, N., & Turner, H. (1997). Causal Theories Actions Change. Proceedings
15th National Conference Artificial Intelligence (AAAI-97), pp. 460465.
McCain, N., & Turner, H. (1998). Satisfiability Planning Causal Theories. Cohn, A. G.,
Schubert, L., & Shapiro, S. C. (Eds.), Proceedings Sixth International Conference Principles Knowledge Representation Reasoning (KR98), pp. 212223. Morgan Kaufmann
Publishers.
70

fiA NSWER ET P LANNING U NDER ACTION C OSTS

Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
Efficient SAT Solver. Proceedings 38th Design Automation Conference, DAC 2001,
Las Vegas, NV, USA, June 18-22, 2001, pp. 530535. ACM.
Nareyek, A. (2001). Beyond Plan-Length Criterion. Local Search Planning Scheduling, ECAI 2000 Workshop, Vol. 2148 Lecture Notes Computer Science, pp. 5578.
Springer.
Niemela, I. (1998). Logic Programs Stable Model Semantics Constraint Programming
Paradigm. Niemela, I., & Schaub, T. (Eds.), Proceedings Workshop Computational Aspects Nonmonotonic Reasoning, pp. 7279, Trento, Italy.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Pednault, E. P. D. (1989). Exploring Middle Ground STRIPS Situation Calculus. Proceedings 1st International Conference Principles Knowledge Representation Reasoning (KR89), pp. 324332, Toronto, Canada. Morgan Kaufmann Publishers,
Inc.
Refanidis, I., & Vlahavas, I. (2001). Framework Multi-Criteria Plan Evaluation Heuristic
State-Space Planning. IJCAI-01 Workshop Planning Resources.
Selman, A. L. (1994). Taxonomy Complexity Classes Functions. Journal Computer
System Sciences, 48(2), 357381.
Simons, P., Niemela, I., & Soininen, T. (2002). Extending Implementing Stable Model
Semantics. Artificial Intelligence, 138, 181234.
Smith, D. E., & Weld, D. S. (1998). Conformant Graphplan. Proceedings Fifteenth
National Conference Artificial Intelligence, (AAAI98), pp. 889896. AAAI Press /
MIT Press.
Son, T. C., & Pontelli, E. (2002). Reasoning Actions Prioritized Default Theory. Flesca,
S., Greco, S., Ianni, G., & Leone, N. (Eds.), Proceedings 8th European Conference
Artificial Intelligence (JELIA), No. 2424 Lecture Notes Computer Science, pp. 369381.
Subrahmanian, V., & Zaniolo, C. (1995). Relating Stable Models AI Planning Domains.
Sterling, L. (Ed.), Proceedings 12 th International Conference Logic Programming,
pp. 233247, Tokyo, Japan. MIT Press.
van Gelder, A., Ross, K., & Schlipf, J. (1991). Well-Founded Semantics General Logic
Programs. Journal ACM, 38(3), 620650.
Weld, D. S., Anderson, C. R., & Smith, D. E. (1998). Extending Graphplan Handle Uncertainty
& Sensing Actions. Proceedings Fifteenth National Conference Artificial Intelligence, (AAAI98), pp. 897904. AAAI Press / MIT Press.
Williams, M., & Hanks, S. (1994). Optimal Planning Goal-Directed Utility Model.
Hammond, K. J. (Ed.), Proceedings Second International Conference Artificial Intelligence Planning Systems (AIPS-94), pp. 176181. AAAI Press.

71

fiJournal Artificial Intelligence Research 19 (2003) 155-203

Submitted 1/03; published 9/03

Representing Aggregating Conflicting Beliefs
Pedrito Maynard-Zhang

maynarp@muohio.edu

Department Computer Science Systems Analysis
Miami University
Oxford, Ohio 45056, USA

Daniel Lehmann

lehmann@cs.huji.ac.il

School Computer Science Engineering
Hebrew University
Jerusalem 91904, Israel

Abstract
consider two-fold problem representing collective beliefs aggregating
beliefs. propose novel representation collective beliefs uses modular,
transitive relations possible worlds. allow us represent conflicting opinions
clear semantics, thus improving upon quasi-transitive relations often
used social choice. describe way construct belief state agent
informed set sources varying degrees reliability. construction circumvents
Arrows Impossibility Theorem satisfactory manner accounting explicitly
encoded conflicts. give simple set-theory-based operator combining information multiple agents. show operator satisfies desirable invariants
idempotence, commutativity, associativity, and, thus, well-behaved iterated,
describe computationally effective way computing resulting belief state.
Finally, extend framework incorporate voting.

1. Introduction
interested multi-agent setting agents informed sources varying
levels reliability, agents iteratively combine belief states. setting
introduces three problems: (1) Finding appropriate representation collective beliefs;
(2) Constructing agents belief state aggregating information informant
sources, accounting relative reliability sources; and, (3) Combining
information multiple agents manner well-behaved iteration.
addressing first problem, take starting point total preorders possible
worlds (i.e., interpretations specified language) used belief revision community
represent individuals beliefs. relations describe opinions relative likelihood
worlds viewed encoding agents conditional beliefs, i.e.,
believes now, would believe conditions. representation
based semantical work (cf. Grove, 1988; Katsuno & Mendelzon, 1991) supporting
Alchourron, Gardenfors, Makinson proposal (Alchourron, Gardenfors, & Makinson,
1985; Gardenfors, 1988) (known AGM theory) belief revision.
social choice community dealt extensively problem representing
collective preferences (cf. Sen, 1986). However, problem formally equivalent
representing collective beliefs, results applicable. classical approach
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiMaynard-Zhang & Lehmann

use quasi-transitive relations relations whose asymmetric restrictions transitive
set objects. (Total preorders special subclass relations.) However,
relations distinguish group indifference group conflict,
distinction crucial. Consider, example, situation members
group indifferent movie movie b. passerby expresses preference
a, group may well choose adopt opinion group borrow a.
However, group already divided relative merits b, would
wise hesitate choosing one new supporter appears
scene. propose representation distinction explicit. Specifically,
propose modular, transitive relations argue solve unpleasant
semantical problems suffered earlier approach. (We define modularity precisely later,
viewed intuitively sufficient relaxation totality requirement total
preorders make distinction indifference conflict possible.)
second problem addresses agent actually go combining
information received set sources create belief state. mechanism
favor opinions held reliable sources, yet allow less reliable sources voice opinions higher ranked sources opinion. True, circumstances would
advisable opinion less reliable source override agnosticism
reliable source, often better accept opinions default assumptions
better information available. define mechanism this, relying
generalized representation circumvent Arrows (1963) Impossibility Theorem
sources equal reliability.
motivate third problem, consider following dynamic scenario: robot controlling ship space receives number communication centers Earth information
status environment tasks. center receives information
group sources varying credibility accuracy (e.g., nearby satellites experts)
aggregates it. Timeliness decision-making space often crucial, want
robot wait center sends information central location
first combined forwarded robot. Instead, center sends
aggregated information directly robot. scheme reduce dead
time, also allows anytime behavior robots part: robot incorporates new
information arrives makes best decisions whatever information
given point. distributed approach also robust since degradation
performance much graceful information individual centers get lost
delayed.
scenario, robot needs mechanism combining fusing belief states
multiple agents potentially arriving different times. Moreover, belief state output
mechanism invariant respect order agent arrivals.
describe simple set-theoretic mechanism satisfies requirements well
computationally effective way computing resulting belief state.
aggregation fusion mechanisms described far take account quality
support opinions, completely ignore quantity support. However, latter often provides sufficient information resolve apparent conflicts. Take, example,
situation sources robot equal credibility except
small minority suggest robot move spaceship avoid potential collision
156

fiRepresenting Aggregating Conflicting Beliefs

oncoming asteroid. situation, often prefer resolve conflict siding
majority. end, describe extend framework allow voting,
introducing novel modular closure operation process.
preliminary definitions, address topics turn.

2. Preliminaries
begin defining various well-known properties binary relations1 ; useful
us throughout paper.
Definition 1 Suppose relation finite set , i.e., .2 use
x denote (x, y) x 6 denote (x, y) 6. relation is:
1. reflexive iff x x x . irreflexive iff x 6 x x .
2. symmetric iff x x x, . asymmetric iff x 6 x
x, . anti-symmetric iff x x x = x, .
3. asymmetric restriction relation 0 iff x x 0 60 x
x, . symmetric restriction 0 iff x x 0 0 x
x, .
4. total iff x x x, .
5. modular iff x x z z x, y, z .
6. transitive iff x z x z x, y, z .
7. quasi-transitive iff asymmetric restriction transitive.
8. transitive closure relation 0 iff, integer n,
x w0 , . . . , wn . x = w0 0 0 wn =
x, . (We generally use + denote transitive closure relation
.)
9. acyclic iff w0 , . . . , wn . w0 < < wn implies wn 6< w0 integers n,
< asymmetric restriction .
10. total preorder iff total transitive. total order iff also antisymmetric.
11. equivalence relation iff reflexive, symmetric, transitive.
12. fully connected iff x x, . fully disconnected iff x 6
x, .
Proposition 1
1. transitive closure modular relation modular.
1. use binary relations paper, refer simply relations.
2. readers convenience, included Appendix B key notational symbols
used throughout paper.

157

fiMaynard-Zhang & Lehmann

2. Every transitive relation quasi-transitive.
3. (Sen, 1986) Every quasi-transitive relation acyclic.
Given relation set alternatives subset alternatives, often
want pick subsets best elements respect relation. define set
best elements subsets choice set:
Definition 2 relation finite set , < asymmetric restriction,
X , choice set X respect
ch(X, ) = {x X :6 x0 X. x0 < x}.
choice function one assigns every (non-empty) subset X (non-empty) subset
X:
Definition 3 choice function finite set function f : 2 \ 2 \
f (X) X every non-empty X .
Now, every acyclic relation defines choice function, one assigns subset
choice set:
Proposition 2 (Sen, 1986) Given relation finite set , choice set operation
ch defines choice function iff acyclic.3
relation acyclic, elements involved cycle said conflict
cannot order them:
Definition 4 Given relation < finite set , x conflict wrt < iff
exist w0 , . . . , wn , z0 , . . . , zm x = w0 < < wn = = z0 < < zm = x,
x, .
Finally, cardinality set denoted kk.
Assume given language L satisfaction relation |= L. Let
W finite, non-empty set possible worlds (interpretations) L. world
w W sentence p L, w |= p iff p evaluates true w. Given sentence p,
|p| = {w W | w |= p}.

3. Representing Collective Beliefs
representation collective beliefs generalizes representation developed belief
revision community conditional beliefs individual, briefly review it.
consider implications social choice representing collective beliefs. Finally,
describe proposal argue desirability.
3. Sens uses slightly stronger definition choice sets, theorem still holds general
case.

158

fiRepresenting Aggregating Conflicting Beliefs

3.1 Belief Revision Representation Conditional Beliefs
Much belief revision field built seminal work Alchourron, Gardenfors,
Makinson (Alchourron et al., 1985; Gardenfors, 1988) refered AGM theory.
work sought formalize Occams razor-like principle minimal change: set
beliefs resulting revision one produced modifying original beliefs
minimally accomodate new information. capture principle precisely,
proposed famous AGM postulates impose restrictions belief change operators.
Subsequent model-theoretic work (Grove, 1988; Katsuno & Mendelzon, 1991; MaynardReid II & Shoham, 2001) showed accepting postulates amounts assuming
individuals belief state represented total preorder W; revision
individuals beliefs sentence p L consists computing ch(|p|, ).
Kraus, Lehmann, Magidor (1990) Lehmann Magidor (1992) developed
similar central role ordered structures semantics nonmonotonic logics,
Gardenfors Makinson (1994) established relation two topics. Semantically, represents weak relative likelihood possible worlds: x means possible
world x considered least likely possible world y.4 x x,
x considered equally likely. also interpret sententially using famous
Ramsey Test (Ramsey, 1931): encodes set conditional beliefs, i.e.,
believed (called belief set), counterfactual beliefs well (what would
believed conditions case). According criteria, conditional
belief p q holds p q sentences L q satisfied worlds
ch(|p|, ); write Bel(p?q). neither belief p?q belief p?q hold belief
state, said agnostic respect p?q, written Agn(p?q). belief set induced
belief state consists sentences q Bel(true?q) holds.
3.2 Social Choice Implications
first inclination, then, would use total preorders represent collective beliefs
since work well individuals beliefs. Unfortunately, approach inherently
problematic discovered early social choice community. communitys
interest lies representing collective preferences rather collective beliefs; however,
results equally relevant since classical representation individuals preferences
also total preorder. Instead relative likelihood, relations represent relative preference;
instead equal likelihood, indifference.
Arrows (1963) celebrated Impossibility Theorem showed aggregation operator
total preorders exists satisfying following small set desirable properties:
Definition 5 Let f aggregation operator relations 1 , . . . , n n individuals, respectively, finite set alternatives , let = f (1 , . . . , n ).
Restricted Range: range f set total preorders .
Unrestricted Domain: domain f set n-tuples total preorders
.
4. direction relation symbol unintuitive, standard practice belief revision community.

159

fiMaynard-Zhang & Lehmann

Pareto Principle: x i, x y.5
Independence Irrelevant Alternatives (IIA): Suppose 0 = f (01 , . . . , 0n ). If,
x, , x iff x 0i i, x iff x 0 y.
Non-Dictatorship: individual that, every tuple domain
f every x, , x implies x y.
Proposition 3 (Arrow, 1963) aggregation operator satisfies restricted
range, unrestricted domain, Pareto principle, independendence irrelevant alternatives,
nondictatorship.
impossibility theorem led researchers look weakenings Arrows framework
would circumvent result. One weaken restricted range condition, requiring
result aggregation satisfy totality quasi-transitivity rather
full transitivity total preorder. weakening sufficient guarantee existence
aggregation function satisfying conditions, still producing relations
defined choice functions (Sen, 1986). However, solution without
problems.
First, perhaps obviously, domain range aggregation operator
different, violating known belief revision literature principle
categorical matching (cf. Gardenfors Rotts 1995 survey). problem closely related
second total, quasi-transitive relations unsatisfactory semantics.
total quasi-transitive total preorder, indifference relation
transitive:
Proposition 4 Let relation finite set let symmetric restriction.
total quasi-transitive transitive, transitive.
much discussion whether indifference transitive.
many cases one feels indifference transitive; Deb indifferent
plums mangoes also indifferent mangoes peaches, would greatly
surprised profess strong preference plums peaches.6 Thus, seems
total quasi-transitive relations total preorders cannot understood easily
preference indifference. Since existence choice function generally sufficient
classical social choice problems, issues least ignorable. However, iterated
aggregation, result aggregation must usable making decisions,
must interpretable new preference relation may involved later aggregations
and, consequently, must maintain clean semantics.
Third, totality assumption excessively restrictive representing aggregate preferences. general, binary relation express four possible relationships
pair alternatives b: b b 6 a, b 6 b, b b a, 6 b
b 6 a. Totality reduces set first three which, interpretation
5. Technically, known weak Pareto principle. strong Pareto principle states x
exists x x i. Obviously, strong version implies weak
version, Arrows theorem applies well.
6. However, see Luces (1956) work semiorders opposing arguments transitivity
indifference debate.

160

fiRepresenting Aggregating Conflicting Beliefs

relations representing weak preference, correspond two strict orderings b,
indifference. However, consider situation couple trying choose
Italian Indian restaurant, one strictly prefers Italian food Indian food,
whereas second strictly prefers Indian Italian. couples opinions conflict,
situation fit three categories. Thus, totality assumption
essentially assumption conflicts exist. This, one may argue, appropriate
want represent preferences one agent (but see Kahneman Tverskys (1979)
persuasive arguments individuals often ambivalent). However, assumption
inappropriate want represent aggregate preferences since individuals almost
certainly differences opinion.
3.3 Generalized Belief States
belief aggregation formally similar preference aggregation, also susceptible problems faced social choice community. take view much
difficulty encountered previous attempts define acceptable aggregation policies
lack explicit representations conflicts among individuals. generalize
total preorder representation capture information conflicts. generalization opens way semantically clear aggregation policies, added benefit
focusing attention culprit sets worlds.
3.3.1 Modular, Transitive States
take strict likelihood primitive. Since strict likelihood necessarily total,
possible represent agnosticism conflicting opinions structure.
choice deviates authors, similar Kreps (1990, p. 19)
interested representing indifference incomparability. Unlike Kreps, rather
use asymmetric relation represent strict likelihood (e.g., asymmetric restriction
weak likelihood relation), impose less restrictive condition modularity.
formally define generalized belief states:
Definition 6 generalized belief state modular, transitive relation W.
set possible generalized belief states W denoted B.
interpret b mean reason consider strictly likely b.
represent equal likelihood, also refer agnosticism, relationship
defined x x 6 6 x. define conflict relation
corresponding , denoted ./, x ./ iff x x. describes situations
reasons consider either pair worlds strictly likely
other. fact, one easily check ./ precisely represents conflicts belief state
sense Definition 4.
convenience, refer generalized belief states simply belief states except
would cause confusion.
3.3.2 Discussion
Let us consider choice representation justified. First, agree social
choice community strict likelihood transitive.
161

fiMaynard-Zhang & Lehmann

discussed above, often compelling reason agnosticism/indifference
transitive; also adopt view. However, transitivity strict likelihood
guarantee transitivity agnosticism. simple example following:
W = {a, b, c} = {(a, c)}, = {(a, b), (b, c)}. However, buy strict likelihood transitive, agnosticism transitive identically strict likelihood
also modular:
Proposition 5 Suppose relation transitive corresponding agnosticism
relation. transitive iff modular.
summary, transitivity modularity necessary strict likelihood agnosticism
required transitive.
point conflicts also transitive framework. first glance,
may appear undesirable: entirely possible group disagree relative
likelihood worlds b, b c, yet agree likely c. However,
note transitivity follows cycle-based definition conflicts (Definition 4),
belief state representation. highlights fact concerned
conflicts arise simple disagreements pairs alternatives,
inferred series inconsistent opinions well.
Now, argue modular, transitive relations sufficient capture relative likelihood, agnosticism, conflicts among group information sources, first point
adding irreflexivity would give us class relations asymmetric restrictions
total preorders, i.e., conflict-free. Let set total preorders W, T< , set
asymmetric restrictions.
Proposition 6 T< B set irreflexive relations B.
Secondly, following representation theorem shows belief state partitions
possible worlds sets worlds either equally likely potentially involved
conflict, totally orders sets; worlds distinct sets relation
sets.
Proposition 7 B iff partition W = hW0 , . . . , Wn W that:
1. every x Wi Wj , 6= j implies < j iff x y.
2. Every Wi either fully connected (w w0 w, w0 Wi ) fully disconnected
(w 6 w0 w, w0 Wi ).
Figure 1 shows three examples belief states: one total preorder, one
asymmetric restrictions total preorder, one neither. (Each circle
represents worlds W satisfy sentence inside. arc circles
indicates w w0 every w head circle w0 tail circle; arc indicates
w 6 w0 pairs. particular, set worlds represented circle
fully connected arc circle itself, fully disconnected otherwise.)
Thus, generalized belief states big change asymmetric restrictions
total preorders. merely generalize weakening assumption sets
worlds strictly ordered equally likely, allowing possibility conflicts.
distinguish agnostic conflicting conditional beliefs. belief state
162

fiRepresenting Aggregating Conflicting Beliefs

P

P

P

P

P

P

(a)

(b)

(c)

Figure 1: Three examples generalized belief states: (a) total preorder, (b) asymmetric restriction total preorder, (c) neither.

agnostic conditional belief p?q (i.e., Agn(p?q)) choice set worlds satisfying
p contains worlds satisfy q q fully disconnected. conflict
belief, written Con(p?q), choice set fully connected.
Finally, compare representational power definitions discussed
previous section. First, companion result Proposition 6, obvious B
subsumes class total preorders and, fact, set reflexive relations B.
Proposition 8 B set reflexive relations B.
Secondly, B neither subsumes subsumed set total, quasi-transitive relations,
intersection two classes . Let Q set total, quasi-transitive
relations W, Q< , set asymmetric restrictions.
Proposition 9
1. Q B = .
2. B 6 Q.
3. Q 6 B W least three elements.
4. Q B W one two elements.
modular, transitive relations represent strict preferences, probably fairer
compare class asymmetric restrictions total, quasi-transitive relations.
Again, neither class subsumes other, time intersection T< :
Proposition 10
1. Q< B = T< .
2. B 6 Q< .
3. Q< 6 B W least three elements.
4. Q< B W one two elements.
Note generalized belief states described extremely rich would require
optimization practice avoid high maintenance cost. Although issue somewhat
outside scope paper, address (in respective sections) ways minimize
explosion complexity complications fusion voting
introduced.
next section, define natural aggregation policy based new representation admits clear semantics obeys appropriately modified versions Arrows
conditions.
163

fiMaynard-Zhang & Lehmann

4. Single-Agent Belief State Construction
Suppose agent informed set sources, individual belief state.
Suppose, further, agent ranked sources level credibility. propose
operator constructing agents belief state aggregating belief states
sources accounting credibility ranking sources.
Example 1 use running example space robot domain help provide
intuition definitions. robot sends earth stream telemetry data gathered
spacecraft, long receives positive feedback data received.
point loses contact automatic feedback system, sends request
information agent earth find failure caused failure
feedback system overload data retrieval system. former case, would
continue send data, latter, desist. happens, overload,
computer running feedback system hung. agent consults following
three experts, aggregates beliefs, sends results back robot:
1. sp , computer programmer developed feedback program, believes nothing
could ever go wrong code, must overload problem.
However, admits program crashed, problem could ripple
cause overload.
2. sm , manager telemetry division, unfortunately out-dated information
feedback system working. also told engineer sold
system overloading could never happen. idea would happen
overload feedback system crashed.
3. st , technician working feedback system, knows feedback system
crashed, doesnt know whether data-overload. familiar
retrieval system, also unable speculate whether data retrieval system
would overloaded feedback system failed.
Let F propositional variables representing feedback data retrieval
systems, respectively, okay. belief states three sources shown Figure 2.
FD

FD

FD

F



F

FD

FD

F

sp

sm

st

Figure 2: belief states sp , sm , st Example 1.

164

fiRepresenting Aggregating Conflicting Beliefs

4.1 Sources
Let us begin formal development defining sources belief states:
Definition 7 finite set sources. source associated belief state
<s B.
denote agnosticism conflict relations source ./s , respectively.
possible assume belief state source conflict-free, i.e., acyclic. However,
necessary allow sources suffer human malady torn
possibilities.
assume agents credibility ranking sources total preorder built
totally ordered set ranks (e.g., integers).
Definition 8 R totally ordered finite set ranks.
Definition 9 rank : R assigns source rank. Also, S, ranks(S) denotes set {r R : S. rank(s) = r}.
Definition 10 total preorder induced ordering R denoted
w. is, w s0 iff rank(s) rank(s0 ); say credible s0 . restriction w
denoted wS .
use denote asymmetric symmetric restrictions w, respectively.7
finiteness (R) ensures maximal source (rank) always exists, necessary results. Weaker assumptions possible, price unnecessarily complicating discussion. Also observe R arbitrary totally
ordered set. Thus, allow numeric ranking systems (such integers),
non-numeric systems well (e.g., military ranks). Furthermore, generality allows
proposal easily accommodate applications new ranks need dynamically
added inconvenient impossible change rank labels existing sources
(e.g., large workers union members ranked relative level quality
experience).
ready consider source aggregation problem. following, assume
agent informed set sources S. look two special casesaggregation
equally ranked strictly ranked sourcesbefore considering general case.
4.2 Aggregating Equally Ranked Sources
Suppose sources rank wS fully connected. Intuitively,
want take offered opinions seriously, take union relations:

Definition 11 S, Un(S) relation sS <s .
simply taking union source belief states, may lose transitivity. However,
lose modularity:
Proposition 11 S, Un(S) modular necessarily transitive.
7. Note that, unlike relations representing belief states, w read intuitive way, is,
greater corresponds better.

165

fiMaynard-Zhang & Lehmann

Thus, know Proposition 1 need take transitive closure Un(S)
get belief state:
Definition 12 S, AGRUn(S) relation Un(S)+ .
Proposition 12 S, AGRUn(S) B.
Intuitively, simply inferring opinions implied conflicts introduced
aggregation. show formally consider general aggregation
operator below.
surprisingly, taking opinions sources seriously, may generate many
conflicts, manifested fully connected subsets W.
Example 2 Suppose three sources space robot scenario Example 1 considered equally credible, aggregate belief state fully connected relation
indicating conflicts every belief.
4.3 Aggregating Strictly Ranked Sources
Next, consider case sources strictly ranked, i.e., wS total order.
define lexicographic operator lower ranked sources refine belief states
higher ranked sources. is, determining ordering pair worlds, opinions
higher ranked sources generally override lower ranked sources, lower ranked
sources consulted higher ranked sources agnostic:
Definition 13 S, AGRRf(S) relation
n


0
(x, y) : S. x <s s0 S. s0 x .
AGRUn(S), AGRRf(S) guaranteed transitive, always modular:
Proposition 13 S, AGRRf(S) modular necessarily transitive.
However, case wS total order, result applying AGRRf guaranteed
belief state.
Proposition 14 wS total order, AGRRf(S) B.
Example 3 Suppose, space robot scenario Example 1, technician considered credible manager who, turn, considered credible
programmer. aggregate belief state, shown Figure 3, informs robot (correctly)
feedback system crashed, shouldnt worry overload problem
keep sending data.
4.4 General Aggregation
general case, may several ranks represented multiple sources rank.
instructive first consider following seemingly natural strawman operator,
AGR : First combine equally ranked sources using AGRUn, aggregate strictly
ranked results using essentially AGRRf.
166

fiRepresenting Aggregating Conflicting Beliefs

FD

FD

FD

FD

Figure 3: belief state aggregation Example 3 st sm sp .
Definition 14 Let S. r R, let <r = AGRUn({s : rank(s) = r})
r , corresponding agnosticism relation. AGR (S) relation



(x, y) : r R. x <r r0 ranks(S). r0 > r x r0 .
AGR indeed defines legitimate belief state:
Proposition 15 S, AGR (S) B.
Unfortunately, problem divide-and-conquer approach assumes
result aggregation independent potential interactions individual sources
different ranks. Consequently, opinions eventually get overridden may still
indirect effect final aggregation result introducing superfluous opinions
intermediate equal-rank aggregation step, following example shows:
Example 4 Let W = {a, b, c}. Suppose = {s0 , s1 , s2 } belief states
<s0 = {(b, a), (b, c)} <s1 =<s2 = {(a, b), (c, b)}, s2 s1 s0 . AGR (S)
{(a, b), (c, b), (a, c), (c, a), (a, a), (b, b), (c, c)}. sources agnostic c, yet
(a, c) (c, a) result transitive closure lower rank involving
opinions ((b, c) (b, a)) actually get overridden final result.
undesired effects, propose another aggregation operator circumvents problem applying refinement (as defined Definition 13) set
source belief states inferring new opinions via closure:
Definition 15 rank-based aggregation set sources S, denoted AGR(S),
AGRRf(S)+ .
Encouragingly, AGR outputs valid belief state:
Proposition 16 S, AGR(S) B.
output running space robot example also reasonable:
Example 5 Suppose, space robot scenario Example 1, technician still considered credible manager programmer, latter two considered
167

fiMaynard-Zhang & Lehmann

F

FD

FD

Figure 4: belief state aggregation Example 5 st sm sp .
equally credible. aggregate belief state, shown Figure 4, still gives robot correct information state system. robot also learns future reference
disagreement whether would data overload
feedback system working.
Furthermore, observe AGR, applied set sources Example 4,
indeed bypass problem described extraneous opinion introduction:
Example 6 Assume W, S, w Example 4; AGR(S) = {(a, b), (c, b)} desired. concerned reader may note s2 dictator sense s2 opinions
override opposing opinions. However, reasonable context
sources strictly lower rank.
observe AGR behaves well special cases weve considered, reducing
AGRUn sources equal rank, AGRRf sources totally
ranked:
Proposition 17 Suppose S.
1. wS fully connected, AGR(S) = AGRUn(S).
2. wS total order, AGR(S) = AGRRf(S).
Another property AGR transitive closure part minimally extends result
AGRRf make complete (i.e., conflicts represented explicitly) sense new
opinions added worlds already involved conflict:
Proposition 18 Suppose S, = AGRRf(S), = AGR(S), x 6 x, W.
x y, x ./ y.
One small observation: AGR() = property definition, reflecting fact
generate opinions nothing.
4.5 Arrow, Revisited
Finally, strong argument favor AGR satisfies Arrows conditions. Technically, setting slightly different Arrows, need modify
condition appropriate setting, yet retains intended spirit
original condition. Let f operator aggregates belief states <s1 , . . . , <sn
W n sources s1 , . . . , sn S, respectively, let = f (<s1 , . . . , <sn ), let wS
total preorder S. consider condition separately.
168

fiRepresenting Aggregating Conflicting Beliefs

Restricted range setting, output aggregation function modular, transitive belief state rather total preorder considered Arrow.
Definition 16 (modified) Restricted Range: range f B.
Unrestricted domain Similarly, input aggregation function modular,
transitive belief states sources rather total preorders.
Definition 17 (modified) Unrestricted Domain: i, <si member B.
Pareto principle Arrows setting, relations represented non-strict relative likelihood (preference, actually) asymmetric restrictions relations used
define Pareto principle. However, setting, generalized belief states already
represent strict likelihood. Consequently, use actual input output relations
aggregation function place asymmetric restrictions define Pareto
principle. Obviously, AGRs ability introduce conflicts, satisfy
original formal Pareto principle would essentially require sources
unconflicted belief one world strictly likely another, must also
true aggregate belief state. Neither condition necessarily stronger other.
Definition 18 (modified) Pareto Principle: x <si i, x y.
Independence irrelevant alternatives Conflicts defined terms cycles,
necessarily binary. allowing existence conflicts, effectively made possible
outside worlds affect relation pair worlds, viz., involving
cycle. result, need weaken IIA say relation worlds
independent worlds unless worlds put conflict. makes
intuitive sense: two worlds put conflict aggregation due cycle involving
worlds, may need access worlds able detect conflict.
Definition 19 (modified) Independence Irrelevant Alternatives (IIA): Suppose
0
0
s01 , . . . , s0n si s0i i, 0 = f (<s1 , . . . , <sn ). suppose x <si
0
iff x <si i, x6 ./ y, x6 ./0 y. x iff x 0 y.
Non-dictatorship Pareto principle definition, use actual input
output relations define non-dictatorship since belief states represent strict likelihood.
perspective, setting requires informant sources highest rank
dictators sense considered Arrow. However, setting originally considered
Arrow one individuals ranked equally. Thus, make explicit
new definition non-dictatorship adding pre-condition sources
equal rank. Now, AGR treats set equally ranked sources equally taking
opinions seriously, price introducing conflicts. So, intuitively, dictators.
However, Arrow account conflicts formulation, sources
dictators definition. need modify definition non-dictatorship
say source always push opinions without ever contested.
Definition 20 (modified) Non-Dictatorship: si sj i, j,
that, every combination source belief states every x, W, x <si 6<si x
implies x 6 x.
169

fiMaynard-Zhang & Lehmann

show AGR indeed satisfies conditions:
Proposition 19 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR(S). AGRf
satisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,
IIA, non-dictatorship.

5. Multi-Agent Fusion
far, considered case single agent must construct update
belief state informed set sources. Multi-agent fusion process
aggregating belief states set agents, respective set informant
sources. proceed formalize setting.
5.1 Formalization
agent informed set sources S.8 Agent induced belief state
belief state formed aggregating belief states informant sources, i.e., AGR(S).
use denote special agents informed S, respectively.
Assume set agents fuse agree upon rank (and, consequently, w).9 define
fusion set agent informed combination informant sources:
Definition 21 Let = {A1 , . . . , } set agents agent
Sn Ai informed
Si S. fusion A, written (A), agent informed = i=1 Si .
surprisingly given set-theoretic definition, fusion idempotent, commutative,
associative. properties guarantee invariance required multi-agent belief aggregation applications space robot domain.
5.2 Computing Fusion Efficiently
multi-agent space robot scenario described Section 1, direct need
belief states result fusion. interested belief states
original sources far want fused belief state reflect informant history.
obvious question whether possible compute belief state induced
agents fusion solely initial belief states, is, without reference
belief states informant sources. highly desirable expense
storingor, case space robot example, transmittingall source belief
states; would like represent agents knowledge compactly possible.
fact, sources equal rank. simply take transitive
closure union agents belief states:
Ai , agent induced belief state,
Proposition 20 Let Definition

+
21,





induced
belief state.
wS , fully connected. = (A),
Ai

8. source thought primitive agent fixed belief state.
9. could easily extend framework allow individual rankings, felt small gain
generality would justify additional complexity loss perspicuity. Similarly, could
consider agent credibility ordering informant sources. However,
unclear how, example, crediblity orderings disjoint sets sources combined
new credibility ordering since union total.

170

fiRepresenting Aggregating Conflicting Beliefs

Unfortunately, equal rank case special. sources different ranks,
generally cannot compute induced belief state fusion using agent belief
states fusion, following simple example demonstrates:
Example 7 Let W = {a, b}. Suppose two agents A1 A2 informed sources s1
belief state <s1 = {(a, b)} s2 belief state <s2 = {(b, a)}, respectively. A1 belief state
s1 A2 s2 s. s1 s2 , belief state induced
(A1 , A2 ) <s1 , whereas s2 s1 , <s2 .
Thus, knowing belief states fused agents sufficient computing
induced belief state. need maintain information agents informants.
question whether better storing original sources.
might wonder whether possible somehow compute credibility rank
agent based credibility informant sources, simply apply AGR
agents induced belief states. works fine if, every pair agents, informants
one credible other. However, work general
agent informants less credible another agent
following example demonstrates:
Example 8 Let W = {a, b, c}. Suppose agent A1 informed source s1 belief state
<s1 = {(a, b), (b, c), (a, c)}, suppose agent A2 informed sources s0 s2 belief
states <s0 = {(c, b), (b, a), (c, a)} <s2 = {(b, a), (c, a)}, respectively. suppose
s2 s1 s0 . A1 induced belief state <s1 A2 <s0 . belief state induced
(A1 , A2 ) {(b, c), (c, a), (b, a)}. otherhand, rank A1 A2 apply
AGR induced belief states, get <s1 ; rank A2 A1 , get <s0 ; and,
rank equally, get fully connected belief state. obviously
incorrect.
Hence, need store information source opinion. However,
still better keeping sources around sources totally preordered
credibility. enough store opinion AGRRf(S) rank highest
ranked source supporting it. define pedigreed belief states enrich belief states
additional information:
Definition 22 Let agent informed set sources S. pedigreed belief
state pair (, l) = AGRRf(S) l : R l((x, y)) = max({rank(s) :
x <s y, S}). use
r denote restriction pedigreed belief state r,

is, r = {(x, y) : l((x, y)) = r}.
verify pairs label is, fact, rank source used determine pairs
membership AGRRf(S), higher ranked source:
Proposition 21 Let agent informed set sources pedigreed
belief state (, l).
r relation
n


0
(x, y) : S. x <s r = rank(s) s0 S. s0 x .
belief state induced pedigreed belief state (, l) is, obviously, transitive closure
.
171

fiMaynard-Zhang & Lehmann

Now, given pedigreed belief states set agents, compute
new pedigreed belief state fusion. simply combine labeled opinions using
refinement techniques. call operation pedigreed fusion:
Definition 23 Let Definition 21, wS , total preorder, PA , set
pedigreed belief states agents A. pedigreed fusion PA , written ped (PA ),
(, l)
1. relation
n


Aj
0
0

(x, y) : Ai A, r R. x




A,
r

R.
r
>
r

x


j
r
r0
W,

2. l : R l((x, y)) = max({r : x
r y, Ai A}).

Proposition 22 Let A, PA , S, wS Definition 23. ped (PA )
pedigreed belief state (A).
perspective induced belief states, essentially discarding unlabeled
opinions (i.e., derived closure operation) fusion. Intuitively,
learning new information may need retract inferred opinions.
fusion, re-apply closure complete new belief state. Interestingly, special
case sources strictly-ranked, closure unnecessary:
Proposition 23 A, PA , Definition 23, wS total order,
ped (PA ) = (, l), + =.
Let us return space robot scenario considered Example 1 illustrate
pedigreed fusion.
Example 9 Suppose arrogant programmer part telemetry team, instead works company side country. robot request
information two separate agents, one query manager technician one
query programmer. Assume agents robot rank sources same,
assigning technician rank 2 two agents rank 1, induces
credibility ordering used Example 5. agents pedigreed belief states result
fusion shown Figure 5.
first agent provide information overloading second agent
provides incorrect information. However, see fusing two, robot
belief state identical computed Example 5 one agent
informed three sources (weve separated top set worlds show
labeling). Consequently, knows correct state system. And, satisfyingly,
final result depend order robot receives agents reports.
savings obtained required storage space scheme substantial. Suppose set agents informant sources, n = kWk, = kSk. Explicitly storing
(along rank source) requires O(n2 m) amount space; worst case
bound reached sources belief states fully connected relations.
172

fiRepresenting Aggregating Conflicting Beliefs

FD
1
FD

FD

1
FD
2

1
2

1

1
FD

1
FD

FD

1
2 2

1

2

FD

2

1

1

F

FD

FD

A1

A2

(A1 , A2)

Figure 5: pedigreed belief states agent A1 informed sm st agent A2
informed sp , result fusion Example 9.

hand, storing pedigreed belief state requires O(n2 ) space.10 Moreover,
enriched representation allow us conserve space, also provides potential
savings efficiency computing fusion since, pair worlds, need
consider opinions agents rather sources combined set
informants.
Incidentally, used strawman AGR basis general aggregation,
simply storing rank maximum supporting sources would give us sufficient
information compute induced belief state fusion. demonstrate this, give
example two pairs sources induce annotated agent belief states, yet
yield different belief states fusion:
Example 10 Let W, S, w Example 4. Suppose agents A1 , A2 , A01 , A02
informed sets sources S1 , S2 , S10 , S20 , respectively, S1 = S2 = {s2 },
S10 = {s0 , s2 }, S20 = {s1 , s2 }. AGR dictates pedigreed belief states four
agents equal <s2 opinions annotated rank(s2 ). spite indistinguishability, = ({A1 , A2 }) A0 = ({A01 , A02 }), induced belief state equals <s2 ,
i.e., {(a, b), (c, b)}, whereas A0 {(a, b), (c, b), (a, c), (c, a), (a, a), (b, b), (c, c)}.
Also notice Maynard-Reid II Shoham (2001) consider essentially special
case fusing two agents informed strictly-ranked sources. show surprising
result standard AGM belief revision modeled fusion two agents,
informant informee, informants sources strictly credible
informees. Furthermore, show that, clean set-theoretic semantics,
fusion provides attractive, semantically well-behaved solution difficult problem
iterated belief revision. general fusion definition satisfies examples iterated
fusion describe.
10. bounds assume amount space needed store rank bounded small
constant.

173

fiMaynard-Zhang & Lehmann

6. Incorporating Voting
potential drawback framework described account
strength support. example, cannot differentiate situation
one thousand sources highest rank support < b one source rank
supports b < a, situation one source supports < b thousand
sources support b < a. cases framework yields simple conflict
b rather acknowledging overwhelming support one way other.
additional information strength support often sufficient resolve would
otherwise appeared conflict.
address problem, generalize framework incorporate voting. first
describe family aggregation operators based voting AGR special case.
process, introduce novel modular closure operator. discuss properties
special members family including indiscriminate aggregation, simple majority,
unanimity, well attractive properties family whole. describe
extension setting accommodate ranked individuals individuals higher
rank given precedence aggregation. Finally, consider fusion.
6.1 Voting Functions
use pairwise voting strategy similar well-known Condorcets method. (For
Condorcet method methods results standard voting
theory cite, see Blacks (1958) classical reference voting theory). Condorcets method
considers pair worlds separately, ranking world x world aggregate
votes ranking x. one world
beats ties worlds, known Condorcet winner. deviate
method use fixed threshold proportion support decide acceptance
opinion aggregate rather size support relative
opposite opinion. Let countS (x, y) = k{s : x <s y}k S.
Definition 24 Let S. p [0, 1], voting function p, written vtp , maps
relation
{(x, y) : countS (x, y) > 0 countS (x, y)/kSk p}.
definition falls class voting systems Black (1958) calls absolute majority
systems. motivated observation relative support many times less relevant
strength support. support two possible rankings two worlds may
low neither justifiably considered part aggregate belief state. Similarly,
support alternative rankings may high may reasonable
introduce create conflict rather choose one slightly higher support.
strategy appropriate applications, course, many instances
appropriate. Also, method satisfies generalization Condorcet
criterion, widely accepted criteria good voting systems requires Condorcet
winner, exists, likely world aggregate belief state. method never
produces strict ranking two worlds opposite Condorcets method, although
cases Condorcets method ranks one world strictly likely
another method produces agnosticism conflict. result, Condorcet
174

fiRepresenting Aggregating Conflicting Beliefs

winner always among likely worlds. rate, aggregation results
depend significantly choice voting strategy; one could easily replace
different strategy desired.
said, make observations voting functions. First, voting
function definition requires accept opinion least p proportion sources
support it. However, often want specify opinions accepted strictly
cutoff proportion sources support it. example, best-known
voting function majority function accept opinion gets
50% vote. easily specify majority vote function vt0.5+ (S)
0 < < 0.5/kSk (e.g., = 0.25/kSk) tied opinions rejected. general,
accept opinions garnering p proportion vote (for 0 p < 1),
suffices use function vtp+ (S)
0<<

1 pkSk + bpkSkc
kSk

e.g., = (1 pkSk + bpkSkc)/(2kSk).11
Second, immediately obvious aggregate relation may contain conflicts
p 0.5, even original source belief states conflict-free. fact, possible
get conflicts aggregate conflict-free belief states even larger p, following
famous example demonstrates:
Example 11 Let W = {a, b, c} 1/3 sources belief state
{(a, b), (b, c), (a, c)}, 1/3 {(b, c), (c, a), (b, a)}, 1/3 {(c, a), (a, b), (c, b)}.
vtp (S) = {(a, b), (b, c), (c, a)}, cycle, 1/3 < p 2/3. known Condorcet
paradox (cf. Brams Fishburns (2002) voting survey).
Many solutions proposed resolving conflicts using Borda counts
instant runoff voting (aka single transferable vote) (cf. Center Voting Democracy,
2002) two popular examples. before, attempt resolve conflicts but,
instead, make explicit way supports flexibility choice resolution
methodology allows semantically well-behaved iteration aggregation.
Third, end-point members family voting functions special significance.
voting function 0 equivalent union operator saw earlier takes
opinions seriously, i.e., indiscriminate:
Proposition 24 S, vt0 (S) = Un(S).
extreme, voting function 1. case, equivalent
taking intersection sources belief states, i.e., accepting unanimous opinions:

Proposition 25 S, vt1 (S) = sS <s .
contrast vt0 generates many conflicts, vt1 generates lot agnosticism.
Fourth, voting functions opinion-centered; is, proportion agnostic
sources larger p, voting function p necessarily reflect agnosticism
would case opinion. If, example, belief states three sources
11. bxc denotes floor x, i.e., largest integer less equal x.

175

fiMaynard-Zhang & Lehmann

W = {a, b} {(a, b)}, {(b, a)}, {}, respectively, voting function p = 1/3
produce conflict respect b, agnosticism. However, say
abstainers impact final result. fact abstainers counted
among total number voters effect agnosticism respect pair
worlds counts vote possible opinions. issue usually arise
standard voting schemes usually assume sources totally rank candidates.
However, important observation members family voting operators produce belief states general. weve already shown, vt0 produces
modular relation necessarily transitive. end spectrum, vt1
produces transitive relation necessarily modular:
Proposition 26 Suppose S. vt1 (S) transitive necessarily modular.
members family, result may neither modular transitive,
Condorcet paradox Example 11 illustrates 1/3 < p 2/3. fact,
construct scenario every 0 < p < 1:
Proposition 27 kWk 3, every p (0, 1), exists vtp (S)
neither modular transitive.
Part problem voting may introduce conflicts may imply conflicts.
before, need take transitive closure infer implied conflicts.
Condorcet paradox example, produces fully connected belief state would
hope. Unfortunately, closing transitivity necessarily restore modularity
well, following example demonstrates:
Example 12 Let W = {a, b, c} 1/3 sources belief state
{(a, b), (b, c), (a, c)}, 1/3 {(b, c), (c, a), (b, a)}, 1/3 {(b, a), (b, c)}. Then,
p > 2/3, vtp (S) = vtp (S)+ = {(b, c)} modular.
solve problem defining natural modular closure operation converts
transitive relation belief state. define modular-transitive closure
operation take result arbitrary voting function transform
belief state using transitive closure followed modular closure.
6.2 Modular-Transitive Closure
start defining helper function returns level world relation, i.e.,
length longest path (along strict edges) world member choice
set W. convenience, throughout modular-transitive closure subsection
use denote arbitrary relation, ./ denote asymmetric symmetric
restrictions, respectively.
Definition 25 level x W transitive relation W, written lev (x),
(
0
x ch(W, )
lev (x) =
1 + max ({lev (y) : x}) otherwise.
yW

(Recall ch choice set function defined Definition 2.) following simple
properties relating lev immediate:
176

fiRepresenting Aggregating Conflicting Beliefs

Proposition 28 Suppose transitive relation W x, W.
1. x y, lev (x) < lev (y).
2. x ./ y, lev (x) = lev (y).
3. lev (x) < lev (y), z. lev (z) = lev (x) z y.
4. lev (x) = lev (y), x iff x.
define modular closure relation relation results fully
connecting equi-level alternatives unless fully disconnected:
Definition 26 modular closure MC() transitive relation W relation
(x, y) MC() iff
1. lev (x) < lev (y)
2. lev (x) = lev (y) x0 , 0 . lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 .
Intuitively, long reason doubt pair level interchangeable,
doubt pairs level, then. Note definition MC similar
one equivalent constructions rational closure Lehmann Magidor (1992)
describe.
see MC indeed makes transitive relation modular preserving transitivity:
Proposition 29 transitive relation W, MC() B.
MC additive process changes relation minimally achieve modularity
preserving transitivity levels worlds:
Proposition 30 Suppose transitive relation W = MC().
1. .
2. modular, =.
3. lev (x) = lev (x) x W.
4. 0 B 0 lev0 (x) = lev (x) x W, 0 .
define modular, transitive closure arbitrary relation MC applied
transitive closure , show result belief state:
Definition 27 modular, transitive closure MT() relation W relation MC (+ ).
Proposition 31 relation W, MT() B.
MT also minimally additive operator:
Proposition 32 Suppose relation W = MT().
1. .
2. transitive, = MC().
3. modular, =+ .
4. modular transitive, =.
5. conflicts, neither .
177

fiMaynard-Zhang & Lehmann

6.3 Aggregation Family
fully equipped solve problem incorporating voting aggregation.
First, consider special case sources rank. aggregation
operators construct aggregate belief state first applying voting, closing
MT:
Definition 28 p [0, 1], AGREqp (S) = MT (vtp (S)).
Proposition 33 p [0, 1], AGREqp (S) B.
easily generalize definition accommodate ranking sources.
accept opinion enough individuals highest rank opinion support it,
close MT:
Definition 29 p [0, 1], AGRRf p (S) relation
n


0
(x, y) : S. x <s (x, y) vtp ({s0 : s0 s}) s0 S. s0 x .
Definition 30 p [0, 1], AGRp (S) = MT(AGRRf p (S)).
Proposition 34 p [0, 1], AGRp (S) B.
aggregation functions encountered far special cases general
family:
Proposition 35 Suppose p [0, 1].
1. wS fully connected, AGRp (S) = AGREqp (S).
2. wS total order, AGRp (S) = AGRRf p (S) = AGRRf(S) = AGR(S).
3. AGR0 (S) = AGR(S).
obvious consequence last property, AGR0 satisfies modified Arrovian conditions.
Corollary 35.1 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR0 (S). AGRf
satisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,
IIA, non-dictatorship.
6.4 Fusion
Fusion still defined Definition 21, i.e., belief state created fusion set
agents aggregate belief state agents cumulative informant sets. However,
use AGRp rather AGR compute aggregate belief state.
again, want compute fusion without storing belief states
informant sources, possible. Unfortunately, possible general aggregation
functions based voting. reason need keep track actual identity
sources supporting opinion avoid double-counting sources shared
multiple agents.
178

fiRepresenting Aggregating Conflicting Beliefs

However, often better O(n2 m) space required store full sources,
n = kWk number informant sources. store parts
matter. Specifically, given source, store opinions source
one highest ranked supporting opinion corresponding worlds.
effectively accomplish extending pedigreed belief state label
opinion rank highest ranking sources supporting opinion
corresponding worlds, also set unique identifiers sources supporting
particular opinion. also maintain table stores, rank represented
set informant sources, set identifiers sources rank. call
resulting representation support pedigreed belief state.
Definition 31 Let agent informed set sources S. support pedigreed
belief state triple (l, sup, rtab)
l : W W R {} l(x, y) = max({rank(s) : x 6s y, S} {})
6 R < r r R,
sup : W W 2S sup(x, y) = {s : rank(s) = l(x, y), x <s y},
rtab : ranks(S) 2S rtab(r) = {s : rank(s) = r}.
Note l symmetric: l(x, y) = l(y, x). hand, sup not. Now,
easily compute agents belief state support pedigreed belief state. compute
proportion support particular opinion, simply divide size support
set opinion number informant sources labeled rank.
Proposition 36 Let agent informed set sources S, support pedigreed belief state (l, sup, rtab), using aggregation function AGRp p [0, 1]. belief
state relation
MT ({(x, y) : ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p})
Observe that, unlike pedigreed belief states, support pedigreed belief states label
possible opinions, appearing agents induced belief state, i.e., whose
support falls threshhold. reason another agent may come along later
enough new votes cross threshold, case votes earlier sources
become relevant. Similarly, support pedigreed belief states maintain rank information even
ranks appearing labels. source particular rank may currently support
opinion, another agent may later bring sources rank supporting opinion
hitherto unsupported source equal higher rank. correct computation
proportion support opinion must take account earlier sources.
address fusion, let us consider space required store support pedigreed
belief state (l, sup, rtab). l requires O(n2 ) space, rtab requires (m) space, and, rmax
denotes number sources rank sources rank, sup
requires O(n2 rmax ) space, total O(n2 rmax + m) space.12 Thus, best-case
scenario where, example, sources strictly ranked, support pedigreed belief state
requires O(n2 + m) space since opinion one supporter. However,
12. before, assume representing ranks requires constant space. assume represent
source label constant space well.

179

fiMaynard-Zhang & Lehmann

worst-case scenario where, example, sources equally ranked rmax = m,
still need O(n2 m) space.
Now, computing support pedigreed belief state resulting fusion straightforward. opinion, set l highest l value opinion among agents
set sup union sup sets opinion agents l value.
rank represented agents rank table, set rtab union
rtab sets agents defined.
Definition 32 Let Definition 21, wS , total preorder, PA , set
support pedigreed belief states agents A. support pedigreed fusion PA ,
written sup (PA ), (l, sup, rtab)
1. l : R l((x, y)) = max({l0 (x, y) : (l0 , sup0 , rtab0 ) PA }),
2. sup : W W 2S
[

sup(x, y) =

sup0 (x, y),

(l0 ,sup0 ,rtab0 )PA , l0 (x,y)=l(x,y)


3. rtab : ranks(S) 2S
[

rtab(r) =

rtab0 (r).

(l0 ,sup0 ,rtab0 )PA , rrange(rtab0 )

Proposition 37 Let A, PA , S, wS Definition 32. sup (PA )
support pedigreed belief state (A).
Thus, addition potential savings space gained using support pedigreed
belief states, also potentially save time needed compute fusion since, given
opinion, need consider opinions lesser ranked sources.

7. Related Work
Much work belief aggregation geared towards unbiased kinds belief
pooling. Besides work social choice described Section 3.2, recent attempts
belief revision community (e.g. Borgida & Imielinski, 1984; Baral, Kraus, Minker, &
Subrahmanian, 1992; Liberatore & Schaerf, 1995; Makinson, 1997; Revesz, 1997; Konieczny
& Perez, 1998; Meyer, 2001; Benferhat, Dubois, Kaci, & Prade, 2002) sought modify
AGM theory capture fair revisions, is, revisions revisee revisers
beliefs treated equally seriously. Like proposal, Benferhat et al. Meyer accommodate iterative merging. Benferhat et al.s proposal also distinct approach
problem possibilistic logic point view. Besides restriction equally-ranked
sources, fairness-based proposals differ generally syntactic
nature sense sentences prioritized rather possible worlds. Meyers
proposal exception; belief states epistemic states, structures style
Spohns (1988) ordinal conditional functions (aka -rankings). fact, Meyer, Ghose,
180

fiRepresenting Aggregating Conflicting Beliefs

Chopra (2001) shown number simple aggregation operators epistemic
belief states also satisfy Arrows postulates appropriately modified context
(unrestricted domain, restricted range, IIA particular need modification). Unfortunately, epistemic states enriched total preorders and, thus, suffer problems
described earlier, i.e., inability explicitly handle conflicts.
Cantwells (1998) work also syntactic nature, allow sources differing credibility. Cantwell addresses complementary problem own: deciding
information reject given subset informing sources rejecting information.
assumes generalization credibility ordering, partial preorder sets sources.
explores ways inducing partial preorder sentences based ordering,
uses ordering determine subset (although all) sentences reject. Another difference work considers non-counterfactual beliefs
sources.
not, course, first consider using lexicographic ordering aggregation purposes. Lexicographic operators long studied fields management
social science; Fishburn (1974) gives good survey much work. recently, researchers artificial intelligence taken interest operators; examples include Grosof (1991), Maynard-Reid II Shoham (2001) Andreka, Ryan,
Schobbens (2002).
Grosof uses lexicographic aggregation preorders means tackling problem
default reasoning presence conflicting defaults. Besides general preorders
aggregated, another interesting difference work although Grosof
allow sources equal rank, allow sources incomparable rank, i.e.,
ranking sources strict partial order. Thus, extreme case ordering
completely disconnected, operator reduces Un operator (and, thus,
necessarily preserve transitivity).
Andreka et al., hand, frame work context preference aggregation. go one step Grosof allow input relations arbitrary.
prove lexicographic operator one satisfies variation
Arrows properties unanimity, IIA, preservation transitivity, weaker version
non-dictatorship. (We point perspective Arrows original framework, relation highest priority always dictator.) describe collection
properties besides transitivity preserved operator. However, work,
preserve totality.
work derives much inspiration Maynard-Reid II Shohams work.
restrict attention total preorders, create problems
assume sources totally ordered. focus, instead, strong connection
belief aggregation iterated belief revision. show used
iterated belief operator AGM-based setting, compare properties
representative sampling well-known iterated belief operator proposals
Boutiliers (1996) natural revision, Darwiche Pearls (1997) operators, Spohns (1988)
ordinal conditional function revision, Lehmanns (1995) widening rank model revision,
Williamss (1994) conditionalization adjustment operators. show
operator among semantically well-behaved: results
operators depend order iteration.
181

fiMaynard-Zhang & Lehmann

Finally, knowledge, none related approaches outside social choice
yet extended incorporate voting.

8. Conclusion
described semantically clean representation class modular, transitive
relations collective qualitative beliefs allows us represent conflicting opinions
without sacrificing ability make decisions. proposed intuitive operator
takes advantage representation agent combine belief states
set informant sources totally preordered credibility. showed operator
circumvents Arrows Impossibility result satisfactory manner. also described
mechanism fusing belief states different agents iterates well extended
framework incorporate voting.
assumed agents share credibility ranking sources. general,
rankings vary among agents, even change time. Furthermore, agents
ranking function depend context; different sources may different areas
expertise. Exploring behavior fusion general settings obvious next
step.
Note although described operators incorporate voting, condition ever side lower rank sources conflict higher rank
sources, matter many disagreeing lower rank sources are. aggregation scheme behaves differently would built fundamentally different
assumptions framework.
Another problem deserves study developing fuller understanding
properties Bel, Agn, Con operators interrelate.

Acknowledgments
want thank Yoav Shoham anonymous referees paper insightful
comprehensive feedback. preliminary version paper appears Proceedings
Seventh International Conference Principles Knowledge Representation
Reasoning (KR2000) (the first authors last name Maynard-Reid II time).
work partially supported National Physical Science Consortium Fellowship
Jean et Helene Alfassa Fund Research Artificial Intelligence.

Appendix A. Proofs
Proposition 1
1. transitive closure modular relation modular.
2. Every transitive relation quasi-transitive.
3. (Sen, 1986) Every quasi-transitive relation acyclic.
Proof:
1. Suppose relation finite set modular, + transitive closure . Suppose x, y, z x + y. exist w0 , . . . , wn
182

fiRepresenting Aggregating Conflicting Beliefs

x = w0 wn = y. Since modular w0 w1 , either w0 z z w1 .
former case, x = w0 z, x + z. latter case, z w1 wn = y,
z + y.
2. Suppose finite set, x, y, z , transitive relation , <
asymmetric restriction. Suppose x < < z. x y, 6 x, z,
z 6 y. x z imply x z, z 6 x imply z 6 x,
transitivity. x < z.
2
Proposition 2 (Sen, 1986) Given relation finite set , choice set operation
ch defines choice function iff acyclic.
Proof:

See Sens (1986) proof. 2

Proposition 3 (Arrow, 1963) aggregation operator satisfies restricted
range, unrestricted domain, (weak) Pareto principle, independendence irrelevant alternatives, nondictatorship.
Proof:

See Arrows (1963) proof. 2

Proposition 4 Let relation finite set let symmetric restriction.
total quasi-transitive transitive, transitive.
Proof: Let total, quasi-transitive, non-transitive relation. Suppose x
z x 6 z. totality, z x, z x. x y, z quasi-transitivity,
contradiction. Thus, x y. Similarly, z, x, contradiction, z.
z x, x 6 z. Therefore, transitive. 2
Proposition 5 Suppose relation transitive corresponding agnosticism
relation. transitive iff modular.
Proof: Suppose transitive suppose x z, x, y, z W. prove contradiction: Suppose x 6 6 z. transitivity, z 6 6 x, x z.
assumption, x z, x 6 z, contradiction.
Suppose, instead, modular suppose x z, x, y, z W. x 6 y,
6 x, 6 z, z 6 y. modularity, x 6 z z 6 x, x z. 2
Proposition 6 T< B set irreflexive relations B.
Proof: Let x, y, z W. first show T< B. Let T< . exists
asymmetric restriction . definition, transitive,
Proposition 1, . Suppose x y. x 6 x. Since total,
x z z x. Suppose x z. z, z 6 x (otherwise x transitivity,
contradiction), x z. if, hand, 6 z, z totality,
z y. Suppose, instead, z x. z transitivity 6 z (otherwise x
transitivity, contradiction), z y. Thus, x z z y, modular.
show B T< irreflexive. T< , asymmetric,
irreflexive. Suppose, instead, irreflexive. define relationship , show
183

fiMaynard-Zhang & Lehmann

asymmetric restriction, show . Let defined x iff 6 x.
first show asymmetric restriction . Suppose 0 asymmetric
restriction . x 0 y, x 6 x, x y. If, instead, x y, 6 x.
totality, x y, x 0 y. next show . x 6 x. Otherwise,
x y. since irreflexive, 6 x (otherwise x x transitivity), x
total. Next, suppose x z. 6 x z 6 y. modularity, z 6 x,
x z, and, thus transitive. 2
Proposition 7 B iff partition W = hW0 , . . . , Wn W that:
1. every x Wi Wj , 6= j implies < j iff x y.
2. Every Wi either fully connected (w w0 w, w0 Wi ) fully disconnected
(w 6 w0 w, w0 Wi ).
Proof: refer conditions proposition conditions 1 2, respectively.
prove direction proposition separately.
(=) Suppose B, is, modular transitive relation W. use
series definitions lemmas show partition W exists satisfying conditions 1
2. first define equivalence relation partition W. Two elements
equivalent look perspective every element W:
Definition 33 x iff every z W, x z iff z z x iff z y.
Lemma 7.1 equivalence relation W.
Proof: Suppose x W. every z W, x z iff x z z x iff z x, x x.
Therefore, reflexive.
Suppose x, W x y. every z W, x z iff z z x iff z y.
every z W, z iff x z z iff z x. Therefore, x,
symmetric.
Suppose x, y, z W, x y, z. Suppose w W. definition
, x w iff w w x iff w y, w iff z w w iff w z. Therefore,
x w iff z w w x iff w z. Since w arbitrary, x z, transitive. 2
partitions W equivalence classes. use [w] denote equivalence class
containing w, is, set {w0 W : w w0 }. Observe two worlds conflict
always appear equivalence class:
Lemma 7.2 x, W x ./ y, [x] = [y].
Proof: Suppose x, W x ./ y. Since [x] equivalence class, suffices show
[x], is, x y. Suppose z W. transitivity, x z, z; z,
x z; z x, z y; and, z z x. Thus, x z iff z z x
iff z y, since z arbitrary, x y. 2
define total order equivalence classes:
Definition 34 x, W, [x] [y] iff [x] = [y] x y.
184

fiRepresenting Aggregating Conflicting Beliefs

Lemma 7.3 well-defined, is, x x0 0 , x iff x0 0 ,
x, x0 , y, 0 W.
Proof: Suppose x x0 0 , x, x0 , y, 0 W. definition , every
z W, x z iff x0 z. particular, x iff x0 y. Also definition ,
every z 0 W, z 0 iff z 0 0 . particular, x0 iff x0 0 . Therefore, x iff x0 0 .
2
Lemma 7.4 total order equivalence classes W defined .
Proof: Suppose x, y, z W. first show total. definition , x
x, [x] [y] [y] [x], respectively. Suppose x 6 6 x, suppose
z W. modularity , x z implies z, z implies x z, z x implies z y,
z implies z x, x y. Therefore, [x] = [y], [x] [y] definition .
Next, show anti-symmetric. Suppose [x] [y] [y] [x]. [x] = [y]
x x. former case done, latter, result follows
Lemma 7.2.
Finally, show transitive. Suppose [x] [y] [y] [z]. Obviously,
[x] = [y] [y] = [z], [x] [z]. Suppose not. x z, x z
transitivity . Therefore, [x] [y] definition . 2
name members partition W0 , . . . , Wn Wi Wj iff j,
n integer. naming exists since every finite, totally ordered set isomorphic
finite prefix integers.
check partition satisfies two conditions. first condition,
suppose x Wi , Wj , 6= j. want show < j iff x y. Since 6= j,
[x] 6= [y]. Suppose < j. j, [x] [y]. Since [x] 6= [y], x definition
. suppose, instead, x y. [x] [y] definition , j. Since
[x] 6= [y], 6 x Lemma 7.2. Since [x] 6= [y] 6 x, [y] 6 [x] definition ,
j 6 i. Thus, < j.
Finally, show Wi either fully connected fully disconnected. Suppose
x, y, z Wi x z. suffices show x x iff z. definition
, x x iff x, x x iff x z. Suppose x x. Then, x x z, z
transitivity . Suppose now, x 6 x. Then, 6 x x 6 z, 6 z modularity
.
(=) Suppose W = hW0 , . . . , Wn partition W relation W satisfying given conditions. want show modular transitive. first
give following lemma:
Lemma 7.5 Suppose W partition W relation W satisfying condition 1. Wi , Wj W, x Wi , Wj , x y, j.
Proof:

= j, done. Suppose 6= j. Then, since x y, < j condition 1. 2

show modular. Suppose x Wi , Wj , x y. j
Lemma 7.5. Suppose z Wk . k k j modularity . Suppose < k
k < j. x z z condition 1. Otherwise = k = j, x, y, z Wi . Since
x y, Wi fully connected condition 2, x z (and z y).
185

fiMaynard-Zhang & Lehmann

Finally, show transitive. Suppose x Wi , Wj , z Wk , x y,
z. Lemma 7.5, j j k, k transitivity . Suppose < k.
x z condition 1. Otherwise = k = j, x, y, z Wi . Since x y, Wi fully
connected condition 2, x z. 2
(END PROPOSITION 7 PROOF)
Proposition 8 B set reflexive relations B.
Proof: first show B. Let x, y, z W. definition, transitive. Suppose x y. Since total, x z z x. z x, z transitivity,
modular. hand, empty relation W modular transitive,
total and, consequently, .
show B reflexive. , total,
reflexive. If, instead, reflexive, x x so, modularity, x x. Thus,
total. And, since B, transitive. 2
Proposition 9
1. Q B = .
2. B 6 Q.
3. Q 6 B W least three elements.
4. Q B W one two elements.
Proof:
1. Suppose Q B. total transitive and, hence, . Suppose .
definition, total. Also definition, transitive, Proposition 1,
quasi-transitive and, thus, Q. Proposition 8, B and, so, Q B.
2. empty relation modular transitive, total and, so, Q.
3. Suppose b distinct elements W. relation W W \ {(b, a)} total,
and, since asymmetric restriction {(a, b)} transitive, also quasitransitive. However, least three elements W, transitive and,
so, B.
4. Suppose W one element. B contains possible relations W, whereas
Q contains fully connected relation W.
Suppose W two elements b. B contains empty relation, fully
connected relation, remaining eight relations contain either (a, b)
(b, a), both. Q, hand, contains three reflexive relations
containing either (a, b) (b, a).
2
Proposition 10
1. Q< B = T< .
186

fiRepresenting Aggregating Conflicting Beliefs

2. B 6 Q< .
3. Q< 6 B W least three elements.
4. Q< B W one two elements.
Proof:
1. Suppose Q< B. Since Q< , irreflexive, since B, T<
Proposition 6. Suppose, instead, T< . Proposition 6, B. Let
relation asymmetric restriction. (Obviously relation must
exist.) Proposition 9, Q, Q< . Thus, Q< B.
2. fully connected relation W B, asymmetric and, so, Q< .
3. Suppose b distinct elements W. W least three elements,
relation {(a, b)} modular and, thus, B, yet asymmetric restriction
relation W W \ {(b, a)} total quasi-transitive (since {(a, b)}
transitive).
4. Suppose W one element. B contains possible relations W, whereas
Q< contains empty relation W.
Suppose W two elements b. B contains empty relation, fully
connected relation, eight remaining relations contain either (a, b)
(b, a), both. Q< , hand, contains three irreflexive
relations.
2
Proposition 11 S, Un(S) modular necessarily transitive.
Proof: Let = Un(S). Suppose x, y, z W x y.
x <s y. assumption, <s modular, x <s z z <s y. definition Un(S),
x z z y, modular.
Suppose a, b, c W = {s1 , s2 } <s1 = {(a, b), (a, c)}

< 2 = {(b, a), (c, a)}. Un(S) transitive. 2
Proposition 12 S, AGRUn(S) B.
Proof: transitive closure relation transitive. Since Un(S) modular,
transitive closure Un(S) also modular Proposition 1. 2
Proposition 13 S, AGRRf(S) modular necessarily transitive.
Proof: first prove modularity. Suppose x, y, z W (x, y) AGRRf(S).
0
exists x <s s0 S, x y. modularity <s ,
either x <s z z <s y. Since finite, implies either exists s0
0
00
0
x <s z s00 s0 S, x z, exists s0 <s z
00
s00 s0 S, z. Thus, (x, z) AGRRf(S) (z, y) AGRRf(S),
AGRRf(S) modular.
Suppose W = {x, y, z} = {s1 , s2 } s1 = {(x, y), (z, y)}, s2 = {(y, x), (y, z)},
s1 s2 . AGRRf(S) = {(x, y), (z, y), (y, x), (y, z)} transitive. 2
187

fiMaynard-Zhang & Lehmann

Proposition 14 wS total order, AGRRf(S) B.
Proof: Weve already proven Proposition 13 AGRRf(S) modular. Let
= AGRRf(S) suppose x, y, z W. remains show transitive. Suppose x z. exists s1 x <s1 and, every s01 S,
0
0
s01 implies x 6<s1 6<s1 x, exists s2 <s2 z and,
0
0
every s02 S, s02 implies 6<s2 z z 6<s2 y. Suppose s1 s2 (the case s2 s1
similar). 6<s1 z z 6<s1 y. modularity, since x <s1 z 6<s1 y, x <s1 z. Let
0
0
0
s0 s0 s1 . x 6<s 6<s x. And, since s1 s2 , s0 s2 , 6<s z
0
0
0
z 6<s y. modularity, x 6<s z z 6<s x. Therefore, x z. 2
Proposition 15 S, AGR (S) B.
Proof: Proposition 12, <r B every r ranks(S). convenience, assume
existence virtual source sr corresponding <r . Precisely, r ranks(S),
assume exists source sr <sr =<r rank(sr ) = r, let 0
set sources. Then,



AGR (S) = (x, y) : r R. x <r r0 ranks(S). r0 > r x r0
n


0
=
(x, y) : 0 . x <s s0 0 . s0 0 x
= AGRRf(S 0 ).
Since one source 0 per rank r, since > total order R, wS 0 total
order. result follows Proposition 14. 2
Proposition 16 S, AGR(S) B.
Proof: Proposition 13, AGRRf(S) modular. AGRRf(S)+ obviously transitive,
and, Proposition 1, modular well. 2
Proposition 17 Suppose S.
1. wS fully connected, AGR(S) = AGRUn(S).
2. wS total order, AGR(S) = AGRRf(S).
Proof:
1. Suppose wS fully connected. second half definition AGRRf
vacuously
AGRRf(S) simplifies {(x, y) : S. x <s y}.
true
exactly sS <s , i.e., Un(S), AGR(S) = AGRRf(S)+ = Un(S)+ = AGRUn(S).
2. Suppose wS total order. Proposition 14, AGRRf(S) transitive, AGR(S) =
AGRRf(S)+ = AGRRf(S).
2
Proposition 18 Suppose S, = AGRRf(S), = AGR(S), x 6 x, W.
x y, x ./ y.
188

fiRepresenting Aggregating Conflicting Beliefs

Proof:

first show following lemma:

Lemma 18.1 Suppose = AGRRf(S). every integer n 2, x, W,
x 6 y, exist x0 , . . . , xn W x = x0 xn = y, n smallest
integer true, xn x0 .
Proof: Suppose x, W, x 6 y, exist x0 , . . . , xn W x = x0
xn = y, n smallest integer true. Consider triple
xi1 , xi , xi+1 , 1 n 1. First, xi1 6 xi+1 , otherwise would chain
shorter length n x y. Now, since xi1 xi , exists s1
0
xi1 <s1 xi and, s0 s1 S, xi1 xi . Similarly, exists s2
0
xi <s2 xi+1 and, s0 s2 S, xi xi+1 . Thus, sources higher rank
max(s1 , s2 ) agnostic respect xi1 xi+1 .
Suppose s1 s2 . xi s1 xi+1 so, modularity, xi1 <s1 xi+1 .
xi1 xi+1 , contradiction. Similarly, derive contradiction s2 s1 . Thus, s1 s2 .
Now, since xi1 6 xi+1 sources rank higher s1 s2 agnostic
respect xi1 xi+1 , xi1 6<s1 xi+1 . modularity, xi+1 <s1 xi . Since s1 s2 ,
sources higher rank s2 agnostic respect xi xi+1 ,
xi+1 xi . Similarly, xi <s2 xi1 , xi xi1 . Since chosen arbitrarily 1
n 1, xn x0 . And, fact, opinions worlds originate
sources rank. 2
suppose x 6 y. x y, exist x0 , . . . , xn x = x0
xn = n smallest positive integer true. Then, Lemma 18.1,
= xn x0 = x, x x ./ y. 2
Proposition 19 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR(S). AGRf
satisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,
IIA, non-dictatorship.
Proof: Let = AGRf (<s1 , . . . , <sn ). = AGR(S).
Restricted range: AGRf satisfies restricted range Proposition 16.
Unrestricted domain: AGRf satisfies unrestricted domain Definition 7.
Pareto principle: Suppose x <si si . particular, x <s maximal
rank source S. Since maximal, vacuously true every s0 S, x 6<s
0
6<s x. Therefore, x y, AGRf satisfies Pareto principle.
IIA: Let 0 = {s01 , . . . , s0n }. First note AGRRf satisfies IIA:
Lemma 19.1 Suppose = {s1 , . . . , sn } S, 0 = {s01 , . . . , s0n } S, si s0i i,
0
= AGRRf(S), 0 = AGRRf(S 0 ). If, x, W, x <si iff x <si i,
x iff x 0 y.
0

Proof: Suppose si s0i , x <si iff x <si y, i. x iff x 0 since
Definition 13 relies relative ranking sources relations
x belief states determine relation x aggregated
state. 2
189

fiMaynard-Zhang & Lehmann

Thus, IIA disobeyed closure step AGR introduces new opinions.
(Note IIA satisfied sources equal rank since, Proposition 17,
closure step introduces new opinions conditions.)
0
Now, suppose x, W, x <si iff x <si i, x6 ./ y, x6 ./0 y. show
x implies x 0 (the direction identical). Suppose x y. Let = AGRRf(S)
0 = AGRRf(S 0 ). Since x6 ./ y, x Proposition 18. x 0
Lemma 19.1, x 0 y.
(END IIA SUB-PROOF)
Non-dictatorship: Suppose wS fully connected suppose x <si 6<si x.
Let sj <sj x. x x, si dictator. 2
(END PROPOSITION 19 PROOF)
Ai , agent induced belief state,
Proposition 20 Let Definition

21,
+


wS , fully connected. = (A),



induced
belief state.
Ai

Proof:

use following lemma:

Lemma 20.1 set relations arbitrary finite set ,



[

+



+ =

[

+






+ transitive closure .


+
+
+
0=
, a, b . Suppose b.
Proof: Let =

,




exist 0 , . . . , n1 w0 , . . . , wn
+
= w0 +
0 n1 wn = b

Thus, exist x00 , . . . , x0m0 , . . ., x(n1)0 , . . . , x(n1)mn1
= w0 = x00 0 0 x0m0 = w1 = = wn1 = x(n1)0 n1 n1 x(n1)mn1 = wn
wn = b, 0 b.
suppose 0 b. exist 0 , . . . , n1 w0 , . . . , wn
= w0 0 n1 wn = b
Obviously, implies
+
= w0 +
0 n1 wn = b

implies
=




+
= w0 +
wn = b


, b. 2

190

fiRepresenting Aggregating Conflicting Beliefs

Now, let belief state induced (A). = AGR(S). Proposition 17,
= AGRUn(S),

= Un(S)+ =

[

!+
<s


=

sS

+

[



Sn

i=1



<s =

[ [

+
<s

Ai sSi

Si

lemma,

=


[



Ai

[
sSi

+ +



<s =

[

+



AGRUn(Si ) =

Ai

[

+
Ai

Ai

2
Proposition 21 Let agent informed set sources pedigreed
belief state (, l).
r relation


n
0
(x, y) : S. x <s r = rank(s) s0 S. s0 x .
Proof: Suppose x
r y. x l((x, y)) = r. Definitions 13 22,
0
0
exists x <s every s0 S, x y. particular, x <s
s0 S, wS s0 , rank(s) rank(s0 ). Thus,
0

r = l((x, y)) = max({rank(s0 ) : x <s y, s0 S}) = rank(s).
suppose exists x <s y, r = rank(s), and, every s0 S,
0
0
x y. x y. Moreover, since every s0 S, x <s implies wS s0 implies
rank(s) rank(s0 ),
0

l((x, y)) = max({rank(s0 ) : x <s y, s0 S}) = rank(s) = r.
Therefore, x
r y. 2
Proposition 22 Let A, PA , S, wS Definition 23. ped (PA )
pedigreed belief state (A).
Proof: Let ped (PA ) = (, l), 0 = AGRRf(S), l0 :0 R l0 ((x, y)) =
max({rank(s) : x <s y, S}). suffices show =0 l = l0 .
Suppose x y. show x 0 y, i.e., exists x <s and,
0
0
every s0 S, x 6<s 6<s x, l0 ((x, y)) = l((x, y)). Since x y,
Aj
0

exists Ai r x
r and, every Aj r > r R, x 6r0



6r0j x. Since x
r y, exists Si x < y, rank(s) = r, and, evs

1
1
ery s1 Si , x 6< 6< x. Si S, exists x <s y.
0
0
suppose s0 maximal rank source x <s <s x. s0 exists since x <s y. Since wS total preorder, suffices show wS s0 . Suppose
0
0
s0 Sj . Since Sj S, s0 also maximal rank source Sj x <s <s x,
191

fiMaynard-Zhang & Lehmann





j
j
Ai y, r = rank(s) rank(s0 ), w s0 . Furx rank(s

0 ) rank(s0 ) x. since x r
thermore, l0 ((x, y)) = rank(s) = r = l((x, y)).

suppose x 0 y. show x y, i.e., exists Ai r x
r
Aj
Aj
0
0
and, every Aj r > r R, x 6r0 6r0 x, l((x, y)) = l ((x, y)).
0
Since x 0 y, exists x <s and, every s0 S, x 6<s
0
6<s x. Suppose Si . Since Si S, also case every s0 Si ,
0
0
Aj
Aj
0

x 6<s 6<s x, x
rank(s) y. Now, let Aj r x r0 r0 x.
0

suffices show rank(s) r0 . Proposition 21, exists s0 Sj x <s
0
<s x rank(s0 ) = r0 . wS s0 , rank(s) rank(s0 ) = r0 . Furthermore,
l((x, y)) = rank(s) = l0 ((x, y)). 2
Proposition 23 A, PA , Definition 23, wS total order,
ped (PA ) = (, l), + =.
Proof: Since wS total order, AGR(S) = AGRRf(S) Proposition 17. Thus, =
AGRRf(S) = AGR(S) = AGRRf(S)+ =+ . 2
Proposition 24 S, vt0 (S) = Un(S).
Proof: Suppose (x, y) Un(S). 6= x <s S. Thus,
countS (x, y) > 0 countS (x, y)/kSk > 0, (x, y) vt0 (S). Suppose, instead, (x, y) 6
Un(S). x 6<s S, countS (x, y) = 0, (x, y) 6 vt0 (S). 2

Proposition 25 S, vt1 (S) = sS <s .

Proof: Suppose (x, y) sS <s . 6= x <s S. Thus,
count
(x, y) > 0 countS (x, y)/kSk 1, (x, y) vt1 (S). Suppose, instead, (x, y) 6



sS < . exists x 6< y, countS (x, y) < kSk. Thus,
countS (x, y)/kSk < 1, (x, y) 6 vt1 (S). 2
Proposition 26 Suppose S. vt1 (S) transitive necessarily modular.
Proof: Let W = {x, y, z} = {s1 , s2 } <s1 = {(x, y), (y, z), (x, z)} <s2 =
{(x, y), (z, y)}. vt1 (S) = {(x, y)} modular. 2
Proposition 27 kWk 3, every p (0, 1), exists vtp (S)
neither modular transitive.
Proof: Note kWk = 2, every relation W either transitive modular
(but necessarily both), kWk = 1 every relation W modular
transitive. Let W set worlds kWk 3 let x, y, z denote three
distinct members W. define parameterized p vtp (S) relation
{(x, y), (y, z)} neither transitive modular.
Let = {s1 , . . . , sn } satisfying following conditions:
1. n = d1/p + 1e p 1/3, d2/(1 p) + 1e otherwise.13
13. dxe denotes ceiling x, i.e., smallest integer greater equal x.

192

fiRepresenting Aggregating Conflicting Beliefs

2. <s1 = {(w, y)|w W, w 6= y}.
3. <s2 = {(y, w)|w W, w 6= y}.
4. dpn1e remaining sources belief state {(x, w)|w W, w 6= x}{(w, z)|w
W, w 6= z}.
5. remaining sources fully disconnected belief states.
clear <si B i. make two observations: First, observe pn > 1.
p 1/3,
pn = pd1/p + 1e 1 + p > 1.
p > 1/3,
pn = pd2/(1 p) + 1e 2p/(1 p) + p
monotonically increasing function,
pn > 2(1/3)/(1 1/3) + 1/3 = 4/3 > 1.
Second, note set described fifth condition non-empty since number
sources described conditions 2-4 2 + dpn 1e < 1 + pn less n
n > 1/(1 p). true p 1/3 since values
n = d1/p + 1e > 1/p > 1/(1 p).
also true p > 1/3 since
n = d2/(1 p) + 1e > 2/(1 p) > 1/(1 p).
(x, y) appears <s1 belief states described condition 3,
countS (x, y) = 1 + dpn 1e 1 + pn 1 = pn
(x, y) vtp (S). Similarly, (y, z) appears <s2 belief states described
condition 3,
countS (y, z) = 1 + dpn 1e 1 + pn 1 = pn
(y, z) vtp (S). remains show vtp (S) members. show
count pair less pn. countS (w, w) = 0 < pn w W.
countS (z, w) = countS (w, x) = 0 w 6= W. w W {x, y}, (w, y)
appears <s1 , countS (w, y) = 1 < pn first observation above.
w W {y, z}, (y, w) appears <s2 , countS (y, w) = 1 < pn. Finally,
w W {x, y, z}, (x, z), (x, w), (w, z) appear belief states described
condition 3,
countS (x, z) = countS (x, w) = countS (w, z) = dpn 1e < pn.
2
Proposition 28 Suppose transitive relation W x, W.
193

fiMaynard-Zhang & Lehmann

1. x y, lev (x) < lev (y).
2. x ./ y, lev (x) = lev (y).
3. lev (x) < lev (y), z. lev (z) = lev (x) z y.
4. lev (x) = lev (y), x iff x.
Proof:
1. Suppose x y.
lev (y) = 1 + max
0

W



lev (y 0 ) : 0

1 + lev (x)
> lev (x).
2. Suppose x ./ y. x ch(W, ) ch(W, ), lev (x) = lev (y) = 0. Suppose x 6 ch(W, ). 6 ch(W, ) lev (x) = 1 + max
({lev (y 0 ) : 0 x}).
0
W

0 one element, 0 transitivity,


lev (y 00 ) : 00 lev (x).
lev (y) = 1 + max
00
W

identical argument, lev (x) lev (y). Thus, lev (x) = lev (y).
3. Suppose lev (x) < lev (y). sufficient prove following: every nonnegative integer l < lev (y), exists z lev (z) = l. prove
induction l. l = lev (y)1, must exist z lev (z) = l definition.
Assume exists z 0 < l < lev (y) 1 lev (z) = l. Since l > 0,
exists z 0 z lev (z 0 ) = l 1. transitivity, z 0 y.
4. Suppose lev (x) = lev (y). x x 6 first part proposition,
otherwise lev (x) < lev (x), x. Similarly, x 6 x, x y.
2
Proposition 29 transitive relation W, MC() B.
Proof: Let = MC() x, y, z W. Suppose x y. lev (x) < lev (y)
lev (x) = lev (y) x0 , 0 W. (lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 ).
lev (x) < lev (z) lev (z) < lev (y), x z z y. Otherwise, lev (x) =
lev (y) = lev (z) x0 , 0 W. (lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 ), x z.
Thus, modular.
also suppose z. lev (y) < lev (z) lev (y) = lev (z) 0 , z 0
W. (lev (y 0 ) = lev (z 0 ) = lev (y) 0 ./ z 0 ). lev (x) < lev (y) lev (y) < lev (z),
lev (x) < lev (z) transitivity <, x z. Otherwise, lev (x) = lev (y) =
lev (z) x0 , 0 W. (lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 ), x z. Thus,
transitive. 2
Proposition 30 Suppose transitive relation W = MC().
194

fiRepresenting Aggregating Conflicting Beliefs

1. .
2. modular, =.
3. lev (x) = lev (x) x W.
4. 0 B 0 lev0 (x) = lev (x) x W, 0 .
Proof:

Let x, W.

1. Suppose x y. lev (x) lev (y). lev (x) < lev (y), x y. Suppose
lev (x) = lev (y). x ./ y, exist x0 , 0 lev (x0 ) = lev (y 0 ) =
lev (x) x0 ./ 0 , i.e., x0 = x 0 = y, x y.
suppose x y. lev (x) < lev (y), x 6 x, x y.
2. first part proposition, , suffices show . Suppose
x y.
Case 1: lev (x) < lev (y). Then, Proposition 28, exists z
lev (z) = lev (x) z y. modularity, z x x y. latter case,
done. former case, z ./ x otherwise lev (z) 6= lev (x). Thus, x
transitivity.
Case 2: lev (x) = lev (y). exist x0 , 0 lev (x0 ) = lev (y 0 ) =
lev (x) x0 ./ 0 . modularity, x0 x x 0 . former case x0 ./ x
Proposition 28, latter x0 ./ x Proposition 28 transitivity. modularity,
x0 x. applying Proposition 28 transitivity, x ./ y,
x y.
3. prove induction level x .
Base case: lev (x) = 0. x ch(W, ). Suppose x. lev (x) =
lev (y) exist x0 0 lev (x0 ) = lev (y 0 ) = lev (x) = lev (y)
x0 ./ 0 , x y. Thus, x ch(W, ), lev (x) = 0 = lev (x).
Inductive case: Assume lev (x0 ) = lev (x0 ) x0 lev (x0 ) <
lev (x); show lev (x) = lev (x). Let z = arg max
({lev (y 0 ) : 0 x});
0
W

lev (x) = 1 + lev (z). Also, let = arg max
({lev (y 0 ) : 0 x});
0
W

lev (x) = 1 + lev (y). first part proposition, , x.
Thus, lev (z) lev (y). Furthermore, lev (y) = lev (y) inductive hypothesis, lev (z) lev (y). lev (z) < lev (x) Definition 26 (otherwise x z, contradiction). inductive hypothesis, lev (z) = lev (z),
lev (z) < lev (x) = 1 + lev (y). Since levels integral, lev (z) lev (y),
lev (z) = lev (y). Thus, lev (x) = 1 + lev (z) = 1 + lev (y) = lev (x).
4. Suppose 0 B, 0 , lev0 (z) = lev (z) z W. clear
00 B x W, x partition corresponding level, i.e., Wlev00 (x) .
Since 0 B preserve levels worlds ,
must identical partitions. Suppose x, W Wi Wj partitions
(for 0 ) x Wi Wj .
195

fiMaynard-Zhang & Lehmann

Suppose x y. Wi 6= Wj < j Proposition 7, x 0 y,
Proposition 7. If, instead Wi = Wj , lev (x) = lev (y). Definition 26,
exist x0 , 0 W lev (x0 ) = lev (y 0 ) = lev (x) x0 ./ 0 . Thus,
x0 , 0 Wi and, since 0 , x0 ./0 0 . Proposition 7, Wi either fully connected
fully disconnected 0 , Wi must fully connected 0 . particular, x 0 y.
2
Proposition 31 relation W, MT() B.
Proof:

Since + transitive, MT() = MC (+ ) B Proposition 29. 2

Proposition 32 Suppose relation W = MT().
1. .
2. transitive, = MC().
3. modular, =+ .
4. modular transitive, =.
5. conflicts, neither .
Proof:

Let x, W.

1. + and, first property Proposition 30, + MC(+ ) = , .
2. Since transitive, + =. Thus, = MC(+ ) = C().
3. Since modular, + modular Proposition 1 so, Proposition 30,
MC(+ ) =+ . Thus, =+ .
4. Since transitive, = MC() since modular, MC() = Proposition 30, =.
5. first prove following lemma:
Lemma 32.1 x conflict wrt + iff conflict wrt .
Proof: direction obvious since transitive closure monotonically
additive operation. direction, suppose x conflict wrt
+ . exist w0 , . . . , wn , z0 , . . . , zm W
x = w0 + + wn = = z0 + + zm = x.
then, 0 n 1, exist wi0 , . . . , wipi W
wi = wi0 wipi = wi+1 .
Similarly, 0 j 1, exist zj0 , . . . , zjqj W
zj = zj0 zjqj = zj+1 .
Thus,
x = w0 wn = = z0 zm = x,
x conflict wrt . 2
196

fiRepresenting Aggregating Conflicting Beliefs

Now, suppose x conflict wrt . x ./ since B Proposition 31. Propositions 28 30,
lev+ (x) = lev (x) = lev (y) = lev+ (y)
So, since x y, exist x0 , 0 W lev+ (x0 ) = lev+ (y 0 ) = lev+ (x)
x0 ./+ 0 Definition 26. Thus, + conflict. lemma above, must
also conflict.
2
Proposition 33 p [0, 1], AGREqp (S) B.
Proof:

Follows immediately definition AGREqp Proposition 31. 2

Proposition 34 p [0, 1], AGRp (S) B.
Proof:

Again, follows immediately Proposition 31. 2

Proposition 35 Suppose p [0, 1].
1. wS fully connected, AGRp (S) = AGREqp (S).
2. wS total order, AGRp (S) = AGRRf p (S) = AGRRf(S) = AGR(S).
3. AGR0 (S) = AGR(S).
Proof:

Assume x, W.

1. suffices show AGRRf p (S) = vtp (S) wS fully connected. Suppose
(x, y) AGRRf p (S). Then, definition AGRRf p , exists
(x, y) vtp ({s0 : s0 s}). Since wS fully connected, {s0 : s0 s} = S,
(x, y) vtp (S).
Suppose, instead, (x, y) vtp (S). definition vtp , countS (x, y) > 0
exists x <s y. Pick one s. Again, {s0 : s0 s} = since
wS fully connected, (x, y) vtp ({s0 : s0 s}). Finally, s0 S. s0
0
x holds vacuously, (x, y) AGRRf p (S).
2. Suppose wS total order.
AGRRf(S) = AGR(S).

already shown Proposition 17

Next show AGRRf p (S) = AGRRf(S). AGRRf p (S) set (x, y)
exists x <s y, (x, y) vtp ({s0 : s0 s}), and,
0
s0 S, x y. w total order, {s0 : s0 s} = {s}. Since
x <s y, count{s} (x, y) = 1 > 0 count{s} (x, y)/k{s}k = 1 p. Consequently,
(x, y) vtp ({s0 : s0 s}), proving requirement redundant
wS total order. Thus, AGRRf p (S) set (x, y) x <s and,
0
s0 S, x y, i.e., AGRRf p (S) = AGRRf(S).
Finally, AGRp (S) = MT(AGRRf p (S)) = MT(AGRRf(S)). Proposiion 14,
AGRRf(S) modular transitive, AGRp (S) = AGRRf(S) Proposition 32.
197

fiMaynard-Zhang & Lehmann

3. suffices show AGRRf 0 (S) = AGRRf(S), since
AGR0 (S) = MT(AGRRf 0 (S)) = MT(AGRRf(S)) = AGRRf(S)+
Propositions 13 32, AGR0 (S) = AGR(S).
0

Suppose (x, y) AGRRf 0 (S). x <s and, s0 S, x y,
(x, y) AGRRf(S). Suppose, instead, (x, y) AGRRf(S). x <s and,
0
s0 S, x y. Let 0 = {s0 : s0 s}. Since x <s
0 , countS 0 (x, y) > 0 countS 0 (x, y)/kS 0 k 0, (x, y) vt0 (S 0 ). Therefore,
(x, y) AGRRf 0 (S).
2
Corollary 35.1 Let = {s1 , . . . , sn } AGRf (<s1 , . . . , <sn ) = AGR0 (S). AGRf
satisfies (the modified versions ) restricted range, unrestricted domain, Pareto principle,
IIA, non-dictatorship.
Proof:

Follows immediately Propositions 35 19. 2

Proposition 36 Let agent informed set sources S, support pedigreed belief state (l, sup, rtab), using aggregation function AGRp p [0, 1]. belief
state relation
MT ({(x, y) : ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p})
Proof:

Let
R = {(x, y) : ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p}

suffices show AGRRf p (S) = R. Suppose (x, y) AGRRf p (S). exists
(a) x <s y, (b) (x, y) vtp ({s0 : s0 s}), (c) s0 S,
0
x y. (a) (c), rank(s) = max({rank(s) : x 6s y, S} {}),
l(x, y) = rank(s). Thus, {s0 : s0 s} = {s0 : rank(s0 ) = l(x, y)} = rtab(l(x, y)),
(x, y) vtp (rtab(l(x, y))) (b). definition vtp , countrtab(l(x,y)) (x, y) > 0
countrtab(l(x,y)) (x, y)/krtab(l(x, y))k p. sup(x, y) = {s0 : rank(s0 ) =
0
0
l(x, y), x <s y} = {s0 rtab(l(x, y)) : x <s y}, ksup(x, y)k = countrtab(l(x,y)) (x, y).
Thus, ksup(x, y)k > 0 ksup(x, y)k/krtab(l(x, y))k p, (x, y) R.
suppose (x, y) R. (a) ksup(x, y)k > 0 (b) ksup(x, y)k/krtab(l(x, y))k
p. Suppose sup(x, y); (a), least one exists. definition sup, x <s y,
satisfying first condition AGRRf p , rank(s) = l(x, y). definition l(x, y),
rank(s) = max({rank(s) : x 6s y, S} {}), s0 rank(s0 ) >
0
rank(s) (i.e., s0 s), x y. remains show (x, y) vtp ({s0 : s0 s}).
Since rank(s) = l(x, y), {s0 : s0 s} = rtab(l(x, y)) showed above,
vtp ({s0 : s0 s})
= vtp (rtab(l(x, y)))
= {(x0 , 0 ) : countrtab(l(x,y)) (x0 , 0 ) > 0, countrtab(l(x,y)) (x0 , 0 )/krtab(l(x, y))k p}.
showed above, ksup(x, y)k = countrtab(l(x,y)) (x, y). Making substitution (a)
(b), see (x, y) vtp ({s0 : s0 s}), (x, y) AGRRf p (S). 2
198

fiRepresenting Aggregating Conflicting Beliefs

Proposition 37 Let A, PA , S, wS Definition 32. sup (PA )
support pedigreed belief state (A).
Proof:

Let sup (PA ) = (l, sup, rtab), l0 : W W R {}
l0 ((x, y)) = max({l00 (x, y) : (l00 , sup00 , rtab00 ) PA }),

sup0 : W W 2S
[

sup0 (x, y) =

sup00 (x, y),

(l00 ,sup00 ,rtab00 )PA , l00 (x,y)=l0 (x,y)

rtab0 : ranks(S) R
rtab0 (r) =

[

rtab00 (r).

(l00 ,sup00 ,rtab00 )PA , rrange(rtab00 )

suffices show l = l0 , sup = sup0 , rtab = rtab0 .
Suppose x, W agent Ai support pedigreed belief state (li , supi , rtabi ).
l(x, y) = max({rank(s) : x <s y, S} {})


[
= max
({rank(s) : x <s y, Si } {})
Si informs Ai , Ai


[
= max
{max ({rank(s) : x <s y, Si } {})}
Si informs Ai , Ai
= max({l00 (x, y) : (l00 , sup00 , rtab00 ) PA })
= l0 (x, y).
Also,
sup(x, y) = {s : rank(s) = l(x, y), x <s y}
[
=
{s Si : rank(s) = l(x, y), x <s y}
Si informs Ai , Ai
[
=
{s Si : rank(s) = l0 (x, y), x <s y}
Si informs Ai , Ai
[
=
{s Si : rank(s) = li (x, y), x <s y}
Si informs Ai , Ai A, li (x,y)=l0 (x,y)
[
=
sup00 (x, y)
(l00 ,sup00 ,rtab00 )PA , l00 (x,y)=l0 (x,y)
0

= sup (x, y).

199

fiMaynard-Zhang & Lehmann

Finally,
rtab(r) = {s : rank(s) = r}
[
=
{s Si : rank(s) = r}
Si informs Ai , Ai
[
=
rtabi (r)
Si informs Ai , Ai A, rrange(rtabi )
[
=
rtab00 (r)
(l00 ,sup00 ,rtab00 )PA , rrange(rtab00 )
0

= rtab (x, y).
2

Appendix B. Notation key
: arbitrary finite set
a, b, c, . . .: specific elements set
x, y, z, . . .: arbitrary elements set
A, B, C, . . .: specific subsets set
X, Y, Z, . . .: arbitrary subsets set
: arbitrary set relations
: arbitrary relation
+ : transitive closure
ch(X, ): choice set X wrt
kXk: cardinality set X
W: finite set possible worlds
w, W : element, subset W, respectively
B: set generalized belief states (modular, transitive relations)
: element B, strict likelihood
: weak likelihood
: equal likelihood, agnosticism
./: conflict
Bel: belief conditional statement
Agn: agnosticism conditional statement
Con: conflict conditional statement
: set total preorders
T< : strict versions total preorders
Q: set total, quasi-transitive relations
Q< : strict versions total, quasi-transitive relations
S: set sources
s, S: element, subset S, respectively
200

fiRepresenting Aggregating Conflicting Beliefs

<s : belief state source
: source agnosticism
./s : source conflict
R: set ranks
r: element R
rank(s): rank source
ranks(S): set ranks sources
w, wS : credibility ordering S, S, respectively
Un: union set belief states
AGRUn: aggregation via union
AGRRf: aggregation via refinement
AGR: general aggregation
A: set agents
A: element
: induced belief state
(, l): pedigreed belief state

r : restriction pedigreed belief state rank r
: fusion
ped : pedigreed fusion
vtp : voting function p
lev: level world transitive relation
MC: modular closure
MT: modular, transitive closure
AGREqp : aggregation voting without refinement
AGRRf p : aggregation voting via refinement
AGRp : general aggregation voting
(l, sup, rtab): support pedigreed belief state
sup : support pedigreed fusion

References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50, 510
530.
Andreka, H., Ryan, M., & Schobbens, P.-Y. (2002). Operators laws combining
preference relations. Journal Logic Computation, 12 (1), 1353.
Arrow, K. J. (1963). Social Choice Individual Values (2nd edition). Wiley, New York.
Baral, C., Kraus, S., Minker, J., & Subrahmanian, V. S. (1992). Combining knowledge
bases consisting first-order theories. Computational Intelligence, 8 (1), 4571.
Benferhat, S., Dubois, D., Kaci, S., & Prade, H. (2002). Possibilistic merging distancebased fusion propositional information. Annals Mathematics Artificial In201

fiMaynard-Zhang & Lehmann

telligence, 34 (13), 217252.
Black, D. (1958). Theory Committees Elections. Cambridge University Press,
Cambridge.
Borgida, A., & Imielinski, T. (1984). Decision making committees: framework
dealing inconsistency non-monotonicity. Proceedings Workshop
Nonmonotonic Reasoning, pp. 2132.
Boutilier, C. (1996). Iterated revision minimal change conditional beliefs. Journal
Philosophical Logic, 25, 263305.
Brams, S. J., & Fishburn, P. C. (2002). Voting procedures. Arrow, K. J., Sen, A. K., &
Suzumura, K. (Eds.), Handbook Social Choice Welfare, Vol. 1 Handbooks
Economics, chap. 4, pp. 173236. Elsevier Science.
Cantwell, J. (1998). Resolving conflicting information. Journal Logic, Language,
Information, 7, 191220.
Center Voting Democracy
http://www.fairvote.org/irv/.

(2002).

Instant

runoff

voting.

Darwiche, A., & Pearl, J. (1997). logic iterated belief revision. Artificial Intelligence, 89 (12), 129.
Fishburn, P. C. (1974). Lexicographic orders, utilities decision rules: survey. Management Science, 20 (11), 14421471.
Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.
MIT Press.
Gardenfors, P., & Makinson, D. (1994). Nonmonotonic inference based expectations.
Artificial Intelligence, 65 (1), 197245.
Gardenfors, P., & Rott, H. (1995). Belief revision. Gabbay, D. M., Hogger, C. J., &
Robinson, J. A. (Eds.), Epistemic Temporal Reasoning, Vol. 4 Handbook
Logic Artificial Intelligence Logic Programming, pp. 35132. Oxford University
Press, Oxford.
Grosof, B. (1991). Generalizing prioritization. Proceedings Second International
Conference Principles Knowledge Representation Reasoning (KR 91), pp.
289300.
Grove, A. (1988). Two modellings theory change. Journal Philosophical Logic, 17,
157170.
Kahneman, D., & Tversky, A. (1979). Prospect theory: analysis decision risk.
Econometrica, 47 (2), 263291.
Katsuno, H., & Mendelzon, A. O. (1991). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.
Konieczny, S., & Perez, R. P. (1998). logic merging. Proceedings Sixth
International Conference Principles Knowledge Representation Reasoning
(KR 98), pp. 488498.
202

fiRepresenting Aggregating Conflicting Beliefs

Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential
models cumulative logics. Artificial Intelligence, 44 (12), 167207. CoRR:
cs.AI/0202021.
Kreps, D. M. (1990). Course Microeconomic Theory. Princeton University Press.
Lehmann, D. (1995). Belief revision, revised. Proceedings Fourteenth International
Joint Conference Artificial Intelligence (IJCAI 95), pp. 15341540.
Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?.
Artificial Intelligence, 55 (1), 160. CoRR: cs.AI/0202022.
Liberatore, P., & Schaerf, M. (1995). Arbitration: commutative operator belief revision. Proceedings Second World Conference Fundamentals Artificial
Intelligence (WOCFAI 95), pp. 217228.
Luce, D. R. (1956). Semiorders theory utility discrimination. Econometrica, 24 (2),
178191.
Makinson, D. C. (1997). Screened revision. Theoria, 63 (12), 1423. Special issue
non-prioritized belief revision.
Maynard-Reid II, P., & Shoham, Y. (2001). Belief fusion: Aggregating pedigreed belief
states. Journal Logic, Language, Information, 10 (2), 183209.
Meyer, T. (2001). semantics combination operations. Journal Applied NonClassical Logics, 11 (12), 5984.
Meyer, T., Ghose, A., & Chopra, S. (2001). Social choice, merging elections. Benferhat, S., & Besnard, P. (Eds.), Proceedings Sixth European Conference Symbolic Quantitative Approaches Reasoning Uncertainty (ECSQARU2001),
pp. 466477.
Ramsey, F. P. (1931). Foundations Mathematics Logical Essays. Routledge
Kegan Paul, New York.
Revesz, P. Z. (1997). semantics arbitration. International Journal Algebra
Computation, 7 (2), 133160.
Sen, A. (1986). Social choice theory. Arrow, K. J., & Intriligator, M. D. (Eds.), Handbook Mathematical Economics, Vol. III, chap. 22, pp. 10731181. Elsevier Science
Publishers.
Spohn, W. (1988). Ordinal conditional functions: dynamic theory epistemic states.
Harper, W. L., & Skyrms, B. (Eds.), Causation Decision, Belief Change,
Statistics, II, pp. 105134. Kluwer Academic Publishers.
Williams, M.-A. (1994). Transmutations knowledge systems. Proceedings Fourth
International Conference Principles Knowledge Representation Reasoning
(KR 94), pp. 619629.

203

fiJournal Artificial Intelligence Research 19 (2003) 355-398

Submitted 10/02; published 10/03

Architectural Approach
Ensuring Consistency Hierarchical Execution
Robert E. Wray

wrayre@acm.org

Soar Technology, Inc., 3600 Green Court, Suite 600
Ann Arbor, MI 48105 USA

John E. Laird

laird@umich.edu

University Michigan, 1101 Beal Avenue
Ann Arbor, MI 48109 USA

Abstract
Hierarchical task decomposition method used many agent systems organize
agent knowledge. work shows combination hierarchy persistent
assertions knowledge lead difficulty maintaining logical consistency asserted
knowledge. explore problematic consequences persistent assumptions
reasoning process introduce novel potential solutions. implemented one
possible solutions, Dynamic Hierarchical Justification, effectiveness demonstrated
empirical analysis.

1. Introduction
process executing task dividing series hierarchically organized subtasks called hierarchical task decomposition. Hierarchical task decomposition
used large number agent systems, including Adaptive Intelligent Systems architecture (Hayes-Roth, 1990), ATLANTIS (Gat, 1991a), Cypress (Wilkins et al., 1995),
Entropy Reduction Engine (Bresina, Drummond, & Kedar, 1993), Procedural Reasoning System (Georgeff & Lansky, 1987), RAPS (Firby, 1987), Soar (Laird, Newell, &
Rosenbloom, 1987; Laird & Rosenbloom, 1990), Theo (Mitchell, 1990; Mitchell et al.,
1991), cornerstone belief-desire-intention-based agent implementations (Rao &
Georgeff, 1991; Wooldridge, 2000). Hierarchical task decomposition helps agents
knowledge developer agent manage environmental complexity. example,
agent may consider high-level tasks find power source fly Miami
independent low-level subtasks go east 10 meters turn heading 135.
low-level tasks chosen dynamically based currently active high level
tasks current situation; thus high-level task progressively decomposed
smaller subtasks. division labor simplifies design agents, thus reducing
cost. Additional advantages hierarchical task decomposition include knowledge sharing
(a low-level subtask invoked many different high-level procedures), modularity
(the decomposition helps insulate subtasks interaction knowledge)
naturalness representation (Simon, 1969).
Without careful design, difficult ensure consistent reasoning agents employing hierarchical task decompositions. consistency, mean reasoning
lead set assertions contains contradiction. Ensuring consistency becomes
c 2003 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWray & Laird

much difficult solve thus costly complexity agents
knowledge grows. Although problem solved careful design agent
knowledge, approach requires understanding possible interactions
hierarchy. Thus, correctness solution depends skill vigilance
knowledge engineer. bias seek solutions operation agents primitive memories processes structured ensure inconsistencies arise. Thus,
prefer architectural solutions knowledge-based ones. Architectural solutions
guarantee consistency tasks domains, reducing brittleness due omissions
task knowledge. Further, developing architectural solution may costly,
less costly repeatedly developing knowledge-based solutions different domains.
following sections describe inconsistency problem introduce space
solutions problem, including two novel solutions. theoretical
empirical analysis, one new solutions, Dynamic Hierarchical Justification, shown
provide efficient architectural solution problem ensuring reasoning consistency
hierarchical execution.

2. Maintaining Reasoning Consistency Hierarchical Agents
section describes inconsistency problem greater detail. review methods
ensuring consistency non-hierarchical systems discuss limitations
approaches hierarchical systems.
2.1 Consistency Non-hierarchical Systems
Truth maintenance systems (TMSs) often used maintain consistency non-hierarchical systems (Doyle, 1979; McDermott, 1991; Forbus & deKleer, 1993). inference engine
uses domain knowledge create two different kinds assertions knowledge agents
knowledge base: assumptions entailments. inference engine enables assumptions
decided treat true, without requiring assertion justified. Agents often treat environmental percepts assumptions unquestioned beliefs
(Shoham, 1993). Entailments justified assertions. data structure, justification,
captures reasons asserting entailment. reasons longer hold (the
entailment longer justified), TMS retracts set asserted beliefs. Thus,
TMS automatically manages assertion retraction entailments agents
situation changes, ensuring entailments consistent external environment
enabled assumptions.
Careful construction domain knowledge required ensure enabled
assumptions contradictory. example, assumption inconsistent
current input, agent must domain knowledge recognizes situation
removes assumption. Thus, agent utilizes TMS, problem maintaining
consistency reasoning largely one managing assumptions agents domain
knowledge.
Assumptions often reflect hypothetical reasoning world (hence assumptions). However, assumptions used represent persistent feature. Although
researchers explored structuring external environment provide persistent memory (Agre & Horswill, 1997), internal, persistent memory usually necessary agent
356

fiEnsuring Consistency Hierarchical Execution

Assumptions
Entailments
Assumptions
Entailments
Assumptions

Inference
Engine

Entailments
Assumptions

TMS

Entailments
Assumptions
Entailments
Assumptions
Entailments
Assumptions
Entailments

Memory
Hierarchy Maintenance

Figure 1: hierarchical agent.
domains. example, persistence required hypothetical reasoning, nonmonotonic
revisions assertions (such counting), remembering.
2.2 Truth Maintenance Hierarchical Agents
TMS agent framework introduced extended hierarchical agent architectures. agent, inference engine TMS less identical
non-hierarchical agent. agent initiates new subtask via dynamic hierarchical
task decomposition, also creates new database contain assumptions entailments specific subtask. decomposition result stack subtasks,
containing entailments assumptions specific subtask, shown Figure 1.
consider creation deletion distinct databases assertions sine qua non
hierarchical architecture. architecture decomposes task identifying
relevant subtasks, also dynamically organizing memory according current
decomposition.
new system component, hierarchy maintenance, responsible creating
destroying subtask databases subtasks begin terminate. subtask
achieved (or determined longer worth pursuing), hierarchy maintenance responds
immediately removing assertions associated subtask. function
central importance hierarchical architectures allows agent automatically
retract assertions associated terminated subtask, requiring agent knowledge
clean remove individual assertions associated terminated subtask.
hierarchy maintenance component efficiently remove assertions
(conceptually) located distinct unit, subtask database.
357

fiWray & Laird

Subtask1

a11
e11

a12

a14
a13

a15

Subtask2
e14
e12
e17
e15
e19
e13
e18
a21
e16a11 a12
a22
a13

e21

e22

(a14, a15, e14, e19)
PSfrag replacements

Subtask3

a31

e23

a34
a32

e31

e32

(a12, a22, e22)

Hierarchy Maintenance

Figure 2: example hierarchy maintenance. Assumptions (a) entailments (e)
asserted within subtasks.

agents hierarchy maintenance function employed help maintain consistency, illustrated notionally Figure 2. agent architecture identifies assertions
higher levels hierarchy led new subtask. assertions together
form subtask support set. Figure 2, assertions 14 , a15 , e14 , e19 form support
set Subtask2 a12 , a22 , e22 support Subtask3 . support sets, effect, form
justifications subtasks hierarchy. assertion support set removed
(e.g., a22 ), agent responds removing subtask (Subtask 3 ). hierarchical architectures use architectural processes create destroy subtask databases,
example illustrates architectural hierarchical maintenance function realized
via process similar justification truth maintenance.
Within specific subtask, reason maintenance go before. However, hierarchical structure adds complication maintenance logical consistency. Assumptions
level hierarchy dependent entailments assumptions higher
levels hierarchy.1 dependence relationship suggested Figure 1 curved
lines extending one subtask one it. Higher levels hierarchy form
context reasoning local subtask.
execution agents embedded dynamic domains, hierarchical context may
change almost time. changing context problematic entailments;
1. Assumptions lower level subtask always least indirectly dependent higher level assertions. observation exploited Section 3.3.

358

fiEnsuring Consistency Hierarchical Execution

TMS readily determine dependent context changes retract affected entailments.
However, changes higher levels hierarchy (such deriving inputs)
may also invalidate assumptions lower levels. Without additional architectural
mechanisms, domain knowledge required ensure consistency among assumptions
hierarchical context non-hierarchical systems. domain knowledge ensuring consistency assumptions complicated necessity spanning multiple
(possibly many) subtasks. refer knowledge across-level consistency knowledge. described detail below, identifying creating across-level consistency
knowledge tedious, costly, often incomplete process. Across-level knowledge must
explicitly consider interactions different subtasks (in different levels hierarchy), rather focus solely local subtask, compromising benefit
hierarchical decomposition.
continuing, note hierarchical architectures contrasted
hierarchical task network (HTN) planners (Sacerdoti, 1975; Erol, Hendler, & Nau, 1994)
execution-oriented systems use HTN representations, DECAF (Graham
& Decker, 2000) RETSINA (Sycara, Decker, Pannu, Williamson, & Zeng, 1996).
planning problem HTN planner represented initial task network
consist primitive non-primitive tasks. planner uses operators find plan
solve tasks. Methods allow planner match non-primitive tasks
task networks describe accomplish task; thus, methods enable hierarchical
decomposition planning problem family connected task networks.
main difference HTN systems hierarchical architectures
planner represents plan single global state. is, methods represent decomposition steps, hierarchical structure evolving plan represented blackboardlike database also reflect structure decomposition. following
sections discuss problems especially solutions depend hierarchical organization asserted knowledge execution, addition hierarchical task decomposition encoded agents task knowledge. Thus, following generally
applicable HTN-based execution systems. However, HTN systems need address
inconsistency problem; Section 5.1.1 examines consequences global state respect
inconsistency arising persistence hierarchy.
2.3 Failing Respond Relevant Changes Hierarchical Context
mentioned introduction, agent fails respond relevant change
hierarchical context leaves now-inconsistent assumption enabled, resulting
behavior become irrational; is, consistent knowledge. section
explores irrational behavior arise several illustrative examples.
2.3.1 Blocks World
use variant blocks world illustrate inconsistency problem domain
familiar readers. domain execution domain rather planning
domain, call Dynamic Blocks World reflect difference static
blocks world used planning. assume agent knowledge build ordered
359

fiWray & Laird

Goal: (1 2) (2 3) (3 table)
Agent Memory
put-on-table(3)
put-on-table(2)
put-down(2)

Agent Memory
put-on-table(3)
put-on-table(2)
put-down(2)

Agent Memory
put-on-table(3)
put-on-table(2)
put-down(2)

empty
space

empty
space

empty
space

2
3
1

2
3
1

Actual World State

2

3
1

time

2

2
3
1

ent
l ev 3

n
r
Exte ks
c
kno

Actual World State

time

2
3 1

3 1

Actual World State

Figure 3: Failing respond relevant changes hierarchical context Dynamic
Blocks World.

tower (1-on-2-on-3) without resorting planning uses hierarchical task decomposition
determine actions take builds tower.
Figure 3, agent placing block-2 table, order reach block-3
begin goal tower. put-down subtask finds empty location table.
agent places empty assertion memory associated put-down subtask.
figure, space immediately left gripper chosen. Whether
space empty may directly observable may need inferred number
facts domain stored assumption memory. Assume empty
assertion assumption. Now, assume block-3 suddenly placed underneath block-2.
result inconsistency assumption (the location good place put
block-2) hierarchical context (the location longer good place put
block table).
agent fails recognize block-3 moved, attempt put block-2
location occupied block-3. behavior irrational, consistent
agents goals knowledge (assuming agent knowledge indicates
blocks placed positions already occupied blocks). inconsistency arises agent failed recognize previously-derived assumption
(empty) longer true current situation.
Although example may appear contrived, specific situation arose experimental system developed explore architecture learning issues. course, possible
simple domain reformulate task problem occur.
reformulation task via changes additions knowledge exactly solution
wish avoid. is, desire architecture guarantee consistency
hierarchical context local assumptions architecture provides priori
constraints (guidance) knowledge development process increased robustness
execution (via consistency). conclusion returns example describe
360

fiEnsuring Consistency Hierarchical Execution

patrol
intercept
attack(defensive)
achieveproximity
turntoheading

Figure 4: Decomposition behavior subtasks.
architectural solution inconsistency problem solves particular problem without
requiring reformulation agents task knowledge.
2.3.2 TacAir-Soar
TacAir-Soar agents pilot virtual military aircraft complex, real-time computer simulation tactical combat (Tambe et al., 1995; Jones et al., 1999). TacAir-Soar domain
indirectly accessible (each agent uses simulated aircraft sensor models perceive
pilot real aircraft would sense), nondeterministic (from point view
agent, behavior agents cannot strictly predicted anticipated), nonepisodic (the decisions agent makes early simulation impact later options
capabilities), dynamic (the world changes real time agent reasoning),
continuous (individual inputs continuous values). Domains characteristics
difficult ones create apply agents (Russell & Norvig, 1995).
domain knowledge TacAir-Soar agents organized 450 subtasks;
execution, resulting hierarchical task decomposition sometimes reaches depths greater
10 subtasks. agent one several different mission roles, among
flying patrol mission, acting partner wing agents lead.
Consider pair planes patrol, given specific instructions
engaging enemy aircraft. enemy aircraft enter patrol area, lead agent decides
intercept aircraft. lead decomposes intercept series situationdependent subtasks, may decomposed. example, Figure 4
shows complex task intercepting enemy aircraft decomposed
decision turn agents aircraft specific heading. agent turns heading
order get close enough enemy agent (via achieve-proximity) launch
attack.
Assume three different kinds attack chosen intercept. first tactic
(scare) engage attempt scare away enemy planes without using deadly force.
tactic selected rules engagement specify deadly force
used, regardless number aircraft area. One remaining two tactics
chosen deadly force allowed. Offensive attack appropriate friendly
patrol
intercept
count (enemy)

patrol
intercept
count(friendly)

patrol
intercept
attack(defensive)
achieveproximity
turntoheading

Figure 5: Trace behavior leading intercept tactic TacAir-Soar.
361

fiWray & Laird

Figure 6: Inconsistency due persistence.

planes outnumber equal enemy planes. Defensive attack used enemy planes
outnumber friendly planes.
Choosing offensive defensive attack requires counting current aircraft
area. Figure 5 shows evolution executing example decomposition.
agent must count relevant enemy friendly planes. Determining planes side
relevance count often requires remembering sufficiently complex entailment count possible. instance, non-combatant aircraft
counted, requiring reasoning type aircraft. agent determines
enemy planes outnumber friendly ones, agent selects defensive-attack, leading
decomposition.
happens enemy plane flees, thus reducing actual count relevant enemy planes one? count maintained agent invalid. Standard TMS
mechanisms insufficient count asserted assumption. actual
number enemy friendly planes equal, agent switch tactic offensive attack. Continuing defensive attack consistent agents
knowledge. Additionally, friendly agents participating attack may base
behavior expectation agent pursuing offensive attack. Thus agent
needs recognize inconsistency remove current count.
Figure 6 presents conceptual illustration problem. Assumptions represented
squares, entailments circles. horizontal line represents hierarchical relationship
assertions (i.e., assumptions entailments) hierarchical context (above
line) assertions local subtask (below line). arrowed lines represent
dependence creation assertion. previous examples, reasoning
subtask may require persistence, leading creation assumption
assumption 1. However, persistent assertion may still depend assertions.
work focuses dependent assertions higher level context, A, B, C, D,
E1 figure.
Suppose world changes E1 retracted memory E2 asserted.
Assumption 1 remains memory. E 2 would also lead 1 (e.g., could lead
new assumption 2, shown), 1 longer justified may consistent
higher level context. Whether potential inconsistency among assertions
leads inconsistent behavior depends use assumption 1 later reasoning.
362

fiEnsuring Consistency Hierarchical Execution

3. Solutions
goal develop architectural solutions allow agent support persistent
assumptions simultaneously avoid inconsistencies across hierarchical context
lead irrational behavior. introducing two new architectural solutions, however,
examine knowledge-based approaches consequences order provide
rationale architectural approach.
3.1 Knowledge-based Solutions
Inconsistency avoided hierarchical agents creating domain knowledge
recognizes potential inconsistencies responds removing assumptions. Many planning agent systems use explicit domain knowledge represent knowledge
interactions among assertions world. example, Entropy Reduction Engine
(ERE) (Bresina et al., 1993) one agent system relies knowledge-based assumption consistency (KBAC). ERE requires domain constraints, knowledge describes
physics task domain. Domain constraints identify impossible conditions.
instance, domain constraint would indicate robot cannot occupy two different
physical locations simultaneously.
ERE, domain constraints specifically used maintain consistency current
world model state execution (Bresina et al., 1993, pp. 166). However, many
architectures use KBAC well (perhaps conjunction methods). KBAC
knowledge viewed simply domain knowledge must added system
achieve consistent behavior.
KBAC always necessary maintain consistency among assumptions within
level hierarchy. However, order guarantee consistency assumptions distributed throughout hierarchy, possible interactions leading inconsistency must
identified throughout hierarchy. knowledge engineering problem add significant cost agent development. knowledge designer must specify conditions
assumption asserted also conditions must
removed. TacAir-Soar interception example, enemy plane flees, agent
requires knowledge disables assumptions depend upon number enemy
airplanes. Similarly, Dynamic Blocks World, agent must knowledge
recognizes situation, subtask, cause disabling empty.
cases, KBAC knowledge crosses levels hierarchy. complete KBAC solution
requires agents knowledge capture potential dependencies assumptions
local subtask higher levels hierarchy.
Although possible encode complete across-level consistency knowledge
simple domains, experience TacAir-Soar complex agent systems convinced
us KBAC requires significant investments time energy. Further,
often possible enumerate conditions assumption must removed,
agents also brittle, failing difficult-to-understand, difficult-to-duplicate ways.
insufficiency knowledge-based solutions led us consider architectural solutions
problem. Architectural solutions eliminate need domain knowledge encoded
address inconsistency hierarchical context assumptions within
subtask. Thus, cost developing individual agents reduced. addition
363

fiWray & Laird

generality, definition, architectural solutions also complete, thus able
guarantee consistency hierarchical context assumptions within subtask,
times, agent task. completeness improve robustness agent
systems, especially situations explicitly anticipated designers.
3.2 Assumption Justification
One potential architectural solution inconsistency problem justify assumption hierarchy respect assertions higher levels hierarchy. Assumption
Justification extension truth maintenance approaches consistency outlined
previously. assumption hierarchy treated entailment
respect dependent assertions higher hierarchy. new data structure, assumption justification, created captures reasons hierarchical context
particular assumption. Locally, assumption treated exactly like assumption
non-hierarchical system. However, assumption justification longer supported
(indicating change dependent hierarchical context), architecture retracts
assumption.
Refer Figure 2. agent asserts 34 , architecture builds assumption justification assumption includes 22 a21 . agent retracts
a21 , assumption justification 34 longer supported architecture also
retracts a34 . architecture ensures reasoning consistency across hierarchy levels
assumption persists longer context assertions led creation.
Assumption Justification solves inconsistency problem dependencies
hierarchical context captured justification. Within subtask, domain knowledge still required ensure consistency among enabled assumptions subtask.
However, across-level consistency knowledge needed. Assumption Justification still
supports local nonmonotonic hypothetical reasoning. Thus, Assumption Justification
appears meet functional evaluation criteria. However, order assess impact
performance, implementation details must considered.
3.2.1 Implementing Assumption Justification
Creating assumption justifications requires computing context dependencies assumption, similar computation justifications entailments. 2 Figure 7 outlines
procedure computing assumption justification data structure. procedure
invoked assertion created. procedure creates assumption justifications
every assertion local subtask; is, entailments well assumptions. approach allows architecture cache context dependencies local assertion.
advantage caching architecture simply concatenate assumption
justifications local assertions contributing directly creation assumption
2. Assumption Justification procedure, presented, requires inference engine record justification every assertion course processing. Soar, architecture Assumption
Justification implemented, calculations available production rule matcher
assumptions entailments. However, justification calculations assumptions may supported
architectures, requiring modifications underlying inference engine. Laird Rosenbloom
(1995) Soar Users Manual (Laird, Congdon, & Coulter, 1999) describe specific mechanisms
justification creation Soar.

364

fiEnsuring Consistency Hierarchical Execution

PROC create new assertion(. . .)
assumption justification computed new assertion
created. Thus, assumption justifications computed
assumptions entailments.
...
Ajust create justif ication(. . .)
Justifications created via well-known, textbook algorithms
(e.g., Forbus & deKleer, 1993; Russell & Norvig, 1995)
Aaj make assumption justif ication f assertion(A)
...
END
PROC make assumption justif ication f assertion(assertion A)
AJ NIL
assertion j Ajust , justification

1
(Level(j) closer root Level(A))
AJ append(j, AJ) (add j assumption justification)

2
ELSE
(j level)
AJ concatenate(jaj , AJ) (add assumption justification j
assumption justification A)
return AJ, list assertions comprising assumption justification
END
PROC Level(assertion A)
Return subtask level associated assertion
Figure 7: procedure building assumption justifications.
(in
2). chose caching option computing assumption justifications ondemand implementation would, assumption created, recursively follow
local dependencies context dependencies determined. advantage
caching implementation context dependencies assertion must computed once, even local assertion contributes creation multiple local
assumptions.
procedure creates assumption justification loops assertions
justification new assertion A. assertions justification either context
local assertions. Context assertions,
1, added assumption justification directly.
However, local assertions added assumption justification
assumption justification include context dependencies. example, architecture retract local assertion reasons change hierarchical
context (e.g., non-monotonic reasoning step change enabled assumptions
subtask) cases, agent necessarily retract dependent assumptions.
assumption justifications local assertions justification already
computed (i.e., cached, described above), assumption justification
365

fiWray & Laird

B C E

B C E

PSfrag replacements

(a)

12

(b)

1

Figure 8: Technical problems Assumption Justification. (a), assumption replaces
another assumption nonmonotonically. (b), multiple assumption justifications
assumption must supported.

local assertion j simply added assumption justification A,
2.
on-demand implementation, procedure would recur local assertions,
context dependencies local assertions contributing identified.
worst-case computational complexity algorithm polynomial number
assertions subtask. addition higher-level assertion done constanttime (a single pointer reference). However,
2 must uniquely add context assertions
assumption justification local assertion. (n 1) local assumption
justifications whenever nth assertion created. Thus, concatenation needs
performed (n1) times call assumption justification procedure.
limit provides upper bound O(n) complexity assumption justification
procedure: worst-case cost building individual assumption justification linear
number assertions, n, level. However, architecture executes assumption
justification procedure every assertion level. Thus, worst case cost building
justifications particular level O(1 + 2 . . . + n) O(n 2 ).
Non-monotonic changes complicate implementation. architecture must disable
replaced assumption, rather delete it, initial assumption may need
restored. example, Figure 8 (a), assume assertion E leads
assertion 2 local subtask retraction 1 (i.e., 2 revision 1).
agent retracts E, Assumption Justification retract 2, desired, must also
re-enable 1. Thus, assumption 1 must remain available memory, although disabled.
Figure 8 (b) illustrates second problem. assumption multiple assumption justifications. justifications change reasoning progresses. Assumption 1
initially depends assertions A, B, C higher levels. assume later
processing, agent removes A, normally would result retraction 1.
However, meantime, context changed 1 also justified {C,
D, E}. agent removes A, architecture immediately retract 1
must determine 1 justified sources.
implementation Assumption Justification Soar completed members
Soar research group University Michigan. Experiments using Air-Soar, flight
simulator domain (Pearson et al., 1993), showed overhead maintaining prior
assumptions level produced significant negative impact agent performance.
domain, Assumption Justification incurred significant computational cost, requiring
366

fiEnsuring Consistency Hierarchical Execution

least 100% time original Air-Soar agent. Further, number assumption
justifications maintained within level continued grow execution, reasons
explained above. subtasks required minutes execute aircraft performed
maneuver, leading large (and problematic) increases amount memory required.
Thus, Assumption Justification failed meet efficiency requirements theoretical
empirical grounds. Although limitations Assumption Justification might improved developing solutions technical problems, abandoned exploration
approach strongly discouraging results.
3.3 Dynamic Hierarchical Justification
Figure 2 introduced notion support set subtasks. Procedural Reasoning
System (PRS) (Georgeff & Lansky, 1987) Soar (Laird et al., 1987) use architectural
mechanisms retract complete levels subtask hierarchy support set longer
holds. section, consider solution leverages hierarchy maintenance
function ensure consistency assumptions higher level context.
significant disadvantage support set existing systems fixed.
Soar, support set computed initiation subtask updated
reflect reasoning occurs within subtask. example, Figure 2, suppose
assumption a34 depends assumptions a22 a21 (represented dashed, arrowed
lines). support set include 21 ; assertion may even present
Subtask3 created. local assumption depends assertion
support set, change assertion directly lead retraction
assumption (or subtask). Thus, approaches using Fixed Hierarchical Justification
(FHJ) still require knowledge-based solutions consistency. FHJ discussed
Section 5.1.2.
propose novel solution, Dynamic Hierarchical Justification (DHJ), similar
Fixed Hierarchical Justification, dynamically updates support set reasoning
progresses. Assumption justifications individual assumptions unnecessary. However,
one consequence simplification subtask (and assertions within it)
retracted dependent context changes. Refer Figure 2. DHJ agent
asserts a34 Figure 2, architecture updates support set Subtask 3 include a21 .
Assumption a22 already member support set need added again.
member support set Subtask 3 changes, architecture retracts entire
subtask. Thus Dynamic Hierarchical Justification enforces reasoning consistency across
hierarchy subtask persists long dependent context assertions.
3.3.1 Implementing Dynamic Hierarchical Justification
Figure 9 outlines procedure computing support set DHJ. Assumption
Justification, architecture directly add context assertions support set
1.
architecture computes dependencies local assertion
3, assertion
marked inspected
4. Inspected assertions simply ignored
future
2, architecture already added assertions dependencies
support set. architecture also ignore dependent, local assumptions
2
dependencies assumptions already added support set.
367

fiWray & Laird

PROC create new assertion(. . .)
Whenever new assumption asserted, support set updated
include additional context dependencies.
...
Ajust create justif ication(. . .)
assumption
subtask asserted
Ssupport set append(Ssupport set , add dependencies support set(A))
...
END
PROC add dependencies support set(assertion A)
assertion j Ajust , justification

1
{Level(j) closer root Level(A)}
append(j, S) (append context dependency support set)

2


3

4

ELSEIF {Level(j) Level(A)
j assumption
j previously inspected }
append(S, add dependencies support set(j))
(compute support set dependencies j add S)
jinspected true
(js context dependencies added support set)

return S, list new dependencies support set
END
PROC Level(assertion A)
Return subtask level associated assertion
Figure 9: procedure Dynamic Hierarchical Justification.
DHJ needs inspect local assertion once, context dependencies
computed on-demand, rather cached Assumption Justification. Condition
2
true whenever local entailment whose context dependencies yet
computed. dependencies determined calling add dependencies support set
recursively. Recursive instantiations add dependencies support set receive local
assertion justification uninspected entailment, j, return list comprising
context dependencies j. return value appended support set
prior instantiation add dependencies support set.
recursive call add dependencies support set
3 non-constant time
operation procedure. must made assertion j thus
worst case complexity compute dependencies linear number assertions
level, Assumption Justification. However, DHJ requires single inspection
individual assertion, rather repeated inspections new assumption As368

fiEnsuring Consistency Hierarchical Execution

sumption Justification. Thus architecture needs call add dependencies support set n times subtask consisting n assertions, worst case cost
updating support set level remains O(n). reduction complexity potentially makes Dynamic Hierarchical Justification efficient solution Assumption
Justification, especially number local assertions increases.
Additionally, two technical problems outlined Assumption Justification
impact DHJ. DHJ never needs restore previous assumption. dependency
changes, architecture retracts entire level. Thus, DHJ immediately delete
replaced assumptions memory. DHJ collects dependencies assumptions,
need switch one justification another. Figure 8 (b), dependencies
A, B, C, D, E added support set. simplifications make
support set overly specific reduce memory computation overhead required
Dynamic Hierarchical Justification.
DHJ retractions sometimes followed regeneration subtask
re-assertion reasoning retracted. example, enemy plane fled
described TacAir-Soar scenario, DHJ would retract entire level associated
counting subtask. count would need re-started beginning.
Section 3.4.3 examines potential problems introduced interruption regeneration.
cost incurred regeneration previously-derived assertions primary
drawback Dynamic Hierarchical Justification.
3.4 Implications Dynamic Hierarchical Justification
Dynamic Hierarchical Justification solves specific problem maintaining reasoning
consistency hierarchy, guaranteeing consistency utilizing efficient algorithm.
heuristic DHJ employs assumes assumptions closely associated
subtasks retracting subtasks nearly equivalent retracting individual assumptions.
section explores implications heuristic, focusing task decompositions,
impact agents ability use persistent assumptions, feasibility interrupting
agent (with subtask retraction) midst reasoning.
3.4.1 Influence Task Decomposition
agents reasoning viewed knowledge search (Newell, 1990). perspective, inconsistency problem failure backtrack knowledge search. world
changes, leading changes agent hierarchy. agent must retract
knowledge previously asserted, backtrack knowledge state consistent
world state.3 solution described terms way achieves (or
avoids) backtracking knowledge search. instance, KBAC leads knowledge-based
backtracking, KBAC knowledge tells agent correct assumptions
given current situation.
3. Obviously, world state usually different agents initial state often impossible
return prior state execution system. use backtrack section refer
retraction asserted execution knowledge remaining asserted knowledge consistent
currently perceived world state.

369

fiWray & Laird

B C E F G H

(a)

1

B C E
(b)

2

1

2

Figure 10: Examples (a) disjoint dependencies (b) intersecting assumption dependencies.

Assumption Justification form dependency-directed backtracking (Stallman &
Sussman, 1977). dependency-directed backtracking, regardless chronological order
architecture makes assertions, architecture identify retract
assertions contributed failure search retain assertions. Assumption Justification, architecture retracts assumptions directly affected
change context. Assumptions created later processing, dependent
change, unaffected. Consider examples Figure 10. (a), assumptions
1 2 depend upon disjoint sets assertions. Assumption Justification, removal assertion 1s assumption justification result retraction 1; 2
unchanged, even architecture asserted 2 1.
Dynamic Hierarchical Justification similar backjumping (Gaschnig, 1979). Backjumping heuristically determines state current search backtrack
backjump. heuristics used backjumping based syntactic features
problem. instance, constraint satisfaction problems, backjumping algorithm
identifies variable assignments related variable assignments via constraints specified problem definition. violation discovered, algorithm
backtracks recent, related variable (Dechter, 1990). Intervening variable assignments discarded. DHJ, assertion hierarchy changes, system
backjumps knowledge search highest subtask hierarchy dependent
change. Figure 10 (a) dependent assertions collected support set
subtask. higher level assertions change, entire subtask removed.
using DHJ, backjumping, previous knowledge search may need
repeated backtracking. Assume removal subtask Figure 10 (a)
due change A. similar subtask reinitiated, assumption 2 may need
regenerated. regeneration unnecessary 2 need retracted
avoid inconsistency. Dynamic Hierarchical Justification, agent retracts
reasoning dependent subtask (and lower levels hierarchy); assertions
dependent change context also removed. Thus, like backjumping, DHJ
uses syntactic feature reasoning (decomposition subtasks) choose backtracking
point backtracking always conservative possible.
Although subtask decomposition syntactic feature knowledge search,
strongly principled one, reflecting semantic analysis task knowledge designer. Hierarchical task decomposition based premise tasks broken
discrete units little interaction units; nearly decom370

fiEnsuring Consistency Hierarchical Execution

posable (Simon, 1969). Thus, goal hierarchical decomposition separate mostly
independent subtasks one another. consequence separation dependencies higher levels limited much possible (interaction subtasks
minimized) dependencies among assertions particular subtask
shared (otherwise, subtask could subdivided two independent subtasks).
course, often possible decompose given task many different ways.
cases domain imposes minimal constraint knowledge engineer significant
latitude crafting task decomposition.
situation illustrated Figure 10, (b) would complete decomposition
task knowledge engineer (a), assuming two alternatives represent
decomposition task. (b), number dependent assertions necessarily grow function number assumptions local level, (a)
does. Further, (a), two independent assumptions pursued. assumptions
could potentially inferred separate subtasks alternate decomposition. (b),
hand, assumptions subtask closely tied together terms
dependencies thus better asserted within subtask. dependencies assumptions 1 2 considerable overlap (b), Assumption Justification
pays high overhead cost track individual assumptions (most) everything
local subtask would removed simultaneously assertions B, C, changed. DHJ incurs overhead, DHJ better choice intersection
assumption dependencies high. Task knowledge structured like situation
(b), rather (a) would lead unnecessary retractions. (b) appears
better reflect well-decomposed tasks, Dynamic Hierarchical Justification constrain
knowledge development process improve resulting decompositions. Consequently,
nearly-decomposed tasks allow DHJ avoid unnecessary regenerations
avoiding processing overhead Assumption Justification.
3.4.2 Limiting Persistence DHJ
DHJ limits persistence subtasks, resulting assumptions persistent
assumptions typical truth maintenance systems. section explores consequences
limitations determine DHJ architectures 4 still provide persistence
necessary agent execution (Section 2.1).
DHJ retract subtask potential inconsistency could impact hypothetical
recursive reasoning like counting. Consider aircraft classification counting example.
Perhaps aircrafts altitude contributes hypothetical classification aircraft
(e.g., particular altitude speed combinations might suggest reconnaissance aircraft).
agent would create assumptions locally depend aircrafts altitude.
altitude (or altitude boundary) changes, assumption retracted.
retraction required avoid inconsistency. contacts altitude longer suggests
reconnaissance aircraft, assumption depended assertion
removed. DHJ captures dependencies performs retraction. agent
4. clarity, Assumption Justification already eliminated candidate solution,
following discussion focuses exclusively DHJ. However, Assumption Justification limits persistence
similarly.

371

fiWray & Laird

opportunity reconsider classification aircraft, pursue tasks
classification longer important.
DHJ also retract subtask assumption created local subtask
purpose remembering input (or elaboration input). example, agent
needed remember particular aircrafts altitude particular point time,
assumption cannot stored local subtask. DHJ limits persistence way
remembering within local subtask generally impossible.
order remember previous situations, assumptions asserted root task.
assumption asserted level never retracted higher level
dependencies (assuming percepts associated top level higher input
level, Theo, Mitchell et al., 1991). primary drawback requirement
remembering remembered items longer local subtask created them,
requiring additional domain knowledge manage remembered assumptions. However,
remembering already requires domain knowledge; possible remember assertion
regardless dependencies also able retract architecturally.
examples show Dynamic Hierarchical Justification still allows forms
persistence, trades capturing dependencies nonmonotonic assumptions local subtasks remembering assumptions root task, dependencies captured.
DHJ forces remembered items root task, also suggests fundamental aspect root task managing remembered assumptions. view
requirement positive consequence DHJ, forces knowledge engineers
better recognize reasons creating assumption (e.g., remembering vs. hypothetical) circumscribes remembering develop adopt functional
temporal theories manage assumptions created remembering (e.g., Allen, 1991;
Altmann & Gray, 2002).
3.4.3 Recovery Interruption DHJ
Dynamic Hierarchical Justification makes agent reactive environment, ensuring relevant changes environment lead retraction dependent
subtasks. DHJ imposes automatic interruption agent subtask retraction,
without evaluating state system first. Although automatic interruption increases
reactivity system, lead difficulties way override it.
section examine two cases uncontrolled interruption cause problems.
problems arise DHJ biases system reactive; is, respond automatically changes environment without deliberation. However, cases,
additional agent knowledge overcome bias make system deliberate
avoid uncontrolled interruption.
first problem arises sequence actions must completed
without interruption order subgoal achieved. processing interrupted,
possible, dynamics world, task cannot resumed.
example, imagine aircraft nearing point launch missile target.
task interrupted resumed, aircrafts position may changed
enough, relative target, additional steering commands necessary
372

fiEnsuring Consistency Hierarchical Execution

missile launched. case, may preferable interrupt original
launch sequence begun.
Consider two possible approaches achieving capability Dynamic Hierarchical
Justification architectures. first move processing root task.
root task interrupted, processing interrupted. However,
approach greatly restricts task hierarchically decomposed thus
considered last resort. second approach add new reasoning task
freezes external situation respect additional reasoning subtask.
new processing initiates execution subtask creates persistent structures
root task. persistent structures represent deliberate commitment
interrupted. remaining processing subtask accesses structures
execution task. Thus, persistent, even changes
surrounding situation would interrupted subtask, processing
insensitive changes interruption prevented. approach also requires
additional reasoning recognize completion uninterruptible behavior remove
persistent structures built initial subtask. reasoning reflects deliberate act,
signaling commitment longer holds. abstract, together additions
provide mechanism overcoming automatic interruption. disadvantage
approach that, part system design, subgoals cannot interrupted
must identified beforehand. subtasks, additional agent knowledge must
implemented create remove encapsulations dynamic data.
critical problem DHJ Wesson Oil problem: someone cooking
dinner higher-priority activity suddenly occurs (a hurt child), cook turn
stove (a cleanup procedure) leaving hospital (Gat, 1991b). problem
occurs change hierarchical context level far terminal
level hierarchy. situation, similar tasks may resumed initiated
following interruption. agent must therefore recognize whether cleanup
external and/or internal states necessary, and, so, perform cleanup. Even
DHJ, agent still behave appropriately right knowledge. particular,
agent must able recognize partially completed tasks (like cooking dinner)
able select cleanup actions specific task state (like turning stove burner).
DHJ requires remembered assumptions asserted root level
hierarchy, recognition task internal state available; need try reconstruct
state external environment alone. However, require analysis
task domain(s) knowledge engineer interruptible activity requiring
cleanup include triggering assertions cleanup root task.
work prompted desire architectural solutions inconsistency, yet
maintaining consistency efficiently lead interruptions, which, DHJ, requires
knowledge-based solutions problems arising automatic interruption. 5 However,
requirements imposed DHJ positive consequences. Subtask retractions
observed recovery development process help define must remembered
root task cleanup, significantly different laborious process debugging
5. Dynamic Hierarchical Justification could also used trigger meta-level deliberation rather
immediate subtask retraction. would possibly provide architectural solution question
deliberate potential inconsistency intention reconsideration (see Section 5.2).

373

fiWray & Laird

agent programs failing due inconsistency. theory, Dynamic Hierarchical
Justification imposes requirements handling interruptions pose serious questions
overall utility. practice, found addressing questions
problem variety recent agent implementations using Soar-DHJ architecture (e.g.,
Laird, 2001; Wray et al., 2002).

4. Empirical Evaluation Dynamic Hierarchical Justification
Architectural mechanisms like DHJ must efficient. demonstrated
algorithm efficient, question impact overall behavior generation
capability agent remains open question due interruption regeneration. Given
complexity agent-based systems domains applied,
analytical evaluations must extremely narrow scope, even require specialized
techniques (Wooldridge, 2000). section instead pursues empirical evaluation
Dynamic Hierarchical Justification, focusing efficiency responsiveness two domains
extremes continua agent domain characteristics. architectural
solution inconsistency motivated cost (and incompleteness) knowledgebased solutions, knowledge development costs also estimated.
4.1 Methodological Issues
Dynamic Hierarchical Justification general solution, applicable wide range agent
tasks. order evaluate solution, number methodological issues must
addressed. following describes three important issues choices made
evaluation.
4.1.1 Relative vs. Absolute Evaluation
constitutes good poor cost performance evaluations? general,
absolute evaluation performance cost difficult task itself, addition
agents knowledge architecture, determines overall cost performance results.
circumvent problem making relative comparisons agents using
original, Fixed Hierarchical Justification Soar architecture (FHJ agents) new agents
(DHJ agents). FHJ agents provide cost performance benchmarks, obviating
need absolute evaluations.
4.1.2 Addressing Multiple Degrees Freedom Agent Design
Even architecture task fixed, many different functional agents developed. one know comparative results valid general experimenter
control benchmarks new agents?
DHJ agents compared agents previously implemented others. systems
provide good performance targets, optimized performance,
minimize bias, developed independently.
FHJ systems used fixed benchmarks, modified. DHJ agents use
identical task decompositions employed FHJ agents initial knowledge
base. observed opportunities improve performance DHJ agents modifying
374

fiEnsuring Consistency Hierarchical Execution

either task decomposition re-designing significant portions agent knowledge
base. However, agent knowledge modified necessary correct behavior,
order ensure DHJ agents remained tightly constrained FHJ counterparts,
thus limiting bias evaluation.
4.1.3 Choice Representative Tasks
evaluation limited execution agents Dynamic Blocks World
reduced-knowledge version TacAir-Soar (micro-TacAir-Soar). choice
tasks domains considerable drawback benchmarks (Hanks, Pollack, & Cohen, 1993). Although choices motivated primarily availability domains
pre-existing FHJ agents, two domains represent opposite extremes many
domain characteristics. Micro-TacAir-Soar, like TacAir-Soar, inaccessible, nondeterministic, dynamic, continuous, Dynamic Blocks World simulator used
experiments accessible, deterministic, static discrete. primary motivation
using Dynamic Blocks World, less representative typical agent tasks
Micro-TacAir-Soar, assess cost employing DHJ domain priori
appears would useful (although Section 6 suggests DHJ prove useful even
relatively static domains). Thus, Dynamic Blocks World provide baseline
actual cost deploying algorithm, even though little benefit expected
deployment domain.
4.2 Evaluation Hypotheses
Although specific expectations differ different domains, differences dimensions
knowledge cost performance anticipated comparing DHJ agents
baseline agents. following discusses expectations metric(s) used
dimension.
4.2.1 Knowledge Engineering Cost
Knowledge engineering effort DHJ agents decrease comparison previously
developed agents. Knowledge Soar represented production rules. production
represents single, independent knowledge unit. assume addition productions represents increase cost measure knowledge cost counting number
productions type agent. number productions, course, provides
coarse metric cost complexity individual productions varies significantly.
However, productions removed DHJ agents often difficult
ones create. Therefore, difference number productions probably conservative
metric knowledge cost DHJ.
4.2.2 Performance: Efficiency Responsiveness
general, overall performance change little DHJ agents, compared FHJ
counterparts. Although Dynamic Hierarchical Justification add new architectural
mechanism, algorithm efficient contribute significant differences performance. Further, less domain knowledge need asserted
375

fiWray & Laird

across-level consistency knowledge incorporated architecture. Thus, applying across-level KBAC knowledge represented significant expense overall cost
executing task, DHJ agents might perform better FHJ agents.
two specific exceptions expectation. First, domains consistency knowledge (mostly) unnecessary task performance, FHJ agents may perform
better DHJ agents. example, Dynamic Blocks World requires little consistency knowledge DHJ architecture still update support set, even though
inconsistency-causing context changes arise.
Second, regeneration problematic, overall performance suffer. DHJ, whenever
dependent context changes, subtask retracted. change lead
different choice subtask, subtask necessarily regenerated. Thus,
DHJ, subtask regeneration occur, and, regeneration significant, performance
degradation result.
CPU execution time provides simple, single dimension gross performance. CPU
time reported individual experiments reflects time agent spends reasoning
initiating actions rather time takes execute actions environment.
Decisions: Soar, subtasks correspond selection operators subgoals
implementing operators. selection operator called decision. Soar
selects operator, tries apply operator. Soar reaches impasse cannot
apply newly selected operator. non-primitive operators lead generation
subgoal subsequent decision. example, Soar selects put-down operator
one decision creates subgoal implement put-down subsequent decision.
Together, two steps constitute notion subtask Soar.
number decisions thus used indication number subtasks
undertaken task. FHJ, subtask generally never interrupted terminated
(either successfully unsuccessfully). DHJ, subtasks interrupted whenever
dependent change occurs. Thus, decisions increase DHJ agents subtasks
interrupted re-started. Further, decisions increase substantially (suggesting
significant regeneration), overall performance degrade.
Production Firings: production rule fires conditions match result
applied current situation. Production firings decrease DHJ two reasons.
First, across-level consistency knowledge previously used FHJ agents
longer necessary (or represented); therefore, knowledge accessed. Second,
reasoning occurred inconsistency arose FHJ agents interrupted
eliminated. However, production firings increase significant regeneration necessary.
4.3 Empirical Evaluation Blocks World
Agents Dynamic Blocks World domain execution knowledge transform
initial configuration three blocks ordered tower using simulated gripper arm.
table simulation width nine blocks. agents task goal always
build 1-on-2-on-3 tower. agent built tower resulting 981 unique,
non-goal, initial configurations blocks. Table 1 summarizes results tasks.
expected, total knowledge decreased. Overall performance improved. Decisions increased,
expected, number rule firings increased well, anticipated.
376

fiEnsuring Consistency Hierarchical Execution

Rules
Decision Avg.
Avg. Rule Firings
Avg. CPU Time (ms)

FHJ
x
s.d.
188

87.1
20.9
720.3 153.5
413.1 121.6

DHJ
x
s.d.
175

141.1
38.7
855.6 199.6
391.6 114.0

Table 1: Summary knowledge performance data Blocks World. agents
performed tower-building task 981 configurations. Task order
randomly determined.

4.3.1 Knowledge Differences
Total knowledge decreased 7% DHJ agent. small reduction consistent
expectation. aggregate comparison misleading knowledge
added (16 productions) deleted (29).
Removing Consistency Knowledge: Soar, subtask operator subgoal
terminated separately. Soar monitors impasse-causing assertions determine subgoal
(such subtask goal) removed via FHJ. However, removal subtask
operator requires knowledge. original, FHJ architecture treats initiation
operator persistent assumption requires knowledge recognize selected
operator interrupted terminated. knowledge categorized consistency knowledge determines time subtask terminated,
even initiating conditions subtask longer hold.
DHJ, effects operators persistent; assertions entailments
situation. Thus, initiation subtask treated entailment
subtask remains selected long initiation conditions subtask hold.
change removes need knowledge terminate subtask: subtask
initiation conditions longer true, subtask automatically retracted. Thus,
termination knowledge removed subtask operators.
Filling Gaps Domain Knowledge: persistence subtasks original architecture allows FHJ agents ignore large parts state space domain knowledge.
example, knowledge initiates stack put-on-table subtasks assumes
gripper currently holding block. tasks executed, gripper,
course, grasp individual blocks. conditions initiating stack put-on-table
holding block ignored original domain knowledge.
DHJ agent requires knowledge determine subtasks choose
holding blocks, subtasks interrupted agent still holds block.
16 productions necessary, primarily stack put-on-table operators.
important note knowledge necessary domain knowledge. FHJ agents could
solve problem began task holding block lacked
domain knowledge states. additions thus positive consequence
DHJ. architectures enforcement consistency revealed gaps domain knowledge.
377

fiWray & Laird

4.3.2 Performance Differences
Somewhat surprisingly, overall performance DHJ agents (measured CPU time) improves slightly comparison FHJ agents, even though decisions production
firings increase. Soar-specific performance metrics considered individually
below, overall performance improvement considered.
Decisions: FHJ agents, average, made considerably fewer decisions DHJ agents.
difference consistent across every task. additional decisions result
removal subsequent regeneration subtasks. example, agent picks
block pursuit stack task, selection stack task must regenerated.
knowledge DHJ agents could modified avoid testing specific configurations
blocks thus avoid many regenerations.
Production Firings: number production firings also increased Blocks World.
increase production firings attributed knowledge added system
regeneration subtasks made additions necessary. relative increase
number production firings (19%) much smaller increase decisions (62%).
smaller difference attributed productions removed (and thus
fire).
CPU Time: Generally, production firings increase Soar, increase CPU
time expected. However, CPU time DHJ decreased slightly comparison FHJ
even though production firings increased. explain result, additional aspects
Soars processing must considered.
match cost production constant grows linearly number
tokens, partial instantiations production (Tambe, 1991). token indicates
conditions production matched variable bindings conditions.
Thus, token represents node search agents memory matching
instantiation(s) production. specific productions conditions are,
constrained search memory, thus costs less generate instantiation.
new productions added DHJ Blocks World agent specific
agents memory (i.e., external internal state) productions removed.
Further, simply fewer total productions also reduce amount total search
memory.6 informal inspection match time tokens several FHJ
DHJ runs showed number tokens decreased DHJ 10-15%. reduction
token activity primary source improvement Dynamic Blocks World DHJ
agent CPU time. improvement, course, general result provides
guarantee task domain cost matching increase rather
decrease.

6. RETE algorithm (Forgy, 1979) shares condition elements across different productions. Thus,
removal productions decreases total search removed productions contain condition elements
appearing remaining productions. perform exhaustive analysis condition
elements determine removed productions reduce number unique condition elements
RETE network.

378

fiEnsuring Consistency Hierarchical Execution

4.4 Empirical Evaluation TacAir-Soar
Converting TacAir-Soar DHJ architecture would expensive, requiring many
months effort. DHJ agents instead developed research instruction version
TacAir-Soar, Micro-TacAir-Soar (TAS). TAS agents use TacAir-Soar simulation
environment (ModSAF) interface knowledge fly missions, resulting order magnitude decrease number productions agents. However,
TAS uses tactics doctrine missions TacAir-Soar.
TAS, team two agents (lead wing) fly patrol mission described
previously. engage hostile aircraft headed toward within
specific range. lead agents primary role fly patrol route intercept
enemy planes. wings responsibility fly formation lead.
total knowledge significantly reduced, converting TAS DHJ agents relatively
inexpensive. However, results representative TacAir-Soar TAS
retains complexity dynamics TacAir-Soar.
patrol mission clearly-defined task termination condition like Dynamic
Blocks World. address problem, agent simulation executes ten
minutes simulator time. time, agent opportunity take off,
fly formation partner patrol, intercept one enemy agent, return patrol
intercept. actual TacAir-Soar scenario, activities would normally
separated much larger time scales. However, agent spends much time
patrol mission simply monitoring situation (waiting), rather taking new actions.
Ten minutes simulated time proved brief enough overall behavior
dominated wait-states, also providing time natural flow events.
running fixed period time, increase number decisions
attributed regeneration simply improvement decision cycle time. avoid
potential confusion running simulator constant cycle time. mode,
simulator update represents 67 milliseconds simulated time. agent
runs fixed period time fixed updates, FHJ DHJ agent execute
number decisions. problems due regeneration apparent
number rule firings degradation responsiveness. Additionally, general results
change significantly scenarios executed real-time mode normally
used TacAir-Soar agents. fixed cycle simply eliminates variability.
Although patrol scenario designed minimize variation run run,
TAS simulator inherently stochastic specific actions taken agent
time course actions varies task repeated. control variation, scenario run lead wing agents approximately 50 times. Logging
data collection significantly impacted CPU time performance statistics.
order control effect, actually ran scenario 99 times, randomly choosing
one agent (lead wing) perform logging functions (and discarding performance measures). agent performed logging functions. Data logging agents used
create Figure 9. performance measures logging agents recorded
conclusion scenario summarized Table 2.
379

fiWray & Laird

Lead Agent
Rules
Number runs (n)
Decisions
Outputs
Rule Firings
CPU Time (msec)

FHJ
591
43
x s.d.
8974
0.0
109.1 6.71
2438 122
1683 301

x
8974
142.8
2064
1030

DHJ
539
53
s.d.
0.0
7.03
81.1
242

Wing Agent
FHJ
DHJ
591
539
56
46
x s.d.
x s.d.
8958
0.0
8958
0.0
1704 42.7
869 12.8
16540 398
6321 104
12576 861
2175 389

Table 2: Summary TAS run data.
4.4.1 Improving Task Decompositions
TacAir-Soar DHJ agents required extensive knowledge revision. revision
unexpected. instance, unlike Dynamic Blocks World, TAS agents remember many
percepts, last known location enemy aircraft. previously described,
assertions remembering must located root level hierarchy, thus
requiring knowledge revision. However, problems discovered.
cases, FHJ agents took advantage inconsistency asserted knowledge. words,
FHJ agent allowed inconsistency assertions actually depended
inconsistencies apply new knowledge. two major categories knowledge.
Within-level consistency knowledge recognized specific inconsistencies (e.g., retraction
proposal subtask) trigger actions clean subtask
state. Complex subtasks allowed non-interruptible execution complex procedure
regardless continuing acceptability subtask. cases, agent knowledge
modified remove dependence inconsistency. Appendix provides
explanation original knowledge subsequent changes. Section 4.4.3 summarizes
changes quantitatively.
4.4.2 Results
Table 2 lists average data FHJ DHJ lead wing agents patrol/intercept
scenario modifications DHJ agents knowledge base completed.
results domain consistent expectations: total knowledge decreases, rule
firings decrease performance improves, substantially DHJ wing agent.
following sections explore results greater detail.
4.4.3 Knowledge Differences
Table 3 quantifies changes Soar production rules described above. 7 Modifications include deletions, additions changes. rule considered changed
conditions changed slightly, made type computation
subtask. example, changed within-level consistency knowledge refers
7. DHJ agent data generated knowledge base included changes accommodate
learning (Wray, 1998) changes included table completeness. presence
rules knowledge base negligible impact performance data reported here.

380

fiAcross-level
Consistency

Remembering

Within-level
Consistency

Complex
Subtasks

Learning

Miscellaneous

Ensuring Consistency Hierarchical Execution

44
0

36
32

9
5

10
21

4
0

8
1

0

33

8

0

24

0

FHJ Agent:
Deletions:
Additions:
DHJ Agent:
Additional Changes:

TOTALS
591
(111)
59
539
65

Table 3: Quantitative summary changes production rules FHJ agent knowledge
base DHJ agents.

entailed structure rather one created assumption, structure
located subtask. somewhat restrictive definition change inflates
addition deletion accounting. many cases production deleted immediately added different subtask. example, productions manipulate
motor commands moved local subtasks highest subtask. Almost
additions deletions Remembering category attributed move,
required synthesis new production knowledge.
Total knowledge required DHJ agents decreased. approximately 9% reduction achieved making type modification 40% FHJ agent
rules, may seem modest gain, given conversion cost. However, cost
artifact chosen methodology. DHJ agents constructed domain
without previously existing FHJ agents, least 9% decrease total knowledge would
expected. result thus suggests reduction cost agent design.
high conversion cost suggest converting much larger system, like TacAir-Soar,
would probably costly. hand, modifications made evident
identifiable regenerations architecture. Thus, 235 total changes made FHJ
knowledge base much easier make constructing similar number rules.
4.4.4 Performance Differences
performance results Table 2 show, DHJ agents improved performance relative
FHJ peers. However, improvements lead wing agents substantially
different. Differences tasks lead wing pilots led differences relative
improvements.
Lead Wing Agents: lead wing agent share knowledge base
perform different tasks TAS scenario. 8 differences lead differences
8. agents share knowledge base dynamically swap roles execution.
instance, lead exhausts long-range missiles, order wing take lead role,
take role wing itself.

381

fiWray & Laird

2000

DHJ Lead
DHJ Wing
FHJ Lead
FHJ Wing

Cumulative Outputs

1500

1000

500
intercept

launch missile

patrol turns

resume patrol
0
0

100

200

300

400

500

600

Time (sec)

Figure 11: Cumulative outputs course one ten minute scenario DHJ (black)
FHJ (gray) agents. Cumulative outputs lead agents represented
solid lines, wing agents dashed lines.

absolute performance. Recall leads primary responsibility fly patrol route
intercept enemy aircraft. hand, wings primary mission role follow
lead. different tasks require different responses agents.
agents overall reasoning activity often correlated output activity;
is, commands sends external environment take action it. Figure 11
summarizes output activity two pairs lead wing agents (FHJ & DHJ)
course ten-minute scenario. output activity leads mostly concentrated
places course scenario (take-off, intercept, launch-missile,
resuming patrol following intercept). wings concentrated output
activity occurs leads turn new leg patrol wings must follow
lead 180 degree turn. remainder section, focus DHJ
agents contrast lead wing agent behavior. discussion performance metrics
examine differences FHJ DHJ leads wings.
lead actually spends scenario waiting, short bursts reasoning
output activity occurring tactically important junctures scenario. patrol,
lead flies straight makes decision turn reaches end patrol
leg. lead monitors environment searches enemy planes. search
382

fiEnsuring Consistency Hierarchical Execution

(mostly) passive; agents radar notifies agent new entities detected.
detecting classifying enemy plane potential threat, lead commits
intercept. lead immediately makes number course, speed, altitude adjustments,
based tactical situation. actions evident figure pulse labeled
intercept. lead spends time intercept closing distance
aircraft get within weapon range, maneuver little thus
requiring actions environment (thus relatively flat slope following intercept).
agent reaches missile range enemy plane, lead executes number
actions quickly. lead steers plane launch window missile, pushes
fire button, waits missile clear, determines course maintain
radar contact missile flies target (at launch-missile). intercept
completed, lead resumes patrol task. Again, issues large number output
commands short period time. examples show leads reasoning focuses
primarily reacting discrete changes tactical situation (patrol leg ended, enemy
range, etc.) behavior generally requires little continuous adjustment.
execution wings follow-leader task, hand, requires reaction
continuous change leads position order maintain formation. Position corrections
require observing leads position, recognizing undesired separation formation,
responding adjusting speed, course, altitude, etc. wing following
lead throughout scenario, executing position maintenance knowledge almost
constantly. lead flying straight level, patrol leg, wings task
require generation many outputs. Figure 11, periods little activity
evident periodic flat segments wings cumulative outputs. lead
begins maneuver (e.g., turn), wing must maintain formation throughout
maneuver. turn wing generates many motor commands follows lead.
turn takes seconds complete, outputs increase gradually
course turn, seen figure. Thus, wing periodically encounters
dynamic situation requires significant reasoning motor responses. Further,
response change discrete, lead, occurs continuously
course leads maneuver.
differences tasks two agents account relatively large absolute
differences performance metrics lead wing agents. wings
adjusting positions relative leads, issue many output commands
leads, requires many inferences determine commands
be.
Decisions: differences decisions lead wing due artifact
data collection. lead agents ran extra second wings halted order
initiate data collection.
Production Firings: lead wing agents, production firings decrease. However, wings production firings decrease 62%, lead, decrease
15%. One reason large improvement DHJ wing due elimination
redundant output commands FHJ agents. FHJ wing sometimes issues
motor command once. reason duplication specific motor command computed locally, thus available subtasks. cases,
two subtasks may issue motor command. command stored locally,
383

fiWray & Laird

command may issued agent cannot recognize command
already issued another subtask. motor commands remembered
top subtask DHJ agents, inspected subtasks. DHJ wing thus
never issues redundant motor command. large relative decrease outputs
wing agent FHJ DHJ (Figure 11) attributed improvement. Production firings decrease decrease output activity reasoning activity
wing concerns reacting leads maneuvers.
contrast wing, leads average number outputs actually increases. Regeneration source additional outputs. situations, DHJ agents
subtask adjusting heading, speed altitude get updated repeatedly highly
dynamic situation (e.g., hard turn). FHJ agent uses subtask knowledge decide
current output command needs updated. However, DHJ, subtask may
retracted due dependence changing value (e.g., current heading). subtask regenerated following retraction, lead may generate slightly different motor
command. example, lead might decide turn heading 90.1 instead 90.2 .
decision causes generation new output command would
re-issued FHJ agents accounts small increase outputs. also suggests
without self-imposed constraint methodology, knowledge base could
modified avoid regeneration decrease production firings.
Although large magnitude improvement wing primarily due remembering motor commands, agents also needed less consistency knowledge thus
accessed less knowledge performing task. agents perform tasks
using less knowledge.
CPU Time: CPU time decreases DHJ lead wing agents. improvement
lead (39%) half improvement wing (81%). differences due
primarily decrease production firings. fewer production firings thus
fewer instantiations generate, leading improvements CPU time. Match time also
improved, contributing overall performance improvement. 9 larger improvements
CPU time compared production firings improvements (39% vs. 15% lead,
81% vs. 62% wing) might attributable decreases number rule
firings match time. Again, results offer guarantee match time always
decrease DHJ. important note, however, two different domains DHJ
reduces total knowledge constrains remaining knowledge. architecture
leveraged small differences improved overall performance.
4.4.5 Differences Responsiveness
CPU time decreases DHJ agents, responsiveness generally improve.
However, agent knowledge split several different subtasks,
actions may initiated quickly would initiated FHJ agent.
section, explore differences responsiveness one situations.
9. Dynamic Blocks World, trends based observations data, rather
significant analysis. particular, TAS, data number tokens generated collected.
results reported consistent expectation token activity falls DHJ agents,
compared FHJ agents.

384

fiEnsuring Consistency Hierarchical Execution

FHJ
DHJ

Avg. In-Range Time
(sec)
161.816
162.048

Avg. Launch Time
(sec)
162.084
162.993

Reaction Time
(sec)
.268
.945

n
95
99

Table 4: comparison average reaction times launching missile TAS.
enemy plane comes range, agent executes series actions, leading
firing missile. Reaction time difference time enemy
agent comes range time agent actually pushes fire button launch
missile. reaction time one measure agents responsiveness. Table 4 shows,
FHJ agent able launch missile quarter second. However,
DHJ agent three-and-a-half times slower FHJ agent launching
missile, taking almost full second, average.
Split subtasks, regeneration, subtask selection contribute increase
reaction time. Splitting subtask n steps, may executed
single decision previously, may take n decisions DHJ agent. actions
necessary launching missile one would expect increase of, most,
hundred milliseconds change. However, dividing subtasks separate steps,
sequential series actions interrupted. particular, number regenerations
occur launch-missile subtask agent prepares fire missile highly
dynamic situation. agent sometimes chooses undertake similar action
situation changed enough slightly different action might necessary, described
above. result DHJ agents taking accurate aim FHJ agents,
responding quickly dynamics environment. aiming,
however takes time, although increase time tactically significant (i.e.,
enemy planes escaping previously hit FHJ agents).
additional re-engineering knowledge would improve reaction time (e.g.,
described Section 3.4.3). However, decreases responsiveness difficult avoid,
general. Dynamic Hierarchical Justification requires subtasks different dependencies initiated terminated separately, risk unnecessary regeneration. However,
splitting complex tasks separate subtasks, individual actions delayed subtasks separate procedures, selection particular
subtask series postponed additional subtask choices available.
4.5 Summary Empirical Evaluations
Figure 12 summarizes results Dynamic Blocks World TAS. domains,
DHJ agents require fewer total productions, suggesting decrease knowledge cost. Performance roughly Dynamic Blocks World lead agents TAS.
DHJ wing agents show much greater improvement overall performance, due
DHJ changes knowledge. results suggest Dynamic Hierarchical
Justification expected reduce engineering effort degrade performance
variety domains, simple complex. However, response time situations may
decrease.
385

fiWray & Laird

600
500

Productions

400
300
200
FHJ Lead
FHJ Wing
FHJ DBW

100

DHJ Lead
DHJ Wing
DHJ DBW

0
0

1

2

3

4

10

11

12

13

14

CPU Time (sec)

Figure 12: Mean CPU Time vs. knowledge productions FHJ (black) DHJ (gray)
agents Dynamic Blocks World TAS. graph includes actual
distribution CPU time agent well mean agent.
Means Dynamic Blocks World agents illustrated squares, TAS
lead agents triangles, TAS wing agents diamonds.

5. Discussion
Solutions KBAC new solutions introduced developed
inconsistency problem (Wray, 1998). briefly introduce additional solutions,
also consider relationship Dynamic Hierarchical Justification intention reconsideration belief-desire-intention agents belief revision.
5.1 solutions inconsistency across hierarchy
section, review existing architectural solutions problem inconsistency arising persistence hierarchy assertions.
5.1.1 Limiting Persistence
One obvious approach eliminating inconsistency arising persistence disallow
persistent assumptions altogether. approach adopted Theo (Mitchell et al.,
1991). reasoning Theo entailed sensors; perceptual inputs unjustified.
Theo cannot reason non-monotonically particular world state; world
change non-monotonically. Thus, Theo cannot generally remember previous inputs.
386

fiEnsuring Consistency Hierarchical Execution

Another possible limitation would restrict assumptions single memory
(global state), or, equivalently, allow assumptions root level hierarchy
hierarchical architecture. solution ensures hierarchical context always
consistent (all assertions within associated subtask entailments) also allows
persistence. HTN execution systems RETSINA DECAF, mentioned
previously, global state, obviously suffer inconsistency
hierarchy. However, interactions persistent assertions new information
derived sensors problem systems global state.
RETSINA recently adopted rationale-based monitoring (Veloso, Pollack, & Cox,
1998) identify environmental changes could impact currently executing task network (Paolucci et al., 1999). Rationale-based monitoring uses structure plan knowledge (in case, plan operators, including task networks) alleviate inconsistency. Monitors relevant world features created dynamically planning progresses identifying
pre-conditions operators instantiating via straightforward taxonomy monitor types (e.g., monitor quantified conditions). collection monitors form plan
rationale, reasons support planners decisions (Veloso et al., 1998). Plan rationales
thus similar justifications used truth maintenance. Monitors activated
pre-condition element world changes. inform planner
change, planner deliberate whether change impact
plan construction and, so, consider appropriate repairs.
Rationale-based monitoring similar Dynamic Hierarchical Justification, especially
leverage structures (different) underlying task representations provide consistency. However, two important differences. First,
DHJ identifies specific subtask impacted change, require deliberation
determine impact change; immediate return consistent knowledge state
possible. monitor activated rationale-based monitoring, planner must
first determine plan affected, require deliberation. Second,
monitors trigger deliberation, rather automatically retracting reasoning,
agent using rationale-based monitoring determine plan repaired
how. DHJ (as implemented) offer flexibility; retraction automatic. Automatic retraction assumes cost retrieving (or regenerating) pre-existing plan knowledge
less costly deliberation determine if/how plan revised. plan
modification expensive plan generation (Nebel & Koehler, 1995), assumption reasonable. However, invoking deliberate revision process could circumvent
potential problems arising recovery interruption (Section 3.4.3).
5.1.2 Fixed Hierarchical Justification
mentioned previously, pre-DHJ version Soar Procedural Reasoning
System (PRS) (Georgeff & Lansky, 1987) use Fixed Hierarchical Justification retract
complete levels hierarchy support set longer holds. PRS, support
set consists set context elements must hold execution subtask.
elements defined knowledge engineer. Fixed Hierarchical Justification offers
complete solution inconsistency problem context references within reasoning
subtask limited support set. approach guarantees consistency. However,
387

fiWray & Laird

requires knowledge designer identify potentially relevant features used
reasoning within subtask. Additionally, resulting system may overly sensitive
features support set features rarely impact reasoning, leading
unnecessary regeneration.
Fixed Hierarchical Justification requires less explicit consistency knowledge knowledgebased solutions. However, KBAC knowledge still required access whole task hierarchy possible. Thus, agents ability make subtask-specific reactions unexpected
changes environment limited knowledge designers ability anticipate
explicitly encode consequences changes.
5.2 Intention Reconsideration
belief-desire-intention (BDI) model agency, intention represents commitment
achieving goal (Rao & Georgeff, 1991; Wooldridge, 2000). intention thus similar
instantiation subtask hierarchical architecture.
Dynamic Hierarchical Justification viewed partial implementation intention reconsideration (Schut & Wooldridge, 2000, 2001). Intention reconsideration
process determining agent abandon intentions (due goal achievement, recognition failure, recognition intention longer desired).
Dynamic Hierarchical Justification partial implementation intention reconsideration able capture syntactic features problem solving (i.e.,
identification dependencies via support set) determine reconsider
intention. Situations require deliberation determine intention
abandoned captured DHJ.10 Schut & Wooldridge (2001) describe initial attempt allow run-time determination reconsideration policies. optimal policy
would maximize likelihood deliberate intention reconsideration actually leads
abandoning intention (i.e., agent reconsiders reconsideration necessary).
contrast, Dynamic Hierarchical Justification offers low-cost, always available, domain general process abandoning intentions, cannot automatically identify reconsiderations
requiring semantic analysis problem state.
BDI models, agents choose execute current action plans without
reconsidering current intentions first. Kinny Georgeff (1991) showed that,
static domains, bold agents never reconsider intentions perform effectively
cautious agents always reconsider executing plan step. opposite
true highly dynamic domains: cautious agents perform bold ones. Soar
PRS described cautious via Fixed Hierarchical Justification. is,
plan step, architectures determine elements support set remain asserted
executing step. FHJ approaches are, effect, bold appear,
reconsider intentions assertions changed dependent
context, support set. Dynamic Hierarchical Justification provides
cautious agents, ensures agents reconsideration function takes
account context dependencies subtask reasoning. perspective intention
10. DHJ preclude deliberate reconsideration. However, Soar (as testbed exploration
DHJ) provide architectural solution deliberate reconsideration. Thus, situations
addressed knowledge deliberative processes architecture.

388

fiEnsuring Consistency Hierarchical Execution

reconsideration, problems introduced dynamic domains prompted us explore
cautious solutions.
results empirical analysis somewhat consistent Kinny &
Georgeff. cautious DHJ agents performed better less cautious FHJ agents
highly dynamic TacAir-Soar domain. Dynamic Blocks World, performance
differences equivocal. comparison FHJ number new intentions
increased Dynamic Hierarchical Justification (measured Soar decisions).
slight overall performance improvement DHJ, due improvements
match time productions, Soar-specific measure likely generalize
systems. results suggest DHJ possibly overly cautious static
domains. However, Dynamic Hierarchical Justification present significant
performance cost unexpectedly played constructive role agent execution even
static domain, DHJ seems warranted static dynamic domains.
5.3 Belief Revision
Belief Revision refers process changing beliefs accommodate newly acquired
information. inconsistency problem example need revision asserted
beliefs: change hierarchical context (deriving ultimately perceived changes
world) leads situation currently asserted assumption would (necessarily) regenerated re-derived. Theories belief revision identify functions
used update belief set remains consistent.
best known theory belief revision AGM theory (Alchourron, Gardenfors,
& Makinson, 1985; Gardenfors, 1988, 1992). AGM coherence theory, meaning
changes beliefs determined based mutual coherence one another. approach contrasts foundations approach, justifications (reasons) determine
when/how revise belief set. Obviously, Dynamic Hierarchical Justification extension foundations approach belief revision. However, foundations
coherence approaches reconciled (Doyle, 1994), section explore repercussions Dynamic Hierarchical Justification context AGM theory belief
revision.
AGM theory, new sentence presented database sentences representing
current knowledge state, agent faced task revising knowledge base
via one three processes: expansion (adding sentences knowledge base), contraction
(removing sentences knowledge base) revision (a combination expansions
contractions). AGM theory emphasizes making minimal changes knowledge base
epistemic entrenchment, notion usefulness sentence within database.
AGM theory prefers sentences high epistemic entrenchment (relative
sentences) retained revision.
Comparing Dynamic Hierarchical Justification Assumption Justification suggests
sometimes cheaper remove subtask (and asserted beliefs associated
subtask) compute minimal revision Assumption Justification.
context belief revision, result surprising, since shown
computing minimal revision knowledge base computationally harder
deduction (Eiter & Gottlob, 1992). theoretical result led applications
389

fiWray & Laird

compute belief updates via incremental derivations belief state, rather via belief
revision (Kurien & Nayak, 2000).
power heuristic approach used DHJ analytic solution follows
characteristics outlined Section 3.4.1: hierarchical structure organization
agent assertions efficiency underlying reasoning system regenerate
unnecessarily removed assertions. Assumptions (persistent beliefs) associated particular subtasks hierarchical architectures. change perception (an epistemic input)
leads revision. Rather determining minimal revision, DHJ uses heuristic
that, context, says persistent beliefs subtask similar epistemic entrenchment subtask/intention itself. cases, heuristic incorrect,
leading regeneration, but, correct, provides much simpler mechanism revision. Gardenfors (1988) anticipates conclusions, suggesting systems possessing
additional internal structure (as compared relatively unstructured belief sets
AGM theory) may provide additional constraints orderings epistemic entrenchment.

6. Conclusion
empirical results Dynamic Blocks World TAS domains consistent expectations: knowledge engineering cost decreased overall performance
DHJ roughly (or slightly improved) comparison independently-developed
FHJ benchmarks. Development cost decreases designer freed task
creating across-level consistency knowledge. One drawback DHJ responsiveness
degrade regeneration occurs.
DHJ incorporated currently released version Soar (Soar 8)
3 years experience users confirms development cost decreases.
partly true developers need deeper understanding architecture realize
benefit. However, DHJ removes need encoding across-level consistency
knowledge, proven difficult understand encode many systems. DHJ also
makes understanding role assumptions Soar systems straightforward, imposing design development constraints. instance, knowledge designer must
think why, when, persistence used agent. knowledge designer determines functional role persistent assumption, DHJ guides
development knowledge necessary assumption. nonmonotonic hypothetical assumption, knowledge must created looks outside subtask
order ensure consistency (i.e., across-level knowledge necessary). Assumptions
remembering must asserted root level hierarchy, knowledge must
created manage remembered assumption. Functions root task include
monitoring, updating, removing remembered assumptions (we developing domaingeneral methods managing remembered assumptions reduce cost). Thus,
DHJ increase complexity architecture, makes design decisions
explicit manageable previous KBAC approaches.
Regeneration, seemingly one drawbacks DHJ, also contributes decreased
knowledge development costs. Regeneration serves debugging tool, allowing immediate
localization problem areas domain knowledge (and specific decomposition).
debugging aid contrasts previous knowledge development inconsistency
390

fiEnsuring Consistency Hierarchical Execution

often became evident irrational behavior, making often difficult determine
actual source problem. Thus, addition reducing total knowledge necessary
task, Dynamic Hierarchical Justification might also reduce cost per knowledge
unit creating agent knowledge localizing problems via regeneration. However,
case domain cannot decomposed nearly decomposable subunits,
regeneration could debilitating.
Another positive consequence DHJ agent may behave robustly novel
situations anticipated knowledge engineer. example, simple experiment,
FHJ DHJ Dynamic Blocks World agents placed situation described
Figure 3. FHJ agent fails block moves lacks knowledge recognize
moving blocks; knowledge designer assumed static domain. knowledge,
however, DHJ agent responds situation gracefully. specific situation
Figure 3, DHJ agent immediately retracts put-on-table(3) subtask,
block-3 table, thus selection subtask longer consistent
current situation. agent chooses stack(2,3) decomposes subtask
actions put block-2 block-3. new block (e.g., block-4) placed
empty space block-2, architecture responds retracting subtask goal
put-down(2) (i.e., subtask contains empty assumption). begins search
empty spaces order continue attempt put block-2 table.
architecture, rather agent knowledge, ensures consistency across hierarchy, DHJ
agents less brittle situations explicitly anticipated agent design.
DHJ also provides solution problem learning rules non-contemporaneous
constraints (Wray, Laird, & Jones, 1996). Non-contemporaneous constraints arise
temporally distinct assertions (e.g., red light, green light) collected single learned
rule via knowledge compilation. rule non-contemporaneous constraints lead
inappropriate behavior rather never apply. problem makes difficult
use straightforward explanation-based learning approaches operationalize agent execution
knowledge. Non-contemporaneous constraints arise architecture creates persistent
assumptions become inconsistent hierarchical context (Wray et al., 1996).
DHJ never allows inconsistency, solves non-contemporaneous problem.
instance, agents Dynamic Blocks World TAS able learn
unproblematically new architecture, no/little knowledge re-design. Wray (1998)
provides additional details empirical assessment learning.
Dynamic Hierarchical Justification operates higher level granularity Assumption Justification knowledge-based solution methods, trading fine-grained consistency lower computational cost. higher level abstraction introduce additional cost execution. particular, necessary regeneration led redundancy
knowledge search Dynamic Blocks World TAS agents. Although overall efficiency improved, improvement due improvements average
match cost productions, cannot guaranteed domains architectures. Further, Dynamic Hierarchical Justification requires complex subtasks
split distinct subtasks. requirement improves knowledge decomposition
reduces regeneration performance reduce responsiveness. However,
straightforward compilation reasoning subtasks DHJ enables, reduction
responsiveness overcome learning (Wray, 1998).
391

fiWray & Laird

Although implementation evaluation DHJ limited Soar, attempted
reduce specificity results Soar two ways. First, identified problems across-level consistency knowledge introduces knowledge-based approaches:
expensive develop, degrades modularity simplicity hierarchical representation, robust knowledge designers imagination. agents
developed sufficiently complex domains, expense creating knowledge grow
prohibitive. cost may lead additional researchers consider architectural assurances
consistency. Second, Dynamic Hierarchical Justification gains power via structure
hierarchically decomposed tasks. Although specific implementations may differ
agent architectures, heuristic simplifications employed DHJ transfer
architecture utilizing hierarchical organization memory task decomposition. Dynamic Hierarchical Justification efficient, architectural solution ensures reasoning
consistency across hierarchy agents employing hierarchical task decompositions.
solution allows agents act reliably complex, dynamic environments
fully realizing low cost agent development via hierarchical task decomposition.

Acknowledgments
work would possible without contributed directly
development evaluation Dynamic Hierarchical Justification. Scott Huffman, John
Laird Mark Portelli implemented Assumption Justification Soar. Ron Chong implemented precursor DHJ. Randy Jones, John Laird, Frank Koss developed TacAirSoar. Sayan Bhattacharyya, Randy Jones, Doug Pearson, Peter Wiemer-Hastings,
members Soar group University Michigan contributed development Dynamic Blocks World simulator. anonymous reviewers provided valuable,
constructive comments earlier versions manuscript. work supported
part University Michigan Rackham Graduate School Pre-doctoral fellowship, contract N00014-92-K-2015 Advanced Systems Technology Office DARPA
NRL, contract N6600I-95-C-6013 Advanced Systems Technology Office
DARPA Naval Command Ocean Surveillance Center, RDT&E division. Portions work presented 15 th National Conference Artificial Intelligence
Madison, Wisconsin.

Appendix A: Improving Task Decompositions
appendix describes detail changes made TAS agent knowledge
DHJ.
Remembering: Figure 4 showed agent computing new heading subtask
achieve-proximity subtask. calculation usually depends upon current heading.
agent generates command turn, heading changes soon thereafter.
situation, DHJ agent must remember already made decision turn
new heading placing assumption reflects new heading top level.
places assumption local level, new current heading trigger
removal turn-to-heading regeneration subtask (if agent determines
still needs turn new heading).
392

fiEnsuring Consistency Hierarchical Execution

FHJ agents, output commands (such turn specific heading)
asserted assumptions local subtask. DHJ agents knowledge changed
issue output commands directly output interface (which, Soar, always part
highest subtask hierarchy). unnecessary regeneration occurs
agent remembers motor commands generates new one different
output necessary. change, course, requires consistency knowledge
motor commands unjustified thus must explicitly removed, true
remembered knowledge DHJ.
Within-level Consistency Knowledge: Dynamic Hierarchical Justification, like solutions across-level consistency problem, still requires consistency knowledge within
individual subtask. knowledge FHJ agents used remove intermediate results execution subtask. clean knowledge allows agent
remove local assertions contributed terminating subtask thus avoid
(mis)use assertions later reasoning.
example, consider achieve-proximity subtask. subtask used
number different situations agent needs get closer another agent.
wing strays far lead, may invoke achieve-proximity get back
formation lead. lead uses achieve-proximity get close enough enemy
aircraft launch missile. subtask requires many local computations agent
reasons heading take get closer another aircraft. specific
computation depends information available aircraft.
wing pursuing lead, may know leads heading thus calculate collision
course maximize rate convergence. Sometimes agents heading
available. case, agent simply moves toward current location agent.
local computations stored local subtask. achieve-proximity
terminated FHJ agent, agent removes local structure. Removing structure
important interrupts entailment local structure (e.g., calculation
current collision course) guarantees agent decides achieve-proximity
different aircraft, supporting data structures properly initialized. knowledge
thus maintains consistency local subtask removing local structure
achieve-proximity subtask longer selected.
FHJ agent could recognize going remove subtask. termination
conditions FHJ agents acted signal within-level consistency knowledge.
knowledge removes local structure achieve-proximity summarized as:
achieve-proximity operator selected, initiation conditions longer hold,
remove local achieve-proximity data structure. Thus, FHJ agent uses
recognition inconsistency assertions trigger activation within-level
consistency knowledge.
subtasks initiating conditions longer supported DHJ agents,
selected subtask removed immediately. Thus, DHJ agent never opportunity
apply FHJ agents within-level consistency knowledge. failure utilize
knowledge led number problems, including regenerations expected.
solve problem, local subtask data structure created entailment
initiation conditions subtask itself. subtask initiation conditions
longer held, subtask selection local structure immediately removed
393

fiWray & Laird

architecture, requiring additional knowledge. Thus, change obviated need
within-level consistency knowledge. However, local data structure may need
regenerated subtask temporarily displaced. instance, FHJ within-level
consistency knowledge could determine conditions local structure
removed. DHJ solution lost flexibility.
Subtasks Complex Actions: FHJ agents execute number actions rapid
succession, regardless inconsistency local assertions. single subtask operator
initiated situation representing conditions apply first
action sequence, terminated last step sequence applied.
intermediate step invalidates initiation conditions, subtask still executes
actions.
Consider process launching missile. actual missile launch requires
push button, assuming previous steps selecting target
appropriate missile accomplished beforehand. pushing fire button,
pilot must fly straight level seconds missile rockets ignite launch
missile flight. missile cleared aircraft, agent supports
missile keeping radar contact target. FHJ agents, push-fire-button
subtask includes act pushing fire button counting missile clears
aircraft. tasks different mutually exclusive dependencies. initiation
condition push-fire-button requires missile already launched. However,
subsequent counting requires monitoring newly launched missile.
DHJ agents using FHJ knowledge base always remove push-fire-button subtask soon missile perceived air, interrupting complete procedure.
Regeneration push-fire-button subtask occurs agent never waits
missile clear thus never realizes missile launched needs supported.
DHJ agent unsuccessfully fires available missiles enemy plane.
Pushing fire button waiting missile clear independent tasks
happen arise serial order domain. enforced independence
creating new subtask, wait-for-missile-to-clear, depends
newly launched missile air. DHJ agent pushes fire button, selects
wait-for-missile-to-clear count seconds taking action,
supports missile clears successfully.
solution reduces regeneration improves behavior quality
non-trivial cost. Whenever subtask split, effects subtask actions longer occur
rapid succession within decision. Instead, effect first subtask occurs one
decision, effect second subtask second decision, etc. Thus, solution
compromise responsiveness.

References
Agre, P. E., & Horswill, I. (1997). Lifeworld analysis. Journal Artificial Intelligence
Research, 6, 111145.
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),
510530.
394

fiEnsuring Consistency Hierarchical Execution

Allen, J. F. (1991). Time time again. International Journal Intelligent Systems,
6 (4), 341355.
Altmann, E. M., & Gray, W. D. (2002). Forgetting remember: functional relationship
decay interference. Psychological Science, 13, 2733.
Bresina, J., Drummond, M., & Kedar, S. (1993). Reactive, integrated systems pose new
problems machine learning. Minton, S. (Ed.), Machine Learning Methods
Planning, pp. 159195. Morgan Kaufmann, San Francisco, CA.
Dechter, R. (1990). Enhancement schemes constraint processing: Backjumping, learning
cutset decomposition. Artificial Intelligence, 41, 273312.
Doyle, J. (1979). truth maintenance system. Artificial Intelligence, 12, 231272.
Doyle, J. (1994). Reason maintenance belief revision. Gardenfors, P. (Ed.), Belief
Revision, pp. 2951. Cambridge University Press, Cambridge, UK.
Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,
updates, counterfactuals. Artificial Intelligence, 57, 227270.
Erol, K., Hendler, J., & Nau, D. S. (1994). HTN planning: Complexity expressivity.
Proceedings 12th National Conference Artificial Intelligence, pp. 11231128.
Firby, R. J. (1987). investigation reactive planning complex domains. Proceedings 6th National Conference Artificial Intelligence, pp. 202206.
Forbus, K. D., & deKleer, J. (1993). Building Problem Solvers. MIT Press, Cambridge,
MA.
Forgy, C. L. (1979). Efficient Implementation Production Systems. Ph.D. thesis,
Computer Science Department, Carnegie-Mellon University.
Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.
MIT Press, Cambridge, MA.
Gardenfors, P. (1992). Belief revision. Pettorossi, A. (Ed.), Meta-Programming Logic.
Springer-Verlag, Berlin, Germany.
Gaschnig, J. (1979). Performance measurement analysis certain search algorithms.
Tech. rep. CMU-CS-79-124, Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.
Gat, E. (1991a). Integrating planning reacting heterogeneous asynchronous architecture mobile robots. SIGART BULLETIN, 2, 7174.
Gat, E. (1991b). Reliable, Goal-directed Control Autonomous Mobile Robots. Ph.D.
thesis, Virginia Polytechnic Institute State University, Blacksburg, VA.
Georgeff, M., & Lansky, A. L. (1987). Reactive reasoning planning. Proceedings
6th National Conference Artificial Intelligence, pp. 677682.
Graham, J., & Decker, K. (2000). Towards distributed, environment-centered agent framework. Wooldridge, M., & Lesperance, Y. (Eds.), Lecture Notes Artificial Intelligence: Agent Theories, Architectures, Languages VI (ATAL-99). Springer-Verlag,
Berlin.
395

fiWray & Laird

Hanks, S., Pollack, M., & Cohen, P. R. (1993). Benchmarks, test beds, controlled experimentation design agent architectures. AI Magazine, 14, 1742.
Hayes-Roth, B. (1990). architecture adaptive intelligent systems. Workshop
Innovative Approaches Planning, Scheduling Control, pp. 422432.
Jones, R. M., Laird, J. E., Neilsen, P. E., Coulter, K. J., Kenny, P., & Koss, F. V. (1999).
Automated intelligent pilots combat flight simulation. AI Magazine, 20 (1), 2741.
Kinny, D., & Georgeff, M. (1991). Commitment effectiveness situated agents.
Proceedings 12th International Joint Conference Artificial Intelligence, pp.
8288.
Kurien, J., & Nayak, P. P. (2000). Back future consistency-based trajectory
tracking. Proceedings 17th National Conference Artificial Intelligence,
pp. 370377.
Laird, J. E. (2001). knows going do: Adding anticipation Quakebot.
Proceedings 5th International Conference Autonomous Agents, pp. 385
392.
Laird, J. E., Congdon, C. B., & Coulter, K. J. (1999). Soar users manual version 8.2.
Manual, Department Electrical Engineering Computer Science, University
Michigan, http://ai.eecs.umiuch.edu/soar/docs.html.
Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). Soar: architecture general
intelligence. Artificial Intelligence, 33, 164.
Laird, J. E., & Rosenbloom, P. S. (1990). Integrating execution, planning, learning
Soar external environments. Proceedings 8 th National Conference
Artificial Intelligence, pp. 10221029.
Laird, J. E., & Rosenbloom, P. S. (1995). evolution Soar cognitive architecture.
Steier, D., & Mitchell, T. (Eds.), Mind Matters: Contributions Cognitive
Computer Science Honor Allen Newell. Lawrence Erlbaum Associates, Hillsdale,
NJ.
McDermott, D. (1991). general framework reason maintenance. Artificial Intelligence,
50, 289329.
Mitchell, T. M., Allen, J., Chalasani, P., Cheng, J., Etzioni, O., Ringuette, M., & Schlimmer,
J. C. (1991). Theo: framework self-improving systems. VanLehn, K. (Ed.),
Architectures Intelligence, chap. 12, pp. 323355. Lawrence Erlbaum Associates,
Hillsdale, NJ.
Mitchell, T. M. (1990). Becoming increasingly reactive. Proceedings 8 th National
Conference Artificial Intelligence, pp. 10511058.
Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: theoretical
empirical analysis. Artificial Intelligence, 76, 427454.
Newell, A. (1990). Unified Theories Cognition. Harvard University Press, Cambridge,
MA.
396

fiEnsuring Consistency Hierarchical Execution

Paolucci, M., Shehory, O., Sycara, K. P., Kalp, D., & Pannu, A. (1999). planning component RETSINA agents. Wooldridge, M., & Lesperance, Y. (Eds.), Lecture Notes
Artificial Intelligence: Agent Theories, Architectures, Languages VI (ATAL99), pp. 147161, Berlin. Springer-Verlag.
Pearson, D. J., Huffman, S. B., Willis, M. B., Laird, J. E., & Jones, R. M. (1993).
symbolic solution intelligent real-time control. Robotics Autonomous Systems,
11, 279291.
Rao, A. S., & Georgeff, M. P. (1991). Modeling rational agents within BDI-architecture.
Proceedings 2nd International Conference Principles Knowledge Representation Reasoning, pp. 471484.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentice Hall,
Upper Saddle River, NJ.
Sacerdoti, E. D. (1975). nonlinear nature plans. Proceedings 4 th International Joint Conference Artificial Intelligence, pp. 206214.
Schut, M., & Wooldridge, M. (2000). Intention reconsideration complex environments.
Proceedings 4th International Conference Autonomous Agents, pp. 209216.
Schut, M., & Wooldridge, M. (2001). Principles intention reconsideration. Proceedings
5th International Conference Autonomous Agents, pp. 340347.
Shoham, Y. (1993). Agent-oriented programming. Artificial Intelligence, 60 (1), 5192.
Simon, H. A. (1969). Sciences Artificial. MIT Press, Cambridge, MA.
Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning dependency-directed
backtracking system computer aided circuit analysis. Artificial Intelligence,
9 (2), 135196.
Sycara, K., Decker, K., Pannu, A., Williamson, M., & Zeng, D. (1996). Distributed intelligent agents. IEEE Expert, 11 (6), 3646.
Tambe, M. (1991). Eliminating Combinatorics Production Match. Ph.D. thesis,
Carnegie-Mellon University. (Also published Technical Report CMU-CS-91-150,
Computer Science Department, Carnegie Mellon University.).
Tambe, M., Johnson, W. L., Jones, R. M., Koss, F., Laird, J. E., Rosenbloom, P. S., &
Schwamb, K. (1995). Intelligent agents interactive simulation environments. AI
Magazine, 16 (1), 1539.
Veloso, M. M., Pollack, M. E., & Cox, M. T. (1998). Rationale-based monitoring planning dynamic environments. Proceedings 4 th International Conference
Artificial Intelligence Planning Systems, pp. 171180.
Wilkins, D. E., Myers, K. L., Lowrance, J. D., & Wesley, L. P. (1995). Planning reacting
uncertain dynamic environments. Journal Experimental Theoretical
Artificial Intelligence, 7 (1), 197227.
Wooldridge, M. (2000). Reasoning Rational Agents. MIT Press, Cambridge, MA.
Wray, R. E. (1998). Ensuring Reasoning Consistency Hierarchical Architectures. Ph.D.
thesis, University Michigan. Also published University Michigan Technical
Report CSE-TR-379-98.
397

fiWray & Laird

Wray, R. E., & Laird, J. (1998). Maintaining consistency hierarchical reasoning.
Proceedings 15th National Conference Artificial Intelligence, pp. 928935.
Wray, R. E., Laird, J., & Jones, R. M. (1996). Compilation non-contemporaneous constraints. Proceedings 13th National Conference Artificial Intelligence, pp.
771778.
Wray, R. E., Laird, J. E., Nuxoll, A., & Jones, R. M. (2002). Intelligent opponents virtual
reality trainers. Proceedings Interservice/Industry Training, Simulation
Education Conference (I/ITSEC) 2002.

398

fiJournal Artificial Intelligence Research 19 (2003) 209-242

Submitted 12/02; published 9/03

Decision-Theoretic Bidding Based Learned Density
Models Simultaneous, Interacting Auctions

pstone@cs.utexas.edu

Peter Stone

Dept. Computer Sciences, University Texas Austin
1 University Station C0500, Austin, Texas 78712-1188 USA

Robert E. Schapire

schapire@cs.princeton.edu

Michael L. Littman

mlittman@cs.rutgers.edu

Department Computer Science, Princeton University
35 Olden Street, Princeton, NJ 08544 USA
Dept. Computer Science, Rutgers University
Piscataway, NJ 08854-8019 USA

janos@pobox.com

Janos A. Csirik

D. E. Shaw & Co.
120 W 45th St, New York, NY 10036 USA

David McAllester

Toyota Technological Institute Chicago
1427 East 60th Street, Chicago IL, 60637 USA

mcallester@tti-chicago.edu

Abstract

Auctions becoming increasingly popular method transacting business, especially Internet. article presents general approach building autonomous
bidding agents bid multiple simultaneous auctions interacting goods. core
component approach learns model empirical price dynamics based past
data uses model analytically calculate, greatest extent possible, optimal
bids. introduce new general boosting-based algorithm conditional density
estimation problems kind, i.e., supervised learning problems goal
estimate entire conditional distribution real-valued label. approach fully
implemented ATTac-2001, top-scoring agent second Trading Agent Competition
(TAC-01). present experiments demonstrating effectiveness boosting-based
price predictor relative several reasonable alternatives.
1. Introduction

Auctions increasingly popular method transacting business, especially
Internet. auction single good, straightforward create automated bidding
strategies|an agent could keep bidding reaching target reserve price, could
monitor auction place winning bid closing time (known sniping).
bidding multiple interacting goods simultaneous auctions,
hand, agents must able reason uncertainty make complex value assessments. example, agent bidding one's behalf separate auctions camera
ash may end buying ash able find affordable camera.
Alternatively, bidding good several auctions, may purchase two ashes
one needed.
c 2003 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiStone, Schapire, Littman, Csirik, & McAllester

article makes three main contributions. first contribution general approach building autonomous bidding agents bid multiple simultaneous auctions
interacting goods. start observation key challenge auctions
prediction eventual prices goods: complete knowledge eventual prices,
direct methods determining optimal bids place. guiding principle
agent model uncertainty eventual prices and, greatest extent possible,
analytically calculate optimal bids.
attack price prediction problem, propose machine-learning approach: gather
examples previous auctions prices paid them, use machine-learning methods predict prices based available features auction. Moreover,
strategy, needed able model uncertainty associated predicted prices;
words, needed able sample predicted distribution prices
given current state game. viewed conditional density estimation
problem, is, supervised learning problem goal estimate entire
distribution real-valued label given description current conditions, typically
form feature vector. second main contribution article new algorithm
solving general problems based boosting (Freund & Schapire, 1997; Schapire &
Singer, 1999).
third contribution article complete description prototype implementation approach form
, top-scoring agent1 second Trading
Agent Competition (TAC-01) held Tampa Bay, FL October 14, 2001 (Wellman, Greenwald, Stone, & Wurman, 2003a). TAC domain main motivation
innovations reported here.
builds top
(Stone, Littman,
Singh, & Kearns, 2001), top-scoring agent TAC-00, introduces fundamentally
new approach creating autonomous bidding agents.
present details
instantiation underlying principles
believe applications wide variety bidding situations.
uses predictive, data-driven approach bidding based expected marginal values available
goods. article, present empirical results demonstrating robustness effectiveness
's adaptive strategy. also report
's performance
TAC-01 TAC-02 ect key issues raised competitions.
remainder article organized follows. Section 2, present
general approach bidding multiple interacting goods simultaneous auctions.
Section 3, summarize TAC, substrate domain work. Section 4 describes
boosting-based price predictor. Section 5, give details
. Section 6,
present empirical results including summary
's performance TAC01, controlled experiments isolating successful aspects
, controlled
experiments illustrating lessons learned competition. discussion
summary related work provided Sections 7 8.
ATTac-2001

ATTac-2001

ATTac-2000

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

2. General Approach

wide variety decision-theoretic settings, useful able evaluate hypothetical
situations. computer chess, example, static board evaluator used heuristically
1. Top-scoring one metric, second place another.

210

fiDecision-Theoretic Bidding Learned Density Models

measure player ahead much given board situation. scenario
similar auction domains, bidding agent
uses situation evaluator,
analogous static board evaluator, estimates agent's expected profit
hypothetical future situation. \profit predictor" wide variety uses agent.
example, determine value item, agent compares predicted profit
assuming item already owned predicted profit assuming item
available.
Given prices goods, one often compute set purchases allocation
maximizes profit.2 Similarly, closing prices known, treated fixed,
optimal bids computed (bid high anything want buy). So, one natural
profit predictor simply calculate profit optimal purchases fixed predicted
prices. (The predicted prices can, course, different different situations, e.g., previous
closing prices relevant predicting future closing prices.)
sophisticated approach profit prediction construct model probability distribution possible future prices place bids maximize expected
profit. approximate solution dicult optimization problem created
stochastically sampling possible prices computing profit prediction
sampled price. sampling-based scheme profit prediction important modeling
uncertainty value gaining information, i.e., reducing price uncertainty.
Section 2.1 formalizes latter approach within simplified sequential auction model.
abstraction illustrates decision-making issues full sampling-based
approach presented Section 2.2. full setting approach addresses considerably complex abstract model, simplifying assumptions allow us
focus core challenge full scenario. guiding principle make decisiontheoretically optimal decisions given profit predictions hypothetical future situations.3
ATTac-2001

2.1 Simplified Abstraction

simple model, n items auctioned sequence (first item 0,
item 1, etc.). bidder must place bid r item i, bid, closing
price chosen corresponding item distribution specific item.
bid matches exceeds closing price, r , bidder holds item i, h = 1. Otherwise,
bidder hold item, h = 0. bidder's utility v(H ) function
final vector holdings H = (h0 ; : : : ; h 1 ) cost function holdings
vector closing prices, H . formalize problem optimal bid selection
develop series approximations make problem solvable.












n

2. problem computationally dicult general, solved effectively non-trivial
TAC setting (Greenwald & Boyan, 2001; Stone et al., 2001).
3. alternative approach would abstractly calculate Bayes-Nash equilibrium (Harsanyi, 1968)
game play optimal strategy. dismissed approach intractability
realistically complex situations, including TAC. Furthermore, even able approximate
equilibrium strategy, reasonable assume opponents would play optimal strategies.
Thus, could gain additional advantage tuning approach opponents' actual behavior
observed earlier rounds, essentially strategy adopted.

211

fiStone, Schapire, Littman, Csirik, & McAllester
2.1.1 Exact Value

value auction, is, bidder's expected profit (utility minus cost)
bidding optimally rest auction? bidder knows value, make
next bid one maximizes expected profit. value function
bidder's current holdings H current item bid on, i. expressed
value(i; H ) = max E max
E +1 : : : max E 1 (v (G + H ) G );
(1)
1
+1
yi

ri

yi

ri

rn

yn

components G new holdings result additional winnings g
. Note H non-zero entries items already sold
(8j i; H = 0) G non-zero entries items yet sold
(8j < i; G = 0). Note also G fully specified g variables
(for j i) bound sequentially expectation maximization operators.
idea bids r r 1 chosen maximize value context
possible closing prices .
Equation 1 closely related equations defining value finite-horizon partially observable Markov decision process (Papadimitriou & Tsitsiklis, 1987) stochastic
satisfiability expression (Littman, Majercik, & Pitassi, 2001). Like problems,
sequential auction problem computationally intractable suciently general representations v() (specifically, linear functions holdings expressive enough
achieve intractability arbitrary nonlinear functions are).
j

rj

j

j

j

j



j

n

j

2.1.2 Approximate Value Reordering

three major sources intractability Equation 1|the alternation maximization expectation operators (allowing decisions conditioned exponential
number possible sets holdings), large number maximizations (forcing exponential number decisions considered), large number expectations
(resulting sums exponential number variable settings).
attack problem interleaved operators moving first maximizations inside expectations, resulting expression approximates value:
(2)
value-est(i; H ) = max E E +1 : : : E 1 max
: : : max(v (G + H ) G ):
1
+1
ri

yi

yi

yn

ri

rn

choices bids r +1 r 1 appear deeply nested bindings
closing prices 1, cease bids altogether, instead represent
decisions whether purchase goods given prices. Let G = opt(H; i; ) vector
representing optimal number goods purchase prices specified vector
given current holdings H starting auction i. Conceptually, computed
evaluating
opt(H; i; ) = argmax (v(G + H ) H ):
(3)
1
Thus, Equation 2 written:
0
0
0
value-est(i; H ) = max E
1 (v (opt(H ; + 1; ) + H ) opt(H ; + 1; ) ) (4)




n

n

gi ;:::;gn

ri

yi ;:::;yn

212

fiDecision-Theoretic Bidding Learned Density Models

H 0 identical H except i-th component ects whether item won|r .
Note approximation made computing expected
prices (as point values) solving optimization problem. approach corresponds
swapping expectations towards core equation:
value-est(i; H ) = max(v(opt(H 0 ; + 1; E ) + H 0) opt(H 0; + 1; E ) E ) (5)


ev



ri







E = E [y +1 ; : : : ; 1], vector expected costs goods. remainder
article, refer methods use approximation Equation 5
expected value approaches reasons come apparent shortly.
technique swapping maximization expectation operators previously used
Hauskrecht (1997) generate bound solving partially observable Markov decision
processes. decrease uncertainty decisions made makes approximation
upper bound true value auction: value-est value. tightness approximations Equations 2 5 depends true distributions expected prices.
example, prices known advance certainty, approximations
exact.




n

2.1.3 Approximate Bidding

Given vector costs , optimization problem opt(H; i; ) Equation 4 still NPhard (assuming representation utility function v() suciently complex).
many representations v(), optimization problem cast integer linear program approximated using fractional relaxation instead exact optimization
problem. precisely approach adopted ATTac (Stone et al., 2001).
2.1.4 Approximation via Sampling

Even assuming opt(H; i; ) solved unit time, literal interpretation Equation 4 says we'll need solve optimization problem exponential number cost
vectors (or even probability distributions Pr(y ) continuous). Kearns, Mansour, Ng (1999) showed values partially observable Markov decision processes
could estimated accurately sampling trajectories instead exactly computing sums.
Littman et al. (2001) stochastic satisfiability expressions. Applying
idea Equation 4 leads following algorithm.
1. Generate set vectors closing costs according product distribution
Pr(y ) Pr(y 1).
2. samples, calculate opt(H 0; +1; ) defined average
results, resulting approximation
X
value-est (i; H ) = max (v(opt(H 0; + 1; ) + H 0) opt(H 0 ; + 1; ) )=jS j: (6)
j



n



ri



2



expression converges value-est increasing sample size.
remaining challenge evaluating Equation 6 computing real-valued bid r
maximizes value. Note want buy item precisely closing prices


213

fiStone, Schapire, Littman, Csirik, & McAllester

value item (minus cost) exceeds value item;
maximizes profit. Thus, make positive profit, willing pay to,
than, difference value item item.
Formally, let H vector current holdings H holdings modified
ect winning item i. Let G (Y ) = opt(H ; i+1; ), optimal set purchases assuming
item won, G(Y ) = opt(H; i+1; ) optimal set purchases assuming otherwise
(except cases ambiguity, write simply G G G (Y ) G(Y ) respectively).
want select r achieve equivalence
X
X
r
(v(G + H ) G )=jS j (v(G + H ) G )=jS j: (7)
w

w

w

w

w









Setting

ri

w

2

w





=



X


2

([v(G + H )
w

2



] [v(G + H )

Gw

]) j j

(8)

G =S:



achieves equivalence desired Equation 7, verified substitution,
therefore bidding average difference holding holding item maximizes
value.4
2.2 Full Approach

Leveraging preceding analysis, define sampling-based approach profit
prediction general simultaneous, multi-unit auctions interacting goods. scenario, let n simultaneous, multi-unit auctions interacting goods a0 ; : : : ; 1 .
auctions might close different times times not, general, known
advance bidders. auction closes, let us assume units available
distributed irrevocably highest bidders, need pay price bid
mth highest bidder. scenario corresponds mth price ascending auction.5
Note bidder may place multiple bids auction, thereby potentially
win multiple units. assume auction closes, bidders longer
opportunity acquire additional copies goods sold auction (i.e.,
aftermarket).
approach based upon five assumptions. G = (g0 ; : : : ; g 1 ) 2 , let v(G) 2
represent value derived agent owns g units commodity sold
auction . Note v independent costs commodities. Note
representation allows interacting goods kinds, including complementarity
substitutability.6 assumptions approach follows:
1. Closing prices somewhat, somewhat, predictable. is, given set
input features X , auction , exists sampling rule outputs
n

n



n

IR







4. Note strategy choosing ri Equation 8 exploit fact sample contains
finite set possibilities yi , might make robust inaccuracies sampling.
5. large enough practically ecient + 1st auction. use mth
price model used TAC's hotel auctions.
6. Goods considered complementary value package greater sum individual
values; goods considered substitutable value package less sum
individual values.

214

fiDecision-Theoretic Bidding Learned Density Models

closing price according probability distribution predicted closing prices
a.
2. Given vector holdings H = (h0 ; : : : ; h 1 ) h 2 represents quantity
commodity sold auction already owned agent,
given vector fixed closing prices = (y0; : : : ; 1), exists tractable
procedure opt(H; ) determine optimal set purchases (g0 ; : : : ; g 1 )
g 2 represents number goods purchased auction
v (opt(H; ) + H ) opt(H; ) v (G + H ) G
G 2 . procedure corresponds optimization problem opt(H; i; )
Equation 3.
3. individual agent's bids appreciable effect economy (large
population assumption).
4. agent free change existing bids auctions yet closed.
5. Future decisions made presence complete price information. assumption corresponds operator reordering approximation previous
section.
assumptions true general, reasonable enough approximations basis effective strategy.
Assumption 3, price predictor generate predicted prices prior considering
one's bids. Thus, sample distributions produce complete sets closing
prices goods.
good consideration, assume next one close.
different auction closes first, revise bids later (Assumption 4). Thus,
would like bid exactly good's expected marginal utility us. is, bid
difference expected utilities attainable without good. compute
expectations, simply average utilities good
different price samples Equation 8. strategy rests Assumption 5
assume bidding good's current expected marginal utility cannot adversely affect
future actions, instance impacting future space possible bids. Note
time proceeds, price distributions change response observed price trajectories,
thus causing agent continually revise bids.
Table 1 shows pseudo-code entire algorithm. fully detailed description
instantiation approach given Section 5.




n







n

n





n



2.3 Example

Consider camera ash interacting values agent shown Table 2.
Further, consider agent estimates camera sell $40 probability
25%, $70 probability 50%, $95 probability 25%. Consider question
agent bid ash (in auction a0). decision pertaining
camera would made via similar analysis.
215

fiStone, Schapire, Littman, Csirik, & McAllester



Let H = (h0 ; : : : ; hn 1 ) agent's current holdings n auctions.
= 0 n

1 (assume auction next close):

{ total-diff = 0
{ counter = 0
{ time permits:



auction aj ; j 6= i, generate predicted price sample yj . Let =
(y0 ; : : : ; yi 1 ; 1; yi+1 ; : : : ; yn 1 ).
Let Hw = (h0 ; : : : ; hi 1 ; hi + 1; hi+1 ; : : : ; hn 1 ), vector holdings agent
wins unit auction ai .
Compute Gw = opt(Hw ; ), optimal set purchases agent wins unit
auction ai . Note additional units good purchased, since
i-th component 1.
Compute G = opt(H; ), optimal set purchases agent never acquires
additional units auction ai prices set .
diff = [v(Gw + H ) Gw ] [v(G + H ) G ]
total-diff = total-diff + diff
counter = counter + 1
{ r = total-diff=counter
{ Bid r auction ai .

Table 1: decision-theoretic algorithm bidding simultaneous, multi-unit, interacting auctions.
utility

$50
10
100
0
Table 2: table values combination camera ash example.
camera alone
ash alone

neither

First, agent samples distribution possible camera prices. price
camera (sold auction a1) $70 sample:
H = (0; 0); H = (1; 0); = (1; 70)
G = opt(H ; ) best set purchases agent make ash,
assuming camera costs $70. case, two options buying
camera not. Buying camera yields profit 100 70 = 30. buying
camera yields profit 10 0 = 10. Thus, G = (0; 1), [v(G + H ) G ] =
v (1; 1) (0; 1) (1; 70) = 100 70.
Similarly G = (0; 0) (since ash owned, buying camera yields profit
50 70 = 20, buying yields profit 0 0 = 0) [v(G + H ) G ] = 0.
w

w

w

w

216

w

w

fiDecision-Theoretic Bidding Learned Density Models

val = 30 0 = 30.

Similarly, camera predicted cost $40, val = 60 10 = 50; camera
predicted cost $95, val = 10 0 = 10. Thus, expect 50% camera price
samples suggest ash value $30, 25% lead value $50
25% lead value $10. Thus, agent bid :5 30 + :25 50 + :25 10 = $30
ash.
Notice analysis bid ash, actual closing price
ash irrelevant. proper bid depends predicted price camera.
determine proper bid camera, similar analysis would done using
predicted price distribution ash.
3. TAC

instantiated approach entry second Trading Agent Competition (TAC),
described section. Building success TAC-00 held July 2000 (Wellman,
Wurman, O'Malley, Bangera, Lin, Reeves, & Walsh, 2001), TAC-01 included 19 agents
9 countries (Wellman et al., 2003a). key feature TAC required autonomous
bidding agents buy sell multiple interacting goods auctions different types.
designed benchmark problem complex rapidly advancing domain emarketplaces, motivating researchers apply unique approaches common task.
providing clear-cut objective function, TAC also allows competitors focus
attention computational game-theoretic aspects problem leave aside
modeling model validation issues invariably loom large real applications
automated agents auctions (see Rothkopf & Harstad, 1994). Another feature TAC
provides academic forum open comparison agent bidding strategies
complex scenario, opposed complex scenarios, trading real stock
markets, practitioners (understandably) reluctant share technologies.
TAC game instance pits eight autonomous bidding agents one another.
TAC agent simulated travel agent eight clients, would like travel
TACtown Tampa back 5-day period. client characterized
random set preferences possible arrival departure dates, hotel rooms,
entertainment tickets. satisfy client, agent must construct travel package
client purchasing airline tickets TACtown securing hotel reservations;
possible obtain additional bonuses providing entertainment tickets well. TAC
agent's score game instance difference sum clients' utilities
packages receive agent's total expenditure. provide selected details
game next; full details design mechanisms TAC server
TAC game, see http://www.sics.se/tac.
TAC agents buy ights, hotel rooms entertainment tickets auctions run
TAC server University Michigan. game instance lasts 12 minutes
includes total 28 auctions 3 different types.
Flights (8 auctions): separate auction type airline ticket: Tampa
(in ights) days 1{4 Tampa (out ights) days 2{5. unlimited
supply airline tickets, every 24{32 seconds ask price changes $10
217

fiStone, Schapire, Littman, Csirik, & McAllester

$x. x increases linearly course game 10 y, 2 [10; 90]
chosen uniformly random auction, unknown bidders.
cases, tickets priced $150 $800. server receives bid
ask price, transaction cleared immediately ask price
resale allowed.
Hotel Rooms (8): two different types hotel rooms|the Tampa Towers (TT)
Shoreline Shanties (SS)|each 16 rooms available days 1{4.
rooms sold 16th-price ascending (English) auction, meaning
8 types hotel rooms, 16 highest bidders get rooms 16th highest
price. example, 15 bids TT day 2 $300, 2 bids $150,
number lower bids, rooms sold $150 15 high bidders plus
one $150 bidders (earliest received bid). ask price current 16thhighest bid transactions clear auction closes. Thus, agents
knowledge of, example, current highest bid. New bids must higher
current ask price. bid withdrawal resale allowed, though price bids
may lowered provided agent reduce number rooms would win
auction close. One randomly chosen hotel auction closes minutes 4{11
12-minute game. Ask prices changed minute.
Entertainment Tickets (12): Alligator wrestling, amusement park, museum tickets
sold days 1{4 continuous double auctions. Here, agents buy
sell tickets, transactions clearing immediately one agent places buy bid
price least high another agent's sell price. Unlike auction
types goods sold centralized stock, agent starts
(skewed) random endowment entertainment tickets. prices sent agents
bid-ask spreads, i.e., highest current bid price lowest current ask price
(due immediate clears, ask price always greater bid price). case, bid
withdrawal ticket resale permitted. agent gets blocks 4 tickets
2 types, 2 tickets another 2 types, tickets 8 types.
addition unpredictable market prices, sources variability game instance game instance client profiles assigned agents random initial
allotment entertainment tickets. TAC agent eight clients randomly assigned travel preferences. Clients parameters ideal arrival day, IAD (1{4); ideal
departure day, IDD (2{5); hotel premium, HP ($50{$150); entertainment values, EV
($0{$200) type entertainment ticket.
utility obtained client determined travel package given
combination preferences. obtain non-zero utility, client must assigned
feasible travel package consisting ight arrival day AD, ight
departure day DD, hotel rooms type (TT SS) days
(days AD < DD). one entertainment ticket type
assigned, one day. Given feasible package, client's utility
defined
1000 travelPenalty + hotelBonus + funBonus

218

fiDecision-Theoretic Bidding Learned Density Models

= 100(jAD IAD j + jDD IDD j)
hotelBonus = HP client TT, 0 otherwise.
funBonus = sum EVs assigned entertainment tickets.
TAC agent's score sum clients' utilities optimal allocation
goods (computed TAC server) minus expenditures. client preferences,
allocations, resulting utilities sample game shown Tables 3 4.
Client IAD IDD HP AW AP MU
1 Day 2 Day 5 73 175 34 24
2 Day 1 Day 3 125 113 124 57
3 Day 4 Day 5 73 157 12 177
4 Day 1 Day 2 102 50 67 49
5 Day 1 Day 3 75 12 135 110
6 Day 2 Day 4 86 197 8 59
7 Day 1 Day 5 90 56 197 162
8 Day 1 Day 3 50 79 92 136
Table 3:
's client preferences actual game. AW, AP, MU EVs
alligator wrestling, amusement park, museum respectively.




travelPenalty

ATTac-2001

Client AD DD Hotel
Ent'ment
Utility
1 Day 2 Day 5 SS
AW4
1175
2 Day 1 Day 2 TT
AW1
1138
3 Day 3 Day 5 SS
MU3, AW4
1234
4 Day 1 Day 2 TT
None
1102
5 Day 1 Day 2 TT
AP1
1110
6 Day 2 Day 3 TT
AW2
1183
7 Day 1 Day 5 SS AF2, AW3, MU4 1415
8 Day 1 Day 2 TT
MU1
1086
Table 4:
's client allocations utilities actual game
Table 3. Client 1's \4" \Ent'ment" indicates day 4.
ATTac-2001

rules TAC-01 largely identical TAC-00, three important
exceptions:
1. TAC-00, ight prices tend increase;
2. TAC-00, hotel auctions usually closed end game;
3. TAC-00, entertainment tickets distributed uniformly agents
relatively minor surface, changes significantly enriched strategic
complexity game. Stone Greenwald (2003) detail agent strategies TAC-00.
219

fiStone, Schapire, Littman, Csirik, & McAllester

TAC-01 organized series four competition phases, culminating
semifinals finals October 14, 2001 EC-01 conference Tampa, Florida. First,
qualifying round, consisting 270 games per agent, served select 16
agents would participate semifinals. Second, seeding round, consisting
315 games per agent, used divide agents two groups eight.
semifinals, morning 14th consisting 11 games group, four teams
group selected compete finals afternoon.
finals summarized Section 6.
TAC designed fully realistic sense agent TAC
immediately deployable real world. one thing, unrealistic assume
agent would complete, reliable access clients' utility functions (or even
client would!); typically, sort preference elicitation procedure would required (e.g.
Boutilier, 2002). another, auction mechanisms somewhat contrived
purposes creating interesting, yet relatively simple game. However, mechanism
representative class auctions used real world. dicult
imagine future agents need bid decentralized, related, yet varying
auctions similarly complex packages goods.
4. Hotel Price Prediction

discussed earlier, central part strategy depends ability predict prices,
particularly hotel prices, various points game. accurately possible,
used machine-learning techniques would examine hotel prices actually paid
previous games predict prices future games. section discusses part
strategy detail, including new boosting-based algorithm conditional density
estimation.
bound considerable uncertainty regarding hotel prices since depend
many unknown factors, time hotel room close,
agents are, kind clients assigned agent, etc. Thus, exactly
predicting price hotel room hopeless. Instead, regard closing price
random variable need estimate, conditional current state knowledge
(i.e., number minutes remaining game, ask price hotel, ight prices, etc.).
might attempt predict variable's conditional expected value. However,
strategy requires predict expected value, also able
estimate entire conditional distribution sample hotel prices.
set learning problem, gathered set training examples
previously played games. defined set features describing example
together meant comprise snap-shot relevant information available
time prediction made. features used real valued; couple
features special value ? indicating \value unknown." used following
basic features:
number minutes remaining game.
price hotel room, i.e., current ask price rooms closed
actual selling price rooms closed.
220

fiDecision-Theoretic Bidding Learned Density Models

closing time hotel room. Note feature defined even rooms

yet closed, explained below.
prices ights.
basic list, added number redundant variations, thought might help
learning algorithm:
closing price hotel rooms closed (or ? room yet closed).
current ask price hotel rooms closed (or ? room already
closed).
closing time hotel room minus closing time room whose price
trying predict.
number minutes current time hotel room closes.
seeding rounds, impossible know play opponents
were, although information available end game, therefore
training. semifinals finals, know identities competitors.
Therefore, preparation semifinals finals, added following features:
number players playing (ordinarily eight, sometimes fewer, instance
one players crashed).
bit player indicating whether player participated game.
trained specialized predictors predicting price type hotel room.
words, one predictor specialized predicting price TT day
1, another predicting SS day 2, etc. would seem require eight separate
predictors. However, tournament game naturally symmetric middle
sense create equivalent game exchanging hotel rooms days 1
2 days 4 3 (respectively), exchanging inbound ights
days 1, 2, 3 4 outbound ights days 5, 4, 3 2 (respectively). Thus,
appropriate transformations, outer days (1 4) treated equivalently,
likewise inner days (2 3), reducing number specialized predictors half.
also created specialized predictors predicting first minute ight prices
quoted prior receiving hotel price information. Thus, total eight
specialized predictors built (for combination TT versus SS, inner versus outer
day, first minute versus first minute).
trained predictors predict actual closing price room per se,
rather much price would increase, i.e., difference closing price
current price. thought might easier quantity predict, and,
predictor never outputs negative number trained nonnegative data,
approach also ensures never predict closing price current bid.
previously played games, able extract many examples.
Specifically, minute game room yet closed,
221

fiStone, Schapire, Littman, Csirik, & McAllester

extracted values features described moment game, plus
actual closing price room (which trying predict).
Note training, problem extracting closing times
rooms. actual play game, know closing times rooms
yet closed. However, know exact probability distribution closing
times rooms yet closed. Therefore, sample vector hotel
prices, first sample according distribution closing times, use
predictor sample hotel prices using sampled closing times.
4.1 Learning Algorithm

described set learning problem, ready describe
learning algorithm used. Brie y, solved learning problem first reducing
multiclass, multi-label classification problem (or alternatively multiple logistic regression
problem), applying boosting techniques developed Schapire Singer (1999,
2000) combined modification boosting algorithms logistic regression proposed
Collins, Schapire Singer (2002). result new machine-learning algorithm
solving conditional density estimation problems, described detail remainder
section. Table 5 shows pseudo-code entire algorithm.
Abstractly, given pairs (x1 ; y1 ); : : : ; (x ; ) x belongs space X
R . case, x 's auction-specific feature vectors described
above; n, X (R [f?g) . target quantity difference closing
price current price. Given new x, goal estimate conditional distribution
given x.
proceed working assumption training test examples (x; y)
i.i.d. (i.e, drawn independently identical distributions). Although assumption
false case (since agents, including ours, changing time), seems like
reasonable approximation greatly reduces diculty learning task.
first step reduce estimation problem classification problem breaking
range 's bins:
[b0; b1 ); [b1 ; b2 ); : : : ; [b ; b +1]
breakpoints b0 < b1 < < b b +1 problem, chose k = 50.7
endpoints b0 b +1 chosen smallest largest values observed
training. choose remaining breakpoints b1; : : : ; b roughly equal
number training labels fall bin. (More technically, breakpoints chosen
entropy distribution bin frequencies maximized.)
breakpoints b (j = 1; : : : ; k), learning algorithm attempts estimate
probability new (given x) least b . Given estimates p
b , estimate probability bin [b ; b +1 ) p +1 p (and
use constant density within bin). thus reduced problem
one estimating multiple conditional Bernoulli variables corresponding event










n





k

k

k

k



k

k



j

j

j

j

j

j

j

j

7. experiment varying k, expect algorithm sensitive suciently
large values k.

222

fiDecision-Theoretic Bidding Learned Density Models
; ym ) xi 2 X , yi 2 R
positive integers k

Input: (x1 ; y1 ); : : : ; (x

b0 < b1 < < bk+1
= mini yi
bk+1 = maxi yi
Pk
b1 ; : : : ; bk chosen minimize
q ln qj q0 ; : : : ; qk fraction yi 's
j =0 j
[b0 ; b1 ); [b1 ; b2 ); : : : ; [bk ; bk+1 ] (using dynamic programing)

Compute breakpoints:





b0

Boosting:



= 1; : : : :
1
compute weights Wt (i; j ) =
1 + esj (yi )ft (xi ;j )
sj (y ) Eq. (10)
use Wt obtain base function ht : X f1; : : : ; k g ! R minimizing

k
X
X
sj (yi )ht (xi ;j )
Wt (i; j )e
decision rules ht considered. decision rules
i=1 j =1

take form. work, use \decision stumps," simple thresholds one
features.

Output sampling rule:




let f =

X


ht
t=1

let f 0 = (f + f )=2
( ) = maxff (x; j 0 ) : j j 0 k g
f (x; j )
= minff (x; j 0 ) : 1 j 0 j g
f x; j



sample, given x 2 X
1
let pj =
1 + e f (x;j )
let p0 = 1; pk+1 = 0
choose j 2 f0; : : : ; k g randomly probability pj
choose uniformly random [bj ; bj +1 ]
output
0

pj +1

Table 5: boosting-based algorithm conditional density estimation.


b , this, use logistic regression algorithm based boosting techniques

described Collins et al. (2002).
learning algorithm constructs real-valued function f : X f1; : : : ; kg ! R
interpretation
1
(9)
1 + exp( f (x; j ))
j

estimate probability b , given x. negative log likelihood
conditional Bernoulli variable corresponding b
j





ln 1 + e

( ) (

sj yi f xi ;j

223

j

)



fiStone, Schapire, Littman, Csirik, & McAllester



(

( ) = +11 ifif yy < bb .
(10)
attempt minimize quantity training examples (x ; ) breakpoints b . Specifically, try find function f minimizing
sj

j
j





j

XX




k

=1 j =1



ln 1 + e

( ) (

sj yi f xi ;j



) :

use boosting-like algorithm described Collins et al. (2002) minimizing objective
functions exactly form. Specifically, build function f rounds.
round t, add new base function h : X f1; : : : ; kg ! R . Let


ft

=

X1


0 =1

ht0



accumulating sum. Following Collins, Schapire Singer, construct h ,
first let
1
W (i; j ) =
(
1+e ) ( )
set weights example-breakpoint pairs. choose h minimize




sj yi ft xi ;j



XX


( )

k

=1 j =1

Wt i; j e

( ) (

sj yi ht xi ;j

(11)

)



space \simple" base functions h . work, considered \decision
stumps" h form
8
>
< (x)
h(x; j ) =
B (x) <
>
: C (x) =?
() one features described above, , , B C real numbers.
words, h simply compares one feature threshold returns
vector numbers h(x; ) depends whether (x) unknown (?),
. Schapire Singer (2000) show eciently search best
h possible choices , , , B C . (We also employed technique
\smoothing" , B C .)
computed sort iterative procedure, Collins et al. (2002) prove
asymptotic convergence f minimum objective function Equation (11)
linear combinations base functions. problem, fixed number
rounds = 300. Let f = f +1 final predictor.
noted above, given new feature vector x, compute p Equation (9)
estimate probability b , let p0 = 1 p +1 = 0.
make sense, need p1 p2 p , equivalently, f (x; 1) f (x; 2) f (x; k),
condition may hold learned function f . force condition, replace
f reasonable (albeit heuristic) approximation f 0 nonincreasing j , namely,


j

j

j

j

j

j

j

j

j

j

j

j





j

j

k

224

k

fiDecision-Theoretic Bidding Learned Density Models

= (f + f )=2 f (respectively, f ) pointwise minimum (respectively, maximum)
nonincreasing functions g everywhere upper bound f (respectively, lower bound
f ).
modified function f 0, compute modified probabilities p . sample
single point according estimated distribution R associated f 0, choose
bin [b ; b +1) probability p p +1, select point bin uniformly
random. Expected value according distribution easily computed


X
b +1 + b
:
(p p +1)
2
=0
Although present results using algorithm trading agent context,
test performance general learning problems, compare
methods conditional density estimation, studied Stone (1994).
clearly area future research.
f0

j

j

j

j

j

k

j

j

j

j

j

5. ATTac-2001

described hotel price prediction detail, present remaining details
's algorithm. begin brief description goods allocator,
used subroutine throughout algorithm. present algorithm
top-down fashion.
ATTac-2001

5.1 Starting Point

core subproblem TAC agents allocation problem: finding profitable
allocation goods clients, G, given set owned goods prices goods.
allocation problem corresponds finding opt(H; i; ) Equation 3. denote
value G (i.e., score one would attain G) v(G ). general allocation
problem NP-complete, equivalent set-packing problem (Garey & Johnson,
1979). However solved tractably TAC via integer linear programming (Stone
et al., 2001).
solution integer linear program value-maximizing allocation owned
resources clients along list resources need purchased. Using linear
programming package \LPsolve",
usually able find globally optimal
solution 0.01 seconds 650 MHz Pentium II. However, since integer linear
programming NP-complete problem, inputs lead great deal search
integrality constraints, therefore significantly longer solution times.
v (G ) needed (as opposed G itself), upper bound produced LPsolve prior
search integrality constraints, known LP relaxation, used
estimate. LP relaxation always generated quickly.
Note means possible formulation allocation
problem. Greenwald Boyan (2001) studied fast, heuristic search variant found
performed extremely well collection large, random allocation problems. Stone
et al. (2001) used randomized greedy strategy fallback cases linear
program took long solve.
ATTac-2001

225

fiStone, Schapire, Littman, Csirik, & McAllester
5.2 Overview

Table 6 shows high-level overview
remainder section.

. italicized portions described

ATTac-2001

first ight quotes posted:
Compute G current holdings expected prices
Buy ights G expected cost postponing commitment exceeds expected
benefit postponing commitment

Starting 1 minute hotel close:
Compute G current holdings expected prices
Buy ights G expected cost postponing commitment exceeds expected



benefit postponing commitment (30 seconds)
Bid hotel room expected marginal values given holdings, new ights, expected hotel
purchases (30 seconds)

Last minute: Buy remaining ights needed G
parallel (continuously): Buy/sell entertainment tickets based expected values

Table 6:

's high-level algorithm. italicized portions described
remainder section.
ATTac-2001

5.3 Cost Additional Rooms

hotel price predictor described Section 4 assumes
's bids affect
ultimate closing price (Assumption 3 Section 2). assumption holds large
economy. However TAC, hotel auction involved 8 agents competing 16 hotel
rooms. Therefore, actions agent appreciable effect clearing price:
hotel rooms agent attempted purchase, higher clearing price would
be, things equal. effect needed taken account solving
basic allocation problem.
simplified model used
assumed nth highest bid hotel
auction roughly proportional c (over appropriate range n) c 1.
Thus, predictor gave price p,
used purchasing two hotel
rooms (the \fair" share single agent 16 rooms), adjusted prices
quantities rooms using c.
example,
would consider cost obtaining 4 rooms 4pc2 . One
two rooms cost p, 3 cost pc, 4 cost pc2 , 5 cost pc3, etc. total, 2
rooms cost 2p, 4 cost 4pc2 . reasoning behind procedure
buys two rooms | fair share given 16 rooms 8 agents, 16th
highest bid (
's 2 bids addition 14 others) sets price.
bids additional unit, previous 15th highest bid becomes price-setting bid:
price rooms sold goes p pc.
constant c calculated data several hundred games seeding
round. hotel auction, ratio 14th 18th highest bids (re ecting
ATTac-2001

ATTac-2001
n

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

226

fiDecision-Theoretic Bidding Learned Density Models

relevant range n) taken estimate c4, (geometric) mean
resulting estimates taken obtain c = 1:35.
LP allocator takes price estimates account computing G assigning higher costs larger purchase volumes, thus tending spread
's
demand different hotel auctions.

, heuristics applied procedure improve stability
avoid pathological behavior: prices $1 replaced $1 estimating c;
c = 1 used purchasing fewer two hotel rooms; hotel rooms divided
early closing late closing (and cheap expensive) ones, c values
corresponding subsets auctions seeding rounds used case.
ATTac-2001

ATTac-2001

5.4 Hotel Expected Marginal Values

Using hotel price prediction module described Section 4, coupled model
effect economy,
equipped determine bids hotel rooms.
Every minute, hotel auction still open,
assumes auction
close next computes marginal value hotel room given predicted
closing prices rooms. auction close next, assumes
chance revise bids. Since predicted prices represented
distributions possible future prices,
samples distributions
averages marginal values obtain expected marginal value. Using full minute
closing times computation (or 30 seconds still ights consider
too),
divides available time among different open hotel auctions
generates many price samples possible hotel room. end,
bids expected marginal values rooms.
algorithm described precisely explanation Table 7.
One additional complication regarding hotel auctions that, contrary one
assumptions Section 2.2 (Assumption 4), bids fully retractable:
changed $1 current ask price. case current active
bids goods
longer wants less $1 current ask
price, may advantageous refrain changing bid hopes ask
price surpass them: is, current bid may higher expected value
best possible new bid. address issue,
samples learned price
distribution find average expected values current potential bids,
enters new bid case potential bid better.
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

5.5 Expected Cost/Benefit Postponing Commitment

makes ight bidding decisions based cost-benefit analysis: particular,
computes incremental cost postponing bidding particular ight versus
value delaying commitment. section, describe determination
cost postponing bid.
Due diculties compounded sophisticated approaches,
used following simple model estimating price ight ticket given
future time. evident formulation that|given y|the expected price increase
time 0 time nearly form t2 . also clear
ATTac-2001

ATTac-2001

ATTac-2001

227

fiStone, Schapire, Littman, Csirik, & McAllester



hotel (in order increasing expected price):




value ith copy room mean Vi



Repeat time bound
1. Generate random hotel closing order (only open hotels)
2. Sample closing prices predicted hotel price distributions
3. Given closing prices, compute V0 ; V1 ; : : : Vn

Vi v (G ) owning hotel
Estimate v (G ) LP relaxation
Assume additional hotel rooms type bought
hotels, assume outstanding bids sampled price already owned
(i.e., cannot withdrawn).
Note V0 V1 : : : Vn : values monotonically increasing since
goods cannot worse terms possible allocations.
Vi

1

samples.

Note V1 V0 V2 V1 : : : Vn Vn 1 : value differences monotonically
decreasing since additional room assigned client derive
value it.
Bid one room value ith copy room value
least much current price. Due monotonicity noted step above,
matter closing price, desired number rooms price purchased.

Table 7: algorithm generating bids hotel rooms.
long price hit artificial boundaries $150 $800, constant
must depend linearly 10. linear dependence coecient estimated
several hundred ight price evolutions qualifying round. Thus, constant
m, expected price increase time time m(T 2 t2 )(y 10).
price prediction needed, formula first used first recent actual
price observations obtain guess y, used formula
estimate future price. change predicted formula yielded price decrease.
approach suffers systemic biases various kinds (mainly due fact
variance price changes gets relatively smaller longer periods time),
thought accurate enough use, predict whether ticket
expected get significantly expensive next minutes.
practice, TAC-01,
started ight-lookahead parameter set
3 (i.e., cost postponing average predicted ight costs 1, 2, 3 minutes
future). However, parameter changed 2 end finals order
cause
delay ight commitments further.
ATTac-2001

ATTac-2001

5.5.1 Expected Benefit Postponing Commitment

Fundamentally, benefit postponing commitments ights additional information eventual hotel prices becomes known. Thus, benefit postponing
commitment computed sampling possible future price vectors determining,
average, much better agent could bought different ight instead
one question. optimal buy ight future scenarios,
value delaying commitment ight purchased immediately. However,
228

fiDecision-Theoretic Bidding Learned Density Models

many scenarios ight best one get, purchase
likely delayed.
algorithm determining benefit postponing commitment similar
determining marginal value hotel rooms. detailed, explanation,
Table 8.
Assume we're considering buying n ights given type
Repeat time bound

1. Generate random hotel closing order (open hotels)
2. Sample closing prices predicted price distributions (open hotels)
3. Given closing prices compute V0; V1 ; : : : V
V = v (G ) forced buy ight
Estimate v(G) LP relaxation
Assume ights bought current price
Note V0 V1 : : : V since never worse retain extra exibility.
value waiting buy copy mean V V 1 samples.
price samples lead conclusion ith ight bought,
V = V 1 benefit postponing commitment.
Table 8: algorithm generating value postponing ight commitments.
n



n









5.6 Entertainment Expected Values

core
's entertainment-ticket-bidding strategy calculation
expected marginal values ticket. ticket,
computes expected
value one one fewer ticket. calculations give bounds
bid ask prices willing post. actual bid ask prices linear function
time remaining game:
settles smaller smaller profit ticket
transactions game goes on. Details functions bid ask price function
game time ticket value remained unchanged
(Stone et al., 2001).
Details entertainment-ticket expected marginal utility calculations given
Table 9.
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2000

6. Results

section presents empirical results demonstrating effectiveness
strategy. First, summarize performance 2001 2002 Trading Agent Competitions (TACs). summaries provide evidence strategy's overall effectiveness,
but, due small number games competitions, anecdotal rather scientifically conclusive. present controlled experiments provide conclusive
evidence utility decision theoretic learning approaches embedded within
.

ATTac-2001

ATTac-2001

229

fiStone, Schapire, Littman, Csirik, & McAllester

Assume n given ticket type currently owned
Repeat time bound

1. Generate random hotel closing order (open hotels)
2. Sample closing prices predicted price distributions (open hotels)
3. Given closing prices compute V 1; V ; V +1
V = v (G ) ticket
Estimate v(G) LP relaxation
Assume tickets bought sold
Note V 1 V V +1 since never worse extra tickets.
value buying ticket mean V +1 V samples; value
selling mean V V 1.
Since tickets considered sequentially, determined buy sell bid leads
price would clear according current quotes, assume transaction goes
computing values buying selling ticket types.
Table 9: algorithm generating value entertainment tickets.
n

n

n



n

n

n

n

n

n

n

6.1 TAC-01 Competition

19 teams entered qualifying round,
one eight agents
make finals afternoon October 14th, 2001. finals consisted
24 games among eight agents. Right beginning, became clear
(Fritschi & Dorer, 2002) team beat finals. jumped
early lead first two games, eight games round,
135 points per game ahead closest competitor (
& Jennings,
2002). 16 games round, 250 points ahead two closest
competitors (

).
point,
, continually retraining price predictors based
recent games, began making comeback. time last game played,
average 22 points per game behind
. thus needed beat
514 points final game overtake it, well within margins observed
individual game instances. game completed,
's score 3979 one
first scores posted server. agents' scores reported one
one,
score left. agonizing seconds (at least us),
TAC server posted final game score 4626, resulting win
.
competition, TAC team University Michigan conducted regression analysis effects client profiles agent scores. Using data seeding
rounds, determined agents better clients had:
1. fewer total preferred travel days;
2. higher total entertainment values;
3. higher ratio outer days (1 4) inner (2 3) preferred trip intervals.
ATTac-2001

livingagents

SouthamptonTAC,

ATTac-2001

whitebear

ATTac-2001

livingagents

livingagents

ATTac-2001

livingagents

livingagents

230

fiDecision-Theoretic Bidding Learned Density Models

Based significant measures, games finals could handicapped based
agents' aggregate client profiles. indicated
' clients
much easier satisfy
, giving
highest handicapped
score. final scores, well handicapped scores, shown Table 10. Complete
results aliations available http://tac.eecs.umich.edu.
Agent
Mean Handicapped score
3622
4154
3670
4094
3513
3931
3421
3909
3352
3812
3074
3766
3253
3679
2859
3338
Table 10: Scores finals. agent played 24 games. Southampton's score
adversely affected game agent crashed buying many
ights hotels, leading loss 3000 points. Discarding game
results average score 3531.
livingagents

ATTac-2001

ATTac-2001

ATTac-2001
livingagents
whitebear
Urlaub01
Retsina

CaiserSose

SouthamptonTAC
TacsMan

6.2 TAC-02 Competition

year TAC-01 competition,
re-entered TAC-02 competition
using models trained end TAC-01. Specifically, price predictors left
unchanged throughout (no learning). seeding round included 19 agents, playing
440 games course 2 weeks.
top-scoring agent
round, shown Table 11. Scores seeding round weighted emphasize
later results earlier results: scores day n seeding round given weight
n. practice designed encourage experimentation early round.
ocial ranking competitions based mean score ignoring agent's
worst 10 results allow occasional program crashes network problems.
one hand, striking
able finish strongly field
agents presumably improved course year. hand,
agents tuned, better worse,
consistent
throughout. particular, told
experimented approach
later days round, perhaps causing fall lead (by weighted
score) end. 14-game semifinal heat,
, restored
learning capability retrained data 2002 seeding round, finished
6th 8 thereby failing reach finals.
number possible reasons sudden failure. One relatively mundane explanation agent change computational environments
seeding rounds finals, may bug computational resource
constraint introduced. Another possibility due small number games
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

SouthamptonTAC

ATTac-2001

231

fiStone, Schapire, Littman, Csirik, & McAllester

Agent

Mean Weighted, dropped worst 10
3050
3131
3100
3129
2980
3118
3018
3091
2998
3055
2952
3000
2945
2966
2738
2855
Table 11: Top 8 scores seeding round TAC-02. agent played 440 games,
worst 10 games ignored computing rankings.
ATTac-2001

SouthamptonTAC
UMBCTAC

livingagents
cuhk

Thalis

whitebear
RoxyBot

semifinals,
simply got unlucky respect clients interaction
opponent strategies. However, also plausible training data 2002
qualifying seeding round data less representative 2002 finals
training data 2001; and/or competing agents improved significantly
seeding round
remained unchanged. TAC team University
Michigan done study price predictors several 2002 TAC agents suggests
bug hypothesis plausible:
predictor 2001 outperforms
predictors 2002 data 2002 semifinals finals; one
agent uses 2002 data produce good predictions based data (Wellman,
Reeves, Lochner, & Vorobeychik, 2003b).8
ATTac-2001

ATTac-2001

ATTac-2001

6.3 Controlled Experiments

's success TAC-01 competition demonstrates effectiveness complete
system. However, since competing agents differed along several dimensions, competition results cannot isolate successful approaches. section, report controlled
experiments designed test ecacy
's machine-learning approach price
prediction.
ATTac-2001

ATTac-2001

6.3.1 Varying Predictor

first set experiments, attempted determine quality
's
hotel price predictions affects performance. end, devised seven price prediction
schemes, varying considerably sophistication inspired approaches taken
TAC competitors, incorporated schemes agent. played
seven agents one another repeatedly, regular retraining described below.
Following seven hotel prediction schemes used, decreasing order
sophistication:
ATTac-2001

8. Indeed, TAC-03 competition, ATTac-2001 entered using trained models 2001,
competition, suggesting failure 2002 due problem learned
models used finals 2002.

232

fiDecision-Theoretic Bidding Learned Density Models

: \full-strength" agent based boosting used
tournament. (The denotes sampling.)

: agent samples prices empirical distribution prices
previously played games, conditioned closing time hotel room (a
subset features used
). words, collects historical
hotel prices breaks time hotel closed (as well
room type, usual). price predictor simply samples collection
prices corresponding given closing time.

: agent samples prices empirical distribution prices
previously played games, without regard closing time hotel room (but
still broken room type). uses subset features used
.

,
,
: agents predict way
corresponding predictors above, instead returning random sample
estimated distribution hotel prices, deterministically return expected
value distribution. (The ev denotes expected value, introduced Section 2.1.)

: agent uses simple predictor always predicts hotel
room close current price.
every case, whenever price predictor returns price current price,
replace current price (since prices cannot go down).
experiments, added eighth agent
, inspired
agent.
used
predict closing prices, determined optimal set
purchases, placed bids goods suciently high prices ensure
would purchased ($1001 hotel rooms,
TAC-01) right
first ight quotes. never revised bids.
agents require training, i.e., data previously played games. However,
faced sort \chicken egg" problem: run agents, need
first train agents using data games involved, get
kind data, need first run agents. get around problem, ran
agents phases. Phase I, consisted 126 games, used training data
seeding, semifinals finals rounds TAC-01. Phase II, lasting 157 games,
retrained agents every six hours using data seeding, semifinals
finals rounds well games played Phase II. Finally, Phase III, lasting
622 games, continued retrain agents every six hours, using
data games played Phases II, including data seeding,
semifinals finals rounds.
Table 12 shows agents performed phases. Much
observe table consistent expectations. sophisticated boostingbased agents (

) clearly dominated agents based simpler
prediction schemes. Moreover, continued training, agents improved markedly
relative
. also see performance simplest agent,
,


ATTac-2001s

Cond'lMeans

ATTac-2001s

SimpleMeans

Cond'lMeans

ATTac-2001ev

Cond'lMeanev

SimpleMeanev

CurrentBid

EarlyBidder

EarlyBidder

livingagents

SimpleMeanev

livingagents

ATTac-2001s

ATTac-2001ev

EarlyBidder

CurrentBid

233

fiStone, Schapire, Littman, Csirik, & McAllester
Agent
ATTac-2001ev
ATTac-2001s
EarlyBidder
SimpleMeanev
SimpleMeans
Cond'lMeanev
Cond'lMeans
CurrentBid

Phase
105:2 49:5
27:8 42:1
140:3 38:6
28:8 45:1
72:0 47:5
8:6 41:2
147:5 35:6
33:7 52:4

Relative Score
Phase II
131:6 47:7 (2)
86:1 44:7 (3)
152:8 43:4 (1)
53:9 40:1 (5)
71:6 42:8 (6)
3:5 37:5 (4)
91:4 41:9 (7)
157:1 54:8 (8)

(2)
(3)
(1)
(5)
(7)
(4)
(8)
(6)

Phase III
166:2 20:8
122:3 19:4
117:0 18:0
11:5 21:7
44:1 18:2
60:1 19:7
91:1 17:6
198:8 26:0

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Table 12: average relative scores ( standard deviation) eight agents three
phases controlled experiment hotel prediction algorithm
varied. relative score agent score minus average score
agents game. agent's rank within phase shown parentheses.
employ kind training, significantly decline relative data-driven
agents.
hand, phenomena table surprising
us. surprising failure sampling help. strategy relies heavily
estimating hotel prices, also taking samples distribution hotel
prices. Yet results indicate using expected hotel price, rather price samples,
consistently performs better. speculate may insucient number
samples used (due computational limitations) numbers derived
samples high variance. Another possibility method using
samples estimate scores consistently overestimates expected score assumes
agent behave perfect knowledge individual sample|a property
approximation scheme. Finally, algorithm uses sampling several different points
(computing hotel expected values, deciding buy ights, pricing entertainment tickets, etc.), quite possible sampling beneficial decisions detrimental
others. example, directly comparing versions algorithm sampling
used subsets decision points, data suggests sampling hotel
decisions beneficial, sampling ights entertainment tickets neutral best, possibly detrimental. result surprising given sampling
approach motivated primarily task bidding hotels.
also surprised

eventually performed worse
less sophisticated

. One possible explanation
simpler model happens give predictions good complicated
model, perhaps closing time terribly informative, perhaps
adjustment price based current price significant. things equal,
simpler model advantage statistics based price data,
regardless closing time, whereas conditional model makes prediction based
eighth data (since eight possible closing times, equally likely).
addition agent performance, possible measure inaccuracy eventual
predictions, least non-sampling agents. agents, measured root
Cond'lMeans

SimpleMeans

Cond'lMeanev

SimpleMeanev

234

fiDecision-Theoretic Bidding Learned Density Models

mean squared error predictions made Phase III. were: 56.0
,
66.6
, 69.8
71.3
. Thus, see
lower error predictions (according measure), higher score
(correlation R = 0:88).
ATTac-2001ev

SimpleMeanev

6.3.2

ATTac-2001

vs.

CurrentBid

Cond'lMeanev

EarlyBidder

sense, two agents finished top standings TAC-01 represented
opposite ends spectrum.
agent uses simple open-loop strategy, committing set desired goods right beginning game,
uses
closed-loop, adaptive strategy.
open-loop strategy relies agents stabilize economy create
consistent final prices. particular, eight agents open loop place high
bids goods want, many prices skyrocket, evaporating potential
profit. Thus, set open-loop agents would tend get negative scores|the open-loop
strategy parasite, manner speaking. Table 13 shows results running 27
games 7 copies open-loop
one
. Although motivated

, actuality identical
except uses

places ight hotel bids immediately first ight quotes. bids
hotels appear G time. hotel bids $1001.
experiments, one copy
included comparison. price predictors
Phase preceding experiments.
's high bidding strategy backfires
ends overpaying significantly goods. experiments indicate,
may improve even allowed train games on-going
experiment well.
livingagents

ATTac-2001

EarlyBidder

livingagents

ATTac-2001

ATTac-2001

SimpleMeanev

ATTac-2001s

EarlyBidder

ATTac-2001

Agent
ATTac-2001

(7)

EarlyBidder

Score
2431 464
4880 337

Utility
8909 264
9870 34

Table 13: results running
7 copies
course
27 games.
achieves high utility, overpays significantly, resulting
low scores.
ATTac-2001

EarlyBidder

EarlyBidder

open-loop strategy advantage buying minimal set goods. is,
never buys use. hand, susceptible unexpected prices
get stuck paying arbitrarily high prices hotel rooms decided
buy.
Notice Table 13 average utility
's clients significantly
greater
's clients. Thus, difference score accounted
entirely cost goods.
ends paying exorbitant prices,
generally steers clear expensive hotels. clients' utility suffers,
cost-savings well worth it.
Compared open-loop strategy,
's strategy relatively stable
itself. main drawback changes decision goods wants
EarlyBidder

ATTac-2001

EarlyBidder

ATTac-

2001

ATTac-2001

235

fiStone, Schapire, Littman, Csirik, & McAllester

may also buy goods hedge possible price changes, end getting stuck
paying goods ultimately useless clients.
Table 14 shows results 7 copies
playing
one copy
. Again, training seeding round finals TAC01: agents don't adapt experiment. Included experiment three
variants
, different ight-lookahead parameter (from section
\cost postponing ight commitments"). three copies agents
ight-lookahead set 2 3 (
(2)
(3), respectively),
one
agent ight-lookahead set 4 (
(4)).
ATTac-2001

EarlyBidder

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

Agent
EarlyBidder

(2)
(3)
ATTac-2001(4)
ATTac-2001
ATTac-2001

Score
2869 69
2614 38
2570 39
2494 68

Utility
10079 55
9671 32
9641 32
9613 55

Table 14: results running
7 copies
course 197 games. three different versions
different ight-lookaheads.
EarlyBidder


slightly

ATTac-2001

ATTac-2001

results Table 14 clear
better committing
ight purchases later game (
(2) opposed
(4)).
comparison Table 13, economy represented significantly better overall.
is, many copies
economy cause suffer.
However, economy,
able invade. gets significantly higher utility
clients pays slightly
agents (as computed
utility minus score).9
results section suggest variance closing prices largest
determining factor effectiveness two strategies (assuming nobody else
using open-loop strategy). speculate large price variances, closedloop strategy (
) better, small price variances, open-loop
strategy could better.
ATTac-2001

ATTac-2001

ATTac-2001

ATTac-2001

EarlyBidder

ATTac-2001

ATTac-2001

7. Discussion

open-loop closed-loop strategies previous section differ handling
price uctuation. fundamental way taking price uctuation account place
\safe bids." high bid exposes agent danger buying something
ridiculously high price. prices fact stable high bids safe. prices
uctuate, high bids, bids stable-price strategy, risky. TAC,
hotel rooms sold Vickrey-style nth price action. separate auction
day hotel auctions done sequentially. Although order
auctions randomized, known agent, placing bids one
9. suspect agents allowed retrain course experiments, ATTac-2001
would end improving, saw Phase III previous set experiments. occur,
possible EarlyBidder would longer able invade.

236

fiDecision-Theoretic Bidding Learned Density Models

auctions agent assumes auction close next. assumed design
agent bids one auction affect prices auctions. assumption
strictly true, large economy one expects bids single individual
limited effect prices. Furthermore, price affected bid price
item bid on; effect auctions seems less direct perhaps
limited. Assuming bids one auction affect prices another, optimal bidding
strategy standard strategy Vickrey auction|the bid item equal
utility bidder. So, place Vickrey-optimal bid, one must able estimate
utility item. utility owning item simply expected final score
assuming one owns item minus expected final score assuming one
item. So, problem computing Vickrey-optimal bid reduced problem
predicting final scores two alternative game situations. use two score prediction
procedures, call stable-price score predictor (corresponding Equation 5)
unstable-price score predictor (Equation 4).
Stable-Price Score Predictor. stable-price score predictor first estimates
expected prices rest game using whatever information available
given game situation. computes value achieved optimal purchases
estimated prices. economy stable prices, estimate quite accurate|
make optimal purchases expected price then, prices near
estimates, performance also near estimated value.
Unstable-Price Score Predictor. Stable-price score prediction take
account ability agent react changes price game progresses. Suppose given room often cheap sometimes expensive. agent first determine
price room, plan price, agent better guessing
price ahead time sticking purchases dictated price. unstable
price predictor uses model distribution possible prices. repeatedly samples
prices distribution, computes stable-price score prediction sampled
price, takes average stable-price scores various price samples.
score prediction algorithm similar algorithm used Ginsberg's (2001) quite
successful computer bridge program score predicted sampling possible
hands opponent and, sample, computing score optimal play case
players complete information (double dummy play). approach
simple intuitive motivation, clearly imperfect. unstable-price score predictor
assumes future decisions made presence complete price information,
agent free change existing bids auctions yet closed.
assumptions approximately true best. Ways compensating
imperfections score prediction described Section 5.
Buy Decide Later. trading agent must decide airline tickets buy
buy them. deciding whether buy airline ticket, agent compare
predicted score situation owns airline ticket predicted score
situation airline ticket may buy later. Airline tickets
tend increase price, agent knows certain ticket needed buy
soon possible. whether given ticket desirable may depend
price hotel rooms, may become clearer game progresses. airline tickets
increase price, case TAC-00, bought
237

fiStone, Schapire, Littman, Csirik, & McAllester

last possible moment (Stone et al., 2001). determine whether airline ticket
bought not, one compare predicted score situation one
bought ticket current price predicted score situation
price ticket somewhat higher yet bought. interesting note
one uses stable-price score predictor predictions, ticket
purchased optimal allocation current price estimate, predicted
score buying ticket always higher|increasing price ticket
reduce score. However, unstable-price score predictor yield advantage
delaying purchase. advantage comes fact buying ticket may
optimal prices optimal others. ticket yet
bought, score higher sampled prices ticket
bought. corresponds intuition certain cases purchase
delayed information available.
guiding principle design agent was, greatest extent possible,
agent analytically calculate optimal actions. key component calculations
score predictor, based either single estimated assignment prices model
probability distribution assignments prices. score predictors, though clearly
imperfect, seem useful. two predictors, unstable-price predictor
used quantitatively estimate value postponing decision information
available. accuracy price estimation clearly central importance. Future research
undoubtedly focus ways improving price modeling score prediction based
price modeling.
8. Related Future Work

Although good deal research auction theory, especially
perspective auction mechanisms (Klemperer, 1999), studies autonomous bidding agents
interactions relatively recent. TAC one example. FM97.6
another auction test-bed, based fishmarket auctions (Rodriguez-Aguilar, Martin,
Noriega, Garcia, & Sierra, 1998). Automatic bidding agents also created
domain (Gimenez-Funes, Godo, Rodriguez-Aguiolar, & Garcia-Calves, 1998).
number studies agents bidding single good multiple auctions (Ito,
Fukuta, Shintani, & Sycara, 2000; Anthony, Hall, Dang, & Jennings, 2001; Preist, Bartolini,
& Phillips, 2001).
notable auction-based competition held prior TAC Santa Fe
Double Auction Tournament (Rust, Miller, & Palmer, 1992). auction involved several
agents competing single continuous double auction similar TAC entertainment
ticket auctions. analyzed Tesauro Das (2001), tournament
parasite strategy that, like
described Section 6.3, relied agents
find stable price took advantage gain advantage. case,
advantage gained waiting last minute bid, strategy commonly known
sniping.
TAC-01 second iteration Trading Agent Competition. rules TAC01 largely identical TAC-00, three important exceptions:
1. TAC-00, ight prices tend increase;
livingagents

238

fiDecision-Theoretic Bidding Learned Density Models

2. TAC-00, hotel auctions usually closed end game;
3. TAC-00, entertainment tickets distributed uniformly agents
minor surface, differences significantly enriched strategic complexity
game. TAC-00, designers discovered dominant strategy
defer serious bidding end game. result, focus solving
allocation problem, agents using greedy, heuristic approach. Since
hotel auctions closed end game, timing issues also important,
significant advantages going agents able bid response last-second price
quotes (Stone & Greenwald, 2003). Nonetheless, many techniques developed 2000
relevant 2001 competition: agent strategies put forth TAC-00 important
precursors second year's field, instance pointed Section 5.1.
Predicting hotel clearing prices perhaps interesting aspect TAC agent
strategies TAC-01, especially relation TAC-00 last-minute bidding created
essentially sealed-bid auction. indicated experiments described Section 6.3,
many possible approaches hotel price estimation problem, approach
chosen significant impact agent's performance. Among observed
TAC-01 following (Wellman, Greenwald, Stone, & Wurman, 2002), associated
cases price-predictor variant experiments motivated it.
1. use current price quote p (
).
2. Adjust based historic data. example, average historical difference
clearing price price time t, predicted clearing price p + .
3. Predict fitting curve sequence ask prices seen current game.
4. Predict based closing price data hotel past games (
,
).
5. above, condition hotel closing time, recognizing closing
sequence uence relative prices.
6. above, condition full ordering hotel closings, hotels
open closed particular point (
,
).
7. Learn mapping features current game (including current prices) closing
prices based historic data (
,
).
8. Hand-construct rules based observations associations abstract features.
demonstrated
's success bidding simultaneous auctions multiple interacting goods TAC domain, extended approach apply
U.S. Federal Communications Commission (FCC) spectrum auctions domain (Weber, 1997).
FCC holds spectrum auctions sell radio bandwidth telecommunications companies. Licenses entitle owners use specified radio spectrum band within specified
geographical area, market. Typically several licenses auctioned simultaneously
bidders placing independent bids license. recent auction brought


CurrentBid





SimpleMeanev

Cond'lMeanev

ATTac-2001s

ATTac-2001

239

Cond'lMeans

ATTac-2001ev



SimpleMeans

fiStone, Schapire, Littman, Csirik, & McAllester

$16 billion dollars. detailed simulation domain (Csirik, Littman, Singh, &
Stone, 2001), discovered novel, successful bidding strategy domain allows
bidders increase profits significantly reasonable default strategy (Reitsma,
Stone, Csirik, & Littman, 2002).
ongoing research agenda includes applying approach similar domains.
particularly expect boosting approach price prediction decision-theoretic
reasoning price distributions transfer domains. candidate real-world
domains include electricity auctions, supply chains, perhaps even travel booking
public e-commerce sites.
Acknowledgements

work partially supported United States{Israel Binational Science Foundation (BSF), grant number 1999038. Thanks TAC team University Michigan
providing infrastructure support required run many experiments.
Thanks Ronggang Yu University Texas Austin running one experiments mentioned article. research conducted
authors AT&T Labs | Research.
References

Anthony, P., Hall, W., Dang, V. D., & Jennings, N. R. (2001). Autonomous agents
participating multiple on-line auctions. Proceedings IJCAI-2001 Workshop
E-Business Intelligent Web Seattle, WA.
Boutilier, C. (2002). pomdp formulation preference elicitation problems. Proceedings
Eighteenth National Conference Artificial Intelligence, pp. 239{246.
Collins, M., Schapire, R. E., & Singer, Y. (2002). Logistic regression, AdaBoost Bregman distances. Machine Learning, 48 (1/2/3).
Csirik, J. A., Littman, M. L., Singh, S., & Stone, P. (2001). FAucS: FCC spectrum
auction simulator autonomous bidding agents. Fiege, L., Muhl, G., & Wilhelm,
U. (Eds.), Electronic Commerce: Proceedings Second International Workshop,
pp. 139{151 Heidelberg, Germany. Springer Verlag.
Freund, Y., & Schapire, R. E. (1997). decision-theoretic generalization on-line learning
application boosting. Journal Computer System Sciences, 55 (1),
119{139.
Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering successful TAC
participation. First International Joint Conference Autonomous Agents
Multi-Agent Systems Bologna.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide
Theory NP-completeness. Freeman, San Francisco, CA.
240

fiDecision-Theoretic Bidding Learned Density Models

Gimenez-Funes, E., Godo, L., Rodriguez-Aguiolar, J. A., & Garcia-Calves, P. (1998). Designing bidding strategies trading agents electronic auctions. Proceedings
Third International Conference Multi-Agent Systems, pp. 136{143.
Ginsberg, M. L. (2001). GIB: Imperfect information computationally challenging game.
JAIR, 14, 303{358.
Greenwald, A., & Boyan, J. (2001). Bidding algorithms simultaneous auctions.
Proceedings Third ACM Conference E-Commerce, pp. 115{124 Tampa, FL.
Harsanyi, J. (1967{1968). Games incomplete information played bayesian players.
Management Science, 14, 159{182,320{334,486{502.
Hauskrecht, M. (1997). Incremental methods computing bounds partially observable
Markov decision processes. Proceedings Fourteenth National Conference
Artificial Intelligence, pp. 734{739.
He, M., & Jennings, N. R. (2002). SouthamptonTAC: Designing successful trading agent.
Fifteenth European Conference Artificial Intelligence Lyon, France.
Ito, T., Fukuta, N., Shintani, T., & Sycara, K. (2000). Biddingbot: multiagent support
system cooperative bidding multiple auctions. Proceedings Fourth
International Conference MultiAgent Systems, pp. 399{400.
Kearns, M., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm nearoptimal planning large Markov decision processes. Proceedings Sixteenth
International Joint Conference Artificial Intelligence (IJCAI-99), pp. 1324{1331.
Klemperer, P. (1999). Auction theory: guide literature. Journal Economic
Surveys, 13 (3), 227{86.
Littman, M. L., Majercik, S. M., & Pitassi, T. (2001). Stochastic Boolean satisfiability.
Journal Automated Reasoning, 27 (3), 251{296.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12 (3), 441{450.
Preist, C., Bartolini, C., & Phillips, I. (2001). Algorithm design agents participate multiple simultaneous auctions. Agent Mediated Electronic Commerce III
(LNAI), pp. 139{154. Springer-Verlag, Berlin.
Reitsma, P. S. A., Stone, P., Csirik, J. A., & Littman, M. L. (2002). Self-enforcing strategic
demand reduction. Agent Mediated Electronic Commerce IV: Designing Mechanisms Systems, Vol. 2531 Lecture Notes Artificial Intelligence, pp. 289{306.
Springer Verlag.
Rodriguez-Aguilar, J. A., Martin, F. J., Noriega, P., Garcia, P., & Sierra, C. (1998). Towards
test-bed trading agents electronic auction markets. AI Communications, 11 (1),
5{19.
241

fiStone, Schapire, Littman, Csirik, & McAllester

Rothkopf, M. H., & Harstad, R. M. (1994). Modeling competitive bidding: critical essay.
Management Science, 40 (3), 364{384.
Rust, J., Miller, J., & Palmer, R. (1992). Behavior trading automata computerized
double auction market. Friedman, D., & Rust, J. (Eds.), Double Auction
Market: Institutions, Theories, Evidence. Addison-Wesley, Redwood City, CA.
Schapire, R. E., & Singer, Y. (1999). Improved boosting algorithms using confidence-rated
predictions. Machine Learning, 37 (3), 297{336.
Schapire, R. E., & Singer, Y. (2000). BoosTexter: boosting-based system text categorization. Machine Learning, 39 (2/3), 135{168.
Stone, C. J. (1994). use polynomial splines tensor products multivariate
function estimation. Annals Statistics, 22 (1), 118{184.
Stone, P., & Greenwald, A. (2003). first international trading agent competition:
Autonomous bidding agents. Electronic Commerce Research. appear.
Stone, P., Littman, M. L., Singh, S., & Kearns, M. (2001). ATTac-2000: adaptive
autonomous bidding agent. Journal Artificial Intelligence Research, 15, 189{206.
Tesauro, G., & Das, R. (2001). High-performance bidding agents continuous double
auction. Third ACM Conference Electronic Commerce, pp. 206{209.
Weber, R. J. (1997). Making less: Strategic demand reduction FCC
spectrum auctions. Journal Economics Management Strategy, 6 (3), 529{548.
Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2002). 2001 trading agent
competition. Proceedings Fourteenth Innovative Applications Artificial
Intelligence Conference, pp. 935{941.
Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003a). 2001 trading
agent competition. Electronic Markets, 13 (1), 4{12.
Wellman, M. P., Reeves, D. M., Lochner, K. M., & Vorobeychik, Y. (2003b). Price prediction
trading agent competition. Tech. rep., University Michigan.
Wellman, M. P., Wurman, P. R., O'Malley, K., Bangera, R., Lin, S.-d., Reeves, D., & Walsh,
W. E. (2001). trading agent competition. IEEE Internet Computing, 5 (2), 43{51.

242

fiJournal Artificial Intelligence Research 19 (2003) 73-138

Submitted 12/02; published 8/03

Optimal Schedules Parallelizing Anytime Algorithms:
Case Shared Resources
Lev Finkelstein
Shaul Markovitch
Ehud Rivlin

lev@cs.technion.ac.il
shaulm@cs.technion.ac.il
ehudr@cs.technion.ac.il

Computer Science Department
Technion - Israel Institute Technology
Haifa 32000, Israel

Abstract
performance anytime algorithms improved simultaneously solving
several instances algorithm-problem pairs. pairs may include different instances
problem (such starting different initial state), different algorithms (if several
alternatives exist), several runs algorithm (for non-deterministic algorithms).
paper present methodology designing optimal scheduling policy based
statistical characteristics algorithms involved. formally analyze case
processes share resources (a single-processor model), provide algorithm
optimal scheduling. analyze, theoretically empirically, behavior
scheduling algorithm various distribution types. Finally, present empirical results
applying scheduling algorithm Latin Square problem.

1. Introduction
Assume task learn concept predefined success rate, measured
given test set. Assume use two alternative learning algorithms, one learns
fast requires preprocessing, another works slowly requires
preprocessing. possibly benefit using learning algorithms parallel
solve one learning task single-processor machine?
Another area application constraint satisfaction problems. Assume
student tries decide two elective courses trying schedule
set compulsory courses. student try solve two sets
constraints sequentially two computations somehow interleaved?
Assume crawler searches specific page site. one
starting point, process could speeded simultaneous application crawler
(or all) them. However, would optimal strategy bandwidth
restricted?
examples common?
potential benefits gained uncertainty amount
resources required solve one instance algorithmproblem pair. use different algorithms (in first example) different
problems (in last two examples). non-deterministic algorithms, also
use different runs algorithm.
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiFinkelstein, Markovitch & Rivlin

process executed purpose satisfying given goal predicate.
task considered accomplished one runs succeeds.
goal predicate satisfied time , also satisfied time > .
property equivalent utility monotonicity anytime algorithms (Dean &
Boddy, 1988; Horvitz, 1987), solution quality restricted Boolean values.
objective provide schedule minimizes expected cost, possibly
constraints (for example, processes may share resources). problem definition
typical rational-bounded reasoning (Simon, 1982; Russell & Wefald, 1991). problem
resembles faced contract algorithms (Russell & Zilberstein, 1991; Zilberstein, 1993).
There, given allocated resources, task construct algorithm providing
solution highest quality. case, given quality requirements, task
construct algorithm solves problem using minimal resources.
several research works deal similar problems. Simple parallelization,
information exchange processes, may speed process due
high diversity solution times. example, Knight (1993) showed using many
reactive agents employing RTA* search (Korf, 1990) beneficial using single
deliberative agent. Another example work Yokoo Kitamura (1996), used
several search agents parallel, agent rearrangement preallotted periods time.
Janakiram, Agrawal, Mehrotra (1988) showed many common distributions
solution time, simple parallelization leads linear speedup. One exception
family heavy-tailed distributions (Gomes, Selman, & Kautz, 1998) possible
obtain superlinear speedup simple parallelization.
superlinear speedup also obtained access internal structure
processes involved. example, Clearwater, Hogg, Huberman (1992) reported
superlinear speedup cryptarithmetic problems result information exchange processes. Another example works Kumar Rao (Rao & Kumar,
1987; Kumar & Rao, 1987; Rao & Kumar, 1993), devoted parallelizing standard search
algorithms, superlinear speedup obtained dividing search space.
interesting domain-independent approach based portfolio construction (Huberman, Lukose, & Hogg, 1997; Gomes & Selman, 1997). approach, different
amount resources allotted process. reduce expected resource
consumption variance.
case non-deterministic algorithms, another way benefit solution time
diversity restart algorithm attempt switch better trajectory.
framework analyzed detail Luby, Sinclair, Zuckerman (1993) case
single processor Luby Ertel (1994) multiprocessor case. particular,
proven single processor, optimal strategy periodically restart
algorithm constant amount time solution found. strategy
successfully applied combinatorial search problems Gomes, Selman, Kautz (1998).
several settings, however, restart strategy optimal.
goal schedule number runs single non-deterministic algorithm,
number limited due nature problem (for example, robotic search),
restart strategy applicable optimal. special case settings
scheduling number runs deterministic algorithm finite set available initial
74

fiOptimal Schedules Parallelizing Anytime Algorithms

configurations (inputs). Finally, case goal schedule set algorithms
different scope restart strategy.
goal research develop methodology designing optimal scheduling
policy number instances algorithm-problem pairs, algorithms
either deterministic non-deterministic. present formal framework scheduling parallel anytime algorithms case processes share resources (a singleprocessor model), based statistical characteristics algorithms involved.
framework assumes know probability goal condition satisfied
function time (a performance profile (Simon, 1955; Boddy & Dean, 1994) restricted
Boolean quality values). analyze properties optimal schedules suspendresume model, allocation resources performed mutual exclusion basis,
show cases extension framework intensity control, resources
may allocated simultaneously proportionately multiple demands, yield
better schedules. also present algorithm building optimal schedules. Finally,
demonstrate experimental results optimal schedules.

2. Motivation
starting formal discussion, would like illustrate different scheduling
strategies affect performance system two search processes. first example
simple setup allows us perform full analysis. second example,
show quantitative results real CSP problem.
2.1 Scheduling DFS Search Processes
Assume DFS random tie-breaking applied simple search space shown Figure 1,
two runs algorithm allowed 1 . large number
paths goal, half length 10, quarter length 40, quarter
length 160. one processes finds solution, task considered
accomplished.
10
40
10
160



B
10
40
10
160

Figure 1: simple search task: two DFS-based agents search path B. Scheduling
processes may reduce costs.
1. limit follow, example, physical constraints, problem robotic search.
unlimited number runs optimal results would provided restart strategy.

75

fiFinkelstein, Markovitch & Rivlin

consider single-processor system, two processes cannot run simultaneously. Let us denote processes 1 A2 , L1 L2 actual path lengths
A1 A2 respectively particular run.
application single processes (without loss generality, 1 ) gives us expected
execution time 1/2 10 + 1/4 40 + 1/4 160 = 55, shown Figure 2.

@
1/2
L1 = 10, cost = 10

@

@


1/2

L1 = 40, cost = 40

1/2
@

@
@t L1 6= 10
@
@
1/2
@
@
@
@t

L1 = 160, cost = 160

Figure 2: Path lengths, probabilities costs running single process
improve performance simulating simultaneous execution two processes. purpose, allow processes expand single node,
switch process (without loss generality, 1 starts first). case,
expected execution time 1/2 19 + 1/4 20 + 1/8 79 + 1/16 80 + 1/16 319 = 49.3125,
shown Figure 3.
Finally, know distribution path lengths, allow 1 open 10 nodes;
A1 fails, stop allow A2 open 10 nodes; A2 fails well, allow
A1 open next 30 nodes, forth. scenario, 1 A2 switch 10
40 nodes (if processes fail find solution 40 nodes, guaranteed
found A1 160 nodes). scheme shown Figure 4, expected time
1/2 10 + 1/4 20 + 1/8 50 + 1/16 80 + 1/16 200 = 33.75.
2.2 Latin Square Example
task Latin Square problem place N symbols N N square
symbol appears row column. example shown
Figure 5.
interesting problem arises square partially filled. problem
case may solvable (see left side Figure 6) unsolvable (see right side
Figure 6). problem satisfiability partially filled Latin Square typical
constraint-satisfaction problem. consider slight variation task. Let us assume
two partially filled squares available, need decide whether least one
solvable. assume allocated single processor. attempt speed
time finding solution starting solve two problems two different
initial configurations parallel.
processes employs deterministic heuristic DFS First-Fail heuristic (Gomes & Selman, 1997). consider 10%-filled 20 20 Latin Squares. behavior
single process measured set 50,000 randomly generated samples shown
76

fiOptimal Schedules Parallelizing Anytime Algorithms

t0

@
1/2
L1 = 10,
L2 10,
cost = 19

t0 = 0
@

@

t1


1/2
@


@
@t1
@

1/2
L1 40,
L2 = 10,
cost = 20

@

L1 40,
L2 10
@

t1 = 19

1/2
@

L1 40,

@
@t2 L2 40
@
@ 1/2
1/2
@
@
L1 = 160,
t3

@

@t3
L2 40
@
@ 1/2
1/2
@
L1 = 160,
@
t4

L2 = 40,
@

@t4
cost = 80

t2


L1 = 40,
L2 40,
cost = 79

t2 = 20

t3 = 79

L1 = 160,
L2 = 160

t4 = 80

1
L1 = 160,
L2 = 160,
cost = 319

t5


A2
A1
...
0

1

2

3

4

5

6

7

8

9

10

Figure 3: Path lengths, probabilities costs simulating simultaneous execution

77

t5 = 319

fiFinkelstein, Markovitch & Rivlin

t0

@
1/2
L1 = 10,
L2 10,
cost = 10

t0 = 0
@

@

t1


1/2
@


@
@t1
@

1/2
L1 40,
L2 = 10,
cost = 20

t2


L1 = 40,
L2 40,
cost = 50

@

L1 40,
L2 10
@

t1 = 10

1/2
@

L1 40,

@
@t2 L2 40
@
@ 1/2
1/2
@
@
L1 = 160,
t3

@
@t3

L2 40
@
@ 1/2
1/2
@
L1 = 160,
@
t4

L2 = 40,
@
@t4

cost = 80

t2 = 20

t3 = 50

L1 = 160,
L2 = 160

t4 = 80

1
L1 = 160,
L2 = 160,
cost = 200

t5


t5 = 200

A2
A1
t0

t1

t2

t3

t4

t5

Figure 4: Path lengths, probabilities costs interleaved execution

1
3
5
2
4

2
4
1
3
5

3
5
2
4
1

4
1
3
5
2

5
2
4
1
3

Figure 5: example 5 5 Latin Square.

78

fiOptimal Schedules Parallelizing Anytime Algorithms

1

1
4

5

? 4 5

1
2

2
3

1
1 2

4

Figure 6: example solvable (to left) unsolvable (to right) prefilled 5 5 Latin
Squares.

Figure 7. Figure 7(a) shows probability finding solution function number
search steps, Figure 7(b) shows corresponding distribution density. Assume
0.8

0.05
0.045

0.7

0.04
0.6
0.035
0.03

0.4

f(t)

F(t)

0.5

0.025
0.02

0.3

0.015
0.2
0.01
0.1

0
300

0.005

400

500

600

700

800

900

0
300

1000

400

500

600



700

800

900

1000



(a)

(b)

Figure 7: behavior DFS First-Fail heuristic 10%-filled 20 20 Latin Squares.
(a) probability finding solution function number search steps;
(b) corresponding distribution density.

run limited 25,000 search steps (only 88.6% problems solvable
condition). apply algorithm one available two initial configurations,
average number search steps 3777. run two processes parallel (alternating
step), obtain result 1358 steps. allow single switch optimal
point (an analogue restart technique (Luby et al., 1993; Gomes et al., 1998) two
processes), get 1376 steps average (the optimal point 1311 steps). Finally,
interleave processes, switching points corresponding 679, 3072, 10208
total steps, average number steps 1177. results averaged
test set 25,000 pairs initial configurations.
last sequence switch points optimal schedule process behavior
described graphs Figure 7. rest paper present algorithm
deriving optimal schedules.
79

fiFinkelstein, Markovitch & Rivlin

3. Framework Parallelization Scheduling
section formalize intuitive description parallelization scheduling. first
part framework similar framework presented (Finkelstein & Markovitch,
2001).
Let set states, time variable non-negative real values,
random process realization (trajectory) A(t) represents mapping
R+ S. Let X0 random variable defined S. Since algorithm Alg starting
initial state S0 corresponds single trajectory (for deterministic algorithms),
set trajectories associated distribution (for non-deterministic algorithms),
pair hX0 , Algi, X0 stands initial state, viewed random process.
Drawing trajectory process corresponds, without loss generality, twostep procedure: first initial state 0 drawn X0 , trajectory A(t) starting
S0 drawn Alg. Thus, source randomness either randomness
initial state, randomness algorithm (which come algorithm
environment), both.
Let designated set states, G : {0, 1} characteristic
function called goal predicate. behavior trajectory A(t) respect
cA (t). say
goal predicate G written G(A(t)), denote G
cA (t) non-decreasing function
monotonic G G
cA (t) step function
trajectory A(t) A. assumptions G
one discontinuity point.
Let monotonic G. definitions see behavior
G trajectory A(t) described single point b
tA,G , first point
cA (t) = 1}. G
cA (t) always 0,
goal predicate true, i.e, b
tA,G = inf {t|G
b
say tA,G defined. Therefore, define random variable,
trajectory A(t) b
tA,G defined, corresponds b
tA,G . behavior variable
described distribution function F (t). points F (t) differentiable,
use probability density f (t) = F 0 (t).
important note practice every trajectory leads goal
predicate satisfaction even infinitely large time. means set trajectories b
tA,G undefined necessarily measure zero. define
probability success p probability A(t) b
tA,G defined2 . Latin
Square example described Section 2.2, probability success 0.886, graphs
Figure 7 correspond pF (t) pf (t).
Assume system n random processes 1 , . . . corresponding distribution functions F1 , . . . , Fn goal predicates G1 , . . . , Gn . distribution
functions Fi Fj identical, refer Ai Aj F -equivalent.
define schedule system set binary functions { },
moment t, i-th process active (t) = 1 idle otherwise. refer scheme
suspend-resume scheduling. possible generalization framework extend
suspend/resume control refined mechanism allows us determine
2. Another way express possibility process reach goal state use F (t)
approach 1 p . prefer use p explicitly distribution function must meet
requirement limt F (t) = 1.

80

fiOptimal Schedules Parallelizing Anytime Algorithms

intensity process acts. software processes, means varying
fraction CPU utilization; tasks like robot navigation implies changing speed
robots. Mathematically, using intensity control equivalent replacing binary
functions (t) continuous functions range zero one 3 .
Note scheduling makes term time ambiguous. one hand,
subjective time process, consumed process active. kind
time corresponds resource consumed process. hand,
objective time measured point view external observer. distribution
function Fi (t) process defined subjective time, cost function (see
below) may use kinds times. Since using several processes, formulas
paper based objective time.
Let us denote (t) total time process active t.
definition,
Z


(x)dx.

(t) =

(1)

0

practice (t) provides mapping objective time subjective time
i-th process, refer functions subjective schedule functions. Since
obtained differentiation, often describe schedules { } instead
{i }.
processes {Ai } goal predicates {Gi } running schedules {i }Wresult
new process A, goal predicate G. G disjunction G (G(t) = Gi (t)),
therefore monotonic G. denote distribution function corresponding random variable Fn (t, 1 , . . . , n ), corresponding distribution density
fn (t, 1 , . . . , n ).
Assume given monotonic non-decreasing cost function u(t, 1 , . . . , tn ),
depends objective time subjective times per process . also assume
u(0, t1 , . . . , tn ) = 0. Since subjective times calculated (t), actually
u = u(t, 1 (t), . . . , n (t)).
expected cost schedule {i } expressed, therefore, as4
Z +
Eu (1 , . . . , n ) =
u(t, 1 , . . . , n )fn (t, 1 , . . . , n )dt
(2)
0

(for sake readability, omit (t)). suspend-resume model assumptions, must differentiable (except countable set process switch points)
derivatives 0 1 would ensure correct values . intensity control
assumptions, derivatives must lie 0 1.
consider two alternative setups resource sharing processes:
1. processes share resources mutual exclusion basis. means exactly
one process active moment, processes active one
another goal reached one them. case sum derivatives
3. special case setup using constant intensities described Huberman, Lukose,
Hogg (1997).
4. generalization case probability success p 1 considered end
next section.

81

fiFinkelstein, Markovitch & Rivlin

always one5 . case shared resources corresponds case several
processes running single processor.
2. processes fully independent: additional constraints .
case corresponds n independent processes running n processors.
goal find schedule minimizes expected cost (2) corresponding
constraints. current paper devoted case shared processes. case
independent resources studied (Finkelstein, Markovitch, & Rivlin, 2002).
scheduled algorithms considered framework viewed anytime algorithms. behavior anytime algorithms usually characterized performance
profile expected quality algorithm output function alloted resources.
goal predicate G viewed quality function two possible values, thus
distribution function F (t) meets definition performance profile, time plays
role resource.

4. Suspend-Resume Based Scheduling
section consider case suspend-resume based control ( continuous
functions derivatives 0 1).
Claim 1 expressions goal-time distribution F n (t, 1 , . . . , n ) expected
cost Eu (1 , . . . , n ) follows6 :
Fn (t, 1 , . . . , n ) = 1
Eu (1 , . . . , n ) =

Z

0

+

u0t

+

n

i=1

n
X

(1 Fi (i )),

(3)

n


(4)

i0 u0i

i=1

!

i=1

(1 Fi (i ))dt.

Proof: Let ti time would take i-th process meet goal acted alone
(if process fails reach goal, consider = ). Let time takes
system n processes reach goal. case, distributed according
Fn (t, 1 , . . . , n ), ti distributed according Fi (t). Thus, processes,
given schedule, independent, obtain
Fn (t, 1 , . . . , n ) = P (t t) = 1 P (t > t) = 1 P (t1 > 1 (t)) . . . P (tn > n (t)) =
n

(1 Fi (i (t))),
1 (1 F1 (1 (t))) . . . (1 Fn (n (t))) = 1
i=1

corresponds (3). Since F (t) distribution time, assume F (t) = 0
0.
5. fact obvious case suspend-resume control, intensity control reflected
Lemma 3.
6. u0t u0i stand partial derivatives u respectively.

82

fiOptimal Schedules Parallelizing Anytime Algorithms

average cost function therefore
Z +
Eu (1 , . . . , n ) =
u(t, 1 , . . . , n )fn (t, 1 , . . . , n )dt =
0
Z +

u(t, 1 , . . . , n )d(1 Fn (t, 1 , . . . , n )) =
0

u(t, 1 , . . . , n )(1

Fn (t, 1 , . . . , n ))|
0

+

Z

+

0

n

du(t, 1 , . . . , n )
(1 Fi (i ))dt.
dt
i=1

Since u(0, 1 , . . . , n ) = 0 Fn (, 1 , . . . , n ) = 1, first term last expression
0. Besides, since full derivative u written
n

X
du(t, 1 , . . . , n )
i0 u0i ,
= u0t +
dt
i=1

obtain
Eu (1 , . . . , n ) =

Z

+

u0t +

0

n
X
i=1

i0 u0i

!

n

i=1

(1 Fi (i ))dt,

completes proof.
Q.E.D.
Note case (t) = Fi (t) = F (t) (parallel application n
F -equivalent processes), obtain formula presented (Janakiram et al., 1988), i.e.,
Fn (t) = 1 (1 F (t))n .
rest section show formal solution (necessary conditions algorithm) framework shared resources. start two processes present
formulas algorithm, generalize solution arbitrary number
processes. case two processes, assume u differentiable.
elaborated setup n processes, assume total cost linear
combination objective time subjective times, subjective times
weight:
n
X
(t).
(5)
u(t, 1 , . . . , n ) = + b
i=1

Since time consumed active process, trivial case
processes idle may ignored, obtain (without loss generality)
Eu (1 , . . . , n ) =

Z

0

n


(1 Fj (j ))dt min .

(6)

j=1

assumption made keep expressions readable. solution process
remains general form u.
4.1 Necessary Conditions Optimal Solution Two Processes
Let A1 A2 two processes sharing resource. working, one process locks
resource, necessarily idle. show dependency yields
83

fiFinkelstein, Markovitch & Rivlin

strong constraint behavior process, allowing building effective
algorithm solving minimization problem.
suspend-resume model, therefore, two states system possible:
A1 active A2 idle (S1 ); A1 idle A2 active (S2 ). ignore
case processes idle, since removing state schedule
increase cost. Therefore, system continuously alternates two states:
S1 S2 S1 S2 . . .. call time interval corresponding pair hS 1 , S2
phase denote phase k k . denote process switch points , phase
k corresponds [t2k2 , t2k ]. See Figure 8 illustration.

2k3

S2

k1



t2k2

t2k1


S1

S2

k



t2k t2k+1

S1
S2
k+1


2k+2
S1

t2k+3

k+2

Figure 8: Notations times, states phases two processes
scheme, A1 active intervals [t0 , t1 ], [t2 , t3 ], . . . , [t2k , t2k+1 ], . . . ,
A2 active intervals [t1 , t2 ], [t3 , t4 ], . . . , [t2k+1 , t2k+2 ], . . . .
Let us denote 2k1 total time A1 active t2k1 ,
2k total time A2 active t2k . phase definition, 2k1 2k
correspond cumulative time spent phases 1 k states 1 S2 respectively.
exists one-to-one correspondence sequences ti :
+ i+1 = ti+1 .

(7)

Moreover, definition
1 (t2k1 ) = 1 (t2k ) = 2k1 ,
2 (t2k ) = 2 (t2k+1 ) = 2k .

(8)

process switch scheme defined above, subjective schedule functions 1
2 time intervals [t2k , t2k+1 ] (state S1 phase k+1 ) form
1 (t) = t2k + 1 (t2k ) = t2k + 2k1 = 2k ,
2 (t) = 2 (t2k ) = 2k .

(9)

Similarly, intervals [t2k+1 , t2k+2 ] (state S2 phase k+1 ), subjective schedule
functions defined
1 (t) = 1 (t2k+1 ) = 2k+1 ,
2 (t) = t2k+1 + 2 (t2k+1 ) = t2k+1 + 2k = 2k+1 .
Let us denote
v(t1 , t2 ) = u0t (t1 + t2 , t1 , t2 ) + u01 (t1 + t2 , t1 , t2 ) + u02 (t1 + t2 , t1 , t2 )
84

(10)

fiOptimal Schedules Parallelizing Anytime Algorithms


vi (t1 , t2 ) = u0t (t1 + t2 , t1 , t2 ) + u0i (t1 + t2 , t1 , t2 ).
provide optimal solution suspend/resume model, may split (4) phases
k write
Eu (1 , . . . , n ) =

Z
X

t2k

k=1 t2k2

v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt.

(11)

last expression may rewritten
Eu (1 , . . . , n ) =
Z t2k+1
X
v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt+
k=0 t2k
Z t2k+2
X
k=0 t2k+1

(12)

v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt.

Using (9) interval [t2k , t2k+1 ], performing substitution x = 2k , using (7),
obtain
Z

t2k+1

t2k
Z t2k+1

v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt =

v1 (t
t2k
Z t2k+1 2k
t2k 2k
Z 2k+1
2k1

2k , 2k )(1 F1 (t 2k ))(1 F2 (2k ))dt =

(13)

v1 (x, 2k )(1 F1 (x))(1 F2 (2k ))dx =

v1 (x, 2k )(1 F1 (x))(1 F2 (2k ))dx.

Similarly, interval [t2k+1 , t2k+2 ]
Z

t2k+2

t2k+1
Z t2k+2

v(1 , 2 )(1 F1 (1 ))(1 F2 (2 ))dt =

v2 (2k+1 ,
t2k+1
Z t2k+2 2k+1
t2k+1 2k+1
Z 2k+2
2k

2k+1 )(1 F1 (2k+1 ))(1 F2 (t 2k+1 ))dt =

v2 (2k+1 , x)(1 F1 (2k+1 ))(1 F2 (x))dx =

v2 (2k+1 , x)(1 F1 (2k+1 ))(1 F2 (x))dx.
85

(14)

fiFinkelstein, Markovitch & Rivlin

Substituting (13) (14) (12), obtain new form minimization problem:
Eu (1 , . . . , n ) =
"
Z

X
(1 F2 (2k ))
k=0

(1 F1 (2k+1 ))

Z

2k+1
2k1

v1 (x, 2k )(1 F1 (x))dx +

2k+2

2k

(15)



v2 (2k+1 , x)(1 F2 (x))dx min

(for sake generality, assume 1 = 0).
minimization problem (15) equivalent original problem (4), dependency solutions described (9) (10). constraint new
problem follows fact processes alternating non-negative periods
time:

0 = 0 < 2 . . . 2n . . .
(16)
1 < 3 . . . 2n+1 . . .
expression (15) reaches optimal values either
dEu
= 0 k = 1, . . . , n, . . . ,
dk

(17)

border described (16). However, two processes can, without loss
generality, ignore border case. Indeed, assume = i+2 > 1 (one
processes skips turn). construct new schedule removing i+1 i+2 :
1 , . . . , i1 , , i+3 , i+4 , i+5 , . . .
easy see process described schedule exactly process
described original one, singularity point removed.
Thus, step time spent processes determined (17). see
2k appears three subsequent terms E u (1 , . . . , n ):

. . . + (1 F1 (2k1 ))
(1 F2 (2k ))

Z

2k+1

Z

2k
2k2

v2 (2k1 , x)(1 F2 (x))dx+

v1 (x, 2k )(1
2k1
Z 2k+2

(1 F1 (2k+1 ))

2k

F1 (x))dx+

v2 (2k+1 , x)(1 F2 (x))dx + . . . .
86

fiOptimal Schedules Parallelizing Anytime Algorithms

Differentiating (15) 2k , therefore, yields
dEu
= v2 (2k1 , 2k )(1 F1 (2k1 ))(1 F2 (2k ))
d2k
Z 2k+1
f2 (2k )
v1 (x, 2k )(1 F1 (x))dx+
2k1

(1 F2 (2k ))

Z

2k+1

2k1

v1
(x, 2k )(1 F1 (x))dx
t2

v2 (2k+1 , 2k )(1 F1 (2k+1 ))(1 F2 (2k )) =

(1 F2 (2k ))(v2 (2k1 , 2k )(1 F1 (2k1 )) v2 (2k+1 , 2k )(1 F1 (2k+1 ))
Z 2k+1
f2 (2k )
v1 (x, 2k )(1 F1 (x))dx+
2k1

(1 F2 (2k ))

Z

2k+1

2k1

v1
(x, 2k )(1 F1 (x))dx.
t2

similar expression derived differentiating (15) 2k+1 . Combining
expressions (17) gives us following theorem:
Theorem 1 (The chain theorem two processes)
value i+1 2 computed given i1 using formulas
f2 (2k )
v2 (2k1 , 2k )(1 F1 (2k1 )) v2 (2k+1 , 2k )(1 F1 (2k+1 ))
+
=
R 2k+1
1 F2 (2k )
2k1 v1 (x, 2k )(1 F1 (x))dx
R 2k+1 v1
2k1 t2 (x, 2k )(1 F1 (x))dx
, = 2k + 1,
R 2k+1
2k1 v1 (x, 2k )(1 F1 (x))dx

v1 (2k , 2k+1 )(1 F2 (2k )) v1 (2k+2 , 2k+1 )(1 F2 (2k+2 ))
f1 (2k+1 )
+
=
R 2k+2
1 F1 (2k+1 )
v
(
,
x)(1

F
(x))dx
2
2
2k+1
2k
R 2k+2 v2
2k
t1 (2k+1 , x)(1 F2 (x))dx
, = 2k + 2.
R 2k+2
v
(
,
x)(1

F
(x))dx
2
2
2k+1
2k

(18)

(19)

Corollary 1 linear cost function (5), value i+1 2 computed
given i1 using formulas
f2 (2k )
F1 (2k+1 ) F1 (2k1 )
, = 2k + 1,
= R
2k+1
1 F2 (2k )
(1 F1 (x))dx

(20)

2k1

f1 (2k+1 )
F2 (2k+2 ) F2 (2k )
, = 2k + 2.
= R
2k+2
1 F1 (2k+1 )
(1 F2 (x))dx

(21)

2k

proof follows immediately fact v (t1 , t2 ) = + b.
Theorem 1 allows us formulate algorithm building optimal solution.
algorithm presented next subsection.
87

fiFinkelstein, Markovitch & Rivlin

4.2 Optimal Solution Two Processes: Algorithm
goal scheduling algorithm minimize expression (15)
Eu (1 , . . . , n ) =
"
Z

X
(1 F2 (2k ))
k=0

(1 F1 (2k+1 ))
constraints



Z

2k+1
2k1

v1 (x, 2k )(1 F1 (x))dx +


2k+2

2k

v2 (2k+1 , x)(1 F2 (x))dx min

0 = 0 < 2 . . . 2n . . .
1 < 3 . . . 2n+1 . . . .

Assume A1 acts first (1 > 0). Theorem 1 see values
0 = 0 1 determine set possible values 2 , values 1 2 determine
possible values 3 , on.
Therefore, non-zero value 1 provides us tree possible values k .
branching factor tree determined number roots (18) (19).
possible sequence 1 , 2 , . . . evaluated using (15).
cases total time limited discussed Section 4.5,
series expression converge, e.g., process finite cost finding
solution, algorithm stops finite number points. cases, however,
extremely heavy-tailed distributions, possible series diverge.
ensure finite number iterations cases, set upper limit maximal
expected cost.
Another limit added probability failure. Since = i1 + , probability
runs would able find solution
(1 F1 (i1 ))(1 F2 (i )).
Therefore, difference
(1 F1 (i1 ))(1 F2 (i )) (1 p1 )(1 p2 )
becomes small enough, conclude runs failed find solution stop
execution.
value 1 find best sequence using one standard search
algorithms, Branch-and-Bound. Let us denote value best sequence
1 Eu (1 ). Performing global optimization E u (1 ) 1 provides us
optimal solution case 1 acts first. Note value 1 may also 0
(A2 acts first), need compare value obtained optimization 1
value obtained optimization 2 1 = 0.
flow algorithm illustrated Figure 9, formal scheme presented
Figure 10, description main routine (realized DFS Branch Bound
method) Figure 11.
88

fiOptimal Schedules Parallelizing Anytime Algorithms

algorithm considers two main branches, one 1 one A2 ,
processed procedure minimize sequence f irst point (Figure 10). step,
initialize array values, pass it, procedure build optimal sequence,
recursive procedure df sbnb, represents core algorithm (Figure 11).
df sbnb procedure, shown Figure 11, acts follows. obtains input
array values, cost involved current moment, best value reached till
now. cost exceeds value, procedure performs classical Branch-and-Bound
cutoff (lines 1-2).
inner loop (lines 4-19) corresponds different roots expressions (18)
(19). new value corresponding k calculated procedure
calculate next zeta (line 5), cannot exceed previously found root saved
last zeta (for first iteration, last zeta initialized k2 ), lines 3 8. Lines 67 correspond case lower bound passed calculate next zeta exceeds
maximal available time, case procedure stopped.
new possible value found, procedure updates current cost (line
9), stopping criteria mentioned validated new array values,
denoted concatenation old array new value (line 10).
task accomplished, cost verified versus best known value (which updated
necessary), procedure returns (lines 10-16). Otherwise, temporarily added
array , Branch-and-Bound procedure called recursively calculation
k+1 .
whole tree traversed (except cutoffs), best known cost returned
(line 20). corresponding array required solution.
Figure 13 shows trace single Branch-and-Bound run example shown
Section 2.2 starting optimal value 1 . optimal schedule derived
run 679, 2393, 7815, 17184 expected cost 1216.49 steps. scheduling points
given subjective times. Using objective (total) time schedule written 679,
3072, 10208, 25000. particular run Branch-and-Bound cutoffs due
small number roots (18) (19).
4.3 Necessary Conditions Optimal Solution n Processes
section generalize solution case two processes case n
processes.
Assume n processes A1 , . . . , using shared resources. One possible
ways present schedule use sequence
h(Ai1 , t1 ), (Ai2 , t2 ), . . . , (Aij , tj ), . . .i,
Aij j-th active process, tj time allocated invocation ij .
simplify formalization problem, however, use following alternative
representation. First, allow j 0, makes possible represent every
schedule
h(A1 , t1 ), (A2 , t2 ), . . . , (An , tn ), (A1 , tn+1 ), (A2 , tn+2 ), . . . , (An , t2n ), . . .i.
89

fiFinkelstein, Markovitch & Rivlin


+


















x
Q

Get optimal schedule costs
Q 1 = 0 1 6= 0,
Q
Q
return
best value
Q
Q
corresponding
schedule
Q
Q

A1 acts first

Q

Q

Q

A2 acts first

?

?

...

Minimization 1
P

QP
QPPP




PP
Q
1 trials


Q
PP


)

q
P

Q


Q

+


Q
/

w

h
?

J

J

J


J


^
Jh
/

=

h
h
x
?



J



J




J




J




J^



h
x
h
x
x
?
.?



?
Jh
@


J
B


J
B@

J



B @
@
J



B

@
J^
/

R
@
?
?
J




BN


minimization procedure
Root Branch Bound tree
2 satisfying (19) k = 0
Branch Bound nodes
3 satisfying (18) k = 1
Branch Bound nodes
4 satisfying (19) k = 1
.....

h

Branch Bound non-leaf nodes

x

Leaf nodes (terminating condition satisfied) cutoff nodes (expected result
worse already known). cost calculated accordance (15).
Figure 9: flow algorithm constructing optimal schedules 2 processes

90

fiOptimal Schedules Parallelizing Anytime Algorithms

procedure optimize
Input: F1 (t), F2 (t) (performance profiles).
Output: optimal sequence value.
[sequence1 , val1 ] minimize sequence f irst point(A 1 )
[sequence2 , val2 ] minimize sequence f irst point(A 2 )
val1 < val2
return [sequence1 , val1 ]
else
return [sequence2 , val2 ]
end
end
procedure minimize sequence f irst point(process)
zetas[1] 0
zetas[0] 0
process = A2
zetas[1] 0
end
Using one standard minimization methods, find zetas,
minimizing value function build optimal sequence(zetas),
corresponding cost.
end

Figure 10: Procedure optimize builds optimal sequence case 1 starts, optimal sequence case A2 starts, compares results, returns best
one. Procedure minimize sequence f irst point returns optimal sequence
value.

91

fiFinkelstein, Markovitch & Rivlin

procedure build optimal sequence(zetas)
curr cost calculate cost(zetas)
return df sbnb(zetas, curr cost, AX V ALU E)
end
procedure df sbnb(zetas, curr cost, thresh)
1:
(curr cost thresh)
// Cutoff
2:
return AX V ALU E
3:
last value zetas[length(zetas) 2]
// previous time value
4:
repeat
5:
calculate next zeta(zetas, last value)
6:
( = last value)
// Skip
7:
return thresh
8:
last value
9:
delta cost calculate partial cost(zetas, )
10:
(task accomplished([zetas || ]))
// Leaf
11:
(curr cost + delta cost < thresh)
12:
optimal zetas [zetas || ]
13:
thresh curr cost + delta cost
14:
end
15:
return thresh
16:
end
17:
tmp result df sbnb([zetas || ], curr cost + delta cost, thresh)
18:
thresh = min(thresh, tmp result)
19: end
// repeat
20: return thresh
end
Figure 11: Procedure build optimal sequence, given prefix time sequence, restores
optimal sequence prefix using DFS Branch Bound search algorithm,
returns sequence value. [x || y] stands concatenation x y.
Auxiliary functions shown Figure 12.

92

fiOptimal Schedules Parallelizing Anytime Algorithms

1. calculate cost(zetas) computes cost sequence (or part) accordance
(15),
2. calculate partial cost(zetas, ) computes additional cost obtained adding
sequence,
3. calculate next zeta(zetas, last value) uses (18) (19) calculate value
next greater last value. solution exists,
maximal time value returned,
4. task accomplished(zetas) returns true task may considered
accomplished (e.g., either maximal possible time over, probability
error negligible, upper limit cost exceeded).
Figure 12: Auxiliary functions used optimal schedule algorithm

1

679.0

2

379.4

3

24620.6

2393.0

7815.4

u=2664.54

4

24321.0
u=1534.06

22607.0
u=1265.67

17184.6
u=1216.49

Figure 13: trace single run Branch-and-Bound procedure starting optimal
value 1 .

93

fiFinkelstein, Markovitch & Rivlin

Therefore, system alternates n states 1 S2 . . . Sn S1 . . .,
state Si corresponds situation active rest processes
idle. time spent k-th invocation tkn+i .
case two processes, call time interval corresponding sequence
states S1 S2 . . . Sn phase denote phase k k . denote process
switch points k t1k , t2k , . . . , tnk ,
tik

=

k1
X

tnj+i.

j=0

0

Process Ai active phase k interval [t i1
k , tk ], entire phase lasts k
tnk . corresponding scheme shown Figure 14.



n1
tk1

Sn
k1

tn = t0k
k1
S1

t1
sk

S2

t2
sk

...
k

tn1
tn = t0k+1 t1k+1
sk
sk

Sn
S1
k+1

Figure 14: Notations times, states phases n processes
simplify following discussion, would like allow indices ik less
0 greater n. purpose, denote
mod n
,
tik = tik+bi/nc

(22)


index process active interval [t i1
k , tk ] denote #i. mod n 6=
0 obtain #i = mod n, mod n = 0 #i = n. Notation (22) claims
shift n upper index equivalent shift 1 phase number:

ti+n
= tik+1 .
k
case two processes, denote ki total time A#i active
tik . ki corresponds cumulative time spent phases 1 k state #i ,
one-to-one correspondence sequences ki tik :

ki k1
= tik ti1
k ,
n1
X
j=0

kij = tik n.

(23)
(24)

first equation corresponds fact time i1
tik accumulated
k
values process A#i , second equation claims switch
objective time system equal sum subjective times process.
sake uniformity also denote
n
1
= . . . = 1
= 00 = 0.
1

94

fiOptimal Schedules Parallelizing Anytime Algorithms


construction ki see, time interval [ti1
k , tk ] subjective time process
Aj following form:
j
j = 1, . . . , 1,
k ,
i1

(t tk ) + k1 , j = i,
(25)
j (t) =
j
k1 ,
j = + 1, . . . , n.

subjective time functions system 3 processes illustrated Figure 15.
6 (t)

1 (t)
21
22
23
12

#

11

#

#

#

#

#

#

#

#
2 (t)
3 (t)

13
t01

t11

t21

t31

t12

t22

t32



Figure 15: Subjective time functions system 3 processes

find optimal schedule system n processes, need minimize
expression given (6). constraints monotonicity sequence
process i:

ki k+1
k, i.
(26)
Given expressions j , prove following lemma:
Lemma 1 system n processes, expression expected cost (6)
rewritten
Z
n i+n1
X

X
k
j
(1 Fi (x))dx.
(27)
(1 F#j (k1 ))
Eu (1 , . . . , n , . . .) =

k1

k=0 i=1 j=i+1

proof given Appendix A.1.
lemma makes possible prove chain theorem arbitrary number
processes:
95

fiFinkelstein, Markovitch & Rivlin

l1
l1 ,
Theorem 2 (The chain theorem) value m+1
may either
computed given previous 2n 2 values using formula

1

l )
fl (m
l )
Fl (m

=

l+n1

j=l+1

l1
X

j
(1 F#j (m1
))

i+n1


i=ln+1 j=i+1
#j6=l

j
(1 F#j (m
))

l+n1

j=l+1

Z

j
(1 F#j (m
))


m+1




(28)
(1 F#i (x))dx

proof theorem given Appendix A.2.
4.4 Optimal Solution n Processes: Algorithm
goal presented algorithm minimize expression (27)
Eu (1 , . . . , n , . . .) =

n i+n1
X

X
k=0 i=1 j=i+1

(1

j
F#j (k1
))

Z

ki

k1

(1 Fi (x))dx

constraints

ki k+1
k, i.

case two processes, assume 1 acts first. Theorem 2, given 2n 2
values
10 , 11 , . . . , 1n , 21 , 22 , 2n3 ,
determine possibilities value 2n2 (either 1n2 process skips
turn, one roots (28)). Given values 2n2 , determine
values 2n1 , on.
idea algorithm similar algorithm two processes. first 2n 2
variables (including 10 = 0) determine tree possible values . Optimization
2n3 first variables, therefore, provides us optimal schedule (as before, compare
results case first k < n variables 0). difference
case two processes process may skip turn. However, ignore case
processes skip turn, since remove loop schedule.
scheme algorithm presented Figure 16, description main
routine (realized DFS Branch Bound method) presented Figure 17.
4.5 Optimal Solution Case Additional Constraints
Assume problem additional constraints: solution time limited
probability success i-th process p necessarily 1.
possible show expressions distribution function expected
cost almost form regular framework:
Claim 2 Let system solution time limited , let p probability success
i-th process. expressions goal-time distribution expected cost
96

fiOptimal Schedules Parallelizing Anytime Algorithms

Procedure optimize builds n optimal schedules (each process may start first), compares
results, returns best one
procedure optimize
best val AX V ALU E
best sequence
loop 1 n
[sequence, val] minimize sequence f irst points(i)
(val < best val)
best val val
best sequence sequence
end
return [best sequence, best val]
end
// Procedure minimize sequence f irst points gets parameter
// index process starts, returns optimal
// sequence value
procedure minimize sequence f irst points(process start)
loop 0 n 1
zetas[i] 0
end
loop 1 process start 1
zetas[i] 0
end
Using one standard minimization methods, find zetas,
minimizing value function build optimal sequence(zetas).
end

Figure 16: algorithm finding optimal schedule n processes. result contains
mod n
vector , = 0i = bi/nc
.

97

fiFinkelstein, Markovitch & Rivlin

procedure build optimal sequence(zetas)
curr cost calculate cost(zetas)
return df sbnb(zetas, curr cost, AX V ALU E, 0)
end
procedure df sbnb(zetas, curr cost, thresh, nskip)
(curr cost thresh)
return AX V ALU E
// Cutoff
// previous time value current process
last value zetas[length(zetas) n]
repeat
calculate next zeta(zetas, last value)
( = last value)
// Skip
break loop
last value
delta cost calculate partial cost(zetas, )
// Leaf
(task accomplished([zetas || ]))
(curr cost + delta cost < thresh)
optimal zetas [zetas || ]
thresh curr cost + delta cost
end
break loop
end
tmp result df sbnb([zetas || ], curr cost + delta cost, thresh, 0)
thresh = min(thresh, tmp result)
end
// repeat
(nskip < n 1)
// Skip possible
zeta zetas[length(zetas) n]
tmp result df sbnb([zetas || ], curr cost, thresh, nskip + 1)
thresh = min(thresh, tmp result)
end
return thresh
end
Figure 17: Procedure build optimal sequence, given prefix time sequence, restores optimal sequence prefix using DFS Branch Bound search algorithm,
returns sequence value. [x || y] stands concatenation x y.
auxiliary functions used similar counterparts Figure 12, deal n
processes instead 2.

98

fiOptimal Schedules Parallelizing Anytime Algorithms

follows:
Fn (t, 1 , . . . , n ) = 1
Eu (1 , . . . , n ) =

Z



0

n

i=1

(1 pi Fi (i )) (for ),

u0t +

n
X

i0 u0i

i=1

!

n

i=1

(1 pi Fi (i ))dt.

(29)
(30)

proof similar proof Claim 1.
claim shows formulas used previous sections valid
current settings, three differences:
1. use pj Fj instead Fj pj fj instead fj .
2. integrals 0 instead 0 .
3. time variables limited .
first two conditions may easily incorporated algorithms. last condition implies additional changes chain theorems algorithms. chain
theorem n processes becomes:
j
Theorem 3 value kj either k1
, computed given previous
2n 2 values using formula (28), calculated formula

kj

=T

n1
X

kjl .

(31)

l=1

first two alternatives similar Theorem 2, third one corresponds
boundary condition given Equation (24). third alternative adds one branch
DFS Branch Bound algorithm; rest algorithm remains unchanged.
Similar changes algorithms performed case maximal allowed
time Ti per process. practice, always use limitation, setting
probability Ai reach goal Ti , pi (1 Fi (Ti )), becomes negligible.

5. Process Scheduling Intensity Control
section analyze problem optimal scheduling case intensity control, equivalent replacing binary scheduling functions (t) continuous
functions range 0 1. paper assume linear cost function
form (5). believe, however, similar analysis applicable setup
differentiable u.
easy see formulas distribution function expected cost
Claim 1 still valid intensity control settings.
linear cost function (5), minimization problem form
! n
Z
n
X

i0
Eu (1 , . . . , n ) =
a+b
(1 Fj (j ))dt min .
(32)
0

i=1

99

j=1

fiFinkelstein, Markovitch & Rivlin

Without loss generality, assume + b = 1. leads equivalent minimization problem
! n
Z
n

X
0
(1 Fj (j ))dt min,
(33)

(1 c) + c
Eu (1 , . . . , n ) =
0

i=1

j=1

c = b/(a + b) viewed normalized resource weight. constraints,
however, complicated suspend/resume model:
1. before, must continuous, (0) = i0 (0) = 0 (at beginning
processes idle).
2. assume partially-continuous derivative i0 , derivative
lie 0 1. requirement follows definition intensity
fact i0 = : process work negative amount time,
process work intensity greater one allowed. Since consider
framework shared resources, total intensity limited,
additional constraint: sum derivatives i0 time point cannot
exceed 1.
Thus, optimization problem following boundary conditions:
(0) = 0, i0 (0) = 0 = 1, . . . , n,
0 i0 1 = 1, . . . , n,
n
X
0
i0 1.

(34)

i=1

looking set functions { } provide solution minimization
problem (33) constraints (34).
Let g(t, 1 , . . . , n , 10 , . . . , n0 ) function integral sign (33):
! n
n

X
0
0
0
(1 Fj (j )).
(35)
g(t, 1 , . . . , n , 1 , . . . , n ) = (1 c) + c

i=1

j=1

traditional method solving problems type use Euler-Lagrange necessary
conditions: set functions 1 , . . . , n provides weak (local) minimum functional
Z
Eu (1 , . . . , n ) =
g(t, 1 , . . . , n , 10 , . . . , n0 )dt
0

1 , . . . , n satisfy system equations form
g0 k

0
g 0 = 0.
dt k

(36)

prove following lemma:
Lemma 2 Euler-Lagrange conditions minimization problem (33) yield two strong
invariants:
100

fiOptimal Schedules Parallelizing Anytime Algorithms

1. processes k1 k2 k1 k2 border described (34),
distribution density functions satisfy
fk1 (k1 )
fk2 (k2 )
=
.
1 Fk1 (k1 )
1 Fk2 (k2 )

(37)

2. schedules processes border described (34), either
c = 1 fk (k ) = 0 k.
proof lemma given Appendix A.3. lemma provides necessary
conditions local minimum inner points described constraints (34).
conditions, however, restricting. Therefore, look general conditions,
suitable boundary points well 7 .
start following lemma:
Lemma 3 optimal solution minimization problem (33) constraints (34)
exists, exists optimal solution 1 , . . . , n , time
resources consumed, i.e.,
n
X
(38)
i0 (t) = 1.

i=1

case time cost zero (c 6= 1), equality necessary condition
solution optimality.
proof lemma given Appendix A.4.

Corollary 2 intensity control settings, case suspend-resume settings,
minimization problem (33) form (6), i.e.
Eu (1 , . . . , n ) =

Z

0

n


(1 Fj (j ))dt min .

j=1

Lemma 3 corresponds intuition: resource available, used.
Without loss generality, restrict discussion schedules satisfying (38), even
case time cost zero. leads following invariant:


n
X

(t) = t.

(39)

i=1

Assume two F -equivalent processes 1 A2 density function
f (t) satisfying normal distribution law mean value m. Let 1 t2
cumulative time consumed processes time t, i.e., 1 (t) = t1 2 (t) = t2 .
question is, process active (or active parallel
partial intensities)?
7. Note also even conditions hold, necessarily provide optimal solution.
Moreover, problems variation calculus necessarily minimum, since analogue
Weierstrass theorem continuous functions closed set.

101

fiFinkelstein, Markovitch & Rivlin

0.4

0.4

0.35

0.35

0.3

0.3

0.25

0.25

0.2

0.2

f(t)

f(t)

Without loss generality, t1 < t2 , means first process required
cover larger area succeed: 1 F (t 1 ) > 1 F (t2 ). supports policy time
activates second process. policy supported 1 lower distribution
density, f1 (t1 ) < f2 (t2 ), illustrated Figure 18(a). If, however, first process
higher density, illustrated Figure 18(b), clear two processes
activated time t. optimal policy general case 8 ? answer relies

0.15

0.15

0.1

0.1

0.05

0.05

0

0

1

2

t1

3

t2

4

5


6

7

8

9

0

10

0

1

2

3

(a)

4

5


6

t1

7

t2

8

9

10

(b)

Figure 18: (a) Process A1 (currently t1 ) lower density larger area cover, therefore
inferior. (b) Process A1 lower density, smaller area cover, decision
unclear.

heavily functions appear (37). functions, described equation
hk (t) =

fk (t)
,
1 Fk (t)

(40)

known hazard functions, play important role following theorem
describing necessary conditions optimal schedules.
Theorem 4 Let set functions { } solution minimization problem (6)
constraints (34). Let t0 point hazard functions processes h (i (t))
continuous, let Ak process active t0 (k0 (t0 ) > 0),
process Ai
hi (i (t0 )) < hk (k (t0 )).
(41)
t0 process k consumes resources, i.e. k0 (t0 ) = 1.
proof theorem given Appendix A.5.
Theorem 4 Equation (37), intensity control may useful hazard
functions least two processes equal. However, even case equilibrium
always stable. Assume within interval [t 0 , t00 ] processes Ai Aj
working partial intensity, implies h (i (t)) = hj (j (t)). Assume
8. Analysis normal distribution given Section 6.3 shows optimal policy example
give resources process A2 cases.

102

fiOptimal Schedules Parallelizing Anytime Algorithms

hi (t) hj (t) monotonically increasing. moment give priority one
processes, obtain higher value hazard function, get
subsequent resources. case stable equilibrium h (i (t)) hj (j (t))
monotonically decreasing functions constants.
intuitive discussion formulated following theorem:
Theorem 5 active process remain active consume resources long
hazard function monotonically increasing.
proof given Appendix A.6.
theorem imply important corollary:
Corollary 3 hazard function one processes greater equal
others = 0 monotonically increasing t, process
one activated.
conclude extension suspend-resume model intensity control
many cases increase power model beneficial monotonically
decreasing hazard functions. time cost taken account (c = 1), however,
intensity control permits us connect two concepts: model shared
resources model independent agents:
Theorem 6 time cost taken account (c = 1), model shared resources
intensity control settings equivalent model independent processes
suspend-resume control settings. Namely, given suspend-resume solution model
independent processes, may reconstruct intensity-based solution
cost model shared resources vice versa.
proof theorem given Appendix A.7.
Theorem 4 claims process maximal value h k (k (t)) active,
take resources. Why, then, would always choose process
highest value hk (k (t)) active? turns strategy optimal.
Let us consider two processes distribution densities shown Figure 19(a).
corresponding values hazard functions shown Figure 19(b). using
strategy, A2 would active process. Indeed, time = 0, h 2 (2 (0)) >
h1 (1 (0)), would lead activation 2 . moment, A1 would remain
idle hazard function remain 0. strategy would result expected time 2.
If, hand, would activated 1 only, result would expected
time 1.5. Thus, although h1 (1 (0)) < h2 (2 (0)), better give resources
A1 beginning due superiority future.
elaborate example shown Figure 20. corresponds case two
processes F -equivalent, one linear combination two normal
distributions, f (t) = 0.5fN (0.6,0.2) (t) + 0.5fN (4.0,2.0) (t), fN (,) (t) distribution
density normal distribution mean value standard deviation , second
process uniformly distributed [1.5, 2.5]. Activating 1 results 0.5 0.6 + 0.5
4.0 = 2.3, activating A2 results expected time 2.0, activating 1 time
1.2 followed activating A2 results (approximately) 0.6 0.5 + (1.2 + 2.0) 0.5 = 1.9.
103

fiFinkelstein, Markovitch & Rivlin

1.2

5

f1(t)
f2(t)

1

h1(t)
h2(t)

4

h(t) (truncated)

f(t)

0.8

0.6

3

2

0.4

1

0.2

0

0

1

2

3

4

0

5

0

1

2



3

4

5



(a)

(b)

Figure 19: density function hazard function two processes. Although h 1 (1 (0)) <
h2 (2 (0)), better give resources A1 .

best solution is, therefore, start execution activating 1 , point t0
transfer control A2 . case interrupt active process greater value
hazard function, preferring idle process zero value hazard function (since
h1 (1 (t0 )) > h2 (2 (t0 )) = 0).
1.2

5

f1(t)
f2(t)

1

h1(t)
h2(t)

4

h(t) (truncated)

f(t)

0.8

0.6

3

2

0.4

1

0.2

0

0

2

4

6

8

0

10



0

2

4

6

8

10



(a)

(b)

Figure 20: density function hazard function two processes. best solution
start A1 , point interrupt favor A2 , although latter
zero hazard function.

examples show straightforward use hazard functions building optimal
schedules problematic. However, since suspend-resume model specific
case intensity control model, hazard functions still may useful understanding
behavior optimal schedules, used next section.
104

fiOptimal Schedules Parallelizing Anytime Algorithms

6. Optimal Scheduling Standard Distributions
section present results optimal scheduling strategy system
processes whose performance profiles meet one well-known distributions: uniform,
exponential, normal lognormal. show results processes bimodal
multimodal distribution functions.
implemented three scheduling policies two agents:
1. Sequential strategy, schedules processes one another, initiating
second process probability first one find solution becomes
negligible. processes F -equivalent, choose best order process
invocation.
2. Simultaneous strategy, simulates simultaneous execution processes.
3. Optimal strategy, implementation algorithm described Section 4.2.
rest section compare three strategies, deadline given,
processes stopped probability still find solution becomes
negligible.
goal compare different scheduling strategies analyze behavior
processes. Absolute quantitative measurements, average cost, process
dependent, therefore appropriate scheduling strategy evaluation. therefore would like normalize results application different scheduling methods
minimize effect process behavior. case F -equivalent processes, good
candidate normalization coefficient expected time individual process.
processes F -equivalent, however, decision straightforward,
therefore use results sequential strategy normalization factor.
define relative quality qref (S) strategy respect strategy ref
qref (S) = 1

u(S)
,
u(Sref )

(42)

u(S) average cost strategy S. measurement corresponds gain
(maybe negative) strategy relative reference strategy. section use
sequential strategy reference strategy.
6.1 Uniform Distribution
Assume goal-time distribution processes meets uniform law
interval [t0 , ], i.e., distribution functions

< t0 ,
0
(t t0 )/(T t0 ) [t0 , ],
(43)
F (t) =

1
>
density functions

f (t) =



0
6 [t0 , ],
1/(T t0 ) [t0 , ].
105

(44)

fiFinkelstein, Markovitch & Rivlin

density function process uniformly distributed [0, 1] shown Figure 21(a).
hazard function uniform distribution form

< t0 ,
0
1/(T t0 )
1
h(t) =
(45)
=
[t0 , ],

1 (t t0 )/(T t0 )

monotonically increasing function. Corollary 3, one process
active, optimal strategy equivalent sequential strategy.
processes F -equivalent, problem solved choosing process
minimal expected time.
interesting setup involves uniformly distributed process guaranteed
find solution. case corresponds probability success p less 1.
claimed Section 4.5, corresponding distribution density function
multiplied p. result, hazard function becomes
h(t) =

(

0

p
(T t0 ) p(t t0 )

< t0 ,
[t0 , ].

(46)

function still monotonically increasing t, conclusions remain same.
graphs hazard functions processes uniformly distributed [0, 1] probability
success 0.5, 0.8 1 shown Figure 21(b).
1.2

10

f(t)

1

h(t) p = 1.0
h(t) p = 0.8
h(t) p = 0.5

8

h(t) (truncated)

f(t)

0.8

0.6

6

4

0.4

2

0.2

0

0

0.2

0.4

0.6


0.8

1

0

1.2

0

0.2

0.4

(a)

0.6


0.8

1

1.2

(b)

Figure 21: (a) density function process, uniformly distributed [0, 1], (b) hazard functions
processes uniformly distributed [0, 1] probability success 0.5, 0.8 1.

6.2 Exponential Distribution
exponential distribution described density function

0
0
f (t) =

e
> 0,
106

(47)

fiOptimal Schedules Parallelizing Anytime Algorithms

distribution function form

0
0
F (t) =

1e
> 0.

(48)

Substituting expressions (6) gives
Eu (1 , . . . , n ) =

Z

0

n


(1 Fj (j ))dt =

j=1

Z



e

Pn

j=1

j j (t)

dt.

0

system F -equivalent processes, Lemma 3
n
X

j j (t) =

n
X

j (t) = t,

j=1

j=1

therefore
Eu (1 , . . . , n ) =

Z


0

et dt =

1
.


Thus, system F -equivalent processes schedules equivalent. interesting fact reflected also behavior hazard function, constant:
h(t) .
However, probability success smaller 1, hazard function becomes
monotonically decreasing function:
h(t) =

pet
p
.
=

1 p(1 e )
p + (1 p)et

processes work simultaneously (with identical intensities F -equivalent processes, intensities maintaining equilibrium hazard functions otherwise), since
process idle advantage working teammate.
Figure 22(a) shows density function exponentially distributed process
= 1. graphs hazard functions processes exponentially distributed
= 1 probability success 0.5, 0.8 1 shown Figure 22(b).
Let us consider somewhat elaborate example, involving processes
F -equivalent. Assume two learning systems, exponential-like
performance profile typical systems. also assume one systems requires
delay preprocessing works faster. Thus, assume first system
distribution density f1 (t) = 1 e1 , second one density f2 (t) = 2 e2 (tt2 ) ,
1 < 2 (the second faster), t2 > 0 (it also delay). Assume
learning systems deterministic given set examples, may fail
learn concept probability 1 p = 0.5. graphs density
hazard functions two systems shown Figure 23.
applied optimal scheduling algorithm Section 4.2 values 1 = 3,
2 = 10, t2 = 5. optimal schedule activate first system 1.15136 time
units, (if found solution) activate second system 5.77652 time units.
107

fiFinkelstein, Markovitch & Rivlin

1.2

f(t)

1

1

0.8

0.8
h(t) (truncated)

f(t)

1.2

0.6

0.6

0.4

0.4

0.2

0.2

0

0

2

4

6

8

0

10

h(t) p = 1.0
h(t) p = 0.8
h(t) p = 0.5

0

2

4

6





(a)

(b)

8

10

Figure 22: (a) density function process, exponentially distributed = 1, (b) hazard
functions processes exponentially distributed = 1 probability success
0.5, 0.8 1.

5

3

f1(t)
f2(t)

h1(t)
h2(t)

2.5

4

2

f(t)

h(t) (truncated)

3

2

1.5

1

1

0

0.5

0

2

4

6

8

0

10



0

2

4

6

8

10



(a)

(b)

Figure 23: (a) Density (b) hazard functions two exponentially distributed systems,
different values time shift.

108

fiOptimal Schedules Parallelizing Anytime Algorithms

first system run additional 3.22276 time units, finally second
system run 0.53572 time units. point solution found,
systems failed probability 1 10 6 each.
Figure 24(a) shows relative quality simultaneous optimal scheduling
strategies function t2 p = 0.8 (for 10000 simulated examples). large values
t2 benefit switching first algorithm second decreases,
reflected relative quality optimal strategy. simultaneous strategy,
see, beneficial relatively small values 2 .
Figure 24(b) reflects behavior strategies fixed value 2 = 5.0
function probability success p. simultaneous strategy inferior, quality
decreases p increases. Indeed, probability success 1, running second
algorithm first one simultaneously waste time. hand,
optimal strategy positive benefit, means resulting schedules
trivial.
40

40

Optimal
Simultaneous

35

20

30
25

0
Relative quality (in percent)

Relative quality (in percent)

Optimal
Simultaneous

20
15
10
5

-20

-40

-60

0
-5

-80

-10
-15

1

2

3

4

5
6
Delay second system

7

8

9

-100
0.1

10

(a)

0.2

0.3

0.4

0.5
0.6
Probability success

0.7

0.8

0.9

1

(b)

Figure 24: Learning systems: Relative quality optimal simultaneous scheduling strategies
(a) function t2 fixed p = 0.8, (b) function p fixed t2 = 5.

6.3 Normal Distribution
normal distribution mean value deviation described density
function
(tm)2
1
(49)
f (t) =
e 22 ,
2
distribution function
Z
(xm)2
1
F (t) =
(50)
e 22 dx.
2
Since use t0 = 0, used truncated normal distribution distribution
density
(tm)2
1
1
e 22 ,

(1 )
2
109

fiFinkelstein, Markovitch & Rivlin

distribution function


Z
(xm)2
1
1

e 22 dx ,

1
2

1
=
2

Z

0

e

(xm)2
2 2

dx,



large enough, may considered 0. density function normally
distributed process = 5 = 1 shown Figure 25(a).
hazard function normal distribution monotonically increasing, leads
conclusions uniform distribution. However, probability success
less 1 completely changes behavior hazard function: point,
starts decrease. graphs hazard functions processes normally distributed
mean value 5, standard deviation 1 probabilities success 0.5, 0.8 1
shown Figure 25(b).
0.4

6

f(t)

h(t) p = 1.0
h(t) p = 0.8
h(t) p = 0.5

0.35
5
0.3
4
h(t) (truncated)

f(t)

0.25

0.2

3

0.15
2
0.1
1
0.05

0

0

2

4

6

8

0

10



0

2

4

6

8

10



(a)

(b)

Figure 25: (a) density function normally distributed process, = 5 = 1, (b)
hazard functions normally distributed processes = 5 = 1,
probabilities success 0.5, 0.8 1.

previous example, consider case two processes F equivalent, running deviation = 1 probability success
p. first process assumed 1 = 1, second process started
delay m. relative quality 10000 simulated examples shown Figure 26.
Figure 26(a) shows relative quality function p = 0.8; Figure 26(b) shows
relative quality function p = 2. Unlike exponential distribution, gain
example optimal strategy rather small.
6.4 Lognormal Distribution
random variable X lognormally distributed, ln X normally distributed.
density function distribution function corresponding parameters
110

fiOptimal Schedules Parallelizing Anytime Algorithms

10

20

Optimal schedule
Simultaneous

0

Relative quality (in percent)

Relative quality (in percent)

0

-10

-20

-30

-40

-50

Optimal schedule
Simultaneous

-20

-40

-60

-80

0

1

2

3
4
Delay second process

5

-100
0.1

6

0.2

0.3

(a)

0.4

0.5
0.6
Probability success

0.7

0.8

0.9

1

(b)

Figure 26: Normal distribution: relative quality (a) function fixed p = 0.8, (b)
function p fixed = 2.

written
(log(t)m)2
1
e 22 ,
f (t) =
2
Z log(t)
(xm)2
1
e 22 dx.
F (t) =
2

(51)
(52)

Lognormal distribution plays significant role AI applications since many cases search
time distributed lognormal law. density function lognormal distribution mean value log(5.0) standard deviation 1.0 shown Figure 27(a),
hazard functions different values p shown Figure 27(b). Let us consider
simulated experiment similar analogue normal distribution. consider two
processes F -equivalent, parameters = 1 probability
success p. first process assumed 1 = 1, second process started
delay, m2 m1 = > 0. relative quality 10000 simulated
examples shown Figure 28. Figure 28(a) shows relative quality function
p = 0.8; Figure 28(b) shows relative quality function p = 2.
graphs show small values optimal simultaneous strategy
significant benefit sequential one. However, larger values, performance optimal strategy approaches performance sequential strategy,
simultaneous strategy becomes inferior.
6.5 Bimodal Multimodal Density Functions
Experiments show case F -equivalent processes unimodal distribution
function, sequential strategy often optimal. section consider less trivial
distributions.
111

fiFinkelstein, Markovitch & Rivlin

0.14

0.18

f(t)

h(t) p = 1.0
h(t) p = 0.8
h(t) p = 0.5

0.16

0.12

0.14
0.1

h(t) (truncated)

0.12

f(t)

0.08

0.06

0.1
0.08
0.06

0.04
0.04
0.02

0

0.02

0

5

10

15

20

25


30

35

40

45

0

50

0

5

10

15

(a)

20

25


30

35

40

45

50

(b)

Figure 27: (a) Density function lognormal distribution mean value log(5.0) standard
deviation 1.0 (b) hazard functions lognormally distributed processes
mean value log(5.0), standard deviation 1, probabilities success 0.5,
0.8 1.

60

40

Optimal schedule
Simultaneous

Optimal schedule
Simultaneous

50
20

Relative quality (in percent)

Relative quality (in percent)

40

30

20

10

0

-20

-40

0
-60
-10

-20

0

0.5

1

1.5
2
2.5
Delay second process

3

3.5

-80
0.1

4

(a)

0.2

0.3

0.4

0.5
0.6
Probability success

0.7

0.8

0.9

1

(b)

Figure 28: Lognormal distribution: relative quality (a) function fixed p = 0.8,
(b) function p fixed = 2.

112

fiOptimal Schedules Parallelizing Anytime Algorithms

Assume first non-deterministic algorithm performance profile
expressed linear combination two normal distributions deviation:
f (t) = 0.5fN (1 ,) + 0.5fN (2 ,) .
example density hazard functions distributions 1 = 2, 2 = 5,
= 0.5 given Figure 29.
0.35

0.9

f(t)

h(t) p = 1.0

0.8

0.3

0.7
0.25

h(t) (truncated)

0.6

f(t)

0.2

0.15

0.5
0.4
0.3

0.1
0.2
0.05

0

0.1

0

1

2

3

4


5

6

7

0

8

0

1

2

3

4


(a)

5

6

7

8

(b)

Figure 29: (a) Density function (b) hazard function process distributed according
density function f (t) = 0.5fN (2,0.5) + 0.5fN (5,0.5) probability success
p = 0.8.

Assume invoke two runs algorithm fixed values 1 = 2, = 0.5,
p = 0.8, free variable 2 . Figure 30 shows relative quality
scheduling strategies influenced distance peaks, 2 1 . results
correspond intuitive claim larger distance peaks,
attractive optimal simultaneous strategies become.
25

Optimal
Simultaneous

20

Relative quality (in percent)

15
10
5
0
-5
-10
-15
-20
-25

0

2

4

6

8

10

12

14

16

Distance peaks

Figure 30: Bimodal distribution: relative quality function distance peaks.

113

fiFinkelstein, Markovitch & Rivlin

let us see number peaks density function affects scheduling
quality. consider case partial uniform distribution, density distributed
k identical peaks length 1 placed symmetrically time interval 0 100.
(Thus, density function equal 1/k belongs one peaks,
0 otherwise.) experiment chosen p = 1.
Figure 31 shows relative quality system function k, obtained
10000 randomly generated examples. see results, simultaneous
strategy inferior, due valleys distribution function. optimal strategy
returns schedules processes switch peak, relative quality
schedules decreases number peaks increases.
50

Optimal
Simultaneous

40

Relative quality (in percent)

30

20

10

0

-10

-20

-30

2

3

4

5

6
Number peaks

7

8

9

10

Figure 31: Multimodal distribution: relative quality function number peaks.

7. Experiments: Using Optimal Scheduling Latin Square Problem
test performance algorithm realistic domain, applied Latin
Square problem described Section 2.2. assume given Latin Square
problem two initial configurations, fully deterministic algorithm distribution
function distribution density shown Figure 7.
compare performance schedule produced algorithm performance sequential simultaneous strategies described Section 6. addition,
test schedule runs processes one another, allowing single switch
optimal point (an analogue restart technique two processes). refer
schedule single-point restart schedule.
Note case two initial configurations corresponds case two processes
framework. general, could think set n initial configurations would
correspond n processes. sufficiently large n, restart strategy restart
starts different initial configuration, becomes close optimal.
experiments performed different values N , 10% square precolored. performance profile induced based run 50, 000 instances,
remaining 50, 000 instances used 25, 000 testing pairs. schedules applied
114

fiOptimal Schedules Parallelizing Anytime Algorithms

fixed deadline , corresponds maximal allowed number generated
nodes.
Since results sequential strategy type problems much worse
results strategies sufficiently large values , instead used
simultaneous strategy reference relative quality measure.
30

Optimal
Single-point restart

25

Relative quality (in percent)

20
15
10
5
0
-5
-10
-15

0

5000

10000

15000

20000
25000
30000
Maximal available time

35000

40000

45000

50000

Figure 32: Relative quality function maximal allowed time

Figure 32 shows maximal available time (the x axis) influences quality
schedules (the axis), simultaneous strategy used reference.
small values , single-point restart optimal strategy
25% gain simultaneous strategy, since produce schedules close
sequential one. However, available time increases, benefit parallelization
becomes significant, simultaneous strategy overcomes single-point restart
strategy. relative quality optimal schedule also decreases increases, since
resulting schedule contains switches two problem instances
solved.
Figure 33 illustrates optimal single-point restart schedules relate
simultaneous schedule different size Latin Squares (given = 25, 000). initial gain
strategies 50%. However, problems N = 20 single-point
restart strategy becomes worse simultaneous one. larger sizes probability
solving Latin Square problem time limit 25, 000 steps becomes smaller
smaller, benefit optimal strategy also approaches zero.
115

fiFinkelstein, Markovitch & Rivlin

50

Optimal
Single-point restart

Relative quality (in percent)

40

30

20

10

0

-10

5

10

15

20
Lain Square size

25

30

35

Figure 33: Relative quality function size Latin Square

8. Combining Restart Scheduling Policies
Luby, Sinclair, Zuckerman (1993) showed restart strategy optimal
infinite number identical runs available. number limited, restart
strategy optimal. Sometimes, however, mixed situation. Assume
two initial states, non-deterministic algorithm, linear time cost. one hand,
perform restarts run corresponding one initial states.
hand, switch runs corresponding two initial states. would
optimal policy case?
expected time run based single initial state
1
E(t ) =
F (t )


Z



0

(1 F (t))dt,

(53)

restart point F (t) distribution function. formula obtained
simple summation geometric series coefficient 1F (t ), continuous
form formula given Luby, Sinclair, Zuckerman (1993). Minimization (53)
gives us optimal restart point.
Assume first sequence restarts single initial state process interruptible restart points. Since probability failure successive restarts
(1 F (t ))i , process exponentially distributed. Thus, problem reduced
scheduling two exponentially distributed processes. According analysis Section 6.2, schedules equivalent problems corresponding two initial states
116

fiOptimal Schedules Parallelizing Anytime Algorithms

solvable. Otherwise, optimal policy alternate two processes
restart point.
interesting case allow rescheduling time point. general,
beneficial switch processes non-restart points (otherwise
rescheduling points would chosen restart). rescheduling, however,
beneficial cost associated restarts higher rescheduling cost 9 .
Let us assume restart constant cost C. Similarly (53), write
expected cost policy performing restarts point
E(t ) =

1
F (t )

Z

0



(1 F (t))dt +

1 F (t )
C,
F (t )2

(54)

second term corresponds series
0 + C(1 F (t )) + 2C(1 F (t ))2 + . . .
Let optimal restart points setups without associated costs
respectively. greater due restart cost.
Let us consider following schedule: first process runs , second
process runs , first process runs (with restart) additional ,
second process runs additional . first process restarts runs
forth.
Let us compare expected time schedule time pure restart
policy, first process runs , second process runs ,
first process restarts runs forth.
Similarly (15), expected time first schedule interval [0, 2t ]
written
Z





(1 F (t))dt + (1 F (t ))

Z



(1 F (t))dt+
Z
Z


(1 F (t ))
(1 F (t))dt + (1 F (t ))
(1 F (t))dt.

Esched =

0

0





hand, expected time second schedule interval

Esimple =

Z



(1 F (t))dt + (1 F (t ))
0
Z
(2 F (t ))
(1 F (t))dt.

Z


0

(1 F (t))dt =

0

9. example setup robotic search, returning robot initial position
expensive suspending resuming robot.

117

fiFinkelstein, Markovitch & Rivlin

Esched rewritten
Z
Z

(1 F (t))dt+
(1 F (t))dt + (1 F (t ))
Esched =
0
0
Z
Z
(1 F (t))dt =
(1 F (t))dt (1 F (t ))
(1 F (t ))
0
0
Z
Z



(1 F (t))dt =
(1 F (t))dt + (2 F (t ) F (t ))
F (t )
0
0
Z
Z


(1 F (t))dt + Esimple .
(1 F (t))dt F (t )
F (t )
0

0

Thus, obtain


Esimple Esched = F (t )
F (t )F (t )

1
F (t )

Z

0

Z


0



(1 F (t))dt F (t )



(1 F (t))dt

1
F (t )

Z


0

Z


0

(1 F (t))dt =
!

(1 F (t))dt ,

since provides minimum (53), last expression positive, means
scheduling improves simple restart policy.
Note, claim proposed scheduling policy optimal example
shows pure restart strategy optimal. optimal
combination interleaving restarts global level scheduling local level,
finding combination left future research.

9. Conclusions
work present algorithm optimal scheduling anytime algorithms
shared resources. first introduce formal framework representing analyzing
scheduling strategies. begin analyzing case allowed scheduling operations suspending resuming processes. prove necessary conditions
schedule optimality present algorithm building optimal schedules based
conditions. analyze general case scheduler increase
decrease intensity scheduled processes. prove necessary conditions show
intensity control rarely needed. analyze, theoretically empirically,
behavior scheduling algorithm various distribution types. Finally, present
empirical results applying scheduling algorithm Latin Square problem.
results show optimal strategy indeed outperforms scheduling strategies. lognormal distribution, showed improvement 50%
naive sequential strategy. general, algorithm particularly beneficial heavy-tailed
distributions, even exponential distribution show benefit 35%.
cases, however, simple scheduling strategies yield results similar obtained algorithm. example, optimal schedule uniform distribution
apply one processes switch. probability succeed within given
time limit approaches 1, simple scheduling strategy also becomes close optimal,
118

fiOptimal Schedules Parallelizing Anytime Algorithms

least unimodal distributions strong skew towards zero. hand,
probability success approaches zero, another simple strategy applies
processes simultaneously becomes close optimal.
behavior meets intuition. heavy-tailed distributions, switching
runs promising chance bad trajectory high enough.
correct distributions low probability success. However, probability
bad trajectory high, best strategy switch runs
fast possible, equivalent simultaneous strategy. hand,
distribution skewed right, often sense switch runs,
since new run pay high penalty reaches promising distribution
area. general, user certain particular application falls one
categories above, cost calculating optimal schedule saved.
high complexity computation one potential weaknesses presented
algorithm. complexity represented multiplication three factors: function
minimization, Branch-and-Bound search, solving Equations (18) (19) case
two agents Equation (28) general case. two agents, exponential component Branch-and-Bound search. found, however, practice
branching factor, roughly number roots equations above, rather
small, depth search tree controlled iterative-deepening strategies.
arbitrary number agents, function minimization may also exponential. practice, however, depends behavior minimized function minimization
algorithm.
Since optimal schedule static applied large number problem
instances, computation beneficial even associated high cost. Moreover,
applications (such robotic search) computational cost outweighed
gain obtained single invocation.
previous work related research restart framework (Luby et al.,
1993). important difference algorithm restart policy
ability handle cases number runs limited, different algorithms
involved. one algorithm available number runs infinite,
restart strategy optimal. However, shown Section 8, problems may
benefit combination two approaches.
algorithm assumes availability performance profiles processes.
performance profiles derived analytically using theoretical models processes
empirically previous experience solving similar problems. Online learning
performance profiles, could expand applicability proposed framework,
subject ongoing research.
framework presented used wide range applications. introduction presented three examples. first example describes two alternative learning
algorithms working parallel. behavior algorithms usually exponential,
analysis setup given Section 6.2. second example CSP problem
two alternative initial configurations, analogous Latin Square example
Sections 2.2 7. last example includes crawling processes limited shared
bandwidth. Unlike first two examples, setup falls framework intensity
control described Section 5.
119

fiFinkelstein, Markovitch & Rivlin

Similar schemes may applied elaborate setups:
Scheduling system n anytime algorithms, overall cost system
defined maximal cost components (unlike analysis Section 4,
function differentiable);
Scheduling non-zero process switch costs;
Providing dynamic scheduling algorithms able handle changes environment;
Building effective algorithms case several resources different types, e.g.,
multiprocessor systems.

Appendix A. Formal Proofs
A.1 Proof Lemma 1
claim lemma follows:
system n processes, expression expected cost (6) rewritten
Z
X
n i+n1
X

k
j
(1 Fi (x))dx.
(55)
Eu (1 , . . . , n , . . .) =
))
(1 F#j (k1

k1

k=0 i=1 j=i+1


Proof: Splitting whole integration range [0, ) intervals [t i1
k , tk ] yields
following expression:
Z
n
n Z ti
X
n
X
k
(1 Fj (j ))dt.
(56)
(1 Fj (j ))dt =
Eu (1 , . . . , n ) =
0

k=0 i=1

j=1

ti1
k
j=1

(25), rewrite inner integral
Z ti
n
k
(1 Fj (j )) =
ti1
k
j=1

Z

tik
ti1
k




i1


i1


j=1

j=i+1n


))
(1 Fj (kj )) (1 Fi (t ti1
+ k1
k

(1 F#j (kj ))

Z

tik
ti1
k

n


j=i+1



j
(1 Fj (k1
)) dt =

(57)


+ k1
))dt.
(1 Fi (t ti1
k


Substituting x ti1
+ k1
using (23), obtain
k
Z ti
i1

k
j

(1 F#j (k ))
(1 Fi (t ti1
+ k1
))dt =
k
ti1
k

j=i+1n
i+n1

j=i+1

i+n1

j=i+1

(1

j
F#j (k1
))

(1

j
F#j (k1
))

Z

Z


tik ti1
k +k1


k1

ki

k1

(1 Fi (x))dx =

(1 Fi (x))dx.

120

(58)

fiOptimal Schedules Parallelizing Anytime Algorithms

Combining (56), (57) (58) gives us (55).
Q.E.D.

A.2 Proof Chain Theorem n Processes
chain theorem claim follows:
l1
l1 , computed given previous 2n 2
value m+1
may either
values using formula

1

l )
fl (m
l )
Fl (m

l+n1

j=l+1

=

l1
X

j
(1 F#j (m1
))

i+n1


i=ln+1 j=i+1
#j6=l

l+n1


j
(1 F#j (m
))

j=l+1

Z

j
(1 F#j (m
))


m+1




(59)
(1 F#i (x))dx

Proof: Lemma 1, expression want minimize described equation
Eu (1 , . . . , n , . . .) =

X
n i+n1

X
k=0 i=1 j=i+1

j
(1 F#j (k1
))

Z

ki

k1

(1 Fi (x))dx.

(60)

expression reaches optimal values either
dEu
= 0 j = 1, . . . , n, . . . ,
dj

(61)

border described (26).
Reaching optimal values border corresponds first alternative described
theorem. Let us consider case derivative E u j 0.
l , 0 l n 1. Let us see
variable j may presented mn+l =
l participating in.
summation terms (60)
l may lower bound integral (60). happens k = + 1
1.
= l. corresponding term

S0 =

l+n1

j=l+1



(1

j
F#j (m
))

Z

l
m+1
l


(1 Fl (x))dx,

l+n1

dS0
l
j
=
(1

F
(
))

(1 F#j (m
)).
l
l
dm
j=l+1

l may upper bound integral, happens k =
2.
= l. corresponding term

Sl =

l+n1

j=l+1

(1

j
F#j (m1
))

121

Z

l

l
m1

(1 Fl (x))dx,

fiFinkelstein, Markovitch & Rivlin



l+n1

dSl
j
l
(1 F#j (m1
)).
=
(1

F
(
))

l
l
dm
j=l+1

l may participate product
3. Finally,
i+n1

j=i+1

j
(1 F#j (k1
)).

= 1 . . . l 1, may happen k = + 1 j = l, corresponding
term
Z
i+n1

m+1
j
(1 Fi (x))dx,
(1 F#j (m ))
Si =



j=i+1

derivative

dSi
l
= fl (m
)
l
dm

i+n1


(1

j
F#j (m
))

j=i+1,#j6=l

Z


m+1



(1 Fi (x))dx.

= l + 1 . . . n, k = j = l + n. corresponding term
Si =

i+n1


(1

j=i+1

j
F#j (m1
))

Z




m1

(1 Fi (x))dx,

derivative
dSi
l
= fl (m
)
l
dm

i+n1


j=i+1,#j6=l

(1

j
F#j (m1
))

Z




m1

(1 Fi (x))dx.

l appears integral, possibility l appear
Since = l,

expression, therefore
n
dEu X dSi
=
.
l
l
dm
dm
i=0

right-hand side sum written follows:
n
X
dSi
=
l
dm
i=0

(1
l1
X

l
Fl (m
))

j=l+1

l
fl (m
)

i=1

n
X

i=l+1

l+n1


i+n1


(1

j=i+1,#j6=l
l
fl (m
)

i+n1


j
F#j (m
))

j
(1 F#j (m
))

(1

l+n1


j
(1 F#j (m1
))

+ (1

l
Fl (m
))

Z

(1 Fi (x))dx


m+1



j
F#j (m1
))

j=i+1,#j6=l

122

Z

j=l+1




m1

(1 Fi (x))dx.

(62)

fiOptimal Schedules Parallelizing Anytime Algorithms

However,
n
X

i+n1


j
F#j (m1
))

(1

i=l+1 j=i+1,#j6=l
0
X

i+n1


(1

j
F#j (m
))

i=ln+1 j=i+1,#j6=l

Z




m1

Z

(1 Fi (x))dx =


m+1



(1 Fi (x))dx.

(63)

Substituting (63) (62), obtain
n
X
dSi
=
l
dm
i=0



l
(1 Fl (m
))
l
fl (m
)

l1
X

l+n1

j=l+1

j
(1 F#j (m1
))

i+n1


(1

j
F#j (m
))

i=ln+1 j=i+1,#j6=l

l+n1

j=l+1

Z



j
(1 F#j (m
))


m+1



(1 Fi (x))dx.

(64)

l ) 0, would mean goal reached probability
1 Fl (m
1, scheduling would redundant. Otherwise, expression (64) 0

1

l )
fl (m
l )
Fl (m

l+n1

j=l+1

=

l1
X

(1

j
F#j (m1
))

i+n1


(1

i=ln+1 j=i+1,#j6=l



l+n1

j=l+1

j
F#j (m
))

Z

j
(1 F#j (m
))


m+1




,
(1 F#i (x))dx

equivalent (59).
l1
l+1
= n(m+1)+l1 ),
= n(m1)+l+1 m+1
Equation (59) includes 2n 1 variables ( m1
l1
providing implicit dependency m+1 remaining 2n 2 variables.
Q.E.D.

A.3 Proof Lemma 2
claim lemma follows:
Euler-Lagrange conditions minimization problem (33) yield two strong invariants:
1. processes k1 k2 k1 k2 border described (34),
distribution density functions satisfy
fk2 (k2 )
fk1 (k1 )
=
.
1 Fk1 (k1 )
1 Fk2 (k2 )
123

(65)

fiFinkelstein, Markovitch & Rivlin

2. schedules processes border described (34), either
c = 1 fk (k ) = 0 k.
Proof: Let g(t, 1 , . . . , n , 10 , . . . , n0 ) function integral sign (33):
g(t, 1 , . . . , n , 10 , . . . , n0 )

(1 c) + c

=

n
X

i0

i=1

!

n


(1 Fj (j )).

(66)

j=1

necessary condition Euler-Lagrange claims set functions 1 , . . . , n provides
weak (local) minimum functional
Eu (1 , . . . , n ) =

Z



0

g(t, 1 , . . . , n , 10 , . . . , n0 )dt

functions satisfy system equations form
g0 k

0
g 0 = 0.
dt k

case,
g0 k


= (1 c) + c

n
X

i0

i=1

!

fk (k )

(67)



j6=k

(1 Fj (j )),

n
n
X


0
g 0 = c
(1 Fj (j )) = c
l0 fl (l ) (1 Fj (j )).
dt k
dt
j=1

l=1

(68)

(69)

j6=l

Substituting last expression (67), obtain
g0 1

=

g0 2

= ... =

g0 n

= c

n
X

l0 fl (l )

l=1


j6=l

(1 Fj (j )),

(68) every k1 k2
fk1 (k1 )



j6=k1

(1 Fj (j )) = fk2 (k2 )



j6=k2

(1 Fj (j )).

ignore case one terms 1 F j (j ) 0. Indeed, possible
goal reached process j probability 1, case optimization
needed. Therefore, obtain
fk1 (k1 )(1 Fk2 (k2 )) = fk2 (k2 )(1 Fk1 (k1 )),
equivalent (65).
124

(70)

fiOptimal Schedules Parallelizing Anytime Algorithms

Let us show correctness second invariant. (69) (65), obtain
n

X

0
l0 fl (l ) (1 Fj (j )) =
g 0 = c
dt k
c

l=1
n
X

l=1
n
X

j6=l

l0

n
fl (l )
(1 Fj (j )) =
1 Fl (l )
j=1
n


fk (k )
(1 Fj (j )) =
1 Fk (k )
j=1
l=1
!
n
X

i0 fk (k )
c
(1 Fj (j )).
c

l0

i=1

j6=k

(36) get
g0 k


g0 0 =
dt k
+c

(1 c) + c
n
X
i=1

i0

!

n
X

i0

i=1

fk (k )

(1 c)fk (k )



!

fk (k )



j6=k

j6=k



j6=k

(1 Fj (j ))

(1 Fj (j )) =

(1 Fj (j )) = 0.

Since ignore case (1 Fj (j )) = 0, second invariant correct.
Q.E.D.

A.4 Proof Lemma 3
claim lemma follows:
optimal solution exists, exists optimal solution 1 , . . . , n ,
time resources consumed, i.e.,


n
X

i0 (t) = 1.

i=1

case time cost zero (c 6= 1), equality necessary condition
solution optimality.
Proof: know {i } provide minimum expression (33)
! n
Z
n
X

i0
(1 c) + c
(1 Fj (j ))dt.
0

i=1

j=1

Let us assume time interval [t 0 , t1 ], {i } satisfy lemmas constraints.
However, possible use amount resources effectively. Let us consider
125

fiFinkelstein, Markovitch & Rivlin

linear time warp (t) = + time interval [t 0 , t1 ], satisfying (t0 ) = t0 .
last condition, follows = 0 (1 ). Let t01 point (t) achieves t1 ,
i.e., t01 = t0 + (t1 t0 )/. Let us consider set new objective schedule functions
e (t)
form

t0 ,
(t),
(t + ),
t0 t01 ,

ei (t) =

(t + t1 t01 ), > t01 .
Thus,
ei (t) behaves (t) t0 , (t) time shift t01 , linearly
speeded version (t) interval [t0 , t01 ]. Since (t0 ) = t0 (t01 ) = t1 ,
ei (t)
continuous points t0 t01 .

ei0 (t) equal i0 (t) within interval [t0 , t1 ], i0 (t) outside interval.
contradiction assumption, meet lemma constraints [t 0 , t1 ], thus
take
1
P
> 1,
=
maxt[t0 ,t1 ] ni=1 i0 (t)

leading valid functions
ei0 (t). Using
ei (t) (33), obtain
! n
Z
n
X

0

ei (t)
(1 c) + c
(1 Fj (e
j (t)))dt =
Eu (e
1 , . . . ,
en ) =
0

Z

i=1

t0

(1 c) + c

0

Z

t01

t0

Z

(1 c) + c

i0 (t)

i=1

(1 c) + c



t01

n
X

n
X

!

i0 (t

j=1

n


j=1

(1 Fj (j (t)))dt +

+ )

i=1

n
X

i0 (t

i=1

+ t1

!

n


(1 Fj (j (t + )))dt +

j=1

t01 )

!

n


(1 Fj (j (t + t1 t01 )))dt.

j=1

substituting x = + second term last sum, x = + 1 t01
third term, obtain
! n
Z t0
n
X

Eu (e
1 , . . . ,
en ) =
(1 c) + c
i0 (t)
(1 Fj (j (t)))dt +
0

Z

i=1

t1

t0

Z



t1

1c
+c


n
X

i0 (x)

i=1

(1 c) + c

Eu (1 , . . . , n )

n
X

i0 (x)

i=1

Z

t1

t0

!

j=1

n


(1 Fj (j (x)))dx +

j=1

!

n


j=1

(1 Fj (j (x)))dx =



1
(1 c) 1


Since > 1, last term non-negative, therefore
Eu (e
1 , . . . ,
en ) Eu (1 , . . . , n ),
126


n

(1 Fj (j ))dt.

j=1

fiOptimal Schedules Parallelizing Anytime Algorithms

meaning set {e
} provides solution least quality { }. c 6= 1,
contradicts optimality original schedule, c = 1, new schedule
also optimal.
Q.E.D.

A.5 Proof Theorem 4
claim theorem follows:
Let set functions {i } solution minimization problem (6) constraints (34). Let t0 point hazard functions processes h (i (t))
continuous, let Ak process active t0 (k0 (t0 ) > 0),
process Ai
hi (i (t0 )) < hk (k (t0 )).
(71)
t0 process k consumes resources, i.e. k0 (t0 ) = 1.
Proof: First want prove theorem case two processes,
generalize proof case n processes. Assume 1 (t) 2 (t) provide
optimal solution, point 0 10 (t0 ) > 0
f2 (2 (t0 ))
f1 (1 (t0 ))
>
.
1 F1 (1 (t0 ))
1 F2 (2 (t0 ))

(72)

continuity functions h (t) point t0 , follows exists
neighborhood U (t0 ) t0 , two points t0 , t00 neighborhood h1 (t0 ) >
h2 (t00 ), i.e.,
f1 (1 (t0 ))
f2 (2 (t00 ))
min
>
max
.
(73)
t0 U (t0 ) 1 F1 (1 (t0 ))
t00 U (t0 ) 1 F2 (2 (t00 ))
Let us consider interval [t0 , t1 ] U (t0 ). order make proof readable,
introduce following notation (for proof only):
denote 1 (t) (t). Lemma 3, 2 (t) = (t).
denote (t0 ) 0 (t1 ) 1 .
interval [t0 , t1 ] first process obtains 1 0 resources, second process
obtains (t1 t0 )( 1 0 ) resources. Let us consider special resource distribution
e,
first gives resources first process, second process, keeping
quantity resources :

(t),
t0 ,



0 + 0 , t0 0 + 1 0 ,

e(t) =
1 ,
t0 + 1 0 1



(t),
t1 .

easy see (t) continuous points 0 , t1 , t0 + 1 0 . want
show that, unless first process consumes resources beginning, schedule
produced
e outperforms schedule produced .
127

fiFinkelstein, Markovitch & Rivlin

Let = t0 + 1 0 , corresponds time first process would
consumed resources working maximal intensity. First, want
show interval [t0 , ]
(1 F1 ((t)))(1 F2 (t (t))) (1 F1 (t t0 + 0 ))(1 F2 (t0 0 )).

(74)

(t) = (t t0 + 0 ) (t).

(75)

Let
inequality (74) becomes
(1F1 (tt0 + 0 (t)))(1F2 (t0 0 +(t))) (1F1 (tt0 + 0 ))(1F2 (t0 0 )). (76)
Let us find value x = (t) provides minimum left-hand side (76)
fixed t. Let us denote
G(x) = (1 F1 (t t0 + 0 x))(1 F2 (t0 0 + x)).
Then,
G0 (x) = f1 (t t0 + 0 x))(1 F2 (t0 0 + x)) f2 (t0 0 + x)(1 F1 (t t0 + 0 x)).
Since valid (t) interval [t 0 , t1 ] obtains values 0 1 , (75)
t0 + 0 x [ 0 , 1 ],

t0 0 + x [t0 0 , t1 1 ].

Therefore, exist t0 , t00 [t0 , t1 ], 1 (t0 ) = (t0 ) = t0 + 0 x 2 (t00 ) =
t00 (t00 ) = t0 0 + x. (73) obtain G0 (x) > 0, meaning G(x) monotonically
increases. Besides, (75) x = (t) 0 (since 0 (t) 1), therefore G(x)
obtains minimal value x = 0. Therefore, denote Ran(t) set valid
values (t),
(1 F1 ())(1 F2 (t )) = (1 F1 (t t0 + 0 (t)))(1 F2 (t0 0 + (t)))
min (1 F1 (t t0 + 0 x))(1 F2 (t0 0 + x)) =

xRan(t)

(1 F1 (t t0 + 0 ))(1 F2 (t0 0 )),
strict equality occurs (t) = 0 + 0 . Thus,
(1 F1 ())(1 F2 (t )) (1 F1 (e
))(1 F2 (t
e))

[t0 , ].
Let us show correctness statement interval [t , t1 ],
equivalent inequality
(1 F1 ((t)))(1 F2 (t (t))) (1 F1 ( 1 ))(1 F2 (t 1 )).

(77)

proof similar. Let
(t) = 1 (t).
128

(78)

fiOptimal Schedules Parallelizing Anytime Algorithms

inequality (77) becomes
(1 F1 ( 1 (t)))(1 F2 (t 1 + (t))) (1 F1 ( 1 ))(1 F2 (t 1 )).

(79)

before, find value x = (t) provides minimum left-hand side
(79)
G(x) = (1 F1 ( 1 x))(1 F2 (t 1 + x)).
derivative G(x)
G0 (x) = f1 ( 1 x))(1 F2 (t 1 + x)) f2 (t 1 + x)(1 F1 ( 1 x)),
since valid (t) interval [t 0 , t1 ] obtains values 0 1 , (78)

1 x [ 0 , 1 ],

1 + x [t0 0 , t1 1 ].

Therefore, exist t0 , t00 [t0 , t1 ], 1 (t0 ) = (t0 ) = 1 x 2 (t00 ) =
t00 (t00 ) = 1 + x. (73), G0 (x) > 0, therefore G(x) monotonically increases.
Since x = 1 (t) 0, G(x) G(0). Thus, [t , t1 ],
(1 F1 ())(1 F2 (t )) = (1 F1 ( 1 (t)))(1 F2 (t 1 + (t)))

min (1 F1 ( 1 x))(1 F2 (t 1 + x)) = (1 F1 ( 1 ))(1 F2 (t 1 )),

xRan(t)

strict equality occurs (t) = 1 .
Combining result previous one, obtain
(1 F1 ())(1 F2 (t )) (1 F1 (e
))(1 F2 (t
e))

holds every [t0 , t1 ]. Since
e(t) behaves (t) outside interval, E u () Eu (e
).
Besides, since equality obtained
e, since E u () optimal,
obtain
e, therefore first process take resources interval
[t0 , t1 ].
proof n processes exactly same. Let { } provide optimal solution,
point t0 process k, j 6= k
hk (k (t0 )) > hj (j (t0 )).
continuity functions h (i (t)) point t0 , follows exists
neighborhood U (t0 ) t0 ,
min hk (k (t0 )) > max max hi (i (t00 )).
i6=k t00 U (t0 )

t0 U (t0 )

Let us take process l 6= k, let
y(t) = k (t) + l (t).
129

(80)

fiFinkelstein, Markovitch & Rivlin

repeat proof substituting y(t) instead function
sign:

k (t),



y(t) y(t0 ) + k (t0 ),

ek (t) =
(t ),


k 1
k (t),

y(t) y(t0 ),
y(t0 ) y(t) y(t0 ) + k (t1 ) k (t0 ),
y(t0 ) + k (t1 ) k (t0 ) y(t) y(t1 ),
y(t) y(t1 ).

substitution produces valid schedule due monotonicity y(t). rest
proof remains unchanged.
Q.E.D.

A.6 Proof Theorem 5
claim theorem follows:
active process remain active consume resources long hazard
function monotonically increasing.
Proof: proof contradiction. Let { j } form optimal schedule. Assume
point t1 process Ak suspended, hazard function h k (k (t1 ))
monotonically increasing t1 .
Let us assume first point 2 process Ak becomes active again. Since
consider case making process active single point, exists > 0,
Ak active intervals [t1 , t1 ] [t2 , t2 + ]. Ak stopped
point monotonicity hazard function, therefore, Theorem 4,
intervals Ak active process. consider two alternative scenarios. first
one, allow Ak active additional time starting 1 (i.e., shifting idle
period ), second suspend k earlier.
first scenario, scheduling functions following form:

k (t),
t1 ,




(t
)
+
(t


),
t1 t1 + ,
1
k 1
ka (t) =
(t ) + = k (t1 ) + , t1 + t2 + ,


k
k (t),
t2 + ;

t1 ,

j (t),

j (t1 ),
t1 t1 + ,
ja (t) =

(t

),


1 + t2 + ,

j
j (t),
t2 + .

(81)

(82)

possible see scheduling functions continuous satisfy invariant (39),
makes set suitable candidate optimality.
130

fiOptimal Schedules Parallelizing Anytime Algorithms

Substituting values (6), obtain
Eu (1a , . . . , na )
Z

t1 +

Z

(1 Fj (j (t)))dt+

j=1

t2 +

t1 +

0

Z

0

n
t1

(1 Fk (k (t1 ) + (t t1 )))

t1

Z

=

Z

(1 Fk (k (t1 ) + ))

n
t1

j=1

(1 Fj (j (t)))dt +

t2

(1 Fk (k (t1 ) + ))

t1



j6=k



j6=k



j6=k



j6=k

(1 Fj (j (t1 )))dt+

(1 Fj (j (t )))dt +

(1 Fj (j (t1 )))

Z

(1 Fj (j (t)))dt +

Z



n


(1 Fj (j (t)))dt =

t2 + j=1



(1 Fk (k (t1 ) + x))dx+

0

Z

n




t2 + j=1

(1 Fj (j (t)))dt.

Subtracting Eu (1 , . . . , n ) given (6) Eu (1a , . . . , na ), get
Eu (1 , . . . , n ) Eu (1a , . . . , na ) =
Z t2

[(1 Fk (k (t))) (1 Fk (k (t1 ) + ))]
(1 Fj (j (t)))dt+
t1

Z

j6=k

n
t2 +

t2

(1 Fj (j (t)))dt

j=1



j6=k

(1 Fj (j (t1 )))

Z

(83)



(1 Fk (k (t1 ) + x))dx.

0

Let us consider first term last equation. Since interval [t 1 , t2 ] k (t) = k (t1 ),
interval
(1 Fk (k (t))) (1 Fk (k (t1 ) + )) = (1 Fk (k (t1 ))) (1 Fk (k (t1 ) + )) =
Z
Z
Z
hk (k (t1 ) + x)(1 Fk (k (t1 ) + x))dx.
fk (k (t1 ) + x)dx =
d(1 Fk (k (t1 ) + x)) =

0

0

0

Due monotonicity hk (k ) t1 ,
(1 Fk (k (t))) (1 Fk (k (t1 ) + )) =
Z
Z
hk (k (t1 ) + x)(1 Fk (k (t1 ) + x))dx > hk (k (t1 ))
0

leads
Z t2
t1

[(1 Fk (k (t))) (1 Fk (k (t1 ) + ))]

hk (k (t1 ))

Z


0

(1 Fk (k (t1 ) + x))dx
131

Z

t2



j6=k



t1 j6=k


0

(1 Fk (k (t1 ) + x))dx,

(1 Fj (j (t)))dt >

(1 Fj (j (t)))dt.

(84)

fiFinkelstein, Markovitch & Rivlin

Let us consider second term (83). Since interval [t 2 , t2 + ] Ak
active, interval

j (t2 ),
j 6= k,
j (t) =
k (t1 ) + (t t2 ), j = k.
Thus,
Z

n
t2 +

t2

j=1

(1 Fj (j (t)))dt =



j6=k

Z

(1 Fj (j (t2 )))



(1 Fk (k (t1 ) + x))dx.

0

(85)

Substituting (84) (85) (83), obtain
Z

Eu (1a , . . . , na )



(1 Fk (k (t1 ) + x))dx
Eu (1 , . . . , n )
>
0


(86)
Z t2


hk (k (t1 ))

(1 Fj (j (t)))dt +
(1 Fj (j (t2 )))
(1 Fj (j (t1 ))) .
t1 j6=k

j6=k

j6=k

proof second scenario, k suspended , similar.
scenario, scheduling functions k (t) j (t) j 6= k represented follows:

k (t),
t1 ,



k (t1 ) = k (t1 ) ,
t1 t2 ,
ki (t) =
(87)

(t

)
+
(t

(t

))
=

(t
)
+
(t


),
t2 2 ,

2
2
k 1

k 1
k (t),
t2 ;

j (t),
t1 ,




(t
+
),

j
1 t2 ,
(88)
ji (t) =
(t ),
t2 2 ,


j 2
j (t),
t2 .
before, scheduling functions continuous satisfy invariant (39).
Substituting (6), obtain
Eu (1i , . . . , ni )
Z

t2

t1

Z

0

Z

0

n
t1

(1 Fj (j (t)))dt+

j=1

(1 Fk (k (t1 ) ))

t2

t2

Z

=

Z



j6=k

(1 Fk (k (t1 ) + (t t2 )))

n
t1

(1 Fj (j (t)))dt +

j=1

t2

t1

(1 Fj (j (t + )))dt+

(1 Fk (k (t1 ) ))



j6=k



j6=k



j6=k

(1 Fj (j (t2 )))dt +

(1 Fj (j (t2 )))

(1 Fj (j (t)))dt +
132

Z

Z

Z


0



n


t2

j=1

(1 Fj (j (t)))dt =

(1 Fk (k (t1 ) x))dx+
n


(1 Fj (j (t)))dt.

t2 + j=1

fiOptimal Schedules Parallelizing Anytime Algorithms

Subtracting Eu (1 , . . . , n ) given (6) Eu (1i , . . . , ni ), get
Eu (1 , . . . , n ) Eu (1i , . . . , ni ) =
Z t2

[(1 Fk (k (t))) (1 Fk (k (t1 ) ))]
(1 Fj (j (t)))dt+
t1

Z

j6=k

t1

n


t1 j=1

(1 Fj (j (t)))dt



j6=k

(1 Fj (j (t2 )))

Z

(89)



0

(1 Fk (k (t1 ) x))dx.

first scenario, interval [t 1 , t2 ]
(1 Fk (k (t))) (1 Fk (k (t1 ) )) = (1 Fk (k (t1 ))) (1 Fk (k (t1 ) )) =
Z 0
Z 0
fk (k (t1 ) + x)dx =
d(1 Fk (k (t1 ) + x)) =




Z



0

fk (k (t1 ) x)dx =

Z





hk (k (t1 ) x)(1 Fk (k (t1 ) x))dx.

0

Due monotonicity hk (k ) t1 ,
(1 Fk (k (t))) (1 Fk (k (t1 ) )) =
Z
Z

hk (k (t1 ) x)(1 Fk (k (t1 ) x))dx > hk (k (t1 ))
0

leads
Z t2
t1

[(1 Fk (k (t))) (1 Fk (k (t1 ) ))]

hk (k (t1 ))

Z


0

(1 Fk (k (t1 ) x))dx

Z

t2



j6=k



t1 j6=k


0

(1 Fk (k (t1 ) x))dx,

(1 Fj (j (t)))dt >
(90)
(1 Fj (j (t)))dt.

transformations second term (89) also similar previous scenario.
Since interval [t1 , t1 ] Ak active, interval

j (t1 ),
j 6= k,
j (t) =
k (t1 ) (t1 t), j = k.
Thus,
Z

t1

n


t1 j=1

(1 Fj (j (t)))dt =



j6=k

(1 Fj (j (t1 )))

Z


0

(1 Fk (k (t1 ) x))dx.

(91)

Substituting (90) (91) (89), obtain
Z


Eu (1 , . . . , n ) Eu (1 , . . . , n ) >
(1 Fk (k (t1 ) x))dx
0


Z t2


(1 Fj (j (t1 ))) .
(1 Fj (j (t2 )))
(1 Fj (j (t)))dt +
hk (k (t1 ))
t1 j6=k

j6=k

j6=k

(92)

133

fiFinkelstein, Markovitch & Rivlin

(86) (92),
sign(Eu (1 , . . . , n ) Eu (1a , . . . , na )) = sign(Eu (1 , . . . , n ) Eu (1i , . . . , ni )),

(93)

therefore one scenarios leads better schedule, contradicts optimality original one.
proof case control return k exactly
omitted here. Informally, viewed replacing 2 formulas
above, results same. results.
Q.E.D.

A.7 Proof Theorem 6
claim theorem follows:
time cost taken account (c = 1), model shared resources intensity control settings equivalent model independent processes
suspend-resume control settings. Namely, given suspend-resume solution model
independent processes, may reconstruct intensity-based solution
cost model shared resources vice versa.

Proof: Let Eshared
optimal value framework shared resources,

Eindependent optimal value framework independent processes. Since
c = 1, two problems minimize expression
! n
Z X
n

Eu (1 , . . . , n ) =
i0
(1 Fj (j ))dt min,
(94)
0

i=1

j=1

set {i } satisfying resource sharing constraints automatically satisfies
process independence constraints, obtain


Eindependent
Eshared
.

Let us prove


.
Eshared
Eindependent

Assume set functions 1 , 2 , . . . , n optimal solution problem
independent processes, i.e.,

Eu (1 , . . . , n ) = Eindependent
.

want construct set functions {ei } satisfying resource sharing constrains,

Eu (f
1 , . . . ,
fn ) = Eu (1 , . . . , n ).
Let us consider set discontinuity points i0

= {t|i : i0 (t ) 6= i0 (t + )}.
134

fiOptimal Schedules Parallelizing Anytime Algorithms

model set countable, write sorted sequence 0 = 0 < t1 <
. . . < tk < . . .. expected schedule cost case form
Eu (1 , . . . , n ) =


X

Euj (1 , . . . , n ),

j=0


Euj (1 , . . . , n ) =

Z

n
X

tj+1
tj

i=1

i0

!

n

l=1

(1 Fl (l ))dt.

want construct functions ei incrementally. time interval [t j , tj+1 ]
define corresponding point tej set functions ei ,
! n
Z tg
n
j+1

X
g
(1 Fl (el ))dt = Eu (1 , . . . n ).
el 0
E
1 , . . . ,
fn ) =
u (f
j

j

tej

l=1

l=1

Let us denote ij = (tj ) f
ei (tj ). beginning, f
ij =
i0 = 0 i,
0 < j,
0
defined

j
e
(t)
defined
interval
te0 = 0. Assume tf

j
tej ej [tej , tg
[f
tj 0 , t]
j+1 ].
j 0 +1 ]. Let us show
Pdefine
n
0
definition tj , k = l=1 l (t) constant [tj , tj+1 ]. Since {i } satisfy
suspend-resume constraints, exactly k 1 processes active interval,
full intensity. Without loss generality, active processes 1 , A2 , . . . , Ak ,
Z tj+1
n
Euj (1 , . . . , n ) = k
(1 Fl (l ))dt =
tj

k
k

n


(1 Fl (lj ))

l=k+1
n


(1 Fl (lj ))

l=k+1

Z

Z

tj+1

tj

0

l=1
k

l=1

(1 Fl (t tj + lj ))dt =

n
tj+1 tj
l=1

(1 Fl (x + lj ))dx.

Let tg
ei (t) segment [tej , tg
j+1 = tej + k(tj+1 tj ), let us define
j+1 ] follows:

0
(t tej )/k + f
ij , > 0 [tj , tj+1 ]
ei (t) =
(95)
f
otherwise.
ij ,

case, segment

n
X
l=1

ei 0 (t) = 1,

means ei satisfy resource sharing constraints. definition,
tg
j+1 tej = k(tj+1 tj ),

therefore processes active [t j , tj+1 ] obtain

^
f
i,j+1
ij =

tg
j+1 tej
= tj+1 tj = i,j+1 ij .
k
135

(96)

fiFinkelstein, Markovitch & Rivlin

processes idle [tj , tj+1 ] equality holds well:

^
f
i,j+1
ij = 0 = i,j+1 ij ,

since ei (t) = 0 obtain invariant

f
ij = ij .

(97)

average cost new schedules may represented
! n
Z tg
n
j+1
X

g
el 0
E
1 , . . . ,
fn ) =
(1 Fl (el ))dt =
u (f
j

n


l=k+1

(1 Fl (f
lj ))

Z

tej

l=1
k
g
tj+1

tej

l=1

l=1

(1 Fl ((t tej )/k + f
lj ))dt.

Substituting x = (t tej )/k using (95), (96) (97), obtain
g
E
1 , . . . ,
fn ) = k
uj (f
k

n


n


l=k+1

(1 Fl (lj ))dt

l=k+1

Euj (1 , . . . , n ).

(1 Fl (f
lj ))

Z

0

k
tj+1 tj
l=1

Z

0

k
(tg
j+1 tej )/k
l=1

(1 Fl (x + f
lj ))dx =

(1 Fl (x + lj ))dx =

last equation, immediately follows
Eu (f
1 , . . . ,
fn ) =


X
j=0

g
E
1 , . . . ,
fn ) =
uj (f


X

Euj (1 , . . . , n ) = Eu (1 , . . . , n ),

j=0

completes proof.
Q.E.D.

References
Boddy, M., & Dean, T. (1994). Decision-theoretic deliberation scheduling problem
solving time-constrained environments. Artificial Intelligence, 67 (2), 245286.
Clearwater, S. H., Hogg, T., & Huberman, B. A. (1992). Cooperative problem solving.
Huberman, B. (Ed.), Computation: Micro Macro View, pp. 3370. World
Scientific, Singapore.
Dean, T., & Boddy, M. (1988). analysis time-dependent planning. Proceedings
Seventh National Conference Artificial Intelligence (AAAI-88), pp. 4954,
Saint Paul, Minnesota, USA. AAAI Press/MIT Press.
Finkelstein, L., & Markovitch, S. (2001). Optimal schedules monitoring anytime algorithms. Artificial Intelligence, 126, 63108.
136

fiOptimal Schedules Parallelizing Anytime Algorithms

Finkelstein, L., Markovitch, S., & Rivlin, E. (2002). Optimal schedules parallelizing
anytime algorithms: case independent processes. Proceedings Eighteenth National Conference Artificial Intelligence, pp. 719724, Edmonton, Alberta, Canada.
Gomes, C. P., & Selman, B. (1997). Algorithm portfolio design: Theory vs. practice.
Proceedings UAI-97, pp. 190197, San Francisco. Morgan Kaufmann.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search randomization. Proceedings 15th National Conference Artificial Intelligence
(AAAI-98), pp. 431437, Menlo Park. AAAI Press.
Horvitz, E. (1987). Reasoning beliefs actions computational resource
constraints. Proceedings Third Workshop Uncertainty Artificial Intelligence, pp. 429444, Seattle, Washington.
Huberman, B. A., Lukose, R. M., & Hogg, T. (1997). economic approach hard
computational problems. Science, 275, 5154.
Janakiram, V. K., Agrawal, D. P., & Mehrotra, R. (1988). randomized parallel backtracking algorithm. IEEE Transactions Computers, 37 (12), 16651676.
Knight, K. (1993). many reactive agents better deliberative ones.
Proceedings Thirteenth International Joint Conference Artificial Intelligence,
pp. 432437, Chambery, France. Morgan Kaufmann.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.
Kumar, V., & Rao, V. N. (1987). Parallel depth-first search multiprocessors. part II:
Analysis. International Journal Parallel Programming, 16 (6), 501519.
Luby, M., & Ertel, W. (1994). Optimal parallelization Las Vegas algorithms. Proceedings Annual Symposium Theoretical Aspects Computer Science
(STACS 94), pp. 463474, Berlin, Germany. Springer.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup Las Vegas algorithms.
Information Processing Letters, 47, 173180.
Rao, V. N., & Kumar, V. (1987). Parallel depth-first search multiprocessors. part I:
Implementation. International Journal Parallel Programming, 16 (6), 479499.
Rao, V. N., & Kumar, V. (1993). efficiency parallel backtracking. IEEE Transactions Parallel Distributed Systems, 4 (4), 427437.
Russell, S., & Wefald, E. (1991). Right Thing: Studies Limited Rationality.
MIT Press, Cambridge, Massachusetts.
Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Proceedings
Twelfth National Joint Conference Artificial Intelligence (IJCAI-91), pp. 212217,
Sydney. Morgan Kaufmann.
Simon, H. A. (1982). Models Bounded Rationality. MIT Press.
Simon, H. A. (1955). behavioral model rational choice. Quarterly Journal Economics,
69, 99118.
137

fiFinkelstein, Markovitch & Rivlin

Yokoo, M., & Kitamura, Y. (1996). Multiagent real-time-A* selection: Introducing
competition cooperative search. Proceedings Second International Conference Multiagent Systems (ICMAS-96), pp. 409416.
Zilberstein, S. (1993). Operational Rationality Compilation Anytime Algorithms.
Ph.D. thesis, Computer Science Division, University California, Berkeley.

138

fi
ff fi

fi fi

# $ % & ' (' ) * +
4


!
"

, + + & - (' % * $ . / 0 ($ ' * 12 3

5 + - $ 16 7 % 8 $ - 4 9 9 & + % 0 :

; < = >= ? @ BCDE C =

F G H J H K L N P Q OH ORS

UVW X Z[ \ ]^Z_ ]Z ` ZW a[YV Z_
b Z_ c dX [^U_ e_ ^f Z[g ^Yh
b ZZ[c\i Zf aj k g [a Zl mn p q
r < > u> = = > = < vw x

J Nz { L G F | RM N ON|H K G } O{ } Q

~ ]X lYh U k _ X g Y[^al _ ^_ ZZ[^_ a_ a_ ZV Z_
Z]i _ ^U_
a^ p p pj k g [a Zl
















































fi fifi










fiGHIJH K

{ K K { K z S|









































fffi


fffi






















fffi










@ C E > > = w < =
















































!" #$ %&'() *+,-. '/$012314 5$ 612/7489 4(/':&4(5' 3&9 14 &4 ;1<<) =8( ;1 ;5<< $/( $11> (:54 '&?&=5<5(@"
AB





fi{ H G K RK P |





G } RK H| { RM R{ K |













ff fi













ff ff









































> = w < =

ff ff










ff ff









w C ! > C E > " # $ %
% & ' (

) B < * C * v w +
#
0 (



uB C = w < =


.

& '

%



,- = + w < = " ./ # ff . ff ff 0 1 1 & '
3 & ' & ' & ' & '

2





4 5







45



























45

45







6


7 8 9 :



45






A;

fiGHIJH K



.

{ K K { K z S|











.



.















.





















& '







7

'




45





%
# ff ff ff
#
# # ff
ff # ff ff ff # # ff
fi
2

2 &7





#

@ - E w < = < E v> w

C=

w E C v w






















fi








'









4 5
7 1 & ' 4 5






45












45



#
2 &7











&





#





#

fi G } RK H| { RM R{ K |

{ H G K RK P |

#










45



45






































7


.











#

45

.
















# ff ff .
# ff ff .



#
#















&







# ff .0




0 # ff ff .0







6







4 5
&' & &





&


ff ff
ff ff






ff

ff






.

.







fi





.











u > < B > E & &' & & & %
& ' 7 2 && 2 &' &
1 7 7 1 (





" /29 &<<@) ;1 ;5<< &4489 1 (:&( (:121 54 & '/$4(&$( !) 48': (:&( 5% (:1 $89=12 /% &'(5/$4 &0&5<&=<1 (/
&31$( 54 ) (:1$ 5( 54 '/9 9 /$<@ $/;$ (:&( &$@ &31$( '&$ :&01 & 9 /4( /% !" # $ ! % & ' (() &'(5/$4"
* (:12 ;&@4 /% %/29 &<565$3 (:54 '/$'1?( &21 (21&(1> 459 5<&2<@"
A+

fiGHIJH K

{ K K { K z S|



< B < vvC B & &' &
7 2 &&
(

) B< <D "

&

&





% &&













45


















4 5
.
ff ff ff
4 5
















4 5













fffi


fffi




































Cs> " < E E <=

<B

>B














< > B < =w C + w < = @









fffi


fffi







4 5

























fi{ H G K RK P |

G } RK H| { RM R{ K |





fffi


fffi













C > " + < E E < = v



= < = < B

C + w < = > w x > < C vv C ! > =w @

> B = !

< > B w > C ! > =w



C=



= < v>

!>

<D wt>





































ff


















fffi














fffi



ff







ff





C > " ) > B D> + w





r < = w<B =! @




















fiGHIJH K

{ K K { K z S|




C > " = < = C + w < =



s>w x>s @











0

0
fffi
0
ff












































0









































0












ff













ff







ff






fffi






ff


fffi



0




ff

ff







ff


ff














fi{ H G K RK P |

G } RK H| { RM R{ K |










ff


ff
























ff




ff
























fi
fi







fi

Cs> "


CB>

< v = < E C v * < - =





< = C +w < =s>w x>s @










#











#

#













































fi
' fi
fi






C > " ) < v = < E C v * < - =






< = C+w < = s>w x>s

-=

= < = E

= ! w E > @







fffi






















fffi





ff








.
















fiGHIJH K

{ K K { K z S|






























0










0



ff






ff

ff
0



























45








fffi























































B

fi G } RK H| { RM R{ K |

{ H G K RK P |






























































'











































5

5





45

5
4 5










fffi




fffi








45

# ff ff # ff
fi



ff ff










#

fi # ff







./ # ff ff .
# .






#

./ # ff ff



./







BA

fiGHIJH K










fffi



{ K K { K z S|





45







0

0








fffi

















fi

fi # ff









fi

ff





? = w C v x > "





ff ff

4 5 0 # ff # ff ff # ff

# ff ff # ff ff ff
# fi

fi # ff fi # ./ # ff ff #

ff ff ./ # ff ff #





% 7 %



7



%




;>

>Cw "
<E -w>

+ w "
%
C = - C w > "
C=



* s> B>

#










# ff









#

#

















ff

ff




ff '




%



u t><B>E

1





1 8 9 : % (


9 & % 7 1' ff ff
& 7 2 1& 1' 7 & 1'
fi . ff ff ( %



(




fffi






BB

fi{ H G K RK P |

G } RK H| { RM R{ K |

1 1 & ' &

fffi & % &&
fi . ff ff % 7 1 &' & ff ff ff





fffi




(











4 5









7



45














45




















:













&







3 &

&&














7 & 8












:



















8 7&






8








:
&
ff 8
fi




8








(

(








8



: (







&(





(





8










: (

: (

8




( ( (




: ( ( 7 & :
' ff :





B;

fi
