Journal Artificial Intelligence Research 16 (2002) 167-207

Submitted 3/01; published 3/02

Learning Geometrically-Constrained Hidden Markov Models
Robot Navigation: Bridging Topological-Geometrical Gap
Hagit Shatkay

hagit.shatkay@celera.com

Informatics Research Group,
Celera Genomics, Rockville, MD 20850

Leslie Pack Kaelbling

Artificial Intelligence Laboratory
Massachusetts Institute Technology, Cambridge, 02139

lpk@ai.mit.edu

come place streets marked.
windows lighted mostly they're darked.
place could sprain elbow chin!
dare stay out? dare go in?...
go in, turn left right...
right-and-three-quarters? or, maybe, quite?...
Simple it's not, I'm afraid find,
mind-maker-upper make mind.

Oh, Places You'll Go, Dr. Seuss.

Abstract
Hidden Markov models (hmms) partially observable Markov decision processes
(pomdps) provide useful tools modeling dynamical systems. particularly
useful representing topology environments road networks oce
buildings, typical robot navigation planning. work presented
describes formal framework incorporating readily available odometric information geometrical constraints models algorithm learns
them. taking advantage information, learning hmms/pomdps made
generate better solutions require fewer iterations, robust face
data reduction. Experimental results, obtained simulated real robot
data, demonstrate effectiveness approach.

1 Introduction

work concerned robots need perform tasks structured environments.
robot moving environment suffers two main limitations: noisy sensors prevent
confidently knowing is, noisy effectors prevent knowing
certainty actions take it. concentrate structured environments,
turn characterized two main properties: environments consist vast uneventful uninteresting areas, interspersed relatively interesting positions
situations. Consider instance robot delivering bagel oce building. interesting
situations doors intersections building hallways, well various
c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiShatkay & Kaelbling

positions bagel might respect robot's arm (e.g., robot holding
bagel, puts down, etc.) aspects environment, desk positions
oces, inconsequential bagel delivery task.
natural way represent combination environment robot's interactions
it, probabilistic automaton, states represent interesting situations,
edges states represent actions leading one situation another. Probability
distributions transitions possible observations robot may perceive
situation model robot's noisy effectors sensors, respectively.
models formally known pomdp (partially observable Markov decision process) models, proven useful robot planning acting inherent world uncertainty (Simmons & Koenig, 1995; Nourbakhsh, Powers, & Birchfield, 1995; Cassandra, Kaelbling, & Kurien, 1996).
Despite much work using models, task learning directly automatically
data widely addressed. Research concerning immediate topic date
consists mostly work done Simmons Koenig (1996b). assumption underlying
work human provides rather accurate topological model states
connections, exact probability distributions learned top model,
using version Baum-Welch algorithm (Rabiner, 1989). Another interesting approach
acquisition topological models Thrun Bucken (1996a,1996b; Thrun, 1999),
focused extracting deterministic topological maps previously acquired geometricalgrid-based maps, latter learned directly data. discussion
related research geometrical topological approaches, probabilistic
deterministic versions, given next section.
work reported first successful attempt aware learn purely probabilistictopological models, directly completely recorded data, without using previous humanprovided grid-based models. based using weak geometric information, recorded
robot, help learn topology environment, represent probabilistic
model. Therefore, directly bridges historically perceived gap topological
geometrical information, addresses claim presented Thrun's work (1999)
main shortcoming topological approach failure utilize inherent geometry
learnt environment.
robots equipped wheel encoders enable odometer record change
robot's position moves environment. data typically noisy
inaccurate. oors environment rarely smooth, wheels robot
always aligned neither motors, mechanics imperfect, resulting slippage
drift. effects accumulate, mark initial position robot,
try estimate current position based summing long sequence odometric recordings,
resulting estimate incorrect. is, raw recorded odometric information
effective tool, itself, determining absolute location robot
environment.
approach aimed determining absolute locations, idea underlying
weak odometric information, despite noise inaccuracy, still provides geometrical cues
help distinguish different states, well identify revisitation
state. Hence, information enhances ability learn topological models. However,
168

fiLearning Geometrically-Constrained HMMs

use geometrical information requires careful treatment geometrical constraints
directional data. demonstrate existing models algorithms extended
take advantage noisy odometric data geometrical constraints. geometrical
information directly incorporated probabilistic topological framework, producing
significant improvement standard Baum-Welch algorithm, without need humanprovided model.
rest paper organized follows: Section 2 provides survey previous work
area learning maps robot navigation, brie refers earlier work learning
automata; Section 3 presents formal framework work; Section 4 presents main
aspects iterative learning algorithm, Section 5 describes strategies selecting
initial point iterative process begins; Section 6 presents experimental results
obtained simulated real robot data traditionally hard-to-learn environments.
experiments demonstrate algorithm indeed converges better models fewer
iterations standard Baum-Welch method, robust face data reduction.

2 Approaches Learning Maps Models

work presented lies intersection theoretical area learning computational models|in particular, learning automata data sequences|and applied area
map acquisition robot navigation. concentrate surveying work latter
area, pointing distinction approach predecessors. brie review
results automata computational learning theory. comprehensive review
theoretical results given Shatkay (1999).

2.1 Modeling Environments Robot Navigation

context maps models robot navigation, distinction usually made two
principal kinds maps: geometric topological. Geometric maps describe environment
collection objects occupied positions space, geometric relationships among
them. topological framework less concerned geometrical positions, models
world collection states connectivity, is, states reachable
states actions lead one state next.
draw additional distinction, world-centric1 maps provide \objective"
description environment independent agent using map, robot-centric models
capture interaction particular \subjective" agent environment.
learning map, agent needs take account noisy sensors actuators try
obtain objectively correct map agents could use well. Similarly, agents
using map need compensate limitations order assess position
according map. learning model captures interaction, agent acquiring
model one also using it. Hence, noisy sensors actuators specific agent
ected model. different model likely needed different agents.
related work described below, especially within geometrical framework, centered
around learning objective maps world rather agent-specific models. shall point
survey work concerned latter kind models.
work focuses acquiring purely topological models, less concerned learning
geometrical relationships locations objects, objective maps, although geometrical
1. thank Sebastian Thrun terminology.

169

fiShatkay & Kaelbling

relationships serve aid acquisition process. concept state used
topological framework general concept geometrical location, since state
include information battery level, arm position etc. information,
great importance planning, non-geometrical nature therefore cannot readily
captured purely geometrical framework. following sections provide survey work
done within geometrical framework within topological framework, well
combinations two approaches.

2.2 Geometric Maps

Geometric maps provide description environment terms objects placed
positions. example, grid-based maps instance geometric approach.
grid-based map, environment modeled grid (an array), position
grid either vacant occupied object (binary values placed array).
approach refined ect uncertainty world, grid cells
contain occupancy probabilities rather binary values. lot work done
learning grid-based maps robot navigation use sonar readings
interpretation, Moravec Elfes others (Moravec & Elfes, 1985; Moravec, 1988; Elfes,
1989; Asada, 1991).
underlying assumption learning maps robot tell (or find out)
grid obtains sonar reading indicating object, therefore
place object correctly grid. similar localization assumption, requiring robot
identify geometrical location, underlies geometric mapping techniques Leonard
et al. (1991), Smith et al. (1991), Thrun et al. (1998b) Dissanayake et al. (2001), even
explicit grid part model. Explicit localization hard satisfy.
Leonard et al. (1991) Smith et al. (1991) address issue use geometrical
beacons estimate location robot. known Kalman filter method,
Gaussian probability distribution used model robot's possible current location, based
observations collected current point, (without allowing refinement previous
position estimates based later observations). Research area recently extended
two directions: Leonard Feder (2000) partition task learning one large map
learning multiple smaller map-sections, thus addressing issue computational eciency.
Dissanayake et al. (2001) conduct theoretical study approach show convergence
properties. latter may lead computational eciency identifying cases
steady-state solution readily obtained, accordingly bounding number steps required
algorithms reach useful solution cases.
Work Thrun et al. (1998a) uses similar probabilistic approach obtaining grid-based maps.
work refined (Thrun et al., 1998b) first learn location significant landmarks
environment fill details complete geometrical grid, based laser range
scans. latter work extends approach Smith et al. , using observations obtained
location visited, order derive probability distribution
possible locations. achieve this, authors use forward-backward procedure similar
one used Baum-Welch algorithm (Rabiner, 1989), order determine possible
locations observed data. approach resembles use forwardbackward estimation procedure, probabilistic basis, aiming obtaining maximum
likelihood map environment. still significantly differs initial
assumptions final results. data assumed provided learner includes
170

fiLearning Geometrically-Constrained HMMs

motion model perceptual model robot. consist transition
observation probabilities within grid. components learnt algorithm,
although grid context coarser-grained, topological framework. end result
algorithm probabilistic grid-based map, probabilistic topological model,
explained next section.
addition concerned locations, rather richer notion state,
fundamental drawback geometrical maps fine granularity high accuracy. Geometrical maps, particularly grid-based ones, tend give accurate detailed picture
environment. cases necessary robot know exact location terms
metric coordinates, metric maps indeed best choice. However, many planning tasks
require fine granularity accurate measurements, better facilitated
abstract representation world. example, robot needs deliver bagel
oce oce b, needs map depicting relative location respect
b, passageways two oces, perhaps landmarks help orient
gets lost. reasonably well-operating low-level obstacle avoidance mechanism
help bypass ower pots chairs might encounter way, objects
need part environment map. driver traveling cities needs
know neither longitude latitude coordinates globe, location specific
houses along way, robot need know exact location within building
exact location various items environment, order get one point
another. Hence, effort obtaining detailed maps usually justified. addition
maps large, makes planning|even though planning polynomial
size map|inecient.

2.3 Topological Maps Models

alternative detailed geometric maps abstract topological maps.
maps specify topology important landmarks situations (states), routes transitions (arcs) them. concerned less physical location landmarks,
topological relationships situations. Typically, less complex
support much ecient planning metric maps. Topological maps built lowerlevel abstractions allow robot move along arcs (perhaps wall- road-following),
recognize properties locations, distinguish significant locations states;
exible allowing general notion state, possibly including information
non-geometrical aspects robot's situation.
two typical strategies deriving topological maps: one learn topological
map directly; first learn geometric map, derive topological model
process analysis.
nice example second approach provided Thrun Bucken (1996a, 1996b; Thrun,
1999), use occupancy-grid techniques build initial map. strategy appropriate
primary cues decomposition abstraction map geometric. However,
many cases, nodes topological map defined terms sensory data (e.g.,
labels door whether robot holding bagel). Learning geometric map first
also relies odometric abilities robot; weak space large,
dicult derive consistent map.

171

fiShatkay & Kaelbling

contrast, work concentrates learning topological model directly, assuming abstraction robot's perception action abilities already done. abstractions
manually encoded lower level robot navigational software, described
Section 6. Work Pierce Kuipers (1997) discusses automatic method extracting
abstract states features raw perceptual information.
Kuipers Byun (1991) provide strategy learning deterministic topological maps. works
well domains noise robot's perception action abstracted
away, learning single visits nodes traversals arcs. strong underlying assumption
strategies, building map, current state reliably identified
based local information, based distance traversed previous well-identified
state. methods unable handle situations long sequences actions
observations necessary disambiguate robot's state.
Mataric (1990) provides alternative approach learning deterministic topological maps,
represented distributed graphs. learning process relies assumption
current state distinguished states based local information includes
compass sonar readings. Uncertainty modeled probability distributions.
Instead, matching current readings already existing states required exact,
thresholds tolerated error set empirically. Another difference work presented
here, learn complete probabilistic topology environment, Mataric's
work overall topology graph assumed advance linear list, additional
edges added learning process. probability distribution associated
edges, mechanism choosing edge take determined part goal seeking
process, part model itself.
Engelson McDermott (1992) learn \diktiometric" maps (topological maps metric relations nodes) experience. uncertainty model use interval-based rather
probabilistic, learned representation deterministic. Ad hoc routines handle problems resulting failures uncertainty representation.
prefer learn combined model world robot's interaction world;
allows robust planning takes account likelihood error sensing action.
work closely related Koenig Simmons (1996b, 1996a), learn pomdp
models (stochastic topological models) robot hallway environment. also recognize
diculty learning good model without initial information; solve problem
using human-provided topological map, together constraints structure
model. modified version Baum-Welch algorithm learns parameters
model. also developed incremental version Baum-Welch used on-line.
models contain weak metric information, representing hallways chains one-meter
segments allowing learning algorithm select probable chain length.
method effective, results large models size proportional hallways' length,
strongly depends quality human-provided initial model.

2.4 Learning Automata Data

Informally speaking, automaton consists set states set transitions lead
one state another. context work, automaton states correspond
states modeled environments, transitions, state changes due actions
performed environment. transition automaton tagged symbol
172

fiLearning Geometrically-Constrained HMMs

input alphabet, , corresponding action input system caused state
transition. Classical automata theory (e.g., Hopcroft & Ullman, 1979) distinguishes
deterministic non-deterministic automata. If, alphabet symbol ff, single
edge tagged it, going state, automaton deterministic. Otherwise,
transition states uniquely determined input symbol automaton
non-deterministic. augment transition edge non-deterministic automaton
probability taking given certain input, ff, resulting automaton called probabilistic.
basic problem learning finite deterministic automata given data roughly
described follows: Given set positive set negative example strings,
respectively, alphabet , fixed number states k, construct minimal deterministic
finite automaton k states accepts accept . problem
shown np-complete (Gold, 1978). Despite hardness, positive results
shown possible various special settings. Angluin (1987) showed oracle
answer membership queries provide counterexamples conjectures automaton,
polynomial time learning algorithm positive negative examples. Rivest
Schapire (1987, 1989), provide several effective methods, various settings, learn
deterministic automata correct high probability. work deals
learning noise-free data, Basye, Dean Kaelbling (1995) presented several algorithms
that, high probability, learn input-output deterministic automata, data observed
learner corrupted various forms noise.
cases, learned automaton deterministic rather probabilistic. basic
learning problem probabilistic context find automaton assigns
distribution true one data sequences, using training data , generated
true automaton. Another form learning problem finding probabilistic
automaton assigns maximum likelihood training data ; is, automaton
maximizes Pr(S j).
Abe Warmuth (1992) show finding probabilistic automaton 2 states, even
small error respect true model allowed probability (the probably
approximately correct, PAC, learning model), cannot done polynomial time polynomial number examples, unless np = rp. work arises broadly accepted
conjecture, yet proven, learning hidden Markov Models hard even
pac sense. two ways address hardness: one restrict class
probabilistic models learned, learn unrestricted hidden Markov models
good practical results pac guarantees quality result.
Work Ron et al. (1994, 1995, 1998) pursues first approach, learning restricted classes
automata, namely, acyclic probabilistic finite automata, probabilistic finite sux automata.
classes useful various applications related natural language processing,
learned polynomial time within pac framework.
second approach, one predominantly taken work, learn model
member complete unrestricted class hidden Markov models. weak guarantees
exist goodness model, learning procedure may directed obtain
practically good results. approach based guessing automaton (model), using
iterative procedure make automaton fit better training data. One algorithm
commonly used purpose Baum-Welch algorithm (Baum, Petrie, Soules, & Weiss,
1970), presented detail Rabiner (1989). iterative updates model
173

fiShatkay & Kaelbling

based gathering sucient statistics data given current automaton,
update procedure guaranteed converge model locally maximizes likelihood
function Pr(datajmodel). Since maximum local, model might close enough
true automaton data generated, challenging problem find
ways force algorithm converging higher-likelihood maxima, least make
converge faster, facilitating multiple guesses initial models, thus raising probability
converging higher-likelihood maxima. approach one taken work
presented here.
assume, throughout paper, number states model learning
known. strong assumption since methods learning number
states. Regularization methods deciding number states model parameters,
discussed, instance, Vapnik's book (1995). address issue here.
rest work describes approach learning topological models. use noisy
odometric information readily available robots. geometrical information
typically used topological mapping methods. demonstrate topological model
algorithm used learn extended directly incorporate weak odometric
information. show so, avoid use human-provided priori
models still learn stochastic environment models eciently effectively.

3 Models Assumptions
section describes formal framework work. starts introducing classic
hidden Markov model. model extended accommodate noisy odometric information
nave form, ignoring information robot's heading orientation, later
adapted accommodate heading information.
concentrate describing models algorithms learning hmms, rather
pomdps. means robot decisions make regarding next action
every state; one action executed state. experiments, human operator gave action command associated state robot gathering data.
Note action necessarily one every state, e.g., robot told
always turn right state 1 move forward state 2. However, state one action taken. extension complete pomdps, implemented,
learning hmm possible actions; straightforward although notationally
cumbersome, thus limit discussion hmms.

3.1 HMMs { Basics
hidden Markov model consists states, transitions, observations probabilistic behavior,
formally defined tuple = hS; O; A; B; i, satisfying following conditions:

= fs0 ; : : : ; sN ,1 g finite set N states.
= fo0 ; : : : ; oM ,1g finite set possible observation values.
174

fiLearning Geometrically-Constrained HMMs

stochastic transition matrix, Ai;j = Pr(qt+1 = sj jqt = si), 0 i; j N ,1.
NX
,1
qt state time t. every state si ,

j =0

Ai;j = 1.

Ai;j holds transition probability state si state sj .
B stochastic observation matrix, Bj;k = Pr(vt = ok jqt = sj ), 0 j N , 1;
MX
,1
0 k , 1. vt observation recorded time t. every state sj ,
Bj;k = 1.
Bj;k holds probability observing ok state sj .

k=0

stochastic initial distribution vector, = Pr(q0 = si), 0 N , 1.

NX
,1
i=0

= 1.

holds probability state si time 0, starting record observations.
model corresponds world whose actual state given time t, qt 2 , hidden
directly observable, observable aspects state, vt 2 O, detected
recorded state visited time t. agent moves one hidden state
next according probability distribution encoded matrix A. observed information
state governed probability matrix B . Although work concerned
discrete observations, extension continuous observations straightforward
well addressed work hidden Markov models (Liporace, 1982; Juang, 1985).
Simply stated, problem learning hmm \reverse engineering" hidden Markov
model stochastic system sampled data, generated system. formalize
learning task Section 4.1. next section extends hmms account geometric
information.

3.2 Adding Odometry Hidden Markov Models

world composed finite set states. fundamental distinction
framework term state term location. state robot
directly correspond location. state may include information, robot's
battery level orientation location. robot standing entrance oce 101
facing right different state robot standing place facing left; similarly,
robot standing bagel arm different state robot
position without bagel.
dynamics world described state-transition distributions specify probability making transitions one state next result certain action.
finite set observations perceived state; relative frequency
observation described probability distribution depends current state.
model, observations multi-dimensional; observation vector values,
chosen finite domain. is, factorize observation associated state
several components. instance, demonstrated Section 6.1, view observation
recorded robot standing oce environment consisting three components,
corresponding three cardinal directions: front, left right. example, observation vector thus 3-dimensional. assumed vector's components conditionally
independent, given state.
175

fiShatkay & Kaelbling

addition components, state assumed associated position
metric space. Whenever state transition made, robot records odometry vector,
estimates position current state relative previous one. time assume odometry vector consists readings along x coordinates global coordinate system, readings corrupted independent normal noise. latter
independence assumption strict one, relaxed introducing complete covariance matrix, although done work. Section 3.3 extend odometry vector include information heading robot, drop global coordinate
framework.
Note odometric relationship characterizes transition rather state and,
described below, receives different treatment observations associated
states.
two important assumptions underlying treatment odometric relations
states: First, inherent \true" odometric relation position every
two states world; second, robot moves one state next,
normal, 0-mean noise around correct expected odometric reading along odometric
dimension. noise ects two kinds odometric error sources:

{ lack precision discretization real world states (e.g. rather

large area robot stand regarded \the doorway AI
lab").
{ lack precision odometric measures recorded robot, due slippage,
friction, disalignment wheels, imprecision measuring instruments, etc.

formally introduce odometric information hidden Markov model framework,
define augmented hidden Markov model tuple = hS; O; A; B; R; i, where:

= fs0 ; : : : ; sN ,1 g finite set N states.
= Qli=1 Oi finite set observation vectors length l. ith element

observation vector chosen finite set Oi .
stochastic transition matrix, Ai;j = Pr(qt+1 = sj jqt = si), 0 i; j N , 1.
NX
,1
qt state time t. every state si , Ai;j = 1.
j =0

Ai;j holds transition probability state si state sj .
B array l stochastic observation matrices, Bi;j;k = Pr(Vt [i] = ok jqt = sj );
1 l; 0 j N , 1; ok 2 Oi ; Vt observation vector time t; Vt [i] ith

component.
Bi;j;k holds probability observing ok along ith component observation
vector, state sj .
R relation matrix, specifying pair states, si sj , mean variance
D-dimensional2 odometric relation them. (Ri;j [m]) mean mth

2. time consider 2, corresponding (x; y) readings.

176

fiLearning Geometrically-Constrained HMMs

component relation si sj 2 (Ri;j [m]), variance. Furthermore,
R geometrically consistent: component m, relation (a; b) = (Ra;b [m])
must directed metric, satisfying following properties states a, b, c:
def

m(a; a) = 0;
m(a; b) = ,m(b; a) (anti-symmetry);
m(a; c) = (a; b) + m(b; c) (additivity ) :
representation odometric relations ects two assumptions, previously stated,
regarding nature odometric information. \true" odometric relation
position every two states represented mean. noise around correct
expected odometric relation, accounting lack precision real-world
discretization inaccuracy measurement, represented variance.

stochastic initial probability vector describing distribution initial state.
simplicity assumed form h0; : : : ; 0; 1; 0; : : : ; 0i, implying
one designated initial state, si , robot always started.

model extends standard hidden Markov model described Section 3.1 two ways:
facilitates observations factored components, represented vectors.
components assumed conditionally independent given
state. factorization, together conditional independence assumption, allows
simple calculation probability complete observation vector
probabilities components. therefore results fewer probabilistic parameters
learnt model view observation vector, consisting possible
combination component-values single \atomic" observation.

introduces odometric relation matrix R constraints components. Using
R constraints it, explained Section 4, proven useful learning
model parameters, demonstrated Section 6.

3.3 Handling Directional Data

extend model accommodate directional changes addition positional
changes. two issues stemming directional changes moving environment: need non-traditional distributions model directional changes, need
correct cumulative rotational error severely interferes location estimation
within global coordinate framework. detailed discussion two problems
solution given earlier paper authors (Shatkay & Kaelbling, 1998). sake
completeness, brie review two issues here.
3.3.1 Circular Distributions

robot's change direction moves environment expressed terms
angular change respect original heading. Since angular measures inherently circular, treating \normally distributed", using standard procedures obtaining
sucient statistics data adequate. trivial example, average
177

fiShatkay & Kaelbling


1

<x 1, y1>
<x 2, y2>
<x 3, y3>
1

173 0
179

0

-1

3

2

3
1

x

-1

Figure 1: Simple average two angles, depicted

vectors unit circle. average angle
formed dashed vector.

Figure 2: Directional data represented angles
vectors unit circle.

two angular readings, 173 ,179 , using simple average obtain angle ,3 ,
far intuitive 180 , illustrated Figure 1.
address circularity issue, use von Mises distribution, circular version
normal distribution, model change heading two states, explained below.
collection changes heading within two dimensional space represented terms
either Cartesian polar coordinates. Using Cartesian system, n changes headings
recorded sequence 2-dimensional vectors, (hx1 ; y1 i; : : : hxn ; yn i), unit circle,
shown Figure 2. changes also represented corresponding angles
radii center unit circle X axis, (1 ; : : : ; n ), respectively.
relationship two representations is:
xi = cos(i ); yi = sin(i ) ; (1 n) :
vector mean n points, hx; yi, calculated as:
Pn cos( )
Pn sin( )

:

=1
i=1
x=

=
;
n
n

(1)

Using polar coordinates, express mean vector terms angle, , length, a,
(except case x = = 0):

= arctan( xy );

= (x2 + 2 ) :
1
2

angle mean angle, length measure (between 0 1)
concentrated sample angles around . closer 1, concentrated
sample around mean, corresponds smaller sample variance.
Intuitively, satisfactory circular version normal distribution would mean
maximum likelihood estimate average angle calculated above. way
analogous Gauss' derivation Normal distribution, von Mises developed circular
version (Gumbel, Greenwood, & Durand, 1953; Mardia, 1972), defined follows:
Definition: circular random variable, , 0 2, said von Mises
distribution parameters , 0 2 > 0, probability density
178

fiLearning Geometrically-Constrained HMMs

function is:

f;() = 2I1 () e cos(,) ;
0

I0 () modified Bessel function first kind order 0:

I0 () =

1 1 1
X
2r
2 ( 2 ) :
r
!
r=0

(2)

parameters correspond distribution's mean concentration respectively.
circular-normal distributions exist, von Mises desirable estimation
procedure alluded earlier: Given set heading samples, angles 1 ; : : : n , von Mises
distribution, maximum likelihood estimate is:

= arctan( xy ) ;

y, x defined Equation 1.
maximum likelihood estimate concentration parameter, , satisfies:
n
I1 () = max[ 1 X
I0 ()
n i=1 cos(i , ); 0] ;

I1 modified Bessel function first kind order 1:

I1 () =

1
X

1 ( 1 )2r+1 :
r=0 r!(r + 1)! 2

(3)

information estimation procedure beyond scope paper
found elsewhere (Gumbel et al., 1953; Mardia, 1972).
conclude, assume change heading von Mises-distributed, around mean
concentration parameter . assumption ected model learning procedures
explained later Section 4.2.3. change heading h (a; b); (a; b)i pair
states (a; b) completes set parameters included relation matrix R
introduced earlier Section 3.2.
3.3.2 Cumulative Rotational Error

tend think environment consisting landmarks fixed global coordinate
system corridors transitions connecting landmarks. idea underlies typical
maps constructed used everyday life. However, view environment may
problematic robots involved.
Conceptually, robot two levels operates; abstract level, centers
corridors, follows walls avoids obstacles, physical level motors
turn wheels robot moves. physical level many inaccuracies manifest
themselves: wheels unaligned resulting drift right
left, one motor slightly faster another resulting similar drifts, obstacle
one wheels cause robot rotate around slightly, uneven oors may cause
179

fiShatkay & Kaelbling



- actual position
- recorded position

Figure 3: robot moving along solid arrow, correcting drift direction dashed
arrow. dotted arrow marks recorded change position.

robot slip certain direction. addition, measuring instrumentation odometric
information may accurate itself. abstract level, corrective actions
constantly executed overcome physical drift drag. example, left wheel
misaligned drags robot leftwards, corrective action moving right constantly
taken higher level keep robot centered corridor.
phenomena described significant effect odometry recorded robot,
data interpreted respect one global framework. example, consider robot
depicted Figure 3. drifts left , moving one state next,
corrects moving right order maintain centered corridor.
Let us assume states 5 meters apart along center corridor, center
corridor aligned axis global coordinate system. robot steps back
forth corridor one state next. Whenever robot reaches state,
odometry reading changes hx; y; along hX; Y; headingi dimensions, respectively.
robot proceeds, deviation respect X axis becomes severe. Thus,
going several transitions, odometric changes recorded every pair
states, taken respect global coordinate system, become larger larger. Similar
problems inconsistent odometric changes recorded pairs states arise along
odometric dimensions. especially severe inconsistencies arise respect
heading, since lead mistakenly switching movement along X
axes, well confusion forwards backwards movement (when deviation
heading around 90 180 respectively).
early work (Shatkay & Kaelbling, 1997) assumed perpendicularity corridors,
taken advantage robot collected data. Odometric readings recorded
respect global coordinate system, robot could re-align origin
turn. trajectory odometry recorded perpendicularity assumption
robot Ramona, along x axes given Figure 4. sequence shown recorded
robot drove repeatedly around loop corridors. details data
gathering process provided Section 6. contrast, Figure 5 shows trajectory another
sequence odometric readings recorded Ramona, driving corridors, without
using perpendicularity assumption. data collected latter setting subjected
cumulative rotational error.
180

fiLearning Geometrically-Constrained HMMs
3000
1200

2500
1000

2000
800

1500

600

1000

400

200

500
200

400

600

800

1000

-2500 -2000 -1500 -1000 -500

Figure 4: Sequence gathered Ramona, perpendicularity assumed.

500

1000

Figure 5: Sequence gathered Ramona, per-

pendicularity assumed.

data handled state-relative coordinate systems (Shatkay & Kaelbling, 1998).
latter implies state si coordinate system, shown Figure 6:
origin anchored si , axis aligned robot's heading state (denoted
bold arrows figure), X axis perpendicular it. contrast global
coordinate system anchored initial starting state. Within global coordinate
system, relations recorded may vary greatly among multiple instances transition
pair states. using state-relative system, recorded learned
relationship pair states, hsi ; sj i, reliable, despite fact based
multiple transitions recorded si sj .
state-relative coordinate systems, geometric relation stored Rij , (which introduced Section 3.2), expressed pair states, si sj , respect
coordinate system associated state si. Accordingly, constraints imposed x
components relation matrix must specified respect explicit coordinate
system used, explained below.
Given pair states b, denote hx;yi (a; b) vector h(Ra;b [x]); (Ra;b [y])i. Let
us define Tab transformation maps hxa ; ya point represented respect
coordinate system state a, point represented respect coordinate
system state b, hxb ; yb i.
explicitly, let ab mean change heading state state b. Applying Tab
vector h xyaa results vector h xybb follows:

* +

* + *

xb
x
x cos(ab ) , ya sin(ab )
= Tab =
yb
ya
xa sin(ab ) + ya cos(ab )

+

:

consistency constraints within framework must restated as:

hx;yi(a; a) = h0; 0i;
hx;yi(a; b) = ,Tba[hx;yi(b; a)] (anti-symmetry);
hx;yi(a; c) = hx;yi (a; b) + Tba[hx;yi (b; c)] (additivity).
181

fiShatkay & Kaelbling


x
Sj
Si





x

Figure 6: robot state Si , faces -axis direction; relation Si ,Sj wrt Si 's coordinate
system.

consistency constraints ones need enforced learning algorithm
constructs hmm. important note transformation
constitute set additional parameters need learnt. Rather, calculated terms
heading-change parameter, , already integral part relation matrix
defined Sections 3.2 3.3.1.
introduced basic formal model use representing environments
robot's interaction them. following section state learning problem
describe basic algorithm learning model data.

4 Learning HMMs Odometric Information

section formalizes learning problem hmms, discusses odometric information
incorporated learning algorithm. overview complete algorithm provided
Appendix paper.

4.1 Learning Problem

learning problem hidden Markov models generally stated follows: Given
experience sequence E, find hidden Markov model could generated sequence
\useful" \close original" according criterion. explicit common statistical
approach look model maximizes likelihood data sequence E given
model. Formally stated, maximizes Pr(Ej). However, given complicated landscape
typical likelihood functions multi-parameter domain, obtaining maximum likelihood
model feasible. studied practical methods, particular well-known BaumWelch algorithm (Rabiner (1989) references therein) guarantee local-maximum
likelihood model.
Another way evaluating quality learned model comparing true model.
note stochastic models (such hmms) induce probability distribution observation sequences given length. Kullback-Leibler (Kullback & Leibler, 1951) divergence
learned distribution true one commonly used measure estimating good
182

fiLearning Geometrically-Constrained HMMs

learned model is. Obtaining model minimizes measure possible learning goal.
culprit practice, learn model data, \ground
truth" model compare learned model with. Still, evaluate learning algorithms
measuring well perform data obtained known models. reasonable expect algorithm learns well data generated model have,
perform well data generated unknown model, assuming models indeed form
suitable representation true generating process. discuss Kullback-Leibler (kl)
divergence detail Section 6.2 context evaluating experimental results.
summarize, learning problem address work obtaining model
attempting (locally) maximize likelihood, evaluating results based
kl-divergence respect true underlying distribution, distribution
available.

4.2 Learning Algorithm

learning algorithm starts initial model 0 given experience sequence E;
returns revised model , (locally) maximizes likelihood P (Ej). experience
sequence E length ; element, Et , 0 (T , 1), pair hrt ; Vt i, rt
observed relation vector along x, dimensions, states qt,1 qt , Vt
observation vector time t.
algorithm extends standard Baum-Welch algorithm deal relational information factored observation sets. Baum-Welch algorithm expectationmaximization (em) algorithm (Dempster, Laird, & Rubin, 1977); alternates
E-step computing state-occupation state-transition probabilities, ,
time sequence given E current model ,
M-step finding new model, , maximizes P (Ej; ; ),
providing monotone convergence likelihood function P (Ej) local maximum.
However, extension introduces additional component, namely, relation matrix R.
viewed two kinds observations: state observations (as ordinary hmm |
distinction observe integer vectors rather integers) transition observations (the odometry relations states). latter must satisfy geometrical constraints.
Hence, extension standard update formulae, described below, required.
4.2.1 State-Occupation Probabilities

Following Rabiner (1989), first compute forward (ff) backward (fi ) matrices. fft (i)
denotes probability density value observing E0 Et qt = si , given ; fit (i)
probability density observing Et+1 ET ,1 given qt = si . Formally:
fft (i) = Pr(E0 ; : : : ; Et ; qt = sij) ;
fit (i) = Pr(Et+1 ; : : : ; ET ,1 jqt = si ; ) :
measurements continuous (as case R), matrices contain
probability density values rather probabilities.
forward procedure calculating ff matrix initialized
(
b = 1
ff0 (i) = 00 otherwise
;
183

fiShatkay & Kaelbling

continued 0 < , 1
fft (j ) =

NX
,1
i=0

fft,1 (i)Ai;j f (rt jRi;j )bjt :

(4)

expression f (rt jRi;j ) denotes density point rt according distribution represented
means variances entry i; j Q
relation matrix R, bjt probability
j
observing vector vt state sj ; is, bt = li=0 Bi;j;vt[i] .
backward procedure calculating fi matrix initialized fiT ,1 (j )=1, continued
0 t<T , 1
NX
,1
fit (i) = fit+1 (j )Ai;j f (rt+1 jRi;j )bjt+1 :
(5)
j =0

Given ff fi , compute given time point state-occupation statetransition probabilities, . state-occupation probabilities, (i), representing
probability state si time given experience sequence current model,
computed follows:
:
(6)
(i) = Pr(qt = si jE; ) = PNff,t1(i)fit (i)
j =0 fft (j )fit (j )
Similarly, (i; j ), state-transition probabilities state state j time given
experience sequence current model, computed as:
(i; j ) = Pr(qt = si ; qt+1 = sj jE; )
fft (i)Ai;j bjt+1 f (rt+1 jRi;j )fit+1 (j )
:
(7)
=
NX
,1 NX
,1
i=0 j =0

fft (i)Ai;j bjt+1 f (rt+1 jRi;j )fit+1 (j )

essentially formulae appearing Rabiner's tutorial (Rabiner, 1989),
also take account density odometric relations.
next phase algorithm, goal find new model, , maximizes likelihood conditioned current transition observation probabilities, Pr(Ej; ; ). Usually,
simply done using maximum-likelihood estimation probability distributions
B computing expected transition observation frequencies. model must also
compute new relation matrix, R, constraint remain geometrically consistent.
rest section use notation v denote reestimated value, v
denotes current value.
4.2.2 Updating Transition Observation Parameters

B matrices straightforwardly reestimated. Ai;j expected number
transitions si sj divided expected number transitions si , B i;j;k
expected number times ok observed along ith dimension state sj , divided
expected number times sj :
PT ,1
PT ,2 (i; j )


=0
; B i;j;k = t=0PT[V,t1[i]=ok ] (j ) :
(8)
Ai;j = PT ,2
t=0 (i)
t=0 (i)
expression c denotes indicator function value 1 condition c true 0 otherwise.
184

fiLearning Geometrically-Constrained HMMs
7.5
P

Q

5

P

2.5

-8

-6

-4

-2

2

4

6

8

-2.5
-5

-6

-4

-2

2

4

6
-7.5

Q

Figure 7: Examples two sets normally distributed points constrained means, 1 2
dimensions.

4.2.3 Updating Relation Parameters

reestimating relation matrix, R, geometrical constraints induce interdependencies
among optimal mean estimates well optimal variance estimates mean
estimates. Parameter estimation form constraints almost untreated mainstream statistics (Bartels, 1984) found previous existing solutions estimation
problem addressed here. illustration issues involved estimation constraints
consider following estimation problem 2 normal means:
Example 4.1 data consists two sample sets points P = fp1; p2 ; : : : ; pn g Q =
fq1; q2 ; : : : ; qk g, independently drawn two distinct normal distributions means P ; Q
variances P2 ; Q2 , respectively. asked find maximum likelihood estimates
two distribution parameters. Moreover, told means two distributions
related, Q = ,P , illustrated Figure 7. latter constraint, task
simple (DeGroot, 1986), have:
Pn p
Pn
; 2 = i=1 (pi , P )2 ;
P = i=1
P
n
n

similarly Q Q2 . However, constraint P = ,Q requires finding single mean, ,
setting one negated value, ,. Intuitively, choosing maximum
likelihood single mean, concentrated sample effect,
varied sample \submissive." Thus, overall sample deviation means
would minimized likelihood data maximized. Therefore, mutual
dependence estimation mean estimation variance.
Since samples independently drawn, joint likelihood function is:
,(pi ,P )2

n
P

f (P; QjP ; Q; P2 ; Q2 ) = e p
i=1 2P
2 2



Yk e
j =1

,(qj ,Q )2
Q

p

2 2

2Q

:

taking derivatives joint log-likelihood function, respect P , P Q,
equating 0, using constraint Q = ,P , obtain following set mutual
equations maximum likelihood estimators:
P
P
(Q2 ni=1 pi) , (P2 kj=1 qj )
P =
; Q = ,P ;
nQ2 + kP2
Pk (q + )2
Pn (p , )2

P

=1
2
2
P =
; Q = j =1 j P :

n

k

185

fiShatkay & Kaelbling

substituting expressions P Q expression P , obtain cubic equation cumbersome, still solvable (in simple case). solution provides maximum likelihood estimate mean variance constraint Q = ,P :
2
proceed actual update relation matrix constraints. clarity,
initially discuss first two geometrical constraints, discuss additivity constraint
Section 4.3. Recall concentrate enforcement global constraints, appropriate
perpendicularity assumption, although idea applied case staterelative constraints.
Zero distances states trivially enforced, setting diagonal
entries R matrix 0, small variance.
Anti-symmetry within global coordinate system enforced using data recorded along
transition state sj si well state si sj reestimating (Ri;j ).
demonstrated Example 4.1, variance taken account, leading following
set mutual equations:



mi;j

=

( mi;j )2 =

PT ,2

rt[m]t (i;j ) , rt [m]t(j;i)
(
(
i;j )2
j;i )2

PT ,2 t(mi;j) + t(mj;i)
t=0 (i;j )2 (j;i )2
PT ,2[ (i; j )(r [m] , )2 ]
t=0
PT ,2 t(i; j ) i;j :
t=0
t=0

;

(9)
(10)

x dimensions, (m = x; y), amounts complicated still solvable cubic
equation. However, general case, accounting orientation robot,
also complete additivity enforced, obtain closed form reestimation
formulae.
avoid hardships, use lag-behind update rule; yet-unupdated estimate
variance used calculating new estimate mean, new mean estimate
used update variance, using Equation 10.3 Thus, mean updated using variance
parameter lags behind update process, reestimation Equation (9) needs
use rather follows: PT ,2 h rt [m]t (i;j) rt [m]t (j;i)
2 , j;i
)2
t=0
:
(11)
mi;j = PT ,2(hi;jt ()i;j) ((j;i
)
t=0

)2 + (j;i
)2
i;j

(

shown (Shatkay, 1999), lag-behind policy instance generalized em (McLachlan & Krishnan, 1997). latter guarantees monotone convergence local maximum
likelihood function, even \maximization" step increases rather strictly maximizes expected likelihood data given current model.
Similarly, reestimation formula von Mises mean () concentration () parameters
heading change states si sj solution equations:

0 TX
1
,
BB [sin(rt [])(t (i; j )i;j , t(j; i)j;i)] CC

CC
= arctan B
B@ TX
,

[cos(rt [])(t (i; j )i;j + (j; i)j;i )]
2

i;j

=0

2

t=0

3. similar approach, termed one step late update, taken others applying em highly non-linear optimization problems (McLachlan & Krishnan, 1997).

186

fiLearning Geometrically-Constrained HMMs

I1 [i;j ]
= max
I0 [i;j ]

" PT ,

#
2
(i; j ) cos(rt [] , i;j )]
t=0 [tP
; 0
,2 (i; j )
t=0

;

(12)

I0 I1 modified Bessel functions defined Equations 2 3 Section 3.3.1.
Again, avoid need solve mutual equations, take advantage lag-behind strategy, updating mean using current estimates concentration parameters, i;j ; j;i,
follows:
PT ,2[sin(r [])( (i; j ) , (j; i) )] !


i;j
j;i
i;j = arctan PTt=0
(13)
,2 [cos(r [])( (i; j ) + (j; i) )] ;


i;j
j;i
t=0
calculating new concentration parameters based newly updated mean,
solution Equation 12, use lookup-tables.
possible alternative lag-behind approach update mean though assumption j;i = i;j holds. assumption, variance terms Equation 9 cancel out,
mean update independent variance again. variances updated
stated Equation 10, without assuming constraints them. approach taken
earlier stages work (Shatkay & Kaelbling, 1997, 1998). lag-behind strategy
superior, according experiments, due instance generalized em.

4.3 Enforcing Additivity

Note additivity constraint directly implies two geometrical constraints4 . Thus,
enforcing results complete geometrical consistency. present method directly
enforcing additivity reestimation procedure along x dimensions.
heading dimension describe complete geometrical consistency achieved
projection anti-symmetric estimates onto geometrically-consistent space. before,
simplify presentation, focus case global coordinate systems. basic
idea applies state-relative coordinate systems, relationship used recover mean
ij individual state coordinates complex.
4.3.1 Additivity x, dimensions

main observation underlying approach additivity constraint result
fact states embedded geometrical space. is, assuming N states,
s0; : : : ; sN ,1, points X , axes, x0 ; : : : ; xN ,1 , y0 ; : : : ; yN ,1 , 0 ; : : : ; N ,1,
respectively, state, si , associated coordinates hxi ; yi ; i. Assuming
one global coordinate system, mean odometric relation state si state sj
expressed as: hxj , xi ; yj , yi ; j , i.
maximization phase em iteration, rather try maximize respect
N 2 odometric relation vectors, hXij , Yij , ij i, reparameterize problem. Specifically,
express odometric relation function two N state positions, maximize
respect unconstrained, N state positions. instance, X dimension, rather
search N 2 maximum likelihood estimates xij , use maximization step find
N 1-dimensional points, x0 ; : : : ; xN ,1 . calculate xij = xj , xi . Moreover, since
interested finding best relationships xi xj , fix one
4. f(a; a)= (a; a) + (a; a)g ) ((a; a)=0) ; f((a; a)=0) ; ((a; a)= (a; b)+(b; a))g ) ((a; b) = ,(b; a)).

187

fiShatkay & Kaelbling

xi 's 0 (e.g. x0 = 0), find optimal estimates remaining N , 1 state positions.
variance reestimation remains before, lag-behind policy used eliminate
interdependency update mean variance parameters.
4.3.2 Additive Heading Estimation

Unfortunately, reparameterization described feasible estimation changes
heading, due von Mises distribution assumption heading measures. reparameterizing ij j , trying maximize likelihood function respect
parameters, obtain set N,1 trigonometric equations terms form cos(j ) sin(i )
enable simple solution.
alternative, possible use anti-symmetric reestimation procedure described
earlier, followed perpendicular projection operator, mapping resulting headings vector
h00 ; : : : ; ij ; : : : ; N ,1;N ,1i, 0 i; j N ,1, satisfy additivity, onto vector
headings within additive linear vector space. Simple orthogonal projection satisfactory
within setting, since simply looks additive vector closest non-additive one.
procedure ignores fact entries non-additive vector based
lot observations, therefore reliable, other, less reliable ones, based
hardly data all. Intuitively, would like keep estimates well accounted
intact, adapt less reliable estimates meet additivity constraint. precisely,
heading-change estimates states better accounted others,
sense transitions
states higher expected counts transition
P
states (higher (i; j )). would like project non-additive heading
estimates vector onto subspace additive vector space, vectors
values non-additive
P vector entries well-accounted for, is,
highest values (i; j ). diculty latter subspace linear vector
space (for instance, satisfy closure scalar multiplication), projection
operator linear spaces cannot applied directly. Still, set vectors form
ane vector space, project onto using algebraic technique, explained below.5
Definition
Rn n-dimensional ane space vectors va2A, set vectors:
def
, va = fua , va jua 2 Ag linear space.
Hence, pick vector ane space, va 2A, define translation Ta : ! V ,
V linear space, V = , va . translation trivially extended vector
v0 2 Rn , defining Ta (v0 ) = v0 , va . order project vector v 2 Rn onto A, apply
translation Ta v project Ta (v) onto V , results vector P (Ta (v)) V .
applying inverse transform Ta,1 it, obtain projection v A, demonstrated
Figure 8. linear space figure two dimensional vector space fhx; yij = ,xg,
ane space fhx; yij = ,x + 4g. transform Ta consists subtracting vector
h0; 4i. solid arrow corresponds direct projection vector v onto point P (v)
ane space. dotted arrows represent projection via translation v Ta (v),
projection latter onto linear vector space, inverse translation result,
P (Ta (v)), onto ane space.
1

1

1

5. Many thanks John Hughes introducing us technique.

188

fiLearning Geometrically-Constrained HMMs

6
<x,-x+4>
4
P(v)

v

2

-2

2

-2

4
Ta (v)
P(Ta (v))

<x,-x>

-4

Figure 8: Projecting v onto ane vector space fhx; yij = ,x + 4g.
Although procedure preserving additivity headings formally proven preserve monotone convergence likelihood function towards local maximum, extensive
experiments consisting hundreds runs shown monotone convergence preserved.

5 Choosing Initial Model

Typically, instances Baum-Welch algorithm, initial model picked uniformly
random space possible models, perhaps trying multiple initial models find different local likelihood maxima. alternative approach reported (Shatkay & Kaelbling,
1997) based clustering accumulated odometric information using simple k-means
algorithm (Duda & Hart, 1973), taking clusters states observations
recorded, obtain state observation counts estimate model parameters.
perpendicularity assumed collecting data, shown Figure 4, k-means
algorithm assigns cluster (state) odometric readings recorded close locations,
leading reasonable initial models. However, assumption dropped, illustrated
Figure 5, cumulative rotational error distorts odometric location recorded within
global coordinate system, location assigned state multiple visits
varies greatly would recognized \the same" simple location-based clustering
algorithm. overcome this, developed alternative initialization heuristics, call
tag-based initialization. based directly recorded relations states, rather
states' absolute location. clarity, description consists mostly illustrative
example, concentrates case global consistency constraints enforced.
Given sequence observations odometric readings E, begin clustering odometric
readings buckets. number buckets number distinct state transitions
recorded sequence. goal stage bucket contain odometric
readings close along three dimensions.
achieve this, start fixing predetermined, small standard deviation value along x,
y, dimensions. Denote standard deviation values x ; ; respectively, (typically
x = ). first odometric reading assigned bucket 0 mean bucket
set value reading. rest process subsequent odometric
readings examined. next reading within 1:5 standard deviations along
three dimensions mean existing non-empty bucket, add bucket
189

fiShatkay & Kaelbling

< 2, 94, 92 >
< -4, 102, 91 >

<1994, 0, 88 >
< 1998, -5, 90 >

< 3, -93, 86 >
< -2, -106, 91 >

< -1999, -1, 94 >
< -2003, 7, 87 >

1:

2:

3:

4:

<-1, 98, 91.5>

<1996, -2.5, 89>

<0.5, -99.5, 88.5>

<-2001, 3, 90.5>

3

4

1

2

Figure 9: bucket assignment example sequence.
update bucket mean accordingly. not, assign empty bucket set mean
bucket reading.
Intuitively, using heuristic resulting buckets tightly concentrated
mean. note clustering algorithms (Duda & Hart, 1973) could used
bucketing stage.
Example 5.1 would like learn 4-state model sequence odometric readings,
hx; y; follows:
h2 94 92i; h1994 0 88i; h3 , 93 86i; h,1999 1 94i;
h,4 102 91i; h1998 , 5 90i; h,2 , 106 91i; h,2003 7 87i :
first stage place readings buckets. Suppose standard deviation constant
20. placement shown Figure 9. mean value associated bucket shown
well.
2
next stage algorithm state-tagging phase, odometric reading,
rt , assigned pair states, si; sj , denoting origin state (from transition took
place) destination state (to transition led), respectively. conjunction,
mean entries, ij , relation matrix, R, populated.

Example 5.1 (cont.) Returning sequence above, process demonstrated Figure 10. assume data recording starts state 0, odometric change
self transitions 0, small standard deviation (we use 20 well).
shown part figure.
Since first element sequence, h2 94 92i, two standard deviations away
mean [0][0] entry relation row state 0 populated, pick 1
next state populate mean [0][1] mean bucket 1,
h2 94 92i belongs. maintain geometrical consistency mean [1][0] set ,[0][1],
shown part B figure. populated 2 off-diagonal entries, state
sequence h0; 1i. entry [0][1] matrix becomes associated bucket 1,
information recorded helping tagging future odometric readings belonging
bucket.
next odometric reading, h1994 0 88i, standard deviations populated mean
row 1 (where 1 current believed state). Hence, pick new state 2, set mean
[1][2] 2|the mean bucket 2|to reading belongs (Figure 10 C). entry
[1][2] recorded associated bucket 2. preserve anti-symmetry additivity, [2][1]
set ,[1][2]. [0][2] set sum [0][1] + [1][2], [2][0] set ,[0][2].
190

fiLearning Geometrically-Constrained HMMs

0
0

1

B
2

3

0

<0,0,0>

1

0
1

<0,0,0>

<0,0,0>

2

2

3

<-1,
<0,0,0> 98,
91.5>
< 1,
-98,
-91.5>

<0,0,0>

<0,0,0>

2
3

<0,0,0>

3

1

<0,0,0>

S: 0

S: 0. 1
Bucket(R[0][1]) = 1

C
0
0
1
2

1


2

3

0

<-1,
<1995,
95.5,
<0,0,0> 98,
91.5> -179.5>
<1996,
< 1,
-98,
<0,0,0> -2.5,
-91.5>
89>

0
1

<-1995, <-1996,
-95.5,
2.5,
<0,0,0>
179.5> -89>

3

2
3

<0,0,0>

S: 0, 1, 2

1

2

3

<-1,
<1995, <1995.5,
95.5,
-4,
<0,0,0> 98,
91.5> -179.5> -91>
<1996, <1996.5,
< 1,
-98,
-102,
<0,0,0> -2.5,
-91.5>
89>
177.5>
<-1995, <-1996,
< 0.5,
-95.5,
2.5,
<0,0,0> -99.5,
179.5> -89>
88.5>
<-1995.5, <-1996.5, <-0.5,
99.5, <0,0,0>
4,
102,
-177.5> -88.5>
91>

S: 0,1,2,3
Bucket(R[2][3]) = 3

Bucket(R[1][2]) = 2

S: 0,1,2,3,0
Bucket(R[3][0]) = 4
,..., S:0, 1, 2, 3, 0, 1, 2, 3, 0

Figure 10: Populating odometric relation matrix creating state tagging sequence.
Similarly, [2][3] updated mean bucket 3, causing setting [3][2], [1][3],
[0][3], [3][1], [3][0]. Bucket 3 associated [2][3].
stage odometric table fully populated, shown part Figure 10. state
sequence point is: h0; 1; 2; 3i. next reading, h,1999 ,1 94i, within one standard
deviation [3][0] therefore next state 0. Entry [3][0] associated bucket 4,
(the bucket reading assigned), state sequence becomes: h0; 1; 2; 3; 0i.
next reading, bucket 1, associated relation state 0 tagged
bucket 1, namely, state 1. repeating last two readings, final state transition
sequence becomes h0; 1; 2; 3; 0; 1; 2; 3; 0i:
2
Note process described illustration simplified. general case,
need take account rotational error data, use state-relative coordinate
systems, therefore populate entries transformed anti-symmetry additivity
constraints:
hx;yi(a; b) = ,Tba [hx;yi(b; a)] ;
hx;yi(a; c) = hx;yi(a; b) + Tba [hx;yi(b; c)],
defined Section 3.3.2.
191

fiShatkay & Kaelbling

possible end tagging algorithm, rows columns relation
matrix still unpopulated. happens little data learn
number states provided algorithm large respect actual model.
cases either \trim" model, using number populated rows number
states, pick random odometric readings populate rest table, improving
estimates later. Note first approach suggests method learning number states
model given, starting gross over-estimate number, truncating number populated rows odometric table initialization performed.
state-transition sequence obtained, rest initialization algorithm
k-means based initialization, deriving state-transition counts state-transition
sequence, assigning observations states assumption state sequence
correct, obtaining state-transition observation probabilities. initialization phase
incur much computational overhead, equivalent time-wise performing one
additional iteration em procedure.

6 Experiments Results

goal work described far use odometry improve learning topological
models, using fewer iterations less data. tested algorithm simple robotnavigation world. experiments consist running algorithm data obtained
simulated model data gathered mobile robot, Ramona. amount
data gathered Ramona used proof concept sucient statistical
analysis. latter, use data obtained simulated model. gathered data
used algorithms without perpendicularity assumption (see Section 3.3.2),
results provided settings.

6.1 Robot Domain

robot used experiments, Ramona, modified RWI B21 robot. cylindrical
synchro-drive base, 24 ultrasonic sensors 24 infrared sensors, situated evenly around
circumference. infrared sensors used mostly short-range obstacle avoidance.
ultrasonic sensors longer ranged, used obtaining (noisy) observations
environment. experiments described here, robot follows prescribed path
corridors oce environment department. Thus, decision-making
involved, hmm sucient model, rather complete pomdp.
Low-level software6 provides level abstraction allows robot move hallways
intersection intersection turn ninety degrees left right. software
uses sonar data distinguish doors, openings, intersections along path, stop
robot's current action whenever landmark detected. stop|either due
natural termination action due landmark detection|is considered robot
\state".
stop, ultrasonic data interpretation allows robot perceive, three
cardinal directions, (front, left right), whether open space, door, wall,
something unknown.
Encoders robot's wheels allow estimate pose (position orientation) respect pose previous intersection. recording sonar-based observations
6. low-level software written maintained James Kurien.

192

fiLearning Geometrically-Constrained HMMs
3

5

4

6

7

8
9

2

12
13

10
11

9
8

23

42

6 7
22 20
21

43
0
19

10

5
4

1

3

2 1
41

14 15
16

18
17

24
25

38
36
37

35
34
40

11

26 27

30 31

12

0
16

15

14

29
28

39

33
32

13

Figure 11: True model corridors Ramona traversed. Arrows represent prescribed path direction.

Figure 12: True model prescribed path
simulated hallway environment.

odometric information, robot goes execute next prescribed action.
action command issued manually human operator. course, action performance perception routines subject error. path Ramona followed consists
4 connected corridors building, include 17 states, shown Figure 11.
simulation, manually generated hmm representing prescribed path robot
complete oce environment department, consisting 44 states,
associated transition, observation, odometric distributions. transition probabilities
ect action failure rate 5 , 10%. is, probability moving
current state correct next state environment, predetermined action
0:85 0:95. probability self transition typically 0:05 0:15.
small probability (typically smaller 0:02) sometimes assigned transitions.
experience real robot proves reasonable transition model, since
typically robot moves next state correctly, error occurs
significant frequency move all, due sonar interpretation indicating
barrier actually none. action command repeated robot usually
performs action correctly, moving expected next state. observation distribution
typically assigns probabilities 0:85 , 0:95 true observation perceived
robot state, probabilities 0:05 , 0:15 observations might
perceived. example, door actually perceived, door typically assigned
probability 0:85,0:9, wall assigned probability 0:09,0:1 open space assigned
probability 0:01 perceived. standard deviation around odometric readings
5% mean.
Figure 12 shows hmm corresponding simulated hallway environment. Observations
orientation omitted figure clarity. Nodes correspond states
environment, directed edges correspond corridors; arrows point direction
corridors traversed. interpretation figures provided
following section.
193

fiShatkay & Kaelbling

6.2 Evaluation Method

number different ways evaluating results model-learning algorithm.
None completely satisfactory, give insight utility results.
domain, transitions observations usually take place, therefore
likely others. Furthermore, relational information gives us rough estimate
metric locations states. get qualitative sense plausibility learnt
model, extract essential map learnt model, consisting states,
likely transitions metric measures associated them, ask whether map
corresponds essential map underlying true world.
Figures 11 12 essential versions true models, Figures 15 17, shown
later, essential versions representative learnt ones (obtained sequences gathered
perpendicularity assumption). Black dots represent physical locations states,
state assigned unique number. Multiple state numbers associated single
location typically correspond different orientations robot location. larger
black circle represents initial state. Solid arrows represent likely non-self transitions
states. Dashed arrows represent transitions probability 0:2
higher. Typically, due predetermined path taken, connectivity
modeled environment low, therefore transitions represented dashed arrows
almost likely likely ones. Note length arrows, within plot,
significant represents length corridors, drawn scale.
important note figures provide complete representation models.
First, lack observation orientation information. stress fact figures
serve visual aid plot true model. looking good topological
model rather geometrical model. figures provide geometrical embedding
topological model. However, even geometry, described relation matrix,
different, topology, described transition observation matrices, still valid.
Traditionally, simulation experiments, learnt model quantitatively compared
actual model generated data. models induces probability distribution
strings observations; asymmetric Kullback-Leibler divergence (Kullback & Leibler,
1951) two distributions measure good learnt model respect
true model. Given true probability distribution P = fp1 ; :::; pn g learnt one
Q = fq1; :::; qn g, kl divergence Q respect P is:

D(P jjQ) =

def

n
X
i=1

pi log2 pqi :


report results terms sampled version kl divergence, described Juang
Rabiner (1985). based generating sequences sucient length (5 sequences 1000
observations case) according distribution induced true model, comparing
log-likelihood according learnt model true model log-likelihood. total
difference log-likelihood divided total number observations, accumulated
sequences, giving number roughly measures difference log-likelihood
per observation. Formally stated, let M1 true model M2 learnt one. generating
K sequences S1 ; : : : ; SK , length , true model, M1 , sampled kl-divergence,
Ds is:
K
X
[log(Pr(Si jM1 )) , log(Pr(Si jM2 ))]

=1
Ds(M1 jjM2 ) =
:
KT
194

fiLearning Geometrically-Constrained HMMs
1000
1200

500
1000

800

-1500 -1250 -1000 -750

-500

-250

600

-500
400

-1000
200

200

400

600

800

-1500

1000

Figure 13: Sequence gathered Ramona,
perpendicularity assumed.

Figure 14: Sequence generated simulator, perpendicularity assumed.

ignore odometric information applying kl measure, thus allowing comparison
purely topological models learnt without odometry.

6.3 Results within Global Framework

let Ramona go around path depicted Figure 11 collect sequence
300 observations, assuming perpendicularity environment, is, every turning
point angle turn 90 . Thus turn Ramona realigns odometric readings
initial X axes. Figure 13 plots sequence metric coordinates, gathered
way, accumulating consecutive odometric readings, projected hx; yi. applied
learning algorithm data 30 times. 10 runs started k-means-based
initial model, 10 started tag-based initial model, 10 started random initial
model. addition also ran standard Baum-Welch algorithm, ignoring odometric
information, 10 times. (Note non-determinism even using biased initial
models, since k-means clustering starts random seeds, low7 random noise added
data algorithms avoid numerical instabilities, thus multiple runs give multiple
results). report results obtained using tag-based method,
appropriate initialization method general case. results contrasted
obtained odometric information used all. comparison four settings
reader referred complete report work (Shatkay, 1999).
Figure 15 shows essential representations typical learnt models starting tag-based
initial model. geometry learnt model strongly corresponds true environment, states' positions learnt correctly. Although figure
show it, learnt observation distributions state usually match well true
observations.
demonstrate effect odometry quality learnt topological model, contrast
plotted models learnt using odometry representative topological model learnt without
7. random number -1cm 1cm added recorded distances typically several meters
long.

195

fiShatkay & Kaelbling
3

4

5

6

3

4

5

6

7

8

7

8

5

9

0

7

9

22

12
8
1
10
10

9
11

16

2

15
3
11
11

13
14

16

11

12

16

12
15

0

14
14

0

mona traversed.

6

4

13

Figure 15: Learnt model corridors Ra15

13

Figure 16: topology model learnt
without use odometry.

use odometric information. Figure 16 shows topology typical model learnt without
use odometric information. case, arcs represent topological relationships,
length meaningful. initial state shown bold circle. clear
topology learnt match characteristic loop topology true environment.

obtaining statistically sucient information, generated 5 data sequences, length
1000, using Monte Carlo sampling hidden Markov model whose projection shown
Figure 12. One sequences depicted Figure 14. figure demonstrates
noise model used simulation indeed compatible noise pattern associated
real robot data. used four different settings learning algorithm:

starting biased, tag-based, initial model using odometric information;
starting biased, k-means-based, initial model using odometric information;
starting initial model picked uniformly random, using odometric information;
starting random initial model without using odometric information (standard BaumWelch).

sequence four algorithmic settings ran algorithm 10 times.
keep discussion focused, concentrate first last settings
reader referred extensive report (Shatkay, 1999) complete discussion.
experiments, N set 44, \correct" number states; generalization, necessary use cross-validation regularization methods select model
complexity. Section 5 also suggests one possible heuristic obtaining estimate number
states.
Figure 17 shows essential version one learnt model, obtained sequence shown
Figure 14, using tag-based initialization. note learnt model completely
196

fiLearning Geometrically-Constrained HMMs
26
14

15
16 27

13
12

25
33
24
23

7

8

6

22

9
0

32 31 21

5
29 17
18
28

34

2 1
4 3

20
19 30

11
10

35 36

43 42

37
41

38
39

40

Figure 17: Learnt model simulated hallway environment.
accurate respect true model. However, obvious correspondence
groups states learnt true models, transitions (as well
observations, shown) learnt correctly. quality geometry
learnt model simulated large environment varies, geometrical results
uniformly good case learning smaller environment real robot data.
environment gets large, global relations remote states, ected
geometrical consistency constraints, become harder learn. Still, topology
learnt model demonstrated statistical experiments good.
Table 1 lists kl divergence true learnt model, well number
runs convergence reached, 5 sequences setting
uses odometric information tag-based initialization learning algorithm
use odometric information, averaged 10 runs per sequence. stress kl
divergence measure calculated based new data sequences generated true
model, described Section 6.2. 5 sequences models learnt
participate testing process.
kl divergence respect true model models learnt using odometry, 5-6
times smaller models learnt without odometric data. standard deviation around
means 0.2 kl distances models learnt odometry 1.5 noodometry setting. check significance results used simple two-sample t-test.
models learnt using odometric information statistically significantly (p 0:0005) lower
average kl divergence others.

Seq. #
kl
Odo Iter #

kl
Odo Iter #

1
0.981
16.70
6.351
124.1

2
1.290
20.90
4.863
126.0

3
1.115
22.30
5.926
113.0

4
1.241
12.70
6.261
107.4

5
1.241
27.50
4.802
122.9

Table 1: Average results two learning settings five training sequences.
197

fiShatkay & Kaelbling

addition, number iterations required convergence learning using odometric
information roughly 4-5 times smaller required ignoring information.
Again, t-test verifies significance result.
three initialization settings, models learnt topologically somewhat inferior (and
high statistical significance), terms kl divergence, learnt without
enforcing additivity, reported earlier papers (Shatkay & Kaelbling, 1997, 1998). likely
result strong constraints enforced learning process, prevent
algorithm searching better areas learning-space, restrict reach poor local
maxima. geometry looks superior cases, significantly better. However,
seems less variability quality geometrical models across multiple runs
additivity enforced.
details extensive comparison different initialization methods
beyond scope paper, point studies small large models
show large models long data sequences involved, random initialization often
results lower KL-divergence tag-based initialization.
strong bias tag-based initialization, lead peaked models compared
less-peaked distributions associated true model. Random initialization leads atter
models. KL-divergence strongly penalizes models much peaked
true ones, randomly initialized models often closer, terms measure, true
models peaked ones learnt initial models. learning small models,
sucient training data available, tag-based initialization results models
clearly superior random ones. Again, reader referred complete report
work (Shatkay, 1999) comparative study initialization methods various
settings.

6.4 Results within Relative Framework

applied algorithm described Section 4.3, extended accommodate state-relative
constraints (as listed Section 3.3.2). data used gathered robot
environment, generated simulated model (Figures 11, 12).
However, data generated without assuming perpendicularity. means x
coordinates realigned turn global x axes, rather,
recorded \as-is." evaluation methods stay described above.
Figure 18 shows projection odometric readings Ramona recorded along
x dimensions, traversing environment. obtaining statistically sucient
information, generated 5 data sequences, length 800, using Monte Carlo sampling
hidden Markov model whose projection shown Figure 12. One sequences
depicted Figure 19.
Figure 20 shows typical model obtained applying algorithm enforcing complete
geometrical consistency, robot data shown Figure 18, using tag-based initialization.
note rectangular geometry environment preserved, although state 0
participate loop. explained observing corresponding area true
environment depicted Figure 11, consisting 4 states clustered bottom left
corner (0, 14, 15 16). Due relatively large number states close together
area true environment, recognized ever returned particularly
state 0 loop. Therefore, one transition recorded state 0 state
198

fiLearning Geometrically-Constrained HMMs
3000

1500

2500

1000

2000

500

1500

-1500

-1000

-500

500

1000

-500

500

-1000

-1500

-2500 -2000 -1500 -1000 -500

500

1000

Figure 18: Sequence gathered Ramona,

Figure 19: Sequence generated simula-

perpendicularity assumed.

tor, perpendicularity assumed.
15

14

16

1
13

12

2

11

3
4
0

5

10

6
7
9

8

Figure 20: Learnt model corridors Ramona traversed. Initialization tag-based.
1 according expected transition counts calculated algorithm. projecting
angles maintain additivity, (as described Section 4.3.2), angle state 0 1
therefore compromised, allowing geometrical consistency maintain rectangular geometry
among regularly visited states.
purpose quantitatively evaluating learning algorithm list Table 2 kl
divergence true learnt model, well number iterations convergence reached, 5 simulation sequences with/without odometric information,
averaged 10 runs per sequence. table demonstrates kl divergence respect true model models learnt using odometric data, 8 times smaller
models learnt without it. check significance results use simple
two-sample t-test. models learnt using odometric information highly statistically significantly (p 0:0005) lower average kl divergence others. addition, number
199

fiShatkay & Kaelbling

Seq. #
kl
Odo Iter #

kl
Odo Iter #

1
2
3
4
5
1.46 1.18 1.20 1.02 1.22
11.8 36.8 30.7 24.6 33.3
6.91 9.93 10.03 9.54 12.43
113.3 113.1 102.0 104.2 112.5

Table 2: Average results 2 learning settings 5 training sequences.
iterations required convergence learning using odometric information smaller
required ignoring information. Again, t-test verifies significance (p < 0:005)
result.
important point number iterations, although much lower, automatically imply algorithm runs less time non-odometric Baum-Welch.
major bottleneck caused need compute within forward-backward calculations,
described Section 4.2.1, values normal von-Mises densities. require calculation exponent terms rather simple multiplications, slowing
iteration, current nave implementation. However, solve augmenting
program look-up tables obtaining relevant values rather calculating them.
addition, take advantage symmetry relations table cut
amount calculation required. also possible use fact many odometric relations remain unchanged (particularly later iterations algorithm) one iteration
next, therefore values cached shared iterations rather
recalculated iteration.

6.5 Reducing Amount Data
Learning hmms obviously requires visiting states transitioning multiple times,
gather sucient data robust statistical estimation. Intuitively, exploiting odometric data
help reduce number visits needed obtaining reliable model.
examine uence reduction length data sequences quality learnt
models, took one 5 sequences used prefixes length 100 800 (the complete
sequence), increments 100, training sequences. ran two algorithmic settings
8 prefix sequences, 10 times repeatedly. used kl-divergence described
evaluate resulting models respect true model. prefix
length averaged kl-divergence 10 runs.
plot Figure 21 depicts average kl-divergence function sequence length
two settings. demonstrates that, terms kl divergence, algorithm,
uses odometric information, robust face data reduction, (down 200 data
points). contrast, learning without use odometry quickly deteriorates amount
data reduced.
note data sequence twice \wide" odometry used
not; is, information element sequence odometry data
recorded. However, effort recording additional odometric information negligible,
well rewarded fact fewer observations less exploration required
obtaining data sequence sucient adequate learning.
200

fiLearning Geometrically-Constrained HMMs
50

40

30

Odometry

KL
20

10
Odometry Used
0

200

400
Seq. Length

600

800

Figure 21: Average kl divergence function sequence length.

7 Conclusions
Odometric information, often readily available robotics domain, makes possible
learn hidden Markov models eciently effectively, using shorter training sequences.
importantly, contrast traditional perception viewing topological
geometric models two distinct types entities, shown odometric information
directly incorporated traditional topological hmm model, maintaining
convergence reestimation algorithm local maximum likelihood function.
method uses odometric information two ways. first choose initial model,
based odometric information. iterative procedure, extends Baum-Welch
algorithm, used learn topological model environment learning
additional set constrained geometric parameters. additional set constrained parameters constitutes extension basic hmm/pomdp model transitions observations.
Even though primarily interested underlying topological model (transition
observation probabilities), experiments demonstrate use odometric relations
reduce number iterations amount data required algorithm, improve
resulting model.
initialization procedure enforcement additivity constraint relatively
small models prove helpful topologically geometrically. extensive study (Shatkay,
1999) shows long data sequences, generated large models, enforcing antisymmetry rather additivity, leads better topological models.
cases, initialization always good, additivity may over-constrain learning
unfavorable area. Learning large models may benefit enforcing anti-symmetry
first iterations, complete additivity later iterations. Alternatively, may use
algorithm, enforcing additivity, learn separate models small portions environment,
combining later one complete model. similar idea combining small modelfragments complete map environments applied, context geometrical
maps, recent work Leonard Feder (2000).
201

fiShatkay & Kaelbling

work presented demonstrates domain-specific information constraints
enforced part statistical estimation process, resulting better models, requiring
shorter data sequences. strongly believe idea applied domains
robotics. particular, acquisition hmms use molecular biology may greatly benefit
exploiting geometrical (and other) constraints molecular structures. Similarly, temporal
constraints may exploited domains pomdps appropriate decision-support,
air-trac control medicine.

Acknowledgments
thank Sebastian Thrun insightful comments throughout work, John Hughes Luis Ortiz
helpful advice, Anthony Cassandra code generating random distributions, Bill Smart
sustaining Ramona Jim Kurien providing low level code driving her. presentation
paper benefited comments made anonymous referees grateful.
work done authors Computer Science department Brown University,
supported DARPA/Rome Labs Planning Initiative grant F30602-95-1-0020, NSF grants
IRI-9453383 IRI-9312395, Brown University Graduate Research Fellowship.

202

fiLearning Geometrically-Constrained HMMs

Appendix A. Overview Odometric Learning Algorithm
algorithm takes input experience sequence E = hr; V i, consisting odometric
sequence r observation sequence V , defined beginning Section 4.2.
number states also assumed given.
Learn Odometric HMM(E)
1 Initialize matrices A; B; R
(See Section 5)
2 max change 1
3 ( max change > )
4 Calculate Forward probabilities, ff
(Equation 4)
5
Calculate Backward probabilities, fi
(Equation 5)
6
Calculate state-occupation probabilities, (Equation 6)
7
Calculate State-transition probabilities, ; (Equation 7)
8
Old A; Old B B
9
Reestimate (A)
(Equation 8, left)
10
B Reestimate (B )
(Equation 8, right)
11
R Reestimate (R )
(Equations 12 13)
x

x

12
hR ; R Reestimate(R ; R ) (Equations 10 11)
13
max change MAX(Get Max Change(A; Old );
Get Max Change(B; Old B ))
equations referenced Step 12 correspond updates perpendicularity assumption, global framework used. See (Shatkay, 1999) update formulae within
state-relative framework.
additivity enforced, step 11 followed projection reestimated R onto additive
ane space, described Section 4.3.2. addition, step 12 substituted procedure
described Section 4.3.1. reader referred (Shatkay, 1999) detail.
Get Max Change function takes two matrices returns maximal element-wise
absolute difference them. constant set denote margin error changes
parameters. change parameters \small enough", model regarded
\unchanged".

203

fiShatkay & Kaelbling

References
Abe, N., & Warmuth, M. K. (1992). computational complexity approximating distributions probabilistic automata. Machine Learning, 9 (2), 205{260.
Angluin, D. (1987). Learning regular sets queries counterexamples. Information
Computation, 75, 87{106.
Asada, M. (1991). Map building mobile robot sensory data. Iyengar, S. S., &
Elfes, A. (Eds.), Autonomous Mobile Robots, pp. 312{322. IEEE Computer Society Press.
Bartels, R. (1984). Estimation bidirectional mixture von Mises distributions. Biometrics,
40, 777{784.
Basye, K., Dean, T., & Kaelbling, L. P. (1995). Learning dynamics: System identification
perceptually challenged agents. Artificial Intelligence, 72 (1).
Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). maximization technique occurring
statistical analysis probabilistic functions Markov chains. Annals
Mathematical Statistics, 41 (1), 164{171.
Cassandra, A. R., Kaelbling, L. P., & Kurien, J. A. (1996). Acting uncertainty: Discrete
Bayesian models mobile-robot navigation. Proceedings IEEE/RSJ International
Conference Intelligent Robots Systems.
DeGroot, M. H. (1986). Probability Statistics (2nd edition). Addison-Wesley.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete
data via EM algorithm. Journal Royal Statistical Society, 39 (1), 1{38.
Dissanayake, G., Newman, P., Clark, S., Durrant-Whyte, H. F., & Csorba, M. (2001). solution
simultaneous localization map building (SLAM) problem. IEEE Transactions
Robotics Automation, 17 (3).
Duda, R. O., & Hart, P. E. (1973). Unsupervised Learning Clustering, chap. 6. John Wiley
Sons.
Elfes, A. (1989). Using occupancy grids mobile robot perception navigation. Computer,
Special Issue Autonomous Intelligent Machines, 22 (6), 46{57.
Engelson, S. P., & McDermott, D. V. (1992). Error correction mobile robot map learning.
Proceedings IEEE International Conference Robotics Automation, pp.
2555{2560, Nice, France.
Gold, E. M. (1978). Complexity automaton identification given data. Information
Control, 37, 302{320.
Gumbel, E. G., Greenwood, J. A., & Durand, D. (1953). circular normal distribution:
Theory tables. American Statistical Society Journal, 48, 131{152.
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction Automata Theory, Languages,
Computation. Addison & Wesley.
204

fiLearning Geometrically-Constrained HMMs

Juang, B. H. (1985). Maximum likelihood estimation mixture multivariate stochastic observations Markov chains. AT&T Technical Journal, 64 (6).
Juang, B. H., & Rabiner, L. R. (1985). probabilistic distance measure hidden Markov
models. AT&T Technical Journal, 64 (2), 391{408.
Koenig, S., & Simmons, R. G. (1996a). Passive distance learning robot navigation.
Proceedings Thirteenth International Conference Machine Learning, pp. 266{
274.
Koenig, S., & Simmons, R. G. (1996b). Unsupervised learning probabilistic models robot
navigation. Proceedings IEEE International Conference Robotics Automation.
Kuipers, B., & Byun, Y.-T. (1991). robot exploration mapping strategy based semantic hierarchy spatial representations. Journal Robotics Autonomous Systems,
8, 47{63.
Kullback, S., & Leibler, R. A. (1951). information suciency. Annals Mathematical
Statistics, 22 (1), 79{86.
Leonard, J., Durrant-Whyte, H. F., & Cox, I. J. (1991). Dynamic map building autonomous mobile robot. Iyengar, S. S., & Elfes, A. (Eds.), Autonomous Mobile Robots,
pp. 331{338. IEEE Computer Society Press.
Leonard, J. J., & Feder, H. J. S. (2000). computationally ecient method large-scale concurrent mapping localization. Hollerbach, J., & Kodischek, D. (Eds.), Proceedings
Ninth International Symposium Robotics Research.
Liporace, L. A. (1982). Maximum likelihood estimation multivariate observations Markov
sources. IEEE Transactions Information Theory, 28 (5).
Mardia, K. V. (1972). Statistics Directional Data. Academic Press.
Mataric, M. J. (1990). distributed model mobile robot environment-learning navigation. Master's thesis, MIT, Artificial Intelligence Laboratory.
McLachlan, G. J., & Krishnan, T. (1997). EM Algorithm Extensions. John Wiley &
Sons.
Moravec, H. P. (1988). Sensor fusion certainty grids mobile robots. AI Magazine, 9 (2),
61{74.
Moravec, H. P., & Elfes, A. (1985). High resolution maps wide angle sonar. Proceedings
International Conference Robotics Automation, pp. 116{121.
Nourbakhsh, I., Powers, R., & Birchfield, S. (1995). Dervish: oce-navigating robot. AI
Magazine, 16 (1), 53{60.
Pierce, D., & Kuipers, B. (1997). Map learning uninterpreted sensors effectors. Artificial Intelligence, 92 (1-2), 169{227.
205

fiShatkay & Kaelbling

Rabiner, L. R. (1989). tutorial hidden Markov models selected applications speech
recognition. Proceedings IEEE, 77 (2), 257{285.
Rivest, R. L., & Schapire, R. E. (1987). Diversity based inference finite automata.
Proceedings IEEE Twenty Eighth Annual Symposium Foundations Computer
Science, pp. 78{87, Los Angeles, California.
Rivest, R. L., & Schapire, R. E. (1989). Inference finite automata using homing sequences.
Proceedings Twenty First Annual Symposium Theory Computing, pp. 411{420,
Seattle, Washington.
Ron, D., Singer, Y., & Tishbi, N. (1994). Learning probabilistic automata variable memory length. Proceedings Seventh Annual Workshop Computational Learning
Theory, pp. 35{46.
Ron, D., Singer, Y., & Tishbi, N. (1995). learnability usage acyclic probabilistic
finite automata. Proceedings Eighth Annual Workshop Computational Learning
Theory, pp. 31{40.
Ron, D., Singer, Y., & Tishby, N. (1998). learnability usage acyclic probabilistic
finite automata. Journal Computer Systems Science, 56 (2).
Shatkay, H. (1999). Learning Models Robot Navigation. Ph.D. thesis, Department Computer Science, Brown University, Providence, RI.
Shatkay, H., & Kaelbling, L. P. (1997). Learning topological maps weak local odometric
information. Proceedings Fifteenth International Joint Conference Artificial
Intelligence, Nagoya, Japan.
Shatkay, H., & Kaelbling, L. P. (1998). Heading right direction. Proceedings
Fifteenth International Conference Machine Learning, Madison, Wisconsin.
Simmons, R. G., & Koenig, S. (1995). Probabilistic navigation partially observable environments. Proceedings International Joint Conference Artificial Intelligence.
Smith, R., Self, M., & Cheeseman, P. (1991). stochastic map uncertain spatial relationships. Iyengar, S. S., & Elfes, A. (Eds.), Autonomous Mobile Robots, pp. 323{330. IEEE
Computer Society Press.
Thrun, S. (1999). Learning metric-topological maps indoor mobile robot navigation. AI
Journal, 1, 21{71.
Thrun, S., & Bucken, A. (1996a). Integrating grid-based topological maps mobile robot
navigation. Proceedings Thirteenth National Conference Artificial Intelligence,
pp. 944{950.
Thrun, S., & Bucken, A. (1996b). Learning maps indoor mobile robot navigation. Tech. rep.
CMU-CS-96-121, School Computer Science, Carnegie Mellon University, Pittsburgh,
PA.
Thrun, S., Burgard, W., & Fox, D. (1998a). probabilistic approach concurrent map acquisition localization mobile robots. Machine Learning, 31, 29{53.
206

fiLearning Geometrically-Constrained HMMs

Thrun, S., Gutmann, J.-S., Fox, D., Burgard, W., & Kuipers, B. J. (1998b). Integrating topological metric maps mobile robot navigation: statistical approach. Proceedings
Fifteenth National Conference Artificial Intelligence, pp. 989{995.
Vapnik, V. N. (1995). Nature Statistical Learning Theory. Springer.

207

fiJournal Artificial Intelligence Research 16 (2002) 389-423

Submitted 2/02; published 6/02

Communicative Multiagent Team Decision Problem:
Analyzing Teamwork Theories Models

David V. Pynadath
Milind Tambe

Information Sciences Institute Computer Science Department

pynadath@isi.edu
tambe@usc.edu

University Southern California
4676 Admiralty Way, Marina del Rey, CA 90292 USA

Abstract

Despite significant progress multiagent teamwork, existing research address optimality prescriptions complexity teamwork problem. Without characterization optimality-complexity tradeoffs, impossible determine
whether assumptions approximations made particular theory gain enough
eciency justify losses overall performance. provide tool use multiagent researchers evaluating tradeoff, present unified framework, COMmunicative Multiagent Team Decision Problem (COM-MTDP). COM-MTDP model
combines extends existing multiagent theories, decentralized partially observable Markov decision processes economic team theory. addition generality
representation, COM-MTDPs also support analysis optimality team
performance computational complexity agents' decision problem. analyzing complexity, present breakdown computational complexity constructing
optimal teams various classes problem domains, along dimensions observability communication cost. analyzing optimality, exploit COM-MTDP's
ability encode existing teamwork theories models encode two instantiations
joint intentions theory taken literature. Furthermore, COM-MTDP model
provides basis development novel team coordination algorithms. derive
domain-independent criterion optimal communication provide comparative analysis two joint intentions instantiations respect optimal policy.
implemented reusable, domain-independent software package based COM-MTDPs
analyze teamwork coordination strategies, demonstrate use encoding
evaluating two joint intentions strategies within example domain.
1. Introduction

central challenge control coordination distributed agents enabling
work together, team, toward common goal. teamwork critical vast
range domains|for future teams orbiting spacecraft, sensors tracking targets, unmanned vehicles urban battlefields, software agents assisting organizations rapid
crisis response, etc. Research teamwork theory built foundations successful
practical agent team implementations domains. forefront theories based
belief-desire-intentions (BDI) frameworks, joint intentions (Cohen & Levesque,
1991b, 1991a; Levesque, Cohen, & Nunes, 1990), SharedPlans (Grosz, 1996; Grosz & Kraus,
1996; Grosz & Sidner, 1990), others (Sonenberg, Tidhar, Werner, Kinny, Ljungberg,
& Rao, 1994; Dunin-Keplicz & Verbrugge, 1996), provided prescriptions co c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiPynadath & Tambe

ordination practical systems. theories inspired construction practical, domain-independent teamwork models architectures (Jennings, 1995; Pynadath,
Tambe, Chauvat, & Cavedon, 1999; Rich & Sidner, 1997; Tambe, 1997; Yen, Yin, Ioerger,
Miller, Xu, & Volz, 2001), successfully applied range complex domains.
Yet, two key shortcomings limit scalability BDI-based theories implementations. First, techniques quantitative evaluation degree
optimality coordination behavior. optimal teamwork may impractical
real-world domains, analysis would aid us comparison different theories/models
identifying feasible improvements. One key reason diculty quantitative
evaluation existing teamwork theories ignore various uncertainties costs real-world environments. instance, joint intentions theory (Cohen &
Levesque, 1991b) prescribes team members attain mutual beliefs key circumstances,
ignores cost attaining mutual belief (e.g., via communication). Implementations blindly follow prescriptions could engage highly suboptimal coordination.
hand, practical systems addressed costs uncertainties real-world
environments. instance, STEAM (Tambe, 1997; Tambe & Zhang, 1998) extends joint
intentions decision-theoretic communication selectivity. Unfortunately, pragmatism approaches often necessarily leads lack theoretical rigor, remains
unanswered whether STEAM's selectivity best agent do, whether even
necessary all. second key shortcoming existing teamwork research lack
characterization computational complexity various aspects teamwork decisions. Understanding computational advantages practical coordination prescription
could potentially justify use prescription approximation optimality
particular domains.
address shortcomings, propose new complementary framework, COMmunicative Multiagent Team Decision Problem (COM-MTDP), inspired work economic team theory (Marschak & Radner, 1971; Yoshikawa, 1978; Ho, 1980).
COM-MTDP model borrows theory developed another field, make several
contributions applying extending original theory, notably adding explicit
models communication system dynamics. extensions, COM-MTDP
generalizes recently developed multiagent decision frameworks, decentralized
POMDPs (Bernstein, Zilberstein, & Immerman, 2000).
definition team (like economic team theory) assumes team
members common goal work sel essly towards goal (i.e.,
private goals own). terms decision-theoretic framework,
assume team members share joint utility function|that is,
team member's individual preferences exactly preferences members and,
thus, team whole. definition may appear \bare-bones" definition
team, since include common concepts assumptions literature
constitutes team (e.g., teammates form joint commitment (Cohen & Levesque,
1991b), attain mutual belief upon termination joint goal, intend teammates succeed tasks (Grosz & Kraus, 1996), etc.). COM-MTDP perspective,
view concepts intermediate concepts, means agents improve
team's overall performance, rather ends themselves. hypothesis
investigation COM-MTDP-based analysis provide concrete justifications
390

fiThe Communicative Multiagent Team Decision Problem

concepts. example, mutual belief inherent value, COM-MTDP
model quantify improved performance would expect team
attains mutual belief important aspects execution.
generally, paper demonstrates three new types teamwork analyses made
possible COM-MTDP model. First, analyze computational complexity
teamwork within subclasses problem domains. instance, researchers advocated teamwork without communication (Goldberg & Mataric, 1997). use COMMTDP model show that, general, problem constructing optimal teams without
communication NEXP-complete, allowing free communication reduces problem
PSPACE-complete. paper presents breakdown complexity optimal
teamwork problem domains classified along dimensions observability communication cost.
Second, COM-MTDP model provides powerful tool comparing optimality
different coordination prescriptions across classes domains. Indeed, illustrate
encode existing team coordination strategies within COM-MTDP evaluation.
analysis, selected two joint intentions-based approaches literature: one
using approach realized within GRATE* joint responsibility model (Jennings,
1995), another based STEAM (Tambe, 1997). encoding, derive
conditions team coordination strategies generate optimal team behavior,
complexity decision problems addressed them. Furthermore, also
derive novel team coordination algorithm outperforms existing strategies
optimality, though eciency. end result well-grounded characterization
complexity-optimality tradeoff among various means team coordination.
Third, use COM-MTDP model empirically analyze specific domain
interest. implemented reusable, domain-independent algorithms allow one
evaluate optimality behavior generated different prescriptive policies within
problem domain represented COM-MTDP. apply algorithms example
domain empirically evaluate aforementioned team coordination strategies, characterizing optimality strategy function properties underlying
domain. instance, Jennings reports experimental results (Jennings, 1995) indicating
joint responsibility teamwork model leads lower waste community effort
competing methods accomplishing teamwork. COM-MTDP model,
able demonstrate benefits Jennings' approach many configurations example domain. However, precisely characterizing types domains showed
benefits, also identified domains competing methods may actually perform
better. addition, use COM-MTDP model re-create explain previous
work noted instance suboptimality STEAM-based, real-world implementation (Tambe, 1997). previous work treated suboptimality anomalous,
COM-MTDP re-evaluation domain demonstrated observed suboptimality
symptom STEAM's general propensity towards extraneous communication
significant range domain types. algorithms example domain model
available public use Online Appendix 1.
Section 2 presents COM-MTDP model's representation places context
related multiagent models literature. Section 3 uses COM-MTDP model
define characterize complexity designing optimal agent teams. Section 4 analyzes
391

fiPynadath & Tambe

optimality existing team coordination algorithms derives novel coordination
algorithm. Section 5 presents empirical results applying COM-MTDP algorithms
example domain. Section 6 summarizes results, Section 7 identifies
promising future directions.
2. COM-MTDP Model

section defines describes COM-MTDP model ability represent
important aspects multiagent teamwork. begin Section 2.1 defining
underlying multiagent team decision problem explicit communication. Section 2.2
defines complete COM-MTDP model extension explicitly represent communication. Section 2.3 provides illustration COM-MTDP model represents
execution team agents. Finally, Section 2.4 describes related models multiagent
coordination shows COM-MTDP model generalizes them.
2.1 Multiagent Team Decision Problems
Given team sel ess agents, ff, intend perform joint task, wish evaluate
possible policies behavior. represent multiagent team decision problem (MTDP)
model tuple, hS; Aff; P;
ff ; ; Bff ; Ri. taken underlying components
model initial team decision model (Ho, 1980), extended
handle dynamic decisions time easily represent multiagent domains (in
particular, agent beliefs). assume model common knowledge
team members. words, agents believe model, believe
believe model, etc.
2.1.1 World States:



= 1 m: set world states, expressed factored representation (a

cross product separate features).
state world state team's environment (e.g., terrain, location
enemy). Thus, represents domain individual feature environment,
represents domain possible combinations values individual
features.
2.1.2 Domain-Level Actions:

Aff

fAi gi2ff set actions agent perform change environment, implicitly
defining set combined actions, Aff Qi2ff Ai (corresponding team theory's decision
variables).
Extension Dynamic Problem: P original team decision problem focused
one-shot, static problem. extend original concept component
time series random variables. effects domain-level actions (e.g., ying action
changes helicopter's position) obey probabilistic distribution, given function P :
Aff ! [0; 1]. words, initial state time t, combined action
392

fiThe Communicative Multiagent Team Decision Problem

taken time t, final state s0 time + 1, Pr(S t+1 = s0jS = s; Atff = a) = P (s; a; s0).
given definition P assumes world dynamics obey Markov assumption.
2.1.3 Agent Observations:
ff
f
igi2ff set observations agent, i, experience world, implicitly
defining combined observation,
ff Qi2ff
i.
may include elements corresponding
indirect evidence state (e.g., sensor readings) actions agents (e.g.,
movement helicopters). original team-theoretic framework, information
structure represented observation process agents set deterministic
functions, Oi : !
i.
Extension Allowable Information Structures: extend information
structure representation allow uncertain observations. use general stochastic
model, borrowed partially observable Markov decision process model (Smallwood &
Sondik, 1973), joint observation function: (s; a; !) = Pr(
tff = !jS = s; Atff 1 =
a). function models sensors, representing errors, noise, etc. cases,

Q
separate joint distribution individual observation functions: i2ff Oi ,
Oi (s; a; !) = Pr(
ti = !jS = s; Atff 1 = a). Thus, probability distribution
specified forms richer information structure used model. make
useful distinctions different classes information structures:
Collective Partial Observability general case, make assumptions observations.
Collective Observability unique world state fort combined observations
team: 8! 2
ff, 9s 2 8s0 6= s, Pr(
ff = !jS = s0) = 0. set
domains collectively observable strict subset domains
collectively partially observable.
Individual Observability unique world state individual agent's observations: 8! 2
i, 9s 2 8s0 6= s, Pr(
ti = !jS = s0) = 0. set
domains individually observable strict subset domains
collectively observable.
Non-Observability agents receive feedback world: 9! 2
i,
8s 2 8a 2 Aff , Pr(
ti = !jS = s; Atff 1 = a) = 1. assumption holds
open-loop systems, come frequent consideration classical planning (Boutilier, Dean, & Hanks, 1999).
2.1.4 Policy (Strategy) Space

iA domain-level policy (or strategy, original team theory specification) map
agent's belief state action. original formalism, agent's beliefs correspond
directly observations (i.e., iA :
! Ai).
Extension Richer Belief State Space: Bff generalize set possible strategies capture complex mental states agents. agent, 2 ff, forms
belief state, bti 2 Bi, based observations seen time t, Bi circumscribes

393

fiPynadath & Tambe

set possible belief states agent. Thus, define set possible domainlevel policies mappings belief states actions, iA : Q
Bi ! Ai . define set
possible combinedt belief states agents Bff i2ff Bi. corresponding
random variable, bff, represents agents' combined belief state time t. elaborate
different types belief states mapping observations belief states (i.e.,
state estimator function) Section 2.2.1.
2.1.5 Reward Function:

R

common reward function central notion teamwork MTDP: R : Aff !
R. function represents team's joint preferences states cost domainlevel actions (e.g., destroying enemy good, returning home base 10%
original force bad). assume that, sel ess team members, agent shares
preferences individual level well. Therefore, team member wants exactly
best team whole.
2.2 Extension Explicit Communication: ff
make explicit separation domain-level actions (Aff) communicative
actions. defined section, communicative actions affect receiving agents' individual belief states, but, unlike domain-level actions, directly change world
state. Although distinction sometimes blurry real-world domains, make
explicit separation isolate, much possible, effects two types
actions. leverage gained separation provides basis informative,
analytical results presented rest paper. capture separation, extend
initial MTDP model communicative multiagent team decision problem (COMMTDP), define tuple, hS; Aff; ff ; P;
ff; Off; B ff; Ri, new component,
ff, extended reward function, R.
2.2.1 Communication: ff
figi2ff set possible messages agent, implicitly defining set combined
communications, ff Qi2ff i. agent, i, may communicate message x 2
teammates, interpret communication updating belief states response.
first step work, assume agents receive messages instantaneously
correctly (i.e., lag noise communication channels). model
common knowledge among team members, agent sent message,
knows team members received message, team members know
knows received message, on.
communication, divide decision epoch two phases: pre-communication post-communication phases, denoted subscripts , respectively.
particular, agents update belief states two distinct points within decision epoch: upon receiving observation
ti (producing pre-communication belief state bti ), upon receiving agents' messages (producing postcommunication belief state bti). distinction allows us differentiate belief
state used agents selecting communication actions \up-to-date"
belief state used selecting domain-level actions. also distinguish
394

fiThe Communicative Multiagent Team Decision Problem

separate state-estimator functions used update phase:
b0i =SEi0 ()
(1)


1

bi =SEi (bi ;
)
(2)



bi =SEi (bi ; ff )
(3)
SEi : Bi
! Bi pre-communication state estimator agent i,
SEi : Bi ff ! Bi post-communication state estimator agent i. initial
state estimator, SEi0 : ; ! Bi, specifies agent's prior beliefs, observations
made. these, also make obvious definitions corresponding
estimators combined belief states: SE ff, SE ff, SE 0ff.
paper, first step, assume agents perfect recall.
words, agents recall observations, well communication
agents. Thus, belief states representtheir entire histories sequences observations received messages: Bi =
ff, X denotes set possible
sequences (of length) elements X . agents realize perfect recall
following state estimator functions:
SEi0 () = hi
(4)


0 0 ff

1 1 ffff
SEi (
; ff ; : : : ;
; ff
;
)


0 0 ff

1 1 ff
ffff
=


; ffff ; : : :
;
; ffff
;
iffff;
(5)
0
0

1

1


SEi (
; ff ; : : : ;
; ff ;
; ; ff )
=


0i ; 0ffff ; : : : ;

ti ; tffffff
(6)
words, SEi0 initializes agent i's belief state empty history, SEi appends
new observation agent i's belief state, SEi appends new messages agent i's belief
state. paper's assumptions perfect recall, three state-estimator functions
take constant time. However, potentially allow complex functions (though
complexity results presented hold state-estimator functions take polynomial
time). instance, although assume perfect, synchronous, instantaneous communication here, could potentially use post-communication state estimator model
noise, temporal delays, asynchrony, cognitive burden, etc. present communication
channel.
extend definition policy behavior include communication policy,
: Bi ! , analogous Section 2.1.4's domain-level policy. define joint policies,
ff ffA , combined policies across agents ff.
2.2.2 Extended Reward Function:

R

extend team's reward function also represent cost communicative acts (e.g.,
communication channels may associated cost): R : Aff ff ! R. assume
cost communication domain-level actions independent other,
decompose reward function two components: communication-level reward,
R : ff ! R, domain-level reward, RA : Aff ! R. total reward
sum two component values: R(s; a; ) = RA(s; a) + R(s; ). assume
395

fiPynadath & Tambe

communication inherent benefit may instead cost,
states, 2 , messages, 2 ff , reward never positive: R(s; ) 0. However,
although assign communication explicit value, significant implicit value
effect agents' belief states and, subsequently, future actions.
observability function, parameterize communication costs associated
message transmissions:
General Communication: make assumptions communication.
Free Communication: R(s; ) = 0 2 ff, 2 . words,
communication actions effect agents' reward.
communication: ff = ;, i.e., explicit communication. Alternatively, communication may prohibitively expensive, 8 2 ff , 2 , R(s; ) = 1.
free-communication case appears literature, researchers wish focus
issues communication cost. Although, real-world domains rarely exhibit
ideal conditions, may able model domains approximately free
communication sucient degree. addition, analyzing extreme case gives us
understanding benefit communication, even results apply across
domains. also identify no-communication case decision problems
interest researchers well (Goldberg & Mataric, 1997). course, even ff = ;,
possible domain-level actions Aff implicit communicative
value acting signals convey information agents. However, still
label agent teams communication purposes work here, since
many results exploit explicit separation domain- communication-level
actions.
2.3 Model Illustration
view evolving state Markov chain separate stages domain-level
communication-level actions. words, agent team member, 2 ff begins
initial state, 0, initial belief states, b0i = SEi0 (). agent receives
observation
0i drawn according probability distribution (S 0; null;
0ff) (there
actions yet). Then, agent updates belief state, b0i = SEi (b0i ;
0i ).
Next, agent 2 ff selects message according communication policy, 0i =
(b0i ), defining combined communication, 0ff . agent interprets communications others updating belief state, b0i = SEi (b0i ; 0ff ).
selects action
according


domain-level
policy,
A0i = iA(b0i ), defining
combined action A0ff . central assumption teamwork, agent receives
joint reward, R0 = R(S 0 ; A0ff ; 0ff). world moves new state, 1 ,
according distribution, P (S 0 ; A0ff ). Again, agent receives observation
1i
drawn
according distribution (S 1 ; A0ff ;
1ff), updates belief state,
b1i = SEi (b0i ;
1i ).
process continues, agents choosing communication- domain-level actions,
observing effects, updating beliefs. Thus, addition time series world
states, 0; 1 ; : : : ; , agents determine time series communication-level
396

fiThe Communicative Multiagent Team Decision Problem

domain-level actions, 0ff ; 1ff ; : : : ; tff A1ff; A1ff ; : : : ; Atff , respectively. also
time series observations agent i,
0i ;
1i ; : : : ;
ti. Likewise, treat
combined observations,
0ff ;
1ff; : : : ;
tff, similar time series random variables.
Finally, agents receive series rewards, R0; R1 ; : : : ; Rt. define value,
V , policies, ffA ff , expected reward received executing
policies. finite horizon, , value equivalent following:
VT

(ffA; ff ) = E

"
X

t=0

fi
fi

Rt fifi
fi

#

ffA ; ff

(7)

2.4 Related Work
COM-MTDP model subsumes many existing multiagent models, presented Table 1 (i.e., map instance models corresponding COM-MTDP).
generality enables us perform novel analyses real-world teamwork domains,
demonstrated Section 4's use COM-MTDP model analyzing optimality
communication decisions.
2.4.1 Decentralized POMDPs

model observability world dynamics, COM-MTDP model closely parallels structure decentralized partially observable Markov decision process (DECPOMDP) (Bernstein et al., 2000). Following notational conventions, DEC-POMDP
tuple, hS; Aff ; P;
ff; ; Ri. set possible messages, ff, DECPOMDP falls class domains communication. DEC-POMDP observational model, O, general enough capture collectively partially observable domains.
2.4.2 Partially Observable Identical Payoff Stochastic Games

Stochastic games provide rich framework multiagent decision making agents
may individual goals preferences. identical payoff stochastic game
(IPSG) restricts agents share single payoff function, appropriate modeling
single, global reward function team context. partially observable IPSG
(POIPSG) (Peshkin, Kim, Meuleau, & Kaelbling, 2000) tuple, hS; Aff ; P;
ff; Off; Ri,
similar DEC-POMDP model. words, observation function, ,
general enough support collectively partially observable domains, communication.
2.4.3 Multiagent MDPs

Another relevant model multiagent Markov decision process (MMDP) (Boutilier,
1996), tuple, hS; Aff; P; Ri, notation. Like DEC-POMDP, MMDP
communication. addition, MMDP multiagent extension completely
observable MDP model, assumes environment individually observable.

397

fiPynadath & Tambe

Model
ff

DEC-POMDP communication collective partial observability
POIPSG
communication collective partial observability
MMDP
communication
individual observability
Xuan-Lesser general communication
collective observability
Table 1: Existing models COM-MTDP subsets.
2.4.4 Xuan-Lesser Framework

COM-MTDP's separation communication actions similar previous
work multiagent decision models (Xuan, Lesser, & Zilberstein, 2001), supported
general communication. However, Xuan-Lesser model generalizes beyond individually observable environments, supports subset collectively observable environments. particular, Xuan-Lesser framework cannot represent agents receive
local observations common world state, observations different agents could
potentially interdependent.
3. COM-MTDP Complexity Analysis

use COM-MTDP model prove results complexity constructing optimal agent teams (i.e., teams coordinate produce optimal behavior
problem domain). problem facing agents (or designer agents)
construct joint policies, ff ffA, maximize joint reward,
represented expected value, V (ffA; ff). results presented,
assume values model instance (e.g., transition probabilities, rewards)
rational numbers, express particular instance finite-sized input.
Theorem 1 decision problem whether exist policies, ff ffA , given
COM-MTDP, general communication collective partial observability, yield
total reward least K finite horizon NEXP-complete jffj 2 (i.e.,
one agent).

Proof: prove COM-MTDP decision problem NEXP-hard, reduce DECPOMDP (Bernstein et al., 2000) COM-MTDP communication copying
model features
given DEC-POMDP.
words,


ff




given DEC-POMDP, S; fA gi=1; P; f
gi=1 ; O; R , construct COM-MTDP,
hS 0 ; fA0i gmi=1 ; 0ff; P 0 ; f
0igmi=1 ; O0ff; B 0ff; R0 i, follows:
S0 =
A0i = Ai
0 = ;
P 0 (s; ha1 ; : : : ; ; s0 ) = P (s0 js; a1 ; : : : ; )
398

fiThe Communicative Multiagent Team Decision Problem


0i =

O0ff (s; ha1 ; : : : ; ; h!1 ; : : : ; !m i) = O(!1 ; : : : ; !m ja1 ; : : : ; ; s)
Bi0 = [Tj=1(
)j
(i.e., observation sequences length finite horizon)
R0 (s; ha1 ; : : : ; ; ) = R(s; a1 ; : : : ; )
DEC-POMDP assumes perfect recall, use state estimator functions
Equations 5 6. Since communication COM-MTDP, fixed
silent policy, ff. translate domain-level policy, ffA, DEC-POMDP
joint policy, , follows:


ff
(oi1 ; : : : ; oit ) iA ( oi1 ; : : : ; oit )
(8)
expected utility following joint policy, , within DEC-POMDP identical
following ff ffA within constructed COM-MTDP. Thus, exists
policy expected utility greater K COM-MTDP
exists one DEC-POMDP. decision problem DEC-POMDP known
NEXP-complete, COM-MTDP problem must NEXP-hard.
show COM-MTDP NEXP, proof proceeds similarly
DEC-POMDP. words, guess joint policy, ff , write
exponential time (we assume jS j). take COM-MTDP plus policy
generate (in exponential time) corresponding MDP state space space
possible combined belief states agents. use dynamic programming
determine (in exponential time) whether ff generates expected reward least K .
2
remainder section, examine effect communication complexity constructing team policies generate optimal behavior. start examining
case condition free communication, would expect benefit
communication greatest. begin with, suppose agent capable
communicating entire observation (i.e.,
i). analyze complexity
team decision problem, first prove agents exploit capability
communicate true observation, long incur cost so:
Theorem 2 free communication, consider team agents using communication
policy: (bti )
ti . domain-level policy ffA maximizes V (ffA ; ff ),
combined policy dominant policies. words, policies, 0ffA
0ff , V ( ffA ; ff ) V ( 0ffA ; 0ff ).
Proof: Suppose communication policy, 0ff, specifies something
complete communication (e.g., keeping quiet, lying). Suppose
domain-level policy, 0ffA , allows team attain expected reward, K ,
used combination 0ff. Then, construct domain-level policy, ffA ,
team attains expected reward, K , used conjunction
complete-communication policy, ff , defined statement Theorem 2.
Thet communication policy, 0ff, produces different set belief states (denoted b0 ti
b0i) ff (denoted bti bti ). particular, use state estimator
399

fiPynadath & Tambe

functions, SEi0 SEi0 defined Equations 5 6 generate b0ti b0ti .
belief state complete history observation communication pairs
agent. hand, complete communication ff, state estimator
functions Equations 5 6 reduce to:


ff


ff
SEi (
0ff ; : : : ;
tff 1 ;
ti ) =
0ff ; : : : ;
tff 1 ;
ti
(9)

0
ff


ff
SEi (
ff ; : : : ;
tff 1 ;
ti ; tff ) =
0ff ; : : : ;
tff 1 ; tff
=

0ff ; : : : ;
tff 1;
tff ff
(10)
Thus, ffA defined different set belief states 0ffA . order determine
equivalent ffA, must first define recursive mapping, m, translates belief
states defined ff defined 0ff :


ff




ffff
mi (bti ) =mi bti1 ;
tff = mi bti1 ;
ti ;
tff
*
*
++


EE


= mi(bti1 );
ti ; 0ff = mi(bti1 );
ti; 0j
=

*

(

mi bti1

*

);
ti;

j 2ff



j 2ff

(

( (

j0 SEj0 mj btj 1

);
tj ))

++

(11)

Given mapping, specify: iA(bti) = iA0 (mi(bti )). Executing domainlevel policy, conjunction communication policy, ff , results identical
behavior execution alternate policies, 0ffA 0ff . Therefore, team following
policies, ffA ff achieve expected value K , 0ffA
0ff . 2
Given dominance complete-communication policy, prove
problem constructing teams coordinate optimally simpler communication
free.
Theorem 3 decision problem determining whether exist policies, ff
ffA , given COM-MTDP free communication collective partial observability, yield total reward least K finite horizon PSPACE-complete.
Proof: prove problem PSPACE-hard, reduce single-agent POMDP
COM-MTDP. particular, given POMDP, hS; A; P;
; O; Ri, construct
COM-MTDP, hS 0 ; A01 ; 01; P 0 ;
01; O10 ; B10 ; R0 i, single-agent team (i.e., ff = f1g):
S0 =
A01 =
01 = ;
P 0 (s; ha1 ; s0 ) = P (s; a1 ; s0 )

01 =

400

fiThe Communicative Multiagent Team Decision Problem
O10 (s; ha1 ; h!1 i) = O(s; a1 ; !1 )

B10 =

[Tj=1(
)j
(i.e., observation sequences length finite horizon)
RA0 (s; ha1 i) = R(s; a1 )

R0 (s; ) =

0
COM-MTDP satisfies assumption free communication. POMDP assumes
perfect recall, use state estimator functions Equations 5 6.
proof Theorem 1, show exists policy expected utility greater
K COM-MTDP exists one POMDP. decision
problem POMDP known PSPACE-hard (Papadimitriou & Tsitsiklis, 1987),
COM-MTDP problem free communication must PSPACE-hard.
show problem PSPACE, take COM-MTDP free communication reduce single-agent POMDP. particular, given COM-MTDP,
hS; Aff; ff ; P;
ff; Off; B ff; Ri, construct single-agent POMDP, hS 0; A0 ; P 0 ;
0; O0 ;
R0 i, follows:
S0 =
0 = Aff
P 0 (s; a; s0 ) = P (s; a; s0 )

0 =
ff
O0 (s; a; ! ) = (s; a; ! )
R0 (s; a) = RA (s; a)
Theorem 2, need consider complete-communication policy
COM-MTDP policy zero reward. Therefore, decision problem
COM-MTDP simply find domain-level policy produces expected reward
exceeding K . Given full communication, state estimator functions COM-MTDP
(as shown proof Theorem 2) reduce Equation 10. policy POMDP
specifies action every history observations: 0 : [Tj=1(
0 )j ! A0 .
history observations single-agent POMDP corresponds belief states
COM-MTDP full communication. Therefore, translate POMDP-policy, 0 ,
equivalent domain-level policy COM-MTDP:
(h! 0 ; ! 1 ; : : : ; ! i) 0 (h! 0 ; ! 1 ; : : : ; ! i)
(12)
team following perform exact domain-level actions single agent
following 0 . Thus, exists policy expected utility greater K COMMTDP exists one POMDP. decision problem POMDP
known PSPACE (Papadimitriou & Tsitsiklis, 1987), COM-MTDP problem
(under free communication) must PSPACE well. 2
401

fiPynadath & Tambe

Theorem 4

decision problem determining whether exist policies, ff
ffA , given COM-MTDP free communication collective observability,
yield total reward least K finite horizon P-complete.

Proof: proof follows Theorem 3, reduction MDP
decision problem, rather POMDP. MDP decision problem P-complete (Papadimitriou & Tsitsiklis, 1987). 2
Theorem 5 decision problem determining whether exist policies, ff
ffA , given COM-MTDP individual observability, yield total reward
least K finite horizon (given integers K ) P-complete.

Proof: proof follows Theorem 4, except reduce problem
MDP regardless communication policy team uses. 2
Theorem 6 decision problem determining whether exist policies, ff
ffA , given COM-MTDP non-observability, yield total reward least K
finite horizon (given integers K ) NP-complete.

Proof: proof follows Theorem 4, except reduce problem
single-agent non-observable MDP (NOMDP) regardless communication
policy team uses. particular, agents equally ignorant state,
communication effect. NOMDP decision problem NP-complete (Papadimitriou & Tsitsiklis, 1987). 2
Thus, used COM-MTDP framework characterize diculty problem
domains agent teamwork along dimensions communication cost observability.
Table 2 summarizes results, use deciding concentrate
energies attacking teamwork problems. use results draw conclusions
challenges designers multiagent teams:
greatest challenges lie domains either collective observability
collective partial observability nonzero communication cost.
collective observability collective partial observability, teamwork without
communication highly intractable, but, free communication, complexity
becomes par single-agent planning problems.
Agent team designers much gain increasing observational capabilities
team (e.g., adding new sensor agents) reduction complexity
gained making domain collectively observable.
Furthermore, results Theorems 3 4 hold domain result
Theorem 2 holds (i.e., complete communication dominant policy).
Therefore, perfectly free communication may rare, results show
investment communication teamwork pay significant simplification
optimal teamwork.
402

fiThe Communicative Multiagent Team Decision Problem

Individually Collectively
Collectively
Observable Observable Partially Observable
Comm. P-complete NEXP-complete NEXP-complete
General Comm. P-complete NEXP-complete NEXP-complete
Free Comm. P-complete P-complete
PSPACE-complete

NonObservable
NP-Complete
NP-Complete
NP-Complete

Table 2: Time complexity COM-MTDPs.
hand, world individually observable non-observable, com-

munication makes difference performance.
noted even conditions problem P-complete,
complexity optimal teamwork polynomial number states
world, may still impractically high.
complexity results pertain finding policies optimal subject
domain properties. find different expected rewards optimal policies
different observability communication properties. instance, cutting
agents' sensors makes domain non-observable reduces complexity
generating optimal policy NEXP NP, would expect associated
drop expected reward achieved team.

4. Evaluating Team Coordination

Table 2 shows providing optimal domain-level communication policies teams
dicult challenge. Many systems alleviate diculty domain experts provide domain-level plans (Tambe, 1997; Tidhar, 1993). Then, problem agents
reduces generating appropriate team coordination, ff , ensure properly execute domain-level plans, ffA. section, demonstrate COM-MTDP
framework's ability analyze existing teamwork approaches literature. methodology analysis begins encoding teamwork method communicationlevel policy. words, translate method algorithm maps agent
beliefs (e.g., observation sequences) communication decisions. evaluate performance policy, instantiate COM-MTDP represents states,
transition probabilities, reward function domain interest. methodology
provides evaluation policy terms expected reward earned team
following policy specified domain.
demonstrate methodology using COM-MTDP framework analyze joint
intentions theory (Cohen & Levesque, 1991b, 1991a; Levesque et al., 1990), provides
common basis many existing approaches team coordination. Section 4.1 models two
key instantiations joint intentions taken literature (Jennings, 1995; Tambe, 1997)
COM-MTDP communication policies. Section 4.2 analyzes conditions
policies generate optimal behavior provides third candidate policy makes
communication decisions locally optimal within context joint intentions.
403

fiPynadath & Tambe

addition providing results particular team coordination strategies investigated,
section also illustrates general methodology one use COM-MTDP
framework encode evaluate coordination strategies proposed existing multiagent
research.
4.1 Joint Intentions COM-MTDP
Joint intention theory provides prescriptive framework multiagent coordination
team setting. make claims optimality teamwork, provides
theoretical justifications prescriptions, grounded attainment mutual belief
among team members. use COM-MTDP framework identify domain
properties attaining mutual belief generates optimal behavior quantify
precisely suboptimal performance otherwise.
Joint intentions theory requires team members jointly commit joint persistent
goal, G. also requires team member privately believes G achieved
(or unachievable irrelevant), must attain mutual belief throughout team
achievement (or unachievability irrelevance). encode prescription
joint intentions theory within COM-MTDP model, first specify joint goal, G,
subset states, G , desired goal achieved (or unachievable irrelevant).
Presumably, prescription indicates joint intentions specifically intended individually observable environments. Upon achieving goal individually
observable environment, agent would simultaneously observe 2 G.
assumption COM-MTDP model components (including Off) common
knowledge team, agent would also simultaneously come believe teammates observed 2 G, teammates believe believes
team members observed 2 G, on. Thus, team immediately
attains mutual belief achievement goal individual observability without
additional communication necessary team.
Instead, joint intention framework aims domains degree unobservability. domains, agents must signal agents, either communication informative domain-level action, attain mutual belief. However,
also assume joint intention theory focus domains free communication,
Theorem 2 shows simply agents communicate everything,
time, without need complex prescriptions.
joint intention framework specify precise communication policy
attainment mutual belief. paper, focus communication case
goal achievement, methodology extends handle unachievability irrelevance
well. One well-known approach (Jennings, 1995) applied joint intentions theory
agents communicate achievement joint goal, G, soon believe G
true. instantiate behavior Jennings' agents within COM-MTDP, construct
communication policy, Jff, specifies agent sends special message, G,
first believes G holds. Following joint intentions' assumption sincerity (Smith &
Cohen, 1996), require agents never select special G message belief
state unless believe G true certainty. requirement
assumption team's common knowledge communication model, assume
404

fiThe Communicative Multiagent Team Decision Problem

agents immediately accept special message, G, true,
agents know team members accept message true, on. Thus,
team attains mutual belief G true immediately upon receiving message, G.
construct communication policy, Jff , constant time.
STEAM algorithm another instantiation joint intentions success
several real-world domains (Tambe, 1997; Pynadath et al., 1999; Tambe, Pynadath, Chauvat, Das, & Kaminka, 2000; Pynadath & Tambe, 2002). Unlike Jennings' instantiation,
STEAM teamwork model includes decision-theoretic communication selectivity. domain
specification includes two parameters joint commitment, G: , probability
miscoordinated termination G; Cmt , cost miscoordinated termination G.
context, \miscoordinated termination" means agents immediately observe
team achieved G rest not. STEAM's domain specification also
includes third parameter, Cc, represent cost communication fact (e.g.,
achievement G). Using parameters, STEAM algorithm evaluates whether
expected cost miscoordination outweighs cost communication. STEAM expresses
criterion following inequality: Cmt > Cc. define communication
policy, Sff based criterion: inequality holds, agent observed
achievement G send message, G; otherwise, not. construct
Sff constant time.
4.2 Locally Optimal Policy
Although STEAM policy selective Jennings', remains unanswered
whether optimally selective, researchers continue struggle question
agents communicate (Yen et al., 2001). reports suboptimal
(in particular, excessive) communication STEAM characterized phenomenon
exceptional circumstance, also possible STEAM's optimal performance
exception. use COM-MTDP model derive analytical characterization optimal communication here, Section 5 provides empirical one creating algorithm
using characterization.
policies, Jff, Sff consider sending G agent first believes
G achieved. agent relevant belief, make different choices,
consider optimal decision point. domain individually
observable, certain agents may unaware achievement G. sending
G message, unaware agents may unnecessarily continue performing actions
pursuit achieving G. performance extraneous actions could potentially
incur costs lead lower utility one would expect sending G message.
decision send G matters team achieves G one agent
comes know fact. define random variable, TG, earliest time
agent knows fact. denote agent KG agent knows
achievement time TG . KG = i, agent, i, TG = t0 , agent
pre-communication belief state, bti0 = fi , indicates G achieved.
precisely quantify difference agent sending G message time TG vs.

405

fiPynadath & Tambe

never sending it, define following value:
(t0; i; fi ) E


"T
X0

t=0
"T
X0

E

fi
fi

= G; TG = t0 ; KG =

Rt0 +t fifi ti0
fi

t=0

fi
fi
Rt0 +t fifi ti0
fi

i; bti0

=fi

#

= null; TG = t0 ; KG = i; bti0 = fi

#

(13)

assume that, times TG, agents follow communication policy,
never specifies G. Thus, measures difference expected reward
hinges agent i's specific decision send send G time t0 . Given definition,
locally optimal agent send special message, G, time t0,
0. define communication policy, ff+ , communication policy
following ff agents times, except agent belief state fi ,
agent sends message . definition, ff+G , policy agent
communicates achievement G, ff+null policy not.
Therefore, alternatively describe agent i's decision criterion choosing ff+G
ff+null 0.
Unfortunately, Equation 13 identifies exact criterion locally optimal communication, criterion yet operational. words, directly implement
communication policy agents. Furthermore, Equation 13 hides underlying complexity computation involved, one key goals analysis.
Therefore, use COM-MTDP model derive operational expression 0.
simplicity, define notational shorthand various sequences combinations
values. define partial sequence random variables, X <t , sequence random variables times t: X 0 , X 1 , : : : , X 1 . make similar definitions
>t , X , etc.). expression, (S )T , denotes cross
relational operators (i.e., XQ
product states world, Tt=0 , distinguished time-indexed random
variable, , denotes value state time . notation, st0 [t], specifies
element slot within vector st0 . define function, , shorthand within
probability expressions. allows us compactly represent particular subsequence
world agent belief states occurring, conditioned current situation, follows:
fi
Pr
t; t0ff ; s; fi Pr(S t;t0 = s; bff t;t0 = fi fiTG = t0 ; KG = i; bti0 = fi )
(14)
Informally, (ht; t0i ; s; fi ) represents event world belief states time
t0 correspond specified sequences, fi , respectively, conditioned
agent first know G's achievement time t0 belief state, fi . define
function, fi, map pre-communication belief state post-communication
belief state arises communication policy:
fi (fi ; ff ) SE ff (fi ; ff (fi ))
(15)
definition fi well-defined function deterministic nature
policy, ff, state-estimator function, SE ff .
ff ,

406

fiThe Communicative Multiagent Team Decision Problem

Theorem 7

assume that, upon achievement G, communication G
possible, condition (t0 ,i,fi ) 0 holds if:
X

X

Pr((h0; t0 ; st0 ; fit0 ))



st0 2(S )t0 fi
0 2B ff0
0

B
@



X

X

fi



Pr (ht0 ; ; st0 ; fit0 ) fifiti0 = G ; (h0; t0 ; st0 ; fit0 )




+1
st0 2(S )T t0 +1 fi
0 2B ff 0





X
RA st0 [t]; ffA fi fi t0 [t]; ff+G
t=t0
fi


X
X
Pr (ht0 ; ; st0 ; fit0 ) fifiti0 = null; (h0; t0 ; st0 ; fit0 )

+1
st0 2(S )T t0 +1 fi
0 2B ff 0
!





X


RA st0 [t]; ffA fi fi 0 [t]; ff+null
t=t0
X X

s2G fi2Bff

Pr ((ht0 ; t0 ; s; fi)) R (s; G )

(16)

Proof: complete proof following theorem appears Online Appendix 1.
definition Equation 13 difference two expectations,
expectation sum possible trajectories agent team. trajectory must
includes sequence possible world states, since agents' reward point time
depends particular state world time. agents' reward also depends
actions (both domain- communication-level). actions deterministic,
given agents' policies, ffA , belief states. Thus, addition summing
possible states world, must also sum possible states agents'

407

fiPynadath & Tambe

beliefs (both pre- post-communication):
(t0; i; fi )
X
X
X
=
Pr = sT ; b = fi ; b = fiT
sT 2(S )T fi 2(B)T fi 2(B)T

jti0 = G; TG = t0; KG = i; bti0 = fi


X

X

X


X
t=0

sT 2(S )T fi 2(B)T fi 2(B)T





R(sT [t]; (fi [t]); (fi [t]))

Pr



= sT ; b = fi ; bT = fiT

jti0 = null; TG = t0; KG = i; bti0 = fi


X
t=0



R(sT [t]; (fi [t]); (fi [t]))

(17)

rewrite summations simply using various shorthand notations:
=

X

X

sT 2(S )T fi 2(B)T

Pr((h0; ; s; fi )jti0 = G)


X

X


X
t=0

sT 2(S )T fi 2(B)T



R(sT [t]; (fi (fi [t]; G )); G (fi [t]))

Pr((h0; ; s; fi )jti0 = null)


X
t=0

R(sT [t]; (fi (fi [t]; null)); null(fi [t]))

(18)

remaining derivation exploits Markovian assumptions rearrange summations
cancel like terms produce theorem's result. 2
Theorem 7 states, informally, prefer sending G whenever cost execution achieving G outweighs cost communication fact G
achieved. precisely, outer summations left-hand side inequality
iterate possible past histories world belief states, producing probability
distribution possible states team time t0. state,
expression inside parentheses computes difference domain-level reward,
possible future sequences world belief states, sending sending G.
theorem's assumption communication G possible G
achieved, ignore communication costs future. However, relax
assumption, extend left-hand side straightforward manner longer
408

fiThe Communicative Multiagent Team Decision Problem

Individually
Observable
Comm.

(1)
General Comm.

(1)
Free Comm.

(1)

Collectively
Collectively
NonObservable Partially Observable Observable

(1)

(1)

(1)


O((jS j j
ff j) )
O((jS j j
ff j) )

(1)

(1)

(1)

(1)

Table 3: Time complexity locally optimal decision.
expression accounts difference future communication costs well. Thus,
left-hand side captures intuition that, communicating, team incur
cost agents unaware G's achievement. right-hand side
inequality summation cost sending G message possible current states
belief states.
use Theorem 7 derive locally optimal communication decision across
various classes problem domains. communication, cannot send G.
free communication, right-hand side 0, inequality always true, know
prefer sending G. assumptions communication, determination
complicated. domain individually observable, left-hand side becomes
0, agents know G achieved (and thus difference
execution sending G). Therefore, inequality always false (unless free
communication), prefer sending G . environment individually
observable communication available free, then, locally optimal time
t0 , agent must evaluate Inequality 16 full complexity. Since inequality sums
rewards possible sequences states observations, time complexity
corresponding algorithm O((jS jj
ffj)T ). complexity unacceptable
real-world problems, still provides exponential savings searching entire policy
space globally optimal policy, agent could potentially send G times
TG. Table 3 provides table complexity required determine locally
optimal policy various domain properties.
show although Theorem 7's algorithm locally optimal communication provides significant computational savings finding global optimum, still
outperforms existing teamwork models, exemplified Jff Sff policies. First,
use criterion Theorem 7 evaluate optimality policy, Jff .
(t0; i; fi ) 0 possible times t0, agents i, belief states fi consistent
achievement goal G, locally optimal policy always specify
sending G. words, Jff identical locally optimal policy. However,
inequality Theorem 7 ever false, Jff even locally, let alone globally,
optimal.
Second, also use Theorem 7 evaluate STEAM viewing STEAM's inequality,
Cmt > Cc, crude approximation Inequality 16. fact, clear correspondence terms two inequalities. left-hand side Inequality 16
computes exact expected cost miscoordination. However, unlike STEAM's monolithic
parameter, optimal criterion evaluates complete probability distribution
possible states miscoordination considering possible past sequences consistent
409

fiPynadath & Tambe

agent's current beliefs. Likewise, unlike STEAM's monolithic Cmt parameter, optimal criterion looks ahead possible future sequences states determine true
expected cost miscoordination. Furthermore, view STEAM's parameter, Cc,
approximation communication cost computed right-hand side Inequality 16.
Again, STEAM uses single parameter, optimal criterion computes expected
cost possible states world.
STEAM exibility representation, Cmt , , Cc
necessarily fixed across entire domain. instance, Cmt may vary based
specific joint plan agents may jointly committed (i.e., may
different Cmt goal G). Thus, Theorem 7 suggests significant additional exibility computing Cmt explicit lookahead, optimal criterion derived
COM-MTDP model also provides justification overall structure behind STEAM's
approximate criterion. Furthermore, STEAM's emphasis on-line computation makes
computational complexity Inequality 16 (as presented Table 3) unacceptable,
approximation error may acceptable given gains eciency. specific domain,
use empirical evaluation (as demonstrated next section) quantify error
eciency precisely judge tradeoff.
5. Empirical Policy Evaluation

addition providing analytical results general classes problem domains,
COM-MTDP framework also supports analysis specific domains. Given particular
problem domain, construct optimal communication policy or, complexity
computing optimal policy prohibitive, instead evaluate compare candidate
approximate policies. provide reusable tool evaluations, implemented
COM-MTDP model Python class domain-independent methods evaluation arbitrary policies generation locally optimal policies using
Theorem 7 globally optimal policies brute-force search policy space.
software available Online Appendix 1.
section presents results COM-MTDP analysis example domain involving
agent-piloted helicopters, focus key communication decision faced many
multiagent frameworks (as described Section 4), vary cost communication
degree observability generate space distinct domains different implications
agents' performance. evaluating communication policies various configurations particular testbed domain, demonstrate methodology one
use COM-MTDP framework model problem domain evaluate candidate
communication policies it.
5.1 Experimental Setup
Consider two helicopters must across enemy territory destination, illustrated Figure 1. first, piloted agent Transport, transport vehicle
limited firepower. second, piloted agent Escort, escort vehicle significant
firepower. Somewhere along path enemy radar unit, location unknown
(a priori) agents. Escort capable destroying radar unit upon encountering
it. However, Transport not, escape detection radar unit traveling
410

fiThe Communicative Multiagent Team Decision Problem

Figure 1: Illustration helicopter team scenario.
low altitude (nap-of-the-earth ight), though lower speed typical,
higher altitude. scenario, Escort worry detection, given superior
firepower; therefore, fast speed typical altitude.
two agents form top-level joint commitment, GD , reach destination.
incentive agents communicate achievement goal, since
eventually reach destination certainty. However, service
top-level goal, GD , two agents also adopt joint commitment, GR, destroying
radar unit. consider problem facing Escort respect communicating
achievement goal, GR. Escort communicates achievement GR, Transport
knows safe normal altitude (thus reaching destination sooner).
Escort communicate achievement GR, still chance
Transport observe event anyway. Transport observe achievement
GR , must nap-of-the-earth whole distance, team receives lower
reward later arrival. Therefore, Escort must weigh increase expected
reward cost communication.
COM-MTDP model scenario (presented Figures 2, 3 4), world
state position (along straight line origin destination) Transport,
Escort, enemy radar. enemy randomly selected position somewhere
agents' initial position destination. Transport possible
communication actions, choose two domain-level actions: ying nap-ofthe-earth ying normal speed altitude. Escort two domain-level actions:
ying normal speed destroying radar. Escort also option communicating special message, GR , indicating radar destroyed. tables
Figures 2, 3 4, \" symbol represents wild-card (or \don't care") entry.
Escort arrives radar, observes presence certainty
destroy achieve GR. likelihood Transport's observing radar's destruction
function distance radar. vary function's observability parameter
411

fiPynadath & Tambe
ff


Aff

ff

= fEscort (E ); Transport (T )g
= E R
Position Escort: E = f0; 1; : : : ; 8; 9; Destinationg
Position Transport: = f0; 0:5; : : : ; 9; 9:5; Destination;
Destroyedg
Position Radar: R = f1; 2; : : : ; 8; Destroyedg
= AE = f y; destroy; waitg f y-NOE; y-normal; waitg
= E = fclear (GR ); nullg fnullg
E
0; : : : ; 9
0; : : : ; 9



0; : : : ; 9:5; Destroyed

RA

0
Destination

rT
Destination 0; : : : ; 9:5; Destroyed
Destination
+ rT
Destination
R (s; hnull; nulli) = 0
R (s; hGR ; nulli) = r 2 [0; 1]
Figure 2: COM-MTDP model states, actions, rewards helicopter scenario.

RA (hE ; ; R ; a)

=

( Figure 4) within range [0; 1] generate distinct domain configurations (0 means
Transport never observe radar's destruction; 1 means Transport always
observe it). observability 1, achieve mutual belief achievement
GR soon occurs (following argument presented Section 4.1). However,
observability less 1, chance agents achieve mutual belief
simply common observation. helicopters receive fixed reward time step
spent destination. Thus, fixed time horizon, earlier helicopters reach
there, greater team's reward. Since ying nap-of-the-earth slower normal
speed, Transport switch normal ying soon either observes GR
achieved Escort sends message, GR . Sending message free,
impose variable communication cost (r Figure 2), also within range [0; 1].
constructed COM-MTDP models scenario combination observability communication cost within range [0; 1] 0.1 increments. combination,
applied Jennings STEAM policies, well completely silent policy.
domain, policy, Jff , dictates Escort always communicate GR upon destroying
radar. STEAM, vary Cc parameters observability communication cost parameters, respectively. used two different settings (low medium)
cost miscoordination, Cmt . Following published STEAM algorithm (Tambe,
1997), Escort sends message GR STEAM's inequality Cmt > Cc, holds.
Thus, two different settings, low medium, Cmt generate two distinct communication policies; high setting strictly dominated two settings domain.
also constructed evaluated locally globally optimal policies. applying
policies, used COM-MTDP model compute expected reward received
team following selected policy. uniquely determine expected
reward given candidate communication policy particular observability communication cost parameters, well COM-MTDP model specified Figures 2, 3,
4.
412

fiThe Communicative Multiagent Team Decision Problem

P (hE0; 0; R0 ; haE ; ; hE1; 1; R1 i) =
PE (E 0 ; aE ; E 1 ) PT (hT 0 ; R0 ; ; 1 ) PR (hE 0 ; R0 ; aE ; R1 )

Escort: Initial distribution, Pr(0E = 0) = 1
E 0

aE

E 1

PE

Destination Destination 1
0; : : : ; 8

E 0 + 1
1
0; : : : ; 8 destroy E0 + 1 1
9
Destination 1
9
destroy Destination 1

wait
E 0
1
Transport: Initial distribution, Pr(0T = 0) = 1
0

R0



Destination


Destroyed


0; : : : ; 9

y-NOE
9:5

y-NOE
0; : : : ; 8:5 Destroyed y-normal
9; 9:5
Destroyed y-normal

6= Destroyed y-normal


wait

1

Destination
Destroyed
0 + 0:5
Destination
0 + 1
Destination
Destroyed
0

PT

1
1
1
1
1
1
1
1

Radar: Initial distribution, 8 2 f1; 2; : : : ; 8g, P r(0R = ) = 0:125
E 0

R0

aE

R1

PR

E0 destroy Destroyed 1

6= destroy
R0
1
6= E0

R0
1

Figure 3: COM-MTDP model transition probabilities helicopter scenario (excludes
zero probability rows).

413

fiPynadath & Tambe


ff =
E


{
E = E
, agent Escort's possible observations radar
consist
= fpresent; destroyed; nullg
{
= E
RT , agent Transport's possible observations radar
consist
RT = fdestroyed; nullg
(s; haE ; ; h!E ; !T i) = OE (s; haE ; ; !E ) OT (s; haE ; ; !T )
{ OE (hE ; ; R ; haE ; ; ; ; !RE i) =
E

R

aE

!RE

OE

destroyed destroy destroyed 1
destroyed 6= destroy null
1
R 1; : : : ; 9

present 1
6= R 1; : : : ; 9

null
1
{ OT (hE ; ; R ; haE ; ; ; ; !RT i) =




R
0; : : : ; 9:5
0; : : : ; 9:5
0; : : : ; 9:5

aE

!RT

destroy destroyed
destroy
null 1
6= destroy null
destroyed

null

OT

e (R )(1 )
e (R )(1 )

1
1

2 [0; 1]

Figure 4: COM-MTDP model observability helicopter scenario. tables exclude
zero probability rows input feature columns independent. example, agents' observation functions independent
transport's selected action, neither table includes column.

414

fiThe Communicative Multiagent Team Decision Problem

Figure 5: Suboptimality silent Jennings policies.

Figure 6: Suboptimality STEAM policy low medium costs miscoordination.
5.2 Experimental Results
Figures 5 6 plot much utility team expect lose following Jennings,
silent, two STEAM policies instead locally optimal communication policy
(thus, higher values mean worse performance). immediately see Jennings
silent policies significantly suboptimal many possible domain configurations.
example, surprisingly, surface policy, Jff, peaks (i.e., poorly)
communication cost high observability high, silent
policy poorly exactly opposite conditions.
Previously published results (Jennings, 1995) demonstrated Jennings policy
led better team performance reducing waste effort produced alternate policies
like silent one. earlier results focused single domain, Figure 5 partially
confirms conclusion shows superiority Jennings policy
silent policy extends broad range possible domain configurations.
hand, COM-MTDP results also show significant subclass domains (e.g.,
communication cost observability high) Jennings policy actually
inferior silent policy. Thus, COM-MTDP model, characterize
types domains Jennings policy outperforms silent policy vice versa.
415

fiPynadath & Tambe

Figure 6 shows expected value lost following two STEAM policies.
view STEAM trying intelligently interpolate Jennings silent policies
based particular domain properties. fact, low setting Cmt , see
two thresholds, one along dimension, STEAM switches following
Jennings silent policies, suboptimality highest thresholds.
medium setting Cmt , STEAM exhibit threshold along dimension
communication cost, due increased cost miscoordination. settings,
STEAM's performance generally follows better two fixed policies, maximum suboptimality (0.587 settings) significantly lower silent
(0.700) Jennings' (1.000) policies. Furthermore, STEAM outperforms two policies
average, across space domain configurations, evidenced mean suboptimality 0.063 low Cmt 0.083 medium Cmt . values significantly
lower silent policy's mean 0.160 Jennings' policy's mean 0.161. Thus,
able quantify savings provided STEAM less selective policies
within example domain.
However, within given domain configuration, STEAM must either always never
communicate, exibility leads significant suboptimality across wide range
domain configurations. hand, Figure 6 also shows domain
configurations STEAM locally optimal. relatively small-scale experimental
testbed, need incur STEAM's suboptimality, agents compute
superior locally optimal policy 5 seconds. larger-scale domains,
hand, increased complexity locally optimal policies may render execution
infeasible. domains, STEAM's constant-time execution would potentially make
preferable alternative. analysis suggests possible spectrum algorithms make
different optimality-eciency tradeoffs.
understand cause STEAM's suboptimality, examine performance
deeply Figures 7 8, plot expected number messages sent using
STEAM (with low medium Cmt ) vs. locally optimal policy, observability
values 0.3 0.7. STEAM's expected number messages either 0 1, STEAM
make two (instantaneous) transitions them: one threshold value
along observability communication cost dimensions.
Figures 7 8, see optimal policy exible STEAM
specifying communication contingent Escort's beliefs beyond simply achievement
GR. example, consider messages sent low Cmt Figure 7, STEAM
matches locally optimal policy extremes communication cost dimension.
Even communication cost high, still worth sending message GR states
Transport still far destination. Thus, surface optimal policy,
makes gradual transition always communicating never communicating.
thus view STEAM's surface crude approximation optimal surface, subject
STEAM's fewer degrees freedom.
also use Figures 7 8 identify domain conditions joint
intentions theory's prescription attaining mutual belief optimal. particular,
domain observability less 1, agents attain mutual belief
without communication. Figures 7 8, many domain configurations
locally optimal policy expected send fewer 1 GR message.
416

fiThe Communicative Multiagent Team Decision Problem

Figure 7: Expected number messages sent STEAM locally optimal policies
observability 0.3.

Figure 8: Expected number messages sent STEAM locally optimal policies
observability 0.7. settings, STEAM sends 0 messages.

417

fiPynadath & Tambe

Figure 9: Suboptimality locally optimal policy.
configurations represents domain locally optimal policy attain
mutual belief least one case. Therefore, attaining mutual belief suboptimal
configurations!
experiments illustrate STEAM, despite decision-theoretic communication
selectivity, may communicate suboptimally significant class domain configurations. Previous work STEAM-based, real-world, agent-team implementations informally
noted suboptimality isolated configuration within realistic helicopter transport domain (Tambe, 1997). Unfortunately, previous work treated suboptimality
(where agents communicated necessary) isolated aberration,
investigation degree suboptimality, conditions
suboptimality may occur practice. re-created conditions within experimental testbed section using medium Cmt . resulting experiments (as shown
Figure 7) illustrated observed suboptimality isolated phenomenon,
but, fact, STEAM general propensity towards extraneous communication
situations involving low observability (i.e., low likelihood mutual belief) high communication costs. result matches situation \aberration" occurred
realistic domain.
locally optimal policy suboptimal respect globally optimal
policy, see Figure 9. domain configurations high observability,
globally optimal policy escort wait additional time step destroying
radar communicate transport continues ying nap-of-the-earth.
escort cannot directly observe method ight transport chosen,
measure change transport's position (since maintains history
past observations) thus infer method ight complete accuracy. sense,
escort following globally optimal policy performing plan recognition analyze
transport's possible beliefs. particularly noteworthy domain specification
explicitly encode recognition capability. fact, algorithm finding
globally optimal policy even make assumptions made locally
observable policy (i.e., single agent deciding whether communicate not, regarding
single message, single point time); rather, general-purpose search algorithm
traverses policy space \discovers" possible means inference own.
418

fiThe Communicative Multiagent Team Decision Problem

expect COM-MTDP analysis provide automatic method discovering
novel communication policies type domains, even modeling real-world
problems.
Indeed, exploiting discovery capability within example domain, globally
optimal policy gains slight advantage expected utility locally optimal policy,
mean difference 0.011, standard deviation 0.027, maximum 0.120.
hand, domain-independent code never requires 5 seconds compute
locally optimal policy testbed, domain-independent search algorithm
always required 150 minutes find globally optimal policy. Thus,
Theorem 7, used COM-MTDP model construct communication policy
that, testbed domain, performs almost optimally outperforms existing teamwork theories, substantial computational savings finding globally optimal
policy. Although results hold isolated communication decision, expect
relative performance policies stay even multiple decisions,
exibility suboptimal policies exacerbate losses (i.e., shapes
graphs would stay roughly same, suboptimality magnitudes would increase).
6. Summary

COM-MTDP model novel framework complements existing teamwork research
providing previously lacking capability analyze optimality complexity
team decisions. grounded within economic team theory, COM-MTDP's extensions include communication dynamism allow subsume many existing multiagent
models. able exploit COM-MTDP's ability represent broad classes
multiagent team domains derive complexity results optimal agent teamwork
arbitrary problem domains. also used model identify domain properties
simplify complexity.
COM-MTDP framework provides general methodology analysis across
general domain subclasses specific domain instantiations. demonstrated Section 4,
express important existing teamwork theories within COM-MTDP framework
derive broadly applicable theoretical results optimality. Section 5 demonstrates
methodology analysis specific domain. encoding teamwork problem
COM-MTDP, use leverage general-purpose software tools (available
Online Appendix 1) evaluate optimality teamwork based potentially
existing theory, demonstrated paper using two leading instantiations joint
intentions theory. combining theory practice, use theoretical results
derived using COM-MTDP framework basis new algorithms extend
software tools, translating Theorem 7 Section 4 implemented
algorithm locally optimal communication Section 5. expect COM-MTDP
framework, theorems complexity results, reusable software form basis
analysis teamwork, others field.

419

fiPynadath & Tambe
7. Future Work COM-MTDP Team Analysis

initial COM-MTDP results promising, remain least three key areas
future progress COM-MTDPs critical. First, analysis using COM-MTDPs (such
one presented Section 5) requires knowledge rewards, transition probabilities, observation probabilities, well competing policies governing agent
behavior. may always possible model domain agents'
policies readily available. Indeed, proposed team-analysis techniques (Nair, Tambe,
Marsella, & Raines, 2002b; Raines, Tambe, & Marsella, 2000), require priori handcoding models, rather acquire automatically machine learning
large numbers runs. Also, interests combating computational complexity
improved understandability, researchers emphasize need multiple models
multiple levels abstraction, rather focusing single model (Nair et al., 2002b).
instance, one level model may focus analysis individual agents' actions support team, another level may focus interactions among subteams
team. potentially extend COM-MTDP model directions
(i.e., machine learning model parameters, hierarchical representations team
provide multiple levels abstraction).
Second, important extend COM-MTDP analysis aspects teamwork
beyond communication. instance, team formation (where agents may assigned specific roles within team) reformation (where failure individual agents leads role
reassignment within team) key problems teamwork appear suitable
COM-MTDP analysis. analysis may require extensions COM-MTDP framework (e.g., explicit modeling roles). Ongoing research (Nair, Tambe, & Marsella, 2002a)
begun investigating impact extensions applications domains
RoboCup Rescue (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjoh, & Shimada, 1999). Analysis complex team behaviors may require extensions
COM-MTDP model explicitly account additional aspects teamwork (e.g.,
notions authority structure within teams).
Third, extending COM-MTDP analysis beyond teamwork model types coordination may require relaxation COM-MTDP's assumption sel ess agents receiving
joint reward. complex organizations may require modeling non-joint
rewards. Indeed, enriching COM-MTDP model manner may enable analysis seminal work multiagent coordination tradition PGP
GPGP (Decker & Lesser, 1995; Durfee & Lesser, 1991). enriched models may first
require new advances mathematical foundations COM-MTDP framework,
ultimately contribute towards emerging sciences agents multiagent systems.
Acknowledgments

article significantly extended version paper, \Multiagent Teamwork: Analyzing
Optimality Complexity Key Theories Models", authors,
Proceedings International Joint Conference Autonomous Agents Multi-Agent
Systems, 2002. article extends initial content providing proofs missing
original paper, well new theoretical results, detailed description experimental
420

fiThe Communicative Multiagent Team Decision Problem

setup, new experimental results, additional discussion explanations key points.
research supported DARPA award No. F30602-98-2-0108, Control
Agent Based Systems program, managed AFRL/Rome Research Site. would
like thank Daniel Bernstein, Ashish Goel, Daniel Marcu, Stacy Marsella, Ranjit Nair,
Paul Rosenbloom valuable discussion feedback. also thank anonymous
reviewers helpful comments suggestions.
References

Bernstein, D. S., Zilberstein, S., & Immerman, N. (2000). complexity decentralized
control Markov decision processes. Proceedings Conference Uncertainty
Artificial Intelligence, pp. 32{37.
Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.
Proceedings Conference Theoretical Aspects Rationality Knowledge,
pp. 195{210.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 1{93.
Cohen, P. R., & Levesque, H. J. (1991a). Confirmation joint action. Proceedings
International Joint Conference Artificial Intelligence.
Cohen, P. R., & Levesque, H. J. (1991b). Teamwork. Nous, 25 (4), 487{512.
Decker, K., & Lesser, V. (1995). Designing family coordination algorithms. Proceedings International Conference Multi-Agent Systems.
Dunin-Keplicz, B., & Verbrugge, R. (1996). Collective commitments. International
Conference Multi-Agent Systems, pp. 56{63.
Durfee, E., & Lesser, V. (1991). Partial global planning: coordination framework
distributed planning. IEEE transactions Systems, Man Cybernetics, 21 (5).
Goldberg, D., & Mataric, M. J. (1997). Interference tool designing evaluating multi-robot controllers. Proceedings National Conference Artificial
Intelligence, pp. 637{642.
Grosz, B. (1996). Collaborating systems. Artificial Intelligence Magazine, 17 (2), 67{85.
Grosz, B., & Kraus, S. (1996). Collaborative plans complex group actions. Artificial
Intelligence, 86, 269{358.
Grosz, B. J., & Sidner, C. L. (1990). Plans discourse. Cohen, P. R., Morgan,
J., & Pollack, M. E. (Eds.), Intentions Communication, pp. 417{444. MIT Press,
Cambridge, MA.
Ho, Y.-C. (1980). Team decision theory information structures. Proceedings
IEEE, 68 (6), 644{654.
Jennings, N. (1995). Controlling cooperative problem solving industrial multi-agent
systems using joint intentions. Artificial Intelligence, 75, 195{240.
421

fiPynadath & Tambe

Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,
S. (1999). Robocuprescue: Search rescue large-scale disasters domain
multiagent research. Proceedings IEEE International Conference Systems,
Man Cybernetics.
Levesque, H. J., Cohen, P. R., & Nunes, J. (1990). acting together. Proceedings
National Conference Artificial Intelligence.
Marschak, J., & Radner, R. (1971). Economic Theory Teams. Yale University Press,
New Haven, CT.
Nair, R., Tambe, M., & Marsella, S. (2002a). Team formation reformation multiagent domains like robocup rescue. Proceedings International Symposium
RoboCup.
Nair, R., Tambe, M., Marsella, S., & Raines, T. (2002b). Automated assistants analyzing
team behaviors. Journal Autonomous Agents Multiagent Systems, appear.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operation Research, 12 (3), 441{450.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate via
policy search. Proceedings Conference Uncertainty Artificial Intelligence, pp. 489{496.
Pynadath, D. V., & Tambe, M. (2002). automated teamwork infrastructure heterogeneous software agents humans. Journal Autonomous Agents MultiAgent Systems: Special Issue Infrastructure Requirements Building Research Grade Multi-Agent Systems, appear.
Pynadath, D. V., Tambe, M., Chauvat, N., & Cavedon, L. (1999). Toward team-oriented
programming. Jennings, N. R., & Lesperance, Y. (Eds.), Intelligent Agents VI:
Agent Theories, Architectures Languages, pp. 233{247. Springer-Verlag.
Raines, T., Tambe, M., & Marsella, S. (2000). Automated agents help humans understand team behaviors. Proceedings International Conference Autonomous
Agents.
Rich, C., & Sidner, C. (1997). COLLAGEN: agents collaborate people.
Proceedings International Conference Autonomous Agents.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
Markov processes finite horizon. Operations Research, 21, 1071{1088.
Smith, I. A., & Cohen, P. R. (1996). Toward semantics agent communications
language based speech-acts. Proceedings National Conference Artificial
Intelligence, pp. 24{31.
Sonenberg, E., Tidhar, G., Werner, E., Kinny, D., Ljungberg, M., & Rao, A. (1994). Planned
team activity. Tech. rep. 26, Australian AI Institute.
Tambe, M. (1997). Towards exible teamwork. Journal Artificial Intelligence Research,
7, 83{124.
422

fiThe Communicative Multiagent Team Decision Problem

Tambe, M., Pynadath, D. V., Chauvat, N., Das, A., & Kaminka, G. A. (2000). Adaptive
agent integration architectures heterogeneous team members. Proceedings
International Conference Multi-Agent Systems, pp. 301{308.
Tambe, M., & Zhang, W. (1998). Towards exible teamwork persistent teams. Proceedings International Conference Multi-Agent Systems, pp. 277{284.
Tidhar, G. (1993). Team-oriented programming: Preliminary report. Tech. rep. 41, Australian Artificial Intelligence Institute.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agent
cooperation: Model experiments. Proceedings International Conference
Autonomous Agents, pp. 616{623.
Yen, J., Yin, J., Ioerger, T. R., Miller, M. S., Xu, D., & Volz, R. A. (2001). CAST:
Collaborative agents simulating teamwork. Proceedings International
Joint Conference Artificial Intelligence, pp. 1135{1142.
Yoshikawa, T. (1978). Decomposition dynamic team decision problems. IEEE Transactions Automatic Control, AC-23 (4), 627{632.

423

fiJournal Artificial Intelligence Research 16 (2002) 359-387

Submitted 12/01; published 6/02

Collective Intelligence, Data Routing Braess' Paradox

David H. Wolpert

NASA Ames Research Center, Mailstop 269-2
Moffett Field, CA 94035

Kagan Tumer

NASA Ames Research Center, Mailstop 269-3
Moffett Field, CA 94035

dhw@ptolemy.arc.nasa.gov
kagan@ptolemy.arc.nasa.gov

Abstract

consider problem designing utility functions utility-maximizing
agents multi-agent system (MAS) work synergistically maximize global
utility. particular problem domain explore control network routing
placing agents routers network. Conventional approaches task
agents use Ideal Shortest Path routing Algorithm (ISPA). demonstrate
many cases, due side-effects one agent's actions another agent's performance,
agents use ISPA's suboptimal far global aggregate cost concerned, even
used route infinitesimally small amounts trac. utility
functions individual agents \aligned" global utility, intuitively
speaking. particular example present instance Braess' paradox
adding new links network whose agents use ISPA results decrease
overall throughput. also demonstrate load-balancing, agents'
decisions collectively made optimize global cost incurred trac currently
routed, suboptimal far global cost averaged across time concerned.
also due \side-effects", case current routing decision future trac.
mathematics Collective Intelligence (COIN) concerned precisely issue
avoiding deleterious side-effects multi-agent systems, time space.
present key concepts mathematics use derive algorithm
whose ideal version better performance agents use
ISPA, even infinitesimal limit. present experiments verifying this, also
showing machine-learning-based version COIN algorithm costs
imprecisely estimated via empirical means (a version potentially applicable real
world) also outperforms ISPA, despite access less information
ISPA. particular, COIN algorithm almost always avoids Braess' paradox.
1. Introduction

long history AI research design distributed computational systems,
stretching Distributed AI (Huhns, 1987) current work multi-agent systems
(MAS's) (Claus & Boutilier, 1998; Hu & Wellman, 1998a; Jennings, Sycara, & Wooldridge,
1998; Sandholm, Larson, Anderson, Shehory, & Tohme, 1998; Sycara, 1998).
individual agents system personal utility functions trying
maximize also `world utility' rates possible dynamic histories
overall system, MAS constitutes `collective'. paper particularly
concerned agents use machine learning techniques (e.g., Reinforcement Learning

c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWolpert & Tumer
(RL) Kaelbing, Littman, & Moore, 1996; Sutton & Barto, 1998; Sutton, 1988; Watkins &
Dayan, 1992) try maximize utilities.
field Collective Intelligence (COIN) concerned central design problem
collectives (Wolpert, Tumer, & Frank, 1999; Wolpert & Tumer, 1999): How, without
detailed modeling overall system, one set utility functions individual
agents COIN overall dynamics reliably robustly achieves large values
provided world utility? words, leverage assumption
learners individually fairly good do, collective whole
perform well? 1
example question looms large problem optimize
ow certain entities (e.g., information packets, cars) sources destinations across
network routing nodes. concerned version problem
\optimization" consists minimizing aggregate cost incurred entities owing
destinations, agent controls routing decisions node
network. problem underlies distributed control large array real-world
domains, including internet routing, voice/video communication, trac ows, etc.
COIN perspective, problem reduces question goals one ought
provide router's agent agent's self-interestedly pursuing utility
results maximal throughput entire system (\incentive engineering").
paper investigate application recently developed COIN techniques,
routing domain. Like work concerning COINs, techniques designed
broadly applicable, particular designed routing domain.
Accordingly, performance domain serves good preliminary indication
general usefulness.
ground discussion, concentrate telecommunications data routing
problem entities routed packets. Currently, many real-world algorithms
problem based Shortest Path Algorithm (SPA). algorithm
routing node network controlled agent maintains \routing table"
\shortest paths" (i.e., sequences links minimal total incurred costs) node
possible destination nodes net. moment agent satisfies
routing requests particular destination node sending packets
associated shortest path. Many Ideal SPA (ISPA) algorithms exist eciently computing
shortest path agent-to-agent path-cost communication available costs
traversing agent's node unvarying time, e.g., Dijkstra's Algorithm (Ahuja,
Magnanti, & Orlin, 1993; Bertsekas & Gallager, 1992; Deo & Pang, 1984; Dijkstra, 1959).
non-infinitesimal amount trac routed particular destination
moment agent, agent's sending trac single path
result minimal cost, matter single path chosen. However must
choose single path trac, routing decisions agents
fixed, tautologically using ISPA agent chooses best path, far
trac routing concerned. Accordingly, limit routing infinitesimally
1. lack detailed modeling ensures face problems \brittleness" sometimes
accompany mismatch real world assumptions concerning built non-adaptive,
\hard-wired" agents large MAS's. turn, lack modeling causes us concentrate
adaptive, RL-based agents.

360

fiCollective Intelligence, Data Routing Braess' Paradox
small amount trac, agents' strategies \background", ISPA
optimal (least aggregate incurred cost) routing strategy trac associated
single agent considered individually.
One might hope generally, agent must allot trac single
path agents' trac decisions fixed, choosing path via
ISPA would choice minimizes total incurred cost trac across net,
least limit infinitesimally little trac. case though, using
SPA agent concerned deleterious side-effects actions
costs trac routed agents (Korilis, Lazar, & Orda, 1997a; Wolpert et al.,
1999). problem made worse agents allowed change
decisions response agent's decision. extreme case, elaborated below,
agents try minimize personal costs via ISPA's, agents would
actually receive higher cost would case alternative set strategies.
instance famous Tragedy Commons (TOC) (Hardin, 1968).
Deleterious side-effects need restricted extend space; also extend
time. Indeed, consider algorithm agents given moment make
routing decisions optimize global cost incurred trac currently routed,
algorithm often called \load-balancing" (LB) (Heusse, Snyers, Guerin, & Kuntz, 1998).
definition, LB avoids deleterious side-effects space result TOC
costs incurred trac currently routed. However, due side-effects
time, even conventional LB suboptimal far global cost averaged across
time concerned. Intuitively, one would use \load-balancing time" ensure
truly optimal performance. even one could somehow construct distributed protocol
governing agents caused implement LB, still one would
gotten theme act perfectly coordinated fashion. diculties make
appropriate domain investigate well COIN techniques work practice.
Real-world SPA's (RSPA) work applying ISPA estimated costs traversing
path every agent. Typically estimates error agent-to-agent
communication instantaneous, therefore routing tables may based
date information. generally though, even communication instantaneous,
cost traverse agent's node may different time packet arrives
node. Accordingly, general performance RSPA's bounded
associated ISPA. paper wish investigate topics, rather
highlight issue side-effects. Accordingly \rig game" experimental
comparisons favor SPA, using ISPA's rather RSPA's.
general, even without side-effects, determining optimal solution ow problem
(e.g., determining loads link need maximize throughput
non-cooperative data network) nontractable (Ahuja et al., 1993; Orda, Rom, & Sidi,
1993b). Therefore, concern providing good solutions avoid
diculties ISPA side-effects. aim present algorithms
find best possible (perfectly load-balanced time) solution. Previous work
using machine learning improve routing sometimes resulted better performance
(non-idealized) SPA's (Littman & Boyan, 1993; Boyan & Littman, 1994; Stone, 2000;
Marbach, Mihatsch, Schulte, & Tsisiklis, 1998). work grappled
central COIN design problem however.
361

fiWolpert & Tumer
Section 2 discuss SPA's deficiencies particular manifestations
Braess' paradox. Then, Section 3 present theory collective intelligence,
approach promises overcome deficiencies. discuss routing model
use experiments, show theory COINs applied
model provide alternative shortest path algorithms Section 3. Section 5
present simulation results model comparing ISPA COINs. results
demonstrate networks running ISPA, per packet costs much 32
% higher networks running algorithms based COIN theory. particular, even
though access imprecise estimates costs (a handicap hold
ISPA), COIN-based algorithm almost always avoids Braess' paradox, stark contrast
ISPA. cost incurred ISPA's presumably lower bound
SPA privy instantaneous communication, implication COINs
outperform real-world SPA's. conclude techniques field collective
intelligence highly effective designing utility functions members MAS
ensure work coordinated ecient manner optimize overall performance.
2. Suboptimality Shortest Path Routing Braess Paradox

section first demonstrate suboptimality SPA multiple
agents making simultaneous routing decisions, agent knows ahead time
other's choice, therefore know ahead time exactly costs be.
demonstrate suboptimality hold even one agent making
decision, knows decisions others previously made. Next present
Braess' paradox, particularly pointed instance effects (for discussion
Braess' paradox SPA routing, see Bass, 1992; Cohen & Kelly, 1990; Cohen & Jeffries,
1997; Hogg, 1995; Glance & Hogg, 1995; Korilis, Lazar, & Orda, 1999).

2.1 Suboptimality SPA
Perhaps simplest example individual greed part agents lead
collective detriment occurs two agents determine shortest path
shared link limited capacity, second option slightly
less preferable. case, using common link degrades performance
parties, since due limited capacity performance link quickly fall
second option.
precisely, consider case shared link cost given x3
traversed x packets, router optional second link destination
cost trac x traverse second link 2x. Acting alone, single
packet send, would send packet shared link (cost 1).
However so, incur larger cost (cost 8) used
second choices (cost 4). Without knowing ahead time
(information conventionally contained routing tables), agents necessarily
mistaken cost estimates therefore make incorrect routing decisions. this, even
limit differentially small packets, use SPA lead wrong routing decision.
362

fiCollective Intelligence, Data Routing Braess' Paradox
2.2 Suboptimality ISPA
analyze situation routers may know loads
acting optimize delays experienced packets alone. Consider network
shown Figure 1. Two source routers X send one packet time, X
sending either intermediate router B , sending either B C . type
network may arise many different topologies subnetwork. Accordingly, diculties
associated network also apply many complex topologies.


JJ
J





JJ

J



B

JJ




JJ


J




C

JJ

X

JJ


J















Figure 1: Independent decisions source
Let xA , xB , yB , yC , packet quantities particular fixed time t, A, B ,
C , originating X , indicated. t, source one packet send.
variables binary, xA + xB = yB + yC = 1. Vi (zi ) cost,
per packet, single instant t, router i, total number packets
instant router zi . total cost incurred packets time t, G(~x; ~y),
equals xA VA (xA ) + (xB + yB )VB (xB + yB ) + (yC )VC (yC ).
ISPA, X chooses xA xB = 1 minimize cost incurred
X's packet alone, gX (~x) xA VA (xA ) + xB VB (xB + yB ). ISPA ignores
yB VB (xB + yB ) term, i.e., ignores \side effects" X 's decision. Real-world SPA's
typically try approximate X choose either B according whether
VA (0) VB (yB ) smaller, two values estimated via pings, example.
right thing point view minimizing global cost course
instead X minimize G(~x; ~y), precisely, components G(~x; ~y)
depend X . Writing case, X ought act minimize xA VA (xA ) + (xB +
yB )VB (xB + yB ). Due constraint xA + xB = 1, means sending iff
VA (1) < (yB + 1)VB (yB + 1) yB VB (yB ), differs ISPA result X
concerned full cost going router B , portion cost
packet receives.
context example, G-minimizing algorithm constitutes \load-balancing"
(LB). Note long sgn[VA (0) VB (yB ) yB VB0 (yB )] 6= sgn[VA (0) VB (yB )], even
limit infinitesimally small trac (so xA + xB equals infinitesimal ),
ISPA LB still disagree. LB considers side-effects current routing decisions
trac currently routed. However consider side-effects routing
decisions future trac, even LB may optimize global cost averaged across time,
363

fiWolpert & Tumer
depending details system. However use \effect sets" COINs
account even delayed side-effects2 .

2.3 Braess' Paradox
Let us conclude section illustration Braess' paradox (Bass, 1992; Cohen
& Kelly, 1990; Cohen & Jeffries, 1997; Glance & Hogg, 1995; Hogg, 1995; Korilis, Lazar,
& Orda, 1997b; Korilis et al., 1999), phenomenon dramatically underscores
ineciency ISPA. apparent \paradox" perhaps best illustrated
highway trac example first given Bass (Bass, 1992): two highways connecting
towns D. cost associated traversing either highway (either terms tolls,
delays) V1 + V2 , illustrated Net Figure 2. x = 1 (a single traveler)
either path, total accrued cost 61 units. hand, six travelers split equally
among two paths, incur cost 83 units get destinations. Now,
suppose new highway built connecting two branches, shown Net B Figure 2.
Further, note cost associated taking highway particularly high (in
fact load higher 1, highway lower cost highway
system). benefit highway illustrated dramatically reduced cost incurred
single traveler: taking short-cut, one traveler traverse network
cost 31 units (2 V1 + V3 ). Adding new road seemingly reduced traversal cost
dramatically.

V2

V1

"y
bDb
"
"
bb
y"
"
byV1

V2


bb
"yV2
"
bb ""
b"
yS

V1

Net
Figure 2: Hex network V1 = 10x ;

"ybDb
"
"
bb
"y"
byV1

V3





yb
"yV2
bb
"
bb"yS""
V2

Net B
= 50 + x ;

V3

= 10 + x

However consider happens six travelers highways net B.
agent uses ISPA, equilibrium three possible paths contains two
travelers.3 Due overlaps paths however, results traveler incurring
cost 92 units, higher incurred new highway
built. net effect adding new road increase cost incurred every traveler.
phenomenon known Braess' paradox.
2. detailed discussion proof suboptimality LB shown appendix A. Since LB
used current systems hard imagine ever used, experiments consider it;
discussed pedagogical reasons.
3. mind Nash equilibrium problem, traveler (or equivalently,
router) gain advantage changing strategies.

364

fiCollective Intelligence, Data Routing Braess' Paradox
3. Mathematics Collective Intelligence

One common solution types side-effect problems particular agents
network (e.g., \network manager" Korilis, Lazar, & Orda, 1995) dictate certain
choices agents. solution incur major brittleness scaling problems
however. Another kind approach, avoids problems centralized manager,
provide agents extra incentives induce take actions
undesirable strict SPA sense. incentive form \taxes"
\tolls" added costs associated traversing particular links discourage
use links. schemes tolls superimposed agents' goals
special case general approach replacing goal agent new
goal. new goals specifically tailored collectively met system
maximizes throughput. priori, agent's goal need particular relation
SPA-type cost incurred agent's packets. Intuitively, approach, provide
agent goal \aligned" global objective, separate concern
goal's relation SPA-type cost incurred trac routed agent.
section, summarize salient aspects Collective Intelligences (COIN) (Wolpert,
Wheeler, & Tumer, 2000; Wolpert & Tumer, 1999). paper consider systems
consist set agents, connected network, evolving across set discrete, consecutive time steps, 2 f0; 1; :::g. Without loss generality, let relevant characteristics
agent time | including internal parameters time well externally
visible actions | encapsulated Euclidean vector ;t components ;t;i .
call \state" agent time t, let ;t state agents time t,
state agent across time.
World utility, G( ), function state agents across time.
agent uses Machine Learning (ML) algorithm \try increase" private
utility, write private utility g ( ), generally, allow utility
vary time, g; ( ).
assume encompasses physically relevant variables, dynamics
system deterministic (though course imprecisely known anyone trying
control system). Note means characteristics agent = 0
affects ensuing dynamics system must included ;0 . ML-based
agents, includes particular algorithmic specification private utility, typically
physical form computer code (the mathematics generalized beyond
ML-based agents, elaborated Wolpert & Tumer, 1999).
focus case goal, COIN designers, maximize world utility
proper selection private utility functions. Intuitively, idea choose
private utilities aligned world utility, also property
relatively easy us configure agent associated private
utility
P
achieves large value. paper, utilities consider form Rt ( ;t )
P
reward functions Rt (simply Rt ( ;t ) non-time-varying utilities). on,
consider world utilities whose associated set fRt g time-translations
one another. particular, shown below, overall network throughput expressible
way.
365

fiWolpert & Tumer
need formal definition concept private utilities \aligned"
Constructing formalization subtle exercise. example, consider systems
world utility sum private utilities individual agents. might
seem reasonable candidate example \aligned" utilities. However systems
examples general class systems \weakly trivial". well-known
weakly trivial systems individual agent greedily trying maximize
utility lead tragedy commons (Hardin, 1968; Crowe, 1969) actually
minimize G.
particular, case private
utilities independent
P
P
time G = g . Evidently, minimum, G = g sucient ensure
\aligned" utilities; alternative formalization concept needed.
Note simple network discussed Section 2.1, utilities weakly trivial,
since G(~x; ~y) = gX (~x) + gy (~y ). provides another perspective suboptimality
ISPA network.
G.

careful alternative formalization notion aligned utilities concept
\factored" systems. system factored time following holds
agent individually: change time state alone, propagated across
time, result increased value g; ( ) results increase
G( ) (Wolpert & Tumer, 1999).
factored system, side-effects change 's = state increases
private utility cannot decrease world utility. restrictions though effects
change private utilities agents and/or times. particular, don't
preclude agent's algorithm two different times \working cross-purposes"
other, long moments agent working improve G. game-theoretic
terms, factored systems optimal global behavior corresponds agents' always
private utility Nash equilibrium (Fudenberg & Tirole, 1991). sense,
tragedy commons factored system. trivial example, system
factored g; = G 8, system conventionally called `team game'.

Furthermore, system factored respect private utilities fg; g, want
agent state time induces high value associated private
utility possible (given initial states agents). Assume ML-based
able achieve fairly large values private utilities likely set time
, i.e., assume given private utility g; , rest components ;
set 's algorithm way achieve relatively high value g; .
problem becomes determining fg; g agents best able achieve high
g (subject other's actions) also causing dynamics factored G
fg; g.

Define effect set agent-time pair (; ) , C(eff
; ) ( ), set agents
0 ;t forward dynamics system non-zero partial derivative
respect state agent = . Intuitively, (; )'s effect set set states
agents 0 ;t would affected change state agent time .
Next, set agents (0 ; t), define CL ( ) \virtual" vector formed
clamping components vector delineated arbitrary fixed value,
366

fiCollective Intelligence, Data Routing Braess' Paradox
paper set 0. 4 operation creates new state vector (e.g., worldline)
clamped components worldline (e.g., one player's action particular time
step) \zeroed" (e.g., removed system).
value wonderful life utility (WLU short) defined as:
W LU ( )

G( )

G(CL ( )):

(1)

particular, interested WLU effect set agent-time pair (; ).
WLU difference actual world utility virtual world utility
agent-time pairs affected (; ) clamped zero state
rest left unchanged.
Since clamping ~0, loosely view (; )'s effect set WLU analogous
change world utility would arisen (; ) \had never existed", hence
name utility - cf. Frank Capra movie. Note however, CL purely
\fictional", counter-factual operator, produces new without taking account
system's dynamics. sequence states agent-time pairs clamped
constructing WLU need consistent dynamical laws system.
dynamics-independence crucial strength WLU. means evaluate
WLU try infer system would evolved agent 's state
set ~0 time system evolved there. long know , extending
time, , function G, know value WLU.
mentioned above, regardless system dynamics, g; = G 8 means
system factored time .

Theorem: Regardless system dynamics, setting
factored system time .

g;

=

W LUC eff

(; )

8 results

Proof: second term, G(CLC eff ( )) is, definition, independent ; . Therefore
(; )

change (; ) component affect first term, G( ). Therefore
effect change value world utility effect
value wonderful life utility. QED.
Since factoredness distinguish team game wonderful life utilities,
need means deciding use choice fg; g. determine
this, note since agent operating large system, may experience diculty
discerning effects actions G G sensitively depends agents
system. Therefore may diculty learning past experience
achieve high g; g; = G. particular, routing large networks, private
rewards given world reward functions means provide router
reward time step need provide full throughput entire network
step. usually infeasible practice. Even weren't though, using
private utilities would mean routers face dicult task trying discern
4. choice clamping parameter used associated COIN affect performance. However
within wide ranges, doesn't affect whether COIN outperforms alternatives like team games.

367

fiWolpert & Tumer
effect actions rewards, therefore would likely unable learn
best routing strategies.
problem mitigated using effect set WLU private utility, since
subtraction clamped term removes much \noise" activity agents,
leaving underlying \signal" agent question affects utility (this
reasoning formalized concept \learnability" Wolpert & Tumer, 1999). Accordingly, one would expect setting private utilities WLU's ought result better
performance g; = G 8; . primary theoretical consideration
leverage COIN techniques investigated paper.
practice, sometimes able estimate \primary", prominent
portion effect set. Technically, associated WLU effect set WLU,
therefore exactly factored. However assuming associated WLU close enough
factored, would expect advantage learnability WLU still
result better performance would using g; = G 8; (see Wolpert et al., 2000;
Wolpert & Tumer, 1999). Indeed, sake improving learnability, sometimes
elect exclude certain agent-time pairs estimate effect set (; ), even
sure affected ; . case expect
changes G due varying ; \mediated" agent-time pairs
relatively insignificant, therefore effectively constitute noise learning process,
effect learnability important effect factoredness.
4. Collective Intelligence Network Routing

section, use theory summarized Section 3 derive individual goals
router, form private utility functions maximized appropriate choice
routing decisions. routers tried achieve maximizations using algorithms
require limited knowledge state network (in particular knowledge
readily available routers common real data networks). simulations
router used Memory Based (MB) machine learning algorithm (nearest neighbor) make
routing decisions. precisely, potential routing decision, routers look
past state closely closely matches current state (e.g., load).
assign "estimated" utility value potential routing decision select action
highest estimated utility value. call algorithm MB COIN5 .

4.1 Model Description
apply COIN formalism network routing model, must formally describe
set deterministically evolving vectors ;t . model used paper,
time step trac router set pairs integer-valued trac amounts
associated ultimate destination tags. time step t, router r sums
integer-valued components current trac time step (one component
5. Relatively minor details algorithm concerning exploration/exploitation issues along \steering" parameter discussed end section.

368

fiCollective Intelligence, Data Routing Braess' Paradox
ultimate destination) get instantaneous load. write load as:
zr (t)



X


xr;d (t);

index runs ultimate destinations, xr;d (t) total trac time
going r towards d. instantaneous load time evaluated, router
sends trac next downstream routers, manner governed underlying
routing algorithm. indicate \next routers" writing:


xr;d (t)

=

X
r0

xr;d;r0 (t);

r0 next router trac (r; d), i.e., first stop path followed
router r ultimate destination d. routed trac goes next
downstream routers, cycle repeats itself, trac reaches destinations.
simulations, simplicity, trac introduced system (at
source routers) beginning successive disjoint waves L consecutive time steps
each6 . use (t) indicate either integer-valued wave number associated time
set times wave, context indicates.
real network, cost traversing router depends \after-effects" recent
instantaneous loads, well current instantaneous load. simulate effect,
use time-averaged values load router rather instantaneous load determine
cost packet incurs traversing router. formally, define router's
windowed load, Zr (t), running average router's load value window
previous W timesteps (W always set integer multiple L):
Zr (t)

W1


X
t0 =t W +1

zr (t0 )

=

X


Xr;d (t);

value Xr;d (t) set
Xr;d (t)

=

1


X

W 0
=t W +1

xr;d (t0 )):

Intuitively, large enough W , using window determine costs across routers
means typically costs change substantially time scales significantly
larger individual routing decisions. Formally, windowed load
argument load-to-cost function, V (), provides cost accrued time
packet traversing router timestep. is, time t, cost
packet traverse router r given V (Zr (t))7 . Note model, costs
accrued routers, links. Also note simplicity physically
instantiate cost temporal delay crossing router. Different routers different
6. L always chosen minimal number necessary trac reach destination
next wave trac initiated.
7. also introduce \dummy routers" denoted V0 () = 0 help translating mathematics
simulations. Omitting effect simulations.

369

fiWolpert & Tumer
V (), ect fact real networks differences router software hardware
(response time, queue length, processing speed etc). simplicity, W
routers however. definitions, world utility given
G( )

=
=

X
t;r
X
t;r;d

=

X
t;r;d

=

X
t;r;d

zr (t) Vr (Zr (t))
xr;d (t)Vr (Zr (t))
0

xr;d (t)Vr @
xr;d (t)Vr

1


X

X

W 0
=t W +1 d0

X
d0

1

xr;d0 (t0 )A

!

Xr;d0 (t)

(2)

:

equation G explicitly demonstrates
that, claimed above, representation
P
express G( ) sum rewards, Rt ( ;t ), R( ;t ) written function
pair (r; d)-indexed vectors:
Rt (xr;d (t); Xr;d (t))

=

X
r;d

xr;d (t)Vr

X
d0

!

Xr;d0 (t)

:

Also claimed, Rt temporal translations one another.
Given model, components ;t must identified values
xr;d;r0 (t) 8 r; d; r 0 t, since x's set actions agents take. Since
arguments G must components , also include Xr;d (t) 8r; d; components
;t . Formally, routing based ML agents, internal parameters ML agents
must also included . parameters affect routing,
turn affected it. evolve deterministically, since includes routing
variables, must also contain internal parameters agents. won't need
explicitly delineate variables however, mostly phrase discussion
though internal parameters.
values fxr;d;r0 (t 1)g 8r; d; r0 specify values fxr;d (t)g 8r; directly. Therefore, concert fxr;d (t0 < t)g, also set fXr;d (t)g directly. Moreover
simulations decisions fxr;d;r0 (t)g 8r; d; r0 fixed routing
algorithms times
P
given fixed function fxr;d (t)g fZr (t) = d0 Xr;d0 (t)g. point
fact map set fxr;d;r0 (t 1); Xr;d0 (t)g 8r; d; r0 full set fxr;d;r0 (t)g 8r; d; r0 ,
fxr;d (t)g. Accordingly, xr;d;r0 undergo deterministic evolution. Since
values across time set values Xr;d (t) across time, see entire set
components ;t undergo deterministic evolution representation, required.
evaluating wonderful life utility need group components ;t
disjoint agents . two types agent, types indexed
router-destination pairs. agent index (r; d), first agent type variable
Xr;d (t), second agent type Euclidean vector components indexed r 0 ,
(xr;d )r0 (t). setting \actions" concerned setting states agents
second type. Accordingly, learners associated agents second
370

fiCollective Intelligence, Data Routing Braess' Paradox
type. Unless explicitly indicated otherwise, implicitly second
type agent mind whenever refer \agent" use symbol .

4.2 ISPA Routing COIN Routing
Based COIN formalism presented Section 3 model described above,
present ISPA COIN-based routing algorithms. time step t, ISPA access
windowed loads time step 1 (i.e., access Zr (t 1) 8r), assumes
values remain times t. Note large window sizes
times close t, assumption arbitrarily accurate. Using assumption,
ISPA, router sends packets along path calculates minimize costs
accumulated packets.
COIN-based routing algorithms, contrast, direct access
Zr . evaluate WLU agent (r; d) time , algorithm must
estimate (primary members the) associated effect set. means determining
components ; will, dynamics system, changed altering
components vector xr;d( ).
first approximation, ignore effects trac changing xr;d;r0 ( ) may
\mediated" learning algorithms running system. is,
ignore changes arise due effects changing xr;d;r0 ( ) rewards,
changes induce changes future training sets, turn get mapped
changes fxr;d;r0 (t)g (and therefore fXr;d (t)g) via learning algorithms running
agents.
another approximation, ignore effects mediated routing algorithms'
observations state network. is, ignore changes fxr00 ;d0 ;r000 (t)g
varying xr;d ( ) may cause due associated changes state network perceived
(r00 ; d0 )'s routing algorithm, changes turn cause algorithm modify routing
decisions accordingly. consider behavior routing algorithms
(potentially) directly affected xr;d ( ) (potentially) route packets
that, time , passed r way d. particular ignore effects
xr;d ( ) fxr00 ;d0 =
6 d;r000 (t)g.
Since packets routed wave arrive destinations end wave,
approximations mean xr00 ;d00 ;r000 (t) estimate xr;d ( )'s
effect set wave . ones are, potentially, directly
affected fxr;d;r0 (t)g \chaining together" sequence xr00 ;d00 ;r000 (t) get
packets xr;d (t) ultimate destination. Due wave nature simulations
though, xr00 ;d00 ;r000 (t) within 's wave affected xr;d ( ) d00 = d.
reasons coding simplicity, concern whether < within
given wave exclude xr00 ;d00 ;r000 (t) accordingly. words, within 's
wave treated equally.
one set members xr;d ( )'s effect set fxr00 ;d;r000 (t) 8r00; d; r000 ; 2 ( )g. Note
members relatively unaffected xr;d ( ) (e.g., r00 far
net away r). simplicity, try determine exclude
them. keeping xr00 ;d;r000 (t < ), inclusion extra agents estimate
effect set hurt learnability, general hurt factoredness. Therefore
371

fiWolpert & Tumer
delay quickly learners determine optimal policies, won't affect
quality (for G) policies finally arrived at. Note also trying determine
whether particular xr00 ;d;r000 (t 2 ( )) included xr;d ( )'s effect set would
mean, part, determining whether packets routed (r; d) would reached r00
(r; d) made routing decision different one actually made. would
non-trivial exercise, general.
contrast case xr00 ;d0 ;r000 (t), Xr00 ;d0 (t) future 's
wave affected xr;d (t) also excluded approximations
far. particular, Xr00 ;d (t) either r00 = r r00 one hop away r
1
directly affected xr;d (t), 2 [W
i=0 ( + iL)) (cf. definition X variables).
simplicity, restrict consideration Xr00 ;d variables router
r, r00 = r.
final estimate effect set clearly rather poor | presumably results better
presented would accrue use accurate effect set. However it's
worth bearing mind \self-stabilizing" nature choice effect sets,
used conjunction effect set WLU's. nature mediated learning
algorithms. one assigns utility function two agents, reward one
agent gets determined part one does. modifies
behavior try increase reward, first agent modifying behavior
way dependent agent does. words, two agents given
WLU estimated other's effect set, ipso facto
other's effect set.
Using estimate effect set, WLU (; ) given difference
total cost accrued 's wave agents network cost accrued
agents agents sharing 's destination \erased." precisely, agent
destination following effect set WLU's, g; :
g; ( )=

=

G( )
X
t;r0 ;d0


=

G(CLC eff ( ))
(; )

xr0 ;d0 (t) Vr0
Vr0

X
d00

X
d0

!

X

Xr0 ;d0 (t)

[ Xr0 ;d00 (t) (1

t;r0 ;d0

(t

2[

xr0 ;d0 (t)(1

W 1
i=0 (

(t

2 ( ))I (d0 = d))

+ iL))I (d00 = d)) ]

!

0
1
X
X
X
X
@
xr0 ;d0 (t) Vr0 (
Xr0 ;d00 (t))
xr0 ;d0 (t) Vr0 (
Xr0 ;d00 (t))A
d0
d00
d0 6=d
d00 6=d
t2( ) r0
0
1
X
X X
X
X
@
+
xr0 ;d0 (t) [Vr0 (
Xr0 ;d00 (t)) Vr0 (
Xr0 ;d00 (t))]A
(3)
0
0
00
00
W
1
r



=
6

t2[
( +iL)
X X

i=1

(:) indicator function equals 1 argument true, 0 otherwise.
allow learner receive feedback concerning actions wave immediately
following wave rather wait W L time steps, approximate second
sum last equality, one times following 's wave, zero. another
way view resultant expression, rather approximation effect
372

fiCollective Intelligence, Data Routing Braess' Paradox
set WLU. view exact WLU approximation effect set,
approximation ignores effects future windowed loads clamping current trac
level. Regardless view adopt, presumably better performance could achieved
implement approximation.
Given approximation, WLU becomes wave-indexed time-translation-invariant
WL \reward function" (WLR):
g; ( ;t2( ) )

X

X

t2( );r0

d0

=

X
d0 6=d

xr0 ;d0 (t) Vr0 (

xr0 ;d0 (t) Vr0 (

X
d00

X

d00 6=d

Xr0 ;d00 (t))
1

Xr0 ;d00 (t))A :

(4)

Notice trac going router r0 6= r destination d0 6= affects value
WLR agent (r; d). ects fact WLR takes account side-effects
(r; d)'s actions agents. Note also r0 -indexed term contributing
WLR computed associated router r0 separately, information available
router. Subsequently terms propagated network ,
much way routing tables updates propagated.
Given choice private utility, must next specify COIN-based routing
algorithm collects initial data (in conjunction utility) used
guide initial routing decisions every agent one routing option must
make. experiments data collected preliminary running ISPA.
preliminary stage, routing decisions made using ISPA, resulting
actions \scored" using WLR given Equation 3. use ISPA generate
routing decisions initial data since likely practice kind SPA
routing algorithm running prior \turning on" COIN algorithm. Alternately
one generate initial data's routing decisions routers make random
decisions, implement sequence decisions \sweeps" across grid
possible set actions. data collected stage provides us initial
input-output training sets used machine learning algorithm agent:
router-destination agent, inputs identified windowed loads outgoing links,
associated WLR values destination question outputs.
sucient initial data collected using ISPA, system switches using
COIN algorithm make subsequent routing decisions. stage, agent routes
packets along link estimates (based training set) would provide best
WLR. perform estimation, MB COIN makes use single-nearest-neighbor
algorithm learner. algorithm simply guesses output would ensue
candidate input output element training set
nearest neighbor (in input space) candidate input.8 words,
learner finds training set input-output pair whose input value (loads outgoing links)
8. simple learning algorithm, use demonstrate potential practical
feasibility COIN-based routing algorithm. performance presumably improved
sophisticated learning algorithms (e.g., Q-learning Sutton & Barto, 1998; Watkins & Dayan, 1992)
used.

373

fiWolpert & Tumer
closest would result potential routing decision. learner
assigns WLR associated training data pair estimate WLR
would result said routing decision. WLR values used choose among
potential routing decisions. input-output data generated algorithm
adding training set generated.
routing algorithm, routers estimate routing decisions (as
ected loads individual time steps) affect WLR values (based
many agents' loads). also possible calculate exactly routing decisions affect
routers' WLR's if, unlike MB COIN, full knowledge loads
agents system. way similar ISPA, router evaluate exact
WLR value would ensue candidate actions, assumption
windowed loads routers one wave future
now. call algorithm directly maximizing WLR (an algorithm call full
knowledge COIN, FK COIN).
Note assumption behind FK COIN, action chooses wave ( )
maximizes WLR also maximize world reward. words, WL reward
perfectly factored respect (wave-indexed) world reward, even though associated
utilities related way (due inaccuracy estimate effect set). Due
factoredness, FK COIN equivalent load balancing world rewards. Since
LB general results inferior performance compared LB time, since FK
COIN equivalent LB, one might expect performance suboptimal. Intuitively,
suboptimality ects fact one choose action regard
effect current reward, also concern reward future waves.
language COIN framework, suboptimality viewed restatement
fact inexactly estimated effect set, system perfectly factored.
learning algorithm MB COIN described extraordinarily crude. addition, associated scheme choosing action purely exploitative, exploration
whatsoever. Rather choose particular sophisticated scheme tune
fit simulations, emulated using sophisticated algorithms general.
modifying MB COIN algorithm occasionally FK COIN determine
router's action rather purely greedy learner outlined above. steering parameter discussed Section 5.5 determines often routing decision based
MB COIN opposed FK COIN.
5. Simulation Results

practice, dicult implement either FK COIN LB. section use
experiments investigate behavior algorithms conceivably used practice.
precisely, based model routing algorithms discussed above, performed simulations compare performance ISPA MB COIN across variety
networks, varying size five eighteen routers. cases trac inserted
network regular, non-stochastic manner sources. results report
averaged 20 runs. report error bars lower 0:05.
Sections 5.1 - 5.4 analyze trac patterns four networks ISPA suffers
Braess' paradox. contrast, MB COIN almost never falls prey paradox
374

fiCollective Intelligence, Data Routing Braess' Paradox
networks (or networks investigated MB COIN significantly
susceptible Braess' paradox). Section 5.5 discuss effect MB
COIN's performance \steering" parameter determines intelligence
MB COIN.9

5.1 Bootes Network
first network type investigate shown Figure 3. many senses trivial
network, Net A, sources even choices make. loads introduced sources change time listed Tables 1 2, along
performances algorithms.


@D@

V1

@
@@yV2

@@
AA
@y@V0
AyV0
AAy
@yS1
S2

y@D
@

V1

@@

@yV2
@@
AA
yV3 AyV0
@yV0
@@y

S1
S2 Ay

Net

Net B
Figure 3: Bootes Network

Loads (S1 ; S2 ) Net
1,1

B
2,1

B
2,2

B
4,2

B

ISPA MB COIN
6.35
6.35
8.35
5.93
8.07
8.07
10.40
7.88
9.55
9.55
10.88
9.71
10.41
10.41
11.55
10.41

Table 1: Average Per Packet Cost BOOTES2 networks V1 = 10 + log(1 + x) ;
4x2 ; V3 = log(1 + x) .

V2

=

MB COIN results identical ISPA results absence additional
link (Network A). However, Braess' paradox arises ISPA, addition
new link network B degrades performance ISPA six eight trac
regimes load-to-cost functions investigated. MB COIN hand
9. Sections 5.1 - 5.4, steering parameter set 0.5.

375

fiWolpert & Tumer

Loads (S1 ; S2 ) Net ISPA MB COIN
1,1
30.35
30.35
B 20.35
20.35
2,2
35.55
35.55
B 40.55
34.99
4,2
41.07
41.07
B 50.47
44.13
6,3
44.63
44.63
B 51.40
44.63
Table 2: Average Per Packet Cost BOOTES4 network
10x ; V3 = log(1 + x) .

V1

= 50 + log(1 + x) ;

V2

=

hurt addition new link once, manages gainfully exploit seven times.
behavior analyzed infinitesimally, MB COIN either uses additional
link eciently chooses ignore seven cases. Moreover, MB COIN's
performance additional link always better ISPA's. example, adding
new link causes degradation performance much 30 % (loads = f2; 1g)
ISPA, whereas load vector MB COIN performance improves 7 %.

5.2 Hex Network
section revisit network first discussed Section 2.1 (redrawn Figure 4
include dummy agents). Table 3 give full results load-to-delay functions
discussed section. use load-to-cost functions qualitatively similar
discussed Section 2.1, incorporate non-linearities better represent
real router characteristics. load-to-cost function associated results reported
Table 4.

V2
V0
V1

"y
bDb
"
"
bb
y"
"
byV1

yV0

bb
"yV2
"
bb ""
b"
yS
Net

V2
V0
V1

"ybDb
"
"
bb
"y"
byV1

Vy3 yV0
yb
"yV2
bb
"
bb"yS""

Figure 4: Hex network

Net B

network demonstrates addition new link may beneficial
low trac cases, leads bottlenecks higher trac regimes. ISPA although
376

fiCollective Intelligence, Data Routing Braess' Paradox
per packet cost loads 1 2 drop drastically new link added, per
packet cost increases higher loads. MB COIN hand uses new
link eciently. Notice MB COIN's performance slightly worse
ISPA absence additional link. caused MB COIN use
learner estimate WLU values potential actions whereas ISPA simply
direct access information needs (costs link).
Load Net ISPA MB COIN
1
55.50
55.56
B 31.00
31.00
2
61.00
61.10
B 52.00
51.69
3
66.50
66.65
B 73.00
64.45
4
72.00
72.25
B 87.37
73.41
Table 3: Average Per Packet Cost HEX network V1 = 50+ x ;
.

V2

= 10x ;

V3

= 10+ x

Load Net ISPA MB COIN
1
55.41
55.44
B 20.69
20.69
2
60.69
60.80
B 41.10
41.10
3
65.92
66.10
B 61.39
59.19
4
71.10
71.41
B 81.61
69.88
Table 4: Average Per Packet Cost HEX network
10x ; V3 = log(1 + x) .

V1

= 50 + log(1 + x) ;

V2

=

5.3 Butter Network
next network investigate shown Figure 5. extension simple
network discussed Section 5.1. doubled size network
three sources route packets two destinations (packets originating
S1 go D1 , packets originating S2 S3 go D2 ). Initially two halves
network minimal contact, addition extra link two sources
two two halves network share common router potential shortest path.
377

fiWolpert & Tumer



TT

yD2

TT


TTy
TTy

V1
V2


V3

yV1
V0 Ty
V0 Ty

@@
@yS3
S1 Ty
S2
D1



TT

yD2

TT


TTy
TTy

V1
V2


V3


V0 Ty V3 V0 Ty
y@V1

@
S2 Ty @yS3
S1
D1

Net

Net B
Figure 5: Butter Network

Table 5 presents two sets results: first present results uniform trac
three sources, results asymmetric trac. first case, Braess'
paradox apparent ISPA: adding new link beneficial network low
load levels average per packet cost reduced nearly 20%, deleterious
higher levels. MB COIN, hand, provides benefits added link
low trac levels, without suffering deleterious effects higher load levels.
Loads (S1 ; S2 ; S3 ) Net ISPA MB COIN
1,1,1
112.1
112.7
B
92.1
92.3
2,2,2
123.3
124.0
B 133.3
122.5
4,4,4
144.8
142.6
B 156.5
142.3
3,2,1

81.8
82.5
B
99.5
81.0
6,4,2

96.0
94.1
B 105.3
94.0
9,6,3
105.5
98.2
B 106.7
98.8
Table 5: Average Per Packet Cost BUTTERFLY network V1 = 50+ log(1+ x) ;
10x ; V3 = log(1 + x).

V2

=

asymmetric trac patterns, added link causes drop performance
ISPA, especially low overall trac levels. true MB COIN. Notice also
high, asymmetric trac regime, ISPA performs significantly worse
MB COIN even without added link, showing bottleneck occurs right side
network alone (similar Braess' paradox observed Section 5.1).
378

fiCollective Intelligence, Data Routing Braess' Paradox
5.4 Ray Network
networks trac regimes discussed far sources routers
one routing option. final network investigate larger network
number routers multiply options significantly higher previous
networks. Figure 6 shows initial network (Net A) \augmented" network (Net
B), new links added. original network relatively choices
routers, packets directed toward destinations along \conduits." new
links added augmented networks provide new choices (crossing patterns)
could beneficial certain original conduits experience large costs.

V2
V0
V1

bDb1 ""y
bDb2
"y
"
"
b"
b
y" V1 y"" bbyV1 bbyV2
"
V0
yV0
yV0
yV
yV
V2
JJ


JJ 2

1
J

J

J%
yV3
V3 J
e

ee
%
%
e


%
S1
S2
Net

V2
V0
V1

"ybDb1 ""ybDb2
"
"
"b
b
"yc" V1 "y" bbyV1 b#byV2
yVc3 cy yV0 V0 #y#V3 yV0
Vc2cyc ##yV#2
yV
JJ

c # JJ

1
J
#c#c J

V3 J
e
ye #yV3 V3cy %J%
yV3
%
e

%
e

S2
S1

Figure 6: Ray network

Net B

Table 6 shows simulation results networks (S1 S2 send packets D1
D2 respectively). low load levels ISPA MB COIN use new links
effectively, although MB COIN performs slightly worse. mainly caused
diculty encountered simple learner (single nearest neighbor algorithm) quickly
learning trac patterns large network. Unlike ISPA however, MB COIN
avoids Braess' paradox cases except high trac regime. Moreover, even
there, effect significantly milder encountered ISPA.

5.5 Steering MB COIN
final aspect COIN-based routing investigate impact choice
value steering parameter. parameter controls amount exploration
algorithm performs determines \intelligence" MB COIN estimating
surface directly calculated FK COIN. Figures 7 - 8, FK COIN results
correspond setting steering parameter MB COIN 1:0. provides
upper bound performance achieved though MB COIN.
HEX network (Figure 7), performance worst setting MB COIN,
corresponds steering, comparable ISPA. contrast, moderate steering
379

fiWolpert & Tumer

Loads S1 andS2 ) Net ISPA MB COIN
2,2
143.6
143.7
B 124.4
126.9
3,3
154.6
154.9
B 165.5
151.0
4,4
165.4
166.0
B 197.7
165.6
6,6
186.7
187.4
B 205.1
191.6
Table 6: Average Per Packet Cost RAY network
10x ; V3 = 10 + log(1 + x).

= 50 + log(1 + x) ;

V2

=

180

80

Per Packet Delay

Per Packet Delay

85

V1

ISPA
FK COIN
MB COIN

75
70
65

ISPA
FK COIN
MB COIN

170
160
150
140

0

0.1 0.2 0.3 0.4
Steering Parameter

0.5

0

0.1 0.2 0.3 0.4
Steering Parameter

0.5

Figure 7: Impact steering Hex4 (left) Ray4 (right) networks.
(0.5) results similar FK COIN, learner information
work (arising extra parts input space represented training
set due occasional use FK COIN), bridges gap suboptimal
algorithm susceptible Braess' paradox one eciently avoids paradox.
RAY network (Figure 7), value steering parameter critical.
steering all, MB COIN performs poorly network | even worse
ISPA. surprising many routing choices affect
performance, simple memory-based learner needs proper \seeding" able
perform well. Even minimal steering though, MB COIN quickly outperforms
ISPA.
Finally, Butter Bootes networks (Figure 8) MB COIN needs
little steering perform well. Although Butter network performance
MB COIN improves slightly information, significantly better ISPA
across board.
380

fi105

Per Packet Delay

Per Packet Delay

Collective Intelligence, Data Routing Braess' Paradox

ISPA
FK COIN
MB COIN

100

95

40
ISPA
FK COIN
MB COIN
35

90
0

0.1 0.2 0.3 0.4
Steering Parameter

0.5

0

0.1 0.2 0.3 0.4
Steering Parameter

0.5

Figure 8: Impact steering Butter y4 (left) Bootes4 (right) networks.
6. Conclusion

Effective routing network fundamental problem many fields, including data
communications transportation. Using shortest path algorithm (SPA)
routers determine router's decisions popular approach problem. However
certain circumstances suffers number undesirable effects. One effect
Braess' paradox, pattern introduced trac network, increasing
capacity network results lower overall throughput, due harmful sideeffects decisions made router trac rest system. Even
theoretical load-balancing algorithm, addresses effects produce
decisions optimal single moment time, still suffer side-effects
result sub-optimal performance. effects extend across time
(i.e., affects performance later) well space.
Collective Intelligence approach novel way controlling distributed systems
avoid deleterious side-effects routing decisions. central idea learning
algorithms control autonomous agents constitute overall distributed system.
Collective Intelligence (COIN), central issue determine personal
objectives assigned autonomous agents. One wants choose
goals greedy pursuit goals associated learning algorithms leads
desirable behavior overall system. paper summarized mathematics
designing goals derived routing algorithm based mathematics.
ran computer simulations compare COIN-based algorithm ideal SPA
(whose performance upper-bounds real-world SPA's) routing. COIN-based algorithm severely handicapped. estimation \effect sets" used algorithm
exceedingly crude. addition, learning algorithms agents particularly
unsophisticated, therefore able effectively maximize individual performances. contrast, ideal SPA access information concerning state
system (real-world-implementable) COIN did, information real-world
SPA could access.
381

fiWolpert & Tumer
Despite biases favor ideal SPA, experiments ideal SPA induced
average costs much 32 % higher COIN-based algorithm. Furthermore
COIN-based algorithm almost always avoided Braess' paradox seriously diminished
performance SPA.
techniques also successfully employed many other, non-routing
domains, coordination autonomous rovers (Tumer, Agogino, & Wolpert, 2002),
combinatorial optimization, \congestion games" (Wolpert & Tumer, 2001), control
data-upload planet (Wolpert, Sill, & Tumer, 2001). conclude results
techniques field collective intelligence highly effective designing
utility functions members MAS ensure work coordinated
ecient manner optimize overall performance. currently investigating extensions
COIN algorithm involve novel goals agents, goals \learnable" learning algorithms. also expanding simulations larger networks
using commercial event driven simulator. Future work focus making approximation current trac levels affect future windowed loads (Equation 3).
also involve investigating better estimates effect sets, particular including
agents destination one's effect set, generally using
\fine-grained" representation agents, example including packet's originating
source, allow fine-grained effect set (and resultant WLU).
Acknowledgments

authors thank Joe Sill reviewers helpful comments.
Appendix A. Suboptimality Load-Balancing

appendix present existence proof suboptimality Load-Balancing
(LB) explicitly constructing situation conventional LB suboptimal.
Consider system discrete time, source agent X consideration
must route one packet (fixed) destination time step. Presume
trac source agent X enters agents X sends to,
trac coming X sole source costs associated X 's outbound links. Let
(t) number times agent sent packet link W time steps
preceding t, take s(t) = A; B mean router uses link B , respectively,
time t. Model queue backups like cost send packet link
time CA (S (t)=W ), cost router instead send packet
link B CB (1 (t)=W ), simplicity assume CA (:) CB (:)
monotonically increasing functions arguments.
Restrict attention agents work s(t) = iff (t) k realvalued threshold k. LB algorithm choose s(t) = iff CA (S (t)=W ) CB (1
(t)=W ). LB algorithm's behavior indistinguishable kind threshold
algorithm, k set CA (k=W ) = CB (1 k=W ). (We implicitly assume CA (:)
CB (:) chosen solution exists 1 < k < W 1.) question
382

fiCollective Intelligence, Data Routing Braess' Paradox
k optimize total averaged cost across time, particular k
kLB , k LB uses.
go one time step next, routing decision made W time steps
ago drops computation (t), routing decision made newly
included. general, (t + 1) = (t) + 1 router used time used link B
time W time steps past. hand, (t + 1) = (t) 1 router
used B used W time steps ago, (t + 1) = (t) routing decision
made routing decision W time steps ago. general, (t)
change -1, 0, +1 go one time step next.
Consider cases 1 < k < W 1, eventually router must choose A,
subsequent time router switches B . time s(t 1) =
s(t ) = B . implies (t 1) k; (t ) > k. Define value (t 1) k .
Note (t ) = k + 1, k 1 < k k.
time t0 , (t0 ) = k + 1, s(t0 + 1) = B , possible next values
(t0 + 1) = k (t0 + 1) = k + 1, depending old decision s(t W ) gets
dropped window. Similarly, (t0 ) = k , s(t0 + 1) = A, possible
next values (t0 + 1) = k (t0 + 1) = k + 1, depending old decision
dropped. see (t0 ) 2 fk ; k + 1g, stays forever.
means relationship k k , interval W
consecutive time steps subsequent , number packets sent along router X
must 2 (k 1; k +1]. (Note possible send k +1 packets along A, k 1
packets. Therefore number sent along B must 2 [W (k + 1); W (k 1)).
time packet sent along cost incurred cost link average trac
level (t)=W , CA (S (t)=W ). Similarly, time link B chosen, cost incurred
CB (1 (t)=W ). Since (t) 2 fk ; k + 1g, CA (:) CB (:) monotonically
increasing, cost sending packet link 2 (CA ((k 1)=W ); CA ((k + 1)=W ],
sending link B contained [CB (1 (k +1)=W ); CB (1 (k 1)=W )).
know choice must average frequency (across time)

k =W (k + 1)=W . Similarly, B average frequency (1 (k + 1)=W )
1 k =W . Accordingly, average cost bounded






k+1
k
k 1
k + 1
CA
+ 1
CB 1
;
(5)
W

W

W

W

first term provides maximum possible average cost using link A,
second term independently provides maximum possible average cost using link
B . Note actual cost lower since two frequencies bound, one
one B , cannot values indicated. k 1 < k k since
+1 , upper bound bounded
1 kW1 = 1 + W2 kW
k+1
W



CA

k+1
W



+



1+

2

k+1

W

W





CB

1+

2

k+1

W

W



:

(6)

optimal k result average cost lower minimum k
upper bound average cost, given Equation 6. average cost optimal
k bounded minimum k upper bound. Lable argmin
Equation 6 k'.
383

fiWolpert & Tumer
Since values k besides kLB result behavior equivalent LB,
suce simply test k' = kLB . Instead let us evaluate lower bounds similar
fashion evaluated upper bounds. Using average frequencies discussed above,
average cost bounded by:






k 1
1
k
k+1
k
CA
+ 1
CB 1
;
(7)
W

W

W

W

W

first term provides minimum possible average cost using link A,
second term provides minimum possible average cost using link B . Again,
k 1 < k k , term Equation 7 bounded
1

k
W



CA

1

k



W

+



1

2
W

1

k
W





CB

1

2

1

k

W



particular bound holds average cost LB algorithm:





kLB 1
kLB 1
2 kLB 1
2 kLB
CA
+ 1
CB 1
W

W

W

W

W

(8)

:

W

W

1



;

(9)

kLB satisfies CA (kLB =W ) = CB (1 kLB =W ).
appropriate choice CA (:) CB (:), ensure lower bound
cost LB algorithm (Equation 9 evaluated k = kLB ) higher upper
bound average cost incurred optimal algorithm (the minimum k Equation 6). is, best possible average cost achieved load balancing worse
worst average cost could arise optimal routing strategy.
establishes LB engage optimal routing.

Example: Let CA (x) = x2 CB (x) = x. Balancing loads B | setting

2
C
pA (S (t)=W ) = CB (1 (t)=W ) | results (S (t)=W ) = 1 (t)=W , leading kLB =W =
5 1 = :618. W = 1000, associated lower bound average cost (Equation 9)
2
(:618)3 + (:998 :618)2 = :380. hand, CA CB given above, Eq 6
k+1 2
( k+1 )3 + (1 + 2
) . Differentiating respect k setting result
W

k0
W

W

1
3

W

1

p28+48=W

zero leads
=
. window size W = 1000, yields
W +
6
k 0 =W = :548, different result kLB . Plugging Equation 6, upper bound
cost k0 (:549)3 + (1:002 :549)2 = :371, less :380.
References

Ahuja, R. K., Magnanti, T. L., & Orlin, J. B. (1993). Network Flows. Prentice Hall, New
Jersey.
Bass, T. (1992). Road ruin. Discover, 13 (5), 56{61.
Bertsekas, D., & Gallager, R. (1992). Data Networks. Prentice Hall, Englewood Cliffs, NJ.
Bonabeau, E., Henaux, F., Guerin, S., Snyders, D., Kuntz, P., & Theraulaz, G. (1999a).
Routing telecommunications networks \smart" and-like agents. (pre-print).
Bonabeau, E., Sobkowski, A., Theraulaz, G., & Deneubourg, J.-L. (1999b). Adaptive task
allocation inspired model division labor social insects. (pre-print).
384

fiCollective Intelligence, Data Routing Braess' Paradox
Boyan, J. A., & Littman, M. (1994). Packet routing dynamically changing networks:
reinforcement learning approach. Advances Neural Information Processing
Systems - 6, pp. 671{678. Morgan Kaufman.
Choi, S. P. M., & Yeung., D. Y. (1996). Predictive Q-routing: memory based reinforcement
learning approach adaptive trac control. Touretzky, D. S., Mozer, M. C., &
Hasselmo, M. E. (Eds.), Advances Neural Information Processing Systems - 8, pp.
945{951. MIT Press.
Claus, C., & Boutilier, C. (1998). dynamics reinforcement learning cooperative
multiagent systems. Proceedings Fifteenth National Conference Artificial
Intelligence, pp. 746{752, Madison, WI.
Cohen, J. E., & Jeffries, C. (1997). Congestion resulting increased capacity singleserver queueing networks. IEEE/ACM Transactions Networking, 5 (2), 305{310.
Cohen, J. E., & Kelly, F. P. (1990). paradox congestion queuing network. Journal
Applied Probability, 27, 730{734.
Crowe, B. L. (1969). tragedy commons revisited. Science, 166, 1103{1107.
Deo, N., & Pang, C. (1984). Shortest path algorithms: Taxonomy annotation. Networks,
14, 275{323.
Dijkstra, E. (1959). note two problems connection graphs. Numeriche Mathematics, 1 (269-171).
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press, Cambridge, MA.
Glance, N. S. (1993). Dynamics Expectations. Ph.D. thesis, Stanford University.
Glance, N. S., & Hogg, T. (1995). Dilemmas computational societies. Lesser, V.
(Ed.), Proc. 1st International Conference Multi-Agent Systems (ICMAS95),
pp. 117{124, Menlo Park, CA. AAAI Press.
Hardin, G. (1968). tragedy commons. Science, 162, 1243{1248.
Heusse, M., Snyers, D., Guerin, S., & Kuntz, P. (1998). Adaptive agent-driven routing
load balancing communication networks. Advances Complex Systems, 1,
237{254.
Hogg, T. (1995). Social dilemmas computational ecosystems. Proceedings
Fourteenth International Joint Conference Artificial Intelligence, pp. 711{716, San
Mateo, CA. Morgan Kaufmann.
Hu, J., & Wellman, M. P. (1998a). Multiagent reinforcement learning: Theoretical framework algorithm. Proceedings Fifteenth International Conference
Machine Learning, pp. 242{250.
Hu, J., & Wellman, M. P. (1998b). Online learning agents dynamic multiagent system. Proceedings Second International Conference Autonomous
Agents, pp. 239{246.
Huberman, B. A., & Hogg, T. (1988). behavior computational ecologies.
Ecology Computation, pp. 77{115. North-Holland.
385

fiWolpert & Tumer
Huberman, B. A., & Lukose, R. M. (1997). Social dilemmas internet congestion. Science,
277 (5325), 535{537.
Huberman, B. A., & Hogg, T. (1993). emergence computational ecologies. Nadel,
L., & Stein, D. (Eds.), 1992 Lectures Complex Systems, Vol. V SFI Studies
Sciences Complexity, pp. 185{205. Addison-Wesley, Reading, MA.
Huhns, M. E. (Ed.). (1987). Distributed Artificial Intelligence. Pittman, London.
Jennings, N. R., Sycara, K., & Wooldridge, M. (1998). roadmap agent research
development. Autonomous Agents Multi-Agent Systems, 1, 7{38.
Kaelbing, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: survey.
Journal Artificial Intelligence Research, 4, 237{285.
Kelly, F. P. (1996). Modeling communication networks, present future. Philosophical
Trends Royal Society London A, 354, 437{463.
Korilis, Y. A., Lazar, A. A., & Orda, A. (1995). Architecting noncooperative networks.
IEEE Journal Selected Areas Communications, 13 (8), 1241{1251.
Korilis, Y. A., Lazar, A. A., & Orda, A. (1997a). Achieving network optima using Stackelberg routing strategies. IEEE/ACM Transactions Networking, 5 (1), 161{173.
Korilis, Y. A., Lazar, A. A., & Orda, A. (1997b). Capacity allocation noncooperative
routing. IEEE Transactions Automatic Control, 42 (3), 309{325.
Korilis, Y. A., Lazar, A. A., & Orda, A. (1999). Avoiding Braess paradox noncooperative networks. Journal Applied Probability, 36, 211{222.
Kumar, S., & Miikkulainen, R. (1997). Dual reinforcement Q-routing: on-line adaptive
routing algorithm. Artificial Neural Networks Engineering, Vol. 7, pp. 231{238.
ASME Press.
Littman, M. L., & Boyan, J. (1993). distributed reinforcement learning scheme network
routing. Proceedings 1993 International Workshop Applications Neural
Networks Telecommunications, pp. 45{51.
Marbach, P., Mihatsch, O., Schulte, M., & Tsisiklis, J. (1998). Reinforcement learning
call admission control routing integrated service networks. Advances
Neural Information Processing Systems - 10, pp. 922{928. MIT Press.
Orda, A., Rom, R., & Shimkin, N. (1993a). Competitive routing multiuse communication
networks. IEEE/ACM Transactions Networking, 1 (5), 510{521.
Orda, A., Rom, R., & Sidi, M. (1993b). Minimum delay routing stochastic networks.
IEEE/ACM Transactions Networking, 1 (2), 187{198.
Sandholm, T., Larson, K., Anderson, M., Shehory, O., & Tohme, F. (1998). Anytime coalition structure generation worst case guarantees. Proceedings Fifteenth
National Conference Artificial Intelligence, pp. 46{53.
Sandholm, T., & Lesser, V. R. (1995). Issues automated negotiations electronic commerce: extending contract net protocol. Proceedings Second International
Conference Multi-Agent Systems, pp. 328{335. AAAI Press.
386

fiCollective Intelligence, Data Routing Braess' Paradox
Schaerf, A., Shoham, Y., & Tennenholtz, M. (1995). Adaptive load balancing: study
multi-agent learning. Journal Artificial Intelligence Research, 162, 475{500.
Shenker, S. J. (1995). Making greed work networks: game-theoretic analysis switch
service disciplines. IEEE Transactions Networking, 3 (6), 819{831.
Stone, P. (2000). TPOT-RL applied network routing. Proceedings Seventeenth
International Machine Learning Conference, pp. 935{942. Morgan Kauffman.
Subramanian, D., Druschel, P., & Chen, J. (1997). Ants reinforcement learning: case
study routing dynamic networks. Proceedings Fifteenth International
Conference Artificial Intelligence, pp. 832{838.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine
Learning, 3, 9{44.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Sycara, K. (1998). Multiagent systems. AI Magazine, 19 (2), 79{92.
Tumer, K., Agogino, A., & Wolpert, D. (2002). Learning sequences actions collectives
autonomous agents. Proceedings First International Joint Conference
Autonomous Agents Multi-Agent Systems, Bologna, Italy.
Tumer, K., & Wolpert, D. H. (2000). Collective intelligence Braess' paradox.
Proceedings Seventeenth National Conference Artificial Intelligence, pp. 104{
109, Austin, TX.
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3/4), 279{292.
Wolpert, D. H., Kirshner, S., Merz, C. J., & Tumer, K. (2000). Adaptivity agent-based
routing data networks. Proceedings fourth International Conference
Autonomous Agents, pp. 396{403.
Wolpert, D. H., Sill, J., & Tumer, K. (2001). Reinforcement learning distributed domains:
Beyond team games. Proceedings Seventeenth International Joint Conference
Artificial Intelligence, pp. 819{824, Seattle, WA.
Wolpert, D. H., & Tumer, K. (1999). Introduction Collective Intelligence. Tech.
rep. NASA-ARC-IC-99-63, NASA Ames Research Center. URL:http://ic.arc.nasa.gov/ic/projects/coin pubs.html. appear Handbook Agent Technology,
Ed. J. M. Bradshaw, AAAI/MIT Press.
Wolpert, D. H., & Tumer, K. (2001). Optimal payoff functions members collectives.
Advances Complex Systems, 4 (2/3), 265{279.
Wolpert, D. H., Tumer, K., & Frank, J. (1999). Using collective intelligence route internet
trac. Advances Neural Information Processing Systems - 11, pp. 952{958. MIT
Press.
Wolpert, D. H., Wheeler, K., & Tumer, K. (2000). Collective intelligence control
distributed dynamical systems. Europhysics Letters, 49 (6).

387

fiJournal Artificial Intelligence Research 16 (2002) 259-292

Submitted 9/01; published 4/02

Efficient Reinforcement Learning Using
Recursive Least-Squares Methods
Xin Xu
Han-gen
Dewen Hu

XUXIN_MAIL@263.NET
HEHANGEN@CS.HN.CN
DWHU@NUDT.EDU.CN

Department Automatic Control
National University Defense Technology
ChangSha, Hunan, 410073, P.R.China

Abstract
recursive least-squares (RLS) algorithm one well-known algorithms used
adaptive filtering, system identification adaptive control. popularity mainly due
fast convergence speed, considered optimal practice. paper, RLS methods
used solve reinforcement learning problems, two new reinforcement learning
algorithms using linear value function approximators proposed analyzed. two
algorithms called RLS-TD( ) Fast-AHC (Fast Adaptive Heuristic Critic), respectively.
RLS-TD( ) viewed extension RLS-TD(0) =0 general 0 1,
multi-step temporal-difference (TD) learning algorithm using RLS methods. convergence
probability one limit convergence RLS-TD( ) proved ergodic Markov
chains. Compared existing LS-TD( ) algorithm, RLS-TD( ) advantages
computation suitable online learning. effectiveness RLS-TD( )
analyzed verified learning prediction experiments Markov chains wide range
parameter settings.
Fast-AHC algorithm derived applying proposed RLS-TD( ) algorithm
critic network adaptive heuristic critic method. Unlike conventional AHC algorithm,
Fast-AHC makes use RLS methods improve learning-prediction efficiency critic.
Learning control experiments cart-pole balancing acrobot swing-up problems
conducted compare data efficiency Fast-AHC conventional AHC.
experimental results, shown data efficiency learning control also improved
using RLS methods learning-prediction process critic. performance
Fast-AHC also compared AHC method using LS-TD( ). Furthermore,
demonstrated experiments different initial values variance matrix RLS-TD( )
required get better performance learning prediction also learning control.
experimental results analyzed based existing theoretical work transient
phase forgetting factor RLS methods.

1. Introduction
recent years, reinforcement learning (RL) active research area machine
learning also control engineering, operations research robotics (Kaelbling et al.,1996;
Bertsekas, et al.,1996; Sutton Barto,1998; Lin,1992). computational approach
2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiXU, HE, & HU

understand automate goal-directed learning decision-making, without relying
exemplary supervision complete models environment. RL, agent placed
initial unknown environment receives evaluative feedback environment.
feedback called reward reinforcement signal. ultimate goal RL learn strategy
selecting actions expected sum discounted rewards maximized.
Since lots problems real world sequential decision processes delayed
evaluative feedback, research RL focused theory algorithms learning
solve optimal control problem Markov decision processes (MDPs) provide
elegant mathematical model sequential decision-making. operations research, many results
presented solve optimal control problem MDPs model information.
However, reinforcement learning, model information assumed unknown,
different methods studied operations research dynamic programming.
dynamic programming, two elemental processes, policy evaluation process policy improvement process, respectively. RL, two similar processes.
One called learning prediction called learning control. goal learning
control estimate optimal policy optimal value function MDP without knowing
model. Learning prediction aims solve policy evaluation problem stationary-policy
MDP without prior model regarded sub-problem learning control.
Furthermore, RL, learning prediction different supervised learning. pointed
Sutton (1988), prediction problems supervised learning single-step prediction
problems reinforcement learning multi-step prediction problems. solve
multi-step prediction problems, learning system must predict outcomes depend future
sequence decisions. Therefore, theory algorithms multi-step learning prediction
become important topic RL much research work done literature (Sutton,
1988; Tsitsiklis Roy, 1997).
Among proposed multi-step learning prediction methods, temporal-difference (TD)
learning (Sutton, 1988) one popular methods. studied applied early
research machine learning, including celebrated checkers-playing program (Minsky, 1954;
Samuel, 1959). 1988, Sutton presented first formal description temporal- difference
methods TD( ) algorithm (Sutton,1988). Convergence results established tabular
temporal-difference learning algorithms cardinality tunable parameters
state space (Sutton, 1988; Watkins,et al.,1992; Dayan,et al., 1994; Jaakkola, et
al.,1994). Since many real-world applications large infinite state space, value function
approximation (VFA) methods need used cases. combined nonlinear
value function approximators, TD( ) guarantee convergence several results
regarding divergence reported literature (Tsitsiklis Roy,1997). TD( )
linear function approximators, also called linear TD( ) algorithms, several convergence
proofs presented. Dayan (1992) showed convergence mean linear TD( )
algorithms arbitrary 0 1 . Tsitsiklis Roy (1994) proved convergence
special class TD learning algorithms, known TD(0), Tsitsiklis Roy (1997),
extended early results general linear TD( ) case proved convergence
probability one.
linear TD( ) algorithms rules updating parameters similar
gradient-descent methods. However, gradient-learning methods, step-size schedule must
carefully designed guarantee convergence also obtain good performance.
260

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

addition, inefficient use data slows convergence algorithms. Based
theory linear least-squares estimation, Brartke Barto (1996) proposed two
temporal-difference algorithms called Least-Squares TD(0) algorithm (LS-TD(0))
Recursive Least- Squares TD(0) algorithm (RLS-TD(0)), respectively. LS-TD(0) RLS-TD(0)
efficient statistical sense conventional linear TD( ) algorithms
eliminate design step-size schedules. Furthermore, convergence LS-TD(0)
RLS-TD(0) provided theory. two algorithms viewed
least-squares versions conventional linear TD(0) methods. However, shown
literature, TD learning algorithms TD( ) 0< <1 update predictions based
estimates multiple steps efficient Monte-Carlo methods well TD(0).
employing mechanism eligibility traces, determined , TD( ) algorithms
0< <1 extract information historical data. Recently, class linear
temporal-difference learning algorithms called LS-TD( ) proposed Boyan
(1999,2002), least-squares methods employed compute value-function estimation
TD( ) 0 1. Although LS-TD( ) efficient TD( ), requires much
computation per time-step online updates needed number state features
becomes large.
system identification, adaptive filtering adaptive control, recursive least-squares
(RLS) (Young,1984; Ljung, 1983; Ljung,1977) method, commonly used reduce
computational burden least-squares methods, suitable online estimation control.
Although RLS-TD(0) makes use RLS methods, employ mechanism
eligibility traces. Based work Tsitsiklis Roy (1994, 1997), Boyan (1999,2002)
motivated ideas, new class temporal-difference learning methods, called
RLS-TD( ) algorithm, proposed analyzed formally paper. RLS-TD( ) superior
conventional linear TD( ) algorithms makes use RLS methods improve
learning efficiency statistical point view eliminates step-size schedules.
RLS-TD( ) mechanism eligibility traces viewed extension
RLS-TD(0) =0 general 0 1. convergence probability 1 RLS-TD( )
proved ergodic Markov chains limit convergence also analyzed. learning
prediction experiments Markov chains, performance RLS-TD( ) TD( ) well
LS-TD( ) compared, wide range parameter settings tested. addition, influence initialization parameters RLS-TD( ) also discussed. observed
rate convergence influenced initialization variance matrix,
phenomenon investigated theoretically adaptive filtering (Moustakides, 1997; Haykin, 1996).
analyzed following sections, two benefits extension
RLS-TD(0) RLS-TD( ). One value (0 1) still affect performance
RLS-based temporal-difference algorithms. Although RLS-TD( ), rate
convergence mainly influenced initialization variance matrix, bound
approximation error dominantly determined parameter . smallest error bound
obtained =1 worst bound obtained =0. bounds suggest
value selected appropriately obtain best approximation error. second
benefit RLS-TD( ) suitable online learning LS-TD( ) since
computation per time-step reduced O(K3) O(K2), K number state
features.
Adaptive-Heuristic-Critic (AHC) learning algorithm class reinforcement learning
261

fiXU, HE, & HU

methods actor-critic architecture used solve full reinforcement learning
learning control problems. applying RLS-TD( ) algorithm critic, Fast-AHC
algorithm proposed paper. Using RLS methods critic, performance learning
prediction critic improved learning control problems solved
efficiently. Simulation experiments learning control cart-pole balancing problem
swing-up acrobot conducted verify effectiveness Fast-AHC method.
comparing conventional AHC methods use TD( ) critic, demonstrated
Fast-AHC obtain higher data efficiency conventional AHC methods. Experiments
performance comparisons AHC methods using LS-TD( ) Fast-AHC also
conducted. learning control experiments, also illustrated initializing constant
variance matrix RLS-TD( ) influences performance Fast-AHC different values
constant selected get better performance different problems.
results analyzed based theoretical work transient phase RLS methods.
paper organized follows. Section 2, introduction previous linear
temporal-difference algorithms presented. Section 3, RLS-TD( ) algorithm proposed
convergence (with probability one) proved. Section 4, simulation example
value-function prediction absorbing Markov chains presented illustrate effectiveness
RLS-TD( ) algorithm, different parameter settings different algorithms
including LS-TD( ) studied. Section 5, Fast-AHC method proposed
simulation experiments learning control cart-pole balancing acrobot
conducted compare Fast-AHC conventional AHC method well
LS-TD( )-based AHC method. simulation results presented analyzed detail.
last section contains concluding remarks directions future work.

2. Previous Work Linear Temporal-Difference Algorithms
section, brief discussion conventional linear TD( ) algorithm RLS-TD(0)
well LS-TD( ) algorithm given. First all, mathematical notations
presented follows.
Consider Markov chain whose states lie finite countable infinite space S. states
Markov chain indexed {1,2,,n}, n possibly infinite. Although
algorithms results paper applicable Markov chains general state space,
discussion paper restricted within cases countable state space
simplify notation. extension Markov chains general state space requires
translation matrix notation operator notation.
Let trajectory generated Markov chain denoted {xt |t=0,1,2,; xt S}.The
dynamics Markov chain described transition probability matrix P whose (i,j)-th
entry, denoted pij, transition probability xt+1=j given xt=i. state transition
xt xt+1, scalar reward rt defined. value function state defined follows:


V (i ) = E{ rt x 0 = i}

(1)

=0

0< 1 discount factor.
TD( ) algorithm, two basic mechanisms temporal difference
262

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

eligibility trace, respectively. Temporal differences defined differences
two successive estimations following form.
~
~
= rt + Vt ( xt +1 ) Vt ( xt )
(2)
~
xt+1 successive state xt, V ( x) denotes estimate value function V(x) rt
reward received state transition xt xt+1.
Eligibility trace viewed algebraic trick improve learning efficiency
without recording data multi-step prediction process. trick based idea
using truncated return Markov chain. temporal-difference learning eligibility
traces, n-step truncated return defined
~
Rtn = rt + rt +1 + ... + n 1 rt + n 1 + nVt ( + n )
(3)
absorbing Markov chain whose length T, weighted average truncated returns

1

Rt = (1 )

n1 Rtn + 1 RT

(4)

n =1

0 1 decaying factor RT= rt + rt +1 + ... + rT Monte-Carlo return
terminal state. step TD( ) algorithm, update rule value function
estimation determined weighted average truncated returns defined above.
corresponding update equation
~
~
Vt ( ) = ( Rt Vt ( ))
(5)
learning factor.
update equation (5) used whole trajectory Markov chain
observed. realize incremental online learning, eligibility traces defined state
follows:

z ( ) + 1,
z +1 ( ) =
z ( ),

=


online TD( ) update rule eligibility traces
~
~
Vt +1 ( si ) = Vt ( si ) + z +1 ( si )

(6)

(7)

temporal difference time step t, defined (2) z0(s)=0 s.
Since state space Markov chain usually large infinite practice, function
approximators neural networks commonly used approximate value function.
TD( ) algorithms linear function approximators popular well-studied ones.
Consider general linear function approximator fixed basis function vector

( x ) = (1 ( x ), 2 ( x ),..., n ( x ))T
estimated value function denoted

~
Vt ( x) = ( x)Wt
263

(8)

fiXU, HE, & HU

Wt =(w1, w2,,wn)T weight vector.
corresponding incremental weight update rule

r
Wt +1 = Wt + (rt + ( xt +1 )Wt ( xt )Wt ) z +1
r
eligibility trace vector z ( ) = ( z1t ( ), z 2t ( ),..., z nt ( )) defined
r
r
z +1 = z + ( xt )

(9)

(10)

Tsitsiklis Roy (1997), linear TD( ) algorithm proved converge
probability 1 certain assumptions limit convergence W* also derived,
satisfies following equation.
E 0 [ A( X )]W * E 0 [b( X )] = 0

(11)

Xt =(xt,xt+1,zt+1) (t=1,2,) form Markov process, E0[] stands expectation
respect unique invariant distribution {Xt}, A(Xt) b(Xt) defined
r
A( X ) = z ( ( xt ) ( xt +1 ))
(12)

r
b( X ) = z rt

(13)

improve efficiency linear TD() algorithms, least-squares methods used
linear TD(0) algorithm, LS-TD(0) RLS-TD(0) algorithms suggested (Brartke
Barto, 1996). LS-TD(0) RLS-TD(0), following quadratic objective function defined.
1

J = [rt ( tT tT+1 )W ] 2

(14)

=1

Thus, aim LS-TD(0) RLS-TD(0) obtain least-squares estimation real
value function satisfies following Bellman equation.
V ( xt ) = E[rt ( xt , xt +1 ) + V ( xt +1 )]

(15)

employing instrumental variables approach (Soderstrom Stoica, 1983),
least-squares solution (14) given




=1

=1

W LS TD ( 0) = ( ( ( +1 ) )) 1 ( rt )

(16)

instrumental variable chosen uncorrelated input output noises.
RLS-TD(0), recursive least-squares methods used decrease computational burden LS-TD(0). update rules RLS-TD(0) follows:
Wt +1 = Wt + Pt (rt ( +1 ) Wt ) /(1 + ( +1 ) Pt )

(17)

Pt +1 = Pt Pt ( +1 ) Pt /(1 + ( +1 ) Pt )

(18)

convergence (with probability one) LS-TD(0) RLS-TD(0) proved periodic
absorbing Markov chains certain assumptions (Brartke Barto,1996).
264

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Boyan (1999,2002), LS-TD( ) proposed solving (11) directly model-based
property LS-TD( ) also analyzed. However, LS-TD( ), computation per time-step
O(K3), i.e., cubic order state feature number. Therefore computation required
LS-TD() increases fast K increases, undesirable online learning.
next section, propose RLS-TD( ) algorithm making use recursive
least-squares methods computational burden LS-TD( ) reduced O(K3)
O(K2). also give rigorous mathematical analysis algorithm, convergence
(with probability 1) RLS-TD( ) proved.

3. RLS-TD( ) Algorithm
Markov chain discussed above, linear function approximators used,
least-squares estimation problem (11) following objective function.
J=





=1

=1

A( X )W b( X )

2

(19)

A( X ) R nn , b( X ) R n defined (12) (13), respectively, Euclid norm
n number basis functions.
LS-TD( ), least-squares estimate weight vector W computed according
following equation.




=1

=1

W LS TD ( ) = AT1bT = ( A( X )) 1 ( b( X ))

(20)



r
= ( A( X )) = z ( ( xt ) ( xt +1 ))

(21)



r
bT = b( X ) = z rt

(22)



=0

=0

=0

=0

well known system identification, adaptive filtering control, RLS methods
commonly used solve computational memory problems least-squares algorithms.
sequel, present RLS-TD( ) algorithm based idea. First, matrix inverse lemma given follows:
Lemma 1(Ljung, et al.,1983). R nn , B R n1 , C R 1n invertible,

( + BC ) 1 = 1 1 B ( + CA 1 B ) 1 CA 1

(23)

Pt = At1

(24)

Let

265

fiXU, HE, & HU

P0 =

(25)

r
K +1 = Pt +1 z

(26)

positive number identity matrix.
weight update rules RLS-TD( ) given
r
r
K +1 = Pt z /( + ( ( xt ) ( xt +1 )) Pt z )
Wt +1 = Wt + K +1 (rt ( ( xt ) ( xt +1 ))Wt )

Pt +1 =

1



r
r
[ Pt Pt z [ + ( ( xt ) ( xt +1 )) Pt z )] 1 ( ( xt ) ( xt +1 )) Pt ]

(27)
(28)

(29)

standard RLS-TD() algorithm, =1; general forgetting factor RLS-TD()
case, 0<1.
forgetting factor (0<1) usually used adaptive filtering improve
performance RLS methods non-stationary environments. forgetting factor RLS-TD( )
algorithm 0<1 derived using similar techniques Haykin (1996). detailed
derivation RLS-TD() referred Appendix A.
follows, descriptions RLS-TD( ) two different kinds Markov chains
given. First, complete description RLS-TD( ) ergodic Markov chains presented below.

Algorithm 1 RLS-TD( ) ergodic Markov chains

1: Given:
termination criterion algorithm.
set basis functions { j (i ) } (j=1,2,,n) state i, n
number basis functions.
2: Initialize:
(2.1) Let t=0.
(2.2) Initialize weight vector Wt, variance matrix Pt , initial state x0.
r
(2.3) Set eligibility traces vector z 0 =0.
3: Loop:
(3.1) current state xt, observe state transition xt xt+1
reward r(xt ,xt+1).
(3.2) Apply equations (27)-(29) update weight vector.
(3.3) t=t+1.
termination criterion satisfied.

RLS-TD( ) algorithm absorbing Markov chains little different
algorithm coping state features absorbing states. Following description
266

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

RLS-TD( ) absorbing Markov chains.

Algorithm 2 RLS-TD( ) absorbing Markov chains

1: Given:
termination criterion algorithm.
set basis functions { j (i ) } (j=1,2,,n) state i, n
number basis functions.
2: Initialize:
(2.1) Let t=0.
(2.2) Initialize weight vector Wt, variance matrix Pt , initial state x0.
r
(2.3) Set eligibility traces vector z 0 =0.
3: Loop:
(3.1) current state xt,
xt absorbing state, set (xt+1)=0, r(xt)=rT, rT terminal
reward.
Otherwise, observe state transition xt xt+1 reward
r(xt ,xt+1).
(3.2) Apply equations (27)-(29) update weight vector.
(3.3) xt absorbing state, re-initialize process setting xt+1 initial
r
state set eligibility traces z zero vector.
(3.4) t=t+1.
termination criterion satisfied.

RLS-TD( ) algorithm absorbing Markov chains, weight updates
absorbing states treated differently process re-initialized absorbing states
transform absorbing Markov chain equivalent ergodic Markov chain.
following convergence analysis, focus ergodic Markov chains.
similar assumptions Tsitsiklis Roy (1997), prove proposed
RLS-TD( ) algorithm converges probability one.
Assumption 1. Markov chain {xt}, whose transition probability matrix P, ergodic,
unique distribution satisfies



P =
(30)
(i)>0 finite infinite vector, depending cardinality S.


Assumption 2. Transition rewards r(xt,xt+1) satisfy

E 0 [r 2 ( xt , xt +1 )] <

(31)

E0[ ] expectation respect distribution .
Assumption 3. matrix = [1 , 2 ,..., n ] R N n full column rank, is, basis
267

fiXU, HE, & HU

functions (i=1,2,,n) linearly independent.
Assumption 4. every (i=1,2,,n), basis function satisfies
2

E 0 [ ( xt )] <

(32)

1
A( X )] non-singular T>0.
=1
Assumptions 14 almost linear TD() algorithms discussed
Tsitsiklis Roy (1997) except Assumption 1, ergodic Markov chains considered.
Assumption 5 specially needed convergence RLS-TD() algorithm.
Based assumptions, convergence theorem RLS-TD() given
follows:
Assumption 5. matrix [ P01 +

Theorem 1. Markov chain satisfies Assumptions 15, asymptotic estimate found
RLS-TD( ) converges, probability 1, W* determined (11).

proof Theorem 1, please refer Appendix B. condition specified
Assumption 5 satisfied setting P0= appropriately.
According Theorem 1, RLS-TD( ) converges solution conventional linear
TD( ) algorithms do, satisfies (11). limit convergence characterized
following theorem.
Theorem 2 (Tsitsiklis Roy ,1997) Let W* weight vector determined (11) V*
true value function Markov chain, Assumption 14, following relation
holds.
1
W * V *
(33)
V * V *


1



X



=

X DX , = ( ) 1 .

explanations notations Theorem 2, please refer Appendix B.
discussed Tsitsiklis Roy (1997), theorem shows distance
limiting function W* true value function V* bounded smallest bound
approximation error obtained =1. every <1, bound actually deteriorates
decreases. worst bound obtained =0. Although bound, strongly
suggests higher values likely produce accurate approximations V*.
Compared LS-TD(), additional parameter RLS-TD(), value
initial variance matrix P0. pointed Haykin (1996,pp.570), exact value
initializing constant insignificant effect data length large enough.
means limit, final solutions obtained LS RLS almost same.
influence transient phase, positive constant becomes large enough goes
infinity, transient behavior RLS almost LS methods (Ljung,
1983). initialized relatively small value, transient phases RLS LS
different. practice, observed variable performance RLS
function initialization (Moustakides, 1997). cases, RLS exhibit
significantly faster convergence initialized relatively small positive definite matrix
initialized large one (Haykin,1996; Moustakides, 1997; Hubing Alexander,
268

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

1989). first effort toward direction statistical analysis RLS soft exact
initialization limits case number iterations less size
estimation vector (Hubing Alexander, 1989). Moustakides (1997) provided theoretical
analysis relation algorithmic performance RLS initialization .
using settling time performance measure, Moustakides proved well-known
rule initialization relatively small matrix preferable cases high medium
signal-to-noise ratio (SNR), whereas low SNR, relatively large matrix must selected
achieving best results. following learning prediction experiments RLS-TD(), well
learning control simulation Fast-AHC, observed value initializing
constant also plays important role convergence performance, theoretical
analyses provide clue explain experimental results.

4. Learning Prediction Experiments Markov Chains
section, illustrative example given show effectiveness proposed
RLS-TD() algorithm. Furthermore, algorithmic performance influence
initializing constant studied.
example finite-state absorbing Markov chain called Hop-World problem (Boyan,
1999). shown Figure 1, Hop-World problem 13-state Markov chain
absorbing state.

Figure 1: Hop-World Problem
Figure 1, state 12 initial state trajectory state 0 absorbing state.
non-absorbing state two possible state transitions transition probability 0.5.
state transition reward 3 except transition state 1 state 0 reward 2.
Thus, true value function state (0i12) 2i.
apply linear temporal-difference algorithms value function prediction problem, set
four-element state features basis functions chosen, shown Figure 1. state
features states 12,8,4 0 are, respectively, [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]
state features states obtained linearly interpolating these.
simulation, RLS-TD( ) algorithm well LS-TD() conventional linear
TD( ) algorithms used solve value function prediction problem without
knowing model Markov chain. experiments, trial defined period
initial state 12 terminal state 0. performance algorithms evaluated
averaged root mean squared (RMS) error value-function predictions 13 states.
parameter setting, performance averaged 20 independent Monte-Carlo runs.
Figure 2 shows learning curves RLS-TD() conventional linear TD() algorithms
three different parameter settings. parameter set 0.3 algorithms
269

fiXU, HE, & HU

step-size parameter TD() following form.

n = 0

N0 +1
N0 + n

(34)

step-size schedule also studied Boyan (1999). experiments, three
different settings used,
(s1) 0 = 0.01 , N 0 = 10 6
(s2) 0 = 0.01 , N 0 = 1000
(s3) 0 = 0.1 , N 0 = 1000 .

(35)

Different Boyan (1999), linear TD() algorithms applied
online forms, update weights every state transitions. parameter n (34)
number state transitions. run, weights initialized zeroes. Figure 2,
learning curves conventional linear TD() algorithms step-size schedules (s1), (s2)
(s3) shown curves 1,2 3, respectively. curve, averaged RMS errors
value function predictions states 20 independent runs plotted trial.
Curve 4 shows learning performance RLS-TD(). One additional parameter RLS-TD()
initial value variance matrix P0. experiment, set 500,
relatively large value. Figure 2, concluded making use RLS methods,
RLS-TD() obtain much better performance conventional linear TD() algorithms
eliminates design problem step-size schedules. experiments linear TD()
RLS-TD() different parameters also conducted similar results obtained
initial values RLS-TD() large conclusion confirmed.

Figure 2: Performance comparison RLS-TD() TD()
1,2,3 ---TD(0.3) step-size parameters specified (s1),(s2) (s3)
4RLS-TD(0.3) initial variance matrix P0=500I
done demonstrative experiments investigate influence performance
RLS-TD() algorithm. Figure 3 shows performance comparison RLS-TD()
270

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

algorithms using two different initial parameters variance matrix P0, P0=0.1I
P0=1000I, respectively. forgetting factor =0.995. performance suggested
algorithm measured averaged RMS errors value function prediction first
200 trials 20 independent runs 13 states. experiments, 11 settings
parameter tested, 0.1n (n=0,1,,10).
Figure 3, clearly shown performance RLS-TD() large initial value
much better RLS-TD() small initial value . experiments
different parameter settings , similar results also obtained. may refer
phenomenon low SNR case forgetting factor RLS studied Moustakides (1997).
Hop-World problem, stochastic state transitions could introduce high equation
residuals A( X )W b( X ) (19), corresponds additive noise large variance,
i.e., low SNR case. discussed Section 2, forgetting factor RLS low
SNR cases, relatively large initializing constant must selected better results. full
understanding phenomenon yet found.

Figure 3: Performance comparison RLS-TD() different initial value (=0.995)
performance RLS-TD() unit forgetting factor =1 also tested
experiments. Although initial value effect RLS =1 discussed intensively
(Moustakides,1997), effects observed empirically case =1
<1, shown Figure 4.
experiments, also found initialized small value,
performance sensitive values parameter . case, convergence
speed RLS-TD() increases increases 0 1, shown Figure 3.
Furthermore, fixed, performance RLS-TD() deteriorates becomes smaller,
shown Figure 5 .

271

fiXU, HE, & HU

Figure 4: Performance comparison RLS-TD() different initial value (=1)

Figure 5: Learning curves LS-TD() RLS-TD() different (=1)

Figure 5, learning curves RLS-TD() different initializing constants
shown compared LS-TD(). experiment, set 0.5. Figure 5,
shown performance RLS-TD() approaches LS-TD() becomes large.
well known, becomes large enough, performance RLS LS methods
almost same. Figure 6 shows performance comparison LS-TD()
RLS-TD() large value . initial variance matrix RLS-TD() set 500I
every runs, identity matrix.

272

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Figure 6: Performance comparison LS-TD() RLS-TD() =1 large initial
value
Based experimental results, concluded convergence speed
RLS-TD( ) mainly influenced initial value variance matrix parameter
. Detailed discussions properties RLS-TD( ) given follows:
(1) relatively large, effect becomes small. large enough goes
infinity, performance RLS-TD( ) LS-TD( ) almost same,
discussed above. cases, effect speed convergence insignificant,
coincides discussion Boyan (1999). However, described Theorem 2,
value still affects ultimate error bound value function approximation.
(2) relatively small, observed convergence performance
RLS-TD() different LS-TD() influenced values .
experiments Hop-World problem, results show smaller values lead
slower convergence. results may explained theoretical analysis transient
phase forgetting factor RLS (Moustakides,1997). According theory Moustakides
(1997), larger values needed better performance cases low SNR
smaller values preferable fast convergence cases high medium SNR.
different values must selected faster convergence RLS-TD( ) different cases.
Especially, cases, high SNR case discussed Moustakides (1997), RLS
methods small values obtain fast speed convergence.
(3) Compared conventional linear TD( ) algorithms, RLS-TD( ) algorithm
obtain much better performance making use RLS methods value function prediction
problems. Furthermore, TD( ), step-size schedule needs carefully designed achieve
good performance, RLS-TD( ), initial value variance matrix selected
according criterion large small value.
(4) comparison LS-TD( ) RLS-TD( ), one preferable depends
objective. online applications, RLS-TD( ) advantages computational efficiency
computation per step RLS-TD( ) O(K2) LS-TD( ), O(K3),
273

fiXU, HE, & HU

K number state features. Moreover, seen later, RLS-TD( ) obtain better
transient convergence performance LS-TD( ) cases. hand, LS-TD( )
may preferable RLS-TD( ) long-term convergence performance, seen
Figure 5. system identification point view, LS-TD( ) obtain unbiased
parameter estimates face white additive noises RLS-TD( ) finite would
possess large parameter discrepancies.

5. Fast-AHC Algorithm Two Learning Control Experiments
section, Fast-AHC algorithm proposed based results learning
prediction solve learning control problems. Two learning control experiments conducted
illustrate efficiency Fast-AHC.
5.1 Fast-AHC Algorithm

ultimate goal reinforcement learning learning control, i.e., estimate optimal
policies optimal value functions Markov decision processes (MDPs). now, several
reinforcement learning control algorithms including Q-learning (Watkins Dayan,1992),
Sarsa-learning (Singh, et al.,2000) Adaptive Heuristic Critic (AHC) algorithm (Barto,
Sutton Anderson,1983) proposed. Among methods, AHC method
different Q-learning Sarsa-learning value-function-based methods.
AHC method, value functions policies separately represented value-functionbased methods policies determined value functions directly. two
components AHC method, called critic actor, respectively. actor
used generate control actions according policies. critic used evaluate
policies represented actor provide actor internal rewards without waiting
delayed external rewards. Since objective critic policy evaluation learning
prediction, temporal-difference learning methods chosen critics learning algorithms.
learning algorithm actor determined estimation gradient policies.
following discussion, detailed introduction AHC method given.
Figure 7 shows architecture learning system based AHC method. learning
system consists critic network actor network. inputs critic network include
external rewards state feedback environment. internal rewards provided
critic network called temporal-difference (TD) signals.
reinforcement learning methods, whole system modeled MDP denoted
tuple {S,A,P,R},where state set, action set, P state transition probability
R reward function. policy MDP defined function :SPr(A),
Pr(A) probability distribution action space. objective AHC method
estimate optimal policy * satisfying following equation.


J = max J = max E [ rt ]
*





(36)

=0

discount factor rt reward time-step tE[ ] stands expectation
respect policy state transition probabilities J expected total
reward.

274

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Figure 7: AHC learning system
value function stationary policy optimal value function optimal
policy defined follows:


V ( ) = E [ rt 0 = ]

(37)

V * ( ) = E *[ rt s0 = ]

(38)

=0


=0

According theory dynamic programming, optimal value function satisfies
following Bellman equation.
(39)
V * ( ) = max[ R ( s, ) + EV * ( ' )]


R(s,a) expected reward received taking action state s.
AHC, critic uses temporal-difference learning approximate value function
current policy. linear function approximators used critic, weight update
equation
Wt +1 = Wt + [rt + V ( +1 ) V ( )]z

(40)

zt eligibility trace defined (10).
action selection policy actor determined current state value
function estimation critic. Suppose neural network weight vector u=[u1, u2,, um]
used actor, output actor network

= f (u , st )

(41)

action outputs actor determined following Gaussian probabilistic distribution.
( )2
p r ( ) = exp( 2 )
(42)



mean value given (41) variance given

= k1 /(1 + exp(k 2V ( ))

(43)

equation, k1 k2 positive constants V(st) value function es275

fiXU, HE, & HU

timation critic network.
obtain learning rule actor, estimation policy gradient given
follows:
J
J

=
rt
u
u
u

(44)

rt internal reward TD signal provided critic:

rt = rt + V ( st +1 ) V ( st )

(45)

Since AHC method, critic used estimate value function actors policy
provide internal reinforcement using temporal-difference learning algorithms,
efficiency temporal-different learning learning prediction greatly influence whole
learning systems performance. Although policy actor changing, may change
relatively slowly especially fast convergence learning prediction critic
realized. previous sections, RLS-TD( ) shown better data efficiency
conventional linear TD( ) algorithms fast convergence speed obtained
initializing constant chosen appropriately. Thus, applying RLS-TD( ) policy
evaluation critic network improve learning prediction performance critic
promising enhance whole systems learning control performance. Based
idea, new AHC method called Fast-AHC algorithm proposed paper. efficiency
Fast-AHC algorithm verified empirically detailed analysis results given.
Following complete description Fast-AHC algorithm.

Algorithm 3: Fast-AHC algorithm
1: Given: critic neural network actor neural network, linear
parameters, stop criterion algorithm.
2: Initialize state MDP learning parameters, set t=0.
3: stop criterion satisfied,
(3.1) According current state , compute output actor network ,

(3.2)

determine actual action actor probability distribution given
(42).
Take action MDP, observe state transition

+1 , set reward rt = r ( st , st +1 ) .
(3.3)
(3.4)

(3.5)

Apply RLS-TD( ) algorithm described (27)-(29) update weights
critic network.
Apply following equation update weights actor network,
J
+1 = +
(46)

learning factor actor.
Let t=t+1, return 3.

276

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

5.2 Learning Control Experiments Cart-Pole Balancing Problem
balancing control inverted pendulums typical nonlinear control problem
widely studied control theory also artificial intelligence. research
artificial intelligence, learning control inverted pendulums considered standard test
problem machine learning methods, especially RL algorithms. studied
early work Michies BOXES system (Michie,et al.,1968) later Barto Sutton (1983),
learning controllers two output values: +10(N) 10(N). Berenji, et
al.(1992) Lin, et al.(1994), AHC methods continuous outputs applied cart-pole
balancing problem. paper, cart-pole balancing problem continuous control values
used illustrate effectiveness Fast-AHC method.
Figure 8 shows typical cart-pole balancing control system, consists cart moving
horizontally pole one end fixed cart. Let x denote horizontal distance
center cart center track, x negative cart
left part track. Variable denotes angle pole upright position (in
degrees) F amount force (N) applied cart move towards left right.
control system four state variables x, x& , ,& , x& ,& derivatives x ,
respectively.
Figure 8, mass cart M=1.0kg, mass pole m=0.1kg, half-pole
length l=0.5m, coefficient friction cart track c=0.0005 coefficient
friction pole cart p=0.000002. boundary constraints state variables
given follows.
12 12
(47)
2.4m x 2.4m
(48)
dynamics control system described following equations.

p (m + )&
(m + ) g sin cos [ F + ml& 2 sin c sgn( x& )]

ml
&& =

4
2
(49)

( + m)l ml cos
3


F + ml (& 2 sin && cos ) c sgn( x& )
&x& =
+m

g acceleration due gravity, 9.8m/s2. parameters
dynamics equations studied Barto et al. (1983).

Figure 8: cart-pole balancing control system
277

fiXU, HE, & HU

learning control experiments pole-balancing problem, dynamics (49)
assumed unknown learning controller. addition four state variables,
available feedback failure signal notifies controller failure occurs,
means values state variables exceed boundary constraints prescribed inequalities
(47) (48). typical reinforcement learning problem, failure signal serves
reward. Since external reward may available long sequence actions, critic
AHC learning controller used provide internal reinforcement signal accomplish
learning task. Learning control experiments pole-balancing problem conducted
using conventional AHC method uses linear TD() algorithms critic
Fast-AHC method proposed paper.
solve continuous state space problem reinforcement learning, class linear
function approximators, called Cerebellar Model Articulation Controller (CMAC)
used. neural network model based neuro-physiological theory human
cerebellarCMAC first proposed Albus (1975) widely used automatic
control function approximation. CMAC neural networks, dependence adjustable
parameters weights respect outputs linear. detailed discussion structure
CMAC neural networks, one may refer Albus (1975) Sutton & Barto (1998).
AHC Fast-AHC learning controllers, two CMAC neural networks four inputs
one output used function approximators critic actor,
respectively. CMAC C tilings partitions every input. total physical
memory CMAC network M4C. reduce computation memory requirements,
hashing technique described following equations employed experiments. (For
detailed discussion parameters CMAC networks, please refer Appendix C).
A( ) =

4

[a(i) + 1 ]

(50)

=1

F(s)=A(s) mod K
(51)
(50) (51), represents input state vector, a(i) (0 a(i) M) activated tile
i-th element s, K total number physical memory F(s) physical
memory address corresponding state s, remainder A(s) divided K.
order compare performance different learning algorithms, initial parameters
learning controller selected follows: weights critic initialized 0
weights actor initialized random numbers interval [0,0.1].
parameters AHC Fast-AHC algorithms = 0.95 , k1 = 0.4 k 2 = 0.5 .
experiments, trial defined period initial state failure state
initial state trial set randomly generated state near unstable equilibrium
(0,0,0,0) maximum distance 0.05. Equation (49) employed simulate dynamics
system using Euler method, time step 0.02s. trial lasts
120,000 time steps, said successful learning controller assumed
able balance pole. reinforcement signal problem defined

1, failure occurs
rt =
0, otherwise

(52)

performance Fast-AHC method tested extensively, different parameter
settings including initial variance matrix P0 chosen. experiments,
278

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

forgetting factor RLS-TD() critic set value equal 1 close 1.
learning control experiments using conventional AHC methods also conducted
comparison. performance comparisons two algorithms shown Figure 9, 10
11.
experiments, initial variance matrixes Fast-AHC algorithm set
P0=0.1I. performance Fast-AHC compared AHC different . numbers
physical memories critic network actor network chosen 30 80,
respectively. parameter setting two algorithms, 5 independent runs tested.
performance evaluated according trial number needed successfully balance pole.
learning factors actor networks set 0.5, manually optimized value
algorithms. experiments, 11 settings tested.

Figure 9: Performance comparison Fast-AHC AHC =0.01

Figure 10: Performance comparison Fast-AHC AHC =0.03
279

fiXU, HE, & HU

Figure 11: Performance comparison Fast-AHC AHC =0.05

Figure 9, 10 11, learning factors critic networks AHC chosen
=0.01, 0.03 0.05, respectively. found <0.01, performance AHC
becomes worse. learning factors greater 0.05, AHC algorithm may
become unstable, even =0.03 =0.05, AHC algorithm becomes unstable
=1. time-varying learning factors specified (s1)-(s3), performance worse
constant learning factors. three settings learning factor typical
near optimal AHC algorithm.
experimental results, concluded using RLS-TD()
critic network, Fast-AHC algorithm obtain better performance conventional AHC
algorithms. Although Fast-AHC requires computation per step AHC,
efficient AHC less trials data needed successfully balance pole.
discussed previous sections, convergence performance RLS-TD()
influenced initial value variance matrix. also case Fast-AHC.
learning control experiments, small value =0.1 selected. experiments,
set small values, performance Fast-AHC satisfactory better AHC.
However, equal relatively large value, example =100 500, performance
Fast-AHC deteriorates significantly. Since RLS-TD() large initializing constant
similar performance LS-TD(), deduced AHC method using LS-TD()
critic also bad performance cart-pole balancing problem. verify this,
experiments conducted using Fast-AHC large initializing constant AHC using
LS-TD(). parameter setting, 5 independent runs tested. experiments,
maximum trials algorithm one run 200 algorithm fails balance
pole within 200 trials, performance set 200.When using LS-TD() AHC method,
may computational problems matrix inversion first steps learning
two methods tried avoid problem. One usage TD() first 60 steps
updates. actor updated early stage learning LS-TD()
280

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

stable. However, similar results found two methods. Figure 12 shows experimental
results clearly verify performance Fast-AHC large initializing constant
similar AHC using LS-TD() much worse Fast-AHC small . detailed
discussion phenomenon provided subsection 5.4.

Figure 12: Performance comparison Fast-AHC different initial variance
following Figure 13 Figure 14, variations pole angle control
force F plotted, successfully trained Fast-AHC learning controller used control
cart-pole system.

Figure 13: Variation pole angle

Figure 14: Variation control force

5.3 Learning Control Experiments Acrobot

subsection, another learning control example, swing-up control acrobot
minimum time, presented. learning control acrobot class adaptive optimal
control problem difficult pole-balancing problem. investigated
Sutton (1996), CMAC-based Sarsa-learning algorithms employed solve
case discrete control actions studied. experiments, case continuous actions
281

fiXU, HE, & HU

considered.
acrobot moving vertical plane shown Figure 15, OA AB first
link second link, respectively. control torque applied point A. goal
swing-up control swing tip B acrobot line CD higher
joint amount length one link.

Figure 15: acrobot
dynamics acrobot system described following equations.

&&1 = (d 2&&2 + 1 ) / d1

(53)

&&2 = ( + 21 / d1 2 )

(54)

d1 = m1l c21 + m2 (l12 + l c22 + 2l1l c 2 cos 2 ) + 1 + 2

(55)

2 = m2 (l c22 + l1l c 2 cos 2 ) + 2

(56)

1 = m2 l1l c 2&22 sin 2 2m2 l1l c 2&1&2 sin 2 + (m1l c1 + m2 l1 ) g cos( 1 / 2) + 2

(57)

2 = m2 l c 2 g cos( 1 + 2 / 2)

(58)



equations, parameters , &i , mi , li , , l ci angle, angle velocity,
mass, length, moment inertia length center mass link (i=1,2),
respectively.
Let sT denote goal state swing-up control. Since control aim swing
acrobot minimum time, reward function rt defined
1, = sT
rt =
0, else

(59)

simulation experiments, control torque continuous bounded [-3N, 3N].
Similar cart-pole balancing problem, CMAC neural networks applied solve
282

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

learning control problem continuous states actions. CMAC-based actor-critic
controller, actor network critic network C=4 tilings M=7 partitions
input. actor network, uniform coding employed non-uniform coding used
critic network. details coding parameters, please refer Appendix C. sizes
physical memories actor network critic network 100 80, respectively.
CMAC networks, following hashing techniques used. (For definition A(s),a(i)
F(s), please refer Subsection 5.2.)
4

A( ) = [ a(i ) 1 ]

(60)

=1

F(s)=A(s) mod K
(61)
simulation, parameters acrobot chosen m1=m2=1kg, I1=I2=1kgm2,
lc1=lc2=0.5m, l1=l2=1m g=9.8m/s2. time step simulation 0.05s time interval
learning control 0.2s. learning parameters =0.6, =0.90, =0.2, k1=0.4, k2=0.5.
trial defined period starts stable equilibrium ends goal state
reached. trial, state acrobot re-initialized stable equilibrium.
parameter setting, 5 independent runs tested. run consists 50 trials 50-th trial,
actor network tested controlling acrobot alone, i.e., setting action variance
defined (43) zero. performance algorithms evaluated according steps
used actor networks swing acrobot.
performance comparisons Fast-AHC AHC shown Figure 16,17
18. experiments, algorithms tested different AHC also tested
different learning factors critic networks.
results, also shown Fast-AHC achieve higher data efficiency AHC.
However, example, relatively large used, different previous
cart-pole balancing example. experiments, good performance obtained large
initializing constant small, performance deteriorates significantly. Thus
problem may referred low SNR case Moustakides (1997), large values
preferable best convergence rate RLS methods.

Figure 16: Performance comparison Fast-AHC AHC =0.02
283

fiXU, HE, & HU

Figure 17: Performance comparison Fast-AHC AHC =0.05

Figure 18: Performance comparison Fast-AHC AHC =0.1

following Figure 19 shows performance comparison Fast-AHC large
(300) small (0.01) value , 6 settings parameter tested
algorithm. performance AHC using LS-TD() also shown. Figure 20, typical curve
angle first link plotted, acrobot controlled actor network
Fast-AHC method (=0.6) 50 trials.

284

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Figure 19: Performance comparison Fast-AHC AHC using LS-TD()

Figure 20: Variation angle link 1(Controlled Fast-AHC 50 trials)

5.4

Analysis Experimental Results

Based experimental results, concluded using RLS-TD()
algorithm critic network, Fast-AHC algorithm obtain better performance
conventional AHC algorithms less trials data needed converge near optimal
policy. well known, one difficulty applications RL methods slow
convergence, especially cases learning data hard generated.
Fast-AHC algorithm, although computation per step required conventional AHC
methods, serious problem number linear state features small.
learning control experiments, hashing techniques used reduce state features
CMAC networks computation Fast-AHC reduced economical amount.
Nevertheless, state feature number large, conventional AHC methods may
preferable.
experiments, observed performance Fast-AHC affected
initializing constant . results consistent property RLS-TD() RLS
285

fiXU, HE, & HU

method adaptive filtering, discussed Section 4. learning control
experiments cart-pole balancing problem, better performance Fast-AHC obtained
using small values . learning control acrobot, higher data efficiency
achieved using Fast-AHC relatively large . two different properties Fast-AHC
may referred different SNR cases RLS methods (Moustakides,1997). thorough
theoretical analysis problem interesting topic future research.
experiments, performance AHC method using LS-TD() also tested.
studied Section 4, initializing constant large, performance
RLS-TD() LS-TD() differ much. performance AHC using LS-TD()
similar Fast-AHC large values .
studied Moustakides (1997), RLS method converge much faster
adaptive filtering methods environment stationary initializing constant selected
appropriately. cases, RLS may converge almost instantly. also verified
learning prediction experiments RLS-TD() algorithm. applying RLS-TD()
actor-critic learning controller, although policy actor change time, still
assumed changing speed policy slow compared fast
convergence speed RLS-TD(). Thus good performance learning prediction obtained
critic. Moreover, since learning prediction performance critic important
policy learning actor, improvement learning prediction efficiency contribute
whole performance improvement controller.
6. Conclusions Future Work

Two new reinforcement learning algorithms using RLS methods, called RLS-TD( )
Fast-AHC, respectively, proposed paper. RLS-TD( ) used solve learning
prediction problems efficiently conventional linear TD( ) algorithms.
convergence probability 1 proved RLS-TD( ) limit convergence also
analyzed. Experimental results learning prediction problems show RLS-TD( )
algorithm superior conventional TD( ) algorithms data efficiency also eliminates
design problem step sizes linear TD( ) algorithms. RLS-TD( ) viewed
extension RLS-TD(0) =0 general 0< 1. Although effect
convergence speed RLS-TD( ) may significant cases, usage >0
still affect approximation error bound. Thus, needs value function
estimation high precision, large values preferable =0. Furthermore, RLSTD( ) superior LS-TD( ) computation weight vector must updated
every observations.
Since learning prediction viewed sub-problem learning control, extend
results learning prediction learning control method called AHC algorithm. Using
RLS-TD( ) critic network, Fast-AHC achieve better performance conventional
AHC method data efficiency. Simulation results learning control pole-balancing
problem acrobot system confirm analyses.
experiments, found performance RLS-TD( ) well Fast-AHC
influenced initializing constant RLS methods. Different values needed best
performance different cases. also well-known phenomenon RLS-based adaptive
286

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

filtering theoretical results Moustakides (1997) provide basis explanations
results. complete investigation problem ongoing work.
idea using RLS-TD( ) critic network may applied reinforcement
learning methods actor-critic architectures. Konda Tsitsiklis (1998), new actor-critic
algorithm using linear function approximators proposed convergence certain
conditions proved. One condition convergence algorithm convergence
rate critic much faster actor. Thus application RLS-TD( )
critic may preferable order ensure convergence algorithm. theoretical
empirical work problem deserves studied future.

Acknowledgements
work supported National Natural Science Foundation China Grants
60075020, 60171003 China University Key Teachers Fellowship. would much like
thank anonymous reviewers Associate Editor Michael L. Littman insights
constructive criticisms, helped improve paper significantly.

287

fiXU, HE, & HU

Appendix A. Derivation RLS-TD() Algorithm

derivation RLS-TD(), two different cases, determined value
forgetting factor.
(1) RLS-TD() unit forgetting factor.
Since

Pt = At1

(62)

P0 =

(63)

r
K +1 = Pt +1 z

(64)

According Lemma 1,

Pt +1 = At+11

r
r
= Pt Pt z [1 + ( ( xt ) ( x +1 )) Pt z )] 1 ( ( x ) ( xt +1 )) Pt

r
K +1 = Pt +1 z
r
r
= Pt z /(1 + ( ( xt ) ( xt +1 )) Pt z )

(65)

(66)

Wt +1 = At+11bt +1

r
= Pt +1 ( z ri )

(67)

=0

r
= Pt +1 ( Pt 1Wt + z rt )
Thus

r
r
Wt +1 = Pt +1 [( Pt +11 z ( ( xt ) ( x +1 )))Wt + z rt ]
r
r
= Wt + Pt +1 ( z rt z ( ( xt ) ( x +1 ))Wt )

(68)

= Wt + K +1 [rt ( ( xt ) ( xt +1 ))Wt ]
(2) RLS-TD() forgetting factor <1
derivation RLS-TD() forgetting factor <1 similar exponentially weighted
RLS algorithm Haykins (1996, pp.566-569). present results:

Pt +1 =

1



r
r
K +1 = Pt z /( + ( ( x ) ( x +1 )) Pt z )

(69)

Wt +1 = Wt + K +1 (rt ( ( x ) ( xt +1 ))Wt )

(70)

r
r
[ Pt Pt z [ + ( ( xt ) ( xt +1 )) Pt z )]1 ( ( xt ) ( xt +1 )) Pt ]

288

(71)

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Appendix B. Proof Theorem 1
study steady property Markov chain defined Section 3, construct stationary
process follows. Let {xt} Markov chain evolves according transition matrix P
already steady state, means Pr{xt=i}= (i) t. Given sample path
Markov chain, define

r
zt =



( ) ( x )

(72)

=

r

X = {xt , xt +1 , z } stationary process, discussed (Tsitsiklis
Roy, 1997).
Let denote NN diagonal matrix diagonal entries (1), (2),, (N), N
cardinality state space X. Lemma 2 derived follows.
Lemma 2. (Tsitsiklis Roy, 1997) Assumption 1-4, following equations hold.
1) E 0 [ ( xt ) ( xt + )] = DP , m>0

r
2) E 0 [ z ( xt )] =

(73)



( ) DP ,

(74)

=0

r
3) E 0 [ z rt ( xt , xt +1 )] =



( ) DP r

(75)

=0

r R N , whose Nth component equal E[r ( xt , xt +1 ) xt = ] .
According Lemma 2, E0[A(Xt)] E0[b(Xt)] well defined finite. Furthermore, E0[A(Xt)]
negative definite, invertible.
equation (67),




WRLS TD ( ) = [ P01 + A( X )] 1 [ P01W0 + b( X )]
=1

=1

1
1
1
1
= [ P01 + A( X )] 1 [ P01W0 + b( X )]

=1

=1


(76)

Since

1
A( X )

=1

(77)

1
b( X )

=1

(78)

E 0 [ A( X )] = lim

E 0 [b( X )] = lim
E0[A(Xt)] invertible,
1

lim W RLS TD ( ) = E 0 [ A( X )]E 0 [b( X )] = W *



289

(79)

fiXU, HE, & HU

Thus W RLS TD ( ) converges W* probability 1.

Appendix C. details coding structures CMAC networks
following discussion, coding structures CMAC networks cart-pole balancing
problem acrobot control problem presented.
(1) CMAC coding structures cart-pole balancing problem
CMAC networks, state variables following boundaries.

[12 ,12 ] ,

& [50 deg/ s, 50 deg/ s]

x [2.4, 2.4] ,
x& [1,1]
critic network, C=4 M=7. hashing technique specified equations (50) (51)
employed total memory size 30.
actor network, C=4 M=7. hashing technique specified equations (60) (61)
employed total memory size 100.
(2) CMAC coding structures acrobot swing-up problem
simulation, angles bounded [ , ] angular velocities bounded

&1 [4 ,4 ] , &2 [9 ,9 ] . tiling numbers actor critic equal 4
(C=4). total memory sizes critic actor 80 100, respectively. actor
network, tiling partitions range input 7 equal intervals (M=7). critic
network, partitions input non-uniform, given

1 : { -, -1, -0.5, 0, 0.5, 1, },

&1 : {-4, -1.5, -0.5, 0, 0.5, 1.5, 4}

2 : {-, -1, -0.5, 0, 0.5, 1, },

&2 : {-9, -2, -0.5,0, 0.5,2, 9}

290

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

References
Albus,J.S.(1975). new approach manipulator control: cerebellar model articulation
controller (CMAC). Journal Dynamic Systems, Measurement, Control, 97(3), 220-227.
Barto,A.G., Sutton R.S., & Anderson C.W. (1983). Neuronlike adaptive elements solve
difficult learning control problems. IEEE Transactions System, Man, Cybernetics,13,
834-846.
Bertsekas D.P. & Tsitsiklis J.N. (1996). Neurodynamic Programming. Belmont, Mass.: Athena
Scientific.
Berenji H.R. & Khedkar P. (1992). Learning tuning fuzzy logic controllers reinforcements, IEEE Trans.On Neural Networks, 3(5), 724-740.
Boyan. J.(1999). Least-squares temporal difference learning. Bratko, I., Dzeroski, S., eds.,
Machine Learning: Proceedings Sixteenth International Conference (ICML).
Boyan, J.(2002). Technical update: least-squares temporal difference learning. Machine Learning,
Special Issue Reinforcement Learning, appear.
Brartke. S.J. & Barto A. (1996). Linear least-squares algorithms temporal difference learning.
Machine Learning, 22, 33-57.
Dayan P.(1992). convergence TD() general . Machine Learning, 8, 341-362.
Dayan P.. & Sejnowski T.J. (1994). TD() converges probability 1. Machine Learning, 14,
295-301.
Eleftheriou E. & Falconer,D.D. (1986). Tracking properties steady state performance RLS
adaptive filter algorithms. IEEE Transactions Acoustics, Speech, Signal Processing, 34,
1097-1110.
Eweda E. & Macchi, O. (1987). Convergence RLS LMS adaptive filters. IEEE Trans.
Circuits Systems, 34, 799-803.
Haykin S. (1996), Adaptive Filter Theory, 3rd edition, Englewood Cliffs, NJ: Prentice-Hall.
Hubing N.E. & Alexander S.T. (1989). Statistical analysis soft constrained initialization
RLS algorithms. Proc. IEEE International Conference Acoustics, Speech
Signal Processing.
Jaakkola T., Jordan M.I., & Singh S.P. (1994). convergence stochastic iterative dynamic
programming algorithms. Neural Computation. 6(6), 1185-1201.
Kaelbling L.P., Littman M.L., & Moore A.W. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research, 4, 237-285.
Konda V.R, & Tsitsiklis J.N. (2000). Actor-critic algorithms. Neural Information Processing
Systems, 2000, MIT Press.
291

fiXU, HE, & HU

Lin L.J. (1992). Self-improving reactive agents based reinforcement learning, planning
teaching. Machine Learning, 8(3/4), 293-321.
Lin C.T. & Lee C.S.G. (1994). Reinforcement structure/parameter learning neural-networkbased fuzzy Logic control system. IEEE Transactions Fuzzy System, 2(1), 46-63.
Ljung L. & Soderstron T. (1983). Theory Practice Recursive Identification. MIT Press.
Ljung L. (1977). Analysis recursive stochastic algorithm. IEEE. Transactions Automatic
Control, 22, 551.
Michie D. & Chambers R.A. (1968). BOXES: experiment adaptive control. Machine
Intelligence 2, Dale E. Michie D., eds., Edinburgh: Oliver Boyd, 137-152.
Minsky M.L. (1954). Theory neural-analog reinforcement systems application
brain-model problem. Ph.D. Thesis, Princeton University.
Moustakides G.V. (1997). Study transient phase forgetting factor RLS. IEEE Trans.
Signal Processing, 45(10), 2468-2476.
Samuel A.L. (1959). studies machine learning using game checkers. IBM Journal
Research Development, 3, 211-229.
Singh, S.P., Jaakkola T., Littman M.L., & Szepesvari C. (2000). Convergence results singlestep on-policy reinforcement-learning algorithms. Machine Learning, 38, 287-308.
Sutton R. & Barto A. (1998). Reinforcement Learning, Introduction. Cambridge MA, MIT
Press.
Sutton R. (1988). Learning predict method temporal differences. Machine Learning,
3(1), 9-44.
Tsitsiklis J.N. (1994). Asynchronous stochastic approximation Q-learning. Machine Learning,
16, 185-202.
Tsitsiklis J.N. & Roy B.V. (1994). Feature-based methods large scale dynamic programming.
Neural Computation. 6(6), 1185-1201.
Tsitsiklis J.N. & Roy B.V. (1997). analysis temporal difference learning function
approximation. IEEE Transactions Automatic Control. 42(5), 674-690.
Watkins C.J.C.H. & Dayan P. (1992). Q-Learning. Machine Learning. 8, 279-292.
Young P. (1984). Recursive Estimation Time-Series Analysis. Springer-Verlag.

292

fiJournal Artificial Intelligence Research 16 (2002) 1-58

Submitted 7/01; published 1/02

Fusions Description Logics
Abstract Description Systems
Franz Baader
Carsten Lutz

baader@cs.rwth-aachen.de
lutz@cs.rwth-aachen.de

Teaching Research Area Theoretical Computer Science,
RWTH Aachen, Ahornstrae 55, 52074 Aachen, Germany

Holger Sturm

holger.sturm@uni-konstanz.de

Fachbereich Philosophie, Universitat Konstanz,
78457 Konstanz, Germany

Frank Wolter

wolter@informatik.uni-leipzig.de

Institut fur Informatik, Universitat Leipzig,
Augustus-Platz 10-11, 04109 Leipzig, Germany

Abstract
Fusions simple way combining logics. normal modal logics, fusions
investigated detail. particular, known that, certain conditions, decidability transfers component logics fusion. Though description logics
closely related modal logics, necessarily normal. addition, ABox
reasoning description logics covered results modal logics.
paper, extend decidability transfer results normal modal logics
large class description logics. cover different description logics uniform way,
introduce abstract description systems, seen common generalization
description modal logics, show transfer results general setting.

1. Introduction
Knowledge representation systems based description logics (DL) used represent knowledge application domain structured formally well-understood
way (Brachman & Schmolze, 1985; Baader & Hollunder, 1991; Brachman, McGuinness,
Patel-Schneider, Alperin Resnick, & Borgida, 1991; Woods & Schmolze, 1992; Borgida,
1995; Horrocks, 1998). systems, important notions domain described concept descriptions, i.e., expressions built atomic concepts (unary
predicates) atomic roles (binary predicates) using concept constructors provided
description logic employed system. atomic concepts concept
descriptions represent sets individuals, whereas roles represent binary relations
individuals. example, using atomic concepts Woman Human, atomic
role child, concept women daughters (i.e., women
children women) represented description Woman u child.Woman,
concept mothers description Woman u child.Human. example,
used constructors concept conjunction (u), value restriction (R.C), existential restriction (R.C). DL literature, also various constructors
considered. prominent example so-called number restrictions, available
almost DL systems. example, using number restrictions concept women
c
2002
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBaader, Lutz, Sturm, & Wolter

exactly two children represented concept description
Woman u ( 2child) u ( 2child).
knowledge base DL system consists terminological component (TBox)
assertional component (ABox). simplest form, TBox consists concept
definitions, assign names (abbreviations) complex descriptions. general TBox
formalisms allow so-called general concept inclusion axioms (GCIs) complex
descriptions. example, concept inclusion
Human u ( 3child) v entitled.Taxbreak
states people least three children entitled tax break. ABox
formalism consists concept assertions (stating individual belongs concept)
role assertions (stating two individuals related role). example,
assertions Woman(MARY), child(MARY, TOM), Human(TOM) state Mary woman,
child, Tom, human.
DL systems provide users various inference capabilities allow
deduce implicit knowledge explicitly represented knowledge. instance, subsumption problem concerned subconcept-superconcept relationships: C subsumed
(C v D) if, if, instances C also instances D, i.e., first description always interpreted subset second description. example, concept
description Woman obviously subsumes concept description Woman u child.Woman.
concept description C satisfiable iff non-contradictory, i.e., interpreted
nonempty set. DLs allowing conjunction negation concepts, subsumption reduced (un)satisfiability: C v iff C u unsatisfiable. instance
checking problem consists deciding whether given individual instance given
concept. example, w.r.t. assertions above, MARY instance concept
description Woman u child.Human. ABox consistent iff non-contradictory,
i.e., model. DLs allowing negation concepts, instance problem
reduced (in)consistency ABoxes: instance C w.r.t. ABox iff A{C(i)}
inconsistent.
order ensure reasonable predictable behavior DL system, reasoning
DL employed system least decidable, preferably low
complexity. Consequently, expressive power DL question must restricted
appropriate way. imposed restrictions severe, however, important
notions application domain longer expressed. Investigating trade-off
expressivity DLs complexity inference problems thus
one important issues DL research (see, e.g., Levesque & Brachman,
1987; Nebel, 1988; Schmidt-Schau, 1989; Schmidt-Schau & Smolka, 1991; Nebel, 1990;
Donini, Lenzerini, Nardi, & Nutt, 1991, 1997; Donini, Hollunder, Lenzerini, Spaccamela,
Nardi, & Nutt, 1992; Schaerf, 1993; Donini, Lenzerini, Nardi, & Schaerf, 1994; De Giacomo
& Lenzerini, 1994a, 1994b, 1995; Calvanese, De Giacomo, & Lenzerini, 1999; Lutz, 1999;
Horrocks, Sattler, & Tobies, 2000).
paper investigates approach extending expressivity DLs (in many
cases) guarantees reasoning remains decidable: fusion DLs. order explain
2

fiFusions Description Logics Abstract Description Systems

difference usual union fusion DLs, let us consider simple
example. Assume DL D1 ALC, i.e., provides Boolean operators u, t,
additional concept constructors value restriction R.C existential restriction
R.C, DL D2 provides Boolean operators number restrictions
( nR) ( nR). application requires concept constructors DLs
expressing relevant concepts, one would usually consider union D1 D2 D1
D2 , allows unrestricted use constructors. example, concept
description C1 := (R.A) u (R.A) u ( 1R) legal D1 D2 description. Note
description unsatisfiable, due interaction constructors D1 D2 .
fusion D1 D2 D1 D2 prevents interactions imposing following restriction:
one assumes set role names partitioned two sets, one used
constructors D1 , another one used constructors D2 . Thus,
description C1 legal D1 D2 description since uses role R
existential restrictions (which D1 -constructors) number restriction
(which D2 -constructor). contrast, descriptions (R1 .A) u (R1 .A) u ( 1R2 )
(R1 .( 1R2 )) admissible D1 D2 since employ different roles D1 D2 -constructors. concepts must expressed require
constructors D1 D2 , ones D1 roles ones
D2 , one really need union D1 D2 ; fusion would sufficient.
advantage taking fusion instead union? Basically, union
two DLs one must design new reasoning methods, whereas reasoning fusion
reduced reasoning component DLs. Indeed, reasoning union may even
undecidable whereas reasoning fusion still decidable. example, consider
DLs (i) ALCF, extends basic DL ALC functional roles (features)
same-as constructor (agreement) chains functional roles (Hollunder & Nutt, 1990;
Baader, Burckert, Nebel, Nutt, & Smolka, 1993); (ii) ALC +,,t , extends ALC
transitive closure, composition, union roles (Baader, 1991; Schild, 1991).
DLs, subsumption concept descriptions known decidable (Hollunder & Nutt,
1990; Schild, 1991; Baader, 1991). However, union ALCF +,,t undecidable
subsumption problem (Baader et al., 1993). undecidability result depends fact
that, ALCF +,,t , role constructors transitive closure, composition, union
applied functional roles also appear within same-as constructor.
allowed fusion ALCF ALC +,,t . course, failure certain undecidability proof
make fusion decidable.
know fusion decidable DLs decidable? Actually,
general dont, main reason writing paper. notion fusion introduced investigated modal logic, basically transfer results like finite
axiomatizability, decidability, finite model property, etc. uni-modal logics (with one
pair box diamond operators) multi-modal logics (with several pairs, possibly satisfying different axioms). led rather general transfer results (see, e.g.,
Wolter, 1998; Kracht & Wolter, 1991; Fine & Schurz, 1996; Spaan, 1993; Gabbay, 1999
results concern decidability), sometimes restricted so-called normal
modal logics (Chellas, 1980). Since close relationship modal logics
DLs (Schild, 1991), clear transfer results also apply DLs. question is, however, DLs exactly inference problems. First, DLs
3

fiBaader, Lutz, Sturm, & Wolter

allow constructors considered modal logics (e.g., same-as constructor
mentioned above). Second, DL constructors considered modal logics, qualified number restrictions ( nR.C), ( nR.C) (Hollunder & Baader, 1991),
correspond graded modalities (Van der Hoek & de Rijke, 1995), easily
shown non-normal. Third, transfer results decidability concerned
satisfiability problem (with without general inclusion axioms). ABoxes related
inference problems considered. ABoxes simulated modal logics allowing
so-called nominals, i.e., names individuals, within formulae (Prior, 1967; Gargov
& Goranko, 1993; Areces, Blackburn, & Marx, 2000). However, see below,
general transfer results apply modal logics nominals.
purpose paper clarify DLs decidability component DLs
transfers fusion. purpose, introduce so-called abstract description systems
(ADSs), seen common generalization description modal logics.
define fusion ADSs, state four theorems say conditions
decidability transfers component ADSs fusion. Two theorems
concerned inference w.r.t. general concept inclusion axioms two inference
without TBox axioms. cases, first formulate prove results
consistency problem ABoxes (more precisely, corresponding problem ADSs)
establish analogous results satisfiability problem concepts.
DL point view, four theorems shown paper concerned
following four decision problems:
(i) decidability consistency ABoxes w.r.t. TBox axioms (Theorem 17);
(ii) decidability satisfiability concepts w.r.t. TBox axioms; (Corollary 22);
(iii) decidability consistency ABoxes without TBox axioms (Theorem 29);
(iv) decidability satisfiability concepts without TBox axioms (Corollary 34).
theorems imply decidability consistency problem satisfiability
problem transfers fusion DLs considered literature. main exceptions (which satisfy prerequisites theorems)
(a) DLs propositionally closed, i.e., contain Boolean connectives;
(b) DLs allowing individuals (called nominals modal logic) concept descriptions;

(c) DLs explicitly allowing universal role negation roles.
Results modal logic problem (iv) usually require component modal logics
normal. Theorem 29 less restrictive, thus also applies DLs allowing
constructors like qualified number restrictions.

2. Description logics
defining abstract description systems next section, introduce main
features DLs must covered definition. purpose, first introduce
4

fiFusions Description Logics Abstract Description Systems

ALC, basic DL containing Boolean connectives, relevant inference problems.
Then, consider different possibilities extending ALC expressive DLs.
Definition 1 (ALC Syntax). Let NC , NR , NI countable pairwise disjoint sets
concept, role, individual names, respectively. set ALC concept descriptions
smallest set
1. every concept name concept description,
2. C concept descriptions R role name, following expressions also concept descriptions:
C (negation), C u (conjunction), C (disjunction),
R.C (existential restriction), R.C (value restriction).
use > abbreviation abbreviation u (where
arbitrary concept name).
Let C concept descriptions. C v general concept inclusion axiom
(GCI). finite set axioms called TBox.
Let C concept description, R role name, i, j individual names. C(i)
concept assertion R(i, j) role assertion. finite set assertions called
ABox.
meaning ALC-concept descriptions, TBoxes, ABoxes defined
help set-theoretic semantics.
Definition 2 (ALC Semantics). ALC-interpretation pair (I , ),
nonempty set, domain interpretation, interpretation function.
interpretation function maps
concept name subset AI ,
role name R subset RI ,
individual name element iI different names mapped
different elements (unique name assumption).
role name R element define RI (a) := {b | (a, b) RI }.
interpretation function inductively extended complex concepts follows:
(C)I := \ C
(C u D)I := C DI
(C D)I := C DI
(R.C)I := {a | RI (a) C 6= }
(R.C)I := {a | RI (a) C }
interpretation model TBox iff satisfies C DI GCIs C v
. model ABox iff satisfies iI C concept assertions C(i)
(iI , j ) RI role assertions R(i, j) A. Finally, model ABox
relative TBox iff model ABox TBox.
5

fiBaader, Lutz, Sturm, & Wolter

Given semantics, formally define relevant inference problems.
Definition 3 (Inferences). Let C concept descriptions, individual name,
TBox, ABox. say C subsumes relative TBox (D vT C)
iff DI C models . concept description C satisfiable relative
TBox iff exists model C 6= . individual instance
C ABox relative TBox iff iI C models relative .
ABox consistent relative TBox iff exists model relative .
three inferences also considered without reference TBox: C subsumes
(C satisfiable) iff C subsumes (C satisfiable) relative empty TBox,
instance C (A consistent) iff instance C (A consistent)
relative empty TBox.
restrict attention DLs propositionally closed (i.e., allow
Boolean operators conjunction, disjunction, negation). Consequently, subsumption
reduced (un)satisfiability since C vT iff C u unsatisfiable relative .
Conversely, (un)satisfiability reduced subsumption since C unsatisfiable relative
iff C vT . reason, irrelevant whether consider subsumption
satisfiability problem results concerning transfer decidability problems
component DLs fusion (informally called transfer results following).
Similarly, instance problem reduced (in)consistency problem vice
versa: instance C relative iff {C(i)} inconsistent relative ;
inconsistent relative iff instance relative ,
arbitrary individual name. Consequently, irrelevant whether consider instance
problem consistency problem transfer results.
Finally, satisfiability problem reduced consistency problem: C satisfiable relative iff ABox {C(i)} consistent relative , arbitrary
individual name. However, converse need true. obvious
implies transfer result satisfiability problem yield corresponding
transfer result consistency problem: decidability consistency problem
component DLs deduce decidability satisfiability problem
fusion. might less obvious transfer result consistency problem need
imply corresponding transfer result satisfiability problem: satisfiability
problems component DLs decidable, transfer result consistency
problem applied (since prerequisite transfer result, namely, decidability consistency problem component DLs, need satisfied). However,
show method used show transfer result consistency problem
also applies satisfiability problem.
2.1 expressive DLs
several possibilities extending ALC order obtain expressive DL.
three prominent adding additional concept constructors, adding role constructors, formulating restrictions role interpretations. addition giving examples
extensions, also introduce naming scheme obtained DLs. Additional
concept constructors indicated appending caligraphic letters language name,
role constructors symbols superscript, restrictions roles letters subscript.
6

fiFusions Description Logics Abstract Description Systems

start introducing restrictions role interpretations, since need refer
restrictions defining certain concept constructors.
2.1.1 Restrictions role interpretations
restrictions enforce interpretations roles satisfy certain properties,
functionality, transitivity, etc. consider three prominent examples:
1. Functional roles. one considers subset NF set role names NR ,
whose elements called features. interpretation must map features f NF
functional binary relations f , i.e., relations satisfying a, b, c.f (a, b)
f (a, c) b = c. sometimes treat functional relations partial functions,
write f (a) = b rather f (a, b). ALC extended features denoted
ALC f .
2. Transitive roles. one considers subset NR+ NR . Role names R NR+
called transitive roles. interpretation must map transitive roles R NR+
transitive binary relations RI . ALC extended transitive roles
denoted ALC R+ .
3. Role hierarchies. role inclusion axiom expression form R v
R, NR . finite set H role inclusion axioms called role hierarchy.
interpretation must satisfy RI R v H. ALC extended role
hierarchy H denoted ALC H(H) . H clear context irrelevant,
write ALCH instead ALC H(H) .
restrictions also combined other. example, ALC HR+ ALC
role hierarchy transitive roles.
Transitive roles DLs first investigated Sattler (1996). Features introduced DLs Hollunder Nutt (1990) (under name attributes)
CLASSIC system (Brachman et al., 1991), cases conjunction feature agreements disagreements (see concept constructors below). Features without agreements
disagreements are, e.g., used DL SHIF (Horrocks & Sattler, 1999), albeit
expressive local way, functionality asserted hold certain individuals, necessarily whole model. According naming scheme, indicate
presence features DL letter f subscript.1
remark role hierarchies also order: definition, H1 H2 different
role hierarchies, ALC H(H1 ) ALC H(H2 ) different DLs. DL literature,
usually one logic ALCH defined role hierarchies treated like TBoxes, i.e.,
satisfiability subsumption defined relative TBoxes role hierarchies (see, e.g.,
Horrocks, 1998). purposes, however, convenient define one DL per role
hierarchy since distinct role hierarchies impose distinct restrictions interpretation
roles. advantages approach become clear later frames abstract
description systems introduced.
1. Note authors (e.g., Horrocks & Sattler, 1999) use appended F denote local features.
Following Hollunder Nutt (1990), use F denote DL allows feature agreements
(see below).

7

fiBaader, Lutz, Sturm, & Wolter

Name
Unqualified
number restrictions
Qualified
number restrictions
Nominals
Feature agreement
disagreement

Syntax
nR
nR
nR.C
nR.C

u1 u2
u1 u2

Semantics
{a | |RI (a)| n}
{a | |RI (a)| n}
{a | |RI (a) C | n}
{a | |RI (a) C | n}
|I | = 1
{a | b . uI1 (a) = b = uI2 (a)}
{a | b1 , b2 .
uI1 (a) = b1 6= b2 = uI2 (b1 )}

Symbol
N
Q

F

Figure 1: description logic concept constructors.
2.1.2 Concept constructors
Concept constructors take concept and/or role descriptions transform
complex concept descriptions. addition constructors available ALC, various
concept constructors considered DL literature. small collection
constructors found Figure 1, |S| denotes cardinality set S.
symbols rightmost column indicate naming scheme resulting DL.
mentioned name modifiers concept constructors written subscript,
appended language name. example, ALC HR+ extended qualified
number restrictions called ALCQHR+ . syntax extended DLs expected, i.e.,
constructors may arbitrarily combined. semantics obtained augmenting
semantics ALC appropriate conditions, found third
column Figure 1. Nominals feature (dis)agreements need explanation:
Nominals. consider set (names for) nominals, pairwise disjoint
sets NC , NR , NI . Elements often denoted (possibly
index). interpretation must map nominals singleton subsets .
intention underlying nominals stand elements , like individual
names. However, since want use nominal (nullary) concept
constructor, must interpret set, namely singleton set consisting
individual denotes.
Feature (dis)agreements. ALCF extension ALC f feature agreements
disagreements. Beside additional concept constructors, ALCF uses feature
chains part (dis)agreement constructor. feature chain expression
form u = f1 fn . interpretation uI feature chain
composition partial functions f1I , . . . , fnI , composition read
left right.
DLs including nominals feature (dis)agreements additional concept constructors
restrictions role interpretations defined (and named) obvious way.
Number restriction available almost DL systems. DL ALCN (i.e., ALC
extended number restrictions) first treated Hollunder Nutt (1990),
ALCF. DL ALCQ first investigated Hollunder Baader (1991), ALCO
Schaerf (1994).
8

fiFusions Description Logics Abstract Description Systems

Name
Role composition

Syntax
R1 R2

Semantics
{(a, b) |
c . (a, c) R1I (c, b) R2I }
Role complement R
{(a, b) | (a, b)
/ RI }
Role conjunction
R1 u R2 {(a, b) | (a, b) R1I (a, b) R2I }
Role disjunction
R1 R2 {(a, b) | (a, b) R1I (a, b) R2I }
Inverse roles
R1
{(a, b) | (b, a) RI }
Transitive closure R+
{(a, b) | (a, b) (RI )+ }
Universal role
U

binary relation R, R+ denotes transitive closure R.

Symbol


u

1
+
U

Figure 2: description logic role constructors.
2.1.3 Role constructors
Role constructors allow us build complex role descriptions. collection role constructors found Figure 2. Again, rightmost column indicates naming scheme,
name modifiers role constructors written superscript separated
commas. example, ALCQ inverse roles transitive closure called ALCQ+,1 .
DLs admitting role constructors, set role descriptions defined inductively, analogously set concept descriptions. semantics role constructors given
third column Figure 2. concept descriptions, used extend
interpretation function role names role descriptions.
DL role constructors, role descriptions used wherever role names may
used corresponding DLs without role constructors. example,
(R1 u R3 ).C u (R2 R2 ).C
,u,t

ALC
-concept description. concept description unsatisfiable since R2 R2
equivalent universal role. Note role descriptions also used within role
assertions ABox.
DL ALC ,t,+ first treated Baader (1991) (under name ALC trans ); Schild
(1991) shown DL notational variant propositional dynamic logic (PDL).
DLs Boolean operators roles investigated Lutz Sattler (2000).
inverse operator available system CRACK (Bresciani, Franconi, & Tessaris,
1995), reasoning DLs inverse roles was, example, investigated Calvanese
et al. (1998) Horrocks et al. (2000). universal role expressed using DLs
Boolean operators roles (see example), turn used simulate
general concept inclusion axioms within concept descriptions.
2.2 Restricting syntax
now, constructors could combined arbitrarily. Sometimes makes sense restrict
interaction constructors since reasoning restricted DL may easier
reasoning unrestricted DL. consider DLs imposing certain restrictions

9

fiBaader, Lutz, Sturm, & Wolter

1. roles may used inside certain concept constructors,
2. roles may used inside certain role constructors,
3. combination role constructors,
4. role constructors may used inside certain concept constructors.
example first case, consider fragment ALCQR+ transitive roles
may used existential universal restrictions, number restrictions (see,
e.g., Horrocks et al., 2000).
result taking fusion two DLs, obtain DLs whose set roles NR
partitioned. example, fusion ALCQ ALC 1 yields fragment ALCQ1
NR partitioned two sets, say NR1 NR2 . fragment, inverse role
constructor roles NR2 may used within qualified number restrictions,
roles NR1 may used inside inverse role constructor.2 Thus, DL
example first, second, fourth case.
consider DL ALCF introduced above, extend ALC f
feature (dis)agreement concept constructor, also provides role composition constructor. However, role chains built using composition comprised exclusively
features non-functional roles may appear inside feature (dis)agreement. Hence,
ALCF also example first, second, fourth case.
example third case, fragment ALC ,u role conjunction
may used inside role complement constructor considered Lutz Sattler
(2000).
restricted DLs, introduce explicit naming scheme. Note that,
paper, deal DLs combinability concept constructors
restricted since DLs would fit framework abstract
description systems introduced next section. example DL would
one atomic negation concepts, i.e., negation may applied concept
names (e.g., DL AL discussed Donini et al., 1997).

3. Abstract description systems
order define fusion DLs prove general results fusions DLs, one needs
formal definition description logics. Since exists wide variety DLs
different characteristics, introduce general formalization,
cover DLs considered literature, also includes logics would usually
subsumed name DL.
3.1 Syntax semantics
syntax abstract description system given abstract description language,
determines set terms, term assertions, object assertions. setting,
concept descriptions represented terms built using abstract description
2. become clearer given formal definition fusion.

10

fiFusions Description Logics Abstract Description Systems

language. General inclusion axioms DLs represented term assertions ABox
assertions DLs represented object assertions.
Definition 4 (Abstract description language). abstract description language (ADL)
determined countably infinite set V set variables, countably infinite set X
object variables, (possibly infinite) countable set R relation symbols arity two,3
(possibly infinite) countable set F functions symbols f , equipped arities
nf . sets pairwise disjoint.
terms tj ADL built using follow syntax rules:
tj

x, t1 , t1 t2 , t1 t2 , f (t1 , . . . , tnf ),

x V , f F, Boolean operators , , different function
symbols F. term t, denote var(t) set set variables used t.
symbol > used abbreviation x x abbreviation x x (where
x set variable).
term assertions ADL
t1 v t2 , terms t1 , t2 ,
object assertions
R(a, b), a, b X R R;
(a : t), X term.
sets term object assertions together form set assertions ADL.
DL point view, set variables correspond concept names, object
variables individual names, relation symbols roles, Boolean operators well
function symbols correspond concept constructors. Thus, terms correspond
concept descriptions. example, let us view concept descriptions DL ALCN u ,
i.e., ALC extended number restrictions conjunction roles, terms ADL.
Value restrictions existential restrictions seen unary function symbols:
role description R, function symbols fR fR , take term tC
(corresponding concept description C) transform complex terms
fR (tC ) fR (tC ) (corresponding concept descriptions R.C R.C). Similarly,
number restrictions seen nullary function symbols: role description R
n N, function symbols fnR fnR . Hence, ALCN u -concept
description u (R1 u R2 ).(B u ( 2R1 )) corresponds term xA f(R1 uR2 ) ((xB
f(2R1 ) )). analyze connection ADLs DLs formally later on.
semantics abstract description systems defined based abstract description
models. models general semantic structures terms ADL
interpreted. already noted here, however, abstract description
system usually take account abstract description models available
language: allows selected subclass models. subclass determines
semantics system.
3. keep things simpler, restrict attention case binary predicates, i.e., roles DL.
However, results easily extended n-ary predicates.

11

fiBaader, Lutz, Sturm, & Wolter

Definition 5. Let L ADL Definition 4. abstract description model (ADM)
L form

E
W = W, F W = {f W | f F}, RW = {RW | R R} ,


ff
W nonempty set, f W functions mapping every sequence X1 , . . . , Xnf
subsets W subset W , RW binary relations W .
Since ADMs interpret variables, need assignment assigns subset
W set variable, evaluate terms ADM. evaluate object
assertions, need additional assignment assigns element W object
variable.
ff


Definition 6. Let L ADL W = W, F W , RW ADM L. assignment
W pair = (A1 , A2 ) A1 mapping set set variables V
2W , A2 injective4 mapping set object variables X W . Let W
ADM = (A1 , A2 ) assignment W. L-term t, inductively
associate value tW,A 2W follows:
xW,A := A1 (x) variables x V ,
t2W,A ,
, (t1 t2 )W,A := tW,A
tW,A
(t)W,A := W \ (t)W,A , (t1 t2 )W,A := tW,A
1
2
1
f (t1 , . . . , tnf )W,A := f W (tW,A
, . . . , tW,A
nf ).
1
x1 , . . . , xn set variables occurring t, often write tW (X1 , . . . , Xn )
shorthand tW,A , assignment xA
= Xi 1 n.
truth-relation |= hW, Ai assertions defined follows:
hW, Ai |= R(a, b) iff A2 (a)RW A2 (b),
hW, Ai |= : iff A2 (a) tW,A ,
.
tW,A
hW, Ai |= t1 v t2 iff tW,A
2
1
case say assertion satisfied hW, Ai. If, ADM W set
assertions , exists assignment W assertion satisfied
hW, Ai, W model .
two differences ADMs DL interpretations. First, DL interpretation, interpretation role names fixes interpretation function
symbols corresponding concept constructors involve roles (like value restrictions,
number restrictions, etc.). interpretation concept names corresponds assignment. Thus, DL model ADM together assignment, whereas ADM
alone corresponds called frame modal logics. Second, DL roles used
concept constructors may, course, also occur role assertions. contrast, definition
ADMs per se enforce connection interpretation function
symbols interpretation relation symbols. connections can, however,
enforced restricting attention subclass possible ADMs ADL.
4. corresponds unique name assumption.

12

fiFusions Description Logics Abstract Description Systems

Definition 7. abstract description system (ADS) pair (L, M), L ADL
class ADMs L closed isomorphic copies.5
DL point view, choice class defines semantics
concept role constructors, allows us, e.g., incorporate restrictions role
interpretations. sense, ADS viewed determining (description) logic.
concrete, DL interpretation interpretation function symbols
determined interpretation role names. Thus one can, example, restrict
class models ADMs interpret certain role transitive relation
composition two roles. Another restriction realized choice
nominals (corresponding nullary function symbols) must interpreted
singleton sets.
Let us define reasoning problems abstract description systems. introduce
satisfiability sets assertions (with without term assertions), corresponds
consistency ABoxes (with without GCIs), satisfiability terms (with without
term assertions), corresponds satisfiability concept descriptions (with without
GCIs).
Definition 8. Given ADS (L, M), finite set assertions called satisfiable
(L, M) iff exists ADM W assignment W hW, Ai
satisfies assertions . term called satisfiable (L, M) iff {a : t} satisfiable
(L, M), arbitrary object variable.
satisfiability problem (L, M) concerned following question: given
finite set object assertions L, satisfiable (L, M).
relativized satisfiability problem (L, M) concerned following question: given finite set assertions L, satisfiable (L, M).
term satisfiability problem (L, M) concerned following question:
given term L, satisfiable (L, M).
relativized term satisfiability problem (L, M) concerned following
question: given term set term assertions L, {a : t} satisfiable
(L, M).
next section, define fusion two ADSs, show (relativized)
satisfiability decidable fusion (relativized) satisfiability component ADSs
decidable. transfer results hold, must restrict so-called local
ADSs.


ff
Wp , RWp pairwise
Definition 9. Given family (Wp )pP
ADMs W
p = Wp , F
ff
disjoint domains Wp , say W = W, F W , RW disjoint union (Wp )pP iff

W = pP Wp ,
5. Intuitively, means that, ADM W belongs M, ADMs differ w.r.t.
names elements domain W also belong M.

13

fiBaader, Lutz, Sturm, & Wolter


f W (X1 , . . . , Xnf ) = pP f Wp (X1 Wp , . . . , Xnf Wp ) f F
X1 , . . . , Xnf W ,

RW = pP RWp R R.
ADS = (L, M) called local iff closed disjoint unions.
remainder section, first analyze connection ADSs DLs
detail, comment relationship modal logics.
3.2 Correspondence description logics
show DLs introduced Section 2 correspond ADSs. order this,
first need introduce frames, notion well-known modal logic. Let L one
DLs introduced Section 2.
Definition 10 (Frames). L-frame F pair (F , F ), F nonempty set,
called domain F, F interpretation function, maps
nominal singleton subset F F ,
role name R subset RF F F restrictions role
interpretations L satisfied. example, ALC R+ , R NR+ mapped
transitive binary relation.
interpretation function F inductively extended complex roles obvious
way, i.e., interpreting role constructors L according semantics given
Figure 2.
interpretation based frame F iff = F , RI = RF roles R NR ,
= F nominals .
frame viewed interpretation partial sense interpretation individual concept names fixed. Note (in contrast case
concept individual names) interpretation nominals already fixed frame.
reason that, interpret nominals frame,
treat set variables ADS side. would, however, variables
singleton sets may assigned. Since restriction possible
framework ADSs defined above, interpret nominals frame. consequence
correspond functions arity 0 ADS side.
Now, define abstract description system = (L, M) corresponding DL L.
straightforward translate syntax L abstract description language L.
Definition 11 (Corresponding ADL). Let L DL concept role constructors
well restrictions role interpretations introduced Section 2. corresponding
abstract description language L defined follows. every concept name L,
exists set variable xA L, every individual name L exists object
variable ai L. Let R set (possibly complex) role descriptions L. set
relation symbols L R, set function symbols L smallest set containing
1. every role description R R, unary function symbols fR fR ,
14

fiFusions Description Logics Abstract Description Systems

2. L provides unqualified number restrictions, then, every n N every role
description R R, function symbols fnR fnR arity 0,
3. L provides qualified number restrictions, then, every n N every role R R,
unary function symbols fnR
fnR
,


4. L provides nominals, then, every , function symbol fI arity 0,
5. L provides feature agreement disagreement, then, every pair feature chains
(u1 , u2 ), two function symbols fu1 u2 fu1 u2 arity 0.
L-concept description C, let tC denote representation C L-term,
defined obvious way: concept names translated set variables xA ,
concept constructors , u, mapped , , , respectively,
concept constructors translated corresponding function symbols. Obviously,
sets function relation symbols L may infinite.
example translation concept descriptions terms ADL already
given above: ALCN u -concept description u (R1 u R2 ).(B u ( 2R1 )) corresponds
term xA f(R1 uR2 ) ((xB f(2R1 ) )).
define set abstract description models corresponding DL L.
every L-frame, contains corresponding ADM.
Definition 12 (Corresponding
Let
F = (F , F ) frame. corresponding

ADM).
ff
abstract description model W = W, F W , RW domain W := F . relation symbols
L role descriptions L, thus interpreted frame F.
relation symbol R R hence define RW := RF .
define F W , need define f W every nullary function symbol f L,
W
f (X) every unary function symbol f L every X . Let arbitrary
concept name. X F , let IX interpretation based F mapping
concept name X every concept name .6 define f W , make case
distinction according type f :
W (X) := (R.A)IX ,
1. fR

W (X) := (R.A)IX ,
fR

W := (nR)I , f W := (nR)I ,
2. fnR
nR
W (X) := (nR.A)IX , f W (X) := (nR.A)IX ,
3. fnR


nR

4. fIW := ,
5. fuW1 u2 = (u1 u2 )I , fuW1 u2 = (u1 u2 )I .
class ADMs thus obtained DL L obviously closed isomorphic copies since also holds set L-frames (independently DL L
consider). Hence, tuple = (L, M) corresponding DL L indeed ADS.
example, let us view DL ALCN u ADS. ADL L corresponding
ALCN u already discussed. Thus, concentrate class ADMs induced
6. Taking empty set arbitrary.

15

fiBaader, Lutz, Sturm, & Wolter

frames ALCN u . Assume F frame, i.e., F consists
nonempty


ff
domain interpretations RF role names R. ADM W = W, F W , RW induced
F defined follows. set W identical domain F. role description
yields relation symbol, interpreted W frame. example,
(R1 u R2 )W = R1F R2F . remains define interpretation function symbols.
illustrate two examples. First, consider (unary) function symbol f(R1 uR2 ) .
W
Given subset X W , function f(R
maps X
1 uR2 )
W
f(R
(X) := {w W | v X v (w, v) R1F R2F },
1 uR2 )

i.e., interpretation concept description (R1 u R2 ).A interpretations based
F interpreting X. Accordingly, value constant symbol f(2R) W
given interpretation ( 2R) interpretations based F.
easy show interpretation concept descriptions L coincides
interpretation corresponding terms = (L, M).


ff
Lemma 13. Let F frame, W = W, F W , RW ADM corresponding F, =
(A1 , A2 ) assignment W, C concept description, let concept names
used C among A1 , . . . , Ak . interpretations based F AIi = A1 (xAi )
1 k,
C = tW,A
.
C
easy consequence lemma, close connection reasoning
DL L reasoning corresponding ADS. Given TBox ABox
DL L, define corresponding set S(T , A) assertions corresponding ADL
(L, M) obvious way, i.e., GCI C v yields term assertion tC v tD ,
role assertion R(i, j) yields object assertion R(ai , aj ), concept assertion
C(i) yields object assertion ai : tC .
Proposition 14. ABox consistent relative TBox L iff S(T , A)
satisfiable corresponding ADS.
treat non-relativized consistency explicitly since special case
relativized consistency TBox empty.
already mentioned above, transfer results require component ADSs
local. call DL L local iff ADS (L, M) corresponding L local. turns
DLs introduced Section 2 local.
Proposition 15. Let L one DLs introduced Section 2. Then, L local iff L
include following constructors: nominals, role complement, universal
role.
Proof. start direction, interesting since shows
ADSs corresponding DLs nominals, role complement, universal role
local. make case distinction according constructors L contains.
Nominals. Consider disjoint union W ADMs W1 W2 , assume
W1 W2 correspond frames DL nominals. definition
16

fiFusions Description Logics Abstract Description Systems

disjoint union, know W1 W2 = . nominal,
definition disjoint union implies fIW = fIW1 fIW2 . Since nominals
interpreted singleton sets W1 W2 , since domains W1 W2
disjoint, implies fIW set cardinality 2. Consequently, W cannot
correspond ADM induced frame DL nominals, since frames
interpret nominals singleton sets.
Universal role. Again, consider disjoint union W ADMs W1 W2 ,
assume W1 W2 correspond frames DL universal role. Let U
denote universal role, i.e., role name interpretation restricted
binary relation relating pair individuals domain. definition
disjoint union, U W = U W1 U W2 = W1 W1 W2 W2 6= W W .
Consequently, W cannot correspond ADM induced frame DL
universal role, since frame would interpret U W W .
Role complement. Again, consider disjoint union W ADMs W1 W2 ,
assume W1 W2 correspond frames DL role negation.
W
W
W
arbitrary role name R, R = R 1 R 2 = (W1 W1 \ RW1 ) (W2
W2 \ RW2 ) 6= (W1 W2 ) \ (RW1 RW2 ) = W \ RW .
remains prove direction. Assume L one DLs introduced
Section 2 allow nominals, role complements,
universal role.
Let
ff
(Fp )pP family L-frames Fp = (Fp , Fp ) let Wp = Wp , F Wp , RWp
ADMs corresponding them. definition, Fp = Wp p P . Assume
domains (Wp )pP pairwise disjoint. must show disjoint union (Wp )pP
also corresponds L-frame. purpose, define frame F = (F , F )
follows:
F :=



RF :=



pP

Fp

pP

RFp R NR .

ff


Let W = W, F W , RW
ADM corresponding
F.WpBy Definition 12 (corW
responding ADM), W = pP Wp R = pP R
R NR .
induction structure complex roles, easy show also holds
R R, i.e., complex role descriptions. example, consider role description


W
W
R1 R2 . induction, know R1W = pP R1 p R2W = pP R2 p . Since
sets (Wp )pP pairwise disjoint,
(R1 R2 )W = R1W R2W =

[

pP

Wp

R1



[

Wp

R2

pP

=

[

pP

Wp

R1

Wp

R2

=

[

(R1 R2 )Wp .

pP

Since RWp = RFp R R p P , obtain following fact:
() p P , Fp , role descriptions R R, following holds: RF (a) =
RFp (a); particular, RF (a) Fp .
17

fiBaader, Lutz, Sturm, & Wolter

remains show that, n 0, X1 , . . . , Xn W , function symbols f
arity n,
[
f W (X1 , . . . , Xn ) =
f Wp (X1 Wp , . . . , Xn Wp ).
pP

proved making case distinction according type f . treat two
cases exemplarily.

f = fu1 u2 . Since W = pP Wp sets Wp pairwise disjoint, fuW1 u2
disjoint union sets fuW1 u2 Wp p P . remains show fuW1 u2 Wp =
W

W

W

p
p
p
fu1 u
(p P ). definition fu1 u
, know fu1 u
iff Fp ,
2
2
2

F

F

F

F

u1 p (a) u2 p (a) defined, u1 p (a) = u2 p (a). (), case iff
Fp , uF1 (a) uF2 (a) defined uF1 (a) = uF2 (a), equivalent
fuW1 u2 Wp .

W (X)
f = fnR
. Since W = pP Wp sets Wp pairwise disjoint, fnR



W (X) W p P . remains show
disjoint union sets fnR
p

W

W

W (X) W = f p (X W ) (p P ). definition f p , know
fnR
p
p



nR
nR
W

p
fnR
(X Wp ) iff Fp |RFp (a) (X Wp )| n. (), case iff


W (X) W .
|RF (a) (X Wp )| n iff |RF (a) X| n, hence iff fnR
p




noted arguments similar ones used proof
direction show that, presence universal role role negation, function
symbols (e.g., fU ) may also violate locality condition.
transfer results decidability developed paper apply fusions
local ADSs. Hence, direction proposition implies results
applicable fusions ADSs corresponding DLs incorporate nominals, role
complement, universal role.
3.3 Correspondence modal logics
paper concern fusions description logics modal logics. Nevertheless,
useful brief look relationship ADSs modal logic. Standard
modal languages regarded ADLs without relation symbols object variables
(just identify propositional formulas terms). Given ADL L, set L L-terms
called classical modal logic iff contains tautologies classical propositional logic
closed modus ponens, substitutions, regularity rule
x1 y1 , . . . , xnf ynf
f (x1 , . . . , xnf ) f (y1 , . . . , ynf )
function symbols f L. minimal classical modal logic language one
unary function symbol known logic E (see Chellas, 1980).
ADS (L, M) based L determines classical modal logic L taking valid
terms, i.e., defining
L iff tW,A = W W assignments W.
18

fiFusions Description Logics Abstract Description Systems

logic E determined ADS precisely one unary operator whose class
ADMs consists models. Chellas formulates completeness result (Theorem
9.8 Chellas, 1980) so-called minimal models (alias neighborhood-frames), are,
however, notational variant abstract description models one unary operator
(Dosen, 1988). classical modal logic L determined ADS decidable term
satisfiability problem, L decidable since L iff unsatisfiable.
classical modal logic L called normal iff additionally contains
f (x1 , . . . , xj1 , xj yj , xj+1 , . . . , xnf ) f (x1 , . . . , xj1 , xj , xj+1 , . . . , xnf )
f (x1 , . . . , xj1 , yj , xj+1 , . . . , xnf )

f (>, , . . . , ), f (, >, , . . . , ), . . . , f (, . . . , , >),
function symbols f j 1 j nf (Jonsson & Tarski, 1951; Jonsson &
Tarski, 1952; Goldblatt, 1989). definition normal modal logics assumes
formulas (terms) built using necessity (box) operators.7 work
necessity operators; corresponding possibility-operators definable putting
f 3 (x1 , . . . , xnf ) = f (x1 , . . . , xnf ).
minimal normal modal logic language one unary operator known K
(Chellas, 1980).
call function F : W n W normal iff 1 j n X1 , . . . , Xn , Yj W
F (X1 , . . . , Xj1 , Xj Yj , Xj+1 , . . . , Xn ) = F (X1 , . . . , Xj1 , Xj , Xj+1 , . . . , Xn )
F (X1 , . . . , Xj1 , Yj , Xj+1 , . . . , Xn ))

F (W, , . . . , ) = F (, W, , . . . , ) = = F (, . . . , , W ) = W.
Note unary function F normal iff F (W ) = W F (X ) = F (X) F (Y ),
X, W . function symbol f called normal ADS (L, M) iff functions
f W normal W M.
role R DL, function symbol fR normal corresponding
ADS. contrary, readily checked neither fnR
fnR
duals


3
3
fnR

f

normal.


nR
Obviously, ADS (L, M) determines normal modal logic iff function symbols
L normal (L, M). Completeness K respect Kripke semantics (Chellas,
1980) implies logic K determined ADS one unary operator whose
class ADMs consists models interpreting operator normal function.
7. Note authors define normal modal logics using possibility (diamond) operators, case
definitions duals introduced thus first sight look quite different.

19

fiBaader, Lutz, Sturm, & Wolter

4. Fusions abstract description systems
section, define fusion abstract description systems prove two transfer theorems decidability, one concerning satisfiability one concerning
relativized satisfiability.
Definition 16. fusion S1 S2 = (L1 L2 , M1 M2 ) two abstract description
systems S1 = (L1 , M1 ) S2 = (L2 , M2 )
disjoint sets function symbols F L1 G L2 ,
disjoint sets relation symbols R L1 Q L2 ,
sets set object variables
defined follows: L1 L2 ADL based
union F G function symbols L1 L2 ,
union R Q relation symbols L1 L2 ,
M1 M2 defined
E
E

E

{ W, F W G W , RW QW | W, F W , RW M1 W, G W , QW M2 }.
example, consider ADSs S1 S2 corresponding DLs ALCF
ALC +,,t introduced Section 2. concentrate function symbols provided
fusion. following, assume set role names employed ALCF
ALC +,,t disjoint.
ADS S1 based following function symbols: (i) unary functions symbol
fR fR every role name R ALCF, (ii) nullary functions symbols corresponding same-as constructor every pair chains functional roles
ALCF.
ADS S2 based following function symbols: (iii) unary functions symbol
fQ fQ every role description Q built role names ALC +,,t using
union, composition, transitive closure.
Since assumed set role names employed ALCF ALC +,,t disjoint,
sets function symbols also disjoint. union sets provides us
symbols same-as constructor symbols value existential restrictions role descriptions involving union, composition, transitive closure.
However, role descriptions contain role names ALC +,,t , thus none
functional roles ALCF occurs descriptions. Thus, fusion ALCF
ALC +,,t yields strict fragment union ALCF +,,t .
4.1 Relativized satisfiability
prove transfer result decidability relativized satisfiability problem, show
also yields corresponding transfer result relativized term satisfiability problem,
investigate transfer results extended ADSs correspond DLs
providing universal role.
20

fiFusions Description Logics Abstract Description Systems

4.1.1 transfer result
section concerned establishing following transfer theorem:
Theorem 17. Let S1 S2 local ADSs, suppose relativized satisfiability
problems S1 S2 decidable. relativized satisfiability problem S1 S2
also decidable.
idea underlying proof theorem translate given set assertions
S1 S2 set assertions 1 S1 set assertions 2 S2
satisfiable S1 S2 iff 1 satisfiable S1 2 satisfiable S2 . first (naive)
idea obtain set (i = 1, 2) replace alien terms (i.e., subterms
starting function symbols belonging Si ) new set variables (the surrogate
variables introduced below). approach, satisfiability would fact imply
satisfiability sets , converse would true. difficulty arises
trying combine models 1 2 one . ensure two models
indeed combined, sets must contain additional assertions make sure
surrogate variables one model corresponding alien subterms
model interpreted compatible way. precise, (finitely many)
different ways adding assertions, one must try (if any) leads
satisfiable pair 1 2 .
proof Theorem 17, fix two local ADSs Si = (Li , Mi ), {1, 2},
L1 based set function symbols F relation symbols R, L2 based G
Q. Let L = L1 L2 = M1 M2 .
follows, use following notation: set assertions , denote
term() obj() set terms object names , respectively.
start explaining alien subterms set replaced new set
variables. L-term form h(t1 , . . . , tn ), h F G, reserve new variable
xt , called surrogate t. assume set surrogate variables
disjoint original sets variables. sketched above, idea underlying
introduction surrogate variables decision procedure S1 (S2 ) cannot deal
terms containing function symbols G (F). Thus, alien function symbols
must replaced applying procedure. precise, replace whole
alien subterm starting alien function symbol surrogate. example,
unary symbol f belongs F, unary symbol g belongs G, f (g(f (x)))
mixed L-term. obtain term L1 , replace subterm g(f (x)) surrogate,
yields f (xg(f (x)) ). Analogously, obtain term L2 , replace whole term
surrogate, yields xf (g(f (x))) . define replacement process
formally.
Definition 18. L-term without surrogate variables, denote sur1 (t) L1 -term
resulting occurrences terms g(t1 , . . . , tn ), g G, within
scope g 0 G replaced surrogate variable xg(t1 ,...,tn ) . set
terms, put sur1 () := {sur1 (t) | } define sur2 (t) well sur2 () accordingly.
Denote sub() set subterms terms , sub1 () variables
occurring well subterms alien terms (i.e., terms starting symbol
21

fiBaader, Lutz, Sturm, & Wolter

G) . formally, define
sub1 () := sub{t | xt var(sur1 ())} var().
Define sub2 () accordingly.
example, let f F unary g G binary. = f (g(x, f (g(x, y)))),
sur1 (t) = f (xg(x,f (g(x,y))) ). Note restriction within scope g 0 G
clarify top-most alien subterms replaced. term
example, sub1 ({t}) = {g(x, f (g(x, y))), f (g(x, y)), g(x, y), x, y}.
Note Boolean operators occurring terms shared function symbols
sense alien neither L1 L2 . Thus, sur1 (f (x)g(x, y)) = f (x)xg(x,y)
sur2 (f (x) g(x, y)) = xf (x) g(x, y).
course, replacing whole terms variables, information lost.
example, consider (inconsistent) assertion (R1 .((1 R2 )u(2 R2 )))(i) assume
R1 role one component fusion, R2 role component. Translated
abstract description language syntax, concept description R1 .((1 R2 ) u (2 R2 ))
yields term := fR1 (f(1 R2 ) f(2 R2 ) ), fR1 function symbol L1
two function symbols belong L2 . Now, sur1 (t) = fR1 (x y), x
surrogate f(1 R2 ) surrogate f(2 R2 ) . decision procedure
first ADS sees fR1 (x y), way know conjunction alien
subterms corresponding x unsatisfiable. fact, procedure x
arbitrary set variables, thus x satisfiable. avoid problem, introduce
so-called consistency set consisting types, type says relevant formula
whether formula negation supposed hold. sets 1 2
contain additional information basically ensures models satisfy
types. allow us merge models one .
Definition 19. Given finite set L-terms, define consistency set C()
C() := {tc | c }, type tc determined c defined
tc :=

^

{ | c}

^

{ | \ c}.

Given finite set assertions L, define subi () := subi (term()). abbreviate
C () := C(subi ()), {1, 2}.
example above,
sub1 (fR1 (f(1 R2 ) f(2 R2 ) ) = {f(1 R2 ) , f(2 R2 ) },
thus C 1 ({ai : fR1 (f(1 R2 ) f(2 R2 ) )}) consists 4 terms
f(1 R2 )
f(1 R2 )
f(1 R2 )
f(1 R2 )






f(2 R2 ) ,
f(2 R2 ) ,
f(2 R2 ) ,
f(2 R2 ) .
22

fiFusions Description Logics Abstract Description Systems

Given set terms , element tc consistency set C() indeed considered
type element e domain ADM w.r.t. . element e
belongs interpretations terms , complements
interpretations terms. Thus, c set terms e belongs,
e also belongs interpretation tc belong interpretation
terms C(). case say e realizes type tc .
ready formulate theorem reduces relativized satisfiability
problem fusion two local ADSs relativized satisfiability component ADSs.
proof theorem found appendix.
Theorem 20. Let Si = (Li , Mi ), {1, 2}, two local ADSs L1 based
set function symbols F relation symbols R, L2 based G Q,
let L = L1 L2 = M1 M2 . finite set assertions L,
following equivalent:
1. satisfiable (L, M).
2. exist
(a) set C 1 (),
(b) every term object variable 6 obj(),
(c) every obj() term ta D,
union 1 following sets assertions L1 satisfiable
(L1 , M1 ):
W
(d) {at : sur1 (t) | D} {> v sur1 ( D)},
(e) {a : sur1 (ta ) | obj()},

(f ) {R(a, b) | R(a, b) , R R},
(g) {sur1 (t1 ) v sur1 (t2 ) | t1 v t2 } {a : sur1 (s) | (a : s) };
union 2 following sets assertions L2 satisfiable (L2 , M2 ):
W
(h) {at : sur2 (t) | D} {> v sur2 ( D)},
(i) {a : sur2 (ta ) | obj()},

(j) {Q(a, b) | Q(a, b) , Q Q}.
Intuitively, (2a) guesses set types (i.e., elements consistency set).
idea exactly types realized model (to constructed
showing (2 1) given showing (1 2)). Condition (2b) introduces
every type name object realizing type, (2c) guesses every object
variable occurring type D.
W Regarding (2d) (2h), one note set assertions {at : | D}{> v
D} states every type realized (i.e., object model
type) every object one types D. sets assertions (2d)
(2h) obtained set surrogation make digestible decision
procedures component logics.
23

fiBaader, Lutz, Sturm, & Wolter

assertions (2e) (2i) state (again surrogated versions) object
interpreting variable type ta . ensures that, models 1 2
(given showing (2 1)), objects interpreting type ta D.
Otherwise, models could combined common one .
sets (2f) (2j) obtained distributing relationship assertions
1 2 , depending relation symbol used assertion.
set (2g) contains (in surrogated version) term assertions form t1 v t2
membership assertions form : .
Condition 2 asymmetric two respects. First, guesses subset C 1 () rather
subset C 2 (). course arbitrary, could also chosen index 2 instead
1 here. Second, set 2 neither contains assertions {sur2 (t1 ) v sur2 (t2 ) | t1 v
t2 } {a : sur2 (s) | (a : s) }. added assertions, theorem would
still true, would unnecessarily increase amount work done
combined decision procedure. fact, since assertions 1 2 enforce tight
coordination models 1 2 , fact membership assertions
term assertions satisfied models 1 implies also satisfied
models 2 (see appendix details).
prove Theorem 17, must show Theorem 20 used construct
decision procedure relativized satisfiability S1 S2 decision procedures
component systems S1 S2 . given finite set assertions S1 S2 , set
C 1 () also finite, thus finitely many sets (2a) choices types
object variables (2c). Consequently, enumerate check whether
one choices leads satisfiable sets 1 2 . definition sets
functions suri , assertions indeed assertions Li , thus satisfiability
algorithm (Li , Mi ) applied . proves Theorem 17.
Regarding complexity obtained decision procedure, costly step guessing
right set D. Since cardinality set sub1 () linear size ,
cardinality C 1 () exponential size (and element size quadratic
). Thus, doubly exponentially many different subsets chosen from. Since
cardinality chosen set may exponential size , also size 1
2 may exponential (because big disjunction D). this,
following corollary follows.
Corollary 21. Let S1 S2 local ADSs, suppose relativized satisfiability
problems S1 S2 decidable ExpTime (PSpace). relativized satisfiability problem S1 S2 decidable 2ExpTime (ExpSpace).
p (n)

Proof. Assume size n. must consider 22 1 (for polynomial p1 )
p (n)
different sets (2a). set size 2p1 (n) thus 22 2 choices
(2c) (for polynomial p2 ). Overall, still leaves us doubly exponentially
many choices. assume relativized satisfiability problems S1 S2
decidable ExpTime. Since call procedures applied set assertions
p (n)
p (n)
exponential size, may take double exponential time, say 22 3 22 4 (for polynomials
p3 p4 ). Overall, thus time complexity
22

p1 (n)

22

p2 (n)

(22
24

p3 (n)

+ 22

p4 (n)

),

fiFusions Description Logics Abstract Description Systems

p(n)

clearly majorized 22
appropriate polynomial p. shows
membership 2ExpTime.
argument regarding space complexity similar. one must additionally
take account doubly exponentially many choices enumerated using
exponentially large counter.

4.1.2 relativized term satisfiability problem
statement Theorem 17 imply transfer result relativized term
satisfiability problem. problem decidability relativized term satisfiability
problem S1 S2 necessarily imply decidability relativized satisfiability
problem ADSs, thus prerequisite theorem apply satisfied.
However, consider statement Theorem 20, easy see theorem
also yields transfer result relativized term satisfiability problem.
Corollary 22. Let S1 S2 local ADSs, suppose relativized term satisfiability problems S1 S2 decidable. relativized term satisfiability problem
S1 S2 also decidable.
Proof. Consider satisfiability criterion Theorem 20. interested relativized term satisfiability, form {a : t} 0 , 0 set term
assertions. case, sets assertions 1 2 contain object assertions
involving relations. Now, assume form {a1 : t1 , . . . , : tn } 0i , 0i
set term assertions. Since two assertions form b : s1 , b : s2 equivalent
one assertion b : s1 s2 , may assume ai distinct other. Since Si
local, easy see following equivalent:
1. {a1 : t1 , . . . , : tn } 0i satisfiable Si .
2. {aj : tj } 0i satisfiable Si j = 1, . . . , n.
Since (1 2) trivial, enough show (2 1). Given models Wj Mi {aj :
tj } 0i (j = 1, . . . , n), disjoint union also belongs Mi , clearly model
{a1 : t1 , . . . , : tn } 0i .
second condition checked applying term satisfiability test Si
n times.

4.1.3 Dealing universal role
stated (Proposition 15), ADSs corresponding DLs universal role
local, thus Theorem 17 cannot applied directly. Nevertheless, cases
theorem also used obtain decidability result fusions DLs
universal role, provided provide universal role. (We comment
usefulness approach detail Section 5.4).
Definition 23. Given ADS = (L, M), denote U ADS obtained
1. extending L two function symbols fUS fUS ,
25

fiBaader, Lutz, Sturm, & Wolter



ff
W
2. extending every ADM W = W, F W , RW unary functions fU

W ,
fU

W (X) = X = , f W (X) = W otherwise;
fU
US

W (X) = W X = W , f W (X) = otherwise.
fU
US


ADSs corresponding DL L, ADS U corresponds extension L
universal role, universal role used within value existential
restrictions.8 close connection relativized satisfiability problem
satisfiability problem U .
Proposition 24. local ADS, following conditions equivalent:
1. relativized (term) satisfiability problem decidable,
2. (term) satisfiability problem U decidable,
3. relativized (term) satisfiability problem U decidable.

Proof. restrict attention term satisfiability problem since equivalences
satisfiability problem proved similarly.
implication (3 2) trivial, (2 1) easy show. fact, satisfiable
relative term assertions {s1 v t1 , . . . , sn v tn } iff tfUS .((t1 s1 ). . .(tn sn ))
satisfiable U .
show (1 3), assume relativized term satisfiability problem
decidable. Let = (L, M) U = (LU , MU ). following, use fU
abbreviation fUS . Since replace equivalently term function symbol
fUS fU , may assume without loss generality fUS occur terms
LU .
Suppose set = {a : s} LU given, set term assertions.
want decide whether satisfiable model W MU . purpose, transform set assertions containing fU . idea underlying transformation
that, given model W MU , fU (t)W {W, }, depending whether tW = W
not. Consequently, replace fU (t) accordingly > , evaluation term
W change. However, satisfiability test model W (we
trying decide whether one exists), thus must guess right replacement.
term LU called U -term iff starts fU . set U -terms
occur (possibly subterms) denoted U . Set, inductively, function
8. Note necessary add universal role U set relation symbols since assertion
form U (a, b) trivially true. However, use universal role within (qualified) number
restrictions covered extension.

26

fiFusions Description Logics Abstract Description Systems

: U {, >} subterms terms :
x := x,
(t1 t2 ) := t1 t2 ,
(t1 t2 ) := t1 t2 ,
(t) := ,
(f (t1 , . . . , tn )) := f (t1 , . . . , tn ) f 6= fU arity n,
(fU (t)) := (fU (t)).
Thus, obtained replacing occurrences U -terms image
, i.e., >. Define, function ,
:= {t1 v t2 | t1 v t2 } {a : }
{> v | fU (t) U (fU (t)) = >}
{at : | fU (t) U (fU (t)) = },
mutually distinct new object variables. Note contain
function symbol fU , thus viewed set assertions S. addition, though
contains one membership assertion, contain assertions involving
relation symbols. Consequently, satisfiability checked using term
satisfiability test (see proof Corollary 22 above). Decidability relativized
term satisfiability problem U follows following claim:
Claim. satisfiable member MU iff exists mapping : U {, >}
satisfiable member M.

prove claim, firstff suppose satisfied assignment member
W = W, F W {fUW }, RW MU . Define setting (fU (t)) = > (fU (t))W,A =
W , (fU (t))
= otherwise.
Obviously, implies satisfied
ff
W
W
assignment W, F , R , member M.
suppose satisfiable mapping . Take member W =

Conversely,
ff
W
W

0
ffand assignment hW, Ai |= . Set W :=

W, F W , R W
W
W, F {fU }, R , prove, induction, terms occur :
()

0

tW ,A = (t )W,A .

critical case one = fU (s). First, assume (fU (s)) = (fU (s)) =
0
>. contains > v , thus W = (s )W,A = sW ,A , second identity
0
0
holds induction. However, sW ,A = W implies (fU (s))W ,A = W = >W,A . case
(fU (s)) = (fU (s)) = treated similarly. term assertion : ensures
(and thus induction s) interpreted whole domain. Consequently,
applying fU yields empty set.
Since hW, Ai |= , identity () implies hW0 , Ai |= . completes proof
claim, thus also proposition.

normal modal logics, result stated proposition already shown
Goranko Passy (1992). proof technique used can, however, transfered
27

fiBaader, Lutz, Sturm, & Wolter

general situation since strongly depends normality modal
operators.
Using Proposition 24, obtain following corollary first transfer theorem.
Corollary 25. Let S1 , S2 local ADSs assume that, {1, 2}, relativized
(term) satisfiability problem Si decidable. relativized (term) satisfiability
problem S1U S2U decidable.
Proof. know Theorem 17 (Corollary 22) relativized (term) satisfiability
problem S1 S2 decidable. Hence, Proposition 24 yields relativized (term)
satisfiability problem (S1 S2 )U decidable. S1U S2U notational variant
(S1 S2 )U : function symbols fUS1 fUS2 replaced fUS1 S2 (and
analogously fUS1 S2 ) since three identical semantics.

4.2 Satisfiability
Note Theorem 17 yield transfer result unrelativized satisfiability
problem. course, relativized satisfiability problems S1 S2 decidable,
theorem implies satisfiability problem S1 S2 also decidable (since
special case relativized satisfiability problem). However, able apply
theorem obtain decidability satisfiability problem fusion, component
ADSs must satisfy stronger requirement relativized satisfiability problemWis decidable. Indeed, set Theorem 20 contains term assertion (namely > v suri ( D))
even contain term assertions.
cases relativized satisfiability problem undecidable whereas
satisfiability problem still decidable. example, Theorem 17 cannot applied
fusion ALCF ALC +,,t since relativized satisfiability problem ALCF
already undecidable (Baader et al., 1993). However, satisfiability problem decidable
DLs.
4.2.1 Covering normal terms
formulate transfer result satisfiability problem, need introduce
additional notion, generalizes notion normal modal logic.
Definition 26 (Covering normal terms). Let (L, M) ADS f function
symbol L arity n. term tf (x) (with one variable x) covering normal term
f iff following holds W M:
tW
f (W ) = W
W
W
X, W , tW
f (X ) = tf (X) tf (Y ),

X, X1 , . . . , Yn W : X Xi = X Yi 1 n implies
W
W
W
tW
f (X) f (X1 , . . . , Xn ) = tf (X) f (Y1 , . . . , Yn ).

ADS (L, M) said covering normal terms iff one effectively determine
covering normal term tf every function symbol f L.
28

fiFusions Description Logics Abstract Description Systems

Intuitively, first two conditions state covering normal term behaves like
value restriction (or box operator). Consider term fR (x), fR function
symbol corresponding value restriction constructor role R. fR (x)
obviously satisfies first two requirements covering normal terms. Note
second condition implies function induced tf monotonic, i.e., X implies
W
tW
f (X) tf (Y ). third condition specifies connection covering normal
term function symbol covers. respect elements tW
f (X), values
W
W
functions f (X1 , . . . , Xn ) f (Y1 , . . . , Yn ) agree provided arguments
agree X. easy see fR (x) covering normal term function symbols
corresponding value, existential, (qualified) number restrictions role R
(see Proposition 35 below).
Given covering normal terms tf function symbols f finite set function
symbols E, one construct term tE covering normal term elements
E.
Lemma 27. Suppose ADS (L, M) covering normal terms L based set
function symbols F . Denote tf covering normal term function symbol f ,
f F . Then, every finite set E F function symbols, term
tE (x) :=

^

tf (x)

f E

covering normal term f E.
4.2.2 Correspondence normal modal logics
following result shows ADS every function symbol normal
covering normal terms. Hence, notion covering normal terms generalizes notion
normality modal logics.
Proposition 28. Let (L, M) ADS, assume f normal function symbol
(L, M).
tf (x) := f (x, , . . . , ) f (, x, . . . , ) f (, . . . , , x)
covering normal term f . particular, f nullary (unary), tf (x) = >
(tf (x) = f (x)) covering normal term f .
Proof. first two conditions definition covering normal terms immediately
follow definition normal function symbols. Thus, concentrate third
condition. Assume, simplicity, f binary. Suppose W X, X1 , X2 , Y1 , Y2
W X Xi = X Yi = 1, 2, set F := f W . F (X X1 , X X2 ) =
F (X Y1 , X Y2 ). Since F normal, know
F (X X1 , X X2 ) = F (X, X) F (X, X2 ) F (X1 , X) F (X1 , X2 ),
F (X Y1 , X Y2 ) = F (X, X) F (X, Y2 ) F (Y1 , X) F (Y1 , Y2 ),
29

fiBaader, Lutz, Sturm, & Wolter

thus
F (X, X) F (X, X2 ) F (X1 , X) F (X1 , X2 ) =
F (X, X) F (X, Y2 ) F (Y1 , X) F (Y1 , Y2 ).
Since, normality F ,
F (X, X) F (X, X2 ) F (X1 , X) tW
f (X),
F (X, X) F (X, Y2 ) F (Y1 , X) tW
f (X),
W
implies tW
f (X) F (X1 , X2 ) = tf (X) F (Y1 , Y2 ).



4.2.3 transfer result
Using covering normal terms, formulate second transfer theorem,
concerned transfer decidability (non-relativized) satisfiability.
Theorem 29. Let S1 S2 local ADSs covering normal terms, suppose
satisfiability problems S1 S2 decidable. satisfiability problem
S1 S2 also decidable.
proof Theorem 17, fix two local ADSs Si = (Li , Mi ), {1, 2},
L1 based set function symbols F relation symbols R, L2 based G
Q. Let L = L1 L2 = M1 M2 .
proof Theorem 29 follows general ideas proof Theorem 17.
are, however, notable differences way satisfiability S1 S2 reduced
satisfiability S1 S2 . Theorem 20 guess set types,
based set additional guesses, pair satisfiability problems 1 2
S1 S2 , respectively, generated. proof Theorem 29, need guess
D. Instead, compute right set. However, computation requires us solve
additional satisfiability problems fusion S1 S2 . Nevertheless, yields reduction
since alternation depth (i.e., number alternations function symbols S1
S2 ) decreases going input set additional mixed satisfiability
problems.
describe reduction detail, must introduce someWnew
notation. case relativized satisfiability, term assertions W
form > v suri ( D)
used assert elements theW domain belong suri ( D). Now, use
covering normal terms propagate suri ( D) terms certain depth.
set function symbols E, define E-depth dE (t) term inductively:
dE (xi ) = 0
dE (t) = dE (t)
dE (t1 t2 ) = dE (t1 t2 ) = max{dE (t1 ), dE (t2 )}
dE (f (t1 , . . . , tn )) = max{dE (t1 ), . . . , dE (tn )} + 1 f E
dE (f (t1 , . . . , tn )) = max{dE (t1 ), . . . , dE (tn )} f 6 E
30

fiFusions Description Logics Abstract Description Systems

finite set assertions,
dE () := max{dE (t) | term()}.
Put, term t(x) one variable x, t0 (x) := x, tm+1 (x) := t(tm (x)), t0 (x) := x,
tm+1 (x) := tm+1 (x) tm (x).
position formulate result reduces satisfiability fusion
two local ADSs covering normal terms satisfiability component ADSs.
Theorem 30. Let Si = (Li , Mi ), {1, 2}, two local ADSs covering normal
terms L1 based set function symbols F relation symbols R,
L2 based G Q, let L = L1 L2 = M1 M2 . Let finite set
object assertions L. Put := dF (), r := dG (), let c(x) (d(x)) covering
normal term function symbols F (G).
{1, 2}, denote set C () term satisfiable
(L, M). following three conditions equivalent:
1. satisfiable (L, M).
2. exist
every 1 object variable 6 obj()
every obj() term ta 1
union 1 following sets object assertions satisfiable
(L1 , M1 ):
W
{at : sur1 (t cm (sur1 ( 1 )) | 1 },
W
{a : sur1 (ta cm (sur1 ( 1 )) | obj()},
{R(a, b) | R(a, b) , R R},
{a : sur1 (s) | (a : s) };
union 2 following sets object assertions satisfiable (L2 , M2 ):
W
{at : sur2 (t dr (sur2 ( 1 )) | 1 },
W
{a : sur2 (ta dr (sur2 ( 1 )) | obj()},
{Q(a, b) | Q(a, b) , Q Q}.

3. condition (2) above, 1 replaced 2 .
sets theorem similarWto ones Theorem 20.
main difference
W term assertion > v suri ( D) longer there. Instead,
disjunction suri ( 1 ) directly inserted terms using covering normals
terms. already mentioned above, another difference set D,
guessed Theorem 20, replaced set 1 (2) 2 (3). Actually, guessing
set
W longer possible case. proof Theorem 30 need know
> v suri ( D) satisfiable Si (i.e., holds least one model Mi ).
way check effectively since algorithm relativized satisfiability
31

fiBaader, Lutz, Sturm, & Wolter

Si . Taking set ensures property satisfied (see proof appendix
details).
definition, set C () term satisfiable (L, M).
Recall term satisfiable iff {a : s} satisfiable (L, M) arbitrary
object variable a. Since elements C () still mixed terms (i.e., terms
fusion), computing set actually needs recursive call decision procedure
satisfiability (L, M). recursion well-founded since alternation depth decreases.
Definition 31. term L, denote a1 (s) a2 (s) 1-alternation
2-alternation depth s, respectively. say, a1 (s) length longest
sequence form (g1 , f2 , g3 , . . .)
g1 (. . . (f2 . . . (g3 . . .)))
gj G fj F appears s. 2-alternation depth a2 (s) defined exchanging
roles F G. Put a(s) := a1 (s) + a2 (s), call alternation depth.
finite set terms, a() maximum a(s) .
Thus, a1 (s) counts maximal number changes symbols first
second ADS, starting first symbol S2 (i.e., first symbol S2
counts change, even occur inside scope symbol S2 ).
2-alternation depth defined accordingly. alternation depth sums 1-
2-alternation depth.
Lemma 32. a(term()) > 0, a(C 1 ()) < a(term()) a(C 2 ()) < a(term()).
Proof. show that, a(term()) > 0, a(sub1 ()) < a(term())
a(sub2 ()) < a(term()), which, definition C , clearly implies lemma. First
note that, definition subi ,
ai (subj ()) ai (term()) i, j.

()

make case distinction follows:
1. a1 (term()) a2 (term()). want show a1 (sub2 ()) < a1 (term()),
since, (), implies a(sub2 ()) < a(term()). Assume contrary
a1 (sub2 ()) a1 (term()). () implies a1 (sub2 ()) = a1 (term()). Hence,
exists term sub2 () sequence (g1 , f2 , g3 , . . . ) function symbols
gi G, fi F length a1 (term()) g1 (. . . (f2 . . . (g3 . . .))) occurs s.
definition sub2 , implies existence term term() function
symbol f F f (. . . g1 (. . . (f2 . . . (g3 . . .)))) occurs t. Since length
(g1 , f2 , g3 , . . . ) a1 (term()), obviously yields a2 (term()) > a1 (term())
contradiction.
2. a1 (term()) a2 (term()). Similar previous case: exchange roles
a1 a2 , F G, sub1 sub2 .

32

fiFusions Description Logics Abstract Description Systems

prove Theorem 29, must show Theorem 30 used construct decision
procedure satisfiability S1 S2 decision procedures component
systems S1 S2 . Let us first consider problem computing sets 1 2 .
a((term()) = 0, consists Boolean combinations set variables.
case, C () consists set variables, , = 1, 2, computed using Boolean
reasoning. a(term()) > 0, Lemma 32 states {1, 2}
a(C ()) < a(term()). induction thus assume effectively
computed. Consequently, remains check Condition (i + 1) Theorem 30 {1, 2}.
Since finite, guess every object variable occurring type ta .
sets 1 2 obtained way indeed sets assertions L1 L2 , respectively.
Thus, satisfiability effectively checked using decision procedures S1
S2 . proves Theorem 29.
argument used also shows Theorem 30 sufficient state
equivalence (1) (2) (as Theorem 20). fact, induction argument used
necessarily always apply computation 1 . cases, alternation
depth may decreases 1 , 2 . noted Theorem 20 could
also formulated symmetric way. done since
necessary proving Theorem 17.
Regarding complexity combined decision procedure, must principle also
consider complexity computing covering normal terms size terms.
examples DL, terms value restrictions, thus size
complexity computing linear. Here, assume polynomial bound both.
assumption, obtain complexity results case relativized
satisfiability. fact, complexity testing Condition (2) (3) Theorem 30 agrees
complexity testing Condition (2) Theorem 20: adds one exponential
complexity decision procedure single ADSs. order compute , need
exponentially many recursive calls procedure. Since recursion depth linear
size , end exponentially many tests Condition (2) (3).
Corollary 33. Let S1 S2 local ADSs covering normal terms, assume
covering normal terms computed polynomial time. satisfiability
problems S1 S2 decidable ExpTime (PSpace), satisfiability problem
S1 S2 decidable 2ExpTime (ExpSpace).
argument case relativized satisfiability, extend
transfer result also term satisfiability.
Corollary 34. Let S1 S2 local ADSs covering normal terms, suppose
term satisfiability problems S1 S2 decidable. term satisfiability
problem S1 S2 also decidable.

5. Fusions description logics
Given two DLs L1 L2 , fusion defined follows. translate
corresponding ADSs S1 S2 , build fusion S1 S2 . fusion L1 L2
L1 L2 DL corresponds S1 S2 . Since definition fusion
ADSs requires sets function symbols disjoint, must ensure ADSs
33

fiBaader, Lutz, Sturm, & Wolter

corresponding L1 L2 built disjoint sets function symbols. DLs
introduced Section 2, achieved assuming sets role names L1
L2 disjoint sets nominals L1 L2 disjoint. DL L1 L2
allows use concept role constructors DLs, restricted way.
Role descriptions either role descriptions L1 L2 . role descriptions
involving constructors names DLs. Concept descriptions may contain concept
constructors DLs; however, constructor Li may use role description
Li (i = 1, 2).
Let us illustrate restrictions two simple examples. fusion ALC + ALC 1
two DLs ALC + ALC 1 fragment ALC +,1 whose set role names
partitioned two sets NR1 NR2
transitive closure operator may applied names NR1 ;
inverse operator may applied names NR2 .
example, concept name, R NR1 Q NR2 , R+ .A u Q1 .A
concept description ALC + ALC 1 , R+ .A u R1 .A (Q1 )+ .A not.
Note that, although two source DLs disjoint sets role names, ALC + ALC 1
role names sets may used inside existential value restrictions since
concept constructors available DLs.
fusion ALCQ ALC R+ two DLs ALCQ ALC R+ fragment
ALCQR+ whose set role names NR (with transitive roles NR+ NR ) partitioned
two sets NR1 NR2 NR+ NR2 that, inside qualifying number restrictions,
role names NR1 may used. particular, means transitive roles
cannot occur within qualified number restrictions.
following, give examples illustrate usefulness transfer results
proved previous section. First, give example case satisfiability
relativized satisfiability. Subsequently, consider complex example
involving so-called concrete domains. Here, general transfer result used prove
decidability result recently proved designing specialized algorithm
fusion. Finally, give example demonstrates restriction
local ADSs really necessary.
5.1 Decidability transfer satisfiability
subsection, give example application Theorem 29
decidability result could obtained using Theorem 17.
Theorem 29 requires ADSs covering normal terms. is, however, satisfied
DLs yield local ADSs.
Proposition 35. Let L one DLs introduced Section 2, let corresponding
ADS = (L, M) local. covering normal terms, terms
computed linear time.
Proof. function symbols f L, term tf form fR (x) role
description R. semantics value restrictions implies terms form satisfy
34

fiFusions Description Logics Abstract Description Systems

first two properties Definition 26. completes proof function symbols
f arity 0 since third condition Definition 26 trivially satisfied. Thus,
nullary function symbols, fR (x) arbitrary role name R job.
remains show that, every unary function symbol f {fR , fR , fnR
, fnR
},


term fR (x) also satisfies third property. immediate consequence
W (X) f W (Y ) = f W (X) f W (X )
fact that, function symbols f , fR
R
models W X, W .

following, consider two description logics ALCF ALC +,,t . Hollunder
Nutt (1990) show satisfiability ALCF-concept descriptions decidable.
true consistency ALCF-ABoxes (Lutz, 1999). Note, however, relativized
satisfiability ALCF-concept descriptions thus also relativized ABox consistency
ALCF undecidable (Baader et al., 1993). ALC +,,t , decidability satisfiability
shown Baader (1991) Schild (1991).9 Decidability ABox consistency ALC +,,t
shown Chapter 7 (De Giacomo, 1995).
unrestricted combination ALCF +,,t two DLs undecidable. precise, satisfiability ALCF +,,t -concept descriptions (and thus also consistency ALCF +,,t ABoxes) undecidable. follows undecidability relativized satisfiability
ALCF-concept descriptions fact role operators ALCF +,,t used
internalize TBoxes (Schild, 1991; Baader et al., 1993). contrast undecidability
ALCF +,,t , Theorem 29 immediately implies satisfiability concept descriptions
fusion ALCF ALC +,,t decidable.
Theorem 36. Satisfiability concept descriptions consistency ABoxes decidable
ALCF ALC +,,t , whereas satisfiability ALCF +,,t -concept descriptions already
undecidable.
Taking fusion thus yields decidable combination two DLs whose unrestricted
combination undecidable. price one pay fusion offers less expressivity unrestricted combination. concept f1 f2 u f1+ .C example
concept description ALCF +,,t allowed fusion ALCF ALC +,,t .
5.2 Decidability transfer relativized satisfiability
example application Corollary 22 (and thus Theorem 17), consider
DL ALC +,,u,t
. DL, satisfiability concept descriptions undecidable. However,
f
expressive fragment decidable relativized satisfiability problem obtained
building fusion two sublanguages ALC +,,t
ALC +,,t,u .
f
Theorem 37. Satisfiability ALC +,,u,t
-concept descriptions undecidable.
f
Undecidability shown reduction domino problem (Berger, 1966;
Knuth, 1973) (see, e.g., Baader & Sattler, 1999, undecidability proofs DLs using
reduction). main tasks solve reduction one express
grid one access points grid. One square grid expressed

N N

9. Note ALC +,,t notational variant test-free propositional dynamic logic (PDL) (Fischer &
Ladner, 1979).

35

fiBaader, Lutz, Sturm, & Wolter

description form (xyuyx).>, x, features. fact, description
expresses points belonging x x successor,
two successors coincide. Accessing point grid achieved
using role description (x y)+ .
Note undecidability result also closely related known undecidability
IDPDL, i.e., deterministic propositional dynamic logic intersection (Harel, 1984).
However, undecidability proof IDPDL Harel (1984) uses test construct,
available ALC +,,u,t
.
f
Next, show relativized satisfiability two rather expressive sublanguages
ALC +,,u,t
decidable.
f
Theorem 38. Relativized satisfiability concept descriptions decidable ALC f+,,t
ALC +,,t,u .
Proof sketch. cases, TBoxes internalized described Schild (1991)
Baader et al. (1993). Thus, sufficient show decidability (unrelativized)
satisfiability.
, follows decidability DPDL (Ben-Ari, Halpern, & Pnueli,
ALC +,,t
f
1982), known correspondence PDL ALC +,,t (Schild, 1991), fact
non-functional roles simulated functional ones presence composition
transitive closure (Parikh, 1980).
ALC +,,t,u , decidability satisfiability follows decidability IPDL, i.e., PDL
intersection (Danecki, 1984).

Given theorem, Corollary 22 yields following decidability result.
Corollary 39. Relativized satisfiability concept descriptions decidable fusion
ALC f+,,t ALC +,,t,u .
5.3 concrete example
Description logics concrete domains introduced Baader Hanschke (1991)
order allow reference concrete objects like numbers, time intervals, spatial
regions, etc. defining concepts. precise, Baader Hanschke (1991)
define extension ALC(D) ALC, concrete domain (see below).
suitable assumptions D, show satisfiability ALC(D) decidable. One
main problems extension DLs relativized satisfiability (and satisfiability DLs TBoxes internalized) usually undecidable (Baader & Hanschke,
1992) (though exceptions, see Lutz, 2001). reason, Haarslev et al. (2001)
introduce restricted way extending DLs concrete domains, show corresponding extension ALCN HR+ decidable relativized satisfiability problem.10
following, show result also obtained easy consequence
10. precise, even show relativized ABox consistency decidable restricted
extension ALCN HR+ concrete domains. Here, restrict ourself satisfiability concepts
since ABoxes introduced Haarslev et al. (2001) also allow use concrete individuals
predicate assertions individuals, covered object assertions ADSs
introduced present paper.

36

fiFusions Description Logics Abstract Description Systems

Theorem 17. Moreover, ALCN HR+ replaced arbitrary local DL
decidable relativized satisfiability problem.
Definition 40 (Concrete Domain). concrete domain pair (D , ),
nonempty set called domain, set predicate names. predicate
name P associated arity n n-ary predicate P nD . concrete
domain called admissible iff (1) set predicate names closed negation
contains name >D , (2) satisfiability problem finite conjunctions
predicates decidable.
Given concrete domain one predicates P (of arity n), one
define new concept constructor f1 , . . . , fn .P (predicate restriction), f1 , . . . , fn
concrete features.11 contrast abstract features considered now, concrete
features interpreted partial functions abstract domain concrete
domain . consider basic DL allows Boolean operators new
concept constructors only.
Definition 41 (B(D)). Let NC set concept names NFc set names
concrete features disjoint NC , let admissible concrete domain. Concepts
descriptions B(D) Boolean combinations concept names predicate restrictions, i.e., expressions form f1 , . . . , fn .P P n-ary predicate
f1 , . . . , fn NFc .
semantics B(D) defined follows. consider interpretation I,
nonempty domain , interprets concept names subsets concrete
features partial functions . Boolean operators interpreted
usual,
(f1 , . . . , fn .P )I = {a | x1 , . . . , xn .
fiI (a) = xi 1 n (x1 , . . . , xn ) P }.
Note concept descriptions interpreted subsets .
Thus, go ADS corresponding B(D), concrete domain explicit
part corresponding ADMs. used define interpretation function
symbols corresponding predicate restrictions. predicate restriction constructor
translated function symbol ff1 ,...,fn .P arity 0, and, ADM W corresponding
W
frame F, ff
defined (f1 , . . . , fn .P )I , interpretation based
1 ,...,fn .P
F maps concept names empty set.
Theorem 42. Let admissible concrete domain. Then, B(D) local
relativized satisfiability problem B(D)-concept descriptions decidable.
Proof. Given family (Wi )iI ADMs Wi corresponding frames Fi pairwise
disjoint
domains Fi (i I), first build union F frames: domain F

F
interprets concrete features obvious way, i.e., f F (x) := f Fi (x)
iI
11. Note general framework introduced Baader Hanschke (1991) allows feature chains
predicate restrictions. Considering feature chains length one main restriction introduced
Haarslev et al. (2001).

37

fiBaader, Lutz, Sturm, & Wolter

x Fi . Let W ADM induced F. ToSprove W fact disjoint union
Wi
W
(Wi )iI , remains show ff
= iI ff
. easy consequence
1 ,...,fn .P
1 ,...,fn .P
semantics predicate restriction constructor, interpretation concrete
features F, fact domains Fi pairwise disjoint.
Decidability unrelativized satisfiability problem immediate consequence
decidability results ALC(D) given Baader Hanschke (1991). Since B(D)
simple DL contain concept constructors requiring generation
abstract individuals, easy see B(D)-concept description C0 satisfiable relative
TBox C1 v D1 , . . . , Cn v Dn iff satisfiable one-element interpretation.
TBox internalized simple way: C0 satisfiable relative
TBox C1 v D1 , . . . , Cn v Dn iff C0 u (C1 D1 ) u . . . u (Cn Dn ) satisfiable.

Given theorem, Corollary 22 yields following transfer result, shows
concrete domains restricted form predicate restrictions introduced
integrated local DL decidable relativized satisfiability problem without
losing decidability.
Corollary 43. Let admissible concrete domain L local DL
relativized satisfiability concept descriptions decidable. Then, relativized satisfiability
concept descriptions B(D) L also decidable.
5.4 Non-local DLs
Proposition 15, DLs allowing nominals, universal role, role negation
local. follows decidability transfer theorems applicable fusions
DLs. following, try clarify reasons restricted applicability
theorems.
First, show DLs decidable satisfiability problem
fusion undecidable satisfiability problem. culprit case universal
role (or role negation).
Theorem 44. Satisfiability concept descriptions decidable ALC U ALCF,
undecidable fusion ALC U ALCF.
Proof. Decidability ALCF shown Hollunder Nutt (1990) ALC U
Baader et al. (1990) Goranko Passy (1992). Undecidability ALC U ALCF
(which identical ALCF U ) follows results Baader et al. (1993) fact
universal role used simulate TBoxes (see Proposition 24).

Note role negation used simulate universal role: replace U.C
R.C u R.C U.C R.C R.C. addition, decidability ALC known
decidable (Lutz & Sattler, 2000). Consequently, theorem also holds replace
ALC U ALC .
noted example given theorem depends fact
one two DLs allows universal role becomes undecidable
universal role added. fact, Corollary 25 shows decidability transfer
DLs already provide universal role.
38

fiFusions Description Logics Abstract Description Systems

Concerning nominals, counterexample transfer decidability
presence. However, think unlikely general
transfer result case. fact, note DL L without nominals introduced
Section 2, fusion ALCO identical L extended nominals. Since (relativized)
satisfiability ALCO decidable, general transfer result case would imply
extension decidable provided L decidable. Consequently, would yield
general transfer result adding nominals.

6. Conclusion
Regarding related work, work closely related one presented
(Wolter, 1998). There, analogs Theorems 20 30 proved normal modal
logics within algebraic framework. present results extend ones Wolter
(1998) two directions. First, added object assertions, thus also prove
transfer results ABox reasoning. Second, show transfer results satisfiability
non-normal modal logics long covering normal terms. allows us handle
non-normal concept constructors like qualified number restrictions (graded modalities)
framework.
also think introduction abstract description systems (ADSs) contribution right. ADSs abstract internal structure concept constructors
thus allow us treat vast range constructors uniform way. Nevertheless, model theoretic semantics provided ADSs less abstract algebraic
semantics employed Wolter (1998). closer usual semantics DLs, thus
easier comprehend people used semantics. results paper show
ADSs fact yield good level abstraction proving general results description logics. Recently, notion used proving general results
so-called E-connections representation formalisms like description logics, modal spatial
logics, temporal logics (Kutz, Wolter, & Zakharyaschev, 2001). contrast fusions,
E-connection two domains merged connected means relations.
Regarding complexity, transfer results yield upper bounds. Basically,
show complexity algorithm fusion one exponent higher
ones components. believe complexity satisfiability
fusion ADSs indeed exponentially higher complexity satisfiability
component ADSs. However, yet matching lower bounds, i.e., know
example exponential increase complexity really happens.
Note Spaans results (1993) transfer NP PSpace decidability
component modal logics fusion restricted normal modal logics,
make additional assumptions algorithms used solve satisfiability problem
component logics. Nevertheless, many PSpace-complete description logics
easy see fusion also PSpace-complete. sense, general techniques
reasoning fusion descriptions logics developed paper give rough
complexity estimate.

39

fiBaader, Lutz, Sturm, & Wolter

Appendix A. Proofs
appendix, give detailed proofs criteria (relativized) satisfiability
fusion local ADSs. Recall that, criteria, transfer theorems decidability
easily follow. deferred proofs theorems appendix since
rather technical.
A.1 Proof Theorem 20
prove theorem, need technical lemma. proof Theorem 20,
going merge models W1 M1 W2 M2 means bijective function b
domain W1 W1 onto domain W2 W2 way surrogates
suri (t), C 1 (), respected b sense
1

w sur1 (t)W1 ,A b(w) sur2 (t)W2 ,A

2

w W1 C 1 (). existence bijection equivalent condi1
1
2
2
tion cardinalities |sur1 (t)W1 ,A | sur1 (t)W1 ,A |sur2 (t)W2 ,A | sur2 (t)W2 ,A
coincide C 1 (): 6= t0 t, t0 C 1 (), contains conjunct

(equivalent to) negation conjunct t0 ; hence, t, t0 , suri (t)Wi ,A

suri (t0 )Wi ,A = {1, 2}, clearly yields equivalence. following
lemma used choose models way cardinality condition satisfied.
(We refer reader to, e.g., Gratzer, 1979 information cardinals.)
Lemma 45. Let (L, M) local ADS set assertions satisfiable (L, M).

exists aff cardinal that, cardinals 0 , exists model


W = W, F W , RW |W | = 0 assignment hW, Ai |=
|sW,A | {0, 0 } terms s.


ff
Proof. assumption, exists ADM W0 = W0 , F W0 , RW0 assignment B = hB1 , B2 hW0 , Bi |= . Let = max{0 , |W0 |}.
show
Let 0 . Take 0 disjoint isomorphic copies hW , B1 i,

isWas required.
ff
W
W = W , F , R
, < 0 , ffof hW0 , B1 i. (The first member list coincides
W0 .) Let W = W, F W , RW disjoint union W , < 0 , define
hW, = hA1 , A2 ii putting A2 (a) = B2 (a), X ,
[
A1 (x) =
B1 (x),
<0

x V . Note object variables interpreted W0 . follows
definitions term semantics disjoint unions
[
sW,A =
sW ,B ,
()
<0

terms s. Hence |W | = 0 hW, Ai |= . remains show |sW,A | {0, 0 }
every term s. Suppose |sW,A | =
6 0. Then, (), 0 |sW,A | 0 = 0 , means
0
W,A
= |s
|.

40

fiFusions Description Logics Abstract Description Systems





noted above, disjointness sets suri (t)Wi ,A suri (t0 )Wi ,A (for 6= t0 )
required order ensure existence bijection b. precisely, order

merge models W1 , W2 , sets suri (t)Wi ,A member relevant subset C 1 ()
must form partition Wi domain satisfies certain cardinality condition.
formalized following definition:
Definition 46. Let cardinal. set {X1 , . . . , Xn } called -partition set W
iff
1. |Xi | = , 1 n,
2. Xi Xj = whenever 6= j,

3. W = 1in Xi .
{X1 , . . . , Xn } -partition ADM W domain W iff -partition W .
proof, enforce Properties 1 3 hold appropriate constructions,
Property 2 holds definition C 1 ().
proving Theorem 20, repeat formulation.
Theorem 20. Let Si = (Li , Mi ), {1, 2}, two local ADSs L1 based
set function symbols F relation symbols R, L2 based G Q,
let L = L1 L2 = M1 M2 . finite set assertions L,
following equivalent:
1. satisfiable (L, M).
2. exist
(a) set C 1 (),
(b) every term object variable 6 obj(),
(c) every obj() term ta D,
union 1 following sets assertions L1 satisfiable
(L1 , M1 ):
W
(d) {at : sur1 (t) | D} {> v sur1 ( D)},
(e) {a : sur1 (ta ) | obj()},

(f ) {R(a, b) | R(a, b) , R R},
(g) {sur1 (t1 ) v sur1 (t2 ) | t1 v t2 } {a : sur1 (s) | (a : s) };
union 2 following sets assertions L2 satisfiable (L2 , M2 ):
W
(h) {at : sur2 (t) | D} {> v sur2 ( D)},
(i) {a : sur2 (ta ) | obj()},

(j) {Q(a, b) | Q(a, b) , Q Q}.
41

fiBaader, Lutz, Sturm, & Wolter

sur1 (s1 )W1 ,A

b

1

b

W1 ,A1

sur1 (s2 )

sur2 (s1 )W2 ,A

2

sur2 (s2 )W2 ,A

2

.
.
.

.
.
.

sur1 (sk )W1 ,A

.
.
.

b

1

sur2 (sk )W2 ,A

W1

2

W2
Figure 3: mapping b.

Proof. start direction (2) (1). Take set C 1 () satisfying
properties listed theorem. Take

cardinals

i1, 1ffff{1, 2}
Lemma

245 2for
ffff
1
2
(Li , Mi ), put = max{
1 , 2 },
take W1 , = A1 , A2 W2 , = A1 , A2
ff
Wi Mi Wi , Ai |= {1, 2}. Lemma 45, {1, 2}

assume |Wi | = and, |suri (s)Wi ,A | {0, } D.

sets {suri (s)Wi ,A : D}
-partitions
WiWfor {0, 1} since (i)
ff

D, (as : suri (s)) , (ii) Wi , |= > v suri ( D), (iii) s, s0 6= s0


implies suri (s)Wi ,A suri (s0 )Wi ,A definition C 1 . Moreover, obj(1 ) = obj(2 )
1
2
and, obj(1 ) D, A12 (a) sur1 (s)W1 ,A iff A22 (a) sur2 (s)W2 ,A .
Together fact A12 A22 injective, implies existence
bijection b W1 onto W2
1

2

{b(w) : w sur1 (t)W1 ,A } = sur2 (t)W2 ,A ,
D,

b(A12 (a)) = A22 (a),

obj(1 ). Figure 3, assumed = {s1 , . . . , sk }, illustrates
mapping b.


ff
Define model W = W, (F G)W , (R Q)W putting
W = W1 ,
f W = f W1 , f F,
g G arity n Z1 , . . . , Zn W ,
g W (Z1 , . . . , Zn ) = b1 (g W2 (b(Z1 ), . . . , b(Zn ))),
b(Z) = {b(z) : z Z},
RW = RW1 , R R,
QW (x, y) iff QW2 (b(x), b(y)), Q Q.
42

fiFusions Description Logics Abstract Description Systems

Since M2 closed isomorphic copies, hard see W M1 M2 . Let
= A1 . prove implication (2) (1) theorem remains show
hW, Ai |= . end suffices prove following claim:
Claim. terms sub1 (),
2

1

tW,A = sur1 (t)W1 ,A = b1 (sur2 (t)W2 ,A ).
prove claim, let us show implies hW, Ai |= . First note that,
claim, obtain
1
tW,A = sur1 (t)W1 ,A term().
(1)
may proved induction construction term() terms sub1 ()
using booleans function symbols L1 , only. basis induction (i.e.,
equality members sub1 ()) stated claim induction step straightforward.
show hW, Ai |= consequence (1). Suppose R(a, b) .
R(a, b) 1 thus hW, Ai |= R(a, b). Similarly, Q(a, b) implies Q(a, b) 2
1
hW, Ai |= Q(a, b). Suppose (a : t) . (a : sur1 (t)) 1 A12 (a) sur1 (t)W1 ,A
implies, (1), A12 (a) tW,A . Hence hW, Ai |= (a : t). t1 v t2 ,
sur1 (t1 ) v sur1 (t2 ) 1 so, (1), tW,A
tW,A
. Hence hW, Ai |= t1 v t2 .
1
2
come proof claim. proved induction structure t.
Due following equalities holding sub1 (), suffices show tW,A =
1
sur1 (t)W1 ,A .
sur1 (t)W1 ,A

1

1

=

[

{sur1 (s)W1 ,A : D, conjunct s}

=

[

{b1 (sur2 (s)W2 ,A ) : D, conjunct s}

2

2

= b1 (sur2 (t)W2 ,A )
W
1
first equality holds since sur1 ( D)W1 ,A = W1 and, D, either
conjunct s. second equality true definition b validity
thirdWequality seen analogously validity first one considering
2
sur2 ( D)W2 ,A = W2 .
1

Hence let us show tW,A = sur1 (t)W1 ,A . induction start, let variable.
1
equation tW,A = sur1 (t)W1 ,A immediate consequence fact = A1 .
induction step, distinguish several cases:

= t1 . induction hypothesis, tW,A
= sur1 (t1 )W1 ,A1 . Hence, tW,A = W \ tW,A
=
1
1
1
1
W
,A
W
,A
1
1
W \ sur1 (t1 )
= sur1 (t)
(since W = W1 ).
= t1 t2 . induction hypothesis, tW,A
= sur1 (ti )W1 ,A1 {1, 2}. Hence,

1
1
1
tW,A = tW,A
tW,A
= sur1 (t1 )W1 ,A sur1 (t2 )W1 ,A = sur1 (t)W1 ,A .
1
2
= t1 t2 . Similar case.
43

fiBaader, Lutz, Sturm, & Wolter

1

= f (t1 , . . . , tn ). induction hypothesis, tW,A
= sur1 (ti )W1 ,A 1 n. Hence,

1
1
1
W,A
W,A
tW,A = f W (t1 , . . . , tn ) = f W (sur1 (t1 )W1 ,A , . . . , sur1 (tn )W1 ,A ) = sur1 (t)W1 ,A
(since f W = f W1 ).
= g(t1 , . . . , tn ). case, tW,A = b1 (g W2 (b(tW,A
), . . . , b(tW,A
))). Since,
n
1
2
2
1
W
,A
1
W
,A
1
2
equalities, sur1 (t)
= b (sur2 (t)
), remains show sur2 (t)W2 ,A =
2
2
2
g W2 (b(tW,A
), . . . , b(tW,A
)). Since sur2 (t)W2 ,A = g W2 (sur2 (t1 )W2 ,A , . . . , sur2 (tn )W2 ,A ),
n
1
2
amounts showing b(tW,A
) = sur2 (ti )W2 ,A 1 n. This, however,

follows induction hypothesis together equations.
concludes proof direction (2) (1).
remains prove direction (1) (2). Suppose hW, Ai |= , W
= hA1 , A2 i. Put
= {s C 1 () : sW,A 6= }.
Note fusion local ADLs local ADL again. Hence (L, M) local may
assume, Lemma 45, sets sW,A infinite.
Take new object name 6 obj() every let, obj(),
^
^
ta = {t sub1 () : A2 (a) tW,A } {t : sub1 (), A2 (a) 6 tW,A }.
prove set assertions 1 based D, ta , obj(), , D, satisfiable
(L1 , M1 ).
W
Let F W denote restriction (F G)W
symbols F.
R
ff Similarly,

1
ff
W
W
W
1
restriction (RQ) symbols R. Set W1 = W, F , R
M1 , = A1 , A12 ,

A11 = A1 {xt 7 tW,A : = g(t1 , . . . , tk ) sub1 ()},
A12 (a) = A2 (a), obj(), A12 (as ) sW,A , D. Note choose
injective function A12 sW,A infinite. show induction
sur1 (t)W1 ,A1 = tW,A term().

(2)

Let = x variable. x surrogate, A11 (x) = A1 (x). induction
step, distinguish several cases:
inductive steps = t1 , = t1 t2 , = t1 t2 , = f (t1 , . . . , tn ), f F,
identical corresponding cases proof Equation 1, occurs
direction (2) implies (1) above.
= g(t1 , . . . , tn ), g G. sur1 (t) = xt . Hence A11 (xt ) = tW,A
equation proved.


ff


ff
Equation 2,
obtain
W1 , A1 |= 1 : prove W1 , A1 |= R(a, b) whenever
ff
R(a, b) 1 W1 , A1 |= sur1 (t1 ) v sur1 (t2 ) whenever sur1 (t1 ) v sur1 (t2 ) 1 .
remaining formulas 1 left

theffreader. Suppose R(a, b) 1 . R(a, b)
hW, Ai |= R(a, b). Hence W1 , A1 |= R(a, b). Suppose sur1 (t1 ) v sur1 (t2 ) 1 .
44

fiFusions Description Logics Abstract Description Systems

t2W,A . Equation 2,
t1 v t2 . Hence hW, Ai |= t1 v t2 means tW,A
1


ff
1
1
sur1 (t1 )W1 ,A sur1 (t2 )W1 ,A means W1 , A1 |= sur1 (t1 ) v sur1 (t2 ).
construction model M2 satisfying 2 similar left reader.

A.2 Proof Theorem 30
proof Theorem 17, fix two local ADSs Si = (Li , Mi ), {1, 2},
L1 based set function symbols F relation symbols R, L2 based
G Q. Let L = L1 L2 = M1 M2 . assume S1 S2 covering
normal terms.
Similarly done previous section, merge models means
1
bijections map points sets sur1 (t)W1 ,A points corresponding sets
2
sur2 (t)W2 ,A . finite set object assertions L, let () denote set
C () term satisfiable (L, M) (for {1, 2}). ensure
merging models succeeds, must enforce elements 1 () 2 ()
form -partitions (for appropriate ) models merged. 1 (),
captured following lemma. Explicitly stating dual lemma 2 ()
omitted brevity.
Lemma 47. Let finite set object assertions L, cardinal satisfying
conditions Lemma 45 (L, M) , 1 = 1 (). 0 ,
1. exists model W M1 assignment
{sur1 (s)W,A | 1 }
0 -partition W;
2. exists model W M2 assignment
{sur2 (s)W,A | 1 }
0 -partition W.
Proof. 1. definition 1 , 1 , find model Ws
assignment sWs ,As 6= . Since fusion two local ADSs local,
set models closed disjoint unions. Hence, exists model W1
assignment A1 sW1 ,A1 6= 1 . follows set
1 := D{as : | 1 } satisfiable
E (L, M). Lemma 45, thus exists model
0
0
0
0
W
W
W = W , (F G) , (R Q)
assignment A0 W0 , A0 |= 1
0

0

{sW ,A | 1 } 0 -partition W 0 . let W denote restriction W0 L1
define
0
0
A1 = A01 {xt 7 tW ,A | = g(t1 , . . . , tk ) sub1 ()}.
0

0

hW, Ai required. prove note sur1 (t)W,A = tW ,A term().
2. similar left reader.

45

fiBaader, Lutz, Sturm, & Wolter

repeat formulation theorem proved.
Theorem 30. Let Si = (Li , Mi ), {1, 2}, two local ADSs covering normal
terms L1 based set function symbols F relation symbols R,
L2 based G Q, let L = L1 L2 = M1 M2 . Let finite set
object assertions L. Put := dF (), r := dG (), let c(x) (d(x)) covering
normal term function symbols F (G).
{1, 2}, denote set C () term satisfiable
(L, M). following three conditions equivalent:
1. satisfiable (L, M).
2. exist
every 1 object variable 6 obj()
every obj() term ta 1
union 1 following sets object assertions satisfiable
(L1 , M1 ):
W
{at : sur1 (t cm (sur1 ( 1 )) | 1 },
W
{a : sur1 (ta cm (sur1 ( 1 )) | obj()},
{R(a, b) | R(a, b) , R R},
{a : sur1 (s) | (a : s) };
union 2 following sets object assertions satisfiable (L2 , M2 ):
W
{at : sur2 (t dr (sur2 ( 1 )) | 1 },
W
{a : sur2 (ta dr (sur2 ( 1 )) | obj()},
{Q(a, b) | Q(a, b) , Q Q}.

3. condition (2) above, 1 replaced 2 .
start proof direction (1) (2) (1) (3). proofs
dual
onlyff give proof (1) (2). Suppose hW, Ai |= ,

other,
W
W = W, (F G) , (R Q)W . Lemma 45, assume that, every 1 ,
|tW,A | infinite. Take new object name 6 obj() every 1 let,
obj(),
^
^
ta = {t sub1 () : A2 (a) tW,A } {t : sub1 (), A2 (a) 6 tW,A }.
prove set 1 assertions based ta , obj(), , 1 , satisfiable
(L1 , M1 ) (the proof rather similar proof direction (1) (2)
proof Theorem 20). Let F W (resp. G W ) denote restriction (F G)W symbols
F (resp. G). Similarly, RW
QW theffrestrictions
(R Q)ffW symbols
R Q, respectively. Set W1 = W, F W , RW M1 , A1 = A11 , A12 ,
A11 = A1 {xt 7 tW,A | = g(t1 , . . . , tk ) sub1 ()},
46

fiFusions Description Logics Abstract Description Systems

A12 (a) = A2 (a), obj(), A12 (at ) tW,A , 1 (we choose injective
function A12 since sets tW,A infinite).
corresponding part proof Theorem 20, show induction
sur1 (t)W1 ,A1 = tW,A term().


ff
Let us see W1 , A1 |= 1 follows
R(a, b) 1

equation.
ff
1 |= R(a, b). hW, Ai |=
R(a,
b)




hW,
Ai
|=
R(a,
b).
Hence
W
,


1 1ff
W
W
( 1 ) = > (by definition


).
Hence
W1 , |= sur1 ( 1 ) = > so,
1
ff


W
definition cm , W1 , A1 |= (cm (sur1 ( 1 ))) = >. remains observe
A12 (a) sur1 (ta )W1 ,A1 obj(), A12 (a) sur1 (s)W1 ,A1 whenever (a : s) ,
A12 (at ) sur1 (t)W1 ,A1 1 .
construction model M2 satisfying 2 similar left reader.
remains show implications (2) (1) (3) (1). similar,
concentrate first. proof Theorem 20 possible construct
required model merging models 1 2 . situation different here.
possible W
merge models 1
W 2 one step, since know whether
satisfy sur1 ( 1 ) = > sur2 ( 1 ) = >,
W respectively. know
W
satisfy approximations : sur1 (s) cm (sur1 ( 1 )) : sur2 (s) dr (sur2 ( 1 )),
respectively, : . merge models type distinguish various
pieces models add new pieces well. define pieces need
technical claim. proof Theorem 17, take cardinals , {1, 2} Lemma 45
(Li , Mi ) put = max{1 , 2 }.
Claim 1. Suppose (2) holds.


ff
(a) exist W1 = W1 , F W , RW M1 , assignment = hA1 , A2 W1 ,
sequence X0 , . . . , Xm subsets W1
[a1] A2 (a) Xm , obj(1 ),
[a2] hW1 , Ai |= 1 ,
[a3] Xn+1 Xn cW1 (Xn ), 0 n < m,
[a4] set {sur1 (s)W1 ,A Xm : 1 } -partition Xm ,
[a5] sets
{sur1 (s)W1 ,A (Xn Xn+1 ) : 1 }
-partitions Xn Xn+1 , 0 n < m.
[a6] |W1 X0 | = .


ff
(b) exist W2 = W2 , G W , QW M2 , assignment B = hB1 , B2 i, sequence
Y0 , . . . , Yr subsets W2
[b1] B2 (a) Yr , obj(1 ),
[b2] hW2 , Bi |= 2 ,
47

fiBaader, Lutz, Sturm, & Wolter

A1 = W1 X0
A0 = X 0 X 1

..
.

..
.

Am2 = Xm2 Xm1
Am1 = Xm1 Xm
Xm

W1
Figure 4: sets Xi .

[b3] Yn+1 Yn dW2 (Yn ), 0 n < r,
[b4] set {sur2 (s)M,A Yr : 1 } -partition Yr ,
[b5] sets
{sur2 (s)M,A (Yn Yn+1 ) : 1 }
-partitions Yn Yn+1 , 0 n < r.
[b6] |W2 Y0 | = .
Figure 4 illustrates relation sets Xi . (We set Ai = Xi Xi+1 0 <
A1 = W
W1 X0 .) Intuitively, Xm set points know points
W1 sur1 ( 1 )W1 ,A far away. Xm1 possibly less far away,
Xm2 possibly even less far, W
Xi , < 1. Finally, members A1
even known whether sur1 ( 1 )W1 ,A not. Note object names
interpreted Xm . come formal construction sets Xi .
Proof Claim 1. prove (a). Part (b) proved
andffleft reader.

similarly
W

assumption Lemma 45, find ADM Wa = Wa , F , RWa M1 |Wa | =
assignment Aa = hAa1 , Aa2 hWa , Aa |= 1 .
Let
_
Zn = (cn (sur1 ( 1 )))Wa ,Aa ,
0
n m. Lemma
47 (1) take every n 1 n ADM
ff
Wn = Wn , F Wn , RWn M1 assignments
n

{sur1 (s)Wn ,A : 1 }
48

fiFusions Description Logics Abstract Description Systems

-partitions Wn .
ff


Take disjoint union W (with W = W, F W , RW ) Wn , 1 n m, Wa .
Define = hA1 , A2 W putting
[
A1 (x) = Aa1 (x)
Ai1 (x),
1im

set variables x A2 (b) = Aa2 (b), object variables b. Let, 0 n m,
[
Xn = Zn
Wi .
nim

show hW, Ai sets Xn , 0 n m, required.
[a1] hWa , Aa |= 1 A2 (b) = Aa2 (b) Zm b obj(1 ). Hence
A2 (b) Xm = Zm Wm b obj(1 ).
[a2] definition disjoint unions hWa , Aa |= 1 .
[a3] Firstly, have, definition cn since cW monotone (it distributes
intersections),
Zn+1 Zn cW (Zn ) Xn cW (Xn ).
(3)
Secondly, definition disjoint unions, first property covering normal
terms, since cW monotone
[
[
[
[
Wi
Wi
Wi cW (
Wi ) Xn cW Xn .
(4)
n+1im

nim

nim

nim

(3) (4) obtain
Xn+1 = Zn+1

[

Wi Xn cW Xn .

(5)

n+1im

[a4] show three properties Definition 46 satisfied. Since
{sur1 (s)Wm ,Am : 1 }
-partition Wm , |sur1 (s)Wm ,Am | = 1 . implies
Property 1 since sur1 (s)W,A Wm = sur1 (s)Wm ,Am , Wm Xm , |Xm | .
Property 2 immediate consequence definition 1 . Property 3,
show that, w Xm , w sW,A 1 . Fix w Xm .
distinguish two cases: firstly, assume w Wm . Then, fact {sur1 (s)Wm ,Am :
1 } -partition Wm , clear
W exists 1 required.
(sur ( )))Wa ,Aa . definition cm t,
Secondly, assume
w

Z
=
(c

1
1
W
w (sur1 ( 1 ))Wa ,Aa w sur1 (s)W,A 1 .
[a5] proof similar Property [a4].
49

fiBaader, Lutz, Sturm, & Wolter

[a6] definition.
finishes proof Claim 1.
Suppose
E

E

W1 = W1 , F W1 , RW1 , A, Xm , . . . , X0 W2 = W2 , G W2 , QW2 , B, Yr , . . . , Y0
satisfying properties listed Claim 1. may assume
(W1 Xm ) (W2 Yr ) = .
Using appropriate bijection b Xm onto Yr may also assume Xm = Yr ,
A2 (a) = B2 (a) object variables obj(1 ),
sur1 (s)W1 ,A Xm = sur2 (s)W2 ,B Xm 1 .

(6)

follows fact object variables mapped A2 B2 Xm
Yr ([a1], [b1]), respectively, injectivity mappings A2 B2 , conditions
[a4] [b4] state {sur1 (s)W1 ,A Xm : 1 } {sur2 (s)W2 ,B Yr : 1 }
form -partitions Xm = Yr . abbreviations useful: set
Ai = Xi Xi+1 , 0 < m,
Bi = Yi Yi+1 , 0 < r,
A1 = W1 X0 , B1 = W2 Y0 .
far merged Xm -part W1 Yr -part W2 . remains take care
sets Ai , 1 < m, Bi , 1 < r: sets Ai merged new
models Wi M2 sets Bi merged new models Vi M1 . Thus,
final model obtained merging disjoint union W1 Wi , 1 <
disjoint union W2 Vi , 1 < r. Figure 5 illustrates merging.
figure, assume 1 = {s1 , . . . , sk }.
course, merging Ai , 0, new model Wi respect partition
{sur1 (t)W1 ,A Ai | 1 }
Ai . merging Bi , 0, new model Vi respect partition
{sur1 (t)W1 ,B Bi | 1 }
Bi . Note A1 B1 partition
care
take
E of. proceed



W
W
formal construction. find models W = Ai , G , Q
M2 assignments

iff

B = B1 , B2 , 1 1, that, 0 1,




sur2 (s)W ,B = sur1 (s)W1 ,A Ai 1 .
follows [a5], [a6], Lemma 47 (2).
50

(7)

fiFusions Description Logics Abstract Description Systems

Xm Am1

...

A0 A1 Vr1

...

V0 V1

sur1 (s1 )

.
.
.

...

...

...
Wm1 . . .

...
...

sur1 (sk )

Yr

W0 W1 Br1

B0 B1

sur2 (s1 )

.
.
.

...

.. .

sur2 (sk )
Figure 5: bijection.


E


find, using [b5], [b6], Lemma 47 (1), models Vi = Bi , F V , RV M1


ff
assignments Ai = Ai1 , Ai2 , 1 r 1, that, 0 r 1,




sur1 (s)V ,A = sur2 (s)W2 ,B Bi 1 .
Let

(8)


E
0
0
W01 = W1 (W2 Yr ), F W1 , RW1 M1

disjoint union Vi , 1 < r, W1 , let

E
0
0
W02 = W2 (W1 Xm ), G W2 , QW2 M2
disjoint union Wi , 1 < m, W2 . assume Xm = Yr
domain ADMs
W1 W2 .
ff
Define model W = W, (F G)W , (R Q)W based W = W1 W2 putting
0

RW = RW1 ,
0

F W = F W1 ,
0

QW = QW2 ,
0

G W = G W2 .
51

fiBaader, Lutz, Sturm, & Wolter

Define assignment C = hC1 , C2 W putting
C2 (a) = A2 (a)(= B2 (a)), obj(1 ).

C1 (x) = A1 (x) 1i<r Ai1 (x), set variables x term().

Notice C1 (x) = B1 (x) 1i<m B1i (x), set variables x term().
C1 (xt ) = A1 (xt )



C1 (xt ) = B1 (xt )



1i<r

Ai1 (xt ), = g(t1 , . . . , tk ) sub1 ().


1i<m B1 (xt ),

= f (t1 , . . . , tk ) sub1 ().

show hW, Ci |= . Firstly, however, make list relevant properties
hW, Ci:
Claim 2.
[c1] C2 (a) Xm = Yr , obj();
[c2] hW, Ci |= 1 2 ;
[c3] sur1 (t)W,C (X0 Y0 ) = sur2 (t)W,C (X0 Y0 ), 1 ;
[c4] sur1 (s)W,C (X0 Y0 ) = sur2 (s)W,C (X0 Y0 ), sub1 ();
[c5] Xn+1 Xn cW (Xn ), 0 n < m;
[c6] Yn+1 Yn dW (Yn ), 0 n < r;
[c7] g G arity l, 0 n < m, C1 , . . . , Cl W :
g W (C1 , . . . , Cl ) Xn = g W (C1 Xn , . . . , Cl Xn ) Xn ;
[c8] f F arity l, 0 n < r, C1 , . . . , Cl W :
f W (C1 , . . . , Cl ) Yn = f W (C1 Yn , . . . , Cl Yn ) Yn .
Proof Claim 2. [c1] follows [a1] [b1] construction hW, Ci. [c2] follows
[a2] [b2]. [c3] follows construction hW, Ci equations (6), (7),
(8). [c4] follows [c3]. [c5] [c6] follow [a3]

[b3], ffrespectively. remains
prove [c7] [c8]. [c7] follows fact W, GW disjoint union
structures based Xn W Xn , 0 n < m, [c8] dual [c7]. Claim 2
proved.
show hW, Ci |= . end first show following:
Claim 3. k1 , k2 0 k1 0 k2 r sub1 () dF (s) k1
dG (s) k2 have, Z {Xk1 , Yk2 },
Z sM,C = Z sur1 (s)M,C = Z sur2 (s)M,C .
52

fiFusions Description Logics Abstract Description Systems

Proof Claim 3. [c4] suffices prove first equation. proof induction
cardinal k1 + k2 . induction base k1 = k2 = 0 follows sur1 (s) = sur2 (s)
dF (s) = dG (s) = 0.
Suppose claim proved Xk , Yk0 k m, k 0 r k + k 0 < k1 + k2 .
prove claim Xk1 , Yk2 . proof induction construction terms
dF (s) k1 dG (s) k2 . boolean cases trivial.
Suppose = f (s1 , . . . , sl ) dF (s) k1 dG (s) k2 . show
following two statements:
(i) Xk1 sW,C = Xk1 sur1 (s)M,C .
(ii) Yk2 sW,C = Yk2 sur1 (s)M,C .
Consider (i) first. induction hypothesis yields
Xk1 1 sW,C
= Xk1 1 sur1 (si )W,C

1 l.
Xk1 1 cW (Xk1 1 ) sW,C = Xk1 1 cW (Xk1 1 ) f W (s1W,C , . . . , slW,C )
= Xk1 1 cW (Xk1 1 ) f W (sur1 (s1 )W,C , . . . , sur1 (sl )W,C )
= Xk1 1 cW (Xk1 1 ) sur1 (s)W,C .
second equation immediate consequence third property covering normal
terms given Definition 26. equation follows [c5], i.e. Xk1 Xk1 1
cW (Xk1 1 ). (i) proved.
(ii) Suppose first k2 = r. Yk2 = Xm claim proved
since Xm Xk1 and, induction hypothesis, Xk1 1 sW,C
= Xk1 1 sur1 (si )W,C ,

1 l.
Assume k2 < r. induction hypothesis,
Yk2 sW,C
= Yk2 sur2 (si )W,C ,

1 l. Hence
, . . . , Yk2 sW,C
) = f W (Yk2 sur2 (s1 )W,C , . . . , Yk2 sur2 (sl )W,C ).
f W (Yk2 sW,C
1
l
intersect sides equation Yk2 derive help [c8]:
Yk2 f W (sW,C
, . . . , sW,C
) = Yk2 f W (sur2 (s1 )W,C , . . . , sur2 (sl )W,C ).
1
l
means Yk2 sW,C = Yk2 sur2 (s)W,C , equation follows. statements
proved.
case = g(s1 , . . . , sl ) dual left reader. proved claim 3.
induction (c.f. proof Theorem 20 proof (1) corresponding
claim), obtain Claim 3:
Xm sW,C = Xm sur1 (s)M,C term().
53

(9)

fiBaader, Lutz, Sturm, & Wolter

Let us see hW, Ai |= follows (9). distinguish three cases: Suppose R(a, b)
. R(a, b) 1 therefore hW, Ci |= R(a, b). Similarly, Q(a, b) implies
Q(a, b) 2 hW, Ci |= Q(a, b). Suppose (a : t) . (a : sur1 (t)) 1 so,
[c2], C2 (a) sur1 (t)W,C implies, (9), C2 (a) tW,C . Hence hW, Ci |= (a : t).
finishes proof Theorem 30.

References
Areces, C., Blackburn, P., & Marx, M. (2000). computational complexity hybrid
temporal logics. Logic Journal IGPL, 8 (5), 653679.
Baader, F. (1991). Augmenting concept languages transitive closure roles: alternative terminological cycles. Proc. 12th Int. Joint Conf. Artificial
Intelligence (IJCAI91).
Baader, F., Burckert, H.-J., Hollunder, B., Nutt, W., & Siekmann, J. H. (1990). Concept
logics. Lloyd, J. W. (Ed.), Computational Logics, Symposium Proceedings, pp.
177201. Springer-Verlag.
Baader, F., Burckert, H.-J., Nebel, B., Nutt, W., & Smolka, G. (1993). expressivity
feature logics negation, functional uncertainty, sort equations. J. Logic,
Language Information, 2, 118.
Baader, F., & Hanschke, P. (1991). schema integrating concrete domains concept
languages. Proc. 12th Int. Joint Conf. Artificial Intelligence (IJCAI91),
pp. 452457.
Baader, F., & Hanschke, P. (1992). Extensions concept languages mechanical engineering application. Proc. 16th German Workshop Artificial Intelligence
(GWAI92), Vol. 671 Lecture Notes Computer Science, pp. 132143, Bonn (Germany). Springer-Verlag.
Baader, F., & Hollunder, B. (1991). terminological knowledge representation system
complete inference algorithm. Proc. Workshop Processing Declarative
Knowledge (PDK91), Vol. 567 Lecture Notes Artificial Intelligence, pp. 6786.
Springer-Verlag.
Baader, F., & Sattler, U. (1999). Expressive number restrictions description logics. J.
Logic Computation, 9 (3), 319350.
Ben-Ari, M., Halpern, J. Y., & Pnueli, A. (1982). Deterministic propositional dynamic logic:
Finite models, complexity, completeness. J. Computer System Sciences,
25, 402417.
Berger, R. (1966). undecidability dominoe problem. Mem. Amer. Math. Soc.,
66, 172.
Borgida, A. (1995). Description logics data management. IEEE Trans. Knowledge
Data Engineering, 7 (5), 671682.
Brachman, R. J., McGuinness, D. L., Patel-Schneider, P. F., Alperin Resnick, L., & Borgida,
A. (1991). Living CLASSIC: use KL-ONE-like language.
54

fiFusions Description Logics Abstract Description Systems

Sowa, J. F. (Ed.), Principles Semantic Networks, pp. 401456. Morgan Kaufmann,
Los Altos.
Brachman, R. J., & Schmolze, J. G. (1985). overview KL-ONE knowledge representation system. Cognitive Science, 9 (2), 171216.
Bresciani, P., Franconi, E., & Tessaris, S. (1995). Implementing testing expressive description logics: Preliminary report. Proc. 1995 Description Logic Workshop
(DL95), pp. 131139.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1999). Reasoning expressive description
logics fixpoints based automata infinite trees. Proc. 16th Int.
Joint Conf. Artificial Intelligence (IJCAI99), pp. 8489.
Calvanese, D., De Giacomo, G., & Rosati, R. (1998). note encoding inverse roles
functional restrictions ALC knowledge bases. Proc. 1998 Description Logic
Workshop (DL98), pp. 6971. CEUR Electronic Workshop Proceedings, http://ceurws.org/Vol-11/.
Chellas, B. F. (1980). Modal logic. Cambridge University Press, Cambridge, UK.
Danecki, R. (1984). Nondeterministic Propositional Dynamic Logic intersection
decidable. Proc. 5th Symp. Computation Theory, Vol. 208 Lecture
Notes Computer Science, pp. 3453. Springer-Verlag.
De Giacomo, G. (1995). Decidability Class-Based Knowledge Representation Formalisms.
Ph.D. thesis, Dipartimento di Informatica e Sistemistica, Universita di Roma La
Sapienza.
De Giacomo, G., & Lenzerini, M. (1994a). Boosting correspondence description
logics propositional dynamic logics. Proc. 12th Nat. Conf. Artificial
Intelligence (AAAI94), pp. 205212. AAAI Press/The MIT Press.
De Giacomo, G., & Lenzerini, M. (1994b). Concept language number restrictions
fixpoints, relationship -calculus. Proc. 11th Eur. Conf.
Artificial Intelligence (ECAI94), pp. 411415.
De Giacomo, G., & Lenzerini, M. (1995). Whats aggregate: Foundations description logics tuples sets. Proc. 14th Int. Joint Conf. Artificial
Intelligence (IJCAI95), pp. 801807.
Donini, F. M., Hollunder, B., Lenzerini, M., Spaccamela, A. M., Nardi, D., & Nutt, W.
(1992). complexity existential quantification concept languages. Artificial
Intelligence, 23, 309327.
Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1991). Tractable concept languages.
Proc. 12th Int. Joint Conf. Artificial Intelligence (IJCAI91), pp. 458463,
Sydney (Australia).
Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1997). complexity concept
languages. Information Computation, 134, 158.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1994). Deduction concept languages: subsumption instance checking. J. Logic Computation, 4 (4),
423452.
55

fiBaader, Lutz, Sturm, & Wolter

Dosen, K. (1988). Duality modal algebras neighbourhood frames. Studia
Logica, 48, 219234.
Fine, K., & Schurz, G. (1996). Transfer theorems stratified modal logics. Copeland,
J. (Ed.), Logic Reality: Essays Pure Applied Logic. Memory Arthur
Prior, pp. 169213. Oxford University Press.
Fischer, M. J., & Ladner, R. E. (1979). Propositional dynamic logic regular programs.
J. Computer System Sciences, 18, 194211.
Gabbay, D. M. (1999). Fibring Logics, Vol. 38 Oxford Logic Guides. Clarendon Press,
Oxford.
Gargov, G., & Goranko, V. (1993). Modal logic names. J. Philosophical Logic, 22,
607636.
Goldblatt, R. I. (1989). Varieties complex algebras. Annals Pure Applied Logic,
38, 173241.
Goranko, V., & Passy, S. (1992). Using universal modality: Gains questions. Journal
Logic Computation, 2 (1), 530.
Gratzer, G. (1979). Universal Algebra. Springer-Verlag, New York.
Haarslev, V., Moller, R., & Wessel, M. (2001). description logic ALCN HR+ extended
concrete domains: practically motivated approach. Proceedings International Joint Conference Automated Reasoning IJCAR01, Lecture Notes
Artificial Intelligence. Springer-Verlag.
Harel, D. (1984). Dynamic logic. Handbook Philosophical Logic, Vol. 2, pp. 497640.
D. Reidel, Dordrecht (Holland).
Hollunder, B., & Baader, F. (1991). Qualifying number restrictions concept languages.
Tech. rep. RR-91-03, Deutsches Forschungszentrum fur Kunstliche Intelligenz (DFKI),
Kaiserslautern (Germany). abridged version appeared Proc. 2nd Int.
Conf. Principles Knowledge Representation Reasoning (KR91).
Hollunder, B., & Nutt, W. (1990). Subsumption algorithms concept languages. Tech. rep.
RR-90-04, Deutsches Forschungszentrum fur Kunstliche Intelligenz (DFKI), Kaiserslautern (Germany).
Horrocks, I. (1998). Using expressive description logic: FaCT fiction?. Proc.
6th Int. Conf. Principles Knowledge Representation Reasoning (KR98),
pp. 636647.
Horrocks, I., & Sattler, U. (1999). description logic transitive inverse roles
role hierarchies. J. Logic Computation, 9 (3), 385410.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning expressive description logics. J. Interest Group Pure Applied Logic, 8 (3), 239264.
Jonsson, B., & Tarski, A. (1951). Boolean algebras operators. I. American Journal
Mathematics, 73, 891939.
Jonsson, B., & Tarski, A. (1952). Boolean algebras operators. II. American Journal
Mathematics, 74, 127162.
56

fiFusions Description Logics Abstract Description Systems

Knuth, D. E. (1973). Art Computer Programming, Vol. 3. Addison-Wesley, Mass.
Kracht, M., & Wolter, F. (1991). Properties independently axiomatizable bimodal logics.
Journal Symbolic Logic, 56 (4), 14691485.
Kutz, O., Wolter, F., & Zakharyaschev, M. (2001). Connecting abstract description systems.
Submitted. Available http://www.informatik.uni-leipzig.de/wolter/.
Levesque, H. J., & Brachman, R. J. (1987). Expressiveness tractability knowledge
representation reasoning. Computational Intelligence, 3, 7893.
Lutz, C. (1999). Reasoning concrete domains. Dean, T. (Ed.), Proc. 16th
Int. Joint Conf. Artificial Intelligence (IJCAI99), pp. 9095, Stockholm, Sweden.
Morgan Kaufmann, Los Altos.
Lutz, C. (2001). Interval-based temporal reasoning general TBoxes. Proc.
17th Int. Joint Conf. Artificial Intelligence (IJCAI 2001), pp. 8994.
Lutz, C., & Sattler, U. (2000). Mary likes cats. Proc. 2000 Description
Logic Workshop (DL 2000), pp. 213226. CEUR Electronic Workshop Proceedings,
http://ceur-ws.org/Vol-33/.
Nebel, B. (1988). Computational complexity terminological reasoning BACK. Artificial
Intelligence, 34 (3), 371383.
Nebel, B. (1990). Terminological reasoning inherently intractable. Artificial Intelligence,
43, 235249.
Parikh, R. (1980). Propositional logics programs: Systems, models complexity.
Proc. 7th ACM SIGACT-SIGPLAN Symp. Principles Programming Languages (POPL80), pp. 186192, Las Vegas (USA).
Prior, A. N. (1967). Past, Present Future. Oxford University Press.
Sattler, U. (1996). concept language extended different kinds transitive roles.
Gorz, G., & Holldobler, S. (Eds.), Proc. 20th German Annual Conf. Artificial
Intelligence (KI96), No. 1137 Lecture Notes Artificial Intelligence, pp. 333345.
Springer-Verlag.
Schaerf, A. (1993). complexity instance checking problem concept languages
existential quantification. J. Intelligent Information Systems, 2, 265278.
Schaerf, A. (1994). Reasoning individuals concept languages. Data Knowledge
Engineering, 13 (2), 141176.
Schild, K. (1991). correspondence theory terminological logics: Preliminary report.
Proc. 12th Int. Joint Conf. Artificial Intelligence (IJCAI91), pp. 466471.
Schmidt-Schau, M. (1989). Subsumption KL-ONE undecidable. Brachman, R. J.,
Levesque, H. J., & Reiter, R. (Eds.), Proc. 1st Int. Conf. Principles
Knowledge Representation Reasoning (KR89), pp. 421431. Morgan Kaufmann,
Los Altos.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions complements. Artificial Intelligence, 48 (1), 126.
57

fiBaader, Lutz, Sturm, & Wolter

Spaan, E. (1993). Complexity Modal Logics. Ph.D. thesis, Department Mathematics
Computer Science, University Amsterdam, Netherlands.
Van der Hoek, W., & de Rijke, M. (1995). Counting objects. J. Logic Computation,
5 (3), 325345.
Wolter, F. (1998). Fusions modal logics revisited. Kracht, M., de Rijke, M., Wansing, H., & Zakharyaschev, M. (Eds.), Advances Modal Logic, pp. 361379. CSLI
Publications.
Woods, W. A., & Schmolze, J. G. (1992). KL-ONE family. Lehmann, F. W. (Ed.),
Semantic Networks Artificial Intelligence, pp. 133178. Pergamon Press. Published
special issue Computers & Mathematics Applications, Volume 23, Number
29.

58

fiJournal Artificial Intelligence Research 16 (2002) 321357

Submitted 09/01; published 06/02

SMOTE: Synthetic Minority Over-sampling Technique
Nitesh V. Chawla

chawla@csee.usf.edu

Department Computer Science Engineering, ENB 118
University South Florida
4202 E. Fowler Ave.
Tampa, FL 33620-5399, USA

Kevin W. Bowyer

kwb@cse.nd.edu

Department Computer Science Engineering
384 Fitzpatrick Hall
University Notre Dame
Notre Dame, 46556, USA

Lawrence O. Hall

hall@csee.usf.edu

Department Computer Science Engineering, ENB 118
University South Florida
4202 E. Fowler Ave.
Tampa, FL 33620-5399, USA

W. Philip Kegelmeyer

wpk@california.sandia.gov

Sandia National Laboratories
Biosystems Research Department, P.O. Box 969, MS 9951
Livermore, CA, 94551-0969, USA

Abstract
approach construction classiers imbalanced datasets described.
dataset imbalanced classication categories approximately equally represented. Often real-world data sets predominately composed normal examples
small percentage abnormal interesting examples. also case
cost misclassifying abnormal (interesting) example normal example
often much higher cost reverse error. Under-sampling majority (normal) class proposed good means increasing sensitivity classier
minority class. paper shows combination method over-sampling
minority (abnormal) class under-sampling majority (normal) class achieve
better classier performance (in ROC space) under-sampling majority class.
paper also shows combination method over-sampling minority class
under-sampling majority class achieve better classier performance (in ROC
space) varying loss ratios Ripper class priors Naive Bayes. method
over-sampling minority class involves creating synthetic minority class examples.
Experiments performed using C4.5, Ripper Naive Bayes classier. method
evaluated using area Receiver Operating Characteristic curve (AUC)
ROC convex hull strategy.

1. Introduction
dataset imbalanced classes approximately equally represented. Imbalance
order 100 1 prevalent fraud detection imbalance 100,000
c
2002
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiChawla, Bowyer, Hall & Kegelmeyer

1 reported applications (Provost & Fawcett, 2001).
attempts deal imbalanced datasets domains fraudulent telephone calls
(Fawcett & Provost, 1996), telecommunications management (Ezawa, Singh, & Norton,
1996), text classication (Lewis & Catlett, 1994; Dumais, Platt, Heckerman, & Sahami,
1998; Mladenic & Grobelnik, 1999; Lewis & Ringuette, 1994; Cohen, 1995a) detection
oil spills satellite images (Kubat, Holte, & Matwin, 1998).
performance machine learning algorithms typically evaluated using predictive
accuracy. However, appropriate data imbalanced and/or costs
dierent errors vary markedly. example, consider classication pixels mammogram images possibly cancerous (Woods, Doss, Bowyer, Solka, Priebe, & Kegelmeyer,
1993). typical mammography dataset might contain 98% normal pixels 2% abnormal
pixels. simple default strategy guessing majority class would give predictive accuracy 98%. However, nature application requires fairly high rate correct
detection minority class allows small error rate majority class
order achieve this. Simple predictive accuracy clearly appropriate situations. Receiver Operating Characteristic (ROC) curve standard technique
summarizing classier performance range tradeos true positive false
positive error rates (Swets, 1988). Area Curve (AUC) accepted traditional performance metric ROC curve (Duda, Hart, & Stork, 2001; Bradley, 1997; Lee,
2000). ROC convex hull also used robust method identifying potentially
optimal classiers (Provost & Fawcett, 2001). line passes point convex
hull, line slope passing another point
larger true positive (TP) intercept. Thus, classier point optimal
distribution assumptions tandem slope.
machine learning community addressed issue class imbalance two ways.
One assign distinct costs training examples (Pazzani, Merz, Murphy, Ali, Hume, &
Brunk, 1994; Domingos, 1999). re-sample original dataset, either oversampling minority class and/or under-sampling majority class (Kubat & Matwin,
1997; Japkowicz, 2000; Lewis & Catlett, 1994; Ling & Li, 1998). approach (Chawla,
Bowyer, Hall, & Kegelmeyer, 2000) blends under-sampling majority class
special form over-sampling minority class. Experiments various datasets
C4.5 decision tree classier (Quinlan, 1992), Ripper (Cohen, 1995b), Naive Bayes
Classier show approach improves previous re-sampling, modifying loss
ratio, class priors approaches, using either AUC ROC convex hull.
Section 2 gives overview performance measures. Section 3 reviews
closely related work dealing imbalanced datasets. Section 4 presents details
approach. Section 5 presents experimental results comparing approach
re-sampling approaches. Section 6 discusses results suggests directions future
work.

2. Performance Measures
performance machine learning algorithms typically evaluated confusion matrix
illustrated Figure 1 (for 2 class problem). columns Predicted class
rows Actual class. confusion matrix, N number negative examples
322

fiSMOTE

Predicted
Negative

Predicted
Positive

Actual
Negative

TN

FP

Actual
Positive

FN

TP

Figure 1: Confusion Matrix
correctly classied (True Negatives), F P number negative examples incorrectly
classied positive (False Positives), F N number positive examples incorrectly
classied negative (False Negatives) P number positive examples correctly
classied (True Positives).
Predictive accuracy performance measure generally associated machine learning algorithms dened Accuracy = (T P + N )/(T P + F P + N + F N ).
context balanced datasets equal error costs, reasonable use error rate
performance metric. Error rate 1 Accuracy. presence imbalanced datasets
unequal error costs, appropriate use ROC curve similar
techniques (Ling & Li, 1998; Drummond & Holte, 2000; Provost & Fawcett, 2001; Bradley,
1997; Turney, 1996).
ROC curves thought representing family best decision boundaries
relative costs TP FP. ROC curve X-axis represents %F P = F P/(T N +F P )
Y-axis represents %T P = P/(T P +F N ). ideal point ROC curve would
(0,100), positive examples classied correctly negative examples
misclassied positive. One way ROC curve swept manipulating
balance training samples class training set. Figure 2 shows illustration.
line = x represents scenario randomly guessing class. Area ROC
Curve (AUC) useful metric classier performance independent decision
criterion selected prior probabilities. AUC comparison establish dominance
relationship classiers. ROC curves intersecting, total AUC
average comparison models (Lee, 2000). However, specic cost class
distributions, classier maximum AUC may fact suboptimal. Hence,
also compute ROC convex hulls, since points lying ROC convex hull
potentially optimal (Provost, Fawcett, & Kohavi, 1998; Provost & Fawcett, 2001).

3. Previous Work: Imbalanced datasets
Kubat Matwin (1997) selectively under-sampled majority class keeping
original population minority class. used geometric mean performance measure classier, related single point ROC curve.
minority examples divided four categories: noise overlapping positive class decision region, borderline samples, redundant samples safe samples.
borderline examples detected using Tomek links concept (Tomek, 1976). Another
323

fiChawla, Bowyer, Hall & Kegelmeyer

ROC

(100, 100)

100
Ideal point

Percent
True

y=x

Positive
increased undersampling
majority class moves
operating point
upper right
original data set

0

Percent False Positive

100

Figure 2: Illustration sweeping ROC curve under-sampling. Increased
under-sampling majority (negative) class move performance
lower left point upper right.

related work proposed SHRINK system classies overlapping region minority (positive) majority (negative) classes positive; searches best positive
region (Kubat et al., 1998).
Japkowicz (2000) discussed eect imbalance dataset. evaluated three
strategies: under-sampling, resampling recognition-based induction scheme. focus
sampling approaches. experimented articial 1D data order easily
measure construct concept complexity. Two resampling methods considered.
Random resampling consisted resampling smaller class random consisted
many samples majority class focused resampling consisted resampling
minority examples occurred boundary minority
majority classes. Random under-sampling considered, involved under-sampling
majority class samples random numbers matched number minority
class samples; focused under-sampling involved under-sampling majority class samples
lying away. noted sampling approaches eective, also
observed using sophisticated sampling techniques give clear advantage
domain considered (Japkowicz, 2000).
One approach particularly relevant work Ling Li (1998).
combined over-sampling minority class under-sampling majority
class. used lift analysis instead accuracy measure classiers performance.
proposed test examples ranked condence measure lift used
evaluation criteria. lift curve similar ROC curve, tailored
324

fiSMOTE

marketing analysis problem (Ling & Li, 1998). one experiment, under-sampled
majority class noted best lift index obtained classes equally
represented (Ling & Li, 1998). another experiment, over-sampled positive
(minority) examples replacement match number negative (majority) examples
number positive examples. over-sampling under-sampling combination
provide signicant improvement lift index. However, approach oversampling diers theirs.
Solberg Solberg (1996) considered problem imbalanced data sets oil slick
classication SAR imagery. used over-sampling under-sampling techniques
improve classication oil slicks. training data distribution 42 oil
slicks 2,471 look-alikes, giving prior probability 0.98 look-alikes. imbalance
would lead learner (without appropriate loss functions methodology modify
priors) classify almost look-alikes correctly expense misclassifying many
oil slick samples (Solberg & Solberg, 1996). overcome imbalance problem,
over-sampled (with replacement) 100 samples oil slick, randomly sampled
100 samples non oil slick class create new dataset equal probabilities.
learned classier tree balanced data set achieved 14% error rate
oil slicks leave-one-out method error estimation; look alikes achieved
error rate 4% (Solberg & Solberg, 1996).
Another approach similar work Domingos (1999). compares
metacost approach majority under-sampling minority over-sampling.
nds metacost improves either, under-sampling preferable minority over-sampling. Error-based classiers made cost-sensitive. probability
class example estimated, examples relabeled optimally
respect misclassication costs. relabeling examples expands decision
space creates new samples classier may learn (Domingos, 1999).
feed-forward neural network trained imbalanced dataset may learn discriminate enough classes (DeRouin, Brown, Fausett, & Schneider, 1991).
authors proposed learning rate neural network adapted statistics
class representation data. calculated attention factor proportion
samples presented neural network training. learning rate network
elements adjusted based attention factor. experimented articially
generated training set real-world training set, multiple (more two)
classes. compared approach replicating minority class samples
balance data set used training. classication accuracy minority class
improved.
Lewis Catlett (1994) examined heterogeneous uncertainty sampling supervised
learning. method useful training samples uncertain classes. training
samples labeled incrementally two phases uncertain instances passed
next phase. modied C4.5 include loss ratio determining class
values leaves. class values determined comparison probability
threshold LR/(LR + 1), LR loss ratio (Lewis & Catlett, 1994).
information retrieval (IR) domain (Dumais et al., 1998; Mladenic & Grobelnik,
1999; Lewis & Ringuette, 1994; Cohen, 1995a) also faces problem class imbalance
dataset. document web page converted bag-of-words representation;
325

fiChawla, Bowyer, Hall & Kegelmeyer

is, feature vector reecting occurrences words page constructed. Usually,
instances interesting category text categorization. overrepresentation negative class information retrieval problems cause problems
evaluating classiers performances. Since error rate good metric skewed
datasets, classication performance algorithms information retrieval usually
measured precision recall:
recall =

TP
TP + FN

precision =

TP
TP + FP

Mladenic Grobelnik (1999) proposed feature subset selection approach deal
imbalanced class distribution IR domain. experimented various
feature selection methods, found odds ratio (van Rijsbergen, Harper, & Porter,
1981) combined Naive Bayes classier performs best domain. Odds
ratio probabilistic measure used rank documents according relevance
positive class (minority class). Information gain word, hand,
pay attention particular target class; computed per word class.
imbalanced text dataset (assuming 98 99% negative class), features
associated negative class. Odds ratio incorporates target class information
metric giving better results compared information gain text categorization.
Provost Fawcett (1997) introduced ROC convex hull method estimate
classier performance imbalanced datasets. note problems unequal
class distribution unequal error costs related little work done
address either problem (Provost & Fawcett, 2001). ROC convex hull method,
ROC space used separate classication performance class cost distribution
information.
summarize literature, under-sampling majority class enables better classiers
built over-sampling minority class. combination two done
previous work lead classiers outperform built utilizing undersampling. However, over-sampling minority class done sampling
replacement original data. approach uses dierent method over-sampling.

4. SMOTE: Synthetic Minority Over-sampling TEchnique
4.1 Minority over-sampling replacement
Previous research (Ling & Li, 1998; Japkowicz, 2000) discussed over-sampling
replacement noted doesnt signicantly improve minority class recognition.
interpret underlying eect terms decision regions feature space. Essentially,
minority class over-sampled increasing amounts, eect identify similar
specic regions feature space decision region minority class.
eect decision trees understood plots Figure 3.
326

fiSMOTE

2attributes, 10% data original Mammography dataset

2attributes, 10% data original Mammography dataset
450

200

400

350

150

Attribute 2

Attribute 2

300

250

200

100

150

100

50

50

0

0

2

4

6

8

10

12

14

0

16

1

2

3

4

Attribute 1

5

6

7

8

Attribute 1

(a)

(b)

2attributes, 10% data original Mammography dataset
200

Attribute 2

150

100

50

0

1

2

3

4

5

Attribute 1

6

7

8

(c)
Figure 3: a) Decision region three minority class samples (shown +) reside
building decision tree. decision region indicated solid-line
rectangle. b) zoomed-in view chosen minority class samples
dataset. Small solid-line rectangles show decision regions result oversampling minority class replication. c) zoomed-in view chosen
minority class samples dataset. Dashed lines show decision region
over-sampling minority class synthetic generation.

327

fiChawla, Bowyer, Hall & Kegelmeyer

data plot Figure 3 extracted Mammography dataset1 (Woods
et al., 1993). minority class samples shown + majority class samples
shown plot. Figure 3(a), region indicated solid-line rectangle
majority class decision region. Nevertheless, contains three minority class samples
shown + false negatives. replicate minority class, decision region
minority class becomes specic cause new splits decision tree.
lead terminal nodes (leaves) learning algorithm tries learn
specic regions minority class; essence, overtting. Replication minority
class cause decision boundary spread majority class region. Thus,
Figure 3(b), three samples previously majority class decision region
specic decision regions.
4.2 SMOTE
propose over-sampling approach minority class over-sampled creating synthetic examples rather over-sampling replacement. approach
inspired technique proved successful handwritten character recognition (Ha
& Bunke, 1997). created extra training data performing certain operations
real data. case, operations like rotation skew natural ways perturb
training data. generate synthetic examples less application-specic manner,
operating feature space rather data space. minority class over-sampled
taking minority class sample introducing synthetic examples along line
segments joining any/all k minority class nearest neighbors. Depending upon
amount over-sampling required, neighbors k nearest neighbors randomly
chosen. implementation currently uses nearest neighbors. instance,
amount over-sampling needed 200%, two neighbors nearest neighbors chosen one sample generated direction each. Synthetic samples
generated following way: Take dierence feature vector (sample)
consideration nearest neighbor. Multiply dierence random number
0 1, add feature vector consideration. causes
selection random point along line segment two specic features.
approach eectively forces decision region minority class become general.
Algorithm SMOTE , next page, pseudo-code SMOTE. Table 4.2 shows
example calculation random synthetic samples. amount over-sampling
parameter system, series ROC curves generated dierent
populations ROC analysis performed.
synthetic examples cause classier create larger less specic decision
regions shown dashed lines Figure 3(c), rather smaller specic
regions. general regions learned minority class samples rather
subsumed majority class samples around them. eect decision trees generalize better. Figures 4 5 compare minority over-sampling
replacement SMOTE. experiments conducted mammography dataset.
10923 examples majority class 260 examples minority class
originally. approximately 9831 examples majority class 233 examples
1. data available USF Intelligent Systems Lab, http://morden.csee.usf.edu/chawla.

328

fiSMOTE

minority class training set used 10-fold cross-validation. minority class
over-sampled 100%, 200%, 300%, 400% 500% original size. graphs
show tree sizes minority over-sampling replacement higher degrees
replication much greater SMOTE, minority class recognition
minority over-sampling replacement technique higher degrees replication isnt
good SMOTE.
Algorithm SMOTE (T, N, k)
Input: Number minority class samples ; Amount SMOTE N %; Number nearest
neighbors k
Output: (N/100) * synthetic minority class samples
1. ( N less 100%, randomize minority class samples random
percent SMOTEd. )
2. N < 100
3.
Randomize minority class samples
4.
= (N/100)
5.
N = 100
6. endif
7. N = (int)(N/100) ( amount SMOTE assumed integral multiples
100. )
8. k = Number nearest neighbors
9. numattrs = Number attributes
10. Sample[ ][ ]: array original minority class samples
11. newindex: keeps count number synthetic samples generated, initialized 0
12. Synthetic[ ][ ]: array synthetic samples
( Compute k nearest neighbors minority class sample only. )
13. 1
14.
Compute k nearest neighbors i, save indices nnarray
15.
Populate(N , i, nnarray)
16. endfor
Populate(N, i, nnarray) ( Function generate synthetic samples. )
17. N = 0
18.
Choose random number 1 k, call nn. step chooses one
k nearest neighbors i.
19.
attr 1 numattrs
20.
Compute: dif = Sample[nnarray[nn]][attr] Sample[i][attr]
21.
Compute: gap = random number 0 1
22.
Synthetic[newindex][attr] = Sample[i][attr] + gap dif
23.
endfor
24.
newindex++
25.
N = N 1
26. endwhile
27. return ( End Populate. )
End Pseudo-Code.

329

fiChawla, Bowyer, Hall & Kegelmeyer

Consider sample (6,4) let (4,3) nearest neighbor.
(6,4) sample k-nearest neighbors identied.
(4,3) one k-nearest neighbors.
Let:
f1 1 = 6 f2 1 = 4 f2 1 - f1 1 = -2
f1 2 = 4 f2 2 = 3 f2 2 - f1 2 = -1
new samples generated
(f1,f2) = (6,4) + rand(0-1) * (-2,-1)
rand(0-1) generates random number 0 1.
Table 1: Example generation synthetic examples (SMOTE).

Pruned decision tree size vs degree minority oversampling
260

240

Decisiion tree size (Number nodes)

220

200

180

160

140
Synthetic data
Replicated data
120

100

80

60

0

50

100

150

200
250
300
350
Degree minority oversampling

400

450

500

Figure 4: Comparison decision tree sizes replicated over-sampling SMOTE
Mammography dataset

330

fiSMOTE

% Minority Correct vs Degree Minority Oversampling
75

%Minority Correct

70

65

60

Synthetic data
Replicated data

55

50
0

50

100

150

200

250

300

350

400

450

500

Degree Minority Oversampling

Figure 5: Comparison % Minority correct replicated over-sampling SMOTE
Mammography dataset

4.3 Under-sampling SMOTE Combination
majority class under-sampled randomly removing samples majority class
population minority class becomes specied percentage majority class.
forces learner experience varying degrees under-sampling higher degrees
under-sampling minority class larger presence training set. describing
experiments, terminology under-sample majority class
200%, would mean modied dataset contain twice many elements
minority class majority class; is, minority class 50 samples
majority class 200 samples under-sample majority 200%, majority
class would end 25 samples. applying combination under-sampling
over-sampling, initial bias learner towards negative (majority) class reversed
favor positive (minority) class. Classiers learned dataset perturbed
SMOTING minority class under-sampling majority class.

5. Experiments
used three dierent machine learning algorithms experiments. Figure 6 provides
overview experiments.
1. C4.5: compared various combinations SMOTE under-sampling plain
under-sampling using C4.5 release 8 (Quinlan, 1992) base classier.
331

fiChawla, Bowyer, Hall & Kegelmeyer

SMOTE
Undersampling.

C4.5

Loss-Ratio
Modify costs majority minority
varied 0.9 0.001.
classes changing priors.

Ripper

Naive Bayes

ROCs generated SMOTE, Undersampling
Loss Ratio comparisons. Performance
evaluated AUC ROC convex hull.

ROCs generated comparison
SMOTE Under-sampling using C4.5,
SMOTE using C4.5 Naive bayes.
Performance evaluated AUC ROC convex hull.

Figure 6: Experiments Overview

2. Ripper: compared various combinations SMOTE under-sampling
plain under-sampling using Ripper (Cohen, 1995b) base classier. also
varied Rippers loss ratio (Cohen & Singer, 1996; Lewis & Catlett, 1994) 0.9
0.001 (as means varying misclassication cost) compared eect
variation combination SMOTE under-sampling. reducing loss
ratio 0.9 0.001 able build set rules minority class.
3. Naive Bayes Classifier: Naive Bayes Classier2 made cost-sensitive
varying priors minority class. varied priors minority
class 1 50 times majority class compared C4.5s SMOTE
under-sampling combination.

dierent learning algorithms allowed SMOTE compared methods
handle misclassication costs directly. %FP %TP averaged 10-fold
cross-validation runs data combinations. minority class examples
over-sampled calculating nearest neighbors generating synthetic examples.
AUC calculated using trapezoidal rule. extrapolated extra point TP
= 100% FP = 100% ROC curve. also computed ROC convex hull
identify optimal classiers, points lying hull potentially optimal
classiers (Provost & Fawcett, 2001).
2. source code downloaded http://fuzzy.cs.uni-magdeburg.de/borgelt/software.html.

332

fiSMOTE

5.1 Datasets
experimented nine dierent datasets. datasets summarized Table 5.2.
datasets vary extensively size class proportions, thus oering dierent
domains SMOTE. order increasing imbalance are:
1. Pima Indian Diabetes (Blake & Merz, 1998) 2 classes 768 samples.
data used identify positive diabetes cases population near Phoenix,
Arizona. number positive class samples 268. Good sensitivity
detection diabetes cases desirable attribute classier.
2. Phoneme dataset ELENA project3 . aim dataset
distinguish nasal (class 0) oral sounds (class 1). 5 features.
class distribution 3,818 samples class 0 1,586 samples class 1.
3. Adult dataset (Blake & Merz, 1998) 48,842 samples 11,687 samples
belonging minority class. dataset 6 continuous features 8 nominal
features. SMOTE SMOTE-NC (see Section 6.1) algorithms evaluated
dataset. SMOTE, extracted continuous features generated new
dataset continuous features.
4. E-state data4 (Hall, Mohney, & Kier, 1991) consists electrotopological state
descriptors series compounds National Cancer Institutes Yeast AntiCancer drug screen. E-state descriptors NCI Yeast AntiCancer Drug Screen
generated Tripos, Inc. Briey, series 60,000 compounds
tested series 6 yeast strains given concentration. test
high-throughput screen one concentration results subject contamination, etc. growth inhibition yeast strain exposed given
compound (with respect growth yeast neutral solvent) measured.
activity classes either active least one single yeast strain inhibited
70%, inactive yeast strain inhibited 70%.
dataset 53,220 samples 6,351 samples active compounds.
5. Satimage dataset (Blake & Merz, 1998) 6 classes originally. chose
smallest class minority class collapsed rest classes one
done (Provost et al., 1998). gave us skewed 2-class dataset, 5809
majority class samples 626 minority class samples.
6. Forest Cover dataset UCI repository (Blake & Merz, 1998).
dataset 7 classes 581,012 samples. dataset prediction forest
cover type based cartographic variables. Since system currently works binary classes extracted data two classes dataset ignored rest.
approaches work two classes (Ling & Li, 1998; Japkowicz,
2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001). two classes considered Ponderosa Pine 35,754 samples Cottonwood/Willow 2,747
3. ftp.dice.ucl.ac.be directory pub/neural-nets/ELENA/databases.
4. would like thank Steven Eschrich providing dataset description us.

333

fiChawla, Bowyer, Hall & Kegelmeyer

Dataset
Pima
Phoneme
Adult
E-state
Satimage
Forest Cover
Oil
Mammography


Majority Class
500
3818
37155
46869
5809
35754
896
10923
435512

Minority Class
268
1586
11687
6351
626
2747
41
260
8360

Table 2: Dataset distribution
samples. Nevertheless, SMOTE technique applied multiple class problem well specifying class SMOTE for. However, paper,
focused 2-classes problems, explicitly represent positive negative classes.
7. Oil dataset provided Robert Holte used paper (Kubat et al.,
1998). dataset 41 oil slick samples 896 non-oil slick samples.
8. Mammography dataset (Woods et al., 1993) 11,183 samples 260 calcications. look predictive accuracy measure goodness classier
case, default accuracy would 97.68% every sample labeled noncalcication. But, desirable classier predict calcications
correctly.
9. dataset generated ExodusII data using AVATAR
(Chawla & Hall, 1999) version Mustafa Visualization tool5 . portion
crushed marked interesting rest
marked unknown. dataset size 443,872 samples 8,360 samples marked
interesting generated.
5.2 ROC Creation
ROC curve SMOTE produced using C4.5 Ripper create classier
one series modied training datasets. given ROC curve produced rst
over-sampling minority class specied degree under-sampling majority
class increasing degrees generate successive points curve. amount
under-sampling identical plain under-sampling. So, corresponding point
ROC curve dataset represents number majority class samples. Dierent
ROC curves produced starting dierent levels minority over-sampling. ROC
curves also generated varying loss ratio Ripper 0.9 0.001
varying priors minority class original distribution 50 times
majority class Naive Bayes Classier.
5. Mustafa visualization tool developed Mike Glass Sandia National Labs.

334

fiSMOTE

Phoneme ROC
100

95

90

85

%TP

UnderC4.5
200 SMOTEC4.5
Naive Bayes
Hull

80

75

70

65
10

20

30

40

50

60

70

80

90

100

%FP

Figure 7: Phoneme. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. SMOTEC4.5 dominates Naive Bayes Under-C4.5 ROC space. SMOTEC4.5 classiers potentially optimal classiers.

Figures 9 23 show experimental ROC curves obtained nine datasets
three classiers. ROC curve plain under-sampling majority class
(Ling & Li, 1998; Japkowicz, 2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001)
compared approach combining synthetic minority class over-sampling (SMOTE)
majority class under-sampling. plain under-sampling curve labeled Under,
SMOTE under-sampling combination ROC curve labeled SMOTE. Depending size relative imbalance dataset, one SMOTE undersampling curves created. show best results SMOTE combined
under-sampling plain under-sampling curve graphs. SMOTE ROC curve
C4.5 also compared ROC curve obtained varying priors minority
class using Naive Bayes classier labeled Naive Bayes. SMOTE, Under,
Loss Ratio ROC curves, generated using Ripper also compared. given family
ROC curves, ROC convex hull (Provost & Fawcett, 2001) generated. ROC
convex hull generated using Grahams algorithm (ORourke, 1998). reference,
show ROC curve would obtained using minority over-sampling replication
Figure 19.
point ROC curve result either classier (C4.5 Ripper) learned
particular combination under-sampling SMOTE, classier (C4.5 Ripper)
learned plain under-sampling, classier (Ripper) learned using loss ratio
classier (Naive Bayes) learned dierent prior minority class. point
represents average (%TP %FP) 10-fold cross-validation result. lower leftmost
point given ROC curve raw dataset, without majority class under335

fiChawla, Bowyer, Hall & Kegelmeyer

Phoneme ROC Ripper
100

95

%TP

90

85

UnderRipper
200 SMOTERipper
Loss Ratio
Hull

80

75

70

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 8: Phoneme. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss
Ratio Ripper. SMOTE-Ripper dominates Under-Ripper Loss Ratio
ROC space. SMOTE-Ripper classiers lie ROC convex hull.

Pima ROC
100

95

90

85

%TP

80

75

UnderC4.5
100 SMOTEC4.5
Naive Bayes
Hull

70

65

60

55

50
10

20

30

40

50

60

70

80

90

100

%FP

Figure 9: Pima Indians Diabetes. Comparison SMOTE-C4.5, Under-C4.5, Naive
Bayes. Naive Bayes dominates SMOTE-C4.5 ROC space.

336

fiSMOTE

Pima ROC Ripper
100

95

90

85

%TP

80
UnderRipper
100 SMOTERipper
Loss Ratio
Hull

75

70

65

60

55
10

20

30

40

50

60

70

80

90

100

%FP

Figure 10: Pima Indians Diabetes. Comparison SMOTE-Ripper, Under-Ripper,
modifying Loss Ratio Ripper. SMOTE-Ripper dominates Under-Ripper
Loss Ratio ROC space.

sampling minority class over-sampling. minority class over-sampled 50%,
100%, 200%, 300%, 400%, 500%. majority class under-sampled 10%, 15%,
25%, 50%, 75%, 100%, 125%, 150%, 175%, 200%, 300%, 400%, 500%, 600%, 700%, 800%,
1000%, 2000%. amount majority class under-sampling minority class oversampling depended dataset size class proportions. instance, consider
ROC curves Figure 17 mammography dataset. three curves one
plain majority class under-sampling range under-sampling varied
5% 2000% dierent intervals, one combination SMOTE majority class
under-sampling, one Naive Bayes one ROC convex hull curve. ROC
curve shown Figure 17 minority class over-sampled 400%. point
SMOTE ROC curves represents combination (synthetic) over-sampling undersampling, amount under-sampling follows range plain under-sampling.
better understanding ROC graphs, shown dierent sets ROC curves
one datasets Appendix A.
dataset, SMOTE lesser degree datasets
due structural nature dataset. dataset structural
neighborhood already established mesh geometry, SMOTE lead creating
neighbors surface (and hence interesting), since looking
feature space physics variables structural information.
ROC curves show trend increase amount under-sampling coupled
over-sampling, minority classication accuracy increases, course expense
majority class errors. almost ROC curves, SMOTE approach dom337

fiChawla, Bowyer, Hall & Kegelmeyer

Satimage ROC
100

95

90

85
UnderC4.5
200 SMOTEC4.5
Naive Bayes
Hull

%TP

80

75

70

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 11: Satimage. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
ROC curves Naive Bayes SMOTE-C4.5 show overlap; however,
higher TPs points SMOTE-C4.5 lie ROC convex hull.

Satimage ROC Ripper
100

95

90

85

%TP

80
UnderRipper
300 SMOTERipper
Loss Ratio
Hull

75

70

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 12: Satimage. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss
Ratio Ripper. SMOTE-Ripper dominates ROC space. ROC convex
hull mostly constructed points SMOTE-Ripper.

338

fiSMOTE

Covtype ROC
100

90

80

70

%TP

60
UnderC4.5
300 SMOTEC4.5
Naive Bayes
Hull

50

40

30

20

10

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 13: Forest Cover. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
SMOTE-C4.5 Under-C4.5 ROC curves close other. However, points SMOTE-C4.5 ROC curve lie ROC convex
hull, thus establishing dominance.

inates. Adhering denition ROC convex hull, potentially optimal
classiers ones generated SMOTE.
5.3 AUC Calculation
Area ROC curve (AUC) calculated using form trapezoid rule.
lower leftmost point given ROC curve classiers performance raw data.
upper rightmost point always (100%, 100%). curve naturally end
point, point added. necessary order AUCs compared
range %FP.
AUCs listed Table 5.3 show datasets combined synthetic minority over-sampling majority over-sampling able improve plain majority
under-sampling C4.5 base classier. Thus, SMOTE approach provides
improvement correct classication data underrepresented class.
conclusion holds examination ROC convex hulls. entries
missing table, SMOTE applied amounts datasets.
amount SMOTE less less skewed datasets. Also, included AUCs
Ripper/Naive Bayes. ROC convex hull identies SMOTE classiers potentially optimal compared plain under-sampling treatments misclassication
costs, generally. Exceptions follows: Pima dataset, Naive Bayes dominates
SMOTE-C4.5; Oil dataset, Under-Ripper dominates SMOTE-Ripper.
dataset, SMOTE-classifier (classifier = C4.5 Ripper) Under-classifier ROC
339

fiChawla, Bowyer, Hall & Kegelmeyer

Covtype ROC RIPPER
100

98

96

%TP

94
UnderRipper
100 SMOTERipper
Loss Ratio
Hull

92

90

88

86

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 14: Forest Cover. Comparison SMOTE-Ripper, Under-Ripper, modifying
Loss Ratio Ripper. SMOTE-Ripper shows domination ROC space.
points SMOTE-Ripper curve lie ROC convex hull.

Oil ROC
100

90

80

70

%TP

60

50

40
UnderC4.5
500 SMOTEC4.5
Naive Bayes
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 15: Oil. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. Although,
SMOTE-C4.5 Under-C4.5 ROC curves intersect points, points
SMOTE-C4.5 curve lie ROC convex hull.

340

fiSMOTE

Oil ROC Ripper
100

90

80

%TP

70
UnderRipper
300 SMOTERipper
Loss Ratio
Hull

60

50

40

30

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 16: Oil. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss Ratio
Ripper. Under-Ripper SMOTE-Ripper curves intersect, points
Under-Ripper curve lie ROC convex hull.

Mammography ROC
100

90

80

70
UnderC4.5
400 SMOTEC4.5
Naive Bayes
Hull

%TP

60

50

40

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 17: Mammography. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
SMOTE-C4.5 Under-C4.5 curves intersect ROC space; however,
virtue number points ROC convex hull, SMOTE-C4.5
potentially optimal classiers.

341

fiChawla, Bowyer, Hall & Kegelmeyer

Mammography ROC RIPPER
100

95

90

85

%TP

80
UnderRipper
400 SMOTERipper
Loss Ratio
Hull

75

70

65

60

55

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 18: Mammography. Comparison SMOTE-Ripper, Under-Ripper, modifying
Loss Ratio Ripper. SMOTE-Ripper dominates ROC space TP > 75%.
Mammography ROC C4.5
100

95

90

85

%TP

80

75

70
400 SMOTE
400 Replicate
Hull

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 19: comparison over-sampling minority class examples SMOTE oversampling minority class examples replication Mammography
dataset.

342

fiSMOTE

Estate ROC
100

90

80

70

%TP

60

50

40
UnderC4.5
500 SMOTEC4.5
Naive Bayes
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 20: E-state. (a) Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
SMOTE-C4.5 Under-C4.5 curves intersect ROC space; however,
SMOTE-C4.5 potentially optimal classiers, based number
points ROC convex hull.

Estate ROC Ripper
100

90

80

70

%TP

60

50

40
UnderRipper
100 SMOTERipper
Loss Ratio
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 21: E-state. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss
Ratio Ripper. SMOTE-Ripper potentially optimal classiers, based
number points ROC convex hull.

343

fiChawla, Bowyer, Hall & Kegelmeyer

ROC
100

90

80

70

%TP

60

50

40

UnderC4.5
100 SMOTEC4.5
Naive Bayes
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 22: Can. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. SMOTEC4.5 Under-C4.5 ROC curves overlap ROC space.

ROC Ripper
100

90

80

70

%TP

60

50

40

UnderRipper
50 SMOTERipper
Loss Ratio
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 23: Can. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss Ratio
Ripper. SMOTE-Ripper Under-Ripper ROC curves overlap
ROC space.

344

fiSMOTE

Dataset



Pima
Phoneme
Satimage
Forest Cover
Oil
Mammography
E-state


7242
8622
8900
9807
8524
9260
6811
9535

50
SMOTE

9560

100
SMOTE
7307
8644
8957
9832
8523
9250
6792
9505

200
SMOTE

300
SMOTE

400
SMOTE

500
SMOTE

8661
8979
9834
8368
9265
6828
9505

8963
9849
8161
9311
6784
9494

8975
9841
8339
9330
6788
9472

8960
9842
8537
9304
6779
9470

Table 3: AUCs [C4.5 base classier] best highlighted bold.

curves overlap ROC space. datasets, SMOTE-classifier
potentially optimal classiers approach.
5.4 Additional comparison changing decision thresholds
Provost (2000) suggested simply changing decision threshold always
considered alternative sophisticated approaches. case C4.5,
would mean changing decision threshold leaves decision trees. example,
leaf could classify examples minority class even 50% training
examples leaf represent majority class. experimented setting decision
thresholds leaves C4.5 decision tree learner 0.5, 0.45, 0.42, 0.4, 0.35, 0.32,
0.3, 0.27, 0.25, 0.22, 0.2, 0.17, 0.15, 0.12, 0.1, 0.05, 0.0. experimented Phoneme
dataset. Figure 24 shows comparison SMOTE under-sampling combination
C4.5 learning tuning bias towards minority class. graph shows
SMOTE under-sampling combination ROC curve dominating entire
range values.
5.5 Additional comparison one-sided selection SHRINK
oil dataset, also followed slightly dierent line experiments obtain results
comparable (Kubat et al., 1998). alleviate problem imbalanced datasets
authors proposed (a) one-sided selection under-sampling majority class (Kubat
& Matwin, 1997) (b) SHRINK system (Kubat et al., 1998). Table 5.5 contains
results (Kubat et al., 1998). Acc+ accuracy positive (minority) examples
Acc accuracy negative (majority) examples. Figure 25 shows trend
Acc+ Acc one combination SMOTE strategy varying degrees undersampling majority class. Y-axis represents accuracy X-axis represents
percentage majority class under-sampled. graphs indicate band
under-sampling 50% 125% results comparable achieved
SHRINK better SHRINK cases. Table 5.5 summarizes results
SMOTE 500% under-sampling combination. also tried combinations SMOTE
100-400% varying degrees under-sampling achieved comparable results.
345

fiChawla, Bowyer, Hall & Kegelmeyer

Phoneme: ROC comparison SMOTE C4.5 variation decision thresholds
100

95

%TP

90
SMOTE
Varying C4.5 decision thresholds
Hull
85

80

75
10

20

30

40

50

60

70

80

90

100

%FP

Figure 24: SMOTE Under-sampling combination C4.5 learning tuning
bias towards minority class

SMOTE Undersampling
100

90

Accuracy

80

Accuracy majority (negative class)
Accuracy minority (positive class)

70

60

50

40

30

0

100

200

300
400
500
600
Percentage undersampling majority class

700

800

Figure 25: SMOTE (500 OU) Under-sampling combination performance

SHRINK approach SMOTE approach directly comparable, though,
see dierent data points. SMOTE oers clear improvement one-sided selection.
346

fiSMOTE

Method
SHRINK
One-sided selection

Acc+
82.5%
76.0%

Acc
60.9%
86.6%

Table 4: Cross-validation results (Kubat et al., 1998)

Under-sampling %
10%
15%
25%
50%
75%
100%
125%
150%
175%
200%
300%
400%
500%
600%
700%
800%

Acc+
64.7%
62.8%
64.0%
89.5%
83.7%
78.3%
84.2%
83.3%
85.0%
81.7%
89.0%
95.5%
98.0%
98.0%
96.0%
90.7%

Acc
94.2%
91.3%
89.1%
78.9%
73.0%
68.7%
68.1%
57.8%
57.8%
56.7%
55.0%
44.2%
35.5%
40.0%
32.8%
33.3%

Table 5: Cross-validation results SMOTE 500% SMOTE Oil data set.

347

fiChawla, Bowyer, Hall & Kegelmeyer

6. Future Work
several topics considered line research. Automated adaptive
selection number nearest neighbors would valuable. Dierent strategies
creating synthetic neighbors may able improve performance. Also, selecting
nearest neighbors focus examples incorrectly classied may improve
performance. minority class sample could possibly majority class sample
nearest neighbor rather minority class sample. crowding likely contribute
redrawing decision surfaces favor minority class. addition
topics, following subsections discuss two possible extensions SMOTE,
application SMOTE information retrieval.
6.1 SMOTE-NC
SMOTE approach currently handle data sets nominal features,
generalized handle mixed datasets continuous nominal features. call
approach Synthetic Minority Over-sampling TEchnique-Nominal Continuous [SMOTE-NC].
tested approach Adult dataset UCI repository. SMOTE-NC
algorithm described below.
1. Median computation: Compute median standard deviations continuous
features minority class. nominal features dier sample
potential nearest neighbors, median included Euclidean distance
computation. use median penalize dierence nominal features
amount related typical dierence continuous feature values.
2. Nearest neighbor computation: Compute Euclidean distance feature
vector k-nearest neighbors identied (minority class sample)
feature vectors (minority class samples) using continuous feature space.
every diering nominal feature considered feature vector
potential nearest-neighbor, include median standard deviations previously
computed, Euclidean distance computation. Table 2 demonstrates example.
F1 = 1 2 3 B C [Let sample computing nearest
neighbors]
F2 = 4 6 5 E
F3 = 3 5 6 B K
So, Euclidean Distance F2 F1 would be:
Eucl = sqrt[(4-1)2 + (6-2)2 + (5-3)2 + Med2 + Med2 ]
Med median standard deviations continuous features minority class.
median term included twice feature numbers 5: BD 6: CE,
dier two feature vectors: F1 F2.

Table 6: Example nearest neighbor computation SMOTE-NC.

348

fiSMOTE

3. Populate synthetic sample: continuous features new synthetic minority
class sample created using approach SMOTE described earlier.
nominal feature given value occuring majority k-nearest neighbors.
SMOTE-NC experiments reported set SMOTE,
except fact examine one dataset only. SMOTE-NC Adult dataset
diers typical result: performs worse plain under-sampling based AUC,
shown Figures 26 27. extracted continuous features separate eect
SMOTE SMOTE-NC dataset, determine whether oddity
due handling nominal features. shown Figure 28, even SMOTE
continuous features applied Adult dataset, achieve better performance
plain under-sampling. minority class continuous features high
variance, so, synthetic generation minority class samples could overlapping
majority class space, thus leading false positives plain under-sampling.
hypothesis also supported decreased AUC measure SMOTE degrees
greater 50%. higher degrees SMOTE lead minority class samples
dataset, thus greater overlap majority class decision space.
Adult SMOTENC
100

95

90

85

%TP

80
UnderC4.5
50 SMOTENCC4.5
Naive Bayes
Hull

75

70

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 26: Adult. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. SMOTEC4.5 Under-C4.5 ROC curves overlap ROC space.

6.2 SMOTE-N
Potentially, SMOTE also extended nominal features SMOTE-N
nearest neighbors computed using modied version Value Dierence Metric (Stanll
& Waltz, 1986) proposed Cost Salzberg (1993). Value Dierence Metric (VDM)
looks overlap feature values feature vectors. matrix dening distance
349

fiChawla, Bowyer, Hall & Kegelmeyer

Adult ROC Ripper
100

95

90

%TP

85

80

UnderRipper
50 SMOTERipper
Loss Ratio
Hull

75

70

65

60

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 27: Adult. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss Ratio Ripper. SMOTE-Ripper Under-Ripper ROC curves overlap
ROC space.
Adult continuous [C4.5]
100

90

%TP

80

70


50 SMOTE

60

50

40

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 28: Adult continuous features. overlap SMOTE-C4.5 UnderC4.5 observed scenario well.

350

fiSMOTE

corresponding feature values feature vectors created. distance
two corresponding feature values dened follows.
(V1 , V2 ) =

n

C1i

|

i=1

C1



C2i k
|
C2

(1)

equation, V1 V2 two corresponding feature values. C1 total
number occurrences feature value V1 , C1i number occurrences feature
value V1 class i. similar convention also applied C2i C2 . k constant,
usually set 1. equation used compute matrix value dierences
nominal feature given set feature vectors. Equation 1 gives geometric distance
xed, nite set values (Cost & Salzberg, 1993). Cost Salzbergs modied VDM
omits weight term wfa included computation Stanll Waltz,
eect making symmetric. distance two feature vectors given by:

(X, ) = wx wy

N


(xi , yi )r

(2)

i=1

r = 1 yields Manhattan distance, r = 2 yields Euclidean distance (Cost &
Salzberg, 1993). wx wy exemplar weights modied VDM. wy = 1
new example (feature vector), wx bias towards reliable examples (feature
vectors) computed ratio number uses feature vector number
correct uses feature vector; thus, accurate feature vectors wx
1. SMOTE-N ignore weights equation 2, SMOTE-N used
classication purposes directly. However, redene weights give weight
minority class feature vectors falling closer majority class feature vectors; thus,
making minority class features appear away feature vector
consideration. Since, interested forming broader accurate regions
minority class, weights might used avoid populating along neighbors fall
closer majority class. generate new minority class feature vectors, create
new set feature values taking majority vote feature vector consideration
k nearest neighbors. Table 6.2 shows example creating synthetic feature vector.
Let F1 = B C E feature vector consideration
let 2 nearest neighbors
F2 = F C G N
F3 = H B C N
application SMOTE-N would create following feature vector:
FS = B C N
Table 7: Example SMOTE-N

351

fiChawla, Bowyer, Hall & Kegelmeyer

6.3 Application SMOTE Information Retrieval
investigating application SMOTE information retrieval (IR). IR problems come plethora features potentially many categories. SMOTE would
applied conjunction feature selection algorithm, transforming given
document web page bag-of-words format.
interesting comparison SMOTE would combination Naive Bayes
Odds ratio. Odds ratio focuses target class, ranks documents according
relevance target positive class. SMOTE also focuses target class creating
examples class.

7. Summary
results show SMOTE approach improve accuracy classiers
minority class. SMOTE provides new approach over-sampling. combination
SMOTE under-sampling performs better plain under-sampling. SMOTE
tested variety datasets, varying degrees imbalance varying amounts
data training set, thus providing diverse testbed. combination SMOTE
under-sampling also performs better, based domination ROC space, varying
loss ratios Ripper varying class priors Naive Bayes Classier: methods
could directly handle skewed class distribution. SMOTE forces focused learning
introduces bias towards minority class. Pima least skewed dataset
Naive Bayes Classier perform better SMOTE-C4.5. Also, Oil
dataset Under-Ripper perform better SMOTE-Ripper. dataset,
SMOTE-classifier Under-classifier ROC curves overlap ROC space.
rest datasets SMOTE-classifier performs better Under-classifier, Loss Ratio,
Naive Bayes. total 48 experiments performed, SMOTE-classifier
perform best 4 experiments.
interpretation synthetic minority over-sampling improves performance
minority over-sampling replacement fairly straightforward. Consider
eect decision regions feature space minority over-sampling done
replication (sampling replacement) versus introduction synthetic examples.
replication, decision region results classication decision minority
class actually become smaller specic minority samples region
replicated. opposite desired eect. method synthetic over-sampling
works cause classier build larger decision regions contain nearby minority
class points. reasons may applicable SMOTE performs better
Rippers loss ratio Naive Bayes; methods, nonetheless, still learning
information provided dataset, albeit dierent cost information. SMOTE
provides related minority class samples learn from, thus allowing learner carve
broader decision regions, leading coverage minority class.

Acknowledgments
research partially supported United States Department Energy
Sandia National Laboratories ASCI VIEWS Data Discovery Program, contract number
352

fiSMOTE

DE-AC04-76DO00789. thank Robert Holte providing oil spill dataset used
paper. also thank Foster Provost clarifying method using Satimage
dataset. would also like thank anonymous reviewers various insightful
comments suggestions.

353

fiChawla, Bowyer, Hall & Kegelmeyer

Appendix A. ROC graphs Oil Dataset
following gures show dierent sets ROC curves oil dataset. Figure 29 (a)
shows ROC curves Oil dataset, included main text; Figure 29(b) shows
ROC curves without ROC convex hull; Figure 29(c) shows two convex hulls,
obtained without SMOTE. ROC convex hull shown dashed lines stars
Figure 29(c), computed including Under-C4.5 Naive Bayes family
ROC curves. ROC convex hull shown solid line small circles Figure 29(c)
computed including 500 SMOTE-C4.5, Under-C4.5, Naive Bayes family
ROC curves. ROC convex hull SMOTE dominates ROC convex hull without
SMOTE, hence SMOTE-C4.5 contributes optimal classiers.
Oil

90

90

80

80

70

70

60

60

%TP

100

50

40

30

20

UnderC4.5
500 SMOTEC4.5
Naive Bayes

30

20

10

0

50

40
UnderC4.5
500 SMOTEC4.5
Naive Bayes
Hull

10

0

10

20

30

40

50
%FP

60

70

80

90

0

100

0

10

20

30

40

(a)

50
%FP

60

70

80

90

(b)
Oil ROC Convex Hulls
100

90

80

70

60

%TP

%TP

Oil ROC
100

50
Convex Hull SMOTE
Convex Hull without SMOTE

40

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

(c)
Figure 29: ROC curves Oil Dataset. (a) ROC curves SMOTE-C4.5, UnderC4.5, Naive Bayes, ROC convex hull. (b) ROC curves SMOTEC4.5, Under-C4.5, Naive Bayes. (c) ROC convex hulls without
SMOTE.

354

100

fiSMOTE

References
Blake, C., & Merz, C. (1998).
UCI Repository Machine Learning Databases
http://www.ics.uci.edu/mlearn/MLRepository.html. Department Information
Computer Sciences, University California, Irvine.
Bradley, A. P. (1997). Use Area ROC Curve Evaluation
Machine Learning Algorithms. Pattern Recognition, 30(6), 11451159.
Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, P. (2000). SMOTE: Synthetic Minority
Over-sampling TEchnique. International Conference Knowledge Based Computer Systems, pp. 4657. National Center Software Technology, Mumbai, India,
Allied Press.
Chawla, N., & Hall, L. (1999). Modifying MUSTAFA capture salient data. Tech. rep.
ISL-99-01, University South Florida, Computer Science Eng. Dept.
Cohen, W. (1995a). Learning Classify English Text ILP Methods. Proceedings 5th International Workshop Inductive Logic Programming, pp. 324.
Department Computer Science, Katholieke Universiteit Leuven.
Cohen, W. W. (1995b). Fast Eective Rule Induction. Proc. 12th International Conference Machine Learning, pp. 115123 Lake Tahoe, CA. Morgan Kaufmann.
Cohen, W. W., & Singer, Y. (1996). Context-sensitive Learning Methods Text Categorization. Frei, H.-P., Harman, D., Schauble, P., & Wilkinson, R. (Eds.), Proceedings
SIGIR-96, 19th ACM International Conference Research Development
Information Retrieval, pp. 307315 Zurich, CH. ACM Press, New York, US.
Cost, S., & Salzberg, S. (1993). Weighted Nearest Neighbor Algorithm Learning
Symbolic Features. Machine Learning, 10 (1), 5778.
DeRouin, E., Brown, J., Fausett, L., & Schneider, M. (1991). Neural Network Training
Unequally Represented Classes. Intellligent Engineering Systems Artificial
Neural Networks, pp. 135141 New York. ASME Press.
Domingos, P. (1999). Metacost: General Method Making Classiers Cost-sensitive.
Proceedings Fifth ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 155164 San Diego, CA. ACM Press.
Drummond, C., & Holte, R. (2000). Explicitly Representing Expected Cost: Alternative
ROC Representation. Proceedings Sixth ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 198207 Boston. ACM.
Duda, R., Hart, P., & Stork, D. (2001). Pattern Classification. Wiley-Interscience.
Dumais, S., Platt, J., Heckerman, D., & Sahami, M. (1998). Inductive Learning Algorithms Representations Text Categorization. Proceedings Seventh
International Conference Information Knowledge Management., pp. 148155.
355

fiChawla, Bowyer, Hall & Kegelmeyer

Ezawa, K., J., Singh, M., & Norton, S., W. (1996). Learning Goal Oriented Bayesian
Networks Telecommunications Risk Management. Proceedings International Conference Machine Learning, ICML-96, pp. 139147 Bari, Italy. Morgan
Kauman.
Fawcett, T., & Provost, F. (1996). Combining Data Mining Machine Learning Effective User Prole. Proceedings 2nd International Conference Knowledge
Discovery Data Mining, pp. 813 Portland, OR. AAAI.
Ha, T. M., & Bunke, H. (1997). O-line, Handwritten Numeral Recognition Perturbation
Method. Pattern Analysis Machine Intelligence, 19/5, 535539.
Hall, L., Mohney, B., & Kier, L. (1991). Electrotopological State: Structure Information
Atomic Level Molecular Graphs. Journal Chemical Information
Computer Science, 31 (76).
Japkowicz, N. (2000). Class Imbalance Problem: Signicance Strategies. Proceedings 2000 International Conference Artificial Intelligence (IC-AI2000):
Special Track Inductive Learning Las Vegas, Nevada.
Kubat, M., Holte, R., & Matwin, S. (1998). Machine Learning Detection Oil
Spills Satellite Radar Images. Machine Learning, 30, 195215.
Kubat, M., & Matwin, S. (1997). Addressing Curse Imbalanced Training Sets: One
Sided Selection. Proceedings Fourteenth International Conference Machine
Learning, pp. 179186 Nashville, Tennesse. Morgan Kaufmann.
Lee, S. (2000). Noisy Replication Skewed Binary Classication. Computational Statistics
Data Analysis, 34.
Lewis, D., & Catlett, J. (1994). Heterogeneous Uncertainity Sampling Supervised Learning. Proceedings Eleventh International Conference Machine Learning, pp.
148156 San Francisco, CA. Morgan Kaufmann.
Lewis, D., & Ringuette, M. (1994). Comparison Two Learning Algorithms Text
Categorization. Proceedings SDAIR-94, 3rd Annual Symposium Document
Analysis Information Retrieval, pp. 8193.
Ling, C., & Li, C. (1998). Data Mining Direct Marketing Problems Solutions.
Proceedings Fourth International Conference Knowledge Discovery Data
Mining (KDD-98) New York, NY. AAAI Press.
Mladenic, D., & Grobelnik, M. (1999). Feature Selection Unbalanced Class Distribution
Naive Bayes. Proceedings 16th International Conference Machine
Learning., pp. 258267. Morgan Kaufmann.
ORourke, J. (1998). Computational Geometry C. Cambridge University Press, UK.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing
Misclassication Costs. Proceedings Eleventh International Conference
Machine Learning San Francisco, CA. Morgan Kaumann.
356

fiSMOTE

Provost, F., & Fawcett, T. (2001). Robust Classication Imprecise Environments. Machine Learning, 42/3, 203231.
Provost, F., Fawcett, T., & Kohavi, R. (1998). Case Accuracy Estimation
Comparing Induction Algorithms. Proceedings Fifteenth International
Conference Machine Learning, pp. 445453 Madison, WI. Morgan Kaumann.
Quinlan, J. (1992). C4.5: Programs Machine Learning. Morgan Kaufmann, San Mateo,
CA.
Solberg, A., & Solberg, R. (1996). Large-Scale Evaluation Features Automatic
Detection Oil Spills ERS SAR Images. International Geoscience Remote
Sensing Symposium, pp. 14841486 Lincoln, NE.
Stanll, C., & Waltz, D. (1986). Toward Memory-based Reasoning. Communications
ACM, 29 (12), 12131228.
Swets, J. (1988). Measuring Accuracy Diagnostic Systems. Science, 240, 12851293.
Tomek, I. (1976). Two Modications CNN. IEEE Transactions Systems, Man
Cybernetics, 6, 769772.
Turney, P. (1996). Cost Sensitive Bibliography. http://ai.iit.nrc.ca/bibiliographies/costsensitive.html.
van Rijsbergen, C., Harper, D., & Porter, M. (1981). Selection Good Search Terms.
Information Processing Management, 17, 7791.
Woods, K., Doss, C., Bowyer, K., Solka, J., Priebe, C., & Kegelmeyer, P. (1993). Comparative Evaluation Pattern Recognition Techniques Detection Microcalcications
Mammography. International Journal Pattern Recognition Artificial Intelligence, 7(6), 14171436.

357

fi
ff fi


!#"$%%$&'%()**

+,-. /0$1%23,
/.$1%$

46587:9;<9=.9>8?A@B9CEDFG?IH8JLKMCG>8CE?GJ ;<J >E7ONP97RQTSPJ#9>8UFEVRW.J ;<J >E7
XYJ#C V0>89>8?[Z]\_^85`J.VR9;<J >E7:aPNP97RQT7RQ8JcbcdIe[H[>gfGh8a7:J ;

i:j'klmRnRo'pqiRlm:rRs

tuv'w xu:yGz'{}| z'~'}~''u':~0| w

`R'
] _}
Ylj:m:o_#lk}j:m

'#u':yGz'{}| '0| w

G 0IR.'}#
G#.}
.}}'6}



Pl s:j'o' Yojp mR
G 0IR.'} 8G[
G#. }
R :'O}
6jp}l.mPjo'p

#w u''{yGz{}| ' w '0| w

#u'w :yE'w{w u''z0| u}'0| z'~'

ERG` 0.
. E



# :
# [ IYE I}<}E[}` }}
R. 0# } E}8}. }I# [ 8
8 E}'. R EY } 8}8}
[80} 8 E}B.`} [[ 8YEG` G }
[ I8 E#} }I}8}8 0}O
} R R.I0E}[ G R G#q}0# R
q:} 8 0:[ 0}.}8} #} G8 #I
.}P[

R .##
G
ff fi}ff ''
ff '
fi
'
fiff


!""#$" !%fifi

&(' )*#ff *
+ ,
% %!"
-

}
-[
/. !""
fi- ff# fi 0 0 0!1
'
2
fifi&,'!fi
!34 q 5'
fi ff'

,-

-

[
fffi
!6!2
73
+ff
} 0
!


/'


`
&8$
#$
`
9 " ff"

:

#7

,'
, "'
fi ;#$ ,#"

fi
!2'
'
fi ,
*
!* #;} 3

, fifi

<3/ , #4 ,
0 '
fi 2

= ">@?ff
ffB 3
*

`
9 !C>$Dff*3<E<
<3<FHG6} 34IJJKLM,


FOG
N
/34IJJPB & '
,fi'
fi fffifi


#$ [

*
H> ffE7BRQST$UV"QT$W@X QY@YZ) C'



2 -fi
"
= +!
fi ;#$ 0#$8
'
*'
fi ;#$ ff[
->$:
E
<3/[4
}

$37F1.
\ } 3]^^^LG6} 3
_<[
3FH`0! <3/IJJKB &
$%%$Rfi'fib/ R /c
aI

4d: ff-. 4e,
;ffi

} @g/f

fi'#u'Ew u''{
fiffYu'w

A<I +G_ [,-`_<'<& .(!*8 fi2!
,I ff8 2
} ,":
: ) .

-
2E7"'
"
* ,"
/&
>
-Y"W !#,T$%
U $& US
'4(W &#*) (W #,+ff(T -.#(W &2(T -.#0/7Q2
V 1#*) 3W@Y@4Y #,(W &)(T -.#,V"U)5&/W(&.6.7 B

] +Mff
2!- !-!- ff
2
2E7"'

*
0] %8. &
A9 +Mff
2!- !-!. "
* ,"
.

%9 %8. &
A.: 684#$'
2".
q
! E7"'


0'
fi *
* ,"
/&
8$0
"[#
0


GR
!-
)E7"'

&
' -!"#$
- ff!*&
[4 ,
"[
0# /} -!* !
<
;


3 ; =;
3%
;

&
: ,N07&

P %N0/ ! , ) ,[

"/
! >
_4
4?I` 5fi,
,.
2`_<'<&

?
' ff fi}*
"!c[ )`_<'<3/.0
2. ff

fi[ )-fi
'

2`
_<'

.G+
%_4
)I34.
6! "' (Aff
C "' B%A &
8%

30!6
).
% *fi'= ' R
BC5 . !68" fi%!,/
3
!1 ,S+,#*0
) } ,W(/& W@T$W@QT$W(3*ff
#
-fi

"
#$
- 0}

! !

+
<&< , fi "
+ "!)'*
!6'
7&*8_ 3
E + Z* + T# V< + }" "



*!) !
* "

-fiF C [4" [
0
!
-
./
3

" ff4"fi
'0
#$
-
fi} 0-"/ ff#}


ff
)`E .@ !&?1 fi,
".


"#< }

!&4G(
"#< E
G
#7!



0

fi'
0!-'fi'+ !-* ,fifi'
ff#4 "''!
2 2
fi';# ff?ffA"3
fifi

#< 34. /4

0#; &4'
.

#<




0
'
fi !
0,

3< 2
00I 5fi ,#[
#4
`
9 ff'

*'



&
8 " 0
<3fi

" 0 '
fi

:

-#< 0

ff

[

fi
!)0 -

ff'


)fi <
4
7


#"!O G %>@.0
I}


" 0


0fi
!3



!"'


*!,
8 5'
fi B 3

E fi0
6.
+
) ff
}
2.
)
GR ff

,# !*37Y
"'
0#


-[

E #$0
} "

3

GR



!
fi
%>@M,

4FHG
N
/3IJJPLA
'

<3<A *3'
IH0
3 J
3/F ? [
3<IJJKL
Dff*34IJJPL<G6} 37E<
<3Dff*37FH?ff'
3IJJKB &ffM,
"* [ 0# 5'
fi
[
=


<37
!+2 #;# fi

- - _
! 5fi 6
+
!)
5'
fi
[
&28. 3
!
"#,
-

)fi

2 -!fi
!Cfi
&C8 %
` _<'
<34#$I
5fi 3
ff
ff" - fi} ff#] K $ fi
7

fi
3< .
4'
,'

)'
.,&
G."
- ,

,fi
q
!
-'
ff'


)
0#$
"

# L+,'



2fi
"(> L)M,[4 B4
2
#$ [

+> ffE7B0> J

'*F E<
/3
IJJ ML:
E
] -$&3]^^^LG6} [
-$&30IJJKLA
<3 G
3E<

<3 F G6} 30IJJJL
G6} 3/]^^^B 3.0
2 ,'
[
"" /*fifi} --
!?ff8fi "0


+ 0
)*
fi -'

fi ;#$
!)
}

).
)
8

[
*>$A)F

N5O*P

fi '# u'}~'w`u''uw #w '

J/3 IJJKL Dff}
/30E<
<3F L) 3 IJJ MB & L) * 'fi
: !30 <L)M,[H ffE
#$
", "2[ )#$fffi
"
=
+
*fi

#; fi-
-/ 3


) !%# *. =;
16 6fi #"

+'
<& ' 2#

'
+ *#;} - ffE(
-'


(_ '
fi )} #;!C.
]
!C )>@
C-
?ff
ffB 3/
'

ff
2 [

[
*>@.0
2
)
8
"
ff fffifi

B 3

*'

! 2 . ,>@.0
2 0!fi
<
* fi} -

,!" B & '
,
2


#
,fifi} 2
ff fi
#$E
fi
)
2fi


-fi
!+.

+*"

4 , fi} 3
ff
!
"'
4#<

"

&'
EffE*fifi}

" / *= `
9
'
. }

4 #;'


,#/ 3.0
0





,[
q
G

,fi

&
G#$ ' !3/ fi}
4fifi

+##ffE+- # fi}2
5=
["fi " !) '
&*G(
* !)#EffE%

* 73fi=
fi


ff ff'
-
"
+" 5
!*fi"
` $3fi'
0 <3
[
5=fi!
6> & /&37> ?
ffF J/37IJ
J ML/R
' /3IJJPBB &Mff



[
ff
fi =
"* ff
GR ,!'

fi # fi *3
+.0
+ 2L)M,[("'
-.
2!<


}

%.
%+fifi

C#,
% 3

ffER
* C+fi
"
= 2 2!<

fi ;#$ &ff_/
ff!'
'
fi "#fifi


<3 "'
0#

*/
,
!2
"

!% )
[
- -6 %
} .
% 2!*&R_< " 3 C#$
5fi!/ "" '
, 2.
- 8 -#$ff#;'



-!*3
$& .
& } `


0! 0
,
*"fi

5 " } 0 ff
-
5#;
, , 0'
fi '
fi
&
'
fi'
fi ,fi *'

C[
!2#$,

ffE%2fi
"
=- -'
+#0

- [,fi
!6 +2
"
6
}
,.
+ )3 _ 5fi'
=
!*'
[
"
,

!2#4 ,fifi} -
* 8
5 #4 ,`_<'*!*&4?
2
) $34I
ffE%[
!)

"
-# fififi
-'
fi ;#$



>@
$& &3 . ,[
B 8

7#$<

3 '
fi![
4#/
ff





! '

'

! 5fi!-

"/ 3



*#
<L)M,[
"'
7#4 fifi

2 }


GR 0}

`
3/
* , '
fi![
#4
!O
- fffi
7

"fi
!)}

0 2
2"'
$&



+]-'

'
"[
#
-fi
!_
" ,-
* ff"
}&A
I9 5fi
,.
#$[0
2'" 2fi
"
="


R)
: '
(

+! .
C R&
:6'
' 2 +
2#
6
` _<'
R!*30.0
%


P%'

'
+.
` _<'
(fi
"
= )
*

%fi
!
#; 5'
fi
[
!2
2

,/ &


IM" fiEfi

4 G

'
fi ;#$ #
` _<'
, +

-fi
!34
+'
"
0 0fffifi}

fi
` _<'
G
fi

-,>@0
[
#$4'
fi ;#$
fi
"
=

B &



IM-fi 0 G

* ff
}
!-# ,
L)M,[3

fi
'
fi ;#$ #/ -fi
!"0 '
fi ;#$
#/ /
.
=;'


*fi



) "
0
& Hffff ,fi
'
fi

#

'
- 3/.0
)fi'
fi !+fifi
70
3 ffE



!* -
!*
fi
0 ff'
fi ;#$
0#4, fi}

,!*&


fiff fi fi!#" $%& ')((#*+-,." (#/ (01) $#&/ (01)/ 1)1) $/ fi / ($# ')*
$#3245&#*4" "675(989
:fi:;<>=" 8+-fi13?>3896+-fi0fifi/@89ABfiB#
Cff

N5O#D

fi'#u'Ew u''{
fiffYu'w

ASR
User

Dialogue
Policy

TTS
_4
,] ?

Database

6
fi
%#"+ fi}%
)!*&(' +

} 0-/ ,!* fi'
* ,!
7 , *

, '
fi +



)! >@?ff
ffB &<' ! ,} *- -
-" 5 " '
fi %>$' ' AB!*&

0 c
ff
fi

0


0 ff
fiO



8 6-!fi
fi})
-!
>@ .G2
6 =;
#$
+_4
-]B 37 -
fi' !(
"
[ ff fi"
fi3
0#;5=$#$O
3
-'
0'
*
#$
#;H"} =* - ,/ &4'
, '
fi ,
4
fi " ff
"
4 '
fi "

= >@?ff
ffB 3 " !<

0 fi
E


! ff 0 4
,ff 5 =;=; '
fi 2>$' ' A0
B fi
&
'


ff 7#< !1 0
W@QY5U 6.
# UYW@XZffff'

'
.04 !
!+>@
YffE)"
!3.0
+QXT$W@
U &)
2 } B } *fi
0
* ff

&
_/<fifi 3 ff?ff
"'

.
,
'
fi ;# 3
!, 7.
,
,
C fi[
D2>@ YQ &.6S*Q 6#"V"*U #Y
6)QV"V"
Q )B
2'
, ' 2"
<
ff!'
fi #
'
fi )



)"
} ff'
& 8 )


2-
!2'
fi

6

2 "
3
*?ff
+ -2 6>@!fi
!% %) =;
}
'
'
)
'
L+
"'
B*

R6
_
)[
#
:/'
6
R +
-#$'
7& '
2 )


fi
*
fi
- ff?ff
1 &
Hff.G


"
0.0
fi !'
fi 4#/'



#;} "
"


fi
!6'


<3 +# .0
% -

! %!+ ?ff
#;} "
&2'
:<,!'
fi
#4'



37#4.0
*. " , !* )
` 5fi 3<
ff. "
+(W &/W@T$W@QT$(W 3*#" ff!
2.( ff
[
!3.0
0!
!

-fi
2fifi

2-
!2'
fi ='
+ '">@#$* # 2-,
+,#*)"




B0 -

!




" ',> + *Z + #V




B &
'

+!'
fi #
-.


"
P
.


! %'


X U &,$) V"(W &.6
0'
'

# , &4? #$
0fifi
2 ff?ff
("
3
-
*ff

0#$4[
ff
#7
,>#$4
3G
.
E7"'

B 3
! "ff'
'
*.0
ffq
:< -'
fi

C
.
+ - &-? #$ff
, fi
%,I
"_4
,I3#$0
5fi 3
` _<'
"4'
'
0.0
4

5fi

!
X U &,$) V(
ff'
'

/3<
2
0A]
)
9&
` _<'

*-
fiY
!


,

.
) -

34,.0
+
,
[
8 5fi


!
:< , - ff.
ff*:
: +
4.

&4G(
. "
fi
4 0
:<

,
'
' !ff#$
"
#/
?ff

:/'
34 _
!2#$ .1
3 fi'
fi ff'
:
:


"0
# C
.
D2
B
C
.
.+
'
!6'
-'
"
_
fi

!6#$ ff 2>#$0
37'
'
fi
+

.0
0'
"
`
9 !
"fi
#

5

B 3
-"
4''
fi -
,4[

#4!
&











N5O



fi '# u'}~'w`u''uw #w '

?0ff.
''
%' .,3<
) `_<')!*3<. "
'
: % !2
GRff

#$ .0
2. ,. 22Y4# Q)5&).0 0 }" !O



"#$ [ 5
fifi&1A
"
!3,. 2
'
: 1 !C
GR*
+
R.0
R. 2. R
).0
ffY
:< ?ff
='
fi

% ,
37I
ffY
:<*& $ G_
ff

* fi

2'
ff
#

0#







:<

*"



!O'


->@G6} ffFHG(
} 3IJJ^L/M,


FOG
N
/34IJJP
L 0 ,F
L0!3IJJK3IJJJL<A"
<3IJJKL<G6} G
,$&34IJJKB &0?0,-
fi ` 5fi 37[

!6!"" [
:< #;
!#
3
6
#,'
'
!3
2
-fi
'


:/'

ff!1
0'
'

ff &'
, G. ='
'
q


.0

0
0"fi

`

3.0
-
fi
!*.0!. 0.
-"3
*"fi

fi
. !3 ,fi
0#

`

0
* ,
.
# fi

/ &



R
fi :

.



.R 0
8 }


8 R
*
<30. )'
'6 + } *[ !C. +fifi)%fifi!ffE 6
*=
"fi
!)'
<& 82 [ 50
<3/. ff.
4'
'"
2'
,


*#

!-
* "
[
` _<'*!*&

fiff

8ff'<0fifi!IffE" '
ff#
fi
!3
7
. !ff ' ::+ T$QT# 1Q+,#5
fi

*#$0

&EJ!2
,. "
fi!6[ 2 ,4ff"0#
#$

0 ,

#; 0
ff
0#$ '

2.0ff}
* "! + }
5
G


2
2,
"
=
`

!` ) ff & HI,
0
fi} 5=

0

"#$
ff ,
,-
fi0 !O*# [
"

37.0
).

'
* -
)-#; 37 -
,
6!) ?ff
"3<

-"'
73
E
:/'
0 -!" ?ff
"3
-'
fi fi
!"



&48 -fi}
3
. [ ) fi 0
0 ,ff"
20fi

fi
- 0!* ,

ff#
0#4#
.
0
*
#$

!-#$0
-)'



&
G_,
.1 ,'


2# 2fififi
, ff fi} ,Q <YW@X QT$W@
U & # #*&#*&/T 3<
) #$
"
)! '


&







ff

N0

#$ 0 # 3 0!H'



-

-"#7 ff
fi}3 %fififi
}
,2 }"
_} 2 &G_' ::* W@QYU56S.# UYW@XZ,)'-
fifi
-#; , 0
# + T$QT #,+"
* , , fi} ,- 0# QXT$W@
U &+&_/[, 3
fi'
fi "}

62 } -!6'
*>#$ff
3
) * "
6 - , 37

!
" 0/ .0
-/
#$

<


/B &_/4 4 3
1,
X -UW@5X # + T$QT #,+34 2!6'
)"
fi +
2}


+>@
%

#




-

:<

B &,\} 2fifi
*#;O
q

5=; ff*-fi
,}



ff



,fi
!&'!fi
!- !1'






-8 ff'



}
"ff }
[} [

5=; & Hff.
ff
E = -fifi} "
ff
-} E

!)4Y # Q )5&/(W &.6<&



fiff

86fi
34*
*! 8 5fi }


6*!
. !
*"fi
"
= ,
ff'

!-
}
-.
*fi
, *&' ff!

.
,'
fi ;#$H0 #7fi
4

0
<&



Aff -" &0



3 1

(fi " ( $

/(



&#>8-? fi/ &"

1)fi$#fi8fiff 0#ff 8






1)(

!# (#- &#" ' $#fi?fi

N5O



fi- fi!> '>$/ " fi0&3!>" /

fi&/



2@,>/ 1)(98



$# / / (fi



(9896=" 8
:fi: Cff

fi'#u'Ew u''{
fiffYu'w

_/0} "
ff
}
<3fffi';#$ 0[ 3 20) #*'4Q) 3
# 7& *
'
,


fi
0 -[

)Q)!
U 30# X"W + W@
U & )UX5#,+5+0>(L)M,[B4.0

"'
0 *
}

.
0!*&GR

fifi} <3 fffi H#

ff

fffi
!-
/
-8 fi
, 0fi
/fi
!#$0

}



" L)M,[
7
3 !<
7
4 } }

0ff5
"
= 5'
fi . 7&
'
E fi

"#< fi
<fi
E
! 6(W 3*#*&- E
L)M,
[
'
0
E `
9
!-

/*'
! "
"fi"

""> J#0FH'


3IJ
J MLA*fi
F J/3
IJJKB &

2


`
9 *+fi
5-}

34 3
C . "
C
3. 2

2'

L)M,[ #; fi )

&(_/.
%A
]
*$&ff>IJJJB 34. q
%
.


2"* !2
+ ` + fi} -'
"
C!) -! }



fi
*










fffi $ fifi ff
*fifi ff


ff


E



.
*
3' 5


0 0 A$ ` 5 3 ff!
}

3/
+ .
3/ 2 * , G 2
& 8*E 5fi'
[0!
"
7
" 8=* . &4Mff
, 0
2#;H


/ %'
2 6fi

!
- *

6fi


>
B->@'

-fi

!6#0*


)2
3

+ ff -! . ff
+
6



}
B 37 ) . *#;'


> B">@'

* [ 5'
fi 6 . )
73<


* )! . *
R
(%}


B &H_/`
5fi 3ff`

)#"



fi

!-

fi! G"'
#7
[
3
-/#7

3 4 !
. ,
34 /34 6
6

3

'
C!+ Y"'
,#
[
" ! . ,

6 %>$ "G

# 5" B &*'

6



6fi


*
% .
#;'


`

, 2L)M,[%"'
<# , fifi

0
}

.
- ff!*&48$
>$'
fi #;!E
B fi 0
ff'

# ff .0
-
}
.
ff!*&
` 4
"'
4, 8
!
:/'
ff
"
"'
$3
fi 0

! =
H"

* !fi
,}

#;H
!fi
ff 3
-fi # !-
!

[
&28 + ff.37

)/ 2""' # <Y
U )QT$
U ) Z).
6 '
fi ,2 `
2}

&8@#4. 8 !-
*. 2}

#;H[
ff 3. G
'E 5'
fi 0
.%

0#7
ff }

"
4 &0
[ fi 0"4
#$. ,. !ff#

[
5fi!"

/ 0
, } 0}


".
! K&G(
ff

0fifi}
. ff.
7 } ff
2
` _<'
<3
0
0 . ,'
8
5 fi

q
! #;<
*'



* ,}


. 68}

5=; 3
)'
ff2
" ff -
<

'
-. !

2"


,
,-
* & >$G
fi*
2"
* ff
G

5fi

-



5=; ff.0
-fififi
}

)
" !+
G
. )
):/
5 6!) "!


'
& E
B H0 fifi}
"
`
5fi!/ ,fi
&
` 537
)
E
L)M,[3< ` 5'
fi "

* . C>@
3QY.S #B *> B #



}
#;H

*'
8 )
*"#4 =;
ff#4







"


$# !





"

"


!



%





&

-

+



.

/
fi











#ff5=3$/ &

/ &

fi&(#/

"

#ff -'

$/

&#fi8# fi(

/

/



/ (

' fi&(# 89($
!#??#/ " / /

'@-#/ - $

?/ (?#"

/

fi1))fi 3/ " "
(

($#fi1% fi!#" fi/ (98/ 5/ (fi5!>fi/ ?#" 3/ (

1)






2


1) fi&#3" fi8#?#&-/ (

' fi1.3 fi!>/ 1)(#" *@&#!98&

10

&#31))'@((fi ff

/ $

('ff

*,

'@(



(

!>fi(#/ " " / 1



' / (#' fi&(#3fi*

fi


3( 3 >

fi )fi)

!>(# 8 fi*
/ (

!/ ff

N*N5O

&#

!#

*) !



&#3 fi!>/ 1)(# $

fi-&*fi/ '@
/

/ (fi

fi

" fi8-($ &



1)

&#

(/ / (






/ "
fi1

33 fi!#" 3" "9fi fi&" "

-/ 3fi3fi fi3

/ (!/ 3/ " "5?>





!#" / (9ff



'@(9ff



'@(/ " "5 fi
!#??#/ " / /

'@

&#fi)()(/ / (

fi '# u'}~'w`u''uw #w '

State

Users
Utterances
DB access

ASR/DB
Reward

User utterances
log-likelihood
semantic tags

Dialogue Estimated
Policy
State
_4
9 ?

State
Estimator


2!
. R- L)M,[&4' 2fifi
C#, fiC
G

[
.00
0",
' :: 2! 0 fi#

'
fi



*>@?ffAffB7!1 " / ff>@M JB &'

fi
!2'
:
: ff 3 , 5=
'
:
: ff "
* 3< 2
/ ,}

0 . 4 "fi
" 0#' ' A*
0'
:
: , "
*


}
- &

#$.
*
8
+>@G6
37IJKJLA*FfiJ/3<IJJKB,

)*> ! B

#> ! B > ! B/ 5 )*> B



!

>IB

+# !

.0 >
B4
#
-

-"' 7 > B4.
* . -"' $&
E 2^
I-
-2
'"#;} , "
#ff "+2* - +*.6
'
.
-4
-
[&G_ #$'- 4#$`_<' fffi
!- -.




0
.
4# ff #$4. [0
'
/3 1I3 #$< . 5fi'
[
fi 6

&-'
=;
,'
:
: C!)3
\

%[

+'
`
)*.

6*'


C
+ =;
2

6# * / 3QY.
#2W@T #*)QT$W@
U &R
> J#
F '


3IJJ MB 3.0
2

!+fi/ 0 [
,# *> B 2
2 8
=; R
#

" -fi4.0
" 0fi/0!
ff
GR 0 4
'
.R
7& HIff
ff


-
fi 73 0fi
<

,fi
!6>$}
",

6"'
B,
"
%!)
+ -}

+.
+ -5
" =;
28}


* &2'7) ` 5" , `
L)M,[(
-
6} -"'
#0 *
fifi

<3/
0fi
"
= )fi
!2 )5
"
= " , . -
*#; ST$.
) # &
G(
+
2fifi} %
2
!(fi'
fi
/30 *#,

C fi +



-} "
[

)
"
*
= *#0 - fi} 3)"

"
= )/ * fi
!
fi "3.0
*

q
+
#$

2
+ "* +
+} "'
$&*8@#
fi -/ *. -
:
:
3
'

= R "
"
'
,!+ -

**#; 3
*% !%'

C# )> & /&R?ff
3 =;
}
* fi
6?ff


:/'
3/
, !
3/ , #/

'
3 & B & 8. E
*" ff
*
!)- .
"'
0#4# G

*!
6
q"0 , fi} &0G(
" ff

fifi @
+ W@V"SYQT$(W &.62 + -
}

26
]C

C/ R>$:
E
*$&3
]^^^
L 84'
/3 ]^^^B 3-fifi} 6
*6.+
!C
R+ ff` #;!R'




2 0 fi}
*>$A

0$&37IJJJB 3 G
. -
*_4
09& J!2
*"


fi


%6fifi5
2 2
2 3 )"'
#,/ +
R+ C
fi
7

"fi
!*#$ ,
L)M,[%
-
"


`
2'
ff !* /
7&





)
+,





1) !

,

,

fiff

N*N*N

fi'#u'Ew u''{
fiffYu'w

'




2#4
0fifi'
0fi

!)
/"
fffi}
[ !-#$

6
#$["
+ 1 SW@Y4*2
*! "fi
"
=
'
,#;
= fi

-
"/ &482[ $3< fifi 2fifi} -
*
I& ? +fififi
. +[ "#$0
34 )fififi
-fi

#$"
) 3 %'
R+
+fi
!C -fi} % *6+ #


,}

&8 2 !- ,!*'
,
!-
"
,}

<&
]&J
)


5= )T)QW(&/W(&.6"!H # ` 5fi!"/ , ,>@
,
34 !)
[
,#;<
} _

5=; 0
3 } +# *}
,. -.6
}*

"
B &7I
fi
0'

8
5fi!3
4! -
/fi
' '


ff#;'



!&

9&

,
ff
- 8fi

L)M,[)"' , fi}&4'


#
)
L M,[6.
7',"'
- fffifi
0}
2 .
#$ ff
0!O}
&

:/& ? fiff fffi
7
"fi
!)}

0 IL)M,[&
P&

G
fi[0 ff!O
- ff 2
fi
!&

' 8 50
-'
ff ,ff#4
ff[ !-'
* ,`_<'*!*&

.

ff
fi

.

` _<'+
2 =;
[- fi}+
-! "fi
' ,.
+
#$
+


"2)
6`E . !&)`_<'+

-
6* fifi*fi#$ #$ff fi}

!">$E:
<3[4
}

$34\. }3/_<
=
/3F `0! <37IJJJB 3/.
2 fifi
#$"/ #$
'
fi *



)>@?ff
ffB 3 fi}

ff'
'

/3 5 =
=; '
fi H>$' ' AB 3/ )} 3ff
R

6

[
&H
` _<'
( 2 )G6

'
fi +

= 0.
2
,


+'
'

)"'
ff
2#;c
5 =
fi , ,>$:
E
$&3<IJJJL:
E
2F [4
}

$34IJJPB 3
*"' ' A-!



"
fi
'
!
->$Afi0F H0

3IJJPB
& Hffff"

5 .
=;




-


".
2
6 *
L)M
fi
6

%>$:
E
-$&3ffIJJJB &%'
2
` _<'

/ ,
,fifi )#;O
". fi
"`

2
#$

2ff ,#$ =
.
}

!"!'
fi * 4" [
fi3
"
3
3

0
3" "3fi3
3/.

3 2= &ff
` _<'
2
'

5 ,
ff/ "
*
* }

!
!'
fi 3


<3< *
[
ff#4/!+>@.0


* [
ff
,"
/3/#$
<3

B &
8 #$!30 )
` _<'
R

6

"

!

2 ) *
%


!3
}


-
[
0
3 '
fi

!&
` _<'
":< 4 0 #$ E

+>$ %fi
!6 - "
3'
'
fi
6
+ -




B &68@#0 `

*
6

*
73,
` _<'
( -#$- +
R>$
(fi
!( +

B
<&C8@#"
` _<'
%
0


%)
30
` _<'
C"
"
%) 5

>@ B &)G(

` _<'
6
#;!%
-2
3
[

:< *
3
"
" 5
>@ B &G(
-
` _<'
*:
:

)}

-
3
)


0/ ">$
-ff.
'
-#$#
} -'

2

B &_/
!

-


# 0 ,
3 ff!-'
,"
fi ,/ 0
3.0
-.
'
,


fiff

N*N

fi '# u'}~'w`u''uw #w '

Nff

G



` G



[fi
' !fi'
Hfffi'
Mff


Mff 0}0
A!O8



8



L)
5 )8




_4
: 4MI ::

*#8


,#$A!O[fi&

, &' ff *#`_<'*
, #; Iff2I] 0' #$,
/ !&-?0 ) -`_<'+
,#;
!+ ->@
*`_<'+ ff#$ff

ff".
B 3 0
#$

-}



2fi#7 0

,

"
""
fi 5*
* <fi '
6>@M,


FHG
N
/37IJJPLA
'

` 0$&3<IJJKB & (
?0ff
C 37ff[
!2#$,
)
#$ [
"
)*fi
"
= -
=

fi
!+
, ff4fi
}

0#$E
} * "'
'
fi

: 78
& G4 ff
[
" 0
,
8
!*#$ff* 22} " [ ,}


)> & /&3/. "
ff.
"!O*'
- ! C/
! ,D-
2 "


4 3<ff
2 "
"

ff# :
E

$&7>$]^^^BB &7_
G ff'
0


,fi

!

*
3/
* *

!
fi
"
= " ,
`
9
->@G6} ff$&37IJJKB &8 )
` _<'
<3/. "
2 "}



-%IB, *!'
fi -#ff



2+ *.0
C
)
)#$"
6
3

]B,.0
"_
:< 6
*
2
2
7&%'
*fi
0}

-!+!
.
2

" 3< + , 0*}

"'
"
2 ,
&,'
"}

q


"
` _<'
2 ff G
. -
*_4
ffP"
M&
'
ff ff!'
fi #4



" ff!O 0 ff'
:
: 2
*_4
:/3 2
-
"


2# ".
*# "! fifi>U #*&-
0
W() # XT$(W 3*#>$Dff*3IJJPBB ! 3
2 ff!'

fi ,#4
` _<'
) /
*?ff
O> ) #,+ ) W@XT$(W 3*#
&/
U & ) #,+ ) W@XT$(W 3*# B &'

5fi -
6_4
2P) .
`_<'

% ) 2 -:<".)


2 " !'
fi ff#



&-8@#
` _<'
+ "
2'
fi


).
)Y

=


3/
ff
,
6S +,#*)"(W &/W@T$W@QT$(W 3*#> & /&37Nff 50B &<'

=



". !
R.
R%



%fifi3 '
+
)#" )


)

"}
-
6 8 &28@#ff
` _<'
+
% *


2fifi,.
6)

3 !1

E
+ *Z + #VH(W &/W@T$W@QT$(W 3*#0> & /&3Nff AB
& E !6
?ff
R

0 0
0

=
! 0fi

0[




6 2fifi&+8@#,
` _<'
% -2





6.
%

=


* 3
"


6V"W #52W(&/W@T$W@QT$(W 3*#37'

,. ff ff- } " "




*!2 fifi!

5

#$

R> & /&R
3 G ?0 /,I L+B &7'

=


*0
"'


%)

= 2
0
G 5fi

!2[


-



,fifi34. 7
#$

" GR

,
& '
,0.. #4 ff:/ , . 0`_<'
2. !0 0!




"#$ "
)
3<'
0 0fi
ff " G

*
!)fi
'
, "
[

#4/!&





ff

&

ff 0



)&#!#!>



(#/ (&&&fi8


1

' &#(



/ (" /



&#" $



?>3 ($# $

/ (



(&13?>3'5 -& fi-

#ff5= #/ " fi '$##(#/ (0 / (#/ / fi/ fi 2@=" 6 = #/ 83
:fi:fiB#<345&#*4" "36 75(98

:fi:;fiC8#/ !>fi/ (" / fi/ (/ fi1)1)(#" !#!#" / $/ ( !>($/ " fi0&) 1) 2@,9fi/ ( "@ff 89
:fi:fi:Cff
;ff
fiff fi) / -fi&#/ " (# )fi/ (0)'@#-fi/ ?#&fiff
" 0fi" / fi)$#fi?fi

($

&#!#!>-'@'@" " &#!

fi& / (?

N*N

& ff

fi'#u'Ew u''{
fiffYu'w

?G

Nff

[fi
G_ [
ff`_<'<&[4, ! *}5=


![
4 2
! ;
}


* #$4

0#}


ff8
.(&
G_ [
2
` _<'
<E
& .H!)80
fi
!

8
.
" [
fi3

"

3
3<

"
3<"
=
"30fi3 3 .

+

= & [4 R !H
H}

!B[

#;H
0
&
[4 [
" -}

!2!'
fi 0
& 84

, <[



-

[
&
[4 + !% [
)#" )G
. %

!- !* ff

*
<&
[4 ,
,[
," ff
#$

<&
[4 /[
G[
#< G
. ff

!- !* ff

*
<&
[4 ) ,[
2 )


C -!

-
<& 84

" 7[

ff
[
&
G(
[
,#4 ff/!--!-.

.
Mff!-. "
2 ""
/3

,#$
<3
*

.


Nff 5
G ?0 /IA

G ?0 /I,L

?0 ]A
?0 ]
G ?0 ]A
G ?0 ]L

?0 .9A
G ?0 .9A

[ fi '!fi'





Nff





fi'





























fi'




































_4
,P 08



0
ff`_<'<&' :< - fi'
: I[ #


}

fi

)* -fififf
+ " <&-'
_
'
fi

: " -fififf!'
fi
+ "#$ q
+ fi'
: " !fi'"#
7&4G
?

0 *'
, } -
* ff [
ff , ff'
fi *
&

` _<'

` *!- ,}
#$# :<"
} *
3/ .G-
-_4
M&8@#
`_<'2 , [ 5fi

!)
#$!2 *
3

ff
# <YW@XW@TX U&,$) V"QT$W@U&)>& /&3
\45fi ?
#;]#$ff
<3R 5fi
: R!+A]-
+_4
2IB &"?0# 5fi

:<
"
!(



340

! 0<
4 73
"
,#;Rfi &
_/E
5fi 3< fifi0`
:<O "
[

,
<
CMff
)!* !*!*.
--



,D/37.0


, fi} 6!2 '
fi

+
# "
[


>$"
/3/#$
<3
B &8@#
` _<'
*


,
`
!
:<

*fifi3


ff
&/U-X U &,$) V"QT$W@U &2>@ ,` ?
#}

B &
!2#$0 -fifi ,#

)
ff'
fi

C>$fffifi +* "
/3.0

.

-ff"[
B 3
` _<'

!-

ff fi

"#<

0 3

*U #*)QT$W@U &+%3*# XT$U )0#4,I :,
&]ff
} ff.0
4 !H

&







N*Nff


fi '# u'}~'w`u''uw #w '

?G

\45fi ? #I

[ fi 'Rfi

Mff
,!0 !0!ff
,
ff


A.A

[ fi0'!fi'





Nff





` ?
#
\45fi ?
#;]

Mff













!R !R!(. *RC
(











=



!

` ?
#
\45fi ?
# 9

=

` ? #

=

Mff


!O!


B



1




_4
? :<
q

*2`_<'<&*' :<I + fi'
: `[
#0 *}
fi
6) *fifi"
% - ] <&%' -


% '
fi

: * )fifi!'
fi )
% *#$
C '
fi

: * 2!fi'*#
?
7&'
,fifi#$ "`
# > =
:<

B }

*
E
fi!&

ffRoj'k
fiRpo

8jfi:o'

0j:m:j'klRm

Nff ">;NffB
?
->@?ffB
? :/' ? :<[
>? B

*> ffB
'
>$' B
Nffff(> L+B

^3I
I3 ]3 93 :
^3I3 ]3 93 :


!+>0B

^3I

G( ff!O * ,
G(
*
,
0'

*.} -
^3I3 ]ff#$ .,3<[

*3
)
2?ff

:/'&93 :
#$#
5fi


!
:<[
73/
2

:<[

G(

0'

"#$R


.( !-
[
E
0
,0'
) }
G(
q

=


62


6.

G(
ff. ,
*
!*fi
ff


^3I
^3I3 ]
^3I

_4
4A 0# 0 * &
ff 3 *.0
*
ff!1
E!*fi
,"
<&4_/.} #
9*
3
:2
" } *.0 ff "! ff
6 -
ff*
.00 "

37 "!<
G

:/'

) ")>@
#
/B 3< "'0#
[
-! } + 2 "" *
34
% -!'
fi -# ?ff
O,"
!2 2 "#$ ,
&

'
#$4 fi} H

C!6
` _<'
)#$, *fifi "#0
+
"


fi + *'
fi

" 34/
*) -/ 2 fi
!_
- !6
7&
'
2

) - fi} B

-

! )
30
= R
%_4

& H

fi ,#;( '
fi

4
,0
.
=;'


*
*&4'
0C Nff D#
} .0
! - ff 4 ff> ff^3
! "IB %
& C?
,D, '
fi

:
.0
*
,
` _<'
-
E
!*fi
""
-

#$!)>$}

! "I3


ff]3

[
93/
,.
)
%:B & C ?
:/'
?
:<[

D* fi 0 8
:/'

` _<'

6,#$0

+-
#$ff
+
&'

^3I3
6]* fi ,







N*N







fi'#u'Ew u''{
fiffYu'w

. 30"
% (
2?ffA :/'+ &
' ) F9% :C) -.0
? C! D)ICD6#$"q :<
%
<&@C ,D+ } ".0 `_<'
ff
0
6"#$
*> ff^3<! "IB &2C '
D* } "'0#
[
,`_<'
6, } + - ,, -
& C Nff D- } ff !fi'#?ffA
,>@ "'
B,"ff!) +-
2
2>$^
=


3
ff

B &_4
!3 C5
.
! Dff fi .0

` _<'
0'
'

"
0
2 8
,fi0#4 [



6>$ ff^3/ "IB & _
G ""
0 ,#;4'
:
:


<3
ff0
` 5fi 3/.0
2
` _<'
2
0.

- ,
)
->@


B 3 "
!

0
,^ff
#
` _<'

E

0
}

!3
}

! #
8

:/'


* ff
3/E
'
).


ff"
* ,}

!&
G I0
0 fi

_
<3
- ff
#} fi
- 0 fi} ff $3
'
'
!*

fi
!*
fi#;7
#$

-

ff,#; &7_/.
5fi 3

ff # 5fi

!- }
ff

?ff
C
4/ 4
4
#;
3 4,. 0} fi
#$

ff.6#
#$4fi
& E .
3
"[


% 3 4
"2'


%* " fi} - "} 8
_







0, fifi
/&'
ff ff# C /
I"'
#7 "
E
! M]3

fifi



"# L)M,[+"'
/ 4
4 fi .
'
fi 0}3



"
-

,/ & % '
fi} .

= ,
3 ""

$3/.
} ,



'



" )
* ,
ff#
#
G

5

37
q

:<

2'

=


0 *
-?ff

:/'
,
*3
2
!
.2>@M,



F G
N
/37IJJPLG6} 0$&3<IJJK<LE<

<3/G6} 3/F1G
37IJJJB &
GR
- , ff fi} " )}

`
,fi
!2'
:
: 73<.
q.('

UYW@XZ
XYQ +5+8 5fi )
2E
5'
fi
[
3<'
:
: )*'
" , 0#'
"


-fifi
0#;
,
2.0
2 ,! 0

"--fi
3/:/
5 q

&ff'
, }


fifi
% fi
%
` _<'
-

)fi
]
! *\8 ? >$\45fi!)#$8



+

? :<
B4
G
. "
_4
ffK&_/0
}

5=; 3.
0.I

#}



&*>$'
"}

q
,
)#;} - , "
G

!)
'


: %0fi
4!
*
%fi
3 C *
R
6'

& B
) 2 E:
]
5=;
.
R]6}


q
} <3 ) G
"'
#"'


6fi

)
C

*
2] K $ & 8
fi
*.
- 8ffE6[
}
!-'

'
6
3 <
0 fi"
2
fi [

"fifi5
!,fi
/fi
!
"
4


,
4:
# ffE*fifi
-G 5fi!


-

&
'
fffi

!
-_4
,Kff

2!-.
-8

ff#!1





.0
, *! ) ), 2#$,
6
3
%!6.
6Y

*#

:<"
ff
fi!"
0
,0 E 5
.0

!

ff
#$
&_/.
5fi 3



7 .0
0 0 !
ff ff5> C Nff D, 0
,^B 3 0!H8

0#
" 0!1

























#ff







&/


/













fi


0

&(









( $#(

' fi1%

( $#(

fi8-







+

fiff

&!#&

/ ( " &$#

7fi $( $#fi

" & / (#

(fi

)!#!#fi/ 1)fi"

!>" /




B#ff

/ ((

fi8#

'@-)fi

3/ (#/ / "-$/ " fi0&

fi

/ " " ( fi

fi

B




(

B)B



fi

fi0(#/

$

/ (0#8-?#&



B #ff



!/ / ( ff

fi& fiff -

&#/ ()$/ " fi0&fiff

N*N5P



fi

fi / fi $

$# #( $

1)!#!#/ (0



31)3/ (#'@1)fi/ (989& fi-
$#

fi1





(fi-/ ( #&(



!#" fi890 B

& 8>" "5fifi -?> 0/ (#(#/ (0/ B

&#ff

" (

$# fi" !1)(# 8

fi-fi'@ fi&# ff

&" "

B)B



!>!#&#" fi $



fi(#&"5$#fi?fi

(fi $

fi -fi



fi&" "

:#ff -$/ & $?> fifi8>3 1.& -/ -!>fi/ (
" & 3' !# fi/ &3fi/ ?#& -'@

(#"

?/ ( $ $&#/ (0 1



/ (#"

2@fiff 0#ff



&"

' &&#3$/ " fi0&

B




3B

!>fi/ ?#" )/ (
B




B



B C

fi '# u'}~'w`u''uw #w '

B

4



/
*

fi

B

B

B

0




B

B







B




B







B




B













B













B












B












B












.


B

B

B



B

B


B


B



B

.
B

B

B
B

B

B

B
B

B
B










B

B






B

B






B

B






B




B

B






B




B

B






B




B






B




B












B

B












B

B












B












B











B

B











B

B











B









B














.
.











-

-



B





B





B

B

B



B




B

B
B
B




B







B




B

B

B




B




B

B








B







B

B







B

B







B




B





B








B



B

B






B

B






B









B




B












B




B
B

B















B




B




B




B

B







B

B




B







B






B

B



.

B

B
B



B

B

B

.




B







_4
"K 4\8 ?

45/


ff






B


B



/ (45/





ff- / 8 ff

fi ff58 +->

4(#'8 ff
4(#'8 ff
4(#'8 ff
4(#'8 ff
!" 8 5# fi!>4(#'

!" 8 5# fi!>4(#'

+->
/ 8 +->

fi ff5 8 +->

/ 8 ff$%

/ 8 ff$%

+-A / 8 fi ff$%&
+-A / 8 fi ff$%&
4(#'8 '$
4(#'8 '$
4(#'8 '$
4(#'8 5# fi!>4(#'A
4(#'8 '$
4(#'8 '$
!" 8 5# fi!>4(#'A
4(#'8 '$
!" 8 5# fi!>4(#'A
4(#'8 '$
!" 8 5# fi!>4(#'A
!" 8 5# fi!>4(#'A
fi ff$%5 8 +-Aff
+-A / 8 +-Aff
+-A / 8 fi ff$%&
+-A / 8 fi ff$%&
4(#'8 5# fi!>4(#'4(#'8 5# fi!>4(#'!" 8 5# fi!>4(#'4(#'8 '(
4(#'8 '(
4(#'8 5# fi!>4(#'!" 8 5# fi!>4(#'4(#'8 '(
4(#'8 '(
4(#'8 5# fi!>4(#'!" 8 5# fi!>4(#'4(#'8 '(

[
! ? &4MI ::

0#$ 0# ff
*
*_4
&

fifiIC [4- !+ 6}

!_[-ff ! ;
}


*#$,*
"#ff}


8ff.
& D) ,



"fifi2C5 .R!-8 fi*!,D+?00 # 5fi3'



:<

)
-ff #$ .0
) C
,D-# "
-I&ff8 ) " 3
-! _
[

:< *
-
*
+#; -?ff
"37"} fi"

0

2 )" 0
- 8 50
&

N*NfiD

fi'#u'Ew u''{
fiffYu'w

'7I
50fffi
fi
!
fi
! \8 ? 3`_<'[ )Q&UV"YZ"' .
.ff}
4#$.0 0
5=; 0


<3 5
"
=
[ 5fi
- -"

"
=5=

ff/ fi .08
ff
L)M,[)"' $&4` /0 "
=

#$}

3 fffifi
-_4
P, <M" '
28 0 G
#fi
}

-
&


[
,-0O
^-I,^^^^^
I"I,]*I,^^^
Iff]]*I,^^*I
9]*I,^^*I

:-^^^^^
%

?G


'

G .

Nff 5
` ?
#
\45fi ?
#;]
\45fi ?
# 9
'R

A<I

^

A]

^

=



^

A9
^

A.:



_4
"J NG
,
,
*_4
I&
_4
"J
,. "

fi

! ff
2_4
"K
0 "

"

_4
-I&\} 2.R

ff , ff 0`_<'2
0
<3/ "}
5 2
2
0 3
fi

2 )
)_4
2I3
+ . +

7&*'
"


fi
-`_<'
%.
:<-fi"+
%
+I&C
` _<'

5 2Nff 5 >$
ff G
. )
+_4
-K3Nff A)
")fi
B 34

2 ":<,

+_4
2I&
? #$0 - , , fi
37 5, " fi , ff`_<'
68
.1 ) -
2
) }

!*

.
)
q

:/'
37!)
2`

=


" &
` _<'

% - 2` ?
#,}

<3+


-fi_
:< )}

!3
.0

0 "fifi4"'

7&4'

- fi
`_<'
"
E
.%.
ff
"
-
,>@


B 3 4
!


.
)


:/'
)>@


2. 0
).
)}

!)#$ , :<,
B 3
6 ff -


*
!)
"7& '
"
[
-
` _<'
, -\45fi ?
#;]2}

<3
q


:<" ,
ff.
* ,
2
` _<'
)
3/
* ff

0
<&
'
"fi

*#4
[
,
0
"
0 #


<3/.0
2 ff
` _<'
2 ff:
: 7 3
.0
*
fi';#$"" 2}

BC R
' 4 D(> fi

+
!
+ */ 34fi

, 0 , 3< 2
- , -fi
'
" . /B &4` ff
2
` _<'
<3/
. *
0. !0[

^
5 fi0 ff"
7 3 G
. -
* ffE
*#4_4
,J&
_4
-I^
" 0
` _<'
2

0.
+
* \8 ? fi
!
&` 4


_4
0I0
)I^0

0.2
W #*) #*&/T

,fi



+ -\8 ? fi
! &-_/I
5fi 34 6
` _<'
6'

+

"
+ ":<
0
*_4
"K3
` _<'

5 ,Nff 5(
*
ff


)Nff A-
* ff &

0 fi


fi ' 8



G_ 5fi'
[
,#$ff )

) +
2ff!*&'72



)
34.
fi[ C`_<'6
) -\8 ?
*fi
! "'
'




fi
fiff5+-

" "9fi5(#"







5'@ fi&# 5/ (

&#(#5fi/ ?#&

/ (#'@1)fi/ ( 0fi$/ (0



!# fi/ &3fi/ ?#& ff

N*N









-fififf fi8-!>fi/ ( fi





(#/ (

fi '# u'}~'w`u''uw #w '

A<I +G_ ["*`_<'<&ff[4" !2 2}

!q[" ! ;
,}


*/#$0
ff#



,8
}
. &
,I ff8.2
} ,"

ff

,
,
2A 'fi ,
0"
/&
> 1UST <ST %'4US4Y *Y"W !#"T$U-(T -.# U2
U -"W + T$
U ) W@X + W@T #,+ff(W &I+ T$
Q &-U # -W"+ T$U) W@X B

] +Mff
2!- !-!- ff
2
*
-"= .

0] ff` /&
A9 68
.(0" [
fi3/
"
3
3<

,
3" "3/fi3
3.

3< *= &[4 , !-
*}

!q[
0#;H
0
&
%9 ff84.*
} ,"

0"

,
&0> (UST <ST %%'4US4Y *Y"W !#,T$E
U 3"W + W@
-"W + T$
U ) W@X + W@T #,+B
A.: +Mff
2!- !-!- ff
2
*
-

,
*
: %8. &

P 6[4 ,
"[
," ff
#$

<&
0P ;S +,#*)%+ Q*Z + &/U(T -(W &.6
6[4 0 <[
0


4!
-
<& 84[

-, <[

[
&
%M 0

,"

&ff> UST <ST E-QT -"W + T$U ) W@X B
+G(
[
ff#4 ,/!-!.
.

ffA 'fi &ff> &/0
U UST <STB

K +Mff!-. "
* ,"
/3
2 ,#$
<3
*

.

0K %L)
/&

J +Mff
2!- !-!. "
* ,"
.

0J %8. &
A<I
^ 8 #$'
CII

-
"
+E
` . !+ , '
fi )
+ -"
/&*'
":<09
& & & G+2!-
} "
" *
,I^ ` /&
A<II ' -!#$
* ff!*&[4 ,

"[
ff# /} *!- !

;


3 ; =;
3
%
;

&
,II 4J7&

















_4
-I^ ?I 5fi,
".
*`_<'<&

)A
<:/&G_" 2 ,
ff-
6 `fi

L)M,[3/ 2 fi 2
fi
/

fffi
!-

L)M,[R>$4'

'
*
*


E9B &8

". '
'
8
5'
fi
[
'


% 6 - %

2fi
!&68 6 5"

+. *fi
#;
,4 *fi
!*

.C 4

fi

fi

- 3
fi ;#$ ,[
'
,. G ,"fi
"
= &
\45'
fi
[
4 ,. "?' FffP
' fi
! [
"

+.
) -
` _<'
+fi &
'
-. *P :2 ,#$ff

6
%]I"#$,
/&)A ". -

%)


- -
fi . ff
-#$4
'
3#
\
2ff:<

3
`
5'
fi =

".
) fi} 2

!"& $ '

) 0. "
#$[
60 '

'
)#
[ 5'
fi
[
ff ff`_<'
)"
G

"
ff'

0/
* [ 5'
fi
[
3<
* 0#
. = 2



>@ ff?fffi'
fi
52?ffB &












Aff

/



&#? fi&(#



'@

ff



("



/ ($/ fi $



fi) 1



!>'@1)(

N*N



$/ $ (fi)$#!>($

/ 0(#/

(#"

(



( '

fi'#u'Ew u''{
fiffYu'w

' )I& 84) +,["
L)
.G-+-
!)#$<& -`_<')
::)""O"/&
' 2]& 84)

? fi'2L+!2 +. 0* }"[,#;
,+ q
q
&
,
` _<'*"::* .0 !fi
0&
' E
9& 842 ,
)
2A '
fi ,#$
!-
! 0ff [ 0 *



"

-
&*'7/!)!+"#
+
&< -`_<'+*::6,.0ff!

- ff
ff"
/&
' 2:/& 84-# 7
!* 2. [,.

-
* ""
/&?ff,
!.

"!-! ff
2E7"'
*
' P&4? #$-/!-#. ?' Fff'R
-_4O[4/3!.*
},- 5
.
2
2 , & "`_<'2"::)
#
0
,fi
"-" .
_4O[4/&



' EM& 84*
"
! ?
!3< 2. - fi'2 "#$Y !
,
ff.
0'
#;$&?ffff , !*fiG!
_4
II 4'
&



M,
C R

R (
/30
(*#;5=$#$T
*.

`_<'2 fi , ,
52fifi

)
*_4
-II&_/ 5fi3/ , 5
!) - ff
+_4
2I. ,'
:2
+_4
2II&-A "_} ) *'
fi
%!

++) fi*. %fi #$[
} 6 C>$}
2#; 2
5'
fi
[
.
fi B 3 +
` _<'
-#;H
`
9 ,fi
&? I2#4 ff /3
` _<'
) } -#$
# /} ,
"
#
5'
fi
-> & /&3
0.
:,
_4
,IB & '
fi" 0fi

,:/ *4ff 4 !,

, . <3 G
. ,
"_4
,I]&[
0 fi
4#$



+]- " G
. <&ff'
. ff- ,:<


C> 6U*U
+ U + U 1Q B0 fi'fi )+I3
^37 = I37 '
fi
!&-_/0





3 ,

6 " 2#

[
02-P-fi
ffE<
} ff 2(> } /3/_/ 3/F
#$73IJJ]B 3.
* fi

> + )U &.6YZ)*Q 6) #5# + U0
V #*'-QT"*Q 6) #5# &#W@(T -.#*)**Q 6) #5# &/
U )<W"+ *Q 6) #5# + U0
V #*'-Q0
W"+ *Q 6) #5# + )
U &.6YZ
W"+ *Q 6) #5# B 3.0
2 ,fi'
fi 2P" )I3 '
fi

!&
?0,
6!+Afi6]*# `ffEC[
!)'

'
%
6


937. ":<"

)Q(W &/(W &.6)
6#0 *!*34
+ 2\8 ? - fi} 2 %}


C

-fi
+

<3 " )
Q &UV # <Y
U )QT$W@
U &<
& J!+
,. -[

) ,
6
!)
#$.0
2. "2 '
fi

: 6[

"#!O}

3/ ,

-!c
"
"!
"
+ 2. R}

.
%'

#$ fi

!&R_
G 2
fi
= ) -#;}
*.
. '


%
%*. !) 8 % " !)

*
%!

[
5fi!2

)! . ,



!6
*)2
+ 34
6'
fi "

ff
#;R
fi

*#4 !- " ff!H.
'
)'
fi ;#$*&4E
`
3/


,
fi 0Y" ff
ff"
fi `,* "!*3/

) ,!2

5GR


! 5'
fi
%"
fi -

-fi

2>$ff
/
%!* -
c
5fi

B 3
.0
" G
5'
fi
+"
3:/
5 73/'
"


fi
!&



ff









&

N*O





fi '# u'}~'w`u''uw #w '

[4,fi'>@
B!# /} *
E
<&0>6UU*


+ Uff + U

Mff
*!` fi , ff - * ff
#$
*!Y ' C>Z#,+

1Q"B


&/U"B


8-
G
<3
0. !-"::2 ,fi}, 084. 7&
8-
G
<38 .(.008# 2 !-E} *fi
0
* ff
&
8-
G
<3`_<'2''2.0084
7&

J *-,!E 5fi'
".
*
-`_<'<3/8 2,`_<'2 !-,::2
fi},".028 O. !,#;O,!` fi &
_4
-I] !&



'

,fi#/ 5fi'
[
09II. fi
0>4

fi )< B 3#$.0
2`_<'- * ff,# * fi


5 -}
&4'
4 "

-290 *II4 3

fi'
!& 8-

3 I"'# fi fffi' #$ ff


4 [


^*I,^^"^^^
Nff "IPP
Nff 5 "IPM
I,]^^"^^^
?0 ]A ffJ 9
?0
]
]
I,]^^"^^*I
?0 ]A 9M
?0
] %:K

)/ -
" ff - }


*[
+# 5fi

) 62#;
!
%}

2



6'
fi ff &,A
"
!34 4 , fi} 3
) "#;} ff
.
!+. 6]2}


'
fi ff 37fi
%-/ - fi "fi *&*'
,


fi ' 0 fi
/

fffi
!
-

ffE-
'

,4
#;
!


2 &4'
0:< 0
*_4
,K3 ff


#$
!-

3. ,"
#;
!"

ff>@.
09II

B & HI!-K 4.
#<0


. ff

2 0 )I^"
[
&
'
-/ ff. - -8

Gfi


L)M,[&?0. 0
ff[


73
[
. # fi
"
=
0
!" . ff#;'


" "
ff

fi

"# `
[
fi

<R
3 [lm:jp
R 0oklRm437 0 }
ff
2
2I"
#
` _<'



" "/

5} !2 -
ff '
fi

: %
+ " *'

fi

<34
=
.
&2A
*! [
C'
*
+.
+.0
6# -
5+ ff * ,.
fi
/37
". ,fi
**
! fi"#; ! ,.0
,I
,
4[
fi /
& JI
! C fi
Dff. [

"


0>$}

!
!'
fi 3


<3 "
[
#</!B7ff 5} 4
'
fi

: -
" ff'

fi

"

"

"

2. 2fi & 8 2
0. !'
3 } 2

-

". 0
!2'
6!

Iff
* ff#48
fi 2 /3 ffIff .
&4_
G 80
0'
:
:


*#4
fi

) 0 " ff
)
2
!2 "/ 8
ff
*
'
fi

:

E
& G 5

#4
ff . 2[
3/ . 4 !'
fi 0#[

+ ff'
) ) . 2[
3/ ff
2
* [ 5

<&
_4
!34. fi + fi


*fi
!6
+
" L)M,[(
=;




+> #&


I9B &4'
,}


G



ff )fi
!) ff
2#;}



















$,

NN

fi'#u'Ew u''{
fiffYu'w


-_4
ffK&4` E8
ff. 4:/5 -#$4 / ff>& /&3CI"I:^,^"^,^DB 3[

ff =; ". "
'
#$ -

<&"' 3: ).0+
2 "
fi
!37
` _<'
*
7[
[ G 0 "!*' .
*}
2fi
&
8


!3 -fi
! ! 4 fi
#<



0
,'
".





3/ -} " G2[
"

5 *!1




,.0
*
,#$
-
&
` 3. 3 4 '
fi

: ff} G*[
"
GR4.

,> & /&3!(





#$
ffI34 !"

5




#$
]B &7GR
" '
fi 4G
:<

<3
fi
fi
!"
ff

!
:< . :

:/'

&?0
<3.
3 fi

.0

:<

6'
[
'
' !+
GR"} "
*> & /&:
3
:/'
*
]
#$"
)I3[

[
". ,
#$"
]+
9B 3
%+'
'
fi -

# #7 0 0'

'
E

:/'
*> & /&34
*
!B &4'
0 0#7?ff


:/'
!ff

fi
!,
" 4fi

"
,fi
4fifi}
3 & /&>$`

"

FHD0!
$&3<IJJ MLE<
2FH[4 <3/]^^^B &4
` _<'

* *
:
: 5=;
)




- -fi
fi
!6
" 6
'
+Y
fi

+#0] K $ fi
5fi!+fi

&
J ff



"

:<

*
ff'

'
*# ff

,.
ff"fi
0#$
` _<'
<&4_4
"I0
0
5fi ff

,
0fi
<fi
!&

,

fi

.

# ]
fi 8



_/, -
+fi3`_<'6.
fi[ %+* +>.H' "


B
fi
!&]I, 0 ff )'
fi ;#$[ ) " [,
52 ff +/
*

/3


CI] :Y fi ,

&-'
-fi

! fi

ff# -fifi 6[
!

3
# 3 q 5* C





:
+#, 2
fi
[

C )
!
fi
"
= )[
->$
!- [
fi

B#;H ff

" fifi

&8 -#;} 3
[
fi

2 [
2! [lm:jp
R 0oklRm+

3#; P
] H
-


2M:H
-
/&'
0#$.


0 ff'
-" ,
!
#4
3. 7
7 2 &










R

fiff

Rjp}lm:r_k}s:o_.ojp m:o'n

:l}]k



k}s:o

ff

pjlmRlm:r

:l}

')I
= "

) "
)fi';#$ #0`_<'<3#$ff
[ =


6[
q
& G "
+ <9II

+

3
` _<'
% C
"_
!
fi

0
ff\8 ? fi
`
! &8 "I] :ff
"

3
` _<'
* -
ff
fi
!&-?0 ) %fi
!+. fffi
"
= +#$0
!) *
"[
[l
m:jp
R 0oklRm437
!*!'
fi 0#[
ff
"'
+ 2` "

"!"
> & /&3/ *
37


!R
3 `
9
!34

!R>@M,


FOG
N
/3IJJPL7Dff
$&3<IJJKBB &7_
G 0 , ff'
fi ;#$
0# ff 2fi
!*.
- '
fi
ff

4 . *[
" 2
"'
#4 fi
. 2[
. ff

fi
"

= , ff !H#$ &
[ fi,,",
fi "
= 6
6 ":<,.). ff#0' +I&
0
8 - 0:< .,3. 0

= ,'
fi ;#$
#$ [lm:jp
R 0oklRm6
. -[
3

6
) fi
)

<&'
-

,
#
" . +[
+} ff
9II,
ff +
* " "
= +

*!O. 0^ ^ :K2>$ 4 "

ffI0-IB 3.0
ff 0
ff#7
[
0[
ff} 4 "I] :,

0
"
6 ff! . ff^ ] :/37 2
fi
[
ff ,,2
fi =;
-# ^ ^PJ
+- /









N







fi '# u'}~'w`u''uw #w '



jfi:j'klRmO6oj
fiRpo


J
! ? fi

G_ ? fi




pjlm

^ ^ :K
]
] :K
^ IK
9 9K
9
9 :]
] K



^ ] :
] IK





9 P]




















9 M:










]
]



jfi:o

^ ^PJ
^ ^]J
^
^ 9K
^ :]
^ JK
^
^ PK
^ PP









^ .
:
^ IJ
^ ^
^ ^I
^ ^
^
^ IP



]
^ II
9 9J




^ ]]M





?ffA
G_# /}
\!
G( !
` _<'

2'
'

G




#o'k



'-I '
2 ff fffi';#$ "#$

+[ &,' ":<G
fi 2
GR -[

' H>@* 5"#$,'
B L 2 =

2
ff " ,
"# "[
"
)
2 "

*/ L/ ,


+
" - "
-
6
+ ,/ L "#$
+ .
"
GR '
. 2 , ff

"
2 "
2

->$-fi


`"'


- C.
.D/3.0
)_


Y"'

- C DB L :#$
Cfi


7



: "
,
)
- ff /* =; &



.=; fi" =; 0 0 ,[ & * '
, 8 fi,* )
fi [0#;
-P]c fi
)""- ,

*
,-EM:c fi
)""-



&
'
,
).(#' -I, . 0fi';#$
"
fi
#;

--

#$ff ` !% %[
`Poj
R 0oklRm KY
& Poj
R 0oklRmC
-2

5



ff# E
fi

ff 7
4fi
}
*
#<
4
.
R
7
.
'
3 0
,
0 #7 G E"'
#4
& H0 .
3/

0
0.0
2> & /&3 ff !0
C E7"'

,D* ff!
0
CL)
G
. .DB 3


0
= I&'
0"


,#$
" :
: *[

.
=



#$

,'

". .
.

"
, / #
fi "0 3
.0

=
2 . %[
, *
#$

6'

C. "fi 3fi'fi

C
C) " #0
-
""#$"
-#0 2 . 7&%'
*




6 2#,. fi

R. )I ]R>@.0
2 +

)
ffI) 9B 3 .0
+
,

2 ". "] IK&*' ". *
-*
-
fi
[
3
"
[
-



:

ff -^ ^]J $&*_
G ff -fi
!6
6!)fi
"
=
)


L)M,[R#$

`
! fi

6>@.0
*.
fi [
2
* ff !B 3
* ,fi
!2
2!
fi
"
=
- ff

E
L)M,[+#$. [
fi

6>@.0
-. E

fi [
/B . 0
!

"
3/.
*
!- !*"

GR 0
2}

`
&










2


#ff4( fi(#/ (" " fi89)!#* " &
( ff
B)


#ff5=



!/ 1

1

!#fi/

/



(/ $# $









.






ff
fi





' " ( ff B 3/

/ ($/

fi

#/ )/

fi1

!#" / (

fi/ fi

(/ $# $

?>



fi/ /

" " / 0(#/

'5)fi/ / "5($>ff



-/ (

fi/ 8>

(0fi 5'@5 fi">$/ >(# " &fi/ (

(#"

" $# $

1) fi&# #/

N



fi1

!#" / (/ (
/ (0"

&#" $





1





(# 8

1

fi

8>?#& fi1



#/ " 3!#* " & "
fi)$# / 0( $



/ ( $!>'@1)(

" 3 fi?> ()& $)fi5&#5 $



1) fi&#fiff

fi'#u'Ew u''{
fiffYu'w

' )[ 2
C 2
(.,3qi234
2

%#



[lm:jp

0oklRm4&

R

. 3
*#R
" , 3qiB

!&8-fi
3

Hfifi5
4 fi'

}} !ff#$ / )!3
. fi -!

qi


*I#$0} [
ff *& P #$. !,.
'7&' 3
#7 ff
,
.

Y0E7"'

2 ""
/3< 2 ,!
ff ,/ ff#$


!-
2`E .B !*
* ,"
/3 [lm:jp
}
R 0oklRm = I3Poj
R 0oklRm "I3


qi ff]&6' 6I- . " " 2

-
2#0?ff

%#; ] :K*/



-*] ,/
*
)>@.0
, "

ff
,^"E9B 3/"



:
ff
fi
[
>
^ ^ :B &"?0
<37
"
fi [
ff
) ) %fi
!% +#$0

. E
fi
"
= 2#$
qi2&
'
" [



'
+-#;0 *.
U 1 ,# XT$(W 3*#- . )[
3<
) "
- . )
"fi
!6'
:
: %"#;'


+# ! 2
)*

34
_

+'

fi 2
!-#;H
0/&4_
G 8O
. 5"
".('
fi ;#$
8

#;H


- ff.0
)- 0%
# +
1 ,# XT$(W 3*#ff

!+[
->$fi
'
+!2
) #$.

}
-

B

'
7#
& G< } -

, ". } fi

)! ff.
!)
+_4
)I]&-'
-[
`Po
fiffoo'n
Rj}%
"
+#; ":<


+


-
!(>$ 2 *
ffI*CIB &C'
2[

Ej#
3 s:j'k k j
3 ff fi0m
fi0mRnRo'p}k Rn(

fiRo6
+#; -,#$


2>$ *


*I
*PB &0A
. "
_
fffi
"
= ,#$0
!2# ,

-[
3/. "_
+Q ) W@U ) W
5'
fi

,#$ff
fi [
,ff'
/

<&2'
-ff:/
*. ff#0' +I . ,.

#;} "

,::R !6

!%



:
`


6 )[

6




%#$
ff[
& . 3.
*I

ffV"
U 3*#,T$U(T -.#,V"(W Y4#I5GR
ff
#;}

-# 0Y
5 [
!2fi


, 5 [
q
!

ff
0" !*
! -
) ,

!*&4_4
*I 93.0
* . 8
ff




#4 ff
0#$ * ff
* 2 !"#$ ,

,[
3 .

fi
"
=
" 4!(#$
fi

-[
3. 1,


!

#$ -.
. !0#;1
#< 0

0[
3
. 4
[



&(?0 C. ) q+:<A
5fi


%#$"
2fi
[

<3
`

!1>@

0"!
-'

0#$ P"

"[
B

. !&
8 6 *3[
fi

ff -
*'
"
%
fi
[
"
% *fi
"
= R
fi

%[
3 %)
fi
[
"
6.

=;fi
"
= 1>$ /B,

&8
[
3 0 6
.

!*



:


#$
"'
#

=;fi
"
= )
"[
3ff -

*"
0 ,"
[5GR &





































ff

o'}k


0o'pklo

8 %

6) - =;
'fi''*fi';#$ ` ,#;


+2
+fi
!
"
73 *. *) =;'fi''-fi';#$ &)_/I 5fi3
. 26



: )
}

5GR 2' . (fi
!
( >$fi ff& ^IB-.0
('
fi ;#$



(
. `
R#$ [lm:jp
R 0oklRm4&
G )'
_


6 -

'
) 5GR -#

5'
fi
6.
C +!
%fi
).6
* q
6 )'
fi ;#$
fi!*` 5'
fi ff#$
!*. >$DffB
ff$&3IJJKB &
" +fi
!










ff -&#- fi!>/ 1)(#"9$# / 0(



'@



!>" /





fiff

($

=
fi>ff

&





(/ $






'9'@



* (" /

'



-/ #/ (#*0&#!

/ (

N




2

3



3C




'@





fi1

!#&

($



/ (#

?> (#*0&#!
/ (


2
>

3?> (

fi '# u'}~'w`u''uw #w '

>$B

>$B

0.7
Train
Test

0.45
Train
Test

0.4

0.6

0.35

0.5
0.3

0.4

0.25
0.2

0.3

0.15

0.2
0.1

0.1

0

>B

0.05

1

0

0

1

1

>@/B

0.7
Train
Test

2

3

4

5

0.45
Train
Test

0.4

0.6

0.35

0.5
0.3

0.4

0.25
0.2

0.3

0.15

0.2
0.1

0.1

0

>B

0.05

1

2

3

4

0

5

1

2

3

4

5

0.35
Train
Test
0.3

0.25

0.2

0.15

0.1

0.05

0

1

_4
I*9

2

3

4

5



R}0_RG}

# 0 #
8 RYI '
0#} [ }} .






N


fi'#u'Ew u''{
fiffYu'w

_4
I,: 8 }
5GR -' . % + Rfi
!&(' )[ .O 2
!
fi
) #$ ff
5- ">@
- ,' !*. ,fi /B#$ ,
2
2fi

&,'

, fffi
!)'
fi ;#$
,
,'
#$ , #$
.0
) -
%fi
!R'
fi ;#$
*
-'
"
+ -:<.2 3fi



'
" ff 2 0fi
!2

!2fi
"
= *#$ 5'
fi ff &

. * CC
5C .
]} R 3
*
2fi
+ - 2 fi
!R
*
!
fi
"
= #$0 5fi'&'7I 5fi0
!fi
3.

' *0 fi
,

.
C
,D6>@ "I 6]Bff C 5'
fi D6>@ 9*= MB0&"G_,#$'+ 0
fi
!2
)
*#;} 2- , 2



:
ff
fi
[

[lm:jp
R 0oklRm+#$
5'
fi 34

+ Y"'
,E
# fi %

"#; :.M /
+

)FMJ
/
)
C>$fi ff& ^^IB &*8 _
3< -. "q

=;



:
' /

2#$8


>@
MM*3 ffPP *34fi ff& 9B &+8 6fi
3" G
. +
6_4
),I :/3 * "[

. * ,
2[
#$ ff:<0. 3/ff

#$ " #$ & '


"fififi
,#$0-!O ff, '
fi ,
L.
+
,' [ " 0
!
0fi

!) )Y
!
" 3 ff!O"
G 2'
"
7&













R

Rjp}l Rm





fiff

k Yj:mRn`o'lrRm:o'n

:l lo'

?0 2 ,fi )-#;

- 2
fi [ #;O

*-
/37
fi

"


%

%2 #,fi

*
6 2\8 ? *! '2 2'

,#$ fi

*" )fi
!& ?1" ff /2
,.)'8 =
fi

- ff !*'
0 .
=;'


2:/
5 )fi
!
& .
3 ff

- [

*

R
3 0"
3<0.00 '
,
.
=;'


6fi
!+"
ff

<&CE
'
` 3

`
+ 6. ,fi
"
= R! fi ")!"
fi!
2"

fi
!)fi
} +!2
5'
fi &?0 *
fi [

)`
"'

# .
=fi
} +fi

37


0#; *3/
q
fi
)! =
O.)'
"
[
5=
"
) q
5'
fi

)>$
2
*#;} 3

5} !2 "
)# '
fi 73



fi [
= .
=; [
!,. fi
ff, fi} B 3

,! =

fi
'
"


, +
!)'
+fi5!&ff8 )
ff

2. , .1

N*P

fi '# u'}~'w`u''uw #w '

-fi';#$ # %fi
!6
"' ff ) C /D-:/5 6fi

3!
fi
" 0 . ,#$/
4
Gfi

L)M,[) E
.

}
-
,fi
!&'
<3/'
I} -#7 ,


!*"
#;70
#
=

,
ff
+ 2L)M,[3<
+ ` 5,

+. -fi " +
!
,# 2L)M,
[
} !&
}






,

-
ffff )
I)Q&UVc
3< !-
"
*


, 4
X U&+ W"+ T#*&/T<.
"fffi
!

,4fi
! fi
' "'
2L)
? -
#
& J!q
0. [ * 04 " c
ff
2 "

.
"
-! &7_
G E
0 . 4
7 E



,



* *'

q

,# " -# &

ff



:l}

'R
A! ` :<
A! ? :<
` :<
? :<
L)
5
P

ff


I]


pj








^
^
^ K
^
^ ]
^ ] ]
^ ^






IP




I*9



r



P



^ P9:
^ ^KP
^ ^
^
^
^ M:
^ 9]
^
^ M9

ff







8jfi:o




jfi:o



^ ^M
^ ^I
^ ^I
^ 9^
^
^






'"] ? fi
)2 /6fi

&FE-. fi-ff "fi
!6.
+
/6fi

-
) 2L) ? 2[ 7&*' ":<I 6fi ,

GR *fi

q


'
>@ * 5#$"'
B L0 2 ] C .
"'

#

-

% 2

%/ L 2
]

C .
fi

. )
2

,
L< "#$ q

. ff
6
-# -fi
!%}
)*ff L)M,[37
+
:#$ _
6 . , -





:
%>$
fi =;
B"# *fi

! .

'
fi 0" , fi
!&







'*]Y fi , *fi';#$ -# ff % ,!*37+ [lm:jp
R
0oklRm(
. %[
3)P*:/
5 Cfi

-
68 " "` "

6


)!"-
3 -. 2
C%2!%

)! '


&
'
ffA! `

:< fi
!2. ! !1




"


#

:<"L ffA! ?
:<
fi
!C. !" !




)

:<"L `

:< fi
!C. !
0



* G

:<"L7 2 ?
:< fi
!6. !ff " 0




*


:<"L< 0L)

5 +fi
!+
ff "




/
2 "

&ff_/0ff 2 =
? :<Hfi
!3 fi
!
' 4.
-ff

: 844' .R 0^ ^P0 $3/
0
GR ff.
E ?
:<1
E




:
&ff>$` fi

!3 :/
5 <
?
:<
fi
!) #; +'
ff
2

fi

2
,"0
"
ff- fi
!). , 7& B+' 3

+


2-fi
"
=
) 0
[ ff#fi

!
, 0



'
!6" " 5=
:: *

!fi
$3 ff
#$ [

-fifi} fi';#$"[
"'
.
#
/2fi

&



N#D

fi'#u'Ew u''{
fiffYu'w

[s:o





Rn.m:o'

ff

fiRp`P



ff

_4
!3. # , .0 7R
# . } !, 8
<.0 7.

fi!*'"#$ '

3.0 4 L)M,[6"
ff} !-'-"
fi fi
#4 ff
,#}
3/ . ,fifi' 2- G *
)fi
!+q
! &0?00[
8
q

'
-
0
ff
.,3<. , GR 0#

fi 5'
fi
[
ff
2.0
2. "
"!*
)
!6>@'
"


B,fi

,
2
fi
! &-_/G
} )
6fi
!
37. 6

)

[


".


fiff
*'

<
L)
? [

"# 5'
fi 6>$
`
! fi

B
#
> 5}
!2 .
ff#$ "
.
=fi
} B
C 5'
fi D-fi

,
)' "]B &'
G


. 4 "fi
-.
,
,#
>#$7 B7
" 2
L)M,[&8@#< L)M,[
. ,'
fi ;# ff"'
7#4 , 0fifi

ff fi
0!O}

3 - L)

? [

".*
fi!2'
,*>
!#
B
0# /3 G

*'
. -
.



ff.+'
"



: ->$0#
# ,'
'
fi '
,* "'
# fi ff

L)
? G

B 3 , '
=$:/4



fi"."'

fi

!
fiff
>@'
fi -I, 2
fiff^B 3.0
ff
ff
!*

_

,
".
*
6 6
*'
[

6" Y"'
,G
#

"
,
&)? "

5 [
3
#ff
L)M,[
+

+2 * "fifi

fi
"2!


}
3 ) ).%'
*'
734
6 -'
". 62
6"ff#0

,:/".%'

>@'
fi *
%
fi^B
,
34.

+
6
fi!
ff

"'
)[

&2'
- "
= 6
6' <9*

* ff. - -"
_

2 #$[

- + - & H0
, )I^^^*
fi


,.
73
8

)'
. * -. 0fi


-
) ) [!fi
ff 0

-'
6. '
.H -^ ^I"
#



:
L4#; " 37

:/ '
fi E
`
9
. "I ^0
"0
! =;
fi. ff^3fi

!- ff
'

= _
, &



#

#

#

#

# %
#




#

#

#

#



#








$#

#







}'#}}

8 *
fffifi'0. ,fi )fi}
4[ !-#$ fifi!
2
#$[0
*
fffi #fi
"
=

"fi
!-'
-
- fi}-
ff!"& Hff [ .=
!2 } ff-
!) ."'ff## 5fi!*
3 )
! fi ff
fifi 4fi
/fi
!.

-0 fi} #7'
fi fi
4#7fi

3
#7'
fi ;#$ =

)-
-#
fi [


"#
!+*
#;# fi
,fi

&*_
G


[
-I

ff

"


#7 0
` _<'
fi}

ff!*3
-
fi

!C'
"
6
fi C'
fi ;#$
-
C
` _<'
%#$,fi
"
=

<&68 %

=
q
5'
fi
[
0.
2 * 0
*
` _<'
<3/. ff


: 2



:
0
fi
[


, . 2[
0#$.0
* ,fi
"
=

*. 0'
fi ;#$[
7&4_
G " . *
.



:
fi [
4#$


0 . [
0>
" "
,fi
!+. 8
fffi
"
= 6#$0 [
B 3[*
fi
[
0#$ff* ff#
=


[
>@'
fi
- *

q


,
)
0



B &ff_4
!37. , .
, )fi
!)


!2'

- [
=;'
"


-\8 ? fi
q
! 3<
+'
" + ff:/
5 ]

-fifi %
6 *
& Hff '
"

0fifi


*#
#$ [

-.
08fi

!*fi
"
= ,ff!<



,fi
!2!
" -,"

fi} 0

-'
5fi *.

" ff


[
&





N

fi '# u'}~'w`u''uw #w '

C^

ff


pj

CP

(I^
'09 ?



ff ff


I^^^
KMK
9MJ

:l lo'







:p}p

^ 9I
^ 9J
^ P





:o





^ ^^
^ ^^
^ ^^







jfi:o

iR 0o



^
J P9
^PK
II





mko'p





^ ^M
^ ^K
^ II










#L)M,[%}} !&4G_0 +I^^^ff' "


fi

, "!&4_/

##"'*

G
2}
*
L)M,[34 %
) C+ *
[
.
6
-
6 *


} 2fi
!2. 8 fi )-fi




/ &'
`"'
,#

,
ff
6.
+fi
!&2' ":<".

#$,I^^^+fi

30 * R. #$0fi

2 -R"*P
=
ff
37
) ,.(#$fffi

" ,60-I^`


&)'
*


!C# `fi

E

-#ff)fi
!%
.




"'
ff#
#

,
&ff'
"
_

+fi ff [ =



`
9
-' . 6 `fi

0
L)M,[
&6'
#$ _

fi 0 ,

7



:
# 8


`
9
&0'
"
)

!fi
4 .0 #<
'


"'
'
!
7&_4
!34 ,.`

fi , '
fi -
+
fi

#; ,'

:/0' . - ,., #4
&









G
#$ [

*0'2fifi
)"
,!"
2fi
0./3
fifi} "
GR#;1fi
.ff
< fi' & J
'- *E<*>IJJMB

0
#$ [
0
-
) -
fi[ )!*3/ 2 5fi'
[ff#E:
q 0$&
>$]^^^B<

=
" " "'
$&G6} :
$&>IJJK,B /[
!ff

"
7
-
3
"
"
#$ [

".

"
fi [
-! .

&
G6} 0
4$&/>IJJK:
B 5fi




fffi


fi

#$7
#$

"fi

,
"
fi} ff

!C#$}
G


< fi
& .
< 7.

! 5fi
fi

!
*I 90 4


3.0



`
! -
0'
5fi -.

" ff


[
">$ fi 2" :[
]
, 5fi +
B &
G ff`
_
ff )fi
!2'
ff

,'



ff *
-?ff

:/'



'


C.
% ,# 3
R+
R




+
]

:<

6'




"-:
: ,
+ 6fi
"./L4,
<34ff Cfi
!6

* /6fi
!


)
)

!
&,_/E
5fi 3. ,. ,
fi

8 fi 52 2

*} =; G6fi
!).
2 '
fi 0




".0
2
#$

&
Hff0!
5'
fi
[
"
"'
'
)2/ ff[
,# [
" fi} 2
=

,!"fffi 0- "fi

) !*
+fifi


).
# ffER> & /&3/

2
'
fi
8

4#< 6
5fi

,.
" 0
5'
fi
0

: '


,!*L
fi
* " ff fi} "0 4fffi

2'
}
*} ,
*/ *= `
9
3/.0



-7
#$


!"#$'



=
B
& .
3 #

,

-'
"/ 7&_
G ff`
0fi
'
""
[
!-#$ /

* ff ff fi} ,
,
0
= &_< " 3
4., E
L)M,[6"'
<
'
fifi
5 =



<3. -!)'
-
/

) -fi #
'
6 ",fi
4

!6




N

fi'#u'Ew u''{
fiffYu'w

fi ##
*fi
}
ff
} 2 &0_/

ff.
)
'+ "

[ H%L)M,[C"' <
0#$-",fififi
->$Dff}
` 0$&3<IJJMB &00!3[4
<3<
' '
*>$]^^^B7 # 8
! 5fi
,.0
40[ H%L)M,
[ =;! fifi}

,!
2
L)M,
[ =;
}
'
fi ff
2 fi} *

"!H#$ 3.0
, ff
ff )- fi " ,



ff * ff!<
&



? ,#; *./34. *.
66'' % 2#$[
C -+ 2

0
[ 3 5fiff ,fi
<
GR"' . -fi
"
=
#$# 5fi' 0 q
3
E
#< fi} - . ,#$7
0!"0>@.0
"
"4[ =
!"
[
-"'
0
B 3

0 0 0#, * . "#;'


+>@G6} .
$&3
IJJKB 3
5fi , " ff#" ff
#$

8
=;"
. &





fffi





'
* , )_< F
2#$,

#5G7,
6
fi[
6`_<'<3GR

\. }3/\4 0E:
<3:0'*[4
}

$3 IL+=
qff
#$
0 '
4 fi<3


' %#$* `
[
*
R6/#$*#"
2fi'
fi 3ff
(M,
L ?0
3 0

A<3/\4 :
E
2 q
0'
[4
}

7#$
fi#;0




&






#



ff fi0m

`om:o'pj



[s:o




fi





#

Yo :o'p}o ff8j}ok r


o'
R



omRnRo'p

`o' p}lRklRm

` _<'2
, 5fi'
[ 4 fi}2
"!O ff. 0!**} ,/ ff#


ff,
-`E . !"
,ff fiG
<&
84".
7'0 } I<`_<'
2
Mff
GR

& 84 * !",8
} "#
`
9
!*! <&4` !
.
'
- '
fi
)2-
GR "

)#
` _<'
+/
q
} +fi
`$34
+ ,`_<'

"
E
*!-
0'

.

2"
fi
I$&
8


#$
*
` _<'
*'ff#$'2 } -
/&[4,*
*

-' #$Y
/& HI%-
3!6!) %fifi!+'%

.0*!$&'
0

ff 7
,,!&8@#4
03/ -fi2 Y7 &
?0/3/[E7\4?ffA\CM H1%
` Hff@
' 0A\%?1A[\4?ffDff\.1[ %Hff`0\ &

? 0 [)# } 2 /3/!*.
'
} )- ! CD/3 C=;D/3/2C D+3<
2'
-fi
',# /} -*!0fi[.
2`_<'<&0[E7\4?ffA\RM HH`%Hff'@ ?ff`ffN 0[R'0\
[ %Hff`0\BJ\4_ HGff\C[.%Hff8$M,8;`ffNH'08;A*_\\4M J? ? D"&/? #$!2 *fi2 "fi3<
.
<"'
0 # .R
#


4#$!,,
. &\4 "
#`_<' -' #$ !8
fi " , /3<[E7\4?ffA\C_8;`08;
H
' 0\R
Eff\ 8H




" 50 /
& HI
!+ ":
:

%?ffE7EC# - 3< -.
)'
*
)fifi '
!)#$ff!)2fi
'

#; [
&
8@#<!" !"fi"/
, E 5fi'
[3 /Mff
04J
9*= 9M^*=K9I,:/3 4A
'
0J 9*= 9M*^ =IP:/&
' !#$fi

fi
*
*
E 5fi'
[*>

N*O

fi '# u'}~'w`u''uw #w '

#j]iR}om:jp}l:
84C M) 6 !6
C
` 5fi'
[&B84% R62 6-+
[3
C
fi
' )' &? #$!"::
q} - 2 fffi
' 2!# /} /3/ fi*
fi
"
-:
:
* ff !"#$ /& HI,!- 0::
2?ffE7E+# ff 3fi
fi
'
" !:
: R
[
&

?
*ff !*' *I
?
*ff !*' ]
?
*ff !*' 29
?
*ff !*' 0:
?
*ff !*' P
?
*ff !*' 2M
?
*ff-fi
'ff::R [







J#3M&[&3FC'


3/&`,&>IJJMB &#S.)UffffZ&/QV"W@X)U56)QV"V"W(&.6&?

:&
J
'<3<?"&/G1&3/FHE</3<[&L%&4>IJJMB &' 8 fi

2#[
2 fi' =;fi


}
"!"&,8) UX5#5#5W(&.6ff
+ U T(-.#
E5&/T#*)5&/QT$W@U&/QY
<ZV U+ W@SV U&FU!#*&
ffW@QYU56 S.# 3/fifi<&/J fiff<I^^&

ff

? = ? $3*/&3F J.G<3,L%&D"&>IJJ B &'}
0







}5=

&8 )UX5#5#5W(&.6+U T(-.#T(- &&/SQY! #5#T$W(&.6ffU T(-.#+5+ UXW@QT$W@U& U)UV <ST$QT$W@U&/QY
/ W(&.6 SW" + T$W@X, + 3fifi<&/]M ]fiff] ^&
4

ff

ff

ff

?
3'"&3FfiJ/3?"&>IJJMB &8;fi
0fi';#$ ,
*
#$[ =

/&8) UX5#5#5W(&.6
+ %
3/fifi<&7I^I fiff<I^]9 &
M,

$3L%&37FONG
/3\ &>IJJPB &L
0#$G
)

ff
6- fi}
,!*&08 )UX5#5#5W(&.6+ffU T(-.# E%%% ) W(&.6E<ZV U+ W@SV U&V <W() W@X QY
#(T -*U +,(W &ff"W + X U.S )+,# 5&/T #*) ) #T$QT$W@U &)Q &#*&#*)QT$W@U &/3/fifi<& 9:ff 9J&



ff







L0!37A<& >IJJKB &-Afi'

? fi
"' "#0"
5 .=;





}
+>$fi,IB &
+,#*) )U*#YW(&.6-Q&
+,#*) %Q <T#525&/T#*)QXT$W@U&/34>9*= :B &

0 3A<&3F



ff



L0!37A<& >IJJJB &-Afi'

? fi
"' "#0"
5 .=;





}
+>$fi0]B &
+,#*) )U*#YW(&.6-Q&
+,#*) %Q <T#525&/T#*)QXT$W@U&/3 >I5=]B &

0 3A<&3F



ff



} /3.L%&3_/ 3/& ? &3F1A
#$73_ &G1&<>IJJ]B &<8
0

- 5=
fiff
&8 5/
& T#*)5/& QT$W@U/& QY!U& ,#*) #*&/X5#ffU&<U!#*&</7Q&.6SQ*6#")UX5#,+5+ W(&.6 fi /!3
fifi<&
IPff
IK&

ff



NN

fi'#u'Ew u''{
fiffYu'w

Dff}
/3E4&[&3E<
<3L%&E4&3F L)3?"&G1&>IJJMB &}G
#$[4
?% !&
U.
)5&/QY7U %) T$W $XW@Q
5&/T #Y@Y4W 6#*&/X5##,+,# Q)X,-33/]9 fiff]KP&

ff

Dff*3 ? &">IJJPB & )
;#;} 2#$2
%fifi

& 8603,M&30F GR
fi<30/&
>$\4& B 3/UW@X5#UV"V"S.&/W@X QT$W@U& 1#T' #5#*&ffSV"Q&+ffQ& )QX,-W(&#,+ 3fifi<&:]]fiff.::]&`0

? '
G
,!*[ &

(

Dff*3 ? &3E<
<3M&3FCG6} 3L%& ?"&>IJJKB &_<6
G 5fi'* '
5GR #/
=
4,0 5fi'
.
" fi},
!"&8)UX5#5#5W(&.6+U (T -.#5&/T#*)5&/QT$W@U&/QY
U & ,#*) #*&/5X #ffU &I U !#*&</7Q&.6SQ*6#")UX5#,+5+ W(&.6 fi /! &

ff

ff



ff



E:
<37\ &3/F [4
}

$3R"&>IJJPB & ? 3/ [ 50
<&08)UX74U 2
U !#*&</7Q&.6SQ*6# <Z*+ T#V +# X,-.&/UYU56Z *U)!*+5-U ffS+ T$W(& # Q+ &







&



E:
<3ff\ &3[4
}

$3G"&3F \. }3G1&0>$]^^^B &C?
)"' 0#, C}


}
)#$,
6
+
&
)Q&+ QXT$W@U&+U& #5# X,-)Q& ffS.W@U
)U5X #,+5+ (W &.63 37II ff] 9&




E:
<3\ &3[4
}

$3#"&37\. }37G1&37_<
=
/3N"&7M&3FO`0! <37A<&>IJJJB &Afi}
ff
_< !--fi}
& 8)UX7 *U)!*+5-U %U&<ffST$UV"QT$W@X
#5# ,X -2# X 5U 6&/W@T$W@U &*Q &&#*)+ T$Q&W(&.6 &









E<
<3<M&/&3<F [4 <3/A<&4>$]^^^B & [

2 +/fi
-*fi fi' )

2
)
fi} "
!*&<8)UX7/U 0T(-.#%#*3*#*&/T#5#*&/T(-"ffQT$W@U&/QYU& ,#*) #*&/X5# U&2) T$W $XW@QY
5&/T #Y@Y4W 6#*&/5X # %% fiff &

ff

ff



ff

E<
<3M&./&3[4 <3A<&3F G6} 3.L%&?"&7>IJJKB &7\4
G fi,A

*,G_=
J 2Afi} *Mff
,?0 &48)UX5#5#5W(&.6+0U -W() T$Z2<W T(-2&&/SQY #5#T$W(&.6U "T(-.#
+5+ UXW@QT$W@U &2U UV <ST$QT$W@U &/Q

/4(W &.6S"W + T$W@,X + 3fifi<& K^fiff K &

ff

ff



ff

&

E<
<3M&./&3G6} 3.L%&?"&3F1DG3.L%&/&7>IJJJB &<?ff
'
-#4fi fi'







$&78
)U5X #5#5W(&.6+U 0(T -.#-(W ) T$Z%#*3*#*&/T(-&&/SQY #5#T$W(&.6
U ,(T -.# +5+ UXW@QT$W@U &2U UV <ST$QT$W@
U &/Q

/4(W &.6S"W + T$W@,X + 3fifi<
& 9^Jfi
ff 9*I M&

ff

ff

ff



!

`

"
$3 "
8 &3FOD0!
$&3
8"&>IJJMB &"?
4 !) +) *


!6#
'
fi *

<&48)UX7<U ,T(-.# 5 &/T#*)5&/QT$W@U&/QY<ZV U+ W@SV U&< U!#*&ffW@QYU56S.#3fifi<&
IP fiff<*I M^&

ff

0!3`,&3[4
<3./&3F





ff

' '
<3A<&<>$]^^^B &/Afi}
[#$&/8)UX5#5#5
W(&.6+ffU ,T(-.# T(-E&&/SQY #5#T$W(&.6U "T(-.#+5+ UXW@QT$W@U& U) UV <ST$QT$W@U&/QY
/4W(&.6SW"+ T$W@X,+ &

ff

ff

ff



' <3?"&3A *3 /&34' H03\ &3 J 3E4&3F ? [34?"& >IJJKB &+\4
6#
2/
C
[
)
#$
%! ' fi'
C 2?Gff8;A\Ofi &(8
5&/T #*)QXT$(W 3*#/UW@5X ## ,X -.&/UY5U 6Z U )#4Y # X UV"V".S &/W@X QT$W@U&+% <YW@X QT$W@U&+ ,3/fifi<&JI ff
J M&

ff




<3A<&3<DG3L%&<A<&37E<
<3<M&/&37FHG6} 3L%&/?"&>IJJJB &
#$ fi} *

,!"&48
)UX 7 % &

N







G
#$[ff


fi '# u'}~'w`u''uw #w '

A"
<3}"&G1&>IJJKB &?I8
,#/
7#$7
!"
#$!
, [


ff fi}
8
/
& 5&/T #*)5&/QT$W@
U &/QY U.
)5&/QYU ffSV"
Q & UV <ST #*)<T$.
W(#,+ 3

3 M] fi
ff M: &

ff

ff





Afi3'"&3F 0
H
3/&>IJJPB &4?I2fifi} - 5 =;=; fi' *!'
&82D0
<37G1&J0&3
F1[4
. $ 3/D"&<D"&>$\4& B 3 #5# X,-U*W(&.6Q&2<Z&/T(-.#,+ W"+ 3fifi<&MII ffM99&\4
&



ff

A<3'"&A<&3/FfiJ/3?"&<N"&7>IJJKB & #W(& U)X5#V0#*&/T/
# Q)5&/W(&.6&
L+8;'([ &
'R /3N"&/&>IJJPB &ff'Rfi
GR-
) ).=;"<&UV"V"S.&/W@X QT$W@U&+"U
(T -.# )33/PKfiff MK&

ff

G6} 3*L%& ?"&>$]^^^B & ?I,fifi

,#/
#$[
ff
!0


*, fi}
-

,!1#$#

$
& U.
)5&/QY7U %) T$W $XW@QY5&/T#Y@YW46#*&/X5#%#,+,# Q)X,-3 ff3
9K fi.ff :/*I M&

ff

G6} 3L%&?"&3_<[ 3./& ? & 3FH`0! <3A<&>IJJKB &E:
*fi
<
" 5=

* ffB
? !)# - fi})
* 0#$G
$&*8)UX5#5#5W(&.6+"U T(-.#
T(&&/SQY #5#T$(W &.6U "(T -.# 5+ + UXW@QT$W@U&2U UV <ST$QT$W@U&/QY
/4W(&.6SW"+ T$W@X,+ /. / 3
fifi<&7*I 9:Pfiff<*I 9P]&

ff

ff

ff





G6} 3 L%&?"&3E<
<34M&/&3Dff*3 ? &?"&3F ?ff' 3?"& > IJJKB &)\4
6 fi}

, .
*[/?G0?0M,8;A\ '.[0
&UV <ST#*)% #5# X,-Q&0/7Q&.6SQ*6#3
ff4> 9B &





G6} 3L%&3Rff".,3H"&34Fc0
$3
L%&>$]^^IB &*Afi* ,?O
-*fi ' &)8
)U5X #5#5W(&.6+*U +(T -.#ffU) (T - ff0

V #*) W@X
Q &
#5#T$(W &.6%U +(T -.# +5+ UXW@QT$W@U & U )UV <ST$QT$W@U &/QY
/4(W &.6S"W + T$W@,X + &

ff

ff

ff



G6} 3L%&?"&3F G(
} 3A<&>IJJ^B &
L)
5 *



"
*
4?I*


-


, [
<&48)UX7 ffT(-E&&/SQY #5#T$W(&.6U ,T(-.# /43fifi<& ^fiff J&

ff

84'/3A<&
/&@>$]^^^B &,[

EL ,
6Afi})Mff
-A!"&&8 -W@YU+ U -W@X QY
) Q& + QXT$W@U&0
+ U ,T(-.#0
UZQY<
UXW(# T$Z#* ) W(#,+ fi
3<fifi<&7I*9 KJfiff<I,: ^]&

ff

ff

ff

G6
3 ? &/&/>IJKJB & )U*#Y + U #YQZ#5 #W(& U)X5#V0#*&/T/
# Q)5&/W(&.6&[<& M&
3 ? "

G

!<&

N



fiJournal Artificial Intelligence Research 16 (2002) 209-257

Submitted 5/01; published 4/02

Structured Knowledge Representation Image Retrieval
Eugenio Di Sciascio
Francesco M. Donini
Marina Mongiello

disciascio@poliba.it
donini@poliba.it
mongiello@poliba.it

Dipartimento di Elettrotecnica ed Elettronica, Politecnico di Bari
Via David, 200 70125 BARI Italy

Abstract
propose structured approach problem retrieval images content
present description logic devised semantic indexing retrieval
images containing complex objects.
approaches do, start low-level features extracted image analysis
detect characterize regions image. However, contrast feature-based approaches, provide syntax describe segmented regions basic objects complex
objects compositions basic ones. introduce companion extensional semantics defining reasoning services, retrieval, classification, subsumption.
services used exact approximate matching, using similarity measures.
Using logical approach formal specification, implemented complete clientserver image retrieval system, allows user pose queries sketch queries
example. set experiments carried testbed images assess
retrieval capabilities system comparison expert users ranking. Results
presented adopting well-established measure quality borrowed textual information
retrieval.

1. Introduction
Image retrieval problem selecting, repository images, images fulfilling maximum extent criterion specified end user. paper,
concentrate content-based image retrieval, criteria express properties
appearance image itself, i.e., pictorial characteristics.
research field till concentrated devising suitable techniques
extracting relevant cues aid image analysis algorithms. Current systems result
effective specified properties so-called low-level characteristics, color
distribution, texture. example, systems IBMs QBIC1 easily retrieve,
among others, stamps containing picture brown horse green field, asked
retrieve images stamps brown central area greenish background.
Nevertheless, present systems fail treating correctly high-level characteristics
image as, retrieve stamps galloping horse. First all, systems cannot
even allow user specify queries, lack language expressing highlevel features. Usually, overcome help examples: retrieve images similar
one. However, examples quite ambiguous interpret: features
1. See e.g., http://wwwqbic.almaden.ibm.com/cgi-bin/stamps-demo
c
2002
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDi Sciascio, Donini & Mongiello

example appear retrieved images? ambiguity produces lot
false positives, one experience.
Even relevant features pointed example, system cannot tell whether
pointed color distribution, interpretation all, galloping
brown horse produces color distribution similar running brown fox
galloping white horse. aspect, image retrieval faces problems object
recognition, central problem robotics artificial vision. effective
solution overcoming problem associate query significant keywords,
match keywords attached way images repository. ambiguities
image understanding transferred text understanding, brown portrait
Crazy Horse famous Indian chief could considered relevant.
Resorting human experts specify expected output retrieval algorithm can,
opinion, worsen ambiguities, since makes correctness approach
depend subjective perception image retrieval system do.
needed formal, high-level specification image retrieval task. need motivates
research report here.
1.1 Contributions Paper
approach problem image retrieval knowledge representation perspective,
particular, refer framework already successfully applied Woods
Schmolze (1992) conceptual modeling semantic data models databases (Calvanese,
Lenzerini, & Nardi, 1998). consider image retrieval knowledge representation
problem, distinguish following aspects:
Interface: user given simple visual language specify (by sketch example)
geometric composition basic shapes, call description. composite shape
description intuitively stands set images (all containing given shapes
relative positions); used either query, index relevant class
images, given meaningful name.
Syntax semantics: system internal syntax represent users
queries descriptions, syntax given extensional semantics terms sets
retrievable images. contrast existing image retrieval systems, semantics
compositional, sense adding details sketch may restrict set
retrievable images. Syntax semantics constitute Semantic Data Model,
relative position, orientation size shape component given explicit notation geometric transformation. extensional semantics allows us define
hierarchy composite shape descriptions, based set containment interpretations descriptions. Coherently, recognition shape description image
defined interpretation satisfying description.
Algorithms complexity: based semantics, prove subsumption
descriptions carried terms recognition. devise exact
approximate algorithms composite shapes recognition image, correct
respect semantics. Ideally, computational complexity problem retrieval
known, algorithms also optimal reference computational
complexity problems. Presently, solved problem exact retrieval,
210

fiStructured Knowledge Representation Image Retrieval

propose algorithm approximate retrieval which, although probably non-optimal,
correct.
Experiments: study complexity problem ongoing, syntax,
semantics, sub-optimal algorithms obtained far already sufficient provide
formal specification prototype system experimental verification approach.
prototype used carry set experiments test database images,
allowed us verify effectiveness proposed approach comparison
expert users ranking.
believe knowledge representation approach brings several benefits research
image retrieval. First all, separates problem finding intuitive semantics
query languages image retrieval problem implementing correct algorithm
given semantics. Secondly, problem image retrieval semantically formalized, results techniques Computational Geometry exploited assessing
computational complexity formalized retrieval problem, devising efficient
algorithms, mostly approximate image retrieval problem. much
spirit finite model theory used study complexity query answering relational databases (Chandra & Harel, 1980). Third, language borrows
object modeling Computer Graphics hierarchical organization classes images
(Foley, van Dam, Feiner, & Hughes, 1996). This, addition interpretation composite shapes one immediately visualize, opens logical approach retrieval
images 3D-objects constructed geometric language (Paquet & Rioux, 1998),
still explored. Fourth, logical formalization, although simple, allows extensions
natural logic, disjunction (OR) components. Although alternative
components complex shape difficult shown sketch, could used
specify moving (i.e., non-rigid) parts composite shape. exemplifies
logical approach shed light extensions syntax suitable for, e.g., video sequence
retrieval.
1.2 Outline Paper
rest paper organized follows. next section, review related work
image retrieval. Section 3 describe formal language, first syntax,
semantics, start proving basic properties. following section, analyze
reasoning problems semantic relations among them, devise algorithms
solve them. Section 5 illustrate architecture system
propose examples pointing distinguishing aspects approach. Section 6
present set experiments assess retrieval capabilities system. Last section
draws conclusions proposes directions future work.

2. Related Work
Content-Based Image Retrieval (CBIR) recently become widely investigated research
area. Several systems approaches proposed; briefly report
significant examples categorize three main research directions.
211

fiDi Sciascio, Donini & Mongiello

2.1 Feature-based Approaches
Largest part research CBIR focused low-level features color, texture,
shape, extracted using image processing algorithms used characterize
image feature space subsequent indexing similarity retrieval.
way problem retrieving images homogeneous content substituted
problem retrieving images visually close target one (Hirata & Kato, 1992; Niblak
et al., 1993; Picard & Kabir, 1993; Jacobs, Finkelstein, & Salesin, 1995; Flickner et al.,
1995; Bach, Fuller, Gupta, Hampapur, Horowitz, Humphrey, Jain, & Shu, 1996; Celentano
& Di Sciascio, 1998; Cox, Miller, Minka, & Papathomas, 2000; Gevers & Smeulders, 2000).
Among various projects, particularly interesting QBIC system (Niblak et al.,
1993; Flickner et al., 1995), often cited ancestor CBIR systems,
allows queries performed shape, texture, color, example sketch using
target media images shots within videos. system currently embedded
tool commercial product, Ultimedia Manager. Later versions introduced
automated foreground/background segmentation scheme. indexing image
made principal shape, aid heuristics. evident limitation:
images main shape, objects often composed various parts.
researchers, rather concentrating main shape, typically assumed located central part picture, proposed index regions images;
focus retrieval similar images, similar regions within
image. Examples idea VisualSeek (Smith & Chang, 1996), NETRA (Ma
& Manjunath, 1997) Blobworld (Carson, Thomas, Belongie, Hellerstein, & Malik,
1999). problem although systems index regions, lack higher
level description images. Hence, able describe hence query
single region time image.
order improve retrieval performances, much attention paid recent
years relevance feedback. Relevance feedback mechanism, widely used textual
information systems, allows improving retrieval effectiveness incorporating
user query-retrieval loop. Depending initial query system retrieves set
documents user mark either relevant irrelevant. system, based
user preferences, refines initial query retrieving new set documents
closer users information need.
issue particularly relevant feature-based approaches, one hand, user
lacks language express powerful way information need,
hand, deciding whether image relevant takes glance. Examples systems
using relevance feedback include MARS (Rui, Huang, & Mehrotra, 1997), DrawSearch
(Di Sciascio & Mongiello, 1999) PicHunter (Cox et al., 2000).
2.2 Approaches Based Spatial Constraints
type approach problem image retrieval concentrates finding similarity images terms spatial relations among objects them. Usually emphasis
relative positions objects, considered symbolic images icons,
identified single point 2D-space. Information content visual appearance images normally neglected.
212

fiStructured Knowledge Representation Image Retrieval

Chang, Shi, Yan (1983) present modeling type images terms
2D-strings, strings accounting position icons along one two
planar dimensions. approach retrieval images basically reverts simpler string
matching.
Gudivada Raghavan (1995) consider objects symbolic image associated
vertexes weighted graph. Edges i.e., lines connecting centroids pair
objects represent spatial relationships among objects associated
weight depending slope. symbolic image represented edge list. Given
edge lists query database image, similarity function computes degree
closeness two lists measure matching two spatial-graphs.
similarity measure depends number edges comparison
orientation slope edges two spatial-graphs. algorithm robust
respect scale translation variants sense assigns highest similarity
image scale translation variant query image. extended algorithm
includes also rotational variants original images.
recent papers topic include Gudivada (1998) El-Kwae
Kabuka (1999), basically propose extensions strings approach efficient
retrieval subsets icons. Gudivada (1998) defines R-strings, logical representation
image. representation also provides geometry-based approach iconic indexing
based spatial relationships iconic objects image individuated
centroid coordinates. Translation, rotation scale variant images variants generated arbitrary composition three geometric transformations considered.
approach deal object shapes, basic image features,
considers sequence names objects. concatenation objects
based euclidean distance domain objects image starting reference
point. similarity database query image obtained spatial
similarity algorithm measures degree similarity query database
image comparing similarity R-strings. algorithm recognizes rotation, scale translation variants image also subimages, subsets
domain objects. constraint limiting practical use approach assumption
image contain one instance icon object.
El-Kwae Kabuka (1999) propose extension spatial-graph approach,
includes topological directional constraints. topological extension
objects obviously useful determining differences images
might considered similar directional algorithm considers locations
objects term centroids. similarity algorithm propose extends
graph-matching one previously described Gudivada Raghavan (1995). similarity
two images based three factors: number common objects, directional
topological spatial constraint objects. similarity measure includes
number objects, number common objects function determines
topological difference corresponding objects pairs query database
image. algorithm retains properties original approach, including invariance
scaling, rotation translation also able recognize multiple rotation variants.
213

fiDi Sciascio, Donini & Mongiello

2.3 Logic-based Structured Approaches
reference previous work Vision Artificial Intelligence, use structural
descriptions objects recognition images dated back Minskys
frames, work Brooks (1981). idea associate parts object (and
generally scene) regions image segmented into. hierarchical
organization knowledge used recognition object first proposed
Marr (1982). Reiter Mackworth (1989) proposed formalism reason maps
sketched diagrams. approach, possible relative positions lines fixed
highly qualitative (touching, intersecting).
Structured descriptions three-dimensional images already present languages
virtual reality like VRML (Hartman & Wernecke, 1996) hierarchical object modeling. However, semantics languages operational, effort made
automatically classify objects respect structure appearance.
Meghini, Sebastiani, Straccia (2001) proposed formalism integrating Description
Logics image text retrieval, Haarslev, Lutz, Moeller (1998) integrate
Description Logics spatial reasoning. extensions approach described
Moeller, Neumann, Wessel (1999). proposals build clean integration
Description Logics concrete domains Baader Hanschke (1991). However, neither
formalisms used build complex shapes nesting simple shapes.
Moreover, proposal Haarslev et al. (1998) based logic spatial relations
named RCC8, enough specifying meaningful relations map,
qualitative specify relative sizes positions regions complex shape.
Also Hacid Rigotti (1999) description logics concrete domains
basis logical framework image databases aimed reasoning query containment.
Unfortunately, proposed formalism cannot consider geometric transformations neither
determine specific arrangements shapes.
similar approach proposal Ardizzone, Chella, Gaglio (1997),
parts complex shape described description logic. However, composition shapes consider positions, hence reasoning cannot take positions
account.
Relative position parts complex shape expressed constraint relational
calculus work Bertino Catania (1998). However, reasoning queries
(containment emptiness) considered approach. Aiello (2001) proposes
multi-modal logic, provides formalism expressing topological properties
defining distance measure among patterns.
Spatial relation parts medical tomographic images considered Tagare,
Vos, Jaffe, Duncan (1995). There, medical images formed intersection
image plane object. image plane changes, different parts object
considered. Besides, metric arrangements formulated expressing arrangements
terms Voronoi diagram parts. approach limited medical image
databases provide geometrical constraints.
Compositions parts image considered work Sanfeliu Fu
(1983) character recognition. However, recognizing characters, line compositions
closed, sense one looks specified lines, more. Instead
214

fiStructured Knowledge Representation Image Retrieval

framework, shape F composed three lines, subsumed shape something unacceptable recognizing characters. Apart different task, approach
make use extensional semantics composite shapes, hence reasoning
possible.
logic-based multimedia retrieval system proposed Fuhr, Govert, Rolleke
(1998); method, based object-oriented logic, supports aggregated objects
oriented towards high-level semantic indexing, neglects low-level features
characterize images parts them.
field computation theories recognition, mention two approaches
resemblance own: Biedermans structural decomposition geometric constraints proposed Ullman, described Edelmann (1999). Unfortunately, neither
appears suitable realistic image retrieval: structural decomposition approach
consider geometric constraints shapes, approach based geometric constraints consider possibility defining structural decomposition
shapes, hence reasoning them.
Starting reasonable assumption recognition object scene
eased previous knowledge context, work Pirri Finzi (1999),
recognition task, interpretation image, takes advantage information
cognitive agent environment, representation data
high-level formalism.

3. Syntax Semantics
section present formalism dealing definition composite shape descriptions, semantics, properties distinguish approach previous
ones.
remark formalism deals image features, like shape, color, texture,
independent way features extracted actual images. interested
reader, algorithms used compute image features implementation
formalism presented Appendix.
3.1 Syntax
main syntactic objects basic shapes, position shapes, composite shape descriptions, transformations. also take account features typically
determine visual appearance image, namely color texture.
Basic shapes denoted letter B, edge contour e(B) characterizing
them. assume e(B) described single, closed 2D-curve space whose origin
coincides centroid B. Examples basic shapes circle, rectangle,
contours e(circle) = , e(rectangle) =
, also complete, rough contour
e.g., one ship basic shape. make language compositional,
consider external contour region. example, region contained
another, , contour outer region external rectangle.
possible transformations simple ones present drawing tool:
rotation (around centroid shape), scaling translation. globally denote
215

fiDi Sciascio, Donini & Mongiello

Figure 1: graphical interface query sketch.
rotation-translation-scaling transformation . Recall transformations composed sequences 1 . . . n , form mathematical group.
basic building block syntax basic shape component hc, t, , Bi,
represents region color c, texture t, edge contour (e(B)). (e(B))
denote pointwise transformation whole contour B. example, could
specify place contour e(B) upper left corner image, scaled 1/2
rotated 45 degrees clockwise.
Composite shape descriptions conjunctions basic shape components one
color texture denoted
C = hc1 , t1 , 1 , B1 u u hcn , tn , n , Bn
expect end users system actually define composite shapes
syntax; internal representation composite shape. system
maintain user draws help graphic tool complex shape
dragging, rotating scaling basic shapes chosen either palette, existing
images (see Figure 1).
example, composite shape lighted-candle could defined
lighted-candle = hc1 , t1 , 1 , rectanglei u hc2 , t2 , 2 , circlei
216

fiStructured Knowledge Representation Image Retrieval

1 , 2 placing circle flame top candle, textures colors defined
accordingly intuition.
remark that, best knowledge, logic present first one
combining shapes explicit transformations one language.
previous paper (Di Sciascio, Donini, & Mongiello, 2000) presented formalism
including nested composite shapes, done hierarchical object modeling (Foley et al.,
1996, Ch.7). However, nested composite shapes always flattened composing
transformations. Hence paper focus two levels: basic shapes compositions
basic shapes. Also, simplify presentation semantics, following
section present color texture features, take account
Section 4.2 on.
3.2 Semantics
consider extensional semantics, syntactic expressions interpreted
subsets domain. setting, domain interpretation set images ,
shapes components interpreted subsets . Hence, also image database
domain interpretation, complex shape C subset domain
images retrieved database C viewed query.
approach quite different previous logical approaches image retrieval
view image database set facts, logical assertions, e.g., one based
Description Logics Meghini et al. (2001). setting, image retrieval amounts
logical inference. However, observe usually Domain Closure Assumption (Reiter,
1980) made image databases: regions ones seen
images themselves. allows one consider problem image retrieval simple
model checking check given structure satisfies description2 .
Formally, interpretation pair (I, ), set images,
mapping shapes components subsets . identify image
set regions {r1 , . . . ,rn } segmented (excluding background, discuss
end section). region r comes edge contour e(r). image
belongs interpretation basic shape component h, BiI contains
region whose contour matches (e(B)). formulae,
h, BiI = {I | r : e(r) = (e(B))}

(1)

definition exact recognition shape components images, due
presence strict equality comparison contours; extended
approximate recognition follows. Recall characteristic function f set
function whose value either 1 0; fS (x) = 1 x S, fS (x) = 0 otherwise. consider
characteristic function set defined Formula (1). Let image;
belongs h, BiI , characteristic function computed value 1, otherwise
value 0. keep number symbols low, use expression h, BiI also
2. Obviously, Domain Closure Assumption regions valid artificial vision, dealing twodimensional images three-dimensional shapes (and scenes), solid shapes surfaces
hidden images. outside scope retrieval problem.

217

fiDi Sciascio, Donini & Mongiello

denote characteristic function (with argument (I) distinguish set).


h, Bi (I) =

(

1 r : e(r) = (e(B))
0
otherwise

reformulate function order make return real number range [0, 1]
usual fuzzy logic (Zadeh, 1965). Let sim(, ) similarity measure pairs
contours range [0, 1] real numbers (where 1 perfect matching). use sim(, )
instead equality compare edge contours. Moreover, existential quantification
replaced maximum possible regions I. Then, characteristic function
approximate recognition image basic component, is:
h, BiI (I) = max{sim(e(r), (e(B)))}
rI

Note sim depends translations, rotation scaling, since looking regions
whose contour matches e(B), reference position size specified .
interpretation basic shapes, instead, includes translation-rotation-scaling invariant recognition, commonly used single-shape Image Retrieval. define
interpretation basic shape
B = {I | r : e(r) = (e(B))}
approximate counterpart function
B (I) = max max{sim(e(r), (e(B)))}


rI

maximization possible transformations max effectively computed
using similarity measure simss invariant reference translation-rotationscaling (see Section 4.2). Similarity color texture added weighted sum
Section 4.2. way, basic shape B used query retrieve images
B . Therefore, approach generalizes usual approaches
single-shape retrieval, Blobworld (Carson et al., 1999).
Composite shape descriptions interpreted sets images contain components composite shape. Components anywhere image, long
described arrangement relative other. Let C composite shape
description h1 , B1 u u hn , Bn i. exact matching, interpretation intersection
sets interpreting component shape:
C = {I | : ni=1 h( ), Bi iI }

(2)

Observe require shape components C transformed image regions
using transformation . preserves arrangement shape components
relative given allowing C include every image
containing group regions right arrangement, wholly displaced .
clarify formula, consider Figure 2: shape C composed two basic shapes
B1 B2 , suitably arranged transformations 1 2 . Suppose
contains image I. Then, C exists transformation ,
218

fiStructured Knowledge Representation Image Retrieval

Figure 2: example application Formula (2).
globally brings C I, is, 1 brings rectangle B1 rectangle recognized
I, 2 brings circle B2 circle recognized I, arranged according
C. Note could contain also shapes, included C.
formally define recognition shape image.
Definition 1 (Recognition) shape description C recognized image
every interpretation (I, ) , C . interpretation (I, ) satisfies
composite shape description C exists image C recognized
I. composite shape description satisfiable exists interpretation satisfying it.
Observe shape descriptions could unsatisfiable: two components define overlapping
regions, image segmented way satisfies components. course,
composite shape descriptions built using graphical tool, unsatisfiability easily
avoided, assume descriptions always satisfiable. Anyway, unsatisfiable shape
descriptions could easily detected, syntactic form, since unsatisfiability
arise overlapping regions (see Proposition 4).
Observe also set-based semantics implies intuitive interpretation conjunction u one could easily prove u commutative idempotent.
approximate matching, modify definition (2), following fuzzy interpretation
u minimum, existential maximum:
n

C (I) = max {min {h( ), Bi iI (I)}}


i=1

(3)

Observe interpretation composite shape descriptions strictly requires presence components. fact, measure image belongs interpreta219

fiDi Sciascio, Donini & Mongiello

tion composite shape description C dominated least similar shape component
(the one minimum similarity). Hence, basic shape component dissimilar
every region I, brings near 03 also measure C (I). strict
than, e.g., Gudivada & Raghavans (1995) El-Kwae & Kabukas (1999) approaches,
non-appearing component decrease similarity value C (I),
still threshold.
Although requirement may seem strict one, captures way details used
refine query: dominant shapes used first, and, retrieved set still
large, user adds details restrict results. refinement process,
happen images match new details, pop enlarging set
results user trying restrict. formalize refinement process
following definition.
Proposition 1 (Downward refinement) Let C composite shape description,
.
let refinement C, = C u h 0 , B 0 i. every interpretation I, shapes
interpreted (2), C ; shapes interpreted (3), every
image holds (I) C (I).
Proof. (2), claim follows fact considers intersection
components one C , plus set h( 0 ), B 0 iI . (3), claim analogously
follows fact (I) computes minimum superset values considered C (I).

property makes language fully compositional. Namely, let C composite shape description; consider meaning C used query
set images potentially retrieved using C. least, meaning perceived end user system. Downward refinement ensures meaning
C obtained starting one component, progressively adding
components order. remark frameworks cited (Gudivada &
Raghavan, 1995; El-Kwae & Kabuka, 1999) property hold. illustrate
problem Figure 3. Starting shape description C, may retrieve (among many
others) two images I1 , I2 , C (I1 ) C (I2 ) threshold t,
another image I3 set C (I3 ) < t. order selective, try adding details, obtain shape description D. Using D, may still
retrieve I2 , discard I1 . However, I3 partially matches new details D.
Downward refinement holds, (I3 ) C (I3 ) < t, I3 cannot pop up. contrast,
Downward refinement hold (as Gudivada & Raghavans approach)
DI (I3 ) > > C (I3 ) matched details raise similarity sum weighted
components. case, meaning sketch cannot defined terms
components.
Downward refinement property linking syntax semantics. Thanks extensional semantics, extended even meaningful semantic relation, namely,
3. exactly 0, since every shape matches every one low similarity measure. Similarity
often computed inverse distance. Similarity 0 would correspond infinite distance.
Nevertheless, recognition algorithm force similarity 0 threshold.

220

fiStructured Knowledge Representation Image Retrieval

Figure 3: Downward refinement: thin arrows denote non-zero similarity approximate
recognition. thick arrow denotes refinement.

221

fiDi Sciascio, Donini & Mongiello

Figure 4: example subsumption hierarchy shapes (thick arrows), images
shapes recognized (thin arrows).

subsumption. borrow definition Description Logics (Donini, Lenzerini, Nardi,
& Schaerf, 1996), fuzzy extensions (Yen, 1991; Straccia, 2001).
Definition 2 (Subsumption) description C subsumes description every
interpretation I, C . (3) used, C subsumes every interpretation
image , (I) C (I).
Subsumption takes account fact description might contain syntactic variant
another, without user system explicitly knowing fact. notion
subsumption extends downward refinement. enables also hierarchy shape descriptions, description another C subsumed C. C
used queries, subsumption hierarchy makes easy detect query containment.
Containment used speed retrieval: images retrieved using query
immediately retrieved also C used query, without recomputing similarities.
query containment important standard databases (Ullman, 1988), becomes
even important image retrieval setting, since recognition specific features
image computationally demanding.
Figure 4 illustrates example subsumption hierarchy basic composite shapes
(thick arrows denote subsumption shapes), two images shapes
recognized (thin arrows).
Although consider background, could added framework
special basic component hc, t, , backgroundi property region b satisfies
222

fiStructured Knowledge Representation Image Retrieval

background simply colors textures match, check edge contours.
Also, one background could added; case background regions
overlap, matching background regions considered regions
basic shapes recognized subtracted background regions.

4. Reasoning Retrieval
envisage several reasoning services carried logic image retrieval:
1. shape recognition: Given image shape description D, decide recognized I.
2. image retrieval: given database images shape description D, retrieve
images recognized.
3. image classification: given image collection descriptions 1 , . . . , Dn , find
descriptions recognized I. practice, classified finding
specific descriptions (with reference subsumption) satisfies. Observe
classification way preprocessing recognition.
4. description subsumption (and classification): given (new) description collection descriptions D1 , . . . , Dn , decide whether subsumes (or subsumed by)
Di , = 1, . . . , n.
services 12 standard image retrieval system, services 34 less obvious,
briefly discuss below.
process image retrieval quite expensive, systems usually perform off-line
processing data, amortizing cost several queries answered on-line.
example, document retrieval systems web4 , images text, use spiders
crawl web extract relevant features (e.g., color distributions textures
images, keywords texts), used classify documents. Then, answering
process uses classified, extracted features documents original data.
system adapt setting composite shapes, too. system, new
image inserted database immediately segmented classified accordance
basic shapes compose it, composite descriptions satisfies (Service 3). Also
query undergoes classification, reference queries already answered
(Service 4). basic shapes present, faster system answer new
queries based shapes.
formally, given query (shape description) D, exists collection descriptions D1 , . . . , Dn images database already classified reference
D1 , . . . , Dn , may suffice classify reference D1 , . . . , Dn find (most
of) images satisfying D. usual way classification Description
Logics amounts semantic indexing help query answering (Nebel, 1990).
example, answer query asking images containing arch, system may
classify arch find subsumes threePortalsGate (see Figure 4). Then, system
4. e.g., Altavista, QBIC, NETRA, Blobworld, also Yahoo (for textual documents).

223

fiDi Sciascio, Donini & Mongiello

include answer images ancient Roman gates recognized,
without recomputing whether images contain arch not.
problem computing subsumption descriptions reduced recognition
next section, algorithm exact recognition given. Then, extend
algorithm realistic approximate recognition, reconsidering color texture.
4.1 Exact Reasoning Images Descriptions
start reformulation (2), suited computational purposes.
Theorem 2 (Recognition mapping) Let C = h1 , B1 u u hn , Bn composite
shape description, let image, segmented regions {r 1 , . . . ,rm }. C
recognized iff exists transformation injective mapping j : {1, . . . , n}
{1, . . . , m} = 1, . . . , n
e(rj(i) ) = (i (e(Bi )))
Proof. (2), C recognized iff
[I

n
\

h( ), Bi iI ] equivalent [

i=1

n
^

h( ), Bi iI ]

i=1

Expanding h( ), Bi iI definition (1) yields
[

n
^

r I.e(r) = (i (e(Bi )))]

i=1

since regions {r1 , . . . ,rm } equivalent
[


n _
^

e(rj ) = (i (e(Bi )))]

i=1 j=1

Making explicit disjunction j conjunctions i, arrange conjunctive formula matrix:




(e(r1 ) = (1 (e(B1 ))) e(rm ) = (1 (e(B1 )))) )


..
.
..

.

.
..
(e(r1 ) = (n (e(Bn ))) e(rm ) = (n (e(Bn )))) )

(4)

note two properties matrix equalities:
1. given transformation, one region among r1 , . . . ,rm equal
component. means row, one disjunct true given
.
2. given transformation, region match one component. means
column, one equality true given .
224

fiStructured Knowledge Representation Image Retrieval

observe properties imply regions different shapes, since
equality contours depends translation, rotation, scaling. use equality
represent true overlap, equal shape.
Properties 12 imply formula true iff injective function
mapping component one region matches with. ease comparison
formulae use symbol j mapping j : {1, . . . , n} {1, . . . , m}. Hence,
Formula (4) rewritten claim:
[j : {1..n} {1..m}

n
^

e(rj(i) ) = (i (e(Bi )))]

(5)

i=1

Hence, even previous section semantics composite shape derived
semantics components, computing whether image contains composite shape
one focus groups regions, one group rj(1) , . . . , rj(n) possible mapping j.
Observe j injective implies n, one would expect. proposition
leaves open one j must chosen first. fact, follows
show optimal choice exact recognition mix decisions j .
approximate recognition considered, however, exchanging quantifiers harmless.
fact, change order approximations made. return
issue next section, discuss one devise algorithms approximate
recognition.
Subsumption simple logic shape descriptions relies composition
contours basic shapes. Intuitively, actually decide subsumed C, check
sketch associated seen image would retrieved using C
query. logical perspective, existentially quantified regions semantics
shape descriptions (1) skolemized prototypical contours. Formal definitions
follow.
Definition 3 (Prototypical image) Let B basic shape. prototypical image
I(B) = {e(B)}. Let C = h1 , B1 u u hn , Bn composite shape description.
prototypical image I(C) = {1 (e(B1 )), . . . , n (e(Bn ))}.
practice, composite shape description one builds prototypical image applying stated transformations components (and color/texture fillings, present).
Recall envisage prototypical image built directly user,
help drawing tool, basic shapes colors palette items. system
keep track transformations corresponding users actions, use
building (internal) shape descriptions stored previous syntax. feature
makes proposal different query-by-sketch retrieval systems, precisely
sketches also logical meaning. So, properties description/sketches
proved, containment query sketches stated formal way, algorithms
containment checking proved correct reference semantics.
Prototypical images important properties. first satisfy (in
sense Definition 1) shape description exemplify intuition would suggest.
225

fiDi Sciascio, Donini & Mongiello

Proposition 3 every composite shape description D, satisfiable interpretation hI, {I(D)}i satisfies D.
Proof. Theorem 2, using identical transformation identity mapping
j.

shape description satisfiable overlapping regions I(D). Since
obvious specified drawing tool, give following proposition
sake completeness.
Proposition 4 shape description satisfiable iff prototypical image I(D) contains
overlapping regions.
turn subsumption. Observe B1 B2 basic shapes, either
equivalent (each one subsumes other) neither two subsumes other.
adopt segmented regions invariant representation, (e.g. Fourier transforms
contour) deciding equivalence basic shapes, recognizing whether basic shape
appears image, call algorithm computing similarity shapes.
usual image recognizers allowing tolerance matching
shapes. Therefore, framework extends retrieval shapes made single
component, effective systems already available.
consider composite shape descriptions, prove main property prototypical images, namely, fact subsumption shape descriptions
decided checking subsumer recognized sketch subsumee.
Theorem 5 composite shape description C subsumes description C
recognized prototypical image I(D).
Proof. Let C = h1 , B1 u u hn , Bn i, let = h1 , A1 u u hm , i. Recall
I(D) defined I(D) = {1 (e(A1 )), . . . , (e(Am ))}. ease reading, sketch
idea proof Figure 5.
If. Suppose C recognized I(D), is, I(D) C every interpretation (I, )
I(D) . Then, Theorem 2 exists transformation suitable
injective function j {1, . . . , n} {1, . . . , m}
e(rj(k) ) = k (e(Bk ))

k = 1, . . . , n

Since I(D) prototypical image D, substitute region basic
shape comes from:
j(k) (e(Aj(k) )) = k (e(Bk ))

k = 1, . . . , n

(6)

suppose recognized image J = {s1 , . . . ,sp }, J . prove
also C recognized J. fact, recognized J exists transformation
another injective mapping q {1, . . . , m} {1, . . . , p} selecting J regions
{sq(1) , . . . , sq(m) }
e(sq(h) ) = h (e(Ah ))
226

h = 1, . . . ,

(7)

fiStructured Knowledge Representation Image Retrieval

(prototypical image of) C





































































prototypical image I(D)



image J
Figure 5: sketch If-proof Theorem 5
composing q j is, selecting regions J satisfying components
used recognize C one obtains
e(sq(j(k)) ) = j(k) (e(Aj(k) ))

k = 1, . . . , n

(8)

Then, substituting equals equals (6), one finally gets
e(sq(j(k)) ) = k (e(Bk ))

k = 1, . . . , n

proves C recognized J, using transformation components,
q(j()) injective mapping {1, . . . , n} {1, . . . , p}. Since J generic image,
follows C . Since (I, ) generic too, C subsumes D.
if. reverse direction easier: suppose C subsumes D. definition,
amounts C every collection images I. every contains I(D),
I(D) Proposition 3. Therefore, I(D) C , is, C recognized I(D).

property allows us compute subsumption recognition, concentrate
complex shape recognition, using Theorem 2. concern decide whether
exists transformation matching j properties stated Theorem 2.
turns exact recognition, quadratic upper bound attained
possible transformations try.

227

fiDi Sciascio, Donini & Mongiello

Theorem 6 Let C = h1 , B1 u u hn , Bn composite shape description, let
image, segmented regions {r1 , . . . ,rm }. Then, m(m 1) exact
matches n basic shapes regions. Moreover, possible match
verified checking matching n pairs contours.
Proof. transformation matching exactly basic components regions also
exact match centroids. Hence concentrate centroids. correspondence
centroid basic component centroid region yields two constraints
. rigid motion scaling, hence four degrees freedom (two
degrees translations, one rotation, one uniform scaling). Hence, exact
match exists centroids basic components centroids
regions, completely determined transformation two centroids
basic shapes two centroids regions.
Fixing pair basic components B1 , B2 , let p1 , p2 denote centroids. Also,
let rj(1) , rj(2) regions correspond B1 , B2 , let vj(1) , vj(2) , denote
centroids. one transformation solving point equations (each one mapping
point another)
(
(1 (p1 )) = vj(1)
(2 (p2 )) = vj(2)
Hence, m(m 1) transformations. second claim,
matching centroids found, one checks edge contours basic components
regions coincide, i.e., (1 (e(B1 ))) = e(rj(1) ), (2 (e(B2 ))) = e(rj(2) ),
k = 3, . . . , n (k (e(Bk )) coincides contour region e(rj(k) ).
Recalling Formula (5) proof Theorem 2, means eliminate
outer quantifier (5) using computed , conclude C recognized iff:
j : {1..n} {1..m}

n
^

e(rj(i) ) = (i (e(Bi )))

i=1

Observe that, prune search, found above, one
check k = 3, . . . , n (k (centr(Bk ))) coincides centroid region rj ,
checking contours.
Based Theorem 6, devise following algorithm:
Algorithm Recognize (C,I);
input composite shape description C = h1 , B1 u u hn , Bn i,
image I, segmented regions r1 , . . . ,rm
output True C recognized I, False otherwise
begin
(1) compute centroids v1 , . . . ,vm r1 , . . . ,rm
(2) compute centroids p1 , . . . ,pn components C
(3) i, h {1, . . . , m} < h
compute transformation (p1 ) = vi (p2 ) = vh ;
228

fiStructured Knowledge Representation Image Retrieval

every k {1, . . . , n}
(k (e(Bk ))) coincides (for j) region rj
return True
endfor
return False
end
correctness Recognize (C,I) follows directly Theorems 2 6. Regarding
time complexity, step (1) requires compute centroids segmented regions. Several
methods computing centroids well known literature (Jahne, Haubecker, &
Geibler, 1999). Hence, abstract detail, assume exists function
f (Nh , Nv ) bounds complexity computing one centroid, Nh , Nv
horizontal vertical dimensions (number pixels). report Appendix
compute centroids, concentrate complexity terms n, m, f (N h , Nv ).
Theorem 7 Let C = h1 , B1 u u hn , Bn composite shape description, let
image Nh Nv pixels, segmented regions {r1 , . . . ,rm }. Moreover, let f (Nh , Nv )
function bounding complexity computing centroid one region. C
recognized time O(m f (Nh , Nv ) + n + m2 n Nh Nv ).
Proof. assumptions, Step (1) performed time O(mf (Nh , Nv )). Instead,
Step (2) accomplished extracting n translation vectors transformations 1 , . . . ,n components C. Therefore, requires O(n) time. Finally,
innermost check Step (3) checking whether transformed basic shape region
coincide performed O(Nh Nv ), using suitable marking pixels
region belong to. Hence, obtain claim.

Since subsumption two shape descriptions C reduced recognizing C I(D), upper bound holds checking subsumption composite
shape descriptions, simplification also Step (1) accomplished without
feature-level image processing.
4.2 Approximate Recognition
algorithm proposed previous section assumes exact recognition. Since
target retrieval real images, approximate recognition needed. start reconsidering proof Theorem 2, particular matrix equalities (4). Using
semantics approximate recognition (3), expanded formula evaluating C (I)
becomes following:

max min




max{sim(e(r1 ), (1 (e(B1 )))),



..
.
max{sim(e(r1 ), (n (e(Bn ))),



. . . , sim(e(rm ), (1 (e(B1 ))))) }

..
..
.
.
...,

sim(e(rm ), (n (e(Bn ))))

}




Properties 12 stated exact recognition reformulated hypotheses
sim, follows.
229

fiDi Sciascio, Donini & Mongiello

1. given transformation, assume one region among r 1 , . . . ,rm
maximally similar component. assumption justified supposing
negation: two regions maximally similar component,
maximal value low one, lowering overall value
external minimization. means maximizing row, assume
maximal value given one index among 1, . . . , m.
2. given transformation, assume region yield maximal similarity
one component. Again, rationale assumption region
yields maximal similarity two components two different rows, value
low one, propagates along overall minimum. means
minimizing maxima rows, consider different region row.
remark also approximate case assumptions imply regions
different shapes, since sim similarity measure 1 true overlap,
equal shapes different pose. assumptions state sim
function near plain equality.
assumptions imply focus injective mappings {1..n}
{1..m} also approximate recognition, yielding formula
max


n

min{sim(e(rj(i) ), (i (e(Bi ))))}

max

j:{1..n}{1..m} i=1

choices j two maxima independent, hence consider groups
regions first:
max

n

j:{1..n}{1..m}

max min{sim(e(rj(i) ), (i (e(Bi ))))}


i=1

(9)

Differently exact recognition, choice injective mapping j directly
lead transformation , since depends similarity transformed shapes
computed, is, choice depends sim.
giving definition sim, reconsider image features (color, texture)
skipped theoretical part ease presentation semantics. introduce weighted sums similarity measure, weights set user according
importance features recognition.
Let sim(r, hc, t, , Bi) similarity measure takes region r (with color c(r)
texture t(r)) component hc, t, , Bi range [0, 1] real numbers (where 1
perfect matching). note color texture similarities depend transformations, hence introduction change Assumptions 12 above. Accordingly,
Formula (9) becomes
max

j:{1..n}{1..m}

n

max min{sim(rj(i) , hc, t, ( ), Bi i)}


i=1

(10)

formula suggests groups regions image might resemble
components, select groups present higher similarity. artificially
constructed examples shapes C resemble other, may generate
exponential number groups tested. However, assume realistic
230

fiStructured Knowledge Representation Image Retrieval

images similarity shapes selective enough yield small number
possible groups try. recall Gudivadas approach (Gudivada, 1998)
even stricter assumption made, namely, basic component C appear
twice, region matches one component C. Hence approach
extends Gudivadas one, also aspect besides fact consider shape,
scale, rotation, color texture component.
spite assumptions made, finding algorithm computing best
Formula (10) proved us difficult task. problem continuous
spectrum searched, best may unique. observed
single points matched instead regions components
problem simplifies Point Pattern Matching Computational Geometry. However, even
recent results research area complete, cannot directly applied
problem. Cardoze Schulman (1998) solve nearly-exact point matching
efficient randomized methods, without scaling. also observe best match
difficult problem nearly-exact match. Also Chew, Goodrich, Huttenlocher,
Kedem, Kleinberg, Kravets (1997) propose method best match shapes,
analyze rigid motions without scaling.
Therefore, adopt heuristics evaluate formula. First all,
decompose sim(r, hc, t, , Bi) sum six weighted contributions.
Three contributions independent pose: color, texture shape. values color texture similarity denoted simcolor (c(r), c) simtexture (t(r), t),
respectively. Similarity shapes (rotation-translation-scale invariant) denoted
simshape (e(r), e(B)). feature, pair (region, component) compute
similarity measure explained Appendix. Then, assign similarities
feature say, color worst similarity group. yields pessimistic estimate
Formula (10); however, estimate Downward Refinement property holds (see
next Theorem 8).
three contributions depend pose, try evaluate pose
region selected group similar pose specified corresponding
component sketch. particular, simscale (e(r), (e(B)) represents similar scale
region transformed component, simrotation (e(r), (e(B)) denotes
e(r) (e(B) similarly (or not) rotated reference arrangement
components. Finally, simspatial (e(r), (e(B)) denotes measure coincident
centroids region transformed component.
summary, get following form overall similarity region
component:
sim(r, hc, t, , Bi) = simspatial (e(r), (e(B)) +
simshape (e(r), e(B)) +
simcolor (c(r), c) +
simrotation (e(r), (e(B)) +
simscale (e(r), (e(B)) +
simtexture (t(r), t)
231

fiDi Sciascio, Donini & Mongiello

coefficients , , , , , weight relevance feature overall similarity
computation. Obviously, impose + + + + + = 1, coefficients greater
equal 0. actual values given coefficients implemented system
reported Table 2 Section 6.
difficulties computing best , compute maximum
possible s. Instead, evaluate whether rigid transformation scaling
1 (e(B1 )), . . . , n (e(Bn )) rj(1) , . . . , rj(n) , similarities simspatial , simscale ,
simrotation . transformation iff similarities 1. not, lower
similarities are, less rigid transformation match components
regions. Hence, instead Formula (10) evaluate following simpler formula:
max

n

min{sim(rj(i) , hc, t, , Bi i)}

j:{1..n}{1..m} i=1

(11)

interpreting pose similarities different way. describe detail estimate
pose similarities.
Let C = hc1 , t1 , 1 , B1 i) u u hcn , tn , n , Bn i), let j injective function
{1..n} {1..m}, matches components regions {rj(1) , . . . , rj(n) } respectively.
4.2.1 Spatial Similarity
given component say, component 1 compute angles
components seen 1. Formally, let i1h
c counter-clockwise-oriented angle
vertex centroid component 1, formed lines linking centroid
centroids component h. n(n 1)/2 angles.
Then, compute correspondent angles region rj(1) , namely, angles j(i)j(1)j(h)

vertex centroid rj(1) , formed lines linking centroid
centroids regions rj(i) rj(h) respectively. pictorial representation angles
given Figure 6.
let difference spatial (e(rj(1) ), 1 (e(B1 )) maximal absolute difference
correspondent angles:
spatial (e(rj(1) ), 1 (e(B1 )) =

max

i,h=2,...,n,i6=h

|}
{|i1h

c j(i)j(1)j(h)

compute analogous measure components 2,. . . ,n, select maximum
differences:
n

spatial [j] = max{spatial (e(rj(i) ), (e(Bi ))}
i=1

(12)

argument j highlights fact measure depends mapping
j. Finally, transform maximal difference perfect matching yields 0
minimal similarity perfect matching yields 1 help function described Appendix. minimal similarity assigned every
simspatial (e(rj(i) ), (e(Bi )), = 1, . . . , n.
Intuitively, estimate measures difference arrangement centroids
composite shape group regions. exists transformation bringing
components regions exactly, every difference 0, simspatial raises 1 every
232

fiStructured Knowledge Representation Image Retrieval

f2

214

f3

f2

213

f1

f1

f3

215

f2

314

f1

f3

315

f4

415

f4

f4

f5

f5

R2

R3

f5

R2

R3

R2

R3

215
214
213
315

R1
R4

314

R4

R5

R1

R1
R4

R5

415

R5

Figure 6: Representation angles used computing spatial similarity component 1
region rj(1) .

233

fiDi Sciascio, Donini & Mongiello

R1

f1

51u

51h
41h
31h
21h

f2

u

R2

h

41u

31u

F5

21u

R5
R3

f3
R4

f4

Figure 7: Representation angles used computing rotation similarity component 1
region rj(1) .

component. arrangement scattered reference arrangement,
higher maximum difference. reason use maximum differences
similarity pair component-region clear prove later
measure obeys Downward Refinement property.
4.2.2 Rotation Similarity
every basic shape one imagine unit vector origin centroid oriented
horizontally right (as seen palette). shape used component
say, component 1 also vector rotated according 1 . Let ~h denote
rotated vector. = 2, . . . , n let c~ counter-clockwise-oriented angle vertex
i1h
centroid component 1, formed ~h line linking centroid component

1 centroid component i.
region rj(1) , analogous ~u ~h constructed finding rotation phase
cross-correlation attains maximum value (see Appendix). Then, = 2, . . . , n
let j(i)j(1)~
u line
u angles vertex centroid rj(1) , formed ~
linking centroid rj(1) centroid rj(i) . Figure 7 clarifies angles
computing.
let difference rotation (e(rj(1) ), 1 (e(B1 )) maximal absolute difference
correspondent angles:
rotation (e(rj(1) ), 1 (e(B1 )) = max {| c~ j(i)j(1)~
u |}
i=2,...,n
i1h

one orientation rj(1) cross-correlation yields maximum
e.g., square four orientations compute maximal difference
orientations, take best difference (the minimal one).
234

fiStructured Knowledge Representation Image Retrieval

fi

mi

Ri

Mi

Dj
dj
Rj
fj

Figure 8: Sizes distances scale similarity computation component 1 region
rj(1) .

repeat process components 2 n, select maximum
differences:
n

rotation [j] = max{rotation (e(rj(i) ), (e(Bi ))}
i=1

(13)

Finally, spatial similarity, transform rotation [j] minimal similarity
help . minimal similarity assigned every simrotation (e(rj(i) ), (e(Bi )),
= 1, . . . , n.
Observe also differences drop 0 perfect match, hence
similarity raises 1. region rotated reference
regions match component, higher rotational differences. Again, fact
use worst difference compute rotational similarities exploited
proof Downward Refinement.
4.2.3 Scale Similarity
concentrate component 1 ease presentation. Let 1 size
component 1, computed mean distance centroid points contour.
Moreover, = 2, . . . , n, let d1i distance centroid component 1
centroid component i. image, let Mj(1) size region rj(i) , let
Dj(1)j(i) distance centroids regions j(1) j(i). Figure 8 pictures
quantities computing.
define difference scale e(rj(1) ) 1 (e(B1 ) as:
)
(

min{Mj(1) /Dj(1)j(i) , m1 /d1i }

scale (e(rj(1) ), 1 (e(B1 )) = max 1

i=2,...,n
max{Mj(1) /Dj(1)j(i) , m1 /d1i }
235

fiDi Sciascio, Donini & Mongiello

repeat process components 2 n, select maximum differences:
n

scale [j] = max{scale (e(rj(i) ), (e(Bi ))}
i=1

(14)

Finally, similarities, transform scale [j] minimal similarity
help . minimal similarity assigned every simscale (e(rj(i) ), (e(Bi )),
= 1, . . . , n.
4.2.4 Discussion Pose Similarities
Using worst difference evaluating pose similarities components may appear
somewhat drastic choice. However, guided choice goal preserving
Downward Refinement property, even abandon exact recognition
previous section.
Theorem 8 Let C composite shape description, let refinement C,
.
is, = C uhc0 , t0 , 0 , B 0 i. every image I, segmented regions r1 , . . . ,rm , C (I)
DI (I) computed (11) using similarities defined above, holds (I) C (I).
Proof. Every injective function j used map components C extended
function j 0 letting j 0 (n + 1) {1, . . . , m} suitable region index range
j. Since (I) computed extended mappings, sufficient show
values computed Formula (11) increase reference values computed
C.
Let j1 mapping maximum value C (I) reached. Every extension

j10 j1 leads minimum value minn+1
i=1 Formula (11) lower C (I). fact,
pose differences (12), (13), (14), computed maximums strictly greater set
values, hence pose similarities either value, lower one. Regarding
color, texture, shape similarities, adding another component worsen values
components C, since assign components worst similarity group.
consider another injective mapping j2 yields non-maximum value v2 < C (I)
Formula (11). Using argument pose differences (12), (13), (14), every
extension j20 leads minimum value v20 v2 . Since v2 < C (I), also every extension
every mapping j different j1 yields value less C (I). completes
proof.

5. Prototype System
order substantiate ideas developed prototype system, written C++.
system client-server application working MS-Windows environment.
client side avails graphical user interface allows one carry
operations necessary query knowledge base, including canvas query sketch
composition using basic shapes module query example using new existing
images queries. client also allows user insert new shape descriptions images
knowledge base. client logical structure shown Figure 9. made
three main modules: sketch, communication configuration.
236

fiStructured Knowledge Representation Image Retrieval

Figure 9: Architecture prototype system.

237

fiDi Sciascio, Donini & Mongiello

Figure 10: process reclassification images new description inserted: a)
insertion description (No. 9); b) insertion.

communication module manages communication server side, using
simple application-level protocol. configuration module allows one modify
parameters relative preview images shapes transferred server
placed cache managed FCFS policy efficient display. sketch module
allows user trace basic shapes palette items, properly insert modify
varying scale rotation factor. available shapes may basic ones
ellipse, circle, rectangle, polygons obtained composing basic shapes complex
shapes defined previous sessions application inserted knowledge
base, also shapes extracted segmented images.
system keeps track transformations corresponding users actions,
uses building (internal) shape descriptions stored previously described
syntax. color texture drawn shapes set according user requirements, client interface provides color palette possibility open images
JPEG format texture content. user also load images local disk
transmit server populate knowledge base. Finally, user define
new objects endowing textual description insert knowledge
base.
server side, also shown Figure 9, composed concurrent threads
manage server-side graphical interface, connections communications
client applications carry processing required client side. Obviously,
238

fiStructured Knowledge Representation Image Retrieval

Figure 11: query retrieved set images.

239

fiDi Sciascio, Donini & Mongiello

server also carries tasks related insertion images knowledge base,
including segmentation, feature extraction region indexing, allows one properly
set various parameters involved. end, server three main subcomponents:
1. image features extractor contains image segmentation module region
data extraction one;
2. image classifier composed classifier module module used
image reclassification;
3. database management system.
feature extractor segments processes images extract relevant features
detected region, characterize images knowledge base. Image segmentation carried algorithm starts extraction relevant edges
carries region growing procedure basically merges smaller regions larger
ones according similarity terms color texture. Detected regions obviously
comply minimal heuristics. region associated description
relevant features.
classifier manages graph used represent hierarchically organizes
shape descriptions: basic shapes, complex ones obtained combining elementary shapes and/or applying transformations (rotation, scaling translation).
basic shapes parents, top hierarchy. Images,
inserted knowledge base segmentation process, linked descriptions
structure depending specific descriptions able satisfy.
classifier module invoked new description inserted
system new query posed. classifier carries search process hierarchy
find exact position new description (a simple complex one)
inserted: position determined considering descriptions new description
subsumed by. position found, image reclassifier compares
images available database determine satisfy it; images
verify recognition algorithm tied D. stage considers images
tied descriptions direct ancestors D, outlined Figure 10.
usual Description Logics, also query process consists description insertion,
query Q new description treated prototypical images: query
Q system considered new description added hierarchical data
structure; images connected either Q descriptions query
hierarchical structure returned retrieved images.
database management module simply keeps track images and/or pointers
images.
Using system straightforward task. logon user draw sketch
canvas combining available basic shapes, enrich query color texture
content. query posed server obtain images ranked according
similarity. Figure 11 shows query sketch two circles retrieved
set. system correctly retrieves pictures cars two circles recognized
relative positions sketch represent wheels, also snow man
black buttons.
240

fiStructured Knowledge Representation Image Retrieval

Figure 12: Downward refinement (contd.): detailed query, picturing car,
retrieved set images.

241

fiDi Sciascio, Donini & Mongiello

Figure 13: Subsumption example: increasing number objects query leads
correct reduction retrieved set.
242

fiStructured Knowledge Representation Image Retrieval

introduction details restricts retrieved set: adding chassis
previous sketch makes query precise, well retrieval results, shown
Figure 12. example points expect user use system. He/she
start generic query objects. number images retrieved set
still large, he/she increase number details obtaining downward refinement.
Notice presence regions/objects included query obviously accepted lack region explicitly introduced query. idea
underlying approach enormous amount available images,
current stage research technology system always ensure complete recognition; yet believe focus reducing false positives, accepting without
much concern higher ratio false negatives. basically means increasing precision,
even cost possibly lower recall. words believe preferable
user looking image containing yellow car, e.g., using sketch Figure 12,
he/she receives result query limited subset images containing almost sure
yellow car, large amount images containing cars, also several images
cars all.
Subsumption another distinguishing feature system. Figure 13 shows queries
composed basic shapes obtained segmentation image picturing
aircrafts, i.e., aircraft basic shape system. Here, better emphasize
example, shape position contribute similarity value. process
subsumption clearly highlighted: query single aircraft retrieves images
one aircraft, also one aircraft. Adding aircrafts graphical
query correctly reduces retrieved set. example also points system
able correctly deal presence one instance object images,
possible approaches Gudivada Raghavan (1995) Gudivada
(1998). negative side noticed system recognize
presence third aircraft (indeed strange one, B2-Spirit) second image
Figure 13-b), segmented considered part background.
ability system retrieve complex objects also images several
different objects, main shapes, anyway seen Figure 14.
real image directly submitted query. Notice case system carry
segmentation process fly, detect composing shapes.

6. Experiments Results
order assess performance proposed approach system implementing
it, carried extensive set experiments test dataset images.
well known evaluating performances image retrieval system difficult
lack ground truth measures. ease possibility comparison, adopted
approach first proposed Gudivada Raghavan (1995). experimental framework
hence largely based one proposed there, relies comparison system
performances versus judgement human experts.
noticed work test images iconic images,
classified terms spatial relationships icons; experiments images
243

fiDi Sciascio, Donini & Mongiello

Figure 14: query example retrieved images.

244

fiStructured Knowledge Representation Image Retrieval

Figure 15: sample images used experiments.

245

fiDi Sciascio, Donini & Mongiello

real classification carried image features, including color, texture,
shape, scale, orientation spatial relationships.
test data set consists collection 93 images; sample shown
Figure 15, complete set available URL:
http://www-ictserv.poliba.it/disciascio/jair images.htm.
Images acquired using digital camera, combining 18 objects, either simple
objects (i.e., single shape) composite ones, variable size color. images
size 1080 720 pixels, 24 bits/pixel. noticed actually
18 different objects, considered similar variants object, e.g., two
pens different color, single test object.
selected test data set 31 images used queries. query set formed
two logical groupings.
first one (namely queries 1 15 queries 27, 30 31) primary
objective testing performance system using query single objects composed
various shapes. is, assessing ability system detect retrieve images
containing object, objects similar query.
query images second group (remaining images test data set) pictured
two objects chosen assess ability system detect
retrieve images according spatial relationships existing objects query.
Obviously difference queries containing single objects composed several
shapes, queries containing two objects, cognitive one: system
queries composite shapes. However, observed performances changed
two groupings.
separately asked five volunteers classify decreasing order, according
judgment, 93 images based similarity image selected query
set. volunteers never used system briefly instructed
rank orderings based degree conformance database images
query images. allowed group images considered equivalent,
query, discard images judged wholly dissimilar query.
obtained five classifications, univocal, created final ranking
merging previous similarity rankings according minimum ranking criterion.
final ranking image respect query determined minimum one
among five available.
example consider classification Query nr.1, shown Table 1.
Notice images grouped together cell given relevance.
Image 2 ranked third position users 1,4, 5, users 2 3 ranked
fourth position, finally ranked position four. Notice image 24
criterion leads withdrawal ranked images. approach limits weight
images badly classified single users final ranking.
submitted set 31 queries system, whose knowledge base
loaded 93 images test set.
behavior system obviously depends configuration parameters,
determine relevance various features involved similarity computation.
configuration parameters fed system experimentally determined test bed
246

fiStructured Knowledge Representation Image Retrieval

user
1
2
3
4
5
final

1st
1
1
1
1
1
1

2nd
44, 88
44, 88
44, 88
44, 88
44, 88
44, 88

ranking
3rd
2, 3, 68, 80
3, 68, 80
3, 68, 80
2, 3, 68, 80
2, 3, 68, 80
3, 68, 80

4th
26
2, 26
2, 26
26
24 26
2, 26

5th
24

24

Table 1: Users rankings query nr.1
Parameter
Fourier descriptors threshold
Circular symmetry threshold
Spatial similarity threshold
Symmetry maxima threshold
Spatial similarity weight
Spatial similarity sensitivity f x
spatial similarity sensitivity f
shape similarity weight
shape similarity sensitivity f x
shape similarity sensitivity f
color similarity weight
color similarity sensitivity f x
color similarity sensitivity f
rotation similarity weight
rotation similarity sensitivity f x
rotation similarity sensitivity f
texture similarity weight
texture similarity sensitivity f x
texture similarity sensitivity f
scale similarity weight
scale similarity sensitivity f x
scale similarity sensitivity f
global similarity threshold

Value
0.98
0.99
0.30
0.10
0.30
90.0
0.40
0.30
0.005
0.20
0.11
110.0
0.40
0.11
90.0
0.40
0.07
110.0
0.40
0.11
0.50
0.40
0.70

Table 2: Configuration parameters, grouped feature type.

approximately 500 images starting test phase. shown Table 2.
parameters reported described Appendix. Notice that, dealing welldefined objects, gave higher relevance shape spatial features reduced
relevance scale, rotation, color texture.
resulting classification gave us called system-provided ranking.
adopted Rnorm quality measure retrieval effectiveness. Rnorm first
introduced LIVE-Project (Bollmann, Jochum, Reiner, Weissmann, & Zuse, 1985)
evaluation textual information retrieval systems used
experiments referenced paper Gudivada Raghavan. make paper
self-contained recall Rnorm defined.
Let G finite set images user-defined preference relation complete
transitive. Let usr rank ordering G induced user preference relation.
Also, let sys system-provided ranking. formulation Rnorm is:
Rnorm (sys ) =

1
S+
(1 +
)
+
2
Smax

+ number image pairs better image ranked system
ahead worse one; number pairs worse image ranked ahead
+
better one Smax
maximum possible number + . noticed
calculation + , , max based ranking image pairs sys relative
ranking corresponding image pairs usr .
247

fiDi Sciascio, Donini & Mongiello

Query nr.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Average Rnorm

Image nr.
1
2
3
4
5
6
7
10
11
12
13
14
15
18
20
25
26
27
28
31
33
34
35
36
37
39
41
42
50
78
79

Rnorm
0.92
0.92
0.93
0.95
0.99
0.94
0.93
0.93
0.95
0.74
0.60
0.84
0.83
0.99
0.91
0.89
0.80
1.00
0.74
1.00
1.00
0.99
0.91
0.89
1.00
0.99
0.93
0.98
1.00
0.88
1.00
0.92

Table 3: Rnorm values. (indicates single-object queries)

Rnorm values range [0,1]; value 1 corresponds system-provided
ordering database images either identical one provided human
experts higher degree resolution, lower values correspond proportional
disagreement two.
Table 3 shows results query final average Rnorm =0.92. Taking closer
look results, first group queries (single compound objects) average value
Rnorm =0.90, Rnorm =0.94 second grouping (various compound objects).
(The complete set result users ranking system ranking available online
appendix).
comparison, average Rnorm resulted 0.98 system presented Gudivada
Raghavan (1995), 24 iconic images used queries database
images, similarity computed spatial relationships icons. remark
system works real images computes similarity several image features,
believe results prove ability system catch good extent
users information need, make refined distinctions images searching
composite shapes. Furthermore, algorithm able correctly deal presence
one instance object images, possible approaches
(Gudivada, 1998). also noteworthy that, though parameters setting
object several experiments, cannot considered optimal yet, believe
room improvement system performance, also pointed
following paragraph.
Obviously system fail segmentation provide accurate enough
results. Figure 16 shows results Query 11, one worst R norm .
system retrieve images users considered relevant,
248

fiStructured Knowledge Representation Image Retrieval

Figure 16: Query results query 11, lowest Rnorm =0.60.
important wrongly confused sugar-drop wrist-watch, resulted
false positive. matter fact various images sweet-drops resulted properly
segmented. Nevertheless, highly relevant images successfully retrieved wrongly
retrieved one slightly selection threshold.
Another observation made human users, comparing query
single object, much driven color feature, including
spatial positioning. appeared various queries clearly visible using
example results Query 11. users selected highest relevance class images
color sugar-drop, gave lower ranking images (with sugar-drops)
closer spatial relationships different colors. observation may significant
related field object recognition.
final comment. reference system behavior terms retrieval time,
carry systematic testing, depends several variables: number images
database, number objects query, important depth hierarchy
- search time decreases basic shapes available. Limiting analysis
database loaded 93 test images, system required average 12 secs
answer query, machine Celeron 400 MHz CPU 128 MB RAM running
client server.
249

fiDi Sciascio, Donini & Mongiello

7. Conclusion
proposed Knowledge Representation approach Image Retrieval. started
observation current sketch-based image retrieval systems lack compositional
query language is, able handle queries made several shapes,
position, orientation size shapes relative meaningful.
recover this, proposed language describe composite shapes, gave
extensional semantics queries, terms sets retrieved images. cope
realistic setting beginning, also generalized semantics fuzzy membership
image description. composition shapes made possible explicit
use language geometric transformations (translation-rotation-scale),
borrow form hierarchical object modeling Computer Graphics. believe
distinguishing feature approach, significantly extends standard invariant
recognition single shapes image retrieval. extensional semantics allows us
properly define subsumption (i.e., containment) queries.
Borrowing also Structured Knowledge Representation, particular Description Logics, stored shape descriptions subsumption hierarchy. hierarchy
provides semantic index images database. logical semantics allowed us
define reasoning services: recognition shape arrangement image,
classification image reference hierarchy descriptions, subsumption
descriptions. tasks aside, speed up, main one, Image
Retrieval.
proved subsumption simple logic reduced recognition,
gave polynomial-time algorithm perform exact recognition. Then, realistic application setting extended algorithm approximate recognition, weighting
shape features (orientation, size, position), color texture.
Using logical approach formal specification, built prototype system using
state-of-the-art technology, set experiments assess efficacy proposal, fine tune parameters weights show approximate retrieval.
results experiments, although exhaustive, show approach catch
good extent users information need make refined distinctions images
searching composite shapes.
believe proposal opens least three directions future research. First,
language describing composite shapes could enriched either logicoriented connectives e.g., alternative components corresponding compositions sequences shape arrangements, cope objects internal movements video sequence retrieval. Second, techniques Computational Geometry could
used optimize algorithms approximate retrieval, study complexity recognition problem composite shapes might prove theoretical optimality
algorithms. Finally, large-scale experiments might prove useful understanding
relative importance attributed end users various features composite shape.
Acknowledgements
wish thank former students G. Gallo, M. Benedetti L. Allegretti
useful comments implementations, Marco Aiello comments earlier draft,
250

fiStructured Knowledge Representation Image Retrieval

Dino Guaragnella discussions Fourier transforms, anonymous referee
constructive criticism helped us improving paper.
research supported European Union, POP Regione Puglia sottomisura 7.4.1 (SFIDA 3), Italian Ministry Education, University Research
(MIUR, ex-MURST) projects CLUSTER22 subcluster Monitoraggio ambiente e territorio, workpackage: Sistema informativo per il collocamento dei prodotti ortofrutticoli
pugliesi Italian National Council Research (CNR), projects LAICO, DeMAnD,
Metodi di Ragionamento Automatico nella modellazione ed analisi di dominio.

Appendix A.
appendix briefly revise methods used extraction image features.
also describe smoothing function way compute similarity image
features introduced Section 4.2.
A.1 Extraction Image Features
order deal objects image, segmentation required obtain partition
image. Several segmentation algorithms proposed literature;
approach depend particular segmentation algorithm adopted. anyway
obvious better segmentation, better system work. system
used simple algorithm merges edge detection region growing.
Illustration technique beyond scope paper; limit
description image features computation, assume successful segmentation.


make description self-contained start defining generic color image { (x, y) | 1
x Nh , 1 Nv }, Nh , Nv horizontal vertical dimensions, respectively,


(x, y) three-components tuple (R, G, B). assume image
partitioned regions (ri ), = 1, . . . , satisfying following properties:
I=



(ri ), = 1, 2, . . . ,

{1, 2, . . . , m}, ri nonempty connected set
ri rj = iff 6= j
region satisfies heuristic physical requirements.
characterize region ri following attributes: shape, position, size, orientation, color texture.
Shape. Given connected region point moving along boundary generates complex
function defined as: z(t) = x(t) + jy(t), = 1, . . . , Nb , Nb number boundary
sample points. Following approach proposed Rui, She, Huang (1996) define
Discrete Fourier Transform (DFT) z(t) as:
Z(k) =

Nb
X

z(t)e

j 2tk
N
b

t=1

k = 1, . . . , Nb .
251

= (k)ej(k)

fiDi Sciascio, Donini & Mongiello

order address spatial discretization problem compute Fast Fourier
Transform(FFT) boundary z(t); use first (2Nc + 1) FFT coefficients form
dense, non-uniform set points boundary as:
zdense (t) =

Nc
X

Z(k)e

j 2tk
N
b

k=Nc

= 1, . . . , Ndense .
interpolate samples obtain uniformly spaced samples zunif (t), =
0, . . . , Nunif . compute FFT zunif (t) obtaining Fourier coefficients Zunif (k),
k = Nc , . . . , Nc . shape-feature region hence characterized vector 2N c +1
complex coefficients.
Position Size. Position determined region centroid computed via moment
invariants (Pratt, 1991). Size computed mean distance region centroid
points contour.
Orientation. order quantify orientation region r use
Fourier representation, stores orientation information phase values.
obviously deal also special cases shape region one symmetry, e.g., rectangle circle. Rotational similarity reference shape B
given region ri obtained finding maximum values via cross-correlation:
C(t) =

2N
Xc
2
1
ZB (k)Zri (k) ej 2Nc kn 0, . . . , 2Nc
2Nc + 1 k=0

Color. Color information region ri stored, quantization 112 values
color space, mean RGB value within region:
Rri =

X

R(p)

G ri =

X

G(p)

B ri =

pri

pri

X

B(p)

pri

Texture. extract texture information region ri method based
work Pok Liu (1999). Following approach, extract texture features
convolving original grey level image I(x, y) bank Gabor filters,
following impulse response:
h(x, y) =

2
2
1
x +y
2 2
ej2(U x+V y)

e
2 2

(U, V ) represents filter location frequency-domain, central frequency, scale factor, orientation, defined as:
=

p

U2 + V 2

= arctan U/V

processing allows extract 24-components feature vector, characterizes
textured region.
252

fiStructured Knowledge Representation Image Retrieval

A.2 Functions Computing Similarities
Smoothing function . similarity measures, use function (x, f x, f y).
role function change distance x (in 0 corresponds perfect matching)
similarity measure (in value 1 corresponds perfect matching),
smooth changes quantity x, depending two parameters f x, f y.
(x, f x, f y) =


x

f +"(1 f y) cos( 2f x )

fy 1

arctan[

(xf x)(1f y)
]
f xf



#

0 x < f x
x > f x

f x > 0 0 < f < 1.

input data approximate recognition algorithm shape description D,
containing n components hck , tk , k , Bk image segmented regions r1 , . . . ,rm .
algorithm provides measure approximate recognition I.
first step algorithm Section 4.2 considers regions image
segmented n components shape description finds
groups n regions rj(k) satisfying higher shape similarity shape components
D. purpose compute shape similarity, based Fourier representation
previously introduced, vector complex coefficients. measure denoted sim ss
invariant respect rotation, scale translation computed cosine
distance two vectors. similarity gives measure range [0,1] assuming
higher similarity simss = 1 perfect matching.
Given vectors X complex coefficients describing respectively shape
region ri shape component Bk , X = (x1 , . . . , x2Nc ) = (y1 , . . . , y2Nc )
P2Nc

l=1 xl yl
simss (Bk , ri ) = qP
P2Nc 2
2Nc 2
l=1 xl
l=1 yl

Shape Similarity. quantity simshape measures similarity shapes
composite shape description regions segmented image.
n

simshape = (max[1 simss (Bk , rj(k) )], f xshape , f yshape )
k=1

Color Similarity. quantity simcolor measures similarity terms color
appearance regions corresponding shapes composite shape description. following formula, color (k).R denotes difference red color
component k-th component region rj(k) , similarly
green blue color components.
color(k) =

q

[color (k).R]2 + [color (k).G]2 + [color (k).B]2

function takes maximum differences obtain similarity:
n

simcolor = (max{color (k)}, f xcolor , f ycolor )
k=1

253

fiDi Sciascio, Donini & Mongiello

Texture Similarity. Finally, simtexture measures similarity texture
features components corresponding regions.
texture (k) denotes sum differences texture components k-th
component region rj(k) dividing standard deviation elements.
n

simtexture = (max texture (k), f xtexture , f ytexture )
k=1

References
Aiello, M. (2001). Computing spatial similarity games. Esposito, F. (Ed.), Proceedings Eighth Conference Italian Association Artificial Intelligence
(AI*IA99), No. 2175 Lecture Notes Artificial Intelligence, pp. 99110. SpringerVerlag.
Ardizzone, E., Chella, A., & Gaglio, S. (1997). Hybrid computation reasoning
artificial vision. Cantoni, V., Levialdi, S., & Roberto, V. (Eds.), Artificial Vision,
pp. 193221. Academic Press.
Baader, F., & Hanschke, P. (1991). schema integrating concrete domains concept
languages. Proceedings Twelfth International Joint Conference Artificial
Intelligence (IJCAI91), pp. 452457, Sydney.
Bach, R., Fuller, C., Gupta, A., Hampapur, A., Horowitz, B., Humphrey, R., Jain, R.,
& Shu, C. (1996). Virage image search engine: open framework image
management. Storage Retrieval Image Video Databases, Vol. 2670, pp.
7687. SPIE.
Bertino, E., & Catania, B. (1998). constraint-based approach shape management
multimedia databases. MultiMedia Systems, 6, 216.
Bollmann, P., Jochum, F., Reiner, U., Weissmann, V., & Zuse, H. (1985). LIVEProject-Retrieval experiments based evaluation viewpoints. Proceedings
8th Annual International ACM SIGIR Conference Research Developement
Information Retrieval (SIGIR 85), pp. 213214. ACM, New York.
Brooks, R. (1981). Symbolic reasoning among 3-D models 2-D images. Artificial
Intelligence, 17, 285348.
Calvanese, D., Lenzerini, M., & Nardi, D. (1998). Description logics conceptual data
modeling. Chomicki, J., & Saake, G. (Eds.), Logics Databases Information
Systems, pp. 229264. Kluwer Academic Publisher.
Cardoze, D., & Schulman, L. (1998). Pattern matching spatial point sets. Proceedings Thirtyninth Annual Symposium Foundations Computer Science
(FOCS98), pp. 156165, Palo Alto, CA.
Carson, C., Thomas, M., Belongie, S., Hellerstein, J. M., & Malik, J. (1999). Blobworld:
system region-based image indexing retrieval. Huijsmans, D., & Smeulders,
A. (Eds.), Lecture Notes Computer Science, Vol. 1614, pp. 509516. Springer-Verlag.
Celentano, A., & Di Sciascio, E. (1998). Features integration relevance feedback analysis
image similarity evaluation. Journal Electronic Imaging, 7 (2), 308317.
254

fiStructured Knowledge Representation Image Retrieval

Chandra, A., & Harel, D. (1980). Computable queries relational databases. Journal
Computer System Sciences, 21, 156178.
Chang, S., Shi, Q., & Yan, C. (1983). Iconic indexing 2D strings. IEEE Transactions
Pattern Analysis Machine Intelligence, 9 (3), 413428.
Chew, L., Goodrich, M., Huttenlocher, D., Kedem, K., Kleinberg, J., & Kravets, D. (1997).
Geometric pattern matching euclidean motion. Computational Geometry, 7,
113124.
Cox, I., Miller, M., Minka, T., & Papathomas, T. (2000). bayesian image retrieval
system, PicHunter. IEEE Transactions Image Processing, 9 (1), 2037.
Di Sciascio, E., Donini, F. M., & Mongiello, M. (2000). Description logic image
retrieval. Lamma, E., & Mello, P. (Eds.), AI*IA 99: Advances Artificial Intelligence, No. 1792 Lecture Notes Artificial Intelligence, pp. 1324. Springer-Verlag.
Di Sciascio, E., & Mongiello, M. (1999). Query sketch relevance feedback contentbased image retrieval web. Journal Visual Languages Computing,
10 (6), 565584.
Donini, F., Lenzerini, M., Nardi, D., & Schaerf, A. (1996). Reasoning description logics.
Brewka, G. (Ed.), Foundations Knowledge Representation, pp. 191236. CSLIPublications.
Edelmann, S. (1999). Representation Recognition Vision. MIT Press.
El-Kwae, E., & Kabuka, M. (1999). Content-based retrieval spatial similarity image
databases. ACM Transactions Information Systems, 17, 174198.
Flickner, M., Sawhney, H., Niblak, W., Ashley, J., Huang, Q., Dom, B., Gorkani, M., Hafner,
J., Lee, D., Petkovic, D., Steele, D., & Yanker, P. (1995). Query image video
content: QBIC system. IEEE Computer, 28 (9), 2331.
Foley, J., van Dam, A., Feiner, S., & Hughes, J. (1996). Computer Graphics. Addison
Wesley Publ. Co., Reading, Massachussetts.
Fuhr, N., Govert, N., & Rolleke, T. (1998). DOLORES: system logic-based retrieval
multimedia objects. Proceedings 21st Annual International ACM SIGIR
Conference Research Developement Information Retrieval (SIGIR 98), pp.
257265, Melbourne, Australia.
Gevers, T., & Smeulders, A. (2000). Pictoseek: Combining color shape invariant features
image retrieval. IEEE Transactions Image Processing, 9 (1), 102119.
Gudivada, V. (1998). R-string: geometry-based representation efficient effective
retrieval images spatial similarity. IEEE Transactions Knowledge Data
Engineering, 10 (3), 504512.
Gudivada, V., & Raghavan, J. (1995). Design evaluation algorithms image
retrieval spatial similarity. ACM Transactions Information Systems, 13 (2),
115144.
Haarslev, V., Lutz, C., & Moeller, R. (1998). Foundations spatioterminological reasoning description logics. Proceedings Sixth International Conference
Principles Knowledge Representation Reasoning (KR98), pp. 112123.
255

fiDi Sciascio, Donini & Mongiello

Hacid, M.-S., & Rigotti, C. (1999). Representing reasoning conceptual queries
image databases. Proceedings Twelfth International Symposium Methodologies Intelligent Systems (ISMIS99), No. 1609 Lecture Notes Artificial
Intelligence, pp. 340348, Warsaw, Poland. Springer-Verlag.
Hartman, J., & Wernecke, J. (1996). VRML 2.0 Handbook. Addison-Wesley.
Hirata, K., & Kato, T. (1992). Query visual example. Pirotte, A., Delobel, C., &
Gottlob, G. (Eds.), Advances Database Technology Proc. 3rd Int. Conf. Extending Database Technology, EDBT, Vol. 580 Lecture Notes Computer Science, pp.
5671. Springer-Verlag.
Jacobs, C., Finkelstein, A., & Salesin, D. (1995). Fast multiresolution image querying.
Proceedings 22nd Annual Conference Computer Graphics Interactive
Techniques (SIGGRAPH 95), pp. 277286.
Jahne, B., Haubecker, H., & Geibler, P. (1999). Handbook Computer Vision Applications. Academic Press.
Ma, W., & Manjunath, B. (1997). NETRA: toolbox navigating large image database.
Proceedings IEEE International Conference Image Processing (ICIP 97),
Vol. 1, pp. 568571, Santa Barbara.
Marr, D. (1982). Vision. W.H. Freeman Co., Oxford.
Meghini, C., Sebastiani, F., & Straccia, U. (2001). model multimedia information
retrieval. Journal ACM, 48 (5), 909970.
Moeller, R., Neumann, B., & Wessel, M. (1999). Towards computer vision description
logics: recent progress. Proceedings IEEE Integration Speech
Image Understanding, pp. 101115.
Nebel, B. (1990). Reasoning Revision Hybrid Representation Systems. No. 422
Lecture Notes Artificial Intelligence. Springer-Verlag.
Niblak, W., Barder, R., Equitz, W., Flickner, M., Glasman, E., Petkovic, D., Yanker, P.,
& Faloustos, C. (1993). QBIC project: Querying images content using color,
texture, shape. Storage Retrieval Still Image Video Databases,
Vol. 1980, pp. 173182. SPIE.
Paquet, E., & Rioux, M. (1998). content-based search engine VRML databases.
Proceedings IEEE International Conference Computer Vision Pattern
Recognition (CVPR98), pp. 541546, Santa Barbara, CA.
Picard, R., & Kabir, T. (1993). Finding similar patterns large image databases.
Proceedings IEEE International Conference Acoustics Speech Signal
Processing (ICASSP 93), pp. 161164, Minneapolis, MN.
Pirri, F., & Finzi, A. (1999). approach perception theory actions: part 1.
Linkoping Electronic Articles Computer Information Science, No. 41. Linkoping University Electronic Press.
Pok, G., & Liu, J. (1999). Texture classification two-level hybrid scheme. Storage
Retrieval Image Video Databases VII, Vol. 3656, pp. 614622. SPIE.
256

fiStructured Knowledge Representation Image Retrieval

Pratt, W. (1991). Digital Image Processing. J. Wiley & Sons Inc., Englewood Cliffs, NJ.
Reiter, R., & Mackworth, A. (1989). logical framework depiction image interpretation. Artificial Intelligence, 41 (2), 125155.
Reiter, R. (1980). Equality domain closure first-order databases. Journal
ACM, 27 (2), 235249.
Rui, Y., Huang, T., & Mehrotra, S. (1997). Content-based image retrieval relevance
feedback MARS. Proceedings IEEE International Conference Image
Processing (ICIP 97), pp. 815818.
Rui, Y., She, A., & Huang, T. (1996). Modified Fourier descriptors shape representation
- practical approach. Proceedings 1st Workshop Image Databases
Multimedia Search, Amsterdam.
Sanfeliu, A., & Fu, K. (1983). distance measure attributed relational graphs
pattern recognition. IEEE Transactions Systems, Man, Cybernetics, 13 (3),
353362.
Smith, J., & Chang, S. (1996). VisualSEEK: fully automated content-based image query
system. Proceedings fourth ACM International Conference Multimedia
(Multimedia96), pp. 8798.
Straccia, U. (2001). Reasoning within fuzzy description logics. Journal Artificial Intelligence Research, 14, 137166.
Tagare, H., Vos, F., Jaffe, C., & Duncan, J. (1995). Arrangement: spatial relation
parts evaluating similarity tomographic section. IEEE Transactions Pattern
Analysis Machine Intelligence, 17 (9), 880893.
Ullman, J. D. (1988). Principles Database Knowledge Base Systems, Vol. 1. Computer
Science Press, Potomac, Maryland.
Woods, W. A., & Schmolze, J. G. (1992). KL-ONE family. Lehmann, F. W. (Ed.),
Semantic Networks Artificial Intelligence, pp. 133178. Pergamon Press. Published
special issue Computers & Mathematics Applications, Volume 23, Number
29.
Yen, J. (1991). Generalizing term subsumption languages Fuzzy logic. Proceedings
Twelfth International Joint Conference Artificial Intelligence (IJCAI91), pp.
472477.
Zadeh, L. (1965). Fuzzy sets. Information Control, 8, 338353.

257

fi ff
fiff ff


!"$#%&ff')(*,+.-//-10321-46521*4

789:; <=/*1>ff/"(?A@9
&'<B/*1>ff/-

CEDGFH;IKJML1NOIPJQNORTS)L8UWVPX1HZY[NOIP\]H^V_FM`PaOXcbedaOVPfPJhg
FfKHiY[NjUWVkX1H]DlLffFnmoNORcpq`KX1H;JraOIKstY[NOIPJ3FudvaOL1IwFMJ

xzy|{%}~8z{%}$k{3uy

n%uuw3w

q{3yfi~8{%$ycqhM}]y|

uM%Mw3w

%=k6%,=u"hA_|=,
n

A|ff,ff6P 36 ff

=fiz
Bff ff| 6|A6^1n.fi68wM|;68n)v| ;"fi]8686A
A.|h|nK ^ z)v8|6h)fifin,1fi686"))6
))6.8A"6A"uz;P68j.)||A6GA6
nnfi86M

w"86jlA)kAz16||z.,Gfifi8|"
fi1wA|6A n8lffA| ))|q;6wnn86A%
jfi1|
|wv
|n,|)
6).,fi6|833|1|fiK8n86A|
ff
A68A|G6. 6|||;l
|j]AnA
fi8]81.jA|))1fiwA
fi1. "_|8
1|8|1_|kfi8|6

,P^|"A6,8Kk68
ff
8"=6]A6^81.fi^h=1fi^G1|filnfi^;.ffl6)
6)nG.AvffAffff
^|8w686B16|A| 1G|G)8BA68 "
)|Bfi|ff
8 hhB8 .6|fi61G|Av),|6.A^8
fi6"_K"6.6w|B6QAz1|66ff]|
^|"

fi!"#$
%&('*),+.-0/2143 57698:3 -0;*<>=$%?@<BA;43,C+ED+,+.'F-08&(-G&9<:+,HI3(<J3>K'*&ML5N+,HO6+P8+.-08+,<+.'Q/:3 /2RN&('S3 '*HT8+M3(<&('0RU'06
V &H0+.5$W V 3 /2;*+ V 3 /2RN)M3 5U5UXZY[&(10'*H0+,HZD&(/2;\&('5N&(69RN),<]3 '*HZ698:3 -0;\/2;*+,&(82XF=$^&ML_3OWa`Mb9c dAfehg_;*&(1069;\/2;*+.X
;43,C+iD+,+.' V 3 RU'05UXj<2/21*HORN+,Hk3(<l3I698:3 -0;0RN)M3 5_RU'Q/+.8mY$3(),+\Yn&(8>5N&(69RN),<l&(8l3(<Z3IHORo3 698:3 VV 3 /2RN)T<2XO<2/+ V
& Y5N&(69RN),<I=pYn&(8ZRU'*<2/:3 '*),+9Wq<+,+srt+.8 V +.5URU'06+.8MWu`Mb9b9vOWhY[&(8Z6+.'*+.8:3 5%?E<+,w10RUC(3 5N+.'/Z/&yx{zu|}AfWh/2;*+.RU8
698:3 -0;O~$/2;*+,&(8+./2RN)uYn&(10'*H*3 /2RN&('*<];43MC+D+,+.'5N+,<:<qRU'QC+,</2RU6Q3 /+,He]y&9<2/qL&(82KO<RU'/2;0RN<_3 8+M3>3 8+5UR V RU/+,H
/&TfpaNl(a:G[O99mBafWG&(8@BaN9mBaT=$^&L3OW{`Mb9c d*%;*+.RU'S1069'0RN+.8MW}`Mb9b9AfW4L;0RN):;
),&(828+,<2-&('*HI/&/2;*+P-G&9<2RU/2RUC+9Wa),&('(10'*)./2RUC+l3 '*HI+fORN<2/+.'Q/2Ro3 5Y8:3 6 V +.'/u& Y!x}zu|LRU/2;*&(10/JY[10'*)./2RN&('*<Me
g_;0RN< V &H0+.57;43(</2;08+,+EY[10'*H*3 V +.'Q/:3 5h);43 8:3()./+.82RN</2RN),<M
`9eJ&(DO2+,)./<3 8+\D0RU-43 82/2RU/+yUQfU(2B4t='*&H0+,<>8+.-08+,<+.'/f4[[[nBT3 '*HfU9[[90iD+./L+,+.'
/2;*+,<:+P+.'Q/2RU/2RN+,<BAf
Oe8+M3(<:&('0RU'06<3 8+@D43(<+,H&('F698:3 -0;O~$/2;*+,&(8+./2RN)&(-+.8:3 /2RN&('*<MW08+.5UXRU'06&('3lKRU'*HI& Yh698:3 -0;I;*& V & ~
V &(82-0;0RN< V )M3 5U5N+,Hsa2mM2,[n(h
eRU/lRN<5N&(69RN)M3 5U5UXtYn&(10'*H0+,HWh8+M3(<&('0RU'06<ZDG+.RU'06y<&(10'*H3 '*Hk),& V -05N+./+sLPe 8Me /Meyx}zu|<+ V 3 '/2RN),<MW
1*<143 5U5UXsDXiL_3,XI& Y{/2;*+@/28:3 '*<25o3 /2RN&(')M3 5U5N+,HyEe
3 RU'j+fO/+.'*<2RN&('*<& Y/2;*+\<R V -05N+\698:3 -0;*< V &H0+.5$W{K+,+.-0RU'06698:3 -0;;*& V & V &(82-0;0RN< V D43(<:+,Ht&(-O~

+.8:3 /2RN&('*<@3 '*H<:&(10'*Hy3 '*Hy),& V -05N+./+Z<+ V 3 'Q/2RN),<MW3 8+ip,ffaBlBNBF=$?@&9<2;yr1QL&('06<+9W!`Mb9b9vO
^O3 5UC93 /]1069'0RN+.8MWa`Mb9b9O^O3 5UC93 /MW*`Mb9b9cA!3 '*HTGB.2E(2B4=$%;*+.RU'7W1069'0RN+.8MWQ^R V &('*+./MWa`Mb9b9cO
8+.5U5N+.8MWO1069'0RN+.8MWO%;*+.RU'7W`Mb9b9cAfY[&(8_QfGf2(]_+,w10RUC93 5N+.'Q//&x}zu|]WO3 'T&(82RU69RU'43 5H0+,HO1*)./2RN&('


-//-]u&&4 < 6 ff<_B!1ffzff" :ff"|9
&'"&|M

!'&M&N1<

fi

3MfiuM%

<2XO<2/+ V =[@+.8HORU5N+,<MW0`Mb9b9A}),& V D0RU'*+,<]3 '43 5UX/2RN)/:3 D05N+M3 1OLRU/2;Z/2;*+_<2R V -05N+]698:3 -0;*<h-08& 2+,)./2RN&('7eq^& V +
KRU'*Hj& Y(*f[2(pai;43,C+sD+,+.'j-08&(-&9<+,H/&C93 5URNH*3 /+s3IK'*&L5N+,HO6+sD43(<+i),& V -&9<+,H& Y<2R V -05N+
698:3 -0;*<=[RU'*+M3 1RN<<:3(&(10R$W7`Mb9b4RUD0RN+9W0E3(+ VV +.825N9Wa|7&(RN<+M3 17W7`Mb9b9cAfe
rt+-08+,<+.'Q/;*+.8+3uY$3 V RU5UXZ& Y7+fO/+.'*<2RN&('*<]& Y/2;*+J<R V -05N+J698:3 -0;*< V &H0+.5$ehg_;*+),& VV &('i698&(10'*H
Yn&(8/2;*+,<+F+f/+.'*<RN&('*<RN<Z/2;43 /&(DO2+,)./<\3 8+j(U(2jBGU(mf48+.-08+,<+.'/2RU'06I,,fWfNB&(8
(*f[2(pafWq3 '*Hk&(-G+.8:3 /2RN&('*<3 8+iD43(<+,Hj10-&('k-08& m+,)./2RN&('7e?uRUC+.'3IK'*&ML5N+,HO6+sD43(<+s3 '*H3
<2R V -05N+698:3 -0;=L;0RN); V 3MX>8+.-08+,<+.'/!3uw1*+.82XW3J6&3 5$W4e,e,e,W9H0+.-G+.'*HORU'06@&('Z/2;*+3 -0-05URN)M3 /2RN&('aAfW9/2;*+
Q29O,[n(a2QfNf3(<2KO<{L;*+./2;*+.8)M3 'lD+_H0+,HO1*),+,HlY8& V eh),),&(8HORU'06@/&u/2;*+KRU'*H0<h& YG&(DO2+,)./<
),&('*<2RNH0+.8+,HRU'WHOR+.8+.'Q/8+M3(<&('0RU'06 V &H0+.5N<3 8+&(D0/:3 RU'*+,HW),& V -&9<2RU'06/2;*+Y$3 V RU5UXe!g_;*&(1069;
<2R V RU5o3 8q'*&(/2RN&('*<& Y82105N+,<3 '*H),&('*</28:3 RU'Q/<)M3 '\DG+Yn&(10'*H\RU'/2;*+%?5URU/+.8:3 /2108+9W/2;*+.RU8]),& V D0RU'43 /2RN&('
RU'\8+M3(<&('0RU'06<];43(H'*+.C+.8]D+,+.'\<2/21*HORN+,He]z'*+RU'/+.8+,<2/& Y&(1083 -0-08&3():;i/2;1*<q8+,<2RNH0+,<]RU'-08&MCRNHORU'06
3l10'0RY[XRU'06Y[8:3 V +.L&(82KF),& V D0RU'0RU'06i82105N+,<3 '*H),&('*<2/28:3 RU'Q/<RU'IHOR+.8+.'/L_3,XO<Me
'F/2;0RN<J-43 -+.8MW0L+@Y[&).1*<J&('I/2;*+EYn&(8 V 3 5H0+f*'0RU/2RN&('*<& Yh/2;*+,<:+ V &H0+.5N<MW*RU'*).5U1*HORU'06/2;*+.RU8&(-+.8:3~
/2RN&('43 5<+ V 3 'Q/2RN),<3 '*Hi8+.5o3 /2RN&('*<;0RU-*<LRU/2;Fx}zu|qW*3 '*HsL+E<2/21*HOX\/2;*+EH0+,).RNH*3 D0RU5URU/XF3 '*HT),& V -05N+fORU/X
& Y!/2;*+.RU8u3(<<:&).Ro3 /+,HIH0+,).RN<RN&('-08&(D05N+ V <MW4'43 V +.5UXI),&('*<2RN<2/+.'*).X3 '*HIH0+,HO1*)./2RN&('7eg_;*+,<+>8+,<2105U/<+f~
/+.'*H3 '*H),& V -05N+./+I/2;*+F&('*+,<\3 5U8+M3(HOX-010D05URN<2;*+,HDX/2;*+I3 10/2;*&(8<y=[3 6+./1069'0RN+.8MW_99O`Afe
g_;*&(1069;yDG&(/2;y),&('*<2RN<2/+.'*).X3 '*HH0+,HO1*)./2RN&('3 8+>10'*H0+,).RNH*3 D05N+lRU'/2;*+ V &9<2/6+.'*+.8:3 5 V &H0+.5{& Yq/2;0RN<
Y$3 V RU5UXWL+\;43(Hj3 5U8+M3(HOX1*<+,H3FH0+,).RNH*3 D05N+i<10D*<+./>& Y_82105N+,<>/&I<:&(5UC+/2;*+TG.{*7.:P-08&(D05N+ V W
3T/+,<2/m~$D+,Ht-08&(-&9<+,HRU'j/2;*+\K'*&ML5N+,HO6+T3(),wQ10RN<2RU/2RN&('k),& VV 10'0RU/X=[3 6+./MWh?E+.'*+,</MW}1069'0RN+.8MW
`Mb9b9bAfert+I-08+,<+.'Q/\;*+.8+FY[&(8/2;*+T*8</\/2R V +3SH0+./:3 RU5N+,H3 '43 5UXO<2RN<\& Y@),& V -05N+fORU/XL;*+.'L+I8+f~
<2/282RN)./_/2;*+uK'*&L5N+,HO6+@D43(<+u/&l/2;0RN<KRU'*HT& Y}82105N+,<P=[)M3 5U5N+,HS2(@Bf[fn,2T82105N+,<BAfehrt+@3 5N<&Z<2/21*HOX
-43 82/2RN).105o3 8)M3(<:+,<J& Yh),&('*</28:3 RU'Q/<Me
'<+,)./2RN&('D43(<RN)FH0+f*'0RU/2RN&('*<3 '*Hk8+,<2105U/<\3 DG&(10/\<R V -05N+i698:3 -0;*<i3 8+i8+,)M3 5U5N+,He^+,)./2RN&('
0- 8+,<+.'/<3 'I&MC+.82CRN+.L& Yh/2;*+@_tY$3 V RU5UXe 'F-43 82/2RN).105o3 8MW*L+P+f-05o3 RU'L;QXsL+P),&('*<2RNH0+.8698:3 -0;0RN)M3 5
Yn+M3 /2108+,<J& Y/2;*+@<2R V -05N+E698:3 -0;*< V &H0+.573(<+,<<+.'/2Ro3 5Y[&(8_K'*&ML5N+,HO6+ V &H0+.5URU'063 '*HT-&(RU'Q/J&(10/_/2;43 /
/2;*+,<+T-08&(-+.82/2RN+,<i3 8+T-08+,<+.82C+,HRU'/2;*+TY$3 V RU5UXe ''*+fO/\<+,)./2RN&('*<L+I<2/21*HOXk/2;*+IHOR+.8+.'/
V + V D+.8<s& YE/2;*+IYn3 V RU5UXe105N+,<i3 8+IRU'/28&HO1*),+,HRU'<:+,)./2RN&('d*W_),&('*<2/28:3 RU'Q/<sRU'<:+,)./2RN&('vOW3 '*H
<+,)./2RN&('ti<2/21*HORN+,< V &H0+.5N<E),& V D0RU'0RU'06F82105N+,<@3 '*HS),&('*<2/28:3 RU'/<Me@<u<&&('S3(<82105N+,<@3 8+lRU'QC&(5UC+,HSRU'
8+M3(<&('0RU'06<MW/2;*+\3(<:<&).Ro3 /+,HSH0+,).RN<2RN&('S-08&(D05N+ V <>3 8+l'*&(/@H0+,).RNH*3 D05N+9WD010/@L+l+fO;0RUD0RU/>3i),&('*HORU/2RN&('
=p*'0RU/++fO-43 '*<2RN&('<+./<BAl10'*H0+.8iL;0RN);),& V -010/:3 /2RN&('*<T3 5UL_3,XO<i</&(-7e '/2;*+I-43 82/2RN).105o3 8T)M3(<+I&
2(Q2B.[Bn,2y82105N+,<Wa/2;*+Z),& V -05N+fORU/Xy& Y]/2;*+,<+-08&(D05N+ V <uYn3 5U5{RU'Q/&s/2;*+-G&(5UX'*& V Ro3 5h;0RN+.8:3 8);QXe
^+,)./2RN&('sRN<!H0+.C&(/+,H/&u/2;*+,<+H0+,).RNH*3 D05N+J)M3(<:+,<Me '<+,)./2RN&('cOW98+.5o3 /2RN&('*<2;0RU-*<qLRU/2;&(/2;*+.8!L&(82KO<q3 8+
+,<2/:3 D05URN<2;*+,He 'I-43 82/2RN).105o3 8L+P-&(RU'Q/&(10/3 5U6&(82RU/2; V RN)>),&('0'*+,)./2RN&('*<LRU/2;),&('*<2/28:3 RU'/<:3 /2RN<mY$3()./2RN&('
-08&(D05N+ V <=$%^ Ai3 '*H<2;*&L/2;43 /s/2;*+I-08&(D05N+ V & Y@):;*+,)KRU'06/2;*+),&('*<2RN<2/+.'*).X& Y>3tK'*&L5N+,HO6+
D43(<+),& V -G&9<+,Hy& Yq<2R V -05N+698:3 -0;*<u3 '*Hy),&('*<2/28:3 RU'Q/<=_7~),&('*<2RN</+.'*).X0AJRN<+,w10RUC(3 5N+.'//&i/2;43 /&
H0+,).RNHORU'06/2;*+@),&('*<RN<2/+.'*).XF& Y!3 V R0+,H%^ =[ u ~^!g@WIxa3 8269RN+.8MW4|3 '06*W4^);0RN+fW7`Mb9b9Afe
h\

z(#tz#$

kqu!q4

rt+8+,)M3 5U5*RU'/2;0RN<<+,)./2RN&('\D43(<2RN)_'*&(/2RN&('*<3 DG&(10/<2R V -05N+J),&('*),+.-0/2143 54698:3 -0;*<=$^&ML3OW`Mb9c d*0%;*+.RU'i
1069'0RN+.8MW`Mb9b9Afe{g_;*+,<+]698:3 -0;*<}3 8+]),&('*<RNH0+.8+,H3(<7/2;*+qK+.82'*+.5Yn&(8 V &9<2/K'*&ML5N+,HO6+8+.-08+,<+.'/:3 /2RN&('
Yn&(8 V 3 5URN< V <JD010RU5U/10-&('y^&ML_3O<L&(82KGe]g_;*+.XT3 8+>3 5N<:&Z/2;*+@D43(<2RN) V &H0+.5Y[&(8/2;*+PY$3 V RU5UXe


fi M]|9M.tM=uwuMM3
F

ff
fi;}]}$k{%}

uA{M}h

3(<2RN)Z&('Q/&(5N&(69RN)M3 5qK'*&ML5N+,HO6+ZRN<E+.'*),&H0+,HSRU'S3s<2/2821*)./2108+)M3 5U5N+,H3BO9fnePxa3()./2143 5!K'*&L5N+,HO6+
RN<l+.'*),&H0+,HjRU'Q/&jBGUZ(2B4QS=$^0?E<BAfWhH0+f*'*+,HLRU/2;8+,<-G+,).//&3F69RUC+.'<210-0-&(82/MeI ^0?RN<Z3
D0RU-43 82/2RU/+5o3 D+.5U5N+,HZ698:3 -0;=[<2/282RN)./25UX<-G+M3 KRU'06*W9RU/hRN<q3lZ[9mBaW<2RU'*),+_/2;*+.8+)M3 'D+_<+.C+.8:3 5*+,HO6+,<
D+./L+,+.'S/L&i'*&H0+,<BAfeuz'*+>).5o3(<<u& Yq'*&H0+,<8+.-08+,<+.'Q/<@+.'/2RU/2RN+,<MWa/2;*+&(/2;*+.8u).5o3(<<8+.-08+,<:+.'Q/<u8+.5o3~
/2RN&('*<2;0RU-*<]DG+./L+,+.'s/2;*+,<+J+.'Q/2RU/2RN+,<
e &H0+,<]3 8+5o3 D+.5U5N+,H\DX+.5N+ V +.'Q/<_& YG/2;*+<210-0-&(82/Me 5N+ V +.'Q/:3 82X
8+M3(<&('0RU'06<u3 8+P),& V -010/+,HFDQXI3l698:3 -0;I;*& V & V &(82-0;0RN< V )M3 5U5N+,HsGmmM2,[n(4e
"%$'&)(+*,&.-+*0/1*324657&)( (a8
&.-(T 9]

fi;}]} <10-0-G&(82/soT ![aUZ#
O(.[n(p;:j(2Qf2 <q*[i,,>=PB$0,['?(f@:m),&('*),+.-0//X-G+,<y(a8+.5o3 /2RN&('/X-+,<A5B&.-pP0(!
[[[[(2S4$yB*BM.C&E-8
F6F6F &H- G ms2.9[n( ::*BTmF(f :JI F6F6FLK 2B02,[M? f;:ON KQP ISRT5VU9]
fNff4l2>(o.[pG,qf0f,.(a(O(2QfNW5YX(mf:Z(Z&)((a[&)-92ZQ.a92IS:Z\
N^]\Q_2(*9`
]olif0, ::*\mY
_R 5+/pOM.JmtRU'*HORUCRNHO143 5 V 3 82K+.8A< 5Y&.(a=&.-(Gb
/
(PO( 9!p,s(pp,(4c 5 2p\BQap>f2(#
/$8
&)(d5eifa9FS :gfS0s6+.'*+.82RN) V 3 82K+.L8 =
9hOfC
fgi h /j5 0(f[n(h(mfZ9k
/JlmTfTn90BnQfBZfNff4m
/ _O9pS 9!oMpG(O(2QfN =
(ao
fI @(29B.JfU.\.4c 5
5U5-43 82/2Ro3 5&(8H0+.8<\LRU5U5DG+FH0+.'*&(/+,HDX\>W3 '*HWRYu'*+,+,H0+,HWRU'*H0+f0+,HDQXk/2;*+F'43 V +I& Yu/2;*+
<+./Z&('L;0RN):;/2;*+.X3 8+TH0+f*'*+,He 'k/2;0RN<l-43 -G+.8Z/2;*+iRU'Q/210RU/2RUC+ V +M3 '0RU'06t& Y1]p\q_tRN<3 5UL_3,XO<r]
pZs02.n( s9[[9kmY
_GeEx{RU6*e`l-43 82/2Ro3 5U5UXH0+,<).82RUD+,<@3s<210-0-G&(82/MWGL;0RN):;LRU5U5{D+1*<+,HyRU'Y[1082/2;*+.8
+f*3 V -05N+,<Me
uv
wyx>z,{y|>}~

u.

w3|,x,z

>|>|y,x}c>|,x

yL}|

| |,x

zx,S>06~,

>>,3|,x

3|3zyx3y6}3x3|3,,~Lz
0

>|y,>ywyx>z,{y|>}~

>|y,x
,3{3z0

& (

&.-"Vm6&H- n

/"m 7+TT3n
28"mL .3a .OyTg )p
n

x}RU69108+`9]^10-0-G&(82/

= QMq
< G(? fSfM0(f>
= pToq
< *[

fi;}]} B`yB{ <R V -05N+F698:3 -0;QCZ
f
O(.[Sl[(2B4$"$(*-4*L*465((aB-(0aMQS,.=\2B02,[M? f;:m
),&('*),+.-0/s'*&H0+,<(am8+.5o3 /2RN&(''*&H0+,<A5 pF0l[oM.l2+,HO6+,<A5p
MBa.nQf4Z(
f[[(aQs(2s$$(p;
: (2Qf2>
0Sy
: (*\:f2fm(I
$yOsQn92m\OaMW5
2Mi*.22Z\
.
9 :fIfU9[n(GMQ[j
(ay9a:GaMEI
o\Qfa2S:$M*y3*3Wi
4 9a
nQf4[
< B>*0. *fNff4_mb5!
9 p( .T>,:ft M._mEB0fI[B
aNB65
QfIaMQ>O lif('(? fST

: O\Ba5 2.9[n(aQH
pPfU2ySF
: /X-+=M A>=
(tfNff42&)-]
= (U2s[>/X-+
= (aOlQn(mI
Zf_Zy. 0(}$iO(B[
: mS/X-+=M A65
U Q>{

= & - G =m$M*3*dW46$MA*>*W4 En " K (GCmWS$MA*13*d64 Enb"VmI* FtFtFK n5 :(a:



fi3MfiuM%



aMbp>UQfN2S:F>O(y=/X-+= MAfW V 3 82K+.8= ,A2A>=d9hO.2\/X-G+= MApl(jfNff4Jm1&)(a=(U2
[s/X-+ =>(a V 3 82K+.8 = MAsps(.UffaPm1/lmTfTnT=(U2t V 3 82K+.8A5Bp9>(.N'>RyoT(
a(M ?M[9O(\( (f1
=]Of :B0 NRTp2.$M465
),&('*),+.-0/]'*&H0+_LRU/2;i3E6+.'*+.82RN) V 3 82K+.8RN<q)M3 5U5N+,H3PfGff[uGMQl=RU/!8+fY[+.8<]/&P3 '\10'*<2-+,).R4+,H

+.'Q/2RU/X& Y@3S),+.82/:3 RU'/X-+Af&(/2;*+.82LRN<+FRU/RN<)M3 5U5N+,H3 'pG('?[(0(_aMj=RU/\8+fYn+.8</&t3S<2-+,).R4)
RU'*HORUCRNHO143 5}H0+f*'*+,HRU'I/2;*+><210-0-&(82/BAfert+PLRU5U5h3(H0&(-0//2;*+EYn&(5U5N&MLRU'06T).5o3(<<2RN)M3 5{),&('QC+.'Q/2RN&('*<P3 D&(10/
^0?E<Me '/2;*+HO8:3,LRU'06k& YP3^0?ZWq),&('*),+.-0/'*&H0+,<3 8+8+.-08+,<+.'Q/+,HDQX8+,)./:3 '0695N+,<s3 '*H8+.5o3 /2RN&('
'*&H0+,<DQXs&MC93 5N<Me 's/+fO/2143 57'*&(/:3 /2RN&('7W08+,)./:3 '0695N+,<u3 8+E8+.-05o3(),+,HFDQ
X i3 '*HF&MC93 5N<DQ
X 6e]?E+.'*+.82RN)

3
2
8

K
.
+

8

<

3

8

+
&
U
R
2
/

/
,
+

H

e
_
g
Q
;
*
1
>
<

3

6
.
+
*
'
.
+
2
8
N
R
\
)
,
)
(
&
*
'
,
)
.
+
0
>
/

5

3
G

.
+
5
1

$


L
*

f
E
4
N
R
@
<
2
<
R
0
U
5

X
*
'
(
&

/
,
+
H
J
.

Z
e


'RU'*HORUCRNHO143 5
V
V
V
),&('*),+.-0/5o3 D+.k
5 $MS*y4lRN<'*&(/+,
H ter;*+.'RU'&(108+f*3 V -05N+,<L+I1*<+FD0RU'43 82X8+.5o3 /2RN&('*<MW_L+
,
3

X

8
.
+
0

5
(
3
,
)
E
+
Q
'
1
G

.
+

8

<
(
&

'
,
+

H

6
+,<_DQXTHORU8+,)./+,HI+,HO6+,<Mq3D0RU'43 82Xs8+.5o3 /2RN&('I'*&H0+uRN<_/2;*+.'IRU'*).RNH0+.'/J/&
V
V
+f*3()./25UX&('*+ZRU'*),& V RU'063 '*H&('*+Z&(10/26&(RU'06I+,HO6+9ex{RU6*es<2;*&MLJ<E/L&j=[),&('0'*+,)./+,H4A@<2R V -05N+l698:3 -0;*<
3 '*H3(<<21 V +,HF/&ZD+PH0+f*'*+,HI&C+.8J/2;*+P<210-0-&(82/J& Y!x}RU6*e`9e
b

wyx>z,{y|>}~

w3|,x,z

| |,x

| |,x
>|>|y,x}c>|,x

| |,x

>|>|y,x}c>|,x V

>|>|y,x}c>|,x

| |,x

wyx>z,{y|>}~

wyx>z,{y|>}~


0

0
yL}|TpLc,

w3|,x,z

| |,x

zx,S>06~,
0

zx,S>06~,

>|y,x

yL}|



x{RU69108+>O]^R V -05N+>698:3 -0;*<Me
g ;*+@+.5N+ V +.'/:3 82Xs8+M3(<:&('0RU'06&(-G+.8:3 /2RN&('7W*-08& m+,)./2RN&('7W0RN<J3lKRU'*HT& Y}698:3 -0;I;*& V & V &(82-0;0RN< V /2;43 /
_
-08+,<+.82C+,<_/2;*+-43 82/2Ro3 57&(8H0+.8H0+f*'*+,Hs&('T5o3 D+.5N<Meh|7+./_1*<*8<2/_-08+,).RN<+u/2;0RN<&(8H0+.8Yn&(8),&('*),+.-0/'*&H0+
5o3 D+.5N<Mert+T;43,C+H0+f*'*+,H/2;*+sY[&(5U5N&LRU'06j-43 82/2Ro3 5&(8H0+.8i&('/2;*+ V 3 82K+.8<+.j
/ /lmTfTnQJ
fRN<Z/2;*+
698+M3 /+,<2/l+.5N+ V +.'Q/F=pY[&(83 5U+
5 /
W \f A@3 '*H+.5N+ V +.'Q/<Z&
/3 8+-43 RU82LRN<+\'*&('k),& V -43 8:3 D05N+9e
g_;*+.'T/2;*+0(f[n((2QfP(9a:G]aMQ@f ERN</2;*+-08&HO1*)./& Y7/2;*+-43 82/2Ro3 5&(8H0+.8<_&(' &)(3 '*H
e $M*y4\$M *y 4R
\ 3 '*7
H \ e 'T&(/2;*+.8L&(8H0<MW43Z),&('*),+.-0/5o3 DG+.a
5 $MS*y4RN<
lgmTfTnQW0R$e+9H
5 $M *y 4R
RN<3\<210D0/X-+P&
3 '*HW4R`
"fffQW*/2;*+.g
' )M3 '
V &(8+><-G+,).R4)@/2;43 'S3),&('*),+.-0/u5o3 DG+.+
D+P3 'QX V 3 82K+.8MW*&(/2;*+.82LRN<E
+ V 1*<2/D+P+,w143 5/
& 7e
E 9]C
}M <qG2(BO(.b
5 -08& 2+,)./2RN&('

fi;}]} , y}J .7(G[
fm( pa$ pj\BQapfm( ( $$C4$J ( $[4y(aiB2( - $$E4$J - $[4g9hQnf
aB,.? f>MQBN$[hoEf
O(.[9mBa(\(\(4QpBYRl(H9fG EOf4ff=](al\:
$*2.n( ^ sZ:(a:_9a\fU9[n(SaMQ>UQf 6
5$MA*y3*3W4 r$$C4*$ $M4*y3*3$ W4y4 r$[4>

W

fi M]|9M.tM=uwuMM3
F

5] $$E4*[$ $M])4y4d\O$M]4

rt+'*&(/+Z P =$<210D*<21 V +,j
< PAJRYq/2;*+.8++fORN<2/<3-08& 2+,)./2RN&('Y[8& V RU'Q/&ZegqX-0RN)M3 5U5UXW
8+.-08+,<+.'/<J3Pw1*+.82XW3PYn3()./MW03 '*Hi-08& m+,)./2RN&('*<Y[8& V /&H0+f*'*+E3 '*<2L+.8<_/&e 'sx{RU6*eqOW

<210-0-&9<+[JBM2(2fOf\J.:,(4WO/2;*+.'F/2;*+.8+ERN<&('*+E-08& 2+,)./2RN&('TY8& V RU'/&8Zeqg_;*+ER V 3 6+@& Yq
DQXT/2;0RN<-08& 2+,)./2RN&('IRN</2;*+><210D0698:3 -0
; &
Ze







y{M}hhM]



[

g_;*+T<+ V 3 'Q/2RN),< V 3 -*<\^0?E<l/&/2;*+s+fORN<2/+.'/2Ro3 5_),&('(m10'*)./2RUC+I3 '*Hk-G&9<RU/2RUC+\Y[8:3 6 V +.'Q/\& Yx{zu|qe
?uRUC+.'3I<210-0-G&(82/@uW}3F),&('*</:3 'Q/PRN<l3(<<2RU69'*+,HS/&I+M3():;jRU'*HORUCRNHO143 5 V 3 82K+.8l3 '*Hj3 ' ~3(HORN)=8+,<2-7e
310'43 82X0AJ-08+,HORN)M3 /+lRN<u3(<<RU69'*+,H/&s+M3(); ~3(HORN)8+.5o3 /2RN&('=8+,<-7e),&('*),+.-0/BA/X-+9ex*&(8E<2R V -05URN).RU/XW
L+u),&('*<2RNH0+.8_/2;43 /_+M3();F),&('*<2/:3 'Q/&(8-08+,HORN)M3 /+u;43(<]/2;*+u<:3 V +'43 V +E3(</2;*+E3(<<&).Ro3 /+,Hi+.5N+ V +.'/_&
/2;*+u<210-0-G&(82/Me!<+./& Y7Yn&(8 V 105o3(<Jj
$d
4!RN<3(<<RU69'*+,H/&Z3 'QX<10-0-G&(82/W/28:3 '*<25o3 /2RU'06-43 82/2Ro3 5&(8H0+.8<
&('/X-+,<Me\y&(8+<2-G+,).R4)M3 5U5UXW7Y[&(8>3 5U5HORN<2/2RU'*)./P/X-+,j
< 3 '*J
H <21*);j/2;43 [
/ \V W}&('*+Z;43(<@/2;*+
Yn&(8 V 105o7
3 ] FtFtF ] $M $M] * FtFtF *y] 4
$M] * FtFtF *y] 4y4fW}L;*+.8+ "IYn&(8),&('*),+.-0/l/X-G+,<MWh3 '*H kRN<

&(/2;*+.82LRN<+/2;*+3 82RU/X& Y/2;*+J8+.5o3 /2RN&('i/X-G+9eq?uRUC+.'T3 'QX^0
? W3uYn&(8 V 105o3lj
$[4!RN<qD010RU5U/3(<qY[&(5U5N&LJ<Me
/+.8 V RN<3(<<RU69'*+,HT/&\+M3();y),&('*),+.-0/'*&H0+9]3ZHORN<2/2RU'*)./C93 82Ro3 D05N+EYn&(8J+M3();6+.'*+.82RN)P'*&H0+9W*3 '*HF/2;*+
),&('*<2/:3 '/P),&(828+,<2-G&('*HORU'06T/&sRU/< V 3 82K+.8P&(/2;*+.82LRN<+9ePg_;*+.'t3 '3 /& V 6$ W4Z=8+,<2-7e L$ * FtFtF *3 G 42ARN<
3(<<&).Ro3 /+,Hs/&+M3():;I),&('*),+.-0/'*&H0+Z=8+,<2-7e8+.5o3 /2RN&('F'*&H0H
+ & Yh3 82RU/X K AfWL;*+.8H
+ RN</2;*+/X-+u& Y/2;*+
'*&H0+9W*3 '*7
H Z=8+,<-7e $ARN<_/2;*+E/+.8 V 3(<<2RU69'*+,HT/&Z/2;0RN<_'*&H0+\=8+,<2-7e*3(<<RU69'*+,HT/&/2;*b
+ /2;F'*+.RU69;QD&(108
& `
Afe|7+./ $[4_D+@/2;*+P),&('(m10'*)./2RN&('& Yh/2;*+,<+>3 /& V <ej
$[4_RN<J/2;*+P+fORN<2/+.'Q/2Ro3 5{).5N&9<2108+P& +$E4fe
; Y4RU'ix{RU6*eq@RN< ] _$Jf,2(2fO.A $M]4 2mM2, $M_4
e 6*eh/2;*+Y[&(8 V 105o33(<:<2RU69'*+,H/&>/2;*+<10D0698:3 -07
JBM2(2fOf $ J4 \.\:f $M]*y_4 yffW $ *y_4 9]( !9! $M]* B4y4fe
8& 2+,)./2RN&('jRN<><&(10'*Ht3 '*Ht),& V -05N+./+\LPe 8Me /MeZ/2;*+<+ V 3 'Q/2RN),<ZEW10-t/&I3T'*&(8 V 3 5URU/X),&('*HORU/2RN&('
Yn&(8),& V -05N+./+.'*+,<</2;*+la9B\(.(f& Y{3l^0
? RN</2;*+E^0?M`
$[4q&(D0/:3 RU'*+,HTDX V +.8269RU'06),&('*),+.-0/
'*&H0+,<;43MCRU'06E/2;*+]<B3 V +]RU'*HORUCRNHO143 5 V 3 82K+.8Mehg_;0RN<q^0?3 5UL3,XO<h+fRN</<=n3 '*HRN<}),& V -010/:3 D05N+RU'l5URU'*+M3 8
/2R V +LRU/2;T3>'43 RUC+u3 5U6&(82RU/2; V Afe]x}RU69108+ <2;*&MLJ<3P),&(10'/+.8m~+f03 V -05N+E/&-08& 2+,)./2RN&('T),& V -05N+./+.'*+,<:<
L;*+.'t^0?E<3 8+'*&(/RU''*&(8 V 3 5Yn&(8 V
Yn&(8RU'*<2/:3 '*),+lH0&+,<'*&(/-08& m+,).//& kW4+.C+.'yRY!DG&(/2;^0?E<
;43,C+/2;*+l<:3 V +P5N&(69RN)M3 5h<+ V 3 'Q/2RN),<W4D010/uRU/-08& 2+,)./</&M
$ J4feJ^0?RU''*&(8 V 3 5}Y[&(8 V RN<<B3 RNHI/&
D+Za(f9 e











fiff






















r





t:a

r

t:a







!



r







t:a


"$#&%('*)+"$#-,.'*)0/21435#617'9835#6:;'<8=3>#6:4'98@?A#61CBD:;'<8FEG#61CBD:;'<8@HC#61CB:;'
%IKJMLF,ON4IPRQTSUIVQTWYXZR[Y\]IW
^`_aIJ4S`WbI^U[bXJdce^UN2f4SDSUIVQ$J4X_UV=IKW7geX_UV@hi;f;^^UN;Q>jIK_UQ([YJ4\5XVkMI_aIi4WYQ(ilj@k;_UXmQ5\n^U[bXJ



u

u

t:a

t:a

u

'OY3$[4+"'OYy$ J4

p

x{RU69108+ ]g_;*+E'*+,+,HTY[&(8'*&(8 V 3 5Y[&(8 V <

q

sr


uvu w

xrzy {

*t Cu|u|}

hy|My hyn }
qhM}]y|
K}$MAy
.\ (a


9] QM<]G2T(\fMQO(.j5kU0fF P ML$[4> >(Gi(4@:\ j$4*B[$[4 I[$$E465

~

2





:

fi

3MfiuM%

rt+y).5o3 R V +,HRU'/2;*+RU'Q/28&HO1*)./2RN&('/2;43 /^0?E<T3 8++,w10RUC(3 5N+.'/F/&j/2;*+-&9<2RU/2RUC+9W),&('(m10'*)./2RUC+
3 '*Hj+fORN<2/+.'/2Ro3 5qY[8:3 6 V +.'Q/Z& Yx{zu|LRU/2;*&(10/Y10'*)./2RN&('*<s=5N+./>1*<H0+.'*&(/+\RU/>DXx}zu|]= _W *A2AfeFz'*+
+ V D+,H0HORU'06FRN<ER VV +,HORo3 /+I=pY[8& V x{zu|/&I^0?E<BAfWD010/E8+,wQ10RU8+,<E/2;*+ZH0+f*'0RU/2RN&('t& Y3<210-0-&(82/E/2;43 /
H0&+,<J'*&(/3(H0H3 'X/2;0RU'06s/&/2;*+P<:+ V 3 'Q/2RN),<u& Y{/2;*+>RU'QC&(5UC+,H698:3 -0;*<Me] 9qfMQO(._RN<3<210-0-&(82/
L;*&9<+/28:3 '*<25o3 /2RN&('DQXSRN<E+ V -0/XWh 5W 5uL;*+.8+Z3 5U5hHORN</2RU'*).//X-+,<@3 8+>'*&('),& V -43 8:3 D05N+9e
RN<
/2;*+JC&)M3 D0105o3 82X=[),&('*<2/:3 '/<3 '*H\-08+,HORN)M3 /+,<BA!Y[&(8_3P<+./& YY[&(8 V 105o3(<MWL+),&('*<RNH0+.8/2;*+ a3 /<210-0-&(82/
$r4"ff$'&.(+*,&)-+*0/4L;*+.8j
+ &)(RN<8+,<2/282RN)./+,H/&Z/2;*+P+.5N+ V +.'Q/ j(WO/2;*+@8+.5o3 /2RN&('I/X-+,<J&
& - 3 8+
/2;*+@-08+,HORN)M3 /+,<& Y!3 82RU/Z
X RU
' \W*3 '*HI/2;*+@RU'*HORUCRNHO143 5 V 3 82K+.8<& /3 8+E/2;*+P),&('*<2/:3 '/<JRU
' e

U





=





D;
5

=


0
D; ` fiD;
2MmW|7+./
D+F3Sx}zu|]=U_W*APY[&(8 V 105o3&MC+.83yC&)M3 D0105o3 82Xekg_;*+^0?D;$`.4lH0+f*'*+,H&('
/2;*+P<210-0-&(82/J $r4RN<D010RU5U/u3(<Yn&(5U5N&MLJ<M!/&+M3();/+.8 V & Y(yL+3(<<&).Ro3 /+P3),&('*),+.-0/'*&H0+@/X-+,H
(=6+.'*+.82RN)@RYh/2;*+E/+.8 V RN<3lC93 82Ro3 D05N+9W0RU'*HORUCRNHO143 57LRU/2;3 V 3 82K+.81uRYh/2;*+@/+.8 V RN</2;*+P),&('*<2/:3 '/
j
MAfW3 '*Hk/&S+M3();3 /& V 6$M] * F6F6F *y]{W 4PL+I3(<<&).Ro3 /+F38+.5o3 /2RN&(''*&H0+Z/X-+,HfW]<21*);/2;43 /MW!Yn&(8
I[\+\xO
WO/2;*+j$/2;I'*+.RU69;DG&(D8& Y`ZRN</2;*+P),&('*),+.-0/J'*&H0+>3(<<:&).Ro3 /+,HT/&] e
g_;*+ V 3 -0-0RU'06D;
RN<_).5N+M3 825UXRU'(2+,)./2RUC+9W05W5hRU/ V 3 -*<HOR+.8+.'Q/_Yn&(8 V 105o3(<P='*&(/_RNH0+.'Q/2RN)M3 510-T/&


I\BaZOM.
yn
_ }A _ UOfo\. M2,[n(
y^ }$T

N += R>.9BZ T? fI?M:Qf(S: $SOiM.PmG(B\(+}iQ <qG2(O 9
BO9f{ $r4EfO.9M=*,((:i 9],(fZU j(a =
k Ofo>Pa2mM2,[n(B2(
$ 4P4$
$ 465

C93 82Ro3 D05N+@8+.'43 V RU'06A_/&\HOR+.8+.'Q/E^0?E<>='*&(/JRNH0+.'/2RN)M3 5}10-F/&3 'FRN<:& V &(82-0;0RN< V Afe]y&(8+,&MC+.8MW4RU/JRN<3
D0R 2+,)./2RN&('IRY{L+@8+,<2/282RN)./u^0?@</&Z/2;*&9<+@RU'I'*&(8 V 3 57Y[&(8 V e
|7+./!1*<h'*&L),&('*<2RNH0+.8!/2;*+x}zu|= _W *A7Yn&(8 V 105o3 " ] $ $ ] 4y4_=L;*+.8+ $ ] c4}RN<q3u),&('(m10'*)./2RN&('
& Y\3 /& V <FL;*&9<+C93 82Ro3 D05N+,<FD+.5N&('06/& ] Afeg_;*+ ( ~+.'082RN);*+,HY[&(8 V 105o3& RN<I/2;*+SY[&(8 V 105o3
$ .4k"
] $ $ ] 4
$ ] 4y4PL;*+.8+ +$ ] 4@RN<>/2;*+s),&('(m10'*)./2RN&('& Y/2;*+i3 /& V < j($M]4fW{Yn&(8l+.C+.82X
/+.8 V ]yRU' he_rt+>'*&L-08&C+l/2;*+P-08&(-+.82/XIDXI-G&(RU'/2RU'06s&(10/u/2;43 /MW{`A_Yn&(8 t3 '*H T/L&Tx}zu|= _W
*A}Yn&(8 V 105o3(<W
ZR7
$ .4 Z $ 4fWO3 '*HiAhY[&(83 'X\x}zu|]= _W *A}Yn&(8 V 105o3 h W $ .4"[
$ $ .4y4fW
3 '*HI),&('*).5U1*H0+@1*<RU'06g_;7e7`9e
x*&(8E/2;*+l&(/2;*+.8@HORU8+,)./2RN&('7W/2;*+l3 -0-43 8+.'/@-08&(D05N+ V RN<u/2;43 /Y[&(8 V 105o3(<P3(<<2RU69'*+,H/&/2;*+Z<210-0-&(82/
DQX3 8+P10'0RUC+.8<:3 5U5UXIwQ143 '/2R4+,H3 '*H3 8+P1*<+,HFRU'I/2;*+PH0+,HO1*)./2RN&('-08&),+,<:<Mequ&ML+.C+.8MWaL+>)M3 'H0&
LRU/2;*&(10/J/2;*+ V W4DQXT+.'*),&HORU'06\/2;*+P-43 82/2Ro3 5&(8H0+.8&('I/X-+,<JHORU8+,)./25UXTRU'F/2;*+^0?E<Me

] ` fi| ` |v ( |v


C+ 5 ` ]
5

U C
z
! ||

v ` |v


= |v


0
U
fi ] ` `D; `


U



C7

f!
6 UOfoMM2,[M? _Ba n9[n(
yn y^ }$ }AT
aI0ZM.mZa(f\(}\QM<qG(TfM0(f] $OZ,.um
N += Ru,(flU fO.
9M =.(S( : 9]
}
(a
QM <q2(1
=iOfSpyGmmM2,[n(B2(
4$B

C7$[4!C7$J465







<



2MmW|7+./CD+3i698:3 -0;tH0+f*'*+,Ht&('t3T<210-0-G&(82/Eue>g_;*++fO-43 '*<2RN&('& YdZW W] $E4fWRN<E/2;*+^0?
H0+f*'*+,H&('j/2;*+ a3 /l<10-0-G&(82/ $Ci
4 =L;*+.8+ RN<>/2;*+iC&)M3 D0105o3 82XY[&(8/2;*+\Yn&(8 V 105o3(<Z& Yuj$d42AfW
D010RU5U/@3(<JYn&(5U5N&MLJ<ME`A_Y[&(8u+.C+.82X),&('*),+.-0/'*&H0+[]" H h& YZ
W W] $EJ
4 ),&('/:3 RU'*<E3 '3(<<&).Ro3 /+,H
( W0<Me /MeUWYn&(8J+M3();),&('*),+.-0/J/X-G+Y 698+M3 /+.8&(8J+,w143 5/&rf W*3l10'43 82X
),&('*),+.-0/J'*&H0+b] "
8+.5o3 /2RN&('j'*&H0+\& Y/X-+h
5URU'0K+,Hj/&7]]
3 '*HjAuY[&(8>+.C+.82X8+.5o3 /2RN&(''*&H0+ok
] & YH =[& Y/X-+I
3 '*H
3 82RU/X K AfW4Yn&(8+.C+.82XF8+.5o3 /2RN&('y/X-G+E <Me /Me8\ W4L+Z3(H0HIRU' 6] `$[
4 3\8+.5o3 /2RN&(''*&H0+P/X-+,H
7
LRU/2;<:3 V +'*+.RU69;QD&(8<>3(<Y}
] eurt+>'*&LH0+f*'*+l/2;*+Z3 -0-05URN)M3 /2RN&('
3(<P W] }W3 '*HS),&('*).5U1*H0+
1*<2RU'06g_;7e`9W*'*&(/2RN).RU'06\/2;43 /uj$ $k4y4" Oe











{ <

<
7 @ <



;A




fi M]|9M.tM=uwuMM3
F



<2RU'06\<2R V RU5o3 8/28:3 '*<mYn&(8 V 3 /2RN&('*<MWG3Z).5N&9<+E8+.5o3 /2RN&('*<2;0RU-I/&Z/2;*+@-08&(D05N+ V & Yd.*fS:s94$(*f4
<2/21*HORN+,HsRU's/2;*+H*3 /:3 D43(<+4+.5NHs;43(<DG+,+.'T<2;*&L'7!);*+,)KRU'06\w1*+.82X),&('Q/:3 RU' V +.'/Y[&(8'*&('T8+,).108<2RUC+
),&('(m10'*)./2RUC+FwQ1*+.82RN+,<lRN<Z+,wQ10RUC93 5N+.'Q/Z/&S);*+,):KRU'06-08& 2+,)./2RN&('DG+./L+,+.'^0?E<F=$%;*+.RU'+./3 5$eUWJ`Mb9b9cO
1069'0RN+.8MW4999Afe


p

v` q

hy

]fiM}





]$A{M}h{

q

< K

B`y $

x*&(8/2;*+E<:3 K+E& Y}D08+.CRU/XWOL+@),&('*<2RNH0+.8_RU'FL;43 /Y[&(5U5N&LJ</2;43 /^0?E<J3 8+u69RUC+.'FRU's'*&(8 V 3 5Yn&(8 V W03 '*H
-010/_RU'Q/&Z'*&(8 V 3 5Yn&(8 V RY'*+,+,H0+,HI3Y[/+.83 V &HOR4)M3 /2RN&('7e]'*HWO<2RU'*),+@3Z^0?H0&+,<_'*&(/_'*+,+,HT/&lDG+@3
),&('0'*+,)./+,HI698:3 -0;7W*L+P),&(' a3 /+l3Z<+./& Y]^0?E<LRU/2;I/2;*+^0?&(D0/:3 RU'*+,HIDQXs-+.8mY[&(8 V RU'06\/2;*+>HORN<[m&(RU'/
10'0RN&('& YhRU/<+.5N+ V +.'/<Me 'F/2;*+@Y[&(5U5N&LRU'06sH0+f*'0RU/2RN&('7W*Y[&(8JRU'*</:3 '*),+9W*/2;*+^0?k8+.-08+,<+.'Q/<E3<+./&
^0?E<Me



*| B .(a>

x

_{


fi;}]}

Q29O:@fm( J P d5

@ 9]r}uQM<]G29lBO(.j5(S:

% ;*+.RU'3 '*H1069'0RN+.8\=m`Mb9b9A;43MC+Z<2;*&ML'y/2;43 /-08& 2+,)./2RN&(');*+,):KRU'06TRN<Y ~),& V -05N+./+ZLRU/2;3

8+,HO1*)./2RN&('SY[8& V
{ e wQ10RUC93 5N+.'*),+ZLRU/2;j%^ =[<:3 /2RN<ma3 D0RU5URU/Xy& Y3s),&('*<2/28:3 RU'/@'*+./L&(82K0AL_3(<
3 5N<&l1*<+,Hs5o3 /+.8>=[x*+,H0+.8 ]3 8HOR$W`Mb9b *1069'0RN+.8%;*+.RU'7W`Mb9b9AfW43 '*HsRU'*H0+.-G+.'*H0+.'/25UXTRU'3>C+.82X
<2R V RU5o3 8 V &H0+.5{DQXJ1*H0&(5Y=m`Mb9b9cA=[<+,+>-43 82/@c\& Yq/2;0RN<u-43 -+.8BAfeJrt+69RUC+>D+.5N&ML3 '*&(/2;*+.8E-08&& Y]&
/2;0RN<u8+,<2105U/ED43(<+,Hy&('38+,HO1*)./2RN&('yY8& V ~^!g@eg_;*&(1069; V &(8+Z),& V -05URN)M3 /+,HS/2;43 'y/2;*+l-08+.CRN&(1*<
&('*+,<MW0/2;0RN<J8+,HO1*)./2RN&('RN</2;*+ED43(<2RN<Y[&(8J&(/2;*+.8J8+,HO1*)./2RN&('*<-08+,<+.'/+,HI5o3 /+.8JRU'F/2;0RN<-43 -+.8Me
g_;*+lYn&(5U5N&MLRU'06I/2;*+,&(8+ V K+,+.-*<@RU'Q/&I3(),),&(10'/P/2;*+),& V -05N+fRU/X& Y),&('*),+.-0/3 '*HS8+.5o3 /2RN&('t/X-+
);*+,):KRU'06*W/2;*&(1069;\RU'/2;0RN<!-43 -+.8!/2;0RN<!/+,<2/q)M3 'i&(DQCRN&(1*<25UXD+_-+.8mY[&(8 V +,HZRU'-&(5UX'*& V Ro3 54/2R V +J<2RU'*),+
),&('*),+.-0/3 '*HF8+.5o3 /2RN&('/X-G+,<3 8+@&('05UXs5o3 D+.5N<-43 82/2Ro3 5U5UXF&(8H0+.8+,HIRU'3l;0RN+.8:3 8):;QXe



p

hy|My

q



hyn



sr
}




uvu _K*{o1!(aN.samfU.C=\

qhM}]y|

:B0lfO2LpTpifU( $ H5

Kv { $

2MmW Zx{RU8<2/@<+,+l/2;43 /@RY/X-+Z);*+,)KRU'06IRN<uRUB
' WG/2;*+.'} RN<P3 5N<&sRU' 3i-08& ~
2+,)./2RN&('7Wq+.'082RN);*+,HDX),+.82/2R4)M3 /+,<lYn&(83 5U5/X-+s):;*+,)KO<Z1*<+,HW!RN<3-G&(5UX'*& V Ro3 5),+.82/2R4)M3 /+9etg_;*+
8+,).RU-08&)M3 57RN<J&(DQCRN&(1*<25UXT/2821*+9eqrt+E'*&L<2;*&L/2;43 /MW*+.C+.'IRYh/X-+@);*+,)KRU'06i)M3 'DG+@H0&('*+@RU' $,I4fW
} jRN
< ~),& V -05N+./+9e
|7+./1*<'*&ML D010RU5NH3y8+,HO1*)./2RN&('Y[8& V ~^!g@e]g_;*+TRU'0-010/s& ~^!gRN<i3Y[&(8 V 105o3
RU' ~
),&('(m10'*)./2RUC+@'*&(8 V 3 5Yn&(8 V = ~
% xqAfW0R$e+9eq3Z),&('(m10'*)./2RN&('& YhHORN<[10'*)./2RN&('*<l=[).5o3 1*<+,<BAfW0+M3():;ILRU/2;3 /
V &9<2//2;08+,+@5URU/+.8:3 5N<W43 '*HT/2;*+@wQ1*+,</2RN&('IRN<_L;*+./2;*+.8J/2;*+.8+@RN<3l/28210/2;3(<:<2RU69' V +.'Q/& Yh/2;*+EC93 82Ro3 D05N+,<
& <21*);T/2;43 / RN</2821*+9
e u&(/2RN),+u/2;*+E).5o3(<<2RN)M3 5 ~^!g-08&(D05N+ V ),&('*<2RNH0+.8<).5o3 1*<+,<_LRU/2;F+f03()./25UX
/2;08+,+@5URU/+.8:3 5N<MW*D010/Y[&(8_Y[1082/2;*+.8J-08&& Y[<_L+P-08+fYn+.8/&1*<+E/2;*+>3 D&MC+PC(3 82Ro3 '/Me
|7+./ q"
DG+>3 'RU'*<2/:3 '*),+>& ~^!g@e*re 5$e&0e 6*e]L+P<210-0-&9<+@/2;43 /+M3():;C93 82Ro3 D05N+
F6F6F

G
3 -0-+M3 8<3 / V &9<2/&('*),+RU'i3E).5o3 1*<+9e!|7+./]1*<q).8+M3 /+Yn&(108]),&('*),+.-0/q/X-+,<!Yn&(8q+M3();iC(3 82Ro3 D05Nd
+ ]{ GW 4W
F3 '*H aert+3 5N<:&\).8+M3 /+>&('*+>8+.5o3 /2RN&('/X-+ hYn&(8+M3():;y).5o3 1*<+ Wa3 '*H38+.5o3 /2RN&('/X-G+ 0e
3():;k),&('*),+.-0/l/X-G+ SRN<P698+M3 /+.8l/2;43 ' 3 '*H 4W}/2;*+,<+i3 8+/2;*+&('05UX-&9<<2RUD05N+),& V -43 82RN<:&('*<
D+./L+,+.'yHORN<2/2RU'*)./J/X-+,<Me
rt+lD010RU5NHS/2;*+Z698:3 -0
; $ 4@3(<uYn&(5U5N&MLJ<MY[&(8@+.C+.82XC(3 82Ro3 D05Nr
+ ]tRU' IWL+Z;43MC+Z/2;08+,+),&('*),+.-0/
'*&H0+,Z
< 4
W 3 '*Q
H lRU7
' $ 43 '*Hi/L&l8+.5o3 /2RN&('s'*&H0+,</X-+,H 5URU'0KRU'06>/2;*+*8<2//&>/2;*+
5o3 /2/+.8_&('*+,<u=RU'/210RU/2RUC+.5UXW0RU/ V +M3 '*</2;43 //2;*+JC93 82Ro3 D05N1
+ ]F)M3 'sDG+C(3 5U143 /+,HiDQX &(
8 TAfeh|7+./

Kv { $







@
9 +

U `


+

9

`



7
9



;<


<






9



9




fi3MfiuM%





b

val

val

val



af

bt

c

val

bf

val

val

val

ct

cf

dt

2
1

2
3

C1

C1

C1

C1

C1

C1

C1

1



C2

C2

b

c



val

val

val

val

val

df

av

bv

cv

dv

...

3

C2



C2

C2

C2


C2



1

2

C1

2
3

1

3

C2

*|G*{$
1*<<B3,XF/2;43 //2;*+>/28210/2;C93 5U1*+.9 =8+,<2-7e
QA_RN<u3(<<&).Ro3 /+,HFLRU/2;ff`9y=8+,<-7eUAfe_g_;*+.'
Yn&(8+.C+.82X).5o3 1*<+"$--4ERU' =L;*+.8+!&0W&T3 '*H&i3 8+5URU/+.8:3 5N<&C+.8C93 82Ro3 D05N+,<C]}W
_3 '*HAfWGL+3(H0Hy/2;*+8+.5o3 /2RN&(''*&H0+,<u/X-G+,H Wa;43,CRU'063(<*8</E3 82691 V +.'/`9
F&(8U 4W3(<
<+,),&('*HI3 82691 V +.'/`
&(8g`aWO3 '*HF3(<_/2;0RU8HI3 82691 V +.'/U9 \&(8U aW/2;43 /J),&(828+,<-G&('*Hs/&\3 '
+.C93 5U143 /2RN&('& Y!/2;*+P).5o3 1*<+@/&\/2821*+i= V &(8+@-08+,).RN<+.5UXW4RY!L+@8+.-05o3(),+>RU'I/2;*+>).5o3 1*<+h
+M3():;-&9<2RU/2RUC+
=8+,<2-7e'*+.6Q3 /2RUC+AE5URU/+.8:3 59WaI7\\OW7DXy/2;*+\/28210/2;jC93 5U1*+=8+,<2-7e\/2;*+\'*+.6Q3 /2RN&('& Y_/2;*+/28210/2;
C93 5U1*+A3(<<&).Ro3 /+,HTLRU/2;I/2;*+@ 5'*+.RU69;QD&(8& Yh/2;*+@8+.5o3 /2RN&(''*&H0+9Wv}
RN<+.C(3 5U143 /+,H/&Z/2821*+Afe
x*&(8E).5o3 1*<+,<8+,<2/282RN)./+,H/&$ 4J&(8k$ 4fWaL+>-08&),+,+,Hy<2R V RU5o3 825UXW3(H0HORU'06 D0RU'43 82XI8+.5o3 /2RN&('
'*&H0+,<u&(8E&('*+l10'43 82XI8+.5o3 /2RN&('S'*&H0+9ebu&(/+>/2;43 /E;43MCRU'06 K ~).5o3 1*<+,<MWGL;*+.8+ K RN<E3i),&('*<2/:3 '/MWRN<u&
-082R V 3 82XSR V -G&(82/:3 '*),+/&F;43,C+s3T-&(5UX'*& V Ro3 5q/28:3 '*<2Y[&(8 V 3 /2RN&('7W{<2RU'*),+L+&(D0/:3 RU' Gfi
I8+.5o3 /2RN&('
'*&H0+,<_Y[&(8J+M3():;).5o3 1*<+9e
'i/2;*+698:3 -0;o$ 4fWO/L&Z),&('*),+.-0/'*&H0+,<7` 3 '*HQU9 \3 8+).8+M3 /+,HsYn&(8+M3():;FC(3 82Ro3 D05N+H]3 '*H
3ZD0RU'43 82Xs8+.5o3 /2RN&(''*&H0+5 9 P5URU'0KO<g` /&U9 aeqx*&(8+M3():;).5o3 1*<+)
" $-@-& 4fW*/2;*+.8+@RN<
&('*+P8+.5o3 /2RN&('K7, 5URU'0K+,H/&OU9 4WHU9 T3 '*HVU9 =n3 '*H<R V RU5o3 825UXTY[&(8u).5o3 1*<+,<LRU/2;S&('*+>&(8/L&
5URU/+.8:3 5N<BAfeIo$ 4@8+.-08+,<+.'Q/</2;*+w1*+,<2/2RN&(' RN<>/2;*+.8+i3FC93 5U143 /2RN&('& Y/2;*+C93 82Ro3 D05N+,<l<21*);/2;43 /l3 5U5
).5o3 1*<+,<J+.C93 5U143 /+@/&d 9 9l
g_;0RN<u/28:3 '*<mYn&(8 V 3 /2RN&('SY8& V /2;*+ ~^!gY[&(8 V 105o3$0`
W4 $UT 4JRN<uRU5U5U1*<2/28:3 /+,H
RU'tx{RU6*ed*e '/2;*+Z698:3 -0;ZW7'*&(/>3 5U5]+,HO6+,<PRN<<21*+,HyY[8& V /2;*+).5o3 1*<+,<P;43MC+DG+,+.'tHO8:3ML'7W7Yn&(8E/2;*+
<:3 K+Z& Y8+M3(H*3 D0RU5URU/Xe /uRN<ER VV +,HORo3 /+/&T);*+,):KS/2;43 /MWY[&(8P3Yn&(8 V 105o3!I
W/2;*+.8+ZRN<P3iC(3 5U143 /2RN&('t&
RU/<_C93 82Ro3 D05N+,<J<21*);/2;43 /J+M3();I).5o3 1*<+ERN<J+.C93 5U143 /+,HT/&Z/2821*+ERYh3 '*HF&('05UXsRY!8$ 4_)M3 'FDG+u-08& 2+,)./+,H
RU'Q/&Z$ 4fe

7

u&(/+/2;43 /l/2;*+i<10D*<21 V -0/2RN&('8+.5o3 /2RN&('RU'*HO1*),+,HkDQXj-08& m+,)./2RN&('&MC+.8^0?E<RN<l3w143(<2R~&(8H0+.8MWhD010/
'*&(/l3 '&(8H0+.8MlRU/RN<l3F8+R4 +fORUC+s3 '*H/28:3 '*<2RU/2RUC+iD010/>'*&(/3 'Q/2R~<2X VV +./282RN)M3 5_8+.5o3 /2RN&('7egL&^0?E<
x{RU69108+@d* * 3 V -05N+>& Yh/28:3 '*<mYn&(8 V 3 /2RN&('FY[8& V ~^!g/&

%AyM}

]]} ]{%}

3 8+P<:3 RNHT/&\DG+y,M?(Nf4RYh/2;*+.Xs-08& m+,)./J/&+M3();&(/2;*+.8Me]^0?RN<<:3 RNHF/&ZD+Z229aQ(4RYhRU/RN<
+,w10RUC(3 5N+.'//&&('*+u& Y{RU/<<2/282RN)./<210D0698:3 -0;*<>=R$e+9e]3l<210D0698:3 -0;F'*&(/+,w143 5/&oRU/<+.5YfAfWO&(/2;*+.82LRN<+ERU/
RN<J<:3 RNHT/&\DG+ZB2(a94ne

p

q

sr




uvu

hy|My hyn }
qhM}]y|
J(G(aL :fOS p( 1!m(aN.ia2QS!
Nf85Jfyy.M?(Nfa:.(:>(Z[P* ,0ZNMt$io,(\($aQofYRlB2(a94{9mBa 5

g_;*+pf2(a(4*.(f& Y}3Z^0?V
RN<_3 'sRU828+,HO10'*H*3 'Q/<210D0698:3 -0;F& Y+,w10RUC93 5N+.'Q/_/&PRU/@=L;*+.'
RN<RU828+,HO10'*H*3 'Q/MW4/2;0RN<_698:3 -0;sRN<RU/<+.5Y2WO&(/2;*+.82LRN<+E/2;*+.8+ V 3,XsDG+u<+.C+.8:3 5<21*):;I<210D0698:3 -0;*<MWD010/

/2;*+.XF3 8+>3 5U57RN<& V &(82-0;0RN)Afe

;



fi M]|9M.tM=uwuMM3
F

h

D#U_ 9

qu

;

^

(#$

dh#as$s O*


F



"







g_;0RN<<+,)./2RN&('iRN<H0+.C&(/+,Hi/&l3 's&MC+.82CRN+.L& Y7/2;*+HOR+.8+.'/ V &H0+.5N<),& V -G&9<RU'06/2;*+SY$3 V RU5UXehrt+
LRU5U5G*8<2/_&(10/25URU'*+E/2;*+ V 3 RU' V &(/2RUC93 /2RN&('*<Y[&(8_&(108_698:3 -0;O~$D43(<+,HI3 -0-08&3();I& Y}K'*&L5N+,HO6+E8+.-08+,<+.'O~
/:3 /2RN&('7e

C8
) 9o


}h

]ynAyn}A{}[{%}

hMy



y|{3M}]}hu

{h

x08& V 3 V &H0+.5URU'06ICRN+.L-G&(RU'/MW7L+\<+,+Z/L&I+,<<+.'/2Ro3 5q-08&(-+.82/2RN+,<PRU'
]y }$ $y
}
/2;*+Z<2R V -05N+l698:3 -0; V &H0+.5$e@g_;*+FQM2,.W<2R V -05N+l698:3 -0;*<MW73 8+l+M3(<2RU5UX10'*H0+.8<2/:3 '*H*3 D05N+ZDXy3 'S+.'*H~
1*<+.8=n3sK'*&ML5N+,HO6+i+.'069RU'*+,+.8>&(8P+.C+.'3 't+fO-G+.82/BAfe'*H2 .(4p(i3 8++M3(<2RU5UX10'*H0+.8</:3 '*H*3 D05N+
/&&0W9Yn&(8q/L&>8+M3(<&('*<-08& 2+,)./2RN&('iRN<3E698:3 -0; V 3 /):;0RU'06l&(-+.8:3 /2RN&('7W/2;Q1*<+M3(<2RU5UXRU'Q/+.82-08+./:3 D05N+E3 '*H
CRN<2143 5URN<:3 D05N+943 '*HF/2;*+@<B3 V +@5o3 '069143 6+@RN<1*<+,H3 /RU'Q/+.8mY$3(),+3 '*HI&(-+.8:3 /2RN&('43 575N+.C+.5N<Me
5U/2;*&(1069;/2;*+.8+RN<3E6Q3 -\D+./L+,+.'i/2;*+/2;*+,&(8+./2RN)M3 5aY[&(10'*H*3 /2RN&('*<<2/21*HORN+,H\;*+.8+3 '*Hi3u5o3 '069143 6+
1*<:3 D05N+TRU'8+M3 53 -0-05URN)M3 /2RN&('*<MWL+IL&(105NH5URUK+T/&SD082RN+ *X V +.'Q/2RN&('/L&-08& 2+,)./<RU'L;0RN):;/2;*+,<+
-08&(-+.82/2RN+,<;43MC+lDG+,+.'S+f;0RUD0RU/+,HeEg_;*+*8<2/u&('*+RN<E3 'S+f-+.82R V +.'Q/ERU'SH0&).1 V +.'/@8+./282RN+.C(3 5hH0&('*+
DQX\?E+.'*+,<2/=n999Afe 'Z/2;0RN<!L&(82KWQ),&('*),+.-0/2143 5a698:3 -0;*<]3 8+_1*<+,HZ/&@H0+f*'*+3u5o3 '069143 6+Y[&(8!RU'*H0+fORU'06
3 '*HFw1*+.82XRU'06H0&).1 V +.'Q/<Me%&('*),+.-0//X-G+,<J3 8+u/:3 K+.'TY[8& V /2;*+/2;*+,<B3 10821*<J& Y{
=n3 D&(10/
dQ9J99_/X-+,<BAfW(3H0&).1 V +.'Q/:3 82X@5o3 '069143 6+1*<+,H@RU' V &9<2/Y[8+.'*);-010D05URN)3 '*H@10'0RUC+.8<RU/:3 82X@5URUD08:3 82RN+,<Me
g_;*+>+fO-+.82R V +.'Q/E-08&MC+,H/2;*+@Yn+M3(<2RUD0RU5URU/X& Yh/2;*+>-08&(-G&9<:+,H<2XO<2/+ V =LPe 8Me /Me),& V -010/2RU'06s/2R V +A3 '*H
3 'R V -08&C+,H8+.5N+.C93 '*),+ILPe 8Me /Me/&/2;*++fRN</2RU'06j<2XO<2/+ V D43(<+,H10-&(' W V 3 RU'05UXHO1*+
/&/2;*+1*<+& Y<+ V 3 '/2RN)t8+.5o3 /2RN&('*<IRU'*</+M3(H& YlK+.XL&(8H0<&('05UXe z'*+t<2RNH0++f+,)./IL3(<3 5N<&/&
-08&MC+y/2;*+RU'Q/+.8+,<2/F& Y><2R V -05N+698:3 -0;*<iY8& V 3 V &H0+.5URU'06kCRN+.L-&(RU'Q/Me '*H0+,+,HW/2;*+.RU8s698:3 -0;0RN)M3 5
-08&(-+.82/2RN+,<+.'43 D05N+,Hi/&@D010RU5NHi3 'iRU'*H0+fRU'06 w1*+.82XRU'06/&&(5*/2;43 /]L_3(<),&('*<RNH0+.8+,Hs3(<+M3(<2XZ/&P1*<+_Yn&(8
/2;*+RU'*H0+f0+.8<Mehg_;*+1*<+.8<!L+.8+ V 3(<2/+.8];Q1 V 3 '0RU/2RN+,<_<2/21*H0+.'Q/<W'*&(/3ML3 8+Y[8& V ),&('*),+.-0/2143 5a698:3 -0;*<
'*+.RU/2;*+.8Y8& V u QLRU/2;i/2;*+u<& Y[/L_3 8+E3 '*HT3 'sRU'*H0+fRU'06l6910RNH0+9WO/2;*+.XDG+,)M3 V +Ew10RN)K5UX3 D05N+
/&ZD010RU5NHIRU'*H0+f*3 /2RN&('*<MW0/2;43 /L+.8+P),&('*<2RNH0+.8+,H& Y{;0RU69;Iw143 5URU/XTDXF3Z<:+.'0RN&(8J5URUD08:3 82Ro3 '7e
g_;*+i<+,),&('*Hj-08& 2+,)./>/:3 K+,<-05o3(),+\RU'K'*&L5N+,HO6+T+.'069RU'*+,+.82RU'06=[_&9<MWh_&(/+.5U5o3OW} ]3 '0;*+,+.69;*+9W
`Mb9b9Afe /<q-01082-&9<+JRN<]/2;*+),&('*<2/2821*)./2RN&('F& Y/&&(5N<qY[&(8 V &H0+.5URU'063 '*H<R V 105o3 /2RU'06l;Q1 V 3 'T&(826Q3 '0R 3~
/2RN&('*<MW3(<E+ V +.826+.'*).X-08&),+,HO108+,<Y[&(8uRU'*<2/:3 '*),+9ePz'*+ V 3 RU'HOR ).105U/XRU'yK'*&L5N+,HO6++.'069RU'*+,+.82RU'06
RN<_/&ZC93 5URNH*3 /+P3 V &H0+.5URU'06*WOR$e+9e!/&):;*+,)Ks/2;43 //2;*+@+f-+.82/8+M3(<&('0RU'06RN<),&(828+,)./25UX V &H0+.5N+,Heqg_;0RN<
C93 5URNH*3 /2RN&('yRN<E1*<143 5U5UXH0&('*+lL;*+.'y/2;*+ZH0+,<2RU69'SRN<@3();0RN+.C+,HW7;*+.8+DX<2R V 105o3 /2RU'06F/2;*+),&('*</2821*)./+,H
V &H0+.5URU'06>& YG/2;*+&(826Q3 '0R 3 /2RN&('7e]_/!/2;0RN<h*'43 54<2/:3 6+9W V &HOR4)M3 /2RN&('*<3 8+JC+.82Xl),&9<2/25UXehg_;*+_K+.XlRNH0+M3
& Y/2;*+J-08& 2+,)./]RN<]/&>&MC+.8),& V +/2;0RN<HOR ).105U/X\DX69RUCRU'06P/2;*++f-+.82/]/2;*+u3 D0RU5URU/X/&>1*<+J<2R V 105o3 /2RN&('
RU'*<2RNH0+E/2;*+PH0+,<RU69'I).X).5N+P3(<3 V +M3 '& Yh+.'082RN):;0RU'06s3 '*HTD010RU5NHORU'06\;0RN< V &H0+.5URU'06*e]g_;0RN<JR V -05URN+,</2;43 /
/2;*+@);*&9<:+.' V &H0+.5URU'065o3 '069143 6+P+.'43 D05N+,</2;*+@+fO-G+.82//&lYn&(5U5N&ML8+M3(<:&('0RU'06<J<2/+.-FDQXT<2/+.-7W0HORU8+,)./25UX
&('I;0RN<&ML' V &H0+.5UR 3 /2RN&('7e /L3(<uH0+,).RNH0+,HI/&D010RU5NH<21*);S3Z5o3 '069143 6+P10-&('),&('*),+.-0/2143 5{698:3 -0;*<Me
?E+.'*+.8:3 5q),&('*),+.-0/2143 5698:3 -0;*<P+,wQ10RUC93 5N+.'Q/>/&sx{zu|L+.8+\'*&(/P),&('*<2RNH0+.8+,Hj3(<E6&&HS)M3 '*HORNH*3 /+,<PD+f~
)M3 1*<+/2;*+.X3 8+RU'*H0+,+,H\3EHORo3 698:3 VV 3 /2RN)<2X</+ V & YG5N&(69RN)/2;43 /!RN<h'*&(/]3 /{/2;*+_+fO-G+.82/!5N+.C+.5$e '*<2/+M3(HW
/2;*+E5o3 '069143 6+EL3(<J698&(10'*H0+,HF10-G&('F<2R V -05N+E698:3 -0;*<3 '*Hs+fO/+.'*<2RN&('*<>=[<1*);3(<_'*+,<2/2RU'06<& Y{698:3 -0;*<BA
K+,+.-0RU'06I/2;*+.RU8@8+M3(H*3 D0RU5URU/Xez-G+.8:3 /2RN&('*< V R0+,H<2R V -05N+Z698:3 -0;tH0+,HO1*)./2RN&('=R$e+9e-08& 2+,)./2RN&('aAuLRU/2;
'*&('H0+,).5o3 8:3 /2RUC+>-08&),+,HO108+,<Me]x}RU8</J+f-+.82R V +.'Q/<L+.8+>),&('*).5U1*<RUC+9e

R










e

6



6

e

6



<

B^A{M}h{ $y } x08& V 3S),& V -010/:3 /2RN&('43 5uCRN+.L-&(RU'Q/MWL+F/2;0RU'0K/2;43 /\698:3 -0;O~
D43(<+,Hj8+M3(<&('0RU'06<MWhD+.'*+f*/2/2RU'06Y[8& V 698:3 -0;O~$/2;*+,&(8+./2RN)M3 58+,<2105U/<W{)M3 'kD082RU'063 'jRU'Q/+.8+,</2RU'06y-+.8m~
<2-+,)./2RUC+s/&5N&(69RN)s-08&(698:3 VV RU'06*ekX+f*3 V -05N+9W!/2;*+F+,wQ10RUC93 5N+.'*),+sDG+./L+,+.'^0?-08& 2+,)./2RN&('3 '*H

;A


fi

3MfiuM%

U


H0+,HO1*)./2RN&('TRU'ix}zu|+$ * 4)M3 'iD+<:+,+.'T3(<_3 's3 5U/+.82'43 /2RUC+uC+.8<2RN&('T& Y7/2;*+PO(\(\($aQof 02(f
=$%;43 '*HO8:3k +.825URU'7W@`Mb99AfW),&('*<RNH0+.8+,H3(<Y[10'*H*3 V +.'Q/:3 5Yn&(8H*3 /:3 D43(<+w1*+.82RN+,<s&(-0/2R V R 3 /2RN&('
=[D0RU/+.DG&(105$W105U5$W uRo3 'Q17W>`Mb9b9vAfez/2;*+.8s8+,<2105U/<s3 8+&(D0/:3 RU'*+,HY8& V ),&('*</28:3 RU'Q/s-08&(698:3 V ~
} p 4
V RU'06*eSg_;*+T<2/28&('06+,wQ10RUC93 5N+.'*),+D+./L+,+.'{ 3 '*Hk/2;*+

=[<:+,+u^+,)./MeOcOWL;*+.8+/2;*+/28:3 '*<mYn&(8 V 3 /2RN&('*<1*<+,H\K+,+.-T3 5U5a<&(5U10/2RN&('*<3 '*H-08+,<+.82C+
/2;*+J<2/2821*)./2108+& Y/2;*+),&('*<2/28:3 RU'/]'*+./L&(82K\RU'\/2;*+wQ1*+.82X*A!3 5U5N&MLJ</&@/28:3 '*<25o3 /+/2;*+8+,<2105U/<&(D0/:3 RU'*+,H
RU'/2;0RN<h5o3 /2/+.8]),& VV 10'0RU/X=DQXZ+f*3 V -05N+9W9/28:3()./:3 D05N+J)M3(<+,<hD43(<+,HZ10-&('/2;*+<2/2821*)./2108+J& YG/2;*+_698:3 -0;7W
?E&(/2/25N&(D7W|+,&('*+9W^)M3 8),+.5U5N&0W4`Mb9b9bAfW(*8<2/h/&u} } hWQ/2;*+.'Z/&EH0+,HO1*)./2RN&('RU'x}zua
| $ * 4fe
g_;*+T698:3 -0;</2821*)./2108+F)M3 '3 5N<&yDG+T1*<:+,H/&yH0+.C+.5N&(-+ ).RN+.'Q/s3 5U6&(82RU/2; V <RU' V &(8+F6+.'*+.8:3 5
V &H0+.5N<]& Y7/2;*+_yYn3 V RU5UX{RU'\/2;*+ V &H0+.54L+)M3 5U5a =[<+,+JD+.5N&MLAfW0%&(105N&('*HO8+u3 '*HT^O3 5UC(3 /E=m`Mb9b9cA
1*<+l/2;*+Z698:3 -0;O~$D43(<+,H'*&(/2RN&('t& Y]a$2:/&sD010RU5NHt3 't+ ).RN+.'Q/PD43():KL_3 8H~);43 RU'0RU'063 5U6&(82RU/2; V elg}&
+.'0;43 '*),+P/2;*+PY[&(82L_3 8H~);43 RU'0RU'063 5U6&(82RU/2; V 1*<+,HIRU'I/2;*+ V &(8+P6+.'*+.8:3 5 V &H0+.5N<& Yh/2;*+>Y$3 V RU5UXW
3 6+./t=n99O`As+f-08+,<<:+,<sH0+.-+.'*H0+.'*).RN+,<TD+./L+,+.'82105N+,<I3 '*H),&('*</28:3 RU'Q/<TRU'/+.8 V <F& Yl3698:3 -0;
;*& V & V &(82-0;0RN< V e
z108!3 R V RN</2;Q1*<{/&JD010RU5NH>Y[&(8 V 3 5O+fO/+.'*<2RN&('*<}& Y4<R V -05N+),&('*),+.-0/2143 5698:3 -0;*<MW(K+,+.-0RU'06u8+M3(H*3 D0RU5URU/X
& Y]&(DO2+,)./<u3(<L+.5U5!3(<8+M3(<&('0RU'06<WG3 '*H-08+fY[+.8:3 D05UXWa5N&(69RN)M3 5U5UXFYn&(10'*H0+,Heg_;*+>kY$3 V RU5UXFRN<u3*8<2/
<2/+.-FRU'I/2;0RN<HORU8+,)./2RN&('7e

$v$|


6

Kv { $

0F 2D{ v ;

K* { $
R
R

; <

U

&

z{T

|7+./}1*<'*&LjRU'OYn&(8 V 3 5U5UXP-08+,<+.'Q/}/2;*+q_sYn3 V RU5UXe{g_;*+q6+.'*+.82RN)-08&(D05N+ V /&JD+q<&(5UC+,HW hW
3(<2KO<MW(69RUC+.'3uK'*&ML5N+,HO6+JD43(<:+E=[@JA{3 '*H\3u<2R V -05N+J698:3 -0;iW9L;*+./2;*+.8)M3 'DG+H0+,HO1*),+,HZY[8& V
e!),),&(8HORU'06>/&>/2;*+JKRU'*H0<]& Y7&(DO2+,)./<]),& V -&9<2RU'06W&('*+&(D0/:3 RU'*</2;*+HOR+.8+.'Q/ V + V DG+.8<& Y/2;*+
Y$3 V RU5UXe '\/2;*+D43(<2RN) V &H0+.5aWRN<]),& V -&9<+,Hs& Y73><+./t& Y<2R V -05N+J698:3 -0;*<8+.-08+,<+.'Q/2RU'06lY$3()./<MW
3 '*H<&(5UCRU'06 3 V &(10'Q/</&S);*+,):KL;*+./2;*+.8/2;*+.8+TRN<\3-08& 2+,)./2RN&('Y[8& V RU'Q/&te
J105N+,<u3 '*HI),&('*<2/28:3 RU'/<E3 8+ V &(8+P),& V -05N+f&(DO2+,)./<D43(<+,HI10-G&('y<2R V -05N+@698:3 -0;*<MWG3 '*H&(-+.8:3 /2RN&('*<
H0+M3 5URU'06\LRU/2;I/2;*+,<:+P&(DOm+,)./<3 8+@D43(<+,HF10-&('F-08& m+,)./2RN&('7e
g_;08&(1069;*&(10//2;0RN<J<+,)./2RN&('7W0L+@LRU5U571*<+@+f*3 V -05N+,<JRU'*<2-0RU8+,HsY[8& V 3 V &H0+.5UR 3 /2RN&('& Yq3lK'*&L5~
+,HO6+\3(),w10RN<2RU/2RN&('j)M3(<+Z<2/21*HOXW7)M3 5U5N+,H*{*77,:.RU/@H0+,<).82RUD+,<>3i8+,<&(108),+3 5U5N&)M3 /2RN&('-08&(D05N+ V W
L;*+.8+/2;*+\3 R V RN<E/&I3(<<RU69't& ),+,<@/&T-G+.8<:&('*<@& Y_3s8+,<+M3 8):;t698&(10-L;0RU5N+ZY[105*5U5URU'06I<& V +),&('O~
<2/28:3 RU'/<=[3 6+./+./3 5$eUW7`Mb9b9bAfe


}

My| $y

$y

* {

v { $

6

4



Office

near

Office

near



Office

adjoin

Office

adjoin

Office

near




Researcher

member

Project

x}RU69108+vOqJ105N+,<



C

BUs+fO-08+,<<+,<K'*&ML5N+,HO6+T& Y_Y[&(8 V RYfiffRN<>-08+,<+.'//2;*+.' )M3 'kDG+s3(H0H0+,H :e /PRN<
y|
+.'*),&H0+,HFRU'Q/&s3Z<2R V -05N+@698:3 -0;I-08&CRNH0+,HLRU/2;I/L&),&(5N&(8<MW0/2;*+@*8<2/J),&(5N&(8<210D0698:3 -0;yH0+f*'0RU'06\/2;*+

;



fi M]|9M.tM=uwuMM3
F



Office: #1

adjoin

Office: #2

adjoin

near



Office: #1

Office: #3

adjoin

near

adjoin

Office: #2

Office: #4

near

adjoin

Office: #3

near

near

near

near

adjoin

Office: #4

x{RU69108+>O!J105N+3 -0-05URN)M3 /2RN&('*<
;QX-&(/2;*+,<2RN<>3 '*H/2;*+\<:+,),&('*H),&(5N&(8P/2;*+\),&('*).5U1*<2RN&('7e 'tHO8:3,LRU'06<WL+8+.-08+,<+.'Q//2;*+;QX-&(/2;*+,<2RN<
DQXyL;0RU/+'*&H0+,<MW3 '*HS/2;*+),&('*).5U1*<2RN&('jDQXy698:3,X&('*+,<MeZx{RU69108+\vT<2;*&MLJ<@/2;08+,+\82105N+,<Me 3 '*H
8+.-08+,<+.'/PK'*&L5N+,HO6+3 D&(10/P/2;*+TG2(s8+.5o3 /2RN&('7W<10-0-G&9<+,Hy/&TD+H0+f*'*+,HD+./L+,+.'& ),+,<P&('05UXe
+ ]FRN<2(3 'T& ),Y
+ _W/2;*+.Z
' _
+fO-08+,<<+,</2;43 /_/2;*+E8+.5o3 /2RN&('SG2(lRN<_<2X VV +./282RN)M3 5]= RY{3 'F& ),b
RN<uG2(E
] AfW /2;43 / RY}3 'F& ),1
+ ]j2.(0@3 's& ),Y
+ _/2;43 /Pm,(0u3 'F& ),+ l/2;*+.
' ]FRN<G2(
:eg_;*+P82105N+ <:3,XO<J/2;43 /E+.C+.82XI8+,<+M3 8):;*+.8ERN< V + V D+.8@& Y3-08& 2+,)./Z= RYq/2;*+.8+RN<E38+,<+M3 8):;*+.8
]}WO/2;*+.8+PRN<3Z-08& 2+,)./J& Y{L;0RN):g
; ]RN<3 V + V D+.8 Afe
105N+,<>3 8+1*<+,HS/&F+.'082RN):;tY$3()./<MERY/2;*+\;QX-G&(/2;*+,<2RN<>& Y3s82105N+\)M3 'jDG+Z-08& 2+,)./+,HRU'Q/&3^0?ZW
/2;*+.'F/2;*+E82105N+ERN<3 -0-05URN)M3 D05N+@/&Z/2;0RN<^0?W*3 '*HTRU/<),&('*).5U1*<2RN&(')M3 'FD+E3(H0H0+,HI/&Z/2;*+>^0?3(),),&(8HORU'06
/&/2;*+-08& 2+,)./2RN&('7
e &(/2RN),+s/2;43 /Z+M3();-08& m+,)./2RN&('& Y3<:3 V +i82105N+i/&S3S^0? H0+f*'*+,<\3HOR+.8+.'/
L3MX& Y3 -0-05UXRU'06k/2;0RN<i82105N+S3 '*HRN<i5URUK+.5UX/&k3(H0H'*+.LRU'OYn&(8 V 3 /2RN&('/&j/2;*+S^0?Ze%&('*<2RNH0+.8Yn&(8
RU'*<2/:3 '*),+@/2;*+l^0#
? & Y!x{RU6*eqOW0L;0RN);yH0+,<).82RUDG+,<<2-43 /2Ro3 57RU'OYn&(8 V 3 /2RN&('3 DG&(10/& ),+,<MW43 '*HF82105N+,<J&
x{RU69108+lvOe RN<E3 -0-05URN)M3 D05N+F=[<2RU'*),+sQm.9pB
\G(9AfW3 '*Hy<&\RN< e|7+./E1*<u),&('*<2RNH0+.8 eg_;*+.8+
3 8+s/L&SL3,XO<& YE3 -0-05UXRU'06S/2;0RN<Z82105N+9WqH0+.-+.'*HORU'06t&('kL;*+./2;*+.8RU/<l;X-&(/2;*+,<2RN<ZRN< V 3 -0-G+,H&('/&
/2;*+P-43 /2V
;
I6
!"
TT# $
> %"$a
&# s&(8&('Q/&/2;*+
-43 /2; &# $
!$
TT $
> %$a
T( ' 4e '/2;*+*8<2/
)M3(<+yYn&(8FRU'*<2/:3 '*),+9W@3k8+.5o3 /2RN&(''*&H0
+ tLRU/2;-08+,H0+,),+,<<&(V
8
I63 '*H<1*),),+,<<&(8
&# kRN<s3(H0H0+,H/&t/2;*+S^0?Za
e u&(/2RN),+I/2;43 /iRU'/2;0RN<s+f03 V -05N+9WJ3 -0-05UXRU'063 5U582105N+,<iRU'3 5U5
-&9<<2RUD05N+@L_3,XO<u3(<5N&('06s3(<J/2;*+.X3(H0H'*+.LRU'OYn&(8 V 3 /2RN&('RN<u3Z*'0RU/+P-08&),+,<<l=5N+M3(HORU'06i/&/2;*+>698:3 -0;
& Yhx{RU69108+>AD010/RU/RN<'*&(//2821*+@RU'I6+.'*+.8:3 5$e
r;*+.'\/2;*+J@RN<q),& V -&9<+,H\& Y73@<+./& YaY$3()./<]3 '*H3@<:+./q& Y82105N+,<&W/2;*+ -08&(DO~
5N+ V 3(<2KO<L;*+./2;*+.8@/2;*+.8+lRN<@3<:+,wQ1*+.'*),+Z& Y]82105N+3 -0-05URN)M3 /2RN&('*<P+.'082RN):;0RU'06F/2;*+>Y$3()./<E<21*);S/2;43 /u/2;*+
6&3 5h)M3 'D+@8+M3():;*+,HW4R$e+9e5N+M3(HORU'06s/&3698:3 -0;RU'Q/&\L;0RN):;/2;*+Z^0?)M3 'D+@-08& 2+,)./+,He e 6*e
),&('*<2RNH0+.8/2;*+_Yn3().
/ & Y7x{RU69108+OW3 '*H5N+./D+_/2;*+u^0%
? &#( ' $
) $a
TTs= RN<
('u'*+M3 8h3 'l& ),+ AfeqH0&+,<'*&(/{-08& m+,)./{RU'Q/&@W D010/h3 -0-05UXRU'06u/2;*+]82105N+,<MW9&('*+_3(H0H0<}/2;*+]RU'OYn&(8 V 3~
/2RN&(
' T( '
) T$
TT F=n3 5N<& TT( '
T"
TT# AfW
/2;Q1*<u3 '*<2L+.82RU'06Fe



*



4

4

5

&

4

&

4

4

4

4










v { $

4 AG

q





&



(0.[m9p4>)M3 'jD+-G&9<RU/2RUC+&(8>'*+.6Q3 /2RUC+9W!+fO-08+,<<2RU'06K'*&L5N+,HO6+T& YYn&(8 V RY
M}h A{% }A
ff ;*&(5NH0<MW<& V 1*<2/* :W&(8 RY+ff;*&(5NH0<MW, V 1*<2/s'*&(/ :e /iRN<s3 5N<:&3D0RN),&(5N&(8+,H<2R V -05N+698:3 -0;7

/2;*+l*8<2/@),&(5N&(8PH0+f*'*+,<@/2;*+),&('*HORU/2RN&('t-43 82/=[&(8\[fUMQfAfW3 '*HS/2;*+<+,),&('*H),&(5N&(8@/2;*+ V 3 '*H*3 /&(82X
=[&(8PYn&(82D0RNH0H0+.'aAE-43 82/MeF ^0? <:3 /2RN<24+,<3F-&9<2RU/2RUC+i),&('*<2/28:3 RU'Q/ RY2.j-08& m+,)./2RN&('Y[8& V /2;*+
),&('*HORU/2RN&('F-43 82/J& RU'/o
& )M3 'TDG+E+fO/+.'*H0+,Hi/&3-08& 2+,)./2RN&('I& Y}/2;*+uL;*&(5N+ eh'*7
H <B3 /2RN<m4+,<
3j'*+.6Q3 /2RUC+y),&('*</28:3 RU'Q/TRYa-08& 2+,)./2RN&('& YP/2;*+y),&('*HORU/2RN&('&
RU'Q/
& )M3 'D+y+f/+.'*H0+,H/&
3-08& m+,)./2RN&('& Yu/2;*+TL;*&(5N+ etx{RU6*e<;*&MLJ<Z/L&t),&('*<2/28:3 RU'/<Meg_;*+T'*+.6Q3 /2RUC+F),&('*</28:3 RU'Q/

fi



0

5

;

!-








fi

3MfiuM%

workswith

Person

HeadOfGroup

Secretary





Person




Office






Office

near

Office



x{RU69108+%&('*<2/28:3 RU'/<

=

4 R
4



+fO-08+,<<+,</2;43 / /L&i-+.8<&('*<L&(82KRU'06s/&(6+./2;*+.8<2;*&(105NHF'*&(/u<2;43 8+>3 'y& ),+ :eg_;*+Z^0? & Yqx{RU6
iH0&+,<u'*&(/@<:3 /2RN<mY[XF/2;0RN<E),&('*<2/28:3 RU'Q/@D+,)M3 1*<+ /2;*+.8+lRN<E38+,<+M3 8):;*+.8@L;*&iL&(82K<uLRU/2;S8+,<+M3 8):;*+.8
s=-08& 2+,)./2RN&('& Y]/2;*+),&('*HORU/2RN&('-43 82/@& oIA 3 '*Hy/2;*+.X<2;43 8+Z& ),+/.s`M T=[+fO/+.'*<2RN&('& Yq/2;*+
-08& 2+,)./2RN&('/&T3-08& 2+,)./2RN&('y& Yq/2;*+L;*&(5N+ IAfeJg_;*+-G&9<RU/2RUC+),&('*<2/28:3 RU'/ +f-08+,<:<+,</2;43 //2;*+
& ),+P& Yq3l;*+M3(HI& Yh698&(10- V 1*<2/DG+E'*+M3 8J/2;*+>& ),+,<& Yq3 5U57<+,).8+./:3 82RN+,<Me
r;*+.'/2;*+F@RN<),& V -&9<+,H& Y@3<:+./\& YY$3()./<\3 '*H3S<+./& YE),&('*<2/28:3 RU'Q/<\hW]/2;*+i8&(5N+F&
),&('*<2/28:3 RU'/<IRN<F/&H0+f*'*+S/2;*+t),&('*<RN<2/+.'*).X& Yl/2;*+SD43(<+9WuR$e+9e& YZeg_;*+D43(<+SRN<F<:3 RNH/&D+
),&('*<2RN<2/+.'/RY3 5U5{),&('*<2/28:3 RU'Q/<P3 8+<:3 /2RN<m4+,He 8&MCRNH0+,H/2;43 //2;*+>D43(<:+PRN<u),&('*<2RN<2/+.'Q/MWH0+,HO1*)./2RN&('yRN<
H0&('*+3(<uRU'__e C+.'yRY/2;*+.Xy3 8+DG&(/2;SD0RN),&(5N&(8+,Hy698:3 -0;*<MW),&('*<2/28:3 RU'/<>3 8+>'*&(/E/&iDG+Z),&('OY[1*<+,H
LRU/2;I82105N+,<Me]%&('*<RNH0+.8JYn&(8RU'*<2/:3 '*),+@/2;*+ED0RN),&(5N&(8+,HI698:3 -0; & Y!x{RU69108+PvO]3(<3l82105N+9W0RU/J<B3,XO</2;43 /
+.C+.82Xt8+,<+M3 8):;*+.8ZRN<l3 V + V D+.8l& YE3F-08& 2+,)./Meg{3 K+/2;*+Yn3()./1
0 0T T2 ac3 '*H/2;*+
w1*+.82Xi+
0 0 2 a$
43 !35 "$a
76 8 F= RN<e V + V D+.8& Y}3@-08& 2+,)./ Afe Y{RN<
3(<2K+,Hi&('
"ff$n+
*) "m n4fW/2;*+3 '*<2L+.8]RN< X+,< :
e u&MLPW<+,+ 3(<3E-&9<2RU/2RUC+),&('*<2/28:3 RU'/ \e /
<:3MX<]/2;43 /]+.C+.82XZ8+,<+M3 8);*+.8 V 1*<2/DG+J3 V + V D+.8& Y3E-08& 2+,)./Me!
"ff$n+
*m
"m kn4!RN<!RU'*),&('*<2RN<2/+.'/MW
/2;Q1*<'*&(/2;0RU'06)M3 'ID+@H0+,HO1*),+,HFY[8& V RU/MW0RU'*).5U1*HORU'06s\eqg_;*+@@;43(</&ZD+@8+.-43 RU8+,HT*8<2/Me


4





4



]

; &

G

4



q

|7+./>1*<@),& V D0RU'*+\82105N+,<3 '*H),&('*<2/28:3 RU'Q/<>RU'8+M3(<&('0RU'06*e
^ }h }h y| {%}}$AA{% }A
rt+PHORN<2/2RU'06910RN<;I'*&MLD+./L+,+.'/L&\KRU'*H0<J& Yh82105N+,<M_M,.2fG:BNBl3 '*HS? ( O[n(SBUffe
'OY[+.8+.'*),+Z82105N+,<P8+.-08+,<+.'Q/PR V -05URN).RU/>K'*&L5N+,HO6+/2;43 /@RN< V 3(H0++f-05URN).RU/@DX82105N+\3 -0-05URN)M3 /2RN&('*<e
g_;0RN<PRN<>/2;*+\)M3(<+ZYn&(8P82105N+,<<+,+.'3 D&MC+=[x{RU69108+svAfex43()./<Z3 '*HRU'OYn+.8+.'*),+\82105N+,<>)M3 'jDG+\<:+,+.'3(<
H0+,<).82RUD0RU'063sL&(825NHW}3 '*Ht3 -0-05UXRU'063s82105N+ V &HOR4+,<P/2;*+\+f-05URN).RU/PH0+,<:).82RU-0/2RN&('j& Y/2;*+ZL&(825NH=/2;*+
Y$3()./<BAfC
e u&MLPWRYL+),&('*<2RNH0+.83T@),& V -G&9<+,Hj& Y_3T<+./@& Y]Y$3()./<>W3T<+./@& YRU'OY[+.8+.'*),+82105N+,<9W
3 '*H3y<+./l& YE),&('*<2/28:3 RU'Q/<l!W!/2;*+s'*&(/2RN&('& Y),&('*<2RN</+.'*).Xj;43(<Z/&/:3 K+T82105N+,<RU'/&t3(),),&(10'Q/Mex*&(8
RU'*<2/:3 '*),+9W03(H0Hi/&>/2;*+@^0?Ed
< 3 '*H
& Yx{RU69108+u>/2;*+Y[&(5U5N&LRU'06lRU'OYn&(8 V 3 /2RN&('I3 DG&(10/_& ),+E3(<<2RU69'O~
;: T"< 8 "=24a
> $
L!"
TT#7
I64
W 4? @7+
:
L!$
TT
V +.'Q/<o
3 '*Q
H 4? @Aa
6
L!$
TT 4eh|7+.
/ 3 '*H D+/2;*+='*&(8 V 3 5pA_^0?E<&(D0/:3 RU'*+,He
%&('*<2RNH0+.8P/2;*+\L&(825NHj),& V -&9<+,Ht& Y_/2;*+i^0? b[WRU'OY[+.8+.'*),+\82105N+,
< * n& Y_x{RU69108+ivOW{3 '*HS/2;*+
-&9<2RU/2RUC+F),&('*<2/28:3 RU'Q/ & YEx{RU69108+eg_;*+^0
? 3 5N&('*+IH0&+,<'*&(/\<:3 /2RN<2YX/2;*+F),&('*<2/28:3 RU'Q/
=D+,)M3 1*<+ /2;*+T;*+M3(Hk& Yu698&(10-|qehRN<lRU'& ),+B7
I9W]3 '*Hk/2;*+F<+,).8+./:3 82X ehRN<lRU'& ),+B :W!D010/
RU/ZH0&+,<'*&(/Z;*&(5NHj/2;43 / %7
IRN<'*+M3 8C Afe10/3Y/+.83),+.82/:3 RU'k'1 V D+.8& Y82105N+s3 -0-05URN)M3 /2RN&('*<W
RU/>H0&+,<Mesg_;Q1*<P/2;*+i@RN<><:3 RNHt/&FDG+),&('*<2RN<2/+.'Q/Me 't/2;0RN<>)M3(<:+RU/PRN<>+M3(<Xy/&H0+f*'*+i3 '*Hj):;*+,)K
),&('*<2RN<2/+.'*).XDG+,)M3 1*<+>/2;*+>L&(825NHSH0+,<).82RU-0/2RN&('y)M3 'yD+>),& V -05N+./+.5UX+fO-05URN).RU/+,HDQX3*'0RU/+Z^0? =/2;*+
W7<:3 RNH/&FD+EfaLPe 8Me /MeCTAfW/2;Q1*<PRU/P<1 ),+,<P/&I):;*+,)K/2;43 /P/2;0RN<@698:3 -0;RN<P),&('*<2RN<2/+.'/Me
698:3 -0;
'6+.'*+.8:3 5J)M3(<+9W!),&('*<RN<2/+.'*).Xj8+.5URN+,<&('kL;*+./2;*+.8\+M3(); ),&('*<2/28:3 RU'/\CRN&(5o3 /2RN&(' T)M3 'D+s8+.-43 RU8+,H

9



7



9





4

4
d;

4 ;





;









.

fi M]|9M.tM=uwuMM3
F

DQXT82105N+l3 -0-05URN)M3 /2RN&('*<MWa3(<LRU5U5D+uY[&(8 V 3 5U5UXIH0+f*'*+,H5o3 /+.8Meu<LRU/2;<R V -05N+.8JL&(825NH0<uH0+,<).82RUD+,HIDQX
Y$3()./<J&('05UXW*H0+,HO1*)./2RN&('RN<J'*&(/-G&9<<RUD05N+@&('IRU'*),&('*<2RN<2/+.'/K'*&ML5N+,HO6+PD43(<:+,<Me
Person



Office

x}RU69108+cO!82105N+
C&(5U10/2RN&('j82105N+,<E8+.-08+,<+.'Q/P-&9<<2RUD05N+3()./2RN&('*<@5N+M3(HORU'06TY[8& V &('*+ZL&(825NHS/&F3 '*&(/2;*+.8>&('*+9e e 6*e
),&('*<2RNH0+.8/2;*+>),&(5N&(8+,HI698:3 -0;y& Yqx}RU69108+lcOeu<3 'RU'OY[+.8+.'*),+>82105N+9W*RU/L&(105NH3 5U5N&L/&\H0+,HO1*),+>/2;43 /
3 5U5-G+.8<:&('*<_3 8+uRU'I3 5U5& ),+,<Mequ<3 'T+.C&(5U10/2RN&('F82105N+9WRU/<:3MX<_/2;43 / L;*+.'T/2;*+.8+P3 8+@3>-+.8<&('F3 '*H
3 'I& ),+9W43l-&9<<2RUD05N+@3()./2RN&('IRN</&\3(<:<2RU69'T/2;0RN<J& ),+@/&Z/2;43 /-+.8<&(' :e%&('*<2RNH0+.83Z@),& V -&9<+,H
& Y3i<+./& YqY$3()./<EW3<+./u& Y]+.C&(5U10/2RN&('S82105N+,<D]W3 '*HS3\<+./E& Y]),&('*<2/28:3 RU'Q/<u!eux43()./<EH0+,<).82RUDG+Z3 '
RU'0RU/2Ro3 5{L&(825NH+.C&(5U10/2RN&('S82105N+,<8+.-08+,<:+.'Q/E-G&9<:<2RUD05N+P/28:3 '*<2RU/2RN&('*<Y[8& V &('*+>L&(825NHy/&s&(/2;*+.8L&(825NH0<M
),&('*<2/28:3 RU'/<H0+f*'*+u),&('*<2RN<2/+.'*).X\& Y+M3():;iL&(825NH03P<21*),),+,<<:&(8& Y}3P),&('*<RN<2/+.'Q/L&(825NHiRN<]&(D0/:3 RU'*+,HsDQX
3 'I+.C&(5U10/2RN&('I82105N+P3 -0-05URN)M3 /2RN&('7069RUC+.'3\^0?\WO/2;*+@H0+,HO1*)./2RN&('I-08&(D05N+ V 3(<2KO<_L;*+./2;*+.8J/2;*+.8+ERN<J3
-43 /2;& Y!),&('*<2RN<2/+.'/JL&(825NH0<u+.C&(5UCRU'06\Y8& V /2;*+@RU'0RU/2Ro3 5}&('*+@/&\3ZL&(825NH<:3 /2RN<mY[XRU'06i\e
g_;*+ V &9<2/l6+.'*+.8:3 5 V &H0+.5& YJ/2;*+sYn3 V RU5UXj),&('*<RNH0+.8<ZDG&(/2;kKRU'*H0<Z& Y82105N+,<W_0 5W 53<+./(
& Y]RU'OYn+.8+.'*),+Z82105N+,<MW3 '*H3i<:+./ED& Y+.C&(5U10/2RN&('82105N+,<e 'y/2;*+l-43 82/2RN).105o3 8P)M3(<+Z& Y]/2;*+i*{*77,:
V &H0+.5UR 3 /2RN&('7WI3 '*H9H0+,<).82RUD+!/2;*+qRU'0RU/2Ro3 5RU'OYn&(8 V 3 /2RN&('Z3 D&(10/}& ),+]5N&)M3 /2RN&('*<MW -+.8<&('*<}3 '*H>/2;*+
698&(10-&(826Q3 '0R 3 /2RN&('7eF3 5N<&@+.'*),&H0+,<q6+.'*+.8:3 54K'*&ML5N+,HO6+>=[<21*):;i3(<!-08&(-+.82/2RN+,<q& Y/2;*+P( i8+.5o3 /2RN&('
-010/D+./L+,+.'/L&\),&('*),+.-0/J'*&H0+,<8+.-08+,<+.'Q/2RU'06iHORN<2/2RU'*)./+.'Q/2RU/2RN+,<fAfe!8+.-08+,<:+.'Q/<&(D05URU6Q3 /2RN&('*<u3 '*H
RU'Q/+.8HORN)./2RN&('*<H0+f*'0RU'06L;43 /3(),),+.-0/:3 D05N+t3(<<2RU69' V +.'/<3 8+=RU'*).5U1*HORU'06)M3 8HORU'43 5URU/X),&('*<2/28:3 RU'/<
<21*):;j3(< 3-+.8<&('t)M3 '0'*&(/@D+spk<+.C+.8:3 5!& ),+,< Z&(8 3s5o3 826+& ),+)M3 '0'*&(/P),&('/:3 RU' V &(8+Z/2;43 '
/L&-+.8<&('*< :W!1*<2RU'06/2;*+( 8+.5o3 /2RN&('aAfeGD),&('*<2RN<2/<& YE&('*+T+.C&(5U10/2RN&('82105N+TL;*&9<+s8+,<2105U/RN<Z/&
-05o3(),+>3Z-+.8<&('IRU'/&3 '& ),+s=RU/),&(105NH3 5N<&D+@),& V -&9<+,HI& Yq<+.C+.8:3 5782105N+,<),&('*<2RNH0+.82RU'06i<2-+,).R4)
-08+,),&('*HORU/2RN&('*<TD+fY[&(8+/282XRU'063 '3(<<2RU69' V +.'/BAfeg_;*+6&3 58+.-08+,<+.'/<F3t<2RU/2143 /2RN&('L;*+.8++M3():;
-+.8<&('& YP/2;*+698&(10-;43(<T3 '& ),+9e <&(5U10/2RN&('/&t/2;*+-08&(D05N+ V RN<T3jL&(825NH&(D0/:3 RU'*+,HY[8& V
/2;*+RU'0RU/2Ro3 5]&('*+\DQXt3T<+,w1*+.'*),+\& Y_& ),+i3(<<2RU69' V +.'Q/<WL;*+.8+\+M3():;t-+.8<&('t;43(<>3 'j& ),+9WL;0RU5N+
<:3 /2RN<mY[XRU'06/2;*+>3 5U5N&)M3 /2RN&('),&('*<2/28:3 RU'Q/<Me

4

4

6



p

4

5

4 5

4

hy _



4

6





z{

4

4

4

4





|7+./J1*<J'*&ML<2-+,).RY[XsH0+f*'0RU/2RN&('*<u3 '*HF'*&(/:3 /2RN&('*<),&('*),+.82'0RU'06i/2;*+@jY$3 V RU5UXe

}



9 0f2 p

fi;}]} uy ),&(5N&(8+,Hj<2R V -05N+698:3 -0;o\O( " $k)* H4oh
9a/Hko\F\BQap@fm( $[4\pa$Zm%I *WIn5UOZ*f B.M,[9$IGMQ\o(U2
}
Ol),&(5N&(8ZmPOPaMQW5
e >fa9ZS: KJ ML OEf0$(mf4mH pa9O:TL:[3!:(922sGMQBW5UO
B*n(2B4 JON L Zf.9BE}
$N p5 J
5 OG. 02(Bum2.9[n(TaMEm
JPN L ZQ.}( .fU(
$ JON L RT5

fi









g ;*+E5o3 /2/+.8),&('*HORU/2RN&('k= JON L V 1*<2/_Yn&(8 V 3\^0?PARN<_'*+,),+,<<B3 82XT3(<<&&('I3(<_L+P),&('*<RNH0+.882105N+,<3(<
_
),&(5N&(8+,HI^0?E<Mh<2;*&(105NHT3P82105N+'*&(/_<:3 /2RN<mY[X/2;0RN<),&('*HORU/2RN&('7WORU/<3 -0-05URN)M3 /2RN&('F&('F3l^0?),&(105NHs6+.'*+.8:3 /+
3l698:3 -0;I/2;43 /JRN<J'*&(/3\^0?Ze
@RN<ZH0+.'*&(/+,HDX
" $n+
*)g
*)
*m
4fW]L;*+.8+FRN<l3y<+./Z& Y<2R V -05N+i698:3 -0;*<Z8+.-08+,<+.'/2RU'06
Y$3()./<MWWDj3 '*HsS3 8+u/2;08+,+@<+./<_& Y{),&(5N&(8+,HF<R V -05N+E698:3 -0;*<_8+,<2-G+,)./2RUC+.5UXi8+.-08+,<+.'Q/2RU'06pMMffa:
fNBfWS ?( [n(BUffW3 '*H(*f[2(paI=-&9<2RU/2RUC+Z&('*+,<ERU'yRQ_WG'*+.6Q3 /2RUC+\&('*+,<ERU'yTS}Afe?uRUC+.'j3

;



fi

3MfiuM%

@3 '*Hi3E6&3 5\W/2;*+>Q(OM[[(amfU.3(<2KO<qL;*+./2;*+.8)M3 'DG+JH0+,HO1*),+,H\Y8& V =L+'*&(/+
P >Afe YL+R V -G&9<:+<& V +J& Y7/2;*+<+./<&yWDS&(8!I/&PD+J+ V -0/XW&('*+&(D0/:3 RU'*<<-G+,).R4)8+M3(<&('0RU'06
e u&(/+E/2;43 /RU'F/2;*+>3 D*<+.'*),+>& Yh),&('*<2/28:3 RU'/<l=p
" AfW*RU'OY[+.8+.'*),+>3 '*H+.C&(5U10/2RN&('82105N+,<J;43MC+
V &H0+.5N<M
/2;*+P<:3 V +PDG+.;43MCRN&(8MW4/2;Q1*<fi 3 '*HUDk)M3 'IDG+@),&('OY[1*<+,Heg_;*+PjY$3 V RU5UXTRN</2;*+.'),& V -&9<+,HI& Yh/2;*+
<2RsY[&(5U5N&LRU'06 V &H0+.5N<Me



F
/2;*+P V &H0+.5Yn&(8JW0 $n+*fig*XD* 4
/2;*+P V &H0+.5Yn&(8JW0$n+* * *J.4
/2;*+P\ V &H0+.5Y[&(8JY0 $na*Eg*@ *4
/2;*+PD V &H0+.57Y[&(8JW0 $n+*F *fiD*_4

V /2;*+P V &H0+.57Y[&(8JW0 $n+* * * 4
V
V
V
V

V /2;*+PCD7 V &H0+.5Yn&(8JW0 $n+*Xg*,Da*.4

^RU'*),+3EYn3()./q;43(<q/2;*+J<:3 V +J<+ V 3 '/2RN),<3(<3E82105N+LRU/2;i3 '+ V -0/XZ;QX-G&(/2;*+,<2RN<W/2;*+J<+./RN<]1*<+,H
RU' V &H0+.5N<h'43 V +,<!&('05UXL;*+.'DG&(/2;Z82105N+_<+./<F3 '*H(D3 8+_+ V -0/Xe!g_;*+;0RN+.8:3 8):;QX& Ya/2;*+,<+ V &H0+.5N<
NR <8+.-08+,<+.'Q/+,HyRU'x{RU6*eGbOe /;0RU69;05URU69;Q/<u/2;*+lH0+,).RNH*3 D0RU5URU/X-08&(-+.82/2RN+,<E3 '*H/2;*+),& V -05N+fRU/X& Y!/2;*+
3(<<&).Ro3 /+,HsH0+,HO1*)./2RN&('s-08&(D05N+ V ea&(/2RN),+L+uHORUCRNH0+'*&('TH0+,).RNH*3 D05N+-08&(D05N+ V <_RU'/&i,fZM!Q2,[fU
3 '*H[f; :GQ2.nQfN-08&(D05N+ V <Me 'T/2;*+*8<2/J)M3(<+9W03 '3 '*<2L+.8J)M3 'FD+E),& V -010/+,HFRU's*'0RU/+E/2R V +
Yn&(83 5U57-G&9<RU/2RUC+@RU'*<2/:3 '*),+,<JD010/'*&(/Y[&(83 5U5'*+.6Q3 /2RUC+>&('*+,<Me 'F/2;*+P<:+,),&('*HI)M3(<+9W0/2;*+.8+@RN<J'*&Z*'0RU/+
-08&),+,HO108+9W0'*+.RU/2;*+.8JYn&(83 5U57-&9<2RU/2RUC+ERU'*<2/:3 '*),+,<MW0'*&(8JY[&(83 5U5'*+.6Q3 /2RUC+&('*+,<Me
Z[\%]

g782105UXT10'*H0+,).RNH*3 D05N+

Z[F]
Z\!]

^+ V R~H0+,).RNH*3 D05N+

Z[

_` ~),& V -05N+./+


Z8^]
Z8^

~),& V -05N+./+

x{RU69108+>bOqg_;*+@_tY$3 V RU5UX V &H0+.5N<3 '*HI),& V -05N+fRU/XF& Y{/2;*+>3(<:<&).Ro3 /+,HFH0+,HO1*)./2RN&('-08&(D05N+ V
aq



E

cb"]$Mk]jEd

!]4





fi

B aUi(2B4fN=$^0?82105N+A+ V D+,H0<\K'*&ML5N+,HO6+y& YEY[&(8 V RY9ff /2;*+.'e :eg_;*+FY[&(5U5N&LRU'06
H0+f*'0RU/2RN&('3(<3),&(5N&(8+,H^0?RN<Z+,wQ10RUC93 5N+.'Q/Z/&/2;*+ V &(8+s/28:3(HORU/2RN&('43 5H0+f*'0RU/2RN&('& Y3I82105N+F3(<3 '
&(DO2+,)./Z),& V -&9<+,H& Yu/L&j^0?@<Z8+.5o3 /+,HLRU/2;),&(8+fYn+.8+.'*),+F5URU'0KO<l1*<+,HkDQX?@&9<2;3 '*Hr1QL&('06<:+
=m`Mb9b9vAfW*&(8u^O3 5UC93 /3 '*HF1069'0RN+.8=m`Mb9b9Afe

;



fi M]|9M.tM=uwuMM3
F

ff


fi;}]}$k{%}

uA{M}h

oTt(U(2J
{1
5 JON L pT:(pN2tT;QX~
yn <2R V -05N+i698:3 -0;82105N+

fi;}]}gf
-&(/2;*+,<2RN<L=(a J L [),&('*).5U1*<2RN&('5
E+,HO1*)./2RN&('H0+.-G+.'*H0<i&('/2;*+F'*&(/2RN&('& Y>3BNfa [[[(hRU/\RN<i3S698:3 -0;/28:3 '*<mYn&(8 V 3 /2RN&('
D43(<+,HF10-&('F-08& 2+,)./2RN&('7e



Z8}

= (a ZifNW5 o3 -0-05UR~
.

fi;}]}ih ` M{M} j{
)M3 D05N+S$7 \OfI)j9o.iGmmM2,[n(=u.T: =}f2( JON L NO\:BO9OfBpsm RI4$Z85gB
9 ,S
= OZBBmZOs3 -0-05URN)M3 /2RN&('j& &('3(),),&(8HORU'06I/&plOr} Q.$(G2SS:
\ sO(o.9p4]4[(mH (aTm>sBF
: J L N Ol(a. Bn(tm Rq
= O.=4,(S? fS:

2M7$MA*y>*364>=jh
9 O.2k JPN L (a J L @
= (S(2MQ!
9 O.9\*fF.
9 f

$ W>
4 9asO>:BF
: md5+ oE.9[i$Fl(R VV +,HORo3 /+ ~H0+.82RUC93 /2RN&('TB2(qo5

H0+.82RUC(3 /2RN&('RN<3I=-&9<<2RUD05UXi+ V -0/X0AJ<+,w1*+.'*),+@& Y{82105N+>3 -0-05URN)M3 /2RN&('*<




fi;}]}gk ,
yn 3{M} .l lM.2@BUf>=(arl{151el(\~H0+.82RUC93 /2RN&('
fm( $ TMy.*fa:F2C{[ q
" N * F6F6F *
" fO.O9M=7.9rIZ\Vj\ K = p\(
G
l29[9 !mQff'?[[(lfm( =h
9 pPfN>pm5





g}&lH0+,HO1*),+E3Z^0?\WQL+ V 1*<2/_DG+u3 D05N+/&lH0+.82RUC+@3l^0?RU'Q/&lL;0RN):;I)M3 'sD+J-08& 2+,)./+,Heqg_;0RN<
'*&(/2RN&('IRN<J)M3 -0/2108+,HDQXs/2;*+EYn&(5U5N&MLRU'06iH0+f*'0RU/2RN&('7

Kv { $

u


. " $n+*)4yonbX(atU.E :J}15 :(:

fi;}]} 2i
Q29O:fm( N a9$9[n(t P n$ +*)4cR \OfF)j(pfi(pg!mQff'? 9[[9sf2( $S}
B0fI9! P 5



MMM{






{%}M

g_;*+<+ V 3 '/2RN),<RN<@+f/+.'*H0+,HS/&s/28:3 '*<25o3 /+Z82105N+,<Mu69RUC+.'j3i82105N+ W5N+./ N 3 '*H DG+l/2;*+l/L&
^0?E<8+,<-G+,)./2RUC+.5UX),&(828+,<2-&('*HORU'06j/&SRU/<;QX-G&(/2;*+,<2RN<i3 '*HRU/<\),&('*).5U1*<2RN&('7W_R$e+9e N " JON L 3 '*H
RN<i/2;*+S^0? &(D0/:3 RU'*+,HY[8& V J L DQX3(H0HORU'06k/2;*+'*+.RU69;DG&(8<T& YP/2;*+8+.5o3 /2RN&(''*&H0+,<i& J L
L;0RN);3 8+I),&('*),+.-0/s'*&H0+,<i& JPN L eg_;*+.'[
$ 4Z" ] FtFtF ] $ +$ N 4
_ FtFtF _ $ 42A


N
N
L;*+.8+ $ 4_3 '*H +$ 4_3 8+E/2;*+@),&('(m10'*)./2RN&('*<& Y!3 /& V <3(<<&).Ro3 /+,HTLRU/2; 3 '*H W ] FtFtF ]
3 8+i/2;*+C93 82Ro3 D05N+,<Z& +$ N 4>3 '*
H _ FtFtF _ \3 8+i/2;*+C93 82Ro3 D05N+,<Z& +$ 4@/2;43 /H0&'*&(/3 -0-G+M3 8lRU'
$ N 4feFx4&(8>RU'*<2/:3 '*),+9Wh),&('*<2RNH0+.8l/2;*+\82105N+ RU'x{RU6*e}vOeTg_;*+.'[
$ 4E"]`$JB,(m.OfA $M])4
_)$2m,,c $M_4 ff $M]*y_4y4y4fe ^;*&(105NHL+SRU'/+.82-08+.//2;*+),&(5N&(8+,H698:3 -0;
RU'x{RU6*eJ3(<

3s82105N+9W7RU/<@Y[&(8 V 105o3FL&(105NHtDG+j
$ 4b"ff]_$y$J.:,( $M]4 JJ.:,( $M_4 9]( MS !c9![ $M]*y_4y4
$ rq: $ 4 p
$M]* 4 $M_* 4y4y4f
e u&(/2RN),+9W 10'05URUK+]RU'Z).5o3 1*<+,<MW C93 82Ro3 D05N+,<-08&(-+.8{/&J/2;*+),&('*).5U1*<2RN&('
3 8+P+fORN<2/+.'Q/2Ro3 5U5UXFw143 'Q/2R4+,He
g_;*+EYn&(5U5N&MLRU'06s<&(10'*HO'*+,<<3 '*H),& V -05N+./+.'*+,<<J8+,<2105U/RN<J&(D0/:3 RU'*+,H







R

p

+

G


=



G

!r

C ` ff



$

<



U


Cuvu w 7t Cu|u


hy|My
qhM}]y|
f + { 3{ kJ .!
{ 3{

Zo}
15UOf P $na*)74> j$4*Bj$n4*Bj$t74 I[$$E465



<

.ff
ff

"$n+*)74\\snYX(ai

u&(/2RN),+J/2;0RN<8+,<2105U/3(<<1 V +,<]/2;43 /698:3 -0;*<3 8+69RUC+.'RU'\'*&(8 V 3 5aY[&(8 V WO3 '*HWRY'*+,+,H0+,HW-010/]RU'/&
/2;*+.RU8J'*&(8 V 3 5Yn&(8 V 3Y[/+.8J+M3();82105N+P3 -0-05URN)M3 /2RN&('7e

;A




fi3MfiuM%





` v`
%&(105N&('*HO8+J3 '*H^O3 5UC(3 /=m`Mb9b9cA-08&MC+,HZ/2;43 /{K*{sRN<h<+ V R~H0+,).RNH*3 D05N+_LRU/2;38+,HO1*)./2RN&('
Y[8& V /2;*+;*Q|C*v${9 <$vu(w/leig_;*+\8+,HO1*)./2RN&('69RUC+.'tDX_3 6+./T=n99O`A
=pY[8& V /2;*+(xRz
y@v${9 $g u> { yi| 7* AJ-&(RU'Q/<u&(10//2;43 /2 K* {
RN<P3T),& V -010/:3 /2RN&(' V &H0+.5$e>rt+Z69RUC+Z;*+.8+\3 '*&(/2;*+.8@8+,HO1*)./2RN&('7W7Y8& V /2;*+~}$D
*|{9
AN7
.h2D PW7/2;43 /@L+ZLRU5U5!1*<+\3(<E/2;*+<2/:3 82/2RU'06I-&(RU'Q/@RU'/2;*+Z-08&& Y& 8&(-7eq`MOeZg_;0RN<
8+,HO1*)./2RN&('IRN<3 5N<&lRU'Q/+.8+,</2RU'06\RU'FRU/<+.5Y}<2RU'*),+ERU/_-08&C+,</2;43 /MW0+.C+.'FL;*+.'F82105N+,<J3 8+@& Y/2;*+uYn&(8 V RY
O9Kk] F6F6F ] G >
/2;*+.'TOUk_ F6F6F _ G: W02Kv { $t 8+ V 3 RU'*<<+ V R~H0+,).RNH*3 D05N+9e



~>hy|Mh{







7t Cu|u k 2Kv { $ pE,fZM!Q2,[fU5
2MmWFx{RU8<2/>):;*+,)Kt/2;43 /i*|RN<'*&(/>/282105UX10'*H0+,).RNH*3 D05N+S=5pW5F/2;*+.8+i+fORN<2/<l3 'k3 5~
6&(82RU/2; V /2;43 /P)M3 'tH0+,).RNH0+ZRU'S*'0RU/+Z/2R V +ZRY_/2;*+\3 '*<2L+.8@/&T/2;*+Z-08&(D05N+ V RN<F X+,<5 AfuL;*+.')M3 '
p

hy|My

}q

r

}]y

{ 3{


D+uH0+,HO1*),+,HFY8& V W*3lD08+M3(HO/2;O~n*8<2/<+M3 8);I& Yh/2;*+E/28+,+@& Yh3 5U57H0+.82RUC93 /2RN&('*<_Y[8& V -08&MCRNH0+,</2;*+
3 '*<2L+.8JRU'F*'0RU/+@/2R V +9e
rt+/2;*+.'-08&C+>/2;43 /u'*&T3 5U6&(82RU/2; V RN<u+.'*<2108+,H/&;43 5U/uL;*+.'/2;*+l3 '*<2L+.8E/&/2;*+>-08&(D05N+ V RN<
'*& :e|+./1*<'*&ML<2;*&ML/2;43 /2 kRN<'*&(/H0+,).RNH*3 D05N+lDQXFD010RU5NHORU'06I3\8+,HO1*)./2RN&('Y[8& V
/2;*+U}


N 7 .h =[g_;Q1*+9W@`MbO`.dAfeg_;0RN<-08&(D05N+ V L_3(<\-08&C+.'
<+ V R~H0+,).RNH*3 D05N+>DQX &9</=m`Mb dW08+,HO1*)./2RN&('/&Z;0RN<),&(828+,<2-&('*H0+.'*),+ 8&(D05N+ V Afe
g_;*+}

)M3 'DG+j+f-08+,<<:+,H3(<M5N+.g
/ 3 '*p
H 7ED+S/L&L&(8H0<W>3 '*H "
* F6F6F * nD+3@<+./]& Y82105N+,<MW+M3();i82105N+ D+.RU'06l3E-43 RU8]& YL&(8H0Y
< $ ,* c4fRN<]/2;*+.8+3@H0+.82RUC93 /2RN&('

G
Y[8& V /Z
& 7 g_;*+.8+ZRN<P3 'R VV +,HORo3 /+\H0+.82RUC93 /2RN&('Y[8& V /
& 7=L+Z'*&(/r
+ 7NARYmWGYn&(8
<& V ++ 9W %" 6 3 '*g
H "p 6 eJ QffM ?9[n(Y[8& V /& =L+l'*&(/C
+

RN<3Z<+,w1*+.'*),j
+ " N


"

e
F6F6F

4

B

Kv { $
$D *|{9 ] 2D
$D *v${9


&

{
C
ff ff ff
%









E

u

` ]*




$M4




ff









$)4

#




u



$Dvv$|
RU'Q/&i*|
g_;0RN<_-08&(D05N+ V )M3 'F+M3(<2RU5UXD+u+f-08+,<<:+,HiRU's/2;*+u V &H0+.5$ez'*+E),&('*),+.-0/_/X-G+9
RN<J3(<<2RU69'*+,H
/&+M3():;Z5N+./2/+.8] e}g_;*+.8+3 8+/2;08+,+&(/2;*+.8{),&('*),+.-0/h/X-G+,<M{=pYn&(8D D+.69RU' AfWZ=pY[&(8 +.'*HC A3 '*H
=pYn&(8
3 'QX/2;0RU'06MAfeRN<!/2;*+J698+M3 /+,<2/q),&('*),+.-0/]/X-+J3 '*H3 5U5*&(/2;*+.8q/X-+,<]3 8+_-43 RU82LRN<+'*&('O~),& V -43 8:3 D05N+9e
g_;*+.8+RN<P&('*+\8+.5o3 /2RN&('t/X-G+Z=pY[&(8;43(<<21*),),+,<<&(85 AfeL&(8HB " ] F6F6F ] G RN<>3(<<&).Ro3 /+,HS/2;*+
698:3 -0;t$M4fW73 '*H/&F3 'XI82105N+B"$M_ F6F6F _2 *F F6F6F AW 4RN<@3(<<:&).Ro3 /+,Hy/2;*+698:3 -0;y82105N+ $.4fW73(<
8+.-08+,<+.'/+,HRU'yx}RU6*e`MOeXI3<2/28:3 RU69;/mY[&(82L_3 8Hy-08&& Y=n3Z8+,).10828+.'*),+l&('/2;*+P< V 3 5U5N+,<2/H0+.82RUC93 /2RN&('
5N+.'069/2;aAfW9L+&(D0/:3 RU'l/2;43 /h/&E+.C+.82X>-43 /2;lY[8& /2;*+'*&H0+/X-G+,H~ /&/2;*+'*&H0+/X-+,H~=& D+.69RU'
/&
+.'*HC A_RU'y3l698:3 -0;B\~H0+.82RUC+,HIY8& V $M4fWaV ),&(828+,<2-&('*H0<3ZL&(8H=n3 '*HF'*&(/3\<210DQL&(8H4AJH0+.82RUC93 D05N+
Y[8& V tW43 '*HT8+,).RU-08&)M3 5U5UXe /_Yn&(5U5N&MLJ</2;43 / * $M 4 P $n$M4* $4y4fe

x{RU69108+\`MOqg8:3 '*<mY[&(8 V 3 /2RN&('Y8& V /2;*+9}

;



fi M]|9M.tM=uwuMM3
F



h

E





fi#

kqu2!q4

| +./J1*<'*&MLRU'Q/28&HO1*),+i(*f[2(pafW0L;0RN):;S3 8+@1*<:+,HF/&C93 5URNH*3 /+@K'*&L5N+,HO6+9e_K'*&ML5N+,HO6+D43(<+
7
LRU5U57D+uC(3 5URNH*3 /+,HFRYhRU/J<:3 /2RN<m4+,<+.C+.82Xs),&('*<2/28:3 RU'/MWa3 '*HT'*&\H0+,HO1*)./2RN&('LRU5U57DG+@3 5U5N&L+,H10'05N+,<<_/2;*+
@;43(<_D+,+.'sC(3 5URNH*3 /+,H!RU's-08+,<+.'*),+@& Y}),&('*</28:3 RU'Q/<MW0H0+,HO1*)./2RN&('FRN<_H0+f*'*+,HT&('05UXi&('F3T(0fo.f4
K'*&ML5N+,HO6+PD43(<:+9e

} ff


K
6
]{Ay


fi;}]}$k{%}

q

y|y|



+ N$B$)
5'*+.6Q3 /2RUC+SR),&('*<2/28:3 RU'/
oF:(922{15

M}h A{% }AB -&9<2RU/2RUCO
JON L o(UO\/282RU696+.8imZO:(0f[2(4M = J L o(U2[&(D05URU6Q3 /2RN&('N$B$)5RU'/+.8HORN)./2RN&(' RT5
#G~$CRN&(5o3 /+,<EO f[M? 8N$B$)5G$9[M? Ri(*f[2(pa oPuGmmM2,[n(mEOP[fUMQf
}
pa$>OB2(a94Q,(fma
N$B$.
54$b
HRP9h(*a{@)jfaQ2N2f$)5O9}9I:
)j9.aQ2S Rl$a2m,,[[9F2 (JE
9h(NW 5CRN&(5o3 /+,< [ `!c?Mn(U9B ,(_.(ha2m,,[[9
a5 OfS 9!oM =`<:3 /2RN<m4+,< 85

fi;}]}












=



r



t))%

r










r



x{RU69108+\`9`9]+,HO10'*H*3 '*).XF3 '*HI),&('*<2/28:3 RU'Q/CRN&(5o3 /2RN&('
r +T;43MC+F/&S-G&(RU'/&(10//2;*+FR V -&(82/:3 '*),+I& Y/2;*+IRU828+,HO10'*H*3 '*).X),&('*HORU/2RN&('&('/2;*+T698:3 -0;/&

D+IC93 5URNH*3 /+,HDQX-G&9<2RU/2RUC+),&('*<2/28:3 RU'Q/<y<;*&(105NHL+FYn&(826+./i/2;0RN<i),&('*HORU/2RN&('7WJ/2;*+.8+ V 3MXD+F/L&
+,w10RUC(3 5N+.'/^0?E<W}<21*):;j/2;43 /l&('*+<:3 /2RN<m4+,<Z3F-&9<2RU/2RUC+\),&('*</28:3 RU'Q/\3 '*Ht/2;*+i&(/2;*+.8H0&+,<>'*&(/MeFx{RU6*e
`9`Z<2;*&MLJ<3 'S+f*3 V -05N+& Y<1*);698:3 -0;*<M
e <:3 /2RN<m4+,< \WD010/u/2;*++,w10RUC93 5N+.'Q/=8+,HO10'*H*3 'Q/BA@698:3 -0;
kW}&(D0/:3 RU'*+,HDQX V 3 KRU'06/2;*+HORN<nm&(RU'/>10'0RN&('& H
3 '*Ht/2;*+/282RU696+.8& W}H0&+,<>'*&(/Mesg}&y3MC&(RNH
HOR+.8+.'Q/}),&('*<RN<2/+.'*).XC93 5U1*+,<Yn&(87+,w10RUC(3 5N+.'/7698:3 -0;*<MWL+!;43MC+]);*&9<:+.'P/&H0+f*'*+q-&9<2RU/2RUC+!),&('*<2/28:3 RU'/
<:3 /2RN<mY$3()./2RN&('LPe 8Me /Meh/2;*+RU828+,HO10'*H*3 'Q/Yn&(8 V & Y}3l^0?ZeQg_;0RN<-08&(D05N+ V H0&+,<]'*&(/&),).108]LRU/2;s'*+.6Q3 /2RUC+
),&('*<2/28:3 RU'/<Me '*H0+,+,HW5N+.E
/ 3 '*
H D+l/L&+,wQ10RUC93 5N+.'Q/@698:3 -0;*<>3 '*H<10-0-G&9<o
+ G~$CRN&(5o3 /+,<>3
'*+.6Q3 /2RUC+\),&('*<2/28:3 RU'/ <2RU'*),+Z/2;*+.8+\+fORN<2/<P3i-08& m+,)./2RN&('Y[8& V RU'Q/
& W7<:3M
X D.
W HRN<P3
-08& 2+,)./2RN&('FY8& V /& WO/2;1*Y
< 3 5N<&ZCRN&(5o3 /+,< \e
gL&T),&('*<2/28:3 RU'Q/< 3 '*H 3 8+l<:3 RNH/&iD+Ty ,M ?(Nf4RY3 'QX698:3 -0;/2;43 /ECRN&(5o3 /+,< 3 5N<&
CRN&(5o3 /+,< 3 '*HT),&('C+.8<+.5UXe]'QXi'*+.6Q3 /2RUC+P),&('*<2/28:3 RU'Q/RN<+,w10RUC93 5N+.'Q/J/&l/2;*+E'*+.6Q3 /2RUC+@),&('*<2/28:3 RU'/
&(D0/:3 RU'*+,HDQX),&(5N&(82RU'06k3 5U5RU/<'*&H0+,<\DX`9ex01082/2;*+.8 V &(8+9WJ'*+.6Q3 /2RUC+y),&('*<2/28:3 RU'Q/<T3 8+IRU'*H0+,+,H3
-43 82/2RN).105o3 8)M3(<:+& Y-&9<2RU/2RUC+&('*+,<M{),&('*<2RNH0+.8q/2;*+J-G&9<2RU/2RUC+),&('*<2/28:3 RU'Q/ &(D0/:3 RU'*+,HY[8& V 3E'*+.6Q3 /2RUC+
),&('*<2/28:3 RU'/ DX),&(5N&(82RU'06\3 5U5G'*&H0+,<& DQXiOW/2;*+.'T3(H0HORU'06\3),&('*),+.-0/_'*&H0+u),&(5N&(8+,HsDQX`9WLRU/2;
/X-+8 *W9L;*+.8+, " ERN<qRU'*),& V -43 8:3 D05N+JLRU/2;i3 5U5*&(/2;*+.8q/X-+,<]3 '*H\H0&+,<h'*&(/3 -0-+M3 8qRU'
3 'QXT698:3 -0;y& Y{/2;*+@uW4+fO),+.-0/JRU'),&('*<2/28:3 RU'Q/<Meg_;*+.'y3\<2R V -05N+@698:3 -0
; CRN&(5o3 /+,</2;*+P),&('*<2/28:3 RU'/
RY3 '*H&('05UXRYqRU/CRN&(5o3 /+,< e &9<2RU/2RUC+),&('*<2/28:3 RU'Q/<E<2/282RN)./25UXRU'*).5U1*H0+'*+.6Q3 /2RUC+Z),&('*<2/28:3 RU'/<MWRU'
/2;*+J<+.'*<:+/2;43 /q/2;*+3(<<:&).Ro3 /+,H\),&('*<RN<2/+.'*).XZ-08&(D05N+ V <3 8+'*&(/]RU'\/2;*+J<:3 V +J),& V -05N+fORU/X\).5o3(<:<=/2;*+
-08&& Y}Yn&(5U5N&MLJ<JY8& V g_;7e4cAfe





@



4UBB

yn

(Gf65










=














.



_ ` ! H+=aO B[[M? E(0.[m9p4E(2Pf[f[,Qffm9 ^sM9[n(2JGn['?


9



fi

3MfiuM%

^RU'*),+F'*+.6Q3 /2RUC+),&('*<2/28:3 RU'Q/<T3 8+TRU'*H0+,+,H3y-43 82/2RN).105o3 8s)M3(<+I& YE-G&9<RU/2RUC+F&('*+,<MWL+FLRU5U5J'*&LPW
10'05N+,<<RU'*HORN)M3 /+,H&(/2;*+.82LRN<+9WH0+.'*&(/+FDX 3<+./& YP),&('*<2/28:3 RU'/< 3<+./& YE-G&9<RU/2RUC+I),&('*<2/28:3 RU'Q/<
<& V +@& Yh/2;*+ V )M3 'ID+@+,wQ10RUC93 5N+.'Q//&Z'*+.6Q3 /2RUC+>&('*+,<Me



5

7


q

X "$n+*m.4ToI),&('*<2RN<2/+.'/I Z

fi;}]} M}h Ayn} F
y]fiM} } _7 nY
.[oM
< Bi((*f[2(paim@a5 OfS9!p,=Epl,(njRU'*),&('*<RN<2/+.'Q/A5 } (H0+,HO1*),+,H
fm( JpP(0fo.f4_(aZ :(jZQ2(0:2@B2( d5
&(/+P/2;43 /3s^0?/2;43 /CRN&(5o3 /+,<3),&('*</28:3 RU'Q/E& Y! V 3,XI<2/2RU5U5DG+PH0+,HO1*),+,HIY8& V
u
'*&(/ V 3 /2/+.8<RU'*),+RN<J30(f[n({8+.-08+,<+.'Q/:3 /2RN&('y& Y{K'*&ML5N+,HO6+H0+,HO1*).RUD05N+@Y[8& V e

e /H0&+,<



}

y{M}hhM]

[MMM

E+,HO1*)./2RN&('FRU'FRN<+,<:<+.'Q/2Ro3 5U5UXi'*&(' V &('*&(/&('0RN)9e]H0HORU'06RU'OYn&(8 V 3 /2RN&('F/&Zk)M3 'T/282RU696+.83'*+.L
),&('*<2/28:3 RU'/MW3 '*H/2;1*<i)M3 ').8+M3 /+3'*+.L CRN&(5o3 /2RN&('7S<2RU'*),+I'*&(/2;0RU'06k)M3 'DG+IH0+,HO1*),+,HY[8& V 3 '
RU'*),&('*<2RN<2/+.'/K'*&ML5N+,HO6+D43(<+9W0-08+.CRN&(1*<H0+,HO1*)./2RN&('*<E3 8+@'*&5N&('06+.8C(3 5URNHeg_;43 /JRN<JL;XiYn&(8
3 '*H V &(8+Z6+.'*+.8:3 5 V &H0+.5N<\='*+fO/@<+,)./2RN&('*<BAfWRU/ERN<uR V -G&9<:<2RUD05N+l/&s&(D0/:3 RU'8+,<105U/<@& YqY[&(8 V )M3 '
D+H0+,HO1*),+,HsY[8& V /2;*+uK'*&ML5N+,HO6+ED43(<+ERSj
$[E
4 Ij
$$E
4 E3(<RU/_L_3(<_/2;*+u)M3(<:+JYn&(8_3 '*Hs2e
u&ML+.C+.8MWQ/2;*+'*&(/2RN&('s& Y),&('*<RN<2/+.'*).Xl)M3 'DG+_/28:3 '*<25o3 /+,HRU'Q/&Px{zu|qeQx*&(8]'*+.6Q3 /2RUC+J),&('*<2/28:3 RU'Q/<W
/2;*+),&(828+,<2-&('*H0+.'*),+RN<!R VV +,HORo3 /+9W3 '*HZ8+.5URN+,<q&('-08& m+,)./2RN&('i<&(10'*HO'*+,<<]3 '*H),& V -05N+./+.'*+,<<]LPe 8Me /Me
/2;*+l<+ V 3 'Q/2RN),<> =/2;*+,&(8+ V `Afe 'Q/210RU/2RUC+.5UXW}3s^0? CRN&(5o3 /+,<@3'*+.6Q3 /2RUC+Z),&('*<2/28:3 RU'Q/ ~SjRY3 '*H
&('05UXRY}/2;*+RU'OYn&(8 V 3 /2RN&('F8+.-08+,<+.'/+,HFDQX RN<H0+,HO1*).RUD05N+uY[8& V /2;*+uRU'OY[&(8 V 3 /2RN&('F8+.-08+,<+.'/+,HFDQX
Ze







fi



p



U



U

V M? n(U9BiGnQ9['?(F(0.[m9p4
"$ *H4 j$4*Bj$E4 j$ 4>=
hy|MyfV }
9hOf b!pPOj}aQff;:
N(Glj$ j4PoPO>U(n(.(fZUs :,M.n92T$iQob}R 5





U

F

%&('*<2RN</+.'*).Xl8+.5o3 /2RUC+J/&@-G&9<RU/2RUC+),&('*<2/28:3 RU'Q/<)M3 '\D++f-05o3 RU'*+,HLRU/2;ix}zu|qW/28:3 '*<5o3 /2RU'06 -08& ~
2+,)./2RN&(' ]RU'Q/&@3'*&(/2RN&('Z& 5N&(69RN)M3 50<210D*</2RU/210/2RN&(' E=$%;*+.RU'1069'0RN+.8MW*`Mb9b9A7DG+./L+,+.'/2;*+]Y[&(8 V 105o3(<
3(<<&).Ro3 /+,H/&F698:3 -0;*<ert+\)M3 5U5_3 '!B*Bf[[[[n(kY[8& V j$[4uRU'/&yj$ J4@3F<10D*<2/2RU/210/2RN&('&
/+.8 V <]& Y}j
$E4{DQX/+.8 V <& Y7j
$ J4!<1*);\/2;43 /),&('*<2/:3 'Q/<& Y[
$[4q3 8+K+.-0/]RU'QC93 82Ro3 'Q/3 '*HWYn&(8]3 'QX
3 /& V 6$ * FtFtF * G 4& Y]j
$[4fW*/2;*+.8+@RNd
< \_<21*):;F/2;43
/ $
$ 4* FtFtF *
$ G 4y4RN<3 '3 /& V & Yqj
$ J4fe
g_;*+EYn&(5U5N&MLRU'06i-08&(-G+.82/Xs;*&(5NH0<





`



K







`

`







? . :a2m,,[[9f2( $
QM<]GBy(!B*Bf[[[O[[(B2(
yn
j$ J465 :fZs9
o@a9B\(9,(fC=qOl9? fB,Z( .9 W5





j$[4$



2MmWJ|+./D+@3>-08& 2+,)./2RN&('TY[8& V /& ke!x*&(8J+M3();TC93 82Ro3 D05N+b]& Yhj$E4fWO5N+./D+/2;*+u10'0RNw1*+
6+.'*+.82RN)),&('*),+.-0/i'*&H0+<21*);/2;43 /8]" [$ 64fW/2;*+.'g$M])4" j$ $ W4y4fe+,).RU-08&)M3 5U5UXW_-08&MCRNH0+,H
/2;43 /
RN<RU'I'*&(8 V 3 57Yn&(8 V W*/2;*+l3 -0-05URN)M3 /2RN&('Y[8& V ),&('*),+.-0/u'*&H0+,<& Ya
/&),&('*),+.-0/'*&H0+,<& kW
/&>/2;*+u'*&H0+ba
<21*);T/2;43 /,$$ j$ W4y4a
" [$ q
4 RN<3>-08& 2+,)./2RN&('sY8& V
/& eu
&(/+
V 3 -0-0RU'06\+M3();7
/2;43 /MW010'05N+,<<
RN<RU''*&(8 V 3 5Y[&(8 V W RN<'*&(/J10'0RNw1*+.5UXFH0+f*'*+,HFL;*+.'yj$
4 RN<3Z),&('*<2/:3 '/Me



q

;



@



U

(



V`!c?Mn(U9BE(0.[m(4 OH
!B*Bf[[[[n(Tfm9j$ JPN L 44$
u {3
(2B4o
j$[4 B.M.n92Z9![(4a9)j9fGQ2T$s(!$f0B.[[O[n(f2( j$ C4E4$l[$[465


U

fi M]|9M.tM=uwuMM3
F

'*&(/2;*+.8]D082RNHO6+)M3 'D+D010RU5U/]1*<2RU'06>82105N+,<Me '*H0+,+,HWO3E698:3 -0;<:3 /2RN<m4+,<3E-&9<2RU/2RUC+J),&('*<2/28:3 RU'/

RY03 '*H&('05UX@RYmW),&('*<2RNH0+.82RU'063(<}3J82105N+9W 3 5U5O3 -0-05URN)M3 /2RN&('*<{& Y|&('r-08&HO1*),+3J698:3 -0;+,w10RUC(3 5N+.'/

/&8Zez8MW V &(8+P<2-+,).R4)M3 5U5UXG

}





=(*BnQffp
yn
} `!?M[99fFTO B[[M? I90f[2(4
BG n9[n(tm (:(2(pF$Eta2M(0:B(2B4IG9y,M?(Nf4_$o5








ISfN=0

2MmW|7+./ D+3i),&('*<2/28:3 RU'Q/P3 '*HJDG+Z3s^0?<21*):;y/2;43 /[<:3 /2RN<m4+,< e Ya N RN<E3\-08& 2+,)./2RN&('
Y[8& V
=[),&('*<2RNH0+.8+,H
JPN L RU'Q/&8WO5N+./1*<),&('*<RNH0+.8/2;*+698:3 -0;bG&(D0/:3 RU'*+,HTDX/2;*+P3 -0-05URN)M3 /2RN&('&
'*&ML3(<3\82105N+A&('J
3(),),&(8HORU'06s/&8 N e|+./1*<J'*&LD010RU5NH/2;*+@Yn&(5U5N&MLRU'06s-08& 2+,)./2RN&(' Y[8& V
RU'Q/&7Z
Yn&(8+M3():;'*&H0++GW0t$ 4d
" TRY&TDG+.5N&('06<u/&Z
&(/2;*+.82LRN<+9WsRN<E3\),&(-X& Y3'*&H0+&
XJ L W3 '*HZRY)T
RN<q&('*+J& Y/2;*+-08& 2+,)./2RN&('*<!Y[8& V RU'Q/&k
/2;43 /]+fO/+.'*H0< N W9L+J;43MC+ t$ 4a"Qt$ jf4 e
g_;*+.'B RN<>3i-08& m+,)./2RN&('j& RU'Q/&gZ
W}3 '*Ht<2RU'*),+o /282RUCRo3 5U5UX-08& 2+,)./<@RU'Q/& W/2;*+.XS3 8+Z/2;1*<









+,w10RUC(3 5N+.'/Me
g_;0RN<@-08&C+,<@/2;*+C -43 82/P& Y-08&(-+.82/XvOelx*&(8@/2;*+C -43 82/MWL+Z1*<:+Z/2;*+lY[&(5U5N&LRU'06F-08&(-G+.82/XW
-08&MC+,HtDQXS%&(69RN<3 '*H?E10RU'43 5NH0&j=m`Mb9b9vAfe 'S/2;*+.RU8@-08&(-G+.82/X=-08&(-7eZs& Y/2;*+.RU8@-43 -+.8BA/2;*+\^0?E<
),&('*<2RNH0+.8+,H3 8+>),&('0'*+,)./+,HI698:3 -0;*<MW0D010//2;*+E-08&& Yh;*&(5NH0<Yn&(8'*&('),&('0'*+,)./+,HI698:3 -0;*<e

q !r

uvu{}


:H
}(apf6 $[4(2][y .' ?9Ufa
yn f
u
] }${M]
,
B2(a(aqf0n9mBaZN 1oEa9]2(G(4_OfyB6$[4a"YRT5UOfSOf)j(pf>iYn&(5NHORU'06
fm( $FB6$[4>=J5pW5sGmmM2,[n( yB2( 4$TfL$[4>=fO.y9OZBf[fn,[[9km $
aMBm@B6 $[4PoPO>nQf4[[ :gN ,(>S ? . :saMQ1
]m@B6 $[4>= $M])4"]R 5




$



^10-0-&9<+'*&Lq G~$CRN&(5o3 /+,< e^RU'*),+),&('*<2/28:3 RU'Q/CRN&(5o3 /2RN&('tRN<>H0+f*'*+,HLRU/2;t8+,<2-+,)./@/&T/2;*+
RU828+,HO10'*H*3 'Q/Yn&(8 V & Y!3698:3 -0;7W*L+>)M3 '),&('*<2RNH0+.8MW*LRU/2;*&(10/5N&9<<J& Yh6+.'*+.8:3 5URU/XWa/2;43 /YRN<RU828+,HO10'O~
H*3 'Q/Me_rt+H0+.'*&(/+PDXY/2;*+P698:3 -0;S&(D0/:3 RU'*+,HIDXF/2;*+3 -0-05URN)M3 /2RN&('S&
=n3 6Q3 RU'7WG),&('*<2RNH0+.8+,H'*&L
3(<3P82105N+Aq&(7
' 3(),),&(8HORU'06l/E
& !e{rt+-08&MC+/2;43 / +,w10RUC(3 5N+.'//r
& 5N+M3(H0<]/&l3P),&('Q/28:3(HORN)./2RN&('7e
RN<]+,w10RUC(3 5N+.'/]/r
& ZW/2;*+.'i/2;*+.8++fORN<2/<3E-08& 2+,)./2RN&('iY8& V RU'Q/
& Zeh'*H\<RU'*),Y
+ RN<3 '

RU828+,HO10'*H*3 'Q/E<210D0698:3 -0;&
WO/2;*+.8+P+fORN<2/<3u,(9p Y[8& V RU'Q/Z
& =-08&(-G+.82/XFAfe%&('*<2RNH0+.8
'*&ML /2;*+I-08& 2+,)./2RN&('Y[8& V
/& H0+f*'*+,H3(<Yn&(5U5N&MLJ<MIY[&(8i3 'X'*&H0
+ ]& JON L
W $M])4C0
$ $M]4y4fW&(/2;*+.82LRN<:+Z5N+.Y
/ ] D+>/2;*+),&(-X&
]jRUB
' WGL+Z;43,C
+ $M]4
0 $M] 4fe^RU'*),+lY[&(8@3 5U`
5 ]tRU'
< !ehg_;0RN<),&('/28:3(HORN)./<_/2;*+;X-&(/2;*+,<2RN< AG~$CRN&(5o3 /+,< :ehg_;1*<
JON L W $ $M]4y4"Q$M])4fW 4+f/+.'*H0d
RN<'*&(/+,w10RUC93 5N+.'Q/J/8
& Ze





















fi

@



4

}
,9[p'<BJO f[M? E90f[2(4 r=}OfI(:(2B4\TEm kn !mQff'? 9[[9
yn eh p[
mHp>3.M?(Nf4_$o5


4

F

2MmW J|+.
/ "p N * FtFtF *H G D+@3m rn,~H0+.82RUC(3 /2RN&('I& Y`eqx08& V -08&(-G+.82/XsvOWO+M3(); mWI[\\ K W
RN<J+,w10RUC(3 5N+.'/J/& WO/2;1*<JDQXs/28:3 '*<2RU/2RUCRU/XW4RN<J+,wQ10RUC93 5N+.'Q//& Ze
<2RU'06S<&(10'*HO'*+,<<3 '*H),& V -05N+./+.'*+,<<& Y/2;*+i H0+,HO1*)./2RN&('7W]3 '*Hk-08&(-+.82/2RN+,<\v3 '*HW!&('*+
&(D0/:3 RU'*<J/2;*+@Y[&(5U5N&LRU'06\8+.5o3 /2RN&('LRU/2;x{zu|jH0+,HO1*)./2RN&('7e

p





F



Ofyj(o.}Y>B0f9
hy|Myp
h {q ?Mn(BIO B[[M? y(0.[m(4
j$d4*Bj$[4*Bj$ C4 Ij$ 4P(aG9j$4*B[$[4 I[$ 4>=9h0f2uj$ C4uoEO@[2(0f[[(Sm
(*BnQf2I >BU5

U



;



U

@

fi

3MfiuM%

g_;0RN<s/2;*+,&(8+ V )M3 'DG+8+fY[&(8 V 105o3 /+,HRU'/+.8 V <T& Yl3 DHO1*)./2RUC+yRU'OY[+.8+.'*),+=1*<2RU'06kRU'Yn3()./sRU'O~
HORU8+,)./s3 DHO1*)./2RN&('7W<+,+9WYn&(8+f03 V -05N+9W_&('*&(5URU6+9WP`Mb9b9Afe '*H0+,+,HW69RUC+.'3D43()K698&(10'*H/2;*+,&(82X
$d
4*Bj
$[43 '*H3 '&(D*<+.82C93 /2RN&('%" ]j$ E4fWaCRN&(5o3 /+,<
R/2;*+.8+TRN<\3 '3 DHO1*)./2RUC+
" j
+fO-05o3 '43 /2RN&('IYn&(8E& Yh/2;*+uY[&(8 V \WOL;*+.8+9RN<3ZY[&(8 V 105o3ZDG+.5N&('069RU'06/&\x}zu|a$ * 4fe

} q

]$A{M}h{



U


q B` < K




$

U

l

g_;*+-08&(D05N+ V H0&+,<i3j69RUC+.'698:3 -0;<:3 /2RN<mY[X369RUC+.'),&('*<2/28:3 RU'Q/
RN<i),& ~c ~),& V -05N+./+yRYP/2;0RN<
),&('*<2/28:3 RU'/RN<'*+.6Q3 /2RUC+t=[<RU'*),+iL+ V 1*<2/);*+,):K/2;*+T3 D*<+.'*),+T& Y-08& 2+,)./2RN&('aAfW!D010/ZD+,),& V +,< _ ` ~

),& V -05N+./+@Yn&(83l-&9<2RU/2RUC+P&('*+\= _ ` RN<),& ~c 2 Afe


p

q

K$72D*4tp _

< K

_
_
aN.> >9p90f[2(4(Pn9[M? R 5
hy|Myk

B`y $ [ }



` !m(aN.bNmfGp]! H!:(r!


2MmWSrRU/2;*&(10/i):;43 '06+& Y>),& V -05N+fORU/XWJ&('*+)M3 '),&('*<2RNH0+.8s/2;43 /\RN<i),& V -G&9<:+,H& Y>&('05UX&('*+
-&9<2RU/2RUC+i),&('*</28:3 RU'Q/MWq<:3MX eIx{RU8<2/8+,)M3 5U5/2;43 /ZH0+,).RNHORU'06SL;*+./2;*+.8\3^0? <B3 /2RN<m4+,< RN<lH0&('*+
&('j/2;*+RU828+,HO10'*H*3 'Q/lYn&(8 V & YHZ
eTrt+<;43 5U5]),&('*<2RNH0+.8l/L&L3MX<l& YJRU'Q/+.698:3 /2RU'06y/2;0RN<>Yn3()./RU't/2;*+
),& V -05N+fORU/XF& Y{_ 7
4]e_z'*+@L3MXTRN</&\3(<<21 V +E/2;43 /J/2;*+ERU828+,HO10'*H*3 '/Yn&(8 V & Ya
RN<


K$ 2D*



),& V -010/+,HD+fY[&(8+Z/2;*+Z),&('*<2RN</+.'*).Xy):;*+,)Keg_;0RN<P)M3 'DG+3():;0RN+.C+,HLRU/2;j3s'Q1 V DG+.8>& Y_)M3 5U5N<@/&I3
-08& 2+,)./2RN&('S&(8:3().5N+5URU'*+M3 8ERU'/2;*+l<2R M+l& Y=[1069'0RN+.8MW!`Mb9b9vAfeu10/MWG<RU'*),+>L+>;43MC+/2;*+.'S/&s<&(5UC+
3\Y10'*)./2RN&('y-08&(D05N+ V =[),& V -010/+/2;*+lRU828+,HO10'*H*3 '/EY[&(8 V &
PAJRU'*<2/+M3(H& Y_3H0+,).RN<2RN&('y-08&(D05N+ V =RN<
RU828+,HO10'*H*3 'Q/ AfWL+l-08+fYn+.8E/&RU'/+.698:3 /+ZRU828+,HO10'*H*3 '*).XRU'Q/&T/2;*+l),&('*<2RN<2/+.'*).X);*+,)K/2;*+.'7W4Yn&(8
3-08& m+,)./2RN&(
' N Y8& V /2;*+>/282RU696+.8@& RU'Q/
& ZWG/2;*+-08& m+,)./2RN&('Y[8& V /7
& L+>5N&&(KFYn&(8H0&+,<
'*&(/'*+,),+,<:<:3 82RU5UXF+f/+.'*H0H
< N W*D010/+fO/+.'*H0<J/2;*+),& V -G&9<RU/2RN&('& Y]3Z-08& 2+,)./2RN&('IY[8& V RU'/&&('*+P&
RU/<J<210D0698:3 -0;*<l=-G&9<:<2RUD05UXs+,w143 5/& RU/<:+.5YfA_3 '*
H N e
x{RU8<2/MW>G 7
4 D+.5N&('06<y/& _` <2RU'*),+RU/y),&(828+,<-G&('*H0</&/2;*+5o3 '069143 6+G "

mW] d_ _ $M]*_ *_ 4>nQWL;*+.8+ ]+.'*),&H0+,<3 'RU'*<2/:3 '*),
+ $k* E4& Y\/2;*+k-08&(D05N+ V 3 '*H


$M]*d_ *d_ 4 R
_ +.'*),&H0+,<3l-08& 2+,)./2RN&(
' N Y8& V XJPN L RU'Q/Z
& 3 '*
H _ +.'*),&H0+,<3l-08& 2+,)./2RN&('
Y[8& V RU'Q/&>&('*+& YRU/<<210D0698:3 -0;*<3 '*Hs3E-08& 2+,)./2RN&(' TY[8& V RU'/k
& <Me /M`
e JON L "Q ) N e
u&(/+E/2;43 /JR
RN<RU'FRU828+,HO10'*H*3 'Q/Yn&(8 V WO/2;*+.g
' RN<3 '3 10/& V &(82-0;0RN< V e
u&MLPW@5N+./1*<),&('*<2RNH0+.8/2;*+-08&(D05N+ V ~ 69RUC+.'3DG&&(5N+M3 'Yn&(8 V 105o3WP3 '*H3-43 82/2RU/2RN&('

*9 ni& YRU/<@C(3 82Ro3 D05N+,<W7RN<@RU/@/2821*+/2;43 /EYn&(8>3 'QXS/28210/2;3(<:<2RU69' V +.'Q/@Yn&(8@/2;*+ZC93 82Ro3 D05N+,<PRU'


/2;*+.8+>+fORN<2/<u3Z/28210/2;y3(<<RU69' V +.'Q/JYn&(8J/2;*+>C(3 82Ro3 D05N+,<RU'B <Me /Me@RN<J/2821*+ Sg_;0RN<-08&(D05N+ V RN< _ ` ~

),& V -05N+./+9W<RU'*),+ZRU/<@),& V -05N+ V +.'Q/:3 82X RN<P<2;*&ML'/&TD+ ` ~),& V -05N+./+DQXt^/&)K V +.X+.8i=m`Mb99Afe

4]W*L+E1*<+P3l8+,<2/282RN)./2RN&('& Y{/2;0RN<
'T&(8H0+.8/&ZD010RU5NHI3l-G&(5UX'*& V Ro3 58+,HO1*)./2RN&('I/&Z 7p
-08&(D05N+ V /& K ~
% ux*<MWOR$e+9e!),&('(m10'*)./2RN&('*<& YhHORN<[10'*)./2RN&('*<JLRU/2;3 / V &9<2/ K 5URU/+.8:3 5N<-G+.8J).5o3 1*<:+9e!|7+./
1*<u)M3 5U5 ~ F1
ff &E /2;*+><2-+,).Ro3 5})M3(<+>L;*+.8+~RN<E3 ~
% ux_W*RU'S&(/2;*+.8L&(8H0<@3 'RU'*<2/:3 '*),+l& ~^!g@e

g_;*+.' ~ F1
ff &E RN<J3 5N<& _ ` ~),& V -05N+./+9e '*H0+,+,HW0RU'F/2;*+@<:3 V +E-43 -+.8=[g_;7eOd*eU`AfWa^/&)K V +.X+.8<2;*&LJ<


/2;43 / LRU/2;8+,<2/282RN)./+,H/&3 ~HORN<n10'*)./2RUC+i'*&(8 V 3 5]Yn&(8 V = ~b
uxqAE8+ V 3 RU'*< ` ~),& V -05N+./+9e

^RU'*),+l/2;*+>'*+.6Q3 /2RN&('t& Y3 ~b
uxRN<E3 ~
% ux_WaRU/Yn&(5U5N&MLJ<E/2;43 /u/2;*+l),& V -05N+ V +.'/:3 82X-08&(D05N+ V ~

LRU/2;B8+,<2/282RN)./+,HI/&\3 ~
% uxkRN< _` ~),& V -05N+./+9e

|7+./]1*<!'*&ML8+,HO1*),+ ~ F1
ff & /&@ 7p
4]eqg_;*+/28:3 '*<2Y[&(8 V 3 /2RN&('i1*<+,HZRN<!C+.82X<2R V R~

5o3 8q/&@/2;*+&('*+_Y8& V ~^!g/&@} k=-08&& Y7& Y/2;*+,&(8+ V AfWQRU5U5U1*<2/28:3 /+,HRU'x{RU6*e{d*eh|7+./
D+3 'sRU'*<2/:3 '*),+& ~^!g@eO|7+.
/ $r
43 '*HFo
$r
4!D+/2;*+E^0?E<&(D0/:3 RU'*+,HsDQX/2;*+/28:3 '*<mYn&(8 V 3 /2RN&('
H0+,<).82RUD+,HZRU'/2;*+_-08&& Y& YGg_;7eOehg_;*+),&('*<2/28:3 RU'/ o$r
4"ff$$o
$r
4*)
H $t 4y4}RN<!&(D0/:3 RU'*+,HDQX3(H0HORU'06
3u),&(5N&(8:3 /2RN&('/&Po
$k
4fh3 5U5O8+.5o3 /2RN&(''*&H0+,<h&(D0/:3 RU'*+,HZY[8& V ).5o3 1*<+,<J='*&H0+,<h/X-+,H $A{3 '*HZ3 5U5O'*&H0+,<





>~

6





K$ 2D*





K AD*

>~

>~

K AD*
Kv { $









d7

fi M]|9M.tM=uwuMM3
F



b

val

val

val



af

bt

c

val

bf

val

val

val

ct

cf

dt

2
1

C1

C1

C1

C1

C1

C1

1

C1

C2

C2

C2



b

c



val

val

val

val

val

df

av

bv

cv

dv

...

3

2
3



C2

C2

C2




C2

>~

1

2

2
3

C1

1

K7pAD*4

3

C2

x{RU69108+\`MO 0 3 V -05N+>& Y{/28:3 '*<mYn&(8 V 3 /2RN&('IY[8& V ~ Fff1&E /&_7


@


0

6


&(D0/:3 RU'*+,HFY[8& V C93 82Ro3 D05N+,<JRU' =[),&('*),+.-0/'*&H0+,<J/X-+,H T&(8 T3 '*HF8+.5o3 /2RN&(''*&H0+,</X-G+,H QA
3 8+@),&(5N&(8+,HFDXy`\=R$e+9e!D+.5N&('06\/&Z/2;*+@&(D05URU6Q3 /2RN&('aAfe_z'*),+>3 6Q3 RU'7W0;43,CRU'06).5o3 1*<+,<& Y}D&(10'*H0+,HI<2R M+
5N+M3(H0<P/&3T-G&(5UX'*& V Ro3 5q/28:3 '*<mYn&(8 V 3 /2RN&('7eTg_;*+\<2R V -05N+698:3 -0
; 3 '*H/2;*+\-G&9<2RU/2RUC+),&('*<2/28:3 RU'Q/
-08+,<+.'/+,HRU'x{RU6*e7`Mi3 8+P&(D0/:3 RU'*+,HY8& V /2;*+ ~^!gYn&(8 V 105o
3 $
`W4 J$

43 '*H
/2;*+@-43 82/2RU/2RN&('K "Vm * n *) "VmA* nQe
3():;t/28210/2;t3(<<2RU69' V +.'/>& Y]/2;*+ZC93 82Ro3 D05N+,<@& Y@ <Me /Me+RN<E/2821*+Z'43 /2108:3 5U5UX69RUC+,<>3i-08& 2+,)./2RN&('
Y[8& V RU'Q/
& W3 '*H8+,).RU-08&)M3 5U5UX=n3(<RU'*HORN)M3 /+,HRU'/2;*+F-08&& Yu& Y@g_;7eqAfex01082/2;*+.8 V &(8+9W3 'QX
/28210/2;y3(<:<2RU69' V +.'Q/Y[&(8/2;*+>C(3 82Ro3 D05N+,<& YT '43 /2108:3 5U5UXT69RUC+,<u3Z-08& m+,)./2RN&('Y8& V XJON L RU'Q/Z
& W43 '*H
8+,).RU-08&)M3 5U5UXeqg_;Q1*<MW/2;*+uwQ1*+,</2RN&(' RN<RU/]/2821*+u/2;43 /Y[&(8_3 'QXi/28210/2;T3(<<RU69' V +.'Q/Y[&(8/2;*+uC(3 82Ro3 D05N+,<RU'
/2;*+.8+l+fORN<2/<u3\/28210/2;S3(<:<2RU69' V +.'Q/Y[&(8u/2;*+PC93 82Ro3 D05N+,<uRU'B <Me /MeX RN</2821*+ RN<u+,wQ10RUC93 5N+.'Q/E/&
/2;*+@w1*+,<2/2RN&(' RN<_RU//2821*+E/2;43 /_Y[&(8J3 'QXs-08& 2+,)./2RN&(
' N Y8& V
& /2;*+.8+@+fRN<2/<3l-08& 2+,)./2RN&('
JPN L RU'Q/8
Y[8& V RU'Q/Z
& +fO/+.'*HORU'08
6 N :e
u&(/+l/2;43 /@/2;0RN<E8+,HO1*)./2RN&('RN<E5N+,<<@<2/28:3 RU69;Q/mYn&(82L3 8Ht/2;43 '/2;*+l&('*+ZL+l-08&(-G&9<+,HSRU'=[3 6+./>
1069'0RN+.8MW499O`AfW0D010/JRU/JLRU5U5D+E1*<+,H3(<3lD43(<2RN<_Yn&(8/2;*+@-08&& Y!& Y!g_;7e7`MOe

K



q



!



;

u {3

0+ U( +T





l

AG





1 (OM[[(_o _` ! (aN.W5


h b"]u



fi#

ZX&QXdpEdo&

' -08+,<:+.'*),+& Y),&('*<2/28:3 RU'Q/<W7/2;*+l/L&FKRU'*H0<E& Y82105N+,<MWRU'OY[+.8+.'*),+Z82105N+,<9

H0+f*'*+@/L&i3 5U/+.82'43 /2RUC+ V &H0+.5N<e
f ff
fi;}]}$k{%}

3 '*HS+.C&(5U10/2RN&('t82105N+,<
D]W

uA{M}h

' 2DhWRN<<+,+.'3(<u/2;*+>RU'0RU/2Ro3 5hL&(825NHW8&&(/E& Y3-G&(/+.'/2Ro3 5U5UXRU'O*'0RU/+l/28+,+& Y-G&9<<RUD05N+>L&(825NH0<MW

3 '*HoDH0+,<).82RUD+,<E/2;*+Z-G&9<:<2RUD05N+Z+.C&(5U10/2RN&('*<@Y[8& V &('*+ZL&(825NHS/&F&(/2;*+.8<Melg_;*+H0+,HO1*)./2RN&('t-08&(D05N+ V
3(<2KO<_L;*+./2;*+.8/2;*+.8+@RN<EOI2>(0fo.f4+
9](BU !fm( $s8
9]9BU,9[ppS : le

K* { $

} .
"#$na
*)Da*m.4P+nbXd=!9alU,} >k}154 (S:

Q 29O:@fm( POfo>(*D!mQff'? 9[[9FB" N * F6F6F * G B0fI9M=4,(fiIo\\ K =+$ ,*m4
pP:(0Bpf.4(GZ9t:2(0:2@f2(q G 5


fi;}]}



'hWy-08&MCRNH0+,HLRU/2; RN<]3*'0RU/+JH0+,<).82RU-0/2RN&('s& Y3E-&(/+.'Q/2Ro3 5U5UXRU'O*'0RU/+JL&(825NHW/2;43 /q;43(<
/&TD+),&('*<2RN<2/+.'Q/Mei-0-05UXRU'063s82105N+/&I)M3 't).8+M3 /+\RU'*),&('*<2RN<2/+.'*).XW}D010/3iY1082/2;*+.8l3 -0-05URN)M3 /2RN&('
& Y3F82105N+ V 3,Xt8+,<2/&(8+i),&('*<2RN<2/+.'*).XeF|7+./l1*<@Yn&(8 V 3 5UR M+i/2;0RN<P'*&(/2RN&('& YP:(0Bpf.aL :y2B.$(29[[94e

6

$-

fi

3MfiuM%



U

^10-0-&9<+Z/2;*+.8+ZRN<P
3 G~$CRN&(5o3 /2RN&('t& Y3s-G&9<RU/2RUC+l),&('*</28:3 RU'Q/ RU'j/2;0RN<ECRN&(5o3 /2RN&('O$ b*34uRN<@<:3 RNH
/ &TD+~g!B.$(mfUiRY/2;*+.8+\+fORN<2/P3 '\~H0+.82RUC93 /2RN&('Y8& V RU'/&3I^0?
3 '*Ht3s-08& m+,)./2RN&('
Y[8& V
RU'Q/&FRU828$ J4@<21*):;t/2;43 />/2;*+\-08& m+,)./2RN&(' Y& Y/2;*+/282RU696+.8& RU'/&IRU828$ J4@)M3 'tD+
+fO/+.'*H0+,HI/&i3\-08& m+,)./2RN&('y& 3(<E3ZL;*&(5N+9eg_;*+PCRN&(5o3 /2RN&('& Y3'*+.6Q3 /2RUC+),&('*<2/28:3 RU'/u)M3 ''*+.C+.8
D+Z8+,<2/&(8+,Hr
e &(/+Z/2;43 /P/2;*+~~$8+,<2/&(8:3 /2RN&('j)M3 'j).8+M3 /+Z'*+.LCRN&(5o3 /2RN&('*<MW7/2;43 / V 1*<2/@/2;*+ V <:+.5UC+,<
D+E-08&MC+.'B~$8+,<2/&(8:3 D05N+9e







K 2D*

! U

Kv {$


4 {%} \

fi;}]} \
<2RN<2/+.'/ >
= .(I(:J}
9l(Cg!mQff'? f2(
? .:k`!cM? n(U9[[9Sm
=+$ Y*34@p,g!2B.$(2QfNW5
(*Bpff4_(aZ(jlQ29O:@fm( n$ a)* 7465

Integer: Zero


@

Integer

n X

" $n+
*)
*m.
4pi),&('O~
=,(S? fS:k:(0f[2(4

=.(
}9lQ(OB2@fm9 p



successor

<R V -05N+@698:3 -0;





Integer

),&(5N&(8+,Hy^0?

x{RU69108+\` %&('*<2RN<2/+.'*).XTRU'FDh
% &('*<2RNH0+.8ZYn&(8ZRU'*<2/:3 '*),+F3y@),&('/:3 RU'0RU'06t/2;*+I^0? RU'x}RU6*e_` Wq+fO-08+,<<2RU'06S/2;*+T+fORN<2/+.'*),+

& Y@/2;*+'1 V D+.8TOWJ3j),&('*<2/28:3 RU'Q/F3 '*H3t82105N+9W_DG&(/2;8+.-08+,<+.'Q/+,HDX/2;*+),&(5N&(8+,H^0? eg_;*+
),&('*<2/28:3 RU'/l3(<<+.82/<P/2;43 /J.9iS ? fS :4nf `=O.2ZQ.uT94nQ. =B0:BB.(Tm qe
/2;*+u82105N+ERN<3 'I+.C&(5U10/2RN&('F82105N+9W RN<<+,+.'I3(<J3 'sRU'*),&('*<RN<2/+.'Q/RU'0RU/2Ro3 5L&(825NHt=/2;*+.8+ERN<_'*&l<1*),),+,<<&(8
& YFRU
' PAP3 '*Hj'*&(/2;0RU'06LRU5U5DG+iH0+,HO1*),+,HtY[8& V /2;0RN<@ue Y/2;*+82105N+\RN<Z3 'jRU'OYn+.8+.'*),+\82105N+9W{RU/<
3 -0-05URN)M3 /2RN&('yR VV +,HORo3 /+.5UXF8+.-43 RU8</2;*+P),&('*</28:3 RU'Q/uCRN&(5o3 /2RN&('7W4L;0RU5N+>).8+M3 /2RU'06s3'*+.LRU'Q/+.6+.8MWa/2;43 /
;43(<'*&<21*),),+,<<:&(8MW4/2;1*<u3\'*+.LCRN&(5o3 /2RN&('7eJx{RU'43 5U5UXW+.C+.82XI),&('*<2/28:3 RU'Q/ECRN&(5o3 /2RN&('),&(105NHy+.C+.'Q/2143 5U5UX
D+E8+.-43 RU8+,HIDQXF3Z82105N+P3 -0-05URN)M3 /2RN&('7W3 '*HT/2;*+P@<2;*&(105NHFD+E-08&MC+.'y),&('*<2RN<2/+.'Q/Me
|7+./1*<]-&(RU'Q/&(10/]/2;43 //2;*+J2 V &H0+.54RN<&(D0/:3 RU'*+,H\Y[8& V 2\&(8]2DFL;*+.'FRN<+ V -0/XW03 '*H
RN<J&(D0/:3 RU'*+,HTY[8& V 2\=8+,<2-7e0D77AL;*+.'K =8+,<2-7eD{ARN<+ V -0/Xe
g_;*+CD7 V &H0+.5G),& V D0RU'*+,<DG&(/2;sH0+.82RUC(3 /2RN&('T<):;*+ V +,<& Y/2;*+J2\3 '*HD V &H0+.5N<M`
e &LPW
H0+,<).82RUD+,<3 'RU'0RU/2Ro3 5JL&(825NHWRU'OYn+.8+.'*),+I82105N+,<& ),& V -05N+./+I/2;*+FH0+,<).82RU-0/2RN&('& Y@3 'XL&(825NHW
),&('*<2/28:3 RU'/<s& Yu+.C93 5U143 /+I/2;*+),&('*<RN<2/+.'*).X& YP3L&(825NHWJ+.C&(5U10/2RN&('82105N+,<& YD/282X/& V 3 K+3
),&('*<2RN<2/+.'/L&(825NH+.C&(5UC+>RU'Q/&i3\'*+.LPW4),&('*<2RN</+.'Q/&('*+9e_g_;*+PH0+,HO1*)./2RN&('y-08&(D05N+ V 3(<2K<JL;*+./2;*+.8E
)M3 '+.C&(5UC+@RU'/&3),&('*<2RN</+.'Q/JL&(825NH<:3 /2RN<mY[XRU'06l/2;*+P6&3 5$e

.



.

K* {





}
Y{p(R VV +,HORo3 /+CD7~+.C&(5U10/2RN&('\B2(C}
CD7
Of@)j(pfu9(!Q.BM?9[n(@fm(ffpa$b (aZ(IR VV +,HORo3 /+
Da!QffM?9[n(@B2(ff 4$
5 CD~+.C&(5U10/2RN&('B2(
{
$F8
}p
oiMy.*fa:imE}Y"V N * F6F6F * G "V
B0ft9M =}.(9J
\C\ K =Y$ ,*)g
*m
4o(*Bpff4u(a =}.(o
I7\C\ K = Pp(Z2(n9
Ca
!mS ?9 O[[9\B2(
5\_' ?(ft~b
n X
" $na
*)
*R
*m
4>=_8
}:(H0+,HO1*),+,HTf2(
OfpP9ss
!mS ? ( O[n(IB
"p N * F6F6F * G B0f9h :(jZQ2(0:2@B2( $ G *)7
465

fi;}]}











r;*+.'KD" =8+,<2-7eF" AfW0&('*+P&(D0/:3 RU'*<J/2;*+@2\ V &H0+.5=8+,<2-7ehDAfe


fi M]|9M.tM=uwuMM3
F

q

f

7

M}h Ay}

}

2\

{%}

Kv { $
K AD*

MMM

g&/28:3 '*<25o3 /+G RU'5N&(69RN),<MWJ3<2/:3 82/2RU'06-&(RU'Q/T),&(105NHDG+/&j+fO/+.'*H/2;*+5N&(69RN)M3 5
/28:3 '*<25o3 /2RN&('& YE 7p
4
69RUC+.'RU'g_;7e]/&j3y/28:3 '*<25o3 /2RN&('& YEG 7
4]e
u&ML+.C+.8MW4/2;*+EYn&(5U5N&MLRU'06/2;*+,&(8+ V -G(& RU'/<&(10/J/2;*+E5UR V RU/:3 /2RN&('*<u& Y{/2;0RN<3 -0-08&3();7e

p

K$ 2D*

u

4 EnbXY5p 0f2u)j(pfj}YB0fZ9aj$d4*Bj$n4*Bj$t4*
hy|My
.*V"ffn$ +)* gm*
j$p4
[$ 4t(aa@[$4*Bj$n4*Bj$t74 j$ 4>= 9hOf[$p.4pyO[m(*BU9[[9m0
(*f[2(paZm90BnQf2(fNB>
= Ofo>a90Bpffac5P9S?(f>=JO\(? .:Mp],(

yOQffm9h(,W5





2MmW irt+i*8<2/Z-08&MC+T/2;*+s-G&9<RU/2RUC+i-43 82/& Y/2;0RN<Z/2;*+,&(8+ V e YJ/2;*+.8+F+fORN<2/<Z<21*):;3698:3 -0
; W
/2;*+.'=[g_;7ehdA>/2;*+.8+TRN<\3$tlS.4~H0+.82RUC93 /2RN&('=[),&('*<2RNH0+.82RU'06j/2;*+F),&(5N&(8+,H698:3 -0;*<\& Y3(<82105N+,<BA
" N * F6F6F * G <21*);y/2;43 j

/ b-08& 2+,)./</
& G eu^+,+/2;43 j
/ G )M3 '0'*&(/D+lH0+,HO1*),+,HIY[8& V $n+
*)7
4fW
&(/2;*+.82LRN<7
+ L&(105NHk3 5N<&DG+iH0+,HO1*).RUD05N+iY8& V $na
*)7
4fe|7+./l1*<l),&('*<2RNH0+.8>/2;*+*8<2r
/ Y8& V /2;0RN<
H0+.82RUC93 /2RN&('/2;43 /sRN<s'*&(/sH0+,HO1*).RUD05N+yY[8& V $na
*)7
4feg_;*+.p
' >RN<s&(D0/:3 RU'*+,HY[8& V =n3j698:3 -0;
\~H0+,HO1*).RUD05N+FY8& V !ADQX3 -0-05UXRU'06382105N+
' !e^RU'*),g
+ RN<
Yn&(5U5N&MLRU'06k3-08& 2+,)./2RN&(
H0+,HO1*).RUD05N+Y8& V $n+
*)
4fW/2;*+.'/2;*+.8++fORN<2/<T3698:3 -0; ~H0+.82RUC(3 D05N+Y[8& V <1*);/2;43
/
-08& 2+,)./<@RU'Q/& ke|+./@1*<@)M3 5U
5 <21*):;j3i-08& m+,)./2RN&('7W{3 '*H),&('*<2RNH0+.8@/2;*+Z-08& m+,)./2RN&(B
' " H
& Y/2;*+l;X-&(/2;*+,<2RN< /282RU696+.8>& Y/2;*+Z82105N+ ),&('*<2/28:3 RU'Q/
RU'Q/& ke@rt+Z'*&ML;43MC+Z/&s-08&MC+Z/2;43 /\`A
~$CRN&(5o3 /+,< W43 '*HA/2;0RN<JCRN&(5o3 /2RN&('IRN<'*&(/X\~$8+,<2/&(8:3 D05N+9e^10-0-&9<+\`A&(8ARN<_Y$3 5N<+9e!g_;*+.'
/2;*+.8+L&(105NH+fORN<2/q3698:3 -0; ~H0+.82RUC+,HlY8& V
<1*);/2;43
/ )M3 'ZDG++fO/+.'*H0+,HZ/&E3-08& 2+,)./2RN&('
_ & 3(<J3>L;*&(5N+ERU's/2;*+uRU828+,HO10'*H*3 '/Y[&(8 V & g[e!g_;0RN<_RN<3 D*<2108HW0<2RU'*),+ _ RN<3-08& m+,)./2RN&('I&
hRU'3Z698:3 -0;K~H0+.82RUC(3 D05N+@Y8& V e
g_;*+Z),&(10'Q/+.8+f*3 V -05N+\-08+,<:+.'Q/+,HSRU'x{RU6*e{`.dTRN<E<21 ).RN+.'/@/&i-08&C+Z/2;*+Z'*+.6Q3 /2RUC+Z-43 82/@& Y]/2;*+
/2;*+,&(8+ V e



(

F

0





1












2














r

F



1

82105N+



1

F

r

-&9<2RU/2RUC+@),&('*<2/28:3 RU'/

698:3 -0;

x{RU69108+\`.d*!),&(10'/+.8+f03 V -05N+/&g_;7e4b



/RN<uR VV +,HORo3 /+Z/&T);*+,):K/2;43 /@+.C+.82X698:3 -0;y/2;43 /@)M3 'SD+om * rn,~H0+.82RUC+,HyY8& V )M3 't3 5N<&
D+[m n,~H0+.82RUC+,HsY[8& V e!&L+.C+.8MW0/2;*+-08& m+,)./2RN&('F& Y/2;*+/282RU696+.8J& RU'Q/&l/2;*+10'0RNwQ1*+u'*&H0+&
H0+f*'*+,<3ZCRN&(5o3 /2RN&('& /2;43 /LRU5U57'*+.C+.8JD+E8+,<2/&(8+,He



$



r +@LRU5U5}<2/21*HOXTRU'I/2;*+@'*+fO/J<+,)./2RN&('k=[g_;7e7`9`AJ3Z-43 82/2RN).105o3 8)M3(<+>& Y{82105N+,<L;*+.8+@/2;*+>),&('C+.8<+

& Y!g_;7e4blRN</2821*+9e

)
|
p 8q < Dh DK*}{$Sp!,fZM!Q2,[fU5+X9JG
2D*4(GP2\*|(2[f;:aQ.[QQfNW5
f Y}hy|Mh{
hy|My

^


hy

AA |{Ay

] fiM}

$ [}









fi3MfiuM%



2MmW lD7RU'*).5U1*H0+,<@2 /2;Q1*<@2Da~H0+,HO1*)./2RN&('RN<E'*&(/@H0+,).RNH*3 D05N+9er;*+.'j RN<EH0+,HO1*).RUD05N+ZY[8& V
W]3D08+M3(HO/2;O~n*8<2/\<+M3 8):;& Y/2;*+s/28+,+F& Yu3 5U5H0+.82RUC(3 /2RN&('*<lY[8& V Wq+M3();698:3 -0;D+.RU'06);*+,):K+,H

Yn&(8P),&('*<2RN<2/+.'*).XW{+.'*<2108+,<P/2;43 /k G RN<@Y[&(10'*HRU't*'0RU/+/2R V +9ex*&(8>\!W7L+<2;*&L/2;43 />):;*+,)KRU'06
),&('*<2RN<2/+.'*).XsRN<_/282105UXs10'*H0+,).RNH*3 D05N+9e]|7+./JD+@3@L;*+.8+u),&('Q/:3 RU'*<3l-&9<2RU/2RUC+E),&('*<2/28:3 RU'/ 9Q
3 '*Hi3P'*+.6Q3 /2RUC+),&('*</28:3 RU'Q/ WQD&(/2;LRU/2;T3 's+ V -0/X/282RU696+.8Meqx*&(8-08&MCRU'06l),&('*<2RN</+.'*).XW&('*+;43(<
/&i-08&MC+Z/2;43 / SP $n+
*)
4fW3 '*H/2;*+3 5U6&(82RU/2; V H0&+,<u'*&(/E'*+,),+,<<:3 82RU5UX<2/&(-yRU'y/2;0RN<u)M3(<:+T=pY[8& V
<+ V R~H0+,).RNH*3 D0RU5URU/X& YPH0+,HO1*)./2RN&('RU'TAfeg_;*+<:3 V +I;*&(5NH0<Y[&(8/2;*+),& V -05N+ V +.'Q/:3 82X-08&(D05N+ V
=-08&MCRU'06RU'*),&('*<RN<2/+.'*).X0A_/:3 KRU'06 Q RU'*</+M3(HI& W*;*+.'*),+@/2;*+@10'*H0+,).RNH*3 D0RU5URU/Xe



@

.

!

6





u<s36+.'*+.8:3 5UR 3 /2RN&('& Y>2\!WJH0+,HO1*)./2RN&('RU'CD7RN<s/282105UX10'*H0+,).RNH*3 D05N+9e#u+f/s<+,)./2RN&('
<2/21*HORN+,<_3@H0+,).RNH*3 D05N+Y8:3 6 V +.'/& YsDqWL;0RN):;iRU'\-43 82/2RN).105o3 8_L_3(<<21 ).RN+.'/]Yn&(8q/2;*+P*{*77,:
V &H0+.5UR 3 /2RN&('7e

6

2

M0#v##







isq# 2




kvfi#0"v

P

z u

28 105N+F3 -0-05URN)M3 /2RN&(' V 3MX3(H0H8+,HO10'*H*3 '/\RU'OYn&(8 V 3 /2RN&('/&t3698:3 -0;7e '6+.'*+.8:3 5$WH0+./+,)./2RU'06t8+f~
HO10'*H*3 '*).XSRN<@HOR ).105U/i=8+,)M3 5U5]H0+./+.8 V RU'0RU'06L;*+./2;*+.83s698:3 -0;RN<@8+,HO10'*H*3 'Q/RN<P3 'B ~),& V -05N+./+
-08&(D05N+ V AfWD010/E/2;*+.8+3 8+Z<& V +l/282RUCRo3 5!)M3(<+,<MWGL;0RN):;L+lLRU5U5h6+./u82RNHS& Y2W<2RU'*),+l/2;*+.X V 3,Xy).8+M3 /+
3 82/2R4).Ro3 5U5UXRU'O*'0RU/+ZH0+.82RUC93 /2RN&('*<MeEx{RU8<2/MWa&('*),+382105N+>;43(<uD+,+.'S3 -0-05URN+,Hy/&T3\698:3 -0;3(),),&(8HORU'06F/&
3F69RUC+.'k-08& 2+,)./2RN&('7W!RU/)M3 'kDG+s3 -0-05URN+,Hk3 6Q3 RU'k/&/2;*+8+,<2105U/2RU'06y698:3 -0;7Wq3(),),&(8HORU'06y/&/2;*+s<:3 V +
-08& 2+,)./2RN&('7Wq3 '*Hj/2;0RN<lRU'*H0+f*'0RU/+.5UXeyg_;*+,<+\Y[1082/2;*+.83 -0-05URN)M3 /2RN&('*<&(DQCRN&(1*<25UXt-08&HO1*),+8+,HO10'*H*3 '/
RU'OYn&(8 V 3 /2RN&('7eg_;*+.X3 8+l<:3 RNHI/&DG+QMfNB:.e'*&(/2;*+.8u)M3(<+& Yq/282RUCRo3 5{8+,HO10'*H*3 '*).XRU'S3698:3 -0;yRN<
/2;43 /q& Y} 9!8+.5o3 /2RN&(''*&H0+,<MW(R$e+9e{LRU/2;\+f03()./25UX/2;*+_<:3 V +_'*+.RU69;QD&(8<!RU'Z/2;*+<:3 V +_&(8H0+.8Meq%&('*<2RNH0+.8
Yn&(8@RU'*<2/:3 '*),+\3s82105N+& YKRU'*H R
$M]*j_4/2;*+.B
' $M]*b_4 :eZg_;0RN<@82105N+)M3 'tD+Z3 -0-05URN+,HtRU'*H0+f*'0RU/+.5UXW
+.C+.'SRY]1*<+.5N+,<<E3 -0-05URN)M3 /2RN&('*<>3 8+Z3,C&(RNH0+,HWD010/P3 5U5!3 -0-05URN)M3 /2RN&('*<P).8+M3 /+l/LRU'S8+.5o3 /2RN&('S'*&H0+,<e '
L;43 /{Y[&(5U5N&LJ<MW(L+),&('*<RNH0+.8{/2;43 /h/2;*+),&('*<2/2821*)./2RN&('\& Y4/2;*+698:3 -0;Z8+,<2105U/2RU'06uY8& V 382105N+3 -0-05URN)M3 /2RN&('
-08+.C+.'Q/<J/2;*+6+.'*+.8:3 /2RN&('I& Y/LRU'T8+.5o3 /2RN&('F'*&H0+,<MW03 '*Hi/2;43 /J3>H0+.82RUC93 /2RN&('IH0&+,<'*&(/_),& V -082RN<+@3 'QX
1*<+.5N+,<<J82105N+>3 -0-05URN)M3 /2RN&('7e
?uRUC+.'j3T<:+./E& Y82105N+,< 3 '*Ht3 '~H0+.82RUC(3 /2RN&('5N+M3(HORU'06I/&F3I^0? kW
RN<@<B3 RNHS/&iD+F.U ,2
RY'*&82105N+,<& YX )M3 'DG+T3 -0-05URN+,Hk/&
RU'3 '&(82RU69RU'43 5L3MXWqR$e+9e3 5U5J3 -0-05URN)M3 /2RN&('*<\& Yu3 'Xt82105N+
& YX &('
3 8+i1*<+.5N+,<<e&(8+iYn&(8 V 3 5U5UXW
RN<Z).5N&9<+,HkLPe 8Me /Me 3 '*HkLPe 8Me /Mej3 '\~H0+.82RUC93 /2RN&('
N FtFtF G G " kWL;*+.8+ !,= IE\+\ K AhRN<q/2;*+J698:3 -0;s&(D0/:3 RU'*+,HDX/2;*+3 -0-05URN)M3 /2RN&('T& Y3E82105N+
& Y&('
' W0RYhYn&(8+.C+.82XT82105N+ & YF W0Yn&(8+.C+.82XF-08& 2+,)./2RN&('
3(),),&(8HORU'06i/&\/2;*+P-08& 2+,)./2RN&(
Y8& V JON L RU'Q/& kW/2;*+.8++fRN<2/<@3i-08& 2+,)./2RN&(B
' qY[8& V JON L /&
,= I8\pY\ K AfW<1*);S/2;43 /
g"Q e
?uRUC+.'I3<+./_& Y82105N+,<@ 3 '*Hs3698:3 -0g
; ZWRY}3l).5N&9<+,Hi698:3 -0;sRN<@~H0+.82RUC(3 D05N+uY[8& V W/2;*+.'TRU/RN<
10'0RNw1*+9eq&(8+,&C+.8MWORY}/2;0RN<_698:3 -0;FRN<H0+.82RUC93 D05N+ELRU/2; 82105N+P3 -0-05URN)M3 /2RN&('*<W0/2;*+.' yRN<_/2;*+ V 3OR V 3 5
5N+.'069/2;& Y!3 'U\~H0+.82RUC93 /2RN&('7W43 '*HI3 5U57H0+.82RUC(3 /2RN&('*<& Y}5N+.'069/2; S5N+M3(HT/&ZRU/Mehr;*+.'FRU/J+fRN<2/<WL+@)M3 5U5
RU//2;*+.U fl&
LPe 8Me /MeWOL;0RN):;L+@'*&(/C
+ [ e
|7+./h1*<h3 5N<&uH0+f*'*+_3 '*&(/2;*+.8h'*&(/2RN&('7W(8+.5o3 /+,Hl/&/2;*+]Y$3()./{/2;43 /}L+_3 8+RU'Q/+.8+,</+,HRU'lRU828+,HO10'*H*3 'Q/
698:3 -0;*<Me '/2;0RN<{-+.8<2-G+,)./2RUC+9W(5N+./{1*<h<:3,X>/2;43 /!3 'ZRU828+,HO10'*H*3 '/!698:3 -0;
RN<afp*LPe 8Me /Meh3u<+./h& Y482105N+,<
RY!+.C+.82XT698:3 -0;I/2;43 /)M3 'FD+P&(D0/:3 RU'*+,HFDXI3 -0-05UXRU'06s&('*+@& Yh/2;*&9<:+E82105N+,<&('
RN<J+,w10RUC(3 5N+.'//&
ke}u<<21 V RU'06@/2;43 +
/ RN<q3 'RU828+,HO10'*H*3 '/]698:3 -0;3 '*HZ/2;43 /q698:3 -0;*<]&(D0/:3 RU'*+,HDQXZ3u82105N+J3 -0-05URN)M3 /2RN&('
3 8+@-010/RU'/&\RU828+,HO10'*H*3 'Q/Y[&(8 V W*RY!3Y[105U5698:3 -0;)M3 'FD+@H0+.82RUC+,HFY[8& V /2;*+.'IRU/RN<J10'0RNwQ1*+9e
'OY[&(8 V 3 5U5UXW/2;*+Z'*&(/2RN&('j& Y_3T).5N&9<:+,H698:3 -0;/28:3 '*<25o3 /+,<P/2;*+lY$3()./@/2;43 /@'*&(/2;0RU'06)M3 'D+\3(H0H0+,H
/2;43 /u;43(<'*&(/uDG+,+.'S3 5U8+M3(HOX3(H0H0+,HWGL;*+.8+M3(<u/2;*+P'*&(/2RN&('& Y3ZY[105U5698:3 -0;S<:3,XO</2;43 /'*&(/2;0RU'06T)M3 '

e



v





@









v

*







+





0

fi M]|9M.tM=uwuMM3
F

D+\3(H0H0+,Ht/2;43 /P8+M3 5U5UXt3(H0H0<@'*+.LRU'OY[&(8 V 3 /2RN&('t/&I/2;*+698:3 -0;7e\r;*+.'t/2;*+\).5N&9<2108+& YJ3s698:3 -0;
+fORN<2/<MWO/2;*+.'/2;*+PRU828+,HO10'*H*3 'Q/Y[&(8 V & Y!/2;0RN<J).5N&9<108+PRN<J+f*3()./25UXT/2;*+EY105U5698:3 -0;H0+.82RUC93 D05N+EY8& V Ze
10/'*&(/+/2;43 /uL;*+.'/2;*+>Y105U5}698:3 -0;+fRN<2/<W*/2;*+l).5N&9<2108+H0&+,<'*&(/'*+,),+,<<B3 82RU5UX+fRN</<l=[<+,+>-08&&
& 8&(-7e7`MAfe
h

}hAy

.



}

9

]{%}$}

yA

6

g_;*+'*&(/2RN&('s& Y3Y[105U5*698:3 -0;\D+.RU'06 V &(8+6+.'*+.8:3 5a/2;43 '\/2;*+'*&(/2RN&('i& Y73@).5N&9<2108+9WQL+J)M3 '6+.'*+.8:3 5UR M+
/2;*+H0+f*'0RU/2RN&('& Y*'0RU/++fO-43 '*<2RN&('k<+./<P1*<+,HjRU'k3I-08+.CRN&(1*<-43 -G+.8T=[3 6+./l 1069'0RN+.8MW!99O`AfW
3 '*H3(H0&(-0/J/2;*+@Y[&(5U5N&LRU'06&('*+9

fi;}]} }]Ayy .h{%}hM}[AyATJ M.JmlBNBE o(U2F*'0RU/+Z+fO-43 '*<2RN&('<+./
>G= .(S? fS:}pC=Of)j9o.Z(K!Q.BM?9[n( F6F6F B0f9_fL$ 4p]f9+5N65oc5E5
e fa9ZS: [ Qp!B4(mf4 5


RN<P3i*'0RU/+Z+fO-43 '*<2RN&('t<+./\=pYme+9e<fAfWH0+,HO1*)./2RN&('tRU'S D+,),& V +,<PH0+,).RNH*3 D05N+=D010/ERU/@RN<u'*&(/
3 G:B:,(S:t),&('*HORU/2RN&('jYn&(8>H0+,).RNH*3 D0RU5URU/X*Afe '*H0+,+,HWRU'k&(8H0+.8>/&IH0+./+.8 V RU'*+iL;*+./2;*+.83^0? RN<

H0+,HO1*).RUD05N+sY[8& V 3y@ $n+*)4fWqRU/<21 ),+,<Z/&S),& V -010/+I [ W!/2;*+.'/&S);*+,):K/2;*+T+fORN<2/+.'*),+T& YE3
-08& 2+,)./2RN&('FY8& V /&\ [ e^R V RU5o3 825UXW*),&('*<2RN</+.'*).XT):;*+,)KO<JRU'I2\t3 8+PH0&('*+P&(' [ e



k

<

}hAycy .]{%}$} Ay|AJ . P " $na*)*m.4knbX 9hOfB pS<q*[
)jB0(0Bn(y,,c5kU0fs pP:(0Bpf.4q V$m [ n *m4@p>(0fo.f4M=(Gi8}9t:ZQ2(0:2
fm( $na*)7>
4 (jlQ2(0:2PB2( $m [ n64 5
yn







g_;0RN<-08&(-+.82/XI3 5U5N&LJ<J1*</&Z-08&C+@/2;43 /J/2;*+P),&('C+.8<+P& Y!g_;7e*blRN</2821*+EL;*+.'KRN<J8+,<2/282RN)./+,H
/&\3l*'0RU/+P+fO-43 '*<2RN&('<:+./Me

p

hy|My , " $n+*)g*m4IBnbX=Y9hOf~ osk<]*FjBO(0f[9,.c5UOft p
a(0fo.f4l Ofj)j(pf } YB0fO9>j$4*B[$n`4*B[$t4*Bj$p4 j$Y4S(Gka9
j$d
4*Bj
$n
4*Bj
$t4 [$ 4>=9hO.2j$p.4>pOZ[2(0f[[(j2O:(0f[2(4Z2J(0f[Q.22
@fNB6 5



0

=

2MmWj$74;*&(5NH0<u3(<u3-43 82/2RN).105o3 8E)M3(<+P& Y]g_;7e4bOe|7+./1*<J'*&L-08&C+P/2;*+ $74_-43 82/Me^RU'*),+>RN<
RU'*),&('*<2RN<2/+.'/MWQ/2;*+J-08+.CRN&(1*<]-08&(-G+.82/X3(<:<+.82/<!/2;43 /H$m [ n m* h
4 RN<!RU'*),&('*<2RN</+.'Q/Me!g_;7eE+.'*<108+,<!/2;43 /
j$ J4fe
/2;*+.8+_+fRN</<{3698:3 -0; <21*);l/2;43 /`A}j$ d4B* jn$ [ 4B* jp$ 4 Ij$ J4fW93 '*HA}j$ d4B* jn$ [ 4
^RU'*),+ij$ d4B* jn$ 4B* jt$ 4 jn$ [ Z
4 =[g_;7e7dAfW7L+\&(D0/:3 RU'kj$ d4B* jn$ 4B* jt$ 4B* [p$ .4 j$ 4fe|7+./
1*<E'*&ML<10-0-G&9<+/2;43 /j$ d4B* jn$ 4B* jt$ 4 jj$ J4fW73 '*HS-08&MC+Z/2;43 /ERU/ERN<P3 D*<2108He 'y/2;43 /P)M3(<+9W
/2;*+.8+PL&(105NHID+>3Z698:3 -0;J \~H0+.82RUC93 D05N+@Y[8& V <21*):;I/2;43 /
-08& 2+,)./<JRU'/&Z =[g_;7e4di3 6Q3 RU'aAfe
[
[
'*HF<2RU'*),+
P P WOL+P<;*&(105NHF;43,C+[$ 4B* jn$ 4 I[$ 4@=[g_;7e`Afh/2;0RN<JRN<3 D*<2108He







.





(







&(8+E6+.'*+.8:3 5U5UXW4&('*+@&(D0/:3 RU'*</2;*+@Y[&(5U5N&LRU'06H0+,).RNH*3 D0RU5URU/XF8+,<2105U/<W0H0+.-G+.'*HORU'06&('FL;*+./2;*+.8XW

D]W0&(8fifflmDtRN<3*'0RU/++f-43 '*<RN&('I<+./Me


uq

yn

<

$

9.

fi;}hAyy

h{%}hM}

yAy|A

e 0f pZ@W5W5=Q2(0,[n(2 p2.nQfN=E:(0Bpf.aL:(aQ2(0,[n(p\
V
92ZQ.[QQfN=_Q(OM[[(2CDoE,.lM!2.nQfNW5

;



fi

3MfiuM%

e 0fDoIW5W5=lQ29O,[n(tDpT2.nQfN=ifOPf\(p*T[f;:taQ2,[fU
V
2CDa5

V es0f*lmDo>W5pW5U>=]Q(OM[[(2CDo>Q2,[fU5
2MmW]^10-0-&9<+E

NR <_3EY2e+9e<MehE+,).RNH*3 D0RU5URU/Xs& Y7-08&(D05N+ V <RU'i 3 '*Hi\FYn&(5U5N&MLJ<Y[8& V -08&(-+.82/X
cOe 'I2CD!W*L;*+.'/2;*+3 '*<2L+.8RN<(fX+,<%(W*RU/)M3 'DG+>&(D0/:3 RU'*+,HIRU'I*'0RU/+>/2R V +9*L+P-08&),+,+,H3(<Yn&(8
D7=[<:+,+E-08&& Y!& Y!/2;*+,&(8+ V `MAD010/),&('*<2RN</+.'*).XT):;*+,)KO<3 8+PH0&('*+&('F/2;*+EY[105U57698:3 -0;IRU'*<2/+M3(HI&
/2;*+@698:3 -0;IRU/<:+.5Yme
u&MLPW<210-0-G&9<:+DRN<F3tYme+9e<Me \ +fORN<2/<MWJ/2;Q1*<T/2;*+yH0+.82RUC(3 /2RN&('/28+,+yRU'DRN<*'0RU/+9Wu3 '*H
),&('*<2RN<2/+.'*).X);*+,):K< V 3,X&('05UX).10/I<:& V +S-43 82/<F& Yl/2;0RN<T/28+,+9eE+,HO1*)./2RN&('RU'sD8+ V 3 RU'*<
10'*H0+,).RNH*3 D05N+lDG+,)M3 1*<:+>L;*+.'p
" OWa&('*+l&(D0/:3 RU'*<u/2;*+>2\ V &H0+.5$WaRU'L;0RN);H0+,HO1*)./2RN&('SRN</282105UX
10'*H0+,).RNH*3 D05N+9e
x{RU'43 5U5UXWRYRO
l9DSRN<3EY2e+9e<MeUW [\ +fORN<2/<MW/2;Q1*</2;*+H0+.82RUC93 /2RN&('T/28+,+JRN<]*'0RU/+9W3 '*Hs),&('*<2RN<2/+.'*).X
);*+,):K< V 3,XF&('05UXT).10/J-43 82/<& Yh/2;0RN</28+,+9e
u&(/+/2;43 /_/2;*+E),&('*HORU/2RN&(' p
l~DSRN<3P*'0RU/+u+fO-43 '*<2RN&('T<+./ RN<_<2/28&('06+.8_/2;43 ' DG&(/2;m3 '*HmD
3 8+E*'0RU/+P+fO-43 '*<2RN&('<:+./< :e!g_;*+uY[&(5U5N&LRU'06-08&(-+.82/Xlm1*<2/2R4+,</2;0RN<),&('*HORU/2RN&('7







5

pS29
G2BB:,(B;:TQ.[QQfNW5
yn



!

5



!

K*}{$pFa9

(aBD (C<q*[yjBO(0f[9M.>=\O.k2CD

$D vv$|
] AD

2MmW rt+@D010RU5NH3Z8+,HO1*)./2RN&('Y8& V }

N 7 t.{ =[g_;Q1*+9W}`MbO`.dA
/&lsDG hW*L;*+.8+E/2;*+u&(D0/:3 RU'*+,HT82105N+E<+./<@3 '*HmDj3 8+uDG&(/2;s*'0RU/+E+fO-43 '*<2RN&('F<+./<Me
g_;0RN<E8+,HO1*)./2RN&('S8+.5URN+,<@&('S/2;*+l&('*+lD010RU5U/uYn&(8-08&CRU'06F/2;*+l<+ V R~H0+,).RNH*3 D0RU5URU/X& Y]2\~
=/2;*+,&(8+ V vAfe
|7+./Z1*<*8<2/Z-08+,<+.'//2;*+i/L&yKRU'*Hk& Y*'0RU/+s+fO-43 '*<2RN&('<+./<1*<+,HkRU'/2;0RN<l8+,HO1*)./2RN&('7eDRN<
3*'0RU/++f-43 '*<RN&('S<:+./E<RU'*),+l&('05UX8+.5o3 /2RN&(''*&H0+,<P3 8+l-08+,<+.'/@RU'S/2;*+),&('*).5U1*<2RN&('t& Y]82105N+,<MEDRN<
RU'*H0+,+,HF3P-43 82/2RN).105o3 8J)M3(<:+& Y78:3 '06+f~$8+,<2/282RN)./+,HI82105N+,<E=[<:+,+ 8&(-7e`9`AfeFRN<_3 5N<&Z3P*'0RU/+u+fO-43 '*<2RN&('
<+./P<2RU'*),+9WYn&(8P+.C+.82Xy82105N+\RU'W/2;*+Z;QX-G&(/2;*+,<2RN<>RN<PHORN<),&('0'*+,)./+,HjY8& V /2;*+),&('*).5U1*<2RN&('=L+\)M3 5U5
/2;*+,<+u82105N+,<@(p.(4G2,2*Af
e u&(/+/2;0RN</2R V +u/2;43 /MWO/2;*&(1069;* RN</282RUCRo3 5U5UXT3@Y2e+9e<MeUW/2;*+E).5N&9<2108+E& Y{3
698:3 -0;ILPe 8Me /MeH0&+,<J'*&(/J'*+,),+,<<:3 82RU5UXF+fORN<2/Me
+,)M3 5U5@/2;*+}

/:3 K+,<3(<TRU'0-010/F/L&L&(8H0
< 3 '*
H 7@3 '*H3<:+./T& Yl82105N+,<
" * F6F6F * G nQWh+M3();k82105N+/ D+.RU'063F-43 RU8& YJL&(8H0Z

< $ ,* c4fW!3 '*H3(<2KO<PL;*+./2;*+.8Z/2;*+.8+\RN<Z3
H0+.82RUC93 /2RN&('\Y[8& V /j
& 7[ehg_;*+.8+JRN<]3 'iR VV +,HORo3 /+H0+.82RUC93 /2RN&('\Y[8& V /j
& 77=L+'*&(/d
+
7NA
RY2WhY[&(8Z<& V +s Q`
W " W 3 '*H " 6 et Q.BM ?9[n(Y[8& V /
& =L+s'*&(/+

ARN<3Z<+,w1*+.'*),j
+ " N


" e
F6F6F
rt+;43MC+\<2;*&L't;*&ML/2;0RN<P-08&(D05N+ V )M3 'D++f-08+,<<:+,HRU'/2;*+ V &H0+.5$@/&I3FL&(8J
H "
] F6F6F ] RN<3(<<&).Ro3 /+,H/2;*+u698:3 -0;I

$M4fW03 '*Hi/&Z3 'QXi82105N+E7

"ff$M_ F6F6F _ *
W
!
4
N
R

<
(
3

<

<

&
).Ro3 /+,H


F6F6F
G
/2;*+698:3 -0;82105N+ $.
4fW{3(<>8+.-08+,<+.'Q/+,HRU'jx{RU6*eq`Mvt=L;*+.8+
RN<P698+M3 /+.8/2;43 'k3 5U5]&(/2;*+.8l),&('*),+.-0/
/X-+,<BAfe!g_;*+.
'
Rt
$M 4 P $n
$M4* $`
4y4>=[<:+,+E-08&& Y!& Yhg_;7eavAfe
|7+./F1*<s'*&L <2-05URU/F+M3();&(D0/:3 RU'*+,H82105N+ $)
4RU'/&&('*+yHORN<:),&('0'*+,)./+,HRU'OY[+.8+.'*),+S82105N+p
$.
4
3 '*H&('*+>8:3 '06+f~$8+,<2/282RN)./+,HS+.C&(5U10/2RN&('82105N+++
$)
4feJrt+>HORN<2/2RU'06910RN<2;RU'/2;*+>;QX-G&(/2;*+,<2RN<u& Y+
$)
4/L&
<210D0698:3 -0;*<MZ/2;*+y(f(4W}L;0RN);),&(828+,<2-&('*H0<>/&/2;*+\;X-&(/2;*+,<2RN<l& $)
4fWq3 '*Hj/2;*+yBf[a9[n(4W
L;0RN);),&(828+,<-G&('*H0<F/&k/2;*+S),&('*).5U1*<2RN&('& Y(
$)
4fe /TRN<T+M3(<2X/&k):;*+,)K/2;43 /I&('*+y-43 82/I& Y>/2;*+
3 D&MC++,w10RUC(3 5N+.'*),+t</2RU5U5u;*&(5NH0<

$M 4 P $n

$M4*)
$
4Hl+
$
4y4feu&ML+.C+.8MW
/2;*+),&('QC+.8<:+tRN<F'*&5N&('06+.8IC93 5URNH);*+,):KDQX+fO+ V -05N+t/2;43 /MWRY/ "
" $ *3W4>nQW@L+;43MC+

K* {

$D *|{9



|

ff

C

` ]*

ff

ff



2

$-

>* {

ff

2

;



fi M]|9M.tM=uwuMM3
F

$M4

B

$)4
u















%



















E

u



u

Origin








D*v$|
RU'/&






















$.4
D+$.4
u




Destination

x{RU69108+\`MvOqg8:3 '*<mY[&(8 V 3 /2RN&('*<Y[8& V /2;*+}



V &H0+.5N<& Y{/2;*+E_Yn3 V RU5UX

+

Z=n3 -0-05UXm$)4&('*),+9WO/2;*+.'F/L&Z/2R V +,<,D+$.41*<2RU'06
$ W4 P $n$ 4*)$`4)l/D$4y4/2;*&(1069;


HOR+.8+.'Q/'*&H0+,<gT\D010//2;*+@<:3 V +@'*&H0+ByAOAfe
rt+/2;Q1*<{'*+,+,H/2;*+]'*&(/2RN&('Z& Ya3M3 -0-05URN)M3 /2RN&('& Ya382105N+2D+$)4fRU/}RN<{<21*);l/2;43 /{/2;*+]-08& 2+,)./2RN&('
& Y]RU/<EH0+,<2/2RU'43 /2RN&('-43 82/ERN<@3 V 3 -0-0RU'06F/&F3i<210D0698:3 -0;S/2;43 /EL3(<@&(D0/:3 RU'*+,HSDQXy3 -0-05UXRU'06F/2;*+82105N+
$.
4fW03 '*H/2;43 /L3(<'*+.C+.81*<+,H\/&-08& m+,).//2;*+uH0+,<2/2RU'43 /2RN&('T-43 82/& Y}3 'X\82105N+& Y{
$
4fWRU'*).5U1*HORU'06
D+$.
4_RU/<+.5Y2e]y&(8+,&MC+.8MWa/2;*+P&(82RU69RU'y3 '*HH0+,<2/2RU'43 /2RN&(' V 1*<2/D+E-08& 2+,)./+,HIRU'Q/&iHORN<[2&(RU'Q/-43 /2;*<l=R$e+9e
3S'*&H0+I& YE/2;*+I&(82RU69RU')M3 '0'*&(/i;43,C+F/2;*+<:3 V +FR V 3 6+F;43(<s3S'*&H0+F& Y@/2;*+IH0+,</2RU'43 /2RN&('aAfe YuL+
8+,<2/282RN)./&(108<+.5UC+,<_RU'F<& V +uL3MX/&l6&&HT3 -0-05URN)M3 /2RN&('*<J& Y}82105N+,<_& YT+
$
4fW/2;*+.'TL+@)M3 'TC+.82RY[X\/2;43 /
$M 4 P $n

$M4* $
4y4R
$M 4 P $n
$M4*)
$
4l*+
$
4y4fe
g_;0RN<8+,<2/282RN)./2RN&('RN<&(D0/:3 RU'*+,HDXT1*<2RU'06i),&('*<2/28:3 RU'/<MWa/2;43 /LRU5U5{3 5U5N&L+.C+.82XF6&&H3 -0-05URN)M3 /2RN&('
& Y>382105N+I& Y9
$
4fW3 '*HD+FCRN&(5o3 /+,HDXk/2;*+&(D0/:3 RU'*+,H698:3 -0;&(/2;*+.82LRN<+9e|7+./i1*<\'*&(/+K $`
4
3 '*HD $`
4E/2;*+'*+.L<+./<& YJRU'OY[+.8+.'*),+82105N+,<l3 '*H+.C&(5U10/2RN&('k82105N+,<eFg_;*+\'*+.L/28:3 '*<2Y[&(8 V 3 /2RN&('kRN<
H0+,<).82RUD+,HRU'x{RU6*e`MOe /i3 5U5N&LJ<T/&j&(D0/:3 RU'/2;*+IYn&(5U5N&MLRU'068+,<2105U/M
7
$M74 P

$n $M4*) $
4*)D $`
4*Lm Q * n4fe!g_;*+J'43 V +,<_& Y78+.5o3 /2RN&('s/X-G+,d
< "ZW 3 '*H
;43,C+uD+,+.'i):;*&9<+.'
/&Z69RUC+>3 'IRU'/210RU/2RUC+PRNH0+M3Z& Yh/2;*+.RU8J8&(5N+@D010/J/2;*+.XF3 8+_m1*<2/_/X-G+,<3(<J&(/2;*+.8<Meq8+.5o3 /2RN&('I'*&H08
+ $i 4
Y[8& V 3T'*&H0
+ 6/&I3F'*&H0
+ ; V +M3 '*<>/2;43 /P/2;*+\5N+./2/+.8 ;43(<>DG+,+.'&(D0/:3 RU'*+,HtDQXt3 -0-05UXRU'06
/2;*+@82105N+ e!8+.5o3 /2RN&('I'*&H08
+ $ 4]Y[8& V G /
& V +M3 '*<J/2;43 /J/2;*+E5N+./2/+.
8 _ G D+.5N&('06</&Z/2;*+
<210DL&(8H&('L;0RN);\/2;*+82105N+, G;43(<!DG+,+.'s3 -0-05URN+,Ha
e "RN<h1*<+,H/&ERU'*HORN)M3 /+/2;43 /q/L&>),&('*),+.-0/q'*&H0+,<
;43,C+/&PD+-08& 2+,)./+,H&('\/2;*+<:3 V +'*&H0+>=RU'T%?/+.8 V <MWL+JL&(105NHs<+,+RU/3(<3@),& ~$8+fYn+.8+.'*),+5URU'0K*Afe
g_;*+Z+.C&(5U10/2RN&('82105N+~.
0$ c4fW<2/:3 82/2RU'06sY8& V 3i-43 /2;S8+.-08+,<+.'Q/2RU'06/2;*+l<10DQL&(8H 1*<+,Hy/&T3 -0-05UX
_3 '*HY8& V /2;*+l8+.-08+,<+.'Q/:3 /2RN&('j& ]6+.'*+.8:3 /+,HSDXB $ c4fW-08&HO1*),+,<u/2;*+/L&T8+.5o3 /2RN&(''*&H0+,<
/X-+,HKP<2R V 105o3 /2RU'06/2;*+P3 -0-05URN)M3 /2RN&('& $ 4fWO/2;Q1*<, W03 '*HF/2;*+E8+.5o3 /2RN&('I'*&H0+,<_/X-G+,H
L;0RN):;
V 3 82KF/2;*+8+.-08+,<+.'Q/:3 /2RN&('& ]3(<1*<+,HDQX3 'S3 -0-05URN)M3 /2RN&('& YT 2eg_;*+>'*+.6Q3 /2RUC+l),&('*<2/28:3 RU'Q/
-08+.C+.'Q/<_3 'i3 -0-05URN)M3 /2RN&('T& YD $ 4hRU'\L;0RN);i/L&>'*&H0+,<& Y/2;*+J&(82RU69RU'i3 '*HH0+,</2RU'43 /2RN&('-43 82/<q;43MC+
/2;*+P<:3 V +PR V 3 6+=n3Z'*&H0+@'*+,),+,<<:3 82RU5UXF&(D0/:3 RU'*+,HIDXs<:& V +>3 -0-05URN)M3 /2RN&('y& Yh/2;*+@82105N+ $ c42Af0L;0RU5N+
/2;*+\-&9<2RU/2RUC+s),&('*<2/28:3 RU'Q/ Q -08+.C+.'/<l<1*);3I<210D0698:3 -0;k/&ID+\1*<+,Hj/LRN),+Y[&(8Z3 -0-05UXRU'06pD $ c4
LRU/2;HOR+.8+.'/@-08& m+,)./2RN&('*<@& YRU/<@&(82RU69RU'7RU/E<:3,XO<u/2;43 /ERU'y/2;0RN<@)M3(<:+9Wa/2;*+l/L&T-08& 2+,)./2RN&('*<@& Y]/2;*+
&(82RU69RU'*< V 1*<2/JD+E/2;*+P<:3 V +9e



U4

4















h

{%}$My

yn A'Ay




Uff

*



fi



`



ff








C

0ff
C







y|

|7+./_1*<_'*&MLYn&).1*<&('T/2;*+u82105N+,<_/2;43 /L+.8+E1*<:+,Hi/&Z<&(5UC+u/2;*+G.{*7.:-08&(D05N+ V eqD0RN),&(5N&(8+,H
698:3 -0;=82105N+&(8]),&('*</28:3 RU'Q/BA{RN<q<B3 RNHZ/&EDG+E2(QB.[Bn,2=8Me 8Me
APRYRU/<!<+,),&('*H-43 82/=[),&('*).5U1*<RN&('i&(8
&(D05URU6Q3 /2RN&('aA_H0&+,<_'*&(/),& V -082RN<+P3 'QXs6+.'*+.82RN)@),&('*),+.-0/'*&H0+9eqrt+1*<:+/2;0RN<+fO-08+,<<2RN&('FDXs3 '43 5N&(69X

7

$-

fi3MfiuM%



$M4

B


$ 4




=




=













#






=



=

4
=

x{RU69108+\`MOJ+,HO1*)./2RN&('TY8& V

u

=




E



=

u
u

















=

u

























=

#





u











#



Q

=



u



u




D.0$ c4







/2;*+}Dvv$|
/&2CDK*}{$hW*LRU/2;*Dj3 '*HmY2e+9e<Me








LRU/2;T/2;*+<:& ~)M3 5U5N+,Hi82105N+,<_RU'T@3 /:3 5N&(6*WL;*+.8+E3 5U5C93 82Ro3 D05N+,<_& Y/2;*+u;*+M3(H V 1*<2/J3 -0-G+M3 8_RU's/2;*+D&HOX
=[D0RU/+.DG&(105E+./s3 5$eUW@`Mb9b9vAfe^1*);82105N+,<T3 8+y3 5N<&j)M3 5U5N+,H.2,RU'/2;*+5URU/+.8:3 /2108+9e%&('*<2RNH0+.8Yn&(8
RU'*<2/:3 '*),+@/2;*+P82105N+,<J& Yhx{RU6*e4vO 3 '*H 3 8+@8:3 '06+@8+,</282RN)./+,HW0L;0RU5N+ RN<'*&(/Me
5N<&S'*&(/2RN),+I/2;43 /s3S8:3 '06+F8+,<2/282RN)./+,H82105N+ )M3 'D+FH0+,),& V -G&9<:+,HRU'Q/&3 '+,wQ10RUC93 5N+.'Q/s<+./
& Y]82105N+,<
7
$ 4LRU/2;+f*3()./25UX&('*+l'*&H0+lRU'S),&('*).5U1*<2RN&('=[+.RU/2;*+.8P3 'SRU'*HORUCRNHO143 5!),&('*),+.-0/@'*&H0+&(8P3
8+.5o3 /2RN&('S'*&H0+AfeEg_;*+.8+lRN<E&('*+>82105N+>Yn&(8u+M3():;y'*&H0+Z& Yq/2;*+Z),&('*).5U1*<2RN&('& _Y[&(8E+M3();SRU'*HORUCRNHO143 5
'*&H0d
+ W&('*+_82105N+_LRU/2;<:3 V +_;X-&(/2;*+,<2RN<]3(< 3 '*H\3E),&('*).5U1*<RN&('\8+,<2/282RN)./+,H/[
& (Y[&(8q+M3():;\8+.5o3 /2RN&('
'*&H08
+ 0W{&('*+82105N+iL;*&9<+i;QX-G&(/2;*+,<2RN<lRN</2;*+HORN<[2&(RU'Q/Z10'0RN&('& YJ/2;*+;X-&(/2;*+,<2RN<l& 3 '*H& Yu3 5U5
RU'*HORUCRNHO143 5q),&('*),+.-0/@'*&H0+,<@& Y/2;*+l),&('*).5U1*<RN&('t& W3 '*H),&('*).5U1*<2RN&('RNb
< 0WGLRU/2;t<:3 V +l'*+.RU69;QD&(8<
3(<@RU' eig_;*+5N&(69RN)M3 5]RU'/+.82-08+./:3 /2RN&('& Y_<21*):;t82105N+,<3 8+y=pY10'*)./2RN&('tY8+,+Au8:3 '06+8+,<2/282RN)./+,Hju&(82'
82105N+,<Me Y]3^0?RN<H0+,HO1*).RUD05N+@Y[8& V 3<:+./& Y!8Me 8Me]82105N+,<XW*/2;*+.'RU/JRN<H0+,HO1*).RUD05N+PY8& V /2;*+P<+./&
/2;*+.RU8H0+,),& V -&9<2RU/2RN&('*<E
$t7
4fW43 '*HI8+,).RU-08&)M3 5U5UXeu&ML+.C+.8MWG3(<<&&('3(<),&('*<2/28:3 RU'Q/<E3 8+@RU'C&(5UC+,HW
/2;0RN<J+,w10RUC(3 5N+.'*),+>H0&+,<'*&(/;*&(5NH3 'QX V &(8+9e




yn





M.2@m(Q2ff[f[,sBUfPo>1<q*[)jfO(0f[(yM.c5

2MmW I^RU'*),+I3 5U5_698:3 -0;*<\3 8+T-010/ZRU'Q/&S'*&(8 V 3 5_Yn&(8 V W]3 'RU'*HORUCRNHO143 5 V 3 82K+.83 -0-+M3 8<3 / V &9<2/
(& '*),+sRU'3698:3 -0;7eSg_;*+'1 V D+.8\& YRU'*HORUCRNHO143 5'*&H0+,<l).8+M3 /+,HDXj/2;*+s<+./Z& YJ82105N+,<RN<lD&(10'*H0+,H
DQX " r\j- [ J L e^&I/2;*+'Q1 V D+.8& YJ8+.5o3 /2RN&(''*&H0+,<l).8+M3 /+,H='*&/LRU'8+.5o3 /2RN&('
'*&H0+,<>3 8+).8+M3 /+,H4AuRN<ED&(10'*H0+,HDQXo " G
$3 ( $[46eV4 WL;*+.8+ RN<E/2;*+Z'1 V D+.8
& Y]8+.5o3 /2RN&('S/X-+,<LRU/2;t369RUC+.'t3 82RU/X 3 -0-+M3 82RUD'0 6sRU't382105N+Z),&('*).5U1*<2RN&('7W73 '*H K RN</2;*+l698+M3 /+,<2/
3 82RU/XT& Y}<21*):;3P8+.5o3 /2RN&('/X-G+9e]^&l/2;*+E).5N&9<2108+E& Yh3698:3 -0;I)M3 'TDG+E&(D0/:3 RU'*+,HTLRU/2;I3lH0+.82RUC(3 /2RN&('I&
5N+.'069/2;BO
\e!rt+E/2;1*<&(D0/:3 RU'S [ RU'T*'0RU/+P/2R V +9e
u&(/+]/2;43 /MW9),&('/28:3 82X/&6+.'*+.8:3 5*'0RU/+_+f-43 '*<2RN&('Z<+./<MW(+fORN<2/+.'*),+& Y4/2;*+).5N&9<108+_3 '*Hl+fORN<2/+.'*),+
& Yh/2;*+uY105U5698:3 -0;3 8+>+,wQ10RUC93 5N+.'Q/J'*&(/2RN&('*<JRU'I/2;*+@)M3(<+@& Yh8:3 '06+@8+,<2/282RN)./+,HF82105N+,<Me /_Yn&(5U5N&MLJ<Y[8& V
/2;*+q-08&& Y*& Y0-08&(-G+.82/X`9`q/2;43 /}/2;*+q5N+.'069/2;& Y43H0+.82RUC(3 /2RN&('>Y8& V T/& [ RN<RU'~
$ G Q 4fW L;*+.8+ \RN<
/2;*+<2R M+&
$n+
*)
4q3 '*H K RN<q/2;*+_698+M3 /+,<2/3 82RU/X& Y3u8+.5o3 /2RN&('/X-+J3 -0-G+M3 82RU'06PRU'3u82105N+),&('*).5U1*<2RN&('7e
g_;0RN<q8&(1069;10-0-G+.8qD&(10'*H),&(105NH\D+_8+f*'*+,HD010/qRU/]RN<q<21 ).RN+.'Q//&@&(D0/:3 RU'/2;*+_Yn&(5U5N&MLRU'06>-08&(-G+.82/XW
L;0RN);LRU5U5DG+1*<+,H/2;08&(1069;*&(10/l/2;*+i-08&& Y[<l& Y),& V -05N+fRU/Xj8+,<2105U/<lRU'C&(5UCRU'06y8:3 '06+s8+,<2/282RN)./+,H
82105N+,<Me

0



6



$-





fi M]|9M.tM=uwuMM3
F



yn
aQfOE :fG[n(T9{O\j(l 9B[ :mfU9[[9I ::*BJoZ(*f$(4M =
(M? fSZm9QEB.[Bn,2,.qmufNB=!OuNf9Fmu(Cg!fBM?9[n(Pf2(pq0(@:(a9ln(@:
f2T$sOEft slmE
$n+
*)
465



p

'FL;43 /Yn&(5U5N&MLJ<MW0L+>3(<<21 V +P/2;43 //2;*+>3 82RU/XF& Yh8+.5o3 /2RN&('/X-+,<RN<D&(10'*H0+,HFDQXF3Z),&('*<2/:3 'Q/Me

q

hy|My


f[fn,2iBNB6

<
p1
!m(aN.W5

$

V 2(0,[n(i2

[{%}huyyn A'Ay

y|

esO.CD9a


(2@m9QP2S!

_ ` !m(aN.W5
V !90BpffGL:i(asQ29O,[n(2\( E

V 2(0,[n(i2D(a>sDp ` !m(aN.W5
2MmWkg_;*+Y[&(5U5N&LRU'06k8+,<105U/<T;*+M3,CRU5UX8+.5UX&('

8&(-7eE`MO3 5U5EH0+.82RUC93 /2RN&('*<TRU'QC&(5UC+,HD+.RU'06k&
-&(5UX'*& V Ro3 55N+.'069/2;7WO/2;*+.XF3(H V RU/3-G&(5UX'*& V Ro3 57),+.82/2R4)M3 /+=/2;*+@<+,w1*+.'*),+E& Y}-08& 2+,)./2RN&('*<1*<+,HT/&
D010RU5NHF/2;*+PH0+.82RUC93 /2RN&('aAfe
H!9GU..GB:Pmq2g
! } he]g_;*+-08&(D05N+ V D+.5N&('06<q/C
& e '*H0+,+,HW3@-G&(5UX'*& V Ro3 5
),+.82/2R4)M3 /+RN<69RUC+.'DX3tH0+.82RUC(3 /2RN&('Y8& V /&k3S698:3 -0
; WYn&(5U5N&ML+,HDQX3S-08& 2+,)./2RN&('Y[8& V
/2;*+Z6&3 5h/g
& Y[ePr;*+.' " OW&('*+&(D0/:3 RU'*<@~ =-08& m+,)./2RN&('t):;*+,)KRU'06AfW/2;1*<E/2;*+
~),& V -05N+./+.'*+,<<Me
!
4(a\`
! !e\+,)M3 5U5h/2;*+),&('*<2RN<2/+.'*).X
_` !(aN.fB:imE2\`

);*+,):KRU'QC&(5UC+,<T/2;*+RU828+,HO10'*H*3 'Q/TYn&(8 V & Ye '&(8H0+.8s/&t5URU69;/+.'/2;*+-08&(D05N+ V Y[&(8 V 105o3 /2RN&('7W
L+3(<<1 V +>;*+.8+l/2;43 /P3 5U5]^0?E<@3 8+>RU828+,HO10'*H*3 '/MW7D010/ERU828+,HO10'*H*3 '*).X)M3 'SD+>RU'/+.698:3 /+,HLRU/2;*&(10/
RU'*).8+M3(<2RU'06E/2;*+),&('*<2RN<2/+.'*).X>):;*+,)K),& V -05N+fORU/X{<+,+q/2;*+-08&& Y4& Y*/2;*+,&(8+ V cOe}G 7
4
D+.5N&('06</& _ ` <2RU'*),+>RU/),&(828+,<2-&('*H0</&\/2;*+5o3 '069143 6+
"#mW]_ _ $M]*1_ *1_ 4>nQWaL;*+.8+

]S+.'*),&H0+,<u3 'RU'*<2/:3 '*),8
+ $n2)m.
4& Yh/2;*+P-08&(D05N+ V 3 '*H $M]*H_ *1_ 4 RJ
_ "$ 3 N 4fW4L;*+.8+
N

N
R
\
<

3
0
H
.
+
2
8
U
R
9
C

3
2
/
N
R
(
&

'



8
&



/
&
B

W


N
R
\
<

3
0

8

&
2

,
+
.
)
2
/
N
R
(
&

'



8
&
2
/
*
;

+
2
/
2
8
U
R
9
6

6
.
+
\
8
& Y@3S),&('*<2/28:3 RU'Q/
JPN L
V
V

RU'Q/7
& W _ " $ 3 4fW RN<u3\H0+.82RUC93 /2RN&('Y[8& V /Z
& 3 '*
H RN<u3-08& 2+,)./2RN&('Y[8& V
RU'/&
!<Me /Mr
e D% JON L; q
0 N e RN<E-&(5UX'*& V Ro3 5U5UXH0+,).RNH*3 D05N+3 '*H-&(5UX'*& V Ro3 5U5UXyD43 5o3 '*),+,H=[<2RU'*),+Z/2;*+
5N+.'069/2;*<J& 3 '*H 3 8+u-G&(5UX'*& V Ro3 5RU'F/2;*+@<2R M+E& Y}/2;*+ERU'0-010/BAfeqr;*+.'U
" OW0&('*+E&(D0/:3 RU'*</2;*+
-08&(D05N+ V _G
4]WO/2;1*<q/2;*+ _` ~),& V -05N+./+.'*+,<<Me]^RU'*),+2\ ),&('*<2RN</<qRU'

<&(5UCRU'06@/L&@RU'*H0+.-G+.'*H0+.'/]-08&(D05N+ V <MWQG
4= _ ` ~),& V -05N+./+Aq3 '*HZ2\~

= ~),& V -05N+./+AfWG3 '*HF<RU'*),[
+ RN<RU'*).5U1*H0+,HIRU' _` WO\G~ jRN<3 5N<& _` ~),& V -05N+./+9e


` !:(aU,fGBBZm2DG heu<Y[&(82\=[<+,+P3 D&MC+AfW0L+>3(<<21 V +@/2;43 /u3 5U5}^0?E<
3 8+RU828+,HO10'*H*3 'Q/Me]g_;*+uw1*+,<2/2RN&('iRN< 3 8+/2;*+.8+E3>H0+.82RUC(3 /2RN&('sY[8& V t/&l3l^0
? 3 '*HT3P-08& 2+,)./2RN&('
Y[8& V RU'/k
& YnWQ<1*);\/2;43 /!Yn&(83 5U5 & Y/2;0RN<]H0+.82RUC93 /2RN&('7WQY[&(83 5U5a),&('*<2/28:3 RU'Q/ 9W9Yn&(83 5U5*-08& 2+,)./2RN&('
JPN L RU'/
kY[8& V
& mW7/2;*+.8+\+fORN<2/<>3F-08& m+,)./2RN&(
' Y[8& V
RU'/
& <Me /Mo
e JON L " :e
RN<-G&(5UX'*& V Ro3 5U5UXH0+,).RNH*3 D05N+3 '*H-G&(5UX'*& V Ro3 5U5UXD43 5o3 '*),+,H=[<2RU'*),+/2;*+<R M+& Y]/2;*+lH0+.82RUC93 /2RN&('Y[8& V
/
& RN<-G&(5UX'*& V Ro3 5U5UXF8+.5o3 /+,H/&/2;*+<R M+>& Y!/2;*+PRU'0-010/BAfeg_;Q1*<MWaDG~ kRN<RU' ` e
'T&(8H0+.8_/&l-08&MC+E/2;*+@),& V -05N+./+.'*+,<:<MWOL+uD010RU5NHI38+,HO1*)./2RN&('TY8& V 3l<2-+,).Ro3 5)M3(<+E& Y/2;*+E-08&(D05N+ V
WQL;*+.8+/2;*+JYn&(8 V 105o3>RN<_3 ~
% ux=R$e+9eq3 'RU'*<2/:3 '*),+u& ~^!gAf69RUC+.'T3@Y[&(8 V 105o3iWQL;0RN):;iRN<_3
),&('(m10'*)./2RN&('& Yh).5o3 1*<:+,<LRU/2;3 / V &9<2/ 5URU/+.8:3 5N<W43 '*HI3l-43 82/2RU/2RN&(B
' *) *) n& Y{RU/<C93 82Ro3 D05N+,<MW
H0&+,<}/2;*+.8++fORN<2/!3/28210/2;3(<:<2RU69' V +.'Q/hYn&(8h/2;*+C93 82Ro3 D05N+,<hRU'~ W9<21*);Z/2;43 /hYn&(8h3 5U5O/28210/2;3(<<2RU69' V +.'/
Yn&(8q/2;*+C93 82Ro3 D05N+,<]& YR W9/2;*+.8++fORN<2/<]3@/28210/2;s3(<<2RU69' V +.'/qYn&(8q/2;*+C93 82Ro3 D05N+,<]& YR <21*):;/2;43 /RN<
/2821*+ g_;0RN<-08&(D05N+ V RN< ` ~),& V -05N+./+=$^/&):K V +.X+.8MW`Mb9W/2;*+,&(8+ V d*eU`Afe!|7+./1*<_)M3 5U5aRU/ ~^!g e



`* { $

2D*

>* {
* |

K$ 2D*
















{


K 2D*


6

K* {

F

K 2D*

>v { $


6



2

$-


6


* |
>* {


l
>v { $

fi3MfiuM%





b



af

c

bt

bf

val

val

val

ct

cf

dt

2
1

2
3

C1

C1

C1

C1

C1

C1

1

C1

C2



b

c



val

val

val

val

val

df

av

bv

cv

dv



...

3

C2

C2

C2

C2

C2

C2

val

av

b

val

bv

Q

1

>~

2

C1











2
3

1

3

C2

val

av

K*}{$
b

val

bv

x}RU69108+` 03 V -05N+& Y{/28:3 '*<2Y[&(8 V 3 /2RN&('FY[8& V ~ Fff1& /&D
g_;*+/28:3 '*<mY[&(8

3 /2RN&('L+1*<+RN<PRU5U5U1*</28:3 /+,HtRU'jx{RU6*eq`esg_;*+ ~^!gY[&(8 105o3F1*<+,HRN<l3 6Q3 RU'

64T$UTV d=T4fWh3 '*H/2;*+\-43 82/2RU/2RN&('kRN<9 " mA*Kn *) " mAV n *) " mAnQeFg_;*+
698:3 -0;&(D0/:3 RU'*+,HRN<u/2;*+l<:3 V +3(<RU'S/2;*+-08&& Y& Y]g_;7ecOWx{RU6*e{`MOWG+f0),+.-0/u/2;43 /E),&('*),+.-0/E'*&H0+,<
`>),&(828+,<2-&('*HORU'06P/&@C93 82Ro3 D05N+,<!RU' 3 8+'*&(/!5URU'0K+,H/&E/2;*+_'*&H0+,< `93 '*HUP8+.-08+,<+.'/2RU'06
$

/2;*+.RU8J-&9<<2RUD05N+EC93 5U1*+,<Me
x{RU8<2/_);*+,):Ki/2;43 /_RU's/2;0RN<RU'0RU/2Ro3 5L&(825NHWO'*&),&('*</28:3 RU'Q/RN<CRN&(5o3 /+,HWOD010//2;*+u6&3 5)M3 '0'*&(/D+
<:3 /2RN<m4+,HeXF3 -0-05UXRU'06F&('*),+P/2;*+l+.C&(5U10/2RN&('S82105N+iW4L+>/282XI<& V +>C(3 5U143 /2RN&('S& Yq/2;*+C(3 82Ro3 D05N+,<uRU'
3 '*HI&(D0/:3 RU'3lL&(825N
H WO/2;43 /),&('Q/:3 RU'*<E3 '3 '*<2L+.8/&eq10//2;0RN<JL&(825NHF;43(<J/&<B3 /2RN<mYXi/2;*+
-&9<2RU/2RUC+),&('*<2/28:3 RU'Q/ Q W9+fO-08+,<<2RU'06@/2;43 / $Yn&(8q+.C+.82X>C93 5U143 /2RN&('& YG/2;*+_C(3 82Ro3 D05N+,<!RU'~ lE W9/2;*+.8+
V 1*</+fORN<2/>3TC93 5U143 /2RN&('& Y/2;*+C93 82Ro3 D05N+,<PRU' <1*);t/2;43 //2;*+ZY[&(8 V 105o3I+.C93 5U143 /+,<>/&j[B* :e
<:3 /2RN<m4+,</2;0RN<),&('*<2/28:3 RU'/MW0RU/ V +M3 '*<_/2;43 /_L+E;43,C+Yn&(10'*Ht=DQXT3 -0-05UXRU'06/lA3C93 5U143 /2RN&('I& Y/2;*+

C93 82Ro3 D05N+,<RU'* <1*);T/2;43 /_Y[&(8J3 5U5C93 5U143 /2RN&('*<J& Y}C93 82Ro3 D05N+,<RU'* lC =L;0RN);I)M3 'FD+E<2R V -05UR4+,H
RU' $Yn&(83 5U5aC(3 5U143 /2RN&('*<& YC(3 82Ro3 D05N+,<RU'C :W<2RU'*),+J/2;*+.8+RN<]&('05UX\&('*+<1*);C93 5U143 /2RN&('iY[&(8& AfWQ/2;*+.8+
RN<3FC(3 5U143 /2RN&('& Y/2;*+C(3 82Ro3 D05N+,<RU' <1*);j/2;43 //2;*+Yn&(8 V 105o3+.C(3 5U143 /+,</&j[f0,esg_;*+.'/2;*+.8+
RN<3 '3 '*<L+.
8 :QBs/&/2;*+ ~ F1
ff & -08&(D05N+ V ej%&('QC+.8<+.5UXW]<210-0-G&9<:+s3 '3 '*<2L+.8sG/&/2;*+sD
-08&(D05N+ V e / V +M3 '*<]/2;43 /]Y[&(8+.C+.82XL&(825NZ
H /2;43 /)M3 'iDG+J&(D0/:3 RU'*+,HiDQX\3 -0-05UXRU'06>/2;*+82105N+XWQ/2;*+
),&('*<2/28:3 RU'/ Q RN<CRN&(5o3 /+,H=[&(/2;*+.82LRN<+>),&(105NHsD+-08& 2+,)./+,HTRU'Q/o
& 3 '*Hs/2;*+@3 '*<2L+.8_L&(105NHTD+
:QBAfeg_;1*<E/2;*+.8+>RN<'*&T3(<<2RU69' V +.'Q/@& Yq/2;*+C(3 82Ro3 D05N+,<uRU'p
B
<

3
2
/
N
R
<mYXRU'06/2;*+l),&('*<2/28:3 RU'/MWh0 5W 5/2;*+

3 '*<2L+.8J/&/2;*+ ~ F1
ff & -08&(D05N+ V RN<@a9e
! hessD <2/:3MX<>RU'/2;*+\<:3 V +).5o3(<<P&
` !:(aU,fGBBsm@sD`
),& V -05N+fORU/Xi3(<D } '*H0+,+,HW/2;*+w1*+,<2/2RN&('RN< 3 8+J/2;*+.8+E3 'CsD~H0+.82RUC(3 /2RN&('iY[8& V
/C
& Y43 '*Hs3E-08& 2+,)./2RN&('Y[8& V /&3>^0
? YnWQ<21*):;/2;43 /qYn&(83 5U5 7& Y/2;0RN<H0+.82RUC(3 /2RN&('s+.RU/2;*+.8+,w143 5
/&Zt&(8_&(D0/:3 RU'*+,HTDQXi3 'sR VV +,HORo3 /+
D~H0+.82RUC93 /2RN&('7WQYn&(83 5U)
5 & Y7/2;0RN<_H0+.82RUC(3 /2RN&('FH0+.82RUC+,HiY8& V
JPN L /& W*/2;*+.8++fORN<2/<
DQX3 'B\~H0+.82RUC93 /2RN&('7W*Yn&(83 5U5}),&('*</28:3 RU'Q/
W0Yn&(83 5U5-08& 2+,)./2RN&(
' Y8& V
3 'U\~H0+.82RUC93 /2RN&('FY[8& V /&\3\^0#
? 3 '*H3>-08& 2+,)./2RN&(g
' aY8& V
@/8
& <e /M
e JON L "Q
3 '*HT/2;*+u5N+.'069/2;*<J& Yh3 5U57H0+.82RUC93 /2RN&('*<3 8+E-&(5UX'*& V Ro3 5RU'T/2;*+@<2R M+E& Y{/2;*+ERU'0-010/Me!r;*+.'* " OW0&('*+
&(D0/:3 RU'*<J2D !W4/2;1*<J/2;*+ ` ),& V -05N+./+.'*+,<<Me
|7+./_1*<-G&(RU'/&(10//2;43 /MWL;*+.8+M3(<RU'T6+.'*+.8:3 5)M3(<+9WH0+,HO1*)./2RN&('FRN< V &(8+uHOR ).105U/_RU's2\=/282105UX
10'*H0+,).RNH*3 D05N+A\/2;43 'RU'D=[<+ V R~H0+,).RNH*3 D05N+AfW_/2;*+),&('QC+.8<+;*&(5NH0<ZY[&(8/2;*+F-43 82/2RN).105o3 8i)M3(<+F&
8:3 '06+f~$8+,<2/282RN)./+,H82105N+,<Me





$





>~

>~

K* { $

* |



G

v { $


* |

F



6
e

.

$-,


l




fi M]|9M.tM=uwuMM3
F

q

h {3M|{3 M}h A{% }A
z'*+ V 3,X),&('*<2RNH0+.8u/2;*+>)M3(<:+>L;*+.8+'*&(/u&('05UXF82105N+,<D010/@3 5N<&),&('*<2/28:3 RU'/<P3 8+>8+,</282RN)./+,He|7+./u1*<
*8<2/J),&('*<2RNH0+.8/2;*+ V +M3 '0RU'06(Y105{)M3 /+.6&(82XF& Y{'*+.6Q3 /2RUC+P),&('*</28:3 RU'Q/<Me

p

<

q

hy|My



$ }$ynu{ My}$ A{%
fNBPsD(fig=fBi9*@:Gn9[M? Z90f[2(46

}A

eT[(OG9:u BBG[[99lO

K$72D*4:2(B!H!:(aU,W5
_K*{:2(B !m(aN.W5
2\:$7
2D** NNGK$7 2D*4 :!Ua2QfNfYRF:2(B,fZM!Q2,[fU5
2D*
| 2f\(0@MfZ'!mQ2.nQfNW5
2\K*}
{ $ (a>sD* | f\([B@:a2.nQfNW5

V _
V
V
V
V

2MmW
!! 1!m(aN.fGf:m_

`

K{$ Av
K* { $

7p
4@Y[8& V ~),& V -05N+./+.'*+,<<J& Y{-08& m+,)./2RN&('F):;*+,)K~
RU'06=/2;7e*cAfe
!:(aU,fGBBZm_ } /2;0RN<J-08&(D05N+ V )M3 'IDG+E+fO-08+,<<+,H3(< RN<JRU/J/2821*+E/2;43 /
)M3 'D+F-08& m+,)./+,HRU'Q/&k3 '*H/2;43 /\'*&),&('*<2/28:3 RU'Q/T& YE)M3 'D+F-08& m+,)./+,HRU'Q/&k
/2;1*<
D+.5N&('06<I/& 1
e u&ML 5N+./I1*<),&('*<2RNH0+.8/2;43 /I),&('Q/:3 RU'*<S&('05UX&('*+j),&('*</28:3 RU'Q/Me 8+,HO1*)./2RN&('
Y[8& V ~^!g /&
=[<:+,+TYme R$e/2;*+F-08&& Y@& Y@/2;7eAl-08&MCRNH0+,<s3t<2/28:3 RU69;Q/mYn&(82L3 8H8+f~
HO1*)./2RN&('kY8& V ,u((Z,u /&_7 } =[<+,+\Yme R$e 3 -43(HOR V RU/282RN&(17W`Mb9b dAfWh/2;Q1*<l/2;*+
~),& V -05N+./+.'*+,<<Me
.lM !2.nQf [ :l2!\:
7p
4
Jg}&E-08&MC+/2;*+_RU'*),&('*<RN<2/+.'*).X& Y3E@EW(L+ V 1*<2/
*'*H<& V +CRN&(5o3 /2RN&('& Y3E),&('*<2/28:3 RU'Q//2;43 /!LRU5U5*'*+.C+.8!DG+_8+,</&(8+,He!10/!'*&ECRN&(5o3 /2RN&('& Y73u'*+.6Q3 /2RUC+
),&('*<2/28:3 RU'/})M3 '>+.C+.8}DG+h8+,<2/&(8+,Hi=pY1082/2;*+.8}82105N+]3 -0-05URN)M3 /2RN&('*<})M3 '&('05UX@3(H0HPRU'OY[&(8 V 3 /2RN&('7W /2;Q1*< V &(8+
-&9<<2RUD05N+J-08& 2+,)./2RN&('*<MWO3 '*Hi)M3 '0'*&(/8+ V &MC+u/2;*+).105U-082RU/&('*+Afe]^&L+u&('05UX;43,C+/&P-08&C+/2;43 /&('*+
),&('*<2/28:3 RU'/u& Yhj)M3 'D+PH0+,HO1*),+,HY8& V $n+
*)
4fRU/RN<u3<+ V R~H0+,).RNH*3 D05N+l-08&(D05N+ V e a2.nQf [ :
mSG Yn&(5U5N&MLJ<L+ V 1*<2/u-08&MC+l/2;43 /E)M3 'yDG+H0+,HO1*),+,HIY[8& V $n+
*)
4fWD010//2;43 /
'*&\),&('*<2/28:3 RU'/& Y{)M3 '7e
g_;*+j3 82691 V +.'/<-08&CRU'06<+ V R~H0+,).RNH*3 D0RU5URU/X& YH0+,HO1*)./2RN&('RU'2D3 '*H10'*H0+,).RNH*3 D0RU5URU/X&
H0+,HO1*)./2RN&('RU'F2CDj3 8+E/2;*+P<B3 V +>3(</2;*+@&('*+,<J1*<:+,HTRU'F/2;*+@-08&& Y!& Y!g_;7e7`MOe
g_;*+8+,<2/282RN)./2RN&('/&j'*+.6Q3 /2RUC+S),&('*<2/28:3 RU'/<TH0+,).8+M3(<+,<T),& V -05N+fRU/X& YP-08&(D05N+ V <TRU'/2;*+I

&
0
H
+.5$WD010/RU/_H0&+,<'*&(/;*+.5U- V 1*);I3(<<:&&('F3(<]82105N+,<3 8+RU'QC&(5UC+,HW0<2RU'*),+/2;*+,<+-08&(D05N+ V <_8+ V 3 RU'
V
10'*H0+,).RNH*3 D05N+9e%& V D0RU'0RU'06t8:3 '06+F8+,<2/282RN)./+,H82105N+,<\3 '*H'*+.6Q3 /2RUC+I),&('*<2/28:3 RU'Q/<W]L+I&(D0/:3 RU' V &(8+
RU'Q/+.8+,</2RU'06i),& V -05N+fORU/XF8+,<2105U/<M

v$G*{ $



Tl

K* { $
{$ Av

K* {

p



q <
&

Bf[f[M2sBNB(aiGnQ9['?(Z(0.[m(4>(a2f,f4O[G9UMQ2 MW
V 2\K{$7pAv429\fZ!`1!m(aN.W5
{ $ 29\f !9GU.5
V 2\K*}
| (a>CD7K* { ( ` !:(aU,W5
V 2D*
hy|My





$

[



y|

$-%-

{%}

}hy|M{

|M}h A{% }A

ps(*;:2(QS!

fi

condition

frontier

3MfiuM%

mandatory part
1

1
2

2

"<

<

2
3

3

1

1

1

3

3

U

2



U

$ C4

D+$ C4

3

2

1

1

U
K{$7pAv4/&3l8+,</282RN)./+,HID7GK$GK*}{$
<

o$ E4

2
3

2
3

x{RU69108+\`McO!g8:3 '*<mYn&(8 V 3 /2RN&('Y[8& V

2MmW '*),&('*<2RN<2/+.'*).XRU't2\3(H V RU/<Z3T-G&(5UX'*& V Ro3 5),+.82/2R4)M3 /+9Wq3IH0+.82RUC(3 /2RN&('=[& Y-G&(5UX'*& V Ro3 5

5N+.'069/2;aAY[8& V 5N+M3(HORU'06I/&F3s698:3 -0;RU'/&TL;0RN);3s),&('*<2/28:3 RU'/>& Y])M3 'DG+l-08& 2+,)./+,HW3 '*HS/2;0RN<
-08& 2+,)./2RN&('7e '*),&('*<2RN</+.'*).XRN<u/2;Q1*<ERU' W3 '*HS),& V -05N+./+.'*+,<<uY[&(5U5N&LJ<uY8& V /2;*+l-43 82/2RN).105o3 8P)M3(<+
L;*+.' RN<P+ V -0/Xeix*&(8>H0+,HO1*)./2RN&('7W{L+ V 1*<2/P-08&C+\/2;43 /P'*&F),&('*<2/28:3 RU'/)M3 'tD+H0+,HO1*),+,HY[8& V
$n+
*)
4fWQD010/!/2;43 /)M3 '7eq^&E/2;*+_-08&(D05N+ V RN<hRU' ex*&(8q),& V -05N+./+.'*+,<:<MW8+ V 3 82Kl/2;43 /!/2;*+_-08&(D05N+ V
RN<J<2/2RU5U57),& V -05N+./+>L;*+.'KRN<+ V -0/Xt=[g_;7eJ` Afe
g}&-08&C+E/2;43 /_D } SLRU/2;s8Me 8Me!82105N+,<3 '*Hi'*+.6Q3 /2RUC+E),&('*<2/28:3 RU'/<RN< ` ~),& V -05N+./+9W

L+yLRU5U5u*8<2/F<2;*&L/2;43 /TRU/sDG+.5N&('06<T/& ` WJ/2;*+.'+fO;0RUD0RU/I3t8+,HO1*)./2RN&('Y[8& V 3 _ ` ~),& V -05N+./+


-08&(D05N+ V /&ZRU/<),& ~$-08&(D05N+ V D7G G } =[<2RU'*),+P),& ~ ` 0 _` Afe
2DG ),&(828+,<-G&('*H0</&I/2;*+i5o3 '069143 6+sff
" mW]p _ _ $M]*y_ *y_ 4>nQW{L;*+.8+ ]
+.'*),&H0+,<Z3 'RU'*<2/:3 '*),
+ $$~ $n+
*)
*m
4y4Z& YJ/2;*+-08&(D05N+ V Wq3 '*Q
H $M]*y_ *y_ 4 R
_ +.'*),&H0+,<l3 'GD~
H0+.82RUC93 /2RN&('Y[8& V /
& 3 '*Hk3-08& 2+,)./2RN&('Y[8& V /J
& Wq3 '*
H _ +.'*),&H0+,<3 V 3 -0-0RU'06Y[8& V
<& V +s),&('*<2/28:3 RU'Q/& YJ/
& Y]/2;43 /lRN<'*&(/\3F-08& 2+,)./2RN&('='*&(/+i/2;43 /lRb
YH0&+,<l'*&(/lCRN&(5o3 /+T3 'QX
),&('*<2/28:3 RU'/MW*/2;*+.'I'*&Z698:3 -0;RU'F/2;*+PH0+.82RUC(3 /2RN&('IY8& V /& H0&+,<BAfe
rt+k+f;0RUD0RU/S'*&L 38+,HO1*)./2RN&('Y8& V /2;*+k6+.'*+.8:3 5ZG 7
4 -08&(D05N+ V /&D7G
G } LRU/2;8Me 8Me82105N+,<F3 '*H'*+.6Q3 /2RUC+S),&('*<2/28:3 RU'Q/<Me|7+.J
/ $n+
*mq
" rn4D+y3 'RU'O~
<2/:3 '*),+u& 7p
4=LPe 5$e&0e 6*eUWOL+u8+,<2/282RN).//2;*+u-08&(D05N+ V /&),&('*<2RNH0+.8&('05UX&('*+u-&9<2RU/2RUC+
),&('*<2/28:3 RU'/BAfetg_;*+s/28:3 '*<mY[&(8 V 3 /2RN&('L+s),&('*<RNH0+.8lD010RU5NH0<\3 'kRU'*<2/:3 '*),+T& Y2DG G
$$o
$ C4 $n+
*)+
$ E4*m $ C4y4y4P3(<@Yn&(5U5N&MLJ<ert+\)M3 5U5/2;*+@fm(a[nfF& Y/2;*+-&9<2RU/2RUC+\),&('*<2/28:3 RU'/ /2;*+
<+./l& Y_'*&H0+,<RU'j/2;*+\/282RU696+.8F= 5pW 5I),&(5N&(8+,HjDXjAE;43MCRU'063 />5N+M3(<2/l&('*+\'*+.RU69;DG&(8ZRU'j/2;*+i&(D05URU6Q3~
/2RN&('7ePg_;*+ZH0+f*'0RU/2RN&('t& Y),&(5N&(8+,HS698:3 -0;*<ER V -05URN+,<E/2;43 /EY[8&('Q/2RN+.8E'*&H0+,<P3 8+l),&('*),+.-0/@'*&H0+,<=/2;*+.RU8
'*+.RU69;QD&(8<i3 8+I/2;Q1*<8+.5o3 /2RN&(''*&H0+,<BAfe|7+./1*<\H0+.'*&(/+F/2;*+,<+sY[8&('Q/2RN+.8i'*&H0+,<D
X I* F6F6F * K eg_;*+
+.C&(5U10/2RN&('S82105N++
$ E4_;43(<Yn&(8;QX-&(/2;*+,<2RN<u/2;*+>/282RU696+.8E& WG3 '*HIYn&(8),&('*).5U1*<2RN&('t3\8+.5o3 /2RN&('y'*&H0+
/X-+,
H 8 4WGL;*+.8r
+ "FRN<@3\'*+.L K ~3 82X8+.5o3 /2RN&('/X-+RU'*),& V -43 8:3 D05N+LRU/2;t3 5U5h&(/2;*+.8E/X-+,<Me
g_;*C
+ 7M'*+.RU69;DG&(8@& Yq/2;0RN<'*&H0+RN</2;*+>),&('*),+.-0/E'*&H0E
+ :eu%;*+,):K/2;43 /+
$ E4RN<u38:3 '06+8+,<2/282RN)./+,H
82105N+9eqg_;*+E'*+.6Q3 /2RUC+P),&('*</28:3 RU'Q/J $ E4RN<_/2;*+@<210D0698:3 -0;& ),& V -&9<+,HI& Y{RU/<J&(D05URU6Q3 /2RN&('= XJ L 4
3(H0H0+,HyLRU/2;S'*&H0+,<E& Y]/2;*+Y[8&('Q/2RN+.8P3 '*Hy/2;*+>8+.5o3 /2RN&(''*&H0+>/X-+,
H "aW45URU'0K+,HS/&i/2;*+Y8&('/2RN+.8
'*&H0+,<JRU'/2;*+<:3 V +@L_3,X3(<u3 DG&C+9ex}RU'43 5U5UXW4/2;*+l^0?
$ C4RN< V 3(H0+& Y!&('*+>8+.5o3 /2RN&(''*&H0+P/X-+,H
"i3 '*HFRU/<'*+.RU69;DG&(8<JY8&('/2RN+.8'*&H0+,<Me!g_;0RN</28:3 '*<mYn&(8 V 3 /2RN&('RN<JRU5U5U1*<2/28:3 /+,HFRU'x{RU6*e`McOe

K* {

K* { $
K$ K* { $




K$ 2D*

$ K* { $
K AD*
U
U U


U

7





U

U

7

$-M

(

7

U

4
K$ Kv { $


U

fi M]|9M.tM=uwuMM3
F

K AD*

5$e&0e 6*eEL+l)M3 't3(<<21 V +/2;43 /@RN<uRU828+,HO10'*H*3 'Q/MRU'/2;43 /@)M3(<+9WG 7p
4RN<E<2/2RU5U5
_ ` ~),& V -05N+./+@=[<+,+]/2;43 /}/2;*+/28:3 '*<mYn&(8 V 3 /2RN&('Z1*<+,H>RU'/2;*+]-08&& Y4& Yag_;7e cJ-08&HO1*),+,<!3 '>RU828+,HO10'*H*3 'Q/


698:3 -0;gPAfeu&ML<210-0-&9<+/2;43 /j$n+* E4]RN<_),&('*<2RN<2/+.'/MhRU/ V +M3 '*<_/2;43 /+.RU/2;*+.8_/2;*+/282RU696+.8& H0&+,<
'*&(/u-08& m+,)./RU'Q/&FWG3 '*HRU'/2;43 /E)M3(<+9Wa/2;*+P82105N+D+$ E4JLRU5U5'*+.C+.8u-08&HO1*),+/2;*+'*+,+,H0+,HQ3
'*&H0+9Wa&(8u+.C+.82Xj=[+fRN<2/2RU'06A-08& m+,)./2RN&('S& Yq/2;*+>),&('*HORU/2RN&('S& RU'Q/&T
"$n`
4J)M3 'D+>+fO/+.'*H0+,H
/&>3E-08& 2+,)./2RN&('i& 3(<]3EL;*&(5N+9e]^&P+.C+.82X3 -0-05URN)M3 /2RN&('s& Yr
$ E4{-08&HO1*),+,<3ECRN&(5o3 /2RN&('i& YG $ E4fe
$ C4_)M3 '0'*&(/D+@H0+,HO1*),+,HFY[8& V /2;*+@K'*&ML5N+,HO6+D43(<+9e_%&('QC+.8<+.5UXWG<210-0-&9<+E/2;43 /
'IDG&(/2;I)M3(<:+,<uo
G~$CRN&(5o3 /+,< \W/2;*+.'k/2;*+i3 -0-05URN)M3 /2RN&('& Y,+

$ C4uYn&(5U5N&MLRU'0J
6 -08&HO1*),+,<l3I698:3 -0;j/2;43 /ZH0&+,<>'*&(/
CRN&(5o3 /+Ela
$ C4fW*3 '*HFL+P)M3 'H0+,HO1*),+o
$ C4fe
g_;*+I3 D&MC+T/2;*+,&(8+ V <2;*&LJ<\3yH0+,).8+M3(<+TRU'),& V -05N+fRU/XL;*+.'6+.'*+.8:3 5-&9<2RU/2RUC+F),&('*<2/28:3 RU'/<
3 8+S8+,<2/282RN)./+,H/&'*+.6Q3 /2RUC+&('*+,<Me 7p
4Y$3 5U5N<sY8& V _` /&),& c~ 3 '*HWL;*+.'

3 5N<&i),&('*<2RNH0+.82RU'06i8:3 '06+>8+,<2/282RN)./+,H82105N+,<MW*2\ 7p
4Yn3 5U5N<JY8& V _ ` /&i W3 '*HID7G

Y$3 5U5N<Y[8& V ` /& ,` e /\L&(105NHDG+IRU'Q/+.8+,<2/2RU'06k/&j+fO;0RUD0RU/-43 82/2RN).105o3 8T)M3(<+,<&

),&('*<2/28:3 RU'/<MW V &(8+Z6+.'*+.8:3 5!/2;43 ''*+.6Q3 /2RUC+&('*+,<MW7/2;43 / V 3 K+Z/2;0RN<@),& V -05N+fRU/XY$3 5U5hRU'Q/&TRU'/+.8 V +f~
HORo3 82XF).5o3(<<+,<P=DXT+f*3 V -05N+P 3 '*H ` Y[&(8_7 7p
4{AfeJ^& V +P<2X'Q/:3()./2RN)@8+,</282RN)./2RN&('*<

L+uH0+f*'*+,HY[&(882105N+,<3 8+6&&H)M3 '*HORNH*3 /+,<Mh/2;*&(1069;F3E*'0RU/+u+fO-43 '*<2RN&('s<:+./& Y7),&('*<2/28:3 RU'Q/<_;43(<'*&
<+.'*<+9W5N+./q1*<]),&('*<2RNH0+.8m(Q2ff[f[,\(0.[m9p4.eh|7+./]1*<]3 5N<:&PH0+f*'*+>(p.9*G2,(*f[2(pa
3(<E),&('*<2/28:3 RU'Q/<EL;*+.8+l/2;*+l/282RU696+.8P3 '*Hy/2;*+&(D05URU6Q3 /2RN&('j3 8+'*&(/@),&('0'*+,)./+,H7<21*);),&('*<2/28:3 RU'/<@RU'O~
).5U1*H0+@/2;*+ /&(-&(5N&(69RN)M3 5),&('*<2/28:3 RU'/< P1*<+,HTRU'=[RU'*+M3 1RN<<:3(&(10R$W`Mb9b9Afe
g_;*+]Y[&(5U5N&LRU'06E-08&(-G+.82/XP;0RU69;05URU69;Q/<h/2;*+8+.5o3 /2RN&('*<2;0RU-*<h& Y*/2;*+,<+-43 82/2RN).105o3 8h)M3(<:+,<LRU/2;l'*+.6Q3 /2RUC+
),&('*<2/28:3 RU'/<M



U


U



U

U

v { $



U

U





U



K{$ Av
K{$ Av

K AD*



5





yn
n9[M ? l(*f[2(pa@(O(.[[,U(> M>2P9s2(S!Bf[fn,2F(*f[2(pa
(as(p.9*G2,I(0.[m9p4W5



2MmW Pu<E'*&(/2RN),+,HtRU't<:+,)./2RN&('jvOW}3s'*+.6Q3 /2RUC+\),&('*<2/28:3 RU'/>RN<P+,w10RUC93 5N+.'Q/P/&I3s-&9<2RU/2RUC+),&('*<2/28:3 RU'/
L;*&9<+F&(D05URU6Q3 /2RN&('RN<),& V -&9<+,H& YE&('*+F),&('*),+.-0/\'*&H0+T& YE/X-G+m " 0W]L;*+.8+*8 "yRN<
RU'*),& V -43 8:3 D05N+lLRU/2;S3 5U5h&(/2;*+.8u/X-G+,<E3 '*HH0&+,<'*&(/u3 -0-+M3 8RU'3 'QXy^0?+f0),+.-0/RU'=RU/RN</2;Q1*<@3
HORN<),&('0'*+,)./+,Hs),&('*<2/28:3 RU'Q/BAfeqre 5$e&0e 6*eh/2;0RN<]'*&H0+)M3 'D+5o3 DG+.5N+,HDQX3 'iRU'*HORUCRNHO143 5 V 3 82K+.8u=L;0RN):;7W
3(<9 *W{3 -0-+M3 8<&('05UXSRU'j7AfW}/2;Q1*<5N+M3(HORU'06/&3I),&('*<2/28:3 RU'/L;0RN):;RN<>DG&(/2;jHORN<:),&('0'*+,)./+,H
3 '*HF8:3 '06+f~$8+,<2/282RN)./+,He



7

p





} q

hy|My



<

$

(4G2,2(0.[m(4W

]W|M}]}$yfiAyM}h A{% }A

esOfu(4$(0(4@:9oS!

K$72D*4:2(B! !m(aN.W5
2\K{$7pAv4k(a2\*|2.9pZaQ2.nQ.U={.GK$72D*4

V

V _

:2(B! !m(aN.r9hO.yfUB92>2(QS!2ff[f[,5

* |

V 2D f\(0MfZ'!mQ2.nQfN=fOi29\f ` !m(aN.B9hOffNBt(
2(S!Bf[fn,25

K* { $

V 2CD }
2(S !Bf[fn,25

f\(0aQ.[QQfN=t.(B ` !9GU.9hOffUB(


$-A

fi

$ 2D*

3MfiuM%

2MmW y:
7
*D+.5N&('06<Z/&y W!<2RU'*),+sL+ V 1*</-08&MC+T/2;43 /lY[&(8&('*+T),&('*<2/28:3 RU'/
/2;*+.8+yRN<F3t-08& 2+,)./2RN&('& Y>RU/<s/282RU696+.83 '*H'*&k-08& 2+,)./2RN&('& YRU/<s&(D05URU6Q3 /2RN&('7e %& V -05N+./+.'*+,<<TRN<
-08&MC+,HyLRU/2;S3\8+,HO1*)./2RN&('Y8& V ^!g
E^!g=n3(<RU'-08&& Y]& Y]g_;7e` Afe
7p
4RN<
/2;Q1*<),& ~ ~),& V -05N+./+9e
82691 V +.'Q/<Y[&(810'*H0+,).RNH*3 D0RU5URU/XI& Y!G
4]WaG k3 '*HI2CD
!W73(<L+.5U5h3(<u<+ V R~H0+,).RNH*3 D0RU5URU/XS& YqD7G hW3 8+>/2;*+<B3 V +l3(<RU'/2;*+-08&& Y]&
g_;7e7`MOh/2;*+P),&('*</28:3 RU'Q/<L+@1*<+,HTL+.8+3 5U8+M3(HOXFHORN<),&('0'*+,)./+,He
r;*+.'82105N+,<Z3 8+\8:3 '06+f~$8+,</282RN)./+,HW!G:
7p
4D+.5N&('06<>/& L+ V 1*<2/-08&C+
/2;43 /P/2;*+Z/282RU696+.8>& Y/2;*+\),&('*<2/28:3 RU'/>)M3 'tD+lH0+,HO1*),+,HY8& V $n+
*)
4fW}D010/P'*&(/@RU/<P&(D05URU6Q3 /2RN&('7W{3 '*H
/2;*+,<+E-08&(D05N+ V <DG+.5N&('06\8+,<-G+,)./2RUC+.5UXs/
& 3 '*HI),& c~ ea%& V -05N+./+.'*+,<<),& V +,<_Y[8& V /2;*+@-43 82/2RN)f~
105o3 8)M3(<+EL;*+.8+9 RN<+ V -0/XeqG 7
4RN</2;1*<),& ~ ~),& V -05N+./+9e
2DG DG+.5N&('06<E/& ` L;*+.'S82105N+,<uRU'QC&(5UC+,H3 8+>8:3 '06+f~$8+,<2/282RN)./+,Heg_;*&(1069;/2;0RN<

-08&(-+.82/XjH0&+,<l'*&(/l3 -0-+M3 8lLRU/2;3 'kR VV +,HORo3 /+Yn&(8 V 105o3 /2RN&('& YJ/2;*+i-08&(D05N+ V W!RU/lDG+,),& V +,<l&(DO~
CRN&(1*<L;*+.'/2;*+i-08&(D05N+ V RN<l</:3 /+,H3(<Yn&(5U5N&MLJ<M H0&+,<l/2;*+.8+T+fRN</Z3<+,w1*+.'*),+i& Y698:3 -0;*<
"
N * F6F6F *r * Q W{L;*+.8+T
" N * F6F6F *r RN<l3 'D~H0+.82RUC93 /2RN&('3 '*
H Q RN<>/2;*+iHORN<[m&(RU'/

10'0RN&('t& 1
3 '*H XJ L W73s-08& m+,)./2RN&('Y[8& V /
& 3 '*Ht3s-08& 2+,)./2RN&('SY8& V XJ L /&I3F^0? G W
\ K \ U

IT<21*);/2;43 /lY[&(8Z+.C+.82Xt698:3 -0
; ,*
\q K 0W N0f6RTY[&(8Z+.C+.82X V 3 -0-0RU'0B
6 & XJPN L
RU'Q/g
& m)
W kRN<u'*&(/>3i-08& m+,)./2RN&(' :[
e u&(/2RN),+Z/2;43 /E'*7
& D+fY[&(8o
+ G RU'<21*);j3s<+,w1*+.'*),+l/282RU696+.8<
/2;*+Z),&('*<2/28:3 RU'Q/s= XJON L H0&+,<E'*&(/@-08& m+,)./ERU'/g
& A3 '*Hy/2;43 />3 5Ua
5 2W PK W<:3 /2RN<mY[XIRU/\=[<2RU'*),+ XJ L
-08& 2+,)./<J/& AfW0/2;1*<u3 5U`
5 q& Y!/2;*+P<+,w1*+.'*),+>3 8+>),&('*<2RN</+.'Q/M
e Q +.'*<2108+,</2;43 / XJ L -08& 2+,)./<
RU'Q/&\3 /_5N+M3(<2/J&('*+u698:3 -0;I& Y{/2;*+u<:+,wQ1*+.'*),+9WOL;0RN):;3 5U5N&MLJ</2;*+@3 D&MC+uYn&(8 V 105o3 /2RN&('& Y{/2;*+u-08&(D05N+ V e
%& V -05N+./+.'*+,<<JY[&(5U5N&LJ<Y[8& V /2;*+E-43 82/2RN).105o3 8)M3(<+>& Y{'*+.6Q3 /2RUC+P),&('*</28:3 RU'Q/<Me
8&& YY[&(8EsD RU'/2;*+)M3(<+& Y8:3 '06+8+,<2/282RN)./+,HS82105N+,<@RN<@<2R V RU5o3 8MERU'/2;*++f~
-08+,<<2RN&('& YG/2;*+-08&(D05N+ V 3 D&MC+9W/2;*+H0+.82RUC(3 /2RN&('RN<{'*&ML3
' $t1
lfi
4~H0+.82RUC93 /2RN&('7W/2;*
+ ),&('*<2RNH0+.8+,H
3 8+J&('05UXZ/2;*+J&('*+,<&(D0/:3 RU'*+,Hs3Y[/+.8q/2;*+3 -0-05URN)M3 /2RN&('s& Y73@82105N+_Y8& V D]W3 '*B
H Nf6R $Yn&(8q+.C+.82X V 3 -0-0RU'06
& JON L RU'Q/& uRN<8+.-05o3(),+,HIDX $Yn&(8+.C+.82Xs698:3 -0;I/2;43 /)M3 'ID+
\~H0+.82RUC+,HIY8& V :e
'OY[&(82/210'43 /+.5UXW*8:3 '06+f~$8+,<2/282RN)./+,H),&('*<2/28:3 RU'/<3 8+E/282RN):KRN+.8J/&</21*HOXG{RU'Q/210RU/2RUC+.5UXWG),&('*<2RN<2/+.'*).X
);*+,):KRU'06I<;*&(105NHD+,),& V ++M3(<2RN+.8u/2;43 'LRU/2;S6+.'*+.8:3 5h),&('*</28:3 RU'Q/<MWD010/E/2;*+l8&(5N+Z& Y]RU828+,HO10'*H*3 '*).X
RN<s<2/2RU5U510'*).5N+M3 8Meg_;*&(1069;RU/iRN<i+M3(<Xk/&k);*+,):K/2;43 /s LRU/2;8:3 '06+8+,<2/282RN)./+,H
),&('*<2/28:3 RU'/<RN<3 /5N+M3(<2/\ ~$;43 8H=/28:3 '*<mYn&(8 V 3 /2RN&('Y[8& V ^!g
E^!gA\3 '*HL+;43,C+F-08&C+.'
=/2;*&(1069;RU/RN<J'*&(/JRU'*).5U1*H0+,HIRU'F/2;0RN<-43 -+.8BA/2;43 /JRU/RN<RU'B ` =R$e+9e
` AfWOL+>HORNHT'*&(/ V 3 '43 6+P/&

+
3();0RN+.C+l3 'I+f03()./),& V -05N+fRU/XF8+,<2105U/_Y[&(8/2;0RN<-08&(D05N+ V e
rt+iHORNHt'*&(/+.RU/2;*+.8 V 3 '43 6+i/&3(<<2RU69'3I),& V -05N+fORU/Xj).5o3(<<PY[&(8>/2;*+\{. } 3 '*H
s: } t-08&(D05N+ V <W0/2;*&(1069;D&(/2;F-08&(D05N+ V <J/282RUCRo3 5U5UXTDG+.5N&('06\/& e
%& V -05N+fORU/X8+,<2105U/<]&(D0/:3 RU'*+,H\RU'/2;0RN<!-43 -+.83 8+<21 VV 3 82R M+,HRU'\/:3 D05N+9e*`9e}rt+3 5N<&E-08+,<+.'/]RU'
x{RU6*e`Mb3 ),& V -05N+fORU/X V 3 - E+ V -0;43(<2R ,RU'06\/2;*+E8+.5o3 /2RN&('*<2;0RU-*<D+./L+,+.'I-08&(D05N+ V <Me 'T/2;0RN<*69108+9W
RY9 H0+.'*&(/+,<s3<:+./& Y@D0RN),&(5N&(8+,H698:3 -0;*<S=82105N+,<i&(8i),&('*<2/28:3 RU'Q/<BAfWX W W 8+,<-G+,)./2RUC+.5UX
H0+.'*&(/+sRU/<>8+,</282RN)./2RN&('k/&y3F*'0RU/+s+fO-43 '*<2RN&('k<:+./MWh8:3 '06+8+,<2/282RN)./+,H+.5N+ V +.'/<MWq&(8HORN<:),&('0'*+,)./+,H
+.5N+ V +.'Q/<e H0+.'*&(/+,<@3i<+./u& Yq'*+.6Q3 /2RUC+Z),&('*<2/28:3 RU'/<Meu5U5{-08&(D05N+ V <u8+.-08+,<+.'Q/+,Ht3 8+),& V -05N+./+
Yn&(8P/2;*+.RU8>).5o3(<<Me HO6+,<3 8+\HORU8+,)./+,HtY8& V DG&(/2/& V /&T/&(-7e'j+,HO6+ZY[8& V 3T-08&(D05N+ V `/&3
-08&(D05N+ V V +M3 '*<\/2;43 / `TRN<\3-43 82/2RN).105o3 8)M3(<+F& Oey&(8+,&MC+.8MWRU'&(8H0+.8/2;43 //2;*+ V 3 8+ V 3 RU'*<_8+M3(H*3 D05N+9W-08&(D05N+ V <_L;0RN);3 8+RU'Q/+.8 V +,HORo3 /+ED+./L+,+.'F/L&Z-08&(D05N+ V < `@3 '*H l& Y7/2;*+
<:3 V +),& V -05N+fORU/X).5o3(<<MWH0&T'*&(/>3 -0-+M3 8@RU'/2;*+l*69108+9eZg_;*+),& V -05N+fRU/X& Y<21*):;-08&(D05N+ V <P)M3 '
D+>&(D0/:3 RU'*+,HyDQX ).5o3(<:<2RYXRU'06 @/2;*+ V RU'/2;*+;0RN+.8:3 8);QXeEx*&(8uRU'*<2/:3 '*),+9WGD7G kRN< V &(8+
6+.'*+.8:3 5G/2;43 'i } =L;0RN);sRN<&(D0/:3 RU'*+,HiRY
" A3 '*H V &(8+<2-+,).R4)J/2;43 '2 D7G

@

|

F





U







F


l












* |




=






* |
@

K* { $




K

K$ 2D*

K* {


K 2D*
Kv { $
Kv { $
AD*

K AD*



6

6


K* { $



$-W

K* { $



Kv { $

F




fi M]|9M.tM=uwuMM3
F



Z^

Z^]
Z^]
Z
[

Z[F]
Z[F]

Z\!]

Z[\!]


[

ff

ff&
|fi5 ff
5 ff
5 ff

&:; 5<2
<"2
<"2
&:; 5<2
<"2

&

\

>

>

>
<2
<2
<2
>
&:; 5<2

>

&

>

>
>
>

>
<
"<2

[m&\

[&r\
$

&
>
>
>
<

<

<

<

<


>
>
>

5ff

fi5 ff
5fiff
5ff
5ff

]"

[&r\
]

$

>

5ff
5ff

ff5

>
ff5.&:; 5<2
"<2
&:; 5<2
"<2

>

| 5ff
5ff
| 5ff
ff5 |
5ff

5ff
ff5

5 ff
5ff



[&r\

$ ]
>
fffi5 5ff



5ff
ff5fi5ff

5 ff
5ff



g}3 D05N+\`9^1 VV 3 82XF& Y]%& V -05N+fRU/XI+,<2105U/<

v{$=L;0RN);t3(H0H0</2;*+><+./ AfWG3 '*HWG<2RU'*),+>/2;*+,<+P-08&(D05N+ V
<&ZRN<2D*
| ! e

<@3 8+PD&(/2;<:+ V R~H0+,).RNH*3 D05N+9W

h bS4M94"! $#;
z'*+RU'Q/+.8+,<2/2RU'06Z8+.5o3 /2RN&('*<2;0RU-iY8& V 3 's3 5U6&(82RU/2; V RN)uCRN+.L-G&(RU'/RN<LRU/2;s/2;*+E%^ Y[8:3 V +.L&(82Ke!+f~
)M3 5U5G/2;*+RU'0-010/_& Y{3),&('*<2/28:3 RU'Q/J<B3 /2RN<mYn3()./2RN&('s-08&(D05N+ V = P A!RN<3>),&('*<2/28:3 RU'/_'*+./L&(82KW0),& V -&9<+,H
& Y3I<+./& Y_C93 82Ro3 D05N+,<MW{<+./<>& YJ-&9<<2RUD05N+\C(3 5U1*+,<PY[&(8P/2;*+C(3 82Ro3 D05N+,<T=[)M3 5U5N+,Ht/2;*+.RU8ZH0& V 3 RU'*<BA>3 '*H3
<+./& Y]),&('*</28:3 RU'Q/<uDG+./L+,+.'y/2;*+PC93 82Ro3 D05N+,<Meg_;*+>w1*+,<2/2RN&('RN<JL;*+./2;*+.8u/2;*+.8+PRN<E3<&(5U10/2RN&('y/&/2;*+
%^ W*R$e+9eq3 '3(<<RU69' V +.'Q/& YhC93 5U1*+,</&Z/2;*+@C93 82Ro3 D05N+,<J/2;43 /J<:3 /2RN<m4+,<J/2;*+P),&('*<2/28:3 RU'Q/<e
g_;*+Z),&('*<2/28:3 RU'/<ERU'QC&(5UC+,HSRU'3i).5o3(<:<2RN)M3 5 > 3 8+Z<2R V -05N+.8u/2;43 '&(108<MeE)./2143 5U5UXW P ),&(8m~
8+,<2-&('*H0</&/2;*+E_{ =-08& 2+,)./2RN&('aA_-08&(D05N+ V e
^+.C+.8:3 5P3 10/2;*&(8<s'*&(/2RN),+,H/2;*+<2/28&('06+,w10RUC(3 5N+.'*),+yD+./L+,+.' P 3 '*H
}4
F *7$ =$?uRUC+.'/L&y5o3 D+.5N+,Hj698:3 -0;*o
< 3 '*H kW{RN<>/2;*+.8+T3;*& V & V &(82-0;0RN< V Y8& V
/&
AfePu<uYn3 8@3(<EL+ZK'*&MLPW/2;*+l*8</u-43 -+.8@&('S/2;0RN<@<210DOm+,)./EL_3(<=[x*+,H0+.8> ]3 8HOR$Wq`Mb9b Afe '
=[1069'0RN+.8MW_999A),&(828+,<2-G&('*H0+.'*),+,<s3 8+IH0+./:3 RU5N+,HY[8& V

=$?uRUC+.'/L&k^0?EZ
< 3 '*H
RN<Z/2;*+.8+3y-08& m+,)./2RN&('Y8& V /&
A/& P @Wh3 '*H8+,).RU-08&)M3 5U5UX=[H0+.C+.5N&(-0RU'06j/2;*+I&('*+,<
-08+,<+.'/+,HTRU'S=[1069'0RN+.8J%;*+.RU'7W`Mb9b9A2Afeq|+./1*<_&(10/25URU'*+u/2;*+uRNH0+M3(<_& Y/2;*+u/28:3 '*<mY[&(8 V 3 /2RN&('TY[8& V
P /&
hW)M3 5U5N+,H
e%&('*<RNH0+.8i3S),&('*<2/28:3 RU'Q/i'*+./L&(82K
e
RN</28:3 '*<mYn&(8 V +,H

RU'Q/&/L&y^0?E< 3 '*H
3(<@Yn&(5U5N&MLJ<M e /28:3 '*<25o3 /+,</2;*+Ff[f0,[l& > +M3():;k
),&('*),+.-0/l'*&H0+RN<
6+.'*+.82RN)T3 '*H),&(828+,<2-&('*H0<l/&y3IC(3 82Ro3 D05N+T3 '*Hk+M3();k8+.5o3 /2RN&('k'*&H0+s),&(82 8+,<2-G&('*H0</&S3I),&('*<2/28:3 RU'/

n {

* |

$


{ v$G*{ $


.

{

{

$ {
{ 7{*
9* T{


@|G*{ $

=RU/<[/2;t'*+.RU69;QD&(8>RN<>/2;*+\),&('*),+.-0/>'*&H0+\),&(828+,<2-&('*HORU'06/&F/2;*+o$/2;jC93 82Ro3 D05N+\& Y/2;*+\),&('*<2/28:3 RU'/BAfe
8+.-08+,<+.'Q/</2;*+(*f[2(paQ <q*[[[(*fh/2;*+.8+PRN<&('*+@RU'*HORUCRNHO143 5{),&('*),+.-0/'*&H0+uY[&(8+M3();C93 5U1*+
& Y_3sC93 82Ro3 D05N+\H0& V 3 RU'7W{3 '*H&('*+Z8+.5o3 /2RN&('t'*&H0+lYn&(8P+M3();j/210-05N+& Y_),& V -43 /2RUD05N+C93 5U1*+,<MeZ&(1069;05UX
<:3 RNHWG/2;*+.8+lRN<P3<:&(5U10/2RN&('y/&
RY]/2;*+.8+lRN<@3 V 3 -0-0RU'06iY[8& V C(3 82Ro3 D05N+,<\=[),&('*),+.-0/E'*&H0+,<@&
PA/&

C93 5U1*+,<i=[),&('*),+.-0/>'*&H0+,<>& Au/2;43 /<:3 /2RN<24+,<@/2;*+),&('*<2/28:3 RU'/<s= V 3 -*<>8+.5o3 /2RN&('j'*&H0+,<>& YH&('/&
8+.5o3 /2RN&('k'*&H0+,<& AfW0 5W 5T3F-08& 2+,)./2RN&('tY[8& V /& keig_;*+i<:3 V +\8+,<105U/P;43(<>D+,+.'3():;0RN+.C+,H
RU'*H0+.-+.'*H0+.'Q/25UXFRU'I/2;*+@_/2/282RUD010/+,HS?u8:3 -0;y?u8:3 VV 3 8Yn&(8 V 3 5URN< V DXFJ1*H0&(5YJ=m`Mb9b9cAfe





0

{

$ 2D*

z'*+\),&(105NH3 5N<&F<+,+ P 3(<3s-43 82/2RN).105o3 8)M3(<+& Y_ 7
*>RU'*H0+,+,HW/2;*+.8+RN<>3
-08& 2+,)./2RN&('Y[8& V 3s^0?RU'Q/&T3i^0?
RY]3 '*H&('05UXFRY
<:3 /2RN<m4+,</2;*+>-&9<2RU/2RUC+),&('*<2/28:3 RU'Q/uLRU/2;
3 '+ V -0/Xs/282RU696+.8u3 '*H3(<RU/<&(D05URU6Q3 /2RN&('7e



2

$-

fi

3MfiuM%

Z[\%]4 1A2*/

3 014%&657 */1,8%9

Z[F] C*=>A2 &

Z[F] 4 1A2*/

2>:(+* ;< */1,81%9
Z[F] B*/>A2 &

Z[EDGFH4\!]4 1A2*/ :(+*-;< *=1,81%9
Z[4 1A2*/

Z J [\ L DGFH ]4 1A2*=

;< *=1,81%9

Z[ED9FH4] C*/>A2 &
$ %'&)(+*-,%.*/0,01 .2&



Z[ID9FHJ4 1A2*/

Z[ \ c ]4 1A2*/






Z[ ]4 1A2*/
Z8^]4 1A2*=
Z8^] B*/>A2 &

Z[ \#] 4 1A2*/
Z\ c ] 4 1A2*/

?







?



Z[ \ ] 4 1A2*/



2; $

Z[ ] C*/CA2 &

Z[ ]K4 1A2*/

Z8^] C*/CA2 &
2@ $

Z[ ] B*/>A2 &

; $

Z^] 4 1A2*=
Z[ 4 1A2*/
Z8^
4 1A2*=

Z^] B*/>A2 &

x{RU69108+\`MbO%& V -05N+fORU/XF+,<2105U/<q3\?E+,&(698:3 -0;QX





@ $

fi M]|9M.tM=uwuMM3
F

u&MLPWRU'&(8H0+.8T/&H0+M3 5uLRU/2;RU'*),& V -05N+./+SK'*&L5N+,HO6+yxa3 8269RN+.8T+./F3 5$eE=m`Mb9b9AMLT+f/+.'*H/2;*+
% ^ Y8:3 V +.L&(82K/& V RO+,H~%^ e '3 V R0+,H~%^ /2;*+<:+./i& YPC93 82Ro3 D05N+,<iRN<sH0+,),& V -&9<+,HRU'/&

),&('Q/28&(5U5o3 D05N+T3 '*HS10'*),&('Q/28&(5U5o3 D05N+iC93 82Ro3 D05N+,<MW<:3MX 3 '*HFeZg_;*+s|NPOX>f,uRQTSGU)VTWfiX1Y[Z)\9]+\

^`_ X1a _ X1S6ZbVTcedfZS9ghYPcji+XkKlJm`nKopcfi\rqU)ds\9cfi\9aGX1dta2u ^`_ cfiq _ q2ZdvVX6SGXw>U)S9YyxTWZaGXkzZ)\`w>U)WeWfiU ^ \2{|cfi\}cea~a9S9xsX
_ ZayX1X1S9gz\GU)WexTa9cfiU)daGUha _ X\9xTVTdsX1a ^ U)S9]cedsk+xsqXkVtgq2ZdVXPXiKaGX1dskTXkaGUZM\GU)WexTa9cfiU)dUwa _ X
^`_ U)WfiXdsX1a ^ U)S9] IrlJnKcfi\`\ _ U ^ daGUVX6lBqU)YPQTWfiX1aGX
_ cfi\`SGX\9xTWea`QTSGUcfikTX\~xs\}ZdsU)a _ X1S
QTSGUKUwUw` lBqU)YPQTWfiX1aGX1dsX\G\yw>U)S6|lBqU)ds\9cfi\9aGX1dsq1gdskTXXkuZdgzYPcji+XkKlJm`nKoq2ZdVXba9SZds\9WZaGXk
cedtaGUZdceds\9aZdsqXzUwy'sfE\9cedTa _ X)[SGXk+xsq1a9cfiU)dkTX\Gq1S9ceVXkZVU2X
u}a _ X
YPcjiTXkKlJm`nKocfi\YZQTQXkaGUnT\h[Zdskcfi\a _ X1dpQTSGUcfikTXk ^ cea _ ^ UqU)WfiU)SG\u`
cecedTZ
QU
\9cea9ceXIqU)ds\9a9SZceda<Pu ^`_ U
\GX<a9S9ce
X1Scfi\a _ XI\9xTVT
SZQ _ qU)S9SGX\9QU)dsk+cedT}aGU~a _ X\9xTVTdsX1a ^ U)S9]cedsk+xsqXk
Vtgh _ XYPcji+XkKlJm`nKocfi\bqU)ds\9cfi\9aGX1dacjwrZdskU)dTWegcjw}\Za9cfi\9fX\bv7ces
vceWeWexs\9a9SZaGX\a _ cfi\
a9SZds\w>U)S9YZa9cfiU)dE _ X6qU)ds\9a9SZceda}dsX1a ^ U)S9]Mcfi\~qU)YPQU
\GXkhUw7a _ Xa ^
ZS9cZVTWfiX\GX1aG\22`
Zdsk Zdska _ SGXXqU)ds\9a9SZcedaG\v u} Zdsk|" Zdsk _ ZX\ZYXkTU)YZced
t)yKy Zdskz>yZdsk _ Z2XP\ZYXbkTU)YZcedy _ XPqU)ds\9a9SZcedakTXsdTcea9cfiU)ds\ZSGXb
ceX1dced
_ Xs
xTSGX
IrWeWqU)dsqX1QTa}agQX\}ZSGX6\9xTQTQU
\GXkhaGUbVXcedsqU)YPQfZSZVTWfiX








e

+

b
b

b 2 2
b 1 3
1 1

>


7




1




1

C

2

2

1

1


2
fiG

2

1
1
2

<

3

fi

fi

3
1

1
1

<

2



2
2

1

7

3

2

>



2
3



fiKG

2
b 1



C
H

C

ce
xTSGX
+{SZds\w>U)S9YZa9cfiU)dvwSGU)YBIrlJnKaGUb|'sf
X1ayxs\6SGX1WZaGXU)xTSkTXsdTcea9cfiU)ds\aGUvU)a _ X1SkTXsdTcea9cfiU)ds\UwqU)ds\9a9SZcedtaG\ywU)xTdskzceda _ Xm`RWeceaGX1SZl
a9xTSGX
rxTS~qU)ds\9a9SZcedaG\yWfiX1a`xs\`q2ZWeWa _ X1YnTrlBqU)ds\9a9SZcedaG\`ZSGXZQfZS9a9cfiq1xTWZS}q2Z)\GX6Uw7a _ XYPcedTceYZW
kTX\Gq1S9ceQTa9ceXqU)ds\9a9SZcedtaG\kTXsdsXkvcedceVTcfiXX1aZWCeu2

{`ZbYPcedTceYZW<kTX\Gq1S9ceQTa9ceXqU)ds\9a9SZcedtaq2Zd
VX~\GXX1dZ)\|Z6\GX1aEUwnTrlBqU)ds\9a9SZcedaG\ ^ cea _ _ X~\ZYX}a9S9ce
X1S2tceaG\Ecedta9xTcea9ceX\GX1YZdta9cfiq\cfi\<Bcjw _ U)WfikT\
\GUYyxs\9a~ U)S U)S6ee2nT\Za9cfi\fX\`ZYPcedTceYZWkTX\Gq1S9ceQTa9ceXqU)ds\9a9SZceda`cjwcea\Za9cfi\fX\~Za
WfiX2Z)\9a|U)dsXrX1WfiX1YX1daUwa _ X\GX1a2<U)aGX~a _ Za|a _ X`k+cfi\xTdsq1a9cfiU)d+rkTUKX\IdsU)a|cedsq1SGX2Z)\GXra _ XrqU)YPQTWfiXi+ceaBgUw
_ XqU)ds\9cfi\9aGX1dsq1gbq _ Xq]SGX1WZa9ceXaGUnTrlBqU)ds\9a9SZcedaG\2 _ XQTSGUKUwUwa _ XU)SGX1YbqU)YPQTWfiXi+ceaBgPUwf|
'sf7rq2ZdVX6xs\GXkvaGU\ _ U ^ _ ZaqU)ds\9cfi\9aGX1dsq1gUwYPcedTceYZW<kTX\Gq1S9ceQTa9ceXqU)ds\Ga9SZcedtaG\cfi\
ZWfi\GUr lBqU)YPQTWfiX1aGX
IceVTcfiXX1aZWC2

_ Z2X}QU)cedtaGXkMU)xTaa _ ZaYPcedTceYZWkTX\Gq1S9ceQTa9ceXqU)ds\9a9SZcedaG\
X1dsX1SZWece2XYU
\9abqU)ds\9a9SZcedtaG\w>U)xTdskceda _ Xhm`WeceaGX1SZa9xTSGX
z`q1a9xfZWeWegua _ X\XWZa9aGX1SPqU)ds\9a9SZcedaG\
ZSGXyZWfi\GUbQfZS9a9cfiq1xTWZSq2Z)\GX\}UwEnTrlBqU)ds\9a9SZcedaG\'w>U)S`ceds\9aZdsqX
uZ)\}ZWeSGX2Z)k+gMdsU)a9cfiqXkusa _ X6aGU)QU)WfiU)
cfiq2ZW
qU)ds\9a9SZcedaG\`xs\GXkVtgvcedsX2Zxvcfi\\Z)U)xTcCu2

ZSGX6k+cfi\qU)dTdsXq1aGXkvnTrlBqU)ds\9a9SZcedtaG\ X1axs\`Z)kTk
_ Za2u7ceda _ X\GXMm` ^ U)S9]K\u7qU)ds\Ga9SZcedtaG\PZSGXxs\GXkaGUvq _ XqG]qU)ds\9cfi\GaGX1dsq1gUwrnT\y\GU)WfiX1WegZdskdsU)a


fiff fffi fi !fi#"%$& fiff'(fi")!fi*'+*
,-.

fi/1032547698;:=<>25?@A4B

UwS9cfiq _ X1S6]dsU ^ WfiXk+XbVfZ)\GX\6qU)YPQU
\GXkzUwS9xTWfiX\>Z)\cedzDCZdska _ X1gZSGXbdsU)acedtaGX1
SZaGXkcedaGU
YU)SGXqU)YPQTWfiXihSGX2Z)\GU)dTcedT\>Z)\`cedDEzU)S`cedFCGE
_ X1SGX\ _ U)xTWfikVXU)a _ X1SMqU)dTdsXq1a9cfiU)ds\ ^ cea _^ U)S9]K\hZVU)xTaX1S9cjfq2Za9cfiU)dUw6]dsU ^ WfiXk+XVfZ)\GX\
qU)YPQU
\GXkUw6WfiU)
cfiq2ZW}S9xTWfiX\'wU)Sceds\GaZdsq=
X HrU)S9dS9xTWfiX\u|dfZYX1Weg ^ cea _ _ X ^ U)S9]K\Uw X1gZdsk
U)xs\\GX1a2
7 Jusced ^`_ cfiq _ qU)ds\9a9SZcedtaG\rZSGX}6\ua _ xs\ _ ZXa _ X\ZYXrw>U)S9Ya _ ZdMU)xTSG\u+VTxTa ^ X
k+cfikhdsU)asdskk+ceSGXq1a}SGX1WZa9cfiU)ds\ _ ceQs\}VX1a ^ XX1dva _ X1ceS`wSZYX ^ U)S9]ZdskU)xTSG\
\MVU)a _ YUKkTX1Wfi\ZSGXzSGUU)aGXkced\GX1YZda9cfiqdsX1a ^ U)S9]K\uqU)YPQfZS9cedTqU)dsqX1QTa9xfZW
SZQ _ \Zdsk
kTX\Gq1S9ceQTa9cfiU)ds\bWfiU)
cfiq\bcfi\PZvQTSGU)VTWfiX1Y _ Za _ Z)\PUwaGX1d VXX1dcfi\G\9xsXkL
K`Z
Z)kTX1S2uIU)WeceaGU)S2u<ZdskU)VTcfiX\
2

_ Z2XcfikTX1da9cjfXk ZwSZ
YX1dtaUwa _ X|pYUKkTX1Wr ^`_ X1SGX\9ceYPQTWfiX
SZQ _ \ZSGXPSGX\Ga9S9cfiq1aGXkaGU
_ U
\GX _ ZKcedTbZa9SGXXlCWece]X\9a9S9xsq1a9xTSGX
uKVTxTa|qU)d)xTdsq1a9ceX}agQX\`ZSGXrZWeWfiU ^ Xkf ^ cea _ Z6WZdT
xfZXq2ZWeWfiXk

E&M
NDCPO {a _ cfi\RX QxTce)ZWfiX1dsqX _ Z)\WfiXkaGUZ`dsX ^ a9SZ)q1aZVTceWeceaBgySGX\GxTWeacedkTX\q1S9ceQTa9cfiU)dWfiU)
cfiq\

HU ^ X1X1S2u
a9S9gKcedT}aGU`cfikTX1da9cjwgWZS9X1SwSZ
YX1daG\\XX1Y\aGU`VXEZ}kTX2Z)kKlBX1dsk{Z)\QU)cedtaGXkU)xTaVgvxT
dTcfiX1S>


u
QTSGU9Xq1a9cfiU)d q2ZdTdsU)a _ Zdsk+WfiXdsX1tZa9cfiU)dU)dQTS9ceYPcea9ceXaBgKQX\rda _ XU)a _ X1S _ ZdskuX1X1d _ XPYU
\9a
Xi+QTSGX\G\9ceXkTX\Gq1S9ceQTa9cfiU)dhWfiU)
cfiq\`q2ZdTdsU)a`XiKQTSGX\\a _ X ^`_ U)WfiX6 F
J
SfiT UVEwSZ
YX1dtay KU)S9
cfiksZ+u2
7
dsqUKk+cedT\GU)YXhXi+cfi\9a9cedTkTX\Gq1S9ceQTa9cfiU)dWfiU)
cfiq\cedtaGUYUKkTX1Wfi\Uwa _ XM|"w>ZYPceWegcfi\ZdcedtaGX1SGX\Ga9cedT
QX1SG\9QXq1a9ceX
uKa _ Za~qU)xTWfikhZWeWfiU ^ U)dsXaGUcfikTX1dta9cjwgdsX ^ kTXq1cfiksZVTWfiX6q1WZ)\\GX\|wU)SU)xTSYUkTX1Wfi\usZ)kTkagQX
Xi+QTSGX\G\9ceX1dsX\G\`aGUqU)dsqX1QTa9xfZW
SZQ _ \2usZdskhYZ2gMVX6q1g+q1WfiX\`ceda _ X6kTX\Gq1S9ceQTa9cfiU)dUw< \}qU)dsqX1QTaG\2

WYX[Z]\
^D_3`baFcdfi\&^
e X _ ZXQTSGU)QU
\GXk Zw>ZYPceWegzUwYUKkTX1Wfi\6a _ Zayq2ZdVXP\GXX1dZ)\a _ XPVfZ)\9cfi\6Uw~ZX1dsX1S9cfiqPYUKkTX1WecedT

wSZYX ^ U)S9]6ZcedwX2Za9xTSGX\6Uwa _ cfi\}wSZYX ^ U)S9]zZSGXa _ Xw>U)WeWfiU ^ cedTs{Zq1WfiX2ZS6k+cfi\9a9cedsq1a9cfiU)dzVX1a ^ XX1d
k+)c fX1SGX1dta6]KcedskT\6UwE]dsU ^ WfiXk+X
ua _ Zasa ^ X1WeW ^ cea _ ceda9xTcea9ceXq2ZaGX1U)S9cfiX\2uZxTdTcjw>U)S9Y
SZQ _ lCVfZ)\Xk
WZdT
xfZXba _ Za]XX1Qs\6X\G\GX1dta9cZW<QTSGU)QX1S9a9cfiX\UwEa _ XnT YUKkTX1WCudfZYX1WegvSGX2Z)ksZVTceWeceagUwU)V+9Xq1aG\6Z)\
^ X1WeWIZ)\rSGX2Z)\GU)dTcedT\2 e X
xsX\G\a _ cfi\}WZa9aGX1SQU)cedtacfi\}QfZS9a9cfiq1xTWZS9WegceYPQU)S9aZdawU)Sra _ Xxs\ZVTceWeceaBgvUw
Zdtg]KdsU ^ WfiXk+XVfZ)\GXk \9gK\GaGX1Yd U)xTSywSZYX ^ U)S9]uZWeW]cedskT\Uw~]dsU ^ WfiXk+XZSGX
SZQ _ \X2Z)\9ceWeg
cedtaGX1S9QTSGX1aGXkuZdskSGX2Z)\GU)dTcedT\q2ZdVXP
SZQ _ cfiq2ZWeWegSGX1QTSGX\GX1daGXkced ZhdfZa9xTSZW|YZdTdsX1Sxs\GcedTa _ X

SZQ _ \~a _ X1Y\X1WeX\2uTa _ xs\rXiKQTWZcedsXkaGUPa _ Xxs\GX1S}U)dhceaG\~U ^ dvYUKkTX1WeceZa9cfiU)d
Xq _ dTcfiq2ZWqU)dta9S9ceVTxTa9cfiU)ds\2u ^ S2 a2<QTSGX1KcfiU)xs\ ^ U)S9]+\EU)dqU)dsqX1QTa9xfZW
SZQ _ \2uq2ZdVX`\GxTYPYZS9ce2Xk
Z)\w>U)WeWfiU ^ \2{

g _ XSGX1QTSGX\GX1daZa9cfiU)dUwk+c)fX1SGX1da<]cedskT\7Uws]dsU ^ WfiXk+X~Z)\qU)WfiU)SGXknT\{w>Z)q1aG\uced+w>X1SGX1dsqX|S9xTWfiX\2u
X1U)WexTa9cfiU)dS9xTWfiX\}ZdskqU)ds\9a9SZcedaG\2

g _ XcedaGX1
SZa9cfiU)dvUwqU)ds\9a9SZcedaG\cedtaGUZySGX2Z)\GU)dTcedTPYUkTX1WCKYU)SGXU)SWfiX\G\\9ceYPceWZSdsU)a9cfiU)ds\`Uw7Z

qU)ds\Ga9SZcedta _ )
Z kzZWeSGX2Z)k+gVXX1dcedta9SGUKk+xsqXkVTxTa ^ X1SGXbU)dTWegxs\GXkaGUq _ XqG]vqU)ds\9cfi\GaGX1dsq1gvUw|Z
\GceYPQTWfiX
SZQ _ > Z)\cedha _ X|zYUkTX1W'I _ X6qU)YPQTWfiXi+ceaBghUw<qU)ds\9cfi\9aGX1dsq1gMq _ XqG]KcedT ^ Z)\`dsU)a
]KdsU ^

X U)VTaZcedsXkwCZYPceWegzUwYUKkTX1Wfi\ ^ cea _
Z qU)YPQTWfiXiKceagzq1WZ)\\9cjfq2Za9cfiU)d
g ZM\GgK\9aGX1YZa9cfiqb\9a9xsk+gzUwa _ P
UwEZ)\G\Uq1cZaGXkqU)ds\Gcfi\9aGX1dsq1g3hkTXk+xsq1a9cfiU)dQTSGU)VTWfiX1Y\2ufcedsq1Wexsk+cedTa _
X \Ga9xsk+gMUwQfZS9a9cfiq1xTWZSq2Z)\GX\
Uw<S9xTWfiX\rZdskhqU)ds\Ga9SZcedtaG\2u ^`_ cfiq _ QTSGUcfikTXcedtaGX1SGX\9a9cedTqU)YPQTWfiXiKceaghSGX\9xTWeaG\2

e XyZWfi\UPX\9aZVTWecfi\ _ XkMWecedT]+\`VX1a ^ XX1dqU)ds\9cfi\9aGX1dsq1gMq _ X qG]KcedTZdskv kTXk+xsq1a9cfiU)dufa9SZds\9WZa9cedT
_ XIqU)ds\9cfi\GaGX1dsq1g3
h kTXk+xsq1a9cfiU)dQTSGU)VTWfiX1Y\cedaGX1S9Y\Uw+ kTX k+xsq1a9cfiU)d<Ba\ _ )U xTWfik6VX<dsU)a9cfiqXka _ Zaa _ X
U)QX1SZa9cfiU)dfZW7\GX1YZda9cfiq\rUwYUKkTX1Wfi\}qU)YVTcedTcedTS9xTWfiX\ZdskvqU)ds\9a9SZcedaG\2ufdfZYX1WeghFCG
E uTFCZdsk

E <u+cfi\`X2Z)\9gaGUbxTdskTX1SG\9aZdskhVTxTa ^ X ^ X1SGX6dsU)a}ZVTWfiXaGU
ceXZ
WfiU)VfZWWfiU)
cfiq2ZW\GX1YZda9cfiq\2IdskTXXku
,-i

fijlk4Pm1n5oDprq47s5@)6utvn5wyx1<>q4rz{0>?>|}m1n5?>z~6B05@A?36z

_ X1SGXcfi\IZdPxTdskTX1S9WegKcedTdsU)dPYU)dsU)aGU)dTcfiq`YXq _ ZdTcfi\GY ^`_ U
\GXWfiU)
cfiq2ZWscedaGX1S9QTSGX1aZa9cfiU)d\ _ )
U xTWfikbSGXRQxTceSGX
U)dz\9aZdsksZSGkWfiU)
cfiq\2r _ XykTXsdTcea9cfiU)dUwEZWfiU)
cfiq2ZW<\GX1YZda9cfiq\}w>U)S}a _ X\XYUKkTX1Wfi\}cfi\ra _

x \ZdU)QX1d
QTSGU)VTWfiX1Y

_r^#\5`fi>D&>^
cuX
e XZSGXMcedskTX1VTaGXkaGUcfiq _ X1Wm _ X1cedZdskX1dsX1Kc1XvnceYU)dsX1aPwU)SPa
X2ZS9WecfiX1S6X1SG\9cfiU)dUwEa _ cfi\ ^ U)S9]Zdsk _ X1WeQ+wxTWqU)YPYX1daG\2 e XbZWfi\GU ^ cfi\ _
Q xTcfi\2uTZ)\ ^ X1WeWZ)\|ZdsU)dgYU)xs\SGXw>X1SGXX\2uw>U)Sa _
U)xs\\GX1a|ZdskocfiX1S9SGXZS

_ 1X ceSq2ZSGXwxTW`SGX2Z)k+cedTUwZd
aGUa _ ZdT]ZS9cfiXlJm _ S9cfi\Ga9cedsX
X1ceS|cedtaGX1SGX\9a9cedT\GxT
X\9a9cfiU)ds\

Zdsk\GU)YX6qU)S9SGXq1a9cfiU)ds\

*>^F_>c
rVTceaGX1VU)xTWCuneu>H}xTWeWCu eusrcZdxub2
7F5{7fiu'RE`kTk+cfi\U)d+l e

X\9WfiX1g

K`Z
Z)kTX1S2ueuU)WeceaGU)S2u euU)VTcfiX\2unI2

SZ)q1aZVTWfiXZdskXq1cfiksZVTWfiXPsSZ
YX1dtaG\yUw~m|U)d+l
qX1QTa9xfZW7SZQ _ \2Id1**{l


5)
usQTQ3t

rt
7+
nQTS9cedTX1S2
K`ZX1a2us lBeuX1dsX\9a2u

eu
b
xT
dTcfiX1S2u l 2


dsU ^ WfiXk+X `qRQxTcfi\9cea9cfiU)d
Z
oxTSGX
SZQ _ lbK`Z)\GXk
dsU ^ WfiXk+X X1QTSGX\GX1daZa9cfiU)d UkTX1W rQTQTWecfiq2Za9cfiU)d
^ cea _
aGU
_ X
ncfi\9gKQ _ xs\9lB mZ)\GX
na9xsk+g

**
Z2)ZceWZVTWfiX
Za
Z 9g-q2
Z h'ynK* h'6 e h'6 e
hfZ'XG\ _ a9Y
_ a9a{h7h\GX9d xsq2ZWet'

K`ZX1a2u(s lBeu xT
dTcfiX2u l >

+y _ Xb|ZYPceWeg{<i+aGX1ds\9c)ds\w`nceYyTWfiXm1)dsqXTa9xfZW
Z' _ \IBd**{5{3 r)u3s
7K+2+
K`ZX1a2u(s lBI>

+b5R5b]r>5~75~]l>'rr3>3Rlu
X1WeWecfiX
5*RR7 Pu7LRr>>b }o _ -bsa _ X\9cfi\2u}dTceXG \GceaBg9 wE) dta
G

K9
cfiksZ+u2
7J7rda _ X{GX1WZa9ceXrXi3GX\G\9ceX1dsX\G\1wkTX\Gq9cATa9c)dW)
cfiq\|Zdsk]3GXk+cfiq2ZaGXrW)
cfiq\2
{ 1 >b Ar55 utu>777JK
K
\2umeu5K)aA+u(KeuDd _ R 1 _
uo7<2

6Kk3ecedTdskncAyx3a9cedT=Hrx3Gd}K _ Kc
a9x>'
~' _ 2 Id1{l
>
{'u2u33sK

+nu39cedT2
cea _ m1) dsqRT
_ dsk~+u(Peueceduo7 I27
M{Ta9cAG'IcAy31dta~a9c)d)dqR)d)xTdsq1a9ceGQx9cR
ceP
a9c )>
'
k ~a y>R2<Bd*
'Dv>{(L[&[*{5u
u
35
7K
+
_ 1cedueu7

xT
dTc2 u l `2

m1)dsqRTa9x>'r~' _ 2{<7xTdsk'1dta~'r)a9c)d23
& >b )r5~ 1 1u
u>7J7rt7J+

_ 1cedufeufxT
dTc2uf l euncA)d1a2uP2

`{RGak~' _ 2{ ~' _ lfi>k9d jl
k+ 3R1dt~a a9c )dpKk3 cea _ nGdta9cfiqR2Bd1*G9 tuY3D
''7's
9r

9
x+w GdTd
m1)
c2ufeuf xTced>'fik3Tu2
7eced'rk3RGq9cATa`wqR)dsqRTa9x>'7~' _ dsk9qw
eg
Pc'c _ c aR9a2d=1*Y*l
r'
uRsu3T7J7K7Kfnu39cedT2
m1)x3)dsk
uKneu"n'ea2ut~2

ocqR RexTa9c)d{l 'Gk3 '9oDq1a9ceRd[1
*{l
l 7
>7t3
u 3 )27+nu39cedT2

,-

fi/1032547698;:=<>25?@A4B

cATc
ulseuHy
ureu~ )cxurn2

nGdta9cfiqv'ecfika9c)dwm1)dsqRTa9x>'
~' _ IB

**{l
&&{R>7u33s
K7+nu39cedT2
>'9
c2uHeu dTsufeunKq _ ciu6I2
7Jbvcji3kqR)d9a~cedta~a9cwfi)q1a9c)d{w~' w
k q1c 9c )d933~xTdsk3~cedsqRy31ad k+
Ed1*#*{{{3 u
u32
+
3
k32u<6eu<D'Gk+cCu<2
7z)d)a)d)d>)k+cfiqhnKodskm1)dGa~cedtana9cwfi)q1a9c)dd
1~~u'*u7G
<
u 3
J+2 J

+
R2uhb>




b37
r 7*5 5~!3
& 7731off-bfiffR 2usfi e
}G

7u





7ffu
Kmeu e x T~
u
bE2
7h Aqo#uw~oKqRk+x3wyfi hm1sqRfi9x>'
~'fiff}o)7~'2E1D*{l
r'&&{rsu3
)+nu3 T2

ufbeu R
uTeuTnKq'GqRATus2

<m1y>' =w<n9xsq9x3~'7m`nKoqRyl
ffKk321**{3 u3>
''

+

Gk AR2u)b2

oq {fi jfq oKqRk+x3w'3x m1sqRfi9x>'s~'fiff2
1*u
F
#
R&
{'52 u3
f+JK
7
+
e
u'P2
7J!+xsq eE"ffR R { jfq '
esqR
K 15R 1Y5# r
!5R>fi
u3
2


+m`n |oIx33 fiq 2

1%$uf&Peu )x2us 'Jm2
7J# jfq LwD( )+ KlRKl)zm1~ '

ffq% TsE1#{{3 )u3
7
+


* xu|b e euE* ~)xfi Cu }2

"ff 3R
~ wnG
fiqm1~
+
sRq fi9x>'~'fiff`n$2,1**l

{{'32uu3f

+Tnu3 T2
1

xT-fi 2uK ' 2
7~' e hnuq ' e Mw`m1sqRfi9x>'~'fiffDu7u5
*.3G5fi &R F >b Ar~1D
u
u
7
'7s
xT-fi 2u ' <
>


{( )+ 3R~ } ~fi T{>~)97~'fiffH'
fifffi E,1D*{l
)u3)2
+nu3 T2
xT-fi 2u ' eumff u2
7J 3R
/3R6qRfi> ~sqRR~ fi2q03R
7~'fiffR21& >b Ar5 Ffi u
u57J+
oY'> A0 )xumH2
'{5fi75 7 7
fi ' e

R$

o72ut~ 2'u
q1x3 e~xfi~e'fi $w33;w"fftx
Du *>G &u
'u}

3 )1 {II% .=uE"ffm1Aq) e Gw60 Io72u
K2 A%ff>
x 2
u K72
'su33>
73'!++

oA2ueuvxT-fi 2u ' euRmff uf2

)- fiqyw{R)~'fiff2y{5fi75
>b )u5~1Y
u u>777K

x3jw9ut2

4 e) Tm1~
}n wfi)q lq5fffifi QtxR|w6q
}~'fiffoF
L
Gqfffi TssG1*>b575 5 7~}&{Fr 7**>
*73Ry~
&G
}Rs
u3
f7
K
7+

nu3 T2

n'e72u~|2

"ffR 32% Tvx Tv7~'fiff~ / 8ffqRsqRfi9x>'|7~'fiffw9'
G' E, 1#*3 u3577J7J
+
n'e72u~eu)vxT-fi 2u ' K2
7JTn)xfiybm1y3Y 'y K)q~ 'mff> fi

~'fiff 3
x R2, **{l
u
&{r7
u33'tK7J
+nu3 T2

,-,

fijlk4Pm1n5oDprq47s5@)6utvn5wyx1<>q4rz{0>?>|}m1n5?>z~6B05@A?36z

n +u&s|I2
'v75~b3 3RDR771~u959r1
fi ' e R$


nKq$2u uf27
7"ff$%0 ':'; A(fffi ~'Gqff$& {5bfi51u(u
K
+

"fftx
uT2+o3(<315=39xfiT?> fiqff3 ff*>)qffM1 1
7R(@u3577
57&&
u )

e T2u<2
7ym1sqRfi9x>'~'fiffA A9'J{3 )- fiq
y,v11*l
u'

rRs
u3
5
77K

,-3B

fi
ff
fi
! #"$ %
'&)( *,+.-//-102-346574!( 3

89:;< =)((?>
/!(@BA: %&=DC1>
/-

EGFIHKJMLONPHKQ1R<NTS9SUWVYXZNTQ[\Q9[^]_Na`bXZJdc\S1efLONPHKQ1RhgiQ1NTSJ)]dF^ej`bXZelkmQRnHJMXpoqJ)X
NsrItuJ)v)e,[wgxQNTS1J)]dF^eyrzU|{2He,L

}p~$ffYn~2

,M92Z

#P~2#2'#u~$

q22M792

b~$B

$M92Z

Zff

6z7M92Z2

'u2ff'

2ffM92Z

TKBq
6
Bd ?,!l
?

B?)

qq Zf! qB
9B

PqZ
B9.BqM?ff? !P M.?M
!PB??q1!?, ^I ?9\ff
. fi

7?B? 19#q| ff19
q fn ?9#6B! qP ? !d
<1
9.d 9!? 6B?!"
7Z?9 .q! #
7.)$ !B?D?%9ZB .?
.
&1 6?q '&().)?9??*?96? q?B?!9B 6B.,+-?%9q
. q&+-?9! d? !! % 9B?!,ff q fi/1 q! . 01?M

: 8< 7 =?
> ff@ AB 6B!q ? !C()
u
87 9;
12 34 .Bq951 96<
+D % !B . .BqC+D !1 $qE
% \.d9.B?q9Y ? 9&F ?

G 9 ^ 6qH7
, qK?^,ff qJ ?d5 B92
5% ?9uq ,9 ^
? ?9&F qqqqP?^d#7
?K
% Lff P M?9BP % !2
q !
?u?
9) 9&Ou
N B M+D % 9B?! . .Bq8+D !! )?q q Bfi

% ?q% P7
!B6 !5
? 6 D/n
\RQS q!,.u zq !1 T?% 9q
.Bq9UTV 4W d?
6B? | *% q?.

XZY\[]

^)_a`fbH^

]

c dfhghiffjIkmlnhopfhqr'iGsStusSvSiffwMsZdyxRfwMlzsSi)iL{J|lpiffjv-nhjmkIjmn vRr'xnhonh||LiffsRs-vSfn*ozn xRqhi~}n xlzivt$fh'lzj'fhxwMn vRlpfj
e
sSfr'x|Liffs\nhjmksSixR}ulz|Liffs&xRfwnhjtdymfj'ihcetusSvSiffwMs&vRmn v\sRr'dmdfhxRv&sR'fhxRv\r'vSvSixnhjm|Liffs,vSfsSiffopiff|LvMn
dyn xRvRl|rmozn x\rmjm|LvRlpfjvRmxRfr'qnsSvRn vSiffw\iffjv6sRry|nhs;cun t|LxRiffkmlpvM|n xk5|Lfoopiff|Lvfhx6d"ixsSfjuHvSf
dixsSfjmUn xi8sRnff}ulzj'qM|LfwMdynhjmlpiffs*wlzozozlpfjysGfh)k'fozozn xsadix*thiffn x ~idyopf thiffkEsStusSvSiffwMsnhjyk0xRiffsSiffn x|
dmxRfhvSfhv tdiffsiL'lzsSvfhxGn dmdyozlz|n vRlzfjms-srm|\nhsdixsSfjynhoiffwnhlzounhjmk,|nhopiffjmkmn xsffhvSxnff}hiffomnhjykCxiffsSvRnhr'xnhjv
lzj'fhxwMn vRlzfjZ"nhjykEdixsRfjmnhoynhjmgelzjmq5n qhqlznuKnhsSvRn qj'ixl/$nhjmlpiffozlHDffhhe-nhopghixff'xRfwMixff
n xnfftnhjmnhjPffhhePceiffj'iLUrmih~DfozlpxRfjml/)n f'aPivR'ixlzjmqhvSfjZ5fukmk'iffnhr~$oznhsRsPffhhe
cunhjmkmixwMnhjZcuvRr'xwOk'iffj$s$af}hiffsIUxRiffw\ixsCffhheM5eru5n xRxRfoo5n xd"iffjvSixff&ffhh
yL\ ! Giv$vR'ixRiCn xi&sSvRlozowMnhjtOxRiffsSiffn x||mnhozoziffj'qhiffs*|r'xRxiffjvsStesRvSiffwMsPn xi&ozlwMlpvSiffklzjvR'i
lzjvSixnh|LvRlpfjOvRmit0sr'dmdfhxRv5nhjmkOmxlzvSvRopilzjOwMnhjtJxiffsSdiff|LvRs
ylzsdyn dix~lzj}hiffsSvRlpqn vSiffsUw\ivR'fukms)et,mlz|MsSdfhghiffjMkmlznhozfhqr'isStusSvSiffwMsa|nhj zR vSf&sRr'dmdfhxRv
w\fhxRiMjyn vRr'xnho)lzjvSixnh|LvRlpfjfjvR'i\ynhsRlzs$fhUvR'ifflpxdyxRi}elzfrmsiLedixlpiffjy|LihMj'i\Un tvRmn vI|r'xRxRiffjv
sSdfhghiffjkmlznhopfhqr'i&sStusSvSiffwMs$n xRi,rmlzvSiCozlzwlpvSiffklzslzjvRmifflpxsRvSxn vSiqlpiffsfhxk'ivSiff|LvRlj'qJnhjykxRidnhlpxlzj'q
dmxRfhopiffwMsvRmn vIn xlzsRi,lj|Lfj}hixsn vRlpfjZ-srm|nhsIwlzsRrmjmkmixsSvRnhjmkmlj'qskmrmi&vSfEsSdiiff|xRiff|LfhqjmlzvRlpfj
ixRxRfhx,fhxMwMlsRlzjvSixRdmxRivRn vRlpfjndmxRfhopiffw|nhji0k'ivSiff|LvSiffkZavR'iOsStesRvSiffw|nhjifflzvR'ix,vSxnhjmsSix
vR'i\|nhozo-vSfOnJermwMnhj|rmsSvSfwMixI|n xRi\n qhiffjvfhxw\fukmlptlpvRskmlznhopfhqr'i,sSvSxn vSiqhtlzjnhjn vSvSiffw\dmv$vSf


-//-n %%!=6T
=5D1
UZ
!fi;l
~:! %&% q&1%2%1 =

fi

2Zy222 S2-



7M1m



72

xRidynhlzxvR'iMdmxRfhopiffwO,i6|nhjvSxnhlzjsStusSvSiffwMs8vSflzw\dmxf }hi\vR'ifflpxIn lzozlpv tvSfEkmivSiff|LvIdmxRfhyoziffwMs$t
iLudyopflpvRlzjmq6kmlznhozfhqr'iffs$|Lfozopiff|LvSiffklzjlzjvSixnh|LvRlpfjmslpvRermwMnhjrmsSixsmixRiCvRmi,ljmlpvRlznho-sSiqw\iffjvRs
fhZvR'iffsSiPkmlznhopfhqr'iffsan xRiPrysSiffk\vSf&vSxnhlzjMnCGxRfhyoziffwMn vRlz|*$lznhopfhqrmiGxRiffkylz|LvSfhxa5vSfI /ff vRmn v
ndyxRfhyopiffwls8ozlpghiffoztvSfEfe||rmxff 'iMfrmvSdyr'vIfhKvR'iJ~$|nhjiJlzwMw\iffkmln vSiffoptn dydyozlpiffkvSfvR'i
sStusSvSiffwOsGkmiff|lzsRlpfj,fh"'ivR'ix~vSfvSxnhjmsSixGvR'i*|nhozomvSfInIrywMnhj\|rmsSvSfw\ixa|n xRin qhiffjvfhx~lzvG|Lfrmozk
dfhvSiffjvRlnhozopt"iMrysSiffknhs8n|r'iMvSfvRmisStusSvSiffwOs8$lznhozfhqr'inhjmn qhixCvSfw\fukmlptElpvRsiffmn }elzfhxIvSf
xRidynhlzxKdmxRfhyopiffws'nhjmkOi}hiffjdixyn dysuvSfMdmxRi}hiffjv*vR'iffw
jdmxRi}ulpfrmsPafhxRg"ZUi&xid"fhxvSiffkljmlpvRlznhoxRiffsRrmopvRsPfhxvSxnhlzjmlzjmq6n0~$rmsRlj'q6nJ} n xlpivtEfhakmlz
ixRiffjviffn vRr'xi8sSivRsC-nhj'qhgulzozk'ih'nhopghix mxlpqvfhxlzjZy-lpvRwMnhjZffhhenhopghixff-nhj'qhgulzozk'ih
xlpqv$$fhxlzjZlpvRwnhjZ hh "'iffj?nhjynhoptfflj'qvR'idixRfhxwMnhjm|LifhvR'irmooptnhr'vSf
wMn vRlz|,iffn vRr'xRi6sSivUi\iLunhwlzj'iffkml|mnhjmk'ozn iffozopiffkEiffn vRr'xiffsIwMnhk'iMozn xqhi\d"ixfhxwnhjm|LiMlzw,
dmxRf}hiffw\iffjvRs"ryjmk'ix*vR'i&nhsRsRrmwMdmvRlpfjOvRmn vPrmvRr'xRiIUfhxRgOsR'frmokOfu|rmsfjk'i}hiffopfhdlzj'qnhr'vSfwn vRlz|
iffn vRr'xRiffsvRmn vCn dmdmxRf ulzwn vSi\vR'ilzj'fhxwMn vRlzfjEdyxRf }ulzk'iffketEvRmiffsSiMmnhjmkuon "iffoopiffkiffn vRr'xRiffsff 'i
nhjmnhoptusRlzslzjmkylz|n vSiffkvRyn vvRmi&mnhjykuozn iffozopiffk'$D *iffn vRr'xRihZml|Eiffjm|LfekmiffsPP'ivR'ix$vR'i
sSdfhghiffjoznhjmqrmn qhiMrmjmk'ixsSvRnhjmkmlzjmqHff-|Lfw\dfj'iffjvI|n dmvRr'xiffkvR'iMw\iffnhjmlj'qOfhaiffnh|iL'|mnhjmqhi
|LfhxRxRiff|LvRozth~'iffjvRmlsKmnhjmkuozn iffozoziffk6iffn vRr'xRiClzsKnhkmk'iffkOvSf6vR'i8nhr'vSfwn vRlz|iffn vRr'xRiffsylzvlzw\dmxf }hiffk
vR'i*dixRfhxwMnhjy|Li*fhvR'i~$t\nhozwMfsSvUJ ylzsjmkylzj'q$opiffkMrms~vSf8k'i}hiffopfhdJnhjOu-
dmxRiffkylz|LvSfhxnhopghixffKxlzqvKnhjmqhgelzok'ihU hhh| Mnhjmknj'i}hixsRlpfjfh$vR'iavRmn v\Ui
xRidfhxRvafjJmixRih 'i$j'i}hixsRlpfjJfhvR'i$~$vRn ghiffs*nhs5lzjmdyr'v~nCryozopt\nhr'vSfwMn vRlz|}hixslpfjJfhvR'i
u- aiffn vRr'xRihmylz|0Ui8|nhozo H u-
i*vSxnhlzj\nhjmk,vSiffsSv)fhvR,vR'i H u- dmxRiffkml|LvSfhxGnhjmk,vR'i*~$fjn|LfhxRdyrmsDfh"hh
kmlznhozfhqr'iffsI|Lfozopiff|LvSiffklzjnhjiLudixlzw\iffjvRnho)vSxlznhofhK sC J
fiff
sSdfhghiffjkmlznhopfhqr'iCsStesRvSiffwH$fhxlzjZ
lz||n xkyl/ xlpqvffhPiffozoznMfhxlzj-ffhhe
l||n xkml
fhxljZu hhu
*'wMwMlz|v* PozfjmsSf'ZffhhajJvRmlzsUvSxlznhoHvR'i sStesRvSiffwUnhsKlzjysSvRnhozopiffk
n v\nhj |rmsSvSfw\ix\|n xRiO|LiffjvSixff nhjmsSUixRiffk|nhozozsCxfwolp}hi6|rmsRvSfw\ix,vSxn{6|0nhjmk
sRrm||LiffssSryozoptnhr'vSfwMn vSiffk?nozn xRqhijermwC"ix,fh$|rmsSvSfw\ix\xiffr'iffsRvRs;jiL'nhw\dyopikmlnhopfhqr'iJvRmn v
|Lfw\dyozivSiffk;srm||LiffsRsSrmozoptlzsCsR'fjljlpqr'xiE 'id'fj'i6jermwCixs)|n xk;jrmwCixs
nhjmkOdlzj0jrmwCixsKlzj0vR'iCsRnhw\dyopikmlnhopfhqr'iffs*n xRiCn xRvRl|lznhoH
eT

4

4
Vfi
Vfi
1
1
"

(K
Z
(,
.q
#9?ffl$q2T89B.?9q?


LqD?ff%9
! 2#
" 1 V 4 T%$ 3'& ! 2#P
" 1
Nql%n.|I.BD?IB(
& 1 !#!)! 2$2$22+* ,.S- @q/
- !9 0


LqDBlffP%<Bq1
& 1 !#!)! 2$2$22
()

q'

lpqrmxRiM Ucunhw\dopi32fi4D )
5
67698 $lznhopfhqr'i
PfhvSiCvRyn v*vR'iCsRtesSvSiffwsr'vSvSixnhjy|LiClzjEcu|LfjmsRlzsRvRsKfhGn\xRidynhlzxlzjmlpvRln vRlpfjZ'w\fhvRlp} n vSiffket0vR'i
sStusSvSiffwOsCn ylzozlpv tvSfkmivSiff|LvIvRmn vIvR'i6rmsSixffsIrmvSvSixnhjm|Li:);05nhs8ozlpghiffoztvSfmn }hiiiffjwMlzsrmjmk'ixS
sSvSfefek miJqhfnho*fhPvR'i H u- CdmxRiffkylz|LvSfhx\lzs,vSflzw\dmxf }hiOvR'iOsStusSvSiffwOsMn lzozlpv tvSf
k'ivSiff|LvJsRrm|?wMlzsRryjmk'ixsSvRnhjykmlzj'qs 'iEkmlznhopfhqr'iffsMvRmn vmn }hivR'ikmiffsRlpxRiffkfrmvR|Lfw\ih*lzjml|
24-< 5ff

676=8" kmlzn
srm||LiffsRsSrmozopt6nhr'vSfwMn vSiffs*vRmiI|rmsSvSfw\ixffsK|nhoo/mn xRixRiixRxRiffk0vSfMnhsKvRm#
4ff3ff1

fi>

72

> 9?f7A@d.22

opfhqr'iffsff$lznhozfhqr'iffs~lzjMmlz|\vR'i sStusSvSiffw kylzk\j'fhvasrm||LiffsRsSrmozopt&|Lfw\dyopivSiPvR'i|nhozozixffs)vRnhsSg
n xRiIxiixRxiffk0vSfnhC
BEDF
Gy
8 4E2 6 miffsSiIn xRi8k'iffs|Lxlpiffk0lzj0rmxRvR'ix*k'ivRnhlzoiffopfI
ylzs5dyn dix*xRidfhxRvRsKxiffsRrmopvRs5xRfw iLudixlzw\iffjvRs5vRmn vPvSiffsSv*'ivRmixlpvlzs5dfsRslpyopi$vSfopiffn xjOvSf
nhr'vSfwMn vRl|nhozoptdmxRiffkmlz|Lv$vRmn vCnkmlznhopfhqrmi&PlzozoDi&dyxRfhyopiffwMn vRl|CfjvR'iMynhsRls$fhUlzjmfhxwn vRlpfjvR'i
sStusSvSiffwmnhs* ~iffn xoptljvR'i$kmlznhozfhqr'ihenhjyk/UlzjxRiffnho"vRlzw\ihivSxnhlzjJnhj0nhr'vSfwMn vRlz||oznhsRslyix
fhxdmxRiffkml|LvRlzj'q;dmxRfhyopiffwn vRlz|kmlznhopfhqr'iffsxRfw iffn vRr'xRiffsvRmn v|nhjinhr'vSfwn vRlz|nhozoptiLevSxnh|LvSiffk
xRfw vR'i |LfhxRdyrmsffPsMkmiffsR|Lxlpiffkn f }hih5fj'ifhvR'iffsSiiffn vRr'xiffsMlzs\vR'iOfrmvSdyr'v\fh$vR'i
H u$D dmxiffkmlz|LvSfhxffDvR'i H u- iffn vRr'xih)mlz|dmxRiffkml|LvRsI'ivRmixCfhx,j'fhv
vR'i0|r'xRxRiffjvCrmvSvSixnhjm|LiJUnhs,|LfhxRxRiff|LvRoztrmjmk'ixsSvSffuknhopghix\iv,nho/p~ hhh| 'i6xRiffsRrmozvRs8sR'f
vRmn v8lzv$ls$d"fssRlpyopi&vSfJ RR /ff dmxRfhyoziffwMn vRlz|,kmlznhopfhqrmiffsrmsRlzj'qJrmozoptnhrmvSfwMn vRlz|\iffn vRr'xRiffsPlpvRnhj
nh||r'xnh|LtExnhj'qlzjmqMxfw vSf uH ;JZk'idiffjmkmlj'q\fj'ivR'ixvR'iCsRtesSvSiffwmnhssSiiffjfj'i&fhx
vUf,iLu|mnhj'qhiffs~ vUlzsGdfsRsRlzyopiKvSf dmxRfhyopiffwn vRlz|*kmlznhozfhqr'iffsGlzvRnhjJnh||r'xnh|Lt6rmdvSf\hJ
ceiff|LvRlzfjk'iffsR|Lxlpiffs nhjmkvRmikmlznhopfhqr'i|LfhxRdyrys0vRmn vOvR'iiLedixlw\iffjvRsOn xinhsSiffk
fjZceiff|LvRlpfjJ;kmlsR|rmsRsSiffs8vR'iJvted"i6fhwMnh|mlzj'iJopiffn xjmlzjmqnhopqhfhxlpvRywnhk'fhdyvSiffkZ)jmnhw\iffoptKD BEBE8ED
nhjmkqlz}hiffsnk'iffsR|LxlpdmvRlpfjfhvR'iiLudixlzw\iffjvRnhoKk'iffsRlpqj?ceiff|LvRlpfjqlz}hiffsnmxRiffn guk'fjfh$vR'i
iffn vRr'xRiffs5rmsSiffkJlzj6vR'iffsSiiLedixlzwMiffjvRs~ceiff|LvRlzfj08dmxRiffsRiffjvRsUvR'i$w\ivR'fukfhdmxRiffkmlz|LvRlj'qIvR'iiffn vRrmxRi
H u$D 5nhjmkOqlp}hiffsnh||r'xnh|Lt0xRiffsRrmozvRsaceiff|LvRlpfj&dyxRiffsSiffjvRs*w\ivR'fukms*rmsRiffk0fhxrmvRlzozlpfflzjmq
B=BE8ED;vSfvSxnhlzjvRmiMnhr'vSfwMn vRlz|M~xRfhyopiffwn vRlz|,lnhopfhqr'iMGxiffkmlz|LvSfhx8nhjmkqlz}hiffsvR'i\xRiffsRryopvRs,i
k'iffozn t6frmx*kmlzsR|rmssRlpfj6fh-xRiffon vSiffk0afhxRgJvSf6ceiff|LvRlpfjC'iffjJaiC|nhj|Lfw\dyn xRiIlzvUvSf\frmx*n dmdmxRfnh|Z
ceiff|LvRlpfjE\sRrmwMwn xlpiffs5vR'iIdn d"ix*nhjykk'iffsR|Lxlz"iffsUr'vRr'xRiUfhxRg"

LYNMPORQJSUT [SWVYX



lzsn6sSdfhghiffjkmlnhopfhqr'i8sStusSvSiffw ynhsSiffkfjvR'i&j'fhvRlpfjfh -S 1Z H$fhxlzjiv$nho/pDffh
5eru5n xRxfozo 5n xRdiffjvSixff-ffhhjvRmi |nhozo-xRfr'vRlj'q6sStusSvSiffwOsSixR}ulz|LiffsvRmn vvR'i,rmsSix
|nhjnh||LiffsRsCn xRi|oznhssRlyiffklzjvSfL|n vSiqhfhxlpiffsDdyozrysnO|n vSiqhfhxt|nhozopiffk h\u
[ fhxIvRnhsRgesvRyn v8n xRi

j'fhv5|Lf }hixRiffktMvRminhr'vSfwn vSiffkOsStesRvSiffwnhjykJw&rmsSvUivSxnhjysSixxRiffkvSf\n&rywMnhjfhdixn vSfhxIHfhxlzj
iv\nho/pffhhJ~nh||n vSiqhfhxRtk'iffsR|Lxlz"iffs&nkmlixRiffjvCvRnhsSg"asrm|nhs,dixsSfjuHvSf HdixsRfj;kylznhozlzj'q'
fhx8xiff|Lifflp}elj'q|LxRiffkmlzvIfhx&nwMlsRkmlznhopiffkjermwC"ix misStusSvSiffw k'ivSixwMlzj'iffs8mlz|vRnhsSgvR'iJ|nhozopix
lzs8xRiffrmiffsSvRlzj'qOfjvR'inhsRlzsfh*lpvRsCrmjmk'ixsSvRnhjmkmlzjmqJfhKvRmi|nhozopixffs8xRiffsSdfjmsSivSfvR'i6fhd"iffj'Hiffjmk'iffk
sStusSvSiffwqhxiivRlzj'^
q ]_a`b_ca L e
d0$jm|LivR'i\vRnhsSgmnhsiiffjkmivSixwMlzj'iffkvR'i
lzj'fhxwMn vRlzfjj'iiffk'iffkIfhx-|LfwMdyopivRlzj'qvR'i~|nhozopix sxRiffer'iffsSvlsZfhmvRnhlzj'iffkCrmsRlzjmq5kylznhopfhqr'i~sr'yw\fukmrmopiffs
vRmn vn xi8sSdiff|l|fhx*iffnh|EvRnhsSgiffozon,fhxlzjZZffhh
mi
Cf sStusSvSiffw|LfjmslzsSvRs$fh5nhjnhr'vSfwMn vRlz|MsRd"iiff|xRiff|LfhqjmlzixffnOsSdfhghiffjoznhjmqrmn qhiMrmju
k'ixsRvRnhjmkmlzj'qMwMfekmryopihynkylznhopfhqr'i8wnhjmn qhixffnhjmkn6|Lfw\dyr'vSixvSiffopidy'fjt0dyozn vSfhxwOK$r'xlzjmqMvR'i
vSxlznhoHvR'iiffmnff}ulpfhxsUfhnhooyvR'isStusSvSiffww\fukmrmopiffsaUixRinhr'vSfwMn vRlz|nhozoztMxRiff|Lfhxk'iffk0lzjJn,opfhqCopih'nhjmk
ozn vSix,vR'i0kmlznhopfhqrmiffsIUixRi6vSxnhjmsR|LxlpiffktermwMnhjys8nhjmk;ozn iffozopiffklpvRfjmi6fhx\w\fhxRi6fh*vR'iff
vRnhsSg|n vSiqhfhxlpiffs)xRidmxiffsSiffjvRlj'qvR'i6vRnhsSgvRmn v&vR'iJ|nhozopix85nhs,nhsSgulzj'q vSfdixRfhxwO-fj;n
dix8r'vSvSixnhjy|Li6ynhslzs 'iopfhqopiffs8nhosSflzjm|ormk'iffkozn iffozslzjykmlz|n vRlzj'qOP'ivR'ix8vRmiMlpffn xkmnhk
vRn ghiffjf}hix8vR'iM|nhozoDfhxIvRmi,rysSixImnhkermj'qOr'd&r'x$iLedixlzwMiffjvRsrmsRi&vRmiMopfhqJopiffsvSfOiLevSxnh|Lv
nhr'vSfwMn vRl|nhozopt0fhmvRnhlzjmn opiiffn vRrmxRiffsrmsRiffknhs*dmxiffkmlz|LvSfhxsnhjmkOvSf6kmiLj'ivR'i8|onhsRsSiffs*fh)kmlnhopfhqr'iffs
vRmn vaaiPUnhjv~vSf&opiffn xjMvSfCdmxRiffkml|Lv 'i*|LfhxRdyrms~fhZhh8kylznhopfhqr'iffsGrysSiffklzjMfr'x~iLudixlzw\iffjvRsG5nhs
|Lfozopiff|LvSiffkJlzj6sSi}hixnhoiLedixlzwMiffjvRnho'vSxlznhozsGfh
Cf fjJozlp}hi|rysSvSfw\ixUvSxn{6|Cg Plz||n xkml" $fhxlzjZ
hhu=
ewMwMlz|vK PozfjmsSf'ffhhynhjmk0lzsaxRiixxRiffkvSf\nhs5&lzjg lz||n xkml"fhxlzjZ' hh
4ff3"

fi

2Zy222 S2-



7M1m



72

i&kmlznhopfhqrmiffs*} n xRtlzjEopiffj'qhvRZZh n xRiIy}hi&iL'|mnhj'qhiffs$fhx$opiffsRsPlpvRh;fh~nhozovRmi&kmlnhopfhqr'iffs
'
|LfjmsRlsSvRlzj'q,fhfjmopt6vUfiL'|mnhj'qhiffsff
sIw\iffjvRlpfj'iffkn f }hihDkmlznhopfhqrmiffsljmlz|
Cf sRrm||LiffsRsRrmooptnhr'vSfwMn vSiffs8vR'iM|rmsSvSfwMixffs
|nhozo/5nhslozozrmsSvSxn vSiffklzjlpqr'xRi Kn xRiExRiixRxRiffkvSfnhsA24- 5)
67698 vR'ix6|nhoozsUmlz|n xRi
dmxRfhopiffwMn vRlz| Zn xRiMkmlz}elzkmiffklzjvSf0vR'xRiiM|n vSiqhfhxlpiffsff miCxsSv$|n vSiqhfhxRth-xiixRxiffkEvSfnhs 4i
j
BZ
xRiffsRryopvRsxRfwn0|rmsSvSfw\ix sk'iff|lzsRlpfjEvSfOmnhj'q0r'dfjvR'i&sRtesSvSiffwI sRnhw\dopi 4i
j
Bkylznhopfhqr'i
lzslzjlpqrmxRi&e|nhozopix$wMn tEmnhj'qJr'd"iff|nhrysSi&ls k'i&ls*xrmsSvSxn vSiffklzvRvR'i,sStusSvSiffwOfr'xqhfnho
lzs5vSfJopiffn xj0xRfw vR'i8|Lfhxdyrms5ml|OsStusSvSiffw iffmn }elzfhxsKopiffkOvSfMvRmiI|nhozopixffsKxrmsRvSxn vRlpfjZ
eT

4

4
Vfi
Vfi
1

(K
Z
(,
.q
#9, !l!q?
Nq< * !q?I
7 *?9/
P
.?'\
\
6on !< !Pff n!9m
? 99u ?G9 q!J !K
7T,G?^ffP%
%C1?|<9B q)n)<p"
n
<.<PG?ffP% * , G=\, 1 @d, ff 0
Nq)n#Tff%l q<qI.B9
tRu%v +
rq;
lpqrmxRi8eUcunhw\dopi 4i

j
B $lznhopfhqrmi

mi5sRiff|Lfjmk,dmxRfhyoziffwMn vRlz|U|n vSiqhfhxRt\ w \x 4DyDhxRiffsRryopvRsDxRfwn$ermwMnhj,|rmsSvSfwMixG|n xRi*n qhiffjvs
k'iff|lzslpfjvSfvRn ghif}hix8vR'i6|nhozo)xRfwvR'isStusSvSiffwO6aiff|nhrysSi lzsiLud"ixlzw\iffjvRnho/Ziffnh|;|nhozo
kmr'xlzj'q5vR'iGiffozkIvSxlnhohUnhsDw\fjmlzvSfhxRiffkIetInermwMnhjCn qhiffjvsSixR}ulzj'qnhsDn*Plpffn xkIP'f|Lfrmozk8f }hixRxlzk'i
vR'isRtesSvSiffw 'ixiMaixinjermw8ixIfhKn qhiffjvRs8P'fdyn xRvRl|lpdyn vSiffknhs8lpffn xkmskyr'xlzj'q0vR'ivSxlznho
fh nhjykOiffnh|Elpffn xk05nhsslzw\dyoptvSfozkOvSfvRn ghiCf }hixvR'iI|nhozolzDzs kmiIdix|Lifflp}hiffkOdmxfhyopiffwMs
lpvR8vR'i5sStesRvSiffwOsDd"ixfhxwnhjm|Lih mi~lpffn xkZs-kmiff|lzsRlpfj8Unhsopfhqhqhiffk&etvR'iaiLudixlzw\iffjvRnhosRivRr'd
xRiffsRryopvRlzj'q&lzj0ozn iffozozlzj'q8vR'iI|nhozonhsKfj'i$vRmn vKvR'i$lzffn xkJvSffhgJf }hixffK-|LfrmxsSi$aiI|nhj0fjyoptlzj'ix
mn vUwMlpqv~mn }hiw\fhvRlz}n vSiffkJvR'iPlpffn xk\vSf&vRn ghif }hix5vR'i$|nhozo/r'vGUiPnhsRsRryw\ivRmn vavR'iPlpffn xk
mnhkqhffukxiffnhsSfjfhx$k'flzj'q6sSf'kmlznhozfhqr'iI'ixi8vR'iClpffn xkEk'iff|lzk'iffkvRmn vPvRmiCkmlznhozfhqr'i85nhs
dmxRfhopiffwMn vRlz|$nhjmkOvSfefhg0f }hixvR'iI|nhozolsKsR'f PjlzjOlpqr'xR3
;e
eT

4

4
Vfi
Vfi
1
1
"

Z(K(,
* -{, 1
| }0

.q



? "+9 K%?
~ z?2d*WndC.q1
SF ?ffd?BZT & ff?9&qE(;(5
. dCnn?z. 1 $q(

()?|)Cq. 6'
mD Lq D?ff%9
F & !
$Dqd 9Td.dffP%K & ! 2"
2 "a1 V 4
? "
+ 9 D9? 6zq?ff
% <qB.'
Nas .
( q-+ u
lpqr'xRi3;e5cunhw\dyopi)w

vSiffw

* ',.-?/|
?qfi,o!
0

gx 4Dylnhopfhqr'i

iMvRmlpxkdmxRfhyopiffwn vRlz|\|n vSiqhfhxRthGvR'iP2fi4-<5
p4 D8kmlznhopfhqrmiffs-n xRiJ|nhsSiffsC'ixRiMvRmi6sStus

|LfwMdyopivSiffkvR'iE|nhozo/~r'vM|n xRxlpiffkfr'vMnvRnhsSgvRmn vM5nhsjmfhvMvR'iOfj'ivRyn vMvR'i|rmsRvSfw\ix
4ff3 2

fi>

> 9?f7A@d.22

72

5nhs8nh|LvRrmnhozoztxiffr'iffsRvRlzj'q'jiL'nhw\dyopi24-<5
<4 D8kmlznhopfhqr'iMlsqlp}hiffjlzjDlzqr'xRi\'
Cf
lzjvSixRdmxRivSiffkEr'vSvSixnhjm|Lb
:nhsnxRiffrmiffsSvvSfJwMn ghiCnvRmlpxkuHdn xRvt0|nhozoZih q'KvSf 5HP,[ehM
[eh" vR'iffjnhsRghiffkvR'iJ|nhozopixIfhx8vR'i6lzj'fhxwMn vRlpfjlpvCj'iiffk'iffkvSf|n xRxRtfr'vIvRylzsvRnhsSg"
vR'i8|nhoopix*|Lfw\dyozlziffkZ'nhjmkOvR'i8sRtesSvSiffw|Lfw\dyopivSiffkvR'iI|nhozo/
eT

4

4
Vfi
Vfi
1
1
"
"
2

Z(K(,
* -{, 1
| }0

.q



~

W

1

? "+9 K%? z?2d* ndC.q
q6G )STC%F )n?
Nql DffP
%
)n.||.B *qm%!I?

1!? ff
%


? #. -L
ql
DffP
%
lBB.
?9 4$4 $4

Nql
%
n.|I.BD?I
B

@
ff
=u@
44 4
() q'




r



&



b



'")" )&)&

& $$ p"'" p&p&* fi|

C,

(

0

lpqr'xi'Ucunhw\dyozi#24-<5
<4 D8J$lznhozfhqr'i

Yb%Q b Q] XKQ b ]
$r'x0iLudixlzw\iffjvRs0n dmdyoptvRmiwMnh|mlj'iopiffn xjmlzjmqdmxRfhqhxnh
w BEBE8=D
HUfmiffjZ&ffhheCffhhJvSf
hn r'vSfwMn vRl|nhozopt|oznhssRlptvR'ikmlznhopfhqr'iffsJnhs0dmxRfhopiffwMn vRlz|fhxsRrm||LiffsRsSrmo/D B=BE8ED lzs6n;nhsSvOnhjmk
iL{6|lpiffjv*xryopiopiffn xjmlzjmq&sStusSvSiffw k'iffsR|Lxlpiffk0lzjOw\fhxiIk'ivRnhlzolzjH5f'iffjZffhhe-ffhh"Ui8k'iffsR|Lxlz"i
lpv*yxlpi ytJ'ixRiIfhxP|Lfw\dopivSiffj'iffsRs
B=BE8EDlzsynhsSiffkOfjvR'i R yH RR RL
Z
/ C~5$nhopqhfhxlzvRmwk'iffsR|Lxlz"iffklzj?mr'xj'gexnhj' lzkyw\ixff~ffhA
BEB=8EDlzw\dmxRf}hiffsfj/ Ca
lpvR0nhjlj'fhxwMn vRlpfjJqnhlzjOw\ivSxl|PvSf\qrylzk'i$xrmopidmxryjmlzj'q&nhjmkOn\Eljmlzw&rmwiffsR|LxlpdmvRlpfjOiffj'qhvR
fhx~$ZHynhsSiffk'iffr'xlzsRvRlz|Gfhx~k'ivSixwlzjmlzj'q'f ?wMnhjtCxrmopiffssRmfrmozk&i5oziffn xj'iffkOsSiiUfmiffjOffhhe
ffhhEfhx,w\fhxRi0k'ivRnhlzoslpghi6fhvR'ix\opiffn xj'ixse
BEBE8EDvRn ghiffs,nhs&lj'dyr'vvR'i0jmnhw\iffsCfh*nsSivCfh
p avSf&i$opiffn xj'iffkvR'ijmnhwMiffsUnhjyk6xnhj'qhiffsUfh} nhozr'iffsUfhnCmuiffk6sSiv5fh Ih mnhjmk
Z
hH sSdiff|lptulzj'qvR'i\|oznhsRs$nhjmkEiffn vRr'xi&} nhozr'iffsfhx$iffnh|iL'nhw\dyopi,lzjn6vSxnhlzjmlzj'qJsSivvRsPfrmvSdyr'v
lzsUn ff ahh
Mff fhxUdmxRiffkmlz|LvRlj'qIvR'i$|oznhsRsUfhr'vRr'xRiiLunhw\dopiffsuiLedyxRiffsRsSiffkJnhs5nhj6fhxk'ixRiffk
sSiv*fhlpHvR'iffjOxryopiffs
opvR'fr'qnhjtfj'i\fhUn0jrmwCix$fhaopiffn xjmixs$|Lfrmozki&n dmdozlpiffkEvSfJvRylzsdmxRfhyopiffw"Ui\mnhkn
jermw8ixKfhDxRiffnhsSfjms*fhx|'fefsRlzj'N
q BEBE8ED)lpxsSvmlzvU5nhslw\dfhxRvRnhjv5vSfMin yopivSf6lzjvSiqhxn vSiIvR'i
xRiffsRryopvRs~fh-n dmdyoptulzj'qvR'i$opiffn xjmixaynh|g6lzjvSf8vRmi sRd"fhghiffjJkmlznhozfhqr'iPsStusSvSiffwOG~xRi}ulpfrmsaafhxRg
sRr'qhqhiffsRvRs*vRmn v*vRmiIlpHvRmiffjOxrmopiffs5vRyn C
v BEB=8EDrmsSiffsKvSfMiLudmxRiffssKvR'iIopiffn xjmiffk|oznhsRsRlp|n vRlpfjOw\fuk'iffo
n xRi5iffnhsSt&fhx)difhdyoziavSfIrmjyk'ixsSvRnhjmk0H5n vRopivSv"ffhu '5f'iffjZmffhhuwMn gelj'qlzvDiffnhslpix)vSfIlzjvSiqhxn vSi
vR'iJopiffn xj'iffkxrmopiffsIljvSfvRmi sStusSvSiffwOEceiff|Lfjmke
BEBE8EDsr'dmdfhxRvRs|LfjvRlzjrmfrms)sStuw8fozlz|
nhjmkvSiLevRrynho~yn qsSivIiffn vRrmxRiffsOHUf'iffjZKffhhGylzopiMfhvR'ix,opiffn xjmixssRrm|;nhsM5oznhsRsl|n vRlpfj
nhjm
k *iqhxRiffsRslpfjMvSxRiiffsH5'
*UxlziffwMnhjZmxlpiffkmwMnhju$ozs'iffjZ cevSfj'ihffhmk'fCj'fhvUsRr'dmdfhxRv
vSiLuvRrmnho~n qEiffn vRr'xRiffs 'ixRin xiJsSi}hixnho~vSiLuvRrmnhoUiffn vRrmxRiffsCljvRmlzsCkmn vRnhsSiv&vRmn v&dmxRf }hi0rmsSirmo
lzj|oznhsRsRlztulzj'qvRmikylznhopfhqr'iffs $jmiEfhCvR'iEiffn vRr'xRiffs0vRyn vJUilzsRmiffkvSfrmsSi5nhs6vRmisRvSxlzj'q
xRidmxiffsSiffjvRlj'q8vR'ixRiff|LfhqjmlzixffsKtedfhvR'iffsRlzsff ylzsalsasRrmdmdfhxRvSiffklz
j BEB=8EDiff|nhrmsSivR'ixRilzsUj'f
h olzwMlpvRn vRlpfj$fjCvR'iUsRlpiGfh'vR'iUsSiv 'i~rmsRirmoj'iffsRsZfhuvR'iavSiLevRrmnhoiffn vRr'xRiffsDlzsiLuiffw\dyozlpyiffklzj
ceiff|LvRlpfj6eH ;e~lzjmnhozopthhdyxRi}elzfrmsafhxg&lzjMmlz|,aiynhkMn dmdyozlziffkCfhvRmixaopiffn xjmixsvSf8vR'i H u-
4ff3

!

fi

2Zy222 S2-



7M1m



72

8dmxRiffkylz|LvSfhxffGr'vRlzolpfflzj'qvRmiJiffsSv,dixRfhxwMlzjmqiffn vRrmxRisSiv,lpvRvR'i0vSiLevRrynho5yn qiffn vRrmxRiffs
xRiffw\f}hiffkZKsRr'qhqhiffsSvSiffk?vRmn vMUi|Lfrmokj'fhvMiLud"iff|Lv6nhjtsRlpqjyl|nhjv\d"ixfhxwnhjm|Lilzw\dmxf }hiffw\iffjvRs
xRfw rmsRlj'q,fhvR'ixopiffn xjmixs8nhopghixPivPnho/py hhh|
jIfhxkmixvSfvSxnhlzjIvRmi~dmxRfhopiffwMn vRlz|)kmlnhopfhqr'iGdmxiffkmlz|LvSfhxUa$UrD BEBE8=DMrmsSiffs-nsSiv-fh'iffn vRr'xiffs
sakmlsR|rmsRsSiffk6n f }hih'lzjmlpvRlnho'iLedixlw\iffjvRsasR'faiffkJvRmn vUvR'iPynhjmkuozn iffozopiffkJ'$D )iffn vRrmxRih
ml|iffjm|Lfuk'iffs-'ivR'ixDnhj8r'vSvSixnhjm|LiUynhsiiffj8wMlsRrmjmk'ixsSvSffukfhxDjmfhv lsmlpqmoztkmlsR|LxlzwMljmn vSfhxRt
lzjJlzk'iffjvRlptelzjmq8dmxRfhopiffwMn vRlz|Pkmlznhopfhqr'iffsffGfai}hixffnhozoyvRmiiffn vRr'xRiffsKrysSiffkJvSf\vSxnhlzj6vR'i~$w&rmsSv
iavSfhvRnhooptCnhr'vSfwn vRlz|KlpmUi*n xRi5vSfIrmsSi5vR'iK~$lj&nUfhxRgulzj'qsSdfhghiffj,kmlznhopfhqr'i5sStusSvSiffwOjCfhxk'ix
vSflzw\dmxRf}hivR'iJdixRfhxwMnhjm|LiMfhvR'irmozoptnhr'vSfwMn vRl|Ja$-DUiJk'i}hiffozfhd"iffknrmozoztnhr'vSfwn vRlz|
n dmdmxfff'lzwMn vRlpfjfh$vR'imnhjykuozn iffozopiffkiffn vRrmxRih5mlz|ai|nhoo5vR'i H u$D &iffn vRrmxRih
lzjsSidyn xn vSiiLedixlw\iffjvRs0PlpvR
BEBE8=D mivSxnhlzjmlzjmqfh&vR'i H u$D Jiffn vRrmxRilzs
kmlzs|rmsRsSiffk0lzjEcuiff|LvRlpfje

~}elzkmiffjm|LiKxfwdmxRi}ulpfrmsvSxlnhozsfh
Cf sRr'qhqhiffsRv~vRmn v~lzvGlzs)lzw\dfhxRvRnhjv)vSf8lzk'iffjvRlpt8dmxfhyopiffwMs
lpvRylzjn|Lfrmdyopi0fhPiL'|ynhj'qhiffsnhjmkhfhPvR'iOkmlnhopfhqr'iffs\lzj;vR'i|LfhxRdrms\n xRi0y}hiOiL'|mnhjmqhiffs
fhxCoziffsRs ermsiffn vRr'xRiffs8fhxIvRmi\yxsSvv afiLu|mnhj'qhiffs&n xRiMiffjm|Lfuk'iffkslzjm|Li,vR'iMqhfnhoalzsvSf0 RRh
nhlzozr'xRiffs&ifhxRi0vR'it;yn dmdiffjZ miJiLudixlzw\iffjvRnhoKn x|mlpvSiff|LvRr'xRiOfhvR'ialzs\lzozormsSvSxn vSiffk;lzj
lpqr'xRie mlzs,sRmf s\'f
B=BE8EDlzs\rmsSiffk;yxsSvCvSfdmxRiffkmlz|Lv H u$D &fhx\vRmi0yxsSv
nhjmk;sSiff|LfjmkiLu|mnhj'qhiffs mlzs8iffn vRrmxRi6lzsCiffklzjvSfvR'i0~$nhopfj'qElpvRvR'i6fhvRmix&nhr'vSfwn vRlz|
iffn vRr'xRiffs miPfr'vSdr'v~fhvRmi$a$k'ivSixwMlzjmiffs~mivR'ixavRmisStusSvSiffw|LfjvRlzjrmiffsufhx5lznCdyxRfhyopiffw
lzsDdmxRiffkylz|LvSiffkZ vR'iK$lznhopfhqr'i*nhjmn qhix~wMn tCnhkmn dyvlzvRs)kmlznhopfhqr'i5sSvSxn vSiqht&fhx)vSxnhjmsRix)vR'iK|rmsRvSfw\ix
vSfnM|rysSvSfw\ixPn qhiffjv
culjm|Lih ; fhCvR'ikmlznhopfhqrmiffs6|LfjmslzsSvSiffkfhIfjmoztvUfiLu|mnhj'qhiffs$UiiLu|ozryk'iEvRmisRiff|Lfjmk
iL'|mnhj'qhiPiffn vRrmxRiffs)fhxGvR'fsSi*kmlnhopfhqr'iffs'ixiKvR'i*sSiff|Lfjmk\iL'|ynhj'qhiP|LfjmslzsSvRsfjmoptCfhvR'i*sStusSvSiffw
dyozn telj'qn|opfsRlzj'qdyxRfw\dmvDinhozsSfiL'|ozrmkmiffk\nhjt,iffn vRr'xRiffs)vRmn valzjmkmlz|n vSiffk&vSfIvRmi|oznhsRsRlpyixvRmn v
vR'i~sRiff|Lfjmk8iL'|ynhj'qhia5nhsvRmiaoznhsSviLu|mnhj'qhi5lzjvR'i~kylznhopfhqr'ih-iU|Lfw\dn xRiGxRiffsRryopvRsZfhx RRh
Z
dmxRfhopiffwMn vRlz|5kmlznhozfhqr'iffshlpvR,xiffsRrmopvRsDfhx Ly


R
x
h
f


p

ff


w

n
R
v

l
U
|

k
z
l
h
n
p

h
f

q
'
r
ff



h

P

'

ff

,
j
R
v
'

*


|
z

h
n
R




l


ix
Z
mnhs*nh||LiffssvSfMiffn vRr'xRiffsxRidmxRiffsRiffjvRlzjmq&vRmiI'fopikmlnhopfhqr'ih
jfhxkmix8vSfvSiffsRvCvR'i H '$D dmxRiffkml|LvSfhxCnhs8lj'dyr'vvSfEvR'i6aD-aiyxsSv8kmiLj'iffk
nvSxnhlzjmlzj'qnhjykvSiffsSv,sSiv8fhx&vR'i6|LfwCylzj'iffkdmxRfhyoziffwO 'iMvSiffsSv,sSiv8fhx&vR'i H u-
dmxRiffkylz|LvSfhx|LfjvRnhlzjms-vR'iGiL'|mnhj'qhiffsDvRmn v-fe||rmx-lzjvRmiakmlznhozfhqr'iffsZfhuvR'iaa$vSiffsSvsSivDiasSiffoziff|LvSiffk
n*xnhjmk'fwhkmlznhopfhqrmiffsnhsvRmi~vSiffsSvDsSiv-nhjmkIvR'iffjIiLuvSxnh|LvSiffk&vR'ia|LfhxxRiffsSdfjmkmlzjmqUiL'|ynhj'qhiffsK ;hhh
iL'|mnhj'qhiffsculzwlzozn xoptfhx6vSxnhlzjmlzjmq'UvRmi~$vSxnhlzjmlj'qsSiv0|LfjvRnhlzjyP
;hhhkmlznhopfhqrmiffsMml|
|LfhxRxRiffsRd"fjykms5vSfn\vSfhvRnhofh5ffh '8iLu|mnhj'qhiffsPfhxvSxnhlzjmlzjmq&vRmi H '$D 5dmxRiffkmlz|LvSfhx
miMiffn vRrmxRi H u- $lzs8dmxRiffkml|LvSiffkfhxCiffnh|r'vSvSixnhjy|Li6lzjvRmivSiffsSv&sSivvRrms8iffju
n yozlj'q6vRmiMsStesRvSiffwvSfOiMrmsSiffkfjj'i kmn vRnlpvR'fr'v$vR'iMjmiiffkfhx8ynhjmkuozn iffozozlj'q'Ifai}hix
vR'ixRi&n xRi&vUf6dfsRsRlzylzozlpvRlziffsafhxvR'i8fhxlzqlzjfh)vRmls*iffn vRr'xRiCljvR'iCvSxnhlzjmlzjmqsSiv miIyxsRv*dfsRsRl
ylzolpvtlzs$fhxvR'i,vSxnhlzjylzj'q0sSivvSfnhozsSfO|LfjmsRlsSv$fhasSfopiffoztEnhr'vSfwMn vRl|,iffn vRr'xiffs mlzsw\ivR'fukmnhs
vR'i,dfhvSiffjvRlnho)nhk'}nhjvRn qhiMvRmn vvRmi\vSxnhlzj'iffk~$PlzozoD|Lfw\diffjmsRn vSihlpaj'iff|LiffsRsRn xRthfhxyn vSi}hix
j'flzsRiiL'lzsSvRslzj0vR'i H u$D UdmxRiffkml|LvRlpfjmsxlpqv hh5PjOnhopvSixjmn vRlp}hiIvSfMvSxnhlzjmlzj'q
vR'iIafjOvR'iInhr'vSfwn vRlz|nhozopt0k'ixlp}hiffk H u- aiffn vRr'xRiIlzs5vSfvSxnhlzjOlpvKfjOvRmiImnhjmku
ozn iffozopiffkEu- aylzopi$sSvRlzozo"vSiffsSvRlzjmqlpv5fjvR'iInhrmvSfwMn vRlz|iffn vRr'xRih mlzsKsSiff|Lfjmkw\ivR'fuklzs
xRiixRxRiffkOvSfJnhs6Synhjmkuozn iffozopiffk'HvSxnhlzjmlzjmqIfhx [! u$D mlzsKwMn tOdmxRf}elzkmiInw\fhxRiCnh||ru
xn vSi\w\fuk'iffo-yr'vPlzvwMn tEj'fhv$|n dmvRr'xiCvR'i\|mn xnh|LvSixlzsRvRlz|s$fhGvR'i,nhr'vSfwMn vRlz|&iffn vRr'xRi\lzjEvR'i&vSiffsSv
sSiva
iffsRrmopvRs*fhx*vRmiffsSiIvUfw\ivRmfekmsn xRiIdmxRiffsRiffjvSiffkljceiff|LvRlpfje '
4ff3p&

fi>

> 9?f7A@d.22

72

SLU Features
Exchange 1

SLU Features
Exchange 2

autoSLUsuccess
Feature Predictor
Exchange 1

autoSLUsuccess
Feature Predictor
Exchange 2

System
Continues

Yes
Automatic
Features
Exchange 1&2

PDP
P(Success)>T


lpqrmxRi8eUcetusSvSiffw



B

C

n x|mlpvSiff|LvRr'xi8rmsRlzjmq&iffn vRr'xRiffsxfw
autoSLUsuccess
predictor training testing





B

C



PDP Training Set

Adapt DM
Transfer
Human
Customer
Agent

vR'i$yxsRv,iLu|mnhj'qhiffs



B

C



Test

TEST
PDP Test set

lpqr'xRi8ean vRnfhxPsSiqw\iffjvRn vRlpfjErysRlzj'q\|LxRfsRsSH}nhozlkmn vRlpfj
miCdmxfhyopiffw lpvRrmsRlzj'q H u$D fhx$vSxnhlzjmlj'qvR'i\alzsvRyn vvR'i&snhw\i\kmn vRn
zl sOrmsRiffkvSfvSxnhlzjvRmi H u$D JdmxRiffkmlz|LvSfhx 'ixRifhxRih$airysSiffkn|LxRfsRsSH}nhozlkmn vRlpfj
vSiff|yjmlzer'iEnhozsSfgej'fjnhs%Snh|gHgujml"j'qMifflzssC8rmozlpghfsSgul/~ffhu )'ixitvR'ivSxnhlzjmlzj'q
sSivIlzs$dn xRvRlpvRlpfj'iffkljvSfOsSivRsff 'xRii,fh5vR'iffsSiMsSivRs8n xRirmsSiffkfhxvSxnhlzjylzj'qOnhjmkvR'i\fr'xRvRfhx
vSiffsSvRlzjmq' 'i&xRiffsRrmozvRsPfhxvR'i,fr'xRvRsSivn xRi\j'fhvSiffknhjmkvR'i,dmxRfu|LiffsRslzsxRidiffn vSiffkZZxfhvRn vRlzj'q0vR'i
sSivRsPxfw vSxnhlzjmlj'q\vSfvSiffsSvRlj'q' mlzs5xRiffsRryopvRs*lzjn6|Lfw\dyopivSi&ozlzsSv5fh)dmxiffkmlz|LvSiffk H u-
fhxKvR'ivSxnhlzjmlj'qCsSiv 'i$iffn vRrmxRiffsKfhxKvR'ivSiffsSv*sSivKiL'|mnhjmqhiffsn xRik'ixlz}hiffk0t6vSxnhlzjmlzjmb
q BEBE8ED
fjOvR'i8'fopi$vSxnhlzjylzj'q\sSiv ylzs5dmxRfu|LiffsRs*lzsKlozozrmsSvSxn vSiffkJljODlzqr'xRi8e
miCfozopflzj'qsRiff|LvRlpfjqlp}hiffs$n6yxRiffn gekmf jEfhGvR'i\lzj'dr'v*iffn vRr'xRiffsIceiff|LvRlpfjJk'iffsR|Lxlz"iffsvR'i
vSxnhlzjylzj'q6nhjykExiffsRrmopvRsPfhGvR'i H u$D *dyxRiffkmlz|LvSfhxnhjmkceiff|LvRlpfj6xRidfhxRvRsPvR'i\nh||r'xnh|Lt
xRiffsRryopvRs5fhxvR'i8~$D

GYNMPORQJQ `l Q


4ff3 3

fi

2Zy222 S2-



7M1m



72

B\~#
P

xiff|Lfhq'xRiff|Lfhq jermwCafhxkysnhsSxRkmr'xn vRlpfj kmvRw\"n q'xRq w\fukmnhozlpv thhxRq HqhxnhwMwn xffvSiffw\df
Z~#
n|Lfj'k'iffjm|LiOw\iffnhsRrmxRi0fhx\iffnh|fhvRmiffdfsRslpyopi6vRnhsSgesMvRmn v,vR'irmsSix\|Lfrmozki
vSxtelzjmq\vSfk'f

snhozlpiffjm|LiL|Lf}hixn qhih ljm|LfjmsRlzsRvSiffjm|Lth|LfjvSiLuvsRmlpvvSfhdmHvRnhsSg 'j iLevSvSfhdmHvRnhsSgvSfhd'
|Lfj'k'iffjm|Lihykyl|Lfj'k'iffjm|Lihm|Lfj'dixRvRlzw\ih'sRnhozd"ixvRlzw\ih'nhr'vSf cu
*
: sRry||LiffsRs
J

9

~2 #}p~2#~$u~2#

<BY2~#

sRteson "iffoHurmvSvlzkZ'dmxRfwMdmv'xRidmxRfwMdmvm|Lfjuyxwn vRlpfjZ'sRr'Zkmlznho

xrmjmjmlzjmq;vRnhoozlpiffsjermw,r'vSvRsffIjrmw\HxRidmxRfw\dyvRs$d"ix|LiffjvHxRidyxRfw\dmvRsjermw,|LfjuxwMs
dix|Liffjv|LfjuyxwMsmjermw,sRr'Zkmlnhozsdix|LiffjvsRr'Zkmlnhozs

P'fopiIkmlznhozfhqr'ihGkmlznhokyr'xn vRlpfjZ
'#
~$

Y~2#P~El
vRs|LxlpdmvrywMnhjuozn iffo/ n qhihqhiffjmkmixff rmsSixSw\fukmnhozlzvth|oziffnhjuHvRsR|Lxlpdyv|opvRs|Lxlpdmv
jermwCafhxkmscu
*
: sRry||LiffsRs
Dlzqr'xRiC~'iffn vRrmxRiffsfhxsSdfhghiffjkylznhopfhqr'iffs

kmlnhopfhqr'i|LfjmslzsSvRsGfhnCsSiffer'iffjm|LifhZiL'|ynhj'qhiffsU'ixRiPiffnh|JiLu|mnhj'qhi$|LfjmsRlzsRvRsGfhZfj'iPvRr'xj
etvRmisStusSvSiffwfozopfaiffktfj'iMvRrmxjetvRmirmsSixffP~nh|kmlnhopfhqr'iMnhjmkiLu|mnhj'qhi6lsiffjm|Lfuk'iffk
rmsRlj'q6vRmiMsSivIfhKh ;0iffn vRrmxRiffsIlzjlpqr'xRi6
anh|iffn vRr'xRi\5nhsIifflzvR'ixInhr'vSfwMn vRlz|nhooptozfhqhqhiffkt
fj'iMfh5vR'i6sStesRvSiffww\fukmrmoziffsmnhjmkuon "iffoopiffktermwMnhjmsfhxCk'ixlz}hiffkxfwxn iffn vRr'xRiffs 'i
mnhjmk'ozn iffozopiffk&iffn vRrmxRiffsan xiPrmsSiffkMvSfCdmxfekmry|Li*
n 2FBy i8ZnhjMiffsRvRlzwMn vRlpfjfh'f ?aiffozonI|oznhsRslyix
|Lfrmozk0k'f,vRmn v*ynhkOnh||LiffsRsKvSf\dixRiff|Lv*lzj'fhxwMn vRlpfjZ fMsSii'ivR'ix5fr'xKxiffsRrmopvRs5|nhjOqhiffj'ixnhozlpih
Ui&nhozsRf6iLudixlzw\iffjvlzvRrmsRlj'qnJsRr'ysSivPfhGiffn vRr'xRiffsvRmn v$n xRiCvRnhsRglzjyk'idiffjmk'iffjvPk'iffsR|Lxlpiffklzj
k'ivRnhlzo"iffozf
'iffn vRrmxRiffsopfhqhqhiffktvR'isStusSvSiffwn xRir'vRlozlpiffk"iff|nhrysSiOvR'itn xRi0dmxRfukmrm|Liffknhr'vSfwMn vRlz|nhoopth
nhjmkvRerms0|nhj"irmsSiffkkmrmxlzj'qxrmjvRlw\iEvSfnhopvSixOvRmi|LfrmxsSifh8vR'ikmlznhozfhqr'ih 'isStusSvSiffw
w\fukmrmopiffsGfhxamlz|opfhqhqlzjmq8lzj'fhxwMn vRlpfjMUnhsU|Lfozopiff|LvSiffk6aixivR'inh|LfrmsSvRlz|dmxRfu|LiffsRsSfhlx knhr'vSfwn vRlz|
sSdiiff|,xiff|Lfhqjmlpixg 4-< D~g Plz||n xkml'fhxlzjh hhvR'iKsSdfhghiffj,oznhj'qrmn qhiKrmjmkmixsSvRnhjmkmlj'qCH-
w\fukmrmopi8H$fhxlzj6ivanho/pffhhmnhjmkMvRmiP$lznhopfhqr'iEnhjyn qhixg iffozon8 $fhxlzjZ"ffhha
~nh|
w\fukmrmopinhjmk0vRmiIiffn vRrmxRiffs*fhmvRnhlj'iffk0xfw lpv*n xRi8kmiffsR|LxlpiffkJiffopf
Aq9 ~\lff12# 'inhr'vSfwMn vRlz|OsRd"iiff|xRiff|LfhqjmlzixEg 4- DCvRn ghiffs6nhs
lzj'dr'vIvR'iO|nhozopix s&sRd"iiff|nhjmk;dmxRfukmrm|Liffs&nEdfhvSiffjvRlznhozoptixxRfhxRrmoGvSxnhjysR|LxlpdmvRlzfjfh*Pmn v&lzvCiL
ozlpi}hiffs*vR'i8|nhozopixPsRnhlzkZ '
4- Diffn vRr'xRiffsfhxiffnh|EiLu|mnhj'qhiCljm|ozrmk'i$vR'i8fr'vSdyr'vKfhvR'i8sRd"iiff|
xRiff|Lfhqjmlzix RR} Z PvR'iEjermwCixfhUfhxkmsJlzjvR'iExRiff|Lfhqjmlpix0fr'vSdr'vE RR Z &~h RvR'i
kmr'xn vRlpfjlzjsSiff|Lfjmkysfh5vR'ilzj'dr'vPvSfvR'iMxRiff|LfhqjmlzixJ GA
n n qOfhx8vSfrm|vSfj'iJlzju
Vp$p$

fi>

72

> 9?f7A@d.22

dyr'v hgI <Z vR'ilzj'dyrmvaw\fukmnhozlpv tMiLud"iff|LvSiffkt6vR'i$xRiff|LfhqjmlpixC Z Mff ! 8fj'ifhGj'fj'ih
sSdiiff|vSfrm|vSfj'ihsSdiiff|9$vSfrm|vSfj'ihDvSfrm|vSfj'iL|n xkZsSdiiff|vSfrm|vSfj'iL|n xk-vSfrm|vSfjmiL
kmn vSihsSdiiff|9
vSfrm|vSfj'iLkyn vSihfhxj'fj'iL/jynhoHdmxRfw\dyv"nhjmkvR'i,qhxnhwwMn xrmsSiffktvR'iCxiff|Lfhq
jmlpix\ Z ZhSr +\h ,g Plz||n xkml$fhxlzjZ hhi\nhozsRfJ|nhoz|ryozn vSiCn6iffn vRrmxRi&|nhoopiffk
kmlp}ulzkmlj'q8vR'i8}nhozrmifhvR'i Sh/ iffn vRr'xiItJvR'i RR} Z ,~ iffn vRr'xRih
4-< Diffn vRr'xRiffslzs&vRmn v\nhjt;fj'i0fhPvRmiffw wMnfft;xRi yiff|Lv,xRiff|LfhqjmlzvRlpfj
mi0wMfhvRlp}n vRlzfjfhx\vR'
dixRfhxwMnhjm|Li6lpvRn|Lfjm|LfwMlzvRnhjv,iLiff|Lv&fjsSdfhghiffj;oznhj'qrmn qhi0rmjmkmixsSvRnhjmkmlj'q''fhx,iLunhwMdyopih
fhvR'ixPafhxRgOmnhsfryjmk h/ vSfiI|LfhxRxiffozn vSiffklzvRlzjm|LfhxRxiff|Lv*xRiff|LfhqjmlpvRlzfjPlzxsR|ixRq'
lzvRwMnhjZceUixRvRsffhh 'iKjmnhw\iKfh"vR'iKqhxnhwwMn xP Z ZhSr +\h ~|Lfrmozk\nhozsRfi*n$dmxiffkmlz|LvSfhx
fh~ixRxRfhxssRlzjm|Li8lzv*lzsKUiffozoZguj'f jvRmn vvR'iCozn xRqhix*vR'i8qhxnhwMwMn xlzsmvR'iCw\fhxRi8ozlzghiffopt6nh
j 4-
ixRxRfhx-lzsjnhkmkmlzvRlpfjZffvR'i g Z Zr +M iffn vRr'xRianhozsSfKiffjm|Lfuk'iffsZiLudiff|LvRn vRlpfjmsn fr'vrysSixr'vSvSixnhjy|Liffs
n v\vRmn v,dflzjv&lzj;vR'iOkmlnhopfhqr'ihGylz|;wMnfft|LfhxRxRiffon vSiOvSfkyl"ixiffjm|Liffs\lzjvR'iOiffnhsSiOlpvRml|
nhjtJfjmi8xRiff|LfhqjmlzixP|Lfrmozk|LfhxRxRiff|LvRoptrmjyk'ixsSvRnhjmk0vRmi8rmsSixffsxRiffsSdfjmsSihK$j'iIw\fhvRlz}n vRlpfjfhxPvR'i
iffn vRrmxRiKlzsDvRmn v)dmxRi}ulpfrms-afhxgCsRr'qhqhiffsRvRs)vRmn vGrmsSixs-vSiffjyk,vSfIsRopf k'fj,vR'ifflpx)sSdiiff|,'iffj
vR'i0sStusSvSiffwmnhs,wMlzsRrmjyk'ixsSvSfefekvR'iffw i}hf 8ffhhecu'xlpixRq'Dnhk'iha Gxlz|Lih*ffhh5vRmlzs
sSvSxn vSiqhtMnh|LvRrynhozopt\opiffnhkms~vSf8w\fhxRiPixRxRfhxsGslzjm|Li*vR'iPsSdiiff|Mxiff|LfhqjmlpixalsGj'fhv~vSxnhlzj'iffk\fjvRmlzs)v tdi
fh-sRd"iiff|Z mi iffn vRr'xRiIwMn t6nhozsSfMljmkmlz|n vSi$'iffsRlpvRn vRlzfjmsudynhrmsSiffsffefhx*ljvSixRxr'dmvRlpfjmsffml|
|LfrmozknhozsRfopiffnhkvS
f 4-< DixxRfhxs$j;vR'i0fhvR'ixMynhjmkZ)vSfrm|vSfj'ilj'dyr'v&lzj|LfwCylzjyn vRlpfj;lpvR
sSdiiff|unhs5iffjy|Lfek'iffk0etMvR'i$iffn vRr'xRi hg < Z mwMlpqv5ljm|LxRiffnhsSivR'iozlpghiffozl'ffukMfhDrmjmk'ixsSvRnhjmkmlzjmq
vR'isRd"iiff|ZsRljm|LiOvR'iOvSfrm|vSfj'ilzj'dr'v&lsMrmjmnhwCylpqr'frys\lpvM|nhj|LfjmsSvSxnhlzjsSdfhghiffjonhj'qrmn qhi
rmjmkmixsSvRnhjmkmlj'q'
l2P~2#9#~$2##q~2#n'# 'iIqhfnho-fh)vR'iCsRd"fhghiffjEoznhj'qrmn qhi&rmjmk'ixsSvRnhjmkmlzjmq
H-Dw\fukmrmopiKlzsvSfClzk'iffjvRlptCmlz|,fhvRmi8ff$dfsRsRlpopiavRnhsRges~vR'i*rmsSix~ls)n vSvSiffw\dmvRlzj'qCnhjmk,iLevSxnh|Lv
xRfwvR'iIr'vSvSixnhjy|Li8nhjt0lpvSiffwMs5fhlzj'fhxwMn vRlzfj6vRyn vn xRixRiffopi} nhjvKvSf|Lfw\dyopivRlj'q,vRmn vKvRnhsSg"mih q'
n\dymfj'ijrmwCix*lzs*j'iiffkmiffkOfhxvR'iIvRnhsSg /

lpvSiiffjfhUvR'i\iffn vRrmxRiffsxRfwvR'i6ffw\fukmrmopi&xRidmxiffsSiffjvvRmiMkmlzsSvSxlzyr'vRlpfjfhxIiffnh|fhUvR'i
ffd"fssRlpyopi,vRnhsSgus&fhKvR'i0ff?w\fekyrmopihsI|Lfju"k'iffjm|Li6ljlpvRs8"iffolpiavRyn v8vR'iJrmsSix,lzs8n vSvSiffw\dyvRlzj'q
vRmn vIvRnhsRgH$fhxlzjiv8nhoHpaffhhJi6nhozsSfljm|ozrmk'i\nOiffn vRr'xRi\vSfExRidmxRiffsRiffjvylz|vRnhsSgynhsvR'i
mlpqmiffsSvK|Lfjuk'iffjm|LisR|Lfhxi H H ffynhjmk0ml|JvRnhsSg0mnhs5vR'isSiff|Lfjmkylpq'iffsSv5|Lfjuk'iffjy|Li$s|LfhxRi
" /H H z ynhsUUiffozonhsUvR'i} nhozr'ifhvR'iylpq'iffsSvU|Lfjuk'iffjm|Li$sR|LfhxRi\ H h< nhjmk6vR'i
kmlixRiffjm|LiIlj0}nhozrmiffsKivUiiffjvR'ivSfhdnhjmkj'iLuvHvSf HvSfhd|Lfj'k'iffjm|LiIsR|LfhxiffsC < UL
vRmixJiffn vRr'xRiffsJxRidyxRiffsSiffjv6fhvR'ixnhsRd"iff|LvRsJfhCvR'iff dmxRfu|LiffsRsRlzjmqfhIvR'ir'vSvSixnhjy|Lih 'i
L" iffn vRr'xRilzs\nhjlzjvSxnr'vSvSixnhjm|LiwMiffnhsRr'xRiOfh$sSiffwMnhjvRlz|Okmlp}hixsRlpvth5nh||Lfhxkmlj'qvSfn
vRnhsSgwMfek'iffofh)vR'i,k'fwMnhlzj;iffozozn6fhxlzjZ-ffhhIcefw\i&vRnhsSg|oznhssSiffsfe||r'xvSfhqhivR'ixIermlpvSi
jmn vRr'xnhozoptMlzvRmlzjJnMsRlzj'qozisSvRn vSiffwMiffjvPfhxKxRiffrmiffsSvmih q'GvR'i h fi
IL 3
vRnhsSg0lzsK|Lfw\dyn vRlpopi$lpvR
vR'i zR vRnhsSg"r'v)lzsj'fhvG|Lfw\dyn vRlpopi5lpvR&vR'
#
Z,RR vRnhsSg" 'i h#HL r hLS< Z
iffn vRr'xRi,w\iffnhsRr'xRiffsPvR'iCdmxRfhdfhxRvRlpfjfh)vR'i&rmvSvSixnhjm|Li&mlz|lzsP|Lf }hixRiffketvR'i&sRnhozlpiffjvqhxnhwwMn x
xn qw\iffjvRs mlzsCwMnfft;lzjm|ozrmkmiMvR'iJ'fopi6fhndy'fj'i6fhx,|n xk;jermwCix&lp*lzv8fu||r'xs&lpvRmljn
xn qw\iffjv 'i hy} [ IL iffn vRr'xRiIlzsKnhjOljvSixSrmvSvSixnhjm|Li8w\iffnhsr'xRi$fh-vRmiiLevSiffjv*fhn&smlpvUfh
|LfjvSiLev*n Un tJxRfwvR'i|r'xRxiffjvUvRnhsSgfu|rms'|nhrmsSiffkJetvR'i$n dmdiffn xnhjm|Li$fh-snhozlpiffjvadymxnhsSiffsavRmn v
n xRi8ljm|Lfw\dyn vRlpopiPPlpvROlpvmnh||Lfhxkmlj'qMvSfn\vRnhsRg0wMfek'iffofhDvR'i8kmfwMnhlzjZ
jnhkmkylpvRlpfjZsRlzwlzozn xOvSfvR'i5nfftUi;|nho|rmozn vSiffkvRmi iffn vRr'xRihCai;j'fhxwMnhozlzivR'i
h#HL r hLS< Z nhjyk H h< iffn vRr'xRiffsPtOkmlz}elzkylzj'q,vR'iffw et vSf6dmxRfukmrm|Li
vR'i h \ nhjyk < \ iffn vRr'xRiffs 'i nhjmk;vR'i p \ nhjmk LLf
iffn vRr'xRiffsn xiIrmsSiffkOfjmozt6fhx*dmxRiffkylz|LvRlzj'q H u$D
Vp$T

fi

2Zy222 S2-



7M1m



72

mi5wMfhvRlp}n vRlzfj\fhx)vRmiffsSiJiffn vRr'xiffsGlzsvSf8wMn ghi*rysSiKfh"lj'fhxwMn vRlpfj&vRmn v)vR'iPffOw\fukmrmopi
nhs6nhsnxiffsRrmopv\fh$dmxRfu|LiffsRsRlj'qvR'ifrmvSdyr'vMfh#4-<DnhjmkvR'iE|r'xRxRiffjvkmlzs|Lfr'xsSi|LfjvSiLevmfhx

iL'nhw\dyopihfhxCr'vSvSixnhjy|LiffsCvRyn vCfozopf vR'iyxsSv8rmvSvSixnhjm|Lih)vR'iO?w\fukmrmopiMguj'fsIyn v8vRnhsSg
lpvOiffozlpi}hiffs0vRmi|nhozopixls0vSxRtelj'qvSf|Lfw\dyozivSih 'i [ iffn vRr'xilzjm|LfhxRdfhxn vSiffsvRmlzs
guj'f oziffk'qhifhKvR'iJkmlzsR|LfrmxsSiMmlzsRvSfhxRthlpvRvR'iJw\fhvRlp} n vRlpfjvRmn vClpKlpvCn dmdiffn xsIvRmn v&vR'iJ|nhozopix
mnhs*|mnhj'qhiffkE'ixwMlzjykZuvR'iffjOvR'iCffw\fukmrmoziwMn tOmnff}hi&wMlzsRrmjyk'ixsSvSfefekJnhjr'vSvSixnhjm|Lih
~22}p~2#~$2 'iJrmjm|LvRlpfj;fhvRmi0$lznhozfhqr'iOEnhjmn qhix6lzs&vSfvRn ghiEnhs\lzj'dyr'vCvR'i
fr'vSdyrmvfhGvR'iMw\fukmrmozihk'iff|lzkmi8mn vPvRnhsSgvRmiCrmsSix$lzsPvSxRtulzj'qvSf0nh||Lfw\dyolzsRZ"k'iff|lzk'iCmn v
vR'iJsStesRvSiffwlzoo)sRnfftj'iLuv)nhjmkrmdkyn vSi\vR'i6kmlsR|Lfr'xsSiMylzsSvSfhxRtP"iffoozn fhxlzjZaffhh 'i
$lznhopfhqr'i,nhjmn qhixIk'iff|lzk'iffsP'ivRmixlpviffozlpi}hiffsvR'ixRi,lzsPnJsRlzjmqopiCrmjynhw8ylzqr'frms*vRnhsRgvRmn v$vR'i
rmsSixPlzs5vSxRtulzj'q\vSfnh||Lfw\dozlzsRZ'nhjmk'f vSfxRiffsSfop}hi8nhjtOnhw8lpqrmlpv th
'iffn vRrmxRiffsKynhsSiffk0fjOlj'fhxwMn vRlpfj6vRmn vKvR'i$lznhopfhqr'inhjmn qhixopfhqhqhiffkOn fr'vKlpvRsKk'iff|lsRlpfjmsafhx
iffn vRr'xRiffs*xidmxRiffsSiffjvRlzj'q\vR'iCfj'qhflzj'q6mlzsSvSfhxRt6fh)vR'i8kmlnhopfhqr'iIwMlpqv*iIrmsSirmodmxiffkmlz|LvSfhxsKfh~ff
ixRxRfhxs8fhx&vRnhsSgnhlzozr'xihcefw\i6fh*vR'idfhvSiffjvRlznhozoptljvSixRiffsRvRlzj'q$lznhopfhqrmiJnhjmn qhix\i}hiffjvRs&n xlzsSi
kmr'ivSfJopf |Lfjuk'iffjy|Li8opi}hiffozsKPmlz|opiffnhkOvR'i&lnhopfhqr'i8nhjmn qhixvSf Sr vR'i8rmsRix*fhx
h< Gl lzvRsrmjmk'ixsRvRnhjmkmlzj'q'$ xidmxRfw\dmv$wMlpqv$i&nJ} n xlznhjvfhavR'i\sRnhw\i\er'iffsSvRlpfjvRmn v5nhs
nhsSghiffk"ifhxRihfhxUlpvG|Lfrmozk\ljm|ozrmk'i5nhsSgulzj'qvR'iPrmsSixGvSf8|'fefsSi*ivUiiffjMv afCvRnhsSges~vRmn v~mn }hiiiffj
nhsRsRlzqj'iffksRlzwMlzon x|Lfjuk'iffjy|Liffs$tvRmiMff;w\fukmrmopih$'fhxiL'nhw\dyopihZlzjEvR'i\kmlznhopfhqrmi&lzjlpqr'xRi\
vR'i&sStesRvSiffw rmvSvSixnhjm|Li,lzj
c ;|LfrmjvRsnhsn6xRidmxRfw\dyviff|nhrmsSi&lpvPlzsPn} n xlznhjvfh)vRmi&er'iffsSvRlpfjElzj
r'vSvSixnhjy|Li&c'e
mi&iffn vRr'xRiffs8vRmn vUi\iLevSxnh|LvCxfwvR'iM$lznhopfhqr'iMnhjmn qhixCn xi\vR'i\vRnhsSgHvted"i6ozn iffo/
p mfsSisSivfh&}nhor'iffsOlzjm|ozryk'in}nhozrmivSf?lzjmkmlz|n vSimiffjvR'isStusSvSiffw mnhklzjmsRru{J|lpiffjv
lzj'fhxwMn vRlzfj0vSfk'iff|lzk'i$fjnsSdiff|l|vRnhsSgHvted"ihvR'iIr'vSvSixnhjy|Li8lzk0lpvRylzjJvR'i8kmlznhozfhqr'iM / /
vR'ijmnhw\i*fhZvR'idmxRfw\dmv~dyoznffthiffkvSfCvRmirmsSix$ h ynhjmkMP'ivR'ixGvR'iPvtedifhdmxRfw\dyvGUnhsUn
xRidmxfw\dmv h "n\|LfjuxwMn vRlpfj h< Gl yfhx*n\sRr'Zkmlznhopfhqrmi*dmxRfw\dyv$n\sRrmd"ixsSivafh-vR'i
xRidmxfw\dmvRs)nhjmk\|LfjuxwMn vRlpfj,dmxfw\dmvRsK =
h miP L iffn vRr'xRi*lzs)lzjvSiffjmk'iffk&vSf8|n dyvRr'xRi
vR'iJnh|Lv,vRmn v,sSfw\iJvRnhsSgus,wMnfft"iJmn xk'ixCvRmnhj;fhvR'ixs 'iE / / iffn vRr'xiJlzs&w\fhvRlp} n vSiffk;t
vR'ilk'iffn0vRmn vIvR'i6opiffj'qhvRfh5vR'ikylznhopfhqr'iMwMn t"i6lzw\dfhxRvRnhjvdfsRsRlpyoztlzj|LfwCylzjmn vRlpfjlpvR
fhvR'ix$iffn vRr'xRiffsozlpghi6 p 'iCkmlp"ixRiffjvdmxRfw\dmvPiffn vRrmxRiffs$fhxlzjylpvRlznhodyxRfw\dmvRs"xRidmxfw\dmvRs
|LfjuyxwMn vRlpfjdmxRfwMdmvRs,nhjmksRr'Zkmlznhozfhqr'idmxfw\dmvRs\n xRiOw\fhvRlp} n vSiffketxiffsRrmopvRs,lzjmkylz|n vRlzj'qEvRmn v
xRidmxfw\dmvRs~nhjyk|LfjuyxwMn vRlpfj\dmxfw\dmvRs~n xixrmsSvSxn vRlj'qfhxU|nhozopixsanhjmkMvRyn va|nhozopixs~n xRiozlpghiffopt&vSf
tdixn xRvRl|rmozn vSiP'iffjvRmitmnff}hivSf,xRidiffn vUvR'iffwMsSiffoz}hiffsumlz|xRiffsrmopvRsUlz
j 4D DixxRfhxs$Hcu'xlpixRq
ivnho/p-ffhhei}hfI-ffhhenhopghixff
InhwMwOy-lpvRwMnhjZy hhhn
miEkmlzs|Lfr'xsSiylzsSvSfhxRtiffn vRr'xRiffs0lzjm|ormk'iffkxrmjyjmlzj'qvRnhozolpiffsMfhx6vRmijermwC"ix6fhIxRidmxfw\dmvRs
R r RDjermwCixPfh~|Lfj'yxwMn vRlpfjEdmxRfwMdmvRs\ h< Gl Rnhjmkjryw8ixfhasRr'Zkmlp
nhopfhqr'iCdmxRfw\dyvRs8 E
/ RvRyn vPmnhkOiiffjdyoznffthiffkifhxRi8vR'i8r'vSvSixnhjy|LiC|r'xRxiffjvRoptJifflzj'q
dmxRfu|LiffsRsSiffkPnhsOUiffozo$nhs0xrmjmjylzj'qdix|LiffjvRn qhiffs Ly R Sr cM LSy < Gl c& Ly
=
h R 'i8rmsRiIfhxrmjmjmlj'qCvRnhoozlpiffs*nhjmkdix|LiffjvRn qhiffs$lzs*ynhsSiffkfjdmxRi}ulpfrmsKUfhxRgOsRr'qhqhiffsRvRlzj'q
vRmn v~jmfhxwMnhozlpiffk&iffn vRr'xRiffsGn xiw\fhxRi*ozlzghiffoptIvSfIdmxfekmry|LiUqhiffjmixnhozlpiffk,dmxiffkmlz|LvSfhxs*-lpvRwMnhjZnhozghixff
iffn xjmsDffhhiffn vRr'xRi,nff} nhlzozn opiIfhx /L

Z dmxfhyopiffwMn vRlz|8kmlznhopfhqr'iffsPlzs h h/
vRmn vlsKj'fhvnff} nhlzozn opifhxPlzjmlpvRlznhosSiqwMiffjvRsfh-vR'iCkmlznhopfhqr'ih
Y~2Y,~=l''' sw\iffjvRlpfj'iffkCn "f}hih vR'iGiffn vRr'xRiffsfhyvRnhlzj'iffkI}ulznKmnhjykuozn iffozozlzjmqan xRiUrmsSiffk
vSf&dmxRf }ulzk'i*+
n 2FBy i8Mn qnhlzjmsSvamlz|MvSf&|Lfw\dyn xRivR'idixRfhxwMnhjy|Li*fhZvR'irmozoztCnhr'vSfwMn vRl|iffn
vRr'xRiffsff miamnhjmk'ozn iffozopiffkiffn vRr'xRiffslzjm|ormk'i~ermwMnhjCvSxnhjmsR|LxlpdmvRsfhyiffnh|\rmsSixr'vSvSixnhjy|Li
nsSivfh$sRiffwMnhjvRl|ozn iffozs,vRmn vn xi|opfsSiffoptxRiffon vSiffkvSfvR'isRtesSvSiffw vRnhsSgHvted"iozn iffozs [
Vp$ 4

fi>

72

> 9?f7A@d.22

p "n qhi6 <Z 5nhjmk0qhiffjmkmixC ZL 5fhDvR'iIrmsRixff'vR'iInh|LvRrmnho-w\fekynhozlpvt6fh-vRmi8rmsSix*r'vSvSixnhjm|Li
e Mff ! fjmifh Gj'fhvRylzj'q'usSdiiff|ZuvSfrm|vSfj'ihsSdiiff|9vSfry|vSfj'ihyj'fjusRd"iiff|mnhjmk0n
|opiffnhj'iffk&vSxnhjysR|LxlpdmvlpvR&j'fjuHUfhxkCjmflzsSialj'fhxwMn vRlpfjIxiffw\f }hiffk ffp G'xRfwvR'iffsRiaiffn
vRr'xRiffsffUi|nho|rmozn vSiffk6vUf,kmixlp}hiffk6iffn vRr'xiffs 'iyxsSvaUnhsUvR'ijrmwCix~fhUfhxkmsUlzjvR'i|oziffnhj'iffk
vSxnhjms|Lxlpdmv8 ff! ,~ Sn qnhlzjfjvR'i&nhsRsRrmwMdmvRlpfj0vRmn vr'vSvSixnhjm|Li&opiffj'qhvRlzsPsSvSxRfj'qozt
|LfhxRxRiffon vSiffklpvR
4- D;nhjmk;ixRxRfhxs 'i,sSiff|Lfjmkk'ixlz}hiffkEiffn vRr'xRi,5nhs$ynhsSiffkfj|nhoz|ryozn vRlzj'q
'ivRmixPvR'i [ L wMn vR|'iffs$vR'i6 L xfwvR'i,$lznhopfhqr'i,nhjmn qhix u$D R
mls5iffn vRr'xiIlzs*k'iffsR|LxlpiffkJljOk'ivRnhlzolzj0vRmiIj'iLevPsSiff|LvRlpfjZ
jvR'i,iLedixlw\iffjvRsff"vRmi\iffn vRr'xiffslzjDlzqr'xRiZiLu|ormkmlzj'qJvR'i\mnhjmk'ozn iffozopiffkiffn vRr'xRiffsffn xRi
xRiixRxRiffkCvSfnhsDvR'
4
2F 4E2 6Ciffn vRr'xRi5sSiv 'iaiLedixlzwMiffjvRsvSiffsSv)'f ;aiffoowMlsRrmjmk'ixsSvRnhjmkmlzjmqs
|nhj\i*lzk'iffjvRlyiffk,nhjmk,P'ivR'ix)dmxRfhopiffwMn vRlz|5kmlznhozfhqr'iffs)|nhjMi5dmxRiffkml|LvSiffk\rmsRlzj'qvR'
4
2F 4E2 6
iffn vRr'xRiffs)iI|Lfw\dn xRiPvRmiPdixRfhxwMnhjm|LifhvR')
4
2F 4E2 6Jiffn vRr'xRi$sSiv5vSf,vR'irmoomiffn vRr'xRi$sSiv
lzjm|ormkmlzj'qCvR'iImnhjmk'ozn iffozopiffk6iffn vRr'xiffsnhjmk0vSfMvRmiIdixRfhxwMnhjm|Li$fhvR'
4
2F 4E2 6iffn vRr'xiIsSiv
lpvR8nhjmkIlzvR'fr'vZvR'i H u- Ziffn vRr'xihDlpqr'xi~Kqlp}hiffsDnhjIiL'nhw\dyopi~fhevRmi~iffjm|Lfukmlzj'qKfh
sSfw\ifhZvR'inhr'vSfwMn vRl|*iffn vRr'xRiffsGfhx~vRmisSiff|LfjmkiLu|mnhj'qhifhvR'
w \x 4Dykmlnhopfhqr'i*lzjMlpqrmxRC
;e
'idyxRiLm Mk'iffsRlzqjmn vSiffs*vR'iCsSiff|LfjmkOiL'|mnhjmqhihai&kmlzsR|rmssKsSi}hixnhofhvR'i8iffn vRr'xiffs*} nhozr'iffs
'ixRivSfiffjmsRrmxRiOvRmn vvRmixRiffnhk'ixJrmjmk'ixsRvRnhjmkms,vR'i5nfftljmlz|vR'iiffn vRr'xRiffs6n xRiErysSiffkZj
r'vSvSixnhjy|Li,c'lzjlpqr'xRb
;eyvR'i&sStesRvSiffw sRn tes8 p H 0L

[e ?
P
[uL u
jlpqrmxRiMeZvRmlzs$lzsiffjm|Lfuk'iffktsSi}hixnhoiffn vRr'xRiffs 'i&iffn vRr'xi h qlp}hiffs$vR'i\jmnhwMi&fh
vRmn vPdmxRfw\dmv
h=11rrZ 'iIiffn vRr'xRi Sr sSdiff|lyiffs5vRmn vc'\lzs*nxRidmxRfwMdmvyn
sSiff|Lfjmkn vSvSiffw\dmvIetvR'isStusSvSiffwvSfEiffozlz|lpvnkmiffsR|LxlpdmvRlzfjfhUvRmiM|nhozopixffs8dmxRfhyopiffw 'i\iffn vRrmxRi
< Gl sSdiff|liffs*vRmn vIc'Mlsj'fhvn6|LfjuxwMn vRlpfjdmxRfw\dmv 'i8iffn vRr'xRi = h sSdiff|lyiffs
vRmn vGc'PlzjmlpvRlzn vSiffs-n$sRr'ZkmlznhopfhqrmiGnhjmk =
h iffjy|Lfek'iffs-vRmn vvRmlzsDlzsvR'i~xsSv-sr'Zkmlznhopfhqr'i
sSf0n xffmlopi z LSL E
/ $iffjm|Lfuk'iffs$vRmn vfr'vfhUnhozoDvR'i\sStusSvSiffwr'vSvSixnhjy|LiffsIsSf0n x
fhvR'iffw lzjylpvRlzn vSisRr'Zkmlznhozfhqr'iffs
s\w\iffjvRlpfj'iffkiffn xozlzixff)aiOn xRinhosSflzjvSixiffsSvSiffklzjqhiffj'ixnhozlpfflj'qEfrmx\dmxRfhyopiffwn vRlz|6kylznhopfhqr'i
dmxRiffkylz|LvSfhxMvSffhvR'ixJsStusSvSiffwMs rmsff~UivSxnhlzj'iff
k BEBE8=DrysRlzj'qfjmopt;iffn vRr'xRiffsMvRyn vn xRifhvR
nhr'vSfwMn vRl|nhozoptEnh|ermlpxn yopi,kmr'xlzjmqxrmjvRlzw\i,nhjmklzjyk'idiffjmk'iffjvfh~vRmi
Cf vRnhsSg" mi,sr'ysSiv
fhKiffn vRr'xRiffsCxRfwlpqr'xRiO0vRmn v8yv8vRmlsIrynhozl|n vRlpfjn xRi6ljlpqr'xRi0eOi6xRiix8vSfvR'iffwnhs
vR'N
4
2Ft24-< 5 iy8EB=2iffn vRr'xRiMsSiv
GunhwMdyopiffs$fh5iffn vRrmxRiffsvRmn v8n xij'fhvvRnhsSglzjmk'idiffjmkmiffjv
lzjm|ormk'i S} Z Zr +M c5 L c r nhjmkOvR'iCmnhjmkuozn iffozoziffkiffn vRr'xRiffs
`l ^p^ `, Q ff Q _~b?h^


iJqhfnhoKfhPvR'i H '$D IdmxRiffkylz|LvSfhx\lzs&vSflzk'iffjvRlpth)fhx,iffnh|iL'|ynhj'qhihaP'ivR'ix,fhx
'
j'fhv$vR'iMsStusSvSiffw|LfhxRxiff|LvRoptryjmk'ixsSvSfefukvR'iMrysSixffs$r'vSvSixnhjm|Lih\sw\iffjvRlpfj'iffkn f }hihP'iffjvR'i
kmlznhozfhqr'iffsKUixRi8vSxnhjysR|Lxlpiffk0et0ermwMnhjysn vSixPvR'iCkmn vRn|Lfozoziff|LvRlpfj5nhsP|LfwMdyopivSiffkZmvRmiCermwMnhj
ozn iffopixsKjmfhv*fjmoptJvSxnhjmsR|Lxlpiffk0vR'i8rmsSixsyr'vSvSixnhjy|Liffsyyrmv*nhozsSfozn iffozoziffk6iffnh|ErmvSvSixnhjm|Li8PlpvRn
sSiffwMnhjvRlz|&|n vSiqhfhxRtxidmxRiffsSiffjvRlzj'q6vR'i8vRnhsSgvRmn vPvRmi&rmsRix5nhsnhsRgelzjmq vSfJd"ixfhxw mlzs
ozn iffols6|nhozoziffkvR'i [ L 'iEsRtesSvSiffwsJ$lznhozfhqr'inhjmn qhixk'iff|lk'iffsnhw\fjmqsRi}hixnho
kmlixRiffjvKtedfhvR'iffsSiffsKdmxfekmry|Liffk6etJvR'iCwMfekmryopih'nhjmkOopfhqs*lpvRsKted"fhvRmiffsRlzs5n fr'vKmn v5vRnhsSg
vR'irmsSixK5nhs*nhsSgulzj'q vSf\dixRfhxwOuvR'i$lznhopfhqrmiIEnhjmn qhix s*tedfhvR'iffsRlzs5lzsUguj'f PjOnhsKvR'i
p iEkmlzsSvRlzjmqrmlzsRfr'xM|onhsRsSiffs\fh$sSdfhghiffjoznhj'qrmn qhiryjmk'ixsSvRnhjykmlzj'qEfr'vR|Lfw\iffsMnhsSiffk
fj|Lfw\dn xlzj'q\vR'i [ \h p L vR'i, L nhjmkxRiff|LfhqjmlpvRlpfjxRiffsRrmopvRs5fhxP|n xknhjmkOvSiffopid'fj'i
jermw8ixsff
D67FDD8962-|LfhxRxiff|LvRoptlzk'iffjvRlyiffkvR'i&vRnhsSgEnhjmknhjtkmlpqlpvsSvSxlzjmqsUixRi,nhozsSf
Vp$ V

fi

2Zy222 S2-



7M1m



72

}(f\gg{
P'&
Z&1&==P
ff N
}(}(}{ H.{ %AB'&!5A !%5.'&1! }(f\gg{( le{}\
(z(}{ H.{ %AB'&
{. {
(}{f{
{ "2A;<A!5'
;
}{g{l{.{
}.<({f =
5.fi5;<
}(<f\}1}<ge
}<g}..<({f
}{ =g{1}<\

}({ o<gf\} / ///
}({f}
}(H<g{ go\<{ / ///
}({f{}
}g{ g}.(fz . / ///
}({f}{
<{<E A95( 5A
}({f
}(f<{<. A;<A!
}({f}{
( ze(f<}9<.g (
}({fz
}Hzg .(f<}9<.g / C
}({fz
}f\}

5!;
}({f}{
}( leg{
g /
}({f{
}lg .g{
g /
}({f}/
}\ \{o} %:B=

}({f}}{
}( l
\ \{o} g (
}({f}{}
}zg \ \{o} g / C
}({f}}{
} . <.( l
1{{g ((
}({f}
(g{\ \
5pff
K"5z fi
}({f}}{
}.lz{ ( \
5!fin=! %
}<{(H{
(.g pf< ;!!nn?
(n&=!=P
+
} \} (.g p ;9n<
(#&9!==P
+
Dlzqr'xRiIea'iffn vRrmxRiIiffjm|Lfukmlzj'q\fhxceiff|Lfjyk )
'|mnhjmqhiIfhw

\x 4Dy

( /
/

{

* *
!(
!(




/
/
/
/
/

!(
/
/
/
/
/
/
/
/
/
(

.<

kmlznhopfhqrmih

B\~#
P

xiff|Lfhq'-xRiff|Lfhq jermwCafhxkms-nhsRxSkmr'xn vRlzfjZk'vRw\n q'xRq wMfekmnholpvtnh|LvRrmnhoGw\fukmnhozlzvtfh
vRmi8rmsSixrmvSvSixnhjm|Lih

Z~#
snhozlpiffjm|Li|Lf}hixn qhihClzjy|LfjmsRlzsSvSiffjy|Lth|LfjvSiLevsmlpvvSfhd |Lfjuk'iffjy|Lih$kyl|Lfj'k'iffjm|Lih
nhrmvSf cu
: sRrm||LiffsRsff
J

9

P~#

~2 #}p~2#~$

rmvSvSixnhjm|Li8et0r'vSvSixnhjm|Lih~rmvSvlzkZ'xRidmxfw\dmvm|LfjuyxwMn vRlpfjZmsRrmkylznho

xrmjmjmlzjmq;vRnhoozlpiffsjermw,r'vSvRsffIjrmw\HxRidmxRfw\dyvRs$d"ix|LiffjvHxRidyxRfw\dmvRsjermw,|LfjuxwMs
dix|Liffjv|LfjuyxwMsmjermw,sRr'Zkmlnhozsdix|LiffjvsRr'Zkmlnhozsukmlznhokmrmxn vRlpfj

lpqr'xRi8e~r'vSfwMn vRl|vRnhsSglzjmk'idiffjmk'iffjv5iffn vRrmxRiffs*nff} nhlzozn opiIn v*xrmjvRlzw\ih

|LfhxRxRiff|LvRozt;xiff|LfhqjmlpiffkZ,/
DBh4D2 4fi
4E26 E|LfhxRxRiff|LvRozt;xiff|Lfhqjmlpiffk?vR'ivRnhsSgyr'vvR'ixRi
5nhsnhjixRxfhxMlzjxRiff|Lfhqjmlpfflj'qn|nhozozlj'q|n xkjermw8ix\fhxnd'fj'ijermw8ixffI;+D 4E26
kmlkj'fhv&|LfhxRxRiff|LvRoptlk'iffjvRlztvR'i0rmsSixffs8vRnhsSg"

F/D86FejGCvR'iJxRiff|Lfhqjmlzix,kylzkj'fhv&qhiv
nhjtElzj'dr'v*vSf0dmxRfu|LiffsRs$nhjmksSf0vR'iw\fukmrmoziCkmlzkj'fhvifflpvR'ixff mlzs|nhjn xlzsSi&ifflpvR'ix$iff|nhrmsSi
vR'i0rmsSix&kmlzkjmfhv&sRn tnhjtvRmlj'qfhx&iff|nhrmsSivRmi6xRiff|Lfhqjylpix,Unhs,j'fhv&ozlzsSvSiffjmlj'q'iffjvR'i0rmsSix
Vp$ 1

fi>

72

> 9?f7A@d.22

sSdfhghih '
i'D767FDD8962J|oznhsRsUnh||LfrmjvRs5fhx*u,;hez afhZvR'iPiL'|ynhj'qhiffsKlzjMvRmiP|LfhxRdyrys 'i
DBh4D2 4fi 4E26 nh||LfrmjvRsGfhxK\uMDfhvR'i5iLu|mnhj'qhiffs 'i%D 4E26 |onhsRs)nh||LfrmjvRs
fhx~mff6/ uM)fhvR'iiLu|mnhj'qhiffsUnhjmkMvR'

F{D8967Fej|onhsRs~nh||LfrmjvRsafhx5hfi
;6fi
;ez MGfhvR'i
iL'|mnhj'qhiffsff
mi H u- MdmxRiffkmlz|LvSfhx0lzs6vSxnhlzj'iffkrmsRlj'qrmozoptnhr'vSfwMn vRl|Eiffn vRr'xRiffsff 'iffsSi
iffn vRr'xRiffsn xRiIvR'i&P|LfrmsRvRlzp| k 4- Diffn vRr'xiffsZffiffn vRr'xRiffsnhjmk$lznhozfhqr'iCnhjmn qhix$nhjmk$lzsR|LfrmxsSi
lzsSvSfhxRtJiffn vRrmxRiffsmqlp}hiffjlzjOlpqr'xRiCJaPnhjmk'ozn iffozopiffk6iffn vRr'xiffs*UixRi8j'fhvrysSiffkZ
iMi} nhozrmn vSi\vRmi\fr'xRHUn H u- |oznhsRslyix$txRidfhxRvRlzjmqJnh||r'xnh|Lth-dmxiff|lzsRlpfjZ
xRiff|nhozonhjmkOvR'i8|n vSiqhfhxlpffn vRlpfj|Lfj'rmsRlpfjOwMn vSxl" mlzsK|onhsRsRlyix*lsKvSxnhlzj'iffkfjnhozoZvRmiIiffn vRrmxRiffs
fhx*vR'iImfopi$vSxnhlzjmlzjmq,sRivynhjmkOvR'iffjvSiffsSvSiffkfjOvRmi8'iffozkuHfrmvKvSiffsSvsSiv
n yopiEsRrmwMwn xlpiffs\vR'iEf }hixnhooPnh||r'xnh|LtxRiffsRrmopvRsMfhvR'iEsStusSvSiffw vSxnhlzj'iffkfjvR'iE'fopi
vSxnhlzjylzj'qsSivnhjmkvSiffsRvSiffkfjvR'ivSiffsRvsSivMk'iffsR|Lxlpiffk;ljceiff|LvRlpf
j ;e mi0yxsSv\ozlzjmiJfh n yopi
xRidmxiffsSiffjvRsCvR'i0nh||r'xnh|LtxRfwnhop5nfftusCqrmiffsRsRlzj'qEvR'i6wn fhxlzvt|oznhssJg
F{D8967F
jDGvRmls8lzsCvR'i
G94- 8m i8n qnhlzjmsSvIPmlz|vR'ifhvR'ixCxRiffsRrmozvRsIsR'frmok"i6|Lfw\dyn xRiffkZ 'i6sSiff|Lfjmkxf Iozn iffozopiffk
4 2F 4E2 6ysRmf svR'iCnh||r'xnh|LtynhsSiffkEfjrmslzj'qnhozovR'i8iffn vRr'xRiffsnff} nhlzozn yozi8xRfw vR'i&sStusSvSiffw
w\fukmrmopiffsff ylzs*|oznhsRslyix|nhjlzk'iffjvRlptOffixxRfhxs*ivSvSixvRynhjvR'iCynhsSiffozlj'ihUjiLudixS
lzw\iffjv5nhsxrmjOvSf0sSii,lp)vR'i&|LxRfsRsH} nhozlzkmn vRlzfjw\ivR'fukk'iffsR|Lxlpiffkljceiff|LvRlpf
j ;d"ixfhxws*UfhxsSi
vRmnhjrmsRlzj'q0vR'i'fopikyn vRnOfjvR'iJsRnhw\iMvSiffsRvCsSiv mlzs$iLudixlzw\iffjvIsR'faiffkvRmn vIvRmixRi5nhs
ozlpvSvRozi$ozfsRsKfhnh||r'xnh|LtO'iffjOrysRlzj'q\|LxRfsRsSH}nhozlkmn vRlpfjuM

~#N B
G=D
4 m8
8 wMnSfhxlpv tO|oznhsRs
42F 4E2 6

Pfi#~=
fie
; z


uz&

n yopiM *iffsRrmozvRs5fhxk'ivSiff|LvRlzj'qJff~xRxRfhxsKrmslzj'qD

BEB=8ED

lpqr'xiJJsR'fs$sSfw\i&vSfhdd"ixfhxwlzj'q\xrmoziffsvRmn v)D B=BE8EDoziffn xjmsP'iffjEqlp}hiffjnhozovRmiCiffn
vRr'xRiffsff 'iffsSi$xrmoziffsUkylpxRiff|LvRoptMxiyiff|LvKvR'iIrmsSirmozjmiffsRs~fh-vR'iCiffn vRr'xiffs~PfhvSiIvRmn vKsSfw\ifhDvR'i
xrmoziffsDrysSi4D D6iffn vRr'xiffsGlzj&|Lfw8yljmn vRlpfj,lpvR\Jiffn vRr'xiffsGsRrm|&nhsK LL\M GGxRi}ulpfrmssSvRrykmlpiffs
nhopghixKivKnho/pm hhh| Umn }hiInhozsSf,sR'f Pj0ffEiffn vRr'xiffs5vSfCi$rmsSirmo/Di$ynhkJnhosSf&ted"fhvRmiffsRlpiffk
vRmn vKiffn vRr'xRiffsKxRfwvR'iI$lznhozfhqr'iEnhjmn qhixnhjmk0vR'ikmlzs|Lfr'xsSimlzsRvSfhxRtwMlpqv5i$rysSiryodyxRiffkmlz|
vSfhxs$fhUff;ixRxRfhxsZ'f Ui}hixvR'iffsSi,iffn vRr'xRiffs$xn xRiffoztn dmdiffn xlzjvRmiCxryopiffsPlpvRvRmi&iL'|LidmvRlpfjfh
p mlzslsPlzjnh||Lfhxkmnhjy|Li&PlpvRdyxRi}elzfrmsiLudixlzw\iffjvRsPmlz|EsRmf vRmn v$vR'iffsSi,iffn vRrmxRiffs
k'fj'fhvMnhkyksRlpqjml"|nhjvRoptvSfvR'iOdixRfhxwMnhjm|LiOfhvR'iE Fi iffn vRr'xisSivEnhopghix6ivnho/p
hhh|
iKnhozsSfxRidfhxRv-dmxRiff|lzsRlzfjCnhjmkCxRiff|nhozofhxDiffnh|\|n vSiqhfhxRt&fjCvRmiU'iffokuHfr'vvSiffsRvsRiv 'i~xRiffsrmopvRs
n xRiIsRmf jOlzj n yopiffs*Mnhjmk ;e n yopiI,sR'fsKvRmn vKvR'iC|oznhsRsRl"|n vRlpfj0nh||r'xnh|Lt0xn vSi8lzsKn,xiffsRrmopv
fhnImlzqMxn vSiPfh|LfhxxRiff|LvU|onhsRsRl|n vRlzfjMfhx~vR'C
D67FDD8692Jnhjm
k
F/D86Fej|oznhsRsen vavR'iP|LfsSvafh
n0opf Uix$xn vSi,fh#
x 4E26 nhjy^
k DBh4D2 4 4E276 mlzsPlzsdmxRfhyn optOkmr'i&vSfJvRmiCnh|Lv$vRmn v
vR'ixRiCn xRiiUixiL'nhw\dyoziffsKfh-vRmiffsSi8|n vSiqhfhxlpiffslzj0vR'i8vSxnhlzjmlzjmq,sRiv
jsSfw\iMsRlpvRryn vRlpfjmsfj'i\wMlpqv$j'fhvIj'iiffkvSfkmlzsSvRlj'qrmlzsR"iv aiiffjvR'i\kmlixRiffjv$wMlzsrmjmk'ixS
sSvRnhjmkylzj'q|n vSiqhfhxlpiffsU
ieF{D8967FejaD C 4E26 nhjmk DBh4D2 4fi
4E26 'ixRifhxRih*iLudixl
w\iffjvRsGUixRidixRfhxw\iffkMvRmn v~|Lfozozn dsSiffkMvR'iffsR
;dmxfhyopiffwMn vRlz|K|n vSiqhfhxlpiffs5lzjvSf8fj'i|n vSiqhfhxRtEg
Vp$

"

fi

2Zy222 S2-



7M1m

72



PZ-) *5k'vRwMn q xRiff|Lfhq|LfjvRnhlzjys6 xRfwMxiff|Lfhq
sStesSozn iffo
jermw8Ufhxkms # L \H[
P-) K^Uk'vRw\n q nhsRxSkmr'xn vRlzfj"!'#xRiff|Lfhq HqhxnhwMwMn x
sStesSozn iff
5lzoozw\ivR'fukuHqhxnhw6#
xRiff|Lfhq&|LfjvRnhlzjmsahw8tu$
xRiff|Lfhq jermwCafhxkys% # L \H[
v '
sRnhopdixRvRlzw\(
u&
vSfhd'|Lfju"k'iffjm|L"
uhu # l & MhH [
sSdfhghiffjukylpqlp&
v '
sRnhopdixRvRlzw\(
u&
|Lfj'dixRvRlzw\)
u # z , MhH [
sSdfhghiffjukylpqlp&
v '
vSfhd'|Lfjuk'iffjy|L"
uh ;h z , MhH [
sSdfhghiffjukylpqlp&
v
snhopdixRvRlzw\)
u
sStusozn iff*
5P5'
l &
sSdfhghiffjukmlpqlp
MhH [
j !
r'vSvl,
k ! sStusozn iff.
ZD)
K^
U
kmlp|Lfju"k'iffjm|Li
nhsSxSkyr'xn vRlpf+
u# z
, MhH [

lpqr'xRiMuPsRr'sSiv~fhxrmopiffsUopiffn xj'iffk6t 'iffj6qlp}hiffjJvR'i$nhr'vSfwMn vRlz|iffn vRr'xRiffs5fhxUkmivSixS
wMlzjmlzjmq H u-

/

~$B

D6FDD8962

F{D8967Fej
C 4E26
DBh4D2 4 fi 4E26

u~2'


u
he#h

0|1q



u
u

iff|nhozoZfhx iffsRvsSivrmslzj'q\PrmvSfwMn vRlz|iffn vRr'xRiffs
n yopi8eUGxRiff|lsRlpfj0nhjmk

67FDD8692Z mlzsIxRiffsRrmopvSiffklzjnxRiff|LfhqjmlpvRlpfjnh||r'xnh|Ltfhhe 0GnEhe lzw\dmxf }hiffw\iffjv8f}hix
vR'i$ynhsSiffolzj'ifhh; J'Pmlz|0lzs5vRmidix|LiffjvRn qhiIfheD
67FDD896O
2 iL'|mnhj'qhiffsff 'i$dmxRiff|lsRlpfj0nhjmk
xRiff|nhozowMn vSxl0lzsKqlp}hiffjlj n yozi'

D67FDD8962

F{D8967F
j
4E26
DBh4D2 4 fi 4E26

D67FDDD8692






F/D86Fej


;fi;u
h;


4E26
uh
h
ff
h

DBh4D2 4





n yozi3;eUUfj'rmsRlpfj0n vSxlOfhx iffsRvsSivrmslzj'q\PrmvSfwMn vRlz|iffn vRr'xRiffs
Vp$ 2

fi>

72

/

> 9?f7A@d.22

1~2'

~$q

67FDD8692

67FDD8692

u
h;ez

0z

heJ
'

iff|nhozoZfhx iffsRvsSivrmslzj'q\PrmvSfwMn vRlz|iffn vRr'xRiffs
n yopi'UGxRiff|lsRlpfj0nhjmk
1Y

^t Q hb? X bH^` Q Q _~b h^

iqhfnho-fhvR'i8alzsKvSf6dmxRiffkml|Lv'fjvR'iInhsRlzs5fhGlzj'fhxwMn vRlzfj0vRmn vlpvynhsKiffn xoptOlzj0vR'iCkmlzn
'
opfhqr'ihhP'ivR'ixDfhxjmfhvDvRmiUsStusSvSiffwlozohiUn opi~vSf|Lfw\dyozivSiUvRmiUrmsRixffsDvRnhsSg mi~fr'vSdyrmvD|oznhssSiffs
n xRi&ynhsSiffkEfjvR'iCfr'x$kmlznhopfhqrmiC|n vSiqhfhxlpiffskmiffsR|Lxlpiffkn f}hih$Pf Ui}hixffnhs 4iej
B
w \x 4Dy
nhjm
k 24- 5e<4
D8n xRi,vSxRiffn vSiffknhs$iffermlp} nhopiffjvRoptdmxRfhopiffwMn vRlz|CtEvR'i\sStusSvSiffwOnhsIlzoozrmsSvSxn vSiffkElzj
lpqr'xRieuvR'iffsS
;&|n vSiqhfhxlpiffs*n xi$|Lfoozn dysSiffk0lzjvSb
f B=D7F
Gy
8 4E2 6DPfhvSivRmn v5vRmlzsU|n vSiqhfhxlpffn vRlpfj
lzs*ljm'ixRiffjvRopt6jmflzsStJ"iff|nhrysSi8lpvls*lzw\dfsRsRlzyopivSfguj'f vR'iIxRiffnhoxRiffnhsSfjmst0n|nhoopixmnhj'qsPr'd
fhxnlzffn xkvRn ghiffsMf}hixvR'i|nhozoH mi0|nhoopixMwMnfftmnhjmqrmd"iff|nhrysSisR'iOlzs,xrmsSvSxn vSiffklpvR
vR'iJsStesRvSiffwO)fhx&s'i6wMn tsRlzw\doptkylzsRozlpghiMnhrmvSfwMn vRlpfjZ)fhx,'ixC|mlzozkwMnfftmnff}hi0sSvRn xRvSiffk|LxRtulzj'q'
culzwlzozn xopthhfjmilpffn xkMwMnfft6mnff}hi$opf |Lfjukmiffjm|LiPlzjMvR'iPsStusSvSiffwOsUn ylzolpvt,vSf8xiff|Lf }hixKxRfw ixRxfhxs
nhjmk,rmsSiKn$|LfjmsSix}n vRlp}hi*n dydmxRfnh|,vRyn vDxiffsRrmopvRslzjCvRn gelj'q$f }hix~wMnhjtC|nhoozs mlzozianhj'fhvR'ix)Plpffn xk
wMn tEi\w\fhxRi,lzoozlzj'qMvSfozivvRmiMsStesRvSiffwvSxRtEvSfOxiff|Lf }hixff\Pi}hixRvR'iffopiffsRsffai\vRn ghivR'iffsSiMermwMnhj
nh|LvRlpfjmsnhsnMermwMnhjOozn iffozolzj'q8fhvR'iffsRiI|nhozozs*nhsKdyxRfhyopiffwMn vRl| a$lp}hiffjOvRmls5ylzjmn xRtJ|oznhsRsRlp|n vRlpfjZ
n dmdmxfff'lzwMn vSiffopA
;h;fh-vRmiI|nhozozsKlzj0vR'i|LfhxRdrms5fh-hhkmlznhopfhqrmiffsKn xR
BEDFGfi
8 4E2 60nhjykh
n xR
24-< 5ff

676=8"
2 4350z= 1

\

~

9

\

Apn

~2 #60zn q

mlsKsSiff|LvRlpfjOdmxiffsSiffjvRs*xRiffsRrmopvRs5fhx*dmxiffkmlz|LvRlzj'q,dmxfhyopiffwMn vRlz|$kmlnhopfhqr'iffs n gelj'qMlzjvSfMnh||LfrmjvPvR'i
nh|LvCvRmn v,nEdmxRfhyopiffwn vRlz|kmlnhopfhqr'iw&rmsSv&iMdmxRiffkml|LvSiffkn v,ndflzjv8lzjvR'i0kmlznhopfhqr'i6'ixRi6vR'i
sStusSvSiffw |nhjkmf6sSfw\ivRylzj'q6n fr'vlpv"ai,|Lfw\dyn xRi8 RR /ff/ nh||r'xnh|LtEn vSixmn }elj'q6sSiiffjfjmozt
vR'ixsSv*iL'|mnhjmqhi8fhxvR'iyxsRvKvUf6iL'|mnhjmqhiffslzvR Ly nh||r'xnh|Ltn vSix$mnff}ulzj'q6sSiiffj
vR'i5'foziakmlznhozfhqr'ihDmfhx)iffnh|\fhmvRmiffsSiKsRlpvRrmn vRlpfjys aiKnhozsSf$|Lfw\dyn xiUxiffsRrmopvRsDfhxDvRm
4
2F 4E2 6
iffn vRr'xRi,sSivMnhsk'iffsR|Lxlpiffkiffn xozlpixKlpvRnhjmkElpvR'fr'vPvR'i H '$D *iffn vRr'xRi\nhjmkElpvR
vR'i8ynhjmkuozn iffozopiffk6iffn vRr'xRi\u-
n yopiCMsRrmwMwMn xlpiffs5vR'iIf}hixnhozonh||rmxnh|LtOxRiffsRrmozvRs 'ivRmxRii8|LfozrmwjmsKdmxRiffsRiffjv*xiffsRrmopvRs5fhx

)'|ynhj'qhiJ 7)'|ynhj'qhiffs,ffInhjykf}hixvRmi8'foziCkmlznhozfhqr'ih 'i8yxsSv*xf qlp}hiffsvR'i8ynhsRiffozlzj'i
xRiffsRryopvCPmlz|;xRidyxRiffsSiffjvRsCvRmiJdyxRiffkmlz|LvRlpfjnh||r'xnh|LtxRfwnhop5nfftus\qr'iffsRsRlj'qEvRmi0wn fhxlzvt|onhsRs
culzjy|Liz fh5vR'ikylznhopfhqr'iffsIn xi2fiD
4 )
5
67698 &kylznhopfhqr'iffsai|nhj;nh|ylpi}hiJz nh||r'xnh|Lt
xRfwsRlzw\dopt0qr'iffsRsRlzjmqA24 )
5 e6698 fhxiffnh|kmlznhopfhqr'ih 'i,sSiff|LfjmkxRfqlp}hiffsxRiffsRryopvRsPrysRlzj'q
fjmoptMnhrmvSfwMn vRlz|iffn vRrmxRiffsuyr'valpvR'fr'vUvR'i H u$D Giffn vRr'xRih miPvRmlzxkMxRfrmsSiffsUvR'i
sRnhw\iMnhrmvSfwMn vRlz|\iffn vRr'xRiffs$yrmv$nhkykmslzj H u$D mlzsiffn vRrmxRiMlzsfhmvRnhlzj'iffkfhxIfhvR
vR'ivSxnhlzjmlj'q8nhjmk6vR'ivSiffsSvUsRivurysRlzj'qIvRmi|LxRfssH}nholzkmn vRlpfjJw\ivR'fuk6kylzsR|rmsRsRiffkMlzjOceiff|LvRlpfjA;e 'i
fr'xRvRnhjyk,yvRMxRfs~sRmf xRiffsRrmopvRs~rmsRlzj'qvRmisRr'ysRivGfhZiffn vRrmxRiffsGvRmn vUn xRifhvRrmooptCnhr'vSfwn vRlz|
nhjmkOvRnhsRglzjyk'idiffjmk'iffjv5nhsk'iffsR|LxlpiffkOlzjcuiff|LvRlpfjO'
Vp$

!

fi

2Zy222 S2-



7M1m



72

mi0nhrmvSfwMn vRlz|JxRiffsRrmozvRsCqlz}hiffjlzj;xRf n xRiOsRlpqjyl|nhjvRoptmlpq'ix,etndynhlpxiffkvHvSiffsRv\vRmnhj
vR'i,ynhsSiffolzj'i&fhxInhozoDvR'xii,sRiff|LvRlpfjms$fh~vRmiMkmlznhopfhqr'iOk'7hhev8ez $u;hek'7hheDv8Ie
$uh' km4 hhM8v ,< ;e 'y9
$uh'
f s0nhjyksR'f nh||r'xnh|Lt?lzw\dmxRf}hiffw\iffjvRsqnhlzjmiffketvR'iEnhkykmlpvRlpfjfhIynhjmkuozn iffozopiffk
iffn vRr'xRiffs miffsSixRf sqlp}hi
n 2FBy i8n qnhljmsSvOml|vSf|Lfw\dn xRivR'ixRiffsRryopvRsOlzjxRf Pse
;eGnhjmkeW
iffsRrmopvRs,rmsRlj'qEnhozoUvR'iOnhr'vSfwn vRlz|Jiffn vRr'xiffs,dyozrmsCvR'iOmnhjmkuon "iffoopiffku-
n xRi0qlp}hiffjlzj;xRfejvR'iffsSi0iLedixlw\iffjvRsff)vR'iOmnhjmkuozn iffozoziffku- Iiffn vRr'xilzs,rmsSiffk
fhx6vSxnhljmlzj'qnhjykvSiffsSvRlj'q'5fw\dyn xlzjmqvRmlzs6xRiffsRrmozvlpvRvR'isSiff|LfjmkxRf sR'fsJvRmn v0lpfj'i
mnhkndixRiff|LvJdyxRiffkmlz|LvSfhx0fh H u- 6lzj?vR'ivSxnhlzjylzj'qnhjyk?vRmivSiffsRvsSivPvRmiffjvRmlzs
iffn vRr'xRi\Ufrmozklzjm|LxRiffnhsRi,nh||rmxnh|Lttefh
x )'|mnhj'qhixRfw uz vSfE eMt
fh)
x Gu|mnhj'qhiffs\ffIxRfw ez vSfhe#hMnhjmketEe fhxvRmiCmfopi&kylznhopfhqr'i0/ vSf
heM miffsSialzjy|LxRiffnhsSiffsDn xRiUslpqjml|nhjvetInPdynhlpxRiffkCvHvSiffsSv*k'7 hheh8v ez h
$uhh' ek'7 hhe
v8ez
$u ;hekm4 hhey8v e#m
$uh'
UfwMdyn xlzj'qvRmixRiffsRryopv~lzjxRf IlzvR\vR'ixiffsRrmopv~lj\xRf ;Is'f savRmn vavR'i H u-
dmxRiffkylz|LvSfhxGvRmn vGUimn }hi*vSxnhlzjmiffkM|nhjlzwMdmxRf }hiKdixRfhxwMnhjy|Lihyr'vG|Lfrmok\dfsRsRlpyoztI'iffopdw\fhxiKlpvR
kmlixRiffjvvSxnhlzjmlj'qOw\ivR'fukmsMk'iffnhozopthvR'iMxRiffsRryopvlzjxRf
;e-fhx8nhr'vSfwn vRlz|Miffn vRr'xRiffs8dyozrms H
u- sRmfrmozkEnhozoiv aiiffjvRmi,yqr'xRiffslzjxRfs80nhjmke-nhjmki,|ozfsSixIvSfOvR'i\xRiffsrmopvRs
lzj;xRfelpvRJ
)'|mnhj'qhiffsffIeanhkmkylzj'q H '$D 8xRiffsrmopvRs,lzj;nhjlzjm|LxRiffnhsSi0fh& z
ml|lsCj'fhv,sRlpqjyl|nhjv6|Lfw\dyn xRi0xRfs,nhjmJ
k ;'fh
x )'|mnhj'qhi6fjmopth
B=BE8EDk'feiffs,j'fhv
rmsSi$vR'i H u- Giffn vRr'xiIlzj6lzvRs5xrmopiffsSiv5nhjmk0k'feiffs5j'fhvKtelziffozk6nhj0lzwMdmxRf }hiffwMiffjvKf}hix*vR'i
sStusSvSiffw vSxnhlzj'iffkfjmoptOfjEvR'iCnhr'vSfwn vRlz|8iffn vRr'xRiffsff 'i&sStesRvSiffwvSxnhlj'iffkfjEvR'i8P'fopiCkylznhopfhqr'i
lpvRnhr'vSfwMn vRlz|iffn vRr'xiffsGdyozrys H '$D )nhozsSf8k'feiffs~jmfhv~tulpiffozk\nhjlw\dmxRf}hiffw\iffjv~f }hix5vR'i
sStusSvSiffw vSxnhlzj'iffk0lpvRmfr'v H '$D
:9;<=;<?>

4D

5 iy8=BE8Eiy
8Ei2A@8E4E2"D8"

*fsaCnhjmkJqlp}hiPvR'i*xRiffsRryopvRs~rmslzj'qvR'i'4"
27Ft72fi4D 5
iy8=B=26iffn vRr'xiPsSivUk'iffsR|Lxlpiffk\lzjDlzqr'xRi

ElpvR'frmvCnhjmk;lpvRvR'i H u$D Iiffn vRr'xRihGxiffsSdiff|LvRlp}hiffopth 'iffsSiJxRiffsRryopvRsCn xi0slpqjmlm
|nhjvRoptEn f }hi\vRmi&nhsSiffozlzj'i,rmsRlj'q6n0dynhlzxRiffkvHvSiffsRvlpvR^)'|ynhj'qhiffsMffIJqlp}ulzj'qJnhjlzjm|LxRiffnhsRi&fh
< ;ez k'7 hhe-8v IeeD9d $uh' IrysRlzj'
q 24- 5
iy
8EB=2iffn vRr'xRiffsIPlpvR H u$D Oat
|Lfw\dyn xlzj'qxf sPJnhjykefj'i&fhysSixR}hiffs$nhjlzjm|LxRiffnhsRi&lzjvR'+
4"

27Ft24- 5
iy
8EB=2iffn vRr'xRiffssSiv
'iffjvR'iiffn vRr'xRi H u$D Glzsanhkmk'iffkJrmsRlzj'
q Gu|mnhj'qhiffs$ffI8nhjmkP'fopikmlnhopfhqr'ih 'i
lzjm|LxRiffnhsSi8fh'
x )'|mnhj'qhiffs,ffIMsR'f PsnMvSxiffjmkk'7 hhe8v , #
$uZmixRiffnhs*vR'i,
lzjm|LxiffnhsSi$fhxKvR'i$mfopikmlznhopfhqr'i$lzs5sSvRn vRlzsRvRlz|nhozoptJsRlpqjml"|nhjvUtJn,dynhlpxRiffk6vHvSiffsSvCkm4 hhey8v #;eu
$uh ;
opvR'fr'qevRm3
2fi4-< 5 iy8EB92Oiffn vRr'xRiIsSivRs*n xi8n\sRr'ysRivUfhDvR'fsSiiffn vRr'xRiffsrmsSiffkOlzjJxRf ;emlpv
lzsUdfsRsRlpopifhxKvR'iffwvSf\dixRfhxw"ivSvSixKiff|nhrmsRivRm3
2fi4-< 5 iy8EB920iffn vRrmxRiffs*n xRiw\fhxRiqhiffjmixnho/
nhjmk"iff|nhrysS3
B=BE8EDrmsRiffsnMqhxiiffk'tnhopqhfhxlpvRyw vSf6kmlsR|Lf }hix$lpvRs*xrmopiIsSivRs*mfh'
x Gu|mnhj'qhiffs&ffIe
vR'iMlzjy|LxRiffnhsSi\xRfwxRf
;JvSfxRf "fhvRfhUml|rmsSi H u- SlzsjmfhvIsRlpqjml"|nhjv
Ufw\dn xlzj'qCxRf s*&nhjmkJ'mj'ifflzvR'ix5fhml|JrmsSi H u$D efjmi$sRiiffsKn&sozlpqvUk'iqhxnhkmn
vRlpfjljExiffsRrmopvRs$fhxIvR'i\mfopi\kmlznhopfhqr'iMrysRlzj'
q 2fi4D 5
iy8=B=2iffn vRr'xRiffsMPf Ui}hixffvR'iMlzjy|LxRiffnhsSi
xRfwxRf Ps~vSfCxRfw ez vSf8 uH ;fha
x Gu|mnhj'qhiffsPffI$lzs)sSvRn vRlzsRvRlz|nhozopt8slpqjml|nhjv*k'7 hhe
v8euD
$u mlzs$sR'fsvRmn v8rysRlzj'q H u$D $lzj|LfwCylzjmn vRlpfjPlpvRvR'iMsSivIfh
24- 5
iy
8EB=2iffn vRr'xiffs*dmxRfukmrm|LiffsnMsSvRn vRlzsSvRl|nhozopt0sRlpqjml"|nhjvKlzjm|LxiffnhsSi8lzjOnh||r'xnh|LtOf }hix$nMsSivfh
nhr'vSfwMn vRl|iffn vRrmxRiffs*vRmn vPk'fiffsj'fhvlzjm|ormk'ivRmlzs5iffn vRr'xRih
Vp$

&

fi>

uB




;






72

> 9?f7A@d.22

~#

UnhsRiffozlzj'i
4"

27Fj'f H u-
4"

27FU H u$D
4"

27Ft24-<5 iy8EB=2;j'f H u-
4"

27Ft24-< 5 iy8EB=2 H u-
4"

27FUu-
4g 4
2F nhjmkuozn iffozopiffk"

CD#~2#2E3

z
uz

uz

e
hz

CDff~,3GFIH

z
ez
e
e
uH ;
he#


y#

z

'
h;e


u #

n yozi8e~P||rmxnh|LtxRiffsRrmozvRs5fhxPdmxRiffkmlz|LvRlj'qCdyxRfhyopiffwMn vRl|kylznhopfhqr'iffs
/

~$B

fi2 4D 5)
67698
B=D7F
Gy8 4E2 6

JNfi#

6
;h;e6

0zn\q

u #6
ffeH ;J

1~2'
hez&
;u 6

0z

e6
he6

iff|nhozoZlzvR)'|mnhj'qhiJPrmvSfwMn vRlz|I'iffn vRrmxRiffs
n yozi8e~GxRiff|lsRlpfjOnhjmk
miwMnhlzj\dr'xRdfsSi5fhZvR'iffsSi*iLudixlzw\iffjvRs~ls)vSfCk'ivSixwMlzj'iKmivR'ixanIkylznhopfhqr'ilsD hyh!
dmxRfhopiffwMn vRlz| vR'ixRifhxRiMrmslzj'qJvR'i,'fopi,kmlznhopfhqrmi,lsjmfhvIrmsSirmo-ljn0k'tujmnhwMl|CsStusSvSiffwO+:sRlzj'q
)'|ynhj'qhiffsffIOdmxRfukmrm|Liffs$nh||r'xn vSi6xRiffsRrmopvRsnhjykafrmokiffjmn yopi\vR'iMsRtesSvSiffwvSfnhkmn dmv8ljEfhxk'ix
vSf|Lfw\dopivSiIvR'i8kylznhopfhqr'ilzj0vR'iCn dmdmxRfhdmxlzn vSiwMnhjmj'ixff
:9;<=;KML

4iyS4G=8yfi8=y@8=4E2D8

*fPlzj&vRn yopi*qlp}hiffsvR'iKxiffsRrmopvRsrmsRlj'qynhjmkuozn iffozopiffkCnhjmk,nhr'vSfwMn vRlz|5iffn vRr'xRiffs~lzjm|ozrmkylzj'q*fhvR

u- Knhjyk H u- Ut|Lfw\dn xlzj'qMxf sMnhjyk"fj'i,|nhjsRiiCvRmn vPvR'ixRi,lzs
j fhvI}hixtw&rm|vSfi,qnhlzj'iffketnhkmkmlzj'qJvR'iMfhvRmixCmnhjmk'ozn iffozopiffkEiffn vRr'xiffsqlp}hiffjljDlzqr'xRi6
'
vSfOvR'iMmnhjykuozn iffozopiffkEnhjmku$D iffn vRr'xRiMsRiv\$jyoptvR'i\lzjm|LxiffnhsSi,fhxGu|mnhj'qhi&xRfw
evSfhz lzssRlzqjml|nhjvMk'4 Ihhe8v eH ;eD
$umfhx8vR'i\mfopi\r'vSvSixnhjm|LiMvRmixRiMlzs
nh|LvRrmnhozoztJnMkmiqhxnhkmn vRlpfjOfhxRiffsrmopvRs5xRfwhevSfJu #hJ
2 HN0z1q~2#

u~2'

'i8d"ixfhxwnhjm|Li8fh)vR'i&sStusSvSiffwvRyn vrmsRiffsnhr'vSfwMn vRl|8iffn vRr'xRiffs\lzjm|ormkmlzj'q H u-
fhxIvR'i,yxsRvIr'vSvSixnhjm|Lils$qlp}hiffjlzj n yopie ylzs$sStesRvSiffw mnhsnhjf}hixnhozoanh||r'xnh|Ltfh5he0
'iffsRi6xRiffsrmopvRs&sR'f vRmn vaqlp}hiffj;vR'iJyxsSvCiLu|mnhj'qhihavR'i0xrmopiffsSivCdmxRiffkml|LvRsCvRyn v0ffeH;fhPvR'i
kmlznhozfhqr'iffsMlozoKidmxfhyopiffwMn vRlz| 5ylzop
;h;fhIvR'iffw nh|LvRrmnhozoptPlzozoKih vRmidmxRfhopiffwMn vRlz|
kmlznhozfhqr'iffs lpv-|nhj&dmxRiffkylz|L

v ;u fh'vR'iffwOG$jm|LiUlpvdyxRiffkmlz|LvRsvRmn vDnkmlznhozfhqr'iGlzoohi~dmxRfhopiffwMn vRlz|
lpv*lzsK|LfhxxRiff|Lvhe fh-vRmiIvRlzw\ih
x )'|ynhj'qhiffsffI&lzsUsRrmw,
miPdixRfhxwMnhjm|LiPfhvR'i$sStusSvSiffwvRmn vUrysSiffs5nhr'vSfwMn vRlz|iffn vRr'xRiffsUfh
wMn xlziffklzj n yopi 'iffsSi0xRiffsrmopvRs,sR'f vRmn v~qlp}hiffjvRmi0yxsSv&vUfiL'|mnhj'qhiffsffavRmlsCxrmopiffsSiv
dmxRiffkylz|LvRsUvRmn v* fhvR'ikmlnhopfhqr'iffsUlzozoiPdyxRfhyopiffwMn vRl| ePmlzop)
;h;fhvR'iffw nh|LvRrmnhooptlozomih
vR'iKdmxRfhyoziffwMn vRlz|5kmlznhopfhqrmiffslpvG|nhj\dmxiffkmlz|LvDefh"vR'iffwOG$jm|Li*lpvdmxiffkmlz|LvRsvRmn v~nkylznhopfhqr'i
lzooZi8dmxRfhopiffwMn vRlz| lpv$lzs|LfhxRxiff|LvI e#hfhGvR'i&vRlzw\ih mlzsP|oznhsRsRlixPmnhs$nhjlzw\dmxf }hiffw\iffjvPfh
Vp$ 3

fi

2Zy222 S2-



7M1m



72

sRnhozlpiffjm|LiL|Lf}hixn qhiOu#h( nhsSxSkyr'xn vRlpfjP!u (i nhr'vSf cu
:*sRry||LiffsRsQieF
# p \
:*srm||LiffsR5
C 4E276 R sStusozn iffoA ZD) K^U? nhsSxR
nhr'vSf cu

kmr'xn vRlpfS
j ! ;ezff p Mh
u# hS
sRnhozlpiffjm|LiL|Lf}hixn qhi?u#T iHxRiff|Lfhq|LfjvRnhljms
isnhozlpiffjm|LiL|Lf}hixn qh?
S'iffopdU
inhsSxSkmrmxn vRlpfO
j e h # z Mh/
uhV
nhr'vSf c'
:sRrm||LiffsRW
4E26 (
sStusozn iff%

iHvSfhd'|Lfj'k'iffjm|L,
5P5'
# p \
uh kml"|Lfjuk'iffjy|LV
uuff
|Lfozoziff|L)
v uh ;h
iHvSfhd'|Lfjuk'iffjy|LX
nhsSxSkyr'xn vRlpfS
j ! eH ;h&
kmlznhopHfhxSwM"
!u p Mh


D8967Fej-

lpqr'xRiMh Psr'ysSiv0fh,xrmopiffs0opiffn xj'iffket BEBE8ED
k'ivSixwMlzjylzj'q,dmxRfhyoziffwMn vRlz|$kmlznhopfhqrmiffs

'iffjqlz}hiffjvR'inhr'vSfwn vRlz|iffn vRr'xRiffsOfhx

HvSfhd'|Lfjukmiffjm|LiYuhh)i nhsSxRkmr'xn vRlpfjP!u ) nhrmvSf cu:sRrm||LiffsRsieF
# p \
:*srm||LiffsR"
C 4E26 &
HxRiff|Lfhq jermwCafhxkmsQh-i nhsRxSkmr'xn vRlzfj!
nhr'vSf cu

eh p Mh
uZ
nhsSxSkyr'xn vRlpf[
j ! u Z
lzjy|LfjmsRlzsSvSiffjy|L,
! uh\

sRnholpiffjm|LiL|Lf }hixn qhT
inhsSxSkyr'xn vRlpfA
j ;eh%
ljm|LfjmsRlzsRvSiffjm|L6
! uzff # p \
uhhS
sRnhozlpiffjm|LiL|Lf}hixn qh?
uhhT
iHxRiff|Lfhq|LfjvRnhljms
isnhozlpiffjm|LiL|Lf}hixn qh?
S'iffopdU
inhsSxSkmrmxn vRlpfO
j H ;h # z Mh/
uh%
kml"|Lfjukmiffjm|L"
uuff%
HxRiff|Lfhq6|LfjvRnhlzjmsMSwCtu'

iHvSfhd'|Lfj'k'iffjm|LV
inhsSxSkyr'xn vRlpfA
j ! # z Mh/
uh"
inhsSxRkmr'xn vRlpf]
j !uV
nhsRxSkmr'xn vRlzf]
j !eH ;h
isRnhozlziffjm|LiL|Lf }hixn qh+
# z
Mh/


D8967Fej-

lpqr'xRiMffePsRrmysSivfh~xrmoziffsoziffn xj'iffkt^D BEB=8EDP'iffjqlp}hiffjvR'iN2fi4D
k'ivSixwMlzjylzj'q,dmxRfhyoziffwMn vRlz|$kmlznhopfhqrmiffs

5
iy8=B=2iffn vRr'xRiffs$fhx

hljExiff|nhozo)nhjmkh;eljEdyxRiff|lzsRlpfj"fhxInhjf }hixnhozoGlzw\dyxRf }hiffw\iffjvIlzjnh||r'xnh|Ltfh5e
f }hix$rmsRlj'q,vR'i$yxsSvKiL'|ynhj'qhiCnhopfjmih
2 _^`CD~=

A#Bq

'~aM#

sRr'ysRiv*fhvR'i8xryopiffs5xfw vR'i8sRtesSvSiffw vRmn vrmsSiffs*nhr'vSfwn vRlz|Iiffn vRr'xRiffsPfhx')'|mnhjmqhiffsCffIn xRi

qlp}hiffj6lzj\lpqr'xRiIh&xRf ;evRn yopia$j'iKfhysSix}n vRlpfjxfwvR'iffsSited"fhvRmiffsSiffs~lzsvRmi|oznhsRsRlpyixffs
dmxRiixRiffjm|Li$fhx*vR'i iffn vRr'xRif }hixPvR'i$iffn vRrmxRifhxKvRmiIjrmwCix5fh-Ufhxkms5xRiff|Lfhqjylpiffk
RR Z &~h R$j'iKUfrmozk\iLudiff|Lv~opfj'qhixUr'vSvSixnhjm|Liffs~vSf8i*w\fhxRikyl{6|rmopvyr'v)vR'ioziffn xj'iffk
xrmoziffsSivRs,lzjmkmlz|n vSi0vRmn v\kmrmxn vRlpfjlzs,nivSvSixMw\iffnhsRr'xRi0fhr'vSvSixnhjm|LiopiffjmqhvRvRmnhjvR'iOjryw8ix
fhUafhxkys\Pj'fhvRmixIfhysSixR} n vRlpfjls$vR'iMrmsSirmozj'iffssPfhUvR'iJff|Lfju"k'iffjm|LiMsR|LfhxRiffs8nhjmkvR'i6ff
VTz$

fi>

/

~$B

fi2 4D 5)
67698
B=D7F
Gy8 4E2 6

72

JNfi#

6
;h;e6

> 9?f7A@d.22

0zn\q

u0
u0

1~2'
'6
e6

0z

ez&
e#

iff|nhozolpvR)'|ynhj'qhiffI\r'vSfwMn vRl|I'iffn vRr'xRiffs
n yopiC~~xRiff|lzsRlpfjOnhjyk *

Rs nhozlziffjm|LiL|Lf }hixn qhiClzjdyxRiffkmlz|LvRlzjmqIdmxRfhyoziffwMn vRlz|kmlznhopfhqr'iffsff 'iffsSiiffn vRr'xiffsUsRiiffwvSf&dyxRf }ulzk'iPqhffuk
qhiffj'ixnhoDlzjmkylz|n vSfhxsKfhvR'i&sStesRvSiffwOsPsrm||LiffsRslzjOxRiff|LfhqjmlzvRlpfjnhjmkErmjmk'ixsSvRnhjmkmlzjmq' 'i8nh|LvPvRmn v
vR'iUwMnhlzjIfu|rmsfh'vR'iaxrmopiffslzsk'ivSiff|LvRlj'
q 4-< DMnhjmk,ixRxRfhxs-nhjmkIvRyn v-j'fj'iafh'vR'ity "iffynff}ulpfhxs
n xRi8rysSiffknhsKdmxiffkmlz|LvSfhxs*nhosSfMlzjmkml|n vSiffsKvRmn vylzjnhozoZozlpghiffozl'ffukZevR'
lzsKdixRfhxwMlj'q&nhsaiffoonhs
lpvI|nhjZ-qlp}hiffjvRmij'flzsStlzj'dyrmvvRyn v8lpvIlzsqhivSvRlj'qxRfw 4-< Dnhjmk)MjnhopvSixjmn vRlz}hiM}elzi lzs
vRmn vKv afrmvSvSixnhjm|Liffsn xiIj'fhvKiffj'fr'q0vSf\dmxf }ulzk'iw\iffnhjmlzjmqhrmo"kmlznhozfhqr'i$iffn vRr'xiffs*sRrm|nhsK|LfrmjvRs
nhjmkOdix|LiffjvRn qhiffs*fhxRidyxRfw\dmvRsm|Lfj'yxwMn vRlpfjysuivR| p
jmi$|nhjOsRiivRyn vKvR'i$vSfhdOvUf\xrmopiffs5rmsSi H u$D 'iyxsRvaxryopiynhslz|nhozoptsRvRn vSiffs
$
vRmn vOlpvR'ixilzs0j'f;xRiff|LfhqjmlzvRlpfjfhx0vR'isSiff|LfjmkiL'|mnhj'qhinhs0dyxRiffkmlz|LvSiffk?tvR'i H u-
S6vR'iffjvR'ikylznhopfhqr'ilzozonhlo/ 'isSiff|Lfjmkxrmopilzs0w\fhxRilzjvSixRiffsSvRlzj'qnhsOlpvOsSvRn vSiffslz
nwMlzsrmjmk'ixsRvRnhjmkmlzj'qEmnhs,iiffjdmxRiffkmlz|LvSiffkfhx\vR'isSiff|LfjykiLu|mnhj'qhinhjykvR'isStusSvSiffwozn iffo5lzs
b(cd)e @ f)ghinhjmkvR'iMrmvSvSixnhjm|Lilzs$opfjmqJvRmiffjvR'i\sStusSvSiffwlzoonhlzo/CjfhvR'ixUfhxkmsffvR'i
sStusSvSiffwxRiffer'iffjvRoptwlzsRlzjvSixRdmxRivRsZozfj'q*r'vSvSixnhjm|Liffs-nhs b(c8d(e @$f)ghixRiffsRrmopvRlj'qKlzj$vRnhsSg$nhlozr'xRih
lpqr'xi0ffOqlp}hiffsIn0sRr'ysRivfhUvR'i\xryopiffsSiv$fhxvRm
2fi4-< 5 iy8EB92iffn vRr'xRiMsSivfhx)'|mnhjmqhiffs
ff Ie*$j'iI|nhjEsSii8nslzwMlzozn xlpvtMiv aiiffjvRmlzsKxryopiffsSiv*nhjmkOvRmiIfj'iIqlp}hiffjElzjlpqr'xiMh ylzsKlzs
kmr'iCvSf6vRmi8nh|LvPvRmn v'iffjEnhozo-vR'iCnhr'vSfwn vRlz|8iffn vRr'xRiffs$n xRi,nff} nhlzozn opih9D BEBE8EDynhsPn6vSiffjmk'iffjm|Lt
vSfdlz|RgJfr'vvRmi8w\fhxRiIqhiffjmixnhovRnhsSglzjmkmid"iffjyk'iffjvKfjmiffsmlpvRvR'iIiL'|LidmvRlpfjfh5 L afj'i
|Lfw\dyn xiffs*vR'i&sSiff|LfjmkxrmopiIlzj0fhvROqr'xRiffs'fj'i&|nhjsSiiCvRmn C
v BEBE8EDrmsRiffs RR Z &~h $nhs
nMsRrmysSvRlpvRr'vSi$fhx*vR'iIvRnhsRgsSdiff|l"|$iffn vRr'xi\ p
2 kj

/

2ql~2'n~

E

}pZml|Y~2#9~ l'1q~2'#'#

}p#Z

s~w\iffjvRlpfj'iffk6n "f}hihenhj6nhopvSixjmn vRlz}hivSf8vSxnhlzjmlzj'q$vR'i~$fjMvR'inhrmvSfwMn vRlz|nhozopt\k'ixlp}hiffk H
u- aiffn vRr'xRi8lzs*vSfMvSxnhlzjlzvKfjOvR'i8mnhjykuozn iffozopiffku$D Umlzozi$sRvRlzozo"vSiffsSvRlzj'q6lpvKfj
vR'i\nhr'vSfwMn vRl|,iffn vRr'xih mlsPsSiff|Lfjykw\ivR'fuklsPxiixRxiffkEvSfnhsOSynhjmkuozn iffozopiffk'HvSxnhlzjmlzjmqMnhjmk
vR'i6xRiffsRrmopvRlj'qOiffn vRrmxRi6lzs [ u- ylzsIwMn tdmxRf }ulzk'i6nw\fhxRiJnh||r'xn vSiOw\fuk'iffo)yrmv8lpv
wMn tj'fhvP|n dyvRr'xRi&vR'iC|mn xnh|LvSixlzsRvRlz|sfh)vR'iCnhr'vSfwn vRlz|&iffn vRr'xiClzjvR'i8vSiffsSv$sSiv n yozi&qlp}hiffs
xRiffsRryopvRs~fhx5vR'i$vUfMw\ivR'fukms~$j'i$|nhjOsSii$xRfwvRmlzsUvRn yopivRmn v5vR'ixRilzsUn,sRozlpqveljmsRlpqjmlp|nhjv
lzjm|LxiffnhsSiKlzj,nh||r'xnh|Lt,fhR
x Gu|mnhj'qhiIKnhjmk,vRmiUP'fopiKkmlznhopfhqrmiUrmslzj'qvR'iKmnhjmkuozn iffozoziffkuHvSxnhlzjmlj'q
w\ivR'fukZ)Pf Ui}hixffuvR'ivSfhvRnhozoztMnhr'vSfwMn vSiffkJw\ivR'fukMtulpiffozkmsGn8ivSvSix~xRiffsRryopvPH e|Lfw\dyn xRiffkvSf
h M-fh
x )'|mnhj'qhiffsKff8ehmlz|Cnhs)w\iffjvRlpfj'iffk,n f }hihlzsvRmiUw\fsRvlw\dfhxRvRnhjv-xRiffsRrmozvfhxDvRmiffsSi
iLud"ixlzw\iffjvRs mlslzjm|LxiffnhsSiCsRmf snvSxiffjmkr'vlzsj'fhvPslpqjml|nhjv&km4 hhe8v , e
uh
'i8jmnhoxRf fhGvR'iCvRn yopi8qlz}hiffsPvRmi8xRiffsRryopvRsrmslzj'qMvR'i,mnhjmk'ozn iffozopiffk0iffn vRr'xRiM'$D *lzj
fhvROvR'ivSxnhljmlzj'q\nhjmkOvSiffsRvRlzj'qMnhjmkls5vRn ghiffjEnhsKvRmiIvSfhdyozlzjmixiffsRrmopv
VT

fi

2Zy222 S2-

~#



7M1m

CD#~2#2+3

5nhsSiffozlzj'i
4
2F
4
2FW [ u$D
4
2FW H u-
4
2FWu-

z
uz
u

e



72

CDff~2#2+3F6H

z
ez
h
e
he#



z


'


n yopi8e||r'xnh|Lt xiffsRrmopvRsOlzjy|ozrmkmlzjmq
[ u$D 0k'ixlp}hiffkrysRlzj'qvR'iynhjmkuozn iffozopiffk'
vSxnhljmlzj'q\w\ivR'fuk

~#

5nhsSiffozlzjmi
4-<

$lznhopfhqr'i
nhjmkuozn iffozopiffk
] H u$D
! u$D

C&Dff#~2,3

z
he#
#

e



CD#~2#2E3FIH

z
e
e
'
'#
u
'z

a#

z

e


hz
h

n yozi8e~P||rmxnh|LtxRiffsRrmozvRs5fhxsRr'ysSivRs5fhiffn vRr'xRiffs

Z
2 n#~
vlzslzjvSixRiffsSvRlj'qKvSf*iL'nhwMlzj'iGPmn vZvtediffsfhuiffn vRrmxRiffsn xRi~vR'i~w\fsSvDkmlzsR|LxlzwMlzjmn vSfhxtlzjk'ivSixwlzjmlzj'q
'ivRmixUnCkmlznhopfhqr'ilzsGdyxRfhyopiffwMn vRl|KfhxUjmfhvR
BEBE8=DOUnhsUvSxnhlj'iffksSidn xn vSiffoptMfjJsSivRsafhiffn vRrmxRiffs
ynhsSiffkfjvR'iOqhxRfr'dys,qlz}hiffjlzjlpqr'xRi5jmnhw\iffopt;P|LfrysSvRlzp| k 4-< D5)a$lznhozfhqr'inhjmknhjmku
ozn iffozopiffklzjy|ozrmkmlzjmqMu- miffsSixRiffsRrmozvRsKn xRiIqlp}hiffjlj n yozi8e
'fh
x Gu|mnhj'qhi *fjmoptvR'iiffn vRr'xRiffs*frmv6fhvR'inhr'vSfwMn vRl|iffn vRr'xRisSivRs*tulpiffozkysMnhj
lzw\dyxRf }hiffw\iffjv\f }hixvR'iOynhsSiffozlj'ihjvSixRiffsRvRlzj'qopthavSxnhlzjmlzjmqEvR'iOsStusSvSiffw fjvRmi 4- D?telziffozkms&vR'i
iffsSv,xRiffsRrmozv&frmv\fhPvR'inhrmvSfwMn vRlz|Oiffn vRr'xRisSivRsMfhN
x Gu|mnhj'qhiffInhjykvR'iO'foziJkmlnhopfhqr'ih
'iffsRisStusSvSiffwMsUfhx6iLunhwMdyopih5rmsSi Shh jermw8ix\fh$xRiff|LfhqjmlziffkUfhxkmsKnhjmkv tdifh
xRiff|LfhqjmlzvRlpfjOqhxnhwMwn xnhs*iffn vRr'xRiffsPlzj0vR'ifflpxKxryopiffsSiv
lzjmnhoopthai\qlp}hi\xiffsRrmopvRsfhxvR'iMsStusSvSiffwvSxnhlzj'iffkfjmoztfj H u$D $nhjmk [ u-
$j'i|nhjsSiiEvRyn v6vR'ixilzsJj'fhv0wCry|kmlixRiffjm|LilzjvR'ivUfsSivRs0fhIxRiffsrmopvRsmfhx
)'|ynhj'qhiffs5ffIe vR'iasStusSvSiffw vSxnhlzj'iffk8fj [! u$D mnhs-nhjCnh||rmxnh|LtIml|8lssRlpqjyl|nhjvRopt
mlpqmixvRmnhj?vR'iEsRtesSvSiffw vSxnhlj'iffkfj H u$D MetndnhlpxRiffkvHvSiffsRvk'7 hheP8v #;eu
$u ;$jiL'nhwMlzjmlj'qvR'iExrmopiffsSiv5fj'ijmkmsMvRmn v6vRmi [ '$D MrmsSiff
DBh4D2 4

E
4

2
6


'



R
x
*

R
v





u

$













x

r
p

ff

R



G
v
'
k

f
ff

~

'
j
h
f

v

'



p


f




U
x
h
n

|

|
'
r

x
h
n
L
|



w
n t,"i*kyr'iKvSf

H










vR'i,nh|LvvRyn vvRmi H u- *dmxRiffkml|LvSfhx$ynhsn0opfxRiff|nhozonhjmkEdmxRiff|lzsRlzfjfh#
x DBh4D2 4

E
4

2
6
h
n






ff


j

l
j

n


z

8

e





2 2po lmq%0z= 1

\

~



~22

sw\iffjvRlpfj'iffkljceiff|LvRlpfjevR'ixin xRiU
; v tdiffsOfh,dmxfhyopiffwMn vRlz|kmlnhopfhqr'iffs24 5e<4 D8

w gx 4Dynhjmk 4i
jBZ&jfhxkmixvSfk'ivSixwMlzjmi&P'ivR'ixIsSfw\i\fhUvR'iffsSi\v tdiffsfhUdmxRfhopiffwMn vRlz|
VT 4

fi>

r6rz~2g

24- 5)e6698
24 5e<4 D8
w gx 4Dy
4i
jB
fhvRnho

72

> 9?f7A@d.22

0zn\qpfiqsn

0z \q

e;
;u ;
he#h/u
' h
uH ;

'z /
heH
H ;fi ;
;he/h
e#h/hu

n=

n opiMu~n vSxl0fhDxiff|Lfhqjmlpiffk24-<5ff
676=8" Pnhjmk

r#Irz~2g#

2fi4-<5ff
67698
2fi4 <5
p4 D8

fhvRnho

0z fqp
Bs#

u z
u H ;


;he

e/h
heffh

n yopiMh Pn vSxlOfh)xRiff|Lfhqjylpiffk2fi4-<5ff
67698 nhjmk^2fi4D
vSiffsRvRlzj'q

q$~2

z /hh
ffe
ffe
e H r;
h / hh

24- 5e<4 D8

0|n\n9n

/h
/h ;
/h

\

~

f

~

2q~2



h / uff

5e<4 D8$rysRlzj'qMiffermnhovSxnhlzjmlzjmqnhjmk

kmlznhozfhqr'iffsMn xRiw\fhxikml{6|ryopv,vSfdmxRiffkml|Lv&vRynhjfhvR'ixsffaUi|Lfjmkmry|LvSiffkndfsSv'fu|nhjmnhoptusRlzs,fh
vR'i,dmxRfhdfhxRvRlzfjfh~dyxRiffkmlz|LvRlpfjnhlozr'xRiffsfhxiffnh|vted"i,fhGdyxRfhyopiffwMn vRl|8kmlznhozfhqr'ih8culzjm|Li&ai&UixRi
dmxlwMn xlzoptlzjvSixRiffsSvSiffklzjvRmi6dixRfhxwMnhjm|Li6fhKvR'i0~$rmsRlzjmqvR'irmozoanhr'vSfwMn vRlz|6iffn vRr'xRi0sSiv
n vSixCmn }elzjmqsSiiffW
j Gu|mnhj'qhiffsJffIe-Ui|Lfjmkyrm|LvSiffkfr'xCnhjmnhoptusRlzs$fjvRylzs$}hixsRlpfjfhUvRmi~$D
n yopi06sRmf svR'i,kmlzsSvSxlpyr'vRlpfj0fh~vR'i,6v tdiffsfh~kmlznhozfhqr'iClzjEvR'i&vSiffsSvsSiv$nhjmkP'ivR'ix$vR'i
)'|ynhj'qhiffsffII~$5nhs~n yoziUvSf8dmxRiffkmlz|LvG|LfhxxRiff|LvRopt&vRmn vGvR'i*kmlnhopfhqr'iKUfrmozk,
2fi4-< 5ff

67698
fhR
x BEDFGfi 8 4E2 6)$j'i5|nhjMsSii5vRmn v)vR'i5UfhxsSv)dixRfhxwMlzj'q|n vSiqhfhxRt\lz
2fi4D 5e<4
D8&nhjmk&vRmn v
vR'i8a$dmxRiffkylz|LvRsKlzjm|LfhxRxiff|LvRopt6vRyn vPhefhvR'
24-< 5
<4
D86kmlnhopfhqr'iffs*n xRb
2fi4-< 5ff

67698 h
$jmiIxRiffnhsSfjEvRmn vvRylzs*wMlpqv*fu||r'x$lzsKvRmn vPvRmlzssr''|n vSiqhfhxRtOfhGkmlnhopfhqr'iffs*n xRi&wCry|w\fhxRi
kml{J|rmopv,vSfdmxiffkmlz|LvMsRlzjy|LilzjvRmlsM|nhsSivR'i sStusSvSiffw mnhs6j'fljmkmlz|n vRlpfjvRyn v6lpvMlzs6j'fhv
sRrm||Liiffkylzj'qlzj?vR'ivRnhsSg"Pf Ui}hixffInhj'fhvR'ixOdfsRsRlzylzozlpv tlzsJvRmn vOvR'i~$ dixRfhxwMsJdffhxopt
fj?vRmlzsM|n vSiqhfhxRt?iff|nhrmsSivRmixRiEn xRiEiUixJiLunhw\dopiffslzjvR'ivSxnhljmlzj'qsSivPnhopvR'fr'q?lpvk'feiffs
ivSvSixfjvR'i 4i
j
BsRr'ysRiv"Pmlz|lzs$n fr'vvR'i\sRnhw\i,dmxRfhdfhxRvRlpfjiM|nhjiffolzwMlzjmn vSiCvR'i
yxsRvUdfsRslpylzozlzvt,t6iLunhwlzjmlzj'q,'fn\opiffn xj'ixKdixRfhxwMsUmiffj6vSxnhlzj'iffk0fj0iffermnhodmxRfhdfhxRvRlpfjysafh
24- 5)

67698nhjy
k 2fi4D 5e<4
D80kmlznhopfhqrmiffs)iC|Lfjmkyrm|LvSiffknhjOiLudixlzw\iffjv*rmsRlzjmq,nsRr'ysSivKfh
24- 5)

67698&kmlznhopfhqr'iffs8lzjvR'iJsRnhw\idyxRfhdfhxRvRlpfjnh+
2fi4-< 5
p4
D8fhxCvR'iMvSxnhljmlzj'qnhjmkvR'i
vSiffsSvsSivPnhjmkvSxnhlzj'iffknsSiff|Lfjmk~$?rysRlzj'q,vR'iIrmozozt6nhr'vSfwMn vRl3
| )'|ynhj'qhi6ffIMiffn vRrmxRiffs mlzs
xRiffsRryopvSiffklzjJnCvSxnhlzjmlzjmqIsSivafhDh &kylznhopfhqr'iffsUnhjmkJn8vSiffsSv5sSivUfh-uffe miylzjyn xRt\|oznhsRsRlixamnhsUnhj
nh||r'xnh|Ltfh-JvRmiP|LfhxRxRiffsRd"fjykmlzj'qxRiff|LfhqjylpvRlpfjwn vSxllzs)dyxRiffsSiffjvSiffk6lzjvRn yopi8h 'i*xRiffsrmopvRs
sR'f vRyn vPiai#
x 24- 5e<4
D8In xRi8dyxRiffkmlz|LvSiffknhsPsrm||LiffsRsSrmo/"sRr'qhqhiffsSvRlzjmqvRmn #
v 2fi4D 5e<4
D8
n xRij'fhv6ljm'ixRiffjvRoptw\fhxRikml{6|ryopv\vSfdyxRiffkmlz|LvMvRynhj?fhvRmixJ|oznhssSiffsJfhdmxRfhyopiffwn vRlz|kmlnhopfhqr'iffs
Uiffopf UikylzsR|rmsRsMvR'iEdfhvSiffjvRlznhoPfh8rmsRlzjmW
q BEB=8EDsopfssxn vRlzfvSf;aifflpqv0kmlixRiffjv6vtediffs6fh
|oznhsRsl|n vRlpfj0ixRxfhxs*lzj0r'vRr'xRiUfhxRg"
VTV

fi

)
uQ

2Zy222 S2-



7M1m



72

Q _wv ^ffx

'i&xRiffsSiffn x|xRidfhxRvSiffk'ixRi,lzsPvR'i&yxsSvvRmn v$Ui&guj'f fhavSfOnhr'vSfwMn vRlz|nhozoztnhjmnhoptei\nJ|Lfhxdyrms
fhopfhqsCxRfwnEsSdfhghiffjkylznhopfhqr'iJsStesRvSiffw fhx,vR'idyrmxRdfsSiMfh*opiffn xjylzj'qvSf RR /ff dmxRfhopiffwMn vRlz|
sRlpvRryn vRlpfjms mlzs$UfhxRgyrylzozkmsfjv afsSvSxnhjykmsIfh5iffn xozlzixIxRiffsSiffn x|ZlpxsSvDvRmlzs8n dmdmxRfnh|5nhs
lzjmsRdylpxRiffk,et,afhxRg\fj6vR''
Bh4D4y 8\i} nhozrmn vRlzfjMxnhw\iUfhxRg\fhxUsSdfhghiffj6kylznhopfhqr'i*sStusSvSiffwMsaml|
r'vRlzolpiffs0fhvRwCrmozvRlp}n xlzn vSiozlzj'iffn xOxRiqhxiffsRsRlpfjnhjyk 5'
vSfdmxRiffkml|Lv0rysSixsRn vRlzsRnh|LvRlpfjnhsn
rmjm|LvRlpfjfhnjrmwCix8fh*fhvRmix,wMivSxlz|s0nhopghixff~-lpvRwMnhjZ
InhwMw~PiffozoznuKffhanhozghix
ivPnhoHp hhhn
*iffsSiffn x|rysRlzj'N
q Bh4D4y < 8Omnhs*frmjmk0vRyn vvRnhsSgO|Lfw\dyozivRlpfjlzs*nhozUn tesnMwMn Sfhx
dmxRiffkylz|LvSfhxfhKrmsSixIsRn vRlzsRnh|LvRlpfj-nhjmkmnhsIiLunhwMlj'iffkdmxRiffkylz|LvSfhxs$fhUvRnhsSg|Lfw\dyopivRlpfj,PixRih-fr'x
qhfnhozsan xRiPsRlwMlzozn x)lzj,vRmn vaaiPn vSvSiffw\dmv~vSf&rmjmk'ixsSvRnhjmk,vR'i*nh|LvSfhxsavRmn vGdmxiffkmlz|Lv)vRnhsSg\|Lfw\dyozivRlpfjZ
ceiff|LfjmkyopthuvRmlzsaafhxRg6yrmlzokms)fj0iffn xozlzixaxRiffsRiffn x|Ofj0opiffn xjmlzj'qCvSf /y kmlznhopfhqr'iffs5lzj6ml|JvR'i
rmsSixiLudixlpiffjm|Liffkdfefhx$sRd"iiff|xRiff|Lfhqjmlpix8d"ixfhxwnhjm|Li0-lpvRwMnhjivInho/pGffhhMUiff|nhrmsRi\vRmn v
UfhxRgOUnhsynhsSiffkEfjiffn vRr'xRiffssStujvR'iffslpiffkf}hixPvR'iCiffjvRlzxRi8kmlznhozfhqr'ihyvR'iCtedfhvR'iffsSiffs*vRyn vUixRi
opiffn xjmiffk|Lfrmokj'fhv&i6rmsRiffkfhxCdyxRiffkmlz|LvRlpfjkmr'xlj'qxrmjvRlzw\ihjnhkmkmlzvRlpfjZlzj|LfjvSxnhsSv,vSfvR'i
|r'xRxiffjvsSvRrmk'th"vR'i8dyxRi}elzfrms*UfhxRgnhr'vSfwn vRlz|nhozoptn dmdyxRfff'lzwMn vSiffkEvR'i,j'fhvRlpfjEfhGnqhfefekEfhx$ynhk
kmlznhozfhqr'irmsRlj'qInvR'xRiffs'fozk\fj6vR'i*dix|LiffjvRn qhiPfhZxiff|LfhqjmlpvRlpfjixRxRfhxs mixRilzsGnCkmnhj'qhixafhZvRmlzs
n dmdmxfnh|ifflzj'q0|lpx|rmon xPmiffjxRiff|LfhqjmlpvRlpfjdixRfhxwMnhjm|Li\n v$vR'iMr'vSvSixnhjm|LiMopi}hiffolzs$n0dmxlzwn xRt
dmxRiffkylz|LvSfhxafhnCqhfefekJfhx5ynhkJkmlznhopfhqr'ihjMvRmls~UfhxRg"evRmiPj'fhvRlpfj6fhnCqhffuk 2fi4D 5)

67698 Dnhjmk
ynhkg B=D7F
Gy
8 4E2 6~kylznhopfhqr'i$5nhsozn iffozopiffkJetJermwMnhjys
jdmxi}elpfrysafhxgKnhopghix,ivCnho/p) hh "IxRidfhxRvSiffkxiffsRrmopvRsxRfw vSxnhlzjmlzjmqndmxRfhopiffwMn vRlz|
kmlznhozfhqr'iIdmxRiffkylz|LvSfhxPlzjmlz|OvR'itjmfhvSiffkvR'iCiLevSiffjvvSf6ylz|OvRmi&mnhjykuozn iffozopiffkEu-
iffn vRr'xRi\lzw\dmxf }hiffs$|oznhsRsRlpyixPdixRfhxwMnhjy|LihsnxiffsRrmopvfhGvRmlzsPdmxlzfhxnhjmnhoztesRls"ljvRmlsUfhxRgUi
xRidfhxRvPxRiffsRrmopvRsxfw vSxnhlzjylzj'qnhj H u$D *|oznhsRslyixfhxiffnh|iLu|mnhj'qhi\nhjmkrmsRlzjmqlpvRs
dmxRiffkylz|LvRlpfjmsanhsanhj0lzj'dr'v)iffn vRrmxRiPvSf&vR'i$GxRfhopiffwMn vRlz|Plnhopfhqr'iP~xRiffkmlz|LvSfhx 'ixRin xin&jryw8ix
fhydyxRi}elzfrmssSvRrmkmlpiffsDfj,dmxiffkmlz|LvRlzj'qPxRiff|LfhqjmlpvRlzfj,ixRxRfhxs)nhjmk\rysSix)|LfhxRxRiff|LvRlpfjmsGmlz|Cn xRiKxiffozn vSiffk,vSf
vR'i H u$D )dmxRiffkylz|LvSfhxavRyn vaUiPxid"fhxv~fj0'ixRi,PlpxsR|ixRq8ivKnho/pffhhemPlzxsR|ixRq'
lzvRwMnhjZy ceUixRvRs" hhu" h'L"i}hf 8-ffhhelpvRwMnhjmPlpxsR|ixRq'y ceUixRvRsy hhuceUixRvRs
lzvRwMnhjZylpxsR|ixRq'y hh
lpxsR|ixRq,ivnhoHpffhh*n dmdyop
BEBE8=DEvSf\dyxRiffkmlz|Lv5xRiff|LfhqjmlzvRlpfj0ixRxRfhxsKljOn\|LfhxRdyrmsUfh)
r'vSvSixnhjy|Liffsj|LfjvSxnhsRv6vSffr'xUfhxRg"KvR'itr'vRlzozlpiOdyxRfsSfukmlz|0iffn vRr'xRiffslj|LfwCylzjmn vRlpfjlpvR
nh|LfrmsSvRl|M|Lfjuk'iffjy|LiMsR|LfhxRiffs mitxRidfhxRvnOiffsSv|oznhssRlyixnh||r'xnh|LtfhKhJPmlz|lsnL
lzw\dyxRf }hiffw\iffjvGf}hixavR'ifflzxGynhsSiffozlj'i5fh0 mlzsxRiffsrmopv)|nhjMi*|Lfw\dyn xiffkMlpvR,frmx~ylzjyn xRt H
u- 5dyxRiffkmlz|LvSfhx,g D767FDD8962}es

67FDD8962KkmlzsR|rmssSiffklzjceiff|LvRlpfje
GunhwMljmn vRlpfj
fhmvR'i5xryopiffsopiffn xj'iffk8etIvR'ifflzxD|oznhssRlyixsRr'qhqhiffsSvRsvRyn vDkmrmxn vRlpfjmnhoiffn vRrmxRiffsn xRiKlzw\dfhxRvRnhjvmlzopi
Ui~k'f*j'fhvrmsRiGnhw\dyozlpvRryk'ifhxKiffn vRrmxRiffsai~kmf*mnff}hi~nhj Sh/ iffn vRrmxRiGml|lzsZopfhqhqhiffk8t
vR'ixRiff|Lfhqjmlpix GlzvR'fr'v5nhjt6fhvR'i$fhvR'ix5dmxRfsSfukmlz|iffn vRrmxRiffsuvR'i H u$D Gdmxiffkmlz|LvSfhx
mnhs&nhjnh||rmxnh|LtfhPhe JGnhe lzw\dmxf }hiffw\iffjv8f}hix\vR'iynhsRiffozlzj'iMfhh ;0vClzs8d"fssRlpyopi
vRmn v~ljm|ozrmkmlj'qdmxfsSfekylz|Uiffn vRr'xiffs~lzj,vR'i H u- DdyxRiffkmlz|LvSfhxG|Lfrmok\lzw\dmxRf}hi*vRmlzsxiffsRrmopv
i}hiffjr'xRvRmixff
~xRi}elzfrmssSvRrykmlpiffs*fjixRxfhx|LfhxRxiff|LvRlpfjxiff|LfhqjmlpvRlpfjn xRi,nhozsSfxiffozn vSiffkvSf6fr'x$w\ivR'fukfhawMlzs
rmjmkmixsSvRnhjmkmlj'q$xRiff|LfhqjmlpvRlpfjKi}hf IZffhhan dmdyolpiffkMsRlzwlzozn x)vSiff|yjmlzer'iffsGvSf&opiffn xjMvSf,kmlzsRvRlzj'qrmlzs
ivUiiffjr'vSvSixnhjm|Liffsljml|EvR'i\rmsSixfhxlpqlzjmnhoopt0dmxRf }ulzk'iffkEsSfw\i\lzj'fhxwMn vRlzfjvSf0vR'i\sStusSvSiffwO
nhjmk RRff/ yylz|dmxRf }ulzk'iffk6vR'i$sRnhw\ilzj'fhxwMn vRlzfjn,sSiff|LfjmkJvRlzw\ihefozopf Plzj'qCn&wMlzsrmjmk'ixS
sSvRnhjmkylzj'q' mlswMn ti8w\fhxRi&xRiffozn vSiffkEvSf6frmxPxRiffsRiffn x|vRmnhjlpv*xsSvn dmdiffn xssRlzjy|LiC|LfhxRxRiff|LvRlzfjms
VT 1

fi>

72

> 9?f7A@d.22

n xRi6fhvSiffjwMlzsrmjmk'ixsRvSffukkmr'iMvSftdixSn xRvRl|rmozn vRlpfjZOi}hf Is&iLud"ixlzw\iffjvRsIvSxnhlzjnk'iff|lsRlpfj
vSxRii$rmsRlzjmqiffn vRr'xiffsasRry|Jnhsakmrmxn vRlpfjZvSiffwMd"f'edylpvR|ZnhwMdyozlpvRrmkmihnhjmk6lpvRmlzj'r'vSvSixnhjm|LiKdynhrysSiffs
)'nhwMlzjyn vRlpfjfh~vR'iCvSxnhlzj'iffkvSxii,ljvRmlzssSvRrmkmtnhozsSfJxRi}hiffnhozsvRmn vvR'i&kyr'xn vRlpfjmnhoiffn vRrmxRiffsn xRi
vR'i$w\fsSvUkmlzsR|LxlwMlzjmn vSfhxRthGculzwlzozn xoptClzjMfr'xUiLud"ixlzw\iffjvRs1
BEBE8EDrmsSiffs Sh/ xRiffer'iffjvRozt
lzj0vR'iCk'i}hiffopfhdiffk0xrmopisSivUi}hffhmvRnhlzjys*nhjnh||r'xnh|LtOxn vSiIfh) lpvROn\nhsSiffozlzj'i$fhG J
HceUixRvRsaivUnhoHpe hhanhjmkElpxs|"ixqIivUnhoHpu h'L"GdixRfhxw sRlzwlzozn xGsSvRrmkylpiffsGfhxanhr'vSfwn v
lz|nhozozt,lk'iffjvRlztulzj'qI|LfhxRxiff|LvRlpfjms5rmsRlzjmqIdmxRfsSfuk'th
4- Diffn vRr'xiffsUnhjyk6kmlnhopfhqr'iP|LfjvSiLuvUUfhxRxRiff|LvRlzfjms
n xRiolpghiffopt&vSfIiwMlzsSxiff|LfhqjmlpiffkZkyr'iKvSfCtdixn xvRlz|rmozn vRlpfj 'it&fhysSix}hivRmn v~|LfhxxRiff|LvRlpfjms~vRmn v
n xRiw\fhxRiPkylzsSvRnhjv~xRfw vR'iPixRxRfhxUvR'itM|LfhxRxRiff|Lv'n xRiw\fhxRiozlpghiffopt&vSfCiL'mlplpv)dmxRfsSfukmlz|Kkyl"ixiffjm|Liffs
'ifflzxsStusSvSiffw nhr'vSfwn vRlz|nhozoptkmlixRiffjvRlzn vSiffs|LfhxxRiff|LvRlpfjmsxRfw j'fju|LfhxRxiff|LvRlpfjmsMPlpvRnhjixxRfhx
xn vSiMfh$ffe# JM$lznhopfhqrmiM|LfjvSiLuvClzsrysSiffklzjvR'iMsSvRryk'tet;lpxsR|ixRq'-lpvRwMnhjceUixRvRs
h'n5'ixRietvR'it;lzjm|LfhxRdfhxn vSi0mivR'ix,vR'iOrmsSix\lzs,nff5n xRifhnwlzsSvRn ghiOn v\vR'iO|r'xRxRiffjv
r'vSvSixnhjy|LiPvSf,'iffopd6dmxRiffkmlz|LvawMlzsRrmjyk'ixsSvRnhjmkylzj'qs)nhjmk6wMlzsSxRiff|LfhqjylpvRlpfjMfhvR'idmxi}elpfrys~r'vSvSixnhjy|Liffs
mlssSvRrmkmtlzssRlzwMlzon xvSfJfr'xs$lzjEvRmn v$vR'itrmsSi,nJdmxRiffkml|LvSiffkiffn vRr'xRi,n fr'vnhjr'vSvSixnhjm|LivR'i
nff5n xRihDiffn vRrmxRi$vSfdyxRiffkmlz|LvI|Lfjm|Lidyv8fhx8Ufhxknh||r'xnh|Lth)nhsIUirmsSi6nOdmxRiffkml|LvSiffkiffn vRrmxRi H
u- alzj0vR'iIaDmPf Ui}hixff"fr'x H u$D Uiffn vRr'xiIlzsKnhr'vSfwMn vRlz|nhooptJn }nhlozn yopi
n vKvR'i$vRlzw\i$vR'i$dmxiffkmlz|LvRlpfj0lzsUifflzj'q,wMnhk'ihu'ixiffnhs5vR'itJn xi$wn gelzjmq&vRmidyxRiffkmlz|LvRlpfjys~xRivSxfnh|
vRlp}hiffopthj;nhkykmlpvRlpfjZDvR'itvSxnhlzjvRmifflpx&sRtesSvSiffwfj;vR'i0mnhjmkuon "iffoopiffkiffn vRr'xRiJxn vR'ix,vRmnhj;vR'i
dmxRiffkylz|LvSiffk0fj'iIylz|0vR'itOopiffn }hi8nhs*r'xRvRmixKafhxRg"
g Ilzx|mmf D' h' adixRfhxwMs~ixxRfhxK|LfhxRxRiff|LvRlpfj0lzk'iffjvRl|n vRlzfj6rmslzj'qIvRnhsSgJlzjmk'idiffjmkmiffjv~nh|Lfrys
vRlz|JnhjmkkmlzsR|LfrmxsSiM} n xlzn yoziffs mlzs8ls&nEvUf5nfftkylzsSvRlzjm|LvRlzfjivUiiffjd"fslpvRlp}hi6nhjyk;jmiqn vRlp}hi
ixRxRfhx|LfhxRxRiff|LvRlpfj*c''i8rmsSiffsvUfJ|nhs|nhk'iffkE|onhsRsRlyixs'vR'iIxsSvlzsnk'iff|lzslpfjOvSxRiiCvSxnhlzj'iffkrysRlzj'q
fhvR'ikmn vRnnhjyk}nholzkmn vRlzj'qfjJ
GunhwMdyopiffs&vRmn vmn }hi|Lfjukmiffjm|LisR|LfhxRiffs\iffopfn
vR'xRiffs'fozkqhf0lzjvSfJnhjiL'|LidmvRlpfjEvSxnhlzjmlj'qsSivfhxnJsSiff|Lfjmk|oznhsRsRlpyixff$r'xlj'qvSiffsSvRlj'q'"lzG|Lfjum
k'iffjm|Li,sR|LfhxRiffsn xiCiffopf n6vR'xRiffsRmfozkvRmiffjvR'i\r'vSvSixnhjm|Li&lsdynhssSiffkfjvSf0vR'i,sSiff|Lfjmk|oznhsRslyixff
cu'iIjmkmsvRmn vPvRmiCw\fsSvkmlzsR|LxlzwMlzjmn vSfhxt6iffn vRr'xRiffsn xRi,kmlznhopfhqr'iC|LfjvSiLuvMvR'iCv tdiCfh)dmxRi}ulpfrms
sStusSvSiffw rmvSvSixnhjm|Li$fozopfaiffktopiL'lz|nhoiffn vRr'xRiffsDlpvRdmxRfsSfukmlz|&iffn vRr'xRiffsIifflzjmqOvR'iopiffnhsSvCkmlzs
|Lxlzwlzjmn vSfhxRth 'i6sStesRvSiffw xRiff|Lfhqjylpiffs8ixxRfhx&|LfhxxRiff|LvRlpfjmsClpvRnhj;nh||r'xnh|Ltfh |Lfw\dn xRiffk
vSf6nynhsSiffozlj'i$fh~u J5jvRmlzs*sSvRryk'tg 8lpx|y'f D" h' kmiffozlpixn vSiffopt6iffsR|'iPs*vR'i&rmsSiIfh)sStus
vSiffwsSdiff|l|iffn vRr'xRiffsffylzopi8lzjfr'xUfhxRg"aiCiLunhwMlj'i8vR'i,sSidyn xn vSi&|LfjvSxlpyrmvRlpfjfhGkmlp"ixRiffjv
iffn vRr'xRiCsRivRsP$r'xPxRiffsRrmopvRssRr'qhqhiffsSvPvRmn vPvRmiCrmsSi8fhGw\fhxRi8qhiffjmixnhoiffn vRr'xRiffsk'feiffsPj'fhvPj'iqn vRlp}hiffopt
lzw\dnh|LvKd"ixfhxwnhjm|Lih
g xnhmw\ixffMceUixRvRs 'iffrmjmih, iiqhiffozsEffhh nnhjmk g xnhyw\ixffMceaixvRs 'iffrmj'ih,
iiqhiffozsffhhGopfefhgMn v5kmlixRiffjv~iffn vRr'xiffs~xRiffon vSiffkvSf&xRiffsSdfjmsSiffs~vSfCdyxRfhyopiffwMn vRl|*sStusSvSiffw vRrmxjms
'i h < ~z Mh/ IvR'itkmlzs|rmsRs$n xRiMxRiffsRd"fjysSiffs$vSfiLudyozlz|lpvfhx8lzwMdyozlz|lpvsStusSvSiffw}hixl"|n vRlpfj
er'iffsSvRlpfjmsff mitfhysSixR}hi&vRmn vkmlzs|LfjuyxwMn vRlzfjmsn xRi,opfj'qhix ynff}hi\n0wMn xRghiffkafhxkEfhxk'ixffZnhjmk
|LfjvRnhlzjsSdiff|l|8oziLulz|LfjsRry|nhsOSj'fujnhkmkmlzvRlpfjZyvRmixRi\n xRi&sRd"iff|lp|IdmxfsSfekylz|8|r'iffs$sRry|nhs
frmjmkmn xtvSfj'iffsMnhjmkdynhrmsSiffscefwMiOfhPvR'iffsRiOiffn vRr'xiffsMsRrm|nhsMopiffj'qhvRa|mflz|LifhafhxkysMn xRi
|n dmvRr'xiffklzj0fr'C
x BEBE8EDxryopiffsSiv*nhskylzsR|rmsRsRiffkJn f}hih
sk'iffs|Lxlpiffk0lzjEceiff|LvRlpfjEeyvUf6wMivR'fekmfopfhqlpiffsKUixRi8|Lfw\dn xRiffkfhxlzjm|Lfhxd"fhxn vRlzj'q,vR'iIiffn
vRr'xRiPu- lzjvSfvR'i5~$D 'iayxsSv-UnhsvSfrmsSiUvR'i5mnhjmkuon "iffoopiffkiffn vRr'xRi5lzj8vRmiUvSxnhlzjmlzj'q
sSivmvRmi8sSiff|LfjmkvSfMdixRfhxw sSidyn xn vSiIiLudixlzw\iffjvRsKvSfMdyxRiffkmlz|LvKvR'iiffn vRr'xRi8fhxPvR'ivSxnhlzjmlj'q\sSiv
sKvR'iiffn vRr'xRiffs*lzj0vRmivSxnhlzjmlzjmq&sSiv*n xiInhr'vSfwMn vRlz|nhozozt6dmxiffkmlz|LvSiffkZ'lpv*lzsKmfhd"iffkJvRmn vvR'iIsStusSvSiffw
Ufrmozk,dylz|gCr'd\vRmilzkmlpfsRtejm|LxnhsRlpiffs-fh"vRmi*j'flzsSt&kmn vRnu mlzsDvSxnhljmlzj'q$w\ivR'fukMmnhs)iiffj\rmsRiffk,dmxRiL
}ulpfrmsRopt6lzjxlpqv hh5mixRinhr'vSfwMn vRlz|nhozozt6lzk'iffjvRlyiffk0lzjvSfjmn vRlpfj0i}hiffjv*iffn vRr'xRiffs*n xiIrmsSiffk
VT

"

fi

2Zy222 S2-



7M1m



72

vSfvSxnhlzjnhjnhr'vSfwMn vRlz|CsSdiiff|unh|Lv$k'ivSiff|LvSfhxff 'iffsSi&nhr'vSfwMn vRlz|nhozozt0kmixlp}hiffkiffn vRr'xRiffsPdmxRf}elk'i8n
ivSvSix5vSxnhlzjmlzjmq8w\fuk'iffomvRmnhjJvR'i$mnhjmk'ozn iffozopiffk,fj'iffs mlzs~ls~vSxrmiPnhozsSf,lzjvR'i|rmxRxRiffjvUsSvRryk'tMnhs
kmlzs|rmsRsSiffk0lzjEcuiff|LvRlpfjez
yYNX

b?ff'`fff bH^ ]


] _

`l`l Q
v

^zx

mlsPdn d"ix$xRidfhxRvRs$xiffsRrmopvRs$fjnhr'vSfwn vRlz|nhozoptvSxnhlzjmlj'qJnO~xRfhyopiffwn vRlz|C$lznhozfhqr'iMGxRiffkylz|LvSfhxvSf
dmxRiffkylz|LvPdyxRfhyopiffwMn vRl|CermwMnhju|LfwMdyr'vSixkmlznhopfhqrmiffsrysRlzj'q0n0|LfhxRdyrms$fhahh0kmlznhopfhqr'iffs|Lfoopiff|LvSiffk
lpvRCvR'i h L sSdfhghiffj,kmlznhopfhqr'i5sStusSvSiffwO 'i5GxRfhopiffwMn vRlz|U$lznhopfhqr'iKGxiffkmlz|LvSfhx
|nhji6lzww\iffkmlzn vSiffoptn dydyozlpiffkvSfvRmi0sRtesSvSiffws&kmiff|lzsRlpfjfhmivR'ixCvSfvSxnhjysSix,vR'iO|nhozoavSfn
ermwMnhj|rmsSvSfw\ixO|n xRin qhiffjvPfhx0iErmsRiffk?nhs0n;|r'iEvSfvR'isStusSvSiffwOs0$lznhopfhqr'inhjmn qhixOvSf
w\fukmlptlpvRsM"iffynff}ulpfhxMvSfxRidnhlpx\vR'idmxfhyopiffwMs\lzkmiffjvRliffkZ 'ixRiffsrmopvRs\sR'f vRmn v \EfsSv
iffn vRr'xRi,sSivRs$sRlpqjml"|nhjvRoptOlzwMdmxRf }hi&f }hixvRmiCynhsRiffozlzj'ih)/
:PsRlj'q6nhr'vSfwn vRlz|8iffn vRr'xRiffsxRfw vR'i
'foziJkmlnhopfhqr'ihGUi|nhjlzk'iffjvRlptdmxRfhyopiffwn vRlz|6kylznhopfhqr'iffsM ivSvSix\vRmnhjvR'iOnhsSiffozlzj'ih$ ;
{ rysSvvR'iCyxsRv*iLu|mnhj'qhi,dmxRf}elk'iffsPslpqjml|nhjvRopt0ivSvSixPdyxRiffkmlz|LvRlpfj; ;M*vRmnhjvR'i8ynhsRiffozlzj'ih)
'isSiff|Lfjmk6iLu|mnhj'qhi$dmxRf}elk'iffs~nhjJnhkmkmlpvRlzfjmnhomsRlpqjyl|nhjvP< ;M~lzw\dmxRf}hiffw\iffjvZ/~|oznhsRslyix
ynhsSiffkJfj0vRnhsSglzjmkmid"iffjyk'iffjv5nhr'vSfwMn vRl|Piffn vRr'xRiffs5dixRfhxwMs5sRolpqvRozt\"ivSvSixKvRynhj6fjmivSxnhlzj'iffkJfj
vR'iIrmozonhr'vSfwn vRlz|iffn vRr'xRi8sSiv
miGlzw\dmxRf}hiffkIn ylzolpvtPvSf*dmxRiffkml|Lv"dyxRfhyopiffwMn vRl|)kmlznhopfhqr'iffsZlslzw\dfhxRvRnhjvfhxZiffozkmlzj'q5vR'i
Cf
sStusSvSiffw lpvR'frmvPvR'i,j'iiffkfhxvRmi&f}hixsRlpqvfh~n0ermwMnhj|rmsSvSfw\ixI|n xRi\n qhiffjv 'iffsRiCxRiffsrmopvRs
n xRi,dmxRfwlzsRlzj'qJnhjmkai,iLudiff|LvvSfOi,n yopi,vSfOlzw\dmxf }hi\r'dfjvRmiffwOZdfsRsRlpyoztJetEljm|LfhxRdfhxn vRlzj'q
dmxRfsRfek't6lzjvSf\vRmiiffn vRr'xi$sRivCPlzxsR|ixRq,iv*nho/pffhhUfhx*iLudynhjmkylzj'q&fj0vR'iCEiffn vRr'xRiIsRivRs
j&nhkykmlpvRlpfjZ vR'i5xRiffsrmopvRssRr'qhqhiffsSv)vRyn vDvRmi*|r'xRxRiffjvDa$lzsozlpghiffopt8vSfqhiffj'ixnhozlpi5vSf$fhvR'ix~kylznhopfhqr'i
sStusSvSiffwMs
j\r'vRr'xRi5UfhxRgaidyoznhj,vSf8lzjvSiqhxn vSi*vRmiopiffn xj'iffk,xrmopiffsSivRsGljvSfvR'i
Cf kmlznhozfhqr'i*sStusSvSiffw
nhjmkEi}nhormn vSiCvRmi&lzwMdynh|LvvRmn v$vRmls*Ufrmozkmnff}hi&fjvR'i&sRtesSvSiffwsPf}hixnhozoDdixRfhxwMnhjm|Lih 'ixRi
n xRi,sSi}hixnho-Un tesUi&wlpqvPiCn yopiCvSfJs'f vRmlzs
iffw\iffwC"ixvRmn vfj'i,rmsSi8fh)vRmiC~$lzsvSf
lzw\dyxRf }hi5vR'iKsStusSvSiffwOsGkmiff|lzsRlpfjCfhP'ivR'ix)nhjmk,'iffj&vSfvSxnhjysSix)n|nhozoevSfvR'iKermwMnhj\|rmsRvSfw\ix
|n xRi\n qhiffjv 'i8fhvRmix$rysSiCUfrmozki&nhs$lzj'dr'v*vSfJvR'i&$lznhozfhqr'i&nhjmn qhixffskylznhopfhqr'i,sSvSxn vSiqht
sSiffopiff|LvRlzfjw\iff|mnhjmlzsRw~iffw\fjmsSvSxn vRlj'qMvR'i8rmvRlzozlpv tfhvR'i8a?fhxPkmlznhozfhqr'iIsSvSxn vSiqhtsSiffoziff|LvRlpfj
xRiffermlpxRiffsDiLudixlzw\iffjvRsvRmn v)vSiffsSvGfrmvGsSi}hixnhomkmlp"ixRiffjvUn tesGvRmn v)vRmlzs)lzjmfhxwn vRlpfj,|Lfrmozk,iKrmsSiffk
et&vRmiP$lznhopfhqr'iEnhjyn qhixff~iffw\fjmsSvSxn vRlzj'q8vRmir'vRlzozlzvt&fhZvR'iPa;fjvR'iPk'iff|lsRlpfj\vSfCvSxnhjmsSix
nJ|nhozoj'iff|LiffsRsRn xloptOlzj}hfoz}hiffsPiL'nhwMlzjylzj'qvRmiCvSxnhkmif snhw\fjmqJkmlp"ixRiffjvgelzjykms*fhGixxRfhxs mlzsPlzs
iff|nhrmsSiPi}hixRt6|nhozoyvRmn vUvR'i
Cf sRtesSvSiffw|nhjJmnhjmkmozisRrm||LiffsRsRrmoopt\sRnff}hiffsKnC|Lfw\dynhjtMvR'i$|LfsSv
fhGrmsRlj'qMnMermwMnhj|rysSvSfw\ix$|n xRiCn qhiffjvPvSfJmnhjmkmozivR'iC|nhozoH ermsyai&|nhjEnhsRsRfe|lzn vSi&vRmlzs|LfsSv
lpvRvR'i6k'iff|lsRlpfjvRmn v wMn ghiffs&vSfvSxnhjmsRixCvR'i6|nhozoHJ'iffj Cf vSxnhjmsSixsIvR'iJ|nhozo
rmjmjmiff|LiffsRsRn xlzozthUi$|nhooyvRmlsa|LfsSv5vR'i p ~ Hh \ UjvR'ifhvR'ix*mnhjykZi}hixRtJ|nhozovRmn v
n vSvSiffw\dmvRs$vSfOmnhjykmopi8nhjmkEnhlzozsafrmokdfhvSiffjvRlznhozoptOnh||Lxr'i\nJkmlixRiffjvP|LfsSvjmnhwMiffoptOvR'i
opfsSvCxRi}hiffjer'i6xRfw|rmsSvSfw\ixs8mfiff|Lfw\i0lpxRxlpvRn vSiffklpvRnhrmozvt|rmsSvSfwMix,sRixR}el|Li6nhjmkvRn ghi
vR'ifflpxrmsRlzj'iffssPiffosSi'ixRih,i|nhoovRmlzs|LfsSvIvRmi0 SJ MjvR'i\xRiffsrmopvRs$vRmn vUi
dmxRiffsRiffjvSiffk'ixRih"ai&xRidfhxRvPfjmoptOf }hixnhozonh||r'xnh|LtxRiffsrmopvRsnhjykvSxRiffn v p * Hh \ nhjmk
nhsPiffermnhozozt|LfsSvRopthfai}hix lzjnhjtdn xRvRlz|rmozn xlzjmsSvRnhoozn vRlpfjfh~vR'i
Cf
sStusSvSiffwO"vR'ixRi&wMnffti&kmlixRiffjm|Liffs*iv aiiffjvR'iffsSi,|LfsSvRsvRmn vPafrmokj'iiffkEvSf6i&nh||LfrmjvSiffkfhx
lzj\vRmi*vSxnhlzjmlj'q$fh"vRmi~$DvGUfrmozk\iKdfsRsRlpopi5vSf8rmsSC
BEBE8ED0vSf&k'fvRmlzsfflp"vR'iffsSiP|LfsRvRsGUixRi
guj'f j't0rmsRlzjmq,lzvRs*n ylzozlzvtvSf}n xRtJvR'i8ozfsRsKxn vRlpf'
VT 2

fi>

72

> 9?f7A@d.22

j'fhvR'ixdfhvSiffjvRlznho)lzsRsRrmi8fhxr'vRrmxRiCUfhxRglzsvR'iMr'vRlzozlzvtOfhanOkylznhopfhqr'i,opi}hiffoDdmxRiffkml|LvSfhxffZih q'
vR'ia$-~}es?nhjr'vSvSixnhjm|Liopi}hiffoKdmxiffkmlz|LvSfhxffaih q'?vR'i H u- &dyxRiffkmlz|LvSfhxffafhx6vR'i
qhfnhofhanhrmvSfwMn vRlz|nhozoptEnhkmn dmvRlzj'q0nJsStesRvSiffwOskmlznhopfhqrmi&sSvSxn vSiqhth mlzslzs$sR'fjEvSfO"iCiL"iff|LvRlz}hi
lzj -lpvRwMnhj nhjZI hhC'ixRivR'itrmsSindmxRfhyopiffwn vRlz|kmlznhopfhqr'ikmivSiff|LvSfhxElzjfhxkmixvSf
nhkmn dmvCvR'ikmlnhopfhqr'isSvSxn vSiqhtfhx&nvSxnhljiffjmermlpxRtsStusSvSiffwO0 v8afrmokiMdfsRsRlpyozihnhjmkfhvRmixs
mn }hin xRqr'iffki}hf 8ffhheMPlzxsR|ixRqivnho/pMffhheN
8lpx|y'f h' vRmn vvR'ikylznhopfhqr'i
wMnhjmn qhix snhkmn dmvRn vRlzfjk'iff|lzslpfjms\|nhjiwMnhkmifjvR'iynhsRlzs,fhopfu|nhoKiffmnff}ulpfhxff5l/ ihfjvR'i
ynhsRlsGfhZxRiff|Lfhqjmlzfflzj'q8vRyn vavR'i|r'xRxRiffjvarmvSvSixnhjm|Li$mnhs~iiffjJwMlzsRryjmk'ixsSvSfefukZhfhxavRyn vavR'i|r'xRxRiffjv
r'vSvSixnhjy|LilzsCn|LfhxRxiff|LvRlpfjZPf Ui}hixffKlpv,lzsC|oziffn x\vRmn v\vR'i0k'iff|lzsRlpfj;vSfvSxnhjmsRix,vR'iO|nhozoUvSfn
ermwMnhjO|rmsSvSfwMix*|n xRiIn qhiffjvP|nhjmj'fhvKiwMnhk'i$fjOvR'i$ynhsRls5fhfjmoptJopfu|nho"lj'fhxwMn vRlpfjJiff|nhrmsSi
vR'iJsStesRvSiffw|nhjfhvSiffjxRiff|Lf}hix&xRfwnsRlzjmqopi\ixRxRfhxff ermsDUiiLud"iff|LvCvRmn v8vRmi6n ylzolpvtvSfEi
n yopi,vSf0dmxRiffkylz|LvPvRmiMkmlznhopfhqr'i&fr'vR|Lfw\iMnhs$UiMk'fO'ixRi,lozo|LfjvRlzjer'i,vSfO"i,lzw\dfhxRvRnhjv$i}hiffjlzj
sStusSvSiffwMsKvRyn vrmsSiIopfu|nhoZdmxiffkmlz|LvSfhxs5fhxrmjmkmixsSvRnhjmkmlj'q&nhjyk|LfhxRxRiff|LvRlpfj
|Y


zx

] ^a}^ Q _a

Qy]



f fj~xnhsRs$lznhjmilpvRwnhjZ
l|mn xkc'r'vSvSfjZ-n fflzjK PnhmlzwnhjmkElz|mn iffoRiffn xjms
mnhjmges$vS
fhxkmlzsR|rysRsRlpfjmsUfjO} n xlpfrysKnhsSdiff|LvRs*fhvRmlzs5UfhxRg"
uQ~/Q

Qy] Q


P"iffooznuyCpfhxlzjy8)ffhhIUfjmsSvSxry|LvnhopqhimxnuKjEnhjynhoptvRl|nhow\ivR'fukEfhx$kmlznhopfhq6wMnhju
n qhiffwMiffjvm
j SffR
Z / _ [ L\ [ ]
Z/ I\ [u ] /hh^

h
" H "
Z /
5n qhqlznuDp-5nhsSvRn qj'ixl/&p $nhjmlziffozl/)ffhhClpiffozk xlznhozsPfhGvR'i, Rv nholznhjE' *Rc x nhlzj
lzw\ivRn yopiCcutesSvSiffw~j fhSm / _ R [ p}Zh#I _ LzRr+ yhh ]5 !/
/ z c
_%_]Iudmdys e
UxlpiffwMnhj~Gp5'xlpiffkywMnhjZ { Cp*$ozsR'iffjR IGCp5cevSfjmih5 { KffhN "
ZhR / _ GnhkmsSUfhxRvRnhjykUxRffhgusyEfjvSixRitE5nhozlpfhxjmlnu
5n vRopivSv


{ ffhu GEiqnhlzjmkyrm|LvRlpfjZGvSiffsRvlpqvjX Z /I\[u5Zr[e\[My"hh
h<ISM [ R mZ

5eru5n xRxfozo/ { p'5n xRdiffjvSixff'ffhh)iff|LvSfhxSHnhsSiffkjmn vRr'xnhooznhjmqrmn qhi|nhooxfr'vRlzj'q'& h
" H " 1Z h7;hu81;h
Uf'iffj "ffhh-ynhsSv~iLiff|LvRlp}hixrmopiljmkmrm|LvRlpfjmj SffR Z /I\[u
h<ISM [ R mZ

_ IL\[,y"hh

Uf'iffj" ~ffhh,iffn xjmlj'q6vSxiiffsInhjmkxrmopiffsPlpvRsSivH} nhozr'iffkiffn vRr'xRiffsffIjO Ly\[
IffLRL",/I8\[' ] L ]$ ffffh/ /I ] aff yL#Z




*PwMwlz|veCe&puopfjmsSf' ffhh8j'f oziffk'qhi|Lfozopiff|LvRlzfjMfhxajmn vRr'xnhoyoznhj'qryn qhisSdfhghiffj
kylznhopfhq8sStusSvSiffwMsff"j Z /P
\'[ ' Rh <ffI LRL"Ih h [ r+ m/
h _ [ " hZ
VT!

fi

2Zy222 S2-



7M1m



72

mr'xj'gxnhj'h { pmlzkmwMixff',Zffh~jm|Lxiffw\iffjvRnho"xRiffkyrm|LiffkJixRxRfhx5dmxrmjylzj'q'ZjX Z /I
\ [' 5z y\[V8 " pIS, [" R mZ
fhxljZCph lz||n xkml/&pxlzqv
h-h< ;ff
{

effhh'fEn tIiffozd8Gfrqm [


hb m/h/

lpxsR|ixRq' { mpy-lpvRwMnhjZm& { p ceUixRvRsm-ffhha~xRfsSfukmlz|$|r'iffsKvSfMxiff|LfhqjmlpvRlpfj0ixRxfhxs
6
j ffG/I8\[' ] HrMh [ }Zm/ h "L H " 1Z [e
lpxsR|ixRq' { pGlpvRwnhjZ)C { p)ceUixRvRs)*/ hhiffj'ixnhozlzfflzj'qdmxfsSfekylz|Mdmxiffkmlz|LvRlpfj
fhsSdiiff|xRiff|LfhqjylpvRlpfjixRxRfhxsj Z /IO\[uT \[Lh/ h<IS/I
1
Z < Z
ZY -u K Gzff8
lpxsR|ixRq' { upu-lpvRwMnhjZu& { p'cuaixRvRsffuZ/ h'nGPivSiff|LvRlj'q\wMlzsSxRiff|LfhqjylpvRlpfjmsUnhjmkJ|LfhxS
xiff|LvRlpfjms)lzj&sSdfhghiffj\kmlznhopfhqrmiUsStusSvSiffwMsDxRfwnff5n xRihsRlzvSiffs'j" ff 1Z /I*\[uVh [e
h ff 0 h [ } Zm ; "L H " 1
Z
lpxsR|ixRq' { DPpG-lpvRwMnhjZ& { pG ceUixRvRs)5/ h'Lk'iffjvRlztulzj'qrysSixC|LfhxRxiff|LvRlpfjms,nhru
vSfwn vRlz|nhozoptlzjsSdfhghiffjkmlnhopfhqr'i\sStusSvSiffwO\j, SffR Z /IM\[u ffZ/I\['
C L\ [ ] L [e &/ IC\ [u ]$ ffff
h " H " Z /

Ilpx|m'f M/ h' e|Lfw\dyn xlzsSfjfhu|oznhsRsRlp|n vRlpfjvSiff|mjmlzer'iffsZfhxvR'ianhr'vSfwMn vRlz|GkmivSiff|LvRlpfj8fh
ixxRfhx~|LfhxRxRiff|LvRlzfjms~lzj\ermwMnhj'|Lfw\dyr'vSixGkmlnhopfhqr'iffsjQ ff 1Z {
\u[ 'C L\[ ]
L
] e[ ] H W$L Z
ff0
Z /C
\u[ - ]')
xnhyw\ixfffip'ceUixRvRsep 'iffrmjmihpeiiqhiffos'ffhh nGxRfhopiffw sSdfhvSvRlzj'qClzj\rywMnhju
wnh|mlzjmiIlzjvSixnh|LvRlpfjZGj6 Sff H R
[ ff
xnhyw\ixffp$ceUixRvRs$p iffrmj'ihPpiiqhiffozs8ffhh~xRfsSfukmlz||LfhxRxRiffozn vSiffsOfh
kylzsR|LfjuyxwMn vRlpfjmsj #] [e hCyLSff\ 8 p}Z 5C ff
nhjmqhgelzok'ihKpPnhopghixffKCpxlpqv { pfhxljZCplpvRwnhjZ*&ffhhPr'vSfwn vRlz|
dyxRiffkmlz|LvRlpfjfh*dmxRfhopiffwMn vRlz|6rmwnhju|Lfw\dyr'vSix,kmlznhozfhqr'iffsCljPf nfftIPiffopd;GfrqEj

Z / I*\ ['aR [u ] Hr Mh/ h R [ } Zm/ " H

Z ce]$ z
i}hfI& Cuffhhy5yn xnh|LvSixlpfflzjmqnhjmkIxiff|Lfhqjmlpfflzj'qsSdfhghiffj8|LfhxRxiff|LvRlpfjmsDlzj8ermwMnhju|LfwMdyr'vSix
kylznhopfhqr'ihUS
j SffR
Z / IC\ [u \ [ ] ff1
ZO/ IC\ [u ]$ ffffh/ { r H
/ h
Z / 'dmdr ;h ue
lzvRwMnhjZm& { pPlpxsR|ixRq' { ypceaixvRsy-/ hhKGxRiffkml|LvRlzj'q\nhr'vSfwMn vRlz|CsSdiiff|Oxiff|Lfhq
jylpvRlpfjd"ixfhxwnhjm|Li6rmslzj'qdyxRfsSfukmlz|M|r'iffsffj SffR Z {IJ\[u Z{I0\['
C L\ [ ] L [e &/ IC\ [u ]$ ffff
h " H " Z /
lzvRwMnhjZ& { pZ nhjZc"/ hhGxRiffkml|LvRlzj'qnhjmknhkmn dmvRlj'qvSfJdffhx$sSdiiff|ExRiff|LfhqjmlpvRlzfjElzjn
sRd"fhghiffjkmlznhopfhqr'i*sRtesSvSiffwyjV Sff/IP\[u yy\[\8 " h<IS$h ] aff
L# Z c]]'] Gzff
lzvRwMnhjZuC { pnhopghix m'Cp' iffn xjmsu { Zffhh)PrmvSfwMn vRlz|$k'ivSiff|LvRlpfj0fhdfefhxUsRd"iiff|
xiff|LfhqjmlpvRlpfj\n v)vR'i*kylznhopfhqr'i5opi}hiffo/'jQ SffR Z /I*\[u _ [ hLy\[ ] ff1Z
{ I8\ [u ] /hh/ r Hh/ Z 'dmd=; 1;uffe
VT

&

fi>

72

> 9?f7A@d.22

l||n xkml/~&pGfhxlzjC*/ hh;cedfhghiffjoznhj'qrmn qhiOnhkmn dyvRn vRlpfj;f }hixMvRlzwMi6nhjmksSvRn vSilzj;n
jyn vRr'xnhoGsSdfhghiffjkylznhopfhqOsStusSvSiffwO R _ ff/ h
] / Sff Z
[
; u

cunhjmkmixwMnhjZGCp5cevRrmxwO { p~k'iffj$st*paUf}hiffsKGpUUxiffw\ixs~CffhhG} nhozrmn vRlzfjfh
vRmikmrmvR|vSxnhlzj?vRlzw\ivRn yopilzj'fhxwMn vRlzfjsStusSvSiffw k'i}hiffopfhdiffklzjvRmi' *RcdmxRf Siff|Lv j
LSff\
_ [h} Z+
IL _ pr + mh/ ]5 !/h/ zc _%_1]dydu8

ceiffj'iLc"peZr'ihCp-folpxfjml/ { pn f'u$pivR'ixlzj'qhvSfjZ~pfukmk'iffnhrZ&poznhsRs { 'ffhh
'iMdmxRiffolzwMlzjmn xtk'i}hiffopfhdywMiffjvIfh*nkmlzsSdyonfftuopiffsRsa5$c:$csStesRvSiffwO6j ] ]h

Z p Z _ R [ p} Zh
[e
cu'xlpixRq'ep)nhk'ihapa~xlz|LihGD*ffhhrmwMnhjuwMnh|mlzj'i6dmxRfhyoziffwsSfoz}elzjmqErmsRlj'qsSdf
ghiffjonhj'qrmn qhi,sStesRvSiffwMs\HcuDcmmnh|LvSfhxs8n"iff|LvRlzjmqJdixRfhxwMnhjy|LiCnhjmkrmsSix$sRn vRlzsRnh|LvRlpfjj

Z / I8\ [u ] ] h R [E )
[u ydyd' e'
ceUixRvRsKp*lpvRwMnhj5& { p*lpxsR|ixRq' { ~/ hh UfhxxRiff|LvRlpfjms6lzjsSdfhghiffjkylznhopfhqr'i
sRtesSvSiffws
j Z /I$\[u \[yLh/ h<IS&/I h L hZ <Ze
5 Gffff

ZS -u*
nhopghixffZ8pmxRfw\ixff { Zpn xnfftnhjmnhjDc")ffhhCiffn xjmlzj'qJfhdmvRlzwMnhoDkmlznhozfhqr'i&sRvSxn vSiL
qlziffs8|nhsSisSvRryk'tEfhKnsRd"fhghiffjkmlznhopfhqr'iMn qhiffjv8fhxIiffwMnhlo/\j+ SffR Z /I\[u \[
] " ZO/ IC\ [u ]$ ffff ;/ r Hh/ Z c& ~ RQ ]) ff
dyd< ; < ;hhe
nhopghixff'Cp18nhwMwOu$8p'-lpvRwMnhjZe& { / hhhn f 5n xkmsUk'i}hiffopfhdylj'qIqhiffj'ixnhow\fuk'iffozs
fh"rmsRn ylozlpvtPlpvRC'

c mj Ch hZ <Ze$KZh"LZ h / e )$
ff/& 6
8 p} Z
nhopghixffa8p)-nhj'qhgelozk'ihpxlzqv { pG$fhxlzjZ)Cp~-lpvRwMnhjZ)&5/ hh iffn xjmlzjmqvSf
~xRiffkmlz|LvUGxRfhopiffwMn vRlz|$culpvRrmn vRlpfjysGlzjn\cedfhghiffjJlnhopfhqr'i$cutesSvSiffwGedixlw\iffjvRs~lpvR6f
nfftEPiffopdGf
r hY
j SffRh
Z / I&\ ['Z
C \ [ ] L
Z{ I&\ [u ]$ ffff

"








h H " Z /
nhopghixffG8pG-lpvRwMnhjZ)&pRInhwMwO~D8p~ P"iffooznuDCKffhh' Sc- qhiffjmixnho
xnhw\iUfhxRgfhxi}nhozryn vRlzj'q6sRd"fhghiffjkmlnhopfhqr'iCn qhiffjvRsjY SffRhZ /I&\[' h\[ ]
ff
Z/ I6\ [u ]$ ffff / r Hh/ h
Z / c]) ]" GdmdDe8
h u
nhopghixffmCpmxlpqv { py nhj'qhgulzozk'ihe/ hhh| t:sRlzj'q\jmn vRr'xnhoZoznhj'qrmn qhidmxfe|LiffsRslzj'q\nhjmk
kylzsR|Lfr'xsRiMiffn vRrmxRiffs8vSflzk'iffjvRlptrmjyk'ixsSvRnhjmkylzj'q0ixRxRfhxsClzjnEsSdfhghiffjkmlnhopfhqr'isStusSvSiffwOOj

Z / I8\ [u yy\ [6yLh/ < IffLRL", [" m1
Z
ifflzsRsffIc*p8rmozlpghfsSgul/8ffhu h " _ [uh R q
h h\ [eff Ih Hh / c & Cff c
[ m1Z c R
Kcunhjn vSif'5CyEfhxRqnh
j 8nhr'wnhjmjZ
xlpqvC$/ hh L#Z ff /; A8/ Z O<I zMh/ UIL
] HrMh/ h R[
R
Zhmh UaZ&mvR'iffsRlsE:Pjylp}hixsRlpv tfh~kylzjr'xRqZ
VT 3

fiJournal Artificial Intelligence Research 16 (2002) 135-166

Submitted 7/01; published 2/02

Improving Eciency Inductive Logic Programming
Use Query Packs
hendrik.blockeel@cs.kuleuven.ac.be

Hendrik Blockeel

Katholieke Universiteit Leuven, Department Computer Science
Celestijnenlaan 200A, B-3001 Leuven, Belgium

luc.dehaspe@pharmadm.com

Luc Dehaspe
PharmaDM, Ambachtenlaan 54D, B-3001 Leuven, Belgium

Bart Demoen
Gerda Janssens
Jan Ramon

bart.demoen@cs.kuleuven.ac.be
gerda.janssens@cs.kuleuven.ac.be
jan.ramon@cs.kuleuven.ac.be

Katholieke Universiteit Leuven, Department Computer Science
Celestijnenlaan 200A, B-3001 Leuven, Belgium

henk.vandecasteele@pharmadm.com

Henk Vandecasteele

PharmaDM, Ambachtenlaan 54D, B-3001 Leuven, Belgium

Abstract

Inductive logic programming, relational learning, powerful paradigm machine
learning data mining. However, order ILP become practically useful,
eciency ILP systems must improve substantially. end, notion query pack
introduced: structures sets similar queries. Furthermore, mechanism described
executing query packs. complexity analysis shows considerable eciency
improvements achieved use query pack execution mechanism.
claim supported empirical results obtained incorporating support query
pack execution two existing learning systems.
1. Introduction

Many data mining algorithms employ extent generate-and-test approach: large
amounts partial complete hypotheses generated evaluated data
mining process. evaluation usually involves testing hypothesis large data set,
process typically linear size data set. Examples data mining
algorithms Apriori (Agrawal et al., 1996), decision tree algorithms (Quinlan, 1993a;
Breiman et al., 1984), algorithms inducing decision rules (Clark & Niblett, 1989), etc.
Even though search hypothesis space seldom exhaustive practical
situations, clever branch-and-bound greedy search strategies employed, number hypotheses generated evaluated approaches may still huge.
especially true complex hypothesis space used, often case inductive
logic programming (ILP), sheer size hypothesis space important
contribution high computational complexity ILP approaches. computational complexity reduced, however, exploiting fact many
similarities hypotheses.

c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
ILP systems build hypothesis one clause time. search single clause
concerned rest paper, word \hypothesis"
usually refer single clause. clause search space typically structured
lattice. clauses close one another lattice similar, computations
involved evaluating similar well. words, many computations
performed evaluating one clause (which boils executing query
consisting body clause) performed evaluating
next clause. Storing certain intermediate results computation later use could
solution (e.g., tabling XSB Prolog engine, Chen & Warren, 1996), may
infeasible practice memory requirements. becomes feasible
search reorganised intermediate results always used shortly
computed; achieved extent rearranging computations.
best way removing redundancy, however, seems re-implement execution
strategy queries way much computation possible effectively
shared.
paper discuss strategy executing sets queries, organised so-called
query packs, avoids redundant computations. strategy presented adaptation standard Prolog execution mechanism. adapted execution mechanism
implemented ilProlog, Prolog system dedicated inductive logic programming. Several inductive logic programming systems re-implemented make use
dedicated engine, using new implementations obtained experimental
results showing cases speed-up order magnitude. Thus,
work significantly contributes applicability inductive logic programming real
world data mining tasks. addition, believe may contribute state art
query optimisation relational databases. Indeed, latter field lot
work optimisation individual queries relatively small sets queries, much
less optimisation large groups similar queries, understandably
get much attention advent data mining. Optimisation groups queries
relational databases seems interesting research area now, believe techniques
similar ones proposed might relevant area.
remainder paper structured follows. Section 2 precisely describe
ILP problem setting work set. Section 3 define notion
query pack indicate would executed standard Prolog interpreter
computational redundancy causes. describe execution mechanism
query packs makes possible avoid redundant computations would arise
queries pack run separately, show implemented making
small significant extensions WAM, standard Prolog execution mechanism.
Section 4 describe query pack execution strategy incorporated two
existing inductive logic programming algorithms (Tilde Warmr). Section 5
present experimental results illustrate speed-up systems achieve
using query pack execution mechanism. Section 6 discuss related work
Section 7 present conclusions directions future work.

136

fiImproving Efficiency ILP Query Packs
2. Inductive Logic Programming

Inductive logic programming (Muggleton & De Raedt, 1994) situated intersection
machine learning data mining one hand, logic programming
hand. shares former fields goal finding patterns data, patterns
used build predictive models gain insight data. logic programming
shares use clausal first order logic representation language data
hypotheses. remainder text use basic notions logic
programming, literals, conjunctive queries, variable substitutions. use
Prolog notation throughout paper. introduction Prolog logic programming
see Bratko (1990).
Inductive logic programming used many different purposes, problem
statements found ILP papers consequently vary. article consider so-called
learning interpretations setting (De Raedt & Dzeroski, 1994; De Raedt, 1997).
argued elsewhere setting, slightly less powerful standard
ILP setting (it problems with, e.g., learning recursive predicates), sucient
practical purposes scales better (Blockeel et al., 1999).
formulate learning task way covers number different problem
statements. specifically, consider problem detecting set conjunctive
queries instantiations certain variables query succeeds. variables
called key variables, grounding substitution called key instantiation.
intuition example learning task uniquely identified single key
instantiation.
link ILP systems learn clauses follows. search performed
ILP system directed regularly evaluating candidate clauses. Let us denote
candidate clause Head(X )
Body (X; ) X represents vector variables
appearing head clause represents additional variables occur
body. assume head single literal list examples given,
example form Head(X ) substitution grounds X . Examples
may labelled (e.g., positive negative), essential setting.
example represented fact Head(X ) learning definite Horn clauses,
also consider tuple X. notations used paper.
Intuitively, positive negative examples given, one wants find clause
covers many positive examples possible, covering negatives.
Whether single example Head(X ) covered clause determined
running query ? Body(X; ). words, evaluating clause boils
running number queries consisting body clause. simplicity notation,
often denote conjunctive query conjunction (without ? symbol).
less typical ILP settings, ILP algorithm search Horn clauses
rather general clauses, e.g., Claudien (De Raedt & Dehaspe, 1997) frequent
patterns expressed conjunctive queries, e.g., Warmr(Dehaspe & Toivonen,
1999). settings handled approach well: needed mapping
hypotheses queries allow evaluate hypotheses. mapping
defined De Raedt Dehaspe (1997) Claudien; Warmr trivial.

137

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
Given set queries set examples E , main task determine
queries Q 2 cover examples e 2 E . formalise using notion result
set:

Definition 1 (Result set) result set set queries deductive database
key K example set E ,
RS (S; K; D; E )

= f(K; i)jQi 2 K 2 E Qi succeeds Dg

Similar learning interpretations setting defined (De Raedt, 1997),
problem setting stated as:

Given: set conjunctive queries , deductive database D, tuple K variables
occur query S, example set E
Find: result set RS (S; K; D; E ); i.e., find query Q ground
instantiations K K 2 E Q succeeds D.
Example 1 Assume ILP system learning definition grandfather/2 wants evaluate following hypotheses:

grandfather(X,Y) :- parent(X,Z), parent(Z,Y), male(X).
grandfather(X,Y) :- parent(X,Z), parent(Z,Y), female(X).

Examples form grandfather(gf ,gc) gf gc constants; hence
example uniquely identified ground substitution tuple (X; ).
problem setting set Prolog queries equals f(?- parent(X,Z), parent(Z,Y),
male(X)), (?- parent(X,Z), parent(Z,Y), female(X))g key K equals (X; ).
Given query Qi 2 , finding tuples (x; y) ((x; y); i) 2 R (with R result
set defined above) equivalent finding grandfather(x,y) facts
example set predicted clause grandfather(X,Y) :- Qi .

generality problem setting follows fact known
queries succeed examples, statistics heuristics typical ILP systems
use readily obtained this. examples:






discovery frequent patterns (Dehaspe & Toivonen, 1999): query Qi
number key instantiations succeeds needs counted, i.e.,
f req (Qi ) = jfK j(K; i) 2 Rgj R result set.
induction Horn clauses (Muggleton, 1995; Quinlan, 1993b): accuracy
clause H :- Qi (defined number examples body head hold,
divided number examples body holds) computed
jfKj(K;i)2R^Dj=Hgj R result set.
jfKj(K;i)2Rgj
induction first order classification regression trees (Kramer, 1996; Blockeel &
De Raedt, 1998; Blockeel et al., 1998): class entropy variance examples
covered (or covered) query computed probability distribution
target variable; computing distribution involves simple counts similar
ones above.

138

fiImproving Efficiency ILP Query Packs
transforming grandfather/2 clauses
grandfather((X,Y)),I) :- parent(X,Z), parent(Z,Y), male(X), = 1.
grandfather((X,Y)),I) :- parent(X,Z), parent(Z,Y), female(X), = 2.

result set clearly computed collecting grounding 's K 2 E
answers query ?- grandfather(K,I) . Section 3 queries literal
= end another goal side-effects results collecting result set.
practice, natural compute result set using double loop: one examples
one queries one choice outer loop. \examples
outer loop" \queries outer loop" used data mining systems;
context decision trees, see instance Quinlan (1993a) Mehta et al. (1996).
shall see redundancy removal approach propose uses \examples
outer loop" strategy. approaches however, given query key instantiation,
interested whether query succeeds key instantiation. implies
particular query succeeded example, execution stopped.
words: computing result set defined boils evaluating
query example, interested existence success
evaluation. Computing one solution one query one example unnecessary.
3. Query Packs

simplicity, make abstraction existence keys following examples.
relevant here, query interested whether succeeds not,
finding answer substitutions.
Given following set queries
p(X),
p(X),
p(X),
p(X),
p(X),

= 1.
q(X,a),
q(X,b),
q(X,Y),
q(X,Y),

= 2.
= 3.
t(X), = 4.
t(X), r(Y,1), = 5.

choose evaluate separately. Since interested one { first {
success query, would evaluate Prolog queries
once((p(X),
once((p(X),
once((p(X),
once((p(X),
once((p(X),

= 1)).
q(X,a), = 2)).
q(X,b), = 3)).
q(X,Y), t(X), = 4)).
q(X,Y), t(X), r(Y,1), = 5)).

wrapper once/1 pruning primitive prevents unnecessary search
solutions. definition Prolog simply
once(Goal) :- call(Goal), !.

139

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
alternative way evaluate queries consists merging one (nested)
disjunction in:
p(X), (I=1

;

q(X,a), I=2

;

q(X,b), I=3

;

q(X,Y), t(X), (I=4

;

r(Y,1), I=5)).

set queries evaluated whole: success one branch
disjunctive query corresponds success corresponding individual query.
Compared evaluation individual queries, disjunctive query
advantage disadvantage:
+ queries prefix p(X), evaluated individual
query, disjunctive query, goal p(X) evaluated once; depending
evaluation cost p/1, lead arbitrary performance gains.
usual Prolog pruning primitives powerful enough prevent unnecessary backtracking branch disjunctive query succeeded;
explained Example 2.

Example 2 example literals

contribute discussion:

= left out,



p(X), q(X).
p(X), r(X).

Evaluating queries separately means evaluating
once((p(X), q(X))).
once((p(X), r(X))).

equivalently
p(X), q(X), !.
p(X), r(X), !.

corresponding disjunctive query
p(X), (q(X) ; r(X)).

try place pruning primitive disjunctive query: !/0 end
branch results
p(X), (q(X), ! ; r(X), !)

scope first cut clearly large: goal q(X) succeeded, cut
prevent entering second branch. means adding cut disjunctive
query leads wrong result.
Using once/1 disjunctive query results
p(X), (once(q(X)) ; once(r(X)))

140

fiImproving Efficiency ILP Query Packs
results correct query. However, branches still executed every
binding goal p(X) produces, even branches succeeded already.

combination advantage disjunctive query advantage
individual query pruning (once cut) results notion query pack. Syntactically, query pack looks like disjunctive query ; control construct
replaced new control construct denoted or. query pack corresponding
disjunctive query
p(X), (I=1



q(X,a), I=2



q(X,b), I=3



q(X,Y), t(X), (I=4



r(Y,1), I=5))

query pack represented tree Figure 1. query pack Q
tree literals conjunctions literals nodes. path root leaf
node represents conjunctive query Q member Q, denoted Q 2 Q.
construct implicit branching points.
p(X)
I=1

q(X,a),
I=2

q(X,b),
I=3

q(X,c),
I=4
I=5

q(X,Y), t(X)
r(Y,1),
I=6

r(Y,2),
I=7

Figure 1: query pack.
intended procedural behaviour construct branch succeeded, effectively pruned away pack evaluation query pack
current example. pruning must recursive, i.e., branches subtree
query pack succeeded, whole subtree must pruned. Evaluation
query pack terminates subtrees pruned remaining
queries fail example.
semantics construct ecient implementation subject
rest section. however clear already case
answers query needed, pruning cannot performed disjunctive query
already sucient, i.e., query packs useful single success per query suces.

3.1 Ecient Execution Query Packs
Section 3.1.2, meta-interpreter given defines behaviour query packs.
practice meta-interpreter useful, many cases meta-interpreter
causes overhead use query packs compensate for. Indeed, previously
reported results (Demoen et al., 1999; Blockeel, 1998) indicate overhead involved
high-level Prolog implementation destroys eciency gain obtained redundancy
reduction. Moreover discussed Section 3.1.2, meta-interpreter
desired time-complexity. shows desired procedural semantics

141

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
implemented Prolog itself, desired performance Prolog lacks
appropriate primitives.
conclusion changes needed level Prolog engine itself.
requires extension WAM (Warren Abstract Machine) underlying
abstract machine Prolog implementations. extended WAM provides
operator discussed above: permanently removes branches pack
need investigated anymore. extended WAM become basis new Prolog
engine dedicated inductive logic programming, called ilProlog. section continues
introduction basic terminology query packs explains high level
query pack execution works. Next meta-interpreter query pack execution
given finally changes needed WAM clarified.
3.1.1 Principles Query Packs (Execution)

discuss query pack execution detail, note following two points: (1)
pack execution, pruning branch must survive backtracking; (2) executing
pack interested variable instantiations, whether member
pack succeeds not. previous description interested binding
variable I. Since branch bind one value { query number { collect
values practice side effect denoted Section 3.2 report success.
starting point query pack execution mechanism usual Prolog execution
query Q given Prolog program P . backtracking Prolog generate
solutions Q giving possible instantiations Q succeeds P .
query pack consists conjunction literals set alternatives,
alternative query pack. Note leaves query packs empty set
alternatives. query pack Q, conj (Q) denotes conjunction children(Q)
denotes set alternatives. set queries represented so-called root query
pack. every query pack Q, path query packs starting root query
pack Qroot ending query pack itself, namely < Qroot , Q1 , ..., Qn , Q >.
query packs path predecessors Q. Every query pack set dependent
queries, dependent queries(Q). Let < Qroot , Qi1 , ..., Qin , Q > path Q,
dependent queries(Q) = fconj (Qroot ) ^ conj (Qi1 ) ^ : : : ^ conj (Qin ) ^ conj (Q) ^ conj (Qj1 ) ^
: : : ^ conj (Qjm ) ^ conj (Ql ) j < Q; Qj1 , ..., Qjm , Ql > path Q leaf Ql g. Note
dependent queries(Qroot ) actually members query pack described
earlier.

Qroot root tree. conj (Qroot )
Qroot ) contains 4 query packs correspond trees

Example 3 query pack Figure 1,
p(X ). set children(

rooted 4 sons root tree. Suppose query packs named (from
left right) Q1 , Q2 , Q3 , Q4 . conj (Q2 ) equals (q(X; a); = 2), children(Q2 )
equals empty set, conj (Q4 ) equals (q(X; ); t(X )), dependent queries(Q4 ) equals
f(p(X ); q(X; ); t(X ); = 4), (p(X ); q(X; ); t(X ); r(Y; 1); = 5)g.

Execution root query pack Qroot aims finding queries set
dependent queries(Qroot ) succeed. query pack executed ors usual
disjunctions, backtracking occurs queries already succeeded many

142

fiImproving Efficiency ILP Query Packs
0
1
2
3
4
5
6
7
8
9
10
11

execute qp( pack Q, substitution ) f
( next solution( conj (Q))

f

Qchild children(Q)

f
g

g

( execute qp( Qchild , ) == success)
children(Q)
children(Q) n fQchild g

( children(Q) empty set) return(success)

return(fail)

g

Figure 2: query pack execution algorithm.
successes detected. avoid this, case soon query succeeds,
corresponding part query pack longer considered backtracking. approach realises reporting success queries (and query packs)
predecessors query pack. (non-root) query pack Q safely removed
queries depend (i.e., queries dependent queries(Q)) succeeded once.
leaf Q (empty set children), success conj (Q) sucient remove it.
non-leaf Q, wait dependent queries report success equivalently
query packs children(Q) report success.
start evaluation root query pack, set children every query
pack contains alternatives given query pack. execution, query
packs removed children sets thus values children(Q) change
accordingly. due backtracking query pack executed again, might case
fewer alternatives considered.
execution query pack Q defined algorithm execute qp(Q; ) (Figure
2) imposes additional control usual Prolog execution.
usual Prolog execution backtracking behaviour modelled loop
(line 1) generates possible solutions conjunction query pack.
solutions found, fail returned backtracking occur level
calling query pack.
additional control manages children(Q). solution , necessary
children Q executed. important notice initial set children
query pack changed destructively execution algorithm. Firstly,
leaf reached, success returned (line 8) corresponding child removed
query pack (line 6). Secondly, query pack initially several children, finally
ends empty set children (line 6), also query pack removed (line 8).
fact children destructively removed, implies due backtracking
query pack executed different , alternatives
initially there, executed more. Moreover, returning success

143

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
qp(1)
ch(1)

ch(3)
ch(2)

b qp(2)
ch(1)

f

q(4)

g qp(3)
ch(1)

ch(3)
ch(2)

c q(1)

q(2)

ch(3)
ch(2)

e q(3)

h q(5)

q(6)

j

q(7)

Figure 3: Query pack numbers qp(i), Query numbers q(i) Child numbers ch(i)
example.
backtracking current query pack conjunction conj (Q) stopped: branches
reported success.
3.1.2 Meta-interpreter Query Packs

first implementation query pack execution algorithm meta-interpreter
meta execute qp(Q). meta-interpreter uses following labelling representation
query pack:

Query pack number non-leaf query packs tree numbered, depth
first, left right (qp(i)).

Query number leaf numbered, left right. original queries
numbered sequentially, numbers leaves correspond (q(i)).

Child number non-leaf query pack N children, children numbered
1 N sequentially (ch(i)).

Consider query pack a, (b, (c e) f g, (h j)). Note
atoms example could general arbitrary conjunctions non-ground terms.
labelling shown Figure 3.
labelled query pack Q represented Prolog term follows (with Qf
father Q):



leaf

Q represented term (c; leaf (qpnbf; chnb; qnb)) c conj (Q),
query pack number Qf , chnb child number Q w.r.t. Qf , qnb
query number Q.



non-leaf Q represented term (c; or(cs; qpnbf; qpnb; chnb; totcs) c
conj (Q), cs list children(Q), qpnbf query pack number Qf , qpnb query
pack number Q, chnb child number Q w.r.t. Qf , totcs total number
children(Q)). query pack number father root query pack
assumed zero.

qpnbf

144

fiImproving Efficiency ILP Query Packs
example Figure 3 following representation (as Prolog term):
(a, or([(b,or([(c,leaf(2,1,1)),(d,leaf(2,2,2)),(e,leaf(2,3,3))],1,2,1,3)),
(f,leaf(1,2,4)),
(g,or([(h,leaf(3,1,5)),(i,leaf(3,2,6)),(j,leaf(3,3,7))],1,3,3,3))],
0,1,1,3))

execution meta-interpreter, solved/2 facts asserted. fact
solved(qpnb, chnb) denotes child number chnb query pack number

succeeded. facts asserted reaching leaf also children
query pack succeeded. meta-interpreter executes children
solved/2 fact asserted.
Note time-complexity meta-interpreter yet desired. Execution
query pack always dependent number original children, instead
number remaining (as yet unsuccessful) children.
qpnb

run QueryPack(Q) :preprocess(Q, Qlabeled, 0, 1, 1, 1, , ),
% code preprocessing given Appendix
retractall(solved( , )),
meta execute qp(Qlabeled),
solved(0, ), !.
meta execute qp((A,B)) :- !,
call(A),
meta execute qp(B).
meta execute qp(or(Cs, QpNbF, QpNb, ChildNb, TotCs)) :!, % 'or' corresponds non-leaf query pack
handlechildren(Cs, QpNb, 1),
solved(QpNb, 0, TotCs),
assert(solved(QpNbF,ChildNb)).
meta execute qp(leaf(QpNbF, ChildNb , QueryNb)) :!, % 'leaf' corresponds end query
write(succeed(QueryNb)), nl,
assert(solved(QpNbF,ChildNb)).
handlechildren([], , ).
handlechildren([C| ], QpNb, ChildNb) :not(solved(QpNb,ChildNb)),
once(meta execute qp(C)), fail.
handlechildren([ |Cs], QpNb, ChildNb) :ChildNb1 ChildNb + 1,
handlechildren(Cs, QpNb, ChildNb1).
solved(QpNb, ChildNb, TotCs) :(ChildNb = TotCs -> true
;
ChildNb1 ChildNb + 1,
solved(QpNb, ChildNb1),
solved(QpNb, ChildNb1, TotCs)
).

145

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
3.1.3 WAM Extensions

fully exploit potential query pack (shared computation avoidance unnecessary backtracking) changes made level Prolog engine itself.
explanation assumes WAM-based Prolog engine (At-Kaci, 1991) short explanation
execution disjunction Prolog given first, becomes easy see
newly introduced WAM.
Assume body clause executed a, (b,c ; ; e). Assume also
predicates several clauses. moment execution reached first
clause c, choice point stack looks like Figure 4(a): choice points
activation a, disjunction itself, b c. choice points linked together
backtracking easily pop top one. choice point contains pointer
next alternative tried: disjunction choice point, alternative pointer
shown. points beginning second branch disjunction.
alternatives b c exhausted, second branch entered becomes
active: situation shown Figure 4(b). point, alternative
disjunction choice point refers last alternative branch disjunction. Finally,
e entered, disjunction choice point already popped.
a, (b, c ; ; e)

a, (b, c ; ; e)

a, (b, c ; ; e)







;

;

e

b



c

(a) Choice points
entering c.

(b) Choice points
entering d.

(c) Choice points
entering e.

Figure 4: Illustration execution disjunction WAM.
goal produces new solution, branches disjunction must tried
again. exactly want avoid query packs: branch succeeded once,
never re-entered. therefore adapt disjunction choice point become
or-choice point set point data structure contains references
alternative disjunction. data structure named pack table. Figure
5(a) shows state execution reached c: similar Figure 4(a).
or-choice point contains information first branch executed.
execution proceeds, two possibilities: either first branch succeeds fails.
describe failing situation first branch explain happens success

146

fiImproving Efficiency ILP Query Packs
second branch. first branch solution, backtracking updates alternative
or-choice point, point next branch pack table. situation
second branch entered shown 5(b) similar 4(b). Suppose
branch goal succeeds: entry pack table or-alternatives
adapted erasing second alternative branch, backtracking occurs, next
alternative branch or-choice point taken. shown 5(c).
produces new solution or-disjunction entered again, pack table
longer contain second alternative branch never re-entered. pack
table actually arranged way entries really removed instead erased
cause overhead later.
a, (b, c e)

a, (b, c e)

a, (b, c e)













b



e

c

(a) choice points
entering c.

(b) choice points
entering (the first
branch succeed).

(c) choice points
entering e (d succeeded).

Figure 5: Illustration execution pack disjunction WAM.
Two issues must explained: first, pack table alternatives must
constructed runtime every time query pack entered evaluation. done
emitting necessary instructions beginning code query pack.
example, show code query pack a, (b,c e) Figure 6.
Finally, example clear moment alternatives ordisjunction succeeded, stop producing solutions. computation
stopped. general - nested query packs - means one pack table entry
next higher or-node erased recursive way. recursive removal
entries pack tables, done instruction query pack prune.
implemented schema ilProlog. Section 5 presents measurements
ilProlog.

3.2 Using Query Packs
Figure 7 shows algorithm makes use pack execution mechanism compute
result set R defined problem statement. set queries typically

147

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
construct pack table @1, @2, @3
call
query pack try
@1: call b
call c
query pack prune
@2: call
query pack prune
@3: call e
query pack prune

Figure 6: Abstract machine code a, (b,c e) .
set refinements given query, i.e., correspond whole hypothesis
space. query pack Q containing queries , derived pack Q0 constructed
adding report success/2 literal leaf pack; (procedural) task
report success(K,i) simply add (K; i) result set R. Obviously specific
ILP system interested result set could provide report success/2
predicate thus avoid overhead explicitly building result set.1
1 evaluate(set examples E , pack Q, key K ) f
2
Q0 Q;
3
q
1;
4
leaf Q0 f
5
add report success(K, q) right conjunction leaf
6
increment q
7
g
8
C
(evaluate pack(K ) :- Q0 );
9
compile load(C);
10
example e E f
11
evaluate pack(e);
12
g
13 g
Figure 7: Using query packs compute result set.
Note algorithm Figure 7 follows strategy running queries
single example moving next example: could called \examples
outer loop" strategy, opposed \queries outer loop" strategy used ILP
1. current implementation result set implemented bit-matrix indexed queries
examples. implementation practically feasible (on typical computers time writing) even
number queries pack multiplied number examples billion, bound
holds current ILP applications.

148

fiImproving Efficiency ILP Query Packs
systems. \examples outer loop" strategy important advantages processing
large data sets, mainly due ability process eciently without data
main memory time (Mehta et al., 1996; Blockeel et al., 1999).

3.3 Computational Complexity
estimate speedup factor achieved using query pack execution two
steps: first consider one-level packs, extend results towards deeper packs.
Lower upper bounds speedup factor achieved executing
one-level pack instead separate queries obtained follows. pack containing
n queries qi = (a; bi ), let Ti time needed compute first answer substitution
qi any, obtain failure otherwise. Let ti part Ti spent within
t0i part Ti spent bi . Ts = (ti + t0i ) Tp = max(ti ) + t0i Ts
representing total time needed executing queries separately Tp total time
needed executing pack. Introducing c = ti = t0i , roughly represents
ratio computational complexity shared part non-shared part,

0
c+1
Ts
ti +
ti
=
= maxi ti
(1)
0
Tp
maxi ti + ti
+1
t0

P

P

P

P P

P
P

P



defining K ratio maximal ti average ti , i.e.

rewrite Equation (1)

Since

P

ti
n

Pmaxt =nt


K

=

Ts
Tp

=



c+1

K
nc

(2)

+1

max ti Pi ti know 1 K n, leads following bounds:
1

Ts
Tp



c+1

c
n

+1

< min(c + 1; n)

(3)

Thus speedup factor bounded branching factor n
ratio c computational complexity shared part computational complexity
non-shared part; maximal speedup attained max ti ' ti =n (or,
K ' 1), words ti queries approximately equal.
multi-level packs, estimate eciency gain follows. Given query qi ,
let Ti defined (the total time finding 1 answer qi obtaining failure).
Instead ti t0i , define ti;l time spent level l pack solving qi ;
counting root level 0 denoting depth pack Ti = dl=0 ti;l .
define Ti;l time spent level l deeper: Ti;l = dj=l ti;j depth
pack. (Thus Ti = Ti;0 .). assume constant branching factor b pack.
Finally, define tl = ti;l =n n = bd . simplicity, formulae implicitly
assume always ranges 1 n n number queries, unless explicitly

P

P

P

149

P

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
specified otherwise.
Tp

= max ti;0


X
+


i;1

= max ti;0


X
+ (max
b

j =1

2

Gj

i;1

XT

+

2

Gj

i;2

)

(4)

j = 1 : : : b index child root Gj set indexes
queries belonging child. define K0 = maxi ti;0 =t0 define K1 smallest
number maxi2Gj ti;1 K1 tj;1 tj;1 = i2Gj ti;1 =b. Note 1 K0 ; K1 b.
follows
b
b
max ti;1 K1 tj;1 = K1 bt1
(5)
i2Gj
j =1
j =1

P

X

X

allows us rewrite Equation (4)
Tp

K0t0 + K1 bt1 +

XT


(6)

i;2

equality holds maxi2Gj ti;1 equal Gj . reasoning continued
till lowest level pack, yielding
Tp

K0t0 + bK1t1 + b2K2 t2 + + bd

finally
Tp

K0 t0 + bK1 t1 + b2 K2t2 + + bd



1K

1 td 1

1K

+



1 td 1

Xt

i;d

(7)



+ bd td

(8)

Kl 1 b. simplify comparison Ts assuming
8l : Kl = 1; Kl dropped inequality becomes equality (because
maxima must equal):
Tp

= t0 + bt1 + b2 t2 + + bd 1 td

1

+ bd td

(9)

1

+ bd td

(10)

Note Ts
Ts

= bd t0 + bd t1 + bd t2 + + bd td

clear, then, speedup governed bd tk terms compare
bk tk terms. (In worst case, Kk = b, latter become bk+1 tk .) therefore
introduce Rl;m follows:

bm tk
(11)
Rl;m = km=l k

k =l b tk

P
P

R coecients always 1 (if tm dominates) bm l (if tl strongly dominates);
tl equal, Rl;m approximately l.
Further, similar c previous analysis, define

P =0 b
c = P

= +1 b
l

l
k


k l

150

k

k

k

k

(12)

fiImproving Efficiency ILP Query Packs
algebra gives

Ts
Tp

=

bd l cl R0;l + Rl+1;d
cl + 1

(13)

needs hold l. interpret follows: certain level l, cl roughly
ects speedup gained fact part till level l needs executed
once; R factors ect speedup obtained within parts pack
mechanism.
inequality holds l, hence find best lower bound speedup factor maximizing right hand side. Note cl increases bd l decreases
monotonically l. clear point cl becomes much larger 1,
speedup factor roughly bd l obtained. hand, cl smaller 1,
behaviour bd l cl crucial. Now,
bd l cl

tl + 1 tl 1 + + b1l t0
= 1 b
:
td + b td 1 + + bd 1l 1 tl+1

conclusion similar one-level pack. l, cl >> 1, i.e.,
upper part pack (up till level l) computations take place expensive
dominate computations level l (even taking account latter
performed bd l times often), speedup bd l expected. cl << 1,
usually case l except near d, speedup roughly
estimated tl =td . maximum factors determine actual speedup.
4. Adapting ILP Algorithms Use Query Packs

section discuss execution method included ILP algorithms, illustrate detail two existing ILP algorithms. Experimental
results concerning actual eciency improvements yields presented next section.

4.1 Refinement Single Rule
Many systems inductive logic programming use algorithm consists repeatedly
refining clauses. systems could principle rewritten make use query
pack evaluation mechanism thus achieve significant eciency gain. first show
concrete algorithm decision tree induction, discuss general case.
4.1.1 Induction Decision Trees

first algorithm discuss Tilde (Blockeel & De Raedt, 1998), algorithm
builds first-order decision trees. first-order decision tree, nodes contain literals
together conjunction literals nodes node (i.e., path
root node) form query run example decide
subtree sorted into. building tree, literal (or conjunction
literals) put one node chosen follows: given query corresponding path
root node, generate refinements query (a refinement query

151

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
formed adding one literals query); evaluate refinements
relevant subset data,2 computing, e.g., information gain (Quinlan, 1993a) yielded
refinement; choose best refinement; put literals added
original clause form refinement node.
point clear lot computational redundancy exists refinement
evaluated separately. Indeed refinements contain exactly literals except
added single refinement step. Organising refinements one query pack,
obtain query pack essentially one level (the root immediately branches
leaves). Tilde's lookahead facility used (Blockeel & De Raedt, 1997), refinements
form lattice query pack may contain multiple (though usually few) levels.
Note root packs may consist conjunction many literals, giving
pack broom-like form. literals root pack, greater benefit
query pack execution expected be.

Example 4 Assume node currently refined following query associated

it: ?- circle(A,C),leftof(A,C,D),above(A,D,E), i.e., node covers examples
circle left object yet another object.
query pack generated refinement could instance

circle(A,C), leftof(A,C,D), above(A,D,E),

triangle(A,F)
circle(A,H)
small(A,I)
large(A,J)
in(A,E,K)
in(A,D,L)
in(A,C,M)
above(A,E,N)
above(A,D,O)
above(A,C,P)
leftof(A,E,Q)
leftof(A,D,R)
leftof(A,C,S)

evaluating pack, backtracking root pack (the \stick"
broom) happen once, instead refinement. words:
evaluating queries one one, query Prolog engine needs search
objects C , E fulfilling constraint circle(A,C), leftof(A,C,D),
above(A,D,E); executing pack search done once.
4.1.2 Algorithms Based Rule Refinement

mentioned, ILP algorithm consists repeatedly refining clauses could principle rewritten make use query pack evaluation mechanism thus achieve
significant eciency gain. Consider, e.g., rule induction system performing search
refinement lattice, Progol (Muggleton, 1995). Since imposes certain order clauses considered refinement, hard reorganise
computation level. However, taking one node list open nodes
producing refinements, evaluation refinements involves executing
them; replaced pack execution, case positive eciency gain
guaranteed. principle one could also perform several levels refinement stage,
2. I.e., subset original data set parent query succeeded; or, decision tree
context: examples sorted node refined.

152

fiImproving Efficiency ILP Query Packs
adding refinements 's queue; part eciency lost,
pack execution mechanism exploited larger extent. two effects
dominant depend application: first-level refinements would
refined anyway point search, clearly gain
executing two-level pack; otherwise may loss eciency. instance,
executing two-level pack takes x times much time one-level pack, bring
eciency gain least x first level refinements would afterwards refined
themselves.

4.2 Level-wise Frequent Pattern Discovery
alternative family data mining algorithms scans refinement lattice breadthfirst manner queries whose frequency exceeds user-defined threshold. bestknown instance level-wise algorithms Apriori method finding frequent
item-sets (Agrawal et al., 1996). Warmr (Dehaspe & Toivonen, 1999) ILP variant
attribute-value based Apriori.
Query packs Warmr correspond hash-trees item-sets Apriori: used
store subgraph total refinement lattice level n. paths root
level n 1 subgraph correspond frequent patterns. paths root
leaves depth n correspond candidates whose frequency computed.
Like hash-trees Apriori, query packs Warmr exploit massive similarity
candidates make evaluation ecient. Essentially Warmr algorithm starts
empty query pack iterates pack evaluation pack extension (see
Figure 8). latter achieved adding potentially frequent refinements3 leaves
pack, i.e., adding another level total refinement lattice.
5. Experiments

goal experimental evaluation empirically investigate actual speedups
obtained re-implementing ILP systems use pack execution
mechanism. moment re-implementations exist Tilde Warmr
systems, hence used experiments. re-implementations
available within ACE data mining tool, available academic use upon request.4
attempt quantify (a) speedup packs w.r.t. separate execution queries (thus
validating complexity analysis), (b) total speedup yield
ILP system.
data sets used experiments following:



Mutagenesis data set : ILP benchmark data set, introduced ILP community Srinivasan et al. (1995), consists structural descriptions 230
molecules classified mutagenic not. Next standard Mutagenesis data set, also consider versions example occurs n times;

3. Refinements found specialisations infrequent queries cannot frequent themselves,
pruned consequently.
4. See http://www.cs.kuleuven.ac.be/~dtai/ACE/.

153

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele

circle(A,B)

triangle(A,B)

leftof(A,B,C) above(A,B,C) leftof(A,B,C)

EXPAND

circle(A,B)

leftof(A,B,C)

triangle(A,B)

above(A,B,C)

leftof(A,B,C)

circle(A,C)triangle(A,C) circle(A,C)triangle(A,C) circle(A,C)triangle(A,C)
EVALUATE
circle(A,B) triangle(A,B)
above(A,B,C)

leftof(A,B,C)

triangle(A,C) circle(A,C)triangle(A,C)

circle(A,B)

EXPAND

above(A,B,C)
triangle(A,C)

triangle(A,B)
leftof(A,B,C)

circle(A,C)

triangle(A,C)

leftof(A,C,D) leftof(A,C,D) above(A,C,D) leftof(A,C,D)

Figure 8: sequence 4 query packs Warmr. Refinement left query
pack results 3-level pack right. Removal queries found infrequent
pack evaluation results bottom left pack. Finally, another level
added second query expansion step produce bottom right pack.
iteration expansion evaluation continues pack empty.
allows us easily generate data sets larger size average example
query complexity constant equal original data set.



Bongard data sets : introduced ILP De Raedt Van Laer (1995), so-called
\Bongard problems" simplified version problems used Bongard (1970)
research pattern recognition. number drawings shown containing
number elementary geometrical figures; drawings classified according
relations hold figures them. use Bongard problem generator
create data sets varying size.

experiments run SUN workstations: Sparc Ultra-60 360 MHz
Tilde, Sparc Ultra-10 333 Mhz Warmr. Tilde Warmr run
default settings, except mentioned differently.

5.1 Tilde
consider three different ways Tilde run ilProlog implementation:
1. packs: normal implementation Tilde described Blockeel De Raedt
(1998), queries generated one one evaluated relevant
examples. Since queries represented terms, evaluation query involves
meta-call Prolog.

154

fiImproving Efficiency ILP Query Packs
2. Disjoint execution packs: query pack executed queries pack
put beside one another; i.e., common parts shared queries.
computational redundancy executing pack executing
queries one another; main difference case queries
compiled.
3. Packed execution packs: compiled query pack executed queries share
much possible.
interesting information obtained comparing (a) actual query evaluation time settings 2 3: gives view eciency gain obtained
removal redundant computation (we abbreviate exec tables);
(b) total execution time settings 1 3: provides indication
much gained implementing packs ILP system, taking effects account (re-implementation computation heuristics via bit matrix, use compiled
queries instead meta-calls, etc.), words: net effect whole
re-implementation (indicated net tables).
first experiment used Bongard problems, varying (1) size data sets;
(2) complexity target hypothesis; (3) Tilde's lookahead parameter.
complexity target hypothesis small, medium, none. latter case
examples random, causes Tilde grow ever larger trees attempt find
good hypothesis; size final tree typically depends size data
set. lookahead parameter used control number levels pack contains;
lookahead n, packs depth n + 1 generated.
Table 1 gives overview results Bongard problems. total induction
time reported, well (for pack-based execution mechanisms) time needed
pack compilation pack execution. Note total time includes pack
compilation execution, also computations directly related packs
(e.g., computation heuristics bitmatrix). results interpreted
follows.
First all, table shows significant speedups obtained using pack
mechanism; net speedups factor 5.5 obtained, execution
75 times faster compared disjoint execution.
observation complex target hypotheses greater speedups
obtained. explained broom-like form packs Tilde. Complex
target hypotheses correspond deep trees, refinement node lower level
tree yields pack long clause branching, accordance
previous analysis yield speedup closer branching factor b case
lookahead 0 (and generally, closer bl+1 lookahead l, although latter
much harder achieve). Note maximum branching factor occurring pack
included table column bf .
Finally, deeper packs also yield higher speedups, effect larger complex
theories. understandable considering following. Let us call clause
refined c. lookahead l, conjunctions l + 1 literals added clause.
cases first l + 1 literals may fail immediately, causes branch
pack almost execution time, cutting away bl queries. Remember

155

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
LA

bf

0
1
2
3

16
24
18
21

0
1
2
3

16
24
18
21

0
1
2
3

19
24
18
21

0
1
2
3

19
21
15
18

0
1
2
3

22
24
27
18

0
1
2
3

25
24
27
27

0
1
2
3

28
24
24
30

0
1
2
3

31
36
33
33

0
1
2
3

31
39
39
42

original

0.74
2.44
7.49
29.9

1.82
5.72
17.2
69.8

3.69
11.4
34.7
142

1.01
3.26
6.36
27.2

3.16
8.38
38.5
124

6.35
18.14
119
384

4.74
16.32
87.5
373

12.7
65.1
430
1934

25.3
154
1185
4256

disjoint
packed
comp exec total comp
Simple target hypothesis
1007 examples
0.62
0.14
0.13
0.49
0.05
1.64
0.35
0.45
1.09
0.14
4.07
0.8
1.57
2.15
0.27
16.52
3.65
7.26
7.18
1.26
2473 examples
1.43
0.17
0.34
1.13
0.07
3.34
0.34
1.17
2.24
0.11
8.45
0.78
3.95
4.4
0.27
33.0
3.57
17.5
13.7
1.13
4981 examples
2.72
0.29
0.67
2.16
0.12
6.22
0.35
2.41
4.17
0.13
16.0
0.74
8.14
8.24
0.25
62.4
3.61
36.5
24.9
1.09
Medium complexity target hypothesis
1031 examples
0.93
0.29
0.18
0.66
0.11
2.8
0.98
0.56
1.66
0.35
3.47
0.68
1.22
1.95
0.25
14.6
3.75
5.75
6.71
1.20
2520 examples
2.82
0.89
0.62
1.91
0.3
5.88
1.5
1.86
3.3
0.44
29.8
13.14 9.52
10.3
2.44
58.02
10.3
28.6
23.9
3.00
5058 examples
5.41
1.47
1.3
3.73
0.56
12.98
3.2
4.15
7.5
0.93
93.2
38.1
31.0
35.3
9.09
275
108
89.1
106
25.9
target hypothesis
1194 examples
6.65
3.34
0.94
3.93
0.98
21.29
10.97 2.24 11.65 3.41
130
82.3
13.8
54.7
20.4
519
316
61.1
220
74.9
2986 examples
16.5
7.04
2.68
9.8
2.16
83.7
42.9
10.7
42.47
11.2
606
396
84
211.3
82.58
2592
1610
375
946
332
6013 examples
30.3
11.8
5.53
18.3
3.53
198
91.2
33.4
99.9
22.0
1733
1076
358
504
197
6932
4441 1091 2006
695
total

speedup

exec

net

exec

0.07

1.51

1.86

0.11

2.24

4.09

0.16

3.48

9.81

0.28

4.17

25.9

0.16

1.61

2.13

0.3

2.55

3.9

0.39

3.92

10.1

0.69

5.11

25.4

0.32

1.71

2.09

0.63

2.74

3.83

0.88

4.21

9.25

1.45

5.69

25.1

0.07

1.53

2.57

0.14

1.96

4

0.15

3.26

8.13

0.27

4.06

21.3

0.24

1.65

2.58

0.41

2.54

4.54

0.6

3.73

15.9

1.11

5.21

25.7

0.53

1.70

2.45

0.91

2.42

4.56

1.7

3.36

18.2

2.83

3.62

31.5

0.20

1.21

4.70

0.31

1.40

7.23

0.57

1.60

24.1

1.34

1.70

45.6

0.56

1.30

4.79

1.14

1.53

9.39

2.57

2.03

32.6

6.58

2.04

57.0

1.27

1.38

4.35

3.13

1.54

10.7

9

2.35

39.8

14.5

2.12

75.4

Table 1: Timings Tilde runs Bongard data sets. LA = lookahead setting; bf =
maximum branching factor. Reported times (in seconds) total time needed
build tree, time spent compilation respectively execution packs.

156

fiImproving Efficiency ILP Query Packs
LA original
0
1
2

31.5
194.99
2193

0
1
2

27.6
38.02
638

disjoint
packed
total comp exec total comp
Regression, 230 examples
52.9 1.96 25.5 45.5 1.02
248 55.9 109 107 12.6
{
{
{
891
192
Classification, 230 examples
27.3 1.83 4.71 25.4 1.13
40.3 7.55 9.09 30.6 3.11
{
{
{
149 74.3

Table 2: Timings Tilde runs Mutagenesis.
ended prematurely.

exec

speedup ratio
net
exec

19.25 0.69
16.6 1.82
32.0 2.46

1.33
6.53
{

3.42
3.65
6.16

1.38
2.49
{

1.09
1.24
4.2

table indicates run

according analysis, speedup limit approximate bl complexity
clause c dominates complexity rest pack; \early failing branches"
pack cause actual situation approximate closer ideal case.
also run experiments Mutagenesis data set (Table 2), regression
classification setting. Here, query packs much larger Bongard data set
(there higher branching factor); lookahead 2 largest packs 20000
queries. large packs significant amount time spent compiling pack,
even clear net speedups obtained.5 comparison execution times turned
infeasible disjoint execution setting pack structures consumed much
memory.

5.2 Warmr
5.2.1 Used Implementations

Warmr consider following implementations:
1. packs: normal implementation Warmr, queries generated,
examples queries evaluated one one.
2. packs: implementation first queries one level generated
put pack, pack evaluated example.
5.2.2 Datasets

Mutagenesis used Mutagenesis dataset 230 molecules, example
repeated 10 times make accurate timings possible better idea
effect larger datasets. used three different language biases. 'small' language
5. one case, relatively small pack, system became slower. timings indicate
due compilation time, changes implementation relatively
simple problem compensated faster execution packs.

157

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele

Level
1
2
3
4
5
6
7
8
9

small

Mutagenesis

medium

large

Queries Frequent Queries Frequent Queries Frequent
8
5
37
26
45
31
60
14
481
48
1071
211
86
24
688
114
3874
1586
132
31
699
253
37
21
697
533
29
18
1534
1149
23
15
{
{
17
12
{
{
4
4
{
{

Table 3: Number queries Mutagenesis experiment Warmr.
bias chosen generate limited number refinements (i.e., relatively
small branching factor search lattice); allows us generate query packs
relatively deep narrow. 'medium' 'large' use broader shallow packs.
Table 3 summarises number queries number frequent queries found
level different languages.

Bongard use Bongard-6013 experiments

Warmr system
construct theory hence existence simple theory expected make much
difference.
5.2.3 Results

Tables 4, 5 6 execution times Warmr Mutagenesis given, maximal
search depth varying 3 large language 9 levels small language. Here,
'total' total execution time 'exec' time needed test queries
examples. Table 7 execution times Warmr Bongard given.
5.2.4 Discussion

execution time Warmr large component used evaluate queries.
caused fact Warmr needs lot administrative work.
particular, theta-subsumption tests done queries check wether query
equivalent another candidate, query specialisation infrequent one.
propositional case (the Apriori algorithm), tests simple,
first order case require exponential time size queries. course,
using larger datasets, relative contribution administrative costs decrease
proportionally. observed deeper levels, costs less setting
using packs. One causes fact no-packs version also uses memory
packs setting (and hence causes proportionally memory management).
again, important numbers speedup factors execution
queries. Speedup factors query execution always increase increasing depth

158

fiImproving Efficiency ILP Query Packs

Level
1
2
3
4
5
6
7
8
9

packs
packs ilProlog
total
exec
total
exec
0.35
0.23
0.18
0.15
6.27
5.60
4.56
4.12
36.93
31.49 14.01
9.87
117.33 84.45
45.14
16.27
215.95 104.36 129.37
20.78
336.35 111.28 249.41
22.39
569.14 115.80 497.86
24.63
902.72 120.99 831.30
25.98
1268.16 119.60 1148.23
32.28

speedup ratio
net
exec
1.94 1.53
1.38 1.36
2.64 3.19
2.60 5.19
1.67 5.02
1.35 4.97
1.14 4.70
1.09 4.66
1.10 3.71

Table 4: Results Warmr Mutagenesis dataset using small language.

Level
1
2
3
4
5
6

packs
packs ilProlog
total
exec
total
exec
2.58
2.27
2.16
2.09
112.98
42.32
34.35
13.39
735.19 128.67 262.83
34.70
4162.15 287.72 1476.06
54.10
17476.98 444.44 6870.16
73.11
65138.72 866.85 25921.73
104.81

speedup ratio
net
exec
1.19 1.09
3.29 3.16
2.80 3.71
2.82 5.32
2.54 6.08
2.51 8.27

Table 5: Results Warmr Mutagenesis dataset using medium language.

Level
1
2
3

packs
packs ilProlog
total
exec
total
exec
2.82
2.42
2.28
2.11
408.85
102.38 102.29
50.67
27054.33 1417.76 3380.19
370.44

speedup ratio
net
exec
1.24 1.15
4.00 2.02
8.00 3.83

Table 6: Results Warmr Mutagenesis dataset using large language.

159

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
Level
1
2
3
4
5
6
7
8
9
10

packs
total
exec
0.24
0.22
0.83
0.75
3.28
2.82
11.56 9.31
38.34 28.11
75.51 46.97
135.64 71.60
186.23 84.93
210.82 88.97
216.61 89.38
Table 7:

packs ilProlog
total
exec
0.24
0.23
0.77
0.68
2.34
1.92
6.08
4.28
16.20
8.15
36.57
12.22
68.96
15.59
102.46
17.82
120.76
18.52
125.84
18.88

Warmr

speedup ratio
net
exec
1.00 0.96
1.08 1.10
1.40 1.47
1.90 2.18
2.37 3.45
2.06 3.84
1.97 4.59
1.82 4.77
1.75 4.80
1.72 4.73

results Bongard.

packs, contrast Tilde larger packs yielded higher speedups. first sight
found surprising; however becomes less following observation made.
refining pack new pack adding level, Warmr prunes away branches
lead infrequent queries. thus two effects adding level pack:
one widening pack lowest level (at least first levels, new
pack typically leaves previous one), second narrowing
pack whole (because pruning). Since speedup obtained using packs largely
depends branching factor pack, speedup factors expected decrease
narrowing effect stronger widening-at-the-bottom effect.
seen, e.g, small-mutagenesis experiment, deepest levels queries
becoming less frequent. mutagenesis experiment medium size language,
query execution speedup factors larger number queries increases much faster.
mutagenesis experiment large language, total speedup large,
language generates many queries time-consuming part becomes
administration storage memory. packs version much faster stores
queries trees, requiring significantly less memory.

5.3 Comparison Engines
Implementing new special-purpose Prolog engine, different already existing ones,
carries risk: given level sophistication popular Prolog engines, useful check
whether new engine performs comparably existing engines, least
tasks consideration here. eciency gain obtained query pack execution
offset less ecient implementation engine itself.
Originally Tilde Warmr systems implemented MasterProLog.
attempt allow run platforms, parts systems reimplemented kind \generic" Prolog implementations specific Prolog engines (SICStus, ilProlog) easily derived (the low level standardisation
Prolog made necessary). Given situation, two questions answered:

160

fiImproving Efficiency ILP Query Packs
Data set
LA
Bongard-1194 0
Bongard-2986 0
Bongard-6013 0
Bongard-1007 0
Bongard-2473 0
Bongard-4981 0
Bongard-1007 2
Bongard-2473 2
Bongard-4981 2
Table 8:

MasterProLog ilProlog(original) ilProlog(packs)
7.8
17.8
35
0.77
2.07
4.1
7.1
17.7
38

4.74
12.7
25
0.74
1.82
3.7
7.5
17.2
35

3.93
9.8
18
0.49
1.13
2.2
2.2
4.4
8.2

compared engines (times seconds) several data sets
lookahead settings.

ilProlog

(a) move MasterProLog Prolog engines uence performance
negative way; (b) performance loss, any, reduce performance improvements due use packs?
Tilde Warmr tuned fast execution MasterProLog ilProlog SICStus, makes comparison latter unfair; therefore
report former 2 engines. Table 8 shows results. confirm
ilProlog competitive state-of-the-art Prolog engines.

5.4 Summary Experimental Results
experiments confirm (a) query pack execution much ecient
executing many highly similar queries separately; (b) existing ILP systems (we use Tilde
Warmr examples) use mechanism advantage, achieving significant
speedups; c) although new Prolog engine needed achieve this, current state
development engine respect execution speed compete
state-of-the-art engines. Further, experiments consistent complexity
analysis execution time packs.
6. Related Work

re-implementation Tilde related work Mehta et al. (1996)
first describe \examples outer loop" strategy decision tree induction.
query pack execution mechanism, described Prolog execution point view,
also seen first-order counterpart Apriori's mechanism counting item-sets
(Agrawal et al., 1996).
lines work eciency improvements ILP involves stochastic methods
trade certain amount optimality eciency by, e.g., evaluating clauses
sample data set instead full data set (Srinivasan, 1999), exploring clause
search space random fashion (Srinivasan, 2000), stochastically testing whether

161

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
query succeeds example (Sebag & Rouveirol, 1997). first entirely
orthogonal query pack execution easily combined it.
idea optimising sets queries instead individual queries existed
database community. typical context considered earlier research
multi-query optimisation (e.g., Sellis, 1988) database system needs
handle disjunctions conjunctive queries, server may receive many queries
different clients brief time interval. several queries expected compute
intermediary relations, may ecient materialise relations
instead recomputed query. Data mining provides sense new
context multi-query optimisation, multi-query optimisation approach
time easier (the similarities among queries systematic, one need
look them) promising (given huge number queries may
generated once).
Tsur et al. (1998) describe algorithm ecient execution so-called query ocks
context. Like query pack execution mechanism, query ock execution mechanism inspired extent Apriori set deductive database setting.
main difference query packs query ocks described Tsur et al.
(1998) query packs hierarchically structured queries pack
structurally less similar queries ock. (A ock represented single query
placeholders constants, equal set queries obtained
instantiating placeholders constants. Flocks could used applications
consider here.)
Dekeyser Paredaens (2001) describe work multi-query optimisation context
relational databases. also consider tree-like structures multiple queries
combined; main difference trees rooted one single table
queries select tuples, whereas queries correspond joins multiple tables. Further,
Dekeyser Paredaens define cost measure trees well operators map trees
onto semantically equivalent (but less costly) trees, whereas considered
creation packs ecient top-down execution mechanism them. Combining
approaches seems interesting topic research.
Finally, optimisation techniques ILP proposed exploit results
program analysis (Santos Costa et al., 2000; Blockeel et al., 2000) propositional
data mining technology (Blockeel et al., 1999). complementary pack
execution optimisation. Especially approach Blockeel et al. (1999) easily
combined pack mechanism. techniques discussed Santos Costa et al.
(2000) Blockeel et al. (2000) involve optimisations single query execution,
extent upgraded pack setting. future work.
7. Conclusions

lot redundancy computations performed ILP systems.
paper identified source redundancy proposed method avoiding it:
execution query packs. discussed query pack execution incorporated
ILP systems. query pack execution mechanism implemented new
Prolog system called ilProlog dedicated data mining tasks, two ILP systems

162

fiImproving Efficiency ILP Query Packs
re-implemented make use mechanism. experimentally evaluated
re-implementations, results experiments confirm large speedups
may obtained way. conjecture query pack execution mechanism
incorporated ILP systems similar speedups expected.
problem setting query pack execution introduced general,
allows technique used kind task many queries executed
data, long queries organised hierarchy.
Future work includes improvements ilProlog engine implementation techniques increase suitability engine handle large data sets.
best case one might hope combine techniques known database optimisation
program analysis pack execution mechanism improve speed
ILP systems.

Acknowledgements
Hendrik Blockeel post-doctoral fellow Fund Scientific Research (FWO)
Flanders. Jan Ramon funded Flemish Institute Promotion Scientific
Research Industry (IWT). Henk Vandecasteele funded part FWO project
G.0246.99, \Query languages database mining". authors thank Luc De Raedt
uence work, Ashwin Srinivasan suggesting term \query packs",
anonymous reviewers useful comments, Kurt Driessens proofreading
text. work motivated part Esprit project 28623, Aladin.
Appendix A. Preparing Query Meta-interpreter

Note following preprocessor assumes pack form a, (b, (c
e) f g, (h j)) already transformed form , or([(b,
or([c,d,e])), f, (g, or([h,i,j]))]).
preprocess((A,B),(A,NewB),PrevNode,NodeNr0,LeafNr0,BranchNr,NodeNr1,LeafNr1):- !,
preprocess(B,NewB,PrevNode,NodeNr0,LeafNr0,BranchNr,NodeNr1,LeafNr1).
preprocess(or(Querys),or(NQuerys,PrevNode,NodeNr0,BranchNr,Length),
PrevNode,NodeNr0,LeafNr0,BranchNr, NodeNr1,LeafNr1):- !,
NodeNr2 NodeNr0 + 1,
preprocessbranches(Querys,NQuerys,NodeNr0,NodeNr2,LeafNr0,
1,NodeNr1,LeafNr1,Length).
preprocess(A,(A,leaf(PrevNode,BranchNr,LeafNr0)),
PrevNode,NodeNr0,LeafNr0, BranchNr,NodeNr0,LeafNr1):LeafNr1 LeafNr0 + 1.
preprocessbranches([],[], ,NodeNr,LeafNr,BranchNr, NodeNr,LeafNr,BranchNr).
preprocessbranches([QueryjQuerys],[NewQueryjNewQuerys],PrevNode,
NodeNr0,LeafNr0,BranchNr, NodeNr1,LeafNr1,Length):preprocess(Query,NewQuery,
PrevNode,NodeNr0,LeafNr0,BranchNr, NodeNr2,LeafNr2),
BranchNr1 BranchNr + 1,
preprocessbranches(Querys,NewQuerys, PrevNode,
NodeNr2,LeafNr2,BranchNr1, NodeNr1,LeafNr1,Length).

163

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele

References

Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., & Verkamo, A. (1996). Fast discovery
association rules. Fayyad, U., Piatetsky-Shapiro, G., Smyth, P., & Uthurusamy,
R. (Eds.), Advances Knowledge Discovery Data Mining, pp. 307{328. MIT
Press.
At-Kaci, H. (1991). Warren's Abstract Machine: Tutorial Reconstruction. MIT
Press, Cambridge, Massachusetts.
http://www.isg.sfu.ca/~hak/documents/wam.html.
Blockeel, H. (1998). Top-down induction first order logical decision trees. Ph.D. thesis,
Department Computer Science, Katholieke Universiteit Leuven.
http://www.cs.kuleuven.ac.be/~ml/PS/blockeel98:phd.ps.gz.
Blockeel, H., & De Raedt, L. (1997). Lookahead discretization ILP. Proceedings
Seventh International Workshop Inductive Logic Programming, Vol. 1297
Lecture Notes Artificial Intelligence, pp. 77{85. Springer-Verlag.
Blockeel, H., & De Raedt, L. (1998). Top-down induction first order logical decision trees.
Artificial Intelligence, 101 (1-2), 285{297.
Blockeel, H., De Raedt, L., Jacobs, N., & Demoen, B. (1999). Scaling inductive logic programming learning interpretations. Data Mining Knowledge Discovery,
3 (1), 59{93.
Blockeel, H., De Raedt, L., & Ramon, J. (1998). Top-down induction clustering trees.
Proceedings 15th International Conference Machine Learning, pp. 55{63.
http://www.cs.kuleuven.ac.be/~ml/PS/ML98-56.ps.
Blockeel, H., Demoen, B., Janssens, G., Vandecasteele, H., & Van Laer, W. (2000). Two
advanced transformations improving eciency ILP system. 10th
International Conference Inductive Logic Programming, Work-in-Progress Reports,
pp. 43{59, London, UK.
Bongard, M. (1970). Pattern Recognition. Spartan Books.
Bratko, I. (1990). Prolog Programming Artificial Intelligence. Addison-Wesley, Wokingham, England. 2nd Edition.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification Regression
Trees. Wadsworth, Belmont.
Chen, W., & Warren, D. S. (1996). Tabled evaluation delaying general logic programs. Journal ACM, 43 (1), 20{74. http://www.cs.sunysb.edu/~sbprolog.
Clark, P., & Niblett, T. (1989). CN2 algorithm. Machine Learning, 3 (4), 261{284.
De Raedt, L. (1997). Logical settings concept learning. Artificial Intelligence, 95, 187{
201.
De Raedt, L., & Dehaspe, L. (1997). Clausal discovery. Machine Learning, 26, 99{146.

164

fiImproving Efficiency ILP Query Packs
De Raedt, L., & Dzeroski, S. (1994). First order jk-clausal theories PAC-learnable.
Artificial Intelligence, 70, 375{392.
De Raedt, L., & Van Laer, W. (1995). Inductive constraint logic. Jantke, K. P., Shinohara, T., & Zeugmann, T. (Eds.), Proceedings Sixth International Workshop
Algorithmic Learning Theory, Vol. 997 Lecture Notes Artificial Intelligence, pp.
80{94. Springer-Verlag.
Dehaspe, L., & Toivonen, H. (1999). Discovery frequent datalog patterns. Data Mining
Knowledge Discovery, 3 (1), 7{36.
Dekeyser, S., & Paredaens, J. (2001). Query pack trees multi query optimization. Tech.
rep. 01-04, University Antwerp. ftp://wins.uia.ac.be/pub/dekeyser/qpt.ps.
Demoen, B., Janssens, G., & Vandecasteele, H. (1999). Executing query flocks ILP.
Etalle, S. (Ed.), Proceedings Eleventh Benelux Workshop Logic Programming,
pp. 1{14, Maastricht, Netherlands. 14 pages.
Kramer, S. (1996). Structural regression trees. Proceedings Thirteenth National
Conference Artificial Intelligence, pp. 812{819, Cambridge/Menlo Park. AAAI
Press/MIT Press.
Mehta, M., Agrawal, R., & Rissanen, J. (1996). SLIQ: fast scalable classifier data
mining. Proceedings Fifth International Conference Extending Database
Technology.
Muggleton, S. (1995). Inverse entailment Progol. New Generation Computing, Special
issue Inductive Logic Programming, 13 (3-4), 245{286.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming : Theory methods.
Journal Logic Programming, 19,20, 629{679.
Quinlan, J. R. (1993a). C4.5: Programs Machine Learning. Morgan Kaufmann series
machine learning. Morgan Kaufmann.
Quinlan, J. (1993b). FOIL: midterm report. Brazdil, P. (Ed.), Proceedings 6th
European Conference Machine Learning, Lecture Notes Artificial Intelligence.
Springer-Verlag.
Santos Costa, V., Srinivasan, A., & Camacho, R. (2000). note two simple transformations improving eciency ILP system. Proceedings Tenth
International Conference Inductive Logic Programming, Vol. 1866 Lecture Notes
Artificial Intelligence, pp. 225{242. Springer-Verlag.
Sebag, M., & Rouveirol, C. (1997). Tractable Induction Classification First-Order
Logic via Stochastic Matching. Proceedings 15th International Joint Conference Artificial Intelligence. Morgan Kaufmann.
Sellis, T. (1988). Multiple-query optimization. ACM Transactions Database Systems,
13 (1), 23{52.
Srinivasan, A. (1999). study two sampling methods analysing large datasets
ILP. Data Mining Knowledge Discovery, 3 (1), 95{123.
Srinivasan, A. (2000). study two probabilistic methods searching large spaces
ILP. Tech. rep. PRG-TR-16-00, Oxford University Computing Laboratory.

165

fiBlockeel, Dehaspe, Demoen, Janssens, Ramon, & Vandecasteele
Srinivasan, A., Muggleton, S., & King, R. (1995). Comparing use background knowledge inductive logic programming systems. De Raedt, L. (Ed.), Proceedings
Fifth International Workshop Inductive Logic Programming.
Tsur, D., Ullman, J., Abiteboul, S., Clifton, C., Motwani, R., Nestorov, S., & Rosenthal, A.
(1998). Query ocks: generalization association-rule mining. Proceedings
ACM SIGMOD International Conference Management Data (SIGMOD-98),
Vol. 27,2 ACM SIGMOD Record, pp. 1{12, New York. ACM Press.

166

fiJournal Artificial Intelligence Research 16 (2002) 59-104

Submitted 5/01; published 2/02

Accelerating Reinforcement Learning Composing
Solutions Automatically Identified Subtasks
Chris Drummond

School Information Technology Engineering
University Ottawa, Ontario, Canada, K1N 6N5

cdrummon@site.uottawa.ca

Abstract

paper discusses system accelerates reinforcement learning using transfer
related tasks. Without transfer, even two tasks similar
abstract level, extensive re-learning effort required. system achieves much
power transferring parts previously learned solutions rather single complete
solution. system exploits strong features multi-dimensional function produced
reinforcement learning solving particular task. features stable easy
recognize early learning process. generate partitioning state space
thus function. partition represented graph. used index
compose functions stored case base form close approximation solution
new task. Experiments demonstrate function composition often produces
order magnitude increase learning rate compared basic reinforcement
learning algorithm.

1. Introduction
standard reinforcement learning algorithm, applied series related tasks, could learn
new task independently. requires knowledge present state infrequent
numerical rewards learn actions necessary bring system desired goal
state. paucity knowledge results slow learning rate. paper shows
exploit results prior learning speed process maintaining
robustness general learning method.
system proposed achieves much power transferring parts previously
learned solutions, rather single complete solution. solution pieces represent
knowledge solve certain subtasks. might call macro-actions (Precup,
Sutton, & Singh, 1997), obvious allusion macro-operators commonly found
Artificial Intelligence systems. main contribution work providing way
automatically identifying macro-actions mapping new tasks.
work uses syntactic methods composition much like symbolic planning,
novelty arises parts composed multi-dimensional real-valued functions.
functions learned using reinforcement learning part complex functions
associated compound tasks. ecacy approach due composition
occurring suciently abstract level, much uncertainty removed.
function acts much like funnel operator (Christiansen, 1992), although individual
actions may highly uncertain, overall result largely predictable.
c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDrummond

subtasks identified basis strong features multi-dimensional
function arise reinforcement learning. features \in world",
system's interaction world. Here, \strong" means features
stable (i.e. relatively insensitive variations low level learning process) easy
recognize locate accurately early learning process. One important aspect
features largely dictate shape function. features differ
small amount, one would expect function differ small amount.
features generate partitioning function. popular technique object
recognition, snake (Kass, Witkin, & Terzopoulus, 1987; Suetens, Fua, & Hanson, 1992),
used produce partition. object recognition, snake produces closed curve
lies along boundary object, defined edges image. application, snake groups together sets features define region function.
boundary region low order polygon, demarcating individual subtask.
repeated whole function covered. polygons converted discrete
graphs, vertex polygon becoming node graph. Merging graphs
produces composite graph representing whole task.
composite graph used control transfer accessing case base previously
learned functions. case base indexed graphs. relevant function determined
matching subgraph composite graph one acting index case.
associated functions transformed composed form solution new task.
used reinitialize lower level learning process. necessary transfer
produce exact solution new task. sucient solution close enough
final solution often enough produce average speed up. Reinforcement learning
refine function quickly remove error.
paper demonstrates applicability transfer two different situations.
first, system learns task particular goal position goal moved.
Although function change significantly, partition generated initial
task used compose function new task. second situation considered, system placed different environment within domain. Here, new
partition extracted control composition process.
paper unifies significantly extends previous work author (Drummond,
1997, 1998). Additional work largely focussed removing limitations
inherent partitioning approach introduced Drummond (1998). One limitation
original approach snake could extract polygons rectangles.
paper relaxes restriction, allowing applied different environment
within domain different task domain. Although lifting restriction
removes desirable bias, experiments demonstrate none ecacy
original system lost. Further, results broadly obtained larger set
related tasks different domain. Overall, function composition approach often
produces order magnitude increase learning rate compared
basic reinforcement learning algorithm.
rest paper begins Section 2 giving high level discussion
approach taken. Section 3 gives depth discussion techniques used. Sections 4 5 present analyze experimental results. Subsequent sections deal
limitations related research.
60

fiAccelerating Reinforcement Learning

2. Overview
intent section appeal intuitions reader, leaving much
detail later sections paper. subsections follow demonstrate turn:
features function produced reinforcement learning; graphs
based features used control composition function pieces;
features easy detect early learning process; features exist
multiple domains.

2.1 Features Reinforcement Learning Function
overview begins high level introduction reinforcement learning
function produces. show features function
extracted converted graphical representation.
One experimental test beds used paper simulated robot environment
different configurations interconnected rooms. robot must learn navigate eciently
rooms reach specified goal start location. Figure 1 shows one
example 5 rooms goal top right corner. robot's actions small
steps eight directions, indicated arrows. Here, location, state,
simply robot's x coordinates. thin lines Figure 1 walls
rooms, thick lines boundary state space.
+1
Goal

Robot
10



11

11
12

12
13

13

14

-1
X
-1

+1

Figure 1: Robot Navigating Series Rooms
61

fiDrummond

action independent preceding actions, task becomes one learning
best action state. best overall action would one takes robot
immediately goal. possible states close goal. Suppose
robot particular state number steps goal
neighboring states known, indicated numbered squares surrounding robot
Figure 1. one step look ahead procedure would consider step select
one reaches neighboring state shortest distance goal. Figure 1
robot would move state 10 steps goal. process repeated, robot
take shortest path goal. practice must, course, learn values.
done using type reinforcement learning (Watkins & Dayan, 1992; Sutton, 1990)
progressively improves estimates distance goal state
converge correct values.
(-1.0,1.0)

(0.25,1.0)

(0.25,0.9)


(-1.0,0.25)

(-0.9,0.25)
(0.25,0.25)

Figure 2: Value Function Obtained Using Reinforcement Learning
function shown Figure 2 called value function. Subsequently, term
function mean value function unless otherwise indicated. function result
reinforcement learning problem Figure 1, instead representing
actual distance goal, represents essentially exponential decay distance goal.
reasons made clear Section 3.1. shaded areas represent large
gradients learned function. Comparing environment shown Figure 1,
apparent correspond walls various rooms. strong
features discussed paper. exist extra distance robot
travel around wall reach inside next room path goal.
features visually readily apparent human, seems natural use vision
processing techniques locate them.
edge detection technique called snake used locate features. snake
produces polygon, instance rectangle, locating boundary room.
doorways room occur differential function, along body
snake, local minimum. direction differential respect edges
62

fiAccelerating Reinforcement Learning

polygon, associated walls room, determines entrance
exit. positive gradient room indicates entrance; positive gradient
room indicates exit. information, plane graph, labeled (x; y)
coordinate node, constructed. Figure 2 shows one example, room
top left corner state space, subsequent graphs show coordinates.
Nodes corresponding doorways labeled \I" \O" respectively;
positions function indicated dashed arrows.

2.2 Composing Function Pieces

overview continues showing graphs, extracted features
function learned reinforcement learning, used produce good approximation
solution new goal position. left hand side Figure 3 shows plane graphs
rooms (ignore dashed lines circles now). node representing
goal labeled \G". directed edge added \I" \O" \I" \G", appropriate.
Associated edge number representing distance nodes.
determined value function points doorways. individual
graph merged neighbor produce graph whole problem, right
hand side Figure 3. doorway nodes relabeled \D". composite
graph represents whole function. individual subgraph represents particular part
function. information stored case base. subgraph index
corresponding part function case.



G



Extract
Graphs

G



Merge
Graphs

G





G












Figure 3: Graphical Representation
suppose goal moved top right corner top left corner
state space. Reinforcement learning basic form would required learn
new function scratch. work goal moved, new goal position
63

fiDrummond

known, node representing goal relocated. new goal position shown
dashed circle Figure 3. edges connecting doorways goal
changed account new goal position. dashed lines representing new edges
replace arrows subgraph. produce new function, idea regress
backwards goal along edges. edge, small subgraph containing
edge extracted. extracted subgraph used index case base functions.
retrieved function transformed added appropriate region state space
form new function.
Rotate
Stretch

Rotate


G

Stretch



Figure 4: Function Composition
example, existing subgraphs match new configuration. two
subgraph originally containing goal subgraph containing
goal. certainly possible exchange two, using appropriate transform.
graphs case base may better match new task. best match
subgraph containing new goal is, fact, subgraph goal original
problem. fit new task, plane graph rotated stretched slightly
new x direction changing coordinates nodes, see Figure 4.
transformation applied function. room containing original goal,
case obtained solving another task better match. three rooms use
functions original problem, since changing goal position little effect
actions taken. fact, height functions must changed. simply
multiplication value representing distance goal \O" doorway (this
discussed detail end Section 3.3). matching subgraphs
allows error asymmetric scaling may used, resulting function may
exact. experiments demonstrate, function often close
reinforcement learning quickly correct error.
64

fiAccelerating Reinforcement Learning

new position goal must established graph modified
function composition occur. system told goal moved, rather
discovers determining longer maximum existing function.
uncertainty exact boundary original goal. robot may reach
state believes part original goal region, fail detect even
goal moved. reasonably certain goal fact moved,
required occur ten times intervening occurrence goal detected
maximum.
system composes search function, assuming particular room contains
goal. Search functions also produced composing previously learned functions.
However, room assumed contain goal function constant.
bias search particular part room allows limited learning
encourage exploration room. search function drives robot room
anywhere else state space. fails find goal fixed number steps,
new search function composed another room assumed contain goal.
process repeated goal located ten times, ensures good estimate
\center mass" goal. \center mass" used new position
goal node composite graph. Requiring old goal new goal positions
sampled fixed number times proven effective domains discussed
paper. Nevertheless, somewhat ad hoc procedure addressed future
work, discussed Section 6.2.

2.3 Detecting Features Early
previous section, existing task new task strongly related, walls
doorways fixed goal position different. section,
relationship assumed. robot faced brand new task must determine
what, any, relationship exists new task previous tasks.
experimental testbed simulated robot environment, time
problem simplified inner rectangular room outer L-shaped room. Figures
5 6 show two possible room configurations. Again, thin lines walls
room, thick lines boundary state space. Suppose robot already
learned function \Old Task" Figure 5. would hope could adapt
old solution fit closely related \New task" Figure 6.
steps, example, essentially previous one.
learning process started afresh, features system must wait
emerge normal reinforcement learning process. proceed much
before. First graph inner room extracted. best matching graph
case base old task rotated stretched fit new task. Next matching
graph outer L-shaped room rotated stretched around larger inner room.
transforms applied associated functions, height adjustments
carried functions composed form approximate solution new task.
example, first step process locate goal.
partition aid search, initial value function set mid-range constant value
(see Figure 7). allows limited learning encourages system move
65

fiDrummond

Goal

Robot

Outer
Room

Inner
Room

Robot

Inner
Room

Outer
Room

Goal

Figure 5: Old Task

Figure 6: New Task

away regions explored previously, prevent completely random walk
state space. goal located, learning algorithm reinitialized function
goal position walls (see Figure 8). function exist
case base, rough approximation could used instead. \no walls" function
used exactly stored case base. difference goal rest
state space reduced scaling function adding constant. reduces
\bias" function, allowing learning algorithm alter relatively easily new
information becomes available.

Figure 7: Start Function

Figure 8: Intermediate Function

Figure 9 shows resultant function 3000 exploratory steps beginning
learning process. Again, large gradients associated walls readily
66

fiAccelerating Reinforcement Learning

apparent. Figure 10 shows function new task allowed converge
good solution. functions roughly form, large gradients
position, although learning latter took 200,000 steps. \no
walls" function introduced features take time clearly emerge. snake
typically filter features small well formed. Additional filtering
graphical level constrains acceptable features. total set features must
produce consistent composite graph, doorways different subgraphs must align
graph must overlay complete state space. must also matching case
case base every subtask. Many checks balances removed
iterative updating technique Section 6.2 incorporated.

Figure 9: Early Function

Figure 10: New Task Function

2.4 Different Task Domain

previous sections dealt simple robot navigation problem. section demonstrates features also exist quite different domain, two degrees
freedom robot arm, shown Figure 11. shoulder joint achieve angle radians, elbow joint angle =2 radians, zero indicated
arrows. arm straight shoulder joint rotated, elbow joint describe
inner dotted circle, hand outer dotted circle. eight actions, small
rotations either clockwise anti-clockwise joint separately together. aim
learn move arm eciently initial position hand reaches
goal perimeter arm's work space.
state space, purposes reinforcement learning, configuration space
arm, sometimes called joint space (see Figure 12). x-axis angle
shoulder joint, y-axis elbow joint. eight actions mapped actions
configuration space become much like actions robot navigation problem, shown
shaded diamond (labeled Arm) Figure 12. map obstacle work space
configuration space, one must find pairs shoulder elbow angles blocked
obstacle. obstacles space become elongated form barriers much like
67

fiDrummond

+/2

Shoulder

Hand

0

Elbow Angle

Obstacle

00
11
11
00
00
11
00
11
11
00

G


L

1111111
0000000
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111

Arm

0

Obstacle

Elbow
0

/2


Obstacle
00
11
11
00
00
11
00
11
11
00

Figure 11: Work Space

0000000
1111111
1111111
0000000
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111

Obstacle

G


L

0
Shoulder Angle

+

Figure 12: Configuration Space

walls experiments previous sections. clear, imagine straightening
arm work space rotating intersects one obstacles,
middle dotted line Figure 11. arm rotated shoulder joint
roughly linearly proportional rotation elbow joint, opposite direction,
keep intersecting obstacle. produces \wall" configuration space.
linearity holds small objects far perimeter work space.
complex, larger objects, would result complex shapes configuration
space. moment feature extraction method limited simpler shapes,
discussed Section 6.
reinforcement learning function produced problem shown Figure 13.
features shaded clarity. large gradient associated obstacle
left hand side configuration space clearly seen. similar large
gradient associated obstacle right hand side configuration space.
Again, features used control composition functions goal
moved different task domain.

3. Details Techniques Used
section discuss detail techniques used. include: reinforcement
learning produce initial function, snakes extract features producing graph,
transformation composition subgraphs, corresponding functions, fit new task.

3.1 Reinforcement Learning

Reinforcement learning typically works refining estimate expected future reward.
goal-directed tasks, ones investigated here, equivalent progressively
68

fiAccelerating Reinforcement Learning

Figure 13: Robot Arm Function
improving estimate distance goal state. estimate updated
best local action, i.e. one moving robot, arm, new state
smallest estimated distance. Early learning process, states close goal
likely accurate estimates true distance. time action taken, estimate
distance new state used update estimate old state. Eventually
process propagate back accurate estimates goal states.
Rather directly estimating Pthe distance goal, system uses expected

discounted reward state E [ 1
t=1 rt ]. uence rewards, rt , reduced
progressively farther future occur using less one. work,
reward reaching goal. farther state goal smaller
value. use expectation allows actions stochastic, robot,
arm, takes particular action particular state, next state always same.
carry reinforcement learning, research uses Q-learning algorithm (Watkins
& Dayan, 1992). algorithm assumes world discrete Markov process, thus
states actions discrete. action state s, Q-learning maintains
rolling average immediate reward r plus maximum value action a0
next state s0 (see Equation 1). action selected state usually one
highest score. encourage exploration state space, paper uses -greedy
policy (Sutton, 1996) chooses random action fraction time.
effect function composition Q-learning algorithm initial value
state-action pair set value zero.
(1)
Qts;a+1 = (1 , ff)Qts;a + ff(r + maxa Qts ;a )
Q-function state action usually referred action-value function.
paper, action-value function transformed composed form
solution new task. value function, discussed previous sections shown
0

69

0

0

fiDrummond

figures, maximum value Q-function. used generate partition
associated graphs needed control process.
Watkins Dayan (1992) proved Q-learning converge optimal value
certain constraints reward learning rate ff. optimal solution produced taking action greatest value state. So, goal-directed tasks,
greedy algorithm take shortest path goal, learning complete.
extension continuous spaces may done using function approximation. simplest
method, one used here, divide state dimensions intervals. resulting action-value function cells representing average Q-value taking action
somewhere within region state space. off-line learning, action
state executed, representation proven converge (Gordon,
1995). on-line learning, current state determined environment,
approach generally successful, exists proof convergence.

3.2 Feature Extraction
Feature extraction uses vision processing technique fits deformable model called
snake (Kass et al., 1987) edges image. initializing snake, process
iterates external forces, due edges, balance internal forces snake
promote smooth shape. Here, external forces due steep gradients value
function. piecewise constant function approximator used, smoothed cubic b-spline
fitted value-function used generate necessary derivatives. left hand
side Figure 14 gradient value function shown Figure 9 extracting
features early learning process. system added gradient around border
represent state space boundary.
locate features, curve found lies along ridge hills, local
maximum differential. right hand side Figure 14, dashed lines
contour lines small inner room indicated. bold lines, right hand side
Figure 14, snake different stages process. snake first positioned
approximately center room, innermost circle. expanded
abuts base hills. simplify exposition, imagine
snake consists number individual hill climbers spread along line representing
snake, indicated small white circles. instead allowed climb
independently, movement relative constrained maintain smooth
shape. snake reaches top ridge, constrained polygon
{ instance quadrilateral { outside dark line Figure 14. point,
tend oscillate around equilibrium position. limiting step size process
brought stationary state. detailed mathematical treatment
approach given Appendix A.
polygon forms \skeleton" graph, shown top left Figure 14.
Nodes graph correspond vertices polygon doorways goal.
Looking gradient plot, doorways regions small differential
ridges. locations determined magnitude gradient along
boundary polygon. example, node added goal (labeled G)
connected \in" doorway (labeled I). polygon delimits region
70

fiAccelerating Reinforcement Learning

Graph

G

Polygon



Doorway

Figure 14: Gradient Resultant Polygon (Left) Extracted Snake (Right)
state space, therefore region action-value function. becomes case
case base, corresponding graph index. Constraining snake
polygon done two reasons. Firstly, vertices needed produce nodes
plane graphs, important part matching process. Secondly, additional
constraint results accurate fit boundaries subtask. This, turn,
results accurate solution function composition.

3.2.1 Three Extensions Snake Approach
section introduces three extensions basic snake approach facilitate extraction features.
first extension affects direction snake moves hill climbing gradient.
normal hill climbing, step taken direction steepest ascent, step size
determined size differential. Roughly, translates forces points
along body snake. force points direction steepest ascent locally,
interacts forces various shape constraints. Looking gradient
function contour lines Figure 14, steep slope leading top
ridge. also significant slope along ridge away doorway towards
boundary state space. Thus force single point body snake
71

fiDrummond

directly towards top ridge turned towards apex, indicated
bold black arrow left hand side Figure 15.
Snake

Steepest
Ascent
Tangent
Normal

Figure 15: Controlling Forces Snake
force broken two components respect snake, normal
tangential force. latter force acts along body snake. shape
constrained quadrilateral, cause relevant side shrink. effect
partially counteracted force towards top ridge adjacent side
quadrilateral. net result shrinking two sides associated
ridges inwards forces balanced. push corner quadrilateral
near doorway inwards, indicated thin black arrow Figure 15. extreme
case, might cause snake collapse something close triangle.
likely outcome degradation accuracy registration ridges.
Drummond (1998) prevented degradation accuracy restricting snakes
rectangular shapes. weakening constraint general polygons,
effect becomes problem. problem addressed removing component
force tangential snake. hill climbing always direction
normal. significantly restrict motion snake: removed
component along body snake. Thus mainly prevents stretching
shrinking snake due gradient.
second extension controls way snake expanded reach base
hills. Drummond (1998) used ballooning force, introduced Cohen Cohen (1993).
problems arose extending system deal general shapes
rectangles, outer L-shaped room Figure 6. ballooning force expands
snake directions normal body. One deleterious effect snake contacts
sharp external corner, inner room, force tends push snake
corner. seen Figure 16; bold continuous lines snake;
bold dashed lines ridges. imagine starting circular snake
72

fiAccelerating Reinforcement Learning

middle L-shaped outer room, time reaches walls inner room
sides snake roughly perpendicular ridges. Thus little restrain
expansion snake passes completely walls inner room.
Ridge

Ballooning
Force

Ridge

Figure 16: Using Ballooning Force
approach adopted analogous ow mercury. imagine starting
somewhere middle L-shaped room progressively adding mercury, would
tend fill lower regions valley first reach bases hills roughly
time. analogy mercury used high surface tension preventing
owing small gaps edges associated doorways. increase
effectiveness idea, absolute value differential gradient thresholded,
values threshold set one zero. smoothed
truncated Gaussian, shown Figure 17. Smoothing thresholding commonly
used techniques machine vision (Tanimoto, 1990). typically used remove
noise, aim strongly blur thresholded image. produces bowls
associated room. example, smoothing almost completely obscured
presence doorway, although generally case.
snake initialized small circle minimum one bowls.
shown circle middle Figure 18, dashed lines contour
lines function. ows outwards, follow contour lines bowl;
largest component ow direction arrows Figure 18.
achieved varying force normal body snake according height
difference average height snake. Thus points along snake
higher average tend get pushed inwards, lower pushed outwards. surface
tension mercury produced various smoothing constraints first second
differentials snake (see Appendix A).
third extension limits changes shape snake expands initial
position reach base hills. smoothness constraints snake, give
mercury-like properties, prevent snake owing gaps associated
73

fiDrummond

Figure 17: Smoothed Function

Figure 18: Mercury Flow

doorways. even proved insucient width rooms width
doorways similar sizes. Figure 12, looking \room" left hand side
configuration space robot arm, \doorway" \room" top
similar width. Increasing surface tension mercury suciently prevent ow
doorways also prevents ow top room.
solution limit amount snake change shape grows.
achieved constraining much second differential snake change
step step. Figure 18, apparent snake takes good approximation
shape room time reaches ridges. shape
locked-in reaching ridges, problem described avoided.
snake initialized, constraint smoothness. snake expanded,
smoothness constraint progressively weakened curvature constraint progressively
strengthened. progressively locks shape still allowing snake make
small local adjustments better fit features.
extensions, discussed section, either modify traditional forces act
snake add new ones. also forces associated knot spacing drag.
snake moves, iteration, depends vector addition forces.
sum acts accelerate body snake mass velocity,
therefore momentum. schematic representation forces shown Figure 19;
detailed mathematical description given Appendix A. dashed line represents
body snake; arrows forces applied one point body. snake
parameterized function, given f^(s) = (x(s); y(s)) x(s) y(s) individual
cubic b-splines giving x coordinates associated variable along body
snake. circles represent points equi-distant necessarily x y.
points kept roughly Euclidean distance apart x due knot
spacing force. momentum, although strictly force, encourages point move
74

fiAccelerating Reinforcement Learning

constant direction; drag opposes motion. stiffness encourages snake
maintain smooth shape. overall stiffness reduced snake grows, keep
exibility per unit length roughly constant, also controlled locally maintain
shape.
Steepest Ascent

MercuryFlow
Momentum

Knot Spacing

Drag
Stiffness

Figure 19: Forces Snake
following algorithmic summary processing snake:

Initialize coecients produce circular snake middle room.
Iterate forces roughly equilibrium snake oscillates around
stationary value.

Modify stiffness enforce polygonal constraints
Iterate 25 steps increasing momentum drag step reduce
oscillation small value.

Use final position snake form polygon delimits boundary
room.

3.3 Transformation

section discusses matching process { subgraph used locate transform
function case base. matching process first finds subgraphs case base
isomorphic extracted subgraph possible isomorphic mappings
nodes, using labeling algorithm (MacDonald, 1992). number isomorphic mappings
75

fiDrummond

potentially exponential number nodes. Here, graphs typically
nodes symmetries, isomorphic mappings. Associated
node subgraph (x; y) coordinate. ane transform, Equation 2, found
minimizes distances coordinates mapped nodes
isomorphic subgraphs. advantage transform relative exibility
simple form.

x0 = C0 x + C1 + C2 y0 = C3 x + C4 + C5
(2)
Ideally transformed nodes would positioned exactly mapped nodes,
usually possible. Even simple rectangular shapes, case base may
contain graph exactly doorway positions. Using graph
exact match introduce error composed function new task.
weighting nodes others error occurs controlled. One aim
minimize introduction errors affect overall path length. However,
equal importance errors introduced easily correctable normal reinforcement
learning.
1

1

2 4

1

2

1

Figure 20: Weighting Graph Nodes
left hand side Figure 20 shows composite graph new task. right
hand side shows result overlaying graph case base. fit
doorway outer L-shaped room error, robot tend miss doorway
collide wall one side. farther doorway position, longer
normal reinforcement learning take correct error. encourage good fit
doorway, weight 4 used. Nodes adjacent doorway given weight 2,
nodes weight one. based intuition trajectories,
different parts state space, pass region close doorway.
error likely broader effect, take longer normal reinforcement
76

fiAccelerating Reinforcement Learning

learning correct, regions far doorway. fit around inner room
improved sacrificing fit far doorway.
exact position doorway inner room critical weight
set 0.5. Whatever position doorway, shape function correct
inside room goal also room. However, doorway
correct position, greater error edge length. produce error
composed function, expectation error small
reinforcement learning quickly correct it.
fit good, would also prefer amount transformation small. transforms produce error particularly true
asymmetric scaling, discussed later section. Generally transform produces
translation, ection, rotation, shearing independent scaling dimension.
robot navigation domain, distance points state space
normal Euclidean distance. reinforcement learning function exponential decay
distance goal. transformation change Euclidean distance,
transformed function directly applicable.

Affine Similar Symmetric
(3)
ane transformation one family hierarchy transformations.
bottom hierarchy, shown Equation 3, symmetric transformations.
solid body transformations change Euclidean distance. next step
hierarchy introduces scaling, equal dimension. affect Euclidean
distance multiplicative factor. Thus change needed transformed
function scale height. ane transformations allow addition asymmetric
scaling shear, distort Euclidean distance. determine amount
distortion, transformation applied unit circle. symmetric, rigid body,
transformations alter circle, transformations will. symmetric
scaling transform changes diameter circle. asymmetric scaling shear
transformations change circle ellipse. amount distortion Euclidean
distance introduced transform determined ratio lengths major
minor axes ellipse.
error = sqrt
(P
wi(
x2 + yi2 )) (node misalignment)
fi2
fi

fi
(Euclidean Distortion)
+ log2 fifi rrmaj
(4)
minfi
2

j
r
+
r
j
maj
min
+ 0:05 log2
(scaling factor)
2
error fit transformed subgraph combined transformation
error using lengths major minor axes, rmaj rmin respectively,
ellipse. penalty Euclidean Distortion asymmetric scaling shear.
log factor added directly error fit shown Equation 4. Log factors
used, penalty functions symmetric. small penalty symmetric
scaling. best matching subgraph found, transformation
applied associated function. isomorphic graph found total error less
1.5, constant function used default. new graph overlays
old graph, values assigned using bilinear interpolation discrete values
77

fiDrummond

function. not, bilinear extrapolation used, based closest values.
cases four values selected, value new point calculated
shown Equation 5. action-value function indexed action well state,
process carried action turn. rotation ection transform
also applied predefined matrix actions. produces necessary mapping
actions original new action-value function.

v = c1 x + c2 + c2 xy + c3

(5)

Finally, height new action-value function must adjusted account
change overall distance goal. height value function \out" doorway
dg dg distance goal discount factor. value random
point within room dg+dd dd distance doorway. action-value
function first normalized dividing dg , height function doorway
original problem. multiplied dng , dng distance
new goal; value point becomes dng+dd . Scaling also affect height
function. Assuming scaling symmetric new value function anywhere
room cdd c scale factor. Thus raising function power c
i.e. ( dd )c account scaling. scaling symmetric result exact, assuming
distance based linear combination two dimensions. asymmetric scaling,
result exact. difference two scale factors relatively small,
useful approximation use maximum.
following algorithmic summary whole matching process:

SG = subgraph extracted new task.
subgraph G acting index case base

{ isomorphic mapping G SG
Find minimum weighted least squares fit G SG using mapping
Ane transform = coecients least squares fit
Penalized fit = least squares error + transform penalty
Keep graph transform lowest penalized fit
Retrieve function associated best graph case base (if none use default)
Apply ane transform function
Apply bilinear interpolation/extrapolation
Adjust function height
Add new function existing function
78

fiAccelerating Reinforcement Learning

3.4 Composition

section describes function composition, transformation applied successively
series subgraphs extracted composite graph. Function composition uses
slightly modified form Dijkstra's algorithm (Dijkstra, 1959) traverse edges
doorway nodes. left hand side Figure 21 shows composite graph moving
goal robot navigation example Section 2.2. right hand side shows
graph traversed Dijkstra's algorithm.

G

d2

G



d1



Gr3

Gr1





d3



Gr2




Gr5



Gr4

Figure 21: Using Dijkstra's Algorithm
begin process, subgraph contains goal extracted best
matching isomorphic subgraph found. edge lengths composite graph
updated using scaled length corresponding edge matching isomorphic
subgraph, d1 d2 Figure 21. d2 less d1, next subgraph extracted,
Gr2, one sharing doorway node edge length d2. best matching
isomorphic subgraph found edge length d3 updated. shortest path
determined. d1 less d2 + d3 subgraph, Gr3 extracted. process
repeated subgraphs updated. stage subgraph matched,
corresponding transformed function retrieved added new function
appropriate region.
example, single path goal room. Often
multiple paths. Suppose room 5 additional doorway lower left corner
room, labeled \B" left hand side Figure 22, addition original doorway
labeled \A". graph, shown right hand side Figure 22, would result.
two possible paths goal lengths d4 d5. length across room 5, d6,
greater absolute difference d4 d5, choice path room
determined decision boundary inside room. produced taking
79

fiDrummond

0110
11111110 Room 3
000000
10

10
1111111
0000000
0
Room 2 1
1010
000 10 Room 5
111
000 10
111
Room 4 1
0
Room 1

d5

n2

d4

d6

n1

Gr5
B

B

n3

Figure 22: Multiple Paths Goal
maximum two functions shown Figure 23: one entering doorway \A"
leaving doorway \B"; one entering doorway \B" leaving doorway \A".
principle repeated two paths goal given
room.
cross-room distance, d6, smaller difference (jd4-d5j) decision
boundary would another room. general, want find room
cross-room distance larger difference incident paths.
repeated every cycle path graph. cycle detected node visited twice,
indicating reachable two separate paths. Let us suppose node n3
graph Figure 22. Dijkstra's algorithm used, know previous nodes,
either path, n1 n2 already closed. must true paths
reached n3. rooms paths nodes cannot contain decision
boundary, must either room 4 5. decide remaining room in,
compare two path lengths. d4 longer d5 + d6 decision boundary
room 4; otherwise room 5.
Whichever room selected, decision boundary produced maximum two
functions. heights two functions, adjusted path lengths, determine
decision boundary occurs within room. paths equal length, taking
maximum correctly put decision boundary doorway.
functions case base, functions already include decision boundaries may used.
technique produces correct decision boundary difference path lengths
entering room less difference heights function \out"
doorways. left hand side Figure 24 room two doorways. path
1 significantly longer path 2, decision boundary far left. shortest
path goal room via right hand doorway. function
combined mirror image itself, produce decision boundary middle
80

fiAccelerating Reinforcement Learning

Maximum
Decision
Boundary

Figure 23: Combining Two Functions

Decision
Boundary

Path 1
Path2

Room

Figure 24: Decision Functions

81

fiDrummond

room, shown right hand side Figure 25. could used new
problem shown left hand side Figure 25 two paths length.
heights two functions changed move decision boundary.
cannot moved anywhere room. decision boundary moved
closer particular doorway original function shown Figure 24
Decision
Boundary
Path 1

Path2

Room

Figure 25: Combining Decision Functions

4. Experiments

section compares learning curves function composition simple baseline algorithm. Four sets results presented; one two types related task
two domains. learning curves represent average distance goal
function number actions taken learning. distance averaged
64 different start positions, distributed uniformly throughout state space,
different experimental runs. determine distance, normal learning stopped
fixed number actions copy function learned far stored. One 64
start positions selected, learning restarted number actions needed reach
goal recorded. trial takes 2000 actions yet reached goal,
stopped distance goal recorded 2000. function reinitialized
stored version another start state selected. repeated 64 times.
function reinitialized normal learning resumed.
baseline algorithm underlying learning algorithm function composition system basic Q-learning algorithm, using discrete function approximator
discussed Section 3.1. learning rate ff set 0.1, greedy policy uses 0.1
(the best action selected 90% time), future discount 0.8 reward 1.0
received reaching goal. Although state spaces different domains represent two quite different things { robot's hx; yi location angle arm's two
joints { actual representation same. state space ranges 1
dimension. step 0:25 dimension either separately together, giving eight
possible actions. actions stochastic, uniformly distributed random value
0:125 added dimension action. robot navigation examples
82

fiAccelerating Reinforcement Learning

robot hits wall, positioned small distance wall along direction
last action. implemented robot arm somewhat
complex calculation. Instead, collision obstacle occurs arm restored
position taking action.
Learning begins randomly selected start state continues goal reached.
new start state selected randomly process repeated. continues
requisite total number actions achieved. Speed calculated dividing
number learning steps one specific point baseline learning curve number
learning steps equivalent point function composition system's learning curve.
knee function composition system's curve used. occurs low
level learning algorithm initialized composed function. compared
approximate position knee baseline curve.

4.1 Robot Navigation, Goal Relocation
first experiment investigates time taken correct learned function goal
relocated robot navigation domain. nine different room configurations,
shown Figure 26, number rooms varying three five four
different goal positions. room one two doorways one two paths
goal. initialize case base, function learned configurations
goal position shown black square. rooms generated randomly,
constraints configuration rooms doorways: room
small narrow, doorway large. case base also includes
functions generated experiments discussed Section 4.3. necessary
give sucient variety cases cover new tasks. Even addition,
subgraphs matched. Constant valued default functions used
match. reduces speed significantly, eliminate altogether.

1

2

3

4

5

6

7

8

9

Figure 26: Different Suites Rooms
83

fiDrummond

case base loaded, basic Q-learning algorithm rerun room
configuration goal position shown. 400,000 steps goal moved,
denoted time x-axis Figure 27. goal moved one
three remaining corners state space, task included case base. Learning
continues 300,000 steps. fixed intervals, learning stopped average
number steps reach goal recorded. curves Figure 27 average
27 experimental runs, three new goal positions nine room configurations.
Function Composition

Average No. Steps Goal

Q-Learning
Q-Learning (No Reinit)

10

10

3

2

1

10
t-400....t-100

t-50



t+50

t+100

t+150

t+200

t+250

t+300

+ No. Learning Steps X 1000

Figure 27: Learning Curves: Robot Navigation, Goal Relocation
basic Q-learning algorithm, top curve Figure 27, performs poorly because,
goal moved, existing function pushes robot towards old goal position.
variant basic algorithm reinitializes function zero everywhere detecting
goal moved. reinitialized Q-learning, middle curve, performed much
better, still learn new task scratch.
function composition system, lowest curve, performed far best.
precise position knee curve dicult determine due effect using
default functions. examples using case base functions considered, knee
point sharp 3000 steps. average number steps goal 3000 steps,
examples, 40. non-reinitialized Q-learning fails reach value within
300,000 steps giving speed 100. reinitialized Q-learning reaches value
120,000 steps, giving speed 40. Function composition generally
produces accurate solutions. Even error introduced, Q-learning quickly
refines function towards asymptotic value 17. 150,000 steps,
84

fiAccelerating Reinforcement Learning

normal Q-learning reaches average value 24 steps slowly refines solution
reach average value 21 300,000 steps.

4.2 Robot Arm, Goal Relocation
second experiment essentially repeat first experiment robot arm
domain. initial number steps, goal moved, reduced 300,000
speed experiments. arm two degrees freedom,
restrictions discussed Section 2.4, number variations small. three obstacle
configurations used, constructed hand, two obstacles each. increase
number experiments, allow greater statistical variation, configuration
repeated goal three possible positions, shown Figure 28.
black diamonds represent obstacles, black rectangles goal. Solutions
tasks loaded case base. composing function, however, system
prevented selecting case comes goal obstacle configuration.

1

2

3

4

5

6

7

8

9

Figure 28: Robot Arm Obstacle Goal Positions
curves Figure 29 average 18 experimental runs, two new goal positions
three original goal positions three obstacle configurations shown
Figure 28. two learning curves, non-reinitialized Q-Learning dropped.
first experiment, function composition system, lower curve, performed
much better Q-learning. knee function composition system occurs 2000
steps, knee Q-learning 50,000 steps, giving speed 25. experiment,
case base contained subgraphs matched new tasks, default functions
needed. composed functions tend accurate little refinement
necessary.
85

fiDrummond

Function Composition

Average No. Steps Goal

Q-Learning

10

10

3

2

1

10
t-300....t-100

t-50



t+50

t+100

t+150

t+200

t+250

t+300

+ No. Learning Steps X 1000

Figure 29: Learning Curves: Robot Arm, Goal Relocation

4.3 Robot Navigation, New Environment
third experiment investigates time taken learn new, related, environment
robot navigation domain. Nine different inner rooms generated randomly,
constraints. single doorway, size position room
location doorway varied shown Figure 30. initialize case base,
function learned configurations goal inside small room
indicate dark square. Learning repeated room configurations
turn. However, composing new function system prevented selecting
case learned goal room configuration. Experimental runs Qlearning algorithm function composition system initialized function
zero 0.75 everywhere respectively, denoted zero x-axis. Learning continues
100,000 steps. improve statistical variation, experiments configuration
repeated three times, time new random seed. curves Figure 31 are,
therefore, average across 27 experimental runs.
top curve Q-learning algorithm, bottom curve function composition
system. experiments, locating goal took typically 400 1200 steps,
although took 2000 steps. function composition system introduces \no
walls" function typically 800 4000 steps taken usable features
generated. Again, certain experimental runs took longer, discussed Section
5.2. Due runs, knee function composition system's curve occurs 12,000
steps. knee basic Q-learning curve occurs approximately 54,000 steps giving
86

fi10

10

10

3

2

0

1

1

5

2

9

6

3

30

40

50

60

70

Q-Learning

80

90

Function Composition

Accelerating Reinforcement Learning

4

8

20

Figure 30: Single Rooms

7

10

No. Learning Steps X 1000

87

100

Figure 31: Learning Curves: Robot Navigation, New Environment

Average No. Steps Goal

fiDrummond

speed 4.5. previous experiments initialized function accurate
little refinement necessary. Basic Q-learning, reaching knee, takes
long time remove residual error.

4.4 Robot Arm, New Environment

10

10

10

3

2

0

1

1

20

30

2

40

50

3

60

70

Q-Learning

80

90

Function Composition

Figure 32: Different Obstacle Positions

10

No. Learning Steps X 1000

Figure 33: Learning Curves: Robot Arm, New Environment
88

100

fourth experiment essentially third experiment except robot
arm domain. Here, three, hand crafted, configurations single obstacle goal
fixed position used, shown Figure 32. increase statistical variation
configuration run five times different random seed. curves Figure 33
therefore average across 15 experimental runs.

Average No. Steps Goal

fiAccelerating Reinforcement Learning

top curve Figure 31 Q-learning algorithm, bottom curve function
composition system. knee function composition system's curve occurs
4400 steps. knee basic Q-learning algorithm 68,000 steps giving speed
15.

5. Analysis Results

experiments previous section shown function composition produces
significant speed across two different types related task across two domains.
addition, composed solutions tend accurate little refinement
required. section begins looking possible concerns experimental
methodology might affect measurement speed up. discusses various
properties task solved affect speed achieved using function
composition.

5.1 Possible Concerns Experimental Methodology

speed obtained using function composition suciently large small variations
experimental set unlikely affect overall result. Nevertheless,
number concerns might raised experimental methodology.
be, least partially, addressed section; others subject future work.
first concern might estimated value speed measured.
value represents speed average set learning tasks, rather
average speed tasks. One diculties estimation,
curves single tasks, average distance goal may oscillate
learning progresses, even though general trend downwards. makes judging
position knee curves dicult, estimate speed questionable. Even
experimental runs using configuration, different random seeds, exhibit
considerable variation. instances, speed measured individual curves may
benefit function composition system, others, baseline algorithm. Nevertheless,
probably overall effects cancel out.
second concern might effect speed limit 2000 steps
measuring distance goal. Comparing two averages values limited way
sometimes misleading (Gordon & Segre, 1996). limit primarily affects
baseline algorithm, significant goal moved function
reinitialized. Estimation speed principally concerned comparing position
knees different curves. Here, average distance goal relatively small,
limiting value likely little effect.
third concern might value speed dependent configuration
baseline algorithm. Certainly, experience author way
function initialized, actions selected, impact speed
learning. previous work (Drummond, 1998), function initialized constant
value 0.75, technique termed \optimistic initial values" Sutton Barto (1998).
Tie breaking actions value achieved adding small amount
noise (circa 5 10,5). expected would increase exploration early
learning process speed learning overall. However, using initial value zero
89

fi3

2

0

1

50

Drummond

100

150

200

250

300

strict tie-breaker, randomly selecting amongst actions value, turned
produce significant speed baseline learning algorithm. configuration
used preceding experiments, one experimental run caused serious
problems baseline algorithm.

10

10

10

No. Learning Steps X 1000

Figure 34: Learning Curves Partially Observable Domain

90

upper learning curve Figure 34 baseline algorithm, one run
goal moved robot arm domain. large impact average
learning curve, replaced lower curve, produced repeating experiment
different random seed. slow learning rate arises interaction
partial observability robot arm domain use initial value
zero. Individual cells function approximator straddle obstacles allowing \leakthrough" value one side obstacle other. Starting zero value,
action receives value remain best action time. Continual
update action decrease value, asymptotically approach zero.
actions state updated, always selected greedy
action. occur higher initial values. may domains
degree partial observability, small initial values better zero means
improving exploration small values might necessary.
variations parameters baseline algorithm explored
paper. instance, constant learning rate 0.1 used. Alternatives,
starting higher rate reducing learning progresses might also improve
overall speed baseline algorithm. preliminary experiments were, however,

Average No. Steps Goal

fiAccelerating Reinforcement Learning

carried using undiscounted reinforcement learning, discounting strictly unnecessary goal-directed tasks. Room configuration 1 Figure 26, goal
lower right hand corner, used experimental task. discounting, discussed
Section 3.1, turned setting 1. addition, value reaching goal state
set zero cost associated every action. form learning simplifies
function composition, normalization procedures needed compensate value function's exponential form longer required. normalization disabled, snake
successfully partitioned function, critical part process. However,
baseline learner took considerably longer learn function discounted case.
discounting, learner reached average distance goal 72 steps
80,000 learning steps. Without discounting, learner reached average 400 steps
point time average 80 steps 300,000 learning steps.
action-value function initialized zero, appears standard practice
literature. However, experience initialization discounted case suggests
might part problem investigated future work.
baseline Q-learning algorithm used basic sophisticated one
would unquestionably reduce speed experimentally obtained. instance,
form reinforcement learning using eligibility traces (Singh & Sutton, 1996) might
used. experiments goal moved, baseline Dyna-Q+ (Sutton,
1990) specifically designed deal changing worlds would probably
better reference point.
speed obtained, transferring pieces action-value function, also
compared alternatives, transferring pieces policy transferring pieces
model. Transferring pieces policy would reduce memory requirements
require rescaling applied pieces action-value function. does, however,
two disadvantages. Firstly, solution directly composed, position
decision boundaries determined. learning would necessary decide
appropriate policy room. Secondly, policy indicates best action.
action-value function orders actions, indicating potentially useful small changes
policy might improve accuracy new task. Transferring pieces
model, would require first learning model consisting probability distribution function
action state. memory requirement considerably larger, unless
states reachable action limited beforehand. Nevertheless, model would need
less modification changing world, goal moved. also carries
information might speed learning. action-value function seems good
compromise terms complexity versus information content, would need
empirically validated subject future work.

5.2 Performance Variation Task Configuration
Generally, function composition outperforms baseline learning algorithm amount
dependent complexity learning problem. robot navigation domain
goal moved, amount speed increased rooms fewer
paths goal. speed 60, average speed 40, obtained
configurations five rooms single path goal. Configurations three
91

fiDrummond

rooms least speed up, due relative simplicity
problem.

10

10

10

3

2

0

1

50

6

100

Q-Learning

Function Composition

No. Learning Steps X 1000

6

92

Figure 35: Failure Robot Navigation Moving Goal

6

150

top Figure 35 shows average four learning curves three room
configurations. bottom Figure 35 shows one configurations produced
curves. one easiest tasks (from experimental set)
baseline algorithm, also solutions case base lowest room.
isomorphic subgraphs form. Rather composing solution,
system introduces constant value function room. room represents almost
half state space, much additional learning required. top Figure 35 shows,
initially significant speed up. refinement reduces advantage
short baseline algorithm better. later, function composition gains
upper hand converges quickly baseline algorithm towards asymptotic
value.

Average No. Steps Goal

fiAccelerating Reinforcement Learning

robot navigation domain learning new task, amount speed varied
size inner room. primarily due number actions needed
features emerged sucient clarity snake locate them. Function
composition successful inner room small. wall long, feature
takes time develop, refinement Q-learning needed make apparent.
short walls also hard identify. likelihood robot colliding
small takes many exploratory actions features emerge clearly.
features may suciently clear snake form partition, yet well
enough defined precisely locate doorways. doorway may appear bit wider
actually is. importantly, may appear displaced true position.
Typically, error composed function small normal reinforcement learning
quickly eliminates it. one experimental runs, configuration 2 Figure 30,
speed reduced factor 2 due doorway incorrectly positioned.
feature representing lower wall completely emerged partition
generated. made doorway appear almost exactly corner.
algorithm, fact, positioned doorway wrong side corner. resulted
significantly reduced speed up. unclear reinforcement learning took
long correct seems, surface least, local error.
investigated future work.

6. Limitations

Limitations come , roughly, two kinds: arising overall approach
arising way implemented. former case, ways address limitations may highly speculative, impossible without abandoning fundamental
ideas behind approach. latter case, reasonable expectation future
work address limitations. following sections deal cases
turn.

6.1 Limitations Approach

explore possible limitations approach, section reviews fundamental
assumptions based.
fundamental assumption features arise reinforcement learning function
qualitatively define shape. features used paper violation
smoothness assumption, neighboring states similar utility values. wall,
preventing transitions neighboring states, typically causes violation.
things, actions significant cost, would similar effect. Smaller,
much varied costs, generate features required approach, offers
little way speed cases. mixture large small costs,
expected system capture features generated former, initialize
function normal reinforcement learning address latter.
smoothness assumption less clear dimensions numeric. neighborhood relation, used here, predefined distance metric continuous space.
nominal, binary mixed domains obvious metric would defined,
although work metrics applications (Osborne & Bridge,
93

fiDrummond

1997). dimensions mixed, feature location might limited continuous
ones. dimensions purely nominal binary, generalization snake may
appropriate. snake is, abstract level, constrained hill climber. whether
idea would usefully generalize way present somewhat speculative.
fundamental assumption features clearly delimit subtasks. domains discussed paper, obstacles walls subdivide state space regions
connected small \doorways". subtask reaching one doorway greatly affected
subsequent subtask. domains may case. doorways
become larger, context sensitivity increases. long composed solution reasonably accurate, reinforcement learning easily correct error although speed
reduced. point however, due large amount context sensitivity,
advantage dividing task subtasks become questionable. would possible
account context dependency graph matching stage, looking
larger units subgraphs. two adjacent subgraphs match new problem, might
used pair, thereby including contextual relationship them. Even
single subgraphs used, context appear, i.e. shape neighboring
subgraphs, could taken account. limit, graph matching whole task might
used. But, argued introduction, would considerably limit transfer
applicable, thus overall effectiveness.
fundamental assumption absolute position features unimportant, shape delimited region matters. increase likelihood
transfer, solutions subtasks subjected variety transformations.
domains, many, all, transformations invalid. actions cannot
rotated ected, many small costs affect different regions state space,
effectiveness transfer reduced. would be, extent, addressed
additional penalties different transformations, would limit opportunities transfer. transformations appropriate, whether
determined automatically domain, subject future research.
fundamental assumption vision processing technique locate
features timely fashion, even high dimensional domains. Learning high
dimensional domains likely slow whatever technique used. Normal reinforcement
learning take time navigate much larger space, slowing emergence
features. Although time taken partition function increase, frequency
partitioning applicable decrease. Thus amortized cost rise
slowly. Further, high dimensional spaces generally problematical, methods
principal components analysis projection pursuit (Nason, 1995) used reduce
dimensionality. may prove practice dimensionality important,
focus feature extraction, much smaller actual dimensionality space.

6.2 Limitations Implementation
assumptions previous section met, expected remaining limitations due present implementation. limitations likely become
apparent system applied domains. Certainly domains may differ
presented paper number ways.
94

fiAccelerating Reinforcement Learning

domain may differ dimensionality space higher two
dimensions tasks investigated paper. implementation snake
updated work higher dimensions. bold lines top Figure 36
one simpler tasks robot navigation domain. task extended
Z-dimension. snake starts sphere expands outwards fills
room. example, polygonal constraint used, everything else
remains same. Figure 37 shows complete partition task.

Figure 36: Adding Z-Dimension

Figure 37: Complete 3D Partition

mathematics behind snake limited three dimensions. also seems
nothing principle would prevent processes graph matching,
planning transformation working higher dimensions. Speed main problem.
problem unique approach large body research addressing
issue. instance, although graph matching general NP-complete,
much active research speeding matching average special cases (Gold &
Rangarajan, 1996; Galil, 1986). present, snake represents principal restriction
speed. issue great importance vision processing community. Current
research investigating problem, least two three dimensions. One example
hierarchical methods (Schnabel, 1997; Leroy, Herlin, & Cohen, 1996) find solutions
snake progressively finer finer resolution scales. results research
undoubtedly importance here.
domain may differ value function learned might produce features locatable snake present parameter settings. values parameters
empirically determined, using hand crafted examples robot navigation
robot arm domains. obvious danger parameters might tuned
examples. demonstrate case, configurations experiments
robot navigation domain generated randomly. configurations robot arm
domain tightly constrained, hand crafted examples used experiments. Nevertheless, experiments shown parameters worked successfully
random examples robot navigation domain. parameters also work successfully second domain, robot arm. following discussion demonstrates
95

fiDrummond

also reasonably effective quite different domain, \car hill".
anticipated using results current research snakes automate selection
many parameters.
\car hill"domain (Moore, 1992), task, simply stated, get car
steep hill, Figure 38. car stationary part way hill, fact anywhere
within dotted line, insucient acceleration make top.
car must reverse hill achieve sucient forward velocity, accelerating
side, accelerating hill. state space, purposes
reinforcement learning, defined two dimensions. position velocity
car, shown Figure 39. goal reach top hill small
positive negative velocity. domain two possible actions: accelerate
forward, accelerate backwards. Unlike previous domains, clear mapping
actions onto state space. state achieved applying action determined
Newton's laws motion. car insucient acceleration make hill
everywhere state space, \wall" effectively introduced, bold line Figure 39.
reach top hill, car must follow trajectory around \wall", dashed
line Figure 39.
Goal

Velocity

+ve

Goal

0



cit

lo


-ve

0

Position

Figure 38: Car Hill

Position

Figure 39: Car State Space

Figure 40 shows reinforcement learning function. exhibits steep gradient
domains. important point note that, unlike domains,
physical object causes gradient. implicit problem itself, yet features
still exist. Figure 41 shows partition produced applying snake \car
hill" domain. main difference previous examples polygonal
constraint used. snake initially comes rest, mercury force
turned snake allowed find minimum energy state. also
necessary reduce scaling edges, factor three quarters, achieve
accuracy fit. fit around top left corner second snake, dashed line,
96

fiAccelerating Reinforcement Learning

also problems: snake growing slowly downwards is, present,
stopped reached maximum number iterations allowed. One diculty
example clear delimitation upper lower regions
end feature. Future work investigate altering stopping condition
eliminate problem.

Figure 40: Steep Gradient

Figure 41: Regions Extracted

domain may differ shape various regions partition complex dealt present snake. Fitting snake task discussed
previous paragraphs goes way towards mitigating concern. Nevertheless,
randomly generated examples Section 4.1 subject certain constraints. Configurations narrower rooms tried informally, snake reliably locate
features. configurations Section 4 represent limit complexity partition
snake produce present. expected using ideas large body
already published research snakes go long way towards addressing limitation.
complex regions, locating subtleties underlying shape may unnecessary,
even undesirable. aim speed low level learning. long solution
reasonably accurate, speed obtained. sensitive minor variations
shape may severely limit opportunities transfer thus reduce speed overall.
domain may differ changes environment complex
investigated paper. present, system detects goal moved
counting often reward received old goal position. rather
ad hoc approach, also account possible changes, paths
becoming blocked short-cuts becoming available. present, learning new task
system restarted required determine present solution longer
applicable. future work, system decide model world longer
correct. also decide what, any, relationship existing task
might best exploited. allow complex interaction function
composition system reinforcement learning. instance, learning new task
97

fiDrummond

robot navigation domain used relatively simple situation two rooms. function
composition system initialized low level algorithm detecting suitable features.
future, address complex tasks, many rooms, incremental approach
used. new task learned, system progressively build
solution function composition different features become apparent.
approach also handle errors system might make feature extraction. experiments simple room configurations, filtering discussed
Section 2.3 proved sucient prevent problems. complex tasks, likely
false \doorways" detected, simply system explored
region state space. composed function including extra doorway drive
system region. become quickly apparent doorway
exist new function composed.

7. Related Work
strongly related work investigating macro actions reinforcement learning. Precup, Sutton Singh (1997, 1998) propose possible semantics macro actions
within framework normal reinforcement learning. Singh (1992) uses policies, learned
solve low level problems, primitives reinforcement learning higher level. Mahadevan Connell (1992) use reinforcement learning behavior based robot control.
learn solution new task, systems require definition subtask
interrelationships solving compound task. work presented gives
one way macro actions extracted directly system's interaction
environment, without hand-crafted definitions. also shows determine
interrelationships macro actions needed solve new task. Thrun's research (1994) identify macro actions, finding commonalities multiple tasks.
unlike research presented here, mapping actions new tasks proposed.
Hauskrecht et al. (1998) discuss various methods generating macro actions. Parr (1998)
develops algorithms control caching policies used multiple tasks.
cases, need given partitioning state space.
automatic generation partition focus much work
presented paper. may well approach generating partitions
determining interrelationships partitions related tasks prove useful
work.
Another group closely connected work various forms instance based case
based learning used conjunction reinforcement learning.
used address number issues: (1) economical representation state
space, (2) prioritizing states updating (3) dealing hidden state. first issue
addressed Peng (1995) Tadepalli Ok (1996) use learned instances
combined linear regression set neighboring points. Sheppard Salzberg
(1997) also use learned instances, carefully selected genetic algorithm.
second issue addressed Moore Atkeson (1993) keep queue \interesting"
instances, predecessors states learning produces large change values.
updated frequently improve learning rate. third issue addressed
McCallum (1995b) uses trees expand state representation include prior
98

fiAccelerating Reinforcement Learning

states, removing ambiguity due hidden states. work, McCallum (1995a) uses
single representation address hidden state problem general problem
representing large state space using case base state sequences associated
various trajectories. Unlike research, work presented case
example value function learning. Instead, result complete
learning episode, method complementary approaches.
work also related case based planning (Hammond, 1990; Veloso & Carbonell,
1993), firstly general connection reinforcement learning planning.
analogous ways. small change world,
goal moved, composite plan modified using sub-plans extracted
composite plans.
Last, least, connection object recognition vision research (Suetens
et al., 1992; Chin & Dyer, 1986). work presented here, many methods {
final application { come field. features reinforcement learning
function akin edges image. located finding zero crossing point
Laplacian introduced Marr (1982). work presented here, proposed
features largely dictate form function. Mallat Zhong (1992)
shown function accurately reconstructed record steep slopes.

8. Conclusions

paper described system transfers results prior learning significantly
speed reinforcement learning related tasks. Vision processing techniques utilized
extract features learned function. features used index case
base control function composition produce close approximation solution
new task. experiments demonstrated function composition often produces
order magnitude increase learning rate compared basic reinforcement
learning algorithm.

Acknowledgements
author would like thank Rob Holte many useful discussions help preparing
paper. work part supported scholarships Natural Sciences
Engineering Research Council Canada Ontario Government.

Appendix A. Spline Representations

appendix presents underlying mathematics associated spline representations snake. meant introduction subject. Rather
added completeness discuss certain important aspects system addressed
elsewhere paper. Knowledge aspects necessary understand basic
principles approach discussed paper, would necessary one wanted
duplicate system. detailed explanation given Drummond (1999).
specific papers address ideas much greater detail are: splines (Terzopoulos,
1986) snakes (Cohen & Cohen, 1993; Leymarie & Levine, 1993).
99

fiDrummond

Splines piecewise polynomials degree polynomial determines
continuity smoothness function approximation. Additional smoothing constraints
introduced penalty terms reduce size various differentials. One way
view spline fitting form energy functional Equation 6.

Espline(f^) =

Z

R





Efit (f^) + Esmooth (f^) ds

(6)

Here, energy associated goodness fit, measure close
approximating function input function. typically least squares
distance functions. energy associated smoothness
function. Two commonly used smoothness controls produce membrane thin
plate splines restricting first second differentials function respectively.
fit spline function, total energy must minimized. necessary condition
Euler-Lagrange differential equation Equation 7. !t controls
tension spline (the resistance stretching) !s stiffness (the resistance
bending). Often error function based individual data points left hand
side Equation 7 would include delta functions.
^

2^

@ (! (s) @ f (s) ) + @ (! (s) @ f (s) ) = f (s) , f^(s)
, @s


@s
@s2 @s2

(7)

work, splines used number purposes. fitting
snake, measures first second differential needed. two dimensional quadratic
spline fitted discrete representation maximum Q-values. !t 0.2 used
(!s zero) limit overshoot (Drummond, 1996) prevent false edges. Values
identical spline except using !t 2.0 squared divided differential
values. normalizes differentials, size edges dependent
occur function. type spline used produce bowls associated
rooms discussed Section 3.2.1. !t 1.0 !s 0.5 giving roughly
Gaussian smoothing. values used produce function weighted. Values close
one given weights 200, lower values weight 1. prevents sides
bowls collapsing smoothing.
one dimensional cubic spline used locating doorways. found
steepest descent value differential along body snake. differential
contains many local minima associated doorways. arise either
inherent noise process errors fit snake. aim remove
ones associated doorways smoothing thresholding. achieved
first sampling gradient points along snake. values normalized lie
zero one. spline !t 0.15 (!s 0.0). weighted least mean
squares fit used. weighting function inverse square values, preventing
spline overwhelmed large values. Starting points steepest descent
changes sign coecients gradient spline. initial step size
set slightly larger knot spacing decreased time. local
minimum found value exceeds threshold (of 0.5), rejected.
represent snake, model spline must changed somewhat. snake
one dimensional cubic spline. energy minimum sought
100

fiAccelerating Reinforcement Learning

differential Qmax function, subject constraints. dynamics
snake defined Euler-Langrange equation shown Equation 8.
2 f^!!
2 f^!
2 f^
^ @ @
@
f
@
@
@
@
@t2 + @t + @t @s2 !c(s) @s2 + @s2 !tp (s) @s2 = F (f^)

(8)

!c 512 minimizes changes snake's shape grows, penalizing
difference second differential previous time step scaled ratio
lengths. !s 8.0 initial stiffness snake. reduced proportionately
snake's length give spline degrees freedom. 96 96 control
momentum drag snake respectively. Cohen Cohen (1993),
factor added energy associated differential direction normal
body snake, shown Equation 9. instead constant,
variable used produce mercury model discussed Section 3.2.1.
2
F (f^) = (f^),!
n (s) + r(, fifirQmax(f^)fifi ),!
n (s)
fi

fi

(9)

energy minimization process carried iteratively interleaving steps x
directions. differential r jQmax j2 x direction given Equation 10,
similar equation used direction.
2
2 Qmax
@Qmax )( @ 2 Qmax )
)
+
(
, @ jrQ@xmax j = ,2 ( @Q@xmax )( @ @x
2
@y
@x@y
"

#

(10)

snake grows forces mercury model reaches approximately
stable position, subject small oscillations. converted polygon
n = 0 : : : 3).
finding corners (where normal passes (2n+1)
4
coecient !1 set zero everywhere. coecient !2 set zero corners
15 them. produces polygon exible vertices.
detect features early possible learning process, discussed Section
2.4, height gradient scaled according signal noise ratio. noise
arises variations low level learning process stochastic nature task.
size features noise grow time somewhat normalized
scaling process. idea collect uniformly sampled values function shown
Equation 10 x directions find median absolute values.
median strongly affected extreme values thus largely ignores size
features, measuring noise regions between.

References
Chin, C. H., & Dyer, C. R. (1986). Model-based recognition robot vision. Computing
Surveys, 18 (1), 67{108.
Christiansen, A. D. (1992). Learning predict uncertain continuous tasks. Proceedings
Ninth International Workshop Machine Learning, pp. 72{81.
101

fiDrummond

Cohen, L. D., & Cohen, I. (1993). Finite element methods active contour models
balloons 2-d 3-d images. IEEE Transactions Pattern Analysis Machine
Intelligence, 15 (11), 1131{1147.
Dijkstra, E. W. (1959). note two problems connexion graphs. Numerische
Mathematik, 1, 269{271.
Drummond, C. (1996). Preventing overshoot splines application reinforcement
learning. Computer science technical report TR-96-05, School Information Technology Engineering, University Ottawa, Ottawa, Ontario, Canada.
Drummond, C. (1997). Using case-base surfaces speed-up reinforcement learning.
Proceedings Second International Conference Case-Based Reasoning, Vol.
1266 LNAI, pp. 435{444.
Drummond, C. (1998). Composing functions speed reinforcement learning changing world. Proceedings Tenth European Conference Machine Learning,
Vol. 1398 LNAI, pp. 370{381.
Drummond, C. (1999). Symbol's Role Learning Low Level Control Functions. Ph.D.
thesis, School Information Technology Engineering, University Ottawa, Ottawa, Ontario, Canada.
Galil, Z. (1986). Ecient algorithms finding maximum matching graphs. ACM
Computing Surveys, 18 (1), 23{38.
Gold, S., & Rangarajan, A. (1996). graduated assignment algorithm graph matching.
IEEE Transactions Pattern Analysis Machine Intelligence, 18 (4), 377{388.
Gordon, G. J. (1995). Stable function approximation dynamic programming. Proceedings Twelfth International Conference Machine Learning, pp. 261{268.
Gordon, G. J., & Segre, A. M. (1996). Nonparametric statistical methods experimental evaluations speedup learning. Proceedings Thirteenth International
Conference Machine Learning, pp. 200{206.
Hammond, K. J. (1990). Case-based planning: framework planning experience.
Journal Cognitive Science, 14 (3), 385{443.
Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L. P., & Dean, T. (1998). Hierarchical
solution Markov decision processes using macro-actions. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, pp. 220{229.
Kass, M., Witkin, A., & Terzopoulus, D. (1987). Snakes: Active contour models. International Journal Computer Vision, 1, 321{331.
Leroy, B., Herlin, I. L., & Cohen, L. D. (1996). Multi-resolution algorithms active
contour models. Proceedings Twelfth International Conference Analysis
Optimization Systems, pp. 58{65.
102

fiAccelerating Reinforcement Learning

Leymarie, F., & Levine, M. D. (1993). Tracking deformable objects plane using
active contour model. IEEE Transactions Pattern Analysis Machine
Intelligence, 15 (6), 617{634.
MacDonald, A. (1992). Graphs: Notes symetries, imbeddings, decompositions. Tech.
rep. Electrical Engineering Department TR-92-10-AJM, Brunel University, Uxbridge,
Middlesex, United Kingdom.
Mahadevan, S., & Connell, J. (1992). Automatic programming behavior-based robots
using reinforcement learning. Artificial Intelligence, 55, 311{365.
Mallat, S., & Zhong, S. (1992). Characterization signals multiscale edges. IEEE
Transactions Pattern Analysis Machine Intelligence, 14 (7), 710{732.
Marr, D. (1982). Vision: Computational Investigation Human Representation
Processing Visual Information. W.H. Freeman.
McCallum, R. A. (1995a). Instance-based state identification reinforcement learning.
Advances Neural Information Processing Systems 7, pp. 377{384.
McCallum, R. A. (1995b). Instance-based utile distinctions reinforcement learning
hidden state. Proceedings Twelfth International Conference Machine
Learning, pp. 387{395.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning
less data less real time. Machine Learning, 13, 103{130.
Moore, A. W. (1992). Variable resolution dynamic programming: Eciently learning action
maps multivariate real-valued state spaces. Proceedings Ninth International
Workshop Machine Learning.
Nason, G. (1995). Three-dimensional projection pursuit. Tech. rep., Department Mathematics, University Bristol, Bristol, United Kingdom.
Osborne, H., & Bridge, D. (1997). Similarity metrics: formal unification cardinal
non-cardinal similarity measures. Proceedings Second International
Conference Case-Based Reasoning, Vol. 1266 LNAI, pp. 235{244.
Parr, R. (1998). Flexible decomposition algorithms weakly coupled Markov decision
problems. Proceedings Fourteenth Conference Uncertainty Artificial
Intelligence, pp. 422{430.
Peng, J. (1995). Ecient memory-based dynamic programming. Proceedings
Twelfth International Conference Machine Learning, pp. 438{439.
Precup, D., Sutton, R. S., & Singh, S. P. (1997). Planning closed-loop macro actions.
Working notes 1997 AAAI Fall Symposium Model-directed Autonomous
Systems, pp. 70{76.
103

fiDrummond

Precup, D., Sutton, R. S., & Singh, S. P. (1998). Theoretical results reinforcement
learning temporally abstract options. Proceedings Tenth European
Conference Machine Learning, Vol. 1398 LNAI, pp. 382{393.
Schnabel, J. A. (1997). Multi-Scale Active Shape Description Medical Imaging. Ph.D.
thesis, University London, London, United Kingdom.
Sheppard, J. W., & Salzberg, S. L. (1997). teaching strategy memory-based control.
Artificial Intelligence Review: Special Issue Lazy Learning, 11, 343{370.
Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning replacing eligibility traces.
Machine Learning, 22, 123{158.
Singh, S. P. (1992). Reinforcement learning hierarchy abstract models. Proceedings Tenth National Conference Artificial Intelligence, pp. 202{207.
Suetens, P., Fua, P., & Hanson, A. (1992). Computational strategies object recognition.
Computing Surveys, 24 (1), 5{61.
Sutton, R. S. (1990). Integrated architectures learning, planning, reacting based
approximating dynamic programming. Proceedings Seventh International
Conference Machine Learning, pp. 216{224.
Sutton, R. S. (1996). Generalization reinforcement learning: Successful examples using
sparse coarse coding. Advances Neural Information Processing Systems 8, pp.
1038{1044.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press.
Tadepalli, P., & Ok, D. (1996). Scaling average reward reinforcement learning approximating domain models value function. Proceedings Thirteenth
International Conference Machine Learning, pp. 471{479.
Tanimoto, S. L. (1990). Elements Artficial Intelligence. W.H. Freeman.
Terzopoulos, D. (1986). Regularization inverse visual problems involving discontinuities.
IEEE Transactions Pattern Analysis Machine Intelligence, 8 (4), 413{423.
Thrun, S., & Schwartz, A. (1994). Finding structure reinforcement learning. Advances
Neural Information Processing Systems 7, pp. 385{392.
Veloso, M. M., & Carbonell, J. G. (1993). Derivational analogy prodigy: Automating
case acquisition, storage utilization. Machine Learning, 10 (3), 249{278.
Watkins, C. J., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning, 8 (3-4),
279{292.

104

fi
