
ff
fi
! #"$ %
'&)(+*, --.0/2113541361

789:; <=,,5>
-
1?A@9 %&<B.0>
-.

CEDGFHJILKMFNPORQTSUOVHJDGF0DWQX+HJY[Z\QO^]TX_Y;OV`bacOVDdX
e HJfhgiOVj0NVXlknmnHJYoQpF0OVH

e jqSrgiKMD

sitou'vuwowyx{z}|~u

dpV2+=ff

$5)J8dipA$+)0
R
0B


)_#8A0
6ni$56

)p
Aff 588A658
B6n0685V5i586
000[
5
AB66 05688$Ai6J$5PP5Vy66558+A#5
5
A##5ffA6
#6A+580)0o58 6!W5B 5
A688
V55B5ABA5_ 5
A6G6 !5p5+65d5A!o;65
i80 550665c 856558_Ad
0
5
A60
5
A6
_c6505
AG6558)50o6255856
VAp6 5
A6B88c56AJi5M5A==5i5d5
ff 50PAo56+8AA
A5882RcP6 R

056V 6)550BA#5
A6A88;
oJAJ5!0c656ff0i6A'[8rA
5 580P;
oRo86UA$A505 580A8A+
8!i !0V
0 5!A !o5J05 580oB685[5506858 6!ff
6A!o5U5Pff56=
A56A;6i6ffA Pi!A6
6!5nJ 6$2;
8)566856A566868U06
ff8 P05
ff
fi8



_


"!$#&%(')+*",.-/ 0 12,43506%7,.'0489%(*:!',;!=<>"1,?;-=%('@)=!-=A*B!=<C-/">"%ED 06%-=A%('F>G,.AA(%7)=,.'04,=H;IJ3506%7,.'0489%(*
K-/">"%06LA(-/ A78M%(N!=">"-='O>P<Q!=P",.*G,.-/ 01R,6 *C%('O>G,6",.*G>G,.S+%('504!'*G>G -=%'F>T*"-/>"%(*G<U-=04>"%7!'V"!=W A7,.;*YXZN[.\*]^
-=A(*G!_06-=AA7,.Sa` b/cdfeUgGh/iQc e5cNj6eUklb/gmndfHop'q!=SR,6r>G!s-=0 1K%7,6#=,t>"1%(*r-=%r^Y#/-/ %7!LK*r-/"!-=01R,.*u1-n#=,
WN,6,.'_>G %(,.S^P* L0 1v-=*;D A(>G,6 %('R)9>G,.0 1K'%(w&LR,.*xXU;-=%'A78y-/ 0u-='SsK-/>"1s04!'K*"%(*G>G,.'06%(,.*]rXUzx!'F>"-='-/%{^
|.}~$R

zx-=0 OP!=">"1^

|.}~=~O

zx!1R,.'KSR,6 *G!'^

WK-=0 O>G -=0 ;"!ff04,.*"*XUY-/ -=A%(0";IA(A(%7!=>6^ |.}=/ffff

|.}==&N

,.*"* %U,6
",=^

|.}=} ]!=%( !n#=,.,.'O>"*>G!V>"1R,

,.01F>G,6,.-/ A^ |.}==&

%('K*GWN,6")R^ |.}=}=& J"!* *G,6.^

|.}=}= ]HT>"1R,6T!="V04!'04,6 'R,.S+>"1,01-/ -=04>G,6 %(.-/>"%7!'r!=<06A(-=* *G,.*T!=<N!A78ff'R!;%(-=AK"!=WKA(,.;*6^FW -=*G,.S
!'x>"1R,* %76,!=<J>"1,.%7YSR!?-=%('*MX
XUR",.LKSR,6.^
`6Red.j6eX

,.01F>G,6n^

|.}=}= ]Y!=Y!'>"1R,*G>G L04>"LR ,!=<J>"1R,04!'*G>G -=%'F>'R,6>P!="

|.}~/ ]^A7,.-=SK%('R)r>G!>"1R,?",.*G,.'O>"-/>"%7!'@!=<CSR,.04!N!*"%(>"%7!'@,6>"1R!ffS*B*"LK0 1t-=*>"1R,x`6`.7j6
,.0 1O>G,6.^

|.}=}/ ];!=?>"1R,teUg"j j9`.&d4ej4gfiQcOX

,.0 1O>G,6,.-/ A{^

|.}==} ],6>"1!&S*.Hs!=",

",.04,.'O>-/ !-=0 1@04!'*"%*G>"*!=<>"-/ff%('R)%'F>G!r-=0604!L'O>d6j6;h/cKeUi{`:"!=N,6">"%(,.*Y!=<P>"1R,;04!'*">G -=%('O>"*;XU-=*
!=N!*G,.S2>G!d4eUg`neU&gGh/!=VebRb/Eb =i{` h/"!=N,6">"%(,.*B!=<C>"1R,+'R,6>P!="]:>G!@-=01%7,6#=,+-/ 0504!'K*"%(*G>G,.'048
,43506%7,.'O>"A78+<!=*GN,.06%ED0:06A(-=*"*G,.*Y!=<J04!'*G>G -=%'F>"*XJ-='Y,.'O>G,.'R"8ff0"^


,6#ff%(A(A7,=^ ,.')R^

zx-=*"%'%{^ |.}== ]^R!=P>G!01-/ -=04>G,6 %(6,*G!;,Y>G -=04>"-/W A7,Y06A(-=* *G,.*l!=<"!=WKA7,.?*X#-='
,6,6+




,.01F>G,6.^

|.}=}RK

%7"!LK*"%(*6^ |.}=}=&N

!&!=N,6.^

!1R,.'^=,.-.#=!'K*6^



|.}=}=&

,6,6 ^

zx!1R

|.}=}=& #/-='

|.}=} ]Hl@,",.*G,.'O>>"1,.*G,

",.*"LKA7>"*T<L">"1R,6%('+>"1%*CK-/N,6.H
!;,Y<",.w&LR,.'F>"A(8,.'04!L'O>G,6",.Sr04!'*G>G -=%'F>"*T-/",Pffc `neUiUb=c h/O04!'K*G>G -=%('O>"*6^ff<Q!=T%('*">"-='04,%'?N,6R
>"%(SR,*G8&'O>"1R,.*"%*X{-='* *G,.'^,6
)=!L^Y!LR)L%(,6.^%(A(-/",.r^





-=*G>G !R^

|.}=}/ ]!=%('

, --.o %% !<6
<TB0
Pp
!fi:y
l9! %&% &0%W%0 <$



!'*G>G -=%'F>!=)%(0

fi$7

J == =;;(J=?.OG.R"ff"M6l={7 .==lx=OM4GG =(OK=G.?G&"G.;J=(=/=.
GYK4"7= 4GG =(O"6R(6(RBQO@UP= (R.=ffn @Uu=6 . (G=ff
J=R6N=R..===FKnN@R"6.;=R.KGa=(RR.==

(KN6. PG"RuK4"7=4GG =(O"ff
.(/"7YR6u ff".G.O/ fi /""(=
U4"7UR="(4RG/" "=M= ffY4GG =F"ff
CG6MG.4"7K&(M=&
x="".6(".7=PG"R2R"N6""7.!=._G@R.

&%'$%()ff ff*/,+@u="/K72R-

Oy@Rff

7ff6=C4"GG.4=#"$

K/6PKR6;G4K7"76(M7ff6=4"GG.4/G

=OG6.;R+.R(GG.4r=G(R"(M=

;/=.R.

.= 76;G0/ 1
2/K "63P+'/ =4G6 546x

"R G6=JR6/(/K7.6 6=(7.879:%ff=KxR.=G.<;>= P.uR?K/7RR6)P="
(@K!==C4 (GG.O6RR.r=FV4"("G.F(KG"=F"/"7V=A;

6=rN((R./7.&G.KR.+G;MG/

(R"(&"?&(.rRG"=O"(/"7r= 6Y(B;<C9D"FEGH5MIKJ-J(K(LN"G."G.YGG(=(6=

"N6""(.C="Gff6(/G.8(M;
+@R.

=r:K4"7=4GG=(F"f

(FG &K4u2ff

6RffvQ=+G!ff(Rt=O_U4"7=._&_R.4N"(7

(OG+)P"RN =K7.;ff
@/KG6/ (?+4"(GG.O:(G"=O"(/"7=R O=:G66=K".4
.ffG.(RK(:K/""(=G"=O"(/"7+G5"(R"7tU(uMK="&G ="O"6?=R6f

V;=QPV67t67./7@G</24"GG.FM(""=F"(/"(=5"&=G66#B


".;=.FNR.F"=F(B(""546=



RP6RffRP3 .G.FSR6"P(R6 6Q="P=(="P567.O

"(K4:R:"&=G6(T="?=("!46=



R=R6r=9N.4MU(_GVN(F5Rr(5/6YK(7=u"V=R>P="s.=((RW7

4GG=(F"RB('0 G."BG;;9N.6Q/ fi"N6""7..(:6RfftR=7xXK(7.YBR.,EH$H
R4GG =(O"N""."JR!=.ZK"N6"r=[K"N6""7. Rl=(G\BR.;K7Z%ffDZl=R.
RR



K(]KN6B(:="=K!46.t=B(7?ff
BG.4"(2^/KGB(OG"&K4.R;K= (MR/ 7"(:=

R="/"7K 6/ (RtN6\B('_Y((JNVG.y(tV(7?(RR>+@+R.yR/ RVK4"7=
4GG=(F"=;"6"` ff&7K#= M=K;7"J .(/"7#Y7M(# 6n



R(=G /"J=NR

G.4"7U .G.F"R="(Y= G&6/G.>Y7U4"7K=4"G =(O"6 (/"M;=((Vx(
= XR6= =a+@R.9(FG &K4\K!==:4 (GG.4u=K=cb?d1e(fgenY=7== 7

=(7?(R5G

=K7ff=CRC(/GG6J(MG.4"72h&F6="@".G.O"(R(MG.4"7jiR: 6 "7."&NK7.:GBBN6
&K(J4"GG.4=#+@L/=(76"NG;6R&;K=G.;?RG"N6""7.=G5&(.
4NG.5=K4"7= 4GG =(O"7;=@7Rl=("7=RRK4"7=4"G =(O"

k`lmnpoXq)rtsur)vwxnyr)oXz
{|g}`~S$? ~uxKuff$ E7'*;N6<$%6y3 8ff*5<>'Y:`F7


Yff9


?[ffffff-SIffffff-SN>%F%ffffj%]dpD2E &%'B`F7MEpffS3-y'tffffff<$%

e

ffffff?9[ffffff9:X^$%RF\%ff@\%LdE7(EgH5(%J

&%ff]9FR"F%%'GH5MEHN%Bff7ZE7'KEyffH!fi:tRF%'5-M9F^H!E7Kg%(RpD2E fi$%
'EHH!jf J
%0FU%ff2U%, &%(79E X%? ffffff-yffffff-xffaff70FU%ffE_<%($D" HKff*?E




' &%7E Xxa:t9M NH- g^E7'KEyffH!%L@BE fi`$%6y )pJ
$%Y&]%ff]F7H!E &%6E%%ffffKE)2#^F6 &%(79E X%3`&7RFR7H!E MX$%
&Y%'X%ffff#]FZ#E7()%'KE j" 79N-SX6%G"Nff (* gZF["FE$7'%a9Dj&NEH$H*2D"FEpKffH!
EHN%J

-?

fi[XXU@XX'5FyXy0X8`N y&5 &XZg:

,'X2c<'6Sff5!fi&R&(9X19'X]ffjff59j!fipB&ff:&:ff
NN2&fiff'Y9x!!UffN9gR&ZX!aNNZpN99yff^N&ff



!&
RffN9g&RpX995gBN8pX599N^XXNR$p!-B&jfiffF !

X1#ffN9g5&U&M55pNfiB&8xffff5UxM& & 2&cN'$( <9'Xg



Z'( K&\GZ\G 'fi!#"L'$%$'&A(p)$*<'+
,$.-/fi0+1X320465c'78$fi(9(Q?8y:%F7(G_yB&)W,fi
!
. K3fi$'
; =j
< (?
> y(@
B( ]
F 7( @ 9- 7j
,6
$ffC D2
9<%Q 0E!
9

gyG

F . K3G0#$B6Gjy^GH"L(6-0+81 %2I$J<fi&#'Bj)KG,82-.LMff B@
;
< G, 8-2
:
N g $V(OL Mff \
$ 4Pp Q p.3
0 XaG8g0GOc
5 '7SRT<&
F
y3I
fiUM
, 6VDW@
> g'$fi)K2
), -2 YX &N$L
$ (
:VCDg ( \Gjy^GM)
ffVZ<F [-0+8 1 %2@
F
,Y2=ffY03B\]^2RG_4`1BF''a<` a(YbVC3G2yc/$7@`Q@d$0`yKZ7

,
4 O0`yKcffVI$#0#?0`yff_F':8$e2? 0#$gUp.p8F7Hf(
Mfi
C '\),2^B!g&-=jCfiBjFXDBZRt\BW -$'%$:(MBj(:
f (e<%fi
F\
C Bj
'F X3@
g

'W <G



NNX5

F g

ihjF xp96FX35X5ANRX@pX995g

ff63NN\a&:xR

&R9ffL`&]X\X!5

lk,mino7prqts8uvo3w=oVqxsIuxvxyinzew={oVqts8uvn{{xq(zivoVqts8u=|}rzJmin}EmUqxsI~



F g



_r8c4B lk8D]R`eWQ$[;Y<ZRV;WI$_>y(]RD>(WQ~
" (3$[1& (p ,$H<
* 'ygpG)$[-0+81 %2I$'5c'78~
_MIc k8L
_Ur88)c k8xL 9 3fi$E;#$GrXB$X F,~
t8()B[8c k8xL :L$J.$E$'" %~
N g $$X FX'~
trI'=B lk8L Mff $:
&!:X\N 25N


kI r8c4QQ8MIc % 8M8^BQ)r8U^B % r8c4BQ4r8'=
)r8U^BQ4UU8^B % )r8)cQtrI'=B ~
&ff2&ZpN99ya9x!!0ffNya&ff!!a&ZN
5&a5,6ffffX'
9UF5!ff&ZN
g9!6&ff5!Rx!p&9&`&^5&N fi9x c&2X 5&ffS&
N
Nff!N 9W5&^N y9'!ff#NcNZ
ff5[5&N 6&^N y9'!ff
5
Nff65&ZfiF
ff X5ff!LpN99yfft[ffU&fi& Xff!BN<&\X 5&@N5pN995g
55NxY5N)N
ZgZ&Yff5!YXN [&]N y9'!'5!9Z2&YN 5N3p^&Yp&
XNNN^&R&
Xff!S&\N g9!p8&R&ffBXN

F g

F g

&R9ffL`&]X6pN995gL5




r8c4BQ8M8^B lkURV K3fi$b"B(WQ$_RV K3fi$b-/fi0+1X32%WI$#R`;Y<D$^"L'$WI$R`;Y<fi$b&A(pWQ$
RV;Y<D$*<'y$pGWI$.RV;Y<D$^-0+8 1 %23WQ$RD
> y'$%$^-/fi0+X1 32%WQ~
; <fi$bL Mff WI$RV
; =<D$^:
N g $WI$
Er8c4BQ4r8'= lkUR` K3D$^xL : WI$_R`Y
RD>y(3$cX&N$WI~
&]X\ff59<5!pX

QI

fiI4d
4U)B8^B
)


8c

r =

8^7
3: c
%3 ^
)r8U^B

%QIfiDfii:=:fi
Q=%%QKfi
3`%Kfi=:Kfi

8 V %3

8M8^B
7
8M%M
c33MdM
=T:`
M7K




rI`(B


[d,:
iJ)MM%^e%()OJ)%bMM)fiZ%U'i
8:I^BQ)r8)cU`#3c%^tMQY ff
fi
,IO8=
,!"I
#$&%')(
*I +,-Q . /
fiD0 fi8 /1B2 354
)r8)cQt8()B[8c lU`x67D^t:ffQY# 8"Q_5!. 9Q31B2,^:D;54
)r8)cQ r =BSlUVtM7fibx&5QY#=<"/>c?I5,=<"/>c?I
31B2 ,231BU^7?54
@T:):6b

,Pfi)MM3bBAtDCJFE%bAHGbJILKYUMINPO0Q6KRNTUW
VYX7Z 3G[G#Ud%H\E3Gb,]:
:,:`:G3G=Gd5CJx KRN ^] K`_ ] Na KRNYb#:,3G[Gdcb Ud3fi3fi>e>d3V0

f ).):d b 1/1,
bJ:?G^MMH:,Ud3 :UgAt?Gd,/,eMH:,:?G^:d)`CJY:
:)h
E83 G^,$
ikj U ] jHb. b 1 1,
l
O`Y:,U4%: X B
E83 G^,m
iK U ] K`:)ffiUMM%b=n
Q K j
b
) Gdb:
Kqpeikj XlU K j a\rsGbM Z :,m
E83 G^,m
ikj U ] j
bZ%,)t:# :,(dt
iKqpei)N XlU KuNfb
)H Gd]d`
ikjYAt::,U4%: K'c:,Gfi)MM3bs
Q K j/)c
iN`#:,GfiUMM%b=:
Q N j
v M:)Uc :!
A(M"
w Z Ux
vj]:)%M:Y:, MYi:,z
yU%MY
{|E%b AHGbL
w Z
C}a ? & a6 %,/ ~BH
Gbb,Pb :,ZfiM ~B& f ,],Vo GdG%,Y
C_[ G[GAtz
Il?pIp&&&5pI&pI"Ha
rsGbM Z :,Y3,#`#Jt
C[ G[G6Ath
"30+)ffk7 )"3" )ff3ffffk ,""3Ha
[ E
v p&&&pI 4 %! AUM z
w Z :, ::do
3
E83b AHGdP
ILK
B
vH
G
E83 Gb)
K ] K bZ3 G[Gd 1B VV > )a HV73 V0

q v aB)3 b)M:=:b:d

p&&&?pei Kqp&&&pei)N/p&&&?pei X b fi):bMM=idx) H
GdHdx& E: fi):M%b= b)& G^),
bB
vH'bE%:bq
yU
=c7
3 G[Gd Z6 pDzF[? & aQ KRN U|VtZ K pei N XU KRN a, ^)M:)fi Z ,d:, s>K
fi/
/ ,
2
fii#3c% 31, / :
fiU:bMM= Z C): 2 s>K0 7DJD^; ) +3c% #$&%')(H
*IJ:
D? &a8




:




:


3


r
b
b

/

:

,



:



`



:


,



(

,



9



f

?

fi



)

:

b







=

f

b

)



:



=

:

b



:









:
wPar
31B3
)^7 ?
&
> V0

M/ Gb):d Mz
w bi:=U 2 s>K0 7D'D^3[tM DxMte &?
)?

fiLHH|HH7[, H )Hc`o ![!H-"


=
[

oe0
:e0
)?
ffD e2
"77












tm7

)=e?2
H 6
) +

))H=

`/!Yk-z,- 2
fiff e

fi 3


!#"$%&(')"*,+-!.!%/*0+13254768 9:;"*
<>=H?@ BAo=$
!n(CoD "Eff&GF?HGI$J0K/L/MI$NDOJ?MDIQPRK/S@NDL#IK#P3T `/!e}Uoee
7 V ;Lz!W["efiff?C
ff
fi@ 73
7 RX?Y[Z6-23e\AHff&fi
!e[D AFL3^]$_`<a C!
Mff&(z3D$Cb C!&3cb
e
fi
Z-3Eff^Coff &7ed?
!oe& fcg
!ffe^cg23h C!)J- C!GA L>He
:
CoVD6&0_
2546j ;!#"$k9ml L#nDo-IpK/qrMsnDNDSRL/N7t-OeoPUuBvWNDI$w5uVxyEqzo\w7oI*MKfo{uVv}| uVx~L F-MDSiNO;Oj7vUL#I vfy
K#Go-S@o5L9PNKM:PKEMDI*o{xsL#IjxP?HfiJ-K#NKET/vxX{ v xogK#QoIpw7o-I$MKfoWxm v xT/vfXMDS
x` v xT/7vXfL F3K#7L;PEn:NDOHoWoL;PRK#P?y,MK#Go-S?q8L;P-o0
J^MIQPRK/S@NDL#IK v xgL;PB fi
fiff e[
o8L puBv|u[x5MDS[uVx|uzv@h?IK#GorF-MDO#OMDq8L#IyqzoEq8L#O#O,J^NDO#OQuzv
K#Gol3[/
NDI*wUuBx5K#Go e3- WM@F3K#GozFRHI*JK/LMDI$NDOjJ^MDIQPK/SNL;IKju v |u[x
!e CH{A!=$
oe e[
Y+5ff&
A!Aofiff aH e e
C!iff
fi@ 3
W- t( C!\-ffz
C!

!Co
AsVY, C!- \r CoW2fi
ff e
ojff
fi@ 39
7 -Y
~ C!t C!&3Co
fiA~B:Y C!J C!&?T# C!

!
fi
fiff e[

fi@ 73
7 RX?_gm-x9Ee3As \Z62fi
ff e
oe"e 3ff
e3
fiPMDo32
fiff e

ff
fi@ 73
7 TV
X?Y
fiAPK/S?LJK/O3FRHGI$JK/LMDI$NDOee MDIfiOff
e3
fi3fiff^C~ff
@ 3
&UV
`X?_

9g4&#%Q!4".:

b C!5=A Eff
fi@ 73
7 eD e@ ff e
oeZ-Y
!> C!\J BW3 e
zZGff ee3h
ceAHfiffffWHfi eD e
eJ_g
cs@ efiAo[>Co3Z6&
ff&eA/fi E ff^CH-3 Co9T/&23

fi e
fiffz Co-&e
ff
3 Co"9
7 eGAofiff e[
} CH,D6&X?_ hCo-cE&^ze sAofiffYJ- e9J
A! eff&ecYj C!i eJhekoeA se3\-fi-_l5-3&YL
Z eff-g=ze 3 C!?tff&
fiffc6
C!-cez3
!&7J- C!GAfi-Y
AY3fiff^CY, C!-csA!
! > eD]3B9
7 cff-ff/fi
3 Co@ff&=$ff&e e[l
C!>oZB WZ>e3AYfi
! eDZ$ecE_u0_ -_VP-oNDI$K/L/J-P C!>ff
fi@ 39
7 -_LhCoVA!=$ff&[
fiffc"
03&?ffJWZc\)[/fiUT
AY3>J@ sr Co5Yff
7 ?Xlefie -_shC!-ccQCoeZH (JWff-[el
ff
fi@ 73
7 s(CHff^CH eff&[He& e-_shC!meefi (
7 g D5ff-[?:
C!

!gCo
fiAY C!goee
oc33e Cfi3@ff&=$ff&9ec|AHDfi azff-[eY(ff
@ 3
-Y
J3eYff&
h Co
c3
o&337e Cfi-Yo

5 C!} Co&Co
fiAYQ C!)ff?Coff &d?
!ff-[^
3 ff eDZ$ioeZ[-_(
Co}ff e
n
C!$@ lz CoHD&YHt:,eQfi) CoD -Y
fi
fi[" C!&e:9>@ eD AY Co3
!- D3]G:e>Z
oc_
:

fi(!*e

fi{^@E-^E79fiEQ@^fifi>(fi?sD@WVQi--W//->BfiG
h
@@^97[@fi@E@-$}fi^^Q-QfiWgD^3fi@@$@-^Q-
QVfi ,rfi@h@{WGfie#)(zU@U9Wfi!(eg-efiWa^fi#

#*^fi#\ D;-^fi#\@@^-khQ-fifi9W\Wff
Ge(B#
fi !BiQ-^-@fi@ fi GV#hQ@3*-e$fi@@^-
(BGUQ-D^Rffi@fie^fi59fi7@7@Q^^a-8?
}fi,(z i>QEs^efi >e>fifi^DQ-!
Wfifi-E5WGQ (e
--^fi" ez*-9 #Defi-^fiQ>@3Qh-@@@^8@Efi^h$ &%
' G$/ (D$
*)D/, + ' G$0/ (D* W (D.
(/ (D/Bfi@@?71
0Q8DQ@r@@^
^@ee{@fiW@ Qzfi fi 2 } fi/ 34D;- fi / Wfi5#* fi / {G
@^??z(z ^fie-}D^Rffi^@@fi fi GV#hQ@3@@^-fi(^9[G
^@@h(eiQ>#^W-(fie-5
hfi@fi-Ufi)Q@g@^fifi9Q)(fi9^9Q7;fi@@^97U#)(^@W
e G-jfi@@\}G-e}@gD^7@-W@@fifi^*3eGQeD
^eV@@eEW3-h fie-
D76z--p)fi0(3fiD3Dfi@fi~fifier@@fiseVQG
@^hD 89(:; (D=
<D? >5 @
DA6B--fiCB}^@-i:EQ0 fiDEeQ@@^EDDie9EQ~@Q
, DGFHfffi@fi5fififiV
fi@fi5I


@ ^->JfiDfi@@^Efi7$KV--L:?NMPO$O:9
@@^-4Q*#zWez:D?e4RTSVU9R4W7--:fihYXNSrfifi@UQ-
MGVi
,:fi>Z
X W QsQ-ff
G9@E^efi5 -e\
[^]`_ / a*-^
bcQ3 b:};?
fi^eQ-3$fifiE@Q,{Q#^fiD{Q\Qeff
G3seGQ9jW
/We fi *, deFfG@@?g
QWW@Q!(fiDQ--^{fi@@?7W-@
fiQ-[-^UQDefii^@@^eQ)Qfi[9i
hkjfWe-@

mrfi@z)eGQQe?efi @{@e!
n`opfir(e\;k
q./?/ (D$G
\l efi9(
@^3#(fi?Q#ZfiD^fi@@?7fiDEQ3^WErMPO$O:fi@@^97?s0?
tvuxw y3z*{|y~}y3v3&rz5~;{VY4{9559V ?VLu
u=z5}x5}}y9/y?9vz5/y.*;{}|;z*x5vz5{`3/|iiz53?ffVz/3vZ;z5}?L;=Y/}/z5{V/9{*}}
;}z*{1{Vv}|;z5/}v$VJ;/{V{}z5}/3{VJ5v;/z5/y3i}x3/?z&$1N${z5}z5v$/{9$3/y3v}1{i5V$z,
z5}1/y}/;i4}/y{Vvi5V$z,Z;=Y.ir"9$u
$u. z5?Z{v3}|z5Y 4z*}./?{VvffVizYi$!$5z/}`}3;/}1;/{Vv}V{3/z5ffz*~^$3/?z*39
/y/V$z5}/};z5i5z5{z|$V/z5;/y4$vZz53}u
u.g3z5;{}|z5 z5}Jr"/z*yz5vV53i z5}J}3;/9z/yVJ;i}Jv;*3}z5
ivL53}1 PP{vff/}VffV5J - z/yV4y}4?4iv}}3;/}z5 4z5}
}3/9rvL5};==u
u.`yL$/yV/i;/J}y3?ey9e/yz5}N/V}35z5}N/r3z5?3V,/};!y}T{}|;z*/}
y9ff;/z,kT;N5}};/;Yi}r"/z*yJ";?,?r{v3}|z5Nz5}Yk"/z5vyz;*z/}3z*;r3/?V{V/z5v}
;/4r"/z*y3z/y34rz5}Y}/5t?VN-t? t9{v3}z*}/V9$/yV~zYz*}*;*~{v3}z*}/V9u
$u.;/y3=}/;ffx$}z5i5z5{z,`xz*533/}3z5?Nz5i3*z5{9?/z5v;ff{}|z5/}Lyz5{y;$VffP;
5/z/yz5}3/}/9$T;|;*55v;/z/y5}i}/$z*~z5~yz5};V9u
$

fiJ==T==";P=Pe=x.P".=i

7=9ff^-N`^Y?^9;?x?r"?
"x ff^x99;
P?
"r;1"?1"?x^9
?



?



P

9







`

ff






1
fi
x


















ff







r

ff

^

=

9

9

;



"



?





?









?





/





^

P




fi
!

"
$#&%"(')fi`* ?,+ff^-
/.10

"P??5
4ff
+r-9!`i9;ff?=
076k "ffZ?xJff^x99;
"?""
98):<;ff=;>@?-
2 39ff
rff^x99

2 4A#B CCD$'4*
"
FEH
Gr9 4IJ>@?LKM#B CCD$'?Zff?x / (@4ff^x99;
"?O N,0QP
??xR
Sfi
xxff ^xNff^x99
"PU
TWVYXZT\[]"~
?xxff??O "^ _/ ^P
fix`
!"ff
#9a
k/ . ff^x99
"PM
bdcfe"gh"I
i\ ^;a
D$')fi==xM
j Vk[ ?-
"=i?ff
+ ff^
/ .l#,

?x^m
^
j<[VZ gx`, ')0 2 ;R


<"m
?\ff^x 99x/ "7n +T 4+k^9ff^x99;
"?? "
xxff ^x-
ff?9^o
,=xff ^xff^x99;
"?p
#,9;xq ,; 9^r
,x=ff ^xNff^x99;
"
"9 ^o
,x=ff ^xN^) ')0a
2
4s5??x"M
R K!
?I
# "=ff?x??-^
?,
+ ff^!

/ .ff^=99;
"?) ')0Yixxxff "^xxff^x99
"P?
fiPxx^ ^4J*
3ff
Zff^=99;
"?
fi;5
"P9ff?=
"t
9
0
ff^-t
" -?v
" ;?PC"r
?^9??x? ??x- ?xw
>@?(?4
!?
u xK9ff^R
ff^x99
"P?!
J?-xn +T 4K`^R
9t
$^
A"?H
#ffKyx-K^fizt/ ^^P
fiJ!"ff"{ fiY;ff
+ ff^
/ .|0,00)')fi
R
^x ! * h"?R
"N9
0 u W
RL}`?xff
fiR
Sfi$"^xN "R
^
fip
"9?9 "m
U" ;ff

fi
"?=!?-?;x?`
+T ?9F
"e?*
" "? R v+rC^x^ ~=@H
?ff^=99;
"?
?h
xxff "^x{ 0
#~ff

"fi CC$')fiaI
?ff
+k ?x
" ? ff^x 99P
| fi!

;
"^=
ff^x 99P7
x9? " ^`/ .9*

99^"x ^]
fi,4

^<
nx9? "9
]
) "=
fi
??K/ .<9? 79
xffK
h,=xff ^xi *
39ff
ff^x99;
"?
?^
#!
R'x9? "9


"=9?R
"^<
n"x??P "9

) "=
0 G?=?!
i?H

".h
"?xa
ff.
Sfi?x
?x4
*?ff^xO "99\
x9? " ^T^ " / .9R
x
9~9^"x ^
0~ff?x"^ fi ?
ff^x 99x/ "~ff??O .~H
+T@
4ff^= "99x/ fi(=$O=@;R~)~n/;|@fiS"~?<


fi1
+Tff + ? ^99?xff +h0

1^J,(*a|,,*|ffH*I(,"ff"dRJ
`1h"Sxxff"^x{fi,"?Yff^x99"P; `QR^
"^x 9"P9kn+T~;=9?0YWZff^?"x?
#R;ff9 ';r? ?9"m?hxxff^x1ff^=99;"?fi*+k?! F"5*?ff9 A,?^TV9
TWhL}ATWVXTW-
fi *tZ
ff^?" xJ?~ h#{K(ffK^
fi xRR" ?ff9 ')fi!,^,x=ff^x=ff^=99;"?0
9F_c#()WUIT
' ?R" W
; x
fi R1hc#(v)W-'W ?* ?ff9
?xR ; 0
ax
$" R,x?? fimT
+ 9-?"9^h ?O
x ^x?x!/~/);$>@;

2 ,?F

?9/.#(
O" =",')x
fi R
?q= =Y
HR ?ff9 ; 1
;ffRx
w=q8T V (;>@(qO nOxq>)*
A-\J,(1^r"*v-< //nTW~o>w
#()W"'F (<q$~/(~a(;vWw>F<>(_#(TVqT7[/ffqTY[@ qT7[{!qTW'F~):R/(">$YTV\XT\[O/ff
TY[ XTY[n ffqTY[ XTW|a@ $K
//nTA~> ?t=q8H>offnIx$B>)#()W!' >@;R=(<H@nOo~H>!/~/);$>@;
8B=@T@K
fim+J9ff.xq= =~ N]R;ff9 t; 0 5"Tq 9R^xi?xZB= =$S"i
2 / .9x ^S
?r95+kR"; ff^?"xr-"m^"U
;9/.|0

> ??]=B8F>@(qO nOrx$B>*v#()W!'9 >@;R@//nt=B8
-\J,(1 ~:R/~ S=B8Z~F

_~r>I!/~//;$>;*18)q=@>@;l? H/;*=B8MoKM~aO>@?(? O-R"-Q (<qH$=ff~r;=O~
>q= =~ Y~):R(">JHoKW~aO>@?(?^
R" x (m/p$=ff/~F;=5$~/>Hq= =~ JH
~):RA(<>5 " LK
v{nO J$ Q


B&Y FkS ff
fi,&)@
&1m&R1R!"
&#m7&|$R"%R&
')(+*

fi,-/.102

314!56567893):;7=<

S!T 8UP4

UV7=<

314!56567893):;7=<

3ONP4!89Q!5O:;7=<

S!T 89UP4

UV7<

3ONP4!89Q!5O:;7=<

3):RQ/:;7=<

UP4):XW17<



3):RQ/:;7=<

>?A@CBEDGFIHffJKFL&M

UP4/:W17<

>/H?A@CBEDGFIH+M

Z"[]\^I_R`bacEd=^OeO\f_&g!h6ijgfkOlm_&nVnfopR`qornfsPtuwv
xEyzG{$|~}/zVz|~}/~yfV$z|})9yff~|z|})m=$xyfx6|}
nf_
314!56567893):;7=<

S!T 8UP4

UV7=<

zVz|})~xyfzG{P|})ff~yfV$z|})VPExyfxO|~}/P9yfffff|Ez|~}

g!_R`enfoRijnhg!oR[e6`f
xEyzG{$|~}/zVz|~}/PExyfx6|~}/~yfVffz|})9yfff~|1Ez|~}

3ONP4!89Q!5O:;7=<

[]phO_R`O=`+ljee6^Io[o[;pkOnfonh6g!oR[]e6`fc


3):RQ/:;7=<

iOgfpkIngfk6`+ponf_

[]koRiO[]pnf_&lI`q_&[]kO\p&^O&i#oRiOg!o



=$xyfx6|}


=$

xEyx6|}

UP4):XW17<

xEyzG{$|~}/=$xEyx6|})~EyVffz|~}/9yf$~|z|})zVz|~}
[]pkInforAnh6g!oR[e6]`fcO[or[]pkInforh6_R`O=`+ljeV
Z"[]\^I_R`IcAnh6g!oR[e6]`gfkOlkInkmAnh6g!oR[e`nf_&lI`q_&[;kI\pnfsPtuwv

Z[\^I_R`a[]]]^6po_&g!o`+poRiO`kOnfoR[nkOpKnfsPlO[_&`+o`+lp&^IeO\f_&g!h6igfkOlnfs$_Rnnfop`qoiI`q_R`fIgXO=_Rnnfo
p`qog!hOh6;[`+lonjn^I_`Igfh6`tEuwv=nfoR[;`oRi6g!ooRiO`g!_[]pnkO]lO[_R`+o`+ls_Rn
9yf$~|z|} OgfkOlkInfors_Rn
gfkOl

9yff~|1Ez|}


9yff~|1Ez|}

9yff~|1Ez|}




~yfVffz|}

~EyVffz|~}

oRiIn^I\iK`bi6g+f`benfoRi



~yVffz|~}

~yVffz|~} $iO[]p[]pnk6snf_oRiI`pRg!f`AnfspR[]h6][]q[owc

oRiO[]p[;kIsXnf_g!oR[nkj[];1kOnfoe1`^6p`qs^6~[]kjoRiI`sn]n[]kI\I
[oRig_Rnnfop`qo1K`gfpRpn=q[]g!o`&!=f!~ghO_&nfh1`q_&omoRiOg!oqgfke`f`q_&[6`+legfk
nf_&lI`q_[]kI\nfsc
mV"X/;!qV
1
~


B





#

B+B!qqq+B

;b!X]Rnh6g!oR[e6`A

!




B9BE

knfoRiI`q_EKnf_lOpqoRiO`r6_&pRo)g!_[]g!e6`+p[;koRiO[]pnf_&lI`q_&[]kO\g!_R`oRiInpR`iO[;&ie`+nkO\onK`oRiI`+kpRg+
oRiOg!ooRiO[;pnf_&lO`q_&[]kI\[]pRPReAmOgfkOljgfknfoRiI`q_r!g!_&[]g!e6]`
[]kjoRiI`bnf_&lI`q_&[]kO\m B9

[;pnf_&lI`q_R`+lje`qsnf_R`

B p&^O&imoRi6g!o

B

B
B

iOgfpg!or`+gfpRoEnkO`gfk6`+ponf_

B9

p`q`Z[\^I_R`V

ff
fi
fi


!"[fnfonkOpR[]po`+k6lI`qh`+kOl6pnkoRiI`gfpRp&[\k=

Z[_&ponfs9gf]imlO[]lE`bqgf]ff[]oE !f&!OX1q

$# &%k`qf`q_R

`+kVonf_&lI`q_&[;kI\

po`qh

nfsEoRiI`gfpRp&[\kO`+kVoqffoEnm`+`+`+kVoRpiOgf`one`lO[]poR[;kI\^O[]pRiO`+l~c

6_&pRoRfoRiI`p`qoEnfs~oRiI`sXnf_&`q_&[]kOpoRgfkVoR[]g!o`+l)g!_[]g!e6`+poRiI`I!6GfRq+;gfkOlp`+nkOl6oRiI`

'!(*)!+-,.+-/10324/658749.:+624;=<>74+19@?03/A0B74CD<=9@?FE +-EG74;=?<H5I03?:J0K:F03A70B74+1LK;M<=9K?E +1EG74;H?F<H5ONQP;=<CR74;H2ffSUTV;W0324/QXZYW[K'@']\>(
^F_R^

fi`badcefgheij@akj3fVlminmoqpFertsuviwnmfxakewixyz&g.{Z`|j

}x~]V|F3Q~W~Ik}vWB}ZBkW~ bI}v~DR}]$}v3kx~R|Q|]$}vBkBW~}v]|vB~RB>~Rh~R~}
~3k}vWB}ZBkW~W~R}vBx~}x~]VF3Q~W~k}vWB}ZBkW~ x~3Bx~W~W~RBb
]$}vWW@k}mDv&BBkRk8B$Q~W&k!whBx~]$}BkWW~}v]t@x~BVDBx3}$}QR.8vk]$}vWW3k}Z
k|Bv8Rkk~Bx~IwF$ Bv~W~RR
F*xw
vx3Bx~R}BvkW~]BQ$} .h~kk.k}ZWBVvv]~Bx~x~]*}vQBQ$}wQ]$}v3kWW~}v]k}BW~RR
]$}vBWW~}v]~Rh~R~}F@kQ~RwBx~}twQBxW~RM}vt*}vkQQJ]$}BkWW~}v]
Bx~OR
Z w h>WW.Kv] m3 O8 * |Z]$wBQQ~
vGw =.v- K @$VK&]. * Z|KdFGZHBb-F
R4GZ=BZh twGBb
x$xD]$~}ZB3W~3$Bvk.x~]*}vQBQ$}
x~O~]xkWW~}]~O]$}vWW@k}mD~Rh~R~}qbb}vMkJ}xD]$vkW3w.Bx~B~kBk$}-
&~v}vQ~R33
J I}v DB~}xb}x~]~BB3Q*~R3~}mRH}vR IOvB~8h@3JV$G R JQBQBW~k>
}BvRW~b~BBk&kkW~Bx~QGmHBZ48Bx~B~kBQ$}v8WBx~Fkv~Z.8Bx~
v$k}=
vkhx~]w}vQBk$}&!&~JW~R~}|QVR~R@BQ$}WW3$}x]$}BkWW~}v]q }v]$}v3k=
W~}RQ~@@
]$}BkWW~}v]*QMkvx~hWBx~|B~B*~}ZFkv~hk}wvWMv!~8BxvB
}tm
]$}BkWW~}v]*}Z]$}vBWW~}mk}vBB}mBkBk$}|HWq&~~]VW~}vx~W
BQ33kQ~xx~R3~|m
}x!}xBvB]$}v3kWW~}v]~vQQ~}ZffW]$}vBWW~}v]$h~.R}v~vv]~hBvB
]$}BkWW~}v]R}~B~R3QWW~}k}W~R3&8|]$BQwkkQ>w
bx3v@K w ROx|>WW.V KGwhHW|mw&MW|m
3|$W3JVG4]

~QB~xt3ktBvIW$~]$}vWW@k}mBIRkk~Bx~wFJ~&wBBkRvkBDW*wh~
}xx~]w}x~Bv~

Z w
fi ff w R4|h |
$.Off *bff|h h
OwQ=ff w ff R 4| q
ff w
|Z=@JVG4]
=}Bx~RDb3Rv$Q~}}mvB~R8BvW~Rff *}v]BQ$}v]$}WW3k}Z8 |W]$&k}x
$x B$
ff bI
ff }|
ffkQff kh}v$}vQUQh}Z]$}vBkWW~}Z
k}vBB}mBkBk$} G B }v }ZBx~RF@kQ~t k} ff&8Bv~RB~~]xkWBQ~W$}x~
kx~IZOk}Bv3BvGZHBFFR}~D~]W~}x~W]$}BkWW~}Z.k}vWB}ZBkBQ$}UGZHBB
|>WMW.$$"
!JGv}v]BQ$}M]$}vWW3k}ZBRh~&RkhhBx~K$ #$}vq.Bx~t4% # hBx~
Q.| |m

&(')'

fi*+-,/.10

6:

28:

5:
287
67 57
97

243
53



63

H1M_`Xp1AM
a/

8


8





;=<?>A@CBEDF@HGEI
JHKMLONMPRQ)S)TCSUfi>@CBADF@HGIPRVC@HGXWY@HZ
JHKMP$VX[]\_^`PRVHNaPbQ-STCcS)deVfP$VgThL_d(Tji/ThKMLOkNal)PbmnVfiSU8opBfG-qrTFKalT
P$Vsut B DFt Gjv qwsux B DFt Gjv ldayzs{x B Dh| Gjv l}R}pKalAQ-Ll~VFMNMNS)mFTPRd
Z
mFLVfNLjcTFPRQ)L_}R`t Z qet Z l)dayx Zjv
dOThKMLS)TFKMLjm8Kal)dayqAPbT8P$VpdMLjPThKML_m/NalTFKc_S)daVFPRVfTFLjd(TjqEVfPRdac_L
sux G Dhx Zjv Kel)VdaSVFMNMNS)mFTpPbd B qEdMS-m8l)mhcwc_S)daVFPRVfTFLjd(TjqEVfPRdac_L
xGKal-VHdMSVfaNMNeS-mfTPRd B

4


;=<?>A@CBEDF@HGEI
JHKMLONMPRQ)S)Tc_ldeyPRyalETFLS)U>A@HBDF@fiGIOP$VC@HGWY@HZ
JHKMP$VH[]\j^P$VdaSTNaPbQ-STc_S)daVFPRVfTFLjd(TjiHs
tMBEDnxG v Kal)VdMSVFMN
NS)mFTPbd Z


fiM`XMpAMe4E

)rp1pEpX4EA8aEe]AM

8{(_jufE8E H4E
eHfiu-{8w {rE 4E{j`_ ~En {j` r8r_r]M%E1
(jj1A8raA{e EEE Er
p1"AM_ezX {aurneAue
4 ff

~8{(Mn
fi EEE
fi
nu"]EErr_r1p_/M
~rEfi 81OMj1M
O4E{`1
_1M eO_Ej H1M p )="p1AMpe4Ezp
M81AMa4
1zAM
/ 1M E`A1Mp
AMp_e
4M_~pr_E r4E1Efip1
A811M"!#$
! r/--&
% E] {u(]' E
A1M4 AM_eE4_r_AEjE) wp pr4EpAMe*&% p
p_/Mfi~E~AM1MOr
+
A/11M!#-,
. AMp11M0/E_p1{O_
1fi CEF2EE{
81AM
a*3
. AMp11M54p6!71A_rAM_eCMEfiMMpEC8nrE9
j ff
;: =< pz p1O_e>
?A@B

fiCEDGFHI0JKHLMDMNIPOQLRQSUTAHVWXLYRQIDHYLZ[6J\]CffM
^+_]`acbed bgfYaihjkmlffngoqp*rts6uwv*xUy{z}|g~{zffK1 Ns*p*srNqPs n*p
sAnr6prY6qQpner#* *** E"sqrYsp*PrY1 n rYsqNqr sqnerYg+ff]6n 0gp
*qeqngr]+gQpNpNqrYng nqrsp *s ngKp
Q K;~=#ff ff sqrY;K KneffYngoAff
sqr"oAsqnsQp(~n gpsq #p*ff;sYngoAG
# K;~=5ff;ff sqrK;(6 z
sqr"oAsqnsQp(~n gpsq #p*ff;sEAqrwpYneoqG
#K
]
rY"oAsnsQp(ne gPpsq]pff;s7neoqG
gPp*rne NsqgpN+s;*gPpuwvxwEsqrY;ynKYngoAffqrnpr
616]PU*P* E**gQP*
P;uwv*x*#&&Nqg(*c5 w55pp(*
6ffYngoAqrnep*r*A#pwNQ0+Pff*N
7N*--*(U7*77]K1NYK
0nggpp1KsYngoA&*p
^+_]`acbed bgfYaihj lffngoqp*r suwvxyz |g~ N sp*K "|g~ NsqrUNqPs n*p
sAnr6prYqQpngr]ffq***ffsrY sNqr sqnerYp* Epff*sqyne]
Nw ffN*n ine sneoqE*p*ffy
h j bfd ;faff
b
d1_#adfi gf
_
*uwv*xUy7Y]* UqE66
*{N+c
P K6ffQ 0**6w**+
NK Uw*gw**6 N;YYngoANrnep*r
*A#puwv*x
^+_]`acbed bgfYaihj z|g~ n;sqep"gPpYQ*gPpuwv*xy z
|g~ ;n
- q" !$ # %
#c
]EsqrY 6 q" !$ & %
& +q(
' U
1EsrY
Q
# neffYneoqffNqrnprgsqrY
nA* )gPp*p 1p(r,p +n.
-YngoANqrnprUg#s5
fi .
-/ sqr
]
zi
-
0K*eNQ E;w*w5QE*
wuvxyQK]P
g (6PE q*] *
1K3
_ 242 5h7j 698 pyz |g~ NpUsuwv*x 8p* &Uuwvxwy z |g~ fi

sqrYy z|g~ :
p;YngoA NqrneprY sp*;s"YngoA7*p z

;=< ? >A @ C B wfi
G7z
DFE E srY+sqrqPs npsAnr6prYffQp*ngr] 8 p*ffgPp uvxH
G z /
K G z J
Nngg+ G z J
zL# G zM# N#
|g~ G z J
q" !+srY(.
Gz z L &OG P
z &
& - 4
q" ! '=G 0.
G :
G
4
Q Pp*r(5N H
neEneoqKqrnprE gPppsq6p *pgsrYsqrgQn7*sq6pNqPs npsAnr6p*r
qQpngr]qsqrY *pNNqrP H
GR
SUTLV

fiWYXLZO[]\

^H_U`a`b$cedgfihj$kmlnk,oqprsl,outlnvHwyxk{zx]|}p}ln~Cpuk$xkmlmflflnk:qfqp}lmfN=$
p}j,f:uj$x]fC}ff


?

L

ukk,oqprl,outlp}jt}s
l,outl. e

7 } 3





4

7 }



P

l,oqfj,fp}j,f

w

4 ?

L
xvHw

qp}jt} gul,oqfj,f.fCqxkml,k



xkzx]|}p}l{~Cpuk,xkmlmfll,oufj,f fCxkml,k









xvHwfiflnxe L
k,u~o


7
? ?C p}j 7

w
Ofx]l,oqfj

7 ?

t}

t}uN





t}kv

p}j


xvt}ugvHp}jt}e nfffl

xkl,oqf:zx|}p}lp}

J

7 }

U(qp}jHt}

7 ?



l,oufzx]|}p}l.rYoupkmfl,tj,}flxk.



l,oqfj,fp}j,f




w

~Cp

tj,f{~Cpukmf3qfl,]y



x

k,u~o4l,outl

lHxkl,oqf{k,t

ztl,x]]f



fx






xk

zx]|}p}lp}/

mv

fl(ukqpLrPk,oqpLrl,outlv







]"

(
w

qj,l,oqfj



w












*

e ]"



*

9 ]"




.






w






t}u

k,u~o l,outl(



t}y







w

k,u~ogl,outl

w

t}u



}urYoux~o x


w



dgfqfu~Cfj,p



t}u






C


U






w

~Cpukmf3qfAl,}.
w










w






kmp

w

qp rfnout|}f









U

w




t}u

Lfffl
w

w arJfl,oqfj,fp}j,fout|}f

w




R


w w

zx]fk


.

t}ufi

UqrJfout|}f



vHw.xkzx]|}p}l(~Cpuk,xkmlmfl







.


L

zx|}p}lp}

v

w Ji~Cpukmlmju~Cl,x]pp}



t}



Lkmp



vHw

k,pn


p}j,f}urJfout|}f

kmpqk,xu~Cf







.





?








w




YpLr




oqf=w





U



l,outlRv

w




vHwt}uNv

{



vHwt}ivHw

C

vj,p

U



t}u



dgf~t}yqprzuj,fkmfll,oqfHzuj$p}zOfj$l,x]fkJrft}qpuu~Cffixl,oqf.=fxuuxup}l,ouxkk,f~Cl,x]pa
^H_U`3_L%ffmu:,}.Ov


v
fffi $}q*Cff}Cfi/
U$ ! ":C#AC $&%

ff}C fffiff}C
^H_U`a`b$c('qzzOpk,f{l,outlv
xk

l,outlJv

*

(


qp}l



t}t}u

. ,53

tUx


tUqx





xkHqp}luux3qf

l,o3ukl,oqfj,f:fCqxkml,kv
- ,

zx]|}p}l~Cpk,xkmlmflqt}u:v

t}rYoux~o ~CpAlmj$t}x~Cl,kl,oqfzuj,f|x]pukt}k,k,



xkzOfjoutzkJl,oqf

^H_U`3_L%
6fi.0/1=v7}v

pk,lYx


)


+*



,

vflukux{l,ouf.0/1:v

oxk.0/1Nxk(zx]|}p}l~Cpuk,xkmlmflRt}uyv

oqfHk,f~Cpu zuj,p}z=fj,lnp}v




v






fff
v



vk,u~o
)



+*243
v




zul,x]pa

xk


z=p}j,l,t}l

,{+893ff:}];<fi4{<=ff}2H>=:?R@9<A


7q>%
^H_U`a`b$cflukffqfup}lmfB

B

v

flukqpLrk,oqpr

p}v
JI



rYoux~o


v


t}uCB

l,outl5B

xk qp}lfitkmpql,x]p




G


v

v




p}v

xk{tkmpql,x]p4p}(v

k,qzzOp}j$l(p}jHtlH]ft}kml.pqf{~Cpuk,lmj$t}xl
l,oqfj,fp}j,fJout|}f

JI IL

N
, u]

p}j

l,oqfx]jj,fkmz=f~Cl,x]|}fk,fl,kffp}ukmpql,x]pukED|x]puk$]}FB

B

JI } L







v

k,qzuz=pkmfl,oqfj,fRfCqxkml,kHG

YoqfLK *

t}k{vxk

Kuik"l
,





G

3




P:QR



]"



tUqx





k,u~$ol,outl

t}l,outl
JI 3



JI


JI









ft}ukl,otl

4
, ]

p}j



,



JI





JI } 3

xkffqp}ltYkmpql,x]p.p}qv{}l,outlxkMB



v


kmpql,x]p
YpLr



out}k{qp



4
, dgf


B

v

fiST@UVWYX4VZ[\T][^W`_&Za&b;c:VdeEf"Za&WT]VZghX5i$SH[

j kl"mFnpo`lmFnpq+r"s=qq+r"kt+kuk>v`wyx#q+xzs{"l"w]|<{k}~YMsfiyk;q+rk:fi^=y>>z:<+zmfi
ux+{^rq+r"s=q5 0 u wyx5s:v`wysfisfil"( w]xHwfimfiq>mlx+w]x#q#kl$qmfit+kmFfiktq+r"w]x}0~rsfix5q+rk
x+sfiukzx#kq4mfix#m]{"q+wml"xsfix4u`q+rk"t+mfi]kwqH>mukx4t^mH{q$kfikl(q+r"m{rw]qw]x4w]l$q#kt+kx#q+w]lq#m
o`lmFnq+r"s=qzx+{"\rs}0~k>v`w]x+q+xw]qx#kkxt+ksfix+ml"s=k2q#mnw]x+rq#m(mfi"q+sfiwylwq5q+rs=qw]xq+rk2{t+0mx#k
mfiEq+rklk>v`qx#k>q+wmlMnr"w]\r"t+mfi0mx#kx5sfilsfi]fimfit^wq+r"sfi\r"wk`w]lsw]fimfiq>ml"x+w]x+q#kl&qH]q#kt^w]l
]OF<:O(=
Ewt^x#qslmfiq+s=q+wmlMq+rk2x#kqmfiEq+rk{l">q+wml"sfiM>mlx#q#t^sfiw]l$q+xH24\rmx#klq#m0kq+rkwfimfiq+xmfi
ksfi^rwyxsfi]]k2s?=fi\==Jfi>\$sfil2"klmfiq#kuEq#kt-q+rk4]q#kt^w]l=Lufiq+rk
wfimfiqx#kqj k2nw]yx+{"0mx#kCwylq+rk2>m{t^x#kmfiq+r"w]xx#k>q+wmlq+r"s=q0mfiq+rq+rksfix+x^wl"ukl$qmfit^"kt^w]l
sfil"2ps=t+ko`lmFnlnkCk>v<ysfiw]lrmFnLq+rks=t+kmfi"q+sfiw]l"kw]ls="0kl""w!vC<
r"ksfifimfit^w]q+r"w]x>mu0mx#kmfix+kfikt^sfi4"t+m`>k"{t+kx r"kwtC"w!0kt+kl$qCkfik]xt+k"t^kx#kl&qq+rk
q+rt+kk4x#q#kxmymFn4kq#mzk>lkwfimfiq>mlx+w]x#q#kl">fij k4"t+kx#kl$qq+rkx+k"t^m<>k"{"t+kxMw]lsfilsfix+>kl"w]l
nHs
Ys=ofikCq+r"kC>ml"x#q#t^sfiwyl&q+x\Csfil04C45>mus=q+w]kfi
<4Hmu{q#ksuwfimfiq5424mfitsfi]05wyl(
^rwkfikCw]fimfiq>ml"x+w]x#q#kl>mfitq+rk2}0~
t^m<>k"{"t+k<"&F&`#5$# : s=ofikxz>ml"x#q#t\sfiw]l&q+x5Husfil"\H>mus=q+w]kfi5wq

t+kumfikxCt+m^q+rmx#kq+{"kxznr"wy^rml"mfiqCr"sfiks>mumlx+{"0mfit+qw]lEmfitq+rkx#kqn4m
>ml"x#q#t\sfiw]l&q+xElk>kx+x^s=t+fiwq>t+ks=q#kxq+rk>ml"x+q#t^sfiw]l$q0^z0kqn4kklq+r"k:s=t\w]s=kxHHusfil"
=Ffififi ff
fi ff:ff
&! "
#fiF$

%'& & (*) + fi

&-,/.!'&021
#3& & (5
4 ) 6& & (5
4 ) fi7&8"%98fi ::;& & <#:=) +
!"0>#=

!"0
t^m<>k"{"t+@
k ?
A`&z4`#5 s=ofikxsfi]>ml"x#q#t\sfiw]l&q+x\>ml$q+sfiw]l"w]lHsfil"(x+{"\rq+r"s=q5 CB DFE
H>mus=q+wk5nw]q+r4C 4$x+{">kx^x+wfiksfi]yx-q#mq+rkx^{"t+m{q+wylkH<:`$`#H&#
q#ktwq+x5>mu{q+s=q+w]ml"HC5w]xHq+r"kt+kmfit^kCsuwfimfiq5mfiE!DFE
=Ffifi GfifiHIffI!
&! "
#fiF$

JK-(<LMNPO:RQSfiTQKUVW(5XYfi;fiffff
!fi Z:[' \K
!"0
rk}0~w]x5wfimfiq>mlx+w]x#q#kl$qw!;sfil&(=s=t^w]s=kzHCmfiE7

7wyxHq+rkq+s=t+fikqmfisuwfimfiq5mfi-DFE>

]!^`_`aWbdce8fgdeihkjlceffmjonqprtsuwvKectvSxyy0cwzShKjle*{P|n}cwz~Zr+|8mg!gdechkgwr+|priR8m0uwe mcn`bcxzkvSgprSw[
r+|
uwepwSw\
|8^
8

fi

!%!!0i !0JM!! !0
ff!
0!$3 ;
\ !;ffV0%!! Zi0 ffff!J ! =
ff 8
!0>
!0
Cq`!'`
Kffoff0wff ffk0`0Pw'2iw'ZC`ffii'VH'0
0Vw'2iq-iw+5dtY[ikC5Mi; k

ff
fifififi
)*$+
*

"!$#&%
* -,

CFq<i8i+q'i* <qV+V
q 00C0
0+i 0i - q>+VP88+++%+VPq+8++ q+ i'0
0i0 PV;+P V `P+ 0
P`0@@+VPq+8+ + 8+ q @P0WV +VPR0+t +V+0iW0
+ViP++8Z-q 00ti0 P P
q ZP8ff++<`P 0 +V
+qC0ffJ+VPq+8++ PV5
@P0
V `V8P +VC` 00+
0
0+P+8i+8 q
q 00t0
i'0C <qV+P
P+P8 `80
V0+Vt+ 0@0 + 2 @ V8V05+V V+t0R+P\P+M0
+V <qP++
0+VPM PP+
Ki
+P +P
q 00 0
i0+
Vq +
H5VJ V0iq 00*0
5'i
q ZP8ff+ [+Vi+
V+
+P+P
T0
\+<V0
2 <q+
*
0
P0 +MP +PPP+8++
W+
\0
@+
P
<R + 0+q V P+ 5PM PP+
\ P5< P q8[ P0Z+
+
\+%*i

PZ8V0* `V8P<` 00+
[0
tq-000+q+Vi+i0+ <qP8-+VCq 00t P+8Zi V+ 0\

'

( )
)*
. "/0213 (4 536

2* 7* 8%
)9
)
(:
;* < ff
fifi=8
>
?fi@ABfiC EDGFIHJ%='
8 Gfi@ABfiC
KJC!$#
L
3A
fifi
=:
%
N
L & 8P: ?: *
RQffSJ
TH2
)* ff
fiUfi=8%WV XD L
Xfi @ fiC
AJC!$# L
QffSR
TH2%ZY
[fi@ fiC
\JC!$# XQ]SJ
TH2%_^ )9
8
L 8+Ei$
kj e ljP@<\mon e @ &% &%[
kj e ljPCkqmo
-` =abcD
]deC *fd @ C
EfiChg
p nUeC
r
kjP@8ljPCksmt
p n @ CT%[QffSR
TH2
)*Z
kj e ljUCu vneC r
kj3@3ljPCu wn @ CT%[x
L 8
L s* -,
)* =
fifi=
* h,
dR)yzP:
{ |}b~HJ
{ |MbD
THbD%fY QffSR
TH2*
>
fi@ fiC
DFHJ%x

rR&$%

0++qPP0O w 0tq+ P+8qi8=: P P Vi)*OV":)*V0 L V-+PMR&$
@
+Vq0
0 P8Zi +P+ 8*2P* + tV* -`, V)*t:]o% )o% &% ;+M0+i&t ;q0 0i023
* 0
0++ P<8Z 0V
* iV+Z\N V+o
+V": V"
: +V;P 0+V`0 0(
!6`> V8P+V
,`i)* P+ L 8%

$ V !>";E
+PC+8+ $
: Pw*+Ps`, iV 000+P P+V +P)0 808P-V` 0<q8%
,q+-+8i0F+VP+0`P+ L 8 02* -` R +Vq* +V88++ +V000w+Pv
+M0@i
it0Iq0
0@i02P* P* 8Qq 02*X2g <q+ L 0V* iVP%
~2\3K2<P
Qq22"UzX GEMPzU
G\qUKs K$]P
G\qUKs K]\2\
[2g <`+ L 0P* iV<&+
Ez3K2"Uz3GEMPzPKK$]2zP K]\2\
0V
* iF0wP&0 q0 0 P+8P8<: [Pi)* <qP3E
GEMUKzPffE\
<
G\qUK3$ff2$ 02*3E
2<P3GEMUKz%
"(

fif33<G2 3 X=P

Z-&zG)G&
G zP<zz3 (()=K~(3G~?)PMG~"
G ?"zz" A<G(83\3-(= <hK z z
k} <fz3 ")Gc z 3z



z&Xz l"3z)"fiff A)38 (
3( () <z h)"8(z G (






&
k "z$ ((J ~ z 3 38
($ 3zz$(< 3) 3 (3
K
hz z( ?)A=(z3= zh)( ]





!#"%$'&)()+*-,-.0/1,2!3#+3#.4"2!/156&782:96;"2!/1"<,-=.#>;"@?A+*-,-.

BDC EAE!F:GHBI8F:J
K!LNM

w
PQRP

G UVC U'FWJ
H
PQ8LNXY[Z
F X]\#^ _`MRZ
Jba]c X_`M8Z

B!dVC GHeEAI8F:J
x ybz{z|1}x~`|1# }'y#|1

PQ c XYfL

UWghC J

Jba]c _`X

x{'yb}Wbz{~u|1
x]~sb~u|1'y#~u |1

BI4eI8F:J
K c Q[_iM
Sj4Xkbj4X
l c MRZ]_`X]\4mj4X
G L
n'oqpj#Qsr
c k4Q[_uk

UVCIsv6F:J

g
B

;#"W&,2!34.4b-+2'.-2!/1,2!3b+34.4"2!/15fi-b ?A>A3,-=.#>!"+2!.#8-A




fi0D

A!:H8:
!N


R

V 'W
H
8N[
]# `R
b] `8

!V HA8:
b{1`1# '#1

f

Wh

b] `

{'bWb{u1
]sbu1'#u 1

48:
[i
4b4
R]`]44

'q#s
4[u

Vs6:




;#W0s46;;%W7qVu67@!

6;# #!

%!{;#-8#;4 1

;# # -V6;:%W7qVu67 ! V<f;b]
#s-
fiff' : ')NV'uA7 ; V : W7NV!b ;W;-
W!+ -W#!+;)-Aff' :%W7qVu67 ! V :!"0!]#1#%$
4#;&1!#4b-+'('
' :!H;%W7qV*)+
;# # -V6;:%W7qV ;7) ;qV]
,
#R-"-Wff 6 :%W7qV ;7) ;qV : W7NV 0!] ;W ;-
W!+ -W #!+;+
;# # -V6;!WN :%W7qVf;#/.10 ]
2
#s-
3Wff 64 ;WNV' ;%W7N !WN%0!5b ;W ;-6]!-;-fi-$
#A+;'
-A2Wff 64 !WNV' ;%W7qV :!-": 0!]7 :!968 H#;:1!44b-+'
' :!V'7!WNV;)+
<>=5?A@BCEDFHG6DEI*JBJKF4LMI;NPORQMST@VUFWXGYDI;J4BZJ1F4LI*NPO
[A
#1W\ +!#] b-+ #! ) "^$
-!$_A #`1!#+44%%-);-;K1## 4 \-4 [!#P##
4 sA1#!-cbed1f>;
-! #! g6 ]-\ -g1!#+44% - #h`'1H+ (8A]4b;{ + #!8#K1#
#! /6
]-\ -71!#844%+ j-i ; s-
-:A #k1!#+44%P -A +4K1#kl 0++
!##4 &! #P ## -A #;:s-04]:\ +; f!1#A->bedK!f nAm -4 #!KH+
-4 #! 1 #+1pA -\ -q1!b+44% +VA#K14>- ;n1!+ 08# 4(1A b+;-!@ 0#

r f!+s(6 #21A#+44%PMAm ;-+4 0#utjvwaxay4z{v{|P}~jMA #`1A#+44%PH+ <#;bR-#! #P 44
!###+ #!b!]A
uA ]-\ -[1!b+44% -!m -#A #1!#+44% -! !+#K1#!-
#Y1!#+#4%P+ 08+-#W!V##! -\ -;1!#844% +0b44b146-\ b#%-A!+#K1#!-
#1A#+44%fi-++


fipu"
7gn
Ej5
!




j ;
5!K
"a

" 15
;j5

j
4%^%najaK%

K{ Mj
"

4

4%
"Z
%aj%

5
;Z
K


""]

K aZ
qa/1V1Vj:^M!;_q6M!q>"KHP;a(]1
5
]
11 ga p
ff[KH% fi a5]E
!#"%$'&)(+*-,/.01!2&32.4&# 5762,'89$/:;: fi
$'.?>KH fi BCfiJ $/83 fiIL *M,/N#"%$'&OQP2:R3S

a] UTe1aK

).=<

e
V]aH]a

$/.?>
6,/89$/:): fiA@%BCfiAD ;.E<GFIH fiI@%BCfiI'J


K E

V




WX!Y>/)8C3M*&OZ,/.?$/:?"%$'&)(G*M,'.0-;1&3.4& 5[6,/8Y$/:):
fi ) .\<]$/.^>_62,'8`$/:): fiA@%BCfiD ).\<
h
F
H
/
$
?
.
K
>
H
/
$
C
8
3
, N#"%$'&OQP2:R3S
fiI@%BCfiI'J
fiADiBCfiAjJ
fiIj
L *-/

bacBCdfe7g


H

kRlPP
K kK2P 1K X j
maKanR]
1ofiAp a]q#57R
amM]aKPr l " ]KV <tsvu n[aa1MPaPEPV ' PaKawfieP 5cPa
E
a]
]"TP aH1aK> aK ]a K>xfiAp a]q,TR
[g
ayl " ]I
KKyfiZaV /a
a]VK
' ] lPK7ZP ay
P
1"0 n6 P>aY]1" : ' ] lPKG
ffn n nTa`
lPP `
[
P1
fi2{ \fiA@{z#|}fij- nf~HPa1
pP"1
^v;I"?),&*-,/.01!2&32.4&A 5X6,/8`$/:;: fiA ;.<tsGu$/.?>b62,/8 $/:;: fi ;.<1V*2(&)($'&

dxe7g FH fi@pz'BCfiI'J $/.?>_H fiDBCfiIJ $/83 fijL *-,/N#"%$'&OQP2:R31F4"?8i,'pZ>j3>
&)($'& fiAp
@ zr|}fiIw

Ya4 0c `
/a P ' fi% aaK /1Kp a1I/)c4

]K1]a f
K X7Zg +lPP aK /Zg-n 2UTe1aK
e1 1KG RlPP aaK ]K ] " 4E "( a/ lPK PK 2 ]P&R

]1aKn 7
qqqVYM!;1 K> #a
# q n;VM!q16`9K>h[1a aK X9K,a24lP
2a a1 Ka4 lPPa1r eaa ] a1^ M5/a
P Y]1{a1`]aP4- nVp lP1Ka(2 Y]/K6P]] U Te1aK
e1 1K
lPP a5K a] a5K 'n&]a aKaKa:{ lPP
a5K ]a a] a5K U TE1
Ka ]) 5&P
P [K
Kaa Mb fin
{p

fio{^

^/4V%h!%

;





/ h
!qZ
- ;
M- !

Vq

V/h4!%


/

M-

C!%
Z
CMC
;--CC
)
CZ

/Mp{2j12/jj2

{)?%




pMMp2
-)M2/2

R0'/f/=24RCCoVb'q




Pivot consistency





Directional path consistency






0i
%

V
V



0

0


0#y%2RVb4''f2VMRCCV2Y'42V'%?/92V!CCV2 900 ff
fio
^ ff 0'4C02VCC-'Rj! 4_2/C "w4''
0
2VRCC42#

fi%$ $ &
$'(*),+- %0,
0'CC 0'0_/ .%CM 0A0 4R`2M/C
1
VM2V'^4/=24RCCV2

243 2

fi57698:;#<:=>?6!>;@ =A BDCE:FHGJI=KA ;L6!:K=LMNO<PQ5>

R SUWV%X/YZUW[\0V/]LS0^`_aSbT\0Y*cV&dfe Ygdf[KS0hgYg^fX/YZibSb^V iX/_a^jkgiHe Ygdf[Kkl^jmX/YZibSb^V iX/_aYgdonS!dV X/^S0YZikg\

[kl^jHX/YZibS!b^V iX/_gpKkgbTcfS!\\KiLY4crqffV%bjLY4cfits
utv!wyx{z}|~J
YX/V ndVal S!b"X/YZUW[KL^V nV/]kgX/^\!_YZiX/VmeYgdWV kgX?jhEkld?S!klq\0VamS!iS!^^jV iX&kg\!\!b
kl^UWYZb^OW^SUWV b,[dYX/V nLdV4LZK*KQ YZiX/Ve Ygd%V kgXjYge7^jVWe YgdUWV&dhEkldSklq\0V b??s
XjS0V&hS!iL&X/YZUW[Kkl^S0qS!\!S!^_qffV&^cV&V ia^cYOX/YZib^dkgS!iQ^bTff*{kgin1}{X/YgddV b[ffYZinb7^YWkgX?jS0V&hS!iL
[kl^j1X/YZibS!b^V iX/_e YgddV \!kl^S!YZia}&c%sd s^&shlkldS!klq\!Vf sRTjS!bYg[ffV&dkl^S!YZi1iLV&V nbTQ*SigV iV&dkg\s
L^1bS!iX/V#^jLVDX/YZib^dkgSi ^Hff*?S!be iX/^S0YZikg\pf^jS!bX/YZUW[\0V/]LS0^_S!baiLY*cQ*?sRTjLV#Y*hgV&d?kg\!\
X/YZUW[\!V/]S0^`_YgeJ^jLV,}\0^V&dS!iLWS!b^jV&dV&eYgdV
"*
&}
S!ib^V kgnYge' e Ygd[kl^jaX/YZibS!b^V iX/_OYgdTnS!dV X/^S0YZikg\K[kl^jaX/YZibS!b^V iX/_ S!b7^jLViQUqffV&d
YgeJhEkld?S!klq\0V b&pLS!b^jLV%bS!&V%Yge^jVnLYZUkgS!ibkgin1"S!b^jLV%bS!&V%Yge^jV%dYYg^TbV&^??s
'

fJt,E4gffZ}Zg`'Z

RTjLVS!i ^d?S!ibS!XXjkldkgX/^V&dS!b^SXYgekm\0YX&kg\X/YZibS!b^V iX/_S!b^YHV ibLdV^jkl^km[kld^S!kg\Sib^kgiQ^S!kl^S0YZi
X&kgiq}VV/]^V inV nH^YakOiLV&chEkldSklq\0VgsV%db^T[dV bV iQ^f^jV[dYg[ffV&d^S!V bYgeJ[S0hgYg^oX/YZibS!b^V iX/_gp}S!i
[kld^SX&\!kld^jLVX/YZinS0^S!YZibinLV&dcfjSXjOk{X/YZibS!b^V ^7[kld^S!kg\KS!ib^kgiQ^S!kl^S0YZiUk _WqffVTV/]^V inLV n^Y
kabYZ\!L^S!YZitsfVW^jLV i#V/][\!kgS!ijLY*c^Y1X/YZUW[^V^jVnkl^kdV S0dV nmq_H^jLV{K\0^V&dSiLkg\0gYgd?S0^jU1p
kgina}ikg\!\0_[dV bV iQ^TkUWV&^jLYn1e YgdfbYZ\0hS!iL"eiX/^S0YZikg\ff b&s
v%Ez~Q44 Q
V[dYX/V&V n1S!i^cYb^klgV b&db^&p^jLVkgnnS0^S!YZiOYgekWiLV&chEkldSklq\0Vf^YW^jV,X&ddV iQ^Sib^kgiQ^S!kl^S0YZitp
^jLV i1^jV%V/]^V ibS0YZi1e dYZU^jLVdYQYg^fbV&^T^YOkbYZ\L^S0YZits
fiff&
// g !#"%$
%Ez~4?v&J ff&

)
'& (
*}, +.-/0 1 "234Z6587"{39L0O:;/3 < $ = >?,9m39@g BA C
3 9 ff& tIH J W: '&Q&K ff&4 24,K3/ g4LF"G$ 5

3

/
g


E
F

G
"
$

%EztN
z MO*P V&^b{bjLY4c^jkl^"kgi _X/YZib^dkgS!iQ^{S!iX&\!nLV nS!i $ aS!b{bkl^S!b KV nts P V&^Q< $ tbX?j
^jkl^RA #SbOk[KS0hgYg^Yge $ = t/pTkginTSQS!^bhlkg\!LVmS!iUff&= t&sWVYZibV Q LV iQ^\0_gp^jLV&dVHV/]Sb^b
S,.
Xl SQ?s P V&^TbTnLV iLYg^VZff&S ,[,[,[ SL,[,[,[ S= Sg?s
ls Qi _OX/YZib^dkgSi ^bkl^S!b KV nOq_ff& t7kgincfjS!X?jnYQV biLYg^X/YZi ^kgS!iaS!b7^jLV&dV&e YgdVYgqQhS0YZb\!_
kg\bYObkl^S!bV naqQ_2ff&Qs
\G
]Ygdkg\!K
\ ^)_ b&s^&s}0Q<W"s(]JS0db^&p' < $ t/\0V&^CS qffVS0^bhEkg\!VS!i`ff& t&sbaV X/YZintp

WS!b,k[S0hgYg^,Yge $ = t&s%}0kgin#A OkldVWX/YZibV LV ^\!_H*X/YZUW[kl^S!q\0Vgs,V
^jbTjk hgV SQZc
<K0ZbYLpK}0{S!bbkl^S!b V n1q2
_ ff&Zs


hgV&d_X/YZib^dkgS!iQ^TS!iX&\!nV nS!i
$

S!bbkl^S!b V naq_ff&QsfefV iX/VZff&S!bkWX/YZibS!b^V iQ^TS!ib^kgiQ^S!kl^S0YZisEg

h#i=j'kmlnopnqrnKrstq/uvoFw6uvx/y{z=u}|oFs~nuvlx/r/rnqIsFlx uno~x/s~IlxC/xs~nuvlxr/F=fuvKnuv'E6FfirxzIx/lnKEF=#
uvx/sFo|n'/x/snulxrsFlxn |truvx,nr|oxln'x/oFs~oF r|uv}zu}|oFsno#zpnq/o|uvyq,nNrF%qo~|or/xs~nuvlxr/sFlx/n |trux,n
uvB3=|l4o~|}=Bz=u}|oFs~no#zu}mu}nl|uvyuxuvE4o~l|o%u}nEntr|yo~nuvxnqouvx/ntrx,nurnuvlx*l|tzo~|uvxy,rxz oFsFlxzx/lx
x/s~nuvlxrsFlx/n |truvx,nrF4oms~|o#rnoFz,Gnqofi|l,s~oF uvx/yj


fiG

QI?=?6#6?8,4686#,4mZ),?@pZ#??%,6
C@N/@

,E3ZG

(F=4{6,3Z'Lb#4G4 ,/4?,N

#3L?3@4C'3/4CtNIKW3`?t'Z34N;W#)
;3c3(3E4bB;t,'2*;F;,C
/N 64 ff
b

;fi *?,6#
8N#4#6;G

4,fi4c6446c4,E4CN44 ?
4,N6##,

! ##,3;

8?#4;4

$&%('*),+ -/.1032146579895:<;;>=Kfi?@032A4CB

;)Nfi

K'44p;N##6 4
,

##";

#

<DFE

GF)#6; 3 ,46H c6RN ;46Z;?##6f?4;B4 4?B?@6
?fip?fi6

mN44m;?##6mZ}6},m4

;)Nfip4;?)64,%4?CN44

#, p*,4`4Z;fi J;?4#,*?,I#";b#*G"Nfi6

NZ#

'Nb

G?;4K LN4M#4M@fQI,4,,34Z6?N%),;?E?4
, 4fQZ
#??{?G), 6;)
;#6#;GN?4#6 4?34G644fi6

pZN6#6

)8;fiONP 24?Z66E4fi;)';;{ON4fi4


AQ6?6`SR ;)N;`44#,

N44),4??N#2,UT


4?4

;)Nfi

4;?)64,

?N?4#,4;;6 V
;6T4W'??6b(#;6*4ZC4NTG,64
#;" ;??6#6;)
;6%6N?6#;C?@
,K4
?6#6,KX4,Z(?6f
4






;)N;4 2 '?N

Z 4?)6,4#4";,

4;?)64,6 [Z4)4Z,4 ,Z?
6??6]\I84?^\Z`_@

4G43bcedgf]hji #2;N#
?Z4;N)64,

{@4G#,G



N6N?kN

\Z 4N

dgfhji 34lT

4?

?
6?NkNW\I`_ m@ 34ZG"%N?4 dgfhCi

on6prqtsuwvAx<y{z6sX|!sv}x1~

N44f,4?N?#G#,f[T Wn6z6s|<svAxzrqt|![svAxAFzrqt|![svAxezwqt[vF]
z6svAx<[zrqt|![svAxfi{prqp1vAx<~
;)'fi84;?)64,BcZ;#C



prqsPuvAxKy[z6sX|!svAxKy{zrqt|![svAx<y[zwqt[{v&z6svAx<y fi[eprqp1vAxt
AQ6?6

'

44G;?##6] #,

4G43bcd#i&
6K=#{ #,@`



{N44;?##6?fi6
AQ6?6J

;)N#6#6;I@

GF?#4;4

4G43bcdgfK6 , f

8@?
,4;?##4G???6*




,8 4Z;?#6N#4;?4 on6prqsPuvAxKy{z6s|<svAx1~
nfi3(=GP3;3/3(I,N4@~
X@

fififiwfifitKfiKJfi

A66<"
33

6 K fi]<

}
jKOt66(

^O"6<6K"]^"(""6

666]
e""6A e6fi
g(

ff fi
[
fffi


&
K

[ !" "fffi

#^$"ff<!K
%'& ()!"ff fi
*+-,$.fi"/X0%1& ( !" "fffi
4 4 76 498;: < 6>=
]32(O"fiO ![ 5
""" /][366O"fi@?AB^W
"U66fiO <


666O

l!el^DCr UU E!6F2!"6^6fi6" HG{JI5Kw6(
KS<6([UO"firfifi


QPtKoHGe!"

C

HG<LC

6O{O"6 E!6Ol{

I6
MW< tNK99O"6 E<<

"P

9"fi} ]9e"6eL QR

OKw

K

O"666({ r6 fiLOI !69[O"TS

KU6<"fie

!6"6"6e UK "6fi



(]]O"fi6OI6VKe9Ke

`b[ c[
OP

fi[U
e"fhgji'fftkmltQ
onOplt3gd
XW$Y9Z[FW3[]\ '^_ Aa
ne` " ql6rfftsmltQ
UondPfid 333gdon "
ffNplt3gfft"Pdg
<"3Uonff!d fi33"33m
g

z9{ |}}}| z [9O"fi<fiK("F r!w
{|}}}3|

=Oy
=
y~
x=3
CrO6 RIw C9Ot6C"L"" ` J !
=
k{
r
C K
C ,U Ofi6KU"
2fi
^

=7
| z
O"6 !
E <>Ofi6E!2 z
6 z
Iw6I{dIj"6
f
= =
O"6(KfiK("C
6(66Jw I[ / "6" ""C6fi6w
)
=
=

2 [
w
K Ifi
K(6<"6 F!
2 "fiIfi9Ot6>^
"6"
O"6 !
E <LOt"6e"6fi(U
K O"6<
6K"66 eO ""fi `

XW$YY9t"u 1
v xw

f1dxh'
P!6

<CO"fiK

<69e JO"6KLWV]OK

66"



6O e
MW6OW@ fi6U""6?AB6 fi9]9"" ]

<!fi6OJ6K

!O"fi6OI1fiH2eO"6(6OI&

O"6K/r3KlO"
6OeNKwP

]fi

fi

O"66O

fK


K Kw

fi6O"6

fiH2O"TS

O"66O fi9">

2

fiH2O"6<L@v1 IFKw6fi"69 fiH2{O"fi6OU62t
fi6O"6-?ABK F
Ifi(6lO"66"firfi6K

9O!fi

LO"6<l(6<"J

""F

K ^ 6<6O"9"" ! fi " "fi9ITK (
O"fiOPA"Ht
2 fi6O"6?AB6-CrLK9"K ] l
3$

fiV3aH

>d H>m-j de"Qm H L mm
"] >d H>9- d9m 9H
m9m
jjk9 mf"d9> m) L m-mm 9H s)>H QjDs dT
mmmdjd d"
d"s"5xQjmX9 mks dTd keH 9 QkVm]mXf

>m] DmJm9
H>mej;
ds93]Vm>dj9mH>
mQm Q VQ>km TH>mO dVH Jm H Vx e

>m] s" ms j mQx d;
>m]De d;d a>" D>H
>mHm9 ) m)
x >s9 9H Q Q5 H"-m mjJ" xm 9H jDjm


d>x"k
>9
"d LV9x
e mk99 H>' mkQ dFkm )d
mmb )ma;> msd H>;md " mU>dNk
f mVs dTN) mV
>e@ "]OFd3V; "; dXm"F
m>d
N Fm) QH1
> Fm H>;@ dXm"F
V x" V9Hked
k dj dXm99Hk
>m 9
ds
>m
Nm"a" H e

>d" m"H Um>9m ;mT9 >sXm H Q f9" -hammdD
Q99 Fa
>Q -
F>d 'Lf dV Q)m 9H Lmmdj>
Qmm
H ds ) >mLm 9 -'m]d k" >>]s 3 -m
>sD
>m] QNmm9
NF; dX"dQ9 F
" ddaHf mVVdkU9 9H
O9m"mX md
>m " a9m m]d >s
>m] VQ
m9m
d>d x X"HVm]JQ >NH ;J
T9H>"H>x
;"
m9dQmJ"m"
]H e-m H QOmFdQ F VL U' ex 1
H QOed>]HNdF de"Vd J"mHTd9H
>m 9
N "j

> 9Hf dx"Xm" 9
1
>mmd Tdj
>m mV Q x5
>m" QxV"
]"Q Tm- 9">)s mmX9
mmm 9@F9 md"m"
]H H>Q
1m99 F
" d>1 Lk @9
>9" ]'k d- @
>9"


fffi' ff

"!$#&%('*),+-.0/1.324'ff56'879/;:<2=5>2=)-)@?''8.BACD.E
)h

Od
TsHm mQ9UQd mH dm] H>k
>s9m H>
Od"mx mXm dO>d H>m X >]

GF

GH

IKJffLNMOPRQffPTSUJffVWJ3XNYffZ\[^]0_3`ffOa3_\`;b8]0QMdceJ3XNYfff*g
h >s9djilk monpoqororor\qsnt>
u m5 >d>Q
>md
;
>s>d Yfffr
v h >s9d"9wYffZk =xqzyZ ]LVd "
|{UV} jx d" mm
H>V
>d>H
>9d

>s>d (n~o} Lm d"
D"">{O
} {$
Vm
>9H1N"X
;">s
;n } Us
N>n
IKJffLNMOPRQffPTSUJffVWJ3XPsc_
J*O]0a\_"J3XNYffZ\[QffV`6Psc_]0JJffP
_PJ3XNYfffg
F h >s9d dZk mG{sRoqorororGqT{Tfi\u @ d>m"
)YffZ
F
)h d> e " >d"
{TUX >kZ "
Nnff 9Vm]Va m"
H VVdX E
N 9ON)s FOOYFf
F
\0

fi8888zD8D38Bd~*D1*18

1













G
ffj G\T3T3
G0
G0
ffj G0T3T\

1


1




8



1RGTo,R1TT1~1GTGff~GDR








GzGT



d1R1To8j
8R1^RG8~GN
8
~NRT~G

1

1





1








$



&

~








(

)%

d1 To8
~ ~~&R
9To
-

.-

/ 01

!



% ff

'



+*

\ U U
\U K\ \
U Uz zU









1


0D\fi
U
\ U

0 R
0








,



" #






ff fi

!

fi243 57698

:<;>=@?ACBCD9EFHGJIDKAC?ALNMO"PRQTS"OSPVUWIC?G)XYX9Z\[]+^R_`^R]bac]edgff)PhOSPYi
?I?lGJIC?"G)mnBC=`G)XKXKZBoNEp;c=@?AqBCDKEF`ALsrtIACBu`DKA'UvGwICE>E)B4AC?Bxi.G)FtyHD9z{ACE@utDKA|B k

j4k

?"rtIAqB|A k
~

?B=`AA k


U.

v

E oB k

E oB k

GJBB k

?lAq?B4UeoN?"mnEwt=@Bq?yHDKAGICEcE)B4Aq?BL

GJBG)FffZ
)?ICBq?n DKAGy@?ACmn?F`ytG)FffBzRICEeG)F ?X9?w?FffBE)zU




FtD9BCD9EFuT

DYA'B k

?+ACG)w?AqBqICEF`X9ZHmnEF`F@?mnBq?ymnEEF@?F>BG)AhhZHy@?nr`

?FHGwy@?Amn?F`y`G)F>B4z}IEB k

E.?XK??FffB+DKFU?X9EF@A"BqE.B k
y`?F@E)Bq?|tB k
B k

?BN?bG)?ICBq?n E)z


ohEwEACACDKtDKXKD9BCDK?AL
j

:F
?X9??F>BE)zUW?X9EF@ABqEB k



DYA|Aq?Bl]b^}_t^}]baT]"i

?")IGJ

qDYF

DKA'?X9?w?FffB

?AqBqIEF@X9ZmnEFtF@?mnBq?ymnEwEF`?FffBo

k cE@u7DKAF@E)BGAqE=@Imn?E)zB k
A=`m k



B k



?ACG)?ACBqICEF@X9ZmnEF`F@?mnBq?ymnEEF@?F>BG)A
k

DKm k

DKA|)IGJ

k

GJB'%{DKA'Gby@?ACmn?F`ytG)FffB'zRICEgg

I?`IC?Aq?F>B4qDKFHU.:mmnE)Iy`DKF`BqEwB k

mnEF>BCG)DKF`AN%u>G)F`yb%D9BCAIC?y`=tmnBCD9EFDKF
mnEF`AC?;>=@?F>BCX9Z)uB k

?B'|?B k


?B+=`A


?"y@?nrF`D9BCD9EFE)zsB k

?IC??nTDKACBCAGpACE=@Imn?

?)?IBq?nE)z

?lIC?yt=`mn?y)IGJ

oN?lm k


EAq?BqE

u@cF@E oDKF@B k
k

GJB4

DYAGy@?Amn?F`y`G)F>BzRICEvq%uG)F>Z.?X9??F>BE)zthDKAGy@?Amn?F`y`G)F>BzRICEWG)F>Z.?X9??F>BE)zgu
G)FtyB k

?IC?zRE)IC?lzRICE|gNsDKA4mnEF`AC?;>=@?F>BCX9ZGwy@?ACmn?Fty`G)FffBz}ICEG)F?X9??F>B4E)zU.

UeDYA4GICEcE)B4Aq?B
~

?F@EoA k



EoB k

GJBUDKA|]b^R_`^}]baT]%^RSKu>B k

?Bh=`Ay@?F`E)Bq?" U:ACAC=t?|B k


?B k

?lAq?B4E)zB k

?+TBqICEF@X9ZhEF`F@?mnBq?y
'EEF@?FffBCA4B k

X9?B'RUK?B k
AqE=@Imn?A

j4k

?Aq?BhE)zB k

?DKINIC?yt=`mnBCD9EF`AhDKF

?B{=`AVy@?F`E)Bq?h"p`B k

E)zB k

?IC?y`=tmn?yp)IGJ
k

u>B k

k

EAq?ACD9?DKAJc

GJBmnEF>BCG)DKFHG)XKXB k

hZpy@?nr7F`DKBCD9EFwE)zU
uTB k

?IC??n@DKAqBCA4GJBX9?G)AqB|EF@?"ACE=@Imn?q"DKF

RUpY>J7x'hEF`Aq?;c=@?F>BCX9Z)u@B k


?IC?y@Ec?AF`E)B?nTDYAqBG)FffZIE>E)BAC?BE)zACwG)XKX9?IAD9? xL

?IC??nTDKACBCAGICEcE)BNAC?BUpTo





DKm k
k

?B'wRUpK


?"?X9??F>BCA|E)zsU

?I?y`=`mn?y)IGJ

y@Ec?A4F@E)B|?X9EF`BqEwRU

G)A

ADKF`mn?

zRICEG)F?XK??FffBE)zsRUpYhBqEwq

?IC?l?n@DKAqBCAF@EtGJB k

?ACBqICEF@X9ZmnEF`F@?mnBq?y+mnEEF@?F>BIC?y`=`mn?ylBqE s@ICEB k
?IC??n@DKAqBCA'F@E+tGJB k

utG)F`y

k7k

z}IEG)F?X9?w?FffBhE)zU


?y@?nrF`D9BCD9EF

BqEbG)F?X9??F>B'E)zw`x

hEF`AC?;>=@?F>BCX9Z)u@UDYA|F@E)BGICEcE)B4Aq?B
UeDYA|B k


=tA4G]b^}_t^}]baT]ICEcE)B4Aq?B

fJ]7KSC^RPBDKAB k

mnEEF@?F>BCAu@B k



?4ACG)?G)AB k

?4mnEtXK?nTD9BZI?;>=`DKIC?ybz}E)INmnEt=@BCDKF`B k

GJB4DKA4p{ j

GJICG)Fu

J xu7D9zlDKAhB k

?4AqBqICEF@XKZ+mnEF`F`?mnBq?y

?Fc=`"?I'E)z?y@)?A|G)F`yB k

?F>=t"?I

E)zs)?ICBCDKmn?A

7{` RRw {c


Y+VR
74 {.Y`t{\sV@ R7t7
?F@EoWy@?ACmnIDK?
~

E)Iy@?IxDKF@@

?rtIxAqB`IC?AC?FffBB k
~

G)FG)X9)E)IxD9B k

EovBqEm k
k

B k

E>EAC?
B k

?
tD9)E)BmG)F`y`DKy`GJBq?AG)F`yBqEmnEt=`Bq? G)FUwmnEtGJBCDKtX9?

?mnEF`y`DKBCD9EF`AB k

GJBpmnEwt=@Bq?AE)B k

?tD9)E)BNmG)Fty`DKy`GJBq?AN=`ACBACGJBCDKAqzRZ

G)F`yG)FUpmnEtGJBCD9tXK?E)Iy@?IDKF`@

tD9)E)B4mnEFtACDKAqBq?F`mnZDKwtXKD9?ANBohEmnEF`ytD9BCD9EF`A'EFB k

:F>Zp'"DKF
DYA|B k



U+=`AqB'?B k

oh?B k

?Fp`IC?Aq?F>B

?Hy@?nr7F`D9BCDKEFE)z

?ltD9)E)BCAL

?BCGJIC)?B4E)zVEF`?G)FtyEFtX9ZwEF@?tD9)E)Bu@G)F`yF`EJGJIDKGJtXK?E)z{U

?BCGJIC)?BE)zsGb7D9)E)B



4%z

@E)I?G)m k





j4k

|4DKF

h+DKA|GtDK)E)Bu`B k





U
u)oN?
k



?F

G)?|BqElm k

DKA'?z}E)I?'+DKFB k



?n@DKAqBCAutADKF`mn?|y@Ec?A4F@E)B?XKEF@BqEwUx
ohE=`XKy?DKF,mnEF>BqIG)y`DKmnBCDKEF,oD9B k



EcEAq?|G GJIxDKGJtX9?N
.E)I?E)?Iu

mnEF`y`D9BCDKEF

ff






DKF

?E)Ixy@?IDKF@@



AqEB k



GJBs



|"R

F`?mn?ACACGJIDKXKZ

+=`AqBF`E)BmnEFffBCG)DYF.G)F>ZmDKIm=`D9BuTo
k

DYm k

:o'GZBqE`IC?)?F>BwmD9Im=`DKBCADKA+BqEwGJI\?)?ICZ

fifi !"#%$'&()* )*+,-./fi
021435146872149;:=<=>?<=@31 AB< C(9D72EF02143514687G<HC*1ffIJ:=<=>?<=@)351K< LMENC*OH7QP*1!R(C(LS<=>T 149DENC*140ffAUIUP(?6PV?0W7QP*1
7Q<=>QO 1ff7E XY<ZX[RC(687Q?5ENC(< 3)68ENC(0272>< ?C/7IUP*EN021\E >?5ON?CW?0< 35>Q14< 9*]SLS<=>T 149_^a`BP(?0bX[RC(687Q?5ENC(< 3)68ENC(0272>< ?C/7
?07QP*14CG?C(6ff3R9*149-?C/72EdcZedA< C(9G?57Q07Q<=>QO 1ff7f[7QP*1gC*1ffI;:"<=>?<=@351ihb?07QP*14CGLS<=>QT 149_^a`BP(1\021ff7cZejI1
E @(7Q< ?Ck7QP(?0lI<4]k?m0U7QP*1d021ff7nE Xa7QP*1po?5: E 7n6ff< C(9(?m9(<=72140lR02149K@q]%7QP*1po?5: E 7n68ENC(0Q?m027214C(68]K< 35O E >?57QP(L
fr0214687Q?5ENCSsq^tsNh^a`UP(?0'021ff7u?C(9(R68140v<\o<=>Q7Q?m< 3/E >9*1ff>wBxyENCdz{f[I1ENC(3]P(<4: 172E|< 99Z7QP(172>< C(0Q?57Q?5:?57}]
68ENC(0272>< ?C7Q0~h^-CE >9*1ff>|72E0Q<=7Q?02X[]7QP*1-68ENC(9(?7Q?5ENC(0nE X\18)C(?57Q?5ENCsq^tsqA'7QP*1W< 0Q0Q?5ONC(LS14C7gE >9(1ff>?C*O
68ENC(0214qR*14C/7Q35]P(< 072EW@1|<-3?C*14<=>187214C(0Q?5ENCKE Xv7QP(1|o<=>Q7Q?< 3E >9*1ff>|wBxo>Q18(149G@/]GK^l687QR< 335]
68ENLMoR(7Q?C*OMwBxF?0aC(E 7aC*1468140Q0Q<=>Q]ucZe?m0u0QRW6ff?514C/7u72E68ENLMo)R*721B7QP*1U3m?C*14<=>u187214C(0Q?5ENC_^u`BP(1ff>Q1B<=>Q1
C*ES68ENC(9?57Q?5ENC(0ENC7QP*1g:=<=>?<=@)35140E Xvk*7QP*1< 35O E >?57QP(L6ff< C7QPqR(0@1|9*1468ENLMoEN02149%?mC72Ed7IEW0Q721ffo0ff
^-2ff[/%
R(LZ@1ff>BX>ENL 72ES !7QP*1g:"<=>?<=@35140E X'J< C9%LS<=>QT7QP*14L
^-2ff[/%z
1ffo14<=7
P*EqEN021g<MC*1ffIR(C(LS<=>T 149G:"<=>?m<=@351\Z?Cz!J0ff^7ff^a7QP(1ff>Q1n18?0Q7Q0<MLS<=>QT 149d0ff^7ff^

R(LZ@1ff>B< C(9%LS<=>TW
C/7Q?3< 33:=<=>?<=@3140<=>Q1LS<=>QT 149
?5ONR*>1 4 o(>Q140214C/7Q0< CW< 3O E >?57QP(L68ENLMoR*7Q?mC*O@E 7QPW7QP*1\0Q1ff7E XYo?5: E 76ff< C9(?9(<=72140cZe< C(9< C
S68ENLSo<=7Q?5@351l< 0Q0Q?5ONC(LS14C7aE >9*1ff>?C*O*^uY1ff7bR(0a>1ffo14<=77QP(<=7< C/]S3?C(14<=>a18q7214C0Q?5ENC-?m0Q0QR*149MX[>QENLcZe
?0< CKS68ENLMo<=7Q?@351gE >9*1ff>?C*O*^
`UP*1|021ff7(N/(W68ENC7Q< ?C07QP*1|:=<=>?<=@31407QP(<=7UP(<: 1< 35>Q14< 9*]@1ff14C72>Q14<=72149
`UP*1%021ff7M*//*//)/!68ENC7Q< ?C0M7QP*1KC*187M:=<=>?<=@35140p7QP(<=7W6ff< CF@1%6P*EN0214CA7QP(<=7W?0d7QP*1
RC(LS<=>QT 149WENC*140IUP(?6P-<=>Q1\7QP*1\7Q<=>QO 1ff7E X<pX[RC(687Q?5ENC(< 3)68ENC(0272>< ?C/7bIlP*EN021\E >?5ON?CW?0L-<=>QT 149
`UP*1021ff7Q0- ) (iB[Ny68ENC7Q< ?C7QP(1E >?5ON?C(0pE XUXrR(C(687Q?ENC(< 368ENC0272>< ?C/7Q0dIUP*EN021G7Q<=>QO 1ff7Q0W<=>Q1
9(140Q68>?5@149<=@E: 1Wf[*/qqi/h
%>Q1ffo>Q140214C/7Q07QP*1CqR(LZ@1ff>E X7QP*16ffR(>Q>Q14C/7:"<=>?m<=@351g?CG7QP*1< 00Q?5ONC(LM14C/7E >9*1ff>?C*O
=~24r8ff
1ff7UcZed
)=rrr=N<lX[R(C687Q?5ENC(< 3N68ENC(0Q72>< ?C/7v?0Y< 9(9(14972E\cZeV14< 6Pp7Q?LM1<lC*1ffI:"<=>?<=@351
n?0a6P*EN0214C_'I1U7QP*1ff>Q1ffX[E >Q1UC(1ff149S72Eo>QE: 1l7QP(<=7< C/]d:"<=>?m<=@351E XzW?0b02143146872149Q/N4r
ENC681 AN< C(9|7QP(<=7C*ENC*1b?m0Y021435146872149ZX[>QENLk^`BP(1a:"<=>?<=@35140v<=>Q102143146872149X[>QENL*/qqi/A
IlP(?6PGENC35]68ENC/7Q< ?C(0lR(C(LS<=>T 149G:"<=>?m<=@35140ff(0Q?mC(681|C*ESLS<=>QT 149K:"<=>?<=@351gLS<]@1|R(CLS<=>QT 149
@)< 6QTA'< C91ff: 1ff>Q]y0Q1435146872149y:=<=>?<=@)351S?0L-<=>QT 149_A1ff: 1ff>Q]:=<=>?<=@351S6ff< C@1-021435146872149H<=7dLMEN027
ENC681 ^
Y1ff7UR(0C*EiIo(>QEi: 1|7QP(<=71ff: 1ff>Q]G:=<=>?<=@351\E Xz!J?0|4r(=[-fr<=7B3514< 0Q7ENC(681ih021435146872149b<=7
< C/]7Q?LM1 A*/q//i/?0\7QP*1S021ff7gE X< 3m3u9(?5>Q14687g9*140Q6814C9(< C7Q0nE X7QP*1SLS<=>T 149!:"<=>?m<=@35140
7QP<=7d9(EC*E 7p@1435ENC*Ok72Ek<=7p14< 6PDENR*721ff>M35E/E oHE Xl0Q721ffo Aa<!C*1ffI:=<=>?<=@351?0p18q72>< 6872149
X[>QENL*//*//)/< C(9GLS<=>T 149_*X>ENL7QP(1\9*18C(?57Q?5ENC-E XYKA*< 33:=<=>?<=@3140bE XzkIU?33
7QP(1ff>Q1ffXE >1|@1g>Q14< 6P*149k< C(9%?C021ff>Q72149?CG*/q//i/(^
==

fiUi5
(/
5Q 54=QN(
* (


fiff fii
j/
ki

(')*+

fi !N)=#"$$%&

, . -0/ 13254687 -09:d<;>=

? (/ (N*AB @ = C
E E GFIHJ'qq((i =
*&FIHLKNMPOfiFIHQ
C ((RMTS
('
) UF H ?
'()
Ofi*VW
C U')
j/XF HZY F\[]>^N
I^ F_[ CW'qq(N(
OfifiF [
U

ff fi/


OfifiFIHa
N! #"dc%
=
= ?

, . -0/Le 13254687 -09:d<;>fhgi=kjmlN(Q.9 4n

=
': 4

=F`[ab @
=

C

Q5N<;_o8p

o8p
? (/ (N*AB @ f C
E E GFrq *T) u
C T)wvW*
fiffC E/iE/
*&Frq&KNMxOfiFrqy
C (N(RMTS
('
) UFrq ?
&(')
Ofi*VW
C U')
E E GFrj
z ts = *){ (*
u jT
)|vx ! )=i}"~C%
OfiFIz Frqy
o8p
j/XFq F_&_^$
r^ F_
Cyq
' q((UC
OfifiF_G
C U*

fiffC Efi iE/

OfifiFrqy
&N! (i#"%
=
= ?
9- fi1 l 6 / 4

Q=Q5N%<;S-ffB<;|/) n ((=.-4}o8p = n 6 /=Q 7 -g<9-09:



`<fi 7fiXn N(2.9 4En Q5N<;d-y.-ffQ ! ) (i}"c(% P"9 7 - FIH %(-09Q.-
!N)=#"c(% 6Q6 -(m=.-4 ;[.-09 F H ( 7 -0-4V 4687 -09-(SN( //-4J;[<9-- 4 C
6 9<- F [< , EF [ - n -4QQ9 4687 -09-;.-09 FrH

(mB QQ:N 6 -4<9-09:J= n 6 /=Q 7 -
= 9 7 -49-| 7 q 4 Q *N.-|Q<-4;d9Q
-E92
( 9- n n N( - 4 -4 n -|<; n N(5Q5Ned 7 <-



6

=

K 2.-0/ -g -09 n N(Q5N

fiEEIEECETE_Er

((
`
I<..0<Q<a..<fi(.0*TEJI|Q<ifi00><
<<] LEQ<a<0.(QQa<{0(LE..0fi
(E..<Q<Ex.#Ex<.ifiQ.X}<0<#a(aC P
0 <
Q
#0fi







ff
fi r
.&QT

rr.0} }E< (


r

"!$#%"&(')%"*+%",.-

CBI(aE.0EDC0GF|<.<H JI (LK
0 <rI <fi<<<(Mfia(E0fi0I0 <rI (Qa<
<<(0N/<.<POEr<0<R
Q_<0<T
S(EM
<(VU(3}
<{JQ<r_E0>#>.ar<`E<W E{<.8X
Z
Y[:=2[\]>@< .
_^.G03257Y`baMc[d?A V2e6?:)7
/#.<103254)6798;:=<?>@6)8=7A.<

%[gL%[h%"b%"f

I0<<5Bw9ij_kmlNn]o "pCrqaTs

pCrq N
nut.n]o "
vC[Q_0

Iu0w <MB9i.i_xXOyK$(..r<8yK$(..r(<bz{n |} . ~E$ re} [.|
ji9 ikC

I<O89i.Ck}Q\<<<QQF|<(<O5. }EFwwOBr(..<LK
.C
nLo [q } [q8 n tnLo JJ t?o t[o9t rq ;
DC8(
Fw<.<3 <z ?

q$m
q
> x5
..X.j
BIfi<0
zn

B5bBI(v.OXS<<(E0MQ>M9i.i_x?Br<.05.<E}(E..<0

|}0 ;~$ e} .;_xMj9X.C

vCQ|39i.i.(


}3 qaL{~ z
n

(<_<iE ^.<Q(..<<RBZDbQX(WC(Q< V/
~] <wE<_
x_fi<. ifi?
B<N
0 3 <"<<

< BI(..<Q3fi>I<9^aFwbK
<$Br.0Zv5(a((zn |}0 ;~$ e} "">u(jXCfi

v5.0bE9i.ik?U3E<aDCa

v5.0ew9i.i<((kC0<.Q(<E<(..r<z{n

|}0 e
~E$0d re} )>;j9kj

<..0 <(..< <(<K
~E$0d re} ?9r_X.fi
Q0z{n | }0 ;

v5.0?=Q| 5
S>9i..(0$I<K

v5.05eQ> ;
Sw9i..i(|00E..0

e}
r

)>u(=..X..fi

0<bK$I.(5N8N

<.L<(E<</

<{BI(E..<Z#0N<fi0Zz{n | }0 ;
~E$0d@

<ZO89i.i<(/.0<W(E..<#G<]<b^.LK
n
}3 q] {N
JL ZE. .fi. .fi


fi;W

H9"9Z"{=L9 XbH[9HW9 []].Nb[H93HW[{Emb[u3[VLb?
X L.?..X..X

eZ y9H]r

{3
3efiff W . (L9.L[rH9.H[]H$H933 W9[r.Cr[]]3.M
3H .rHW [3. W9({r u;"@ reyr ..Xb9X
."H]9= Wfi .?! W#
" [W9 {W
$ 39P % {W# N.]]HJ{L9.ff'&("*)fi+-,/.
, HH ]]99HW$
5[H[V]bH[9H .[1m
{
0
2 be. LCrE r43 L T..


"]5HMM 5L9..T{X[.GN.HX]3.3X[ be.=]J{r u["r
. X

X

5W3[HJ L9.. ".]C"3
W r[]]3.MCHH.rHW= {r 9.N"@ reyr76

9e9b
)"39H T{) W 9
8 ]5.3=;(L9$N."[[].b:. 3 HW[R<;.=;b?>
@ 3 rr @ r;yy BA C E F X
.HF
G .HH?V
, H
5 ZL9.. N[H]]9"r1I" JG .3X
*Zff H[ {r u "r reyr
K L=.e.9X
$9#
ff W! L
9[3]?!
) N{L9..M, 3$.[N H+ "H]]9[rff PXO 3W]9=ff{y 9
er e K C

. F. .X
$9+ff W
$.H [u L9..Q
ff [[[[
W9MRW r"H]]9[r.S @ 3 " 9u?>
@ [9b]DT .uryUZ 33 [u[;[?Vb 9F. b)X[.%W$ X b X 9 W 5H
$H.[u[V
Z L9\M ]
" ^ G .Hb
Z []]3. HP.( [[[.9H
[3.;3HW9.[[ HW[
]G"rHH"HXr93H 9y @ reyyy L=.e9
X

$.3T[, W [].m_, W` Q(
$ .[[75a .[9?){L9.
P9P 3Hb.O G"H..3. P
"H.].X5
% []H.[r HX]]9c
. .BLd 9re% .39]9 [H..3.GfG WHgZG WH Hb[
UJT L9y3X> @ E]ihjhT @<k jlm [)?9E_ F
b ?($ ."rB.O 9;N.".[b
L
(3H]9 ML9..'
M"3
W ..3H[D .)H r"]]3.M)HHR .rHW [H. 9P5.;= b ;
er e[
uD. .F. .X
)n]d .?+
ff L9 {[H<%C"o 3HCH933.[p 9 3"
W ..3H[ @ ]{ be..
.;
= b C e9b
O. ;q (L9.
/
8 $HE$ [
C.[${9r5He W P
[]].(
" ^ G .HX9*L
L3X> L{{{4lem [?<
9
X )b.r ].;N
@ .3"b
J
O.1 eH W {93M]9s
ff L9.EM $N[]]3.M:Z) H93j.O 3H[ W ."
[HH]9[r.]L
L3X> L*tj*lm " [? 3.M.
J
$(.u
9M]9HbH;F WM{PbO W .E
& WF v)) 9{ % eL9.
w+
, .93J3x% r"H]]9[r
W ..WH[
." WHZ]59
W9HW[ {r ue"@ reyr[y C
Xw. bX
b
z{z

fiJournal Artificial Intelligence Research 2 (1995) 541-573

Submitted 9/94; published 5/95

Pac-learning Recursive Logic Programs:
Negative Results
William W. Cohen

AT&T Bell Laboratories
600 Mountain Avenue, Murray Hill, NJ 07974 USA

wcohen@research.att.com

Abstract

companion paper shown class constant-depth determinate k-ary
recursive clauses eciently learnable. paper present negative results showing
natural generalization class hard learn Valiant's model paclearnability. particular, show following program classes cryptographically
hard learn: programs unbounded number constant-depth linear recursive
clauses; programs one constant-depth determinate clause containing unbounded
number recursive calls; programs one linear recursive clause constant locality.
results immediately imply non-learnability general class programs.
also show learning constant-depth determinate program either two linear
recursive clauses one linear recursive clause one non-recursive clause hard
learning boolean DNF. Together positive results companion paper,
negative results establish boundary ecient learnability recursive function-free
clauses.

1. Introduction
Inductive logic programming (ILP) (Muggleton, 1992; Muggleton & De Raedt, 1994)
active area machine learning research hypotheses learning system
expressed logic programming language. many different learning problems
considered ILP, including great practical interest (Muggleton, King,
& Sternberg, 1992; King, Muggleton, Lewis, & Sternberg, 1992; Zelle & Mooney, 1994;
Cohen, 1994b), class problems frequently considered reconstruct simple
list-processing arithmetic functions examples. prototypical problem sort
might learning append two lists. Often, sort task attempted using
randomly-selected positive negative examples target concept.
Based similarity problems studied field automatic programming
examples (Summers, 1977; Biermann, 1978), (informally) call class
learning tasks automatic logic programming problems. number experimental
systems built (Quinlan & Cameron-Jones, 1993; Aha, Lapointe, Ling, & Matwin,
1994), experimental success automatic logic programming systems limited.
One common property automatic logic programming problems presence recursion . goal paper explore analytic methods computational limitations
learning recursive programs Valiant's model pac-learnability (1984). (In brief,
model requires accurate approximation target concept found polynomial time using polynomial-sized set labeled examples, chosen stochastically.)
surprise nobody limitations exist, far obvious previous
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCohen

research limits lie: provably fast methods learning recursive
logic programs, even fewer meaningful negative results.
starting point investigation series positive learnability results appearing companion paper (Cohen, 1995). results show single constant-depth
determinate clause constant number \closed" recursive calls pac-learnable.
also show two-clause constant-depth determinate program consisting one nonrecursive clause one recursive clause type described pac-learnable,
additional \hints" target concept provided.
paper, analyze number generalizations learnable languages.
show relaxing restrictions leads dicult learning problems: particular, learning problems either hard learning DNF (an open problem
computational learning theory), hard cracking certain presumably secure cryptographic schemes. main contribution paper, therefore, delineation
boundaries learnability recursive logic programs.
paper organized follows. Section 2 define classes logic programs
learnability models used paper. Section 3 present cryptographic
hardness results two classes constant-depth determinate recursive programs: programs
n linear recursive clauses, programs one n-ary recursive clause. also
analyze learnability clauses constant locality, another class clauses paclearnable nonrecursive case, show even single linearly recursive local
clause cryptographically hard learn. turn, Section 4, analysis
even restricted classes recursive programs. show two different classes
constant-depth determinate programs prediction-equivalent boolean DNF: class
programs containing single linear recursive clause single nonrecursive clause,
class programs containing two linearly recursive clauses. Finally, summarize
results paper companion, discuss related work, conclude.
Although paper read independently companion paper suggest
readers planning read papers begin companion paper (Cohen, 1995).

2. Background

completeness, present technical background needed state results;
however, aside Sections 2.2 2.3, introduce polynomial predictability
prediction-preserving reducibilities, respectively, background closely follows presented companion paper (Cohen, 1995). Readers encouraged skip section
already familiar material.

2.1 Logic Programs

assume reader familiarity logic programming (such
obtained reading one standard texts (Lloyd, 1987).) treatment logic
programs differs usually consider body clause ordered
set literals. also consider logic programs without function symbols|i.e.,
programs written Datalog.
semantics Datalog program P defined relative database , DB ,
set ground atomic facts. (When convenient, also think DB
542

fiPac-Learning Recursive Logic Programs: Negative Results

conjunction ground unit clauses). particular, interpret P DB subset
set extended instances . extended instance pair (f; D)
instance fact f ground fact, description set ground unit clauses.
extended instance (f; D) covered (P; DB ) iff
DB ^ ^ P ` f
extended instances allowed, function-free programs encode many computations usually represented function symbols. example, function-free
program tests see list append two lists written follows:

Program P :

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
append(Xs1,Ys,Zs1).

Database DB :

null(nil).
predicate components(A,B,C) means list head B tail C; thus
extended instance equivalent append([1,2],[3],[1,2,3]) would instance fact
f = append (list12 ; list3 ; list123 ) description containing atoms:
components(list12,1,list2), components(list2,2,nil),
components(list123,1,list23), components(list23,2,list3),
components(list3,3,nil)
use extended instances function-free programs closely related \ attening"
(Rouveirol, 1994; De Raedt & Dzeroski, 1994); experimental learning systems also
impose similar restriction (Quinlan, 1990; Pazzani & Kibler, 1992). Another motivation
using extended instances technical. (sometimes quite severe) syntactic
restrictions considered paper, often polynomial number possible
ground facts|i.e., Herbrand base polynomial. Hence programs interpreted
usual model-theoretic way would possible learn program equivalent
given target simply memorizing appropriate subset Herbrand base. However,
programs interpreted sets extended instances, trivial learning algorithms
become impossible; even extremely restricted program classes still exponential number extended instances size n. discussion found
companion paper (Cohen, 1995).
define terminology logic programs used
paper.
2.1.1 Input/Output Variables

B1 ^ : : : ^ Br (ordered) definite clause, input variables literal Bi
variables also appear clause B1 ^ : : : ^ Bi,1 ; variables
appearing Bi called output variables .
543

fiCohen

2.1.2 Types Recursion

literal body clause recursive literal predicate symbol
arity head clause. every clause program one recursive
literal, program linear recursive . every clause program k recursive
literals, program k-ary recursive . every recursive literal program contains
output variables, program closed recursive.
2.1.3 Depth

depth variable appearing (ordered) clause B1 ^ : : : ^ Br defined follows.
Variables appearing head clause depth zero. Otherwise, let Bi first
literal containing variable V , let maximal depth input variables
Bi ; depth V +1. depth clause maximal depth variable
clause.
2.1.4 Determinacy

literal Bi clause B1 ^ : : : ^ Br determinate iff every possible substitution
unifies fact e
DB ` B1 ^ : : : ^ Bi,1

one maximal substitution DB ` Bi . clause determinate
literals determinate. Informally, determinate clauses
evaluated without backtracking Prolog interpreter.
term ij -determinate (Muggleton & Feng, 1992) sometimes used programs
depth i, determinate, contain literals arity j less. number experimental systems exploit restrictions associated limited depth determinacy (Muggleton
& Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). learnability constant-depth determinate clauses also received formal study (Dzeroski,
Muggleton, & Russell, 1992; Cohen, 1993a).
2.1.5 Mode Constraints Declarations

Mode declarations commonly used analyzing Prolog code describing Prolog code;
instance, mode declaration \components (+; ,; ,)" indicates predicate components used first argument input second third arguments
outputs. Formally, define mode literal L appearing clause C
string initial character predicate symbol L, j > 1
j -th character \+" (j , 1)-th argument L input variable
\," (j , 1)-th argument L output variable. (This definition assumes
arguments head clause inputs; justified since considering
clauses behave classifying extended instances, ground.) mode constraint
set mode strings R = fs1 ; : : :; sk g, clause C said satisfy mode constraint
R p every literal L body C , mode L R.
define declaration tuple (p; a0; R) p predicate symbol, a0
integer, R mode constraint. say clause C satisfies declaration
544

fiPac-Learning Recursive Logic Programs: Negative Results

head C arity a0 predicate symbol p, every literal L body
C mode L appears R.
2.1.6 Determinate Modes

typical setting, facts database DB extended instances arbitrary:
instead, representative \real" predicate, may obey certain restrictions. Let us assume database extended-instance facts drawn
(possibly infinite) set F . Informally, mode determinate input positions
facts F functionally determine output positions. Formally, f = p(t1 ; : : :; tk )
fact predicate symbol p pff mode, define inputs (f; pff) hti1 ; : : :; ti i,
i1 , : : : , ik indices ff containing \+", define outputs (f; pff)
htj1 ; : : :; tj i, j1, : : : , jl indices ff containing \,". define mode
string pff predicate p determinate F iff
k

l

fhinputs (f; pff); outputs (f; pff)i : f 2 Fg
function. clause satisfies declaration Dec 2 DetDEC must determinate.
set declarations containing modes determinate F denoted
DetDEC F . Since paper set F assumed fixed, generally omit
subscript.
2.1.7 Bounds Predicate Arity

use notation a-DB set databases contain facts arity
less, a-DEC set declarations (p; a0; R) every string 2 R
length + 1 less.
2.1.8 Size Measures

learning models presented following section require learner use resources polynomial size inputs. Assuming predicates arity
less constant allows simple size measures used. paper,
measure size database DB cardinality; size extended instance (f; D)
cardinality D; size declaration (p; a0; R) cardinality R;
size clause B1 ^ : : : ^ Br number literals body.

2.2 Model Learnability
2.2.1 Preliminaries

Let X set. call X domain , call elements X instances . Define
concept C X representation subset X , define language Lang
set concepts. paper, rather casual distinction
concept set represents; risk confusion refer
set represented concept C extension C . Two sets C1 C2
extension said equivalent . Define example C pair (e; b) b = 1
e 2 C b = 0 otherwise. probability distribution function, sample C
545

fiCohen

X drawn according pair multisets + ; , drawn domain X according
D, + containing positive examples C , , containing negative ones.
Associated X Lang two size complexity measures , use
following notation:

size complexity concept C 2 Lang written j C j .
size complexity instance e 2 X written j ej .
set, Sn stands set elements size complexity greater
n. instance, Xn = fe 2 X : j ej ng Langn = fC 2 Lang : j C j ng.
assume size measures polynomially related number bits needed
represent C e; holds, example, size measures logic programs
databases defined above.
2.2.2 Polynomial Predictability

define polynomial predictability follows. language Lang polynomially
predictable iff algorithm PacPredict polynomial function m( 1 ; 1 ; ne ; nt)
every nt > 0, every ne > 0, every C 2 Langn , every : 0 < < 1, every
: 0 < < 1, every probability distribution function D, PacPredict following
behavior:


1. given sample + ; , C Xn drawn according containing least
m( 1 ; 1 ; ne ; nt) examples, PacPredict outputs hypothesis H
e

Prob(D(H , C ) + D(C , H ) > ) <
probability taken possible samples + , (if PacPredict
randomized algorithm) coin ips made PacPredict;
2. PacPredict runs time polynomial 1 , 1 , ne , nt , number examples;

3. hypothesis H evaluated polynomial time.
algorithm PacPredict called prediction algorithm Lang, function m( 1 ; 1 ; ne ; nt ) called sample complexity PacPredict. sometimes
abbreviate \polynomial predictability" \predictability".
first condition definition merely states error rate hypothesis
must (usually) low, measured probability distribution
training examples drawn. second condition, together stipulation
sample size polynomial, ensures total running time learner polynomial.
final condition simply requires hypothesis usable weak sense
used make predictions polynomial time. Notice worst case
learning model, definition allows adversarial choice inputs learner.
546

fiPac-Learning Recursive Logic Programs: Negative Results

2.2.3 Relation Models

model polynomial predictability well-studied (Pitt & Warmuth, 1990),
weaker version Valiant's (1984) criterion pac-learnability . language Lang
pac-learnable iff algorithm PacLearn
1. PacLearn satisfies requirements definition polynomial predictability,

2. inputs + , , PacLearn always outputs hypothesis H 2 Lang.
Thus language pac-learnable predictable.
companion paper (Cohen, 1995), positive results expressed model
identifiability equivalence queries, strictly stronger pac-learnability;
is, anything learnable equivalence queries also necessarily pac-learnable.1
Since paper contains negative results, use relatively weak model
predictability. Negative results model immediately translate negative results
stronger models; language predictable, cannot pac-learnable,
identifiable equivalence queries.
2.2.4 Background Knowledge Learning

typical ILP system, setting slightly different, user usually provides clues
target concept addition examples, form database DB
\background knowledge" set declarations. account additional inputs
necessary extend framework described setting learner accepts
inputs training examples. Following formalization used companion
paper (Cohen, 1995), adopt notion \language family".
Lang set clauses, DB database Dec declaration, define
Lang[DB ; Dec] set pairs (C; DB ) C 2 Lang C satisfies Dec .
Semantically, pair denote set extended instances (f; D) covered
(C; DB ). Next, DB set databases DEC set declarations, define
Lang[DB ; DEC ] = fLang[DB ; Dec ] : DB

2 DB Dec 2 DECg

set languages called language family .
extend definition predictability queries language families follows.
language family Lang[DB; DEC ] polynomially predictable iff every language set
predictable. language family Lang[DB; DEC ] polynomially predictable iff
single algorithm Identify(DB ; Dec ) predicts every Lang[DB ; Dec] family
given DB Dec .
usual model polynomial predictability worst-case choices target
concept distribution examples. notion polynomial predictability
language family extends model natural way; extended model also worstcase possible choices database DB 2 DB Dec 2 DEC . worst-case
1. equivalence query question form \is H equivalent target concept?" answered
either \yes" counterexample. Identification equivalence queries essentially means
target concept exactly identified polynomial time using polynomial queries.

547

fiCohen

model may seem unintuitive, since one typically assumes database DB provided
helpful user, rather adversary. However, worst-case model reasonable
learning allowed take time polynomial size smallest target concept
set Lang[DB ; Dec ]; means database given user
target concept cannot encoded succinctly (or all) learning allowed take
time.
Notice language family Lang[DB ; Dec] polynomially predictable, every
language family must polynomially predictable. Thus show family
polynomially predictable sucient construct one language family
learning hard. proofs paper form.

2.3 Prediction-Preserving Reducibilities

principle technical tool used negative results notion prediction-preserving
reducibility , introduced Pitt Warmuth (1990). Prediction-preserving reducibilities
method showing one language harder predict another. Formally,
let Lang1 language domain X1 Lang2 language domain X2.
say predicting Lang1 reduces predicting Lang2, denoted Lang1 Lang2 ,
function fi : X1 ! X2 , henceforth called instance mapping , function
fc : Lang1 ! Lang2 , henceforth called concept mapping , following hold:
1. x 2 C fi (x) 2 fc (C ) | i.e., concept membership preserved
mappings;
2. size complexity fc (C ) polynomial size complexity C |i.e., size
concept representations preserved within polynomial factor;
3. fi (x) computed polynomial time.
Note fc need computable; also, since fi computed polynomial time,
fi(x) must also preserve size within polynomial factor.
Intuitively, fc (C1) returns concept C2 2 Lang2 \emulate" C1|i.e., make
decisions concept membership|on examples \preprocessed"
function fi . predicting Lang1 reduces predicting Lang2 learning
algorithm Lang2 exists, one possible scheme learning concepts Lang1
would following. First, convert examples unknown concept C1
domain X1 examples domain X2 using instance mapping fi .
conditions definition hold, since C1 consistent original examples,
concept fc (C1) consistent image fi ; thus running learning
algorithm Lang2 produce hypothesis H good approximation
fc (C1). course, may possible map H back original language Lang1,
computing fc ,1 may dicult impossible. However, H still used predict
membership C1: given example x original domain X1, one simply predict
x 2 C1 true whenever fi (x) 2 H .
Pitt Warmuth (1988) give rigorous argument approach leads
prediction algorithm Lang1 , leading following theorem.
548

fiPac-Learning Recursive Logic Programs: Negative Results

Theorem 1 (Pitt Warmuth) Assume Lang1 Lang2. Lang1 polynomially predictable, Lang2 polynomially predictable.

3. Cryptographic Limitations Learning Recursive Programs

Theorem 1 allows one transfer hardness results one language another.
useful number languages, known prediction hard breaking
cryptographic schemes widely assumed secure. example, known
predicting class languages accepted deterministic finite state automata
\cryptographically hard", class languages accepted log-space bounded Turing
machines.
section make use Theorem 1 previous cryptographic hardness
results show certain restricted classes recursive logic programs hard learn.

3.1 Programs n Linear Recursive Clauses

companion paper (Cohen, 1995) showed single linear closed recursive clause
identifiable equivalence queries. section show program
polynomial number clauses identifiable equivalence queries, even
polynomially predictable.
Specifically, let us extend notion \family languages" slightly, let
DLog[n; s] represent language log-space bounded deterministic Turing machines
states accepting inputs size n less, usual semantics complexity
measure.2 Also let d-DepthLinRecProg denote family logic programs containing
depth-d linear closed recursive clauses, containing number clauses.
following result:
Theorem 2 every n s, exists database DB n;s 2 1-DB declaration
Dec n;s 2 1-DetDEC sizes polynomial n
DLog[n; s] 1-DepthLinRecProg[DB n;s ; Dec n;s ]
Hence 1 1, d-DepthLinRecProg[DB; a-DetDEC ] uniformly polynomially predictable cryptographic assumptions.3
Proof: Recall log-space bounded Turing machine (TM) input tape length
n, work tape length log2 n initially contains zeros, finite state control
state set Q. simplify proof, assume without loss generality tape
input alphabets binary, single accepting state qf 2 Q,
machine always erase work tape position work tape head far left
decides accept input.
time step, machine read tape squares input tape head
work tape head, based values current state q ,
2. I.e., machine represents set inputs accepts, complexity number states.
3. Specifically, language uniformly polynomially predictable unless following cryptographic problems solved polynomial time: solving quadratic residue problem, inverting
RSA encryption function, factoring Blum integers. result holds cryptographic problems reduced learning DLOG Turing machines (Kearns & Valiant, 1989).

549

fiCohen






write either 1 0 work tape,
shift input tape head left right,
shift work tape head left right,
transition new internal state q 0
deterministic machine thus specified transition function

: f0; 1g f0; 1g Q ,! f0; 1g fL; Rg fL; Rg Q
Let us define internal configuration TM consist string symbols
written worktape, position tape heads, internal state q
machine: thus configuration element set
CON f0; 1glog2 n f1; : : :; log2 ng f1; : : :; ng Q

simplified specification machine transition function

0 : f0; 1g CON ! CON
component f0; 1g represents contents input tape square
input tape head.
Notice machine whose worktape size bounded log n, cardinality
CON p = jQjn2 log2 n, polynomial n = jQj. use fact
constructions.
background database DB n;s follows. First, = 0; : : :; p, atom
form coni(ci ) present. constant ci represent different internal configuration
Turing machine. also arbitrarily select c1 represent (unique) accepting
configuration, add DB n;s atom accepting(c1). Thus
DB n;s fcon (ci)gpi=1 [ faccepting (c1)g

Next, define instance mapping. instance Turing machine's domain
binary string X = b1 : : :bn ; mapped fi extended instance (f; D)

f accepting (c0 )
ftruei gb 2X :b =1 [ ffalsei gb 2X :b =0








description atoms effect defining predicate truei true iff i-th
bit X \1", defining predicate falsei true iff i-th bit X
\0". constant c0 represent start configuration Turing machine,
predicate accepting(C) defined true iff Turing machine accepts input
X starting state C.
let Dec n;s = (accepting ; 1; R) R contains modes coni (+) coni (,),
= 1; : : :; p; truej falsej j = 1; : : :; n.
Finally, concept mapping fc , let us assume arbitrary one-to-one mapping
internal configurations Turing machine predicate names
550

fiPac-Learning Recursive Logic Programs: Negative Results

con0,: : : ,conp,1 start configuration (0log2 n ; 1; q0) maps con0 accepting configuration (0log2 n ; 1; qf ) maps con1. construct program fc (M )
follows. transition 0(1; c) ! c0 0, c c0 CON , construct
clause form

accepting(C)

conj (C) ^ truei ^ conj 0 (C1) ^ accepting(C1).

position input tape head encoded c, con j = (c),
con j 0 = (c0). transition 0(0; c) ! (c0) 0 construct analogous clause,
truei replaced falsei.
Now, claim program P , machine accept started
configuration ci iff
DB n;s ^ ^ P ` accepting (ci )
hence construction preserves concept membership. perhaps easiest see considering action top-down theorem prover given goal
accepting (C ): sequence subgoals accepting (ci ), accepting (ci +1 ), : : : generated
theorem-prover precisely parallel sequence configurations ci , : : : entered Turing
machine.
easily verified size program polynomial n s,
clauses linear recursive, determinate, depth one, completing proof.
number ways result strengthened. Precisely
construction used used reduce class nondeterministic log-space
bounded Turing machines constant-depth determinate linear recursive programs.
Further, slight modification construction used reduce class log-space
bounded alternating Turing machines (Chandra, Kozen, & Stockmeyer, 1981) constantdepth determinate 2-ary recursive programs. modification emulate configurations
corresponding universal states Turing machine clauses form
accepting(C)
conj (C) ^ truei ^
conj 10 (C1) ^ accepting(C1) ^
conj 20 (C2) ^ accepting(C2).
conj1 0 conj2 0 two successors universal configuration conj .
strong result, since log-space bounded alternating Turing machines known
able perform every polynomial-time computation.

3.2 Programs One n-ary Recursive Clause

consider learning single recursive clause arbitrary closed recursion.
Again, key result section observation expressive power:
background database allows every log-space deterministic Turing machine
emulated single recursive constant-depth determinate clause. leads
following negative predictability result.
551

fiCohen

Theorem 3 every n s, exists database DB n;s 2 3-DB declaration
Dec n;s 2 3-DetDEC sizes polynomial n
DLog[n; s] 3-DepthRec[DB n;s ; Dec n;s ]
Hence 3 3, d-DepthRec[DB n ; a-DetDEC ] uniformly polynomially
predictable cryptographic assumptions.

Proof: Consider DLOG machine . proof Theorem 2, assume without
loss generality tape alphabet f0; 1g, unique starting configura-

tion c0, unique accepting configuration c1. also assume without
loss generality unique \failing" configuration cf ail; exactly
one transition form
0(b; cj) ! c0j
every combination 2 f1; : : :; ng, b 2 f0; 1g, cj 2 CON , fc1; cf ailg. Thus
input X = x1 : : :xn machine starts CONFIG=c0 , executes transitions
reaches CONFIG=c1 CONFIG=cf ail, point X accepted rejected
(respectively). use p number configurations. (Recall p polynomial
n s.)
emulate , convert example X = b1 : : :bn extended instance
fi(X ) = (f; D)

f accepting (c0 )
fbit (bi)gni=1
Thus predicate bit (X ) binds X i-th bit TM's input tape. also
define following predicates background database DB n;s .

every possible b 2 f0; 1g j : 1 j p(n), predicate statusb;j (B,C,Y)
defined given bindings variables B C , statusb;j (B,C,Y) fail
C = cf ail; otherwise succeed, binding active B = b C = cj
binding inactive otherwise.
j : 1 j p(n), predicate nextj (Y,C) succeed iff bound
either active inactive. = , C bound cj ; otherwise, C
bound accepting configuration c1.
database also contains fact accepting (c1 ).
easy show size database polynomial n s.
declaration Dec n;s defined (accepting ; 1; R) R includes modes
status bj (+; +; ,), next j (+; ,), bit (,) b 2 f0; 1g, j = 1; : : :; p, = 1; : : :; n.
Now, consider transition rule 0(b; cj ) ! c0j , corresponding conjunction
TRANSibj biti (Bibj ) ^ statusb;j (C,Bibj ,Yibj ) ^ nextj 0 (Yibj ,C1ibj ) ^ accepting(C1ibj )
552

fiPac-Learning Recursive Logic Programs: Negative Results

Given DB n;s D, assuming C bound configuration c, conjunction
fail c = cf ail. succeed xi 6= b c 6= cj ; case Yibj bound
inactive, C1ibj bound c1, recursive call succeeds accepting(c1)
DB n;s . Finally, xi = b c = cj , TRANSibj succeed atom accepting(cj 0 )
provable; case, Yibj bound active C1ibj bound cj 0 .
clear clause fc (M )
^
accepting(C)
TRANSibj


2f1;:::;ng; b2f0;1g
j 2f1;:::;pg

correctly emulate machine examples preprocessed
function fi described above. Hence construction preserves concept membership.
also easily verified size program polynomial n s,
clause determinate depth three.

3.3 One k-Local Linear Closed Recursive Clause

far considered one class extensions positive result given
companion paper (Cohen, 1995)|namely, relaxing restrictions imposed recursive
structure target program. Another reasonable question ask linear closed
recursive programs learned without restriction constant-depth determinacy.
earlier papers (Cohen, 1993a, 1994a, 1993b) studied conditions
constant-depth determinacy restriction relaxed still allowing learnability nonrecursive clauses. turns generalizations constant-depth
determinate clauses predictable, even without recursion. However, language
nonrecursive clauses constant locality pac-learnable generalization constant-depth
determinate clauses. Below, define language, summarize relevant previous
results, address question learnability recursive local clauses.
Define variable V appearing clause C free appears body C
head C . Let V1 V2 two free variables appearing clause. V1 touches V2
appear literal, V1 uences V2 either touches V2, touches
variable V3 uences V2. locale free variable V set literals
either contain V , contain free variable uenced V . Informally, variable
V1 uences variable V2 choice binding V1 affect possible choices
bindings V2.
locality clause size largest locale. Let k-LocalNonRec denote
language nonrecursive clauses locality k less. (That is, k-LocalNonRec
set logic programs containing single nonrecursive k-local clause.) following facts
known (Cohen, 1993b):
fixed k a, language family k-LocalNonRec[a-DB; a-DEC] uniformly
pac-learnable.
every constant d, every constant a, every database DB 2 a-DB, every declaration
Dec 2 a-DetDEC , every clause C 2 d-DepthNonRec[DB ; Dec ],
553

fiCohen

equivalent clause C 0 k-LocalNonRec[DB ; Dec] size bounded kj C j , k
function (and hence constant also constants.)
Hence
k-LocalNonRec[DB; a-DEC]
pac-learnable generalization

d-DepthNonRec[DB; a-DetDEC ]
thus plausible ask recursive programs k-local clauses pac-learnable.
facts learnability k-local programs follow immediately previous results.
example, immediate consequence construction Theorem 2 programs
polynomial number linear recursive k-local clauses predictable k 2.
Similarly, Theorem 3 shows single recursive k-local clause predictable k 4.
still reasonable ask, however, positive result bounded-depth determinate
recursive clauses (Cohen, 1995) extended k-ary closed recursive k-local clauses.
Unfortunately, following negative result, shows even linear closed
recursive clauses learnable.

Theorem 4 Let Dfa[s] denote language deterministic finite automata states,

let k-LocalLinRec set linear closed recursive k-local clauses. constant exists database DB 2 3-DB declaration Dec 2 3-DEC , size
polynomial s,
Dfa[s] 3-LocalLinRec[DB ; Dec ]

Hence k 3 3, k-LocalLinRec[a-DB ; Dec] uniformly polynomially
predictable cryptographic assumptions.

Proof: Following Hopcroft Ullman (1979) represent DFA alphabet

tuple (q0; Q; F; ) q0 initial state, Q set states, F set
accepting states, : Q ! Q transition function (which sometimes
think subset Q Q). prove theorem, need construct database
DB size polynomial every s-state DFA emulated linear
recursive k-local clause DB .
Rather directly emulating , convenient emulate instead modification . Let M^ DFA state set Q^ Q [ fq(,1); qe ; qf g, q(,1) , qe qf
new states found Q. initial state M^ q(,1) . final state M^
qf . transition function M^
[
^ [ f(q(,1); a; q0); (qe; c; qf )g [
f(qi; b; qe)g
2

qi F

a, b, c new letters . Note M^ DFA alphabet
[ fa; b; cg, and, described, need complete DFA alphabet. (That
is, may pairs (qi ; a) ^(qi ; a) undefined.) However, M^ easily
554

fiPac-Learning Recursive Logic Programs: Negative Results



1


q


0

?

0

M^

1



q



- ?

1

0

1

1







q
q
q
q
q


,1

0

?



-

0

M0




-

0

?

1

b,c,0,1








1







b

-

a,b,c
1

-

c

e


q

?

r

a,b,c

-

f

a,b,c,0,1
a,b,c,0,1


6

a,b,
0,1







q
q
q
q
q









,1





0

?

-



0
0

-

?

1





,
,

,
, b -



e

c

-

f

Figure 1: DFA modified emulation local clause

555

fiCohen

made complete introducing additional rejecting state qr , making every undefined
transition lead qr . precisely, let 0 defined
0 ^ [ f(qi; x; qr) j qi 2 Q^ ^ x 2 [ fa; b; cg ^ (6 9qj : (qi ; x; qj ) 2 ^)g
Thus 0 = (q(,1); Q^ [fqr g; fqf g; 0) \completed" version M^ , Q0 = Q^ [fqr g.
use 0 construction below; also let Q0 = Q^ [ fqr g 0 = [ fa; b; cg.
Examples , M^ 0 shown Figure 1. Notice aside arcs
rejecting state qr , state diagram 0 nearly identical .
differences 0 new initial state q(,1) single outgoing arc
labeled old initial state q0 ; also every final state 0 outgoing arc
labeled b new state qe , turn single outgoing arc labeled c final
state qf . easy show

x 2 L(M ) iff axbc 2 L(M 0)
Now, given set states Q0 define database DB contains following
predicates:
arcq ;;q (S,X,T) true 2 Q0, 2 Q0, X 2 0, unless = qi,
X = , 6= qj .
state(S) true 2 Q0.
accept(c,nil,qe,qf ) true.
motivation arc predicates, observe emulating 0 clearly useful
able represent transition function 0. usefulness arc predicates
transition function 0 represented using conjunction arc literals. particular,
conjunction
^
arc q ;;q (S; X; )


j



(q ;;q )20


j

j

succeeds 0 (S; X ) = , fails otherwise.
Let us define instance mapping fi fi (x) = (f; D)

f = accept (a; xbc; q(,1); q0)
set facts defines components relation list corresponds
string xbc. words, x = 1 : : :n , set facts
components(1 : : :n bc; 1; 2 : : :n bc)
components(2 : : :n bc; 2; 3 : : :n bc)
..
.
components(c,c,nil)
declaration Dec n Dec n = (accept ; 4; R) R contains modes
components (+; ,; ,), state (,), arc q ;;q (+; +; +) qi , qj Q0 , 2 0 .
Finally, define concept mapping fc (M ) machine clause


j

556

fiPac-Learning Recursive Logic Programs: Negative Results

accept(X,Ys,S,T)
V
(q ;;q )20 arcq ;;q (S,X,T)
^ components(Ys,X1,Ys1) ^ state(U) ^ accept(X1,Ys1,T,U).
0 transition function corresponding machine 0 defined above.
easy show construction polynomial.
clause X letter 0, Ys list letters, states
Q0 . intent construction predicate accept succeed exactly
(a) string XYs accepted 0 0 started state , (b) first action
taken 0 string XYs go state state .
Since initial transitions 0 q(,1) q0 input a,
predicate accept claimed behavior, clearly proposed mapping satisfies requirements Theorem 1. complete proof, therefore, must verify
predicate accept succeeds iff XYs accepted 0 state initial transition
T.
definition DFAs string XYs accepted 0 state initial
transition iff one following two conditions holds.
0(S; X ) = , Ys empty string final state 0, or;
0(S; X ) = , Ys nonempty string (and hence head X 1 tail
Ys1) Ys1 accepted 0 state , initial transition.
base fact accept(c,nil,qe,qf ) succeeds precisely first case holds, since
0 transition one final state. second case, conjunction
arc conditions fc (M ) clause succeeds exactly (S; X ) = (as noted above).
second conjunction clause succeeds Ys nonempty string
head X 1 tail Ys1 X1Ys1 accepted 0 state initial transition
state U , corresponds exactly second case above.
Thus concept membership preserved mapping. completes proof.


j



j

4. DNF-Hardness Results Recursive Programs

summarize previous results determinate clauses, shown single
k-ary closed recursive depth-d clause pac-learnable (Cohen, 1995), set n linear closed
recursive depth-d clauses not; further, even single n-ary closed recursive depth-d clauses
pac-learnable. still large gap positive negative results,
however: particular, learnability recursive programs containing constant number
k-ary recursive clauses yet established.
section investigate learnability classes programs.
show programs either two linear closed recursive clauses one linear closed recursive clause one base case hard learn boolean functions disjunctive
normal form (DNF). pac-learnability DNF long-standing open problem computational learning theory; import results, therefore, establishing
learnability classes require substantial advance computational learning
theory.
557

fiCohen

4.1 Linear Recursive Clause Plus Base Clause

Previous work established two-clause constant-depth determinate programs consisting one linear recursive clause one nonrecursive clause identified, given
two types oracles: standard equivalence-query oracle, \basecase oracle' (Cohen,
1995). (The basecase oracle determines example covered nonrecursive clause
alone.) section show absence basecase oracle, learning
problem hard learning boolean DNF.
discussion below, Dnf[n; r] denotes language r-term boolean functions
disjunctive normal form n variables.

Theorem 5 Let d-Depth-2-Clause set 2-clause programs consisting one

clause d-DepthLinRec one clause d-DepthNonRec. n
r exists database DB n;r 2 2-DB declaration Dec n;r 2 2-DEC , sizes
polynomial n r,
Dnf[n; r] 1-Depth-2-Clause[DB n;r ; Dec n;r ]

Hence 2 1 language family d-Depth-2-Clause[DB; a-DetDEC ]
uniformly polynomially predictable DNF polynomially predictable.

Proof: produce DB n;r 2 DB Dec n;r 2 2-DetDEC predicting
DNF reduced predicting 1-Depth-2-Clause[DB n;r ; Dec n;r ]. construction
makes use trick first used Theorem 3 (Cohen, 1993a), DNF formula
emulated conjunction containing single variable existentially quantified
restricted range.
begin instance mapping fi . assignment = b1 : : :bn converted
extended instance (f; D)
f p(1)
fbit (bi)gni=1
Next, define database DB n;r contain binary predicates true1 , false1, : : : , truer ,
falser behave follows:

truei(X,Y) succeeds X = 1, 2 f1; : : :; rg , fig.
falsei(X,Y) succeeds X = 0, 2 f1; : : :; rg , fig.
Further, DB n;r contains facts define predicate succ(Y,Z) true whenever
Z = + 1, Z numbers 1 r. Clearly size DB n;r
polynomial r.
Let Dec n;r = (p; 1; R) R contains modes bit (,), = 1; : : :; n; true j (+; +)
false j (+; +), j = 1; : : :; r, succ (+; ,).
let r-term DNF formula = _ri=1 ^sj =1 lij variables v1 ; : : :; vn.
may assume without loss generality contains exactly r terms, since DNF
formula fewer r terms padded exactly r terms adding terms


558

fiPac-Learning Recursive Logic Programs: Negative Results

Background database:

= 1; : : :; r
truei (b; ) b; : b = 1 2 f1; : : :; rg 6=
falsei (b; ) b; : b = 0 2 f1; : : :; rg 6=
succ(y,z)
z = + 1 2 f1; : : :; rg z 2 f1; : : :; rg

DNF formula: (v1 ^ v3 ^ v4) _ (v2 ^ v3) _ (v1 ^ v4)
Equivalent program:
p(Y) succ(Y,Z)^p(Z).
p(Y) bit1 (X1 ) ^ bit2 (X2 ) ^ bit3 (X3 ) ^ bit4 (X4 ) ^
true1 (X1,Y) ^ false1 (X3 ,Y) ^ true1(X4 ,Y) ^
false2 (X2,Y) ^ false2 (X3,Y)^
true3 (X1,Y) ^ false3 (X4 ,Y).
Instance mapping: fi(1011) = (p(1); fbit1(1); bit 2(0); bit3(1); bit4(1)g)
Figure 2: Reducing DNF recursive program
form v1 v1. define concept mapping fc () program CR; CB CR
linear recursive depth 1 determinate clause

p(Y ) succ(Y; Z ) ^ p(Z )
CB nonrecursive depth 1 determinate clause

n
^
^r ^
p(Y )
bit k (Xk ) ^
Bij


i=1 j =1

k =1

Bij defined follows:

Bij

(

truei (Xk ,Y) lij = vk
falsei (Xk ,Y) lij = vk

example construction shown Figure 2; suggest reader refer
figure point. basic idea behind construction first, clause
CB succeed variable bound i-th term succeeds (the
definitions truei falsei designed ensure property holds); second,
recursive clause CR constructed program fc () succeeds iff CB succeeds
bound one values 1; : : :; n.
argue rigorously correctness construction. Clearly, fi ( )
fc () size respectively. Since DB n;r also polynomial
size, reduction polynomial.
Figure 3 shows possible proofs constructed program fc ();
notice program fc () succeeds exactly clause CB succeeds value
559

fiCohen

p(1)

A@
A@
AA @
@
B(1)





succ(1,2) p(2)


@



A@


AA @
@
B(2)

succ(2,3) p(3)



@


A@


AA @
@
B(3)
:::

p(n-1)

B (i) V bit (X ) ^ V V B




ij



@


A@


AA @
B(n-1)
@

succ(n-1,n) p(n)
B(n)

Figure 3: Space proofs possible program fc ()
Vs l must true;

1

r
.
Now,



true


term

=
j =1 ij
V
V
s0

case j =1 Bij succeeds bound value j =1 Bi0 j every i0 6= also
succeeds bound i. hand, false assignment, Ti
fails, hence every possible binding generated repeated use recursive
clause CR base clause CB also fail. Thus concept membership preserved
mapping.
concludes proof.






4.2 Two Linear Recursive Clauses

Recall single linear closed recursive clause identifiable equivalence
queries (Cohen, 1995). construction similar used Theorem 5 used
show result cannot extended programs two linear recursive clauses.
Theorem 6 Let d-Depth-2-Clause0 set 2-clause programs consisting two
clauses d-DepthLinRec. (Thus assume base case recursion given
background knowledge.) constants n r exists database DB n;r 2
2-DB declaration Dec n;r 2 2-DEC , sizes polynomial n,
Dnf[n; r] 1-Depth-2-Clause0[DB n;r ; Dec n;r ]
Hence constants 2 1 language family
d-Depth-2-Clause0 [DB; a-DetDEC ]
560

fiPac-Learning Recursive Logic Programs: Negative Results

uniformly polynomially predictable DNF polynomially predictable.

Proof: before, proof makes use prediction-preserving reducibility DNF

d-Depth-2-Clause0[DB ; Dec ] specific DB Dec . Let us assume DNF
r terms, assume r = 2k . (Again, assumption made without
loss generality, since number terms increased padding vacuous
terms.) consider complete binary tree depth k + 1. k-th level tree
exactly r nodes; let us label nodes 1, : : : , r, give nodes arbitrary labels.

construct database DB n;r Theorem 5, except following changes:
predicates truei (b,y) falsei(b,y) also succeed label node
level k.
Rather predicate succ, database contains two predicates leftson
rightson encode relationship nodes binary tree.
database includes facts p(!1), : : : , p(!2r), !1, : : : , !2r leaves
binary tree. used base cases recursive program
learned.
Let label root binary tree. define instance mapping

fi (b1 : : :b1) (p(); fbit1 (b1); : : :; bit n (bn )g)
Note except use rather 1, identical instance mapping
used Theorem 5. Also let Dec n;r = (p; 1; R) R contains modes bit (,), =
1; : : :; n; true j (+; +) false j (+; +), j = 1; : : :; r; leftson (+; ,); rightson (+; ,).
concept mapping fc () pair clauses R1; R2, R1 clause

n
^
^r ^
p(Y )
bit k (Xk ) ^
Bij ^ leftson(Y; Z ) ^ p(Z )


k =1

R2 clause

p(Y )

n
^
k =1

bit k (Xk ) ^

i=1 j =1


^r ^


i=1 j =1

Bij ^ rightson (Y; Z ) ^ p(Z )

Note clause linear recursive, determinate, depth 1. Also,
construction clearly polynomial. remains show membership preserved.
Figure 4 shows space proofs
V constructed
V V program fc ();
Figure 3, B (i) abbreviates conjunction bit (Xi) ^ Bij . Notice program
succeed recursive calls manage finally recurse one base cases
p(!1), : : : , p(!2r ), correspond leaves binary tree. clauses
succeed first k , 1 levels tree. However, reach base cases
recursion leaves tree, recursion must pass k-th level tree;
is, one clauses must succeed node binary tree,
k-th level tree, hence label number 1 r.
program thus succeeds fi ( ) precisely number 1
561

fiCohen

p()

"
, b
@b
"

H

"H
,
@bb
"
" ,
@ b
" ,
@ bb
"
"
b
,
@
"
b
"
,
@
b
"
b
,
@
B() p(L)
B()
p(R)
Z
Z
`
` \
Z
X \\
Z

X
\

Z

Z

\ Z

\ Z


\ Z
\
\ Z


E
X
X
X
EX


E


E


E

:::

:::

:::

B
B
B




B
B

:::

:::

B(1) p(LL: : : L) B(1) p(LL: : : R)

p(!1 )

:::



J
J

J


J

J

B
B
B

B

B

B(n) p(RR: : : LR) B(n) p(RR: : : R)

p(!2 )

p(!2 ,1 )
r

p(!2 )
r

Figure 4: Proofs possible program fc ()

r conjunction B(i) succeeds, (by argument given Theorem 5)
happen satisfied assignment . Thus, mappings preserve

concept membership. completes proof.

Notice programs fc () used proof property depth
every proof logarithmic size instances. means hardness
result holds even one additionally restricts class programs logarithmic
depth bound.

4.3 Upper Bounds Diculty Learning

previous sections showed several highly restricted classes recursive programs
least hard predict DNF. section show restricted
classes also harder predict DNF.
wish restrict depth proof constructed target program. Thus, let
h(n) function; use Langh(n) set programs class Lang
proofs extended instance (f; D) depth bounded h(j Dj ).
562

fiPac-Learning Recursive Logic Programs: Negative Results

Theorem 7 Let Dnf[n; ] language DNF boolean functions (with number

terms), recall d-Depth-2-Clause language 2-clause programs consisting one clause d-DepthLinRec one clause d-DepthNonRec,
d-Depth-2-Clause0 language 2-clause programs consisting two clauses
d-DepthLinRec.
constants a, databases DB 2 DB declarations Dec 2 a-DetDEC ,
polynomial function poly (n)

d-Depth-2-Clause[DB ; Dec] Dnf[poly (j DB j ); ]
d-Depth-2-Clause0h(n) [DB ; Dec] Dnf[poly (j DB j ); ] h(n) bounded c log n
constant c.
Hence either language families uniformly polynomially predictable, Dnf[n; ]
polynomially predictable.

Proof: proof relies several facts established companion paper (Cohen, 1995).
every declaration Dec, clause BOTTOM d(Dec) every nonrecursive depth-d determinate clause C equivalent subclause BOTTOM .
Further, size BOTTOM polynomial Dec . means language subclauses BOTTOM normal form nonrecursive constant-depth
determinate clauses.

Every linear closed recursive clause CR constant-depth determinate equivalent subclause BOTTOM plus recursive literal Lr ; further,
polynomial number possible recursive literals Lr .
constants a, a0, d, database DB 2 a-DB, declaration Dec =
(p; a0; R), database DB 2 a-DB , program P d-Depth-2-Clause[DB ; Dec ],
depth terminating proof constructing using P hmax,
hmax polynomial size DB Dec .
assumed without loss generality database DB decsriptions
contain equality predicate , equality predicate simply predicate
equal(X,Y) true exactly X = .
idea proof contruct prediction-preserving reduction two
classes recursive programs listed DNF. begin two lemmas.

Lemma 8 Let Dec 2 a-DetDEC , let C nonrecursive depth-d determinate clause

consistent Dec. Let SubclauseC denote language subclauses C , let
Monomial[u] denote language monomials u variables. polynomial poly 1 database DB 2 DB,
SubclauseC [DB ; Dec] Monomial[poly 1(j DB j )]

563

fiCohen

Proof lemma: Follows immediately construction used Theorem 1

Dzeroski, Muggleton, Russell (Dzeroski et al., 1992). (The basic idea construction introduce propositional variable representing \success" connected
chain literals C . subclause C represented conjunction
propositions.)
lemma extended follows.

Lemma 9 Let Dec 2 a-DetDEC , let = fC1; : : :; Crg set r nonrecursive depth-

determinate clauses consistent Dec, length n less. Let SubclauseS denote
set programs form P = (D1; : : :; Ds) Di subclause
Cj 2 .
polynomial poly 2 database DB 2 DB,
SubclauseS [DB ; Dec] Dnf[poly 2 (j DB j ; r); ]

Proof lemma: Lemma 8, Ci 2 , set variables Vi size
polynomial j DB j
every clause SubclauseC emulated monomial
Sr

V
V
.
Clearly,
jV j polynomial n r, every clause
. Let V =

i=1





SubclauseC also emulated monomial V . Further, every disjunction

r clauses represented disjunction monomials.
Since Ci 's satisfy single declaration Dec = (p; a; R), heads
principle function arity; further, may assume (without loss generality, since
equality predicate assumed) variables appearing heads clauses
distinct. Since Ci's also nonrecursive, every program
P 2 SubclauseS

represented disjunction D1 _ : : : _ Dr i, Di 2 ( SubclauseC ). Hence
every P 2 SubclauseS represented r-term DNF set variables V .




Let us introduce additional notation. C clauses, use
C u denote result resolving C together, C denote result
resolving C times. Note C u unique C linear recursive C
predicate heads (since one pair complementary
literals.)
Now, consider target program

P = (CR; CB ) 2 d-Depth-2-Clause[DB ; Dec]
CR recursive clause CB base. proof extended instance
(f; D) must use clause CR repeatedly h times use clause CB resolve away
final subgoal. Hence nonrecursive clause CRh u CB could also used cover
instance (f; D).
Since depth proof class programs bounded number hmax
polynomial j DB j ne , nonrecursive program

P 0 = fCRh u CB : 0 h hmax g
564

fiPac-Learning Recursive Logic Programs: Negative Results

equivalent P extended instances size ne less.
Finally, recall assume CB subclause BOTTOM ; also,
polynomial-sized set LR = Lr1 ; : : :; Lr closed recursive literals
Lr 2 LR , clause CR subclause BOTTOM [ Lr . means let
polynomial-sized set
S1 = f(BOTTOM [ Lr )h u BOTTOM j 0 h hmax Lr 2 LR g
P 0 2 SubclauseS1 . Thus Lemma 9, d-Depth-2-Clause Dnf. concludes
proof first statement theorem.
show
d-Depth-2-Clause0h(n) [DB ; Dec] Dnf[poly (j DB j ; ]
similar argument applies. Let us introduce notation, define
MESHh;n (CR1 ; CR2 ) set clauses form
p









CR 1 u CR 2 u : : : u CR 0
j , CR = CR1 CR = CR2 , h0 h(n). Notice functions
h(n) c log n number clauses polynomial n.
let p predicate appearing heads CR1 CR2 , let C^ (respectively
^ ) version C (DB ) every instance predicate p replaced
DB
new predicate p^. P recursive program P = fCR1 ; CR2 g d-Depth-2-Clause0
^ ,
database DB , P ^ DB equivalent4 nonrecursive program P 0 ^ DB
i;

ij



i;

i;h

ij

P 0 = fC^ j C 2 MESHh;n (CR1 ; CR2 )g
e

recall polynomial number recursive literals Lr , hence
polynomial number pairs recursive literals Lr ; Lr . means set clauses
[
S2 =
fC^ j C 2 MESHh;n (BOTTOM [ Lr ; BOTTOM [ Lr )g




(L

ri

e

2

;Lrj ) LR LR

j



j

also polynomial-sized; furthermore, program P language d-Depth-2-Clause,
P 0 2 SubclauseS2 . second part theorem follows application Lemma 9.
immediate corollary result Theorems 6 5 strengthened
follows.
Corollary 10 constants 1 2, language family
d-Depth-2-Clause[DB; a-DetDEC ]
uniformly polynomially predictable DNF polynomially predictable.
constants 1 2, language family
d-Depth-2-Clause0 [DB; a-DetDEC ]
uniformly polynomially predictable DNF polynomially predictable.
4. extended instances size n less.
e

565

fiCohen

Thus important sense learning problems equivalent learning boolean
DNF. resolve questions learnability languages,
show learnability dicult formal problem: predictability boolean DNF
long-standing open problem computational learning theory.

5. Related Work
work described paper differs previous formal work learning logic programs simultaneously allowing background knowledge, function-free programs, recursion. also focused exclusively computational limitations ecient learnability
associated recursion, considered languages known paclearnable nonrecursive case. Since results paper negative,
concentrated model polynomial predictability; negative results model immediately imply negative result stronger model pac-learnability, also imply
negative results strictly expressive languages.
Among closely related prior results negative results previously
obtained certain classes nonrecursive function-free logic programs (Cohen, 1993b).
results similar character results described here, apply nonrecursive
languages. Similar cryptographic results obtained Frazier Page (1993)
certain classes programs (both recursive nonrecursive) contain function symbols
disallow background knowledge.
prior negative results also obtained learnability firstorder languages using proof technique consistency hardness (Pitt & Valiant, 1988).
Haussler (1989) showed language \existential conjunction concepts" paclearnable showing hard find concept language consistent
given set examples. Similar results also obtained two restricted languages
Horn clauses (Kietz, 1993); simple description logic (Cohen & Hirsh, 1994);
language sorted first-order terms (Page & Frisch, 1992). results, however,
specific model pac-learnability, none easily extended polynomial
predictability model considered here. results also extend languages
expressive specific constrained languages. Finally, none languages allow
recursion.
knowledge, negative learnability results first-order languages. discussion prior positive learnability results first-order languages
found companion paper (Cohen, 1995).

6. Summary
paper companion (Cohen, 1995) considered large number different
subsets Datalog. aim comprehensive, systematic: particular, wished find precisely boundaries learnability lie various syntactic
restrictions imposed relaxed. Since easy reader \miss forest
trees", brie summarize results contained paper, together
positive results companion paper (Cohen, 1995).
566

fiPac-Learning Recursive Logic Programs: Negative Results

Local
Clauses

Constant-Depth Determinate
Clauses

nCR,

nCR,

nCR jCB,

nCR ; CB,

k nCR,

n nCR,

kCR,

kCR+

kCRjCB+

kCR; CBDNF

k k0CRDNF

n kCR,

1CR,

1CR+

1CRjCB+

1CR; CB=DNF

2 1CR=DNF

n 1CR,

Table 1: summary learnability results
Throughout papers, assumed polynomial amount background
knowledge exists; programs learned contain function symbols;
literals body clause small arity. also assumed recursion
closed , meaning output variables appear recursive clause; however, believe
restriction relaxed without fundamentally changing results paper.
companion paper (Cohen, 1995) showed single nonrecursive constantdepth determinate clause learnable strong model identification equivalence
queries . learning model, one given access oracle counterexamples|that
is, oracle find, unit time, example current hypothesis
incorrect|and must reconstruct target program exactly polynomial number
counterexamples. result implies single nonrecursive constant-depth determinate clause pac-learnable (as counterexample oracle emulated drawing
random examples pac setting). result novel (Dzeroski et al., 1992); however
proof given independent, also independent interest. Notably, somewhat
rigorous earlier proofs, also proves result directly, rather via reduction propositional learning problem. proof also introduces simple version
forced simulation technique, variants used positive results.
showed learning algorithm nonrecursive clauses extended
case single linear recursive constant-depth determinate clause, leading
result restricted class recursive programs also identifiable equivalence
queries. bit effort, algorithm extended learn single
k-ary recursive constant-depth determinate clause.
also considered extended learning algorithm learn recursive programs consisting one constant-depth determinate clauses. interesting extension
simultaneously learn recursive clause CR base clause CB , using equivalence
queries also \basecase oracle" indicates counterexamples covered
base clause CB . model, possible simultaneously learn recursive clause
nonrecursive base case situations recursive clause learned
567

fiCohen

Language Family
d-DepthNonRec[a-DB; a-DetDEC]
d-DepthLinRec[a-DB; a-DetDEC]
d-Depth-k-Rec[a-DB; a-DetDEC]
d-Depth-2-Clause[a-DB; a-DetDEC]
kd-MaxRecLang[a-DB; a-DetDEC ]
d-Depth-2-Clause[a-DB; a-DetDEC]
d-Depth-2-Clause [a-DB; a-DetDEC ]
d-DepthLinRecProg[a-DB; a-DetDEC ]
d-DepthRec[a-DB; a-DetDEC ]
k-LocalLinRec[a-DB; a-DEC ]
0

B
1
0
0
1
1
1
0
0
0
0

R
0
1
1
1
1
1
2

L/R Oracles
, EQ
1
EQ
k
EQ
1
EQ,BASE
k
EQ,BASE
1
EQ
1
EQ
n 1
EQ
1 n
EQ
1 1
EQ

Notation Learnable
CB
yes
1CR
yes
kCR
yes
1CRjCB yes
kCRjCB
yes
1CR; CB =DNF
2 1CR =DNF
n 1CR
nCR

1CR


Table 2: Summary language learnability results. Column B indicates number
base (nonrecursive) clauses allowed program; column R indicates number recursive clauses; L/R indicates number recursive literals allowed
single recursive clause; EQ indicates oracle equivalence queries BASE
indicates basecase oracle. languages except k-LocalLinRec, clauses
must determinate depth d.
alone; instance, one learn k-ary recursive clause together nonrecursive
base case. strongest positive result.
results summarized Tables 1 2. Table 1, program one rary recursive clause denoted rCR, program one r-ary recursive clause one
nonrecursive basecase denoted rCR; CB , rCRjCB \basecase" oracle,
program different r-ary recursive clauses denoted rCR . boxed results
associated one theorems paper, companion paper,
unmarked results corollaries results. \+" program class indicates
identifiable equivalence queries; thus positive results described
summarized four \+" entries lower left-hand corner section table
concerned constant-depth determinate clauses.
Table 2 presents information slightly different format, also relates
notation Table 1 terminology used elsewhere paper.
paper considered learnability various natural generalizations
languages shown learnable companion paper. Consider moment single
clauses. companion paper showed fixed k single k-ary recursive constantdepth determinate clause learnable. showed restrictions
necessary. particular, program n constant-depth linear recursive clauses
polynomially predictable; hence restriction single clause necessary. Also, single
clause n recursive calls hard learn; hence restriction k-ary recursion
necessary. also showed restriction constant-depth determinate clauses
necessary, considering learnability constant locality clauses . Constant locality
clauses known generalization constant-depth determinate clauses
pac-learnable nonrecursive case. However, showed recursion allowed,
568

fiPac-Learning Recursive Logic Programs: Negative Results

language learnable: even single linear recursive clause polynomially
predictable.
Again, results summarized Table 1; \," program class means
polynomially predictable, cryptographic assumptions, hence neither
pac-learnable identifiable equivalence queries.
negative results based cryptographic hardness give upper bound expressiveness learnable recursive languages, still leave open learnability programs
constant number k-ary recursive clauses absence basecase oracle.
final section paper, showed following problems are, model
polynomial predictability, equivalent predicting boolean DNF:
predicting two-clause constant-depth determinate recursive programs containing one
linear recursive clause one base case;
predicting two-clause recursive constant-depth determinate programs containing two
linear recursive clauses, even base case known.
note program classes nearly simplest classes multi-clause
recursive programs one imagine, pac-learnability DNF longstanding open problem computational learning theory. results suggest, therefore,
pac-learning multi-clause recursive logic programs dicult; least,
show finding provably correct pac-learning algorithm require substantial advances
computational learning theory. Table 1, \= Dnf" (respectively Dnf) means
corresponding language prediction-equivalent DNF (respectively least hard
DNF).
summarize Table 1: sort recursion, programs containing
constant-depth determinate clauses learnable. constant-depth determinate
recursive programs learnable contain single k-ary recursive clause
(in standard equivalence query model) single k-ary recursive clause plus base
case (if \basecase oracle" allowed). classes recursive programs either
cryptographically hard, hard boolean DNF.

7. Conclusions

Inductive logic programming active area research, one broad class learning
problems considered area class \automatic logic programming" problems.
Prototypical examples genre problems learning append two lists,
multiply two numbers. target concepts automatic logic programming recursive
programs, often, training data learning system simply examples
target concept, together suitable background knowledge.
topic paper pac-learnability recursive logic programs random
examples background knowledge; specifically, wished establish computational
limitations inherit performing task. began positive results established
companion paper. results show one constant-depth determinate closed k-ary
recursive clause pac-learnable, further, program consisting one recursive
clause one constant-depth determinate nonrecursive clause also pac-learnable given
additional \basecase oracle".
569

fiCohen

paper showed positive results likely improved.
particular, showed either eliminating basecase oracle learning two recursive clauses simultaneously prediction-equivalent learning DNF, even case
linear recursion. also showed following problems hard breaking (presumably) secure cryptographic codes: pac-learning n linear recursive determinate clauses,
pac-learning one n-ary recursive determinate clause, pac-learning one linear recursive
k-local clause.
results contribute machine learning several ways. point view
computational learning theory, several results technically interesting. One
prediction-equivalence several classes restricted logic programs boolean DNF;
result, together others like (Cohen, 1993b), reinforces importance learnability problem DNF. paper also gives dramatic example adding recursion
widely differing effects learnability: constant-depth determinate clauses
remain pac-learnable linear recursion added, constant-locality clauses become cryptographically hard.
negative results show systems apparently learn larger class recursive
programs must taking advantage either special properties target concepts
learn, distribution examples provided with. believe
likely opportunity obtaining positive formal results area
identify analyze special properties. example, many examples
FOIL learned recursive logic programs, made use \complete example sets"|
datasets containing examples certain size, rather sets randomly
selected examples (Quinlan & Cameron-Jones, 1993). possible complete datasets
allow expressive class programs learned random datasets; fact,
progress recently made toward formalizing conjecture (De Raedt & Dzeroski,
1994).
Finally, importantly, paper established boundaries learnability
determinate recursive programs pac-learnability model. many plausible automatic programming contexts would highly desirable system offered
formal guarantees correctness. results paper provide upper bounds
one hope achieve ecient, formally justified system learns recursive
programs random examples alone.

Acknowledgements
author wishes thank three anonymous JAIR reviewers number useful suggestions presentation technical content.

References
Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication small
training sets. Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture
Notes Computer Science # 784.
570

fiPac-Learning Recursive Logic Programs: Negative Results

Biermann, A. (1978). inference regular lisp programs examples. IEEE Transactions Systems, Man Cybernetics, 8 (8).
Chandra, A. K., Kozen, D. C., & Stockmeyer, L. J. (1981). Alternation. Journal
ACM, 28, 114{113.
Cohen, W. W. (1993a). Cryptographic limitations learning one-clause logic programs.
Proceedings Tenth National Conference Artificial Intelligence Washington,
D.C.
Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. appear Artificial
Intelligence.
Cohen, W. W. (1993c). Rapid prototyping ILP systems using explicit bias. Proceedings
1993 IJCAI Workshop Inductive Logic Programming Chambery, France.
Cohen, W. W. (1994a). Pac-learning nondeterminate clauses. Proceedings Eleventh
National Conference Artificial Intelligence Seattle, WA.
Cohen, W. W. (1994b). Recovering software specifications inductive logic programming. Proceedings Eleventh National Conference Artificial Intelligence
Seattle, WA.
Cohen, W. W. (1995). Pac-learning recursive logic programs: ecient algorithms. Journal
AI Research, 2, 501{539.
Cohen, W. W., & Hirsh, H. (1994). learnability description logics equality
constraints. Machine Learning, 17 (2/3).
De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories PAC-learnable.
Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive
Logic Programming Bad Honnef/Bonn, Germany.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability determinate logic
programs. Proceedings 1992 Workshop Computational Learning Theory
Pittsburgh, Pennsylvania.
Frazier, M., & Page, C. D. (1993). Learnability recursive, non-determinate theories:
basic results techniques. Proceedings Third International Workshop
Inductive Logic Programming Bled, Slovenia.
Haussler, D. (1989). Learning conjunctive concepts structural domains. Machine Learning, 4 (1).
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction Automata Theory, Languages,
Computation. Addison-Wesley.
Kearns, M., & Valiant, L. (1989). Cryptographic limitations learning Boolean formulae
finite automata. 21th Annual Symposium Theory Computing. ACM
Press.
571

fiCohen

Kietz, J.-U. (1993). computational lower bounds computational complexity
inductive logic programming. Proceedings 1993 European Conference
Machine Learning Vienna, Austria.
King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug design
machine learning: use inductive logic programming model structureactivity relationships trimethoprim analogues binding dihydrofolate reductase.
Proceedings National Academy Science, 89.
Lavrac, N., & Dzeroski, S. (1992). Background knowledge declarative bias inductive
concept learning. Jantke, K. P. (Ed.), Analogical Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. Lectures
Artificial Intelligence Series #642.
Lloyd, J. W. (1987). Foundations Logic Programming: Second Edition. Springer-Verlag.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.
Journal Logic Programming, 19/20 (7), 629{679.
Muggleton, S., & Feng, C. (1992). Ecient induction logic programs. Inductive Logic
Programming. Academic Press.
Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structure
prediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.
Page, C. D., & Frisch, A. M. (1992). Generalization learnability: study constrained
atoms. Inductive Logic Programming. Academic Press.
Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. Machine
Learning, 9 (1).
Pitt, L., & Warmuth, M. K. (1988). Reductions among prediction problems: difficulty predicting automata. Proceedings 3rd Annual IEEE Conference
Structure Complexity Theory Washington, D.C. Computer Society Press
IEEE.
Pitt, L., & Valiant, L. (1988). Computational limitations learning examples. Journal
ACM, 35 (4), 965{984.
Pitt, L., & Warmuth, M. (1990). Prediction-preserving reducibility. Journal Computer
System Sciences, 41, 430{467.
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Brazdil, P. B.
(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notes
Computer Science # 667.
Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5 (3).
572

fiPac-Learning Recursive Logic Programs: Negative Results

Quinlan, J. R. (1991). Determinate literals inductive logic programming. Proceedings
Eighth International Workshop Machine Learning Ithaca, New York. Morgan
Kaufmann.
Rouveirol, C. (1994). Flattening saturation: two representation changes generalization. Machine Learning, 14 (2).
Summers, P. D. (1977). methodology LISP program construction examples.
Journal Association Computing Machinery, 24 (1), 161{175.
Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11).
Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers treebanks:
machine learning approach. Proceedings Twelfth National Conference
Artificial Intelligence Seattle, Washington. MIT Press.

573

fiJournal Artificial Intelligence Research 2 (1994) 131-158

Submitted 4/94; published 12/94

Wrap-Up: Trainable Discourse
Module Information Extraction
Stephen Soderland

Wendy Lehnert
Department Computer Science, University Massachusetts
Amherst, 01003-4610

soderlan@cs.umass.edu
lehnert@cs.umass.edu

Abstract

vast amounts on-line text available led renewed interest information
extraction (IE) systems analyze unrestricted text, producing structured representation selected information text. paper presents novel approach
uses machine learning acquire knowledge higher level IE processing.
Wrap-Up trainable IE discourse component makes intersentential inferences
identifies logical relations among information extracted text. Previous corpusbased approaches limited lower level processing part-of-speech tagging,
lexical disambiguation, dictionary construction. Wrap-Up fully trainable,
automatically decides classifiers needed, even derives feature set
classifier automatically. Performance equals partially trainable discourse
module requiring manual customization domain.

1. Introduction

information extraction (IE) system analyzes unrestricted, real world text newswire
stories. contrast information retrieval systems return pointer entire
document, IE system returns structured representation information
within text relevant user's needs, ignoring irrelevant information.
first stage IE system, sentence analysis, identifies references relevant objects
typically creates case frame represent object. second stage, discourse
analysis, merges together multiple references object, identifies logical relationships objects, infers information explicitly identified sentence analysis.
IE system operates terms domain specifications predefine types information relationships considered relevant application. Considerable domain
knowledge used IE system: domain objects, relationships objects,
texts typically describe objects relationships.
Much domain knowledge automatically acquired corpus-based techniques. Previous work centered knowledge acquisition lower level
processing part-of-speech tagging lexical disambiguation. N-gram statistics
highly successful part-of-speech tagging (Church, 1988; DeRose, 1988). Weischedel
(1993) used corpus-based probabilities part-of-speech tagging guide
parsing. Collocation data used lexical disambiguation Hindle (1989), Brent
(1993), others. Examples training corpus driven part-of-speech
semantic tagging (Cardie, 1993) dictionary construction (Riloff, 1993).

c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiSoderland Lehnert

paper describes Wrap-Up (Soderland & Lehnert, 1994), first system automatically acquire domain knowledge higher level processing associated discourse
analysis. Wrap-Up uses supervised learning induce set classifiers training corpus representative texts, text accompanied hand-coded target output.
implemented Wrap-Up ID3 decision tree algorithm (Quinlan, 1986), although
machine learning algorithms could selected.
Wrap-Up fully trainable system unique decides
classifiers needed domain, automatically derives feature set
classifier. user supplies definition objects relationships interest
domain training corpus hand-coded target output. Wrap-Up rest
hand coding needed tailor system new domain.
Section 2 discusses IE task detail, introduces microelectronics domain,
gives overview CIRCUS sentence analyzer. Section 3 describes Wrap-Up,
giving details ID3 trees constructed discourse decision, features
automatically derived tree, requirements applying Wrap-Up new
domain. Section 4 shows performance Wrap-Up two domains compares
performance partially trainable discourse component. Section 5 draw
conclusions contribution research. detailed example
microelectronics domain given appendix.

2. Information Extraction Task

section gives overview information extraction illustrates IE processing
sample text fragment microelectronics domain. discuss need
trainable IE components acquire knowledge new domain.

2.1 Overview IE

information extraction system operates two levels. First, sentence analysis identifies
information relevant IE application. discourse analysis,
focus paper, takes output sentence analysis assembles
coherent representation entire text. done according predefined guidelines specify objects text relevant relationships
objects reported.
Sentence analysis broken several stages, applying different
types domain knowledge. lowest level preprocessing, segments text
words sentences. word assigned part-of-speech tag possibly semantic
tag preparation processing. Different IE systems varying amounts
syntactic parsing point. research sites participated ARPA-sponsored
Message Understanding Conferences (MUC-3, 1991; MUC-4, 1992; MUC-5, 1993) found
robust, shallow analysis pattern matching performed better elaborate,
brittle, parsing techniques.
CIRCUS sentence analyzer (Lehnert, 1990; Lehnert et al., 1992) shallow syntactic analysis identify simple syntactic constituents, distinguish active passive
voice verbs. shallow syntactic analysis sucient extraction task, uses
132

fiWrap-Up: Trainable Discourse Module

local linguistic patterns instantiate case frames, called concept nodes (CN's) used
CIRCUS.
CN definition trigger word syntactic pattern relative word.
Whenever trigger word occurs text, CIRCUS looks one syntactic buffers
appropriate information extract. CN definitions extract information
subject direct object, first testing active passive voice. CN
definitions look prepositional phrase particular preposition. Examples CN
extraction patterns particular domain shown Section 2.3.
Discourse analysis starts output sentence analyzer, case set
concept nodes representing locally extracted information. work discourse
often involved tracking shifts topic speaker/writer's goals (Grosz & Sidner,
1986; Liddy et al., 1993) resolving anaphoric references (Hobbs, 1978). Discourse
processing IE system may concern issues,
means main objective transforming bits pieces extracted information
coherent representation.
One first tasks discourse analysis merge together multiple references
object. domain company names important, involve recognizing equivalence full company name (\International Business Machines, Inc.")
shortened forms name (\IBM") generic references (\the company", \the
U.S. computer maker"). manually engineered rules seem unavoidable coreference
merging. Another example merging domain object less specific reference
object. microelectronics domain reference \DRAM" chips may merged
reference \memory" \I-line" process merged \lithography."
Much work discourse analysis identify logical relationships extracted objects, represented pointers objects output. Discourse analysis
must also able infer missing objects explicitly stated text
cases split object multiple copies discard object erroneously
extracted.
current implementation Wrap-Up begins discourse processing coreference
merging done separate module. primarily manual engineering
seems unavoidable coreference. Work underway extend Wrap-Up include IE
discourse processing incorporating limited amount domain-specific code handle
things company name aliases generic references domain objects.
Wrap-Up divides processing six stages, described fully
Section 3. are:
1. Filtering spuriously extracted information
2. Merging objects attributes
3. Linking logically related objects
4. Deciding split objects multiple copies
5. Inferring missing objects
6. Adding default slot values
point example specific domain might help. following sections introduce microelectronics domain, illustrate sentence analysis discourse analysis
short example domain.
133

fiSoderland Lehnert

2.2 Microelectronics Domain

microelectronics domain one two domains targetted Fifth Message
Understanding Conference (MUC-5, 1993). According domain task guidelines
developed MUC-5 microelectronics corpus, information extracted microchip fabrication processes along companies, equipment, devices associated
processes. seven types domain objects identified: entities (i.e.
companies), equipment, devices, four chip fabrication processes (layering, lithography,
etching, packaging).
Identifying relationships objects equal importance domain identifying objects themselves. company must identified playing least one four
possible roles respect microchip fabrication process: developer, manufacturer,
distributor, purchaser/user. Microchip fabrication processes reported
associated specific company least one roles. equipment object
must linked process uses equipment, device object linked
process fabricates device. Equipment objects may point company
manufacturer equipment modules.
following sample MUC-5 microelectronics domain two companies
first sentence, associated two lithography processes second sentence.
GCA Sematech developers UV I-line lithography processes,
GCA playing additional role manufacturer. lithography process linked
stepper equipment mentioned sentence one.
GCA unveiled new XLS stepper, developed
assistance Sematech. system available
deep-ultraviolet I-line configurations.

Figure 1 shows five domain objects extracted sentence analysis final
representation text discourse analysis identified relationships objects. relationships directly indicated pointers objects.
roles companies play respect microchip fabrication process indicated
creating \microelectronics-capability" object pointers process
companies.

2.3 Extraction Patterns

sentence analysis identify GCA Sematech company names, extract
domain objects stepper equipment, UV lithography I-line lithography?
CN dictionary domain includes extraction pattern \X unveiled" identify
company names. subject active verb \unveiled" domain nearly always
company developing distributing new device process. However, pattern
occasionally pick company fails domain's reportability criteria. company
unveils new type chip discarded text specify fabrication
process.
Extracting company name \Sematech" dicult since pattern \assistance
X" reliable predictor relevant company names. always trade-off
accuracy complete coverage deciding extraction patterns reliable
134

fiWrap-Up: Trainable Discourse Module

A. Five concept nodes extracted sentence analysis.
Entity
Type: company
Name: GCA

Equipment
Type: stepper
Name: XLS

Lithography
Type: UV

Lithography
Type: I-line

Entity
Type: company
Name: Sematech

B. Final representation text discourse analysis.
Template
Contents:
ME-Capability
Manufacturer:
Developer:
Process:

ME-Capability
Manufacturer:
Developer:
Process:

Entity
Type: company
Name: GCA

Entity
Type: company
Name: Sematech
Lithography
Type: UV
Equipment:

Lithography
Type: I-line
Equipment:

Equipment
Type: stepper
Name: XLS
Manufacturer:
Status: in-development

Figure 1: Output (A) sentence analysis (B) discourse analysis
enough include CN dictionary. Including less reliable patterns increases coverage
expense spurious extraction. specific pattern \developed
assistance X" reliable, missed dictionary construction tool
(Riloff, 1993).
many domain objects, equipment, devices, microchip fabrication
processes, set possible objects predefined list keywords refer
objects created. extraction pattern \unveiled X" looks direct object
active verb \unveiled", instantiating equipment object keyword indicating
equipment type found. example equipment object type \stepper"
created equipment name \XLS". stepper equipment also extracted
135

fiSoderland Lehnert

pattern \X developed", looks equipment subject passive verb
\developed". equipment object extracted third time keyword \stepper"
itself, sucient instantiate stepper equipment object whether occurs
reliable extraction pattern.
keyword \deep-ultraviolet" extraction pattern \available X" used
extract lithography object type \UV" second sentence. Another lithography
object type \I-line" similarly extracted. Case frames created objects
identified sentence analysis. set objects becomes input next stage
processing, discourse analysis.

2.4 Discourse Processing

full text fragment comes, likely references
\GCA" \GCA Corp." One first jobs discourse analysis merge
multiple references. much harder task merge pronominal references generic
references \the company" appropriate company name. part
coreference problem handled processes separate Wrap-Up.
main job discourse analysis determine relationships objects
passed sentence analysis. Considerable domain knowledge needed make
discourse-level decisions. knowledge concerns writing style, specific phrases
writers typically use imply relationships referents given domain.
phrase \<company> unveiled <equipment>" sucient evidence infer company
developer microelectronics process? word \unveiled" alone enough,
since company unveiled new DRAM chip may developer new
process. may simply using someone else's microelectronics process produce chip.
inferences, particularly role company plays process, often
subtle two human analysts may disagree output given text. human
performance study task found experienced analysts agreed
80% text interpretations domain (Will, 1993).
World knowledge also needed relationships possible domain objects. lithography process may linked stepper equipment, steppers never
used layering, etching, packaging processes. delicate dependencies
types process likely fabricate types devices. Knowledge
kinds relationships typically reported domain also help guide discourse processing. Stories lithography, example, often give developer, manufacturer,
distributor process, roles hardly ever mentioned packaging processes. Companies associated packaging tend limited purchaser/user
packaging technology.
wide range domain knowledge needed discourse processing, related
world knowledge, writing style. next section discusses need trainable components levels IE processing, including discourse analysis. Wrap-Up uses
machine learning techniques avoid months manual knowledge engineering otherwise
required develop specific IE application.

136

fiWrap-Up: Trainable Discourse Module

2.5 Need Trainable IE Components

highest performance ARPA-sponsored Fifth Message Understanding Conference
(MUC-5, 1993) achieved cost nearly two years intense programming effort,
adding domain-specific heuristics domain-specific linguistic patterns one one, followed various forms system tuning maximize performance. many real world
applications, two years development time team half dozen programmers would
prohibitively expensive. make matters worse, knowledge used one domain
cannot readily transferred IE applications.
Researchers University Massachusetts worked facilitate IE system development use corpus-driven knowledge acquisition techniques (Lehnert et
al., 1993). 1991 purely hand-crafted UMass system highest performance
site MUC-3 evaluation. following year UMass ran hand-crafted system alternate system replaced key component output AutoSlog,
trainable dictionary construction tool (Riloff, 1993). AutoSlog variant exhibited performance levels comparable dictionary based 1500 hours manual coding. Encouraged
success one trainable component, architecture corpus-driven system
development proposed uses machine learning techniques address number
natural language processing problems (Lehnert et al., 1993). MUC-5 evaluation,
output CIRCUS sentence analyzer sent TTG (Trainable Template Generator), discourse component developed Hughes Research Laboratories (Dolan, et al.,
1991; Lehnert et al., 1993). TTG used machine learning techniques acquire much
needed domain knowledge, still required hand-coded heuristics turn acquired
knowledge fully functioning discourse analyzer.
remainder paper focus Wrap-Up, new IE discourse module
development explores possibility fully automated knowledge acquisition
discourse analysis. detailed following sections, Wrap-Up builds ID3 decision
trees guide discourse processing requires hand-coded customization new
domain training corpus provided. Wrap-Up automatically decides
ID3 trees needed domain derives feature set tree output
sentence analyzer.

3. Wrap-Up, Trainable IE Component

section describes Wrap-Up algorithm, decision trees used discourse
analysis, trees tree features automatically generated. conclude
discussion requirements Wrap-Up experience porting new
domain.

3.1 Overview

Wrap-Up domain-independent framework IE discourse processing instantiated automatically acquired knowledge new IE application. training
phase, Wrap-Up builds ID3 decision trees based representative set training texts,
paired hand-coded output keys. ID3 trees guide Wrap-Up's processing
run time.
137

fiSoderland Lehnert

run time Wrap-Up receives input objects extracted text sentence analysis. objects represented case frame along list
references text, location reference, linguistic patterns used
extract it. Multiple references object throughout text merged together
passing Wrap-Up. Wrap-Up transforms set objects discarding
spurious objects, merging objects add attributes object, adding pointers
objects, inferring presence missing objects slot values.
Wrap-Up six stages processing, set decision trees designed
transform objects passed one stage next.
Stages Wrap-up Algorithm:
1. Slot Filtering
object slot decision tree judges whether slot contains reliable
information. Discard slot value object tree returns \negative".
2. Slot Merging
Create instance pair objects type. Merge two objects
decision tree object type returns \positive". stage merge
object separately extracted attributes object.
3. Link Creation
Consider possible pairs objects might possibly linked. Add pointer
objects Link Creation decision tree returns \positive".
4. Object Splitting
Suppose object linked object B object C. Object Splitting
decision tree returns \positive", split two copies one pointing B
C.
5. Inferring Missing Objects
object object pointing it, instance created decision
tree returns likely parent object. Create parent link
\orphan" object unless tree returns \none". use decision trees
Link Creation Object Splitting stages tie new parent objects.
6. Inferring Missing Slot Values
object slot closed class possible values empty, create instance
decision tree returns context-sensitive default value slot, possibly
\none".

3.2 Decision Trees Discourse Analysis

key making machine learning work complex task discourse processing
break problem number small decisions build separate classifier
138

fiWrap-Up: Trainable Discourse Module

each. six stages Wrap-Up described Section 3.1 set
ID3 trees, exact number trees depending domain specifications.
Slot Filtering stage separate tree slot object domain; Slot
Merging stage separate tree object type; Link Creation stage tree
pointer defined output structure; forth stages.
MUC-5 microelectronics domain (as explained Section 2.2) required 91 decision trees: 20
Slot Filtering stage, 7 Slot Merging, 31 Link Creation, 13 Object Splitting,
7 Inferring Missing Objects , 13 Inferring Missing Slot Values.
example Link Creation stage tree determines pointers
lithography objects equipment objects. Every pair lithography equipment objects
found text encoded instance sent Lithography-Equipment-Link tree.
classifier returns \positive", Wrap-Up adds pointer two objects
output indicate equipment used lithography process.
ID3 decision tree algorithm (Quinlan, 1986) used experiments, although
machine learning classifier could plugged Wrap-Up architecture. vector
space approach might seem appropriate, performance would depend weights
assigned feature (Salton et al., 1975). hard see principled way assign
weights heterogeneous features used Wrap-Up's classifiers (see Section 3.3), since
features encode attributes domain objects others encode linguistic context
relative position text.
Let's look example Section 2.2 \XLS stepper" see
Wrap-Up makes discourse decision whether add pointer UV lithography equipment object. Wrap-Up encodes instance LithographyEquipment-Link decision tree features representing attributes lithography
equipment objects, extraction patterns, relative position text.
Wrap-Up's training phase, instance encoded every pair lithography
equipment objects training text. Training instances must classified positive
negative, Wrap-Up consults hand-coded target output provided training text
classifies instance positive pointer found matching lithography
equipment objects. creation training instances discussed fully Section
3.4. ID3 tabulates often feature value associated positive negative
training instance encapsulates statistics node tree builds.
Figure 2 shows portion Lithography-Equipment-Link tree, showing path used
classify instance UV lithography XLS stepper positive. parenthetical
numbers tree node show number positive negative training instances represented node. priori probability pointer lithography equipment
training corpus 34%, 282 positive 539 negative training instances.
ID3 uses information gain metric select effective feature partition
training instances (p.89-90, Quinlan, 1986), case choosing equipment type
test root tree. feature alone sucient classify instances
equipment type modular equipment, radiation source, etching system,
negative instances. Apparently types equipment never used lithography
processes (a useful bit domain knowledge).
branch equipment type \stepper" leads node tree representing 202
positive 174 negative training instances, raising probability link 54%. ID3
139

fiSoderland Lehnert

(282 pos, 539 neg)

Equipment-type
modularequipment

...
radiationsource

(0 pos, 11 neg)

etchingsystem

...

Stepper

lithographysystem

(0 pos, 125 neg)

(0 pos, 15 neg)

(80 pos, 141 neg)

(202 pos, 174 neg)

Lithography-type
...
G-line

...

E-beam

I-line

optical

UV
(15 pos, 27 neg)

(6 pos, 25 neg)
(2 pos, 31 neg)

(87 pos, 20 neg)

(27 pos, 14 neg)

Distance
-2

...

...

0

-1
(0 pos, 1 neg)

(18 pos, 12 neg)

(4 pos, 0 neg)

Figure 2: decision tree pointers lithography equipment objects.
recursively selects feature partition partition, case selecting lithography
type. branch UV lithography leads partition 27 positive 14 negative
instances, contrast E-beam optical lithography nearly negative
instances. next test distance, value -1 case since equipment
reference one sentence earlier lithography. branch leads leaf node
4 positive negative instances, tree returns classification positive
Wrap-Up adds pointer UV lithography stepper.
example shows decision tree acquire useful domain knowledge:
lithography never linked equipment etching systems, steppers
often used UV lithography hardly ever E-beam optical lithography. Knowledge
sort could manually engineered rather acquired machine learning,
hundreds rules needed might take weeks months effort create test.
Consider another fragment text tree Figure 3 decides whether add
pointer PLCC packaging process ROM chip device.
: : :a

new line 256 Kbit 1 Mbit ROM chips.
available PLCC priced : : :



instance classified Packaging-Device-Link tree includes features
packaging type, device type, distance two referents, extraction patterns
used sentence analysis.
140

fiWrap-Up: Trainable Discourse Module

(325 pos, 750 neg)

Distance

(0 pos, 12 neg)

...

...

-50

-20

(7 pos, 40 neg)

50

0

-1
(60 pos, 70 neg)

(130 pos, 93 neg)

(0 pos, 12 neg)

Device-type
...
EPROM
(6 pos, 2 neg)

memory

(0 pos, 11 neg)

...
DRAM

ROM
(13 pos, 2 neg)

none

(1 pos, 4 neg)

(0 pos, 19 neg)

pp-available-1
true

false

(13 pos, 0 neg)

(0 pos, 2 neg)

Figure 3: tree pointers packaging device objects.
ID3 selects \distance" root tree, feature counts distance sentences packaging device references text. closest references
20 sentences apart, hardly training instances positive.
distance -1 example text, ROM device mentioned one sentence earlier
PLCC packaging process. Figure 3 shows, branch distance -1 followed
test device type. branch device type ROM leads partition
15 instances, 13 positive 2 negative. PLCC packaging found pattern
\available X" (encoded pp-available-1) positive instances.
two trees illustrate different trees learn different types knowledge.
significant features determining whether equipment object linked lithography process real world constraints type equipment used lithography.
ected tree Figure 2 choosing equipment type root node followed lithography type. overriding constraint type device
linked packaging technique. linguistic clues play prominent role,
relative position references text particular extraction patterns.
following section discusses linguistic-based features encoded.

3.3 Generating Features ID3 Trees

Let's look detail Wrap-Up encodes ID3 instances, using information available
sentence analysis automatically derive features used tree. ID3
tree handles discourse decision domain object relationship pair
objects, different stages Wrap-Up involving different sorts decisions.
141

fiSoderland Lehnert

information encoded object comes concept nodes extracted
sentence analysis. Concept nodes case frame slots extracted information, also location extraction patterns reference text.
Consider example Section 2.2.
GCA unveiled new XLS stepper, developed
assistance Sematech. system available
deep-ultraviolet I-line configurations.

Sentence analysis extracts five objects text: company GCA, equipment XLS stepper, company Sematech, UV lithography, I-line lithography. One
several discourse decisions made whether UV lithography uses XLS stepper
mentioned previous sentence. Figure 4 shows two objects form basis
instance Lithography-Equipment-Link tree.
Equipment
Type: stepper
Name: XLS

Lithography
Type: UV
Extraction Patterns:
pp-available-in
keyword-deep-ultraviolet

Extraction Patterns:
obj-active-unveiled
subj-passive-developed
keyword-stepper

Figure 4: Two objects extracted sample text
object includes location reference patterns used extract
them. extraction pattern combination syntactic pattern specific lexical
item \trigger word" (as explained Section 2.1). pattern pp-available-in means
reference UV lithography found prepositional phrase following triggers
\available" \in".
Figure 5 shows instance UV lithography XLS stepper. encodes attributes extraction patterns object relative position text. WrapUp encodes case frame slot object using actual slot value closed classes
lithography type. Open class slots equipment names encoded
value \t" indicate name present, rather actual name. Using
exact name would result enormous branching factor feature might overly
uence ID3 classification low frequency name happened occur positive
negative instances.
Extraction patterns encoded binary features include trigger word
syntactic pattern feature name. Patterns two trigger words \pp-availablein" split two features, \pp-available" \pp-in". instances encode pair
objects features encoded \pp-available-1" \pp-in-1" refer
first object. count many extraction patterns used also encoded
142

fiWrap-Up: Trainable Discourse Module

(lithography-type . UV)
(extraction-count-1 . 3)
(pp-available-1 . t)
(pp-in-1 . t)
(keyword-deep-ultraviolet-1 . t)

(equipment-type . stepper)
(equipment-name . t)
(extraction-count-2 . 3)
(obj-unveiled-2 . t)
(subj-passive-developed-2 . t)
(keyword-stepper-2 . t)

(common-triggers . 0)
(common-phrases . 0)
(distance . -1)

Figure 5: instance Lithography-Equipment-Link tree.
object. feature \extraction-count" motivated Slot Filtering stage
since objects extracted several times likely valid extracted
twice text.
Another type feature, encoded instances involving pairs objects, relative
position references two objects, may significant determining two
objects related. One feature easily computed distance sentences
references. case feature \distance" value -1, since XLS stepper found
one sentence earlier UV lithography process. Another feature might indicate
strong relationship objects count many common phrases contain
references objects. features list \common triggers", words included
extraction patterns objects. example would word \using"
text phrase \the XLS stepper using UV technology".
important realize included instance. human making
discourse decision might reason follows. sentence UV lithography indicates
associated \the system", refers back \its new XLS stepper"
previous sentence. Part reasoning involves domain independent use definite
article, part requires domain knowledge \system" nonspecific reference
equipment object. current version Wrap-Up look beyond information
passed sentence analysis misses reference \the system" entirely.
Using specific linguistic patterns resulted extremely large, sparse feature sets
trees. Lithography-Equipment-Link tree 1045 features, 11 encoding
extraction patterns. Since typical instance participates dozen extraction
patterns, serious time space bottle neck would occur hundreds linguistic
patterns present explicitly listed instance. implemented
sparse vector version ID3 able eciently handle large feature spaces
tabulating small number true-valued features instance.
links added discourse processing, objects may become complex, including
many pointers objects. time Wrap-Up considers links companies
microelectronics processes, lithography object may pointer equipment
object device object, equipment object may turn pointers
objects. Wrap-Up allows objects inherit linguistic context position text
objects point. object pointer object B, location
143

fiSoderland Lehnert

extraction patterns references B treated references A. version
inheritance helpful, little strong, ignoring distinction direct
references inherited references.
looked encoding instances isolated discourse decisions
section. entire discourse system complex series decisions, affecting
environment used processing. training phase must ect changing
environment run time well provide classifications training instance based
target output. issues discussed next section.

3.4 Creating Training Instances

ID3 supervised learning algorithm requires set training instances, labeled
correct classification instance. create instances Wrap-Up begins
tree building phase passing training texts sentence analyzer, creates
set objects representing extracted information. Multiple references
object merged form initial input Wrap-Up's first stage. Wrap-Up encodes
instances builds trees stage, repeats process using trees stage one
build trees stage two, forth trees built six stages.
encodes instances, Wrap-Up repeatedly consults target output assign
classification training instance. building trees Slot Filtering stage
instance classified positive extracted information matches slot target
output. Consider example reference \Ultratech stepper" microelectronics
text. Sentence analysis creates equipment object two slots filled, equipment type
stepper equipment name \Ultratech". stage Wrap-Up separate ID3 tree
judge validity slot, equipment type equipment name.
Suppose target output equipment object type \stepper"
\Ultratech" actually manufacturer's name equipment model name.
equipment type instance classified positive equipment name instance classified negative since equipment object target output name Ultratech.
instance include features capture human analyst would consider
\Ultratech" equipment name? human probably using world knowledge
recognize Ultratech familiar company name recognize names
\Precision 5000" familiar equipment names. Knowledge lists known company
names known equipment names presently included Wrap-Up, although
could derived easily training corpus.
create training instances second stage Wrap-Up, entire training corpus
processed again, time discarding slot values spurious according Slot
Filtering trees creating instances Slot Merging trees. instance created
pair objects type. objects mapped object
target output, instance classified positive. example, instance would
created pair device objects, one device type RAM size 256
KBits. positive instance output single device object type RAM
size 256 KBits.
time instances created later stages Wrap-Up, errors crept
previous stages. Errors filtering, merging, linking resulted
144

fiWrap-Up: Trainable Discourse Module

objects retained longer match anything target output objects
partially match target output. Since degree error unavoidable,
best let training instances ect state processing occur later
Wrap-Up used process new texts. training perfectly filtered, merged,
linked, representative underlying probabilities run time use
Wrap-Up.
later stages Wrap-Up objects may become complex partially match anything target output. aid matching complex objects, one slot object
type identified output structure definition key slot. object considered
match object output key slots match. Thus object missing
equipment name spurious equipment name still match equipment type, key
slot, matches. object pointer object B, object matching output
must also pointer object matching B.
recursive matching becomes important Link Creation stage. Among
last links considered microelectronics roles company plays towards process.
company may developer x-ray lithography process uses ABC stepper,
developer x-ray lithography process linked different equipment object.
Wrap-Up needs sensitive distinctions classifying training instances trees
Link Creation Object Splitting stages.
Instances Inferring Missing Objects stage Inferring Missing Slot Values
stage classifications go beyond simple positive negative. instance
Inferring Missing Objects stage created whenever object found training
higher object pointing it. matching object indeed exists target output,
Wrap-Up classifies instance type object points output.
example training text may reference \stepper" equipment,
mention process uses stepper. target output lithography
object type \unknown" points stepper equipment. legitimate inference make, since steppers type lithography equipment. instance
orphaned stepper equipment object classified \lithography-unknown-equipment".
classification gives Wrap-Up enough information run time create appropriate object.
instance Inferring Missing Slot Values created whenever slot missing
object closed class possible values, \status" slot equipment
objects, value \in-use" \in-development". matching object
found target output, actual slot value used classification. slot
empty object exists output, instance classified negative.
Inferring Missing Objects stage, negative likely classification many trees.
Next consider effects tree pruning confidence thresholds make
ID3 cautious aggressive classifications.

3.5 Confidence Thresholds Tree Pruning

machine learning technique tendency toward \overfitting", making
generalizations based accidental properties training data. ID3
likely happen near leaf nodes decision tree, partition size may
145

fiSoderland Lehnert

grow small ID3 select features much predictive power. feature chosen
discriminate among half dozen training instances likely particular
instances useful classifying new instances.
implementation ID3 used Wrap-Up deals problem setting pruning level confidence threshold tree empirically. new instance classified
traversing decision tree root node node reached partition
size pruning level. classification halts node classification
positive returned proportion positive instances greater equal
confidence threshold.
high confidence threshold make ID3 tree cautious classifications,
low confidence threshold allow positive classifications. effect changing
confidence threshold pronounced pruning level increases. large
enough pruning level, nearly branches terminate internal nodes confidence
somewhere 0.0 1.0. low confidence threshold classify
instances positive, high confidence threshold classify negative.
Wrap-Up automatically sets pruning level confidence threshold tree using
tenfold cross-validation. training instances divided ten sets set
tested tree built remaining nine tenths training. done
various settings find settings optimize performance.
metrics used domain \recall" \precision", rather accuracy.
Recall percentage positive instances correctly classified, precision
percentage positive classifications correct. metric combines recall
precision f-measure, defined formula f = (fi 2 + 1)P R=(fi 2P + R) fi
set 1 favor balanced recall precision. Increasing decreasing fi selected
trees fine-tune Wrap-Up, causing select pruning confidence thresholds
favor recall favor precision.
seen Wrap-Up automatically derives classifiers needed feature
set classifier, tunes classifiers recall/precision balance.
look requirements using Wrap-Up, special attention issue
manual labor system development.

3.6 Requirements Wrap-Up

Wrap-Up domain-independent architecture applied domain
well defined output structure, domain objects represented case frames
relationships objects represented pointers objects. appropriate
information extraction task important identify logical relationships
extracted information. user must supply Wrap-Up output definition
listing domain objects extracted. output object one slots,
may contain either extracted information pointers objects output.
One slot object labeled key slot, used training match extracted
objects objects target output.
domain application already well defined, user able create
output definition less hour. new application, whose information needs established, likely certain amount trial error
146

fiWrap-Up: Trainable Discourse Module

developing desired representation. need well defined domain unique
discourse processing trainable components Wrap-Up. IE systems require clearly defined specifications types objects extracted
relationships reported.
time consuming requirement Wrap-Up associated acquisition
training texts importantly, hand-coded target output. hand-coded targets
represent labor-intensive investment part domain experts, knowledge
natural language processing machine learning technologies needed generate
answer keys, domain expert produce answer keys use Wrap-Up.
thousand microelectronics texts used provide training Wrap-Up. actual
number training instances training texts varied considerably decision
tree. Trees handled common domain objects ample training instances
two hundred training texts, dealt less frequent objects
relationships undertrained thousand texts.
easier generate hundred answer keys write explicit
comprehensive domain guidelines. Moreover, domain knowledge implicitly present set
answer keys may go beyond conventional knowledge domain expert
reliable patterns information transcend logical domain model. available,
corpus training texts used repeatedly knowledge acquisition levels
processing.
architecture Wrap-Up depend particular sentence analyzer
particular information extraction task. used sentence analyzer uses
keywords local linguistic patterns extraction. output representation produced
Wrap-Up could either used directly generate database entries MUC-like task
could serve internal representation support information extraction tasks.

3.7 Joint Ventures Domain

Wrap-Up implemented tested microelectronics domain, tried
another domain, MUC-5 joint ventures domain. information extracted
domain companies involved joint business ventures, products services,
ownership, capitalization, revenue, corporate ocers, facilities. Relationships
companies must sorted identify partners, child companies, subsidiaries.
output structure complex microelectronics, back-pointers, cycles
output structure, redundant information, longer chains linked objects.
Figure 6 shows text joint ventures domain diagram target output.
pointers back-pointers, output even moderately complicated text
becomes dicult understand glance. text describes joint venture
Japanese company, Rinnai Corp., unnamed Indonesian company build factory
Jakarta. tie-up identified Rinnai Indonesian company partners
third company, joint venture itself, child company. output includes
\entity-relationship" object duplicates much information tie-up object.
corporate ocer, amount capital, ownership percentages, product \portable
cookers", facility also reported output.
147

fiSoderland Lehnert

RINNAI CORP., JAPAN'S LEADING GAS APPLIANCE MANUFACTURER, SET
JOINT VENTURE INDONESIA AUGUST PRODUCE PORTABLE COOKERS
LOCAL USERS, PRESIDENT SUSUMU NAITO SAID MONDAY.
NEW FIRM CAPITALIZED ONE MILLION DOLLARS,
RINNAI SCHEDULED PUT 50 PCT LOCAL DEALER 50 PCT, SAID.
MANUFACTURE 3,000 4,000 UNITS MONTH INITIALLY PLANT
26,000-SQUARE-METER SITE JAKARTA, NAITO SAID, ADDING RINNAI AIMS
START FULL-SCALE PRODUCTION NEXT SPRING.
NAGOYA-BASED COMPANY SEVEN OVERSEAS PRODUCTION UNITS.

Template
Doc-Nr: 1485
Content:

Tie-Up
Status: existing
Entity:
Joint-venture:
Ownership:
Activity:

Activity
Site: (
Industry:

Entity
Type: company
Location: Indonesia
Relationship:
Facility:

Entity
Type: company
Name: Rinnai Corp
Aliases: "Rinnai"
Location: Nagoya, Japan
Relationship:
Person:

Entity
Type: company
Nationality: Indonesia
Relationship:

Facility
Type: factory
Location: Jakarta,
Indonesia

Person
Name: Susumu Naito
Position: pres
Entity:

Relationship
Entity-1:
Entity-2:
Relation: child
Status: future

)

Industry
Type: Production
Product:
"portable
cookers"

Ownership
Capital: 1000000 $
Ownership-Percent: (
Owned:

50) (

50)

Figure 6: sample text target output joint ventures domain.

148

fiWrap-Up: Trainable Discourse Module

special handling required joint ventures domain since output
structure defined MUC-5 evaluation included slots activity site
ownership percent whose values mixture extracted information pointers.
slot values internal structure thought pseudoobjects,
activity site object pointers facility object company, ownership
percent object pointer company another slot giving numeric value.
pseudoobjects reformulated standard objects conforming requirements
Wrap-Up, activity site slot pointing activity site object forth.
transformed back complex slot fills printing final representation
output.
output specifications joint ventures less well-behaved ways,
graph cycles, back pointers, redundant objects whose content must agree information elsewhere output. Modifications Wrap-Up needed relax implicit
requirements domain structure, allowing graph cycles giving special handling
pointer slot user labeled output definition back pointer.
Joint ventures also implicit constraints relationships objects.
company play single role tie-up joint venture relationship: cannot
joint venture child also parent partner company. Wrap-Up diculty
learning constraint performed better certain pointer slots labeled
\single-role" constraint output definition.
strategy letting user indicate constraints annotating slots output
definition implemented ad hoc fashion. general approach would allow
user declare several types constraint output. pointer slot may required
optional, may one pointer allow several. slots object may
mutually exclusive, entry one prohibiting entry another slot. may
required agreement value slot one object slot another object.
fully domain-independent discourse tool needs mechanism implement generalized
constraints.

4. System Performance

point comparison performance Wrap-Up, UMass/Hughes system
run TTG discourse module, used ocial MUC-5 evaluation. Overall system performance Wrap-Up compared performance TTG,
holding rest system constant.
Wrap-Up takes idea TTG extends fully trainable system. TTG
used decision trees acquire domain knowledge, often relied hand-coded heuristics
apply acquired knowledge, particular decisions splitting merging
objects, Wrap-Up handles Object Splitting stage; inferring missing objects,
Wrap-Up Inferring Missing Objects stage; adding context sensitive
default slot values, Wrap-Up Inferring Missing Slot Values stage.
Several iterations hand tuning required adjust thresholds decision trees
produced TTG, whereas Wrap-Up found thresholds pruning levels optimize recall
precision tree automatically. day CPU time devoted decision tree
training, Wrap-Up produced working system programming needed.
149

fiSoderland Lehnert

comparison TTG made microelectronics domain
joint ventures domain. metrics used recall precision. Recall percentage possible information reported. Correctly identifying two five
possible company names gives recall 40. Precision percent correct reported
information. four companies reported, two correct, precision 50.
Recall precision combined single metric f-measure, defined f =
(fi 2 + 1)P R=(fi 2P + R), fi set 1 balanced recall precision.

4.1 Microelectronics Domain

Wrap-Up's scores ocial MUC-5 microelectronics test sets generally little
higher TTG, overall recall precision.
Wrap-Up
Rec. Prec. F

TTG
Rec. Prec.

Part 1
Part 2

32.3 44.4 37.4
36.3 38.6 37.4

27.1 39.5 32.1
32.7 37.0 34.7

Part 3

34.6 37.7 36.1

34.7 40.5 37.5

Avg.

34.4 40.2 36.8

31.5 39.0 34.8

F

Figure 7: Performance MUC-5 microelectronics test sets
put scores perspective, highest scoring systems MUC-5 evaluation
f-measures high 40's. dicult task sentence analysis
discourse analysis.
Another way assess Wrap-Up measure performance baseline
provided output sentence analysis. Lack coverage sentence analyzer
places ceiling performance discourse level. test set part 1 208
company names extracted. CIRCUS analyzer extracted total 404 company
names, 131 correct 2 partially correct, giving baseline 63% recall 33%
precision slot. Wrap-Up's Entity-Name-Filter tree managed discard little
half spurious company names, keeping 77% good companies. resulted
49% recall 44% precision slot, raising f-measure 5 points,
expense recall.
Limited recall extracted objects compounded comes links
objects. half possible companies third microelectronics processes
missing, discourse processing chance large proportion possible links
companies processes.
Although precision often increased expense recall, Wrap-Up also mechanisms increase recall slightly. Inferring Missing Objects stage infers missing
process equipment object Object Splitting stage splits process points
multiple equipment, Wrap-Up sometimes gain recall produced
sentence analyzer.

150

fiWrap-Up: Trainable Discourse Module

4.2 Joint Ventures Domain

joint ventures domain Wrap-Up's scores MUC-5 test sets little lower
ocial UMass/Hughes scores. Wrap-Up tended lower recall slightly
higher precision.
Wrap-Up
Rec. Prec. F

TTG
Rec. Prec.

Part 1
Part 2

23.5 52.9 32.5
22.7 53.6 31.9

26.0 53.9 35.1
26.0 52.1 34.7

Part 3

23.3 51.4 32.1

27.7 49.7 35.6

Avg.

23.2 52.7 32.2

26.5 52.0 35.1

F

Figure 8: Performance MUC-5 joint ventures test sets
performance Wrap-Up TTG roughly comparable two
domains. systems tend favor domain first developed, WrapUp developed microelectronics ported joint ventures, opposite true
TTG. certain amount bias probably crept design decisions meant
domain independent system. higher scores TTG joint ventures
partly due hand-coded heuristics altered output TTG printing final
output, something done TTG microelectronics Wrap-Up either
domain.
noticeable difference Wrap-Up TTG output joint ventures
domain filtering spuriously extracted company names. Discourse processing
started 38% recall 32% precision sentence analysis company names.
systems included filtering stage attempted raise precision discarding spurious
companies, expense discarding valid companies well.
system used threshold settings control cautiously aggressively discarding
done (as example Section 3.5). TTG's set hand Wrap-Up's
selected automatically cross-validation training set. TTG mild filtering
slot, resulting gain 2 precision points drop 6 recall points. Wrap-Up
chose aggressive settings gained 13 precision points lost 17 points recall
slot.
result, Wrap-Up ended two thirds many correct companies
TTG. turn meant two thirds many pointers companies tie-ups entity
relationships. objects Wrap-Up scored higher recall TTG, getting
three times total recall activity, industry, facility objects.

5. Conclusions

recent accessibility large on-line text databases news services, need
information extraction systems growing. systems go beyond information retrieval
create structured summary selected information contained within relevant documents. gives user ability skim vast amounts text, pulling information
151

fiSoderland Lehnert

particular topic. IE systems knowledge-based, however, must individually
tailored information needs application.
research laboratories focused sophisticated user interfaces ease
burden knowledge acquisition. GE's NLToolset example approach (Jacobs et
al., 1993), BBN typifies systems combine user input corpus-based statistics
(Ayuso et al., 1993). University Massachusetts moving direction
machine learning create fully trainable IE system. ultimate goal turnkey
system tailored new information needs users special linguistic
technical expertise.
Wrap-Up embodies goal. user defines information need output structure,
provides training corpus representative texts hand-coded target output
text. Wrap-Up takes instantiates fully functional IE discourse
system new domain customization needed user. Wrap-Up
first fully trainable system handle discourse processing,
degradation performance. automatically decides classifiers needed based
domain output structure derives feature set classifier sentence
analyzer output.
intriguing aspect Wrap-Up automatic generation features.
effective this, trees actually learn? greatest leverage seems
come features encode attributes domain objects. trees microelectronics
often based classification probabilities conditioned device type, equipment
type, process type. example tree Section 3.2 first tested equipment type
lithography type determining whether piece equipment used lithography
process. type real world domain knowledge important thing
Wrap-Up learned microelectronics.
Useful knowledge also provided features encoded relative position
references text. Distance, measured number sentences apart, played prominent
role many classifications, trees relying fine-grained features
number times references noun phrase overlapping
linguistic context.
enhancement Wrap-Up's feature generation would increase expressiveness
relative position. addition direct references object object B, Wrap-Up
could look indirect references (pronominal anaphoric) found near references
B vice versa. instance shown Section 3.3 example features
indirect relationships might useful.
Wrap-Up currently encodes instance pair objects might related,
incapable expressing rule \attach object B recent object type A."
blind existence objects alternate candidates relationship
considered. Features could encoded ect whether object recently
mentioned object type.
features least successful tantalizing encoded
local linguistic context, extraction patterns. included exact lexical item
nearly low frequency added noise often aiding useful
discriminations. Tree pruning partial solution, experiment combining
semantically similar terms caused sharp drop classification accuracy.
152

fiWrap-Up: Trainable Discourse Module

Low frequency terms built-in problem system processes unrestricted
text. Dunning (93) estimated 20-30% typical English news wire reports composed
words frequency less one 50,000 words. Yet discourse decisions made
human reader often seem hinge use one infrequent terms.
challenging open question find methods utilize local linguistic context without
drowning noise produced low frequency terms.
Finding mechanism choosing appropriate features critical machine learning algorithm applied. ID3 chosen easy implement, although
approaches vector spaces worth trying. obvious, however, craft
weighting scheme gives greatest weight useful features vector
space nearly zero useful making desired discrimination. Cost
Salzberg (1993) describe weighting scheme nearest neighbor algorithm looks
promising lexically-based features. Another candidate effective classifier back
propagation network, might naturally converge weights give uence
useful features.
hope Wrap-Up inspire machine learning community consider analysis
unrestricted text fruitful application ML research, challenging natural
language processing community consider ML techniques complex processing tasks.
broader context, Wrap-Up provides paradigm user customizable system design,
technological background part user assumed. fully functional
system brought new domain without need months development
time, signifying substantial progress toward fully scalable portable natural language
processing systems.

Appendix A: Walk-through Sample Text

see Wrap-Up algorithm action, consider sample text Figure 9.
desired output company, Mitsubishi Electronics America, Inc., linked purchaser/user two packaging processes, TSOP SOJ packaging. processes
point device, 1 Mbit DRAM. packaging material, plastic, attached
TSOP SOJ. details text considered extraneous
domain.
sentence analysis, followed step merges multiple references,
eight objects passed input Wrap-Up. Sentence analysis fairly well identifying
relevant information, missing \1 M" reference 1 MBits. Three
eight objects spurious discarded Wrap-Up's Slot Filtering stage.
According domain guidelines, name \Mitsubishi Electronics America, Inc."
reported, \The Semiconductor Division ...". packaging material EPOXY
device MEMORY also discarded.
Slot Filtering stage creates instance slot object. EntityName-Filter tree classifies \Mitsubishi Electronics America, Inc." positive instance,
\The Semiconductor Division ..." negative discarded. reliable
discriminator valid company names \extraction-count", selected root
feature tree. Training instances participating several extraction patterns
twice likely valid extracted twice. held true text.
153

fiSoderland Lehnert

Semiconductor Division Mitsubishi Electonics America, Inc. offers
1M CMOS DRAMs Thin Small-Outline Packaging (TSOP*), providing
highest memory density available industry. Developed Mitsubishi,
TSOP also lets designers increase system memory density standard
reverse "mirror image," pin-outs. Mitsubishi's 1M DRAM TSOP provides
density chip-on-board much higher reliability
plastic epoxy-resin package allows device 100% burned-in fully
tested. *Previously referred VSOP (very small-outline package) USOP
(ultra small-outline package). 1M DRAM TSOP height 1.2 mm,
plane measurement 16.0 mm x 6.0 mm, lead pitch 0.5 mm, making
nearly three times thinner four times smaller volume 1M
DRAM SOJ package. SOJ height 3.45 mm, plane dimension
17.15 mm x 8.45 mm, lead pitch 1.27 mm. Additionally, TSOP
weighs 0.22 grams, contrast 0.75 gram weight SOJ.
Full text available PTS New Product Announcements.

Figure 9: microelectronics text

Entity
Type: company
Name:Mitsubishi Electronics
America, Inc.

Entity
Type: company
Name: Semiconductor Division
Mitsubishi Electronics America, Inc.

Device
Type: DRAM

Packaging
Type: TSOP

Device
Type: MEMORY

Packaging
Material: EPOXY

Packaging
Material: PLASTIC

Packaging
Type: SOJ

Figure 10: Input Wrap-Up sample text

154

fiWrap-Up: Trainable Discourse Module

\Mitsubishi Electronics America, Inc." extraction count 5, spurious name
extracted 2 patterns.
Slot Filtering stage continues, packaging material EPOXY classified negative Packaging-Material-Filter tree, whose root test packaging type. turns
EPOXY usually extracted erroneously training corpus. contrasts
material PLASTIC usually reliable classified positive. TSOP
SOJ packaging types classified positive Packaging-Type-Filter tree. Instances
types usually positive training set, particularly extracted multiple
times text. Device-Type-Filter tree, root feature device type, finds
DRAM reliable device type MEMORY usually spurious training
corpus. usually merged specific device type.
Slot Merging stage Wrap-Up considers pair remaining objects
type. three packaging objects, one type TSOP, one material
PLASTIC, one type SOJ. Packaging-Slotmerge tree easily rejects TSOPSOJ instance, since packaging objects never multiple types training. testing
second object packaging type, feature \distance" tested. led
positive classification TSOP-PLASTIC, sentence, negative
SOJ-PLASTIC, nearest references two sentences apart. point four objects
remain:
Entity
Type: company
Name:Mitsubishi Electronics
America, Inc.

Packaging
Type: TSOP
Material: PLASTIC

Device
Type: DRAM

Packaging
Type: SOJ

Link Creation stage considers pair objects could linked according
output structure. first links considered pointers packaging device
objects. Separate instances Packaging-Device-Link tree created possible
TSOP-DRAM link possible SOJ-DRAM link. Although 25% training
instances positive, tree found 78% positive packaging type TSOP
\distance" 0 sentences, 77% positive packaging type SOJ device
type DRAM. testing features, tree found instances
positive pointers added output. Notice tree interleaves knowledge
types packaging types devices knowledge relative position
references text.
next Link Creation decision concern roles Mitsubishi plays towards
packaging processes. output structure \microelectronics-capability" object
one slot pointing lithography, layering, etching, packaging process, four
slots (labeled developer, manufacturer, distributor, purchaser/user) pointing companies. Wrap-Up accordingly encodes four instances Mitsubishi TSOP packaging,
one possible role. done Mitsubishi SOJ packaging.
Instances Mitsubishi roles developer, manufacturer, distributor
classified negative. Training instances trees almost positive instances.
155

fiSoderland Lehnert

Template
Doc-Nr: 2523814
Contents:
ME-Capability
Purchaser/User:
Developer:
Process:

ME-Capability
Purchaser/User:
Process:

Packaging
Type: TSOP
Material: plastic
Device:

Packaging
Type: SOJ
Device:

Entity
Type: company
Name: Mitsubishi
Electronics
America, Inc.

Device
Type: DRAM

Figure 11: Final output links added
seems stories packaging processes corpus almost exclusively
companies purchasing using someone else's packaging technology.
seldom explicit linguistic clues relationship company process
corpus, Packaging-User-Link tree tests first relative distance
references. 20% training instances positive, distance 0 jumped
43% positive. Mitsubishi sentence TSOP Mitsubishi-SOJ
instance also distance 0 inheritance. Even though nearest reference SOJ
two sentences Mitsubishi, SOJ linked DRAM occurs sentence
Mitsubishi. instances classified positive testing packaging type
features.
last discourse decision Link Creation stage add pointers microelectronics capability \template object", created dummy root object
domain's output. Object Splitting stage finally gets make decision, albeit vacuous
one, decides let template object point multiple objects \content" slot.
\orphan" objects missing slot values last two stages Wrap-Up
consider. final output text shown Figure 11.

Acknowledgements
research supported NSF Grant no. EEC-9209623, State/Industry/University
Cooperative Research Intelligent Information Retrieval.

156

fiWrap-Up: Trainable Discourse Module

References

Ayuso, D., Boisen, S., Fox, H., Gish, H., Ingria, R., & Weischedel, R. (1992). BBN: Description PLUM System Used MUC-4. Proceedings Fourth Message
Understanding Conference, 169-176. Morgan Kaufmann Publishers.
Brent, M. (1993). Robust Acquisition Subcategorization Frames. Proceeding
Association Computational Linguistics.
Cardie, C. (1993). Case-Based Approach Knowledge Acquisition Domain-Specific
Sentence Analysis. Proceedings Eleventh National Conference Artificial
Intelligence, 798-803.
Church, K. (1988). stochastic parts program noun phrase parser unrestricted text.
Proceedings Second Conference Applied Natural Language Processing
ACL, 136-143.
Cost, S., & Salzberg, S. (1993). Weighted Nearest Neighbor Algorithm Learning
Symbolic Features. Machine Learning, 10(1), 57-78.
DeRose, S. (1988). Grammatical Category Disambiguation Statistical Optimization.
Computational Linguistics, 14(1), 31-39.
Dolan, C. P., Goldman, S. R., Cuda, T. V., & Nakamura, A. M. (1991). Hughes Trainable
Text Skimmer: Description TTS System used MUC-3. Proceedings
Third Message Understanding Conference, 155-162. Morgan Kaufmann Publishers.
Dunning, T. (1993). Accurate Methods Statistics Surprise Coincidence. Computational Linguistics, 19(1), 61-74.
Grosz, B., & Sidner C. (1986). Attention, intention structure discourse. Computational Linguistics, 12(3), 175-204.
Hindle, D. (1989). Acquiring Disambiguation Rules Text. Proceeding Association Computational Linguistics, 118-125.
Hobbs, J. (1978). Resolving Pronoun References. Lingua, 44(4), 311-338.
Jacobs, P., Krupka, G., Rau, L., Mauldin, M., Mitamura, T., Kitani, T., Sider, I., &
Childs, L. (1993). GE-CMU: Description SHOGUN System used MUC5. Proceedings Fifth Message Understanding Conference, 109-120. Morgan
Kaufmann Publishers.
Lehnert, W. (1990). Symbolic/Subsymbolic Sentence Analysis: Exploiting Best Two
Worlds. Advances Connectionist Neural Computation Theory. vol. 1.. Norwood,
NJ: Ablex Publishing, 151-158.
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Riloff, E., & Soderland, S. (1992).
University Massachusetts: Description CIRCUS System Used MUC-4.
Proceedings Fourth Message Understanding Conference, 282-288. Morgan
Kaufmann Publishers.
157

fiSoderland Lehnert

Lehnert, W., McCarthy, J., Soderland, S., Riloff, E., Cardie, C., Peterson, J., Feng, F.,
Dolan, C., & Goldman, S. (1993). UMass/Hughes: Description CIRCUS System
Used MUC-5. Proceedings Fifth Message Understanding Conference,
257-259. Morgan Kaufmann Publishers.
Liddy, L., McVearry, K., Paik, W., Yu, E., & McKenna, M. (1993). Development, Implementation, Testing Discourse Model Newspaper Texts. Proceedings
Human Language Technology Workshop, 159-164. Morgan Kaufmann Publishers.
MUC-3. (1991). Proceedings Third Message Understanding Conference. Morgan Kaufmann Publishers.
MUC-4. (1992). Proceedings Fourth Message Understanding Conference. Morgan
Kaufmann Publishers.
MUC-5. (1993). Proceedings Fifth Message Understanding Conference. Morgan Kaufmann Publishers.
Quinlan, J.R. (1986). Induction Decision Trees. Machine Learning, 1, 81-106.
Riloff, E. (1993). Automatically Constructing Dictionary Information Extraction
Tasks. Proceedings Eleventh National Conference Artificial Intelligence,
811-816.
Salton, G., Wong, A., & Yang, C.S. (1975). vector space model automatic indexing.
Correspondences ACM, 18(11), 613-620.
Soderland, S., & Lehnert, W. (1994). Corpus-Driven Knowledge Acquisition Discourse
Analysis. Proceedings Twelfth National Conference Artificial Intelligence,
827-832.
Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., & Palmucci, J. (1993). Coping
Ambiguity Unknown Words Probabilistic Models. Computational
Linguistics, 19(2), 359-382.
Will, C. (1993). Comparing human machine performance natural language information extraction: Results English microelectronics MUC-5 evaluation.
Proceedings Fifth Message Understanding Conference, 53-67. Morgan Kaufmann Publishers.

158

fiJournal Artificial Intelligence Research 2 (1995) 361{367

Submitted 8/94; published 3/95

Research Note

Informativeness DNA
Promoter Sequences Domain Theory

Julio Ortega
Computer Science Dept., Vanderbilt University
P.O. Box 1679, Station B
Nashville, TN 37235 USA

julio@vuse.vanderbilt.edu

Abstract
DNA promoter sequences domain theory database become popular
testing systems integrate empirical analytical learning. note reports simple
change reinterpretation domain theory terms M-of-N concepts, involving
learning, results accuracy 93.4% 106 items database. Moreover,
exhaustive search space M-of-N domain theory interpretations indicates
expected accuracy randomly chosen interpretation 76.5%, maximum
accuracy 97.2% achieved 12 cases. demonstrates informativeness
domain theory, without complications understanding interactions various
learning algorithms theory. addition, results help characterize diculty
learning using DNA promoters theory.

1. Introduction
DNA promoter sequences domain theory database, contributed M. Noordewier
J. Shavlik UCI repository (Murphy & Aha, 1992), become popular
testing systems integrate empirical analytical learning (Hirsh & Japkowicz, 1994;
Koppel, Feldman, & Segre, 1994b; Mahoney & Mooney, 1994, 1993; Norton, 1994; Opitz &
Shavlik, 1994; Ortega, 1994; Ourston, 1991; Towell, Shavlik, & Noordewier, 1990; Shavlik,
Towell, & Noordewier, 1992). original domain theory, usually interpreted, overly
specific classifies promoter sequences database negative instances. Since database consists 53 positive instances 53 negative instances,
accuracy database 50%. learning systems cited take advantage
initial domain theory order achieve higher accuracy rates, especially fewer
training examples, rates achieved purely inductive methods C4.5
backpropagation. Thus, informativeness theory acknowledged, despite 50%
accuracy rate using naive interpretation. However, extent theory
informative easily ascertained; implicit interactions
learning algorithms theory. note reports simple change reinterpretation
domain theory terms M-of-N concepts, involve learning, results
accuracy 93.4% 106 data items. Moreover, exhaustive search space
M-of-N interpretations reveals achieve 97.2% accuracy.

c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiOrtega

promoter

contact

conformation

minus_35

p-37=c
p-36=t
p-35=t
p-34=g
p-33=a
p-32=c

p-36=t
p-35=t
p-34=g
p-32=c
p-31=a

p-36=t
p-35=t
p-34=g
p-33=a
p-32=c
p-31=a

minus_10

p-36=t
p-35=t
p-34=g
p-33=a
p-32=c

p-14=t
p-13=a
p-12=t
p-11=a
p-10=a
p-9=t

p-13=t
p-12=a
p-10=a
p-8=t

p-13=t
p-12=a
p-11=t
p-10=a
p-9=a
p-8=t

p-12=t
p-11=a

p-7=t

p-47=c
p-46=a
p-45=a
p-43=t
p-42=t
p-40=a
p-39=c
p-22=g
p-18=t
p-16=c
p-8=g
p-7=c
p-6=g
p-5=c
p-4=c
p-2=c
p-1=c

p-45=a
p-44=a
p-41=a

p-49=a
p-44=t
0-27=t
p-22=a
p-18=t
p-16=t
p-15=g
p-1=a

p-45=a
p-41=a
p-28=t
p-27=t
p-23=t
p-21=a
p-20=a
p-17=t
p-15=t
p-4=a

Figure 1: DNA Promoters Theory

2. DNA Promoter Sequences Database Domain Theory
sources UCI promoter sequences database domain theory described
Towell (1990). rules theory derived biological research
O'Neill (1989). negative examples contiguous strings long DNA sequence
believed contain promoters. positive examples promoters taken
compilation Hawley McClure (1983). database recently augmented (Harley & Reynolds, 1987; Lisser & Margalit, 1993) new theories promoter
action still appearing (Lisser & Margalit, 1993). Nevertheless, UCI promoter sequences database examples domain theory remained prominent testbed
evaluating machine learning methods.
DNA promoters domain theory obtained UCI repository shown
Figure 1 AND-OR tree. box leaf tree Figure 1 usually interpreted
conjunction conditions. condition box requires particular nucleotide
appear particular position sequence. According theory, DNA sequence
classified promoter two regions DNA sequence identified.
first region called contact, second conformation. conformation region
identified, one four specific nucleotide sequences shown right-hand side
Figure 1 need present. contact region identified, minus 10
minus 35 region also need identified. Again, minus 10 minus 35 region
362

fiOn Informativeness DNA Promoter Sequences Domain Theory

identified, one respective four specific nucleotide sequences need present.
sequence classified promoter domain theory, sequence
classified negative (i.e., promoter).
noted earlier, promoters domain theory coupled database 106 items
UCI repository: 53 examples promoters 53 non-examples. interpreting
leaf domain theory logical conjunction, theory classifies data
items negative. Thus, clearly restrictive: sequence database satisfies
conditions specified theory. Two pieces domain knowledge suggest ways
loosen conditions domain theory. First, conformation condition
weak biological support. implied initial KBANN experiments (Shavlik
et al., 1992), none learned rules referenced conformation conditions.
addition, EITHER system (Ourston, 1991) eliminated rules involving conformation
altogether domain theory. Eliminating conformation also supported
domain expert (Ourston, 1991). second piece domain knowledge concepts
domain tend take form M-of-N concepts. final rules extracted
KBANN approach take form. also made clear NEITHER-MofN
system (Baffes & Mooney, 1993), added mechanism handle M-of-N concepts
original learning mechanism EITHER/NEITHER system.

3.

M-of-N

Interpretations DNA Promoters Theory

modify original DNA domain theory follows, allow sequences
positively classified promoters: a) eliminate conformation condition altogether
theory, b) reinterpret conjunctions conditions leaves Figure 1
M-of-N concepts. usually interpreted, leaves Figure 1 equivalent
concept form (N-of-N c1c2:::cN ). example, conjunction (p-37=c ^ p-36=t ^
p-35=t ^ p-34=g ^ p-33=a ^ p-32=c) leftmost leaf logically equivalent (6-of-6
p-37=c p-36=t p-35=t p-34=g p-33=a p-32=c).
Progressively less restrictive theories created lowering number conditions
need satisfied leaf theory. Thus, new theory constructed
N-of-N concepts substituted (N { 1)-of-N, (N { 2)-of-N, etc.
variable N (i.e., number conditions leaf) decremented constant value
obtain value M-of-N concepts leaves theory. Figure 2
shows accuracy theories constructed manner examples
database. number conditions met M-of-N concepts
lowered, number false negatives decreases, number false positive increases.
total number misclassifications (false negatives plus false positives) minimized
leaf interpreted (N { 2)-of-N concept, resulting accuracy 93.4%.
Even better accuracies obtained remove constant decrement restriction.
is, allow greater exibility choosing different values leaves
corresponding minus 35 minus 10 regions Figure 1. exhaustive search
388800 possible combinations values found twelve theories
correctly classify 103 106 examples database (i.e., accuracy theories
97.2%), 5148 theories accuracy equal better 93.4%. Figure 3 shows
probabilities obtaining theories different accuracies value
363

fiOrtega

Correct
False
False
Percent
M-of-N criteria
Predictions Positives Negatives Accuracy
N-of-N (original theory)
53
0
53
50.00%
N-of-N w/ conformation rule removed
57
0
49
53.77%
(N { 1)-of-N
78
0
28
73.58%
(N { 2)-of-N
99
1
6
93.40%
(N { 3)-of-N
90
16
0
84.91%
(N { 4)-of-N
62
44
0
58.49%
Figure 2: Accuracy DNA promoters theory different M-of-N interpretations
0.06
P[accuracy]
0.05

Probability

0.04

0.03

0.02

0.01

0
0

0.2

0.4

0.6

0.8

1

Accuracy

Figure 3: Probability distribution DNA-theory-interpretation accuracies
contact leaves Figure 1 chosen random (but restriction N,
N total number conditions particular leaf). probabilities Figure 3
computed counting total number combinations values produced
theories specific accuracies. mean accuracy randomly chosen theory 76.5%,
standard deviation 9.3%.
results show leaves interpreted appropriate M-of-N concepts,
existing DNA domain theory possesses large amount predictive information, fact
also pointed Koppel et al. (1994a). much better null
power suggested initial 50% accuracy, would equivalent random guessing
theory all. least, theory allows us make single random guess
M-of-N interpretation expected accuracy 76.5%. shown Figure 2
random guesses allow us much better that.
364

fiOn Informativeness DNA Promoter Sequences Domain Theory

4. Learning DNA Promoters Theory

accuracies various systems integrate analytical empirical learning around
93% (Baffes & Mooney, 1993). results typically means computed multiple
trials 80-85 training examples 21-26 tests examples. reported accuracies
93.4% 97.2% based splits training test data. Instead,
represent maximum accuracies (relative database 106 examples) could
obtained learning algorithms certain representational biases. example, 93.4%
maximum accuracy may achieved learning system identifies (N
{ i)-of-N concepts leaves, constant across leaves. approach
converted learning task learner identifies optimal value given set
training examples, evaluates resultant classifier using test set. results
algorithm, averaged 100 trials, produce mean accuracies 88.7% 10 training
examples, 92.5% 85 training examples (on test set 21 examples). results
similar best algorithms reported Baffes Mooney (1993).
Koppel et al. (1994a) also show considerable information \reinterpreted" promoters domain theory. DOP (Degree Provedness) classification
methodology, logical operations propositional domain theory (AND, OR, NOT)
replaced arithmetic equivalents contain degree uncertainty. Rather
directly returning truth value indicating whether example positive, system first
calculates DOP numerical score example. DOP score value greater
pre-specified threshold value, example considered \suciently" proved
thus classified positive example theory. Otherwise, example classified
negative. Koppel et al. determine threshold value two pieces knowledge: a)
distribution DOP score examples, b) proportion (n%) positive
examples database. DOP values examples sorted, threshold
value set value separates n% examples highest DOP values
rest. important assumption domain theory certain proofadditive nature, DOP values higher positive examples negative
examples. DOP classification methodology achieves high accuracy (92.5%) applied DNA promoter sequences domain theory. approach, accuracy
based split available data training test sets, represents
upper bound accuracy could obtained method converted
learning algorithm. DOP classification methodology could converted learning
algorithm estimating distribution DOP values set training examples.

5. Concluding Remarks

note detail new learning algorithm. Rather, demonstrates suitable
learning model promoters domain finding correct number, M,
M-of-N concepts leaves original domain theory. 1 Assessing diculty
learning using available theory usually complicated need understand
learning algorithms exploit theory. theory-accuracy distribution Figure 3
1. algorithms may introduce structural modification theory (i.e., add/delete clauses
conditions). However, increase accuracy due structural modifications negligible
case promoters domain, illustrated high accuracies obtained without them.

365

fiOrtega

helps characterize learning complexity domain (under M-of-N model) provides dimension along evaluate performance learning algorithms use
DNA promoter's theory testbed.

Acknowledgements
research supported grant NASA Ames Research Center (NAG 2-834)
Doug Fisher. grateful suggestions Doug Fisher, Stefanos Manganaris, Doug
Talbert, Jing Lin, well comments pointers Larry Hunter anonymous
JAIR referees.

References

Baffes, P. T., & Mooney, R. J. (1993). Symbolic revision theories M-of-N rules.
Proceedings Thirteenth International Joint Conference Artificial Intelligence
Chambery, France.
Harley, C. B., & Reynolds, R. P. (1987). Analysis e. coli promoter sequences. Nucleic
Acids Research, 15 (5), 2343{2361.
Hawley, D. K., & McClure, W. R. (1983). Compilation analysis escherichia coli
promoter DNA sequences. Nucleic Acids Research, 11 (8), 2237{2255.
Hirsh, H., & Japkowicz, N. (1994). Boostraping training-data representations inductive
learning: case study molecular biology. Proceedings Twelfth National
Conference Artificial Intelligence, pp. 639{644 Seattle, WA.
Koppel, M., Segre, A. M., & Feldman, R. (1994a). Getting awed theories.
Proceedings Eleventh International Conference Machine Learning, pp.
139{147 New Brunswick, NJ.
Koppel, M., Feldman, R., & Segre, A. M. (1994b). Bias-driven revision logical domain
theories. Journal Artificial Intelligence Research, 1, 159{208.
Lisser, S., & Margalit, H. (1993). Compilation e. coli mRNA promoter sequences. Nucleic
Acids Research, 21 (7), 1507{1516.
Mahoney, J. J., & Mooney, R. J. (1993). Combining connectionist symbolic learning
refine certainty-factor rule-bases. Connection Science, 5 (3{4), 339{364.
Mahoney, M. J., & Mooney, R. J. (1994). Comparing methods refining certainty-factor
rule-bases. Proceedings Eleventh International Conference Machine Learning, pp. 173{180 New Brunswick, NJ.
Murphy, P. M., & Aha, D. W. (1992). UCI Repository Machine Learning Databases.
Department Information Computer Science, University California Irvine,
Irvine, CA.
366

fiOn Informativeness DNA Promoter Sequences Domain Theory

Norton, S. W. (1994). Learning recognize promoter sequences e. coli modeling
uncertainty training data. Proceedings Twelfth National Conference
Artificial Intelligence, pp. 657{663 Seattle, WA.
O'Neill, M. C. (1989). Escherichia coli promoters I: Consensus relates spacing class,
specificity, repeat structure, three dimensional organization. Journal Biological
Chemistry, 264, 5522{5530.
O'Neill, M. C., & Chiafari, F. (1989). Escherichia coli promoters II: spacing-class dependent promoter search protocol. Journal Biological Chemistry, 264, 5531{5534.
Opitz, D. W., & Shavlik, J. W. (1994). Using genetic search refine knowledge-based
neural networks. Proceedings Eleventh International Conference Machine
Learning, pp. 208{216 New Brunswick, NJ.
Ortega, J. (1994). Making you've got: using models data improve learning rate prediction accuracy. Tech. rep. TR-94-01, Computer Science
Dept., Vanderbilt University. Abstract appears Proceedings Twelfth National
Conference Artificial Intelligence, p. 1483, Seattle, WA.
Ourston, D. (1991). Using Explanation-Based Empirical Methods Theory Revision.
Ph.D. thesis, University Texas, Austin, TX.
Shavlik, J. W., Towell, G., & Noordewier, M. O. (1992). Using neural networks refine
existing biological knowledge. International Journal Human Genome Research, 1,
81{107.
Towell, G. G. (1990). Symbolic Knowledge Neural Networks: Insertion, Refinement,
Extraction. Ph.D. thesis, University Wisconsin, Madison, WI.
Towell, G. G., Shavlik, J. W., & Noordewier, M. O. (1990). Refinement approximate
domain theories knowledge-based neural networks. Proceedings Eighth
National Conference Artificial Intelligence, pp. 861{866 Boston, MA.

367

fiJournal Artificial Intelligence Research 2 (1995) 475-500

Submitted 10/94; published 5/95

Adaptive Load Balancing: Study Multi-Agent
Learning
Andrea Schaerf

aschaerf@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma \La Sapienza", Via Salaria 113, I-00198 Roma, Italy

Yoav Shoham

Robotics Laboratory, Computer Science Department
Stanford University, Stanford, CA 94305, USA

Moshe Tennenholtz

Faculty Industrial Engineering Management
Technion, Haifa 32000, Israel

shoham@flamingo.stanford.edu
moshet@ie.technion.ac.il

Abstract
study process multi-agent reinforcement learning context load balancing distributed system, without use either central coordination explicit communication. first define precise framework study adaptive load balancing,
important features stochastic nature purely local information
available individual agents. Given framework, show illuminating results
interplay basic adaptive behavior parameters effect system eciency.
investigate properties adaptive load balancing heterogeneous populations,
address issue exploration vs. exploitation context. Finally, show
naive use communication may improve, might even harm system eciency.

1. Introduction
article investigates multi-agent reinforcement learning context concrete
problem undisputed importance { load balancing. Real life provides us many examples emergent, uncoordinated load balancing: trac alternative highways tends
even time; members computer science department tend use powerful networked workstations, eventually find lower load machines
inviting; on. would like understand dynamics emergent
load-balancing systems apply lesson design multi-agent systems.
define formal yet concrete framework study issues, called multiagent multi-resource stochastic system, involves set agents, set resources,
probabilistically changing resource capacities, probabilistic assignment new jobs agents,
probabilistic job sizes. agent must select resource new job,
eciency resource handles job depends capacity resource
lifetime job well number jobs handled resource
period time. performance measure system aims globally optimizing
resource usage system ensuring fairness (that is, system shouldn't made
ecient expense particular agent), two common criteria load balancing.
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiSchaerf, Shoham, & Tennenholtz

agent choose appropriate resource order optimize measures?
make important assumption, spirit reinforcement learning (Sutton,
1992): information available agent prior experience. particular,
agent necessarily know past, present, future capacities resources,1
unaware past, current, future jobs submitted various agents, even
relevant probability distributions. goal agent thus adapt resourceselection behavior behavior agents well changing capacities
resources changing load, without explicitly knowing are.
interested several basic questions:

good resource-selection rules?
fact different agents may use different resource-selection rules affect
system behavior?

communication among agents improve system eciency?
following sections show illuminating answers questions. contribution paper therefore twofold. apply multi-agent reinforcement learning
domain adaptive load balancing use basic domain order demonstrate
basic phenomena multi-agent reinforcement learning.
structure paper follows. Section 2 discuss general setting.
objective section motivate study point impact. formal
framework defined discussed Section 3. Section 4 completes discussion
framework introducing resource selection rule parameters, function
\control knobs" adaptive process. Section 5 present experimental results
adaptive behavior within framework show various parameters affect
eciency adaptive behavior. case heterogeneous populations investigated
Section 6, case communicating populations discussed Section 7. Section 8
discuss impact results. Section 9 put work perspective
related work. Finally, Section 10 conclude brief summary.

2. General Setting

paper applies reinforcement learning domain adaptive load balancing. However, presenting model use detailed study, need clarify several
points general setting. particular, need explain interpretation
reinforcement learning interpretation load balancing adopt.
Much work devoted recent years distributed adaptive load balancing. One find related work field distributed computer systems (e.g., Pulidas,
Towsley, & Stankovic, 1988; Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993;
Glockner & Pasquale, 1993; Mirchandaney, Towsley, & Stankovic, 1989; Zhou, 1988; Eager,
Lazowska, & Zahorjan, 1986), organization theory management science (e.g., Malone,
1. many applications capacities resources known, least extent. point
discussed later. Basically, paper wish investigate far one go using purely
local feedback without use global information (Kaelbling, 1993; Sutton, 1992).

476

fiAdaptive Load Balancing: Study Multi-Agent Learning

1987), distributed AI (e.g., Bond & Gasser, 1988). Although motivations
above-mentioned lines research similar, settings discussed essential
differences.
Work distributed computer systems adopts view set computers
controls certain resources, autonomous decision-making capability, jobs
arrive dynamic fashion. decision-making agents different computers
(also called nodes) try share system load coordinate activities means
communication. actual action performed, based information received
computers, may controlled various ways. One ways adopted control
related decisions learning automata (Narendra & Thathachar, 1989).
above-mentioned work agent associated set resources,
agent related resources associated node distributed system.
Much work management science distributed AI adopts somewhat complementary
view. difference classical work distributed operating systems, agent
associated set resources controls. agents autonomous entities
negotiate among (Zlotkin & Rosenschein, 1993; Kraus & Wilkenfeld, 1991)
use shared resources. Alternatively, agents (called managers case) may
negotiate task executed processors may execute (Malone, 1987).
model adopt avor models used distributed AI organization
theory. assume strict separation agents resources. Jobs arrive agents
make decisions execute them. resources passive (i.e.,
make decisions). typical example setting computerized framework set
PCs, controlled different user submits jobs executed
one several workstations. workstations assumed independent
shared among users. example real-life situation motivated
study terminology adopt taken framework. However,
real-life situations related model areas different classical distributed
computer systems.
canonical problem related model following one (Arthur, 1994): agent,
embedded multi-agent system, select among set bars (or set restaurants).
agent makes autonomous decision performance bar (and therefore
agents use it) function capacity number agents use it.
decision going bar stochastic process decision bar use
autonomous decision respective agent. similar situation arises product
manager decides processor use order perform particular task. model
present Section 3 general model situations investigated.
situations job arrives agent (rather node consisting particular resources)
decides upon resource (e.g., restaurant) job executed;
a-priori association agents resources.
discuss way agents behave framework. common theme
among above-mentioned lines research load-balancing achieved means
communication among active agents active resources (through related decisionmaking agents). study adopt complementary view. consider agents
act purely local fashion, based purely local information described recent
reinforcement learning literature. mentioned, learning automata used
477

fiSchaerf, Shoham, & Tennenholtz

field distributed computer systems order perform adaptive load balancing. Nevertheless, related learning procedures rely heavily communication among agents (or
among decision-making agents autonomous computers). work applies recent work
reinforcement learning AI information agent gets purely local. Hence,
agent know ecient service restaurant choosing
place eat. don't assume agents may informed agents load
restaurants restaurants announce current load. makes
work strictly different work applying reinforcement learning adaptive load
balancing.
features make model study basic general. Moreover,
discussion raises question whether reinforcement learning (based purely
local information feedback) guarantee useful load balancing. combination
model use perspective reinforcement learning makes contribution
novel. Nevertheless, mentioned (and discuss Section 9) model
use original us captures many known problems situations distributed
load balancing. apply reinforcement learning, discussed recent AI literature,
model investigate properties related process.

3. Multi-Agent Multi-Resource Stochastic System

section define concrete framework study dynamic load balancing.
model present captures adaptive load balancing general setting mentioned
Section 2. restrict discussion discrete, synchronous systems (and thus
definition refer N , natural numbers); similar definitions possible
continuous case. concentrate case job executed using
resources. Although somewhat restricting, common practice much work
distributed systems (Mirchandaney & Stankovic, 1986).

Definition 3.1 multi-agent multi-resource stochastic system 6-tuple hA; R; P ; D; C;
SRi, = fa1; : : :; g set agents, R = fr1; : : :; rM g set resources,
P : N ! [0; 1] job submission function, : N ! < probabilistic job size
function, C : RN ! < probabilistic capacity function, SR resource-selection
rule.

intuitive interpretation system follows. resources
certain capacity, real number; capacity changes time, determined
function C . time point agent either idle engaged. idle, may
submit new job probability given P . job certain size also
real number. size submitted job determined function D. (We
use unit token referring job sizes resource capacities, mean
tokens come integer quantities.) new job agent selects one
resources. choice made according rule SR; since much say
rule, discuss separately next section.
model, job may run resource. Furthermore, limit
number jobs served simultaneously given resource (and thus queuing occurs).
However, quality service provided resource given time deteriorates
478

fiAdaptive Load Balancing: Study Multi-Agent Learning

number agents using time. Specifically, every time point resource
distributes current capacity (i.e., tokens) equally among jobs served it.
size job reduced amount and, drops (or below) zero, job
completed, agent notified this, becomes idle again. Thus, execution time
job j depends size, capacity time resource processing it,
number agents using resource execution j .
measure system's performance twofold: aim minimize timeper-token, averaged jobs, well minimize standard deviation
random variable. Minimizing quantities ensure overall system eciency well
fairness. question selection rules yield ecient behavior; turn next
definition rules.

4. Adaptive Resource-Selection Rules
rule agents select resource new job, selection rule (SR),
heart adaptive scheme topic section. Throughout section
following one make assumption homogeneity. Namely, assume
agents use SR. Notice although system homogeneous, agent
act based local information. Sections 6 7 relax homogeneity
assumption discuss heterogeneous communicating populations.
already emphasized, among possible adaptive SRs interested
purely local SRs, ones access experience particular agent.
setting experience consists results previous job submissions; job submitted
agent already completed, agent knows name r resource used,
point time, tstart , job started, point time, tstop , job finished,
job size . Therefore, input SR is, principle, list elements form
(r; tstart; tstop ; ). Notice type input captures general type systems
interested in. Basically, wish assume little possible information
available agent order capture real loosely-coupled systems global
information unavailable.
Whenever agent selects resource job execution, may get feedback
non-negligible time, feedback may depend decisions made agents
agent i's decision. forces agent rely non-trivial portion
history makes problem much harder.
uncountably many possible adaptive SRs aim gain exhaustive understanding them. Rather, experimented family intuitive
relatively simple SRs compared non-adaptive ones. motivation choosing particular family SRs partially due observations made
cognitive psychologists people tend behave multi-agent stochastic recurrent situations. principle, set SRs captures two robust aspects
observations: \The law effect" (Thronkide, 1898) \Power law practice" (Blackburn, 1936). family rules, called
, partially resembles learning rules
discussed learning automata literature (Narendra & Thathachar, 1989), partially resembles interval estimation algorithm (Kaelbling, 1993), agents maintain
complete history experience. Instead, agent, A, condenses history
479

fiSchaerf, Shoham, & Tennenholtz

vector, called eciency estimator, denoted eeA . length vector
number resources, i'th entry vector represents agent's evaluation
current eciency resource (specifically, eeA (R) positive real number).
vector seen state learning automaton. addition eeA , agent keeps
vector jdA, stores number completed jobs submitted agent
resources, since beginning time. Thus, within
, need specify
two elements:
1. agent updates eeA job completed
2. agent selects resource new job, given eeA jdA
Loosely speaking, eeA maintained weighted sum new feedback
previous value eeA , resource selected probably one highest
eeA entry except low probability resource chosen. two
steps explained precisely following two subsections.

4.1 Updating Eciency Estimator
take function updating eeA

eeA (R) := WT + (1 , W )eeA (R)
represents time-per-token newly completed job computed
feedback (R; tstart; tstop; ) following way:2

= (tstop , tstart)=S
take W real value interval [0; 1], whose actual value depends jdA(R).
means take weighted average new feedback value old
value eciency estimator, W determines weights given pieces
information. value W obtained following function:

W = w + (1 , w)=jdA(R)
formula w real-valued constant. term (1 , w)=jdA(R) correcting
factor, major effect jdA (R) low; jdA (R) increases, reaching
value several hundreds, term becomes negligible respect w.

4.2 Selecting Resource

second ingredient adaptive SRs
function pdA selecting resource
new job based eeA jdA . function probabilistic. first define following
function
(
jdA(R) > 0
(R),n
0
pdA(R) := ee
,
n
E [ee ]
jd (R) = 0




2. Using parallel processing terminology, viewed stretch factor, quantifies stretching
program's processing time due multiprogramming (Ferrari, Serazzi, & Zeigner, 1983).

480

fiAdaptive Load Balancing: Study Multi-Agent Learning

n positive real-valued parameter E [eeA] represents average values
eeA (R) resources satisfying jdA (R) > 0. turn probability function,
define pdA normalized version pd0A :

pdA(R) := pd0A(R)=
= Rpd0A (R) normalization factor.3
function pdA clearly biases selection towards resources performed
well past. strength bias depends n; larger value n,
stronger bias. extreme cases, value n high (e.g., 20), agent
always choose resource best record. strategy \always choosing
best", although perhaps intuitively appealing, general good one;
allow agent exploit improvements capacity load resources.
discuss SR following subsection, expand issue exploration versus
exploitation Sections 6 7.
summarize, defined general setting investigate emergent load
balancing. particular, defined family adaptive resource-selection rules,
parameterized pair (w; n). parameters serve knobs tune
system optimize performance. next section turn experimental
results obtained system.

4.3 Best Choice SR (BCSR)

Best Choice SR (BCSR) learning rule assumes high value n, i.e,
always chooses best resource given point. assume w fixed given
value discussing BCSR. previous work (Shoham & Tennenholtz, 1992, 1994),
showed learning rules strongly resemble BCSR useful several natural
multi-agent learning settings. suggests need carefully study case
adaptive load balancing. demonstrate, BCSR always useful load
balancing setting.
difference BCSR learning rule value n low,
latter case agent gives relatively high probability selection resource
didn't give best results past. case agent might able notice
behavior one resources improved due changes system.
Note exploration \non-best" resources crucial dynamics
system includes changes capacities resources. cases, agent could
take advantage possible increases capacity resources uses BCSR. One
might wonder, however, whether cases main dynamic changes system
stem load changes, relying BCSR sucient. latter true,
able ignore parameter n concentrate BCSR, systems
capacity resources fixed. order clarify point, consider following
example.
3. R jdA (R) = 0, (i.e., agent going submit first job), assume
agent chooses resource randomly (with uniform probability distribution).

481

fiSchaerf, Shoham, & Tennenholtz

Suppose two resources, R1 R2 , whose respective (fixed) capacities,
cR1 cR2 , satisfy equality cR1 = 2cR2 . Assume load system varies

certain low value certain high one.
system's load low agents adopt BCSR, system evolve
way almost agents would preferring R1 R2. due
fact that, case low load, overlaps jobs, hence R1 much
ecient. hand, system's load high, R1 could busy
agents would prefer R2, since performance obtained using
less crowded resource R2 could better one obtained using overly crowded
resource R1. extreme case high load, expect agents use R2 one
third time.
Assume load system starts low level, increases
high value, decreases reach original value. load increases,
agents, mostly using R1, start observing R1's performance becoming
worse and, therefore, following BCSR start using R2 too. Now, load
decreases, agents using R2 observe improvement performance
R2, value stored R1 (i.e., eeA (1)), still ect previous
situation. Hence, agents keep using R2, ignoring possibility obtaining
much better results moved back R1. situation, randomized selection
makes agents able use R1 (with certain probability) therefore
may discover performance R1 better R2 switch back R1.
improve system's eciency significant manner.
example shows BCSR is, general case, good choice.
general true value n high.
discussion assumed changes load unforeseen.
able predict changes load, agents simply use BCSR
load fixed use low value n changes. case, instead,
without even realizing system changed way, agents would need
(and, see, would able to) adapt dynamic changes well other.

5. Experimental Results
section compare SRs
another, well non-adaptive,
benchmark selection rules.
non-adaptive SRs consider paper agents partition
according capacities load system fixed predetermined
manner agent uses always resource. Later paper, SR
kind identified configuration vector, specifies, resource, many
agents use it. test adaptive SRs, compare performance nonadaptive SRs perform best particular problem. creates highly competitive
set benchmarks adaptive SRs.
addition, compare adaptive SRs load-querying SR defined
follows: agent, new job, asks resources busy
always chooses less crowded one.
482

fiAdaptive Load Balancing: Study Multi-Agent Learning

5.1 Experimental Setting
introduce particular experimental setting, many results described
obtained. present order concrete experiments; however,
qualitative results experiments observed variety experimental
settings.
One motivation particular setting stems PCs workstations problem
mentioned Section 2. example, part study related set computers
located single site. computers relatively high load peak hours
day low load night (i.e., chances user PC submits job
higher day time week days night weekend). Another
part study related set computers split around world,
load quite random structure (i.e., due difference time zones, users may use PCs
unpredictable hours).
Another motivation particular setting stems restaurant problem mentioned Section 2 (for discussion related \bar problem" see Arthur, 1994).
example, consider set snack bars located industrial park. snack
bars relatively high loads peak hours day low load night
(i.e., chances employee choose go snack-bar higher day
employees present day). Conversely, assume
set bars near airport load quite random structure (i.e., airport
employees may like use snack-bars quite unpredicted hours).
Although particular real-situations, would like emphasize general
motivation study fact related phenomena observed
various different settings.
take N , number agents, 100, , number resources,
5. first set experiments take capacities resources fixed.
particular, take c1 = 40; c2 = 20; c3 = 20; c4 = 10; c5 = 10. assume
agents probability submitting new job. also assume
agents distribution size jobs submit; specifically, assume
uniform distribution integers range [50,150].
ease exposition, assume point time corresponds second,
consequently count time minutes, hours, days, weeks. hour
main point reference; assume, simplicity, changes system (i.e., load
change capacity change) happen beginning new hour. probability
submitting job second, corresponds load system, vary
time; crucial factor agents must adapt. Note agents
submit jobs second, probability submission may change. particular
concentrate three different values quantity, called Llo ; Lhi Lpeak ,
assume system load switches values. actual values Llo ; Lhi
Lpeak following quantitative results 0:1%, 0:3% 1%, roughly correspond
agent submitting 3.6, 10.8, 36 jobs per hour (per agent) respectively.
483

fiSchaerf, Shoham, & Tennenholtz

load

configuration
time-per-token
Llo
f100; 0; 0; 0; 0g
38.935
Lhi
f66; 16; 16; 1; 1g
60.768
Lpeak f40; 20; 20; 10; 10g
196.908
Figure 1: Best non-adaptive SRs fixed load
following, measuring success, refer average time-pertoken.4 However, adaptive SRs give best average time-per-token also
found fair.

5.2 Fixed Load

start case load fixed. case interesting
adaptive behavior; however, satisfactory SR show reasonably ecient behavior
basic case, order useful system stabilizes.
start showing behavior non-adaptive benchmark SRs case fixed
load.5 Figure 1 shows give best results, three loads.
see, big difference three loads mentioned above.
load particularly high, agents scatter around resources rate
proportional capacities; load low use best resource.
Given above, easy see adaptive SR effective enables
moving quickly one configuration other.
static setting this, expect best non-adaptive SRs perform better adaptive ones, since information gained exploration adaptive SRs
built-in non-adaptive ones. experimental results confirm intuition,
shown Figure 2 Lhi . figure shows performance obtained population
value n varies 2 10 three values w: 0.1, 0.3, 0.5.
Note values (n; w) good choices dynamic cases (see later
paper, values intervals [3; 5] [0:1; 0:5], respectively), deterioration
performance adaptive SRs respect non-adaptive ones small.
encouraging result, since adaptive SRs meant particularly suitable dynamic
systems. following subsections see indeed are.

5.3 Changing Load

begin explore dynamic settings. consider case
load system (that is, probability agents submitting job time) changes
time. paper present two dynamic settings: One load changes
according fixed pattern random perturbations another
load varies random fashion. Specifically, first case fix load Lhi
4. data shown later refer, convenience, time 1000 tokens.
5. non-adaptive SRs human-designed SRs used benchmarks; assume knowledge
load capacity, available adaptive SRs design.

484

fiAdaptive Load Balancing: Study Multi-Agent Learning


v
e
r

g
e



e
p
e
r


k
e
n

6
67





Weight: w = 0.5
Weight: w = 0.3
Weight: w = 0.1

66





65
64





63
62
61
2



























3
4 5
6 7
8
9 10
Exponent Randomization Function: n

-

Figure 2: Performance adaptive Selection Rules fixed load
ten consecutive hours, five days week, two randomly chosen hours
Lpeak , Llo rest week. second case, fix number
hours week load first case, distribute completely
randomly week.
results obtained two cases similar. Figure 3 shows results obtained
adaptive SRs case random load. best non-adaptive deterministic
SR gives time-per-token value 69:201 obtained configuration (partition
agents) f52; 22; 22; 2; 2g; adaptive SRs superior. load-querying SR instead gets
time-per-token value 48:116, obviously better, far
performances adaptive SRs.
also observe following phenomenon: Given fixed n (resp. fixed w) average
time-per-token non-monotonic w (resp. n). phenomenon strongly related
issue exploration versus exploitation mentioned phenomena observed
study Q-learning (Watkins, 1989).
also notice two parameters n w interplay. fact, value
w minimum time per token value obtained different value n.
precisely, higher w lower n must order obtain best results. means
that, order obtain high performance, highly exploratory activity (low n)
matched giving greater weight recent experience (high w). \parameter
485

fiSchaerf, Shoham, & Tennenholtz


v
e
r

g
e



e
p
e
r


k
e
n

6
71
70
69


Weight: w = 0.5




Weight: w = 0.3
Weight: w = 0.1













68












67
66
65
2













3
4 5
6 7
8
9 10
Exponent Randomization Function: n

-

Figure 3: Performance adaptive Selection Rules random load
matching" intuitively explained following qualitative way: exploration
activity pays allows agent detect changes system. However,
effective if, change detected, significantly affect eciency estimator
(i.e., w high). Otherwise, cost exploration activity greater gain.

5.4 Changing Capacities
consider case capacity resources vary time.
particular, demonstrate results case previously mentioned setting.
assume capacities rotate randomly among resources and, five consecutive
days, resource gets capacity 40 one day, 20 2 days, 10
2 days.6 load also varies randomly.
results experiment shown Figure 4. best non-adaptive SR
case gives time-per-token value 118:561 obtained configuration
f20; 20; 20; 20; 20g.7 adaptive SRs give much better results, slightly
6. Usually capacities change less dramatic fashion. use above-mentioned setting
order demonstrate applicability approach severe conditions.
7. load-querying SR gives results case fixed capacities, SR
obviously uenced change.

486

fiAdaptive Load Balancing: Study Multi-Agent Learning


v
e
r

g
e



e
p
e
r


k
e
n

6




92.5




90





87.5



82.5






80




85

77.5



2








Weight: w = 0.5

Weight: w = 0.3
Weight: w = 0.1




-

3
4
5
6 7 8
9 10
Exponent Randomization Function: n

Figure 4: Performance adaptive Selection Rules changing capacities
worse case fixed capacities. phenomena mentioned visible
case too. See example weight 0:1 mismatches low values n.

6. Heterogeneous Populations
Throughout previous section assumed agents use SR, i.e.
Homogeneity Assumption. assumption models situation sort
centralized off-line controller which, beginning, tells agents behave
leaves agents make decisions.
situation described different on-line centralized controller makes every decision. However, would like move even
investigate situation agent able make decision
strategy use and, maybe, adjust time.
step toward study systems kind, drop Homogeneity Assumption
consider situation part population uses one SR part
uses second one.
first set experiments, consider setting discussed Subsection 5.1
confront one other, two populations (called 1 2) size (50 agents
each). population uses different SR
. SR population (for = 1; 2)
487

fiSchaerf, Shoham, & Tennenholtz




v
e
r

g
e



e
p
e
r


k
e
n

6



67
66



65























63
61










64
62


: T1
: T2



2 3
4 5
6 7
8
9 10
Exponent Randomization Function (n2 )

-

Figure 5: Performance 2 populations 50 agents n1 = 4 w1 = w2 = 0:3
determined pair parameters (wi; ni ). measure success population
defined average time-per-token members, denoted Ti .
Figure 5 shows result obtained w1 = w2 = 0:3, n1 = 4, different
values n2 , case randomly varying load.
results expose following phenomenon: two populations obtain different
outcomes ones obtain homogeneous case. specifically, 4
n2 6 , results obtained agents use n2 generally better results
obtained ones use n1 , despite fact homogeneous population
uses n1 gets better results homogeneous population uses n2 .
phenomenon described following intuitive explanation. n2
above-mentioned range, population uses n2 less \exploring" (i.e.,
\exploiting") one, left might able
adapt changes satisfactory manner. However, joined
population, gets advantages experimental activity agents population,
without paying it. fact, exploring agents, trying unload
crowded resources, make service agents well.
worth observing Figure 5 n2 low (e.g., n2 3) agents use
n2 take role explorers lose lot, agents use n1 gain
situation. Conversely, high values n2 (e.g., n2 7) performances exploiters,
488

fiAdaptive Load Balancing: Study Multi-Agent Learning


v
e
r

g
e



e
p
e
r


k
e
n



6
67
66



65


: T1
: T2










64
63
62










61
2
















3
4 5
6 7
8
9 10
Exponent Randomization Function: n2

-

Figure 6: Performance 2 populations 90/10 agents n1 = 4 w1 = w2 = 0:3
use n2 , deteriorate. means exploiters static, hinder
other, explorers take advantage it.
better understanding phenomena involved, experimented
asymmetric population, composed one large group one small one, instead two
groups similar size. Figure 6 shows results obtained using setting similar
one above, population 1 composed 90 members population 2 consists
10 members. case, every value n2 4, exploiters better
explorers. experiments also show case, higher n2 better T2
is, i.e. exploiters exploit, gain.
results suggest single agent gets best results noncooperative always adopting resource best performance (i.e., use BCSR),
given rest agents use adaptive (i.e., cooperative) SR. However,
agents non-cooperative lose.8 conclusion, selfish interest
agent match interest population. contrary results
obtained basic contexts multi-agent learning (Shoham & Tennenholtz, 1992).
shown how, fixed value w, coexisting populations adopting
different values n interact. Similar results obtained fix value n
8. fact illuminating instance well-known prisoners dilemma (Axelrod, 1984).

489

fiSchaerf, Shoham, & Tennenholtz


v
e
r

g
e



e
p
e
r


k
e
n

6
67




: T1
: T2

66
65
64




















63

























62
61

-

0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Weight estimator parameter (w2)

1

Figure 7: Performance 2 populations 50 agents n1 = n2 = 4 w1 = 0:3
use two different values w. cases, agents adopting lower value w
general winners, shown Figure 7 n1 = n2 = 4 w1 = 0:3. w
low corresponding agents get poor results longer winners,
case high n Figure 5.
Another interesting phenomenon obtained confronting adaptive agents
load-querying agents. Load-querying agents agents able consult resources
submit jobs. load-querying agent submit job
unloaded resource given point. confronting load-querying agents
adaptive ones, results obtained adaptive agents obviously worse
results obtained load-querying ones, better results obtained
complete population adaptive agents. means load-querying agents play
role \parasites", above-mentioned \exploiters"; load-querying agents help
maintaining load balancing among resources, therefore help rest
agents. Another result obtain agents adopt deterministic SRs may behave
parasites worsen performance adaptive agents.
assertions supported experiments described Figure 8, population 90 agents, uses adaptive SR parameters (n; w), faced
minority 10 agents use different SRs, stated above. particular, four
cases consider, minority behaves following ways: (i) choose resource
490

fiAdaptive Load Balancing: Study Multi-Agent Learning

90 agents
10 agents
T1
(.3,4)
(.3,20)
65.161
(.3,4)
(.1,4)
64.630
(.3,4) Load-querying 62.320
(.3,4)
Using Res. 0 65.499

T2

59.713
63.818
47.236
55.818

Figure 8: Performance 2 populations 90/10 agents various SRs
gave best results, (ii) conservative updating history, (iii)
load-querying agents, (iiii) use deterministically resource capacity 40
(in basic experimental setting).

7. Communication among Agents
point, assumed direct communication among agents.
motivation considered situations absolutely
transmission channels protocols. assumption agreement idea
multi-agent reinforcement learning. systems massive communication feasible
much concerned multiple agent adaptation, problem reduces
supplying satisfactory communication mechanisms. Multi-agent reinforcement learning
interesting real life forces agents act without a-priori arranged communication channels must rely action-feedback mechanisms. However, interest
understand effects communication system eciency (as Shoham & Tennenholtz, 1992; Tan, 1993), agents augmented sort communication
capabilities. study extension led illuminating results,
present.
assume agent communicate agents,
call neighbors. therefore consider relation neighbor-of assume exive, symmetric transitive. consequence, relation neighbor-of partitions
population equivalence classes, call neighborhoods.
form communication consider based idea eciency estimators agents within neighborhood shared among decision made
(i.e., agent chooses resource). reader notice naive
form communication sophisticated types communication possible.
However, form communication natural concentrate agents
update behavior based past information. particular, type
communication similar ones used above-mentioned work incorporating
communication framework multi-agent reinforcement learning.
suppose different SRs may used different agents population,
impose condition within single neighborhood, SR used
members.
also assume agent keeps history updates
usual way. choice, instead, based agent eciency estimator,
491

fiSchaerf, Shoham, & Tennenholtz


v
e
r

g
e



e
p
e
r


k
e
n

6
71
70














68





67



66



65
2














69
















: 5 CNs 20 agents
: 20 CNs 5 agents
: 50 CNs 2 agents

3
4 5
6 7
8
9 10
Exponent Randomization Function: n

-

Figure 9: Performance adaptive Selection Rules random load profile communicating agents
average eciency estimators agents corresponding neighborhood.
average called neighborhood eciency estimator. neighborhood eciency
estimator physical storage: value recalculated time member needs it.
order compare behavior communicating agents non-communicating ones,
assume single population might be, aside neighborhoods defined
above, also neighborhoods allow sharing eciency estimators among
members. members neighborhoods behave described previous
sections, i.e., agent relies history. thing common
among members neighborhood members use SR.
call communicating neighborhood (CN), neighborhood eciency estimators shared decision taken non-communicating neighborhood (NCN),
neighborhood done.
first set experiments ran, regards population composed CNs,
size. particular, considered CNs various sizes, starting 50 CNs
size 2, going 5 CNs size 20. load profile exploited random load change
defined Subsection 5.3, value w taken 0:3, n taken various
values. results obtained shown Figure 9.
492

fiAdaptive Load Balancing: Study Multi-Agent Learning

results show communicating populations get good results.
reason members CN tend conservative, sense
mostly use best resource. fact, since rely average several agents,
picture system tends much static. particular, bigger
CN conservative members tend be. example, consider values
(n; w) give best results non-communicating agents, values give quite bad
performance CNs since turn conservative.
Using adaptive values (n; w), behavior communicating population improves reaches performance slightly worse performance
non-communicating population. Tuning parameters using finer grain, possible
obtain performance equal one obtained non-communicating population.
However, seems clear obvious gain achieved form communication
capability. intuitive explanation two opposite effects caused
communication. one hand, agents get fairer picture system prevents using bad resources therefore getting bad performance.
hand, since agents CN \better" picture system, tend
use best resources thus compete them. fact, agents behave
selfishly selfish interest may agree interest population
whole.
interesting message get fact agents may
\distorted" picture system (which typical non-communicating populations),
turns advantage population whole.
Sharing data among agents leads poorer performances also case
agents common views loads target jobs toward (lightly loaded)
resources, quickly become overloaded. order profitably use shared data,
allow form reasoning fact data shared.
problem however scope paper (see e.g., Lesser, 1991).
order understand behavior system CNs NCNs face other,
consider NCN 80 agents together set CNs equal size, different values
size. results corresponding experiments shown Figure 10.
members CNs, inclined use best resources, behave parasites
sense explained Section 6. exploit adaptiveness rest population
obtain good performance best resources. reason get better results
rest population, shown experimental results.
interesting observe NCN uses conservative selection rule,
CNs obtain even better results. intuitive explanation behavior
although groups, i.e., communicating ones one high value n,
tend conservative, communicating ones \win" conservative
\clever" way, making use better picture situation.
conclusion draw section proposed form communication
agents may provide useful means improve performance population
setting. However, claim communication agents completely
useless. Nevertheless, observed provide straightforward significant
improvement. results support claim sole past history agent
493

fiSchaerf, Shoham, & Tennenholtz

80 agents
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN

20 agents
(.3,4) 1 CN
(.3,4) 2 CNs
(.3,4) 5 CNs
(.3,4) 10 CNs
(.3,4) 1 CN
(.3,4) 2 CNs
(.3,4) 5 CNs
(.3,4) 10 CNs

T1

65.287
65.069
65.091
64.895
68.419
68.319
68.529
68.351

T2

63.054
63.307
62.809
63.840
60.018
59.512
60.674
61.711

Figure 10: Performance CNs NCNs together
reasonable information base decision, assuming consider available
kind real-time information (e.g., current load resources).

8. Discussion
previous sections devoted report experimental study. synthesize observations view motivation, discussed Sections 1 2.
mentioned, model general model active autonomous agents
select among several resources dynamic fashion based local information.
fact agents use local information makes possibility ecient loadbalancing questionable. However, showed adaptive load balancing based purely
local feedback feasible task. Hence, results complementary ones obtained
distributed computer systems literature. Mirchandaney Stankovic (1986) put
it: \: : : significant work illustrated possible design
learning controller able dynamically acquire relevant job scheduling information
process trial error, use information provide good performance."
study presented paper supplies complementary contribution able
show useful adaptive load balancing obtained using purely local information
framework general organizational-theoretic model.
study identified various parameters adaptive process investigated
affect eciency adaptive load balancing. part study supplies
useful guidelines systems designer may force agents work based
common selection rule. observations, although somewhat related previous observations made contexts models (Huberman & Hogg, 1988), enable demonstrate
aspects purely local adaptive behavior non-trivial model.
results disagreement selfish interest agents common
interest population sharp contrast previous work multi-agent learning
(Shoham & Tennenholtz, 1992, 1994) dynamic programming perspective
earlier work distributed systems (Bertsekas & Tsitsiklis, 1989). Moreover, explore
interaction different agent types affects system's eciency well
494

fiAdaptive Load Balancing: Study Multi-Agent Learning

individual agent's eciency. related results also interpreted guidelines
designer may partial control system.
synthesis observations teaches us adaptive load balancing
one adopts reinforcement learning perspective agents rely local
information activity. additional step performed attempts bridge
gap local view previous work adaptive load balancing communicating
agents, whose decisions may controlled learning automata means.
therefore rule possibility communication current status resources
joint decision-making, enable limited sharing previous history. show
limited communication may help, even deteriorate system eciency.
leaves us major gap previous work communication among agents
basic tool adaptive load balancing work. Much left done attempting
bridge gap. see major challenge research.

9. Related Work
Section 2 mentioned related work field distributed computer systems
(Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993; Glockner & Pasquale, 1993;
Mirchandaney et al., 1989; Zhou, 1988; Eager et al., 1986). typical example work
paper Mirchandaney Stankovic (1986). work learning automata
used order decide action taken. However, suggested algorithms heavily
rely communication information sharing among agents. sharp contrast
work. addition, differences type model use
model presented above-mentioned work work distributed computer
systems.
Applications learning algorithms load balancing problems given Mehra
(1992), Mehra Wah (1993). However, work well, agents (sites,
authors' terminology) ability communicate exchange workload values,
even though values subject uncertainty due delays. addition, differently
work, learning activity done off-line. particular, learning phase
whole system dedicated acquisition workload indices. load indices
used running phase threshold values job migration different sites.
spite differences, similarities work abovementioned work. One important similarity use learning procedures.
difference classical work parallel distributed computation (Bertsekas
& Tsitsiklis, 1989) applies numerical iterative methods solution problems
network ow parallel computing. similarities related study
division society groups. somewhat resembles work group formation
(Billard & Pasquale, 1993) distributed computer systems. information sharing
allow Section 7 similar limited communication discussed Tan (1993).
classification load-balancing problems given Ferrari (1985), work falls
category load-independent non-preemptive pure load-balancing. problems
investigate also seen sender-initiated problems, although case sender
agent (overloaded) resource.
495

fiSchaerf, Shoham, & Tennenholtz

One may wonder work differs work adaptive load balancing
Operations Research (OR) (e.g., queuing theory Bonomi, Doshi, Kaufmann, Lee, &
Kumar, 1990). Indeed, commonalities. work, individual
decisions made locally, based information obtained dynamically runtime.
cases systems constructed suciently complex interesting
results tend obtained experimentally. However, careful look relevant
literature reveals essential difference perspective topic
reinforcement-learning perspective: permits free communication within system,
thus significant element uncertainty framework. particular, issue
exploration versus exploitation, lies heart approach, completely
absent work OR.
work adaptive load balancing related topics carried also
Artificial Intelligence community (see e.g., Kosoresow, 1993; Gmytrasiewicz, Durfee, &
Wehe, 1991; Wellman, 1993). work too, however, tends based form
communication among agents, whereas case load balancing obtained purely
learning activity.
article related previous work co-learning (Shoham & Tennenholtz,
1992, 1994). framework co-learning framework multi-agent learning,
differs frameworks discussed multi-agent reinforcement learning (Narendra &
Thathachar, 1989; Tan, 1993; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994) due
fact considers case stochastic interactions among subsets agents,
purely local feedback revealed agents based interactions.
framework co-learning similar respects number dynamic frameworks
economics (Kandori, Mailath, & Rob, 1991), physics (Kinderman & Snell, 1980), computational ecologies (Huberman & Hogg, 1988), biology (Altenberg & Feldman, 1987).
study adaptive load balancing treated study co-learning.
Relevant work also literature field Learning Automata (see Narendra & Thathachar, 1989). fact, agent setting seen learning automaton. Therefore, one may hope theoretical results interconnected automata
N-player games (see e.g., El-Fattah, 1980; Abdel-Fattah, 1983; Narendra & Wheeler Jr.,
1983; Wheeler Jr. & Narendra, 1985) could imported framework. Unfortunately,
due stochastic nature job submissions (i.e., agent interactions) real-valued
(instead binary) feedback, problem fit completely theoretical
framework learning automata. Hence, results concerning optimality, convergence expediency learning rules Linear Reward-Penalty Linear Reward-Inaction,
easily adapted setting. fact use stochastic model
interaction among agents, makes work closely related above-mentioned work
co-learning. Nevertheless, work largely uenced learning automata theory
resource-selection rules closely resemble reinforcement schemes learning automata.
Last least, work related work applying organization theory management techniques field Distributed AI (Fox, 1981; Malone, 1987; Durfee, Lesser,
& Corkill, 1987). model closely related models decision-making management
organization theory (e.g., Malone, 1987) applies reinforcement learning perspective context. makes work related psychological models decision-making
(Arthur, 1994).
496

fiAdaptive Load Balancing: Study Multi-Agent Learning

10. Summary

work applies idea multi-agent reinforcement learning problem load
balancing loosely-coupled multi-agent system, agents need adapt one another well changing environment. demonstrated adaptive behavior
useful ecient load balancing context identified pair parameters
affect eciency non-trivial fashion. parameter, holding parameter
fixed, gives rise certain tradeoff, two parameters interplay non-trivial
illuminating way. also exposed illuminating results regarding heterogeneous
populations, group parasitic less adaptive agents gain exibility agents. addition, showed naive use communication may
improve, might even deteriorate, system eciency.

Acknowledgments

thank anonymous reviewers Steve Minton, whose stimulating comments helped
us improving earlier version paper.

References

Abdel-Fattah, Y. M. (1983). Stochastic automata modeling certain problems collective
behavior. IEEE Transactions Systems, Man, Cybernetics, 13 (3), 236{241.
Altenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission,
evolution modifier genes. I. reduction principle. Genetics, 117, 559{572.
Arthur, W. (1994). Inductive reasoning, bounded rationality bar problem. Tech. rep.
94-03-014 (working paper), Santa Fe Institute. Appeared also American Economic
Review 84.
Axelrod, R. (1984). Evolution Cooperation. New York: Basic Books.
Bertsekas, D., & Tsitsiklis, J. (1989). Parallel Distributed Computation: Numerical
Methods. Prentice Hall.
Billard, E., & Pasquale, J. (1993). Effects delayed communication dynamic group
formation. IEEE Transactions Systems, Man, Cybernetics, 23 (5), 1265{1275.
Blackburn, J. M. (1936). Acquisition skill: analysis learning curves. IHRB Report
No. 73.
Bond, A. H., & Gasser, L. (1988). Readings Distributed Artificial Intelligence. Ablex
Publishing Corporation.
Bonomi, F., Doshi, B., Kaufmann, J., Lee, T., & Kumar, A. (1990). case study
adaptive load balancing algorithm. Queuing Systems, 7, 23{49.
Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent cooperation among communicating problem solvers. IEEE Transactions Computers, 36, 1275{1291.
497

fiSchaerf, Shoham, & Tennenholtz

Eager, D., Lazowska, E., & Zahorjan, J. (1986). Adaptive load sharing homogeneous
distributed systems. IEEE Transactions Software Engineering, 12 (5), 662{675.
El-Fattah, Y. M. (1980). Stochastic automata modeling certain problems collective
behavior. IEEE Transactions Systems, Man, Cybernetics, 10 (6), 304{314.
Ferrari, D. (1985). study load indices load balancing schemes. Tech. rep. Ucb/CSD
86/262, Computer Science Division (EECS), Univ. California, Berkeley.
Ferrari, D., Serazzi, G., & Zeigner, A. (1983). Measurement Tuning Computer
Systems. Prentice Hall.
Fox, M. S. (1981). organizational view distributed systems. IEEE Transactions
Systems, Man, Cybernetics, 11, 70{80.
Glockner, A., & Pasquale, J. (1993). Coadaptive behavior simple distributed job
scheduling system. IEEE Transactions Systems, Man, Cybernetics, 23 (3),
902{907.
Gmytrasiewicz, P., Durfee, E., & Wehe, D. (1991). utility communication coordinating intelligent agents. Proc. 9th Nat. Conf. Artificial Intelligence
(AAAI-91), pp. 166{172.
Huberman, B. A., & Hogg, T. (1988). behavior computational ecologies. Huberman, B. A. (Ed.), Ecology Computation. Elsevier Science.
Kaelbling, L. (1993). Learning Embedded Systems. MIT Press.
Kandori, M., Mailath, G., & Rob, R. (1991). Learning, mutation long equilibria
games. Mimeo. University Pennsylvania.
Kinderman, R., & Snell, S. L. (1980). Markov Random Fields Applications.
American Mathematical Society.
Kosoresow, A. P. (1993). fast first-cut protocol agent coordination. Proc.
11th Nat. Conf. Artificial Intelligence (AAAI-93), pp. 237{242.
Kraus, S., & Wilkenfeld, J. (1991). function time cooperative negotiations.
Proc. 9th Nat. Conf. Artificial Intelligence (AAAI-91), pp. 179{184.
Lesser, V. R. (1991). retrospective view FA/C distributed problem solving. IEEE
Transactions Systems, Man, Cybernetics, 21 (6), 1347{1362.
Malone, T. W. (1987). Modeling coordination organizations markets. Management
Science, 33 (10), 1317{1332.
Mehra, P. (1992). Automated Learning Load-Balancing Strategies Distributed
Computer System. Ph.D. thesis, Department Electrical Computer Engineering,
University Illinois Urbana-Champaign.
498

fiAdaptive Load Balancing: Study Multi-Agent Learning

Mehra, P., & Wah, B. W. (1993). Population-based learning load balancing policies
distributed computer system. Proceedings Computing Aerospace 9 Conference,
AIAA, pp. 1120{1130.
Mirchandaney, R., & Stankovic, J. (1986). Using stochastic learning automata job
scheduling distributed processing systems. Journal Parallel Distributed
Computing, 3, 527{552.
Mirchandaney, R., Towsley, D., & Stankovic, J. (1989). Analysis effects delays
load sharing. IEEE Transactions Computers, 38 (11), 1513{1525.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.
Prentice Hall.
Narendra, K., & Wheeler Jr., R. M. (1983). N-player sequential stochastic game
identical payoffs. IEEE Transactions Systems, Man, Cybernetics, 13 (6), 1154{
1158.
Pulidas, S., Towsley, D., & Stankovic, J. (1988). Imbedding gradient estimators load balancing algorithms. Proceedings 8th International Conference Distributed
Computer Systems, IEEE, pp. 482{489.
Sen, S., Sekaran, M., & Hale, J. (1994). Learning coordinate without sharing information.
Proc. 12th Nat. Conf. Artificial Intelligence (AAAI-94).
Shoham, Y., & Tennenholtz, M. (1992). Emergent conventions multi-agent systems: initial experimental results observations. Proc. 3rd Int. Conf. Principles
Knowledge Representation Reasoning (KR-92), pp. 225{231.
Shoham, Y., & Tennenholtz, M. (1994). Co-learning evolution social activity.
Tech. rep. STAN-CS-TR-94-1511, Dept. Computer Science, Stanford University.
Sutton, R. (1992). Special issue reinforcement learning. Machine Learning, 8 (3{4).
Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents.
Proceedings 10th International Conference Machine Learning.
Thronkide, E. L. (1898). Animal intelligence: experimental study associative
processes animals. Psychological Monographs, 2.
Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge University.
Wellman, M. P. (1993). market-oriented programming environment application
distributed multicommodity ow problems. Journal Artificial Intelligence Research,
1, 1{23.
Wheeler Jr., R. M., & Narendra, K. (1985). Learning models decentralized decision
making. Automatica, 21 (4), 479{484.
499

fiSchaerf, Shoham, & Tennenholtz

Yanco, H., & Stein, L. (1993). adaptive communication protocol cooperating mobile robots. Animals Animats: Proceedings Second International
Conference Simulation Adaptive Behavior, pp. 478{485.
Zhou, S. (1988). trace-driven simulation study dynamic load balancing. IEEE Transactions Software Engineering, 14 (9), 1327{1341.
Zlotkin, G., & Rosenschein, J. S. (1993). domain theory task oriented negotiation.
Proc. 13th Int. Joint Conf. Artificial Intelligence (IJCAI-93), pp. 416{422.

500

fi
ff fi


!"!#%$'&)(*+*-,+.0/-(*21-/+34



BDCFEHGJILKNMPOQKSRUTWVXTYKSRUTYK[Z\A^]+_[CJ`9IZ9aUE

kmlnPomnqpsrmtQuwv

56789":;3-<2*+=>?7
# @:;(+<*-,

b CJ`dce]2GfKgAdRUGYVhZiGWZ9I2CjK

xzyz{z|~}[z} y}6x){z{9

rmtinYk;nz

F i[z} y}6xz){z{9

0F+F9@+z22FF)26)
0)++FUhL)L!
z-


; P Fm!0"@N! P ) 00P+!@LN0 PJ0N+ 0
w [ @ +!F ;[ 0@LF " !+P!0X ) )X@

J@ +f + @ +U@0JPP+F9N0@)S NP+P6P+F @@ "
) @ P+F @!@J@P%@ Pz2 F +! zPh!@@6N@)P F@Y
!F @0+ @ h @@0FN! @+! @
0f @ !JP!- U+ ) F@L! X %P@!0 -
6 +@ J )0 N!!@W! [ W@ WPF +'"@ F@ J @ h )
! P +F6 +@@-wh ) YN 0! !F )N@ !!N WN!
)X %! N!@@ PW ! N!-@@ )@ @X @@ d+
0 !! FF[!P 6F+ +@@[L ) !UN+ +![0 +P6!
@6@; !@@;PJ!@+ w"! ! 0 ; @@0U F06@+ 6@9+ @0@L!Pi!
0 @S @! ;P++@P@-!U! +@! UN!;@@!+N!2+ ! @+
@;;F![;@0 ;!- P0@N!-! NN@!;i@0%
ih ;@

ff
fi ff
fi
fi fi!""#$&%'ff(ff)
*+,+#"#
fi+$ff
- +.
- /0%'1ff2

.34$65879
-.$:ff"# ff
fi
-
fi$; $<=<=ff />+?#@().;ffA>CBD +E;-$ff +%
F fffi/>F+?+fi
-%'< , ff-/G2H+fi
fi C%'/>"#5<I$"#4:"#/G .
fi3
"#/Gfffi#@ff
fiJ+ /GD#K= +EL=+K$!ff
fi M%'/ON.K !
fi"P
fifffi$Q$Rff
-$
fi/G;#@ ff

fi
= T&
-U+%*= fffi/V5FI(!/>KP=+KM+,+-
fiK"+Wff
SXfiY3
C=ff
ff-/
DL+-
fi ZZ!/>3XfiW fffi/V[?=G+%J +
fi Z\ P
fi>fi
fi]\= \"# ffG.B45
^Hffff
fi
->P#XKF=_/?+DW E` Pfffi/G2Hfi
-
fi+
-SF"PfiD#-G$/,fffiM+?&
fi
fi:
= #%'b= Pb/>3>)Yff++D#+ MG&
fi W""#$$c>!+fiM Ed Pfffi/>5
7e;#@e+/Wfffi:"2'$fff .;Dff
'"3XfiG""#/WffX
=f= #
-.Ba
fif= b =!$[
gih?jMkJh?ljmbnfo [9p
fiq?T%1
fiff
-
38"#
fi
fic+f ff3: P
fiM%'/r= sX
fi .+P
W&
fi/,
X'+;ff+fftu C=+=cE;B$W
-V"P
fi."# />#+"#$P=+$/,fffibP= b
fi .65v;=
P
fi+3* =J/?$?3,
-Dfi+/GJ !Nw"P
3K/G(
xNw"+
fi?+%K= bx
fi .+Wff+*:ff%'
#@+/Gff-:++/,
fi J"#.+ff.+,/>+B
fi b= FX
- .+bff*yz9
fiff
fi
'3eJ D3w"#
fi!
fi
/?+."P=>=
fi T)$"P
XNw"+
-5
gin nf{Jk;nbkCl3|a} [(~
X%'f= JP
fi$Aff+?t 5 5X:eDV
- +A/G+
fi >6:
ff>"P=+ +
fi ?f.ff#:ff;D?/G %
fi ,S+
'+fffiMff
fi
fi DFt ff
X*= bP$fffi
fi ,ff
"P=ff
fi$F=
fi ff3 5
(*+*-,wfizfi@@!" 2 :! :<W+<mff8
F

2_07
)#Hfiz

#"0!" 6:+

fi

yz{z|~}a



gjM}VjMh>nfoFl3QnCkJl3|G} [ffpM .3X
fiU;= c E;fiY"#P$+$,ff>+,P<
-<<M E0"

fi>P= X
- .+?H +
ff$a
fiF
;ff"P
fifffiG
X*ffF%/ff+;"# PD-,
fi?= MX
fi .#5
v=ff
C+W% "#$WL= \ + .+!
fi] ("#$PC
fiL= .3"#D#@ ,+%JZ"2'!$
ff+ ff
- q /V= +E;$:4 , + .
fiZ3fi
fi= /"#ff?#%ffF%J.&%/>+&
fiS
+?ffVff +
fi f />;;E;#X5



l ozrzl




ZPB?
fiq"!2'$ff+ ff
fi ?=M=ff
!
"SXX>)"#ff"#$A
fi\+!
"#ff+T ffX
"+
-Vff2
/>3
-:T+=?Pff$0]%'("#?0 P$D#+
fi.+= fP=+3fi !
fi= /,
"?
' $5v;=
$$."=f ff$P$; ff-/>QX
fiBME=+F%'$+ $F+%8sX
- .+,ff+f/?+BM
fi
"#$F%'_! 2
$R D!
fiS3:$= +E%$+P $4+%= QX
fi .Tff'+Y"+, $!1#*$"#
fi< + .+!
fiJ#+ +
fi$6:
+fW*5
E;Bbff#fiMff/>3
fi 2-
-D)ffD<Sfi
- = /%9ff+Y + .+
-*:$+s
4P= #%
"#/Gfffi/WD.w[Q
fiM +
'ff$T?"#/G/GVff+!%P/E;
fi=fEc=ff
"=\ ,"+Z+3fiffUYV"#/G
= \S+!
fi? $!D.
fi0"= /W$f+0 + .+!
fi`!.+
fi$:WE;#Xs?#@(fffi
fi0=
+#"#= Cff
S1 #N.M%<= Y"!2'$\+ D"=Gff+ ff
- 5bI$"#
-bG+ 5z

"#MP= a9* \ M+/G/W4:13ff+ XKLb+/, =+/G+
<-$:<$ff_ /?
&
fi ? c%./GE;B4:(ZI$"#!
fiqe5XJ"P=+."#!
fiU$; $"P
!#fif= J)ff
3 #N.M+%<ff'+
+ .
fi?.ff+f .
fi*5
v;=ff
'+)b $ff.M+Z3fi !
fi= /V:wuHP= V(!/>+
fi"sff+q + .(K%Tff + 2
.+
-P=+C
G 4:;"#/Gff-:_u />
"+5LI ff $G/W$+,=+W= f Gff'+

+#+DP$\V+!
&%'V= , DS :*"#/GfffiP $b/G$b=+bP= Gff+ CE;
XX_3-E6(cN
+fi !
fiZffZ
X% ?#@ff
.WH ff+.fi$s+%= ?X
fi #+Vff'+ PS
ff$D= >P
fi6.:K+
/>+!
"P
fiHW/G$+F=+P= 3-
fiP= /#@ fffi$;P= J"#b+%8 + .+
-F 2'$ff +fffi
'
fiV= $:
fi;E;
XX* b"#&
'ffT+V + #+
fi?/GPT=f"#+.5
/>
"P
fi&\
,=
"PB
fi$, )\ .+ff:_]%'GE;qP$579
fi.$:Q=
+ F#+$
-C"#+%4
fi"#/GfffiMff+5 (G "P=W
fi"#/GfffiPff+>"G#@(W
fiDPJ+
#@() ff
3;D /,C+%T"#/Gfffi!
fiK />
"P
fi&V$Rff
fi$s=+JP= > + .
fiZ3fi
fi= /
"#&
ff;E;b
fi"#/Gfffiff'+F=+=+cf b"#/Gfffi
-*:E= P$;"#/Gfffi $P
$Rff
fi$M=+b\ff
SQ"#/Wfffi
fi\)"#
ff$45fI$"#4:*ff+] + .+!
fiVP$Rff
fiP$
G"#/,ff
fi+!
fi?+%1#"#
fi ,
fi;ff'+ ff
fi >ff$"P

fic "P= +
"#J+?#ff
fi G+%ff+V6:
ff
fi
fi ?+%KS+
'+fffi$;E;
-=ff
fif= ,"#
fiVP"= />#.:4;E;#X8/>+B
fi G Eff$"P
&
fi65MI(P/G2
+
"P
- ,$Rff
fi$;P=+TGff$"P

fi*:ff"#JP."#$4:ff MJ"#&
ff$+ D3
fi*5
;%'.+/GE;PB,%ff+ ff
fi WDV .+
fi+?
F!$f E;, /,
'$;=3
- YP>ff>E
fi=
= b+ b%QP$?ff+c+?= +E= f+b/>+ff
- ff+$4[
X
fi #+Jff'+G;";
FP$G<C"#/GfffiPT+?"#&
PDKff,%_+-
fi bP=
-
fffi/V5fv;=ff
sff+"#ff.3
-M= !J+.ff
fi ffb=+,+fi$VP= >
-b fffi/
3- fE;
fi=L
fi!
fi3_"#!.3
fiff.b+]ffff3"#Z
-ff%P/>+
fiqP=+J$"#.ZEc=DZP=
PYZ#ff
fi DC+ $+,= P5 fffi
fi ZV"fq E ff-/N#b
fiff+-$
3
- = AX
fi .Lff+/?+."P=u=
fiff
fi!
3T+ D3b"#
fi
fiG+%JP= "# PD
fffi/V:4? ("#$PM=+M (ff"#$C>"#&
ffM
fi"#/GfffiPYff'+*5bv;= G + .+!
fi
"#$/G .F?"#/GfffibP=ff
Fff+*5

g

.W$#;'b6PGG+bSP'-!zY#'$'G.H!?SP'-!z, #$''H!3f.-X6*3f!
'6'_P$6-.+ ''$*#*.3''H'*X#44'PH.K'3<#!fi
3$

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

v = + .+
-G "#$;"#&
!.K+%KY!.+ +.W;+%1ff+ 2'P#N /GD;).+.;HP=
g ;
=+_ f;+?"#.S
fiD#K,= Mff+4KfffiF= J+ff
xX
fiH,,P."#<P= b#N /Gff.
/>ffbE= = MX
fi .Gff+fEF!
fi +
fi3xW .+$45
J
fiEdP= .3Kff+ ff
fi f fffi/T?$+."P=\= =q?
fi$"#$ . =V+%;+


ff+65v;= f #+ =*yzCff, $ff.,= fffffXff+u\
fi#b-$$$JP $ff,Nff
= $ff+5
pT #+
fi?ff+ ff
- !.+.C+,= ?DY%T= f . =i+]$+#"= $C%'G (ff\&ff+4b=+
+!
&N$M= G D3 5W^HJ .+$M= ? . =qffZ""#$&
-#fi\#Nff
fi "##3
fiff
fi ;= Gff'+*5
v;= P
fi+3 =%+> + .+
- 2'$sff+ $:G= = K=+4:P <>+ff
fiP.+
ff,
fiZ= , #+ =*:1+= ? + b) +
fiJ!$+."P=ff
fi ?%P/P=+b+
-D$5,^&b/,b>fffiG
$+."P=Vff+EVP= b .+ =AX
fiBb, .
fibff TSG/,)Y+ff-b>$+#"="BffE+.
= =>= b .+ =?DGP."#
- Y"#P.3
fiff.: (ff"P
- G/Gb+P."#<ff'+5 T"#/Gff-
+u!(P/>+
"? + #+
fiuSfi
X= /
W+fffiVP$+."P=LL fff
-u= .+ =LE;
-=
"#&
ff!
fi >+ff> ffb/GPb=+f"#5
ZF=3K
fi/Wfffi/Gff$b K3fi
X= / $
fiW</G/WJ)
,,Tc^HE;B .+&
fi9+b$!$

fib!.3< fffi/ff/>3
-5 @
-/GD#3_
fi$b"#/G+G J3fi !
fi= /rVV&
-/,
X+
#*$:; xK0 b+/, =+/G
Tfi3:b$ff#5 G$!fffi.,= +EZ /?+
"G!$ff
%P/ff+fs%T"#.3
-&
-/GfffiJ+? ff+ fffi/"P$65
E;PBGV+]/>+B$F= s%Xfi$E
fi Y"#ff
-
fi[

K3fi
X= /"+ $9= F$"#<%(= Fff+ 2H .+
fiT "#$9E;
fi=ff
-J+b#@(P/G#fi

fi/Gfffi%'.+/GE;PBw5 *"P=b
fi
*+/GfffiK;%'/>S3fi &
:$b +
ff$*F%'.+/GE;PB
E
fi=VEc=ff
"=>+3fi+PG= Jff/?3
fi 2-
fiff)ffD,3fi !
fi= />;x
fiB,= ? XK!(P/
b+/, =+/G+
Kcfi$:K3ff.:)+ff+3-DU,ff/>3
fi 2Hff)ff b $ff.+!
fi
f + .+!
fif.+P +
fi$5
g

g TM= c%#+/GE;B,s
fiD$
fi M*9 (yzF#+&%'/>+!
3w+ D"=?Jff'+ + 2
#+
fi*:++G!= SEL= +E0*9 (yzQ3
fi<!.+
fi$1"#ff,M ff$GPJ+qK!$+."P= 2H"#DP+
= !

"65
g Y+3-DU,= C.ff+]E;qff'+V #+
fi\+V + .+
-*:w"P=+."#
fiU#
fi W=

fi/,
XH
-HG$Rff
fi$?ff?= Jfff
fi+3*
- b, (ff"#,$ff *5
g YPM\/Gff
fi!
"3K#@()
fi/Gff.M+\ff/G#+C%?&
fi/Gff-a"P'c+%< fffi/?
f!(P/>+
"s#+!
fi=ff
fi\) E;"#/G .+
-V
-/GG+\= >
fi/,
X
X f)E_=

- fffi/r+?= MX
fi #+Gfff= .+
fi>3fi
fi= /
fi;E;
fiP=*5
v = ,+)b "#$ Jc%'+Xfi+ET6[;E_CN.!T
fiE
fiME;PB?
fi\ff+ ff
fi fffq + !
fi
;
3
-
fi G
fic+fi
-5<c#@(ME;b
fiE= sfi$2H"#/G/,
fi/WD; .
fiMff+ ff
fi
3fi !
fi= /ZEc=ff
"=]+
b!$4:*
-iff+
- qA
fiDP(ff"P
- V/>+ff\+%T+<yzC +."# P$5
I$"#
fi?,= f#@(ff'3
fiF= bff.3
XQ+%1 .+
fi>3fi
fi= /V59^H\I$"#
-CE;M +TP=+
+\
! 4:"#/Gff-J+f />
"+5
I+
fi"#f= V!$L% + .
fiff)W]= VRSX
fiZ+%TP= fff+L!
fi$%/P=
X
fi .+P:
-"Vs%;G)%/ .
fiMff+ ff
fi W=+V+P/G G + M+?
fi+ 2

+sX
fi .>ff'+*
-ZI$"#
fiqWE_,+3fiffUJP=ff
P.ff++\3!>
"#M/Wb
fiff$!
fi

fiDP."#
fi<)E_\= ,3fi
X= />Q% .+
fi+Aff+AN
fi 5v= f
fiI$"#
fiJE;
w6$bD'F$$P# ff
fiffff #<X#)X#'-.M.JP-$FX'K'.3'<$K#Xff
3

fi

yz{z|~}a



= +EO= SE .+&%'/?+
fi+31ff'+ .M"=\MT+/W/G4yzs.$ff<*9 /"+V)G++2
fiffU$G
fi J Q%#+/GE;B45KI$"#
-s.K </Gff
fi!
"3w
-$5 %F
fiE;
fi b#'+$
E;B?I$"#
-(.:)I$"#
fi\,
"#!$< P$<+?)D$;R $
fiQ%'F% P TP$$+."P=*5
"#<@w%$
!

i&

(';)#*"

,+



.-

v;=
ff$C+%ff+ ff
fi ,ff> + .
fiG=<)f
fi?= cX
fi#+ ;%';/>+ffG$.: +a
fi>/?+D

X*ffF%P/>5^Hf=ff
'_!$"#
fifE;M
-E=ff
FE;BG !
fi /:ff
fi ,,/G
fi++b+? ;
fiff
#$"#
-J M"# ffE;BGV<5
v= b&
"c
ff$a=ff
fiff+ ff
- GDf + #+
fiHM&
fi/,
X'$;E;BW
fiV"!2'$fff'+ ff
fi :
.&%/>+
XSff+ ff
fi :;ff+ ff
fi CD?+fi
-Gff3)4
FY!+fiTs E ff-/fff.+
!
fi
- ,%P//G/GW ff-/==f)Z+fi$? P
fi&fi: = ] (_ +
- G=
+f!+fi
fi>,= C E ff-/V5
v= V** Z / T+/W/G4:$ ffM
,Z"2'$Lff G=+G+-$C fffi/>Y
fi
= Vff/>S
fi%YIU$"P=DE+u"#ffB
fi 5`= L
fiu D3\ ff"#q\
!=LE;
fi=]+
'"#ff+
)
fi$:ff9* bN.F
fi$Q1
024365 7598:0 36;;+ffY Pfffi/>Q_"#
/
'"#.K=</,
fi =D<+!
;%'/=
E D3 :+]$C=+,+3fi &
s=
<ff;>36<5 ;?>;C%//G/GZq"
+f+fi !
fi]H#x
fi
ff+4.5Kv;= b#X
fi Mff+W
F= ?/>+ff
fi ff$Gff>%
@BAC 5 DE5 2GFbSfi
-= /=+;
-$<PY+!
&%'
+ff Er D3'G+LP3
fiG fffi/>,=>
L +!
?
fiL= #X
- q"#+!
fi5^H?=
#@($"# P$= bff'+*: +W
X%K#@($"#
-f$fffi#K
-?%3
xfi b1
<ff;H8:0 5 <_Sfi
-= /+SfiDU$= M%S
Xfi
+W$F= TP$fffi;+%*=++3-(&
'C
fi/G +P=
fiff#@W%F=ff
;+-
fi>!J=+Q
fi;E;
XX
C
fi$W
fiV&
fi
fi+FE= b
-_E
XX4%S
X1+ D3
fi*5
9* Z ff$!$G\E;
fff#+ ?+% Pfffi/>s
fi/G).D,Z"2'$Lff ff
fi [?= +E
+ff
"P
fi+PV ff-/>:= +ErL!
fiq]+-
fi%/ = ]"X
fi .:;= +Eu + >
/G
X%f++f+-
fi*:f= +EG!J#@ $"#
fiA%3
xfi b,
-/G +J $R ffTP
fiSS5
J+u /r
fi/>
XfiV ffP$$M= , + .+!
fiV Pfffi/V:4+
fiI$"#
fi],E;Y!Y
%#+/GE;BGPa3fiffUJ*9 (yz/G(
xNw"+
fiV!.+
fi$
fiV!/GJff.3
x 5
v;= c
JLKwc(!/ fiP/>+*:ff$(w"#ff%'ff.*P= < ff-/d+%K +
fi<ff+ ff
- :zc
3, ff$!$<P= fffi/+.
% <MG2ONP365 @B;T + #+
fi,Cff+W%S
Xfi 5Q
JLKwC+ D"P= $Kff'+
+ .
fiZE
fi=\"#/,ff
fi+!
fi\+%T."#
"3_"#ff+]&
fi
fiZ/?+."P=ff
fi 5f0= LVff'+
%S
Xfi c
ff$"#P$?
fiF
"P'&
XN$f;#
fi W#
fi= TC%H3
Xx
fi , $"#
fi
-*: C%S
XX
fi , ."#/G:
"%9
x!
fi J ff3:+;JPG <%.ff$5^H
fi JP= T)$"#.F+%K
JLKwM=<ff$3*E;
fi=

fi"#/Gff-J+W
fi"#P$"#;BD +E;fi$ff :DP= b ./VyzF/>3
fi?3
-!.+ ,
fiff+fi$Fff'"P
fi
,%H3
X-$>ff'+qPVE;
fiP=V C=+/,
- =DM"=ff
-J= ,+/GC ff5b
JQKw,$M>!/>+ff
"
E;PBG, $ffT+!."#
-"P'$;%8"#
fi<P=+T"P=ff
fiM= J+/Wb D!GHE3fiB
X
+fff
fi
fi >+Pb=?
-.+"#$;%8#+.+
Xf"#
-:%_#@+/Gff-+.5
v;=
RTK(
Uw) Kw, /PI+
fi/G/G6:*$(1
'V
36<ff0 2OWXDA <@B0 365XA240 Y8(YZ02424;><.5F0=ff
Xfib= J
xP2
"#GE;Z?.+&%'/?+
X31ff'+ J+Z?"2'!$\ff+ b=M bZ $"P
'#fi
ff#N $4:8?/?+
xP"#>"#"#s= SEP= YE;? ff$s+%_ff'+ .M b= ?.+!
fi G
fiD
%Fff+f + .
fi*5Q_$ff2'!$> /?K ;=ff
Qff+?
'b!
fiS34+%1C_!+fi
fi,%P/
MX
fi #+:+ [
R*TQ(
Uw) KwT"#/Yff
- $</>SXff+W%#+ /Gff.9%'<
X*ffH= )#%ffxfiC
fiffffff
$"##+%<P= Y"# ff fffi/V[
5 RTK(
Uw) Kw,
X*.%/9* ,
fiVE;,= ME$ [N#T%Q3x :
R*TQ(
Uw6 K4GffD$c M%'/=
0 2\365X7>5]80 365XA23fi &
;\= Cff+*:)
fi ?G
'ffD
x%? ff-
#J)#%?X
fi .\
fi+3 5\^&$
fiG""# #JP= ?%"#,=J= fP
fi$ff+]E;
XX;
/w$E;$4:*+\"# ff.M
fi#S
fib=
!
"M?+."P=V
-$5J9* :4\= G= M=+4: /G$
3$

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

=+C= ?
fi$X
fi .VffE;
xX<f\"PfiDG =ZN,= G E ff-/ !V=+sX
fi2
fi,C q .+
fiVE;
Xx<G $"#$P+5I$"#4:/,"=Z%_= 1
RTQ
U46 KwWE;BA
Jff$\
ff#fiff
fi >Gc+%13
-#+.Q%'TR+ff
XN$f+?/GP
"MS
+fffi$65
v= G/>3
fi
ff$f)=ff
fi]= >0 /!+.+$
fib%'/P= >= P> />s/GD!
fi $
+)S[G=+,= ? "#$,+%Mff+^
0 C 08360365XA 2
,A%3
-fiZ&
fi/Gff->#@ &
fi\= f P("#$C+%
ff+%
F ;2\;<ff0 365 25 ;C"#$R "#TE;T"+? /GM=+KP= T3fi
fi= /=K .+$8X
fi .
ff+ t`+f= ,"#P J+%K= D!Jff+ tL
P= J+/G,;= , + .+!
fiV3fi
fi= /+A=
ff+V!"# $
fi; .$5F^HVP= Y+Z
fiEJ:eff+f .
fi?
8 MG$"P
'38"C+%Kff+
+ #+
fiH
-fE=ff
"P=f=
; W
fi$f"# Pb,.@ fffi+
-.5
v;E;Gff
-$"#$+%KE;Bfff#fi)$\+= ,+/GC
fi/GJM+Lff
fi/,
X /G
-[F=
XKu / b+/, =+/G
cfi$:3ffT+]P= V
_STQ`8) ac bu / e dK#fiD!:<$ :
$+e.5
v;= b/?3
fif
X*"#J) E;V]+\ xK?
;= b ff-
fi Wff+ ff
fi >Sfi
-= /V[1
$GZ"#.3
-D2')D
X \$"P= ff
R V&
fi/,
X'+J]<= />+*yzW.$fff
b(gfQ
ha/G(
xN$]D
~\" X-$+V
iDffffX
fiM.$ +.:E= $M xKV$c,S+!
+ff_
% jTQjkIS) jZ v+:4$(.:ff
=ff
fi.+#"=ff
"Sff'+ $5;I$"#
-VG"#/G$;= $bE;,ff+ ._
fiV/GJff#3
X 5
v;= l
_STL`K6 ac bq /3!Y#+B$;,$+."P= 2'
fiff$?+ D"=?,ff+ ff
fi )51^&T
X*.Q%'/
+
fiL= VP+fiV]"Vff'$ C
fiLP= V ff-/G2H+fi
fi Z P("#$65 X
- .+]ff+0 "+J
-u
.+ %P/>+
X3f"2'$ff2'ff+ ff
fi %'.+/GE;BL$>+fi
-LLZ
-G ff-/
3fi \E;
-=Z /W/>+\+%bE=+C E fffi/?b
-YE;ff'Z)Vq!ff
fi.+fffif+-
fi\%'$:Q ,
fi
"#ff.3
fiWX
-X
fiff%/>+
X0+ V=
8(<PA>7;nWWGP=+f .+P$uP= +fi !
fi*p
5 oM
fiS
fi3
+3fi :fP= b= ;=+4:P$; .D
'31ff$"#
fi
-F+%1= !
C ;75cW5XA 2OWK=+;$fffiP$W
fi
= Y!+fi
fi*5;^H\+
'"#ff+$
: dK#-DyzM /r$"#. c/GPb
fiff%'/>
XV+M$"=\"=
"#J)+
fiff
=+ffff$J<[YX
M+%;%H3
Xfi$\3fiP+
fi$:)%b#@e+/Wfffi5fv;= G#
fi,#*$"#
fi $PM+%=
E_?+ D"P= $b/>MP>=ff
fi YV= ,#@ DbP>E=ff
'"=+fff'+ ff
fi Vff$"P
&
-b D$
G= bff'+;= />!#fi$#<"+b ff.PD V+A#@(ff-+
fi$a
fiV&
fi/a
X+;ff+ ff
- Gff
ff$5
^&> /G/>:+E;"#&
ff< KE_Bb>fPbT"#/Wfffi/Gff.+CT/WDK#@ff

fi bE;PBM
fi
.+ %P/>+
X3 ;"2'$Wff+ ff
- 5Kv;= ;+PK=<"#"#D#+$W>ff#fiff
fi J=
'2

"3xfif#*$"#
fiC fffi/fi.%b+
"#ff'+ff/>3
fi5a_2'!$ff2'ff+ ff
fi f$!$+."P=q=
3,#@ fffi$?P= b fffi/%8= +E,P
fib"$F%'/P= bff+?X
fi #+tL
fi>
"#ff_P=
fffi/+%*= +E0b
fiff#@?= /#$"#!
fi#fi5+<:ff?= T= ;=+4:
<,ff/>S
fi 2-
fiffffff
3fi !
fi= /V: +fffD$c ff$;= bP
fi+3*;
fiff#@ff
fi G fffi/>F
fiVDfffVE35
v;= ,/>3
-V $"#
fi$;+%_P=ff
E;PBV+G#+_P>#@ fffi,=
ff$?=+Mff+Z .+
fi+A

%3
-fi/,
- 4 $ff.+
-3D+JSfi
X= /a
"*++
D4+% = K&
'" fffi/%(ffb .+
-*:
ffKG +
ffM #X
fi/,
-+G
ff"#JP=+;=ff
;
fiE+%ff+V + #+
fiW
F/Gff
fi!
"3XfiW
fffi:
+Lff qffT\ +
ff?V= f"#/G/, ff
fiZ+
fi/Gfffi/WD.
fiZ%ci3-
fiP= /=+CE;
XX3Xfi+E
#$"#!
fiM fffi/!+fi.FWJ ff
xfi;$ff=ff
F
ff$e5
ZC SE) +
fi\= Gff#fi /GDM+%< M%'.+/WE_B>E
fi=qfff$P"#
fi
fiV+%<= , ff-
fi
%./GE;B,%'; #fif #+
fiMff+ ff
- 5
r;%s "

" )#*"


!+



.-Lt

uv"^wLxVy{z


+|-

u.}

I+
fi"#>P= 03fi
fi= /r
C+]#@(
fiZ+%T+!
3fi2'+.ff$:"##3
fiff2'D!
fi :)fi$Y"#/W2
/
fi/Gffb .
fi,ff+ ff
- q3fi
X= /V:4E;G) +
fiZff\ $ff
fi VP= G .+!
fiZ3fi
fi= /
,

fi.#X%5,+E;$:4E;GffVV
fi f= G #+
fi+\+%_P= >+u /V:*
fiZ= , "#$
fiDP2
3$

fi

yz{z|~}a



ff"#?/>+ffV+%= > .f"#P $J+q% "#
-b $ff$f
-/Gfffi/Gffb= ,%'ffX_ + #+
fi
3fi
fi= /V5 b$+/WDc
c
fi#%-t`,#E= ? ~\" Xfi$PTp
i;DffffX
fi3:1$ (
~_
#4:4$+D(1%';/GPJff.3
X 5


rzlr

k~lklk nzv

0 7>365XA 2a
GP"= />
"M $!D.
fi?+%<+f#+T3+3
X+ff-T,= Cff+ $5 \"#
fi
=
"#&
'.b%c2\0 @B;D:KV!Y%8(<P;7A 24C 5 365 2WD:+0 CCY5W3':KC ;>YZ;>36;Y5W3.:K+]V!Y%5X2\C 5 2GF
7A 2W36<ff0 5X2\3 WD5v= fN.C%' >+P
;8(<P;nWW5 2WJP=+G"+`"#ff.3
-
?>0 <5 0YZ;nWD5]ZV!VR $!
fi
/>+PB(cW
ffD!
X%f++
fffi$
: ?%'c
fi.+"#
5 ~Q
fi
fi V"#.3
-D.c+J!$VPG
fi
'"+,=+
?+!
"#ff+MS+!
+fffiG"+ b)>) \V?
"#ff"#!.+ffbbV/WY= bS+!
+fffi5
cM
+V"#
fiV"#P$
fi GGG&
-/Gfffibfffi "B E;'B
:>#+$[
4 6 k
:
G ( nOG (G
6 G

G ( nOG (
GO

G ( nOG (
OO
G n p GnG ( nG ( (
^GG( GG(G

W36;H8w5^H#+D!
+
X b+Y"#!
fib
-Dfi$
C
fi.+"#F+%Y"#
-b
9
-$
fiDPMffYK
.+K +
fi
fi , ff
R J+/W$;G= b++
fffi$F
fif= J"#
fi*: + ff<&
- ff
fi ,= JVa ff
R

fiff#@]
fiP= fff+*:<Zff+L"+u"#ff.3
-Z/G?=+] ?
fi!.+"#?+%T= V/G"#
fi*5

P= #%Y+A
fi."#J+%_+\"#
fi?
fi$
fiff>Wff+VE
fi=qf
fiff#@V=T ff
R #fi

ffff
XN$F
fi35
% 7A 2W36<ff0 5X2\3 WD:3E=ff
"P=G#
fi= <"#!.3
fi,= ;#ff<+%4 E;b8
fi
ff+>3'"#D.S
fiKJK
= ;ff+,<"#!.3
fiC= ;ff
fi
fi D%wS
+fffi$9
-YP= !5 ,#ff
fi J"#P.3
fiff*.+B$=
%
/ \S:+E= {
44+%
__P:D+s
fi
'"+$9=+K= ,E;
-=b
fiff#!
@ 4/,K ""#
#%';= ,E;
-=b
fiff#
@ 45 ff
-
fi J"##3
fiff*
9+%4= F%P/ ( $ w;
( $ .:+Ec=
(
'T?++
+ff-Y+ )$+
fi W
fiZ/GGPV
-V= ,ff+Z+m
$
c#
fi= JWS+
'+fffiJJ"#.+ff
+ $
fi ,
fifP= bff+*5 /
ZZ3'i .+0"#.S
fiD?E;
fiP=$"##u%YE=ffL
fifEWff"#$
fi0= \ff*5
v;= #%'fff+*yz,"#P.3
fiff.M
'J"#SXfiZ\,+%T3
fi#J%T= W%P
/ [+v
: GE= V

,
#
fi= Tf.ff
fi W;ff
fi
fi >"#!.3
fiff$: +


<P;0WA 2G +.,P"# , ff#N $)#fi+Eb.5
v;= CN3_"#/G ffJ%?ff+
bf!b+%"+P3fY5X2OW :$"P=Z+%;= C%/ :*Ec=
4<+J+J!:4+
+f#@ $&
-*5_v= bX
fi B?$"## = M%H"#P=+ C D!J+%
49

fif= bff+?
'<PG/>+BP : E=
;, $"#
-
fi?+%S5K^%Q,ff'+V"#ff.3
fi;sX
fi B

-_/,M3G"#ff.3
fi?= C.ff
fi % ^ 5

ff+V"#&
'.F+%<G+%<6:(W+%<"##3
fiff.:DVGc+%*X
fi B 5
Lc*
: ;
: ( bS
5 0
'JV!J+%;#@( $P&
fiCff$"#
fiff
fi
8Y0 242\5X2Fl8<ffAYZ;>@
J?P
fifffiV
= V fffi/Vyzs
fiff
fi!
3T"#
fi
-
:
GZW+%#@ $
fi?ff$"#
fiff
fi = V fffi/Vyz, D3 :
+
'b= fJ%c3+3
X+ff-a"#
fi5?Zf! /GG=+l
(
C3S3
X'+fffi,VP=
3fi !
fi= / , +fi3*S+
'+fffi5


49$6S>fi|$S&TPP-#$'-3-#!'F.H!3*$$;.3''H'ffe, 4
!*';'3ffV
4
3.HP-#b$'cM'$K''*P'$KSP+

3

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

c#@ E;J#@ fffi+
fi?.+ +#? $ff.+
-3*
"PB>+"#ffTWff+ ff
fi W fffi/P>
ff+fff? ff
X
fi >aff+f=+"#ff.3
-
5J?E;
fiP=>+/W5 245 365X0Y:
fiff#@ :ff=3
fi J , $"#
-
fiF ff#ficX
!$:+ E;
fiP=>+
WX
"#

fi ,+%K= b fffi/VyzF
-ff
fi
3*"#
fi
-
:
e5J?E;
fiP=>+/WFA0 Y:+
-ff#@u:ffE;
fiP=G $"#
fi!
fi;"#&
!
fi J%P= ff3w#@ $&
-
<: c/G +fff#fiMx
.:
qe5;= J&
fi +-T#ff
fi >"#!.3
fiff{
4

^?:

)5; GS+!
+fffi2'ff
fi
- a"#.3
-D.6:
e5; ,X
fi B 5
fffV/,b"#D#3
fiqfi$TP= JE;PT+= ,#ff
fi :4+fE;G"3X= ,ff+
E;
fiP=ffffiG=ff
'"#P b= !24MY Y8Y0 25
vE_c
fiff$!
fi )
fi$%wMff+G+Q
fi.K+
% A8:;28<ff;7A 2\C 5 365XA 2OW_+s
fi.K+L
% 3 G<ff;03 N
;2\;C%Y5X2OW5Kv;= b%'/GF
= Yc+%8#@ $
fi;=T+ $;
fiqDf*yz; P$"#
fi
fif
=3b >"+S9! cE;
fi=ff
fi?= bff* = M+PF
F= J+%K#@ ffX
"P
fi"+SP#+
fi=ff
fi
=+b/,
- =DMWDffXx
XN$D= Jc
-qP= Yff'+*5Y7e/>SXfiV+\)"#
fi
-f
fi]Gff*:
.+P$^
OS:4
bf!
b
fi\= Wff+\=+b=M P$"#
fi
fi
W:1+A%'bE=ff
"P=\= ,
'M
X
fi BW
fif= bfff+%8P= M%P/\ %'T+ff!m45
X
fi B?+%<= M%'/\
'3 G<ff;036;24;C;;
fi\"b= Pb
= bm4Q
fif= Jff'+
"P=V=+
5;P= Yff'+*yzc.ff
fi V"#.3
-D.cE;ff3Xfi+E4;f)G.ffP$qS%l4;+\#%'%+:

5{\=JAD!."#
fi
-LH#
fiP= = Yff#fi+M=+C= >ff'+*yzb++
fffi2'ff
fi
fi \"# 2
!.3
fiff.QE_fff3Xfi+EG ff
X%'>E
fi=G5
ff+\E;
fi= V)\ $"#
fi
-b+ V= P$+ $AX
fi B c
M"3Xfi$\WA YMG365 2ZPf=
"P
+P$Gff ff
fi G ff-/V5
79
fiSXfi]E_A
fiff ff"#V=
<ff;0WA 2] +.]"#P : $"#$P+L%'? .+!
fifff+ ff
-
J$!D
'3<%'J + #+
fi*5 Pq!
fi/G>fP*:*x
fi B\Y"#!.3
fiffM
b ff$\fff+]+
"P
+P$1
<ff;0WA 2b$"##
fi. PD5 $!>"#&
'.+%4 E;M+.6[<+KbD/,)+$"#.
-
E=ff>= b"#P.3
fiff;E_ ff$H#
fi= ;L
UkU( n b :)Dn bDL
\IS-+1:
TkbD\
b.:+f(8#
-=
X
fi B4: *: ;= P$+F
fif= bff+?
'ffD
x%
fi ,= b+;+%= bff+f)#
fi G3
-$45;I$"#
fiV
q 5

"#!$;$F
fif/GPJff.3
X 5
$4#_K+#QS3w#$'-'6-.F'SP* )'$#T'3'!P'S >Kzff4+*.c'L9z'(9X'!z-
'$$''$K4 e
$' *$'# kA
X'.]+&$F''$L*
3f>*
wff$'($4$|S6-.P$#$'c'#
9z'' 3 4.'6z'Q' & +43 'HP'b'_3'$'<'''P-&-#w[ KP<3SP_+--D 4
X#1

6'$'-.







3




fi

fiff
k



n

{



rmt9ti!t

n

fi


l

yz{z|~}a





v;= , .
fibff ff
fi Sfi
-= /
c$A\= b
ff$?%_.+!
WE;
fi== ,DffXff+Z+
""#$
fi#fi
<P;2\5X2FM
fiMD\"= ffD&
- ,
/w3EH)Z"#
fi
fiVb= P$+ $AX
fi B)q
fi
EdP:ffX
fi B : "#.3
-D.FaN @>
fi35<v;= J3fi
fi= //,
-+$;#
fiP= _Ec= qG"#/Wfffi
ff+s
)%' W ""#$w9E= G3XDP&
fifffiF#N /GffK
fi=$K,#@(=$W'%H3
Xfi +#5
F&
ff<Mff ff
fi b fffi/E;
fi=C
fiff
-
3("#
fi
fi
>k( , D3
59ZTP /G
= = <P= +;=K= !<+%1"#
-K3S3
X'+fffi;JP= ff+ $
: ( :+
'N @ $45KZ
+Eff#N JJP 2-fi#4%' "#
fi*
: (
w:E=ff
"P=G
fiff
-
3X
XU$F= b$+."P=f+f"3XF
%' "#
fif=%'/?<P= J#N /Gff "#$5



tkl { rmt ntmn:rzl omnfi !k( #" G %$ [<8+?LD0 5 YZM<ff;
&\[<P"#;= JffffX*ff+?%'/'>k( +(
nPlk0t*) k :G (
&



) n k G W!$+."P= $K= =>P= "#T+%+
Sff+Q%_,+fi !
fiGff*:D
fi
= q!$+."P=L=
fiU]
-DP3Xfi:K$"=L
-/GV"= ffD&
- \ff+uu"3Xx
fi +) n *:KE=ff
"P=
"= ffD$c+?3
fi#G&
fi fi[
/w3E0
-f=+ff*5


tkl h n-,Wt~n ntl w/. & G $ [<>QD0 5 YMG<ff;
0::( 2143
& /5
4. nomn:6

0 ::( ,
';/G l nt= nlk t,D0 5 YMG<ff;
7?
^
1 #fi$"#Tf#fi/Gff;%P/80 (
oM#fi9?
%'/8 0 :(
W

Gfi
fil ntnPlk0t:
nzvn> fSX*#fi/Gff.;+%#) (
M;0:




q








i;#Nff
- aCff"#
.Q+%1E;J+#[1#-$"#
fi G{
/w3E
fi?= Tff'+V+>? $"#
fi
-
JP= $+ $x
fi B#:*= Z .+
fi \3X<)D&
fiff->"#$"#!
fiM= B/w3EJ5>v= #-$"#
fi\+%
E=ff
"P
= /w3EZ"#P$"#Y $] Y)f$"#&
'ff$4:< ,= ?/>+
fiEc=ff
"=
fiC
,"#P$"#$
/,
fi =ff;=3b,:)E=ff
"P=?
;E=fff3X*DP&
fifffiJ"#P$"#
fi;+PJ ff$f,= ,$+."P=?%'ff
fi$5



tkl h n-,Wt~n { rmt< $ [<)
!_%8ff
0 7 1AI#fi$"#Tl/w3E%'/8
nPlk0t:
= >0 @L? 0 " (



<P$"#
fi Gl/w3Ed+/G D.F,$!+fi
fi G+V)V"#
fi
-f$!+fi
fi G,= $+3[


q



tkl tBA nl
CrD(E0 " $ [<)
!+%1ff

0
+f)V $"#
fi
fiXl nt
nPlk0t*) G GF 0 " (
nzvn1nl 0t*) G -
H 0 "
3


fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

G)"#
fi
fi?"+Gb )$W#
fi= FDG"P= DD
fi Y+W#@D
!
fi J?=+<.Q=
)D&
fi!
fi?ff
- >, EdPf=+Tffff$[



tkl h nv omn | . ntK:J L " $ [<)
!+%Kff+
C$"P=V L "# PD-G
fi(X
L 8"VC.ff$?)#% L S: + L K G"#
-
fif ff
X%'
fi ,E;
fi=
J l nt
"J L "
n:l L G4G L "
C$"P=V"#
fi%W
fi ( E= DJ X
"#ff.3
-_W"#
fi
fi> ff
X%'
fi GE;
fiP= J
LM :-N T[ G L " (


"
" L " N
n:l L G4G LM
J
nl 0tV= sX
;+%ff+"#+xfi$"#$f+;x
fi $qG+Ve5


q







v= G%' "#
fi L Z.+B$J+"#
fiZffi
fi .:/>B$J\"#ff\+%T= Gff'+*:

fi#+D!
+$;= ,"#
fi?
-DfGP*:w+\
fiPG= ,ff+E;
fi=f= C$RD
-$f#ff
fi f+
ff
fi
fi >"#.3
-D.65*^HM P;P=f= b E;fif ff$V!q+?P= b E;fif"#ff
fi$fff'+*5



tkl n 'kmln . " $ [;PI*:'+4
L [d, EfE
fi=f"#
fif+f+?
-ff#@V ff
R bPO
) [d, E$f+%K= M%'/'P LUkU( nb LM
Q
\
N [Y"#D?+%#
L R N
?$"=+%Lwyz% n: RN :ff$"P=V#+ $WE;
fi=()
?= J#ff
fi L 4 LGM + LM L OfiN :e=f#+ $?E;
-=;)
nPlk0tZ LM :%XN


q







c+E L GOa _C"+34X
- BY)E_?E;J#@ff

fi ,Q
fiGP= Tff L + L :(Sfi
E;
fi=fP= b$Rff
-$f#ff
fi >+?ff
-
fi "#.3
-D.658c
'"#b=+= Pb/,
fi =ffC/GbP=+
GE3VAX
fi B\= GE;Vs$"+!>= PG/,
fi =ffb?/GPG=+Z GD!."#
fi
-\+% L
=+;"+> ff
x%,E;
fiP=G= cX
fi B, )D&
fi
- 5ffv;=ff
K.+!
fi,
8
ffff
"34J= cE3G jIa
J
"+39X
fi B <#@"# M=+;= Y"#.3
-D.F+J+ .+$?E;
-=
<P;0WA 2GP"# 5



q










tkl k* .<. l@ L " J " L " $ [K)
c+%8ff'+
C$"=V!+%Kff
fi
fi Df"+&
- L KG! J
fN 2>
1 G"#D?+%#
^7?
1 C EX
- B L :S L
) 7?
^
1 C Ed$TP DnbDL\IS-+ Q
%>UGN
?= J#ff
fi G"##3
fiff L L R N : #+ $WE;
fi=()
%>UG'N :ff.+ $?E;
fi=V)
n:lW N
nPlk0tV= ,+%1ff'+"#+X-$"#$f+TPV

3
X



fi

yz{z|~}a

L :L



i$"3X =+<M= $1bcX
fi B K
KT!%44=+<"+G"#&
'fffib;#ff$,E;
_+
aq""#&
!D!fiVPb#
fi= *rb?)D."#
-
fi]'
5 5J#
fiP= J

L

L

RY

ff#fi$ ,.5ZbbP= b .+
-V%4> @Z40Gff b=ff
'<P= $+$5Kv;= b= PbD
fifffi
E3(M$+->fP= $+tu /G
fi*:*ff/G
fi*:1+]+.
fifftL
fiff+-SG
- q.ff
fi
+?ff
fi
- "#!.3
fiff.QG= bff*[






q










$
q

$

tkl h nv omn k nrzl@ L :S L [Z L " $ [<)
;+%Kff+
) 71?C EdP$TP TkbD\b L :S L \Z L Q
^
L <"+V)J"#&
!D!fiG.ffP$f#%' L 9l nt
fN 2>
1 W"#D?+%#
?= Y"#.3
-D L . L R N : #+ $WE;
fi=()
n:lW N
L <"+q"#&
fffi,)J.ffP$V3%' L Jl nt
N 2>
1 W"#D?+%#


?

= Y"#.3
-D L L KPOG-N : .+ $GE
fi=()

n:lWGN
C$"=V!+%Kff
fi
fi D?=+; D. L yzF#*$"#.F%'/ ff
X%
fi ,E;
fiP= J
fN 2>
1 G"#D?+%#


f

"#.S
fiD# L L + L L U N :ffP=V.+ $GE
fi=()



%

>

UG'N :ff.+ $?E;
fi=V)

n:lW N




nPlk0tq3X* EOff+"#Xfi$"#$f+FX
- $ : :+\6>++

cM=+FX
fi G> b
',ff
fi -b$"+CP=V"#(ff$&
- +
fi+? "#(ff$&
- +
fiV"# 2
.S
fiD#,/,>)Z ff$45 , 7e>#@+/Gff-:= \+ E;
xPD?/,
fiff
fi/?3.W+%Jff
fi
-
"##3
fiff.F=+;/,, ff$fPG $"# & ]|#]| . %'/G! P=+ff#fi$

^kffW
[ 3 `
_/_ 5D:4+a
3 `__ !:> b
_c_^*2 5D5Jw
fi V$a
M3W
fiff$
- Gt = G"#P.3
fiff.
L


<

+




L


b

+



J





ff

$



fi


f



.

ffP> PJ(!/>+
'"P
fiH5
L L
L L


C





r

n l nv-6k ti;tmnzv ved
{ .

.

-d

nPlnt~nzv v 9k 9vln





rzl 6 l

~\" X-$+ViDffffX
fiM.$ +1 PSb= b )
fi$+%8P=ff
;3fi !
fi= /V[
5MI ff $[0%'Z+ff0
fi \ ff-/ E
fi=0
fiff
-
3Y"#
-
fif : DS):d"#!
fi
:
X
% g " 4Z""#$P&%ffxfiuP >]ff'+i
hG:c= 0#@($"# 2

- J= JPK
-;
hO
fif+ff>&
fiP+
fi+>
&%'
-
JE;
xX3-E3 ff"#YJ!.+c
fi?E=ff
"Pj
=

'<P 5
5MF/Gfffi $P[< k YE
XX4NVG!+fi
fi>ff?
X%1 C#@D
!.5

l

q 5MI /?+
"P
fi&w[

k



ff-+K/Gb=f"#5

,E;
XX4 T"#&
'ff_P= J+/GMff+H+!
3w"#/G2

/Gfffi $+C(!/>+
'"P
fi"+C;#@ ff3
fi $s%' = Db
fiE;
fi 9 (

<
$."=ff
- >G
fi$"#P$f . =f+%8
34ff+5;v;= b #+ =V=c, ff
R Jff$:ff= CDffX*ff*:

m/n31'Xff#o\F'DE4X#PJ!''SP-#<3$'&
3
p

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

+]\"3X_P:
) n q #+$, (ffyz,"P=ff
Xff]ff"= ffD&
- q/w3E Z #+
fi

fi.C""#$.WH= ?+
S<ff+C$fffi!
fi ?%P/ "#&
ff
- q3X;DP&
fifffiGE3 b+%N @ff
fi
fi.5
79
fi P?K+
q q ff
XX-.+P$<= +E0#N /GffMff"#$Y%D
-_ ffJE
fi=W
fi."P=ff
Xff*5
v= T"#/GfffiP $$fffiF/G$+K&
-/GfffiJP=+KG+-
fiCff+>+ )$+.FKMfi$S%w ff;
fi
=ff
.+ =*: +A=+ { rmt nt~nrzl omnfiE;
XX*ff3X-G
'&
fi;Wfi$3%K (ffM
x%8 $"#$P+5
/?+
"P
fi&M
fi/GffX
-$=+K=ff
K
-$"#$, #+ =C

fiC%H"#<!
36<ff;;$:K79
- b $!.5Kv;=
. =f=F=ff
; G)$"+JP= J"=ff
X'fff+%8,
34ff+f ffJ+J3x9Sfi+!
N @ $
%'J%
/w3r
E qetu$"P=i"P=ff
X=CV
X*ffJ*:9.ff
fi ):4Jff
fi
- q"#P.3
fiff ff$\fN @
q5 >&
-"#J $R D;#N /Gff.;ff-a ?/GT"#.3
-D.6:$"P=>% 5X3 W<"P=ff
Xff?
- =
fi
=ff
b"#/W/,
fi/GffMf= +s
E qL= ff,N @ $45Gv;= #%'>Dff+\\= C%PD!
fibE;
XX<
X*
%P/?= cff+ff= s%PD!
fiF
fif= JE3,
fiN @(${
WA @B;/w3EJ:)+f= JP+/Gbff+fE
XX
T+ )$+Mf= M%'ff
fi;/GbP=+f"#5

ut8vXv !t/xw;0WA 2OWl



n
h :

h

e,

n Wtmn



ntln v



tiv

E;;/Gff
fi $,+)+:+= ;$a .b"#P F
$"#$+Pb
fi>cff+ F=+%'/?

fffi?#N /Gffb#+
fi65_jkI:e%c#@e+/Wfffi:wffff$c ,= /V5c+E;$:ff= f +
ff
= ,&
c%'bP."#
fi ?Jff$"P
'&
fiME=ff
'"=
M? $"#$+P"#/G ffb+%;ff+Z + .
fi
?
"#$i
fiuP= V #@ $"#!
fi*5 ~F#%V#@ ff3
fiff
- = ."#
fi] "#$:= +E;$:FE;
/G/>
fiUf=
<ff;0WA 2Z .!"# $a=+?$"#.= SEr+LE=ffuff+uEa#N $45

X*ff_P$VP"#
$?%';$"P=f+%8P= J= bff$;%8#N /WD$[
IPd
fi
-*50= ] EP
4C
f ff$0PuLff+ '% "#!
fi^
L 4.:c=
++
fffi2'ff
fi
fi V"#!.3
fiff. "P
+$E;
fiP=V
-.T"#
fiZ"P= />f+G3'V ff$4:1Sfi
E
fi=? E;,.ff!
fi G"#.S
fiD#<!
fi G=_P= b EPf(""# .c3%P=
fiff
fi!
3!
V)#%,= J ff3Q!*5bv;= ,$M""#/W+D
fi ?= $G"#P.3
fiff.M+YSX8%<=
%''
/ PQ
UkU b V
\ Q 5
g

;+3*x
fi B
fi
fi*5;0= \,X
- B>+%<P= b%'/4> OT
c ff$G= ,ff+\% "#
-
^
'J3! ff$4:K3- >E
fi=\S+!
+fffi2'ff
fi
-
L G4 ).:wZ.ff
- V"#.S
fiDl
"#.3
-D.9
- J=+KP= T#fi$"#$aD#"#
fi
fiY*
% 4*"#SXfiJ!.= PD
fi2
!
fi?$Rff
-$?D?= ,#fi$"#$? $"#
fi
fif
% +5<v;= $J"#!.3
fiff.FE;
Xx)J+ .$
g

E
fi=f,$V"# Pb+%8P= M%P/yPDbffL4IS-+ Q 5

v = P$+J$!+fi
fi*5f0= iAX
fi B4 OY
'b= $ $Zff\ 4:K= WX
fi BZ"+]
g ;
$!+fi$'%' "#
fiT) eH MD
fi \ f+%T= PV.s+%"#!.3
fiff.[G
#ff
fi \+%b= ?%/ 4L^4:;+u#ff
fi \+%b= f%'/ ^4:;,S+
'+fffi2'ff
fi
fi
"#!.3
fiff. !
fi ,=+<P= T= P$+ff
fi CD#"#
fi
fiG+.
% 4ffD$F "#SXfib%H3
X%
= cX
fi B4yzF )D&
fi!
fi1
W58v= $J"##3
fiff.KE;
xXw)J+ .+P$GE;
-=C$f"# P
+%= b%''
/ P
TkbD\
b L >:
L \Z L Q 5
v;=ff
M"#/Gff-$M b
fiE+%; .+
-GHP#N /GD6ff+ ff
fi :*!fE_, +E \fP=
#@(
fi;=_P V=ff
'<ff'+ ;
fiff>+V +
fib3fi
fi= /V5

z;!!+


(''@

v;= J+bE;,/>+
X*"#$;)E_ .+!
fib+f +
-bff+ ff
fi )[
3
{

fi

yz{z|~}a



Null Plan

Retraction

Fitted
Library Plan

Extension
Working Plan?

79
fi ,[<8+?#N /Gff?."#!
fif$+#"=?
fi

.

rmtZ"#5

Plan Extension

79
fi P [M>#N /WDPff"#$Cff+?.+ $U(T(gjVE;
-=W;+%8 E
+V
fi!
fi3*fT"#.3
-D$5

ff+:ff$"P=fE;
fi=

5;^Hu + !
fifff+ ff
- Z= ?
W=Y5)<ff0 <E<ff;36<5X;>?0 =!?
fiLE=ff
"P=L= fff+]x
fi .+

$+."P= $?%'TWff+f="Pfiff#fi?/>+."P= $;=
fi
-ff
fi
31+A D39%P/> = sX
fi .
ff+?
F=
0 C2|M W36;CsG/>+#"=f= J"# PDc fffi/V5
e5
+ !
fiJff'+ ff
fi f
fibE;
-=V=ff
'M
fi$qq3 P$V
3Kff+*:*q"Z2
."#ff'+ ff
fi f"#.S
fiD#T ff$E= \= ,ffVEc
fi +
-3fi> .+$44 #+
fi
ff+ ff
fi ,
fiE
fi=f,ff+fE;
-=f >"#P.3
fiff.;+f"+fff- E $5
&^ >= ;E_. :)=f #+
fiM+> +
fiMff+ ff
- Y+b$+#"=ff
fi C%'_,fi
fiWff+

fi?PG+%;+
S<ff+:9( C .+!
fi,ff+ ff
fi V!.+.bb= fH ff
R +TPDb%_= Gff'+
?E= $,
fiGff'+ ff
fi V) +
fiGY/W+Pff
fi.+PVff"#W
fi]= ?HDP&
fifffiZ+,
+fi !
fi*:ffDP&
fifffi>= bff$:ffff&
fifffif+/WM
fiDP
fi; ff+.5
79
fi Yb= +ET=+ + .
fif.+P.<+M+?
fiff
fiF (ff:)+fG+-
fi?/,
fi =ff+ $
.#-SEM\
fiW
fi0= \\W
fiL
X*ff! Z3fi = $5 f]$fffi?= Z + .
fi
3fi
fi= //,Cf+fffi?V/G+. = ?WD
<ff;>@A?5 2GF?"#.S
fiD#c%/= ?ff+
E;#X1;/G+Wff+E,= bPbDm
0 C C5X2FJ"#P.3
fiff.5
79
fi $bf+
q= +E= GE3V=ff
M/WS/GDM
'b""#/GffX
'= $4[bff+ <P;2\;@B;2\34
s=
Hfffi)K#+
fif)%'/G$ADf, .
fibff $5<^&.+B$,+!
3*ff+f= b=
-U
+ff"#$c
-bE;
fi==+Mff+*yzB
7nG5XYC <ff;>2:4fb+%<ff+c
'ffD
'"38P>= C
fi Cff+#@e"#
%=3
fi , b/W{
/w3E3
-$45
8+!
<ff;36<ff0 7>365XA 2M.B$*cff+JJ= F=
fiU,+J"P= ffD$T"+P3X
fi B4:38!1+%"#P.3
fiff.

<ff;@BA ?>;$5v;=Gff+]
Wff"#$LL= =
fiULE;
fi=L= +ffV&E=ff
"P=]
,/>+PB$]%'

fi
-3<#"#
fi4#:13fi fE;
fi=ZP= >ff*yzJ&
-ffX
fi DaH $ff
fi \3Xf
0 Y36;><2\0 365X?>;,E$ M+%
3$

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

Plan Retraction

79
fi Pq [ML."#!
fi]ff"#$?+ K9.+ $]ff+E;
fi=u+ = VK9ff+0+u!.3
&
fiffX
- Yff'+;.+ $
U(T(gj15

2-N @ff
fi ?= /w3EE= ffbN @VE;P."#$W%'/ = Jff4.5_v= Y&
fiffx
fi D+Pb= V. $?%'

fi!
fi3*#N /Gff$5
^&LI$"#
fi?E;>= +E=+b=ff
'J&
fi/WfffiG"= /W#t`+ /Gff
fi \? .+
-,ff+ $yzs2
N /Gffq+ff
xX
fiLE;
fi=0= Z+ff
xX
fiLL#"#V"##3
fiff.&tL
?ff"P
fifff]
fi/Wfffi/Gfffff+
+ #+
fi*59^H?= FE_. :E; PS=+K= b + !
fi;ff+ ;E;
Xxw
XXfffi, (ff"#MSSX

+-
fiM ff $##:ff
fib"+fN>fi
fif W/>+cE= b
-V= Cff+q!"#J= sX
fi .
ff+Lff'"#$C
fiC
fiff
fi!
3XXL "#/Gfffi $.:<+
fiG!
XXTffD$!*yG#@ fffif+$a+%T= fffL
$ff +fffi /?+
"P
fi&#5


k
n

n

wr

.

l omn

{



rmt9ti!t

n

fi


l

<

v;= b + .
fiG3fi
fi= /)%'/>;J!.+ +., P$ff= 2-N.$."=*:/>3
fiff.3
-ff
fi b= b$+."P=
%PD!
fi8<bF+%w3
-.$"P=G+%wP= ;%/ }h>:K*(L%'/ }h?:UT(gj*C5^HG#
fiP= _"}h0
'
?H)D&
fiff-,
fi"#/Gff-+<ff+f+
K*G
UT gjf
fi
"$F= JE3G,/>+ff
- ff+M= Jff'+f
.+PT= bff'+*yzF #
fi =ff#F
fi?= J$+."P=V"#[ UT(gjf/G$+ .+W
hGy_!""#$.D
% P= #Nff
fi C
fiM
fi G EdP_c
~$T"#!.3
fiff.##@e"#fi>Q
fif .+
fiTff ff
fi
K*,/G$+F .+P
hGyz;""#$P.FffG."#
fi J T+%= TP#N /GD#</?ffTEc= f= Tff'+
EF
fi +
-3XfiG"#P"#$45







q

tkl { rmt n ir . l omnfi !k( #" G #" (!^ $ [<8+?LD0 5 YZM<ff;
(!^% 71<ff;36<5X;>?;MJff'+f%'9k> +* %'/ (g^:
G 2
1 0 C2| W3k(!^% >,/>+#"=+> >( +G #@e"#!fi
& \? 7,
1 : G g^>
36A<P;& @? W
fig^ G

nPlk0t*& \?

l%b

xJ b'effLI%#BUKKwbaMj4b*LjkUOb

I3Lj

TQ kRK

<
";ff+ 2'P
fi+3w3fi
fi= /
<Rff
fiM&
fi/Gfffi[9E;T"+GP= ff+,x
fi .+:+/?+."P=ff
fi M%/>

fi,= FX
fi .Tff'+*yz DSDE;
fi=,= F
fi K D3 54Z;P
fiF= FX
fi .+Pbff+E;
fiP=J= ; $$
/,)T+%K/>+#"= $:ff= $+B?
fi$;fff"# D!
fi G= bff /,M+%8/?+."P= $;E;f= b
-

fiff
fi
'3e"#
fi
fiF+,=
fiff
fi
Se"#
-
fiQ+%w=
fi$Cff5KvF
fi$
-G= ff /,;+%w/?+."P= $
%P=f D31+W
fiff
-
3*#@ $&
fi+M B\+ff
fiP.+&
Xfi5
3

fi

yz{z|~}a



v=ff
b "#$,#fi$"##JV
fi +fi,x
fi .+ff+*: C
fi.c
fiff
-
3_+ DS_"#
fi
fib $
/>+#"=?=
fi ;
-ff
fi
3*+? DSw#@ $&
fi;#@e"#fi5Kv;= !0 C2|MW36@B;2\34 "#$ ; D3F
= sX
fi .+PGff+=++ $
fi=
fi D3* b+C T3fiP$ffG
-f= Jff*:+Vff#-$
DSc%P/= ,X
- .+ff+\=+,ffV J+ )$+b
-Z= ,
fi C D3<#@ $
fi*5>v= ,X
fi .
ff+*y4
fiff
-
3("#
fi
fi9+"= $,&
fi/,
X'$!fiTPT/>."=C= K
- K fffi/ff$"#
fi !
fi*51v=
"+S1X
- B(M+G3$4[M,X
- Bf
fi= CX
fi .+Pfff+V%<= C%P/ 4 OE= P1
\<=M)
ff#fi$A$"#/W$b+V)Z"#
fi
fif+%<= b%'/ Se
X%[b=cZff#fi$4:)= sX
fi B?
fi.!#X%
"+Z)>P/G+$45>cE"#
fi
fiC+>SV ff$%',+D\ E D3K%'/>65>v;=ff
E
ff+A
c +.+ff$?GW#N /Gffb+%<= ,Dffx8fff%'TP= a"# PDM fffi/:4 ffX
-B
= FX
fi .bff+C
fi
1 $"#$P+
Xfi,"#/GfffiP5<IsI$"#
fiGc%'1/Wff.3
X'*W= ;
fi+3
+f3/GffT3-
fi= />65
v;= = + .+
-> =b
_
fiff
fi
$4:ffE=ff
"P=V/G
XN$c= JP
fi$?fff> P(ff"#,
+fi !
fiVff'+%bP= G E fffi/5Yv;=ff
'b+fi
-
M$fP= Jx
fi .+ W36A<P0>F ;b
fi :
E=ff
"P=qff$"P
ff$MEc= = f= +EdP>PJ= Cff+f%'!J
-V $R Dbff ff
fi >ff
'(ff$65
v;= VR $
fi]+O
% G;3 ;<CZf\ E;-i $!+fi
fi"PB
fiDPV= fff]X
fi .+PV
'
+
fi/G).+ffM :1&
-"#G=3
fi ?/GPYff'+c
fiZ= ,ffVx
fi .+/>+B$c= ,x
fi .+ff2'!
+3
"#$C.+BWfi 35 L= ?= ,=+4:<P
fi />+ffqff'+s
fiP= GX
- .+
fi"#$$C=
"="#$=_ bE;
XX*,G"PfiD!b/>+."P=fGG $R ff
fi c fffi/V5
^ff$3Xfib= ;ffJx
fi .+J!= ff,"#&
!1+%wMP#+
fi#fi!/>3X+%<R3X
-.+&
fi#fib
X*ff.
+fi !
fiF"#/G/WfffiG ""#
fi ffJ Pfffi/>:ff TW"=+#"#
fiU$
X>%8RSX
fi.+!
fi+#fiG
X*2
DM+?+%<"#/G/Gff-> ""#
fi ?"+fC=+.?G"#/Gbff5;+Z/>+B$< ?"#DP
fi
fi>W=
R $!
fi?+%1E=!= ff?+ $+
fi>P= TffGX
- .+:ff+? ;/Gff
fi
'"3wE;Ba
fiVI$"#!
fifY2
/G$;b P$ff/,
fi $?ff'+,X
fi #+,E=ff
"P=,
Q + /Gff$Gff
- YP= T#@
-/GD#3w!
35
IGe dK#-D:4$ff%+?
xXfi /,
fi
fi b
-D$!
fi D+!
>+%K= $M
' $5

tfiff R
k

n

n

ir

.

lrzl





w/.

v;=
:GL%' "#
fi
f&
-/,
X+?]
fi#? .+
-Z"# DP+(
) n k GG
#@e"#
fi>P= M+;"M?ff+f#fi$"#$W%';#N /GffF
Q% P= #N $45K^&f= J"M+%
+ .
fi*:*?+P
3Kff+Z/a
fi =ffbG/>B$A%b#N /GffYJ3fi
fi+#fif%'!
<ff;36<ff0 7>365XA2:
+\= ?3fi !
fi= /r/Y!bBP."PBV+%;E=ff
"P=*5>v;=ffb= a%D
-)$"#/G$CVC+%_3
-.M+%
= C%P/ hGK
: \E= P
h
MW+
S1ff+\++
W
T?ff/,+<ff
fi f
C 5X<ff;7365 2:)#
fi=
UT(gjVf
K958v= B
UT(gjq"T/W$+;#N b= Mff+?%' = 3:
->E=ff
'"=f"b= 9
)
% "#
-f
M"3X-$4: #@e"#fif= ,+/G,;
-V .
fi*5
fiP$"#
fi+
% K9P$fffi.F
-q?"3XK
) G :E=ff
"P=?
ff#N $#fi+EJ5






q








9tkl n ir . lrzl */. !>k( $ [<8+fLD0 5 YMG<ff;
0 :( 213Gk( :OK*4b:kW>k( *:U(T(gj*95
4. no~n6

0 :( ,
/G l nt=nl 0t!D0 5 YMG<ff;
}w
: 2>
1 !#fi$"#T+f#fi/WD;%'/80:
oM#fiB4
: %/0
G

'G+-
fil nt nlk t+
,
UT(gjl nt
b$"P=Vff( $?ff() 4 N
3$

fi yz){ {z {z0{









nzvn

h

/

nPlr4l !t

h

!

mez)zx 0 yz{



zy yz"z{

: UT(gj*0P;0 :(

K9Ul



nt

fSX*#fi/Gff.;+%#) ) ( (
M;0::(

-,

n %tmn



ntl v

^H!$+%
- q+ $"#!
fi V"+3FX
fi B :*P."#
-\/G+$b"P= +
"#$b/?ffGE= ]=
X
fi #+ZffuEC
fi
fi3Xfi .+$45`+?R) u#fi$"##G/w3E
fiL= \"# ff
ff+?+G QJ= %D
-<3Xw
X*ffKE$ +%4N @ff
fi J= /w$EC:e) G) n k :C.+B$
?
fib#N /GffY"P= +
"#:*!$M= >!("P
$$ZP"# ,V"#/Wfffi#fi/G+,=+
#N /Gff$:+\ Mf= C%'ff
fib3XK+%<= >3-+
XCE$ c=+M= G#N /Gffb/,
fi =D
=3b/>ff5
: K*
;E
fi=Z
b79
fi P q
XXfi!.+$:4P."#
fiqPff"#$bVR ?DPV+%= ,%'/ }hGQ
.+PD.?
% h?yz,3f. $ K*b3fi fE
fi=J+
% hGyzC&
fiffX
- D:*$"=].+ $
U(T(gj15
$"P
Mff#Nff
fi
fiW+%;&
fiffX
- D;
'K= T!<+%4#N /Gff.FW
hGyzKDF=+<+P [
5cWA @BA <)84G5 7
36AWh>5QZJff#N b
!/G =ff
'/F%'+Xfi+E[

e,
t#6OLAl8(YZ02W}h ( 0 24Ch $ 0 <ff;c
/G =ff
"|MW3v5X2m70W;
- (36;ff84W0F<P;;@
g 3 ;<ff;!5cWS0 @B0885X2FD<ffA @ W36;H84WS5X2Vh ( 0 24Ch $ WM7V3 G037A <<ff;W|8:A 2\C 5X2F
W36;ff8\W0 ?;l5XC ;>24365 70Y240 @B;WRe360;3 G;7A <<ff;nWe8A 24C ;>247;f36A,;{ ( Z $ Z
k!36A
( ZE $ Z

!29@
> 5 2W{0F <ff;;@
(h $
g \ W(h ( 5 b{
> <ffC ;><5 2GFW0>F <ff;;[
(h $
g \*^W(h ( 5 {
5 24C 5 2GF,7A2W36<P05X2\3 W0F<ff;;[
(h ( 5
b_ G9 4h $k ;<ff;_ [5W0?0 <5 0Y;B5X2W36;H8 AffDh (
g _ )
024CU_ 5cW3 G;,7A <<ff;W|8:A 2\C 5 2GF!?>0 <5X0Y;5 2W36;H8{AffDh $ 0 24C
5WS0%7A 2W360 243
5W;LDA <Mb
_ 9)
0 2\CG>b
_ )

g Y5c;kL
b_ _ $Th $! ;<ff;T_ 0 24Ca_ 0<P;
g _ _ +:h ( 5
?>0 <5X0Y;W5X2W36;H84W 0 24C AHDh ( <ff;nWe8;7>365X?>;Y E 024CU_ :}_ G
0<P;l3 G;B7A<<ff;N
W|8:A24C 5 2GF,?0 <5X0:Y;WS5 2W36;ff84Wf{0 2\Cf AffDh $ <ff;W|8:;7365 ?;>YZE
5W;LDA <Mb
_ _
$0 2\C?`
_ 4[G
$
g Y5c;kL

n Wtil

v;=ff
'Jff#Nff
fi
-\
fi/Gffx
fi$M=+C E;?
!/G =ff
'"Jff'+b=3GP= /GG)i"#
-
fiC+
= $+P $fx
fi B(McE;#X 5TcC=+ME_Wff+c/>3f=$Y"#P$
- VM+A
ffD!
"3
.ff
- D<+
24A 3 )
'/GP =ff
"+:+= +E_$:D&
-"#= G"+f
X*KG MK/GPT"+3)X
fi B 5
3$

fi

yz{z|~}a



v= JR $!
fiV +Ed
$;;PGE=ff
"P=qff$"P

fi"+C.$AE= V/WS
fi , ffE.W
fi
= G"#,+%<
3ff+5Jv=
fi/Gfffi$b+E;c
c=+9) G ) Gf/,T)G+fffi
C#X
fi/,
fiT+ff>ff$"P
&
->P=+"#ffW=$/>ffMDO) n 5i;#N /GffTff$"P
&
-
/>ffbffV
) n "+fP$fffi;
fifP= M%+xfi+E;
fi J#-/GD#;#
fi f ff$??,ff+*[
g] &
fi +fi,"+3X
fi B4: fffibV.ff!
fi "#.3
-Dff-ff
fi
- "#!.3
fiff.F
-$
N @q+V)Z"#
fi
fi*5<^&qP=ff
",3X1P= a"#.3
-D.FE;
xX1,. $?E;
fiP=f= J$
PD bffL 4I+fi
\ Q 5
ELff-Y\"+P3;X
fi B4:
fiP$]N @i+L)u"#
fi
fi*5\^HuP=ff
,"? E;
.ff
fi G"#!.3
fiff.;+G+%Kff
fi
fi f"#P.3
fiff.; "P
+P$GE;
-=f= Y!fE;
XX*)
.+ $aE;
fi=?= b$!:
PL
UkU b
Q :+?+f.ff!
fi G"#.S
fiD;+f,$"#f;+%
ff
-
fi >"#P.3
fiff.KE
XX*, ff$V3- ,E;
fi=f= b EX
fi B4:+)+5

g]

#ff
fi Y"#.3
-D
fi!$WMN @bP= $+<#
-= <ffG /W
fi,;ff/G
-*51v;=ff
'
g G
"#.3
-DbE;
xX<?.+ $VE
fi=iP TkbD\b L >:
L \Z L Q E= 4
b= ?= $ff
fi
!*5
C+%T++
+ff-2'ff
fi
fi q"#.3
-D.Mff-bE;f.ff
fi \"##3
fiff.c
fiP$ZAN @
P= $+<ffG+.
fi*5Kv;= $b"#P.3
fiff.E;
XxwM.+ $,E;
fiP=*P :TbD\b L %:
L \Z L Q 5

g


fi +fiJ"3xPO) ) k G>= ff?= #%'bP."# J!"=#N /Gffff$"P

fi*:
E=ff
"P=V+/G ff.F,/G+
fi ,= J!("P
$f;+%K.ff
fi ff:ffff
fi
fi >"#!.3
fiff.:ff:)+
X
fi B ;%'/ = Jff*5Tc
"#C=+T?ff$"P
&
fiq"#P$ M"PfiD!#fif>?M+%K
ffff
"3
<ff;0WA 2
"#P $F
fi\,ff+*:!G."#
fi G>ff$"P
'&
fi?%'/aff+f$3xfi/G ff.;,/G+
fi >G!
+%<"#.S
fiD#KE;
fiP=?
ffD!
"3*.+ D6:(Sfi ,E;
fi=fP= #
fi "P
+$Wx
fi B(;+5
v;= b J#@"#
fiG=ff
c"#$)ff"#C
;= s%"#=+P= Jff$"P
&
fi> G
Gff'+]H$
PL
UU b :
k
Q ;
'S
0 YuL0 E>W/>ffaM+M+%_>ff$"P
'&
fi\f \,x
fi BZHP$
PD bffL \ISfi

k Q .:ff,= $ME_,ff$"P
&
fi<= ff'G)b."#P$G;b3
-_FE;#X 59ZTE
XXwP$+
= JE;GM+.+PJff$"P
&
fi6: 3-
X= /E;
Xx1 C=+T?A
;/G+$W%'/r
ff+VcDVF
fi.Q"+3*x
fi B,
;P."#$45
fi= =f= G"= +
'"#J+%_Gff$"P
'&
fi>P."#;
'/>ffJ ffP/,
fiff
!
"3Xfi:(
fiT"+
/>ffV+Pff
fi.+!
-:8
fi"#f= fff+ G"#ff'] ,=$f .+$= Vff$"P

fiC
fiL+ff.ff$5
7 ?#@+/Gfffi:;E= ff
X
fi ]ff+B
h>:;= ff+ ?/,
fi =ffG=3\"#$+$0X
- ^
B 4 Oq+
+
fiDP(ff"#$\?M+%<.ff
- >Mff
fi
fi f"#P.3
fiff.
? $"#M=ff
X
fi B?%'/r#
-
= $+P $]DL+ = >!^
45iv= V#"#
fi]3fi !
fi= //,WV+fffifPqP."#,#
fi=
ff$"P
&
fi ff#fiC= bx
fi B>T= ,"##3
fiff.#.:ff M= $JE;>ff$"P

fiM+b bff/G/G
'"+5;^'%

;ff#fi$4%: L)$"#/G$;= P$+ $?+ D3
fi*: ;
X%
'_ff#-$4:ff= *
O$"#/W$ H / 5
v9Z $"#f+ DS
fiCfi$3
- Z= Vff'+uE;
-=` )ff / fP:FX
fi B :F"#.3
-D.6:KE;
3Xfi+E0= J3fi
fi= /W."#;ff-f= DJff$"P

fi;=T+!
;n8:AW;C5F^Hff%'/>3xfi: Gff$"P
&
fi

F#@()D$W
X%* GP= "#.3
-D.9
->= bff'+ff)f?= b"#P ff$?,= Tff'+>ff
=+ff$"P
&
fi*5<v;= M%'/>31ff#Nff
fi!
fif+%K#@()D$W
c.+$W
-f/>F+[
% <ff;0WA 2OW8E
fi=ff
fiVCff+*:
&
fi"#YE;J $V+)+Yff$"P

fiM V"#!.3
fiff.;P>Gff'+V=T+C.+ $AE;
fi=?
'ffD
'"3
$65


3

fi yz){ {z {z0{


-,
e

n %til



mez)zx 0 yz{

#6p<ff;0WA 2( 5cW#@()D$5X2 8Y0 2Vh5
5cWSAffDS3 G;vDA<@P :TbD\b4 Q DA <WA @B;BYZ5 2l4




zy yz"z{



PDbffL\ISfi=

5cWSAffDS3 G;vDA<@

Q









DA <{WA @B;BY5X2O 0 2\C

|0Th7A 24360 5 2W24A%<ff;0WA 2mAffDS3 ;DA<@8P :Tb1D\b Q 0 2\C
H
;5 3 G;<,4{8:0<365 7598:0 36;nW5 2024A 3 G;><Y5X2O4 c <,4!C A;nWY V08 8;0 <%5 2
0 24E{8(<PA36;7>36;C%3 G<ff;03.AffDB3 G;vDA <@P :TbD4b %* Z4 Q


PLUkU( nb
Q

5cWSAffDS3 G;vDA<@

Rh

0 24C

7A 2\360 5 2W24A%YZ5 2!AffDS3 ;DA<@p



v = GN.!a]=ff
fi.]"$,+W%H3
fifiZP.3
fi =ff%'+E+4[G"#.S
fiD#b=+,$!+fif= $+

"+V3-E3 K)J#"#$4: +fGPV"+ffffiG)J/WS$,
X%
fi; ,fi ;+
"P
-+$
-V+D
"+S*X
fi B 5
v= $"#f"Jff$$_!/GM#@(ff+
fi*:= +E_$5Kv;= MN. 4"J3 <P=+_sX
fi B
"+ Fbff#fi$C%'/Mff+>fi ,= M+T"#!.3
fiff.9
fiW= ff+W $"#
fi
-K%'/
,= $tu= E
M= J"##3
fiff.< ff$A,$+-TP= b= $+;E;ff?)$"#/GJ )ff / 5
v;= T$"#> 4"M +. F+ D3
-K= ;%'+Xfi+E;
X J)$"P
3*"[K ffTP=+
hd"#D.S
fifffi
E_cX
fi B
: 4 _+,
< ff5K7 = P/G: )D;=+
\ff$,M= $+Kl
< ff:+ <

fiKff$"P
&
-J$!+fi$C=+= $$5 /a
fi =ff1)_P/G $Cb/G+F= FX
fi BB4> O_+
3fi
fi=,
fiKP=
4:D&
fi"#
44E_ff, b- <T+ffY PDc
fiGP= ff+*5L~F _ff+
-
fE;ffAfi$3G )ff / J!"# C
fi= ,ff+*:*/G#fiVP= a"#.3
-D.c=+bE;G ff$
,$+-= b= P$+
% GZn4J5 Tff#Nff
-
fiW%_#@ ff$? +.D$QN.!_P=+Y!
E;
XX<)>P/G+$E= C
fiJ"#$$sq!>? )DW
fiZ= Wff+*yzC"+3_P"# f
55
E= Q
fi.9!X
fi Bb
'K/G+$)#:+ <=+<ff
fi JJE
XX Kfi$3 )ff / ;"#P.3
fiff.

fif= bff*5
c+E`P= T#ff;
fi?E=ff
"P=Vff$"P
&
fi;"+f)JP."#$?"+fJ!.+$?&
fi/Gff-w[K,ff$"P
&
fiV"+
bP."#$?ff-,
X%*
fi#_ "P
+$W$?
F#@()D$45 )
fi ?=ff
F#ff
fi ,/G$+;P=+;=
ff+\E;
Xx< Y"#ff.3
-q )ff / ,"##3
fiff.:)X
fi B :4J!$Rff
-S3fifffiE;Y/a
fi =ffJ3
=+."#
fi ff-b#@()D$,ff$"P
&
fiK"#$) P= _P.F.ff
fiCE=ff
"P=ac .
fi
ff+
@B5FG3vG0?;B@B0 C ;M= DJff$"P
'&
fi;
fi +
fiSXX5
<.3
-ff
fi b."#
fiGPJ ""# ;
fi?=ff
K.ff;/,
fi =ff_/,)T+-YP$
"#!
fi:,E;
/>+B;E;
-/GP.+ffK++
fi65179
fi.!$:$ =+KP= .ffK+%4."#
fiC

2\A 3 "#.S
fi $
J)b= M.M+%1P= T#ff_!$GE= fP= X
- .+,ff+?E<"#P$+$atfffi,= M.M+%
24;c+%1= ,ff$"P
&
fi 2'.ff
fi ff<P=+
7A MYCJ=3M\$?P>"#$+bP= MX
fi .+Wff+*5FI$"#4:
E_?
fi$"#b= GP$ffJ\I$"#!
fiLD:9E=ff
"P=\#@(ffS
fib= +E9* f3
fi,. +
X$:4"# ff$
+Z=
!
": "#ff&
ff$Pf= $b$P
"#
fi<fff"#
fi G;/?"#G).+.5
c#@(CE_W $ffJ=
) G G ) n f%' "#
fi*5fc
"#,= +E= ?ff#Nff
fi
fi/,
fi.
=++%K
-.; .
fiJ"# ff
) *[K= s+P"P= ffD$Ml
/w3E+A MCX
'
=+K
-"Pfiff$3X4DP&
fifffiE3 K+%*N @ff
fi C
fi$:= c%P/G_"P= Dff$;+>#@ ff$fff$"P
&
fi*:4<ff;@BA ?>;W
= J"##3
fiff.F=+;
fi
fi3XfiCN @ $?
fi3:(fR $T3xP= J3fi
fiME$ F+%N @D
fi
fi$5

3


fi


q






tkl h nPlr4l h ne,Wtmn ntl@ $ [<)
c+% 8+*:oc
fi$"#!
fik
)\[d#fi$"#M+f#@()D$A$
= Pb
F G#@ D!$f$!Xl nt=nPlk0tT3e5D5
7)
0 : N [) - L G ) "
n:l } N Z K9:
C$"P=fff+(NN P $fffj= G >0 O\L? 0 " NS
NN
F ;
/G =ff
"cO
l nt nl NN Z UT(gj




yz{z|~}a



nl 0tq3xff'+*:
fi$"#!
fif3
fi#_"#Xfi$"#$W
fi?x
fi $;>+fe5

v = fE3ZPqP/G+>P= V"# PP("P
'+$E;
fi=u+]#@ D$$uff WL=

D)>%P= >$*5Vv;= W%' "#
fia) e L s= !/w3E!("P
$\E;
fiP=Z=

fi b$qME_#xQc= ,ff+\ ff"#$\ffV/WS
fi ?= G+ !
+J"#!.3
fiff.:eX
fi B :
+Z!5f
"#G=J= >"# ffX
fi V)E_X
fi B\+]ff$"P

fiM
'b/>ffW= [JEc=
= sFX
fi BfP>GA
ff#fi$A= Y!f
cff#fi$fPD5;7 =ff
;P$E_Jff? =3C
=+fiGP= >"G%P/G+
fi VA$q%P= ,%
/ PL
UkU( n b VI Q [,fZ)$"#/G$C#@ D$
fffiGEc= Vbx
fi B,
;ff#fi$4:e _P=ff
Q% "#
-f/G+$Q= JW
fi/G/G$
'+#fi5FIGC$?+%
= M%'
/ PQ
UkU b VI Q E;
XX* $+#@ ff$?
fiVaff+*5



q











$
q

tkl h n o~nkml klkn) " $ [;793EJ:D8+4
W
)
;+%K= M%'/P TkbD\b%4 \Z L Q l nt
0 [%4 \Z L

N [Y"#D?+%#
oM#fic%'/8*
N 3X1"##3
fiff.K. $?E;
fiP=;)
nPlk0tZ7)
0 :% N
nzvn G
)
F+%K= M%'/P DbffL4IS-+ 4 Q l nt
0 [: L

1N [Y"#D?+%#
oM#fil4> c%/8GN
oM#fic%'/8 N 3X1"##3
fiff.K. $?E;
fiP=;)

N "#D.S
fiF ,X
fi B?+%K= M%/ L >: LM %'T+fff LM +A#@( $P&
fiVl nt
ff#fi L 9%/8GN4
N 3fi ,E;
-=V3X1"#P.3
fiff.K#+ $?E;
-=:P LUkUb L Q
nPlk0tZ7)
0 :%XN

v =ff
'T"#"P-ff$b= >ff$P"#
fi
fiV+%;= >+u3-
X= /V4E;, #@ b#@e+/a
fi ,= G3fi !
fi= /Vyz
;
%/>3
-$:ff +
fi C=+Q
fiK
F +ff,ff+W
fi<P ;"#
fiP $<C+fi
-GC=

fi ff ff
fi G ff-/f.: "#/GfffiPJ'
x%1=
{
0 24Eb+-
fi?,=
fi ff'+ ff
fi G fffi/V:
+uE
XX<ff3X-fN
fi3:* D.fi$M+%= aX
fi .+PVff+
fiJ"= ffD$sV + 6.:1 /?+
"
H= J .+
fi>E;
xX* T"#&
'ffT,+!
3*ff+f/GTP=+f"#+.5

V

9k}^'

w
"

+ "z "



"


<

"\}e!



v*, +c%'/>34 )
fi$;%1= J+Z3fi
fi= /E;T) +
fifff>"=."#
-U#
fi Gbx
X%P$G#&
fi
+%YP= q .+
fiq3fi
fi= / ff#fi)$0D0~\" X-$f+ i;D!DffX
-$yzA.$e+Y3fi
fi= /
3


fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

H= $S%,"3Xfi$\ jkI
fiZP/>M+%TV$."== == !"#G+%+!
3Kff+5?Z?=
"#&
'ff>P."#
fiu,E;#X 5v;=ff
'>
P"#&
fiL!$?/>+ff+%JP= q"#"# .fL/?,%'/
I$"#
fiV
qGff$"#
fiff
- Gff+?ff+ ff
fi G Pfffi/>5
F&
ff,q
-$"#$] . =s
fiL79
fi \?E= fV fff $!D.,ff+L+]+L+."
$!D.?]ff+ 2'P#N /GD?).+35ZZ"+0ff#N Z= Z"=ff
xffu%YZ ff]Hff+4R
hG:
$"#?, ff/a
fiff

'"T"P= +
"#:K%'+X-SEM[

-,
t#6OG;b"=ff
X'ffAffDS08(YZ02;h0 <ff;B;0 736YE!3 ;W;@
eW Dh5cWS7A@[8(YZ;>36;B3 G;2m5 3LG0WS2\A17nG5 YZC<P;>2
3 G;<L 5W;lW;>Y;7>3LA 24;BAffDhT WSA8:;>2m7A 2\C 5 365XA 2OWA<S3 G<ff;0 36;2\;C%Y5X2OW
W D3 G;%7A 5X7;!5cW3 G;%A8;27A 2\C 5 365XA 2 3 ;2hT WB7G5 YC <ff;20 <ff;%0 Y8Y0 2OW3 G03
70 2,;[7A 2W36<M736;ClE0C C 5 2GF0lYZ5 2 > 0 2!A <ffC ;><5 2GF{ 0 24Cl0@B5X2\5X@B0
?0 <5X0:Y;5 24C 5 2GF7A2W36<P05X2\3
G;<ff;%4S5cW1;>5X3 G;><102;5cW365X2FW36;ff8A <%0m24;
YE
7><ff;0 36;C%W36;ff8 3 0 3[70 27A 2W5W36;2\36YE%H;%A <ffC ;><P;Cl8<5XA<!36A 024CV3 0 3f0C CWBWA@;
8(<PA8:AW5X365 2i
G;><ff;Sil
b
3 ;<L 5W; 5 D3 ;7nGA 5 7;m5cW3 ;V3 G<ff;0 3 {4> 4 3 ;23 ;2\AC ;G0WV3 G;

n %til

7nG5 YZC<P;>2VA360 5 24;CE

e0
H
| 72

0C C 5 2GF%3 G;BA <ffC ;<5 2GF%4L^\
0C C 5 2GF%3 G;BA <ffC ;<5 2GF%f4
0C C 5 2GF%3 G;!A <ffC ;><5 2GFWf 0 2\C! 5X2m0 CC 5X365 2m36A0@B5X2\5X@B0 Y(?>0 <5cN
0:Y;,5 24C 5 2GF%7A 2W36<ff0 5X2\3
3 G0 3\DA <ff7;nWS0 YDA<@lW 5 24 WS0 C CV024C%C ;Y;36;
Y5cW3 C A;nW2 3vMG2\5 DE L5 3

w
e

c


8(<PA?5 C ;C%3 G;nW;B0 <ff;B7A2W5W36;2\3#L
5X3 3 G;!7A 2W36<ff0 5 243 W7M<<ff;2\36YE5 2Vh

~ " Xfi$b+mi;D!DffX
-J#$ +"P3
-/= PY P!
fi$M+%_P=ff
$ff.+!
fiqqSfi2
\

-= /V[
g

ff $[<sfi$3% (ffJ"#P$ ;YC+P
34ff+*: +ff>"#/Gfffi!
fi?+%1E=ff
"P=?E;
XX)
fi
%H"#+!
&%'G=
fi D35
g

F/Gfffi $P
[ 0 2\E>ff'+L=+>!+fi$,P= fff+ ff
fi Z Pfffi/
a$3X
fiU$
-u= f #+ =
Yqfi$3%b (ff5uv= #%V+ffiP.+ %?$+."P=ff
fi \= f . =L=+,
C .+ff$

P>"#&
ffMf ff,ff3Xfi?E;
Xx*NV?+fi !
fif?= Jff'+ ff
fi G fffi/
x%<
#@ff
#5
g

/?+
"P
fi&w[cE_V
'
fi"#M (ff$
fiZ= , #+ =Z $!Db 2-
'/GP =ff
"Cff+:+
%' = P/G:= .+ =G #+$WD>Cff ff
fi J fffi/
<C5Kv;= P#%PC$+."P=
%_= Gff'+Z . =Z=+,ffD$s J)$+YA (ffWE;
XX< ,"#&
ff,f+P
3<ffq
DG%1
fi#;#N /Gff.;/Gb=+f"#5
3
X

fi





k

yz{z|~}a



9tQ'tmnvv

v;= , ff $c )?%M+Z%'+X-$ET;
fi$"#!fiG%'/rjkIyT ff $:4&
fi"#Y! ff $

Ta P ?+%K= J3fi
fi= /Vyz;$."=V!.+ :D M"#/G/Gff.;ff->fP= b+ C+%1-$3%
ff$; "#/Wfffiff#.5KI+
-"#\ff#N $Fff+K+G!+fi
fi9
fiG= TP+/G;E$,<jkI:ff+
ff,
; 45

fi ffA .

nlntmnvv

</Wfffi $:ff$"SX : "#&
!.;+%KE_,"P3
fi/?[
5;=+bZfi
fiVP= Gff+ ff
fi f Pfffi/
'M$3X
fiU$b?fi$S%_ ff>+%= W .+ =*:
+
e5;=+;= J$+#"=V3-
fiP= /E
XXwDSXfi?

fi;Wfi$3%1 ffb
fifP= b .+ =*5
v= N.!_"#
fi!
fiG"#b+ DS
fi>ffff$< _ff?>= ME3J= .+ =W
F$+."P= $4:ff= 2
%'c
fiF
KP b+%1+Z$"M
fiF
F M+%8jkI5<v;= b$"#f"#
fi
fiW
8fi$;"Pfi$+$:ff= +E;$[
jIY/?+B$<!
-_"#+.Q= Tff
fiP .+ =WD>!.+
- J+<= Mff_,#@(
fi ,= .+ =

ff+EffE.A
fif />+!
"s%!=ff
fi*:*Ec= $J`..M+Y+Z+Pff
fi.+PV
fiDM
-Z= G #+ =
+?.3#$Q
fi;
fif)=V
fi$"#!
fi5
h 2
PD+%M+%"#/Gff- $G+/W D#bZff/G!.+
- V=+C%'G+ffZ+
'3_ff
$ff
fi ]= V) +
fi ff
fi ]
fiDW%'>+4tuP= "Z'x
fi .+]ff+4G ffX
-$Lffu=
fi+3
/G$"P=+ff
/,tuP= >3fi
X= /E
XXKDP3XfifP."#J"#P.3
fiff.%/= Wff+\ D!
XK
fiM

fi.
= MD_ ffCHDffxff'+4.:Dfff+
fi Y!aSb
-/GffX
fi$F=+Q
fi;E;
XX
&
-_3X1 P$F+%*= bff
(ff,;E;#X 5~M%'/>3xfiG.+$4:eE_b=3[
kR

n n
5X3 0Y5)<ff0 <E!8(YZ02ThL 5 YXY70 W;V;>?;><E,8:0<365 0
6* 70YXYv36AV G iL
xe;>?;><E!2\AC ;B5 2V3 G;8Y0 2F <ff084C ;24;CEWhT W8Y0 242\5X2F{8<ffAYZ;>@136A;B?5cW5 36;C

8Y0 2

Zc,
fiff"#
fi+P /GffKJ +c=ff
KP= /V:ff= +E;
fi b=+F= b .+ =WD$
+ }h
"#/Wfffi#fiG#@ fffiP$4:(?=+;= b3fi !
fi= /E
XX%'+X-SE0,+=? f,= bffMHffffX
ff+4<#@ fffi!
fi ,f .+ =?
-f= J "#$5
Zf) +
fiDuff/G.+!
fi
fiff%/>3X-Z=+?ID yG/GP= (+% <ff;245 2GFf+
Sff'+

fi ?"#.S
fiD#_; ff$f,P."#
fi e1
';$Rff
-S3fiffW= b .+ =V!$+."P=V ff.B
DL jI5LP i$"3X=+G+.+P$Jff]/>+ff
fi ff
fi q$+."P=Z%'ff
fi,E= ff>ff
-$Y
}h>\
: UT(gjO+m
hG(
: K94
J:"#P$
fi >P$$"#
-#fi?>
fi ?+Vff#fi!
fi >"#P.3
fiff.
%P
/ h>5

ASr 6 k G; ;2\36<5 ;W F ;>24;<ff0 36;C E W 8<ffA7;WW5X2F0 2;>2436<E AHD3 G;DA <@
}h UT gj* 7A <<P;nWe8A 24C;n0 7>36YE36A3 G;VjkIF<ff084 AffD!80 <365X0 Yv8(YZ02W<ffA>A 36;C 0 3Wh
0WWM@B5X2F3 G;BW0 @B;17nGA 5 7;,5WB@0C ;!0W!36A(G0 3 7A 2\C 5 365XA 2BeA8;2A<S3 G<ff;0 336A<P;nWAYZ?>;
0 3;07%W360F;

^&Tff"#$;Pa!= SE =+;= b Eff
fi$F #+$?ff
fif$)b>fDPG+%K=
%/ }h?\
: UT(gj*"#P$VP>= G/GJ+P
31ff'+P=+"#/G
h>yzM"P=ff
XffA
fi\=
.+ =?Kff#N $f++M K [
q qff.5v;= E;c= +#Kb= Mff#Nff
fi
fi*<
[ hd"#/Gfffi: h
#N $fff"P= ffD&
fi >f\"#
fi
fi>?+
%
: h#N $ADf"= ffD&
- >,= $_PG$+-5
^H,= ;";=+l
hu
'1"#/Wfffik: h`=* J"P=ff
Xff*:++sX
fiBE;
<?/a
fi+$9 .+!
fi
G Edff
-$5
3
p

fi yz){ {z {z0{


K)

mez)zx 0 yz{



zy yz"z{



P= E;
;+V"3X' n
:Ec=ff
"=G"P= Dff$KT"#
fi
fiCM$+fi_+C .$ E
UT(gj>ff
fi$:+ %'K$"P=Gff&
fifffic$+fi !
fi*5*c= P#%P=+< UT(gj?DPJ .$
2\YZEUT(gf
j ff
fi$6
->P= _E;# <P#N /GDcE;
XX4fffiG #+M/GM#N /Gff.;


fi$"#P$?+=?
fif= C .+ =?-$ ;G""#$
fi#fi?/GJ"#.3
- $?ff+5
^&,= $"#G"!Y>"#
-
fis
K"= ff*e) n #+$ E^UT(gj>ff
fi$
%'SX8#@ff
!
fi VM)D&
fiff-f
fiMf= ,)"#
fi
-q+q%b3X<"#
-P=+J =
)q"#
fi!
fi*yzF )D&
fi!
fi*5Kv;=ff
;"#P$ c#@"#fiGP>",P q(Q+5
^&G= ;'<"Mb= $<"#
fi
fi>MX
fi B,+Gs= $+ff
- Y44
'<"P= D*e) n
s= G.ff!
fi Db+c
~$bff
fi
fi q"#.3
-D.c=+C ffb= G= P$+$:*#@"#fi\c
fi"
H <+)S5
M3
fi b
XN$W=+_q .+$Q=
-/G/G$
+PT"=ff
xffG+%Kb+!
3ff+W
fi>C/>
$Rff
-S3fifffu jkI:J+%' = /W\=3
fi L $0=+?
-fD#?= $"P=ff
Xff0 =
%PD!
fiGE;
fiP^
= UT(gj0.+ D,WE;#XJ !Z= #
fi>"P=ff
XffPuE;
XxcSZ)q#@ ff$).:;P= f%'+Xfi+E;
X
fi/G/>s%'+Xfi+ETF
fi$"#-G%'/rKS
fi/+: = J"#/Gfffi $+%Q jkI:fJP$
"#!
fi?
= J$+#"=V3-
fiP= / $A#fi+EJ[
n r 6 ;>?;<0 C CWm36A3 G;BD<ffA 24365 ;<3 G;;2\36<E}h UT(gj3 G;>25 3RL5XY

<h|5 247YMC 5 2GFh

;?>;2436M0 YE!;8YA <ff;!0 Y8:0 <365 0 Y8(YZ02WS7A 24360 5 24;C%5X2m3 G;SF<ff084V<ffAA 36;C%0 3
5X3 W;Y

k

G/,b)G $"P
!a bE=M
fib/G$+M]##@(fffi$f?+
'31ff+*:9b$Rff
fiS3-D-
\.
&
-.C= Y"#P$
fi G . =V ff5 f"#D#3
fiC-DA
fifE=ff
"P=f
-T#fi$"#.c+
DPb%'/= %D
-;'
55bff+V
~b
fi$"#
fiYS
fi.:"P= $"B
-%<"#/Gfffi $H/a
fi+
X

X%w #:S+s= E
<P#N $= <ff*5<IW.#@ fffi
- D;c"#&
'ff
fi Dcff+C/G$+K#-$"#
fi
= ff*yzDP,G= T!$+."P=,%PD!
fi$5*4/G/?GT"#3XfiC#X
fi$>J$."= 2H"#DP+e!.+
=+Q
K .+ff$GDSXfiJPY"#&
'ff_Gff,?= %'ff
fi351v;=ff
F"#$! F,
$+."P=>P.+ bP=+<E;
xX(DSXfiJ
&
-K, (ffc
fiGb .+ =G +
-, =?
fi/G#tL
fi,P=
E;. :ff b=E;
XX* )\+?
fiffNff
fiJ/G ff_%8
-/G
-VG . =VE;
fiP= ;#@(fffi
fi
= ,+$M+%= W .+ =*5f jIyzM
-.+
-2Hffff
fi \$+."P=P.+ =bP=ff
)
ffD$M+<y< P$ff= 2-N.M$+."P=*5
v;= K<"%'1"#/Wfffi $)%'+X-SEM
fi$"#!fi%'/4/W/>b<+MP= K%H"#4=+k G

fiff
fi
'3XXG .;)
= }h\
: K*

h \
: UT gj*f= M%PD!
fi$[

n

r ff6Vk;SWMO)F<P084<ffA>A 36;C03KhL5XY Y;vDMYXYE!;n8YA <ff;C


+EE;b"+q!.+M=
fiff"#
-V"#
fi
fiVCfi/W/> [
n r 6 Df0{80 <365X0 Y8Y0 2Oh 5cWkDMGY YES;n8YA <ff;C 024C9h 5cWf3 G; 8:0 <365X0Y 8(YZ02!F ;2\;<ff0 36;C

*

h 3 G;>2=3 G;,WMeF <ff084
<h;

v;= s%"#c=+9hb
'T"#
ff$\W$fffiM+%<GP."#
-?%P/8h/G$+c=+P= Jff
}h>:\K*EW"#&
ff$4:F$ff-
fi
fiuZ"3xPT) ) ( \%'/ E=ff
"P=xhV
E
.+$LJP= f+ffG (ff(hK
N
fiLP= V"3X+) ) k :)5v9!= SE =+Rh yz
0W!0V<ff;WMGY3AffD |24A 2\C ;36;><@B5 245cW365 70YXYE 1<ff;36<ff0 7365 2GF0V7A 5X7; D<ffA @
<PA>A 36;C10 3
L5 YXY;vDMY YZE,;8(YZA<P;C%0W L;Y

.+ =?
'F%ffxfi>#@ fffiP$?E_b $AG= +E=+
5}h




;
&
fi$4:

5;= J! .+ =f) +
fi ff
fi f+}h
Q%'ffXfi?#@(ff-$4:

h

q 5;=SX*+%

yz"P=ff
XffA3 G;<;=(h+M%'ffXfiW#@(fffi$45
3
{

fi

yz{z|~}a



v;= MN.!;
;P b$"+
) ) k :, #+$;= bffm
}hD(
: K*4
b: Ec=ff
"=f/W$+
=+9hYE;
xX1ff3X->)Y
&
-$45Tv;= G$"#q"#
fi
fiA
;= C
fiff"#!
fiV=ffff= $
5bv;=
=ff
fi#"#
fi
-+/G ff.?uff/G.+!
fi L=+\.G= "P=ff
XffP $0ff`
) G c
) k :f"#SXfif $!D9 h yzT"P=ff
XffPTff#N $Z+:4+ ff;=+M= $G"=ff
xff
E;
XxP= />#fi$M%'ffXfi?#@(fffi$45
v= bN.c
;$&
Xfi?
XN$4}
[ ) G G ) n ( G
fi/W/G$
+#->"3X
=GG %0 O@? =
/w3E
fiq"P= Dff$0."#3:bE=ff
"P=
'f#@"#fiP= Z%' "#
fi"3X-$ffb
) 0ff2
ff$A=
/w3E
-= \N.!Vff"#5 ^HO= E_. :M= Z E (ff$ .$%'(
hD









k









) G) n :V+PJ#@"#fi= DC=+bE;ffAG #+$D
)

*: E=ff
"P=\D
KS
fi/J+W
hyz"P=ff
Xff*5
h
fi.!#X%_+C b
%M= G"P=D
xff\#
fi ?%'ffXfif#@ fffiP$4:*3X1= G"P=ff
Xff\#@"# M%
= b%'ff
fi;E;
-=
UT(gj\.+ : +A= #%'bff/G/>bE;
XX*)b%'ffXfi?#@(fffi$4#
5 h
-.#X%*
'
%ffxfi>#@ fffiP$?Df /W
fi*:ffE=ff
"P=V"#"Pfiff$c= b ff+%K+%<4/G/>!
qe5
79
fiSXfi\E;> $Zff/G#+?= V"3Xm
: w h.'bff3X-Z."#.C\=
.+ =*yfff$5r79
fi.+%>3X :M= \N#q"3xY
G .+P$V+ff0+%G= %'/
}h (
: K94
b:1+ "#$&
fi \+\DPV+%;= C%/ }h (
: K94
.+P$b+\DPV+%;= ,%'/
}h ( (
: K94
J:ffE=
h ( $!D.= b."#
fi?+%<G&
fi +-"#!.3
fiffF%P
/ h5
v;= <"SX[
: w h.'P= #%K .+P$wc$R "#F+% D!
fi$4+%DP= K%'
/ }h ( (
: K*4
b:
}h $ 4
: K9
b
: k
#[
: h : K*4
b:<Ec= j

'JP= fD /,W+%sff$"P
'&
fi 3
fia
h 5^HuP=ff
,$R "#
h ( hT+a
h =, "#P.3
fiff.5Z7e = /G:<4/G/?Z\#X'Ja=+,= V #+ =
ff$V
h (
'c%ffX-f#@(fffi$V+\4/W/>
qGP#XcM=+M= ,$M+%<= R
h< .+ =b
%ffxfi>#@ fffiP$f;E;#X 5
v;= N31R $!
fi,
QE= = }
h : bff'+GE;
fiP=G G"#.S
fiD#:3
K $"#$P+
X-YP= TffffXwff'+
ff#N $f+)SJ)sff+GE
fi=b<P=
fiff
-
3w+YN31<G= M&
fi +fiM"#.S
fiDK2
ff
fi
fiff
-
3D)#%FN3H.5Z;Bff SE]=+<"SX9
) G G ) n ME;
XX ff3XfiJff#-
3X8"3X
fi B +\3X1.ff
fi Dc=+cE_PY ff$\c= JP$fffiM+%< $"#!
fi >W= $+$5MI 2
ff / _!; !K=K=$ YP("P
'+$sX
fi B)8+a.ff
fi ffFH=+KE;T ff$GE;
-=
V"#P$
fi f= $+,"#
fi
-4_/a
fi =ffJ+ $+s
fiT
h:*= +E;$:*++
) G G ) n
E;ffW ;N?= /V<
5 h :e= *:ffE_ff>"#D.S
fi> ,/GTP."#
-G
-:ff ;E_ffG
C= bDffXff+*5
Z\"+LN @u=ff
',$&
X- =*:;#
fi= ?DL$Rff
fi
- Z= fx
fi .+ff2'!
fiS3/>"=ff
fi PL
fffiZffbE;
fiP= )ff / G!,+"#.3
-D.6:4,D
fiP
fi q#@ ffX
"P
fi,"= $"PB

fi+
) G ) f=+M/G+$b )ff / Cb+\"#P.3
fiff.cE= Z= G+PY
/Gb !
fiFGP."#$5
v;= ?%'/GW/,
fi =ffG G)qff$&
-.+fffi[= fx
fi .+ff+u/a
fi =ffa"#D.S
fiu,P=+>ff*y

fiff
fi
SXXa $+Pa!M= b D3: ;_"#/WM
fi>=ffwff-$$
fi C= /
fi?= bff+?/G$+
= Tff'+ < $W <P2-
fiDP(ff"#M= /
fiDPJ= ff'+*5Kv;= '+K !
fi,
8
fi #@ &
fi:(

"#SXfi,
fi/Wfffi/Gff$W
fif T"# ff5;IWI$"#
fiV 5 qb%';%' =
"#&
-f+%1=ff
'F
5
h
P= JffffX8ff'+*:P= a"#/Gfffi $c ff+%K
Nff
= $4[cE_G= +E;$fP=+
/,
fi ?=+
"3XX
fi
G 4 h;%ffX-V#@ fffi$
fi.M+EZ . =*:1+%' = P/G, .$J?+P=
,= .+ =*yzQDT&= TffffX4ff+4K
fi ,=SXw ff$F#fi+E0= M+=f+M

fi$a
fi>P=
("#$P5



#'<3'H'&#$'$K3<+1P'-3 |7!K'''3 '3'
nS

fi yz){ {z {z0{





/


k vln

mez)zx 0 yz{



zy yz"z{

E



rzl 6 l

/>+!
"P
fi:9X
fiBf"#/GfffiP $:
,\E;2'+PY"P3
-/V5Zv;= ?N.a
s%P/>3 [,=YP= fff+
. =G
FC#tL
fi?= ;E;. :=+F= T)+X
"#W+%* .+
- (ffy;+ff.KffG/>B
fi ,
ffP/,
fiff
!
"F KN @ $>"P= +
"#;+%1b"#
fi!
fifHGKP= $+*bP$+fi:= G #+
fi
= \ ffyzV"P=ff
Xff0ff`+ ff-
fi L3Xbff&
fifffi\E3(W]$+-\=+f"#
fi
fi`/W$+?=+
+fffE_?

fi"#cff+V ff$M $!DM 2-
/G =ff
"sff+5v= a!$"#V"PS
fi/
';=+M=
#+ ,%'$+#"=ff
fi W= b .+ =f
&
fi#_Cfff (ffC/Gb=f"#5
v= N.K"P3
fi/+ ffX
-$ff !1c= %/>3(ff#Nff
-
fib+% = Fff+b . =*:Sc= _ /?+
"P
fi&
+%< jkI>ff"#$c, +b= J />
"P
fi,+%<+<5
v9?
X%'f= G$"#\"P3
-/E;, $fffiVP= +EO=+c%+ffV
3ff+*
h?:wLE;
XX
.+Pb=+;ff+,"#5FZJff/G.+PT=ff
'F
fifE;Y.[

Tt<6W

n
r
<ffA>7;nWW52GF,0 2;>2436<E!AffD3 ;DA<@
F ;>24;<ff0 36;C%0F 0 5 2





h

h


L5XY Y24;?>;<70 MW;9h36AH;

UT gj*

h

v;=ff
';
c C$"+!G .+!
fi V G: UT(gj*
"+!$ ?yzT"P=ff
XffPV?, .$E;
fi=
UT(gjZ#+ D:*+\f*5 q!""#$&
fi, ffG=+b .M .+P$VE
XXK=$a!
"#->/G

"##3
fiff.F;/GMX
- B(;P=+(hG:+?= P#%PbE;
XX* )b
!/G =ff
'"+5

6x<ffA>7;nWW5X2 F 0 2^;2\36<EAHDV3 G;BDA <@ h K*4L5 YXY[2\;?>;<70 W;Oh 36A H;
F ;>24;<ff0 36;C%0F 0 5 2
Q("#$P&
fi }?
h :(K*4"$OhGyG+PDOh Z\ #+$LE
fi=u+K*. +xhGy
&
fiffX
- DJ) .$,E;
fiP=>BUT gj>. 51cP=+#
h
'24A 3 .+$?+ D3
-aK=ff
K)+
fiff$5
cb%' = ;#@ &
fiW+%1Y
fiffX
fi C+%K
h "+?FM
/G =ff
"FRG
h :ff&
fi"#M= GE;
Xx
X*
+9fi$6,= _!#fi$"#
fiC+%M+-
fibM= "#
fi!
fiJP$+fi$sE;Rh
+s
fi.*"P=ff
XffP*5
)
fiBE
: ,&
fiffX
- J+%Kh
"+G_)TP#N $GCc
/W =ff
";PG
h :D
fi"#
-<E;
Xxw
X*K%'/
h <-$
fif= J"#.3
-D;=+.+$}h %P/
fi.;&
-ffX
fi D5


n



r

v;= P#%P>sfi \Jff+
C J#@ ffX
"P
fi!fi\DP$Z= G%'ff
fiCE;
fi== UT(gj
+ K*f.+ D6:ff
fiME;
XXK bG"#
ff$/GC=+\"#5 "#3xfi>P= JNP$AX
fi .+P>ff*:
h :
Q
fiff
fi
S-Jff$W>P= R TE
fi=G)
= UT(gjV+
K*,.+ D:ffYq/?$G"#
ff<=ff
'
+
'38ff'+\/G,=+\"#:+A
M= #%', J!
"#-V(!/>+
'"+5 PV= b
3
ff+*:)= +E_$:ff
c .+P$Vff
fi f+
fi#+
Xf+%<= sfifff
fi
: : Ec=ff
"= .+P$
$"P=`%b
fi.,ffGff-L"#:#
fiP= V
K*L
UT(gj15I= \+ . = 2H$+."P=`3-
fi= /
'
/>+!
"#@"# c%;= s%"#_P=+;
fi;/,
fi =D"#&
'ff;
fi.F
-ff
fi
34ff+fE;
"#5

;h

!1"4"


" m@ X["

"z |"#i +

\(''@

0=ff
XfiJP= G fffiBV%_ M$$+."P=\=M]ff$\P>= G .+
fiV =,+%;= ,ff+ ff
fi
("#$P:4
fiM
'c
fi/Gff&
fifffi,V"#&
ffb=ff
=>"#/Gff-#fif
-V
'++!
fi+*5J^&Z=ff
b$"#!
fi\E;
"#&
ff?= f#@ $"#$ #Nf%s .+
fiiaE_#xG/GV fiA
fiff."#
-SaE;
+ .+!
fi>+?P
fi+3 5K79
fi.;E;b"#/G+bP= J"#/Gfffi#@ff
fiG+%1ff'+V + .
fi+>E
fi=?=+;+%
ff+V .+
fi?%/"#.+#"=*)=ff
;#+
fi, +
ff$TV$
-/>+b+%K= +E"P-Db= sX
fi .+P>ff
/Y!K/>+."P=,= T"# ff_
fi+
-b
fiW.ff%'< + .
fi,b);%K=+W .+
-*51c#@(
E_W X
fi W= SEffJ+?$
fiZP
fi$%'/ +<yzX
fi .5V79
fi3xfiVE;>ff$"#!
fi
/GM
fiff$!
fi C
fiff."#
-SF) E;= b "#$$c+%1!
fiS3K+f + .
fi*5
nG

fi





k



Q

tmnrir

yz{z|~}a



. @l

X *ff+ .F=+;!b+?"$Q%"#T= M%' +/Gff.3* fffi/+%8ff/a
fiff
fi ,E=ff
"P=>ff'+
+W. ff JPJ!
fi:
55X:E=ff
"P=>ff'+;"+fJ"P= $+fffi> + $?C= J"# ff; fffi/V5^&
=ff
C$"#
fiZE;> $!D,q
fi/Gfffi>3fi &
M+%= f"#
fi
-b ff,E=ff
"P=L + .+!
fi+Z+%T+
#@ff

fi G"!M
FX
fiB#fiGG)b/Gb#@ $
-
fi;P=+f .+!
fibff+ ff
- 5
v= J&
'"b
'ff$,
c=+b+b+ffV ff:* + .+!
fif=M#@"#fifP= Y+/WY
fiM 2
+
-bff+ ff
fi ): fffic= , P ff
fi>PG."#TG P
fiTff$"P

fi*5v;=ffP= Y$+#"=\"#
.+"P=ff
fi ,%H"#<
'; b $F%M + .+!
fi?=+?%'; .+
-bff+ ff
fi )5
)Db=_P= b .+
-T #+"=ff
- ,%"#K
'
bfJE;PB
fi Cff+f+%9fi *
= #@ff
.

%'G= f Er fffi/5L^H=ff
G"!:;= V"#DG% .
fi]
U
5ucSEr D!V=+W=
X
fi .D2'P
fi+3/G(ffff-qP ?Zffu=+f"+\#@(Pff$uPiE;B
fi Zff+E;
fi`
=
+ .
fi=ff
'J"#P$ =fffi\= f
fi
fiZ+
% ECJP= >ff'"#/Gff
+% M$
fi+ !
+,5?v;=DC + .
fi
cX
-B#fiVPf,%HbP=+Z .
fi,ff+ ff
-
E=
7 `




v;=ff
'K
- $R3x
fi,

&N$?E=





G (

]fi

?
fi"#$$:;= Afi D+!
fi= /
fi"#P$$W+E. ,X
fi/,
fi,+%J 5
GP= V .+"P=ff
fi %"#(
v =ff_!/>3X ."=ff
fi s%"#.Q#@e"#F= $+$!K ?W= #+
fi5k~F &
fi"#M .
fi
;
ff+ ff
fi Z3-/GDG3fiE3 b=,\ ."=ff
fi %H"#,+%b+Cfi$!qZ+L
fi"#G- = q :E;
"#"Pfiff,=+b + #+
fi?
';X
-B#fi? #%#+fffi,E= b= C
-S3K/G fffffiJP b>"
=+P$Rff
fiP$T+c/GDT
;/?+Df/W(
XNw"
fi;c .+!
fibff+ ff
- GE;ff?$Rff
fi5
"#++
fi>$!
fi/>+f $.C=+,=ff
,"#P$ GZAN$x
fi .+ff+
fiLE=ff
"P=u+
/GDcG
+%8P= Y"#
-+s
fi+
+5F0=ff
XfiCE_,"PBD +E;fi$ff b=+=ff
'T+Sfi(
F/Y!
;.+BJ-DD!#fi:$E;F#X
fi<
-1 +
ff$#%'ffD
-Dff
-
fi4YP= "2HRSX
fiTP$Rff
fiP$JPT/>B
+ .
fi?E;=ffE=ff
XX5

fiff kR

nnPlnozr

.

rvn

;/G(ff#*+%*P
fiSS+? + .
fi+,
Q$W>P= /,
!T=<= b+qSfi
-= /O
fi.#x%
.+P$
fi.9x
fi .+Cff+5K8+K .+$,ffG+q />+!
"3XXJ=3M$CE;
-=G= /3X
= Gffff"
fi$c
fiff ff"#$f
-qP= J "#$M+%< ff
X'
fi >P= Yff'+*:)
55M3XK+%K
fi.M"+3X
fi B
+f"#P.3
fiff.5
~D+%*Mff+*yz9 )D&
fi
-1;S+
'+ff
XX
fiU$C)#%P_P= ff+C
KP$M
-G= FX
fi .tuE;
ff>P= JS
+ff
Xx
fiU$+
X?
fi\G Pfffi/G2H)$"P
XNw"C/>+ $:) M= J #31
' J%8E=T+P.;+%
bff>CS+!
+ff
XX
-U">)T
fiE;$><b Pfffi/%1#@(ff'++
fi 2'$, .SX
fiU$+
-S*:ff+C


"#$?ffGM$ +2_#XX
1+?~\"3_+P=D?.$ff1+?ff>b+/, =+/G
+?M$ +M.$ +.5
)
fi #+b!
fiS3
1cE;2Hb "#$[ +
-YM+%
fiff
fi
'3(+s D3("#
fi
fi6:$= 3- 2

fi= /N.
ffD!
XN$;= C/GD; /a
&
fi CX
fi #+Gff*: = qffff$/GJ=3X-SE0/G
XNw"+!
fi
G/>+BT= Cff+*yzQ
fiff
-
3*+f ff31"#
fi
fi</?+."P=f=
fi .5
nS

fi yz){ {z {z0{


@

//

mez)zx 0 yz{



zy yz"z{

e

`8) K 04 ffLI

v;= AN.G =!q%=
-S3T P("#$W$W#
fi= >`+ ffX
'"+
fi 2H! ffX
fi$]/G= L>
ff/>S
fi 2-
fiffffff<3fi
fi= /&
-/,
X+9= F ;$bffYb+/, =+/W+
(+C-;.$(
G#fi$"#"+
' +bff+65879
-.<P=
fi DS<+PT/>."= $f D3
fiF= b$"P=Gx
fi .+Wff+*yz
DS:*+\P= GX
-+Vff'+ME;
fi== G $$bff /Y)J+%/>."= $C+,
'ffD
xN$45>v;=ff
C"+
$ff-M
fi\/>+ffq"+
$:*&
fi"#?.3Kff+b"Z/>+."P=*:*+\f&
fi fi,ff+Z"+\/>+."P=
fi
\ff /,?+%s
xPDGE3 5Lv*"P= ffDq/G Z= fP/>3
fiff
fi ]3fi
fi+$C= q3-
fi= /
#@+/,
fi $C= ?/>+."P=Z) E;L= W
fiff
fi!
3_"#
fi!
fi5^HG"#/G P$b%'C$"=uSfi+!
G=
ff /Y),+%T)i"#
-
fiC"#$+$Dff"P
fi = WX
fi .+PVff+*yb
-ff
fi
3_"#
fi
fisE;
fi=
=
fi G
-ff
fi
3T"#
fi!
fi5v;=ff
',

fiDPff$uPZ/G$ PV= \+/G ff>%ff ff
fi E;PB
$"#$+PL\ G=
fi ,
fiff
fi!
3E;].+P>= V.+Pf#@()$"#$D]= ?X
fi #+Zff*5
^H>"# D.a= fD /,)>+%b)`"#
fi!
fis%G$"P=L
fiL+"= ffD$,P= fff+LE;
fiP=L=
/,
fiff
fi/, /V:ff $B
fi W
fi$;+ff
fiP.+
XX5

//

b(b*6j(Rb19Z4 b1eKUuI3Lj

T3
fi ,/>+."P= $fCX
- .+Gff'+*:ffN
- J
-,= J E

fffi/
'&
fi/Gff-[

5;^&.+ff
= cX
fi .Gff+?E;
fiP=>= MS
+fffiMff
fi
fi ff< P(ff"#$ADGP= b/>+."P=+5
5[iff"#b= sX
fi .+PGff+*yzF ff31"#
fi
fi;E;
fi=?= b EO D31"#
-
fi5
q 5MF$+,G E)Z"#
fi
fif%'T$"=\ DS1 )D&
fi
-f=+b+ $.c
fi\= J E D3

! ;
-f= MX
fi .Gff+*y; D3135

5[iff"#M= ;X
fi #+,ff+*yz9
-ff
fi
3w"#
fi
fiQE;
fi=,= E0 fffi/yz9
fiff
fi
'3w"#
fi
-5
57e;$"P=q"+S*X
fi B?=+,"# /G$W, )D&
fi
-G%'/ = b+W
fiff
-
31"#
fi!
fi:
X%
P=+b )D&
fi
-\
C+ffb%'/P= G E
fiff
fi
S_"#
fi
fi:*= Lff#fiW= Gx
fi BZ+
fG"#$!
fi W E\"#
fi
-*5
57e<$"=V"+P34X
fi BG=b. ff"#$GC D&
-
fiW%<= b+'G ff3"#
-
fi:
X%=+
PD
fi
fiW
;+ff;%'/P= b E D3':ff= Vff#fiC= MX
fi B45









9nt~n iv,;ll !t

tQvn:ozrzl omno9v

v;= G3fi
fi= /+Yffff$M ? ff
fi f+%_ )ff/ C[c= JffqP $\"+Z"#ff.3
fi
M=T#@ff
$]# (ff"#$f"3KX
fi B %b PD
fi
fi
fi\= CX
fi #+fff+*y ff3Q!
=+G+f G+G%T= V E D365c"#V= AN$]ff+u"u"#D#3
fiX
fi B :_!:;+
"##3
fiff.;E=ff
"P=Z+? + D!fiQ
fi#fi++ffT?= G"# ffb ff-/V5 %"# .:4 ff
XK=
+ .
fiG3fi
fi= /"#SXfiJP
fiQ
9
fi/G)D&
fiff-JP#XE= = F= $T.+%*= ;x
fi .+
ff+JE
XX("#P3XfiM C 1PT)<#%'ff 5^'%4/G+$bff
fi M= FN
- "#$:+= ; + .+!
fi
3fi !
fi= //a
fi =ff
'"#+=+F
fic $ ,2' .+b= JP+/GJP"# $5
v;= JR $
fiV= #%'J
$;GE= P= T= sN
fi ?3fi !
fi= / != ffVff#-Y3x8"P=
X
fi B :$)ff
3xM/G+
fi ;/>+ff!1J"#P.3
fiff.9 "#!S+!
fi<#+ ).:= ff
fi
fi$3;= /
fiW= Tff'+,= ff
fi JP=+<= ,E;
xXff3XfiC +;#%'ff8M F+ D"P=4 _
"Y$&
XfiJ"#!"#K"$9
fi,E=ff
"P=J#
-= <. M%'/>9E;#X+C= ;= %'/>
fffi5
nS

fi

yz{z|~}a



$?++b+,
-DP$
fi
fiDP."#
fiY)E_?= ;P.+ ,+W + 2
.+!
fiYSfi
- = /5k : y4."#
fi,3fi !
fi= /
P= <
-D#;+%w#@ &
fi*:+E=ff
"P=Y/W$+
=+
fi\"+fffi0P."#Zff$"P
&
fiVP=+f
fi\/,
- =D=3i"#3Xfi0/>ffiff
- `#@ &
-*5
: iE
XXff
fi&fi] > .+\ )ff / ?ff`*:uLX
fi #+]ff+
"#ff.3
fiff
- J ff / FX
fi B K;F"#ffW <=3)f ff"#$f
fi$"#fi,ff,= + $5
^'%<
: : a/a
fi =ff_ _)J+fffiMJP."#3X4= bff+ ff
- aff$"P
'&
fiQ
fi?= cX
fi .+PYff'+*:
+
'M= #%'G J"#/Gfffi5AI+
fi"#,
fiJ" JP."#J3X< P
fiMff+ ff
fi \ff$"P
&
fi6:4
fi
"+ F."#<3X= ME$C"PBYPb= ffffXwff*:D+,P= #%/>3C%S
Xb#@ fffiP= D!
fi
ff+f"#5
i;$"3X4%'/ I$"#
fif 5zC=+F= M."#!
fi>3-
fiP= / $ff$a
fiI$"#
fi?C

fffi,"#/GfffiFE= G$Y
fiY"#+ "#
fiYE
fi=Gb"#++
fiKN
- P.+ :+83-+
-#X
DC/G
X%
fi M=
) G G ) n b"#(ffc
fiKff#fi$K )ff / <!&t`= 1P=+
=
fiff
fi!
31+? DS9!;=+MffG ; ff"#a,"+P3*X
fi BtL%'/+ff>ff?
fi P5




k

+i }e!

+

(''@

~D?
-?E;BL"2'$0ff+ ff
- L=f"#"#ff.+$N
fi L
fiff#@D
-
"P= /G$C%'J= fff'+Zx
fi .+:E;
-=Z= ?
ff$=Y
fi q]
fi
fi \+
'+>"!$
E;ff/,
fiff
fi/,
-UG= f $%'G + .+!
fi*5Z"L = #fi$CfP= %./GE;B\
+3fiffUJP= + #+
fiV"#/G ffM+%<= b />5v= YP3
fiTP.+ +
-$F
fi"Pfiff$A
fi=
9* / M+/G/G4:K$ ff#:ff%T#@+/Gfffi:*!$"P
X%'V#+&%'/>+&
fi;=+b"+\>ff$"#/G2
D!$W
fiDPY$R "#$%8+
<ff;2\;_m
<P;>36<ff0 73)
fi/,
-
fi$5 T+3-(&
'K +$F#%'ff4
fi>E;

X*ff_E3 [
5;^&!= SEM,= +E9* (y,
fiff#@ff
fi L+LP3
fif.+P +
X$G"#ff\#@(ff-+
fi$
fi= q
%'.+/WE_B,D? +
'
fi G=
!
"M$+."P= 2H"#ff+4
fiff%'/>
fi*5
5;^&>ff/G!.+$a= SE+<yG&
fi/Gff-V"# Pq"+L)q!$L3fiffUV/GPV"#/Gfffi#@
+ .+
-qP.+ +
-$:*+\fffi!
fi/>+#-V"#ff\)>$q"#/W+G3fiP+
fi,=
fi$
%1ff+fS
fi$5
?.+ME;
fiP=f$"#
fi /G/>
fiU#
fi f9* (yJff$&
fi *5>v;=
fiLI$"#
-]D5z?E;>"#&
'ff
Z

fi.;P3
fiTP.+ +
X$!$R ff
3xfi: ff$"#/Gff&
fi G= /
fiffG+]..5I$"#
fi\D5 q, +$
=+J9* yzbb+%<P3
fi.c
c
-"#/Gfffi:*ZI$"#!
fi]D5V
P"#$ME3(cf"# ff>9* (y
=
!
"F
fiVFyzQ%./GE;B45<I$"#!
fi\D5z,
P"#$c= SE0 M+3fi &
;"#ff?)J#@ ff$
= P.+&%'/>H
-3wff'+ ."P=q[
R*TQ(
Uw) Kw,I+
-/G/G:4$ff.5





{

rmt

rir

.

lrzl



t!ti9*

9* V$,?N2H.+ W ("#$Pb%'Y +
fi \+Z#@ff

fi ff+ZPq"P=ff
fiG E ff35f9*
N..B$CX
- .+Gff'+*:ffN.F
-,= J EO ff-/V: +V&
-/Yff'+$Q
fi.;#@ $"#
fi*: &
fi GP=
E
-ff
fi
3_"#
fi
fiC+ D36V
5 i; =ff-$B
fi ):89* yzs%H3
Xfi $C"#$!q\
ff+fE;
fiP=V+Ffi$;
/w$Et`,= $+P $WX
fi B?; $"#
fi
-*5
9* #@ <!$9%PE.,+C"BffE+.,"P=3
fiff
fi MJ+3-DU;= ;%H3
X- :S
'"#+
fi M=ff
fi
X
fiBCE=+TPqPT"+!$VP= b%H3
Xfi :4+AE=+M D3= D,ME;Y!
'"P
fi 5Tv=
$fffi9
*M"+3 E;Bb"#$)
fi MM= _"+P3Dx
fi B("#P"#$bffJ+W
fiJP= _ P("#$
n

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

+%ffJ .
fi*5 W+>P= #%<)%/>9= FN. E;T!.+ $4%w*9 (yzK + .+
-b "#$

fif= b P("#$c+%Kff+f #+
fi*5
9* P= Y!$= "+3D#@ ff+
fiC!#fi$"#1 _%w&
X@ J $!$C
+ D!$:S"SXfi$
v FFID"= Bw:)$ff#5 v Z3b"#ff.3
fiKb!K+%wS
fi8!.+
fi$&tuff+C.+ %P/>+
fi+
=+M/,
fi =D#x
fi/,
fi+Pb= C%3
xfi #t`+A$"=3
fib#+ ?=M+qP("P
'+$fP$TP"P= $"B

fi.,+ ffx
"+ff
XX
-H59* yz,v I+V
fi
ff$
fifffNV"P$6[G%H3
X- $Jff Z&
fff#*$"#.:
ff$&
fiP$G#*$"#.: &
'ff%'$+ $6:Dff$&
fiP$,%'$+ $6:(>P>+#+/G.651v;= Jn b1DT"#
fi? 2
$ff.+!
fiJ!$,D,+qffff$K <

fi ff
=J)E_W $"#%$+P $K+C= K D&
-
fi
+ffD$M M3Xfi+E+#+/G
-U$f:4?fffif= sN.E;>"PP$%Qv FTJ#-S+ff
? T+3-(&
'5;^HZ+ffV":)= $C E;>"P'$M+b= ,/Gff;
fi/G).D$:)&
fi"#JP= V""# ff
%/GC=+f=3x%<+%8P= Yv F5v;=
'
fi"#
-fE;q?&
ffC#$"#M+V?ff$&
fi$A#*$"#

;.S
fi =ff% E+#4[1&
ffb#*$"#.c+M.;D!."#
fi
-F=+ff*yT! Mbx
fi Bw:ffE=ff
xfi
ff$&
fi$A#*$"#.TffW=$Y, )DC
fif= Cff+*5M+ .SXfi:ff= Y!T+%<+
+bP3
fi.

X*ff<
-f= JE;G"$5
%P1"= ffD&
- Tv :+*9 ;
fi.+ff
$= _P("P
'+$c3
fiK. +
X$4&
fi c= _ff.S
X
+%1= J"# PD;ffff
fi ,.B45K7 ;$"P=fDP&
fifffiM3
fi9* G ;,P$;GM
X%1= M3
-<
'
+ ffX
"fffi: &
fi ?= ,$ff-+%<=ff
c$cG
-.+ff
3b= C3
fi357 M#@e/Gfffi:)= J$!%'
+q."#M3
-T"#$!
fi fP,
fi!
fiV%_\.Ec=ff
fibBffff
fi =ff.fP<=+ />+*:$(<E;ff
ff/,
- bE=ff
"P=V"#ff'f$P;= Jff$&
fiP$f )D&
fi!
fi*5
79
fiSXfi:$9* $+% =
!
"Kff-$4. Bb= <++
fiw
fi.+ff
$P3
fi.:+"P= DD!$
= M$$: +?+ ffX
-$K
-$5 "#b= Tff'+G
'K.&%/G$4:+
-K
;&
fi/,ff$,"#b+ D3
-*Dff$"#
-
+%<, E0%H3
Xfi b..F= Y"# "PfiJ+ EC5

fi ff
{

lrmtQv

rmt



rzl




!ti9*

?+%1*9 (yzFff?3
fi;.+P +
fi$KffJ _+ fffi,C _nbeD2-x
fiBT"#!
fi, $ 2
.+
-*5K7 #@+/Gff-: = b3
-=+T3 #;= Jff .+!
fif+%<+V"#
-?
F
fi ffX
"+fffiJ
fi"#
3X< b'D3"#
fi+G! /G$f? ""# b
fi#+D#+ &fi5Mv;= JP$b+%<=ff
M$"#
-qff$"#!
fi$
= b#fiSD;3
-.+?$ff"#$c= /P>+Z
-/,
fi
fi$5
7 *S
fi.* EL9P= /w$E;$Mff*51^&J$"P=J"F= <ff'+M%S
Xfi <"#P$
g e
PaYX
fi B% F= $+P $fff+ = Tm\ 5
5%
ek@Xt LV ErL3%P1\T=YE
XX_P/G+G= V&
ff2'#*$"#, )D&
fi!
fi
Y* #%'JP= J"# /,
fi ?*:(#:ff
'<#@ $"# $45

5}} @kefi#t O] E0=V"=+ $f+0 ff$&
fi$0P.3
fi,
-D] ZP=+
/>."= $F= C D35

% ek! k e#t



q 5




'"+5



fC E!>P=+ff#fi$;+? $"#$yzK ff$
fi$f"P=+."#!
2

w6.;;Pff
fi 9fi3'KP6PSP-#$<P'<;#'<ff$''-#;'SP *3H)z*-$<'3KP''1P$3.
ff 3' 'M;H#!M'$&Vb#$!
*P''HX6QP-#$
*'JP$??'$C''<#$bPS!-
4b #
-$<#b'$;XP'$'K

fiJ'SP<+'HJ'
"!#";6P$6$3%
$Jc!P$-.C'$'P$<'+KPJP

fi 4$3.P-#$<P'K'H.#cM$#
&'!("Q$#!3<3b'$'SP1#$KPS!-c$ '-fi#+
$#_6'#
n\

fi

yz{z|~}a



v;= $G= P>S
fi.J,
ffff
"3K%'/ +<yzs.!$"#
fi:8
fi"#nbeD?ffD$C ,
2

fi ff
!=) E;Le $"#C%'$+ $G]= ,D)$,+%b ff&
fi
fi5]I+
fi"#VP= f3
fi
.+P ,
;P= J+/Gs
fif= JP= J"$:(
fi;
; "P-$+TP=+= J
!
fi"#
fiV*9 Y/>B$
E;f $"#.+W%'$+ $ +
ff$c+DW#%ff<"#ff+*Bff +E;fi$ff c
fif= $,"$5
"P=b+%= $F3
fi#*"#$! 9= Q
fiff ff"#
fib+%a.E=ff
fiKBffff
fi =ff$5z ""#/Gffx
= 2

fi ,= $b3
-. />
"3fi,$Rff
fi$;P."#
- YP= b= $+ $Wx
fi BG= V
- >
Ex
fi B\ (ff"#$]ffZ E !LH.= J=i&
-/GfffiZ
fi = > EP4.5Vv;=ff
+q"+f&
fi/,ff= $M= c.+&%'/?+
fi+KE
fi=>s."#P2'#N !$R "#:Sfi= =

fi
fi3_P."#
fiZ/,
fi =ffJ)f $ff$L\#X
-/,
fi+fff$"P
&
-C=+Gffff$LL=

fi +
fiSw= $ $WX
fi B45
5%)+*--, '.0/@21'ctoc
fi
'ffJG?
fiff,E;a
'
fi"#=+; P= "=ff
fib=
ff$
fi$?$ff-.;+%1P= J
- +
fi3H5
^&+/,
fi fi :)
fiM
'b"Pfi$+b=+C= >PqPV?ffX
fi$:D:*/,J)G ff"P
fi VE;
"3X
fi B :*&
fi"#,
-
b""#/GffX
!=ff
fi >E;? D!$5Gv;=ffb+u"+Z#*$"#b=ff
MP3
fi
ffJP."#
- b= = $ $CX
fi BWHE=ff
"P=>+ P/>+
'"3fib/G+$K/GS
+fffi;ff
fi
-
,.ff
- Y"#P.3
fiff.#:
fi ,b E



2

:ff+GP= >
fi Gb EX
fi B%



2



5

g vE_,#+&%'/>+&
fi<Pff"#J+f#@ff

- a!?
fif= Cff+*5
5}} @fi3*e .54761,7/@!18K
Htu^&f=ff
F"M= ;%H3
Xfi F
FMX
- B1 KE=ff
'"=,
'K= $ $
ff` = q!
E= D!ZD!."#
fi
-
Y*
? ?
fiD+fi$
fi+ P= fX
fi B45
v= f3
fi,
',Zff'"#m
4bE;
fiP=u+ = G*{
: 92 =+?ffD$*y G=3+
Y*W
)D."#
fi
fi*5
5}} @fi3*e .54:*@fi
;.0/<, , =.tv;=ff
'%H3
Xfi Q
b%9E=ff
"P=J#
fi= K=K+,, $"# 2

-
fi1
O<E= D!T $"#
fi
fia
; P$Gff>C= $ $,x
fi Bw5Kv;= M3
-
fi
P.+&%'/>H
-Gff"#${
FE;
fi=VC Ef=ffff$; c=$
C $"#
fi2
!
fi*5

v= $,.+&%'/?+
X;=3,= ,$!bff
S1%'M +

- LE
fi=q!$+."P= 2H"#DP+
= !

"65~F=u%Y= $\S
fi.?/>+Bq]ff"#/GD\<P;>36<ff0 73a%+xfi+E<$Lff <ff;24;
Pq?x
fi B\
fiZP= >/,
' fi>%c\"+P3_ E;B4V
5 i;$"3x := SE;3:*=+,+0fffi\/>+B$
"P=+ $?P= %'
fi $Z+%,= \ E_Bw[]+fffiLP."#.?ff$"P
&
-?=+?=3\
= ff$"P
'&
fiMff(
fi f\= /V5J7eM#@e+/Wfffi:w"#&
ffb= C%'+Xfi+E;
X G#.
fi
%P= (
} @fi3*e .54#6>,7/@!18K
,#@e+/Wfffi+5]I fff=+C= V"# ff,ff+L"#D#3
fi
E;>
-
fi3Kff$"P
&
fi6[= Gff$"P
&
-V?$.ffX
=\f"+3X
fi
B 4> +?VHP=ff
;
'E=
"$?= c
fi"Pfi&
fif+.
% 4, +
fifE
fi=48f3G,ff$"P
&
fi>, $"#=ff
QX
- B,%P/
+ = ;= $+ff
- a!m
5I+
fi"#b= M+P"= +
'"#Jff cf= bW#@D
!"#b+%
= cX
fi B4:ff= bff$"P
&
fi?G
4,= Tff'+_! %{
4 +?Y"+ ;)b."#P$
ff
X*= Jff$"P

fifG P$"#F
fi=VP."#$45
/Yff'+
fi+M+%(P= #} @fi3*e .54#6>,7/@! 8K
19} @fi3*e .54@*\fi
;.0/<, , ;..+ %P/>+
X)E;ff
$!fffi*
-
>;+f#"#*).+
fi4%'+Xfi3E;$MDR
><P#N $:+E=

*P= _ff /,+%
ff)ffDbff$"P
&
fiF
fif= ,"+3* E;B451^&V= J"# PDM+Z
-/Gfffi/Gff.+
-S*:ff= P
n\I

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{


F J%H"P
Xx
fiJ%'F=ff
FD)b+%1/>"#PY.+P$: c= $J9* J#+&%'/>+!
fi+_! $
= J
xX
fi>%_
- G= G$R "#Y%_ff$"P
&
fiMP."#$
fiVP= Y"# .!Y%<
fi +
fi
b= c%
- :D+W= ?ff3
fi b= $bff$"P
&
fiKE;
fiP=>b "#$;&
fi/a
X+Yff!
fiS+!
fi3
+3fi f_+P #X:$
q
dK#fiD>;+ #X :4$ qeG
dK#-D:4$ff.5
J3
-"P=+ $= Jff*yzFS+
'+fffi2'ff
fi
fi ?"#.S
fiD#5
g

ff5}} @fi3, Er $"#W
G !
fi $Z#X
fi/a
fi+f ffE+DP$]%'$+ $,Ec=ff
Xfi
/>3
fiff.3
Xff
fi Gff$&
fiP$f"=+#"#
!
"5
v =ff
93
fi<"+,)$,J"#$"#KMD /,F+%?%3
xfi $[4= P$+ $sX
fi B :3
fi"#
2
;
D"#!.3
fiff.: +A=
fi+ff
xX
fi,> )b+f\"#
fi
-Vff JG P$+fi++fffi
= $+..5G+`"#ff\#*$"#J= W3
fibff\."#!
fi f= >ff$"P

fiZ=J ff$ZP= >"# 2
.3
-D.FH/WDKx
fiB#fi,= J
-
fiG%8,"+34X
fi B)8?#Nff
fi G,&
fi/,
X'+_ff$"P

fi?=+
) Va E $"#$5
g v= \.+&%'/?+&
fi,/G
X%'u= \ff*yz?/G).3J"##3
fiff.:;#ff
fi ]#@D
!
fi
!5
5}} @fi3*e @.k>4-@Xt`v=ff
M3
fiM"#$) cG /W
fif+A$Rff
-$; ?2
P."#
fi*5
5}} @fi3*e @.k>4:Ae1#t`v;=ff
P3
fiK"#$) KPff/G
fi,+,3P$Rff
fiP$
P."#
-*5

5%!B/@@xt`v;= G#ffb
fiZEc=ff
"= E;VPJ+WV)>
b.$45fv;=ff
C"+Z
""#/GffX
!= $]DLP."#
fi = qff$"P

fiuP=+> ff$= V!
fi +
fi3#ff
fi L+

- YP= b
fib.ff!
fi 5
v =ff
'Y3fi &
b%s9* Z3

fiL , ff.!.+
fi Z%T.+ %P/>+
X3;ff+ .Y
fiE;
;
E$ 579
fi.Q
fi_"P
XN$K9* yzF.
fi*: +

- YJ
fi/Gfffi#@ ff+
fiG%w
-.KS
fi_P.+2
+
fi$C+Z= +E;
fi E=+J.b%_.&%/>+
-Sc
-Y"+"+ Y""#/WffX
=*5I$"#
fi
3(Q= b ffE_B,%'K
fi"##+
fi G9* yz;.+P +
fi$
-D,+<yz; .+
fi3-
X= /
fi
= M%'/+%K"#DP+*+x
"P
fi$MI$"#!
fi\D5e.5






n
kR

.

nPlnt~nzv v





9*

\$fffi?+%Y+3-DU#
fi L9* yzGP3
fif.+P +
fi$C
fi0+<yza%'.+/GE;B
?ff/W.
fi
=+,*9 (yzs+
'"#ff+JC+%3
fi.

-"#/Gfffi#tuP=+M
:= ?+>/G>"#/,ff
fi+!
fi
+%<x
fi .+fff'+b+
fi C fffi/?c%ME=ff
"P=9* fE;
Xx8)G +fffiGP> #+>fff
-.+fffi
$R "#J+%K3
fi#5F<&
ff3:ff%_#@+/Gff-: = J"+S8"#P J= +E?
fiV79
fi T)5
/G\=f.ff
fi u"#.3
-D.W$P
"#?= Zff+P?L= \N PyzWfi#%'2'2'
- =D
.ff
- 4
fi\P= JE;. b )DGP=+b= Gff-q"#&
'ffMX
fi $+
-U$+
XV%P=ff
Mff+Z..
E;
fi
= ffCP= V
4=
+?J*5<v;= b++ETKff P"+P3X
- B(: ;fffi,E_Mx
fi B(Q=3
+)#fi$fE
fi=f= C ff&
fi
fif (ff"#$HP= J= #+s
fi#fi++ff.5cI+
fi"#!
4;ff#fi$%



+
$Rff
-$Q
fi$:
fiK
'_"P-$+_P=+{
41P= $+ ffC+ E 5;I+
fi"#B
F?G"# /W$
YD,:eG
=
YDd+P#%'ffw#*$"#.<,= = P$+</,K/>."=W +%19* yz<ff$&
-$,#*$"#_v F5^HW%"#$:
79
fi G
,"P&
'"G#@+/Gfffi?+%= H
Ae >I/@3*@
;./<, , ;.Zv OE=ff
"P=]=CfffiZE;VS
fi
n\X

fiyz{z|~}a



P



Sa



Goal

b


P

St

u

79
fi M[<9* ,3
-."+ N @Aff$&
-$ff2'#*$"#Tff?%3
xfi $FE;
fi=?=ff
c"+38!"# 5
#+ +
fi$6
[ >
ek@1+U
} @fi3*e .54@*\fi
;.0/<, , ;.D5F^H?+
"#ff'+$:9* b
8%ff
ffW%/
fi
= R} @3*e7.54761, /@!18K
bS
fi
fi"#J= ,= P$+TP$fffi.%P/?ff$&
fiP$V#*$"#C'%' ? ; b
&
ffb#*$"#$5Kv;=ff
';/G$+F=+9* GE;
xXw M"#&
ffPff"P
fi %4E;
fi=V,Pf=+ffD$!*y
ff#fiJ
D,:1Z= ==+b/>3V)>P= Gfffi\E3VV"P=ff
fiGAE_B
- fff+*5>v9V?=+
=ff
.+&%'/?+
XW
"++ff-J+%K$fffi!
fi C
fiq,E;PB
fi ,ff*: C=+;= Y"P= +
"#C+
% F?G
)JP= G D3</?$\=3,
fi"#P$"#$5G^&= JE;. 6:)
fiJ/>3?D
fifffiGfPff"#
F?GE
fi=V+ P= T=+ffD$c M$Rff
fiR
YD,:)E=ff
"P=fE_ff?/>+BbP= b%H3
Xfi JW&
ff2'#*$"#
%H3
Xfi c
fiP$?+%<Gff$&
fi$ff2'#*$"#;%H3
X- : +?E;ff?+fffiB
4y;ff"#/WD$5
0=+b+,= C
fi/Gffx
"+
fi;+%_P=ff
M$fffig _Q+ff-V*9 (yzc
-"#/Gfffi $Pc
M+%_/a
fi
"#$R "#:<$!$"P
3xfi&
fi"#fP=+J $"#3yzb ff3_EC (ff"#V=
'
"3xfiZff$R
J%_
-ff#@D
fi \+.+&%'/?$!
fiZ. +
X$M.+= C=+%/>3X-V
xNw+fffiG3fi
fi= /V5
V3fi &
QX
fiBc=ff
F
F P= #fi$F
fi!"#
fiT&
fi"#M
fi;/>B$F $"P
bE=_P.ff+1;9* (y
3fi
fi= /d/?+B$5^H<"G;
-"#
-<J;
B GEM
"#ff83-
fiP= /
! 4:+
fi"#/G2
fffi:) /?+
"+:fE=+bff++D#+ $F
fi#@( P$&
fiC+E;M#@()$"#$%'/?+"#
+ ff3
fi $CDG"#
XNw"P
fi J/G;%'/>3 )5Z#x
fi=<+>3fi
fi= /Vyz%'/>3 P 2

fi$F +
ffb b+%K,D /,)+%KE3 F, ff..+A= J3fi
X= /VyzQ=3
fi$:ff TffG
"#
- M= Jfffi!
fi/>+M.+ +#?D?E=ff
"P=V+qSfi
X= /yzK+3fi Y!= ff?c ff $45
Z #@(? ]= Vff
"f+%J= +ErZ\= \9*
fi>#+ +
fi$aE;
fi=ff
fi= q
%#+/GE;BGPY ff
ffb= + .+
-Sfi
X= /5

u t9* XlrmtQv

rzl



tivrv>



n\
vl 60v

= =ff
fi = $afi# :;9* +u#+f
-LL
X*DWE3 5u9* #+.CE;
fiP=u
G
"#/GfffiPfff+uP=+,%H3
XCP+
&%'Z= D3T+$W.+&%'/?+
XCZ #+V E
"#/GfffiPff+*5<9* ,ff/,
fi.Q J !
fiG+%1C+!
3ff+f+, ,#@ ffX
"P
fiF
fiG+%*."#
fi
L"#/G/,
-/Gff$5<ff.?=ff
?E
fi=uP= D"P=0.+B0ff`+<:ME=ff
'"="+0#"#D

fiKff'+ ff
fi Yff$"P
'&
fi*:ff$ff-
fi
fi>G
fi"#/Gfffi#fiG)$"P
XN$Wff+*5Kv;=ff:ffJff+E`
E;
fi=]$+."P= 2H"#ff+<=
'
"b"#P$
fi V\9* (yb.&%/>+
X6:4E_W $]V"=S
fi
P= Y+<ycfi("S_#N @ ~$P."#,ff$"P
&
fisV#*$"#,]H /G?%'/ +P$f+%= Gff'+
"#b?+ = $5
v;= ,&
fi/Gff-$E3>+%<
fi
- >+]=ff
M"++ff
XX
-
P>#%'/,ff J<yzc 2-fi#8"#DP+
fiD,%P/s $ff= 2-N.!<#@ fffi.+!
fi,+%*= b"#+%*ff&&
fi J,R MF
fi
- JR +
>?ff = 2-N.Mc
fi.+!
2Hffff
fi fff = 2-N.b$+#"=H&
fi fG."B).5F^HZ"P=q?"= /W
) n >E;ff? Wfi R ,3X1P= J EOff+;P $fffj
=G G %0 O\?
fi$

fiTE;ff'q"P= ffD,= .$!.f""#$bff+]H&
fi f/WY= !

"s.+ B
fi ?
fiff%'/>
fi+4
#@(fffiC
fi$:)fi$3
fi >P= aSfi+P$\= ?."PB?%M+M#@(ff-.+
-f
X%<"Bff."PB
fi > PS$
$"#$+9
5 ) G G) n k :?E;fffff?X
-BE;

fi=f= ,#"#$f ffyzM&
fiffX
- D5v;=ff

n\p

fi yz){ {z {z0{


mez)zx 0 yz{



zy yz"z{

/G
XNw"+
-G,+<yzF 2--#"#ff fiD>#X
-/,
fi+$P= $W%'b +-3wff'+ 2'S3-+
fi
=
!
"+:ff&
fi ,
-$?= M%'+X-SE
fi b%' = ffB F%=
!
"T"#DP+*BD +E;fi$ff [

5;`= = O) G) n k :f "#$ff W
b
fiZfffZf#"#$:4=
!
"C
fiff%'2
/>+
fiG
F =DcG$Gff$"P
ffJEc=ff
"=Vff$"P
'&
fifW."#35

e5 '% <) G ) :M .+P$9
fi. ELff+9
-K$=
!
"J"P= DD!_Ec= =
Y"#ff
fiff M."#!
fi G"#.S
fiD#9%/P= b+ff__Ec= = ;,#N J,"=ff
X'f ,
X%

fi"= ffD$;P= M+3:DEc=ff
"=V
fiffX
fi ,GP#N +.5

})

qe5

n Jx
fiBE;
!$;=
'
"
fiff%'/>+!
GGff/,
- TE=ff
'"=?V"#
fi
fiW
= $+ $Wx
fi B!= ff?, ff$!$WN.$5

)5 %'<) n #+$
fi.""#$ff+
fi$9= !

"9b#fi$"#E=ff
"P=Y&
-ffX
fi
Pa"#D
-D b#Nff
- 5
<&
ffJP= f.
fiZ%T=ff
,ff = 2-N.!GS+!
+ffJ+%<:K +
fi= W
fiff
fi!
3<NP$Zff'+
.+ $]=^K9i+ UT gj85 i;fffiq+ ffX
fi$:;&
fi"#=ff
G"P= +
"#fP$Rff
fiP$Gff$"P

fi ]E;
."#
fi0+#@(
fi*5ZZ"#ff'u"# ffL*9 (yz?P3
fif.+P +
fi$,ffu/>B
fi ifffiZ
#@e/,
fi ?= "# PDCff+*yz,"+P3P"# :?=+,"# P>\"= ffDf+L+ !
+
v :+0"= ffDZ]S
fi!.+ 5 fff$"#
-$
fi0= \
fif!$"#
fi:;$"=P3
fi
. L"+0\"#(ff$ >]/?"#Z).+>+%JP#N $V+."##,t = $Z"#ff'L
E
fiPL
fiffL] +fiSXXu""#$&
fiff-q/W/Gu+P$+ .P$+\ff`! $R ffVPfffi$5
>
-
@1P3
fi</a
fi =ff_#@ +?,C E;2H!!$R "#[KP."#FX
fi B4:P#N MX
fi B4.5 i;fffib,E;ff
"= ffDZP."#G= AN$ff+*:;= ^
i;fffiLfE;ff"= ffDf= ff-$/GAX
fi B]Z
."#$4:=
i;fffibbE;ff'G"= ffDM= T"P=ff
XW"#$)
fi ,,
fi JP= P>$"P
xN$?D
=
>
-
@43
fi$5 K=ff
)+
fiff$:+= ;/>"#Pb.E_ffC=$_)"#/Gfffi#fib#@ $"# $45
I+
fi"#M=ff
Q E"#DP+wP"# M$Ffffi,= b.+ +#G+Vff'+>/W(
XNw"
fiG.+P.
+ffffi>P cE= V= ,M+%8Z"#
fi
-+A= $+ $X
fi B +bffffX :) ff $

?/>3
-D.S
fi $45I+
-/,
X+!fi:Wfi ufff = 2-N.A
fi.+!
fi2Hffff
- u$+."P=0
?$4:M=ff
'
+ D"=Z P$$,+<yY"#/Gfffi $65\I(!/>+
'"P
fif
C
fi++P$VffZ= ??+%
fiP.+
X2
ffff
fi 0$+."P=*:;= +E;$:+=
?+ = ? fffi/ E;
fiP=` />
"P
fi& fff=ff
'
+ D"=f;E;#X [9/,fffi
fifffiM3
-.;"+ ; $"#$+!
Xfi?b!%P/G$W
fif$R "#5;v;=
fffi/P/?K%'/= M%H"#;=+3x9* ,3
fi.Q
fiff+-#N $?/GDF
fiff+fi."#.
%+xfi+E<$WDGP#N $L
5 KK$:ff= MfffiGff'+F $?ff>,"3X4R
) n aT#+ $
UT(gj
+?=ff= f"+ c=$J,.+ %P/>+&
fiW
fiff+fi
fi JP."#
- ffX
fi$?GP= /HE;
fiP=

fi+
X G />+!
"P
fiH).5v;= $+PG,.3*)D&
fiff-J+fi
-F,=ff
; fffi/V[
g oM#3>/G
- Y+ff>P3
fi.F=+MffG F
fiff+fiM#"#
fi*:ff"P=V}} @3*e7@.
4
@++
} @3*e7@.
4:A@1 fi:1 ff
X+ = bP3
fiJP=+!
C A;nWb."#b=J)i 2
ffX
-$45
g %'/3xP."#
-Q
fiff
fi
S-:#%'C
fi G+ff>#@ &
fiV + .+!
fi57M
g ^& C= %K*+ UT gjq#+ D+\3xfi+EP=V#@ &
fi\+."#
fiV+b+fff (ff5
0=ff
Xfi;=ff
'<+ ff"=G"#
XNw"#$<!(P/>+
"P
- :+P= = c
=<= TffS+ff.+ $%9!$+."P=
"#ffs
fiP$"#$D0*9 S2Hfi\.+ %P/>+
XE;
XXb+1?=
fi"#$$ &
fiU\+%YP=
+.SX$."=V!"#5K^H\+ff"!: = J+ ff"=V!
XX4 +.+ff$"#/Gfffi $65
499SPz;*ff'';&-<P4'_'3<+3H-.b'&$$$'bO!(
N

n\{

fi



jQP

l

lntQ9!t

yz{z|~}a



fi

nrmtmr v v

v;=ff
b!$"#
fiZ"+\fffi\B."P=\= G)D&
fiff
xX
fi
X$%M
-D .+
fi W= ,
ff$M%_.&%/>+&
fiS
ff+ #
fiffM= T+A%./GE;B45*7 c$$+."P=,E;
XX
-/Gfffi/GffK= $F
ff$F+,$!KJ
E= = bP= VE;BVMP= P
fib$"#
fi\ $!.5
fi/Gfffi/Gff.+!
fifE;ff\3!3xfi+E
+ff
fi,
fi$QS3fi
fi M= #
fiF
XX
fib+%*
X*ffK3
fiK=
'
"5Z!$"#;=+
} @3*e7.54761, /@!8K
<+
} @fi3*e .54@*\fi
;.0/<, 1, ;.bE;ffs +
ff;= c $+$!1 ff
' +"#: KE;
E;
XxP$;=ff
;)#X
fi#%5
^&<E;ffW3b)
fiff$!
fi b,ff ffX
"+c _+Sfi(
+%19* b%'<P= <.&%/>+&
fiS
ff+ #5KZb)#X
fib=ff
'<E;ff'G)Y,.S
fi =ff%E_+#,#@(#"P

fif/>D>"$5<7e;#@e+/Wfffi:
= WN.G]=+!
RTK(
Uw) KwI+
-/G/G:;$ffT#+B$CE= Lff +
fi ffZ
s\ ff
X]
"+SP"# ;X
-B= ff
X 5I+
fi"#
RTQ
U46 Kws'X
-BT9* D$;b
'"=>!<+%1P3
fi
=
!
"=+K/>."=s%fffiY"+SeP"# $:E;!$"#F=+K= ,"+GMff$"#/Gff$C
fiff
+*2-X
-B
fi/a
fi
fi$K<E;b9* yz5 J
X"#fffiC
fiGP=ff
;+3fi &
E;ff?"#"#
RTK(
Uw) Kwy
/G =&
'Z"#!
fi;E;
-=V/GP
"b#*$"#.65JI+
-"#a<yzM b'DC $!D.
fi\ffD$M b3X-SE
"#
fi
-3<#*$"#.,& J= ff>"#/G P$ZffZ+
fi= /W
"s% "#!
fi#b?N.J!E;ff'\?
Z92HfiW."#!
fiVPV=
K4*G
TQZ ffP=DZ#4:;$ff;S
Rff
jT ff=ffZ
Z#4:ff$ q(ff+ .6510=ff
X-
Kw9G
TKM=+fi$K"#
fi!
fi3#*$"#.+b ff
fi.3RD
xNw"+
fi*:

fiJffff$b C/>+."P
= R*TQ(
Uw6 K4,
fi#@( $P&
fi $6T
5 Rff
jTF:*= +E;$:9=+fi$s/G
"C#*$"#.C+
"#ff
fiff "P=+ 5
U;WV

9

}^'<@@z +





Zb=A E;, DS
fiV"#ff"#
fi ?/Gff
fi
"S9!
fi$[
5;PT/>B;/G; $"P
!_P= + +C#@ ffK+%)$ff >P=+<"#ffCc$3X
fiU$CffJ&
-
x
fi .+ff2'#N;ff+ ff
- : +
5;P"#/G+V0 exK0 b+/, =+/W+
T-$:T3ff.:E=ff
'"=L .! $>&
-/,
X+

'ff$;E;
fiP=ff
fiW
X*D;ff ff
fi ,%'.+/GE;B45
v;= cE_B,f XQ"PfiD!#fi,+.3X-# <+E*[= MBC
ff$
fi>)=>"$Q
K=<J 2
+
fiTff ff
fi G3fi !
fi= /"fJ!$?> + FX
- .+Gff'+:ff +
'ff$?=+M.+K= bff
BM/GC$"#.A+%8P= J$!F%'
-."P= +
"#$:\ ffF= Jff "+."#T;E;#X8
/>+BW#N /GffY"P= +
"#$5I+
fi"#f xKi0=+WG+%TVn beD2-x
fiB, $ff.+!
fi*:
E;E;T+fffi;CffX
";= #@
fi/WD.Q ff.+B>ffGb+/, =+/G+
wG-T#$ff
+f"#/Gb $!fffi.;E;
-=>P= #
fi. * 5
X





{ ZY

n



vlrzln



ntl

7
fi.,/GW"Bff 4[,= V xKL#@()
fi/Gff.sfE_f .3"P'$C+%fffi "Bff2H."B
fi
9
fffi/?:K+/G$\[O L +][O L_^ 5][
,+
fiff ?H.+
fi f%'/ qL$ffff$
fi +
- qP=
/,M+%1ff-("PB(_
fiD+fi$a
fif=+c fffi/V5
v;= sN.b"P:e5 F
5 ` L :ff
fiff+-$+A
fiff
fi
S9"#ffN .+
-?
fiVEc=ff
"=\3X1P= Jfffi "B M+J
= >#+fffi>+]"Pfi$+35qv;= ? DS<
fiZP=
[O L_^ ff-/>M
,3'V ff
X]V#"B+%= #
fi =ffb
[*:
Y3XK= Gff-("PB(b, J"Pfi$+b= ,.+fffiC
fiff
fi!
3XX?
fiZP= $, fffi/> t`/GGff-("PB(b"

w6_-#WKX#1F' 3'-#b '$K6 '$1+X4b'$K 4<''';
-c

k $

fi yz){ {z {z0{




B

C

mez)zx 0 yz{


B
C

J

K




L

3BS

zy yz"z{


K
J
L



5BS1

79
fi J [Kv;E;1~Qfi "PB(E;!x?KPfffi/>
W\\+%<$"P=Z= 35Jv;= C
fiff
fi
'3Q!.+s%d LZ^ : %'b#@+/Gfffi:)
-ff
fi
3xf #Mfffi("PBT?
L%Tfffi "
B w5 = J
[ LZ^ fffi/?J=3fE;VG= f3
-.C+%Tfffi "B G#"B$\
fiff
-
3X-:
= =V= ,+C G
fiff
-
31."B ;+%K= CT/WJff-("PB(6579
fi JG!= SEM;
fiff
-
31+N3
.$4%'E_M#fi$"#$C ff-/>K"#/Gfffi;$"P
xNw"+
fi9%'= L
LZ^ fffi/>K"+C;%'
#Ec= , M+ B(cZ#4:4$ b+/, =+/G+!
9-$:*$ff.5
v= \ exKu#@ !
fi/Gff.C
fiff+fi+$]"#/G+!
fi Z= fff $yzC%'/?+"#Vu fffi/
E= = fff+LEC .$%P/ "#.."=LE
fi=
fi.C%'/?+"#VE= LP= q+-
fi]Z
/>SXfi fffi/Ec$\MWX
fi .>ff'+*5%e L,gf L +H` LB LZ^ +PJE;G#@e/Gfffi
#@
fi/WD.6587e;#@e/Gfffi;
: ` LB LZ^
fiD+fi$K"#/W+
fi ,=
fi/GM$Rff
fi$WJ #+b
ff+W%!+fi
fi J=
LZ^ %'/"#.+#"=?E;
fi=W= b
fi/GM$Rff
fi$a%!+fi
fi J= h
L_^ ff-/
.+P
fi ,E;
fi=fGfi
fiW%'i
` L 5

cGP=+J= $!>#@
-/GD#b
-DfiG= f + #+
fi\ "#$,fffituP= > fffi/+%b2
fi$"#
fi G+\+ !
+cX
fi .Gff+fEF M"#&
ff$45

/% J 4D+j\bDb* TLj
j

I$LjRQKkRK

Z!
fi$,M
fi/a
fi.+c XK*yzF P$D#+
fiC + K"Pfiff#fiGff&
fifffi[)=G P$D#+2

fi)=3KE;; $
"+$6: F &b+
= G) F w:#+bE;< !
fi/,
fi
-8"#
fig: k F =ec F &c F =e
+V
k:c F =- F &GG5 (4
xKY!$K=ff
fi#+."P=ff
"3D P$D#+
fi*:6
fi"Pfi
- 2' !
fi/,
fi
-_"#!
fi4#@ $&
- J"# 2
#" .cX
fiBf.f fmw:eN.b #+G?ff+V"Pfi$+{w:4P= q .+G?ff+\f"Pfi$+{4:
= ]#@($"# fP= VH !
fi/,
fi
-S+9kc F =- F &c F =-q"#
-*5zFyzC $ff.+
-"#&
'.
fffib+%wff$P"#
fi
fi)%1= FE_c
fi/,
fi!
fiK"#
fi651v;= "PfiD!$1+3-
fi,+f=ff
fi#+."P=ff
"3
ff/>3
- 2H$"P
XNw"cBff SEfi$ff c
K=
-G+%1$+#"= 2H"#ff
fiff%/>+
-*[1+ ffx
"+
fi 2H ffX
fi$
% "#!
fiM=+JffP/,
fi ,E=ff
"P=\ (ffa
fi\= G . =q%<+
S8ffMV"#&
ffb #@ $:4E=
"#
fi<P,
fiff ff"#:ff
fifE=+;#ff$: = +E $"#
-
fi;+bPG,"=ff
-$4: +V,*5
l\*$K3.!M'$.'c$''' MKP<$+;+-oH*$6ff.6+13S3Z9n9'<;-#$*HPF$
3f P[
4&z8P;.P-#$'6;z:.M$#'$,.6?+m1nasHFq$M$,HP<$ S#$'
G'3bP +-;'3' ''W
KP<3SP_+-
oo*3 #*-# &(
wc.F''?'$ '#
.3_'3' 'HP-#p
!-": '$''H-.?'b3&$3<'.H!1$#> ##_). $ #$M'SP;$| S$
am1nGP$#'$]
f3 M$#M$'#$MX.c$#!V ##;1'B
4
q r1s u v m1wYPM'$JP$'$JP<#$J6
$#'S#HP' b'+96X.'P-#'_'3<-.b3b$'# L3| S-#$
k

fi

/%
j

yx

yz{z|~}a



TQj4b:TQI>)j*TQa;b*XTQj

v;= C
fD
fiM"#$!ff"#?E;] exK*yzs=ff
fi.+."P=ff
"3Kff+\ P$D#+
fi\+
+<y"#ff+4BD +E;fi$ff :,= JR $
fi?
-/G/G$
+P#fi>+ff;,E=+T"#DP+4
fiff%/>+
X
E;Y= ff +
ffs
fiVP ff
fi >P= Y#@ !
fi/Gff.5Tu"+V#@ fffi+
fibff/>S
fi 2HffffffJ"#ff+

fiff%'/>
fi+G
-f= Jff'"#$[
5;>ff$"P
ffC= SE G/>+#"=fe $"#._
fiV= WH +
fi41x
fi .+?ff+V D3
fi;= C $"#.F
fi=

fi ff-/VyzQ
fiff
fi
'31+? D34%'/>6:
e5;Gff$"P
ffJE=ff
'"=f+P
3*ff+fPa"#&
ff #@ $:+
qe5;Gff$"P
ffJE=ff
'"=f+P+%1= C+
'31'
-"#/Gfffi+Fff+f,E;BGf #@($5

v = bN.cff
fi$"#J+%<ff/>S
fi 2HffffffJ"#ff+4
fiff%'/>
fiW
fiff+fi$F= +EO,N= sX
fi .

ff+?,= b E0 ff-/V: (+( E=ff
"P=G
-Dfi$<"P= ffD&
fi G"##+D#
fi?=
fi ; ff-/Y! 2

- ;%'"#.D.Q
fi?= MX
fi #+,ff+*5ZJff $GP= J+/GM+x
"#>;
fb+/, =+/W+

+Vcfi$[;"P= DD!J= J !
fi
-f=+/?3@D
-/,
fiU$;= bff /,M+%K
fi D3*%'/>=+
"#3xfi>+ )$+
-f= J#+&%'/G$aX
fi .Gff+*:)+?
fifP= Y"b%Q,!
fiJ"= ffDC= J !
fi2

fiG=+F/>3@ff
fi/,
fiU$F= MD /,)_%*
fiff
fi
'3w"#
fi
-
fi?= c
fi ; fffi/=_+ )$+F
fi
= b.&%/G$WX
fi #+Gff*5
v;= , fffi/
'=N
fi fP=
fi/>3/>+ ff
fi V"+\)aRff
fiC#@()&
fi[c
X%<P= J
-
fffi//WD
-
] $"##T+= CX
fi .+Pf fffi//Gff
fi
$"#.:)N
- f= G$!
/>+ ff
fi M/>3M
fiff+fiK#@e+/a
fiff
fi b3X{z





<|

)D&
-ff
XX
fi
X$5v;= _3fi &
)
fi? M+ B 1Z#'4:D$ff

/GP.+$;P= Y)ff
31"#D!T+%</?+ ff
fi ?&
fi G= ,#@+/GfffiC+%<+fi
fi G= f L ff-/
ff
&
fi >!""#$&
fi#-G ;X
- .+Gff'+5<v;= ,"#/Gfffi#@ff
fiG+%<"#/G !
fi G= b !
fi/>3*/>+ ff
-
+ETs#@() D!
3Xfi\E;
-=Z= f&
fiU?+%T= WX
fi #+\ff+]VP= >)+
fiffYEc= !+fi
fi V= f L
fffi/&
fi ZV!+fi
fiZV#@"#-qP= /GG ff-/ JAX
fi .VffZ
C"#3xfiV/G
#@()&
fiC=+&
fi >?/>3xfiFX
fi .+P>ffZHfP= =f
-$Rff
fi$; f + .
fiV+M3XH.5
ZG PG=+b=ff
'c
J&
-/,
X+Mf=
MG365 Y5X36E 8<ffAY;@ ff$$\ffZ~A
fiDP
fiZ= >"#D#@ b+%
~;0~f
-D*:K$ff#5,^H !$R ff,#@()
fi/Gff.bE;,$]?=
!
"+:1ff/?3
fi 2Hffffff$:
X
fi $+P2'
fi/GM/>+ ff
fi ?3fi !
fi= /V: ff$"#
fi)$?
fi\ M+ B OZ#4:9$ff#5
"#ff+;+x
"#%,= f$"#]ff$"P
&
-P$Rff
fiP$J=ff
X%'
fi A%P/ $ff= 2-N.G$+#"=]
f)$2-N.GP.+ 5v;= G- JG
"#!$J Y#+ B
- f%' "#
fi
fiff.S
X 5Vv9Z"# 2
+<ff$"P
&
-M+%_P= G=ff
fi.,&E=+ /w$EO
-qP= >"# ffbff+\V ffP$M #@(6E_, ff
X-J
$+."P= 2H"#ff+= !

"F=<$ff
SXfiM
fi/Gff-/GDP$Gb)+X
"#,+%<# ff
XG#"B 9%'/P= P2
/r *5z0ZG"PBD +E;-$ff J=T= G
-
fi+%_ff/>3
-q)$"P
XNw",=
!
"M"#/GffX
"$P=
"#/G+
'?E;V+<yzF!%P/>+"#J+?=_%< XK*: cE_b+ b=+;=ff
;
fi!
fi

,P%3
-.,$"? exK\$=
!
"M
fiff%'/?+
fi?
-.#X%5^& xK*yzM",= Yff/>3
fi\2
"P
XNw"Bff +E;fi$ff cDBJ= c%/+%1C<+%*.BD2'P$ff"#
fi>P"= />.,F=+ff
+B,d~\" oM/W$:
$+ 4.+P= K=+G. B
fi M%' "#
fi:+ <)=> /?=
!
""#ff(Bff +E;fi$ff 5 2
%P +#fi:e
fic
$+-?
fi/Gff&
fifffiCV!$P= a"#$)ff"#GE;Z= ,E;,%'/>c+%
ff/>3
fifBff +E;fi$ff :e #x
fi/,
fi+P>#@
-/GD#:ff%'#@+/Gfffis
fi\P ~;+PTZ#'4:*$+4.:
= +Ed=T.BD2'P$ff"#
fiV"P= />+#>"+\ +
ffJff'+ !$ff Z=
QT&
- ff
XNw"+ff
;=+; #3
fi $?ffFyz;.+ B
fi ,%' "#
fi5

#.KP<3SP;SP-QP$T9$6;!!z'F'$S}.~ ~5u ;$'#WF4&zff3*W,'$cz'H'3'.*P$,
6'$''MS
fi6HSP Wo\+H'.D > &G
fi6H+ + 1N. &3B
9 '3D >N# 4X#wff $P;#
k $

fi yz){ {z {z0{


KPfffi/
q ~cI
q c
~
Gc
~
Gc
~
c
~
Gc
~
Gc
~
c
~
c
~
c
~
Gc
~
c
~
c
~
q ~cI
c
~
> c
~

G~cI
~cI

~cI

~cI

~cI
~I

~cI

~cI

~cI

~cI

~I
~I
~I
$
~I
$
~I
$ ~cI

mez)zx 0 yz{

KP("+5K!
fi/GCH/>$"3

xK
5
5
)5
5 q
e5z
q 5
e5z
5
5z
5x
$e5z
5
5 q
$ 5
$e5z
5x
q e5z
$ 5x
+)5z
5
5
5
e5
G
q 5

qe5
5
q qe5z
D5x
)5
5
qe5X

5



zy yz"z{

I)$ff \w"# 5
+
xK



G



%



q
q+%



D@




ff

@
%

ffe
q



e@
+%


ffe
e@
+%





3



q
ffe
(e



v+fffi,[;</G+.+!
fi)%/>+"#:1+\ xK
X

fiffA .


r4rzl omnV nzvil v

v= ,N.C= f"#+fi /Gs+%Tv1+ff-V>= +E= SE+<yzs%'/?+"#"#/G+$sZ xK*yz

fi\++fi Pb/>5 (+$ Z,"+
fif$ff.M+ D3
-_
fi >P=ff
F
fiff%'/>
fi?>ff.3E+ff> ff
"#"Pfi&
-<+) K= M#+!
fi+_/W
fi.+%*= c E;b+ D"= $[= c E;M #+/>E;E!
fi

fiu
X*ffC+ + $:K Lu
xPD,/>"P=ff
fi $:;+] #
fiP= GEC
fi/,
-U$]\ (ff"#
= J)$MD
fifffib.3E0%'/?+"#Jff /,.5 (/ c = #fi$P;E;J bP=+= ,++fi Pb
fi/G
/,).TJ"#/G+#+fffi[<]ff$AGE;BW%;q!/>3Xfi; fffi/>:4 xKV)M
+ _ Pfffi/>:ff M= J +.WffG $;=+#
-= .+/
;"Pfi$+- )
fi35
b+/, =+/W+
8+\cfiJ!$b xK*yzM!%P/>+"#JP#+
fibG
-.+E=3
fic
fi
.+!
fi ,ff+Q%P/"#.+."P=*5<v;=ff
'<ff /,$:)"3Xfi$W= B
W0 ?5 2GFWL8:;><P7;2\360F ;3:
;ff#N $f,
J
3; :4E= W
C
'= ,
-/GJ$Rff
fi$Af+fi,? fffi/:5 )5 ^ L_^ :e%P/P"#.+."P=q+m
C
'

= b
fi/W$Rff
fi$?G+-=+/GT Pfffi/&
fi G,/>3xfiFX
fi .+PYff'+*:ff5 5 M%%
` L 5
v;= M%' =V+N %P=V"#+fi /G;%Qv+fffiGb"#/GJ+L+ XKf=ff
;/W
"+5
v;= ?R $!
fi]= #%'>+!
$JbE=D exK*yzC$ff /,#Y>"#&
'fffiq!
/,"=C+P
fi,/>+ ff
-ff;=+>Fyz:++!
"#ff+Xba+ ff-/>:+GP= =>+!+fi
%'/?+"#G
'b G&
fi ff
XNw"+fffiV)$5\v;= f+E;J=JPqffVE
fi=]= > />6y*#
fi
%'/?+"#f
fiLff'+ ff
fi %/ P"#.+."P=*5 ?79
fi VZff/W.$:c exK*yzW%'/?+"#
ff .ff$c/,"=%Hc=+q<yzcV #+
fib#B 5ZC=3J ?#@(ff'++
fi?%C XQy
=3
fi$: c
fi.#$"#M= Y3
fi D2')."#ff.+ bff /,c
T"P-$+$[K= $!Jff /Y).b+b=ff
- =
$"+!V xK*yzC%'/>"#> .+
-G.!B(bff #ff$C/Y"P=Z/G>Rff
"Bfi\=+Lffff$
)*D+ -X#'3 F$FS'9X.Z!-G3SP*KP<$+;+-o9$6D.66 -. &
4[
9ff3 Wo9'$;;.3)>l3B91PGffPzX+[o_>;11>_>33L :&&1h>h&
3O :3>:>F@Q&1>'O3



fiO;;ff

1800000
1600000
1400000

CPU Msec

1200000
1000000
800000

SPA
PRIAR

600000
400000
200000
0
4

6

8

10

12

Problem size

9;h;O;3 F 3&T"F1BQ ; >3i<&;<9;

9>F Fi1Fi13; {"F3hW W;F9 <BO1B"39F3<Bi;SQ;QFp;;
9 ;FL-;19;39WJB< ( =TQ;%F9pBJ<""- o;
Q;%F;<;b+ B5<T;;& ; >3;;ZQB1>">;=L;FO+;
39Wp9O TO<&h3<hF9<>;F";JSF3hF3J1 Wa ";399 0F39;
<a3 F3;3h1B9h3F <;F; F9F1h; 1F09WW3;39;S<9

ff
fi


;



F;;<9

+ 3{ 3O

3 F1T1F; <>SiH9;"9T3;3 G

<B9;S%; J ; 1%<B"FL&



SF3%>"%5W 1;5%;h _19 ;O"

WB3F9<>;

1>1SF3S";;39;p;

;h;F< 9;W ; >9WF<B3

"!#!$&%'
(


;O3{p5&{ ip3><B3Tp3 T3&O1B9F3<9S+ 9;>bF

F;< W

)

399;J FG1WFS;3;3 -F 3&JF1WJFhO;W9SBp;=L

'

;QF>3{ 3%Oi;%FT1 19W;<9 W53;><9S;QF%;\TQ1F> <; O&
J <B"

*

FH3;3 J3B1B3F3<9F 3O



,+

W;;<95;W F1W+

213)4

;<9

;SFhF 3O 9;>GF9;;;h;<9 J

.-/

]F

0

i"FG;W39Wp <9H3S;

{;<Bh;;{ F;{W<9F";LF;O<TFh;;<9 J

)5
7

<S91hJF3B1<&39<9FW;S

H9S <B"W;<9 '399;1 F35

26

< SOF ;3W<9>TG;h1 F
;G31F 3OG< WF31F;

F 3&JF1h;G3O3 TS%p;<9

1WF><T+\F39; TW"F

9

T;p1B3%009;L1F9;F

8 :9 <;>= ? A@CB
3







;1 >1

FE HG HG
J 7J
L

O330 3

&{1 FO& &3h&O&:>& :3



FJ ffE

K

ff31':>SO&3>&3&:>p{ ff&0&3 :&


0 &h1h&:( ' %@i 33

fiMONQPR TVU XW


4

&Y [Z P]\ SY^ R`_[P\Ca

&;F ;F



;





dc

bSF3hF1T5

<B3W ;3995 ; 39;J;33<9>

; ;B<SW;19S9F"TB;"39{" -99;
Fpp;F

W bYS P

;

+

Q9;;

OSF19

"F39;h TFH1W

_(<9 >><B F;;<;'1B;O1;F 39 5>O %1F<F1pQ

<&%FT9W<9h;<9

<T9<%<&9OSFOF>1 39 ;h9 h

p;<9

O<BJF9;WS9;"9i<;SF +FS&;; 9FOWF">1 3 i;1; 5OS%<&
9;1J<BHO<BJFJ3<9><BF3p9H399;SB" O9;;;h;<9

5G1F39;J;

<;b+ >%3&JO9T;p3{S<B"F
=;;O

bFW; 1&1F1 <>\O J;G%;"9T;<9

9 h5;W<&JB
{9;F9

gc

;<9

*
e5

;%9;>

9SB"{F&{";F>{9

*ih

< <9

;0QL9



139

|{~}

S;FWF

F3&1<Bp9>SFGh;G ; >39T<B";;

< F F< Z9hF" B h+W >1ZS;F

F;;;

>

*ih

SFb<G 53;;SO];FL



9; >5FJ;i;5

f6
jXj /

< pLF S+ 3{ 9;>"T<&

kglnmo&prq&s out`vxwy z
4

b/

Q<B+ ]F;F

&339h<TF;

h;

j j /


Q F9 L

i9<3

;B<J9;>

<BFJF

]F &3aFS;T"WW F&59

;3 <>39O;WT09 F1WF 3O H;W3Oa";;5;QB;W;F< 9;

0
* j j b/



<B;<9;G093;% \F3h1F3>9<&F<339;W;< ;TSBQ

: g
| *


c

LOW;FJ5

j X/



Z



H %93

O53 <<9



F;T-

* j /


; 1%; ;FLF3QS0B<

>5S< >><B <B;;

*

;FL-<BH ; 5>"39ZF<F%;W39;TOFhJ<9;<B; G"W



1W<& hF -O; %{; % 535+ffB;"39F

:/



1B3F3<9FF 3O



;<;O<Bp>; 1 &1=<9 \;9 L1F93;1F939Fh;F<



<5

O1ff _ 5

V6


50

><F139S 1B Q&;%<BWF ;3 51h< >><B ff<1F539

|

F>F1;09;O9Fi <

> "9Z& F99;iB;"399;

<>3

<BO ;3 <>39TOG -<B ; 5>"391FB39;S F9;OF3 ;&1<
1F319<>

h

* j j
5

;FL-W%1<B p<B";<9;FSB;9bF1;OT"9LF>< Lh 09F"
;b093;

&T09

}

S;FWF

F

Q F9



FW;9<BTB;F 3p91W<9 ;

j/

;%

;0W3;F<;Q3<

>O%B9

FL;&>W OS1 >

%F<9<9JW;ff 3FBS<B"FSFW<13F<9O9<9];G139 ; 5>"39

26

<139+9<B%F"F

hBQ;9"h;W1=1h1B3F3<9F 3O <B;F 3

"F

;i1W< ;O;F 3

n * }

O;OOhW;";>39B 3 &F3p<B;<;hFT9<hF T1F1 ;SO9pFF9;
5;S3>" 9_B;<99;



x

;;>3F 139

%O%B1F3

j j /

iWFZ;





9< >"F9<19;J9Q9 J%% -h;;>3G;3S939 S9F1T" F3;J F&5

;3 <>3a"FG ;;Q ;3 <i9S<91; FO139Fi139F9J WF>1; <

&

{LO <9h<9L1FB< h

}

(3 5 p F93> 9 Q{ O1FB<

9L9< 39;hF<9;;3>F&J39F "WFh F1;<9W<L19; ;3O
F; ];9S930

j j /

139



1

;<9

* j j /

JB3B1F

195



g * c
ff

j / C | *

Q9

0B3b<139FQhWF<B3

ff0



%

{

19<3

{"F;1

<W%<B 3W WpB1; F<<;39;

399;F=9F9F;W9"<9;J;< ;


Q
&O9;9;J;S9S&39TW 3BhF

fiO;;ff

9;1 39 LO;3O&3;F%F 1W< 19p9;p9W;O< 19; 5 <;;
9BO+5<9hF%; a1<&JFb;<S;i3 G

* j j / c

n}

W% 3 &F3<B;<9;hOZ&L1="W<9O5

WWF



F

* j /

Q9 T"



u

1

BT;<iF39F%<BG;p<B;; -Q<T9W;<1B9F1W<9 F OF1

*

1H%;33;3 Tp ; >"T<BF9;HG;1p 39 "F];";>39

7bfff

;

<

;S<B

u/

&O11;3B

;;;SW<1LF113T;&1HFQ9

O<B3F 1 >Z;3W3;3 JO9G;p3FQ%<ff"F
3;T3;3

<5p;hJW;WSB31W5T oB<3 F<BpF9 ;9;

;< G{9F19J;<3<9\;;<9
;<

p;h'9;h9W W



;<&&&39W<& ; 139 ;W;

H

L11;3GFJ %1 O;

&<B";;

*V


Q

{4

"B< L19<

F3W<B";<9;W&> {a9SB%T -ff< 1<9;T ;3 <>9GF99;
; >9%<B";<9;h9;%;3 F1h"ffF1W<9 9<BT3 ;
%F%F"3B+9<&T99OFB%11<;O



4

W 59;9\13]Fh;5BQ9

<B

jXj /

1BZZ



&

&+<B";; {33ff3;

;<B9;;J 3;3

T3 %F

W<3LOh;;39O<B";<9;F;95 5BFLQ&T<&>W O"

HFWFHF9< W 3 &F3

c

<B;; >J;F< >31FH;

;<9

<T09F hh9<9 W <h3;3

;<9

39 > GO;J B919GF;;99H

;

F 1 9HF3&1<B


>

{;FFL +{Fh

139

<J9;'< F F< <T & 39

9393;TOF; ;B33B1<3>

4

&gl.xwp[rwA
JFT; 5;a"

393;GL\;;<

*

3 &F3<&;<9;GF3;

B<pFO;;";>39T; F<B;<9;W <&3<

/

9;>"S<BF B9Tp;;1O

;;;B19p >139;"BS<B;<;<&9FOFJ;9;p;
9>< QWTi;9;><BG31 G3i;;<
O;S93;



3;3 TB9'
F\;

Bi9W<9F"FJF%<&1h&T;F 3 {;F<; Z1W<9 ; ZF

'&3WTi9"i;WB39F139GF 3O

<T;'< F F< <

(

<T9;9F< + F< ;9393;T

c

;<& 39\"<J9;'< F F< <G1<5;O9<

<B9p&h3F<9;;S;<9



F 9;;FJ1F3>9<>

"%FFZ

hTF9SBW"1F119 <

" 3 &F3<B;<9;J9H;WF>1hFBF9<19;

3> L&%<& F9

;h1;F 3W 513<91%31<B3;H3;3 J3B1B3F3<9F 3O ]1W;;>"39F

0

L <9W"FS;%1; 5h%<BTQ;>p<BW b<;O9;;;;<9 G<FW&3
1WFp;L3;3 G(+ 3BTF1OhF"F;9SB"L;FL;3 G-F 3&T"F1

B F1;>"9;FZ;;OS;

Fh;S3<>h3;<B

FS9< ; O9<9H;W1<1;h

;9W<FJ <B"%;<9 JL%<BG; JO 1F<F1Z

ffO.7b:%X!$&"% % &Q$& u%b b"!
4


g}

%3;<Bp1WW <L;39 FhS;9<B 3F_ff;L9393;G-ffBT5;F 3

i9;

;F 39_91<9 ;LFp3;3 TB9'h<<;;13iT"{S939;o<Z
;

<<;SF3 F1W;3;F 3%;139Jb"

&> {Bp<B;<9;HSF3S<93T19H+W3;



;<9 S;B<10 >%"
9 i"F



3

9

093;FZhO;S09;p
9>S<9'H3S9< 39;

95 339;S<TW ip;<9 J"3F<9

;F<; h1W<9 ; hFo3;3 TB9'"H;19; ;1";



3 a9 <%S;<B 139<B;;





fiMONQPR TVU XW


}



"4

;





p<B;<9;J99;T <;O <%<B

oFB9i"F;



W bYS P

;

u ff &


QO <;;F 9%<J;19GFO

0

BO FW9F15

4

&Y [Z P]\ SY^ R`_[P\Ca

&;F ;F



:

WF%T<% 1399J>%1<;T<T



oFZB_;L3 <3B; F1LF 3O ;Q;FLSF;{" SB1W O; W;J5F9;B
%99;



4

H;B<p9F9<%9<%F"hT9h 139



W3;OF

;-

>"FBT3G> "9%1WS;b15i{"9F1<9 b093;;;;h;F< >3>"F9;
%FpF>";T3>]3F1W; 1F9F<T ;19FF b;F< 13>FH;

;



139
=T0



;F 3 ]0B3G1S

a1<W 9F



=

139F 3&T"F1

1V (

];; F"FZ9B59F99F15hTJ3;JF

W<&;';;>"39G39;
3Bp939;

B<9"39HiG<91<9W;F 3GFhB

O<&\;&W1FW<9F3;3 T"3B3HB;1&

<5;

1;9FOb Q9%F>"5 pFhT3 W

1

OL;1;9S hF3&<BF

0
|:b%):%[

G3 T3B'S95FW5G3;

<9 W <BT9393;G;;OL;i"WO39W9L;B<LW FF9<LhF1;

(

0

;iOW>BJ5

c

;339FF%3;i 5;{B&;;h>

Q9;;T; T<W;<{B%<J9;'< F F< <Q9<BT39;%;;>"39

n0

093;;OhF;h3 39F9J1<9;

}

Q

15 < 5JB

F93W";b3">;'159< 3'1

F&o; ;3B33& J]<B<];;139

:

9

WH0B339

<T9

(

*

;LF&ZF>"FBT30F05<B;<9;h3;3

S9W<9 5;b";;5

: *

>F&T"'Fff<&;; >
;%9< <&S+ <B9FJ<

ff ff

(

(

91&%T1FB<



*

; F&5S19J ;3 <>3

&<iWL1;;9B;F"

*i

;9; >139H

: /

ff ff >


V &
E
V &



; JW1<B5;QB3;



hJJFS<9h ; > h;

r

X;> J
fJ


Q3ZO5313>

ffE

L

*

;F9;B

'1



b/

319; BOJ9< 39;

9\<&SFF Bp9>S< + F; F1

| *i

'{<BJFp3FB0995 339;SS1; F;
&1FW<03W <9;Q9L

{O4

5+ <

093;

j j /

1&Z

O;S<B";<9;T13JB95311BJF{<9

4

%;

4

;i;;<9 J9\;G;;5<;

<

9WF

%FOTOS5;J<BS 39 ;FJ%F"&O;hF

%WpF%FO<BJ&O;h<BG;";
>;;iSFJB

;-%>F&

'hO<BH&3T+S9<

<;9;S;i<&;& 3 0Z;1%F;i<&;

;;>3S;1F;OF O1Z1<Z
<_ 3 &F3<B;; >


j /

)

1W;;>3F+ ;1F

39;GGF B;WJ35+T"Q"F9;Bh

4

=B;19S9F" ;F

<19F; WF< S;bQ<B

J39F0 F91%;B<S;S3



n /

13

jX /

<F<95

*

>+h;SB";FSF3H<

/

+Hb5<;B1399 >

J;< 3 &FG<9;WF<

ffE



0O09<11>&3&&11&&0 &&311&O&3&

FE

- O3& 331L>3>OO>h& 3O3

[G

11%3>3% &&3 @ &0 @3ffO&& >33'&@+&0F& 33 : Z1
&'&33





fiO;;ff

F>W

*4

< B1F9;&<Z&y3 <<FSB

j j /


j j

9T



{4

Q 9

1BZ

1

zgAw]vpoutAouous


O<&3>S%9W;p<WB1FFO9W<bO

(

h





19<3F

%


c

"b;F"WF3

x


<B3

>"F 9;




3B"39;WF ;;h;;<F9;G9W;3;3 G

hB

F& TOF";SB

S9

x

9 > O%;5

95 QB1

;33FSW;\;

n

Op&;F<S9F<

j X
j

j

' 0;+F

;3 5139

(
H

';<F<S><B

|

a1p

;

g

0



>"F

;]3WSO;T1;<Z9<&;

H5G;1FHOG;S W<3B

i"39FZ59 F1=;F;39

' 5;5<



<ffW <B3

%;;F

;a";5<WF

<BF+ G3;F3><3B09





><>

O3>

j
Xi j
Qj j
,
'

><>

;;



<B

;

"' WF

i;

G1+>9

moVo&yXo&Aou
h
c
}
{
c * / * j j /
bQ
. u&
&

uAc
c
* j / c
<Afi u
>ff

uf [ j
gc A{4 * jXj /


Hff

f&ff>
f * b/
c {4 & * j j :/
fi
g[ fiffH
!"$#&%'#)('+*,+-!.+/#&0212
fi fiX "c
* j / n

0
*
x
/ fif
3fi 5 4A
>, fuff& ff
fi u 6
&87fXfu
> :9fi &;+<
fi >=
g
?=


* j X/

@
Hfff

>X" fuff
FX& * / [

{
* j / Bf&fffi ff
>fiXffCfi fff

> &ff
FX c

4
.
&
&,c

b * j / bc
nfiffX
>b
Efi F
fuff& ff
fi &X"fi uVdfiXH G V&ff
> u.fi
>I G

h
( { 4 * j j ]/ (
g fi
gb fi ff X AAfi "fi fi J
h * j Xj/ nbK <)LQbf. &u
> MCN"
+ Og
&u
> eb B 6+ P)fi QGSR&bQ ; <c
}

h * j j / Q
}
$Hff

fuff
FX& >TU
b
Q9 5= Q F9 <;
5

H F

Q9 h




; >



b

1BZ





5



1BZ

{+;1;



1



<;

>

&

_

&9;BO

QFF" FWF





>

WF< O9;F



19<



1&Z

i"WWFZ



< B

i"WWFZ







5

;B;



3F1;T;<9;i";;5F9

JF W <9<B;<;F

>


FW >;;


&<





B;<9;QB1 ;F19O5&

;<B<9;



0F39;HF<9<9 9 F1

" >L35'<1WF5939JBS<BF">9;F

H W

9

F 55;

Z 1

Z %< >93S




390"39F{"F99;<9 g3"9;GF<;O9<W <B39

BFB3 h

%

<BJ;

> 5 ff;

0&<9hBSL

F<&ff

<

3& %1< W<B;<9;+



F;T"

>

> h;;39h<B;<9;F

<9F







>

%

W<9+ F1O;;SW<B9 F1 F3

'

>

0

h





0 ;



1

;<B9<9;JF F93;W<BFQFQ0


V





fiMONQPR TVU XW




}

;FF

&Y [Z P]\ SY^ R`_[P\Ca

&;F ;F

{O

BZ %

;

}

9W <>39;FJ;<9G"

&{4


;ZF

h

W 5Q



& * j j /




W bYS P

;

;



WX6X

&ZT

|c

>;3 T3&<BG;;1

)j j

<B;<;FG;<B 0 F



;; h59 F1SF



F;J; ;3F; F15W1;F

<i><91;

* jXj /

:

1BZ SZF

3 &F3



>

&T0B;F;39J&

[=Q<9 193HQ4]3<9;" F"

' "'F

;9; 39;+

* /

* j j / (
\=
b
# fifVX
>bxfi3
^]DW3<^_

X
h
* j j / c
{ }
`Hff

fuff
FX UU j [
h
* j j / c
h * / ufi2
}
&af& gAfi K + "fi fi JK FX&&
b Gff+ P
b Xj c<cc<
{

c
g * jXj /

n
HQ e9fi &;fifi f
f&ufi Xff
>g Xfi nfffi &u
>
hA:
>
hX&fi &fffi
h
j
c
{
* j j b/
g ifi
_[ fi ffH
fi fiX
b Xj
* j /

fi2 / fi ffH AAfi K + "fi fi JK
b j

& &{4 & * j j / g
j9fi ^;b
>k ffHfi ff
Efi g
8?5$4 & G
PQfib
> P fi : dH
FXg fi"P~ G
fX:
>u


b * jX /

*
"fi PffXff
>fi u fuff
FX& /

*
{ 4 * j j / uc
/ Q 7fiV
bdEfi g ?85)" _"_lA,b G
Pfi
Pfi f
4fi u ff
fi dEfi 5ff"fi P) ff

&u
> MmRxFf
> :ifi uk L Gbfi &


{~4 * j j / W=H ( :c
cHW c fi2 nAXi:fu QAfi fi C
&
n3fi aod&"fi :i &ff ff
fi # &,f
<
pb !q#&%'r# (>'*s + -!.q+ #&0"1

fi u
>
c
b [{
* j j /
efi2
X[ fi ff X AAfi fi C J j [ j j
* jX / tduGbu" P)
>5 6+ P)fi QGMnvRfi QG)3fi + P)
&
) uF 7fXfu
>d
Afi PQ:ffV

X&fi
u=
& &{c
& * j X/ g:
Q hg ]w h xQfiX2Q hg &z
& Vff u
>
* j / Hc
H fi2 X \ ffH
fi fiX &j bj j
S;FWF"3F



>

o;G;393]"b3 T3&9'Z

QF< >31F9;\;><

F 3O \<;F;F1F\1S9W <p9\FB9&< S<B;<;F

'

<; ff =

S;FWF"3+F

Q F9 F=



>

00B;3aF1;hF3J; JL<BW;<

ff 39aFJ F3

< <9 F;



>



iB9;p<93'B

0 ;

9G 3 &FJ<B;<9;F



iWFZ

; ff 5

;



;WWFZ





>W">H



Z >

>

;F 9W <>%59 F1J

B;<9;+

'

:

;

ff F 5

H %3

<BJ

53 <<9



; ;3 TB;<9;"{<&;<9;F



@<;F ;

95F



1 %F<39>"39<9>1F1 <9;p;O;393ff1;<BF"39;&F9<9;F



@<;F" =

%

1BZ



1

';;0"39S<9H%3;3 T"3Bh31

'



<F<9 %



><<5;919;<BFF"1<>9S139F_O9S151'< + F< 5 1>


5+ <;;+F
F >

1BZ S;F

>; ; 0;





;

;;5J F>0ff<B;<;

;19S9F



<; Z<

5+ <;
L

1BZ



>



3;FZ1W<9 F3&>< h<&;; h&

'

; ff F 00B<9hBSL









W



5O;"& W00=3> +BLF3B09&>< <&;<9;F '
;+ 5

5F";ff h

B

<

>


{S;3&<

5F";ff

F1&3 h

9WWF h





Q<9 193





>



; G%< ;;9;<BFhF9< ; >39F

@<; "

;



{

'

L39F";G

fiO;;ff

:c * j X/
@
H 7fi
U[ |f& D]fiX
>uHAfi K+ Afi |F
Xj
* j j /
{
gc

t} Hc
c 0
:=
6X
&~7fXfu
> :
j[ [
" * jXj / k7f &
,GHu fif
ffi u
>
xQ& fi+P:fi
2
u
=
c
|= j b
[ * j j /
:c
fi
g[ fi ffH Afi fi fi F j
* j j / :

Cfi2 [ fi ff X AAfi "fi fi J
j[
} <{
H * j j /

j






>

; 139;; 1h; 3O;



Z;



0=

195F

{+;1;





<B99;>;F

195F





195F



<9

;F



ff

F





;T9;

Q<9 >93 0&<9O;<B 0_ F

O3

;



% % F

" >91<9<9O3> %<9;F F9B 5 <BiF;<9 y39;Q <B3;<
5;+ ;

&J 5



iF09J9;



19

'

4

909F



;B {"; 9



1

%99





> '< >139;&FS9< <39

'

<;F

&3;



F >"> ;<B

=%<9 >93Z



>

F

9<;;1< &1B

SB9'<&F3o 39 bF

% "; F 5%"



+

F>{;9F9

<

F3B0 F3WT1

W;; h59 F1

F< <9

fiJournal Artificial Intelligence Research 2 (1994) 227-262

Submitted 10/94; published 12/94

Total-Order Partial-Order Planning:
Comparative Analysis
Steven Minton
John Bresina
Mark Drummond

Recom Technologies
NASA Ames Research Center, Mail Stop: 269-2
Moffett Field, CA 94035 USA

minton@ptolemy.arc.nasa.gov
bresina@ptolemy.arc.nasa.gov
med@ptolemy.arc.nasa.gov

Abstract

many years, intuitions underlying partial-order planning largely taken
granted. past years renewed interest fundamental
principles underlying paradigm. paper, present rigorous comparative
analysis partial-order total-order planning focusing two specific planners
directly compared. show subtle assumptions underly
wide-spread intuitions regarding supposed eciency partial-order planning.
instance, superiority partial-order planning depend critically upon search
strategy structure search space. Understanding underlying assumptions
crucial constructing ecient planners.

1. Introduction

many years, superiority partial-order planners total-order planners
tacitly assumed planning community. Originally, partial-order planning introduced Sacerdoti (1975) way improve planning eciency avoiding \premature
commitments particular order achieving subgoals". utility partial-order
planning demonstrated anecdotally showing planner could eciently
solve blocksworld examples, well-known \Sussman anomaly".
Since partial-order planning intuitively seems like good idea, little attention
devoted analyzing utility, least recently (Minton, Bresina, & Drummond,
1991a; Barrett & Weld, 1994; Kambhampati, 1994c). However, one looks closely
issues involved, number questions arise. example, advantages partialorder planning hold regardless search strategy used? advantages hold
planning language expressive reasoning partially ordered plans intractable
(e.g., language allows conditional effects)?
work (Minton et al., 1991a, 1992) shown situation much interesting might expected. found \unstated assumptions"
underlying supposed eciency partial-order planning. instance, superiority
partial-order planning depend critically upon search strategy search heuristics
employed.
paper summarizes observations regarding partial-order total-order planning. begin considering simple total-order planner closely related partialorder planner establishing mapping search spaces. examine
c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiMinton, Bresina, & Drummond

relative sizes search spaces, demonstrating partial-order planner
fundamental advantage size search space always less equal
total-order planner. However, advantage necessarily translate
eciency gain; depends type search strategy used. example,
describe domain partial order planner ecient total order
planner depth-first search used, eciency gain lost iterative
sampling strategy used.
also show partial-order planners second, independent advantage
certain types operator ordering heuristics employed. \heuristic advantage"
underlies Sacerdoti's anecdotal examples explaining least-commitment works. However,
blocksworld experiments, second advantage relatively unimportant compared
advantage derived reduction search space size.
Finally, look results extend partial-order planners general.
describe advantages partial-order planning preserved even highly expressive languages used. also show advantages necessarily hold
partial-order planners, depend critically construction planning space.

2. Background
Planning characterized search space possible plans. total-order
planner searches space totally ordered plans; partial-order planner defined
analogously. use terms, rather terms \linear" \nonlinear",
latter overloaded. example, authors used term \nonlinear"
focusing issue goal ordering. is, \linear" planners, solving
conjunctive goal, require subgoals one conjunct achieved subgoals
others; hence, planners arbitrarily interleave subgoals often called \nonlinear".
version linear/nonlinear distinction different partial-order/totalorder distinction investigated here. former distinction impacts planner completeness,
whereas total-order/partial-order distinction orthogonal issue (Drummond &
Currie, 1989; Minton et al., 1991a).
total-order/partial-order distinction also kept separate distinction \world-based planners" \plan-based planners". distinction one
modeling: world-based planner, search state corresponds state
world plan-based planner, search state corresponds plan. totalorder planners commonly associated world-based planners, Strips, several
well-known total-order planners plan-based, Waldinger's regression planner (Waldinger, 1975), Interplan (Tate, 1974) Warplan (Warren, 1974). Similarly,
partial-order planners commonly plan-based, possible world-based
partial-order planner (Godefroid & Kabanza, 1991). paper, focus solely
total-order/partial-order distinction order avoid complicating analysis.
claim significant difference partial-order total-order planners planning eciency. might argued partial-order planning preferable
partially ordered plan exibly executed. However, execution exibility also achieved total-order planner post-processing step removes
unnecessary orderings totally ordered solution plan yield partial order (Back228

fiTotal-Order Partial-Order Planning

strom, 1993; Veloso, Perez, & Carbonell, 1990; Regnier & Fade, 1991). polynomial
time complexity post-processing negligible compared search time plan
generation.1 Hence, believe execution exibility is, best, weak justification
supposed superiority partial-order planning.
following sections, analyze relative eciency partial-order totalorder planning considering total-order planner partial-order planner
directly compared. Elucidating key differences planning algorithms
reveals important principles general relevance.

3. Terminology

plan consists ordered set steps, step unique operator instance.
Plans totally ordered, case every step ordered respect every
step, partially ordered, case steps unordered respect other.
assume library operators available, operator preconditions,
deleted conditions, added conditions. conditions must nonnegated propositions, adopt common convention deleted condition precondition.
Later paper show results extended expressive languages,
simple language sucient establish essence argument.
linearization partially ordered plan total order plan's steps
consistent existing partial order. totally ordered plan, precondition plan
step true added earlier step deleted intervening step.
partially ordered plan, step's precondition possibly true exists linearization
true, step's precondition necessarily true true linearizations.
step's precondition necessarily false possibly true.
state consists set propositions. planning problem defined initial
state set goals, goal proposition. convenience, represent
problem two-step initial plan, propositions true initial state
added first step, goal propositions preconditions final
step. planning process starts initial plan searches space
possible plans. successful search terminates solution plan, i.e., plan
steps' preconditions necessarily true. search space characterized tree,
node corresponds plan arc corresponds plan transformation.
transformation incrementally extends (i.e., refines) plan adding additional steps
orderings. Thus, leaf search tree corresponds either solution plan
dead-end, intermediate node corresponds unfinished plan
extended.

1. Backstrom (1993) formalizes problem removing unnecessary orderings order produce \leastconstrained" plan. shows problem polynomial one defines least-constrained plan
plan orderings removed without impacting correctness plan. Backstrom
also shows problem finding plan fewest orderings given operator set
much harder problem; NP-hard.

229

fiMinton, Bresina, & Drummond

TO(P; G)
1. Termination check: G empty, report success return solution plan P.
2. Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.
3. Operator selection: Let Oadd operator library adds c. Oadd ,
terminate report failure. Choice point: operators must considered completeness.
4. Ordering selection: Let Odel last deleter c. Insert Oadd somewhere Odel
Oneed , call resulting plan P . Choice point: positions must considered completeness.
5. Goal updating: Let G set preconditions P true.
6. Recursive invocation: TO(P ; G ).
0

0

0

0

0

Figure 1: planning algorithm
Plan P




del



B

Oneed

F

+

add





del

Oadd



Oneed F

B





del



Oadd

B





del

Oneed F



B

Oadd

Oneed F

Figure 2: extends plan: Adding Oadd plan P generates three alternatives.

4. Tale Two Planners
section define two simple planning algorithms. first algorithm, shown
Figure 1, to, total-order planner motivated Waldinger's regression planner
(Waldinger, 1975), Interplan (Tate, 1974), Warplan (Waldinger, 1975). purpose
characterize search space planning algorithm, pseudo-code
Figure 1 accomplishes defining nondeterministic procedure enumerates
possible plans. (If plans enumerated breadth-first search, algorithms
presented section provably complete, shown Appendix A.)
230

fiTotal-Order Partial-Order Planning

accepts unfinished plan, P , goal set, G, containing preconditions

currently true. algorithm terminates successfully returns totally ordered
solution plan. Note two choice points procedure: operator selection
ordering selection. procedure need consider alternative goal choices.
purposes, function select-goal deterministic function selects
member G.
used Step 4, last deleter precondition c step Oneed defined
follows. Step Odel last deleter c Odel deletes c, Odel Oneed ,
deleter c Odel Oneed . case step Oneed deletes c,
first step considered last deleter.
Figure 2 illustrates to's plan extension process. example assumes steps
B add delete c. three possible insertion points Oadd plan P ,
yielding alternative extension.
second planner ua, partial-order planner, shown Figure 3. ua similar
uses procedures goal selection operator selection; however,
procedure ordering selection different. Step 4 ua inserts orderings,
\interacting" steps ordered. Specifically, say two steps interact
unordered respect either:
one step precondition added deleted step,
one step adds condition deleted step.
significant difference ua lies Step 4: orders new step
respect others, whereas ua adds orderings eliminate interactions.
sense ua less committed to.
Figure 4 illustrates ua's plan extension process. Figure 2, assume steps
B add delete c; however, step Oadd interact respect
condition. interaction yields two alternative plan extensions: one Oadd
ordered one Oadd ordered A.
Since ua orders steps interact, plans generated special
property: precondition plan either necessarily true necessarily false.
call plans unambiguous. property yields tight correspondence two
planners' search spaces. Suppose ua given unambiguous plan U given
plan , linearization U . Let us consider relationship
way ua extends U extends . Note two planners
set goals since, definition, goal U precondition necessarily
false, precondition necessarily false false every linearization.
Since two plans set goals since planners use goal
selection method, algorithms pick goal; therefore, Oneed both.
Similarly, algorithms consider library operators achieve goal. Since
linearization U , Oneed plans, algorithms find
last deleter well.2 adds step plan, orders new step respect
2. unique last deleter U . follows requirement operator
language, deleted conditions must subset preconditions. two unordered steps delete
condition, condition must also precondition operators. Hence, two
steps interact ordered ua.

231

fiMinton, Bresina, & Drummond

UA(P; G)
1. Termination check: G empty, report success return solution plan P.
2. Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.
3. Operator selection: Let Oadd operator library adds c. Oadd ,
terminate report failure. Choice point: operators must considered completeness.
4. Ordering selection: Let Odel last deleter c. Order Oadd Odel Oneed.
Repeat interactions:
Select step Oint interacts Oadd .
Order Oint either Oadd .
Choice point: orderings must considered completeness.
Let P resulting plan.
5. Goal updating: Let G set preconditions P necessarily false.
6. Recursive invocation: UA(P ; G ).
0

0

0

0

0

Figure 3: ua planning algorithm

Plan P







F

need

del

B
+

add





add



add



del

need



F




del

need

F

B

B

Figure 4: ua extends plan: Adding Oadd plan P generates two alternatives.
example assumes Oadd interacts step A.

232

fiTotal-Order Partial-Order Planning

existing steps. ua adds step plan, orders new step respect
interacting steps. ua considers possible combinations orderings eliminate
interactions; hence, plan produced to, ua produces corresponding plan
less-ordered equivalent.
following sections exploit tight correspondence search spaces
ua to. next section analyze relative sizes two planners' search
spaces, later compare number plans actually generated different search
strategies.

5. Search Space Comparison

search space ua characterized tree plans. root
node tree corresponds top-level invocation algorithm, remaining
nodes correspond recursive invocation algorithm. Note generating
plan, algorithms make operator ordering choices, different set
choices corresponds single branch search tree.
denote search tree treeTO and, similarly, search tree ua
treeUA . number plans search tree equal number times planning
procedure (ua to) would invoked exhaustive exploration search space.
Note every plan treeUA treeTO unique, since step plan given
unique label. Thus, although two plans tree might instances
particular operator sequence, O1 O2 O3, plans distinct
steps different labels. (We defined plans way make proofs
concise.)
show given problem, treeTO least large treeUA , is,
number plans treeTO greater equal number plans treeUA .
done proving existence function L maps plans treeUA sets
plans treeTO satisfies following two conditions.
1. Totality Property: every plan U treeUA , exists non-empty set
fT1; : : :; Tmg plans treeTO L(U ) = fT1; : : :; Tmg.
2. Disjointness Property: L maps distinct plans treeUA disjoint sets plans
treeTO ; is, U1; U2 2 treeUA U1 6= U2, L(U1) \ L(U2) = fg.
Let us examine existence L two properties sucient prove
size ua's search tree greater to. Figure 5 provides guide
following discussion. Intuitively, use L count plans two search trees.
plan counted treeUA , use L count non-empty set plans treeTO .
totality property means every time count
P plan treeUA , count least
one plan treeTO ; implies j treeUA j U 2treeUA j L(U ) j. course, must
show plan counted treeTOPis counted once; guaranteed
disjointness property, implies U 2treeUA j L(U ) j j treeTO j. Thus,
conjunction two properties implies j treeUA j j treeTO j.
define function L two properties follows. Let U plan
treeUA , let plan treeTO , let parent function plan parent
233

fiMinton, Bresina, & Drummond

ua search tree

h

L

h

?
,@
,

@
@
R
@

h

search tree

L

h

h

L

- f
- f

L

- f
g

H
HH



j
H

h

hg

,@
, @

R

h

hg

- f




ff


h


AA
U

hg

Figure 5: L maps treeUA treeTO
plan tree. 2 L(U ) (i) linearization U (ii) either
U root nodes respective search trees parent(T ) 2 L(parent(U )).
Intuitively, L maps plan U treeUA linearizations share common derivation
ancestry.3 illustrated Figure 5, plan treeUA dashed line
drawn corresponding set plans treeTO .
show L satisfies totality disjointness properties induction
depth search trees. Detailed proofs appendix. prove first property,
show every plan contained treeUA , linearizations plan contained
treeTO . prove second property, note two plans different depths
treeUA disjoint sets linearizations, show induction two plans
depth treeUA also property.
much smaller treeUA treeTO ? mapping described provides
answer. plan U treeUA j L(U ) j distinct plans to, j L(U ) j
number linearizations U . exact number depends unordered U is.
totally unordered plan factorial number linearizations totally ordered plan
single linearization. Thus, time size treeUA equals size
treeTO every plan treeUA totally ordered; otherwise, treeUA strictly smaller
treeTO possibly exponentially smaller.

6. Time Cost Per Plan
size ua's search tree possibly exponentially smaller to,
follow ua necessarily ecient. Eciency determined two factors:
3. reader may question L maps U linearizations treeTO share common derivation ancestry, opposed simply mapping U linearizations treeTO . reason
planners systematic, sense may generate two plans
operator sequence. distinguish plans derivational history. example, suppose
two instantiations operator sequence O1 O2 O3 exist within treeTO correspond different plans treeUA . L relies different derivations determine appropriate
correspondence.

234

fiTotal-Order Partial-Order Planning

Step Executions Per Plan Cost UA Cost
1
1
O(1)
O(1)
2
1
O(1)
O(1)
3
<1
O(1)
O(1)
4
1
O(1)
O(e)
5
1
O(n)
O(e)
Table 1: Cost per plan comparisons
time cost per plan search tree (discussed section) size subtree
explored search process (discussed next section).
section show ua indeed take time per plan, extra
time relatively small grows polynomially number steps plan,4
denote n. comparing relative eciency ua to, first consider
number times algorithm step executed per plan search tree
consider time complexity step.
noted preceding sections, node search tree corresponds plan,
invocation planning procedure ua corresponds attempt
extend plan. Thus, ua to, clear termination check
goal selection (Steps 1 2) executed per plan. Analyzing number
times remaining steps executed might seem complicated, since
steps executed many times internal node leaf. However,
analysis actually quite simple since amortize number executions
step number plans produced. Notice Step 6 executed plan
generated (i.e., node root node). gives us bound
number times Steps 3, 4, 5 executed.5 specifically,
algorithms, Step 3 executed fewer times Step 6, Steps 4 5 executed
exactly number times Step 6 executed, is, plan
generated. Consequently, algorithms, step executed per
plan, summarized Table 1. words, number times step executed
planning process bounded size search tree.
examining costs step, first note algorithms, Step 1,
termination check, accomplished O(1) time. Step 2, goal selection, also
accomplished O(1) time; example, assuming goals stored list,
select-goal function simply return first member list. execution
Step 3, operator selection, also requires O(1) time; assume operators
indexed effects, required \pop" list relevant operators
execution.
4. assume size operators (the number preconditions effects) bounded
constant given domain.
5. Since Steps 3 4 nondeterministic, need clear terminology. say Step
3 executed time different operator chosen, Step 4 executed different
combination orderings selected.

235

fiMinton, Bresina, & Drummond

Steps 4 5 less expensive ua. Step 4 accomplished
inserting new operator, Oadd , somewhere Odel Oneed . possible
insertion points considered starting Oneed working towards Odel , execution Step 4 accomplished constant time, since insertion constitutes one
execution step. contrast, Step 4 ua involves carrying interaction detection
elimination order produce new plan P 0 . step accomplished O(e)
time, e number edges graph required represent partially ordered
plan. (In worst case, may O(n2 ) edges plan, best case, O(n)
edges.) following description ua's ordering step, Figure 3,
additional implementation details:
4. Ordering selection: Order add del need . Label steps preceding add








steps following Oadd . Let stepsint unlabeled steps interact Oadd . Let Odel
last deleter c. Repeat stepsint empty:



Let Oint = Pop(stepsint )
Oint still unlabeled either:
{ order Oint Oadd , label Oint unlabeled steps Oint ;
{ order Oint Oadd , label Oint unlabeled steps Oint .
Choice point: orderings must considered completeness.

Let P resulting plan.
0

ordering process begins preprocessing stage. First, steps preceding following Oadd labeled such. labeling process implemented depth-first traversal
plan graph, starting Oadd root, first follows edges one direction follows edges direction. requires O(e) time.
labeling process complete, steps unordered respect Oadd
unlabeled, thus interacting steps (which must unordered respect Oadd)
identifiable O(n) time. last deleter identifiable O(e) time.
preprocessing stage, procedure orders interacting step respect
Oadd, updating labels iteration. Since edge graph need traversed
once, entire ordering process takes O(e) time (as described
Minton et al., 1991b). see this, note process labeling steps (or
after) Oint stop soon labeled step encountered.
shown Step 4 O(1) complexity Step 4 ua O(e) complexity, consider Step 5 algorithms, updating goal set. accomplishes
iterating steps plan, head tail, requires O(n)
time. ua accomplishes similar manner, requires O(e) time traverse
graph. (Alternatively, ua use procedure to, provided O(e) topological
sort first done linearize plan.)
summarize complexity analysis, use partial order means ua incurs
greater cost operator ordering (Step 4) updating goal set (Step 5). Overall,
ua requires O(e) time per plan, requires O(n) time per plan. Since totally
ordered plan requires representation size O(n), partially ordered graph requires
representation size O(e), designing procedures lower costs would possible
entire plan graph need examined worst case.
236

fiTotal-Order Partial-Order Planning

7. Role Search Strategies

previous sections compared ua terms relative search space size
relative time cost per node. extra processing time required ua node
would appear justified since search space may contain exponentially fewer nodes.
However, complete analysis, must consider number nodes actually visited
algorithm given search strategy.
breadth-first search, analysis straightforward. completing search
particular depth, planners explored entire trees depth.6
ua find solution depth due correspondence
search trees. Thus, degree ua outperform to, breadth-first, depends
solely \expansion factor" L, i.e., number linearizations ua's plans.
formalize analysis follows. node U treeUA , denote number
steps plan U nu , number edges U eu . node U
ua generates, ua incurs time cost O(eu ); whereas, incurs time cost O(nu ) j L(U ) j,
j L(U ) j number linearizations plan node U . Therefore, ratio
total time costs ua follows, bf (treeUA ) denotes subtree
considered ua breadth-first search.
P
cost(tobf ) = u2bfP(treeUA ) O(nu ) j L(U ) j
cost(uabf )
u2bf (treeUA ) O(eu )

analysis breadth-first search simple search strategy preserves
correspondence two planners' search spaces. breadth-first search, two
planners synchronized exhaustively exploring level, explored
(exactly) linearizations plans explored ua. search strategy
similarly preserves correspondence, iterative deepening, similar analysis
carried out.
cost comparison clear-cut depth-first search, since correspondence
guaranteed preserved. easy see that, depth-first search,
necessarily explore linearizations plans explored ua. simply
planners nondeterministically choose child expand. deeper reason:
correspondence L preserve subtree structure search space. plan
U treeUA , corresponding linearizations L(U ) may spread throughout treeTO .
Therefore, unlikely corresponding plans considered order
depth-first search. Nevertheless, even though two planners synchronized,
might expect that, average, ua explore fewer nodes size treeUA less
equal size treeTO .
Empirically, observed ua tend outperform depth-first
search, illustrated experimental results Figure 6. first graph compares
mean number nodes explored ua 44 randomly generated blocksworld
problems; second graph compares mean planning time ua
problems demonstrates extra time cost per node ua relatively insignificant. problems partitioned 4 sets 11 problems each, according minimal
6. perspicuity, ignore fact number nodes explored two planners last
level may differ planners stop reach first solution.

237

fiMinton, Bresina, & Drummond

10000

7500

Time Solution

Nodes Explored

50


5000

UA
2500

40

30
UA
20

10

0

0
3

4

5

6

3

Depth Problem

4

5

6

Depth Problem

Figure 6: ua Performance Comparison Depth-First Search
solution \length" (i.e., number steps plan). problem, planners
given depth-limit equal length shortest solution.7 Since planners
make nondeterministic choices, 25 trials conducted problem. source code
data required reproduce experiments found Online Appendix 1.
pointed out, one plausible explanation observed dominance ua
to's search tree least large ua's search tree. fact, experiments
often observed to's search tree typically much larger. However, full story
interesting. Search tree size alone sucient explain ua's dominance;
particular, density distribution solutions play important role.
solution density search tree proportion nodes solutions.8
solution density to's search tree greater ua's search tree, might
outperform ua depth-first search even though to's search tree actually larger.
example, might case ua solution plans completely unordered
plans remaining leaves treeUA { failed plans { totally ordered.
case, ua solution plan corresponds exponential number solution plans,
ua failed plan corresponds single failed plan. converse also possible:
solution density ua's search tree might greater to's search tree, thus
favoring ua depth-first search. example, might single totally
ordered solution plan ua's search tree large number highly unordered failed
7. Since depth-limit equal length shortest solution, iterative deepening (Korf, 1985)
approach would yield similar results. Additionally, note increasing depth-limit past
depth shortest solution significantly change outcome experiments.
8. definition solution density ill-defined infinite trees, assume depth-bound
always provided, finite subtree explicitly enumerated.

238

fiTotal-Order Partial-Order Planning

UA Search Tree

*

*

Search Tree

*

*

*

*

* = Solution plan

Figure 7: Uniform solution distribution, solution density 0.25
plans. Since failed ua plans would correspond large number failed
plans, solution density would considerably lower.
blocksworld problems, found solution densities two planners'
trees differ greatly, least way would explain performance
results. saw tendency treeUA higher solution density treeTO .
example, 11 problems solutions depth six, average solution density9
exceeded ua 7 12 problems. particularly surprising
since see priori reason suppose solution densities two planners
differ greatly.
Since solution density insucient explain ua's dominance blocksworld experiments using depth-first search, need look elsewhere explanation.
hypothesize distribution solutions provides explanation. note
solution plans distributed perfectly uniformly (i.e., even intervals) among
leaves search tree, solution densities similar, planners
expected search similar number leaves, illustrated schematic search
tree Figure 7. Consequently, explain observed dominance ua
hypothesizing solutions uniformly distributed; is, solutions tend cluster.
see this, suppose treeUA smaller treeTO two trees
solution density. solutions clustered, Figure 8, depth-first search
expected produce solutions quickly treeUA treeTO .10 hypothesis
9. experiments, nondeterministic goal selection procedure used planners, meant
solution density could vary run run. compared average solution density 25
trials problem obtain results.
10. Even solutions distributed randomly amongst leaves trees uniform probability
(as opposed distributed \perfectly uniformly"), clusters nodes. Therefore,
small disadvantage. see this, let us suppose leaf treeUA treeTO
solution equal probability p. is, treeUA NUA leaves, kUA solutions,

239

fiMinton, Bresina, & Drummond

UA Search Tree

*

Search Tree

*

*

**

*

* = Solution plan

Figure 8: Non-uniform solution distribution, solution density 0.25
solutions tend clustered seems reasonable since easy construct problems
\wrong decision" near top search tree lead entire subtree
devoid solutions.
One way test hypothesis compare ua using randomized search
strategy, type Monte Carlo algorithm, refer \iterative sampling" (cf.
Minton et al., 1992; Langley, 1992; Chen, 1989; Crawford & Baker, 1994). iterative
sampling strategy explores randomly chosen paths search tree solution
found. path selected traversing tree root leaf, choosing randomly
branch point. leaf solution search terminates; not, search
process returns root selects another path. path may examined
since memory maintained iterations.
contrast depth-first search, iterative sampling relatively insensitive distribution solutions. Therefore, advantage ua disappear hypothesis correct. experiments, find ua use iterative
sampling, expand approximately number nodes set blocksworld
problems.11 (For planners, performance iterative sampling worse
depth-first search.) fact difference ua iterative
sampling, difference depth-first search, suggests solutions
treeTO NTO leaves, kTO solutions, p = kUA =NUA = kTO =NTO . general,
k N nodes solutions, expected number nodes must tested find solution
:5N=k k = 1 approaches N=k k (and N ) approaches 1. (This simply expected
number samples binomial distribution.) Therefore, since kTO kUA , expected number
leaves explored greater equal expected number leaves explored ua,
factor 2.
11. iterative sampling strategy depth-limited exactly way depth-first strategy
was. note, however, performance iterative sampling relatively insensitive actual
depth-limit used.

240

fiTotal-Order Partial-Order Planning

indeed non-uniformly distributed. Furthermore, result shows ua necessarily
superior to; search strategy employed makes dramatic difference.
Although blocksworld domain may atypical, conjecture results
general relevance. Specifically, distribution-sensitive search strategies like depth-first
search, one expect ua tend outperform to. distribution-insensitive
strategies, iterative sampling, non-uniform distributions effect. note
iterative sampling rather simplistic strategy, sophisticated
search strategies, iterative broadening (Ginsberg & Harvey, 1992), also
relatively distribution insensitive. explore strategies Section 8.2.

8. Role Heuristics

preceding sections, shown partial-order planner ecient
simply search tree smaller. search strategies, breadthfirst search, size differential obviously translates eciency gain.
strategies, depth-first search, size differential translates eciency gain,
provided make additional assumptions solution density distribution.
However, often claimed partial-order planners ecient due
ability make informed ordering decisions, rather different argument. instance,
Sacerdoti (1975) argues reason noah performs well problems
blocksworld's \Sussman anomaly". delaying decision whether stack
B stacking B C, noah eventually detect con ict occur
stacks B first, critic called \resolve-conflicts" order steps
intelligently.
section, show argument formally described terms
two planners. demonstrate ua fact potential advantage
exploit certain types heuristics readily to. advantage
independent fact ua smaller search space. Whether advantage
significant practice another question, course. also describe experiments
evaluate effect commonly-used heuristic blocksworld problems.

8.1 Making Informed Decisions

First, let us identify ua make better use certain heuristics to.
ua planning algorithm, step 4 arbitrarily orders interacting plan steps. Similarly,
Step 4 arbitrarily chooses insertion point new step. easy see,
however, orderings tried others heuristic search.
illustrated Figure 9, compares ua particular problem. key
figure describes relevant conditions library operators, preconditions
indicated left operator added conditions indicated right (there
deletes example). brevity, initial step final step plans
shown. Consider plan treeUA unordered steps O1 O2 . ua
introduces O3 achieve precondition p O1 , Step 4 ua order O3 respect
O2, since steps interact. However, makes sense order O2 O3, since O2
achieves precondition q O3. illustrates simple planning heuristic refer
min-goals heuristic: \prefer orderings yield fewest false preconditions".
241

fiMinton, Bresina, & Drummond

UA



O1

O1

O1
O1

O2

O1

O2

O2
O1
O2

O3

O1

O1

O2

q O3

p

O3

O3

O2

O3

O1

O3

O2

O1

O2

KEY
p

O1

r

O2 q

Figure 9: Comparison ua example.
heuristic guaranteed produce optimal search optimal plan,
commonly used. basis \resolve con icts" critic Sacerdoti employed
blocksworld examples.
Notice, however, cannot exploit heuristic effectively ua
prematurely orders O1 respect O2 . Due inability postpone ordering
decision, must choose arbitrarily plans O1 O2 O2 O1,
impact decision evaluated.
general case, suppose h heuristic applied partially ordered
plans totally ordered plans. Furthermore, assume h \useful" heuristic; i.e., h
rates one plan highly another, planner explores highly rated
plan first perform better average. Then, ua potential advantage
provided h satisfies following property: ua plan U corresponding
plan , h(U ) h(T ); is, partially ordered plan must rated least high
linearizations. (Note unambiguous plans, min-goals heuristic satisfies
property since gives identical ratings partially ordered plan linearizations.)
ua advantage ua expanding plan U expanding
corresponding plan , h rate child U least high highly
rated child . true since every child linearization child U ,
therefore child rated higher child U . Furthermore, may
child U none linearizations child , therefore child
U rated higher every child . Since assumed h useful heuristic,
means ua likely make better choice to.
242

fiTotal-Order Partial-Order Planning

5000

UA
TO-MG
UA-MG

Nodes Explored

4000

without MinGoals
UA without MinGoals
MinGoals
UA MinGoals



3000
UA
2000
TO-MG
1000
UA-MG
0
3

4

5

6

Depth Problem

Figure 10: Depth first search without min-goals

8.2 Illustrative Experimental Results
previous section showed ua potential advantage better
exploit certain ordering heuristics. examine practical effects incorporating
one heuristic ua to.
First, note ordering heuristics make sense search strategies.
particular, breadth-first search, heuristics improve eciency search
meaningful way (except possibly last level). Indeed, need consider search
strategy ua \synchronized", defined earlier, since ordering heuristics
significantly affect relative performance ua strategies. Thus,
begin considering standard search strategy synchronized: depth-first
search.
use min-goals heuristic basis experimental investigation, since
commonly employed, presumably could choose heuristic meets criterion
set forth previous section. Figure 10 shows impact min-goals behavior
ua depth-first search. Although heuristic biases order
two planners' search spaces explored (cf. Rosenbloom, Lee, & Unruh, 1993), appears
effect largely independent partial-order/total-order distinction, since
planners improved similar percentage. example, depth-first search
problems solutions depth six, ua improved 88% improved 87%. Thus,
obvious evidence extra advantage ua, one might expected
analysis previous section. hand, contradict
theory, simply means potential heuristic advantage significant enough
show up. domains, advantage might manifest significantly.
all, certainly possible design problems advantage significant,
243

fiMinton, Bresina, & Drummond

Nodes Explored

100
TO-IB
UA-IB
TO-IS
UA-IS

75

Iterative Broadening
UA Iterative Broadening
Iterative Sampling
UA Iterative Sampling

TO-IB

UA-IB
50

TO-IS

25

UA-IS

3

4

5

6

Depth Problem

Figure 11: Iterative sampling & iterative broadening, min-goals
example Figure 9 illustrates. results simply illustrate blocksworld
domain, making intelligent ordering decisions produces negligible advantage ua,
contrast significant effect due search space compression (discussed previously).12
min-goals heuristic seem help ua to, results
nevertheless interesting, since heuristic significant effect performance
planners, much min-goals outperforms ua without min-goals.
effectiveness min-goals domain dependent, find interesting
experiments, use min-goals makes difference use partial orders.
all, blocksworld originally helped motivate development partial-order planning
subsequent planning systems employed partial orders. deeply
surprising, result help reinforce already know: attention
paid specific planning heuristics min-goals.
analysis search space compression Section 7, described \distribution
insensitive" search strategy called iterative sampling showed iterative sampling ua perform similarly, although performance worse
depth-first search. combine min-goals iterative sampling, find produces much powerful strategy, one ua still perform
equally. simplicity, implementation iterative sampling uses min-goals pruning heuristic; choice point, explores plan extensions fewest
goals. strategy powerful, although incomplete.13 incompleteness,
note one problem removed sample set iterative sampling
12. Section 9.2, discuss planners \less-committed" ua. planners, advantage
due heuristics might pronounced since \delay" decisions even longer ua.
13. Instead exploring plan extensions fewest goals choice point, alternative
strategy assign extension probability inversely correlated number goals,

244

fiTotal-Order Partial-Order Planning

min-goals would never terminate problem. caveat mind, turn
results Figure 11, compared Figure 10, show performance
ua iterative sampling was, general, significantly better
performance depth-first search. (Note graphs Figures 10 11
different scales.) results clearly illustrate utility planning bias introduced
min-goals blocksworld domain, since 43 44 problems, solution exists
small subspace preferred min-goals.
experiments show advantage ua compared
heuristic, consistent conclusions above. However, could equally well
min-goals powerful, leading solutions quickly, smaller uences
obscured.
dramatic success combining min-goals iterative sampling led us consider
another search strategy, iterative broadening, combines best aspects depthfirst search iterative sampling. sophisticated search strategy initially behaves
like iterative sampling, evolves depth-first search breadth-cutoff increases
(Langley, 1992). Assuming solution within specified depth bound, iterative
broadening complete. early stages iterative broadening distribution-insensitive;
later stages behaves like depth-first search and, thus, becomes increasingly sensitive
solution distribution. one would expect iterative sampling experiments,
iterative broadening, solutions found early on, shown Figure 11. Thus,
surprising ua performed similarly iterative broadening.
point results presented subsection illustrative,
since deal single domain single heuristic. Nevertheless,
experiments illustrate various properties identified paper
interact.

9. Extending Results
established basic results concerning eciency ua various
circumstances, consider results extend types planners.

9.1 Expressive Languages
preceding sections, showed primary advantage ua
ua's search tree may exponentially smaller to's search tree, also showed
ua pays small (polynomial) extra cost per node advantage. Thus far
assumed restricted planning language operators propositional;
however, practical problems demand operators variables, conditional effects,
conditional preconditions. expressive planning language, time cost
per node significantly greater ua to? One might think so, since work
required identify interacting steps increase expressiveness operator
language used (Dean & Boddy, 1988; Hertzberg & Horz, 1989). cost detecting step
pick accordingly. Given depth bound, strategy advantage asymptotically
complete. used simpler strategy pedagogical reasons.

245

fiMinton, Bresina, & Drummond

interaction high enough, savings ua enjoys due reduced search space
outweighed additional expense incurred node.
Consider case simple breadth-first search. Earlier showed ratio
total time costs ua follows, subtree considered ua
breadth-first search denoted bf (treeUA ), number steps plan U denoted
nu , number edges U denoted eu :

P

cost(TObf ) = U 2bfP(treeUA ) O(nu ) j L(U ) j
cost(UAbf )
U 2bf (treeUA ) O(eu )
cost comparison specific simple propositional operator language used
far, basic idea general. ua generally outperform whenever cost
per node less product cost per node number nodes
correspond L. Thus, ua could incur exponential cost per node still
outperform cases. happen, example, exponential number
linearizations ua partial order greater exponential cost per node ua.
general, however, would like avoid case ua pays exponential cost per
node and, instead, consider approach guarantee cost per node ua
remains polynomial (as long cost per node also remains polynomial).
cost per node ua dominated cost updating goal set (Step 5)
cost selecting orderings (Step 4). Updating goal set remains polynomial
long plan unambiguous. Since precondition unambiguous plan either
necessarily true necessarily false, determine truth value given precondition
examining truth value arbitrary linearization plan. Thus, simply
linearize plan use procedure uses calculating goal set.
result, cost maintaining unambiguous property (i.e., Step 4)
impacted expressive languages. One approach eciently maintaining
property relies \conservative" ordering strategy operators ordered
even possibly interact.
illustration approach, consider simple propositional language conditional effects, \if p q, add r". Hence, operator add (or delete)
propositions depending state applied. refer conditions
\p" example dependency conditions. (Note that, like preconditions, dependency
conditions simple propositions.) Chapman (1987) showed type language NP-hard decide whether precondition true partially ordered plan.
However, pointed above, special case unambiguous plans, decision
accomplished polynomial time.
Formally, language specified follows. operator O, before, list preconditions, pre(O), list (unconditional) adds, adds(O), list (unconditional) deletes,
dels(O). addition, list conditional adds, cadds(O), list conditional
deletes, cdels(O); containing pairs hDe ; ei, De conjunctive set dependency conditions e conditional effect (either added deleted condition).
Analogous constraint every delete must precondition, every conditional
delete must member dependency conditions; is, every hDe ; ei 2 cdels(O),
e 2 De.
246

fiTotal-Order Partial-Order Planning

Figure 12 shows version ua algorithm, called ua-c, appropriate
language. primary difference ua ua-c algorithms Steps
3 4b operator may specialized respect set dependency conditions.
function specialize(O, ) accepts plan step, O, set dependency conditions,
D; returns new step O0 like O, certain conditional effects made
unconditional. effects selected transformation exactly whose
dependency conditions subset D. Thus, act specializing plan step
act committing expanding causal role plan.14 step specialized, ua-c
made commitment use given set effects. course, step
specialized later search node, specializations never retracted.
precisely, definition O0 = specialize(O; D), step, conjunctive set dependency conditions O, n set difference operator, follows.







pre(O0) = pre(O) [ D.
adds(O0) = adds(O) [ fe j hDe; ei 2 cadds(O) ^ De Dg.
dels(O0) = dels(O) [ fe j hDe ; ei 2 cdels(O) ^ De Dg.
cadds(O0) = fhDe ; ei j hDe ; ei 2 cadds(O) ^ De 6 ^ De = De nDg.
cdels(O0) = fhDe ; ei j hDe ; ei 2 cdels(O) ^ De 6 ^ De = De nDg.
0

0

0

0

definition step interaction generalized ua-c follows. say two
steps plan interact unordered respect following
disjunction holds:
one step precondition dependency condition added deleted
step,
one step adds condition deleted step.
difference definition step interaction one given earlier indicated italic font. modified definition allows us detect interacting operators
simple inexpensive test, original definition. example, two steps
unordered interact one step conditionally adds r precondition r.
Note first step need actually add r plan, ordering two operators
might unnecessary. general, definition interaction sucient criterion
guaranteeing resulting plans unambiguous, necessary criterion.
Figure 13 shows schematic example illustrating ua-c extends plan. preconditions operator shown left operator, unconditional
adds right. (We show preconditions effects necessary illustrate
specialization process; deletes used example.) Conditional adds shown
14. simplicity, modifications used create ua-c sophisticated. result, ua-c's space
may larger needs circumstances, since aggressively commits specializations.
sophisticated set modifications possible; however, subtlies involved eciently planning
dependency conditions (Pednault, 1988; Collins & Pryor, 1992; Penberthy & Weld, 1992) largely
irrelevant discussion.

247

fiMinton, Bresina, & Drummond

UA-C(P; G)
1 Termination check: G empty, report success return solution plan P.
2 Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.
3 Operator selection: Let Oadd operator schema library possibly adds c; is,
either c 2 adds(O), exists hDc ; ci 2 cadds(O). former case, insert step Oadd
latter case, insert step specialize(Oadd ; Dc). Oadd , terminate report
failure. Choice point: ways c added must considered completeness.
4a Ordering selection: Let Odel (unconditional) last deleter c. Order Oadd Odel
Oneed .
Repeat interactions:
Select step Oint interacts Oadd .
Order Oint either Oadd .
Choice point: orderings must considered completeness.
Let P resulting plan.
4b Operator role selection: exists step Ocadd unmarked conditional add hDc ; ci
step Ouse precondition c, Ouse Ocadd (unconditional)
deleter c Ouse Ocadd .
Either mark hDc; ci, replace Ocadd specialize(Ocadd ; Dc ).
Choice point: options must considered completeness.
5 Goal updating: Let G set preconditions P necessarily false.
6 Recursive invocation: UA-C(P ; G ).
0

0

0

0

0

Figure 12: ua-C planning algorithm
underneath operator. instance, first operator plan top
page precondition p. operator adds q conditionally adds u true.
figure illustrates two plans produced result adding new conditional operator
plan. one plan, conditional effects [u ! s] [t ! u] selected
specialization process, plan not.
new step, Step 4b, requires polynomial time per plan generated, time
cost steps ua. Hence, original ua algorithm,
cost per node ua-c algorithm polynomial.
also handle language given corresponding modifications (changing Step
3 adding Step 4b), time cost per plan also remains polynomial.15 Moreover,
relationship holds two planners' search spaces { treeUA never larger
treeTO exponentially smaller. example illustrates theoretical
advantages ua preserved expressive language.
pointed out, definition interaction sucient criterion guaranteeing
resulting plans unambiguous, necessary criterion. Nevertheless,
conservative approach allows interactions detected via simple inexpensive syntactic
test. Essentially, kept cost per node ua-c low restricting search space
considers, shown Figure 14. ua-c considers unambiguous plans
generated via \conservative" ordering strategy. ua-c still partial-order planner,
15. fact, Step 4b implemented time cost O(e), using graph traversal techniques
described Section 6. result ua-c implementation corresponding to-c implementation
time cost per node new language original language, O(e)
(n), respectively.

248

fiTotal-Order Partial-Order Planning

q
r

p

q


[t

u]




Add Operator:
[u


p
[t

[u

q



r
s]

r
s]

q
r


u
p

u]





q



r


q
r


Ou


Figure 13: example illustrating ua-c algorithm
complete, consider partially ordered plans even unambiguous
partially ordered plans.
\trick" used languages well, provided devise
simple test detect interacting operators. example, previous work (Minton et al.,
1991b) showed done language operators variables
preconditions effects. general case, given ua plan corresponding
plan, Steps 1,2, 3 ua algorithm cost corresponding steps
algorithm. long plans considered ua unambiguous, Step 5
ua algorithm accomplished arbitrary linearization plan, case
costs O(e) Step 5 algorithm. Thus, possibility
additional cost Step 4. general, devise \local" criterion interaction
resulting plan guaranteed unambiguous, ordering selection
step accomplished polynomial time. \local", mean criterion
considers operator pairs determine interactions; i.e., must examine rest
plan.
Although theoretical advantages ua preserved
expressive languages, cost. unambiguous plans considered may
orderings necessary, addition unnecessary orderings increase
size ua's search tree. magnitude increase depends specific language,
domain, problem considered. Nevertheless, guarantee ua's search
tree never larger to's.
general lesson cost plan extension solely dependent
expressiveness operator language, also depends nature plans
249

fiMinton, Bresina, & Drummond

partially ordered plans
unambiguous
partially ordered plans
unambiguous partially
ordered plans produced
conservative ordering strategy
totally ordered
plans

Figure 14: Hierarchy Plan Spaces
planner considers. So, although extension partially ordered plans NP-hard
languages conditional effects, space plans restricted (e.g., unambiguous
plans considered) worst-case situation avoided.

9.2 Less Committed Planners

shown ua, partial-order planner, certain computational advantages
total-order planner, to, since ability delay commitments allows
compact search space potentially intelligent ordering choices. However,
many planners even less committed ua. fact, continuum
commitment strategies might consider, illustrated Figure 15. Total-order
planning lies one end spectrum. extreme strategy maintaining
totally unordered set steps search exists linearization steps
solution plan.
Compared many well-known planners, ua conservative since requires plan
unambiguous. required noah (Sacerdoti, 1977), NonLin (Tate, 1977),
Totally
Ordered


Completely
Unordered
UA

Figure 15: continuum commitment strategies
250

fiTotal-Order Partial-Order Planning

MT(P; G)
1. Termination check: G empty, report success stop.
2. Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.
3. Operator selection: Let Oadd either plan step possibly Oneed adds c operator
library adds c. Oadd , terminate report failure.
Choice point: operators must considered completeness.
4. Ordering selection: Order Oadd Oneed. Repeat steps possibly
Oadd Oneed delete c:
Let Odel step; choose one following ways make c true Oneed
Order Odel Oneed .
Choose step Oknight (possibly Oadd ) adds c possibly Odel Oneed;
order Odel Oneed .
Choice point: alternatives must considered completeness.
Let P resulting plan.
5. Goal updating: Let G set preconditions P necessarily true.
6. Recursive invocation: MT(P ; G ).
0

0

0

0

0

Figure 16: Propositional Planner based Modal Truth Criterion
Tweak (Chapman, 1987), example. less-committed planners compare
ua to? One might expect less-committed planner advantages
ua ua to. However, necessarily true. example,
section introduce Tweak-like planner, called mt, show search space
larger even to's circumstances.16
Figure 16 presents mt procedure. mt propositional planner based Chapman's
Modal Truth Criterion (Chapman, 1987), formal statement characterizes Tweak's
search space. straightforward see mt less committed ua. algorithms
quite similar; however, Step 4, whereas ua orders interacting steps, mt not.
Since mt immediately order interacting operators, may add additional
orderings previously introduced operators later planning process produce
correct plans.
proof ua's search tree larger to's search tree rested two
properties L elaborated Section 5. investigating relationship mt
to, found second property, disjointness property, hold mt,
failure illustrates mt explore plans (and, consequently,
ua) certain problems. disjointness property guarantees ua generate
\overlapping" plans. example Figure 17 shows mt fails satisfy property
generate plans share common linearizations, leading considerable
redundancy search tree. figure shows three steps, O1, O2, O3 , Oi
precondition pi added conditions gi, p1, p2, p3 . final step preconditions
g1, g2, g3, initial final steps shown figure. top
figure, plan constructed mt, goals g1 , g2, g3 achieved, p1 , p2,
p3 remain achieved. Subsequently, solving precondition p1, mt generates plans
share linearization O3 O2 O1 (among others). comparison,
16. use Tweak comparison because, like ua to, formal construct rather realistic
planner, therefore easily analyzed.

251

fiMinton, Bresina, & Drummond

O1
O2
O3

O1

O2

O2

O3

O3

O2

O1

O3

O1

O3

O2

O1

KEY
p O1
1

g1
p
1
p2
p3

p2
2

g2
p
1
p2
p3

p3 O3

g3
p
1
p2
p3

Figure 17: \Overlapping" plans.
ua generate plan O3 O2 O1 once. fact, simple show that,
breadth-first search, mt explores many plans example (and also
ua, transitivity) due redundancy search space.
result may seem counterintuitive. However, note search space size
partial-order planner potentially much greater total-order planner since
many partial orders set steps total orders. (Thus,
designing partial-order planner, one may preclude overlapping linearizations order
avoid redundancy, discussed McAllester & Rosenblitt, 1991 Kambhampati,
1994c.)
course, one also construct examples mt smaller search space
ua to. example simply illustrates although one planner may
less committed another, search space necessarily smaller. commitment
strategy used planner simply one factor uences overall performance.
particular, effect redundancy partial-order planner overwhelm considerations. comparing two planners, one must carefully consider mapping
search spaces concluding \less committed ) smaller search space".

10. Related Work
many years, intuitions underlying partial-order planning largely taken
granted. past years renewed interest fundamental
principles underlying issues.
252

fiTotal-Order Partial-Order Planning

Barrett et al. (1991) Barrett Weld (1994) describe interesting novel
analysis partial-order planning complements work. compare partialorder planner two total-order planners derived it, one searches space
plans, searches space world states. study focuses
goal structure problem affects eciency partial-order planning.
Specifically, examine partial-order total-order planning compare problems
independent, serializable, non-serializable goals, using resource-bounded
depth-first search. refine Korf's work serializable goals (Korf, 1987), introducing
distinction trivially serializable subgoals, subgoals solved
order without violating previously solved subgoal, laboriously serializable subgoals,
subgoals serializable, least 1=n orderings cause previously
solved subgoal violated. study describes conditions partial-order
planner may advantage. instance, show domain goals
trivially serializable partial-order planner laboriously serializable
total-order planners, partial-order planner performs significantly better.
study provides interesting contrast Barret Weld's work, since investigate relative eciencies partial-order total-order planning algorithms independent
particular domain structure. Instead, focus underlying properties
search space search strategy affects eciency planners. Nevertheless,
believe interesting relationships forms serializability
investigate, ideas solution density clustering discussed here.
illustrate this, consider artificial domain Barret Weld refer D1S 1,
where, problem, goals subset fG1; G2; : : :G15g, initial conditions
fI1; I2; : : :I15g, operator Oi2f1;2;:::;15g precondition Ii , adds Gi , deletes
Ii,1. follows solution D1S 1 contains operators Oi Oj < j , Oi
must precede Oj . domain, goals trivially serializable partial-order
planner laboriously serializable total-order planners; thus, partial-order
planner performs best. note also artificial domain, exactly one
solution per problem totally ordered. Therefore, immediately clear that,
give ua problems domain, ua's search tree generally
much smaller to's search tree. Since single solution planners,
solution density ua clearly greater to. Thus, properties
discussed paper provide basis analyzing differences subgoal
serializibility manifest effect search. subject, however, simple
might seem deserves study.
related work, Kambhampati written several papers (Kambhampati, 1994a,
1994b, 1994c) analyze design space partial-order planners, including ua
planner presented here. Kambhampati compares ua, Tweak, snlp (McAllester & Rosenblitt, 1991), ucpop (Penberthy & Weld, 1992), several planners along variety
dimensions. presents generalized schema partial order planning algorithms (Kambhampati, 1994c) shows commitment strategy used ua viewed
way increase tractability plan extension (or refinement) process. Kambhampati
also carries empirical comparison various planning algorithms particular problem (Kambhampati, 1994a), showing differences commitment strategies
affects eciency planning process. distinguishes two separate components
253

fiMinton, Bresina, & Drummond

branching factor, bt , former resulting commitment strategy
operator ordering (or terms, \tractability refinements") latter resulting
choice operator (\establishment refinements"). Kambhampati's experiments
demonstrate \eager" commitment strategies tend increase bt, sometimes
also decrease , number possible establishers reduced plans
ordered. is, course, closely related issues investigated paper.
addition, Kambhampati Chen (1993) compared relative utility reusing
partially ordered totally ordered plans \learning planners". showed
reuse partially ordered plans, rather totally ordered plans, result \storage compaction" represent large number different orderings. Moreover, partialorder planners advantage exploit plans effectively
total-order planners. many respects, advantages fundamentally similar
advantages ua derives potentially smaller search space.

11. Conclusions
focusing analysis single issue, namely, operator ordering commitment,
able carry rigorous comparative analysis two planners. shown
search space partial-order planner, ua, never larger search space
total-order planner, to. Indeed certain problems, ua's search space exponentially
smaller to's. Since ua pays small polynomial time increment per node
to, generally ecient.
showed ua's search space advantage may necessarily translate
eciency gain, depending subtle ways search strategy heuristics
employed planner. example, experiments suggest distribution-sensitive
search strategies, depth-first search, benefit partial orders
search strategies distribution-insensitive.
also examined variety extensions planners, order demonstrate
generality results. argued potential benefits partial-order
planning may retained even highly expressive planning languages. However,
showed partial-order planners necessarily smaller search spaces, since
\less-committed" strategies may create redundancies search space. particular, demonstrated Tweak-like planner, mt, larger search space
total-order planner problems.
general results? Although analysis considered two specific
planners, examined important tradeoffs general relevance.
analysis clearly illustrates planning language, search strategy, heuristics
used affect relative advantages two planning styles.
results paper considered investigation possible benefits
partial-order planning. ua constructed order us analyze
total-order/partial-order distinction isolation. reality, comparative behavior two
planners rarely clear (as witnessed discussion mt). general points
make applicable planners, chose two arbitrary planners, would
expect one planner clearly dominate other.
254

fiTotal-Order Partial-Order Planning

observations regarding interplay plan representation search strategy raise new concerns comparative analyses planners. Historically,
assumed representing plans partial orders categorically \better" representing plans total orders. results presented paper begin tell accurate
story, one interesting complex initially expected.

Appendix A. Proofs
A.1 Definitions

section defines terminology notation used proofs. notion plan
equivalence introduced plan step is, definition, uniquely labeled
operator instance, noted Section 3 Section 5. Thus, two plans
set steps. Although formalism simplifies analysis, requires us define plan
equivalence explicitly.

plan pair h; i, set steps, \before" relation ,
i.e., strict partial order . Notationally, O1 O2 (O1; O2) 2.
given problem, define search tree treeTO complete tree plans
generated algorithm problem. treeUA corresponding








search tree generated ua problem.
Two plans, P1 = h1; 1 P2 = h2 ; 2i said equivalent, denoted
P1 ' P2, exists bijective function f 1 2 that:
{ 2 1, f (O) instances operator,
{ O0; O00 2 1, O0 O00 f (O0) f (O00).
plan P2 1-step to-extension (or 1-step ua-extension) plan P1 P2
equivalent plan produced P1 one invocation (or ua).
plan P to-extension (or ua-extension) either:
{ P initial plan,
{ P 1-step to-extension (or 1-step ua-extension) to-extension (or uaextension).
immediately follows definition P member treeTO (or treeUA ),
P to-extension (or ua-extension). addition, P to-extension (or
ua-extension), plan equivalent P member treeTO (or
treeUA ).
P1 linearization P2 = h; 2i exists strict total order 1
2 1 P1 ' h; 1i.
Given search tree, let parent function plan parent plan tree.
Note P1 parent P2 , denoted P1 = parent(P2 ), P2 1-step
extension P1 .
255

fiMinton, Bresina, & Drummond

Given U 2 treeUA 2 treeTO , 2 L(U ) plan linearization
plan U either U root nodes respective search trees,
parent(T ) 2 L(parent(U )).
length plan number steps plan excluding first last
steps. Thus, initial plan length 0. plan P n steps length n , 2.
P1 subplan P2 = h2; 2i P1 ' h1; 1i,
{ 1 2
{ 1 2 restricted 1, i.e., 1 = 2 \ 1 1.
P1 strict subplan P2, P1 subplan P2 length P1 less
length P2.
solution plan P compact solution strict subplan P solution.

A.2 Extension Lemmas
TO-Extension Lemma: Consider totally ordered plans T0 = h0; 0i T1 = h1; 1i,
1 = 0 [ fOadd g 0 1 . Let G set false preconditions T0.

T1 1-step to-extension T0 if:
c = select-goal(G), c precondition step Oneed T0,
Oadd adds c,
(Oadd; Oneed) 21,
(Odel; Oadd) 21, Odel last deleter c T1.

Proof Sketch: lemma follows definition to. Given plan T0, false
precondition c, selects c goal, consider operators achieve c,
operator considers positions c last deleter c.
UA-Extension Lemma: Consider plan U0 = h0; 0i produced ua plan
U1 = h1; 1i, 1 = 0 [ fOaddg 0 1. Let G set false
preconditions steps U0 . U1 1-step ua-extension U0 if:
c = select-goal(G), c precondition step Oneed U0,
Oadd adds c,
1 minimal set consistent orderings
{ 0 1,
{ (Oadd; Oneed) 21,
{ (Odel; Oadd) 21, Odel last deleter c U1,
{ step U1 interacts Oadd
256

fiTotal-Order Partial-Order Planning

Proof Sketch: lemma follows definition ua. Given plan U0, false

precondition c, ua considers operators achieve c, operator ua
inserts plan c last deleter. ua considers
consistent combinations orderings new operator operators
interacts. orderings added plan.

A.3 Proof Search Space Correspondence L
Mapping Lemma: Let U0 = h0; u0i unambiguous plan let U1 = h1; u1i
1-step ua-extension U0. T1 = h1; t1i linearization U1 , exists

plan T0 T0 linearization U0 T1 1-step to-extension T0 .
Proof: Since U1 1-step ua-extension U0, step Oadd 1 = 0 [
fOaddg. Let T0 subplan produced removing Oadd T1; is, T0 = h0; t0i,
t0 = t1 \ 0 0 . Since u0 = u1 \ 0 0 t1 \ 0 0 = t0 , follows T0
linearization U0.
Using TO-Extension lemma, show T1 1-step to-extension T0.
First, T0 linearization U0 , two plans set goals. Therefore,
ua selects goal c expanding U0 , selects c extending T0. Second, must
case Oadd adds c since Oadd step ua inserted U0 make c true. Third,
Oadd Oneed T1, since Oadd Oneed U1 (by definition ua) since
T1 linearization U1. Fourth, Oadd last deleter c, Odel, T1, since Oadd
Odel U1 (by definition ua) since T1 linearization U1. Therefore,
conditions TO-Extension lemma hold and, thus, T1 1-step to-extension T0.
Q.E.D.

Totality Property every plan U treeUA , exists non-empty set fT1; : : :; Tmg
plans treeTO L(U ) = fT1; : : :; Tmg.
Proof: suces show plan U1 ua-extension plan T1 linearization

U1 , T1 to-extension. proof induction plan length.
Base case: statement trivially holds plans length 0.
Induction step: hypothesis statement holds plans length n,
prove statement holds plans length n + 1. Suppose U1 uaextension length n + 1 T1 linearization U1 . Let U0 plan U1
1-step ua-extension U0 . Mapping lemma, exists plan T0 T0
linearization U0 T1 1-step to-extension T0. induction hypothesis, T0
to-extension. Therefore, definition, T1 also to-extension. Q.E.D.

Disjointness Property: L maps distinct plans treeUA disjoint sets plans treeTO ;
is, U1 ; U2 2 treeUA U1 =
6 U2, L(U1) \ L(U2) = fg.
Proof: definition L, T1; T2 2 L(U ), T1 T2 tree depth
treeTO ; furthermore, U also depth treeUA . Hence, suces prove
plans U1 U2 depth treeUA U1 6= U2 , L(U1 ) \ L(U2 ) = fg.

Base case: statement vacuously holds depth 0.
Induction step: hypothesis statement holds plans depth n,
prove, contradiction, statement holds plans depth n + 1. Suppose
257

fiMinton, Bresina, & Drummond

exist two distinct plans, U1 = h1; 1 U2 = h2 ; 2i, depth n + 1
treeUA 2 L(U1) \L(U2). (by definition L), parent(T ) 2 L(parent(U1))
parent(T ) 2 L(parent(U2 )). Since parent(U1 ) 6= parent(U2 ) contradicts induction
hypothesis, suppose U1 U2 parent U0 . Then, definition
ua either (i) 1 6= 2 (ii) 1 = 2 1 6=2 . first case, since two
plans contain set plan steps, disjoint linearizations and,
hence, L(U1 ) \ L(U2 ) = fg, contradicts supposition. second case,
1 = 2; hence, plans resulted adding plan step Oadd parent plan. Since
16=2, exists plan step Oint interacts Oadd one plan Oint
ordered Oadd plan Oadd ordered Oint . Thus, either
case, linearizations two plans disjoint and, hence, L(U1 ) \ L(U2 ) = fg,
contradicts supposition. Therefore, statement holds plans depth n + 1.
Q.E.D.

A.4 Completeness Proof

prove complete breadth first search control strategy. so,
suces prove exists solution problem, exists to-extension
compact solution. so, prove following lemma.

Subplan Lemma: Let totally ordered plan T0 strict subplan compact solution Ts.

exists plan T1 T1 subplan Ts T1 1-step to-extension
T0.
Proof: Since T0 strict subplan Ts Ts compact solution, set false
preconditions T0, G, must empty. Let c = select-goal(G), let Oneed
step T0 precondition c, let Oadd step Ts achieves c. Consider
totally ordered plan T1 = h0 [ fOadd g; 1i, 1 . Clearly, T1 subplan
Ts. Furthermore, TO-Extension Lemma, T1 1-step extension T0 to.
see this, note Oadd ordered Oneed T1 since ordered Oneed Ts .
Similarly, Oadd ordered last deleter c T0 since deleter c T0
deleter c Ts , Oadd ordered deleters c Ts . Thus, conditions
TO-Extension Lemma hold. Q.E.D.


Completeness Theorem: plan Ts totally ordered compact solution, Ts

to-extension.
Proof: Let n length Ts. show k n, exists subplan Ts
length k to-extension. sucient prove result since subplan
exactly length n equivalent Ts . proof induction k.
Base case: k = 0 statement holds since initial plan, length 0,
subplan solution plan.
Induction step: assume statement holds k show k < n
statement holds k + 1. induction hypothesis, exists plan T0 length k
strict subplan Ts . Subplan Lemma, exists plan T1
subplan Ts 1-step to-extension T0. Thus, exists subplan Ts length
k + 1. Q.E.D.
258

fiTotal-Order Partial-Order Planning

A.5 Completeness Proof UA

prove ua complete breadth-first search strategy. result follows
search space correspondence defined L fact complete.
particular, show to-extension , exists ua-extension U
linearization U . Since ua produces unambiguous plans, must
case solution, U also solution. this, follows immediately ua
complete.

Inverse Mapping Lemma: Let T0 = h0; t0i totally ordered plan. Let T1 =
h1; t1i 1-step to-extension T0. Let U0 = h0; u0i plan produced ua
T0 linearization U0 . exists plan U1 T1 linearization
U1 U1 1-step ua-extension U0 .
Proof: definition to, 1 = 0 [ fOaddg, Oadd added c
false precondition plan step Oneed U0 . Consider U1 = h1 ; u1 i, u1
minimal subset t1 that:
u0 u1,
(Oadd; Oneed) 2u1,
(Odel; Oadd) 2u1 , Odel last deleter c U1,
step U1 interacts Oadd
Since u1 t1 , T1 linearization U1. addition, U1 extension U0 since
meets three conditions UA-Extension Lemma, follows. First, since c must
goal selected extending T0, c must likewise selected ua
extending U0 . Second, Oadd adds c since Oadd achieves c T0. Finally, construction,
u1 satisfies third condition UA-Extension Lemma. Q.E.D.
UA Completeness Theorem: Let Ts totally ordered compact solution.

exists ua-extension Us Ts linearization Us .
Proof: Since complete, suces show T1 to-extension,
exists ua-extension U1 T1 linearization U1 . proof induction
plan length.
Base case: statement trivially holds plans length 0.
Induction step: hypothesis statement holds plans length n,
prove statement holds plans length n + 1. Assume T1 to-extension
length n + 1, let T0 plan T1 1-step to-extension T0.
induction hypothesis, exists ua-extension U0 length n T0
linearization U0. Inverse Mapping Lemma, exists plan U1
linearization T1 1-step ua-extension U0 . Since U1 1-step ua-extension
U0, length n + 1. Q.E.D.

259

fiMinton, Bresina, & Drummond

Acknowledgements
work present paper originally described two conference papers
(Minton et al., 1991a, 1992). thank Andy Philips many contributions
project. wrote code planners helped conduct experiments. also
thank three anonymous reviewers excellent comments.

References

Backstrom, C. (1993). Finding least constrained plans optimal parallel executions
harder thought. Proceedings Second European Workshop
Planning.
Barrett, A., Soderland, S., & Weld, D. (1991). effect step-order representations
planning. Tech. rep. 91-05-06, Univ. Washington, Computer Science Dept.
Barrett, A., & Weld, D. (1994). Partial-order planning: Evaluating possible eciency gains.
Artificial Intelligence, 67 (1), 71{112.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32, 333{377.
Chen, P. (1989). Heuristic Sampling Backtrack Trees. Ph.D. thesis, Dept. Computer
Science, Stanford Univ., Stanford, CA.
Collins, G., & Pryor, L. (1992). Achieving functionality filter conditions partial
order planner. Proceedings Tenth National Conference Artificial Intelligence.
Crawford, J., & Baker, A. (1994). Experimental results application satisfiability
algorithms scheduling problems. Proceedings Twelfth National Conference
Artificial Intelligence.
Dean, T., & Boddy, M. (1988). Reasoning partially ordered events. Artificial Intelligence, 36, 375{399.
Drummond, M., & Currie, K. (1989). Goal-ordering partially ordered plans. Proceedings Eleventh International Joint Conference Artificial Intelligence.
Ginsberg, M., & Harvey, W. (1992). Iterative broadening. Artificial Intelligence, 55, 367{
383.
Godefroid, P., & Kabanza, F. (1991). ecient reactive planner synthesizing reactive
plans. Proceedings Ninth National Conference Artificial Intelligence.
Hertzberg, J., & Horz, A. (1989). Towards theory con ict detection resolution
nonlinear plans. Proceedings Eleventh International Joint Conference
Artificial Intelligence.
Kambhampati, S. (1994a). Design tradeoffs partial order (plan space) planning.
Proceedings Second International Conference AI Planning Systems.
260

fiTotal-Order Partial-Order Planning

Kambhampati, S. (1994b). Multi contributor causal structures planning: formalization
evaluation. Artificial Intelligence, 69, 235{278.
Kambhampati, S. (1994c). Refinement search unifying framework analyzing plan
space planners. Proceedings Fourth International Conference Principles
Knowledge Representation Reasoning.
Kambhampati, S., & Chen, J. (1993). Relative utility EBG-based plan reuse partial ordering vs. total ordering planning. Proceedings Eleventh National Conference
Artificial Intelligence.
Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. Artificial Intelligence, 27, 97{109.
Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33,
65{88.
Langley, P. (1992). Systematic nonsystematic search strategies. Proceedings
First International Conference AI Planning Systems.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proceedings
Ninth National Conference Artificial Intelligence.
Minton, S., Bresina, J., & Drummond, M. (1991a). Commitment strategies planning:
comparative analysis. Proceedings Twelfth International Joint Conference
Artificial Intelligence.
Minton, S., Bresina, J., Drummond, M., & Philips, A. (1991b). analysis commitment
strategies planning: details. Tech. rep. 91-08, NASA Ames, AI Research
Branch.
Minton, S., Drummond, M., Bresina, J., & Philips, A. (1992). Total order vs. partial order
planning: Factors uencing performance. Proceedings Third International
Conference Principles Knowledge Representation Reasoning.
Pednault, E. (1988). Synthesizing plans contain actions context-dependent effects.
Computational Intelligence, 4, 356{372.
Penberthy, J., & Weld, D. (1992). UCPOP: sound, complete, partial-order planner
adl. Proceedings Third International Conference Principles Knowledge
Representation Reasoning.
Regnier, P., & Fade, B. (1991). Complete determination parallel actions temporal
optimization linear plans action. Hertzberg, J. (Ed.), European Workshop
Planning, Vol. 522 Lecture Notes Artificial Intelligence, pp. 100{111 Sankt
Augustin, Germany. Springer.
Rosenbloom, P., Lee, S., & Unruh, A. (1993). Bias planning explanation-based learning. Minton, S. (Ed.), Machine Learning Methods Planning. Morgan Kaufmann
Publishers.
261

fiMinton, Bresina, & Drummond

Sacerdoti, E. (1975). nonlinear nature plans. Proceedings Fourth International Joint Conference Artificial Intelligence.
Sacerdoti, E. (1977). Structure Plans Behavior. American Elsivier, New York.
Tate, A. (1974). Interplan: plan generation system deal interactions
goals. Tech. rep. Memo MIP-R-109, Univ. Edinburgh, Machine Intelligence
Research Unit.
Tate, A. (1977). Generating project networks. Proceedings Fifth International
Joint Conference Artificial Intelligence.
Veloso, M., Perez, M., & Carbonell, J. (1990). Nonlinear planning parallel resource
allocation. Proceedings Workshop Innovative Approaches Planning,
Scheduling Control.
Waldinger, R. (1975). Achieving several goals simultaneously. Machine Intelligence 8.
Ellis Harwood, Ltd.
Warren, D. (1974). Warplan: system generating plans. Tech. rep. Memo 76, Computational Logic Dept., School AI, Univ. Edinburgh.

262

fiJournal Artificial Intelligence Research 2 (1995) 411-446

Submitted 11/94; published 4/95

Rerepresenting Restructuring Domain Theories:
Constructive Induction Approach
Steven K. Donoho
Larry A. Rendell

Department Computer Science, Univeristy Illinois
405 N. Mathews Ave., Urbana, IL 61801 USA

donoho@cs.uiuc.edu
rendell@cs.uiuc.edu

Abstract

Theory revision integrates inductive learning background knowledge combining
training examples coarse domain theory produce accurate theory.
two challenges theory revision theory-guided systems face. First,
representation language appropriate initial theory may inappropriate
improved theory. original representation may concisely express initial theory,
accurate theory forced use representation may bulky, cumbersome,
dicult reach. Second, theory structure suitable coarse domain theory may
insucient fine-tuned theory. Systems produce small, local changes
theory limited value accomplishing complex structural alterations may
required.
Consequently, advanced theory-guided learning systems require exible representation
exible structure. analysis various theory revision systems theory-guided
learning systems reveals specific strengths weaknesses terms two desired
properties. Designed capture underlying qualities system, new system uses
theory-guided constructive induction. Experiments three domains show improvement
previous theory-guided systems. leads study behavior, limitations,
potential theory-guided constructive induction.

1. Introduction
Inductive learners normally use training examples, also use background knowledge. Effectively integrating knowledge induction widely studied research problem. work date area theory revision
knowledge given coarse, perhaps incomplete incorrect, theory problem domain,
training examples used shape initial theory refined, accurate
theory (Ourston & Mooney, 1990; Thompson, Langley, & Iba, 1991; Cohen, 1992; Pazzani
& Kibler, 1992; Baffes & Mooney, 1993; Mooney, 1993). develop exible
robust approach problem learning data theory knowledge
addressing two following desirable qualities:

Flexible Representation. theory-guided system utilize knowledge contained initial domain theory without adhere closely initial
theory's representation language.

Flexible Structure. theory-guided system unnecessarily restricted
structure initial domain theory.

c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDonoho & Rendell

giving precise definitions terms, motivate work intuitively.

1.1 Intuitive Motivation
first desirable quality, exibility representation, arises theory representation appropriate describing coarse, initial domain theory may inadequate
final, revised theory. initial domain theory may compact concise
one representation, accurate theory may quite bulky cumbersome representation. Furthermore, representation best expressing initial theory may
best carrying refinements. helpful refinement step may clumsy
make initial representation yet carried quite simply another representation.
simple example, coarse domain theory may expressed logical conjunction
N conditions met. accurate theory, though, one
N conditions holds. Expressing accurate theory DNF representation
used describe initial theory would cumbersome unwieldy (Murphy & Pazzani,
1991). Furthermore, arriving final theory using refinement operators suitable
DNF (drop-condition, add-condition, modify-condition) would cumbersome task.
M-of-N representation adopted, refinement simply involves empirically
finding appropriate M, final theory expressed concisely (Baffes & Mooney,
1993).
Similarly, second desirable quality, exibility structure, arises theory
structure suitable coarse domain theory may insucient fine-tuned
theory. order achieve desired accuracy, restructuring initial theory may
necessary. Many theory revision systems act making series local changes,
lead behavior two extremes. first extreme rigidly retain backbone
structure initial domain theory, allowing small, local changes. Figure 1 illustrates
situation. Minor revisions made { conditions added, dropped,
modified { refined theory trapped backbone structure initial
theory. local changes needed, techniques proven useful (Ourston
& Mooney, 1990), often required. required, systems often
move extreme; drop entire rules groups rules build entire
new rules groups rules scratch replace them. Thus restructure,
forfeit valuable knowledge process. ideal theory revision system would glean
knowledge theory substructures cannot fixed small, local changes
use restructured theory.
intuitive illustration, consider piece software \almost works." Sometimes
made useful local operations: fixing couple bugs, adding
needed subroutine, on. cases, though, piece software \almost
works" fact far full working order. may need redesigned restructured.
mistake one extreme try fix program like making series patches
original code. mistake extreme discard original program without
learning anything start scratch. best approach would examine
original program see learned design use knowledge
redesign. Likewise, attempting improve coarse domain theory series
local changes may yield little improvement theory trapped initial
412

fiRerepresenting Restructuring Domain Theories

Initial Theory



b

c

j

e

h

Refined Theory

n

k l





b

c

p q r

j

w e

f g



n

k l

u



p

v

r

f g

Figure 1: Typical theory revision allows limited structural exibility. Although conditions added, dropped, modified, revised theory much
constrained structure initial theory.
structure. render original domain theory useless; careful analysis
initial domain theory give valuable guidance design best final theory.
illustrated Figure 2 many substructures taken initial
theory adapted use refined theory. Information initial theory
used, structure revised theory restricted structure
initial theory.
Initial Theory



b

c

n

j

e

h

Refined Theory

k l

k

p q r

l

f



g



e

h







p

u

v

f g q r

f g

Figure 2: exible structural modification. revised theory taken many substructures initial theory adapted recombined use,
structure revised theory restricted structure
initial theory.

413

fiDonoho & Rendell

1.2 Terminology

paper, training data consist examples classified vectors feature/value pairs. assume initial theory set conditions combined using
operators AND, OR, indicating one classes. unreasonable
believe theories always form, covers much existing theory revision
research.
work intended informal exploration exible representation exible
structure. Flexible representation means allowing theory revised using representation language initial theory. example exible representation
introduction new operator combining features | operator used
initial theory. Section 1.1 example given introducing M-of-N operator
represent theory originally expressed DNF. Flexible structure means limiting
revision theory series small, incremental modifications. example
breaking theory components using building blocks
construction new theory.
Constructive induction process whereby training examples redescribed using
new set features. new features combinations original features. Bias
knowledge may used construction new features. subtle point
speak exible representation, referring representation
domain theory, training data. Although phrase \change representation"
often applied constructive induction, refers change data. paper,
term exible representation reserved change theory representation. Thus
system performing constructive induction (changing feature language
data) without exhibiting exible representation (changing representation theory).

1.3 Overview

Theory revision constructive induction embody complementary aspects machine
learning research community's ultimate goals. Theory revision uses data improve
theory; constructive induction use theory improve data facilitate learning.
paper present theory-guided constructive induction approach addresses
two desirable qualities discussed Section 1.1. initial theory analyzed, new
features constructed based components theory. constructed features
need expressed representational language initial theory
refined better match training examples. Finally, standard inductive learning
algorithm, C4.5 (Quinlan, 1993), applied redescribed examples.
begin analyzing landmark theory revision learning systems exhibited exibility handling domain theory part played performance. analysis, extract guidelines system design apply
design limited system. effort integrate learning theory data,
borrow heavily theory revision, multistrategy learning, constructive induction communities, guidelines system design fall closest classical constructive
induction methods. central focus paper presentation \another new
system" rather study exible representation structure, manifestation
previous work, guidance future design.
414

fiRerepresenting Restructuring Domain Theories

Section 2 gives context work analyzing previous research uence work. Section 3 explores Promoter Recognition domain demonstrates
related theory revision systems behave domain. Section 4, guidelines
theory-guided constructive induction presented. guidelines synthesis
positive aspects related research, address two desirable qualities, exibility
representation exibility structure. Section 4 also presents specific theory-guided
constructive induction algorithm instantiation guidelines set forth earlier
section. Results experiments three domains given Section 5 followed
discussion strengths theory-guided constructive induction Section 6. Section 7 presents experimental analysis limits applicability simple algorithm
followed discussion limitations future directions work Section 8.

2. Context Related Work

Although work bears resemblance form objective many papers constructive induction (Michalski, 1983; Fu & Buchanan, 1985; Utgoff, 1986; Schlimmer, 1987;
Drastal & Raatz, 1989; Matheus & Rendell, 1989; Pagallo & Haussler, 1990; Ragavan &
Rendell, 1993; Hirsh & Noordewier, 1994), theory revision (Ourston & Mooney, 1990; Feldman, Serge, & Koppel, 1991; Thompson et al., 1991; Cohen, 1992; Pazzani & Kibler, 1992;
Baffes & Mooney, 1993), multistrategy approaches (Flann & Dietterich, 1989; Towell,
Shavlik, & Noordeweir, 1990; Dzerisko & Lavrac, 1991; Bloedorn, Michalski, & Wnek, 1993;
Clark & Matwin, 1993; Towell & Shavlik, 1994), focus upon handful systems, significant, underlying similarities work. section
analyze Miro, Either, Focl, LabyrinthK , Kbann, Neither-MofN, Grendel
discuss related underlying contributions relationship perspective.

2.1

Miro

(Drastal & Raatz, 1989) seminal work knowledge-guided constructive induction. takes knowledge low-level features interact uses knowledge
construct high-level features training examples. standard learning algorithm
run examples described using new features. domain theory used
shift bias induction problem (Utgoff, 1986). Empirical results showed
describing examples high-level, abstract terms improved learning accuracy.
Miro approach provides means utilizing knowledge domain theory without
restricted structure theory. Substructures domain theory
used construct high-level features standard induction algorithm arrange
concept. constructed features used are, others ignored,
others combined low-level features, still others may used differently
multiple contexts. end result knowledge domain theory utilized,
structure final theory restricted structure initial theory.
Miro provides exible structure.
Another benefit Miro-like techniques applied even partial
domain theory exists, i.e., domain theory specifies high-level features
link together domain theory specifies high-level features
others. One Miro's shortcomings provided means making minor changes
Miro

415

fiDonoho & Rendell

domain theory rather constructed features exactly domain theory
specified. Also representation Miro's constructed features primitive | either
example met conditions high-level feature not. example Miro's
behavior given Section 3.2.

2.2

,

, LabyrinthK

Either Focl



Either

(Ourston & Mooney, 1990), LabyrinthK (Thompson et al., 1991),
Focl (Pazzani & Kibler, 1992) systems represent broad spectrum theory revision
work. make steps toward effective integration background knowledge inductive
learning. Although systems many superficial differences regard supervised/unsupervised learning, concept description language, etc., share underlying
principle incrementally revising initial domain theory series local changes.
discuss Either representative class systems. Either's theory
revision operators include: removing unwanted conditions rule, adding needed conditions rule, removing rules, adding totally new rules. Either first classifies
training examples according current theory. misclassified, seeks repair
theory applying theory revision operator result correct classification
previously misclassified examples without losing correct examples. Thus
series local changes made allow improvement accuracy training
set without losing examples previously classified correctly.
Either-type methods provide simple yet powerful tools repairing many important
common faults domain theories, fail meet qualities exible representation exible structure. theory revision operators make small, local
modifications existing domain theory, final theory constrained similar
structure initial theory. accurate theory significantly different structure initial theory, systems forced one two extremes discussed
Section 1. first extreme become trapped local maximum similar
initial theory unable reach global maximum local changes made.
extreme drop entire rules groups rules replace new
rules built scratch thus forfeiting knowledge contained domain theory.
Also, Either carries theory revision steps representation initial
theory. Consequently, representation final theory initial
theory. Another representation may appropriate revised theory
one initial theory comes, facilities provided accommodate this.
advanced theory revision system would combine locally acting strengths Eithertype systems exibility structure exibility representation. example
Either's behavior given Section 3.3.

2.3

Kbann

Neither-MofN

Kbann system (Towell et al., 1990; Towell & Shavlik, 1994) makes unique contributions theory revision work. Kbann takes initial domain theory described symbolically
logic creates neural network whose structure initial weights encode theory.
Backpropagation (Rumelhart, Hinton, & McClelland, 1986) applied refinement tool fine-tuning network weights. Kbann empirically shown give
416

fiRerepresenting Restructuring Domain Theories

significant improvement many theory revision systems widely-used Promoter
Recognition domain. Although work different implementation Kbann,
abstract ideologies similar.
One Kbann's important contributions takes domain theory one representation (propositional logic) translates less restricting representation (neural
network). logic appropriate representation initial domain theory
promoter problem, neural network representation convenient refining
theory expressing best revised theory. change representation
Kbann's real source power. Much attention given fact Kbann combines symbolic knowledge subsymbolic learner, combination viewed
generally means implementing important change representation. may
change representation gives Kbann power, necessarily specific
symbolic/subsymbolic implementation. Thus Kbann system embodies higher-level
principle allowing refinement occur appropriate representation.
alternative representation Kbann's source power, question must raised
whether actual Kbann implementation always best means achieving
goal. neural network representation may expressive required. Accordingly, backpropagation often refinement power needed. Thus Kbann may
carry excess baggage translating neural net representation, performing expensive
backpropagation, extracting symbolic rules refined network. Although full
extent Kbann's power may needed problems, many important problems may
solvable applying Kbann's principles symbolic level using less expensive tools.
Neither-MofN (Baffes & Mooney, 1993), descendant Either, second example
system allows theory revised representation
initial theory. domain theory input Neither-MofN expressed propositional
logic AND/OR tree. Neither-MofN interprets theory less rigidly |
rule true time N conditions true. Initially set equal N (all
conditions must true rule true), one theory refinement operator
lower particular rule. end result examples close enough
partial match initial theory accepted. Neither-MofN, since built upon
Either framework, also includes Either-like theory revision operators: add-condition,
drop-condition, etc.
Thus Neither-MofN allows revision take place representation appropriate
revision appropriate concisely expressing best refined theory. NeitherMofN achieved results comparable Kbann Promoter Recognition domain,
suggests change representation two systems share
give power rather particular implementation. Neither-MofN also
demonstrates small amount representational exibility sometimes enough.
M-of-N representation employs big change original representation
neural net representation Kbann employs yet achieves similar results
arrives much quickly Kbann (Baffes & Mooney, 1993).
shortcoming Neither-MofN since acts making local changes
initial theory, still become trapped structure initial theory. advanced
theory revision system would incorporate Neither-MofN's Kbann's exibility
417

fiDonoho & Rendell

representation allow knowledge-guided theory restructuring. Examples
Neither-MofN's behavior given Sections 3.4 3.5.

2.4

's

Kbann

Grendel

Cohen (1992) analyzes class theory revision systems draws insightful conclusions. One \generality [in theory interpretation] comes expense power."
draws principle fact system Either Focl treats every
domain theory therefore must treat every domain theory general way. argues rather applying general refinement strategy
every problem, small set refinement strategies available narrow
enough gain leverage yet narrow apply single problem. Cohen
presents Grendel, toolbox translators transforms domain theory
explicit bias. translator interprets domain theory different way,
appropriate interpretation applied given problem.
apply Cohen's principle representation domain theories. domain
theories translated representation, general, adaptable representation used order accommodate general case. comes
expense higher computational costs possibly lower accuracy due overfit
stemming unbridled adaptability. neural net representation Kbann
translates domain theories allows 1) measure partial match domain theory 2) different parts domain theory weighted differently 3) conditions added
dropped domain theory. options adaptability probably
necessary problems may even detrimental. options Kbann also
require computationally expensive backpropagation method.
representation used Neither-MofN adaptable Kbann's |
allow individual parts domain theory weighted differently. NeitherMofN runs quickly Kbann small problems probably matches even
surpasses Kbann's accuracy many domains | domains fine-grained weighting
unfruitful even detrimental. toolbox theory rerepresentation translators analogous
Grendel would allow domain theory translated representation
appropriate forms adaptability.

2.5 Outlook Summary

summary, brie reexamine exible representation exible structure, two
desirable qualities set forth Section 1. consider various systems exemplify
subset desirable qualities.
Kbann Neither-MofN interpreted theory exibly original
representation allowed revised theory adaptable representation.
final, refined theory often many exceptions rule; may tolerate partial
matches missing pieces evidence; may weight evidence heavily
evidence. Kbann's Neither-MofN's new representation may
concise, appropriate representation initial theory, new
representation allows concise expression otherwise cumbersome final theory.
cases principle exible representation.
418

fiRerepresenting Restructuring Domain Theories

Standard induction programs quite successful building concise theories
high predictive accuracy target concept concisely expressed using
original set features. can't, constructive induction means creating
new features target concept concisely expressed. Miro uses
constructive induction take advantage strengths domain theory
standard induction. Knowledge theory guides construction appropriate
new features, standard induction structures concise description
concept. Thus Miro-like construction coupled standard induction provides
ready powerful means exibly restructuring knowledge contained
initial domain theory. case principle exible structure.

following section introduce DNA Promoter Recognition domain order
illustrate tangibly systems discussed integrate knowledge
induction.

3. Demonstrations Related Work
section introduces Promoter Recognition domain (Harley, Reynolds, & Noordewier,
1990) brie illustrates Miro-like system, Either, Kbann, NeitherMofN behave domain. implemented Miro-like system promoter domain; versions Either Neither-MofN available Ray Mooney's group;
Kbann's behavior described analyzing (Towell & Shavlik, 1994). chose promoter domain non-trivial, real-world problem number theory
revision researchers used test work (Ourston & Mooney, 1990; Thompson
et al., 1991; Wogulis, 1991; Cohen, 1992; Pazzani & Kibler, 1992; Baffes & Mooney, 1993;
Towell & Shavlik, 1994). promoter domain one three domains evaluate
work, theory-guided constructive induction, Section 5.

3.1 Promoter Recognition Domain

promoter sequence region DNA marks beginning gene. example promoter recognition domain region DNA classified either promoter
non-promoter. illustrated Figure 3, examples consist 57 features representing sequence 57 DNA nucleotides. feature take values A,G,C,
representing adenine, guanine, cytosine, thymine corresponding DNA position.
features labeled according position p-50 p+7 (there zero
position). notation \p-N " denotes nucleotide N positions upstream
beginning gene. goal predict whether sequence promoter
nucleotides. total 106 examples available: 53 promoters 53 non-promoters.
promoter recognition problem comes initial domain theory shown Figure 4 (quoted almost verbatim Towell Shavlik's entry UCI Machine Learning
Repository). theory states promoter sequences must two regions make
contact protein must also acceptable conformation pattern.
four possibilities contact region minus 35 (35 nucleotides upstream beginning gene). match four possibilities satisfy minus 35
contact condition, thus joined disjunction. Similarly, four possibilities
419

fiDonoho & Rendell

DNA Sequence

p-50

p+7

C G C
Figure 3: instance promoter domain consists sequence 57 nucleotides
labeled p-50 p+7. nucleotide take values A,G,C,
representing adenine, guanine, cytosine, thymine.
contact region minus 10 four acceptable conformation patterns. Figure 5
gives pictorial presentation portions theory. 106 examples
dataset, none matched domain theory exactly, yielding accuracy 50%.

3.2

Miro

Promoter Domain

Miro-like system promoter domain would use rules Figure 4 construct new high-level features DNA segment. Figure 6 shows example this.
DNA segment shown position p-38 position p-30. minus 35 rules
theory also shown, four new features (feat minus35 feat minus35 D)
constructed DNA segment, one minus 35 rule. new features feat minus35 feat minus35 value 1 DNA fragment
matches first fourth minus 35 rules. Likewise, feat minus35 B feat minus35 C
value 0 DNA fragment match second third
rules. Furthermore, since four minus 35 rules joined disjunction, new feature,
feat minus35 all, created group would value 1 least one
minus 35 rules matches.
New features would similarly created minus 10 rules conformation
rules, standard induction algorithm could applied. implemented Mirolike system; Figure 7 gives example theory created it. (Drastal's original Miro used
candidate elimination algorithm (Mitchell, 1977) underlying induction algorithm.
used C4.5 (Quinlan, 1993).) opposed theory revision systems incrementally
modify domain theory, Miro broken theory components
fashioned components new theory using standard induction program. Thus
Miro exhibited exible structure principle domain { restricted
way structure initial theory. Rather, Miro exploited strengths
standard induction concisely characterize training examples using new features.
420

fiRerepresenting Restructuring Domain Theories

Promoters region protein (RNA polymerase) must make contact
helical DNA sequence must valid conformation two pieces
contact region spatially align. Prolog notation used.
promoter :- contact, conformation.
two regions "upstream" beginning gene
RNA polymerase makes contact.
contact

:- minus_35, minus_10.

following rules describe compositions possible contact regions.
minus_35
minus_35
minus_35
minus_35

:- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
:p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

minus_10
minus_10
minus_10
minus_10

:- p-14=t, p-13=a, p-12=t, p-11=a, p-10=a, p-9=t.
:p-13=t, p-12=a,
p-10=a,
p-8=t.
:p-13=t, p-12=a, p-11=t, p-10=a, p-9=a, p-8=t.
:p-12=t, p-11=a,
p-7=t.

following rules describe sequences produce acceptable conformations.
conformation :- p-47=c,
p-18=t,
p-1=c.
conformation :- p-45=a,
conformation :- p-49=a,
conformation :- p-45=a,
p-15=t,

p-46=a, p-45=a, p-43=t, p-42=t, p-40=a, p-39=c, p-22=g,
p-16=c, p-8=g, p-7=c, p-6=g, p-5=c, p-4=c, p-2=c,
p-44=a, p-41=a.
p-44=t, p-27=t, p-22=a, p-18=t, p-16=t, p-15=g, p-1=a.
p-41=a, p-28=t, p-27=t, p-23=t, p-21=a, p-20=a, p-17=t,
p-4=t.

Figure 4: initial domain theory recognizing promoters (from Towell Shavlik).
weakness Miro displays example allows exibility representation
theory. representation features constructed Miro basically
all-or-none representation initial theory; either DNA segment matched rule,
not.

3.3

Either

Promoter Domain

Either-like system refines initial promoter theory dropping adding
conditions rules. simulated Either turning M-of-N option Neither
ran promoter domain. Figure 8 shows refined theory produced using
randomly selected training set size 80. initial promoter domain theory
lend revision small, local changes, Either limited success.
421

fiDonoho & Rendell

DNA Sequence

p-50

p+7

Contact minus_35

Contact minus_10

-37 -36 -35 -34 -33 -32 -31

C

G C

*

-14 -13 -12 -11 -10 -9

-8 -7



* *









* G * C

*





*



* G C

*





* G C

* *


*



*

*

*

*

*

*

Figure 5: contact portion theory. four possibilities
minus 35 minus 10 portions theory. \*" matches nucleotide.
conformation portion theory spread display pictorially.
run, program exhibited second behavioral extreme discussed Section 1;
entirely removed groups rules tried build new rules replace
lost. minus 10 conformation rules essentially removed, new rules
added minus 35 group. new minus 35 rules contain condition
p-12=t previously found minus 10 group condition p-44=a previously found
conformation group.
Either's behavior example direct result lack exibility representation exibility structure. dicult transform minus 10 conformation
rules something useful initial representation using Either's locally-acting operators. Either handles dropping sets rules, losing knowledge,
attempting rediscover lost knowledge empirically. end result loss
knowledge lower optimal accuracy shown later Section 5.

3.4

Kbann

Promoter Domain

Figure 9, modeled figure Towell Shavlik (1994), shows setup Kbann
network promoter theory. slot along bottom represents one nucleotide
DNA sequence. node first level bottom embodies single
domain rule, higher levels encode groups rules final concept top.
links shown figure ones initially high-weighted. net next filled
fully connected low-weight links. Backpropagation applied refine
network's weights.
422

fiRerepresenting Restructuring Domain Theories

DNA segment fragment:
:::

p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=t, p-30=t

:::

minus 35 group rules corresponding constructed features:
minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
minus 35 :p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

feat minus35 = 1
feat minus35 B = 0
feat minus35 C = 0
feat minus35 = 1

feat minus35 = (feat minus35 _ feat minus35 B _ feat minus35 C _ feat minus35 D) = 1

Figure 6: example feature construction Miro-like system. constructed
features first fourth rules minus 35 group true (value = 1)
DNA segment matches rules. constructed feature
entire group, feat minus35 all, true four minus 35 rules joined
disjunction.
feat_minus10_all
0

1
promoter

feat_conf_B
0

1

feat_minus35_D
0
non-promoter

promoter
1

promoter

Figure 7: example theory created Miro-like system. DNA segment recognized
promoter matches minus 10 rules, second conformation
rule, fourth minus 35 rule.
neural net representation appropriate domain propositional
logic representation initial theory. allows measurement partial match
weighting links way subset rule's conditions enough surpass
node's threshold. also allows variable weightings different parts theory; therefore, predictive nucleotides weighted heavily, slightly predictive
nucleotides weighted less heavily. Kbann limited exibility structure.
refined network result series incremental modifications
initial network, fundamental restructuring theory embodies unlikely. Kbann
423

fiDonoho & Rendell

promoter :- contact, conformation.
contact

:- minus_35, minus_10.

minus_35
minus_35
minus_35
minus_35
minus_35
minus_35

::::::-

p-35=t,
p-36=t,
p-36=t,
p-34=g,
p-34=g,
p-35=t,

p-34=g.
p-33=a, p-32=c.
p-32=c, p-50=c.
p-12=t.
p-44=a.
p-47=g.

minus_10 :- true.
conformation :- true.

Figure 8: revised theory produce Either.
promoter

contact

conformation
minus_35

p-50

minus_10

DNA Sequence

p+7

Figure 9: setup Kbann network promoter theory.
limited finding best network fundamental structure imposed
initial theory.
One Kbann's advantages uses standard learning algorithm foundation. Backpropagation widely used consequently improved previous
researchers. Theory refinement tools built ground use standard
tool tangentially suffer invent methods handling standard
problems overfit, noisy data, etc. wealth neural net experience resources
available Kbann user; neural net technology advances, Kbann technology
passively advance it.
424

fiRerepresenting Restructuring Domain Theories

3.5

Neither-MofN

Promoter Domain

refines initial promoter theory dropping adding conditions rules also allowing conjunctive rules true subset
conditions true. ran Neither-MofN randomly selected training set size
80, Figure 10 shows refined promoter theory produced. theory expressed
9 M-of-N rules would require 30 rules using propositional logic, initial theory's
representation. importantly, unclear system using initial representation would reach 30-rule theory initial theory. Thus M-of-N representation
adopted allows concise expression final theory also facilitates
refinement process.
Neither-MofN

promoter :- 2 ( contact, conformation ).
contact

:- 2 ( minus_35, minus_10 ).

minus_35 :- 2 ( p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a ).
minus_35 :- 5 ( p-36=t, p-35=t, p-34=g, p-33=a, p-32=c
).
minus_10
minus_10
minus_10
minus_10

::::-

2
2
6
2






(
(
p-13=t,
( p-14=t, p-13=a,
(
p-13=t,

p-12=t, p-11=a,
p-12=a,
p-10=a,
p-12=t, p-11=a, p-10=a, p-9=t
p-12=a,
p-10=a,

p-7=t
p-8=t

).
).
).
p-34=g ).

conformation :- true.

Figure 10: revised theory produced Neither-MofN.
Neither-MofN displays exibility representation allowing M-of-N interpretation original propositional logic, allow fine-grained refinement
Kbann. allow measure partial match, Kbann could weight
predictive features heavily. example, minus 35 rules, perhaps p-36=t
predictive DNA segment promoter p-34=g therefore
weighted heavily. Neither-MofN simply counts number true conditions
rule; therefore, every condition weighted equally. Kbann's fine-grained weighting may
needed domains others. may actually detrimental domains.
advanced theory revision system offer range representations.
Like Kbann, Neither-MofN limited exibility structure. refined
theory reached series small, incremental modifications initial theory
precluding fundamental restructuring. Neither-MofN therefore limited finding
best theory fundamental structure initial theory.

4. Theory-Guided Constructive Induction

first half section present guidelines theory-guided constructive induction
summarize work discussed Sections 2 3. remainder section
425

fiDonoho & Rendell

presents algorithm instantiates guidelines. evaluate algorithm
Section 5.

4.1 Guidelines

following guidelines synthesis strengths previously discussed related
work.
Miro, new features constructed using components domain
theory. new features combinations existing features, final theory
created applying standard induction algorithm training examples described
using new features. allows knowledge gleaned initial theory
without forcing final theory conform initial theory's backbone structure.
takes full advantage domain theory building high-level features
original low-level features. also takes advantage strength standard induction
| building concise theories high predictive accuracy target concept
concisely expressed using given features.
Either, constructed features modifiable various operators
act locally, adding dropping conjuncts constructed feature.
Kbann Neither-MofN, representation constructed features
need exact representation initial theory given. example,
initial theory may given set rules written propositional logic.
new feature constructed rule, need boolean feature
telling whether conditions met; example may count
many conditions rule met. allows final theory formed
expressed representation suitable representation
initial theory.
Like Grendel, complete system offer library interpreters allowing
domain theory translated range representations differing adaptability. One interpreter might emulate Miro strictly translating domain theory
boolean constructed features. Another interpreter might construct features
count number satisfied conditions corresponding component domain theory thus providing measure partial match. Still another interpreter
might construct features weighted sums satisfied conditions.
weights could refined empirically examining set training examples. Thus
appropriate amount expressive power applied given problem
without incurring unnecessary expense.

4.2 Specific Interpreter

section describes algorithm limited instantiation guidelines
described. algorithm intended demonstration distillation synthesis
principles embodied previous landmark systems. contains main module,
Tgci described Figure 12, specific interpreter, Tgci1 described Figure 11.
main module Tgci redescribes training testing examples calling Tgci1
426

fiRerepresenting Restructuring Domain Theories

applies C4.5 redescribed examples (just Miro applied candidate
elimination algorithm examples redescribing them). Tgci1 viewed
single interpreter potential Grendel-like toolbox. takes input single example
domain theory expressed AND/OR tree one shown Figure 13.
returns new vector features example measure partial match
example theory. Thus creates new features components domain theory
Miro, measures partial match, allows exibility representing
information contained initial theory Kbann Neither-MofN. One
aspect guidelines 4.1 appear algorithm Either's locally
acting operators adding dropping conditions portion theory.
following two paragraphs explain detail workings Tgci1 Tgci
respectively.
Given: example E domain theory root node R. domain
theory AND/OR/NOT tree leaves conditions
tested true false.
Return: pair P = (F; F ) F top feature measuring partial
match E whole domain theory, F vector new features measuring partial match E various parts subparts domain theory.

1. R directly testable condition, return P=(1,<>) R true E
P=(-1,<>) R false E .
2. n = number children R
3. child Rj R, call Tgci1(Rj ,E ) store respective results
Pj = (Fj ; Fj ).
4. major operator R OR, Fnew = MAX (Fj ).
Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).
P
5. major operator R AND, Fnew = ( nj=1 Fj )=n.
Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).
6. major operator R NOT, Fnew = ,1 F1 .
Return P = (Fnew ; F1).
Figure 11: Tgci1 algorithm
Tgci1 algorithm, given Figure 11, recursive. inputs example E
domain theory root node R. ultimately returns redescription E form
vector new features F . also returns value F called top feature used
intermediate calculations described below. base case occurs domain theory
single leaf node (i.e., R simple condition). case (Line 1), Tgci1 returns
top feature 1 condition true -1 condition false. new features
returned base case would simply duplicate existing features.
427

fiDonoho & Rendell

domain theory single leaf node, Tgci1 recursively calls R's children
(Line 3). child R, Rj , processed, returns vector new features Fj (which
measures partial match example j th child R various subparts).
also returns top feature Fj included Fj marked special
measures partial match example whole j th child R. n
children, result Line 3 n vectors new features, F1 Fn , n top features, F1
Fn . operator node R (Line 4), Fnew , new feature created
node, maximum Fj . Thus Fnew measures closely best R's children come
conditions met example. vector new features returned
case concatenation Fnew new features R's children. operator
node R (Line 5), Fnew average Fj . Thus Fnew measures closely
R's children group come conditions met example.
vector new features returned case concatenation Fnew new
features R's children. operator node R (Line 6), R
one child, Fnew F1 negated. Thus Fnew measures extent conditions
R's child met example.
Given: set training examples Etrain , set testing examples Etest ,
domain theory root node R.
Return: Learned concept accuracy testing examples.
1. example Ei 2 Etrain , call Tgci1(R,Ei) returns Pi =
(Fi ; Fi). Etrain,new = fFig.
2. example Ei 2 Etest, call Tgci1(R,Ei). returns Pi =
(Fi ; Fi). Etest,new = fFi g.
3. Call C4.5 training examples Etrain,new testing examples
Etest,new . Return decision tree accuracy Etest,new .
Figure 12: Tgci algorithm
Tgci1 called twice two different examples domain theory,
two vectors new features size. Furthermore, corresponding features
measure match corresponding parts domain theory. Tgci main module
Figure 12 takes advantage creating redescribed example sets input
example sets. Line 1 redescribes example training set producing new training
set. Line 2 testing set. Line 3 runs standard induction program
C4.5 redescribed example sets. returned decision tree easily interpreted
examining new features used part domain theory
correspond to.

4.3

Tgci1

Examples

example Tgci1 interpreter works, consider toy theory shown
Figure 13. Tgci1 redescribes input example constructing new feature node
428

fiRerepresenting Restructuring Domain Theories

input theory. Consider situation input example matches conditions A,
B, C E. Tgci1 evaluates children Node 6, gets values
F1 = 1, F2 = 1, F3 = ,1, F4 = 1, F5 = ,1. Since operator Node 6 AND, Fnew
average values received children, 0.20 ((1 + 1 + (,1) + 1 + (,1))=5 =
0:20). Likewise, condition G matchs F H, Fnew Node 5 value
0.33 (,1 ((1 + (,1) + (,1))=3)) two three matching conditions Node 7 give
value ,0:33, negated Node 5. Since Node 2 disjunction,
new feature measures best partial match two children value 0.33
(MAX(0.20,0.33)), on.
1
3


2
4
5


6

B C E

7

F G H

8

J K

9

L N

Figure 13: example theory form AND/OR tree might used
interpreter generate constructed features.
Figure 14 shows Tgci1 redescribes particular DNA segment using minus 35
rules promoter theory. partial DNA segment shown along four minus 35
rules new feature constructed rule (We given new features names
simplify illustration). first rule, four six nucleotides match; therefore, DNA segment feat minus35 value 0.33 ((1+1+1+1+(,1)+(,1))=6).
second rule, four five nucleotides match; therefore, feat minus35 B
value 0.60. two minus 35 rules joined disjunction original domain theory, feat minus35 all, new feature constructed
group, takes maximum value four children; therefore, feat minus35
value 0.60 feat minus35 B value 0.60, highest group. Intuitively, feat minus35 represents best partial match grouping | extent
disjunction partially satisfied. results running Tgci1 DNA
sequence set redescribed training examples. redescribed example value
feat minus35 feat minus35 D, feat minus35 all, nodes promoter domain theory. training set essentially redescribed using new feature vector
derived information contained domain theory. form, off-the-shelf
induction program applied new example set.
Anomalous situations created Tgci1 gives \good score" seemingly
bad example bad score good example. Situations also created
logically equivalent theories give different scores single example. occur
429

fiDonoho & Rendell

DNA segment fragment:
:::

p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=c, p-32=a, p-31=a, p-30=t

:::

minus 35 group rules corresponding constructed features:
minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
minus 35 :p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

feat minus35 = 0.33
feat minus35 B = 0.60
feat minus35 C = 0.33
feat minus35 = 0.20

feat minus35 = max(feat minus35 A, feat minus35 B, feat minus35 C, feat minus35 D) = 0.60

Figure 14: example Tgci1 generates constructed features portion
promoter domain theory DNA segment. Four conditions first
minus 35 rule match DNA segment; therefore, constructed feature
rule value 0.33 ((1 + 1 + 1 + 1 + (,1) + (,1))=6). Feat minus35 all,
new feature entire minus 35 group takes maximum value
children thus embodying best partial match group.
biased favor situations matched conditions desirable,
matched conditions necessarily better. Eliminating anomalies
would remove bias.
Tgci1

5. Experiments Analysis
section presents results applying theory-guided constructive induction three
domains: promoter domain (Harley et al., 1990), primate splice-junction domain (Noordewier, Shavlik, & Towell, 1992), gene identification domain (Craven & Shavlik,
1995). case Tgci1 interpreter applied domain's theory examples
order redescribe examples using new features. C4.5 (Quinlan, 1993)
applied redescribed examples.

5.1 Promoter Domain
Figure 15 shows learning curve theory-guided constructive induction promoter
domain accompanied curves Either, LabyrinthK , Kbann, Neither-MofN.
Following methodology described Towell Shavlik [1994], set 106 examples
randomly divided training set size 80 testing set size 26. learning
curve created training subsets training set size 8, 16, 24, : : : 72, 80,
using 26 examples testing. curves Either, LabyrinthK , Kbann
taken Ourston Mooney (1990), Thompson, Langley, Iba (1991), Towell
430

fiRerepresenting Restructuring Domain Theories

Shavlik (1994) respectively obtained similar methodology1 . curve
forTgci average 50 independent random data partitions given along 95%
confidence ranges. Neither-MofN program obtained Ray Mooney's group
used generating Neither-MofN curve using 50 data partitions
used Tgci2.
42.5
40

EITHER
Labyrinth-k
NEITHER-MofN
KBANN
TGCI
95% confidence NEITHER-MofN
95% confidence TGCI

37.5
35
32.5
30
27.5
% Error

25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0
0

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size Training Sample

Figure 15: Learning curves theory-guided constructive induction systems
promoter domain.
Tgci showed improvement Either LabyrinthK portions curve
also performed better Kbann Neither-MofN except smallest training sets. Confidence intervals available Either, LabyrinthK ,

1.

used testing set size 25 use conformation portion domain theory.
testing set LabyrinthK always consisted 13 promoters 13 non-promoters.
2. Baffes Mooney (1993) report slightly better learning curve Neither-MofN obtained,
communication Paul Baffes, think difference caused random selection
data partitions.
Either

431

fiDonoho & Rendell

Kbann, pairwise comparison Neither-MofN, improvement Tgci
significant 0.0005 level confidence training sets size 48 larger.

Structure Initial Promoter Theory

100%
first
conf.
rule

100%
first
minus_35
rule

100%
second
minus_35
rule

100%
third
minus_35
rule

100%
fourth
minus_35
rule

100%
first
minus_10
rule

100%
second
minus_10
rule

100%
third
minus_10
rule

100%
second
conf.
rule

100%
third
conf.
rule

100%
fourth
conf.
rule

100%
fourth
minus_10
rule

Structure Revised Promoter Theory

>20%
second
minus_35
rule

>33%
first
minus_10
rule

>33%
second
minus_10
rule

>33%
third
minus_10
rule

>33%
fourth
minus_10
rule

Figure 16: revised theory produced theory-guided constructive induction borrowed substructures initial theory, whole restricted structure.
Figure 16 compares initial promoter theory theory created Tgci. Reasons
Tgci's improvement inferred figure. Tgci extracted components original theory helpful restructured
concise theory. Neither Kbann Neither-MofN facilitates radical extraction
restructuring. seen leaf nodes, new theory also measures partial match
example components original theory. aspect similar Kbann
Neither-MofN.
Part Tgci's improvement Kbann Neither-MofN may due knowledge/bias con ict latter two systems, situation revision biases con ict
knowledge way undo knowledge's benefits. occur
whenever detailed knowledge opened revision using set examples. revision
guided examples rather examples interpreted set
432

fiRerepresenting Restructuring Domain Theories

algorithmic biases. Biases useful absence knowledge may undo good
knowledge improperly applied. Yet biases developed perfected pure induction often unquestioningly applied revision theories. biases governing
dropping conditions Neither-MofN reweighting conditions Kbann may
neutralizing promoter theory's potential. speculate conducted
experiments allowed bias-guided dropping adding conditions within Tgci.
found techniques actually reduced accuracy domain.
45
42.5
40
37.5
c4.5
backpropagation
KBANN
TGCI
95% confidence TGCI
domain theory

35
32.5
% Error

30
27.5
25
22.5
20
17.5
15
12.5
10
7.5
0

20

40

60

80

100

120

140

160

180

200

Size Training Sample

Figure 17: Learning curves Tgci systems primate splice-junction domain.

5.2 Primate Splice-junction Domain

primate splice-junction domain (Noordewier et al., 1992) involves analyzing DNA
sequence identifying boundaries introns exons. Exons parts
DNA sequence kept splicing; introns spliced out. task involves placing
433

fiDonoho & Rendell

given boundary one three classes: intron/exon boundary, exon/intron boundary, neither. imperfect domain theory available 39.0% error rate
entire set available examples.
Figure 17 shows learning curves C4.5, backpropagation, Kbann, Tgci
primate splice-junction domain. results Kbann backpropagation taken
Towell Shavlik (1994). curves plain C4.5 Tgci algorithm
created training sets size 10,20,30,...90,100,120,...200 testing set size
800. curves C4.5 Tgci average 40 independent data partitions.
comparison made Neither-MofN implementation obtained
could handle two-class concepts. training sets larger 200, Kbann, Tgci,
backpropagation performed similarly.
accuracy Tgci appears slightly worse Kbann perhaps significantly. Kbann's advantage Tgci ability assign fine-grained weightings
individual parts domain theory. Tgci's advantage Kbann ability
easily restructure information contained domain theory. speculate
Kbann's capability assign fine-grained weights outweighted somewhat rigid structuring domain theory. Theory-guided constructive induction advantage
speed Kbann C4.5, underlying learner, runs much quickly
backpropagation, Kbann's underlying learning algorithm.

5.3 Gene Identification Domain
gene identification domain (Craven & Shavlik, 1995) involves classifying given DNA
segment coding sequence (one codes protein) non-coding sequence.
domain theory available gene identification domain; therefore, created
artificial domain theory using information organisms may favor certain nucleotide
triplets others gene coding. domain theory embodies knowledge DNA
segment likely gene coding segment triplets coding-favoring triplets
triplets noncoding-favoring triplets. decision triplets codingfavoring, noncoding-favoring, favored neither, made empirically
analyzing makeup 2500 coding 2500 noncoding sequences. specific artificial domain theory used described Online Appendix 1.
Figure 18 shows learning curves C4.5 Tgci gene identification domain.
original domain theory yields 31.5% error. curves created training
example sets size 50,200,400,...2000 testing separate example set size 1000.
curves average 40 independent data partitions.
partial curve given Neither-MofN became prohibitively slow
larger training sets. promoter domain training sets smaller 100,
Tgci Neither-MofN ran comparable speeds (approximately 10 seconds Sun4
workstation). domain Tgci ran approximately 2 minutes larger training sets.
Neither-MofN took 21 times long Tgci training sets size 400, 69 times
long size 800, 144 times long size 1200. Consequently, Neither-MofN's
curve extends 1200 represents five randomly selected data partitions.
reasons, solid comparison Neither-MofN Tgci cannot made
curves, appears Tgci's accuracy slightly better. speculate Neither434

fiRerepresenting Restructuring Domain Theories

45
42.5
40

TGCI
95% confidence TGCI
C4.5
NEITHER-MofN
domain theory

37.5

% Error

35
32.5
30
27.5
25
22.5
20
0

200

400

600

800

1000 1200 1400 1600 1800 2000

Number training examples

Figure 18: Learning curves Tgci systems gene identification domain.
's slightly lower accuracy partially due fact revises theory
correctly classify training examples. result theory likely overfits
training examples. Tgci need explicitly avoid overfit handled
underlying learner.
MofN

5.4 Summary Experiments
goal paper present new technique rather understand
behavior landmark systems, distill strengths, synthesize simple
system, Tgci. evaluation algorithm shows accuracy roughly matches
exceeds predecessors. promoter domain, Tgci showed sizable improvement
many published results. splice-junction domain, Tgci narrowly falls short
Kbann's accuracy. gene identification domain, Tgci outperforms Neither-MofN.
domains Tgci greatly improves original theory alone C4.5 alone.
435

fiDonoho & Rendell

faster closest competitors. Tgci runs much 100 times faster
large datasets. strict quantitative comparison speeds Tgci
Kbann made 1) backpropagation known much slower
decision trees (Mooney, Shavlik, Towell, & Gove, 1989), 2) Kbann uses multiple hidden
layers makes training time even longer (Towell & Shavlik, 1994), 3) Towell
Shavlik (1994) point run Kbann must made multiple times
different initial random weights, whereas single run Tgci sucient.
Overall, experiments support two claims paper: First, accuracy Tgci
substantiates delineation system strengths terms exible theory representation
exible theory structure, since characterization basis algorithm's
design. Second, Tgci's combination speed accuracy suggest unnecessary computational complexity avoided synthesizing strengths landmark systems.
following section take closer look strengths theory-guided constructive
induction.
Tgci

Neither-MofN

6. Discussion Strengths
number strengths theory-guided constructive induction discussed within
context Tgci algorithm used experiments.

6.1 Flexible Representation

discussed Section 1, many domains representation appropriate
initial theory may appropriate refined theory. theory-guided constructive induction allows translation initial theory different representation,
accommodate domains. experiments paper representation
needed allowed measurement partial match domain theory. Tgci1
accomplished simply counting matching features propagating information theory appropriately. Either LabyrinthK easily afford
measure partial match therefore appropriate problems best
representation final theory initial theory. Kbann allows
finer-grained measurement partial match Neither-MofN work,
price paid computational complexity. theory-guided constructive induction framework allows variety potential tools varying degrees granularity
partial match, although one tool used experiments.

6.2 Flexible Structure

discussed Section 2.5, strength existing induction programs fashioning concise
highly predictive description concept target concept concisely
described given features. Consequently, value domain theory lies
overall structure. feature language sucient, induction program build
good overall theory structure. Instead, value domain theory lies information
contains redescribe examples using high-level features. high-level
features facilitate concise description target concept. Systems Either
Neither-MofN reach final theory series modifications initial
436

fiRerepresenting Restructuring Domain Theories

theory hope gain something keeping theory's overall structure intact. initial
theory suciently close accurate theory, method works, often clinging
structure hinders full exploitation domain theory. Theory-guided constructive
induction provides means fully exploiting information domain theory
strengths existing induction programs. Figure 16 Section 5.1 gives comparison
structure initial promoter theory structure revised theory produced
theory-guided constructive induction. Substructures borrowed, revised
theory whole restructured.

6.3 Use Standard Induction Underlying Learner

theory-guided constructive induction uses standard induction program
underlying learner, need reinvent solutions overfit avoidance, multi-class
concepts, noisy data, etc. Overfit avoidance widely studied standard induction,
many standard techniques exist. system modifies theory accommodate
set training examples must also address issue overfit training examples.
many theory revision systems existing overfit avoidance techniques cannot easily adapted,
problem must addressed scratch. Theory-guided constructive induction
take advantage full range previous work overfit avoidance standard induction.
multiple theory parts available multi-class concepts, interpreter
run multiple theory parts, resulting new feature sets combined.
primate splice-junction domain presented Section 5.2 three classes: intron/exon
boundaries, exon/intron boundaries, neither. Theories given intron/exon
exon/intron. theories used create new features, new features
concatenated together learning. Interpreters Tgci1 also trivially handle
negation domain theory.

6.4 Use Theory Fragments

Theory-guided constructive induction limited using full domain theories.
part theory available, used. demonstrate this, three experiments
run fragments promoter domain theory used. first
experiment, four minus 35 rules used. Five features constructed | one
feature rule additional feature group. Similar experiments
run minus 10 group conformation group.
Figure 19 gives learning curves three experiments along curves entire theory theory (C4.5 using original features). Although conformation
portion theory gives significant improvement C4.5, minus 35
minus 10 portions theory give significant improvements performance. Thus even
partial theories theory fragments used theory-guided constructive induction
yield sizable performance improvements.
use theory fragments explored means evaluating contribution
different parts theory. Figure 19, conformation portion theory shown
yield improvement. could signal knowledge engineer knowledge
conveyed portion theory useful learner
present form.
437

fiDonoho & Rendell

45
C4.5
conformation portion theory
minus_10 portion theory
minus_35 portion theory
whole theory

42.5
40
37.5
35
32.5
30

% Error

27.5
25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0
0

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size Training Sample

Figure 19: Learning curves theory-guided constructive induction fragments
promoter domain theory. minus 35 portion theory, minus 10
portion theory, conformation portion theory used
separately feature construction. Curves also given full theory
C4.5 alone comparison.

6.5 Use Multiple Theories

Theory-guided constructive induction use multiple competing even incompatible
domain theories. multiple theories exist, theory-guided constructive induction provides
natural means integrating way extract best theories.
Tgci1 would called input theory producing new features. Next, new
features simply pooled together induction program selects among
fashioning final theory. seen small scale promoter domain.
438

fiRerepresenting Restructuring Domain Theories

% Error

Figure 4 minus 35 rules subsume minus 35 rules. According entry
UCI Database, \the biological evidence inconclusive respect
correct specificity." handled simply using four possibilities, selection
useful knowledge left induction program.
Tgci could also used evaluate contributions competing theories
used evaluate theory fragments above. knowledge engineer could use evaluation
guide revision synthesis competing theories.
25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0

TGCI using C4.5
TGCI using LFC
95% confidence LFC

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size Training Sample

Figure 20: Theory-guided constructive induction Lfc C4.5 underlying
learning system. Theory-guided constructive induction use inductive
learner underlying learning component. Therefore, sophisticated
underlying induction programs improve accuracy.

6.6 Easy Adoption New Techniques

Since theory-guided constructive induction use standard induction method
underlying learner, improvements made standard induction, theory-guided constructive induction passively improves. demonstrate this, tests also run Lfc
(Ragavan & Rendell, 1993) underlying induction program. Lfc decision tree
learner performs example-based constructive induction looking ahead combinations features. Characteristically, Lfc improves accuracy moderate number
examples. Figure 20 shows resulting learning curve along C4.5 Tgci curve.
curves average 50 separate runs data partitions used
program. pairwise comparison improvement Lfc C4.5 significant
0.025 level confidence training sets size 72 80. sophisticated underlying
induction programs improve accuracy.
439

fiDonoho & Rendell

7. Testing Limits Tgci
purpose section explore performance theory-guided constructive
induction theory revision problems ranging easy dicult. easy problems
underlying concept embodied training testing examples matches domain
theory fairly closely; therefore, examples match domain theory fairly
closely. dicult problems underlying concept embodied examples
match domain theory well examples either. Although many
factors determine diculty individual problem, aspect important component worth exploring. experiment section intended relate ranges
diculty amount improvement produced Tgci.
Since number factors affect problem diculty chose theory revision
problems experiment variations single problem.
able hold factors constant vary closeness match domain
theory. wanted avoid totally artificial domains, chose start
promoter domain create \new" domains perverting example set.
\new" domains created perverting examples original promoter
problem either closely match promoter domain theory less closely match
promoter domain theory. positive examples altered. example, one domain
created 30% fewer matches domain theory original promoter
domain follows: feature value given example examined see matched
part theory. so, 30% probability, randomly reassigned new value
set possible values feature. end result set examples 30%
fewer matches domain theory original example set3. experiment
new domains created 10%, 30%, 60%, 90% fewer matches.
features, multiple values may match theory different disjuncts
theory specify different values single feature. example, referring back
Figure 4, feature p-12 matches two minus 10 rules value another
two rules value t. single feature might accidentally match one part
theory fact example whole closely matches another part theory.
cases these, true matches separated accidental matches examining
part theory clearly matched example whole expecting
match part theory.
New domains closely matched theory created similar manner.
example, domain created 30% fewer mismatches domain theory
original promoter domain follows: feature value given example examined
see matched corresponding part theory. not, 30% probability,
reassigned value matched theory. end result set examples
30% mismatches domain theory eliminated. experiment
new domains created 30%, 60%, 90% fewer mismatches.
Ten different example sets created level closeness domain theory:
10%, 30%, 60%, 90% fewer matches, 30%, 60%, 90% fewer mismatches. total, forty
example sets created matched original theory less closely original
3. precisely, would slightly matches 30% fewer matches features
would randomly reassigned back original matching value.

440

fiRerepresenting Restructuring Domain Theories

55
50
45
40

% Error

35

C4.5
TGCI

30
25
20
15
10
5
0
-100

-80

-60

-40

-20

0

20

40

60

80

100

Closeness theory

Figure 21: Seven altered promoter domains created, three closely matched
theory original domains four less closely matched.
100 x-axis indicates domain positive examples match
domain theory 100%. negative 100 indicates domain match
positive examples domain theory purely chance. accuracy
C4.5 Tgci plotted different levels proximity domain
theory.

example set, thirty example sets created matched original theory
closely original example set. example sets tested using leaveone-out methodology using C4.5 Tgci algorithm. results summarized
Figure 21. x-axis measure theory proximity { closeness example set
domain theory. \0" x-axis indicates change original promoter examples.
\100" x-axis means positive example exactly matches domain theory.
\-100" x-axis means match feature value positive example
441

fiDonoho & Rendell

domain theory totally chance4 . datapoint Figure 21 result averaging
accuracies ten example sets level theory proximity (except
point zero accuracy exact original promoter examples).
One notable portion Figure 21 section 0 60 x-axis. Domains
region greater trivial level mismatch domain theory
moderate mismatch. region Tgci's best performance.
domains, Tgci achieves high accuracy standard learner, C4.5, using original
feature set gives mediocre performance. second region examine -60 0
x-axis level mismatch ranges moderate extreme. region
Tgci's performance falls improvement original feature set remains high
shown Figure 22 plots improvement Tgci C4.5. final two
regions notice greater 60 less -60 x-axis. level
mismatch theory examples becomes trivially small (x-axis greater 60),
C4.5 able pick theory's patterns leading high accuracy approaches
Tgci's. level mismatch becomes extreme (x-axis less -60) theory gives
little help problem-solving resulting similarly poor accuracy methods.
summary, shown Figure 22 variants promoter problem wide range
theory proximity (centered around real promoter problem) theory-guided
constructive induction yields sizable improvement standard learners.
20
17.5

error difference

% Error

15
12.5
10
7.5
5
2.5
0
-100

-80

-60

-40

-20

0

20

40

60

80

100

Closeness theory

Figure 22: difference error C4.5 Tgci different levels proximity
example set domain theory.

4. scale 0 -100 left half graph may directly comparable scale 0 100
right half graph since equal number matches mismatches
original examples.

442

fiRerepresenting Restructuring Domain Theories

8. Conclusion
goal paper present another new system, rather
study two qualities exible representation exible structure. capabilities
intended frame reference analyzing theory-guided systems. two principles
provide guidelines purposeful design. distilled essence systems
Miro, Kbann, Neither-MofN, theory-guided constructive induction
natural synthesis strengths. experiments demonstrated even
simple application two principles effectively integrate theory knowledge
training examples. Yet much room improvement; two principles could
quantified made precise, implementations proceed
explored refined.
Quantifying representational exibility one step. Section 4 gave three degrees
exibility: one measured exact match theory, one counted number matching
conditions, one allowed weighted sum matching conditions. amount
exibility quantified, finer-grained degrees exibility explored.
accuracy assorted domains evaluated function representational
exibility.
Finer-grained structural exibility would advantageous. presented systems
make small, incremental modifications theory lacking structural exibility. Yet
theory-guided constructive induction falls extreme, perhaps allowing excessive
structural exibility. Fortunately, existing induction tools capable fashioning simple
yet highly predictive theory structures problem features suitably high-level.
Nevertheless, approaches explored take advantage structure
initial theory without unduly restricted it.
strength discussed Section 6.5 given attention. Although
promoter domain gives small example synthesizing competing theories,
explored domain entire competing, inconsistent theories available
synthesizing knowledge given multiple experts. point made Section 6.4
Tgci use theory fragments evaluate contribution different parts
theory. also explored further.
exploration bias standard induction, Utgoff (1986) refers biases ranging
weak strong incorrect correct. strong bias restricts concepts
represented weak bias thus providing guidance learning.
bias becomes stronger, may also become incorrect ruling useful concept descriptions. similar situation arises theory revision | theory representation language
inappropriately rigid may impose strong, incorrect bias revision. language
allows adaptability along many dimensions may provide weak bias. Grendellike toolbox would allow theory translated range representations
varying dimensions adaptability. Utgoff advocates starting strong, possibly incorrect bias shifting appropriately weak correct bias. Similarly, theory could
translated successively adaptable representations appropriate bias
found. implemented single tool; many open problems remain along line
research.
443

fiDonoho & Rendell

converse relationship theory revision constructive induction warrants
examination | theory revision uses data improve theory; constructive induction
use theory improve data facilitate learning. Since long-term goal machine
learning use data, inference, theory improve them, believe
consideration related methods beneficial, particularly
research area strengths lacks.
analysis landmark theory revision theory-guided learning systems led
two principles exible representation exible structure. theory-guided
constructive induction based upon high-level principles, simple yet achieves
good accuracy. principles provide guidelines future work, yet discussed above,
principles imprecise call exploration.

Acknowledgements
would like thank Geoff Towell, Kevin Thompson, Ray Mooney, Jeff Mahoney
assistance getting datapoints Kbann, LabyrinthK , Either. would
also like thank Paul Baffes making Neither program available advice
setting program's parameters. thank anonymous reviewers constructive
criticism earlier draft paper. gratefully acknowledge support
work DoD Graduate Fellowship NSF grant IRI-92-04473.

References
Baffes, P., & Mooney, R. (1993). Symbolic revision theories M-of-N rules.
Proceedings 1993 IJCAI.
Bloedorn, E., Michalski, R., & Wnek, J. (1993). Multistrategy constructive induction:
AQ17-MCI. Proceeding second international workshop multistrategy learning.
Clark, P., & Matwin, S. (1993). Using qualitative models guide inductive learning.
Proceedings 1993 International Conference Machine Learning.
Cohen, W. (1992). Compiling prior knowledge explicit bias. Proceedings
1992 International Conference Machine Learning.
Craven, M. W., & Shavlik, J. W. (1995). Investigating value good input representation. Computational Learning Theory Natural Learning Systems, 3. Forthcoming.
Drastal, G., & Raatz, S. (1989). Empirical results learning abstraction space.
Proceedings 1989 IJCAI.
Dzerisko, S., & Lavrac, N. (1991). Learning relations noisy examples: empirical
comparison LINUS FOIL. Proceedings 1991 International Conference
Machine Learning.
444

fiRerepresenting Restructuring Domain Theories

Feldman, R., Serge, A., & Koppel, M. (1991). Incremental refinement approximate
domain theories. Proceedings 1991 International Conference Machine
Learning.
Flann, N., & Dietterich, T. (1989). study explanation-based methods inductive
learning. Machine Learning, 4, 187{226.
Fu, L. M., & Buchanan, B. G. (1985). Learning intermediate concepts constructing
hierarchical knowledge base. Proceedings 1985 IJCAI.
Harley, C., Reynolds, R., & Noordewier, M. (1990). Creators original promoter dataset.
Hirsh, H., & Noordewier, M. (1994). Using background knowledge improve inductive
learning DNA sequences. Tenth IEEE Conference AI Applications San
Antonio, TX.
Matheus, C. J., & Rendell, L. A. (1989). Constructive induction decision trees.
Proceedings 1989 IJCAI.
Michalski, R. S. (1983). theory methodology inductive learning. Artificial Intelligence, 20 (2), 111{161.
Mitchell, T. (1977). Version spaces: candidate elimination approach rule learning.
Proceedings 1977 IJCAI.
Mooney, R. J. (1993). Induction unexplained: Using overly-general domain theories
aid concept learning. Machine Learning, 10 (1), 79{110.
Mooney, R. J., Shavlik, J. W., Towell, G. G., & Gove, A. (1989). experimental comparison symbolic connectionist learning algorithms. Proceedings 1989
IJCAI.
Murphy, P., & Pazzani, M. (1991). ID2-of-3: Constructive induction M-of-N concepts
discriminators decision trees. Proceedings 1991 International Conference
Machine Learning.
Noordewier, M., Shavlik, J., & Towell, G. (1992). Donors original primate splice-junction
dataset.
Ourston, D., & Mooney, R. (1990). Changing rules: comprehensive approach theory
refinement. Proceedings 1990 National Conference Artificial Intelligence.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. Machine
Learning, 5 (1), 71{99.
Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. Machine
Learning, 9 (1), 57{94.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. San Mateo, CA: Morgan
Kaufmann.
445

fiDonoho & Rendell

Ragavan, H., & Rendell, L. (1993). Lookahead feature construction learning hard concepts. Proceedings 1993 International Conference Machine Learning.
Rumelhart, D. E., Hinton, G. E., & McClelland, J. L. (1986). general framework
parallel distributed processing. Rumelhart, D. E., & McClelland, J. L. (Eds.),
Parallel Distributed Processing: Explorations Microarchitecture Cognition,
Volume I. Cambridge, MA: MIT Press.
Schlimmer, J. C. (1987). Learning representation change. Kaufmann, M. (Ed.),
Proceedings 1987 National Conference Artificial Intelligence.
Thompson, K., Langley, P., & Iba, W. (1991). Using background knowledge concept
formation. Proceedings 1991 International Conference Machine Learning.
Towell, G., & Shavlik, J. (1994). Knowledge-based artificial neural networks. Artificial
Intelligence, 70, 119{165.
Towell, G., Shavlik, J., & Noordeweir, M. (1990). Refinement approximately correct
domain theories knowledge-based neural networks. Proceedings 1990
National Conference Artificial Intelligence.
Utgoff, P. E. (1986). Shift bias inductive concept learning. Michalski, Carbonell,
& Mitchell (Eds.), Machine Learning, Vol. 2, chap. 5, pp. 107{148. San Mateo, CA:
Morgan Kaufmann.
Wogulis, J. (1991). Revising relational domain theories. Proceedings 1991 International Conference Machine Learning.

446

fiJournal Artificial Intelligence Research 2 (1994) 1-32

Submitted 4/94; published 8/94

System Induction Oblique Decision Trees
Sreerama K. Murthy
Simon Kasif
Steven Salzberg

Department Computer Science
Johns Hopkins University, Baltimore, MD 21218 USA

murthy@cs.jhu.edu
kasif@cs.jhu.edu
salzberg@cs.jhu.edu

Abstract

article describes new system induction oblique decision trees. system,
OC1, combines deterministic hill-climbing two forms randomization find good
oblique split (in form hyperplane) node decision tree. Oblique decision
tree methods tuned especially domains attributes numeric, although
adapted symbolic mixed symbolic/numeric attributes. present extensive empirical studies, using real artificial data, analyze OC1's ability
construct oblique trees smaller accurate axis-parallel counterparts. also examine benefits randomization construction oblique
decision trees.

1. Introduction
Current data collection technology provides unique challenge opportunity automated machine learning techniques. advent major scientific projects
Human Genome Project, Hubble Space Telescope, human brain mapping initiative generating enormous amounts data daily basis. streams data
require automated methods analyze, filter, classify presenting
digested form domain scientist. Decision trees particularly useful tool context perform classification sequence simple, easy-to-understand tests
whose semantics intuitively clear domain experts. Decision trees used
classification tasks since 1960s (Moret, 1982; Safavin & Landgrebe, 1991).
1980's, Breiman et al.'s book classification regression trees (CART) Quinlan's work ID3 (Quinlan, 1983, 1986) provided foundations become
large body research one central techniques experimental machine learning.
Many variants decision tree (DT) algorithms introduced last decade.
Much work concentrated decision trees node checks value
single attribute (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1986, 1993a).
Quinlan initially proposed decision trees classification domains symbolic-valued
attributes (1986), later extended numeric domains (1987). attributes
numeric, tests form xi > k, xi one attributes example
k constant. class decision trees may called axis-parallel, tests
node equivalent axis-parallel hyperplanes attribute space. example
decision tree given Figure 1, shows tree partitioning
creates 2-D attribute space.

c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiFigure 1: left side figure shows simple axis-parallel tree uses two attributes.
right side shows partitioning tree creates attribute space.
Researchers also studied decision trees test node uses boolean
combinations attributes (Pagallo, 1990; Pagallo & Haussler, 1990; Sahami, 1993)
linear combinations attributes (see Section 2). Different methods measuring
goodness decision tree nodes, well techniques pruning tree reduce overfitting
increase accuracy also explored, discussed later sections.
paper, examine decision trees test linear combination attributes
internal node. precisely, let example take form X = x1 ; x2; : : :; xd; Cj
Cj class label xi 's real-valued attributes.1 test node
form:

X
ai xi + ad+1 > 0
(1)
i=1

a1 ; : : :; ad+1 real-valued coecients. tests equivalent hyperplanes oblique orientation axes, call class decision trees oblique
decision trees. (Trees form also called \multivariate" (Brodley & Utgoff,
1994). prefer term \oblique" \multivariate" includes non-linear combinations variables, i.e., curved surfaces. trees contain linear tests.) clear
simply general form axis-parallel trees, since setting ai = 0
coecients one, test Eq. 1 becomes familiar univariate test. Note
oblique decision trees produce polygonal (polyhedral) partitionings attribute
space, axis-parallel trees produce partitionings form hyper-rectangles
parallel feature axes.
intuitively clear underlying concept defined polygonal space partitioning, preferable use oblique decision trees classification.
example, exist many domains one two oblique hyperplanes
best model use classification. domains, axis-parallel methods ap1. constraint x1 ; : : : ; xd real-valued necessarily restrict oblique decision trees
numeric domains. Several researchers studied problem converting symbolic (unordered)
domains numeric (ordered) domains vice versa; e.g., (Breiman et al., 1984; Hampson & Volper,
1986; Utgoff & Brodley, 1990; Van de Merckt, 1992, 1993). keep discussion simple, however,
assume attributes numeric values.

2

fiFigure 2: left side shows simple 2-D domain two oblique hyperplanes define
classes. right side shows approximation sort axis-parallel
decision tree would create model domain.
proximate correct model staircase-like structure, oblique tree-building
method could capture tree smaller accurate.2 Figure 2
gives illustration.
Breiman et al. first suggested method inducing oblique decision trees 1984. However, little research trees relatively recently (Utgoff
& Brodley, 1990; Heath, Kasif, & Salzberg, 1993b; Murthy, Kasif, Salzberg, & Beigel, 1993;
Brodley & Utgoff, 1994). comparison existing approaches given detail
Section 2. purpose study review strengths weaknesses existing
methods, design system combines strengths overcomes weaknesses, evaluate system empirically analytically. main contributions
conclusions study follows:

developed new, randomized algorithm inducing oblique decision trees

examples. algorithm extends original 1984 work Breiman et al.
Randomization helps significantly learning many concepts.

algorithm fully implemented oblique decision tree induction system
available Internet. code retrieved Online Appendix 1
paper (or anonymous ftp ftp://ftp.cs.jhu.edu/pub/oc1/oc1.tar.Z).

randomized hill-climbing algorithm used OC1 ecient

existing randomized oblique decision tree methods (described below). fact,
current implementation OC1 guarantees worst-case running time
O(log n) times greater worst-case time inducing axis-parallel trees (i.e.,
O(dn2 log n) vs. O(dn2)).

ability generate oblique trees often produces small trees compared
axis-parallel methods. underlying problem requires oblique split, oblique

2. Note though given oblique tree may fewer leaf nodes axis-parallel tree|which
mean \smaller"|the oblique tree may cases larger terms information content,
increased complexity tests node.

3

fiMurthy, Kasif & Salzberg

trees also accurate axis-parallel trees. Allowing tree-building system
use oblique axis-parallel splits broadens range domains
system useful.
remaining sections paper follow outline: remainder section
brie outlines general paradigm decision tree induction, discusses complexity issues involved inducing oblique decision trees. Section 2 brie reviews
existing techniques oblique DT induction, outlines limitations approach,
introduces OC1 system. Section 3 describes OC1 system detail. Section 4
describes experiments (1) compare performance OC1 several
axis-parallel oblique decision tree induction methods range real-world datasets
(2) demonstrate empirically OC1 significantly benefits randomization
methods. Section 5, conclude discussion open problems directions
research.

1.1 Top-Down Induction Decision Trees

Algorithms inducing decision trees follow approach described Quinlan top-down
induction decision trees (1986). also called greedy divide-and-conquer
method. basic outline follows:
1. Begin set examples called training set, . examples belong
one class, halt.
2. Consider tests divide two subsets. Score test according
well splits examples.
3. Choose (\greedily") test scores highest.
4. Divide examples subsets run procedure recursively subset.
Quinlan's original model considered attributes symbolic values; model,
test node splits attribute values. Thus test attribute
three values three child nodes, one corresponding value.
algorithm considers possible tests chooses one optimizes pre-defined
goodness measure. (One could also split symbolic values two subsets values,
gives many choices split examples.) explain next, oblique
decision tree methods cannot consider tests due complexity considerations.

1.2 Complexity Induction Oblique Decision Trees

One reason relatively papers problem inducing oblique decision trees
increased computational complexity problem compared axis-parallel
case. two important issues must addressed. context top-down
decision tree algorithms, must address complexity finding optimal separating
hyperplanes (decision surfaces) given node decision tree. optimal hyperplane
minimize impurity measure used; e.g., impurity might measured total
number examples mis-classified. second issue lower bound complexity
finding optimal (e.g., smallest size) trees.
4

fiFigure 3: n points dimensions
(n d), n distinct axis-parallel splits,
,
2d nd distinct d-dimensional oblique splits. shows distinct
oblique axis-parallel splits two specific points 2-D.
Let us first consider issue complexity selecting optimal oblique hyperplane single node tree. domain
n training instances, described using
,
real-valued attributes, 2d nd distinct d-dimensional oblique splits; i.e.,
hyperplanes3 divide training instances uniquely two nonoverlapping subsets.
upper bound derives observation every subset size n points
define d-dimensional hyperplane, hyperplane rotated slightly
2d directions divide set points possible ways. Figure 3 illustrates
upper limits two points two dimensions. axis-parallel splits, n
distinct possibilities, axis-parallel methods C4.5 (Quinlan, 1993a) CART
(Breiman et al., 1984) exhaustively search best split node. problem
searching best oblique split therefore much dicult searching
best axis-parallel split. fact, problem NP-hard.
precisely, Heath (1992) proved following problem NP-hard: given
set labelled examples, find hyperplane minimizes number misclassified
examples hyperplane. result implies method
finding optimal oblique split likely exponential cost
(assuming P 6= NP ).
,
Intuitively, problem impractical enumerate 2d nd distinct hyperplanes
choose best, done axis-parallel decision trees. However, non-exhaustive
deterministic algorithm searching hyperplanes prone getting stuck
local minima.
3. Throughout paper, use terms \split" \hyperplane" interchangeably refer test
node decision tree. first usage standard (Moret, 1982), refers fact
test splits data two partitions. second usage refers geometric form test.

5

fiMurthy, Kasif & Salzberg

hand, possible define impurity measures problem
finding optimal hyperplanes solved polynomial time. example, one
minimizes sum distances mis-classified examples, optimal solution
found using linear programming methods (if distance measured along one dimension
only). However, classifiers usually judged many points classify correctly,
regardless close decision boundary point may lie. Thus standard
measures computing impurity base calculation discrete number examples
category either side hyperplane. Section 3.3 discusses several commonly
used impurity measures.
let us address second issue, complexity building small tree.
easy show problem inducing smallest axis-parallel decision tree
NP-hard. observation follows directly work Hyafil Rivest (1976). Note
one generate smallest axis-parallel tree consistent training
set polynomial time number attributes constant. done
using dynamic programming branch bound techniques (see Moret (1982) several
pointers). tree uses oblique splits, clear, even fixed number
attributes, generate optimal (e.g., smallest) decision tree polynomial time.
suggests complexity constructing good oblique trees greater
axis-parallel trees.
also easy see problem constructing optimal (e.g., smallest) oblique
decision tree NP-hard. conclusion follows work Blum Rivest (1988).
result implies dimensions (i.e., attributes) problem producing
3-node oblique decision tree consistent training set NP-complete.
specifically, show following decision problem NP-complete: given training
set n examples Boolean attributes, exist 3-node neural network
consistent ? easy show following question NP-complete:
given training set , exist 3-leaf-node oblique decision tree consistent
T?
result complexity considerations, took pragmatic approach trying
generate small trees, looking smallest tree. greedy approach used
OC1 virtually decision tree algorithms implicitly tries generate small trees.
addition, easy construct example problems optimal split node
lead best tree; thus philosophy embodied OC1 find locally
good splits, spend excessive computational effort improving quality
splits.

2. Previous Work Oblique Decision Tree Induction
describing OC1 algorithm, brie discuss existing oblique DT
induction methods, including CART linear combinations, Linear Machine Decision
Trees, Simulated Annealing Decision Trees. also methods induce
tree-like classifiers linear discriminants node, notably methods using
linear programming (Mangasarian, Setiono, & Wolberg, 1990; Bennett & Mangasarian,
1992, 1994a, 1994b). Though methods find optimal linear discriminants
specific goodness measures, size linear program grows fast number
6

fiInduction Oblique Decision Trees

induce split node decision tree:
Normalize values attributes.
L=0
(TRUE)
L = L+1

Let current split sL v c, v = Pdi=1 ai xi.
= 1; : : :;
= -0.25,0,0.25
Search maximizes goodness split v , (ai + ) c.
Let , settings result highest goodness 3 searches.
ai = ai , , c = c , .
Perturb c maximize goodness sL , keeping a1 ; : : :; ad constant.
jgoodness(sL) - goodness(sL,1)j exit loop.
Eliminate irrelevant attributes fa1; : : :; ad g using backward elimination.
Convert sL split un-normalized attributes.
Return better sL best axis-parallel split split .
Figure 4: procedure used CART linear combinations (CART-LC) node
decision tree.
instances number attributes. also less closely related work
algorithms train artificial neural networks build decision tree-like classifiers (Brent,
1991; Cios & Liu, 1992; Herman & Yeung, 1992).
first oblique decision tree algorithm proposed CART linear combinations (Breiman et al., 1984, chapter 5). algorithm, referred henceforth CART-LC,
important basis OC1. Figure 4 summarizes (using Breiman et al.'s notation)
CART-LC algorithm node decision tree. core idea CARTLC algorithm finds value maximizes goodness split.
idea also used OC1, explained detail Section 3.1.
describing CART-LC, Breiman et al. point still much room
development algorithm. OC1 represents extension CART-LC
includes significant additions. addresses following limitations CART-LC:

CART-LC fully deterministic. built-in mechanism escaping local

minima, although minima may common domains. Figure 5
shows simple example CART-LC gets stuck.

CART-LC produces single tree given data set.
CART-LC sometimes makes adjustments increase impurity split.
feature probably included allow escape local minima.

upper bound time spent node decision tree. halts
perturbation changes impurity , impurity may
increase decrease, algorithm spend arbitrarily long time node.
7

fiMurthy, Kasif & Salzberg

1

OC1

2

1

Initial Loc.

1

2

CART-LC

1

2

2

Figure 5: deterministic perturbation algorithm CART-LC fails find correct
split data, even starts location best axis-parallel
split. OC1 finds correct split using one random jump.
Another oblique decision tree algorithm, one uses different approach
CART-LC, Linear Machine Decision Trees (LMDT) system (Utgoff & Brodley, 1991;
Brodley & Utgoff, 1992), successor Perceptron Tree method (Utgoff, 1989;
Utgoff & Brodley, 1990). internal node LMDT tree Linear Machine (Nilsson,
1990). training algorithm presents examples repeatedly node linear
machine converges. convergence cannot guaranteed, LMDT uses heuristics
determine node stabilized. make training stable even set
training instances linearly separable, \thermal training" method (Frean, 1990)
used, similar simulated annealing.
third system creates oblique trees Simulated Annealing Decision Trees
(SADT) (Heath et al., 1993b) which, like OC1, uses randomization. SADT uses simulated
annealing (Kirkpatrick, Gelatt, & Vecci, 1983) find good values coecients
hyperplane node tree. SADT first places hyperplane canonical
location, iteratively perturbs coecients small random amounts. Initially, temperature parameter high, SADT accepts almost perturbation
hyperplane, regardless changes goodness score. However, system
\cools down," changes improve goodness split likely accepted.
Though SADT's use randomization allows effectively avoid local minima,
compromises eciency. runs much slower either CART-LC, LMDT OC1,
sometimes considering tens thousands hyperplanes single node finishes
annealing.
experiments Section 4.3 include results showing methods
perform three artificial domains.
next describe way combine strengths methods mentioned,
avoiding problems. algorithm, OC1, uses deterministic hill climbing
time, ensuring computational eciency. addition, uses two kinds
randomization avoid local minima. limiting number random choices,
algorithm guaranteed spend polynomial time node tree. addition,
randomization produced several benefits: example, means algorithm
8

fiInduction Oblique Decision Trees

find split set examples :
Find best axis-parallel split . Let impurity split.
Repeat R times:
Choose random hyperplane H .
(For first iteration, initialize H best axis-parallel split.)
Step 1: impurity measure improve, do:
Perturb coecients H sequence.
Step 2: Repeat J times:
Choose random direction attempt perturb H direction.
reduces impurity H , go Step 1.
Let I1 = impurity H . I1 < , set = I1 .
Output split corresponding .
Figure 6: Overview OC1 algorithm single node decision tree.
produce many different trees data set. offers possibility new
family classifiers: k-decision-tree algorithms, example classified
majority vote k trees. Heath et al. (1993a) shown k-decision tree methods
(which call k-DT) consistently outperform single tree methods classification
accuracy main criterion. Finally, experiments indicate OC1 eciently finds
small, accurate decision trees many different types classification problems.

3. Oblique Classifier 1 (OC1)

section discuss details oblique decision tree induction system OC1.
part description, include:
method finding coecients hyperplane tree node,
methods computing impurity goodness hyperplane,
tree pruning strategy,
methods coping missing irrelevant attributes.
Section 3.1 focuses complicated algorithmic details; i.e. question
find hyperplane splits given set instances two reasonably \pure" nonoverlapping subsets. randomized perturbation algorithm main novel contribution
OC1. Figure 6 summarizes basic OC1 algorithm, used node decision
tree. figure explained following sections.

3.1 Perturbation algorithm

OC1 imposes restrictions orientation hyperplanes. However, order
least powerful standard DT methods, first finds best axis-parallel (univariate)
split node looking oblique split. OC1 uses oblique split
improves best axis-parallel split.4
4. pointed (Breiman et al., 1984, Chapter 5), make sense use oblique split
number examples node n less almost equal number features d,

9

fiMurthy, Kasif & Salzberg

search strategy space possible hyperplanes defined procedure
perturbs current hyperplane H new location. exponential
number distinct ways partition examples hyperplane, procedure
simply enumerates unreasonably costly. two main alternatives
considered past simulated annealing, used SADT system (Heath
et al., 1993b), deterministic heuristic search, CART-LC (Breiman et al., 1984).
OC1 combines two ideas, using heuristic search finds local minimum,
using non-deterministic search step get local minimum. (The nondeterministic step OC1 simulated annealing, however.)
start explaining perturb hyperplane split training set
node decision tree. Let n number examples , number
attributes (or dimensions) example, k number categories.
write Tj = (xj 1 ; xj 2; : : :; xjd; Cj ) j th example training set , xji
value attribute Cj category label. defined P
Eq. 1, equation
current hyperplane H node decision tree written di=1
(a x )+ad+1 = 0.
Pd
substitute point (an example) Tj equation H , get i=1 (aixji )+ ad+1 = Vj ,
sign Vj tells us whether point Tj hyperplane H ;
i.e., Vj > 0, Tj H . H splits training set perfectly, points
belonging category sign Vj . i.e., sign(Vi) = sign(Vj ) iff
category(Ti) = category(Tj ).
OC1 adjusts coecients H individually, finding locally optimal value one
coecient time. key idea introduced Breiman et al. works follows.
Treat coecient variable, treat coecients constants.
Vj viewed function . particular, condition Tj H
equivalent
Vj > 0
= Uj
> xxjm , Vj def
jm

(2)

assuming xjm > 0, ensure normalization. Using definition Uj ,
point Tj H > Uj , otherwise. plugging points
equation, obtain n constraints value .
problem find value satisfies many constraints
possible. (If constraints satisfied, perfect split.) problem
easy solve optimally: simply sort values Uj , consider setting
midpoint pair different values. illustrated Figure 7. figure,
categories indicated font size; larger Ui 's belong one category,
smaller another. distinct placement coecient , OC1 computes
impurity resulting split; e.g., location U6 U7 illustrated here, two
examples left one example right would misclassified (see Section 3.3.1
different ways computing impurity). figure illustrates, problem simply
find best one-dimensional split U s, requires considering n , 1 values
. value a0m obtained solving one-dimensional problem considered
data underfits concept. default, OC1 uses axis-parallel splits tree nodes n < 2d.
user vary threshold.

10

fiFigure 7: Finding optimal value single coecient . Large U's correspond
examples one category small u's another.

Perturb(H,m)
j = 1; : : :; n
Compute Uj (Eq. 2)
Sort U1; : : :; Un non-decreasing order.
a0m = best univariate split sorted Uj s.
H1 = result substituting a0m H .
(impurity(H1 ) < impurity(H))
f = a0m ; Pmove = Pstag g
Else (impurity(H) = impurity(H1 ))
f = a0m probability Pmove
Pmove = Pmove , 0:1 Pstag g
Figure 8: Perturbation algorithm single coecient .

replacement . Let H1 hyperplane obtained \perturbing" a0m .
H better (lower) impurity H1, H1 discarded. H1 lower impurity, H1
becomes new location hyperplane. H H1 identical impurities,
H1 replaces H probability Pstag .5 Figure 8 contains pseudocode perturbation
procedure.
method locally improving coecient hyperplane, need
decide + 1 coecients pick perturbation. experimented
three different methods choosing coecient adjust, namely, sequential, best
first random.
Seq: Repeat none coecient values modified loop:
= 1 d, Perturb(H; i)
Best: Repeat coecient remains unmodified:
= coecient perturbed, results
maximum improvement impurity measure.
Perturb(H; m)
R-50: Repeat fixed number times (50 experiments):
= random integer 1 + 1
Perturb(H; m)
5. parameter Pstag , denoting \stagnation probability", probability hyperplane perturbed
location change impurity measure. prevent impurity remaining
stagnant long time, Pstag decreases constant amount time OC1 makes \stagnant"
perturbation; thus constant number perturbations occur node. constant
set user. Pstag reset 1 every time global impurity measure improved.

11

fiMurthy, Kasif & Salzberg

previous experiments (Murthy et al., 1993) indicated order perturbation
coecients affect classification accuracy much parameters,
especially randomization parameters (see below). Since none orders uniformly better other, used sequential (Seq) perturbation experiments
reported Section 4.

3.2 Randomization

perturbation algorithm halts split reaches local minimum impurity
measure. OC1's search space, local minimum occurs perturbation
single coecient current hyperplane decrease impurity measure. (Of course,
local minimum may also global minimum.) implemented two ways
attempting escape local minima: perturbing hyperplane random vector,
re-starting perturbation algorithm different random initial hyperplane.
technique perturbing hyperplane random vector works follows.
system reaches local minimum, chooses random vector add
coecients current hyperplane. computes optimal amount
hyperplane beP perturbed along random direction. precise,
hyperplane H = di=1 ai xi + ad+1 cannot improved deterministic perturbation,
OC1 repeats following loop J times (where J user-specified parameter, set 5
default).

Choose random vector R = (r1; r2; : : :; rd+1).
Let ff amount
want perturb H direction R.
Pd
words, let H1 = i=1 (ai + ffri )xi + (ad+1 + ffrd+1 ).
Find optimal value ff.
hyperplane H1 thus obtained decreases overall impurity, replace H H1,
exit loop begin deterministic perturbation algorithm individual
coecients.

Note treat ff variable equation H1 . Therefore
n examples , plugged equation H1, imposes constraint value
ff. OC1 therefore use coecient perturbation method (see Section 3.1) compute
best value ff. J random jumps fail improve impurity, OC1 halts uses
H split current tree node.
intuitive way understanding random jump look atPthe dual space
algorithm actually searching. Note equation H = di=1 ai xi + ad+1 defines
space axes coecients ai rather attributes xi . Every point
space defines distinct hyperplane original formulation. deterministic
algorithm used OC1 picks hyperplane adjusts coecients one time. Thus
dual space, OC1 chooses point perturbs moving parallel axes.
random vector R represents random direction space. finding best value
ff, OC1 finds best distance adjust hyperplane direction R.
12

fiInduction Oblique Decision Trees

Note additional perturbation random direction significantly increase time complexity algorithm (see Appendix A). found experiments
even single random jump, used local minimum, proves helpful.
Classification accuracy improved every one data sets perturbations
made. See Section 4.3 examples.
second technique avoiding local minima variation idea performing
multiple local searches. technique multiple local searches natural extension
local search, widely mentioned optimization literature (see Roth
(1970) early example). steps perturbation algorithm
deterministic, initial hyperplane largely determines local minimum
encountered first. Perturbing single initial hyperplane thus unlikely lead best
split given data set. cases random perturbation method fails escape
local minima, may helpful simply start afresh new initial hyperplane.
use word restart denote one run perturbation algorithms, one node
decision tree, using one random initial hyperplane.6 is, restart cycles
perturbs coecients one time tries perturb hyperplane
random direction algorithm reaches local minimum. last perturbation
reduces impurity, algorithm goes back perturbing coecients one time.
restart ends neither deterministic local search random jump find
better split. One optional parameters OC1 specifies many restarts use.
one restart used, best hyperplane found thus far always saved.
experiments, classification accuracies increased one restart.
Accuracy tended increase point level (after 20{50 restarts,
depending domain). Overall, use multiple initial hyperplanes substantially
improved quality decision trees found (see Section 4.3 examples).
carefully combining hill-climbing randomization, OC1 ensures worst case time
O(dn2 log n) inducing decision tree. See Appendix derivation upper
bound.

Best Axis-Parallel Split. clear axis-parallel splits suitable

data distributions oblique splits. take account distributions, OC1 computes best axis-parallel split oblique split node, picks better
two.7 Calculating best axis-parallel split takes additional O(dn log n) time,
increase asymptotic time complexity OC1. simple variant
OC1 system, user opt \switch off" oblique perturbations, thus building
axis-parallel tree training data. Section 4.2 empirically demonstrates
axis-parallel variant OC1 compares favorably existing axis-parallel algorithms.
6. first run algorithm node always begins location best axis-parallel
hyperplane; subsequent restarts begin random locations.
7. Sometimes simple axis-parallel split preferable oblique split, even oblique split slightly
lower impurity. user specify bias input parameter OC1.

13

fiMurthy, Kasif & Salzberg

3.3 Details
3.3.1 Impurity Measures

OC1 attempts divide d-dimensional attribute space homogeneous regions; i.e.,
regions contain examples one category. goal adding new nodes
tree split sample space minimize \impurity" training
set. algorithms measure \goodness" instead impurity, difference
goodness values maximized impurity minimized. Many different
measures impurity studied (Breiman et al., 1984; Quinlan, 1986; Mingers,
1989b; Buntine & Niblett, 1992; Fayyad & Irani, 1992; Heath et al., 1993b).
OC1 system designed work large class impurity measures. Stated
simply, impurity measure uses counts examples belonging every category
sides split, OC1 use it. (See Murthy Salzberg (1994) ways
mapping kinds impurity measures class impurity measures.) user
plug impurity measure fits description. OC1 implementation includes
six impurity measures, namely:
1.
2.
3.
4.
5.
6.

Information Gain
Gini Index
Twoing Rule
Max Minority
Sum Minority
Sum Variances

Though six measures defined elsewhere literature,
cases made slight modifications defined precisely Appendix B.
experiments indicated that, average, Information Gain, Gini Index Twoing Rule
perform better three measures axis-parallel oblique trees.
Twoing Rule current default impurity measure OC1, used
experiments reported Section 4. are, however, artificial data sets
Sum Minority and/or Max Minority perform much better rest measures.
instance, Sum Minority easily induces exact tree POL data set described
Section 4.3.1, methods diculty finding best tree.

Twoing Rule. Twoing Rule first proposed Breiman et al. (1984). value

computed defined as:

k
X

TwoingValue = (jTLj=n) (jTRj=n) (

i=1

jLi=jTLj , Ri=jTRjj)2

jTLj (jTRj) number examples left (right) split node , n
number examples node , Li (Ri ) number examples category
left (right) split. TwoingValue actually goodness measure rather
impurity measure. Therefore OC1 attempts minimize reciprocal value.
remaining five impurity measures implemented OC1 defined Appendix B.
14

fiInduction Oblique Decision Trees

3.3.2 Pruning

Virtually decision tree induction systems prune trees create order avoid
overfitting data. Many studies found judicious pruning results smaller
accurate classifiers, decision trees well types machine learning
systems (Quinlan, 1987; Niblett, 1986; Cestnik, Kononenko, & Bratko, 1987; Kodratoff
& Manago, 1987; Cohen, 1993; Hassibi & Stork, 1993; Wolpert, 1992; Schaffer, 1993).
OC1 system implemented existing pruning method, note tree
pruning method work fine within OC1. Based experimental evaluations
Mingers (1989a) work cited above, chose Breiman et al.'s Cost Complexity
(CC) pruning (1984) default pruning method OC1. method, also
called Error Complexity Weakest Link pruning, requires separate pruning set.
pruning set randomly chosen subset training set, approximated
using cross validation. OC1 randomly chooses 10% (the default value) training data
use pruning. experiments reported below, used default value.
Brie y, idea behind CC pruning create set trees decreasing size
original, complete tree. trees used classify pruning set, accuracy
estimated that. CC pruning chooses smallest tree whose accuracy within k
standard errors squared best accuracy obtained. 0-SE rule (k = 0) used,
tree highest accuracy pruning set selected. k > 0, smaller tree size
preferred higher accuracy. details Cost Complexity pruning, see Breiman et
al. (1984) Mingers (1989a).
3.3.3 Irrelevant attributes

Irrelevant attributes pose significant problem machine learning methods (Breiman
et al., 1984; Aha, 1990; Almuallin & Dietterich, 1991; Kira & Rendell, 1992; Salzberg, 1992;
Cardie, 1993; Schlimmer, 1993; Langley & Sage, 1993; Brodley & Utgoff, 1994). Decision
tree algorithms, even axis-parallel ones, confused many irrelevant attributes.
oblique decision trees learn coecients attribute DT node, one
might hope values chosen coecient would ect relative importance
corresponding attributes. Clearly, though, process searching good coecient
values much ecient fewer attributes; search space much
smaller. reason, oblique DT induction methods benefit substantially using
feature selection method (an algorithm selects subset original attribute set)
conjunction coecient learning algorithm (Breiman et al., 1984; Brodley & Utgoff,
1994).
Currently, OC1 built-in mechanism select relevant attributes. However, easy include several standard methods (e.g., stepwise forward selection
stepwise backward selection) even ad hoc method select features running
tree-building process. example, separate experiments data Hubble
Space Telescope (Salzberg, Chandar, Ford, Murthy, & White, 1994), used feature selection methods preprocessing step OC1, reduced number attributes 20
2. resulting decision trees simpler accurate. Work currently
underway incorporate ecient feature selection technique OC1 system.
15

fiMurthy, Kasif & Salzberg

Regarding missing values, example missing value attribute, OC1 uses
mean value attribute. One course use techniques handling
missing values, considered study.

4. Experiments

section, present two sets experiments support following two claims.
1. OC1 compares favorably variety real-world domains several existing
axis-parallel oblique decision tree induction methods.
2. Randomization, form multiple local searches random jumps, improves quality decision trees produced OC1.
experimental method used experiments described Section 4.1. Sections 4.2 4.3 describe experiments corresponding two claims. experimental section begins description data sets, presents experimental
results discussion.

4.1 Experimental Method

used five-fold cross validation (CV) experiments estimate classification
accuracy. k-fold CV experiment consists following steps.
1. Randomly divide data k equal-sized disjoint partitions.
2. partition, build decision tree using data outside partition, test
tree data partition.
3. Sum number correct classifications k trees divide total number
instances compute classification accuracy. Report accuracy
average size k trees.
entry Tables 1 2 result ten 5-fold CV experiments; i.e., result tests
used 50 decision trees. ten 5-fold cross validations used different random
partitioning data. entry tables reports mean standard deviation
classification accuracy, followed mean standard deviation decision
tree size (measured number leaf nodes). Good results high values
accuracy, low values tree size, small standard deviations.
addition OC1, also included experiments axis-parallel version OC1,
considers axis-parallel hyperplanes. call version, described Section 3.2,
OC1-AP. experiments, OC1 OC1-AP used Twoing Rule (Section
3.3.1) measure impurity. parameters OC1 took default values unless stated
otherwise. (Defaults include following: number restarts node: 20. Number
random jumps attempted local minimum: 5. Order coecient perturbation:
Sequential. Pruning method: Cost Complexity 0-SE rule, using 10% training
set exclusively pruning.)
comparison, used oblique version CART algorithm, CART-LC.
implemented version CART-LC, following description Breiman et
al. (1984, Chapter 5); however, may differences version
16

fiInduction Oblique Decision Trees

versions system (note CART-LC freely available). implementation
CART-LC measured impurity Twoing Rule used 0-SE Cost Complexity
pruning separate test set, OC1 does. include feature selection
methods CART-LC OC1, implement normalization.
CART coecient perturbation algorithm may alternate indefinitely two locations
hyperplane (see Section 2), imposed arbitrary limit 100 perturbations
forcing perturbation algorithm halt.
also included axis-parallel CART C4.5 comparisons. used implementations algorithms IND 2.1 package (Buntine, 1992). default
cart0 c4.5 \styles" defined package used, without altering parameter
settings. cart0 style uses Twoing Rule 0-SE cost complexity pruning
10-fold cross validation. pruning method, impurity measure defaults
c4.5 style described Quinlan (1993a).

4.2 OC1 vs. Decision Tree Induction Methods

Table 1 compares performance OC1 three well-known decision tree induction
methods plus OC1-AP six different real-world data sets. next section
consider artificial data, concept definition precisely characterized.
4.2.1 Description Data Sets

Star/Galaxy Discrimination. Two data sets came large set astronom-

ical images collected Odewahn et al. (Odewahn, Stockwell, Pennington, Humphreys, &
Zumach, 1992). study, used images train artificial neural networks
running perceptron back propagation algorithms. goal classify example either \star" \galaxy." image characterized 14 real-valued attributes,
attributes measurements defined astronomers likely relevant
task. objects image divided Odewahn et al. \bright" \dim"
data sets based image intensity values, dim images inherently
dicult classify. (Note \bright" objects bright relation others
data set. actuality extremely faint, visible powerful
telescopes.) bright set contains 2462 objects dim set contains 4192 objects.
addition results reported Table 1, following results appeared
Star/Galaxy data. Odewahn et al. (1992) reported accuracy 99.8% accuracy
bright objects, 92.0% dim ones, although noted study
used single training test set partition. Heath (1992) reported 99.0% accuracy
bright objects using SADT, average tree size 7.03 leaves. study also used
single training test set. Salzberg (1992) reported accuracies 98.8% bright
objects, 95.1% dim objects, using 1-Nearest Neighbor (1-NN) coupled
feature selection method reduces number features.
Breast Cancer Diagnosis. Mangasarian Bennett compiled data problem diagnosing breast cancer test several new classification methods (Mangasarian
et al., 1990; Bennett & Mangasarian, 1992, 1994a). data represents set patients
breast cancer, patient characterized nine numeric attributes plus
diagnosis tumor benign malignant. data set currently 683 entries
17

fiMurthy, Kasif & Salzberg

Bright S/G
98.90.2
4.31.0
CART-LC
98.80.2
3.91.3
OC1-AP
98.10.2
6.92.4
CART-AP
98.50.5
13.95.7
C4.5
98.50.5
14.32.2
Algorithm
OC1

Dim S/G
95.00.3
13.08.7
92.80.5
24.28.7
94.00.2
29.38.8
94.20.7
30.410
93.30.8
77.97.4

Cancer
96.20.3
2.80.9
95.30.6
3.50.9
94.50.5
6.41.7
95.01.6
11.57.2
95.32.0
9.82.2

Iris
94.73.1
3.10.2
93.52.9
3.20.3
92.72.4
3.20.3
93.83.7
4.31.6
95.13.2
4.60.8

Housing
82.40.8
6.93.2
81.41.2
5.83.2
81.81.0
8.64.5
82.13.5
15.110
83.23.1
28.23.3

Diabetes
74.41.0
5.43.8
73.71.2
8.05.2
73.81.0
11.47.5
73.93.4
11.59.1
71.43.3
56.37.9

Table 1: Comparison OC1 decision tree induction methods six different
data sets. first line method gives accuracies, second line gives
average tree sizes. highest accuracy domain appears boldface.
available UC Irvine machine learning repository (Murphy & Aha, 1994).
Heath et al. (1993b) reported 94.9% accuracy subset data set (it
470 instances), average decision tree size 4.6 nodes, using SADT. Salzberg
(1991) reported 96.0% accuracy using 1-NN (smaller) data set. Herman
Yeung (1992) reported 99.0% accuracy using piece-wise linear classification, using
somewhat smaller data set.

Classifying Irises. Fisher's famous iris data, extensively studied

statistics machine learning literature. data consists 150 examples,
example described four numeric attributes. 50 examples
three different types iris ower. Weiss Kapouleas (1989) obtained accuracies 96.7%
96.0% data back propagation 1-NN, respectively.

Housing Costs Boston. data set, also available part UCI ML repos-

itory, describes housing values suburbs Boston function 12 continuous
attributes 1 binary attribute (Harrison & Rubinfeld, 1978). category variable (median value owner-occupied homes) actually continuous, discretized
category = 1 value < $21000, 2 otherwise. uses data, see (Belsley,
1980; Quinlan, 1993b).

Diabetes diagnosis. data catalogs presence absence diabetes among Pima

Indian females, 21 years older, function eight numeric-valued attributes.
original source data National Institute Diabetes Digestive Kidney
Diseases, available UCI repository. Smith et al. (1988) reported 76%
accuracy data using ADAP learning algorithm, using different experimental
method used here.
18

fiInduction Oblique Decision Trees

4.2.2 Discussion

table shows that, six data sets considered here, OC1 consistently finds better
trees original oblique CART method. accuracy greater six domains,
although difference significant (more 2 standard deviations) dim
star/galaxy problem. average tree sizes roughly equal five six domains,
dim stars galaxies, OC1 found considerably smaller trees. differences
analyzed quantified using artificial data, following section.
five decision tree induction methods, OC1 highest accuracy four
six domains: bright stars, dim stars, cancer diagnosis, diabetes diagnosis.
remaining two domains, OC1 second highest accuracy case. surprisingly,
oblique methods (OC1 CART-LC) generally find much smaller trees axisparallel methods. difference quite striking domains|note, example,
OC1 produced tree 13 nodes average dim star/galaxy problem,
C4.5 produced tree 78 nodes, 6 times larger. course, domains
axis-parallel tree appropriate representation, axis-parallel methods compare
well oblique methods terms tree size. fact, Iris data, methods
found similar-sized trees.

4.3 Randomization Helps OC1

second set experiments, examine closely effect introducing randomized steps algorithm finding oblique splits. experiments demonstrate
OC1's ability produce accurate tree set training data clearly enhanced
two kinds randomization uses. precisely, use three artificial data sets
(for underlying concept known experimenters) show OC1's performance improves substantially deterministic hill climbing augmented
three ways:

multiple restarts random initial locations,
perturbations random directions local minima,
randomization steps.
order find clear differences algorithms, one needs know concept
underlying data indeed dicult learn. simple concepts (say, two linearly
separable classes 2-D), many different learning algorithms produce accurate
classifiers, therefore advantages randomization may detectable.
known many commonly-used data sets UCI repository easy
learn simple representations (Holte, 1993); therefore data sets may
ideal purposes. Thus created number artificial data sets present different
problems learning, know \correct" concept definition. allows
us quantify precisely parameters algorithm affect performance.
second purpose experiment compare OC1's search strategy
two existing oblique decision tree induction systems { LMDT (Brodley & Utgoff, 1992)
SADT (Heath et al., 1993b). show quality trees induced OC1
good as, better than, trees induced existing systems three
19

fiMurthy, Kasif & Salzberg

artificial domains. also show OC1 achieves good balance amount
effort expended search quality tree induced.
LMDT SADT used information gain experiment. However,
change OC1's default measure (the Twoing Rule) observed, experiments
reported here, OC1 information gain produce significantly different
results. maximum number successive, unproductive perturbations allowed
node set 10000 SADT. parameters, used default settings provided
systems.
4.3.1 Description Artificial Data

LS10 LS10 data set 2000 instances divided two categories. instance

described ten attributes x1 ,: : : ,x10, whose values uniformly distributed range
[0,1]. data linearly separable 10-D hyperplane (thus name LS10) defined
equation x1 + x2 + x3 + x4 + x5 < x6 + x7 + x8 + x9 + x10. instances
generated randomly labelled according side hyperplane fell on.
oblique DT induction methods intuitively prefer linear separator one
exists, interesting compare various search techniques data set
know separator exists. task relatively simple lower dimensions, chose
10-dimensional data make dicult.

POL data set shown Figure 9. 2000 instances two dimensions,

divided two categories. underlying concept set four parallel oblique lines
(thus name POL), dividing instances five homogeneous regions. concept
dicult learn single linear separator, minimal-size tree still quite
small.

RCB RCB stands \rotated checker board"; data set subject

experiments hard classification problems decision trees (Murthy & Salzberg,
1994). data set, shown Figure 9, 2000 instances 2-D, belonging one
eight categories. concept dicult learn axis-parallel method, obvious
reasons. also quite dicult oblique methods, several reasons. biggest
problem \correct" root node, shown figure, separate
class itself. impurity measures (such Sum Minority) fail miserably
problem, although others (e.g., Twoing Rule) work much better. Another problem
deterministic coecient perturbation algorithm get stuck local minima
many places data set.
Table 2 summarizes results experiment three smaller tables, one
data set. smaller table, compare four variants OC1 LMDT SADT.
different results OC1 obtained varying number restarts
number random jumps. random jumps used, twenty random jumps
tried local minimum. soon one found improved impurity
current hyperplane, algorithm moved hyperplane started running
deterministic perturbation procedure again. none 20 random jumps improved
impurity, search halted restarts (if any) tried. training
test partitions used methods cross-validation run (recall results
20

fiInduction Oblique Decision Trees

lr1

ot

rr1

-1

l-1

1

r-1

rl-

1
rr-

r-1

t-1
oo
R

3
4
4
4
77
4
33
333
33
4
33
33
7
3 4 4
4 44 44 4
1 33
3
3 3 3
4 4 7
4 44
3
4
4
44
33 3 3
3 33
4
4
7
4
33 3 3
4 44
111 1 3 33 3
7 7 77 7
4
3
3 3 3 33 3 3
4
1
4 4 444
1
44
7
4 4
3
3
4
4 4
3 333 3
4 44444
44 4 4 44
47
1 1
777
3
33 3 4 4
3
4
7
4 4
444 4
7
4444 4
3
7
334 4 4
44
44
33
1
77 77 777
3
4 4
34 44
1 1
7 777 7
44
113
4 7 777 7
4
7
7
4
1
1
4
1
4 4 44 4 4 44 4 4
1
3
7
1
7
1 111
4
4
1
7
7 7 7 777 7
44
4 4 4447
44 4 4
4
77
7
111 1 1 1 4 4 44 4 4
7 7 77 7
44 4
1
4
1 1
4 44 4
7
444
7 7
44 4 4
1 111 1
4
1
4
1
7 77
11
1 1
44 4
4
4
1
7
7
7
7 77
1 1 11 14
1 1 111
4 4 44 477 7
7
4 44
7
1
77
4 4
77 7 7 7 7 7
11
4
8
4
11
7
1 1 1
4
8
2
77 77
7
4 4 7
1
2 4 44 4
7
11 1
7
8
7 77
7
1
1
1
2
1
2
7
7
8
11
11
22 2444 4 4
7 777 77 7 77 7 7 77 7 7 8
4
1 2 22 2 2
7
1
88 8
ll-2
2
7
77 7
4
1 1 22
8
7 7
22 2 22 44 4 4 77
7
7 77888 8
11
1
22 22
8
888
2 22 22
5
7
1
8
22 222
1
2 2
88
7 77 7
2
2
2
11 2 2
8
22
2
5
7777
8
8
222 22 2
8
222
2 2
2
77 8 8 8
2
2
55
77 8
2 2 22 2
22
8 88 8 8 8
5
55
8
2 22 2
2
2 2
8 88
7 8
2 222
8
5 5 77
888 8
2 2
8
2
5
5
5
5
2
2
8
2
55 55
8
88 8
22 222 2
7
2 22
8
2
25
8 88 8
5
2
22
88
55
5
8 8 8 8 8 88
5 5
2 22 2 22 22 2 22 22
8
8 88 8
5
5
2 2
55
22
8 8 88 8
5
2 22 2 5 5 5
8
2 2
5 56
8
8 8
5
5
8
2
2
55
5
5 5 55 5 6 6
8 88 8888
22
5
2
8
5
5
8 88
6 66 8 8
2 2 2 2 2 2 2 2 5 5 55 5 5
6
8
6
8
8
2 2 2 2
8
55
8
5
6
8
8 8
5
6
55
2 2222 2 5
5
5
5
5
5
6
6
6
5 5 5
5 55
6
55 5 5
222 22 2 22
8 8
6
5
55 5 5
6
6
6
66
8
5
55
8
22
6 66 6
8 8
5 5 5
2 22
55
6
6
6 6
2 2
2
5 55 5 5 5
5
6 66 8 8
6
2
5
66 6
5
6 66
55 5
5
5 6
5
55 5
6 6 66
55
5
22 2
55 5
5 5
66 6 6
6
55
6666
8
5 5
6
5
6
6
6
66
55
6 6 66
55
55
8 8
6 66 6
5 5555
6 66 6
5
6
66
2
5 5555
5
66
6
5
5
8
66 6 6
5 5 5 55 555 5
6 66
55 5
6 68
5
6 6
6
6 6

Ro

-1
rrr

2
2
1
2
11
1
22
222
11
2
22
22
1
2 2 2
1 11 11 1
1 11
2
2 2 2
1 1 1
2 22
2
2
1
11
22 2 2
2 22
2
2
1
2
22 2 2
2 22
111 1 1 11 2
1 1 11 1
1
2
1 2 2 22 2 2
1
1
2 2 222
1
22
1
1 1
2
2
2
2 2
2 222 2
2 22222
22 2 2 11
11
1 1
111
2
22 2 2 2
1
2
1
2 1
222 2
1
2222 2
1
1
222 2 2
22
22
22
1
11 11 111
1
2 2
22 22
1 1
1 111 1
22
111
2 2 222 1
2
1
2
2
1
1
2
1
2 2 22 2 2 22 2 2
1
1
1
1
1
1 111
2
2
1
2
2 1 1 111 1
22
2 2 2222
22 2 2
1
22
2
111 1 1 1 1 1 11 2 2
1 1 11 1
22 2
1
2
1 1
2 22 2
2
222
1 1
22 2 2
1 111 1
2
1
2
1
2 22
11
1 1
22 2
2
1
1
2
1
2
2 22
1 1 11 11
1 1 111
2 2 22 222 2
2
2 22
2
1
22
2 2
22 2 2 2 2 2
11
1
2
2
11
2
1 1 1
2
2
1
22 22
2
2 2 2
1
1 1 11 1
2
21 1
2
2
2 22
2
1
1
1
1
2
1
2
2
2
11
111
11 1111 1 1
2 222 22 2 22 2 2 22 2 2 2
1
11 1
2
1
22 2
1 1
1
2
22 2
1
2 1 11
2
2 2
11 1 11 11 1 1 12
2
2 22222 2
22
2
11 11
2
222
1 11 11
1
2
2
2
22 111
2
1 1
22
2 22 2
1
1
1
22 2 2
2
11
1
1
2222
2
2
111 11 1
2
111
2 2
2
22 2 2 2
2
1
11
22 2
2 2 22 2
11
2 22 2 2 2
1
11
2
1 11 1
2
1 1
2 22
2 2
2 222
2
1 1 11
222 2
2 2
2
1
1
1
1
1
1
1
2
1
11 11
2
22 2
22 222 2
1
2 22
2
1
11
2 22 2
1
2
22
22
11
1
1 2 2 2 2 22
1 1
2 22 2 22 22 2 22 21
2
2 22 2
1
1
2 2
11
22
2 2 22 2
1
2 22 2 1 1 1
2
2 2
1 11
2
2 2
1
1
1
2
2
22
1
1 1 11 1 1 1
1 22 2222
22
2
2
2
1
2
2 22
1 11 1 1
1 2 2 2 2 2 2 2 2 2 22 1 1
1
2
1
1
1
1 1 2 2
2
11
2
1
1
1
2 2
1
1
11
2 2222 2 2
1
1
1
1
2
1
1
1
2 2 2
1 11
1
22 2 2
111 11 1 22
2 2
1
1
22 2 2
1
1
1
11
1
1
22
2
11
1 11 1
1 2
2 2 2
1
1 11
11
1
1
1 1
1 1
1
2 22 2 2 2
2
1 11 1 2
1
1
2
11 1
2
1 11
11 1
2
1 1
2
22 2
1 1 11
22
1
11 1
22 2
1 2
11 1 1
1
22
1111
1
2 2
2
2
1
1
2
11
11
1 1 11
11
22
1 1
1 11 1
1 1111
1 11 1
2
1
11
1
2 2222
1
22
1
1
2
1
22 1 1
1 1 1 11 111 2
1 11
11 1
1 11
1
2 2
1
1 1

Figure 9: POL RCB data sets
Linearly Separable 10-D (LS10) data
R:J Accuracy
Size
Hyperplanes
0:0 89.81.2 67.05.8
2756
0:20 91.51.5 55.27.0
3824
20:0 95.00.6 25.62.4
24913
20:20 97.20.7 13.93.2
30366
LMDT 99.70.2 2.20.5
9089
SADT 95.21.8 15.55.7
349067
Parallel Oblique Lines (POL) data
R:J Accuracy
Size
Hyperplanes
0:0 98.30.3 21.61.9
164
0:20 99.30.2 9.01.0
360
20:0 99.10.2 14.21.1
3230
20:20 99.60.1 5.50.3
4852
LMDT 89.610.2 41.919.2
1732
SADT 99.30.4 8.42.1
85594
Rotated Checker Board (RCB) data
R:J Accuracy
Size
Hyperplanes
0:0 98.40.2 35.51.4
573
0:20 99.30.3 19.70.8
1778
20:0 99.60.2 12.01.4
6436
20:20 99.80.1 8.70.4
11634
LMDT 95.72.3 70.19.6
2451
SADT 97.91.1 32.54.9
359112
Table 2: effect randomization OC1. first column, labelled R:J, shows
number restarts (R) followed maximum number random jumps (J)
attempted OC1 local minimum. Results LMDT SADT
included comparison four variants OC1. Size average tree size
measured number leaf nodes. third column shows average
number hyperplanes algorithm considered building one tree.
21

fiMurthy, Kasif & Salzberg

average ten 5-fold CVs). trees pruned algorithms,
data noise-free furthermore emphasis search.
Table 2 also includes number hyperplanes considered algorithm
building complete tree. Note OC1 SADT, number hyperplanes considered generally much larger number perturbations actually made,
algorithms compare newly generated hyperplanes existing hyperplanes
adjusting existing one. Nevertheless, number good estimate much effort
algorithm expends, every new hyperplane must evaluated according
impurity measure. LMDT, number hyperplanes considered identical
actual number perturbations.
4.3.2 Discussion

OC1 results quite clear. first line table, labelled 0:0, gives
accuracies tree sizes randomization used | variant similar
CART-LC algorithm. increase use randomization, accuracy increases
tree size decreases, exactly result hoped decided
introduce randomization method.
Looking closely tables, ask effect random jumps alone.
illustrated second line (0:20) table, attempted 20 random
jumps local minimum restarts. Accuracy increased 1-2% domain,
tree size decreased dramatically, roughly factor two, POL RCB
domains. Note noise domains, high accuracies
expected. Thus increases percent accuracy possible.
Looking third line sub-table Table 2, see effect multiple restarts
OC1. 20 restarts random jumps escape local minima, improvement
even noticeable LS10 data random jumps alone used.
data set, accuracy jumped significantly, 89.8 95.0%, tree size dropped
67 26 nodes. POL RCB data, improvements comparable
obtained random jumps. RCB data, tree size dropped factor 3
(from 36 leaf nodes 12 leaf nodes) accuracy increased 98.4 99.6%.
fourth line table shows effect randomized steps. Among
OC1 entries, line highest accuracies smallest trees three
data sets, clear randomization big win kinds problems.
addition, note smallest tree RCB data eight leaf nodes,
OC1's average trees, without pruning, 8.7 leaf nodes. clear data
set, thought dicult one, OC1 came close finding optimal
tree nearly every run. (Recall numbers table average 10 5-fold
CV experiments; i.e., average 50 decision trees.) LS10 data show dicult
find simple concept higher dimensions|the optimal tree
single hyperplane (two nodes), OC1 unable find current parameter
settings.8 POL data required minimum 5 leaf nodes, OC1 found minimalsize tree time, seen table. Although shown Table,
8. separate experiment, found OC1 consistently finds linear separator LS10 data
10 restarts 200 random jumps used.

22

fiInduction Oblique Decision Trees

OC1 using Sum Minority performed better POL data Twoing Rule
impurity measure; i.e., found correct tree using less time.
results LMDT SADT data lead interesting insights.
surprisingly, LMDT well linearly separable (LS10) data,
require inordinate amount search. Clearly, data linearly separable, one
use method LMDT linear programming. OC1 SADT diculty finding
linear separator, although experiments OC1 eventually find it, given sucient
time.
hand, non-linearly separable data sets, LMDT produces
much larger trees significantly less accurate produced OC1
SADT. Even deterministic variant OC1 (using zero restarts zero random jumps)
outperforms LMDT problems, much less search.
Although SADT sometimes produces accurate trees, main weakness
enormous amount search time required, roughly 10-20 times greater OC1 even
using 20:20 setting. One explanation OC1's advantage use directed search,
opposed strictly random search used simulated annealing. Overall, Table 2 shows
OC1's use randomization quite effective non-linearly separable data.
natural ask randomization helps OC1 task inducing decision trees.
Researchers combinatorial optimization observed randomized search usually
succeeds search space holds abundance good solutions (Gupta, Smolka,
& Bhaskar, 1994). Furthermore, randomization improve upon deterministic search
many local maxima search space lead poor solutions. OC1's search
space, local maximum hyperplane cannot improved deterministic
search procedure, \solution" complete decision tree. significant fraction
local maxima lead bad trees, algorithms stop first local maximum
encounter perform poorly. randomization allows OC1 consider many
different local maxima, modest percentage maxima lead good trees,
good chance finding one trees. experiments OC1 thus far indicate
space oblique hyperplanes usually contains numerous local maxima,
substantial percentage locally good hyperplanes lead good decision trees.

5. Conclusions Future Work
paper described OC1, new system constructing oblique decision trees.
shown experimentally OC1 produce good classifiers range real-world
artificial domains. also shown use randomization improves upon
original algorithm proposed Breiman et al. (1984), without significantly increasing
computational cost algorithm.
use randomization might also beneficial axis-parallel tree methods. Note
although find optimal test (with respect impurity measure)
node tree, complete tree may optimal: well known, problem
finding smallest tree NP-Complete (Hyafil & Rivest, 1976). Thus even axis-parallel
decision tree methods produce \ideal" decision trees. Quinlan suggested
windowing algorithm might used way introducing randomization C4.5, even
though algorithm designed another purpose (Quinlan, 1993a). (The windowing
23

fiMurthy, Kasif & Salzberg

algorithm selects random subset training data builds tree using that.)
believe randomization powerful tool context decision trees,
experiments one example might exploited. process
conducting experiments quantify accurately effects different forms
randomization.
clear ability produce oblique splits node broadens capabilities decision tree algorithms, especially regards domains numeric attributes.
course, axis-parallel splits simpler, sense description split
uses one attribute node. OC1 uses oblique splits impurity less
impurity best axis-parallel split; however, one could easily penalize
additional complexity oblique split further. remains open area
research. general point domain best captured tree uses
oblique hyperplanes, desirable system generate tree.
shown problems, including used experiments, OC1 builds small
decision trees capture domain well.

Appendix A. Complexity Analysis OC1

following, show OC1 runs eciently even worst case. data
set n examples (points) attributes per example, OC1 uses O(dn2 log n)
time. assume n > analysis.
analysis here, assume coecients hyperplane adjusted sequential order (the Seq method described paper). number restarts node
r, number random jumps tried j . r j constants, fixed
advance running algorithm.
Initializing hyperplane random position takes O(d) time. need
consider first maximum amount work OC1 finds new location
hyperplane. need consider many times move hyperplane.
1. Attempting perturb first coecient (a1 ) takes O(dn + n log n) time. Computing
Ui 's points (equation 2) requires O(dn) time, sorting Ui 's takes
O(n log n). gives us O(dn + n log n) work.
2. perturbing a1 improve things, try perturb a2 . Computing new
Ui 's take O(n) time one term different Ui . Re-sorting
take O(n log n), step takes O(n) + O(n log n) = O(n log n) time.
3. Likewise a3; : : :; ad take O(n log n) additional time, assuming still
found better hyperplane checking coecient. Thus total time cycle
attempt perturb additional coecients (d , 1) O(n log n) =
O(dn log n).
4. Summing up, time cycle coecients O(dn log n)+O(dn+n log n) =
O(dn log n).
5. none coecients improved split, attempt make j random
jumps. Since j constant, consider j = 1 analysis. step
24

fiInduction Oblique Decision Trees

involves choosing random vector running perturbation algorithm solve
ff, explained Section 3.2. before, need compute set Ui 's sort
them, takes O(dn + n log n) time. amount time dominated
time adjust coecients, total time far still O(dn log n).
time OC1 spend node either halting finding improved
hyperplane.
6. Assuming OC1 using Sum Minority Max Minority error measure,
reduce impurity hyperplane n times. clear improvement
means one example correctly classified new hyperplane. Thus
total amount work node limited n O(dn log n) = O(dn2 log n). (This
analysis extends, linear cost factors, Information Gain, Gini Index
Twoing Rule two categories. apply measure that,
example, uses distances mis-classified objects hyperplane.) practice,
found number improvements per node much smaller n.
Assuming OC1 adjusts hyperplane improves impurity measure,
O(dn2 log n) work worst case.
However, OC1 allows certain number adjustments hyperplane
improve impurity, although never accept change worsens impurity.
number allowed determined constant known \stagnant-perturbations". Let
value s. works follows.
time OC1 finds new hyperplane improves old one, resets counter
zero. move new hyperplane different location equal impurity
times. moves repeats perturbation algorithm. Whenever
impurity reduced, re-starts counter allows moves equally good
locations. Thus clear feature increases worst-case complexity OC1
constant factor, s.
Finally, note overall cost OC1 also O(dn2 log n), i.e., upper
bound total running time OC1 independent size tree ends
creating. (This upper bound applies Sum Minority Max Minority; open question
whether similar upper bound proven Information Gain Gini Index.)
Thus worst-case asymptotic complexity system comparable systems
construct axis-parallel decision trees, O(dn2 ) worst-case complexity.
sketch intuition leads bound, let G total impurity summed
leaves partially constructed tree (i.e., sum currently misclassified points
tree). observe time run perturbation algorithm node
tree, either halt improve G least one unit. worst-case analysis one node
realized perturbation algorithm run every one n examples,
happens, would longer mis-classified examples tree
would complete.

Appendix B. Definitions impurity measures available OC1

addition Twoing Rule defined text, OC1 contains built-in definitions five
additional impurity measures, defined follows. following definitions,
25

fiMurthy, Kasif & Salzberg

set examples node split contains n (> 0) instances belong
one k categories. (Initially set entire training set.) hyperplane H divides
two non-overlapping subsets TL TR (i.e., left right). Lj Rj number
instances category j TL TR respectively. impurity measures initially
check see TL TR homogeneous (i.e., examples belong category),
return minimum (zero) impurity.

Information Gain. measure information gained particular split pop-

ularized context decision trees Quinlan (1986). Quinlan's definition makes
information gain goodness measure; i.e., something maximize. OC1 attempts
minimize whatever impurity measure uses, use reciprocal standard value
information gain OC1 implementation.

Gini Index. Gini Criterion (or Index) proposed decision trees Breiman et
al. (1984). Gini Index originally defined measures probability misclassification
set instances, rather impurity split. implement following
variation:
GiniL = 1:0 ,
GiniR = 1:0 ,

k
X
i=1
k
X
i=1

(Li =jTLj)2
(Ri=jTRj)2

Impurity = (jTLj GiniL + jTRj GiniR)=n
GiniL Gini Index \left" side hyperplane GiniR
right.

Max Minority. measures Max Minority, Sum Minority Sum Variances

defined context decision trees Heath, Kasif, Salzberg (1993b).9 Max
Minority theoretical advantage tree built minimizing measure
depth log n. experiments indicated great advantage
practice: seldom impurity measures produce trees substantially deeper
produced Max Minority. definition is:
MinorityL =
MinorityR =

k
X
i=1;i6=max Li
k
X
i=1;i6=max Ri

Li
Ri

Max Minority = max(MinorityL; MinorityR)
9. Sum Variances called Sum Impurities Heath et al.

26

fiInduction Oblique Decision Trees

Sum Minority. measure similar Max Minority. MinorityL MinorityR defined Max Minority measure, Sum Minority sum
two values. measure simplest way quantifying impurity, simply
counts number misclassified instances.
Though Sum Minority performs well domains, obvious aws.
one example, consider domain n = 100; = 1, k = 2 (i.e., 100 examples, 1
numeric attribute, 2 classes). Suppose examples sorted according
single attribute, first 50 instances belong category 1, followed 24 instances category 2, followed 26 instances category 1. possible splits distribution
sum minority 24. Therefore impossible using Sum Minority distinguish split preferable, although splitting alternations categories
clearly better.
Sum Variances. definition measure is:
jX
TL j
jX
TL j
VarianceL = (Cat(TLi ) , Cat(TLj )=jTLj)2
VarianceR =

i=1

j =1

jX
TRj

jX
TRj

i=1

(Cat(TRi ) ,

j =1

Cat(TRj )=jTRj)2

Sum Variances = VarianceL + VarianceR
Cat(Ti) category instance Ti . measure computed using actual
class labels, easy see impurity computed varies depending numbers
assigned classes. instance, T1 consists 10 points category 1 3
points category 2, T2 consists 10 points category 1 3 points category
5, Sum Variances values different T1 T2. avoid problem,
OC1 uniformly reassigns category numbers according frequency occurrence
category node computing Sum Variances.

Acknowledgements
authors thank Richard Beigel Yale University suggesting idea jumping
random direction. Thanks Wray Buntine Nasa Ames Research Center providing
IND 2.1 package, Carla Brodley providing LMDT code, David Heath
providing SADT code assisting us using it. Thanks also three anonymous
reviewers many helpful suggestions. material based upon work supported
National Science foundation Grant Nos. IRI-9116843, IRI-9223591, IRI-9220960.

References
Aha, D. (1990). Study Instance-Based Algorithms Supervised Learning: Mathematical, empirical psychological evaluations. Ph.D. thesis, Department Information
Computer Science, University California, Irvine.
27

fiMurthy, Kasif & Salzberg

Almuallin, H., & Dietterich, T. (1991). Learning many irrelevant features. Proceedings Ninth National Conference Artificial Intelligence, pp. 547{552. San
Jose, CA.
Belsley, D. (1980). Regression Diagnostics: Identifying uential Data Sources
Collinearity. Wiley & Sons, New York.
Bennett, K., & Mangasarian, O. (1992). Robust linear programming discrimination two
linearly inseparable sets. Optimization Methods Software, 1, 23{34.
Bennett, K., & Mangasarian, O. (1994a). Multicategory discrimination via linear programming. Optimization Methods Software, 3, 29{39.
Bennett, K., & Mangasarian, O. (1994b). Serial parallel multicategory discrimination.
SIAM Journal Optimization, 4 (4).
Blum, A., & Rivest, R. (1988). Training 3-node neural network NP-complete. Proceedings 1988 Workshop Computational Learning Theory, pp. 9{18. Boston,
MA. Morgan Kaufmann.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification Regression
Trees. Wadsworth International Group.
Brent, R. P. (1991). Fast training algorithms multilayer neural nets. IEEE Transactions
Neural Networks, 2 (3), 346{354.
Brodley, C. E., & Utgoff, P. E. (1992). Multivariate versus univariate decision trees. Tech.
rep. COINS CR 92-8, Dept. Computer Science, University Massachusetts
Amherst.
Brodley, C. E., & Utgoff, P. E. (1994). Multivariate decision trees. Machine Learning,
appear.
Buntine, W. (1992). Tree classification software. Technology 2002: Third National
Technology Transfer Conference Exposition.
Buntine, W., & Niblett, T. (1992). comparison splitting rules decision-tree
induction. Machine Learning, 8, 75{85.
Cardie, C. (1993). Using decision trees improve case-based learning. Proceedings
Tenth International Conference Machine Learning, pp. 25{32. University
Massachusetts, Amherst.
Cestnik, G., Kononenko, I., & Bratko, I. (1987). Assistant 86: knowledge acquisition
tool sophisticated users. Bratko, I., & Lavrac, N. (Eds.), Progress Machine
Learning. Sigma Press.
Cios, K. J., & Liu, N. (1992). machine learning method generation neural network
architecture: continuous ID3 algorithm. IEEE Transactions Neural Networks,
3 (2), 280{291.
28

fiInduction Oblique Decision Trees

Cohen, W. (1993). Ecient pruning methods separate-and-conquer rule learning systems. Proceedings 13th International Joint Conference Artificial Intelligence, pp. 988{994. Morgan Kaufmann.
Fayyad, U. M., & Irani, K. B. (1992). attribute specification problem decision tree
generation. Proceedings Tenth National Conference Artificial Intelligence,
pp. 104{110. San Jose CA. AAAI Press.
Frean, M. (1990). Small Nets Short Paths: Optimising neural computation. Ph.D.
thesis, Centre Cognitive Science, University Edinburgh.
Gupta, R., Smolka, S., & Bhaskar, S. (1994). randomization sequential distributed
algorithms. ACM Computing Surveys, 26 (1), 7{86.
Hampson, S., & Volper, D. (1986). Linear function neurons: Structure training. Biological Cybernetics, 53, 203{217.
Harrison, D., & Rubinfeld, D. (1978). Hedonic prices demand clean air. Journal
Environmental Economics Management, 5, 81{102.
Hassibi, B., & Stork, D. (1993). Second order derivatives network pruning: optimal
brain surgeon. Advances Neural Information Processing Systems 5, pp. 164{171.
Morgan Kaufmann, San Mateo, CA.
Heath, D. (1992). Geometric Framework Machine Learning. Ph.D. thesis, Johns
Hopkins University, Baltimore, Maryland.
Heath, D., Kasif, S., & Salzberg, S. (1993a). k-DT: multi-tree learning method.
Proceedings Second International Workshop Multistrategy Learning, pp. 138{
149. Harpers Ferry, WV. George Mason University.
Heath, D., Kasif, S., & Salzberg, S. (1993b). Learning oblique decision trees. Proceedings
13th International Joint Conference Artificial Intelligence, pp. 1002{1007.
Chambery, France. Morgan Kaufmann.
Herman, G. T., & Yeung, K. D. (1992). piecewise-linear classification. IEEE Transactions Pattern Analysis Machine Intelligence, 14 (7), 782{786.
Holte, R. (1993). simple classification rules perform well commonly used
datasets. Machine Learning, 11 (1), 63{90.
Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees NPcomplete. Information Processing Letters, 5 (1), 15{17.
Kira, K., & Rendell, L. (1992). practical approach feature selection. Proceedings
Ninth International Conference Machine Learning, pp. 249{256. Aberdeen,
Scotland. Morgan Kaufmann.
Kirkpatrick, S., Gelatt, C., & Vecci, M. (1983). Optimization simulated annealing.
Science, 220 (4598), 671{680.
29

fiMurthy, Kasif & Salzberg

Kodratoff, Y., & Manago, M. (1987). Generalization noise. International Journal
Man-Machine Studies, 27, 181{204.
Langley, P., & Sage, S. (1993). Scaling domains many irrelevant features. Learning
Systems Department, Siemens Corporate Research, Princeton, NJ.
Mangasarian, O., Setiono, R., & Wolberg, W. (1990). Pattern recognition via linear programming: Theory application medical diagnosis. SIAM Workshop
Optimization.
Mingers, J. (1989a). empirical comparison pruning methods decision tree induction. Machine Learning, 4 (2), 227{243.
Mingers, J. (1989b). empirical comparison selection measures decision tree induction. Machine Learning, 3, 319{342.
Moret, B. M. (1982). Decision trees diagrams. Computing Surveys, 14 (4), 593{623.
Murphy, P., & Aha, D. (1994). UCI repository machine learning databases { machinereadable data repository. Maintained Department Information Computer
Science, University California, Irvine. Anonymous FTP ics.uci.edu
directory pub/machine-learning-databases.
Murthy, S. K., Kasif, S., Salzberg, S., & Beigel, R. (1993). OC1: Randomized induction
oblique decision trees. Proceedings Eleventh National Conference Artificial
Intelligence, pp. 322{327. Washington, D.C. MIT Press.
Murthy, S. K., & Salzberg, S. (1994). Using structure improve decision trees. Tech. rep.
JHU-94/12, Department Computer Science, Johns Hopkins University.
Niblett, T. (1986). Constructing decision trees noisy domains. Bratko, I., & Lavrac,
N. (Eds.), Progress Machine Learning. Sigma Press, England.
Nilsson, N. (1990). Learning Machines. Morgan Kaufmann, San Mateo, CA.
Odewahn, S., Stockwell, E., Pennington, R., Humphreys, R., & Zumach, W. (1992). Automated star-galaxy descrimination neural networks. Astronomical Journal,
103 (1), 318{331.
Pagallo, G. (1990). Adaptive Decision Tree Algorithms Learning Examples. Ph.D.
thesis, University California Santa Cruz.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. Machine
Learning, 5 (1), 71{99.
Quinlan, J. R. (1983). Learning ecient classification procedures application
chess end games. Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), Machine
Learning: Artificial Intelligence Approach. Morgan Kaufmann, San Mateo, CA.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.
30

fiInduction Oblique Decision Trees

Quinlan, J. R. (1987). Simplifying decision trees. International Journal Man-Machine
Studies, 27, 221{234.
Quinlan, J. R. (1993a). C4.5: Programs Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.
Quinlan, J. R. (1993b). Combining instance-based model-based learning. Proceedings
Tenth International Conference Machine Learning, pp. 236{243 University
Massachusetts, Amherst. Morgan Kaufmann.
Roth, R. H. (1970). approach solving linear discrete optimization problems. Journal
ACM, 17 (2), 303{313.
Safavin, S. R., & Landgrebe, D. (1991). survey decision tree classifier methodology.
IEEE Transactions Systems, Man Cybernetics, 21 (3), 660{674.
Sahami, M. (1993). Learning non-linearly separable boolean functions linear threshold unit trees madaline-style networks. Proceedings Eleventh National
Conference Artificial Intelligence, pp. 335{341. AAAI Press.
Salzberg, S. (1991). nearest hyperrectangle learning method. Machine Learning, 6,
251{276.
Salzberg, S. (1992). Combining learning search create good classifiers. Tech. rep.
JHU-92/12, Johns Hopkins University, Baltimore MD.
Salzberg, S., Chandar, R., Ford, H., Murthy, S. K., & White, R. (1994). Decision trees
automated identification cosmic rays Hubble Space Telescope images. Publications Astronomical Society Pacific, appear.
Schaffer, C. (1993). Overfitting avoidance bias. Machine Learning, 10, 153{178.
Schlimmer, J. (1993). Eciently inducing determinations: complete systematic
search algorithm uses optimal pruning. Proceedings Tenth International
Conference Machine Learning, pp. 284{290. Morgan Kaufmann.
Smith, J., Everhart, J., Dickson, W., Knowler, W., & Johannes, R. (1988). Using
ADAP learning algorithm forecast onset diabetes mellitus. Proceedings
Symposium Computer Applications Medical Care, pp. 261{265. IEEE
Computer Society Press.
Utgoff, P. E. (1989). Perceptron trees: case study hybrid concept representations.
Connection Science, 1 (4), 377{391.
Utgoff, P. E., & Brodley, C. E. (1990). incremental method finding multivariate
splits decision trees. Proceedings Seventh International Conference
Machine Learning, pp. 58{65. Los Altos, CA. Morgan Kaufmann.
Utgoff, P. E., & Brodley, C. E. (1991). Linear machine decision trees. Tech. rep. 10,
University Massachusetts Amherst.
31

fiMurthy, Kasif & Salzberg

Van de Merckt, T. (1992). NFDT: system learns exible concepts based decision
trees numerical attributes. Proceedings Ninth International Workshop
Machine Learning, pp. 322{331.
Van de Merckt, T. (1993). Decision trees numerical attribute spaces. Proceedings
13th International Joint Conference Artificial Intelligence, pp. 1016{1021.
Weiss, S., & Kapouleas, I. (1989). empirical comparison pattern recognition, neural
nets, machine learning classification methods. Proceedings 11th International Joint Conference Artificial Intelligence, pp. 781{787. Detroit, MI. Morgan
Kaufmann.
Wolpert, D. (1992). overfitting avoidance bias. Tech. rep. SFI TR 92-03-5001,
Santa Fe Institute, Santa Fe, New Mexico.

32

fiJournal Artificial Intelligence Research 2 (1995) 501-539

Submitted 9/94; published 5/95

Pac-Learning Recursive Logic Programs:
Ecient Algorithms
William W. Cohen

AT&T Bell Laboratories
600 Mountain Avenue, Murray Hill, NJ 07974 USA

wcohen@research.att.com

Abstract

present algorithms learn certain classes function-free recursive logic programs polynomial time equivalence queries. particular, show single
k-ary recursive constant-depth determinate clause learnable. Two-clause programs consisting one learnable recursive clause one constant-depth determinate non-recursive
clause also learnable, additional \basecase" oracle assumed. results immediately imply pac-learnability classes. Although classes learnable
recursive programs constrained, shown companion paper
maximally general, generalizing either class natural way leads computationally dicult learning problem. Thus, taken together companion paper,
paper establishes boundary ecient learnability recursive logic programs.

1. Introduction
One active area research machine learning learning concepts expressed firstorder logic. Since researchers used variant Prolog represent learned
concepts, subarea sometimes called inductive logic programming (ILP) (Muggleton,
1992; Muggleton & De Raedt, 1994).
Within ILP, researchers considered two broad classes learning problems.
first class problems, call logic based relational learning problems,
first-order variants sorts classification problems typically considered within
AI machine learning community: prototypical examples include Muggleton et al.'s (1992)
formulation ff-helix prediction, King et al.'s (1992) formulation predicting drug activity, Zelle Mooney's (1994) use ILP techniques learn control heuristics
deterministic parsers. Logic-based relational learning often involves noisy examples ect relatively complex underlying relationship; natural extension propositional
machine learning, already enjoyed number experimental successes.
second class problems studied ILP researchers, target concept Prolog
program implements common list-processing arithmetic function; prototypical
problems class might learning append two lists, multiply two numbers.
learning problems similar character studied area automatic
programming examples (Summers, 1977; Biermann, 1978), hence might appropriately called automatic logic programming problems. Automatic logic programming
problems characterized noise-free training data recursive target concepts. Thus
problem central enterprise automatic logic programming|but not, perhaps,
logic-based relational learning|is problem learning recursive logic programs.
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCohen

goal paper formally analyze learnability recursive logic programs
Valiant's (1984) model pac-learnability, thus hopefully shedding light
task automatic logic programming. summarize results, show
simple recursive programs pac-learnable examples alone, examples plus
small number additional \hints". largest learnable class identify standard
learning model class one-clause constant-depth determinate programs
constant number \closed" recursive literals. largest learnable class identify
requires extra \hints" class constant-depth determinate programs consisting
single nonrecursive base clause single recursive clause class described
above. results proved model identification equivalence queries
(Angluin, 1988, 1989), somewhat stronger pac-learnability. Identification
equivalence queries requires target concept exactly identified, polynomial
time, using polynomial number equivalence queries . equivalence query
asks hypothesis program H equivalent target program C ; answer
query either \yes" adversarily chosen example H C differ.
model learnability arguably appropriate automatic logic programming tasks
weaker model pac-learnability, unclear often approximately
correct recursive program useful.
Interestingly, learning algorithms analyzed different existing ILP
learning methods; employ unusual method generalizing examples called forced
simulation . Forced simulation simple analytically tractable alternative
methods generalizing recursive programs examples, n-th root finding
(Muggleton, 1994), sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) recursive
anti-unification (Idestam-Almquist, 1993), rarely used experimental
ILP systems (Ling, 1991).
paper organized follows. presenting preliminary definitions,
begin presenting (primarily pedagogical reasons) procedure identifying
equivalence queries single non-recursive constant-depth determinate clause. Then,
Section 4, extend learning algorithm, corresponding proof correctness,
simple class recursive clauses: class \closed" linear recursive constant-depth
determinate clauses. Section 5, relax assumptions made make analysis
easier, present several extensions algorithm: extend algorithm linear
recursion k-ary recursion, also show k-ary recursive clause non-recursive
clause learned simultaneously given additional \basecase" oracle. discuss
related work conclude.
Although learnable class programs large enough include well-known
automatic logic programming benchmarks, extremely restricted. companion paper
(Cohen, 1995), provide number negative results, showing relaxing
restrictions leads dicult learning problems: particular, learning problems
either hard learning DNF (an open problem computational learning theory),
hard cracking certain presumably secure cryptographic schemes. Thus, taken together
results companion paper, results delineate boundary learnability
recursive logic programs.
Although two papers independent, suggest readers wishing read
paper companion paper read paper first.
502

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2. Background

section present technical background necessary state results.
assume, however, reader familiar basic elements logic programming; readers without background referred one standard texts, example
(Lloyd, 1987).

2.1 Logic Programs

treatment logic programs standard, except usually consider body
clause ordered set literals.
paper, consider logic programs without function symbols|
i.e., programs written Datalog.1 purpose logic program answer
certain questions relative database , DB , set ground atomic facts. (When
convenient, also think DB conjunction ground unit clauses.) simplest
use Datalog program check status simple instance . simple instance
(for program P database DB ) fact f . pair (P; DB ) said cover f iff
DB ^ P ` f . set simple instances covered (P; DB ) precisely minimal model
logic program P ^ DB .
paper, primarily consider extended instances consist two parts:
instance fact f , simply ground fact, description D, finite set
ground unit clauses. extended instance e = (f; D) covered (P; DB ) iff
DB ^ ^ P ` f

extended instances allowed, function-free programs expressive enough
encode surprisingly interesting programs. particular, many programs usually
written function symbols re-written function-free programs, example
illustrates.

Example. Consider usual program appending two lists.
append([],Ys,Ys).
append([XjXs1],Ys,[XjZs1])

append(Xs1,Ys,Zs1).

One could use program classify atomic facts containing function symbols
append([1,2],[3],[1,2,3]). program rewritten Datalog
program classifies extended instances follows:

Program P :

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
1. assumption made primarily convenience. Section 5.2 describe assumption
relaxed.

503

fiCohen

append(Xs1,Ys,Zs1).

Database DB :
null(nil).

predicate components(A,B,C) means list head B tail
C; thus extended instance equivalent append([1,2],[3],[1,2,3]) would

Instance fact f :

append(list12,list3,list123).

Description D:

components(list12,1,list2).
components(list2,2,nil).
components(list123,1,list23).
components(list23,2,list3).
components(list3,3,nil).
note using extended instances examples closely related using ground
clauses entailed target clause examples: specifically, instance e = (f; D)
covered P; DB iff P ^ DB ` (f D). example shows, also close
relationship extended instances literals function symbols
removed \ attening" (Rouveirol, 1994; De Raedt & Dzeroski, 1994). elected
use Datalog programs model extended instances paper several
reasons. Datalog relatively easy analyze. close connection Datalog
restrictions imposed certain practical learning systems, FOIL (Quinlan,
1990; Quinlan & Cameron-Jones, 1993), FOCL (Pazzani & Kibler, 1992), GOLEM
(Muggleton & Feng, 1992).
Finally, using extended instances addresses following technical problem. learning problems considered paper involve restricted classes logic programs. Often,
restrictions imply number simple instances polynomial; note
polynomial-size domain, questions pac-learnability usually trivial. Requiring
learning algorithms work domain extended instances precludes trivial learning
techniques, however, number extended instances size n exponential n even
highly restricted programs.

2.2 Restrictions Logic Programs

paper, consider learnability various restricted classes logic programs. define restrictions; however, first introduce
terminology.
B1 ^ : : : ^ Br (ordered) definite clause, input variables literal
Bi variables appearing Bi also appear clause B1 ^ : : : ^ Bi,1 ;
variables appearing Bi called output variables . Also, B1 ^ : : : ^ Br
definite clause, Bi said recursive literal predicate symbol
arity A, head clause.
504

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2.2.1 Types Recursion

first set restrictions concern type recursion allowed program.
every clause program one recursive literal, program linear
recursive . every clause program k recursive literals, program
k-ary recursive . Finally, every recursive literal program contains output variables,
say program closed recursive.
2.2.2 Determinacy Depth

second set restrictions variants restrictions originally introduced Muggleton
Feng (1992). B1 ^ : : : ^ Br (ordered) definite clause, literal Bi
determinate iff every possible substitution unifies fact e
DB ` B1 ^ : : : ^ Bi,1

one maximal substitution DB ` Bi . clause determinate
literals determinate. Informally, determinate clauses
evaluated without backtracking Prolog interpreter.
also define depth variable appearing clause B1 ^ : : : ^ Br follows.
Variables appearing head clause depth zero. Otherwise, let Bi first
literal containing variable V , let maximal depth input variables
Bi ; depth V +1. depth clause maximal depth variable
clause.
Muggleton Feng define logic program ij -determinate determinate,
constant depth i, contains literals arity j less. paper use phrase
\constant-depth determinate" instead denote class programs.
examples constant-depth determinate programs, taken Dzeroski, Muggleton
Russell (1992).

Example. Assuming successor functional, following program determinate. maximum depth variable one, variable C second
clause, hence program depth one.
less than(A,B)
less than(A,B)

successor(A,B).
successor(A,C) ^ less than(C,B).
!

following program, computes C , determinate depth
two.
choose(A,B,C)
zero(B) ^
one(C).
choose(A,B,C)
decrement(B,D) ^
decrement(A,E) ^
505

fiCohen

multiply(B,C,G) ^
divide(G,A,F) ^
choose(E,D,F).
program GOLEM (Muggleton & Feng, 1992) learns constant-depth determinate
programs, related restrictions adopted several practical learning
systems (Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). learnability
constant-depth determinate clauses also received formal study,
review Section 6.
2.2.3 Mode Constraints Declarations

define mode literal L appearing clause C string initial
character predicate symbol L, j > 1 j -th character \+"
(j , 1)-th argument L input variable \," (j , 1)-th argument L
output variable. (This definition coincides usual definition Prolog modes
arguments head clause inputs. simplification justified,
however, considering clauses behave classifying extended instances,
ground.) mode constraint simply set mode strings R = fs1 ; : : :; sk g,
clause C said satisfy mode constraint R p every literal L body
C , mode L R.

Example. following append program, every literal annotated
mode.

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
append(Xs1,Ys,Zs1).

% mode: null+
% mode: components + ,,
% mode: components + +,
% mode: append + ++

clauses program satisfy following mode constraint:
f components + ,,; components + +,; components + ,+;
components , ++; components + ++; null +
append + +,;
append + ,+;
append , ++;
append + ++
g
Mode constraints commonly used analyzing Prolog code; instance,
used many Prolog compilers. sometimes use alternative syntax mode
constraints parallels syntax used Prolog systems: instance, may
write mode constraint \components + ,," \components (+; ,; ,)".
define declaration tuple (p; a0; R) p predicate symbol, a0
integer, R mode constraint. say clause C satisfies declaration
head C arity a0 predicate symbol p, every literal L body
C mode L appears R.
506

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2.3 Model Learnability

section, present model learnability. first review necessary
definitions standard learning model, model learning equivalence queries
(Angluin, 1988, 1989), discuss relationship learning models.
introduce extension model necessary analyzing ILP problems.
2.3.1 Identification Equivalence Queries

Let X set. call X domain , call elements X instances . Define
concept C X representation subset X , define language Lang
set concepts. paper, rather casual distinction
concept set represents; risk confusion refer set
represented concept C extension C . Two concepts C1 C2
extension said (semantically) equivalent .
Associated X Lang two size complexity measures , use
following notation:

size complexity concept C 2 Lang written j C j .
size complexity instance e 2 X written j ej .
set, Sn stands set elements size complexity greater
n. instance, Xn = fe 2 X : j ej ng Langn = fC 2 Lang : j C j ng.
assume size measures polynomially related number bits needed
represent C e.
first learning model consider model identification equivalence
queries . goal learner identify unknown target concept C 2 Lang|
is, construct hypothesis H 2 Lang H C . Information
target concept gathered equivalence queries . input equivalence
query C hypothesis H 2 Lang. H C , response query
\yes". Otherwise, response query arbitrarily chosen counterexample |an
instance e symmetric difference C H .
deterministic algorithm Identify identifies Lang equivalence queries iff
every C 2 Lang, whenever Identify run (with oracle answering equivalence queries
C ) eventually halts outputs H 2 Lang H C . Identify
polynomially identifies Lang equivalence queries iff polynomial poly (nt; ne )
point execution Identify total running time bounded
poly (nt ; ne ), nt = j C j ne size largest counterexample seen far,
0 equivalence queries made.
2.3.2 Relation Pac-Learnability

model identification equivalence queries well-studied (Angluin, 1988,
1989). known language learnable model, also learnable
Valiant's (1984) model pac-learnability. (The basic idea behind result
equivalence query hypothesis H emulated drawing set random
507

fiCohen

examples certain size. counterexample H , one returns
found counterexample answer equivalence query. counterexamples
found, one assume high confidence H approximately equivalent
target concept.) Thus identification equivalence queries strictly stronger model
pac-learnability.
existing positive results pac-learnability logic programs rely showing
every concept target language emulated boolean concept
pac-learnable class (Dzeroski et al., 1992; Cohen, 1994). results
illuminating, also disappointing, since one motivations considering firstorder representations first place allow one express concepts cannot
easily expressed boolean logic. One advantage studying exact identification
model considering recursive programs essentially precludes use sort
proof technique: many recursive programs approximated boolean functions
fixed set attributes, exactly emulated boolean functions.
2.3.3 Background Knowledge Learning

framework described standard, one possible formalization usual
situation inductive concept learning, user provides set examples (in
case counterexamples queries) learning system attempts find useful
hypothesis. However, typical ILP system, setting slightly different, usually
user provides clues target concept addition examples. ILP
systems user provides database DB \background knowledge" addition set
examples; paper, assume user also provides declaration.
account additional inputs necessary extend framework described
setting learner accepts inputs training examples.
formalize this, introduce following notion \language family". Lang
set clauses, DB database Dec declaration, define Lang[DB ; Dec]
set pairs (C; DB ) C 2 Lang C satisfies Dec . Semantically,
pair denote set extended instances (f; D) covered (C; DB ). Next,
DB set databases DEC set declarations, define
Lang[DB ; DEC ] = fLang[DB ; Dec ] : DB

2 DB Dec 2 DECg

set languages called language family .
extend definition identification equivalence queries language families follows. language family Lang[DB; DEC ] identifiable equivalence
queries iff every language set identifiable equivalence queries. language
family Lang[DB; DEC ] uniformly identifiable equivalence queries iff single
algorithm Identify (DB ; Dec) identifies language Lang[DB ; Dec ] family
given DB Dec .
Uniform polynomial identifiability language family defined analogously:
Lang[DB; DEC ] uniformly polynomially identifiable equivalence queries iff
polynomial time algorithm Identify (DB ; Dec ) identifies language Lang[DB ; Dec]
family given DB Dec . Note Identify must run time polynomial
size inputs Dec DB well target concept.
508

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2.3.4 Restricted Types Background Knowledge

describe number restricted classes databases declarations.
One restriction make throughout paper assume
predicates interest bounded arity. use notation a-DB set
databases contain facts arity less, notation a-DEC set
declarations (p; a0; R) every string 2 R length + 1 less.
technical reasons, often convenient assume database contains
equality predicate |that is, predicate symbol equal equal (ti ; ti) 2 DB every
constant ti appearing DB , equal (ti ; tj ) 62 DB ti 6= tj . Similarly,
often wish assume declaration allows literals form equal(X,Y), X
input variables. DB (respectively DEC ) set databases (declarations)
use DB = (DEC = ) denote corresponding set, additional restriction
database (declaration) must contain equality predicate (respectively mode
equal (+; +)).
sometimes also convenient assume declaration (p; a0; R) allows
single valid mode predicate: i.e., predicate q R
single mode constraint form qff. declaration called unique-mode
declaration. DEC set declarations use DEC 1 denote corresponding
set declarations additional restriction declaration unique-mode.
Finally, note typical setting, facts appear database DB
descriptions extended instances arbitrary: instead, representative
\real" predicate (e.g., relationship list components example above).
One way formalizing assume facts drawn restricted set F ;
using assumption one define notion determinate mode . f = p(t1 ; : : :; tk )
fact predicate symbol p pff mode, define inputs (f; pff)
tuple hti1 ; : : :; tik i, i1, : : : , ik indices ff containing \+". Also define
outputs (f; pff) tuple htj1 ; : : :; tjl i, j1 , : : : , jl indices ff containing
\,". mode string pff predicate p determinate F iff relation

fhinputs (f; pff); outputs (f; pff)i : f 2 Fg
function. Informally, mode determinate input positions facts F
functionally determine output positions.
set declarations containing modes determinate F denoted
DetDEC F . However, paper, set F assumed fixed, thus
generally omit subscript.
program consistent determinate declaration Dec 2 DetDEC must determinate, defined above; words, consistency determinate declaration
sucient condition semantic determinacy. also condition verified
simple syntactic test.
2.3.5 Size Measures Logic Programs

Assuming predicates arity less constant also allows simple
size measures used. paper, measure size database DB
cardinality; size extended instance (f; D) cardinality D; size
509

fiCohen

declaration (p; a0; R) cardinality R; size clause B1 ^ : : : ^ Br
number literals body.

3. Learning Nonrecursive Clause

learning algorithms presented paper use generalization technique
call forced simulation. way introduction technique, consider
learning algorithm non-recursive constant-depth clauses. result presented
primarily pedagogical reasons, may interest own: independent
previous proofs pac-learnability class (Dzeroski et al., 1992), also
somewhat rigorous previous proofs.
Although details analysis algorithm non-recursive clauses somewhat involved, basic idea behind algorithm quite simple. First, highlyspecific \bottom clause" constructed, using two operations call DEEPEN
CONSTRAIN . Second, bottom clause generalized deleting literals covers positive examples: algorithm generalizing clause cover example
(roughly) simulate clause example, delete literals would cause
clause fail. remainder section describe analyze learning
algorithm detail.

3.1 Constructing \Bottom Clause"

Let Dec = (p; a0; R) declaration let B1 ^ : : : ^ Br definite clause.
define
^
DEEPEN Dec (A B1 ^ : : : ^ Br ) B1 ^ : : : ^ Br ^ (
Li )
Li 2LD

LD maximal set literals Li satisfy following conditions:
clause B1 ^ : : : ^ Br ^ Li satisfies mode constraints given R;
Li 2 LD mode predicate symbol Lj 2 LD ,
input variables Li different input variables Lj ;
every Li least one output variable, output variables Li
different other, also difference output variables
Lj 2 LD .
extension notation, define DEEPEN iDec (C ) result applying
function DEEPEN Dec repeatedly times C , i.e.,
(
= 0

DEEPEN Dec (C ) C
i,
1
DEEPEN Dec (DEEPEN Dec (C )) otherwise
define function CONSTRAIN Dec
^
CONSTRAIN Dec (A B1 ^ : : : ^ Br ) B1 ^ : : : ^ Br ^ (
Li )
Li 2LC

LC set literals Li B1 ^ : : : ^ Br ^ Li satisfies mode
constraints given R, Li contains output variables.
510

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Example. Let D0 declaration (p; 2; R) R contains mode
constraints mother (+; ,), father (+; ,), male (+), female (+), equal (+; +).



DEEPEN D0(p(X,Y) )
p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)
DEEPEN 2D0(p(X,Y) ) DEEPEN D0 (DEEPEN D0 (p(X,Y) ))
p(X,Y)
mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
mother(XM,XMM)^father(XM,XMF)^ mother(XF,XFM)^father(XF,XFF)^
mother(YM,YMM)^father(YM,YMF)^ mother(YF,YFM)^father(YF,YFF)
CONSTRAIN D0(DEEPEN D0(p(X,Y) ))
p(X,Y)
mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
male(X)^female(X)^male(Y)^female(Y)^
male(XM)^female(XM)^male(XF)^female(XF)^
male(YM)^female(YM)^male(YF)^female(YF)^
equal(X,X)^equal(X,XM)^equal(X,XF)^
equal(X,Y)^equal(X,YM)^equal(X,YF)^
equal(XM,X)^equal(XM,XM)^equal(XM,XF)^
equal(XM,Y)^equal(XM,YM)^equal(XM,YF)^
equal(XF,X)^equal(XF,XM)^equal(XF,XF)^
equal(XF,Y)^equal(XF,YM)^equal(XF,YF)^
equal(Y,X)^equal(Y,XM)^equal(Y,XF)^
equal(Y,Y)^equal(Y,YM)^equal(Y,YF)^
equal(YM,X)^equal(YM,XM)^equal(YM,XF)^
equal(YM,Y)^equal(YM,YM)^equal(YM,YF)^
equal(YF,X)^equal(YF,XM)^equal(YF,XF)^
equal(YF,Y)^equal(YF,YM)^equal(YF,YF)

Let us say clause C1 subclause clause C2 heads C1 C2
identical, every literal body C1 also appears C2 , literals
body C1 appear order C2. functions DEEPEN
CONSTRAIN allow one easily describe clause interesting property.
Theorem 1 Let Dec = (p; a0; R) declaration a-DetDEC =, let X1; : : :; Xa distinct
variables, define clause BOTTOM follows:
BOTTOM (Dec ) CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))
constants a, following true:
size BOTTOM d(Dec) polynomial j Decj ;
every depth-d clause satisfies Dec (and hence, determinate) (semantically)
equivalent subclause BOTTOM (Dec ).
0

0

511

fiCohen

begin algorithm Force1NR (d ; Dec; DB ):

% BOTTOM specific possible clause
let H BOTTOM d(Dec)

repeat

Ans answer query \Is H correct?"
Ans =\yes" return H
elseif Ans negative example
return \no consistent hypothesis"
elseif Ans positive example e+
% generalize H minimally cover e+
let (f; D) components extended instance e+
H ForceSimNR (H ; f ; Dec; (DB [ ))
H = FAILURE
return \no consistent hypothesis"

end

endif
endif
endrepeat

Figure 1: learning algorithm nonrecursive depth-d determinate clauses

Proof: See Appendix A. related result also appears Muggleton Feng (1992).
Example. C1 D1 equivalent, C2 D2. Notice D1
D2 subclauses BOTTOM 1 (D0).

C1 : p(A,B) mother(A,C)^father(A,D)^ mother(B,C)^father(B,D)^male(A)
D1 : p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
male(X)^equal(XM,YM)^equal(XF,YF)
C2 : p(A,B) father(A,B)^female(A)
D2 : p(X,Y) father(X,XF)^female(X)^equal(XF,Y)
C1 D1, p(X,Y) true X 's brother. C2 D2, p(X,Y)
true X 's daughter, X 's father.

3.2 Learning Algorithm

Theorem 1 suggests may possible learn non-recursive constant-depth determinate clauses searching space subclauses BOTTOM ecient
manner. Figures 1 2 present algorithm called Force1 NR Dec
unique-mode declaration.
Figure 1 presents top-level learning algorithm, Force1 NR . Force1 NR takes
input database DB declaration Dec , begins hypothesizing clause
BOTTOM (Dec ). positive counterexample e+ , current hypothesis generalized little possible order cover e+ . strategy means hypothesis
512

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSimNR(H ; f ; Dec; DB ):

% \forcibly simulate" H fact f
f 2 DB return H
elseif head H f cannot unified
return FAILURE

else

let H 0 H
let mgu f head H 0
literal L body H 0
substitution 0 L0 2 DB
0, 0 general substitution
else
delete L body H 0 , together
literals L0 supported (directly indirectly) L

end

endif
endfor
return H 0
endif

Figure 2: Forced simulation nonrecursive depth-d determinate clauses
always least general hypothesis covers positive examples; hence, negative
counterexample e, ever seen, algorithm abort message consistent
hypothesis exists.
minimally generalize hypothesis H , function ForceSimNR used. subroutine shown Figure 2. figure, following terminology used.
output variable L input variable L0 , say L directly supports L0.
say L supports L0 iff L directly supports L0, L directly supports literal
L00 supports L0. (Thus \supports" transitive closure \directly supports".)
ForceSim NR deletes H minimal number literals necessary let H cover e+ .
this, ForceSim NR simulates action Prolog interpreter evaluating H , except
whenever literal L body H would fail, literal deleted, along
literals L0 supported L.
idea learning repeated generalization old one; particular, previous
methods exist learning definite clause generalizing highly-specific one. example, CLINT (De Raedt & Bruynooghe, 1992) generalizes \starting clause" guided
queries made user; PROGOL (Srinivasan, Muggleton, King, & Sternberg, 1994)
guides top-down generalization process known bottom clause; Rouveirol (1994)
describes method generalizing bottom clauses created saturation. Force1 NR algorithm thus interest novelty, provably correct ecient,
noted theorem below.
513

fiCohen

particular, let d-DepthNonRec language nonrecursive clauses depth
less (and hence i-DepthNonRec[DB; j -DetDEC ] language nonrecursive ij determinate clauses). following result:

Theorem 2 constants d, language family
d-DepthNonRec[DB= ; a-DetDEC =1]
uniformly identifiable equivalence queries.

Proof: show Force1 NR uniformly identifies language family polyno-

mial number queries. begin following important lemma, characterizes
behavior ForceSimNR .

Lemma 3 Let Dec declaration DetDEC =1 , let DB database, let f fact, let

H determinate nonrecursive clause satisfies Dec. one following conditions

must hold:
ForceSimNR(H ; f ; Dec; DB ) returns FAILURE, subclause H 0 H satisfies
Dec constraint H 0 ^ DB ` f ; or,
ForceSimNR(H ; f ; Dec; DB ) returns clause H 0, H 0 unique syntactically
largest subclause H satisfies Dec constraint H 0 ^ DB ` f .

Proof lemma: avoid repetition, refer syntactically maximal subclauses
H 0 H satisfy Dec constraint H 0 ^ DB ` f \admissible subclauses"

proof below.
Clearly lemma true H FAILURE returned ForceSim NR . remaining
cases loop algorithm executed, must establish two claims
(under assumptions f unify, f 62 DB ):
Claim 1. L retained, every admissible subclause contains L.
Claim 2. L deleted, admissible subclause contains L.
First, however, observe deleting literal L may cause mode
literals violate mode declarations Dec . easy see L deleted
clause C , mode literals L0 directly supported L change. Thus C
satisfies unique-mode declaration prior deletion L, deletion L
literals L0 directly supported L invalid modes.
Now, see Claim 1 true, suppose instead false. must
maximal subclause C 0 H satisfies Dec , covers fact f ,
contain L. argument above, C 0 contain L satisfied Dec , C 0
contains literals L0 H supported L. Hence output variables L
disjoint variables appearing C 0. means L added
C 0 resulting clause would still satisfy Dec cover f , leads contradiction
since C 0 assumed maximal.
verify Claim 2, let us introduce following terminology. C = (A B1 ^ : : : ^ Br )
clause DB database, say substitution (DB ; f )-witness
514

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

C iff associated proof C ^ DB ` f (or precisely, iff = f
8i : 1 r; Bi 2 DB .) claim following condition invariant
loop ForceSim NR algorithm.
Invariant 1. Let C admissible subclause contains literals H 0 preceding L (i.e., contains literals H retained previous
iterations algorithm). every (DB ; f )-witness C superset .
easily established induction number iterations loop.
condition true loop first entered, since initially general unifier
f . condition remains true iteration L deleted, since
unchanged. Finally, condition remains true iteration L retained:
0 maximally general, may assign values output variables L,
determinacy one assignment output variables L make L true. Hence
every (DB ; f )-witness C must contain bindings .
Next, inductive argument Claim 1 one show every admissible
subclause C must contain literals retained previous iterations
loop, leading following strengthening Invariant 1:
Invariant 10. Let C admissible subclause. every (DB ; f )-witness C
superset .
Now, notice two types literals deleted: (a) literals L superset
make L true, (b) literals L0 supported literal L preceding
type. case (a), clearly L cannot part admissible subclause, since superset
makes L succeed, supersets witnesses admissible clauses.
case (b), L0 cannot part admissible subclause, since declaration invalid
unless L present clause, argument L cannot clause.
concludes proof lemma.
prove theorem, must establish following properties identification
algorithm.
Correctness. Theorem 1, target program d-DepthNonRec[DB ; Dec],
clause CT equivalent target, subclause
BOTTOM (Dec ). H initially BOTTOM hence superclause CT . consider
invoking ForceSim NR positive counterexample e+ . Lemma 3, invocation
successful, H replaced H 0, longest subclause H covers e+ . Since
CT subclause H covers e+ , means H 0 superclause
CT . Inductively, then, hypothesis always superclause target.
Further, since counterexample e+ always instance covered
current hypothesis H , every time hypothesis updated, new hypothesis proper
subclause old. means Force1 NR eventually identify target clause.
Eciency. number queries made polynomial j Decj j DB j , since H
initially size polynomial j Dec j , reduced size time counterexample
provided. see counterexample processed time polynomial nr , ne ,
nt, notice since length H polynomial, number repetitions
loop ForceSim NR also polynomial; further, since arity literals L bounded
515

fiCohen

a, anb + ane constants exist DB [ D, hence (anb + ane )a
substitutions 0 check inside loop, polynomial. Thus execution

ForceSim NR requires polynomial time.
concludes proof.

4. Learning Linear Closed Recursive Clause

Recall clause one recursive literal, clause linear recursive ,
recursive literal contains output variables, clause closed linear
recursive. section, describe Force1 algorithm extended
learn single linear closed recursive clause.2 presenting extension, however,
would first like discuss reasonable-sounding approach that, closer examination, turns
incorrect.

4.1 Remark Recursive Clauses

One plausible first step toward extending Force1 recursive clauses allow recursive
literals hypotheses, treat way literals|that is, include
recursive literals initial clause BOTTOM , delete literals gradually
positives examples received. problem approach simple
way check recursive literal clause succeeds fails particular example.
makes impossible simply run ForceSimNR clauses containing recursive literals.
straightforward (apparent) solution problem assume oracle exists
queried success failure recursive literal. closed recursive
clauses, sucient assume oracle MEMBERCt (DB ; f ) answers
question
DB ^ P ` f ?
Ct unknown target concept, f ground fact, DB database. Given
oracle, one determine closed recursive literal Lr retained
checking MEMBERCT (DB ; Lr ) true. oracle close notion
membership query used computational learning theory.
natural extension Force1NR learning algorithm recursive clauses|in
fact algorithm based similar ideas previously conjectured pac-learn
closed recursive constant-depth determinate clauses (Dzeroski et al., 1992). Unfortunately,
algorithm fail return clause consistent positive counterexample.
illustrate this, consider following example.

Example. Consider using extension Force1NR described learn
following target program:
append(Xs,Ys,Zs)
2. reader may object useful recursive programs always least two clauses|a recursive
clause nonrecursive base case. posing problem learning single recursive clause,
thus assuming non-recursive \base case" target program provided background knowledge,
either background database DB , description atoms extended instances.

516

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

components(Xs,X,Xs1),
components(Zs,Z,Zs1),
X1=Z1,
append(Xs1,Ys,Zs1).
program determinate, depth 1, satisfies following set
declarations:
components(+,,,,).
null(+).
equal(+,+).
odd(+).
append(+,+,+).
assume also database DB defines predicate null true
empty lists, odd true constants 1 3.
see forced simulation fail, consider following positive instance
e = (f; D):

f = append (l12 ; l3 ; l123 )
= f cons(l123,1,l23), cons(l23,2,l3), cons(l3,3,nil),
cons(l12,1,l2), cons(l2,2,nil),
append(nil,l3,l3) g

simply \ attened" form append([1,2],[3],[1,2,3]), together
appropriate base case append([],[3],[3]). consider beginning clause
BOTTOM 1 generalizing using ForceSimNR cover positive instance.
process illustrated Figure 3. clause left figure
BOTTOM (Dec ); clause right output forcibly simulating
clause f ForceSimNR . (For clarity we've assumed
single correct recursive call remains forced simulation.)
resulting clause incorrect, cover given example e.
easily seen stepping actions Prolog interpreter
generalized clause Figure 3. nonrecursive literals succeed, leading subgoal append(l2,l3,l23) (or usual Prolog notation,
append([2],[3],[2,3])). subgoal fail literal odd(X1), X1
bound 2 subgoal, fact odd(2) true DB [ D.
example illustrates pitfall policy treating recursive non-recursive
literals uniform manner (For discussion, see also (Bergadano & Gunetti, 1993; De
Raedt, Lavrac, & Dzeroski, 1993).) Unlike nonrecursive literals, truth fact Lr
(corresponding recursive literal Lr ) imply clause containing Lr
succeed; may first subgoal Lr succeeds, deeper subgoals fail.
517

fiCohen

BOTTOM 1 (Dec ):
ForceSimNR (BOTTOM 1(Dec); f; Dec; DB [ D) :
append(Xs,Ys,Zs)
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^
components(Xs,X1,Xs1)^
components(Ys,Y1,Ys1)^
components(Ys,Y1,Ys1)^
components(Zs,Z1,Zs1)^
components(Zs,Z1,Zs1)^
null(Xs)^
null(Ys1)^
null(Ys)^
equal(X1,Z1)^
..
odd(X1)^
.
odd(Y1)^
null(Ys1)^
odd(Z1)^
null(Zs1),
append(Xs1,Ys,Zs1).
equal(Xs,Xs)^
..
.
equal(X1,Z1)^
..
.
equal(Zs1,Zs1)^
odd(Xs)^
..
.
odd(X1)^
odd(Y1)^
odd(Z1)^
..
.
odd(Zs1)^
append(Xs,Xs,Xs)^
..
.
append(Zs1,Zs1,Zs1).

Figure 3: recursive clause generalization ForceSimNR

4.2 Forced Simulation Recursive Clauses
solution problem replace calls membership oracle algorithm
sketched call routine forcibly simulates actions top-down
theorem-prover recursive clause. particular, following algorithm suggested.
First, build nonrecursive \bottom clause", done ForceSimNR . Second, find
recursive literal Lr appending Lr bottom clause yields recursive clause
generalized cover positive examples.
nonrecursive case, clause generalized deleting literals, using straightforward generalization procedure forced simulation nonrecursive clauses.
forced simulation, failing nonrecursive subgoals simply deleted; however,
recursive literal Lr encountered, one forcibly simulates hypothesis clause recursively
518

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSim (H ; f ; Dec; DB ; h ):

% \forcibly simulate" recursive clause H f
% 1. check infinite loops
h < 0 return FAILURE
% 2. check see f already covered
elseif f 2 DB return H
% 3. check see f cannot covered
elseif head H f cannot unified
return FAILURE

else

let Lr recursive literal H
let H 0 H , fLrg

% 4. delete failing non-recursive literals ForceSimNR
let head H 0
let mgu e
literal L body H 0
substitution 0 L0 2 DB
0, 0 general substitution

else

delete L body H 0 , together
literals L0 supported (directly indirectly) L

endif
endfor

% 5. generalize H 0 recursive subgoal Lr
Lr ground return ForceSim(H 0 [ fLr g; Lr; Dec; DB ; h , 1)
else return FAILURE

end

endif
endif

Figure 4: Forced simulation linear closed recursive clauses

519

fiCohen

corresponding recursive subgoal. implementation forced simulation linear
closed recursive clauses shown Figure 4.
extended algorithm similar ForceSimNR , differs recursive
literal Lr reached simulation H , corresponding subgoal Lr created,
hypothesized clause recursively forcibly simulated subgoal. ensures
generalized clause also succeed subgoal. reasons become clear
shortly, would like algorithm terminate, even original clause H enters
infinite loop used top-down interpreter. order ensure termination, extra
argument h passed ForceSim . argument h represents depth bound forced
simulation.
summarize, basic idea behind algorithm Figure 4 simulate hypothesized clause H f , generalize H deleting literals whenever H would fail
f subgoal f .

Example.

Consider using ForceSim forcibly simulate following recursive clause
BOTTOM 1(Dec ) [ Lr
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Xs)^: : : ^null(Zs1)^
odd(Xs)^: : : ^odd(Zs1)^
equal(Xs,Xs)^: : : ^equal(Zs1,Zs1)^
append(Xs1,Ys,Zs1)
recursive literal Lr append(Xs1,Ys,Zs1). also assume f
taken extended query e = (f; D), attened version
instance append([1,2],[3],[1,2,3]) used previous example; Dec
set declarations previous example; database DB
[ null (nul ).
executing steps 1-4 ForceSim, number failing literals deleted,
leading substitution3 fXs = [1; 2], Ys = [3], Zs = [1; 2; 3], X1 = 1,
Xs1 = [2], Y1 = 3, Ys1 = [], Z1 = 1, Zs1 = [2; 3]g following reduced
clause:
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Ys1)^odd(X1)^odd(Y1)^odd(Z1)^equal(X1,Z1)^
append(Xs1,Ys,Zs1)
Hence recursive subgoal

Lr = append (Xs1 ; Ys ; Zs1 ) = append ([2]; [3]; [2; 3])
3. Note readability, using term notation rather attened notation Xs = l12,
Ys = l3, etc.

520

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Recursively applying ForceSim goal produces substitution fXs = [2],
Ys = [3], Zs = [2; 3], X1 = 2, Xs1 = [], Y1 = 3, Ys1 = [], Z1 = 2, Zs1 = [3]g
also results deleting additional literals odd(X1) odd(Z1).
next recursive subgoal Lr = append ([]; [3]; [3]); since clause included
database DB , ForceSim terminate. final clause returned
ForceSim case following:
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Ys1)^odd(Y1)^equal(X1,Z1)^
append(Xs1,Ys,Zs1)
Notice clause cover e.
Section 3 begin analysis showing correctness forced simulation
algorithm|i.e., showing forced simulation indeed produce unique maximally
specific generalization input clause covers example.
proof correctness uses induction depth proof. Let us introduce
additional notation, write P ^ DB `h f Prolog program (P; DB )
used prove fact f proof depth h less. (The notion depth proof
usual one; define looking f database DB proof depth zero.)
following result concerning ForceSim algorithm.

Theorem 4 Let Dec declaration DetDEC =1, let DB database, let f fact,

let H determinate closed linear recursive clause satisfies Dec. one
following conditions must hold:

ForceSim(H; f; Dec; DB ; h) returns FAILURE, recursive subclause H 0 H
satisfies Dec constraint H 0 ^ DB `h f ; or,
ForceSim(H; f; Dec; DB ; h) returns clause H 0, H 0 unique syntactically
largest recursive subclause H satisfies Dec constraint H 0^DB `h f .

Proof: avoid repetition, refer syntactically maximal recursive (nonrecursive) subclauses H 0 H satisfy Dec constraint H 0 ^ DB `h f

\admissible recursive (nonrecursive) subclauses" respectively.
proof largely parallels proof Lemma 3|in particular, similar arguments
show clause returned ForceSim satisfies conditions theorem whenever
FAILURE returned whenever H returned. Note correctness ForceSim
H returned establishes base case theorem h = 0.
case depth h > 0, let us assume theorem holds depth h , 1
proceed using mathematical induction. arguments Lemma 3 show following
condition true loop terminates.

Invariant 10. H 0 unique maximal nonrecursive admissible subclause H , every
(DB ; f )-witness H 0 superset .
521

fiCohen

begin algorithm Force1 (d ; Dec; DB ):

% BOTTOM specific possible clause
let Lr1 ; : : :; Lrp possible closed recursive literals BOTTOM d(Dec)
choose unmarked recursive literal Lri
let H BOTTOM d(Dec) [ fLri g

repeat

answer query \Is H correct?"

Ans

Ans =\yes" return H
elseif Ans negative example e,
H

FAILURE

elseif Ans positive example e+

% generalize H minimally cover e+
let (f; D) components e+
H ForceSim (H ; f ; Dec; (DB [ ); (a j Dj + j DBj )a )
a0 arity clause head given Dec
0

endif
H = FAILURE
recursive literals marked
return \no consistent hypothesis"
else

mark Lri
choose unmarked recursive literal Lrj
let H BOTTOM d(Dec) [ fLrj g

end

endif
endif
endrepeat

Figure 5: learning algorithm nonrecursive depth-d determinate clauses
Now, let us assume admissible recursive subclause H . Clearly H must
contain recursive literal Lr H , since Lr recursive literal H . Further,
nonrecursive clause H^ = H , fLr g must certainly satisfy Dec also H^ ^ DB ` f ,
must (by maximality H 0) subclause H 0. Hence H must subclause
H 0 [ fLr g. Finally, Lr ground (i.e., Lr closed clause H 0 [ Lr )
Invariant 10, clause H must also satisfy H ^ DB ` Lr proof depth h , 1.
(This simply equivalent saying recursive subgoal Lr generated proof
must succeed.)
inductive hypothesis, then, recursive call must return unique maximal
admissible recursive subclause H 0 [ Lr , argument must also
unique maximal admissible recursive subclause H .
Thus induction theorem holds.
522

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

4.3 Learning Algorithm Linear Recursive Clauses

Given method generalizing recursive clauses, one construct learning algorithm recursive clauses follows. First, guess recursive literal Lr , make
H = BOTTOM [ Lr initial hypothesis learner. Then, ask series equivalence
queries. positive counterexample e+ , use forced simulation minimally generalize
H cover e+ . negative example, choose another recursive literal L0r , reset
hypothesis H = BOTTOM [ L0r .
Figure 5 presents algorithm operates along lines. Let d-DepthLinRec
denote language linear closed recursive clauses depth less.
following result:
Theorem 5 constants d, language family
d-DepthLinRec[DB=; a-DetDEC =1]
uniformly identifiable equivalence queries.
Proof: show Force1 uniformly identifies language family polynomial number queries.
Correctness query eciency. aj Dj + aj DBj constants
set DB [ D, (aj Dj + aj DB j )a a0 -tuples constants, hence
(aj Dj + aj DB j )a distinct recursive subgoals Lr might produced proving
linear recursive clause C covers extended instance (f; D). Thus every terminating proof
fact f using linear recursive clause C must depth (aj Dj + aj DB j )a less; i.e.,
h = (aj Dj + aj DB j )a ,
C ^ DB ^ `h f iff C ^ DB ^ ` f
Thus Theorem 4 strengthened: value h used Force1, subroutine
ForceSim returns syntactically largest subclause H covers example (f; D)
whenever subclause exists, returns FAILURE otherwise.
argue correctness algorithm follows. Assume hypothesized recursive literal \correct"|i.e., target clause CT subclause
BOTTOM [ Lr . case easy see Force1 identify CT , using argument parallels one made Force1 NR . analogy Force1 NR , easy
see polynomial number equivalence queries made involving correct
recursive literal.
Next assume Lr correct recursive literal. CT need subclause
BOTTOM [ Lr , response equivalence query may either positive
negative counterexample. positive counterexample e+ received ForceSim
called, result may FAILURE, may proper subclause H covers
e+ . Thus result choosing incorrect Lr (possibly empty) sequence
positive counterexamples followed either negative counterexample FAILURE. Since
equivalence queries involving correct recursive literal answered either
positive counterexample \yes"4, negative counterexample FAILURE
obtained, must Lr incorrect.
0

0

0

0

4. Recall answer \yes" equivalence query means hypothesis correct.

523

fiCohen

number variables BOTTOM bounded aj BOTTOM (Dec )j ,
closed recursive literal completely defined a0-tuple variables, number
possible closed recursive literals Lr bounded

p = (aj BOTTOM (Dec )j )a

0

Since j BOTTOM (Dec )j polynomial j Dec j , p also polynomial j Dec j . means
polynomial number incorrect Lr 's need discarded. since
successive hypothesis using single incorrect Lr proper subclause previous hypothesis, polynomial number equivalence queries needed discard incorrect
Lr . Thus polynomial number equivalence queries made involving incorrect
recursive literals.
Thus Force1 needs polynomial number queries identify Ct.
Eciency. ForceSim runs time polynomial arguments H , f , Dec, DB [
h. ForceSim called Force1, h always polynomial ne j DB j ,
H always larger j BOTTOM d(Dec)j + 1, turn polynomial size
Dec . Hence every invocation ForceSim requires time polynomial ne , Dec , DB ,
hence Force1 processes query polynomial time.
completes proof.
result somewhat surprising, shows recursive clauses learned
even given adversarial choice training examples. contrast, implemented ILP
systems require well-choosen examples learn recursive clauses.
formal result also strengthened number technical ways. One
interesting strengthenings consider variant Force1 maintains
fixed set positive negative examples, constructs set least general
clauses consistent examples: could done taking
clauses BOTTOM [ Lr1 , : : : , BOTTOM [ Lrp , forcibly simulating
positive examples turn, discarding clauses cover one negative
examples. set clauses could used tractably encode version space
consistent programs, using [S; N ] representation version spaces (Hirsh, 1992).

5. Extending Learning Algorithm

consider number ways result Theorem 5 extended.

5.1 Equality-Predicate Unique-Mode Assumptions
Theorem 5 shows language family

d-DepthLinRec[DB=; a-DetDEC =1]
identifiable equivalence queries. natural ask result extended
dropping assumptions equality predicate present declaration
contains unique legal mode predicate: is, result extended
language family
d-DepthLinRec[DB; a-DetDEC ]
524

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

extension fact straightforward. Given database DB declaration Dec =
(p; a0; R) satisfy equality-predicate unique-mode assumptions, one
modify follows.
1. every constant c appearing DB , add fact equal (c ; c ) DB .
2. every predicate q k valid modes qs1 , : : : , qsk R:
(a) remove mode declarations q , replace k mode strings
k new predicates qs1 , : : : , qsk , letting qsi si unique legal mode
predicate qsi ;
(b) remove every fact q (t1 ; : : :; ta) predicate q DB , replace
k facts qs1 (t1 ; : : :; ta ), : : : , qsk (t1 ; : : :; ta).
Note arity predicates bounded constant a, number modes
k predicate q bounded constant 2a , hence transformations
performed polynomial time, polynomial increase size Dec
DB .
Clearly target clause Ct 2 d-DepthLinRec[DB ; Dec ] equivalent clause
Ct0 2 d-DepthLinRec[DB 0; Dec0], DB 0 Dec 0 modified versions DB
Dec constructed above. Using Force1 possible identify Ct0 . (In learning Ct0, one
must also perform steps 1 2b description part every counterexample
(f; D).) Finally, one convert Ct0 equivalent clause d-DepthLinRec[DB ; Dec]
repeatedly resolving clause equal(X,X) , also replacing every predicate
symbol qsi q .
leads following strengthening Theorem 5:

Proposition 6 constants d, language family
d-DepthLinRec[DB; a-DetDEC ]
uniformly identifiable equivalence queries.

5.2 Datalog Assumption

far assumed target program contains function symbols,
background knowledge provided user database ground facts. convenient
formal analysis, assumptions relaxed.
Examination learning algorithm shows database DB used two
ways.

forcibly simulating hypothesis extended instance (f; D), necessary
find substitution 0 makes literal L true database DB [ D.
done algorithmically DB sets ground facts, also plausible
assume user provided oracle answers polynomial time
mode-correct query L database DB . Specifically, answer oracle
either
525

fiCohen

{ (unique) most-general substitution 0 DB ^ ` L0 L0
ground;
{ \no" 0 exists.

oracle would presumably take form ecient theorem-prover DB .

calling ForceSim, top-level learning algorithm uses DB determine

depth bound length proof made using hypothesis program. Again,
reasonable assume user provide information directly,
form oracle. Specifically, oracle would provide fact f polynomial
upper bound depth proof f target program.

Finally note ecient (but non-ground) background knowledge allowed,
function symbols always removed via attening (Rouveirol, 1994). transformation also preserves determinacy, although may increase depth|in general, depth
attened clause depends also term depth original clause. Thus, assumption
target program Datalog replaced assumptions term depth
bounded constant, two oracles available: oracle answers queries
background knowledge, depth-bound oracle. types oracles
frequently assumed literature (Shapiro, 1982; Page & Frisch, 1992; Dzeroski et al.,
1992).

5.3 Learning k-ary Recursive Clauses

also natural ask Theorem 5 extended clauses linear recursive.
One interesting case case closed k-ary recursive clauses constant k.
straightforward extend Force1 guess tuple k recursive literals Lr1 , : : : , Lrk ,
extend ForceSim recursively generalize hypothesis clause facts
Lr1 , : : : , Lrk . arguments Theorems 4 5 modified show
extension identify target clause polynomial number equivalence queries.
Unfortunately, however, longer case ForceSim runs polynomial time.
easily seen one considers tree recursive calls made ForceSim;
general, tree branching factor k polynomial depth, hence exponential
size. result unsurprising, implementation ForceSim described forcibly
simulates depth-bounded top-down interpreter, k-ary recursive program take
exponential time interpret interpreter.
least two possible solutions problem. One possible solution
retain simple top-down forced simulation procedure, require user provide
depth bound tighter (aj Dj + aj DB j )a , maximal possible depth tree.
example, learning 2-ary recursive sort quicksort, user might specify logarithmic depth bound, thus guaranteeing ForceSim polynomial-time. requires
additional input user, would easy implement. also advantage
(not shared approach described below) hypothesized program executed using simple depth-bounded Prolog interpreter, always shallow proof
trees. seems plausible bias impose learning k-ary recursive Prolog
programs, many tend shallow proof trees.
0

526

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

second solution possible high cost forced simulation k-ary recursive
programs forcibly simulate \smarter" type interpreter|one execute
k-ary recursive program polynomial time.5 One sound complete theorem-prover
closed k-ary recursive programs implemented follows.
Construct top-down proof tree usual fashion, i.e., using depth-first left-to-right
strategy, maintain list ancestors current subgoal, also list VISITED
records, previously visited node tree, subgoal associated
node. Now, suppose course constructing proof tree one generates subgoal
f VISITED list. Since traversal tree depth-first left-to-right,
node associated f either ancestor current node, descendant
left sibling ancestor current node. former case, proof tree contains
loop, cannot produce successful proof; case theorem-prover exit
failure. latter case, proof must already exist f 0 , hence nodes
current node tree need visited; instead theorem prover simply assume
f true.
top-down interpreter easily extended forced simulation procedure:
one simply traverses tree order, generalizing current hypothesis H
needed justify inference step tree. additional point note
one performing forced simulation revisits previously proved subgoal f node
n, current clause H need generalized order prove f , hence
permissible simply skip portion tree n. thus following
result.

Theorem 7 Let d-Depth-k-Rec set k-ary closed recursive clauses depth d.
constants a, d, k language family
d-Depth-k-Rec[DB; a-DetDEC]
uniformly identifiable equivalence queries.

Proof: Omitted, following informal argument made above.
Note give result without restrictions database contains
equality relation declaration unique-mode, since tricks used relax
restrictions Proposition 6 still applicable.

5.4 Learning Recursive Base Cases Simultaneously

far, analyzed problem learning single clauses: first single nonrecursive
clause, single recursive clause. However, every useful recursive program contains
least two clauses: recursive clause, nonrecursive base case. natural ask
possible learn complete recursive program simultaneously learning
recursive clause, associated nonrecursive base case.
general, possible, demonstrated elsewhere (Cohen, 1995). However,
several cases positive result extended two-clause programs.
5. Note plausible believe theorem-prover exists, polynomial
number possible theorem-proving goals|namely, (aj Dj + aj DB j )a possible recursive subgoals.
0

527

fiCohen

begin algorithm Force2 (d ; Dec; DB ):
let Lr1 ; : : :; Lrp possible recursive literals BOTTOM d(Dec)
choose unmarked recursive literal Lri
let HR BOTTOM d(Dec) [ fLri g
let HB BOTTOM d(Dec)
let P = (HR; Hb)

repeat

Ans answer query \Is HR ; HB correct?"
Ans =\yes" return HR ; HB
elseif Ans negative example e,
P FAILURE
elseif Ans positive example e+
let (f; D) components e+
P ForceSim2 (HR; HB ; f ; Dec; (DB [ ); (a j Dj + j DBj )a )
0

endif
P = FAILURE
recursive literals Lrj marked
return \no consistent hypothesis"
else

mark Lri
choose unmarked recursive literal Lrj
let HR BOTTOM d(Dec) [ fLrj g
let HB BOTTOM (Dec)
let P = (HR ; HB )

end

endif
endif
endrepeat

Figure 6: learning algorithm two-clause recursive programs

528

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSim2 (HR; HB ; f ; Dec; DB ; h ):

% \forcibly simulate" program HR ; HB f
h < 1 return FAILURE
% check see f covered HB
elseif BASECASE (f )
return current Hr generalized HB
return (HR; ForceSimNR(HB ; f ; Dec; DB ))
elseif head HR f cannot unified
return FAILURE

else

let Lr recursive literal HR
let H 0 H , fLrg
let head H 0
let mgu e
literal L body H 0
substitution 0 L0 2 DB
0, 0 general substitution
else
delete L body H 0 , together
literals L0 supported (directly indirectly) L

endif
endfor

% generalize H 0; HB recursive subgoal Lr
Lr ground
% continue simulation program
return ForceSim2(H 0 [ fLr g; HB; Lr; Dec; DB ; h , 1)
else return FAILURE

end

endif
endif

Figure 7: Forced simulation two-clause recursive programs

529

fiCohen

section, first discuss learning recursive clause base clause simultaneously, assuming determinate base clause possible, also assuming
additional \hint" available, form special \basecase" oracle.
discuss various alternative types \hints".
Let P target program base clause CB recursive clause CR. basecase
oracle P takes input extended instance (f; D) returns \yes" CB ^ DB ^ ` f ,
\no" otherwise. words, oracle determines f covered nonrecursive
base clause alone. example, append program, basecase oracle return
\yes" instance append(Xs,Ys,Zs) Xs empty list, \no" otherwise.
Given existence basecase oracle, learning algorithm extended
follows. before, possible recursive literals Lri clause BOTTOM generated;
however, case, learner test two clause hypotheses initially
form (BOTTOM [ Lri ; BOTTOM ). forcibly simulate hypothesis fact f ,
following procedure used. checking usual termination conditions, forced
simulator checks see BASECASE(f) true. so, calls ForceSimNR (with appropriate
arguments) generalize current hypothesis base case. BASECASE(f)
false, recursive clause Hr forcibly simulated f , subgoal Lr generated
before, generalized program recursively forcibly simulated subgoal.
Figures 6 7 present learning algorithm Force2 two clause programs consisting
one linear recursive clause CR one nonrecursive clause CB , assumption
equivalence basecase oracles available.
straightforward extend arguments Theorem 5 case, leading
following result.

Theorem 8 Let d-Depth-2-Clause set 2-clause programs consisting one

clause d-DepthLinRec one clause d-DepthNonRec. constants
language family

d-Depth-2-Clause[DB; a-DetDEC ]
uniformly identifiable equivalence basecase queries.

Proof: Omitted.
companion paper (Cohen, 1995) shows something like basecase oracle
necessary: particular, without \hints" base clause, learning two-clause
linear recursive program hard learning boolean DNF. However, several
situations basecase oracle dispensed with.
Case 1. basecase oracle replaced polynomial-sized set possible base
clauses. learning algorithm case enumerate pairs base clauses CBi
\starting clauses" BOTTOM [ Lrj , generalize starting clause forced
simulation, mark pair incorrect overgeneralization detected.
Case 2. basecase oracle replaced fixed rule determines base
clause applicable. example, consider rule says base clause
applicable atom p(X1; : : :; Xa) Xi non-null list. Adopting
530

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

rule leads immediately learning procedure pac-learns exactly
two-clause linear recursive programs rule correct.
Case 3. basecase oracle also replaced polynomial-sized set rules
determining base clause applicable. learning algorithm case
pick unmarked decision rule run Force2 using rule basecase oracle.
Force2 returns \no consistent hypothesis" decision rule marked incorrect,
new one choosen. algorithm learn two-clause linear recursive
programs given decision rules correct.
Even though general problem determining basecase decision rule arbitrary
Datalog program may dicult, may small number decision procedures
apply large number common Prolog programs. example, recursion
list-manipulation programs halts argument reduced null list
singleton list. Thus Case 3 seems likely cover large fraction automatic
logic programming programs practical interest.
also note heuristics proposed finding basecase decision rules
automatically using typing restrictions (Stahl, Tausend, & Wirth, 1993).

5.5 Combining Results

Finally, note extensions described compatible. means
let kd-MaxRecLang language two-clause programs consisting one
clause CR k-ary closed recursive depth-d determinate, one clause CB
nonrecursive depth-d determinate, following holds.

Proposition 9 constants a, k language family
kd-MaxRecLang[DB; a-DetDEC ]
uniformly identifiable equivalence basecase queries.
5.5.1 Extensions
notation kd-MaxRecLang may seem point unjustified; although

expressive language recursive clauses proven learnable,
numerous extensions may eciently learnable. example, one might generalize
language allow arbitrary number recursive clauses, include clauses
determinate. generalizations might well pac-learnable|given results
presented far.
However, companion paper (Cohen, 1995) presents series negative results showing
natural generalizations kd-MaxRecLang eciently learnable,
kd-MaxRecLang eciently learnable without basecase oracle. Specifically, companion paper shows eliminating basecase oracle leads
problem hard learning boolean DNF, open problem computational
learning theory. Similarly, learning two linear recursive clauses simultaneously hard
learning DNF, even base case known. Finally, following learning problems
hard breaking certain (presumably) secure cryptographic codes: learning n
531

fiCohen

linear recursive determinate clauses, learning one n-ary recursive determinate clause,
learning one linear recursive \k-local" clause. negative results hold
model identification equivalence queries, also weaker models
pac-learnability pac-predictability.

6. Related Work
discussing related work concentrate previous formal analyses employ
learning model similar considered here: namely, models (a) require computation polynomial natural parameters problem, (b) assume either neutral
source adversarial source examples, equivalence queries stochastically presented examples. note, however, much previous formal work exists relies
different assumptions. instance, much work member subset
queries allowed (Shapiro, 1982; De Raedt & Bruynooghe, 1992), examples
choosen non-random manner helpful learner (Ling, 1992; De Raedt
& Dzeroski, 1994). also work eciency requirements
imposed pac-learnability model relaxed (Nienhuys-Cheng & Polman, 1994).
requirement eciency relaxed far enough, general positive results obtained using simple learning algorithms. example, model learnability
limit (Gold, 1967), language recursively enumerable decidable (which
includes Datalog) learned simple enumeration procedure; model
U-learnability (Muggleton & Page, 1994) language polynomially enumerable
polynomially decidable learned enumeration.
similar previous work Frazier Page (1993a, 1993b). analyze
learnability equivalence queries recursive programs function symbols
without background knowledge. positive results provide program classes
satisfy following property: given set positive examples + requires
clauses target program prove instances + , polynomial number
recursive clauses possible; base clause must certain highly constrained
form. Thus concept class \almost" bounded size polynomial. learning
algorithm program class interleave series equivalence queries
test every possible target program. contrast, positive results exponentially
large classes recursive clauses. Frazier Page also present series negative results
suggesting learnable languages analyzed dicult generalize without
sacrificing ecient learnability.
Previous results also exist pac-learnability nonrecursive constant-depth determinate programs, pac-learnability recursive constant-depth determinate
programs model also allows membership subset queries (Dzeroski et al.,
1992).
basis intelligent search used learning algorithms technique
forced simulation . method finds least implicant clause C covers
extended instance e. Although developed method believed
original, subsequently discovered case|an identical technique
previously proposed Ling (1991). Since extended instance e converted
(via saturation) ground Horn clause, also close connection forced
532

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

simulation recent work \inverting implication" \recursive anti-unification";
instance, Muggleton (1994) describes nondeterministic procedure finding clauses
imply clause C , Idestam-Almquist (1993) describes means constraining
implicant-generating procedure produce least common implicant two clauses.
However, techniques obvious applications learning,
extremely expensive worst case.
CRUSTACEAN system (Aha et al., 1994) uses inverting implication constrained
settings learn certain restricted classes recursive programs. class programs
eciently learned system formally well-understood, appears
similar classes analyzed Frazier Page. Experimental results show
systems perform well inferring recursive programs use function symbols certain
restricted ways. system cannot, however, make use background knowledge.
Finally, wish direct reader several pieces research relevant. noted above, companion paper exists presents negative learnability results
several natural generalizations language kd-MaxRecLang (Cohen, 1995). Another related paper investigates learnability non-recursive Prolog programs (Cohen,
1993b); paper also contains number negative results strongly motivate
restriction constant-depth determinacy. final prior paper may interest
presents experimental results Prolog implementation variant Force2
algorithm (Cohen, 1993a). paper shows forced simulation basis
learning program outperforms state-of-the art heuristic methods FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993) learning randomly chosen examples.

7. Conclusions
often desirable guarantees correctness program, many
plausible contexts would highly desirable automatic programming system
offer formal guarantees correctness. topic paper learnability
recursive logic programs using formally well-justified algorithms. specifically,
concerned development algorithms provably sound ecient
learning recursive logic programs equivalence queries. showed one constantdepth determinate closed k-ary recursive clause identifiable equivalent queries;
implies immediately language also learnable Valiant's (1984) model paclearnability. also showed program consisting one recursive clause
one constant-depth determinate nonrecursive clause identifiable equivalence queries
given additional \basecase oracle", determines positive example covered
non-recursive base clause target program alone.
obtaining results, introduced several new formal techniques analyzing learnability recursive programs. also shown soundness
eciency several instances generalization forced simulation . method may
applications practical learning systems. Force2 algorithm compares quite well experimentally modern ILP systems learning problems restricted class
identify (Cohen, 1993a); thus sound learning methods like Force2 might useful
filter general ILP system like FOIL (Quinlan, 1990; Quinlan & CameronJones, 1993). Alternatively, forced simulation could used heuristic programs.
533

fiCohen

example, although forced simulation programs many recursive clauses nondeterministic hence potentially inecient, one could introduce heuristics would make
forced simulation ecient, cost completeness.
companion paper (Cohen, 1995) shows positive results paper
likely improved: either eliminating basecase oracle language
learning two recursive clauses simultaneously hard learning DNF, learning n
linear recursive determinate clauses, one n-ary recursive determinate clause, one linear
recursive \k-local" clause hard breaking certain cryptographic codes. positive results paper, negative results establish boundaries learnability
recursive programs function-free pac-learnability model. results thus
give prescription building formally justified system learning recursive programs;
taken together, also provide upper bounds one hope achieve
ecient, formally justified system learns recursive programs random examples
alone.

Appendix A. Additional Proofs

Theorem 1 states: Let Dec = (p; a0; R) declaration 2 a-DetDEC = , let nr = j Rj , let
X1; : : :; Xa distinct variables, define clause BOTTOM follows:
0

BOTTOM (Dec ) CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))
0

constants a, following true:

size BOTTOM d(Dec) polynomial nr ;
every depth-d clause satisfies Dec equivalent subclause
BOTTOM (Dec ).

Proof: Let us first establish polynomial bound size BOTTOM d. Let C
clause size n. number variables C bounded an, size set LD

bounded

Thus clause C
similar argument

nr
|{z}

(|an{z)a,1}

(# modes) (# tuples input variables)

j DEEPEN Dec (C )j n + (an)a,1nr

(1)

j CONSTRAIN Dec (C )j n + (an)anr

(2)

Since functions DEEPEN Dec CONSTRAIN Dec give outputs polynomially larger size inputs, follows composing functions constant
number times, done computing BOTTOM constant d, also produce
polynomial increase size.
Next, wish show every depth-d determinate clause C satisfies Dec
equivalent subclause BOTTOM . Let C depth-d determinate clause,
534

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

without loss generality let us assume pair literals Li Lj body
C mode, predicate symbol, sequence input variables.6
Given C , let us define substitution C follows:
1. Initially set
C fX1 = X1; : : :; Xa = Xa g
X1; : : :; Xa arguments head BOTTOM X1 ; : : :; Xa
arguments head C .
Notice variables head BOTTOM distinct, mapping
well-defined.
2. Next, examine literals body C left-to-right order.
literal L, let variables T1; : : :Tk input variables. literal L
body BOTTOM mode predicate symbol whose input variables
T1; : : :; Tk 8i : 1 r; TjC = Tj , modify C follows:
0

0

0

0

C [ fU1 = U1 ; : : :; Ul = Ul g
U1 ; : : :; Ul output variables L U1; : : :; Ul output variables
L .
Notice assume C contains one literal L given predC

icate symbol sequence input variables, output variables
literals L BOTTOM distinct, mapping well-defined. also
easy verify (by induction length C ) executing procedure
variable BOTTOM always mapped input variable Ti , least
one L meeting requirements exists. Thus mapping C onto
variables appearing C .7
Let head BOTTOM , consider clause C 0 defined follows:
head C 0 A.
body C 0 contains literals L body BOTTOM either
{ LC body C
{ L literal equal (Xi; Xj) XiC = XjC .
claim C 0 subclause BOTTOM equivalent C . Certainly C 0
subclause BOTTOM . One way see equivalent C consider
clause C^ substitution ^C generated follows. Initially, let C^ = C 0
let ^C = C . Then, every literal L = equal (Xi; Xj) body C^ , delete L
^ ij replace ^C (^C )ij , ij
C^ , finally replace C^ C
substitution fXi = Xij ; Xj = Xij g Xij new variable previously appearing
6. assumption made without loss generality since determinate clause C , output
variables Li Lj necessarily bound values, hence Li Lj could unified
together one deleted without changing semantics C .
7. Recall function f : X onto range 8y 2 9x 2 X : f (x) = y.

535

fiCohen

C^ . (Note: (^C )ij refer substitution formed replacing every occurrence
Xi Xj appearing ^C Xij .) C^ semantically equivalent C 0
operation described equivalent simply resolving possible L body
C 0 clause \equal(X,X) ".
following straightforward verify:
^C one-to-one mapping.
see true, notice every pair assignments Xi = Xj =
C must literal equal (Xi; Xj) C 0. Hence following process
described assignments Xi = Xj = ^C would eventually
replaced Xij = Xij = .

^C onto variables C .
Notice C onto variables C , every assignment Xi = C
assignment ^C right-hand side (and assignment
either form Xi = Xij = ). Thus ^C also onto variables C .
literal L^ body C^ iff L^^C body C .
follows definition C 0 fact every literal L
C 0 form equal (Xi; Xj) corresponding literal C^ .
Thus C^ alphabetic variant C , hence equivalent C . Since C^ also equivalent
C 0, must C 0 equivalent C , proves claim.

Acknowledgements
author wishes thank three anonymous JAIR reviewers number useful suggestions presentation technical content.

References
Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication small
training sets. Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture
Notes Computer Science # 784.
Angluin, D. (1988). Queries concept learning. Machine Learning, 2 (4).
Angluin, D. (1989). Equivalence queries approximate fingerprints. Proceedings
1989 Workshop Computational Learning Theory Santa Cruz, California.
Bergadano, F., & Gunetti, D. (1993). interactive system learn functional logic programs. Proceedings 13th International Joint Conference Artificial Intelligence Chambery, France.
536

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Biermann, A. (1978). inference regular lisp programs examples. IEEE Transactions Systems, Man Cybernetics, 8 (8).
Cohen, W. W. (1993a). pac-learning algorithm restricted class recursive logic
programs. Proceedings Tenth National Conference Artificial Intelligence
Washington, D.C.
Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. appear Artificial
Intelligence.
Cohen, W. W. (1993c). Rapid prototyping ILP systems using explicit bias. Proceedings
1993 IJCAI Workshop Inductive Logic Programming Chambery, France.
Cohen, W. W. (1994). Pac-learning nondeterminate clauses. Proceedings Eleventh
National Conference Artificial Intelligence Seattle, WA.
Cohen, W. W. (1995). Pac-learning recursive logic programs: negative results. Journal
AI Research, 2, 541{573.
De Raedt, L., & Bruynooghe, M. (1992). Interactive concept-learning constructive
induction analogy. Machine Learning, 8 (2).
De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories PAC-learnable.
Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive
Logic Programming Bad Honnef/Bonn, Germany.
De Raedt, L., Lavrac, N., & Dzeroski, S. (1993). Multiple predicate learning. Proceedings
Third International Workshop Inductive Logic Programming Bled, Slovenia.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability determinate logic
programs. Proceedings 1992 Workshop Computational Learning Theory
Pittsburgh, Pennsylvania.
Frazier, M., & Page, C. D. (1993a). Learnability inductive logic programming:
basic results techniques. Proceedings Tenth National Conference
Artificial Intelligence Washington, D.C.
Frazier, M., & Page, C. D. (1993b). Learnability recursive, non-determinate theories:
basic results techniques. Proceedings Third International Workshop
Inductive Logic Programming Bled, Slovenia.
Gold, M. (1967). Language identification limit. Information Control, 10.
Hirsh, H. (1992). Polynomial-time learning version spaces. Proceedings Tenth
National Conference Artificial Intelligence San Jose, California. MIT Press.
Idestam-Almquist, P. (1993). Generalization implication recursive anti-unification.
Proceedings Ninth International Conference Machine Learning Amherst,
Massachusetts. Morgan Kaufmann.
537

fiCohen

King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug design
machine learning: use inductive logic programming model structureactivity relationships trimethoprim analogues binding dihydrofolate reductase.
Proceedings National Academy Science, 89.
Lavrac, N., & Dzeroski, S. (1992). Background knowledge declarative bias inductive
concept learning. Jantke, K. P. (Ed.), Analogical Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. Lectures
Artificial Intelligence Series #642.
Ling, C. (1991). Inventing necessary theoretical terms scientific discovery inductive
logic programming. Tech. rep. 301, University Western Ontario.
Ling, C. (1992). Logic program synthesis good examples. Inductive Logic Programming. Academic Press.
Lloyd, J. W. (1987). Foundations Logic Programming: Second Edition. Springer-Verlag.
Muggleton, S. (1994). Inverting implication. appear Artificial Intelligence.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.
Journal Logic Programming, 19/20 (7), 629{679.
Muggleton, S., & Feng, C. (1992). Ecient induction logic programs. Inductive Logic
Programming. Academic Press.
Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structure
prediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S., & Page, C. D. (1994). learnability model universal representations.
Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive
Logic Programming Bad Honnef/Bonn, Germany.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.
Nienhuys-Cheng, S., & Polman, M. (1994). Sample pac-learnability model inference.
Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture notes
Computer Science # 784.
Page, C. D., & Frisch, A. M. (1992). Generalization learnability: study constrained
atoms. Inductive Logic Programming. Academic Press.
Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. Machine
Learning, 9 (1).
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Brazdil, P. B.
(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notes
Computer Science # 667.
Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5 (3).
538

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Quinlan, J. R. (1991). Determinate literals inductive logic programming. Proceedings
Eighth International Workshop Machine Learning Ithaca, New York. Morgan
Kaufmann.
Rouveirol, C. (1994). Flattening saturation: two representation changes generalization. Machine Learning, 14 (2).
Shapiro, E. (1982). Algorithmic Program Debugging. MIT Press.
Srinivasan, A., Muggleton, S. H., King, R. D., & Sternberg, M. J. E. (1994). Mutagenesis:
ILP experiments non-determinate biological domain. Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive Logic Programming Bad
Honnef/Bonn, Germany.
Stahl, I., Tausend, B., & Wirth, R. (1993). Two methods improving inductive logic
programming. Proceedings 1993 European Conference Machine Learning
Vienna, Austria.
Summers, P. D. (1977). methodology LISP program construction examples.
Journal Association Computing Machinery, 24 (1), 161{175.
Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11).
Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers treebanks:
machine learning approach. Proceedings Twelfth National Conference
Artificial Intelligence Seattle, Washington. MIT Press.

539

fiJournal Artificial Intelligence Research 2 (1995) 575-609

Submitted 12/94; published 5/95

Provably Bounded-Optimal Agents
Stuart J. Russell

Computer Science Division, University California
Berkeley, CA 94720, USA

Devika Subramanian

Computer Science Department, Cornell University
Ithaca, NY 14853, USA

russell@cs.berkeley.edu
devika@cs.cornell.edu

Abstract

Since inception, artificial intelligence relied upon theoretical foundation centred around perfect rationality desired property intelligent systems. argue,
others done, foundation inadequate imposes fundamentally
unsatisfiable requirements. result, arisen wide gap theory
practice AI, hindering progress field. propose instead property called bounded
optimality. Roughly speaking, agent bounded-optimal program solution
constrained optimization problem presented architecture task environment. show construct agents property simple class machine
architectures broad class real-time environments. illustrate results using
simple model automated mail sorting facility. also define weaker property,
asymptotic bounded optimality (ABO), generalizes notion optimality classical
complexity theory. construct universal ABO programs, i.e., programs
ABO matter real-time constraints applied. Universal ABO programs
used building blocks complex systems. conclude discussion
prospects bounded optimality theoretical basis AI, relate similar trends
philosophy, economics, game theory.

1. Introduction

Since beginning artificial intelligence, philosophers, control theorists
economists looked satisfactory definition rational behaviour. needed
underpin theories ethics, inductive learning, reasoning, optimal control, decision-making,
economic modelling. Doyle (1983) proposed AI defined computational study rational behaviour|effectively equating rational behaviour intelligence. role definitions AI ensure theory practice correctly
aligned. define property P , hope able design system
provably possesses property P . Theory meets practice systems exhibit P reality. Furthermore, exhibit P reality something actually care
about. sense, choice P study determines nature field.
number possible choices P :
Perfect rationality: classical notion rationality economics philosophy.
perfectly rational agent acts every instant way maximize
expected utility, given information acquired environment. Since
action selection requires computation, computation takes time, perfectly rational
agents exist non-trivial environments.
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiRussell & Subramanian

Calculative rationality: notion rationality studied AI. calculatively rational

agent eventually returns would rational choice beginning
deliberation. exist systems uence diagram evaluators exhibit
property decision-theoretic definition rational choice, systems
nonlinear planners exhibit logical definition rational choice.
assumed interesting property system exhibit since constitutes
\in-principle" capacity right thing. Calculative rationality limited
value practice, actual behaviour exhibited systems absurdly
far rational; example, calculatively rational chess program choose
right move, may take 1050 times long so. result, AI systembuilders often ignore theoretical developments, forced rely trial-and-error
engineering achieve goals. Even simple domains chess, little
theory designing analysing high-performance programs.
Metalevel rationality: natural response problems calculative rationality.
metalevel rational system optimizes object-level computations performed service selecting actions. words, decision finds
optimal combination computation-sequence-plus-action, constraint
action must selected computation. Full metalevel rationality
seldom useful metalevel computations take time, metalevel decision problem often dicult object-level problem. Simple
approximations metalevel rationality proved useful practice|for example, metalevel policies limit lookahead chess programs|but engineering
expedients merely serve illustrate lack theoretical basis agent design.
Bounded optimality: bounded optimal agent behaves well possible given
computational resources. Bounded optimality specifies optimal programs rather
optimal actions optimal computation sequences. former approach
avoid placing constraints intelligent agents cannot met
program. Actions computations are, all, generated programs,
programs designers control.
make three claims:
1. system exhibits bounded optimality desirable reality.
2. possible construct provably bounded optimal programs.
3. Artificial intelligence usefully characterized study bounded optimality,
particularly context complex task environments reasonably powerful
computing devices.
first claim unlikely controversial. paper supports second claim
detail. third claim may, may not, stand test time.
begin section 2 necessarily brief discussion relationship
bounded optimality earlier notions rationality. note particular important distinctions missed without precise definitions terms. Thus section 3
provide formal definitions agents, programs, behaviour rationality.
576

fiProvably bounded-optimal agents

Together formal descriptions task environments, elements allow us prove
given agent exhibits bounded optimality. Section 4 examines class agent architectures problem generating bounded optimal configurations eciently
soluble. solution involves class interesting practically relevant optimization
problems appear addressed scheduling literature. illustrate results showing throughput automated mail-sorting facility
might improved. Section 5 initiates discussion bounded optimal configurations
might learned experience environment. section 6, define weaker property, asymptotic bounded optimality (ABO), may robust tractable
strict version bounded optimality. particular, construct universal ABO
programs. program universally ABO ABO regardless specific form
time dependence utility function.1 Universal ABO programs therefore used
building blocks complex systems. conclude assessment prospects
development approach artificial intelligence.

2. Historical Perspective
classical idea perfect rationality, developed Aristotle's theories ethics,
work Arnauld others choice uncertainty, Mill's utilitarianism, put
formal footing decision theory Ramsey (1931) vonNeumann Morgernstern
(1947). stipulates rational agent always act maximize expected utility.
expectation taken according agent's beliefs; thus, perfect rationality
require omniscience.
artificial intelligence, logical definition rationality, known philosophy
\practical syllogism", put forward McCarthy (1958), reiterated strongly
Newell (1981). definition, agent take action believes
guaranteed achieve goals. AI said theoretical foundation, definition rationality provided it. McCarthy believed, probably
correctly, early stages field important concentrate \epistemological adequacy" \heuristic adequacy" | is, capability principle rather
practice. methodology resulted involves designing programs exhibit
calculative rationality, using various speedup techniques approximations
hope getting close possible perfect rationality. belief, albeit unproven,
simple agent designs fulfill specification calculative rationality may
provide good starting points approach bounded optimality. Moreover, theoretical foundation based calculative rationality cannot provide necessary guidance
search.
clear AI would embarked quest calculative rationality
operating halcyon days formal intractability results discovered.
One response spectre complexity rule bounds. Levesque
Brachman (1987) suggest limiting complexity environment calculative
perfect rationality coincide. Doyle Patil (1991) argue strongly position.
1. usage term \universal" derives use scheduling randomized algorithms
Luby, Sinclair Zuckerman (1993).

577

fiRussell & Subramanian

Economists used perfect rationality abstract model economic entities,
purposes economic forecasting designing market mechanisms. makes
possible prove theorems properties markets equilibrium. Unfortunately,
Simon (1982) pointed out, real economic entities limited time limited powers
deliberation. proposed study bounded rationality, investigating \: : : shape
system effectiveness computation one important weapons
survival." Simon's work focussed mainly satisficing designs, deliberate
reaching solution satisfying preset \aspiration level." results descriptive value modelling various actual entities policies, general prescriptive
framework bounded rationality developed. Although proved possible calculate
optimal aspiration levels certain problems, structural variation allowed
agent design.
theory games, bounds complexity players become topic
intense interest. example, troubling fact defection equilibrium
strategy unbounded agents playing fixed number rounds Prisoners' Dilemma
game. Neyman's theorem (Neyman, 1985), recently proved Papadimitriou Yannakakis (1994), shows essentially cooperative equilibrium exists agent
finite automaton number states less exponential number
rounds. essentially bounded optimality result, bound space rather
speed computation. type result made possible shift
problem selecting actions problem selecting programs.
I. J. Good (1971) distinguished perfect \type I" rationality, metalevel
\type II" rationality. defines \the maximization expected utility taking
account deliberation costs." Simon (1976) also says: \The global optimization problem
find least-cost best-return decision, net computational costs." Although type II
rationality seems step right direction, entirely clear whether
made precise way respects desirable intuition computation important.
try one interpretation, although may others.2 key issue space
\maximization" \optimization" occurs. Good Simon seem
referring space possible deliberations associated particular decision.
Conceptually, \object-level machine" executes sequence computations
control \meta-level machine." outcome sequence selection
external action. agent exhibits type II rationality end deliberation
subsequent action, utility maximized compared possible deliberate/act pairs
could engaged. example, Good discusses one possible application type
II rationality chess programs. case, object-level steps node expansions
game tree, followed backing leaf node evaluations show best move.
simplicity assume per-move time limit. type II rational agent execute
whichever sequence node expansions chooses best move, finish
2. example, conceivable Good Simon really intended refer finding agent design
minimizes deliberation costs general. discussions, however, seem couched terms
finding right deliberation decision. Thus, type II metalevel rationality coincides
bounded optimality bounded optimal agent designed single decision single
situation.

578

fiProvably bounded-optimal agents

time limit.3 Unfortunately, computations required \metalevel machine"
select object-level deliberation may extremely expensive. Good actually proposes
fairly simple (and nearly practical) metalevel decision procedure chess, far
optimal. hard see type II rational agent could justify executing suboptimal
object-level computation sequence limit scope optimization problem
single decision. diculty resolved thinking design
agent program, generates unbounded set possible deliberations response
unbounded set circumstances may arise life agent.
Philosophy also seen gradual evolution definition rationality.
shift consideration act utilitarianism | rationality individual acts |
rule utilitarianism, rationality general policies acting. shift caused
diculties individual versus societal rationality, rather consideration
diculty computing rational acts. consideration given recently
tractability general moral policies, view making understandable
usable persons average intelligence (Brandt, 1953). Cherniak (1986) suggested
definition \minimal rationality", specifying lower bounds reasoning powers
rational agent, instead upper bounds. philosophical proposal generally consistent
notion bounded optimality found Dennett's \Moral First Aid Manual"
(1986). Dennett explicitly discusses idea reaching equilibrium within space
decision procedures. uses example PhD admissions procedure philosophy
department. concludes, we, best procedure may neither elegant
illuminating. existence procedure, process reaching it,
main points interest.
Many researchers AI, whose work discussed below, worked
problem designing agents limited computational resources. 1989 AAAI Symposium AI Limited Rationality (Fehling & Russell, 1989) contains interesting
variety work topic. Much work concerned metalevel rationality.
Metareasoning | reasoning reasoning | important technique area,
since enables agent control deliberations according costs benefits.
Combined idea anytime (Dean & Boddy, 1988) exible algorithms (Horvitz,
1987), return better results time goes by, simple form metareasoning allows
agent behave well real-time environment. simple example provided
iterative-deepening algorithms used game-playing. Breese Fehling (1990) apply similar ideas controlling multiple decision procedures. Russell Wefald (1989) give
general method precompiling certain aspects metareasoning system eciently estimate effects individual computations intentions, giving fine-grained
control reasoning. techniques seen approximating metalevel rationality; provide useful insights general problem control reasoning,
reason suppose approximations used optimal sense.
intuitive notion bounded optimality seems become current AI
community mid-1980's. Horvitz (1987) uses term bounded optimality refer
\the optimization computational utility given set assumptions expected
3. One would imagine cases move selected move selected Type
agent, sense \accidental" deliberation might cause program
abandon it.

579

fiRussell & Subramanian

problems constraints reasoning resources." Russell Wefald (1991) say
agent exhibits bounded optimality given task environment \if program solution
constrained optimization problem presented architecture." Recent work
Etzioni (1989) Russell Zilberstein (1991) seen optimizing welldefined set agent designs, thereby making notion bounded optimality precise.
next section, build suitable set general definitions ground up,
begin demonstrate examples provably bounded optimal agents.

3. Agents, Architectures Programs
Intuitively, agent physical entity wish view terms perceptions
actions. counts first instance does, necessarily thinks,
even whether thinks all. initial refusal consider constraints
internal workings agent (such reason logically, example) helps
three ways: first, allows us view \cognitive faculties" planning reasoning
occurring service finding right thing do; second, makes room
among us (Agre & Chapman, 1987; Brooks, 1986) take position systems
right thing without cognitive faculties; third, allows freedom consider
various specifications, boundaries interconnections subsystems.
begin defining agents environments terms actions percepts
exchange, sequence states go through. agent described
agent function percept sequences actions. treatment fairly standard
(see, e.g., Genesereth & Nilsson, 1987). go \inside" agent look agent
program generates actions, define \implementation" relationship
program corresponding agent function. consider performance measures
agents, problem designing agents optimize performance measure.

3.1 Specifying agents environments

agent described abstractly mapping (the agent function) percept
sequences actions. Let set percepts agent receive instant,
set possible actions agent carry external world. Since
interested behaviour agent time, introduce set time points
instants, T. set totally ordered < relation unique least element.
Without loss generality, let set non-negative integers.
percept history agent sequence percepts indexed time. define
set percept histories OT = fOT : ! Og. prefix history OT 2 OT
till time denoted Ot projection OT [0::t]. define set
percept history prefixes Ot = fOt j 2 T; OT 2 OTg. Similarly, define set
action histories = fAT : ! Ag. set action history prefixes At, defined
set projections histories 2 .

Definition 1 Agent function: mapping
f : Ot !
580

fiProvably bounded-optimal agents



AT(t) = f (Ot)
Note agent function entirely abstract entity, unlike agent program
implements it. Note also \output" agent function given percept sequence
may null action, example agent still thinking do. agent
function specifies agent time step. crucial distinction
perfect rationality calculative rationality.
Agents live environments. states environment E drawn set X.
set possible state trajectories defined XT = fX : ! Xg. agent
necessarily full access current state X T(t), percept received agent
depend current state perceptual filtering function fp. effects
agent's actions represented environment's transition function fe,
specifies next state given current state agent's action. environment
therefore defined follows:

Definition 2 Environment E : set states X distinguished initial state X0,
transition function fe perceptual filter function fp

X (0) = X0
X T(t + 1) = fe(AT (t); X T(t))
OT(t) = fp(X T(t))
state history X thus determined environment agent function.
use notation effects(f; E ) denote state history generated agent function
f operating environment E . also use notation [E; ] denote
state history generated applying action sequence starting initial state
environment E .
Notice environment discrete deterministic formulation.
extend definitions cover non-deterministic continuous environments,
cost additional complexity exposition. None results depend significant
way discreteness determinism.

3.2 Specifying agent implementations

consider physical agent consisting architecture program.
architecture responsible interfacing program environment,
running program itself. architecture , associate finite programming
language LM , set programs runnable architecture. agent
program program l 2 LM takes percept input internal state drawn
set initial state i0. (The initial internal state depends program l,
usually suppress argument.) set possible internal state histories
= fI : ! Ig. prefix internal state history 2 till time denoted
projection [0::t].
581

fiRussell & Subramanian

Definition 3 architecture fixed interpreter agent program runs

program single time step, updating internal state generating action:
: LM !

hI T(t + 1); AT(t)i = (l; T(t); OT(t))
Thus, architecture generates stream actions according dictates program.
physical properties architecture, running program single
time step results execution finite number instructions. program may
often fail reach \decision" time step, result action produced
architecture may null (or previous action, depending program
design).

3.3 Relating agent specifications implementations

relate agent programs corresponding agent functions. say
agent program l running machine implements agent function Agent(l; ).
agent function constructed following definition specifying action sequences
produced l running possible percept sequences. Note importance
\Markovian" construction using internal state agent ensure actions
based past, future.

Definition 4 program l running implements agent function f = Agent(l; ),
defined follows. environment E = (X; fe; fp), f (Ot) = (t)
hI T(t + 1); AT(t)i = (l; T(t); OT(t))
OT (t)
X T(t + 1)
X T(0)
T(0)

=
=
=
=

fp(X T(t))
fe(AT(t); X T(t))
X0
i0

Although every program l induces corresponding agent function Agent(l; ),
action follows given percept necessarily agent's \response" percept;
delay incurred deliberation, may ect percepts occurring much
earlier sequence. Furthermore, possible map every agent function
implementation l 2 LM . define subset set agent functions f
implementable given architecture language LM :
Feasible(M ) = ff j 9l 2 LM ; f = Agent(l; )g
Feasibility related to, clearly distinct from, notion computability. Computability refers existence program eventually returns output specified
function, whereas feasibility refers production output appropriate
point time. set feasible agent functions therefore much smaller set
computable agent functions.
582

fiProvably bounded-optimal agents

3.4 Performance measures agents

evaluate agent's performance world, define real-valued utility function U
state histories:
U : XT ! <
utility function seen external agent environment. defines
problem solved designer agent. agent designs may incorporate
explicit representation utility function, means required.
use term task environment denote combination environment utility
function.
Recall agent's actions drive environment E particular sequence
states accordance function effects(f; E ). define value agent
function f environment E utility state history generates:
V (f; E ) = U (effects(f; E ))
designer set E environments probability distribution p them,
instead single environment E , value agent E defined
expected value elements E. slight abuse notation,

V (f; E) =

X

E2E

p(E )V (f; E )

assign value V (l; M; E ) program l executed architecture
environment E simply looking effect agent function implemented
program:
V (l; M; E ) = V (Agent(l; ); E ) = U (effects(Agent(l; ); E ))
above, extend set possible environments follows:

V (l; M; E) =

X

E2E

p(E )V (l; M; E )

3.5 Perfect rationality bounded optimality

discussed Section 2, perfectly rational agent selects action maximizes
expected utility, given percepts far. framework, amounts agent
function maximizes V (f; E) possible agent functions.
Definition 5 perfectly rational agent set E environments agent function
fopt
fopt = argmaxf (V (f; E))
definition persuasive specification optimal agent function given
set environments, underlies several recent projects intelligent agent design (Dean
& Wellman,1991; Doyle, 1988; Hansson & Mayer, 1989). direct implementation
specification, ignores delay incurred deliberation, yield reasonable
583

fiRussell & Subramanian

solution problem { calculation expected utilities takes time real agent.
terms simple formal description agents introduced above, easy see
diculty arisen. designing agent program, logicists decision theorists
concentrated specifying optimal agent function fopt order guarantee
selection best action history. function fopt independent architecture .
Unfortunately, real program LM implements function non-trivial environment,
optimal actions cannot usually computed next percept arrives.
is, quite frequently, fopt 62 Feasible(M ).
Suppose environment consists games chess tournament rules
population human grandmasters, suppose standard personal computer.
fopt describes agent always plays way maximize total
expected points opposition, maximization moves makes.
claim possible program play way. quite possible, using depth-first
alpha-beta search termination, execute program chooses (say) optimal
minimax move situation, agent function induced program
fopt. particular, ignores percepts dropping ag indicating
loss time.
trouble perfect rationality definition arose unconstrained optimization space f 's determination fopt , without regard feasibility.
(Similarly, metalevel rationality assumes unconstrained optimization space deliberations.) escape quandary, propose machine-dependent standard rationality, maximize V implementable set agent functions Feasible(M ).
is, impose optimality constraints programs rather agent functions
deliberations.

Definition 6 bounded-optimal agent architecture set E environments
agent program lopt
lopt = argmaxl2LM V (l; M; E)

see immediately specification avoids obvious problems
Type Type II rationality. Consider chess example,
suppose computer
26
2
total program memory 8 megabytes. 2 possible programs
represented machine, much smaller number play legal chess.
tournament conditions, one programs best expected performance. suitable candidate lopt. Thus bounded optimality is, definition,
feasible specification; moreover, program achieves highly desirable.
yet ready announce identity lopt chess eight-megabyte PC,
begin restricted problem.

4. Provably Bounded-Optimal Agents

order construct provably bounded optimal agent, must carry following
steps:
Specify properties environment actions taken,
utility function behaviours.
584

fiProvably bounded-optimal agents

Specify class machines programs run.
Propose construction method.
Prove construction method succeeds building bounded optimal agents.
methodology similar formal analysis used field optimal control,
studies design controllers (agents) plants (environments). optimal control
theory, controller viewed essentially instantaneous implementation optimal
agent function. contrast, focus computation time required agent,
relation computation time dynamics environment.

4.1 Episodic, real-time task environments

section, consider restricted class task environments call episodic
environments. episodic task environment, state history generated actions
agent considered divided series episodes, terminated
action. Let A? distinguished set actions terminate episode.
utility complete history given sum utilities episode,
determined turn state sequence. 2 A?, environment
\resets" state chosen random stationary probability distribution Pinit .
order include effects choice utility episode, notionally
divide environment state \configuration" part \value" part,
configuration part determines state transitions value part determines
utility state sequence. Actions A? reset configuration part, \value"
recorded value part. restrictions mean episode treated
separate decision problem, translate following property: agent program l1
higher expected utility individual episodes agent l2, higher expected
utility corresponding episodic task environment.
real-time task environment one utility action depends
time executed. Usually, dependence suciently strong make
calculative rationality unacceptably bad approximation perfect rationality.
automated mail sorter4 provides illustrative example episodic task environment (see Figure 1). machine scans handwritten printed addresses (zipcodes)
mail pieces dispatches appropriate bins. episode starts arrival
new mail piece terminates execution physical action recommended
sorter: routing piece specific bin. \configuration part" environment corresponds letter feeder side, provides new, randomly selected letter
previous letter sorted. \value part" state corresponds state
receiving bins, determines utility process. aim maximize
accuracy sorting minimizing reject percentage avoiding jams. jam occurs
current piece routed appropriate bin, rejected, arrival
next piece.
provide formal definitions three varieties real-time task environments:
fixed deadlines, fixed time cost stochastic deadlines.
4. See (Sackinger et al. 1992; Boser et al. 1992) details actual system. application
suggested us Bernhard Boser early presentation work 1992 NEC Symposium.

585

fiRussell & Subramanian

camera
sacks mail

zipcode
buckets

reject

Figure 1: automated mail-sorting facility provides simple example episodic,
real-time task environment.
4.1.1 Fixed deadlines

simplest commonly studied kind real-time task environment contains
deadline known time. work real-time systems, deadlines described
informally systems built meet deadline. Here, need formal specification
order connect description deadline properties agents running
deadline task environments. One might think deadlines part environment
description, fact mainly realized constraints utility function. One
see considering opposite deadline | \starter's pistol." two
distinguished differing constraints utilities acting specific
time.
Definition 7 Fixed deadline: task environment hE; U fixed deadline time td
following conditions hold.
Taking action A? time deadline results utility:

U ([E; At1]) = U ([E; A(2td ,1) AT1 (t)])
\" denotes sequence concatenation, td , AT1 (t) 2 A? , A(1t,1) A(2td ,1)
contain action A?.
Actions taken td effect utility:

U ([E; At1]) U ([E; At2]) U ([E; At1d ]) U ([E; At2d ]) td
4.1.2 Fixed time cost

Task environments approximately fixed time cost also common. Examples
include consultations lawyers, keeping taxi waiting, dithering invest
one's money. define task environment fixed time cost c comparing
utilities actions taken different times.
586

fiProvably bounded-optimal agents

Definition 8 Fixed time cost:
task environment hE; U fixed time cost if,


action history prefixes A11 A22 satisfying
(1) AT1 (t1) 2 A? AT2 (t2) = AT1 (t1)
(2) A(1t1 ,1) A(2t2 ,1) contain action A?
utilities differ difference time cost:
U ([E; At22 ]) = U ([E; At11 ]) , c(t2 , t1)

Strictly speaking, task environments fixed time cost. Utility values
finite range, one cannot continue incurring time costs indefinitely. reasonably short
times reasonably small costs, linear utility penalty useful approximation.
4.1.3 Stochastic deadlines

fixed-deadline fixed-cost task environments occur frequently design
real-time systems, uncertainty time-dependence utility function
common. also turns interesting, see below.
stochastic deadline represented uncertainty concerning time occurrence
fixed deadline. words, agent probability distributionPpd deadline
time td . assume deadline must come eventually, t2T pd (t) = 1.
also define cumulative deadline distribution Pd .
deadline occur known time, need distinguish
two cases:
agent receives percept, called herald (Dean & Boddy, 1988), announces
impending deadline. model using distinguished percept Od :

OT(td ) = Od
agent responds immediately, \meets deadline."
percept available, case agent walking blindfolded towards
utility cliff. deliberating further, agent risks missing deadline may
improve decision quality. example familiar readers deciding
whether publish paper current form, embellish risk
\scooped." treat case current paper.
Formally, stochastic deadline case similar fixed deadline case, except td
drawn distribution pd . utility executing action history prefix E
expectation utilities state history prefix possible deadline times.
Definition 9 Stochastic deadline: task environment class hE; U fixed-deadline task
environments stochastic deadline distributed according pd if, action history
prefix ,
X
U ([E; ]) = pd (t0)U ([Et ; ])
2T

0

0

hEt ; U task environment hE; U fixed deadline t0.
0

587

fiRussell & Subramanian

mail sorter example well described stochastic deadline. time
arrival mail pieces image processing station distributed according density
function pd , usually Poisson.

4.2 Agent programs agent architecture

consider simple agent programs episodic task environments, constructed elements set R = fr1 ; : : : ; rn g decision procedures rules. decision procedure
recommends (but execute) action Ai 2 A?, agent program fixed
sequence decision procedures. purposes, decision procedure black box
two parameters:

run time ti 0, integer represents time taken procedure
compute action.

quality qi 0, real number. gives expected reward resulting
executing action Ai start episode:

qi = U ([E; Ai])

(1)

Let MJ denote agent architecture executes decision procedures language J .
Let tM denote maximum runtime decision procedures accommodated
. example, runtime feedforward neural network proportional
size, tM runtime largest neural network fits .
architecture executes agent program = s1 : : : sm running decision
procedure turn, providing input obtained initial percept.
deadline arrives (at fixed time td , heralded percept Od ),
entire sequence completed, agent selects action recommended
highest-quality procedure executed:

(s; T(td); OT(td)) = hi0; action(I T(td ))i
(s; T(ts); OT(ts)) = hi0; action(I T(ts))i ts = Psi2s ti
(s; T(t); Od) = hi0; action(I T(t))i

(2)

updates agent's internal state history T(t) action(I T(t))
action recommended completed decision procedure highest quality.
action executed, internal state agent re-initialized i0 . agent
design works three task environment categories described above.
Next derive value V (s; M; E ) agent program environment E running
three real-time regimes show construct bounded optimal agents
task environments.

4.3 Bounded optimality fixed deadlines

Equation 2, know agent picks action A? recommended
decision procedure r highest quality executed deadline td arrives.
588

fiProvably bounded-optimal agents

P

Let s1 : : : sj longest prefix program ji=1 ti td . Definition 7
Equation 1, follows
V (s; M; E ) = Qj
(3)
Qi = maxfq1; : : : ; qi g. Given expression value agent program,
easily show following:
Theorem 1 Let r = arg maxri 2 R;titd qi . singleton sequence r bounded optimal
program episodic task environment known deadline td.
is, best program single decision procedure maximum quality whose runtime
less deadline.

4.4 Bounded optimality fixed time cost

Equation 2, know agent picks action A? recommended best
decision procedure sequence, since runs entire sequence = s1 : : : sm
deadline. Definition 8 Equation 1,

V (s; M; E ) = Qm , c


X
i=1

ti

(4)

Given expression value agent program, easily show following:
Theorem 2 Let r = arg maxri 2 R qi , cti . singleton sequence r bounded optimal
program episodic task environment fixed time cost c.
is, optimal program single decision procedure whose quality, net time
cost, highest.

4.5 Bounded optimality stochastic deadlines

stochastic deadline distributed according pd, value agent program
: : sm expectation. Definition 9, calculate
P ps (t=)V s(s;1 :M;
Et), hEt; U task environment fixed deadline t. Aft2T
ter substituting V (s; M; Et) Equation 3, expression simplifies summation,
procedures sequence, probability interruption ith procedure
sequence multiplied quality best completed decision procedure:

X
Pi
V (s) V (s; M; E) = [Pd(Pij+1
(5)
=1 tj ) , Pd ( j =1 tj )]Qi
i=1
Rt
Pd (t) = ,1
pd(t0)dt0 Pd (t) = 1 Pmi=1 ti.
simple example serves illustrate value function. Consider R = fr1 ; r2 ; r3g.
rule r1 quality 0.2 needs 2 seconds run: represent r1 = (0:2; 2).
rules r2 = (0:5; 5); r3 = (0:7; 7). deadline distribution function pd
uniform distribution 0 10 seconds. value sequence r1 r2r3
V (r1 r2r3 ) = [:7 , :2]:2 + [1 , :7]:5 + [1 , 1]:7 = :25
geometric intuition given notion performance profile, shown Figure 2.

589

fiRussell & Subramanian

q

0.7
0.5
0.2

p(t)


2

5

7

Figure 2: Performance profile r1r2 r3, pd superimposed.

Definition 10 Performance profile: sequence s, performance profile Qs (t) gives
quality action returned agent interrupted t:

Qs(t) = maxfqi :


X

j =1

tj tg

uniform deadline density function, value sequence proportional
area performance profile last possible interrupt time. Note
height profile interval length ti rule running quality
best previous rules.
Definition 10, following obvious property:
Lemma 1 performance profile sequence monotonically nondecreasing.
also case sequence higher quality decisions times better
sequence:
Lemma 2 8t Qs1 (t) Qs2 (t), V (s1) V (s2 ).
case say Qs1 dominates Qs2 .
use idea performance profiles establish useful properties optimal
sequences.
Lemma 3 exists optimal sequence sorted increasing order q's.

P

Without Lemma 3, ni=1 i! possible sequences consider. ordering constraint eliminates 2n sequences. also means proofs properties sequences, need consider ordered sequences. addition, replace Qi
Equation 5 qi .
following lemma establishes sequence always improved addition
better rule end:
Lemma 4 every sequence = s1 : : : sm sorted increasing order quality, single
step z qz qsm , V (sz ) V (s).
590

fiProvably bounded-optimal agents

Corollary 1 exists optimal sequence ending highest-quality rule R.
following lemma ects obvious intuition one get better result
less time, there's point spending time get worse result:
Lemma 5 exists optimal sequence whose rules nondecreasing order ti .
apply preparatory results derive algorithms construct bounded
optimal programs various deadline distributions.
4.5.1 General distributions

general deadline distribution, dynamic programming method used obtain
optimal sequence decision rules pseudo-polynomial time. construct optimal
sequence using definition V (s; M; E ) Equation 5. Optimal sequences generated
methods ordered qi, accordance Lemma 3.
construct table (i; t), entry table highest value
sequence ends rule ri time t. assume rule indices arranged

P
increasing order quality, ranges start time 0 end time L = ri 2R ti .
update rule is:

(i; t) = maxk2[0:::i,1][S (k; , ti ) + (qi , qk )[1 , Pd (t)]]
boundary condition
(i; 0) = 0 rule (0; t) = 0 time
Corollary 1, read best sequence highest value row n
matrix .
Theorem 3 DP algorithm computes optimal sequence time O(n2L) n
number decision procedures R.

dependence L time complexity DP algorithm means algorithm polynomial input size. Using standard rounding scaling methods,
however, fully polynomial approximation scheme constructed. Although
hardness proof problem, John Binder (1994) shown deadline
distribution used constant-time oracle finding values P (t), algorithm
require exponential number calls oracle worst case.
4.5.2 Long uniform distributions

deadline uniformly distributed time interval greater sum
running times rules, call distribution long uniform distribution. Consider
rule sequence = s1 : : : sm drawn rule set R. long uniform distribution,
probability deadline arrives rule si sequence independent
time si starts. permits simpler form Equation 5:

V (s; M; E) = Pmi=1,1 Pd (ti+1)qi + qm (1 , Pmi=1 Pd (ti))
591

(6)

fiRussell & Subramanian

derive optimal sequence long uniform distribution, obtain recursive
specification value sequence 2 R = s1 : : : sm sequence
R.

V (as; M; E) = V (s; M; E) + qaPd (t1) , qmPd (ta)

(7)

allows us define dynamic programming scheme calculating optimal sequence
using state function (i; j ) denoting highest value rule sequence starts
rule ends rule j . Lemma 3 Equation 7, update rule is:

(i; j ) = maxi<kj [S (k; j ) + Pd (tk )qi , Pd (ti)qj ]

(8)

boundary condition

(i; i) = (1 , Pd (ti))qi

(9)

Corollary 1, know optimal sequence long uniform distribution ends
rn , rule highest quality R. Thus, need examine (i; n); 1
n. entry requires O(n) computation, n entries compute. Thus,
optimal sequence long uniform case calculated O(n2 ).

Theorem 4 optimal sequence decision procedures long uniform deadline distribution determined O(n2) time n number decision procedures
R.
4.5.3 Short uniform distributions

P

ni=1 Pd (ti) > 1, uniform deadline distribution Pd, call short. means
sequences longer last possible deadline time, therefore rules
sequences possibility executing deadline. sequences,
cannot use Equation 7 calculate V (s). However, sequence truncated
removing rules would complete execution last possible deadline.
value sequence unaffected truncation, truncated sequences use
Equation 7 justified. Furthermore, optimal sequence truncated
sequence.
Since update rule 8 correctly computes (i; j ) truncated sequences, use
short uniform distributions provided add check ensure sequences
considered truncated. Unlike long uniform case, however, identity last rule
optimal sequence unknown, need compute n2 entries (i; j ) table.
entry computation takes O(n) time, thus time compute optimal sequence
O(n3).

Theorem 5 optimal sequence decision procedures short uniform deadline distribution determined O(n3) time n number decision procedures
R.
592

fiProvably bounded-optimal agents

4.5.4 Exponential distributions

exponential distribution, Pd(t) = 1,e,fit . Exponential distributions allow optimal
sequence computed polynomial time. Let pi stand probability rule
interrupted, assuming starts 0. pi = Pd(ti ) = 1 , e,fiti : exponential
distribution, V (s; M; E) simplifies as:

V (s; M; E) =



h
ij =1(1 , pj ) pi+1qi + mj=1(1 , pj ) qm

mX
,1 h
i=1

yields simple recursive specification value V (as; M; E) sequence
begins rule a:

V (as; M; E) = (1 , pa )p1qa + (1 , pa)V (s; M; E)
use state function (i; j ) represents highest value rule sequence
starting ending j .

(i; j ) = maxi<kj [(1 , pi )pk qi + (1 , pi)S (k; j )]
boundary condition (i; i) = qi(1 , pi). given j , (i; j ) calculated
O(n2). Corollary 1, know optimal sequence whose last element
highest-valued rule R.

Theorem 6 optimal sequence decision procedures exponentially distributed
stochastic deadline determined O(n2) time n number decision
procedures R.

proof similar long uniform distribution case.

4.6 Simulation results mail-sorter

preceding results provide set algorithms optimizing construction agent
program variety general task environment classes. section, illustrate
results possible gains realized specific task environment, namely,
simulated mail-sorter.
First, let us precise utility function U episodes. four
possible outcomes; utility outcome ui.
1. zipcode successfully read letter sent correct bin delivery.
2. zipcode misread letter goes wrong bin.
3. letter sent reject bin.
4. next letter arrives recognizer finished, jam. Since
letter arrival heralded, jams cannot occur machine architecture given
Equation 2.
593

fiRussell & Subramanian

1

1
mu=9

0.8

0.8

0.6

0.6
P(t)

Accuracy

lambda=0.9

0.4

0.4

0.2

0.2

0

0
0

2

4
6
Computation Time (sec)

8

10

0

2

4
6
Time (sec)

8

10

Figure 3: (a) Accuracy profile (1 , e,x ), = 0:9. (b) Poisson arrival distribution,
mean = 9 sec
Without loss generality, set u1 = 1:0 u2 = 0:0. probability rule
recommending correct destination bin pi, qi = piu1 + (1 , pi)u2 = pi . assume
u2 u3, hence threshold probability letter sent
reject bin instead. therefore include rule set R rule rreject
zero runtime recommends rejection. sequence construction algorithm
automatically exclude rules quality lower qreject = u3. overall utility
episode chosen linear combination quality sorting (qi ), probability
rejection rejection rate (given P (t1), t1 runtime first non-reject
rule executed), speed sorting (measured arrival time mean).
agent program (Boser et al. 1992) uses single neural network chip.
show variety conditions optimized sequence networks
significantly better single network terms throughput accuracy. examine
following experimental conditions:
assume network executes time recognition accuracy p
depends t. consider p = 1,e,t . particular choice irrelevant
scale chosen arbitrary. choose = 0:9, convenience (Figure 3(a)).
include rreject qreject = u3 treject = 0.
consider arrival time distributions Poisson varying means. Figure 3(b) shows three example distributions, means 1, 5, 9 seconds.
create optimized sequences sets 40 networks execution times taken
equal intervals = 1 40.
compare
(a) BO sequence: bounded optimal sequence;
(b) Best singleton: best single rule;
(c) 50% rule: rule whose execution time mean distribution (i.e.,
complete 50% cases);
594

fiProvably bounded-optimal agents

1
BO Sequence
Best Singleton
50% Rule
90% Rule

Average utility per second

0.8

0.6

0.4

0.2

0
5

10

15

20
25
Mean arrival time

30

35

40

Figure 4: Graph showing achievable utility per second function average time
per letter, four program types. = 0:9.
(d) 90% rule: rule whose execution time guarantees complete 90%
cases.
last three cases, add rreject initial step; BO sequence include
automatically.
measure utility per second function mean arrival rate (Figure 4).
shows optimal setting sorting machinery 6 letters per
minute (inter-arrival time = 10 seconds) bounded optimal program, given
fixed 0.9.
Finally, investigate effect variance arrival time relative
performance four program types. purpose, use uniform distribution
centered around 20 seconds different widths vary variance without
affecting mean (Figure 5).
notice several interesting things results:
policy choosing rule 90% probability completion performs poorly
rapid arrival rates ( 3), catches performance best single
rule slower arrival rates ( > 4). artifact exponential accuracy
profile > 0:5, difference quality rules run times
greater 6 seconds quite small.
policy choosing rule 50% probability completion fares well
best single rule high arrival rates ( 2), rapidly diverges
thereafter, performing far worse arrival time means greater 5 seconds.
595

fiRussell & Subramanian

1
BO Sequence
Best Singleton
50% Rule
90% Rule

Average utility per second

0.8

0.6

0.4

0.2

0
0

20

40

60
80
Variance arrival time

100

120

Figure 5: Graphs showing utility gain per second function arrival time
variance, four program types uniform distribution mean
20 seconds.

best sequence best single rule give best overall performance
arrival rate around 6 letters per minute. performance advantage
optimal sequence best single rule 7% arrival rate.
noted significant performance advantage obtainable
extra computational resources. slower arrival rates ( 7), difference
performance best rule best sequence arises decreased
rejection rate best sequence. exponential accuracy profile ( 0:5)
advantage running rule shorter completion time ahead longer rule
ability reduce probability rejecting letter. high arrival rates
(inter-arrival times 1 4 seconds), useful short rules instead
longer single rule.

Figure 5 shows best sequence performs better best single rule

variance arrival time increases.5 performance optimal sequence also
appears largely unaffected variance. exactly behaviour expect
observe | ability run sequence rules instead committing single
one gives robustness face increasing variance. Since realistic environments
involve unexpected demands many kinds, possession variety default
behaviours graded sophistication would seem optimal design choice
bounded agent.

5. performance 50% rule uniform distributions used experiment
fixed mean symmetric, 50% rule always rule runs 20 seconds.
90% rule changes variance, curve exhibits discretization effects. could
eliminated using finer-grained set rules.

596

fiProvably bounded-optimal agents

5. Learning Approximately Bounded-Optimal Programs

derivations assume suitable rule set R available ab initio, correct
qualities qi runtimes ti , deadline distribution known. section,
study ways information learned, implications
bounded optimality resulting system. concentrate learning rules
qualities, leaving runtimes deadline distributions future work.
basic idea learning algorithms converge, time, set
optimal components | accurate rules accurate quality estimates
them. happens, value agent constructed rules, using quality
estimates, converges value lopt. Thus two sources suboptimality
learned agent:
rules R may best possible rules | may recommend actions
lower utility would recommended rules.
may errors estimating expected utility rule. cause
algorithms given construct suboptimal sequences, even best rules
available.
notional method constructing bounded optimal agents (1) learns sets individual decision procedures episodic interactions, (2) arranges sequence
using one algorithms described earlier performance agent using
sequence least good agent. assume parameterized learning algorithm LJ ;k used learn one rule possible runtime
k 2 f1; : : : ; tM g. Since never need include two rules runtime
R, obviates need consider entire rule language J optimization
process.
setting places somewhat unusual requirements learning algorithm. Like
learning algorithms, LJ ;k works observing collection training episodes E,
including utility obtained episode. not, however, make assumptions
form correct decision rule. Instead, make assumptions
hypotheses, namely come finite language Jk , set programs
J complexity k. setting called agnostic learning setting
Kearns, Schapire Sellie (1992), assumptions made environment
all. shown (Theorems 4 5 Kearns, Schapire Sellie, 1992) that,
languages J , error learned approximation bounded within
best rule Jk fits examples, probability 1 , . sample size needed
guarantee bounds polynomial complexity parameter k, well 1 1 .
addition constructing decision procedures, LJ ;k outputs estimates
quality qi . Standard Chernoff-Hoeffding bounds used limit error quality
estimate within q probability 1 , q . sample size estimation quality
also polynomial 1q 1q .
Thus error agnostically learned rule bounded within best rule
complexity class probability 1 , . error quality estimation
rules bounded q probability 1 , q . bounds, calculate bound
utility deficit agent program construct, comparison lopt :
597

fiRussell & Subramanian

Theorem 7 Assume architecture MJ executes sequences decision procedures
agnostically learnable language J whose runtimes range [1::tM ]. real time task

environments fixed time cost, fixed deadline, stochastic deadline, construct
program l
V (lopt ; M; E) , V (l; M; E) + 2q
probability greater 1 , m( + q ), number decision procedures
lopt .

Proof: prove theorem stochastic deadline regime, bounded

optimal program sequence decision procedures. proofs fixed cost
fixed deadline regimes, bounded optimal program singleton, follow
special case. Let best decision procedures E set R = fr1 ; : : : ; rn g,
let lopt = s1 : : : sm optimal sequence constructed R. Let R = fr1 ; : : : rng
set decision procedures returned learning algorithm. probability greater
1 , m, qi , qi i, qi refers true quality ri . error
estimated quality q^i decision procedure ri also bounded: probability greater
1 , mq , jq^i , qi j q i.
Let = s1 : : : sm rules R come runtime classes
rules s1 : : : sm R . Then, Equation 5,
V (lopt ; M; E) , V (s; M; E)
error V weighted average errors individual qi . Similarly,

jV^ (s; M; E) , V (s; M; E)j q
suppose sequence construction algorithm applied R produces sequence
l = s1 : : : sl . definition, sequence appears optimal according estimated
value function V^ . Hence
V^ (l; M; E) V^ (s; M; E)
before, bound error estimated value:
jV^ (l; M; E) , V (l; M; E)j q
Combining inequalities,
V (lopt ; M; E) , V (l; M; E) + 2q
0

0

2

Although theorem practical applications, mainly intended illustration
learning procedure converge bounded optimal configuration.
additional work, general error bounds derived case rule
execution times ti real-time utility variation (time cost, fixed deadline, deadline
distribution) estimated training episodes. also obtain error bounds
case rule language J divided smaller number coarser
runtime classes, rather potentially huge number currently use.
598

fiProvably bounded-optimal agents

6. Asymptotic Bounded Optimality

strict notion bounded optimality may useful philosophical landmark
explore artificial intelligence, may strong allow many interesting, general
results obtained. observation made ordinary complexity theory:
although absolute eciency aim, asymptotic eciency game. sorting
algorithm O(n log n) rather O(n2) considered significant, replacing \multiply
2" \shift-left 1 bit" considered real advance. slack allowed
definitions complexity classes essential building earlier results, obtaining robust
results restricted specific implementations, analysing complexity
algorithms use algorithms subroutines. section, begin reviewing
classical complexity. propose definitions asymptotic bounded optimality
advantages, show classical optimality special case
asymptotic bounded optimality. Lastly, report preliminary investigations
use asymptotic bounded optimality theoretical tool constructing universal
real-time systems.

6.1 Classical complexity

problem, classical sense, defined pair predicates output
z solution input x (x) (x; z ) hold. problem instance
input satisfying , algorithm problem class always terminates output
z satisfying (x; z ) given input x satisfying (x). Asymptotic complexity describes
growth rate worst-case runtime algorithm function input size.
define formally follows. Let Ta (x) runtime algorithm input x,
let Ta (n) maximum runtime input size n. algorithm
complexity O(f (n))
9k; n0 8n n > n0 ) Ta (n) kf (n)
Intuitively, classically optimal algorithm one lowest possible complexity.
purposes constructing asymptotic notion bounded optimality,
useful definition classical optimality mention complexity
directly. done follows:
Definition 11 Classically optimal algorithm: algorithm classically optimal

9k; n0 8a0; n n > n0 ) Ta (n) kTa (n)
relate classical complexity framework, need define special case task
environments traditional programs appropriate. task environments,
input provided program initial percept, utility function
environment histories obeys following constraint:
Definition 12 Classical task environment: hEP ; U classical task environment
problem P
(
l outputs correct solution P
V (l; M; EP ) = u0 (T (l; M; EP )) ifotherwise
0

599

fiRussell & Subramanian

(l; M; EP ) running time l EP , universal Turing machine,
u positive decreasing function.
notion problem class classical complexity theory thus corresponds class
classical task environments unbounded complexity. example, Traveling Salesperson Problem contains instances arbitrarily large numbers cities.

6.2 Varieties asymptotic bounded optimality

first thing need complexity measure environments. Let n(E ) suitable
measure complexity environment. assume existence environment
classes unbounded complexity. Then, analogy definition classical
optimality, define worst-case notion asymptotic bounded optimality (ABO).
Letting V (l; M; n; E) minimum value V (l; M; E ) E E complexity n,

Definition 13 Worst-case asymptotic bounded optimality: agent program l timewise
(or spacewise) worst-case asymptotically bounded optimal E iff
9k; n0 8l0; n n > n0 ) V (l; kM; n; E) V (l0; M; n; E)
kM denotes version machine speeded factor k (or k times
memory).
English, means program basically along right lines needs
faster (larger) machine worst-case behaviour good program
environments.
probability distribution associated environment class E, use
expected value V (l; M; E) define average-case notion ABO:
Definition 14 Average-case asymptotic bounded optimality: agent program l timewise
(or spacewise) average-case asymptotically bounded optimal E iff
9k 8l0 V (l; kM; E) V (l0 ; M; E)
worst-case average-case definitions ABO, would happy
program ABO nontrivial environment nontrivial architecture , unless
k enormous.6 rest paper, use worst-case definition ABO.
Almost identical results obtained using average-case definition.
first observation made ABO programs classically optimal
programs special case ABO programs:7
6. classical definitions allow optimality constant factor k runtime algorithms.
One might wonder chose use constant factor expand machine capabilities, rather
increase time available program. context ordinary complexity theory,
two alternatives exactly equivalent, context general time-dependent utilities,
former appropriate. would possible simply \let l run k times longer," programs
wish consider control execution time, trading solution quality. One could
imagine slowing entire environment factor k, merely less realistic version
propose.
7. connection suggested Bart Selman.

600

fiProvably bounded-optimal agents

Theorem 8 program classically optimal given problem P
timewise worst-case ABO corresponding classical task environment class hEP ; U i.
observation follows directly Definitions 11, 12, 13.
summary, notion ABO provide degree theoretical robustness
machine-independence study bounded systems asymptotic complexity
classical programs. set basic framework, begin exercise
definitions.

6.3 Universal asymptotic bounded optimality

Asymptotic bounded optimality defined respect specific value function V .
constructing real-time systems, would prefer certain degree independence
temporal variation value function. achieve defining family V value
functions, differing temporal variation. mean value function
preserves preference ordering external actions time, value functions
family preference ordering.8
example, fixed-cost regime vary time cost c generate family
value functions; stochastic deadline case, vary deadline distribution Pd
generate another family. Also, since three regimes uses quality measure
actions, union three corresponding families also family.
show single program, call universal program, asymptotically
bounded-optimal regardless value function chosen within particular family.
Definition 15 Universal asymptotic bounded optimality (UABO): agent program l
UABO environment class E family value functions V iff l ABO E
every Vi 2 V .
UABO program must compete ABO programs every individual value function
family. UABO program therefore universal real-time solution given task.
UABO programs exist? so, construct them?
turns use scheduling construction (Russell & Zilberstein,
1991) design UABO programs. construction designed reduce task environments unknown interrupt times case known deadlines, insight
applies here. construction requires architecture provide program concatenation (e.g., LISP prog construct), conditional-return construct, null program
. universal program lU form concatenation individual programs
increasing runtime, appropriate termination test each. written
lU = [l0 l1 lj ]
lj consists program termination test. program part lj
program LM ABO E value function Vj corresponds fixed deadline
td = 2j , time increment smaller execution time non-null
program LM .
8. value function must therefore separable (Russell & Wefald, 1989), since preservation rank
order allows separate time cost defined. See chapter 9 (Keeney & Raiffa, 1976) thorough
discussion time-dependent utility.

601

fiRussell & Subramanian

q

l U 4M

0.7

l opt

0.5
0.2

p(t)


0

10

Figure 6: Performance profiles lU running 4M , lopt running
proceeding statement lU indeed UABO, let us look example.
Consider simple, sequential machine architecture described earlier. Suppose
select rules three-rule set r1 = (0:2; 2), r2 = (0:5; 5) r3 = (0:7; 7). Since
shortest runtime rules 2 seconds, let = 1. look optimal
programs l0 ; l1; l2; l3 ; : : : fixed-deadline task environments td = 1; 2; 4; 8; : : :.
are:

l0 = ; l1 = r1; l2 = r1; l3 = r3 ; : : :
Hence sequence programs lU [; r1; r1 ; r3; : : :].
consider task environment class value function Vi specifies stochastic
deadline uniformly distributed range [0: : : 10]. class, lopt = r1 r2
bounded optimal sequence.9 turns lU higher utility lopt provided
run machine four times faster. see plotting two performance
profiles: QU lU 4M Qopt lopt . QU dominates Qopt, shown Figure 6.
establish lU construction yields UABO programs general, need
define notion worst-case performance profile. Let Q (t; l; M; n; E) minimum
value obtained interrupting l t, E E complexity n. know
lj lU satisfies following:

8l0; n n > nj ) Vj(lj ; kj M; n; E) Vj(l0 ; M; n; E)
constants kj , nj . aim prove

8Vi 2 V 9k; n0 8l0; n n > n0 ) Vi(lU ; kM; n; E) Vi(l0; M; n; E)
Given definition worst-case performance profile, fairly easy show following
lemma (the proof essentially identical proof Theorem 1 Russell Zilberstein,
1991):
9. Notice that, simple model, output quality rule depends execution time
input complexity. also means worst-case average-case behaviour same.

602

fiProvably bounded-optimal agents

1
BO Sequence
ABO sequence

Average utility per second

0.8

0.6

0.4

0.2

0
5

10

15

20
25
Mean arrival time

30

35

40

, function mean
Figure 7: Throughput accuracy improvement lU lopt
arrival time, = 0.2, Poisson arrivals.

Lemma 6 lU universal program E V , li ABO E Vi 2 V ,
Q(t; lU ; kM; n; E) dominates Q(t; li ; M; n; E) k 4 maxj kj , n > maxj nj .
lemma establishes that, small constant penalty, ignore specific realtime nature task environment constructing bounded optimal programs. However,
still need deal issue termination. possible general lU
terminate appropriate time without access information concerning timedependence utility function. example, fixed-time-cost task environment,
appropriate termination time depends value time cost c.
general case deterministic time-dependence, help lU supplying, Vi , \aspiration level" Qi (ti; li ; M; n; E), ti time
li acts. lU terminates completed lj qj Qi (ti ; li; M; n; E).
construction, happen later ti Lemma 6.

Theorem 9 task environments deterministic time-dependence, lU suitable
aspiration level UABO E .
deadline heralds, termination test somewhat simpler require
additional input lU .
Theorem 10 task environment stochastic deadlines, lU UABO E
terminates herald arrives.
Returning mail-sorting example, fairly easy see lU (which consists
sequence networks, like optimal programs stochastic deadline case)
ABO fixed-deadline regime. obvious also ABO particular
603

fiRussell & Subramanian

stochastic deadline case | recall regimes considered single family.
programmed constructor function universal programs, applied
mail-sorter environment class. Varying letter arrival distribution gives us different value
functions Vi 2 V . Figure 7 shows lU (on 4M ) higher throughput accuracy
across entire range arrival distributions.
lopt
Given existence UABO programs, possible consider behaviour compositions thereof. simplest form composition functional composition,
output one program used input another. complex, nested compositional structures entertained, including loops conditionals (Zilberstein, 1993).
main issue constructing UABO compositions allocate time among
components. Provided solve time allocation problem know
total runtime allowed, use construction technique used generate composite UABO programs, optimality among possible compositions
components. Zilberstein Russell (1993), show allocation problem
solved linear time size composite system, provided composition tree
bounded degree.

7. Conclusions Work
examined three possible formal bases artificial intelligence, concluded
bounded optimality provides appropriate goal constructing intelligent systems.
also noted similar notions arisen philosophy game theory
less reason: mismatch classically optimal actions
called feasible behaviours|those generated agent program running
computing device finite speed size.
showed careful specification task environment computing
device one design provably bounded-optimal agents. exhibited simple
agents, likely bounded optimality strict sense dicult goal
achieve larger space agent programs considered. relaxed notions
asymptotic bounded optimality (ABO) may provide theoretically robust tools
progress. particular, ABO promises yield useful results composite agent
designs, allowing us separate problem designing complex ABO agents discrete
structural problem continuous temporal optimization problem tractable
many cases. Hence, reason optimistic artificial intelligence
usefully characterized study bounded optimality. may speculate provided
computing device neither small (so small changes speed size cause
significant changes optimal program design) powerful (so classically
optimal decisions computed feasibly), ABO designs stable reasonably
wide variations machine speed size environmental complexity. details
optimal designs may rather arcane, learning processes play large part
discovery; expect focus type research questions
convergence optimality various structural classes end result itself.
Perhaps important implication, beyond conceptual foundations field
itself, research bounded optimality applies, design, practice artificial
intelligence way idealized, infinite-resource models may not. given,
604

fiProvably bounded-optimal agents

way illustrating definition, bounded optimal agent: design simple system
consisting sequences decision procedures provably better program
class. theorem exhibits bounded optimal design translates, definition,
agent whose actual behaviour desirable.
appear plenty worthwhile directions continue exploration
bounded optimality. foundational point view, one interesting
questions concept applies agents incorporate learning component.
(Note section 5, learning algorithm external agent.)
case, necessarily largely stable bounded optimal configuration
agent program large enough; instead, agent adapt shorter-term
horizon rewrite becomes obsolete.
results preservation ABO composition, start examine
much interesting architectures simple production system studied above.
example, look optimal search algorithms, algorithm constrained
apply metalevel decision procedure step decide node expand,
(Russell & Wefald, 1989). also extend work asymptotic bounded optimality
provide utility-based analogue \big-O" notation describing performance
agent designs, including suboptimal.
context computational learning theory, obvious stationarity
requirement environment, necessary satisfy preconditions PAC
results, restrictive. fact agent learns may effect
distribution future episodes, little known learning cases (Aldous &
Vazirani, 1990). could also relax deterministic episodic requirement allow
non-immediate rewards, thereby making connections current research reinforcement
learning.
computation scheduling problem examined interesting itself,
appear studied operations research combinatorial optimization literature. Scheduling algorithms usually deal physical rather computational tasks,
hence objective function usually involves summation outputs rather picking
best. would like resolve formal question tractability general case,
also look cases solution qualities individual processes interdependent
(such one use results another). Practical extensions include computation
scheduling parallel machines multiple agents, scheduling combinations computational physical (e.g., job-shop ow-shop) processes, objective functions
combination summation maximization. latter extension broadens scope
applications considerably. industrial process, designing manufacturing
car, consists computational steps (design, logistics, factory scheduling, inspection
etc.) physical processes (stamping, assembling, painting etc.). One easily imagine
many applications real-time financial, industrial, military contexts.
may turn bounded optimality found wanting theoretical framework.
case, hope refuted interesting way, better framework
created process.
605

fiRussell & Subramanian

Appendix: Additional Proofs
appendix contains formal proofs three subsidiary lemmata main body
paper.

Lemma 3 exists optimal sequence sorted increasing order q's.
Proof: Suppose case, optimal sequence. must

two adjacent rules i, + 1 qi > qi+1 (see Figure 8). Removal rule + 1 yields
sequence s0 Qs (t) Qs (t), Lemma 1 fact ti+2 ti+1 + ti+2 .
Lemma 2, s0 must also optimal. repeat removal process s0 ordered
qi , proving theorem reductio ad absurdum.2
0

Lemma 4 every sequence = s1 : : : sm sorted increasing order quality, single
step z qz qsm , V (sz ) V (s).
Proof: calculate V (sz ) , V (s) using Equation 5 show non-negative:
V (sz ) , V (s) = qz [1 , Pd ((Pmj=1 tj ) + tz )] , qm[1 , Pd ((Pmj=1 tj ) + tz )]
P
= (qz , qm )[1 , Pd (( mj=1 tj ) + tz )]

non-negative since qz qm .2
q

i+2

qi+2
qi
qi-1
qi+1



ti

i+1

Figure 8: Proof ordering qi; lower dotted line indicates original profile; upper dotted
line indicates profile removal rule + 1.

Lemma 5 exists optimal sequence whose rules nondecreasing order ti .
Proof: Suppose case, optimal sequence. must
two adjacent rules i, + 1 qi qi+1 ti > ti+1 (see Figure 9). Removal rule
yields sequence s0 Qs (t) Qs (t), Lemma 1. Lemma 2, s0 must also
0

optimal. repeat removal process s0 ordered ti, proving theorem
reductio ad absurdum.2
606

fiProvably bounded-optimal agents

q

qi+1
qi

i+1

qi-1



i+1

ti

Figure 9: Proof ordering ti ; dotted line indicates profile removal rule i.

Acknowledgements

would like acknowledge stimulating discussions Michael Fehling, Michael Genesereth, Russ Greiner, Eric Horvitz, Henry Kautz, Daphne Koller, Bart Selman
subject bounded optimality; Dorit Hochbaum, Nimrod Megiddo, Kevin Glazebrook subject dynamic programming scheduling problems; Nick
Littlestone Michael Kearns subject agnostic learning. would also like
thank reviewers many constructive suggestions. Many early ideas
work based arose discussions late Eric Wefald. Thanks also
Ron Parr work uniform-distribution case, Rhonda Righter extending
results exponential distribution, Patrick Zieske help implementing dynamic programming algorithm. first author supported NSF grants IRI-8903146,
IRI-9211512 IRI-9058427, visiting fellowship SERC sabbatical
UK, NEC Research Institute. second author supported NSF
grant IRI-8902721.

References

Agre, P., & Chapman, D. (1987). Pengi: implementation theory activity.
Proc. 6th National Conference Artificial Intelligence, Seattle, WA. Morghan Kaufmann.
Aldous, D., & Vazirani, U. (1990). markovian extension valiant's learning model.
Proc. 31st Annual Symposium Foundations Computer Science, St. Louis, MO.
IEEE Comput. Soc. Press.
Binder, J. (1994). complexity deliberation scheduling stochastic deadlines..
Boser, B. E., Sackinger, E., Bromley, J., & LeCun, Y. (1992). Hardware requirements
neural network pattern classifiers | case study implementation. IEEE Micro,
12, 32{40.
Brandt, R. (1953). search credible form rule utilitarianism. Nakhnikian, G., &
Castaneda, H. (Eds.), Morality Language Conduct.
607

fiRussell & Subramanian

Breese, J. S., & Fehling, M. R. (1990). Control problem-solving: Principles architecture. Shachter, R. D., Levitt, T., Kanal, L., & Lemmer, J. (Eds.), Uncertainty
Artificial Intelligence 4. North Holland: Amsterdam.
Brooks, R. A. (1986). robust, layered control system mobile robot. IEEE Journal
Robotics Automation, 2, 14{23.
Cherniak, C. (1986). Minimal rationality. MIT Press: Cambridge.
Dean, T., & Boddy, M. (1988). analysis time-dependent planning. Proc. AAAI88, pp. 49{54.
Dean, T. L., & Wellman, M. P. (1991). Planning control. Morgan Kaufmann: San
Mateo, CA.
Dennett, D. (1986). moral first aid manual. Tanner lectures human values, University
Michigan.
Doyle, J. (1983). rational psychology? toward modern mental philosophy. AI
Magazine, 4, 50{53.
Doyle, J. (1988). Artificial intelligence rational self-government. Tech. rep.. Technical
report CMU-CS-88-124.
Doyle, J., & Patil, R. (1991). Two theses knowledge representation: language restrictions, taxonomic classification, utility representation services. Artificial
intelligence, 48, 261{297.
Etzioni, O. (1989). Tractable decision-analytic control. Proc. 1st International Conference Knowledge Representation Reasoning, pp. 114{125.
Fehling, M., & Russell, S. J. (1989). Proceedings AAAI Spring Symposium Limited
Rationality. AAAI.
Genesereth, M. R., & Nilsson, N. J. (1987). Logical Foundations Artificial Intelligence.
Morgan Kaufmann: Mateo, CA.
Good, I. J. (1971). Twenty-seven principles rationality. Godambe, V. P., & Sprott,
D. A. (Eds.), Foundations Statistical Inference, pp. 108{141. Holt, Rinehart, Winston.: Toronto.
Hansson, O., & Mayer, A. (1989). Heuristic search evidential reasoning. Proceedings
Fifth Workshop Uncertainty Artificial Intelligence, Windsor, Ontario.
Horvitz, E. J. (1988). Reasoning beliefs actions computational resource
constraints. Levitt, T., Lemmer, J., & Kanal, L. (Eds.), Uncertainty Artificial
Intelligence 3. North Holland: Amsterdam.
Kearns, M., Schapire, R., & Sellie, L. (1992). Toward ecient agnostic learning. Proc. 5th
Ann. Workshop Computational Learning Theory, Pittsburgh, PA. Morgan Kaufmann.
608

fiProvably bounded-optimal agents

Keeney, R., & Raiffa, H. (1976). Decisions multiple objectives: Preferences value
tradeoffs. Wiley: New York.
Levesque, H., & Brachman, R. (1987). Expressiveness tractability knowledge representation reasoning. Computational Intelligence, 3, 78{93.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup las vegas algorithms.
Information Processing Letters, 47, 173{80.
McCarthy, J. (1958). Programs common sense. Proceedings Symposium
Mechanization Thought Processes, Teddington, England: HMSO.
Newell, A. (1981). knowledge level. AI Magazine, 2, 1{20.
Neyman, A. (1985). Bounded complexity justifies cooperation finitely repeated prisoners' dilemma. Economics Letters, 19, 227{229.
Papadimitriou, C., & Yannakakis, M. (1994). complexity bounded rationality.
Proc. ACM Symposium Theory Computation.
Ramsey, F. P. (1931). Truth probability. Braithwaite, R. (Ed.), foundations
mathematics logical essays. Harcourt Brace Jovanovich: New York.
Russell, S. J., & Wefald, E. H. (1989a). optimal game tree search using rational metareasoning. Proc. IJCAI-89.
Russell, S. J., & Wefald, E. H. (1989b). Principles metareasoning. Proc. KR-89.
Russell, S. J., & Wefald, E. H. (1991). right thing: Studies limited rationality.
MIT Press: Cambridge, MA.
Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Proc. IJCAI-91,
Sydney.
Sackinger, E., Boser, B. E., Bromley, J., & LeCun, Y. (1992). Application anna
neural network chip high-speed character recognition. IEEE Transactions Neural
Networks, 3, 498{505.
Simon, H. A. (1976). decide do. Models bounded rationality,
Volume 2.
Simon, H. A. (1982). Models bounded rationality, Volume 2. MIT Press: Cambridge.
von Neumann, J., & Morgenstern, O. (1947). Theory games economic behavior.
Princeton University Press: Princeton.
Zilberstein, S. (1993). Operational Rationality Compilation Anytime Algorithms.
Ph.D. thesis, Computer Science Division, University California, Berkeley.
Zilberstein, S., & Russell, S. (1993). Optimal composition real-time systems. Submitted
Artificial Intelligence.
609

fiJournal Artificial Intelligence Research 2 (1995) 287-318

Submitted 9/94; published 1/95

Truncating Temporal Differences:
Ecient Implementation TD()
Reinforcement Learning
Pawe Cichosz

Institute Electronics Fundamentals, Warsaw University Technology
Nowowiejska 15/19, 00-665 Warsaw, Poland

cichosz@ipe.pw.edu.pl

Abstract

Temporal difference (TD) methods constitute class methods learning predictions
multi-step prediction problems, parameterized recency factor . Currently
important application methods temporal credit assignment reinforcement
learning. Well known reinforcement learning algorithms, AHC Q-learning, may
viewed instances TD learning. paper examines issues ecient
general implementation TD() arbitrary , use reinforcement learning
algorithms optimizing discounted sum rewards. traditional approach, based
eligibility traces , argued suffer ineciency lack generality. TTD
(Truncated Temporal Differences ) procedure proposed alternative, indeed
approximates TD(), requires little computation per action used
arbitrary function representation methods. idea derived fairly
simple new, probably unexplored far. Encouraging experimental results
presented, suggesting using > 0 TTD procedure allows one obtain
significant learning speedup essentially cost usual TD(0) learning.

1. Introduction
Reinforcement learning (RL, e.g., Sutton, 1984; Watkins, 1989; Barto, 1992; Sutton, Barto,
& Williams, 1991; Lin, 1992, 1993; Cichosz, 1994) machine learning paradigm relies
evaluative training information. step discrete time learning agent observes
current state environment executes action . receives reinforcement value, also called payoff reward (punishment), state transition takes
place. Reinforcement values provide relative measure quality actions executed
agent. state transitions rewards may stochastic, agent
know either transition probabilities expected reinforcement values state-action
combinations. objective learning identify decision policy (i.e., state-action
mapping) maximizes reinforcement values received agent long term .
commonly assumed formal model reinforcement learning task Markovian decision
problem (MDP, e.g., Ross, 1983). Markov property means state transitions
reinforcement values always depend solely current state current action:
dependence previous states, actions, rewards, i.e., state information supplied
agent sucient making optimal decisions.
information agent external world task contained
series environment states reinforcement values. never told actions
execute particular states, actions (if any) would better
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCichosz

actually performs. must learn optimal policy observing consequences
actions. abstract formulation generality reinforcement learning paradigm
make widely applicable, especially domains game-playing (Tesauro, 1992),
automatic control (Sutton et al., 1991), robotics (Lin, 1993). formulate particular
task reinforcement learning task, one design appropriate state action
representation, reinforcement mechanism specifying goal task. main
limitation RL applications nature trial-and-error learning method,
hardly applicable domains making errors costs much.
commonly studied performance measure maximized RL agent
expected total discounted sum reinforcement:

E

"1
X

t=0

trt

#

;

(1)

rt denotes reinforcement value received step t, 0 1 discount
factor , adjusts relative significance long-term rewards versus short-term ones.
maximize sum positive , agent must take account delayed
consequences actions: reinforcement values may received several steps
actions contributed performed. referred learning delayed
reinforcement (Sutton, 1984; Watkins, 1989). reinforcement learning performance
measures also considered (Heger, 1994; Schwartz, 1993; Singh, 1994),
work limit exclusively performance measure specified Equation 1.
key problem must solved order learn optimal policy
conditions delayed reinforcement known temporal credit assignment problem
(Sutton, 1984). problem assigning credit blame overall outcomes
learning system (i.e., long-term reinforcement values) individual actions,
possibly taken several steps outcomes could observed. Discussing reinforcement
learning algorithms, concentrate temporal credit assignment ignore issues
structural credit assignment (Sutton, 1984), aspect credit assignment RL
systems.

1.1 Temporal Difference Methods

temporal credit assignment problem reinforcement learning typically solved using
algorithms based methods temporal differences (TD). introduced
Sutton (1988) class methods learning predictions multi-step prediction
problems. problems prediction correctness revealed once,
one step since prediction made, though partial information relevant
correctness revealed step. information available observed
current state prediction problem, corresponding prediction computed
value function states.
Consider multi-step prediction problem step necessary learn
prediction final outcome. could example predicting outcome game
chess subsequent board situations, predicting weather Sunday day
week, forecasting economic indicators. traditional approach learning
predictions would wait outcome occurs, keeping track predictions
288

fiTruncating Temporal Differences

computed intermediate steps, then, them, use difference
actual outcome predicted value training error. supervised learning,
directed training information obtained comparing outcome predictions
produced step. predictions modified make closer
outcome.
Temporal difference learning makes unnecessary always wait outcome.
step difference two successive predictions used training error.
prediction modified make closer next one. fact, TD class
methods referred TD(), 0 1 called recency factor . Using > 0
allows one incorporate prediction differences time steps, hopefully speed
learning.
Temporal credit assignment reinforcement learning may viewed prediction
problem. outcome predict state simply total discounted reinforcement
received starting state following current policy. predictions used modifying policy optimize performance measure given
Equation 1. Example reinforcement learning algorithms implement idea, called
TD-based algorithms , presented Section 2.2.

1.2 Paper Overview

Much research concerning TD-based reinforcement learning algorithms concentrated simplest TD(0) case. However, experimental results obtained TD( > 0)
indicate often allows one obtain significant learning speedup (Sutton, 1988;
Lin, 1993; Tesauro, 1992). also suggested (e.g., Peng & Williams, 1994)
TD( > 0) perform better non-Markovian environments TD(0) (i.e.,
less sensitive potential violations Markov property). thus important
develop ecient general implementation techniques would allow TD-based RL
algorithms use arbitrary . motivation work.
remainder paper organized follows. Section 2 formal definition
TD methods presented application reinforcement learning discussed. Three
example RL algorithms brie described: AHC (Sutton, 1984), Q-learning (Watkins,
1989; Watkins & Dayan, 1992), advantage updating (Baird, 1993). Section 3 presents
traditional approach TD() implementation, based called eligibility traces,
criticized ineciency lack generality. Section 4 analysis
effects TD algorithm leads formulation TTD (Truncated Temporal
Differences ) procedure. two remaining sections devoted experimental results
concluding discussion.

2. Definition TD()

Sutton (1988) introduced TD methods, assumed would use parameter estimation techniques prediction representation. According original formulation,
states prediction problem represented vectors real-valued features, corresponding predictions computed use set modifiable parameters (weights).
representation learning consists adjusting weights appropriately
basis observed state sequences outcomes. present alternative formula289

fiCichosz

tion, adopted Dayan (1992), simplifies analysis effects TD()
algorithm. formulation states may elements arbitrary finite state space,
predictions values function states. Transforming Sutton's original definition
TD() alternative form straightforward.
discussing either generic RL-oriented form TD methods, consequently ignore issues function representation. assumed TD predictions functions maintained reinforcement learning algorithms represented
method allows adjusting function values using error values, controlled learning rate parameter. Whenever write value n-argument function '
arguments p0; p1; : : :; pn,1 updated using error value , mean
'(p0; p1; : : :; pn,1) moved towards '(p0; p1; : : :; pn,1) + , degree controlled
learning rate factor . general form abstract update operation written

update ('; p0 ; p1; : : :; pn,1; ):
(2)
convention, learning algorithm defined rule uses computing
error values.

2.1 Basic Formulation

Let x0; x1; : : :; xm,1 sequence states multi-step prediction problem.
state xt observed time step t, step m, passing whole sequence,
real-valued outcome z observed. learning system required produce corresponding sequence predictions P (x0 ); P (x1); : : :; P (xm,1 ), estimate
z .
Following Dayan (1992), let us define state x:
(

x (t) =

1 xt = x
0 otherwise:

TD() prediction error state x determined step given by:

x(t) = (P (xt+1) , P (xt ))


X
t,k

k=0

x (k);

(3)

0 1 P (xm ) = z definition, total prediction error state x
determined whole observed sequence accordingly is:

x =

mX
,1
t=0

x(t) =

mX
,1 (
t=0

(P (xt+1 ) , P (xt


X
)) t,k
k=0

)

x (k )

:

(4)

Thus, learning step driven difference two temporally successive
predictions. > 0, prediction difference time affects P (xt ), also
predictions previous time steps, exponentially decaying degree.1
1. Alternatively, learning prediction step relies prediction difference
step, also future prediction differences. equivalent formulation play significant role
Section 4.

290

fiTruncating Temporal Differences

two possibilities using defined errors learning. first compute total errors x states x, accumulating x (t) errors computed time
step t, use passing whole state sequence update predictions P (x).
corresponds batch learning mode. second possibility, called incremental on-line
learning, often attractive practice, update predictions step using
current error values x (t). necessary modify appropriately Equation 3,
take account predictions changed step:

x (t) = (Pt(xt+1 ) , Pt (xt ))


X
t,k

k=0

x (k);

(5)

Pt (x) designates prediction state x available step t.
Sutton (1988) proved convergence batch TD(0) linear representation,
states represented linearly independent vectors, assumption state sequences generated absorbing Markov process .2 Dayan (1992) extended proof
arbitrary .3

2.2 TD() Reinforcement Learning

far, paper presented TD general class prediction methods multi-step
prediction problems. important application methods, however, reinforcement learning. matter fact, TD methods formulated Sutton (1988)
generalization techniques previously used context temporal credit
assignment reinforcement learning (Sutton, 1984).
already stated above, straightforward way formulate temporal credit
assignment prediction problem predict time step discounted sum
future reinforcement
1
X
zt = k rt+k ;
k=0

called TD return time t. corresponding prediction designated U (xt )
called predicted utility state xt . TD returns obviously depend policy
followed; therefore assume U values represent predicted state utilities respect
current policy. perfectly accurate predictions would have:
U (xt) = zt = rt + zt+1 = rt + U (xt+1):
Thus, inaccurate predictions, mismatch TD error rt + U (xt+1) , U (xt).
resulting RL-oriented TD() equations take form:

x(t) = (rt + Ut(xt+1 ) , Ut (xt))


X

( )t,k x (k)

(6)

k=0
2. absorbing Markov process defined set terminal states XT , set non-terminal states XN ,
set transition probabilities Pxy x 2 XN 2 XN [ XT . absorbing property
means cycles among non-terminal states cannot last indefinitely long, i.e., starting
non-terminal state terminal state eventually reached (all sequences eventually terminate).
3. Recently stronger theoretical results proved Dayan Sejnowski (1994) Jaakkola, Jordan,
Singh (1993).

291

fiCichosz



x =

1
X
t=0

x (t) =

1
X
t=0

(

(rt + Ut(xt+1) , Ut (xt ))


X
k=0

)

( )t,kx (k) :

(7)

Note following additional differences equations Equations 3 4:

time step subscripts used U values emphasize on-line learning mode,
discount applied sum Equation 6 includes well reasons
may unclear now, made clear Section 4.1,

summation Equation 7 extends infinity, predicted final outcome
not, general, available finite number steps.

TD-based reinforcement learning algorithms may viewed less direct implementations general rule described Equation 6. see this, consider
three algorithms: well known AHC (Sutton, 1984) Q-learning (Watkins, 1989; Watkins
& Dayan, 1992), recent development Baird (1993) called advantage updating .
algorithms rely learning certain real-valued functions defined state state
action space task. superscript used described functions
designates optimal values (i.e., corresponding optimal policy). Simplified versions
algorithms, corresponding TD(0), presented related Equation 6.
presentation limited solely function update rules | elaborated
description algorithms reader consult original publications
developers or, AHC Q-learning, Lin (1993) Cichosz (1994). closely
related dynamic programming methods (Barto, Sutton, & Watkins, 1990; Watkins, 1989;
Baird, 1993), relations, though theoretically practically important fruitful, essential subject paper discussed.
2.2.1 AHC Algorithm

variation AHC algorithm described adopted Sutton (1990). Two
functions maintained: evaluation function V policy function f . evaluation
function evaluates environment state essentially called
U function, i.e., V (x) intended estimate discounted sum
future reinforcement values received starting state x following current policy.
policy function assigns state-action pair (x; a) real number representing
relative merit performing action state x, called action merit . actual
policy determined action merits using some, usually stochastic, action selection
mechanism, e.g., according Boltzmann distribution (as described Section 5).
optimal evaluation state x, V (x), expected total discounted reinforcement
received starting state x following optimal policy.
functions updated step t, executing action state xt,
according following rules:

updateff(V; xt ; rt + Vt(xt+1 ) , Vt(xt));
updatefi (f; xt; at; rt + Vt(xt+1) , Vt (xt)).
292

fiTruncating Temporal Differences

update rule V -function directly corresponds Equation 6 = 0. update
rule policy function increases decreases action merit action depending
whether long-term consequences appear better worse expected.
present this, simplified form AHC corresponding TD(0), paper proposes
alternative way using TD( > 0) implemented original AHC algorithm
presented Sutton (1984).
2.2.2 Q-Learning Algorithm

Q-learning learns single function states actions, called Q-function .
state-action pair (x; a) assigns Q-value action utility Q(x; a), estimate
discounted sum future reinforcement values received starting state x executing
action following greedy policy respect current Q-function (i.e.,
performing state actions maximum Q-values). current policy implicitly
defined Q-values. optimal Q-function learned, greedy policy
respect action utilities optimal policy.
update rule Q-function is:
updateff(Q; xt; at; rt + maxa Qt (xt+1; a) , Qt (xt; at)).
show correspondence TD(0) version Equation 6, simply assume
predicted state utilities represented Q-values Qt (xt; at) corresponds Ut (xt)
maxa Qt (xt+1 ; a) corresponds Ut (xt+1).
2.2.3 Advantage Updating Algorithm

advantage updating two functions maintained: evaluation function V
advantage function A. evaluation function essentially interpretation
counterpart AHC, though learned different way. advantage function assigns
state-action pair (x; a) real number A(x; a) representing degree
expected discounted sum future reinforcement increased performing action
state x, relative action currently considered best state. optimal action
advantages negative suboptimal actions equal 0 optimal actions,
related optimal Q-values by:
(x; a) = Q(x; a) , max
Q(x; a0):

0

Similarly action utilities, action advantages implicitly define policy.
evaluation advantage functions updated step applying following
rules:
updateff(A; xt; at; maxa At(xt; a) , At(xt; at) + rt + Vt(xt+1) , Vt (xt));
updatefi (V; xt; ff1 [maxa At+1 (xt) , maxa At(xt )]).
update rule advantage function somewhat complex AHC
Q-learning rules, still contains term directly corresponds TD(0) form
Equation 6, replacing V U .
Actually, presented simplified version advantage updating.
original algorithm differs two details:
293

fiCichosz

time step duration explicitly included update rules,
presentation assumed = 1,
besides learning updates , described above, called normalizing updates performed.

3. Eligibility Traces

obvious direct implementation computation described Equation 6
tempting. requires maintaining x (t) values state
x past time step t.
P
Note, however, one needs maintain whole sums tk=0 ( )t,kx (k) x
one (current) t, much easier due simple trick. Substituting

ex (t) =


X

( )t,k x (k);

k=0

define following recursive update rule:

ex(0) =
ex(t) =

(
(

1 x0 = x
0 otherwise;
ex(t , 1) + 1 xt = x
ex(t , 1)
otherwise:

(8)

quantities ex (t) defined way called activity eligibility traces (Barto,
Sutton, & Anderson, 1983; Sutton, 1984; Watkins, 1989). Whenever state visited,
activity becomes high gradually decays visited again. update
predicted utility state x resulting visiting state xt time may
written
x (t) = (rt + Ut(xt+1) , Ut (xt ))ex(t);
(9)
direct transformation Equation 6.
technique (with minor differences) already used early works Barto
et al. (1983) Sutton (1984), actual formulation TD(). especially
suitable use parameter estimation function representation methods, connectionist networks. Instead one ex value state x one one ei
value weight wi . eligibility traces actually used Barto et al.
(1983) Sutton (1984), inspired earlier work Klopf (1982). Note case
AHC algorithm, different values may used maintaining traces used
evaluation policy functions.
Unfortunately, technique eligibility traces general enough easy implement arbitrary function representation method. clear, example,
could used important class function approximators memory-based
(or instance-based) function approximators (Moore & Atkeson, 1992). Applied pure
tabular representation, significant drawbacks. First, requires additional memory locations, one per state. Second, even painful, requires modifying U (x)
ex x time step. operation dominates computational complexity
294

fiTruncating Temporal Differences

TD-based reinforcement learning algorithms, makes using TD( > 0) much expensive TD(0). eligibility traces implementation TD() thus, large state
spaces, absolutely impractical serial computers, unless appropriate function approximator used allows updating function values eligibility traces many states
concurrently (such multi-layer perceptron). even approximator
used, still significant computational (both memory time) additional costs
using TD() > 0 versus TD(0). Another drawback approach revealed
Section 4.1.

4. Truncating Temporal Differences

section departs alternative formulation TD() reinforcement learning.
follow relating TD() training errors used alternative formulation
TD() returns. Finally, propose approximating TD() returns truncated TD()
returns, show computed used on-line reinforcement learning.

4.1 TD Errors TD Returns

Let us take closer look Equation 7. Consider effects experiencing sequence
states x0 ; x1; : : :; xk ; : : : corresponding reinforcement values r0; r1; : : :; rk ; : : :.
sake simplicity, assume states sequence different (though
course impossible finite state spaces). Applying Equation 7 state xt
assumption have:

xt = rt +h Ut(xt+1) , Ut(xt ) +

rt+1 + Ut+1(xt+2) , Ut+1(xt+1) +
h

( )2 rt+2 + Ut+2(xt+3 ) , Ut+2 (xt+2) + : : :
1
X

=

k=0

h



( )k rt+k + Ut+k (xt+k+1 ) , Ut+k (xt+k ) :

state occurs several times sequence, visit state yields similar update.
simple observation opens way alternative (though equivalent) formulation
TD(), offering novel implementation possibilities.
Let
0t = rt + Ut(xt+1 ) , Ut(xt)
(10)
TD (0) error time step t. define TD () error time using TD(0) errors
follows:

=

1
X

h



( )k rt+k + Ut+k (xt+k+1 ) , Ut+k (xt+k ) =

k=0

1
X

( )k 0t+k :

k=0

(11)

Now, express overall TD() error state x, x , terms errors:

x =

1
X
t=0

tx (t):

295

(12)

fiCichosz

fact, Equation 7 have:

x =

1
X
t=0

0t


X

k=0

( )t,k x (k) =

1 X

X

( )t,k0t x (k):

(13)

t=0 k=0

Swapping order two summations get:

x =

1 X
1
X

k=0 t=k

( )t,k0t x (k):

Finally, exchanging k other, receive:

x =

1X
1
X
t=0 k=t

( )k,t0k x (t) =

1 X
1
X

t=0 k=0

( )k 0t+k x (t) =

(14)
1
X
t=0

x(t):

(15)

Note following important difference x (t) (Equation 6) : former
computed time step x latter computed step
xt. Accordingly, step error value x (t) used adjusting U (x) x
used adjusting U (xt). crucial learning procedure proposed
Section 4.2. applying defined errors on-line makes changes predicted
state utilities individual steps clearly different described Equation 6,
overall effects experiencing whole state sequence (i.e., sums individual error
values state) equivalent, shown above.
expressed TD() terms errors, gain insight operation role . definitions helpful. Recall TD return time
defined
1
X
zt = k rt+k :
k=0

m-step truncated TD return (Watkins, 1989; Barto et al., 1990) received taking
account first terms sum, i.e.,
mX
,1
[

]
zt = k rt+k :
k=0

Note, however, rejected terms rt+m + m+1rt+m+1 + : : : approximated
mUt+m,1 (xt+m ). corrected m-step truncated TD return (Watkins, 1989; Barto et al.,

1990) thus:

zt(m) =

mX
,1
k=0

k rt+k + mUt+m,1 (xt+m ):

Equation 11 may rewritten following form:



=
=

1
X

h



( )k rt+k + (1 , )Ut+k (xt+k+1 ) + Ut+k (xt+k+1 ) , Ut+k (xt+k )

k=0
1
X

h



( )k rt+k + (1 , )Ut+k (xt+k+1 ) , Ut (xt) +

k=0
1
X

h



( )k Ut+k,1 (xt+k ) , Ut+k (xt+k ) :

k=1

296

(16)

fiTruncating Temporal Differences

Note = 1 yields:

1t =

1
X
kr

k=0

t+k , Ut (xt) +

= zt , Ut (xt

1 h
X
k U

k=1

t+k,1 (xt+k ) , Ut+k (xt+k )

1 h
X
) + k U
k=1





t+k,1 (xt+k ) , Ut+k (xt+k )

:

relax moment assumption on-line learning mode leave time
subscripts U values, last term disappears simply have:

1t = zt , U (xt ):
Similarly general , define TD () return (Watkins, 1989) time
weighted average corrected truncated TD returns:
1
1
h

X
X
(
k
+1)

k
zt = (1 , ) zt = ( )k rt+k + (1 , )Ut+k (xt+k+1 )
k=0
k=0

(17)

omit time subscripts, receive:

= zt , U (xt):

(18)

last equation brings light exact nature computation performed
TD(). error time step difference TD() return step
predicted utility current state, is, learning error value
bring predicted utility closer return. = 1 quantity zt usual TD
return time t, i.e., discounted sum future reinforcement values.4 < 1
term rt+k replaced rt+k + (1 , )Ut+k (xt+k+1 ), is, actual immediate reward
augmented predicted future reward.
definition TD() return (Equation 17) may written recursively

zt = rt + (zt+1 + (1 , )Ut(xt+1 )):

(19)

probably best explains role TD() learning. determines return
used improving predictions obtained. = 1, exactly actual observed
return, discounted sum rewards. = 0 1-step corrected truncated
return, i.e., sum immediate reward discounted predicted utility
successor state. Using 0 < < 1 allows smoothly interpolate two extremes,
relying partially actual returns partially predictions.
Equation 18 holds true batch learning mode, fact TD methods
originally formulated batch learning. incremental version, practically useful,
4. observation corresponds equivalence \generic" TD() = 1 supervised learning
shown Sutton (1988). receive result necessary discount prediction differences
instead alone Equation 6, though Sutton presenting RL-oriented form TD make
modification.

297

fiCichosz

introduces additional term. Let Dt designate term. comparing Equations 16
17 get:

Dt = , (zt , Ut(xt)) =

1
X

k=1

h



( )k Ut+k,1 (xt+k ) , Ut+k (xt+k ) :

(20)

magnitude discrepancy term, consequently uence learning
process, obviously depends learning rate value. examine further, suppose
learning rate used learning U basis errors. Let corresponding
learning rule be:
Ut+1(xt) := Ut(xt) + t:

Ut+1 (xt ) , Ut(xt) = (zt , Ut (xt )) + Dt
= (z , Ut (xt )) +



1
X

h

( )k Ut+k,1 (xt+k ) , Ut+k (xt+k )

k=1
1
X
(z , Ut (xt )) , 2 ( )kt+k,1;
k=1



(21)

equality xt+k = xt+k,1 k. similar result may obtained
eligibility traces implementation, learning driven x (t) errors defined Equation 9.
would have:

Ut+1 (xt) , Ut(xt) = (z , Ut(xt )) , 2

1
X

k=1

( )k0t+k,1 ext+k (t + k , 1):

(22)

effect may considered another drawback eligibility traces implementation
TD(), apart ineciency lack generality. Though small learning rates
effect Dt negligible, may still harmful cases, especially large
.5

4.2 TTD Procedure

shown TD errors zt , Ut (xt ) used almost equivalently TD()
learning, yielding overall results eligibility traces implementation, has,
however, important drawbacks practice. Nevertheless, impossible use either TD()
errors TD() returns zt on-line learning, since available. step
knowledge rt+k xt+k required k = 1; 2; : : :, way
implement practice. Recall, however, definition truncated TD return.
define truncated TD() error truncated TD() return? appropriate
definitions are:
mX
,1
;m
=
( )k0t+k
(23)

k=0
5. Sutton (1984) presented technique eligibility traces implementation recency
frequency heuristics . context, phenomenon examined may considered harmful
effect frequency heuristic. Sutton discussed example finite-state task heuristic might
misleading (Sutton, 1984, page 171).

298

fiTruncating Temporal Differences



zt;m =
=

mX
,2

h



h



h



( )k rt+k + (1 , )Ut+k (xt+k+1 ) + ( )m,1 rt+m,1 + Ut+m,1 (xt+m )

k=0
mX
,1

( )k rt+k + (1 , )Ut+k (xt+k+1 ) + ( )mUt+m,1 (xt+m ):

(24)

k=0
call ;m
m-step truncated TD() error, simply TTD (; m) error time
step t, zt;m m-step truncated TD() return, TTD (; m) return time t.
Note zt;m defined Equation 24 corrected , i.e., obtained simply truncating Equation 17. correction term ( )mUt+m,1 (xt+m ) results multiplying
last prediction Ut+m,1 (xt+m ) alone instead (1 , ), virtually equivalent
using = 0 step. done order include zt;m available infor-

mation expected returns time steps (t + m; + + 1; : : :) contained
Ut+m,1 (xt+m ). Without correction large information would almost
completely lost.
defined, m-step truncated TD() errors returns, used on-line learning
keeping track last visited states, updating step predicted
utility least recent state states. idea leads call TTD
Procedure (Truncated Temporal Differences ), good approximation TD()
suciently large m. procedure parameterized values. m-element
experience buffer maintained, containing records hxt,k ; at,k ; rt,k ; Ut,k (xt,k+1 )i
k = 0; 1; : : :; , 1, current time step. step writing x[k] , a[k] ,
r[k], u[k] refer corresponding elements buffer, storing xt,k , at,k , rt,k ,
Ut,k (xt,k+1 ).6 References U subscripted time steps, since
concern values available current time step | practical implementation
directly corresponds restoring function value function approximator
look-up table. notational convention, operation TTD(; m) procedure
presented Figure 1. uses TTD(; m) returns learning. alternative version, using
TTD(; m) errors instead (based Equation 11), also possible straightforward
formulate, reason use \weaker" version (subject harmful effects
described Equations 20 21) \stronger" one available cost.
beginning learning, first steps made, learning take
place. initial steps operation TTD procedure reduces updating
appropriately contents experience buffer. obvious technical detail left
Figure 1 sake simplicity.
TTD(; m) return value z computed step 5 repeated application
Equation 19. computational cost propagating return time acceptable
practice reasonable values m. function representation methods,
neural networks, overall time complexity dominated costs retrieving
function value learning performed steps 4 6, cost computing z
negligible. One advantage implementation allows use adaptive values:
step 5 one use k depending whether a[k,1] non-policy action,
6. naturally means buffer's indices shifted appropriately time tick.

299

fiCichosz

time step t:
1. observe current state xt ; x[0] := xt ;
2. select action state xt ; a[0] := ;
3. perform action ; observe new state xt+1 immediate reinforcement rt;
4. r[0] := rt; u[0] := U (xt+1 );
5. k = 0; 1; : : :; , 1
k = 0 z := r[k] + u[k]
else z := r[k] + (z + (1 , )u[k]);
6. update (U; x[m,1] ; a[m,1]; z , U (x[m,1]));
7. shift indices experience buffer.
Figure 1: TTD(; m) procedure.
\how much" non-policy was. refinement TD() algorithm suggested
Watkins (1989) recently Sutton Singh (1994). Later see TTD return
computation performed fully incremental way, using constant time step
arbitrary m.
Note function update carried step 6 time applies state
action time , + 1, i.e., , 1 time steps earlier. delay experience
event learning might found potential weakness presented approach, especially
large m. Note, however, baseline computing error value current utility
U (x[m,1] ) = Ut(xt,m+1) used. important point, guarantees
learning desired effect moving utility (whatever value currently has)
towards corresponding TTD return. error used step 6 z , Ut,m (xt,m+1 )
instead z , Ut (xt,m+1 ), applying learning time would problematic.
Anyway, seems large.
TTD procedure exact implementation TD methods two reasons.
First, approximates TD() returns TTD(; m) returns. Second, introduces
aforementioned delay experience learning. believe, however,
possible give strict conditions convergence properties TD() hold
true TTD implementation.
4.2.1 Choice

reasonable choice obviously depends . = 0 best possible = 1
= 1 = 1 finite value large enough accurately approximate
TD(). Fortunately, seem painful. rather unlikely
application one wanted use combination = 1 = 1, existing
300

fiTruncating Temporal Differences

previous empirical results TD() indicate = 1 usually optimal value
use, best comparable other, smaller values (Sutton, 1984; Tesauro, 1992;
Lin, 1993). Similar conclusions follow discussion choice presented
Watkins (1989) Lin (1993). < 1 < 1 would probably like
value discount ( )m small number. One possible definition `small'
could be, e.g., `much less '. obviously completely informal criterion.
Table 1 illustrates practical effects heuristic. hand, large m,
delay experience learning introduced TTD procedure might become
significant cause problems. experiments described Section 5
designed order test different values fixed 0 < < 1.


0:99 0:975 0:95 0:9 0:8 0:6
minfm j ( )m < 101 g 231 92
46 23 12 6
Table 1: Choosing m: illustration.
4.2.2 Reset Operation

now, assumed learning process, started, continues infinitely
long. true episodic tasks (Sutton, 1984) many real-world tasks,
learning must usually stop time. imposes necessity designing
special mechanism TTD procedure, called reset operation . reset
operation would invoked end episode episodic tasks,
overall end learning.
much done. problem must dealt
experience buffer contains record last steps learning taken
place yet, steps would make learning remaining
steps possible. implementation reset operation find natural
coherent TTD procedure simulate additional fictious steps,
learning takes place real steps left buffer, TTD returns
remain unaffected simulated fictious steps. corresponding algorithm, presented
Figure 2, formulated replacement original algorithm Figure 1
final time step. final step, successor state, fictious successor
state utility assumed 0. corresponds assigning 0 u[0] . actual reset
operation performed step 5.
4.2.3 Incremental TTD

stated above, cost iteratively computing TTD(; m) return relatively small
reasonable m, function representation methods, restoring
updating function values computationally expensive, may really negligible. also
argued reasonable values large. hand, iterative
return computation easy understand ects well idea TTD.
301

fiCichosz

final time step t:
1. observe current state xt ; x[0] := xt ;
2. select action state xt ; a[0] := ;
3. perform action ; observe immediate reinforcement rt ;
4. r[0] := rt; u[0] := 0;
5. k0 = 0; 1; : : :; , 1
(a) k = k0; k0 + 1; : : :; , 1
k = k0 z := r[k] + u[k]
else z := r[k] + (z + (1 , )u[k]);
(b) update (U; x[m,1] ; a[m,1]; z , U (x[m,1]));
(c) shift indices experience buffer.
Figure 2: reset operation TTD(; m) procedure.
presented TTD procedure form. possible, however, compute
TTD(; m) return fully incremental manner, using constant time arbitrary m.
see this, note definition TTD(; m) return (Equation 24) may
rewritten following form:

zt;m =
=

mX
,1

( )krt+k +

mX
,2

( )k (1 , )Ut+k (xt+k+1 ) + ( )m,1 Ut+m,1 (xt+m )

k=0
k=0
;m
;m
St + Tt + Wt;m;



St;m =
Tt;m =
Wt;m

mX
,1
k=0
mX
,2
k=0

( )krt+k ;
( )k (1 , )Ut+k (xt+k+1 );

= ( )m,1 Ut+m,1(xt+m ):

Wt;m directly computed constant time m. dicult convince

oneself that:

1 ;m

St;m
+1 = St , rt + ( ) rt+m ;
1 hT ;m , (1 , )U (x ) + (1 , )W ;mi :
Tt;m
=
t+1

+1

h



302

(25)
(26)

fiTruncating Temporal Differences

two equations define algorithm computing incrementally St;m Tt;m,
consequently computing zt;m constant time arbitrary m, small computational expense. algorithm strictly mathematically equivalent algorithm
presented Figure 1.7 Modifying appropriately TTD procedure straightforward
discussed. drawback modification probably allow
learner use different (adaptive) values step, i.e., may possible
combine refinements suggested Watkins (1989) Sutton Singh (1994).
Despite this, implementation might beneficial one wanted use really large m.
4.2.4 TTD-Based Implementations RL Algorithms

implement particular TD-based reinforcement learning algorithms basis
TTD procedure, one substitute appropriate function values U , define
updating operation step 6 Figure 1 step 5b Figure 2. Specifically,
three algorithms outlined Section 2.2 one should:

AHC:
1. replace U (xt+1) V (xt+1 ) step 4 (Figure 1);
2. implement step 6 (Figure 1) step 5b (Figure 2) as:
v := V (x[m,1] );
updateff(V; x[m,1] ; z , v );
updatefi (f; x[m,1] ; a[m,1]; z , v );

Q-learning:
1. replace U (xt+1) maxa Q(xt+1; a) step 4 (Figure 1);
2. implement step 6 (Figure 1) step 5b (Figure 2) as:
updateff(Q; x[m,1] ; a[m,1]; z , Q(x[m,1] ; a[m,1]));

advantage updating:
1. replace U (xt+1) V (xt+1 ) step 4 (Figure 1);

2. implement step 6 (Figure 1) step 5b (Figure 2) as:
Amax := maxa A(x[m,1] ; a);
updateff(A; x[m,1]; a[m,1]; Amax , A(x[m,1] ; at) + z , V (x[m,1]));
updatefi (V; x[m,1]; ff1 [maxa A(x[m,1]) , Amax]).

4.3 Related Work

simple idea truncating temporal differences implemented TTD procedure new. probably first suggested Watkins (1989). paper owes much
work. But, best knowledge, idea never explicitly
7. necessarily numerically equivalent, may sometimes cause problems practical
implementations.

303

fiCichosz

exactly specified, implemented, tested. sense TTD procedure original
development.
Lin (1993) used similar implementation TD(), called
experience replay , actual on-line reinforcement learning. approach sequence past experiences replayed occasionally, replay experience
TD() return (truncated length replayed sequence) computed applying Equation 19, corresponding function update performed. learning
method means computationally expensive TTD procedure (especially implemented fully incremental manner, suggested above), since requires
updating predictions sequentially replayed experiences, besides \regular" TD(0) updates performed step (while TTD always requires one update per time step),
allow learner take full advantage TD( > 0), applied
occasionally.
Peng Williams (1994) presented alternative way combining Q-learning
TD(), different discussed Section 2.2. motivation better estimate TD
returns use TD errors. Toward end, used standard Q-learning error

rt + max
Qt (xt+1 ; a) , Qt (xt; )
one-step updates modified error

rt + max
Qt (xt+1 ; a) , max
Qt (xt ; a);
propagated using eligibility traces, thereafter. TTD procedure achieves similar objective straightforward way, use truncated TD() returns.
related work Pendrith (1994). applied idea eligibility traces
non-standard way estimate TD returns. approach computationally ecient
classical eligibility traces technique (it requires one prediction update per time
step) free potentially harmful effect described Equation 22. method
seems roughly equivalent TTD procedure = 1 large m, though
probably much implementationally complex.

5. Demonstrations

demonstrations presented section use AHC variant TTD procedure.
reason AHC algorithm simplest three described algorithms
update rule evaluation function directly corresponds TD(). Future work
investigate TTD procedure two algorithms.
tabular representation evaluation policy functions used. abstract
function update operation described Equation 2 implemented standard way

'(p0; p1; : : :; pn,1 ) := '(p0; p1; : : :; pn,1 ) + :

(27)

Actions execute step selected using simple stochastic selection mechanism based Boltzmann distribution. According mechanism, action selected
304

fiTruncating Temporal Differences

state x probability

Prob(x; a) = Pexp(f (x; )=T ) ;
exp(f (x; a)=T )

(28)

temperature > 0 adjusts amount randomness.

5.1 Car Parking Problem

section presents experimental results learning control problem relatively
large state space hard temporal credit assignment. call problem car parking
problem, though attempt simulate real-world problem all. Using words
`car', `garage', `parking' convention simplifies problem description
interpretation results. primary purpose experiments neither
solve problem provide evidence usefulness tested algorithm
particular practical problem. use example problem order illustrate
performance AHC algorithm implemented within TTD framework
empirically evaluate effects different values TTD parameters m.
car parking problem illustrated Figure 3. car, represented rectangle,
initially located somewhere inside bounded area, called driving area. garage
rectangular area size somewhat larger car. important dimensions
distances shown figure. agent | driver car | required park
garage, car entirely inside. task episodic, though neither
time-until-success time-until-failure task (in Sutton's (1984) terminology), rather
combination both. episode finishes either car enters garage
hits wall (of garage driving area). episode car reset
initial position.
5.1.1 State Representation

state representation consists three variables: rectangular coordinates center
car, x , angle car's axis x axis coordinate
system. orientation system shown figure. initial location
orientation car fixed described x = 6:15 m, = 10:47 m, = 3:7 rad.
chosen make task neither easy dicult.
5.1.2 Action Representation

admissible actions `drive straight on', `turn left', `turn right'. action
driving straight effect moving car forward along axis, i.e., without
changing . actions turning left right equivalent moving along arc
fixed radius. distance move determined constant car velocity v
simulation time step . Exact motion equations details given Appendix A.
5.1.3 Reinforcement Mechanism

design reinforcement function fairly straightforward. agent receives
reinforcement value 1 (a reward) whenever successfully parks car garage,
305

fiCichosz

y0

x

xG

x1

0

x0

yG

l

w

y1

0

1

2

3





Figure 3: car parking problem. scale dimensions preserved: w = 2 m,
l = 4 m, x0 = ,1:5 m, xG = 1:5 m, x1 = 8:5 m, y0 = ,3 m, yG = 3 m, y1 = 13 m.
reinforcement value ,1 (a punishment) whenever hits wall. time
steps reinforcement 0. is, non-zero reinforcements received last
step episode. involves relatively hard temporal credit assignment problem,
providing good experimental framework testing eciency TTD procedure.
problem hard reinforcement delay, also punishments
much frequent rewards: much easier hit wall park car
correctly.
reinforcement mechanism presented above, optimal policy
0 < < 1 policy allows park car garage smallest possible
number steps.
306

fiTruncating Temporal Differences

5.1.4 Function Representation

car parking problem continuous state space. artificially discretized | divided
finite number disjoint regions quantizing three state variables,
function value region stored look-up table. quantization thresholds are:

x: ,0:5, 0:0, 0:5, 1:0, 2:0, 3:0, 4:0, 6:0 m,
y: 0:5, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 8:0, 10:0 m,
21
29 3 31
: 19
20 , , 20 , : : :, 20 , 2 , 20 rad.
yields 9 10 14 = 1260 regions. course many never visited.

threshold values chosen make resulting discrete state space moderate
size. quantization dense near garage, becomes sparse distance
garage increases.
5.1.5 Experimental Design Results

experiments applying TTD procedure car parking problem divided
two studies, testing effects two TTD parameters m. parameter
settings experiments presented Table 2. symbols ff fi used
designate learning rates evaluation policy functions, respectively. initial
values functions set 0, since assumed knowledge available
expected reinforcement levels.
Study TTD Parameters
Number

0
0:3
0:5
1
0:7
25
0:8
0:9
1
5
10
2
0:9
15
20

Learning Rates

ff
0:7
0:5
0:5
0:5
0:5
0:25
0:25
0:25
0:25
0:25
0:25

fi

0:7
0:5
0:5
0:5
0:5
0:25
0:25
0:25
0:25
0:25
0:25

Table 2: Parameter settings experiments car parking problem.
stated above, experiments designed test effects two TTD
parameters. parameters assigned values according following principles:

discount factor fixed equal 0:95 experiments,
307

fiCichosz

temperature value also fixed set 0:02, seemed equally good

experiments,
learning rates ff fi roughly optimized experiment.8
experiment continued 250 episodes, number selected allow
almost runs experiments converge. results presented experiments
averaged 25 individual runs, differing initial seed random
number generator. number chosen reasonable compromise reliability results computational costs. results presented plots average
reinforcement value per time step previous 5 consecutive episodes versus episode
number.
Study 1: Effects . objective study examine effects various
values learning speed quality, set 25. value = 25 found
large enough tested values (perhaps except = 1).9 Smaller values might
used small (in particular, = 1 = 0), kept constant consistency.
Reinf/Step
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
0
50

= 0:0
= 0:3
= 0:5
= 0:7
100 150
Episode

200

Reinf/Step
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
250
0
50

= 0:7
= 0:8
= 0:9
= 1:0
100 150
Episode

200

250

Figure 4: car parking problem, learning curves study 1.
learning curves study presented Figure 4. observations
brie summarized follows:
= 0 gives worst performance (not 25 runs managed converge
within 250 episodes),
increasing improves learning speed,
values equal 0:7 similarly effective, greatly outperforming = 0
clearly better = 0:5,
8. optimization procedure cases follows: rather large value tested
runs; give effects overtraining premature convergence, accepted; otherwise
(usually twice) smaller value tried, etc.
9. Note = 0:9, = 25, = 0:95 ( )m 0:02 0:855 = .

308

fiTruncating Temporal Differences

using large caused necessity reducing learning rates (cf. Table 2) ensure

convergence.
main result using large TTD procedure (including 1) always
significantly improved performance. quite consistent empirical results
Sutton (1988), found performance TD() best intermediate ,
worst = 1. Lin (1993), used > 0 experience replay experiments, reported
close 1 successful, similarly work. speculated difference
results Sutton's might caused switching occasionally (for
non-policy actions) = 0 studies.10 results, obtained held fixed
time11 , suggest good explanation. seems likely optimal
value simply strongly depends particular problem. Another point neither
TTD(1; 25) Lin's implementation exactly equivalent TD(1).
Study 2: Effects m. study designed investigate effects using several
different values fixed relatively large value. best (approximately)
study 1 used, 0:9. smallest tested value 5, find rather
small value.12
Reinf/Step
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
0
50

m= 5
= 10
= 15
= 20
= 25
100 150
Episode

200

250

Figure 5: car parking problem, learning curves study 2.
learning curves study presented Figure 5. results = 25
taken study 1 comparison. observations summarized follows:
= 5 worst = 25 best,
differences intermediate values seem statistically
significant,
10. matter fact, non-policy actions replayed Lin's experience replay experiments.
11. Except using = 0 recent time step covered TTD return, follows
definition (Equation 24).
12. = 0:95, = 0:9, = 5 ( )m 0:457, means comparable
= 0:855.

309

fiCichosz

even smallest = 5 gives performance level much better obtained
study 1 small , i.e., even relatively small values allow us advantages
large , though larger values generally better small ones,
last observation probably important. also optimistic. suggests
that, least problems, TTD procedure > 0 allows obtain significant
learning speed improvement traditional TD(0)-based algorithms practically
additional costs, small space time complexity induced TTD
always negligible.

5.2 Cart-Pole Balancing Problem
experiments section one basic purpose: verify effectiveness
TTD procedure applying AHC implementation realistic complex problem,
long reinforcement delay, exist many previous results comparison.
cart-pole balancing problem, classical benchmark control specialists,
problem. particular, would like see whether possible obtain performance
(learning speed quality final policy) worse reported Barto
et al. (1983) Sutton (1984) using eligibility traces implementation.
Figure 6 shows cart-pole system. cart allowed move along one-dimensional
bounded track. pole move vertical plane cart track.
controller applies either left right force fixed magnitude cart time
step. task episodic: episode finishes failure occurs, i.e., pole falls
cart hits edge track. objective delay failure long possible.
problem realistically simulated numerically solving system differential
equations, describing cart-pole system. equations simulation details
given Appendix B. parameters simulated cart-pole system exactly
used Barto et al. (1983).
5.2.1 State Representation

state cart-pole system described four state variables:

x | position cart track,
x_ | velocity cart,
| angle pole vertical,
_ | angular velocity pole.
5.2.2 Action Representation

step agent controlling cart-pole system chooses one two possible
actions applying left right force cart. force magnitude fixed
equal 10 N.
310

fiTruncating Temporal Differences



2l

F

x


Figure 6: cart-pole system. F force applied cart's center, l half
pole length, half length track.
5.2.3 Reinforcement Mechanism

agent receives non-zero reinforcement values (namely ,1) end
episode, i.e., failure. failure occurs whenever jj > 0:21 rad (the pole begins
fall) jxj > 2:4 (the cart hits edge track). Even beginning learning,
poor policy, episode may continue hundreds time steps, may
many steps bad action resulting failure. makes temporal
credit assignment problem cart-pole task extremely hard.
5.2.4 Function Representation

case car parking problem, deal continuous state space
cart-pole system dividing disjoint regions, called boxes Mitchie Chambers
(1968). quantization thresholds used Barto et al. (1983), i.e.:

x: ,0:8, 0:8 m,
x_ : ,0:5, 0:5 m/s,
: ,0:105, ,0:0175, 0, 0:0175, 0:105 rad,
_: ,0:8727, 0:8727 rad/s,
yields 3 3 6 3 = 162 boxes. box memory location, storing
function value box.
311

fiCichosz

5.2.5 Experimental Design Results

Computational expense prevented extensive experimental studies car parking
problem. one experiment carried out, intended replication experiment presented Barto et al. (1983). values TTD parameters seemed
best previous experiments used, = 0:9 = 25. discount
factor set 0:95. learning rates evaluation policy functions
roughly optimized small number preliminary runs equal ff = 0:1 fi = 0:05,
respectively. temperature Boltzmann distribution action selection mechanism
set 0:0001, give nearly-deterministic action selection. initial values
evaluation policy functions set 0. attempt strictly replicate
learning parameter values work Barto et al. (1983), since used
different TD() implementation13 , also different policy representation (based
fact two actions, representation general), action
selection mechanism (for reasons), function learning rule.
experiment consisted 10 runs, differing initial seed random
number generator, presented results averaged 10 runs. run continued 100 episodes. individual runs terminated 500; 000 time steps,
completing 100 episodes. produce reliable averages 100 episodes, fictious
remaining episodes added runs, duration assigned according
following principle, used experiments Barto et al. (1983). duration
last, interrupted episode less duration immediately preceding (complete) episode, fictious episodes assigned duration preceding episode.
Otherwise, fictious episodes assigned duration last (incomplete) episode.
prevented short interrupted episodes producing unreliably low averages.
results presented Figure 7 plots average duration (the number time steps)
previous 5 consecutive episodes versus episode number, linear logarithmic
scale.
observe TTD-based AHC achieved similar (slightly better, exact)
performance level, learning speed quality final policy (i.e.,
balancing periods), reported Barto et al. (1983). final balancing periods lasted
130; 000 steps, average. obtained without using 162 additional memory
locations storing eligibility traces, without expensive computation necessary
update time step, well evaluation policy function values.

5.3 Computational Savings

experiments presented illustrate computational savings possible
TTD procedure conventional eligibility traces. direct implementation eligibility
traces requires computation proportional number states, i.e., 1260 car
parking task 162 cart-pole task | potentially many larger tasks.
Even straightforward iterative version TTD may beneficial, requires
computation proportional m, may reasonably assumed many times less
13. eligibility traces implementation, eligibility traces updated applying somewhat
different update rule specified Equation 8. particular, discounted alone
instead . Moreover, two different values used evaluation policy functions.

312

fiTruncating Temporal Differences

Episode Duration
140000
120000
100000
80000
60000
40000
20000
0
0
20

(a)

40 60
Episode

80

Episode Duration
100000
10000
1000
100
10
1
100
0
20

(b)

40 60
Episode

80

100

Figure 7: cart-pole balancing problem, learning curve (a) linear (b) logarithmic
scale.
size state space. course, incremental version TTD, requires
always small computation independent m, much ecient.
many practical implementations, improve eciency, eligibility traces predictions updated relatively recently visited states. Traces maintained
n recently visited states, eligibility traces states assumed
0.14 even \ecient" version eligibility traces, savings offered
TTD considerable. good approximation infinite traces tasks considered here, n least large m. conventional eligibility traces,
always concern keeping n low, reducing , , accuracy approximation.
problem occurs iterative TTD,15 incremental TTD, hand,
none issue. small computation needed independent m.

6. Conclusion
informally derived TTD procedure analysis updates introduced
TD methods predicted utilities states, shown approximated use truncated TD() returns. Truncating temporal differences allows easy
ecient implementation. possible compute TTD returns incrementally constant time, irrespective value (the truncation period), computational
expense using TD-based reinforcement learning algorithms > 0 negligible (cf.
Equations 25 26). cannot achieved eligibility traces implementation.
latter, even function representation methods particularly well
14. modification cannot applied parameter estimation function representation technique
used (e.g., multi-layer perceptron), traces maintained weights rather states.
15. relative computational expense iterative TTD \ecient" version eligibility traces
depends cost function update operation, always performed one state
former, n states latter.

313

fiCichosz

suited (e.g., neural networks), always associated significant memory time costs.
TTD procedure probably computationally ecient (although approximate)
on-line implementation TD(). also general, equally good function representation method might used.
important question concerning TTD procedure whether computational
eciency obtained cost reduced learning eciency. low computational costs per control action may attractive number actions necessary
converge becomes large. now, theoretically grounded answer important
question provided, though unlikely answer eventually
found. Nevertheless, informal consideration may suggest TTD-based implementation TD methods perform worse classical
eligibility traces implementation, even advantages. follows
Equations 20, 21, 22, using TD(0) errors on-line TD() learning, eligibility
traces implementation, introduces additional discrepancy term, whose uence
learning process proportional square learning rate. term, though often
negligible, may still harmful certain cases, especially tasks agent likely
stay states long periods. TTD procedure, based truncated TD()
returns, free drawback.
Another argument supporting TTD procedure associated using large values,
particular 1. exact TD() implementation, provided eligibility
traces, means learning relies solely actually observed outcomes, without regard
currently available predictions. may beneficial early stages learning,
predictions almost completely inaccurate, general rather risky | actual
outcomes may noisy therefore sometimes misleading. TTD procedure never
relies entirely, even = 1, since uses m-step TTD returns finite m,
corrected always using = 0 discounting predicted utility recent step
covered return (cf. Equation 17). deviation TTD procedure TD()
may turn advantageous.
TTD procedure using TTD returns learning suitable implementation TD methods applied reinforcement learning. RL part
predicted outcome available step, current reinforcement value. However,
straightforward formulate another version TTD procedure, using truncated
TD() errors instead truncated TD() returns, would cover whole scope
applications generic TD methods.
experimental results obtained TTD procedure seem promising. results presented Section 5.1 show using large TTD procedure give significant performance improvement simple TD(0) learning, even relatively small m.
say anything relative performance TTD eligibility
traces implementation TD(), least suggests TTD procedure useful.
best results obtained largest values, including 1. observation,
contradicting results reported Sutton (1988), may positive consequence
TTD procedure's deviation TD() discussed above.
experiments cart-pole balancing problem supplied empirical evidence
learning control problem long reinforcement delay TTD procedure
equal outperform eligibility traces implementation TD(), even value
314

fiTruncating Temporal Differences

many times less average duration episode. performance level obtained
TTD procedure much lower computational (both memory time) expense.
summarize, informal consideration empirical results suggest TTD
procedure may following advantages:

possibility implementation reinforcement learning algorithms may
viewed instantiations TD(), using > 0 faster learning,
computational eciency: low memory requirements (for reasonable m) little computation per time step,

generality, compatibility various function representation methods,
good approximation TD() < 1 (or = 1 < 1),
good practical performance, even relatively small m.
seems one important drawback: lack theoretical analysis convergence proof. know either parameter values assure convergence
values make impossible. particular, estimate available potential harmful
effects using large m. advantages drawbacks cause TTD procedure interesting promising subject work. work concentrate,
one hand, examining theoretical properties technique, and,
hand, empirical studies investigating performance various TD-based reinforcement
learning algorithms implemented within TTD framework variety problems,
particular stochastic domains.

Appendix A. Car Parking Problem Details

motion car experiments Section 5.1 simulated applying
time step following equations:
1. r 6= 0
(a) (t + ) = (t) + vr ;
(b) x(t + ) = x(t) , r sin (t) + r sin (t + );
(c) (t + ) = (t) + r cos (t) , r sin (t + );
2. r = 0
(a) (t + ) = (t);
(b) x(t + ) = x(t) + v cos (t);
(c) (t + ) = (t) + v sin (t);
r turn radius, v car's velocity, simulation time step.
experiments r = ,5 used `turn left' action, r = 5 `turn right', r = 0
`drive straight on'. velocity constant set 1 m/s, simulation time
315

fiCichosz

step = 0:5 used. parameter settings, shortest possible path
car's initial location (x = 6:15 m, = 10:47 m, = 3:7 rad) garage requires 21 steps.
step, determining current x, , values, coordinates
car's corners computed. test intersection side car
lines delimiting driving area garage performed determine whether failure
occurred. result negative, test performed corner car whether
inside garage, determine success occurred.

Appendix B. Cart-Pole Balancing Problem Details

dynamics cart-pole system described following equations motion:
h

F (t) + mpl _2(t) sin (t) , cos (t) , c sgn x_ (t)
x(t) =
mc + mp




2
(t)+c sgn x_ (t)
g sin (t) + cos (t) ,F (t),mp l_ (mt)sin
c +mp
h
(t) =
2 (t)
l 43 , mmp cos
c +mp



, mp_p(lt)

= 9:8 m/s2 | acceleration due gravity,
= 1:0 kg | mass cart,
= 0:1 kg | mass pole,
= 0:5
| half pole length,
= 0:0005 | friction coecient cart track,
= 0:000002 | friction coecient pole cart,
= 10:0 N | force applied center cart time t.
equations simulated using Euler's method simulation time step = 0:02 s.

g
mc
mp
l
c
p
F (t)

Acknowledgements
wish thank anonymous reviewers paper many insightful comments.
unable follow suggestions, contributed much improving paper's
clarity. Thanks also Rich Sutton, whose assistance preparation final
version paper invaluable.
research partially supported Polish Committee Scientific Research
Grant 8 S503 019 05.

References

Baird, III, L. C. (1993). Advantage updating. Tech. rep. WL-TR-93-1146, Wright Laboratory, Wright-Patterson Air Force Base.
Barto, A. G. (1992). Reinforcement learning adaptive critic methods. White, D. A.,
& Sofge, D. A. (Eds.), Handbook Intelligent Control, pp. 469{491. Van Nostrand
Reinhold, New York.
316

fiTruncating Temporal Differences

Barto, A. G., Sutton, R. S., & Anderson, C. (1983). Neuronlike adaptive elements
solve dicult learning control problems. IEEE Transactions Systems, Man,
Cybernetics, 13, 835{846.
Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. (1990). Learning sequential
decision making. Gabriel, M., & Moore, J. (Eds.), Learning Computational
Neuroscience. MIT Press.
Cichosz, P. (1994). Reinforcement learning algorithms based methods temporal
differences. Master's thesis, Institute Computer Science, Warsaw University
Technology.
Dayan, P. (1992). convergence TD() general . Machine Learning, 8, 341{362.
Dayan, P., & Sejnowski, T. (1994). TD() converges probability 1. Machine Learning,
14, 295{301.
Heger, M. (1994). Consideration risk reinforcement learning. Proceedings
Eleventh International Conference Machine Learning (ML-94). Morgan Kaufmann.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1993). convergence stochastic iterative
dynamic programming algorithms. Tech. rep. 9307, MIT Computational Cognitive
Science. Submitted Neural Computation .
Klopf, A. H. (1982). Hedonistic Neuron: Theory Memory, Learning, Intelligence. Washington D.C.: Hempisphere.
Lin, L.-J. (1992). Self-improving, reactive agents based reinforcement learning, planning
teaching. Machine Learning, 8, 293{321.
Lin, L.-J. (1993). Reinforcement Learning Robots Using Neural Networks. Ph.D. thesis,
School Computer Science, Carnegie-Mellon University.
Mitchie, D., & Chambers, R. A. (1968). BOXES: experiment adaptive control.
Machine Intelligence, 2, 137{152.
Moore, A. W., & Atkeson, C. G. (1992). investigation memory-based function approximators learning control. Tech. rep., MIT Artificial Intelligence Laboratory.
Pendrith, M. (1994). reinforcement learning control actions noisy
non-markovian domains. Tech. rep. UNSW-CSE-TR-9410, School Computer Science Engineering, University New South Wales, Australia.
Peng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. Proceedings
Eleventh International Conference Machine Learning (ML-94). Morgan Kaufmann.
Ross, S. (1983). Introduction Stochastic Dynamic Programming. Academic Press, New
York.
317

fiCichosz

Schwartz, A. (1993). reinforcement learning method maximizing undiscounted rewards. Proceedings Tenth International Conference Machine Learning
(ML-93). Morgan Kaufmann.
Singh, S. P. (1994). Reinforcement learning algorithms average-payoff markovian decision
processes. Proceedings Twelfth National Conference Artificial Intelligence
(AAAI-94).
Sutton, R. S. (1984). Temporal Credit Assignment Reinforcement Learning. Ph.D. thesis,
Department Computer Information Science, University Massachusetts.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine
Learning, 3, 9{44.
Sutton, R. S. (1990). Integrated architectures learning, planning, reacting based
approximating dynamic programming. Proceedings Seventh International
Conference Machine Learning (ML-90). Morgan Kaufmann.
Sutton, R. S., Barto, A. G., & Williams, R. J. (1991). Reinforcement learning direct
adaptive optimal control. Proceedings American Control Conference, pp.
2143{2146. Boston, MA.
Sutton, R. S., & Singh, S. P. (1994). step-size bias temporal-difference learning.
Proceedings Eighth Yale Workshop Adaptive Learning Systems, pp.
91{96. Center Systems Science, Yale University.
Tesauro, G. (1992). Practical issues temporal difference learning. Machine Learning, 8,
257{277.
Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, King's College,
Cambridge.
Watkins, C. J. C. H., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning,
8, 279{292.

318

fiJournal Artificial Intelligence Research 2 (1994) 89-110

Submitted 3/94; published 8/94

Pattern Matching Discourse Processing Information
Extraction Japanese Text
Tsuyoshi Kitani

tkitani@cs.cmu.edu

Yoshio Eriguchi
Masami Hara

eriguchi@rd.nttdata.jp
masami@rd.nttdata.jp

Center Machine Translation
Carnegie Mellon University
Pittsburgh, PA 15213 USA
Development Headquarters
NTT Data Communications Systems Corp.
66-2 Horikawa-cho, Saiwai-ku, Kawasaki-shi, Kanagawa 210 JAPAN

Abstract
Information extraction task automatically picking information interest
unconstrained text. Information interest usually extracted two steps.
First, sentence level processing locates relevant pieces information scattered throughout
text; second, discourse processing merges coreferential information generate
output. first step, pieces information locally identified without recognizing
relationships among them. key word search simple pattern search achieve
purpose. second step requires deeper knowledge order understand relationships
among separately identified pieces information. Previous information extraction systems
focused first step, partly required link piece
information pieces. link extracted pieces information map
onto structured output format, complex discourse processing essential.
paper reports Japanese information extraction system merges information using
pattern matcher discourse processor. Evaluation results show high level system
performance approaches human performance.

1. Introduction
recent information extraction systems, individual pieces information extracted directly text usually identified key word search simple pattern
search preprocessing stage (Lehnert et al., 1993; Weischedel et al., 1993; Cowie et al.,
1993; Jacobs et al., 1993). Among systems presented Fifth Message Understanding Conference (muc-5), however, main architectures ranged pattern matching
full fragment parsing (Onyshkevych, 1993). Full fragment parsing systems,
several knowledge sources syntax, semantics, domain knowledge combined
run-time, generally complicated changing part system tends affect
components. past information extraction research, interference slowed
development (Jacobs, 1993; Hobbs et al., 1992). pattern matcher, identifies
patterns interest, appropriate information extraction texts narrow
domains, since task require full understanding text.

c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiKitani, Eriguchi, & Hara

textract, information extraction system described here, uses pattern matcher
similar sri's fastus pattern matcher (Hobbs et al., 1992). matcher implemented
finite-state automaton. Unlike pattern matchers, textract's matcher deals
word matching problems caused word segmentation ambiguities often found
Japanese compound words.
goal pattern matcher identify concepts represented words
phrases text. pattern matcher first performs simple key-word-based concept
search, locating individual words associated concepts. second step template
pattern search locates phrasal patterns involving critical pieces information identified preprocessor. template pattern search identifies relationships
matched objects defined pattern well recognizing concept behind relationship. One typical concept relationship \economic activity" companies
participate other.
usually dicult determine relationships among pieces information
identified separate sentences. relationships often stated implicitly,
even text explicitly mentions descriptions often located far enough
apart make detection dicult. Although importance discourse processing information extraction emphasized Message Understanding Conferences (Lehnert
& Sundheim, 1991; Hirschman, 1992), system presented satisfactorily addressed
issue.
discourse processor textract able correlate individual pieces information
throughout text. textract merges concepts pattern matcher identified
separately (and usually different sentences) concepts involve companies. textract unify multiple references company even company
name missing, abbreviated, pronominalized. Furthermore, processor segments
discourse isolate portions text relevant particular conceptual relationship.
discourse segmentation lessens chance merging unrelated information (Kitani, 1994).
paper analyzes evaluation results textract's discourse module describes tipster/muc-5 evaluation results order assess overall system performance.

2. tipster information extraction task
goal tipster project sponsored arpa capture information interest
English Japanese newspaper articles corporate joint ventures microelectronics. system must fill generic template information extracted text
fully automated process. template composed several objects, containing several slots. Slots may contain pointers related objects (Tipster, 1992). Extracted
information stored object-oriented database.
joint ventures domain, task extract information concerning joint venture
relationships organizations form dissolve. template structure represents
relationships tie-up-relationship objects, contain pointers organization
entity objects representing organizations involved. Entity objects contain pointers
objects person facility objects, shown Figure 1.
microelectronics domain, extraction focuses layering, lithography, etching,
packaging processes semiconductor manufacturing microchip fabrication. entities
90

fiPattern Matching Discourse Processing

TEMPLATE
doc. no.
doc. date
doc. source
content (*)

ENTITY
name
aliases
location
nationality
type
entity rel. (+)
person(*)
facility (*)

TIE-UP-REL.
tie-up status
entity (+)
created
entity (*)
activity (*)

ACTIVITY
industry (*)

ENTITY-REL.
entity1 (+)
entity2 (*)
rel. ent2
ent1
PERSON
name
persons entity
position
FACILITY
name
location
type
INDUSTRY
industry type
product /
service

denotes instantiations multiple objects
(*) points zero objects, (+) points one objects

Figure 1: Object-oriented template structure joint ventures domain
extracted include manufacturer, distributor, user, addition detailed manufacturing
information materials used microchip specifications wafer size device
speed. microelectronics template structure similar joint ventures
fewer objects slots.
extraction tasks must identify individual entities also certain
relationships among them. Often, however, particular piece extracted information
describes part relationship. partial information must merged
pieces information referring entities. merging produce correct results,
therefore, correct identification entity references crucial.

3. Problem definition
section first describes word matching problems caused word segmentation ambiguities. Diculties reference resolution company names explained. Issues
discourse segmentation concept merging also discussed using example text.

3.1 Word segmentation
Japanese word segmentation preprocessor gives rise subsequent under-matching
problem. key word text found word segmentor's lexicon,
segmentor tends divide separate words. current lexicon, example,
91

fiKitani, Eriguchi, & Hara

compound noun \ F " (teikei-kaisyo), consisting two words, \ " (teikei: joint
venture) \ F " (kaisyo: dissolve), segmented two individual nouns. Thus
key word search \ F " (teikei-kaisyo) succeed segmented sentence.
hand, pattern matching process allows, default, partial matching
key word word text. \ " (teikei) \ [Z " (gyoum-teikei),
meaning \a joint venture", matched single key word \ "
(teikei). exibility creates over-matching problem. example, key word
\ JS " (silicon) matches \ f) JS" (nisanka-silicon: silicon dioxide), although
different materials reported microelectronics domain. segmentation diculties compound nouns also cause major problems word-based Japanese
information retrieval systems (Fujii & Croft, 1993).

3.2 Company name references

corporate joint ventures domain, output templates mostly describe relationships
among companies (as described Section 2). Information interest therefore found
sentences mention companies activities. essential extractor
identify topic companies|the main concern sentences appear in|in order
correlate information identified sentence. three problems make
dicult identify topic companies.
1. Missing subject
Topic companies usually subject sentence. Japanese sentences frequently
omit subjects, however|even formal newspaper articles. veniex system
nec presented muc-5 identify company implied missing subject
explicit reference immediately preceding sentence (Doi et al.,
1993; Muraki et al., 1993). clear whether veniex resolve missing
reference explicit reference appears sentence separated
subjectless sentence.
2. Company name abbreviations
also seen English, company names often abbreviated Japanese text first appearance. variety ways abbreviate company names Japanese
given (Karasawa, 1993). following examples show typical abbreviations
Japanese company names:
(a) partial word
\ AK' 9S$" ! \ 9S$ "
(Mercedes-Benz)
(Benz)
(b) English abbreviation
\ o%*NTT+ " ! \ NTT "
(Nippon Telegraph Telephone)
(c) first Katakana character + \ "
\ AJffS 7L " ! \ "
(American Express Corp.)
92

fiPattern Matching Discourse Processing
(d) first character primitive segment
\o % " ! \ "1
(Japan Airlines)
(e) randomly selected characters
\o % " ! \ "
(Shin-nihon Steel)
Locating company name abbreviations dicult, since many identified
companies either morphological analyzer name recognizer preprocessor. Another problem variety ways abbreviating names makes
dicult unify multiple references one company.
Almost muc-5 systems include string matching mechanism identify company name abbreviations. abbreviations specified aliases slot
company entity object. authors' knowledge, none systems
textract detect company name abbreviations type (d) (e) without
using pre-defined abbreviation table.
3. Company name pronouns
Company name pronouns often used formal texts. Frequently used expressions
include \ ! " (ryosya: companies), \ $ " (dosya: company), \ r
" (jisya: company itself). shown following examples, resolving
references particularly important full understanding text. Direct English
translation follows Japanese sentences.
(a) \ X/Y( $ .WR r 6IS)'K "

*Y+

*X+

\X Corp. tied Corp. sells products company
brand name."
(Y Corp.) (X Corp.)
(b) \ X/.'/ $ .p/NJ "

*X+

\X Corp. biggest company field. president company
Mr. Suzuki."
(X Corp.)
Reference resolution \ $ " (dosya: company) implemented veniex (Doi
et al., 1993). veniex resolves pronominal reference way identifies
missing company references. crl/brandeis diderot system presented muc5 simply chooses nearest company name referent \dosya". algorithm
later improved Wakao using corpus-based heuristic knowledge (Wakao, 1994).
systems handle pronominalized company names \dosya".
three problems described section often cause individual information
correlated wrong company tie-up-relationship object. avoid error,
topic companies must tracked context, since used determine
company objects information fragment assigned to. Abbreviated
pronominalized company names must unified references company.
1. \ o% " (nihon: Japan) \ " (koukuu: airlines) primitive segments example.
93

fiKitani, Eriguchi, & Hara

3.3 Discourse segmentation concept merging

joint ventures domain, tie-up-relationship object contains pointers objects
economic activities (as shown Figure 1). company involved multiple tie-ups, merging information tie-up relationship according topic companies
sometimes yields incorrect results. Consider following example:
"X Corp. tied Corp. X start selling products
Japan next month. Last year X started similar joint venture
Z Inc."

Obviously, sale second sentence related tie-up relationship X
Y. However, since topic company, subject sentence, X three
sentences, sale could also related X Z tie-up relationship. incorrect
merging avoided separating text two blocks: first two sentences
describe X tie-up, last sentence describes X Z tie-up. Thus,
discourse segmentation necessary identify portions text containing related pieces
information. crl/brandeis diderot system segments joint ventures text
two types text structures (Cowie et al., 1993). known well discourse
segmentation performed, however.
text segmented, concepts identified pieces information merged
within discourse segment. example, expected income joint venture
often stated sentence explicitly mention participating companies;
appear previous sentence. case, joint venture concept identifying
companies income concept identifying expected income must merged
latter linked correct entity objects.
4. solution

section describes details textract's pattern matcher discourse processor
well system architecture.

4.1

textract architecture
textract information extraction system developed tipster Japanese do-

mains corporate joint ventures microelectronics (Jacobs, 1993; Jacobs et al., 1993).
shown Figure 2, textract joint ventures system comprises four major components: preprocessor, pattern matcher, discourse processor, template generator.
shorter development time, textract microelectronics system simpler configuration joint ventures system. include template pattern search
pattern matcher, discourse segmentation concept merging discourse
processor, also shown Figure 2.
preprocessor, Japanese segmentor called majesty segments Japanese text
primitive words tagged parts speech (Kitani, 1991). Next, name recognizer
identifies proper names monetary, numeric, temporal expressions. majesty tags
proper names appear lexicon; name recognizer identifies additional proper
names locating name designators \ " (sya, corresponding \Inc." \Corp.")
94

fiPattern Matching Discourse Processing

Pattern matcher
Preprocessor

concept
search

template
pattern
search

- concept
- concept
identification
identification
- information
merging within
sentence

- morphological
analysis
- name
recognition

Discourse processor
company
discourse
concept
name
segmentamerging
unification
tion
- company
name
reference
resolution

- information
- text
segmentation merging
within text

joint ventures
system

Template
generator
- output
generation

Figure 2: textract system architecture
company names. recognizer extends name string forward backward
designator meets search stop conditions (Kitani & Mitamura, 1993). name
segments grouped units meaningful pattern matching process
(Kitani & Mitamura, 1994). strings extracted directly text identified
majesty name recognizer.
Details pattern matcher discourse processor given following sections. template generator assembles extracted information creates output
described Section 2.

4.2 Pattern matcher

following subsections describe concept search template pattern search
pattern matcher identify concepts sentence. Whereas former simply
searches key words, latter searches phrasal patterns within sentence.
template pattern search also identifies relationships matched objects defined
pattern. course textract development, key words template patterns
obtained manually system developer using kwic (Key Word Context) tool
referring word frequency list obtained corpus.
4.2.1 Concept search

Key words representing concept grouped list used recognize
concept sentence. list written simple format: (concept-name word1 word2
...). example, key words recognizing dissolved joint venture concept written
following way:
95

fiKitani, Eriguchi, & Hara
(DISSOLVED

F Fn)


(DISSOLVED dissolve terminate cancel).

concept search module recognizes concept locates one associated
words sentence. simple procedure sometimes yields incorrect concepts. example, concept \dissolved" erroneously identified expression \cancel
hotel reservation". Key-word-based concept search successful processing text
narrow domain words used restricted meanings.
under-matching problem occurs compound noun key word list
concept fails match text instance compound text
segmented separate primitive words. avoid problem, adjacent nouns text
automatically concatenated concept search process, generating compound
nouns run-time. over-matching problem, hand, arises key word
successfully matches part compound noun whole associated
concept. Over-matching prevented anchoring beginning and/or end
key word pattern word boundaries (with symbol \>" beginning \<"
end). example, \> JS <" (silicon) must matched single complete
word text. Since problem rare, solution automatic: system developers
attach anchors key words likely over-match.
4.2.2 Template pattern search
textract's pattern matcher implemented finite-state recognizer. choice

implementation based assumption finite-state grammar eciently handle
many inputs context-free grammar covers (Pereira, 1990). pattern matcher
similar pattern recognizer used muc-4 fastus system developed sri
(Hobbs et al., 1992).
Patterns textract template pattern matcher defined rules similar
regular expressions. pattern definition specifies concept associated
pattern. (For joint ventures domain, textract uses eighteen concepts.)
matcher, state transitions driven segmented words grouped units
preprocessor. matcher identifies possible patterns interest text
match defined patterns, recognizing concepts associated patterns.
inputs, matcher must skip words explicitly defined pattern.
Figure 3 shows definitions equivalent Japanese English patterns recognizing
concept *joint-venture*. English pattern used capture expressions
\XYZ Corp. created joint venture PQR Inc." notation \@string" represents
variable matching arbitrary string. Variables whose names begin \@cname"
called company-name variables used company name expected
appear. definitions shown, string matched \@cname partner subj" likely
contain least one company name referring joint venture partner functioning
subject sentence.
pattern \ /#fi:strict:P" matches grammatical particles \ / " (wa ) \ fi "
(ga ), serve subject case markers. symbol \strict" specifies full string match
(the default case template pattern search), whereas \loose" allows partial string
96

fiPattern Matching Discourse Processing
(a)

(JointVenture1 6
@CNAME_PARTNER_SUBJ
/#fi:strict:P
@CNAME_PARTNER_WITH
(:strict:P
@SKIP
:loose:VN)

(b)

(JointVenture1 3
@CNAME_PARTNER_SUBJ
create::V
joint venture::NP
with::P
@CNAME_PARTNER_WITH)

Figure 3: matching pattern (a) Japanese (b) English

match. Partial string matching useful matching defined pattern compound words.
verbal nominal pattern \ : loose:VN" matches compound words \ [ "
(kigyo-teikei: corporate joint venture) well \ " (teikei: joint venture).
first field pattern pattern name, refers concept associated
pattern. second field number indexing field pattern. field's
contents used decide quickly whether search within given string.
matcher applies entire pattern string string contains text
indexed field. eciency, therefore, field contain least frequent word
entire pattern (in case, \ " (teikei) Japanese \a joint venture"
English).
order noun phrases relatively unconstrained Japanese sentence. Case
markers, usually attached ends noun phrases, provide strong clue identifying
case role phrase (subject, object, etc.). Thus pattern matching driven mainly
case markers recognizes case roles well without parsing sentence.
Approximately 150 patterns used extract various concepts Japanese joint
ventures domain. Several patterns usually match single sentence. Moreover, since patterns
often defined case markers \ / " (wa), \ fi " (ga), \ ( " (to), single
pattern match sentence one way several case markers
appear sentence. template generator accepts best matched pattern,
chosen applying following three heuristic rules order shown:
1. select patterns include largest number matched company-name variables
containing least one company name;
2. select patterns consume fewest input segments (the shortest string match);

3. select patterns include largest number variables defined words.
heuristic rules obtained examination matched patterns reported
system. obtain reliable heuristics, large-scale statistical evaluation must
performed. Heuristics similar problem pattern selection English discussed
(Rau & Jacobs, 1991). system chooses pattern consumes input
segments (the longest string match), opposed textract's choice shortest string
match second heuristic rule.2
2. Rau Jacobs' system, third heuristic rule seems applied second rule.
case, little difference performance heuristic rules two systems.
97

fiKitani, Eriguchi, & Hara
Another important feature pattern matcher rules grouped according
concept. rule name \JointVenture1" Figure 3, example, represents
concept *joint-venture*. Using grouping, best matched pattern
selected matched patterns particular concept group instead matched
patterns. feature enables discourse template generation processes narrow
search best information fill particular slot.
4.3 Discourse processor

following subsections describe algorithm company name reference resolution
throughout discourse. Discourse segmentation concept merging processes also
discussed.
4.3.1 Identifying topic companies

Since syntactic analysis performed textract, topic companies simply identified wherever subject case marker \ fi " (ga), \ / " (wa), \ B " (mo) follows
company names. topic companies found sentence, previous sentence's
topic companies inherited (even current sentence contains non-company subject). based supposition sentence introduces new companies
usually mentions explicitly subject.
4.3.2 Abbreviation detection unification

Company name abbreviations following observed characteristics:


majesty tags abbreviations \unknown", \company", \person", \place";



company name precedes abbreviations;
abbreviation composed two characters company name,
original order;





characters need consecutive within company name;
English word abbreviations must identical English word appearing
company name.

Thus following regarded abbreviations: \unknown", \company", \person",
\place" segments composed two characters also appear company
names previously identified text. comparing possible abbreviations
known company names, length longest common subsequence LCS (Wagner &
Fischer, 1974) computed determine maximum number characters appearing
order strings.3
unify multiple references company, unique number assigned
source abbreviated companies. Repeated company names contain strings
appearing earlier text treated abbreviations (and thus given unique numbers)
3. example, LCS \abacbba" \bcda" \bca".
98

fiPattern Matching Discourse Processing
1. Step 1: Initialization assign entity C unique number.
C (1 cmax)
C [i; \id"]
done
2. Step 2: Search abbreviations give unique numbers
C (1 cmax)
C [i; \id"] 6=
# already recognized abbreviation
continue loop
LENSRC length C [i; \string"]
j C (i + 1 j cmax)
C [j; \id"] 6= j
# already recognized abbreviation
continue j loop
LEN length C [j; \string"]
LCS length LCS C [i; \string"] C [j; \string"]
LCS 2
C [i; \eg"] = \YES" LENSRC = LCS = LEN
C[j; \id"] C [i; \id"] # English word abbreviation
else C [i; \eg"] = \NO" LCS = LEN
# abbreviation
C[j; \id"] C [i; \id"]
done
done
done
Figure 4: Algorithm unify multiple references company
algorithm described Figure 4. pseudocode shown, identified company
names stored associative array named C . \Unknown", \company", \person",
\place" segments also stored array possible abbreviations. Company names
sorted ascending order starting position text numbered 1 cmax
(Step 1). company name string indexed addressed C [i; \string"].
ag C [i; \eg"] records whether company name English word abbreviation not.
Step 2 compares company name array C names higher array
(and thus later text). LCS pair earlier later company names
equal length later company name, later company name recognized
abbreviation earlier company name. Then, \id" later company name
replaced earlier company name. LCS must two characters,
abbreviation English word, LCS must equal length
earlier company name.
end execution, number given C [i; \id"]. C [i; \id"] changed
execution, C [i; \string"] recognized company name abbreviation.
99

fiKitani, Eriguchi, & Hara
4.3.3 Anaphora resolution company name pronouns

approach reference resolution described section based heuristics obtained
corpus analysis rather linguistic theories. Three company name pronouns
target reference resolution: \ ! " (ryosya), \ $ " (dosya), \ r " (jisya), meaning
\both companies", \the company", \the company itself". three
frequent company name pronouns appearing corpus provided arpa tipster
information extraction project. \Ryosya", \dosya", \jisya" appeared 456, 277, 129
times, respectively, 1100 newspaper articles containing average 481 characters per
article.
following heuristics, derived analysis pronoun reference corpus,
used reference resolution:






\ryosya" almost always referred \current" tie-up company, one exception
hundred occurrences;
ninety percent \dosya" occurrences referred topic company
one possible referent sentence, but:
two companies, including topic company, preceded \dosya"
sentence, seventy-five percent pronoun occurrences referred
nearest company, necessarily topic company;
eighty percent \jisya" occurrences referred topic company.

Two additional heuristic rules discovered implemented textract:
four percent \jisya" occurrences referred one company;
eight percent \jisya" occurrences referred entities general expressions company \ fi " (kaisya: company).
result discourse processing described above, every company name, including
abbreviations pronominal references, given unique number.
4.3.4 Discourse segmentation concept merging
150 articles tipster/muc-5 joint ventures test set, multiple tie-up relationships

appeared thirty-one articles included ninety individual tie-up relationships.
two typical discourse models representing discourse structures tie-up relationships
shown Figure 5.




Type-I: tie-ups described sequentially
Descriptions tie-ups appear sequentially model. One tie-up mentioned
new tie-up described.
Type-II: main tie-up reappears tie-ups mentioned
major difference Type-I model description main tie-up
reappears text tie-up relationships introduced. Non-main
tie-ups usually mentioned brie y.
100

fiPattern Matching Discourse Processing

tie-up-1

tie-up-1

tie-up-2

tie-up-2
.
.non-main tie-ups
.

tie-up-3
.
.
.

tie-up-n

tie-up-n

tie-up-1

Type-I

Type-II

Figure 5: Discourse structure tie-up relationships
Eleven Type-I structures thirteen Type-II structures appeared thirty-one articles. Seven articles contained complicated discourse structures regarding tie-up
relationships.
two types text structure described similar ones implemented
crl/brandeis diderot joint ventures system. difference Type-II
structure: diderot processes tie-up relationships reappear text,
reappearing main tie-up focused textract.
textract's discourse processor divides text different tie-up relationship
identified template pattern search. different tie-up relationship recognized
numbers assigned joint venture companies identical appearing
previous tie-up relationships. diderot segments discourse related pieces
information date entity location different tie-up relationships.
strict merging preferable pieces information comparison correctly
identified. merging conditions discourse segments chosen according
accuracy identification information compared.
discourse segmented, identified concepts extracted words phrases
merged. Figure 6 shows merging process following text passage actually
appeared tipster/muc-5 test set (a direct English translation follows):
\ /8o;.DAffAK..o%U'.$

RKRS .fi' KH+*K56'+/!
fiZ&fiRfiK(B4 "

\On eighth (of month), Tanabe Pharmaceuticals made joint
venture contract German pharmaceutical maker, Merck Co.
Inc., develop sell new medicine Japan. also agreed
companies would invest equally establish joint venture company
five six years start selling new medicine."
101

fiKitani, Eriguchi, & Hara

First sentence:
"On eighth (of month),
Tanabe Pharmaceuticals made
joint venture contract
German pharmaceutical maker,
Merck Co. Inc., develop
sell new medicine
Japan."

Second sentence:
"They also agreed
companies would invest equally
establish joint venture
company five six years
start selling new medicine."

"Tanabe Pharmaceuticals"
Template
pattern
search

*ESTABLISH*
"a joint venture
"both
company"
companies"

*ECONOMICACTIVITY*
"Merck Co. Inc."

"Tanabe Pharmaceuticals"
Discourse
processor

"both
companies"

*ECONOMICACTIVITY*

*ESTABLISH*

"a joint venture
company"

"Merck Co. Inc."

Figure 6: Example concept merging
two company names first sentence, \ " (tanabe seiyaku: Tanabe
Pharmaceuticals) \ AK " (ei meruku sya: Merck Co. Inc.), identified
either majesty name recognizer preprocessing. Next, template pattern
search locates first sentence \economic activity" pattern shown Figure 7 (a).
*economic-activity* concept relating two companies recognized.
template pattern search also recognizes *establish* concept second sentence
template pattern shown Figure 7 (b).
sentence-level processing, discourse processing recognizes \ ! " (ryosya:
companies) second sentence refers Tanabe Pharmaceuticals Merck
first sentence current tie-up companies. Since second sentence
introduce new tie-up relationship, sentences discourse
segment. Concepts separately identified two sentences merged
subjects two sentences same. *establish* concept therefore joined
*economic-activity* concept.
(a)

(EconomicActivityE 6
@CNAME_PARTNER_SUBJ
:strict:P
@CNAME_PARTNER_SUBJ
:strict:P
@SKIP
:loose:VN)

(b)

(Establish3 6
@CNAME_PARTNER_SUBJ
:strict:P
@CNAME_CREATED_OBJ
:strict:P
@SKIP
:loose:VN)

/#fi
R
fi

/#fi
.
$

Figure 7: Economic activity pattern (a) establish pattern (b)
102

fiPattern Matching Discourse Processing

5. Performance evaluation
section shows evaluation results textract's discourse module. muc-5 evaluation metrics overall textract performance also discussed.

5.1 Unique identification company name abbreviations
hundred joint ventures newspaper articles used tipster 18-month evaluation
chosen blind test set evaluation. evaluation measures recall,
percentage correct answers extracted compared possible answers, precision,
percentage correct answers extracted compared actual answers. majesty
name recognizer identified company names evaluation set recall seventyfive percent precision ninety-five percent partial matches expected
recognized strings allowed, recall sixty-nine percent precision
eighty-seven percent exact matching condition.
Company names appeared form different first appearance
article considered company name abbreviations. Among 318 abbreviations,
recall precision abbreviation detection sixty-seven eighty-nine percent, respectively. importantly, detected abbreviations unified correctly source
companies long source companies identified correctly majesty
name recognizer.
evaluation results clearly show company name abbreviations accurately
detected unified source companies long company names correctly
identified preceding processes. possible, however, simple string matching
algorithm currently used could erroneously unify similar company names, often
seen among family companies.

5.2 Anaphora resolution company name pronouns
accuracy reference resolution \ryosya", \dosya", \jisya" shown Table
1. numbers parentheses obtained restricting attention pronouns
referred companies identified correctly preceding processes. Since companies
referred \ryosya" (both companies) usually \current" tie-up companies
joint ventures domain, reference resolution accuracy depended accuracy
tie-up relationships identified.
company name pronouns

number
references
\ ! " (ryosya: companies) 101 (93)
\ $ " (dosya: company)
100 (90)
\ r " (jisya: company itself) 60 (53)

resolution
accuracy
64% (70%)
78% (87%)
78% (89%)

Table 1: Accuracy reference resolutions
103

fiKitani, Eriguchi, & Hara
major cause incorrect references \dosya" failure locate topic companies. simple mechanism searching topic companies using case markers
work well. typical problem seen following example: \ '/X " (A
joint venture partner X Corp.). X Corp topic company, subject \ X "
(X Corp.) followed subject case marker. errors attributed
fact \dosya" always refer topic company discussed heuristic rules
\dosya" reference resolution.
Regarding \jisya" resolutions, five instances referred multiple
companies bound single company. Since multiple companies usually listed
using conjunctions \ ( " (to: and) \ " (comma), identified easily
simple phrase analysis performed.
became clear evaluation resolving \dosya" references non-topic
company required intensive text understanding. Forty-seven percent occurrences
\dosya" \jisya" bound topic companies inherited previous sentence.
result strongly supported importance keeping track topic companies throughout
discourse.

5.3 Discourse segmentation
Thirty-one 150 tipster/muc-5 evaluation test articles included ninety multiple tieup relationships. textract's discourse processor segmented thirty-one articles
seventy-one individual tie-up relationship blocks. thirty-eight blocks correctly segmented. Main tie-up relationships reappeared Type-II discourse structures detected well, caused structures incorrectly recognized
Type-I. error caused fact joint venture relationships usually mentioned implicitly reappeared text. example, noun phrase,
\ ./ " (the joint venture time), detected template patterns used, brought focus back main tie-up. result, textract identified eight
percent fewer tie-up relationships possible number expected tipster/muc-5
evaluation. merging error must affected system performance since information reappearing main tie-up segment would correctly linked
earlier main tie-up segment.
preliminary study suggested recognizing segmentation points text
regarded crucial performance. template pattern matching alone good
enough recognize segmentation points. discourse processor simply segmented
text found new tie-up relationship. discourse models, currently unused
run-time textract, could used help infer discourse structure system
sure whether merge separate discourse segments. Reference resolution definite
indefinite noun phrases must also solved accurate discourse segmentation future
research.
accuracy discourse segmentation might improved checking difference
identity date entity location, well entity name, deciding whether
merge tie-up relationship. textract take date location objects account
making segmentation decisions, textract's identification objects
considered reliable enough. example, date object identified recall
104

fiPattern Matching Discourse Processing
twenty-seven percent precision fifty-nine percent. hand, entities
identified eighty percent accuracy recall precision. avoid
incorrect discourse segmentation, therefore, textract's merging conditions included
entity names reliable information.

5.4 Overall textract performance

250 newspaper articles, 150 Japanese corporate joint ventures 100 Japanese
microelectronics, provided arpa use tipster/muc-5 system evaluation.
Six joint ventures systems five microelectronics systems, including textract developed cmu optional system ge-cmu shogun, presented Japanese
system evaluation muc-5. scoring program automatically compared system output
answer templates created human analysts. human decision necessary,
analysts instructed scoring program whether two strings comparison completely matched, partially matched, unmatched. Finally, scoring program calculated
overall score combined newspaper article scores. Although various evaluation metrics measured evaluation (Chinchor & Sundheim, 1993),
following error-based recall-precision-based metrics discussed paper.
basic scoring categories used are: correct (COR), partially correct (PAR), incorrect (INC),
missing (MIS), spurious (SPU), counted number pieces information
system output compared possible information.
(1) Error-based metrics
Error per response fill (ERR):
wrong = INC + P AR=2 + MIS + SPU
total COR + PAR + INC + MIS + SP U


Undergeneration (UND):

MIS
MIS =
possible COR + PAR + INC + MIS


Overgeneration (OVG):

SPU =
SPU
actual COR + P AR + INC + SP U


Substitution (SUB):

INC + P AR=2
COR + P AR + INC

105

fiKitani, Eriguchi, & Hara

domain
ERR UND OVG SUB REC PRE P&R
textract (JJV)
50
32
23
12
60
68 63.8
System (JJV)
54
36
27
12
57
64 60.1
System B (JJV)
63
51
23
12
42
67 52.1
textract (JME) 59
43
28
12
51
63 56.4
System (JME)
58
30
38
14
60
53 56.3
System B (JME)
65
54
24
12
40
66 50.4
Table 2: Scores textract two top-ranking ocial systems tipster/muc-5
(2) Recall-precision-based metrics


Recall (REC):



Precision (PRE):



P&R F-measure (P&R):

COR + PAR=2
possible
COR + PAR=2
actual

2 REC P
REC + PRE
error per response fill (ERR) ocial measure muc-5 system performance.
Secondary evaluation metrics undergeneration (UND), overgeneration (OVG),
substitution (SUB). recall, precision, F-measure metrics used unocial
metrics muc-5.
Table 2 shows scores textract two top-ranking ocial systems taken
tipster/muc-5 system evaluation results.4 textract processed Japanese
domains corporate joint ventures (JJV) microelectronics (JME), whereas two
systems processed English Japanese text. textract performed well
top-ranking systems two Japanese domains.
human performance four well-trained analysts reported eighty
percent recall precision English microelectronics domain (Will, 1993).
thirty percent better best tipster/muc-5 systems' performance
P&R F-measure language domain. Japanese joint ventures domain,
textract scored recall seventy-five percent precision eighty-one percent
core template comprising essential objects. result suggests current
technology could used support human extraction work task well-constrained.
4. textract scores submitted muc-5 unocial. scored ocially conference.
Table 2 shows textract's ocial scores.
106

fiPattern Matching Discourse Processing
Running SUN SPARCstation IPX, textract processed joint ventures article
sixty seconds microelectronics article twenty-four seconds average.
human analysts took fifteen minutes complete English microelectronics
template sixty minutes Japanese joint ventures template (Will, 1993).
Thus human-machine integrated system would best solution fast, high quality,
information extraction.
tipster/muc-5 systems processed Japanese English domains.
systems generally performed better Japanese domains corresponding English domains. One likely reason structure Japanese articles fairly standard,
particularly Japanese joint ventures domain, readily analyzed
two discourse structure types described paper. Another possible reason characteristic writing style: expressions need identified tend appear first
sentences form suitable pattern matching.
textract Japanese microelectronics system copied preprocessor, concept
search pattern matcher, company name unification discourse processor
used textract Japanese joint ventures system. microelectronics system
developed three weeks one person replaced joint ventures concepts key
words representative microelectronics concepts key words. lower performance
textract microelectronics system compared joint ventures system largely
due short development time. also probably due less homogeneous discourse
structure writing style microelectronics articles.
6. Conclusions future research

paper described importance discourse processing three aspects information extraction: identifying key information throughout text, i.e. topic companies
company name references tipster/muc-5 domains; segmenting text select relevant portions interest; merging concepts identified sentence level processing.
basic performance system depends preprocessor, however, since many
pieces identified information put directly slots otherwise used fill slots
later processing. textract's pattern matcher solves matching problem caused
segmentation ambiguities often found Japanese compound words. pattern
matching system based finite-state automaton simple runs fast. factors
essential rapid system development performance improvement.
improve system performance pattern matching architecture, increase
number patterns unavoidable. Since matching large number patterns
lengthy process, ecient pattern matcher required shorten running time.
Tomita's new generalized LR parser, known one fastest parsers practical
purposes, skips unnecessary words parsing (Bates & Lavie, 1991). parser
evaluation investigate appropriate information extraction Japanese
text (Eriguchi & Kitani, 1993). Pattern matching alone, however, able improve
system performance human levels complicated information extraction task
tipster/muc-5, even task well-defined suitable pattern matching.
efforts made discourse processing discourse segmentation reference
resolution definite indefinite noun phrases.
107

fiKitani, Eriguchi, & Hara
research discussed paper based application-oriented, domain-specific,
language-specific approach relying patterns heuristic rules collected
particular corpus. obvious patterns heuristic rules described paper
cover wide range applications, domains, languages. empirical approach
described worth investigating even entirely new task, however, since
achieve high level system performance relatively short development time.
linguistic theory-based systems tend become complex dicult maintain, especially
incorporate full text parsing, simplicity empirically-based, pattern-oriented
system textract keeps development time short evaluation cycle quick.
Corpus analysis key element corpus-based paradigm. estimated
corpus analysis took half development time textract. Statistically-based
corpus analysis tools necessary obtain better performance shorter development
time. tools could help developers extract important patterns heuristic
rules corpus, also monitor system performance evaluationimprovement cycle.

Acknowledgements

authors wish express appreciation Jaime Carbonell, provided opportunity pursue research Center Machine Translation, Carnegie Mellon
University. Thanks also due Teruko Mitamura Michael Mauldin many
helpful suggestions.
References

Bates, J., & Lavie, A. (1991). Recognizing Substrings LR(k) Languages Linear Time.
Tech. rep. CMU-CS-91-188, Carnegie Mellon University, School Computer Science.
Chinchor, N., & Sundheim, B. (1993). MUC-5 Evaluation Metrics. Proceedings
Fifth Message Understanding Conference (MUC-5), pp. 69{78.
Cowie, J., Guthrie, L., et al. (1993). CRL/BRANDEIS: Description Diderot System
Used MUC-5. Proceedings Fifth Message Understanding Conference
(MUC-5), pp. 161{179.
Doi, S., Ando, S., & Muraki, K. (1993). Context Analysis Information Extraction System
Based Keywords Text Structure. Proceedings Forty-seventh Annual
Conference IPSJ (in Japanese).
Eriguchi, Y., & Kitani, T. (1993). Preliminary Study Using Tomita's Generalized LR
Parser Information Extraction. Unpublished paper, Center Machine Translation, Carnegie Mellon University.
Fujii, H., & Croft, B. (1993). Comparison Indexing Techniques Japanese Text Retrieval. Proceedings Sixteenth Annual International ACM SIGIR Conference
Research Development Information Retrieval, pp. 237{246.
Hirschman, L. (1992). Adjunct Test Discourse Processing MUC-4. Proceedings
Fourth Message Understanding Conference (MUC-4), pp. 67{77.
108

fiPattern Matching Discourse Processing
Hobbs, J., Appelt, D., et al. (1992). FASTUS: System Extracting Information
Natural-Language Text. Tech. rep. 519, SRI International.
Jacobs, P. (1993). TIPSTER/SHOGUN 18-Month Progress Report. Notebook TIPSTER 18-Month Meeting.
Jacobs, P., Krupka, G., et al. (1993). GE-CMU: Description Shogun System Used
MUC-5. Proceedings Fifth Message Understanding Conference (MUC-5),
pp. 109{120.
Karasawa, I. (1993). Detection Company Name Abbreviations Japanese Texts. Unpublished paper, Center Machine Translation, Carnegie Mellon University.
Kitani, T. (1991). OCR Post-processing Method Handwritten Japanese Documents.
Proceedings Natural Language Processing Pacific Rim Symposium, pp. 38{45.
Kitani, T. (1994). Merging Information Discourse Processing Information Extraction.
Proceedings Tenth IEEE Conference Artificial Intelligence Applications, pp. 412{418.
Kitani, T., & Mitamura, T. (1993). Japanese Preprocessor Syntactic Semantic
Parsing. Proceedings Ninth IEEE Conference Artificial Intelligence
Applications, pp. 86{92.
Kitani, T., & Mitamura, T. (1994). Accurate Morphological Analysis Proper Name
Identification Japanese Text Processing. Journal Information Processing Society
Japan, 35 (3), 404{413.
Lehnert, W., McCarthy, J., et al. (1993). UMASS/HUGHES: Description CIRCUS
System Used MUC-5. Proceedings Fifth Message Understanding Conference (MUC-5), pp. 277{291.
Lehnert, W., & Sundheim, B. (1991). Performance Evaluation Text-Analysis Technologies. AI Magazine, Fall, 81{94.
Muraki, K., Doi, S., & Ando, S. (1993). NEC: Description VENIEX System Used
MUC-5. Proceedings Fifth Message Understanding Conference (MUC-5),
pp. 147{159.
Onyshkevych, B. (1993). Technology Perspective. Notebook Fifth Message Understanding Conference (MUC-5).
Pereira, F. (1990). Finite-State Approximations Grammars. Proceedings DARPA
Speech Natural Language Workshop, pp. 20{25.
Rau, L., & Jacobs, P. (1991). Creating Segmented Databases Free Text Text
Retrieval. Proceedings Fourteenth Annual International ACM/SIGIR Conference
Research Development Information Retrieval, pp. 337{346.
Tipster (1992). Joint Venture Template Fill Rules. Plenary Session Notebook
TIPSTER 12-Month Meeting.
109

fiKitani, Eriguchi, & Hara
Wagner, R., & Fischer, M. (1974). String-to-String Correction Problem. Journal
ACM, 21 (1), 168{173.
Wakao, T. (1994). Reference Resolution Using Semantic Patterns Japanese Newspaper
Articles. Proceedings COLING 94, pp. 1133{1137.
Weischedel, R., Ayuso, D., et al. (1993). BBN: Description PLUM System Used
MUC-5. Proceedings Fifth Message Understanding Conference (MUC-5),
pp. 93{107.
Will, C. (1993). Comparing Human Machine Performance Natural Language Information Extraction: Results English Microelectronics MUC-5 Evaluation.
Proceedings Fifth Message Understanding Conference (MUC-5), pp. 53{67.

110

fiJournal Artificial Intelligence Research 2 (1995) 369-409

Submitted 10/94; published 3/95

Cost-Sensitive Classification: Empirical Evaluation
Hybrid Genetic Decision Tree Induction Algorithm
Peter D. Turney
Knowledge Systems Laboratory, Institute Information Technology
National Research Council Canada, Ottawa, Ontario, Canada, K1A 0R6.

TURNEY@AI.IIT.NRC.CA

Abstract
paper introduces ICET, new algorithm cost-sensitive classification. ICET
uses genetic algorithm evolve population biases decision tree induction algorithm. fitness function genetic algorithm average cost classification
using decision tree, including costs tests (features, measurements)
costs classification errors. ICET compared three algorithms
cost-sensitive classification EG2, CS-ID3, IDX also C4.5, classifies without regard cost. five algorithms evaluated empirically five realworld medical datasets. Three sets experiments performed. first set examines
baseline performance five algorithms five datasets establishes ICET
performs significantly better competitors. second set tests robustness
ICET variety conditions shows ICET maintains advantage. third
set looks ICETs search bias space discovers way improve search.

1. Introduction
prototypical example problem cost-sensitive classification medical diagnosis, doctor would like balance costs various possible medical tests
expected benefits tests patient. several aspects problem:
benefit test, terms accurate diagnosis, justify cost test?
time stop testing make commitment particular diagnosis? much
time spent pondering issues? extensive examination various
possible sequences tests yield significant improvement simpler, heuristic choice
tests? questions investigated here.
words cost, expense, benefit used paper broadest sense,
include factors quality life, addition economic monetary cost. Cost
domain-specific quantified arbitrary units. assumed costs tests
measured units benefits correct classification. Benefit treated
negative cost.
paper introduces new algorithm cost-sensitive classification, called ICET
(Inexpensive Classification Expensive Tests pronounced iced tea). ICET uses
genetic algorithm (Grefenstette, 1986) evolve population biases decision tree
induction algorithm (a modified version C4.5, Quinlan, 1992). fitness function
genetic algorithm average cost classification using decision tree, including
costs tests (features, measurements) costs classification errors. ICET
following features: (1) sensitive test costs. (2) sensitive classification
error costs. (3) combines greedy search heuristic genetic search algorithm. (4)
handle conditional costs, cost one test conditional whether second

1995 National Research Council Canada. rights reserved. Published permission.

fiT URNEY

test selected yet. (5) distinguishes tests immediate results tests
delayed results.
problem cost-sensitive classification arises frequently. problem medical
diagnosis (Nez, 1988, 1991), robotics (Tan & Schlimmer, 1989, 1990; Tan, 1993), industrial production processes (Verdenius, 1991), communication network troubleshooting
(Lirov & Yue, 1991), machinery diagnosis (where main cost skilled labor), automated
testing electronic equipment (where main cost time), many areas.
several machine learning algorithms consider costs tests,
EG2 (Nez, 1988, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX
(Norton, 1989). also several algorithms consider costs classification
errors (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon &
Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al.,
1994). However, little work considers costs together.
good reasons considering costs tests costs classification errors. agent cannot rationally determine whether test performed without
knowing costs correct incorrect classification. agent must balance cost
test contribution test accurate classification. agent must also consider testing economically justified. often happens benefits
testing worth costs tests. means cost must assigned
tests classification errors.
Another limitation many existing cost-sensitive classification algorithms (EG2, CSID3) use greedy heuristics, select step whatever test contributes
accuracy least cost. sophisticated approach would evaluate interactions among tests sequence tests. test appears useful considered isolation,
using greedy heuristic, may appear useful considered combination
tests. Past work demonstrated sophisticated algorithms superior
performance (Tcheng et al., 1989; Ragavan & Rendell, 1993; Norton, 1989; Schaffer, 1993;
Rymon, 1993; Seshu, 1989; Provost, 1994; Provost & Buchanan, press).
Section 2 discusses decision tree natural form knowledge representation
classification expensive tests measure average cost classification
decision tree. Section 3 introduces five algorithms examine here, C4.5
(Quinlan, 1992), EG2 (Nez, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993),
IDX (Norton, 1989), ICET. five algorithms evaluated empirically five realworld medical datasets. datasets discussed detail Appendix A. Section 4 presents three sets experiments. first set (Section 4.1) experiments examines baseline performance five algorithms five datasets establishes ICET
performs significantly better competitors given datasets. second set (Section 4.2) tests robustness ICET variety conditions shows ICET
maintains advantage. third set (Section 4.3) looks ICETs search bias space
discovers way improve search. discuss related work future work Section 5. end summary learned research statement
general motivation type research.

2. Cost-Sensitive Classification
section first explains decision tree natural form knowledge representation classification expensive tests. discusses measure average
cost classification decision tree. method measuring average cost handles
370

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

aspects problem typically ignored. method applied standard
classification decision tree, regardless tree generated. end discussion
relation cost accuracy.
2.1

Decision Trees Cost-Sensitive Classification

decision trees used decision theory (Pearl, 1988) somewhat different
classification decision trees typically used machine learning (Quinlan, 1992).
refer decision trees paper, mean standard classification decision
trees machine learning. claims make classification decision trees also
apply decision theoretical decision trees, modification. full discussion
decision theoretical decision trees outside scope paper.
decision test must based cost tests cost classification errors. test costs $10 maximum penalty classification error $5,
clearly point test. hand, penalty classification
error $10,000, test may quite worthwhile, even information content relatively low. Past work algorithms sensitive test costs (Nez, 1988, 1991;
Tan, 1993; Norton, 1989) overlooked importance also considering cost classification errors.
tests inexpensive, relative cost classification errors, may rational
tests (i.e., measure features; determine values attributes) seem possibly relevant. kind situation, convenient separate selection tests
process making classification. First decide set tests relevant, focus problem learning classify case, using results
tests. common approach classification machine learning literature.
Often paper focuses problem learning classify case, without mention
decisions involved selecting set relevant tests.1
tests expensive, relative cost classification errors, may suboptimal separate selection tests process making classification. may
able achieve much lower costs interleaving two. First choose test,
examine test result. result test gives us information, use influence choice next test. point, decide cost tests
justified, stop testing make classification.
selection tests interleaved classification way, decision tree
natural form representation. root decision tree represents first test
choose. next level decision tree represents next test choose.
decision tree explicitly shows outcome first test determines choice
second test. leaf represents point decide stop testing make classification.
Decision theory used define constitutes optimal decision tree, given (1)
costs tests, (2) costs classification errors, (3) conditional probabilities
test results, given sequences prior test results, (4) conditional probabilities
classes, given sequences test results. However, searching optimal tree infeasible
(Pearl, 1988). ICET designed find good (but necessarily optimal) tree,
good defined better competition (i.e., IDX, CS-ID3, EG2).
1. papers like this. Decision tree induction algorithms C4.5 (Quinlan, 1992) automatically
select relevant tests. Aha Bankert (1994), among others, used sequential test selection procedures
conjunction supervised learning algorithm.

371

fiT URNEY

2.2

Calculating Average Cost Classification

section, describe calculate average cost classification decision
tree, given set testing data. method described applied uniformly decision trees generated five algorithms examined (EG2, CS-ID3, IDX, C4.5,
ICET). method assumes standard classification decision tree (such generated
C4.5); makes assumptions tree generated. purpose
method give plausible estimate average cost expected real-world
application decision tree.
assume dataset split training set testing set.
expected cost classification estimated average cost classification testing set. average cost classification calculated dividing total cost
whole testing set number cases testing set. total cost includes
costs tests costs classification errors. simplest case, assume
specify test costs simply listing test, paired corresponding cost.
complex cases considered later section. assume specify
costs classification errors using classification cost matrix.
Suppose c distinct classes. classification cost matrix c c matrix,
element C i, j cost guessing case belongs class i, actually
belongs class j. need assume constraints matrix, except costs
finite, real values. allow negative costs, interpreted benefits. However, experiments reported here, restricted attention classification cost
matrices diagonal elements zero (we assume correct classification
cost) off-diagonal elements positive numbers. 2
calculate cost particular case, follow path decision tree.
add cost test chosen (i.e., test occurs path root
leaf). test appears twice, charge first occurrence test.
example, one node path may say patient age less 10 years another node
may say patient age 5 years, charge cost determining patients age. leaf tree specifies trees guess class case.
Given actual class case, use cost matrix determine cost trees
guess. cost added costs tests, determine total cost classification
case.
core method calculating average cost classification decision tree. two additional elements method, handling conditional test
costs delayed test results.
allow cost test conditional choice prior tests. Specifically,
consider case group tests shares common cost. example, set blood
tests shares common cost collecting blood patient. common cost
charged once, decision made first blood test. charge
collecting blood second blood test, since may use blood collected
first blood test. Thus cost test group conditional whether another
member group already chosen.
Common costs appear frequently testing. example, diagnosis aircraft
engine, group tests may share common cost removing engine plane
2. restriction seems reasonable starting point exploring cost-sensitive classification. future work,
investigate effects weakening restriction.

372

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

installing test cell. semiconductor manufacturing, group tests may share
common cost reserving region silicon wafer special test structure. image
recognition, group image processing algorithms may share common preprocessing
algorithm. examples show realistic assessment cost using decision
tree frequently need make allowances conditional test costs.
often happens result test available immediately. example,
medical doctor typically sends blood test laboratory gets result next day.
allow test labelled either immediate delayed. test delayed, cannot
use outcome influence choice next test. example, blood tests
delayed, cannot allow outcome one blood test play role decision
second blood test. must make commitment (or doing) second
blood test know results first blood test.
Delayed tests relatively common. example, many medical tests must shipped
laboratory analysis. gas turbine engine diagnosis, main fuel control frequently shipped specialized company diagnosis repair. classification problem requires multiple experts, one experts might immediately available.
handle immediate tests decision tree described above. handle delayed tests
follows. follow path case root decision tree appropriate
leaf. encounter node, anywhere along path, delayed test,
committed performing tests subtree rooted node. Since
cannot make decision tests node conditional outcome test
node, must pledge pay tests might possibly need perform,
point onwards decision tree.
method handling delayed tests may seem bit puzzling first. difficulty
decision tree combines method selecting tests method classifying
cases. tests delayed, forced proceed two phases. first phase,
select tests. second phase, collect test results classify case. example,
doctor collects blood patient sends blood laboratory. doctor must tell
laboratory tests done blood. next day, doctor gets results
tests laboratory decides diagnosis patient. decision
tree naturally handle situation like this, selection tests isolated
classification cases. method, first phase, doctor uses decision
tree select tests. long tests immediate, problem. soon
first delayed test encountered, doctor must select tests might possibly
needed second phase.3 is, doctor must select tests subtree rooted
first delayed test. second phase, test results arrive next day,
doctor information required go root tree leaf, make
classification. doctor must pay tests subtree, even though
tests along one branch subtree actually used. doctor know
advance branch actually used, time necessary order
blood tests. laboratory blood tests naturally want doctor pay
tests ordered, even used making diagnosis.
general, makes sense desired immediate tests
desired delayed tests, since outcome immediate test used influence
decision delayed test, vice versa. example, medical doctor question
3. simplification situation real world. realistic treatment delayed tests one
areas future work (Section 5.2).

373

fiT URNEY

patient (questions immediate tests) deciding blood tests order (blood
tests delayed tests).4
tests delayed (as BUPA data Appendix A.1),
must decide advance (before see test results) tests performed.
given decision tree, total cost tests cases. situations
type, problem minimizing cost simplifies problem choosing best subset
set available tests (Aha Bankert, 1994). sequential order tests
longer important reducing cost.
Let us consider simple example illustrate method. Table 1 shows test costs
four tests. Two tests immediate two delayed. two delayed tests
share common cost $2.00. two classes, 0 1. Table 2 shows classification cost matrix. Figure 1 shows decision tree. Table 3 traces path tree
particular case shows cost calculated. first step test root
tree (test alpha). second step, encounter delayed test (delta), must
calculate cost entire subtree rooted node. Note epsilon costs $8.00,
since already selected delta, delta epsilon common cost. third
step, test epsilon, need pay, since already paid second step.
fourth step, guess class case. Unfortunately, guess incorrectly,
pay penalty $50.00.
Table 1: Test costs simple example.
Test

Group

Cost

Delayed

1

alpha

$5.00



2

beta

$10.00



3

delta



$7.00 first test group A,
$5.00 otherwise

yes

4

epsilon



$10.00 first test group A,
$8.00 otherwise

yes

Table 2: Classification costs simple example.
Actual Class

Guess Class

Cost

0

0

$0.00

0

1

$50.00

1

0

$50.00

1

1

$0.00

4. real world, many factors influence sequence tests, length delay
probability delayed test needed. ignore many factors pay attention
simplified model presented here, makes sense desired immediate tests
desired delayed tests. know extent actually occurs real world. One
complication medical doctors industrialized countries directly affected cost
tests select. fact, fear law suits gives incentive order unnecessary tests.

374

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

alpha < 3


F

beta > 6


delta = 2
F

0



F

1
beta < 5


epsilon < 4
F

1



0

0

F

1

Figure 1: Decision tree simple example.

Table 3: Calculating cost particular case.
Step

Action

Result

Cost

1

alpha

alpha = 6

$5.00

2

delta

delta = 3

$7.00 + $10.00 + $8.00 = $25.00

3

epsilon

epsilon = 2

already paid, step #2

4

guess class = 0

actual class = 1

$50.00

total cost

$80.00

summary, section presents method estimating average cost using
given decision tree. decision tree standard classification decision tree;
special assumptions made tree; matter tree generated.
method requires (1) decision tree (Figure 1), (2) information calculation test
costs (Table 1), (3) classification cost matrix (Table 2), (4) set testing data (Table
3). method (i) sensitive cost tests, (ii) sensitive cost classification
errors, (iii) capable handling conditional test costs, (iv) capable handling delayed
tests. experiments reported Section 4, method applied uniformly five
algorithms.
2.3

Cost Accuracy

method calculating cost explicitly deal accuracy; however,
handle accuracy special case. test cost set $0.00 tests classification cost matrix set positive constant value k guess class equal
actual class j, set $0.00 equals j, average total cost using
decision tree pk , p [0,1] frequency errors testing dataset
375

fiT URNEY

100 ( 1 p ) percentage accuracy testing dataset. Thus linear relationship average total cost percentage accuracy, situation.
generally, let C classification cost matrix cost x diagonal,
C i, = x , cost diagonal, ( j ) ( C i, j = ) , x less y, x < .
call type classification cost matrix simple classification cost matrix. cost
matrix simple called complex classification cost matrix.5
simple cost matrix test costs zero (equivalently, test costs ignored), minimizing
cost exactly equivalent maximizing accuracy.
follows algorithm sensitive misclassification error costs
ignores test costs (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974;
Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press;
Knoll et al., 1994) interesting complex cost matrix.
simple cost matrix, algorithm CART (Breiman et al., 1984) sensitive
misclassification error cost advantage algorithm C4.5 (Quinlan, 1992)
maximizes accuracy (assuming differences two algorithms negligible). experiments paper use simple cost matrix (the exception
Section 4.2.3). Therefore focus comparison ICET algorithms sensitive
test cost (IDX, CS-ID3, EG2). future work, examine complex cost matrices
compare ICET algorithms sensitive misclassification error cost.
difficult find information costs misclassification errors medical practice, seems likely complex cost matrix appropriate simple cost
matrix medical applications. paper focuses simple cost matrices because,
research strategy, seems wise start simple cases attempt complex cases.
Provost (Provost, 1994; Provost & Buchanan, press) combines accuracy classification error cost using following formula:
score = accuracy B cost

(1)

formula, B arbitrary weights user set particular application. accuracy cost, defined Provost (Provost, 1994; Provost & Buchanan, press), represented using classification cost matrices. represent
accuracy using simple cost matrix. interesting applications, cost represented complex cost matrix. Thus score weighted sum two classification cost
matrices, means score classification cost matrix. shows
equation (1) handled special case method presented here. loss
information translation Provosts formula cost matrix. mean
criteria represented costs. example criterion cannot represented cost stability (Turney, press).

3. Algorithms
section discusses algorithms used paper: C4.5 (Quinlan, 1992), EG2 (Nez,
1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX (Norton, 1989), ICET.

5. occasionally say simple cost matrix complex cost matrix. cause confusion,
since test costs represented matrix.

376

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

3.1

C4.5

C4.5 (Quinlan, 1992) builds decision tree using standard TDIDT (top-down induction
decision trees) approach, recursively partitioning data smaller subsets, based
value attribute. step construction decision tree, C4.5 selects
attribute maximizes information gain ratio. induced decision tree pruned
using pessimistic error estimation (Quinlan, 1992). several parameters
adjusted alter behavior C4.5. experiments C4.5, used default settings parameters. used C4.5 source code distributed (Quinlan,
1992).
3.2

EG2

EG2 (Nez, 1991) TDIDT algorithm uses Information Cost Function (ICF)
(Nez, 1991) selection attributes. ICF selects attributes based information gain cost. implemented EG2 modifying C4.5 source code
ICF used instead information gain ratio.
ICF i-th attribute, ICF , defined follows:6


2 1
ICF = ------------------------
( Ci + 1)

0 1

(2)

equation, information gain associated i-th attribute given stage
construction decision tree C cost measuring i-th attribute. C4.5
selects attribute maximizes information gain ratio, function
information gain . modified C4.5 selects attribute maximizes ICF .
parameter adjusts strength bias towards lower cost attributes.
= 0 , cost ignored selection ICF equivalent selection .
= 1 , ICF strongly biased cost. Ideally, would selected way sensitive classification error cost (this done ICET see Section 3.5). Nez (1991)
suggest principled way setting . experiments EG2, set 1.
words, used following selection measure:


2 1
----------------Ci + 1

(3)

addition sensitivity cost tests, EG2 generalizes attributes using ISA
tree (a generalization hierarchy). implement aspect EG2, since
relevant experiments reported here.
3.3

CS-ID3

CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993) TDIDT algorithm selects
attribute maximizes following heuristic function:
2

( )
--------------Ci

(4)

6. inverse ICF, defined Nez (1991). Nez minimizes criterion. facilitate comparison
algorithms, use equation (2). criterion intended maximized.

377

fiT URNEY

implemented CS-ID3 modifying C4.5 selects attribute maximizes
(4).
CS-ID3 uses lazy evaluation strategy. constructs part decision tree
classifies current case. implement aspect CS-ID3, since
relevant experiments reported here.
3.4

IDX

IDX (Norton, 1989) TDIDT algorithm selects attribute maximizes following heuristic function:

------Ci

(5)

implemented IDX modifying C4.5 selects attribute maximizes (5).
C4.5 uses greedy search strategy chooses step attribute highest
information gain ratio. IDX uses lookahead strategy looks n tests ahead, n
parameter may set user. implement aspect IDX. lookahead strategy would perhaps make IDX competitive ICET, would also complicate comparison heuristic function (5) heuristics (3) (4) used EG2
CS-ID3.
3.5

ICET

ICET hybrid genetic algorithm decision tree induction algorithm. genetic
algorithm evolves population biases decision tree induction algorithm.
genetic algorithm use GENESIS (Grefenstette, 1986).7 decision tree induction
algorithm C4.5 (Quinlan, 1992), modified use ICF. is, decision tree induction
algorithm EG2, implemented described Section 3.2.
ICET uses two-tiered search strategy. bottom tier, EG2 performs greedy
search space decision trees, using standard TDIDT strategy. top
tier, GENESIS performs genetic search space biases. biases used
modify behavior EG2. words, GENESIS controls EG2s preference one
type decision tree another.
ICET use EG2 way designed used. n costs, C , used
EG2s attribute selection function, treated ICET bias parameters, costs.
is, ICET manipulates bias EG2 adjusting parameters, C . ICET, values
bias parameters, C , direct connection actual costs tests.
Genetic algorithms inspired biological evolution. individuals evolved
GENESIS strings bits. GENESIS begins population randomly generated
individuals (bit strings) measures fitness individual. ICET,
individual (a bit string) represents bias EG2. individual evaluated running EG2
data, using bias given individual. fitness individual average cost classification decision tree generated EG2. next generation, population replaced new individuals. new individuals generated
previous generation, using mutation crossover (sex). fittest individuals
first generation offspring second generation. fixed number
7. used GENESIS Version 5.0, available URL ftp://ftp.aic.nrl.navy.mil/pub/galist/src/ga/genesis.tar.Z ftp://alife.santafe.edu/pub/USER-AREA/EC/GA/src/gensis-5.0.tar.gz.

378

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

generations, ICET halts output decision tree determined fittest individual. Figure 2 gives sketch ICET algorithm.

GENESIS
fittest

genetic algorithm

decision tree
population
biases
EG2

data

fitness

decision tree

classifier

function
EG2
decision tree

classifier
EG2

decision tree

classifier

Figure 2: sketch ICET algorithm.
GENESIS several parameters used alter performance. parameters used listed Table 4. essentially default parameter settings
(Grefenstette, 1986). used population size 50 individuals 1,000 trials,
results 20 generations. individual population consists string n + 2 numbers, n number attributes (tests) given dataset. n + 2 numbers
represented binary format, using Gray code.8 binary string used bias
EG2. first n numbers string treated n costs, C , used ICF
(equation (2)). first n numbers range 1 10,000 coded 12 binary digits each. last two numbers string used set CF. parameter
used ICF. parameter CF used C4.5 control level pruning decision
tree. last two numbers coded 8 binary digits each. ranges 0 (cost
ignored) 1 (maximum sensitivity cost) CF ranges 1 (high pruning) 100 (low
pruning). Thus individual string 12n + 16 bits.
trial individual consists running EG2 (implemented modification
C4.5) given training dataset, using numbers specified binary string set C
( = 1, , n ), , CF. training dataset randomly split two equal-sized subsets
( 1 odd-sized training sets), sub-training set sub-testing set. different random
split used trial, outcome trial stochastic. cannot assume
identical individuals yield identical outcomes, every individual must evaluated.
means duplicate individuals population, slightly different fitness
scores. measure fitness individual average cost classification
sub-testing set, using decision tree generated sub-training set. aver8. Gray code binary code designed avoid Hamming cliffs. standard binary code, 7 represented 0111 8 represented 1000. numbers adjacent, yet Hamming distance
0111 1000 large. Gray code, adjacent numbers represented binary codes small
Hamming distances. tends improve performance genetic algorithm (Grefenstette, 1986).

379

fiT URNEY

Table 4: Parameter settings GENESIS.
Parameter

Setting

Experiments

1

Total Trials

1000

Population Size

50

Structure Length

12n + 16

Crossover Rate

0.6

Mutation Rate

0.001

Generation Gap

1.0

Scaling Window

5

Report Interval

100

Structures Saved

1

Max Gens w/o Eval

2

Dump Interval

0

Dumps Saved

0

Options

acefgl

Random Seed

123456789

Rank Min

0.75

age cost measured described Section 2.2. 1,000 trials, fit (lowest cost)
individual used bias EG2 whole training set input. resulting
decision tree output ICET given training dataset.9
n costs (bias parameters), C , used ICF, directly related true costs
attributes. 50 individuals first generation generated randomly, initial
values C relation true costs. 20 generations, values C may
relation true costs, simple relationship. values
C appropriately thought biases costs. Thus GENESIS searching
bias space biases C4.5 result decision trees low average cost.
biases C range 1 10,000. bias C greater 9,000, i-th
attribute ignored. is, i-th attribute available C4.5 include decision tree, even might maximize ICF . threshold 9,000 arbitrarily chosen.
attempt optimize value experimentation.
chose use EG2 ICET, rather IDX CS-ID3, EG2 parameter , gives GENESIS greater control bias EG2. ICF partly based
data (via information gain, ) partly based bias (via pseudo9. 50/50 partition sub-training sub-testing sets could mean ICET may work well small
datasets. smallest dataset five examine Hepatitis dataset, 155 cases.
training sets 103 cases testing sets 52 cases. sub-training sub-testing sets 51 52
cases. see Figure 3 ICET performed slightly better algorithms dataset
(the difference significant).

380

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

cost, C ). exact mix data bias controlled varying . Otherwise,
reason prefer EG2 IDX CS-ID3, could easily used instead EG2.
treatment delayed tests conditional test costs hard-wired EG2.
built fitness function used GENESIS, average cost classification (measured described Section 2). makes relatively simple extend ICET handle
pragmatic constraints decision trees.
effect, GENESIS lies EG2 costs tests. lies improve
performance EG2? EG2 hill-climbing algorithm get trapped local optimum. greedy algorithm looks one test ahead builds decision tree.
looks one step ahead, EG2 suffers horizon effect. term taken
literature chess playing programs. Suppose chess playing program
fixed three-move lookahead depth finds loose queen three moves,
follows certain branch game tree. may alternate branch program first sacrifices pawn loses queen four moves. loss
queen three-move horizon, program may foolishly decide sacrifice pawn.
One move later, faced loss queen. Analogously, EG2 may try
avoid certain expensive test selecting less expensive test. One test later,
faced expensive test. exhausted cheaper tests, may
forced expensive test, spite efforts avoid test. GENESIS prevent
short-sighted behavior telling lies EG2. GENESIS exaggerate cost
cheap tests understate cost expensive test. Based past trials, GENESIS
find lies yield best performance EG2.
ICET, learning (local search EG2) evolution (in GENESIS) interact. common
form hybrid genetic algorithm uses local search improve individuals population
(Schaffer et al., 1992). improvements coded strings represent
individuals. form Lamarckian evolution. ICET, improvements due EG2
coded strings. However, improvements accelerate evolution altering fitness landscape. phenomenon (and phenomena result form
hybrid) known Baldwin effect (Baldwin, 1896; Morgan, 1896; Waddington, 1942;
Maynard Smith, 1987; Hinton & Nowlan, 1987; Ackley & Littman, 1991; Whitley & Gruau,
1993; Whitley et al., 1994; Anderson, press). Baldwin effect may explain much
success ICET.

4. Experiments
section describes experiments performed five datasets, taken Irvine collection (Murphy & Aha, 1994). five datasets described detail
Appendix A. five datasets involve medical problems. test costs based information Ontario Ministry Health (1992). main purpose experiments
gain insight behavior ICET. cost-sensitive algorithms, EG2, CS-ID3,
IDX, included mainly benchmarks evaluating ICET. C4.5 also included
benchmark, illustrate behavior algorithm makes use cost information.
main conclusion experiments ICET performs significantly better
competitors, wide range conditions. access Irvine collection
information Appendix A, possible researchers duplicate
results reported here.
Medical datasets frequently missing values.10 conjecture many missing values medical datasets missing doctor involved generating dataset
381

fiT URNEY

decided particular test economically justified particular patient. Thus
may information content fact certain value missing. may
many reasons missing values cost tests. example, perhaps
doctor forgot order test perhaps patient failed show test. However,
seems likely often information content fact value missing.
experiments, information content hidden learning algorithms,
since using (at least testing sets) would form cheating. Two five
datasets selected missing data. avoid accusations cheating, decided
preprocess datasets data presented algorithms missing values.
preprocessing described Appendices A.2 A.3.
Note ICET capable handling missing values without preprocessing inherits ability C4.5 component. preprocessed data avoid accusations
cheating, ICET requires preprocessed data.
experiments, dataset randomly split 10 pairs training testing
sets. training set consisted two thirds dataset testing set consisted
remaining one third. 10 pairs used experiments, order facilitate
comparison results across experiments.
three groups experiments. first group experiments examines baseline performance algorithms. second group considers robust ICET
variety conditions. final group looks ICET searches bias space.
4.1

Baseline Performance

section examines baseline performance algorithms. Section 4.1.1, look
average cost classification five algorithms five datasets. Averaged
across five datasets, ICET lowest average cost. Section 4.1.2, study test
expenditures error rates functions penalty misclassification errors.
five algorithms studied here, ICET adjusts test expenditures error rates functions penalty misclassification errors. four algorithms ignore penalty
misclassification errors. ICET behaves one would expect, increasing test expenditures
decreasing error rates penalty misclassification errors rises. Section 4.1.3,
examine execution time algorithms. ICET requires 23 minutes average
single-processor Sparc 10. Since ICET inherently parallel, significant room
speed increase parallel machine.
4.1.1

AVERAGE COST CLASSIFICATION

experiment presented establishes baseline performance five algorithms.
hypothesis ICET will, average, perform better four algorithms. classification cost matrix set positive constant value k guess
class equal actual class j, set $0.00 equals j. experimented seven settings k, $10, $50, $100, $500, $1000, $5000, $10000.
Initially, used average cost classification performance measure,
found three problems using average cost classification compare
five algorithms. First, differences costs among algorithms become relatively

10. survey 54 datasets Irvine collection (URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/
SUMMARY-TABLE) indicates 85% medical datasets (17 20) missing values,
24% (8 34) non-medical datasets missing values.

382

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

small penalty classification errors increases. makes difficult see
algorithm best. Second, difficult combine results five datasets fair
manner.11 fair average five datasets together, since test costs different scales (see Appendix A). test costs Heart Disease dataset, example,
substantially larger test costs four datasets. Third, difficult combine average costs different values k fair manner, since weight given
situations k large situations small.
address concerns, decided normalize average cost classification.
normalized average cost dividing standard cost. Let f [0,1] frequency class given dataset. is, f fraction cases dataset
belong class i. calculate f using entire dataset, training set. Let C i, j
cost guessing case belongs class i, actually belongs class j. Let
total cost possible tests. standard cost defined follows:
+ min (1 f i) max C i, j

(6)

i, j



decompose formula (6) three components:


(7)

min (1 f i)

(8)

max C i, j

(9)



i, j

may think (7) upper bound test expenditures, (8) upper bound error
rate, (9) upper bound penalty errors. standard cost always less
maximum possible cost, given following formula:
+ max C i, j
i, j

(10)

point (8) really upper bound error rate, since possible
wrong every guess. However, experiments suggest standard cost better
normalization, since realistic (tighter) upper bound average cost.
experiments, average cost never went standard cost, although occasionally came close.
Figure 3 shows result using formula (6) normalize average cost classification. plots, x axis value k axis average cost classification
percentage standard cost classification. see that, average (the sixth plot
Figure 3), ICET lowest classification cost. one dataset ICET
perform particularly well Heart Disease dataset (we discuss later, Sections 4.3.2
4.3.3).
come single number characterizes performance algorithm,
averaged numbers sixth plot Figure 3.12 calculated 95% confidence
regions averages, using standard deviations across 10 random splits
11. want combine results order summarize performance algorithms five datasets.
analogous comparing students calculating GPA (Grade Point Average), students
courses algorithms datasets.
12. Like GPA, datasets (courses) weight. However, unlike GPA, algorithms (students) applied datasets (have taken courses). Thus approach perhaps fair
algorithms GPA students.

383

fiT URNEY

BUPA Liver Disease

Heart Disease
100
Average % Standard Cost

Average % Standard Cost

100
80
60
40
20
0
10

80
60
40
20
0

100

1000

10000

10

Cost Misclassification Error

Hepatitis Prognosis

Pima Indians Diabetes
Average % Standard Cost

Average % Standard Cost

60
40
20
0

80
60
40
20
0

100

1000

10000

10

Cost Misclassification Error

100

1000

10000

Cost Misclassification Error

Thyroid Disease

Average Five Datasets
100
Average % Standard Cost

100
Average % Standard Cost

10000

100

80

80
60
40
20
0
10

1000

Cost Misclassification Error

100

10

100

80
60
40
20
0

100

1000

10000

10

Cost Misclassification Error

100

1000

10000

Cost Misclassification Error

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 3: Average cost classification percentage standard cost
classification baseline experiment.
datasets. result shown Table 5.
Table 5 shows averages first three misclassification error costs alone ($10,
$50, $100), addition showing averages seven misclassification error costs
($10 $10000). two averages (the two columns Table 5), based two groups
data, address following argument: penalty misclassification errors increases,
cost tests becomes relatively insignificant. high misclassification error
cost, test cost effectively zero, task becomes simply maximize accuracy.
384

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 5: Average percentage standard cost baseline experiment.
Algorithm

Average Classification Cost Percentage Standard
95% Confidence
Misclassification Error Costs
10.00 10,000.00

Misclassification Error Costs
10.00 100.00

ICET

49 7

29 7

EG2

58 5

43 3

CS-ID3

61 6

49 4

IDX

58 5

43 3

C4.5

77 5

82 4

see Figure 3, gap C4.5 (which maximizes accuracy) algorithms becomes smaller cost misclassification error increases. Therefore benefit
sensitivity test cost decreases cost misclassification error increases. could
argued one would bother algorithm sensitive test cost tests
relatively expensive, compared cost misclassification errors. Thus realistic measure performance examine average cost classification cost
tests order magnitude cost misclassification errors ($10 $100).
Table 5 shows averages.
conclusion, based Table 5, ICET performs significantly better
four algorithms cost tests order magnitude cost
misclassification errors ($10, $50, $100). cost misclassification errors dominates test costs, ICET still performs better competition, difference less
significant. three cost-sensitive algorithms (EG2, CS-ID3, IDX) perform significantly better C4.5 (which ignores cost). performance EG2 IDX indistinguishable, CS-ID3 appears consistently costly EG2 IDX.
4.1.2

TEST EXPENDITURES ERROR RATES FUNCTIONS PENALTY ERRORS

argued Section 2 expenditures tests conditional penalty
misclassification errors. Therefore ICET designed sensitive cost tests
cost classification errors. leads us hypothesis ICET tends spend
tests penalty misclassification errors increases. also expect
error rate ICET decrease test expenditures increase. two hypotheses
confirmed Figure 4. plots, x axis value k axis (1) average expenditure tests, expressed percentage maximum possible expenditure
tests, , (2) average percent error rate. average (the sixth plot Figure 4), test
expenditures rise error rate falls penalty classification errors increases.
minor deviations trend, since ICET guess value test
(in terms reduced error rate), based sees training dataset. testing
dataset may always support guess. Note plots four algorithms, corresponding plots ICET Figure 4, would straight horizontal lines, since four
algorithms ignore cost misclassification error. generate decision trees
every possible misclassification error cost.

385

fiT URNEY

80

60

60
40
40
20

20
0

0
1000

10000

30
40
20
20

10

Average % Maximum Test Expenditures

30
20
20
10
0

Average % Error Rate

Average % Maximum Test Expenditures

40

0
1000

10000

80

60

40
40
20
20

0
10

0
100

1000

10000

Cost Misclassification Error

Average Five Datasets

80

Average % Maximum Test Expenditures

Thyroid Disease
10
8

60

6
40
4
20

2

0

Average % Error Rate

Average % Maximum Test Expenditures

10000

60

Cost Misclassification Error

0
100

1000

Pima Indians Diabetes
40

10

0
100

Cost Misclassification Error

Hepatitis Prognosis
50

100

10

0

Cost Misclassification Error

10

40

60

Average % Error Rate

100

50

1000

10000

Cost Misclassification Error

80

50
40

60

30
40
20
20

10

0
10

Average % Error Rate

10

80

Average % Error Rate

Average % Maximum Test Expenditures

Heart Disease
80

Average % Error Rate

Average % Maximum Test Expenditures

BUPA Liver Disease
100

0
100

1000

10000

Cost Misclassification Error

% Test Expenditures:
% Error Rate:

Figure 4: Average test expenditures average error rate
function misclassification error cost.
4.1.3

EXECUTION TIME

essence, ICET works invoking C4.5 1000 times (Section 3.5). Fortunately, Quinlans
(1992) implementation C4.5 quite fast. Table 6 shows run-times algorithms,
using single-processor Sun Sparc 10. One full experiment takes one week (roughly
23 minutes average run, multiplied 5 datasets, multiplied 10 random splits, multiplied 7 misclassification error costs equals one week). Since genetic algorithms
easily executed parallel, substantial room speed increase parallel
machine. generation consists 50 individuals, could evaluated parallel,
reducing average run-time half minute.
4.2

Robustness ICET

group experiments considers robust ICET variety conditions.
section considers different variation operating environment ICET. ICET
386

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 6: Elapsed run-time five algorithms.
Algorithm

Average Elapsed Run-Time Dataset Minutes:Seconds
BUPA

Heart

Hepatitis

Pima

Thyroid

Average

ICET

15:43

13:14

10:29

28:19

45:25

22:38

EG2

0:1

0:1

0:1

0:3

0:3

0:2

CS-ID3

0:1

0:1

0:1

0:3

0:3

0:2

IDX

0:1

0:1

0:1

0:3

0:3

0:2

C4.5

0:2

0:1

0:1

0:4

0:3

0:2

algorithm modified. Section 4.2.1, alter environment labelling
tests immediate. Section 4.2.2, recognize shared costs, discount
group tests common cost. Section 4.2.3, experiment complex classification cost matrices, different types errors different costs. Section 4.2.4,
examine happens ICET trained certain penalty misclassification
errors, tested different penalty. four experiments, find ICET continues perform well.
4.2.1

TESTS IMMEDIATE

critic might object previous experiments show ICET superior
algorithms due sensitivity test costs classification error costs. Perhaps
ICET superior simply handle delayed tests, algorithms treat
tests immediate.13 is, method estimating average classification cost
(Section 2.2) biased favor ICET (since ICET uses method fitness function)
algorithms. experiment, labelled tests immediate. Otherwise, nothing changed baseline experiments. Table 7 summarizes results
experiment. ICET still performs well, although advantage algorithms
decreased slightly. Sensitivity delayed tests part explanation ICETs performance, whole story.
4.2.2

GROUP DISCOUNTS

Another hypothesis ICET superior simply handle groups tests
share common cost. experiment, eliminated group discounts tests share
common cost. is, test costs conditional prior tests. Otherwise, nothing
changed baseline experiments. Table 8 summarizes results experiment.
ICET maintains advantage algorithms.
4.2.3

COMPLEX CLASSIFICATION COST MATRICES

far, used simple classification cost matrices, penalty classification error types error. assumption inherent ICET.
13. algorithms cannot currently handle delayed tests, possible alter
way, handle delayed tests. comment also extends groups tests share common
cost. ICET might viewed alteration EG2 enables EG2 handle delayed tests common
costs.

387

fiT URNEY

Table 7: Average percentage standard cost no-delay experiment.
Algorithm

Average Classification Cost Percentage Standard
95% Confidence
Misclassification Error Costs
10.00 10,000.00

Misclassification Error Costs
10.00 100.00

ICET

47 6

28 4

EG2

54 4

36 2

CS-ID3

54 5

39 3

IDX

54 4

36 2

C4.5

64 6

59 4

Table 8: Average percentage standard cost no-discount experiment.
Algorithm

Average Classification Cost Percentage Standard
95% Confidence
Misclassification Error Costs
10.00 10,000.00

Misclassification Error Costs
10.00 100.00

ICET

46 6

25 5

EG2

56 5

42 3

CS-ID3

59 5

48 4

IDX

56 5

42 3

C4.5

75 5

80 4

element classification cost matrix different value. experiment,
explore ICETs behavior classification cost matrix complex.
use term positive error refer false positive diagnosis, occurs
patient diagnosed sick, patient actually healthy. Conversely, term
negative error refers false negative diagnosis, occurs patient diagnosed healthy, actually sick. term positive error cost cost
assigned positive errors, negative error cost cost assigned negative
errors. See Appendix examples. interested ICETs behavior ratio
negative positive error cost varied. Table 9 shows ratios examined.
Figure 5 shows performance five algorithms ratio.
hypothesis difference performance ICET algorithms would increase move away middles plots, ratio 1.0,
since algorithms mechanism deal complex classification cost;
designed implicit assumption simple classification cost matrices. fact,
Figure 5 shows difference tends decrease move away middles.
pronounced right-hand sides plots. ratio 8.0 (the
extreme right-hand sides plots), advantage using ICET. ratio
0.125 (the extreme left-hand sides plots), still advantage using ICET.
388

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

BUPA Liver Disease

Heart Disease
100
Average % Standard Cost

Average % Standard Cost

100
80
60
40
20
0
0.1

80
60
40
20
0

1.0

10.0

0.1

Ratio Negative Positive Error Cost

Hepatitis Prognosis
Average % Standard Cost

Average % Standard Cost

100

80
60
40
20
0

80
60
40
20
0

1.0

10.0

0.1

Ratio Negative Positive Error Cost

Thyroid Disease

10.0

Average Five Datasets
100
Average % Standard Cost

Average % Standard Cost

1.0
Ratio Negative Positive Error Cost

100
80
60
40
20
0
0.1

10.0

Pima Indians Diabetes

100

0.1

1.0
Ratio Negative Positive Error Cost

80
60
40
20
0

1.0

10.0

0.1

Ratio Negative Positive Error Cost

1.0

10.0

Ratio Negative Positive Error Cost

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 5: Average cost classification percentage standard cost
classification, complex classification cost matrices.
interpretation plots complicated fact gap algorithms tends decrease penalty classification errors increases (as see
Figure 3 retrospect, held sum negative error cost positive error cost constant value, varied ratio). However, clearly
asymmetry plots, expected symmetrical vertical line centered
1.0 x axis. plots close symmetrical algorithms,
asymmetrical ICET. also apparent Table 10, focuses comparison
performance ICET EG2, averaged across five datasets (see sixth plot
Figure 5). suggests difficult reduce negative errors (on right-hand
sides plots, negative errors weight) reduce positive errors (on
389

fiT URNEY

Table 9: Actual error costs ratio negative positive error cost.
Ratio Negative
Positive Error Cost

Negative
Error Cost

Positive
Error Cost

0.125

50

400

0.25

50

200

0.5

50

100

1.0

50

50

2.0

100

50

4.0

200

50

8.0

400

50

Table 10: Comparison ICET EG2
various ratios negative positive error cost.

Algorithm

Average Classification Cost Percentage Standard
95% Confidence, Ratio
Negative Positive Error Cost Varied
0.125

0.25

0.5

1.0

2.0

4.0

8.0

ICET

25 10

25 8

29 6

29 4

34 6

39 6

39 6

EG2

39 5

40 4

41 4

44 3

42 3

41 4

40 5

ICET/EG2 (as %)

64

63

71

66

81

95

98

left-hand sides, positive errors weight). is, easier avoid false positive diagnoses (a patient diagnosed sick, patient actually healthy)
avoid false negative diagnoses (a patient diagnosed healthy, actually
sick). unfortunate, since false negative diagnoses usually carry heavier penalty,
real-life. Preliminary investigation suggests false negative diagnoses harder avoid
sick class usually less frequent healthy class, makes
sick class harder learn.
4.2.4

POORLY ESTIMATED CLASSIFICATION COST

believe advantage ICET sensitive test costs classification error costs. However, might argued difficult calculate cost classification errors many real-world applications. Thus possible algorithm
ignores cost classification errors (e.g., EG2, CS-ID3, IDX) may robust
useful algorithm sensitive classification errors (e.g., ICET). address
possibility, examine happens ICET trained certain penalty classification errors, tested different penalty.
hypothesis ICET would robust reasonable differences
penalty training penalty testing. Table 11 shows happens
ICET trained penalty $100 classification errors, tested penalties
390

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 11: Performance training set classification error cost $100.

Algorithm

Average Classification Cost Percentage
Standard 95% Confidence, Testing Set
Classification Error Cost of:
$50

$100

$500

ICET

33 10

41 10

62 9

EG2

44 3

49 4

63 6

CS-ID3

49 5

54 6

65 7

IDX

43 3

49 4

63 6

C4.5

82 5

82 5

78 7

$50, $100, $500. see ICET best performance five algorithms,
although edge quite slight case penalty $500 testing.
also examined happens (1) ICET trained penalty $500
tested penalties $100, $500, $1,000 (2) ICET trained penalty
$1,000 tested penalties $500, $1,000, $5,000. results show essentially
pattern Table 11: ICET relatively robust differences training
testing penalties, least penalties order magnitude. suggests ICET applicable even situations reliability estimate
cost classification errors dubious.
penalty errors testing set $100, ICET works best penalty
errors training set also $100. penalty errors testing set
$500, ICET works best penalty errors training set also $500.
penalty errors testing set $1,000, ICET works best penalty errors
training set $500. suggests might advantage situations
underestimating penalty errors training. other, words ICET may
tendency overestimate benefits tests (this likely due overfitting training
data).
4.3

Searching Bias Space

final group experiments analyzes ICETs method searching bias space. Section
4.3.1 studies roles mutation crossover operators. appears crossover
mildly beneficial, compared pure mutation. Section 4.3.2 considers happens
ICET constrained search binary bias space, instead real bias space. constraint actually improves performance ICET. hypothesized improvement
due hidden advantage searching binary bias space: searching binary
bias space, ICET direct access true costs tests. However, advantage
available searching real bias space, initial population biases seeded
true costs tests. Section 4.3.3 shows seeding improves performance ICET.
4.3.1

CROSSOVER VERSUS MUTATION

Past work shown genetic algorithm crossover performs better genetic
algorithm mutation alone (Grefenstette et al., 1990; Wilson, 1987). section
391

fiT URNEY

attempts test hypothesis crossover improves performance ICET. test
hypothesis, sufficient merely set crossover rate zero. Since crossover
randomizing effect, similar mutation, must also increase mutation rate, compensate loss crossover (Wilson, 1987; Spears, 1992).
difficult analytically calculate increase mutation rate required
compensate loss crossover (Spears, 1992). Therefore experimentally tested
three different mutation settings.14 results summarized Table 12. crossover rate set zero, best mutation rate 0.10. misclassification error costs
$10 $10,000, performance ICET without crossover good performance ICET crossover, difference statistically significant. However,
comparison entirely fair crossover, since made attempt optimize
crossover rate (we simply used default value). results suggest crossover
mildly beneficial, prove pure mutation inferior.
Table 12: Average percentage standard cost mutation experiment.
Average Classification Cost Percentage
Standard 95% Confidence

ICET
Crossover
Rate

Mutation
Rate

Misclassification
Error Costs
10.00 10,000.00

Misclassification
Error Costs
10.00 100.00

0.6

0.001

49 7

29 7

0.0

0.05

51 8

32 9

0.0

0.10

50 8

29 8

0.0

0.15

51 8

30 9

4.3.2

SEARCH BINARY SPACE

ICET searches biases space n + 2 real numbers. Inspired Aha Bankert
(1994), decided see would happen ICET restricted space n
binary numbers 2 real numbers. modified ICET EG2 given true cost
test, instead pseudo-cost bias. conditional test costs, used nodiscount cost (see Section 4.2.2). n binary digits used exclude include test.
EG2 allowed use excluded tests decision trees generated.
precise, let B 1, , B n n binary numbers let C 1, , C n n real numbers. experiment, set C true cost i-th test. experiment, GENESIS change C . is, C constant given test given dataset. Instead,
GENESIS manipulates value B i. binary number B used determine
whether EG2 allowed use test decision tree. B = 0 , EG2 allowed
use i-th test (the i-th attribute). Otherwise, B = 1 , EG2 allowed use i-th
test. EG2 uses ICF equation usual, true costs C . Thus modified version
ICET searching binary bias space instead real bias space.
hypothesis ICET would perform better searching real bias space
14. three experiments took one week Sparc 10, tried three settings
mutation rate.

392

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

searching binary bias space. Table 13 shows hypothesis confirmed. appears better search binary bias space, rather real bias space.
However, differences statistically significant.
Table 13: Average percentage standard cost binary search experiment.
Algorithm

Average Classification Cost Percentage
Standard 95% Confidence
Misclassification
Error Costs
10.00 10,000.00

Misclassification
Error Costs
10.00 100.00

ICET Binary Space

48 6

26 5

ICET Real Space

49 7

29 7

EG2

58 5

43 3

CS-ID3

61 6

49 4

IDX

58 5

43 3

C4.5

77 5

82 4

searched binary space, set C true cost i-th test. GENESIS
manipulated B instead C . searched real space, GENESIS set C whatever
value found useful attempt optimize fitness. hypothesized gives
advantage binary space search real space search. Binary space search direct
access true costs tests, real space search learns true costs
tests indirectly, feedback gets fitness function.
examined experiment detail, found ICET well Heart
Disease dataset searching binary bias space, although poorly
searching real bias space (see Section 4.1.1). hypothesized ICET,
searching real space, suffered lack direct access true costs
applied Heart Disease dataset. hypotheses tested next experiment.
4.3.3

SEEDED POPULATION

experiment, returned searching real bias space, seeded initial population biases true test costs. gave ICET direct access true test costs.
conditional test costs, used no-discount cost (see Section 4.2.2). baseline
experiment (Section 4.1), initial population consists 50 randomly generated strings,
representing n + 2 real numbers. experiment, initial population consists 49 randomly generated strings one manually generated string. manually generated
string, first n numbers true test costs. last two numbers set 1.0 (for
) 25 (for CF). string exactly bias EG2, implemented (Section
3.2).
hypotheses (1) ICET would perform better (on average) initial
population seeded purely random, (2) ICET would perform better (on
average) searching real space seeded population searching binary
space,15 (3) ICET would perform better Heart Disease dataset ini393

fiT URNEY

tial population seeded purely random. Table 14 appears support first
two hypotheses. Figure 6 appears support third hypothesis. However, results
statistically significant.16
Table 14: Average percentage standard cost seeded population
experiment.
Algorithm

Average Classification Cost Percentage
Standard 95% Confidence
Misclassification
Error Costs
10.00 10,000.00

Misclassification
Error Costs
10.00 100.00

ICET Seeded
Search Real Space

46 6

25 5

ICET Unseeded
Search Real Space

49 7

29 7

ICET Unseeded
Search Binary Space

48 6

26 5

EG2

58 5

43 3

CS-ID3

61 6

49 4

IDX

58 5

43 3

C4.5

77 5

82 4

experiment raises interesting questions: seeding population
built ICET algorithm? seed whole population true costs, perturbed random noise? Perhaps right approach, prefer modify
ICF (equation (2)), device GENESIS controls decision tree induction.
could alter equation contains true costs bias parameters.17
seems make sense current approach, deprives EG2 direct
access true costs. discuss ideas modifying equation
Section 5.2.
Incidentally, experiment lets us answer following question: genetic
search bias space anything useful? start true costs tests reasonable values parameters CF, much improvement get
genetic search? experiment, seeded population individual represents exactly bias EG2 (the first n numbers true test costs last two numbers 1.0 25 CF). Therefore determine value genetic search
comparing EG2 ICET. ICET starts bias EG2 (as seed first genera15. Note make sense seed binary space search, since already direct access true
costs.
16. would need go current 10 trials (10 random splits data) 40 trials make
results significant. experiments reported took total 63 days continuous computation Sun
Sparc 10, 40 trials would require six months.
17. idea suggested conversation K. De Jong.

394

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Heart Disease
100

80

80

Average % Standard Cost

Average % Standard Cost

BUPA Liver Disease
100

60

40

20
0
10

60

40

20
0

100

1000

10000

10

Cost Misclassification Error

80

80

60

40

20
0

40

20
0

100

1000

10000

10

100

1000

10000

Cost Misclassification Error

Thyroid Disease

Average Five Datasets

100

100

80

80

Average % Standard Cost

Average % Standard Cost

10000

60

Cost Misclassification Error

60

40

20
0
10

1000

Pima Indians Diabetes
100
Average % Standard Cost

Average % Standard Cost

Hepatitis Prognosis
100

10

100

Cost Misclassification Error

60

40

20
0

100

1000

10000

10

Cost Misclassification Error

100

1000

10000

Cost Misclassification Error

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 6: Average cost classification percentage standard cost
classification seeded population experiment.
tion) attempts improve bias. score EG2 Table 14 shows value
bias built EG2. score ICET Table 14 shows genetic search bias space
improve built-in bias EG2. cost misclassification errors
order magnitude test costs ($10 $100), EG2 averages 43% standard cost,
ICET averages 25% standard cost. cost misclassification errors
ranges $10 $10,000, EG2 averages 58% standard cost, ICET averages
46% standard cost. differences significant 95% confidence. makes clear genetic search adding value.
395

fiT URNEY

5. Discussion
section compares ICET related work outlines possibilities future work.
5.1

Related Work

several algorithms sensitive test costs (Nez, 1988, 1991; Tan &
Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989). discussed, main limitation algorithms consider cost classification errors. cannot rationally determine whether test performed know cost
test cost classification errors.
also several algorithms sensitive classification error costs (Breiman
et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al., 1994). None
algorithms consider cost tests. Therefore focus complex classification
cost matrices, since, tests cost classification error matrix simple,
problem reduces maximizing accuracy.
FIS system (Pipitone et al., 1991) attempts find decision tree minimizes
average total cost tests required achieve certain level accuracy. approach
could implemented ICET altering fitness function. main distinction
FIS (Pipitone et al., 1991) ICET FIS learn data. information
gain test estimated using qualitative causal model, instead training cases. Qualitative causal models elicited domain experts, using special knowledge acquisition
tool. training data available, ICET used avoid need knowledge
acquisition. Otherwise, ICET applicable FIS approach suitable.
Another feature ICET perform purely greedy search. Several
authors proposed non-greedy classification algorithms (Tcheng et al., 1989; Ragavan &
Rendell, 1993; Norton, 1989; Schaffer, 1993; Rymon, 1993; Seshu, 1989). general,
results show advantage sophisticated search procedures. ICET
different algorithms uses genetic algorithm applied minimizing test costs classification error costs.
ICET uses two-tiered search strategy. bottom tier, EG2 performs greedy search
space classifiers. second tier, GENESIS performs non-greedy search
space biases. idea two-tiered search strategy (where first tier
search classifier space second tier search bias space) also appears (Provost,
1994; Provost & Buchanan, press; Aha & Bankert, 1994; Schaffer, 1993). work goes
beyond Aha Bankert (1994) considering search real bias space, rather search
binary space. work fits general framework Provost Buchanan (in
press), differs many details. example, method calculating cost special
case (Section 2.3).
researchers applied genetic algorithms classification problems. example, Frey Slate (1991) applied genetic algorithm (in particular, learning classifier system (LCS)) letter recognition. However, Fogarty (1992) obtained higher accuracy using
simple nearest neighbor algorithm. recent applications genetic algorithms classification successful (De Jong et al., 1993). However, work described
first application genetic algorithms problem cost-sensitive classification.
mentioned Section 2.1 decision theory may used define optimal solution problem cost-sensitive classification. However, searching optimal solution computationally infeasible (Pearl, 1988). attempted take decision theoretic
396

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

approach problem implementing AO* algorithm (Pearl, 1984) designing
heuristic evaluation function speed AO* search (Lirov & Yue, 1991).
unable make approach execute fast enough practical.
also attempted apply genetic programming (Koza, 1993) problem costsensitive classification. Again, unable make approach execute fast enough
practical, although faster AO* approach.
cost-sensitive classification problem, treated here, essentially
problem reinforcement learning (Sutton, 1992; Karakoulas, preparation). average
cost classification, measured described Section 2.2, reward/punishment signal
could optimized using reinforcement learning techniques. something
might explored alternative approach.
5.2

Future Work

paper discusses two types costs, cost tests cost misclassification
errors. two costs treated together decision theory, ICET first
machine learning system handles costs together. experiments paper
compared ICET machine learning systems handle test costs (Nez, 1988,
1991; Tan & Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989), compared
ICET machine learning systems handle classification error costs (Breiman
et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al., 1994). future
work, plan address omission. proper treatment issue would make
paper long.
absence comparison machine learning systems handle classification
error costs impact experiments reported here. experiments
paper focussed simple classification cost matrices (except Section 4.2.3).
classification cost matrix simple cost tests ignored, minimizing cost exactly
equivalent maximizing accuracy (see Section 2.3). Therefore, C4.5 (which designed
maximize accuracy) suitable surrogate systems handle classification error costs.
also experiment setting test costs zero. However, behavior
ICET penalty misclassification errors high (the extreme right-hand sides
plots Figure 3) necessarily behavior cost tests
low, since ICET sensitive relative differences test costs error costs,
absolute costs. Therefore (given behavior observe extreme right-hand
sides plots Figure 3) expect performance ICET tend converge performance algorithms cost tests approaches zero.
One natural addition ICET would ability output dont know class.
easily handled GENESIS component, extending classification cost matrix
cost assigned classifying case unknown. need also make small
modification EG2 component, generate decision trees leaves
labelled unknown. One way would introduce parameter defines
confidence threshold. Whenever confidence certain leaf drops confidence
threshold, leaf would labelled unknown. confidence parameter would made
accessible GENESIS component, could tuned minimize average classification cost.
mechanism ICET handling conditional test costs limitations.
397

fiT URNEY

currently implemented, handle cost attributes calculated
attributes. example, Thyroid dataset (Appendix A.5), FTI test calculated
based results TT4 T4U tests. FTI test selected, must pay
TT4 T4U tests. TT4 T4U tests already selected, FTI test free
(since calculation trivial). ability deal calculated test results could
added ICET relatively little effort.
ICET, currently implemented, handles two classes test results: tests
immediate results tests delayed results. Clearly continuous range
delays, seconds years. chosen treat delays distinct test costs,
could argued delay simply another type test cost. example, could
say group blood tests shares common cost one-day wait results. cost
one blood tests conditional whether prepared commit
one tests group, see results first test.
One difficultly approach handling delays problem assigning cost
delay. much cost bring patient two blood samples, instead one?
include disruption patients life estimate cost? avoid
questions, treated delays another type test cost, approach
readily handle continuous range delays.
cost test function several things: (1) function prior
tests selected. (2) function actual class case. (3)
function aspects case, information aspects may
available tests. (4) function test result. list seems
comprehensive, may possibilities overlooked. Let us consider
four possibilities.
First, cost test function prior tests selected. ICET
handles special case this, group tests shares common cost. currently
implemented, ICET handle general case. However, could easily add
capability ICET modifying fitness function.
Second, cost test function actual class case. example,
test heart disease might involve heavy exercise (Appendix A.2). patient actually
heart disease, exercise might trigger heart attack. risk included
cost particular test. Thus cost test vary, depending whether
patient actually heart disease. implemented this, although could easily
added ICET modifying fitness function.
Third, cost test function results tests. example, drawing blood newborn costly drawing blood adult. assign cost
blood test, need know age patient. age patient represented result another test patient-age test. slightly complex
preceding cases, must insure blood test always accompanied patient-age test. implemented this, although could added
ICET.
Fourth, cost test function test result. example, injecting
radio-opaque die X-ray might cause allergic reaction patient. risk
added cost test. makes cost test function one possible outcomes test. situation like this, may wise precede injection
die screening test allergies. could simple asking question
patient. question may relevance determining correct diagnosis
398

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

patient, may serve reduce average cost classification. case similar
third case, above. Again, implemented this, although could added
ICET.
Attribute selection EG2, CS-ID3, IDX shares common form. may view
n
attribute selection function { 1, , n } , takes input n information
gain values 1, , n (one attribute) generates output index one
attributes. may view C 1, , C n parameters attribute selection function. parameters may used control bias attribute selection procedure.
view, ICET uses GENESIS tune parameters EG2s attribute selection function.
future, would like investigate general attribute selection functions.
n
example, might use neural network implement function { 1, , n } .
GENESIS would used tune weights neural network.18 attribute
selection function might also benefit addition input specifies depth
decision tree current node, information gain values measured.
would enable bias test vary, depending many tests already
selected.
Another area future work explore parameter settings control GENESIS
(Table 4). many parameters could adjusted GENESIS. think significant ICET works well default parameter settings GENESIS, since
shows ICET robust respect parameters. However, might possible
substantially improve performance ICET tuning parameters. recent
trend genetic algorithm research let genetic algorithm adjust
parameters, mutation rate crossover rate (Whitley et al., 1993). Another possibility stop breeding fitness levels stop improving, instead stopping
fixed number generations. Provost Buchanan (in press) use goodness measure
stopping condition bias space search.

6. Conclusions
central problem investigated problem minimizing cost classification
tests expensive. argued requires assigning cost classification
errors. also argued decision tree natural form knowledge representation
type problem. presented general method calculating average cost
classification decision tree, given decision tree, information calculation
test costs, classification cost matrix, set testing data. method applicable
standard classification decision trees, without regard decision tree generated.
method sensitive test costs, sensitive classification error costs, capable handling conditional test costs, capable handling delayed tests.
introduced ICET, hybrid genetic decision tree induction algorithm. ICET uses
genetic algorithm evolve population biases decision tree induction algorithm.
individual population represents one set biases. fitness individual
determined using generate decision tree training dataset, calculating
average cost classification decision tree testing dataset.
analyzed behavior ICET series experiments, using five real-world medical datasets. Three groups experiments performed. first group looked
baseline performance five algorithms five datasets. ICET found sig18. idea suggested conversation M. Brooks.

399

fiT URNEY

nificantly lower costs algorithms. Although executes slowly, average time 23 minutes (for typical dataset) acceptable many applications,
possibility much greater speed parallel machine. second group experiments studied robustness ICET variety modifications input.
results show ICET robust. third group experiments examined ICETs search
bias space. discovered search could improved seeding initial population biases.
general, research concerned pragmatic constraints classification problems (Provost & Buchanan, press). believe many real-world classification problems involve merely maximizing accuracy (Turney, press). results
presented indicate that, certain applications, decision tree merely maximizes
accuracy (e.g., trees generated C4.5) may far performance possible
algorithm considers realistic constraints test costs, classification error
costs, conditional test costs, delayed test results. pragmatic
constraints faced real-world classification problems.

Appendix A. Five Medical Datasets
appendix presents test costs five medical datasets, taken Irvine collection (Murphy & Aha, 1994). costs based information Ontario Ministry
Health (1992). Although none medical data gathered Ontario, reasonable
assume areas similar relative test costs. purposes, relative costs
important, absolute costs.
A.1

BUPA Liver Disorders

BUPA Liver Disorders dataset created BUPA Medical Research Ltd.
donated Irvine collection Richard Forsyth.19 Table 15 shows test costs
BUPA Liver Disorders dataset. tests group blood tests thought
sensitive liver disorders might arise excessive alcohol consumption. tests
share common cost $2.10 collecting blood. target concept defined using
sixth column: Class 0 defined drinks < 3 class 1 defined drinks
3. Table 16 shows general form classification cost matrix used
experiments Section 4. experiments, classification error cost equals
positive error cost equals negative error cost. exception Section 4.2.3,
experiments complex classification cost matrices. terms positive error cost
negative error cost explained Section 4.2.3. 345 cases dataset,
missing values. Column seven originally used split data training testing sets. use column, since required ten different random splits
data. ten random splits, ten training sets 230 cases ten testing sets
115 cases.
A.2

Heart Disease

Heart Disease dataset donated Irvine collection David Aha.20 princi19. BUPA Liver Disorders dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liverdisorders/bupa.data.
20. Heart Disease dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/heart-disease/
cleve.mod.

400

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 15: Test costs BUPA Liver Disorders dataset.
Test

Description

Group

Cost

Delayed

1

mcv

mean corpuscular volume



$7.27 first test group A,
$5.17 otherwise

yes

2

alkphos

alkaline phosphotase



$7.27 first test group A,
$5.17 otherwise

yes

3

sgpt

alamine aminotransferase



$7.27 first test group A,
$5.17 otherwise

yes

4

sgot

aspartate aminotransferase



$7.27 first test group A,
$5.17 otherwise

yes

5

gammagt

gamma-glutamyl transpeptidase



$9.86 first test group A,
$7.76 otherwise

yes

6

drinks

number half-pint equivalents
alcoholic beverages drunk per day

diagnostic class: drinks < 3
drinks 3

-

7

selector

field used split data two sets

used

-

Table 16: Classification costs BUPA Liver Disorders dataset.
Actual Class

Guess Class

Cost

0 (drinks < 3)

0 (drinks < 3)

$0.00

0 (drinks < 3)

1 (drinks 3)

Positive Error Cost

1 (drinks 3)

0 (drinks < 3)

Negative Error Cost

1 (drinks 3)

1 (drinks 3)

$0.00

pal medical investigator Robert Detrano, Cleveland Clinic Foundation. Table 17
shows test costs Heart Disease dataset. nominal cost $1.00 assigned
first four tests. tests group blood tests thought relevant
heart disease. tests share common cost $2.10 collecting blood. tests
groups B C involve measurements heart exercise. nominal cost $1.00
assigned tests first test groups. class variable
values buff (healthy) sick. fifteenth column, specified class
variable H (healthy), S1, S2, S3, S4 (four different types sick),
deleted column. Table 18 shows classification cost matrix. 303 cases
dataset. deleted cases missing values. reduced
dataset 296 cases. ten random splits, training sets 197 cases testing
sets 99 cases.
A.3

Hepatitis Prognosis

Hepatitis Prognosis dataset donated Gail Gong.21 Table 19 shows test costs
Hepatitis dataset. Unlike four datasets, dataset deals prognosis,
21. Hepatitis Prognosis dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/hepatitis/
hepatitis.data.

401

fiT URNEY

Table 17: Test costs Heart Disease dataset.
Test

Description

1

age

2

Group

Cost

Delayed

age years

$1.00



sex

patients gender

$1.00



3

cp

chest pain type

$1.00



4

trestbps

resting blood pressure

$1.00



5

chol

serum cholesterol



$7.27 first test group A,
$5.17 otherwise

yes

6

fbs

fasting blood sugar



$5.20 first test group A,
$3.10 otherwise

yes

7

restecg

resting electrocardiograph

$15.50

yes

8

thalach

maximum heart rate
achieved

B

$102.90 first test group B,
$1.00 otherwise

yes

9

exang

exercise induced angina

C

$87.30 first test group C,
$1.00 otherwise

yes

10

oldpeak

ST depression induced
exercise relative rest

C

$87.30 first test group C,
$1.00 otherwise

yes

11

slope

slope peak exercise ST
segment

C

$87.30 first test group C,
$1.00 otherwise

yes

12

ca

number major vessels
coloured fluoroscopy

$100.90

yes

13

thal

3 = normal; 6 = fixed defect;
7 = reversible defect

$102.90 first test group B,
$1.00 otherwise

yes

14

num

diagnosis heart disease

diagnostic class

-

B

Table 18: Classification costs Heart Disease dataset.
Actual Class

Guess Class

Cost

buff

buff

$0.00

buff

sick

Positive Error Cost

sick

buff

Negative Error Cost

sick

sick

$0.00

diagnosis. prognosis, diagnosis known, problem determine likely
outcome disease. tests assigned nominal cost $1.00 either involve
asking question patient performing basic physical examination patient.
tests group share cost $2.10 collecting blood. Note that, although performing histological examination liver costs $81.64, asking patient whether
histology performed costs $1.00. Thus prognosis exploit information
conveyed decision (to perform histological examination) made
diagnosis. class variable values 1 (die) 2 (live). Table 20 shows classification costs. dataset contains 155 cases, many missing values. ten random
402

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 19: Test costs Hepatitis Prognosis dataset.
Test

Description

1

class

2

Group

Cost

Delayed

prognosis hepatitis

prognostic class: live die

-

age

age years

$1.00



3

sex

gender

$1.00



4

steroid

patient steroids

$1.00



5

antiviral

patient antiviral

$1.00



6

fatigue

patient reports fatigue

$1.00



7

malaise

patient reports malaise

$1.00



8

anorexia

patient anorexic

$1.00



9

liver big

liver big physical exam

$1.00



10

liver firm

liver firm physical exam

$1.00



11

spleen palpable

spleen palpable physical

$1.00



12

spiders

spider veins visible

$1.00



13

ascites

ascites visible

$1.00



14

varices

varices visible

$1.00



15

bilirubin

bilirubin blood test



$7.27 first test group A,
$5.17 otherwise

yes

16

alk phosphate

alkaline phosphotase



$7.27 first test group A,
$5.17 otherwise

yes

17

sgot

aspartate aminotransferase



$7.27 first test group A,
$5.17 otherwise

yes

18

albumin

albumin blood test



$7.27 first test group A,
$5.17 otherwise

yes

19

protime

protime blood test



$8.30 first test group A,
$6.20 otherwise

yes

20

histology

histology performed?

$1.00



Table 20: Classification costs Hepatitis Prognosis dataset.
Actual Class

Guess Class

Cost

1 (die)

1 (die)

$0.00

1 (die)

2 (live)

Negative Error Cost

2 (live)

1 (die)

Positive Error Cost

2 (live)

2 (live)

$0.00

splits, training sets 103 cases testing sets 52 cases. filled missing values, using simple single nearest neighbor algorithm (Aha et al., 1991). missing
values filled using whole dataset, dataset split training
testing sets. nearest neighbor algorithm, data normalized mini403

fiT URNEY

mum value feature 0 maximum value 1. distance measure used
sum absolute values differences. difference two values
defined 1 one two values missing.
A.4

Pima Indians Diabetes

Pima Indians Diabetes dataset donated Vincent Sigillito. 22 data collected National Institute Diabetes Digestive Kidney Diseases. Table 21
shows test costs Pima Indians Diabetes dataset. tests group share
cost $2.10 collecting blood. remaining tests assigned nominal cost
$1.00. patients females least 21 years old Pima Indian heritage.
class variable values 0 (healthy) 1 (diabetes). Table 22 shows classification costs.
dataset includes 768 cases, missing values. ten random splits, training
sets 512 cases testing sets 256 cases.
Table 21: Test costs Pima Indians Diabetes dataset.
Test

Description

Group

Cost

Delayed

1

times pregnant

number times pregnant

$1.00



2

glucose tol

glucose tolerance test

$17.61 first test group A,
$15.51 otherwise

yes

3

diastolic bp

diastolic blood pressure

$1.00



4

triceps

triceps skin fold thickness

$1.00



5

insulin

serum insulin test

$22.78 first test group A,
$20.68 otherwise

yes

6

mass index

body mass index

$1.00



7

pedigree

diabetes pedigree function

$1.00



8

age

age years

$1.00



9

class

diagnostic class

diagnostic class

-





Table 22: Classification costs Pima Indians Diabetes dataset.

A.5

Actual Class

Guess Class

Cost

0 (healthy)

0 (healthy)

$0.00

0 (healthy)

1 (diabetes)

Positive Error Cost

1 (diabetes)

0 (healthy)

Negative Error Cost

1 (diabetes)

1 (diabetes)

$0.00

Thyroid Disease

Thyroid Disease dataset created Garavan Institute, Sydney, Australia.
file donated Randolf Werner, obtained Daimler-Benz. Daimler-Benz obtained
22. Pima Indians Diabetes dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/pimaindians-diabetes/pima-indians-diabetes.data.

404

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

data J.R. Quinlan.23 Table 23 shows test costs Thyroid Disease dataset.
nominal cost $1.00 assigned first 16 tests. tests group share cost
$2.10 collecting blood. FTI test involves calculation based results
TT4 T4U tests. complicates calculation costs three tests,
chose use FTI test experiments. class variable values 1
(hypothyroid), 2 (hyperthyroid), 3 (normal). Table 24 shows classification costs.
3772 cases dataset, missing values. ten random splits,
training sets 2515 cases testing sets 1257 cases.
Table 23: Test costs Thyroid Disease dataset.
Test

Description

1

age

2

Group

Cost

Delayed

age years

$1.00



sex

gender

$1.00



3

thyroxine

patient thyroxine

$1.00



4

query thyroxine

maybe thyroxine

$1.00



5

antithyroid

antithyroid medication

$1.00



6

sick

patient reports malaise

$1.00



7

pregnant

patient pregnant

$1.00



8

thyroid surgery

history thyroid surgery

$1.00



9

I131 treatment

patient I131 treatment

$1.00



10

query hypothyroid

maybe hypothyroid

$1.00



11

query hyperthyroid

maybe hyperthyroid

$1.00



12

lithium

patient lithium

$1.00



13

goitre

patient goitre

$1.00



14

tumour

patient tumour

$1.00



15

hypopituitary

patient hypopituitary

$1.00



16

psych

psychological symptoms

$1.00



17

TSH

TSH value, measured



$22.78 first test group A, yes
$20.68 otherwise

18

T3

T3 value, measured



$11.41 first test group A, yes
$9.31 otherwise

19

TT4

TT4 value, measured



$14.51 first test group A, yes
$12.41 otherwise

20

T4U

T4U value, measured



$11.41 first test group A, yes
$9.31 otherwise

21

FTI

FTI calculated
TT4 T4U

used

-

22

class

diagnostic class

diagnostic class

-

23. Thyroid Disease dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/thyroid-disease/ann-train.data.

405

fiT URNEY

Table 24: Classification costs Thyroid Disease dataset.
Actual Class

Guess Class

Cost

1 (hypothyroid)

1 (hypothyroid)

$0.00

1 (hypothyroid)

2 (hyperthyroid)

Minimum(Negative Error Cost, Positive Error Cost)

1 (hypothyroid)

3 (normal)

Negative Error Cost

2 (hyperthyroid)

1 (hypothyroid)

Minimum(Negative Error Cost, Positive Error Cost)

2 (hyperthyroid)

2 (hyperthyroid)

$0.00

2 (hyperthyroid)

3 (normal)

Negative Error Cost

3 (normal)

1 (hypothyroid)

Positive Error Cost

3 (normal)

2 (hyperthyroid)

Positive Error Cost

3 (normal)

3 (normal)

$0.00

Acknowledgments
Thanks Dr. Louise Linney help interpretation Ontario Ministry
Healths Schedule Benefits. Thanks Martin Brooks, Grigoris Karakoulas, Cullen Schaffer, Diana Gordon, Tim Niblett, Steven Minton, three anonymous referees JAIR
helpful comments earlier versions paper. work presented
informal talks University Ottawa Naval Research Laboratory. Thanks
audiences feedback.

References
Ackley, D., & Littman, M. (1991). Interactions learning evolution. Proceedings Second Conference Artificial Life, C. Langton, C. Taylor, D. Farmer, S.
Rasmussen, editors. California: Addison-Wesley.
Aha, D.W., Kibler, D., & Albert, M.K. (1991). Instance-based learning algorithms, Machine
Learning, 6, 37-66.
Aha, D.W., & Bankert, R.L. (1994). Feature selection case-based classification cloud
types: empirical comparison. Case-Based Reasoning: Papers 1994 Workshop, edited D.W. Aha, Technical Report WS-94-07, pp. 106-112. Menlo Park, CA:
AAAI Press.
Anderson, R.W. (in press). Learning evolution: quantitative genetics approach. Journal Theoretical Biology.
Baldwin, J.M. (1896). new factor evolution. American Naturalist, 30, 441-451.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification regression
trees. California: Wadsworth.
De Jong, K.A., Spears, W.M., & Gordon, D.F. (1993). Using genetic algorithms concept
learning. Machine Learning, 13, 161-188.

406

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Fogarty, T.C. (1992). Technical note: First nearest neighbor classification Frey Slates
letter recognition problem. Machine Learning, 9, 387-388.
Frey, P.W., & Slate, D.J., (1991). Letter recognition using Holland-style adaptive classifiers.
Machine Learning, 6, 161-182.
Friedman, J.H., & Stuetzle, W. (1981). Projection pursuit regression. Journal American Statistics Association, 76, 817-823.
Gordon, D.F., & Perlis, D. (1989). Explicitly biased generalization. Computational Intelligence, 5, 67-81.
Grefenstette, J.J. (1986). Optimization control parameters genetic algorithms. IEEE
Transactions Systems, Man, Cybernetics, 16, 122-128.
Grefenstette, J.J., Ramsey, C.L., & Schultz, A.C. (1990). Learning sequential decision rules
using simulation models competition. Machine Learning, 5, 355-381.
Hermans, J., Habbema, J.D.F., & Van der Burght, A.T. (1974). Cases doubt allocation
problems, k populations. Bulletin International Statistics Institute, 45, 523-529.
Hinton, G.E., & Nowlan, S.J. (1987). learning guide evolution. Complex Systems,
1, 495-502.
Karakoulas, G. (in preparation). Q-learning approach cost-effective classification. Technical Report, Knowledge Systems Laboratory, National Research Council Canada. Also
submitted Twelfth International Conference Machine Learning, ML-95.
Knoll, U., Nakhaeizadeh, G., & Tausend, B. (1994). Cost-sensitive pruning decision trees.
Proceedings Eight European Conference Machine Learning, ECML-94, pp.
383-386. Berlin, Germany: Springer-Verlag.
Koza, J.R. (1992). Genetic Programming: programming computers means
natural selection. Cambridge, MA: MIT Press.
Lirov, Y., & Yue, O.-C. (1991). Automated network troubleshooting knowledge acquisition.
Journal Applied Intelligence, 1, 121-132.
Maynard Smith, J. (1987). learning guides evolution. Nature, 329, 761-762.
Morgan, C.L. (1896). modification variation. Science, 4, 733-740.
Murphy, P.M., & Aha, D.W. (1994). UCI Repository Machine Learning Databases. University California Irvine, Department Information Computer Science.
Norton, S.W. (1989). Generating better decision trees. Proceedings Eleventh International Joint Conference Artificial Intelligence, IJCAI-89, pp. 800-805. Detroit, Michigan.
Nez, M. (1988). Economic induction: case study. Proceedings Third European
Working Session Learning, EWSL-88, pp. 139-145. California: Morgan Kaufmann.

407

fiT URNEY

Nez, M. (1991). use background knowledge decision tree induction. Machine
Learning, 6, 231-250.
Ontario Ministry Health (1992). Schedule benefits: Physician services health
insurance act, October 1, 1992. Ontario: Ministry Health.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing misclassification costs: Knowledge-intensive approaches learning noisy data. Proceedings Eleventh International Conference Machine Learning, ML-94, pp. 217225. New Brunswick, New Jersey.
Pearl, J. (1984). Heuristics: Intelligent search strategies computer problem solving. Massachusetts: Addison-Wesley.
Pearl, J. (1988). Probabilistic reasoning intelligent systems: Networks plausible inference. California: Morgan Kaufmann.
Pipitone, F., De Jong, K.A., & Spears, W.M. (1991). artificial intelligence approach
analog systems diagnosis. Testing Diagnosis Analog Circuits Systems,
Ruey-wen Liu, editor. New York: Van Nostrand-Reinhold.
Provost, F.J. (1994). Goal-directed inductive learning: Trading accuracy reduced error
cost. AAAI Spring Symposium Goal-Driven Learning.
Provost, F.J., & Buchanan, B.G. (in press). Inductive policy: pragmatics bias selection. Machine Learning.
Quinlan, J.R. (1992). C4.5: Programs machine learning. California: Morgan Kaufmann.
Ragavan, H., & Rendell, L. (1993). Lookahead feature construction learning hard concepts. Proceedings Tenth International Conference Machine Learning, ML-93,
pp. 252-259. California: Morgan Kaufmann.
Rymon, R. (1993). SE-tree based characterization induction problem. Proceedings
Tenth International Conference Machine Learning, ML-93, pp. 268-275. California: Morgan Kaufmann.
Schaffer, C. (1993). Selecting classification method cross-validation. Machine Learning, 13, 135-143.
Schaffer, J.D., Whitley, D., & Eshelman, L.J. (1992). Combinations genetic algorithms
neural networks: survey state art. Combinations Genetic Algorithms Neural Networks, D. Whitley J.D. Schaffer, editors. California: IEEE
Computer Society Press.
Seshu, R. (1989). Solving parity problem. Proceedings Fourth European Working
Session Learning, EWSL-89, pp. 263-271. California: Morgan Kaufmann.
Spears, W.M. (1992). Crossover mutation? Foundations Genetic Algorithms 2, FOGA92, edited D. Whitley. California: Morgan Kaufmann.

408

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Sutton, R.S. (1992). Introduction: challenge reinforcement learning. Machine Learning, 8, 225-227.
Tan, M., & Schlimmer, J. (1989). Cost-sensitive concept learning sensor use approach
recognition. Proceedings Sixth International Workshop Machine Learning,
ML-89, pp. 392-395. Ithaca, New York.
Tan, M., & Schlimmer, J. (1990). CSL: cost-sensitive learning system sensing
grasping objects. IEEE International Conference Robotics Automation. Cincinnati, Ohio.
Tan, M. (1993). Cost-sensitive learning classification knowledge applications
robotics. Machine Learning, 13, 7-33.
Tcheng, D., Lambert, B., Lu, S., Rendell, L. (1989). Building robust learning systems
combining induction optimization. Proceedings Eleventh International Joint
Conference Artificial Intelligence, IJCAI-89, pp. 806-812. Detroit, Michigan.
Turney, P.D. (in press). Technical note: Bias quantification stability. Machine
Learning.
Verdenius, F. (1991). method inductive cost optimization. Proceedings Fifth
European Working Session Learning, EWSL-91, pp. 179-191. New York: SpringerVerlag.
Waddington, C.H. (1942). Canalization development inheritance acquired characters. Nature, 150, 563-565.
Whitley, D., Dominic, S., Das, R., & Anderson, C.W. (1993). Genetic reinforcement learning
neurocontrol problems. Machine Learning, 13, 259-284.
Whitley, D., & Gruau, F. (1993). Adding learning cellular development neural networks: Evolution Baldwin effect. Evolutionary Computation, 1, 213-233.
Whitley, D., Gordon, S., & Mathias, K. (1994). Lamarckian evolution, Baldwin effect
function optimization. Parallel Problem Solving Nature PPSN III. Y. Davidor, H.P. Schwefel, R. Manner, editors, pp. 6-15. Berlin: Springer-Verlag.
Wilson, S.W. (1987). Classifier systems animat problem. Machine Learning, 2, 199228.

409

fiJournal Artificial Intelligence Research 2 (1995) 263{286

Submitted 8/94; published 1/95

Solving Multiclass Learning Problems via
Error-Correcting Output Codes
Thomas G. Dietterich

tgd@cs.orst.edu

Department Computer Science, 303 Dearborn Hall
Oregon State University
Corvallis, 97331 USA

Ghulum Bakiri

eb004@isa.cc.uob.bh

Department Computer Science
University Bahrain
Isa Town, Bahrain

Abstract

Multiclass learning problems involve finding definition unknown function (x)
whose range discrete set containing
2 values (i.e., \classes"). definition
acquired studying collections training examples form hx (x )i. Existing approaches multiclass learning problems include direct application multiclass algorithms
decision-tree algorithms C4.5 CART, application binary concept learning
algorithms learn individual binary functions classes, application
binary concept learning algorithms distributed output representations. paper
compares three approaches new technique error-correcting codes
employed distributed output representation. show output representations improve generalization performance C4.5 backpropagation wide
range multiclass learning tasks. also demonstrate approach robust
respect changes size training sample, assignment distributed representations particular classes, application overfitting avoidance techniques
decision-tree pruning. Finally, show that|like methods|the error-correcting
code technique provide reliable class probability estimates. Taken together, results demonstrate error-correcting output codes provide general-purpose method
improving performance inductive learning programs multiclass problems.
f

k >

k

i; f



k

1. Introduction

task learning examples find approximate definition unknown
function f (x) given training examples form hx ; f (x )i. cases f takes
values f0; 1g|binary functions|there many algorithms available. example,
decision-tree methods, C4.5 (Quinlan, 1993) CART (Breiman, Friedman,
Olshen, & Stone, 1984) construct trees whose leaves labeled binary values.
artificial neural network algorithms, perceptron algorithm (Rosenblatt,
1958) error backpropagation (BP) algorithm (Rumelhart, Hinton, & Williams,
1986), best suited learning binary functions. Theoretical studies learning
focused almost entirely learning binary functions (Valiant, 1984; Natarajan, 1991).
many real-world learning tasks, however, unknown function f often takes values
discrete set \classes": fc1; : : : ; c g. example, medical diagnosis, function
might map description patient one k possible diseases. digit recognition (e.g.,




k

c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDietterich & Bakiri
LeCun, Boser, Denker, Henderson, Howard, Hubbard, & Jackel, 1989), function maps
hand-printed digit one k = 10 classes. Phoneme recognition systems (e.g., Waibel,
Hanazawa, Hinton, Shikano, & Lang, 1989) typically classify speech segment one
50 60 phonemes.
Decision-tree algorithms easily generalized handle \multiclass" learning
tasks. leaf decision tree labeled one k classes, internal
nodes selected discriminate among classes. call direct
multiclass approach.
Connectionist algorithms dicult apply multiclass problems. standard approach learn k individual binary functions f1 ; : : : ; f , one class.
assign new case, x, one classes, f evaluated x, x
assigned class j function f returns highest activation (Nilsson, 1965).
call one-per-class approach, since one binary function learned
class.
alternative approach explored researchers employ distributed output
code. approach pioneered Sejnowski Rosenberg (1987) widelyknown NETtalk system. class assigned unique binary string length n;
refer strings \codewords." n binary functions learned, one bit
position binary strings. training example class i, desired
outputs n binary functions specified codeword class i. artificial
neural networks, n functions implemented n output units single
network.
New values x classified evaluating n binary functions generate
n-bit string s. string compared k codewords, x assigned
class whose codeword closest, according distance measure, generated
string s.
example, consider Table 1, shows six-bit distributed code ten-class
digit-recognition problem. Notice row distinct, class unique
codeword. applications distributed output codes, bit positions (columns)
chosen meaningful. Table 2 gives meanings six columns.
learning, one binary function learned column. Notice column
also distinct binary function learned disjunction original
classes. example, f (x) = 1 f (x) 1, 4, 5.
classify new hand-printed digit, x, six functions f ; f ; f ; f ; f ; f
evaluated obtain six-bit string, 110001. distance string
ten codewords computed. nearest codeword, according Hamming
distance (which counts number bits differ), 110000, corresponds class
4. Hence, predicts f (x) = 4.
process mapping output string nearest codeword identical decoding step error-correcting codes (Bose & Ray-Chaudhuri, 1960; Hocquenghem, 1959).
suggests might advantage employing error-correcting codes
distributed representation. Indeed, idea employing error-correcting, distributed
representations traced early research machine learning (Duda, Machanik, &
Singleton, 1963).
k



j

vl

vl

264

hl

dl

cc

ol



fiError-Correcting Output Codes

Table 1: distributed code digit recognition task.
Class
0
1
2
3
4
5
6
7
8
9

vl
0
1
0
0
1
1
0
0
0
0

Code Word
hl dl cc ol
0 0 1 0
0 0 0 0
1 1 0 1
0 0 0 1
1 0 0 0
1 0 0 1
0 1 1 0
0 1 0 0
0 0 1 0
0 1 1 0


0
0
0
0
0
0
1
0
0
0

Table 2: Meanings six columns code Table 1.
Column position Abbreviation
Meaning
1
vl
contains vertical line
2
hl
contains horizontal line
3
dl
contains diagonal line
4
cc
contains closed curve
5
ol
contains curve open left
6

contains curve open right
Table 3: 15-bit error-correcting output code ten-class problem.
Class
0
1
2
3
4
5
6
7
8
9

f0

1
0
1
0
1
0
1
0
1
0

f1

1
0
0
0
1
1
0
0
1
1

f2

0
1
0
1
1
0
1
0
0
1

f3

0
1
1
1
0
0
1
1
1
1

f4

0
1
0
0
1
1
1
1
0
0

f5

0
1
0
1
0
1
0
1
1
0

f6

1
0
0
1
1
0
0
1
1
0

Code Word
f7

0
1
1
1
1
1
0
0
0
0

265

f8

1
0
1
0
0
1
0
1
0
1

f9

0
1
1
0
0
1
1
0
1
0

f10

0
1
1
0
1
0
0
1
0
1

f11

1
0
0
0
0
0
1
1
0
0

f12

1
0
1
1
0
0
0
0
0
0

f13

0
1
0
0
0
0
0
0
1
1

f14

1
0
1
1
1
1
1
1
1
1

fiDietterich & Bakiri
Table 3 shows 15-bit error-correcting code digit-recognition task. class
represented code word drawn error-correcting code. distributed
encoding Table 1, separate boolean function learned bit position errorcorrecting code. classify new example x, learned functions f0 (x); : : :; f14(x)
evaluated produce 15-bit string. mapped nearest ten
codewords. code correct three errors 15 bits.
error-correcting code approach suggests view machine learning kind
communications problem identity correct output class new
example \transmitted" channel. channel consists input features,
training examples, learning algorithm. errors introduced
finite training sample, poor choice input features, aws learning process,
class information corrupted. encoding class error-correcting code
\transmitting" bit separately (i.e., via separate run learning algorithm),
system may able recover errors.
perspective suggests one-per-class \meaningful" distributed
output approaches inferior, output representations constitute
robust error-correcting codes. measure quality error-correcting code
minimum Hamming distance pair code words. minimum Hamming
distance d, code correct least b ,2 1 c single bit errors.
single bit error moves us one unit away true codeword (in Hamming distance).
make b ,2 1 c errors, nearest codeword still correct codeword. (The
code Table 3 minimum Hamming distance seven hence correct errors
three bit positions.) Hamming distance two codewords oneper-class code two, one-per-class encoding k output classes cannot correct
errors.
minimum Hamming distance pairs codewords \meaningful" distributed representation tends low. example, Table 1, Hamming
distance codewords classes 4 5 one. kinds codes, new
columns often introduced discriminate two classes. two classes
therefore differ one bit position, Hamming distance output
representations one. also true distributed representation developed
Sejnowski Rosenberg (1987) NETtalk task.
paper, compare performance error-correcting code approach
three existing approaches: direct multiclass method (using decision trees),
one-per-class method, (in NETtalk task only) meaningful distributed output
representation approach. show error-correcting codes produce uniformly better
generalization performance across variety multiclass domains C4.5 decisiontree learning algorithm backpropagation neural network learning algorithm.
report series experiments designed assess robustness error-correcting
code approach various changes learning task: length code, size training
set, assignment codewords classes, decision-tree pruning. Finally, show
error-correcting code approach produce reliable class probability estimates.
paper concludes discussion open questions raised results.
Chief among questions issue errors made different bit
positions output somewhat independent one another. Without independ



266

fiError-Correcting Output Codes
Table 4: Data sets employed study.
Name
glass
vowel
POS
soybean
audiologyS
ISOLET
letter
NETtalk

Number Number
Number
Number
Features
Classes
Training Examples Test Examples
9
6
214
10-fold xval
10
11
528
462
30
12
3,060
10-fold xval
35
19
307
376
69
24
200
26
617
26
6,238
1,559
16
26
16,000
4,000
203
54 phonemes
1000 words =
1000 words =
6 stresses
7,229 letters
7,242 letters

dence, error-correcting output code method would fail. address question|for
case decision-tree algorithms|in companion paper (Kong & Dietterich, 1995).

2. Methods
section describes data sets learning algorithms employed study.
also discusses issues involved design error-correcting codes describes four
algorithms code design. section concludes brief description methods
applied make classification decisions evaluate performance independent test sets.

2.1 Data Sets
Table 4 summarizes data sets employed study. glass, vowel, soybean, audiologyS, ISOLET, letter, NETtalk data sets available Irvine Repository
machine learning databases (Murphy & Aha, 1994).1 POS (part speech) data set
provided C. Cardie (personal communication); earlier version data set
described Cardie (1993). use entire NETtalk data set, consists
dictionary 20,003 words pronunciations. Instead, make experiments
feasible, chose training set 1000 words disjoint test set 1000 words
random NETtalk dictionary. paper, focus percentage letters
pronounced correctly (rather whole words). pronounce letter, phoneme
stress letter must determined. Although 54 6 syntactically possible
combinations phonemes stresses, 140 appear training test
sets selected.
1. repository refers soybean data set \soybean-large", \audiologyS" data set \audiology.standardized", \letter" data set \letter-recognition".

267

fiDietterich & Bakiri

2.2 Learning Algorithms

employed two general classes learning methods: algorithms learning decision trees
algorithms learning feed-forward networks sigmoidal units (artificial neural networks). decision trees, performed experiments using C4.5, Release 1,
older (but substantially identical) version program described Quinlan (1993).
made several changes C4.5 support distributed output representations,
affected tree-growing part algorithm. pruning, confidence
factor set 0.25. C4.5 contains facility creating \soft thresholds" continuous
features. found experimentally improved quality class probability
estimates produced algorithm \glass", \vowel", \ISOLET" domains,
results reported domains computed using soft thresholds.
neural networks, employed two implementations. domains, used
extremely fast backpropagation implementation provided CNAPS neurocomputer
(Adaptive Solutions, 1992). performs simple gradient descent fixed learning
rate. gradient updated presenting training example; momentum term
employed. potential limitation CNAPS inputs represented
eight bits accuracy, weights represented 16 bits accuracy. Weight
update arithmetic round, instead performs jamming (i.e., forcing lowest
order bit 1 low order bits lost due shifting multiplication).
speech recognition, letter recognition, vowel data sets, employed opt system
distributed Oregon Graduate Institute (Barnard & Cole, 1989). implements
conjugate gradient algorithm updates gradient complete pass
training examples (known per-epoch updating). learning rate required
approach.
CNAPS opt attempt minimize squared error computed
desired outputs network. Many researchers employed error measures,
particularly cross-entropy (Hinton, 1989) classification figure-of-merit (CFM, Hampshire II & Waibel, 1990). Many researchers also advocate using softmax normalizing layer
outputs network (Bridle, 1990). configurations good
theoretical support, Richard Lippmann (1991) report squared error works
well measures producing accurate posterior probability estimates. Furthermore, cross-entropy CFM tend overfit easily squared error (Lippmann,
personal communication; Weigend, 1993). chose minimize squared error
CNAPS opt systems implement.
either neural network algorithm, several parameters must chosen user.
CNAPS, must select learning rate, initial random seed, number
hidden units, stopping criteria. selected optimize performance
validation set, following methodology Lang, Hinton, Waibel (1990).
training set subdivided subtraining set validation set. training
subtraining set, observed generalization performance validation set determine
optimal settings learning rate network size best point
stop training. training set mean squared error stopping point computed,
training performed entire training set using chosen parameters
stopping indicated mean squared error. Finally, measure network performance
test set.
268

fiError-Correcting Output Codes
data sets, procedure worked well. However, letter
recognition data set, clearly choosing poor stopping points full training set.
overcome problem, employed slightly different procedure determine
stopping epoch. trained series progressively larger training sets (all
subsets final training set). Using validation set, determined best
stopping epoch training sets. extrapolated training
sets predict best stopping epoch full training set.
\glass" \POS" data sets, employed ten-fold cross-validation assess
generalization performance. chose training parameters based one \fold"
ten-fold cross-validation. creates test set contamination, since examples
validation set data one fold test set data folds. However, found
little overfitting, validation set little effect choice
parameters stopping points.
data sets come designated test sets, employed measure
generalization performance.

2.3 Error-Correcting Code Design

define error-correcting code matrix binary values matrix shown
Table 3. length code number columns code. number
rows code equal number classes multiclass learning problem.
\codeword" row code.
good error-correcting output code k-class problem satisfy two properties:
Row separation. codeword well-separated Hamming distance
codewords.
Column separation. bit-position function f uncorrelated
functions learned bit positions f ; j 6= i: achieved
insisting Hamming distance column columns
large Hamming distance column complement
columns also large.


j

power code correct errors directly related row separation,
discussed above. purpose column separation condition less obvious. two
columns j similar identical, deterministic learning algorithm
C4.5 applied learn f f , make similar (correlated) mistakes. Errorcorrecting codes succeed errors made individual bit positions relatively
uncorrelated, number simultaneous errors many bit positions small.
many simultaneous errors, error-correcting code able correct
(Peterson & Weldon, 1972).
errors columns j also highly correlated bits columns
complementary. algorithms C4.5 backpropagation treat
class complement symmetrically. C4.5 construct identical decision trees
0-class 1-class interchanged. maximum Hamming distance two
columns attained columns complements. Hence, column separation
condition attempts ensure columns neither identical complementary.


j

269

fiDietterich & Bakiri

Table 5: possible columns three-class problem. Note last four columns
complements first four first column discriminate
among classes.
Class
c0
c1
c2

f0

0
0
0

f1

0
0
1

f2

0
1
0

Code Word
f3

f4

0
1
1

1
0
0

f5

1
0
1

f6

1
1
0

f7

1
1
1

Unless number classes least five, dicult satisfy properties. example, number classes three, 23 = 8 possible
columns (see Table 5). these, half complements half. leaves us
four possible columns. One either zeroes ones,
make useless discriminating among rows. result left
three possible columns, exactly one-per-class encoding provides.
general, k classes, 2 ,1 , 1 usable columns
removing complements all-zeros all-ones column. four classes, get
seven-column code minimum inter-row Hamming distance 4. five classes, get
15-column code, on.
employed four methods constructing good error-correcting output codes
paper: (a) exhaustive technique, (b) method selects columns
exhaustive code, (c) method based randomized hill-climbing algorithm, (d) BCH
codes. choice method use based number classes, k. Finding
single method suitable values k open research problem. describe
four methods turn.
k

2.3.1 Exhaustive Codes

3 k 7, construct code length 2 ,1 , 1 follows. Row 1 ones. Row 2
consists 2 ,2 zeroes followed 2 ,2 , 1 ones. Row 3 consists 2 ,3 zeroes, followed
2 ,3 ones, followed 2 ,3 zeroes, followed 2 ,3 , 1 ones. row i, alternating
runs 2 , zeroes ones. Table 6 shows exhaustive code five-class problem.
code inter-row Hamming distance 8; columns identical complementary.
k

k

k

k

k

k

k

k



2.3.2 Column Selection Exhaustive Codes

8 k 11, construct exhaustive code select good subset
columns. formulate propositional satisfiability problem apply
GSAT algorithm (Selman, Levesque, & Mitchell, 1992) attempt solution. solution
required include exactly L columns (the desired length code) ensuring
Hamming distance every two columns L , d,
chosen value d. column represented boolean variable. pairwise mutual
270

fiError-Correcting Output Codes

Row

1
1
0
0
0
0

1
2
3
4
5

2
1
0
0
0
1

Table 6: Exhaustive code k=5.
Column
3 4 5 6 7 8 9 10 11 12
1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 1 1 1 1
0 0 1 1 1 1 0 0 0 0
1 1 0 0 1 1 0 0 1 1
0 1 0 1 0 1 0 1 0 1

13
1
1
1
0
0

14
1
1
1
0
1

1

0

1

0

1

0

0

1

15
1
1
1
1
0

Figure 1: Hill-climbing algorithm improving row column separation. two closest
rows columns indicated lines. lines intersect, bits
code words changed improve separations shown right.
exclusion constraint placed two columns violate column separation
condition. support constraints, extended GSAT support mutual exclusion
\m-of-n" constraints eciently.
2.3.3 Randomized Hill Climbing

k > 11, employed random search algorithm begins drawing k random
strings desired length L. pair random strings separated
Hamming distance binomially distributed mean L=2. Hence, randomly
generated codes generally quite good average. improve them, algorithm
repeatedly finds pair rows closest together Hamming distance pair
columns \most extreme" Hamming distance (i.e., either close
far apart). algorithm computes four codeword bits rows
columns intersect changes improve row column separations shown
Figure 1. hill climbing procedure reaches local maximum, algorithm
randomly chooses pairs rows columns tries improve separations.
combined hill-climbing/random-choice procedure able improve minimum Hamming
distance separation quite substantially.
271

fiDietterich & Bakiri
2.3.4 BCH Codes

k > 11 also applied BCH algorithm design codes (Bose & Ray-Chaudhuri,
1960; Hocquenghem, 1959). BCH algorithm employs algebraic methods Galois
field theory design nearly optimal error-correcting codes. However, three practical drawbacks using algorithm. First, published tables primitive polynomials
required algorithm produce codes length 64, since largest word
size employed computer memories. Second, codes always exhibit good column
separations. Third, number rows codes always power two. number classes k learning problem power two, must shorten code
deleting rows (and possible columns) maintaining good row column separations.
experimented various heuristic greedy algorithms code shortening.
codes used NETtalk, ISOLET, Letter Recognition domains,
used combination simple greedy algorithms manual intervention design good
shortened BCH codes.
data sets studied, designed series error-correcting codes
increasing lengths. executed learning algorithm codes.
stopped lengthening codes performance appeared leveling off.

2.4 Making Classification Decisions
approach solving multiclass problems|direct multiclass, one-per-class, errorcorrecting output coding|assumes method classifying new examples. C4.5
direct multiclass approach, C4.5 system computes class probability estimate
new example. estimates probability example belongs
k classes. C4.5 chooses class highest probability class
example.
one-per-class approach, decision tree neural network output unit
viewed computing probability new example belongs corresponding
class. class whose decision tree output unit gives highest probability estimate
chosen predicted class. Ties broken arbitrarily favor class comes
first class ordering.
error-correcting output code approach, decision tree neural network
output unit viewed computing probability corresponding bit
codeword one. Call probability values B = hb1; b2; : : : ; b i, n length
codewords error-correcting code. classify new example, compute L1
distance probability vector B codewords W (i = 1 : : :k)
error correcting code. L1 distance B W defined
n



L1 (B; Wi)

=

Xj



L

j

=0

bj

, W j:
i;j

class whose codeword smallest L1 distance B assigned class
new example. Ties broken arbitrarily favor class comes first class
ordering.
272

fiPerformance relative Multiclass


yb
ea
n

ud
io
lo
gy


LE

Le
tte
r
N
ET
ta
lk

PO



el
ow
V

G

la

ss

Error-Correcting Output Codes

*

10

*

*
*

*

*

0

C4.5 Multiclass
*
*

-10

*

-20
-30

*

C4.5 one-per-class

C4.5 ECOC

Figure 2: Performance (in percentage points) one-per-class ECOC methods relative direct multiclass method using C4.5. Asterisk indicates difference
significant 0.05 level better.

3. Results
present results experiments. begin results decision trees.
Then, consider neural networks. Finally, report results series experiments
assess robustness error-correcting output code method.

3.1 Decision Trees
Figure 2 shows performance C4.5 eight domains. horizontal line corresponds
performance standard multiclass decision-tree algorithm. light bar shows
performance one-per-class approach, dark bar shows performance
ECOC approach longest error-correcting code tested. Performance displayed
number percentage points pair algorithms differ. asterisk
indicates difference statistically significant p < 0:05 level according
test difference two proportions (using normal approximation binomial
distribution, see Snedecor & Cochran, 1989, p. 124).
figure, see one-per-class method performs significantly worse
multiclass method four eight domains behavior statistically
indistinguishable remaining four domains. Much encouraging observation
error-correcting output code approach significantly superior multiclass
approach six eight domains indistinguishable remaining two.
273

fiDietterich & Bakiri
NETtalk domain, also consider performance meaningful distributed representation developed Sejnowski Rosenberg. representation gave
66.7% correct classification compared 68.6% one-per-class configuration,
70.0% direct-multiclass configuration, 74.3% ECOC configuration.
differences figures statistically significant 0.05 level better
except one-per-class direct-multiclass configurations statistically distinguishable.

3.2 Backpropagation

Figure 3 shows results backpropagation five challenging domains.
horizontal line corresponds performance one-per-class encoding
method. bars show number percentage points error-correcting
output coding representation outperforms one-per-class representation. four
five domains, ECOC encoding superior; differences statistically significant
Vowel, NETtalk, ISOLET domains.2
letter recognition domain, encountered great diculty successfully training
networks using CNAPS machine, particularly ECOC configuration. Experiments
showed problem arose fact CNAPS implementation backpropagation employs fixed learning rate. therefore switched much slower opt
program, chooses learning rate adaptively via conjugate-gradient line searches.
behaved better one-per-class ECOC configurations.
also diculty training ISOLET ECOC configuration large networks (182 units), even opt program. sets initial random weights led
local minima poor performance validation set.
NETtalk task, compare performance Sejnowski-Rosenberg
distributed encoding one-per-class ECOC encodings. distributed encoding
yielded performance 71.5% correct, compared 72.9% one-per-class encoding,
74.9% ECOC encoding. difference distributed encoding
one-per-class encoding statistically significant. results previous
results C4.5, conclude distributed encoding advantages
one-per-class ECOC encoding domain.

3.3 Robustness

results show ECOC approach performs well as, often better than,
alternative approaches. However, several important questions must
answered recommend ECOC approach without reservation:

results hold small samples? found decision trees learned using

error-correcting codes much larger learned using one-per-class
multiclass approaches. suggests small sample sizes, ECOC method
may perform well, since complex trees usually require data learned
reliably. hand, experiments described covered wide range

2. difference ISOLET detectable using test paired differences proportions. See
Snedecor & Cochran (1989, p. 122.).

274

fiPerformance relative one-per-class

10

lk
ET
ta
N

te
Le





r

LE


el
ow
V

G

la

ss

Error-Correcting Output Codes

*

5
*
*

0

Backprop one-per-class

Backprop ECOC

Figure 3: Performance ECOC method relative one-per-class using backpropagation. Asterisk indicates difference significant 0.05 level better.
training set sizes, suggests results may depend large
training set.

results depend particular assignment codewords classes?

codewords assigned classes arbitrarily experiments reported above,
suggests particular assignment may important. However,
assignments might still much better others.

results depend whether pruning techniques applied decision-

tree algorithms? Pruning methods shown improve performance
multiclass C4.5 many domains.

ECOC approach provide class probability estimates? C4.5 back-

propagation configured provide estimates probability test
example belongs k possible classes. ECOC approach
well?

3.3.1 Small sample performance

noted, became concerned small sample performance ECOC
method noticed ECOC method always requires much larger decision trees
OPC method. Table 7 compares sizes decision trees learned C4.5
multiclass, one-per-class, ECOC configurations letter recognition task
NETtalk task. OPC ECOC configurations, tables show average
number leaves trees learned bit position output representation.
275

fiDietterich & Bakiri

Table 7: Size decision trees learned C4.5 letter recognition task
NETtalk task.
Letter Recognition Leaves per bit Total leaves
Multiclass
2353
One-per-class
242
6292
207-bit ECOC
1606
332383
NETtalk
Multiclass
One-per-Class
159-bit ECOC

Leaves per bit
phoneme stress
61
901

600
911

Total leaves
phoneme stress
1425 1567
3320 3602
114469 29140

letter recognition, trees learned 207-bit ECOC six times larger
learned one-per-class representation. phoneme classification part
NETtalk, ECOC trees 14 times larger OPC trees. Another way
compare sizes trees consider total number leaves trees.
tables clearly show multiclass approach requires much less memory (many fewer
total leaves) either OPC ECOC approaches.
backpropagation, dicult determine amount \network resources" consumed training network. One approach compare
number hidden units give best generalization performance. ISOLET task,
example, one-per-class encoding attains peak validation set performance 78hidden-unit network, whereas 30-bit error-correcting encoding attained peak validation
set performance 156-hidden-unit network. letter recognition task, peak performance one-per-class encoding obtained network 120-hidden units
compared 200 hidden units 62-bit error-correcting output code.
decision tree neural network sizes, see that, general, errorcorrecting output representation requires complex hypotheses one-per-class
representation. learning theory statistics, known complex hypotheses
typically require training data simple ones. basis, one might expect
performance ECOC method would poor small training sets. test
prediction, measured performance function training set size two
larger domains: NETtalk letter recognition.
Figure 4 presents learning curves C4.5 NETtalk letter recognition tasks,
show accuracy series progressively larger training sets. figure
clear 61-bit error-correcting code consistently outperforms two configurations nearly constant margin. Figure 5 shows corresponding results backpropagation NETtalk letter recognition tasks. NETtalk task, results
same: sample size apparent uence benefits error-correcting output coding. However, letter-recognition task, appears interaction.
276

fiError-Correcting Output Codes

NETtalk

Letter Recognition

75

100
C4 61-bit ECOC

Percent Correct

70

C4 Multiclass
C4 One-per-class

90
C4 Multiclass

65

80

60

70

55

60

50

50

45

40

40

30

35
100

C4 62-bit ECOC

C4 One-per-class

20
100

1000

1000

Training Set Size

10000
Training Set Size

Figure 4: Accuracy C4.5 multiclass, one-per-class, error-correcting output
coding configurations increasing training set sizes NETtalk letter
recognition tasks. Note horizontal axis plotted logarithmic scale.
NETtalk

Letter Recognition
100

75
90
70

CNAPS 61-bit ECOC

Percent Correct

80
65

opt OPC
opt 62-bit ECOC

70

CNAPS One-per-class

60

60

55

50

50

45
100

1000
Training Set Size (words)

40
100

1000
Training Set Size

10000

Figure 5: Accuracy backpropagation one-per-class error-correcting output
coding configurations increasing training set sizes NETtalk letter
recognition tasks.
Error-correcting output coding works best small training sets, statistically significant benefit. largest training set|16,000 examples|the one-per-class
method slightly outperforms ECOC method.
experiments, conclude error-correcting output coding works
well small samples, despite increased size decision trees increased
complexity training neural networks. Indeed, backpropagation letter recognition task, error-correcting output coding worked better small samples
277

fiDietterich & Bakiri

Table 8: Five random assignments codewords classes NETtalk task.
column shows percentage letters correctly classified C4.5 decision trees.
Multiclass One-per-class
70.0
68.6

61-Bit Error-Correcting Code Replications

b
c

e
73.8 73.6 73.5 73.8
73.3

large ones. effect suggests ECOC works reducing variance learning
algorithm. small samples, variance higher, ECOC provide benefit.
3.3.2 Assignment Codewords Classes

results reported thus far, codewords error-correcting code
arbitrarily assigned classes learning task. conducted series experiments
NETtalk domain C4.5 determine whether randomly reassigning codewords
classes effect success ECOC. Table 8 shows results five
random assignments codewords classes. statistically significant variation
performance different random assignments. consistent similar
experiments reported Bakiri (1991).
3.3.3 Effect Tree Pruning

Pruning decision trees important technique preventing overfitting. However,
merit pruning varies one domain another. Figure 6 shows change performance due pruning eight domains three configurations
studied paper: multiclass, one-per-class, error-correcting output coding.
figure, see cases pruning makes statistically significant
difference performance (aside POS task, decreases performance
three configurations). Aside POS, one statistically significant changes
involves ECOC configuration, two affect one-per-class configuration, one
affects multiclass configuration. data suggest pruning occasionally
major effect configurations. evidence suggest pruning
affects one configuration another.
3.3.4 Class Probability Estimates

many applications, important classifier cannot classify new cases
well also estimate probability new case belongs k classes.
example, medical diagnosis, simple classifier might classify patient \healthy"
because, given input features, likely class. However,
non-zero probability patient life-threatening disease, right choice
physician may still prescribe therapy disease.
mundane example involves automated reading handwritten postal codes
envelopes. classifier confident classification (i.e., estimated
278

fiPerformance relative pruning

lk
ET
N

Le

tte

r

ta


LE

lo
gy




ud

io








PO



el
ow
V

G

la

ss

Error-Correcting Output Codes

10
*

*
*

5

0
-2

Pruning
**

*
*

C4.5 Multiclass

C4.5 one-per-class

C4.5 ECOC

Figure 6: Change percentage points performance C4.5 without pruning
three configurations. Horizontal line indicates performance pruning.
Asterisk indicates difference significant 0.05 level better.

279

fiDietterich & Bakiri
probabilities strong), proceed route envelope. However,
uncertain, envelope \rejected", sent human
attempt read postal code process envelope (Wilkinson, Geist, Janet, et al.,
1992).
One way assess quality class probability estimates classifier
compute \rejection curve". learning algorithm classifies new case, require
also output \confidence" level. plot curve showing percentage
correctly classified test cases whose confidence level exceeds given value. rejection curve
increases smoothly demonstrates confidence level produced algorithm
transformed accurate probability measure.
one-per-class neural networks, many researchers found difference
activity class highest activity class second-highest
activity good measure confidence (e.g., LeCun et al., 1989). difference large,
chosen class clearly much better others. difference small,
chosen class nearly tied another class. measure applied
class probability estimates produced C4.5.
analogous measure confidence error-correcting output codes computed
L1 distance vector B output probabilities bit
codewords classes. Specifically, employ difference L1
distance second-nearest codeword L1 distance nearest codeword
confidence measure. difference large, algorithm quite confident
classification decision. difference small, algorithm confident.
Figure 7 compares rejection curves various configurations C4.5 backpropagation NETtalk task. curves constructed first running test
examples learned decision trees computing predicted class example confidence value prediction. generate point along curve,
value chosen parameter , defines minimum required confidence.
classified test examples processed determine percentage test examples
whose confidence level less (these \rejected") percentage remaining examples correctly classified. value progressively incremented
(starting 0) test examples rejected.
lower left portion curve shows performance algorithm
small, least confident cases rejected. upper right portion curve
shows performance large, confident cases classified.
Good class probability estimates produce curve rises smoothly monotonically.
decreasing region rejection curve reveals cases confidence estimate
learning algorithm unrelated inversely related actual performance
algorithm.
rejection curves often terminate prior rejecting 100% examples. occurs
final increment causes examples rejected. gives idea
number examples algorithm highly confident classifications.
curve terminates early, shows examples algorithm
could confidently classify.
Figure 7, see that|with exception Multiclass configuration|the rejection curves various configurations C4.5 increase fairly smoothly,
280

fiError-Correcting Output Codes
C4.5

Backpropagation

100

100
61-bit ECOC

61-bit ECOC

159-bit ECOC

95

95
OPC

OPC

159-bit ECOC

Percent Correct

90
90

Multiclass

Distributed

Distributed

85
85
80
80
75
75

70

65

70
0

10

20

30

40
50
60
Percent Rejected

70

80

90

100

0

10

20

30

40
50
60
Percent Rejected

70

80

Figure 7: Rejection curves various configurations C4.5 backpropagation
NETtalk task. \Distributed" curve plots behavior SejnowskiRosenberg distributed representation.
producing acceptable confidence estimates. two error-correcting configurations smooth curves remain configurations. shows
performance advantage error-correcting output coding maintained confidence
levels|ECOC improves classification decisions examples, borderline ones.
Similar behavior seen rejection curves backpropagation. configurations backpropagation give fairly smooth rejection curves. However, note
159-bit code actually decreases high rejection rates. contrast, 61-bit code gives
monotonic curve eventually reaches 100%. seen behavior several
cases studied: extremely long error-correcting codes usually best method
low rejection rates, high rejection rates, codes \intermediate" length (typically
60-80 bits) behave better. explanation behavior.
Figure 8 compares rejection curves various configurations C4.5 backpropagation ISOLET task. see ECOC approach markedly superior
either one-per-class multiclass approaches. figure illustrates another phenomenon frequently observed: curve multiclass C4.5 becomes quite
terminates early, one-per-class curve eventually surpasses it. suggests
may opportunities improve class probability estimates produced C4.5
multiclass trees. (Note employed \softened thresholds" experiments.)
backpropagation rejection curves, ECOC approach consistently outperforms
one-per-class approach close 100% correct. Note configurations backpropagation confidently classify 50% test examples
100% accuracy.
graphs, clear error-correcting approach (with codes intermediate length) provide confidence estimates least good provided
standard approaches multiclass problems.
281

90

100

fiDietterich & Bakiri
C4.5

Backpropagation

100

101
107-bit ECOC

98
45-bit ECOC

100

96

30-bit ECOC

Multiclass
Percent Correct

94

99

92
90

98

One Per Class

88
97

86
84

96

One Per Class
82
80

95
0

10

20

30

40
50
60
Percent Rejected

70

80

90

100

0

5

10

15

20
25
Percent Rejected

30

35

Figure 8: Rejection curves various configurations C4.5 backpropagation
ISOLET task.

4. Conclusions

paper, experimentally compared four approaches multiclass learning problems:
multiclass decision trees, one-per-class (OPC) approach, meaningful distributed
output approach, error-correcting output coding (ECOC) approach. results
clearly show ECOC approach superior three approaches.
improvements provided ECOC approach quite substantial: improvements
order ten percentage points observed several domains. Statistically significant
improvements observed six eight domains decision trees three five
domains backpropagation.
improvements also robust:
ECOC improves decision trees neural networks;
ECOC provides improvements even small sample sizes;
improvements depend particular assignment codewords classes.
error-correcting approach also provide estimates confidence classification decisions least accurate provided existing methods.
additional costs employing error-correcting output codes. Decision
trees learned using ECOC generally much larger complex trees constructed using one-per-class multiclass approaches. Neural networks learned using
ECOC often require hidden units longer careful training obtain
improved performance (see Section 3.2). factors may argue using errorcorrecting output coding domains. example, domains important
humans understand interpret induced decision trees, ECOC methods
appropriate, produce complex trees. domains training must
rapid completely autonomous, ECOC methods backpropagation cannot
recommended, potential encountering diculties training.
282

40

45

fiError-Correcting Output Codes
Finally, found error-correcting codes intermediate length tend give better
confidence estimates long error-correcting codes, even though long codes
give best generalization performance.
many open problems require research. First foremost,
important obtain deeper understanding ECOC method works. assume
learned hypotheses makes classification errors independently, coding
theory provides explanation: individual errors corrected codewords
\far apart" output space. However, hypotheses learned
using algorithm training data, would expect errors made
individual hypotheses would highly correlated, errors cannot corrected
error-correcting code. key open problem understand classification
errors different bit positions fairly independent. error-correcting output
code result independence?
closely related open problem concerns relationship ECOC approach
various \ensemble", \committee", \boosting" methods (Perrone & Cooper, 1993;
Schapire, 1990; Freund, 1992). methods construct multiple hypotheses
\vote" determine classification example. error-correcting code also
viewed compact form voting certain number incorrect votes
corrected. interesting difference standard ensemble methods
ECOC approach ensemble methods, hypothesis attempting predict
function, whereas ECOC approach, hypothesis predicts different
function. may reduce correlations hypotheses make
effective \voters." Much work needed explore relationship.
Another open question concerns relationship ECOC approach
exible discriminant analysis technique Hastie, Tibshirani, Buja (In Press).
method first employs one-per-class approach (e.g., neural networks)
applies kind discriminant analysis outputs. discriminant analysis maps
outputs k , 1 dimensional space class defined \center point". New
cases classified mapping space finding nearest \center
point" class. center points similar codewords continuous
space dimension k , 1. may ECOC method kind randomized,
higher-dimensional variant approach.
Finally, ECOC approach shows promise scaling neural networks large
classification problems (with hundreds thousands classes) much better oneper-class method. good error-correcting code length n
much less total number classes, whereas one-per-class approach requires
one output unit class. Networks thousands output units would
expensive dicult train. Future studies test scaling ability
different approaches large classification tasks.

Acknowledgements
authors thank anonymous reviewers valuable suggestions improved
presentation paper. authors also thank Prasad Tadepalli proof-reading
283

fiDietterich & Bakiri
final manuscript. authors gratefully acknowledge support National Science
Foundation grants numbered IRI-8667316, CDA-9216172, IRI-9204129. Bakiri
also thanks Bahrain University support doctoral research.

References

Adaptive Solutions (1992). CNAPS back-propagation guide. Tech. rep. 801-20030-04, Adaptive Solutions, Inc., Beaverton, OR.
Bakiri, G. (1991). Converting English text speech: machine learning approach. Tech.
rep. 91-30-2, Department Computer Science, Oregon State University, Corvallis,
OR.
Barnard, E., & Cole, R. A. (1989). neural-net training program based conjugategradient optimization. Tech. rep. CSE 89-014, Oregon Graduate Institute, Beaverton,
OR.
Bose, R. C., & Ray-Chaudhuri, D. K. (1960). class error-correcting binary group
codes. Information Control, 3, 68{79.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification
Regression Trees. Wadsworth International Group.
Bridle, J. S. (1990). Training stochastic model recognition algorithms networks
lead maximum mutual information estimation parameters. Touretzky, D. S.
(Ed.), Neural Information Processing Systems, Vol. 2, pp. 211{217 San Francisco, CA.
Morgan Kaufmann.
Cardie, C. (1993). Using decision trees improve case-based learning. Proceedings
Tenth International Conference Machine Learning, pp. 17{24 San Francisco,
CA. Morgan Kaufmann.
Duda, R. O., Machanik, J. W., & Singleton, R. C. (1963). Function modeling experiments.
Tech. rep. 3605, Stanford Research Institute.
Freund, Y. (1992). improved boosting algorithm implications learning complexity. Proc. 5th Annu. Workshop Comput. Learning Theory, pp. 391{398.
ACM Press, New York, NY.
Hampshire II, J. B., & Waibel, A. H. (1990). novel objective function improved
phoneme recognition using time-delay neural networks. IEEE Transactions Neural
Networks, 1 (2), 216{228.
Hastie, T., Tibshirani, R., & Buja, A. (In Press). Flexible discriminant analysis optimal
scoring. Journal American Statistical Association.
Hinton, G. (1989). Connectionist learning procedures. Artificial Intelligence, 40, 185{234.
Hocquenghem, A. (1959). Codes corecteurs d'erreurs. Chiffres, 2, 147{156.
284

fiError-Correcting Output Codes
Kong, E. B., & Dietterich, T. G. (1995). error-correcting output coding works
decision trees. Tech. rep., Department Computer Science, Oregon State University,
Corvallis, OR.
Lang, K. J., Hinton, G. E., & Waibel, A. (1990). time-delay neural network architecture
isolated word recognition. Neural Networks, 3 (1), 23{43.
LeCun, Y., Boser, B., Denker, J. S., Henderson, B., Howard, R. E., Hubbard, W., & Jackel,
L. D. (1989). Backpropagation applied handwritten zip code recognition. Neural
Computation, 1 (4), 541{551.
Murphy, P., & Aha, D. (1994). UCI repository machine learning databases [machinereadable data repository]. Tech. rep., University California, Irvine.
Natarajan, B. K. (1991). Machine Learning: Theoretical Approach. Morgan Kaufmann,
San Mateo, CA.
Nilsson, N. J. (1965). Learning Machines. McGraw-Hill, New York.
Perrone, M. P., & Cooper, L. N. (1993). networks disagree: Ensemble methods
hybrid neural networks. Mammone, R. J. (Ed.), Neural networks speech
image processing. Chapman Hall.
Peterson, W. W., & Weldon, Jr., E. J. (1972). Error-Correcting Codes. MIT Press, Cambridge, MA.
Quinlan, J. R. (1993). C4.5: Programs Empirical Learning. Morgan Kaufmann, San
Francisco, CA.
Richard, M. D., & Lippmann, R. P. (1991). Neural network classifiers estimate bayesian
posteriori probabilities. Neural Computation, 3 (4), 461{483.
Rosenblatt, F. (1958). perceptron: probabilistic model information storage
organization brain. Psychological Review, 65 (6), 386{408.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations error propagation. Parallel Distributed Processing { Explorations
Microstructure Cognition, chap. 8, pp. 318{362. MIT Press.
Schapire, R. E. (1990). strength weak learnability. Machine Learning, 5 (2), 197{227.
Sejnowski, T. J., & Rosenberg, C. R. (1987). Parallel networks learn pronounce
english text. Journal Complex Systems, 1 (1), 145{168.
Selman, B., Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiability
problems. Proceedings AAAI-92, pp. 440{446. AAAI/MIT Press.
Snedecor, G. W., & Cochran, W. G. (1989). Statistical Methods. Iowa State University
Press, Ames, IA. Eighth Edition.
Valiant, L. G. (1984). theory learnable. Commun. ACM, 27 (11), 1134{1142.
285

fiDietterich & Bakiri
Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition using time-delay networks. IEEE Transactions Acoustics, Speech, Signal
Processing, 37 (3), 328{339.
Weigend, A. (1993). Measuring effective number dimensions backpropagation
training. Proceedings 1993 Connectionist Models Summer School, pp. 335{
342. Morgan Kaufmann, San Francisco, CA.
Wilkinson, R. A., Geist, J., Janet, S., et al. (1992). first census optical character recognition systems conference. Tech. rep. NISTIR 4912, National Institute Standards
Technology.

286

fi
ff fi


!"$#%$&('*),+--.0/!+++21+3'-

435678:9;<-.=>5
#& 8?-0;-.

@BADCFE0GHAIAIJALKNMPOIJE0QSRTQUGWVXAIJALK

Y?Z:[]\_^a`UY_bac2deb
w:xy{z}|e~t}z~yH3o~
323F ,<<~
3| 03yW" 3

f3gahjiag_kmlafon paqarsn gtlun s,v

Z^{^"?^?Z_`,

("f3pa _kWs,_n a}l!paas%:n gtlun s,v

!|e~a?o}o}} y3o~
%x
0 je 2e

e _!
7 !!uo o!:*%3:e7 u_, 7 X0$ U7 $ 3j__ 3
ej!e$ UI3joe 3j?7 eU}3}oe77 },o 7
$m$2 <2u 7ooeL $U($ }7%ee
} a,$:_, U7 am$j$0oU ,3!$o0I} 3}0
$0$ } WeX*:3 t< $ 37 ," e% 7$ W,$
o$7 }oo$2 <2e
: }7e7"! 7ooem $ $0a} $}oe_}
u $t$ % $00< }}$"}:3$0U aW(<Loe $ ,L 0
uo 7{!
aa"! "" a0, ff
Ifi, !}fia3 !"a $#%&
'a(a)*# + $,*-7

!.!0/ ff
21}a, fi# 3*a

"!4fi,

$5%


a6ff7, "98ff,:#!,;$# 50/%<7, 2U 1a "!0>=5, ?(+/"+/ 0@! 4AB!o5ff7W
a<a a"%,$#oUa$1!aC28ff,:#!,;a
DoIa!}fi! a3W oE, <F=5,G-7

# &C% fi; e $a3 fiHfi,$#a $#ff2Iff0" $ aa a"%,$#}
I!Jfi3!# &CK,
M"

/ N

fi9% 6 "B0O+

P ,CQ $#R
# a5CX+7,$#!SETUV%WX!Y[Z \
L
7,$#3,.o*WX^]_a`7bffX!cAWX^Y>%3ff,$#AffBfi+/MA#/ R# &0I,T UffB ",fi,7
IfffiaM#&
%d R
B e
5 ff, L
fJ
# &O+
"
fi,!
<
R}

!Bgah !a3ff&fi;B!+\

A,
h"ff7, 2 A"
35
1 $
3 a!7, G0 fi% 7 JX
753 +,Gm
fi/ff, $I

&Q
5
1 (3 6!0
/ Wff

9iBW+jfiUff]_abff`}
!+
H fi, $#!J 7+
/ L5
"ff7, A5J
1 A}
3a

# fia

$H
7 MB! O"ffBa
, Zk% "#
/ l
+
7
"}34fi "
+!0/ff, Oa
fi/% 3 $I

fi
fi,] fiBl% 9a
n% !0O"+!0/%,
"B e fi/ff, $

&, ]
e
*!PZp!

!o
;C!,fi
D7, Ra
""
+!0/ff, @a
&/%, $I

fi,GZDq
# &C!%E#
/

$"
<+(
J@a

75J
+,U
r ffBfi, $#no
! sBa
753 E%
"4% ! *I
!+
3+,
ZRq 3
# &0%+{
a0ff{
affI
Bo
7,tfffi, +/fi,ffu"
De


+
2!3
a"!4fi,"+!0/%0Oafi/ff, $
&BaI!} aff fiMva!Im%7, ""!
U fi%w,
xZy l
!+ F,{
z 3C
/ ?
+
/ '6 I+
/ 3]"}34fi]B e2"
+!0/%, J6a

fi/ff, $

&"a
fi 0 6
v a! mff7, "!,
a"Z| 0
# &0IN<"
De
77O"Aa

+
!JZ}+o
aaffOffBfi, +
/ E<
# fi0I,s
~ ? N

fiffff
"4% !mo
&/%, $I

fiA7(
"}g<fi,
"+!0/%, 0M +
"l ' ,AI3
De
"
Ra
+
!Pa
Ifi'ff7, !",
a3Z R0 9
# &0
+--3.:fiafi"$##$ 8: 8EU3O_ff6O5
#& #I fi

o%&# #>8


fiff$$*$$xO%$$ff$!$!

$
+$M+fiff%$+&><6""fiff$"D5">ff fi$M%&0$fff%fi+CJfi<3
$P+fiff%$+&"!+!0ffff+ff5%$R6
+ff+B$<B++%+PM$O+%7Mfi;BMR^&$fiffE$$+&%ff$+fi[;[$ff@%fi+ff
6R27ff +7!3!&B4fi<Hff|D%fi+ff9< fffi0Iff$$!&(B! M!J$ffM$MJ!ff
!fi.7*!J(B$+2&7tff7fftRB![M5ff+'&'Ds<%$3t$+ff2!A$
+fiffff$ +&R!+!0ffRff+ff5%$' Bl75J2 fiRff!7+fit%++fi %$
75J6B! 7fi5m%B$MMff<%*g0O5%+D $+$P+fiffff$ +& ^+!0ff ff+ff%<$
"A4ff$%7ffG7$$!&!P$+B&7Pff7ffl!^'!ffl+ff++%+57P
5+GI fiO+ff7ff%+0 0hffBfi+ff5dff&0IfffiP>$M$>$Peff^!57E5ff
!!^ff+ff5%$R$Pfi<<$275JMfiBfi($+$+6B$P+&%ff$+fi6^+!0ff ff+ff%<$
ffOff+%5%$(BM60$+fi5ff ff7ffRMBPff+O!ffBAP$fi70@75Jff@[!0ff!$fi
$M$fi+fiD7%$+!MJdff<%!E% $+6B$M ff$A6"+ff7ff
4ff+$+5CK%}D0M^+7>ff|7ff 6ff7ff!G!Jfi!6B$ &$A^+O!&g<fi"!+!0%<%5
$+fiff%$+&E>I:ffF!ff7E$Ofi$fiff![t!ff%P$O+fiff%$+&F5fi 0ff%$
ff7ff!9!PD0RffwRI0<$BA{fiJm$D7!&B4fi<R!+!0%ff5PJm$+fi%<Bff$+fi
ff+ff%<$'*DMfi$fiffffD0l+ff+fi!0'+$ff$ffJ!ff$B$2!+!0ffffRP$
+fiffff$ +&"DB$&fi7M$A$fi+fiDff7ff!ff$D7!+B+ fi"$$>+
D% ff!^fiG&aMff!a&ff
Qfi;E!^+6%!7Q$"+ff3sG$fi$R fiMfi$!$fiAff!+&B57$Aff
$>7ff0>&+a>AJO!!%$2;gKffBfffi>I:ff ffBfi+ffDB$ff&0+&J5%P
fiBD^&4fiP!+!0%<%O$ +fi%<Bff$+fiE$6ff!C!ff$5ff%mB$%$(B!OPff+ fi;J
6B74Iffffff$3$hP77ff*fi!$PA!ff4">5 +m%D$+(s;J$fi<$2fiBfi<$
+!(!% Iwff7fflA(%!$Aff$$5<%!0sff7^5mM!fiR.Bfi;!^+
!7fifi %;7$fffi72!+7++$+D07&^+$7+fi5<%&!A^+7m++ $+
D0*ff$$5<%!0s++%ff7+4ff<%! $AMB;%4s$fi$2 fi<Jfi$2!ff7
ffB$7f%$$57ff!C++%ff7O!H+$7+fi57ffR7&^+GIff7ff2P6%! +ff+50
$+!4ff!PBDff$P!%43+ff"I'ff7ffxA$$Pff$">5+ff2!*B7fi5l$
!+7!7ffARffBe
@RO'2C4%e$D%
ff
!g;fi+R$$xNJfiR fi!0ff2ff<ff$n!fiff}$2!7D!fi;

5%+x>ffw%+0(fiffDC< %!4fi+nw5$RI+*7h+ $+$+lDfi
ff!7+BC7ff!RB!n+J^*Dfffi%w!7+fi27ff}P7ff!R5Jff+tfit$*fihBff2D0
B!$ffR$7ff!+C<%!+.NffM!fiffB$DfihBffMD0*!2fi2ff!7+ff $+$+A$
!7+fi[fi&$3$7$E[fi<%(%M!3 $+$+G$O!<+fi!ff[fiffAff+ !+5B$ff
ff7 ff$"ff!7+BC7ff!dDfffifffi$6fi$ffED0A$+35ff"Dff7fffi fiBDR3R$
ffRA$+ff!7+7ff!+
%7RffR$7P$+ff!7+7ff!+N$ffi$ffD0$+RBHB$fi&7
27$!7%$+fiMff7ff@J!.2ffM$+3M 7P!&B4fi<
g$<9!P$A!<+fiPAfffi
7fi+%$$$P$3Nff"!$B(Js$ffB!0
g$B2"$ff"&$ *$7ff7I J$50ff$
5"$Dff7ffw5ff+*fi$Dfi$ffPRfi+^+!'ff*$DffB!0@
7$l$A!7+fi
$6fi$ffE$+$EMfi+ff7M"fiR!Ee5% $ !<+fi>ff fidfi70@ff!7+BCfiP75
M9ff&0!75BP>I:ffff
$&$4+0D75Jfi<P75J
fi5$"fih%m+%!7+ff B$ !7+fimffB

ff
fi "!$#$#&%'()**()'( '+ fi,-.0/!21'31141567!2()8941*!2:fi0;<(6#$#2(.!2,=
1'>7!2( @?A6/"!$#$#B%C!2/,7!$DE/+.!2fiF4( 7!G#H!2fi,I'3#ff41567!2()J17!2K>
LL)M

fiNOQPSRTHOHOHUVOffWYXZHU<R[8\ff[@TH]HOHUVOffW

^ff_a`cbedf`<b d`<g h@iejbffkmlchjanbJoj@p qCjrff_@sHdCd6tHhujvp dsffj,le`cbwsHnxzy`5{$h@{<idtHhj@p dsffjElffhbA|v`<n_@bHg hbedCrffhtffj,|v`c_vn}
kesHn`<bH~:dtHhh Hhp sHd`<_@b_oIdtHhJ^el9jb {+hbffp h@i.hJ~@hdj bffjd6sHn)j,lIq`<dsffjd`<_@bYtHhnh &lVjabHbe`cbH~:te`Gl<h
hjn6be`cbH~z`9qbHhp hq6qjnx@{
sHnqhp _@bffkh jgz^elch`Vqd j@hbon6_@gjQd6n)jbffq^_@nd)jd`c_vbke_@g-jE`cb { hdy*)z}rffhj
kv`<nhp dhkQ~@n j^Ht iCtHhnh:d6tHh-|@hn6d`9p hq kehbH_@dhzl<_Bpjd`c_vbffqJ`<bjtH_eqd`l<hhbe|@`<n_@bHgzhbAd{utHh:hke~@hq
kehbH_@dh:qj,ohn_@sHdhqmon_@g_vbHhlc_Hpjd`<_@bd6_jbH_@d6tHhniCdtffjdjnhd)j@hbQd_8rffh:_@bHhj,xQy5d_vuj,x
n_@sHd6hqIjnh0kehqp n`<rffhk8j@qIjm^ffj,`<n_ao"_vbHhuj,xJn_@sHdhq } { bHhj,xn_@sHdhq&`<b:dte`9qC^ffjnd`Vp sel9jn0ke_@g-j,`<b
_Hpp sHnj@qj nhqselcd0_oCdtHhqdnsffp dsHnh_ao&dtHhJhbA|v`<n_@bHg hbedjbffk:_oCdtHhJ|@hte`9p6lchsffqhk8rex-d6tHhj~@hbed{
bffp hnd)j,`<bed>xQ`<bdte`9qzdn)jbffq^_@nd)jad`<_@bke_@g-j,`<bjn`9qhqon_@gdtHhojvp d-dtffjad dtHhnh`Vq`<bffp _@g ^el<hdh
`<beo_vng-jd`<_@bjarff_@sHd0dtHhhbffke^ff_a`cbed_oIq_@g hn6_@sHdhq*_@n`c~a`cbffjad`<bH~ on_@gq_@gzh^ffjnd`9p sel9jnlc_Hpjd`<_@bffq
y`5{$h@{<idtHh g-ja^Y`9q^ffjnd`VjElGl<xsHbHebH_b.}){z5bq_@g h pj@qhqmdte`9qm`cbffp _vg ^el<hdh `<beo_@ng-jad`<_@bp _@bffp hnbffq
_@bel<xjqg-jElGlCbesHgrhn_aoClc_Hpjd`<_@bffqjabffk:n_@sHd6hqtHhn6h+dtHhbesHgrhn_aoI^ff_Aqq>`crel<h hbffke^_`<bAd q0_oj
n_@sHd6h*`9qIj,l9q_qg-j,ll5{20b-j~@hbedIg _|v`<bH~j,l<_@bH~JdtHhqhn_vsHdhqCebH_qfdtHh0^ff_Aqq>`crel<hj,l<dhnbffjd`c|vhqCo_@n
dtHh+qd6nsffp dsHnh0_oSdtHhhbe|v`cn6_@bHg hbedjbffkpjb`9kehbAd`ox:dtHh0l<_Bpjad`<_@bffq`<djnn`c|vhqjad{C*tHh_@rHwhp d`<|@h
_oCdtHh+j~@hbed*`9q*d_znhj@p6tj~`<|@hb:d)jn6~@hdlc_Hpjd`<_@bYqd)jn6d`<bH~on6_@gj~`<|@hb:`<be`<d`9j,lfl<_Bpjd`c_vb {
utHhjarff_|@hh jag ^el<hq+jnhd)ja@hb8on_vgnhjElc7l`ohq`<dsffjd`<_@bffq{*tHhxjnhdxe^e`VpjElq`<dsffjd`<_@bffq0_o
rff_vsHbffkehksHbffp hnd)j,`<bed>x@{u`cg`Gl9jn0q`<dsffjd`<_@bffq_Hpp sHntHhbHh|@hn*h0tffj,|@hd_J_@^ffhn jdhjg-j@p6te`<bHhdtffjd
*_@nHq*`<bQ_@bHh_ouqh|@hn)j,lI_@^Hd`c_vbffq{5bgjbAx8_od6tH_Aqh pj@qhqidtHh ^_Aqq`<rel<h-_vrffqhn|Ejad`<_@bffq+pjbYrh
qd)jadhk.iCjbffkYdtHh:qhd_o^ff_Aq6q`<relch:hbe|v`cn6_@bHg hbedrhtffj,|@`<_@n)qzpjbrffhl`9qdhk.dtHh:j@p dsffj,l*rffhtffj,|v`c_vni
tH_*h|@hnig-j,x0rffhsHbHAbH_bj^Hn`<_@n`5{fll<sHg`<bffjd`<bH~+n6hqsel<d)q n6h~Ajn)kv`<bH~0dtHhqhIh ffjg ^el<hqfjnh`cgz^elG`<hk
rex-_@sHn0qdsffkex@{Ih|@hndtHh l<hqqiff*h0ffn)qdtffj|vhd_keh ffbHh+_@sHnrffjvq`9p0on)jg h*_@n.{
v <V C

0b@,ff7ff,9@ff
y> 6- ).8}+p _@bffq`9qd)q_omjYqhd_o
A,),AY@-zijqhd:_aoB Y@,9@-ijb7ff79@@: jbffk@@e@
@H79@meff,9v-. :d6tffjdkehdhng`<bHhquo_@n*hj@p6tqd)jadh++8jbffk:j@p d`c_vb-
dtHhbHh Hdqd)jdh,ff"Yy>e)}){
@
j qhk_@b-d6tHh+jrff_|@h+keh ffbe`<d`<_@bY*hpjabkeh ffbHh+tffjdj &l9jbHbe`<bH~-te`l<h hjnbe`<bH~-qxBqd6hg`9q{
_@d`9p hd6tffjdI*hj@qq_Hp6`9jdh0dtHh*`<beo_vng-j,l dhng rffhtffj,|v`<_@n)J*`<dtdtHhd6hng d6n)jbffq`<d`<_@bzosHbffp d`c_vbffHi
tHhnhd6tHhj@p dsffjEl"rhtffj,|@`<_@n`9qdtHh+jvp dsffj,l dn)jbffq>`cd`c_vb:osHbffp d`<_@b {
v <V 7

*@ff9efe9Y @)9e,yf}p _vbffq`9qd)q_ojbj~vhbAd
hbe|@`<n_@bHgzhbAd:qxHqdhg
) .Y})i*jbffkjqhd _oH)@79v:A7@H
0,<<< ei&jElGlqtffjan`<bH~d6tHh qjag h-qhd_o_@rffqhn6|Ejrel<hqd)jadhq+ziStHhnh-d6tHh j@p d6sffj,ldn jbffq`<d`<_@b
osHbffp d`<_@b:`9q_@bHh_oCdtHhqh^ff_eqq`<rel<hd6n)jbffq`<d`<_@bosHbffp d`<_@bffq{
_@d`9p hdtffjd0*hsffqhkYdtHhdhngA,),Az@n)jdtHhnd6tffjb wsffqd+qd)jadhq{0bY_@rffqhn|jrel<h
q d)jadh_oujbQj~@hbed0`VqmtffjddtHh j~vhbAd^hn)p h `<|@hqjd+j:~`<|@hb^ff_`<bed y5h@{$~{<i`<d)q^HtexBq>`VpjElClc_Hpjd`<_@b.}
n)jd6tHhn dtffjb0`<d)qfp _@g ^el<hdh*qd)jdhC_aoBebH_*l<hke~@h@{ hj@q6qsHg hCdtffjdfjbj~vhbAd pjbj,l<uj,xHq kv`9qd`<bH~@se`9qt
rffhd*hhbkv` hnhbAd_@rffqhn|jrel<hqd)jadhq{C*tHh+p _@g ^el<hdh+qd)jdh0_oCAbH_*l<hke~@h+pjb:rh+keh ffbHhk8rffj@qhk
_@bdtHhte`9qd_@n6x_oj@p d`<_@bffq jbffkQ_vrffqhn|Ejarelchqd)jdhqJ_od6tHhj~@hbed{*te`9q te`9qd6_@nxY`9q jb_vn)kehnhk
qh vsHhbffp h-_aou_vrffqhn|Ejarelch:qd)jadhq0dtHh-j~vhbAd|v`9q`<dhkjabffkj@p d`<_@bffq`cdJ^ffhno_@ngzhk.
{ H_vnh jgz^elch@i`o
jbj~vhbAd^ffhno_@ng hk:jabj@p d`<_@bdtffjadIl<hk on_vg jab-_@rffqhn|Ejrel<hqd)jd
h d_ jb:_@rffqhn|Ejrel<hqd)jdh

fiff

!#"$&%&'$$)(*+'-,.'$/0,1%2'-43*56-7859!+':";<'-="6>0";)?38?@A"B.'-59C.'-D@Eff 830"B?.'->AF

:"B?G"B33$'->6"B.'-59H%&#3*6"BIJ"B(*598)8#!+(*@?K56LM8>?A"'-N?5EO'-+-5EPQ"B?.'-!+'->EM%<fi'$-G8G8!R(*?56L
3*59.'-(fi'$$':.':SLT5E?<6"B>AU>A ?5EV'-V(*598,8@, (7 "+!#"$

WW9X

>598A"6ff

fiY[Z

jk*l
t6}

m*noHpJqrm[o





Crnt1o{u{rs4n

t=t6}{vs={n

u0wqTro{drNvsJn

m^sBvnEtm*nonm*t6

t#{n

w}
t6}

t6}

6|Hu*xOt6}

4Ku*yr~drltB}

sKt6u1t6}

r46rm[o{r s}

r#n{

o{rt6r6JvTn



u[{qon

rron

u[t Eu[n{xsr4u[



rNvt9squ

r




rwmzu*~[rRCuo{rEq

srd

6rd{v6r t6}

Usru*xym*

rEvn<lU[[

u*Gs

6t6}

r odvs6Es6svu[n

us;rns66}

rEvn

rGEu[C1u[n
r[l<



u[n{q|

Crntr}m8~dvu[9sSvtwEudnsvo{r9sSvs

m8qstEm*t6r=vs=n

6rsrn{t9m*t;vTudnsl)mdst6}

vsVx



CrntO}{vsS

m*1rn{t9m8qm*no1s;u[1rRrE
u*m*{qr t6u1o{rEn

6u[{qrvsVx

6t6}

u[t=n

r u[n

rErs6sBm*vq|6r

r4t6}

n

rn

}

u*qt6[lOd

*9

t6rnsvu[nsSu*xvtOvqq)r#odvsBEs6sroJvTn[rEtvu[n

rUm[sv

6u[{qrvnVqm*n

rRodvs6EsBsrovn1t6}

N#}{vqTr=&rm*6n{vn

rxu*qqu*vn

n{vn

#}{vqr&rm*6n{vn

s|

rm*[rn{t#Nl[vsUms;

t9m0vTn

r46r

xBu[

E[l

no{rUm*n|Nu{s6sv{qr4t69mznsvtvu[nNx

=ru*xMm[Etvu[nsSt6}m*tymz6r#rE

6rs;rnt9mztvu[nHu*xOt6}

s6m*tvsxAm[Et6ud6|{qm*n

vst6}

r4Vqm*n

n{vn



t6ro=vnmEu[

N#}{vqTr1&rmz6n{vn

r6rExu[6rm{qmznvn#}{v6}t6}

Crnt=r}m8~dvu[lvnu[Eo{r4t6u[m*9m*n{t6rr=t6}

st6rlK#}

srtGuzxt6}

}m[stBu4noHmJ

r6r

BuErs6sfil&t6u



r=st9m*tBrsyC



s|

s@vTtBm*tvu[nsU#}

6u[rUEu[={vTnmztvu[nHu*xVqTrmz6n{vn

u1qu[n

m*noHm[Etvn



1

u[=vm8qq|

st6r9

rm*[rn{t1qrm*6nsrn

rHm[B}{vTr~dr1rntu*x#t6}

r6rvt9sU[um8qVvsUn

rm*drnt

nEt;vTudnNvTnOM{qmzn

9sr#uzx)vt+vs+u*q|{n

u[

[}m*u[

tHt6}

r

rm*drnt [um0q#udtvEr

r9m8qAlOm{qm*nJvTd}t=rN~[r6|Eu[C{qTrE<Gnbm*[rn{t4=v[}{t1m*6v~[r[lvnt6}



ro&lvxRr

rUus6s@vT{qrr}m8~[vu[Es

nEtvu[nxBu[vTtEs#}{vst6ud6|uzxSstEm*t6rsUvn^=ym*noHmdEtvu[ns vTn1RtBum*n

stEm*6tvn
r#n{



srEtvu[n1m*novTn{~[rst;vT{m*t6ro

vT~drnmJ[um8q<<lmQd.Qd[ &:d1vsUm1{qm*n^t6}mzt#[m*9mznt6rrswt6}m*tUt6}

vqqM6rm[B}m=st9m*tBr#vnN

t6}m*t8l&vnb[rn

n{vn



*v~[rn1m*n{|4us6sv{qrGr}m8~[vu[Ou*x

u*qo&l<m*nos@vTJv:qm*6rs{qt9sUm*n^r=u[

&rt@BVrmbVqm*n

vsm8qqTro^.{vxMt6}
no{rovTnHtB}

BrsrntBro

rsUodvs6Es6srovnKvsBE6rt6r

r m*[rn{tytBum[B}{vr~[rUvt9s+[um8q&u[n{q|1vnmUx9m[Et;vTudnAr[ld{1MuzxMt6}



Gm*novs==6}Cu[6r

}m*lR[d{Gm*novTnu[6vTnU t6}m*t vnEu[Bu[9mzt6rs

rnt4srEt;vTudns *v=vqm*o{rEn{vtvu[ns}

m[Etvu[nvn^19

vt9sKqrm*6n{vn

r6rUvtUn

r

m*nou[sr6~{

6rswvt6}^u[t6}

OUm8qr6nusrsld*

ru[BrtvU1uo{rEqs@usrs#rn

)[=xu[Gm*n^m*[rn{t+vsUmx



rEvn<l[[

t6u[Nm*t9m*.qv[rs;t66Et6

@C6EE929lm*noH^G*8E[+[8[dxu[#tB}

rn{~[v6u[n

Et6u[l*K}{v:qr

st6rNs @Um8qrBnusrsfilMd*Um8qr6n<l

rHm*[rn{t-sUqu

@#m*Nm[o{[ru[n

r4m*BrGn

Ndy<<)U

ud

ro^{|m[Etvn

{q:vBvtq|[

tBu[m*t9m@us;rns66}

t6ros|

6u[{qrvsSt6uKno1mGsBm*tvsxAm[Et6u[6| {qm*nCt6}m*tRm[B}{vr~[rsOmU[um8q

rrn{~[v6u[n

vns

rGo{u

r

Et6u[8-s4m[Et;vTudns *v~[rnt6}

1r=t6}mzt4m*nm*[rn{tm[EtEs m[sKvxyvtvsmCn{vt6rAst9m*tBr1m[B}{vn

m8qGst9m*t6r@usrnsB6}

v~[rnHtB}{vsw1uo{rEqAl

}

6rsrn{t6rorE

=ru*x<usBsv{qTru[sr6~*m*tvu[ns+m*no1us6sv{qr rn{~[v6u[n

nEr6t9m0vTn{t|tBuEu[n{t66u*qt6}

t6}



{q:vBvtq|[ }{vs#zvT~drsGsBvnEt4m*nosrEx{qM6r

w}

rxAm[Et9swvtUqTrmz6n

udt r=6r

rsvt6mzt6rom*

ro{u

m*tvu[nsfil4m*no
1rn{tKAn^t6}

r=m*[rn{t-s#o{rBvsvu[nsUvqqOr4mdsroHu[nvt9s#}{vstBu[6|Nu*xRu[sr6~*m*tvu[nsUm*noHm[Etvu[nsK#}{vB}

~drntC[|stBrs=@



rqTu

rrn~dv6u[n

r=m*[rn{tm8|16rm[B}m1st9mzt6r #}

Crntl&mdsrou[nt6}

Eu[1{qrEbt6}m*nvt9s=u[srB~0m*{qrst9mzt6r[}




6r[

srsu*xRvtro{u[n<t4m[s6s
t6}m*t+t6}

t6rsyvTn^t6}

rNrE<rEt9sUu*x#t6}

st6uHt6}

novnt6}

n{vt6r[#}

r1m[Et6m0qM6u[

r#us6sv{qrGudsr6~0mztvu[nsOu*x&t6}

l[[{m*noH{n

t9m8vn

6rsrn{t9m*tvu[n}m[s=m8q6rm[o{|

nEtvu[nbEu[66rs;u[no

rsr1Eu[1{qrEstEm*t6rsKn

r

rEvn<l[d{2mzno vn=u[6 u[n=6rmdsu[n{vn

r#u[s;r6~0m*{qr st9m*tBrsOm*6rt6}

r=rn{~[v6u[n

u*t6}m*tt6}

^t6}{vsmdEtvu[n<l

9

u*qro{[rvTnodvs;t6v

w}

rsxu*qqu*vn

r^t69m*ns;u[6tEm*tvu[no{u[m8vnrEm*1{qrmz6r^t6}



[d{Vq:vt6rEm*t6

rnHyr4mzns6m8|1t6}m*t

rm*[rn{t4vqqG{n

rstEm*t6r=vt=6rm[B}

t6u[Nm*t9mU@wusrns6B}

u*x t6}{vs=t6u[{vm*nbrxu[
m*rEq{qvn

u[t1*w}

u[t#*ltB}

rHu[sr6~*m*{qrst9m*t6rNjk*}{vs=rnm*{qrsCst6u^u[

nEtvu[nEu[66rsu[no

tUt6}

g6hi

1rn{t#r}m~dvu[vsKn

r m*tvrn{tOnudt6}HrEmz1{qrs#t6}

Eu[1{qrExm[EtEs m*u[

ef

^t6u^vnbt6}

u0wqTro{dr1@Um8qr6n^usrsl<8[*

m[Et6m0q)vn*

c[_

6rs;rnt9mztvu[nuzxKmz[rntEs}{vs=t|{rNu*x#6r

rudsr6~0mz{qTrbst9mzt6rs=vnt6}

m*6r#o{u[m0vTn<ldt6}

_

Crnt=r}m8~dvu[vsn

rEvn<-sRs@vTtBm*t6ro=m*

rmdEt6m8qytB9m*nsvtvu[n^x

vn

`baRcd_

u[t v1{qvro|tB}

9m0qy6r

r=m[Et6m8qMtB9m*nsvtvu[nx

t69mz

rNrn{~[v6u[n

_

r rn~dv6u[n

udt4r}m8~[rHm[Eu[EodvTn

rrnCsrovTnNusrns6B}
m*u[

Z^Z

sKt6u1jkwv:xymznoNu[n{q|1vxOt6}

rHm*[rn{tE[99=tB}m*t4t6}

rn{~[v6u[n

\*]

r

Eud

dr4m[6}{vr~*m*{qr[KrnEr[l<t6}

9sr1u*x

r1m*drnt

}mdsrsOs6m*tvsxAm[Et6ud6|1{qm*nm*nHr

fi
fiff ffffffffff

!#"%$'&($)+*#,-*.)/$01"2,3"456 7 $'$8&fi9$'7 $:$*019;$)/<#$:"2,*, ,=401"2*>6 $)&("6 9?*A@*B"74>CD*E5;4#F,=$'7 !G*EF/H%$,=6I*>6 $
*>5)*>5.*06="45.6 48F$@$'7JCK471LM$):"5.6 9*E6N,=6I*E6 $OQP(9/"2,R"2,N*8<$'5$'7*BHS7 $'@7 $,=$'5/6I*>6="454>C045)#"6="45*BH
@/H2*>5,'OTU5+$VW01"$'5X6:@/H2*>5&("YHYHD619$'7 $CK4#7 $M047 7 $,=@Z45)[6 4\*\)/$01"2,3"45]6 71$'$^4ECD@Z4>H_X54#L:"2*BH-)/$'@6 9SO
` 46="20$a94>&($'!$'7a/6 9*>6fi619$b,3"c'$b4>Cd*>5.$VW01"$'5/6-@/H2*>5Le*B_W,=6J"fHYHFZ$b$g@45$'5/6="2*BHhO
iQjMkmlenSoqpdrtsusv3nSwduyxUz{sErR|Q}
~ :
5 6 9$R@7 $'!#"%4#,Q,=$06="45:&($N)/$5$)M*F*,3"20tCK7*>LM$'&(47 U4>C{H2*>55/"5<8&fi9/"YH%$fiy$*E7 5/"5<aX*>5)8&fi9*E6*
, *>6J",C*06147 _@/H2*>5ACK47d*E5*E<$'5X6Q",O ~ 5M6 9/"2,d,J$06="45e*>5)8"5M619$(CK4EHfH4>&("5<b4#5$,Q&($(&N4#/H)^Hf"$U6 4
045,3"2)/$'7U6 9$04LM@/H$gX"6=_.4>CS5)#"5<.,=019\*b@/H2*>547fi0 9$01#"%5<A6 9*>6fi*:<>"!$'5.@/H2*>5"2,fi, *E6="2,3C*#06 47 _O
~ 547)/$'7fi6 4.)#",10, ,U6 9$"2, ,J$^4ECN04LM@/H$gX"6=_\&($5$'$)6 4.)#"2, 0, ,U47ULM$*,=71$,fi4>Cd04LM@/H$g/"6_a
*>5)6 9$:6=_X@Z$b4>C(7 $'@7 $,=$'5/6I*>6J"%4#5,fi4>CNH2*>55/"5<\&fi9/"YH$W$*>7 5/"5<\,=_,=6 $'Le,&($^&(4/H2)HY"$M6 4AH4X4
*>6O



S8\AB'Z'
$&("YHYHd)#",J6="5</"2,=9F$'6=&($'$'5\6 97 $'$F*#,3"20b{H2*>55/"5<.&fi9/"YH$^$*>7 5/"5<\,=_,=6 $'L7 $'@7 $,=$'5/6I*>6J"%4#5,'
OU ZBdA{B'SS( 46 9.6 9$U5/L^FZ$'7fi4>C*E<$'5X6B,R4F,=$'7 !>*>F/H$b,=6I*>6 $,"hO$Oae
*>5):6 9$fi5/L:F$'7(4>CS@4/, ,3"F/H$6 7I*>5,3"6="45CK506="45,RLe*B_8F$U$g@Z45$'5X6J"*GH%U"56 9$-,3"c'$U4>CS6 9$
*06 *BHS7 $'@7 $,J$'5X6I*E6="45SO
OUe 'MB'Z'SS P(9$(5/L:F$'7Q4>C*><#$'5X6,y4F,=$'7 !>*>F/H$-,=6I*E6 $,yL:"<9/6
F$M$g@45$'5/6="2*BH("5?619$e,3"c'$e4ECD619$e,=_,=6 $'L7 $'@7 $,J$'5X6I*E6="45Sa{F6b6 9$M5/L:F$'7:4>C(@4X, ,"%F/H$
6 7I*>5,3"6="45AC506="45,R"2,fi*>6(LM4X,J6d@4EH%_/54L:"2*BH"%56 9*>6(,3"c'$ONP(9/"2,R"2,(*8LM4X,=6fi*>@@Z$*BHY"5<e6=_/@$
4>C7 $'@7 $,J$'5X6I*E6="45.C47fi,=_,=6 $'Le,(&D"%619.F45)/$)50$'7 6I*G"%5/6=_\=-*BH@Z$'7 5;d*>7I)#"ha IO
P(9$.6 7I*>L*>h0'*>7 $e,J_,=61$'LLA$'5X6="45$)"5$06="45 "2,M*>5]$g*>LM@/H$\4EC*,=_,J6 $'L&("6 9*
*,"%LA4)/$'7I*E6 $\7 $'@71$,=$'5X6*>6="45SO ~ 5q,=0 9*?,J_,=61$'L&($.,=*BHYH_9*B!$;*,=$'6M4>C^*E6 4L:"20
4F,=$'7 !>*>6="45,h$O<OaX&9$'6 9$'7(6 9$F/H4X4).@7 $,1,=7 $U"2,(9/"<9.47RH4>&IOQP(9$5/L^FZ$'7fi4>C*E6 4L:"20
4F,=$'7 !>*>6="45,t"2,RHY"%5$*E7("5.6 9$-@714F/H$'L\,R"5@6aF6(6 9$U5/L^FZ$'7fi4>C@Z4X, ,3"F/H$b4#F,=$'7 !G*E6="45,
"hO$OaB4F,=$'7 !>*>F/H$D,J6I*>6 $,y&9/"019M*>7 $Q6 @/H$,{4>C*E6 4L:"20Q4#F,=$'7 !G*E6="45,Z",$g@45$'5/6="2*BHhOdP(9$tHf"2,=6
4>CS@4X, ,"%F/H$U"5>=7="$,(6 9*>6Q6 9$U@*>6="$'5/6dL:"<9/6d9*B!$D",R,=*GHfH_@4>H_/54L:"2*BHy"5e619$-@7 4#F/H%$'L,
"%5@6BOMfi$'50$a&N$:<$'6b*.#*,3"LM4)/$'7I*>61$M7 $'@7 $,=$'5/6I*>6J"%4#5;4ECD*.{H2*>55/"5<&fi9/"YH%$.y$*>715/"%5<
,=_,J6 $'L\O

'.A{B'SSfi 4619M6 9$fi5/L:F$'7(4>C*><$'5/6,{4F,J$'7 !G*>F/H$b,=6*>6 $,Q*>5)A6 9$
5XL:F$'74>C@4/, ,3"F/H$b6 7I*E5,3"6="45CK506J"%4#5,R"2,(@4>H_/54L:"2*BHS"56 9$-71$'@7 $,=$'5/6I*>6="45\,3"c'$OdPD9/",
6_/@$84>Cd7 $'@7 $,J$'5X6I*E6="45",H$, ,U<#$'5$'7I*BHd6 9*>5*e#*,3"LM4)/$'7I*>61$^71$'@7 $,=$'5/6I*>6="45SayF6U"6",
,=6="YHYH$g@7 $, ,3"!$*>5)M04LA@/H%$'61$H%_5456 7J"%!#"2*BH*#,Q&($fi&("YHYHH2*>6 $'7()#"2, 0, ,'O(P(9$fi6 7*>5,=@4#7 6I*>6="45
)/4Le*B"5$g*>LM@/H$4>Cb6 9$\@71$'!"4,,=$06="45",LA4)/$'7I*E6 $H_]7 $'@7 $,=$'5/6 $)]F/_]*+<7*>@9HY"%#$
,=6 7 06 71$aS"50'*,J$,b&9$'7 $e619$'7 $\*>7 $e*E6^LA4X,=6b@Z4>H_X54#L:"2*BHYH%_Le*>5/_+*BH6 $'7 5*>6="!$,C47:6 9$
*06 *BH,J6 7 06 7 $84>Cd6 9*>6U<7I*E@9+h$O<ZOa/"5+*M@*>716="20/H*E7b*>@@/HY"0'*E6="45SaS6 9$'71$*E7 $^*045,=6I*E5X6
5XL:F$'74>C@Z4X, ,3"F/"YHY"6="$,6 4e*^H%4#<X*>7="6 9L:"205XL:F$'74>C7146 $,IO
b:y(bB:='G='fJ- B\> 'BIRfJ-y3h BG1N 1:Y I>1'dfJ1
='G='fJBd =GJB='h1b=


fi>]d#

3. %/' ff
1fi ffZ'( = '/'> = h !#"$%& ')(*/'>
= ,+
'^-.//'0>1e 1 ='X>)1h #"2$&% ')(*/'0> ='/0>!3#4+56
5
798:>fi;,fifi<8>= > fi;? ! =1@"

A#BDCFEHGI JDKMLHN#OQP2R4SGS J)NUTVGWXLGSYfiZN4[JYI
\ )]'^M7_8Efi%;`fifi<8a=y E fi;b = @cQ1' @> 'de^f%>)e8gf> ;&
>h(f ':/' "
"j TS [eGKffSGklWY4mn f-o&%;' 'p fi8:>H
q{ ff;'/U 2 ->: '
r%f : 2f%s0>)e8<8)
>04c>,d A0ffot'/:e8!3A%"
hQ798:>fi;,fifi<8=y >1fi%;M = (1>
re8<89%/. fi: f>1;%%c]#';1%ufi8vff

='/;: gfi8:>@E]'!w
*&%;A >( fi: h fi8<% xfi8:>d: >: '
r%f#H hf%s0ff(
)e8<8),X10%f0ffufi8d*r"%")cZ ' %fb' /
qU J'X0E),: Ut'/:e82
]'<zVf>)/f (0ffo# (t{'/:e8)Mff+0"

| " }~h! J*T4YQh[ GK6SGklWYVm >: '
r%f#fi8:>,
*fi1ff;'/% J fi 1 ='X>)
*r"%")c{ff8)X%82% : 1 ='X>)@6
&ufi8%x+0cVEaf1 f-o%;,fi' 'sfi8E
>: '
r%f-f>@uZ>f>1 d%fi?f-'/8),@{ff8)X%8)M%"
=1 { >U
8w8/ fi: pfE ;%d ffyu:U :e8V>8' -l-f cfffifi:f1
>dX 8<8<);'// ');'"%")c:d>4+ ;%;# =0 h %Mfi8:>x
q# !ff8)]%;,y%ufi8@"h(
;%; =1 `fi8E` J'X E]'<z 4"]
gs
w8: ,]'<zVf>!3#f - c( '
bt%fh
re<8) H U1 1 4cV> >/ 3;#'ee -.;''0E >:fi8:>#"gDfi
:e8l>A' hf : 8fi:fe8l 683!3#,
*/ ');,%ufi8 " / ');' ;ff)]'@
f-<zVfy&ufi8%/c#>/e =:'t''fs. !;%; =;,fi8:>#"H(yfi8E. J%fi8:
u\1 ='/ >]'!wz ?f-'/8)%"*
b /fi8E^: A, >: '
r%f-%cd ?f-'X
]'<zVf>)/f - h8)f>
re<8) >dfi
* y/ ');' c{fid ,f1 Jb
;''0> hU^fi8:>#c#'0fff"2(fi: 2ff%f1y% lE9z0 =2%/(tfi8<f-M #8< '0>- %c
u > Jye8_6' ff
Mf n E fi>@ e Mf n E fiF e c &% +2fi' X%

K'1 ?& U ,: !%2%;06@"(fi: fffi%f /: /' +1>#> J /ad>
0%f:fe8 '>) cg f1% M1a# dM'X!3# ]&)% , = f)#"
/'0e<8 @/ J 0>),6
Ep:/ d>s
* 'g&: f 3#.fE,u
*%?%M@fi =
S''ff8%c %& ffS''ff8%c %6 +0"
fi'f%cfi@ = d1>
e8<84X-^1fi f> ;%-%c]ff>)% hfi8:> f>uZ sa>dff#(D8<
/ ');f cp { `ufibMf%1')' ]?f-'/]'<zVf>)-f fi %cfifi:f1
%
*fi8<83 fi80 >: '
r%f#fi8:>#"
" } JTVYM[eGKffSGk9WYVm >: '
r%f#yfi8Ed
qR ff;'/% hHff8)/%y:e8S J'(
0>)#c1>h: fi#fi8).?f-'X!83/]'<zVffufi8%c4uf>,u%f8w8)@f%s M83;& (
y:fe8<83{+%/ff8)/%y:e8Q)M%"
A#B)AEHGI JDK@sYIffR#W)S
+yh%fi8:x8w)oA?f-8:% - '<
qM798:>fi;@fifi<8=y E fi;a = u% = / yffuff]f>1;% "
(X
qff8<8)ffh;. fi80 > ')fi8f#ff8<8:> ff
Q fi80 ff] dufi@/ = >M''ff8H
>1'f#X tS''ff8M/ J c %% @fi = v''ff8%c %i% S''ff8%c
%i +p> g-ff
9: h%y 1
% ufi,ff
Q fi: 6' "
0

fi`l M) /%

& )%&0e fi0ff)%yffV 9:fffi)yfi<)g4 ff-fi3!-@e&)> ff:'r%
%dfi:ff,:h{h2*ff04hhH%fi)ve) h{*ff0x%,<ph>%fi
%fi)?-)figfi:ffh3Hff2:ff- dfi6@ed&:er
h*ghfiM%H :y%0!3& -@?-)fi>fi:ff2&3@-ff:'r%%Mfi:ff)
%3*sfi06: h{*ff04hh>3Hff2:ff xfi6ad s&
:er
U3&),ffd?-)ff:'r%&yfi6,) &')*fiff9:fffi)dfi<)>4 ff-fi3,
23-%fi)% -')fi3-0ff'))%sq!3&@*r%%fi:fffi)h),%fi))fiq%-ff
)%42: 2*ff04r@-fi %%3%%/%':fi fi6ff263fi%y:ep')%
bffff%M !fi3,ff)%b!%0e !:)%@&,6,h<wyb6fi3`%e))

%-fiVhffQ&%-ff &)a@-ff:'r%%,fi:ff#U)y?-)H% )?-)
h<<_{dr%,ff#D<)ff0%06fi363&.6fi)0ffd !06)%hff_6fi3,fiw)4 ff-fi3
b'^%&))%#hh@ff%y%{@ff_6fi3`fi<)Q4 fffi)3 &')*fiff
!06)%hh<<_{%D<)y0%fffi)%2Q3/ff&3-@%0e9%': )%
& )% %')*fi0ffd fi0ff!3&Qff5/9:fffi)Qfi<)@4 fffi)Q@U:H
%fi)6#&)@ff,?-)ff!%-%fi:ff/ff#Dw)H0%06fi3ff
)%Hfi0ff fi0ff)%sffl9:fffi)fiw)4 6fi)!-@ff:&fi3
ff#&)@ff,) ?-) %gff-fi3-0ff-ff:'r%%sfi6@0&0fffi)r)ff#Dw)H%
%D<3ff-
aw< 6,fiffM fi0ff!3&d0e@ fi)0xff&3 &')*fiff

!06)%9h<<g')fi)s&<Vff)%sff fi)p-ff0&)fi0ff fi0ff!3&
Q h%fi:sw)%ff1ff^fi&fi3ff29:fffi)fi<)>4 6fi)%s),fiff
!06)%fi:h%*-)%:er
%':fiffa6% ,fi % e%>&fi)H3fi*%dff)%@&a-fi%)%s{
e%)%e{%)#-e,d%y6a&>{')fi),e&)%ffhy&)%fi yd fi:ff#
) %sff#)y ffl%)%% & fi -:')%,-%22%)%#&)fi:&%ff:h
*)%#%%fi)ff# %-fffi)06%%e:yff# %hfi:%->6l9rd 6fi)
fi)% H)3fifi)%#&&9 0206%p %)%&:)%fi:de%)%
*%e%)% g fi2fi:ff,%h&<#fiffd*%h% fi%)%s{e%
%%&3d ffsq&% ) e% &%0&)-
%-ff &)-fi-),h%-% yff0ffsh),)0fi:ff,fi:h: fi:e<3/
&<#{h@! yff,fi -:')%@-%hd2fi:ffxeffyff&:e
fi)ff*d),536-ffpfi:ff#6eyff&)fi0%06fi3 0%sfflfi%:')d%p%!w*&3
Hfi:ff#fi%d)fi0ff-g- ff)%2hfi:hff:h&dh5&':fi?-)fi6
%V_6fi3dfi<)4 fffi)d%,)@sfi06:g&fi)),%*)&


pfiff'

2d,g'p

rfi: )%`hff ffff)%dfi0ffdr !# %3*sfi06ffg- ff)%Qff
9:fffi)`fi<)@4 fffi)b/2%yd:yMff:'r%&Mfi6r #2ff`?-)
ff!%-%@fi:ff4 *%>6bff%fi #-y:y@ff!%-%@fi:ff1r !#lffQ,-3fiff:'r%&



fi !"$#%'&$(
) *,+-. /'0$+1/32+-54$687'6)796:;6-</96=?>@-5)$A1*@B -ACD>,+E*F:;)$+2G6H+-$=I2+-54$6J29062'K!6=D>L-I)$A1*@B -ACD>,+E*M/;>LCN6O
P :RQR68CN6-</S>@A!-6=H/'0 >,:T>,:U+V-A-W /97;>LX>,+E*MYZ+!2G/36X!6-5Y[A7RCNA= 67+/96J:;B\:S/'6C]:OM^68)7'AX68/90687'6:;_ *L/
A78CNA= 67+/'6N:;B:;/'6C]:`Ha+-$=/'06-b:S0AcQdQe0<Bf>@/g>,:J+)) *h>,2+4 *L65Y[A!78/'06D7;>,2'067D2GA-</96Gi\/8AYkj_$+!:l>LW
CNA= 67+/'6J:SB\:;/96C]:O
068)7'A<A1YnAYFA_7kA1opWq*h>L-6J/97+2G/+4 >h*h>L/;B]7'6:S_ *@/QR>h*h*rY A*h*LAQsY[79ACt/'06Y A*h*LAcQu>@-v?*L6CNC]+:`O
wFxzy{y}|~r zpEr'D5 Lq GhVJ!<qE, ]N!\ qID!; DR!$$q<
<qh8a!z, qqg hJ'`<E,9Nfi LhF ;q , $ qfin!$88<E`
<`V 5< q<;N`cqJ5 , !ghI$D`<,Eq<gbq}q R V <E;
q<Nh `zqS '!<zIVUG
JE$GR YF/'06+1v6-</R)$67;Y A7'C]:e+E*LA-v]+- B])$+1/'0AY CNA7968/'0$+- +2G/;>LA-$:RQu>@/90A_/k*L6+79- >@-v
+- B</90 >@-v5 >fiO6OH\+c*@A!-v +2G/;>LA-$:T/'068+v6- /k= A<6:T-A/Rv6/k+- BN-6Q>L- Y[A79C]+/;>LA-I+4$A!_/k/'068+2G/'_$+E*
4$60$+EX!>LA7.H/'06->@/8CD_$:;/X!>:>@/D+5)$+7'/;>,2G_ *,+7VA4$:;67'X+4 *L6]:;/+/96J/;QR>,2G6HQR>L/'0A_/8v6/9/;>L-v+-<B-6Q
>L- Y[A!7'C]+/;>LA-+14$A_/k/906J+2G/'_$+E*a4$60$+EX!>LA7H+-$=5/'067'6GY A7'68QR682+-}:;07S>@-K 4 BI= 7'A)) >L-v+!2G/;>LA-$:
Qk0 >,290}/9A<AKI) *,+2G6?4$6/;QR66-}/906:;6JX!>,:l>L/:O8^6N2+-)$67;Y A7'C/90 >:)79A\2G6:':e_- /;>h* /'067'6DQR>h*h* 4$6D-A
:;6j!_6-$2G6JAY +2G/;>LA-$:T>L-Qk0 >,2'0-AD*L6+7'- >L-vIA\22G_7G:O
06*@6+17'- >L-v]AYF/'06J+v!6-</T>,:RCNA-A!/'A- >,2FQk06-6X67e>L/R*L6+7'-6=:;ACN6/'0 >L-v]+14$A_/k/90686-<X!>LW
7'A-C?6-</84z60$+X!>LA7HzY _/'_7'6V>L- Y[A!7'C]+/;>LA-{2+-5_$:;/8C5+K6J/90 >:gK<-AQR*L6= v6DCNA!7'6N2GA-$2G7'6/'6OD>L-$2G6
/'06V-<_CD4$67AYF)$A<:9:l>L4 *@6D4z60$+X!>LA7:u>,: H$QR68v6/k/90$+/U+?K<-AQR*L6= v6g>@-$2G796+:;62+1-A22G_7+/eCNA<:;/
/S>@C?6:O
ACD4 >@- >L-v5/'06J+4zAcX!68A4$:;67'X+/;>LA-$:T*L6+=:e/'AN/'06J= 6:>@796=I7'6:;_ *L/O
wFxzy{y}|~rqk a89 Lq GfhkDE; 3hz$q< <qhaG$q<
qq?\1[`h]'`<E,9Gu!$ ;Iq<5!<`q,ShE<q5a<
q<EVLq5;;;`E$ qI;kl,fT\h$Dq,55$8Z Iq< EG , Dq<F
8 qE GN!9D!G,G5<Rq qD\h$!qaq]f$V
JE$G}m 062GA-$29>,:;67'6)7'6:;6- /+/;>LA- AY 2GA-$:l>,:;/G:DAY+/G+4 *L6H Qk067966+2906-</97'BAY8/'06
/+4 *L6J2GA797'6:;)$A!-$=:R/'AN+D=!>:S/;>L-$2G/UA4$:S67'Xc+4 *L6V0 >:S/'A7'BNA1Y+-?>L-</967+2G/;>LA-IAYM/'06J+v6- /RdQR>L/'0I/'06
6- X>L7'A-C?6-</Hz+-$=2GA- /+E>L-$:k+1-+2G/;>LA-I/'AN4z6J/+1K6-]4 BIY A7k/'0$+/k:;)z629>h2Nl)$+7'/;>,+E* .3:'2G6-$+17;>LA$O
06- _C4z67IAY=!>,:;/;>L-$2G/I6- /'7;>L6:?>L-/906/+4 *L6{ .N2+-4z6*h>LCD>L/'6=/'A>L-$29*L_$= 6bA!- *@B/'06
) *,+_$:l>L4 *L6N=!>:S/;>L-$2G/k0 >,:;/'A7;>L6:Z>ZO6OLH /'06U0 >,:;/9A7;>L6:RQk0 >,2902+-]4z68v6-67+/96=z.rY A7R/'068:;B:;/'6C +-$=
/'068) *,+- 068- _C4z67kAY :;_$290=!>:S/;>L-$2G/U0 >,:;/'A!7;>L6:T>,:k4zA_-$= 6=I4 B fi/'068- _CD4$67kAYM)$A<:':>@4 *L6
4$60$+EX!>LA7:eAYr/'06V6-<X!>L7'A-CN6- /. /;>LCN6: l/'06J=!>hop6796-</U:S/+v6:T>L-+N:;)$629>h2g>L-</967+2G/;>LA-.O
-]A!7= 67 /'AVX67;>hY B/90$+/3+g) *+1- Qk0 >,2'05>,:T7'6)7'6:;6- /'6=>L-]/'0$+1/ C]+--67>,:3:9+/;>,:lYfi+2G/'A7'BH!A-6
-66=:e/'ADvADAcX!67u+c**M)$A<:':>@4 *L6D4$60$+EX!>LA7:k+-$=?Y A7k6+!2'0IA-68A1Yr/'06C29062'K5/'0$+/ *L6+=:e/'AD]:
vA<+c*ZO
P :k+-5>LCNC?6=!>+1/'62GA79A*h*+17'B]QR68A4/G+E>L-/'06Y A*h*LAcQu>@-vz
x Ex$y~rLe ,zq<{ ,E fh$DE; Rh$z, <qhaG$q<
L8;k;q$D ;E <hc
A-$:l>,= 67]+-+v!6-</DQk0AQu>:S06:D/'A7'6+2900 >,:]= 6:;/S>@-$+1/;>LA->L-/'06I0A<:S/;>h*@66-<X!>L7'A-CN6- /NAY
62G/;>LA-sO -)7;>L-$29>L) *L6HU/9067'6ICD>Lv0 /4z66Gi)$A-6- /;>,+E*h*LB<W C5+-<B0 >,:;/'A7;>L6:DA1YA4$:S67'Xc+/S>@A!-$:D/'06


fik{r$8L$$
I]E$G 'te''GL'`8I'; L'E\5'$15@5,f'G!$l, L
$1@ D,EhL ] 'L 'I$9 }9';,' ,aff
R ,e,N<;Gfi[
k'J a;Rhh$ @V'N'9;<e;!;'I;1@S@!LD'G,;LGL}G!$9S
kE
39;!;'SL;L!U';, fiG'!'$9 fiG uh z I9@ ;L} 9G'a$
$9$ $R $' $EL'
R 9 L $EL<1;LNLI8<S;h@ L'?< 3eN<S@!
zc!eG'$EhL;Lf' , kE
e;,GD'$ !N5# "$fi $g& % L'5';, fiG'9# [N 'N;;'5 'k RL(
%$!lfi
N ';;'5N' ,) !N] ,I!I$; [ fi8lL$G @'$I;+ *]@!<, z} -$!<;,E
Ls'G'$E8' '; S@!lfi ./
'e 3! 90 9 $'S@5 EL1
<2
!N]3
"$fi $
$'<EL1
G$l, ;L 9L<, ,$`5
4N?<; !G;,E6 7 $<;` 3 {?L<;
cL;, ]'S';,G;LNU1'' ;LI'D 9L<8 ,$lL$G ,8 Rhhn! $D LJ'
-G'] -$!<;,EhL{] !G;L$L{']GSN U,
,a) 9kL{'$ff !pN], "z; :
,# %!$lfi N 98' '; ;L$` R8= <
> ?A@BDC@FEHGJIGLKNM OQP=RS(T)U=VTWOfiXY[Z\R=]T^R`_baTScSFOQSed#fhgeOQaRijRT]kScOlSd`WnmW^R=Y&oApbSF\OlSdqTS(RsrtX
u OQR=Sc^LWT^OvWlwT u ^Z]km8xAayTS0OvWffZz XaOlScRff^]T u ^T{aR}|

u M9; LF,
%! L'U'S~ ZG9'lL$G6 %!$lfi N 'k;;'5F'k;,'5G<9-\h4F-
L;?U9 L'G''R;$9!F'k! !,'G$';# qT' $D ,RGhL8 9<
$=p1
$''9! %!$lfi N 'US\;9]L
RVL$9@$ J8zL<!D,Ea< $TpS
;GA k ,'{G'' z$'1} -$!<;,ER< zJ$<' L5$;';L$M$99;'
<c@$SUVh,;U J$<9lfi @D L'!N<8 $$E!L8 fi'J ' ;,'J;$;! aG'kN9
,;,I9
,;I J<!L'N , $$E!@!
8` 3N ;L1
$ '8 %$!lfi
N ';;']5G'' $$'G L - ;;']` k'' [ zI ff$ 'lfi L
R;,
'G;fi L'N L'!N<g,8 9L ;L<? Lq
RS]G$;;L'9]5;,'{$ zEfi
L) ZL{ U;;'] ~'Uc z'
!fi $=$) kD'; LD; J [!J -$ LF'$V'
'1] fi';\S't!9G$';?LI'8 'L$e;G;L# $ hLUR5 -z'k;;''$
!SR' -\kG;L5'& $ $ [9N $!;N!]' ,;'9N r $S'cS@!$ N'8 G'

R
9 L kGDL u@9' ,5 $5';L!,Efij J!8'; LJS} d9$J
G$9,;k9 '; ;LD 8 , e 9D,M 9L ;L!; LU < -<,;L e 9L<
'S~ ZG9'# 1} - ,;
R9 [9$'D a'e S@5'N ' ;,'ff , jqhLV
R'' k hL
J#L8}DAs

fi?' , ;G;L`3k;{'$MLM,FMhfiGLN'$1F''T, cEL;L'd'JG!NRRL'
I'S~ZG9' ,3 J N '#M, L+k h@,!9 @;;'alL$Gff;$; 9,!L
k'5;$' 1- SV,]8L $1,'5'I'; L`[D',$!l,# NR
G;L5
:1
lLDh,I'; L5,N' !Lb I9@ 'S~ ZG9'
,$

R , uh8L L
lLDh,8'; L '!;J I9@ J'S~ ZG9'# 1$e@
%!$lfi N ' n1 @, e L
!9 @I;;']`\1$#
R'J -'$ # ? 3! \R!,'G$'S5L' [1L RLI;G;La

u f'; L9'#
RL'' '; L5 c@ L'+ '!L$;G;L G` @9'
9,'l S@!} M, L, k hL !' LI!9G$';fL G;L+


> ?A@BDC@FEDIKM OQP=RS,TqY Z=\R=]T^R_aTSFScOlSd fhgeOlaRiRT]SFOQSed&WmWn^RYffoh\R u Ol\OlSdfhgeR^QgeR=]^QgeR]R
ROfiW^QWTST]n{Ol^]T]mZ]&Rr u OlRSF^ WnT^OvWQwnT u ^Z]m8xDaTS&wnZ]ff^QgeR T=dR=SF^OfiWffTS)8_XgeT]\xA]~Z{aR=Y|
=k

fi+tFF

ffAF fie57[lt8,}lfi==fififi=tF5Flfi27lF=
=fifiv +FFhficfie l6`b+`khlfit fiA=fit
F7tF7, elk#lsl
fi [Ffiff,lk#
~eq=fifififfF8l F=

F l Dyfi 7[llfi
ff8LFkA
lfifi=ff7F6=7lfi+ 8 vk,lqlj)k7[
ql8
ff8LF,fi
8=Ffi+l ff 7fiffl
lF8
l
F}=
fi
=fififi ,Fe~fi
FFfik 8,efiq7
! " $# fififi % " $# & " # =vfifi& " # ')( 7 7+
*,1cefific
Ffi -& fifilk&l.
qFe~fi,fiF6/
v0
21 13& 4 =fifiv& 46576 98;
e}Dl =7,[k/

9
l;
:<>=?@BADCFEHG%<JIh (fiF
kL K=67M
=fik F7 K(N

( l7,l )FF=kL -
e,k7.
P~R
QTSUH3=vfifi& ,V=fiF = .
kL K=[W
13ffF
fi`eLcF==Lkfiff7X
PY" $# F& 7Z
2k[ K=j7fi 21 F&efi
87ffFFfi.
PY" # =fifikl/
( 98;e\ -& fiO8FFfi
Z
kL K=fiF 21 137k
P~c =fifikl/
P]AL ( JL Kfiq7fiF
4 =fifiv& 465Lfi= k/
B-&
Obeq7lF _ ^ fi,=7 7fi ( fflfi=fi=`fi _ ^ -LF l F=


4 =fifiv& 465 ( t7lF+fi3}lfi=,Fa
`(lfi
_ ^ b77l7+ fiF
~e)fiklfi= -& cbsF}k l
^ LFd
L K=8,fi;
4He
l7fflL77l7=ff7[~ehFL~F 2 ( 3L-F7lfiL fFefi
FF=&kfi)

( Fh
98 Llff7fi}fi
f " #g
( f " $# -k=fifi+k&l.
e`L -&ickL Kfi,fiW
4 e k
^ AFff8FfifiqcF=fikcfi=
qk/
^ A%
j3}ttFjJ vl elkL7ll` LA
kZl 3fififi ,VnmM 2 B 3L Ffffifflfi=8 fififi Fl F=

FjF7F
`v
oFp,n
bfi l fi+N
Qq>r 7jF kL K= fi 21 13[cF)
}}` k ( Qs-& jAfiW
,tcuAFe8kL K=8fiW
46eFFff=cF8#
=7lfi, k lfi=F F=fi _ ^ Fbl=Fn
oFp`fi=F=
ekfi=LffF}Jefi cF=fi
hFFA8v~s78v9

qb vkL[fi
kwv F vD F=

kwv l FF=,kfi x
Fkn
,+Fy
( cFfik
Fhvj=7[F=h -!zN l7 [FjFfi 21 13 k v l F=Jl
lcff 7vF _ ^ F7qFbq~eFbvD F= _ ^ elk6fi
k v 8;fi J8}lfi=8L f l,l,F7# ( ,t9u{- q>r 7j
fi8FFfi/
f " $# n
f " #

|~}~wTT~R!9V
fiFff=F8fie7F=F fie=fie=3#kh7K, hlfiyfi
A=7{
L5q[ ~ =8j`JlF= fi,7fiF=fiFbvFb76e
=ffB j= fie&FF l8l.fiF7kLfiq0ylF b
qt= ,ql1(F~v,+F7lvqFfifi0v ( fiFFfi

&

fin!!!wc/!]aw!!!w

7&[w]])!w]w&fiLw)7!_h!;7%_7!a7!h_wL!xL]7wL
67]6]
s) _>waa6>/7&LwDs%;\!w]fi]H7_]!!7w]7s6\7
7B6]!W76L_!7!%NwL ]F]7!h7&Lw 7&[]696_]wd]]!
!9B
;7%w6+can!7L%]!B]! [76L7!_ 76&\!7!7
L6&n%!7!7!7/
6x/6!7h7!/7wLL6&fi%!+%.7%!7
9B7%B7!.L6H7w7L!+[wh
>J 76n7!.L6L6nB]!
g7!7_wL.L7!w67!!R.677[gfi!%7!/]677]!a7wD%

6
>].7hw6nx7!./[6]]L6fi!7ws7[D![[H!
_LT7!LwL_w]w/L77!&B>]L]wLT!w&[
7Rn
!7
\7%!/!Y>!g]H_&&L6>>]W76]&!Lc]W7!g7w6U\&L
7!
JhnL

!]w7HL%!LY>]!/7&L6>J]!6hw/L7\6>]+wD6_RTW
>>6w!/7!
!7w6h\&L
_
Lwg]6]HL_/7!.LY>]!.7&L6>>]D[7!._[7 76]9
\h~RRWa.{RHL
h%Z]\F fi\BwcZ>{wYHaHYJ&wYH
\
ff fi
!"#
wLZ9s/L%$'&!()&>*\\ %gh/L,+-$./&>0\1$-2hLYwY\YB{ 3 4\%\
657s/LXB8(19$8ffBw%&_ H:(1$;<~L;YwY\~B8(B=9$>ffNBw2&_ H(B=$:?57[wW
@$'&_F/-\1$*A B680\1$C"#!DFEGHEI JKHEI_wL.677]!X_W&L75Lg
wwB%]?M5CgwLwa]CN569 7!/!&L7.[whwwB]4
O=O>PQ"# ).N &

R n_wLF]9!7w6SFL!6]!n6>]?T L76]!s79%7!nL]wBR7&LwDs%!ws%
9B9_wL!7!/wwB%%U\ H_!N7w _&Lw]]h\!w]R
Ly_!/H%]7!6
!fi6w6]]LHB[wL.7[%77 [9
>>79B]9%F]9_!nw &[
7R
V 7W _JJ]
U%7!c7wL_!U[H66h!h]6]B>] 6!L 7!a66]LWL
!!
!7 6% L_wL.
>>Xwd7H[&%]7Lfi7!66]L% !+L%H
!L
6!Lfi"XF
!n7s! ]wF
>> 7!72Sy[!6!/x6J]%T L76]!/]7!
_w6
\&L
7Rng6&[77 w%]\6L7L
7w6YSFL!6]!n6>]TR[76]!s7
/SyL!6]!n6>TRL76]! s_ ]Wn67c7!H!{/LT]HxG L]H!0
Lwh7!6!N{nL]6&]I L]6!BP_ 6 !Hs]7!.]/L_!.7wB
7 7sH&[]T
!L6]!LF7!/6w6s%w
n]7!/wD<SyL!6]!n6>]ZTRL_6!
\&L
7R
[ 6\77!wL_\TR9]w_^h6H!!Ld\.!6&L_9!7w67SFL!6]!Wx6J]
TRL_6!!79xLZ%Z%hZa6]!)] ]`]w
7! 7 7] !&B]!)H
TRh*]w_^/Law/7H! Hw6]!a+_]6 LwT!xH &s ! 6!

6!~]Hs/!&B>]Hg77]!!nL7_H]g7.a+_]6 Lw!]w. L!!n
>>!
wL6]7a!_ !6B>9L6W]wX7!9%!&Ln Lh//
Hs%!
w\76Ln\ ];7!]LX_L77 Lw9BsJ>nw H7]6]>fi
97s7
!&s%7ca+_]H Lw6]W9[9!7! s%wLw%!N_!]LX_!7L7
6_]c77
77!w[7]8TR9b]w66n![ \n!7w6CSyL!6]!n6>TR[76]!s79

!
76L!Lw76R]h
\7!_w6/\&LgZ%7nF]>L~7
]& 76L!].7!w&[
7R
c 6]6]!9_!.LwL/
/!D
R 6Bw \!fe =(Bg!B6._h9ff*$
D%\N\Bw
>w{6H>i wYH4
\
ffUj)kZ{YH
c/lm0BBwn\o1A0B/$M+>c_U$qp2hD>Yw./0B\'&>.r
sts

fiuv-w.x-vIv-y-z7{P|:y-y-|y-}8~-;8

X 1X_!o;8>86 >G
M-;- _' ;!99_-U-; 8_Wo,:8 ;-!Z- :92- ;2;- P=n;\;8.*-%.: '
_;-29-9; C-!9.8-8)_9_X)\@@;q-P;8;- ;% 'X;8 - .;
9_8
MY); ;!q9_-I!;-;.M;-*)_9_. ) 9=4;8.*. '**_)*-U>*@=.
_;-2'
8*;-_)_9=
9.;-2:;- .: '!2 8-o-)-*2)@_<!.;)_-*;/
-.7;U-% 9\_)o6.. ),;9_!n;U) :2_n'
-2)@_M!;)_-W8-,;-8 8
_
.,;-< )_;-b '!>.8C.>-2;-<:'
=2.8b_)_9=
P9.;!.:;- 2. )
: X_!oXX18 H*'@/o!:!>P@8>=)*'=@4X:>=)* ! /!=%*-)
8`U
4! M2- i)=,%9 .'

' @ //8 !
8'!=,M9
i-=,;-4=>=/1) //<9%!:!>=\<9 ,.
8
=9>Z
8 )!1
.>./ @2:!/1- ' <

=%.=>=/1!>!198*!>P;'
=8.8 * F=M
@'/9- 1/=4 '>!/18.2) ; ;*_-!!;68M.P9.! U. 4.)8 8
_
.89._'9_7%;-C. )b)!' ,;-6--Z:89 ;)I9.!U%;-
. ).8;-<- ':_;- )%> 8
_ ,Q .



m. )Z8:4_.2'.8*)=.7=G)!=_;; 6;8.*;/ *:)7G;8.
. ) 89 ;..)_Z9;! ff

-Z) fi8)_9_8M.Pb) ;U.8
:8_/-) .U; -;!q '9_8
.;9;:'9/;2.M -
@ !.9_8n.;-_P) fi8)_9_8n/-; 8)!=.-)_-U2)@_%!.)-
9-9;
7-:_9_X\;9-;8.4!L. )Z .79;<_->._'-:*=
@7.'
_)_9=
X89 ;..)_?9.;! .8W
Z8
-.X8._)-*=
@_'*'o#_\*_'\8M; :)!
;*;)_
2 ;-_!; -!6. )!*-
'-.28.\;-<8,_)_9=
9.;M.X;-%:;-
. )M= 8I28.<;-b <:'
P.;-;- U 'M=
Y.;4_' ;!9;! _L;.9=;:;
*'@/o!:!>@ P ;_-<8

:X_!oXX
_ **)_9_. )%o-)-6)_*!.;)_-C9-9; C-6*)@/_9!
8:
=248
_.9 \.)o8 --<9 \:!:;6. )!, ! ) -;<;-<9 .)=.82.: '
;`*)_9_. ))=.=M:/_1:!/@X/! .: ',.8*/\!;8);_)_U'#"..: '
;:;- ;=M)=.b
$%!_9);8.P)_ !&
"W9.;9_-M/; '48';)_M_)_9=
9.; ':_!;
.-%)=./_6;-<;;!9>8:_-'
$%!)(
n8*_)_9=
9.;2.;-%:;- 2. )!'.8b; ':_!;
.2;-*_)_9=
28 8
:_*.2;- )_;-b '!MQ+ *m!1!>:/_1:!/IZ)@/_9!
8n@o4
;.qo;6*)_9_. )<)o 8.?:8=9M.P)=.8i2)=;L;Z)!=_G; !M.P8)-*=

) -;X


-Z.>:U) fi8)q:L .-;-;!_')q:_;8.9_8.P=.-)_-C)_!.)-_I*)_9_
. )Z):
_8 G<;9-b/U8.)_6;8Z- ; ;96/!4;8U8
;C.b_7;-
-'9q_I )_;-b '.,!9_.--/
- m9.;b._-G10.32L\.87- !;G!;7-
)!99_8.q:8U)5
4
26 2 ;-_!; ,;- .G-48C9-;.8-*;-b 4_)_9=
_ .q:
.,!C;- M.86.8:-%!6;- 7 2)!99_8.q:Xff
Q8-Z.8) %.; -;)*_
;8.2 q=\;) :oqUU9; ;/)=.68.9-:)o><)W:X)
= :)_9=
#- .9_88.8
)_;-b '<> 8!:_

)=M9; /)oC=i;-Z;9=:;;6*)_9_. )<)=.I4_)8 6/!
9=U;82. )\.,-Z.: '2!7 9X!2;-U> 8
_M.,;-U9-9; .8;-U!9)_
:7;=<?> @6ACBEDGFH> DGDIBEJKMLK A3@GNHDG>COEPGDRQ7JSOEAUTJ@RLSKWVXFJK DGPYLKWP?KW> Z\[%NH@?JST=L]^NHKWPGDH;
_^` `

fiacbedRf ghbhbhijb#kml3nhif oqp#o8ghrhbhijb#k
s%tus8vSwhxWy{z}|8xW~vW sxWxW~vW6vx z8vs$HxWxcvSw#z%v\Mz%ycSxW~#z}yHsshWhy~vSwhxcvSy^z}h'z%YWz%ySx
x#z%!xz%~#~q'z%~!'s8vwhxWyc~#z%vShy^zHvxW'W
x'~hs%whsvSw#z}v$s8hys%6I)~hxvSy^z8v^z}!)vH5yx H!vMWz}~5xxvSxW~#!x evsvwhx'!vHYz%|8xW~!v
Wz8Hx$zffffxY x3ff))#x$vSwhxUtGs%)s~h|vHffs$xWz8W
e6I#cI 7 !%WH 87G)Gj #&{)8#I?I)$68#IW WG \?7H
8W $!%8#)8878' 8.+ $W8GjW87G8^e)Gj U=)8mW8WI 7!
IC$ 8$CjI\I HCjI!W8m+ #W8GjW8 G^)Gj #=)8I8c8
7# 8I8)%#8I8u8W8 {'Ih8)86G%
$7u# ~vSw!ffWz8HxUx z8Swz%|8xW~!v\!~hs%vSwhxU|8sz7us%t6vSwhxsvSwhxWycz%|8xW~!v z%~#whxW~#xUvffffx z}ycvSw#z%v
vc|8w!vcx z%y~s8~!t+z8vcz%#shvcvSwhx3#s!S!x$~!vHz7\Hv^z%vx {z}~##xWw#z7s8y^W
8xW~vSw#z%vvwhxWySxs8~!zms%~hsz7~!h#xWys}tsS!xq~!vHz7$Hv^z}vSx z%~#exW~!8ySs~h
xW~!vc#xWw#z7s8y^W#z}~#|%8xW~vSwhx3s%~hsz7s8h~#s~vSwhx$!xWhvSwqs%tRvSwhx!z%~#WhvSwhxWyxz%ySxUs8~!
#s}!~hs8z7)m'z%~!x 8hxW~#x Xs%ts8#HxWyS%z%vHs8~#Yx z8ws%tXw!jwms%t#s%!~hs8z7?xW~h|vSwu{s}tx z8w
z%|8xW~!vUvSw#z%v3z%ySx$s}t~!vSxWySx v$zff~5uxW'zE!^UcxW~#x8uffxWz}~xW~#sh!x8u~m#s}!~hs8z7{#z8x8
z!x js~v^z%!xtsycx z8wz}|8xW~vXxW~vs~!~h|qs8~!vSwhx HxHx hxW~#x W6z%~#mwhx Suh~m#s}!~hs8z7
vHx8hXwhxWvSwhxWycv!xWvxWyS~hx zSz%vHtYz8vSs8y!vYz}|8xW~vU!z%~6
e6.cI 737}j H8GU!)GH 87#{)8#IX?)ff68#IffW7G7.=?7H
8W 87#%\}^W )\878? H\mj?8)%#8I8I 883IU8 G!8hHH=HC7#G8GI8
WWH%\I HW%WI$7%Wj 87H8G'!)GH 87#ffff##I?IM8#IW WG RUY?7H
!%WH 87GHGW GIm87GHHuC7#G8GI8jqHI!^)8I#87WG $II
!#I !c8 WW8c8W'78 I6WCMI!8I H3CjIc8G77#3GI G8$!Gj78 hu
$88#) $I HCjI38Y+ W8GjW8 G^!)GH 87#=)8mqc
$7u#ff ff)#xffh!vffz8utGs%)s%Wffwhxffs8#HxWyS%z%!xHv^z}vSx us%t=z%|xW~v~ ff))##xffvSwhxcWz%yvSx z%~
hySsh!#vs}tRvSwhx$s#HxWySz}!xHv^z%vx \s}t&z%|xW~v~ ffvSwqvSwhx$HxWvcs%t\Hv^z}vSx W
C! HH^%W 7%7 S^%W7 % % 'ffwhxM~!vHz7Hvz%vSxs%tz}|8xW~v$~ ff))c#xv^z}8xW~vSs
#xvwhx'#z7ys8~#Hv~h|s%tffv3~!vHz7v^z%vSx~ z%~# WH7 z}~#mv^|8sz\v^z%xW~vSsxvSwhx
HxWvs}tHvz%vSx ~cw!
w zs#s8~hxW~!v 1ffwhxxW~!8ySs8~hxW~v~ ff))xzWz%yvSx z%~
hySsh!#vs}tvSwhx3xWw#z78s8y^ff
~
ffvSwqvffsHxWv^ fi3z%~#
fi%hcwhxWySx fi w#z8 HvH~#v3xxWxW~vW

% cw!Swqwhx$#Hvffxx hvx
|8xW~!vcff))w#z78x$zjvH~h|8!Hwhx mz8vHs8~6#Wz7)x %W %!
~ v~!vHz7v^z%vSx8.ffwhxHvz%vSxvSy^z}~#vHs8~ th~#vHs8~1ff))$#xmz8M~ ffhhvcwhxW~/#xWyts8y'
%W 7! % v^
~hxW s#s8~hxW~!v ~vSwhxWz%ySvx z%~5hys!#vHz%~#s~!vW3ff))Sw#z}~h|8x
vSwhx$w#z%~h|8xff)?#x$vs %W %! )t{z%~#s8~!)t\vSwhx3hySs x vs~s%tvwhxU~!vHz7\xWw#z78s8ycs
~ fi j
z%ySxUvSwhx3#s!x|8sz7&tGs8y{z%|xW~vc~ vSwhxW~vSwhx
~z8hvHs8~6#z8Shxvw#z%v


vSy^z}~#vHs8~tGh~#vHs8~~ ff)){Sw#z%~h|x3vSwhx$~hxWs8s8~hxW~vUs%tvSwhx$s#HxWySz}!xHv^z%vx3vS!s )t
z%~#s8~!)tRvwhx~hxW.s8s8~hxW~!v{s}tRvSwhxUxW~ySs8~hxW~!vj{~v^z%vS"x hz%~#zHv^z%vxSz%vHtG~h|
w#z8ffxWxW~ySx zSwhx u
whx\z%#s%8xvSyz%~#tGs8yS'z%vs~UtySs vSs z%8x uvSwhx?!xW~!vHvH$s%t#z%~$z%|8xW~!v E|8s!z7zcs8#s8~hxW~!v
s%t6vSwhxff~!vHz7)h~h~hs%c~#xWw#z78s8y7\cs%ffxW8xWy 8z}|8xW~v\?z%~#~hss8vSwhxWyffz%|8xW~!vff)6s8#xWyS8xffv^&|8sz7
z7tGvSxWyv^$ ##y^Hvcz8vHs8~6?v\x z8HvSsHxWxcvSw#z}v\vSwhx3z%#s%8xXvSy^z%~#tGs8ySz%vHs8~8xWxW#vSwhxHhHvSxW8#z
sh!xWy^z%vSx8z%~#vw#z%vvSwhxWySxxHv^zz%vHtYz8vSs8ySm!vHYz%|8xW~!v!jz}~e~ )t$z%~#s8~!)t3vSwhxWySx
x!jv^#Swz!z%~ ~ hcwhxWySxU~ x z8wz%|8xW~!v{w#zffs8~!s8~hx$sS!x$|sz7Y
%&'

fi()+*-,+).)+/+0 132 /+/+2/+465+7986:

;=<>!?@BA@CA+DFE9G+HI-?J<LK HNMBHO>P>QIR"S"HTDHOEU
V!WYXZ\[X6]_^a`cbYdfegihjklnmol-pOgCqsr!tujvwl xyjr!ozcxyg{qwl| jkJx~}zclk6kJgk|~gzcjNajl vkJgk|TpOpxyjr k6uq
gik|Flk.jygjk6xr!ozcxyg{qwl| jkJx3pOlxyg{pOlxytv\zclk.gCpts qwzcgkJj!xyvwl xylzj-
G+HI-?J<-KH+<<-RsG+<-SR=E9G6I-E$MI-A+A@CA+DQSG@cMCHHI-9A@CA+D!@R"<-aiMc@CA+HTE9IEI?MBHN@CA>!MCEs@CI-DHOAE
IRsHR"Rs6GIRE9G+H< A+HRHR9s@C?6HI-?6<-KH= @BK HOAQE9G+HRE996E9+9H <-E9G+HTI-?6<-KH MCHO>n>QIR@CE"@R=HIRs
E9<+<LK HR@C>!@cMI-9HRsMCERy<< E9G+HO<AE9H+ERaS G+HO9H3E9G+HOH@RI"6<MBA+<>!@IMcMC!?6<+A6H+A6HO9EI@CAEw
I-?J<+EIT>!MCEs@CI-DHOAERRsEHO>+<=HI->PMBH@~SHNS"<M!Mc@CHTE9<6A6IT>!MCEs@CI-DHOAE"M{IASG+HO9H
IRGI@cMC+9HR=<-IDHOAERa>!@CDGE3<+O+@BAPE9G6I-EOIRsHE9G+H=I-MCEsIDHOAE~>!@CDGE3A+<E3I 9G@CHOKH"@CERD<IM
?++ES"HN9H@C9HE9G6I-E"E9G+HT<EG+HOIDHOAE"S"@cMcM3RsEs@cMcM?6HI-?MCHE9<nI9G@CHOKHN@CERD<IMyE9G+HOAS"HOI-ARsG+<-S
E9G6I-E"E9G@R +9<?MCHO>@{R <-aiMc@CA+HE9I EI-?MCH+6R@CA+DFE9G+HI-?6<-KHTEH9G+A@ +HRO
n.6JJa
IsMCQS"<9@BAE9G+HI-9HI<-MI-A+A@CA+DQSIR"HOK< E9HFE9<nK-I-s@C<6ROIRHR"<-MI-A+A@CA+DS"@CE9G<>PMBHOEH
C@ A< 9>QI-Es@C<AFwRHOHMcMCHOAa+HOA6 MCHO+ IE9H++< >QI-AT6I-6HOR<A!E9G6I-E~E9<@~TR~9HRsHI-G
@CAE9G@RTI-9HIF+<D9HR9RH@CAK-I-s@C<6RT @C9HEs@C<A6R$RHOKHOIM~@CA6HO6HOA6HOAES"<R <?6RsHO9K H.E9G6I-ENE9G+H
IR9R+>n+Es@C<AE9G6I-EI!MI-A+A+HONG6IR< >nMCHOE9HT@CAy<9>QI-E@B< AF@{R+A+9HIMc@RsEs@<">QIAR@CE96I-Es@C<A6R+E9G+H
Rs+?+I-HInE9G6IEE9HI-ER"EG6I-EIRsJHE<-3MI-A+A@CA+DF@R6R6IMcMB9HyHO99HFE9<QI R"MI-A+A@CA+D@CA+A6HO9EI@CA
E9HO9@BE<s@CHRO
I>nMCHR<9HRHI-G@CA E9G@RnRs+?+I9HI@BA6MC6H.S"<9< A6HO9A@CA+DA+<-S"MCHDHIA6 IEs@C<A
w<<9H~+aNIMC6HOAaJS"<9<A<A6 @CEs@C<A6ILMI-A69HIE@BK HnMI-A6R!wNHIAHMcMC>QI-Aa
-IA6S"<<A@BAE9HOsMCHIK @BA+D.MI-A+A@CA+DI-A6HH+E@B< N>?+<RsyA+D HORs<AE9HOHM=
G+HT9HI Es@CKHI-++9<I 9GF@R"+9< 6<RsHIR"I!E9<<-MY@CAE9G+H<AE99<M\<-9<?J<ER=<6HOI-Es@CA+D!@CA+A6HO9EI@CA
HOAK@C9<A+>PHOAERI-A6@BAE9G+HffHR@CDA<-9HIMCiMc@cH< AE9<-MI-G@CE9HE9+9HRFEG6I-EnS"<M?6HI-?MCHE9<
9HIE @CAIFR9I-Es@RIE9<Q>QIA+A+HOYD-@CKHOA.+A++9H @E9H.HOKHOAERw=9<<+ROa G+H@BAE9HOsMCHIK @BA+D
<-3MI-A+A@CA+DI-A6HH+E@B< A>QIRs<>nHOE@B>PHR?JHQIn6RHMIMCE9HOA6I-Es@CKH!E9<<A6 @CEs@C<A6IMMI-A+A@CA+D6
<LS"HOKHO@CA>QI-AF9HIMc@RsEs@n<>QIL@BA6R E9G+HO9HN@RTI!A+HOHE9<F<A6R@HOI!SG+<-MCH<MI-DHT6<Es@C<A<-
I!MI-A?JH<HH@{ @CA+D<AIAIEs@C<Aa G@R=@R"EG+HOIRsHN@CAE9G+HTEI-A6RsJ<9EI-E@B< A<>FI@CAI-A6FE9G+H
E9I+>QI-OI-9H!<>QI@CA.S"H @R96R9RHYHOK HO9E9G+HMCHR9ROSH!RsHOH!E9G+HT@CAE9HOsMCHIK@CA+D<-3HH+E@B< AS"@CE9G
~MI-A+A@CA+DSG@cMCHHI-9A@CA+DI+9<>!@R@CA+D @BHEs@C<A<=y+E9+9H9HRsHI-Ga
HRsHI-G@CAE9G+H @BHEs@C<A<-<A6 @CEs@C<A6ILMMI-A6RPHIMRS@BEG M{IA6RT@CA SG@GEG+HQ<+E<>nH
<-"E9G+HQI-DHOAERNIEs@C<A.>QI.IaHETE9G+H!A+HE!IEs@C<AEIHOA?.E9G+HnI-DHOAE G+HO<9HOEs@OIM"S< 9<A
E9G@R @R9Rs+H!@RN>QIL@BAMCHOK<EHE<IR6HERN<-"9HIRs< A@BA+DI?6<+ENA+<-S"MCHDHFI-A6.IEs@C<A_w<<9H
+NIMC6HO9Aa! +T<9DHOA6RsEHO9AaT IA6E<E9G+HMC<D-@OIMTy<9>!MI-Es@C<A<-<A6 @CEs@C<A6IM
MI-A6R <RsHOA6R9G+H@CAaT- JH@c\.>nHG6I-A@Rs>QR!E9<<A6RsE996EF<A6 @CEs@C<A6IMMI-A6R@CASG@G
<?6RsHOKLI-?MCHFHOKHOAERTI-A6.E9HRsERTI-9HnH+Mc@{@CEsMC HMI-9HI-9HQ @R96R9RHIRNS"HMcMHMcMB>FI-Aa"
G+HRsHNIR~S"HMM\I R~E9G+H">n<9HMIR9R@OIMYS"<9T<AQ< A6 @BE@B< A6IM\MI-A6RI-9HOAa+LLI-A6S"<9E9G6I-E
y<-McMB<-S"HIA6H+E9HOA6H@BE @CAK-I-s@C<6RN @BHEs@C<A6Rw~HO<E>!@CE9Ga~+ E9@C<A@aI-A++RO+HM{Y
NI-JHOYHRsGa@cMMc@I->QR<AaYYG6IKH"A+<E<A6HOAEI-E9H<AnD HOA+HOIM6<>n++EI-E@B< A6IM\IRsJHER
<-$$MI-A+A@CA+DQSG@cMCHHI-9A@CA+D6 +S"<Q<HRA+<E<A6HOAEI-E9HN<ARsJH@\>nHG6I-A@Rs>QR=y<"E9G+H
<A6RsE96Es@C<AF<-<A6 @CEs@C<A6IMMI-A6RO IE9G+HO @CE< A6HOAE9I-E9HR< AQDHOA+HOIM<>P++EI-Es@C<A6IMaIRsJHER
<-< A6 @BE@B< A6IMMI-A+A@CA+D6<>nH9HHOAEnS"<9G6I RnIMRs<?JHOHOA<A6HO9A+HS@BEG<>n++EI-Es@C<A6IM
IRsJHER"<-<A6 @CEs@C<A6IMMI-A6RO?++E<A6HOAE9I-E9HF< ARsHOK HOIMaA6I-E9+ILM++A@BA+DP9MCHRE9G6IEOIAQ?JH
6RsH@BAE9G+H<A6RsE96Es@C<A<$<A6 @CEs@C<A6IMMI-A6RTHOA+HRsHO9HOEGf <+9?6I-G6RsGaa


fi+++{6.T+C++{6


fiff
ff
ff!#"%$&$(')*+',-ff'.ff0/$%'12*'$%
3ff+#'/546#.ff'$%'17+#
'89$%':;ff'<
ff7#'/5589$= >46#?ff@'<+##'ff6 Aff'B:;4 AC4D+E<F#$ff<>G$9ff/+$%'1HI /ffB#'J$%7$%

K#L#$?LM$%ff7+#
>,N$J$%'17+#
'89$%'JOK 1P*L#$Q46
A<MR3SUTJ$?ff0
$>8Gff#'#V'$%
'$<W46#
/$%'$%
3ffXN89 3ff)*+'ffNff M$35,9YZAff''#'/[4 #$]\;$ff
'#'/:(ff0'<^AffK,_`YaAff''#'/b4c!#$
\;$ff
'*'/@Lff$<F'$%7$%
ffd89 3ff)*+'ffa%ff$%/+
#$ ff'<@
$%
$$%'1ff#'51 M$%S
e '$%
g//+$$<(ff0
1ffh,i
j Eff0''*'/h#'='$%
3ff#'9$%'7+*
'89$%'3%:4 1)$Dff A%ffLA
f
ff
+$<fik#$6ff6$ff$<fi<+AK#'=
$$%'1)*:Aj
$,_$%

$<f ff;'#7$%
ff Eff0'ZOl1 $%
3m:nopqRS
er'#7$%
3ffD Aff'QAh'$s*'Q4 At$G
$ff#'J,f$Gff/$%'($%7$%
C 1P*L#$5$%7$%'=, $
$%'7#
'8s$%'1&Efi) $u$<Q$v A##S
fi
$#3fA0Eff0$9/$%'$%
3ffXDEff+$f, )$%8Gc#'J4
$ff/+$%'1HI9ff#'9%ff0']L$> $u$<w$v !A##[#'bff0't$xy#$%'1@8Gff''$%
:z#'w
3<$%
9C$%'ffL#$
ff+8GffA67$%
)!u{%ff0#'BS-|
$%
89
$:1$%8G}ff6<('},~ffM*'=$ff0L7$fiEff+$z8Gff(LM$
#'1
3ff3ffL#$h$%7$%'@,-$(ff/$%' ff +89 #$%$fi#',_
8Gff)*+'5'5$fi$%'7#
'8s$%'1fL$%ff7+*+

$%
(89$%4 ff0fi
$Aff$<>46
FA('$%
'$<46#J Aff''#'/>
$f4 $%
$9$9/$%+/
3ff
A6''X4c'>OPYgff ff<+#8=#
#>-ff''ffffA%:jnopo>%<$%
89+ fiff7+A%:;nopR3Sd|
6$vff8s *$:
'$f8Gff9L$f#'$%
$$<=#'5u'<+#'/Gffh
+$ #$ff<+#'/=,_
8U'$fi#G=ff'$%
646#+2ff+%$6
ff'Gff0

Aff$c8Gff BSd2Ez46
h8Gff(LM$ 7#$%46$<@ffgff( M$Affd%ff$6,B$ /$%'$%
ff,i
ff89$%46
fi,
YZAff''#'/=4 #$\$ff
'#'/SZTJ
h'9$ <$K#/'s,{ PE%ffX{ ff
g+
#$%'1$%
zOL$#3%:+ ff'ff'<+#$%
3R
ffgff%$% gff'=L$;#'='$6,{$%7$%
3ffX1 1K#L*$f
#$%'3ff#'Zff'<& Z#B#'9ffc
$<$%$%
8=#'$<

#$%'3ff#'>Offff
3ffff'B:MnopRg8GffGffXE)9L$h7#$%46$<ffcff9 $Aff-%ff$fi0,a
2,i
3ff089$%46
;S

z46
fA6'$%
'$<=46#s$ B#'$ff'<'*'$f
3ff+3ffL*=,BYZAff''#'/=4 *$fi\;$ff
'
#'/SU6A5
$Aff$s#5t46
]'$%
'$<[46#W$>
3ff3ff0L!#],9<+$%
$%'15 $@,= Eff0'
'#'/Og
:;ff0B:UlL
3ff08Gff'Aff'B:Znooz+Eff0'<$%
:gnooR3S-6Ac4D+
98Gff#'#.+'$%'1
ff$<
'w'*'$C
3ff3ff0L!#t,NffJK#'/#$%ff/+$%'15 Aff''#'/]42*b89 #$%$#',_
8Gff)*+'BS
@4D+

'$%'
3ff$='t/$%'$%
3ffXc+89 3ff#'ffXff M$3=, Aff''#'/t46##'89 #$%$5#',_
8Gff0#'B:
'KA<$%
3@ffA>8=##ff/$%'GK#ff#'m:Dff'<t<+A$9LMt'#'$.ff0'<QB!#'$
3ff3ff0L!#S
$%ff6fffB!#'$5<$K#/'?ff'<
ff3ffL#P:Zff#/J'PE<$%
$<?ff'Jff
3ff#7$h #'QOP>
Dff
1ff$%:gnooR3:MfffLM$%$%'ff#89fi'$%/#$$<#'Q$=
$$%'1h$ff
3&OL($%$FOP1$fi
j$%''$%'#":dnooMlff8j$%''$%'#":dnooRRS
$$ff
3?+'J#',_$%
$%'$5, u'#$%ff8Gff3ffO #7$@l1ff #
$: no+pq1:-nopoRff89$=ff'
ff /$%'6ff6
#$zh#',i$%
f$fi

$fi,aff'5ff+8Gff'BSZ6$(ff/$%'zA6/#7$%'ff&*8*$<.ff+%$

Q$.ff08Gff'B:Dff0'<tA@$v $$<wQ/1ff#'w$%'/w#',i
8Gff#'w<$<$C$.8s *$%$


$@,$ff8Gff0'BSJz?'
3ff):j*'w$@,_
3ff89$%46
J<+A)$<Q#']A= ff0 $%
:z$
ff/$%'fi'$%$<fi+'*>/1ffX*'#',_
8Gff)*+'Jfff46A<>$# >#'?
$ff+#'/5$9/0*7+$%'J/1ffS(2$%
$,i+
$:
#'54 ff}E2
LffL#Gffh891)-'ff
ff%ff)$:$fiff8@ff'=A},ff#
#y8s !A%ff$<;:g#$ff
'*'/
#3=89 #$%$

$=A9+89 3ff#'ffX!#>#',_$ffK#L#$S?c4D$%7+$%
:ZL$#'/J'#>#'$%
$$<C#'tff
M$!u{/ff:g'$58GffQL$ffL#$.+L3ff#']$5'$$ff
C#',i
8Gff#'B:6ff'<t%$%$<*'wff
/1ffX~S>K']ff<<+##'B:z4D+
>'+89 3ff#'ffXD#$ff
'*'/Qff89$hff0N$G/#7$%']ff08Gff'E
,_!#t''$$<;:Z$%'ffL#$5
$ff*'/Jff'J3ff$N0,2$yff8Gff+'ff0'<>$*8*'ff0#'/?$s'$%$<
,-ff7A<+#'/3ff$D,i
+8r4 A>$%
3ff$ ff
$h'
$ffffL#$S2Efff)89 #'5
ff0 $
ff+8Gff'sA6,_#''$$<@8Gff97$%
@46$dL$&,~ffA$&#'8Gff'9
$ffX*,_$ff0 !A%ff)*+'%S
2$D ff0
g,{+
g4D+
(4 AG<+A$z8=##ff/$%'g Aff'ZAg
$Eff0$<=fA$a#'G<+A
#L$<
effiOPz'<fiff$%
:jnoppR6ff'<G$N89 #$v#P,-8=##ff/$%'fi Eff0''*'/COPj$%''$%'#"G
1$%:fnop+oR3z4D$F#'17$)#/1ff$$89 3ff0#'ff<+xy#]ff0Gff
A$@<$C'$%
3ff#'P
'$%
'#'/@$Nff+#7##$c,}ff0'ff<<+##'ff-ff/$%'%OPR3S
3

fi>t-+

fi2~f E0%%3#f=%;;h9a6(%1fAf+%%1f_
AKA% %%1#f#Q5##N01c6P2dKfiA=APE%XccA%Gg+%1s%G
9PcG@UJD3929%%3 #Q%6%%?A#JQ
02%Q=%%[+A#]%+#sD+tPf]J#GBfi3JKb+##B
j%%#?[19)XC#[_+1#_%%A9A59.+96
+AsJ5sAKA%=1 66#>%%3)*+@Pj%%#J%f
6%>V9E%X 1 6P2*+G%%1#.%0.M955+K#_9%
%)%130#B.c6%%Z51#_%%)Es%)%130#hD5KA%t0y+%
#J+%%1&i+APE%XDA#>9A%g95h~fi%.s-E##
@1K#*%#h%3hJGB3h#h#%]+B%%=%1+#9%
%+#3%%-G%%D+%%3;~+36 @%#s%1z6N3KfiN_I
30 d5%cE@Az+%%30%1-+A)s*5fZ*%3=
+3M6! fiM9%)%1A##Q*C(%%1#J>6 9#h#9E##
hh %B+#gh%#%gc%691g+ADP*!Az9
X2*+fi%%3#@#}E2(K#50G961%#BB+3g +Afi
#5%%3=% j*%d306y0%16=#69A6+%1Aj*5fi1=%6
#36)%X#930%
g@GBzzXKB-
)igA#)%%c@fi%h%Aa%)*m
j**fiA>#
=9AU_ fi%%3#50aA%2;#)A#9Ah ic%%
%#5A%=6#3;;#N)AJ#J> y#%1h90E)ifiN+% {%#J-A
6_6)*@%+A=A%
K ?E%=6y+%19QA#J#t%#t%#dc%Gy+%1h
#JA-#_G)*+?Q9%+*9%NM%#J50f*hAh#330*@
At5_DA#?%%+%J_fi9%3s%%3#_ #j%%3##
AJ9=%fi6%#yf>K##GM%+#3cAfiM#1+=A-#J@
%)%130#GK#%3-c6%%+-K###36]}#gAz1P*##59%3
+K#_9%3h%%3# %fifi=%fd)%X#930z=#1c(+%
A_3+(AK_=

3h+%#z9%#9Mg %B#K#s9
3+3#6+A>%93#h%

A=2!66+KA%(P*_s%309)%Gfy#%1fiA%;cE>Afis91
c
K#0#B6#3D*s*@ aE0*@ #ff
;#sAD+-5*%zA#
6#s*%f*_@#B
fizfB#fi330*fi@#fi#3#- 6%%0#
9%39%@%;A#.2*?9#%=#_G0#>A(+#9#+AEcA&G%=
303>fc{0iM
6#GBg
3+ #(#%)fiD()XWaE0*
#

;0#=A }_3;gf%%3#6f3#5N+9#%(AK {%#aaE0*
#

#N%@B)=9%%3X%%1#9g=9#{%%#%
+##Bfi6?+A%K#50ZA#[ #

#(V5aA#bc!#

;*s#=##%+G#%
2=i9%6ZA#J #

;#A=5%%3X-_39%6 %@A#>#
%3X*t%#Q%?M+#;t6s*#tfAhi9%6;gJ5A
M1K##N0@%1#-#3%*=



fi!"###$%'&)(+*#$,-/.'-0"#1##$%'&
2 304 57698:<;>=7?@;>AB;'5DC0E
)
FHGJILKNMPO%QROS,T0G UVKWUVX'Y[Z#T]\+Y^ZFHGO_Qa`@Y[Z'QRUVX#bVGcG Y[Z#K0ZPdfeWK0M'gbhGci0S,GcIjGcbgc`9klKNbUVX#GS,b+X#GO,mPknMPOLoK0e p
e GcZPUgcq
r);#sV;'tN;'573#;>E
u X#K'` u q>vwq,`@xKNmyobVK[knU`az#qavwq,`y{}|~OOeY[Z@`!z#qP#q@f0HN' +>0c%ff70yPn
+00_Pffcq u Q#QNS_gKNZ#pnFHGgO,Gcd]M#POS_gXPS,Z#7K0e m'Y[ZPd0q
u OO,GcZ@`az#q,`PxGcZ'QNO,Gcb`az#q,`#{!Y[UhG0` u q'7Q#gcqq#N0PqDL00f^_RL0>'f0qa/K0bVfY^Z+Y[MPkneY[Z#Z
M#POS_gX#Gcbgcq
u e#bhKfgpn<Z#NGcbgK0Z@`#z#q,`#{0UVGcGO<`a'q>00PqaZfUhGcbO,GYiNS,Z#O_Y[Z#ZPS,Z#'`'#GoM#US,K0Z)Y[Z'Q/K0ZPS,UVKNbS,Z#'q
<Z)LN_P[+V0^`fm#m@q>009q
K0Z'Qa` u q!xJq,`!{YNgVgGcb` q00q/L00f[_+,cnlfn+n L_N >n0'q u PO,G
M#POS_gXPS,Z#)7K0bVm>K0bY[US,K0Z@q
bVKPK0Tg`0+q u qP0NPq u jKN#M'gUL@Yd0GcbhGQ K0ZfUhbVK[Oa0dgUhGceklK0b7Y+/K0PSO,G+jK0'KNUqaV70P'0
+Lfh0ncff0>fn0w0n0>`@0`@c^0#q
d0O_Y[Z'QPGcb`+qjN0Pq7K0eWmPOGPS,UdjGgMPO,UgklK0bR0GcbS_YO\GoKNe m'KfghY[PSOS,UdaqZBLN_P[
+h[0`Pm#m@qa[0'^'q
\GY[Z@`[+qc q,`{FHGOO,eY[Z@`0qc!q^0#^q[L0>'f0'J70>nNq[/K0bVfY^Z+Y[MPkneY[Z#Zw M#POS_gX#Gcbgcq
bhK[O<`fwq,`PY[M@`P\wq,`0{0M##bY[X#eY[ZPS_Y[Z@`Nvq90NPq>ZUhX#G+7K0e mPO,GPS,UdwK[k!\K0eYS,Z#pn<Z'QPGcm'GcZ'QPGcZPU
O_Y[Z#ZPS,Z#'q<ZHj0f[+V[N`fm#m@qa0#00#q
UhSKNZPS`ffq%`yxY[Z#T#gc`@'q%`#FHGO_Qa`@\wq,`'\bY^m'Gcb`a\wq,`'aGgX@`@ffq,`>{FSOOS%Y^egK0Z@`@qD00q u Z u m#p
m#bVKfY0ohXffUVK+O_Y[Z#ZPS,Z#JIjS,UVXffZ'oK0e mPO,GcUVGZPklK0bheY[US,K0Z@qP<Zj0_P[7f0 0cc>
0/L_>yc++ff'0j0JLycc>n0n_N0> j[c0>_P0`Pm#m@q@0y0#q
GcZ#GgGcbVGcUVX@`@q,`'{K0M#bh'Y[TfX'gX@`aqa+q@00PqS,e G fYiNSZ#LSm'gknK0b+ bVKNPOGce0K[O,iNSZ#/IjS,UVX
<Z'oK0e mPO,GcUVGJ<ZPknK0bVeY^US,K0Z@q <ZL0f^VV[0q
xYO,m'GcbVZ@`ffz9q,`L{KPgGgc`wq0[9qZ#KI~OGQPNGY[Z'QoK0e e KNZTPZ#K[IjO,GQP0G)S,ZYQNS_gUVbS,#M#UVGQ
GcZfiNS,bVK0Z#e GcZPUq7DGoVX@q9bVGcm@q'zW0f#N`fjq
xYO,m'GcbVZ@`'z9qfwq>0NPqajGY0gK0ZPS,Z#Y^'K0M#UjTPZ#KI~OGQPNG0 u ZK[i0GcbViNS,GcIffq@Z@P0n_0f
L[N'f+ffh0P7ff>0jN~L0f^fff0000cc'`0m#m@qDy[fq
xYO,m'GcbVZ@`z#q'wq,`a{v7Y[bQNS<`@q'wq@0#[q/KQPGO7ohX#GoVTNS,Z#igcqaUhX#GcK0bVGcem#bVK[i0S,Z#'aY eY[ZPSklGgUhK'q
<Z)L'yc+ff'0j0La'n0n00> j^0>_Pfj0f[+fJy0'
'n>0n_N'070'`[m#m@qa000['q
)oLY[bVUhXfd0`z#q,`!{xYd0Ggc`q N0PqR0K0e GXPSO,KfgK0m#XPS_ocYO bhK0PO,GcegknbVK0eUVX#G0UY[Z'QPm>K[S,ZfUwK[k
u bVUSyohS_YO<ZPUVGOOS,0GcZ'oG0q]0f'J'n_0'`0#`#P00N#q


fi0#[#)##N#0

fiff "!$#&%(')+*,.-0/1&243507698(!;::%<:=?>@A'CB@DA=&DFEC:ffGG!$%:FB7H%<IKJ"LGMON PRQ
NUTV.W0XYMOZVUV[N]\Z$X Z^^&

_&`>a7bcd-0/1&2e50C>@!&'f:%:=F!4g&Aihj:lkm8< =&n!;: Fo"ffGf%:7"BdffD7Yp7K/1/&.q>@rjrs:t
:!4f%:!$8s

_=:'H:7uv-G/12w5G7hj:lkm8< =&i6KffG: &%f%:'yxz@o"ffGf%:'@!4: ?6{8|!4:')7rs:fi}@Lf~ Z0ZGNUX\4C~f
MUZ4MU?W0XYMOZLGXTMONU~XT&V~NUXM,~XZLfZ$X Z"~X?J"L0MON P N|T&V.W0XMOZ$V|V]N]\Z$X Z4pp72+w+2w3Y

_'f'YB7:::D;8<
7-G/11/45GCbpY0!4f%:_%:EC:ffGG!$%:Bdf%&IFEc'%<:=F!
_A8f%t
*K:f%I
_ G8fir:RAff'fG%:7KoKG8| !4:7KR-* ')]50KJ"LGMON P NUTVKW0XYMOZVUV]N[\&ZX ZTXY
~,`MOZLF{N(NU~X*{8|'f#&%qff%<:ffG

_'f'jCBd:::D48
j-G/115G@7tsuY%:>@!'f:%:=xzC:tsuY%:*{ff%:ffGI.r:
}yLf~ Z0ZGNUX\;m~HMUZ4MU?WGXMOZLGXT&MON|~XYTVY+~&N|XYMC~X)Z$LfZX Zi~&XJaL0MON P N|TVYW0XYMOZVUV]N[\&ZX Z
j!40!40!;H!;:7h;-0/12+50oj:"oC8=H%<D%|ffojpp!&ffD"@DRocA!4f%|ffKj'%=:";x6K!40'dCf%:G'
rs:}yLf~ ZGZ0NUX\;~fCMUZ"^MUWR.4C7~&X7~XT&MON|~Xy~f~,`MOZL@ NUZX Z)pp7/$
/3
6K!4p! &%%H%<&A7bjl!4::!4l!;%|'
-G/1215GqD&'f6K!4D'{%D&A!i
fi!4p7r:J"MO~TMOT4
T&X\T\&Z?TX}yLf~0\&LfTNUX\"4MUW0XYMOZLGXTMONU~XYTVj~VUV[~$N|}yLf~ Z0ZGNUX\;{pp7@/e$
e
6{7
.o7q%D7{{-0/11&50b&: &%<H%<&:!$8m,:8]%:!46{8|!4::%:=r:}yLf~ ZGZ0&N|X\4i~fMUZ
$MvW0XMOZ$L0XYTMONU~XTV{~&XZLfZX Z"~X?JaWc}@V]TXXYN|X\`$MOZ$pp77/2&1$`/1w
>C!4! =6{:D!4F Y-0/12&1507B@Db&:48`4x,%|'ffGi*v#:cqI'f'K}@L~ ZGZGN|X\4
~fiMUZWR@-0/450Y2/G+12
>R%#'f$`>".uKYqffD!4p%7>".*C.-0/$12w50R,%<#&0'%fIts@!'f rs:xO:ffG4xd%<:%ojA!;0!Kr:
}yLf~ Z0ZGNUX\;y~f"MUZi^4MUWR`4,d~X7~XTMONU~Xc~f~,`MOZL, NUZX Z$4pp7Ywl2$+2w
>R%#'f$i>"CuKCqffD!4p%>"c*,j-0/121+50 rs:xO:ffG4xnd%:%ojA!40!Ej'%:=,%:=
qA:ffG'rs:}yLf~ ZGZ0NUX\;~ffiMUZ^4MUJK`4,a~X7ZG~LG~H~,`MONUX\,pp7
3//G3e
>@'H:'ffDG%:7dq.-G/125G7!$8B@D&f%'C4xKhj:lkm8< =&"%:?ocr@!4: ?>@gYf%|ff'@"ZCZXYZLQ
TMON|~&XR~,`MONUX\&7d-5043l+w
>@'H:'ffDG%:7qCCh"!;G8<g8]%:=uKC6{j-0/$1250B@D'fI:D'%|'?4x &%<=;%<G!$8!&ffD%:'_k@%D
p4#l!4g8@p%|'f%|ff@p&pf%'rs:}yLf~ ZGZ0NUX\;~fCMUZ~XZLfZ$X Zy~&X7Z0~LfZ$MON TV4J"+Z MU
~f"yZGT4~XNUX\T~&MRiX~&yV]ZG\Z$pp72$+1&2
>@'H:'ffDG%:7,qK-0/$12/4506{8|!4:qI:D'%|'o8<&=4%|ff!$8c6{0'fpffGH%<#&?rs:}yLf~ ZGZ0NUX\;~f?MUZMU
W0XMOZ$L0XYTMON|~&XTVY~N|XYMC~&XZLfZX Z"~X?J"LGMON P NUTVYW0XYMOZVUV[N]\Z$X Z
qffDpp0')+
&-0/12+w50EC:%#G'!$8698|!4:'{xOK>@!ffGf%#">@gY0'd%:E,:p &%|ffG0!4g8"*K:#%&::G'
rs:fi}yLf~ ZGZ0&N|X\4c~f~fcJjJcJaWQHpp77/e&1$`/e;3
0

fiC(fi"_(
4_d4.G4G{+|$9dO fff
`|$fi j+f U[
7d @
f l

4_0+0 !"f#%$& G4 Kd77.!')(C**+s
(,
jG{04- .#0/, 1 .) 2
d4K{G4G4365879;:<6=?> :)@BA):DCE:F<G=IH=I9;J<KH<GLM>N:OHCEJP<G9;<RQS9T<VU%W-Xff=I9YIZ Q:<6=\[B]FCE=I:F^_C
$K7
`G4b
sG fH<c de04Gs
d4K(`_Rf.{0Pf0_CK(0;f_<?fhgvf)2_i#Gs_skj AdJY
7.:O:.LP9T<-QClJ)m4=;n-:poo=;n0q.<6=I:AO<GH=I9;J<6HX6riJ9;<G=s?J<DmE:FAd:F<G7O:!J<0Z!AO=I9 79;HX6q.<G=I:FXTXff9ffQ:<67.:$
`c7 vu 70$-wyx0zj04f b(&G#&<H<&$N${|4G"4G#b$& 0;p s{jNA)J7O:.:OL9;<RQC|J)m
=;nR:_[BW-^}^~:FA,s?J<DmE:FA):<G7O:!J<0Zq|H<6L,[v9T^~W-XffH=I9TJP<%Jem!:DnRHF9TJAc?3 L9;<BW-AIQn
`G]47${Y0+0JA.^~W-XffH=I9TJP<%JempGA)HL:OJ)"C!9;<0jNXffH<6<G9;<RQ<6L:A0<G7O:AO=IH9;<G=I]4${47
.G# 7

.

fi
ff fi


!"$#%$&('*)+,,-/.1002433

@BADCFEHGJI

K

GDL*M/EHNOADCFEQP

_a`cbedgfihkjmlonqper

56789:;-/<=,->?7
#& :*3/<,-

ASRHTIVUWI

XYC[Z;L;GD\^]

sutwv!xqyqz{tqy!|y }qtq~!;"qy~e~qvq

iSi/4^ =^wo =w =wH4
*w4 e
f;nrwh9ab"*rql

q}q y1tqez{}q i}quy D~qvq

* oFk/4 =DD4{io
4/=9"o
b"eranrwl

q}q1qqyqz[~q| tquy yyu

4 wk4 {"1=J^ /
6="4!

io e!
91 { o^ $ ^!o 6(o H$ $ ou% $"* 6(
$ 1 =4q 4D/9J=% / % { o"!$
$ *
; "* koo [o *[* $^o 6 $

1 1 $ * $ i* $ e!o 6i 66 9^ oo
ff fi
oe $ [ $$ *% o{q 6 [91 e 1
$ ${!= $[!{$ k$ *%
$99oo i$
$ *9 1(o
6 1

6 $ 9o
1 ok
* $ [o$ = == 9
$9o 6 ${ $i

ko 99 9$ =% $q

4DD = %1{
$ $$ 9 okq ;$ ;;D u e ;o{

$ q 99%o $ ! ;#
" u$6^$ {6 i$ ; $
$ [$ o$( %$ $ D6$ a$
1 6[$
$ & $ o$ 1 J* $1;$ 6;1D 1o% $

'
$ H$i $ kH ( $^ $o 1i 6
1) ' $ e^$ * De+
'
{$

,-/.0

1&243i!51

0

6879;:=<?>A@B&C9/CD@9AE&F7B@GIH;@BJEK:)LI:)E#@MON;PQ<REJST:)7MU@&<V9AWX7 B#MYCE)<V79%C[Z;7\IE&C]H;CB#E^<?_\A`CB8:^\IZIa=@b_Edce:)\;_JS
C :!<R9AE#@B#9;Ce`fMU@b> <?_J<V9I@gh7MU@]WiC _E:cj:^\;_#SOC:kCd``lH;C[E)<V@9jE:4P]<VE#SmSI@H;CE)<VE)<:]@GISA<VZA<VE&a^C[\I9;> <?_@bnoc;_C9
Z;@9;CE#\IBCd``RL/@GIHIB#@b:#:)@b>p<V9YC%:^EC9;>ICB>Tq;B:)E#r*7B>A@Bs`R7 D<_cjPtSA<u`V@7E#SI@B:+cj:)\;_JSYC:kvwAxy7WH;CE)<V@9AE:
E#S;CE@GISA<RZA<VEta^C\I9;> <_@mS;Cdz@%SI@H;CE^<RE^<?:JnIcCB#@:)EC[E)<:)E)<_Cd`g/h\IHIH;7A:)@E#SI@CD@9AEPQC[9jE:QE#7Y\;:^@E#SA<:
<V9AWX7 B#MYCE)<V79{EJ7|MYC}@O>A@b_J<: <V79;:gy~I7BU@GoCMUHA`R@c]C|>A7I_E#7BMp<RD SjE/9I@@b>{EJ7>A@b_J<>A@PSI@E#SI@BmE#7
C>AM/<V9A<:)E#@BC[9jE)<VZA<V7E)<_:mE#7C|H;CBJE)<_\A`?C[BYH;CE^<R@9AEO&B)<_g7{CHIHA`VL:)EC9;>ICB>{E#7A7`:7[W>A@b_J<: <V79
E#SI@7BJLOF=:)@@F=\;_@/Ce<uKCIcb ANW*7BC9m<V9AE#B#7I>A\;_E)<V79NcAE#SI@%CD@9AE]M/\;:)EC:J: <VD9OHIB#7Z;CZA<`<VE)<V@b:c
7B)#()TJb**cE#7/zCB)<V7\;:]@z@9AE:g4~I7B]@G;CMHA`V@cIE#SI@>A7_EJ7BMYCdL/9I@@b>OE#7C:J: <VD9CU>A@DB#@@
7W]Z;@`<V@WE#7C[9@z @9jE%:)\;_JSC:k+&B)<_/S;C:SI@H;CE)<VE)<:#nIg/@P]7\A`>E#SI@B#@W*7B#@`<V}@YEJ@b_#SI9A< \I@b:W*7B
_7MHI\IE^<R9ID/>A@DB#@@b:s7WZ;@`<V@W<V9YCHIB)<V9;_J<VHA`V@b>mMYC9I9I@Bbc \;: <V9ID%Cd``lE#SI@>ICECCE&S;C9;>gs 9E#SA<:sH;CH;@B
P]@<V9jz@b:^E)<VDjCE#@E#SI@HIBJ7H;@B#E^<R@b:t7W&79I@H;CB#E^<?_\A`CBtW*7B#MYCd`<:)MWX7 BQ>A7[<R9IDmE#SA<:g
QSI@%M@E#SI7I>mP]@T_79;:=<?>A@Bdc;PSA<_JSP@%_Ce`u`&E#SI@U^[l/ 8/bAbcS;C:]7B^<RD[<R9;:EJS;CED7
Z;C_J}EJ74@B#9I7 \A`u`<TC9;>{CHA`C_@|FdvwANgE/<:/@b:J:)@9jE^<?Ce`u`VLyC9CHIHA`<_CE)<V79y7W(PS;C[ES;C:/Z@@9


+,,6-fiuqfi"$##I : :468eff8i817
#& # fi

%&# #*:

fiAI&]IbIsQobd

dVbO#Il+l) (#=AIbdIs]I; tAb AV#%)#eR j)*#
II;A)%]%#VA##b)JbUR# #AVIY/A#&;V#*#/A/VAIeQRbA
;)yI;)*A(b # VAVI^YA)/#AJb;VT*#/AJ|;)
OIJ;AR) )J)VII)V{%)8IU8=dV;#]b #;#JR
)IIj)]*sIe{J;K]#]#b )AVI%; V; R A;dX

8u&UARJ
Ab#)VI)V8A#R; V A;d/;d%bJ8#I/I##)Vb&VA##b)b I#mdRU])
=;)/OIA&VA##I#[)Vse %I ;) *AI; o% ;AV(I?[
; Q ; )&[l#I]I;#/I#b Jb )
j*A *;m l #I]AV;#
I#b # #d * ;s;#IY ;)A 8XAJIO])|AbJ)V;bA#{)I;)#I
V; V ?A;e?J) ;b4bJO#II;#IJb ?[#bIAJ)]K;dV 4VO#I #b *
#)V];AJ#I
R; V A;dm 4* VI)VI#;[AuV) )#)VII)V
#IY^j# VAVO])IJIp[j%[|IdVA#[;VRyV{ j
; V)VAVIm#/IdV;A)#)V( )J)VII)Vl[;#I II)VI#II# ;AuV)
RI#m#AI )#)VII)Vm]I/;A * )I#II;^b#I/I)V;JRAVO
V; fJ;Y#OJIjj^pU;#^?AI)V% )#)VII)Ve (#I)] )I (d&#I/])I
#/[#Yb ;dRVV ?b)#)/#;#IAJT[;VV V{ ?
#II#bJ)V#I*)VO[!])IJ) *VIO #;d)O#^?=XO]
QIp[II#jJ)m%Ab#)Vb{IARbII %]#;dV|AIe JII#bJ)OAYdV

jIA*#JI;#VO#A]tdV)VOI; YA)b+II bAV]#b) ;AV%#
;V#; J# |#t#j; ;#)A)VUVj#Jb)#bpROJI()AI#^?Q;;dV]
#A X)V A#;sA](]IA#;V!J%;#I)AI#^?QedVI#A X)V

#( #
;AV)IIj)%]Q[jJ#b) ImAYeR8Ij)AVeK;)VA;
JI% =I;^R &#I*R]VIUX I]X #/A?



"!
.(
1( 32



,+



"- *
/

0

4

65

;: =<
C

9

?>
fi<



fiff


$#&%
'! )(*



+

"- *

87

9

@ BA




"
87
DFEHG )IG&JK"!Al[X G#]L>eus;)Vj]]V#OI;)V)IAVARM#I; @A
^ P e G
Q'"j! * IG
NRS 'T L+> II#j VY[#VUT Wy
V ;;^RAs#;[KI AVARB#I
?;dI;^R^?X@I
< ]I A?eROJA? *#Yeu) ;p#I(Jb)](Jb0> [II#j VYJR
WVU
@ JIJ;Y> ; )V1T WVU@ V b^R
DOQ;Q '! )(* oIG 8Q;Q R Z U
[ L> IIJA VY#VO[WV [K;)VA;dAVI/bA@
D3"A! l[XX8( X]\^#&% *u4( *8>8&_ )t#AII AVAR&# I; A@
` 8 ;a ^ P eX8( X< #;?+4j# #)V;A#];;V##I)#Ab>)&_ )
;]I;[)V)I
@ II;A)/#ITAmdV;=c:
8]I]%AJY; A%dK])It]V#
AYeRFj
^%e );JJ;#I%)tKV; V ?A;eX%#[) X VI0 ) P (m)I;)t!JIj)
#^?=X VI^"j
! *AII#A Rm#VfT V #IV; VA;dJ) *VI^"A! l*Yd)#[) X
) II#j VY[#V[WV ]#IV; VA;d*TJ) *|'! )(* ;JI%VA##I#)V

b]8
( X &V; VA;d#) * RI."A! l*;1#& % XB9 ?&^#dVA)*#t#()I#;bd

;b#b] ^ P eX8( X 8IIQRII#A Rm#V0T WV &#Ib)/)##;#IJb#bI
J#I4*)VUf)J#;#I#bVAJ0
) P 4( *II]A#bs#; )VgIT
V;h
W
V ;#I];^RA#;s AVAVfi# I; ;d4I;)V)&;.&_ )4I ARAVi^ [I; ?
%A #;V!KI'
#;&_ ^?Q;&I;^R^?])m$) ;))6l AVktj #]#;[bdVY#A UAR
#I/VA*#Y[)V#;l&
_ ^?p%OJA?b#^j)dV|#Jb#b|tV#JRAb|
U A?#I
#U;^]O ?I T;dU#ImVAXJY)Vm#&% *;X8
( Xn9 {d);)I#;
o8p

fiqrstHuZvxwyuz{tH|arst=}~rvvBszuZ"
0W8X80"?6;l;OC'8$M
P"UW8lXgn48
"38X
1W88"h;l"b'8MP"&^"$"P
WM
P"?8fZgBW."
g"ngC?aU 6k 8$XWa8" .
WP 80X"
Cg8M

"3"WH"P
,;^;W88W8WX=."W;18"084.n8W8n
.X
"aWf;0 X
"W W"."a 8 ;a8UX"
W;8l8?X ;8.88l;ab6
8M".; .;yW4ZW;1"WP .48=c" 8
.X""6;"
P8 ;~W;^~"W0
"
,h? 8~IB"&fiB"6;$
WW;8;;8$
fiaC X.;W; ; W;
& WW8P8W?8W88"
a1C81lWW8l. 8C~lW88"6c
f" 8=,
1 c)h4fcOW8
?;mI8X)?hX
;8fi&a"6c8k"
./^""8fi~ ""
WP a8,8
~"X"W;f8/PW"?"I;~W/W8

,W4IWWc)k W;
c8a"FP880X.X W;yIaWW 1X W;
."l k8W
8;C"h ""~8;1;yZW;0""4W;1886c
W1~"Wf8l8 ;k4kW1~ /$ W8];XW"8~W
C 81 ;MI 6;~a
"WP "XfiWCWXM;
C X8"
"W 8anW84P833I8W48;"Z? XO/;xyP
"=Z.;" ;y"C8;.,;y0 W; ;"C
Wk /
"W;y""fi
Wl"3 8
C84"X8~ ^ ;~
" ~ ;~f88WcX..6".1XW8
lC""C1 W
4W6; ;."" ;;"88^U"W;B;y1WUk P
W8P.W8
;6;P"CyUP.W,;=f "?P ; W."." 4
W6; h;80~,I";fikW;8 ga"fi&;fig ""HW
X8X&08"WXWfWXOXh
" "fia
"k88&0"466l
""acX;Wa

MM6&k;~"C; ;X"W66cc "";fi$W"W

8n^.;
W8?1^".W88"k;aaXUW;W";c
"W";;W8B; ;,1C"W"Cfi
/ f"W;y"8
4h" ;8fia"6c8h&"$
B/W8
;1"fiC"M"8kc1Xh
XP
W8C8 C88b
" 8;WX~"
;$$;X"8&W8
6&^h "="i;UB8M"]; &CW;"XUX8h W."
;~X""W84W;~WC8 W Cccfi"W66; ;M;~88
IBCM;W "CW"c
W
k;k6HXh ;MC.;WC"W&l
X1WX;~88C6;.; ;"W66;IW
hW;W;WX"W; "88
6 .n,8WbcXyW;^"=8"0 W."."y
"W
8B8y
;0aXUW4nCWFk"XccWc,lX" /H"W;.;"XWcX
" 6;,.W
I, c^CWF.WXWc;="^8 "
]~X ; ;C"W6;
W
h18"1.icc
/yW."81/W8~.Xy PW8 181";W11

86k P
W8bcWP" ;fia;W?" W;
a"CXh"?8?;BZ
"$
k4 C"&?"8h? h"X8 ;1XUXU8 ?80?W";Wla
;^"""lC?" C W;a;W";a ";1"? W ;
;X"W66cc8"4"X8UhXlIcM$8'H6hC".8HIW8
"fmF8$"W


fiC
fiff b
fi

"!$#"%&'#)(*!+,&-/.0%1324.5.6 #77 89 : !+883!$;#)(=< ?> %A@BC%A79D>E !F2G!+#"!H&-I#7(
Q[Z \^]&_5`
%&79587fiJ2G> %&#)/!+9.5@&BG!LKNM&OPfiPfiP+OEKQRNSTU> %&V>E !W2C #7(YX
#)(%&#a> %&b@ -cJ7.bfid-^7 .
#7(fi!+587fiJ2G> %&#)V!+9.e@&BG! :f%&.bEB49 :f#7(d-^ 7.5BG% !g&-h#7(d-^ 7.iKYMg
j k PPfiP k KYQ j :flm( 75fi% >)(nKYo j 2C!
E24#7( UK JUpNK RSI> %&=< 24 lq#)(Ur&l B4fis I@% !+I% !L8BG% >)24st>E !$#7"%24#"!L V#)(I87 8J7#+24
&-u .0%24bEB4 .b 3#E!!7%&#+2G!F-^9 24sIfi% >)(b%&#7 .,RNvJNE1%&.b8B4 : #)(h>E !+#)"%243#UwxKNMxyWzf{ |}KN~xyWz{wEd&X
!7%9!#7(%&##)(m87 8 7#+24 0&-#7(U .0%24t!7%A#+2G!F-c9J24se!+J.bU%&#7J.#7(%&#a>EJ3#"%2! K~m% !L%[>E &+>E#
2G!L#Wl2C>EI#)(U87J8 7#$2Jt!7%A#+2G!F-c9J24sb%&#7 .V!#7(%&# >E 3#E%24V@ #7(=K %&0K ~ % !a>EJ&W>E#"!xR m24< =%
.bEB&-Ie:l /> %&0EI#7(U #77J8395&-#7(2G!L.bEB%J!#7(Y 3#)7 89eA-#)(U< fi>E#) h #$2s
#7(b8) 8 )#+24 !I&-m#7(0J2 7 #e%A#7 .V! R5STV!+(&l#7(%A#fi:% ![sJ7&lU! BG%&)s :#7( 7V%&)e.0%&39
.b )*.bEBG!=l 24#7((24s (q #77 89'#7(%&l 24#7(B4&l V #77 89 R ( 7E-^ 7 :g.b?EBG!l 24#7((2sJ(
#77 89b .524%&#7 RSY!+Y#7(2G!U ]& " u\7Z \^]&[ f]&_ u]A #75!+(&lD#7(%A#a 7 Y&-@EB24E24*sA2<J I% > >E EJ2s5#75#7(IE%& .b^l +BG!.b #7(H2C!g>)B!+EB49V7EBG%&#)fi0#75#7(/%J!7!F24s .b #
&-N8) 8 )#+24 ! #7b%&#7J.V!L#7(%&# (% ! .V%1324.5. #77 89V%&.bJs%BB% !7!F24s .b #"!m>EJ!F2G!+#7 #ml 24#7(
#7([>E !$#7"%24#"!L24.b8!+fi=@9*IbR
(e>E >E #7"%&#$2J*8( .b J;)EBC%A#+24sV #77 890#7V#)(["%&J.b^l +BG! .b #7(02G!Yl EBB4
rlgyW %93fi!x:mfi X:h {"RT'8(9?!W2C> !x: #7(El +BG!7%A7V#)(=83!7!W2@B4>E E%&#+24 !5&%!+9!+#7 .#+9382G> %BB49D>E !F2G!+#+24sA-U.V%&98%&7#$2C>)B4fi!5Je.H&B4fi>EBfi!x:h%&#)(V.5#7%BB49E1>)B4!F24<
87 8 7#+24fi!Hy 5%&#7 .V!{U> %&@ :-c IE1%&.b8B4 :aJ%&#7.!$#"%&#7fi! R= (0>E 77fi!+8 J24s #77 89
.bfi% !$7m2G!m%A#h#)(U(fi%&)#hA-!+#"%&#$2C!$#+2G> %B.bfi>7(%A2C> !g%&0#7( 7.b9%&.52G> ! Ra ( )[%&7I!+@#+B4I@#
24.b8J7#"%&#J2 7 >Efi!@ #+l eJN<J24 lm8&24#h%&d#7(%A#N&-#)(h8(9!F2G>)2C!$#"! R ( .V%245 LB2fi!24
>7(A2C>Eg&-BG%&s %&s RS lm%&#N#)UE187fi!7!!+ .bL24#7EBB24s #h%As 3#!r&l B4fis :&lm(2G>)(52G!lm(39
l a#"%Ar "!$#7^ " B4 s&2G> % !Ja!+#"%&7#$2sg8&24#fiR ( .b3!+#!+8fi>)2u>mJ2 7 >EU>EJ>E 7!>E !+#"%A3#
!+9.5@&BG! RYSd fi*#)(fi!+5@fi> %&!+5#7(d.b3!+#g243#) 7fi!+#+24s*Jfi!+#+24 !g-^ m!I%&+2G!+dlm( l [(%<
!+ .Her&l B4fis V%&@ #+%&Tl 2G!+(#)*% !7!F24s 7 fi!&-m@EB24E-U#7*!+#E%&#7 .b #"!I>E >E 724s&%
8%&7#$2C>EBG%&g24J24< 2G%BWRI (d8%&"%BB4EB28(9?!W2C> !gl BG*% )fi!7!m8) 8 7#$2fi!YA-%b!W2sAB58%&)#+2G>)B :
lm(2G>)(=2G!msJ "%BB49*>E !F2G 7fi,#75@dlhEBBNJ#"!F2G[#7([!)>E 8d&-!$#"%&#+2G!+#+2G> %B.bfi>)(%&2G> ! R
#7( glhJ7r=#7(%&#mE1%&.524fi!g#7(b>E fi>E#+24 @ #+l *E%& .l +BG!Y%&= #77J8390-^7 .
8A2# &-N<J24 l >E .b8#+24s= 7 fi!L&-N@EB2E--^ -c ).eBG% !24*%I8%A7#+2G>EBG%& B4 s&2G>)'2G!m#)(%&#a&%&+2G!%& >E<!+r&%5y"fiJ {"R ( 957fi!+#7+2G>E##7(gr3&l B4fis m@%J!+m#7[>E !W2C!$#a&-%[>E &+>E#+24 b&>E !+#)"%243#E!f#7(%&#Ly^24[ #"%A#+24 f{(%< #7(-c ).w yWz{)| ayWzf{"wEdDU%AV|4|}yWz{)|C| [?:lm( 7
%&d=%&7aJ%&3#$2 )-c7 -cJ7.5BC%J!243< AB<J24sI%&79I87fiJ2G> %&#)fi! B49 :&l 24#7(dU>E !+#E%&3#!+93.5@&BG! R
g # B4952G! .b3!+#L&-N#7(YE187fi!7!F24< I8l L&-"!+#)^ " LB4 s&2G>Y #h%<&%2BC%A@B24=#7(E24U%&8873%J>7(:
@# #7(U!+#E%&#+2G!+#+2G> %B2-^ 7.0%&#+24 0#7(%&#a> %&0@YE1?8)fi!7!+fi2C! J24#7mB24.524#7fifRmv E1%A.b8B4 :24#2G!L #
8!7!F24@B4*#7,.V%&r 0s "%Bm% !7!+ )#+24 !5%&@ #b!+#E%&#+2G!+#+2G> %BL2 8 >E RD%&+2G!d%& >E&<!+r%
!+(&lD#)(%&#a#7(I 7 Y&-@EB24E-h> %&0@I>E .b8#7fi0!W2s5.V%1324.5. 3#7) 839d-^ a#)(E2LBG%&s %As R
(% !+#)+2my"fi J{m(% ![%BC!$;!+(&lm!$>7('%=7fi!$B#:NA-fi%A+B49;fiJ24<%B4 #V!7>E 8 R*L#fi:% !Yl b(%<
%B47fi% 9'!+s sJfi!+#7fif:l V@EB24 < #7(%A#I24#52G!2.H8 7#E%&3#5#7,B4 r%&#b%-%Ae$2C>)( dBG%&s %&s Rm
BG%&s %&sJn%BB4&lU!=%&)@2#)"%&79"!$#7^ " =% !7!+ )#+24 ! : -^BBeL3&B4fi%&B4 s&2G>&:I%&7@24#7E%&79'8AB9 .52G%B
>E .5@24%&#+24 !d&-U!+#"%A#+2G!+#+2G> %BhE187fi!7!W2J! :%&.H 7 #7(fi!+=%&7V%BBL-cfi%A#77fi!I#)(%&#e%&)V% >E#7%BB49
!+E-^BN#7br&l B4fis ^7 87fi!$ 3#"%A#+24 =8"% >E#+24#+24 E! Rhv7#7( 7.H 7 :#7(I"%&J.b^l +BG! .b #7(
.V%&rJfi!m8 +-cfi>E#[!$ !+d2T#7(2G!Y+2G>7(!+ #7#$2sR[ (5s 3%B&-h#7(2G!Y8%&8 Y2G!Y#7tJ2G!7>E&< Ilg( #7( I#7(
>E fi>E#+24 #7.V%124.e. 3#7) 839%BG!+(&BG! RS*!+(&l#7(%&#H.V%12.5. #77 89>E #+243fi!
#7@*l 2GEB49'!+E-^B:U>E&< +24s.V%A3987 @B4 .V!H#7(%&#b%&70-%&bJ#"!F2G*#7(*!)>E 8,&-ey+%&+2G!b
>E&<?!$r%: (% !+#)+2:fi ?{"R
fi

fiau'uYVCa?&

m')= 7 b&f4b77 b7&5 * & 5VA =7GbE fiE+4 cJb 5 3$)
G& &J aJU74 m437fiHfi&+4 fi7 7I&7g7 4 VN m7757[E J4+4 =JV
fiJI +Y7&L4)fig ^&)V7fiJG &)fi /$+fiEm)&aV345 37) 3H
*)&4bm&"$3 Y47G[ $ b J4+4 h0+&7&I7 70&7b++4 +4fi57A[&+G+
43J&4 407I47 " E+4 = +h +"&$C$+G f^ 70&+4 *&t"+7^ E J&+u &+4

^ Eh7&5&4b 7"&eEJ37$$J&m7Gd& [4fid4&4+4 e$ b5454"&+4 d&
V45b^ 3)7 Vb )?
m7fi+ &7GL& LGL 73A fi0 ^&&UxNFV)UEm+fiE+4 UJG7E7 cJ7V
^"&b 7
fi7+ +G4 7&b&
7 ff

fifi
U4 7

fifi"
T*JC)E7b);+3E
&,+ V&+G &h+E&+G++G J7+ 7+4 xG7+fi &4J4
&70&7E4VfiJG f&,E
7bE& b^ +GYb )?^ 7V4 bF
fiE+4
0 b+"A757b FG5)fi+4"I7A/E fiE
V45i 77 07VE& b^ +G &
fiE+4
V 5JC)E7Y&7H+[7fi$e)fi+4"
g! fiE+4 eE H"&+4 7Efi7fi g"
fiE+4
#b d7 77,7b7C)+5&a&7= E+
^&)V7fiJG &)fi &07I fi+$J=&N& GE=&G &457U$)445&NV345
77 bG $
[E )4d
fiE+4 &
% 47;+Jb/JG7E)F4

')(+*ff,.-/0213-547698;:<,56=13>?130;4@:A13,7B
57G+fiE$J&hL&4 a)^ 7VE$Jb& G& & m&7aE& b^ +Gb 7f
) U4@

fifiD5"
IVA7 +GGCA7 E4VE& 0^7 C

EGF=HJILKGMNPO@QGRTSGO5R7M
*&)07 7fi$7fiT4,^ 7Va4 &G mG&J& =7&0&U57E?)fi75J7+E&+G++G
4cJ7V&+4 q&"$7^ " H4c )V&+4 JT;) 7EcJ7E$"&+G++G YG& &JU;VL
mG)'Gb&&+G&b&/,G&J& *fiF4 fi3 7WX
YfifiZ" b7=) V4 b&I7
& fiJ \
[//)m"$7^ " L ? ACA7 3E WC$+4b&u)fiJC A7/AVE $"&a+35&G
&H4
]eb+ g&&&+G&4fi _ ^
m5+"&$C$+G aG& V& H 3"d+"&A","+)^ " I4 ACH 47tcJ7 &U+E&+G++G
J&3$ bc )eG
`ff ab"707 7d
cec `fga@hcPc i0GVk
j.lm!j5mAlonqp=mArtsvuwj.lsvx!xXp=mArfb U
43) 77 7fi0 IE&+4 5 +h
[Ay

&7&L7 7fi+ "7m7 7+4 H&N V4
E4 b "[7A+GFcJ4z
`ff ab"{
b E744& &&747"&);+ I&m&&+G&4fiT7V+7E$
&4)*^ 75G"
`I gcJ=E&H +
cec}|2~p=7 aT7hcec ifi7E+4fixI^ =T?fi
fY7
7 7+4 ,&h VE4 b "Y7&Y&7e)G7 &;
f
cPcq|2~p=.ga7hcPc fi7E$fi f^ Ufi
af)e) )+4 &h V4E4 b "Im+b7G,G9
a&
cecq|2~p=5 aT7Xcec iA fi)E+4fid7
7 7+4 =A4"g&JV4*E4 b "g7&m&7Y4*)[7G,7EG&+4 _
ThG+[4&'7 7+4 5E7fi7F4 &7LcJ7
o`fga@hcga@o &mG)5hm )
!mArGDp}nqp=mArG<
j.lm!j5mAlonqp=mArsvuwj.lsvx!xXp=mAr7x )*&=E?)fi7F4 =GL47 fi,7b 7I7I) )+4 =&a V4
E4 b " 7&+GF^ 4{
`Tc7J &bJI73+YE4 b 3Eh)&+GF^ 4
a44 &5"&+4 ue
G5G+;E WC )fi7===7J 7$JE7fi7F4 &T7V$ [&m7 7+4 TE?7fi)F4 C
)43+fi / J4+4 &=54+4G &+4

! .AX@q_v_q!@)w== 2=w)WYv !Gg!v3g_ffeY gq_X=vA_gq_Y 2=Yg=2hwA2 gff_f=g=
.Y=gv_=gg
Gv=q_gq_9=Aw!_ Y= g= =!!=q_! Y= =q_!{= !v_q_==g@_==g=)w.Xwq_g2_9=Y2= Y= q_Yg
_yeY_ =< =w ho@gXggT g{= _v=g= =g3q_!{_3_wg5Gwe= {Y =YX3wq_!Lw {Y=X
= 2=gY3__=g=g


fiZ5725 5
5e+.wXAZf<bvwv.!7vgvv5fgZZXYzA.y.DD vZ.vYZw5.g9A

!7hL
fiff +!+.Aw5.<gey!55 e.5v9D.wy5Zeh5ZvA.
5X!hPv <g.ZW!!!+.Dgg.ffvAy.e.A555hPD ;.Zv9+hXAv+vZfg.w

tA.AhvZX$e"
!A5.<!. #<5v.Ageg% $&(' )Z;g!Av+vZ;A5. DX$y
)Z5*
+<
.gDe2; e+Zg;!vXYe e{5vf<2g5+ Aw9A),
A% +7- @Ab5DX& /.e2< PL.ff& +
evw5vfw50
#A e5+Z1
evX 2e Az.D 3 4 2.AgevZXffe5
!A5.<!
. #5v.AhhP 7655w5v+AwZPevw5vXAgeW e+ ey & +5vZe& +!)w.A25
Z57v)D !A5.<! .Agev!@{ ege eD9 8:# ;/Zb5 < )= e{.g+vwZ& +ffev.>
DGFff e.vT HZ!
? ff5v!=ff.g5A55<yZ !ge. e vZ.v @e@ - A:B Cff- 2evE
A.W!+.Aw5.<geW!5 we..e& +y=e.gw WA/ A.K
Jff;5ffAA e 8.Zw3A{2 e
!55 !g< #
LNA.P
OQNXZ=
RQI SfiHTSfiUWVVVX =R@Y3D55Ye Aw= W Z<. [$+\
=R]Y3A55YZWA= e

e
.A" Z. [$Z= ^65!D+ e2 vA!5 w9w5 g!Av+vZ_
< A7 !A5.<! ".AgevZX
. #5v.AhhP` $9Z+5 .!A =- a\bvA= 0 &d
ce"g g fbfi hji 9a g fb c=k Lml\& V&(5eZZegey.vZe.
59gvWAhPvA$D55Ye Awf Z<. e W\.A wz!+.AhPhzg5 y7eZv5v W. & +
g gW 2- evXD.!+3!v!5Z={+ g5wv+vvYwA+ ed
#AAgAge.;A."g
n o5A555hPDz v!A.!z2 2<v+=5
#Age. Ze ! WAeZ}< Ageh5p
+A
2eAf9<vvZg5.!h5!5q
A55Ye D= e& Z<. % $!55 !g< # prJ={ yg."
v cs7j g fb` ht g f@ c=k Lmw"YL.A+7 cs7j g fbX hjt % u g fb cXk A.
cs7j g fb` ht q% u g fb cXk Lml\
vD55Ye Aw= & $ Y+.9<vvZ
cs7j g fb` ht
g f@ c=k AA55Ye D= ezZ555ff5geWA*
ey w vh x59!. 5A!A)g5.w!ge5}<W
L5Z .!A 7FA #v De;$e+.<XAZ
y.g+<vvZg5.!h5!ff=9<bvwv{A55Ye D+!+.Agg<.ff5 w5+w- evXA.! ff=
59<vvZf+ <g5v+vZXAwm
)Z5Ay+7959D+
+A< #9{ !5!< #Z= 8.ZegeA25
A& +.- +<0
y7zQ
? 9vAy5Ao
z P,
G5v
zw59gvA!<.gXAZg{7- e
{5|}~n<n~ & 59gv\A v=1 e^
e gffgv.A
759gvA .!DX =- a.% ==}- P\5
!!Ye.5XAh<. )Z57vXv
3@y!!Ye. .!5A}-a v=0 D5}<
h<h \h<h A. c Mh c }<={
"Sfi1Py z A.
8.Zegv\A #DgA e

.A.
P\` Zg y5.Zv5<egeA.W{ ege vAh<n
59gv\A)={ eP
z w5
<gfgvw.A
!!Ye. =-1 3boDfi1 & A25}w
] gl-SVVV%S` AXZ5v1
fL5 <vAh{. e
+A.
gl-SVVV%S` Dv
I"9A;Ageg

3@y!!Ye. .!DX=-aPboDfi1 & Af5W=
LN0-A.
OQN0-5v^
D.
-A
5.<gey!55 e.ffA.5
R2ff{.A5! 5{.vY.A.
P\` Zg y5.Zv!- !g5.!ge.5 +ZAge.A._
8.Xg% Y=<XZvW
Z.AZg2 8GvAgen
z 2eAf5{.g{AZ.eg5v! .Age&+yv @5555v! .Age&+
{.A"
57gey!55 <.
ZP{Z= 8.Zege 2eAf{AZeXD5 gge&+AmZ<.Ah8.v!D.5<.h<!55 e.^r
.gv` # e vZ.v@ <ZXw59g5.!ge5"
f 5.<gey!55 e.Ze.5\5"
#DgA e
fWe5!5w en
B.Zv @@9vA
#<ev
hh2<h<h k <{5vtgZ.A7 Z.Dg2 8GvAh<n


fi7&&T&&&&d&&7&G(&&4T-

0&-G&P`^ =:&d%&
5-9M-`&\<-pEd:2
:=<-\
9"-%&
=: <<:
-`1%fi< %W-fi>5&5g5 -%&%M%:-1-:
&% 5
-%d&&%9<
d-`=
:-[:1-:=:<<:W&%%<=&&%%g:51<9`%&%\`&d-&&% 5-%
=&&=<11->%1
-`:-W[%`<9"%d/K%"&``=T<[W*` P
%
d7`-<fi-:=1@=%"%d2<
% [":g&PP 2%->fip= -
%

@ % <<=<^1 -T2x%&&,-x1-:-%<% < -(%%&

x=&fi9&"&%&
< *`% [Q[W%:&fi2<^*2&fi
ff9&- %&"-<&\- ("9%&0[%=:`]<^-
% [*`&\& 2%&"&=&E=<n
&%&^ _= <-W& < %%&%&o=:<<:0&%%<=4&%`]<: &
&% < %:-
&&: =:& =:<<:0&%: 2<oW&o`&%-%
&%]<&p9fi%P%&5[- <1<
`:-\ [%-g
/-:K-&&
< K9
%: &fi:&-fi :2[1:fi-%&1%:-&&%9<d% :<% [
&%
]2<-fi=
]<
< -[<&q=:<<:W&%:- 2<<<%&d- [\<%%&
&<-<&
<:%n"
!#$# &% T1-^=2<1<:-%5=:<<:7&%%<K=4&`%]<:"<%%&
<<&Kfi-`
91:` '
)
+ -,./+ 0
( 1--&&%[-<*
( ++ \
:>1&0=`:2K %1
&<9fi&%: 2<<5 [\-&&% %-%"=
<%=<
<%%&^fi-:&fi<9%`&%fi-<-\=:<<:[dP<& 2
++ 4 3
5
+678 <
fi=,%&0= <%:-, %1 [W:`q9
:



;










=

<


%\%`&-9
(
(
?,%5%:-1^ &
%<-`0:`W:-1:-&:1< :% QP<&0
( >
=:][`9m:=2@ =:&>W* = <%&% fi&\%:- <\%&= %
^: `& `-
q&&%9`o- -<Kfi]< -\% [:-:
A@%-B
m<%n
C
D-2<E
!#$#$F % <`&=9%=&-p[-&:-1%:m:"&&%9<d-`\ :2H
GW %`&:-%=<
,%&" -2<-<&d=:-
<
&-m&` ,:,&% <dI
GW 2+ ff1%&1" W`%&\ :2<g27
1 << :
++ %`<-d2J
&< fi/^ P&
&fi< < p`0 [P2
++ [W&&`:
KMLONQPSRUT8VWQXYWEZ[<fid=:g `&\ff9&-< K:[]_^ `a7cbd e % cf7gihdjO e % k &%
[^`&5%:-`&9&1^-9:
l <&& <:`d<9 &%:%o-1:& <:

27< %%&%"=:<<:7&`:`<:mW-:-1-:1 << <^&&\&fiK%&
,
ff &
* 0:']_^ +ca7cb& e % ,\fgmh&j[U e % + k
9n +cf7gihdjO e % + k&:, `K[ <
%.+oa7pbqe % ,2f7gihdjO e % + kM
]_^ mr :7%&/%:-/`&W9&17-sl:<&1:& <: [d2@
-::M<91%&] :9%g <<
:`fi- % %5-<%:-1%&59&1:p-fil:&:& <:
d>%2Yj \Y9
,/h&u.%v WqwU4
x gmh&[j sXy z@ (["
`*=:][% /<%']_^ %:-/24& <:"l:
@ &%-[ "%&Q%fi 9&1:(-&:& <:x(d %|{ % [>[(&(=:][% *`}]_

^
/
~ <-<-%&

&%&=%/ 1 << <<&
&=% %/-d-&&` <d-%=&&=<" 9Q&/&%%W%&,<9`:
<9`%&%fi--x`&`0 [

=:-
<d p:%fi%"-P&: ]<fi- <p %fi=<:%&
5-91:
*
`&91 &&%9<d-`:2<d-:p%&W&%&=%/ 1 << <<& &W%"=2<1<:-%\=:<<:
&%%<:W=4=J
M( `"
-1%:-
[/<% <d
W%-<fi:= - (
"%&1<&
+678 % [ [%&"%-
"%<&1%:-_
+ -,./+ [<% < ++ - ( 8+ "
W^`&\%&:-:>%&\=&&%%gq`:-W% <fi, 0 << <&K&,H
+ -,./+
( ++ \
[5%4%:-
+ [,.s+ [p<% < @& 8+ % - ( ++ Ep^%-:-Q%&
n%:=\gK%&"< %%&%fi<:W-K:0g 9
Q-:\-7% [,&% <^>\-&&%%m=:<<:7&%:%:m,-&&%<:"-:
<:%5:\& 5%& &\=&&%%]<:Q<K%&[-&:-
=&= "&- :


fifi7&s"B$$OU$

A\m+$5A`q`.m&q`_A/im_A}$&+`1/A|mA-&Si}&+*cH[}B$&
+$[A.+&|+>A[$d`dq+$/'/M`B/$$/+q>Am$/i$$Im}+$
+B$q5fi`A+ Q|+ :
EB\fim&I A&`$2A|q+$m_`5`d|
Am8 + A/m.8AiqI QqHfi`A+ Q`+ BB\id9$AH&$
mfiA5AI BJ$*I | Q|
JA/'AA$2B`*m*dH/`Hm
fi7m$`$5&oq&+\A&`
Bfi}BI|m$BBs`A/m/B+}&+q
+B+dA7&/
.$&+`A}A/i"BoOs+&&m`"Bfi/$&_
A_.i$fiAB+&A`AA`mq+$}m"5$A
&o+Oq5A/mBB`m\/iA}$/+q+$/"A$/Aq+$/B&.H&+q+&`mq+$[m/'$``A|$
mEA$/$`/Im"&m+*qs
-m+$Hq`.m&q`fi.Am$/m$HEd\Am25A|i/mq+$\A$$AH&$
+[E
AI$AH&$U+\Im$/i$ESfidq`.m&q`U
}$Afi'&Aq+/QBBBm$/i$J
8
Aq`&q+}A
m$/mfimq+s`AO/ |A/m"/qAAB/qH|m`"A/m}mA&'mA
/qfi`$}/iq+$/$q+$7A+$/`[$A'Aq+$BA\&/&+q+$m_E8
&`d8`A5A&/&+q+$m
m+$`'+I/&+q+$[+d/`fiA/i


_/

m/+/qA\mBBm/




_+mJq`BmUA$/Aq+$'7A+$/BA9+/+/&;mA&+AmA}A&;s`|IOq/q
|mq+$/O&H/`|


&5$fim$/+q+$/"A$s$Aq+$\AA+$/


}$q}}A/i;
`_A`

/$92qsBmo+[iiqm&+|Q$_

|`Q+&A`AA`

B9mqm&+9`"fi/\A}&+&A`AA`IAmA&+.mA_/+ $q+$Bm/

AEfi/MA&U"&$$$BB`m}AHH&+q+&+;"Afi$/+q+$/`$U&Aq+/'m/m$$"$A
Aq+$Q`$`A5$AH&H2
`m\/H$AqmA'B+A9$AH&HI2
io+m`


`$`A'A$/Aq+$'$AH&}} ms+2A| sBA`&$\&*9[mO


`$`AA$/Aq+$$AH&B; +>A| sHA`&$&2$m /
_[mO z\ m[}


//$Q$/+q+$/"A$s$Aq+$\AA /m_o+H+/m2&';&+q+&+$

&HAm/mq+$+I/9A`H/ +dS
E[d/`z$HA'A`.+/&`HmA'/ms`
B
B_A`dm\E$B}q&m/m$i EB&B`H/+2$mBA_$&+` `/$&A`A*+
U/m}&+c&/`m/q9fi`\BH&+q+&+2A}+m//$/$q+$/
A_Am+`|m/MmA
&o+Om/\.mAq5;&+q+&+[$Bm$qmA$
q`.m&q`E B$&+A_qA+$d$i|Qi/HB+HmBAA/iUq+s`AOQ$
[Hm+$.`.m&q`IA2E+[A`A'fim ff
Q$I/&+AH/| fi$|&`_}&`
$m&2/iA|
&;s`
Q+`
)$/q9mEB$q_B+A&$.+
dA dHB&/`Q+
HOBH/$
$_B$q5$B$\/dA &+&A`A`|mq+$\m"A_q&;sm
+
m$`BA_&$.
d$q &
`
=_&`$A
!"H
MmQ$/&`}q$}HB$q$
#
9m$`IA}&$.+

% d$ d$}H+/m
& ( '*)+% $fiiqm&+M$
'2sm/2q}_Am+`|m/9$A$ . , E[H+H&m`$/+A+$
A9$A5A$/Aq+$'7A+$*9_A&;s`"
/p
0214365 785 9 m/HA9$A5$AH&>
=;_AA}m+

:<;

?A@

fiBCEDEFHGJILKGEMENOFHP>CEDEFQRCESETUIVEIXWYDEZEM[GJ\]
^`_ba ced fhgjikfhlaYa m"njopq`pst8r uvYwxm8gja!myEa cEfzl|{2}~kgfhgYmy[a|cEfz8fk8_baj_bms}d f`lmsi8{bfa f{bgja}kE}dgjm^`f
ms_baa|cEfcEfd fsvik}d aj_2l~8{2}dhA}ffdj_2}8{bfhg}d|f"_b(a|fd iEd fa fh~kg_bEqka cEf"a|m{bfd}klf}dj_2}8{bfhg>
}d f_b(a fd|iEd fa fh"~kg_E"a|cEfa m{bfd}ffklfhg!t ha|cEfiEd fh_2l}a fhg}ffklmskgja}(ag!}d f_8a fd iEd|fa fh"~kg_bEo
cEfm8m{bfh}lmsEEfhla_fhg}k-a|cEf>kdgja msd8fds~k}ff(aj_kfdg6}d f8fkEfh-_ba cEfgja}kE}ffdy}gjc8_bms!
}k$^6cEf$_b(a fd|iEd faj_bExiEd mikmsd a_mf[iEd|fhg g_bmskgYa cEfd|fh}O{(~Efdg}E_a_m!~8{baj_bi8{_2l}aj_bms!
}kL}d fe_bsf$a cEf_bdga}kE}dfh}8_bEkv6a>d fR}O_bkg6a m_b(a|fd iEd fa>iEd msikmd aj_bmsxa|fd Rgv`fhl}A{ {
ck}a`^`f>f{ _b_bk}a|flmsk_baj_bmsk}O{YiEd msikmd aj_bms-a fd Rg8~8{a_i8{b_bERms~EaO[gma|ck}a^`f>Effha m8fh}O{
ms8{b^`_ba c~Eklmk_a_mk}O{iEd|msikmsd|aj_bmsa|fd Rgvy_Uga cEfRiEd|msikmsd|aj_bmsf[iEd|fhg g_bmsLb b |
nymsd`>jhh8su8a cEf
ff22 8 n8hpOhh|pux njopq 8hpOhh|p hpstr uz "



8c ~kg_y$ Ya cEfRiEd|msikmsd|aj_bmsfEiEd fhg g_mLb b 8fEmsa|fhga|cEfyd}slaj_bmsmya cEf
~Ei8{bfhg_b ck}ffag }aj_2gy"vJmsdfk}i8{bfs b4sJn p|kub 4 [ < _2ga|cEf`yd}slaj_bmsmy8msR}O_b
f{bff8aga ck}a}d|fl c8_{28d f$myqnkuv
g_bEms~Ed"fkfhE_bE$my_b(a mx`^`fEm^ck}OsfgjfR}8aj_2lg6ymd`vJmsd"`^`f
g }O$a ck}anopq`p8rt u _Lnjopq`pst8r u v-ae_2ggmsfaj_bfhg~kgjfy~8{`_bms~Ed"y~Ea ~Ed f-d fhgj~8{bagea
_bklmsd imsd}a f6ik}d aj_2l~8{2}d}O{b~EfhgYymsda cEfa|m{bfd}klfhg_b(a|ma cEfymsd ~8{2}> v c(~kg{bfa tEr Ed fiEd fhgjf8a
cEfeymsd ~8{2}a ck}ffad fhg~8{ag6yd _yzfh}sl|c$A}dj_2}8{bf_2g>d fi8{2}slfh$^`_ba c_bag>A}A{~Ef-}sllmsd_bEa
!rt Ea ck}ffa_2gt v
8i8_2l}O{{^`f}d|f_b8a fd fhgja fh_bl|{bm(gjfhgjf(a|fklfhgEa ck}a_2g8ymsd ~8{2}sg`^_a|cEmyd|ff"A}dj_2}8{bfhgv
ck}a"l}sgjfs_ba>_2g"Emsa>ck}da mxgjcEm^a ck}a>a cEfA}O{b~k}a_mi8{2}hEg"Emd|m{bfsv c8~kg_y_2g"l|{bm(gjfh
^`f^d_a|fRnjoptEr u d}ffa cEfda|ck}nopq`pst8r u v_bk}O{{bs_y"}ffk}d fl|{bm(gjfhymsd ~8{2}sg
^`f"^dj_ba f" _ ynjoptEr u "_bi8{_bfhgnoptJr u v
H4-8HO8[YU(
gY^zf`fEi8{2}O_bEfh_ba cEf_b8a mE8~klaj_bms!8^zf`_bsfgjf-}(a_UlgYa|m8fsd ffhgYmykf{_bfy(lmskg_28fdj_bE}O{{
^`msdj{2Egmffy g_ f mekffh~k}O{{b{_ sf{bsklmsk_baj_bms8_bEms"8}ka cEfRl|cEfhl s_bEa cEfiEd mk}8_{ _baj

msfd`a cEf"d|fhgj~8{baj_bEiEd msk}8_{_baj_2gja dj_bE~Eaj_bms!va|cEf"iEd fs_bms~kg>gjfhlaj_bms!J^zf"8fkEfh^6ck}a`_ba
fh}kg ymdz}gf(a fklf>xa|mkf" g }a_Ugkfhe_bx}>^zmdj{2my!g_ f ~kg_E}a m{bfd}klf>sfhla msde!rt fi
v ff_bsf

nua mf"a cEf"(~Efdmy^`msdj{2Eg_b g~kl ca ck}ffanjoptkr uz v
ff
}
k



!

r
(

`
^

f
8


f
k

E


f






<








_bklf^zf}ffd f"a} _bE}O{{^`msd{UEga mkfefhs~k}A{ {b{ _ sf{bsa cEf8fsd ffemy f{_fy_b
ff_f"X^`_ba c
fhgjifhlaa|
}krt _2g
< n "!x"u
n ""u



< n "u

y# n "u %$ Ea c8_2g8fsd|ffmykf{_bfy_2g6Emsa`^zf{{b8fkEfhv
cEfl}d fy~8{d fh}s8fd>R}Ock}OsfEmaj_2lfh}imsa f8aj_2}O{iEd ms8{bf ^`_ba c$a c8_2g"8fk8_baj_bms!v dj_2laj{b
gjifh} s_bEk^`fgjcEms~8{2^6dj_ba &
f n 'ud}a cEfda|ck}
kg_bklfa cEfgjfa6my ^`msd{UEg~Ek8fdlmskg_U8fd|
}aj_bmsl|{bfh}dj{bx8fikfkEg6ms-a cEf"smEl}E~8{2}d sv (6fklfsEa cEf"8~Ekfd`my^`msd{UEgz_
}O{2gjm8fikfkEg







ms-a cEf>sm[l}ffE~8{U}ffd sv c8~kg8kmsa|c < n u}k) < n *!"eu8fikfkmRa cEf"l|cEm_2lf

+-,/.1032546257892;:50=<#4>09?@257A4B2509CD4EF8GAH4:6I <68JLKM4fiNDE5EF892OND03GM8PCQ@ND2;ND:R?03E257-ND:SE5498:50=G257M82TU4V8PCWCD09TU4 XNDE5EF892OND03GM8PC
GZYA<;KM4E5:[NDG]\AE503\0=E52OND03G&4_^A\AE54:5:OND03GA:`NDG]aUbU,
ced

fif6gihkjlnmRo#pqArslsgt@m`uwvfihxqAqylsg

z{&|>}~xzs99/3s`9kkZ/Z/kZ/3ZZ/33zs=L5{|6|VR9Z9Z93zs/P=k
R#yF 5Z L
R=}
/9/#{5zs]y1{5zs9ks#zsZ6ziZk|V@*F W5



6



]


kL9AkA[{59zsBkL{5zzM>&/&9*kZs9Z*z{6/33{
>/kZ/Z/kZkz{



zsVz3&z{
szZk9siB9zsA
9zs6Z>yF}@AsU=}
kkZysB#)ez39Z)zs
3/s3s}6V#ezw9/9A/
9/
Zy} k/Z6##zsks9z3sz s939yD55 Z ] 9z
/ ] 6] =}Bzs39/*9zs3kZz{&#z3)zsZ%
/%

/zs =kA}5{9) /A9Asy69Z%#SzkeM%zskk/

9zyk96A/ysi /3]6 9Ak##zsknVAkMyZk69z&9A9zye6As/M
s#3/s36A/ys}
k]k3/kzs@[zSZZA>zs&PkZeAU&&*A)zV3kA}]zs/PZ9sA/zs
9/)R 6] &*zs)y6AS3Fk3/AU}F%/3kA#)y/9ZsP/{5zs
3Z9=M%yA*z{@ 6R 6] *z#3Fk3/A%{5zsk=9}%F%zs=kZ*9z
kAy&#9%9k*9zkZz{#3Fk3/AkAZ]#k3/ ] 9z/ -M-Os=/3Z5ZU {
{5zsM&kZk*y%
/k)Zk9s> 9] }>zsz9Z
9keZZZe9/Mx3zs/ P9Z/3LkA9/`9 ] R9_k;/k96kzsy/`z{9k9=
9s*_Zs}~xzs]3/ksR ] =9/39Z93/s3kzsM3ZZk=9*zs
ZsZk9/y3zs/_9ZkA}~xzs69]Zy/kZVz{19&//ZA#&s]9/6y1kz#Aks&/A
9ZsZk9/y3zs/_9ZkA}_=s3P3sB#3/A3ZsZk9/y&3zs/_9Z/3%9z/z/=kZ
9zA99/3zs/_9Z/3s}kzzs3/A3ez#Aks/s)9zks3nzs/zs9
kzsM_ZsR3/3Z6Znzs/&As/ZeA})~xzs&9zk/_/A#99k
ekPz@Ryznze9_k9z//{5zs9Mfi3zs/zs/Z/ZsZk9/yV3zs/ P9Z/3s}~zs
/=/3sfi*/zk9_k9zz9/){5zz#3zs/zs/9k)Zk)9zs/=e9Z
9/ ] #ZsZk9/y)3zs/_9ZkA k ] kzkA#zs#/]ezs5/9AZ9AZk//
A/y/Z#ZZ9Z9`/ U ] 3zs/ P9Ze`{5zs kzsy_Z#6Zy/9zyk*9
3zs/zs/>9&Zk3Ak3/s3V3z/zs/-}]/3#3zs/3Zk9=9&zs/9*s/sA
9kB//Zy9k6AkV3zsZ=#zk6ZsA#z{`eZ9AA}
sZ{ ] ZsZe/y3zs/_9ZkA@yzs]3kA}~xzs]3/ks[Lyn
9]Zs>9/ 6] Rze99A#/Z#ZZ))3
/*
*3`{ zs#zs s#sZ3`9ss}`_
9k>Zssx{ zs>e*/9P3k@ x9]sB9zV##
zV3kA
} BzM#ZsZys]ZZ>s;{
9> )
s9zV#M1zkUx9k6Zs/&;/_/3L9]ze9zs/]nzsVsz9z}
&ysz/9)zskZ6k)3zs/_kZ)9 O3 / k=Z69/)9y}6~zs
kPZ

fiff ]k/ z
{ Sik
{ S6&s9A9A;zM#Z6nzs/z
{ S} OA z{
AZ/3]B9>]z{
9>k/Zn9/;Z
k{ k{
]

]

#k{13k=`{5zs#eAZ/3>/zs/kA{59zsn3z&kZsZ{@966kzeA;zsy}
3 Bk3/A/yzszs/_s6Z9&P
kZzs9A#6AsB/Z>/zs/*z
{ S}R {@ ]






kzkA3ey9Z ]
k{ ]
P ]
}/3s;{ zsk @ ;9
AZ/3 6] >y fiyL/zs/kA { 9z /zs)//3z& ;9*/ k{
y6yiU3kA} e/-A#SkzBzs@/ysR9z6#zs9Vnzs1R9zskZz{izs3k9Z/3;{5zs@/9P3k
MMABz{&@ }R&ZzAZeB9]//y[{ zs z{
zs>k3/kPz@}

!#"%$!'&)(#*,+

{

k{ 6] /

]


-.

% 6]


]

fi/103234)57698:53;3<=4)>?03234A@B03C3D%61E36GF23H3;I57JLK
MONPRQTSVUWYXZP\[L]O^[L_RS\S`aO[=bdc!PeQ3Sf]gPeQ3SihjlknmojVjpZqBrejsutvjvq\txwzy{kntY|Ljfwz}~c,_oWPePRSf]z_lyu}~c7W%X
^SVO]3S^T[XPRQ3SVNN]bWWP=,NPRQ3Sf_RWYXZS\_lyu}~^NSX]3NPSVUW%XoP
S\ebNXZSPRQWYXXZSVPoWN]gWPeQg[vSf9_eSfB[L_R3XN]Na3_^SVO]W#PZW#N]!W#_XZP?]3NPRSPRQO[LP=c3SfSf]aOXlW]3
PRQWYX^SVO]W#PZW#N]!c1PeQ3Sf_RSB[L_RSB[L]f[XZSXQ3Sf_RSPRQ3S^Sf_RSfSNLMOSVbW#SV\^NSX]3NPSVUWYXZPNLSfSf_c
[XXoNSNLNa3_bY[LPRSf_SVUO[Lb#SXXZQ3NL\cW#]:B[L]
XlWPeaO[LPZW#N]OXPRQ3S]3N]3SVUWYXZPRSf]OVSTNL[z^Sf_RSfSNL
MOSVbW#SVif[L]MSga3]O^Sf_VXZPRNNI^fiW#]PRaW#PZW#SVb#GvvN_W]OXoP[L]OVScXZSfSzUO[Lb#STOA[L]O^fiPRQ3SzXZa3MOXZS`a3Sf]P
^WYXRVaOXRXW#N],V STVNabY^,c[=b#PRSf_R]O[LPZW# SVb#c?QO[=STP[LSf]fiPRQ3Sz^Sf_RSfSTNL\MSVbW#SVB PeNMSgPeQ3SW#]PRSf_RL[=b
^SVO]3S^MbW#A bW#W]f _ lyu}~[L]O^bW#A bW#XZa3 _ lyu}~c33_RNLWY^S^
S[eQBNnPRQ3SfSVUWYXZP1QWYXNabY^QO[=S?MOSfSf][OSf_oSVPZb#_RS[XZN]O[LMb#SRQ3NLWYVSNXZPNLPRQ3S_eSXZab#PX
SXZP[LPeSNab%^ANzPRQ3_RNa3Q:W#PRQSf_ezbW#PRPZb#SgeQO[L]3SWSQO[^P[LSf]:PRQWYX^SVO]W#PZW#N]!a3_
^SVO]W#PZW#N]zXW#bWOSX?PRQ3S\SVU3ONXlWPoWN]gXbW#QPZb#
W#]O[=bb#c3S?_RSfB[L_RPRQO[LPW#PB[=BXZSfSfa3]3_RS[XZN]O[LMb#SPRNP[LSbWWPVXWS?]3NLPeQ3S\^NB[=W#]
XW#fS\N_QO[=S\[MONa3]O^N]PRQ3S\^NB[=W#]gXW#fSbS[n_Zb#c3W1S]3NL[L]O^ c3PRQ3Sf]W#PXZSfSfBXN_RS
_RS[XoN]O[LMb#SPRNaOXoS_ _[LPeQ3Sf_PRQO[n]_[X!Na3_^Sf_eSfSNn)MOSVbW#SVRd]O^SfS^,c3[XXZQ3NL]W#]BZ[fRQaOX
SfP[b%c=L3c,[L]TNL PRQ3SW#ON_RP[L]P3_RNSf_RPZW#SXPRQO[LPQ3Nnb%^vN_\PRQ3S^Sf_RSfSNLMOSVbW#SV^SVO]3S^
M_Q3NLbY^vN__ cN_[bbeQ3NLWYVSXNL[L]O^ Q3SVN]3]3SVPZW#N]fiPRNB[UW#a3Sf]PR_RN
PRQO[LPSB[LSW#]PeQW%X?O[nOSf_\Q3Nnb%^3XN]b#z[LPPRQ3SbWWP=cM3a3PMOSf[LaOXoSl[XNa3_\3_eNNLdX?XZQ3NLPRQ3S
VN]Sf_RSf]OVSWYX_V[LWY^,cPRQ3S^Sf_RSfSNL,MOSVbW#SV_VZyu}~\WYXPZWYf[=bb#B[Sf_RNN3^[L33_RN=UW[LPZW#N]
PRNB_ ly}~\cSfSf]N_N3^Sf_[LPeSVbbY[L_eS\[L]O^N3^Sf_[LPRSVb#XZB[=bb
z7O,7vd3)GO!
!vLO=!L%!fif:g3vzO=)
Q3SW%^S[NLB[=UW#W#VW]3jw)xmRpVO:QO[Xb%[=S^fi[L]:W#N_RP[n]P_eNLb#SW#]B[n]OSVbY^3XfcW#]Oeb#aO^W#]3
PRQ3SXZPRaO^TNL13_eNMO[LMWbW%XoPZWYNI^SVbYXN_W#]Sf_e_ZW#]3^Sf_RSfSX?NLMSVbWSVl[=]3SXcQO[n]3]3N
]
S[=Sf_c=Ld]PRQ3SBXW#b#SXZPXZSfPRPZW#]3OcSf[L]TW#SfSf]Pe_RN[X\[_RS[b vL[=b#a3S^a3]OVPoWN]zN]
O]W#PRS3_RNMO[LMWbW#PZBXZO[VSXfv WYX1[?O]W#PRS?XZSfP1[L]O^ WYX1[?3_eNMO[LMWbWPZS[XZa3_RSN
] cLPRQ3SSf]PR_RN







%
W
?
X

^
V



3
]



^
R
P

N



fi

ff
















#
b
]
















P
n
[






#
b
]











]3S?XZP[L]O^3[L_V^[n3bWYf[LPoWN]BNL,Sf]Pe_RNWYXPRQ3SNLbb#NLW#]3Oa33NXZSS]3NL:PRQ3SXZO[VS cM3a3P
QO[=S\N]bO[L_RPZWY[=bW]vN_R[LPZW#N]z[LMONa3P cSVUI3_RSXeXZS^W]PRQ3SvN_RNnVN]OXZPR_[W]PXf?3N_SVU7[LbSc
SW#QPQO[SB[VN]OXoPR_[=W#]PXZaOeQ
[X = !L "$ #3fi
%bPeQ3Na3Q:PRQ3Sf_RSB[=gMSB[L]
S[Xoa3_RS&X PRQO[LP[L_RSzVN]OXlW%XoPRSf]PW#PRQQO[LPST]3NL\cPRQ3SOmVtx(w 'ftu)sjTpZ*
q )&+$,-t )./) jw)xmRpVO
XZa3SXZPXPRQO[nPBS[^N3PPRQO[L0
P 21QWYRQQO[XPRQ3Sb%[n_RSXZPSf]Pe_RN
[LN]3
[bbPRQ3STVN]OXWYXZPRSf]P
ONXRXW#MWbW#PZW#SXf4
3?XW#]3zPRQ3S[n33_RN3_ZWY[LPRS^SVO]W#PZW#N]OXfcW#PBf[L]:MSXZQ3NL]:PRQO[LPPRQ3Sf_eSWYX[gXoSf]OXZSW#]
QWYeQzPRQWYX 1 W#]OVN_eON_[nPRSX?PRQ3
5eb#S[XZ7P 6[^3^WPoWN]O[=bW#]N_eB[LPZW#N]ARQO[L]3]3N8
] S[=Sf_cLV
3N_1SVUO[Lb#ScWSQO[=S?]3NVN]OXZPR_[=W#]PXN
] cPRQ3Sffi
] 1 Wbb!MOSPRQ3SS[XZa3_RS?PRQO[LP[XRXW#]OXS`aO[=b
3_RNMO[nMWbW#PZzPRN[=bb1SVb#SfSf]PXNL9 & :Na3Qb#zXZOS[nW#]3O;c 1 [XRXlW]OX?3_RNMO[LMWbW#PZW#SX[X?S`aO[=bbA[X
ONXRXW#Mb#SnWSf]PRQ3S\VN]OXoPR_[=W#]PXf
!-<>==!@?d,z2A%CBf:L!DBf=OOEB
F W#S?B[=UW#a3Sf]PR_RNcPeQ3S_[n]O^N& vN_ZbY^3XSfPRQ3NI^WYX[=bYXZN\aOXZS^PeN\^SfPRSf_RW#]3S\^Sf_RSfSXNL!MSG
bW#SVW S#c3_eNMO[LMWbWPoWSX _eSVb%[nPZW#SPRN[]3NLb#S^SMO[XoSD %XWY^S_eN9PeQW%XcLWYXPRQ3Sf_RS[L]VN]3]3SVPoWN]
MOSfPZSfSf]PRQ3SPZNW%^S[IX H!VNa3_XZScLPRQ3Sf_eS1WYXPRQ3S_[nPRQ3Sf_Pe_ZW#WY[=b3NMOXZSf_RL[LPZW#N]PRQO[LP_[L]O^N&
vN_ZbY^3X
VN]OXWY^Sf_X[a3]WvN_R3_RNMO[LMWbW#PZB^WYXZPR_oWM3a3PoWN]TdNLSf_PRQ3SXZSfPNL,N_ZbY^3X1XR[nPZWYXW#]3}~Vc[L]O^W#PW%X

JLK

fiMONQPSRUTWVXZYU[E\]T]NU^DV;_a`9P[E[bT]N
cZdIefeghSiUj$cOifikmlCn$kkmlUdOoUiSpfqj]rmsutvpxwykmryp{zUoUkyp{j]ifij$|]dGr}n$iS~&wydGk}lCn]wkmlUdOlSp{]lUdEwykCj/wwp{zSeddGiSkmrmjv/~]oUk
p{ikmlSpxwwydEIkyp{j]icZdwlUjLcn$iUj]kmlUdGrbdGi/kprdIe~tvpfDdGrmdGi/k&n$iCtsoCl tSdGdGWdGrE}Ij]iUiUdEIkpjvizCdGkycZdGdGi
r7n$iCtSjvs&gcZj]ryextUwZn$iCt&klUdUryp{iCp{Se{dj$q;snbSpsoUsdGiSkmrmj]S~]ZlSpxwIjviUiUdEIkyp{j]ilUj$extUwCm$b]I&-/v
}0-IE}-S&b($Of7b]$G-S]Z}SGG(S(7CmI$7]$7CG$(-EZikmlSpxw
Gn]wydcZdGniIj]iCwptSdGrUrmj]zCn$zSpfefp{k~tvpxwykmryp{zUoUkyp{j]iCw(n$iCtp{iCn$rmkypxIoSexn$rZkmlUdsnL/p{soUs&gdGiSkmrmj]S~&tvpwg
kmryp{zUoUkyp{j]iDj$|]dGrkmlUd&wydGkjq9n$kmjvswGOkj]swn$rmdj$q9IjvoUr7wyd&|]dGr~tvpfDdGrmdGi/kqrj]sWj/wmwp{zSe{dficjvryextUwG
qj]rp{iCwyk7n$iCId]kmlUdGrmd*n$rmdj]iSe{~CiSp{kmdIe{~sn$iS~j$qklUdGsp{iCtSdGCdGiCtSdGiSkj$qkmlUd*tSj]snbp{i4wp{Gd7
oUrkmlUdGrms&j]rd]$kmlUdOsnbSp{soUs&gdGi/krmj]S~tvpxwykmryp{zUoUkyp{j]iCw9cdOIj]iCwptSdGrZcZpfefe(ky~SSpxGnbefe~0iUj]k}zWdOoUiSpfqj]rs
dG|]dGrmkmlUdIe{dEwmwGsnbSp{soUsdGiSkmrmjv/~p{ikmlSpxwiUdGcuwyCn]IdGn$ikmdIefeOoCw&nfie{j]kfin$zCj]oUkkmlUdtSdG]rmdGdEwj$q
zCdIefp{dIqtSdICiUdEt*zS~rIn$iCtSj]scZj]ryextUwGi*Cn$rmkypxIoSexn$rEkmlSpxwIj]iUiUdEIkyp{j]icZpfefe9nLee{j$coCwkmjfioCwyd&snL/p{g
soUsdGiSkmrmjv/~n]wn0kmjSj$e;qj]rIj]s&UoUkpiUtSdGvrmdGdEwj$qzWdIefpdIqmdzCdIefp{dG|]dklCn$kklUdrmdEwykrypxIkyp{j]ikmj
oUiCn$rm~UrdEtvpGnkmdEwpxwOiUdEIdEwmwn$rm~fiqj]rOkmlUdIj]iUiUdEIkyp{j]i*cZdn$rmdn$zCjvoUkkmj0sn$h]d]
iCtSdGdEt;n]we{j]iUn]w
kmlUdh/iUj$cZe{dEtS]dzCn]wydsn$hvdEwoCwydj$qnzSp{iCn$rm~&UrdEtvpGnkmdwy~SszWj$ej]r}oUiCn$r~qoUiCIkpjviwy~SszCj$e7ScZd
wyoCwyWdEIkklCn$kZkmlUdGrmdpxwOiUj0oCwydIqoSeIj]iUiUdEIkyp{j]izWdGkycdGdGiklUdkcZj&n$UUrmjSn]mlUdEwn$kOnbefewydGd]dEIkyp{j]i*
qj]rwyj]s0dtvpxwmIoCwmwpjviD
dGkzCdkmlUdwoUzSeniU]oCn$]dj$q}cOlUdGrmdjviSe~oUiCn$rm~UrmdEtvpxGn$kmdw~/szCjew0n$iCtIj]iCwyk7ni/k
wy~SszCj$exwZn$UCdEnr}p{ifiqj]rmsoSexn]wGvp{iCnrmkypxIoSexn$rEUcZdn]wmwyoUs&dkmlCn$kZdE]oCnLep{ky~&zWdGkcZdGdGikmdGrsw}tSjSdEwiUj]k
jUGIoUrp{i&qj]rmsoSexn]wp{i ZyZdEGnbefe(kmlCnk;pifi ]cZdnLee{j$cdEvoCnbefpky~&zWdGkycdGdGi0kmdGrmswG$zUoUkZtvpxwmnbefe{jLc
dEvoCnbefpky~zCdGkycZdGdGiUrmj]Wj]rmkyp{j]idIQUrdEwmwp{j]iCwGf dGk}zCd*kmlUd*Ij]rmrmdEwCj]iCtvp{iUwyoUzSexn$iU]oCn]dj$q
ZikmlSpxwwyoUzCwydEIkyp{j]iDcZdwylUj$ckmlCn$kOklUddIUUrmdEwmwp{|]dCj$cZdGrj$q}n0h/iUj$cZe{dEtS]dzCn]wyd&p{i*kmlUd
exn$iU]oCn$vd9fipxw;]oSp{kmdefp{sp{kmdEt}iqn]IkbLwyoCl&nGn$idEwmwdGi/kypxnbefe{~j]iSe{~Sexn]IdOIj]iCwykr7nbp{i/kIwDj]ikmlUd
Urmj]Wj]rmkyp{j]iCwj$qkmlUdn$kj]swGqCcZdZkmlUdGikmlSp{iUhj$qCkmlUdEwdn]w;Ij]iCwykmrInbp{i/k7wj]ikmlUdIUrmj]zCn$zSpfefp{kyp{dEwj$qCkmlUd
n$kmjvswmUvkmlUdGicZdlCnE|vdkmlUdp{iU]rmdEtvp{dGi/kIwiUdEIdEwmwmn$rm~fikmj&n$USe{~snbSp{soUsudGi/krmj]S~]i]dEIkyp{j]i*U
cZdOwylUj$ckmlCn$k;kmlUdGrmdpxw}nwykmrj]iUIjviUiUdEIkyp{j]i&zCdGkycZdGdGi&kmlUdZsnL/p{soUs&gdGiSkmrmj]S~tvpxwykmrpzUoUkpjvi&qj]oUiCt
kmlSpxwc9nb~niCtfikmlUdtSdG]rmdGdj$qzWdIefpdIqZ]dGiUdGr7n$kdEtfiz/~fir7n$iCtSj]s0gcjvryextUwZs&dGkmlUjUt
jwydGdficOlCn$kIj]iCwkmr7nbp{iSk7wnfiqj]rmsoSexnSexn]IdEwj]iklUd&Urmj]zCn$zSpfefp{kyp{dEwj$qn$kmjvswGWp{kpxwoCwdIqoSeOkmj
Ij]iS|]dGrmkOklUdqj]rsoSexnkjn0IdGrmk7nbp{iGniUj]iSpxGnbeqj]rmswOnCrIwykwykmdGkjtSj$p{iUklSpwUcZdqj]rsnbefp{Gd
kmlUdtSdICiSp{kyp{j]ij$qn$kmjvs>$p{|]dGifip{iklUdOp{iSkmrmjUtSoCIkyp{j]iD dGk4L $EEbm ]Ij]iCwpwkjqkmlUdoUiCn$rm~
UrmdEtvpxGn$kmdwy~SszWj$exwZp{ikmlUd|]jUGn$zUoSexn$rm~
fivD{EDD v$y$LpxwIj]i$yoUiCIkyp{j]ijq}kmlUdqj]rsb ; 27WcOlUdGrmd
dEn]l pxwdIp{kmlUdGr jvr $p{iCIdkmlUd|Ln$rypxn$zSe{d& pxwZp{rmrmdIe{dG|$n$i/kkj&j]oUrIj]iCIdGrmiCwGWcZdky~/SpxGnbefe{~
wyoUUUrmdEwwZp{kn$iCttSdEwmIryp{zCdn$inkmj]sun]wOnIj]i$yoUiCIkyp{j]i*j$qkmlUdqjvrmsu EE
j]kmdklCn$k&kmlUdGrmdn$rd 9
n$kj]swj$|]dGr&@n$iCt kmlCn$kfikmlU dG~n$rmdsoUkmoCnbefe{~dICeoCwp|vdn$iCt
dIUlCn$oCwykyp{|]d]ZlUrmjvoU]lUj]oUkkmlSpxwCn$CdGrEcZdoCwyd
@kmjtSdGiUj]kmd
n$iCtfiff EbEbm ff kmjtSdGiUj]kmdkmlUd
n$kmjvswjL|vdGrfiSefpxwykmdEtfip{i*wyj]s&dUUdEtj]rItSdGrE
8 ( ZlUdGrmdnrm

n$kmjvswZj$|]dGrOa $ b G !ff 4 " $ ff# $ $
ff%&
" $' ff (
) $
9lUd v$ CmIQ-$ 7+
*,*-ff . *,* / EbEm *,*-ff . *,* /cZpfefeSexnb~ nwp{]iSpf(Gn$i/krmj$e{dp{i
j]oUr&kdEmlUiSpxGnbetSdG|]dIe{j]Us0dGi/kE
kfikmoUrmiCw0j]oUk&klCn$k pxwn*rIn$kmlUdGr&cZdEn$hexn$iU]oCn$]0d *nqj]rmsoSexn
2
1*}tSjSdEwZefp{kmkye{ds&j]rmdklCn$iIj]iCwkmr7nbp{iklUdUrmj]Wj]rmkyp{j]ijqkmlUdn$kmj]sfiwG
i*j]klUdGrOcZj]r7tUwGSqj]r
354'687:9;7=<?>@9;A#B;CD>EB<?>EFHGJI@KLINMO99;7=P;MRQSB;PUTE>EFVD77XWOB;7YFOZ57YZ8B;I[B;CO7:T\>@P;7]^CO7Y9;7B;C57?_`a<!7YFHBbSINFOPU7YcHMd>\Q-bSBG.e VOM5B
B;CO7U7XWOB;9f>[TYIN<!gRQS7XWHbSBhGINVOP;TYMO9;7YP<?>EFHGJIEKB;CO7U7YP;P;7YFHBbi>\QbSZO7E>EPY4
jOj

fiklmnLoqpsrtouv5nLwJlmnyxzl{|}p~pmuoqD
Y'@y[HDfi''DfiO0 ,ddH E
},a }@aE
0 ,fiE00EY,0fiNEOEX,0'
DEEOY'N0',Y,0'd#E00EY,0'D#DE0zRz[0EzN0 NtY .0N\}z0@ H@fi,
^ @0'0@Y,0':dHUE ,OH'.YE;0N H!'D\iLHY,0 OYEOE0'EY,0'H D'N0',Y,0'5
E00EY,0' O:00NEO@X,0#D!H5UfX! O0N0H ,H'N0 D[H0HOD
HD)D;EH)8NEOEYOD0EY'H@,'NY,)EJ; d[H?"EhD'D0Jh?'\O
,0H$D )@J'.Y#Y@H)ED.D ,,zzd,
H EE0
h?@'\ [ h0@zD
NEOEX,YfiD')EH h0N5"@0'0@Y,0$EH@z[ $DE0
hE0'EY,0$@HEzH:fh#5hY'YN;
EzD0DDY,0'
EHX, LHDY,0'EE'D8i#N,t',0ON\t')a)N0
,
EOYJYEH''
EO @?E'D[(?E.D'=0@O E
Y'O@h5NDLDN0@.(? h@z[[D
OENY,0
!,O},[ fi0;D
;8;HNDdR!&h!'D, 0
h5"D0HU@HEz:DE0@,-=\,
?HEhDDED0H0'EDE0}EHE}DN;hD?H0HEaN N@,H
D[E
' 0
h5UUh'=\0
!,O}U
=@,YO#YH EH'N( hJ,NDD';.0HDN,})h='N\D
N0DY'NY,0'H?HEO0@$N0DY'N?h0DUEJ;DD[,'
yf+ff:?tXiR=N?fX:
Uf! f,R=EN0?HEJD'fDEDE0
h[@HEz[D'
h?X,Y,00
#=YU0 J[Y=!Y0DE[E0
t[Y 5#Y0DE[D')Y8N0'Y.
@EHE0@00}='N?HD0N.d'0EE 5D'
5;0 ff

fi8N0DY'N.H0HD

,8N0 .5,$0Ea[Y 5?D'fi JE=. 0E8@'D#@OYh0Y8N'\'DEX, ,$
, ,z5
N0'XhY@H'N)EO0 ,EHH.R
5
DH#HDN) h8 hD0H!;J.i hy.DD';N0DHD. "!DE$#

DDH %! &R\8D( '#.5hD['E5. )* \ &0 %,+hDHaDN
.
- %[.D O8Da hD0HL
HD.
0
)
/- haNDL';.05HDN.

[JE D"DE h[EHEH$'DU5@H0EHz)@ }:'DHO'HD';0',)E'H'
8EHzEz@'D#@,H0EfiDUE;0E
h
/- hJ= hH5,$N'0H Yh5U,fiE,H0EfiD1-
0'@
,D?\HHz:,HEH ?,)E@H8 N'O EHEzDDEzH
H0EH3
25 4hz0HH.57 6ODY,0D
8Uh5,9
4; : 7 < =J5,'H@><@?A?0 NXO:,EE:h
0HH.di7 6OD\D'?[N,;D?
EOY ,U? h@E5^E'D"J'.\E;0. H^E
}[,E0 ,8'DE
EOhHDEOhJO0 ,ddH
E)0
[,Efi0 ,a HE;0z'D Y'HOYY,'
B[00 ,aYOD0,'
E h}'OH'Y;0#'D Y'O;0E
hY'@0 C - 'Y ;0E
h0[.d,'0#dYhD ,
D)0EHJE'DafiHDfi'
D0O 0.Xh
EEN0
DE'D Y'HO8[ hhJ' EX, ,'OHD'\
,EH.5!Y ;0E
h)HDfi.5,$D08aD'0
D)E0HEHOF
E?E D!'\OJEED} OD'
NEH':,[E
E0'EY,0)Y.DEHH.R"X)'DEYhN hDO?YD#:E'U;0?D.- EHEh?D
D5,HH
-
G ? h@$'0OYEO0'D Y'H.0[OYEO E0'EY,0'H
0YhN0D[H0HOE'D8Y'@t$@OY , O08}y'Nz[z5,D
H0H X,D,) ,'DE


EOhHDE,EhD0'D00 0N'D ,0?E E
}J
ID KYM LND8@,ODY,HOOY@O
'D\iLHY,0t'OHD'\O
K=P LND?.5,D'0
0EtD'Q
'Yfi
'Y
EHzdt@ E
RTS

fiUCVXWZY\[^]1_a`\b@cA[AV\dM]fehgWib@bj[AV

kNlmAn^opm*qsr^mAtNuwvAxy{z|t~};ok$}7tNurZ}7zy*Nn\No@}ly*tNo@kPo@yAluJy\}t}mzyjZon\tNuwm*qsz\o@kt~}7z\No@yj;7
ZmZo@k1yA\o\n\No@kNk}7AoCn^m*oj auZ}ksku\mTffkMtNuy{t tNu\ooly{zproaz\m

ly*z\mzZ}lyT\qmA

;}7Aoau\omANo\5qmAa~}lu\oy{z\Axy*Ao@kauZ}k1}kNkx\off}kmAz\offm*qMtNu\off

Ctu\omANovxZ}to

yj}7zNo@yAkmAzk1CuZpaoCNo@ktN~}lt

tNu\o>tNm yFx\zy*y{z\Axy*Aoff}7ztuZ}kny*n^o@aNAoo.Ao@lt~}7mAzOqmAaqx\NtNu\o}kNlxkNk}mzM;
}7AozOy*zZ.qmAxZy"}7zOly*z\mAzZ}lyjfqmANaoly{z}7ppo@}y*tNo7Zo~}7AoFqNm}7t@\}7zy k~ZzZtyAlt~}l


y*z\z\ojypk~otCm{qlmAzktNyj}7zZtkmAztNu\on^m|kNk}7rZ7on\NmAn^mANt~}7mAzkam{qy*tmA

k

M7@MM7Mff otF>r^o>}7zOly{z\mAzZ}lyj qmAN01wolmzk~tNNxltyqmAxZyp,N>1}7zOtu\offy{z\
Axy*osm*qNo@yjlmZk~o@Fo\ka}oA@m*AoftNu\oaAm\ly*r\xZy*Np*\@A$AaAyAkPqm*;7m*k*Cu\oNoa*@@jNN^
y*NoffqNo@k~u*y*~}y*rZ7o@kF}k~t~}7zltqmAtNu\o>tNm*7oy*zloFTy*}y{rZo@kff$@
o>onZyloo@yAlumXllx\Nozlom*q1tNu\offqmANxZypj1rZ\1\
o0NonZyAloo@yAlum\llx\NNozlom*qp|ff rZH
| ~1r|

y*zNonZyAloo@yAlum\llx\NNozlom*q



o>onZyloo@yAlumXllx\Nozlom*q7 7 Fr|


mAt~}loCtNuy*tafN>F uyAktamfft~|n^o@k1m*qMTy{~}y*rZ7o@k1tNu\oCz\oTy*}y{rZo@k tNuy*t1aofxkt1}7z|tNmXZxlo@
y*z.tNu\o>tNm*7oy{zlo>Ty*~}y*rZ7o@ka$zmAZoatNmo;}7}7zy*tNoFtNu\oZon^ozZozlomAz

tu\oCy*tNto@Zoffm*qtNoz

lmAzk}ZofftNu\offqmANxZypfN>p\
fqmAk~mpotm*7oy*zloo@ltNmApM



M7@MMM

Ao@ltNmk}7z


}7Aoz y.qmANxZy. m*Ao>tNu\oTy*}y{rZo@k*j@@NF7otA~ MMr^optNu\opk~ot>m*q




{kNy*t}kq}7z\pimAN yT7A}q> @j@~ H



5\@

tNu\ozQ|j@j@NAsA~ Mi}H
M7@MM7f

ZCu\oo}kffyTyTxy{t~}7mAzOk~xlutuy*tCp

au\oA5\Q ff
. fi>

MZoz\mAto@ ;>i}kZoz\o@tm


r^optNu\o

l7m|k~x\NoFm*qaA fN>\

qF>}kFz\mAt>}7zHly*z\mAzZ}lyjaqmANao
No@k~n^o@lt~}7Ao7ACu\oNo

>


Zoz\of>y{z ;>ffstNm0ro

}katu\o>qmANxZyF}7zOly*z\mAzZ}lyjfqmANo@vxZ}7Tyj7ozZt>tNm>mr\tyj}7z\o@0r|tNu\o

n\Nm\lo@Zx\Noy{n\no@y*~}7z\.}7ztNu\o>n\NmZm*q1m*qsau\omANo

\

!#"%$'&)(JP+* ot!,ro- *.-0/$|\a}7tNutu\oy*tNmA

>
au\oly*z\mzZ}lyT qmANxZy

>


7 8- #9

k1o\no@ltNo@ >


kmAZoNo@yAk}z21fy*pnZ7oi43\65mAzk}Zo

;:- <9=- / >: @?

o@vxZ}7Tyj7ozZt>tNm>}kBA

|!C*<9
H

f>
>y*z >


lmAzk~tyj}7zk1r^mAtNuw7

|ff!D*#9

^7

C 7 >y*zO

k~oo>tNuy{t7ff~7 }oA7Zj1}k>y{n\n\Nmj|}7
;> KJ j@j@N

7 FE

FGH

(}( oA7A C

y*z tNm>r^o\fwoffyjk~m

y*to^sy*tCpmZk~t>IT\1au\oNoqmANoA


A8G

IT\ C

Dff

L>

MON;P#Q RTS UBU0VXWSTVYVXWQ[Z \4].^_`baYcedfS8gXQ Vihkjml npoRqc.rsgt4gXVXg!cTdfSuUBU0v8wVXxyU4Q gYVXWzSeV{Se|XQVXWsQ!UBt4}~t4VcedfS8gXQ xsQ rsR Q8cTd
v8wVXxyU4Q gpt4rh@N
ON;0c.VXQ6VXWzSeV)WsQ |XQ6%QiST|XQ6.t4Q %t4r[)'Seg0S~d+c.|X}ixOUS~t4rF<xsrsQ |0VXWQ6VX|STrgUSTVt4c.r@sQzrQ FQuSe|UBt4Q | %Q)scVXWOt4g
VXWs|Xc.xs.Wsc.xsV6VXWQ6yzSeyzQ |)%t4VXWsc.xsVpd+xs|XVXWQq|pRqc.}Y}YQqrV N

z

fii<<8i)

#+<p;ss#<%#2;%#
+T T2Y 2

#Y eT;e .[T+<ss e;ff#e~

T8Xff . + =fT[+T b+T Y)+


{#+s##Xp8

+ 2[Y u

KYF.;qi=



e

>BOqe +B8qff+ sTz+ F %ff+
TT[+s86T[T; Tu ;e8 TTeTs=




e

eTs

Y+T

fiff




.;T








T;Fq





@TT;T[s.T 6



{=.{

%ssseT;ffTTYTT F#
Te;6 6T;



T;@T@TT {6 %T;

<ssTe;F





O% {6

)Y{T 6TF [TYTT

ue ~+;T ~#zY

' .%TTe = T@Y 8sF+ff

{+

TT8T ;e
Te T;

fi ff !
#"$ %

#f'& s(*),+,-Y;
+s[se uz+T. T[YT
.
;/ ; ;s! Y+eTs Y+2TF0.+@21XeT ;+q.~4367
5 8:9<;>=


5
7
L
? =A@CB
EDGF IH:JK
8 @ = 9<; ee@;=[Y u= qNM PO
;eT; q,Q
+RDGF IH:JK L 57 8 = 9<;
9<; ; ;TT;FqFA@>%X
TTGD2F SHTJUK 7L 5 8NV X
W = 9<;
{. ffu2D2F SH:JK L 57 8 = 9<; i!s.sTTu+E
T+sTF
>Y
. ffY
eF+ ++s86(Z[O
zY 6TT@T[ .8 eTsY+TE
]\
zY[s +eFT.+@_
^ +T ..T.+;q TTe b;. + #


+ T= TT[+ff

. + Ts.+.



` #X;badc ff ffffSe KS KSf ffg[h %Ubi j Olkmj "
Jfi F nKS oHpqK% r ff ,Hs
Ufto",H
gh oK'u wvj Oxkxj K hz c c "$ { g U <
{6 gtDGF IH:JK L 57 8 = <
{
9 ; }|~ "U
J
9 ;
c ff
L4
L4
[5 D2F SH:JK L 57 8 = 9<; wOv<O
I5

Z u



i. ffu +!b +~XT eF+

T.+@++ T=Y eTs

T;YT+s8uFTe
Y+T



T;

e

.

+@;

uTT beT

0.
Y!sT +=T. ;.Tff + =
]

,( ( @ .e; + +s Tez
@X Y+++.

2
TFz


XT +



'pXz;q

%'

GA V W; 9<;

;




TTT









;

V


P





*(2

C

. . @TT8;+{eFT=T. eTs ;;
;6e;e + +

;uTffTs

hzff[;






ff. ;







(Wi

? AOY;T + + T2{ @+ L
? ~

. + e .@+p
? + T'F
(

;+>
Z[
wO2 Z[O
8


E q T[8 eq . +

Te =TA
.+.T X
;. ~T=@ ;eY
L L>

L ?p+ T-uz8YTt^; +ff + @;s+ 8iTYX;. + X YT z+s8


[

! FT@ u8. ;.Tffu T;fifi
s[s



Y{ [. Tes ; ;+2T



fi6zz*,,zd*0,

#worlds4

||P(x)|| x



0

0.25

0.5

0.75

1

X,zn,ww,fi]P',S,$NzG2

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

X,zt],S[w,fizzSz,GSz,fio,6,wzUS}
SU*#wREz,,w0z*#NzPbz*,z,ww:Nzpzz*z

Rw,d>6,#S[Gnz,,w0<Uzz0RNnI2
fi zt
,<0o
wzw#Nzwzn%,>zzSq$,w0S!tSU*#wR ',>

ff
ff


6**>2
z'S,RSz<$SzS[0 S,#tww*4%$ z'#<4,#N

:zp>zz0RUw,2E0,,$]60, 2,<Sod*wwR>6$z>Nz<z
!S2n
z,ztzS$Td:



fi"!"#%$
&(')$"*"+,#%-"!"#/.0"1"23&4"&657!"8"*9$
:<;

=?>A@CB<DEF>HGJIKffL">M>ON9PQGSR">CRTKFU3BVWX>CP>CRWX>CR@O>YGSR[Z\K]U_^>AEKL">`>CRXKffaffGSPXbScdKffL">MRTD"^MeQ>CafG<IghGSa]ViW"E
BSEffEFG9@UiB<Kff>AWjghU_KffLkPQG<U_RTKOElG<ImLXU_nSLo>CRXKffaffGSPXb)E]gB<^PpBVqVGSKffL">CalgGra]ViW"E0BSEsZtnSaffG<guEvViB<affnS>SwyxzLXU3E
@OGSR@O>CRXKffaB<KFU{GrR|P"L">CR"GS^s>CR"GSR}c~gh>OVV{XR"G<gR/U_R)KffL">s>OV3W/GJImE]KB<KFU3EFK]Ui@CB,VP"LXb9EU3@CEc7IGraff^0EKffL">leBSEUiE
IGSaGSD"al^0B,U_Roaff>AE]DXV_KsU{RoKffLXUiElE]>A@OK]U_GSR}wKYBSEffE]>CaffKOEKffLBJKMU_KUiElPGXEffEU_eXV_>KffG|@OGS^sP"D"Kff>WX>CnSa>C>AEG<I
e>OVU_>OIBS@C@OGSaWrU_R"nMKffG`aOB<RWXGS^ghGSaFV3W"EgLXUV_>U{nrR"GSa]U_R"nB,VV%e"D"KKL"GTE]>ghGSa]ViW"EgL"GTE]>>CRTKaffGSPXbfUiE?R">AB<a
^0B,NXU_^MD"^wxhL">fR">ON"KuKL">CGSaff>C^>AEffE]>CRXK]UiB,VV_bYIGSaff^0B,VU_C>AEKffLXUiEhP"L">CR"GS^>CR"GSR}w
MdX%,}X}<fSOOkCC%S~d T?CSqJ~i"if"},`Xh9Jidu~
<ffOSCC%ffOi/" fu <d,h Y<O"C)CAO<d<iQi"Mj}T0C<0ri
M? <d< VU_^Y [ VU{^oE]D"PFVU{^jU_RXI T<
<CqJ fM
<C< f`

>aff>C^0BJafffKffLB<K}KLXU3EUiE~rDXU_Kff>BuWrU@ODXV_KKL">CGSaff>C^w>hLBAr>WrUiEff@ODEffE]>AWgLTb`>C^^0B"w_SV_>CKE
DEV_GTGrHB<K ArA] f gL"GXE]>>CRXKffaffGSPXbfUiE R">ABJa ^0B,NXU_^HD"^w=?D"K~KffL">hKffL">CGSaff>C^6K>OVqViEDE7KffGV_GTGS
B<KKL">f^0B,NXU{^MD"^>CRXKffaffGrPTblPG<U_RXKEG<I ff c"gLXUi@L[g>`WX>OR">AWDEU_R"nYB E]GIB<aD"R"^GSK]U_BJKff>AW
E]bXRTKOBS@OK]Ui@fP"affG"@O>AWXD"aff>MB<P"PXVU{>AWKffG f whKuE]>C>C^0Eaff>ABSEFGSRB<eXV_>`KffGs>ON9PQ>A@OKfKffLB<K 9 f E]L"GSDXViWKff>OVV
DE C<AS9 B<eGSD"K^G"WX>OViEG<I f wm=?D"K^0B<rU_R"nYKffLXUiEf@OGSR"R">A@OK]U_GSRP"aff>A@UiE]>Sc}BJRWYU_R[PB<affKFU3@ODXViB<a
E]L"G<ghU_R"n[L"G<gKffL">l^0B,NXU_^HD"^s>CRTKffaGSPTb[PQG<U_RTKEG<I 9 ff a>OV3BJKff>0KG^G"WX>OViE`GJI f ghU_KffL/R">AB<aff
^0B,NXU_^MD"^>CRTKffaGSPTbScQUiEWrUqY@ODXV{K,wG<gh>CS>CaAcQgh>WX>OI>CaHBVqVWX>CKOB,UV3EvG<IKL">HP"aGTG<IhG<IKLB<Kua>AE]DXV_K`KffG
KffL">`B<P"PQ>CRWrUNw
R[nS>CR">CaB,Vc}xhL">CGSa>C^"w_AM^lBAbEF>C>C^KffG0eQ>G<I~VU_^MU_Kff>AWDE]>OIDXV_R">AEffEuXR"G<ghU_R"nYKffLB<Kgh>`GSRXV_b
LB,S>[KffG/V_GTGrpB<KYghGSa]ViW"ElR">AB<aKffL">[^0B,NXU_^HD"^s>CRTKffaGSPTbPG<U_RXKWXGX>AEYR"GSKEFD"eE]KB<RXK]UiB,VV_bpa>AWXD@O>
KffL">MRXD"^HeQ>Ca`G<IhghGSa]ViW"Egh>HR">C>AWKffGY@OGSREUiWX>CaAw RWX>C>AWdcKffL">gL"G<V_>PQG<U_RTKG<IKffL">0@OGrR@O>CRTKffaOB<K]U_GSR
P"L">CR"GS^>CR"GrRUiEKffLB<KfB,V_^GTE]KB,VVghGSa]ViW"ELB,S>fLXU_nSL[>CRTKffaGSPTbSw >CS>CaffKffL">OV_>AEffEc}BSEhKffL">aff>AE]KG<IKffLXUiE
PB<PQ>CamE]L"G<guEcTKLXU3Ezaff>AE]DXV_Km@CBJRYe>HrDXU_Kff>`DEF>OIDXVgL">CR@OGS^MeXU_R">AWYghU_KffLKffL">IG<VV{G<ghU_R"nlK]gGMaff>AEFDXV{KOECw
xhL">?aE]K~GJI%KffL">AE]>EffB,b"E}KLB<K7UIB,VVKffL">ghGSaFV3W"E7R">ABJa~KffL">h^0B,NXU{^MD"^>CRXKffaffGrPTbfPQG<U_RTKELB,S>Bf@O>CaffKB,U_R
P"affGSPQ>CaffK]bSc"KffL">CRYgh>`E]L"GSDXViWLBAr>`WX>CnSaff>C>`GJIe>OVU_>OI`fKffLB<KhKffLXUiEP"affGSP>CaKbsUiEhKffaffD">Sw
qV U_^ ~ f
9 fH~ zVqU_f^
f

dAdi3,pS}<uri
OkCCdSd T~CSqJ~i"iuO"}A~fTh"<%~
<ffOSCC7C%ffOQH f q,7) f<9C,~J%<9M <%,
<<O<TS?TrqJ~<`l<CqM O"CYTS 0s}T
f`~

d73
GSafKL">TR"G<ghV_>AWXnS>leBSE]> " U_fi
R ff7NB<^PXV_>Y"w_ "cQU_KvU3E>ABSEFbKGE]>C>0KLB<KfKffL">
^0B,NXU_^MD"^>CRTKaffGSPXbPG<U_RXKhUiE w UqNE]GS^s>mB<aeXU{KaB<aff
b
w ~V_>AB<a]V_bScQKffL">Caff>UiEE]GS^>GSPQ>CR






E]>CK B<affGSD"RWKffLXUiEPG<U_RXKHE]D@L[KffLB<KfKffL">BSEffEF>CaffK]U_GSR _ _
C
L"G<ViW"EIGSa


>CS>CaffblghGSa]ViWsU_R whxhL">Caff>OIGSaff>ScXgh>`@CB<R@OGSR@V_DWX>MKffLB<K
Sa 3 _ H
C
O9 ~
!

" Ehgh>`E]L"G<goU_R =hBS@C@LTDEh>CKfB,Vw_c}$ #%#'& crIGSaff^MDXViBSE gzU{KLWX>CnSaff>C>G<I~e>OVU_>OIv@CB<RY>AEffE]>CRXK]UiB,VV_b
e>sKffaff>AB<Kff>AW)(]DE]KVU_S>0GrKffL">CafTR"G<ghV_>AWXnS>U{R f wMxhLB<KUiECcKffL">0WX>CnSa>C>AEG<IeQ>OVqU_>OIfaff>OViB<K]U_S>KG f
B<RW f ghUVqVhe>MUiWX>CRXK]Ui@CB,V >CS>CRUqI f BJRW f B<aff>MR"GSKV_GSnJU3@CBVqV_b>ArDXU_B,V_>CRXK +
w *GSaff>
IGSaff^0B,VV_bd

,.-

fi/!0214357698;:<=%5%0>?6@BAC1D<<$5%0
EGFIH4JLK$HNMPO?QSR4TVUXWY;Z%[\[^]4_N`ba\cdZ$egfih;jklk'mnporqds9t'vw u Wxyz{bnp|}j~'L
ei\ei`_eifii4%
dq\'G~'N!q\'G2~+!
ei s9t v u W!yz{n9| e w s9t v u W!yz{xln
w
!
!
K$JIJNUD%t[%+4eia\c^a\a``\h;;at^a\7aZ'cc]at^t^lPWY;Z%[\[]_N`Ga\cZ$egfih jk%k'mDnp]a\t^a%fYZl`[
t^%NZ4e`c[bt^aZ%`%4i`]'`c^]NZch4S%t!Z'Z'N?
s9t uv W!yz{pn9|s9t v u W!yxlns9t v u Wx2yz{pnst v u W!yxlnst v u W^xyz{Gn
YZ%``_+ci%?h s9t'v u Wx2yz{dnGc^a\N`
c^j!]a\acZ'%aXec`\h`fic^]aNt`cc^a\t^c^a\N`
c^
s9t'v u W!yz{fidxlnfC!c^]a%c^]a\t;]NZ'NIhs9t'v u W^xyz{Gn9]NZ%`eiGicf9Ya[\Z'_N`apst'v u W!y)x%nr`
Nl_N4aIhN;ap[%N[ei_N4aGc^]NZ'c!c]ap`a[%Nt24_N[cZ.er`+c^a\N`c^+fV;]at^a`_4ecS'ee'`f
`9;a`]NZ$ee`a\a;ic^]aa2c;`a[ci%?h4c^]a[%G4iNZ'ci%
'9V%t'eerZt^+fij\mpZ'N);]a\%t^a\fij
`l_4ic^ap7'a\t_4egf
9
?9'g9lNN
IG!SgD
e c]%_%]pc]aV[%N[a\ctZ'ci%b]a\%+a\%r`ica\t^a`ciNh$ic`?Z4e[\Z'cldcZ%[c^_NZ$eeip[%+_c
4a\%t^a\a`'V7aeia;Z$%c!NaG%4ll_N`\f 'iN[ad;ab'c^]NZcZ$ei+`cZ$ee;%te`;eeV]NZla]4l]
a\4c^t^%4%hNZ
lta[cZ'4e[\Z'ci%'C]a\%t^a\}fijG44a`!%c`_N`cZ'crZ.eeit^a4_N[apc^]ab_G7a\t'
;%te` ;a+G_N`c[%N`4a\tf+9a\c$hZ%` ;a+`]'ic^]4`p`a[cl?hc]a[lN[a\c^tZ'ci%c^]a\lt^a\[\Z'
S%t^c^]apNZ%`r` CZ)tZ%[c[\Z$e9c^a[]4rl_aGS%t[%+_ci4a\%t^a\a` VNaeia!i
Z'[\Zl`a`\f
Na\%a[ci%mNfrjt^a`a\4cic^]ai4c^_4ici%N`_N4a\telic^]4` c^a[]4%_a%fg%a[ci%mNf
;aG_4e%c^]a`api4c^_4ici%N`bt^a`a\4cit^a`_4ec` S%tpZ
t^a`c^tr[ca[eZ%`^` '%td_4eZ%` c]`a
l_a\tia`
!]4[^]Z't^a%_NZcNa\t St^a\aS%t^G_4eZ%`+'%a\tZ_NZ't^eZ'%_NZ'laCc]Z`i'eia[%N`cZc
`4GN'egf+ `4ca+'!c^]4`t^a`c^t[ci%?hZ'4';c^]aG`^`_a`GZ'tr`Xic^]a)%a\a\tZ$e[\Z%`a[\Z'7a
`a\a\]a\t^a%
f %t^a\'%a\t$hNZ%`;;ad`].%a[ci%mNfhc^]4` t^a`ct[c^a`_4eZ'l_NZ'%a` t[]a\%_%]
c^+Z$eei'_N`;c^Ga\GNaXc;baee S4.t^%7`ici%NZ$e9Z't^Z%[]a`c^]NZ'c;Z'la_N`a'Z.iG_
a\4c^t^%4
` 7fi ff^ ~ ff

W e`^`%?h!jk l4nCZNc^]a
Z$4G_
Sa\4c^t^la2c^a\N`l
e`^`%

'
g`a\
Z'cr[\`)W ?a\!

saZ'te h;jklk%4n4_a
c^"
'e` #\r4c$$
h %t^t`\hsaZ'te!Wjklk%4nW`a\a
Z$e`
W 'e` #\G4c%
h %tt`\&h saZ'tegh9jk%k%2n^nf gfi%a[clmNfmNh7;a+[%N`4a\tp!]a\c]a\tc^]aGt^a`_4eic` S%t
c^]abt^a`c^t[c^aeZ'%_NZ'%a+[\Z'NaGac^a\N4aIf!aG`]'c^]NZ'c c^]a\[\Z'?h7_c`a\%a\tZ$e9l( '[_4ecGZ'N
`_ceiar``_a`Zt`a%f
QSR EFLH"*NH,+IH2K.-0/2143K.-53\Hfi*06
ec]%_%]c^]atZN4%S;%te`+a\c^])`!4aNa4[%_4ci+;%te`\h;a[\Z'`%)a\ci+a`NN
+%talit^a[cG!Z$2`c^[\Z$e[_4eZ'c^a
c^]a4a\lt^a\a`';Naeiaicb%iae`\f
gfiWYZl[\[^]4_N`a\cpZ$egfihjk%k'mDn;a
t^a`a\4cZ+4_GNa\t'V`_N[]ca[^]4l_a`\hI+`c 'V!]4[]Z'4ei%4ei
i%a\t^`7a[rZ.e[\Z%`a`\f!a
'?c^]a`)4ea`cCZN++4`cic^_4ici%a `c^]a!S'eei';idla\t`i%+'?!]NZc9]4ei`%]a\t`;]NZ$%a!ca\t^+a
q 7 9W 8;a[^]a\4NZ%[]?h!jk'm4k4nf%_N`ac^]NZ'cpZ$ee!;a+'Z'N%_c+Z'Nlil4_NZ$;

e :dr`
`%)apZ%`^`a\t^cl=
< 9W :$?n >i%c^]a\t;;%t`hNz{]NZ%`;c^]a S%t^@
< 9W :$n7z

{ AShZ'N
c^]ap[%N`cZ'
c :44a`
%cZNaZ't iz{ f e``_7`abc^]NZ'cz{+h7c^%%a\c^]a\t;ic^]ZGNZt^c[_4eZ'tc^'eia\tZ'N[a hi+4eia`
c^]NZ'
c B\9W Cny < 9W CIn BDG`i`%+ai4c^a\t^'Z$
e EGF H%Igf;cp`a\a\`t^aZ%`lNZ'4eiadcZ't%_apc^]NZJ
c :r``]%_4e
NaGc^taZ'c^aZl`L
Z Kc44[\Z$Ne M+aeia\+a\4cd`^Zc`liO
< 9W CInhDNa[\Z'_N`aG4Z%``_+ci%z{ [%4cZ$iN`
)

PRQ

fiS$T5U5V&WYX[Z\W5]5^.V&_`T5U5Vba=T5c5deX$f5XhgiU5j5]kWYlm

nofiprqtsu=vwnqoLxy5ztz{Rxwno5zOqw|5{4s~}2nNx{|5{4s{prqts{}2{unz|fiw
|5q0{wqOy0x{w|5{xw?vwnNxwne4x
tns{Rw
vo0qo0~y0fi{
w|0vw2s
99.4

$LG% 2|finNxnNxno0fi{4{ROw|5{`4vx{5vxw~|5{pqq}no5zw|5{4qts{4u
x|5q}x4
5

N
7fi&.0%fi 9 v4~| y0x{4wve&. .
~
.&R!~4
Jfi$4h.7
7= ,47?r&?&
R?

% ? ` ?

L$%




kNb
`
&?04&

9R

JG%

t9!.G
4.N4!N

|finexs{RxyfiwRYnoLqufino0vwnqo}2nw|w|5{!s~{Rxyfiw?xqpw|5{5s{4tnqty0x
x{Rwnqo,5sqnNfi{Rxy0x}2nw|"v
{4s0q}2{4spyfiwq q2qy5z|fix,{Rvno5z0,}2{
5sq,q x{
wqy0x{
w|5{`prq(q}2no5zOxws?vw~{4z&2|5{
0vtxnN
qo0{4ofiws?vwnqtoL5|5{4o5qu{4o5qox~vR5x`w|0vwuq xw
}2qse5xvs~{{4sLxnunNvsnov{4sw?v.no\x{4o0x{O
x
x|5q}ono$qsq(Nvs~=54,}{`4vo=y0x{w|finNxwq
0o0xqu{`vxx{4swnqto0x$w~|0vw$vs{v.uq xw2{4sw?v.nofik
wsy5{ n{eR}2nw|fi{4zs~{4{qp00{n{p {4{4on(p7w|5{4vs{$o5qwqznN4v.nufin{Rfi




|5{4qs{4uh5R

w|5{4ow{NxOy0xw|0vw=}2{L4vows{Rvww|5{Rx{"o5{4}vx~x{4swnqo0xOvxnpw|5{4vs{nopvw=fio5q}o}2nw|
{4sw?vnofiwi|5{4ow|5{Rx{o5{4}vxx{4swnqto0xxw?vw{xwvwnNxwnN4v.fio5q}2{Rfiz{R5tw|5{44vovxwno0s{Rvx{
qy5sqt50qsw~y5ofinwn{Rxwq=v5fitns{Rwnofipr{4s~{4o0{2|5{;prq(q}2no5z{Yvufi{;n(y0xw~s?vw{Rxw|finNx2nNfi{Rv5


eififf%

qo0xnNfi{4svO{4sLxnufi{Ofio5q}2{Rfiz{=0vtx{=q{4s!v

q54v5yfiNvs

qofiw?v.nofino5z"w|5{

xno5z{y5o0vs5s{RtnN4vw{


2|5{4s{!vs{w}2qvwqux
w|finNx


e


?
?


vo0q{4s=fi}2nw|





nex`~{Rvs

zn{4o

#

7

%$




&$ $'

hvo0 "! $2|5{!xqy5wnqox0v{
qp


$


xwsv.nz| wprqs~}vs?qu5y5wvwnqo!x|5q}x7w~|0vwRprqs
*
0qnofiw 1
) 3 24



)



(



,+ 4w~|finex|0vxvy5ofin.-ty5{2u=v/finuy5u0{4o ws~q




5 q}!0qo0x9nefi{4s
w~|5{-ty5{4s 9. 65qsv(798

Y{4w;: 7 ,{w~|5{
pqsuyfiNv ? \ )



<

4






7


)
=
)
7

2

fi
|
N
n
=
x

x

v

w
N
n

x
0

R
{

x
~
w
5
|

{



q
0



nwnqoqp$qsq(Nvs~540xqLnwprq(q}xw|0vw



: 7 9
$ ?> xno5z |5{4qs{4u5eR5fi}2{fio5q} w|0vw2pqsnuA@ 4nu nofip nuxy5B

nu @
C

CD



nu @
C

9.4

$

9.(


CD

: 7

y5w2o5q}}2{
4vo=y0x{!tns~{Rw2nofipr{4s{4o0{ 5qw{`w|0vw|5{4s{5qy5s!fio5q}2{Rfiz{Rv0qy5w nNx2vy5qty0x4
n{% ?549. 5

{qo0~y0fi{w|0vwRtnpw|5{4s{
nNxvo nunw!vwv(,w|5{4oo5{R{Rx~xvsn



F q0fipqsv.G7 8


9.4


: 7 L

)




4

7 4


)

)E7






9.

$





)

H4

7 4


)

)I7

F no0{
w|finNxinNxwsy5{2pqs2v.J74fiw|5{qtofi0q xx9nfi{
v.y5{pqss 9.(

nNx K) }|finN~|nNxw|5{


n{e Jt w|5{u=v/finuy5uL0 {4ofiwsqt 0qnofiwR5qw{w|0vwnwnNx
v.NxqO~{Rvs
}|0vw


5
|0v5,{4o0xvx w{4o05xw~q $s .

nNx


vy5{qp $

MON

fiPQSRUT?VXWZY[?\^]OVOQ?_W
`bacRd\^\/VOQ

ecfUg.hBijlknmLoUpqi<rUismLtvulhxwzy{knw&i^hlw&f?im|k/gqu}hxwzisolh~tn?tOu?iolthzhgUpqihxwzyknwzisOtOyHtvmLo??wxgqu?rUisvyzisi^h
tnlXipgqizBgqy{hxw
wzf?im|k/jUgqm?misuwzy&tOo9oXtngquwhtnlw&f?icholkOiS
399 kny&i<tOmLo??w&i^rkvhHkc?ulwgtvu
99 kOhzhx?mgu?wzf?ip3gmgwijghxwh g3<u?tvw^~wzf?i
tnL ;ef?isuw&f?i^hxiLknyzi}lhxi^rwzttOmLo??w&i|Zy
p3gqmhx?oknulrp3gqmIgquU9tnKKyz 9; kny&i;tOmLo??wzi^rAgulhwzi^kOr Bgqulk/p3pqO~i;tOmLo??wzi9w&f?i9p3gqm}gqwtn
wzfUgho?yztOlknUg3p3gqwkvhL OtUi^hwztLsisyztX
uUtOyzwz?ulknw&ipOnw&fUg.hZhwzy{knwzisv9flkOhHk9hisyxgqtOlh
oltOw&isuwxgk/p?o?yztOUpqismKi<&pqi^knyxpqLsknu?u?tOwtvmLo??wzi
Zy x 9; hxisolkny{knw&iptOyi^kvzfLtJw&f?igquUluUgqwzipq|mknu}wztpisyknulivi^wztOy{h knulrwzf?isuLw{knviwzf?i
p3gqm}gqw;kOh vti^hwzt|?im}gqOfw f?tOoXi;wzt|tvmLo??wzi;w&fUg.h o?y&tOlknUg3p3gwxkOhknuijSoUp3g&gqw9?ulwxgqtOutn
~knulrwzf?isutOmLo??wzi}wzf?ip3gmgw/?tvygqulhxw{kuliO?gqu
jlknmLoUpqi}l,LKy 99 kOhtO?ulrwzt
li?, ^ Zknulrhtgqwg.hi^kOhw&thxisiflknwflkno?olisulh}kOh ^
dA?w}wzf?isyzig.hu?tyzi^kOhtOuwzt
lip3gqisOiLw&flknwKy 99 ghsXgquvisu?isy{k/p%kui^kOhg3pqzflky{kOwzisyxgq^knUpqi?ulwxgqtOutn; gwgh u?tOw^
wzf?isuEtvmLo??wxgqu?wzf?i}p3gqm}gqw|kOh OtUi^h9w&tsknuXiArvg3AUpqwLtvy gmolthzhgUpqiOiLtOUprp3gqOiwzt
lulrkk^}wzt}k/OtngrLw&fUg.hcijSoUp3g&gqwp3gqm}gqwxgqu?|o?y&tSi^hzhk/pqwztOOiswzf?isy/Hwwz?yzulhtO?wwzflknwwzfUghghZgqulrUisi^r
oltUhzhgqUpqigquhxtOmLi&gqy{?m|hxwknuli^hsef?im|kguyzi^OUgqyzismisuwKg.hKwzflknwZwzf?im|k/jUgm}?mLisuUwzyztvooltngquUw{h
tn 399 tOuvisyzOiw&twzf?im|kjgqm}?mLisuUwzyztOoU;oXtngquw{h
399 dtOyB?wz?y&i<y&iisyzisuliOnu?tvwxgi
9
wzflknw 39 gh;w&f?iA&pqthx?y&i|tnwzf?ihxtnpq?wxgqtOuIhxolkOitnwzf?i|tvulhxwzy{k/gquUw{h9tv?w{k/gqu?i^ryztOm
yzisoUpkO&gqu?k/p3pZtSs?yzy&isuli^h tn<kulrk/p3pZtSs?yzy&isuli^h tn9 umknuhxl&fskOhi^hs
isknutOmo??wziKy 9; rvgqyzi^wxpqgquw&isyzm|htnw&f?imk/jgqm}?mLisuUwzyztOoUoltguUw{htn 9
gqwzf?tO?ww{knvgu?pgqm}gqw{h9kwckpp%
h wzf?i}tnp3pqtngqu?ijlknmLoUpqi|hf?t
h ~wzfUghwUoli}tntOuUwxgquUUgwxrUtUi^h9u?tOw9f?tnprgquOisu?isy{k/p%wzf?i
m|k/jUgqm}?mLisuw&yztOoU|oXtngquw{hctn< 39 rUtLu?tOwu?i^i^hzhzknyxg3pqtvuOisy&Oiw&tLwzf?thxi9t< 39
~
.
q~9tvulhgrUisywzf?i;Uu?tnpqi^rUOi9lkOhxi
9


. q ?,

q q ? ~

q . |
?

%wgh9i^kOhxwzthxisiLw&flknw} 39 ghxlhxw} ??{? z ;ef?iLoXtngquw ?l?, gh;rvghzkppqtni^rUwzf?i

hxi^tOulrtOunx?ulw^tn;
tOulhgrUisy 39 tOy
U


wzf?isu 9 gqulrUisi^rIrUtUi^h
u?tOwtvuw{kguoltngquUw{hf?isyzi9 ghu?i^kny ?lUwzf?im|k/jUgm}?mLisuUwzyztvooltngquUwtGwzfUgh hxolkOig.hi^kOhg3pq
hxisisuw&tli?, tnisOisy^Xg3 Lwzf?isuwzf?isy&iLg3p3pcXiLoltguUw{hgu 39 f?isyziL g.h
knyztv?ulrE?lBtOygqulhxw{kuliOZwzf?thxif?isyzi?} ?} ngquliwzf?i^hxioltguUw{hflk/Oi
kfUgqOf?isy}isuwzy&tOowzflknuwzf?i|oXtngquwh9gquIwzf?ivg.&gquUgqwxItn?,?
wzf?itOyzmLisyg3pp9rUtOm}gqulknwziOefUlhs
wzf?ihxisw;tm|k/jUgqm?misuwzy&tOooXtngquw{htn9 399 rUtUi^hu?tOwtvuOisy&Oi|w&tkhgu?piip3pq%rUilu?i^r
ecfUg.h u?tvultOuvisyzOisuli
hxisw^flknwcgw9tOuUOisyzOi^hwzt g<kuUwzfUgqu? rUisolisulr?h9tvuf?tn vti^hwzt ?
flkOhtvulhxi^O?isuli^htOyrUisvyzisi^htGXipgqiz%wghu?tOwflkny{rwzt|hf?tZy 99 skuli;igqwzf?isy
?,c tOy? /SrUisoXisulrvgqu?tOu|w&f?i9o?yzi^&ghxi9yzipknwxgqtOulhxfUgqoliswxisisu n?knulr
%wtnp3pqtnh
wzflknwKy 99 rUtUi^hu?tOwijUghxw^
9 tOyZtn
ih&k^wzflknwk9rUisOy&isitn~lip3gqiKKy 99 g.hKu?tOwzUs
g3Jw&f?ilisflk/vgtvytnZy

rUisolisulr?htOu Oti^hwzt ?

u
p3gqmIgquUHZy x knulrpgqmhx?oKy z kOh vti^hwzt }

tOwzf?isy tOyxpr?hs~u?tOu?y&tO?lhxwzu?i^hzh rUi^hzyxgqXi^hhgw&lknwxgqtOulh f?isuZy x 9; rUti^h u?tvwijUghxw9li^sknlhi
tnhxisulhgqwxgqvgwxwzt}wzf?i9ijlkOw&f?tngi;tHwztnpqisy{knuli^h Zi;hxflk/p3pGhisiku?m}Xisytn
tOwzf?isyijlknmLoUpqi^htn
u?tOu?yztv?lhxwzu?i^hzhgqup.kwzisyhi^wxgqtOulhs
wZm}gqOfUw<hxisismbwzflknwZwzf?iu?tvwxgqtOuLtnyztO?lhwzu?i^hzhBghknuLknyzwxg3%kOwtn~tO?y<ko?o?yztkO&fu|olkny&wxgUp.ky^
gqwGhisism|hw&trUisoXisulr;tvu;wzf?iK kOw
wzflknwBtO?ypknu?vlknOiflkvhwzf?iijSo?y&i^hzhgqOioltnisywzt9hzk/w&flknwwzf?iwt
wztnpqisy{kuli^h
yziso?yzi^hxisuUwGk9rvffg
isyzisuUwGrUisvyzisitnkno?o?yzt/jUgmknwxgqtOuOhgqmLoUpq}9lhgu?;rvffg
isyzisuUwhx?lhzyxgqo?w{h
fi

fi

"!#$%&')(*+,- .

/1032546870:9:;=<?>1@A3ACBED FHGJI=A KL7NMO<PD <#D GGQ1RJDTS1@U>1RVQ1A8GQWA6XA8<Y>Z;[<\HD GGQWRBJ;=F#D >1A
<RT>cF#D dTA5>1@A6XA

A]_^EDIff;[>X`H>W@ED >baJRJA6

a_;&6Z>X;=<ESC>X;=RT<E68ef5A3D QWAbghRT^<EaU>1RV\TA8>i>W@A3D <E6Xf5A8Qbjkmln;=<H>1@A:ACBED FHGJI=AoD gER 4TATeJ6p;=<ESCA

>1@A8<q=qrs/utvKLq=q w:
x0
9yjkzyQ1AD{I|I=`"f5RT^JI}a"gEAP>1@A?<A8\YD >X;=RT<"R ~q&qrs/utvKLq=q w032?jkzE7APf5RT^JI}aDQ1\T^A
>1@ED >

>1@AD <E6Xf5A8Qjkmls;}6

a_;ffA8Q1A8<Y>

;=<E6X>CD <ESCA6

<RT>oDT6:QWADT6XRT<ED gJI=ADT65;=>oF;=\T@J>oD >:EQC6X>6ZA8A8Fy7oT^GGhRY6XAHRT<AsR ~>1@As>uf5R

R ~bjkzU;=<>1@AGQ1A84T;=RT^E6

ACBED FHGJI=A@EDTagEA8A8<"6pIff;[\_@Y>XI=`a_;ffA8Q1A8<Y>h~RTQnACBD FUGJI[ATe

6X^GGhRY6XA?fbAU@EDTa^E6ZAajklTTUQLD >1@A8Qs>1@ED <jkzP;=<>W@AEQL6X>sR ~:>1@A8Fy7PMp<>1@J;}6S8DT6XATe*>1@A#6XASCR_<Ea
SCRT< X^<ESC>*;}6iA616XA8<J>X;}DIffI=`H4{D_SC^RT^E68eTD <EaHS8D<ghAc;[\_<RTQ1Aav75@A5F#D{BY;=F^FHA8<J>1Q1RTGJ`VGhR ;=<Y>*;=<P{
ffo



;}6*<R fjkmlTTeD <Easf5A;=<EaJA8AaHaJA8QZ;[4_A3D3aJA8\_Q1A8A5R ~ghACI|;=AC~R ~jkmlTTb;[<?rs/uKL7@Y^E68eTDQ1gJ;=>1QLD QX;ffI=`6ZF#DIffI
SW@ED <\TA6>1R:>1@Ac<Y^FgEA8QC6v;=<>1@AR_QX;=\ ;=<EDI,dJ<R f5I=AaJ\TAgED_6XAbS8D <S8D ^E6XAcI}D Q1\TASW@ED <\TA6;=<RT^QaJA8\_Q1A8A6
R ~ghACIff;[AC~175^>5>1@A6ZA3<J^FgEA8QL6:DQ1AD{I[FURY6X>bD{I[f:D`6i>1@AoQ1A6Z^JI[>5R ~iD GGQ1RBY;=F#D>1A:RTgE6XA8QW4{D >X;=RT<E6>1@J;}6
;}6cQ1ACEASC>1Aa?gJ`HRT^Q:aJASW;}6p;=RT<P>1R^E6XAoD GGQ1RBY;=F#D>1A3A]_^EDIff;=>X`HQLD >1@A8Q5>1@ED<#A]_^EDIff;=>u`Uf:@A8<PQ1AC~A8Q1QX;=<\
>1RH>W@A8Fy7iMO>

aJRYA65<R_>36XA8A8FQ1ADT6XRT<EDgJI[Ao>WRHgEDT6XAVDTSC>Z;[R_<E65RT<DaJA8\TQWA8AVR ~igEACIff;=AC~b>1@ED>3S8D <ySW@ED <\TA

6XRaJQLDT6X>X;}S8DIffI=`;[<>W@AH~ODTSCAHR ~o6XF#DIffISW@ED <\TA6;=<>1@AHFUADT6X^Q1A8FHA8<J>R ~3aD >LD7:RT>WAH>1@ED >e;ff~3f5A
dJ<R{f>1@ED >s>1@A#>Xf5R?;=<E6X>LD <ESCA6R ~3jkzaJRhe;=<"~ODTSC>e*aJA8<RT>1A?ACBDTSC>ZI[`">1@A#6WD FHA?<Y^FgEA8Qef5APS8D <
Q1A8GQ1A6ZA8<Y>5>1@J;}6cgY`U^E6p;=<\H>1@A361DFHADGGQ1RBY;=F#D >WA:A]T^ED{I|;=>X`PSCRT<<ASC>X;=4TA

;=<PgERT>W@a_;}6X^<ESC>L687iMp<P>1@J;}6

S8DT6XATe,;[>;&6nADT6X`?>1R#6ZA8Ao>1@ED >:f5AoaJR#\TA8>5>1@AD <E6Xf5A8Q:jkml7


SWI=RY6XA

I=RJRTd#D >c>1@A

ACBD FUGJI[AV6Z@R{f

6i>1@ED >c>1@A

<RT<Q1RTg^E6X>W<A6165D QX;}6XA6cgEAS8D ^E6ZAoR ~>1@A3<A8\JD >1Aa

GQ1RTGhRTQ1>X;=RT<"ACBGQ1A616p;=RT<q=qrs/utKLq=q w":
x0
9jkzE7?MO<EaJA8Aaveif5A#S8D <6X@R f>W@ED >o;ff~3f5A#6X>CD Q1>of5;=>1@D
;=<S8D <RT<J;}S8DIi~RTQ1F>1@ED >VaJRJA6

<RT>VSCRT<J>LD;=<<A8\YD >WAayGQWRTGERTQW>X;=RT<ACB,GQ1A6W6p;=RT<E6o>1@A8<eh;=<D?GQWASW;&6ZA

6XA8<E6XATe5>W@Ay6XA8>HR ~oF#DBJ;=F^FUA8<Y>1QWRTGY`"GER;[<J>L6UR ~V
F#DBJ;=F^FHA8<Y>WQ1RTGJ`PGER ;=<J>L6



R ~:

ffo

7




[o3

<D Q1\_^FHA8<Y>

<ASCA616WD QX;ffI=`SCRT<Y4_A8Q1\TA6>1R>1@Ay6XA8>HR ~

S8D <gEAF#DTaJA>1@ED >nfbAH6X@R_^JI&aACIff;=F;=<ED >1A

<A8\YD >WAaGQWRTGERTQW>X;=RT<ACBGQ1A616p;=RT<E6~Q1R_F>1@AI}D <\T^ED\TADI=>1RT\_A8>1@A8Q7)Mp>P;}6PRT<A>1@J;=<\>1RDQ1\T^A
>1@ED >

6XRTFHA8>X;=FHA6nf5Ao@ED4TAV6X>LD >Z;&6Z>X;}S8DI4{D{I[^A6

f:D <J>o>1RyF#DdTAsI[R_\ ;}S8DI3DT616XA8Q1>Z;[R_<E6
>1R>1@J;=<dR ~3S8DT6ZA6

fn@RY6XAVDTS8SC^QLD_SC`PfbAVDQ1AV^<E6X^QWAD ghRT^>eE6XRU>1@ED >:f5A

I=A6166X>1QX;=<\TA8<J>V>W@ED <ACBEDTSC>V<J^FHA8QX;}S8DI5A]T^EDIff;=>X`T7PMO>;}6V@EDQLaJA8Q

;=<f:@J;}S1@>W@AHRTGGERJ6p;=>1A?;}6V>WQ1^ATeiD <EaD{I|I5f5A#dJ<R f);}6s>1@ED >6ZRTFHAP6X>LD >X;}6X>Z;&S;&6

<RT>?A84TA8<D GGQ1RBJ;=F#D >1ACI=`A]_^EDIo>1R6ZRTFHAy4 DI=^AT7nR{f5A84TA8Qef5AaJR<R_>#ACIff;=F;=<ED >1A<A8\JD >1Aa
GQ1RTGhRTQ1>X;=RT<ACB,GQWA616p;=RT<E6~Q1R_F>1@API}D <\_^ED \TATe:6p;=<ESCAf5;=>1@RT^>?>1@A8FfbAPf5RT^JI}a<RT>#ghAD gJI=A>1R
GQ1R 4TAHD <D <EDI=RT\_^A>WRy5@A8RTQ1A8Fl77/u5@A8`D QX;}6XAf:@A8<f5A>1Q1`>1RUD >1>1A8<<A6X>1AaGQ1RTGER_Q1>X;=RT<
ACBGQ1A616p;=RT<E68eh~RTQ

ACBDFHGJI=AT7ffKPMO<E6X>1AD_aveEf5AV@ED4TA;}aJA8<Y>X;ffEAa"DHf5AD d_A8Q3SCRT<Ea_;=>X;=RT<>1@ED>5;}6o6X^JSW;=A8<Y>

>1RoGQ1A84_A8<Y>cGQ1RTgJI=A8F#656X^ESW@#DT6i>W@ED >6XA8A8<U;[<P*BED FHGJI=A:zh7ml7cCT, CH6p;=FHGJI=`H>1A6X>L6N>1@ED >
<A8\YD >Z;[R_<E6:D Q1Ao<RT>;[<J>1A8QLD_SC>X;=<\#f5;=>1@P>W@AoF#DBJ;[F^FHA8<J>1Q1R_GY`PSCRTFHG^>LD>X;=RT<?;=<D@ED QWF~^JI*fD`T7
A8>*i/ oj KvgEAn>1@A5Q1A6X^JI=>R ~vQ1A8GJI}DTSW;=<\VADTSW@#6X>WQX;}SC>*;[<A]_^EDIff;=>X`;=<#*/ oj K

?_b=&ffE5

f5;=>1@;=>L6of5AD dTA8<Aa4TA8QL6p;=RT<7PRTQ1A~RTQ1F?DIffI[`Te*f5AHQ1A8GJI}DTSCA?ADTSW@6X^gJ~RTQ1F^JI}DR ~b>1@AU~RTQ1Fj
f5;=>1@ojevD <EaAD_S1@6X^gJ~RTQ1F^JI}DHR~b>1@As~RTQ1FjHf;[>W@oj7/u5AS8D{I|I>W@ED >3>W@A6XAHD Q1A>1@A
RT<JI=`SCRT<E6X>1QCD;=<Y>L6nGERJ616p;=gJI=AH;=<*/ j KLe6p;=<ESCA#DIffI>1R I=A8QLD<ESCAH4{D QZ;&DgJI[A683D Q1AHDT6W6p;=\T<Aaj7ffK



v

ffo



gEA

vT


/ j K eJf:@A8Q1AsfbAo^E6ZA

;}6yL88v_}H C{;ff~V>1@Ay6XA8>L6?



ffo

>1R#aJA8<R_>1Ao>1@ASWI=RY6Z^Q1AoR ~i7*AV6WD`?>1@ED >




<Ea



A8>



ffo3

@ED4TA?>1@A61D FHA?F#DBJ;=F^FUA8<Y>1QWRTGY`



~Q1RTFBDFHGJI=AVzE7ml7b5@AVSCR_<E6X>1QLD;=<J>

GER;[<J>L687

nv*&*

RT<E6p;}aJA8QoD \YD;=<y>W@AodY<R f5I=AaJ\TAsgEDT6XA

~RTQ1F^JI}DH*/ oj K*;}6o/XD~>WA8Q6u;[FUGJI|;ffS8D >Z;[R_<vKL

/Onjkl:Hv

MO>C6

f5AD dTA8<Aa4_A8QL6p;=RT<?;}63

/O





jkzKvP/O

jkz3?

jkzKLk

/ j KL

jkl:H



jkzKvP/O





jkz3?



jkzKL

fi
fiff

:,Jh

TT







!"$#%&')(*+*-,/.10/24357698/.:0;2<3 5>=@?A#@)BCEF&D G!HJILK 2NM$OP.:0'Q-.1R)SUTWV R X
. 0ZY 3 576[\)-$]C_^ F G`HJIJK 2aC F G`HJIbK@c M$Od35>=@Qe357fSg[h?ji!(@eU*\*gk,+l@meJ@'%mn$`op)-)(*
q'rq]#q\st)(h*-,mllu,(h*vwB*-Jx(,&y!nz$|{@mg HJI }(,m*-g)(*g'`!~l@,hd!*!%$m?
*-Jt,`,(z-#!*Jg,)B-)(h*g'
l@,h*g%$*d/g#e*,\zm#@v(h*-)J*@**-|q&rhq]#q\s
)(*--,mllu,(h*e,C G!HJIK e,m(%m)-z$J*-,]*-,hgJ,C F G`HJIbK ?
|'1
:p
hveh\bhm HJI )ew: 1|`
hb)g\ ]h]
)1-e@ :g C FD G`HJIKE 1]mhbm]g CE^ FD G`HJIKh] )bJmm 3 1]m )1t
ej$Etm`)-:e ete/ P hw-ve )1t+ej$]\mw:/ wv/\ ]h]
)1-e@J 1_g C G`HJIKE +]gJj9\ ]g)1@] :
aZ:]::1
1@

]
(,g,W,*-,Ue,mq\l#*-ZOd HJI S,m|Ue)*v'(/-g*-ge*-n$-b,_@eg*-st,mvn)}t,m-s
q]#m|(@n/x(,nzm\{@m HJI ?ZA]q\,hg*|d!z$(:)(h*gg*-ge*g,m(J*-@*J*j"$#)-
g,m#n/{u\"$#@(*g`@)-s)UOt@vg*st,mvn))Sk)(h*-)(@e],%m)b*-]%m,){#-+ c M&'[hu*-h#@wB*bJ
"$#)-\{@,m#*J(z(@n$%$n#@&B1? b*-_m-e*-)-g*ge*g%mmB*k#eE*-,Jerl-
q(+'s`tUer q\l)?~,m-),%m)'Bu!*|zm(`:)(*g)|*@(*-](z$#@zme,m(@dn)n
{}(@nZ)(@e,%gxUOemmSe?
Ayb,`,(zne@(*g,m(/el@bne@(*|$-,p(h*-)g*?
Z$kpp& t,m-q]#E )e)): mu 1m *1
"m#@(*g`@)-st-)k(@nJl-,mlu,m-*g,m(s
t-)\t,m-q]#Z(*-(zm#@z$EEO-M&
0Q'555Qvm[mS9Odg,+*@*B(l@-*ge#B
*]@m9(,e,m(@g*v(h*
gq]{@,eSk(@nU@m,$(!Z,m(-)J%g{\p?
|-'\*-@*EOg'S b:`wwvb) HJI X
EOd1S
g)(h*&Ul-,$l@,h*g,m(@&B
HJI b,*|t,m-qbOd'S1 HJIJ B1})-|bOdSb-g)(*g'`!/l-,$l@,h*g,m(@&(@n HJIJ n,
(,m*q\)(h*!,$(?
A#@)Bg#@g*]mb(A),m-)q=@?mBkZg#ll@,h*-@*JOg'Syg#qqg)]'`k*-@*JJx(,({u,m#*
?EP(+*Je*g,m(pB@bt,e#@},m(/e,mq\l#*g(zZ*-n)zm)|,{@e`e OdEOd'S) HJI St,m9t,m-q]#
EOd'SUq\l"$#)-Z,$ HJI ?
,m*-~*-@*(g)(h*&l-,mlu,h*g,m(@'bt,m-q]#OdS|Z"$#!%')(h*~*-,n$g#(@e*g,m(,
*-,$q)?,mer@q\lmBu,&%$)k*J%m,){#-/M0QeRw[hB*-bt,m-q]#\
0'Od1S18
R'Od1Sy"m#%&&!)(*
*-,J 0 Og1S8 R Od1S'8k
Od1S
OP)-E*-*-,mqZp-,mvn)n$1
(
r@q\l6? Se?,$(9-g)(*g'`
l-,mlu,h*g,m(@'u,$-q]#Bh*exm
\OPS
{@b*-|O#("$#S_g)*_,*-,mqg#@Z*-@*bE"m#%&&!)(*
*-
,
fiff
&Og1Sv?
T~
*-,
pk}%m)J*-#l .+
V Wm9l-,m{@{`*gm-d!z$(q\)(h*E*-,|*L*,mq)B)(er*-)(@n .U
Zl-,m{@{`*g $-zm(q\)(*J,m('`-g)(*g'`!l-,ml@,*g,m(@'_t,m-q]#mb#@(z/*-bn)(*g`:)*g,m(
,_(U-g)(*g'`+l-,$l@,h*g,m(@&p,$-q]#j*-+\g)*,*,mq X
Z$kpp p
)*J{@(+-g)(*g'`/l-,ml@,*g,m(@'
,mq#? jne@(\j#(@e*!,$( "! $# X
V &%(' )$E,, X

! $# .p S2 *
fi. 5


+ff, -

./

fi021fi3fi465+798:5fi;fi<=46>?1fi3fi4A@ 1fiBfiC72Dfi7FEG3fiHfi;I5+JK
LfiMONQPSR-RTPUWVTXZY=[\[^]`_fiN-Ma_bMWR$XVTXMaUbY=[bcdMaN-efW[ZYaRQgihkjmlYUbno?hkjml6pPqnWPsrbUfiPtVvufiPhw_bYN-VXYx[ylzcdffiUb{sVTXMaU}|Q~ +
&( VvMbPa
| ~ OW hk l
| ~ + hT lQ |"~ whT l

MaV-PV-ubYVV-uWXZRcyffiUb{sVX^MOU}XZRqffiUbnWPsrbUfiPSnpqufiPU|"~ WwhT l2fi
RV-ufiPQcdM[\[MxpX^UfiNvPSRTfW[V2RTufiMptRXcg`XZRQYtR$Xe_W[PtOffiPN-]?cdMaNihwMcfiV-ufiPicyMON-eo?hk=ltlV-ufiPU
Y=[\[qV-ubYVe YV-V-PNsRX^U:{sMae_fiffiVX^UfiAQN-hkgql?XZR|Q~ + dhT lqcdMaNV-ffi_W[PSR` Mcqe Y=WXeffiePUWV-N-MO_]a
uWfbRzXU:Y}RTPUbRTPapPYN-PMaUW[]fbR$XUfi V-MnWPV-PN-eX^UfiPV-ufiP R_bYa{sPMaPNpquWXZ{vupPe Y=WXeXP
PUWV-N-Ma_W]aqtY=OXUfinWPsrbUfiPSnV-uWXZRRT_bYa{sPapP{YUcyMfi{sfbRMaUoYUbngAXUnWPV-PN-eXUWXUfi}V-ufiPnWPON-PPMc
bPs[\XPsc-
zW6=bG`b=aI gihk=lt } Z 6 \I, ` aZ zd OZ
SqT 6-s fi 6 :qI \? 6 |"~ dhS l ,
},
[\Xe [\XeRffi_T[\Xe:XUWcS x
[\X^e "N hkgihk=l,?l XUWc |"~ fi WdhS lsRTffi_ |Q~ + dhS l
q
fi

ufiP2cdM[\[MpXUfi?XRiYUXeePSnOXZYV-PqfiffiVGXe_MaN-VYUVQ{sMaN-M[\[ZYN-]Mc6VvuWXRGVvufiPMaN-PeGwV2YOR-RTPN-VRV-ubYV=X\c
V-ufiPtR_bYa{sP \?bubYaRiYffiUWXZOffiPe Y=WX^effiedPUWV-N-MO_]_bMXUWVSWV-ufiPUXVR"xY=[ffiPffiUWXZOffiPs[^]}nWPV-PN-eX^UfiPSR
V-ufiP_fiN-MObYWX\[XVT]"N hkgihk=l,tl
z SbG
zb=aI, gihk=l? zI
aZ ss 6d
Z z-sbI 6 qfi \t z Q| ~ WdhS l2 2W
QN hTgihk=l,?l2| ~ fi W hS l
ff PYN-P?XUVvPN-PSRTV-PSn`XU"N hkgihk=l,lfipquWXZ{vuePSYUbRV-ubYVpPYN-P?XUVvPN-PSRTV-PSnX^UV-ufiP?[\XeXVtMc
QN hTgihk=l,ltYaR W fiaffi_fi_bMRTP XZRPSRvRTPUVXYx[[]_MR$XVTXaPaufiPUW]VvufiPN-PSRTfW[VR?McV-ufiP
_fiN-POXMafbRqRTPS{sVTXMaUYUbnV-ufiP{sMaUWVTXUfWXVT]Mc | ~ fi OXViXRPUfiMaffiauV-M[^MWM }nOXN-PS{sVT[]YVVvufiPte YxXeffie
PUWV-N-Ma_W]_bMX^UWVRMc \?w MaN-PcyMaNve Y=[\[]afi]}{sMaeWXUWXUfiufiPMON-P
e b pXV-uQNvMa_bMRkX^VX^MO
U b fi
pP{YURTufiMp
zW6=b fi?b=aI gihT=l Z 6\Iq W$IO? \t W `W fi
6-sI 6 s,6 aZ?I 2z | ~ W h= l 2
QN-hTgihk=l,?l2| ~ fi W hS l
ff PtPs[XPaPV-ubYViV-uWXZRV-ufiPMaN-Pe pX\[\[V-ffiN-UMaffiVV-M{sMaPNqY [^MOVMc {YaRTPSRiV-ubYViMI{{sffiNX^U}_fiNsYa{sVTXZ{sPa
RQMaffiNiPs+Ye`_W[^PSRiYUbn`V-ufiPtnOXZR-{sfbR-R$XMaU`XU VvufiPqUfiPsIVRTPS{sVTXMaU}RTufiMpapPMcdV-PU nWMaPVR$Xe_W[PaffiPNX^PSR
YUb
n UfiMp[PSnWaPbYaRTPSRV-ubYVYN-PPSR-RTPUWVTXZY=[\[]_bMWR$XVTXaPa
2MaUb{sPN-UWXUfi}V-ufiPYaR-RTffie_fiVX^MOUMcYffiUWXZaffiP
e Y=WXeffiedPUVvN-Ma_W]}_bMXUWVSzUfiMaV-PV-ubYV?V-ufiPPUV-NvMa_]cdffiUb{sVTXMaUXZR{sMaUWaPs YUbnRTMV-uWXZRYaR-Rffie_fiVTXMaU
XZRYffiV-Mae YVTXZ{Y=[\[^]R-YVTXZR$rbPSnX\c ^tQXZRY sb RT_bYa{sPa
PS{Y=[\[qV-ubYVYRT_bYa{sP XZR{sMaUWaPsX\c
cdMaN Yx[[ YUbnY=[\![ fi "wWXV?XZRY=[ZRTM}V-ufiP{YaRTPV-ubY#
V "
$ %h "'&(QlT ufiPRT_bYa{sP

\?GXRRffiN-Ps[] {sMOUaPs}X\cXV?XZRnWPsrbUfiPSnfbR$XUfiY{sMa*U )TffiUb{sVTXMaUMcQ[\XUfiPSYN{sMOUbRTV-NY=XUWVR+
ffuWX\[PXV
XZRq{v[PSYNT[]}_bMWR-R$XW[PV-M{sN-PSYVv+
P UfiMp[PSnWaPbYaRPSRpqufiPN-P \?zubYaRefW[VTX_W[^Pe Y=WXeffie`dPUV-NvMa_]
,-,

fi.0/21436578:96;-<5/6=>7@?BA1C;-;5/

DFEHGJI4KMLONPEQ@R%SCTHUVD4WXRYZFL[GXI6\^]_GLa`bZ6IFc%KbGXEIFL%d%Y
e:ROR%S2DR-c%KfKhgFT*KLbZFcigkj
I6E*e:WXR-]4\R0lFT_LbR-LT*QbGLbR:QMT*QiR%WJmnGXI
D6QMTc%KoGcpTqWrT*D6D4WsGcpTHKbGXEIFLpt:u@RpQhgFT*DFL@Khg6R:UVE
LoKQhR-LbKhQbGc%KbGXvR0TLiLbZ6UVD6KbGXEIVUwT]4R:l
mKhg4GLKhg6RpEQiRpUxGL!Khg6R
LbRpRpUkGXI6\*WXmwGXI6I6E6c%Z6EZFLyQhR-z_Z4GJQiRpUVRpI
KyKhgFT*K0{!| }
~N df6t:g4GLyTLhLbZ6U D6KbGXEIwGL0El4v_GJE_ZFL[WXmI6R-c%R-LhLhT*Qhm
PEQKhg6R:Khg6RpEQhRpUBKhEyg6E*W]
eGJKig6EZ6KGXK-Y_Khg6ROPZ6IFc%KbGXEIw{ | 6 }4~ GLfL[GXUVD4WXmVI6E_Kf]4R%FI6R-]tf0I4PEQhKiZ6IFT*KhR%WXmY
e:RfLbg6E*eGXIwR-c%KbGXEIFt0KhgFT*K>Kig4GLQhR-z_Z4GXQhRpUVRpI4K>GLpYGXI+PTc%K-Y*T0LbRpv_RpQhRfEI6RGXIDFT*QhKbGc%Z4WT*Q-YGXK@D6QhRpvRpI4KML
Khg6R0Kig6RpEQhRpUPQhEUlR%GXI6\T*D6D4WsGXR-]wKhE+UVE
LbKOR%SCTHUVD4WXR-L]4RpQoGJv_R-]kPQhEU]4R%PT*Z4WXK:QhR-TLbEI4GXI6\FY_ZFL[GXI6\kEZ6Q
LbKMTHKbGLbKbGcpTW>GXI4KhRpQhD6QhRpKMTHKbGXEIE*Pf]4R%PT*Z4WXKML#N:Tcpchg4ZFLRpK'TWtXY-*CdMt
RciWJE4LbRKhg4GL0LoZ6lFLbR-c%KbGXEIe:GXKhgT*IR%SFT*UVD4WXREHPKhg6R+Kig6RpEQhRpUGXIT_c%KbGXEI>t
>@@6# RpK+Khg6R+WT*I6\ZFTH\R^c%EIFLGLoK+E*Pf_#o2aa2*4rH%p06pf%T*IF]Khg6R
c%EIFLbK%T*I
Kf%4t:g6RpQhRT*QiR0R%GX\g
KyT*KhEULGXIKhg4GLWT*I6\_ZFT*\Rt RyZFLbR> > > KhE^]4RpI6E_KhR+Khg6R+T*KhEU
Nd Nd- Nd%Y-e0g6RpQiR GL!R%GJKig6RpQfN]4RpI6EKbGXI6\k+b6aaa*drE_Q N]4RpI6EKbGXI6\k+b2_aaadMY
GLEQ NPEQ+6*
*MVT*IF]2H
*%6YQhR-LbDFR-c%KoGJv_R%WJmdMY>T*IF] GL+EQ NPE_Q+06pOM
T*IF]02f%>Y6QhR-LbDR-c%KbGXvR%WXmFd%t
EIFL[G]4RpQ0Khg6Rj
I6E*e:WXR-]4\R+lFTLoRV+#X

n
Nh#o2aaaqNbd@6*4r*M_Ndhd
+b6aaa*NdM6*4r*%Nd % 6
X06pf%6NdX 6
6*
*M_Nhf%*dM
Pe:R'E_QM]4RpQ:Khg6R#THKhEUL:TLO0yp'Y 2 Y2 p
r Y4Khg6RpI
GXK:GL0I6E_K0gFT*QM]wKhEwLbg6E*eKigFT*K'@Nh+p*dGLp

F
N
N
N
N
N











: r
r
:







NC q diN !

F *d
NC diN
NC q
NC
q
6

















EkFIF]Khg6RLoDFTc%R
fi ff + X eRLGJU D4WJmLoRpK 6t:g6RpIGJKGLyz_Z4GJKiR^LbKhQ%TGX\g
KoPEQieTHQM]kKhE

FIF]Kig6R+UTS4GJUkZ6URpI4KhQhE_D
mwDFE*GXI4K0GXIKhg4GL0LbDFTc%RY6eg4Gcig>YKMT*j_GJI6\ Y_GLp




N[ h*qi-iq*h hfi df

M6%6






6Nb CN 6Nb CN
yL[GXI6\( Y6e:RcpT*Ic%EUVD6Z6KiR+vqT*QbGXEZFLyTLbm4UVD6KhE_KbGcyD6QhElFT*l4GsWsGXKbGXR-LyvRpQhmwR-TL[GsWXmtCEQ0R%SFT*UVD4WXRY
u!Q Nh#o2aaaqNif%*dp+#pd

{| X!#"%$&"'$)( + * !-,.fi/-$10a ~N

*
3 25 4 325 4
66
3 25 4 3 25 4 6a3 25487 63 25487





TLkR%S6DFR-c%KiR-]t *GXUkGsWT*QbWXmYye:RcpT*ILbg6E*eKhgFT*KVu!Q Nh06pOMNhOM*d++pX_d 6T*IF] KhgFT*K
uQ Ni6pOH%Nhf%*d#o2aaaqNif%*dp+ p d: 66t:9EKhRKhgFT*KyKhg6R+FQMLoK'Kbe:EVT*IFLbe:RpQMLyTWLbE

;<

fi= >@?@AB5CEDFB@G@HIAJ>@?@ALKM>@N@OPC Q@CSRT?@U@GVB5WfiX
Y[Zfi\)\&Zfi]^Y`_3Zbadc3e@fMgbh1_3ficjh1klY[f_3fkmifon@_ph1kmiqh1nl\&fsrutve@fZ_qfadwmx1y{z|}]:elh~i3eFem{n@nmfkmjc3Zf{n@nl\h~ifil\1f
h1kc3elh~ibpfxFtve@foc3elh1_g{kmp]vf_#e@Z8]cqemfico:V Ffikmgp@[~fi_3fofh1k@c3_qffic3fgL
h1kmglfnmfkmglfklcx:c:h~pnfiqhP8\ipfZfiYaZ_3ffk@f_8\h1kmglfnmfkmglfkmifn@e@fk@Zafk@Zkc3emfic:fin@nl\)h1f
c3Z_fikmglZa]vZ_p\~g@mpffruviiqemvfcI\-x1|}yfiwm|@tve@fZb_3fa@x'Vzx
1j}mT~IT}P1mPm
-kocqelhPvpfic#h&Zbko]fiZkmh~glf_:cp]vZ8{_ph~fikc Z{Y#l3`[m3@fi[[fi}){`fiT cqe@f:Y[Zfi\)\1Z8]h&k@
gbh~3im3h1Zkope@Zfi]|fiZc3eifikfh)\1fifin@c3@_3fgjlZ@_TY[_fiaf]Zb_3xTtve@fvfamfg@gbh1k@]f:gbh~3im3
mpfvh1anl\1fb@f_ph1fvc3e@_qZ@e@Z@c|lI\)\1Z8]h&k@mvcqZfin@nfI\c3Zc3e@f_3fpl\1cZfiYc3e@fn@_qfh1Zm:#ficph1Zkx
h)\~3pZkryblzMiZkmh~glf_3fgc3e@fLn@_3Zl\1faZfiY]:emficiZbl\Pgmfsh1klY`f_q_3fgE{mZ@cc3e@fn@_qZmfi
lh)\)h&cpFZfiYif_3c8h&kFn@_qZnmZuh&c#h&Zbkmfih1fkpZafoiZkmpcq_Ih1kcxF@Z_fmfianl\1f|v]vfMah1elclk@Zfi]c3emfic
_r1v13mz@)ofikmgLc3emfic _r3I~)fiz@@|fikmgfh1klc3f_3fpcqfgsh&k _r1v1~3)fizx
Z@bel\&#nmffibh1k@m| h)\PqpZk#@fpciZan@@cph1k@Lc3elh~liZkmh~glf_ph1k@I\)\n@_3Zbmfilh)\h1cpgbh~pcq_ph1
@@cph1ZkmiZkmh~pcqfkcj]vh1c3ec3e@fMiZbkmpc3_Ih1klc|fikmgsc3e@fkiZban@@cph1k@c3e@f_fik@fZ{Y88\&@fjfih1fkLc3Z
_r1v13)fiz:c3e@fpfMgbh~pcq_ph1@@cph1ZkmxM5Z_3aM8\\1|Tp@n@nmZ#fZ @_\~fik@mfibfiZbkmh~pcZ{Y
n@_ph1ah1cph1fon@_3ZbnmZh1cph1Zk|mfiI3@xZkmh~glf_c3e@f#fcZfiY cq_3@c3eqh1k@afklcjc3e@fpf
n@_3ZnZh1cph1Zkmxf{h&bfpfaM{kcph~ic3Zn@_3Zmfilh)\)h~pcph~ipc{c3fafklcvZfif_:c3elh~\~fik@mfifh1kcqf_3aMvZfiY
n@_3Zbmfilh)\h1cpgbh~pc3_ph1@@cph1ZkZfif_:cqe@fpfc^rupffrum{fih1k|m8\&nf_3k|ffih~g@glZm|TyblzTY`Z_glf
cIh)\~zxfih&kmiffbi3ec3_3@c3e3h1k@afklcglfc3f_qah1k@fvc3e@fc3_3@c3efiI\1@fZfiYff_3n@_qZnmZuh&c#h&ZbkmI\
Y[Z_3al\~j|@]vfifikglfc3f_qah1k@fc3e@fn@_3Zbmfilh)\h1cpZ{Y ff_qpmiqeY[Z_3al\~
_r[ z

Tr[:z

ff


\1ffi_p\1|T]vfifikglfc3f_3ah1k@f]:e@fc3e@f_on@_qZmfilh)\)h&cpgbh~pc3_ph1@@cph1ZkL3fic#hPfimfopfc ZfiYn@_qZmfi
lh)\)hP#cph~iiZkmpc3_Ih1kcxtve@fpcfikmg@{_gk@Zcph1ZkZfiY n@_qZmfilh)\)hP#cph~in@_3ZnZh1cph1ZkmI\Th1klY`f_3fkmif]Zbl\Pg3I
c3emfic E _r[z
5hY _ @r[ zhP]vh1c3elh1koc3e@f_fik@f ffmY[Z_vff_3gbhP#c3_ph1@@cph1Zkoc3emfic
3fic#hP fimfvc3e@fiZbkmpc3_Ih1klc h1
k x
f fimklh1cph1Zk|5c3e@fiZkmpcq_Ih1kcTc3emficZk@f:ifikMglf_#h&bf
klY[Z_3c3@kmficqf\&|b]:elh)\1fc3elh~ hPf_3kmfic3@_I\5gl
Y[_3Zah1cfi_qfcpnlh~iI\)\1blh&cqf]vffi}x 5Z_:c3emficv_3fbpZk| h)\PqpZkp@f#c3fgopc3_3fk@bc3e@fklh1k@Mc3elh~k@Z
cph1ZkZ{Yh1klY[f_3fkmifl{n@nl\&bh1k@c3e@f:n@_#h&kmiqh1nl\1fZ{YaMIlh1a@afklc3_3Zbn _{c3e@f_c3emfikMiZbkmh~glf_ph1k@MI\)\
gbh~pc3_ph1@@cph1ZkmL3ficph~Y[h1k@
|@]vfiZkmh~glf_Zkl\1oc3e@fgbh~pcq_ph1@@cph1Zk}ruzv c3emficembfc3e@f_qffic3fpc
fklc3_3ZnlfiaZk@oc3e@ZlpfM3fic#hPuY`bh1k@cqe@fMiZkmpc3_Ih1kcxM]vfk@Zfi]pe@Zfi]|Zk@fh&anl\h~ific#h&ZbkFZfiY:Z@_
_3fpl\1ch~Tc3em{cc3e@f _{kmglZa[]vZ_p\~g@Tafc3e@Z@gjn@_3Zfih~glfn@_ph1kmiqh1nl\1fgaZcph1fificph1ZkjY`Zb_c3elh~h1klc3_3Z@glmi
cph1ZkZfiY aM8h1a@afklc3_3Zbnoc3Zn@_3Zm{lh\)h~pcph~in@_3ZnZh1cph1ZkmI\_3fpZbklh&k@xkY icI|mc3e@fiZk@k@fic#h&Zbk
mfcp]vffkn@_3Zmfilh)\)h~pcph~in@_3ZnmZlh1cph1ZkmI\m_3f#Zklh1k@fikmgj_fikmglZaS]vZ_p\~g@pe@Zbl\Pgjk@Zfi]FmfY-Ih1_p\1iq\&f{_
"fiII3$ "x
! tve@fn@_ph1ah1cph1fn@_3ZnZh1cph1ZkmmII3-@iZ_3_3f#nmZkmgc3Zc3e@f@kmfi_3n@_3fgbh~ific3f#
! Fn@_3ZnmZlh1cph1ZkmI\lY`Z_qal\~oZfif_m I3 iZb_3_3fpnZkmg@T@klh~@f\1c3Zfikjf3pfklcph~I\)\1n@_3ZnZ
h1cph1ZkmI\}Y`Zb_3al\P&
%
Y`Zfi\)\1Zfi] ]vf_qfnl\PbiffiqeZVii@_3_qfkmifZfiYc3e@fn@_3ZnZh1cph1ZkmI\plamZfi\
' ]h&cq
(
e ")'3r * zx
c Z{Y n@_3Zbmfilh)\h~pc#hPisiZkmpc3_8h&klciZb_3_3fpnZkmg@cqZlk@Z8]\&fglbfmp,
f +. -/0 12
! tve@fpf
iZkmpcfiklc3Y[_3ff:lk@Zfi]v\1fglfmpfiZklcIh1klh1k@MZkl\1n@_qZnmZ_qcph1ZkofVn@_qf3h1Zkmx te@fiZ_3_3fpnZk@
glfkmifh~bY[Zfi\)\1Zfi]
354

fi687:9(;=<?>A@1B=C5DE<E7=F >HGJIK9ffC5CL<E7

MONQP=RSETUVT(WXWZY\[^]_:P=R]5``aWZSEbcSVd#Ye=]dfSgRhjiARlk/mn&UoP=P]5UVR\WZb=pqWZbsrOWt`R]lP(XtUEu]5vwTx[yYe=]
P=RSEPSgRY\WZSEbq]_:P=R]5``aWZSEb{zZz |}k~)n$zZz xVWZhWXtUVR\XZ[EUuSEbvgWZY\WZSEbULX1P=RSETUVT(WXWZY\[]_=P=R]5``WSgb
iARk/mKz mHtnHWt`R]lP(XtUEu]5vT([|}k~n$z | }x k\~)n(
MUEueuSEhPUoR\Wt`\SEbuSEb=b=]5uYWg]cWt`R]lP(XtUEu]5vTx[1)d/SER`SEh]8xUVbv]5UEuecKWYe
k1e=]PUoRY\Wtu(XtUVRue=SoWu]5`d/SER8Ye=]UoP=P=RSL_xWZhUVY]]5gULXWY\[quSEb=b=]5uY\WZE]5`v(Sb=SEY.hUVYY]lR
WbYe(Wt`8uSEb(Y]_=Y5n
1e=]SEYe=]lR]XZ]lh]lb(Y$`#YeUVYulUVbUVP=P?]5UVR#WZbUP=RSEP?SERY\WZSEbd/SERh(XtUk`\ueUE`#R$UVY\WZSEbUXb(=h
T]lR$`UVbvUoR\WZYe=h]lY\WtulULXuSEb=b=]5uY\WZE]5`nR]lhUWb=bueUVb=pg]5v)=SER]_UVhP(XZ]EYe=]qd/SERh(XtU
iARkZ1zZt?n8{ff1SE(XtvuSERR]5`\PSgbvYSYe=]P=RSEPSERY\WZSEbd/SERh(XtU=k\~)n$z1t)k\~)n$
=x


1e=]lR]Wt`AU8SEb=]l/YSE/SEb=]8uSERR]5`PSEbv(]lbu]8T?]lY\]l]lbYR=YeUE``aWZpEb=h]lb(Y$`AUVbvUVYSEh`l)Ye=]YR=Ye

UE``aWZpEb=h]lb(YA^uSERR]5`\P?SEbv=`YSYe=]UVYSEh{ )

5L 8e=]lR] Wt` Wd 8kt:nA:
SEYe=]lR1Wt`\]E^)]lY V 55 T?]Ye=]YR=YeUE``aWZpEb=h]lb(Y$`uSERR]5`\PSEbvgWZb=pyYSqYe=]

UVbvs
UVYSEh`1


L5


ffR]5`\P]5uYWg]X[E

1e=]lR]Wt`USEb=]l/YSE/Sgb=]uSgRR]5`\P?SEbv(]lbu]T]lY\1]l]lbP=RSETUoT(WXWZY\[vgWt`\YR\WZT==Y\WZSEb`SVE]lRYe=]`\]lY



SVdAYR=YeUE``aWZpEb=h]lb(Y$`.UobvPSoWb(Y$`KWZb
ffSER8]5UEueP?SVWZbxY&q

(XZ]lY v(]lb=SEY]Ye=]


uSERR]5`\P?SEbvgWZb=pP=RSETUVT(WXWY\[vgWt`\YRWT==YWSgbSVE]lR
ff8e=]lR] k/ \n


qVy/x)&

XZ]5UVR\XZ[E( z cm,WK



k| } n$1e=]lR]d/SER]E(d/SER8ULXX ff]eULE]

/l
k\ nAiARE k/mn$
Ke=]dfSoXXZSV1WZb=pqR]5`\(XZYv(]lhSEb`\YR$UVY]5`#Ye=]Y\WZpEexYuSEb=b=]5uY\WZSEbyT]lY\1]l]lbP=RSETUVT(WXW`Y\WtuP=RSEPS(`aWZ
Y\WZSEbULXHR]5UE`\SEb(WZb=p`Wb=phUL_(WZh=hQ]lbxYRSEP([UVbvRUVbv(SEhQ1SER\Xtv=`l
(L/gJ


5rc$LfO^$ lo
)ff

fiff xo

iARk/mKz nc

(lqffs?:xx/fco
l$=/{!^fi"#L

lE
ffff'&&r(*)V+,oE(mm , .iAR.-lk/m n0/c,Aff x$

iARk/mKz mn

iAR1k|}k2Lnz |} k2Ln 43


QV

$%

5
r /nH{iAR.-lk/mKz n

Ke=]lSER]lh6?8796Wt`UVby]5UE`\[uSgRSVXXtUVR[SVd.1e=]lSgR]lh:687;7E Sque=]5u<YeUVYYe=]P=R]5uSEbvgWZY\WZSEb`
SVdYe=]XtUVYY]lRYe=]lSER]lh
UoP=P(X[E1b=SgY]YeUVYYe=]quSEb`\YR$UWb(Y$`&WZbcrQUVR]XWZb=]5UVR58Uobvc`\SqYe=]q`\PUEu]
= > 3 0
r ?eUE`U=b(Wtg=]hUL_xWZh=h/]lb(YRSEP([P?SVWZbxY@ BAb UEuYL?WY#Wt`&]5UE`\[YS`\e=SVOYeUVY# C W`

Ye=]qk=b(Wtg=]VnhUL_(WZh=h/]lbxYRSEP([P=RSETUVT(WXWZY\[vgWt`\YRWT==YWSgbcSVE]lR
`UVY\Wt`ad/[EWZb=pYe=]uSEb`YR$ULWZb(Y$`
rDAbUEv=vgWZY\WZSEb 8T?]5ulUV`\]qYe=]lR]qUVR]b=Syb=]lpxUVY]5vyP=RSEPSgRY\WZSEbc]_=P=R]5``WSgb`WbrYe=]d/SERh(XtU

3


E

c| } k2Ln 43



0
r ?Wt`8u]lRYULWZb(X[]5``\]lb(Y\WtULXXZ[PSx`aWZY\WZE]E

Sx`YUVP=P(XWtulUVY\WZSEb`SVd#P=RSETUVT(WXWt`\Y\WtuP=RSEPS(`aWZY\WZSEbULX.R]5Ug`\SEb(WZb=puSEb`aWtv(]lR`aWZhP(XZ]uSEb`YR$ULWZb(Y$`

SVd8Ye=]dfSgRh`\]5v

WZbYe=]Ye=]lSgR]lhAUVbvy`\S`\ue^UVP=P(XWulUoY\WZSEb`ulUVbyT]EWZ]l1]5v^UE`&E]lR[`\P]5uWtULX

ulUE`\]5`SVd#Ye=]RUVbv(SEh/1SER$v=`UVP=P=RSxUgue FAbydUEuY5HYe(Wt`Ye=]lSER]lh
1e=]uSgb=b=]5uY\WZSEbT?]lY\]l]lbyuSE=bxYWb=pHG1SER\Xtv=`JIUVbv

Wt`]5``]lbxY\WtULXXZ[^Ug]lR[SVXtv,SEb=]E
WZb^U`\PUEu]v(]%Kb=]5v

Ye=]]lb(YRSEP([qhUL_(Wh=h

UE`UuSEbL=buYWSgbSVdXWZb=]5UVRuSEb`\YR$ULWZbxY`W`&E]lR[q1]XXM<(b=SV8b NAaYeUE`#T?]l]lb]_=Y]lb`aWZE]XZ[`YvgWZ]5v

WZbYe=]*K]XvSodYe=]lRhS=v([xbUVhWul``\Y$UVRY\WZb=p1WZYeYe=]B7POVYeu]lb(Y=R[SgRJ<SVd

R

E

U_:1]XXUVbv4QWZT=T`l

]5u]lb(Y\XZ[E=Ye(Wt`AY\[(P]8SodR]5UE`\SEb(WZb=peUg`AT]l]lbUVP=P(XWZ]5vYSP=RSET(XZ]lh`WZbUVbNSAuSEb(Y]_=YATx[iAUVRW`UVbv

TPU

fiVXWZYZ[]\^`_H\ZaZb[]cdWZYZ[fegWZhZi?^XjZ^lkmYZnZa'\op

qsr9t u%vw'xfiy{z}|~P;.zt ; z;xJm|~P;.5Zrd5v;JyBvsz
xXzt Nqsr9t u%vwZxy{z
x zJ
u%
z8Jr%8r9
w{ztsrPu9z xrdJZr9Nz
xvJrPz89rSZrtZrPu%rPxJx8Bvz;v;Z8tZ(zdMv;Jz'tZv.8v;tNvX9zZZJvz8v;t Z
z8JZv;Z.4JZr(ZJrPu
xrBr9z
xvJZr%8zZZJvz.uJ.r95MJv;v;Z%x9
v4JZrB rPxfi(v5v;ZytZv58rP;r;Nvxv0JZrB5v;JyFv;tZJv; z
x
uZJv;vx88v;t zXJrPz.xv;tZ
8tZ"zt zXMv;JgzsZJrPxr9tzfiv.t xv5JZrNr9tJJv; 0v.
ZxSu%v;tZtZrPu%fiv.t|M8t zJ
u%
zPmJZvxrBv
|szfi?xqsr9t u%v{wZxyzZ5~P;;Zm; z.xJ5~P;;'J0 zw;r8B8JrPfJZr9gxfir%w.rPxJvu%v;tZt u%8v;t xv8tZ
rPz(u%v;t xfiJz8tx9Z(}v;JrB;r9tZr9zs
ztZ. z;rN8w;rPx x(z;rPz(rPzXvz;Z.88v;t z0r%ZZJrPxJxw.r
v5r9P4Zv.(r% zN8r;
xB;8JrJrPz;xfiv;t z8rNJvFztJZr4z8Jvr%ZZJrPxJxJ zZv; r9JfirPx
zJrF|zZZvP8gzJr%8xz
x
u9z88t r9 r9t r9tPDZv;Br% zN8r;55rgzP5
xJvz;xxr9JBJ z
5
J|Nzt Pffd|NzJrt r9r9t r9tZJv; r98rPxDxJz;8tZ`85ffJ |
d|98
85ffJ |8 8
*|? s8rPz8; xfi uJu%v;t xfiJz8tx5zJrdtZv;8tZrPz0*r9w;r9JJZr%8rPxJx$v.ZS5Zr9
v;Jr9 8~;~u%vw;r9x5x u"u9z;xfirPxzt B uJ4}v;Jr;
w;r9xv.tvXZJv. z
xfi?uZJv; vx88v;t zsJrPz;xv;t8tZ z;xdz
xvN r9r9t xrPFJv}ZJvw;
r(Zv; z
?xfi
uNxr9gzt
u9xv;dr%z8JrPz.xv;t8tZ|mrPz~P;.X*r9Jrz{?xfiv Zr(u%v;tZtZrPu%8v;tJvNzt v;
5v;
Zx*
xv58tJr9JrPxPtf zJfi?u%
z8Mv8v{dx*Jv.:Xv;Jv
zJ ?~PN zJZrBJrPu%r9t(5v;Jy4v
v?Zxfi9B
PFv;J
x9zt mrPz5|~P;.0u9ztF rBr9B rPZrPF8tJZrBzt v;NM5v;
ZxzNr95v;Jy
tFJZrJrPx*v?x*xZ xrPu%8v;t5rr%'
z8tJZr%8zZZJvz;uzt 4JZrr9B rPZ.8tZ
v;t x
r9Xz*
ztZ; z;ru%v;t x?xfi8tZ(vZJv; vx88v;t zZMv;JB
z;xvw;r9mJZr5ZJv;vx88v;t zw{zfi?zrPx
PPZ mzt r%z8rPxv5JZrBMv;J:|JrPz;$NxzJrN?u9z{8BxJZr9Jrg
zt NzJr*ZJv; vx88v;t z Mv;JB
z;x9 .
xJ8ZZ8v;t4
xxJz
Nv(%9;ffff
%gzr%z5J8rl:
(m|NB~B9tz.Z.fiv.tJvr%z8J8rPx95JZrMzNr95v;Jyfz
xv r9B8xNZr xr4v
gzr9
z8N
u9z8v;t8t"z}J8r;z;x8tB .
xJfiZZfiv.t"?xdxJz
JvgxJzfi?xx u"zJ8r
Sm|NN~. JNPM$ P% J

Mff"ffff% Zff
H|X

*
xzu%v8rPu%8v;fi
ffP
gv
ZJv; z8".
xJfiZZfiv.t xdv{w.r9
Z z%zNr9Jr989rP9 X
ffP xz
x rPxzNxr9
v
J8rPxv.(r9w;r9J9m fixJz
x rPxr9w;r9Jr%z8BJ8
r ! zt fxJzfi?"x rPx(r9w.r9JtZv;tZr%z
J8#
r $ xr9%
vr%z8J8rPx(fiMr9tz{
x v;dr9w;r9JX
zfixJz
x rP&
x "
8 (') |NB`~;
xNxZvt8t | r%tZr9mrPz~P.;XfiMr9tz{8Nr9t vxJxfirPxJxrPxNztZB r9NvJrPz.xv;t z8r
ZJv;r9J8rPx*
u9z8z;xJxvZu
zJrP4r%z8dJrPz;xv.ttZ8t u .8tZzBZr%r9Jr9t u%rv.Nv;Jrxr9
u*]u58tMv;Jgz8v;tsv5r9w;r9PJZr9rzJrSzdtZr9XvrPx8z8rdZJv; r98rPxX zm80vrPxmtZv;X zw;r;
Nv;tZv;JZr9dJ8tZx98JJr%8r9w{zttMv;Jz8v;t4
xdtZv;*8;tZv;JrP|J;r9rN"| +5z;u9u xdr9z8s~;ZsMv;
zt4r%ZJr9t x8w;rB.?xu% xJx8v;tvsJ
x
xJxZr;
v(v.Zz8tz.Z.fiv.t zrPx8z8rZJv;r9J8rPx9 xfir9gzt
u9xs
xr%ZJr9t rP}8t| v
Zx9?5r95z8
P~ ;.Szt zZ
u9zfiv.tvSZrgZ8t u88rvdgzBZ:r9tJJv;;t xJrPz;vSu%v;t x
r98tZHz
vxJx88r
Bx9z;x4z vw;r;d5ru%v.t x
r9"v.tDJZrfX
-,Z/ . 0 132 xfi uJJ zPv.4rPz;uJ9

/ . 0 1 z;x*JZrBgz8Zr9tJJv;zNv;tZ4.
xJ8ZZ8v;t xJ zSfixJz
xM"zXJZrJ8rPx*8t "|J;r9r
| v
Zx9B
r9Sz8~P.;Mv;5ZJrPu
xrBr tfiv.t xzt NrPuJZt
u9zXr9z
x9*v;JrdJ z'xt u%rJZr
u%v;t xz8t%x xrPJvr tZr5/
. 0 1 zJr5z8tZrPzP.JZr9Jr
xt r9rP}zZt
.ZrSx u( vtsv gz{8BZ
r9tJJv;; J8r"
xgz5
476X
% 99
8
:;Z; 8v%
8 (') . 0 1 |NB4 ~;
5ZrtZv;8v;t4vX9
<sM
z x88rgu%v;t xrP.Zr9t u%r
xzt z89rP48tr9z8tf| v
Zx9?dr9Sz8~P;;%
Zr9Jr8d
xxZvtJv}tZZr9fiBzXJZr(t
u%rNZv; r9JfirPxv0Mr9tz8Nr9tN|x uz;x*JZrBZJr%r9r9t u%r
Mv;Nv;JrNxrPu* ]uB8tMv;Jgzfiv.t8rx u9u%rPxx8.tZv;8tZ48JJr%8r9wzt8tv;gz8v;t#
<; z8
8N v.Jzt8;Zz8;v;8JZgxzJrdZJvw;
rPv.0u%v;NZZfitZBJZrd=
<
z x88ru%v;t xrP.Zr9t u%rPx5vzxr9
vsJ8rPx58tu%r9Jz8tu9z;xrPx$
>?

fi@BADCFEGIHKJMLNOPGPAQ/HSRTUCVNNWGPA

XBYZ%[]\W^_[`Y[#a(b;cFdZePfFghZibjkYFldmj3n;\cpoqb#Yqjkbrsdetjvuexwydiuq\d%dub#\ffZe\znu9e{)|}el~rjk;[_rFd
b;d3\WlKzPFn;\c9oIb#b;[`oqbrrFbr7_c

ePYZ){(Zm\[b;wezZ#_c\$jvdZm\W_PuFdk{(ePZwB\Zmr[\ccb;Z

b#j_[#fFlg

dZm\cqjl~\db3\#rFb{\YFldBZYFlb3`e{Kdub){(ePZ[_cFde\qZmjkda(ePZrFb;ZrFb{\YFld)ZYFlb
%B;U;|"m ;M|"%P
\Pj_c$ePYZb\Zkl*_b;ZdZ\cqjl~\dk_ePc$e{KM_*l~jjkePc/j\ffZie\Pniu/ePdb%diuq\dMdub){ezZ[`YFl\zjMduq\dB\Zk_~jkb%YcqrFb;Z
duF_~j%diZm\cqjl~\dk_ePc\Wl*l
Yqjkb#dubji\[#b\ffZie^F_[\db`bzYq\Wl*_d"g

nePccbndv_zbh

#MubZb\PjkePc7_~jduq\d

dubh\ffZie\Pniue{|}el~rjk;[_rFdb;d\WlBzPFYqjvbj`dub$j\[#bh{(ePZ#\Wl*lrFb{\YFldZYFlbj;

bhn;\c

j_[`_*l~\ZklgdZm\cqjl~\dib\s|cePcarFb{\YFld;ZiYFlb]$e{Mdub`{(ePZ[_cdie=\]qZjkda(ePZmrFb;Z3nezcqjkdZm\W_cFd
Yqj_c]YcF_zb;Zmj\WlzYq\cFdk_*n;\dk_ePc/




;(/

&|

|"K

|"

cqrFb;ZduF_~jMdiZm\cqjl~\dk_ePc/Vwb3n;\c$fZePb%dub){(el*lewM_c]dub;ePZb;[7


`FWq-(V]/W7]hq;(
P/~$]q;*PP;;;hi;
;/v3P(;mm;*%m





cfq\Zdk_~nYFl~\ZduF_~j`dub;ezZb;[

%9h
*Fm;*`mq;mD%kB

ZF

| | /

P

;



_[fFl_bj]duq\d\xllBdubhnez[#fYdm\dk_ePcq\xlUdibnucF_~zYbj\cqrZibjkYFldmj

rFbjnZk_oIbr$_c|})el~rjk;[`_~rFdb;d)\Wl/PzF
n;\ZZg]ePb;ZBde#diuF_j)jkfIbni_\xl
[#b;duer

d\xljve

jkuewjduq\d/Zm\cqrFeP[#a(wMePZkl~r)fZez_rFbjS\MfZk_cqni_fFlbr

n;\Pjvb3e{Kdub%Zm\cqrFez[#a(wMePZkl~rj

Yqjkdk_*n;\dv_ezc3{(ePZ/diub
\ffZe\Pniu

|})el~rjk;[`_~rFd%b;d3\WlSPPFfZbjkb;cFd`|ePcb3wuF_niu9_~j%zYF_dibrz_b;Zib;cd{ZieP[diub
_c

Yqjkdk_*n;\dv_ezc=_zb;c

|})el~rjk;[`_~rFd)b;d\WlPPD_dmjvbl{Wm

* ff
fiDfi
c Pbndk_ePcq]wMb#ZbjvdZk_~ndbr\ddb;cFdk_ePc9de=j"_[fFlb$zYb;Zk_bj;9XBYZ`[\W_cZbjkYFldKMub;ePZb;[qPz
cb;brFbr$ezdub;ZB\PjjkY[#fdv_ezcqjB\PjwMbllKbjijkb;cdv_\xlfqej"_dv_z_dkgPdub%b^F_jvdb;cqnb3e{K\3YcF_~zYb3[\x^_[`Y[#a
b;cFdZePfFgfIe_cd
!

x\cqr&dub
ZibPYF_Zb;[#b;cFdKduq\d"$# %&(| ('!/
!

boqbl*_b;PbBduq\d/duF_~j/diub;ePZb;[fi_j

_cjkfF_db`e{K_dmjl*_[`_dm\dk_ePcqj\zjrFb;[#ezcqjkdZm\dibr$oFghdub#rz_~jnYqjj_ePcs_c)Pbndk_ePc*q+V

Yqjkb{(YFl

b;Pb;Zdiublbjij;

duF_~jZbjkYFld3\Wl*lexw)jBYqjde#dm\Pb3\PrFx\cdm\Pb%e{
ePcFlgh\#jk[]\Wl*l{(Zm\P[b;cdBe{ePYZ)Zk_~niu$l~\cPYq\zbP-,\c
wMbqcqr=\][ePZb3zb;cb;Zm\WlKdub;ePZb;[/.0B{(db;Z3\Wl*l/diuboq\zj_~nnezcqnb;cdZ\dk_ePc9ZbjkYFld`|"Mub;ezZb;[1++D
uel~rj)wM_du9bjjkb;cFdk_~\Wl*lg9ceZbjvdZk_~ndk_ePcqj;

c9duF_~j%jkbndk_ePc9wMbjkuew



b^db;cqrUub;ePZb;[2qP]j_PcF_*n;\cFdklgP*3BewMb;Pb;Z

duq\dM_d)_~j_cqrFb;br9fqejij_oFlb]de

dub;Zb$\Zb$jkb;Zk_ePYqj&l*_[_d\dk_ePcqj#\cqrjkYodklb;dk_bj;

b)_*l*lYqjkdZm\dibdiubjkb%fZePoFlb;[jog][#b\cqjUe{b^q\[#fFlbj;I\cqr$dub;chjvdm\db3\c$b^db;cqrFbr
XBYZ#\didb;[#fd3dies\zrrFZbjjdubjkb]fZePoFlb;[j]|kjke7{\Z`\Pj&_~j`fqejij_oFlb&lb\zrj`de
fFl*_~n;\dbrsqcq\Wl
ZbjkYFld
dub$dub;ezZb;[

ZbjkYFld

\$Zm\dub;Z#neP[#a

c9{\PnddiubfZiePoFlb;[j%wMbrz_~jnYqjij\Zb#\PjM_cFdb;Zbjkdv_c9\cqr7_[#fIePZdm\cFd%\Pj

wMbh\PndiYq\Wl*lg_zbP$dub;gublf

b;cFdZePfFgP`XM{UnezYZmjkbPb;Pb;Zg

YqjYcqrFb;Zmjkdm\cqr[#ePZib$e{

_~jjkYb`wMbrz_~jnYqjij)_c

de7[\W^F_[`Y[b;cFdZePfFgj)[\W_c

diub]l_[`_dmj]e{

[\x^_[`Y[

duF_~j%jkYoqjkbndv_ezc9_~j%Zbl~\dk_Pblg9[`_cePZ3neP[fq\Zbr

|k\ffq\Zb;cFd;ZibjkdZk_~ndk_ePc/KwuF_niunePcqnb;Zcqj`dubYqjkbe{cePca(Ycq\Zg

fZbrz_~n;\dbj54ePZ)dubZb\PrFb;Z%wBue_~jlbjij%nePcqnb;Zcbr

\oIePYd)dub`ePdub;ZVlbjjkb;ZI_~jjkYbj)wMb`Zb;[\Z

duq\d_d_jfqeFjj_oFlbdiejvP_f=rz_Zbndvlg7de6Pbndk_ePc7V
pb3qZjkdnePcqj"_rFb;Z`dubZbjkdZk_~ndk_ePcqj&wMbfFl~\PnbrpePc

diub/8:9K\cqr

jvuexwydiubrz_;tnYFldk_bj`duq\d

\Zk_~jkbU_{Swb)rFZePf#dub;[7pbjkdm\ZidKwM_dudiubBZbjkdZv_ndv_ezcde3\j_clb%[\W^F_[Y[a(b;cdZiePfgfqe_cFd$0%j

<=

fi>@?BABCEDGFIHJDBKBLMCEN-?BABCOP?BQBRSF@TBFVUABWBKDGXY

Z\[B]_^a`cbd^a]ebfZ\gahZjik`lb6Z\[B]e`lg\]emonqpr[B]e`lg\]emtsGukvswyxj[B`z_x|{lZ\[B]}]ebZ\g\`c~f`yhMkm`fxjZ]el]eg\zr`lgjixbB]hg
mPhMikmBmu(BZrikZ:`f]xbB`lZr`k`zZ\[dhZ_hZ\[B]5mPhfikmBm]ebZ\g\`l~P~`ikbfZax_hg]5xjBg\g\`lBbd]f
xikmihg@bBmd]egx`Ezr`lgSBx|up[fdxe{MikbZ[B]}~Bg\]xj]ebd^a]}`Em`lg](Z[dhb`lbB]rmPhMimBm]ebZ\g\`c~f~d`ikbZ{
zr]hl^a]6Z\[B]6~Bg\`lk]em`rdbdcikbBZ[B]Pg\]ahZic]ikm~`lg\Zhbd^a]l{`lgz(]aikl[ZjikbBd{$`_]hl^[mPhfikmBm
]ebZ\g\`l~P~`ikbfZMu@:x}Z[B]-`k`zrikbB/]aGhm~k]:idxZ\ghZ\]x|{dZ\[ixzr]aic[fZjikbBix}`Z\]ebxj]ebdxqiZic]Z`Z\[B]
Z\`k]eghbd^a]:hMkB]xeurB`lgrZ\[ixg\]hlx`lb{bB`lbBBbilB]]ebZ\g\`l~mPhMikmPh`Z\]ebk]hl6Z`bB`cbBg\`lBdxjZ\bB]xxeu

Sf

B~B~d`fxj]JraM{hbd/^a`lbdxi]eg-Z\[B]5bB`zrk]l]:dhlxj]

:

nenqffwa5}Bsw/nenjwa}rBlwa

:x\xjBm]:zr]:z}hbZ(Z`P^a`lm~BBZ\]5gnqnqMw :

ln

hbd

:_



`lZ\]:Z\[dhZ-




Bs

_

iSx

`lgr

:

B} f

Bs`lgr

:

Bcf

ix
ln





w@

wubZ\[ix_^ehcxj]l{}B
:_

:_



w@

V


_

[dhcxrZjz(`mPhMikmBm]ebfZ\g`l~f6~d`ibZx

`z^a`lbdxi]eg}Z\[B]:mPhfikmBm]ebZ\g\`l~P~`ikbfZaxr`@





nqBsBaBlwyhbdnqBfBsw\u
:-

u$ZrixrbB`cZ}[dhgZ\`Pxj[B`z


`cg


Z\[dhZi({dZ\[B]ebZ\[ix:xj~dhl^a][dhlx:hBbiScB]mPhfikmBm]ebZ\g\`l~~`ikbfZ{njBs-JMB-*Mwu
bZ\[ix^ehlxj]l{@g

nqnqMw :

w5Bs u*}bZ\[B]6`lZ\[B]eg[dhbd{i_ /

{Z\[B]ebZ[B]BbilB]

mPhMikmBm]ebfZg\`l~~d`ikbZr`yZ\[ix}xj~dhc^a]}ix-nqG$ Gsr/ w{ib/z[iS^[^ehcxj]g
{rZ\[B]ebJZ[B]xj~dhl^a]}B
:_

B u 5

xjmm]eZ\g/z(]`lBZhMikbZ\[dhZ:$g

nqnqMwe :

`d{h~B~Bg\`l~BgjihZ]a^\[B``fxikbBh6xj]cB]ebd^a]

wrIBBu

]aikZ\[B]eg:BsB{BBG{f`cgGfu$pr[dx}$gnqnjMw :

wr`f]xrbB`cZ}]afixjZMu

ZiSxbB`lZciSxqBbd^aZjik`lbdxBe6Z\[dhZ^ehdxj]/Z\[B]/~Bg`lk]em[B]eg\]
::

nenqffw



Bsw nenjw


w@

{@zr]^ehb mPhl]/Z[B]hlxjm~BZ\`lZji^6hMkB]`Z\[ixgahl^aZjik`lb
B

`:Z\`k]eghbd^a]l]^aZ\`lgax^a`lbl]eg\ikbB*Z\`

BhZhdhlxj]

nqnqMw :

[dhcxZjzr`mPhMimBm]ebZ\g\`c~f~d`ikbZxe{}hbd)f

h/BbicB]/mPhMikmBm]ebfZ\g`l~f~`ikbfZ`



:_





izr]/^a`lbdxi]egikbdxjZ]hlZ\[B]

Bw{Z\[B]ebZ\[B]eg\]:ixbB`ci^akZjlu:pr[B]eg\]

nqBBaBBw

iSx

hbdZ\[B]Phlxjm~BZ\`cZji^~Bg\`cdhiikZj

$ganjnqMw :- w$BB{Bhlxrzr]:zr`l/z}hbZu

bikl[Z_`@Z[iSx]adhm~k]6nqhbd/m6hbfxqimihg:`cbB]x}zr]^ehb^a`lbdxjZg\d^aZew{Bzr]^a`lbfZibB]Z\`PhlxxjBm]
Z\[dhZ-Z\[B]eg\]ix:h6xikbBk]mPhMikmBm]ebfZ\g`l~f/~`ikbfZMu:xzr]hg\lB]]hgjik]eg{zr]5]aB~d]^aZ:Z[iSx-Z`P]
Z\g\B]-ikbZq~i^ehMy~Bghc^aZji^ehMh~B~i^ehZjik`lbdxe{x`Z[B]5g\]xjZgji^aZjik`lb`f]xbB`lZ_x]e]eml]eg\/xj]egjik`ldxeu



]bB`ztZ\Bg\b`lBghZZ\]ebfZi`cb*Z\`Z[B]Pg\]cikg\]em]ebZZ\[dhZ

nM wtGuP:xzr][dhMl]PhMkg\]hl

`ldxj]egl]{$Z\[ixxj]e]em6x5Z`]hb`lfcik`ldxg\]xjZ\giS^aZi`cbJZ\`mPhl]l{$^a`lbdxi]egjikbBZ\[dhZZ\[B]Bbd^aZi`cb

ff


fi

nM wriSx-bB`cZ5]adbB]`lZ\[B]eg\ziSx]lu

}`zr]el]eg{Z\[ix:ci^akZjixhl^aZ\dhMkh6mPhbi]xjZhZjik`lb`(h

md^\[J]e]e~d]eg~Bg\`c]emu:x:Z\[B]``zrikbB]adhm~k]x[B`z-xe{$hbf*h~B~Bg`fhl^[*Z\[dhZqdxjZdxj]xZ\[B]
mPhMikmBm]ebfZg\`l~P~`ikbfZ}`(

ff





::



zribB]^a]xx\hgjik/hMiikbxj`cm]5^ehlxj]xrz[B]eg\]

nM w$Bu

`lbdxi]eg}Z\[B]fbB`zrk]l]:dhcxj]


!" #
$ %& 'ff# ( *),+ .-"
/0213546387:9;46<>=6?#9@=%AB=ABC!9EDBC6F:G#FHC6CABIDB3:=6FKJLFHMC6=646N.JL=O3QPR9@7KG'DB3LCOF;S27!NDB=ABG'DB3:7:9EP'AB7!N.75T(3LM'=646FUG'VG#F;ABM'=6CWI'VXN.CABMY
Z NR9;[.4\9;=ABJKJ8FUM.C6=64\9EABM'=6CO4\9;=6?.3L4,=6?R9;M]['ABCB^QNM.JL=ABFHM_0
:

nek (

nqffwS

_

w

/n

Bnqwe (

`*a

nqw

_

w

(

n

@a

Bwa

fib:cd efffhgOiKjffk.l*f*cffm,gonqprdsk.kf*c
t*uffvffv2w xLy]zKy:z:{#|}5}@w~Uw_vffuff}@y]OH@!ff*,!U.ff'# ,*,!U.ff@UWy~'{#|y.{*xQ~Uw*|2~;u2
6@w*Kffy'w_@y'h]}@2{#}X}@ (xX y'*@y'yw#O2yUyU(x]ffh{*x:zKy$zw_u yUvhy.~U}.>w#zKy'*y'.ffzKy~'{|ff|ffw*}
@y.{*~;}@ (x]~Uw*|2~;u2xQw*|u2xQ|ffKffy'w*@y'2*]w*]{#| }@ |ff*y}.sw*]~Uw*|2xQ( y'}@ffy{Ruff
y'| }@@w*v vhw#|}w#!# ]> Kffy~Uw w*H_|2{#};y.x*#K~Uw*@@y.xLvhw*|2_|ff};w!# K{#|2*
~Uw*@;y.xLv2w*|2_|ff]}@w$O!,K# *{#;y!2w_}@ffo:y'|2~Uy* h('QH#6s Off*xLw:};2{#}rffy'w*@y'2*
w y.x:|ffw*}:{#vffv *
uff}.h{*x:zKyx@{Rh}@ffy$vff@w* y'(x:w_@y]6uff|2ff{#y'| }H{\:Kffy| 6w*@{#}Lw*|zKy]|ffy'y.\}@2{}:}@ffy
vff@w*vhw*@}Lw*|&w,2*|ffvhy'|ff*u |2xK(xK'y';wffO(x:xQv &|ffw*}Kvff@y.xLy'| }5{WzKyX|ffw#z(xK}@ffy]{Ruff
y'| }@@w*v v2w| } y&~'{#|w*ff}U{|}@ffyx;{#yxLv2{*~Uy ]> 8{#|2}@ u2x$}@ffyx@{#y{Ruff
y'| }@@w*v vhw#|}];w*_u };y_,y'@y'|} |ffwRzry. _y2{*xLy.xQ|v2{#@}E~Uu ({#:~Uw*|2xQ( y' ]] z: (~;
xQv {*x;xLy'@}Hx}@2{#}'ff# (%8H O# W*%!HffHK (x$|ffy'z |ffw#zKy. *y2{*xLy
}@yU(x]u2x|ffw*}@ |ff&z:2{#}UxLwy'_y'{#hw*uff}]}@ffy$6H{*~U}Lw*|w#O2*|ffv2y'|ff*u |2x'o{#|2|\{*~U}}X(xXy.{*xE}@w
xLffw#z}@2{#}]O @!ff*,!U.ff' ] :ffBff uff}>w#~Uw_uffHxLy$}:(xv2w x@xQ y}@w&_(xL}L|ff*u (xL}@ (x
~'{*xLy@w_}@ffyvff@y'_w*u2x$w*|ffy]Lu2xL} w w*_|ff{}& \}]6w#w#z>x]}@2{}]|ffw@y.xLu }$|}@ffy&xLv L}w#
Kffy'w*;y'2*$\z ~;Lu2xL}:u2xEy.xK}@ffy]R{Ruffyw#]ff ~'{|&2y~Uw_vff@y'ffy'|2xQ*y*
rffy]yUs{#v yxEffwRzXxK}@2{#}K}@ffy]vff wxLw*vff &2y' |2Kffy'w*;y'2*X~'{#|ff|ffw*}:hy]yU};y'|2 y.&*y'@
\{#.,{#}W{\%},(x%|ffy'*}H{ y}@2{#}o}@ffy'@y!zr2hyvff;w* y'x,zffy'| 2 Os uff},}o(x%|2{#}@uffH{ff}@w
{*xL$z:ffy'}@ffy'5}@ffy'@yK(x!{$_y';y'|}K{#vffvff@w{*~;{}@w**y'};ffy'O|z: (~;}@ (x@y.xL}@L(~U}Lw*|~'{#|2yX@yU({ffy.
K2{#}(x'_x}vhwx@xQ y]}@w~Uw*|2xL}@@u2~U}K{]}@y.~;ff| (*uffy6w*~Uw*vffuff}L|ff y'_@y'y.x5w#hyUyUO|}@ffwxLy>~'{_xLy.x
z:ffy'@y xzKyy'|}Ew_|ffy.&|t*y.~U}Lw*|h*ffzKy* }>ffw*vhy}@w w&}; xX ~Uw*vffuff}E|ff
L: ] ]{*x]{6uff|2~U}Lw*|w# {#|2}@ffy'|}H{#_|ff}@ffy}{_x *w y.x]};wff\|*y'|ffy'U{\O}@ (x
xLy'y'x*y'@2{#H uff}.O| }@y'@y.xL}E|ff*}@ffy~Uw*vffuff}U{#}Lw*|2{:}@y.~;ff| (*uffyw#;>w#(ffxL'( }y'}{\
.*_ > wy.x$u2xLy}@ (x$}Lvhyw#:v2{#U{#y'}@L(~{|2{xQ(x'5 y'w*|2xL}@U{#}L|ff}@2{#}}; |ff x_}$|ffw*}hy
xLw2{_6w*:R{Lw*u2x:;y.xL}@L(~U}@y.~'{*xLy.x'KX|ffw*}@ffy'>xEw*uffH~Uyw!ffw*v2yx}@w;y'y'2y'X}@2{#}:{Ruff
y'| }@@w*v &(x'%6w*]u2x',y'@yUw*|ffy}@w w#O6w*~Uw*vffuff}E|ffH{|2 w*6zKw*L(ffx] y'*@y'y.xw#:2yUyUEKffy'@y
{hyw*}@ffy':{#vffvff;w{*~;ffy.x!};2{#}K v2{*x@xKy'| }@@w_vy'| }L@yU*5\|v2{@}L(~Uu ({#.2xLw_y>wW}@ffy]}@ffy'w*;y'x
zKy]#*yX| {*~'~;u2xKy'}:{\%.*#ffO~'{#|&2yxLy'y'|{*xK w#|ff}@ (x'ff}@ffy.xLy]};ffy'w*@y'x5zKw}@y'|&{#vffv
y'*y'|!
X|ffw*}@ffy'K{*x@xLuffvff}Lw*|{* y:}@ff@w_uff*ffw*uff}:t*y.~U}Ew_|2 X(x5}@2{#}!};ffy:|ffw#zKy. *yX2{*xLyX2{*x5{xLvhy'
~;({26w*@#|2{#yU X * ]: *zffy'@y xy.x@xLy'| }L({vff@w*vhwxQ}Lw*|2{,{#|2 ]] w y.xO|ffw_}~Uw*| }H{|
{#| wff~'~Uuff@@y'|2~Uy.xw# #Kffyw_@y*y'|ffy'H{!}@ffy'w_@y'zKyxE}H{#}@y$({#}@y']@yU({ffy.x]}; x$xLw*y'z2{#}.{*x
6w#w#z>x
q|ffw#zKy. *y2{*xEy ] x]x;{(};why O6 @ .! 'ff U
}2{*x$}@ffy6w*@ ]] oz:ffy';y ~Uw*| }H{|2x$|ffyU}@ffy'_u2{#| }L 2y'Hx$|ffw*vff@w*v2w_@}Lw*|2x'O{#|2 ]>
~Uw*| }H{|2xK|ffw*|ffy$w#O}@ffy~Uw*|2xL}H{|}:xL 2w#(xX{#vffv2y.{#E|ff|w*5| ]
\}KxLffw*u (2y]~;y.{!}@2{#}%{$*uffy';5 x5xQv yXw_ ] 8{*xO{*x@xEuffy.$|vff;y'*w*u2xKxLuff2xLy.~U}Ew_|H
}@ffy'|}@ffyxLy'v2{#H{ }L~Uw*|2_}Lw*|(x:x@{#}L(x 2y.
]xO};ffyKww#zK|ffyUs{v yxLffw#z>x#,zyX w|ffw*}K{*x@xLuffyXxLy'v2{#H{# }L*ffzKy>~'{#|y.{*x8;uff|| }@w
|ffw*|ff@w_ffu2xL}:2y'2{_w*.
w_|2xQ( y'O}@ffyO6w#wRzr|ff] |ffw#zKy. *yK2{*xLy ] w#*y',}@ffyK*wff~'{#ffu ({#@

$8H $ ff $ @ $8H ] $ @U










! #"%$'&($)


fiff
4

5

+*,& -."

0/

132

-&

76

8

:9

;

.6

KJ

<>=?A@CBD EFGIH

:N

UT

5N



AV

:N

UT

N

MLONIPQSR



WYX[Z]\U^(_(`+\Ua0bdc+eUfSg^ihQjS`_lkk`h-_lgnmoeUcpc^q_lf-r+eUg0b'eUr_;\Uch>cj-sg;^cte:uvrm^xw-fhlyA\U^;z{0^|'_(c^eUc^;}-j-eU~Q_#\U^;f-rxrhIhQfS^xh(uvrmYeUc
uhQ`qX



fi[S[5%'

od8;-v'0'GI((#v08l(fi-(MSQI;vd0'
o0dv#Q!'
Gd!]'
UQ7dQ]G(KS;,(5SGK-l(dGE]'S8id(fiv0Q'oo
'(0dv#(-!llE
SK-G((dGvGo;IEfi
0%]08(v5(,(#-v0
'(,0d';v0Q5;S'vS(,fi+v00
fiGd!S0(EG-fi(-,']070(
Iv'-7K,I-(,vG(.G(ddldEfi(0fi3(l''((-0'v0-oIIdG
0v08x0;So!M'I-d-IlG

(-d;didv.G-!(8]-vSdd!'
x




;Sod





Q7

Q
'x0dvG-;fi5

Soox

8;Gl-G;#5S7;3(v'




dl'7(E-vAIGGv05(vx
d(-4

(:A(




:;A#

S0o

;Sooox



(d(-iv'vAG7
7













E-:tl




ff


fi



'((d
(5fi-;>'(



GI-55
G

Q7
U



x

pQ;o;];

,+







/



:tQ

'IGdvd;


;0';;;-SGd(5;d

)

U
'v,(fi-(


-

v

.


ox

-



(id(;(I7K0G#Q0QxII'l(-0((.dIl;5

.



(-o




oG-]-vfi!O

,

o(S(fiG-
;77K'vl

nx;3(v'



-lol



oo



.

l:t#


3:tQ


5ld(-iv'vA

d,IGv-(7K;v-vSddK'

;lv'x(
ooxfi(dv;


d(

0

0fi

;-5;->'x(dvfi(;d80((fivId-#Y

2143657198;:=<4>?365@<
4C

E
-H
KJLJM

0d0#fi0((#Q';;;-SGGfi(5';dK(

vinS;ov0dG(('d-Q.SnSQp('i



o'iU

-(ldvG'

id('7'(8-vG'7'>(v'ox

(d

t0ooIdI-(0d(d]G8I-d(-

U
7



(fid8I>G(-v3(

fi-;lv'Ifi'd

UAnppd;ol;-lv'(iGfiI5SG

Q




it>(;-G;S78(d].;dvSn

(


dvG-(

:;AQ



d0#fiv I(#(;0(

I'7O
0v'Go77v;(Q'l(fi70G;-

!"$#&%'
*
,+

d(fi;(d;I'S





i;SQG-(

d;(Gl;(;0;dIKGdIG(-v3(5S7'

G

Sx(
;Sooidi(dv;

'l(-#fi(!(;l;0;dvd(5fi-(d!.

dl

oo



ooQix';dvG5'ld;'(0

l;d-v0'('-Qv0d0(d00dGd-('(
'(G0d0;d5'G'GU'fidGdid0G;'S7n

;0fivEfi+;v0AI

UG(d'v,dU



fi

GId-I(o0%]0(!'S'xU

!

(ofi(;K8lK7


dId-l;G-l;]-vfid>-(




;StlS





t0oodI-(0d(dAGI-


0G#Q0Qo.v0K(K-IEdG30((.d3v0fiv.;(-GI(3(
dQO(v'x

oo>

(-(,5SGK-G((fiv'G8'

xiKdG

GQS';vl

BA

G7K(

;

'IGdIvd##dG






v'Giv

Kv(ll'xIId(
S#-5'G;-xIIvfiG vldG((v'xI0dv.G-S7vv(G

(

('-Qv0xdoldvdl;d50((fi

v;vSl;G

I;

Ifi#(;.(v'id(dv#((I(

GF

KKdG'(d(v'(K('-Q'v0>fi-;;fiGCvS7OI3(!G-v-v

dllG;08d!(-0'v0

0((.dv,(

(G>lG#fidoGd '7-

'IGdvd;dinI>vl.GK(dl5S7 -

GS7,I((d]d(;d,0((.dv

Sv-l

GQ(
'd',S(-(v';d;-
'#->d

OC

%0vidlo(;'x(;0#fi8d

LN

(G!n' v0-(

BC

*

v0;(dQG-ofi-;-v-fi-El;lv'Ifi'dK(fi

-'('(G(Q'(!';;!'v!fivS7S(dfi!#;7 x-3((#(;0(
(
(Kv'(
'dvd



A-



AQ.xid;K(

SC

'IGdvd;

Qo>OI-d-Svd>('7'I!0G5;'-Adv; v-Ql!-'

-'v;oldG-5-



iQ'l-I

vQ#(dQG-fi-;-

.!nd0]ddGS(5S7''(G(Q(

BPRQ
!"$#7T=',U WV YX 9Z
Y]

9C

fi-;(v'o'(-,'dod(K(d;fiv'Gv-vSfifi

dvG-0';dv'(GS(-(v;d5;-5;-IdG,dv

OIQ o'v
0dv.G-oo
#-(v'I





_K`

\[





YX

^Z

[o;AQ]o







*

:tQ



'v

d;' OvK(-(0d(lGd

fiacb&d=egf@hffi0jgkKlLfLbgmhnpoqdrkKk9fLb
s2t9u=vGs(wgsBxzy|{~}^L=6vG{=}cvG{L}^vO -gK 60y|Ly|Kgff-cE,9vW^GyKt6.G!rcgy|^yKtL
ff4B.cEffv,tM4}wt9SG)LR2.y|y}^gyWt }}y|0tL4}K=L.y|^LyE}t6}c}^gyEtL.}s2t6.L.vG}.B6ffsBg=y4
60EEt6Lwg{ tL4}wt9SG!^t6}.vzD~W*-4R0gy|yvWtL4}wtGL{=GDtOvG{g6Gy0L.tL.g^vt6}yK
0vG}-g96t6}Wc=v^\~,*.vt9.yLq=vy4urt sB=Gyvy4t6}yK}ut6sB=Gy(G6~@yK|t w.y(vG}
vSSw}t6}yKWt6{gL}^gy|,|tL.yWvG{D=v^)L E,|t6{g{gL},.w=4yE}2=y|}y|sv{gy(=y|Ly|yK0 @y4vGy4
{}gy|tL.y6}^gy)~{g60GyK=LytLyE6}gys2t9u=vs(wgsBxzy|{=}M~6vG{=})-g96EvMw=v}^y
s(vGyKtLMvG{g!t Lwg}W}gy({t6}wgy*6R{gyKt6=0L.g|W)y*sw}t9L6v}=vE.L}c |Mv4L{=}.vG{~w=vG}.&
cgy|{{Mv{g\}^gyD=y|Ly|yD @y4vGy4B6tzLs(w=t}t6}(vG{=L6GLyK.}xzM=y|2Mwt6{~}vy|4|Y0gy
{gL}.vGL{O6.}t =vSvG}.2=y4{gyKOy4G6vffvG{=}y|{=yK}=yKt90vG}O}=vRg^L=Gy|sDR=y4{gyWvG}KL0y04.}
{gy|yK}^gyW?6SG60vG{g2{gM}.vGL{D6fftOG9M|||4;7$z6
OMGK$R 49L7$z6.6BEOvBt4L{6-wg{4}vM{6zLs(w=tL|?L(yKtL^
t6}Ms06Ly|2=vG}v{^Gw=yK*y4utL4}.GL{gy(60~,qK-qt6{ff~,q6-crLW) )((@}gyB49
L7$z6 |K?Lz4Wffz$E 6.vG}}y|{B. 9v}t }v |yc=yK4.vGg}.vGL{c=v(vG{^Gw=yK0ff~W -
vSR Bt6{~, -.vSR \g
qgyg^L=Gy|s2E}t },0y(ct6{~}W}Dt9L6vg|4wgEcgy|{)}gy|y*vtOsOt9u~vGs(wgsBxzy|{=}L=D@6vG{~}
0vG})v |y2=yK4vg}vM{Kg c.w^}t6}v{tB{gy4vGL= L^g~g6( sB~.}W6R}gy(0L.gEt }.v?MvG{g
EYt6yctL.g^vt6}yK*0vG}(L}gy|R-v |yW=yK4.vGg}.vGL{| {~}^w=v}vMy4Lr}gy0gL=Gy|s 0vG}(}=vv}^t6}ff}gy
4=LMvG{t6}yK60 t9GL{gy06vGLy0w s(vGyKtLMvG{gEvG{=?Ms2t6}.vGL{Ot6Lwg}}gyc{t }wgyR 70L.g{gyKt6E Lt6{
.t6Mwg}=y|L^y|yKc6R@y4Svy4 )y vMyt.w=^vGy|{~}4M{Mv}vM{)c=v)|t6{y(w.yK}t9L v}=v
gL=Gy|s vG{D}gy4L{=}y4ug}c6ffLwgc}gy|My|s2|ff0=vW4L{MvG}.vGL{vy yK4}vMyt6{Ow.yKs2tL^=vG{gy|zvG{
t6}v4w=t69}gyt6=vSSvG}.D}({. wg}vM{tL4yK40}t6}0vW{gy|yK=yK}Ow.y}gy*s2t9u=vGswgsxzy|{~}^L~
t6gg=tLOvG{t6{~|tL.yL
OMGK$ y|} yts2t9u=vGs(wgsBxzy|{~}^L=)6vG{=}(6E E, yt9}t6} v| |
0vG}^yK.yK4}W}E t6{ ffvSB v{gL},4L{=}t9vG{gyKOvG{ E Kg z)yt92}^t6}EE6
) 6E|z q qvSzL0y|Ly|Bs2tu~vGs(wgsBxzy|{=}L=(6vG{=}B2 EWytKMy,}t }9r R
t6{O}t }BB vc^t9?yEqv}^DyK.@yK4},}^2E t6{
qgyE{gy4u&}c^yK.w=G}cv}gyELy|OgLy|^}-O6R.}t6=vSSvG}.}t6}00yE{gy|yK
= 9 $= EE/6) 6(|z ($~Dff . Eff~
wgE}^gy|Ly|s20vSSRw.y(}gyBtL.wgsBg}vM{!}^t6},}^gy|y(y4u~v.}ELsBy .w^}t6}Kr?Mt9S.w=zx
7^vGy|{~}.s2t9S E t6{! t6^y.}t =y?M, 0)y*{gL}yE}t6}W}=vW=~yK{gL}vs=}t6L}, v
{gyK4yKt .vS}gyv |y=yK^4.vGg}.vGL{)tL.g^vt6}yKO0vG}}gyEs2t9u=vGswgsxzy|{~}^L~O v{=}|-R6RR E,z
!$ L{v=y|}^gy={g60GyK=LyBtLy2Ev{ut6sB=GyB rL7t {D^yK|t9S}t6}O
-g96vc}gy*s2t9u=vGswgsxzy|{~}^L~O v{=}W6 E zc0gy( v |y=yK4vg}vM{Kg vff~W -
~W,9-60y|Ly|}^gycs2t9u=vGswgsxzy|{~}^L~*6vG{=}R6 Er?L vtL4}wt9SG K9K K
.B}^t6}0}gyt6gg^Lg.vt6}y qzL,.w^Dt vE~WW9- D~WW|-
















ff


fi

























! #"



%$

&









'













(*)+,".-0/

#1

32

54













6

7

,

89#:<;=?>A@CBD=E E5F*>AGffHAIJ;>@CBLKC;,@CMN=?@OLKCGffPQE5;RTS,GVUW@CMX5S#SCGffKC@>Y=?HLHZG[@#=?K\X5SC;,X5HT@CML;,>GffHQ@C;6]L@^GVU=_R`=]QX5RaBLRabc;HQ@CKCG[OQFTODG?X5HQ@
9#r%GffKC;sOLKC;>JX5SC;JE5F[tuUG[K,SCBQv`>JX5;HQ@\E5F%SCR`=E Em =VHZwx=aR`=Y]QX5RBLRabc;HQ@CKCGffOQF%ODGVX5HQ@pm G?Uzd0D
GVU!d0D
ef gihkj_l UG[K`np

q
ef gihkj_l
n

0
h
}
j

|

~

{ ;>G[HIJ;A>@CBZKC;@CMD=V@s^K ef g lCW yL
hkj 0{ ML;KC;
X5S=?H<GffON;HSC;@k@CMD=V@s>GffHQ@6=X5HZS<y PLBZ@
{ X5@CM

HLG`Gff@CML;KsR`=Y]QX5RBLRabc;HQ@CKCGffOQF%ODGVX5HQ@aGVU!d ef gihkj_l 9U#@CMQX5S0X5S_X5HLwL;;w@CML;a>Y=?SC;fftW@CML;H%@CML;aR`=V>JMQX5HL;KCFGVU#SC@6=?PQX E X5@\F
@CMN=?@

{ ;=?KC;=?PNGffBL@s@CGX5HQ@CKCGQwLBL>;TX5SsBZHLHZ;A>;SCS6=?KCF[tS\X5HZ>A;X5@kMZGVE5wZSsX5H=YE E!>V=?SC;Ss@CMN=?@
MN=ff;PD;;HpBZHN=?PQE5;T@CGTOLKCGVff;@CMX5S9

Q

{

;HZ;;AwX5@9skG {

;ff;AKt {

;

fi!z}Z!0 zD

%TT<DYNV<?LVcffW#*WLD#DW<D#&WLffYu#&ff%CW`upff
VLWffu&QuVQL`^ffffV6u[Qp?[D?%Vc`VLYZ#T%QLWuuYu#Z
ffW#ffQLTLQZsV#D*WLffW#5Tff VL?LDLVYQcZ?W#AYu#ZkCWVcaA,
Wuc\#ffWD?WQD#ffTY?Zc}ffW#JcQCWVcW#J}WVV#D
WffW#ffDD#WC#DVVLucQDVLQ_QLcVQ<ffW#ffQ%?#DDVWpTc*cV
WYDDW#Q#V<ff#WDffD!LCW[i!\CD#Q

Qp^<W<QTD0NDcDLD#ffW#ffDffQ0

uLk\ ,`



DWQ



D#cp




fiff

(ff )$* ! !!" % +,ff )- !!!.# %
0/21

WTQWQVVLucQDV

ffzWffYWD


3



[p,ZCWQQQff5,\


!! !"#$&%'

W#D#LffWD#ffWD0CWVcW`#?V#D

<DDY



W

D#3 u`QWQV







ffW[Z#

TW`ffWD#ffL

c`ffW#AYVQZ

W3Q?xLVffYYu#A3ffVQ#?`VDDkDVW_V%ffW?ff aD!LVYQcZVW#J
W#ZCWVcW0V%?`WVTWQQffZ!QV#0%_V`QWY`D

VWQ

W[QYVffWDA#ffff*ffWQVL?ffWVQffW3Q?

fi3


Zffz?#JW#ff#D#YuW

54

sLDWk

ffW[ZLD#*?cc

CWVc

64

#?V#


64 87 94
:;=<->,?&@"A


B4



WY#V*CWVc

ZQ

WVV#YLDVVQZucVff#VJW}JffzffQ%TV?D6ff[^TDV%VQVL?L



QDuQLVffW


IHKJLEGM

DC

DWQWaLVff?pu

FEG

Wxffu#VV#ffW,*QffW#JcQ

N

VL^Lff%V

H

uQVT^ff#VQx

EGOM

%06#VcffcDLVc*LD#T?#D
ffW3Q?xLVffYYu

Q SR UR

V

AD#





/



Q?QaVW#Q?L!Vpffu#[D[_&

DVCWV

k*?c%Q#DffDTV

V&ffu[NVL?cffVYffW#ffDffa.




P



VVVQ`#V[

V#DWDW#DVVLucQDVL

VPQUR WT XR

uLW?<VffW#[D^Dcff[TV}



WDu



Wi DQ

W#ZD#?#QVQV

Z9Q SR #T UR

VffW#ffDffDV W#N<V}LW?uVQQ[D#V}!





\[ /

L

NY

`cV}D

`cVffW#Jc[D

WDWWC#DVVLucQDVL3A#?#%QVQ?L<?WC#DV



VW^QVLWVQ%?#D u#ZAD?
ffWQVLVffW



D#&

,TsQYu&ffW#Y[D

`c?ffW#Jc[xV&ffuDA#ff[3D

V#QVQVLDDVu

ffuDA#ff[D

upYQpDffu#[D[`T

DWQ<YQDkffW#ffDffsQD#%LffW^L

`c?ffW#Jc[Dp?ffWDA#ffffD

VY

`QVc*V



WffWQVLVffW#NuQpDQ3AYQpDDDcDLQ

6

3 !^p%QsD!ffW3Q?

* uDZQVVuc\#ffW

Q`ffW#Y[D[TD#3 ?Q<NcDLT



`CVQNDYLD#ffu#[D[DV

u#D#QVpD#?W#W?W\ ?QW,c WDZQVuuY

#ffWD!ffW3Q?pLVffW#aDWQs
LVffW#DWQ

W3pDVV[V

L?ffW#<D?<V



ffu#[D[ff[T%TuWLVL,,V#D?#DVD#?

^] _
gf

ff VQ#VWWD

F`ba ced

u`Vc`?#W^T<Zcff VQ#

CCWpDVW

QDVW

VVQWD&ffu#[D[Q

h`ia ;

D#ff#

, upLVffW





6VWffWTDV<?VLDLWVD?%#uWc#Z W#Q#Q#QLsV#D`VVu#D

6j /

V#DffW#[D

kfmln
6fml9

VDcJ#L*DVW

6j /

V%Vu#D?#D

%D#

VDcJ#L

jPo

V#D`ffW#ffD

Ofmlqp

pVcJ#L

OjPo

%D#V<?W#DV#

A#*V<V#ff`D

f=lqp

p?DcJ#L

N

r


sfmln )j / =Js! !!tJf=lqu )j.v
5j / ! !!2#j.v
w"xzy{| }2~#| W0qq).| }2fi0b-~#qW0z0p 2}".q6~.Sq 6q=~.2P-20WyO-~#b.=P0t0|}0 )= = p " b.
-~#q^W "#"| ^W ""000|}2~|W0qq2|P2y
uLk\

uDZQ?

WffuQVLVffW









TVuDDcDL`#D?#DV

%CCWuc#ffffW#ffDff





<D#CW^D

fi6XtUb=UUme
Bfifi=^tU


b q b Uq$
n
"-6 * tPP B " U - -"=- B
=5P Ui P h U " " 6 " .-^
UqU - - tfi 6 5U #- ' " q "
fiU5qP ." U "t - fi Uh "5" U (.O"- 8 9&
#b -" = .O"- 6 9& " U t5 "
" Ps U ^ =&tN F N
"- q U ff

fi #=^ U . -
- " " = - .^ D& . " 5"-
ff!^ (. " #"8 " 5 " & 8 DS "
&')( +*, 8" t5 " " -
. "= - * & U^ % $ & " ff
P5= . 8-" P " ) U. " . - b" 5U
fi / =
" q tP " 5. " 5U " - t- . " *
" . " - U " - & - 1 0 -# " PBt . "
" " 5 . 6 . - 0 .56 U
- = ="- " 5 U " " " 5P " 2 - "
" ^. " - tU UN 5t& " U . - 1 0 4325
6 N + 6 P .- #-. b" 5 U P "5" mt
= P "# "U * " -" = UqP ".
U " = .- b b.-b" " "" 6 6
P -" 5U #- " . -9 n " t&h + 6 P N 5U U "
- = t6-,. ' + 6 P . " PB ^ U ) U fiUkP ""
U " "t 87 49;:U

<>=@?ACBD?EGFHJILKMON2PRQ TS P *U >LV*WJYX[ZT*@\ff PRQ ^]`_>a P * Pcb b P Q d*eJ!fg b Q ih P
jR@, P \Rk P Sl* b P i8X Z ,.meon mp bqb P fL*)*rS P ,. Q ms Pcb f P n QQ Bt NuPDQ[v PwQ PwbPDQ xYn bQ * QJb
*fef P *U Jgk`J
yYJ ]{z b Q mr* Q n Q *i b C P x Q P n U b Q * QJb v}| *UC\p PRQ#~/ P
Q P >g+*px e>8 t8 b b r PQ mr* QQ P P`P b QJb * b P \ Pcb nc !f Q 3 5 b gncm Q mr* QJ
*eJ p *C\ * PbQ *S P 3 5 *UC\ Q mr* QQ Pb fg*en P[e m* b *wo P ff* J>r '
P Q )ffLi Q tW2m P
fi 3 5 _ \ e
r
)

fi ^b
L e

Q P \ P C>JC* Q b fg b Q ih P
7 " W3 5 O_ - fit ff3 5 O_ & -S. q
"O. 6 6 9& " P

fi 3 5 _ " =' N . & "
-="O. 6 9& " 5U " " ^ U U "
6U " - -6" - .-" q fit ~/ U . " P
mtO -S. q = - = " "^ n 6 . U5U
- r%dc P C* Pcb " . - P " Bt # U
P

R

fi[LLCLLDCOLLYLL[LLLg
T+1ff)iO)L el RO) L)Rx Y1e+C L D+1Rx1e8/L e) 1ee1xx++
erLff le#)Rx)xix1ecL )i#LffeLRx1eYl[xeff2)LRx Y%1rx LRW)qeff
L) rDD1)LeLepDeL 81>)>pwlRU /e1x1e1e+C x1We)
e L ul)LeLR#)Dex )R@8O1RDeO1e L)e1 L )L xl
x Ue1x1eOix /eL xeLlLffrVC UrO)Rx)DeYe})
D+11LYe1LlYL)Ri lR1lL>#ii)xL}i)LqLLR)LLgRx1e2
ff2[[#@#e L U
Le oexiLff lLgY R.eff1 )x d)Ld+LiLe @D1 R@oe
e xilL iLeR #`)Le)L U@LeLL+i x1epYr1>L ))effi#+1>1)R
)>L) rL1ReeR )i)Rx)R)x xiD T+1ffLeDe )L ) )
)iO 2R R@u1OOL e 8l1RU)L>e 1LYU/x)8W)L e) {ei @
exR)L))#1i
) D11LeL>)LlR)LiLRRxR`1p)iU p>e L D+ R1e+
x1ewLY 1`ilR@e>>Rw1l4L qx1xL)Ri )effx >L
D1eL)>)LLex1e[)ew/ULY L oe)lL e) eRx1e L > `)L
L).L e) L e i)l>1ReWR.1ff%r)11>) /wULRx)Rx+
L)e)x1eL DeL oL)L)Ri )Rc .e pei x1LY)l #e4>We)
1rlw )LwL)e1 Yx d)>iL1e L)`iLeUee.)eYU)ffxo/%
x)L ff)1ff1R)x1Lx1el[xLffYUx1 e lr)1L).>1e[% LffeL
eYW W r#1 }ffL [Lei OiLeUe1)Y1L11lffL)Ri )

8eL )i#iLe1l1V1) L L ))1
Li1Lff+
)L )pRe>o)e 8l s)iOiLee2)L )ffl>1lsY4Lle )R})
YDff%))LOe#Dixq%eLffeY1 ff R eff1eL)L)x )x1e lLLg e
L1er%e `)>LL)R)11w18)L>iLeUedeR) > @18 ))8)L
eYw1 ff e e 1L>)lLL) )x1R#)L Ylxioeeei>L RwlqL1)1
Y) LL))L lffe)e1 p%L) x V1 e11WrxD+eL
/x l x18elR1 R)pLLlxiR 2 ) r # x Lle )Rc
1ffrx#e LcrD+2)LOLgRU) 2 ) LLxqlx1x1eYUeYD1w1 ff r.1)ff}l
Lq% ff c)eY R+1)x1px xe >i>e )s )8x L)LeiwL) )
eDeR@/LffYx11L>ffL.rw+ixx1LlLLq% U[ W r 1)`x1e
L)e% )x1R @ )11d.L )e
L 1x>L1LqffeL) rl)eO>)LL)R C1eL x1xL)Ri )R
ixLL))RT^)RWd) O)e D1 e T)i% 4DepxLT)
YD1>L{ r)lerp pYwd)rUoeeffLLL ) R+11 ) exR %+
)L> e1eRe1wL)wL)Ri )R O)e C e xLl)L ) %
e L D[effLL Ux1eD.)R)LiLff)eWLL)ff e) RO[%V1q>De>Le oL)
L)Ri )r>U1)LffLL) 21LY e) R#+1.l ei1e1
LRliL1effi)ReL)e `1)LeLx Uxixxi D[e)x )p)LiLeUee2 1
effLiLoeY1)R LULeeRWO x oe 1eU e e#1`iLe
1)LeLwRD+xT1) e1^ L) ox x+ YLRxL% e +q)L liYeff
e L V1 RWe 1eWYD1qL rl)eeD.V+@lL [g)l ff1qlRx)xi)R}1YL+i x1e
eV+.x1R)/WeffLL D)







fiff























!





"

#

%$

&

'




)(

+*, .-0/213

4

8-

#

913 ,

fi:

+;

7=

<

-

<

+:

?

;



2=

#=>



@;

B;

5*, 6-0/71





7

A=



7



2

C 7D

FEGEG H

7D

JEGEG ?

#

KML



7

#

N .:

8

OFP

fiQ%RHS?T#UVWYX#ZF[GUGR#\0V^]`_aSbZFZJUGR

cd&egf0h)i?jkfilJmf0h
nYo?prqYs3tsu'v%o3tGqYo3txw&y.zY{,|gt}6{Gvfiy2o#v7~3q6yAq'nYo#u43vq6y)pqay7{uFq6yAt?prq6ot{G##uFy6p{G93u'y6zYu'u'9|gtJ#
p|~#|!u'y2v7{Gs?t3w&y2o#uvAt3w?{x|zY{Gv6rw#qYts#s#v7{tG2o&{Gv
tqpG?pM't?y4vAtG|u'?y{{G~#vYrt#G~3tGuG
{G#utv4v6pr2o#u'v+y7o3ty2o3ty{x3qprw?u'v7uFw>tvpq4t3w>u'3{Hqt>AFxG?{Gv,Go3tGq6y7v6p)AFGGHA%nYo#u
q6uF{G3wprqYy7{gq6~#xGuFq6y%y7o3tyaq~37otv2uFq6~?y%prq~#?pGu>y7{{G#yAtp&{Gv%y2o#u4~?^t#G~3tGuG
nao#utGy+y7o3ty+zuo3tJGut9{G##uFy6p{G3u'y6zYu'u'|gtJ?p|@~#|u'y2v7{Gs?t3w>vAt3w?{x|8z{xv6rw#qpq
qpG?pM't?yF
b{Gv%{G#u,y7o?p#3bpy+tJ{z
q%~3qy7{~#ypp'ugtJy7o#uy2{{rqy7o3ty%o3tJGu+u'u'w?u'Gu{Gs3uFw>{Gv
{G|s#~#yp#|gtp|~#|u'y7v2{Gsu2pu'?y66q6u'u2
{rw?|gt0YFx?Gt3wy7o#u~#v7y7o#u'vv2uu'v7u'3uFq
y7o#u'v7upA?t3w,|gtJy7o?~3quFtxwy7{+u2pu'?ytJG{Gvpy2o#|gq{xv{G|s#~#y6p#@w?u'Gv7u'uFq^{M3upu0{Gvttv7Gu
2rtGq7qY{#{zYuFw?Gu+3txq6uFq'tGw#wxpy6p{G03|&tJp|~#|u'y2v7{Gs?prqY#{z%9y2{@o3tJGu4|gt?gty2y7vAtGy6pGu
s#v7{Gsu'v7y6puFq&.xtF?#uFq'%J??A%~#vv7uFq6~?yq6o#{z
q,y7o#uFq6u&s#v7{Gsu'v7y6puFqtv7u9q6o3tv2uFwy2o#ugvAt3w?{G|
zY{Gv6rw#qts#s#v2{tG2opy2o#uw?{G|gtJpz%o#u'v7u>y7o#uFq6u9y6zY{ts#s#v2{tG2o#uFqtGv7u'uG3w?u'uFw%tGqq6o#{z%p
.YtG'2o~3qu'y@tJ)FGbAy7o#uvAt3w?{G|zY{Gv6rw#q+ts#s#v7{?tG7oo3tGq+|gt>{%y7o#uFq6us#v7{xs3u'v7y6puFq{Gv+y7o#u
~?Y#{G#~#3tv73^rt#G~3tGuG
%y7o#u
{xy7o#u'vo3t3w#t+?~#|3u'vfi{s#v2{Gs3u'v7ypuFq){0|gtp|~#|u'?y7v7{Gs?Gq~37ogtxq^pyAqfiw?u'su'3w?u'3u
{Gy7o#ug2o#{pru{)rt#G~3tGu&t3w>pyAqp3t?ppy6y7{9o3t3wxug't~3q2tJfiv7uFtGq6{x?p#ts#s#v7{Gs#vpty7uG0o3tJGu
3u'u'Iq6u'Gu'v7uv6py6pr2p'uFw`.^uFtv6JGG#,
{w#q'|prw?yu'ytJFGx?A%{xyq6~#v7s#v6prqp#G,y7o#uFq6u
v6py6pr2prq6|gqts#s?y7{vAt3w?{G|zY{Gv6rw#q,tGq+zYu9!wxpq2~3q7qp{G{y7o#uFq6u9v6py6pr2prq6|gq')t3wzo#u'y7o#u'v
y7o#u'&v7uFtJq6o#{x~?w3u,Gpu'zYuFwtGqq6o#{Gv7yA{x|p#qY{y7o#u+vAt3w?{x|zY{Gv6rw#qY|u'y7o#{#w?prq3u'G{G3wy7o#u
q7{Gsu{fiy7o?prqs3ts3u'vFy7o#u,py7u'v2uFq6y7uFwv7uFtxw?u'vq6o#{G~?rw{G3q6~?y.YtG'7o?~3q4u'y+tJfiFx3GuFy6p{GG
{Gv
t,|{Gv7u+y2o#{Gv7{G~#Gowxprq7~3q2qp{G{y7o#uFq6uprq7q6~#uFq4t3w9tGw#wxpy6p{G3tJv7uu'v7u'3uFq'
u93upu'Guy7o3ty{G~#v{G3qu'v7typ{x3qv7u'?tvAwxp#y7o#u&p|pyAq{y2o#u{G##uFy6p{Gu'y6zu'u'y7o#u
vAt3w?{x|zY{Gv6rw#q,|u'y7o#{#wt3w|&tJp|~#|u'y2v7{Gs?tv7utq{q.px?pM'tyFnYo#ux~#uFq6y6p{G{%o#{z
zYprw?u|gtJ?p|~#|8u'?y7v7{Gs?ts#s?puFqpq+x~?py7u,p|s3{Gv2yAt?yFtJ?p|~#|u'y2v7{Gs?9o3tGq43u'u'?tJp?p#
s#v7{G|Bp#u'3u&tGq
t&|uFt3q%{w?uFtJp#>zYpy7o~#3u'v7yAtJp?y63{xy7o>p4t3w9{Gy7o#u'vtv7uFtGq'4%{zYu'Gu'vF
y7o#uwxp9~?ypuFq{~3qp#y7o#u|u'y2o#{Hw{G3u>zu9|{xuy7{#{x#~#3tv7s#v7uFwxpr'ty7uFq9qu'u'|#{Gy&y7{
o3tJGu3u'u'~?ts#s#v2uF2pty7uFw&v7u'y2v7{q6suFyF0y7o?prqpq#{Gy+y7o3ty+o3tvAwy7{9u#s?rtJp0^ptJ|{?q6ytJ
ts#s?pr'ty6p{G3qYzo#u'v7u+|gtJ?p|@~#|`u'y2v7{Gs?o3tGqu'u'9~3q6uFw>.t3wBz%o#u'v7u4pyAqYts#s?pr'ty6p{G't&3u+uFq6y
}6~3q6y6p3uFwpy7u'v2|gq%{fiy7o#uvAt3w?{G|zY{Gv6rw#q|u'y7o#{#wy7o#u,#{zYuFw?Gu3tGquprq4w?uFq7v6p3uFwpy7u'v7|gq
{~#3tv7&s#v7uFwxpr'ty7uFq{GvFbuFG~?ptu'?y6G~#3tv7&~#3y6p{G3qzYpy7ot,3?py7uvAt#xuAfi#{xvu3t|s?uGbp
s#o?Hqpr'qts#s?pr'ty6p{G3qzYugtv7u,p?y7u'v7uFqy7uFwpq6~32os#v7uFwxpr'ty2uFq@txq+G~3t?y7~#|q6yAty7u&.qu'u.4u'??pGo


4u'??pxo0FxG?7A5p|ptv6G
%ts#s?pr'ty6p{G3q+t3w&u#s3u'v2y
q6#q6y7u'|gqYy6?s?pr'tJ>~3q6u+{G?9~#3tv7

s#v7uFwxpr'ty7uFqq~37otGq0q6?|s#y7{G|&q0t3wwxprq6uFtxq6uFq)7fio#u'uFq6u'|&t0JGG?Auq6~3q6suFyy2o3ty0y7o?prqpq^#{xyt
tG'2prw?u'?yF#t3wy2o3tyw?u'u's&s#v7{G?u'|gq)zYptv6prq6up&|{Gv2u%Gu'#u'vAtJ0'tGq6uFq')nYo?prqfis{q6uFq)t@2o3tJu'#Gu+y7{
s#v7{Gs{G#u'?yAq){0|gtJ?p|~#|`u'y7v2{Gsqp3uG#u'xu'p{G#u+tG'u's#yAq)y7o#u|gtJ?p|~#|u'?y7v7{xss#vp32ps?uG
y7o#uYwxpq2~3q7qp{Gt3{Gufiq~#GGuFq6yAqy7o3typy^|gtJqp|s?,3up3ts#s?pr't?uptYrtv2Gufi2rtGq7q{p?y7u'v7uFq6yp#
u3t|s?uFq'

fi% hfi5m



dGffl7f0 i?mf0hd

?MJ30fiAH&fi+G&H&A3 GJff
fi7
'?'7@
37A?4A#
'A#%9
$ rM3 AGJ'


&'





A!"A#'?rG'

fi()#*#+,.-0/1,#2#3+4)#*#+65ff)#7#89-:#-<;=*#>#2?,.@BA

CEDFGF!HIJLKNMPO#QBRSO#QTRVUWQXKYZK[UP\^]K_^`ffUWabBc!Med QaWf<gihkjmlnUWQobBcpKqrs\^]TbT_tK"csUud QaWfors_vbw\tck["bxc#Qcs\v["b_
QaWfkyLJKpz!aMPUoaWK"c!bBfK

]TbBaP\vbB{s_^KM|\}dNc#K[KMWM~bBaW`uMPQUWO!bBUib_}_]TbxaP\vbB{s_^KMir!MPKL\^cSgbBaWKk\vMPU\tc![U

\yKy^c#QoURuQEqr!bBcsUP\}z!K"aM"\^c![~_tr!\^c#p#aWQQaWUP\^QcpK?#a~KMWMe\^Qc!M"sK"]K"au{s\^c!pUWO#KMWbBfiK]BbBaP\vbB{s_^KEMP`sfi
{!Qx_y
JLK

c#K?UpUWabBc!MdQa~fg%\tcsUWQ1bBcKqrs\^]Tb_^K"csUoXdQaWfors_9bgVhjmnu
uRO#K"aWKb%bBUod QaWfors_vb

\vMpQc#KkR O#K"aWKc#Qqr!bBcsUP\}z!K"aM

\tc![~_^r!\^c#V#aWQQaWUP\^QcVqr!bBcsUP\}z!K"aMiO!b]K

R\tU~Os\tcUWO#K\^akMW[Q!Kb

[Qc!MPUbBcUmQa]BbBaP\vbB{s_^KQUWO#K"amUWO!bBciU~O#K]BbBaP\vbB{s_^K MUWO#Kqr!bxcUP\}z!K"am\^UMPK_}d{s\tc!#My



QUWKUWO!bxU\^c

UWOs\vM

UWabxc!MedQaWfffbBUP\^QcpRuKEsQic#QUa~Kqrs\^aWKXUWO!bBUgo{!Ko[~_^QMPKGym_vMPQ!.Q{!MPK"aW]KU~O!bBUubBUWc#KM~Mm\^fis_}\tKMwUWO!bBU
UWO#K"aWKEbxaWKc#Qic#KMPUWK%qr!bxcUP\}z!K"aMy}
JLKsKz!c#KoUWO#KEU~abBc!Med QaWfffbxUP\^Qc

{`p\^c!sr![UP\^QcLQckUWO#KoMPUWaWr![U~r#aWKEQBdg#yuO#K"aWKbxaWKEUWO#a~K"KEKbMP`

MPUWK"!M
6 dgE\vMbBc

r#c!qr!bBcU\z!Kd QaWfors_vbM"sUWO#K"c%gXg#y



gBigBXgB


W gB^X

ffgB

gsy

__UWO!bBU aWK"fffb\^c!M\9MU~Qff[Qc!Me\vsK"aEqr!bBcU\z!Kd QaWfors_vbMuQBdUWO#KwdQa~fgB ^ gT ^ !QaogB gTv? U


UWr#aWc!MmQr#UuUWO!bxUUWO#KEMWbxfiKUWabxc!MedQaWfffbBUP\^QcRQaW?M\^ckb_}_UWO#aWK"K["bMPKM"y=JK\}_}_^r!MPUWabxUWKUWO#KUWabBc!Med QaW
fffbBU\tQci{`X_^QsQ\^c#ibBUmUWO#K["bMPK RO#K"aWKg\9MmQBdZUWO#KdQa~f^ g ^ ym`oUWO#Ku\^c!sr![UP\^]KOs`QUWO#KMe\vM"sRuK

["bBcffbMWMPr#fiKUWO!bBUg \vMbBUy#QamUWO#K#r#aWQMPKMmQBdZUWOs\vMm#aWQsQBdRuKsKz!c#KNbpWx ud QaWfors_vbUWQX{!KbBc
bBUWQfo\v[d QaWfors_vb

\yKy9!Qc#KoQBdU~O#KEd QaWf

\yKy^Qc#KuQBdGUWO#KmdQa~f yGK"U
bBcs`ff]BbBaP\vbB{s_^KX\^c
yGK"U


_^KbBaP_^`





nBW Z {!Kb_}_!{!bMe\v[MPr#{sd QaWfors_vbM=QBd!g UWO!bBUsQc#QUfiK"csUP\^Qc

{Kbo]BbBaP\vbB{s_^KoQa[Qc!MUbBcsUMP`sf{QB_c#QU\tc
UWO!bBUu\vM fiK"cU\tQc#Kp\^cg

fr!MUQ?["[r#a\^cMPQfKE{!bMe\v[oMPr#{sd QaWfors_vbpQBdgBvGMWb`

\^U\vMKbMP`kU~QMPK"KoUWO!bxU


~b#aWQQaWUP\^Qc%d QaWfors_vb#QaNbpqr!bBcU\z!Kd QaWfors_vb

nBW Gy

["bBc#c#QUfiK"csUP\^QcbBcs`

nB~ Gic#QUfK"cUP\^QcbBcs`



cQUWO#K"aRuQa#M"c#QUQcs_^`sQ

Z ym`kU~O#K\^c!sr![UP\^]KpO`s!QU~O#KMe\vM"

]TbBaP\vbB{s_^Ko\^c
bBc!MPQ!{s`[Qc!MUWaWr![UP\^QcZ\^U\vM \^c

UWO#K"`bT_9MQ[QcsUb\^cbT__Q#["[r#aWaWK"c![KMQBdUWO#KpQUWO#K"ao]BbBaP\vbB{s_^KMffbxc![Qc!MUbBcsUM"y

]TbxaP\vbB{s_^Ko\^c0
G{#r#U


QUP\v[K

UWO!bBUoUWOs\vM

bBaWr#fiK"cUdb\}_vMw\dUWO#Ki_vbBc#r!bBKff[QcsUb\^c!MXbBc`Os\^O#bBa\tUP`#a~K\9["bxUWKM"=\^c![~_^r!\^c#Kqr!b_}\^U`y%#Qa
UWO#K"cg

fo\^OsU\^c![~_^r!sKkMPr#{sdQaWfors_9bMQBduUWO#Kod QaWf

Qr#UM\9sK

nm

Ru\^UWO

QBRE_tK"U




W QaE

ZR Os\9[~O1["bBcfo\}]TbBa\9bx{s_tKM

U~O#QMPK\^c
y}
n W Z{!Kb_}_#UWO#KE"bBUWQfpMWuQB]K"aZ
RO#K"aWK


\9M K\^UWO#K"a


Qa








n W yuO!bBU\9MBRK[Qc!M\9sK"abT__sd QaWfors_vbM

QBR[Qc!Me\vsK"aUWO#Ko\vMr#c![U\tQcZ



l


n



^ g ^



uOs\vM\vMMPr#a~K_t`Kqrs\^]Tb_^K"csUU~Q^ g ^ G{!K["bBr!MPKpMPQfiKi
for!MPU{!KUWaWr#KyE QBRK"]K"a#\}duRKibM~MPr#fiK


\vMU~aWr#KZRuKi["bBcMe\^fis_}\}d`9 g ^ {`aWK"s_vb[~\^c#bT__uUWO#Ko
MPr#{sd QaWfors_vbM{`



?mQaZ"""b["[Qa\tc#XUWQE
P QU~KU~O!bBUUWOs\vM=\vMub_}_^QTRuKoQcs_t`o{K["bBr!MPKUWO#K
sQEc#QUfiK"csUP\^Qc


^ g ^ [Qc!Me\vsK"abx{s_t`y
bBcs`]TbBa\9bx{s_tK \^c6G
yuO#KuaWKMPrs_^U=\vM=UWO!bBU=RuK["bBciMe\^fis_}\d `oKb[~O|\vMPr#c![U


n P xUWO#K"aWKR\_}_{KEc#Q [Qc!MPUbBcsUM


c



b

[

U
#

!
{

K
"
[
x
b
!
r
P


K
B
Q



Q
#
r


#

W

"
K

]
^
\

Q
!
r



Q
!
{
P

"
K
W

B
]
B
b
P
U
^
\

Q

c
x
b
!
{

Q
#
r
U


UWO!bBUEbp!bxaWUP\v[rs_vbBa

Qai]BbBaP\vbB{s_^KMQr#UMe\vsKV

_^KdUpRu\^UWOs\^cUWO#K

#a~Q!Qa~UP\^Qcqr!bBcsUP\}z!K"ayuOs\vMp[Qfis_^K"UWKMUWOs\vMiMPUWK"1QBd

UWO#Kw\tc!sr![U\tQcZyEB\^c![KiU~O#KEQUWO#K"aqr!bBcU\z!K"aM["bBc%{!KoUWaWKbxUWK
aWKMPrs_^Uy



Me\^fo\}_vbBaP_^`ZUWOs\vM#aWQB]KMUWO#KbBU~c#KMWM

fi?s#u##Z=.

e#BVW^ u
W
#
Bff#
Bfi B
x!Wxi WB#!#"%$&('()
'!P*,+.-%/012 B"43G"5*768+.-:0;
9 ! #% <=?>@^"pW8*=#^# ACBD^(E #
Wx
B

^ i@F
$
^ #GH"JI&"LKM
> "B5#
' W&
' W
^&'B
P^* 6 Nm@Z #O QPSRTPVUXWY=Z #"W[P
\P U B
B
~Z!
' @Js
B #&5u
@
> "M B
:#
=^
^ ]L#
' W&
' W
^ =
" eV& ^=P U SJ'JB2#
' W( @
B
Z#
=t _t :#
' W&
' W
^ ` Z#
"WX#
'#
B#
' W( Z
@
" ~!
I@b"S
c WdP p B

^ p^ B
W
Z B# =t _t Z#
' W&!
' W
^ eB#5^
J'JB&^ (E4BfP U ^gXZP U%hji #
"AP ePk^
P #
B
[ 5NlPmRTP U W N[@#
W=B
XJB W#
&"o
n dB
%#
"Wk WW'& X#
&'x

P
BT`VPVU hpi bFqD`VPRrPUsWtYFu#PVUev b"D$@J5XB

JB&
^ uB
~'N= w%
#ECB
W &'x

P TB,B

kx(#
' W ^ %@
#
[ zyF`VPSRTP U WY
b{uP U v "
|#
#k?
x W'4
WW"
^ W#]s
#
B
5#
=^
^ e#
' W&
' W
^ 2^ 1W"o@}x
W&5
' W&
#
' W
^ tV
" ffx
CBT
ff#
' ~&!
' ~
^ fi~J~ * U ~N~ u
^ #
5N* U NA
s@^ B
1 &5t x

^ @

2`YbF Z#
' ~ =Nx
W e+jB
8Y:+.!
"m| C.
^ #
e 55*@Ug
<=J>]^ "sW=
#

^



` 0 `Ytb:u, %uw
`YJObb
^ Z#
"~ & B
TB
W& ]
> "x
Q hz YJZ&"

|S#
P 5=#
kB
~e#X?B kxJJ
> B
p
#
dP B_N
WW x
S=N
^ >@B

@^ e&S
kt (E
^s
!
' "s
"s ^{P



~N~ * U ~J~ hff
~J~ `b~J~


0
9
n 5!

' "_
W'& " & ##
' W=!
' W_t kx(#
' W {"d(#
W#
"iW&^gB
B W"P U ^
1kx(#
B
' W ^ 17
#
PRrP U W SXX!
f##
' ~ i7
1
kx(#
' W ^ ^SB
TP SXX

' ^
J
!
> &"
c k#

x #
^ ue5P Z'(#
eX'(#
W!
P )
"S 5&%^ #

JE&s
Z #"M
n e!
W"
^ W2i
* W
'(
X#
#ECB

^ !^ sx
(L=gVB
F&{
' CJ^ &^C x
JB5B
W&5%
( 5N=LB
X
kxP W"s

5&SB
W#ECx
W g":
c k?
x
^ #W B
Z^ e*eM B

^ &p
kxCP W"s
L
( 52 :
>
#
zCZ*@U#
^ Z#
"~*@Ue<=B
_s!
") W"2 5o
Z w"
^
# P B
sZx
JB
#
5>]x

@^ !"d
" &^!* U dm
7t x
, &5^ B

^ @m2`
gbS e#
' W =B
W 2+4"e[EC^ {^
#
5* U
<&J>@^ "XW=
#

^ @:B
W=u@
#
e [{@LJ `b^P CO* U N
<=?>@^ "XW {@LJ C[`gb"O
n oW'& CZ* U k
B N
kx(#
' W {"d!^ X?B&
^ ud5P es

S^
&
@
#
p2`u
b eyL2`b% }+f"Z|
&
BgS
n x
w@EC]t %W'&
55o
* @
#
[ 2`

b eyL2`b%Cp
B #
d=
#

^ {@LJ `b"
|#
:!
' "^
JBWZP ~'[N=
~
> "W{*%^ ~}=
#

J
> u#]& #"L| N
P "s
XJBd#

^ (EC
^ (EC}t sWfB
#S #"
c Wo B
5^ ffus
^ XS^ f 5&
@u
#
5 yL2`bO^
#
e#
' W>=^ &[P W'{#
^ uds
i#O
> Ws

S^ f @#
ku
@
#
[ zyLY`bk"
|#
e!
:P W'8
~#
%
B
u5s
ff#[
> `bx
k^ #
"5yFC[ `
b e `b:
&Z#h
P
&S @
#

7MP =id=#

"
uds
#
^ udJ'JB W]
> x
=#
"

f:Z:Mf5&Fg_!4F@V{QF
FAw#!{Cte {@FkCg @8@ @:lkZ7@%&@7&

kCg @d
4 (4C={7ep+,-%/0 7 +T52Z \:@X@ 5
`edb5h

Ct




R1:@X@

``Db ]!`Dbb
`e2b:Rg`
4b`Db
+ B
B=t(Ee
k xM`Qb h ^#u5PZ'xW
^
^

#es&^@i(EX #eBW&F==^(EiWX #Z'#W&'W
^M^8 ^B #" #sCPeB=JE()
i"sS u #dPxM^k #[Nx
(E&@
E
(H ~i#
P ~^ kFJ'CP CB8e":!^X?B&^

fiff ffkm| #PdX




fi!#"$%&fi'(*)+,-./1023%4!576

8:9;8:<>=?@;ABC?EDGF1HI8:<J=LKM@;<N(@;<JOMPQA<R7STPUS8VCKXW:R7=8N:YZK\[=?8:S8IR7ST8]R7<JP+<@C<^_A<R7SPQUST8VCK.W:R`=8NaKM<
=?8G9;@W:R7bAJOXR7SPc8GHdANL=eWT?@f@fNg8
RIVJ8:<@;=hR7=gKi@C<[_@;S=?8:HQj
k AUUZ@fNL8m$
l npoqm r7stfittsTmuGvhY2R7<VwOi8:=dxzy{n|m!y}x~[_@;S
zn;stfittLsh*j?8+<JAHdb8:S@7[eUR7S=LKM^
=LKM@;<N@`[=?8GVJ@;H+RfiKM<KM<f=T@+R7=@;HN2KXNd


8R;WT?QNLAW?UR`S=LKM=LKM@;<QW@;HIUJOM8:=8OMPVJ8:=8:SHdKM<8Na=?8
7
VJ8:<@;=hR`=LKM@;<[_@;Se=?8A<R7SPUS8VCKXW:R7=8N:jaE8GHdANL=GRfiOXNL@+NLUZ8WTK[_PQ=T?8]VJ8:<@;=hR`=LKM@;<Ne@`[=?8dW@;<NL=hR`<f=
NLPJHdb@7OXN:j?8:S8GR7S8(R7=HI@fNL=x* ceRfiP4N2@`[WT?@J@fNKM<B
=?8Ng8;je<=?8e@C=?8:S?R7<V YCc8ef<@7c=?8:S8

KXNR7=2OM8R;NL=@;<8HI@VJ8Oo*s;Z
l v@7[2DGFNLAWT?d=?R7=2ovnm
l Y;NL@G=T?8:S8=?8:S8eR7=Oi8RCNL=@;<8eW?@`K.W8;j2<
[qR;W=Y;=T?8:S8KXNR7=OM8R;NL=@C<8ec@;SLOXVIQ

NLAWT?=?R7=o_s;Z
l v nDGF[@CS8R;W?@7[=?8I



ceRfiP4N@7[UR7S=LKM=LKM@;<JKM<B=?8G8OM8:HI8:<J=hNa@7[=?8]VJ@;HRfiKM<EoR7<V8RCW?>NLAW?c@;SLOXV KXNK.Ng@;HI@;SU?JKXW=@
vhjeKM<RfiO\OiPc8GH]ANg=WT?@f@JNL8G=?8]VJ8:<@C=hR7=LKM@;<@7[=?8
<@C<^_A<R7SPUS8VCKXW:R7=8Njee@7c8:9;8:SYZmE
l VJ@J8N
<@;=
W@;<Ng=ShRfiKM<>=?JKXN
WT?@7KXW8+R7<V YbfP>R;NNLAHIU=gKi@C<Y<8KM=?8:SIVJ@f8NDGFIjd?8:S8[_@;S8d=?8I<JAHdb8:SG@7[
rr
NLAWT?WT?@7KXW8NKXNeNg@;HI8([_A<W=LKM@;<oxvca?JK.WT?KXNKM<VJ8:U8:<VJ8:<J=
@7[m2
l j
E8
W@C<WTOiAVJ8d=?R`=
x

mZ
l _oDGFv
xdrfistfittsx
uaI7:\7




oLxEv

x



oxvx

xdrsfitttTshxue



q=aS8:H+RfiKM<Na=@d8NL=LKMH+R7=8


x

x
n

xdrsfittfitshxue

xdrfixdfittfitqxu]



k =LKMSLO\Ki<BZN{R`UUS@fifKMH+R7=gKi@C<[@;Sa=?8([}RCW=@;SLKXRfiOXN:Yca?JK.WT?NTRPN=?R`=

@I@;b=hRfiKM<@;ASaS8NLAJOM=YZc8(ANL8

EJn

7
7J
ohoh7>vTvht

q=[@7O\OM@7c{N=?R`=e8fKXNL=GW@;<NL=R7<f=Nas]NgAW?=?R7=
IG7J

E


e`J


[_@;S{RfiO\OEj(NKM<B+=?8Ng8
b@CA<VN:YR;Nc8O\OR;N=?8([}RCW=e=?R7={x



u


x

u

x

Q



u
yXr

x*

u
x
yX r

xdrfixdfitttx
ud


(x
u


x*Yfc8GB;8:=fi
x





u
yXr


u
x
yXr Q

@cY4W@C<NKXVJ8:S{=?884UST8NNKM@;<W@CHIHI@;<=@IbZ@;=?b@CA<VN:
x






u
yX r

n

u
Xy r x


n

n
n
X_(_hLM{_7(.TL







u

x
u
Xy r x
x



x
yXra z
u


.}M
yXr
+




;` X n e


!"

ff
fi
# L_%$2_&fiT_eMq')(L:T+*!,.-0/1324 56 ,fiL_2_T{T_7fi_8,__9(%:7<; 5
=?>

fi@BADCFEGIHJLKMON?G?APQHSRUTVC MOM!G?A

WYXZ?[\0]!^3_`\<ab]c\
egYhi
f
j g
e<h%vg dfe8gh
sutwvyx.z%{c|9}~c b
sOt<
kl9nLoBprq
jBk g kml9nLoBprq
q
n
aF^am^B\aXFXO^3<XOm<XOF\!
WX._X0D\ ]c_F\\<Z<Zc?XLaX9Z <X93OmZYFZ\aF^uQ^3\^bX0FB\<Z`ab]!?X]_]!3\<X9_b]c\^3?X
<X9<XOX9_\)]\^3Z?_yZc\<aXZc3\^3Z?_b] 0X Z ]c0.\<aF^X9_bf Xab]!?X\<aXZc3Z ^_
q
FX0b_F^3\^3Z?_Q
Q3OQyfX9\ %O eh+

q
^3^3\Zc6\<aXOXb]?0XO9wZ?n <]!?
f
q


n

e8
h

.fX9\ [IX.\<aX
q

gO3D+gg
n


^3
n
q
n
nB
VaXZc3Z ^3_\aX9Z?<X9XO\0]c[F^aXO]\^ a\0Z __XO0\^3Z?_`[IX9\ 9X X9_ ]c_bm
q
q
F+!brb
0 {| }r}fg

~ f
z% c Q
q
q
n
< {| }r})09 }0 }} f z% c f
q
q
e8 h Z Z? X

!fb0.]c<\ e ] h ^^3..XO ^]c\X \<aX9_
ba
q
\<ab]c\ e8 h \^.] Z\^3..XO ^n ]c\<X<Z? \<aX`FX0b_F^3\^3Z?_b \<ab]c\ e8 h b\.n <]\^
e< F h Z e8 h% +{?} e< h VaX^_b3b^3Z?_ Q _Z Zc3Z 9
q
q
_X` ^3<XO0\^Z _Zcb]c<\ e [ h Zc3Z ^3. XO ^]\<X03<Z n b]<\ e ] h LXO9]!\<ab]\
q
]c_b \<ab]c\%\aXIZc^3_\) ^3_m ]<XL^3^3\)LZc6]X ?X9_b0XZcQIZc^3_\) ^3_` n c^3_b0X
q
q
q
^3ZXOfF^3\ Z 3Z L\ab]c\ 6
n
q
q
q
Z?L\<aXZ?IZ^3\<X^3_b3b^3Z?_Qf\<aX?X9_X9)]!6\)]c\<X9? Zc \aX<ZZ6^ \<Z.aZ \<aXZcZ ^3_









ff









e ^ h ^. ^3X9_\3]!\<aX9_Z?]! \<aX9<X ^ Z?.XX ?X9_b0XmZcbZc^3_F\)
q


!O!
+{?} e< h ba`\<ab]\OFZ?]! g yg \aX0ZFZ?
n n
n
n


^_b]\<XOLZc
] <X]!Q^3_F\<X9?X9BF3\^3F3XOZcB g ]c_b ^
c
n
n
nB
e ^^ h ^ +{ } e h ]c_b]!%^3\)0ZFZ?) ^3_b]c\XO]c<X^3_F\<X9?X9F3\^3F3XOZc g f\<aX9_

q
n
LaF^XO]3` 0XO \<Z<Zc?X\<ab]c\
q
WX[bX9c^3_ ^\a \<aX<ZFZcZc e ^^ h aF^a ^ \<)]^ a\Z? ]c0f
?bZFX\<aXIZc^3_\
e




g
gO!O< k gh ^^3_ +{ } e h YWYX`0Z _b\<<b0\] Z

ba
\<ab]c\ e8h ]? Zc3Z 9.LaX.FX9_Z?\)]\^3Z?_YZcB]c\<Z?
^ \<aX.X9\ZX03X9.X9_F\) OOn !
]c_b`Z Z?_Q \B<X9]^_b \<ZaZFZX
\<aXFX9_Z?\)]c\^3Z?_mZ ]c\Z?
^ \<aXX9\
!O!<
\<aX`FX9_Z?\0]c\^3Z?_b Zc\<aX0Z?_b\)]c_F\) e ^3_b0X\<aXFX9_Z?\0]c\^3Z?_Zc\<aXm<XO ^9]c\<XO Zc]c^3\ <XO]c\<X9
\<ab]c_ ^ ^3<<X03X9c]c_\ h .W ^3\<aZ?\ ZF<ZcL?X9_X9)]!^3\ X9]c_ ]?<.X ^ ^3_9]c_Z _F^9]%Z?<`
e _Z?\O X`0Z _b^FX9 h LaFb9 ^] ^ 8_b0\^3Z?_ Zc 0Z _ 8_b0\^3Z?_buB<]!
c^3_b0X
f{?} e< F h Xb\ab]!?X f{?} e h Z Z? X fmWYXbX
\<ZFX0b_Xm\<aX
e !h Z Z?.X]c\<Z?
<Z?IX9<\^3XOZc\<aX0Z _b\)]c_F\)9
0Z?_F\)]!^3_b
<b\aX9_ X] ?X
<]\^

fi





! "

!

ff

$# #
!

! 4

%#
(&

,+







!




#
*)

#

.4


31
!
3 1
8&:9 <;
@A

#

'&
#
*)

65
=&29

0/213 1 4
73 1
?> ;



fiB CEDEFHGJILK,GEMENOFHPQCEDEFSR7CETEUVI WEIYXZDE[EM\GJ] ^
_:`acbedgfhcfij fklffmonQhpqffrsf<rsdtqvuJaxwEkyr{ze|}6ijt~8~<%ivj7dgq EqffiEhqE`Eaz|}dtqfffjOrsq~QqEd
j fdgcrdgq EqfhqfrdgqffrqEfiEhdtq~<fj?qf6 kfiEhq2hc7j thc~j frV~z.m_2`z.dt6~dthj lffrsfj?m
j fdg2rsfiJ`0Ea$fe~<iEdtffqEd lh6%shj 2fij fQSgJ ~%j f<r~ph~2|}kEj q~<d~j frV~ph~e8a*bdtfh
fij fQrsq,fiffr~dtq~<ffrdgqrsf8r~rsdgfj qfff6fij?f8h~<fj f%h:rf%i rsq0t$Z8E $ kj fiEh
fij q~<frsq,fiEh7%sd~<Eh~<jthe {8 dgfiEh2r~<htkZfiEhd rsqfffcdgffVqEdtf6qEhh~~%j <r{sm~j?f<r~z.m
Z8ff
hqEd 4dtq~rffh8dtqgrsf<rsdtqr a*2iffr~r~Q~<EE<r~rsqE smgr{ffsfQfdcEd th fiEhE%dd zrqfftd sth~
fh%iEqffrtEh~zdtjghlEjOrQthdthf%mta*eEZ<dtl2dtfflyh%hVj?f<rsthsm7hjt~m6r{z t$Z8E $ 2hh
j qdtyhq~hfa6qffz.dt%fEqj fhsmtkyrsfQr~QqEdtfaeqfiEhcdtfiEhQij qkyrsf82dtfflhijOthh~~hqf<rjO{sm{rgh
j qdthq~<hfr{z hcdtffhffjthcfiEh6dEEhqh~Qd zrsq8$ lffm8a2$fQfEq~dtEfQfij fk
zdtedtEeEE%d~<h~:iEhhtkf%iffrV~:hffjthhqfffer~d~~rlffshta
hfZ*%8E lhf%iEho~j hojt~8$E hhEf7f%ij f7hghm0EqEqEhj fh dtq <Eqf7d z
fiEhz.dg$c `. r~6%hffVjghSlm, `. abdtf<rhfij?f6fiffr~r~ch~~<hqfff<rjO{smvfiEhdtEd~rf%h
fj?q~z.dg7j f<rsdtqcfd8f%iEhedtqEhe~<heiEhqffhpqffrsqEch~~<hqfff<rjOd~rsf<rsgrsfmrqnQhpqffrsf<rsdtqaa 8 rsqjO{mtk
shfc {8 lh Htx 8$ $ af8fEq~QdtEfQfij fOkEzdt8jO{~<ff%rshqfm~<jO{6 kZ {8 *
{8 a2iffr~8h~<ffsfkeiffrihj lyh:jt~ h7jQaxEk:r{2lh7~<fj fhj qEd thj fhOa dt
qEd 42h8~<h8fiEhQsh7jfddtqfff<rsqEhfiEh6E%dd zZd zf%iEh87jOrsqh~fffOa
dtq~rffh6~<dthS
, {8 a8$f6~<ffh~Qfd~iEdfij fQzdt8jO{LfiEhhchr~<f~6~<%i
fij fz.dgjO{"6 ktf%iEhhhffr~<f~2j8yd rsqfQy t<xZZ8E $ ~<%ifij?fjO{fiEh8ddtgrqj?fh~
z j hrsqfhgh*ffsf<rsffsh~d z =j qc~<%if%ij f: c ff0ffa* dtZfiEhqchej qcfj?th~<jO{h
j q~<7jO{sheffx~Zfd%hj fhj~<htEhqh dtqgh rsqE6fdc 8 hqhtk\hfe0Ea*m h7jcQaxEk
2hj qpqc~<dthe Htx 8$ $ ~ifij fe ff0gEaZ m6ffhpqffrsf<rsdtqkffhth%mdtq <Eqf
rsqo %8E r~d zf%iEhQz.dt%( Z8 * Ek Z8 0EkEg ` Z8 kEdteg6 ` 8 kJeiEhh
Or~8jyd~rsf<rsthyd smqEdtrVjactqEdthz.dtQfiEhcdthqf8fiEhdtq~fjOrsqfff~Qd z fiEhczdt'O8 e Ek
j qdtq~rVffhfiEh8h7jOrsqffrsqEdtq~<fjrqfff~fij f ~j frV~ph~a*2iEh~<h6dtq~<f%jOrsqf~jrqfftd sth6~f<rf
rsqEhtjrsf<rsh~kZj qf%iEh8zEqf<rsdtq~rsqtd?ghj?q j?hdtqfff<rsqffEdt~ae2iff~kEfiEh%h6hffrV~f~~dth
O~<ifij f2zdtj zdteeiffr%i y0OkEfiEh~<hcdtq~<fjrqfff~Qj h6jO~<d~j frV~phlffm
bd,dtq~rffh jQdtq <Eqf*d z\f%iEh zdt Z fij fr~*~j?f<r~phlmc rsqh r~Zd~rsf<rsthtkgfiffr~
ij Eyhq~rzej qdgqffmr{z2fiEh8zd {sd:rqEdgqgrfrdgqoiEd?VE~z.dtQhthmdffdtgrsqj f%h ` fij?fjtfjO{sm
j Eyhj ~Qrsqv k2hijOthc ` Eacqj f<rffj kr{z, j q ijghf%iEh~%j h7dffdtgrsqj fh~2rsfi
jEhEkfiEhqvOsZ Q Jac$f8zd {sd ~8fij?fQz.dt6jS kr{z ZYOj?q j q ijOthcfiEh
~j h6ddtgrqj?fh~22rsfijOsEhJkEfiEhq jO~<d~%j f<r~ph~eZ*%8E
hoqEd dtq~ff fij?f~j f<r~ph~fiEhhgffrshhqfff~a hflhSfiEhrsqffhLd zfij f
dtydtqEhqfff zZ 2rsfifiEhj th~f*jOsEhta heffhpqEhy0lffmdtq~rffh<rsqEhjgicd zyrf~ dtydtqEhqfff~
` kgz.dg68,20S

`
j q ` 0
` `
` } ` } }

$fQr~hjt~<mfdthrzmf%ij ff%iEhdgdtqEhqfff~d ze ~<E'fdta 2fiEhdtydtqEhqf~rqS kdgfiEh
fij qfiEh8 fikj hQrsqhjg~<hlmj fd~<f6?va22iEhcdtdgqEhqfQ ` r~ffhhjg~<hlmj fd~<f
Sa hQ2r{{~<iEd 0f%ij fe ijt~fiEhQ<rstif2Edghf<rsh~zdtejO{0 kiEhh r~2~<%ifij f
4rq$E` E tSa62iEhz$jtf8fij f84J` tj?j qfffhh~Qfij f8 rV~rsq zdt
jO{0a2iEhQzjgfefij fet"t 6gj j qfffhh~fij fQ r~e:rf%iffrqgd z kj?qiEhqh
2rsfiffrsq,d zeZ rqhg YOkrsf8zd {sd ~Qfij f7 ZYa rsqh r~6dtq~<f%fh






fi

ff


















fi!#"%$&'(()*,+.-/0&& (

1325476 8(9:9<;76(=<1?>A@1B49DC36FE:9HG5=<2I2(J<KMLON76P139CQ6(CST!
R UV,W
J39oMXLAJ39Kqprs2(N7KMLA1?LA2(N
{ 1vN2 W

JB9uED6 LAN7CQ132}|J32F8(9D~q9uE:EH6pc V,W

4LANI1B9K96PJ?>zLO9uJ V

134LCJ39oMXLAJ39C1322F>C

|J39C?9uN1t6F1wLO2MNLAN
L x

9=<2MN7=B>OX7K951B476F1QT!
R Y[Z]\_^(`?acbedgf3hiQa j
R ff
k lmknV

f L ltV 6FN7KH49uN7=<9v1349S9uNI1wLOJB91349u2(J39uE

LA1%LCK9<7N76F>A9LAN1B49>6PN(X76F(92 x
x 2(J3EQX>6D

R
R l
f Lmp9(p VFf Tg



6MCX7C?9KLAN1349H|J322 x# X7Cw1PLO8M9uN*pC
W

6(C

Cup
9
W

x J32(E6 >A(9uJt6 L=QM9u2(E:9u13J3@(p9v76MC?9Q2(XJK9<7NLA1?LA2(N7C2(N1349

f s2=34N76P V r2C?139 V]

67JtC?13ff2(J<K9uJ

4L=B4

V N2 Wyx 2F>z>A2 W

2F@ V((l pqC?X7C?9u1#2 x LC#C36LK132!9uuQ ` <3Fff

J396 >Am=B>A2IC?9K79<>KCup/476F1sLC V LCC?9uEQLAm6 >A(9uJt6L=L x

\ ` ^F}

uQ:

#!HA,

v
R Fl

f C?9u9

x J39u9S86FJ?L6F>A9CS6FJ39

1349uJB9LC

f?7 ?t
lW
!F ?t
W 42IC?9v2MN>O@}N2(N


3
>A2(FL=u6 >gC?@EQ72F>C%6FJ39G V7MVvV,Vq
6FN7K5 V C?X7=B4H13476F1 fmT B3T
l Lz
fnT 33T
lZ Sp



x XN7=<1wLO2MNF V(W 49uJB9
6FN7K
V LCCB6 LK:132!9Cw9uEQLAm6 >A(9uJt6 L=L x LA1tC#MJt6F|4
R l
f Tq

42IC?9

6PN7K

LCQC?9uELOm6>OM9uJt6 L=Fp/49DED6LON1322F>

9:X7C?9LCv1B49
W

x 2P>>A2 W

LAN#tF

f s2=34N76PD9u16>p V*(V |*p7F l3l

\ ^ u:I(sHuuQ? ` F<3FffQ 5

^F ^ <uuQ? ` F<3Fff< ^F

G uk







_

R

Z

<I(

f G l



:*
R

I3:u u

_



fn3lZ

^

``%Zf G uk


XJ7JtC?1SX7C?9D2 x

1349}rXJ38(9(9<>O9=<1wLO2MN~q9uE:ED6LCLAN1349

x 2F>z>O2 W

LAN VW

4L=34C36 @C1B476F1 V LAN6

=<9uJ31t6LONQC?9uN7C?9 V C?9uELOm6>OM9uJt6 L=

x XN7=<1?LA2(N7C,79u476 8(9<NL=<9<>O@N96FJ>zLOELO1<Cup%49s1@|79s2 x |49uN2(E:9uN2MN

9

L
?
C
}
4
3
1
5
2

6

8
F
2

L

K

L

C
z
L
z
>

>
7
X
?
C
3
1

J
P
6
3
1

9
K}I@ CnLAN
W
W
W 4L=B45LC=<2(N1?LANX2(X7C6P1G V X1476(C/LAN7NLA139<>A@EH6FNI@

>A2=u6>gED6 ;LAED6:6FN7KEQLANLOEH6:N96FJGp

H% \ ( ^ }I( G uk 5 ^F ^ tuuQ ` F<3Ffft ^P
<I( fmT*l
G
G q f G l
G!*
Iw5 ^ :ff

GtuIM
utff ` fi w<F<
fftF `*a G
k
v q<
(X||!2IC?9 V @ W 6@Q2 x =<2(NI1BJt6(KML=<1?LA2(N V 13476F1CB6F1?LCn79Cs13494I@|72(1B49C?9Cs2 x 1349|JB2(|72ICLO1wLO2MN
X11349uJB9LC/N2

C?X7=B413476P1
LCLON7=<JB96(CnLANHLAN51349LANI139uJB86 > G
k pg9vK9<7N96S|72FLAN1 LAN G k
132!9B L x*x 2(JC?2ME:9 U Za G 3T*lqW 9476 8(9 fmT U l
fmT*l p,~q9u1]!91349C?9u12 x 6 >z>_134976(KQ|!2FLANI1<Cup
FLAN7=<9:
LCC?9uELOm6>OM9uJt6 L=QC?2SLC V CnLAN7=<9 U Z Lz
v q,_


FLAN7=<9 V



s9=u6FX7C?9S2 x



G uk

1B49=<2(N1?LANIXLA1?@52 x

@6(C3CwXE:|1?LA2(N
2F8(9uJ1349QJt6FN(9

V







fmT*l

fmT U l3l3lt

V }LCN2(1LAN7=<J396(CLON}LAN6FN@5LAN139uJ38F6 >

6FJ3LA13J<6FJ?Lz>A@=B>A2IC?9Q1325GD6PN7KCw2G

1349K9<7NLA1?LA2(N2 x

U Tl



@6(C3CwXE:|1?LA2(N

C?9uEQLAm6 >A(9uJ<6 L==<XJB8(9

5f

U fBf G






9:=u6FN7N7K76(K

kmV!W

|72FLAN1tC

Sps@51349HrXJ38(9(9<>O9=<1wLO2MN~q9uE:ED6 V 1349uJ39LC65=<2(N1?LANX2(X7C
Z


C?X7=341B476F1:



V 1349vJ<6FN(92 x

ffZ

V!f G Fk ]vp%FLAN7=<9QG





V LA1

f G l

V Lmp9(p V
x 2F>z>A2 W



G6FN7K






9=u6 >z> V 1349J39C?X>A1
W

Zf G uk p

G 1349uJ39 x 2(J39

fl GpFLAN7=<9!}LCS65=<2(N1?LANIX2(X7C x XN7=<1wLO2MN V
G $F
k pr2MN7CnLK9uJv1349QEQLANLAEQXE|72PLON1LAN1B49vLAN139uJ38F6 >

|J32F8(9~q9uE:ED6Hpcp

x 2(JD6>>

G Fk!x 2MJC?2(E:9

f<Fl

%}2(J39|JB9=BLCw9<>O@ V >A9u1 79Q1349LAN7EQXE2 x 1349Cw9u1 U
fmT*l ("qp%FLAN7=<9" G W 92M1t6 LAN13476P1 T) G:6PN7K51349uJ39 x 2(J39 T}Z
E:96FN7C13476F11349uJ39LC6:|!2FLANI1 U Tx 2(J W 4L=34* fmT U l+
fmT*l<VW
"56FN7K p



fm3lDZ

:Za G k p#@
GQ6FN7K5C?2 V
LA16(=B4LA9u8(9Cv6HEH6 ;ILAEQXE#"
G

f<a G kffltV LC
C1B476F1

6(=B4LA9u8(9Kqp

9v=u6FN5N2 W

aG

$

49uJ39Q134LCED6 ;LAEQXE
W

Za G Fk

&


Sp%4X7C VT

fnT U l

'" 7 =B>A96FJw>O@

LC



LC76(KqpsX113476F1

4L=34=<2(N13Jt6MKML=<1<C1349Q=342PL=<9Q2 x

9N9u9KHLC6(C

x 2F>z>A2 W

Cup

,-/.021436587:9<;=1?>@+ACBD>CEF. GH,JILKLMON<GP>QRACBSTQDB>@LAC1 BR7VU?AWQRACXSZYR>CET[LACU=\>JST9VSZ]^STBSTQD_`Aa?1P>$U:ETAU:bc>d:eP>$1:B6SZaPAC\6YF]F\RACAW]F7O\RXe:Ef>
ST1gBR;=AWEf>$1=[LeP>$[OA7$]h\RA>JEh9<ET7LQRAC_4aPA<ET_=QC.Dij7$k8AClLAC\CGP>Qm7OU`QRAn\RlLAC_4ST1365D7:9;=1?>@4ACBj>JEF.TG^,JILKOMLN<G?Q6ST1=9CABR;`AVBR;=AC7O\Rbo7]
\RA>CE9<ET7OQRAC_4aPA<ET_`Q>_`XSTBRQWA<EZSTXST1P>$B6ST7O17$]d:eP>$1:B6SZaPAC\RQ36p>\RQR@LSFG^,IOq/,JN<G?BR;=AB2kD7_=A<aP1/STB6ST7O1`Qr>$\RAVAnd:e/STlO>JETAn1:BC.
s:t

V

fiuvwx8y{z}|~y`x84vwxvzzWwhy{?

Vff684?H6DLO(//8R O!H6D Vj g4D + gg
`DLVF=?CF W g * g < ? ) :LC*$F< <2^: g

~:? ^2:`g =^ `F: g! <2^:=26CL g
C O?LF :
?nLJ '$?!2 26CLO&F g
$F=?CFC^L=& C ?
TW R< W H RRW $C! +` j $OH2LC ? FO?CF
CPfg $:L^2CO+ !
H R +^2$ff:C O?=R J: H R
F 24 +F*FOg$ C g* 4 ff` `?g 2g:C?HRLR H R
:42o H C*?

<2^:$ C ? C FL*F H ?C ?O?$= R
R<

ff
fi fi fi fi
H W R 2+C ?L?=&R H
VF=?CF 2C:`H L`2 (HC C :)? FLF ? FOC(J`
F:OL =L Lff FL 2g ^=C C: !#"%$'&($ +:*) 2g!^ :
F:OL P= F :?*C
=+ ? -, LF:OJOH $: $F^?4^2
C: FOoH4?H:L^2?C ^4+ C/. OJ :o2 { ^ $:42 F
fi 01 fi F 2 4 LF=LC :! :2Vg ^CF VC:`H O`2cLJ
3 fi4 '5 ! "%$'&($ C$? 3 ^ V fi 01 fi 3 76 6 8 fi94
FL 3 oC: `H O`2 F2+C: `H L`2o= FO ^ OF?= : :! 4<;
:424 !>= C$? 3 2 C$C2LCFffL$=H<F?!
F*=H WL:. OC() ^ OF?$= ? :
3
! P fi = 1 C!? = + fi fi LA+
@ <2^:! C$O`FLo W R
? !& CB 0 ED B 0 =C COP^O?`c?J8. = ?)$: `
F J ?
=`FCF= ( CF^^Fn ^ 2`2(gB FB :$ L^2CO !
G( C$? +` J*?F G P ?C27. =o=C CO`F^O
@o ($ ^CF^FC 3 $:L^JO FH= 8 fi = C^$:^CF}$ C =
C$ $?g 3 = fi fi :

fiIG OH FL 3 = fi fi ?
(
F W? K J H Rm + $F^ $ !C F?g
V^2m



F < F r W C ? H j CL`FOV =?CF?
R
:? + c?*
B 0 L B 0 { B 0 ED B 0 L *B 0 ED B 0
+:B 2 <FC F^ 2` H L 3 = *^M ? H TW ^ R
C$O`FL r $NB 0 gO ?PB 0 ?2^:J:PF R F RO
?!:L +?C27. = ! : FL Q G H 2Ln !Fff ?L-H
F F=! CL`FO +& CB 0 ED B 0 ?`2 ?J8. = PR F`
<2^:! CO`F^ 4 SB 0 ED B 0 Lr = F?
C$O`Fc W R
2AB 0 B 0 OU FL* C27. =g^2 : CO`F^ *^M ?AB B
=:`rgr ? =)? 3 2 L$=H<F? ? : fi = F. OC>) ^ OF?$=
?$C2L^ ED V ^E. ^FC :? ^2:`W B $? $F^
B B B L W :L CL`FO $^oC ?$`2 $?C27. = * r ^
$F^= ^ +? + H g $^:C F? * $OH2LC ~? P F?
g=J^

?/.`F=H^ff ! ? :YX 4 X

Z\[^]`_Aa2bcEd9egfh9ai2cEdkjl/mn9cEeaobcEp q-cEd9adkmerfd>mn9asfdkt-dkfmavuEa8bmcEiVx w mnzy{mKy{bm|zy}j<jlAy\qqza\y{ifd*~E ?gxEw ]
9

fi(?'2v?9?s%9
* ` rr -Fg2EI^^ UE/ g/kg-2?HgI?z9/{Ug*?-g
2g-\Ekko^ g\E`g(? >V -U9: /{*-`UE^k9vI -g`gU*kQ-
g`*:ff-/Q-vU>PE?K}zo1-z
(> 22 > >2 %> 1 o-ok-k- # >A>*


E H

ff-?! 9" ff- #$ F%
ff #P &ffv'

fi ffz k ff k


(ffk k) *-+ , k *./0{$ 1ff $ 2) 34059 6 8796ff86: A;
05) (ff` ffz <ff= -
<7 > #??
ff8( ff7@ 2 > ?A- V 10{: E! 2BC/ ff05$ - } 0E7 > #? 6ff8*ff #%
k #?
-F<ffH
GJILK
MN &6&k) 2D-+
* ) I#" PQ Q
)- 2 87v D,5R 6k 05
-
NOSUTVNXW TYN 06Zff @ ) 340{$ kO }! 2> ff DG ff [ 2H -) \ ^
] >V 7*
P
ff* k) ) 2Eff8( : *NOS'ff- # /< ffv$ 1ff > -) A\ '
] >/ 7v$ \
\`
_
ff-
#Dk) ) 2Eff8:! 1ff N W
Qf- 05-* ff05& 0{$ : 1#B+Dk) ) 2
b ff ff Bc05`7> #) } ff05 >Ve
? 05$ U
(ffg9" ^
ff fi ffhR*?i
[jffz U
ffk05l:! EN `f- 05- ff05-#))9
*
0{ #)mff-) 2 -) fi7v$ `k) 2nN 9 -ff -ffz# B7U`: ffh[ENOSYToNp
?2Z ki
qBr]* >> (? >V Es B @ ff-) 2tGnBe kO 2
- ff-) 2 C -O
* ff8# N)Sr
] >/
b ff B$ NW u[jffz 1'
>) [8ff NOS6 N @ 5Rvff-! N)S,wmN x ff- #
$yD uff-) 2FC -O % " +?2fiu05O $ z2D-c kO 2? 05$ e B <5R I(: u{"|U}
06Zff8/s ff(\ 7v$ <\ ~ T{)B~7 ff1[k \ u N W ?10ff-`: < kZ
7fi0ff-e B 2
05`7> #) -ff }fi
ff$ {>o 1059\6 ff 2Bff\? <6ff8 \ v TU{1 $ 91\ ?2EA 105 #
ff -e ka

Bh :+ ff/ 1 k 05` -) I2\ ]* >> 0,ffs (> \
.H
ff > 0E> ff B@ G > ff <k
7ffh[F \ ) TU{)B` 6ff8* \ |`NW?B? 8[ ff

~05 $ "A )- B)7>`: &k
ff$FF05 0{ #),ff8%@ ff= GnB



(
+
o-Q- >1+ o-k- \ >A+ z G x8 G {z = z G xj G \z
6< ff- #B


o-k \ >A
o-k >1

8+
? C$
6
/ ] >/e P
_ * G G

GwUF G G z

Es <6/ ffk05$E-2X#)EH- > 7H06tff <I7>#)* ff8v
GwUF G z G G z GwUF * $ 5 G6 G



G z

f-
05 Gw9F 8 G G < ffC -$ 2 *> ff2m
GnB# ff05$

k #? E}Eff Gi 87V:> ff
9}!#s-!87V
1

fi+pv`hpnfi+% Xv8

'+*C+sa-kcEeQ=C)j~
uh p11e- $em5()*O<Y(@k>qc8(sj*8u1e%6*OHhe81,O,
5*8p**+ <
z8,Oq,*
<

fiffe)8< D8,>v( ssq
5k>?s8
5 j 1s85
cO
( ssq-5k> ,8t:Xp'8
!j('81,O,
5<8>+" " <
?>#
cs$
,<"8h>,) p5CjX8p>mfi

uh&5% ')(+*, -/.0123415!68759;:=<>3?@(A7:B(C59EDF3G=GBH/IKJ$:=L&3!:K:=L&3!:M:=L/JN<OJP(AGPGB5IMJGBJRQ@H/JN9&7J
5!6:O5!SCJN<>3!9&7JUTJR7:=5<>GV; W DYX[Z]\^_/^R`4`N`aDb:=L&3!:M759;TJN<=cJRGd:=5 / De3!9&?V6f5@<gJR3@7=LhXi3jIk3l*
(CIfiH/IMm
JN9;:=<=5n;1on&5p(q9;:Ur W 5pu
6 "N <GBH&7OL:OL&3!:d6f5<M34S+SXoDr W (AGM3!:#SCJR3Gt:k M3423R1u6f<O5I8.hv!(C9&7J
:=L/JGBn&37Jowfixy(AGU75IMn&37:4D2zJ{7N3!9|3G=GBH/IKJ$2z(C:=L/5H/:USC5
GOGM5!68c@JN9/JN<>34S+(C:}1~:=L&3!:U:=L;(AGPGBJRQ@H/JN9&7J
759;TJN<=cJRG:=5oGB5IMJUn&5p(q9;:Mr .zJR7N34S+S:=L&3p:gb <fiF(AGfi3P&9;(C:=J$75@Ig;(C9&3!:t(q5@9|H&G(C9/cN3!9&?/3!9&?
5<;85!6g759&Gt:=<>34(C9;:>GND2L/JN<=J$JNTJN<O1GBH&7OL759&GB:=<>3l(q9;:K(GU5p68:=L/Jj6f5<OI4Ab $Z/D4 {/D
@ |NN b >D&5@<@b ~NR b >D&GBH&7OL:=L&3!: (AG#3fin5
G(C:B(CTJMn5!SC1
9/5@Ifi(A34S.8v!(C9&7JM:OL/J85!TJN<>34S+S
9;H/IgJN<5!6e759&GB:=<34(C9
:>GY(AG&9;(q:OJ82zJ87N3!9$3G=GtH/IMJD/3!c
34(C9P2z(C:=L/5H/:Sq5;G=G5p6cJN9/JN<>34S+(C:B1D&:OL&3!:34S+S:=L/J
Wfi GGO3!:B(AG61Pn/<=JR7O(AGBJSC1{:OL/JGO3!IMJg75@9&GB:=<>34(C9;:>GN.zoJM7OSA34(CIy:=L&3!:F:=L/Jfi75<=<=JRGBn59&?@(C9/c$759!BH/9&7:>G(C9
r/
b < ! 3!<=J8G=3p:B(AG&JR?U
1Ur .e'5<3g75@9!}H/9&7:5p6:=L/J6f5<OI b Z|fi9/5:=JF:=L&3!:RDp(+6Y Br W Z
65<34S+S/XoD!:=L/JN98:=L;(AG34SAGB5FL/5!SA?/G3!:E:OL/JS+(CIfi(C:RD
GB5F:=L&3!:bBr Z/.)759!BH/9&7:5!6:OL/Je65<=I b e
:=<>3p9&GSA3!:=JRG(q9;:=5$ K(C9o < 4 GBH&7OLo759!BH/9&7:>G#3!<=Jfi:=<B(CT@(A34S+Sq1oG=3p:B(AG&JR?j
13!9
1Pn5!(C9
:
(C9w x .P63759!BH/9&7:fi5!6:=L/JK6f5<OI@ MN4 F(AGfiG=3!:B(AG&JR?V6f5@<g34S+Sr W 3!9&? W Db:=L/JN93!:
:=L/JKS+(qI(q:fi2zJUL&3RT@Jk@}r gD2L;(7OL(Gdn/<=JR7O(AGBJSC1h:=L/JP75<=<=JRGBn59&?@(C9/co759!}H/9&7:d(C9 < 4 >.
')(C9&34S+SC1D65<3U759!}H/9&7:F5!6e:=L/J6f5@<=I@b ~ b >D;(+6Br W W Br W 6f5@<34S+SXoD:=L/JN9o3!:
:=L/JKS+(qI(q:fi2zJUL&3RT@Jk@}r g/DE2L;(7OL3!c
3l(q9V(Gd:=L/JP75<=<OJRGBn&59&?@(C9/co759!BH/9&7:d(q9 < 4 .k:
65!S+Sq5!2G:=L&3!:Fr (G(CZ
9 ! <#.
01o3G=GtH/IMn/:B(C59EDe34S+Szn&5!(C9;:>Gfir W 3!<=Jk3p:#SCJR3GB:M fi342341u6f<=5@I d.$JN9&7JDer 7N3p9/9/5:fi&JK(q9
9 d.
6F2JKSCJN:fio<=JNn/<=JRGBJN9;:fi:=L/JUJN9
:=<O5n
15!6F:=L/Jkn5!(C9
:G#(C
9 dDeG(C9&7m
J (G:=L/JGtJN:85!63lSSIk3l*
(CIfiH/IMm
JN9;:=<=5n;1Mn&5p(q9;:>G(CZ
9 <#D;(C:z6f5pSSC5!2Gz:OL&3!:Br e.zeL/5;5
GBJ#;$3!9&?U/GBH&7OL$:OL&3!:oBr
;-y.v!(C9&7Jo:=L/JJN9
:O<=5n;1h6fH/9&7:B(C59~(AGP759;:B(C9
H/5@H&GND2zJ
9/5!2:=L&3!:K6f5@<GBH;P7O(qJN9;:BSC1
SA3!<=cJXoDBr; W .v!(C9&7J~r/
W (Gj3Ik3l*
(CIfiH/IMmJN9;:=<=5n;1n5!(C9
:j5!fi
6 " <D(C:P65!S+SC5!2G
:=L&3!::=L/JfiJN9
:=<O5n
1P37OL;(CJNTJR?u(q9:=L;(AG#GBn&3@7J#65<GtH;7O(CJN9
:BSC1uS3p<=cJ8X(AG#3!:IK5
GB:;E.oJfi?;JN<B(CTJM3
759;:=<>3?@(A7:B(C59fi;1gGBL/5!2z(C9/c#:=L&3p:E65<GBH;7O(CJN9
:tSq1dSA3!<=c@JXoD4:=L/JN<=J(AGeGt5IMJen5!(C9
:b(C9P
b < W
2z(C:=LJN9;:=<=5n;1$3!:SqJR3@GB:#.8zL/JM3!<OcH/IMJN9;:(AG#3G6f5pSSC5!2GN.dJN:P JGt5IMJ8n5!(C9
:(C9 d.fiv!(C9&7Jh
(AG#3fiIk34*;(CIfiH/IMmJN9
:O<=5n;1Pn&5!(C9;:5!*
6 l <D:=L/JN<=Jfi3!<=J8n5!(C9
:Gz(C9hp
b < ! 3!<=;(C:=<>3p<B(+Sq1$7OSC5
GtJ

:=5~ .U9hn&3!<=:B(A7H;SA3!<RDb:=L/JN<=JK(AG8GB5@IMJMn&5p(q9;:Mr z p
k b6 < R2L/5
GBJKJN9
:=<O5n
1j(AG83p:#SCJR3GB:d.
#Gfi2zJ$9/5!2GBL/5!28D:OL;(GKn&5p(q9;:K(GU34SAGB5(C9p
k b6 < e65<k3lSS#GBH;7O(CJN9
:tSq1|GBIk34S+SP .~Fc
34(C9ED
759&G(A?;JN<M34S+S:OL/Jk759!BH/9&7:>GF(C9b < ! G=3!:t(G}&JR?
1hr 3!9&?:=L/Jk75@<=<=JRGBn59&?@(C9/cV75@9!}H/9&7:>G(C9
b < >.e59!BH/9&7:>G5!6e:=L/Jd6f5@<=I ZU3!9&?$ b (C9b < ! e<OJNIk34(C9H/9&7=L&3!9/c@JR?
(C9b < >.|e5@9!}H/9&7:>GK5!6#:=L/JP65<=Ib $ b #(C9b < g3!<=J7JN<=:>34(C9;SC1G=3!:t(G}&JR?
;1or DG(C9&7JU:=L/Jk75<=<OJRGBn&59&?@(C9/co759!BH/9&7:#(C9b < ! >D9&3!IKJSq1hb 8/D(AG8GO3!:B(AG&JR?
1r
GB5j:=L&3!:M@}r go Br fi<OJR7N34S+S:OL&3!:g (AGfi3$n5
G(C:B(CTJUn&5!SC1;9/5Ifi(A34S>.h')(q9&3lSSC1Dz759&G(A?;JN<M3
759!BH/9&7:z(C9$b < 5!6E:=L/JF65<=I@b e b .L/J875<=<=JRGBn59&?@(C9/ck759!BH/9&7:z(C9$b < !
(AGfib g.$vH/n/n5
GBJ$@Br 8Z/.v!(C9&7JP:=L/JMT!34SCH/JP5!6 (G&5H/9&?;JR?h5!TJN<d:=L/Jk75IKn&37:
GBn&37J#w x D@(q:65!S+SC5l2FG:=L&3!:Y6f5@<3lSSEGBH;7O(CJN9;:BSC1GBIU34S+S Br ;.zL;H&GND/@}r e }r 65<
34S+SGBH;7O(CJN9;:BSC1$GBIk34S+S lD/3G<=JRQ@H;(q<OJR?.e:e65!S+SC5!2Gz:=L&3!:r (G(q9
b < 6f5@<34S+SGBH;P7O(qJN9;:BSC1
GBIk3lSS# 3!9&?D(C9{n&3p<=:B(A7H;SA3!<RD(q9h
< ; W 65<34S+SeGBH;7O(CJN9;:BSC1PSA3!<=cJdXo.0H/:#}r ~D
2L/JN<=JR3@GF2JMGBL/5!2zJR?j:=L&3!:F:=L/JfiIk34*;(CIgH/IyJN9;:=<=5@n
137=L;(CJNTJR?{(q9 " <#)(Gd3!:FIM5
GB:# .
R

fi;/z/R/EbR4

z;A$@
=@ABC|/=!RMO&!k@/$=B/M/tq@4AB#BO&!P=/o&OC&C!fi=/
/=
CBCj/RR==!B+C$/p/
fi;4&bCE-&4 ff
fifiFN!"$#&%E')* ( +-,.fi/.012344'5367fi $8
fi91:;<1 4=<fifi;> 01=?k/fi01fi50A@CFBD E "HG0IKJML N;O2<P (
QSRT9'CPV
( U 861:;.0K
+CXW UZY +Ct/\[B+CC;<]^ :;_`
+q W\d BD eeff
f3"H
Uhg C; J L lm NnO <P (
[B/ J L lom N;O <P (
qpsr
aCbc
<Bi jk
Bi jFk
4 vu w Nx Uzy
! x AOCR!tq

( |~ x
Fz/8!4C/gpe=/8/=@&=tq@$//=R=C
W p&UCN {}


ffe
ff |
{ | J L N;O {(
r
v
N

v
N

j`\
j`\

ff
ff

J L NnO {(
HQR =/NZ
PO/O!M8ORB;CPzM&OC&;M=&!F=/fil4C/Mps e
f
!
x <@&4= JML lm NnO2e{(

!8bCNKp&F&PC;
J L lom N;O <P (
!&B/
J L lm NnO <P (
=Rt&RBCCC;/U
Bi jk
Bi jFk
B/M/tq@E J L lm N;O ffP (
Fz+;&/Rf@4+ PZ


!

q

&



K

=

/

M;N/fiC&!=AF/ R J L lm N;O
( U 8
M;BC;/&/&BC{!R=PUff
Cfi/4N;==;U&!C;Rez;&N&C& J L lm NnO <P (
U E [ G
4+k`
Cfi/4N;==;k!C
N/=/#!4C/gp J L lom N;O {(
f@ {( OC
t<fi=MBM PK
( U 8 ;z++C=/N
&qh=/M! E [v G #N=Oq;BM=uq44$@=fi/=ROABCe=/;
BU!
Z QR !&;&/
E G O4&d=/Ff@=fi;

f
U E 9[. K Gr

! C& QR CKUOCR!U=&!fi=/N=uUBK$BnOCN
tq|Bk4+@&NtNp=/& 8 B&O
=&!=;A/=@&=tq@P/=ROCjz+;&/R!&UzC=;C=/RB/&/F!4+zBA/CM
z;&N?
}e=!+A!=& c BD E Gef3"
| FC$z/N=N/ff/&zd'/>4C$O&!+qKW
&!
+C W BD eff
f3"H
| +C W BD eeff
f"A E G2
r
abc
aCbc
/8/!zkNp&BK=/@C=RdC;N=N&kOR=/;5@/P/B+C/RhR!B+CNRKokp=fiC
ON=RB=RVC
=/#/=&;+CBP! eff
@/N=#=/#;CKC;fOk!BCPz#&48/ qj=/H
/!zCRn!&B
Heff
p&/N=fizfi&R@t>!BABBAN9 e
f
gz/Rtp=fi/=ROABCo=/k&@CBC&
/&;NF;A=z/N=N#!/;+qR)o8&OC&;fi=&!
C W BD eff
f3"
U E F4[ Gr
abc
!C&fi=;Az/!A/zl7 Q-R q/RR==pB+qPO/gNB#O&!

+C W7d BD eeff
f3"
U E [ G [
aCbc
zO<;C=R
fi;4&-2v R@Ceff
7fi !fiFNNC"9#C1:;F,@ B E "G:;sdn0vfi2N
4=<fifi;> 01=?fifi01P ( 67"fi.01fi2'53Ffi21fi5_60IJ L NnO ffP (
Q-RF61:;0
c eff
f3"H
|
<

J L lm NnO < P (
r

fioAffH9Fo

ffvCKv'4vK v'4Zn59ffn42;?;n;<$'
v;Kvn55K 5 z K2'
fiff 7v; ' ff < v
!9ffn42nvnH75s v'<< !"Z#; %& $ ff' '<5 ()
*)Xn; 9 4,+-/.01)K'v#2nKv&/' ! 3X; "A 9456 $ 7 !99`;4
n%vn!
H )!587v)n9+& $ ff C:;n5#< 4X=7n=%>?-@. ff%A v
BDC EFHG&J$ -/.0!2! !vKBDC LNM EF754; n'vO& $ ffQP /'\4n '4+-/.
vvHR $ S)nT+!U& $ ;B C L0M EF GVRW
$ S)nT>T*B C LNM EF G &0$ ffUX Yv 55 n5&&
' /$459ffn42;?;T
2'945 3XnZ) !,6 $ ff[P n !
v-N) P '\ ff] 2 'v#2nv2' ! 3XnZ) 9 !^6 $ 22'_`ba
c N;dZAnef g
` hi
g GZjkGVl Inm o)n9>KpB C LNM EF G&N$ ff^ vn5&54 5 Z
' H2'q
g
` hi
g GZjkGVl Inm ffqA vn5)r/'H !s>:-.Nn)2! v

h

g GVjkGVl Inm Iut N
g GVjkGVl Im Ivt B C LNM EF G &J$ Ixw

h ghi

h ghi
P nvN7,n(yvn 'WWM GVjkGVl Inm{zQ|oI1t B C LNM EF G&N$ ff

W8=Y9((HZ%(2=rZ%_n(7GH 1 I^t
}U~ 2 1
H
V



=^QJ2=#(H9rxfi(NH[ ` Q rU=(2
7GH 2

0
r(4S =(4xnK1u *7=GH1 -9.v
7 Gfi(2GZl Inm GVl I;t

7= GH (I w

ffv8v<Z)H2'5'jkGZ Ivt (2GZ 28GV I1t ( GV < ;45)v; 'v4 ff
P <;q 7v' 5)Z2' M' v 'Kv' 'K2'5'
'KCn5Z ff < HV K$'v ff;K' 5)
8GVl I2 5 4vT2' ff <)4' v 'K'v `nkCn5
b7v}<''v /G;
$ I% G;$ ff@ zKv288GVl <
'TK;< K7nv'vffn 457vCn5Zv '!2' q -[. ffq U7v ^7
'vrn !u Gv )K 'S 'vffn
2'C ff
k H 5Un(yvY7 'evVq<9'vffnCn59G '5v <
<v'''2nvv'7Hn5'Cff;42n:?; ; O& $ ffs v `

7vp5 "7' '<Zv2 O& $ ff;X 5C#<Cvv ffn7s G . $
'K?;CuU<5 () 4<C')n 1 ffuP /' ` n5'
9ffn42; )7 'V 5 $v ffnu ff u^)9#< ff) FH2!
vUB C F G &N$ IUt ` GH ff?A vO)4v 'X 4}v ` G -@.N;)X9K 4;-
vn 'Tv P ' ff)ff;X C9ffvU'< n 5 1?; )Z ff
< !\vp!v7vZ),;vG $ . $ 2;G $ . $ 5)v)5)9 v '
'T ' v)2' -9. ff <#<' n(yvn 'v Nff 2 Nff ) Cv v9
ZNv<H')n( GVl 2o q C' )v*B C F G;$ -[. ff )
v '4' Tv:B C F G&J$ -.K2Z & $ 5!9ffn42nOvn ` '8)(! ff
P nv<; 5 ;v; )VKH2K JN7 P '@ ff)

GVjkGVl Im 8Gl I;t
')<'n ff



B C LNM EF Gfi ` Iut

MZ G

fiNJvqNNW;0

U 2;HNW# ff
fi fiff
fi"!# $% &' fi( )
*fi,+&ff-=ff+.fi0/ff
fi2134 5
6
&7n8:9;
6 + 6 ff-!%&ff<"!=5?>A@CBD *fiFEHGJILK( <Mff-!
finffN<NfiOx%
6 9P Q
RTSffUWVYXZ\[ 8]?^ X_\[ 8]:`ba
cihkjmln
^
^
cd egf
^
^
p(%qOrst%u%v?wYrx3ys'r2z
S%}8~(~JY rxMs ~0}8~(~ } xwy

c
%
{
|
j
cid e

~ r } S~Y r sy7 yr%
s' }
f
- e {?
RTS U [-XZ\[ 8] X_T[ 8]:`t%u v ]
j

~ rs%:wYrx3ys'r

RS[ B2>]

r r r }8~( r }8~. r } xs' } syx3r wYr . wYrffx3rw ( s'3rys3r S. }~L ywYrffx3rw
L
3
{
r } s'yHr } } xYs x '} . r [ t%u v ] ( s3r S'S r yxw x3w . yx
-'} . r 9
3r rffy r s'3r 0}8M32 rxYs xYs t%u v [ . .0 x r
{
~( x3r } ] S'S r yxw r . r ~ s'y

s'3r r } r7s } r = r s'3r *[ >] A0y
{0
RTS' [-X_\[ 8]']
[8 ] ys'3r ~.} s's'r S#L
[ >]

xs'3r } r r ,} S'
j
{H
{l
j -O-
}8~. yHy r
.7}~L ~( r s' } . yx . srxMs s's'3r yx Sff}8 xMs %[-[ 8]']rxYs }8(~ rw
{H
X_[ 8] yFs } .}8~. ys'3r x r 0}8M32 rxYs xYsy % t%u [ 3r r
F[ 8]
j
X_\[ 8]Y` t%u v ]
t%u
r } x2s'3r rffy r r ~(~.} S' } xw r ,}*S'
s'y yx ~ wYr%s' }
j
{
{l
{l
RTS U [X Z [ 8]t%u%]
[ ]
RTS [ B2>] } xws' } }8~(~ rr2s'r S'0 } r r ~(~ wYrffx3rw
jb '"L- -L-(
j
{
'3 r yxs'3r ys3r } xw s' } [ >g]
ys' } RS'[ B2>] . x3ys r ~(~ wYrffx3rw
xHs' .
j
{
} r r } x r }g x3y x r Y~ [ rr [RT} SL\ rx }
]]:y s'3r ,}8Y73 rxMs
l
xYs r S%} } r0wYrffx3rw ~( x3r }*Sg yx S}8 xYs } xw yx ~ wYr0s' } sy S%}8~(~ '} . x3H9
x3s'3r yx3x3r yx0rs rrx,w . yx } . x3 9 } xwgy xYs
x3r r ''}*S(~ [ >]

j
{T
x t%u v - r yx ~ wYrs' } L#.2}8~. yHs'3r } r)y S}8~(~
t%u v
} [} ]y
{
3ry r
s' . r } x s' } x } x S~ w '} . x3t%u v s'3r yy yx X_[ ]i L

{
x3r r ''}*S(~
t%u v . x yx . srxMs s' X _ [ 8] } xw RS U [-X Z [ 8]i X _ [ 8]3`t%u v ] .\}8~. yx3ys
{T
(
~

~

r

w
ff
r


3
x

r
w

{
JJ2TO'0': -:
&= M=5 6
&
: $fi RTS U [ : t%u ]
H

j l

p(

r s'y 3y s' } s3r r . ryrxx3r Yy 3yMy3w yx
3ry r


3{l
}8 x x3fiff s'3r ,}8Y32 rxYs xMs y*J 3 t%u - 0s' } s\rr S' S~
w t%u xs' .
x3r Yy 3yYy3w2 } [ ]
: { 3y rs' .. x3ysTs'3r } r { 3rx,s'3r r .J r r rx r
j
s' } [ ' ] t%u
S~ w 8
` } xw ~( U x
[
]

nn8n
j
j
{


x r2 . } ss'3r r rx
[
[





}
}
~

}


}


ff

3

7



.
~
}

r ]ff !ff]'
r r syx3r
yx

nn8n
x } ff . }~ rw
xYs '}8
. xYs s%r xs'3r ~ 3S r2y s'3r r"
ff
{,
{0
rs [ r } rrxYs L}2 yxYs x x yxO] } xw F
ff
} [} ]Ty
3ry r
{

{
$} xw x r2s L% } r L%~ rw
t%u
[ fi-] t%u`# )y rr S'#
` }
r ~(~ { ss' . r } x } .\} x x '} r ,}8Y32 rxYs xYs yxMs S} S' s'y%s'3rwYrffx yx
} xw }3 3s yxHy } (~(
{

U 2

11 6 t%u

*fi+


xs'3r r ,}8 xwYr \s' .7 r yx r2
r
3ry r P
's . 3S r t%u


{{2|
j
` t%u v %\ } xw& :s'yr }# xs'3r } s'r rxYs %s' . s'3ry r }

x w ~ rs rs'3r x r
,}8Y32 rxMs xMsy* ' t%u
{


(*)

fi+-,/./0214365&1/7/8902:;,/./0=<",/>/?@3-A/3BC./D/714EGF

HJILKNMPOQSRLT9U*V9V*V URXWNYZ[IK \/I^]_ILK;`Gacb`ed[]fKhgGdKN]_ijZ[`Gk]lgGm/mnI*gGo_pqd/rpqdstgGd[uvpqdxwzyl{;|/IK `
K \/I^]_ILm[gGogGZp}k~pqK_ige] ]_|/jm/K_pq`edCllb`edKhg9pqd[];d/`ed/I^`Ga-KX\/Ib`d[]_KhgGdKN]fijZ[`k@]pqdMylHILKlc ZnI
R yI;[oh]fKm/o `GeIlK \[gGKz \[g]zm/o `eZ[gGZp}k}pqK_ilrGpqeILdlly
K \/I;a`eo j|kg^ R cO

4
n=fi[;Gzt
G l G GGLohc
l
9[IgebK |[g9k}kqifi]f\/`SK \[gGK
oh l q Ot/yHILKlRlgGd[uR ZnIK_z`"b`ed[]fKhgGdK
]_ijZn`Gk]

pqdQSRLT9U*V*V9V UhRXWlYgGd[ub`ed[]puILo
ROR l; yCINgrg9pqd"|[]fIlK \/INupqo I*bKpqdaILo ILd[bIK I*bX\/dpe|/Iey
`eK I^K \[gK;a`eogdic`o_ku`Ga
]pqLI"K \/I^m/oX`em[`eoXK_pq`edI/m/o I*] ]pq`ed @ @ G * uILd/`eK I*]I4gebKfki
Gh=yKp]K \|[]CI*ge]_iK `]_ILI;K \[gGK-o q @ G n l O6;a`eo-gdi^bX\/`GpbI
`Ga yCz\|[]L
Zi"z\/IL`eo ILj/y@*/o ROR lN Oo ROR l
G q /q G * y|/K
]pqd[bIR;gGd[u
R gGm/m[I*go
d/`G\/ILo Ipqdl;zIbLgGd|[]_I\/IL`eo ILj[yq;K `#b`ed[bXkq|[uIKX\[gGK
R;OtR l; Ot/y
Kp@]]_K og9pqre\Kfa`eoXgohuK `ILo_p}ai=K \[gGK*]pqd[bIz p@]I*|pGg9kqILdK#KX`xgfi[dpKXIfiup]_|/d[bK_pq`edI*gebX\
up]_|/d[bK`ac\pbX\pqj^mk}pI*]R^OR a`ogGK;kqI*ge]_K`d/Im[gSpo`Ga
b`ed[]_KhgdKh];R^gGd[u!R zIj|[]_Kl\[g9eI
X l
O4y
]cI]_KhgGKXI*upd=eI*bK_pq`ed[y[/`e|/o;reILd/ILohg9kK I*bX\/dpe|/Ia`eoNb`j^m/|/K_pqd/r#K \/Im/oX`eZ[gGZp}k}pK_i`GacgGd
gGo ZpqK ogGo ia`eoXj|kg^wp]K `m[gGo K_pqK_pq`edK \/Ilz`eo_ku/]zpqdK `#g[dpqK I^b`Gk}kI*bKfp`dfi`abXkge] ]fI*]
]_|[bX\KX\[gGK
w
Z[IL\[g9eI*]|/dp}a`o jkqi`GeILoI*gb \bXk@g] ]lgGd[u#KX\/ILdK `#b`ej^m/|/K IK \/IoXIk@gK_pqeIcIpqre\Kh]z`a-K \/IbXkge] ]fI*]Ly
]zI]_\/`GkgGK ILo*K \/IfibXkge]X]_I*]"go I#I*] ]_ILdK_pg9k}kqiuI[d/I*u&|[]pd/r&b`ej^mkqILK IuI*] bofpm/Kfp`d[]Ly\/Ipo
IkgGK_pqeIlzIpqre\K
b`eo oXI*]_m[`ed[u/]K `K \/Ilm/o `eZ[gGZp}k}pqK_pqI*];`GaK \/Iup}ILo ILdK
b`ej^mkqILK IuI*]Xbo_pqm/K_pq`ed[]rpILd
l^y
9C2*@*@9lOl "s G G GGS; ^ ohs l @ [
* # ;2 9 L GGL
- [
^s



2 [ ^ - l O[



l [
z
l







ff fi


!


ff fi

V

9[#" pqoh]_K*`eZ[]_ILoXeIlK \[gGKzp}a-g9k}kk}pqjpqKh];Ip]_KlgGd[uK \/IuILd/`jpqd[gGK `eozp]d/`ed/LILo `[4K \/ILd
oh c
l
V
l


s!#l


\im[`eK \/I*]p@] GK \/I-uILd/`ejpqd[gGK `op]pqd[uILI*ud/`ed/LILo `[y
"/|/o KX\/ILo j^`eo eI ZilHILj^j"g;{yqeLo c
l %$ c
l
/y'&ILd[bIo zc
l Oo cc
l
yIbLgGd
K \/ILo Ia`eo Il|[]fIz\/IL`eo ILj 4yq*K `"b`ed[bXkq|[uIK \[gGK
_ l Oo _ l ^ V
gGo K;g `aK \/Ilm/oX`em[`]pKfp`d#a`Gk}k`G
]pqj^j^I*upgGK Ikqiey
`^m/o `GeIm[gGo KlZ [oXI*bLg9k}kK \[gK
p];I*e|pqGg9kqILdKK `^K \/Iup@]|/d[bK_pq`ed(*) +
, yi]pqj^mkqI
m/o `eZ[gZp~k}p]_K_pbo I*g]_`edpqd/r[KX\/I"ge] ]_|/jm/K_pq`edK \[gGKo l /gGd[uxm[gGo Kg
cI^b`ed[bXkq|[uI
K \[gGK
^s l
^s l
sl

V
) , l
l

-/.

fi0#13254!687:9<;!=?>/6/1!@7
ACBD2=?=E6/1

FHGJI/KLKNM!O%P!QSRT/UVWCRXK#YTUK RXKSQLZ[U5Q]\DRQ_^a`]c b IdUefRgK RUihfjlk*mon:pRUYZqWCRXK#IqYT/O%P5rZ[QLZse5Z?KLYtSRP!QSRT/UV
\<ZJOqMKSQu^IEv/ZJQ_^IQwWyx kzRXKuv+I+r{RXe|n~}<^5MK[V
QL^!ZUM!O%Z[tIQLT/tuT/UQ_^!ZJtSR/^5QL^IUe~KRge5ZaTdQL^5RXK
Z?MIQSRT/URXK#K RO%P5rGi:tLjSW#mon:Z[UYZ/VQ_^!ZP!tLT5rZ[OTYT/O%P!M!QNRU!:tofjWsm:tLZ?e5MYZ?KQLT
I%KSZ[tNRZ?KTYTO%P!M!QoIQSRT/UKTQ_^!ZT/tLO:t ja m:T/t<v+IdtSRT/MKYTO%P5rZ[QLZwe5Z?KLYtNRP!QNRTUKJn
RIdUGKNMYL^~e5Z?KLYtSRP!QSRT/UJn<Z?Y[IErrQL^IQ%Y[IUZie5Z?YTO%PTKNZ?eRU5QLTQL^!tLZ[ZPItLQK[qQL^!Z
M!UItLGPItLQq?V|QL^!ZqU!T/U!M!UIdtLGPItLQ%EVIUeQL^!ZqZ?MIErRQGPIt_Q c nqpRUYZJCRXK*RU'hfjl` c b moV
\<ZYT/UY_rMe5ZsQ_^IQ:` c b RgKHZ?M5Rv+IErZ[U5Q#QLTw c n:K RU!%}<^!Z[T/t_Z[On?*QS\<RXYZIdUe%KST/O%Z*P!tLT/Id5R{rRXKSQSRXY
tLZ?I/KNT/U5RU!V!\<Z/Z[Q?
:tofj : : c





tofj
tofj
j
tofj

:






`
`

c { ` c b
` cb
c b mE:t j ` c b
c b mE:tofj mo

lUT/toe5Z[t*QLTiK RO%P5rRG'Q_^!ZstKSQZ!P!tLZ?KLKRTUVtLZ?Y[IErrQL^IdQU!T/U!ZqT]QL^!ZqP!tLZ?eRXY[IQ_ZJKSG5Ow8TrXKRU
T!Y[YM!tIdUG5\#^!Z[tLZqRU `]c b ni}<^!Z[tLZT/tLZ/VQL^!ZJP!t_T/I5RrRQSG'Td Rv/Z[U `]c b RgK
Z?MIEr3Q_TQL^!Z<P!tLTI5Rr{RQSGqQL^IQQ_^!Z]ZrZ[O%Z[U5QoK:e5Z[U!T/QNRU!Q_^!Z+jeRZ[t_Z[UQ[mYT/UKSQIUQKKLIdQSRXK GsKNT/O%Z
PItLQNRgYM5rXItYT/U5/M!tIQSRT/UiTdU!T/U!M!UIdtLGaP!tLT/P8Z[tLQSRZ?K[n<lQ*KS^!T/M5rXeZwY_rZ?It*QL^IQEV!GaKSG5O%O%Z[QLt_G/VIErr
KSMY_^YTU5/M!toIQSRT/UKuItLZ%Z?MIErrG'rR/ZrG/n}<^!Z[tLZT/tLZ/V
QL^!Z%P!tLTI5Rr{RQSGTIUGT/U!ZfT#QL^!Z[ORXKqI
YT/UKSQIUQEVZ?MIErQLTqTv/Z[tQ_^!ZwQ_T/QoIErU5M!OqZ[tT#YT/U5/M!toIdQSRT/UK[n S~ Z[Qe5Z[U!T/QLZqQL^!Z%YT/UKSQoIdUQ
\#^5RXY_^aRXK#Z?MIErQ_T tofjf+ ` c b qm:TtIErrn
}D^!Z*rgIKSQKSQLZ[PRXK#QLTKS^!T\Q_^IQ?V5R] RXKZ?/M5RvIErZ[UQQLT< < j moV!QL^!Z[Ui j
c

jE mo

:tLj j

c





tofj g j j m? tofj j j


c|
c8
?E?[:t%j
jS m[ j mq:tofj j
X %E?s jlMK RU!}D^!Z[T/tLZ[Ong//KNZ[ZsZrT\m

j? mo

}<^!ZtoKNQKNQLZ[PRXK%K RO%P5rG~P!tLT/I5RrRXKSQSRXYtLZ?I/KSTU5RU!8n}D^!ZiKSZ?YT/Ue~KSQLZ[P~MKNZ?KqIdP!P5r{RXY[IQNRTUK%T
}<^!Z[T/t_Z[On//nClQRXKaZ?I/KSGQLTKSZ[Z'Q_^IQ jS mqRXKiI~K RO%P5rZ~/M!Z[tLGT/t {l/ j L ?E

j n
'Zs\<T/M5rXefrR/ZqQLT%KS^!T\QL^IQ

:t%j j jS m::tofj j m{ m: +
c L
\#^!Z[tLZs}D^!Z[T/tLZ[On/ MKNQSRZ?K<QL^!ZrgIKSQ<Z?/MIErRQSG/n]}TqP!tLTv/Z*QL^!Z#tKSQ<Z?/MIErRQSG/V!\<ZKN^!T+\QL^IdQT/t
IErr|VQ_^!ZKSPIYZ?K <IUe N
j <^I?vZ%QL^!ZiKLIOfZ%OJIE5ROwM!OfZ[UQLt_T/PG
PTdRU5Q?VUIO%ZrG ns}<^5RXKRXKP!tLTv/Z?eGc I/Y_\#Itoe!KDRUe5MYQSRT/U:QL^!ZY[I/KSZuRXKQLtNRvRXIErrGQ_tLM!Z/n
}<^!ZeR{Z[tLZ[UYZZ[QS\<Z[Z[UQL^!ZjamSKSQ%IUe'5QL^~Y[I/KSZRgKfQL^!ZiI/e!e5Z?eYT/USM!UYQ j moVH\#^5RXY_^
IO%TM!UQoK
Q_TwI/e!eRU!qQL^!Z*U!Z[\YT/UKSQ_toIERUQH n}D^!Z[tLZItLZQ\<TP8TKLK R5RrRQNRZK[n#RtoKSQ?VdR { V
SEEoEo
E
Eo


fiff N H ! [
L ! "# $S% '&
ff
()&

fi

6
'
7
:
8

9

;
>
<

=
?
_ ' ff fifi*,+.-/)0 2 1435

'fi@BADC EFC

G H

fiIKJMLMNPO.QSRTOMUMVNPWXJMLMNZY[JM\M]#QK^MQ`_LMaMUbO.ced
fgMhi`km
j ln fpo lrq h l f$gso l iMhtvuwxi l fy n o)isf n iszst n z n i|{ l w}yh~ n o)i l fgMh~ n o)~M~'hify$w4zD|weo)isf
uw4~,s)hf
o)iMTfgso f$hwefgMho)i|{s|uf
o)w4i%' kZ fgso l l iMwxf,fgMhu n4l h4 n i|{o)i|{shh {XfgMh
Myw4hyf
zth n yhFfy$z4o)iMfwMy$wxhu n i,hF n l h'wxyT}Mffgso l {swh l iMw4f~ n ff$hy e|h u n l h
thKfgMhisiMwetmfg n f!ypr :
$ p ) r FX) fiyr : r$>X k : fieo)i|uh
|wxfgwefifgMhMywM{s|uf l o)ixMh l f
o)w4io)i|u$)|{sh n, n uf$w4y xo)fFo l oy$yh)h n isf nxl fw,tFgMhf$gMhyfgMhwxfgMhy
fhy~ lFn 4yhh4
hu n iiMwetMMfFh4hyzsfgso)iMfw44hfgMhyfwuw4i|u$)|{shf$g n f
fiy
> Dfi:!|y 4 | r}fi>y r > fie!. x| k j j

k
MywexoiM n yf
hiMwt n {M{syh l$l fgMhXo Mhw2!uw4~MMf
o)iMfiy rF>'w4y n n yso)fy n y$z,'w4y~s n Fw[{sw
fg n feth"~ l f q l fo)ixh l f
o) n fh"fgMhF|hg n xo)w4ywe!y rF>'w4y l ~ n :F j fio l w4~,h l su$ohisf
)z
l ~ n j n i|{)hf,hfgMh l hfFwefi~ n o~M~,'hisfywxz|weo)isf l fi l$l M~,h n i|{
j n yh l f n shX'w4yF%Kz[{sh q iso)f
o)w4i%fgso l ~,h n l f$g n fw4yh4hy$zmkj thXg n 4h k j % hf
hfgMh l hfwe l w4yt"gso#u$gm uw4isf n o)i l fgMh,uw4ie
Mi|uf,| reoi|uh k j wxy n :k j
th~ l fg n 4hf$g n f k fi wxy n % eo)i|uho l"n u$)w l h { l hff$gso l o~so)h l fg n ffgMhyhh l f l
l w4~h l |ugf$g n fw4y n :k[j n i|{'w4y n :% thXg n 4h k hfF
%|hf$gMhF'w4y~s n
)( r)
4
gMhX'we:)wtoiMMyw4w l o)f
o)w4io l iMweth n4l zfwMywe4h4


fiff "!$# #%& j #'()#$*,+-/.0' #%&"!$# 2 1( 1K
) 1 #%&436 5
#'(7#8#$*9:;<4=fi!$,%

!y rF>

!y rF>
. [? fiy r}>
> :

@ACB )h n
)z4
l$n f
lq h l fgMhuw4i|{xo)f
o)w4i l B w4yw2 n y$zEDM
FHG| n :)wtoiM l fw[uw4i|u$)|{shfg n f
fiye

r> F4 eo)~o: n
)z4zgMhw4yh~IG|KJ0G n i|{Tf$gMh nxll M~,Mf
o)w4i l wegMhwxyh~IG|KJL.
thu n i}uw4i|u$)|{sh,fg n ffiye
F4eo)i|uhfgMh,uw4ie
Mi|uf
o)w4i}weKf
tw n4ll hy$f
o)w4i l f$g n fFg n 4h
Myw4 n so:o)f
zMF n l wFg n4l Mywx n so:o)f

z Fx thKu n l hgMhw4yhN
~ DM FOfwuwxi|u$|{shfg n ffiy rF
fiye
F>[
[
P wetyh u n :fg n
f Q l h xso n )hif}fwTf$gMh {xo l
Mi|uf
o)w4S
R e z l fy n o)4gfpw4y$t n y{

Myw4 n so:o l f
ouyh n4l w4iso)iM||thu n if$gMhyhwxyhuw4i|u$)|{shfg n f
fiy rF[
[

fiy rF>[
fi? !y r}>[
[
e >

z,gMhw4yh~TDMFO n n o)i%sfiye r}> p%e fiye r}>gMh{sh l o)yh {h Myh l$l o)w4i,iMwet
'we:wet l
hiMwt l o)~,s:o'zf$gMhh Myh l$l o)w4i!y rF>[ p|[%!
UV

fiWYX[Z]\^`_ba/cde^Xffi_gihjZkdd^X

l8mnonpq
rq
nfistu-vfiwCxfiyz|{~}b27}fi,}}0]K/8$9y;,}b 2b
2]}"$,
{Y2]KM Mb {Y
7"C;K2H)"$,z8yz2y)8"]bA2"]-)),z`zy]($)"MyKz()HHzA)yzA"yz(Hz
Hyz7-}~2"[,,by8"])y-Hz(;H
l8mnnAw6j(C]()]CA(HC22/~( ( (/HY`AYH(
YY(/(2](
(0]-](9)]HYH](HA2YHA
$/]209Y)(fi Y](4H(H]-Y/Y~H-4yHHHzA"y0
( (YH|;
fi0(fiA9H8bH97-
0|(9]CH`8((
(A)()]2A9H47(C(AA2E]K]|fiAC(]
-7 2(b(~A)()]($9)]2(~7Ab)YAH9
(2(AH$2]H-H(9AH(4(2(]HY9 ( 9;|H|;
)/)]]H-H
j] ]
9MA9-7]((2 [;](((9
0(//(C[]A]9A
(C 7(H(NY]99H$
;AAY|,;),Ayy0H/](]A
A(H](C(7$]((9A((Y(4[]AY]A]A4]H9
](AH7/A(H]M-](]YY]A]Y$7(H]9AM4
(2(b2](A ,;),Y|]9AE -;A A0 (/8AA

A(H] ~(j9
(
fi YH(2 /8A4]H9
](AH


2A]H(](AH
]9
(4bA[9H)b`(464bA(H~ 9(A(H]/(

(H ;9 ]/S8A]H( (C)29)(7] A9H8 M2
]
/
S(H b Ck( /-)-|2(0CY [(H
b k fi~/2 C ]A(]((N0|H7/7]-H9HA)(
((]Y 9 (H/Y(7/H (i( 0
HHH /
A2 E]K M]9Y

`$ 7Y9A(H]

fi /N(

b k2 ]Kfi 7Y$7(T(7($()0]2(
Ab2A
(H$
CH(E99HC(](]C0|j9 jH(H


(/b 22~b {Y22]K A2/|(C(7(6 226k
)-|~(06b 2 8]K8fi86k$4HA72 8]K8fi 6`A(H]
-]M; H
A]HC(8HY6Y()2M]KE
A]H889A]
0 ]MAH4H$,fi(
( `(2() YH97(
AA(HC9]H0Y0(7A0$(8- ( 6k|70H
k]H(Y(/HH]9
( $H
( 2() H7(0
(
kH2 ]K]fi7 A](Y9YA
(
0b/(C(]--M
]]/|]H(0$A0$0|HY`2H]4]Y7(2$/]2
/]HY9]
4(0]-
9 9)- ` j ]9A ]H9H(7/$`
8(H(
/((H(
/((
7(Y]9
87CA 99A
0CH/HH
7(7(HM;];] (
(8)0((H(]7H A`CA(H]|j9 98](A
- ]-

7YA]E7AA
~M]A]fi]Kfi7~(H(A)(
A] (EA TH HAb((]-- Y](
Y7
H8$ 9H](
fi(9]--0j]9
YY]( 8-fiH(H$)(
C(]( $(M]A]Y(4A[(H ]/7C7 /
9





ff fi


%'&

*

!

)(

+



/. +0

4=&>< 4

64 fi @? 4
4

fi
;F

fi

4

ff4 fi


ff fi fi



,

ff fi

- fi

*

%3

A? 4

'45

647fi
24

64 fi 64

C4

fi

#"$


21

98
:4

ff4;fi
24

64 fi
=B

E fi GF
IH@J
Kfi
L

6fi
*
H J fi
H J
NHOJ P 'Q
R4 SH@J
:0
K4 UHOJ
6VW
XZY

fi[2\^]^_`bacd`^e^f _g\^]^_ih:\^j^kla2m^ano]^p^e `bq7r
s^tuwvx;vy{z|y~}S}x;}:5

67
6Kx zudx7}yR}^=tZ:x y~y~^S6uw7^6}ffu;CN@7y~wZ}x7}>yl
y~Sx7}uw@)x7i57
67:x;}yi oy+wZtR:x z|z%v^^}Evuw^
Sx %x tu^ibuwt+}y
}9u= ^u7zo@+^Z
K}^xw^s^}y~uwK}x7}9}^^uw^^x;ts^t
yZx7}
)x;t^uw}9Z}y~uw^
y~i}^
|DOylyC}^EZxwEv
Zx7+}^+}uw}6x z^EvZt9u72suy~vz~:%x O}uD^uu5+}^)s^tuwsZt}y/

u72=xw}^Z=t6zx7}9}u!5

67ZAyOy/ZsZZ5}Lu72RoK+Zx7=}^Zt6uwt)6uz/+}x;}%}^
s^tuwvx;vy{z|y~}Ru;bEuwt+Dy~Z}z~Kzx7tDff7y~wZR}x7}5

67x7}yK'y+vu^

x %x
tuwvRuwKy~ZsZZ}:u;i7y~6=}^:s^tuwsZt}y/
Lu79x7R6z~ZZ}Kx7>y~}ff
t6zx7}y~uw}uD7

ff;EZx7v!^u5Z'y~ZsZZ5}z~iu7}^Es^tuwsZt}y/
Cu7Ox=y|$ZtZ}96z~ZZ5}
^}^9y|$ZtZ5}ZwZ5}6%+bff6+ ff

2x;txz{zy/ZsZZ5} %@^Zt6uwtw}^9s^tuwvx;vy{z|y~}=}x7}
}^Zt!y^uR uw:xy/6z~ZZ5}=x7}:xz{z}x7}
A}uwwZ}^ZtOy/}S57

67@x7}yl
Ny:x7}u5}
ffL d@y+vuw^^}^=s^tuwvx7vy|z|y~}du;}^=6^}Zy~uwx5y~uwv6y~^xzlw2t6zx7}y/'}u
^^
'


ff;@^Zt)x7t= ^ %x ^2u;}^
)^uu5y~^+6z~ZZ }ffZu+}^)s^tuvx7vy|z{y~}=u7A}^
x y~uwv6y/^x z9x75^Zt@y~DxCu^6zy@x7}Au} ffo= A@y}Z^A}uE)x2wu

}uy~y~}w@^Zt6uw9tw}:^=6 :}Z$y~uwx y~uw7

ff$ 5 $N~)xEx5s^}uw}y
s^tuwvx;vy{z|y~}K97y~wZ
^xw%
y~t

y~x z|z~w)x;ty~x+su5y~}y~uw'}us^tu7w9@^ZuwtZw^
9 7 ZZG

+ oG $ %76+ |iGS27 i|
9
%| wKfi w Z ff 75Z| ff
9 $
- 5! )
ff fi ffG 7
67"Z7+ ff#ff ff76G$G> 7G &% Z'5w 9 67;');!:567"Z7EG)('7*
|
, +./#- ! 59Z76+ | 02143 16587fi9;:!< : !=>65! 5w9 5= ?
Z @ ffB
C wDff ff G7 6$
5w EoZ7:wG2
GIF H E 9 7 JFG 7C Z5Z|; E7* 5w 5C ff # 6L KNOM |99 5 : PZ^
?
G+5; QZG ff ff 7GR F UTV5
W X
9 [Z\ 7]_^a`cbed"gif 7h ]_W ^ajt`cX!bed" f k SbF l knm \po SbF
gqh \po
Z \
) 5L w7+Gw79 ff fi ffG 7r
" t62
u ^+@y~}^uw^}z~u5u72Z^Ztffx z|y~}}x7} Z}y~uwx z|z2}^6uw}ffx7}+vu7zy~
u}x7w} v + /- N xIv ff W tuwsuy~}y~uw{ z+} |b
W ~X
9 7 ]_^a` W ~X 9:^= =V W ~X 9 ff
h
\
uw}%}x;}@Zx;^^uw}@
xwy|z~:}ffxfiw@z|y~+y~}ff@u7 W ~ X
9K)7) 2xw G!F wu
A}u F v
Zx7
}yA6^s^t
W y/u:ZsZ^uw x7L}^%7x z~^u;
ZsZ^uw+}^^u7y6%u7 G$F %u7@ZwZt

x7s^sz~y/^ tuwsu5y~}y~uw z+N@9wZ}
W ~X
9 7r ]p^` W X =p W ~X 9 ff
\

h

KKZx7^u7 }6xw}^=z|y~+y~}Kxw GF u5
!W }u ^F 9ud ud}yZ W tusu5y~}y~uwz+^@^
5suw7r}]_^^a`"
b#
+f u;}^!F }^ZuwtZy~sz~i}x7W } tX LHiuwtLuw}^ZtOylwA}^'Z^uw+y~x7}uwt
ZZtu^ff x;t}@x$u7$}^s^tuwsuy~}y~uw:}6z{zA@Zx;+y~w^uwt%}^u5
g h k \po S$@uwzv
Z \
6uwsz~Z})
6ty~s^}y~uw%}x7}%x7ty/6uy}Z}%@y~} + /- AZx7=^u7 x7s^sz~!sx7t}vA}u+wZ}@}^

y~t

z~}



fi$q'wq##qVpc#

#Nc6"n_""#
;j)#jC#fijVjCjj4e2j##,jq;"6*jq2"jjUifinjqC4
#B#Bl#fie8BrB"B)qBLwBj;jqwqj"Vje"iB4B#L2}qpww#>ae
jL"q'Bw#"qwBj!q#w4fijjCw#jeBqL#jR#qfij;j4jV
"I!i4qCNeR#;"4#ULieq#B#"C;j;"'qB;B"
#laefi"j"'Nq")B,jrjN,"2$fii[CwN2"q**
#L,"jpj4fij#V'4#jL>aRq[2j#N")qqq2a,wj. #
RaeB4"Cwjq2lwBew4Njwwj#,4"#"#ja!"j
jqBc#U
; Bi4Cw4j
6)
'q";N#j#2
ff
fffiff 4
ff ff q
fiLlw 2#"caaV'"LLRB#B4L.#j;j },pi4qj#r
qB4BV

je"Nq'
w#i"'wffff .4'D$!#"$w%fiD'q'&,j ()(+*}-,)*.&*"e!l njj
.;q4##q#j

w#i"'wB*re#i;/q80ra'jV#/q1;B2 2fiB*;#%3ff.cj#4444Vqrae
"#42j#)"BjVVjV4ffe#q*lw;2rBiqeiL#"Rjj#
ae#i j4B"} 5qq#
5"#i"*eV#2Vjj4
675ff5B##i i4Bi"
qjIqjB$

a"fij)#B#;qjpw#j)q"fij>aC,r9
8V#D*)j#
#fi: ff*;$fi*_, <j*
>
=2D ?.@ *" *A*
ae*!
B) ;*C
=)E
F.GH#4
ffIq#"eff
ffI3Jqff
ffq
,$q"* /q8qNij##Bff2 w#i%3ffKffMLON PN Q=R* SN (+TqUN *A*63fi2V=*
W <RX{3 "#"% Y)ZX[#$# q4Bq#)4'VNB\0R8B'j"
."Vr>ff%3#ff eVfiff*3]Vi#"ff#"wW<N,j ()(+*}##B#Bl*N#)jj#Ne"
."VN)^%efffi_8Vi N"
N#"

`<j:"qAYraXi3" RaeB4 N# njj

Nq4CVNp,^%ffffIb Ljqq)#Lqq4Bq#q7c q#"a4Id
qR#2 eq"j4q4jC26I,rNBi!j##fi:ff*#;q),<j*\=2D ?.@*
j3 *f*
B#*'
B) ;
=)E
g.GH#qqV4
ffSJbfiff fffiq
2aeV;nB42 2aeV9/c hff#L,j"!iw*j# !j*fi#*$,)*.&*%"e
.;q4#RaeB4jj"w;q4##"};
"fiBV")#%3KS
pje"B4B2#"BjLq;q#ff*_`<)"E(jff*
)Vfilkbm#."ff SJ$ffq
"fiBV")B02B"jV/c18b2 !q"lnl%ff *B#fi8e.#4#Bq'#qwqj#"B4B
j <o
e\
#",2"$+e*Cgl%.5SfiblKS3J:fiq
pVq0B2 p /c%3ffff Lw#j #Lj##BqIwBj 6B 6 2iqq"
/B# 0l qB#r*#6V)B42 .44eVV;4sq,qjV,).&*%"#!w4"j*+#\:"at)+<%fi(*
. Dfi"$#eqV fi.iS Jbfiff
qRaw)#;jj"2#jnjq8fi"q

uv

fiwxyz:{}|~i{3z:-xyzEx||yb{}.
dWl)#b#s#7%l%ff%^ff.Ab.-A[R.lo:ff+.l@..Ab9Zff .R.@a.4
.>.)l!l .[}%.f)
dW!d - .4 !%dldffff4ffd3f..:l%3ffff%l.ffR.aff)
.C)3jd.jjds.M@fffff7Z)R)WWf)a)f.)!3f7R)%#AASffl M.bff}
Qa.44l!4%3ffff%9d R)b+ffSf.`7ZE3d))ff>fW)WdWdff
[W))f#ZsP^E-.)$
Q`!fUff>)Wfffd _.W }%ff%E37!))ff) ff$)Rffffff
)fffR)Wffd).MffA+.#ff^.)#%'.[-A +ff#:+A7#%@-Q .ff
4. 3ffff
Q`!fUff>)Wfffd _.W }%ff%E37!))ff) ff$)Rffffff
)fff$ffWffd)^`ffA+.$Wa++ -#ff Ao:
ff ffA: ):+3fdff:
W%ffff3ffff
Q.
s4l%ff%^ff7dWE.h)fiff%WW %)ff)a.3d$ W3jW)))^).
oA@.\.#>^.:A)ffj ff3ff.

QSff !#d-3d)44}!d -.j734!#%ff%ffff` .ES d! )) ff
)3S )^

V.4%@f.+:+#%34ff3b
QSff!CQ3dl)4h#C -.jdh!%ffff%RRW$)ffWf!ffdWdff3)ff.jjdWd
)Rff+.)ffWff^W.%>ff)-Eff.!
QSff!4d"Q3dl)4C#!d4 - dh!M%ff.#%RRW$)ffWf!ffdWdff3)ff.jjdWd
)>. ffWffPW%))lff)ffffff#UPMi..d$sfi&%%#ffQ
.
Afff

-3d)4}ff!%ff%lQ.3df^.'ff%W)+ffhdff.f^.C).j dWffC @ff}):+3fdff:
(*) bffff}
ff3S S.%ffbff%3+ff)E.`7 )ff.'W%.``Wf3 $.fl+ @ff.3, @.- ) }%
ffff3bff

ff3ff.R%Sb%0/)1W%.'ff'E3d)) 32SE#dff Rff!S 4Wd
Usj%R4 5
6A7
#A) 8 .%$ffA%$M433:ff}4>W h)).!Wfffff
[)

ff3: R#ffff+9Z) .Wdff3d 9ES d!$+))ffE)b^`3SM)^:Ms%
ffff3bff
ff34#RM%ffffb%-ff)%.`7>.fW)`7`7 .- )ffE37aO W%.)ff
R:!Vs#j4-94 % :. ; .\^) )++ < :+ffAAA@ < .:=#+ffAfA+%ffV+ @
43ffff 7> -ff))@ ? )Wf.
-:}ff.C )%Af .^`) @+ 9ffo jf.4##ffff4

ACB

fiDFE*G>H@I3J:KML@NCO0I0E@PJ"QSRTGUNCN
I0E
VWXYX[Z\C]^7_[]>`badceXgf3Z\ih]j@_>k7_3lnmCo0o0p*qn_'rsX[W0tuwvyxzW0\Fcf@f@\iW
{|u[}~ciZF\iZCc0W0h>u[h@t_"hyZZX]_[]@uwvi]
_[]+`0Fc\iW0@C]:b_Ml!@_Yqn]T+iC7wii'z0z'0+Ci.5n.Y.#
1FYn
0y#3..z0z5Mn3w@1
0]0f@f_3mC0

_CW0\it>chV1c>x}=ch@h]
|chciZW] r7]3|chU\nchvuwivW_
V>@@\it]j\
_[]a_F_3lnmCo0>qn_"M@Z\iZxzZ\iZhvZ7vXci_M0Y~#'z'n
]04l!>qn]@>
*0o>>_
cf>Xwc0vZ0]M"__'_lnmC0p>qn_nF0Y.*n>Y.+i||z@. _:h@tXYuw#i\nchXwc#u[W0hu
+0zY0zn03n~>i|z.]>^W0Z\d:@>XYuwvc#u[W0h.]yZk:W0\i']mCo@m0_
'vZ0]'1_^7_[]@`SFc
uYc@]a_lmCo0>0q_FF71n.w_uYXgZ0]yZk:W0\_
TuXwiW0h]_'lnm
o00>qn_":\iWc>uYXuw#uvfiX[W0tuwv_M Mz03iz
w[0'n
]>*]4|m**|_
:c\#uw]3j@__[]`:ZhvWe@#c@]@r7_'lnmCo0o>qn_Fhi@Zcf@f>Xuwvc>uYXYuw#Wx:}~c
{>ug}7@}Zh>i\iWf|=iWu[h@Z{Uc0v
\iZCc0#W0h>u[h@t_Tiz.n'003@|#dT0iC0z7Mn3w@0]'0]m@_
"ZCc\#X]:j@_lnmCo00*qn_=+i||zwzMn3w@=wzCg'+3z._~W0\t|chV1c>xz}~ch@h]|ch
@\nchvuwivW3] c
XYuYx_
"ZCc\#X]jU_lnm
o00o>qn_:\iW0c>uYXYuw##uwv#Z}~ch|#uwvxzW0\Fh@Wh@}W0h@W0iWh>uvfi\iZCc0#Wh>ugh@t3+r#@\0Z0_+h\nc0v@
}~ch]|fi_|j@_[]>'Z0ZCi@Z0]@a_>j@_[]>`MZu[iZ\
]*fi_@l!@_Yqn]>
e3:4iz.n'0z0'Ci'n
n.Y.#1FYC0#3..z0zs*1 00]Mf@f_+0
*@mCU_
MZf@\#u[h|ZCugh0*F'n.z=*0]>7_@0c
xzZ\Fchj@_@"ZCc\XlZC@_Yqn]>W0\it|ch
V1c>x}=ch@h]'|chU\nchvuwivW] c
XYuxi_[]m
o0o0@]>f@f_0oo
|mCU_
"WXYX[W*v']j@_:_4lnmCo@qn_"@W0@h@cugWhxW\du[\iZCvFu[h>xzZ\iZhvZ0_|n=']>C]'p0p@m*p0@_
MZuwv@Zh|c0v]a_lmCo|o>q_4|nn~1+i||z0_:Fh>u[0Z\u[!5Wx ceXuYxzW0\ih>uwc=:\iZCi.]*Z\iZXgZ0_
0ch@h@W0h] _[]|`$ZCcCZ\C]>_3lnm
o|o>qn_4|10|0z0M|#+77|30z_"Fh>u[0Z\
u[!=Wx:zXYXYu[h@Wuw1:\iZCi._
0c0#\#u]:_lmCo00o>q_^Zxc>X[1\iZCc0#Wh>ugh@tu[h #Z}~ch|#uwvfih@Z!MW0\*Mc xW0\}~c
XYu c#u[W0hWx+\iZCvW0th>ugugWh
chu[h@@Z\#u[nchvZ0_F Mz03iz
w[0.C]@| l!>qn]3p00
*0@_
0f>u[Zt0ZX[c
X[iZ\C]'^7_|j@_3lnm
o00>qn_':\W0c>uYXYu#uwv1\iZCc0W0h>u[h@tu[h5f@\iZCuwv#u[0Z1Z{@fZ\T*#Z}~_hV1c}~c
X]
:_e_[]`'Z}}Z\C]0j@_0M_l#+@_Yq]4zw .0i'zC[0.C]f@f_@@0@_FW\ii@
aFWXYXwch']3rd}=#iZ\n@c}_
"c\n#u]|r7_@lmCo0@mq_b1nz5
|CTY. .zfi e7'5FC5l!ph7ZCu[#u[W0h'qn_
Fh>u['_@Wx c
XYuYxW0\h>uc=:\ZCi_



fi
