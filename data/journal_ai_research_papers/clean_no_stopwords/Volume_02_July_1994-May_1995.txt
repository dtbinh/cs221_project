fffi! #"$ %'&)(+*, --.0/2113541361789:; <=,,5>-1?A@9 %&<B.0>-.CEDGFHJILKMFNPORQTSUOVHJDGF0DWQX+HJY[Z\QO^]TX_Y;OV`bacOVDdXe HJfhgiOVj0NVXlknmnHJYoQpF0OVHe jqSrgiKMDsitou'vuwowyx{z}|~udpV2+=ff$5)J8dipA$+)0R0B)_#8A06ni$56)pAff 588A658B6n0685V5i586000[5AB66 05688$Ai6J$5PP5Vy66558+A#55A##5ffA6#6A+580)0o58 6!W5B 5A688V55B5ABA5_ 5A6G6 !5p5+65d5A!o;65i80 550665c 856558_Ad05A605A6_c6505AG6558)50o6255856VAp6 5A6B88c56AJi5M5A==5i5d5ff 50PAo56+8AAA5882RcP6 R056V 6)550BA#5A6A88;oJAJ5!0c656ff0i6A'[8rA5 580P;oRo86UA$A505 580A8A+8!i !0V0 5!A !o5J05 580oB685[5506858 6!ff6A!o5U5Pff56=A56A;6i6ffA Pi!A66!5nJ 6$2;8)566856A566868U06ff8 P05fffi8_"!$#&%(')+*",.-/ 0 12,43506%7,.'0489%(*:!',;!=<>"1,?;-=%('@)=!-=A*B!=<C-/">"%ED 06%-=A%('F>G,.AA(%7)=,.'04,=H;IJ3506%7,.'0489%(*K-/">"%06LA(-/ A78M%(N!=">"-='O>P<Q!=P",.*G,.-/ 01R,6 *C%('O>G,6",.*G>G,.S+%('504!'*G>G -=%'F>T*"-/>"%(*G<U-=04>"%7!'V"!=W A7,.;*YXZN[.\*]^-=A(*G!_06-=AA7,.Sa` b/cdfeUgGh/iQc e5cNj6eUklb/gmndfHop'q!=SR,6r>G!s-=0 1K%7,6#=,t>"1%(*r-=%r^Y#/-/ %7!LK*r-/"!-=01R,.*u1-n#=,WN,6,.'_>G %(,.S^P* L0 1v-=*;D A(>G,6 %('R)9>G,.0 1K'%(w&LR,.*xXU;-=%'A78y-/ 0u-='SsK-/>"1s04!'K*"%(*G>G,.'06%(,.*]rXUzx!'F>"-='-/%{^|.}~$Rzx-=0 OP!=">"1^|.}~=~Ozx!1R,.'KSR,6 *G!'^WK-=0 O>G -=0 ;"!ff04,.*"*XUY-/ -=A%(0";IA(A(%7!=>6^ |.}=/ffff|.}==&N,.*"* %U,6",=^|.}=} ]!=%( !n#=,.,.'O>"*>G!V>"1R,,.01F>G,6,.-/ A^ |.}==&%('K*GWN,6")R^ |.}=}=& J"!* *G,6.^|.}=}= ]HT>"1R,6T!="V04!'04,6 'R,.S+>"1,01-/ -=04>G,6 %(.-/>"%7!'r!=<06A(-=* *G,.*T!=<N!A78ff'R!;%(-=AK"!=WKA(,.;*6^FW -=*G,.S!'x>"1R,* %76,!=<J>"1,.%7YSR!?-=%('*MXXUR",.LKSR,6.^`6Red.j6eX,.01F>G,6n^|.}=}= ]Y!=Y!'>"1R,*G>G L04>"LR ,!=<J>"1R,04!'*G>G -=%'F>'R,6>P!="|.}~/ ]^A7,.-=SK%('R)r>G!>"1R,?",.*G,.'O>"-/>"%7!'@!=<CSR,.04!N!*"%(>"%7!'@,6>"1R!ffS*B*"LK0 1t-=*>"1R,x`6`.7j6,.0 1O>G,6.^|.}=}/ ];!=?>"1R,teUg"j j9`.&d4ej4gfiQcOX,.0 1O>G,6,.-/ A{^|.}==} ],6>"1!&S*.Hs!=",",.04,.'O>-/ !-=0 1@04!'*"%*G>"*!=<>"-/ff%('R)%'F>G!r-=0604!L'O>d6j6;h/cKeUi{`:"!=N,6">"%(,.*Y!=<P>"1R,;04!'*">G -=%('O>"*;XU-=*!=N!*G,.S2>G!d4eUg`neU&gGh/!=VebRb/Eb =i{` h/"!=N,6">"%(,.*B!=<C>"1R,+'R,6>P!="]:>G!@-=01%7,6#=,+-/ 0504!'K*"%(*G>G,.'048,43506%7,.'O>"A78+<!=*GN,.06%ED0:06A(-=*"*G,.*Y!=<J04!'*G>G -=%'F>"*XJ-='Y,.'O>G,.'R"8ff0"^,6#ff%(A(A7,=^ ,.')R^zx-=*"%'%{^ |.}== ]^R!=P>G!01-/ -=04>G,6 %(6,*G!;,Y>G -=04>"-/W A7,Y06A(-=* *G,.*l!=<"!=WKA7,.?*X#-=',6,6+,.01F>G,6.^|.}=}RK%7"!LK*"%(*6^ |.}=}=&N!&!=N,6.^!1R,.'^=,.-.#=!'K*6^|.}=}=&,6,6 ^zx!1R|.}=}=& #/-='|.}=} ]Hl@,",.*G,.'O>>"1,.*G,",.*"LKA7>"*T<L">"1R,6%('+>"1%*CK-/N,6.H!;,Y<",.w&LR,.'F>"A(8,.'04!L'O>G,6",.Sr04!'*G>G -=%'F>"*T-/",Pffc `neUiUb=c h/O04!'K*G>G -=%('O>"*6^ff<Q!=T%('*">"-='04,%'?N,6R>"%(SR,*G8&'O>"1R,.*"%*X{-='* *G,.'^,6)=!L^Y!LR)L%(,6.^%(A(-/",.r^-=*G>G !R^|.}=}/ ]!=%(', --.o %% !<6<TB0Pp!fi:yl9! %&% &0%W%0 <$!'*G>G -=%'F>!=)%(0fi$7J == =;;(J=?.OG.R"ff"M6l={7 .==lx=OM4GG =(OK=G.?G&"G.;J=(=/=.GYK4"7= 4GG =(O"6R(6(RBQO@UP= (R.=ffn @Uu=6 . (G=ffJ=R6N=R..===FKnN@R"6.;=R.KGa=(RR.==(KN6. PG"RuK4"7=4GG =(O"ff.(/"7YR6u ff".G.O/ fi /""(=U4"7UR="(4RG/" "=M= ffY4GG =F"ffCG6MG.4"7K&(M=&x="".6(".7=PG"R2R"N6""7.!=._G@R.&%'$%()ff ff*/,+@u="/K72R-Oy@Rff7ff6=C4"GG.4=#"$K/6PKR6;G4K7"76(M7ff6=4"GG.4/G=OG6.;R+.R(GG.4r=G(R"(M=;/=.R..= 76;G0/ 12/K "63P+'/ =4G6 546x"R G6=JR6/(/K7.6 6=(7.879:%ff=KxR.=G.<;>= P.uR?K/7RR6)P="(@K!==C4 (GG.O6RR.r=FV4"("G.F(KG"=F"/"7V=A;6=rN((R./7.&G.KR.+G;MG/(R"(&"?&(.rRG"=O"(/"7r= 6Y(B;<C9D"FEGH5MIKJ-J(K(LN"G."G.YGG(=(6="N6""(.C="Gff6(/G.8(M;+@R.=r:K4"7=4GG=(F"f(FG &K4u2ff6RffvQ=+G!ff(Rt=O_U4"7=._&_R.4N"(7(OG+)P"RN =K7.;ff@/KG6/ (?+4"(GG.O:(G"=O"(/"7=R O=:G66=K".4.ffG.(RK(:K/""(=G"=O"(/"7+G5"(R"7tU(uMK="&G ="O"6?=R6fV;=QPV67t67./7@G</24"GG.FM(""=F"(/"(=5"&=G66#B".;=.FNR.F"=F(B(""546=RP6RffRP3 .G.FSR6"P(R6 6Q="P=(="P567.O"(K4:R:"&=G6(T="?=("!46=R=R6r=9N.4MU(_GVN(F5Rr(5/6YK(7=u"V=R>P="s.=((RW74GG=(F"RB('0 G."BG;;9N.6Q/ fi"N6""7..(:6RfftR=7xXK(7.YBR.,EH$HR4GG =(O"N""."JR!=.ZK"N6"r=[K"N6""7. Rl=(G\BR.;K7Z%ffDZl=R.RRK(]KN6B(:="=K!46.t=B(7?ffBG.4"(2^/KGB(OG"&K4.R;K= (MR/ 7"(:=R="/"7K 6/ (RtN6\B('_Y((JNVG.y(tV(7?(RR>+@+R.yR/ RVK4"7=4GG=(F"=;"6"` ff&7K#= M=K;7"J .(/"7#Y7M(# 6nR(=G /"J=NRG.4"7U .G.F"R="(Y= G&6/G.>Y7U4"7K=4"G =(O"6 (/"M;=((Vx(= XR6= =a+@R.9(FG &K4\K!==:4 (GG.4u=K=cb?d1e(fgenY=7== 7=(7?(R5G=K7ff=CRC(/GG6J(MG.4"72h&F6="@".G.O"(R(MG.4"7jiR: 6 "7."&NK7.:GBBN6&K(J4"GG.4=#+@L/=(76"NG;6R&;K=G.;?RG"N6""7.=G5&(.4NG.5=K4"7= 4GG =(O"7;=@7Rl=("7=RRK4"7=4"G =(O"k`lmnpoXq)rtsur)vwxnyr)oXz{|g}`~S$? ~uxKuff$ E7'*;N6<$%6y3 8ff*5<>'Y:`F7Yff9?[ffffff-SIffffff-SN>%F%ffffj%]dpD2E &%'B`F7MEpffS3-y'tffffff<$%effffff?9[ffffff9:X^$%RF\%ff@\%LdE7(EgH5(%J&%ff]9FR"F%%'GH5MEHN%Bff7ZE7'KEyffH!fi:tRF%'5-M9F^H!E7Kg%(RpD2E fi$%'EHH!jf J%0FU%ff2U%, &%(79E X%? ffffff-yffffff-xffaff70FU%ffE_<%($D" HKff*?E' &%7E Xxa:t9M NH- g^E7'KEyffH!%L@BE fi`$%6y )pJ$%Y&]%ff]F7H!E &%6E%%ffffKE)2#^F6 &%(79E X%3`&7RFR7H!E MX$%&Y%'X%ffff#]FZ#E7()%'KE j" 79N-SX6%G"Nff (* gZF["FE$7'%a9Dj&NEH$H*2D"FEpKffH!EHN%J-?fi[XXU@XX'5FyXy0X8`N y&5 &XZg:,'X2c<'6Sff5!fi&R&(9X19'X]ffjff59j!fipB&ff:&:ffNN2&fiff'Y9x!!UffN9gR&ZX!aNNZpN99yff^N&ff!&RffN9g&RpX995gBN8pX599N^XXNR$p!-B&jfiffF !X1#ffN9g5&U&M55pNfiB&8xffff5UxM& & 2&cN'$( <9'XgZ'( K&\GZ\G 'fi!#"L'$%$'&A(p)$*<'+,$.-/fi0+1X320465c'78$fi(9(Q?8y:%F7(G_yB&)W,fi!. K3fi$'; =j< (?> y(@B( ]F 7( @ 9- 7j,6$ffC D29<%Q 0E!9gyGF . K3G0#$B6Gjy^GH"L(6-0+81 %2I$J<fi&#'Bj)KG,82-.LMff B@;< G, 8-2:N g $V(OL Mff \$ 4Pp Q p.30 XaG8g0GOc5 '7SRT<&Fy3IfiUM, 6VDW@> g'$fi)K2), -2 YX &N$L$ (:VCDg ( \Gjy^GM)ffVZ<F [-0+8 1 %2@F,Y2=ffY03B\]^2RG_4`1BF''a<` a(YbVC3G2yc/$7@`Q@d$0`yKZ7,4 O0`yKcffVI$#0#?0`yff_F':8$e2? 0#$gUp.p8F7Hf(MfiC '\),2^B!g&-=jCfiBjFXDBZRt\BW -$'%$:(MBj(:f (e<%fiF\C Bj'F X3@g'W <GNNX5F gihjF xp96FX35X5ANRX@pX995gff63NN\a&:xR&R9ffL`&]X\X!5lk,mino7prqts8uvo3w=oVqxsIuxvxyinzew={oVqts8uvn{{xq(zivoVqts8u=|}rzJmin}EmUqxsI~F g_r8c4B lk8D]R`eWQ$[;Y<ZRV;WI$_>y(]RD>(WQ~" (3$[1& (p ,$H<* 'ygpG)$[-0+81 %2I$'5c'78~_MIc k8L_Ur88)c k8xL 9 3fi$E;#$GrXB$X F,~t8()B[8c k8xL :L$J.$E$'" %~N g $$X FX'~trI'=B lk8L Mff $:&!:X\N 25NkI r8c4QQ8MIc % 8M8^BQ)r8U^B % r8c4BQ4r8'=)r8U^BQ4UU8^B % )r8)cQtrI'=B ~&ff2&ZpN99ya9x!!0ffNya&ff!!a&ZN5&a5,6ffffX'9UF5!ff&ZNg9!6&ff5!Rx!p&9&`&^5&N fi9x c&2X 5&ffS&NNff!N 9W5&^N y9'!ff#NcNZff5[5&N 6&^N y9'!ff5Nff65&ZfiFff X5ff!LpN99yfft[ffU&fi& Xff!BN<&\X 5&@N5pN995g55NxY5N)NZgZ&Yff5!YXN [&]N y9'!'5!9Z2&YN 5N3p^&Yp&XNNN^&R&Xff!S&\N g9!p8&R&ffBXNF gF g&R9ffL`&]X6pN995gL5r8c4BQ8M8^B lkURV K3fi$b"B(WQ$_RV K3fi$b-/fi0+1X32%WI$#R`;Y<D$^"L'$WI$R`;Y<fi$b&A(pWQ$RV;Y<D$*<'y$pGWI$.RV;Y<D$^-0+8 1 %23WQ$RD> y'$%$^-/fi0+X1 32%WQ~; <fi$bL Mff WI$RV; =<D$^:N g $WI$Er8c4BQ4r8'= lkUR` K3D$^xL : WI$_R`YRD>y(3$cX&N$WI~&]X\ff59<5!pXQIfiI4d4U)B8^B)8cr =8^73: c%3 ^)r8U^B%QIfiDfii:=:fiQ=%%QKfi3`%Kfi=:Kfi8 V %38M8^B78M%Mc33MdM=T:`M7KrI`(B[d,:iJ)MM%^e%()OJ)%bMM)fiZ%U'i8:I^BQ)r8)cU`#3c%^tMQY fffi,IO8=,!"I#$&%')(*I +,-Q . /fiD0 fi8 /1B2 354)r8)cQt8()B[8c lU`x67D^t:ffQY# 8"Q_5!. 9Q31B2,^:D;54)r8)cQ r =BSlUVtM7fibx&5QY#=<"/>c?I5,=<"/>c?I31B2 ,231BU^7?54@T:):6b,Pfi)MM3bBAtDCJFE%bAHGbJILKYUMINPO0Q6KRNTUWVYX7Z 3G[G#Ud%H\E3Gb,]::,:`:G3G=Gd5CJx KRN ^] K`_ ] Na KRNYb#:,3G[Gdcb Ud3fi3fi>e>d3V0f ).):d b 1/1,bJ:?G^MMH:,Ud3 :UgAt?Gd,/,eMH:,:?G^:d)`CJY::)hE83 G^,$ikj U ] jHb. b 1 1,lO`Y:,U4%: X BE83 G^,miK U ] K`:)ffiUMM%b=nQ K jb) Gdb:Kqpeikj XlU K j a\rsGbM Z :,mE83 G^,mikj U ] jbZ%,)t:# :,(dtiKqpei)N XlU KuNfb)H Gd]d`ikjYAt::,U4%: K'c:,Gfi)MM3bsQ K j/)ciN`#:,GfiUMM%b=:Q N jv M:)Uc :!A(M"w Z Uxvj]:)%M:Y:, MYi:,zyU%MY{|E%b AHGbLw ZC}a ? & a6 %,/ ~BHGbb,Pb :,ZfiM ~B& f ,],Vo GdG%,YC_[ G[GAtzIl?pIp&&&5pI&pI"HarsGbM Z :,Y3,#`#JtC[ G[G6Ath"30+)ffk7 )"3" )ff3ffffk ,""3Ha[ Ev p&&&pI 4 %! AUM zw Z :, ::do3E83b AHGdPILKBvHGE83 Gb)K ] K bZ3 G[Gd 1B VV > )a HV73 V0q v aB)3 b)M:=:b:dp&&&?pei Kqp&&&pei)N/p&&&?pei X b fi):bMM=idx) HGdHdx& E: fi):M%b= b)& G^),bBvH'bE%:bqyU=c73 G[Gd Z6 pDzF[? & aQ KRN U|VtZ K pei N XU KRN a, ^)M:)fi Z ,d:, s>Kfi// ,2fii#3c% 31, / :fiU:bMM= Z C): 2 s>K0 7DJD^; ) +3c% #$&%')(H*IJ:D? &a8::3rbb/:,:`:,(,9f?fi):b=fb):=:b::wPar31B3)^7 ?&> V0M/ Gb):d Mzw bi:=U 2 s>K0 7D'D^3[tM DxMte &?)?fiLHH|HH7[, H )Hc`o ![!H-"=[oe0:e0)?ffD e2"77tm7)=e?2H 6) +))H=`/!Yk-z,- 2fiff efi 3!#"$%&(')"*,+-!.!%/*0+13254768 9:;"*<>=H?@ BAo=$!n(CoD "Eff&GF?HGI$J0K/L/MI$NDOJ?MDIQPRK/S@NDL#IK#P3T `/!e}Uoee7 V ;Lz!W["efiff?Cfffi@ 737 RX?Y[Z6-23e\AHff&fi!e[D AFL3^]$_`<a C!Mff&(z3D$Cb C!&3cbefiZ-3Eff^Coff &7ed?!oe& fcg!ffe^cg23h C!)J- C!GA L>He:CoVD6&0_2546j ;!#"$k9ml L#nDo-IpK/qrMsnDNDSRL/N7t-OeoPUuBvWNDI$w5uVxyEqzo\w7oI*MKfo{uVv}| uVx~L F-MDSiNO;Oj7vUL#I vfyK#Go-S@o5L9PNKM:PKEMDI*o{xsL#IjxP?HfiJ-K#NKET/vxX{ v xogK#QoIpw7o-I$MKfoWxm v xT/vfXMDSx` v xT/7vXfL F3K#7L;PEn:NDOHoWoL;PRK#P?y,MK#Go-S?q8L;P-o0J^MIQPRK/S@NDL#IK v xgL;PB fififf e[o8L puBv|u[x5MDS[uVx|uzv@h?IK#GorF-MDO#OMDq8L#IyqzoEq8L#O#O,J^NDO#OQuzvK#Gol3[/NDI*wUuBx5K#Go e3- WM@F3K#GozFRHI*JK/LMDI$NDOjJ^MDIQPK/SNL;IKju v |u[x!e CH{A!=$oe e[Y+5ff&A!Aofiff aH e eC!ifffi@ 3W- t( C!\-ffzC!!CoAsVY, C!- \r CoW2fiff eojfffi@ 397 -Y~ C!t C!&3CofiA~B:Y C!J C!&?T# C!!fififf e[fi@ 737 RX?_gm-x9Ee3As \Z62fiff eoe"e 3ffe3fiPMDo32fiff efffi@ 737 TVX?YfiAPK/S?LJK/O3FRHGI$JK/LMDI$NDOee MDIfiOffe3fi3fiff^C~ff@ 3&UV`X?_9g4&#%Q!4".:b C!5=A Efffi@ 737 eD e@ ff eoeZ-Y!> C!\J BW3 ezZGff ee3hceAHfiffffWHfi eD eeJ_gcs@ efiAo[>Co3Z6&ff&eA/fi E ff^CH-3 Co9T/&23fi efiffz Co-&eff3 Co"97 eGAofiff e[} CH,D6&X?_ hCo-cE&^ze sAofiffYJ- e9JA! eff&ecYj C!i eJhekoeA se3\-fi-_l5-3&YLZ eff-g=ze 3 C!?tff&fiffc6C!-cez3!&7J- C!GAfi-YAY3fiff^CY, C!-csA!! > eD]3B97 cff-ff/fi3 Co@ff&=$ff&e e[lC!>oZB WZ>e3AYfi! eDZ$ecE_u0_ -_VP-oNDI$K/L/J-P C!>fffi@ 397 -_LhCoVA!=$ff&[fiffc"03&?ffJWZc\)[/fiUTAY3>J@ sr Co5Yff7 ?Xlefie -_shC!-ccQCoeZH (JWff-[elfffi@ 737 s(CHff^CH eff&[He& e-_shC!meefi (7 g D5ff-[?:C!!gCofiAY C!goeeoc33e Cfi3@ff&=$ff&9ec|AHDfi azff-[eY(ff@ 3-YJ3eYff&h Coc3o&337e Cfi-Yo5 C!} Co&CofiAYQ C!)ff?Coff &d?!ff-[^3 ff eDZ$ioeZ[-_(Co}ff enC!$@ lz CoHD&YHt:,eQfi) CoD -Yfifi[" C!&e:9>@ eD AY Co3!- D3]G:e>Zoc_:fi(!*efi{^@E-^E79fiEQ@^fifi>(fi?sD@WVQi--W//->BfiGh@@^97[@fi@E@-$}fi^^Q-QfiWgD^3fi@@$@-^Q-QVfi ,rfi@h@{WGfie#)(zU@U9Wfi!(eg-efiWa^fi##*^fi#\ D;-^fi#\@@^-khQ-fifi9W\WffGe(B#fi !BiQ-^-@fi@ fi GV#hQ@3*-e$fi@@^-(BGUQ-D^Rffi@fie^fi59fi7@7@Q^^a-8?}fi,(z i>QEs^efi >e>fifi^DQ-!Wfifi-E5WGQ (e--^fi" ez*-9 #Defi-^fiQ>@3Qh-@@@^8@Efi^h$ &%' G$/ (D$*)D/, + ' G$0/ (D* W (D.(/ (D/Bfi@@?710Q8DQ@r@@^^@ee{@fiW@ Qzfi fi 2 } fi/ 34D;- fi / Wfi5#* fi / {G@^??z(z ^fie-}D^Rffi^@@fi fi GV#hQ@3@@^-fi(^9[G^@@h(eiQ>#^W-(fie-5hfi@fi-Ufi)Q@g@^fifi9Q)(fi9^9Q7;fi@@^97U#)(^@We G-jfi@@\}G-e}@gD^7@-W@@fifi^*3eGQeD^eV@@eEW3-h fie-D76z--p)fi0(3fiD3Dfi@fi~fifier@@fiseVQG@^hD 89(:; (D=<D? >5 @DA6B--fiCB}^@-i:EQ0 fiDEeQ@@^EDDie9EQ~@Q, DGFHfffi@fi5fififiVfi@fi5I@ ^->JfiDfi@@^Efi7$KV--L:?NMPO$O:9@@^-4Q*#zWez:D?e4RTSVU9R4W7--:fihYXNSrfifi@UQ-MGVi,:fi>ZX W QsQ-ffG9@E^efi5 -e\[^]`_ / a*-^bcQ3 b:};?fi^eQ-3$fifiE@Q,{Q#^fiD{Q\QeffG3seGQ9jW/We fi *, deFfG@@?gQWW@Q!(fiDQ--^{fi@@?7W-@fiQ-[-^UQDefii^@@^eQ)Qfi[9ihkjfWe-@mrfi@z)eGQQe?efi @{@e!n`opfir(e\;kq./?/ (D$G\l efi9(@^3#(fi?Q#ZfiD^fi@@?7fiDEQ3^WErMPO$O:fi@@^97?s0?tvuxw y3z*{|y~}y3v3&rz5~;{VY4{9559V ?VLuu=z5}x5}}y9/y?9vz5/y.*;{}|;z*x5vz5{`3/|iiz53?ffVz/3vZ;z5}?L;=Y/}/z5{V/9{*}};}z*{1{Vv}|;z5/}v$VJ;/{V{}z5}/3{VJ5v;/z5/y3i}x3/?z&$1N${z5}z5v$/{9$3/y3v}1{i5V$z,z5}1/y}/;i4}/y{Vvi5V$z,Z;=Y.ir"9$u$u. z5?Z{v3}|z5Y 4z*}./?{VvffVizYi$!$5z/}`}3;/}1;/{Vv}V{3/z5ffz*~^$3/?z*39/y/V$z5}/};z5i5z5{z|$V/z5;/y4$vZz53}uu.g3z5;{}|z5 z5}Jr"/z*yz5vV53i z5}J}3;/9z/yVJ;i}Jv;*3}z5ivL53}1 PP{vff/}VffV5J - z/yV4y}4?4iv}}3;/}z5 4z5}}3/9rvL5};==uu.`yL$/yV/i;/J}y3?ey9e/yz5}N/V}35z5}N/r3z5?3V,/};!y}T{}|;z*/}y9ff;/z,kT;N5}};/;Yi}r"/z*yJ";?,?r{v3}|z5Nz5}Yk"/z5vyz;*z/}3z*;r3/?V{V/z5v};/4r"/z*y3z/y34rz5}Y}/5t?VN-t? t9{v3}z*}/V9$/yV~zYz*}*;*~{v3}z*}/V9u$u.;/y3=}/;ffx$}z5i5z5{z,`xz*533/}3z5?Nz5i3*z5{9?/z5v;ff{}|z5/}Lyz5{y;$VffP;5/z/yz5}3/}/9$T;|;*55v;/z/y5}i}/$z*~z5~yz5};V9u$fiJ==T==";P=Pe=x.P".=i7=9ff^-N`^Y?^9;?x?r"?"x ff^x99;P?"r;1"?1"?x^9??P9`ff1fixffrff^=99;"???/^Pfi!"$#&%"(')fi`* ?,+ff^-/.10"P??54ff+r-9!`i9;ff?=076k "ffZ?xJff^x99;"?""98):<;ff=;>@?-2 39ffrff^x992 4A#B CCD$'4*"FEHGr9 4IJ>@?LKM#B CCD$'?Zff?x / (@4ff^x99;"?O N,0QP??xRSfixxff ^xNff^x99"PUTWVYXZT\[]"~?xxff??O "^ _/ ^Pfix`!"ff#9ak/ . ff^x99"PMbdcfe"gh"Ii\ ^;aD$')fi==xMj Vk[ ?-"=i?ff+ ff^/ .l#,?x^m^j<[VZ gx`, ')0 2 ;R<"m?\ff^x 99x/ "7n +T 4+k^9ff^x99;"?? "xxff ^x-ff?9^o,=xff ^xff^x99;"?p#,9;xq ,; 9^r,x=ff ^xNff^x99;""9 ^o,x=ff ^xN^) ')0a24s5??x"MR K!?I# "=ff?x??-^?,+ ff^!/ .ff^=99;"?) ')0Yixxxff "^xxff^x99"P?fiPxx^ ^4J*3ffZff^=99;"?fi;5"P9ff?="t90ff^-t" -?v" ;?PC"r?^9??x? ??x- ?xw>@?(?4!?u xK9ff^Rff^x99"P?!J?-xn +T 4K`^R9t$^A"?H#ffKyx-K^fizt/ ^^PfiJ!"ff"{ fiY;ff+ ff^/ .|0,00)')fiR^x ! * h"?R"N90 u WRL}`?xfffiRSfi$"^xN "R^fip"9?9 "mU" ;fffi"?=!?-?;x?`+T ?9F"e?*" "? R v+rC^x^ ~=@H?ff^=99;"??hxxff "^x{ 0#~ff"fi CC$')fiaI?ff+k ?x" ? ff^x 99P| fi!;"^=ff^x 99P7x9? " ^`/ .9*99^"x ^]fi,4^<nx9? "9]) "=fi??K/ .<9? 79xffKh,=xff ^xi *39ffff^x99;"??^#!R'x9? "9"=9?R"^<n"x??P "9) "=0 G?=?!i?H".h"?xaff.Sfi?x?x4*?ff^xO "99\x9? " ^T^ " / .9Rx9~9^"x ^0~ff?x"^ fi ?ff^x 99x/ "~ff??O .~H+T@4ff^= "99x/ fi(=$O=@;R~)~n/;|@fiS"~?<fi1+Tff + ? ^99?xff +h01^J,(*a|,,*|ffH*I(,"ff"dRJ`1h"Sxxff"^x{fi,"?Yff^x99"P; `QR^"^x 9"P9kn+T~;=9?0YWZff^?"x?#R;ff9 ';r? ?9"m?hxxff^x1ff^=99;"?fi*+k?! F"5*?ff9 A,?^TV9TWhL}ATWVXTW-fi *tZff^?" xJ?~ h#{K(ffK^fi xRR" ?ff9 ')fi!,^,x=ff^x=ff^=99;"?09F_c#()WUIT' ?R" W; xfi R1hc#(v)W-'W ?* ?ff9?xR ; 0ax$" R,x?? fimT+ 9-?"9^h ?Ox ^x?x!/~/);$>@;2 ,?F?9/.#(O" =",')xfi R?q= =YHR ?ff9 ; 1;ffRxw=q8T V (;>@(qO nOxq>)*A-\J,(1^r"*v-< //nTW~o>w#()W"'F (<q$~/(~a(;vWw>F<>(_#(TVqT7[/ffqTY[@ qT7[{!qTW'F~):R/(">$YTV\XT\[O/ffTY[ XTY[n ffqTY[ XTW|a@ $K//nTA~> ?t=q8H>offnIx$B>)#()W!' >@;R=(<H@nOo~H>!/~/);$>@;8B=@T@Kfim+J9ff.xq= =~ N]R;ff9 t; 0 5"Tq 9R^xi?xZB= =$S"i2 / .9x ^S?r95+kR"; ff^?"xr-"m^"U;9/.|0> ??]=B8F>@(qO nOrx$B>*v#()W!'9 >@;R@//nt=B8-\J,(1 ~:R/~ S=B8Z~F_~r>I!/~//;$>;*18)q=@>@;l? H/;*=B8MoKM~aO>@?(? O-R"-Q (<qH$=ff~r;=O~>q= =~ Y~):R(">JHoKW~aO>@?(?^R" x (m/p$=ff/~F;=5$~/>Hq= =~ JH~):RA(<>5 " LKv{nO J$ QB&Y FkS fffi,&)@&1m&R1R!"&#m7&|$R"%R&')(+*fi,-/.102314!56567893):;7=<S!T 8UP4UV7=<314!56567893):;7=<3ONP4!89Q!5O:;7=<S!T 89UP4UV7<3ONP4!89Q!5O:;7=<3):RQ/:;7=<UP4):XW17<3):RQ/:;7=<>?A@CBEDGFIHffJKFL&MUP4/:W17<>/H?A@CBEDGFIH+MZ"[]\^I_R`bacEd=^OeO\f_&g!h6ijgfkOlm_&nVnfopR`qornfsPtuwvxEyzG{$|~}/zVz|~}/~yfV$z|})9yff~|z|})m=$xyfx6|}nf_314!56567893):;7=<S!T 8UP4UV7=<zVz|})~xyfzG{P|})ff~yfV$z|})VPExyfxO|~}/P9yfffff|Ez|~}g!_R`enfoRijnhg!oR[e6`fxEyzG{$|~}/zVz|~}/PExyfx6|~}/~yfVffz|})9yfff~|1Ez|~}3ONP4!89Q!5O:;7=<[]phO_R`O=`+ljee6^Io[o[;pkOnfonh6g!oR[]e6`fc3):RQ/:;7=<iOgfpkIngfk6`+ponf_[]koRiO[]pnf_&lI`q_&[]kO\p&^O&i#oRiOg!o=$xyfx6|}=$xEyx6|}UP4):XW17<xEyzG{$|~}/=$xEyx6|})~EyVffz|~}/9yf$~|z|})zVz|~}[]pkInforAnh6g!oR[e6]`fcO[or[]pkInforh6_R`O=`+ljeVZ"[]\^I_R`IcAnh6g!oR[e6]`gfkOlkInkmAnh6g!oR[e`nf_&lI`q_&[;kI\pnfsPtuwvZ[\^I_R`a[]]]^6po_&g!o`+poRiO`kOnfoR[nkOpKnfsPlO[_&`+o`+lp&^IeO\f_&g!h6igfkOlnfs$_Rnnfop`qoiI`q_R`fIgXO=_Rnnfop`qog!hOh6;[`+lonjn^I_`Igfh6`tEuwv=nfoR[;`oRi6g!ooRiO`g!_[]pnkO]lO[_R`+o`+ls_Rn9yf$~|z|} OgfkOlkInfors_RngfkOl9yff~|1Ez|}9yff~|1Ez|}9yff~|1Ez|}~yfVffz|}~EyVffz|~}oRiIn^I\iK`bi6g+f`benfoRi~yVffz|~}~yVffz|~} $iO[]p[]pnk6snf_oRiI`pRg!f`AnfspR[]h6][]q[owcoRiO[]p[;kIsXnf_g!oR[nkj[];1kOnfoe1`^6p`qs^6~[]kjoRiI`sn]n[]kI\I[oRig_Rnnfop`qo1K`gfpRpn=q[]g!o`&!=f!~ghO_&nfh1`q_&omoRiOg!oqgfke`f`q_&[6`+legfknf_&lI`q_[]kI\nfscmV"X/;!qV1~B#B+B!qqq+B;b!X]Rnh6g!oR[e6`A!B9BEknfoRiI`q_EKnf_lOpqoRiO`r6_&pRo)g!_[]g!e6`+p[;koRiO[]pnf_&lI`q_&[]kO\g!_R`oRiInpR`iO[;&ie`+nkO\onK`oRiI`+kpRg+oRiOg!ooRiO[;pnf_&lO`q_&[]kI\[]pRPReAmOgfkOljgfknfoRiI`q_r!g!_&[]g!e6]`[]kjoRiI`bnf_&lI`q_&[]kO\m B9[;pnf_&lI`q_R`+lje`qsnf_R`B p&^O&imoRi6g!oBBBiOgfpg!or`+gfpRoEnkO`gfk6`+ponf_B9p`q`Z[\^I_R`Vfffifi!"[fnfonkOpR[]po`+k6lI`qh`+kOl6pnkoRiI`gfpRp&[\k=Z[_&ponfs9gf]imlO[]lE`bqgf]ff[]oE !f&!OX1q$# &%k`qf`q_R`+kVonf_&lI`q_&[;kI\po`qhnfsEoRiI`gfpRp&[\kO`+kVoqffoEnm`+`+`+kVoRpiOgf`one`lO[]poR[;kI\^O[]pRiO`+l~c6_&pRoRfoRiI`p`qoEnfs~oRiI`sXnf_&`q_&[]kOpoRgfkVoR[]g!o`+l)g!_[]g!e6`+poRiI`I!6GfRq+;gfkOlp`+nkOl6oRiI`'!(*)!+-,.+-/10324/658749.:+624;=<>74+19@?03/A0B74CD<=9@?FE +-EG74;=?<H5I03?:J0K:F03A70B74+1LK;M<=9K?E +1EG74;H?F<H5ONQP;=<CR74;H2ffSUTV;W0324/QXZYW[K'@']\>(^F_R^fi`badcefgheij@akj3fVlminmoqpFertsuviwnmfxakewixyz&g.{Z`|j}x~]V|F3Q~W~Ik}vWB}ZBkW~ bI}v~DR}]$}v3kx~R|Q|]$}vBkBW~}v]|vB~RB>~Rh~R~}~3k}vWB}ZBkW~W~R}vBx~}x~]VF3Q~W~k}vWB}ZBkW~ x~3Bx~W~W~RBb]$}vWW@k}mDv&BBkRk8B$Q~W&k!whBx~]$}BkWW~}v]t@x~BVDBx3}$}QR.8vk]$}vWW3k}Zk|Bv8Rkk~Bx~IwF$ Bv~W~RRF*xwvx3Bx~R}BvkW~]BQ$} .h~kk.k}ZWBVvv]~Bx~x~]*}vQBQ$}wQ]$}v3kWW~}v]k}BW~RR]$}vBWW~}v]~Rh~R~}F@kQ~RwBx~}twQBxW~RM}vt*}vkQQJ]$}BkWW~}v]Bx~ORZ w h>WW.Kv] m3 O8 * |Z]$wBQQ~vGw =.v- K @$VK&]. * Z|KdFGZHBb-FR4GZ=BZh twGBbx$xD]$~}ZB3W~3$Bvk.x~]*}vQBQ$}x~O~]xkWW~}]~O]$}vWW@k}mD~Rh~R~}qbb}vMkJ}xD]$vkW3w.Bx~B~kBk$}-&~v}vQ~R33J I}v DB~}xb}x~]~BB3Q*~R3~}mRH}vR IOvB~8h@3JV$G R JQBQBW~k>}BvRW~b~BBk&kkW~Bx~QGmHBZ48Bx~B~kBQ$}v8WBx~Fkv~Z.8Bx~v$k}=vkhx~]w}vQBk$}&!&~JW~R~}|QVR~R@BQ$}WW3$}x]$}BkWW~}v]q }v]$}v3k=W~}RQ~@@]$}BkWW~}v]*QMkvx~hWBx~|B~B*~}ZFkv~hk}wvWMv!~8BxvB}tm]$}BkWW~}v]*}Z]$}vBWW~}mk}vBB}mBkBk$}|HWq&~~]VW~}vx~WBQ33kQ~xx~R3~|m}x!}xBvB]$}v3kWW~}v]~vQQ~}ZffW]$}vBWW~}v]$h~.R}v~vv]~hBvB]$}BkWW~}v]R}~B~R3QWW~}k}W~R3&8|]$BQwkkQ>wbx3v@K w ROx|>WW.V KGwhHW|mw&MW|m3|$W3JVG4]~QB~xt3ktBvIW$~]$}vWW@k}mBIRkk~Bx~wFJ~&wBBkRvkBDW*wh~}xx~]w}x~Bv~Z wfi ff w R4|h |$.Off *bff|h hOwQ=ff w ff R 4| qff w|Z=@JVG4]=}Bx~RDb3Rv$Q~}}mvB~R8BvW~Rff *}v]BQ$}v]$}WW3k}Z8 |W]$&k}x$x B$ff bIff }|ffkQff kh}v$}vQUQh}Z]$}vBkWW~}Zk}vBB}mBkBk$} G B }v }ZBx~RF@kQ~t k} ff&8Bv~RB~~]xkWBQ~W$}x~kx~IZOk}Bv3BvGZHBFFR}~D~]W~}x~W]$}BkWW~}Z.k}vWB}ZBkBQ$}UGZHBB|>WMW.$$"!JGv}v]BQ$}M]$}vWW3k}ZBRh~&RkhhBx~K$ #$}vq.Bx~t4% # hBx~Q.| |m&(')'fi*+-,/.106:28:5:28767 57972435363H1M_`Xp1AMa/88;=<?>A@CBEDF@HGEIJHKMLONMPRQ)S)TCSUfi>@CBADF@HGIPRVC@HGXWY@HZJHKMP$VX[]\_^`PRVHNaPbQ-STCcS)deVfP$VgThL_d(Tji/ThKMLOkNal)PbmnVfiSU8opBfG-qrTFKalTP$Vsut B DFt Gjv qwsux B DFt Gjv ldayzs{x B Dh| Gjv l}R}pKalAQ-Ll~VFMNMNS)mFTPRdZmFLVfNLjcTFPRQ)L_}R`t Z qet Z l)dayx ZjvdOThKMLS)TFKMLjm8Kal)dayqAPbT8P$VpdMLjPThKML_m/NalTFKc_S)daVFPRVfTFLjd(TjqEVfPRdac_Lsux G Dhx Zjv Kel)VdaSVFMNMNS)mFTpPbd B qEdMS-m8l)mhcwc_S)daVFPRVfTFLjd(TjqEVfPRdac_LxGKal-VHdMSVfaNMNeS-mfTPRd B4;=<?>A@CBEDF@HGEIJHKMLONMPRQ)S)Tc_ldeyPRyalETFLS)U>A@HBDF@fiGIOP$VC@HGWY@HZJHKMP$VH[]\j^P$VdaSTNaPbQ-STc_S)daVFPRVfTFLjd(TjiHstMBEDnxG v Kal)VdMSVFMNNS)mFTPbd ZfiM`XMpAMe4E)rp1pEpX4EA8aEe]AM8{(_jufE8E H4EeHfiu-{8w {rE 4E{j`_ ~En {j` r8r_r]M%E1(jj1A8raA{e EEE Erp1"AM_ezX {aurneAue4 ff~8{(Mnfi EEEfinu"]EErr_r1p_/M~rEfi 81OMj1MO4E{`1_1M eO_Ej H1M p )="p1AMpe4EzpM81AMa41zAM/ 1M E`A1MpAMp_e4M_~pr_E r4E1Efip1A811M"!#$! r/--&% E] {u(]' EA1M4 AM_eE4_r_AEjE) wp pr4EpAMe*&% pp_/Mfi~E~AM1MOr+A/11M!#-,. AMp11M0/E_p1{O_1fi CEF2EE{81AMa*3. AMp11M54p6!71A_rAM_eCMEfiMMpEC8nrE9j ff;: =< pz p1O_e>?A@BfiCEDGFHI0JKHLMDMNIPOQLRQSUTAHVWXLYRQIDHYLZ[6J\]CffM^+_]`acbed bgfYaihjkmlffngoqp*rts6uwv*xUy{z}|g~{zffK1 Ns*p*srNqPs n*psAnr6prY6qQpner#* *** E"sqrYsp*PrY1 n rYsqNqr sqnerYg+ff]6n 0gp*qeqngr]+gQpNpNqrYng nqrsp *s ngKpQ K;~=#ff ff sqrY;K KneffYngoAffsqr"oAsqnsQp(~n gpsq #p*ff;sYngoAG# K;~=5ff;ff sqrK;(6 zsqr"oAsqnsQp(~n gpsq #p*ff;sEAqrwpYneoqG#K]rY"oAsnsQp(ne gPpsq]pff;s7neoqGgPp*rne NsqgpN+s;*gPpuwvxwEsqrY;ynKYngoAffqrnpr616]PU*P* E**gQP*P;uwv*x*#&&Nqg(*c5 w55pp(*6ffYngoAqrnep*r*A#pwNQ0+Pff*N7N*--*(U7*77]K1NYK0nggpp1KsYngoA&*p^+_]`acbed bgfYaihj lffngoqp*r suwvxyz |g~ N sp*K "|g~ NsqrUNqPs n*psAnr6prYqQpngr]ffq***ffsrY sNqr sqnerYp* Epff*sqyne]Nw ffN*n ine sneoqE*p*ffyh j bfd ;faffbd1_#adfi gf_*uwv*xUy7Y]* UqE66*{N+cP K6ffQ 0**6w**+NK Uw*gw**6 N;YYngoANrnep*r*A#puwv*x^+_]`acbed bgfYaihj z|g~ n;sqep"gPpYQ*gPpuwv*xy z|g~ ;n- q" !$ # %#c]EsqrY 6 q" !$ & %& +q(' U1EsrYQ# neffYneoqffNqrnprgsqrYnA* )gPp*p 1p(r,p +n.-YngoANqrnprUg#s5fi .-/ sqr]zi-0K*eNQ E;w*w5QE*wuvxyQK]Pg (6PE q*] *1K3_ 242 5h7j 698 pyz |g~ NpUsuwv*x 8p* &Uuwvxwy z |g~ fisqrYy z|g~ :p;YngoA NqrneprY sp*;s"YngoA7*p z;=< ? >A @ C B wfiG7zDFE E srY+sqrqPs npsAnr6prYffQp*ngr] 8 p*ffgPp uvxHG z /K G z JNngg+ G z JzL# G zM# N#|g~ G z Jq" !+srY(.Gz z L &OG Pz && - 4q" ! '=G 0.G :G4Q Pp*r(5N HneEneoqKqrnprE gPppsq6p *pgsrYsqrgQn7*sq6pNqPs npsAnr6p*rqQpngr]qsqrY *pNNqrP HGRSUTLVfiWYXLZO[]\^H_U`a`b$cedgfihj$kmlnk,oqprsl,outlnvHwyxk{zx]|}p}ln~Cpuk$xkmlmflflnk:qfqp}lmfN=$p}j,f:uj$x]fC}ff?Lukk,oqprl,outlp}jt}sl,outl. e7 } 347 }Pl,oqfj,fp}j,fw4 ?LxvHwqp}jt} gul,oqfj,f.fCqxkml,kxkzx]|}p}l{~Cpuk,xkmlmfll,oufj,f fCxkml,kxvHwfiflnxe Lk,u~o7? ?C p}j 7wOfx]l,oqfj7 ?t}t}uNt}kvp}jxvt}ugvHp}jt}e nffflxkl,oqf:zx|}p}lp}J7 }U(qp}jHt}7 ?l,oufzx]|}p}l.rYoupkmfl,tj,}flxk.l,oqfj,fp}j,fw~Cptj,f{~Cpukmf3qfl,]yxk,u~o4l,outllHxkl,oqf{k,tztl,x]]ffxxkzx]|}p}lp}/mvfl(ukqpLrPk,oqpLrl,outlv]"(wqj,l,oqfjw*e ]"*9 ]".wt}uk,u~o l,outl(t}ywk,u~ogl,outlwt}u}urYoux~o xwdgfqfu~Cfj,pt}uCUw~Cpukmf3qfAl,}.wwkmpwqp rfnout|}fUwt}uLffflww arJfl,oqfj,fp}j,fout|}fwRw wzx]fk.t}ufiUqrJfout|}fvHw.xkzx]|}p}l(~Cpuk,xkmlmfl.Lzx|}p}lp}vw Ji~Cpukmlmju~Cl,x]pp}t}LkmpvHwk,pnp}j,f}urJfout|}fkmpqk,xu~Cf.?wYpLroqf=wUl,outlRvwvHwt}uNv{vHwt}ivHwCvj,pUt}udgf~t}yqprzuj,fkmfll,oqfHzuj$p}zOfj$l,x]fkJrft}qpuu~Cffixl,oqf.=fxuuxup}l,ouxkk,f~Cl,x]pa^H_U`3_L%ffmu:,}.Ovvfffi $}q*Cff}Cfi/U$ ! ":C#AC $&%ff}C fffiff}C^H_U`a`b$c('qzzOpk,f{l,outlvxkl,outlJv*(qp}lt}t}u. ,53tUxtUqxxkHqp}luux3qfl,o3ukl,oqfj,f:fCqxkml,kv- ,zx]|}p}l~Cpk,xkmlmflqt}u:vt}rYoux~o ~CpAlmj$t}x~Cl,kl,oqfzuj,f|x]pukt}k,k,xkzOfjoutzkJl,oqf^H_U`3_L%6fi.0/1=v7}vpk,lYx)+*,vflukux{l,ouf.0/1:voxk.0/1Nxk(zx]|}p}l~Cpuk,xkmlmflRt}uyvoqfHk,f~Cpu zuj,p}z=fj,lnp}vvfffvvk,u~o)+*243vzul,x]paxkz=p}j,l,t}l,{+893ff:}];<fi4{<=ff}2H>=:?R@9<A7q>%^H_U`a`b$cflukffqfup}lmfBBvflukqpLrk,oqprp}vJIrYoux~ovt}uCBl,outl5Bxk qp}lfitkmpql,x]pGvvp}vxk{tkmpql,x]p4p}(vk,qzzOp}j$l(p}jHtlH]ft}kml.pqf{~Cpuk,lmj$t}xll,oqfj,fp}j,fJout|}fJI ILN, u]p}jl,oqfx]jj,fkmz=f~Cl,x]|}fk,fl,kffp}ukmpql,x]pukED|x]puk$]}FBBJI } Lvk,qzuz=pkmfl,oqfj,fRfCqxkml,kHGYoqfLK *t}k{vxkKuik"l,G3P:QR]"tUqxk,u~$ol,outlt}l,outlJI 3JIJIft}ukl,otl4, ]p}j,JIJI } 3xkffqp}ltYkmpql,x]p.p}qv{}l,outlxkMBvkmpql,x]pYpLrout}k{qp4, dgfBvfiST@UVWYX4VZ[\T][^W`_&Za&b;c:VdeEf"Za&WT]VZghX5i$SH[j kl"mFnpo`lmFnpq+r"s=qq+r"kt+kuk>v`wyx#q+xzs{"l"w]|<{k}~YMsfiyk;q+rk:fi^=y>>z:<+zmfiux+{^rq+r"s=q5 0 u wyx5s:v`wysfisfil"( w]xHwfimfiq>mlx+w]x#q#kl$qmfit+kmFfiktq+r"w]x}0~rsfix5q+rkx+sfiukzx#kq4mfix#m]{"q+wml"xsfix4u`q+rk"t+mfi]kwqH>mukx4t^mH{q$kfikl(q+r"m{rw]qw]x4w]l$q#kt+kx#q+w]lq#mo`lmFnq+r"s=qzx+{"\rs}0~k>v`w]x+q+xw]qx#kkxt+ksfix+ml"s=k2q#mnw]x+rq#m(mfi"q+sfiwylwq5q+rs=qw]xq+rk2{t+0mx#kmfiEq+rklk>v`qx#k>q+wmlMnr"w]\r"t+mfi0mx#kx5sfilsfi]fimfit^wq+r"sfi\r"wk`w]lsw]fimfiq>ml"x+w]x+q#kl&qH]q#kt^w]l]OF<:O(=Ewt^x#qslmfiq+s=q+wmlMq+rk2x#kqmfiEq+rk{l">q+wml"sfiM>mlx#q#t^sfiw]l$q+xH24\rmx#klq#m0kq+rkwfimfiq+xmfiksfi^rwyxsfi]]k2s?=fi\==Jfi>\$sfil2"klmfiq#kuEq#kt-q+rk4]q#kt^w]l=Lufiq+rkwfimfiqx#kqj k2nw]yx+{"0mx#kCwylq+rk2>m{t^x#kmfiq+r"w]xx#k>q+wmlq+r"s=q0mfiq+rq+rksfix+x^wl"ukl$qmfit^"kt^w]lsfil"2ps=t+ko`lmFnlnkCk>v<ysfiw]lrmFnLq+rks=t+kmfi"q+sfiw]l"kw]ls="0kl""w!vC<r"ksfifimfit^w]q+r"w]x>mu0mx#kmfix+kfikt^sfi4"t+m`>k"{t+kx r"kwtC"w!0kt+kl$qCkfik]xt+k"t^kx#kl&qq+rkq+rt+kk4x#q#kxmymFn4kq#mzk>lkwfimfiq>mlx+w]x#q#kl">fij k4"t+kx#kl$qq+rkx+k"t^m<>k"{"t+kxMw]lsfilsfix+>kl"w]lnHsYs=ofikCq+r"kC>ml"x#q#t^sfiwyl&q+x\Csfil04C45>mus=q+w]kfi<4Hmu{q#ksuwfimfiq5424mfitsfi]05wyl(^rwkfikCw]fimfiq>ml"x+w]x#q#kl>mfitq+rk2}0~t^m<>k"{"t+k<"&F&`#5$# : s=ofikxz>ml"x#q#t\sfiw]l&q+x5Husfil"\H>mus=q+w]kfi5wqt+kumfikxCt+m^q+rmx#kq+{"kxznr"wy^rml"mfiqCr"sfiks>mumlx+{"0mfit+qw]lEmfitq+rkx#kqn4m>ml"x#q#t\sfiw]l&q+xElk>kx+x^s=t+fiwq>t+ks=q#kxq+rk>ml"x+q#t^sfiw]l$q0^z0kqn4kklq+r"k:s=t\w]s=kxHHusfil"=Ffififi fffi ff:ff&! "#fiF$%'& & (*) + fi&-,/.!'&021#3& & (54 ) 6& & (54 ) fi7&8"%98fi ::;& & <#:=) +!"0>#=!"0t^m<>k"{"t+@k ?A`&z4`#5 s=ofikxsfi]>ml"x#q#t\sfiw]l&q+x\>ml$q+sfiw]l"w]lHsfil"(x+{"\rq+r"s=q5 CB DFEH>mus=q+wk5nw]q+r4C 4$x+{">kx^x+wfiksfi]yx-q#mq+rkx^{"t+m{q+wylkH<:`$`#H&#q#ktwq+x5>mu{q+s=q+w]ml"HC5w]xHq+r"kt+kmfit^kCsuwfimfiq5mfiE!DFE=Ffifi GfifiHIffI!&! "#fiF$JK-(<LMNPO:RQSfiTQKUVW(5XYfi;fiffff!fi Z:[' \K!"0rk}0~w]x5wfimfiq>mlx+w]x#q#kl$qw!;sfil&(=s=t^w]s=kzHCmfiE77wyxHq+rkq+s=t+fikqmfisuwfimfiq5mfi-DFE>]!^`_`aWbdce8fgdeihkjlceffmjonqprtsuwvKectvSxyy0cwzShKjle*{P|n}cwz~Zr+|8mg!gdechkgwr+|priR8m0uwe mcn`bcxzkvSgprSw[r+|uwepwSw\|8^8fi!%!!0i !0JM!! !0ff!0!$3 ;\ !;ffV0%!! Zi0 ffff!J ! =ff 8!0>!0Cq`!'`Kffoff0wff ffk0`0Pw'2iw'ZC`ffii'VH'00Vw'2iq-iw+5dtY[ikC5Mi; kfffifififi)*$+*"!$#&%* -,CFq<i8i+q'i* <qV+Vq 00C00+i 0i - q>+VP88+++%+VPq+8++ q+ i'00i0 PV;+P V `P+ 0P`0@@+VPq+8+ + 8+ q @P0WV +VPR0+t +V+0iW0+ViP++8Z-q 00ti0 P Pq ZP8ff++<`P 0 +V+qC0ffJ+VPq+8++ PV5@P0V `V8P +VC` 00+00+P+8i+8 qq 00t0i'0C <qV+PP+P8 `80V0+Vt+ 0@0 + 2 @ V8V05+V V+t0R+P\P+M0+V <qP++0+VPM PP+Ki+P +Pq 00 0i0+Vq +H5VJ V0iq 00*05'iq ZP8ff+ [+Vi+V++P+PT0\+<V02 <q+*0P0 +MP +PPP+8++W+\0@+P<R + 0+q V P+ 5PM PP+\ P5< P q8[ P0Z++\+%*iPZ8V0* `V8P<` 00+[0tq-000+q+Vi+i0+ <qP8-+VCq 00t P+8Zi V+ 0\'( ))*. "/0213 (4 5362* 7* 8%)9)(:;* < fffifi=8>?fi@ABfiC EDGFIHJ%='8 Gfi@ABfiCKJC!$#L3Afifi=:%NL & 8P: ?: *RQffSJTH2)* fffiUfi=8%WV XD LXfi @ fiCAJC!$# LQffSRTH2%ZY[fi@ fiC\JC!$# XQ]SJTH2%_^ )98L 8+Ei$kj e ljP@<\mon e @ &% &%[kj e ljPCkqmo-` =abcD]deC *fd @ CEfiChgp nUeCrkjP@8ljPCksmtp n @ CT%[QffSRTH2)*Zkj e ljUCu vneC rkj3@3ljPCu wn @ CT%[xL 8L s* -,)* =fifi=* h,dR)yzP:{ |}b~HJ{ |MbDTHbD%fY QffSRTH2*>fi@ fiCDFHJ%xrR&$%0++qPP0O w 0tq+ P+8qi8=: P P Vi)*OV":)*V0 L V-+PMR&$@+Vq00 P8Zi +P+ 8*2P* + tV* -`, V)*t:]o% )o% &% ;+M0+i&t ;q0 0i023* 00++ P<8Z 0V* iV+Z\N V+o+V": V": +V;P 0+V`0 0(!6`> V8P+V,`i)* P+ L 8%$ V !>";E+PC+8+ $: Pw*+Ps`, iV 000+P P+V +P)0 808P-V` 0<q8%,q+-+8i0F+VP+0`P+ L 8 02* -` R +Vq* +V88++ +V000w+Pv+M0@iit0Iq00@i02P* P* 8Qq 02*X2g <q+ L 0V* iVP%~2\3K2<PQq22"UzX GEMPzUG\qUKs K$]PG\qUKs K]\2\[2g <`+ L 0P* iV<&+Ez3K2"Uz3GEMPzPKK$]2zP K]\2\0V* iF0wP&0 q0 0 P+8P8<: [Pi)* <qP3EGEMUKzPffE\<G\qUK3$ff2$ 02*3E2<P3GEMUKz%"(fif33<G2 3 X=PZ-&zG)G&G zP<zz3 (()=K~(3G~?)PMG~"G ?"zz" A<G(83\3-(= <hK z zk} <fz3 ")Gc z 3zz&Xz l"3z)"fiff A)38 (3( () <z h)"8(z G (&k "z$ ((J ~ z 3 38($ 3zz$(< 3) 3 (3Khz z( ?)A=(z3= zh)( ]!#"%$'&)()+*-,-.0/1,2!3#+3#.4"2!/156&782:96;"2!/1"<,-=.#>;"@?A+*-,-.BDC EAE!F:GHBI8F:JK!LNMwPQRPG UVC U'FWJHPQ8LNXY[ZF X]\#^ _`MRZJba]c X_`M8ZB!dVC GHeEAI8F:Jx ybz{z|1}x~`|1# }'y#|1PQ c XYfLUWghC JJba]c _`Xx{'yb}Wbz{~u|1x]~sb~u|1'y#~u |1BI4eI8F:JK c Q[_iMSj4Xkbj4Xl c MRZ]_`X]\4mj4XG Ln'oqpj#Qsrc k4Q[_ukUVCIsv6F:JgB;#"W&,2!34.4b-+2'.-2!/1,2!3b+34.4"2!/15fi-b ?A>A3,-=.#>!"+2!.#8-Afi0DA!:H8:!NRV 'WH8N[]# `Rb] `8!V HA8:b{1`1# '#1fWhb] `{'bWb{u1]sbu1'#u 148:[i4b4R]`]44'q#s4[uVs6:;#W0s46;;%W7qVu67@!6;# #!%!{;#-8#;4 1;# # -V6;:%W7qVu67 ! V<f;b]#s-fiff' : ')NV'uA7 ; V : W7NV!b ;W;-W!+ -W#!+;)-Aff' :%W7qVu67 ! V :!"0!]#1#%$4#;&1!#4b-+'('' :!H;%W7qV*)+;# # -V6;:%W7qV ;7) ;qV],#R-"-Wff 6 :%W7qV ;7) ;qV : W7NV 0!] ;W ;-W!+ -W #!+;+;# # -V6;!WN :%W7qVf;#/.10 ]2#s-3Wff 64 ;WNV' ;%W7N !WN%0!5b ;W ;-6]!-;-fi-$#A+;'-A2Wff 64 !WNV' ;%W7qV :!-": 0!]7 :!968 H#;:1!44b-+'' :!V'7!WNV;)+<>=5?A@BCEDFHG6DEI*JBJKF4LMI;NPORQMST@VUFWXGYDI;J4BZJ1F4LI*NPO[A#1W\ +!#] b-+ #! ) "^$-!$_A #`1!#+44%%-);-;K1## 4 \-4 [!#P##4 sA1#!-cbed1f>;-! #! g6 ]-\ -g1!#+44% - #h`'1H+ (8A]4b;{ + #!8#K1##! /6]-\ -71!#844%+ j-i ; s--:A #k1!#+44%P -A +4K1#kl 0++!##4 &! #P ## -A #;:s-04]:\ +; f!1#A->bedK!f nAm -4 #!KH+-4 #! 1 #+1pA -\ -q1!b+44% +VA#K14>- ;n1!+ 08# 4(1A b+;-!@ 0#r f!+s(6 #21A#+44%PMAm ;-+4 0#utjvwaxay4z{v{|P}~jMA #`1A#+44%PH+ <#;bR-#! #P 44!###+ #!b!]AuA ]-\ -[1!b+44% -!m -#A #1!#+44% -! !+#K1#!-#Y1!#+#4%P+ 08+-#W!V##! -\ -;1!#844% +0b44b146-\ b#%-A!+#K1#!-#1A#+44%fi-++fipu"7gnEj5!j ;5!K"a" 15;j5j4%^%najaK%K{ Mj"44%"Z%aj%5;ZK""]K aZqa/1V1Vj:^M!;_q6M!q>"KHP;a(]15]11 ga pff[KH% fi a5]E!#"%$'&)(+*-,/.01!2&32.4&# 5762,'89$/:;: fi$'.?>KH fi BCfiJ $/83 fiIL *M,/N#"%$'&OQP2:R3Sa] UTe1aK).=<eV]aH]a$/.?>6,/89$/:): fiA@%BCfiAD ;.E<GFIH fiI@%BCfiI'JK EVWX!Y>/)8C3M*&OZ,/.?$/:?"%$'&)(G*M,'.0-;1&3.4& 5[6,/8Y$/:):fi ) .\<]$/.^>_62,'8`$/:): fiA@%BCfiD ).\<hFH/$?.K>H/$C83, N#"%$'&OQP2:R3SfiI@%BCfiI'JfiADiBCfiAjJfiIjL *-/bacBCdfe7gHkRlPPK kK2P 1K X jmaKanR]1ofiAp a]q#57RamM]aKPr l " ]KV <tsvu n[aa1MPaPEPV ' PaKawfieP 5cPaEa]]"TP aH1aK> aK ]a K>xfiAp a]q,TR[gayl " ]IKKyfiZaV /aa]VK' ] lPK7ZP ayP1"0 n6 P>aY]1" : ' ] lPKGffn n nTa`lPP `[P1fi2{ \fiA@{z#|}fij- nf~HPa1pP"1^v;I"?),&*-,/.01!2&32.4&A 5X6,/8`$/:;: fiA ;.<tsGu$/.?>b62,/8 $/:;: fi ;.<1V*2(&)($'&dxe7g FH fi@pz'BCfiI'J $/.?>_H fiDBCfiIJ $/83 fijL *-,/N#"%$'&OQP2:R31F4"?8i,'pZ>j3>&)($'& fiAp@ zr|}fiIwYa4 0c `/a P ' fi% aaK /1Kp a1I/)c4]K1]a fK X7Zg +lPP aK /Zg-n 2UTe1aKe1 1KG RlPP aaK ]K ] " 4E "( a/ lPK PK 2 ]P&R]1aKn 7qqqVYM!;1 K> #a# q n;VM!q16`9K>h[1a aK X9K,a24lP2a a1 Ka4 lPPa1r eaa ] a1^ M5/aP Y]1{a1`]aP4- nVp lP1Ka(2 Y]/K6P]] U Te1aKe1 1KlPP a5K a] a5K 'n&]a aKaKa:{ lPPa5K ]a a] a5K U TE1Ka ]) 5&PP [KKaa Mb fin{pfio{^^/4V%h!%;/ h!qZ- ;M- !VqV/h4!%/M-C!%ZCMC;--CC)CZ/Mp{2j12/jj2{)?%pMMp2-)M2/2R0'/f/=24RCCoVb'qPivot consistencyDirectional path consistency0i%VV000#y%2RVb4''f2VMRCCV2Y'42V'%?/92V!CCV2 900 fffio^ ff 0'4C02VCC-'Rj! 4_2/C "w4''02VRCC42#fi%$ $ &$'(*),+- %0,0'CC 0'0_/ .%CM 0A0 4R`2M/C1VM2V'^4/=24RCCV2243 2fi57698:;#<:=>?6!>;@ =A BDCE:FHGJI=KA ;L6!:K=LMNO<PQ5>R SUWV%X/YZUW[\0V/]LS0^`_aSbT\0Y*cV&dfe Ygdf[KS0hgYg^fX/YZibSb^V iX/_a^jkgiHe Ygdf[Kkl^jmX/YZibSb^V iX/_aYgdonS!dV X/^S0YZikg\[kl^jHX/YZibS!b^V iX/_gpKkgbTcfS!\\KiLY4crqffV%bjLY4cfitsutv!wyx{z}|~JYX/V ndVal S!b"X/YZUW[KL^V nV/]kgX/^\!_YZiX/VmeYgdWV kgX?jhEkld?S!klq\0VamS!iS!^^jV iX&kg\!\!bkl^UWYZb^OW^SUWV b,[dYX/V nLdV4LZK*KQ YZiX/Ve Ygd%V kgXjYge7^jVWe YgdUWV&dhEkldSklq\0V b??sXjS0V&hS!iL&X/YZUW[Kkl^S0qS!\!S!^_qffV&^cV&V ia^cYOX/YZib^dkgS!iQ^bTff*{kgin1}{X/YgddV b[ffYZinb7^YWkgX?jS0V&hS!iL[kl^j1X/YZibS!b^V iX/_e YgddV \!kl^S!YZia}&c%sd s^&shlkldS!klq\!Vf sRTjS!bYg[ffV&dkl^S!YZi1iLV&V nbTQ*SigV iV&dkg\sL^1bS!iX/V#^jLVDX/YZib^dkgSi ^Hff*?S!be iX/^S0YZikg\pf^jS!bX/YZUW[\0V/]LS0^_S!baiLY*cQ*?sRTjLV#Y*hgV&d?kg\!\X/YZUW[\!V/]S0^`_YgeJ^jLV,}\0^V&dS!iLWS!b^jV&dV&eYgdV"*&}S!ib^V kgnYge' e Ygd[kl^jaX/YZibS!b^V iX/_OYgdTnS!dV X/^S0YZikg\K[kl^jaX/YZibS!b^V iX/_ S!b7^jLViQUqffV&dYgeJhEkld?S!klq\0V b&pLS!b^jLV%bS!&V%Yge^jVnLYZUkgS!ibkgin1"S!b^jLV%bS!&V%Yge^jV%dYYg^TbV&^??s'fJt,E4gffZ}Zg`'ZRTjLVS!i ^d?S!ibS!XXjkldkgX/^V&dS!b^SXYgekm\0YX&kg\X/YZibS!b^V iX/_S!b^YHV ibLdV^jkl^km[kld^S!kg\Sib^kgiQ^S!kl^S0YZiX&kgiq}VV/]^V inV nH^YakOiLV&chEkldSklq\0VgsV%db^T[dV bV iQ^f^jV[dYg[ffV&d^S!V bYgeJ[S0hgYg^oX/YZibS!b^V iX/_gp}S!i[kld^SX&\!kld^jLVX/YZinS0^S!YZibinLV&dcfjSXjOk{X/YZibS!b^V ^7[kld^S!kg\KS!ib^kgiQ^S!kl^S0YZiUk _WqffVTV/]^V inLV n^YkabYZ\!L^S!YZitsfVW^jLV i#V/][\!kgS!ijLY*c^Y1X/YZUW[^V^jVnkl^kdV S0dV nmq_H^jLV{K\0^V&dSiLkg\0gYgd?S0^jU1pkgina}ikg\!\0_[dV bV iQ^TkUWV&^jLYn1e YgdfbYZ\0hS!iL"eiX/^S0YZikg\ff b&sv%Ez~Q44 QV[dYX/V&V n1S!i^cYb^klgV b&db^&p^jLVkgnnS0^S!YZiOYgekWiLV&chEkldSklq\0Vf^YW^jV,X&ddV iQ^Sib^kgiQ^S!kl^S0YZitp^jLV i1^jV%V/]^V ibS0YZi1e dYZU^jLVdYQYg^fbV&^T^YOkbYZ\L^S0YZitsfiff&// g !#"%$%Ez~4?v&J ff&)'& (*}, +.-/0 1 "234Z6587"{39L0O:;/3 < $ = >?,9m39@g BA C3 9 ff& tIH J W: '&Q&K ff&4 24,K3/ g4LF"G$ 53/gEFG"$%EztNz MO*P V&^b{bjLY4c^jkl^"kgi _X/YZib^dkgS!iQ^{S!iX&\!nLV nS!i $ aS!b{bkl^S!b KV nts P V&^Q< $ tbX?j^jkl^RA #SbOk[KS0hgYg^Yge $ = t/pTkginTSQS!^bhlkg\!LVmS!iUff&= t&sWVYZibV Q LV iQ^\0_gp^jLV&dVHV/]Sb^bS,.Xl SQ?s P V&^TbTnLV iLYg^VZff&S ,[,[,[ SL,[,[,[ S= Sg?sls Qi _OX/YZib^dkgSi ^bkl^S!b KV nOq_ff& t7kgincfjS!X?jnYQV biLYg^X/YZi ^kgS!iaS!b7^jLV&dV&e YgdVYgqQhS0YZb\!_kg\bYObkl^S!bV naqQ_2ff&Qs\G]Ygdkg\!K\ ^)_ b&s^&s}0Q<W"s(]JS0db^&p' < $ t/\0V&^CS qffVS0^bhEkg\!VS!i`ff& t&sbaV X/YZintpWS!b,k[S0hgYg^,Yge $ = t&s%}0kgin#A OkldVWX/YZibV LV ^\!_H*X/YZUW[kl^S!q\0Vgs,V^jbTjk hgV SQZc<K0ZbYLpK}0{S!bbkl^S!b V n1q2_ ff&ZshgV&d_X/YZib^dkgS!iQ^TS!iX&\!nV nS!i$S!bbkl^S!b V naq_ff&QsfefV iX/VZff&S!bkWX/YZibS!b^V iQ^TS!ib^kgiQ^S!kl^S0YZisEgh#i=j'kmlnopnqrnKrstq/uvoFw6uvx/y{z=u}|oFs~nuvlx/r/rnqIsFlx uno~x/s~IlxC/xs~nuvlxr/F=fuvKnuv'E6FfirxzIx/lnKEF=#uvx/sFo|n'/x/snulxrsFlxn |truvx,nr|oxln'x/oFs~oF r|uv}zu}|oFsno#zpnq/o|uvyq,nNrF%qo~|or/xs~nuvlxr/sFlx/n |trux,nuvB3=|l4o~|}=Bz=u}|oFs~no#zu}mu}nl|uvyuxuvE4o~l|o%u}nEntr|yo~nuvxnqouvx/ntrx,nurnuvlx*l|tzo~|uvxy,rxz oFsFlxzx/lxx/s~nuvlxrsFlx/n |truvx,nrF4oms~|o#rnoFz,Gnqofi|l,s~oF uvx/yjfiGQI?=?6#6?8,4686#,4mZ),?@pZ#??%,6C@N/@,E3ZG(F=4{6,3Z'Lb#4G4 ,/4?,N#3L?3@4C'3/4CtNIKW3`?t'Z34N;W#);3c3(3E4bB;t,'2*;F;,C/N 64 ffb;fi *?,6#8N#4#6;G4,fi4c6446c4,E4CN44 ?4,N6##,! ##,3;8?#4;4$&%('*),+ -/.1032146579895:<;;>=Kfi?@032A4CB;)NfiK'44p;N##6 4,##";#<DFEGF)#6; 3 ,46H c6RN ;46Z;?##6f?4;B4 4?B?@6?fip?fi6mN44m;?##6mZ}6},m4;)Nfip4;?)64,%4?CN44#, p*,4`4Z;fi J;?4#,*?,I#";b#*G"Nfi6NZ#'NbG?;4K LN4M#4M@fQI,4,,34Z6?N%),;?E?4, 4fQZ#??{?G), 6;);#6#;GN?4#6 4?34G644fi6pZN6#6)8;fiONP 24?Z66E4fi;)';;{ON4fi4AQ6?6`SR ;)N;`44#,N44),4??N#2,UT4?4;)Nfi4;?)64,?N?4#,4;;6 V;6T4W'??6b(#;6*4ZC4NTG,64#;" ;??6#6;);6%6N?6#;C?@,K4?6#6,KX4,Z(?6f4;)N;4 2 '?NZ 4?)6,4#4";,4;?)64,6 [Z4)4Z,4 ,Z?6??6]\I84?^\Z`_@4G43bcedgf]hji #2;N#?Z4;N)64,{@4G#,GN6N?kN\Z 4Ndgfhji 34lT4??6?NkNW\I`_ m@ 34ZG"%N?4 dgfhCion6prqtsuwvAx<y{z6sX|!sv}x1~N44f,4?N?#G#,f[T Wn6z6s|<svAxzrqt|![svAxAFzrqt|![svAxezwqt[vF]z6svAx<[zrqt|![svAxfi{prqp1vAx<~;)'fi84;?)64,BcZ;#CprqsPuvAxKy[z6sX|!svAxKy{zrqt|![svAx<y[zwqt[{v&z6svAx<y fi[eprqp1vAxtAQ6?6'44G;?##6] #,4G43bcd#i&6K=#{ #,@`{N44;?##6?fi6AQ6?6J;)N#6#6;I@GF?#4;44G43bcdgfK6 , f8@?,4;?##4G???6*,8 4Z;?#6N#4;?4 on6prqsPuvAxKy{z6s|<svAx1~nfi3(=GP3;3/3(I,N4@~X@fififiwfifitKfiKJfiA66<"336 K fi]<}jKOt66(^O"6<6K"]^"(""6666]e""6A e6fig(ff fi[fffi&K[ !" "fffi#^$"ff<!K%'& ()!"ff fi*+-,$.fi"/X0%1& ( !" "fffi4 4 76 498;: < 6>=]32(O"fiO ![ 5""" /][366O"fi@?AB^W"U66fiO <666Ol!el^DCr UU E!6F2!"6^6fi6" HG{JI5Kw6(KS<6([UO"firfifiQPtKoHGe!"CHG<LC6O{O"6 E!6Ol{I6MW< tNK99O"6 E<<"P9"fi} ]9e"6eL QROKwKO"666({ r6 fiLOI !69[O"TSKU6<"fie!6"6"6e UK "6fi(]]O"fi6OI6VKe9Ke`b[ c[OPfi[Ue"fhgji'fftkmltQonOplt3gdXW$Y9Z[FW3[]\ '^_ Aane` " ql6rfftsmltQUondPfid 333gdon "ffNplt3gfft"Pdg<"3Uonff!d fi33"33mgz9{ |}}}| z [9O"fi<fiK("F r!w{|}}}3|=Oy=y~x=3CrO6 RIw C9Ot6C"L"" ` J !=k{rC KC ,U Ofi6KU"2fi^=7| zO"6 !E <>Ofi6E!2 z6 zIw6I{dIj"6f= =O"6(KfiK("C6(66Jw I[ / "6" ""C6fi6w)==2 [wK IfiK(6<"6 F!2 "fiIfi9Ot6>^"6"O"6 !E <LOt"6e"6fi(UK O"6<6K"66 eO ""fi `XW$YY9t"u 1v xwf1dxh'P!6<CO"fiK<69e JO"6KLWV]OK66"6O eMW6OW@ fi6U""6?AB6 fi9]9"" ]<!fi6OJ6K!O"fi6OI1fiH2eO"6(6OI&O"6K/r3KlO"6OeNKwP]fifiO"66OfKK Kwfi6O"6fiH2O"TSO"66O fi9">2fiH2O"6<L@v1 IFKw6fi"69 fiH2{O"fi6OU62tfi6O"6-?ABK FIfi(6lO"66"firfi6K9O!fiLO"6<l(6<"J""FK ^ 6<6O"9"" ! fi " "fi9ITK (O"fiOPA"Ht2 fi6O"6?AB6-CrLK9"K ] l3$fiV3aH>d H>m-j de"Qm H L mm"] >d H>9- d9m 9Hm9mjjk9 mf"d9> m) L m-mm 9H s)>H QjDs dTmmmdjd d"d"s"5xQjmX9 mks dTd keH 9 QkVm]mXf>m] DmJm9H>mej;ds93]Vm>dj9mH>mQm Q VQ>km TH>mO dVH Jm H Vx e>m] s" ms j mQx d;>m]De d;d a>" D>H>mHm9 ) m)x >s9 9H Q Q5 H"-m mjJ" xm 9H jDjmd>x"k>9"d LV9xe mk99 H>' mkQ dFkm )dmmb )ma;> msd H>;md " mU>dNkf mVs dTN) mV>e@ "]OFd3V; "; dXm"Fm>dN Fm) QH1> Fm H>;@ dXm"FV x" V9Hkedk dj dXm99Hk>m 9ds>mNm"a" H e>d" m"H Um>9m ;mT9 >sXm H Q f9" -hammdDQ99 Fa>Q -F>d 'Lf dV Q)m 9H Lmmdj>QmmH ds ) >mLm 9 -'m]d k" >>]s 3 -m>sD>m] QNmm9NF; dX"dQ9 F" ddaHf mVVdkU9 9HO9m"mX md>m " a9m m]d >s>m] VQm9md>d x X"HVm]JQ >NH ;JT9H>"H>x;"m9dQmJ"m"]H e-m H QOmFdQ F VL U' ex 1H QOed>]HNdF de"Vd J"mHTd9H>m 9N "j> 9Hf dx"Xm" 91>mmd Tdj>m mV Q x5>m" QxV"]"Q Tm- 9">)s mmX9mmm 9@F9 md"m"]H H>Q1m99 F" d>1 Lk @9>9" ]'k d- @>9"fffi' ff"!$#&%('*),+-.0/1.324'ff56'879/;:<2=5>2=)-)@?''8.BACD.E)hOdTsHm mQ9UQd mH dm] H>k>s9m H>Od"mx mXm dO>d H>m X >]GFGHIKJffLNMOPRQffPTSUJffVWJ3XNYffZ\[^]0_3`ffOa3_\`;b8]0QMdceJ3XNYfff*gh >s9djilk monpoqororor\qsnt>u m5 >d>Q>md;>s>d Yfffrv h >s9d"9wYffZk =xqzyZ ]LVd "|{UV} jx d" mmH>V>d>H>9d>s>d (n~o} Lm d"D"">{O} {$Vm>9H1N"X;">s;n } UsN>nIKJffLNMOPRQffPTSUJffVWJ3XPsc_J*O]0a\_"J3XNYffZ\[QffV`6Psc_]0JJffP_PJ3XNYfffgF h >s9d dZk mG{sRoqorororGqT{Tfi\u @ d>m")YffZF)h d> e " >d"{TUX >kZ "Nnff 9Vm]Va m"H VVdX EN 9ON)s FOOYFfF\0fi8888zD8D38Bd~*D1*181Gffj G\T3T3G0G0ffj G0T3T\1181RGTo,R1TT1~1GTGff~GDRGzGTd1R1To8j8R1^RG8~GN8~NRT~G111$&~()%d1 To8~ ~~&R9To-.-/ 01!% ff'+*\ U U\U K\ \U Uz zU10D\fiU\ U0 R0," #ff fi!fi243 57698:<;>=@?ACBCD9EFHGJIDKAC?ALNMO"PRQTS"OSPVUWIC?G)XYX9Z\[]+^R_`^R]bac]edgff)PhOSPYi?I?lGJIC?"G)mnBC=`G)XKXKZBoNEp;c=@?AqBCDKEF`ALsrtIACBu`DKA'UvGwICE>E)B4AC?Bxi.G)FtyHD9z{ACE@utDKA|B kj4k?"rtIAqB|A k~?B=`AA kU.vE oB kE oB kGJBB k?lAq?B4UeoN?"mnEwt=@Bq?yHDKAGICEcE)B4Aq?BLGJBG)FffZ)?ICBq?n DKAGy@?ACmn?F`ytG)FffBzRICEeG)F ?X9?w?FffBE)zUFtD9BCD9EFuTDYA'B k?+ACG)w?AqBqICEF`X9ZHmnEF`F@?mnBq?ymnEEF@?F>BG)AhhZHy@?nr`?FHGwy@?Amn?F`y`G)F>B4z}IEB kE.?XK??FffB+DKFU?X9EF@A"BqE.B ky`?F@E)Bq?|tB kB k?BN?bG)?ICBq?n E)zohEwEACACDKtDKXKD9BCDK?ALj:F?X9??F>BE)zUW?X9EF@ABqEB kDYA|Aq?Bl]b^}_t^}]baT]"i?")IGJqDYFDKA'?X9?w?FffB?AqBqIEF@X9ZmnEFtF@?mnBq?ymnEwEF`?FffBok cE@u7DKAF@E)BGAqE=@Imn?E)zB kA=`m kB k?ACG)?ACBqICEF@X9ZmnEF`F@?mnBq?ymnEEF@?F>BG)AkDKm kDKA|)IGJkGJB'%{DKA'Gby@?ACmn?F`ytG)FffB'zRICEggI?`IC?Aq?F>B4qDKFHU.:mmnE)Iy`DKF`BqEwB kmnEF>BCG)DKF`AN%u>G)F`yb%D9BCAIC?y`=tmnBCD9EFDKFmnEF`AC?;>=@?F>BCX9Z)uB k?B'|?B k?B+=`A?"y@?nrF`D9BCD9EFE)zsB k?IC??nTDKACBCAGpACE=@Imn??)?IBq?nE)z?lIC?yt=`mn?y)IGJoN?lm kEAq?BqEu@cF@E oDKF@B kkGJB4DYAGy@?Amn?F`y`G)F>BzRICEvq%uG)F>Z.?X9??F>BE)zthDKAGy@?Amn?F`y`G)F>BzRICEWG)F>Z.?X9??F>BE)zguG)FtyB k?IC?zRE)IC?lzRICE|gNsDKA4mnEF`AC?;>=@?F>BCX9ZGwy@?ACmn?Fty`G)FffBz}ICEG)F?X9??F>B4E)zU.UeDYA4GICEcE)B4Aq?B~?F@EoA kEoB kGJBUDKA|]b^R_`^}]baT]%^RSKu>B k?Bh=`Ay@?F`E)Bq?" U:ACAC=t?|B k?B k?lAq?B4E)zB k?+TBqICEF@X9ZhEF`F@?mnBq?y'EEF@?FffBCA4B kX9?B'RUK?B kAqE=@Imn?Aj4k?Aq?BhE)zB k?DKINIC?yt=`mnBCD9EF`AhDKF?B{=`AVy@?F`E)Bq?h"p`B kE)zB k?IC?y`=tmn?yp)IGJku>B kkEAq?ACD9?DKAJcGJBmnEF>BCG)DKFHG)XKXB khZpy@?nr7F`DKBCD9EFwE)zUuTB k?IC??n@DKAqBCA4GJBX9?G)AqB|EF@?"ACE=@Imn?q"DKFRUpY>J7x'hEF`Aq?;c=@?F>BCX9Z)u@B k?IC?y@Ec?AF`E)B?nTDYAqBG)FffZIE>E)BAC?BE)zACwG)XKX9?IAD9? xL?IC??nTDKACBCAGICEcE)BNAC?BUpToDKm kk?B'wRUpK?"?X9??F>BCA|E)zsU?I?y`=`mn?y)IGJy@Ec?A4F@E)B|?X9EF`BqEwRUG)AADKF`mn?zRICEG)F?XK??FffBE)zsRUpYhBqEwq?IC?l?n@DKAqBCAF@EtGJB k?ACBqICEF@X9ZmnEF`F@?mnBq?y+mnEEF@?F>BIC?y`=`mn?ylBqE s@ICEB k?IC??n@DKAqBCA'F@E+tGJB kutG)F`yk7kz}IEG)F?X9?w?FffBhE)zU?y@?nrF`D9BCD9EFBqEbG)F?X9??F>B'E)zw`xhEF`AC?;>=@?F>BCX9Z)u@UDYA|F@E)BGICEcE)B4Aq?BUeDYA|B k=tA4G]b^}_t^}]baT]ICEcE)B4Aq?BfJ]7KSC^RPBDKAB kmnEEF@?F>BCAu@B k?4ACG)?G)AB k?4mnEtXK?nTD9BZI?;>=`DKIC?ybz}E)INmnEt=@BCDKF`B kGJB4DKA4p{ jGJICG)FuJ xu7D9zlDKAhB k?4AqBqICEF@XKZ+mnEF`F`?mnBq?y?Fc=`"?I'E)z?y@)?A|G)F`yB k?F>=t"?IE)zs)?ICBCDKmn?A7{` RRw {cY+VR74 {.Y`t{\sV@ R7t7?F@EoWy@?ACmnIDK?~E)Iy@?IxDKF@@?rtIxAqB`IC?AC?FffBB k~G)FG)X9)E)IxD9B kEovBqEm kkB kE>EAC?B k?tD9)E)BmG)F`y`DKy`GJBq?AG)F`yBqEmnEt=`Bq? G)FUwmnEtGJBCDKtX9??mnEF`y`DKBCD9EF`AB kGJBpmnEwt=@Bq?AE)B k?tD9)E)BNmG)Fty`DKy`GJBq?AN=`ACBACGJBCDKAqzRZG)F`yG)FUpmnEtGJBCD9tXK?E)Iy@?IDKF`@tD9)E)B4mnEFtACDKAqBq?F`mnZDKwtXKD9?ANBohEmnEF`ytD9BCD9EF`A'EFB k:F>Zp'"DKFDYA|B kU+=`AqB'?B koh?B k?Fp`IC?Aq?F>B?Hy@?nr7F`D9BCDKEFE)z?ltD9)E)BCAL?BCGJIC)?B4E)zVEF`?G)FtyEFtX9ZwEF@?tD9)E)Bu@G)F`yF`EJGJIDKGJtXK?E)z{U?BCGJIC)?BE)zsGb7D9)E)B4%z@E)I?G)m kj4k|4DKFh+DKA|GtDK)E)Bu`B kUu)oN?k?FG)?|BqElm kDKA'?z}E)I?'+DKFB k?n@DKAqBCAutADKF`mn?|y@Ec?A4F@E)B?XKEF@BqEwUxohE=`XKy?DKF,mnEF>BqIG)y`DKmnBCDKEF,oD9B kEcEAq?|G GJIxDKGJtX9?N.E)I?E)?IumnEF`y`D9BCDKEFffDKF?E)Ixy@?IDKF@@AqEB kGJBs|"RF`?mn?ACACGJIDKXKZ+=`AqBF`E)BmnEFffBCG)DYF.G)F>ZmDKIm=`D9BuTokDYm k:o'GZBqE`IC?)?F>BwmD9Im=`DKBCADKA+BqEwGJI\?)?ICZfifi !"#%$'&()* )*+,-./fi021435146872149;:=<=>?<=@31 AB< C(9D72EF02143514687G<HC*1ffIJ:=<=>?<=@)351K< LMENC*OH7QP*1!R(C(LS<=>T 149DENC*140ffAUIUP(?6PV?0W7QP*17Q<=>QO 1ff7E XY<ZX[RC(687Q?5ENC(< 3)68ENC(0272>< ?C/7IUP*EN021\E >?5ON?CW?0< 35>Q14< 9*]SLS<=>T 149_^a`BP(?0bX[RC(687Q?5ENC(< 3)68ENC(0272>< ?C/7?07QP*14CG?C(6ff3R9*149-?C/72EdcZedA< C(9G?57Q07Q<=>QO 1ff7f[7QP*1gC*1ffI;:"<=>?<=@351ihb?07QP*14CGLS<=>QT 149_^a`BP(1\021ff7cZejI1E @(7Q< ?Ck7QP(?0lI<4]k?m0U7QP*1d021ff7nE Xa7QP*1po?5: E 7n6ff< C(9(?m9(<=72140lR02149K@q]%7QP*1po?5: E 7n68ENC(0Q?m027214C(68]K< 35O E >?57QP(Lfr0214687Q?5ENCSsq^tsNh^a`UP(?0'021ff7u?C(9(R68140v<\o<=>Q7Q?m< 3/E >9*1ff>wBxyENCdz{f[I1ENC(3]P(<4: 172E|< 99Z7QP(172>< C(0Q?57Q?5:?57}]68ENC(0272>< ?C7Q0~h^-CE >9*1ff>|72E0Q<=7Q?02X[]7QP*1-68ENC(9(?7Q?5ENC(0nE X\18)C(?57Q?5ENCsq^tsqA'7QP*1W< 0Q0Q?5ONC(LS14C7gE >9(1ff>?C*O68ENC(0214qR*14C/7Q35]P(< 072EW@1|<-3?C*14<=>187214C(0Q?5ENCKE Xv7QP(1|o<=>Q7Q?< 3E >9*1ff>|wBxo>Q18(149G@/]GK^l687QR< 335]68ENLMoR(7Q?C*OMwBxF?0aC(E 7aC*1468140Q0Q<=>Q]ucZe?m0u0QRW6ff?514C/7u72E68ENLMo)R*721B7QP*1U3m?C*14<=>u187214C(0Q?5ENC_^u`BP(1ff>Q1B<=>Q1C*ES68ENC(9?57Q?5ENC(0ENC7QP*1g:=<=>?<=@)35140E Xvk*7QP*1< 35O E >?57QP(L6ff< C7QPqR(0@1|9*1468ENLMoEN02149%?mC72Ed7IEW0Q721ffo0ff^-2ff[/%R(LZ@1ff>BX>ENL 72ES !7QP*1g:"<=>?<=@35140E X'J< C9%LS<=>QT7QP*14L^-2ff[/%z1ffo14<=7P*EqEN021g<MC*1ffIR(C(LS<=>T 149G:"<=>?m<=@351\Z?Cz!J0ff^7ff^a7QP(1ff>Q1n18?0Q7Q0<MLS<=>QT 149d0ff^7ff^R(LZ@1ff>B< C(9%LS<=>TWC/7Q?3< 33:=<=>?<=@3140<=>Q1LS<=>QT 149?5ONR*>1 4 o(>Q140214C/7Q0< CW< 3O E >?57QP(L68ENLMoR*7Q?mC*O@E 7QPW7QP*1\0Q1ff7E XYo?5: E 76ff< C9(?9(<=72140cZe< C(9< CS68ENLSo<=7Q?5@351l< 0Q0Q?5ONC(LS14C7aE >9*1ff>?C*O*^uY1ff7bR(0a>1ffo14<=77QP(<=7< C/]S3?C(14<=>a18q7214C0Q?5ENC-?m0Q0QR*149MX[>QENLcZe?0< CKS68ENLMo<=7Q?@351gE >9*1ff>?C*O*^`UP*1|021ff7(N/(W68ENC7Q< ?C07QP*1|:=<=>?<=@31407QP(<=7UP(<: 1< 35>Q14< 9*]@1ff14C72>Q14<=72149`UP*1%021ff7M*//*//)/!68ENC7Q< ?C0M7QP*1KC*187M:=<=>?<=@35140p7QP(<=7W6ff< CF@1%6P*EN0214CA7QP(<=7W?0d7QP*1RC(LS<=>QT 149WENC*140IUP(?6P-<=>Q1\7QP*1\7Q<=>QO 1ff7E X<pX[RC(687Q?5ENC(< 3)68ENC(0272>< ?C/7bIlP*EN021\E >?5ON?CW?0L-<=>QT 149`UP*1021ff7Q0- ) (iB[Ny68ENC7Q< ?C7QP(1E >?5ON?C(0pE XUXrR(C(687Q?ENC(< 368ENC0272>< ?C/7Q0dIUP*EN021G7Q<=>QO 1ff7Q0W<=>Q19(140Q68>?5@149<=@E: 1Wf[*/qqi/h%>Q1ffo>Q140214C/7Q07QP*1CqR(LZ@1ff>E X7QP*16ffR(>Q>Q14C/7:"<=>?m<=@351g?CG7QP*1< 00Q?5ONC(LM14C/7E >9*1ff>?C*O=~24r8ff1ff7UcZed)=rrr=N<lX[R(C687Q?5ENC(< 3N68ENC(0Q72>< ?C/7v?0Y< 9(9(14972E\cZeV14< 6Pp7Q?LM1<lC*1ffI:"<=>?<=@351n?0a6P*EN0214C_'I1U7QP*1ff>Q1ffX[E >Q1UC(1ff149S72Eo>QE: 1l7QP(<=7< C/]d:"<=>?m<=@351E XzW?0b02143146872149Q/N4rENC681 AN< C(9|7QP(<=7C*ENC*1b?m0Y021435146872149ZX[>QENLk^`BP(1a:"<=>?<=@35140v<=>Q102143146872149X[>QENL*/qqi/AIlP(?6PGENC35]68ENC/7Q< ?C(0lR(C(LS<=>T 149G:"<=>?m<=@35140ff(0Q?mC(681|C*ESLS<=>QT 149K:"<=>?<=@351gLS<]@1|R(CLS<=>QT 149@)< 6QTA'< C91ff: 1ff>Q]y0Q1435146872149y:=<=>?<=@)351S?0L-<=>QT 149_A1ff: 1ff>Q]:=<=>?<=@351S6ff< C@1-021435146872149H<=7dLMEN027ENC681 ^Y1ff7UR(0C*EiIo(>QEi: 1|7QP(<=71ff: 1ff>Q]G:=<=>?<=@351\E Xz!J?0|4r(=[-fr<=7B3514< 0Q7ENC(681ih021435146872149b<=7< C/]7Q?LM1 A*/q//i/?0\7QP*1S021ff7gE X< 3m3u9(?5>Q14687g9*140Q6814C9(< C7Q0nE X7QP*1SLS<=>T 149!:"<=>?m<=@351407QP<=7d9(EC*E 7p@1435ENC*Ok72Ek<=7p14< 6PDENR*721ff>M35E/E oHE Xl0Q721ffo Aa<!C*1ffI:=<=>?<=@351?0p18q72>< 6872149X[>QENL*//*//)/< C(9GLS<=>T 149_*X>ENL7QP(1\9*18C(?57Q?5ENC-E XYKA*< 33:=<=>?<=@3140bE XzkIU?337QP(1ff>Q1ffXE >1|@1g>Q14< 6P*149k< C(9%?C021ff>Q72149?CG*/q//i/(^==fiUi5(/5Q 54=QN(* (fiff fiij/ki(')*+fi !N)=#"$$%&, . -0/ 13254687 -09:d<;>=? (/ (N*AB @ = CE E GFIHJ'qq((i =*&FIHLKNMPOfiFIHQC ((RMTS(') UF H ?'()Ofi*VWC U')j/XF HZY F\[]>^NI^ F_[ CW'qq(N(OfifiF [Uff fi/OfifiFIHaN! #"dc%== ?, . -0/Le 13254687 -09:d<;>fhgi=kjmlN(Q.9 4n=': 4=F`[ab @=CQ5N<;_o8po8p? (/ (N*AB @ f CE E GFrq *T) uC T)wvW*fiffC E/iE/*&Frq&KNMxOfiFrqyC (N(RMTS(') UFrq ?&(')Ofi*VWC U')E E GFrjz ts = *){ (*u jT)|vx ! )=i}"~C%OfiFIz Frqyo8pj/XFq F_&_^$r^ F_Cyq' q((UCOfifiF_GC U*fiffC Efi iE/OfifiFrqy&N! (i#"%== ?9- fi1 l 6 / 4Q=Q5N%<;S-ffB<;|/) n ((=.-4}o8p = n 6 /=Q 7 -g<9-09:`<fi 7fiXn N(2.9 4En Q5N<;d-y.-ffQ ! ) (i}"c(% P"9 7 - FIH %(-09Q.-!N)=#"c(% 6Q6 -(m=.-4 ;[.-09 F H ( 7 -0-4V 4687 -09-(SN( //-4J;[<9-- 4 C6 9<- F [< , EF [ - n -4QQ9 4687 -09-;.-09 FrH(mB QQ:N 6 -4<9-09:J= n 6 /=Q 7 -= 9 7 -49-| 7 q 4 Q *N.-|Q<-4;d9Q-E92( 9- n n N( - 4 -4 n -|<; n N(5Q5Ned 7 <-6=K 2.-0/ -g -09 n N(Q5NfiEEIEECETE_Er((`I<..0<Q<a..<fi(.0*TEJI|Q<ifi00><<<] LEQ<a<0.(QQa<{0(LE..0fi(E..<Q<Ex.#Ex<.ifiQ.X}<0<#a(aC P0 <Q#0fifffi r.&QTrr.0} }E< (r"!$#%"&(')%"*+%",.-CBI(aE.0EDC0GF|<.<H JI (LK0 <rI <fi<<<(Mfia(E0fi0I0 <rI (Qa<<<(0N/<.<POEr<0<RQ_<0<TS(EM<(VU(3}<{JQ<r_E0>#>.ar<`E<W E{<.8XZY[:=2[\]>@< ._^.G03257Y`baMc[d?A V2e6?:)7/#.<103254)6798;:=<?>@6)8=7A.<%[gL%[h%"b%"fI0<<5Bw9ij_kmlNn]o "pCrqaTspCrq Nnut.n]o "vC[Q_0Iu0w <MB9i.i_xXOyK$(..r<8yK$(..r(<bz{n |} . ~E$ re} [.|ji9 ikCI<O89i.Ck}Q\<<<QQF|<(<O5. }EFwwOBr(..<LK.CnLo [q } [q8 n tnLo JJ t?o t[o9t rq ;DC8(Fw<.<3 <z ?q$mq> x5..X.jBIfi<0znB5bBI(v.OXS<<(E0MQ>M9i.i_x?Br<.05.<E}(E..<0|}0 ;~$ e} .;_xMj9X.CvCQ|39i.i.(}3 qaL{~ zn(<_<iE ^.<Q(..<<RBZDbQX(WC(Q< V/~] <wE<_x_fi<. ifi?B<N0 3 <"<<< BI(..<Q3fi>I<9^aFwbK<$Br.0Zv5(a((zn |}0 ;~$ e} "">u(jXCfiv5.0bE9i.ik?U3E<aDCav5.0ew9i.i<((kC0<.Q(<E<(..r<z{n|}0 e~E$0d re} )>;j9kj<..0 <(..< <(<K~E$0d re} ?9r_X.fiQ0z{n | }0 ;v5.0?=Q| 5S>9i..(0$I<Kv5.05eQ> ;Sw9i..i(|00E..0e}r)>u(=..X..fi0<bK$I.(5N8N<.L<(E<</<{BI(E..<Z#0N<fi0Zz{n | }0 ;~E$0d@<ZO89i.i<(/.0<W(E..<#G<]<b^.LKn}3 q] {NJL ZE. .fi. .fifi;WH9"9Z"{=L9 XbH[9HW9 []].Nb[H93HW[{Emb[u3[VLb?X L.?..X..XeZ y9H]r{33efiff W . (L9.L[rH9.H[]H$H933 W9[r.Cr[]]3.M3H .rHW [3. W9({r u;"@ reyr ..Xb9X."H]9= Wfi .?! W#" [W9 {W$ 39P % {W# N.]]HJ{L9.ff'&("*)fi+-,/., HH ]]99HW$5[H[V]bH[9H .[1m{02 be. LCrE r43 L T.."]5HMM 5L9..T{X[.GN.HX]3.3X[ be.=]J{r u["r. XX5W3[HJ L9.. ".]C"3W r[]]3.MCHH.rHW= {r 9.N"@ reyr769e9b)"39H T{) W 98 ]5.3=;(L9$N."[[].b:. 3 HW[R<;.=;b?>@ 3 rr @ r;yy BA C E F X.HFG .HH?V, H5 ZL9.. N[H]]9"r1I" JG .3X*Zff H[ {r u "r reyrK L=.e.9X$9#ff W! L9[3]?!) N{L9..M, 3$.[N H+ "H]]9[rff PXO 3W]9=ff{y 9er e K C. F. .X$9+ff W$.H [u L9..Qff [[[[W9MRW r"H]]9[r.S @ 3 " 9u?>@ [9b]DT .uryUZ 33 [u[;[?Vb 9F. b)X[.%W$ X b X 9 W 5H$H.[u[VZ L9\M ]" ^ G .HbZ []]3. HP.( [[[.9H[3.;3HW9.[[ HW[]G"rHH"HXr93H 9y @ reyyy L=.e9X$.3T[, W [].m_, W` Q($ .[[75a .[9?){L9.P9P 3Hb.O G"H..3. P"H.].X5% []H.[r HX]]9c. .BLd 9re% .39]9 [H..3.GfG WHgZG WH Hb[UJT L9y3X> @ E]ihjhT @<k jlm [)?9E_ Fb ?($ ."rB.O 9;N.".[bL(3H]9 ML9..'M"3W ..3H[D .)H r"]]3.M)HHR .rHW [H. 9P5.;= b ;er e[uD. .F. .X)n]d .?+ff L9 {[H<%C"o 3HCH933.[p 9 3"W ..3H[ @ ]{ be...;= b C e9bO. ;q (L9./8 $HE$ [C.[${9r5He W P[]].(" ^ G .HX9*LL3X> L{{{4lem [?<9X )b.r ].;N@ .3"bJO.1 eH W {93M]9sff L9.EM $N[]]3.M:Z) H93j.O 3H[ W ."[HH]9[r.]LL3X> L*tj*lm " [? 3.M.J$(.u9M]9HbH;F WM{PbO W .E& WF v)) 9{ % eL9.w+, .93J3x% r"H]]9[rW ..WH[." WHZ]59W9HW[ {r ue"@ reyr[y CXw. bXbz{zfiJournal Artificial Intelligence Research 2 (1995) 541-573Submitted 9/94; published 5/95Pac-learning Recursive Logic Programs:Negative ResultsWilliam W. CohenAT&T Bell Laboratories600 Mountain Avenue, Murray Hill, NJ 07974 USAwcohen@research.att.comAbstractcompanion paper shown class constant-depth determinate k-aryrecursive clauses eciently learnable. paper present negative results showingnatural generalization class hard learn Valiant's model paclearnability. particular, show following program classes cryptographicallyhard learn: programs unbounded number constant-depth linear recursiveclauses; programs one constant-depth determinate clause containing unboundednumber recursive calls; programs one linear recursive clause constant locality.results immediately imply non-learnability general class programs.also show learning constant-depth determinate program either two linearrecursive clauses one linear recursive clause one non-recursive clause hardlearning boolean DNF. Together positive results companion paper,negative results establish boundary ecient learnability recursive function-freeclauses.1. IntroductionInductive logic programming (ILP) (Muggleton, 1992; Muggleton & De Raedt, 1994)active area machine learning research hypotheses learning systemexpressed logic programming language. many different learning problemsconsidered ILP, including great practical interest (Muggleton, King,& Sternberg, 1992; King, Muggleton, Lewis, & Sternberg, 1992; Zelle & Mooney, 1994;Cohen, 1994b), class problems frequently considered reconstruct simplelist-processing arithmetic functions examples. prototypical problem sortmight learning append two lists. Often, sort task attempted usingrandomly-selected positive negative examples target concept.Based similarity problems studied field automatic programmingexamples (Summers, 1977; Biermann, 1978), (informally) call classlearning tasks automatic logic programming problems. number experimentalsystems built (Quinlan & Cameron-Jones, 1993; Aha, Lapointe, Ling, & Matwin,1994), experimental success automatic logic programming systems limited.One common property automatic logic programming problems presence recursion . goal paper explore analytic methods computational limitationslearning recursive programs Valiant's model pac-learnability (1984). (In brief,model requires accurate approximation target concept found polynomial time using polynomial-sized set labeled examples, chosen stochastically.)surprise nobody limitations exist, far obvious previousc 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiCohenresearch limits lie: provably fast methods learning recursivelogic programs, even fewer meaningful negative results.starting point investigation series positive learnability results appearing companion paper (Cohen, 1995). results show single constant-depthdeterminate clause constant number \closed" recursive calls pac-learnable.also show two-clause constant-depth determinate program consisting one nonrecursive clause one recursive clause type described pac-learnable,additional \hints" target concept provided.paper, analyze number generalizations learnable languages.show relaxing restrictions leads dicult learning problems: particular, learning problems either hard learning DNF (an open problemcomputational learning theory), hard cracking certain presumably secure cryptographic schemes. main contribution paper, therefore, delineationboundaries learnability recursive logic programs.paper organized follows. Section 2 define classes logic programslearnability models used paper. Section 3 present cryptographichardness results two classes constant-depth determinate recursive programs: programsn linear recursive clauses, programs one n-ary recursive clause. alsoanalyze learnability clauses constant locality, another class clauses paclearnable nonrecursive case, show even single linearly recursive localclause cryptographically hard learn. turn, Section 4, analysiseven restricted classes recursive programs. show two different classesconstant-depth determinate programs prediction-equivalent boolean DNF: classprograms containing single linear recursive clause single nonrecursive clause,class programs containing two linearly recursive clauses. Finally, summarizeresults paper companion, discuss related work, conclude.Although paper read independently companion paper suggestreaders planning read papers begin companion paper (Cohen, 1995).2. Backgroundcompleteness, present technical background needed state results;however, aside Sections 2.2 2.3, introduce polynomial predictabilityprediction-preserving reducibilities, respectively, background closely follows presented companion paper (Cohen, 1995). Readers encouraged skip sectionalready familiar material.2.1 Logic Programsassume reader familiarity logic programming (suchobtained reading one standard texts (Lloyd, 1987).) treatment logicprograms differs usually consider body clause orderedset literals. also consider logic programs without function symbols|i.e.,programs written Datalog.semantics Datalog program P defined relative database , DB ,set ground atomic facts. (When convenient, also think DB542fiPac-Learning Recursive Logic Programs: Negative Resultsconjunction ground unit clauses). particular, interpret P DB subsetset extended instances . extended instance pair (f; D)instance fact f ground fact, description set ground unit clauses.extended instance (f; D) covered (P; DB ) iffDB ^ ^ P ` fextended instances allowed, function-free programs encode many computations usually represented function symbols. example, function-freeprogram tests see list append two lists written follows:Program P :append(Xs,Ys,Ys)null(Xs).append(Xs,Ys,Zs)components(Xs,X,Xs1) ^components(Zs,X,Zs1) ^append(Xs1,Ys,Zs1).Database DB :null(nil).predicate components(A,B,C) means list head B tail C; thusextended instance equivalent append([1,2],[3],[1,2,3]) would instance factf = append (list12 ; list3 ; list123 ) description containing atoms:components(list12,1,list2), components(list2,2,nil),components(list123,1,list23), components(list23,2,list3),components(list3,3,nil)use extended instances function-free programs closely related \ attening"(Rouveirol, 1994; De Raedt & Dzeroski, 1994); experimental learning systems alsoimpose similar restriction (Quinlan, 1990; Pazzani & Kibler, 1992). Another motivationusing extended instances technical. (sometimes quite severe) syntacticrestrictions considered paper, often polynomial number possibleground facts|i.e., Herbrand base polynomial. Hence programs interpretedusual model-theoretic way would possible learn program equivalentgiven target simply memorizing appropriate subset Herbrand base. However,programs interpreted sets extended instances, trivial learning algorithmsbecome impossible; even extremely restricted program classes still exponential number extended instances size n. discussion foundcompanion paper (Cohen, 1995).define terminology logic programs usedpaper.2.1.1 Input/Output VariablesB1 ^ : : : ^ Br (ordered) definite clause, input variables literal Bivariables also appear clause B1 ^ : : : ^ Bi,1 ; variablesappearing Bi called output variables .543fiCohen2.1.2 Types Recursionliteral body clause recursive literal predicate symbolarity head clause. every clause program one recursiveliteral, program linear recursive . every clause program k recursiveliterals, program k-ary recursive . every recursive literal program containsoutput variables, program closed recursive.2.1.3 Depthdepth variable appearing (ordered) clause B1 ^ : : : ^ Br defined follows.Variables appearing head clause depth zero. Otherwise, let Bi firstliteral containing variable V , let maximal depth input variablesBi ; depth V +1. depth clause maximal depth variableclause.2.1.4 Determinacyliteral Bi clause B1 ^ : : : ^ Br determinate iff every possible substitutionunifies fact eDB ` B1 ^ : : : ^ Bi,1one maximal substitution DB ` Bi . clause determinateliterals determinate. Informally, determinate clausesevaluated without backtracking Prolog interpreter.term ij -determinate (Muggleton & Feng, 1992) sometimes used programsdepth i, determinate, contain literals arity j less. number experimental systems exploit restrictions associated limited depth determinacy (Muggleton& Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). learnability constant-depth determinate clauses also received formal study (Dzeroski,Muggleton, & Russell, 1992; Cohen, 1993a).2.1.5 Mode Constraints DeclarationsMode declarations commonly used analyzing Prolog code describing Prolog code;instance, mode declaration \components (+; ,; ,)" indicates predicate components used first argument input second third argumentsoutputs. Formally, define mode literal L appearing clause Cstring initial character predicate symbol L, j > 1j -th character \+" (j , 1)-th argument L input variable\," (j , 1)-th argument L output variable. (This definition assumesarguments head clause inputs; justified since consideringclauses behave classifying extended instances, ground.) mode constraintset mode strings R = fs1 ; : : :; sk g, clause C said satisfy mode constraintR p every literal L body C , mode L R.define declaration tuple (p; a0; R) p predicate symbol, a0integer, R mode constraint. say clause C satisfies declaration544fiPac-Learning Recursive Logic Programs: Negative Resultshead C arity a0 predicate symbol p, every literal L bodyC mode L appears R.2.1.6 Determinate Modestypical setting, facts database DB extended instances arbitrary:instead, representative \real" predicate, may obey certain restrictions. Let us assume database extended-instance facts drawn(possibly infinite) set F . Informally, mode determinate input positionsfacts F functionally determine output positions. Formally, f = p(t1 ; : : :; tk )fact predicate symbol p pff mode, define inputs (f; pff) hti1 ; : : :; ti i,i1 , : : : , ik indices ff containing \+", define outputs (f; pff)htj1 ; : : :; tj i, j1, : : : , jl indices ff containing \,". define modestring pff predicate p determinate F iffklfhinputs (f; pff); outputs (f; pff)i : f 2 Fgfunction. clause satisfies declaration Dec 2 DetDEC must determinate.set declarations containing modes determinate F denotedDetDEC F . Since paper set F assumed fixed, generally omitsubscript.2.1.7 Bounds Predicate Arityuse notation a-DB set databases contain facts arityless, a-DEC set declarations (p; a0; R) every string 2 Rlength + 1 less.2.1.8 Size Measureslearning models presented following section require learner use resources polynomial size inputs. Assuming predicates arityless constant allows simple size measures used. paper,measure size database DB cardinality; size extended instance (f; D)cardinality D; size declaration (p; a0; R) cardinality R;size clause B1 ^ : : : ^ Br number literals body.2.2 Model Learnability2.2.1 PreliminariesLet X set. call X domain , call elements X instances . Defineconcept C X representation subset X , define language Langset concepts. paper, rather casual distinctionconcept set represents; risk confusion referset represented concept C extension C . Two sets C1 C2extension said equivalent . Define example C pair (e; b) b = 1e 2 C b = 0 otherwise. probability distribution function, sample C545fiCohenX drawn according pair multisets + ; , drawn domain X accordingD, + containing positive examples C , , containing negative ones.Associated X Lang two size complexity measures , usefollowing notation:size complexity concept C 2 Lang written j C j .size complexity instance e 2 X written j ej .set, Sn stands set elements size complexity greatern. instance, Xn = fe 2 X : j ej ng Langn = fC 2 Lang : j C j ng.assume size measures polynomially related number bits neededrepresent C e; holds, example, size measures logic programsdatabases defined above.2.2.2 Polynomial Predictabilitydefine polynomial predictability follows. language Lang polynomiallypredictable iff algorithm PacPredict polynomial function m( 1 ; 1 ; ne ; nt)every nt > 0, every ne > 0, every C 2 Langn , every : 0 < < 1, every: 0 < < 1, every probability distribution function D, PacPredict followingbehavior:1. given sample + ; , C Xn drawn according containing leastm( 1 ; 1 ; ne ; nt) examples, PacPredict outputs hypothesis HeProb(D(H , C ) + D(C , H ) > ) <probability taken possible samples + , (if PacPredictrandomized algorithm) coin ips made PacPredict;2. PacPredict runs time polynomial 1 , 1 , ne , nt , number examples;3. hypothesis H evaluated polynomial time.algorithm PacPredict called prediction algorithm Lang, function m( 1 ; 1 ; ne ; nt ) called sample complexity PacPredict. sometimesabbreviate \polynomial predictability" \predictability".first condition definition merely states error rate hypothesismust (usually) low, measured probability distributiontraining examples drawn. second condition, together stipulationsample size polynomial, ensures total running time learner polynomial.final condition simply requires hypothesis usable weak senseused make predictions polynomial time. Notice worst caselearning model, definition allows adversarial choice inputs learner.546fiPac-Learning Recursive Logic Programs: Negative Results2.2.3 Relation Modelsmodel polynomial predictability well-studied (Pitt & Warmuth, 1990),weaker version Valiant's (1984) criterion pac-learnability . language Langpac-learnable iff algorithm PacLearn1. PacLearn satisfies requirements definition polynomial predictability,2. inputs + , , PacLearn always outputs hypothesis H 2 Lang.Thus language pac-learnable predictable.companion paper (Cohen, 1995), positive results expressed modelidentifiability equivalence queries, strictly stronger pac-learnability;is, anything learnable equivalence queries also necessarily pac-learnable.1Since paper contains negative results, use relatively weak modelpredictability. Negative results model immediately translate negative resultsstronger models; language predictable, cannot pac-learnable,identifiable equivalence queries.2.2.4 Background Knowledge Learningtypical ILP system, setting slightly different, user usually provides cluestarget concept addition examples, form database DB\background knowledge" set declarations. account additional inputsnecessary extend framework described setting learner acceptsinputs training examples. Following formalization used companionpaper (Cohen, 1995), adopt notion \language family".Lang set clauses, DB database Dec declaration, defineLang[DB ; Dec] set pairs (C; DB ) C 2 Lang C satisfies Dec .Semantically, pair denote set extended instances (f; D) covered(C; DB ). Next, DB set databases DEC set declarations, defineLang[DB ; DEC ] = fLang[DB ; Dec ] : DB2 DB Dec 2 DECgset languages called language family .extend definition predictability queries language families follows.language family Lang[DB; DEC ] polynomially predictable iff every language setpredictable. language family Lang[DB; DEC ] polynomially predictable iffsingle algorithm Identify(DB ; Dec ) predicts every Lang[DB ; Dec] familygiven DB Dec .usual model polynomial predictability worst-case choices targetconcept distribution examples. notion polynomial predictabilitylanguage family extends model natural way; extended model also worstcase possible choices database DB 2 DB Dec 2 DEC . worst-case1. equivalence query question form \is H equivalent target concept?" answeredeither \yes" counterexample. Identification equivalence queries essentially meanstarget concept exactly identified polynomial time using polynomial queries.547fiCohenmodel may seem unintuitive, since one typically assumes database DB providedhelpful user, rather adversary. However, worst-case model reasonablelearning allowed take time polynomial size smallest target conceptset Lang[DB ; Dec ]; means database given usertarget concept cannot encoded succinctly (or all) learning allowed taketime.Notice language family Lang[DB ; Dec] polynomially predictable, everylanguage family must polynomially predictable. Thus show familypolynomially predictable sucient construct one language familylearning hard. proofs paper form.2.3 Prediction-Preserving Reducibilitiesprinciple technical tool used negative results notion prediction-preservingreducibility , introduced Pitt Warmuth (1990). Prediction-preserving reducibilitiesmethod showing one language harder predict another. Formally,let Lang1 language domain X1 Lang2 language domain X2.say predicting Lang1 reduces predicting Lang2, denoted Lang1 Lang2 ,function fi : X1 ! X2 , henceforth called instance mapping , functionfc : Lang1 ! Lang2 , henceforth called concept mapping , following hold:1. x 2 C fi (x) 2 fc (C ) | i.e., concept membership preservedmappings;2. size complexity fc (C ) polynomial size complexity C |i.e., sizeconcept representations preserved within polynomial factor;3. fi (x) computed polynomial time.Note fc need computable; also, since fi computed polynomial time,fi(x) must also preserve size within polynomial factor.Intuitively, fc (C1) returns concept C2 2 Lang2 \emulate" C1|i.e., makedecisions concept membership|on examples \preprocessed"function fi . predicting Lang1 reduces predicting Lang2 learningalgorithm Lang2 exists, one possible scheme learning concepts Lang1would following. First, convert examples unknown concept C1domain X1 examples domain X2 using instance mapping fi .conditions definition hold, since C1 consistent original examples,concept fc (C1) consistent image fi ; thus running learningalgorithm Lang2 produce hypothesis H good approximationfc (C1). course, may possible map H back original language Lang1,computing fc ,1 may dicult impossible. However, H still used predictmembership C1: given example x original domain X1, one simply predictx 2 C1 true whenever fi (x) 2 H .Pitt Warmuth (1988) give rigorous argument approach leadsprediction algorithm Lang1 , leading following theorem.548fiPac-Learning Recursive Logic Programs: Negative ResultsTheorem 1 (Pitt Warmuth) Assume Lang1 Lang2. Lang1 polynomially predictable, Lang2 polynomially predictable.3. Cryptographic Limitations Learning Recursive ProgramsTheorem 1 allows one transfer hardness results one language another.useful number languages, known prediction hard breakingcryptographic schemes widely assumed secure. example, knownpredicting class languages accepted deterministic finite state automata\cryptographically hard", class languages accepted log-space bounded Turingmachines.section make use Theorem 1 previous cryptographic hardnessresults show certain restricted classes recursive logic programs hard learn.3.1 Programs n Linear Recursive Clausescompanion paper (Cohen, 1995) showed single linear closed recursive clauseidentifiable equivalence queries. section show programpolynomial number clauses identifiable equivalence queries, evenpolynomially predictable.Specifically, let us extend notion \family languages" slightly, letDLog[n; s] represent language log-space bounded deterministic Turing machinesstates accepting inputs size n less, usual semantics complexitymeasure.2 Also let d-DepthLinRecProg denote family logic programs containingdepth-d linear closed recursive clauses, containing number clauses.following result:Theorem 2 every n s, exists database DB n;s 2 1-DB declarationDec n;s 2 1-DetDEC sizes polynomial nDLog[n; s] 1-DepthLinRecProg[DB n;s ; Dec n;s ]Hence 1 1, d-DepthLinRecProg[DB; a-DetDEC ] uniformly polynomially predictable cryptographic assumptions.3Proof: Recall log-space bounded Turing machine (TM) input tape lengthn, work tape length log2 n initially contains zeros, finite state controlstate set Q. simplify proof, assume without loss generality tapeinput alphabets binary, single accepting state qf 2 Q,machine always erase work tape position work tape head far leftdecides accept input.time step, machine read tape squares input tape headwork tape head, based values current state q ,2. I.e., machine represents set inputs accepts, complexity number states.3. Specifically, language uniformly polynomially predictable unless following cryptographic problems solved polynomial time: solving quadratic residue problem, invertingRSA encryption function, factoring Blum integers. result holds cryptographic problems reduced learning DLOG Turing machines (Kearns & Valiant, 1989).549fiCohenwrite either 1 0 work tape,shift input tape head left right,shift work tape head left right,transition new internal state q 0deterministic machine thus specified transition function: f0; 1g f0; 1g Q ,! f0; 1g fL; Rg fL; Rg QLet us define internal configuration TM consist string symbolswritten worktape, position tape heads, internal state qmachine: thus configuration element setCON f0; 1glog2 n f1; : : :; log2 ng f1; : : :; ng Qsimplified specification machine transition function0 : f0; 1g CON ! CONcomponent f0; 1g represents contents input tape squareinput tape head.Notice machine whose worktape size bounded log n, cardinalityCON p = jQjn2 log2 n, polynomial n = jQj. use factconstructions.background database DB n;s follows. First, = 0; : : :; p, atomform coni(ci ) present. constant ci represent different internal configurationTuring machine. also arbitrarily select c1 represent (unique) acceptingconfiguration, add DB n;s atom accepting(c1). ThusDB n;s fcon (ci)gpi=1 [ faccepting (c1)gNext, define instance mapping. instance Turing machine's domainbinary string X = b1 : : :bn ; mapped fi extended instance (f; D)f accepting (c0 )ftruei gb 2X :b =1 [ ffalsei gb 2X :b =0description atoms effect defining predicate truei true iff i-thbit X \1", defining predicate falsei true iff i-th bit X\0". constant c0 represent start configuration Turing machine,predicate accepting(C) defined true iff Turing machine accepts inputX starting state C.let Dec n;s = (accepting ; 1; R) R contains modes coni (+) coni (,),= 1; : : :; p; truej falsej j = 1; : : :; n.Finally, concept mapping fc , let us assume arbitrary one-to-one mappinginternal configurations Turing machine predicate names550fiPac-Learning Recursive Logic Programs: Negative Resultscon0,: : : ,conp,1 start configuration (0log2 n ; 1; q0) maps con0 accepting configuration (0log2 n ; 1; qf ) maps con1. construct program fc (M )follows. transition 0(1; c) ! c0 0, c c0 CON , constructclause formaccepting(C)conj (C) ^ truei ^ conj 0 (C1) ^ accepting(C1).position input tape head encoded c, con j = (c),con j 0 = (c0). transition 0(0; c) ! (c0) 0 construct analogous clause,truei replaced falsei.Now, claim program P , machine accept startedconfiguration ci iffDB n;s ^ ^ P ` accepting (ci )hence construction preserves concept membership. perhaps easiest see considering action top-down theorem prover given goalaccepting (C ): sequence subgoals accepting (ci ), accepting (ci +1 ), : : : generatedtheorem-prover precisely parallel sequence configurations ci , : : : entered Turingmachine.easily verified size program polynomial n s,clauses linear recursive, determinate, depth one, completing proof.number ways result strengthened. Preciselyconstruction used used reduce class nondeterministic log-spacebounded Turing machines constant-depth determinate linear recursive programs.Further, slight modification construction used reduce class log-spacebounded alternating Turing machines (Chandra, Kozen, & Stockmeyer, 1981) constantdepth determinate 2-ary recursive programs. modification emulate configurationscorresponding universal states Turing machine clauses formaccepting(C)conj (C) ^ truei ^conj 10 (C1) ^ accepting(C1) ^conj 20 (C2) ^ accepting(C2).conj1 0 conj2 0 two successors universal configuration conj .strong result, since log-space bounded alternating Turing machines knownable perform every polynomial-time computation.3.2 Programs One n-ary Recursive Clauseconsider learning single recursive clause arbitrary closed recursion.Again, key result section observation expressive power:background database allows every log-space deterministic Turing machineemulated single recursive constant-depth determinate clause. leadsfollowing negative predictability result.551fiCohenTheorem 3 every n s, exists database DB n;s 2 3-DB declarationDec n;s 2 3-DetDEC sizes polynomial nDLog[n; s] 3-DepthRec[DB n;s ; Dec n;s ]Hence 3 3, d-DepthRec[DB n ; a-DetDEC ] uniformly polynomiallypredictable cryptographic assumptions.Proof: Consider DLOG machine . proof Theorem 2, assume withoutloss generality tape alphabet f0; 1g, unique starting configura-tion c0, unique accepting configuration c1. also assume withoutloss generality unique \failing" configuration cf ail; exactlyone transition form0(b; cj) ! c0jevery combination 2 f1; : : :; ng, b 2 f0; 1g, cj 2 CON , fc1; cf ailg. Thusinput X = x1 : : :xn machine starts CONFIG=c0 , executes transitionsreaches CONFIG=c1 CONFIG=cf ail, point X accepted rejected(respectively). use p number configurations. (Recall p polynomialn s.)emulate , convert example X = b1 : : :bn extended instancefi(X ) = (f; D)f accepting (c0 )fbit (bi)gni=1Thus predicate bit (X ) binds X i-th bit TM's input tape. alsodefine following predicates background database DB n;s .every possible b 2 f0; 1g j : 1 j p(n), predicate statusb;j (B,C,Y)defined given bindings variables B C , statusb;j (B,C,Y) failC = cf ail; otherwise succeed, binding active B = b C = cjbinding inactive otherwise.j : 1 j p(n), predicate nextj (Y,C) succeed iff boundeither active inactive. = , C bound cj ; otherwise, Cbound accepting configuration c1.database also contains fact accepting (c1 ).easy show size database polynomial n s.declaration Dec n;s defined (accepting ; 1; R) R includes modesstatus bj (+; +; ,), next j (+; ,), bit (,) b 2 f0; 1g, j = 1; : : :; p, = 1; : : :; n.Now, consider transition rule 0(b; cj ) ! c0j , corresponding conjunctionTRANSibj biti (Bibj ) ^ statusb;j (C,Bibj ,Yibj ) ^ nextj 0 (Yibj ,C1ibj ) ^ accepting(C1ibj )552fiPac-Learning Recursive Logic Programs: Negative ResultsGiven DB n;s D, assuming C bound configuration c, conjunctionfail c = cf ail. succeed xi 6= b c 6= cj ; case Yibj boundinactive, C1ibj bound c1, recursive call succeeds accepting(c1)DB n;s . Finally, xi = b c = cj , TRANSibj succeed atom accepting(cj 0 )provable; case, Yibj bound active C1ibj bound cj 0 .clear clause fc (M )^accepting(C)TRANSibj2f1;:::;ng; b2f0;1gj 2f1;:::;pgcorrectly emulate machine examples preprocessedfunction fi described above. Hence construction preserves concept membership.also easily verified size program polynomial n s,clause determinate depth three.3.3 One k-Local Linear Closed Recursive Clausefar considered one class extensions positive result givencompanion paper (Cohen, 1995)|namely, relaxing restrictions imposed recursivestructure target program. Another reasonable question ask linear closedrecursive programs learned without restriction constant-depth determinacy.earlier papers (Cohen, 1993a, 1994a, 1993b) studied conditionsconstant-depth determinacy restriction relaxed still allowing learnability nonrecursive clauses. turns generalizations constant-depthdeterminate clauses predictable, even without recursion. However, languagenonrecursive clauses constant locality pac-learnable generalization constant-depthdeterminate clauses. Below, define language, summarize relevant previousresults, address question learnability recursive local clauses.Define variable V appearing clause C free appears body Chead C . Let V1 V2 two free variables appearing clause. V1 touches V2appear literal, V1 uences V2 either touches V2, touchesvariable V3 uences V2. locale free variable V set literalseither contain V , contain free variable uenced V . Informally, variableV1 uences variable V2 choice binding V1 affect possible choicesbindings V2.locality clause size largest locale. Let k-LocalNonRec denotelanguage nonrecursive clauses locality k less. (That is, k-LocalNonRecset logic programs containing single nonrecursive k-local clause.) following factsknown (Cohen, 1993b):fixed k a, language family k-LocalNonRec[a-DB; a-DEC] uniformlypac-learnable.every constant d, every constant a, every database DB 2 a-DB, every declarationDec 2 a-DetDEC , every clause C 2 d-DepthNonRec[DB ; Dec ],553fiCohenequivalent clause C 0 k-LocalNonRec[DB ; Dec] size bounded kj C j , kfunction (and hence constant also constants.)Hencek-LocalNonRec[DB; a-DEC]pac-learnable generalizationd-DepthNonRec[DB; a-DetDEC ]thus plausible ask recursive programs k-local clauses pac-learnable.facts learnability k-local programs follow immediately previous results.example, immediate consequence construction Theorem 2 programspolynomial number linear recursive k-local clauses predictable k 2.Similarly, Theorem 3 shows single recursive k-local clause predictable k 4.still reasonable ask, however, positive result bounded-depth determinaterecursive clauses (Cohen, 1995) extended k-ary closed recursive k-local clauses.Unfortunately, following negative result, shows even linear closedrecursive clauses learnable.Theorem 4 Let Dfa[s] denote language deterministic finite automata states,let k-LocalLinRec set linear closed recursive k-local clauses. constant exists database DB 2 3-DB declaration Dec 2 3-DEC , sizepolynomial s,Dfa[s] 3-LocalLinRec[DB ; Dec ]Hence k 3 3, k-LocalLinRec[a-DB ; Dec] uniformly polynomiallypredictable cryptographic assumptions.Proof: Following Hopcroft Ullman (1979) represent DFA alphabettuple (q0; Q; F; ) q0 initial state, Q set states, F setaccepting states, : Q ! Q transition function (which sometimesthink subset Q Q). prove theorem, need construct databaseDB size polynomial every s-state DFA emulated linearrecursive k-local clause DB .Rather directly emulating , convenient emulate instead modification . Let M^ DFA state set Q^ Q [ fq(,1); qe ; qf g, q(,1) , qe qfnew states found Q. initial state M^ q(,1) . final state M^qf . transition function M^[^ [ f(q(,1); a; q0); (qe; c; qf )g [f(qi; b; qe)g2qi Fa, b, c new letters . Note M^ DFA alphabet[ fa; b; cg, and, described, need complete DFA alphabet. (Thatis, may pairs (qi ; a) ^(qi ; a) undefined.) However, M^ easily554fiPac-Learning Recursive Logic Programs: Negative Results1q0?0M^1q- ?1011qqqqq,10?-0M0-0?1b,c,0,11b-a,b,c1-ceq?ra,b,c-fa,b,c,0,1a,b,c,0,16a,b,0,1qqqqq,10?-00-?1,,,, b -ec-fFigure 1: DFA modified emulation local clause555fiCohenmade complete introducing additional rejecting state qr , making every undefinedtransition lead qr . precisely, let 0 defined0 ^ [ f(qi; x; qr) j qi 2 Q^ ^ x 2 [ fa; b; cg ^ (6 9qj : (qi ; x; qj ) 2 ^)gThus 0 = (q(,1); Q^ [fqr g; fqf g; 0) \completed" version M^ , Q0 = Q^ [fqr g.use 0 construction below; also let Q0 = Q^ [ fqr g 0 = [ fa; b; cg.Examples , M^ 0 shown Figure 1. Notice aside arcsrejecting state qr , state diagram 0 nearly identical .differences 0 new initial state q(,1) single outgoing arclabeled old initial state q0 ; also every final state 0 outgoing arclabeled b new state qe , turn single outgoing arc labeled c finalstate qf . easy showx 2 L(M ) iff axbc 2 L(M 0)Now, given set states Q0 define database DB contains followingpredicates:arcq ;;q (S,X,T) true 2 Q0, 2 Q0, X 2 0, unless = qi,X = , 6= qj .state(S) true 2 Q0.accept(c,nil,qe,qf ) true.motivation arc predicates, observe emulating 0 clearly usefulable represent transition function 0. usefulness arc predicatestransition function 0 represented using conjunction arc literals. particular,conjunction^arc q ;;q (S; X; )j(q ;;q )20jjsucceeds 0 (S; X ) = , fails otherwise.Let us define instance mapping fi fi (x) = (f; D)f = accept (a; xbc; q(,1); q0)set facts defines components relation list correspondsstring xbc. words, x = 1 : : :n , set factscomponents(1 : : :n bc; 1; 2 : : :n bc)components(2 : : :n bc; 2; 3 : : :n bc)...components(c,c,nil)declaration Dec n Dec n = (accept ; 4; R) R contains modescomponents (+; ,; ,), state (,), arc q ;;q (+; +; +) qi , qj Q0 , 2 0 .Finally, define concept mapping fc (M ) machine clausej556fiPac-Learning Recursive Logic Programs: Negative Resultsaccept(X,Ys,S,T)V(q ;;q )20 arcq ;;q (S,X,T)^ components(Ys,X1,Ys1) ^ state(U) ^ accept(X1,Ys1,T,U).0 transition function corresponding machine 0 defined above.easy show construction polynomial.clause X letter 0, Ys list letters, statesQ0 . intent construction predicate accept succeed exactly(a) string XYs accepted 0 0 started state , (b) first actiontaken 0 string XYs go state state .Since initial transitions 0 q(,1) q0 input a,predicate accept claimed behavior, clearly proposed mapping satisfies requirements Theorem 1. complete proof, therefore, must verifypredicate accept succeeds iff XYs accepted 0 state initial transitionT.definition DFAs string XYs accepted 0 state initialtransition iff one following two conditions holds.0(S; X ) = , Ys empty string final state 0, or;0(S; X ) = , Ys nonempty string (and hence head X 1 tailYs1) Ys1 accepted 0 state , initial transition.base fact accept(c,nil,qe,qf ) succeeds precisely first case holds, since0 transition one final state. second case, conjunctionarc conditions fc (M ) clause succeeds exactly (S; X ) = (as noted above).second conjunction clause succeeds Ys nonempty stringhead X 1 tail Ys1 X1Ys1 accepted 0 state initial transitionstate U , corresponds exactly second case above.Thus concept membership preserved mapping. completes proof.jj4. DNF-Hardness Results Recursive Programssummarize previous results determinate clauses, shown singlek-ary closed recursive depth-d clause pac-learnable (Cohen, 1995), set n linear closedrecursive depth-d clauses not; further, even single n-ary closed recursive depth-d clausespac-learnable. still large gap positive negative results,however: particular, learnability recursive programs containing constant numberk-ary recursive clauses yet established.section investigate learnability classes programs.show programs either two linear closed recursive clauses one linear closed recursive clause one base case hard learn boolean functions disjunctivenormal form (DNF). pac-learnability DNF long-standing open problem computational learning theory; import results, therefore, establishinglearnability classes require substantial advance computational learningtheory.557fiCohen4.1 Linear Recursive Clause Plus Base ClausePrevious work established two-clause constant-depth determinate programs consisting one linear recursive clause one nonrecursive clause identified, giventwo types oracles: standard equivalence-query oracle, \basecase oracle' (Cohen,1995). (The basecase oracle determines example covered nonrecursive clausealone.) section show absence basecase oracle, learningproblem hard learning boolean DNF.discussion below, Dnf[n; r] denotes language r-term boolean functionsdisjunctive normal form n variables.Theorem 5 Let d-Depth-2-Clause set 2-clause programs consisting oneclause d-DepthLinRec one clause d-DepthNonRec. nr exists database DB n;r 2 2-DB declaration Dec n;r 2 2-DEC , sizespolynomial n r,Dnf[n; r] 1-Depth-2-Clause[DB n;r ; Dec n;r ]Hence 2 1 language family d-Depth-2-Clause[DB; a-DetDEC ]uniformly polynomially predictable DNF polynomially predictable.Proof: produce DB n;r 2 DB Dec n;r 2 2-DetDEC predictingDNF reduced predicting 1-Depth-2-Clause[DB n;r ; Dec n;r ]. constructionmakes use trick first used Theorem 3 (Cohen, 1993a), DNF formulaemulated conjunction containing single variable existentially quantifiedrestricted range.begin instance mapping fi . assignment = b1 : : :bn convertedextended instance (f; D)f p(1)fbit (bi)gni=1Next, define database DB n;r contain binary predicates true1 , false1, : : : , truer ,falser behave follows:truei(X,Y) succeeds X = 1, 2 f1; : : :; rg , fig.falsei(X,Y) succeeds X = 0, 2 f1; : : :; rg , fig.Further, DB n;r contains facts define predicate succ(Y,Z) true wheneverZ = + 1, Z numbers 1 r. Clearly size DB n;rpolynomial r.Let Dec n;r = (p; 1; R) R contains modes bit (,), = 1; : : :; n; true j (+; +)false j (+; +), j = 1; : : :; r, succ (+; ,).let r-term DNF formula = _ri=1 ^sj =1 lij variables v1 ; : : :; vn.may assume without loss generality contains exactly r terms, since DNFformula fewer r terms padded exactly r terms adding terms558fiPac-Learning Recursive Logic Programs: Negative ResultsBackground database:= 1; : : :; rtruei (b; ) b; : b = 1 2 f1; : : :; rg 6=falsei (b; ) b; : b = 0 2 f1; : : :; rg 6=succ(y,z)z = + 1 2 f1; : : :; rg z 2 f1; : : :; rgDNF formula: (v1 ^ v3 ^ v4) _ (v2 ^ v3) _ (v1 ^ v4)Equivalent program:p(Y) succ(Y,Z)^p(Z).p(Y) bit1 (X1 ) ^ bit2 (X2 ) ^ bit3 (X3 ) ^ bit4 (X4 ) ^true1 (X1,Y) ^ false1 (X3 ,Y) ^ true1(X4 ,Y) ^false2 (X2,Y) ^ false2 (X3,Y)^true3 (X1,Y) ^ false3 (X4 ,Y).Instance mapping: fi(1011) = (p(1); fbit1(1); bit 2(0); bit3(1); bit4(1)g)Figure 2: Reducing DNF recursive programform v1 v1. define concept mapping fc () program CR; CB CRlinear recursive depth 1 determinate clausep(Y ) succ(Y; Z ) ^ p(Z )CB nonrecursive depth 1 determinate clausen^^r ^p(Y )bit k (Xk ) ^Biji=1 j =1k =1Bij defined follows:Bij(truei (Xk ,Y) lij = vkfalsei (Xk ,Y) lij = vkexample construction shown Figure 2; suggest reader referfigure point. basic idea behind construction first, clauseCB succeed variable bound i-th term succeeds (thedefinitions truei falsei designed ensure property holds); second,recursive clause CR constructed program fc () succeeds iff CB succeedsbound one values 1; : : :; n.argue rigorously correctness construction. Clearly, fi ( )fc () size respectively. Since DB n;r also polynomialsize, reduction polynomial.Figure 3 shows possible proofs constructed program fc ();notice program fc () succeeds exactly clause CB succeeds value559fiCohenp(1)A@A@AA @@B(1)succ(1,2) p(2)@A@AA @@B(2)succ(2,3) p(3)@A@AA @@B(3):::p(n-1)B (i) V bit (X ) ^ V V Bij@A@AA @B(n-1)@succ(n-1,n) p(n)B(n)Figure 3: Space proofs possible program fc ()Vs l must true;1r.Now,trueterm=j =1 ijVVs0case j =1 Bij succeeds bound value j =1 Bi0 j every i0 6= alsosucceeds bound i. hand, false assignment, Tifails, hence every possible binding generated repeated use recursiveclause CR base clause CB also fail. Thus concept membership preservedmapping.concludes proof.4.2 Two Linear Recursive ClausesRecall single linear closed recursive clause identifiable equivalencequeries (Cohen, 1995). construction similar used Theorem 5 usedshow result cannot extended programs two linear recursive clauses.Theorem 6 Let d-Depth-2-Clause0 set 2-clause programs consisting twoclauses d-DepthLinRec. (Thus assume base case recursion givenbackground knowledge.) constants n r exists database DB n;r 22-DB declaration Dec n;r 2 2-DEC , sizes polynomial n,Dnf[n; r] 1-Depth-2-Clause0[DB n;r ; Dec n;r ]Hence constants 2 1 language familyd-Depth-2-Clause0 [DB; a-DetDEC ]560fiPac-Learning Recursive Logic Programs: Negative Resultsuniformly polynomially predictable DNF polynomially predictable.Proof: before, proof makes use prediction-preserving reducibility DNFd-Depth-2-Clause0[DB ; Dec ] specific DB Dec . Let us assume DNFr terms, assume r = 2k . (Again, assumption made withoutloss generality, since number terms increased padding vacuousterms.) consider complete binary tree depth k + 1. k-th level treeexactly r nodes; let us label nodes 1, : : : , r, give nodes arbitrary labels.construct database DB n;r Theorem 5, except following changes:predicates truei (b,y) falsei(b,y) also succeed label nodelevel k.Rather predicate succ, database contains two predicates leftsonrightson encode relationship nodes binary tree.database includes facts p(!1), : : : , p(!2r), !1, : : : , !2r leavesbinary tree. used base cases recursive programlearned.Let label root binary tree. define instance mappingfi (b1 : : :b1) (p(); fbit1 (b1); : : :; bit n (bn )g)Note except use rather 1, identical instance mappingused Theorem 5. Also let Dec n;r = (p; 1; R) R contains modes bit (,), =1; : : :; n; true j (+; +) false j (+; +), j = 1; : : :; r; leftson (+; ,); rightson (+; ,).concept mapping fc () pair clauses R1; R2, R1 clausen^^r ^p(Y )bit k (Xk ) ^Bij ^ leftson(Y; Z ) ^ p(Z )k =1R2 clausep(Y )n^k =1bit k (Xk ) ^i=1 j =1^r ^i=1 j =1Bij ^ rightson (Y; Z ) ^ p(Z )Note clause linear recursive, determinate, depth 1. Also,construction clearly polynomial. remains show membership preserved.Figure 4 shows space proofsV constructedV V program fc ();Figure 3, B (i) abbreviates conjunction bit (Xi) ^ Bij . Notice programsucceed recursive calls manage finally recurse one base casesp(!1), : : : , p(!2r ), correspond leaves binary tree. clausessucceed first k , 1 levels tree. However, reach base casesrecursion leaves tree, recursion must pass k-th level tree;is, one clauses must succeed node binary tree,k-th level tree, hence label number 1 r.program thus succeeds fi ( ) precisely number 1561fiCohenp()", b@b"H"H,@bb"" ,@ b" ,@ bb""b,@"b",@b"b,@B() p(L)B()p(R)ZZ`` \ZX \\ZX\ZZ\ Z\ Z\ Z\\ ZEXXXEXEEE:::::::::BBBBB::::::B(1) p(LL: : : L) B(1) p(LL: : : R)p(!1 ):::JJJJJBBBBBB(n) p(RR: : : LR) B(n) p(RR: : : R)p(!2 )p(!2 ,1 )rp(!2 )rFigure 4: Proofs possible program fc ()r conjunction B(i) succeeds, (by argument given Theorem 5)happen satisfied assignment . Thus, mappings preserveconcept membership. completes proof.Notice programs fc () used proof property depthevery proof logarithmic size instances. means hardnessresult holds even one additionally restricts class programs logarithmicdepth bound.4.3 Upper Bounds Diculty Learningprevious sections showed several highly restricted classes recursive programsleast hard predict DNF. section show restrictedclasses also harder predict DNF.wish restrict depth proof constructed target program. Thus, leth(n) function; use Langh(n) set programs class Langproofs extended instance (f; D) depth bounded h(j Dj ).562fiPac-Learning Recursive Logic Programs: Negative ResultsTheorem 7 Let Dnf[n; ] language DNF boolean functions (with numberterms), recall d-Depth-2-Clause language 2-clause programs consisting one clause d-DepthLinRec one clause d-DepthNonRec,d-Depth-2-Clause0 language 2-clause programs consisting two clausesd-DepthLinRec.constants a, databases DB 2 DB declarations Dec 2 a-DetDEC ,polynomial function poly (n)d-Depth-2-Clause[DB ; Dec] Dnf[poly (j DB j ); ]d-Depth-2-Clause0h(n) [DB ; Dec] Dnf[poly (j DB j ); ] h(n) bounded c log nconstant c.Hence either language families uniformly polynomially predictable, Dnf[n; ]polynomially predictable.Proof: proof relies several facts established companion paper (Cohen, 1995).every declaration Dec, clause BOTTOM d(Dec) every nonrecursive depth-d determinate clause C equivalent subclause BOTTOM .Further, size BOTTOM polynomial Dec . means language subclauses BOTTOM normal form nonrecursive constant-depthdeterminate clauses.Every linear closed recursive clause CR constant-depth determinate equivalent subclause BOTTOM plus recursive literal Lr ; further,polynomial number possible recursive literals Lr .constants a, a0, d, database DB 2 a-DB, declaration Dec =(p; a0; R), database DB 2 a-DB , program P d-Depth-2-Clause[DB ; Dec ],depth terminating proof constructing using P hmax,hmax polynomial size DB Dec .assumed without loss generality database DB decsriptionscontain equality predicate , equality predicate simply predicateequal(X,Y) true exactly X = .idea proof contruct prediction-preserving reduction twoclasses recursive programs listed DNF. begin two lemmas.Lemma 8 Let Dec 2 a-DetDEC , let C nonrecursive depth-d determinate clauseconsistent Dec. Let SubclauseC denote language subclauses C , letMonomial[u] denote language monomials u variables. polynomial poly 1 database DB 2 DB,SubclauseC [DB ; Dec] Monomial[poly 1(j DB j )]563fiCohenProof lemma: Follows immediately construction used Theorem 1Dzeroski, Muggleton, Russell (Dzeroski et al., 1992). (The basic idea construction introduce propositional variable representing \success" connectedchain literals C . subclause C represented conjunctionpropositions.)lemma extended follows.Lemma 9 Let Dec 2 a-DetDEC , let = fC1; : : :; Crg set r nonrecursive depth-determinate clauses consistent Dec, length n less. Let SubclauseS denoteset programs form P = (D1; : : :; Ds) Di subclauseCj 2 .polynomial poly 2 database DB 2 DB,SubclauseS [DB ; Dec] Dnf[poly 2 (j DB j ; r); ]Proof lemma: Lemma 8, Ci 2 , set variables Vi sizepolynomial j DB jevery clause SubclauseC emulated monomialSrVV.Clearly,jV j polynomial n r, every clause. Let V =i=1SubclauseC also emulated monomial V . Further, every disjunctionr clauses represented disjunction monomials.Since Ci 's satisfy single declaration Dec = (p; a; R), headsprinciple function arity; further, may assume (without loss generality, sinceequality predicate assumed) variables appearing heads clausesdistinct. Since Ci's also nonrecursive, every programP 2 SubclauseSrepresented disjunction D1 _ : : : _ Dr i, Di 2 ( SubclauseC ). Henceevery P 2 SubclauseS represented r-term DNF set variables V .Let us introduce additional notation. C clauses, useC u denote result resolving C together, C denote resultresolving C times. Note C u unique C linear recursive Cpredicate heads (since one pair complementaryliterals.)Now, consider target programP = (CR; CB ) 2 d-Depth-2-Clause[DB ; Dec]CR recursive clause CB base. proof extended instance(f; D) must use clause CR repeatedly h times use clause CB resolve awayfinal subgoal. Hence nonrecursive clause CRh u CB could also used coverinstance (f; D).Since depth proof class programs bounded number hmaxpolynomial j DB j ne , nonrecursive programP 0 = fCRh u CB : 0 h hmax g564fiPac-Learning Recursive Logic Programs: Negative Resultsequivalent P extended instances size ne less.Finally, recall assume CB subclause BOTTOM ; also,polynomial-sized set LR = Lr1 ; : : :; Lr closed recursive literalsLr 2 LR , clause CR subclause BOTTOM [ Lr . means letpolynomial-sized setS1 = f(BOTTOM [ Lr )h u BOTTOM j 0 h hmax Lr 2 LR gP 0 2 SubclauseS1 . Thus Lemma 9, d-Depth-2-Clause Dnf. concludesproof first statement theorem.showd-Depth-2-Clause0h(n) [DB ; Dec] Dnf[poly (j DB j ; ]similar argument applies. Let us introduce notation, defineMESHh;n (CR1 ; CR2 ) set clauses formpCR 1 u CR 2 u : : : u CR 0j , CR = CR1 CR = CR2 , h0 h(n). Notice functionsh(n) c log n number clauses polynomial n.let p predicate appearing heads CR1 CR2 , let C^ (respectively^ ) version C (DB ) every instance predicate p replacedDBnew predicate p^. P recursive program P = fCR1 ; CR2 g d-Depth-2-Clause0^ ,database DB , P ^ DB equivalent4 nonrecursive program P 0 ^ DBi;iji;i;hijP 0 = fC^ j C 2 MESHh;n (CR1 ; CR2 )gerecall polynomial number recursive literals Lr , hencepolynomial number pairs recursive literals Lr ; Lr . means set clauses[S2 =fC^ j C 2 MESHh;n (BOTTOM [ Lr ; BOTTOM [ Lr )g(Lrie2;Lrj ) LR LRjjalso polynomial-sized; furthermore, program P language d-Depth-2-Clause,P 0 2 SubclauseS2 . second part theorem follows application Lemma 9.immediate corollary result Theorems 6 5 strengthenedfollows.Corollary 10 constants 1 2, language familyd-Depth-2-Clause[DB; a-DetDEC ]uniformly polynomially predictable DNF polynomially predictable.constants 1 2, language familyd-Depth-2-Clause0 [DB; a-DetDEC ]uniformly polynomially predictable DNF polynomially predictable.4. extended instances size n less.e565fiCohenThus important sense learning problems equivalent learning booleanDNF. resolve questions learnability languages,show learnability dicult formal problem: predictability boolean DNFlong-standing open problem computational learning theory.5. Related Workwork described paper differs previous formal work learning logic programs simultaneously allowing background knowledge, function-free programs, recursion. also focused exclusively computational limitations ecient learnabilityassociated recursion, considered languages known paclearnable nonrecursive case. Since results paper negative,concentrated model polynomial predictability; negative results model immediately imply negative result stronger model pac-learnability, also implynegative results strictly expressive languages.Among closely related prior results negative results previouslyobtained certain classes nonrecursive function-free logic programs (Cohen, 1993b).results similar character results described here, apply nonrecursivelanguages. Similar cryptographic results obtained Frazier Page (1993)certain classes programs (both recursive nonrecursive) contain function symbolsdisallow background knowledge.prior negative results also obtained learnability firstorder languages using proof technique consistency hardness (Pitt & Valiant, 1988).Haussler (1989) showed language \existential conjunction concepts" paclearnable showing hard find concept language consistentgiven set examples. Similar results also obtained two restricted languagesHorn clauses (Kietz, 1993); simple description logic (Cohen & Hirsh, 1994);language sorted first-order terms (Page & Frisch, 1992). results, however,specific model pac-learnability, none easily extended polynomialpredictability model considered here. results also extend languagesexpressive specific constrained languages. Finally, none languages allowrecursion.knowledge, negative learnability results first-order languages. discussion prior positive learnability results first-order languagesfound companion paper (Cohen, 1995).6. Summarypaper companion (Cohen, 1995) considered large number differentsubsets Datalog. aim comprehensive, systematic: particular, wished find precisely boundaries learnability lie various syntacticrestrictions imposed relaxed. Since easy reader \miss foresttrees", brie summarize results contained paper, togetherpositive results companion paper (Cohen, 1995).566fiPac-Learning Recursive Logic Programs: Negative ResultsLocalClausesConstant-Depth DeterminateClausesnCR,nCR,nCR jCB,nCR ; CB,k nCR,n nCR,kCR,kCR+kCRjCB+kCR; CBDNFk k0CRDNFn kCR,1CR,1CR+1CRjCB+1CR; CB=DNF2 1CR=DNFn 1CR,Table 1: summary learnability resultsThroughout papers, assumed polynomial amount backgroundknowledge exists; programs learned contain function symbols;literals body clause small arity. also assumed recursionclosed , meaning output variables appear recursive clause; however, believerestriction relaxed without fundamentally changing results paper.companion paper (Cohen, 1995) showed single nonrecursive constantdepth determinate clause learnable strong model identification equivalencequeries . learning model, one given access oracle counterexamples|thatis, oracle find, unit time, example current hypothesisincorrect|and must reconstruct target program exactly polynomial numbercounterexamples. result implies single nonrecursive constant-depth determinate clause pac-learnable (as counterexample oracle emulated drawingrandom examples pac setting). result novel (Dzeroski et al., 1992); howeverproof given independent, also independent interest. Notably, somewhatrigorous earlier proofs, also proves result directly, rather via reduction propositional learning problem. proof also introduces simple versionforced simulation technique, variants used positive results.showed learning algorithm nonrecursive clauses extendedcase single linear recursive constant-depth determinate clause, leadingresult restricted class recursive programs also identifiable equivalencequeries. bit effort, algorithm extended learn singlek-ary recursive constant-depth determinate clause.also considered extended learning algorithm learn recursive programs consisting one constant-depth determinate clauses. interesting extensionsimultaneously learn recursive clause CR base clause CB , using equivalencequeries also \basecase oracle" indicates counterexamples coveredbase clause CB . model, possible simultaneously learn recursive clausenonrecursive base case situations recursive clause learned567fiCohenLanguage Familyd-DepthNonRec[a-DB; a-DetDEC]d-DepthLinRec[a-DB; a-DetDEC]d-Depth-k-Rec[a-DB; a-DetDEC]d-Depth-2-Clause[a-DB; a-DetDEC]kd-MaxRecLang[a-DB; a-DetDEC ]d-Depth-2-Clause[a-DB; a-DetDEC]d-Depth-2-Clause [a-DB; a-DetDEC ]d-DepthLinRecProg[a-DB; a-DetDEC ]d-DepthRec[a-DB; a-DetDEC ]k-LocalLinRec[a-DB; a-DEC ]0B1001110000R0111112L/R Oracles, EQ1EQkEQ1EQ,BASEkEQ,BASE1EQ1EQn 1EQ1 nEQ1 1EQNotation LearnableCByes1CRyeskCRyes1CRjCB yeskCRjCByes1CR; CB =DNF2 1CR =DNFn 1CRnCR1CRTable 2: Summary language learnability results. Column B indicates numberbase (nonrecursive) clauses allowed program; column R indicates number recursive clauses; L/R indicates number recursive literals allowedsingle recursive clause; EQ indicates oracle equivalence queries BASEindicates basecase oracle. languages except k-LocalLinRec, clausesmust determinate depth d.alone; instance, one learn k-ary recursive clause together nonrecursivebase case. strongest positive result.results summarized Tables 1 2. Table 1, program one rary recursive clause denoted rCR, program one r-ary recursive clause onenonrecursive basecase denoted rCR; CB , rCRjCB \basecase" oracle,program different r-ary recursive clauses denoted rCR . boxed resultsassociated one theorems paper, companion paper,unmarked results corollaries results. \+" program class indicatesidentifiable equivalence queries; thus positive results describedsummarized four \+" entries lower left-hand corner section tableconcerned constant-depth determinate clauses.Table 2 presents information slightly different format, also relatesnotation Table 1 terminology used elsewhere paper.paper considered learnability various natural generalizationslanguages shown learnable companion paper. Consider moment singleclauses. companion paper showed fixed k single k-ary recursive constantdepth determinate clause learnable. showed restrictionsnecessary. particular, program n constant-depth linear recursive clausespolynomially predictable; hence restriction single clause necessary. Also, singleclause n recursive calls hard learn; hence restriction k-ary recursionnecessary. also showed restriction constant-depth determinate clausesnecessary, considering learnability constant locality clauses . Constant localityclauses known generalization constant-depth determinate clausespac-learnable nonrecursive case. However, showed recursion allowed,568fiPac-Learning Recursive Logic Programs: Negative Resultslanguage learnable: even single linear recursive clause polynomiallypredictable.Again, results summarized Table 1; \," program class meanspolynomially predictable, cryptographic assumptions, hence neitherpac-learnable identifiable equivalence queries.negative results based cryptographic hardness give upper bound expressiveness learnable recursive languages, still leave open learnability programsconstant number k-ary recursive clauses absence basecase oracle.final section paper, showed following problems are, modelpolynomial predictability, equivalent predicting boolean DNF:predicting two-clause constant-depth determinate recursive programs containing onelinear recursive clause one base case;predicting two-clause recursive constant-depth determinate programs containing twolinear recursive clauses, even base case known.note program classes nearly simplest classes multi-clauserecursive programs one imagine, pac-learnability DNF longstanding open problem computational learning theory. results suggest, therefore,pac-learning multi-clause recursive logic programs dicult; least,show finding provably correct pac-learning algorithm require substantial advancescomputational learning theory. Table 1, \= Dnf" (respectively Dnf) meanscorresponding language prediction-equivalent DNF (respectively least hardDNF).summarize Table 1: sort recursion, programs containingconstant-depth determinate clauses learnable. constant-depth determinaterecursive programs learnable contain single k-ary recursive clause(in standard equivalence query model) single k-ary recursive clause plus basecase (if \basecase oracle" allowed). classes recursive programs eithercryptographically hard, hard boolean DNF.7. ConclusionsInductive logic programming active area research, one broad class learningproblems considered area class \automatic logic programming" problems.Prototypical examples genre problems learning append two lists,multiply two numbers. target concepts automatic logic programming recursiveprograms, often, training data learning system simply examplestarget concept, together suitable background knowledge.topic paper pac-learnability recursive logic programs randomexamples background knowledge; specifically, wished establish computationallimitations inherit performing task. began positive results establishedcompanion paper. results show one constant-depth determinate closed k-aryrecursive clause pac-learnable, further, program consisting one recursiveclause one constant-depth determinate nonrecursive clause also pac-learnable givenadditional \basecase oracle".569fiCohenpaper showed positive results likely improved.particular, showed either eliminating basecase oracle learning two recursive clauses simultaneously prediction-equivalent learning DNF, even caselinear recursion. also showed following problems hard breaking (presumably) secure cryptographic codes: pac-learning n linear recursive determinate clauses,pac-learning one n-ary recursive determinate clause, pac-learning one linear recursivek-local clause.results contribute machine learning several ways. point viewcomputational learning theory, several results technically interesting. Oneprediction-equivalence several classes restricted logic programs boolean DNF;result, together others like (Cohen, 1993b), reinforces importance learnability problem DNF. paper also gives dramatic example adding recursionwidely differing effects learnability: constant-depth determinate clausesremain pac-learnable linear recursion added, constant-locality clauses become cryptographically hard.negative results show systems apparently learn larger class recursiveprograms must taking advantage either special properties target conceptslearn, distribution examples provided with. believelikely opportunity obtaining positive formal results areaidentify analyze special properties. example, many examplesFOIL learned recursive logic programs, made use \complete example sets"|datasets containing examples certain size, rather sets randomlyselected examples (Quinlan & Cameron-Jones, 1993). possible complete datasetsallow expressive class programs learned random datasets; fact,progress recently made toward formalizing conjecture (De Raedt & Dzeroski,1994).Finally, importantly, paper established boundaries learnabilitydeterminate recursive programs pac-learnability model. many plausible automatic programming contexts would highly desirable system offeredformal guarantees correctness. results paper provide upper boundsone hope achieve ecient, formally justified system learns recursiveprograms random examples alone.Acknowledgementsauthor wishes thank three anonymous JAIR reviewers number useful suggestions presentation technical content.ReferencesAha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication smalltraining sets. Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. LectureNotes Computer Science # 784.570fiPac-Learning Recursive Logic Programs: Negative ResultsBiermann, A. (1978). inference regular lisp programs examples. IEEE Transactions Systems, Man Cybernetics, 8 (8).Chandra, A. K., Kozen, D. C., & Stockmeyer, L. J. (1981). Alternation. JournalACM, 28, 114{113.Cohen, W. W. (1993a). Cryptographic limitations learning one-clause logic programs.Proceedings Tenth National Conference Artificial Intelligence Washington,D.C.Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. appear ArtificialIntelligence.Cohen, W. W. (1993c). Rapid prototyping ILP systems using explicit bias. Proceedings1993 IJCAI Workshop Inductive Logic Programming Chambery, France.Cohen, W. W. (1994a). Pac-learning nondeterminate clauses. Proceedings EleventhNational Conference Artificial Intelligence Seattle, WA.Cohen, W. W. (1994b). Recovering software specifications inductive logic programming. Proceedings Eleventh National Conference Artificial IntelligenceSeattle, WA.Cohen, W. W. (1995). Pac-learning recursive logic programs: ecient algorithms. JournalAI Research, 2, 501{539.Cohen, W. W., & Hirsh, H. (1994). learnability description logics equalityconstraints. Machine Learning, 17 (2/3).De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories PAC-learnable.Wrobel, S. (Ed.), Proceedings Fourth International Workshop InductiveLogic Programming Bad Honnef/Bonn, Germany.Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability determinate logicprograms. Proceedings 1992 Workshop Computational Learning TheoryPittsburgh, Pennsylvania.Frazier, M., & Page, C. D. (1993). Learnability recursive, non-determinate theories:basic results techniques. Proceedings Third International WorkshopInductive Logic Programming Bled, Slovenia.Haussler, D. (1989). Learning conjunctive concepts structural domains. Machine Learning, 4 (1).Hopcroft, J. E., & Ullman, J. D. (1979). Introduction Automata Theory, Languages,Computation. Addison-Wesley.Kearns, M., & Valiant, L. (1989). Cryptographic limitations learning Boolean formulaefinite automata. 21th Annual Symposium Theory Computing. ACMPress.571fiCohenKietz, J.-U. (1993). computational lower bounds computational complexityinductive logic programming. Proceedings 1993 European ConferenceMachine Learning Vienna, Austria.King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug designmachine learning: use inductive logic programming model structureactivity relationships trimethoprim analogues binding dihydrofolate reductase.Proceedings National Academy Science, 89.Lavrac, N., & Dzeroski, S. (1992). Background knowledge declarative bias inductiveconcept learning. Jantke, K. P. (Ed.), Analogical Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. LecturesArtificial Intelligence Series #642.Lloyd, J. W. (1987). Foundations Logic Programming: Second Edition. Springer-Verlag.Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.Journal Logic Programming, 19/20 (7), 629{679.Muggleton, S., & Feng, C. (1992). Ecient induction logic programs. Inductive LogicProgramming. Academic Press.Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structureprediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.Page, C. D., & Frisch, A. M. (1992). Generalization learnability: study constrainedatoms. Inductive Logic Programming. Academic Press.Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. MachineLearning, 9 (1).Pitt, L., & Warmuth, M. K. (1988). Reductions among prediction problems: difficulty predicting automata. Proceedings 3rd Annual IEEE ConferenceStructure Complexity Theory Washington, D.C. Computer Society PressIEEE.Pitt, L., & Valiant, L. (1988). Computational limitations learning examples. JournalACM, 35 (4), 965{984.Pitt, L., & Warmuth, M. (1990). Prediction-preserving reducibility. Journal ComputerSystem Sciences, 41, 430{467.Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Brazdil, P. B.(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notesComputer Science # 667.Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5 (3).572fiPac-Learning Recursive Logic Programs: Negative ResultsQuinlan, J. R. (1991). Determinate literals inductive logic programming. ProceedingsEighth International Workshop Machine Learning Ithaca, New York. MorganKaufmann.Rouveirol, C. (1994). Flattening saturation: two representation changes generalization. Machine Learning, 14 (2).Summers, P. D. (1977). methodology LISP program construction examples.Journal Association Computing Machinery, 24 (1), 161{175.Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11).Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers treebanks:machine learning approach. Proceedings Twelfth National ConferenceArtificial Intelligence Seattle, Washington. MIT Press.573fiJournal Artificial Intelligence Research 2 (1994) 131-158Submitted 4/94; published 12/94Wrap-Up: Trainable DiscourseModule Information ExtractionStephen SoderlandWendy LehnertDepartment Computer Science, University MassachusettsAmherst, 01003-4610soderlan@cs.umass.edulehnert@cs.umass.eduAbstractvast amounts on-line text available led renewed interest informationextraction (IE) systems analyze unrestricted text, producing structured representation selected information text. paper presents novel approachuses machine learning acquire knowledge higher level IE processing.Wrap-Up trainable IE discourse component makes intersentential inferencesidentifies logical relations among information extracted text. Previous corpusbased approaches limited lower level processing part-of-speech tagging,lexical disambiguation, dictionary construction. Wrap-Up fully trainable,automatically decides classifiers needed, even derives feature setclassifier automatically. Performance equals partially trainable discoursemodule requiring manual customization domain.1. Introductioninformation extraction (IE) system analyzes unrestricted, real world text newswirestories. contrast information retrieval systems return pointer entiredocument, IE system returns structured representation informationwithin text relevant user's needs, ignoring irrelevant information.first stage IE system, sentence analysis, identifies references relevant objectstypically creates case frame represent object. second stage, discourseanalysis, merges together multiple references object, identifies logical relationships objects, infers information explicitly identified sentence analysis.IE system operates terms domain specifications predefine types information relationships considered relevant application. Considerable domainknowledge used IE system: domain objects, relationships objects,texts typically describe objects relationships.Much domain knowledge automatically acquired corpus-based techniques. Previous work centered knowledge acquisition lower levelprocessing part-of-speech tagging lexical disambiguation. N-gram statisticshighly successful part-of-speech tagging (Church, 1988; DeRose, 1988). Weischedel(1993) used corpus-based probabilities part-of-speech tagging guideparsing. Collocation data used lexical disambiguation Hindle (1989), Brent(1993), others. Examples training corpus driven part-of-speechsemantic tagging (Cardie, 1993) dictionary construction (Riloff, 1993).c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiSoderland Lehnertpaper describes Wrap-Up (Soderland & Lehnert, 1994), first system automatically acquire domain knowledge higher level processing associated discourseanalysis. Wrap-Up uses supervised learning induce set classifiers training corpus representative texts, text accompanied hand-coded target output.implemented Wrap-Up ID3 decision tree algorithm (Quinlan, 1986), althoughmachine learning algorithms could selected.Wrap-Up fully trainable system unique decidesclassifiers needed domain, automatically derives feature setclassifier. user supplies definition objects relationships interestdomain training corpus hand-coded target output. Wrap-Up resthand coding needed tailor system new domain.Section 2 discusses IE task detail, introduces microelectronics domain,gives overview CIRCUS sentence analyzer. Section 3 describes Wrap-Up,giving details ID3 trees constructed discourse decision, featuresautomatically derived tree, requirements applying Wrap-Up newdomain. Section 4 shows performance Wrap-Up two domains comparesperformance partially trainable discourse component. Section 5 drawconclusions contribution research. detailed examplemicroelectronics domain given appendix.2. Information Extraction Tasksection gives overview information extraction illustrates IE processingsample text fragment microelectronics domain. discuss needtrainable IE components acquire knowledge new domain.2.1 Overview IEinformation extraction system operates two levels. First, sentence analysis identifiesinformation relevant IE application. discourse analysis,focus paper, takes output sentence analysis assemblescoherent representation entire text. done according predefined guidelines specify objects text relevant relationshipsobjects reported.Sentence analysis broken several stages, applying differenttypes domain knowledge. lowest level preprocessing, segments textwords sentences. word assigned part-of-speech tag possibly semantictag preparation processing. Different IE systems varying amountssyntactic parsing point. research sites participated ARPA-sponsoredMessage Understanding Conferences (MUC-3, 1991; MUC-4, 1992; MUC-5, 1993) foundrobust, shallow analysis pattern matching performed better elaborate,brittle, parsing techniques.CIRCUS sentence analyzer (Lehnert, 1990; Lehnert et al., 1992) shallow syntactic analysis identify simple syntactic constituents, distinguish active passivevoice verbs. shallow syntactic analysis sucient extraction task, uses132fiWrap-Up: Trainable Discourse Modulelocal linguistic patterns instantiate case frames, called concept nodes (CN's) usedCIRCUS.CN definition trigger word syntactic pattern relative word.Whenever trigger word occurs text, CIRCUS looks one syntactic buffersappropriate information extract. CN definitions extract informationsubject direct object, first testing active passive voice. CNdefinitions look prepositional phrase particular preposition. Examples CNextraction patterns particular domain shown Section 2.3.Discourse analysis starts output sentence analyzer, case setconcept nodes representing locally extracted information. work discourseoften involved tracking shifts topic speaker/writer's goals (Grosz & Sidner,1986; Liddy et al., 1993) resolving anaphoric references (Hobbs, 1978). Discourseprocessing IE system may concern issues,means main objective transforming bits pieces extracted informationcoherent representation.One first tasks discourse analysis merge together multiple referencesobject. domain company names important, involve recognizing equivalence full company name (\International Business Machines, Inc.")shortened forms name (\IBM") generic references (\the company", \theU.S. computer maker"). manually engineered rules seem unavoidable coreferencemerging. Another example merging domain object less specific referenceobject. microelectronics domain reference \DRAM" chips may mergedreference \memory" \I-line" process merged \lithography."Much work discourse analysis identify logical relationships extracted objects, represented pointers objects output. Discourse analysismust also able infer missing objects explicitly stated textcases split object multiple copies discard object erroneouslyextracted.current implementation Wrap-Up begins discourse processing coreferencemerging done separate module. primarily manual engineeringseems unavoidable coreference. Work underway extend Wrap-Up include IEdiscourse processing incorporating limited amount domain-specific code handlethings company name aliases generic references domain objects.Wrap-Up divides processing six stages, described fullySection 3. are:1. Filtering spuriously extracted information2. Merging objects attributes3. Linking logically related objects4. Deciding split objects multiple copies5. Inferring missing objects6. Adding default slot valuespoint example specific domain might help. following sections introduce microelectronics domain, illustrate sentence analysis discourse analysisshort example domain.133fiSoderland Lehnert2.2 Microelectronics Domainmicroelectronics domain one two domains targetted Fifth MessageUnderstanding Conference (MUC-5, 1993). According domain task guidelinesdeveloped MUC-5 microelectronics corpus, information extracted microchip fabrication processes along companies, equipment, devices associatedprocesses. seven types domain objects identified: entities (i.e.companies), equipment, devices, four chip fabrication processes (layering, lithography,etching, packaging).Identifying relationships objects equal importance domain identifying objects themselves. company must identified playing least one fourpossible roles respect microchip fabrication process: developer, manufacturer,distributor, purchaser/user. Microchip fabrication processes reportedassociated specific company least one roles. equipment objectmust linked process uses equipment, device object linkedprocess fabricates device. Equipment objects may point companymanufacturer equipment modules.following sample MUC-5 microelectronics domain two companiesfirst sentence, associated two lithography processes second sentence.GCA Sematech developers UV I-line lithography processes,GCA playing additional role manufacturer. lithography process linkedstepper equipment mentioned sentence one.GCA unveiled new XLS stepper, developedassistance Sematech. system availabledeep-ultraviolet I-line configurations.Figure 1 shows five domain objects extracted sentence analysis finalrepresentation text discourse analysis identified relationships objects. relationships directly indicated pointers objects.roles companies play respect microchip fabrication process indicatedcreating \microelectronics-capability" object pointers processcompanies.2.3 Extraction Patternssentence analysis identify GCA Sematech company names, extractdomain objects stepper equipment, UV lithography I-line lithography?CN dictionary domain includes extraction pattern \X unveiled" identifycompany names. subject active verb \unveiled" domain nearly alwayscompany developing distributing new device process. However, patternoccasionally pick company fails domain's reportability criteria. companyunveils new type chip discarded text specify fabricationprocess.Extracting company name \Sematech" dicult since pattern \assistanceX" reliable predictor relevant company names. always trade-offaccuracy complete coverage deciding extraction patterns reliable134fiWrap-Up: Trainable Discourse ModuleA. Five concept nodes extracted sentence analysis.EntityType: companyName: GCAEquipmentType: stepperName: XLSLithographyType: UVLithographyType: I-lineEntityType: companyName: SematechB. Final representation text discourse analysis.TemplateContents:ME-CapabilityManufacturer:Developer:Process:ME-CapabilityManufacturer:Developer:Process:EntityType: companyName: GCAEntityType: companyName: SematechLithographyType: UVEquipment:LithographyType: I-lineEquipment:EquipmentType: stepperName: XLSManufacturer:Status: in-developmentFigure 1: Output (A) sentence analysis (B) discourse analysisenough include CN dictionary. Including less reliable patterns increases coverageexpense spurious extraction. specific pattern \developedassistance X" reliable, missed dictionary construction tool(Riloff, 1993).many domain objects, equipment, devices, microchip fabricationprocesses, set possible objects predefined list keywords referobjects created. extraction pattern \unveiled X" looks direct objectactive verb \unveiled", instantiating equipment object keyword indicatingequipment type found. example equipment object type \stepper"created equipment name \XLS". stepper equipment also extracted135fiSoderland Lehnertpattern \X developed", looks equipment subject passive verb\developed". equipment object extracted third time keyword \stepper"itself, sucient instantiate stepper equipment object whether occursreliable extraction pattern.keyword \deep-ultraviolet" extraction pattern \available X" usedextract lithography object type \UV" second sentence. Another lithographyobject type \I-line" similarly extracted. Case frames created objectsidentified sentence analysis. set objects becomes input next stageprocessing, discourse analysis.2.4 Discourse Processingfull text fragment comes, likely references\GCA" \GCA Corp." One first jobs discourse analysis mergemultiple references. much harder task merge pronominal references genericreferences \the company" appropriate company name. partcoreference problem handled processes separate Wrap-Up.main job discourse analysis determine relationships objectspassed sentence analysis. Considerable domain knowledge needed makediscourse-level decisions. knowledge concerns writing style, specific phraseswriters typically use imply relationships referents given domain.phrase \<company> unveiled <equipment>" sucient evidence infer companydeveloper microelectronics process? word \unveiled" alone enough,since company unveiled new DRAM chip may developer newprocess. may simply using someone else's microelectronics process produce chip.inferences, particularly role company plays process, oftensubtle two human analysts may disagree output given text. humanperformance study task found experienced analysts agreed80% text interpretations domain (Will, 1993).World knowledge also needed relationships possible domain objects. lithography process may linked stepper equipment, steppers neverused layering, etching, packaging processes. delicate dependenciestypes process likely fabricate types devices. Knowledgekinds relationships typically reported domain also help guide discourse processing. Stories lithography, example, often give developer, manufacturer,distributor process, roles hardly ever mentioned packaging processes. Companies associated packaging tend limited purchaser/userpackaging technology.wide range domain knowledge needed discourse processing, relatedworld knowledge, writing style. next section discusses need trainable components levels IE processing, including discourse analysis. Wrap-Up usesmachine learning techniques avoid months manual knowledge engineering otherwiserequired develop specific IE application.136fiWrap-Up: Trainable Discourse Module2.5 Need Trainable IE Componentshighest performance ARPA-sponsored Fifth Message Understanding Conference(MUC-5, 1993) achieved cost nearly two years intense programming effort,adding domain-specific heuristics domain-specific linguistic patterns one one, followed various forms system tuning maximize performance. many real worldapplications, two years development time team half dozen programmers wouldprohibitively expensive. make matters worse, knowledge used one domaincannot readily transferred IE applications.Researchers University Massachusetts worked facilitate IE system development use corpus-driven knowledge acquisition techniques (Lehnert etal., 1993). 1991 purely hand-crafted UMass system highest performancesite MUC-3 evaluation. following year UMass ran hand-crafted system alternate system replaced key component output AutoSlog,trainable dictionary construction tool (Riloff, 1993). AutoSlog variant exhibited performance levels comparable dictionary based 1500 hours manual coding. Encouragedsuccess one trainable component, architecture corpus-driven systemdevelopment proposed uses machine learning techniques address numbernatural language processing problems (Lehnert et al., 1993). MUC-5 evaluation,output CIRCUS sentence analyzer sent TTG (Trainable Template Generator), discourse component developed Hughes Research Laboratories (Dolan, et al.,1991; Lehnert et al., 1993). TTG used machine learning techniques acquire muchneeded domain knowledge, still required hand-coded heuristics turn acquiredknowledge fully functioning discourse analyzer.remainder paper focus Wrap-Up, new IE discourse moduledevelopment explores possibility fully automated knowledge acquisitiondiscourse analysis. detailed following sections, Wrap-Up builds ID3 decisiontrees guide discourse processing requires hand-coded customization newdomain training corpus provided. Wrap-Up automatically decidesID3 trees needed domain derives feature set tree outputsentence analyzer.3. Wrap-Up, Trainable IE Componentsection describes Wrap-Up algorithm, decision trees used discourseanalysis, trees tree features automatically generated. concludediscussion requirements Wrap-Up experience porting newdomain.3.1 OverviewWrap-Up domain-independent framework IE discourse processing instantiated automatically acquired knowledge new IE application. trainingphase, Wrap-Up builds ID3 decision trees based representative set training texts,paired hand-coded output keys. ID3 trees guide Wrap-Up's processingrun time.137fiSoderland Lehnertrun time Wrap-Up receives input objects extracted text sentence analysis. objects represented case frame along listreferences text, location reference, linguistic patterns usedextract it. Multiple references object throughout text merged togetherpassing Wrap-Up. Wrap-Up transforms set objects discardingspurious objects, merging objects add attributes object, adding pointersobjects, inferring presence missing objects slot values.Wrap-Up six stages processing, set decision trees designedtransform objects passed one stage next.Stages Wrap-up Algorithm:1. Slot Filteringobject slot decision tree judges whether slot contains reliableinformation. Discard slot value object tree returns \negative".2. Slot MergingCreate instance pair objects type. Merge two objectsdecision tree object type returns \positive". stage mergeobject separately extracted attributes object.3. Link CreationConsider possible pairs objects might possibly linked. Add pointerobjects Link Creation decision tree returns \positive".4. Object SplittingSuppose object linked object B object C. Object Splittingdecision tree returns \positive", split two copies one pointing BC.5. Inferring Missing Objectsobject object pointing it, instance created decisiontree returns likely parent object. Create parent link\orphan" object unless tree returns \none". use decision treesLink Creation Object Splitting stages tie new parent objects.6. Inferring Missing Slot Valuesobject slot closed class possible values empty, create instancedecision tree returns context-sensitive default value slot, possibly\none".3.2 Decision Trees Discourse Analysiskey making machine learning work complex task discourse processingbreak problem number small decisions build separate classifier138fiWrap-Up: Trainable Discourse Moduleeach. six stages Wrap-Up described Section 3.1 setID3 trees, exact number trees depending domain specifications.Slot Filtering stage separate tree slot object domain; SlotMerging stage separate tree object type; Link Creation stage treepointer defined output structure; forth stages.MUC-5 microelectronics domain (as explained Section 2.2) required 91 decision trees: 20Slot Filtering stage, 7 Slot Merging, 31 Link Creation, 13 Object Splitting,7 Inferring Missing Objects , 13 Inferring Missing Slot Values.example Link Creation stage tree determines pointerslithography objects equipment objects. Every pair lithography equipment objectsfound text encoded instance sent Lithography-Equipment-Link tree.classifier returns \positive", Wrap-Up adds pointer two objectsoutput indicate equipment used lithography process.ID3 decision tree algorithm (Quinlan, 1986) used experiments, althoughmachine learning classifier could plugged Wrap-Up architecture. vectorspace approach might seem appropriate, performance would depend weightsassigned feature (Salton et al., 1975). hard see principled way assignweights heterogeneous features used Wrap-Up's classifiers (see Section 3.3), sincefeatures encode attributes domain objects others encode linguistic contextrelative position text.Let's look example Section 2.2 \XLS stepper" seeWrap-Up makes discourse decision whether add pointer UV lithography equipment object. Wrap-Up encodes instance LithographyEquipment-Link decision tree features representing attributes lithographyequipment objects, extraction patterns, relative position text.Wrap-Up's training phase, instance encoded every pair lithographyequipment objects training text. Training instances must classified positivenegative, Wrap-Up consults hand-coded target output provided training textclassifies instance positive pointer found matching lithographyequipment objects. creation training instances discussed fully Section3.4. ID3 tabulates often feature value associated positive negativetraining instance encapsulates statistics node tree builds.Figure 2 shows portion Lithography-Equipment-Link tree, showing path usedclassify instance UV lithography XLS stepper positive. parentheticalnumbers tree node show number positive negative training instances represented node. priori probability pointer lithography equipmenttraining corpus 34%, 282 positive 539 negative training instances.ID3 uses information gain metric select effective feature partitiontraining instances (p.89-90, Quinlan, 1986), case choosing equipment typetest root tree. feature alone sucient classify instancesequipment type modular equipment, radiation source, etching system,negative instances. Apparently types equipment never used lithographyprocesses (a useful bit domain knowledge).branch equipment type \stepper" leads node tree representing 202positive 174 negative training instances, raising probability link 54%. ID3139fiSoderland Lehnert(282 pos, 539 neg)Equipment-typemodularequipment...radiationsource(0 pos, 11 neg)etchingsystem...Stepperlithographysystem(0 pos, 125 neg)(0 pos, 15 neg)(80 pos, 141 neg)(202 pos, 174 neg)Lithography-type...G-line...E-beamI-lineopticalUV(15 pos, 27 neg)(6 pos, 25 neg)(2 pos, 31 neg)(87 pos, 20 neg)(27 pos, 14 neg)Distance-2......0-1(0 pos, 1 neg)(18 pos, 12 neg)(4 pos, 0 neg)Figure 2: decision tree pointers lithography equipment objects.recursively selects feature partition partition, case selecting lithographytype. branch UV lithography leads partition 27 positive 14 negativeinstances, contrast E-beam optical lithography nearly negativeinstances. next test distance, value -1 case since equipmentreference one sentence earlier lithography. branch leads leaf node4 positive negative instances, tree returns classification positiveWrap-Up adds pointer UV lithography stepper.example shows decision tree acquire useful domain knowledge:lithography never linked equipment etching systems, steppersoften used UV lithography hardly ever E-beam optical lithography. Knowledgesort could manually engineered rather acquired machine learning,hundreds rules needed might take weeks months effort create test.Consider another fragment text tree Figure 3 decides whether addpointer PLCC packaging process ROM chip device.: : :anew line 256 Kbit 1 Mbit ROM chips.available PLCC priced : : :instance classified Packaging-Device-Link tree includes featurespackaging type, device type, distance two referents, extraction patternsused sentence analysis.140fiWrap-Up: Trainable Discourse Module(325 pos, 750 neg)Distance(0 pos, 12 neg)......-50-20(7 pos, 40 neg)500-1(60 pos, 70 neg)(130 pos, 93 neg)(0 pos, 12 neg)Device-type...EPROM(6 pos, 2 neg)memory(0 pos, 11 neg)...DRAMROM(13 pos, 2 neg)none(1 pos, 4 neg)(0 pos, 19 neg)pp-available-1truefalse(13 pos, 0 neg)(0 pos, 2 neg)Figure 3: tree pointers packaging device objects.ID3 selects \distance" root tree, feature counts distance sentences packaging device references text. closest references20 sentences apart, hardly training instances positive.distance -1 example text, ROM device mentioned one sentence earlierPLCC packaging process. Figure 3 shows, branch distance -1 followedtest device type. branch device type ROM leads partition15 instances, 13 positive 2 negative. PLCC packaging found pattern\available X" (encoded pp-available-1) positive instances.two trees illustrate different trees learn different types knowledge.significant features determining whether equipment object linked lithography process real world constraints type equipment used lithography.ected tree Figure 2 choosing equipment type root node followed lithography type. overriding constraint type devicelinked packaging technique. linguistic clues play prominent role,relative position references text particular extraction patterns.following section discusses linguistic-based features encoded.3.3 Generating Features ID3 TreesLet's look detail Wrap-Up encodes ID3 instances, using information availablesentence analysis automatically derive features used tree. ID3tree handles discourse decision domain object relationship pairobjects, different stages Wrap-Up involving different sorts decisions.141fiSoderland Lehnertinformation encoded object comes concept nodes extractedsentence analysis. Concept nodes case frame slots extracted information, also location extraction patterns reference text.Consider example Section 2.2.GCA unveiled new XLS stepper, developedassistance Sematech. system availabledeep-ultraviolet I-line configurations.Sentence analysis extracts five objects text: company GCA, equipment XLS stepper, company Sematech, UV lithography, I-line lithography. Oneseveral discourse decisions made whether UV lithography uses XLS steppermentioned previous sentence. Figure 4 shows two objects form basisinstance Lithography-Equipment-Link tree.EquipmentType: stepperName: XLSLithographyType: UVExtraction Patterns:pp-available-inkeyword-deep-ultravioletExtraction Patterns:obj-active-unveiledsubj-passive-developedkeyword-stepperFigure 4: Two objects extracted sample textobject includes location reference patterns used extractthem. extraction pattern combination syntactic pattern specific lexicalitem \trigger word" (as explained Section 2.1). pattern pp-available-in meansreference UV lithography found prepositional phrase following triggers\available" \in".Figure 5 shows instance UV lithography XLS stepper. encodes attributes extraction patterns object relative position text. WrapUp encodes case frame slot object using actual slot value closed classeslithography type. Open class slots equipment names encodedvalue \t" indicate name present, rather actual name. Usingexact name would result enormous branching factor feature might overlyuence ID3 classification low frequency name happened occur positivenegative instances.Extraction patterns encoded binary features include trigger wordsyntactic pattern feature name. Patterns two trigger words \pp-availablein" split two features, \pp-available" \pp-in". instances encode pairobjects features encoded \pp-available-1" \pp-in-1" referfirst object. count many extraction patterns used also encoded142fiWrap-Up: Trainable Discourse Module(lithography-type . UV)(extraction-count-1 . 3)(pp-available-1 . t)(pp-in-1 . t)(keyword-deep-ultraviolet-1 . t)(equipment-type . stepper)(equipment-name . t)(extraction-count-2 . 3)(obj-unveiled-2 . t)(subj-passive-developed-2 . t)(keyword-stepper-2 . t)(common-triggers . 0)(common-phrases . 0)(distance . -1)Figure 5: instance Lithography-Equipment-Link tree.object. feature \extraction-count" motivated Slot Filtering stagesince objects extracted several times likely valid extractedtwice text.Another type feature, encoded instances involving pairs objects, relativeposition references two objects, may significant determining twoobjects related. One feature easily computed distance sentencesreferences. case feature \distance" value -1, since XLS stepper foundone sentence earlier UV lithography process. Another feature might indicatestrong relationship objects count many common phrases containreferences objects. features list \common triggers", words includedextraction patterns objects. example would word \using"text phrase \the XLS stepper using UV technology".important realize included instance. human makingdiscourse decision might reason follows. sentence UV lithography indicatesassociated \the system", refers back \its new XLS stepper"previous sentence. Part reasoning involves domain independent use definitearticle, part requires domain knowledge \system" nonspecific referenceequipment object. current version Wrap-Up look beyond informationpassed sentence analysis misses reference \the system" entirely.Using specific linguistic patterns resulted extremely large, sparse feature setstrees. Lithography-Equipment-Link tree 1045 features, 11 encodingextraction patterns. Since typical instance participates dozen extractionpatterns, serious time space bottle neck would occur hundreds linguisticpatterns present explicitly listed instance. implementedsparse vector version ID3 able eciently handle large feature spacestabulating small number true-valued features instance.links added discourse processing, objects may become complex, includingmany pointers objects. time Wrap-Up considers links companiesmicroelectronics processes, lithography object may pointer equipmentobject device object, equipment object may turn pointersobjects. Wrap-Up allows objects inherit linguistic context position textobjects point. object pointer object B, location143fiSoderland Lehnertextraction patterns references B treated references A. versioninheritance helpful, little strong, ignoring distinction directreferences inherited references.looked encoding instances isolated discourse decisionssection. entire discourse system complex series decisions, affectingenvironment used processing. training phase must ect changingenvironment run time well provide classifications training instance basedtarget output. issues discussed next section.3.4 Creating Training InstancesID3 supervised learning algorithm requires set training instances, labeledcorrect classification instance. create instances Wrap-Up beginstree building phase passing training texts sentence analyzer, createsset objects representing extracted information. Multiple referencesobject merged form initial input Wrap-Up's first stage. Wrap-Up encodesinstances builds trees stage, repeats process using trees stage onebuild trees stage two, forth trees built six stages.encodes instances, Wrap-Up repeatedly consults target output assignclassification training instance. building trees Slot Filtering stageinstance classified positive extracted information matches slot targetoutput. Consider example reference \Ultratech stepper" microelectronicstext. Sentence analysis creates equipment object two slots filled, equipment typestepper equipment name \Ultratech". stage Wrap-Up separate ID3 treejudge validity slot, equipment type equipment name.Suppose target output equipment object type \stepper"\Ultratech" actually manufacturer's name equipment model name.equipment type instance classified positive equipment name instance classified negative since equipment object target output name Ultratech.instance include features capture human analyst would consider\Ultratech" equipment name? human probably using world knowledgerecognize Ultratech familiar company name recognize names\Precision 5000" familiar equipment names. Knowledge lists known companynames known equipment names presently included Wrap-Up, althoughcould derived easily training corpus.create training instances second stage Wrap-Up, entire training corpusprocessed again, time discarding slot values spurious according SlotFiltering trees creating instances Slot Merging trees. instance createdpair objects type. objects mapped objecttarget output, instance classified positive. example, instance wouldcreated pair device objects, one device type RAM size 256KBits. positive instance output single device object type RAMsize 256 KBits.time instances created later stages Wrap-Up, errors creptprevious stages. Errors filtering, merging, linking resulted144fiWrap-Up: Trainable Discourse Moduleobjects retained longer match anything target output objectspartially match target output. Since degree error unavoidable,best let training instances ect state processing occur laterWrap-Up used process new texts. training perfectly filtered, merged,linked, representative underlying probabilities run time useWrap-Up.later stages Wrap-Up objects may become complex partially match anything target output. aid matching complex objects, one slot objecttype identified output structure definition key slot. object consideredmatch object output key slots match. Thus object missingequipment name spurious equipment name still match equipment type, keyslot, matches. object pointer object B, object matching outputmust also pointer object matching B.recursive matching becomes important Link Creation stage. Amonglast links considered microelectronics roles company plays towards process.company may developer x-ray lithography process uses ABC stepper,developer x-ray lithography process linked different equipment object.Wrap-Up needs sensitive distinctions classifying training instances treesLink Creation Object Splitting stages.Instances Inferring Missing Objects stage Inferring Missing Slot Valuesstage classifications go beyond simple positive negative. instanceInferring Missing Objects stage created whenever object found traininghigher object pointing it. matching object indeed exists target output,Wrap-Up classifies instance type object points output.example training text may reference \stepper" equipment,mention process uses stepper. target output lithographyobject type \unknown" points stepper equipment. legitimate inference make, since steppers type lithography equipment. instanceorphaned stepper equipment object classified \lithography-unknown-equipment".classification gives Wrap-Up enough information run time create appropriate object.instance Inferring Missing Slot Values created whenever slot missingobject closed class possible values, \status" slot equipmentobjects, value \in-use" \in-development". matching objectfound target output, actual slot value used classification. slotempty object exists output, instance classified negative.Inferring Missing Objects stage, negative likely classification many trees.Next consider effects tree pruning confidence thresholds makeID3 cautious aggressive classifications.3.5 Confidence Thresholds Tree Pruningmachine learning technique tendency toward \overfitting", makinggeneralizations based accidental properties training data. ID3likely happen near leaf nodes decision tree, partition size may145fiSoderland Lehnertgrow small ID3 select features much predictive power. feature chosendiscriminate among half dozen training instances likely particularinstances useful classifying new instances.implementation ID3 used Wrap-Up deals problem setting pruning level confidence threshold tree empirically. new instance classifiedtraversing decision tree root node node reached partitionsize pruning level. classification halts node classificationpositive returned proportion positive instances greater equalconfidence threshold.high confidence threshold make ID3 tree cautious classifications,low confidence threshold allow positive classifications. effect changingconfidence threshold pronounced pruning level increases. largeenough pruning level, nearly branches terminate internal nodes confidencesomewhere 0.0 1.0. low confidence threshold classifyinstances positive, high confidence threshold classify negative.Wrap-Up automatically sets pruning level confidence threshold tree usingtenfold cross-validation. training instances divided ten sets settested tree built remaining nine tenths training. donevarious settings find settings optimize performance.metrics used domain \recall" \precision", rather accuracy.Recall percentage positive instances correctly classified, precisionpercentage positive classifications correct. metric combines recallprecision f-measure, defined formula f = (fi 2 + 1)P R=(fi 2P + R) fiset 1 favor balanced recall precision. Increasing decreasing fi selectedtrees fine-tune Wrap-Up, causing select pruning confidence thresholdsfavor recall favor precision.seen Wrap-Up automatically derives classifiers needed featureset classifier, tunes classifiers recall/precision balance.look requirements using Wrap-Up, special attention issuemanual labor system development.3.6 Requirements Wrap-UpWrap-Up domain-independent architecture applied domainwell defined output structure, domain objects represented case framesrelationships objects represented pointers objects. appropriateinformation extraction task important identify logical relationshipsextracted information. user must supply Wrap-Up output definitionlisting domain objects extracted. output object one slots,may contain either extracted information pointers objects output.One slot object labeled key slot, used training match extractedobjects objects target output.domain application already well defined, user able createoutput definition less hour. new application, whose information needs established, likely certain amount trial error146fiWrap-Up: Trainable Discourse Moduledeveloping desired representation. need well defined domain uniquediscourse processing trainable components Wrap-Up. IE systems require clearly defined specifications types objects extractedrelationships reported.time consuming requirement Wrap-Up associated acquisitiontraining texts importantly, hand-coded target output. hand-coded targetsrepresent labor-intensive investment part domain experts, knowledgenatural language processing machine learning technologies needed generateanswer keys, domain expert produce answer keys use Wrap-Up.thousand microelectronics texts used provide training Wrap-Up. actualnumber training instances training texts varied considerably decisiontree. Trees handled common domain objects ample training instancestwo hundred training texts, dealt less frequent objectsrelationships undertrained thousand texts.easier generate hundred answer keys write explicitcomprehensive domain guidelines. Moreover, domain knowledge implicitly present setanswer keys may go beyond conventional knowledge domain expertreliable patterns information transcend logical domain model. available,corpus training texts used repeatedly knowledge acquisition levelsprocessing.architecture Wrap-Up depend particular sentence analyzerparticular information extraction task. used sentence analyzer useskeywords local linguistic patterns extraction. output representation producedWrap-Up could either used directly generate database entries MUC-like taskcould serve internal representation support information extraction tasks.3.7 Joint Ventures DomainWrap-Up implemented tested microelectronics domain, triedanother domain, MUC-5 joint ventures domain. information extracteddomain companies involved joint business ventures, products services,ownership, capitalization, revenue, corporate ocers, facilities. Relationshipscompanies must sorted identify partners, child companies, subsidiaries.output structure complex microelectronics, back-pointers, cyclesoutput structure, redundant information, longer chains linked objects.Figure 6 shows text joint ventures domain diagram target output.pointers back-pointers, output even moderately complicated textbecomes dicult understand glance. text describes joint ventureJapanese company, Rinnai Corp., unnamed Indonesian company build factoryJakarta. tie-up identified Rinnai Indonesian company partnersthird company, joint venture itself, child company. output includes\entity-relationship" object duplicates much information tie-up object.corporate ocer, amount capital, ownership percentages, product \portablecookers", facility also reported output.147fiSoderland LehnertRINNAI CORP., JAPAN'S LEADING GAS APPLIANCE MANUFACTURER, SETJOINT VENTURE INDONESIA AUGUST PRODUCE PORTABLE COOKERSLOCAL USERS, PRESIDENT SUSUMU NAITO SAID MONDAY.NEW FIRM CAPITALIZED ONE MILLION DOLLARS,RINNAI SCHEDULED PUT 50 PCT LOCAL DEALER 50 PCT, SAID.MANUFACTURE 3,000 4,000 UNITS MONTH INITIALLY PLANT26,000-SQUARE-METER SITE JAKARTA, NAITO SAID, ADDING RINNAI AIMSSTART FULL-SCALE PRODUCTION NEXT SPRING.NAGOYA-BASED COMPANY SEVEN OVERSEAS PRODUCTION UNITS.TemplateDoc-Nr: 1485Content:Tie-UpStatus: existingEntity:Joint-venture:Ownership:Activity:ActivitySite: (Industry:EntityType: companyLocation: IndonesiaRelationship:Facility:EntityType: companyName: Rinnai CorpAliases: "Rinnai"Location: Nagoya, JapanRelationship:Person:EntityType: companyNationality: IndonesiaRelationship:FacilityType: factoryLocation: Jakarta,IndonesiaPersonName: Susumu NaitoPosition: presEntity:RelationshipEntity-1:Entity-2:Relation: childStatus: future)IndustryType: ProductionProduct:"portablecookers"OwnershipCapital: 1000000 $Ownership-Percent: (Owned:50) (50)Figure 6: sample text target output joint ventures domain.148fiWrap-Up: Trainable Discourse Modulespecial handling required joint ventures domain since outputstructure defined MUC-5 evaluation included slots activity siteownership percent whose values mixture extracted information pointers.slot values internal structure thought pseudoobjects,activity site object pointers facility object company, ownershippercent object pointer company another slot giving numeric value.pseudoobjects reformulated standard objects conforming requirementsWrap-Up, activity site slot pointing activity site object forth.transformed back complex slot fills printing final representationoutput.output specifications joint ventures less well-behaved ways,graph cycles, back pointers, redundant objects whose content must agree information elsewhere output. Modifications Wrap-Up needed relax implicitrequirements domain structure, allowing graph cycles giving special handlingpointer slot user labeled output definition back pointer.Joint ventures also implicit constraints relationships objects.company play single role tie-up joint venture relationship: cannotjoint venture child also parent partner company. Wrap-Up dicultylearning constraint performed better certain pointer slots labeled\single-role" constraint output definition.strategy letting user indicate constraints annotating slots outputdefinition implemented ad hoc fashion. general approach would allowuser declare several types constraint output. pointer slot may requiredoptional, may one pointer allow several. slots object maymutually exclusive, entry one prohibiting entry another slot. mayrequired agreement value slot one object slot another object.fully domain-independent discourse tool needs mechanism implement generalizedconstraints.4. System Performancepoint comparison performance Wrap-Up, UMass/Hughes systemrun TTG discourse module, used ocial MUC-5 evaluation. Overall system performance Wrap-Up compared performance TTG,holding rest system constant.Wrap-Up takes idea TTG extends fully trainable system. TTGused decision trees acquire domain knowledge, often relied hand-coded heuristicsapply acquired knowledge, particular decisions splitting mergingobjects, Wrap-Up handles Object Splitting stage; inferring missing objects,Wrap-Up Inferring Missing Objects stage; adding context sensitivedefault slot values, Wrap-Up Inferring Missing Slot Values stage.Several iterations hand tuning required adjust thresholds decision treesproduced TTG, whereas Wrap-Up found thresholds pruning levels optimize recallprecision tree automatically. day CPU time devoted decision treetraining, Wrap-Up produced working system programming needed.149fiSoderland Lehnertcomparison TTG made microelectronics domainjoint ventures domain. metrics used recall precision. Recall percentage possible information reported. Correctly identifying two fivepossible company names gives recall 40. Precision percent correct reportedinformation. four companies reported, two correct, precision 50.Recall precision combined single metric f-measure, defined f =(fi 2 + 1)P R=(fi 2P + R), fi set 1 balanced recall precision.4.1 Microelectronics DomainWrap-Up's scores ocial MUC-5 microelectronics test sets generally littlehigher TTG, overall recall precision.Wrap-UpRec. Prec. FTTGRec. Prec.Part 1Part 232.3 44.4 37.436.3 38.6 37.427.1 39.5 32.132.7 37.0 34.7Part 334.6 37.7 36.134.7 40.5 37.5Avg.34.4 40.2 36.831.5 39.0 34.8FFigure 7: Performance MUC-5 microelectronics test setsput scores perspective, highest scoring systems MUC-5 evaluationf-measures high 40's. dicult task sentence analysisdiscourse analysis.Another way assess Wrap-Up measure performance baselineprovided output sentence analysis. Lack coverage sentence analyzerplaces ceiling performance discourse level. test set part 1 208company names extracted. CIRCUS analyzer extracted total 404 companynames, 131 correct 2 partially correct, giving baseline 63% recall 33%precision slot. Wrap-Up's Entity-Name-Filter tree managed discard littlehalf spurious company names, keeping 77% good companies. resulted49% recall 44% precision slot, raising f-measure 5 points,expense recall.Limited recall extracted objects compounded comes linksobjects. half possible companies third microelectronics processesmissing, discourse processing chance large proportion possible linkscompanies processes.Although precision often increased expense recall, Wrap-Up also mechanisms increase recall slightly. Inferring Missing Objects stage infers missingprocess equipment object Object Splitting stage splits process pointsmultiple equipment, Wrap-Up sometimes gain recall producedsentence analyzer.150fiWrap-Up: Trainable Discourse Module4.2 Joint Ventures Domainjoint ventures domain Wrap-Up's scores MUC-5 test sets little lowerocial UMass/Hughes scores. Wrap-Up tended lower recall slightlyhigher precision.Wrap-UpRec. Prec. FTTGRec. Prec.Part 1Part 223.5 52.9 32.522.7 53.6 31.926.0 53.9 35.126.0 52.1 34.7Part 323.3 51.4 32.127.7 49.7 35.6Avg.23.2 52.7 32.226.5 52.0 35.1FFigure 8: Performance MUC-5 joint ventures test setsperformance Wrap-Up TTG roughly comparable twodomains. systems tend favor domain first developed, WrapUp developed microelectronics ported joint ventures, opposite trueTTG. certain amount bias probably crept design decisions meantdomain independent system. higher scores TTG joint venturespartly due hand-coded heuristics altered output TTG printing finaloutput, something done TTG microelectronics Wrap-Up eitherdomain.noticeable difference Wrap-Up TTG output joint venturesdomain filtering spuriously extracted company names. Discourse processingstarted 38% recall 32% precision sentence analysis company names.systems included filtering stage attempted raise precision discarding spuriouscompanies, expense discarding valid companies well.system used threshold settings control cautiously aggressively discardingdone (as example Section 3.5). TTG's set hand Wrap-Up'sselected automatically cross-validation training set. TTG mild filteringslot, resulting gain 2 precision points drop 6 recall points. Wrap-Upchose aggressive settings gained 13 precision points lost 17 points recallslot.result, Wrap-Up ended two thirds many correct companiesTTG. turn meant two thirds many pointers companies tie-ups entityrelationships. objects Wrap-Up scored higher recall TTG, gettingthree times total recall activity, industry, facility objects.5. Conclusionsrecent accessibility large on-line text databases news services, needinformation extraction systems growing. systems go beyond information retrievalcreate structured summary selected information contained within relevant documents. gives user ability skim vast amounts text, pulling information151fiSoderland Lehnertparticular topic. IE systems knowledge-based, however, must individuallytailored information needs application.research laboratories focused sophisticated user interfaces easeburden knowledge acquisition. GE's NLToolset example approach (Jacobs etal., 1993), BBN typifies systems combine user input corpus-based statistics(Ayuso et al., 1993). University Massachusetts moving directionmachine learning create fully trainable IE system. ultimate goal turnkeysystem tailored new information needs users special linguistictechnical expertise.Wrap-Up embodies goal. user defines information need output structure,provides training corpus representative texts hand-coded target outputtext. Wrap-Up takes instantiates fully functional IE discoursesystem new domain customization needed user. Wrap-Upfirst fully trainable system handle discourse processing,degradation performance. automatically decides classifiers needed baseddomain output structure derives feature set classifier sentenceanalyzer output.intriguing aspect Wrap-Up automatic generation features.effective this, trees actually learn? greatest leverage seemscome features encode attributes domain objects. trees microelectronicsoften based classification probabilities conditioned device type, equipmenttype, process type. example tree Section 3.2 first tested equipment typelithography type determining whether piece equipment used lithographyprocess. type real world domain knowledge important thingWrap-Up learned microelectronics.Useful knowledge also provided features encoded relative positionreferences text. Distance, measured number sentences apart, played prominentrole many classifications, trees relying fine-grained featuresnumber times references noun phrase overlappinglinguistic context.enhancement Wrap-Up's feature generation would increase expressivenessrelative position. addition direct references object object B, Wrap-Upcould look indirect references (pronominal anaphoric) found near referencesB vice versa. instance shown Section 3.3 example featuresindirect relationships might useful.Wrap-Up currently encodes instance pair objects might related,incapable expressing rule \attach object B recent object type A."blind existence objects alternate candidates relationshipconsidered. Features could encoded ect whether object recentlymentioned object type.features least successful tantalizing encodedlocal linguistic context, extraction patterns. included exact lexical itemnearly low frequency added noise often aiding usefuldiscriminations. Tree pruning partial solution, experiment combiningsemantically similar terms caused sharp drop classification accuracy.152fiWrap-Up: Trainable Discourse ModuleLow frequency terms built-in problem system processes unrestrictedtext. Dunning (93) estimated 20-30% typical English news wire reports composedwords frequency less one 50,000 words. Yet discourse decisions madehuman reader often seem hinge use one infrequent terms.challenging open question find methods utilize local linguistic context withoutdrowning noise produced low frequency terms.Finding mechanism choosing appropriate features critical machine learning algorithm applied. ID3 chosen easy implement, althoughapproaches vector spaces worth trying. obvious, however, craftweighting scheme gives greatest weight useful features vectorspace nearly zero useful making desired discrimination. CostSalzberg (1993) describe weighting scheme nearest neighbor algorithm lookspromising lexically-based features. Another candidate effective classifier backpropagation network, might naturally converge weights give uenceuseful features.hope Wrap-Up inspire machine learning community consider analysisunrestricted text fruitful application ML research, challenging naturallanguage processing community consider ML techniques complex processing tasks.broader context, Wrap-Up provides paradigm user customizable system design,technological background part user assumed. fully functionalsystem brought new domain without need months developmenttime, signifying substantial progress toward fully scalable portable natural languageprocessing systems.Appendix A: Walk-through Sample Textsee Wrap-Up algorithm action, consider sample text Figure 9.desired output company, Mitsubishi Electronics America, Inc., linked purchaser/user two packaging processes, TSOP SOJ packaging. processespoint device, 1 Mbit DRAM. packaging material, plastic, attachedTSOP SOJ. details text considered extraneousdomain.sentence analysis, followed step merges multiple references,eight objects passed input Wrap-Up. Sentence analysis fairly well identifyingrelevant information, missing \1 M" reference 1 MBits. Threeeight objects spurious discarded Wrap-Up's Slot Filtering stage.According domain guidelines, name \Mitsubishi Electronics America, Inc."reported, \The Semiconductor Division ...". packaging material EPOXYdevice MEMORY also discarded.Slot Filtering stage creates instance slot object. EntityName-Filter tree classifies \Mitsubishi Electronics America, Inc." positive instance,\The Semiconductor Division ..." negative discarded. reliablediscriminator valid company names \extraction-count", selected rootfeature tree. Training instances participating several extraction patternstwice likely valid extracted twice. held true text.153fiSoderland LehnertSemiconductor Division Mitsubishi Electonics America, Inc. offers1M CMOS DRAMs Thin Small-Outline Packaging (TSOP*), providinghighest memory density available industry. Developed Mitsubishi,TSOP also lets designers increase system memory density standardreverse "mirror image," pin-outs. Mitsubishi's 1M DRAM TSOP providesdensity chip-on-board much higher reliabilityplastic epoxy-resin package allows device 100% burned-in fullytested. *Previously referred VSOP (very small-outline package) USOP(ultra small-outline package). 1M DRAM TSOP height 1.2 mm,plane measurement 16.0 mm x 6.0 mm, lead pitch 0.5 mm, makingnearly three times thinner four times smaller volume 1MDRAM SOJ package. SOJ height 3.45 mm, plane dimension17.15 mm x 8.45 mm, lead pitch 1.27 mm. Additionally, TSOPweighs 0.22 grams, contrast 0.75 gram weight SOJ.Full text available PTS New Product Announcements.Figure 9: microelectronics textEntityType: companyName:Mitsubishi ElectronicsAmerica, Inc.EntityType: companyName: Semiconductor DivisionMitsubishi Electronics America, Inc.DeviceType: DRAMPackagingType: TSOPDeviceType: MEMORYPackagingMaterial: EPOXYPackagingMaterial: PLASTICPackagingType: SOJFigure 10: Input Wrap-Up sample text154fiWrap-Up: Trainable Discourse Module\Mitsubishi Electronics America, Inc." extraction count 5, spurious nameextracted 2 patterns.Slot Filtering stage continues, packaging material EPOXY classified negative Packaging-Material-Filter tree, whose root test packaging type. turnsEPOXY usually extracted erroneously training corpus. contrastsmaterial PLASTIC usually reliable classified positive. TSOPSOJ packaging types classified positive Packaging-Type-Filter tree. Instancestypes usually positive training set, particularly extracted multipletimes text. Device-Type-Filter tree, root feature device type, findsDRAM reliable device type MEMORY usually spurious trainingcorpus. usually merged specific device type.Slot Merging stage Wrap-Up considers pair remaining objectstype. three packaging objects, one type TSOP, one materialPLASTIC, one type SOJ. Packaging-Slotmerge tree easily rejects TSOPSOJ instance, since packaging objects never multiple types training. testingsecond object packaging type, feature \distance" tested. ledpositive classification TSOP-PLASTIC, sentence, negativeSOJ-PLASTIC, nearest references two sentences apart. point four objectsremain:EntityType: companyName:Mitsubishi ElectronicsAmerica, Inc.PackagingType: TSOPMaterial: PLASTICDeviceType: DRAMPackagingType: SOJLink Creation stage considers pair objects could linked accordingoutput structure. first links considered pointers packaging deviceobjects. Separate instances Packaging-Device-Link tree created possibleTSOP-DRAM link possible SOJ-DRAM link. Although 25% traininginstances positive, tree found 78% positive packaging type TSOP\distance" 0 sentences, 77% positive packaging type SOJ devicetype DRAM. testing features, tree found instancespositive pointers added output. Notice tree interleaves knowledgetypes packaging types devices knowledge relative positionreferences text.next Link Creation decision concern roles Mitsubishi plays towardspackaging processes. output structure \microelectronics-capability" objectone slot pointing lithography, layering, etching, packaging process, fourslots (labeled developer, manufacturer, distributor, purchaser/user) pointing companies. Wrap-Up accordingly encodes four instances Mitsubishi TSOP packaging,one possible role. done Mitsubishi SOJ packaging.Instances Mitsubishi roles developer, manufacturer, distributorclassified negative. Training instances trees almost positive instances.155fiSoderland LehnertTemplateDoc-Nr: 2523814Contents:ME-CapabilityPurchaser/User:Developer:Process:ME-CapabilityPurchaser/User:Process:PackagingType: TSOPMaterial: plasticDevice:PackagingType: SOJDevice:EntityType: companyName: MitsubishiElectronicsAmerica, Inc.DeviceType: DRAMFigure 11: Final output links addedseems stories packaging processes corpus almost exclusivelycompanies purchasing using someone else's packaging technology.seldom explicit linguistic clues relationship company processcorpus, Packaging-User-Link tree tests first relative distancereferences. 20% training instances positive, distance 0 jumped43% positive. Mitsubishi sentence TSOP Mitsubishi-SOJinstance also distance 0 inheritance. Even though nearest reference SOJtwo sentences Mitsubishi, SOJ linked DRAM occurs sentenceMitsubishi. instances classified positive testing packaging typefeatures.last discourse decision Link Creation stage add pointers microelectronics capability \template object", created dummy root objectdomain's output. Object Splitting stage finally gets make decision, albeit vacuousone, decides let template object point multiple objects \content" slot.\orphan" objects missing slot values last two stages Wrap-Upconsider. final output text shown Figure 11.Acknowledgementsresearch supported NSF Grant no. EEC-9209623, State/Industry/UniversityCooperative Research Intelligent Information Retrieval.156fiWrap-Up: Trainable Discourse ModuleReferencesAyuso, D., Boisen, S., Fox, H., Gish, H., Ingria, R., & Weischedel, R. (1992). BBN: Description PLUM System Used MUC-4. Proceedings Fourth MessageUnderstanding Conference, 169-176. Morgan Kaufmann Publishers.Brent, M. (1993). Robust Acquisition Subcategorization Frames. ProceedingAssociation Computational Linguistics.Cardie, C. (1993). Case-Based Approach Knowledge Acquisition Domain-SpecificSentence Analysis. Proceedings Eleventh National Conference ArtificialIntelligence, 798-803.Church, K. (1988). stochastic parts program noun phrase parser unrestricted text.Proceedings Second Conference Applied Natural Language ProcessingACL, 136-143.Cost, S., & Salzberg, S. (1993). Weighted Nearest Neighbor Algorithm LearningSymbolic Features. Machine Learning, 10(1), 57-78.DeRose, S. (1988). Grammatical Category Disambiguation Statistical Optimization.Computational Linguistics, 14(1), 31-39.Dolan, C. P., Goldman, S. R., Cuda, T. V., & Nakamura, A. M. (1991). Hughes TrainableText Skimmer: Description TTS System used MUC-3. ProceedingsThird Message Understanding Conference, 155-162. Morgan Kaufmann Publishers.Dunning, T. (1993). Accurate Methods Statistics Surprise Coincidence. Computational Linguistics, 19(1), 61-74.Grosz, B., & Sidner C. (1986). Attention, intention structure discourse. Computational Linguistics, 12(3), 175-204.Hindle, D. (1989). Acquiring Disambiguation Rules Text. Proceeding Association Computational Linguistics, 118-125.Hobbs, J. (1978). Resolving Pronoun References. Lingua, 44(4), 311-338.Jacobs, P., Krupka, G., Rau, L., Mauldin, M., Mitamura, T., Kitani, T., Sider, I., &Childs, L. (1993). GE-CMU: Description SHOGUN System used MUC5. Proceedings Fifth Message Understanding Conference, 109-120. MorganKaufmann Publishers.Lehnert, W. (1990). Symbolic/Subsymbolic Sentence Analysis: Exploiting Best TwoWorlds. Advances Connectionist Neural Computation Theory. vol. 1.. Norwood,NJ: Ablex Publishing, 151-158.Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Riloff, E., & Soderland, S. (1992).University Massachusetts: Description CIRCUS System Used MUC-4.Proceedings Fourth Message Understanding Conference, 282-288. MorganKaufmann Publishers.157fiSoderland LehnertLehnert, W., McCarthy, J., Soderland, S., Riloff, E., Cardie, C., Peterson, J., Feng, F.,Dolan, C., & Goldman, S. (1993). UMass/Hughes: Description CIRCUS SystemUsed MUC-5. Proceedings Fifth Message Understanding Conference,257-259. Morgan Kaufmann Publishers.Liddy, L., McVearry, K., Paik, W., Yu, E., & McKenna, M. (1993). Development, Implementation, Testing Discourse Model Newspaper Texts. ProceedingsHuman Language Technology Workshop, 159-164. Morgan Kaufmann Publishers.MUC-3. (1991). Proceedings Third Message Understanding Conference. Morgan Kaufmann Publishers.MUC-4. (1992). Proceedings Fourth Message Understanding Conference. MorganKaufmann Publishers.MUC-5. (1993). Proceedings Fifth Message Understanding Conference. Morgan Kaufmann Publishers.Quinlan, J.R. (1986). Induction Decision Trees. Machine Learning, 1, 81-106.Riloff, E. (1993). Automatically Constructing Dictionary Information ExtractionTasks. Proceedings Eleventh National Conference Artificial Intelligence,811-816.Salton, G., Wong, A., & Yang, C.S. (1975). vector space model automatic indexing.Correspondences ACM, 18(11), 613-620.Soderland, S., & Lehnert, W. (1994). Corpus-Driven Knowledge Acquisition DiscourseAnalysis. Proceedings Twelfth National Conference Artificial Intelligence,827-832.Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., & Palmucci, J. (1993). CopingAmbiguity Unknown Words Probabilistic Models. ComputationalLinguistics, 19(2), 359-382.Will, C. (1993). Comparing human machine performance natural language information extraction: Results English microelectronics MUC-5 evaluation.Proceedings Fifth Message Understanding Conference, 53-67. Morgan Kaufmann Publishers.158fiJournal Artificial Intelligence Research 2 (1995) 361{367Submitted 8/94; published 3/95Research NoteInformativeness DNAPromoter Sequences Domain TheoryJulio OrtegaComputer Science Dept., Vanderbilt UniversityP.O. Box 1679, Station BNashville, TN 37235 USAjulio@vuse.vanderbilt.eduAbstractDNA promoter sequences domain theory database become populartesting systems integrate empirical analytical learning. note reports simplechange reinterpretation domain theory terms M-of-N concepts, involvinglearning, results accuracy 93.4% 106 items database. Moreover,exhaustive search space M-of-N domain theory interpretations indicatesexpected accuracy randomly chosen interpretation 76.5%, maximumaccuracy 97.2% achieved 12 cases. demonstrates informativenessdomain theory, without complications understanding interactions variouslearning algorithms theory. addition, results help characterize dicultylearning using DNA promoters theory.1. IntroductionDNA promoter sequences domain theory database, contributed M. NoordewierJ. Shavlik UCI repository (Murphy & Aha, 1992), become populartesting systems integrate empirical analytical learning (Hirsh & Japkowicz, 1994;Koppel, Feldman, & Segre, 1994b; Mahoney & Mooney, 1994, 1993; Norton, 1994; Opitz &Shavlik, 1994; Ortega, 1994; Ourston, 1991; Towell, Shavlik, & Noordewier, 1990; Shavlik,Towell, & Noordewier, 1992). original domain theory, usually interpreted, overlyspecific classifies promoter sequences database negative instances. Since database consists 53 positive instances 53 negative instances,accuracy database 50%. learning systems cited take advantageinitial domain theory order achieve higher accuracy rates, especially fewertraining examples, rates achieved purely inductive methods C4.5backpropagation. Thus, informativeness theory acknowledged, despite 50%accuracy rate using naive interpretation. However, extent theoryinformative easily ascertained; implicit interactionslearning algorithms theory. note reports simple change reinterpretationdomain theory terms M-of-N concepts, involve learning, resultsaccuracy 93.4% 106 data items. Moreover, exhaustive search spaceM-of-N interpretations reveals achieve 97.2% accuracy.c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiOrtegapromotercontactconformationminus_35p-37=cp-36=tp-35=tp-34=gp-33=ap-32=cp-36=tp-35=tp-34=gp-32=cp-31=ap-36=tp-35=tp-34=gp-33=ap-32=cp-31=aminus_10p-36=tp-35=tp-34=gp-33=ap-32=cp-14=tp-13=ap-12=tp-11=ap-10=ap-9=tp-13=tp-12=ap-10=ap-8=tp-13=tp-12=ap-11=tp-10=ap-9=ap-8=tp-12=tp-11=ap-7=tp-47=cp-46=ap-45=ap-43=tp-42=tp-40=ap-39=cp-22=gp-18=tp-16=cp-8=gp-7=cp-6=gp-5=cp-4=cp-2=cp-1=cp-45=ap-44=ap-41=ap-49=ap-44=t0-27=tp-22=ap-18=tp-16=tp-15=gp-1=ap-45=ap-41=ap-28=tp-27=tp-23=tp-21=ap-20=ap-17=tp-15=tp-4=aFigure 1: DNA Promoters Theory2. DNA Promoter Sequences Database Domain Theorysources UCI promoter sequences database domain theory describedTowell (1990). rules theory derived biological researchO'Neill (1989). negative examples contiguous strings long DNA sequencebelieved contain promoters. positive examples promoters takencompilation Hawley McClure (1983). database recently augmented (Harley & Reynolds, 1987; Lisser & Margalit, 1993) new theories promoteraction still appearing (Lisser & Margalit, 1993). Nevertheless, UCI promoter sequences database examples domain theory remained prominent testbedevaluating machine learning methods.DNA promoters domain theory obtained UCI repository shownFigure 1 AND-OR tree. box leaf tree Figure 1 usually interpretedconjunction conditions. condition box requires particular nucleotideappear particular position sequence. According theory, DNA sequenceclassified promoter two regions DNA sequence identified.first region called contact, second conformation. conformation regionidentified, one four specific nucleotide sequences shown right-hand sideFigure 1 need present. contact region identified, minus 10minus 35 region also need identified. Again, minus 10 minus 35 region362fiOn Informativeness DNA Promoter Sequences Domain Theoryidentified, one respective four specific nucleotide sequences need present.sequence classified promoter domain theory, sequenceclassified negative (i.e., promoter).noted earlier, promoters domain theory coupled database 106 itemsUCI repository: 53 examples promoters 53 non-examples. interpretingleaf domain theory logical conjunction, theory classifies dataitems negative. Thus, clearly restrictive: sequence database satisfiesconditions specified theory. Two pieces domain knowledge suggest waysloosen conditions domain theory. First, conformation conditionweak biological support. implied initial KBANN experiments (Shavliket al., 1992), none learned rules referenced conformation conditions.addition, EITHER system (Ourston, 1991) eliminated rules involving conformationaltogether domain theory. Eliminating conformation also supporteddomain expert (Ourston, 1991). second piece domain knowledge conceptsdomain tend take form M-of-N concepts. final rules extractedKBANN approach take form. also made clear NEITHER-MofNsystem (Baffes & Mooney, 1993), added mechanism handle M-of-N conceptsoriginal learning mechanism EITHER/NEITHER system.3.M-of-NInterpretations DNA Promoters Theorymodify original DNA domain theory follows, allow sequencespositively classified promoters: a) eliminate conformation condition altogethertheory, b) reinterpret conjunctions conditions leaves Figure 1M-of-N concepts. usually interpreted, leaves Figure 1 equivalentconcept form (N-of-N c1c2:::cN ). example, conjunction (p-37=c ^ p-36=t ^p-35=t ^ p-34=g ^ p-33=a ^ p-32=c) leftmost leaf logically equivalent (6-of-6p-37=c p-36=t p-35=t p-34=g p-33=a p-32=c).Progressively less restrictive theories created lowering number conditionsneed satisfied leaf theory. Thus, new theory constructedN-of-N concepts substituted (N { 1)-of-N, (N { 2)-of-N, etc.variable N (i.e., number conditions leaf) decremented constant valueobtain value M-of-N concepts leaves theory. Figure 2shows accuracy theories constructed manner examplesdatabase. number conditions met M-of-N conceptslowered, number false negatives decreases, number false positive increases.total number misclassifications (false negatives plus false positives) minimizedleaf interpreted (N { 2)-of-N concept, resulting accuracy 93.4%.Even better accuracies obtained remove constant decrement restriction.is, allow greater exibility choosing different values leavescorresponding minus 35 minus 10 regions Figure 1. exhaustive search388800 possible combinations values found twelve theoriescorrectly classify 103 106 examples database (i.e., accuracy theories97.2%), 5148 theories accuracy equal better 93.4%. Figure 3 showsprobabilities obtaining theories different accuracies value363fiOrtegaCorrectFalseFalsePercentM-of-N criteriaPredictions Positives Negatives AccuracyN-of-N (original theory)5305350.00%N-of-N w/ conformation rule removed5704953.77%(N { 1)-of-N7802873.58%(N { 2)-of-N991693.40%(N { 3)-of-N9016084.91%(N { 4)-of-N6244058.49%Figure 2: Accuracy DNA promoters theory different M-of-N interpretations0.06P[accuracy]0.05Probability0.040.030.020.01000.20.40.60.81AccuracyFigure 3: Probability distribution DNA-theory-interpretation accuraciescontact leaves Figure 1 chosen random (but restriction N,N total number conditions particular leaf). probabilities Figure 3computed counting total number combinations values producedtheories specific accuracies. mean accuracy randomly chosen theory 76.5%,standard deviation 9.3%.results show leaves interpreted appropriate M-of-N concepts,existing DNA domain theory possesses large amount predictive information, factalso pointed Koppel et al. (1994a). much better nullpower suggested initial 50% accuracy, would equivalent random guessingtheory all. least, theory allows us make single random guessM-of-N interpretation expected accuracy 76.5%. shown Figure 2random guesses allow us much better that.364fiOn Informativeness DNA Promoter Sequences Domain Theory4. Learning DNA Promoters Theoryaccuracies various systems integrate analytical empirical learning around93% (Baffes & Mooney, 1993). results typically means computed multipletrials 80-85 training examples 21-26 tests examples. reported accuracies93.4% 97.2% based splits training test data. Instead,represent maximum accuracies (relative database 106 examples) couldobtained learning algorithms certain representational biases. example, 93.4%maximum accuracy may achieved learning system identifies (N{ i)-of-N concepts leaves, constant across leaves. approachconverted learning task learner identifies optimal value given settraining examples, evaluates resultant classifier using test set. resultsalgorithm, averaged 100 trials, produce mean accuracies 88.7% 10 trainingexamples, 92.5% 85 training examples (on test set 21 examples). resultssimilar best algorithms reported Baffes Mooney (1993).Koppel et al. (1994a) also show considerable information \reinterpreted" promoters domain theory. DOP (Degree Provedness) classificationmethodology, logical operations propositional domain theory (AND, OR, NOT)replaced arithmetic equivalents contain degree uncertainty. Ratherdirectly returning truth value indicating whether example positive, system firstcalculates DOP numerical score example. DOP score value greaterpre-specified threshold value, example considered \suciently" provedthus classified positive example theory. Otherwise, example classifiednegative. Koppel et al. determine threshold value two pieces knowledge: a)distribution DOP score examples, b) proportion (n%) positiveexamples database. DOP values examples sorted, thresholdvalue set value separates n% examples highest DOP valuesrest. important assumption domain theory certain proofadditive nature, DOP values higher positive examples negativeexamples. DOP classification methodology achieves high accuracy (92.5%) applied DNA promoter sequences domain theory. approach, accuracybased split available data training test sets, representsupper bound accuracy could obtained method convertedlearning algorithm. DOP classification methodology could converted learningalgorithm estimating distribution DOP values set training examples.5. Concluding Remarksnote detail new learning algorithm. Rather, demonstrates suitablelearning model promoters domain finding correct number, M,M-of-N concepts leaves original domain theory. 1 Assessing dicultylearning using available theory usually complicated need understandlearning algorithms exploit theory. theory-accuracy distribution Figure 31. algorithms may introduce structural modification theory (i.e., add/delete clausesconditions). However, increase accuracy due structural modifications negligiblecase promoters domain, illustrated high accuracies obtained without them.365fiOrtegahelps characterize learning complexity domain (under M-of-N model) provides dimension along evaluate performance learning algorithms useDNA promoter's theory testbed.Acknowledgementsresearch supported grant NASA Ames Research Center (NAG 2-834)Doug Fisher. grateful suggestions Doug Fisher, Stefanos Manganaris, DougTalbert, Jing Lin, well comments pointers Larry Hunter anonymousJAIR referees.ReferencesBaffes, P. T., & Mooney, R. J. (1993). Symbolic revision theories M-of-N rules.Proceedings Thirteenth International Joint Conference Artificial IntelligenceChambery, France.Harley, C. B., & Reynolds, R. P. (1987). Analysis e. coli promoter sequences. NucleicAcids Research, 15 (5), 2343{2361.Hawley, D. K., & McClure, W. R. (1983). Compilation analysis escherichia colipromoter DNA sequences. Nucleic Acids Research, 11 (8), 2237{2255.Hirsh, H., & Japkowicz, N. (1994). Boostraping training-data representations inductivelearning: case study molecular biology. Proceedings Twelfth NationalConference Artificial Intelligence, pp. 639{644 Seattle, WA.Koppel, M., Segre, A. M., & Feldman, R. (1994a). Getting awed theories.Proceedings Eleventh International Conference Machine Learning, pp.139{147 New Brunswick, NJ.Koppel, M., Feldman, R., & Segre, A. M. (1994b). Bias-driven revision logical domaintheories. Journal Artificial Intelligence Research, 1, 159{208.Lisser, S., & Margalit, H. (1993). Compilation e. coli mRNA promoter sequences. NucleicAcids Research, 21 (7), 1507{1516.Mahoney, J. J., & Mooney, R. J. (1993). Combining connectionist symbolic learningrefine certainty-factor rule-bases. Connection Science, 5 (3{4), 339{364.Mahoney, M. J., & Mooney, R. J. (1994). Comparing methods refining certainty-factorrule-bases. Proceedings Eleventh International Conference Machine Learning, pp. 173{180 New Brunswick, NJ.Murphy, P. M., & Aha, D. W. (1992). UCI Repository Machine Learning Databases.Department Information Computer Science, University California Irvine,Irvine, CA.366fiOn Informativeness DNA Promoter Sequences Domain TheoryNorton, S. W. (1994). Learning recognize promoter sequences e. coli modelinguncertainty training data. Proceedings Twelfth National ConferenceArtificial Intelligence, pp. 657{663 Seattle, WA.O'Neill, M. C. (1989). Escherichia coli promoters I: Consensus relates spacing class,specificity, repeat structure, three dimensional organization. Journal BiologicalChemistry, 264, 5522{5530.O'Neill, M. C., & Chiafari, F. (1989). Escherichia coli promoters II: spacing-class dependent promoter search protocol. Journal Biological Chemistry, 264, 5531{5534.Opitz, D. W., & Shavlik, J. W. (1994). Using genetic search refine knowledge-basedneural networks. Proceedings Eleventh International Conference MachineLearning, pp. 208{216 New Brunswick, NJ.Ortega, J. (1994). Making you've got: using models data improve learning rate prediction accuracy. Tech. rep. TR-94-01, Computer ScienceDept., Vanderbilt University. Abstract appears Proceedings Twelfth NationalConference Artificial Intelligence, p. 1483, Seattle, WA.Ourston, D. (1991). Using Explanation-Based Empirical Methods Theory Revision.Ph.D. thesis, University Texas, Austin, TX.Shavlik, J. W., Towell, G., & Noordewier, M. O. (1992). Using neural networks refineexisting biological knowledge. International Journal Human Genome Research, 1,81{107.Towell, G. G. (1990). Symbolic Knowledge Neural Networks: Insertion, Refinement,Extraction. Ph.D. thesis, University Wisconsin, Madison, WI.Towell, G. G., Shavlik, J. W., & Noordewier, M. O. (1990). Refinement approximatedomain theories knowledge-based neural networks. Proceedings EighthNational Conference Artificial Intelligence, pp. 861{866 Boston, MA.367fiJournal Artificial Intelligence Research 2 (1995) 475-500Submitted 10/94; published 5/95Adaptive Load Balancing: Study Multi-AgentLearningAndrea Schaerfaschaerf@dis.uniroma1.itDipartimento di Informatica e SistemisticaUniversita di Roma \La Sapienza", Via Salaria 113, I-00198 Roma, ItalyYoav ShohamRobotics Laboratory, Computer Science DepartmentStanford University, Stanford, CA 94305, USAMoshe TennenholtzFaculty Industrial Engineering ManagementTechnion, Haifa 32000, Israelshoham@flamingo.stanford.edumoshet@ie.technion.ac.ilAbstractstudy process multi-agent reinforcement learning context load balancing distributed system, without use either central coordination explicit communication. first define precise framework study adaptive load balancing,important features stochastic nature purely local informationavailable individual agents. Given framework, show illuminating resultsinterplay basic adaptive behavior parameters effect system eciency.investigate properties adaptive load balancing heterogeneous populations,address issue exploration vs. exploitation context. Finally, shownaive use communication may improve, might even harm system eciency.1. Introductionarticle investigates multi-agent reinforcement learning context concreteproblem undisputed importance { load balancing. Real life provides us many examples emergent, uncoordinated load balancing: trac alternative highways tendseven time; members computer science department tend use powerful networked workstations, eventually find lower load machinesinviting; on. would like understand dynamics emergentload-balancing systems apply lesson design multi-agent systems.define formal yet concrete framework study issues, called multiagent multi-resource stochastic system, involves set agents, set resources,probabilistically changing resource capacities, probabilistic assignment new jobs agents,probabilistic job sizes. agent must select resource new job,eciency resource handles job depends capacity resourcelifetime job well number jobs handled resourceperiod time. performance measure system aims globally optimizingresource usage system ensuring fairness (that is, system shouldn't madeecient expense particular agent), two common criteria load balancing.c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiSchaerf, Shoham, & Tennenholtzagent choose appropriate resource order optimize measures?make important assumption, spirit reinforcement learning (Sutton,1992): information available agent prior experience. particular,agent necessarily know past, present, future capacities resources,1unaware past, current, future jobs submitted various agents, evenrelevant probability distributions. goal agent thus adapt resourceselection behavior behavior agents well changing capacitiesresources changing load, without explicitly knowing are.interested several basic questions:good resource-selection rules?fact different agents may use different resource-selection rules affectsystem behavior?communication among agents improve system eciency?following sections show illuminating answers questions. contribution paper therefore twofold. apply multi-agent reinforcement learningdomain adaptive load balancing use basic domain order demonstratebasic phenomena multi-agent reinforcement learning.structure paper follows. Section 2 discuss general setting.objective section motivate study point impact. formalframework defined discussed Section 3. Section 4 completes discussionframework introducing resource selection rule parameters, function\control knobs" adaptive process. Section 5 present experimental resultsadaptive behavior within framework show various parameters affecteciency adaptive behavior. case heterogeneous populations investigatedSection 6, case communicating populations discussed Section 7. Section 8discuss impact results. Section 9 put work perspectiverelated work. Finally, Section 10 conclude brief summary.2. General Settingpaper applies reinforcement learning domain adaptive load balancing. However, presenting model use detailed study, need clarify severalpoints general setting. particular, need explain interpretationreinforcement learning interpretation load balancing adopt.Much work devoted recent years distributed adaptive load balancing. One find related work field distributed computer systems (e.g., Pulidas,Towsley, & Stankovic, 1988; Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993;Glockner & Pasquale, 1993; Mirchandaney, Towsley, & Stankovic, 1989; Zhou, 1988; Eager,Lazowska, & Zahorjan, 1986), organization theory management science (e.g., Malone,1. many applications capacities resources known, least extent. pointdiscussed later. Basically, paper wish investigate far one go using purelylocal feedback without use global information (Kaelbling, 1993; Sutton, 1992).476fiAdaptive Load Balancing: Study Multi-Agent Learning1987), distributed AI (e.g., Bond & Gasser, 1988). Although motivationsabove-mentioned lines research similar, settings discussed essentialdifferences.Work distributed computer systems adopts view set computerscontrols certain resources, autonomous decision-making capability, jobsarrive dynamic fashion. decision-making agents different computers(also called nodes) try share system load coordinate activities meanscommunication. actual action performed, based information receivedcomputers, may controlled various ways. One ways adopted controlrelated decisions learning automata (Narendra & Thathachar, 1989).above-mentioned work agent associated set resources,agent related resources associated node distributed system.Much work management science distributed AI adopts somewhat complementaryview. difference classical work distributed operating systems, agentassociated set resources controls. agents autonomous entitiesnegotiate among (Zlotkin & Rosenschein, 1993; Kraus & Wilkenfeld, 1991)use shared resources. Alternatively, agents (called managers case) maynegotiate task executed processors may execute (Malone, 1987).model adopt avor models used distributed AI organizationtheory. assume strict separation agents resources. Jobs arrive agentsmake decisions execute them. resources passive (i.e.,make decisions). typical example setting computerized framework setPCs, controlled different user submits jobs executedone several workstations. workstations assumed independentshared among users. example real-life situation motivatedstudy terminology adopt taken framework. However,real-life situations related model areas different classical distributedcomputer systems.canonical problem related model following one (Arthur, 1994): agent,embedded multi-agent system, select among set bars (or set restaurants).agent makes autonomous decision performance bar (and thereforeagents use it) function capacity number agents use it.decision going bar stochastic process decision bar useautonomous decision respective agent. similar situation arises productmanager decides processor use order perform particular task. modelpresent Section 3 general model situations investigated.situations job arrives agent (rather node consisting particular resources)decides upon resource (e.g., restaurant) job executed;a-priori association agents resources.discuss way agents behave framework. common themeamong above-mentioned lines research load-balancing achieved meanscommunication among active agents active resources (through related decisionmaking agents). study adopt complementary view. consider agentsact purely local fashion, based purely local information described recentreinforcement learning literature. mentioned, learning automata used477fiSchaerf, Shoham, & Tennenholtzfield distributed computer systems order perform adaptive load balancing. Nevertheless, related learning procedures rely heavily communication among agents (oramong decision-making agents autonomous computers). work applies recent workreinforcement learning AI information agent gets purely local. Hence,agent know ecient service restaurant choosingplace eat. don't assume agents may informed agents loadrestaurants restaurants announce current load. makeswork strictly different work applying reinforcement learning adaptive loadbalancing.features make model study basic general. Moreover,discussion raises question whether reinforcement learning (based purelylocal information feedback) guarantee useful load balancing. combinationmodel use perspective reinforcement learning makes contributionnovel. Nevertheless, mentioned (and discuss Section 9) modeluse original us captures many known problems situations distributedload balancing. apply reinforcement learning, discussed recent AI literature,model investigate properties related process.3. Multi-Agent Multi-Resource Stochastic Systemsection define concrete framework study dynamic load balancing.model present captures adaptive load balancing general setting mentionedSection 2. restrict discussion discrete, synchronous systems (and thusdefinition refer N , natural numbers); similar definitions possiblecontinuous case. concentrate case job executed usingresources. Although somewhat restricting, common practice much workdistributed systems (Mirchandaney & Stankovic, 1986).Definition 3.1 multi-agent multi-resource stochastic system 6-tuple hA; R; P ; D; C;SRi, = fa1; : : :; g set agents, R = fr1; : : :; rM g set resources,P : N ! [0; 1] job submission function, : N ! < probabilistic job sizefunction, C : RN ! < probabilistic capacity function, SR resource-selectionrule.intuitive interpretation system follows. resourcescertain capacity, real number; capacity changes time, determinedfunction C . time point agent either idle engaged. idle, maysubmit new job probability given P . job certain size alsoreal number. size submitted job determined function D. (Weuse unit token referring job sizes resource capacities, meantokens come integer quantities.) new job agent selects oneresources. choice made according rule SR; since much sayrule, discuss separately next section.model, job may run resource. Furthermore, limitnumber jobs served simultaneously given resource (and thus queuing occurs).However, quality service provided resource given time deteriorates478fiAdaptive Load Balancing: Study Multi-Agent Learningnumber agents using time. Specifically, every time point resourcedistributes current capacity (i.e., tokens) equally among jobs served it.size job reduced amount and, drops (or below) zero, jobcompleted, agent notified this, becomes idle again. Thus, execution timejob j depends size, capacity time resource processing it,number agents using resource execution j .measure system's performance twofold: aim minimize timeper-token, averaged jobs, well minimize standard deviationrandom variable. Minimizing quantities ensure overall system eciency wellfairness. question selection rules yield ecient behavior; turn nextdefinition rules.4. Adaptive Resource-Selection Rulesrule agents select resource new job, selection rule (SR),heart adaptive scheme topic section. Throughout sectionfollowing one make assumption homogeneity. Namely, assumeagents use SR. Notice although system homogeneous, agentact based local information. Sections 6 7 relax homogeneityassumption discuss heterogeneous communicating populations.already emphasized, among possible adaptive SRs interestedpurely local SRs, ones access experience particular agent.setting experience consists results previous job submissions; job submittedagent already completed, agent knows name r resource used,point time, tstart , job started, point time, tstop , job finished,job size . Therefore, input SR is, principle, list elements form(r; tstart; tstop ; ). Notice type input captures general type systemsinterested in. Basically, wish assume little possible informationavailable agent order capture real loosely-coupled systems globalinformation unavailable.Whenever agent selects resource job execution, may get feedbacknon-negligible time, feedback may depend decisions made agentsagent i's decision. forces agent rely non-trivial portionhistory makes problem much harder.uncountably many possible adaptive SRs aim gain exhaustive understanding them. Rather, experimented family intuitiverelatively simple SRs compared non-adaptive ones. motivation choosing particular family SRs partially due observations madecognitive psychologists people tend behave multi-agent stochastic recurrent situations. principle, set SRs captures two robust aspectsobservations: \The law effect" (Thronkide, 1898) \Power law practice" (Blackburn, 1936). family rules, called, partially resembles learning rulesdiscussed learning automata literature (Narendra & Thathachar, 1989), partially resembles interval estimation algorithm (Kaelbling, 1993), agents maintaincomplete history experience. Instead, agent, A, condenses history479fiSchaerf, Shoham, & Tennenholtzvector, called eciency estimator, denoted eeA . length vectornumber resources, i'th entry vector represents agent's evaluationcurrent eciency resource (specifically, eeA (R) positive real number).vector seen state learning automaton. addition eeA , agent keepsvector jdA, stores number completed jobs submitted agentresources, since beginning time. Thus, within, need specifytwo elements:1. agent updates eeA job completed2. agent selects resource new job, given eeA jdALoosely speaking, eeA maintained weighted sum new feedbackprevious value eeA , resource selected probably one highesteeA entry except low probability resource chosen. twosteps explained precisely following two subsections.4.1 Updating Eciency Estimatortake function updating eeAeeA (R) := WT + (1 , W )eeA (R)represents time-per-token newly completed job computedfeedback (R; tstart; tstop; ) following way:2= (tstop , tstart)=Stake W real value interval [0; 1], whose actual value depends jdA(R).means take weighted average new feedback value oldvalue eciency estimator, W determines weights given piecesinformation. value W obtained following function:W = w + (1 , w)=jdA(R)formula w real-valued constant. term (1 , w)=jdA(R) correctingfactor, major effect jdA (R) low; jdA (R) increases, reachingvalue several hundreds, term becomes negligible respect w.4.2 Selecting Resourcesecond ingredient adaptive SRsfunction pdA selecting resourcenew job based eeA jdA . function probabilistic. first define followingfunction(jdA(R) > 0(R),n0pdA(R) := ee,nE [ee ]jd (R) = 02. Using parallel processing terminology, viewed stretch factor, quantifies stretchingprogram's processing time due multiprogramming (Ferrari, Serazzi, & Zeigner, 1983).480fiAdaptive Load Balancing: Study Multi-Agent Learningn positive real-valued parameter E [eeA] represents average valueseeA (R) resources satisfying jdA (R) > 0. turn probability function,define pdA normalized version pd0A :pdA(R) := pd0A(R)== Rpd0A (R) normalization factor.3function pdA clearly biases selection towards resources performedwell past. strength bias depends n; larger value n,stronger bias. extreme cases, value n high (e.g., 20), agentalways choose resource best record. strategy \always choosingbest", although perhaps intuitively appealing, general good one;allow agent exploit improvements capacity load resources.discuss SR following subsection, expand issue exploration versusexploitation Sections 6 7.summarize, defined general setting investigate emergent loadbalancing. particular, defined family adaptive resource-selection rules,parameterized pair (w; n). parameters serve knobs tunesystem optimize performance. next section turn experimentalresults obtained system.4.3 Best Choice SR (BCSR)Best Choice SR (BCSR) learning rule assumes high value n, i.e,always chooses best resource given point. assume w fixed givenvalue discussing BCSR. previous work (Shoham & Tennenholtz, 1992, 1994),showed learning rules strongly resemble BCSR useful several naturalmulti-agent learning settings. suggests need carefully study caseadaptive load balancing. demonstrate, BCSR always useful loadbalancing setting.difference BCSR learning rule value n low,latter case agent gives relatively high probability selection resourcedidn't give best results past. case agent might able noticebehavior one resources improved due changes system.Note exploration \non-best" resources crucial dynamicssystem includes changes capacities resources. cases, agent couldtake advantage possible increases capacity resources uses BCSR. Onemight wonder, however, whether cases main dynamic changes systemstem load changes, relying BCSR sucient. latter true,able ignore parameter n concentrate BCSR, systemscapacity resources fixed. order clarify point, consider followingexample.3. R jdA (R) = 0, (i.e., agent going submit first job), assumeagent chooses resource randomly (with uniform probability distribution).481fiSchaerf, Shoham, & TennenholtzSuppose two resources, R1 R2 , whose respective (fixed) capacities,cR1 cR2 , satisfy equality cR1 = 2cR2 . Assume load system variescertain low value certain high one.system's load low agents adopt BCSR, system evolveway almost agents would preferring R1 R2. duefact that, case low load, overlaps jobs, hence R1 muchecient. hand, system's load high, R1 could busyagents would prefer R2, since performance obtained usingless crowded resource R2 could better one obtained using overly crowdedresource R1. extreme case high load, expect agents use R2 onethird time.Assume load system starts low level, increaseshigh value, decreases reach original value. load increases,agents, mostly using R1, start observing R1's performance becomingworse and, therefore, following BCSR start using R2 too. Now, loaddecreases, agents using R2 observe improvement performanceR2, value stored R1 (i.e., eeA (1)), still ect previoussituation. Hence, agents keep using R2, ignoring possibility obtainingmuch better results moved back R1. situation, randomized selectionmakes agents able use R1 (with certain probability) thereforemay discover performance R1 better R2 switch back R1.improve system's eciency significant manner.example shows BCSR is, general case, good choice.general true value n high.discussion assumed changes load unforeseen.able predict changes load, agents simply use BCSRload fixed use low value n changes. case, instead,without even realizing system changed way, agents would need(and, see, would able to) adapt dynamic changes well other.5. Experimental Resultssection compare SRsanother, well non-adaptive,benchmark selection rules.non-adaptive SRs consider paper agents partitionaccording capacities load system fixed predeterminedmanner agent uses always resource. Later paper, SRkind identified configuration vector, specifies, resource, manyagents use it. test adaptive SRs, compare performance nonadaptive SRs perform best particular problem. creates highly competitiveset benchmarks adaptive SRs.addition, compare adaptive SRs load-querying SR definedfollows: agent, new job, asks resources busyalways chooses less crowded one.482fiAdaptive Load Balancing: Study Multi-Agent Learning5.1 Experimental Settingintroduce particular experimental setting, many results describedobtained. present order concrete experiments; however,qualitative results experiments observed variety experimentalsettings.One motivation particular setting stems PCs workstations problemmentioned Section 2. example, part study related set computerslocated single site. computers relatively high load peak hoursday low load night (i.e., chances user PC submits jobhigher day time week days night weekend). Anotherpart study related set computers split around world,load quite random structure (i.e., due difference time zones, users may use PCsunpredictable hours).Another motivation particular setting stems restaurant problem mentioned Section 2 (for discussion related \bar problem" see Arthur, 1994).example, consider set snack bars located industrial park. snackbars relatively high loads peak hours day low load night(i.e., chances employee choose go snack-bar higher dayemployees present day). Conversely, assumeset bars near airport load quite random structure (i.e., airportemployees may like use snack-bars quite unpredicted hours).Although particular real-situations, would like emphasize generalmotivation study fact related phenomena observedvarious different settings.take N , number agents, 100, , number resources,5. first set experiments take capacities resources fixed.particular, take c1 = 40; c2 = 20; c3 = 20; c4 = 10; c5 = 10. assumeagents probability submitting new job. also assumeagents distribution size jobs submit; specifically, assumeuniform distribution integers range [50,150].ease exposition, assume point time corresponds second,consequently count time minutes, hours, days, weeks. hourmain point reference; assume, simplicity, changes system (i.e., loadchange capacity change) happen beginning new hour. probabilitysubmitting job second, corresponds load system, varytime; crucial factor agents must adapt. Note agentssubmit jobs second, probability submission may change. particularconcentrate three different values quantity, called Llo ; Lhi Lpeak ,assume system load switches values. actual values Llo ; LhiLpeak following quantitative results 0:1%, 0:3% 1%, roughly correspondagent submitting 3.6, 10.8, 36 jobs per hour (per agent) respectively.483fiSchaerf, Shoham, & Tennenholtzloadconfigurationtime-per-tokenLlof100; 0; 0; 0; 0g38.935Lhif66; 16; 16; 1; 1g60.768Lpeak f40; 20; 20; 10; 10g196.908Figure 1: Best non-adaptive SRs fixed loadfollowing, measuring success, refer average time-pertoken.4 However, adaptive SRs give best average time-per-token alsofound fair.5.2 Fixed Loadstart case load fixed. case interestingadaptive behavior; however, satisfactory SR show reasonably ecient behaviorbasic case, order useful system stabilizes.start showing behavior non-adaptive benchmark SRs case fixedload.5 Figure 1 shows give best results, three loads.see, big difference three loads mentioned above.load particularly high, agents scatter around resources rateproportional capacities; load low use best resource.Given above, easy see adaptive SR effective enablesmoving quickly one configuration other.static setting this, expect best non-adaptive SRs perform better adaptive ones, since information gained exploration adaptive SRsbuilt-in non-adaptive ones. experimental results confirm intuition,shown Figure 2 Lhi . figure shows performance obtained populationvalue n varies 2 10 three values w: 0.1, 0.3, 0.5.Note values (n; w) good choices dynamic cases (see laterpaper, values intervals [3; 5] [0:1; 0:5], respectively), deteriorationperformance adaptive SRs respect non-adaptive ones small.encouraging result, since adaptive SRs meant particularly suitable dynamicsystems. following subsections see indeed are.5.3 Changing Loadbegin explore dynamic settings. consider caseload system (that is, probability agents submitting job time) changestime. paper present two dynamic settings: One load changesaccording fixed pattern random perturbations anotherload varies random fashion. Specifically, first case fix load Lhi4. data shown later refer, convenience, time 1000 tokens.5. non-adaptive SRs human-designed SRs used benchmarks; assume knowledgeload capacity, available adaptive SRs design.484fiAdaptive Load Balancing: Study Multi-Agent Learningvergeeperken667Weight: w = 0.5Weight: w = 0.3Weight: w = 0.1666564636261234 56 789 10Exponent Randomization Function: n-Figure 2: Performance adaptive Selection Rules fixed loadten consecutive hours, five days week, two randomly chosen hoursLpeak , Llo rest week. second case, fix numberhours week load first case, distribute completelyrandomly week.results obtained two cases similar. Figure 3 shows results obtainedadaptive SRs case random load. best non-adaptive deterministicSR gives time-per-token value 69:201 obtained configuration (partitionagents) f52; 22; 22; 2; 2g; adaptive SRs superior. load-querying SR instead getstime-per-token value 48:116, obviously better, farperformances adaptive SRs.also observe following phenomenon: Given fixed n (resp. fixed w) averagetime-per-token non-monotonic w (resp. n). phenomenon strongly relatedissue exploration versus exploitation mentioned phenomena observedstudy Q-learning (Watkins, 1989).also notice two parameters n w interplay. fact, valuew minimum time per token value obtained different value n.precisely, higher w lower n must order obtain best results. meansthat, order obtain high performance, highly exploratory activity (low n)matched giving greater weight recent experience (high w). \parameter485fiSchaerf, Shoham, & Tennenholtzvergeeperken6717069Weight: w = 0.5Weight: w = 0.3Weight: w = 0.168676665234 56 789 10Exponent Randomization Function: n-Figure 3: Performance adaptive Selection Rules random loadmatching" intuitively explained following qualitative way: explorationactivity pays allows agent detect changes system. However,effective if, change detected, significantly affect eciency estimator(i.e., w high). Otherwise, cost exploration activity greater gain.5.4 Changing Capacitiesconsider case capacity resources vary time.particular, demonstrate results case previously mentioned setting.assume capacities rotate randomly among resources and, five consecutivedays, resource gets capacity 40 one day, 20 2 days, 102 days.6 load also varies randomly.results experiment shown Figure 4. best non-adaptive SRcase gives time-per-token value 118:561 obtained configurationf20; 20; 20; 20; 20g.7 adaptive SRs give much better results, slightly6. Usually capacities change less dramatic fashion. use above-mentioned settingorder demonstrate applicability approach severe conditions.7. load-querying SR gives results case fixed capacities, SRobviously uenced change.486fiAdaptive Load Balancing: Study Multi-Agent Learningvergeeperken692.59087.582.5808577.52Weight: w = 0.5Weight: w = 0.3Weight: w = 0.1-3456 7 89 10Exponent Randomization Function: nFigure 4: Performance adaptive Selection Rules changing capacitiesworse case fixed capacities. phenomena mentioned visiblecase too. See example weight 0:1 mismatches low values n.6. Heterogeneous PopulationsThroughout previous section assumed agents use SR, i.e.Homogeneity Assumption. assumption models situation sortcentralized off-line controller which, beginning, tells agents behaveleaves agents make decisions.situation described different on-line centralized controller makes every decision. However, would like move eveninvestigate situation agent able make decisionstrategy use and, maybe, adjust time.step toward study systems kind, drop Homogeneity Assumptionconsider situation part population uses one SR partuses second one.first set experiments, consider setting discussed Subsection 5.1confront one other, two populations (called 1 2) size (50 agentseach). population uses different SR. SR population (for = 1; 2)487fiSchaerf, Shoham, & Tennenholtzvergeeperken667666563616462: T1: T22 34 56 789 10Exponent Randomization Function (n2 )-Figure 5: Performance 2 populations 50 agents n1 = 4 w1 = w2 = 0:3determined pair parameters (wi; ni ). measure success populationdefined average time-per-token members, denoted Ti .Figure 5 shows result obtained w1 = w2 = 0:3, n1 = 4, differentvalues n2 , case randomly varying load.results expose following phenomenon: two populations obtain differentoutcomes ones obtain homogeneous case. specifically, 4n2 6 , results obtained agents use n2 generally better resultsobtained ones use n1 , despite fact homogeneous populationuses n1 gets better results homogeneous population uses n2 .phenomenon described following intuitive explanation. n2above-mentioned range, population uses n2 less \exploring" (i.e.,\exploiting") one, left might ableadapt changes satisfactory manner. However, joinedpopulation, gets advantages experimental activity agents population,without paying it. fact, exploring agents, trying unloadcrowded resources, make service agents well.worth observing Figure 5 n2 low (e.g., n2 3) agents usen2 take role explorers lose lot, agents use n1 gainsituation. Conversely, high values n2 (e.g., n2 7) performances exploiters,488fiAdaptive Load Balancing: Study Multi-Agent Learningvergeeperken6676665: T1: T264636261234 56 789 10Exponent Randomization Function: n2-Figure 6: Performance 2 populations 90/10 agents n1 = 4 w1 = w2 = 0:3use n2 , deteriorate. means exploiters static, hinderother, explorers take advantage it.better understanding phenomena involved, experimentedasymmetric population, composed one large group one small one, instead twogroups similar size. Figure 6 shows results obtained using setting similarone above, population 1 composed 90 members population 2 consists10 members. case, every value n2 4, exploiters betterexplorers. experiments also show case, higher n2 better T2is, i.e. exploiters exploit, gain.results suggest single agent gets best results noncooperative always adopting resource best performance (i.e., use BCSR),given rest agents use adaptive (i.e., cooperative) SR. However,agents non-cooperative lose.8 conclusion, selfish interestagent match interest population. contrary resultsobtained basic contexts multi-agent learning (Shoham & Tennenholtz, 1992).shown how, fixed value w, coexisting populations adoptingdifferent values n interact. Similar results obtained fix value n8. fact illuminating instance well-known prisoners dilemma (Axelrod, 1984).489fiSchaerf, Shoham, & Tennenholtzvergeeperken667: T1: T2666564636261-0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Weight estimator parameter (w2)1Figure 7: Performance 2 populations 50 agents n1 = n2 = 4 w1 = 0:3use two different values w. cases, agents adopting lower value wgeneral winners, shown Figure 7 n1 = n2 = 4 w1 = 0:3. wlow corresponding agents get poor results longer winners,case high n Figure 5.Another interesting phenomenon obtained confronting adaptive agentsload-querying agents. Load-querying agents agents able consult resourcessubmit jobs. load-querying agent submit jobunloaded resource given point. confronting load-querying agentsadaptive ones, results obtained adaptive agents obviously worseresults obtained load-querying ones, better results obtainedcomplete population adaptive agents. means load-querying agents playrole \parasites", above-mentioned \exploiters"; load-querying agents helpmaintaining load balancing among resources, therefore help restagents. Another result obtain agents adopt deterministic SRs may behaveparasites worsen performance adaptive agents.assertions supported experiments described Figure 8, population 90 agents, uses adaptive SR parameters (n; w), facedminority 10 agents use different SRs, stated above. particular, fourcases consider, minority behaves following ways: (i) choose resource490fiAdaptive Load Balancing: Study Multi-Agent Learning90 agents10 agentsT1(.3,4)(.3,20)65.161(.3,4)(.1,4)64.630(.3,4) Load-querying 62.320(.3,4)Using Res. 0 65.499T259.71363.81847.23655.818Figure 8: Performance 2 populations 90/10 agents various SRsgave best results, (ii) conservative updating history, (iii)load-querying agents, (iiii) use deterministically resource capacity 40(in basic experimental setting).7. Communication among Agentspoint, assumed direct communication among agents.motivation considered situations absolutelytransmission channels protocols. assumption agreement ideamulti-agent reinforcement learning. systems massive communication feasiblemuch concerned multiple agent adaptation, problem reducessupplying satisfactory communication mechanisms. Multi-agent reinforcement learninginteresting real life forces agents act without a-priori arranged communication channels must rely action-feedback mechanisms. However, interestunderstand effects communication system eciency (as Shoham & Tennenholtz, 1992; Tan, 1993), agents augmented sort communicationcapabilities. study extension led illuminating results,present.assume agent communicate agents,call neighbors. therefore consider relation neighbor-of assume exive, symmetric transitive. consequence, relation neighbor-of partitionspopulation equivalence classes, call neighborhoods.form communication consider based idea eciency estimators agents within neighborhood shared among decision made(i.e., agent chooses resource). reader notice naiveform communication sophisticated types communication possible.However, form communication natural concentrate agentsupdate behavior based past information. particular, typecommunication similar ones used above-mentioned work incorporatingcommunication framework multi-agent reinforcement learning.suppose different SRs may used different agents population,impose condition within single neighborhood, SR usedmembers.also assume agent keeps history updatesusual way. choice, instead, based agent eciency estimator,491fiSchaerf, Shoham, & Tennenholtzvergeeperken6717068676665269: 5 CNs 20 agents: 20 CNs 5 agents: 50 CNs 2 agents34 56 789 10Exponent Randomization Function: n-Figure 9: Performance adaptive Selection Rules random load profile communicating agentsaverage eciency estimators agents corresponding neighborhood.average called neighborhood eciency estimator. neighborhood eciencyestimator physical storage: value recalculated time member needs it.order compare behavior communicating agents non-communicating ones,assume single population might be, aside neighborhoods definedabove, also neighborhoods allow sharing eciency estimators amongmembers. members neighborhoods behave described previoussections, i.e., agent relies history. thing commonamong members neighborhood members use SR.call communicating neighborhood (CN), neighborhood eciency estimators shared decision taken non-communicating neighborhood (NCN),neighborhood done.first set experiments ran, regards population composed CNs,size. particular, considered CNs various sizes, starting 50 CNssize 2, going 5 CNs size 20. load profile exploited random load changedefined Subsection 5.3, value w taken 0:3, n taken variousvalues. results obtained shown Figure 9.492fiAdaptive Load Balancing: Study Multi-Agent Learningresults show communicating populations get good results.reason members CN tend conservative, sensemostly use best resource. fact, since rely average several agents,picture system tends much static. particular, biggerCN conservative members tend be. example, consider values(n; w) give best results non-communicating agents, values give quite badperformance CNs since turn conservative.Using adaptive values (n; w), behavior communicating population improves reaches performance slightly worse performancenon-communicating population. Tuning parameters using finer grain, possibleobtain performance equal one obtained non-communicating population.However, seems clear obvious gain achieved form communicationcapability. intuitive explanation two opposite effects causedcommunication. one hand, agents get fairer picture system prevents using bad resources therefore getting bad performance.hand, since agents CN \better" picture system, tenduse best resources thus compete them. fact, agents behaveselfishly selfish interest may agree interest populationwhole.interesting message get fact agents may\distorted" picture system (which typical non-communicating populations),turns advantage population whole.Sharing data among agents leads poorer performances also caseagents common views loads target jobs toward (lightly loaded)resources, quickly become overloaded. order profitably use shared data,allow form reasoning fact data shared.problem however scope paper (see e.g., Lesser, 1991).order understand behavior system CNs NCNs face other,consider NCN 80 agents together set CNs equal size, different valuessize. results corresponding experiments shown Figure 10.members CNs, inclined use best resources, behave parasitessense explained Section 6. exploit adaptiveness rest populationobtain good performance best resources. reason get better resultsrest population, shown experimental results.interesting observe NCN uses conservative selection rule,CNs obtain even better results. intuitive explanation behavioralthough groups, i.e., communicating ones one high value n,tend conservative, communicating ones \win" conservative\clever" way, making use better picture situation.conclusion draw section proposed form communicationagents may provide useful means improve performance populationsetting. However, claim communication agents completelyuseless. Nevertheless, observed provide straightforward significantimprovement. results support claim sole past history agent493fiSchaerf, Shoham, & Tennenholtz80 agents(.3,4) 1 NCN(.3,4) 1 NCN(.3,4) 1 NCN(.3,4) 1 NCN(.3,10) 1 NCN(.3,10) 1 NCN(.3,10) 1 NCN(.3,10) 1 NCN20 agents(.3,4) 1 CN(.3,4) 2 CNs(.3,4) 5 CNs(.3,4) 10 CNs(.3,4) 1 CN(.3,4) 2 CNs(.3,4) 5 CNs(.3,4) 10 CNsT165.28765.06965.09164.89568.41968.31968.52968.351T263.05463.30762.80963.84060.01859.51260.67461.711Figure 10: Performance CNs NCNs togetherreasonable information base decision, assuming consider availablekind real-time information (e.g., current load resources).8. Discussionprevious sections devoted report experimental study. synthesize observations view motivation, discussed Sections 1 2.mentioned, model general model active autonomous agentsselect among several resources dynamic fashion based local information.fact agents use local information makes possibility ecient loadbalancing questionable. However, showed adaptive load balancing based purelylocal feedback feasible task. Hence, results complementary ones obtaineddistributed computer systems literature. Mirchandaney Stankovic (1986) putit: \: : : significant work illustrated possible designlearning controller able dynamically acquire relevant job scheduling informationprocess trial error, use information provide good performance."study presented paper supplies complementary contribution ableshow useful adaptive load balancing obtained using purely local informationframework general organizational-theoretic model.study identified various parameters adaptive process investigatedaffect eciency adaptive load balancing. part study suppliesuseful guidelines systems designer may force agents work basedcommon selection rule. observations, although somewhat related previous observations made contexts models (Huberman & Hogg, 1988), enable demonstrateaspects purely local adaptive behavior non-trivial model.results disagreement selfish interest agents commoninterest population sharp contrast previous work multi-agent learning(Shoham & Tennenholtz, 1992, 1994) dynamic programming perspectiveearlier work distributed systems (Bertsekas & Tsitsiklis, 1989). Moreover, exploreinteraction different agent types affects system's eciency well494fiAdaptive Load Balancing: Study Multi-Agent Learningindividual agent's eciency. related results also interpreted guidelinesdesigner may partial control system.synthesis observations teaches us adaptive load balancingone adopts reinforcement learning perspective agents rely localinformation activity. additional step performed attempts bridgegap local view previous work adaptive load balancing communicatingagents, whose decisions may controlled learning automata means.therefore rule possibility communication current status resourcesjoint decision-making, enable limited sharing previous history. showlimited communication may help, even deteriorate system eciency.leaves us major gap previous work communication among agentsbasic tool adaptive load balancing work. Much left done attemptingbridge gap. see major challenge research.9. Related WorkSection 2 mentioned related work field distributed computer systems(Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993; Glockner & Pasquale, 1993;Mirchandaney et al., 1989; Zhou, 1988; Eager et al., 1986). typical example workpaper Mirchandaney Stankovic (1986). work learning automataused order decide action taken. However, suggested algorithms heavilyrely communication information sharing among agents. sharp contrastwork. addition, differences type model usemodel presented above-mentioned work work distributed computersystems.Applications learning algorithms load balancing problems given Mehra(1992), Mehra Wah (1993). However, work well, agents (sites,authors' terminology) ability communicate exchange workload values,even though values subject uncertainty due delays. addition, differentlywork, learning activity done off-line. particular, learning phasewhole system dedicated acquisition workload indices. load indicesused running phase threshold values job migration different sites.spite differences, similarities work abovementioned work. One important similarity use learning procedures.difference classical work parallel distributed computation (Bertsekas& Tsitsiklis, 1989) applies numerical iterative methods solution problemsnetwork ow parallel computing. similarities related studydivision society groups. somewhat resembles work group formation(Billard & Pasquale, 1993) distributed computer systems. information sharingallow Section 7 similar limited communication discussed Tan (1993).classification load-balancing problems given Ferrari (1985), work fallscategory load-independent non-preemptive pure load-balancing. problemsinvestigate also seen sender-initiated problems, although case senderagent (overloaded) resource.495fiSchaerf, Shoham, & TennenholtzOne may wonder work differs work adaptive load balancingOperations Research (OR) (e.g., queuing theory Bonomi, Doshi, Kaufmann, Lee, &Kumar, 1990). Indeed, commonalities. work, individualdecisions made locally, based information obtained dynamically runtime.cases systems constructed suciently complex interestingresults tend obtained experimentally. However, careful look relevantliterature reveals essential difference perspective topicreinforcement-learning perspective: permits free communication within system,thus significant element uncertainty framework. particular, issueexploration versus exploitation, lies heart approach, completelyabsent work OR.work adaptive load balancing related topics carried alsoArtificial Intelligence community (see e.g., Kosoresow, 1993; Gmytrasiewicz, Durfee, &Wehe, 1991; Wellman, 1993). work too, however, tends based formcommunication among agents, whereas case load balancing obtained purelylearning activity.article related previous work co-learning (Shoham & Tennenholtz,1992, 1994). framework co-learning framework multi-agent learning,differs frameworks discussed multi-agent reinforcement learning (Narendra &Thathachar, 1989; Tan, 1993; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994) duefact considers case stochastic interactions among subsets agents,purely local feedback revealed agents based interactions.framework co-learning similar respects number dynamic frameworkseconomics (Kandori, Mailath, & Rob, 1991), physics (Kinderman & Snell, 1980), computational ecologies (Huberman & Hogg, 1988), biology (Altenberg & Feldman, 1987).study adaptive load balancing treated study co-learning.Relevant work also literature field Learning Automata (see Narendra & Thathachar, 1989). fact, agent setting seen learning automaton. Therefore, one may hope theoretical results interconnected automataN-player games (see e.g., El-Fattah, 1980; Abdel-Fattah, 1983; Narendra & Wheeler Jr.,1983; Wheeler Jr. & Narendra, 1985) could imported framework. Unfortunately,due stochastic nature job submissions (i.e., agent interactions) real-valued(instead binary) feedback, problem fit completely theoreticalframework learning automata. Hence, results concerning optimality, convergence expediency learning rules Linear Reward-Penalty Linear Reward-Inaction,easily adapted setting. fact use stochastic modelinteraction among agents, makes work closely related above-mentioned workco-learning. Nevertheless, work largely uenced learning automata theoryresource-selection rules closely resemble reinforcement schemes learning automata.Last least, work related work applying organization theory management techniques field Distributed AI (Fox, 1981; Malone, 1987; Durfee, Lesser,& Corkill, 1987). model closely related models decision-making managementorganization theory (e.g., Malone, 1987) applies reinforcement learning perspective context. makes work related psychological models decision-making(Arthur, 1994).496fiAdaptive Load Balancing: Study Multi-Agent Learning10. Summarywork applies idea multi-agent reinforcement learning problem loadbalancing loosely-coupled multi-agent system, agents need adapt one another well changing environment. demonstrated adaptive behavioruseful ecient load balancing context identified pair parametersaffect eciency non-trivial fashion. parameter, holding parameterfixed, gives rise certain tradeoff, two parameters interplay non-trivialilluminating way. also exposed illuminating results regarding heterogeneouspopulations, group parasitic less adaptive agents gain exibility agents. addition, showed naive use communication mayimprove, might even deteriorate, system eciency.Acknowledgmentsthank anonymous reviewers Steve Minton, whose stimulating comments helpedus improving earlier version paper.ReferencesAbdel-Fattah, Y. M. (1983). Stochastic automata modeling certain problems collectivebehavior. IEEE Transactions Systems, Man, Cybernetics, 13 (3), 236{241.Altenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission,evolution modifier genes. I. reduction principle. Genetics, 117, 559{572.Arthur, W. (1994). Inductive reasoning, bounded rationality bar problem. Tech. rep.94-03-014 (working paper), Santa Fe Institute. Appeared also American EconomicReview 84.Axelrod, R. (1984). Evolution Cooperation. New York: Basic Books.Bertsekas, D., & Tsitsiklis, J. (1989). Parallel Distributed Computation: NumericalMethods. Prentice Hall.Billard, E., & Pasquale, J. (1993). Effects delayed communication dynamic groupformation. IEEE Transactions Systems, Man, Cybernetics, 23 (5), 1265{1275.Blackburn, J. M. (1936). Acquisition skill: analysis learning curves. IHRB ReportNo. 73.Bond, A. H., & Gasser, L. (1988). Readings Distributed Artificial Intelligence. AblexPublishing Corporation.Bonomi, F., Doshi, B., Kaufmann, J., Lee, T., & Kumar, A. (1990). case studyadaptive load balancing algorithm. Queuing Systems, 7, 23{49.Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent cooperation among communicating problem solvers. IEEE Transactions Computers, 36, 1275{1291.497fiSchaerf, Shoham, & TennenholtzEager, D., Lazowska, E., & Zahorjan, J. (1986). Adaptive load sharing homogeneousdistributed systems. IEEE Transactions Software Engineering, 12 (5), 662{675.El-Fattah, Y. M. (1980). Stochastic automata modeling certain problems collectivebehavior. IEEE Transactions Systems, Man, Cybernetics, 10 (6), 304{314.Ferrari, D. (1985). study load indices load balancing schemes. Tech. rep. Ucb/CSD86/262, Computer Science Division (EECS), Univ. California, Berkeley.Ferrari, D., Serazzi, G., & Zeigner, A. (1983). Measurement Tuning ComputerSystems. Prentice Hall.Fox, M. S. (1981). organizational view distributed systems. IEEE TransactionsSystems, Man, Cybernetics, 11, 70{80.Glockner, A., & Pasquale, J. (1993). Coadaptive behavior simple distributed jobscheduling system. IEEE Transactions Systems, Man, Cybernetics, 23 (3),902{907.Gmytrasiewicz, P., Durfee, E., & Wehe, D. (1991). utility communication coordinating intelligent agents. Proc. 9th Nat. Conf. Artificial Intelligence(AAAI-91), pp. 166{172.Huberman, B. A., & Hogg, T. (1988). behavior computational ecologies. Huberman, B. A. (Ed.), Ecology Computation. Elsevier Science.Kaelbling, L. (1993). Learning Embedded Systems. MIT Press.Kandori, M., Mailath, G., & Rob, R. (1991). Learning, mutation long equilibriagames. Mimeo. University Pennsylvania.Kinderman, R., & Snell, S. L. (1980). Markov Random Fields Applications.American Mathematical Society.Kosoresow, A. P. (1993). fast first-cut protocol agent coordination. Proc.11th Nat. Conf. Artificial Intelligence (AAAI-93), pp. 237{242.Kraus, S., & Wilkenfeld, J. (1991). function time cooperative negotiations.Proc. 9th Nat. Conf. Artificial Intelligence (AAAI-91), pp. 179{184.Lesser, V. R. (1991). retrospective view FA/C distributed problem solving. IEEETransactions Systems, Man, Cybernetics, 21 (6), 1347{1362.Malone, T. W. (1987). Modeling coordination organizations markets. ManagementScience, 33 (10), 1317{1332.Mehra, P. (1992). Automated Learning Load-Balancing Strategies DistributedComputer System. Ph.D. thesis, Department Electrical Computer Engineering,University Illinois Urbana-Champaign.498fiAdaptive Load Balancing: Study Multi-Agent LearningMehra, P., & Wah, B. W. (1993). Population-based learning load balancing policiesdistributed computer system. Proceedings Computing Aerospace 9 Conference,AIAA, pp. 1120{1130.Mirchandaney, R., & Stankovic, J. (1986). Using stochastic learning automata jobscheduling distributed processing systems. Journal Parallel DistributedComputing, 3, 527{552.Mirchandaney, R., Towsley, D., & Stankovic, J. (1989). Analysis effects delaysload sharing. IEEE Transactions Computers, 38 (11), 1513{1525.Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.Prentice Hall.Narendra, K., & Wheeler Jr., R. M. (1983). N-player sequential stochastic gameidentical payoffs. IEEE Transactions Systems, Man, Cybernetics, 13 (6), 1154{1158.Pulidas, S., Towsley, D., & Stankovic, J. (1988). Imbedding gradient estimators load balancing algorithms. Proceedings 8th International Conference DistributedComputer Systems, IEEE, pp. 482{489.Sen, S., Sekaran, M., & Hale, J. (1994). Learning coordinate without sharing information.Proc. 12th Nat. Conf. Artificial Intelligence (AAAI-94).Shoham, Y., & Tennenholtz, M. (1992). Emergent conventions multi-agent systems: initial experimental results observations. Proc. 3rd Int. Conf. PrinciplesKnowledge Representation Reasoning (KR-92), pp. 225{231.Shoham, Y., & Tennenholtz, M. (1994). Co-learning evolution social activity.Tech. rep. STAN-CS-TR-94-1511, Dept. Computer Science, Stanford University.Sutton, R. (1992). Special issue reinforcement learning. Machine Learning, 8 (3{4).Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents.Proceedings 10th International Conference Machine Learning.Thronkide, E. L. (1898). Animal intelligence: experimental study associativeprocesses animals. Psychological Monographs, 2.Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge University.Wellman, M. P. (1993). market-oriented programming environment applicationdistributed multicommodity ow problems. Journal Artificial Intelligence Research,1, 1{23.Wheeler Jr., R. M., & Narendra, K. (1985). Learning models decentralized decisionmaking. Automatica, 21 (4), 479{484.499fiSchaerf, Shoham, & TennenholtzYanco, H., & Stein, L. (1993). adaptive communication protocol cooperating mobile robots. Animals Animats: Proceedings Second InternationalConference Simulation Adaptive Behavior, pp. 478{485.Zhou, S. (1988). trace-driven simulation study dynamic load balancing. IEEE Transactions Software Engineering, 14 (9), 1327{1341.Zlotkin, G., & Rosenschein, J. S. (1993). domain theory task oriented negotiation.Proc. 13th Int. Joint Conf. Artificial Intelligence (IJCAI-93), pp. 416{422.500fiff fi!"!#%$'&)(*+*-,+.0/-(*21-/+34BDCFEHGJILKNMPOQKSRUTWVXTYKSRUTYK[Z\A^]+_[CJ`9IZ9aUEkmlnPomnqpsrmtQuwv56789":;3-<2*+=>?7# @:;(+<*-,b CJ`dce]2GfKgAdRUGYVhZiGWZ9I2CjKxzyz{z|~}[z} y}6x){z{9rmtinYk;nzF i[z} y}6xz){z{90F+F9@+z22FF)26)0)++FUhL)L!z-; P Fm!0"@N! P ) 00P+!@LN0 PJ0N+ 0w [ @ +!F ;[ 0@LF " !+P!0X ) )X@J@ +f + @ +U@0JPP+F9N0@)S NP+P6P+F @@ ") @ P+F @!@J@P%@ Pz2 F +! zPh!@@6N@)P F@Y!F @0+ @ h @@0FN! @+! @0f @ !JP!- U+ ) F@L! X %P@!0 -6 +@ J )0 N!!@W! [ W@ WPF +'"@ F@ J @ h )! P +F6 +@@-wh ) YN 0! !F )N@ !!N WN!)X %! N!@@ PW ! N!-@@ )@ @X @@ d+0 !! FF[!P 6F+ +@@[L ) !UN+ +![0 +P6!@6@; !@@;PJ!@+ w"! ! 0 ; @@0U F06@+ 6@9+ @0@L!Pi!0 @S @! ;P++@P@-!U! +@! UN!;@@!+N!2+ ! @+@;;F![;@0 ;!- P0@N!-! NN@!;i@0%ih ;@fffi fffifi fi!""#$&%'ff(ff)*+,+#"#fi+$ff- +.- /0%'1ff2.34$65879-.$:ff"# fffi-fi$; $<=<=ff />+?#@().;ffA>CBD +E;-$ff +%F fffi/>F+?+fi-%'< , ff-/G2H+fifi C%'/>"#5<I$"#4:"#/G .fi3"#/Gfffi#@fffiJ+ /GD#K= +EL=+K$!fffi M%'/ON.K !fi"Pfifffi$Q$Rff-$fi/G;#@ fffi= T&-U+%*= fffi/V5FI(!/>KP=+KM+,+-fiK"+WffSXfiY3C=ffff-/DL+-fi ZZ!/>3XfiW fffi/V[?=G+%J +fi Z\ Pfi>fifi]\= \"# ffG.B45^Hfffffi->P#XKF=_/?+DW E` Pfffi/G2Hfi-fi+-SF"PfiD#-G$/,fffiM+?&fifi:= #%'b= Pb/>3>)Yff++D#+ MG&fi W""#$$c>!+fiM Ed Pfffi/>57e;#@e+/Wfffi:"2'$fff .;Dff'"3XfiG""#/WffX=f= #-.Bafif= b =!$[gih?jMkJh?ljmbnfo [9pfiq?T%1fiff-38"#fific+f ff3: PfiM%'/r= sXfi .+PW&fi/,X'+;ff+fftu C=+=cE;B$W-V"Pfi."# />#+"#$P=+$/,fffibP= bfi .65v;=Pfi+3* =J/?$?3,-Dfi+/GJ !Nw"P3K/G(xNw"+fi?+%K= bxfi .+Wff+*:ff%'#@+/Gff-:++/,fi J"#.+ff.+,/>+Bfi b= FX- .+bff*yz9fifffi'3eJ D3w"#fi!fi/?+."P=>=fi T)$"PXNw"+-5gin nf{Jk;nbkCl3|a} [(~X%'f= JPfi$Aff+?t 5 5X:eDV- +A/G+fi >6:ff>"P=+ +fi ?f.ff#:ff;D?/G %fi ,S+'+fffiMfffifi DFt ffX*= bP$fffifi ,ff"P=fffi$F=fi ff3 5(*+*-,wfizfi@@!" 2 :! :<W+<mff8F2_07)#Hfiz#"0!" 6:+fiyz{z|~}agjM}VjMh>nfoFl3QnCkJl3|G} [ffpM .3XfiU;= c E;fiY"#P$+$,ff>+,P<-<<M E0"fi>P= X- .+?H +ff$afiF;ff"PfifffiGX*ffF%/ff+;"# PD-,fi?= MXfi .#5v=ffC+W% "#$WL= \ + .+!fi] ("#$PCfiL= .3"#D#@ ,+%JZ"2'!$ff+ ff- q /V= +E;$:4 , + .fiZ3fifi= /"#ff?#%ffF%J.&%/>+&fiS+?ffVff +fi f />;;E;#X5l ozrzlZPB?fiq"!2'$ff+ fffi ?=M=ff!"SXX>)"#ff"#$Afi\+!"#ff+T ffX"+-Vff2/>3-:T+=?Pff$0]%'("#?0 P$D#+fi.+= fP=+3fi !fi= /,"?' $5v;=$$."=f ff$P$; ff-/>QXfiBME=+F%'$+ $F+%8sX- .+,ff+f/?+BMfi"#$F%'_! 2$R D!fiS3:$= +E%$+P $4+%= QXfi .Tff'+Y"+, $!1#*$"#fi< + .+!fiJ#+ +fi$6:+fW*5E;Bbff#fiMff/>3fi 2--D)ffD<Sfi- = /%9ff+Y + .+-*:$+s4P= #%"#/Gfffi/WD.w[QfiM +'ff$T?"#/G/GVff+!%P/E;fi=fEc=ff"=\ ,"+Z+3fiffUYV"#/G= \S+!fi? $!D.fi0"= /W$f+0 + .+!fi`!.+fi$:WE;#Xs?#@(fffifi0=+#"#= CffS1 #N.M%<= Y"!2'$\+ D"=Gff+ ff- 5bI$"#-bG+ 5z"#MP= a9* \ M+/G/W4:13ff+ XKLb+/, =+/G+<-$:<$ff_ /?&fi ? c%./GE;B4:(ZI$"#!fiqe5XJ"P=+."#!fiU$; $"P!#fif= J)ff3 #N.M+%<ff'++ .fi?.ff+f .fi*5v;=ff'+)b $ff.M+Z3fi !fi= /V:wuHP= V(!/>+fi"sff+q + .(K%Tff + 2.+-P=+CG 4:;"#/Gff-:_u />"+5LI ff $G/W$+,=+W= f Gff'++#+DP$\V+!&%'V= , DS :*"#/GfffiP $b/G$b=+bP= Gff+ CE;XX_3-E6(cN+fi !fiZffZX% ?#@ff.WH ff+.fi$s+%= ?Xfi #+Vff'+ PSff$D= >Pfi6.:K+/>+!"PfiHW/G$+F=+P= 3-fiP= /#@ fffi$;P= J"#b+%8 + .+-F 2'$ff +fffi'fiV= $:fi;E;XX* b"#&'ffT+V + #+fi?/GPT=f"#+.5/>"Pfi&\,="PBfi$, )\ .+ff:_]%'GE;qP$579fi.$:Q=+ F#+$-C"#+%4fi"#/GfffiMff+5 (G "P=Wfi"#/GfffiPff+>"G#@(WfiDPJ+#@() ff3;D /,C+%T"#/Gfffi!fiK />"Pfi&V$Rfffi$s=+JP= > + .fiZ3fifi= /"#&ff;E;bfi"#/Gfffiff'+F=+=+cf b"#/Gfffi-*:E= P$;"#/Gfffi $P$Rfffi$M=+b\ffSQ"#/Wfffifi\)"#ff$45fI$"#4:*ff+] + .+!fiVP$RfffiP$G"#/,fffi+!fi?+%1#"#fi ,fi;ff'+ fffi >ff$"Pfic "P= +"#J+?#fffi G+%ff+V6:fffifi ?+%KS+'+fffi$;E;-=fffif= ,"#fiVP"= />#.:4;E;#X8/>+Bfi G Eff$"P&fi65MI(P/G2+"P- ,$Rfffi$;P=+TGff$"Pfi*:ff"#JP."#$4:ff MJ"#&ff$+ D3fi*5;%'.+/GE;PB,%ff+ fffi WDV .+fi+?F!$f E;, /,'$;=3- YP>ff>Efi== b+ b%QP$?ff+c+?= +E= f+b/>+ff- ff+$4[Xfi #+Jff'+G;";FP$G<C"#/GfffiPT+?"#&PDKff,%_+-fi bP=-fffi/V5fv;=ffsff+"#ff.3-M= !J+.fffi ffb=+,+fi$VP= >-b fffi/3- fE;fi=Lfi!fi3_"#!.3fiff.b+]ffff3"#Z-ff%P/>+fiqP=+J$"#.ZEc=DZP=PYZ#fffi DC+ $+,= P5 fffifi ZV"fq E ff-/N#bfiff+-$3- = AXfi .Lff+/?+."P=u=fifffi!3T+ D3b"#fifiG+%JP= "# PDfffi/V:4? ("#$PM=+M (ff"#$C>"#&ffMfi"#/GfffiPYff'+*5bv;= G + .+!fi"#$/G .F?"#/GfffibP=ffFff+*5g.W$#;'b6PGG+bSP'-!zY#'$'G.H!?SP'-!z, #$''H!3f.-X6*3f!'6'_P$6-.+ ''$*#*.3''H'*X#44'PH.K'3<#!fi3$fi yz){ {z {z0{mez)zx 0 yz{zy yz"z{v = + .+-G "#$;"#&!.K+%KY!.+ +.W;+%1ff+ 2'P#N /GD;).+.;HP=g ;=+_ f;+?"#.SfiD#K,= Mff+4KfffiF= J+ffxXfiH,,P."#<P= b#N /Gff./>ffbE= = MXfi .Gff+fEF!fi +fi3xW .+$45JfiEdP= .3Kff+ fffi f fffi/T?$+."P=\= =q?fi$"#$ . =V+%;+ff+65v;= f #+ =*yzCff, $ff.,= fffffXff+u\fi#b-$$$JP $ff,Nff= $ff+5pT #+fi?ff+ ff- !.+.C+,= ?DY%T= f . =i+]$+#"= $C%'G (ff\&ff+4b=++!&N$M= G D3 5W^HJ .+$M= ? . =qffZ""#$&-#fi\#Nfffi "##3fifffi ;= Gff'+*5v;= Pfi+3 =%+> + .+- 2'$sff+ $:G= = K=+4:P <>+fffiP.+ff,fiZ= , #+ =*:1+= ? + b) +fiJ!$+."P=fffi ?%P/P=+b+-D$5,^&b/,b>fffiG$+."P=Vff+EVP= b .+ =AXfiBb, .fibff TSG/,)Y+ff-b>$+#"="BffE+.= =>= b .+ =?DGP."#- Y"#P.3fiff.: (ff"P- G/Gb+P."#<ff'+5 T"#/Gff-+u!(P/>+"? + #+fiuSfiX= /W+fffiVP$+."P=LL fff-u= .+ =LE;-="#&ff!fi >+ff> ffb/GPb=+f"#5ZF=3Kfi/Wfffi/Gff$b K3fiX= / $fiW</G/WJ),,Tc^HE;B .+&fi9+b$!$fib!.3< fffi/ff/>3-5 @-/GD#3_fi$b"#/G+G J3fi !fi= /rVV&-/,X+#*$:; xK0 b+/, =+/GTfi3:b$ff#5 G$!fffi.,= +EZ /?+"G!$ff%P/ff+fs%T"#.3-&-/GfffiJ+? ff+ fffi/"P$65E;PBGV+]/>+B$F= s%Xfi$Efi Y"#ff-fi[K3fiX= /"+ $9= F$"#<%(= Fff+ 2H .+fiT "#$9E;fi=ff-J+b#@(P/G#fifi/Gfffi%'.+/GE;PBw5 *"P=bfi*+/GfffiK;%'/>S3fi &:$b +ff$*F%'.+/GE;PBEfi=VEc=ff"=>+3fi+PG= Jff/?3fi 2-fiff)ffD,3fi !fi= />;xfiB,= ? XK!(P/b+/, =+/G+Kcfi$:K3ff.:)+ff+3-DU,ff/>3fi 2Hff)ff b $ff.+!fif + .+!fif.+P +fi$5gg TM= c%#+/GE;B,sfiD$fi M*9 (yzF#+&%'/>+!3w+ D"=?Jff'+ + 2#+fi*:++G!= SEL= +E0*9 (yzQ3fi<!.+fi$1"#ff,M ff$GPJ+qK!$+."P= 2H"#DP+= !"65g Y+3-DU,= C.ff+]E;qff'+V #+fi\+V + .+-*:w"P=+."#fiU#fi W=fi/,XH-HG$Rfffi$?ff?= Jffffi+3*- b, (ff"#,$ff *5g YPM\/Gfffi!"3K#@()fi/Gff.M+\ff/G#+C%?&fi/Gff-a"P'c+%< fffi/?f!(P/>+"s#+!fi=fffi\) E;"#/G .+-V-/GG+\= >fi/,XX f)E_=- fffi/r+?= MXfi #+Gfff= .+fi>3fifi= /fi;E;fiP=*5v = ,+)b "#$ Jc%'+Xfi+ET6[;E_CN.!TfiEfiME;PB?fi\ff+ fffi fffq + !fi;3-fi Gfic+fi-5<c#@(ME;bfiE= sfi$2H"#/G/,fi/WD; .fiMff+ fffi3fi !fi= /ZEc=ff"=]+b!$4:*-iff+- qAfiDP(ff"P- V/>+ff\+%T+<yzC +."# P$5I$"#fi?,= f#@(ff'3fiF= bff.3XQ+%1 .+fi>3fifi= /V59^H\I$"#-CE;M +TP=++\! 4:"#/Gff-J+f />"+5I+fi"#f= V!$L% + .fiff)W]= VRSXfiZ+%TP= fff+L!fi$%/P=Xfi .+P:-"Vs%;G)%/ .fiMff+ fffi W=+V+P/G G + M+?fi+ 2+sXfi .>ff'+*-ZI$"#fiqWE_,+3fiffUJP=ffP.ff++\3!>"#M/Wbfiff$!fifiDP."#fi<)E_\= ,3fiX= />Q% .+fi+Aff+ANfi 5v= ffiI$"#fiJE;w6$bD'F$$P# fffiffff #<X#)X#'-.M.JP-$FX'K'.3'<$K#Xff3fiyz{z|~}a= +EO= SE .+&%'/?+fi+31ff'+ .M"=\MT+/W/G4yzs.$ff<*9 /"+V)G++2fiffU$Gfi J Q%#+/GE;B45KI$"#-s.K </Gfffi!"3w-$5 %FfiE;fi b#'+$E;B?I$"#-(.:)I$"#fi\,"#!$< P$<+?)D$;R $fiQ%'F% P TP$$+."P=*5"#<@w%$!i&(';)#*",+.-v;=ff$C+%ff+ fffi ,ff> + .fiG=<)ffi?= cXfi#+ ;%';/>+ffG$.: +afi>/?+DX*ffF%P/>5^Hf=ff'_!$"#fifE;M-E=ffFE;BG !fi /:fffi ,,/Gfi++b+? ;fiff#$"#-J M"# ffE;BGV<5v= b&"cff$a=fffiff+ ff- GDf + #+fiHM&fi/,X'$;E;BWfiV"!2'$fff'+ fffi :.&%/>+XSff+ fffi :;ff+ fffi CD?+fi-Gff3)4FY!+fiTs E ff-/fff.+!fi- ,%P//G/GW ff-/==f)Z+fi$? Pfi&fi: = ] (_ +- G=+f!+fifi>,= C E ff-/V5v= V** Z / T+/W/G4:$ ffM,Z"2'$Lff G=+G+-$C fffi/>Yfi= Vff/>Sfi%YIU$"P=DE+u"#ffBfi 5`= Lfiu D3\ ff"#q\!=LE;fi=]+'"#ff+)fi$:ff9* bN.Ffi$Q1024365 7598:0 36;;+ffY Pfffi/>Q_"#/'"#.K=</,fi =D<+!;%'/=E D3 :+]$C=+,+3fi &s=<ff;>36<5 ;?>;C%//G/GZq"+f+fi !fi]H#xfiff+4.5Kv;= b#Xfi Mff+WF= ?/>+fffi ff$Gff>%@BAC 5 DE5 2GFbSfi-= /=+;-$<PY+!&%'+ff Er D3'G+LP3fiG fffi/>,=>L +!?fiL= #X- q"#+!fi5^H?=#@($"# P$= bff'+*: +WX%K#@($"#-f$fffi#K-?%3xfi b1<ff;H8:0 5 <_Sfi-= /+SfiDU$= M%SXfi+W$F= TP$fffi;+%*=++3-(&'Cfi/G +P=fiff#@W%F=ff;+-fi>!J=+Qfi;E;XXCfi$WfiV&fifi+FE= b-_EXX4%SX1+ D3fi*59* Z ff$!$G\E;fff#+ ?+% Pfffi/>sfi/G).D,Z"2'$Lff fffi [?= +E+ff"Pfi+PV ff-/>:= +ErL!fiq]+-fi%/ = ]"Xfi .:;= +Eu + >/GX%f++f+-fi*:f= +EG!J#@ $"#fiA%3xfi b,-/G +J $R ffTPfiSS5J+u /rfi/>XfiV ffP$$M= , + .+!fiV Pfffi/V:4+fiI$"#fi],E;Y!Y%#+/GE;BGPa3fiffUJ*9 (yz/G(xNw"+fiV!.+fi$fiV!/GJff.3x 5v;= cJLKwc(!/ fiP/>+*:ff$(w"#ff%'ff.*P= < ff-/d+%K +fi<ff+ ff- :zc3, ff$!$<P= fffi/+.% <MG2ONP365 @B;T + #+fi,Cff+W%SXfi 5QJLKwC+ D"P= $Kff'++ .fiZEfi=\"#/,fffi+!fi\+%T."#"3_"#ff+]&fifiZ/?+."P=fffi 5f0= LVff'+%SXfi cff$"#P$?fiF"P'&XN$f;#fi W#fi= TC%H3Xxfi , $"#fi-*: C%SXXfi , ."#/G:"%9x!fi J ff3:+;JPG <%.ff$5^Hfi JP= T)$"#.F+%KJLKwM=<ff$3*E;fi=fi"#/Gff-J+Wfi"#P$"#;BD +E;fi$ff :DP= b ./VyzF/>3fi?3-!.+ ,fiff+fi$Fff'"Pfi,%H3X-$>ff'+qPVE;fiP=V C=+/,- =DM"=ff-J= ,+/GC ff5bJQKw,$M>!/>+ff"E;PBG, $ffT+!."#-"P'$;%8"#fi<P=+T"P=fffiM= J+/Wb D!GHE3fiBX+ffffifi >+Pb=?-.+"#$;%8#+.+Xf"#-:%_#@+/Gff-+.5v;=RTK(Uw) Kw, /PI+fi/G/G6:*$(1'V36<ff0 2OWXDA <@B0 365XA240 Y8(YZ02424;><.5F0=ffXfib= JxP2"#GE;Z?.+&%'/?+X31ff'+ J+Z?"2'!$\ff+ b=M bZ $"P'#fiff#N $4:8?/?+xP"#>"#"#s= SEP= YE;? ff$s+%_ff'+ .M b= ?.+!fi GfiD%Fff+f + .fi*5Q_$ff2'!$> /?K ;=ffQff+?'b!fiS34+%1C_!+fifi,%P/MXfi #+:+ [R*TQ(Uw) KwT"#/Yff- $</>SXff+W%#+ /Gff.9%'<X*ffH= )#%ffxfiCfiffffff$"##+%<P= Y"# ff fffi/V[5 RTK(Uw) Kw,X*.%/9* ,fiVE;,= ME$ [N#T%Q3x :R*TQ(Uw6 K4GffD$c M%'/=0 2\365X7>5]80 365XA23fi &;\= Cff+*:)fi ?G'ffDx%? ff-#J)#%?Xfi .\fi+3 5\^&$fiG""# #JP= ?%"#,=J= fPfi$ff+]E;XX;/w$E;$4:*+\"# ff.Mfi#Sfib=!"M?+."P=V-$5J9* :4\= G= M=+4: /G$3$fi yz){ {z {z0{mez)zx 0 yz{zy yz"z{=+C= ?fi$Xfi .VffE;xX<f\"PfiDG =ZN,= G E ff-/ !V=+sXfi2fi,C q .+fiVE;Xx<G $"#$P+5I$"#4:/,"=Z%_= 1RTQU46 KwWE;BAJff$\ff#fifffi >Gc+%13-#+.Q%'TR+ffXN$f+?/GP"MS+fffi$65v= G/>3fiff$f)=fffi]= >0 /!+.+$fib%'/P= >= P> />s/GD!fi $+)S[G=+,= ? "#$,+%Mff+^0 C 08360365XA 2,A%3-fiZ&fi/Gff->#@ &fi\= f P("#$C+%ff+%F ;2\;<ff0 365 25 ;C"#$R "#TE;T"+? /GM=+KP= T3fifi= /=K .+$8Xfi .ff+ t`+f= ,"#P J+%K= D!Jff+ tLP= J+/G,;= , + .+!fiV3fifi= /+A=ff+V!"# $fi; .$5F^HVP= Y+ZfiEJ:eff+f .fi?8 MG$"P'38"C+%Kff++ #+fiH-fE=ff"P=f=; Wfi$f"# Pb,.@ fffi+-.5v;E;Gff-$"#$+%KE;Bfff#fi)$\+= ,+/GCfi/GJM+Lfffi/,X /G-[F=XKu / b+/, =+/Gcfi$:3ffT+]P= V_STQ`8) ac bu / e dK#fiD!:<$ :$+e.5v;= b/?3fifX*"#J) E;V]+\ xK?;= b ff-fi Wff+ fffi >Sfi-= /V[1$GZ"#.3-D2')DX \$"P= ffR V&fi/,X'+J]<= />+*yzW.$fffb(gfQha/G(xN$]D~\" X-$+ViDffffXfiM.$ +.:E= $M xKV$c,S+!+ff_% jTQjkIS) jZ v+:4$(.:ff=fffi.+#"=ff"Sff'+ $5;I$"#-VG"#/G$;= $bE;,ff+ ._fiV/GJff#3X 5v;= l_STL`K6 ac bq /3!Y#+B$;,$+."P= 2'fiff$?+ D"=?,ff+ fffi )51^&TX*.Q%'/+fiL= VP+fiV]"Vff'$ CfiLP= V ff-/G2H+fifi Z P("#$65 X- .+]ff+0 "+J-u.+ %P/>+X3f"2'$ff2'ff+ fffi %'.+/GE;BL$>+fi-LLZ-G ff-/3fi \E;-=Z /W/>+\+%bE=+C E fffi/?b-YE;ff'Z)Vq!fffi.+fffif+-fi\%'$:Q ,fi"#ff.3fiWX-Xfiff%/>+X0+ V=8(<PA>7;nWWGP=+f .+P$uP= +fi !fi*p5 oMfiSfi3+3fi :fP= b= ;=+4:P$; .D'31ff$"#fi-F+%1= !C ;75cW5XA 2OWK=+;$fffiP$Wfi= Y!+fifi*5;^H\+'"#ff+$: dK#-DyzM /r$"#. c/GPbfiff%'/>XV+M$"=\"="#J)+fiff=+ffff$J<[YXM+%;%H3Xfi$\3fiP+fi$:)%b#@e+/Wfffi5fv;= G#fi,#*$"#fi $PM+%=E_?+ D"P= $b/>MP>=fffi YV= ,#@ DbP>E=ff'"=+fff'+ fffi Vff$"P&-b D$G= bff'+;= />!#fi$#<"+b ff.PD V+A#@(ff-+fi$afiV&fi/aX+;ff+ ff- Gffff$5^&> /G/>:+E;"#&ff< KE_Bb>fPbT"#/Wfffi/Gff.+CT/WDK#@fffi bE;PBMfi.+ %P/>+X3 ;"2'$Wff+ ff- 5Kv;= ;+PK=<"#"#D#+$W>ff#fifffi J='2"3xfif#*$"#fiC fffi/fi.%b+"#ff'+ff/>3fi5a_2'!$ff2'ff+ fffi f$!$+."P=q=3,#@ fffi$?P= b fffi/%8= +E,Pfib"$F%'/P= bff+?Xfi #+tLfi>"#ff_P=fffi/+%*= +E0bfiff#@?= /#$"#!fi#fi5+<:ff?= T= ;=+4:<,ff/>Sfi 2-fiffffff3fi !fi= /V: +fffD$c ff$;= bPfi+3*;fiff#@fffi G fffi/>FfiVDfffVE35v;= ,/>3-V $"#fi$;+%_P=ffE;PBV+G#+_P>#@ fffi,=ff$?=+Mff+Z .+fi+A%3-fi/,- 4 $ff.+-3D+JSfiX= /a"*++D4+% = K&'" fffi/%(ffb .+-*:ffKG +ffM #Xfi/,-+Gff"#JP=+;=ff;fiE+%ff+V + #+fiWF/Gfffi!"3XfiWfffi:+Lff qffT\ +ff?V= f"#/G/, fffiZ+fi/Gfffi/WD.fiZ%ci3-fiP= /=+CE;XX3Xfi+E#$"#!fiM fffi/!+fi.FWJ ffxfi;$ff=ffFff$e5ZC SE) +fi\= Gff#fi /GDM+%< M%'.+/WE_B>Efi=qfff$P"#fifiV+%<= , ff-fi%./GE;B,%'; #fif #+fiMff+ ff- 5r;%s "" )#*"!+.-Ltuv"^wLxVy{z+|-u.}I+fi"#>P= 03fifi= /rC+]#@(fiZ+%T+!3fi2'+.ff$:"##3fiff2'D!fi :)fi$Y"#/W2/fi/Gffb .fi,ff+ ff- q3fiX= /V:4E;G) +fiZff\ $fffi VP= G .+!fiZ3fifi= /,fi.#X%5,+E;$:4E;GffVVfi f= G #+fi+\+%_P= >+u /V:*fiZ= , "#$fiDP23$fiyz{z|~}aff"#?/>+ffV+%= > .f"#P $J+q% "#-b $ff$f-/Gfffi/Gffb= ,%'ffX_ + #+fi3fifi= /V5 b$+/WDccfi#%-t`,#E= ? ~\" Xfi$PTpi;DffffXfi3:1$ (~_#4:4$+D(1%';/GPJff.3X 5rzlrk~lklk nzv0 7>365XA 2aGP"= />"M $!D.fi?+%<+f#+T3+3X+ff-T,= Cff+ $5 \"#fi="#&'.b%c2\0 @B;D:KV!Y%8(<P;7A 24C 5 365 2WD:+0 CCY5W3':KC ;>YZ;>36;Y5W3.:K+]V!Y%5X2\C 5 2GF7A 2W36<ff0 5X2\3 WD5v= fN.C%' >+P;8(<P;nWW5 2WJP=+G"+`"#ff.3-?>0 <5 0YZ;nWD5]ZV!VR $!fi/>+PB(cWffD!X%f++fffi$: ?%'cfi.+"#5 ~Qfifi V"#.3-D.c+J!$VPGfi'"+,=+?+!"#ff+MS+!+fffiG"+ b)>) \V?"#ff"#!.+ffbbV/WY= bS+!+fffi5cM+V"#fiV"#P$fi GGG&-/Gfffibfffi "B E;'B:>#+$[4 6 k:G ( nOG (G6 GG ( nOG (GOG ( nOG (OOG n p GnG ( nG ( (^GG( GG(GW36;H8w5^H#+D!+X b+Y"#!fib-Dfi$Cfi.+"#F+%Y"#-b9-$fiDPMffYK.+K +fifi , ffR J+/W$;G= b++fffi$Ffif= J"#fi*: + ff<&- fffi ,= JVa ffRfiff#@]fiP= fff+*:<Zff+L"+u"#ff.3-Z/G?=+] ?fi!.+"#?+%T= V/G"#fi*5P= #%Y+Afi."#J+%_+\"#fi?fi$fiff>Wff+VEfi=qffiff#@V=T ffR #fiffffXN$Ffi35% 7A 2W36<ff0 5X2\3 WD:3E=ff"P=G#fi= <"#!.3fi,= ;#ff<+%4 E;b8fiff+>3'"#D.SfiKJK= ;ff+,<"#!.3fiC= ;fffifi D%wS+fffi$9-YP= !5 ,#fffi J"#P.3fiff*.+B$=%/ \S:+E= {44+%__P:D+sfi'"+$9=+K= ,E;-=bfiff#!@ 4/,K ""##%';= ,E;-=bfiff#@ 45 ff-fi J"##3fiff*9+%4= F%P/ ( $ w;( $ .:+Ec=('T?+++ff-Y+ )$+fi WfiZ/GGPV-V= ,ff+Z+m$c#fi= JWS+'+fffiJJ"#.+ff+ $fi ,fifP= bff+*5 /ZZ3'i .+0"#.SfiD?E;fiP=$"##u%YE=ffLfifEWff"#$fi0= \ff*5v;= #%'fff+*yz,"#P.3fiff.M'J"#SXfiZ\,+%T3fi#J%T= W%P/ [+v: GE= V,#fi= Tf.fffi W;fffifi >"#!.3fiff$: +<P;0WA 2G +.,P"# , ff#N $)#fi+Eb.5v;= CN3_"#/G ffJ%?ff+bf!b+%"+P3fY5X2OW :$"P=Z+%;= C%/ :*Ec=4<+J+J!:4++f#@ $&-*5_v= bXfi B?$"## = M%H"#P=+ C D!J+%49fif= bff+?'<PG/>+BP : E=;, $"#-fi?+%S5K^%Q,ff'+V"#ff.3fi;sXfi B-_/,M3G"#ff.3fi?= C.fffi % ^ 5ff+V"#&'.F+%<G+%<6:(W+%<"##3fiff.:DVGc+%*Xfi B 5Lc*: ;: ( bS5 0'JV!J+%;#@( $P&fiCff$"#fifffi8Y0 242\5X2Fl8<ffAYZ;>@J?PfifffiV= V fffi/Vyzsfifffi!3T"#fi-:GZW+%#@ $fi?ff$"#fifffi = V fffi/Vyz, D3 :+'b= fJ%c3+3X+ff-a"#fi5?Zf! /GG=+l(C3S3X'+fffi,VP=3fi !fi= / , +fi3*S+'+fffi549$6S>fi|$S&TPP-#$'-3-#!'F.H!3*$$;.3''H'ffe, 4!*';'3ffV43.HP-#b$'cM'$K''*P'$KSP+3fi yz){ {z {z0{mez)zx 0 yz{zy yz"z{c#@ E;J#@ fffi+fi?.+ +#? $ff.+-3*"PB>+"#ffTWff+ fffi W fffi/P>ff+fff? ffXfi >aff+f=+"#ff.3-5J?E;fiP=>+/W5 245 365X0Y:fiff#@ :ff=3fi J , $"#-fiF ff#ficX!$:+ E;fiP=>+WX"#fi ,+%K= b fffi/VyzF-fffi3*"#fi-:e5J?E;fiP=>+/WFA0 Y:+-ff#@u:ffE;fiP=G $"#fi!fi;"#&!fi J%P= ff3w#@ $&-<: c/G +fff#fiMx.:qe5;= J&fi +-T#fffi >"#!.3fiff{4^?:)5; GS+!+fffi2'fffi- a"#.3-D.6:e5; ,Xfi B 5fffV/,b"#D#3fiqfi$TP= JE;PT+= ,#fffi :4+fE;G"3X= ,ff+E;fiP=ffffiG=ff'"#P b= !24MY Y8Y0 25vE_cfiff$!fi )fi$%wMff+G+Qfi.K+% A8:;28<ff;7A 2\C 5 365XA 2OW_+sfi.K+L% 3 G<ff;03 N;2\;C%Y5X2OW5Kv;= b%'/GF= Yc+%8#@ $fi;=T+ $;fiqDf*yz; P$"#fifif=3b >"+S9! cE;fi=fffi?= bff* = M+PFF= J+%K#@ ffX"Pfi"+SP#+fi=fffi=+b/,- =DMWDffXxXN$D= Jc-qP= Yff'+*5Y7e/>SXfiV+\)"#fi-ffi]Gff*:.+P$^OS:4bf!bfi\= Wff+\=+b=M P$"#fifiW:1+A%'bE=ff"P=\= ,'MXfi BWfif= bfff+%8P= M%P/\ %'T+ff!m45Xfi B?+%<= M%'/\'3 G<ff;036;24;C;;fi\"b= Pb= bm4Qfif= Jff'+"P=V=+5;P= Yff'+*yzc.fffi V"#.3-D.cE;ff3Xfi+E4;f)G.ffP$qS%l4;+\#%'%+:5{\=JAD!."#fi-LH#fiP= = Yff#fi+M=+C= >ff'+*yzb++fffi2'fffifi \"# 2!.3fiff.QE_fff3Xfi+EG ffX%'>Efi=G5ff+\E;fi= V)\ $"#fi-b+ V= P$+ $AXfi B cM"3Xfi$\WA YMG365 2ZPf="P+P$Gff fffi G ff-/V579fiSXfi]E_Afiff ff"#V=<ff;0WA 2] +.]"#P : $"#$P+L%'? .+!fifff+ ff-J$!D'3<%'J + #+fi*5 Pq!fi/G>fP*:*xfi B\Y"#!.3fiffMb ff$\fff+]+"P+P$1<ff;0WA 2b$"##fi. PD5 $!>"#&'.+%4 E;M+.6[<+KbD/,)+$"#.-E=ff>= b"#P.3fiff;E_ ff$H#fi= ;LUkU( n b :)Dn bDL\IS-+1:TkbD\b.:+f(8#-=Xfi B4: *: ;= P$+Ffif= bff+?'ffDx%fi ,= b+;+%= bff+f)#fi G3-$45;I$"#fiVq 5"#!$;$Ffif/GPJff.3X 5$4#_K+#QS3w#$'-'6-.F'SP* )'$#T'3'!P'S >Kzff4+*.c'L9z'(9X'!z-'$$''$K4 e$' *$'# kAX'.]+&$F''$L*3f>*wff$'($4$|S6-.P$#$'c'#9z'' 3 4.'6z'Q' & +43 'HP'b'_3'$'<'''P-&-#w[ KP<3SP_+--D 4X#16'$'-.3fififfkn{rmt9ti!tnfilyz{z|~}av;= , .fibff fffi Sfi-= /c$A\= bff$?%_.+!WE;fi== ,DffXff+Z+""#$fi#fi<P;2\5X2FMfiMD\"= ffD&- ,/w3EH)Z"#fifiVb= P$+ $AXfi B)qfiEdP:ffXfi B : "#.3-D.FaN @>fi35<v;= J3fifi= //,-+$;#fiP= _Ec= qG"#/Wfffiff+s)%' W ""#$w9E= G3XDP&fifffiF#N /GffKfi=$K,#@(=$W'%H3Xfi +#5F&ff<Mff fffi b fffi/E;fi=Cfiff-3("#fifi>k( , D359ZTP /G= = <P= +;=K= !<+%1"#-K3S3X'+fffi;JP= ff+ $: ( :+'N @ $45KZ+Eff#N JJP 2-fi#4%' "#fi*: (w:E=ff"P=Gfiff-3XXU$F= b$+."P=f+f"3XF%' "#fif=%'/?<P= J#N /Gff "#$5tkl { rmt ntmn:rzl omnfi !k( #" G %$ [<8+?LD0 5 YZM<ff;&\[<P"#;= JffffX*ff+?%'/'>k( +(nPlk0t*) k :G (&) n k G W!$+."P= $K= =>P= "#T+%+Sff+Q%_,+fi !fiGff*:Dfi= q!$+."P=L=fiU]-DP3Xfi:K$"=L-/GV"= ffD&- \ff+uu"3Xxfi +) n *:KE=ff"P="= ffD$c+?3fi#G&fi fi[/w3E0-f=+ff*5tkl h n-,Wt~n ntl w/. & G $ [<>QD0 5 YMG<ff;0::( 2143& /54. nomn:60 ::( ,';/G l nt= nlk t,D0 5 YMG<ff;7?^1 #fi$"#Tf#fi/Gff;%P/80 (oM#fi9?%'/8 0 :(WGfifil ntnPlk0t:nzvn> fSX*#fi/Gff.;+%#) (M;0:qi;#Nff- aCff"#.Q+%1E;J+#[1#-$"#fi G{/w3Efi?= Tff'+V+>? $"#fi-JP= $+ $xfi B#:*= Z .+fi \3X<)D&fiff->"#$"#!fiM= B/w3EJ5>v= #-$"#fi\+%E=ff"P= /w3EZ"#P$"#Y $] Y)f$"#&'ff$4:< ,= ?/>+fiEc=ff"=fiC,"#P$"#$/,fi =ff;=3b,:)E=ff"P=?;E=fff3X*DP&fifffiJ"#P$"#fi;+PJ ff$f,= ,$+."P=?%'fffi$5tkl h n-,Wt~n { rmt< $ [<)!_%8ff0 7 1AI#fi$"#Tl/w3E%'/8nPlk0t:= >0 @L? 0 " (<P$"#fi Gl/w3Ed+/G D.F,$!+fifi G+V)V"#fi-f$!+fifi G,= $+3[qtkl tBA nlCrD(E0 " $ [<)!+%1ff0+f)V $"#fifiXl ntnPlk0t*) G GF 0 " (nzvn1nl 0t*) G -H 0 "3fi yz){ {z {z0{mez)zx 0 yz{zy yz"z{G)"#fifi?"+Gb )$W#fi= FDG"P= DDfi Y+W#@D!fi J?=+<.Q=)D&fi!fi?ff- >, EdPf=+Tffff$[tkl h nv omn | . ntK:J L " $ [<)!+%Kff+C$"P=V L "# PD-Gfi(XL 8"VC.ff$?)#% L S: + L K G"#-fif ffX%'fi ,E;fi=J l nt"J L "n:l L G4G L "C$"P=V"#fi%Wfi ( E= DJ X"#ff.3-_W"#fifi> ffX%'fi GE;fiP= JLM :-N T[ G L " ("" L " Nn:l L G4G LMJnl 0tV= sX;+%ff+"#+xfi$"#$f+;xfi $qG+Ve5qv= G%' "#fi L Z.+B$J+"#fiZffifi .:/>B$J\"#ff\+%T= Gff'+*:fi#+D!+$;= ,"#fi?-DfGP*:w+\fiPG= ,ff+E;fi=f= C$RD-$f#fffi f+fffifi >"#.3-D.65*^HM P;P=f= b E;fif ff$V!q+?P= b E;fif"#fffi$fff'+*5tkl n 'kmln . " $ [;PI*:'+4L [d, EfEfi=f"#fif+f+?-ff#@V ffR bPO) [d, E$f+%K= M%'/'P LUkU( nb LMQ\N [Y"#D?+%#L R N?$"=+%Lwyz% n: RN :ff$"P=V#+ $WE;fi=()?= J#fffi L 4 LGM + LM L OfiN :e=f#+ $?E;-=;)nPlk0tZ LM :%XNqc+E L GOa _C"+34X- BY)E_?E;J#@fffi ,QfiGP= Tff L + L :(SfiE;fi=fP= b$Rff-$f#fffi >+?ff-fi "#.3-D.658c'"#b=+= Pb/,fi =ffC/GbP=+GE3VAXfi B\= GE;Vs$"+!>= PG/,fi =ffb?/GPG=+Z GD!."#fi-\+% L=+;"+> ffx%,E;fiP=G= cXfi B, )D&fi- 5ffv;=ffK.+!fi,8ffff"34J= cE3G jIaJ"+39Xfi B <#@"# M=+;= Y"#.3-D.F+J+ .+$?E;-=<P;0WA 2GP"# 5qtkl k* .<. l@ L " J " L " $ [K)c+%8ff'+C$"=V!+%Kfffifi Df"+&- L KG! JfN 2>1 G"#D?+%#^7?1 C EX- B L :S L) 7?^1 C Ed$TP DnbDL\IS-+ Q%>UGN?= J#fffi G"##3fiff L L R N : #+ $WE;fi=()%>UG'N :ff.+ $?E;fi=V)n:lW NnPlk0tV= ,+%1ff'+"#+X-$"#$f+TPV3Xfiyz{z|~}aL :Li$"3X =+<M= $1bcXfi B KKT!%44=+<"+G"#&'fffib;#ff$,E;_+aq""#&!D!fiVPb#fi= *rb?)D."#-fi]'5 5J#fiP= JLLRYff#fi$ ,.5ZbbP= b .+-V%4> @Z40Gff b=ff'<P= $+$5Kv;= b= PbDfifffiE3(M$+->fP= $+tu /Gfi*:*ff/Gfi*:1+]+.fifftLfiff+-SG- q.fffi+?fffi- "#!.3fiff.QG= bff*[q$q$tkl h nv omn k nrzl@ L :S L [Z L " $ [<);+%Kff+) 71?C EdP$TP TkbD\b L :S L \Z L Q^L <"+V)J"#&!D!fiG.ffP$f#%' L 9l ntfN 2>1 W"#D?+%#?= Y"#.3-D L . L R N : #+ $WE;fi=()n:lW NL <"+q"#&fffi,)J.ffP$V3%' L Jl ntN 2>1 W"#D?+%#?= Y"#.3-D L L KPOG-N : .+ $GEfi=()n:lWGNC$"=V!+%Kfffifi D?=+; D. L yzF#*$"#.F%'/ ffX%fi ,E;fiP= JfN 2>1 G"#D?+%#f"#.SfiD# L L + L L U N :ffP=V.+ $GEfi=()%>UG'N :ff.+ $?E;fi=V)n:lW NnPlk0tq3X* EOff+"#Xfi$"#$f+FX- $ : :+\6>++cM=+FXfi G> b',fffi -b$"+CP=V"#(ff$&- +fi+? "#(ff$&- +fiV"# 2.SfiD#,/,>)Z ff$45 , 7e>#@+/Gff-:= \+ E;xPD?/,fifffi/?3.W+%Jfffi-"##3fiff.F=+;/,, ff$fPG $"# & ]|#]| . %'/G! P=+ff#fi$^kffW[ 3 `_/_ 5D:4+a3 `__ !:> b_c_^*2 5D5Jwfi V$aM3Wfiff$- Gt = G"#P.3fiff.L<+Lb+Jff$fif.ffP> PJ(!/>+'"PfiH5L LL LCrn l nv-6k ti;tmnzv ved{ ..-dnPlnt~nzv v 9k 9vlnrzl 6 l~\" X-$+ViDffffXfiM.$ +1 PSb= b )fi$+%8P=ff;3fi !fi= /V[5MI ff $[0%'Z+ff0fi \ ff-/ Efi=0fiff-3Y"#-fif : DS):d"#!fi:X% g " 4Z""#$P&%ffxfiuP >]ff'+ihG:c= 0#@($"# 2- J= JPK-;hOfif+ff>&fiP+fi+>&%'-JE;xX3-E3 ff"#YJ!.+cfi?E=ff"Pj='<P 55MF/Gfffi $P[< k YEXX4NVG!+fifi>ff?X%1 C#@D!.5lq 5MI /?+"Pfi&w[kff-+K/Gb=f"#5,E;XX4 T"#&'ff_P= J+/GMff+H+!3w"#/G2/Gfffi $+C(!/>+'"Pfi"+C;#@ ff3fi $s%' = DbfiE;fi 9 (<$."=ff- >Gfi$"#P$f . =f+%834ff+5;v;= b #+ =V=c, ffR Jff$:ff= CDffX*ff*:m/n31'Xff#o\F'DE4X#PJ!''SP-#<3$'&3pfi yz){ {z {z0{mez)zx 0 yz{zy yz"z{+]\"3X_P:) n q #+$, (ffyz,"P=ffXff]ff"= ffD&- q/w3E Z #+fifi.C""#$.WH= ?+S<ff+C$fffi!fi ?%P/ "#&ff- q3X;DP&fifffiGE3 b+%N @fffifi.579fi P?K+q q ffXX-.+P$<= +E0#N /GffMff"#$Y%D-_ ffJEfi=Wfi."P=ffXff*5v= T"#/GfffiP $$fffiF/G$+K&-/GfffiJP=+KG+-fiCff+>+ )$+.FKMfi$S%w ff;fi=ff.+ =*: +A=+ { rmt nt~nrzl omnfiE;XX*ff3X-G'&fi;Wfi$3%K (ffMx%8 $"#$P+5/?+"Pfi&Mfi/GffX-$=+K=ffK-$"#$, #+ =CfiC%H"#<!36<ff;;$:K79- b $!.5Kv;=. =f=F=ff; G)$"+JP= J"=ffX'fff+%8,34ff+f ffJ+J3x9Sfi+!N @ $%'J%/w3rE qetu$"P=i"P=ffX=CVX*ffJ*:9.fffi ):4Jfffi- q"#P.3fiff ff$\fN @q5 >&-"#J $R D;#N /Gff.;ff-a ?/GT"#.3-D.6:$"P=>% 5X3 W<"P=ffXff?- =fi=ffb"#/W/,fi/GffMf= +sE qL= ff,N @ $45Gv;= #%'>Dff+\\= C%PD!fibE;XX<X*%P/?= cff+ff= s%PD!fiFfif= JE3,fiN @(${WA @B;/w3EJ:)+f= JP+/Gbff+fEXXT+ )$+Mf= M%'fffi;/GbP=+f"#5ut8vXv !t/xw;0WA 2OWlnh :he,n Wtmnntln vtivE;;/Gfffi $,+)+:+= ;$a .b"#P F$"#$+Pbfi>cff+ F=+%'/?fffi?#N /Gffb#+fi65_jkI:e%c#@e+/Wfffi:wffff$c ,= /V5c+E;$:ff= f +ff= ,&c%'bP."#fi ?Jff$"P'&fiME=ff'"=M? $"#$+P"#/G ffb+%;ff+Z + .fi?"#$ifiuP= V #@ $"#!fi*5 ~F#%V#@ ff3fiff- = ."#fi] "#$:= +E;$:FE;/G/>fiUf=<ff;0WA 2Z .!"# $a=+?$"#.= SEr+LE=ffuff+uEa#N $45X*ff_P$VP"#$?%';$"P=f+%8P= J= bff$;%8#N /WD$[IPdfi-*50= ] EP4Cf ff$0PuLff+ '% "#!fi^L 4.:c=++fffi2'fffifi V"#!.3fiff. "P+$E;fiP=V-.T"#fiZ"P= />f+G3'V ff$4:1SfiEfi=? E;,.ff!fi G"#.SfiD#<!fi G=_P= b EPf(""# .c3%P=fifffi!3!V)#%,= J ff3Q!*5bv;= ,$M""#/W+Dfi ?= $G"#P.3fiff.M+YSX8%<=%''/ PQUkU b V\ Q 5g;+3*xfi Bfifi*5;0= \,X- B>+%<P= b%'/4> OTc ff$G= ,ff+\% "#-^'J3! ff$4:K3- >Efi=\S+!+fffi2'fffi-L G4 ).:wZ.ff- V"#.SfiDl"#.3-D.9- J=+KP= T#fi$"#$aD#"#fifiY*% 4*"#SXfiJ!.= PDfi2!fi?$Rff-$?D?= ,#fi$"#$? $"#fifif% +5<v;= $J"#!.3fiff.FE;Xx)J+ .$gEfi=f,$V"# Pb+%8P= M%P/yPDbffL4IS-+ Q 5v = P$+J$!+fifi*5f0= iAXfi B4 OY'b= $ $Zff\ 4:K= WXfi BZ"+]g ;$!+fi$'%' "#fiT) eH MDfi \ f+%T= PV.s+%"#!.3fiff.[G#fffi \+%b= ?%/ 4L^4:;+u#fffi \+%b= f%'/ ^4:;,S+'+fffi2'fffifi"#!.3fiff. !fi ,=+<P= T= P$+fffi CD#"#fifiG+.% 4ffD$F "#SXfib%H3X%= cXfi B4yzF )D&fi!fi1W58v= $J"##3fiff.KE;xXw)J+ .+P$GE;-=C$f"# P+%= b%''/ PTkbD\b L >:L \Z L Q 5v;=ffM"#/Gff-$M bfiE+%; .+-GHP#N /GD6ff+ fffi :*!fE_, +E \fP=#@(fi;=_P V=ff'<ff'+ ;fiff>+V +fib3fifi= /V5z;!!+(''@v;= J+bE;,/>+X*"#$;)E_ .+!fib+f +-bff+ fffi )[3{fiyz{z|~}aNull PlanRetractionFittedLibrary PlanExtensionWorking Plan?79fi ,[<8+?#N /Gff?."#!fif$+#"=?fi.rmtZ"#5Plan Extension79fi P [M>#N /WDPff"#$Cff+?.+ $U(T(gjVE;-=W;+%8 E+Vfi!fi3*fT"#.3-D$5ff+:ff$"P=fE;fi=5;^Hu + !fifff+ ff- Z= ?W=Y5)<ff0 <E<ff;36<5X;>?0 =!?fiLE=ff"P=L= fff+]xfi .+$+."P= $?%'TWff+f="Pfiff#fi?/>+."P= $;=fi-fffi31+A D39%P/> = sXfi .ff+?F=0 C2|M W36;CsG/>+#"=f= J"# PDc fffi/V5e5+ !fiJff'+ fffi ffibE;-=V=ff'Mfi$qq3 P$V3Kff+*:*q"Z2."#ff'+ fffi f"#.SfiD#T ff$E= \= ,ffVEcfi +-3fi> .+$44 #+fiff+ fffi ,fiEfi=f,ff+fE;-=f >"#P.3fiff.;+f"+fff- E $5&^ >= ;E_. :)=f #+fiM+> +fiMff+ ff- Y+b$+#"=fffi C%'_,fifiWff+fi?PG+%;+S<ff+:9( C .+!fi,ff+ fffi V!.+.bb= fH ffR +TPDb%_= Gff'+?E= $,fiGff'+ fffi V) +fiGY/W+Pfffi.+PVff"#Wfi]= ?HDP&fifffiZ+,+fi !fi*:ffDP&fifffi>= bff$:ffff&fifffif+/WMfiDPfi; ff+.579fi Yb= +ET=+ + .fif.+P.<+M+?fifffiF (ff:)+fG+-fi?/,fi =ff+ $.#-SEM\fiWfi0= \\WfiLX*ff! Z3fi = $5 f]$fffi?= Z + .fi3fifi= //,Cf+fffi?V/G+. = ?WD<ff;>@A?5 2GF?"#.SfiD#c%/= ?ff+E;#X1;/G+Wff+E,= bPbDm0 C C5X2FJ"#P.3fiff.579fi $bf+q= +E= GE3V=ffM/WS/GDM'b""#/GffX'= $4[bff+ <P;2\;@B;2\34s=Hfffi)K#+fif)%'/G$ADf, .fibff $5<^&.+B$,+!3*ff+f= b=-U+ff"#$c-bE;fi==+Mff+*yzB7nG5XYC <ff;>2:4fb+%<ff+c'ffD'"38P>= Cfi Cff+#@e"#%=3fi , b/W{/w3E3-$458+!<ff;36<ff0 7>365XA 2M.B$*cff+JJ= F=fiU,+J"P= ffD$T"+P3Xfi B4:38!1+%"#P.3fiff.<ff;@BA ?>;$5v;=Gff+]Wff"#$LL= =fiULE;fi=L= +ffV&E=ff"P=],/>+PB$]%'fi-3<#"#fi4#:13fi fE;fi=ZP= >ff*yzJ&-ffXfi DaH $fffi \3Xf0 Y36;><2\0 365X?>;,E$ M+%3$fi yz){ {z {z0{mez)zx 0 yz{zy yz"z{Plan Retraction79fi Pq [ML."#!fi]ff"#$?+ K9.+ $]ff+E;fi=u+ = VK9ff+0+u!.3&fiffX- Yff'+;.+ $U(T(gj152-N @fffi ?= /w3EE= ffbN @VE;P."#$W%'/ = Jff4.5_v= Y&fiffxfi D+Pb= V. $?%'fi!fi3*#N /Gff$5^&LI$"#fi?E;>= +E=+b=ff'J&fi/WfffiG"= /W#t`+ /Gfffi \? .+-,ff+ $yzs2N /Gffq+ffxXfiLE;fi=0= Z+ffxXfiLL#"#V"##3fiff.&tL?ff"Pfifff]fi/Wfffi/Gfffff++ #+fi*59^H?= FE_. :E; PS=+K= b + !fi;ff+ ;E;XxwXXfffi, (ff"#MSSX+-fiM ff $##:fffib"+fN>fifif W/>+cE= b-V= Cff+q!"#J= sXfi .ff+Lff'"#$CfiCfifffi!3XXL "#/Gfffi $.:<+fiG!XXTffD$!*yG#@ fffif+$a+%T= fffL$ff +fffi /?+"Pfi&#5knnwr.l omn{rmt9ti!tnfil<v;= b + .fiG3fifi= /)%'/>;J!.+ +., P$ff= 2-N.$."=*:/>3fiff.3-fffi b= b$+."P=%PD!fi8<bF+%w3-.$"P=G+%wP= ;%/ }h>:K*(L%'/ }h?:UT(gj*C5^HG#fiP= _"}h0'?H)D&fiff-,fi"#/Gff-+<ff+f+K*GUT gjffi"$F= JE3G,/>+ff- ff+M= Jff'+f.+PT= bff'+*yzF #fi =ff#Ffi?= J$+."P=V"#[ UT(gjf/G$+ .+WhGy_!""#$.D% P= #Nfffi CfiMfi G EdP_c~$T"#!.3fiff.##@e"#fi>Qfif .+fiTff fffiK*,/G$+F .+PhGyz;""#$P.FffG."#fi J T+%= TP#N /GD#</?ffTEc= f= Tff'+EFfi +-3XfiG"#P"#$45qtkl { rmt n ir . l omnfi !k( #" G #" (!^ $ [<8+?LD0 5 YZM<ff;(!^% 71<ff;36<5X;>?;MJff'+f%'9k> +* %'/ (g^:G 21 0 C2| W3k(!^% >,/>+#"=+> >( +G #@e"#!fi& \? 7,1 : G g^>36A<P;& @? Wfig^ GnPlk0t*& \?l%bxJ b'effLI%#BUKKwbaMj4b*LjkUObI3LjTQ kRK<";ff+ 2'Pfi+3w3fifi= /<RfffiM&fi/Gfffi[9E;T"+GP= ff+,xfi .+:+/?+."P=fffi M%/>fi,= FXfi .Tff'+*yz DSDE;fi=,= Ffi K D3 54Z;PfiF= FXfi .+Pbff+E;fiP=J= ; $$/,)T+%K/>+#"= $:ff= $+B?fi$;fff"# D!fi G= bff /,M+%8/?+."P= $;E;f= b-fifffi'3e"#fifiF+,=fifffiSe"#-fiQ+%w=fi$Cff5KvFfi$-G= ff /,;+%w/?+."P= $%P=f D31+Wfiff-3*#@ $&fi+M B\+fffiP.+&Xfi53fiyz{z|~}av=ffb "#$,#fi$"##JVfi +fi,xfi .+ff+*: Cfi.cfiff-3_+ DS_"#fifib $/>+#"=?=fi ;-fffi3*+? DSw#@ $&fi;#@e"#fi5Kv;= !0 C2|MW36@B;2\34 "#$ ; D3F= sXfi .+PGff+=++ $fi=fi D3* b+C T3fiP$ffG-f= Jff*:+Vff#-$DSc%P/= ,X- .+ff+\=+,ffV J+ )$+b-Z= ,fi C D3<#@ $fi*5>v= ,Xfi .ff+*y4fiff-3("#fifi9+"= $,&fi/,X'$!fiTPT/>."=C= K- K fffi/ff$"#fi !fi*51v="+S1X- B(M+G3$4[M,X- Bffi= CXfi .+Pfff+V%<= C%P/ 4 OE= P1\<=M)ff#fi$A$"#/W$b+V)Z"#fifif+%<= b%'/ SeX%[b=cZff#fi$4:)= sXfi B?fi.!#X%"+Z)>P/G+$45>cE"#fifiC+>SV ff$%',+D\ E D3K%'/>65>v;=ffEff+Ac +.+ff$?GW#N /Gffb+%<= ,Dffx8fff%'TP= a"# PDM fffi/:4 ffX-B= FXfi .bff+Cfi1 $"#$P+Xfi,"#/GfffiP5<IsI$"#fiGc%'1/Wff.3X'*W= ;fi+3+f3/GffT3-fi= />65v;= = + .+-> =b_fifffi$4:ffE=ff"P=V/GXN$c= JPfi$?fff> P(ff"#,+fi !fiVff'+%bP= G E fffi/5Yv;=ff'b+fi-M$fP= Jxfi .+ W36A<P0>F ;bfi :E=ff"P=qff$"Pff$MEc= = f= +EdP>PJ= Cff+f%'!J-V $R Dbff fffi >ff'(ff$65v;= VR $fi]+O% G;3 ;<CZf\ E;-i $!+fifi"PBfiDPV= fff]Xfi .+PV'+fi/G).+ffM :1&-"#G=3fi ?/GPYff'+cfiZ= ,ffVxfi .+/>+B$c= ,xfi .+ff2'!+3"#$C.+BWfi 35 L= ?= ,=+4:<Pfi />+ffqff'+sfiP= GX- .+fi"#$$C="="#$=_ bE;XX*,G"PfiD!b/>+."P=fGG $R fffi c fffi/V5^ff$3Xfib= ;ffJxfi .+J!= ff,"#&!1+%wMP#+fi#fi!/>3X+%<R3X-.+&fi#fibX*ff.+fi !fiF"#/G/WfffiG ""#fi ffJ Pfffi/>:ff TW"=+#"#fiU$X>%8RSXfi.+!fi+#fiGX*2DM+?+%<"#/G/Gff-> ""#fi ?"+fC=+.?G"#/Gbff5;+Z/>+B$< ?"#DPfifi>W=R $!fi?+%1E=!= ff?+ $+fi>P= TffGX- .+:ff+? ;/Gfffi'"3wE;BafiVI$"#!fifY2/G$;b P$ff/,fi $?ff'+,Xfi #+,E=ff"P=,Q + /Gff$Gff- YP= T#@-/GD#3w!35IGe dK#-D:4$ff%+?xXfi /,fifi b-D$!fi D+!>+%K= $M' $5tfiff Rknnir.lrzlw/.v;=:GL%' "#fif&-/,X+?]fi#? .+-Z"# DP+() n k GG#@e"#fi>P= M+;"M?ff+f#fi$"#$W%';#N /GffFQ% P= #N $45K^&f= J"M+%+ .fi*:*?+P3Kff+Z/afi =ffbG/>B$A%b#N /GffYJ3fifi+#fif%'!<ff;36<ff0 7>365XA2:+\= ?3fi !fi= /r/Y!bBP."PBV+%;E=ff"P=*5>v;=ffb= a%D-)$"#/G$CVC+%_3-.M+%= C%P/ hGK: \E= PhMW+S1ff+\++WT?ff/,+<fffi fC 5X<ff;7365 2:)#fi=UT(gjVfK958v= BUT(gjq"T/W$+;#N b= Mff+?%' = 3:->E=ff'"=f"b= 9)% "#-fM"3X-$4: #@e"#fif= ,+/G,;-V .fi*5fiP$"#fi+% K9P$fffi.F-q?"3XK) G :E=ff"P=?ff#N $#fi+EJ5q9tkl n ir . lrzl */. !>k( $ [<8+fLD0 5 YMG<ff;0 :( 213Gk( :OK*4b:kW>k( *:U(T(gj*954. no~n60 :( ,/G l nt=nl 0t!D0 5 YMG<ff;}w: 2>1 !#fi$"#T+f#fi/WD;%'/80:oM#fiB4: %/0G'G+-fil nt nlk t+,UT(gjl ntb$"P=Vff( $?ff() 4 N3$fi yz){ {z {z0{nzvnh/nPlr4l !th!mez)zx 0 yz{zy yz"z{: UT(gj*0P;0 :(K9UlntfSX*#fi/Gff.;+%#) ) ( (M;0::(-,n %tmnntl v^H!$+%- q+ $"#!fi V"+3FXfi B :*P."#-\/G+$b"P= +"#$b/?ffGE= ]=Xfi #+ZffuECfifi3Xfi .+$45`+?R) u#fi$"##G/w3EfiL= \"# ffff+?+G QJ= %D-<3XwX*ffKE$ +%4N @fffi J= /w$EC:e) G) n k :C.+B$?fib#N /GffY"P= +"#:*!$M= >!("P$$ZP"# ,V"#/Wfffi#fi/G+,=+#N /Gff$:+\ Mf= C%'fffib3XK+%<= >3-+XCE$ c=+M= G#N /Gffb/,fi =D=3b/>ff5: K*;Efi=Zb79fi P qXXfi!.+$:4P."#fiqPff"#$bVR ?DPV+%= ,%'/ }hGQ.+PD.?% h?yz,3f. $ K*b3fi fEfi=J+% hGyzC&fiffX- D:*$"=].+ $U(T(gj15$"PMff#NfffifiW+%;&fiffX- D;'K= T!<+%4#N /Gff.FWhGyzKDF=+<+P [5cWA @BA <)84G5 736AWh>5QZJff#N b!/G =ff'/F%'+Xfi+E[e,t#6OLAl8(YZ02W}h ( 0 24Ch $ 0 <ff;c/G =ff"|MW3v5X2m70W;- (36;ff84W0F<P;;@g 3 ;<ff;!5cWS0 @B0885X2FD<ffA @ W36;H84WS5X2Vh ( 0 24Ch $ WM7V3 G037A <<ff;W|8:A 2\C 5X2FW36;ff8\W0 ?;l5XC ;>24365 70Y240 @B;WRe360;3 G;7A <<ff;nWe8A 24C ;>247;f36A,;{ ( Z $ Zk!36A( ZE $ Z!29@> 5 2W{0F <ff;;@(h $g \ W(h ( 5 b{> <ffC ;><5 2GFW0>F <ff;;[(h $g \*^W(h ( 5 {5 24C 5 2GF,7A2W36<P05X2\3 W0F<ff;;[(h ( 5b_ G9 4h $k ;<ff;_ [5W0?0 <5 0Y;B5X2W36;H8 AffDh (g _ )024CU_ 5cW3 G;,7A <<ff;W|8:A 2\C 5 2GF!?>0 <5X0Y;5 2W36;H8{AffDh $ 0 24C5WS0%7A 2W360 2435W;LDA <Mb_ 9)0 2\CG>b_ )g Y5c;kLb_ _ $Th $! ;<ff;T_ 0 24Ca_ 0<P;g _ _ +:h ( 5?>0 <5X0Y;W5X2W36;H84W 0 24C AHDh ( <ff;nWe8;7>365X?>;Y E 024CU_ :}_ G0<P;l3 G;B7A<<ff;NW|8:A24C 5 2GF,?0 <5X0:Y;WS5 2W36;ff84Wf{0 2\Cf AffDh $ <ff;W|8:;7365 ?;>YZE5W;LDA <Mb_ _$0 2\C?`_ 4[G$g Y5c;kLn Wtilv;=ff'Jff#Nfffi-\fi/Gffxfi$M=+C E;?!/G =ff'"Jff'+b=3GP= /GG)i"#-fiC+= $+P $fxfi B(McE;#X 5TcC=+ME_Wff+c/>3f=$Y"#P$- VM+AffD!"3.ff- D<+24A 3 )'/GP =ff"+:+= +E_$:D&-"#= G"+fX*KG MK/GPT"+3)Xfi B 53$fiyz{z|~}av= JR $!fiV +Ed$;;PGE=ff"P=qff$"Pfi"+C.$AE= V/WSfi , ffE.Wfi= G"#,+%<3ff+5Jv=fi/Gfffi$b+E;cc=+9) G ) Gf/,T)G+fffiC#Xfi/,fiT+ff>ff$"P&->P=+"#ffW=$/>ffMDO) n 5i;#N /GffTff$"P&-/>ffbffV) n "+fP$fffi;fifP= M%+xfi+E;fi J#-/GD#;#fi f ff$??,ff+*[g] &fi +fi,"+3Xfi B4: fffibV.ff!fi "#.3-Dff-fffi- "#!.3fiff.F-$N @q+V)Z"#fifi*5<^&qP=ff",3X1P= a"#.3-D.FE;xX1,. $?E;fiP=f= J$PD bffL 4I+fi\ Q 5ELff-Y\"+P3;Xfi B4:fiP$]N @i+L)u"#fifi*5\^HuP=ff,"? E;.fffi G"#!.3fiff.;+G+%Kfffifi f"#P.3fiff.; "P+P$GE;-=f= Y!fE;XX*).+ $aE;fi=?= b$!:PLUkU bQ :+?+f.ff!fi G"#.SfiD;+f,$"#f;+%ff-fi >"#P.3fiff.KEXX*, ff$V3- ,E;fi=f= b EXfi B4:+)+5g]#fffi Y"#.3-Dfi!$WMN @bP= $+<#-= <ffG /Wfi,;ff/G-*51v;=ff'g G"#.3-DbE;xX<?.+ $VEfi=iP TkbD\b L >:L \Z L Q E= 4b= ?= $fffi!*5C+%T+++ff-2'fffifi q"#.3-D.Mff-bE;f.fffi \"##3fiff.cfiP$ZAN @P= $+<ffG+.fi*5Kv;= $b"#P.3fiff.E;XxwM.+ $,E;fiP=*P :TbD\b L %:L \Z L Q 5gfi +fiJ"3xPO) ) k G>= ff?= #%'bP."# J!"=#N /Gffff$"Pfi*:E=ff"P=V+/G ff.F,/G+fi ,= J!("P$f;+%K.fffi ff:fffffifi >"#!.3fiff.:ff:)+Xfi B ;%'/ = Jff*5Tc"#C=+T?ff$"P&fiq"#P$ M"PfiD!#fif>?M+%Kffff"3<ff;0WA 2"#P $Ffi\,ff+*:!G."#fi G>ff$"P'&fi?%'/aff+f$3xfi/G ff.;,/G+fi >G!+%<"#.SfiD#KE;fiP=?ffD!"3*.+ D6:(Sfi ,E;fi=fP= #fi "P+$Wxfi B(;+5v;= b J#@"#fiG=ffc"#$)ff"#C;= s%"#=+P= Jff$"P&fi> GGff'+]H$PLUU b :kQ ;'S0 YuL0 E>W/>ffaM+M+%_>ff$"P'&fi\f \,xfi BZHP$PD bffL \ISfik Q .:ff,= $ME_,ff$"P&fi<= ff'G)b."#P$G;b3-_FE;#X 59ZTEXXwP$+= JE;GM+.+PJff$"P&fi6: 3-X= /E;Xx1 C=+T?A;/G+$W%'/rff+VcDVFfi.Q"+3*xfi B,;P."#$45fi= =f= G"= +'"#J+%_Gff$"P'&fi>P."#;'/>ffJ ffP/,fiff!"3Xfi:(fiT"+/>ffV+Pfffi.+!-:8fi"#f= fff+ G"#ff'] ,=$f .+$= Vff$"PfiCfiL+ff.ff$57 ?#@+/Gfffi:;E= ffXfi ]ff+Bh>:;= ff+ ?/,fi =ffG=3\"#$+$0X- ^B 4 Oq++fiDP(ff"#$\?M+%<.ff- >Mfffifi f"#P.3fiff.? $"#M=ffXfi B?%'/r#-= $+P $]DL+ = >!^45iv= V#"#fi]3fi !fi= //,WV+fffifPqP."#,#fi=ff$"P&fi ff#fiC= bxfi B>T= ,"##3fiff.#.:ff M= $JE;>ff$"PfiM+b bff/G/G'"+5;^'%;ff#fi$4%: L)$"#/G$;= P$+ $?+ D3fi*: ;X%'_ff#-$4:ff= *O$"#/W$ H / 5v9Z $"#f+ DSfiCfi$3- Z= Vff'+uE;-=` )ff / fP:FXfi B :F"#.3-D.6:KE;3Xfi+E0= J3fifi= /W."#;ff-f= DJff$"Pfi;=T+!;n8:AW;C5F^Hff%'/>3xfi: Gff$"P&fiF#@()D$WX%* GP= "#.3-D.9->= bff'+ff)f?= b"#P ff$?,= Tff'+>ff=+ff$"P&fi*5<v;= M%'/>31ff#Nfffi!fif+%K#@()D$Wc.+$W-f/>F+[% <ff;0WA 2OW8Efi=fffiVCff+*:&fi"#YE;J $V+)+Yff$"PfiM V"#!.3fiff.;P>Gff'+V=T+C.+ $AE;fi=?'ffD'"3$653fi yz){ {z {z0{-,en %tilmez)zx 0 yz{#6p<ff;0WA 2( 5cW#@()D$5X2 8Y0 2Vh55cWSAffDS3 G;vDA<@P :TbD\b4 Q DA <WA @B;BYZ5 2l4zy yz"z{PDbffL\ISfi=5cWSAffDS3 G;vDA<@QDA <{WA @B;BY5X2O 0 2\C|0Th7A 24360 5 2W24A%<ff;0WA 2mAffDS3 ;DA<@8P :Tb1D\b Q 0 2\CH;5 3 G;<,4{8:0<365 7598:0 36;nW5 2024A 3 G;><Y5X2O4 c <,4!C A;nWY V08 8;0 <%5 20 24E{8(<PA36;7>36;C%3 G<ff;03.AffDB3 G;vDA <@P :TbD4b %* Z4 QPLUkU( nbQ5cWSAffDS3 G;vDA<@Rh0 24C7A 2\360 5 2W24A%YZ5 2!AffDS3 ;DA<@pv = GN.!a]=fffi.]"$,+W%H3fifiZP.3fi =ff%'+E+4[G"#.SfiD#b=+,$!+fif= $+"+V3-E3 K)J#"#$4: +fGPV"+ffffiG)J/WS$,X%fi; ,fi ;+"P-+$-V+D"+S*Xfi B 5v= $"#f"Jff$$_!/GM#@(ff+fi*:= +E_$5Kv;= MN. 4"J3 <P=+_sXfi B"+ Fbff#fi$C%'/Mff+>fi ,= M+T"#!.3fiff.9fiW= ff+W $"#fi-K%'/,= $tu= EM= J"##3fiff.< ff$A,$+-TP= b= $+;E;ff?)$"#/GJ )ff / 5v;= T$"#> 4"M +. F+ D3-K= ;%'+Xfi+E;X J)$"P3*"[K ffTP=+hd"#D.SfifffiE_cXfi B: 4 _+,< ff5K7 = P/G: )D;=+\ff$,M= $+Kl< ff:+ <fiKff$"P&-J$!+fi$C=+= $$5 /afi =ff1)_P/G $Cb/G+F= FXfi BB4> O_+3fifi=,fiKP=4:D&fi"#44E_ff, b- <T+ffY PDcfiGP= ff+*5L~F _ff+-fE;ffAfi$3G )ff / J!"# Cfi= ,ff+*:*/G#fiVP= a"#.3-D.c=+bE;G ff$,$+-= b= P$+% GZn4J5 Tff#Nff-fiW%_#@ ff$? +.D$QN.!_P=+Y!E;XX<)>P/G+$E= CfiJ"#$$sq!>? )DWfiZ= Wff+*yzC"+3_P"# f55E= Qfi.9!Xfi Bb'K/G+$)#:+ <=+<fffi JJEXX Kfi$3 )ff / ;"#P.3fiff.fif= bff*5c+E`P= T#ff;fi?E=ff"P=Vff$"P&fi;"+f)JP."#$?"+fJ!.+$?&fi/Gff-w[K,ff$"P&fiV"+bP."#$?ff-,X%*fi#_ "P+$W$?F#@()D$45 )fi ?=ffF#fffi ,/G$+;P=+;=ff+\E;Xx< Y"#ff.3-q )ff / ,"##3fiff.:)Xfi B :4J!$Rff-S3fifffiE;Y/afi =ffJ3=+."#fi ff-b#@()D$,ff$"P&fiK"#$) P= _P.F.fffiCE=ff"P=ac .fiff+@B5FG3vG0?;B@B0 C ;M= DJff$"P'&fi;fi +fiSXX5<.3-fffi b."#fiGPJ ""# ;fi?=ffK.ff;/,fi =ff_/,)T+-YP$"#!fi:,E;/>+B;E;-/GP.+ffK++fi65179fi.!$:$ =+KP= .ffK+%4."#fiC2\A 3 "#.Sfi $J)b= M.M+%1P= T#ff_!$GE= fP= X- .+,ff+?E<"#P$+$atfffi,= M.M+%24;c+%1= ,ff$"P&fi 2'.fffi ff<P=+7A MYCJ=3M\$?P>"#$+bP= MXfi .+Wff+*5FI$"#4:E_?fi$"#b= GP$ffJ\I$"#!fiLD:9E=ff"P=\#@(ffSfib= +E9* f3fi,. +X$:4"# ff$+Z=!": "#ff&ff$Pf= $b$P"#fi<fff"#fi G;/?"#G).+.5c#@(CE_W $ffJ=) G G ) n f%' "#fi*5fc"#,= +E= ?ff#Nfffifi/,fi.=++%K-.; .fiJ"# ff) *[K= s+P"P= ffD$Ml/w3E+A MCX'=+K-"Pfiff$3X4DP&fifffiE3 K+%*N @fffi Cfi$:= c%P/G_"P= Dff$;+>#@ ff$fff$"P&fi*:4<ff;@BA ?>;W= J"##3fiff.F=+;fifi3XfiCN @ $?fi3:(fR $T3xP= J3fifiME$ F+%N @Dfifi$53fiqtkl h nPlr4l h ne,Wtmn ntl@ $ [<)c+% 8+*:ocfi$"#!fik)\[d#fi$"#M+f#@()D$A$= PbF G#@ D!$f$!Xl nt=nPlk0tT3e5D57)0 : N [) - L G ) "n:l } N Z K9:C$"P=fff+(NN P $fffj= G >0 O\L? 0 " NSNNF ;/G =ff"cOl nt nl NN Z UT(gjyz{z|~}anl 0tq3xff'+*:fi$"#!fif3fi#_"#Xfi$"#$Wfi?xfi $;>+fe5v = fE3ZPqP/G+>P= V"# PP("P'+$E;fi=u+]#@ D$$uff WL=D)>%P= >$*5Vv;= W%' "#fia) e L s= !/w3E!("P$\E;fiP=Z=fi b$qME_#xQc= ,ff+\ ff"#$\ffV/WSfi ?= G+ !+J"#!.3fiff.:eXfi B :+Z!5f"#G=J= >"# ffXfi V)E_Xfi B\+]ff$"PfiM'b/>ffW= [JEc== sFXfi BfP>GAff#fi$A= Y!fcff#fi$fPD5;7 =ff;P$E_Jff? =3C=+fiGP= >"G%P/G+fi VA$q%P= ,%/ PLUkU( n b VI Q [,fZ)$"#/G$C#@ D$fffiGEc= Vbxfi B,;ff#fi$4:e _P=ffQ% "#-f/G+$Q= JWfi/G/G$'+#fi5FIGC$?+%= M%'/ PQUkU b VI Q E;XX* $+#@ ff$?fiVaff+*5q$qtkl h n o~nkml klkn) " $ [;793EJ:D8+4W);+%K= M%'/P TkbD\b%4 \Z L Q l nt0 [%4 \Z LN [Y"#D?+%#oM#fic%'/8*N 3X1"##3fiff.K. $?E;fiP=;)nPlk0tZ7)0 :% Nnzvn G)F+%K= M%'/P DbffL4IS-+ 4 Q l nt0 [: L1N [Y"#D?+%#oM#fil4> c%/8GNoM#fic%'/8 N 3X1"##3fiff.K. $?E;fiP=;)N "#D.SfiF ,Xfi B?+%K= M%/ L >: LM %'T+fff LM +A#@( $P&fiVl ntff#fi L 9%/8GN4N 3fi ,E;-=V3X1"#P.3fiff.K#+ $?E;-=:P LUkUb L QnPlk0tZ7)0 :%XNv =ff'T"#"P-ff$b= >ff$P"#fifiV+%;= >+u3-X= /V4E;, #@ b#@e+/afi ,= G3fi !fi= /Vyz;%/>3-$:ff +fi C=+QfiKF +ff,ff+Wfi<P ;"#fiP $<C+fi-GC=fi ff fffi G ff-/f.: "#/GfffiPJ'x%1={0 24Eb+-fi?,=fi ff'+ fffi G fffi/V:+uEXX<ff3X-fNfi3:* D.fi$M+%= aXfi .+PVff+fiJ"= ffD$sV + 6.:1 /?+"H= J .+fi>E;xX* T"#&'ffT,+!3*ff+f/GTP=+f"#+.5V9k}^'w"+ "z ""<"\}e!v*, +c%'/>34 )fi$;%1= J+Z3fifi= /E;T) +fifff>"=."#-U#fi GbxX%P$G#&fi+%YP= q .+fiq3fifi= / ff#fi)$0D0~\" X-$f+ i;D!DffX-$yzA.$e+Y3fifi= /3fi yz){ {z {z0{mez)zx 0 yz{zy yz"z{H= $S%,"3Xfi$\ jkIfiZP/>M+%TV$."== == !"#G+%+!3Kff+5?Z?="#&'ff>P."#fiu,E;#X 5v;=ff'>P"#&fiL!$?/>+ff+%JP= q"#"# .fL/?,%'/I$"#fiVqGff$"#fiff- Gff+?ff+ fffi G Pfffi/>5F&ff,q-$"#$] . =sfiL79fi \?E= fV fff $!D.,ff+L+]+L+."$!D.?]ff+ 2'P#N /GD?).+35ZZ"+0ff#N Z= Z"=ffxffu%YZ ff]Hff+4RhG:$"#?, ff/afiff'"T"P= +"#:K%'+X-SEM[-,t#6OG;b"=ffX'ffAffDS08(YZ02;h0 <ff;B;0 736YE!3 ;W;@eW Dh5cWS7A@[8(YZ;>36;B3 G;2m5 3LG0WS2\A17nG5 YZC<P;>23 G;<L 5W;lW;>Y;7>3LA 24;BAffDhT WSA8:;>2m7A 2\C 5 365XA 2OWA<S3 G<ff;0 36;2\;C%Y5X2OWW D3 G;%7A 5X7;!5cW3 G;%A8;27A 2\C 5 365XA 2 3 ;2hT WB7G5 YC <ff;20 <ff;%0 Y8Y0 2OW3 G0370 2,;[7A 2W36<M736;ClE0C C 5 2GF0lYZ5 2 > 0 2!A <ffC ;><5 2GF{ 0 24Cl0@B5X2\5X@B0?0 <5X0:Y;5 24C 5 2GF7A2W36<P05X2\3G;<ff;%4S5cW1;>5X3 G;><102;5cW365X2FW36;ff8A <%0m24;YE7><ff;0 36;C%W36;ff8 3 0 3[70 27A 2W5W36;2\36YE%H;%A <ffC ;><P;Cl8<5XA<!36A 024CV3 0 3f0C CWBWA@;8(<PA8:AW5X365 2iG;><ff;Silb3 ;<L 5W; 5 D3 ;7nGA 5 7;m5cW3 ;V3 G<ff;0 3 {4> 4 3 ;23 ;2\AC ;G0WV3 G;n %til7nG5 YZC<P;>2VA360 5 24;CEe0H| 720C C 5 2GF%3 G;BA <ffC ;<5 2GF%4L^\0C C 5 2GF%3 G;BA <ffC ;<5 2GF%f40C C 5 2GF%3 G;!A <ffC ;><5 2GFWf 0 2\C! 5X2m0 CC 5X365 2m36A0@B5X2\5X@B0 Y(?>0 <5cN0:Y;,5 24C 5 2GF%7A 2W36<ff0 5X2\33 G0 3\DA <ff7;nWS0 YDA<@lW 5 24 WS0 C CV024C%C ;Y;36;Y5cW3 C A;nW2 3vMG2\5 DE L5 3wec8(<PA?5 C ;C%3 G;nW;B0 <ff;B7A2W5W36;2\3#L5X3 3 G;!7A 2W36<ff0 5 243 W7M<<ff;2\36YE5 2Vh~ " Xfi$b+mi;D!DffX-J#$ +"P3-/= PY P!fi$M+%_P=ff$ff.+!fiqqSfi2\-= /V[gff $[<sfi$3% (ffJ"#P$ ;YC+P34ff+*: +ff>"#/Gfffi!fi?+%1E=ff"P=?E;XX)fi%H"#+!&%'G=fi D35gF/Gfffi $P[ 0 2\E>ff'+L=+>!+fi$,P= fff+ fffi Z Pfffi/a$3XfiU$-u= f #+ =Yqfi$3%b (ff5uv= #%V+ffiP.+ %?$+."P=fffi \= f . =L=+,C .+ff$P>"#&ffMf ff,ff3Xfi?E;Xx*NV?+fi !fif?= Jff'+ fffi G fffi/x%<#@ff#5g/?+"Pfi&w[cE_V'fi"#M (ff$fiZ= , #+ =Z $!Db 2-'/GP =ff"Cff+:+%' = P/G:= .+ =G #+$WD>Cff fffi J fffi/<C5Kv;= P#%PC$+."P=%_= Gff'+Z . =Z=+,ffD$s J)$+YA (ffWE;XX< ,"#&ff,f+P3<ffqDG%1fi#;#N /Gff.;/Gb=+f"#53Xfikyz{z|~}a9tQ'tmnvvv;= , ff $c )?%M+Z%'+X-$ET;fi$"#!fiG%'/rjkIyT ff $:4&fi"#Y! ff $Ta P ?+%K= J3fifi= /Vyz;$."=V!.+ :D M"#/G/Gff.;ff->fP= b+ C+%1-$3%ff$; "#/Wfffiff#.5KI+-"#\ff#N $Fff+K+G!+fifi9fiG= TP+/G;E$,<jkI:ff+ff,; 45fi ffA .nlntmnvv</Wfffi $:ff$"SX : "#&!.;+%KE_,"P3fi/?[5;=+bZfifiVP= Gff+ fffi f Pfffi/'M$3XfiU$b?fi$S%_ ff>+%= W .+ =*:+e5;=+;= J$+#"=V3-fiP= /EXXwDSXfi?fi;Wfi$3%1 ffbfifP= b .+ =*5v= N.!_"#fi!fiG"#b+ DSfi>ffff$< _ff?>= ME3J= .+ =WF$+."P= $4:ff= 2%'cfiFKP b+%1+Z$"MfiFF M+%8jkI5<v;= b$"#f"#fifiW8fi$;"Pfi$+$:ff= +E;$[jIY/?+B$<!-_"#+.Q= TfffiP .+ =WD>!.+- J+<= Mff_,#@(fi ,= .+ =ff+EffE.Afif />+!"s%!=fffi*:*Ec= $J`..M+Y+Z+Pfffi.+PVfiDM-Z= G #+ =+?.3#$Qfi;fif)=Vfi$"#!fi5h 2PD+%M+%"#/Gff- $G+/W D#bZff/G!.+- V=+C%'G+ffZ+'3_ff$fffi ]= V) +fi fffi ]fiDW%'>+4tuP= "Z'xfi .+]ff+4G ffX-$Lffu=fi+3/G$"P=+ff/,tuP= >3fiX= /EXXKDP3XfifP."#J"#P.3fiff.%/= Wff+\ D!XKfiMfi.= MD_ ffCHDffxff'+4.:Dfff+fi Y!aSb-/GffXfi$F=+Qfi;E;XX&-_3X1 P$F+%*= bff(ff,;E;#X 5~M%'/>3xfiG.+$4:eE_b=3[kRn n5X3 0Y5)<ff0 <E!8(YZ02ThL 5 YXY70 W;V;>?;><E,8:0<365 06* 70YXYv36AV G iLxe;>?;><E!2\AC ;B5 2V3 G;8Y0 2F <ff084C ;24;CEWhT W8Y0 242\5X2F{8<ffAYZ;>@136A;B?5cW5 36;C8Y0 2Zc,fiff"#fi+P /GffKJ +c=ffKP= /V:ff= +E;fi b=+F= b .+ =WD$+ }h"#/Wfffi#fiG#@ fffiP$4:(?=+;= b3fi !fi= /EXX%'+X-SE0,+=? f,= bffMHffffXff+4<#@ fffi!fi ,f .+ =?-f= J "#$5Zf) +fiDuff/G.+!fififf%/>3X-Z=+?ID yG/GP= (+% <ff;245 2GFf+Sff'+fi ?"#.SfiD#_; ff$f,P."#fi e1';$Rff-S3fiffW= b .+ =V!$+."P=V ff.BDL jI5LP i$"3X=+G+.+P$Jff]/>+fffi fffi q$+."P=Z%'fffi,E= ff>ff-$Y}h>\: UT(gjO+mhG(: K94J:"#P$fi >P$$"#-#fi?>fi ?+Vff#fi!fi >"#P.3fiff.%P/ h>5ASr 6 k G; ;2\36<5 ;W F ;>24;<ff0 36;C E W 8<ffA7;WW5X2F0 2;>2436<E AHD3 G;DA <@}h UT gj* 7A <<P;nWe8A 24C;n0 7>36YE36A3 G;VjkIF<ff084 AffD!80 <365X0 Yv8(YZ02W<ffA>A 36;C 0 3Wh0WWM@B5X2F3 G;BW0 @B;17nGA 5 7;,5WB@0C ;!0W!36A(G0 3 7A 2\C 5 365XA 2BeA8;2A<S3 G<ff;0 336A<P;nWAYZ?>;0 3;07%W360F;^&Tff"#$;Pa!= SE =+;= b Efffi$F #+$?fffif$)b>fDPG+%K=%/ }h?\: UT(gj*"#P$VP>= G/GJ+P31ff'+P=+"#/Gh>yzM"P=ffXffAfi\=.+ =?Kff#N $f++M K [q qff.5v;= E;c= +#Kb= Mff#Nfffifi*<[ hd"#/Gfffi: h#N $fff"P= ffD&fi >f\"#fifi>?+%: h#N $ADf"= ffD&- >,= $_PG$+-5^H,= ;";=+lhu'1"#/Wfffik: h`=* J"P=ffXff*:++sXfiBE;<?/afi+$9 .+!fiG Edff-$53pfi yz){ {z {z0{K)mez)zx 0 yz{zy yz"z{P= E;;+V"3X' n:Ec=ff"=G"P= Dff$KT"#fifiCM$+fi_+C .$ EUT(gj>fffi$:+ %'K$"P=Gff&fifffic$+fi !fi*5*c= P#%P=+< UT(gj?DPJ .$2\YZEUT(gfj fffi$6->P= _E;# <P#N /GDcE;XX4fffiG #+M/GM#N /Gff.;fi$"#P$?+=?fif= C .+ =?-$ ;G""#$fi#fi?/GJ"#.3- $?ff+5^&,= $"#G"!Y>"#-fisK"= ff*e) n #+$ E^UT(gj>fffi$%'SX8#@ff!fi VM)D&fiff-ffiMf= ,)"#fi-q+q%b3X<"#-P=+J =)q"#fi!fi*yzF )D&fi!fi*5Kv;=ff;"#P$ c#@"#fiGP>",P q(Q+5^&G= ;'<"Mb= $<"#fifi>MXfi B,+Gs= $+ff- Y44'<"P= D*e) ns= G.ff!fi Db+c~$bfffifi q"#.3-D.c=+C ffb= G= P$+$:*#@"#fi\cfi"H <+)S5M3fi bXN$W=+_q .+$Q=-/G/G$+PT"=ffxffG+%Kb+!3ff+Wfi>C/>$Rff-S3fifffu jkI:J+%' = /W\=3fi L $0=+?-fD#?= $"P=ffXff0 =%PD!fiGE;fiP^= UT(gj0.+ D,WE;#XJ !Z= #fi>"P=ffXffPuE;XxcSZ)q#@ ff$).:;P= f%'+Xfi+E;Xfi/G/>s%'+Xfi+ETFfi$"#-G%'/rKSfi/+: = J"#/Gfffi $+%Q jkI:fJP$"#!fi?= J$+#"=V3-fiP= / $A#fi+EJ[n r 6 ;>?;<0 C CWm36A3 G;BD<ffA 24365 ;<3 G;;2\36<E}h UT(gj3 G;>25 3RL5XY<h|5 247YMC 5 2GFh;?>;2436M0 YE!;8YA <ff;!0 Y8:0 <365 0 Y8(YZ02WS7A 24360 5 24;C%5X2m3 G;SF<ff084V<ffAA 36;C%0 35X3 W;YkG/,b)G $"P!a bE=Mfib/G$+M]##@(fffi$f?+'31ff+*:9b$RfffiS3-D-\.&-.C= Y"#P$fi G . =V ff5 f"#D#3fiC-DAfifE=ff"P=f-T#fi$"#.c+DPb%'/= %D-;'55bff+V~bfi$"#fiYSfi.:"P= $"B-%<"#/Gfffi $H/afi+XX%w #:S+s= E<P#N $= <ff*5<IW.#@ fffi- D;c"#&'fffi Dcff+C/G$+K#-$"#fi= ff*yzDP,G= T!$+."P=,%PD!fi$5*4/G/?GT"#3XfiC#Xfi$>J$."= 2H"#DP+e!.+=+QK .+ff$GDSXfiJPY"#&'ff_Gff,?= %'fffi351v;=ffF"#$! F,$+."P=>P.+ bP=+<E;xX(DSXfiJ&-K, (ffcfiGb .+ =G +-, =?fi/G#tLfi,P=E;. :ff b=E;XX* )\+?fiffNfffiJ/G ff_%8-/G-VG . =VE;fiP= ;#@(fffifi= ,+$M+%= W .+ =*5f jIyzM-.+-2Hfffffi \$+."P=P.+ =bP=ff)ffD$M+<y< P$ff= 2-N.M$+."P=*5v;= K<"%'1"#/Wfffi $)%'+X-SEMfi$"#!fi%'/4/W/>b<+MP= K%H"#4=+k Gfifffi'3XXG .;)= }h\: K*h \: UT gj*f= M%PD!fi$[nr ff6Vk;SWMO)F<P084<ffA>A 36;C03KhL5XY Y;vDMYXYE!;n8YA <ff;C+EE;b"+q!.+M=fiff"#-V"#fifiVCfi/W/> [n r 6 Df0{80 <365X0 Y8Y0 2Oh 5cWkDMGY YES;n8YA <ff;C 024C9h 5cWf3 G; 8:0 <365X0Y 8(YZ02!F ;2\;<ff0 36;C*h 3 G;>2=3 G;,WMeF <ff084<h;v;= s%"#c=+9hb'T"#ff$\W$fffiM+%<GP."#-?%P/8h/G$+c=+P= Jff}h>:\K*EW"#&ff$4:F$ff-fifiuZ"3xPT) ) ( \%'/ E=ff"P=xhVE.+$LJP= f+ffG (ff(hKNfiLP= V"3X+) ) k :)5v9!= SE =+Rh yz0W!0V<ff;WMGY3AffD |24A 2\C ;36;><@B5 245cW365 70YXYE 1<ff;36<ff0 7365 2GF0V7A 5X7; D<ffA @<PA>A 36;C10 3L5 YXY;vDMY YZE,;8(YZA<P;C%0W L;Y.+ =?'F%ffxfi>#@ fffiP$?E_b $AG= +E=+5}h;&fi$4:5;= J! .+ =f) +fi fffi f+}hQ%'ffXfi?#@(ff-$4:hq 5;=SX*+%yz"P=ffXffA3 G;<;=(h+M%'ffXfiW#@(fffi$453{fiyz{z|~}av;= MN.!;;P b$"+) ) k :, #+$;= bffm}hD(: K*4b: Ec=ff"=f/W$+=+9hYE;xX1ff3X->)Y&-$45Tv;= G$"#q"#fifiA;= Cfiff"#!fiV=ffff= $5bv;==fffi#"#fi-+/G ff.?uff/G.+!fi L=+\.G= "P=ffXffP $0ff`) G c) k :f"#SXfif $!D9 h yzT"P=ffXffPTff#N $Z+:4+ ff;=+M= $G"=ffxffE;XxP= />#fi$M%'ffXfi?#@(fffi$45v= bN.c;$&Xfi?XN$4}[ ) G G ) n ( Gfi/W/G$+#->"3X=GG %0 O@? =/w3Efiq"P= Dff$0."#3:bE=ff"P='f#@"#fiP= Z%' "#fi"3X-$ffb) 0ff2ff$A=/w3E-= \N.!Vff"#5 ^HO= E_. :M= Z E (ff$ .$%'(hDk) G) n :V+PJ#@"#fi= DC=+bE;ffAG #+$D)*: E=ff"P=\DKSfi/J+Whyz"P=ffXff*5hfi.!#X%_+C b%M= G"P=Dxff\#fi ?%'ffXfif#@ fffiP$4:*3X1= G"P=ffXff\#@"# M%= b%'fffi;E;-=UT(gj\.+ : +A= #%'bff/G/>bE;XX*)b%'ffXfi?#@(fffi$4#5 h-.#X%*'%ffxfi>#@ fffiP$?Df /Wfi*:ffE=ff"P=V"#"Pfiff$c= b ff+%K+%<4/G/>!qe579fiSXfi\E;> $Zff/G#+?= V"3Xm: w h.'bff3X-Z."#.C\=.+ =*yfff$5r79fi.+%>3X :M= \N#q"3xYG .+P$V+ff0+%G= %'/}h (: K94b:1+ "#$&fi \+\DPV+%;= C%/ }h (: K94.+P$b+\DPV+%;= ,%'/}h ( (: K94J:ffE=h ( $!D.= b."#fi?+%<G&fi +-"#!.3fiffF%P/ h5v;= <"SX[: w h.'P= #%K .+P$wc$R "#F+% D!fi$4+%DP= K%'/ }h ( (: K*4b:}h $ 4: K9b: k#[: h : K*4b:<Ec= j'JP= fD /,W+%sff$"P'&fi 3fiah 5^HuP=ff,$R "#h ( hT+ah =, "#P.3fiff.5Z7e = /G:<4/G/?Z\#X'Ja=+,= V #+ =ff$Vh ('c%ffX-f#@(fffi$V+\4/W/>qGP#XcM=+M= ,$M+%<= Rh< .+ =b%ffxfi>#@ fffiP$f;E;#X 5v;= N31R $!fi,QE= = }h : bff'+GE;fiP=G G"#.SfiD#:3K $"#$P+X-YP= TffffXwff'+ff#N $f+)SJ)sff+GEfi=b<P=fiff-3w+YN31<G= M&fi +fiM"#.SfiDK2fffififf-3D)#%FN3H.5Z;Bff SE]=+<"SX9) G G ) n ME;XX ff3XfiJff#-3X8"3Xfi B +\3X1.fffi Dc=+cE_PY ff$\c= JP$fffiM+%< $"#!fi >W= $+$5MI 2ff / _!; !K=K=$ YP("P'+$sXfi B)8+a.fffi ffFH=+KE;T ff$GE;-=V"#P$fi f= $+,"#fi-4_/afi =ffJ+ $+sfiTh:*= +E;$:*++) G G ) nE;ffW ;N?= /V<5 h :e= *:ffE_ff>"#D.Sfi> ,/GTP."#-G-:ff ;E_ffGC= bDffXff+*5Z\"+LN @u=ff',$&X- =*:;#fi= ?DL$Rfffi- Z= fxfi .+ff2'!fiS3/>"=fffi PLfffiZffbE;fiP= )ff / G!,+"#.3-D.6:4,DfiPfi q#@ ffX"Pfi,"= $"PBfi+) G ) f=+M/G+$b )ff / Cb+\"#P.3fiff.cE= Z= G+PY/Gb !fiFGP."#$5v;= ?%'/GW/,fi =ffG G)qff$&-.+fffi[= fxfi .+ff+u/afi =ffa"#D.Sfiu,P=+>ff*yfifffiSXXa $+Pa!M= b D3: ;_"#/WMfi>=ffwff-$$fi C= /fi?= bff+?/G$+= Tff'+ < $W <P2-fiDP(ff"#M= /fiDPJ= ff'+*5Kv;= '+K !fi,8fi #@ &fi:("#SXfi,fi/Wfffi/Gff$Wfif T"# ff5;IWI$"#fiV 5 qb%';%' ="#&-f+%1=ff'F5hP= JffffX8ff'+*:P= a"#/Gfffi $c ff+%KNff= $4[cE_G= +E;$fP=+/,fi ?=+"3XXfiG 4 h;%ffX-V#@ fffi$fi.M+EZ . =*:1+%' = P/G, .$J?+P=,= .+ =*yzQDT&= TffffX4ff+4Kfi ,=SXw ff$F#fi+E0= M+=f+Mfi$afi>P=("#$P5#'<3'H'&#$'$K3<+1P'-3 |7!K'''3 '3'nSfi yz){ {z {z0{/k vlnmez)zx 0 yz{zy yz"z{Erzl 6 l/>+!"Pfi:9XfiBf"#/GfffiP $:,\E;2'+PY"P3-/V5Zv;= ?N.as%P/>3 [,=YP= fff+. =GFC#tLfi?= ;E;. :=+F= T)+X"#W+%* .+- (ffy;+ff.KffG/>Bfi ,ffP/,fiff!"F KN @ $>"P= +"#;+%1b"#fi!fifHGKP= $+*bP$+fi:= G #+fi= \ ffyzV"P=ffXff0ff`+ ff-fi L3Xbff&fifffi\E3(W]$+-\=+f"#fifi`/W$+?=++fffE_?fi"#cff+V ff$M $!DM 2-/G =ff"sff+5v= a!$"#V"PSfi/';=+M=#+ ,%'$+#"=fffi W= b .+ =f&fi#_Cfff (ffC/Gb=f"#5v= N.K"P3fi/+ ffX-$ff !1c= %/>3(ff#Nff-fib+% = Fff+b . =*:Sc= _ /?+"Pfi&+%< jkI>ff"#$c, +b= J />"Pfi,+%<+<5v9?X%'f= G$"#\"P3-/E;, $fffiVP= +EO=+c%+ffV3ff+*h?:wLE;XX.+Pb=+;ff+,"#5FZJff/G.+PT=ff'FfifE;Y.[Tt<6Wnr<ffA>7;nWW52GF,0 2;>2436<E!AffD3 ;DA<@F ;>24;<ff0 36;C%0F 0 5 2hhL5XY Y24;?>;<70 MW;9h36AH;UT gj*hv;=ff';c C$"+!G .+!fi V G: UT(gj*"+!$ ?yzT"P=ffXffPV?, .$E;fi=UT(gjZ#+ D:*+\f*5 q!""#$&fi, ffG=+b .M .+P$VEXXK=$a!"#->/G"##3fiff.F;/GMX- B(;P=+(hG:+?= P#%PbE;XX* )b!/G =ff'"+56x<ffA>7;nWW5X2 F 0 2^;2\36<EAHDV3 G;BDA <@ h K*4L5 YXY[2\;?>;<70 W;Oh 36A H;F ;>24;<ff0 36;C%0F 0 5 2Q("#$P&fi }?h :(K*4"$OhGyG+PDOh Z\ #+$LEfi=u+K*. +xhGy&fiffX- DJ) .$,E;fiP=>BUT gj>. 51cP=+#h'24A 3 .+$?+ D3-aK=ffK)+fiff$5cb%' = ;#@ &fiW+%1YfiffXfi C+%Kh "+?FM/G =ff"FRGh :ff&fi"#M= GE;XxX*+9fi$6,= _!#fi$"#fiC+%M+-fibM= "#fi!fiJP$+fi$sE;Rh+sfi.*"P=ffXffP*5)fiBE: ,&fiffX- J+%Kh"+G_)TP#N $GCc/W =ff";PGh :Dfi"#-<E;XxwX*K%'/h <-$fif= J"#.3-D;=+.+$}h %P/fi.;&-ffXfi D5nrv;= P#%P>sfi \Jff+C J#@ ffX"Pfi!fi\DP$Z= G%'fffiCE;fi== UT(gj+ K*f.+ D6:fffiME;XXK bG"#ff$/GC=+\"#5 "#3xfi>P= JNP$AXfi .+P>ff*:h :QfifffiS-Jff$W>P= R TEfi=G)= UT(gjV+K*,.+ D:ffYq/?$G"#ff<=ff'+'38ff'+\/G,=+\"#:+AM= #%', J!"#-V(!/>+'"+5 PV= b3ff+*:)= +E_$:ffc .+P$Vfffi f+fi#+Xf+%<= sfiffffi: : Ec=ff"= .+P$$"P=`%bfi.,ffGff-L"#:#fiP= VK*LUT(gj15I= \+ . = 2H$+."P=`3-fi= /'/>+!"#@"# c%;= s%"#_P=+;fi;/,fi =D"#&'ff;fi.F-fffi34ff+fE;"#5;h!1"4"" m@ X[""z |"#i +\(''@0=ffXfiJP= G fffiBV%_ M$$+."P=\=M]ff$\P>= G .+fiV =,+%;= ,ff+ fffi("#$P:4fiM'cfi/Gff&fifffi,V"#&ffb=ff=>"#/Gff-#fif-V'++!fi+*5J^&Z=ffb$"#!fi\E;"#&ff?= f#@ $"#$ #Nf%s .+fiiaE_#xG/GV fiAfiff."#-SaE;+ .+!fi>+?Pfi+3 5K79fi.;E;b"#/G+bP= J"#/Gfffi#@fffiG+%1ff'+V + .fi+>Efi=?=+;+%ff+V .+fi?%/"#.+#"=*)=ff;#+fi, +ff$TV$-/>+b+%K= +E"P-Db= sXfi .+P>ff/Y!K/>+."P=,= T"# ff_fi+-bfiW.ff%'< + .fi,b);%K=+W .+-*51c#@(E_W Xfi W= SEffJ+?$fiZPfi$%'/ +<yzXfi .5V79fi3xfiVE;>ff$"#!fi/GMfiff$!fi Cfiff."#-SF) E;= b "#$$c+%1!fiS3K+f + .fi*5nGfikQtmnriryz{z|~}a. @lX *ff+ .F=+;!b+?"$Q%"#T= M%' +/Gff.3* fffi/+%8ff/afifffi ,E=ff"P=>ff'++W. ff JPJ!fi:55X:E=ff"P=>ff'+;"+fJ"P= $+fffi> + $?C= J"# ff; fffi/V5^&=ffC$"#fiZE;> $!D,qfi/Gfffi>3fi &M+%= f"#fi-b ff,E=ff"P=L + .+!fi+Z+%T+#@fffi G"!MFXfiB#fiGG)b/Gb#@ $-fi;P=+f .+!fibff+ ff- 5v= J&'"b'ff$,c=+b+b+ffV ff:* + .+!fif=M#@"#fifP= Y+/WYfiM 2+-bff+ fffi ): fffic= , P fffi>PG."#TG PfiTff$"Pfi*5v;=ffP= Y$+#"=\"#.+"P=fffi ,%H"#<'; b $F%M + .+!fi?=+?%'; .+-bff+ fffi )5)Db=_P= b .+-T #+"=ff- ,%"#K'bfJE;PBfi Cff+f+%9fi *= #@ff.%'G= f Er fffi/5L^H=ffG"!:;= V"#DG% .fi]U5ucSEr D!V=+W=Xfi .D2'Pfi+3/G(ffff-qP ?Zffu=+f"+\#@(Pff$uPiE;Bfi Zff+E;fi`=+ .fi=ff'J"#P$ =fffi\= ffifiZ+% ECJP= >ff'"#/Gff+% M$fi+ !+,5?v;=DC + .ficX-B#fiVPf,%HbP=+Z .fi,ff+ ff-E=7 `v;=ff'K- $R3xfi,&N$?E=G (]fi?fi"#$$:;= Afi D+!fi= /fi"#P$$W+E. ,Xfi/,fi,+%J 5GP= V .+"P=fffi %"#(v =ff_!/>3X ."=fffi s%"#.Q#@e"#F= $+$!K ?W= #+fi5k~F &fi"#M .fi;ff+ fffi Z3-/GDG3fiE3 b=,\ ."=fffi %H"#,+%b+Cfi$!qZ+Lfi"#G- = q :E;"#"Pfiff,=+b + #+fi?';X-B#fi? #%#+fffi,E= b= C-S3K/G fffffiJP b>"=+P$RfffiP$T+c/GDT;/?+Df/W(XNw"fi;c .+!fibff+ ff- GE;ff?$Rfffi5"#++fi>$!fi/>+f $.C=+,=ff,"#P$ GZAN$xfi .+ff+fiLE=ff"P=u+/GDcG+%8P= Y"#-+sfi++5F0=ffXfiCE_,"PBD +E;fi$ff b=+=ff'T+Sfi(F/Y!;.+BJ-DD!#fi:$E;F#Xfi<-1 +ff$#%'ffD-Dff-fi4YP= "2HRSXfiTP$RfffiP$JPT/>B+ .fi?E;=ffE=ffXX5fiff kRnnPlnozr.rvn;/G(ff#*+%*PfiSS+? + .fi+,Q$W>P= /,!T=<= b+qSfi-= /Ofi.#x%.+P$fi.9xfi .+Cff+5K8+K .+$,ffG+q />+!"3XXJ=3M$CE;-=G= /3X= Gffff"fi$cfiff ff"#$f-qP= J "#$M+%< ffX'fi >P= Yff'+*:)55M3XK+%Kfi.M"+3Xfi B+f"#P.3fiff.5~D+%*Mff+*yz9 )D&fi-1;S+'+ffXXfiU$C)#%P_P= ff+CKP$M-G= FXfi .tuE;ff>P= JS+ffXxfiU$+X?fi\G Pfffi/G2H)$"PXNw"C/>+ $:) M= J #31' J%8E=T+P.;+%bff>CS+!+ffXX-U">)TfiE;$><b Pfffi/%1#@(ff'++fi 2'$, .SXfiU$+-S*:ff+C"#$?ffGM$ +2_#XX1+?~\"3_+P=D?.$ff1+?ff>b+/, =+/G+?M$ +M.$ +.5)fi #+b!fiS31cE;2Hb "#$[ +-YM+%fifffi'3(+s D3("#fifi6:$= 3- 2fi= /N.ffD!XN$;= C/GD; /a&fi CXfi #+Gff*: = qffff$/GJ=3X-SE0/GXNw"+!fiG/>+BT= Cff+*yzQfiff-3*+f ff31"#fifi</?+."P=f=fi .5nSfi yz){ {z {z0{@//mez)zx 0 yz{zy yz"z{e`8) K 04 ffLIv;= AN.G =!q%=-S3T P("#$W$W#fi= >`+ ffX'"+fi 2H! ffXfi$]/G= L>ff/>Sfi 2-fiffffff<3fifi= /&-/,X+9= F ;$bffYb+/, =+/W+(+C-;.$(G#fi$"#"+' +bff+65879-.<P=fi DS<+PT/>."= $f D3fiF= b$"P=Gxfi .+Wff+*yzDS:*+\P= GX-+Vff'+ME;fi== G $$bff /Y)J+%/>."= $C+,'ffDxN$45>v;=ffC"+$ff-Mfi\/>+ffq"+$:*&fi"#?.3Kff+b"Z/>+."P=*:*+\f&fi fi,ff+Z"+\/>+."P=fi\ff /,?+%sxPDGE3 5Lv*"P= ffDq/G Z= fP/>3fifffi ]3fifi+$C= q3-fi= /#@+/,fi $C= ?/>+."P=Z) E;L= Wfifffi!3_"#fi!fi5^HG"#/G P$b%'C$"=uSfi+!G=ff /Y),+%T)i"#-fiC"#$+$Dff"Pfi = WXfi .+PVff+*yb-fffi3_"#fifisE;fi==fi G-fffi3T"#fi!fi5v;=ff',fiDPff$uPZ/G$ PV= \+/G ff>%ff fffi E;PB$"#$+PL\ G=fi ,fifffi!3E;].+P>= V.+Pf#@()$"#$D]= ?Xfi #+Zff*5^H>"# D.a= fD /,)>+%b)`"#fi!fis%G$"P=LfiL+"= ffD$,P= fff+LE;fiP=L=/,fifffi/, /V:ff $Bfi Wfi$;+fffiP.+XX5//b(b*6j(Rb19Z4 b1eKUuI3LjT3fi ,/>+."P= $fCX- .+Gff'+*:ffN- J-,= J Efffi/'&fi/Gff-[5;^&.+ff= cXfi .Gff+?E;fiP=>= MS+fffiMfffifi ff< P(ff"#$ADGP= b/>+."P=+55[iff"#b= sXfi .+PGff+*yzF ff31"#fifi;E;fi=?= b EO D31"#-fi5q 5MF$+,G E)Z"#fifif%'T$"=\ DS1 )D&fi-f=+b+ $.cfi\= J E D3! ;-f= MXfi .Gff+*y; D31355[iff"#M= ;Xfi #+,ff+*yz9-fffi3w"#fifiQE;fi=,= E0 fffi/yz9fifffi'3w"#fi-557e;$"P=q"+S*Xfi B?=+,"# /G$W, )D&fi-G%'/ = b+Wfiff-31"#fi!fi:X%P=+b )D&fi-\C+ffb%'/P= G EfifffiS_"#fifi:*= Lff#fiW= Gxfi BZ+fG"#$!fi W E\"#fi-*557e<$"=V"+P34Xfi BG=b. ff"#$GC D&-fiW%<= b+'G ff3"#-fi:X%=+PDfifiW;+ff;%'/P= b E D3':ff= Vff#fiC= MXfi B459nt~n iv,;ll !ttQvn:ozrzl omno9vv;= G3fifi= /+Yffff$M ? fffi f+%_ )ff/ C[c= JffqP $\"+Z"#ff.3fiM=T#@ff$]# (ff"#$f"3KXfi B %b PDfififi\= CXfi #+fff+*y ff3Q!=+G+f G+G%T= V E D365c"#V= AN$]ff+u"u"#D#3fiXfi B :_!:;+"##3fiff.;E=ff"P=Z+? + D!fiQfi#fi++ffT?= G"# ffb ff-/V5 %"# .:4 ffXK=+ .fiG3fifi= /"#SXfiJPfiQ9fi/G)D&fiff-JP#XE= = F= $T.+%*= ;xfi .+ff+JEXX("#P3XfiM C 1PT)<#%'ff 5^'%4/G+$bfffi M= FN- "#$:+= ; + .+!fi3fi !fi= //afi =ff'"#+=+Ffic $ ,2' .+b= JP+/GJP"# $5v;= JR $fiV= #%'J$;GE= P= T= sNfi ?3fi !fi= / != ffVff#-Y3x8"P=Xfi B :$)ff3xM/G+fi ;/>+ff!1J"#P.3fiff.9 "#!S+!fi<#+ ).:= fffifi$3;= /fiW= Tff'+,= fffi JP=+<= ,E;xXff3XfiC +;#%'ff8M F+ D"P=4 _"Y$&XfiJ"#!"#K"$9fi,E=ff"P=J#-= <. M%'/>9E;#X+C= ;= %'/>fffi5nSfiyz{z|~}a$?++b+,-DP$fifiDP."#fiY)E_?= ;P.+ ,+W + 2.+!fiYSfi- = /5k : y4."#fi,3fi !fi= /P= <-D#;+%w#@ &fi*:+E=ff"P=Y/W$+=+fi\"+fffi0P."#Zff$"P&fiVP=+ffi\/,- =D=3i"#3Xfi0/>ffiff- `#@ &-*5: iEXXfffi&fi] > .+\ )ff / ?ff`*:uLXfi #+]ff+"#ff.3fiff- J ff / FXfi B K;F"#ffW <=3)f ff"#$ffi$"#fi,ff,= + $5^'%<: : a/afi =ff_ _)J+fffiMJP."#3X4= bff+ ff- aff$"P'&fiQfi?= cXfi .+PYff'+*:+'M= #%'G J"#/Gfffi5AI+fi"#,fiJ" JP."#J3X< PfiMff+ fffi \ff$"P&fi6:4fi"+ F."#<3X= ME$C"PBYPb= ffffXwff*:D+,P= #%/>3C%SXb#@ fffiP= D!fiff+f"#5i;$"3X4%'/ I$"#fif 5zC=+F= M."#!fi>3-fiP= / $ff$afiI$"#fi?Cfffi,"#/GfffiFE= G$YfiY"#+ "#fiYEfi=Gb"#++fiKN- P.+ :+83-+-#XDC/GX%fi M=) G G ) n b"#(ffcfiKff#fi$K )ff / <!&t`= 1P=+=fifffi!31+? DS9!;=+MffG ; ff"#a,"+P3*Xfi BtL%'/+ff>ff?fi P5k+i }e!+(''@~D?-?E;BL"2'$0ff+ ff- L=f"#"#ff.+$Nfi Lfiff#@D-"P= /G$C%'J= fff'+Zxfi .+:E;-=Z= ?ff$=Yfi q]fifi \+'+>"!$E;ff/,fifffi/,-UG= f $%'G + .+!fi*5Z"L = #fi$CfP= %./GE;B\+3fiffUJP= + #+fiV"#/G ffM+%<= b />5v= YP3fiTP.+ +-$Ffi"Pfiff$Afi=9* / M+/G/G4:K$ ff#:ff%T#@+/Gfffi:*!$"PX%'V#+&%'/>+&fi;=+b"+\>ff$"#/G2D!$WfiDPY$R "#$%8+<ff;2\;_m<P;>36<ff0 73)fi/,-fi$5 T+3-(&'K +$F#%'ff4fi>E;X*ff_E3 [5;^&!= SEM,= +E9* (y,fiff#@fffi L+LP3fif.+P +X$G"#ff\#@(ff-+fi$fi= q%'.+/WE_B,D? +'fi G=!"M$+."P= 2H"#ff+4fiff%'/>fi*55;^&>ff/G!.+$a= SE+<yG&fi/Gff-V"# Pq"+L)q!$L3fiffUV/GPV"#/Gfffi#@+ .+-qP.+ +-$:*+\fffi!fi/>+#-V"#ff\)>$q"#/W+G3fiP+fi,=fi$%1ff+fSfi$5?.+ME;fiP=f$"#fi /G/>fiU#fi f9* (yJff$&fi *5>v;=fiLI$"#-]D5z?E;>"#&'ffZfi.;P3fiTP.+ +X$!$R ff3xfi: ff$"#/Gff&fi G= /fiffG+]..5I$"#fi\D5 q, +$=+J9* yzbb+%<P3fi.cc-"#/Gfffi:*ZI$"#!fi]D5VP"#$ME3(cf"# ff>9* (y=!"FfiVFyzQ%./GE;B45<I$"#!fi\D5z,P"#$c= SE0 M+3fi &;"#ff?)J#@ ff$= P.+&%'/>H-3wff'+ ."P=q[R*TQ(Uw) Kw,I+-/G/G:4$ff.5{rmtrir.lrzlt!ti9*9* V$,?N2H.+ W ("#$Pb%'Y +fi \+Z#@fffi ff+ZPq"P=fffiG E ff35f9*N..B$CX- .+Gff'+*:ffN.F-,= J EO ff-/V: +V&-/Yff'+$Qfi.;#@ $"#fi*: &fi GP=E-fffi3_"#fifiC+ D36V5 i; =ff-$Bfi ):89* yzs%H3Xfi $C"#$!q\ff+fE;fiP=V+Ffi$;/w$Et`,= $+P $WXfi B?; $"#fi-*59* #@ <!$9%PE.,+C"BffE+.,"P=3fifffi MJ+3-DU;= ;%H3X- :S'"#+fi M=fffiXfiBCE=+TPqPT"+!$VP= b%H3Xfi :4+AE=+M D3= D,ME;Y!'"Pfi 5Tv=$fffi9*M"+3 E;Bb"#$)fi MM= _"+P3Dxfi B("#P"#$bffJ+WfiJP= _ P("#$nfi yz){ {z {z0{mez)zx 0 yz{zy yz"z{+%ffJ .fi*5 W+>P= #%<)%/>9= FN. E;T!.+ $4%w*9 (yzK + .+-b "#$fif= b P("#$c+%Kff+f #+fi*59* P= Y!$= "+3D#@ ff+fiC!#fi$"#1 _%w&X@ J $!$C+ D!$:S"SXfi$v FFID"= Bw:)$ff#5 v Z3b"#ff.3fiKb!K+%wSfi8!.+fi$&tuff+C.+ %P/>+fi+=+M/,fi =D#xfi/,fi+Pb= C%3xfi #t`+A$"=3fib#+ ?=M+qP("P'+$fP$TP"P= $"Bfi.,+ ffx"+ffXX-H59* yz,v I+Vfiff$fifffNV"P$6[G%H3X- $Jff Z&fff#*$"#.:ff$&fiP$G#*$"#.: &'ff%'$+ $6:Dff$&fiP$,%'$+ $6:(>P>+#+/G.651v;= Jn b1DT"#fi? 2$ff.+!fiJ!$,D,+qffff$K <fi ff=J)E_W $"#%$+P $K+C= K D&-fi+ffD$M M3Xfi+E+#+/G-U$f:4?fffif= sN.E;>"PP$%Qv FTJ#-S+ff? T+3-(&'5;^HZ+ffV":)= $C E;>"P'$M+b= ,/Gff;fi/G).D$:)&fi"#JP= V""# ff%/GC=+f=3x%<+%8P= Yv F5v;='fi"#-fE;q?&ffC#$"#M+V?ff$&fi$A#*$"#;.Sfi =ff% E+#4[1&ffb#*$"#.c+M.;D!."#fi-F=+ff*yT! Mbxfi Bw:ffE=ffxfiff$&fi$A#*$"#.TffW=$Y, )DCfif= Cff+*5M+ .SXfi:ff= Y!T+%<++bP3fi.X*ff<-f= JE;G"$5%P1"= ffD&- Tv :+*9 ;fi.+ff$= _P("P'+$c3fiK. +X$4&fi c= _ff.SX+%1= J"# PD;fffffi ,.B45K7 ;$"P=fDP&fifffiM3fi9* G ;,P$;GMX%1= M3-<'+ ffX"fffi: &fi ?= ,$ff-+%<=ffc$cG-.+ff3b= C3fi357 M#@e/Gfffi:)= J$!%'+q."#M3-T"#$!fi fP,fi!fiV%_\.Ec=fffibBfffffi =ff.fP<=+ />+*:$(<E;ffff/,- bE=ff"P=V"#ff'f$P;= Jff$&fiP$f )D&fi!fi*579fiSXfi:$9* $+% =!"Kff-$4. Bb= <++fiwfi.+ff$P3fi.:+"P= DD!$= M$$: +?+ ffX-$K-$5 "#b= Tff'+G'K.&%/G$4:+-K;&fi/,ff$,"#b+ D3-*Dff$"#-+%<, E0%H3Xfi b..F= Y"# "PfiJ+ EC5fi ff{lrmtQvrmtrzl!ti9*?+%1*9 (yzFff?3fi;.+P +fi$KffJ _+ fffi,C _nbeD2-xfiBT"#!fi, $ 2.+-*5K7 #@+/Gff-: = b3-=+T3 #;= Jff .+!fif+%<+V"#-?Ffi ffX"+fffiJfi"#3X< b'D3"#fi+G! /G$f? ""# bfi#+D#+ &fi5Mv;= JP$b+%<=ffM$"#-qff$"#!fi$= b#fiSD;3-.+?$ff"#$c= /P>+Z-/,fifi$57 *Sfi.* EL9P= /w$E;$Mff*51^&J$"P=J"F= <ff'+M%SXfi <"#P$g ePaYXfi B% F= $+P $fff+ = Tm\ 55%ek@Xt LV ErL3%P1\T=YEXX_P/G+G= V&ff2'#*$"#, )D&fi!fiY* #%'JP= J"# /,fi ?*:(#:ff'<#@ $"# $455}} @kefi#t O] E0=V"=+ $f+0 ff$&fi$0P.3fi,-D] ZP=+/>."= $F= C D35% ek! k e#tq 5'"+5fC E!>P=+ff#fi$;+? $"#$yzK ff$fi$f"P=+."#!2w6.;;Pfffi 9fi3'KP6PSP-#$<P'<;#'<ff$''-#;'SP *3H)z*-$<'3KP''1P$3.ff 3' 'M;H#!M'$&Vb#$!*P''HX6QP-#$*'JP$??'$C''<#$bPS!-4b #-$<#b'$;XP'$'KfiJ'SP<+'HJ'"!#";6P$6$3%$Jc!P$-.C'$'P$<'+KPJPfi 4$3.P-#$<P'K'H.#cM$#&'!("Q$#!3<3b'$'SP1#$KPS!-c$ '-fi#+$#_6'#n\fiyz{z|~}av;= $G= P>Sfi.J,ffff"3K%'/ +<yzs.!$"#fi:8fi"#nbeD?ffD$C ,2fi ff!=) E;Le $"#C%'$+ $G]= ,D)$,+%b ff&fifi5]I+fi"#VP= f3fi.+P ,;P= J+/Gsfif= JP= J"$:(fi;; "P-$+TP=+= J!fi"#fiV*9 Y/>B$E;f $"#.+W%'$+ $ +ff$c+DW#%ff<"#ff+*Bff +E;fi$ff cfif= $,"$5"P=b+%= $F3fi#*"#$! 9= Qfiff ff"#fib+%a.E=fffiKBfffffi =ff$5z ""#/Gffx= 2fi ,= $b3-. />"3fi,$Rfffi$;P."#- YP= b= $+ $Wxfi BG= V- >Exfi B\ (ff"#$]ffZ E !LH.= J=i&-/GfffiZfi = > EP4.5Vv;=ff+q"+f&fi/,ff= $M= c.+&%'/?+fi+KEfi=>s."#P2'#N !$R "#:Sfi= =fifi3_P."#fiZ/,fi =ffJ)f $ff$L\#X-/,fi+fff$"P&-C=+Gffff$LL=fi +fiSw= $ $WXfi B455%)+*--, '.0/@21'ctocfi'ffJG?fiff,E;a'fi"#=+; P= "=fffib=ff$fi$?$ff-.;+%1P= J- +fi3H5^&+/,fi fi :)fiM'b"Pfi$+b=+C= >PqPV?ffXfi$:D:*/,J)G ff"Pfi VE;"3Xfi B :*&fi"#,-b""#/GffX!=fffi >E;? D!$5Gv;=ffb+u"+Z#*$"#b=ffMP3fiffJP."#- b= = $ $CXfi BWHE=ff"P=>+ P/>+'"3fib/G+$K/GS+fffi;fffi-,.ff- Y"#P.3fiff.#:fi ,b E2:ff+GP= >fi Gb EXfi B%25g vE_,#+&%'/>+&fi<Pff"#J+f#@ff- a!?fif= Cff+*55}} @fi3*e .54761,7/@!18KHtu^&f=ffF"M= ;%H3Xfi FFMX- B1 KE=ff'"=,'K= $ $ff` = q!E= D!ZD!."#fi-Y*? ?fiD+fi$fi+ P= fXfi B45v= f3fi,',Zff'"#m4bE;fiP=u+ = G*{: 92 =+?ffD$*y G=3+Y*W)D."#fifi*55}} @fi3*e .54:*@fi;.0/<, , =.tv;=ff'%H3Xfi Qb%9E=ff"P=J#fi= K=K+,, $"# 2-fi1O<E= D!T $"#fifia; P$Gff>C= $ $,xfi Bw5Kv;= M3-fiP.+&%'/>H-Gff"#${FE;fi=VC Ef=ffff$; c=$C $"#fi2!fi*5v= $,.+&%'/?+X;=3,= ,$!bffS1%'M +- LEfi=q!$+."P= 2H"#DP+= !"65~F=u%Y= $\Sfi.?/>+Bq]ff"#/GD\<P;>36<ff0 73a%+xfi+E<$Lff <ff;24;Pq?xfi B\fiZP= >/,' fi>%c\"+P3_ E;B4V5 i;$"3x := SE;3:*=+,+0fffi\/>+B$"P=+ $?P= %'fi $Z+%,= \ E_Bw[]+fffiLP."#.?ff$"P&-?=+?=3\= ff$"P'&fiMff(fi f\= /V5J7eM#@e+/Wfffi:w"#&ffb= C%'+Xfi+E;X G#.fi%P= (} @fi3*e .54#6>,7/@!18K,#@e+/Wfffi+5]I fff=+C= V"# ff,ff+L"#D#3fiE;>-fi3Kff$"P&fi6[= Gff$"P&-V?$.ffX=\f"+3XfiB 4> +?VHP=ff;'E="$?= cfi"Pfi&fif+.% 4, +fifEfi=48f3G,ff$"P&fi>, $"#=ffQX- B,%P/+ = ;= $+ff- a!m5I+fi"#b= M+P"= +'"#Jff cf= bW#@D!"#b+%= cXfi B4:ff= bff$"P&fi?G4,= Tff'+_! %{4 +?Y"+ ;)b."#P$ffX*= Jff$"PfifG P$"#Ffi=VP."#$45/Yff'+fi+M+%(P= #} @fi3*e .54#6>,7/@! 8K19} @fi3*e .54@*\fi;.0/<, , ;..+ %P/>+X)E;ff$!fffi*->;+f#"#*).+fi4%'+Xfi3E;$MDR><P#N $:+E=*P= _ff /,+%ff)ffDbff$"P&fiFfif= ,"+3* E;B451^&V= J"# PDM+Z-/Gfffi/Gff.+-S*:ff= Pn\Ifi yz){ {z {z0{mez)zx 0 yz{zy yz"z{F J%H"PXxfiJ%'F=ffFD)b+%1/>"#PY.+P$: c= $J9* J#+&%'/>+!fi+_! $= JxXfi>%_- G= G$R "#Y%_ff$"P&fiMP."#$fiVP= Y"# .!Y%<fi +fib= c%- :D+W= ?ff3fi b= $bff$"P&fiKE;fiP=>b "#$;&fi/aX+Yff!fiS+!fi3+3fi f_+P #X:$qdK#fiD>;+ #X :4$ qeGdK#-D:4$ff.5J3-"P=+ $= Jff*yzFS+'+fffi2'fffifi ?"#.SfiD#5gff5}} @fi3, Er $"#WG !fi $Z#Xfi/afi+f ffE+DP$]%'$+ $,Ec=ffXfi/>3fiff.3Xfffi Gff$&fiP$f"=+#"#!"5v =ff93fi<"+,)$,J"#$"#KMD /,F+%?%3xfi $[4= P$+ $sXfi B :3fi"#2;D"#!.3fiff.: +A=fi+ffxXfi,> )b+f\"#fi-Vff JG P$+fi++fffi= $+..5G+`"#ff\#*$"#J= W3fibff\."#!fi f= >ff$"PfiZ=J ff$ZP= >"# 2.3-D.FH/WDKxfiB#fi,= J-fiG%8,"+34Xfi B)8?#Nfffi G,&fi/,X'+_ff$"Pfi?=+) Va E $"#$5g v= \.+&%'/?+&fi,/GX%'u= \ff*yz?/G).3J"##3fiff.:;#fffi ]#@D!fi!55}} @fi3*e @.k>4-@Xt`v=ffM3fiM"#$) cG /Wfif+A$Rff-$; ?2P."#fi*55}} @fi3*e @.k>4:Ae1#t`v;=ffP3fiK"#$) KPff/Gfi,+,3P$RfffiP$P."#-*55%!B/@@xt`v;= G#ffbfiZEc=ff"= E;VPJ+WV)>b.$45fv;=ffC"+Z""#/GffX!= $]DLP."#fi = qff$"PfiuP=+> ff$= V!fi +fi3#fffi L+- YP= bfib.ff!fi 5v =ff'Y3fi &b%s9* Z3fiL , ff.!.+fi Z%T.+ %P/>+X3;ff+ .YfiE;;E$ 579fi.Qfi_"PXN$K9* yzF.fi*: +- YJfi/Gfffi#@ ff+fiG%w-.KSfi_P.+2+fi$C+Z= +E;fi E=+J.b%_.&%/>+-Sc-Y"+"+ Y""#/WffX=*5I$"#fi3(Q= b ffE_B,%'Kfi"##+fi G9* yz;.+P +fi$-D,+<yz; .+fi3-X= /fi= M%'/+%K"#DP+*+x"Pfi$MI$"#!fi\D5e.5nkR.nPlnt~nzv v9*\$fffi?+%Y+3-DU#fi L9* yzGP3fif.+P +fi$Cfi0+<yza%'.+/GE;B?ff/W.fi=+,*9 (yzs+'"#ff+JC+%3fi.-"#/Gfffi#tuP=+M:= ?+>/G>"#/,fffi+!fi+%<xfi .+fff'+b+fi C fffi/?c%ME=ff"P=9* fE;Xx8)G +fffiGP> #+>fff-.+fffi$R "#J+%K3fi#5F<&ff3:ff%_#@+/Gff-: = J"+S8"#P J= +E?fiV79fi T)5/G\=f.fffi u"#.3-D.W$P"#?= Zff+P?L= \N PyzWfi#%'2'2'- =D.ff- 4fi\P= JE;. b )DGP=+b= Gff-q"#&'ffMXfi $+-U$+XV%P=ffMff+Z..E;fi= ffCP= V4=+?J*5<v;= b++ETKff P"+P3X- B(: ;fffi,E_Mxfi B(Q=3+)#fi$fEfi=f= C ff&fifif (ff"#$HP= J= #+sfi#fi++ff.5cI+fi"#!4;ff#fi$%+$Rff-$Qfi$:fiK'_"P-$+_P=+{41P= $+ ffC+ E 5;I+fi"#BF?G"# /W$YD,:eG=YDd+P#%'ffw#*$"#.<,= = P$+</,K/>."=W +%19* yz<ff$&-$,#*$"#_v F5^HW%"#$:79fi G,"P&'"G#@+/Gfffi?+%= HAe >I/@3*@;./<, , ;.Zv OE=ff"P=]=CfffiZE;VSfin\Xfiyz{z|~}aPSaGoalbPStu79fi M[<9* ,3-."+ N @Aff$&-$ff2'#*$"#Tff?%3xfi $FE;fi=?=ffc"+38!"# 5#+ +fi$6[ >ek@1+U} @fi3*e .54@*\fi;.0/<, , ;.D5F^H?+"#ff'+$:9* b8%ffffW%/fi= R} @3*e7.54761, /@!18KbSfifi"#J= ,= P$+TP$fffi.%P/?ff$&fiP$V#*$"#C'%' ? ; b&ffb#*$"#$5Kv;=ff';/G$+F=+9* GE;xXw M"#&ffPff"Pfi %4E;fi=V,Pf=+ffD$!*yff#fiJD,:1Z= ==+b/>3V)>P= Gfffi\E3VV"P=fffiGAE_B- fff+*5>v9V?=+=ff.+&%'/?+XW"++ff-J+%K$fffi!fi Cfiq,E;PBfi ,ff*: C=+;= Y"P= +"#C+% F?G)JP= G D3</?$\=3,fi"#P$"#$5G^&= JE;. 6:)fiJ/>3?DfifffiGfPff"#F?GEfi=V+ P= T=+ffD$c M$RfffiRYD,:)E=ff"P=fE_ff?/>+BbP= b%H3Xfi JW&ff2'#*$"#%H3Xfi cfiP$?+%<Gff$&fi$ff2'#*$"#;%H3X- : +?E;ff?+fffiB4y;ff"#/WD$50=+b+,= Cfi/Gffx"+fi;+%_P=ffM$fffig _Q+ff-V*9 (yzc-"#/Gfffi $PcM+%_/afi"#$R "#:<$!$"P3xfi&fi"#fP=+J $"#3yzb ff3_EC (ff"#V='"3xfiZff$RJ%_-ff#@Dfi \+.+&%'/?$!fiZ. +X$M.+= C=+%/>3X-VxNw+fffiG3fifi= /V5V3fi &QXfiBc=ffFF P= #fi$Ffi!"#fiT&fi"#Mfi;/>B$F $"PbE=_P.ff+1;9* (y3fifi= /d/?+B$5^H<"G;-"#-<J;B GEM"#ff83-fiP= /! 4:+fi"#/G2fffi:) /?+"+:fE=+bff++D#+ $Ffi#@( P$&fiC+E;M#@()$"#$%'/?+"#+ ff3fi $CDG"#XNw"Pfi J/G;%'/>3 )5Z#xfi=<+>3fifi= /Vyz%'/>3 P 2fi$F +ffb b+%K,D /,)+%KE3 F, ff..+A= J3fiX= /VyzQ=3fi$:ff TffG"#- M= Jfffi!fi/>+M.+ +#?D?E=ff"P=V+qSfiX= /yzK+3fi Y!= ff?c ff $45Z #@(? ]= Vff"f+%J= +ErZ\= \9*fi>#+ +fi$aE;fi=fffi= q%#+/GE;BGPY ffffb= + .+-SfiX= /5u t9* XlrmtQvrzltivrv>n\vl 60v= =fffi = $afi# :;9* +u#+f-LLX*DWE3 5u9* #+.CE;fiP=uG"#/GfffiPfff+uP=+,%H3XCP+&%'Z= D3T+$W.+&%'/?+XCZ #+V E"#/GfffiPff+*5<9* ,ff/,fi.Q J !fiG+%1C+!3ff+f+, ,#@ ffX"PfiFfiG+%*."#fiL"#/G/,-/Gff$5<ff.?=ff?Efi=uP= D"P=0.+B0ff`+<:ME=ff'"="+0#"#DfiKff'+ fffi Yff$"P'&fi*:ff$ff-fifi>Gfi"#/Gfffi#fiG)$"PXN$Wff+*5Kv;=ff:ffJff+E`E;fi=]$+."P= 2H"#ff+<='"b"#P$fi V\9* (yb.&%/>+X6:4E_W $]V"=SfiP= Y+<ycfi("S_#N @ ~$P."#,ff$"P&fisV#*$"#,]H /G?%'/ +P$f+%= Gff'+"#b?+ = $5v;= ,&fi/Gff-$E3>+%<fi- >+]=ffM"++ffXX-P>#%'/,ff J<yzc 2-fi#8"#DP+fiD,%P/s $ff= 2-N.!<#@ fffi.+!fi,+%*= b"#+%*ff&&fi J,R MFfi- JR +>?ff = 2-N.Mcfi.+!2Hfffffi fff = 2-N.b$+#"=H&fi fG."B).5F^HZ"P=q?"= /W) n >E;ff? Wfi R ,3X1P= J EOff+;P $fffj=G G %0 O\?fi$fiTE;ff'q"P= ffD,= .$!.f""#$bff+]H&fi f/WY= !"s.+ Bfi ?fiff%'/>fi+4#@(fffiCfi$:)fi$3fi >P= aSfi+P$\= ?."PB?%M+M#@(ff-.+-fX%<"Bff."PBfi > PS$$"#$+95 ) G G) n k :?E;fffff?X-BE;fi=f= ,#"#$f ffyzM&fiffX- D5v;=ffn\pfi yz){ {z {z0{mez)zx 0 yz{zy yz"z{/GXNw"+-G,+<yzF 2--#"#ff fiD>#X-/,fi+$P= $W%'b +-3wff'+ 2'S3-+fi=!"+:ff&fi ,-$?= M%'+X-SEfi b%' = ffB F%=!"T"#DP+*BD +E;fi$ff [5;`= = O) G) n k :f "#$ff WbfiZfffZf#"#$:4=!"Cfiff%'2/>+fiGF =DcG$Gff$"PffJEc=ff"=Vff$"P'&fifW."#35e5 '% <) G ) :M .+P$9fi. ELff+9-K$=!"J"P= DD!_Ec= =Y"#fffiff M."#!fi G"#.SfiD#9%/P= b+ff__Ec= = ;,#N J,"=ffX'f ,X%fi"= ffD$;P= M+3:DEc=ff"=VfiffXfi ,GP#N +.5})qe5n JxfiBE;!$;='"fiff%'/>+!GGff/,- TE=ff'"=?V"#fifiW= $+ $Wxfi B!= ff?, ff$!$WN.$5)5 %'<) n #+$fi.""#$ff+fi$9= !"9b#fi$"#E=ff"P=Y&-ffXfiPa"#D-D b#Nff- 5<&ffJP= f.fiZ%T=ff,ff = 2-N.!GS+!+ffJ+%<:K +fi= Wfifffi!3<NP$Zff'+.+ $]=^K9i+ UT gj85 i;fffiq+ ffXfi$:;&fi"#=ffG"P= +"#fP$RfffiP$Gff$"Pfi ]E;."#fi0+#@(fi*5ZZ"#ff'u"# ffL*9 (yz?P3fif.+P +fi$,ffu/>Bfi ifffiZ#@e/,fi ?= "# PDCff+*yz,"+P3P"# :?=+,"# P>\"= ffDf+L+ !+v :+0"= ffDZ]Sfi!.+ 5 fff$"#-$fi0= \fif!$"#fi:;$"=P3fi. L"+0\"#(ff$ >]/?"#Z).+>+%JP#N $V+."##,t = $Z"#ff'LEfiPLfiffL] +fiSXXu""#$&fiff-q/W/Gu+P$+ .P$+\ff`! $R ffVPfffi$5>-@1P3fi</afi =ff_#@ +?,C E;2H!!$R "#[KP."#FXfi B4:P#N MXfi B4.5 i;fffib,E;ff"= ffDZP."#G= AN$ff+*:;= ^i;fffiLfE;ff"= ffDf= ff-$/GAXfi B]Z."#$4:=i;fffibbE;ff'G"= ffDM= T"P=ffXW"#$)fi ,,fi JP= P>$"PxN$?D=>-@43fi$5 K=ff)+fiff$:+= ;/>"#Pb.E_ffC=$_)"#/Gfffi#fib#@ $"# $45I+fi"#M=ffQ E"#DP+wP"# M$Ffffi,= b.+ +#G+Vff'+>/W(XNw"fiG.+P.+ffffi>P cE= V= ,M+%8Z"#fi-+A= $+ $Xfi B +bffffX :) ff $?/>3-D.Sfi $45I+-/,X+!fi:Wfi ufff = 2-N.Afi.+!fi2Hffff- u$+."P=0?$4:M=ff'+ D"=Z P$$,+<yY"#/Gfffi $65\I(!/>+'"PfifCfi++P$VffZ= ??+%fiP.+X2fffffi 0$+."P=*:;= +E;$:+=?+ = ? fffi/ E;fiP=` />"Pfi& fff=ff'+ D"=f;E;#X [9/,fffififffiM3-.;"+ ; $"#$+!Xfi?b!%P/G$Wfif$R "#5;v;=fffi/P/?K%'/= M%H"#;=+3x9* ,3fi.Qfiff+-#N $?/GDFfiff+fi."#.%+xfi+E<$WDGP#N $L5 KK$:ff= MfffiGff'+F $?ff>,"3X4R) n aT#+ $UT(gj+?=ff= f"+ c=$J,.+ %P/>+&fiWfiff+fifi JP."#- ffXfi$?GP= /HE;fiP=fi+X G />+!"PfiH).5v;= $+PG,.3*)D&fiff-J+fi-F,=ff; fffi/V[g oM#3>/G- Y+ff>P3fi.F=+MffG Ffiff+fiM#"#fi*:ff"P=V}} @3*e7@.4@++} @3*e7@.4:A@1 fi:1 ffX+ = bP3fiJP=+!C A;nWb."#b=J)i 2ffX-$45g %'/3xP."#-QfifffiS-:#%'Cfi G+ff>#@ &fiV + .+!fi57Mg ^& C= %K*+ UT gjq#+ D+\3xfi+EP=V#@ &fi\+."#fiV+b+fff (ff50=ffXfi;=ff'<+ ff"=G"#XNw"#$<!(P/>+"P- :+P= = c=<= TffS+ff.+ $%9!$+."P="#ffsfiP$"#$D0*9 S2Hfi\.+ %P/>+XE;XXb+1?=fi"#$$ &fiU\+%YP=+.SX$."=V!"#5K^H\+ff"!: = J+ ff"=V!XX4 +.+ff$"#/Gfffi $65499SPz;*ff'';&-<P4'_'3<+3H-.b'&$$$'bO!(Nn\{fijQPllntQ9!tyz{z|~}afinrmtmr v vv;=ffb!$"#fiZ"+\fffi\B."P=\= G)D&fiffxXfiX$%M-D .+fi W= ,ff$M%_.&%/>+&fiSff+ #fiffM= T+A%./GE;B45*7 c$$+."P=,E;XX-/Gfffi/GffK= $Fff$F+,$!KJE= = bP= VE;BVMP= Pfib$"#fi\ $!.5fi/Gfffi/Gff.+!fifE;ff\3!3xfi+E+fffi,fi$QS3fifi M= #fiFXXfib+%*X*ffK3fiK='"5Z!$"#;=+} @3*e7.54761, /@!8K<+} @fi3*e .54@*\fi;.0/<, 1, ;.bE;ffs +ff;= c $+$!1 ff' +"#: KE;E;XxP$;=ff;)#Xfi#%5^&<E;ffW3b)fiff$!fi b,ff ffX"+c _+Sfi(+%19* b%'<P= <.&%/>+&fiSff+ #5KZb)#Xfib=ff'<E;ff'G)Y,.Sfi =ff%E_+#,#@(#"Pfif/>D>"$5<7e;#@e+/Wfffi:= WN.G]=+!RTK(Uw) KwI+-/G/G:;$ffT#+B$CE= Lff +fi ffZs\ ffX]"+SP"# ;X-B= ffX 5I+fi"#RTQU46 Kws'X-BT9* D$;b'"=>!<+%1P3fi=!"=+K/>."=s%fffiY"+SeP"# $:E;!$"#F=+K= ,"+GMff$"#/Gff$Cfiff+*2-X-Bfi/afifi$K<E;b9* yz5 JX"#fffiCfiGP=ff;+3fi &E;ff?"#"#RTK(Uw) Kwy/G =&'Z"#!fi;E;-=V/GP"b#*$"#.65JI+-"#a<yzM b'DC $!D.fi\ffD$M b3X-SE"#fi-3<#*$"#.,& J= ff>"#/G P$ZffZ+fi= /W"s% "#!fi#b?N.J!E;ff'\?Z92HfiW."#!fiVPV=K4*GTQZ ffP=DZ#4:;$ff;SRffjT ff=ffZZ#4:ff$ q(ff+ .6510=ffX-Kw9GTKM=+fi$K"#fi!fi3#*$"#.+b fffi.3RDxNw"+fi*:fiJffff$b C/>+."P= R*TQ(Uw6 K4,fi#@( $P&fi $6T5 RffjTF:*= +E;$:9=+fi$s/G"C#*$"#.C+"#fffiff "P=+ 5U;WV9}^'<@@z +Zb=A E;, DSfiV"#ff"#fi ?/Gfffi"S9!fi$[5;PT/>B;/G; $"P!_P= + +C#@ ffK+%)$ff >P=+<"#ffCc$3XfiU$CffJ&-xfi .+ff2'#N;ff+ ff- : +5;P"#/G+V0 exK0 b+/, =+/W+T-$:T3ff.:E=ff'"=L .! $>&-/,X+'ff$;E;fiP=fffiWX*D;ff fffi ,%'.+/GE;B45v;= cE_B,f XQ"PfiD!#fi,+.3X-# <+E*[= MBCff$fi>)=>"$QK=<J 2+fiTff fffi G3fi !fi= /"fJ!$?> + FX- .+Gff'+:ff +'ff$?=+M.+K= bffBM/GC$"#.A+%8P= J$!F%'-."P= +"#$:\ ffF= Jff "+."#T;E;#X8/>+BW#N /GffY"P= +"#$5I+fi"#f xKi0=+WG+%TVn beD2-xfiB, $ff.+!fi*:E;E;T+fffi;CffX";= #@fi/WD.Q ff.+B>ffGb+/, =+/G+wG-T#$ff+f"#/Gb $!fffi.;E;-=>P= #fi. * 5X{ ZYnvlrzlnntl7fi.,/GW"Bff 4[,= V xKL#@()fi/Gff.sfE_f .3"P'$C+%fffi "Bff2H."Bfi9fffi/?:K+/G$\[O L +][O L_^ 5][,+fiff ?H.+fi f%'/ qL$ffff$fi +- qP=/,M+%1ff-("PB(_fiD+fi$afif=+c fffi/V5v;= sN.b"P:e5 F5 ` L :fffiff+-$+AfifffiS9"#ffN .+-?fiVEc=ff"=\3X1P= Jfffi "B M+J= >#+fffi>+]"Pfi$+35qv;= ? DS<fiZP=[O L_^ ff-/>M,3'V ffX]V#"B+%= #fi =ffb[*:Y3XK= Gff-("PB(b, J"Pfi$+b= ,.+fffiCfifffi!3XX?fiZP= $, fffi/> t`/GGff-("PB(b"w6_-#WKX#1F' 3'-#b '$K6 '$1+X4b'$K 4<''';-ck $fi yz){ {z {z0{BCmez)zx 0 yz{BCJKL3BSzy yz"z{KJL5BS179fi J [Kv;E;1~Qfi "PB(E;!x?KPfffi/>W\\+%<$"P=Z= 35Jv;= Cfifffi'3Q!.+s%d LZ^ : %'b#@+/Gfffi:)-fffi3xf #Mfffi("PBT?L%Tfffi "B w5 = J[ LZ^ fffi/?J=3fE;VG= f3-.C+%Tfffi "B G#"B$\fiff-3X-:= =V= ,+C Gfiff-31."B ;+%K= CT/WJff-("PB(6579fi JG!= SEM;fiff-31+N3.$4%'E_M#fi$"#$C ff-/>K"#/Gfffi;$"PxNw"+fi9%'= LLZ^ fffi/>K"+C;%'#Ec= , M+ B(cZ#4:4$ b+/, =+/G+!9-$:*$ff.5v= \ exKu#@ !fi/Gff.Cfiff+fi+$]"#/G+!fi Z= fff $yzC%'/?+"#Vu fffi/E= = fff+LEC .$%P/ "#.."=LEfi=fi.C%'/?+"#VE= LP= q+-fi]Z/>SXfi fffi/Ec$\MWXfi .>ff'+*5%e L,gf L +H` LB LZ^ +PJE;G#@e/Gfffi#@fi/WD.6587e;#@e/Gfffi;: ` LB LZ^fiD+fi$K"#/W+fi ,=fi/GM$Rfffi$WJ #+bff+W%!+fifi J=LZ^ %'/"#.+#"=?E;fi=W= bfi/GM$Rfffi$a%!+fifi J= hL_^ ff-/.+Pfi ,E;fi=fGfifiW%'i` L 5cGP=+J= $!>#@-/GD#b-DfiG= f + #+fi\ "#$,fffituP= > fffi/+%b2fi$"#fi G+\+ !+cXfi .Gff+fEF M"#&ff$45/% J 4D+j\bDb* TLjjI$LjRQKkRKZ!fi$,Mfi/afi.+c XK*yzF P$D#+fiC + K"Pfiff#fiGff&fifffi[)=G P$D#+2fi)=3KE;; $"+$6: F &b+= G) F w:#+bE;< !fi/,fi-8"#fig: k F =ec F &c F =e+Vk:c F =- F &GG5 (4xKY!$K=fffi#+."P=ff"3D P$D#+fi*:6fi"Pfi- 2' !fi/,fi-_"#!fi4#@ $&- J"# 2#" .cXfiBf.f fmw:eN.b #+G?ff+V"Pfi$+{w:4P= q .+G?ff+\f"Pfi$+{4:= ]#@($"# fP= VH !fi/,fi-S+9kc F =- F &c F =-q"#-*5zFyzC $ff.+-"#&'.fffib+%wff$P"#fifi)%1= FE_cfi/,fi!fiK"#fi651v;= "PfiD!$1+3-fi,+f=fffi#+."P=ff"3ff/>3- 2H$"PXNw"cBff SEfi$ff cK=-G+%1$+#"= 2H"#fffiff%/>+-*[1+ ffx"+fi 2H ffXfi$% "#!fiM=+JffP/,fi ,E=ff"P=\ (ffafi\= G . =q%<+S8ffMV"#&ffb #@ $:4E="#fi<P,fiff ff"#:fffifE=+;#ff$: = +E $"#-fi;+bPG,"=ff-$4: +V,*5l\*$K3.!M'$.'c$''' MKP<$+;+-oH*$6ff.6+13S3Z9n9'<;-#$*HPF$3f P[4&z8P;.P-#$'6;z:.M$#'$,.6?+m1nasHFq$M$,HP<$ S#$'G'3bP +-;'3' ''WKP<3SP_+-oo*3 #*-# &(wc.F''?'$ '#.3_'3' 'HP-#p!-": '$''H-.?'b3&$3<'.H!1$#> ##_). $ #$M'SP;$| S$am1nGP$#'$]f3 M$#M$'#$MX.c$#!V ##;1'B4q r1s u v m1wYPM'$JP$'$JP<#$J6$#'S#HP' b'+96X.'P-#'_'3<-.b3b$'# L3| S-#$kfi/%jyxyz{z|~}aTQj4b:TQI>)j*TQa;b*XTQjv;= CfDfiM"#$!ff"#?E;] exK*yzs=fffi.+."P=ff"3Kff+\ P$D#+fi\++<y"#ff+4BD +E;fi$ff :,= JR $fi?-/G/G$+P#fi>+ff;,E=+T"#DP+4fiff%/>+XE;Y= ff +ffsfiVP fffi >P= Y#@ !fi/Gff.5Tu"+V#@ fffi+fibff/>Sfi 2HffffffJ"#ff+fiff%'/>fi+G-f= Jff'"#$[5;>ff$"PffC= SE G/>+#"=fe $"#._fiV= WH +fi41xfi .+?ff+V D3fi;= C $"#.Ffi=fi ff-/VyzQfifffi'31+? D34%'/>6:e5;Gff$"PffJE=ff'"=f+P3*ff+fPa"#&ff #@ $:+qe5;Gff$"PffJE=ff'"=f+P+%1= C+'31'-"#/Gfffi+Fff+f,E;BGf #@($5v = bN.cfffi$"#J+%<ff/>Sfi 2HffffffJ"#ff+4fiff%'/>fiWfiff+fi$F= +EO,N= sXfi .ff+?,= b E0 ff-/V: (+( E=ff"P=G-Dfi$<"P= ffD&fi G"##+D#fi?=fi ; ff-/Y! 2- ;%'"#.D.Qfi?= MXfi #+,ff+*5ZJff $GP= J+/GM+x"#>;fb+/, =+/W++Vcfi$[;"P= DD!J= J !fi-f=+/?3@D-/,fiU$;= bff /,M+%Kfi D3*%'/>=+"#3xfi>+ )$+-f= J#+&%'/G$aXfi .Gff+*:)+?fifP= Y"b%Q,!fiJ"= ffDC= J !fi2fiG=+F/>3@fffi/,fiU$F= MD /,)_%*fifffi'3w"#fi-fi?= cfi ; fffi/=_+ )$+Ffi= b.&%/G$WXfi #+Gff*5v;= , fffi/'=Nfi fP=fi/>3/>+ fffi V"+\)aRfffiC#@()&fi[cX%<P= J-fffi//WD-] $"##T+= CXfi .+Pf fffi//Gfffi$"#.:)N- f= G$!/>+ fffi M/>3Mfiff+fiK#@e+/afifffi b3X{z<|)D&-ffXXfiX$5v;= _3fi &)fi? M+ B 1Z#'4:D$ff/GP.+$;P= Y)ff31"#D!T+%</?+ fffi ?&fi G= ,#@+/GfffiC+%<+fifi G= f L ff-/ff&fi >!""#$&fi#-G ;X- .+Gff'+5<v;= ,"#/Gfffi#@fffiG+%<"#/G !fi G= b !fi/>3*/>+ ff-+ETs#@() D!3Xfi\E;-=Z= f&fiU?+%T= WXfi #+\ff+]VP= >)+fiffYEc= !+fifi V= f Lfffi/&fi ZV!+fifiZV#@"#-qP= /GG ff-/ JAXfi .VffZC"#3xfiV/G#@()&fiC=+&fi >?/>3xfiFXfi .+P>ffZHfP= =f-$Rfffi$; f + .fiV+M3XH.5ZG PG=+b=ff'cJ&-/,X+Mf=MG365 Y5X36E 8<ffAY;@ ff$$\ffZ~AfiDPfiZ= >"#D#@ b+%~;0~f-D*:K$ff#5,^H !$R ff,#@()fi/Gff.bE;,$]?=!"+:1ff/?3fi 2Hffffff$:Xfi $+P2'fi/GM/>+ fffi ?3fi !fi= /V: ff$"#fi)$?fi\ M+ B OZ#4:9$ff#5"#ff+;+x"#%,= f$"#]ff$"P&-P$RfffiP$J=ffX%'fi A%P/ $ff= 2-N.G$+#"=]f)$2-N.GP.+ 5v;= G- JG"#!$J Y#+ B- f%' "#fififf.SX 5Vv9Z"# 2+<ff$"P&-M+%_P= G=fffi.,&E=+ /w$EO-qP= >"# ffbff+\V ffP$M #@(6E_, ffX-J$+."P= 2H"#ff+= !"F=<$ffSXfiMfi/Gff-/GDP$Gb)+X"#,+%<# ffXG#"B 9%'/P= P2/r *5z0ZG"PBD +E;-$ff J=T= G-fi+%_ff/>3-q)$"PXNw",=!"M"#/GffX"$P="#/G+'?E;V+<yzF!%P/>+"#J+?=_%< XK*: cE_b+ b=+;=ff;fi!fi,P%3-.,$"? exK\$=!"Mfiff%'/?+fi?-.#X%5^& xK*yzM",= Yff/>3fi\2"PXNw"Bff +E;fi$ff cDBJ= c%/+%1C<+%*.BD2'P$ff"#fi>P"= />.,F=+ff+B,d~\" oM/W$:$+ 4.+P= K=+G. Bfi M%' "#fi:+ <)=> /?=!""#ff(Bff +E;fi$ff 5 2%P +#fi:efic$+-?fi/Gff&fifffiCV!$P= a"#$)ff"#GE;Z= ,E;,%'/>c+%ff/>3fifBff +E;fi$ff :e #xfi/,fi+P>#@-/GD#:ff%'#@+/Gfffisfi\P ~;+PTZ#'4:*$+4.:= +Ed=T.BD2'P$ff"#fiV"P= />+#>"+\ +ffJff'+ !$ff Z=QT&- ffXNw"+ff;=+; #3fi $?ffFyz;.+ Bfi ,%' "#fi5#.KP<3SP;SP-QP$T9$6;!!z'F'$S}.~ ~5u ;$'#WF4&zff3*W,'$cz'H'3'.*P$,6'$''MSfi6HSP Wo\+H'.D > &Gfi6H+ + 1N. &3B9 '3D >N# 4X#wff $P;#k $fi yz){ {z {z0{KPfffi/q ~cIq c~Gc~Gc~c~Gc~Gc~c~c~c~Gc~c~c~q ~cIc~> c~G~cI~cI~cI~cI~cI~I~cI~cI~cI~cI~I~I~I$~I$~I$ ~cImez)zx 0 yz{KP("+5K!fi/GCH/>$"3xK55)55 qe5zq 5e5z55z5x$e5z55 q$ 5$e5z5xq e5z$ 5x+)5z555e5Gq 5qe55q qe5zD5x)55qe5X5zy yz"z{I)$ff \w"# 5+xKG%qq+%D@ff@%ffeqe@+%ffee@+%3qffe(ev+fffi,[;</G+.+!fi)%/>+"#:1+\ xKXfiffA .r4rzl omnV nzvil vv= ,N.C= f"#+fi /Gs+%Tv1+ff-V>= +E= SE+<yzs%'/?+"#"#/G+$sZ xK*yzfi\++fi Pb/>5 (+$ Z,"+fif$ff.M+ D3-_fi >P=ffFfiff%'/>fi?>ff.3E+ff> ff"#"Pfi&-<+) K= M#+!fi+_/Wfi.+%*= c E;b+ D"= $[= c E;M #+/>E;E!fifiuX*ffC+ + $:K LuxPD,/>"P=fffi $:;+] #fiP= GECfi/,-U$]\ (ff"#= J)$MDfifffib.3E0%'/?+"#Jff /,.5 (/ c = #fi$P;E;J bP=+= ,++fi Pbfi/G/,).TJ"#/G+#+fffi[<]ff$AGE;BW%;q!/>3Xfi; fffi/>:4 xKV)M+ _ Pfffi/>:ff M= J +.WffG $;=+#-= .+/;"Pfi$+- )fi35b+/, =+/W+8+\cfiJ!$b xK*yzM!%P/>+"#JP#+fibG-.+E=3ficfi.+!fi ,ff+Q%P/"#.+."P=*5<v;=ff'<ff /,$:)"3Xfi$W= BW0 ?5 2GFWL8:;><P7;2\360F ;3:;ff#N $f,J3; :4E= WC'= ,-/GJ$Rfffi$Af+fi,? fffi/:5 )5 ^ L_^ :e%P/P"#.+."P=q+mC'= bfi/W$Rfffi$?G+-=+/GT Pfffi/&fi G,/>3xfiFXfi .+PYff'+*:ff5 5 M%%` L 5v;= M%' =V+N %P=V"#+fi /G;%Qv+fffiGb"#/GJ+L+ XKf=ff;/W"+5v;= ?R $!fi]= #%'>+!$JbE=D exK*yzC$ff /,#Y>"#&'fffiq!/,"=C+Pfi,/>+ ff-ff;=+>Fyz:++!"#ff+Xba+ ff-/>:+GP= =>+!+fi%'/?+"#G'b G&fi ffXNw"+fffiV)$5\v;= f+E;J=JPqffVEfi=]= > />6y*#fi%'/?+"#ffiLff'+ fffi %/ P"#.+."P=*5 ?79fi VZff/W.$:c exK*yzW%'/?+"#ff .ff$c/,"=%Hc=+q<yzcV #+fib#B 5ZC=3J ?#@(ff'++fi?%C XQy=3fi$: cfi.#$"#M= Y3fi D2')."#ff.+ bff /,cT"P-$+$[K= $!Jff /Y).b+b=ff- =$"+!V xK*yzC%'/>"#> .+-G.!B(bff #ff$C/Y"P=Z/G>Rff"Bfi\=+Lffff$)*D+ -X#'3 F$FS'9X.Z!-G3SP*KP<$+;+-o9$6D.66 -. &4[9ff3 Wo9'$;;.3)>l3B91PGffPzX+[o_>;11>_>33L :&&1h>h&3O :3>:>F@Q&1>'O3fiO;;ff180000016000001400000CPU Msec12000001000000800000SPAPRIAR60000040000020000004681012Problem size9;h;O;3 F 3&T"F1BQ ; >3i<&;<9;9>F Fi1Fi13; {"F3hW W;F9 <BO1B"39F3<Bi;SQ;QFp;;9 ;FL-;19;39WJB< ( =TQ;%F9pBJ<""- o;Q;%F;<;b+ B5<T;;& ; >3;;ZQB1>">;=L;FO+;39Wp9O TO<&h3<hF9<>;F";JSF3hF3J1 Wa ";399 0F39;<a3 F3;3h1B9h3F <;F; F9F1h; 1F09WW3;39;S<9fffi;F;;<9+ 3{ 3O3 F1T1F; <>SiH9;"9T3;3 G<B9;S%; J ; 1%<B"FL&SF3%>"%5W 1;5%;h _19 ;O"WB3F9<>;1>1SF3S";;39;p;;h;F< 9;W ; >9WF<B3"!#!$&%'(;O3{p5&{ ip3><B3Tp3 T3&O1B9F3<9S+ 9;>bFF;< W)399;J FG1WFS;3;3 -F 3&JF1WJFhO;W9SBp;=L';QF>3{ 3%Oi;%FT1 19W;<9 W53;><9S;QF%;\TQ1F> <; O&J <B"*FH3;3 J3B1B3F3<9F 3O,+W;;<95;W F1W+213)4;<9;SFhF 3O 9;>GF9;;;h;<9 J.-/]F0i"FG;W39Wp <9H3S;{;<Bh;;{ F;{W<9F";LF;O<TFh;;<9 J)57<S91hJF3B1<&39<9FW;SH9S <B"W;<9 '399;1 F3526< SOF ;3W<9>TG;h1 F;G31F 3OG< WF31F;F 3&JF1h;G3O3 TS%p;<91WF><T+\F39; TW"F9T;p1B3%009;L1F9;F8 :9 <;>= ? A@CB3;1 >1FE HG HGJ 7JLO330 3&{1 FO& &3h&O&:>& :3FJ ffEKff31':>SO&3>&3&:>p{ ff&0&3 :&0 &h1h&:( ' %@i 33fiMONQPR TVU XW4&Y [Z P]\ SY^ R`_[P\Ca&;F ;F;dcbSF3hF1T5<B3W ;3995 ; 39;J;33<9>; ;B<SW;19S9F"TB;"39{" -99;Fpp;FW bYS P;+Q9;;OSF19"F39;h TFH1W_(<9 >><B F;;<;'1B;O1;F 39 5>O %1F<F1pQ<&%FT9W<9h;<9<T9<%<&9OSFOF>1 39 ;h9 hp;<9O<BJF9;WS9;"9i<;SF +FS&;; 9FOWF">1 3 i;1; 5OS%<&9;1J<BHO<BJFJ3<9><BF3p9H399;SB" O9;;;h;<95G1F39;J;<;b+ >%3&JO9T;p3{S<B"F=;;ObFW; 1&1F1 <>\O J;G%;"9T;<99 h5;W<&JB{9;F9gc;<9*e5;%9;>9SB"{F&{";F>{9*ih< <9;0QL9139|{~}S;FWFF3&1<Bp9>SFGh;G ; >39T<B";;< F F< Z9hF" B h+W >1ZS;FF;;;>*ihSFb<G 53;;SO];FL9; >5FJ;i;5f6jXj /< pLF S+ 3{ 9;>"T<&kglnmo&prq&s out`vxwy z4b/Q<B+ ]F;F&339h<TF;h;j j /Q F9 Li9<3;B<J9;><BFJF]F &3aFS;T"WW F&59;3 <>39O;WT09 F1WF 3O H;W3Oa";;5;QB;W;F< 9;0* j j b/<B;<9;G093;% \F3h1F3>9<&F<339;W;< ;TSBQ: g| *cLOW;FJ5j X/ZH %93O53 <<9F;T-* j /; 1%; ;FLF3QS0B<>5S< >><B <B;;*;FL-<BH ; 5>"39ZF<F%;W39;TOFhJ<9;<B; G"W1W<& hF -O; %{; % 535+ffB;"39F:/1B3F3<9FF 3O;<;O<Bp>; 1 &1=<9 \;9 L1F93;1F939Fh;F<<5O1ff _ 5V650><F139S 1B Q&;%<BWF ;3 51h< >><B ff<1F539|F>F1;09;O9Fi <> "9Z& F99;iB;"399;<>3<BO ;3 <>39TOG -<B ; 5>"391FB39;S F9;OF3 ;&1<1F319<>h* j j5;FL-W%1<B p<B";<9;FSB;9bF1;OT"9LF>< Lh 09F";b093;&T09}S;FWFFQ F9FW;9<BTB;F 3p91W<9 ;j/;%;0W3;F<;Q3<>O%B9FL;&>W OS1 >%F<9<9JW;ff 3FBS<B"FSFW<13F<9O9<9];G139 ; 5>"3926<139+9<B%F"FhBQ;9"h;W1=1h1B3F3<9F 3O <B;F 3"F;i1W< ;O;F 3n * }O;OOhW;";>39B 3 &F3p<B;<;hFT9<hF T1F1 ;SO9pFF9;5;S3>" 9_B;<99;x;;>3F 139%O%B1F3j j /iWFZ;9< >"F9<19;J9Q9 J%% -h;;>3G;3S939 S9F1T" F3;J F&5;3 <>3a"FG ;;Q ;3 <i9S<91; FO139Fi139F9J WF>1; <&{LO <9h<9L1FB< h}(3 5 p F93> 9 Q{ O1FB<9L9< 39;hF<9;;3>F&J39F "WFh F1;<9W<L19; ;3OF; ];9S930j j /1391;<9* j j /JB3B1F195g * cffj / C | *Q90B3b<139FQhWF<B3ff0%{19<3{"F;1<W%<B 3W WpB1; F<<;39;399;F=9F9F;W9"<9;J;< ;Q&O9;9;J;S9S&39TW 3BhFfiO;;ff9;1 39 LO;3O&3;F%F 1W< 19p9;p9W;O< 19; 5 <;;9BO+5<9hF%; a1<&JFb;<S;i3 G* j j / cn}W% 3 &F3<B;<9;hOZ&L1="W<9O5WWFF* j /Q9 T"u1BT;<iF39F%<BG;p<B;; -Q<T9W;<1B9F1W<9 F OF1*1H%;33;3 Tp ; >"T<BF9;HG;1p 39 "F];";>397bfff;<;S<Bu/&O11;3B;;;SW<1LF113T;&1HFQ9O<B3F 1 >Z;3W3;3 JO9G;p3FQ%<ff"F3;T3;3<5p;hJW;WSB31W5T oB<3 F<BpF9 ;9;;< G{9F19J;<3<9\;;<9;<p;h'9;h9W W;<&&&39W<& ; 139 ;W;HL11;3GFJ %1 O;&<B";;*VQ{4"B< L19<F3W<B";<9;W&> {a9SB%T -ff< 1<9;T ;3 <>9GF99;; >9%<B";<9;h9;%;3 F1h"ffF1W<9 9<BT3 ;%F%F"3B+9<&T99OFB%11<;O4W 59;9\13]Fh;5BQ9<BjXj /1BZZ&&+<B";; {33ff3;;<B9;;J 3;3T3 %FW<3LOh;;39O<B";<9;F;95 5BFLQ&T<&>W O"HFWFHF9< W 3 &F3c<B;; >J;F< >31FH;;<9<T09F hh9<9 W <h3;3;<939 > GO;J B919GF;;99H;F 1 9HF3&1<B>{;FFL +{Fh139<J9;'< F F< <T & 399393;TOF; ;B33B1<3>4&gl.xwp[rwAJFT; 5;a"393;GL\;;<*3 &F3<&;<9;GF3;B<pFO;;";>39T; F<B;<9;W <&3</9;>"S<BF B9Tp;;1O;;;B19p >139;"BS<B;<;<&9FOFJ;9;p;9>< QWTi;9;><BG31 G3i;;<O;S93;3;3 TB9'F\;Bi9W<9F"FJF%<&1h&T;F 3 {;F<; Z1W<9 ; ZF'&3WTi9"i;WB39F139GF 3O<T;'< F F< <(<T9;9F< + F< ;9393;Tc;<& 39\"<J9;'< F F< <G1<5;O9<<B9p&h3F<9;;S;<9F 9;;FJ1F3>9<>"%FFZhTF9SBW"1F119 <" 3 &F3<B;<9;J9H;WF>1hFBF9<19;3> L&%<& F9;h1;F 3W 513<91%31<B3;H3;3 J3B1B3F3<9F 3O ]1W;;>"39F0L <9W"FS;%1; 5h%<BTQ;>p<BW b<;O9;;;;<9 G<FW&31WFp;L3;3 G(+ 3BTF1OhF"F;9SB"L;FL;3 G-F 3&T"F1B F1;>"9;FZ;;OS;Fh;S3<>h3;<BFS9< ; O9<9H;W1<1;h;9W<FJ <B"%;<9 JL%<BG; JO 1F<F1ZffO.7b:%X!$&"% % &Q$& u%b b"!4g}%3;<Bp1WW <L;39 FhS;9<B 3F_ff;L9393;G-ffBT5;F 3i9;;F 39_91<9 ;LFp3;3 TB9'h<<;;13iT"{S939;o<Z;<<;SF3 F1W;3;F 3%;139Jb"&> {Bp<B;<9;HSF3S<93T19H+W3;;<9 S;B<10 >%"9 i"F39093;FZhO;S09;p9>S<9'H3S9< 39;95 339;S<TW ip;<9 J"3F<9;F<; h1W<9 ; hFo3;3 TB9'"H;19; ;1";3 a9 <%S;<B 139<B;;fiMONQPR TVU XW}"4;p<B;<9;J99;T <;O <%<BoFB9i"F;W bYS P;u ff &QO <;;F 9%<J;19GFO0BO FW9F154&Y [Z P]\ SY^ R`_[P\Ca&;F ;F:WF%T<% 1399J>%1<;T<ToFZB_;L3 <3B; F1LF 3O ;Q;FLSF;{" SB1W O; W;J5F9;B%99;4H;B<p9F9<%9<%F"hT9h 139W3;OF;->"FBT3G> "9%1WS;b15i{"9F1<9 b093;;;;h;F< >3>"F9;%FpF>";T3>]3F1W; 1F9F<T ;19FF b;F< 13>FH;;139=T0;F 3 ]0B3G1Sa1<W 9F=139F 3&T"F11V (];; F"FZ9B59F99F15hTJ3;JFW<&;';;>"39G39;3Bp939;B<9"39HiG<91<9W;F 3GFhBO<&\;&W1FW<9F3;3 T"3B3HB;1&<5;1;9FOb Q9%F>"5 pFhT3 W1OL;1;9S hF3&<BF0|:b%):%[G3 T3B'S95FW5G3;<9 W <BT9393;G;;OL;i"WO39W9L;B<LW FF9<LhF1;(0;iOW>BJ5c;339FF%3;i 5;{B&;;h>Q9;;T; T<W;<{B%<J9;'< F F< <Q9<BT39;%;;>"39n0093;;OhF;h3 39F9J1<9;}Q15 < 5JBF93W";b3">;'159< 3'1F&o; ;3B33& J]<B<];;139:9WH0B339<T9(*;LF&ZF>"FBT30F05<B;<9;h3;3S9W<9 5;b";;5: *>F&T"'Fff<&;; >;%9< <&S+ <B9FJ<ff ff((91&%T1FB<*; F&5S19J ;3 <>3&<iWL1;;9B;F"*i;9; >139H: /ff ff >V &EV &; JW1<B5;QB3;hJJFS<9h ; > h;rX;> JfJQ3ZO5313>ffEL*;F9;B'1b/319; BOJ9< 39;9\<&SFF Bp9>S< + F; F1| *i'{<BJFp3FB0995 339;SS1; F;&1FW<03W <9;Q9L{O45+ <093;j j /1&ZO;S<B";<9;T13JB95311BJF{<94%;4;i;;<9 J9\;G;;5<;<9WF%FOTOS5;J<BS 39 ;FJ%F"&O;hF%WpF%FO<BJ&O;h<BG;";>;;iSFJB;-%>F&'hO<BH&3T+S9<<;9;S;i<&;& 3 0Z;1%F;i<&;;;>3S;1F;OF O1Z1<Z<_ 3 &F3<B;; >j /)1W;;>3F+ ;1F39;GGF B;WJ35+T"Q"F9;Bh4=B;19S9F" ;F<19F; WF< S;bQ<BJ39F0 F91%;B<S;S3n /13jX /<F<95*>+h;SB";FSF3H</+Hb5<;B1399 >J;< 3 &FG<9;WF<ffE0O09<11>&3&&11&&0 &&311&O&3&FE- O3& 331L>3>OO>h& 3O3[G11%3>3% &&3 @ &0 @3ffO&& >33'&@+&0F& 33 : Z1&'&33fiO;;ffF>W*4< B1F9;&<Z&y3 <<FSBj j /j j9T{4Q 91BZ1zgAw]vpoutAouousO<&3>S%9W;p<WB1FFO9W<bO(h19<3F%c"b;F"WF3x<B3>"F 9;3B"39;WF ;;h;;<F9;G9W;3;3 GhBF& TOF";SBS9x9 > O%;595 QB1;33FSW;\;nOp&;F<S9F<j Xjj' 0;+F;3 5139(H';<F<S><B|a1p;g0>"F;]3WSO;T1;<Z9<&;H5G;1FHOG;S W<3Bi"39FZ59 F1=;F;39' 5;5<<ffW <B3%;;F;a";5<WF<BF+ G3;F3><3B09><>O3>jXi jQj j,'><>;;<B;"' WFi;G1+>9moVo&yXo&Aouhc}{c * / * j j /bQ. u&&uAcc* j / c<Afi u>ffuf [ jgc A{4 * jXj /Hfff&ff>f * b/c {4 & * j j :/fig[ fiffH!"$#&%'#)('+*,+-!.+/#&0212fi fiX "c* j / n0*x/ fif3fi 5 4A>, fuff& fffi u 6&87fXfu> :9fi &;+<fi >=g?=* j X/@Hfff>X" fuffFX& * / [{* j / Bf&fffi ff>fiXffCfi fff> &ffFX c4.&&,cb * j / bcnfiffX>bEfi Ffuff& fffi &X"fi uVdfiXH G V&ff> u.fi>I Gh( { 4 * j j ]/ (g figb fi ff X AAfi "fi fi Jh * j Xj/ nbK <)LQbf. &u> MCN"+ Og&u> eb B 6+ P)fi QGSR&bQ ; <c}h * j j / Q}$HfffuffFX& >TUbQ9 5= Q F9 <;5H FQ9 h; >b1BZ51BZ{+;1;1<;>&_&9;BOQFF" FWF>WF< O9;F19<1&Zi"WWFZ< Bi"WWFZ5;B;3F1;T;<9;i";;5F9JF W <9<B;<;F>FW >;;&<B;<9;QB1 ;F19O5&;<B<9;0F39;HF<9<9 9 F1" >L35'<1WF5939JBS<BF">9;FH W9F 55;Z 1Z %< >93S390"39F{"F99;<9 g3"9;GF<;O9<W <B39BFB3 h%<BJ;> 5 ff;0&<9hBSLF<&ff<3& %1< W<B;<9;+F;T">> h;;39h<B;<9;F<9F>%W<9+ F1O;;SW<B9 F1 F3'>0h0 ;1;<B9<9;JF F93;W<BFQFQ0VfiMONQPR TVU XW};FF&Y [Z P]\ SY^ R`_[P\Ca&;F ;F{OBZ %;}9W <>39;FJ;<9G"&{4;ZFhW 5Q& * j j /W bYS P;;WX6X&ZT|c>;3 T3&<BG;;1)j j<B;<;FG;<B 0 F;; h59 F1SFF;J; ;3F; F15W1;F<i><91;* jXj /:1BZ SZF3 &F3>&T0B;F;39J&[=Q<9 193HQ4]3<9;" F"' "'F;9; 39;+* /* j j / (\=b# fifVX>bxfi3^]DW3<^_Xh* j j / c{ }`HfffuffFX UU j [h* j j / ch * / ufi2}&af& gAfi K + "fi fi JK FX&&b Gff+ Pb Xj c<cc<{cg * jXj /nHQ e9fi &;fifi ff&ufi Xff>g Xfi nfffi &u>hA:>hX&fi &fffihjc{* j j b/g ifi_[ fi ffHfi fiXb Xj* j /fi2 / fi ffH AAfi K + "fi fi JKb j& &{4 & * j j / gj9fi ^;b>k ffHfi ffEfi g8?5$4 & GPQfib> P fi : dHFXg fi"P~ GfX:>ub * jX /*"fi PffXff>fi u fuffFX& /*{ 4 * j j / uc/ Q 7fiVbdEfi g ?85)" _"_lA,b GPfiPfi f4fi u fffi dEfi 5ff"fi P) ff&u> MmRxFf> :ifi uk L Gbfi &{~4 * j j / W=H ( :ccHW c fi2 nAXi:fu QAfi fi C&n3fi aod&"fi :i &ff fffi # &,f<pb !q#&%'r# (>'*s + -!.q+ #&0"1fi u>cb [{* j j /efi2X[ fi ff X AAfi fi C J j [ j j* jX / tduGbu" P)>5 6+ P)fi QGMnvRfi QG)3fi + P)&) uF 7fXfu>dAfi PQ:ffVX&fiu=& &{c& * j X/ g:Q hg ]w h xQfiX2Q hg &z& Vff u>* j / HcH fi2 X \ ffHfi fiX &j bj jS;FWF"3F>o;G;393]"b3 T3&9'ZQF< >31F9;\;><F 3O \<;F;F1F\1S9W <p9\FB9&< S<B;<;F'<; ff =S;FWF"3+FQ F9 F=>00B;3aF1;hF3J; JL<BW;<ff 39aFJ F3< <9 F;>iB9;p<93'B0 ;9G 3 &FJ<B;<9;FiWFZ; ff 5;;WWFZ>W">HZ >>;F 9W <>%59 F1JB;<9;+':;ff F 5H %3<BJ53 <<9; ;3 TB;<9;"{<&;<9;F@<;F ;95F1 %F<39>"39<9>1F1 <9;p;O;393ff1;<BF"39;&F9<9;F@<;F" =%1BZ1';;0"39S<9H%3;3 T"3Bh31'<F<9 %><<5;919;<BFF"1<>9S139F_O9S151'< + F< 5 1>5+ <;;+FF >1BZ S;F>; ; 0;;;;5J F>0ff<B;<;;19S9F<; Z<5+ <;L1BZ>3;FZ1W<9 F3&>< h<&;; h&'; ff F 00B<9hBSLW5O;"& W00=3> +BLF3B09&>< <&;<9;F ';+ 55F";ff hB<>{S;3&<5F";ffF1&3 h9WWF hQ<9 193>; G%< ;;9;<BFhF9< ; >39F@<; ";{'L39F";GfiO;;ff:c * j X/@H 7fiU[ |f& D]fiX>uHAfi K+ Afi |FXj* j j /{gct} Hcc 0:=6X&~7fXfu> :j[ [" * jXj / k7f &,GHu fifffi u>xQ& fi+P:fi2u=c|= j b[ * j j /:cfig[ fi ffH Afi fi fi F j* j j / :Cfi2 [ fi ff X AAfi "fi fi Jj[} <{H * j j /j>; 139;; 1h; 3O;Z;0=195F{+;1;<B99;>;F195F195F<9;FffF;T9;Q<9 >93 0&<9O;<B 0_ FO3;% % F" >91<9<9O3> %<9;F F9B 5 <BiF;<9 y39;Q <B3;<5;+ ;&J 5iF09J9;19'4909F;B {"; 91%99> '< >139;&FS9< <39'<;F&3;F >"> ;<B=%<9 >93Z>F9<;;1< &1BSB9'<&F3o 39 bF% "; F 5%"+F>{;9F9<F3B0 F3WT1W;; h59 F1F< <9fiJournal Artificial Intelligence Research 2 (1994) 227-262Submitted 10/94; published 12/94Total-Order Partial-Order Planning:Comparative AnalysisSteven MintonJohn BresinaMark DrummondRecom TechnologiesNASA Ames Research Center, Mail Stop: 269-2Moffett Field, CA 94035 USAminton@ptolemy.arc.nasa.govbresina@ptolemy.arc.nasa.govmed@ptolemy.arc.nasa.govAbstractmany years, intuitions underlying partial-order planning largely takengranted. past years renewed interest fundamentalprinciples underlying paradigm. paper, present rigorous comparativeanalysis partial-order total-order planning focusing two specific plannersdirectly compared. show subtle assumptions underlywide-spread intuitions regarding supposed eciency partial-order planning.instance, superiority partial-order planning depend critically upon searchstrategy structure search space. Understanding underlying assumptionscrucial constructing ecient planners.1. Introductionmany years, superiority partial-order planners total-order plannerstacitly assumed planning community. Originally, partial-order planning introduced Sacerdoti (1975) way improve planning eciency avoiding \prematurecommitments particular order achieving subgoals". utility partial-orderplanning demonstrated anecdotally showing planner could ecientlysolve blocksworld examples, well-known \Sussman anomaly".Since partial-order planning intuitively seems like good idea, little attentiondevoted analyzing utility, least recently (Minton, Bresina, & Drummond,1991a; Barrett & Weld, 1994; Kambhampati, 1994c). However, one looks closelyissues involved, number questions arise. example, advantages partialorder planning hold regardless search strategy used? advantages holdplanning language expressive reasoning partially ordered plans intractable(e.g., language allows conditional effects)?work (Minton et al., 1991a, 1992) shown situation much interesting might expected. found \unstated assumptions"underlying supposed eciency partial-order planning. instance, superioritypartial-order planning depend critically upon search strategy search heuristicsemployed.paper summarizes observations regarding partial-order total-order planning. begin considering simple total-order planner closely related partialorder planner establishing mapping search spaces. examinec 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiMinton, Bresina, & Drummondrelative sizes search spaces, demonstrating partial-order plannerfundamental advantage size search space always less equaltotal-order planner. However, advantage necessarily translateeciency gain; depends type search strategy used. example,describe domain partial order planner ecient total orderplanner depth-first search used, eciency gain lost iterativesampling strategy used.also show partial-order planners second, independent advantagecertain types operator ordering heuristics employed. \heuristic advantage"underlies Sacerdoti's anecdotal examples explaining least-commitment works. However,blocksworld experiments, second advantage relatively unimportant comparedadvantage derived reduction search space size.Finally, look results extend partial-order planners general.describe advantages partial-order planning preserved even highly expressive languages used. also show advantages necessarily holdpartial-order planners, depend critically construction planning space.2. BackgroundPlanning characterized search space possible plans. total-orderplanner searches space totally ordered plans; partial-order planner definedanalogously. use terms, rather terms \linear" \nonlinear",latter overloaded. example, authors used term \nonlinear"focusing issue goal ordering. is, \linear" planners, solvingconjunctive goal, require subgoals one conjunct achieved subgoalsothers; hence, planners arbitrarily interleave subgoals often called \nonlinear".version linear/nonlinear distinction different partial-order/totalorder distinction investigated here. former distinction impacts planner completeness,whereas total-order/partial-order distinction orthogonal issue (Drummond &Currie, 1989; Minton et al., 1991a).total-order/partial-order distinction also kept separate distinction \world-based planners" \plan-based planners". distinction onemodeling: world-based planner, search state corresponds stateworld plan-based planner, search state corresponds plan. totalorder planners commonly associated world-based planners, Strips, severalwell-known total-order planners plan-based, Waldinger's regression planner (Waldinger, 1975), Interplan (Tate, 1974) Warplan (Warren, 1974). Similarly,partial-order planners commonly plan-based, possible world-basedpartial-order planner (Godefroid & Kabanza, 1991). paper, focus solelytotal-order/partial-order distinction order avoid complicating analysis.claim significant difference partial-order total-order planners planning eciency. might argued partial-order planning preferablepartially ordered plan exibly executed. However, execution exibility also achieved total-order planner post-processing step removesunnecessary orderings totally ordered solution plan yield partial order (Back228fiTotal-Order Partial-Order Planningstrom, 1993; Veloso, Perez, & Carbonell, 1990; Regnier & Fade, 1991). polynomialtime complexity post-processing negligible compared search time plangeneration.1 Hence, believe execution exibility is, best, weak justificationsupposed superiority partial-order planning.following sections, analyze relative eciency partial-order totalorder planning considering total-order planner partial-order plannerdirectly compared. Elucidating key differences planning algorithmsreveals important principles general relevance.3. Terminologyplan consists ordered set steps, step unique operator instance.Plans totally ordered, case every step ordered respect everystep, partially ordered, case steps unordered respect other.assume library operators available, operator preconditions,deleted conditions, added conditions. conditions must nonnegated propositions, adopt common convention deleted condition precondition.Later paper show results extended expressive languages,simple language sucient establish essence argument.linearization partially ordered plan total order plan's stepsconsistent existing partial order. totally ordered plan, precondition planstep true added earlier step deleted intervening step.partially ordered plan, step's precondition possibly true exists linearizationtrue, step's precondition necessarily true true linearizations.step's precondition necessarily false possibly true.state consists set propositions. planning problem defined initialstate set goals, goal proposition. convenience, representproblem two-step initial plan, propositions true initial stateadded first step, goal propositions preconditions finalstep. planning process starts initial plan searches spacepossible plans. successful search terminates solution plan, i.e., plansteps' preconditions necessarily true. search space characterized tree,node corresponds plan arc corresponds plan transformation.transformation incrementally extends (i.e., refines) plan adding additional stepsorderings. Thus, leaf search tree corresponds either solution plandead-end, intermediate node corresponds unfinished planextended.1. Backstrom (1993) formalizes problem removing unnecessary orderings order produce \leastconstrained" plan. shows problem polynomial one defines least-constrained planplan orderings removed without impacting correctness plan. Backstromalso shows problem finding plan fewest orderings given operator setmuch harder problem; NP-hard.229fiMinton, Bresina, & DrummondTO(P; G)1. Termination check: G empty, report success return solution plan P.2. Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.3. Operator selection: Let Oadd operator library adds c. Oadd ,terminate report failure. Choice point: operators must considered completeness.4. Ordering selection: Let Odel last deleter c. Insert Oadd somewhere OdelOneed , call resulting plan P . Choice point: positions must considered completeness.5. Goal updating: Let G set preconditions P true.6. Recursive invocation: TO(P ; G ).00000Figure 1: planning algorithmPlan PdelBOneedF+adddelOaddOneed FBdelOaddBdelOneed FBOaddOneed FFigure 2: extends plan: Adding Oadd plan P generates three alternatives.4. Tale Two Plannerssection define two simple planning algorithms. first algorithm, shownFigure 1, to, total-order planner motivated Waldinger's regression planner(Waldinger, 1975), Interplan (Tate, 1974), Warplan (Waldinger, 1975). purposecharacterize search space planning algorithm, pseudo-codeFigure 1 accomplishes defining nondeterministic procedure enumeratespossible plans. (If plans enumerated breadth-first search, algorithmspresented section provably complete, shown Appendix A.)230fiTotal-Order Partial-Order Planningaccepts unfinished plan, P , goal set, G, containing preconditionscurrently true. algorithm terminates successfully returns totally orderedsolution plan. Note two choice points procedure: operator selectionordering selection. procedure need consider alternative goal choices.purposes, function select-goal deterministic function selectsmember G.used Step 4, last deleter precondition c step Oneed definedfollows. Step Odel last deleter c Odel deletes c, Odel Oneed ,deleter c Odel Oneed . case step Oneed deletes c,first step considered last deleter.Figure 2 illustrates to's plan extension process. example assumes stepsB add delete c. three possible insertion points Oadd plan P ,yielding alternative extension.second planner ua, partial-order planner, shown Figure 3. ua similaruses procedures goal selection operator selection; however,procedure ordering selection different. Step 4 ua inserts orderings,\interacting" steps ordered. Specifically, say two steps interactunordered respect either:one step precondition added deleted step,one step adds condition deleted step.significant difference ua lies Step 4: orders new steprespect others, whereas ua adds orderings eliminate interactions.sense ua less committed to.Figure 4 illustrates ua's plan extension process. Figure 2, assume stepsB add delete c; however, step Oadd interact respectcondition. interaction yields two alternative plan extensions: one Oaddordered one Oadd ordered A.Since ua orders steps interact, plans generated specialproperty: precondition plan either necessarily true necessarily false.call plans unambiguous. property yields tight correspondence twoplanners' search spaces. Suppose ua given unambiguous plan U givenplan , linearization U . Let us consider relationshipway ua extends U extends . Note two plannersset goals since, definition, goal U precondition necessarilyfalse, precondition necessarily false false every linearization.Since two plans set goals since planners use goalselection method, algorithms pick goal; therefore, Oneed both.Similarly, algorithms consider library operators achieve goal. Sincelinearization U , Oneed plans, algorithms findlast deleter well.2 adds step plan, orders new step respect2. unique last deleter U . follows requirement operatorlanguage, deleted conditions must subset preconditions. two unordered steps deletecondition, condition must also precondition operators. Hence, twosteps interact ordered ua.231fiMinton, Bresina, & DrummondUA(P; G)1. Termination check: G empty, report success return solution plan P.2. Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.3. Operator selection: Let Oadd operator library adds c. Oadd ,terminate report failure. Choice point: operators must considered completeness.4. Ordering selection: Let Odel last deleter c. Order Oadd Odel Oneed.Repeat interactions:Select step Oint interacts Oadd .Order Oint either Oadd .Choice point: orderings must considered completeness.Let P resulting plan.5. Goal updating: Let G set preconditions P necessarily false.6. Recursive invocation: UA(P ; G ).00000Figure 3: ua planning algorithmPlan PFneeddelB+addaddadddelneedFdelneedFBBFigure 4: ua extends plan: Adding Oadd plan P generates two alternatives.example assumes Oadd interacts step A.232fiTotal-Order Partial-Order Planningexisting steps. ua adds step plan, orders new step respectinteracting steps. ua considers possible combinations orderings eliminateinteractions; hence, plan produced to, ua produces corresponding planless-ordered equivalent.following sections exploit tight correspondence search spacesua to. next section analyze relative sizes two planners' searchspaces, later compare number plans actually generated different searchstrategies.5. Search Space Comparisonsearch space ua characterized tree plans. rootnode tree corresponds top-level invocation algorithm, remainingnodes correspond recursive invocation algorithm. Note generatingplan, algorithms make operator ordering choices, different setchoices corresponds single branch search tree.denote search tree treeTO and, similarly, search tree uatreeUA . number plans search tree equal number times planningprocedure (ua to) would invoked exhaustive exploration search space.Note every plan treeUA treeTO unique, since step plan givenunique label. Thus, although two plans tree might instancesparticular operator sequence, O1 O2 O3, plans distinctsteps different labels. (We defined plans way make proofsconcise.)show given problem, treeTO least large treeUA , is,number plans treeTO greater equal number plans treeUA .done proving existence function L maps plans treeUA setsplans treeTO satisfies following two conditions.1. Totality Property: every plan U treeUA , exists non-empty setfT1; : : :; Tmg plans treeTO L(U ) = fT1; : : :; Tmg.2. Disjointness Property: L maps distinct plans treeUA disjoint sets planstreeTO ; is, U1; U2 2 treeUA U1 6= U2, L(U1) \ L(U2) = fg.Let us examine existence L two properties sucient provesize ua's search tree greater to. Figure 5 provides guidefollowing discussion. Intuitively, use L count plans two search trees.plan counted treeUA , use L count non-empty set plans treeTO .totality property means every time countP plan treeUA , count leastone plan treeTO ; implies j treeUA j U 2treeUA j L(U ) j. course, mustshow plan counted treeTOPis counted once; guaranteeddisjointness property, implies U 2treeUA j L(U ) j j treeTO j. Thus,conjunction two properties implies j treeUA j j treeTO j.define function L two properties follows. Let U plantreeUA , let plan treeTO , let parent function plan parent233fiMinton, Bresina, & Drummondua search treehLh?,@,@@R@hsearch treeLhhL- f- fL- fgHHHjHhhg,@, @Rhhg- fffhAAUhgFigure 5: L maps treeUA treeTOplan tree. 2 L(U ) (i) linearization U (ii) eitherU root nodes respective search trees parent(T ) 2 L(parent(U )).Intuitively, L maps plan U treeUA linearizations share common derivationancestry.3 illustrated Figure 5, plan treeUA dashed linedrawn corresponding set plans treeTO .show L satisfies totality disjointness properties inductiondepth search trees. Detailed proofs appendix. prove first property,show every plan contained treeUA , linearizations plan containedtreeTO . prove second property, note two plans different depthstreeUA disjoint sets linearizations, show induction two plansdepth treeUA also property.much smaller treeUA treeTO ? mapping described providesanswer. plan U treeUA j L(U ) j distinct plans to, j L(U ) jnumber linearizations U . exact number depends unordered U is.totally unordered plan factorial number linearizations totally ordered plansingle linearization. Thus, time size treeUA equals sizetreeTO every plan treeUA totally ordered; otherwise, treeUA strictly smallertreeTO possibly exponentially smaller.6. Time Cost Per Plansize ua's search tree possibly exponentially smaller to,follow ua necessarily ecient. Eciency determined two factors:3. reader may question L maps U linearizations treeTO share common derivation ancestry, opposed simply mapping U linearizations treeTO . reasonplanners systematic, sense may generate two plansoperator sequence. distinguish plans derivational history. example, supposetwo instantiations operator sequence O1 O2 O3 exist within treeTO correspond different plans treeUA . L relies different derivations determine appropriatecorrespondence.234fiTotal-Order Partial-Order PlanningStep Executions Per Plan Cost UA Cost11O(1)O(1)21O(1)O(1)3<1O(1)O(1)41O(1)O(e)51O(n)O(e)Table 1: Cost per plan comparisonstime cost per plan search tree (discussed section) size subtreeexplored search process (discussed next section).section show ua indeed take time per plan, extratime relatively small grows polynomially number steps plan,4denote n. comparing relative eciency ua to, first considernumber times algorithm step executed per plan search treeconsider time complexity step.noted preceding sections, node search tree corresponds plan,invocation planning procedure ua corresponds attemptextend plan. Thus, ua to, clear termination checkgoal selection (Steps 1 2) executed per plan. Analyzing numbertimes remaining steps executed might seem complicated, sincesteps executed many times internal node leaf. However,analysis actually quite simple since amortize number executionsstep number plans produced. Notice Step 6 executed plangenerated (i.e., node root node). gives us boundnumber times Steps 3, 4, 5 executed.5 specifically,algorithms, Step 3 executed fewer times Step 6, Steps 4 5 executedexactly number times Step 6 executed, is, plangenerated. Consequently, algorithms, step executed perplan, summarized Table 1. words, number times step executedplanning process bounded size search tree.examining costs step, first note algorithms, Step 1,termination check, accomplished O(1) time. Step 2, goal selection, alsoaccomplished O(1) time; example, assuming goals stored list,select-goal function simply return first member list. executionStep 3, operator selection, also requires O(1) time; assume operatorsindexed effects, required \pop" list relevant operatorsexecution.4. assume size operators (the number preconditions effects) boundedconstant given domain.5. Since Steps 3 4 nondeterministic, need clear terminology. say Step3 executed time different operator chosen, Step 4 executed differentcombination orderings selected.235fiMinton, Bresina, & DrummondSteps 4 5 less expensive ua. Step 4 accomplishedinserting new operator, Oadd , somewhere Odel Oneed . possibleinsertion points considered starting Oneed working towards Odel , execution Step 4 accomplished constant time, since insertion constitutes oneexecution step. contrast, Step 4 ua involves carrying interaction detectionelimination order produce new plan P 0 . step accomplished O(e)time, e number edges graph required represent partially orderedplan. (In worst case, may O(n2 ) edges plan, best case, O(n)edges.) following description ua's ordering step, Figure 3,additional implementation details:4. Ordering selection: Order add del need . Label steps preceding addsteps following Oadd . Let stepsint unlabeled steps interact Oadd . Let Odellast deleter c. Repeat stepsint empty:Let Oint = Pop(stepsint )Oint still unlabeled either:{ order Oint Oadd , label Oint unlabeled steps Oint ;{ order Oint Oadd , label Oint unlabeled steps Oint .Choice point: orderings must considered completeness.Let P resulting plan.0ordering process begins preprocessing stage. First, steps preceding following Oadd labeled such. labeling process implemented depth-first traversalplan graph, starting Oadd root, first follows edges one direction follows edges direction. requires O(e) time.labeling process complete, steps unordered respect Oaddunlabeled, thus interacting steps (which must unordered respect Oadd)identifiable O(n) time. last deleter identifiable O(e) time.preprocessing stage, procedure orders interacting step respectOadd, updating labels iteration. Since edge graph need traversedonce, entire ordering process takes O(e) time (as describedMinton et al., 1991b). see this, note process labeling steps (orafter) Oint stop soon labeled step encountered.shown Step 4 O(1) complexity Step 4 ua O(e) complexity, consider Step 5 algorithms, updating goal set. accomplishesiterating steps plan, head tail, requires O(n)time. ua accomplishes similar manner, requires O(e) time traversegraph. (Alternatively, ua use procedure to, provided O(e) topologicalsort first done linearize plan.)summarize complexity analysis, use partial order means ua incursgreater cost operator ordering (Step 4) updating goal set (Step 5). Overall,ua requires O(e) time per plan, requires O(n) time per plan. Since totallyordered plan requires representation size O(n), partially ordered graph requiresrepresentation size O(e), designing procedures lower costs would possibleentire plan graph need examined worst case.236fiTotal-Order Partial-Order Planning7. Role Search Strategiesprevious sections compared ua terms relative search space sizerelative time cost per node. extra processing time required ua nodewould appear justified since search space may contain exponentially fewer nodes.However, complete analysis, must consider number nodes actually visitedalgorithm given search strategy.breadth-first search, analysis straightforward. completing searchparticular depth, planners explored entire trees depth.6ua find solution depth due correspondencesearch trees. Thus, degree ua outperform to, breadth-first, dependssolely \expansion factor" L, i.e., number linearizations ua's plans.formalize analysis follows. node U treeUA , denote numbersteps plan U nu , number edges U eu . node Uua generates, ua incurs time cost O(eu ); whereas, incurs time cost O(nu ) j L(U ) j,j L(U ) j number linearizations plan node U . Therefore, ratiototal time costs ua follows, bf (treeUA ) denotes subtreeconsidered ua breadth-first search.Pcost(tobf ) = u2bfP(treeUA ) O(nu ) j L(U ) jcost(uabf )u2bf (treeUA ) O(eu )analysis breadth-first search simple search strategy preservescorrespondence two planners' search spaces. breadth-first search, twoplanners synchronized exhaustively exploring level, explored(exactly) linearizations plans explored ua. search strategysimilarly preserves correspondence, iterative deepening, similar analysiscarried out.cost comparison clear-cut depth-first search, since correspondenceguaranteed preserved. easy see that, depth-first search,necessarily explore linearizations plans explored ua. simplyplanners nondeterministically choose child expand. deeper reason:correspondence L preserve subtree structure search space. planU treeUA , corresponding linearizations L(U ) may spread throughout treeTO .Therefore, unlikely corresponding plans considered orderdepth-first search. Nevertheless, even though two planners synchronized,might expect that, average, ua explore fewer nodes size treeUA lessequal size treeTO .Empirically, observed ua tend outperform depth-firstsearch, illustrated experimental results Figure 6. first graph comparesmean number nodes explored ua 44 randomly generated blocksworldproblems; second graph compares mean planning time uaproblems demonstrates extra time cost per node ua relatively insignificant. problems partitioned 4 sets 11 problems each, according minimal6. perspicuity, ignore fact number nodes explored two planners lastlevel may differ planners stop reach first solution.237fiMinton, Bresina, & Drummond100007500Time SolutionNodes Explored505000UA25004030UA20100034563Depth Problem456Depth ProblemFigure 6: ua Performance Comparison Depth-First Searchsolution \length" (i.e., number steps plan). problem, plannersgiven depth-limit equal length shortest solution.7 Since plannersmake nondeterministic choices, 25 trials conducted problem. source codedata required reproduce experiments found Online Appendix 1.pointed out, one plausible explanation observed dominance uato's search tree least large ua's search tree. fact, experimentsoften observed to's search tree typically much larger. However, full storyinteresting. Search tree size alone sucient explain ua's dominance;particular, density distribution solutions play important role.solution density search tree proportion nodes solutions.8solution density to's search tree greater ua's search tree, mightoutperform ua depth-first search even though to's search tree actually larger.example, might case ua solution plans completely unorderedplans remaining leaves treeUA { failed plans { totally ordered.case, ua solution plan corresponds exponential number solution plans,ua failed plan corresponds single failed plan. converse also possible:solution density ua's search tree might greater to's search tree, thusfavoring ua depth-first search. example, might single totallyordered solution plan ua's search tree large number highly unordered failed7. Since depth-limit equal length shortest solution, iterative deepening (Korf, 1985)approach would yield similar results. Additionally, note increasing depth-limit pastdepth shortest solution significantly change outcome experiments.8. definition solution density ill-defined infinite trees, assume depth-boundalways provided, finite subtree explicitly enumerated.238fiTotal-Order Partial-Order PlanningUA Search Tree**Search Tree***** = Solution planFigure 7: Uniform solution distribution, solution density 0.25plans. Since failed ua plans would correspond large number failedplans, solution density would considerably lower.blocksworld problems, found solution densities two planners'trees differ greatly, least way would explain performanceresults. saw tendency treeUA higher solution density treeTO .example, 11 problems solutions depth six, average solution density9exceeded ua 7 12 problems. particularly surprisingsince see priori reason suppose solution densities two plannersdiffer greatly.Since solution density insucient explain ua's dominance blocksworld experiments using depth-first search, need look elsewhere explanation.hypothesize distribution solutions provides explanation. notesolution plans distributed perfectly uniformly (i.e., even intervals) amongleaves search tree, solution densities similar, plannersexpected search similar number leaves, illustrated schematic searchtree Figure 7. Consequently, explain observed dominance uahypothesizing solutions uniformly distributed; is, solutions tend cluster.see this, suppose treeUA smaller treeTO two treessolution density. solutions clustered, Figure 8, depth-first searchexpected produce solutions quickly treeUA treeTO .10 hypothesis9. experiments, nondeterministic goal selection procedure used planners, meantsolution density could vary run run. compared average solution density 25trials problem obtain results.10. Even solutions distributed randomly amongst leaves trees uniform probability(as opposed distributed \perfectly uniformly"), clusters nodes. Therefore,small disadvantage. see this, let us suppose leaf treeUA treeTOsolution equal probability p. is, treeUA NUA leaves, kUA solutions,239fiMinton, Bresina, & DrummondUA Search Tree*Search Tree****** = Solution planFigure 8: Non-uniform solution distribution, solution density 0.25solutions tend clustered seems reasonable since easy construct problems\wrong decision" near top search tree lead entire subtreedevoid solutions.One way test hypothesis compare ua using randomized searchstrategy, type Monte Carlo algorithm, refer \iterative sampling" (cf.Minton et al., 1992; Langley, 1992; Chen, 1989; Crawford & Baker, 1994). iterativesampling strategy explores randomly chosen paths search tree solutionfound. path selected traversing tree root leaf, choosing randomlybranch point. leaf solution search terminates; not, searchprocess returns root selects another path. path may examinedsince memory maintained iterations.contrast depth-first search, iterative sampling relatively insensitive distribution solutions. Therefore, advantage ua disappear hypothesis correct. experiments, find ua use iterativesampling, expand approximately number nodes set blocksworldproblems.11 (For planners, performance iterative sampling worsedepth-first search.) fact difference ua iterativesampling, difference depth-first search, suggests solutionstreeTO NTO leaves, kTO solutions, p = kUA =NUA = kTO =NTO . general,k N nodes solutions, expected number nodes must tested find solution:5N=k k = 1 approaches N=k k (and N ) approaches 1. (This simply expectednumber samples binomial distribution.) Therefore, since kTO kUA , expected numberleaves explored greater equal expected number leaves explored ua,factor 2.11. iterative sampling strategy depth-limited exactly way depth-first strategywas. note, however, performance iterative sampling relatively insensitive actualdepth-limit used.240fiTotal-Order Partial-Order Planningindeed non-uniformly distributed. Furthermore, result shows ua necessarilysuperior to; search strategy employed makes dramatic difference.Although blocksworld domain may atypical, conjecture resultsgeneral relevance. Specifically, distribution-sensitive search strategies like depth-firstsearch, one expect ua tend outperform to. distribution-insensitivestrategies, iterative sampling, non-uniform distributions effect. noteiterative sampling rather simplistic strategy, sophisticatedsearch strategies, iterative broadening (Ginsberg & Harvey, 1992), alsorelatively distribution insensitive. explore strategies Section 8.2.8. Role Heuristicspreceding sections, shown partial-order planner ecientsimply search tree smaller. search strategies, breadthfirst search, size differential obviously translates eciency gain.strategies, depth-first search, size differential translates eciency gain,provided make additional assumptions solution density distribution.However, often claimed partial-order planners ecient dueability make informed ordering decisions, rather different argument. instance,Sacerdoti (1975) argues reason noah performs well problemsblocksworld's \Sussman anomaly". delaying decision whether stackB stacking B C, noah eventually detect con ict occurstacks B first, critic called \resolve-conflicts" order stepsintelligently.section, show argument formally described termstwo planners. demonstrate ua fact potential advantageexploit certain types heuristics readily to. advantageindependent fact ua smaller search space. Whether advantagesignificant practice another question, course. also describe experimentsevaluate effect commonly-used heuristic blocksworld problems.8.1 Making Informed DecisionsFirst, let us identify ua make better use certain heuristics to.ua planning algorithm, step 4 arbitrarily orders interacting plan steps. Similarly,Step 4 arbitrarily chooses insertion point new step. easy see,however, orderings tried others heuristic search.illustrated Figure 9, compares ua particular problem. keyfigure describes relevant conditions library operators, preconditionsindicated left operator added conditions indicated right (theredeletes example). brevity, initial step final step plansshown. Consider plan treeUA unordered steps O1 O2 . uaintroduces O3 achieve precondition p O1 , Step 4 ua order O3 respectO2, since steps interact. However, makes sense order O2 O3, since O2achieves precondition q O3. illustrates simple planning heuristic refermin-goals heuristic: \prefer orderings yield fewest false preconditions".241fiMinton, Bresina, & DrummondUAO1O1O1O1O2O1O2O2O1O2O3O1O1O2q O3pO3O3O2O3O1O3O2O1O2KEYpO1rO2 qFigure 9: Comparison ua example.heuristic guaranteed produce optimal search optimal plan,commonly used. basis \resolve con icts" critic Sacerdoti employedblocksworld examples.Notice, however, cannot exploit heuristic effectively uaprematurely orders O1 respect O2 . Due inability postpone orderingdecision, must choose arbitrarily plans O1 O2 O2 O1,impact decision evaluated.general case, suppose h heuristic applied partially orderedplans totally ordered plans. Furthermore, assume h \useful" heuristic; i.e., hrates one plan highly another, planner explores highly ratedplan first perform better average. Then, ua potential advantageprovided h satisfies following property: ua plan U correspondingplan , h(U ) h(T ); is, partially ordered plan must rated least highlinearizations. (Note unambiguous plans, min-goals heuristic satisfiesproperty since gives identical ratings partially ordered plan linearizations.)ua advantage ua expanding plan U expandingcorresponding plan , h rate child U least high highlyrated child . true since every child linearization child U ,therefore child rated higher child U . Furthermore, maychild U none linearizations child , therefore childU rated higher every child . Since assumed h useful heuristic,means ua likely make better choice to.242fiTotal-Order Partial-Order Planning5000UATO-MGUA-MGNodes Explored4000without MinGoalsUA without MinGoalsMinGoalsUA MinGoals3000UA2000TO-MG1000UA-MG03456Depth ProblemFigure 10: Depth first search without min-goals8.2 Illustrative Experimental Resultsprevious section showed ua potential advantage betterexploit certain ordering heuristics. examine practical effects incorporatingone heuristic ua to.First, note ordering heuristics make sense search strategies.particular, breadth-first search, heuristics improve eciency searchmeaningful way (except possibly last level). Indeed, need consider searchstrategy ua \synchronized", defined earlier, since ordering heuristicssignificantly affect relative performance ua strategies. Thus,begin considering standard search strategy synchronized: depth-firstsearch.use min-goals heuristic basis experimental investigation, sincecommonly employed, presumably could choose heuristic meets criterionset forth previous section. Figure 10 shows impact min-goals behaviorua depth-first search. Although heuristic biases ordertwo planners' search spaces explored (cf. Rosenbloom, Lee, & Unruh, 1993), appearseffect largely independent partial-order/total-order distinction, sinceplanners improved similar percentage. example, depth-first searchproblems solutions depth six, ua improved 88% improved 87%. Thus,obvious evidence extra advantage ua, one might expectedanalysis previous section. hand, contradicttheory, simply means potential heuristic advantage significant enoughshow up. domains, advantage might manifest significantly.all, certainly possible design problems advantage significant,243fiMinton, Bresina, & DrummondNodes Explored100TO-IBUA-IBTO-ISUA-IS75Iterative BroadeningUA Iterative BroadeningIterative SamplingUA Iterative SamplingTO-IBUA-IB50TO-IS25UA-IS3456Depth ProblemFigure 11: Iterative sampling & iterative broadening, min-goalsexample Figure 9 illustrates. results simply illustrate blocksworlddomain, making intelligent ordering decisions produces negligible advantage ua,contrast significant effect due search space compression (discussed previously).12min-goals heuristic seem help ua to, resultsnevertheless interesting, since heuristic significant effect performanceplanners, much min-goals outperforms ua without min-goals.effectiveness min-goals domain dependent, find interestingexperiments, use min-goals makes difference use partial orders.all, blocksworld originally helped motivate development partial-order planningsubsequent planning systems employed partial orders. deeplysurprising, result help reinforce already know: attentionpaid specific planning heuristics min-goals.analysis search space compression Section 7, described \distributioninsensitive" search strategy called iterative sampling showed iterative sampling ua perform similarly, although performance worsedepth-first search. combine min-goals iterative sampling, find produces much powerful strategy, one ua still performequally. simplicity, implementation iterative sampling uses min-goals pruning heuristic; choice point, explores plan extensions fewestgoals. strategy powerful, although incomplete.13 incompleteness,note one problem removed sample set iterative sampling12. Section 9.2, discuss planners \less-committed" ua. planners, advantagedue heuristics might pronounced since \delay" decisions even longer ua.13. Instead exploring plan extensions fewest goals choice point, alternativestrategy assign extension probability inversely correlated number goals,244fiTotal-Order Partial-Order Planningmin-goals would never terminate problem. caveat mind, turnresults Figure 11, compared Figure 10, show performanceua iterative sampling was, general, significantly betterperformance depth-first search. (Note graphs Figures 10 11different scales.) results clearly illustrate utility planning bias introducedmin-goals blocksworld domain, since 43 44 problems, solution existssmall subspace preferred min-goals.experiments show advantage ua comparedheuristic, consistent conclusions above. However, could equally wellmin-goals powerful, leading solutions quickly, smaller uencesobscured.dramatic success combining min-goals iterative sampling led us consideranother search strategy, iterative broadening, combines best aspects depthfirst search iterative sampling. sophisticated search strategy initially behaveslike iterative sampling, evolves depth-first search breadth-cutoff increases(Langley, 1992). Assuming solution within specified depth bound, iterativebroadening complete. early stages iterative broadening distribution-insensitive;later stages behaves like depth-first search and, thus, becomes increasingly sensitivesolution distribution. one would expect iterative sampling experiments,iterative broadening, solutions found early on, shown Figure 11. Thus,surprising ua performed similarly iterative broadening.point results presented subsection illustrative,since deal single domain single heuristic. Nevertheless,experiments illustrate various properties identified paperinteract.9. Extending Resultsestablished basic results concerning eciency ua variouscircumstances, consider results extend types planners.9.1 Expressive Languagespreceding sections, showed primary advantage uaua's search tree may exponentially smaller to's search tree, also showedua pays small (polynomial) extra cost per node advantage. Thus farassumed restricted planning language operators propositional;however, practical problems demand operators variables, conditional effects,conditional preconditions. expressive planning language, time costper node significantly greater ua to? One might think so, since workrequired identify interacting steps increase expressiveness operatorlanguage used (Dean & Boddy, 1988; Hertzberg & Horz, 1989). cost detecting steppick accordingly. Given depth bound, strategy advantage asymptoticallycomplete. used simpler strategy pedagogical reasons.245fiMinton, Bresina, & Drummondinteraction high enough, savings ua enjoys due reduced search spaceoutweighed additional expense incurred node.Consider case simple breadth-first search. Earlier showed ratiototal time costs ua follows, subtree considered uabreadth-first search denoted bf (treeUA ), number steps plan U denotednu , number edges U denoted eu :Pcost(TObf ) = U 2bfP(treeUA ) O(nu ) j L(U ) jcost(UAbf )U 2bf (treeUA ) O(eu )cost comparison specific simple propositional operator language usedfar, basic idea general. ua generally outperform whenever costper node less product cost per node number nodescorrespond L. Thus, ua could incur exponential cost per node stilloutperform cases. happen, example, exponential numberlinearizations ua partial order greater exponential cost per node ua.general, however, would like avoid case ua pays exponential cost pernode and, instead, consider approach guarantee cost per node uaremains polynomial (as long cost per node also remains polynomial).cost per node ua dominated cost updating goal set (Step 5)cost selecting orderings (Step 4). Updating goal set remains polynomiallong plan unambiguous. Since precondition unambiguous plan eithernecessarily true necessarily false, determine truth value given preconditionexamining truth value arbitrary linearization plan. Thus, simplylinearize plan use procedure uses calculating goal set.result, cost maintaining unambiguous property (i.e., Step 4)impacted expressive languages. One approach eciently maintainingproperty relies \conservative" ordering strategy operators orderedeven possibly interact.illustration approach, consider simple propositional language conditional effects, \if p q, add r". Hence, operator add (or delete)propositions depending state applied. refer conditions\p" example dependency conditions. (Note that, like preconditions, dependencyconditions simple propositions.) Chapman (1987) showed type language NP-hard decide whether precondition true partially ordered plan.However, pointed above, special case unambiguous plans, decisionaccomplished polynomial time.Formally, language specified follows. operator O, before, list preconditions, pre(O), list (unconditional) adds, adds(O), list (unconditional) deletes,dels(O). addition, list conditional adds, cadds(O), list conditionaldeletes, cdels(O); containing pairs hDe ; ei, De conjunctive set dependency conditions e conditional effect (either added deleted condition).Analogous constraint every delete must precondition, every conditionaldelete must member dependency conditions; is, every hDe ; ei 2 cdels(O),e 2 De.246fiTotal-Order Partial-Order PlanningFigure 12 shows version ua algorithm, called ua-c, appropriatelanguage. primary difference ua ua-c algorithms Steps3 4b operator may specialized respect set dependency conditions.function specialize(O, ) accepts plan step, O, set dependency conditions,D; returns new step O0 like O, certain conditional effects madeunconditional. effects selected transformation exactly whosedependency conditions subset D. Thus, act specializing plan stepact committing expanding causal role plan.14 step specialized, ua-cmade commitment use given set effects. course, stepspecialized later search node, specializations never retracted.precisely, definition O0 = specialize(O; D), step, conjunctive set dependency conditions O, n set difference operator, follows.pre(O0) = pre(O) [ D.adds(O0) = adds(O) [ fe j hDe; ei 2 cadds(O) ^ De Dg.dels(O0) = dels(O) [ fe j hDe ; ei 2 cdels(O) ^ De Dg.cadds(O0) = fhDe ; ei j hDe ; ei 2 cadds(O) ^ De 6 ^ De = De nDg.cdels(O0) = fhDe ; ei j hDe ; ei 2 cdels(O) ^ De 6 ^ De = De nDg.0000definition step interaction generalized ua-c follows. say twosteps plan interact unordered respect followingdisjunction holds:one step precondition dependency condition added deletedstep,one step adds condition deleted step.difference definition step interaction one given earlier indicated italic font. modified definition allows us detect interacting operatorssimple inexpensive test, original definition. example, two stepsunordered interact one step conditionally adds r precondition r.Note first step need actually add r plan, ordering two operatorsmight unnecessary. general, definition interaction sucient criterionguaranteeing resulting plans unambiguous, necessary criterion.Figure 13 shows schematic example illustrating ua-c extends plan. preconditions operator shown left operator, unconditionaladds right. (We show preconditions effects necessary illustratespecialization process; deletes used example.) Conditional adds shown14. simplicity, modifications used create ua-c sophisticated. result, ua-c's spacemay larger needs circumstances, since aggressively commits specializations.sophisticated set modifications possible; however, subtlies involved eciently planningdependency conditions (Pednault, 1988; Collins & Pryor, 1992; Penberthy & Weld, 1992) largelyirrelevant discussion.247fiMinton, Bresina, & DrummondUA-C(P; G)1 Termination check: G empty, report success return solution plan P.2 Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.3 Operator selection: Let Oadd operator schema library possibly adds c; is,either c 2 adds(O), exists hDc ; ci 2 cadds(O). former case, insert step Oaddlatter case, insert step specialize(Oadd ; Dc). Oadd , terminate reportfailure. Choice point: ways c added must considered completeness.4a Ordering selection: Let Odel (unconditional) last deleter c. Order Oadd OdelOneed .Repeat interactions:Select step Oint interacts Oadd .Order Oint either Oadd .Choice point: orderings must considered completeness.Let P resulting plan.4b Operator role selection: exists step Ocadd unmarked conditional add hDc ; cistep Ouse precondition c, Ouse Ocadd (unconditional)deleter c Ouse Ocadd .Either mark hDc; ci, replace Ocadd specialize(Ocadd ; Dc ).Choice point: options must considered completeness.5 Goal updating: Let G set preconditions P necessarily false.6 Recursive invocation: UA-C(P ; G ).00000Figure 12: ua-C planning algorithmunderneath operator. instance, first operator plan toppage precondition p. operator adds q conditionally adds u true.figure illustrates two plans produced result adding new conditional operatorplan. one plan, conditional effects [u ! s] [t ! u] selectedspecialization process, plan not.new step, Step 4b, requires polynomial time per plan generated, timecost steps ua. Hence, original ua algorithm,cost per node ua-c algorithm polynomial.also handle language given corresponding modifications (changing Step3 adding Step 4b), time cost per plan also remains polynomial.15 Moreover,relationship holds two planners' search spaces { treeUA never largertreeTO exponentially smaller. example illustrates theoreticaladvantages ua preserved expressive language.pointed out, definition interaction sucient criterion guaranteeingresulting plans unambiguous, necessary criterion. Nevertheless,conservative approach allows interactions detected via simple inexpensive syntactictest. Essentially, kept cost per node ua-c low restricting search spaceconsiders, shown Figure 14. ua-c considers unambiguous plansgenerated via \conservative" ordering strategy. ua-c still partial-order planner,15. fact, Step 4b implemented time cost O(e), using graph traversal techniquesdescribed Section 6. result ua-c implementation corresponding to-c implementationtime cost per node new language original language, O(e)(n), respectively.248fiTotal-Order Partial-Order Planningqrpq[tu]Add Operator:[up[t[uqrs]rs]qrupu]qrqrOuFigure 13: example illustrating ua-c algorithmcomplete, consider partially ordered plans even unambiguouspartially ordered plans.\trick" used languages well, provided devisesimple test detect interacting operators. example, previous work (Minton et al.,1991b) showed done language operators variablespreconditions effects. general case, given ua plan correspondingplan, Steps 1,2, 3 ua algorithm cost corresponding stepsalgorithm. long plans considered ua unambiguous, Step 5ua algorithm accomplished arbitrary linearization plan, casecosts O(e) Step 5 algorithm. Thus, possibilityadditional cost Step 4. general, devise \local" criterion interactionresulting plan guaranteed unambiguous, ordering selectionstep accomplished polynomial time. \local", mean criterionconsiders operator pairs determine interactions; i.e., must examine restplan.Although theoretical advantages ua preservedexpressive languages, cost. unambiguous plans considered mayorderings necessary, addition unnecessary orderings increasesize ua's search tree. magnitude increase depends specific language,domain, problem considered. Nevertheless, guarantee ua's searchtree never larger to's.general lesson cost plan extension solely dependentexpressiveness operator language, also depends nature plans249fiMinton, Bresina, & Drummondpartially ordered plansunambiguouspartially ordered plansunambiguous partiallyordered plans producedconservative ordering strategytotally orderedplansFigure 14: Hierarchy Plan Spacesplanner considers. So, although extension partially ordered plans NP-hardlanguages conditional effects, space plans restricted (e.g., unambiguousplans considered) worst-case situation avoided.9.2 Less Committed Plannersshown ua, partial-order planner, certain computational advantagestotal-order planner, to, since ability delay commitments allowscompact search space potentially intelligent ordering choices. However,many planners even less committed ua. fact, continuumcommitment strategies might consider, illustrated Figure 15. Total-orderplanning lies one end spectrum. extreme strategy maintainingtotally unordered set steps search exists linearization stepssolution plan.Compared many well-known planners, ua conservative since requires planunambiguous. required noah (Sacerdoti, 1977), NonLin (Tate, 1977),TotallyOrderedCompletelyUnorderedUAFigure 15: continuum commitment strategies250fiTotal-Order Partial-Order PlanningMT(P; G)1. Termination check: G empty, report success stop.2. Goal selection: Let c = select-goal(G), let Oneed plan step c precondition.3. Operator selection: Let Oadd either plan step possibly Oneed adds c operatorlibrary adds c. Oadd , terminate report failure.Choice point: operators must considered completeness.4. Ordering selection: Order Oadd Oneed. Repeat steps possiblyOadd Oneed delete c:Let Odel step; choose one following ways make c true OneedOrder Odel Oneed .Choose step Oknight (possibly Oadd ) adds c possibly Odel Oneed;order Odel Oneed .Choice point: alternatives must considered completeness.Let P resulting plan.5. Goal updating: Let G set preconditions P necessarily true.6. Recursive invocation: MT(P ; G ).00000Figure 16: Propositional Planner based Modal Truth CriterionTweak (Chapman, 1987), example. less-committed planners compareua to? One might expect less-committed planner advantagesua ua to. However, necessarily true. example,section introduce Tweak-like planner, called mt, show search spacelarger even to's circumstances.16Figure 16 presents mt procedure. mt propositional planner based Chapman'sModal Truth Criterion (Chapman, 1987), formal statement characterizes Tweak'ssearch space. straightforward see mt less committed ua. algorithmsquite similar; however, Step 4, whereas ua orders interacting steps, mt not.Since mt immediately order interacting operators, may add additionalorderings previously introduced operators later planning process producecorrect plans.proof ua's search tree larger to's search tree rested twoproperties L elaborated Section 5. investigating relationship mtto, found second property, disjointness property, hold mt,failure illustrates mt explore plans (and, consequently,ua) certain problems. disjointness property guarantees ua generate\overlapping" plans. example Figure 17 shows mt fails satisfy propertygenerate plans share common linearizations, leading considerableredundancy search tree. figure shows three steps, O1, O2, O3 , Oiprecondition pi added conditions gi, p1, p2, p3 . final step preconditionsg1, g2, g3, initial final steps shown figure. topfigure, plan constructed mt, goals g1 , g2, g3 achieved, p1 , p2,p3 remain achieved. Subsequently, solving precondition p1, mt generates plansshare linearization O3 O2 O1 (among others). comparison,16. use Tweak comparison because, like ua to, formal construct rather realisticplanner, therefore easily analyzed.251fiMinton, Bresina, & DrummondO1O2O3O1O2O2O3O3O2O1O3O1O3O2O1KEYp O11g1p1p2p3p22g2p1p2p3p3 O3g3p1p2p3Figure 17: \Overlapping" plans.ua generate plan O3 O2 O1 once. fact, simple show that,breadth-first search, mt explores many plans example (and alsoua, transitivity) due redundancy search space.result may seem counterintuitive. However, note search space sizepartial-order planner potentially much greater total-order planner sincemany partial orders set steps total orders. (Thus,designing partial-order planner, one may preclude overlapping linearizations orderavoid redundancy, discussed McAllester & Rosenblitt, 1991 Kambhampati,1994c.)course, one also construct examples mt smaller search spaceua to. example simply illustrates although one planner mayless committed another, search space necessarily smaller. commitmentstrategy used planner simply one factor uences overall performance.particular, effect redundancy partial-order planner overwhelm considerations. comparing two planners, one must carefully consider mappingsearch spaces concluding \less committed ) smaller search space".10. Related Workmany years, intuitions underlying partial-order planning largely takengranted. past years renewed interest fundamentalprinciples underlying issues.252fiTotal-Order Partial-Order PlanningBarrett et al. (1991) Barrett Weld (1994) describe interesting novelanalysis partial-order planning complements work. compare partialorder planner two total-order planners derived it, one searches spaceplans, searches space world states. study focusesgoal structure problem affects eciency partial-order planning.Specifically, examine partial-order total-order planning compare problemsindependent, serializable, non-serializable goals, using resource-boundeddepth-first search. refine Korf's work serializable goals (Korf, 1987), introducingdistinction trivially serializable subgoals, subgoals solvedorder without violating previously solved subgoal, laboriously serializable subgoals,subgoals serializable, least 1=n orderings cause previouslysolved subgoal violated. study describes conditions partial-orderplanner may advantage. instance, show domain goalstrivially serializable partial-order planner laboriously serializabletotal-order planners, partial-order planner performs significantly better.study provides interesting contrast Barret Weld's work, since investigate relative eciencies partial-order total-order planning algorithms independentparticular domain structure. Instead, focus underlying propertiessearch space search strategy affects eciency planners. Nevertheless,believe interesting relationships forms serializabilityinvestigate, ideas solution density clustering discussed here.illustrate this, consider artificial domain Barret Weld refer D1S 1,where, problem, goals subset fG1; G2; : : :G15g, initial conditionsfI1; I2; : : :I15g, operator Oi2f1;2;:::;15g precondition Ii , adds Gi , deletesIi,1. follows solution D1S 1 contains operators Oi Oj < j , Oimust precede Oj . domain, goals trivially serializable partial-orderplanner laboriously serializable total-order planners; thus, partial-orderplanner performs best. note also artificial domain, exactly onesolution per problem totally ordered. Therefore, immediately clear that,give ua problems domain, ua's search tree generallymuch smaller to's search tree. Since single solution planners,solution density ua clearly greater to. Thus, propertiesdiscussed paper provide basis analyzing differences subgoalserializibility manifest effect search. subject, however, simplemight seem deserves study.related work, Kambhampati written several papers (Kambhampati, 1994a,1994b, 1994c) analyze design space partial-order planners, including uaplanner presented here. Kambhampati compares ua, Tweak, snlp (McAllester & Rosenblitt, 1991), ucpop (Penberthy & Weld, 1992), several planners along varietydimensions. presents generalized schema partial order planning algorithms (Kambhampati, 1994c) shows commitment strategy used ua viewedway increase tractability plan extension (or refinement) process. Kambhampatialso carries empirical comparison various planning algorithms particular problem (Kambhampati, 1994a), showing differences commitment strategiesaffects eciency planning process. distinguishes two separate components253fiMinton, Bresina, & Drummondbranching factor, bt , former resulting commitment strategyoperator ordering (or terms, \tractability refinements") latter resultingchoice operator (\establishment refinements"). Kambhampati's experimentsdemonstrate \eager" commitment strategies tend increase bt, sometimesalso decrease , number possible establishers reduced plansordered. is, course, closely related issues investigated paper.addition, Kambhampati Chen (1993) compared relative utility reusingpartially ordered totally ordered plans \learning planners". showedreuse partially ordered plans, rather totally ordered plans, result \storage compaction" represent large number different orderings. Moreover, partialorder planners advantage exploit plans effectivelytotal-order planners. many respects, advantages fundamentally similaradvantages ua derives potentially smaller search space.11. Conclusionsfocusing analysis single issue, namely, operator ordering commitment,able carry rigorous comparative analysis two planners. shownsearch space partial-order planner, ua, never larger search spacetotal-order planner, to. Indeed certain problems, ua's search space exponentiallysmaller to's. Since ua pays small polynomial time increment per nodeto, generally ecient.showed ua's search space advantage may necessarily translateeciency gain, depending subtle ways search strategy heuristicsemployed planner. example, experiments suggest distribution-sensitivesearch strategies, depth-first search, benefit partial orderssearch strategies distribution-insensitive.also examined variety extensions planners, order demonstrategenerality results. argued potential benefits partial-orderplanning may retained even highly expressive planning languages. However,showed partial-order planners necessarily smaller search spaces, since\less-committed" strategies may create redundancies search space. particular, demonstrated Tweak-like planner, mt, larger search spacetotal-order planner problems.general results? Although analysis considered two specificplanners, examined important tradeoffs general relevance.analysis clearly illustrates planning language, search strategy, heuristicsused affect relative advantages two planning styles.results paper considered investigation possible benefitspartial-order planning. ua constructed order us analyzetotal-order/partial-order distinction isolation. reality, comparative behavior twoplanners rarely clear (as witnessed discussion mt). general pointsmake applicable planners, chose two arbitrary planners, wouldexpect one planner clearly dominate other.254fiTotal-Order Partial-Order Planningobservations regarding interplay plan representation search strategy raise new concerns comparative analyses planners. Historically,assumed representing plans partial orders categorically \better" representing plans total orders. results presented paper begin tell accuratestory, one interesting complex initially expected.Appendix A. ProofsA.1 Definitionssection defines terminology notation used proofs. notion planequivalence introduced plan step is, definition, uniquely labeledoperator instance, noted Section 3 Section 5. Thus, two plansset steps. Although formalism simplifies analysis, requires us define planequivalence explicitly.plan pair h; i, set steps, \before" relation ,i.e., strict partial order . Notationally, O1 O2 (O1; O2) 2.given problem, define search tree treeTO complete tree plansgenerated algorithm problem. treeUA correspondingsearch tree generated ua problem.Two plans, P1 = h1; 1 P2 = h2 ; 2i said equivalent, denotedP1 ' P2, exists bijective function f 1 2 that:{ 2 1, f (O) instances operator,{ O0; O00 2 1, O0 O00 f (O0) f (O00).plan P2 1-step to-extension (or 1-step ua-extension) plan P1 P2equivalent plan produced P1 one invocation (or ua).plan P to-extension (or ua-extension) either:{ P initial plan,{ P 1-step to-extension (or 1-step ua-extension) to-extension (or uaextension).immediately follows definition P member treeTO (or treeUA ),P to-extension (or ua-extension). addition, P to-extension (orua-extension), plan equivalent P member treeTO (ortreeUA ).P1 linearization P2 = h; 2i exists strict total order 12 1 P1 ' h; 1i.Given search tree, let parent function plan parent plan tree.Note P1 parent P2 , denoted P1 = parent(P2 ), P2 1-stepextension P1 .255fiMinton, Bresina, & DrummondGiven U 2 treeUA 2 treeTO , 2 L(U ) plan linearizationplan U either U root nodes respective search trees,parent(T ) 2 L(parent(U )).length plan number steps plan excluding first laststeps. Thus, initial plan length 0. plan P n steps length n , 2.P1 subplan P2 = h2; 2i P1 ' h1; 1i,{ 1 2{ 1 2 restricted 1, i.e., 1 = 2 \ 1 1.P1 strict subplan P2, P1 subplan P2 length P1 lesslength P2.solution plan P compact solution strict subplan P solution.A.2 Extension LemmasTO-Extension Lemma: Consider totally ordered plans T0 = h0; 0i T1 = h1; 1i,1 = 0 [ fOadd g 0 1 . Let G set false preconditions T0.T1 1-step to-extension T0 if:c = select-goal(G), c precondition step Oneed T0,Oadd adds c,(Oadd; Oneed) 21,(Odel; Oadd) 21, Odel last deleter c T1.Proof Sketch: lemma follows definition to. Given plan T0, falseprecondition c, selects c goal, consider operators achieve c,operator considers positions c last deleter c.UA-Extension Lemma: Consider plan U0 = h0; 0i produced ua planU1 = h1; 1i, 1 = 0 [ fOaddg 0 1. Let G set falsepreconditions steps U0 . U1 1-step ua-extension U0 if:c = select-goal(G), c precondition step Oneed U0,Oadd adds c,1 minimal set consistent orderings{ 0 1,{ (Oadd; Oneed) 21,{ (Odel; Oadd) 21, Odel last deleter c U1,{ step U1 interacts Oadd256fiTotal-Order Partial-Order PlanningProof Sketch: lemma follows definition ua. Given plan U0, falseprecondition c, ua considers operators achieve c, operator uainserts plan c last deleter. ua considersconsistent combinations orderings new operator operatorsinteracts. orderings added plan.A.3 Proof Search Space Correspondence LMapping Lemma: Let U0 = h0; u0i unambiguous plan let U1 = h1; u1i1-step ua-extension U0. T1 = h1; t1i linearization U1 , existsplan T0 T0 linearization U0 T1 1-step to-extension T0 .Proof: Since U1 1-step ua-extension U0, step Oadd 1 = 0 [fOaddg. Let T0 subplan produced removing Oadd T1; is, T0 = h0; t0i,t0 = t1 \ 0 0 . Since u0 = u1 \ 0 0 t1 \ 0 0 = t0 , follows T0linearization U0.Using TO-Extension lemma, show T1 1-step to-extension T0.First, T0 linearization U0 , two plans set goals. Therefore,ua selects goal c expanding U0 , selects c extending T0. Second, mustcase Oadd adds c since Oadd step ua inserted U0 make c true. Third,Oadd Oneed T1, since Oadd Oneed U1 (by definition ua) sinceT1 linearization U1. Fourth, Oadd last deleter c, Odel, T1, since OaddOdel U1 (by definition ua) since T1 linearization U1. Therefore,conditions TO-Extension lemma hold and, thus, T1 1-step to-extension T0.Q.E.D.Totality Property every plan U treeUA , exists non-empty set fT1; : : :; Tmgplans treeTO L(U ) = fT1; : : :; Tmg.Proof: suces show plan U1 ua-extension plan T1 linearizationU1 , T1 to-extension. proof induction plan length.Base case: statement trivially holds plans length 0.Induction step: hypothesis statement holds plans length n,prove statement holds plans length n + 1. Suppose U1 uaextension length n + 1 T1 linearization U1 . Let U0 plan U11-step ua-extension U0 . Mapping lemma, exists plan T0 T0linearization U0 T1 1-step to-extension T0. induction hypothesis, T0to-extension. Therefore, definition, T1 also to-extension. Q.E.D.Disjointness Property: L maps distinct plans treeUA disjoint sets plans treeTO ;is, U1 ; U2 2 treeUA U1 =6 U2, L(U1) \ L(U2) = fg.Proof: definition L, T1; T2 2 L(U ), T1 T2 tree depthtreeTO ; furthermore, U also depth treeUA . Hence, suces proveplans U1 U2 depth treeUA U1 6= U2 , L(U1 ) \ L(U2 ) = fg.Base case: statement vacuously holds depth 0.Induction step: hypothesis statement holds plans depth n,prove, contradiction, statement holds plans depth n + 1. Suppose257fiMinton, Bresina, & Drummondexist two distinct plans, U1 = h1; 1 U2 = h2 ; 2i, depth n + 1treeUA 2 L(U1) \L(U2). (by definition L), parent(T ) 2 L(parent(U1))parent(T ) 2 L(parent(U2 )). Since parent(U1 ) 6= parent(U2 ) contradicts inductionhypothesis, suppose U1 U2 parent U0 . Then, definitionua either (i) 1 6= 2 (ii) 1 = 2 1 6=2 . first case, since twoplans contain set plan steps, disjoint linearizations and,hence, L(U1 ) \ L(U2 ) = fg, contradicts supposition. second case,1 = 2; hence, plans resulted adding plan step Oadd parent plan. Since16=2, exists plan step Oint interacts Oadd one plan Ointordered Oadd plan Oadd ordered Oint . Thus, eithercase, linearizations two plans disjoint and, hence, L(U1 ) \ L(U2 ) = fg,contradicts supposition. Therefore, statement holds plans depth n + 1.Q.E.D.A.4 Completeness Proofprove complete breadth first search control strategy. so,suces prove exists solution problem, exists to-extensioncompact solution. so, prove following lemma.Subplan Lemma: Let totally ordered plan T0 strict subplan compact solution Ts.exists plan T1 T1 subplan Ts T1 1-step to-extensionT0.Proof: Since T0 strict subplan Ts Ts compact solution, set falsepreconditions T0, G, must empty. Let c = select-goal(G), let Oneedstep T0 precondition c, let Oadd step Ts achieves c. Considertotally ordered plan T1 = h0 [ fOadd g; 1i, 1 . Clearly, T1 subplanTs. Furthermore, TO-Extension Lemma, T1 1-step extension T0 to.see this, note Oadd ordered Oneed T1 since ordered Oneed Ts .Similarly, Oadd ordered last deleter c T0 since deleter c T0deleter c Ts , Oadd ordered deleters c Ts . Thus, conditionsTO-Extension Lemma hold. Q.E.D.Completeness Theorem: plan Ts totally ordered compact solution, Tsto-extension.Proof: Let n length Ts. show k n, exists subplan Tslength k to-extension. sucient prove result since subplanexactly length n equivalent Ts . proof induction k.Base case: k = 0 statement holds since initial plan, length 0,subplan solution plan.Induction step: assume statement holds k show k < nstatement holds k + 1. induction hypothesis, exists plan T0 length kstrict subplan Ts . Subplan Lemma, exists plan T1subplan Ts 1-step to-extension T0. Thus, exists subplan Ts lengthk + 1. Q.E.D.258fiTotal-Order Partial-Order PlanningA.5 Completeness Proof UAprove ua complete breadth-first search strategy. result followssearch space correspondence defined L fact complete.particular, show to-extension , exists ua-extension Ulinearization U . Since ua produces unambiguous plans, mustcase solution, U also solution. this, follows immediately uacomplete.Inverse Mapping Lemma: Let T0 = h0; t0i totally ordered plan. Let T1 =h1; t1i 1-step to-extension T0. Let U0 = h0; u0i plan produced uaT0 linearization U0 . exists plan U1 T1 linearizationU1 U1 1-step ua-extension U0 .Proof: definition to, 1 = 0 [ fOaddg, Oadd added cfalse precondition plan step Oneed U0 . Consider U1 = h1 ; u1 i, u1minimal subset t1 that:u0 u1,(Oadd; Oneed) 2u1,(Odel; Oadd) 2u1 , Odel last deleter c U1,step U1 interacts OaddSince u1 t1 , T1 linearization U1. addition, U1 extension U0 sincemeets three conditions UA-Extension Lemma, follows. First, since c mustgoal selected extending T0, c must likewise selected uaextending U0 . Second, Oadd adds c since Oadd achieves c T0. Finally, construction,u1 satisfies third condition UA-Extension Lemma. Q.E.D.UA Completeness Theorem: Let Ts totally ordered compact solution.exists ua-extension Us Ts linearization Us .Proof: Since complete, suces show T1 to-extension,exists ua-extension U1 T1 linearization U1 . proof inductionplan length.Base case: statement trivially holds plans length 0.Induction step: hypothesis statement holds plans length n,prove statement holds plans length n + 1. Assume T1 to-extensionlength n + 1, let T0 plan T1 1-step to-extension T0.induction hypothesis, exists ua-extension U0 length n T0linearization U0. Inverse Mapping Lemma, exists plan U1linearization T1 1-step ua-extension U0 . Since U1 1-step ua-extensionU0, length n + 1. Q.E.D.259fiMinton, Bresina, & DrummondAcknowledgementswork present paper originally described two conference papers(Minton et al., 1991a, 1992). thank Andy Philips many contributionsproject. wrote code planners helped conduct experiments. alsothank three anonymous reviewers excellent comments.ReferencesBackstrom, C. (1993). Finding least constrained plans optimal parallel executionsharder thought. Proceedings Second European WorkshopPlanning.Barrett, A., Soderland, S., & Weld, D. (1991). effect step-order representationsplanning. Tech. rep. 91-05-06, Univ. Washington, Computer Science Dept.Barrett, A., & Weld, D. (1994). Partial-order planning: Evaluating possible eciency gains.Artificial Intelligence, 67 (1), 71{112.Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32, 333{377.Chen, P. (1989). Heuristic Sampling Backtrack Trees. Ph.D. thesis, Dept. ComputerScience, Stanford Univ., Stanford, CA.Collins, G., & Pryor, L. (1992). Achieving functionality filter conditions partialorder planner. Proceedings Tenth National Conference Artificial Intelligence.Crawford, J., & Baker, A. (1994). Experimental results application satisfiabilityalgorithms scheduling problems. Proceedings Twelfth National ConferenceArtificial Intelligence.Dean, T., & Boddy, M. (1988). Reasoning partially ordered events. Artificial Intelligence, 36, 375{399.Drummond, M., & Currie, K. (1989). Goal-ordering partially ordered plans. Proceedings Eleventh International Joint Conference Artificial Intelligence.Ginsberg, M., & Harvey, W. (1992). Iterative broadening. Artificial Intelligence, 55, 367{383.Godefroid, P., & Kabanza, F. (1991). ecient reactive planner synthesizing reactiveplans. Proceedings Ninth National Conference Artificial Intelligence.Hertzberg, J., & Horz, A. (1989). Towards theory con ict detection resolutionnonlinear plans. Proceedings Eleventh International Joint ConferenceArtificial Intelligence.Kambhampati, S. (1994a). Design tradeoffs partial order (plan space) planning.Proceedings Second International Conference AI Planning Systems.260fiTotal-Order Partial-Order PlanningKambhampati, S. (1994b). Multi contributor causal structures planning: formalizationevaluation. Artificial Intelligence, 69, 235{278.Kambhampati, S. (1994c). Refinement search unifying framework analyzing planspace planners. Proceedings Fourth International Conference PrinciplesKnowledge Representation Reasoning.Kambhampati, S., & Chen, J. (1993). Relative utility EBG-based plan reuse partial ordering vs. total ordering planning. Proceedings Eleventh National ConferenceArtificial Intelligence.Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. Artificial Intelligence, 27, 97{109.Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33,65{88.Langley, P. (1992). Systematic nonsystematic search strategies. ProceedingsFirst International Conference AI Planning Systems.McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. ProceedingsNinth National Conference Artificial Intelligence.Minton, S., Bresina, J., & Drummond, M. (1991a). Commitment strategies planning:comparative analysis. Proceedings Twelfth International Joint ConferenceArtificial Intelligence.Minton, S., Bresina, J., Drummond, M., & Philips, A. (1991b). analysis commitmentstrategies planning: details. Tech. rep. 91-08, NASA Ames, AI ResearchBranch.Minton, S., Drummond, M., Bresina, J., & Philips, A. (1992). Total order vs. partial orderplanning: Factors uencing performance. Proceedings Third InternationalConference Principles Knowledge Representation Reasoning.Pednault, E. (1988). Synthesizing plans contain actions context-dependent effects.Computational Intelligence, 4, 356{372.Penberthy, J., & Weld, D. (1992). UCPOP: sound, complete, partial-order planneradl. Proceedings Third International Conference Principles KnowledgeRepresentation Reasoning.Regnier, P., & Fade, B. (1991). Complete determination parallel actions temporaloptimization linear plans action. Hertzberg, J. (Ed.), European WorkshopPlanning, Vol. 522 Lecture Notes Artificial Intelligence, pp. 100{111 SanktAugustin, Germany. Springer.Rosenbloom, P., Lee, S., & Unruh, A. (1993). Bias planning explanation-based learning. Minton, S. (Ed.), Machine Learning Methods Planning. Morgan KaufmannPublishers.261fiMinton, Bresina, & DrummondSacerdoti, E. (1975). nonlinear nature plans. Proceedings Fourth International Joint Conference Artificial Intelligence.Sacerdoti, E. (1977). Structure Plans Behavior. American Elsivier, New York.Tate, A. (1974). Interplan: plan generation system deal interactionsgoals. Tech. rep. Memo MIP-R-109, Univ. Edinburgh, Machine IntelligenceResearch Unit.Tate, A. (1977). Generating project networks. Proceedings Fifth InternationalJoint Conference Artificial Intelligence.Veloso, M., Perez, M., & Carbonell, J. (1990). Nonlinear planning parallel resourceallocation. Proceedings Workshop Innovative Approaches Planning,Scheduling Control.Waldinger, R. (1975). Achieving several goals simultaneously. Machine Intelligence 8.Ellis Harwood, Ltd.Warren, D. (1974). Warplan: system generating plans. Tech. rep. Memo 76, Computational Logic Dept., School AI, Univ. Edinburgh.262fiJournal Artificial Intelligence Research 2 (1995) 411-446Submitted 11/94; published 4/95Rerepresenting Restructuring Domain Theories:Constructive Induction ApproachSteven K. DonohoLarry A. RendellDepartment Computer Science, Univeristy Illinois405 N. Mathews Ave., Urbana, IL 61801 USAdonoho@cs.uiuc.edurendell@cs.uiuc.eduAbstractTheory revision integrates inductive learning background knowledge combiningtraining examples coarse domain theory produce accurate theory.two challenges theory revision theory-guided systems face. First,representation language appropriate initial theory may inappropriateimproved theory. original representation may concisely express initial theory,accurate theory forced use representation may bulky, cumbersome,dicult reach. Second, theory structure suitable coarse domain theory mayinsucient fine-tuned theory. Systems produce small, local changestheory limited value accomplishing complex structural alterations mayrequired.Consequently, advanced theory-guided learning systems require exible representationexible structure. analysis various theory revision systems theory-guidedlearning systems reveals specific strengths weaknesses terms two desiredproperties. Designed capture underlying qualities system, new system usestheory-guided constructive induction. Experiments three domains show improvementprevious theory-guided systems. leads study behavior, limitations,potential theory-guided constructive induction.1. IntroductionInductive learners normally use training examples, also use background knowledge. Effectively integrating knowledge induction widely studied research problem. work date area theory revisionknowledge given coarse, perhaps incomplete incorrect, theory problem domain,training examples used shape initial theory refined, accuratetheory (Ourston & Mooney, 1990; Thompson, Langley, & Iba, 1991; Cohen, 1992; Pazzani& Kibler, 1992; Baffes & Mooney, 1993; Mooney, 1993). develop exiblerobust approach problem learning data theory knowledgeaddressing two following desirable qualities:Flexible Representation. theory-guided system utilize knowledge contained initial domain theory without adhere closely initialtheory's representation language.Flexible Structure. theory-guided system unnecessarily restrictedstructure initial domain theory.c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiDonoho & Rendellgiving precise definitions terms, motivate work intuitively.1.1 Intuitive Motivationfirst desirable quality, exibility representation, arises theory representation appropriate describing coarse, initial domain theory may inadequatefinal, revised theory. initial domain theory may compact conciseone representation, accurate theory may quite bulky cumbersome representation. Furthermore, representation best expressing initial theory maybest carrying refinements. helpful refinement step may clumsymake initial representation yet carried quite simply another representation.simple example, coarse domain theory may expressed logical conjunctionN conditions met. accurate theory, though, oneN conditions holds. Expressing accurate theory DNF representationused describe initial theory would cumbersome unwieldy (Murphy & Pazzani,1991). Furthermore, arriving final theory using refinement operators suitableDNF (drop-condition, add-condition, modify-condition) would cumbersome task.M-of-N representation adopted, refinement simply involves empiricallyfinding appropriate M, final theory expressed concisely (Baffes & Mooney,1993).Similarly, second desirable quality, exibility structure, arises theorystructure suitable coarse domain theory may insucient fine-tunedtheory. order achieve desired accuracy, restructuring initial theory maynecessary. Many theory revision systems act making series local changes,lead behavior two extremes. first extreme rigidly retain backbonestructure initial domain theory, allowing small, local changes. Figure 1 illustratessituation. Minor revisions made { conditions added, dropped,modified { refined theory trapped backbone structure initialtheory. local changes needed, techniques proven useful (Ourston& Mooney, 1990), often required. required, systems oftenmove extreme; drop entire rules groups rules build entirenew rules groups rules scratch replace them. Thus restructure,forfeit valuable knowledge process. ideal theory revision system would gleanknowledge theory substructures cannot fixed small, local changesuse restructured theory.intuitive illustration, consider piece software \almost works." Sometimesmade useful local operations: fixing couple bugs, addingneeded subroutine, on. cases, though, piece software \almostworks" fact far full working order. may need redesigned restructured.mistake one extreme try fix program like making series patchesoriginal code. mistake extreme discard original program withoutlearning anything start scratch. best approach would examineoriginal program see learned design use knowledgeredesign. Likewise, attempting improve coarse domain theory serieslocal changes may yield little improvement theory trapped initial412fiRerepresenting Restructuring Domain TheoriesInitial TheorybcjehRefined Theorynk lbcp q rjw ef gnk lupvrf gFigure 1: Typical theory revision allows limited structural exibility. Although conditions added, dropped, modified, revised theory muchconstrained structure initial theory.structure. render original domain theory useless; careful analysisinitial domain theory give valuable guidance design best final theory.illustrated Figure 2 many substructures taken initialtheory adapted use refined theory. Information initial theoryused, structure revised theory restricted structureinitial theory.Initial TheorybcnjehRefined Theoryk lkp q rlfgehpuvf g q rf gFigure 2: exible structural modification. revised theory taken many substructures initial theory adapted recombined use,structure revised theory restricted structureinitial theory.413fiDonoho & Rendell1.2 Terminologypaper, training data consist examples classified vectors feature/value pairs. assume initial theory set conditions combined usingoperators AND, OR, indicating one classes. unreasonablebelieve theories always form, covers much existing theory revisionresearch.work intended informal exploration exible representation exiblestructure. Flexible representation means allowing theory revised using representation language initial theory. example exible representationintroduction new operator combining features | operator usedinitial theory. Section 1.1 example given introducing M-of-N operatorrepresent theory originally expressed DNF. Flexible structure means limitingrevision theory series small, incremental modifications. examplebreaking theory components using building blocksconstruction new theory.Constructive induction process whereby training examples redescribed usingnew set features. new features combinations original features. Biasknowledge may used construction new features. subtle pointspeak exible representation, referring representationdomain theory, training data. Although phrase \change representation"often applied constructive induction, refers change data. paper,term exible representation reserved change theory representation. Thussystem performing constructive induction (changing feature languagedata) without exhibiting exible representation (changing representation theory).1.3 OverviewTheory revision constructive induction embody complementary aspects machinelearning research community's ultimate goals. Theory revision uses data improvetheory; constructive induction use theory improve data facilitate learning.paper present theory-guided constructive induction approach addressestwo desirable qualities discussed Section 1.1. initial theory analyzed, newfeatures constructed based components theory. constructed featuresneed expressed representational language initial theoryrefined better match training examples. Finally, standard inductive learningalgorithm, C4.5 (Quinlan, 1993), applied redescribed examples.begin analyzing landmark theory revision learning systems exhibited exibility handling domain theory part played performance. analysis, extract guidelines system design applydesign limited system. effort integrate learning theory data,borrow heavily theory revision, multistrategy learning, constructive induction communities, guidelines system design fall closest classical constructiveinduction methods. central focus paper presentation \another newsystem" rather study exible representation structure, manifestationprevious work, guidance future design.414fiRerepresenting Restructuring Domain TheoriesSection 2 gives context work analyzing previous research uence work. Section 3 explores Promoter Recognition domain demonstratesrelated theory revision systems behave domain. Section 4, guidelinestheory-guided constructive induction presented. guidelines synthesispositive aspects related research, address two desirable qualities, exibilityrepresentation exibility structure. Section 4 also presents specific theory-guidedconstructive induction algorithm instantiation guidelines set forth earliersection. Results experiments three domains given Section 5 followeddiscussion strengths theory-guided constructive induction Section 6. Section 7 presents experimental analysis limits applicability simple algorithmfollowed discussion limitations future directions work Section 8.2. Context Related WorkAlthough work bears resemblance form objective many papers constructive induction (Michalski, 1983; Fu & Buchanan, 1985; Utgoff, 1986; Schlimmer, 1987;Drastal & Raatz, 1989; Matheus & Rendell, 1989; Pagallo & Haussler, 1990; Ragavan &Rendell, 1993; Hirsh & Noordewier, 1994), theory revision (Ourston & Mooney, 1990; Feldman, Serge, & Koppel, 1991; Thompson et al., 1991; Cohen, 1992; Pazzani & Kibler, 1992;Baffes & Mooney, 1993), multistrategy approaches (Flann & Dietterich, 1989; Towell,Shavlik, & Noordeweir, 1990; Dzerisko & Lavrac, 1991; Bloedorn, Michalski, & Wnek, 1993;Clark & Matwin, 1993; Towell & Shavlik, 1994), focus upon handful systems, significant, underlying similarities work. sectionanalyze Miro, Either, Focl, LabyrinthK , Kbann, Neither-MofN, Grendeldiscuss related underlying contributions relationship perspective.2.1Miro(Drastal & Raatz, 1989) seminal work knowledge-guided constructive induction. takes knowledge low-level features interact uses knowledgeconstruct high-level features training examples. standard learning algorithmrun examples described using new features. domain theory usedshift bias induction problem (Utgoff, 1986). Empirical results showeddescribing examples high-level, abstract terms improved learning accuracy.Miro approach provides means utilizing knowledge domain theory withoutrestricted structure theory. Substructures domain theoryused construct high-level features standard induction algorithm arrangeconcept. constructed features used are, others ignored,others combined low-level features, still others may used differentlymultiple contexts. end result knowledge domain theory utilized,structure final theory restricted structure initial theory.Miro provides exible structure.Another benefit Miro-like techniques applied even partialdomain theory exists, i.e., domain theory specifies high-level featureslink together domain theory specifies high-level featuresothers. One Miro's shortcomings provided means making minor changesMiro415fiDonoho & Rendelldomain theory rather constructed features exactly domain theoryspecified. Also representation Miro's constructed features primitive | eitherexample met conditions high-level feature not. example Miro'sbehavior given Section 3.2.2.2,, LabyrinthKEither FoclEither(Ourston & Mooney, 1990), LabyrinthK (Thompson et al., 1991),Focl (Pazzani & Kibler, 1992) systems represent broad spectrum theory revisionwork. make steps toward effective integration background knowledge inductivelearning. Although systems many superficial differences regard supervised/unsupervised learning, concept description language, etc., share underlyingprinciple incrementally revising initial domain theory series local changes.discuss Either representative class systems. Either's theoryrevision operators include: removing unwanted conditions rule, adding needed conditions rule, removing rules, adding totally new rules. Either first classifiestraining examples according current theory. misclassified, seeks repairtheory applying theory revision operator result correct classificationpreviously misclassified examples without losing correct examples. Thusseries local changes made allow improvement accuracy trainingset without losing examples previously classified correctly.Either-type methods provide simple yet powerful tools repairing many importantcommon faults domain theories, fail meet qualities exible representation exible structure. theory revision operators make small, localmodifications existing domain theory, final theory constrained similarstructure initial theory. accurate theory significantly different structure initial theory, systems forced one two extremes discussedSection 1. first extreme become trapped local maximum similarinitial theory unable reach global maximum local changes made.extreme drop entire rules groups rules replace newrules built scratch thus forfeiting knowledge contained domain theory.Also, Either carries theory revision steps representation initialtheory. Consequently, representation final theory initialtheory. Another representation may appropriate revised theoryone initial theory comes, facilities provided accommodate this.advanced theory revision system would combine locally acting strengths Eithertype systems exibility structure exibility representation. exampleEither's behavior given Section 3.3.2.3KbannNeither-MofNKbann system (Towell et al., 1990; Towell & Shavlik, 1994) makes unique contributions theory revision work. Kbann takes initial domain theory described symbolicallylogic creates neural network whose structure initial weights encode theory.Backpropagation (Rumelhart, Hinton, & McClelland, 1986) applied refinement tool fine-tuning network weights. Kbann empirically shown give416fiRerepresenting Restructuring Domain Theoriessignificant improvement many theory revision systems widely-used PromoterRecognition domain. Although work different implementation Kbann,abstract ideologies similar.One Kbann's important contributions takes domain theory one representation (propositional logic) translates less restricting representation (neuralnetwork). logic appropriate representation initial domain theorypromoter problem, neural network representation convenient refiningtheory expressing best revised theory. change representationKbann's real source power. Much attention given fact Kbann combines symbolic knowledge subsymbolic learner, combination viewedgenerally means implementing important change representation. maychange representation gives Kbann power, necessarily specificsymbolic/subsymbolic implementation. Thus Kbann system embodies higher-levelprinciple allowing refinement occur appropriate representation.alternative representation Kbann's source power, question must raisedwhether actual Kbann implementation always best means achievinggoal. neural network representation may expressive required. Accordingly, backpropagation often refinement power needed. Thus Kbann maycarry excess baggage translating neural net representation, performing expensivebackpropagation, extracting symbolic rules refined network. Although fullextent Kbann's power may needed problems, many important problems maysolvable applying Kbann's principles symbolic level using less expensive tools.Neither-MofN (Baffes & Mooney, 1993), descendant Either, second examplesystem allows theory revised representationinitial theory. domain theory input Neither-MofN expressed propositionallogic AND/OR tree. Neither-MofN interprets theory less rigidly |rule true time N conditions true. Initially set equal N (allconditions must true rule true), one theory refinement operatorlower particular rule. end result examples close enoughpartial match initial theory accepted. Neither-MofN, since built uponEither framework, also includes Either-like theory revision operators: add-condition,drop-condition, etc.Thus Neither-MofN allows revision take place representation appropriaterevision appropriate concisely expressing best refined theory. NeitherMofN achieved results comparable Kbann Promoter Recognition domain,suggests change representation two systems sharegive power rather particular implementation. Neither-MofN alsodemonstrates small amount representational exibility sometimes enough.M-of-N representation employs big change original representationneural net representation Kbann employs yet achieves similar resultsarrives much quickly Kbann (Baffes & Mooney, 1993).shortcoming Neither-MofN since acts making local changesinitial theory, still become trapped structure initial theory. advancedtheory revision system would incorporate Neither-MofN's Kbann's exibility417fiDonoho & Rendellrepresentation allow knowledge-guided theory restructuring. ExamplesNeither-MofN's behavior given Sections 3.4 3.5.2.4'sKbannGrendelCohen (1992) analyzes class theory revision systems draws insightful conclusions. One \generality [in theory interpretation] comes expense power."draws principle fact system Either Focl treats everydomain theory therefore must treat every domain theory general way. argues rather applying general refinement strategyevery problem, small set refinement strategies available narrowenough gain leverage yet narrow apply single problem. Cohenpresents Grendel, toolbox translators transforms domain theoryexplicit bias. translator interprets domain theory different way,appropriate interpretation applied given problem.apply Cohen's principle representation domain theories. domaintheories translated representation, general, adaptable representation used order accommodate general case. comesexpense higher computational costs possibly lower accuracy due overfitstemming unbridled adaptability. neural net representation Kbanntranslates domain theories allows 1) measure partial match domain theory 2) different parts domain theory weighted differently 3) conditions addeddropped domain theory. options adaptability probablynecessary problems may even detrimental. options Kbann alsorequire computationally expensive backpropagation method.representation used Neither-MofN adaptable Kbann's |allow individual parts domain theory weighted differently. NeitherMofN runs quickly Kbann small problems probably matches evensurpasses Kbann's accuracy many domains | domains fine-grained weightingunfruitful even detrimental. toolbox theory rerepresentation translators analogousGrendel would allow domain theory translated representationappropriate forms adaptability.2.5 Outlook Summarysummary, brie reexamine exible representation exible structure, twodesirable qualities set forth Section 1. consider various systems exemplifysubset desirable qualities.Kbann Neither-MofN interpreted theory exibly originalrepresentation allowed revised theory adaptable representation.final, refined theory often many exceptions rule; may tolerate partialmatches missing pieces evidence; may weight evidence heavilyevidence. Kbann's Neither-MofN's new representation mayconcise, appropriate representation initial theory, newrepresentation allows concise expression otherwise cumbersome final theory.cases principle exible representation.418fiRerepresenting Restructuring Domain TheoriesStandard induction programs quite successful building concise theorieshigh predictive accuracy target concept concisely expressed usingoriginal set features. can't, constructive induction means creatingnew features target concept concisely expressed. Miro usesconstructive induction take advantage strengths domain theorystandard induction. Knowledge theory guides construction appropriatenew features, standard induction structures concise descriptionconcept. Thus Miro-like construction coupled standard induction providesready powerful means exibly restructuring knowledge containedinitial domain theory. case principle exible structure.following section introduce DNA Promoter Recognition domain orderillustrate tangibly systems discussed integrate knowledgeinduction.3. Demonstrations Related Worksection introduces Promoter Recognition domain (Harley, Reynolds, & Noordewier,1990) brie illustrates Miro-like system, Either, Kbann, NeitherMofN behave domain. implemented Miro-like system promoter domain; versions Either Neither-MofN available Ray Mooney's group;Kbann's behavior described analyzing (Towell & Shavlik, 1994). chose promoter domain non-trivial, real-world problem number theoryrevision researchers used test work (Ourston & Mooney, 1990; Thompsonet al., 1991; Wogulis, 1991; Cohen, 1992; Pazzani & Kibler, 1992; Baffes & Mooney, 1993;Towell & Shavlik, 1994). promoter domain one three domains evaluatework, theory-guided constructive induction, Section 5.3.1 Promoter Recognition Domainpromoter sequence region DNA marks beginning gene. example promoter recognition domain region DNA classified either promoternon-promoter. illustrated Figure 3, examples consist 57 features representing sequence 57 DNA nucleotides. feature take values A,G,C,representing adenine, guanine, cytosine, thymine corresponding DNA position.features labeled according position p-50 p+7 (there zeroposition). notation \p-N " denotes nucleotide N positions upstreambeginning gene. goal predict whether sequence promoternucleotides. total 106 examples available: 53 promoters 53 non-promoters.promoter recognition problem comes initial domain theory shown Figure 4 (quoted almost verbatim Towell Shavlik's entry UCI Machine LearningRepository). theory states promoter sequences must two regions makecontact protein must also acceptable conformation pattern.four possibilities contact region minus 35 (35 nucleotides upstream beginning gene). match four possibilities satisfy minus 35contact condition, thus joined disjunction. Similarly, four possibilities419fiDonoho & RendellDNA Sequencep-50p+7C G CFigure 3: instance promoter domain consists sequence 57 nucleotideslabeled p-50 p+7. nucleotide take values A,G,C,representing adenine, guanine, cytosine, thymine.contact region minus 10 four acceptable conformation patterns. Figure 5gives pictorial presentation portions theory. 106 examplesdataset, none matched domain theory exactly, yielding accuracy 50%.3.2MiroPromoter DomainMiro-like system promoter domain would use rules Figure 4 construct new high-level features DNA segment. Figure 6 shows example this.DNA segment shown position p-38 position p-30. minus 35 rulestheory also shown, four new features (feat minus35 feat minus35 D)constructed DNA segment, one minus 35 rule. new features feat minus35 feat minus35 value 1 DNA fragmentmatches first fourth minus 35 rules. Likewise, feat minus35 B feat minus35 Cvalue 0 DNA fragment match second thirdrules. Furthermore, since four minus 35 rules joined disjunction, new feature,feat minus35 all, created group would value 1 least oneminus 35 rules matches.New features would similarly created minus 10 rules conformationrules, standard induction algorithm could applied. implemented Mirolike system; Figure 7 gives example theory created it. (Drastal's original Miro usedcandidate elimination algorithm (Mitchell, 1977) underlying induction algorithm.used C4.5 (Quinlan, 1993).) opposed theory revision systems incrementallymodify domain theory, Miro broken theory componentsfashioned components new theory using standard induction program. ThusMiro exhibited exible structure principle domain { restrictedway structure initial theory. Rather, Miro exploited strengthsstandard induction concisely characterize training examples using new features.420fiRerepresenting Restructuring Domain TheoriesPromoters region protein (RNA polymerase) must make contacthelical DNA sequence must valid conformation two piecescontact region spatially align. Prolog notation used.promoter :- contact, conformation.two regions "upstream" beginning geneRNA polymerase makes contact.contact:- minus_35, minus_10.following rules describe compositions possible contact regions.minus_35minus_35minus_35minus_35:- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.:p-36=t, p-35=t, p-34=g,p-32=c, p-31=a.:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.minus_10minus_10minus_10minus_10:- p-14=t, p-13=a, p-12=t, p-11=a, p-10=a, p-9=t.:p-13=t, p-12=a,p-10=a,p-8=t.:p-13=t, p-12=a, p-11=t, p-10=a, p-9=a, p-8=t.:p-12=t, p-11=a,p-7=t.following rules describe sequences produce acceptable conformations.conformation :- p-47=c,p-18=t,p-1=c.conformation :- p-45=a,conformation :- p-49=a,conformation :- p-45=a,p-15=t,p-46=a, p-45=a, p-43=t, p-42=t, p-40=a, p-39=c, p-22=g,p-16=c, p-8=g, p-7=c, p-6=g, p-5=c, p-4=c, p-2=c,p-44=a, p-41=a.p-44=t, p-27=t, p-22=a, p-18=t, p-16=t, p-15=g, p-1=a.p-41=a, p-28=t, p-27=t, p-23=t, p-21=a, p-20=a, p-17=t,p-4=t.Figure 4: initial domain theory recognizing promoters (from Towell Shavlik).weakness Miro displays example allows exibility representationtheory. representation features constructed Miro basicallyall-or-none representation initial theory; either DNA segment matched rule,not.3.3EitherPromoter DomainEither-like system refines initial promoter theory dropping addingconditions rules. simulated Either turning M-of-N option Neitherran promoter domain. Figure 8 shows refined theory produced usingrandomly selected training set size 80. initial promoter domain theorylend revision small, local changes, Either limited success.421fiDonoho & RendellDNA Sequencep-50p+7Contact minus_35Contact minus_10-37 -36 -35 -34 -33 -32 -31CG C*-14 -13 -12 -11 -10 -9-8 -7* ** G * C*** G C** G C* ********Figure 5: contact portion theory. four possibilitiesminus 35 minus 10 portions theory. \*" matches nucleotide.conformation portion theory spread display pictorially.run, program exhibited second behavioral extreme discussed Section 1;entirely removed groups rules tried build new rules replacelost. minus 10 conformation rules essentially removed, new rulesadded minus 35 group. new minus 35 rules contain conditionp-12=t previously found minus 10 group condition p-44=a previously foundconformation group.Either's behavior example direct result lack exibility representation exibility structure. dicult transform minus 10 conformationrules something useful initial representation using Either's locally-acting operators. Either handles dropping sets rules, losing knowledge,attempting rediscover lost knowledge empirically. end result lossknowledge lower optimal accuracy shown later Section 5.3.4KbannPromoter DomainFigure 9, modeled figure Towell Shavlik (1994), shows setup Kbannnetwork promoter theory. slot along bottom represents one nucleotideDNA sequence. node first level bottom embodies singledomain rule, higher levels encode groups rules final concept top.links shown figure ones initially high-weighted. net next filledfully connected low-weight links. Backpropagation applied refinenetwork's weights.422fiRerepresenting Restructuring Domain TheoriesDNA segment fragment::::p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=t, p-30=t:::minus 35 group rules corresponding constructed features:minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.minus 35 :p-36=t, p-35=t, p-34=g,p-32=c, p-31=a.minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.feat minus35 = 1feat minus35 B = 0feat minus35 C = 0feat minus35 = 1feat minus35 = (feat minus35 _ feat minus35 B _ feat minus35 C _ feat minus35 D) = 1Figure 6: example feature construction Miro-like system. constructedfeatures first fourth rules minus 35 group true (value = 1)DNA segment matches rules. constructed featureentire group, feat minus35 all, true four minus 35 rules joineddisjunction.feat_minus10_all01promoterfeat_conf_B01feat_minus35_D0non-promoterpromoter1promoterFigure 7: example theory created Miro-like system. DNA segment recognizedpromoter matches minus 10 rules, second conformationrule, fourth minus 35 rule.neural net representation appropriate domain propositionallogic representation initial theory. allows measurement partial matchweighting links way subset rule's conditions enough surpassnode's threshold. also allows variable weightings different parts theory; therefore, predictive nucleotides weighted heavily, slightly predictivenucleotides weighted less heavily. Kbann limited exibility structure.refined network result series incremental modificationsinitial network, fundamental restructuring theory embodies unlikely. Kbann423fiDonoho & Rendellpromoter :- contact, conformation.contact:- minus_35, minus_10.minus_35minus_35minus_35minus_35minus_35minus_35::::::-p-35=t,p-36=t,p-36=t,p-34=g,p-34=g,p-35=t,p-34=g.p-33=a, p-32=c.p-32=c, p-50=c.p-12=t.p-44=a.p-47=g.minus_10 :- true.conformation :- true.Figure 8: revised theory produce Either.promotercontactconformationminus_35p-50minus_10DNA Sequencep+7Figure 9: setup Kbann network promoter theory.limited finding best network fundamental structure imposedinitial theory.One Kbann's advantages uses standard learning algorithm foundation. Backpropagation widely used consequently improved previousresearchers. Theory refinement tools built ground use standardtool tangentially suffer invent methods handling standardproblems overfit, noisy data, etc. wealth neural net experience resourcesavailable Kbann user; neural net technology advances, Kbann technologypassively advance it.424fiRerepresenting Restructuring Domain Theories3.5Neither-MofNPromoter Domainrefines initial promoter theory dropping adding conditions rules also allowing conjunctive rules true subsetconditions true. ran Neither-MofN randomly selected training set size80, Figure 10 shows refined promoter theory produced. theory expressed9 M-of-N rules would require 30 rules using propositional logic, initial theory'srepresentation. importantly, unclear system using initial representation would reach 30-rule theory initial theory. Thus M-of-N representationadopted allows concise expression final theory also facilitatesrefinement process.Neither-MofNpromoter :- 2 ( contact, conformation ).contact:- 2 ( minus_35, minus_10 ).minus_35 :- 2 ( p-36=t, p-35=t, p-34=g,p-32=c, p-31=a ).minus_35 :- 5 ( p-36=t, p-35=t, p-34=g, p-33=a, p-32=c).minus_10minus_10minus_10minus_10::::-2262((p-13=t,( p-14=t, p-13=a,(p-13=t,p-12=t, p-11=a,p-12=a,p-10=a,p-12=t, p-11=a, p-10=a, p-9=tp-12=a,p-10=a,p-7=tp-8=t).).).p-34=g ).conformation :- true.Figure 10: revised theory produced Neither-MofN.Neither-MofN displays exibility representation allowing M-of-N interpretation original propositional logic, allow fine-grained refinementKbann. allow measure partial match, Kbann could weightpredictive features heavily. example, minus 35 rules, perhaps p-36=tpredictive DNA segment promoter p-34=g thereforeweighted heavily. Neither-MofN simply counts number true conditionsrule; therefore, every condition weighted equally. Kbann's fine-grained weighting mayneeded domains others. may actually detrimental domains.advanced theory revision system offer range representations.Like Kbann, Neither-MofN limited exibility structure. refinedtheory reached series small, incremental modifications initial theoryprecluding fundamental restructuring. Neither-MofN therefore limited findingbest theory fundamental structure initial theory.4. Theory-Guided Constructive Inductionfirst half section present guidelines theory-guided constructive inductionsummarize work discussed Sections 2 3. remainder section425fiDonoho & Rendellpresents algorithm instantiates guidelines. evaluate algorithmSection 5.4.1 Guidelinesfollowing guidelines synthesis strengths previously discussed relatedwork.Miro, new features constructed using components domaintheory. new features combinations existing features, final theorycreated applying standard induction algorithm training examples describedusing new features. allows knowledge gleaned initial theorywithout forcing final theory conform initial theory's backbone structure.takes full advantage domain theory building high-level featuresoriginal low-level features. also takes advantage strength standard induction| building concise theories high predictive accuracy target conceptconcisely expressed using given features.Either, constructed features modifiable various operatorsact locally, adding dropping conjuncts constructed feature.Kbann Neither-MofN, representation constructed featuresneed exact representation initial theory given. example,initial theory may given set rules written propositional logic.new feature constructed rule, need boolean featuretelling whether conditions met; example may countmany conditions rule met. allows final theory formedexpressed representation suitable representationinitial theory.Like Grendel, complete system offer library interpreters allowingdomain theory translated range representations differing adaptability. One interpreter might emulate Miro strictly translating domain theoryboolean constructed features. Another interpreter might construct featurescount number satisfied conditions corresponding component domain theory thus providing measure partial match. Still another interpretermight construct features weighted sums satisfied conditions.weights could refined empirically examining set training examples. Thusappropriate amount expressive power applied given problemwithout incurring unnecessary expense.4.2 Specific Interpretersection describes algorithm limited instantiation guidelinesdescribed. algorithm intended demonstration distillation synthesisprinciples embodied previous landmark systems. contains main module,Tgci described Figure 12, specific interpreter, Tgci1 described Figure 11.main module Tgci redescribes training testing examples calling Tgci1426fiRerepresenting Restructuring Domain Theoriesapplies C4.5 redescribed examples (just Miro applied candidateelimination algorithm examples redescribing them). Tgci1 viewedsingle interpreter potential Grendel-like toolbox. takes input single exampledomain theory expressed AND/OR tree one shown Figure 13.returns new vector features example measure partial matchexample theory. Thus creates new features components domain theoryMiro, measures partial match, allows exibility representinginformation contained initial theory Kbann Neither-MofN. Oneaspect guidelines 4.1 appear algorithm Either's locallyacting operators adding dropping conditions portion theory.following two paragraphs explain detail workings Tgci1 Tgcirespectively.Given: example E domain theory root node R. domaintheory AND/OR/NOT tree leaves conditionstested true false.Return: pair P = (F; F ) F top feature measuring partialmatch E whole domain theory, F vector new features measuring partial match E various parts subparts domain theory.1. R directly testable condition, return P=(1,<>) R true EP=(-1,<>) R false E .2. n = number children R3. child Rj R, call Tgci1(Rj ,E ) store respective resultsPj = (Fj ; Fj ).4. major operator R OR, Fnew = MAX (Fj ).Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).P5. major operator R AND, Fnew = ( nj=1 Fj )=n.Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).6. major operator R NOT, Fnew = ,1 F1 .Return P = (Fnew ; F1).Figure 11: Tgci1 algorithmTgci1 algorithm, given Figure 11, recursive. inputs example Edomain theory root node R. ultimately returns redescription E formvector new features F . also returns value F called top feature usedintermediate calculations described below. base case occurs domain theorysingle leaf node (i.e., R simple condition). case (Line 1), Tgci1 returnstop feature 1 condition true -1 condition false. new featuresreturned base case would simply duplicate existing features.427fiDonoho & Rendelldomain theory single leaf node, Tgci1 recursively calls R's children(Line 3). child R, Rj , processed, returns vector new features Fj (whichmeasures partial match example j th child R various subparts).also returns top feature Fj included Fj marked specialmeasures partial match example whole j th child R. nchildren, result Line 3 n vectors new features, F1 Fn , n top features, F1Fn . operator node R (Line 4), Fnew , new feature creatednode, maximum Fj . Thus Fnew measures closely best R's children comeconditions met example. vector new features returnedcase concatenation Fnew new features R's children. operatornode R (Line 5), Fnew average Fj . Thus Fnew measures closelyR's children group come conditions met example.vector new features returned case concatenation Fnew newfeatures R's children. operator node R (Line 6), Rone child, Fnew F1 negated. Thus Fnew measures extent conditionsR's child met example.Given: set training examples Etrain , set testing examples Etest ,domain theory root node R.Return: Learned concept accuracy testing examples.1. example Ei 2 Etrain , call Tgci1(R,Ei) returns Pi =(Fi ; Fi). Etrain,new = fFig.2. example Ei 2 Etest, call Tgci1(R,Ei). returns Pi =(Fi ; Fi). Etest,new = fFi g.3. Call C4.5 training examples Etrain,new testing examplesEtest,new . Return decision tree accuracy Etest,new .Figure 12: Tgci algorithmTgci1 called twice two different examples domain theory,two vectors new features size. Furthermore, corresponding featuresmeasure match corresponding parts domain theory. Tgci main moduleFigure 12 takes advantage creating redescribed example sets inputexample sets. Line 1 redescribes example training set producing new trainingset. Line 2 testing set. Line 3 runs standard induction programC4.5 redescribed example sets. returned decision tree easily interpretedexamining new features used part domain theorycorrespond to.4.3Tgci1Examplesexample Tgci1 interpreter works, consider toy theory shownFigure 13. Tgci1 redescribes input example constructing new feature node428fiRerepresenting Restructuring Domain Theoriesinput theory. Consider situation input example matches conditions A,B, C E. Tgci1 evaluates children Node 6, gets valuesF1 = 1, F2 = 1, F3 = ,1, F4 = 1, F5 = ,1. Since operator Node 6 AND, Fnewaverage values received children, 0.20 ((1 + 1 + (,1) + 1 + (,1))=5 =0:20). Likewise, condition G matchs F H, Fnew Node 5 value0.33 (,1 ((1 + (,1) + (,1))=3)) two three matching conditions Node 7 givevalue ,0:33, negated Node 5. Since Node 2 disjunction,new feature measures best partial match two children value 0.33(MAX(0.20,0.33)), on.132456B C E7F G H8J K9L NFigure 13: example theory form AND/OR tree might usedinterpreter generate constructed features.Figure 14 shows Tgci1 redescribes particular DNA segment using minus 35rules promoter theory. partial DNA segment shown along four minus 35rules new feature constructed rule (We given new features namessimplify illustration). first rule, four six nucleotides match; therefore, DNA segment feat minus35 value 0.33 ((1+1+1+1+(,1)+(,1))=6).second rule, four five nucleotides match; therefore, feat minus35 Bvalue 0.60. two minus 35 rules joined disjunction original domain theory, feat minus35 all, new feature constructedgroup, takes maximum value four children; therefore, feat minus35value 0.60 feat minus35 B value 0.60, highest group. Intuitively, feat minus35 represents best partial match grouping | extentdisjunction partially satisfied. results running Tgci1 DNAsequence set redescribed training examples. redescribed example valuefeat minus35 feat minus35 D, feat minus35 all, nodes promoter domain theory. training set essentially redescribed using new feature vectorderived information contained domain theory. form, off-the-shelfinduction program applied new example set.Anomalous situations created Tgci1 gives \good score" seeminglybad example bad score good example. Situations also createdlogically equivalent theories give different scores single example. occur429fiDonoho & RendellDNA segment fragment::::p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=c, p-32=a, p-31=a, p-30=t:::minus 35 group rules corresponding constructed features:minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.minus 35 :p-36=t, p-35=t, p-34=g,p-32=c, p-31=a.minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.feat minus35 = 0.33feat minus35 B = 0.60feat minus35 C = 0.33feat minus35 = 0.20feat minus35 = max(feat minus35 A, feat minus35 B, feat minus35 C, feat minus35 D) = 0.60Figure 14: example Tgci1 generates constructed features portionpromoter domain theory DNA segment. Four conditions firstminus 35 rule match DNA segment; therefore, constructed featurerule value 0.33 ((1 + 1 + 1 + 1 + (,1) + (,1))=6). Feat minus35 all,new feature entire minus 35 group takes maximum valuechildren thus embodying best partial match group.biased favor situations matched conditions desirable,matched conditions necessarily better. Eliminating anomalieswould remove bias.Tgci15. Experiments Analysissection presents results applying theory-guided constructive induction threedomains: promoter domain (Harley et al., 1990), primate splice-junction domain (Noordewier, Shavlik, & Towell, 1992), gene identification domain (Craven & Shavlik,1995). case Tgci1 interpreter applied domain's theory examplesorder redescribe examples using new features. C4.5 (Quinlan, 1993)applied redescribed examples.5.1 Promoter DomainFigure 15 shows learning curve theory-guided constructive induction promoterdomain accompanied curves Either, LabyrinthK , Kbann, Neither-MofN.Following methodology described Towell Shavlik [1994], set 106 examplesrandomly divided training set size 80 testing set size 26. learningcurve created training subsets training set size 8, 16, 24, : : : 72, 80,using 26 examples testing. curves Either, LabyrinthK , Kbanntaken Ourston Mooney (1990), Thompson, Langley, Iba (1991), Towell430fiRerepresenting Restructuring Domain TheoriesShavlik (1994) respectively obtained similar methodology1 . curveforTgci average 50 independent random data partitions given along 95%confidence ranges. Neither-MofN program obtained Ray Mooney's groupused generating Neither-MofN curve using 50 data partitionsused Tgci2.42.540EITHERLabyrinth-kNEITHER-MofNKBANNTGCI95% confidence NEITHER-MofN95% confidence TGCI37.53532.53027.5% Error2522.52017.51512.5107.552.500510 15 20 25 30 35 40 45 50 55 60 65 70 75 80Size Training SampleFigure 15: Learning curves theory-guided constructive induction systemspromoter domain.Tgci showed improvement Either LabyrinthK portions curvealso performed better Kbann Neither-MofN except smallest training sets. Confidence intervals available Either, LabyrinthK ,1.used testing set size 25 use conformation portion domain theory.testing set LabyrinthK always consisted 13 promoters 13 non-promoters.2. Baffes Mooney (1993) report slightly better learning curve Neither-MofN obtained,communication Paul Baffes, think difference caused random selectiondata partitions.Either431fiDonoho & RendellKbann, pairwise comparison Neither-MofN, improvement Tgcisignificant 0.0005 level confidence training sets size 48 larger.Structure Initial Promoter Theory100%firstconf.rule100%firstminus_35rule100%secondminus_35rule100%thirdminus_35rule100%fourthminus_35rule100%firstminus_10rule100%secondminus_10rule100%thirdminus_10rule100%secondconf.rule100%thirdconf.rule100%fourthconf.rule100%fourthminus_10ruleStructure Revised Promoter Theory>20%secondminus_35rule>33%firstminus_10rule>33%secondminus_10rule>33%thirdminus_10rule>33%fourthminus_10ruleFigure 16: revised theory produced theory-guided constructive induction borrowed substructures initial theory, whole restricted structure.Figure 16 compares initial promoter theory theory created Tgci. ReasonsTgci's improvement inferred figure. Tgci extracted components original theory helpful restructuredconcise theory. Neither Kbann Neither-MofN facilitates radical extractionrestructuring. seen leaf nodes, new theory also measures partial matchexample components original theory. aspect similar KbannNeither-MofN.Part Tgci's improvement Kbann Neither-MofN may due knowledge/bias con ict latter two systems, situation revision biases con ictknowledge way undo knowledge's benefits. occurwhenever detailed knowledge opened revision using set examples. revisionguided examples rather examples interpreted set432fiRerepresenting Restructuring Domain Theoriesalgorithmic biases. Biases useful absence knowledge may undo goodknowledge improperly applied. Yet biases developed perfected pure induction often unquestioningly applied revision theories. biases governingdropping conditions Neither-MofN reweighting conditions Kbann mayneutralizing promoter theory's potential. speculate conductedexperiments allowed bias-guided dropping adding conditions within Tgci.found techniques actually reduced accuracy domain.4542.54037.5c4.5backpropagationKBANNTGCI95% confidence TGCIdomain theory3532.5% Error3027.52522.52017.51512.5107.5020406080100120140160180200Size Training SampleFigure 17: Learning curves Tgci systems primate splice-junction domain.5.2 Primate Splice-junction Domainprimate splice-junction domain (Noordewier et al., 1992) involves analyzing DNAsequence identifying boundaries introns exons. Exons partsDNA sequence kept splicing; introns spliced out. task involves placing433fiDonoho & Rendellgiven boundary one three classes: intron/exon boundary, exon/intron boundary, neither. imperfect domain theory available 39.0% error rateentire set available examples.Figure 17 shows learning curves C4.5, backpropagation, Kbann, Tgciprimate splice-junction domain. results Kbann backpropagation takenTowell Shavlik (1994). curves plain C4.5 Tgci algorithmcreated training sets size 10,20,30,...90,100,120,...200 testing set size800. curves C4.5 Tgci average 40 independent data partitions.comparison made Neither-MofN implementation obtainedcould handle two-class concepts. training sets larger 200, Kbann, Tgci,backpropagation performed similarly.accuracy Tgci appears slightly worse Kbann perhaps significantly. Kbann's advantage Tgci ability assign fine-grained weightingsindividual parts domain theory. Tgci's advantage Kbann abilityeasily restructure information contained domain theory. speculateKbann's capability assign fine-grained weights outweighted somewhat rigid structuring domain theory. Theory-guided constructive induction advantagespeed Kbann C4.5, underlying learner, runs much quicklybackpropagation, Kbann's underlying learning algorithm.5.3 Gene Identification Domaingene identification domain (Craven & Shavlik, 1995) involves classifying given DNAsegment coding sequence (one codes protein) non-coding sequence.domain theory available gene identification domain; therefore, createdartificial domain theory using information organisms may favor certain nucleotidetriplets others gene coding. domain theory embodies knowledge DNAsegment likely gene coding segment triplets coding-favoring tripletstriplets noncoding-favoring triplets. decision triplets codingfavoring, noncoding-favoring, favored neither, made empiricallyanalyzing makeup 2500 coding 2500 noncoding sequences. specific artificial domain theory used described Online Appendix 1.Figure 18 shows learning curves C4.5 Tgci gene identification domain.original domain theory yields 31.5% error. curves created trainingexample sets size 50,200,400,...2000 testing separate example set size 1000.curves average 40 independent data partitions.partial curve given Neither-MofN became prohibitively slowlarger training sets. promoter domain training sets smaller 100,Tgci Neither-MofN ran comparable speeds (approximately 10 seconds Sun4workstation). domain Tgci ran approximately 2 minutes larger training sets.Neither-MofN took 21 times long Tgci training sets size 400, 69 timeslong size 800, 144 times long size 1200. Consequently, Neither-MofN'scurve extends 1200 represents five randomly selected data partitions.reasons, solid comparison Neither-MofN Tgci cannot madecurves, appears Tgci's accuracy slightly better. speculate Neither434fiRerepresenting Restructuring Domain Theories4542.540TGCI95% confidence TGCIC4.5NEITHER-MofNdomain theory37.5% Error3532.53027.52522.52002004006008001000 1200 1400 1600 1800 2000Number training examplesFigure 18: Learning curves Tgci systems gene identification domain.'s slightly lower accuracy partially due fact revises theorycorrectly classify training examples. result theory likely overfitstraining examples. Tgci need explicitly avoid overfit handledunderlying learner.MofN5.4 Summary Experimentsgoal paper present new technique rather understandbehavior landmark systems, distill strengths, synthesize simplesystem, Tgci. evaluation algorithm shows accuracy roughly matchesexceeds predecessors. promoter domain, Tgci showed sizable improvementmany published results. splice-junction domain, Tgci narrowly falls shortKbann's accuracy. gene identification domain, Tgci outperforms Neither-MofN.domains Tgci greatly improves original theory alone C4.5 alone.435fiDonoho & Rendellfaster closest competitors. Tgci runs much 100 times fasterlarge datasets. strict quantitative comparison speeds TgciKbann made 1) backpropagation known much slowerdecision trees (Mooney, Shavlik, Towell, & Gove, 1989), 2) Kbann uses multiple hiddenlayers makes training time even longer (Towell & Shavlik, 1994), 3) TowellShavlik (1994) point run Kbann must made multiple timesdifferent initial random weights, whereas single run Tgci sucient.Overall, experiments support two claims paper: First, accuracy Tgcisubstantiates delineation system strengths terms exible theory representationexible theory structure, since characterization basis algorithm'sdesign. Second, Tgci's combination speed accuracy suggest unnecessary computational complexity avoided synthesizing strengths landmark systems.following section take closer look strengths theory-guided constructiveinduction.TgciNeither-MofN6. Discussion Strengthsnumber strengths theory-guided constructive induction discussed withincontext Tgci algorithm used experiments.6.1 Flexible Representationdiscussed Section 1, many domains representation appropriateinitial theory may appropriate refined theory. theory-guided constructive induction allows translation initial theory different representation,accommodate domains. experiments paper representationneeded allowed measurement partial match domain theory. Tgci1accomplished simply counting matching features propagating information theory appropriately. Either LabyrinthK easily affordmeasure partial match therefore appropriate problems bestrepresentation final theory initial theory. Kbann allowsfiner-grained measurement partial match Neither-MofN work,price paid computational complexity. theory-guided constructive induction framework allows variety potential tools varying degrees granularitypartial match, although one tool used experiments.6.2 Flexible Structurediscussed Section 2.5, strength existing induction programs fashioning concisehighly predictive description concept target concept conciselydescribed given features. Consequently, value domain theory liesoverall structure. feature language sucient, induction program buildgood overall theory structure. Instead, value domain theory lies informationcontains redescribe examples using high-level features. high-levelfeatures facilitate concise description target concept. Systems EitherNeither-MofN reach final theory series modifications initial436fiRerepresenting Restructuring Domain Theoriestheory hope gain something keeping theory's overall structure intact. initialtheory suciently close accurate theory, method works, often clingingstructure hinders full exploitation domain theory. Theory-guided constructiveinduction provides means fully exploiting information domain theorystrengths existing induction programs. Figure 16 Section 5.1 gives comparisonstructure initial promoter theory structure revised theory producedtheory-guided constructive induction. Substructures borrowed, revisedtheory whole restructured.6.3 Use Standard Induction Underlying Learnertheory-guided constructive induction uses standard induction programunderlying learner, need reinvent solutions overfit avoidance, multi-classconcepts, noisy data, etc. Overfit avoidance widely studied standard induction,many standard techniques exist. system modifies theory accommodateset training examples must also address issue overfit training examples.many theory revision systems existing overfit avoidance techniques cannot easily adapted,problem must addressed scratch. Theory-guided constructive inductiontake advantage full range previous work overfit avoidance standard induction.multiple theory parts available multi-class concepts, interpreterrun multiple theory parts, resulting new feature sets combined.primate splice-junction domain presented Section 5.2 three classes: intron/exonboundaries, exon/intron boundaries, neither. Theories given intron/exonexon/intron. theories used create new features, new featuresconcatenated together learning. Interpreters Tgci1 also trivially handlenegation domain theory.6.4 Use Theory FragmentsTheory-guided constructive induction limited using full domain theories.part theory available, used. demonstrate this, three experimentsrun fragments promoter domain theory used. firstexperiment, four minus 35 rules used. Five features constructed | onefeature rule additional feature group. Similar experimentsrun minus 10 group conformation group.Figure 19 gives learning curves three experiments along curves entire theory theory (C4.5 using original features). Although conformationportion theory gives significant improvement C4.5, minus 35minus 10 portions theory give significant improvements performance. Thus evenpartial theories theory fragments used theory-guided constructive inductionyield sizable performance improvements.use theory fragments explored means evaluating contributiondifferent parts theory. Figure 19, conformation portion theory shownyield improvement. could signal knowledge engineer knowledgeconveyed portion theory useful learnerpresent form.437fiDonoho & Rendell45C4.5conformation portion theoryminus_10 portion theoryminus_35 portion theorywhole theory42.54037.53532.530% Error27.52522.52017.51512.5107.552.500510 15 20 25 30 35 40 45 50 55 60 65 70 75 80Size Training SampleFigure 19: Learning curves theory-guided constructive induction fragmentspromoter domain theory. minus 35 portion theory, minus 10portion theory, conformation portion theory usedseparately feature construction. Curves also given full theoryC4.5 alone comparison.6.5 Use Multiple TheoriesTheory-guided constructive induction use multiple competing even incompatibledomain theories. multiple theories exist, theory-guided constructive induction providesnatural means integrating way extract best theories.Tgci1 would called input theory producing new features. Next, newfeatures simply pooled together induction program selects amongfashioning final theory. seen small scale promoter domain.438fiRerepresenting Restructuring Domain Theories% ErrorFigure 4 minus 35 rules subsume minus 35 rules. According entryUCI Database, \the biological evidence inconclusive respectcorrect specificity." handled simply using four possibilities, selectionuseful knowledge left induction program.Tgci could also used evaluate contributions competing theoriesused evaluate theory fragments above. knowledge engineer could use evaluationguide revision synthesis competing theories.2522.52017.51512.5107.552.50TGCI using C4.5TGCI using LFC95% confidence LFC510 15 20 25 30 35 40 45 50 55 60 65 70 75 80Size Training SampleFigure 20: Theory-guided constructive induction Lfc C4.5 underlyinglearning system. Theory-guided constructive induction use inductivelearner underlying learning component. Therefore, sophisticatedunderlying induction programs improve accuracy.6.6 Easy Adoption New TechniquesSince theory-guided constructive induction use standard induction methodunderlying learner, improvements made standard induction, theory-guided constructive induction passively improves. demonstrate this, tests also run Lfc(Ragavan & Rendell, 1993) underlying induction program. Lfc decision treelearner performs example-based constructive induction looking ahead combinations features. Characteristically, Lfc improves accuracy moderate numberexamples. Figure 20 shows resulting learning curve along C4.5 Tgci curve.curves average 50 separate runs data partitions usedprogram. pairwise comparison improvement Lfc C4.5 significant0.025 level confidence training sets size 72 80. sophisticated underlyinginduction programs improve accuracy.439fiDonoho & Rendell7. Testing Limits Tgcipurpose section explore performance theory-guided constructiveinduction theory revision problems ranging easy dicult. easy problemsunderlying concept embodied training testing examples matches domaintheory fairly closely; therefore, examples match domain theory fairlyclosely. dicult problems underlying concept embodied examplesmatch domain theory well examples either. Although manyfactors determine diculty individual problem, aspect important component worth exploring. experiment section intended relate rangesdiculty amount improvement produced Tgci.Since number factors affect problem diculty chose theory revisionproblems experiment variations single problem.able hold factors constant vary closeness match domaintheory. wanted avoid totally artificial domains, chose startpromoter domain create \new" domains perverting example set.\new" domains created perverting examples original promoterproblem either closely match promoter domain theory less closely matchpromoter domain theory. positive examples altered. example, one domaincreated 30% fewer matches domain theory original promoterdomain follows: feature value given example examined see matchedpart theory. so, 30% probability, randomly reassigned new valueset possible values feature. end result set examples 30%fewer matches domain theory original example set3. experimentnew domains created 10%, 30%, 60%, 90% fewer matches.features, multiple values may match theory different disjunctstheory specify different values single feature. example, referring backFigure 4, feature p-12 matches two minus 10 rules value anothertwo rules value t. single feature might accidentally match one parttheory fact example whole closely matches another part theory.cases these, true matches separated accidental matches examiningpart theory clearly matched example whole expectingmatch part theory.New domains closely matched theory created similar manner.example, domain created 30% fewer mismatches domain theoryoriginal promoter domain follows: feature value given example examinedsee matched corresponding part theory. not, 30% probability,reassigned value matched theory. end result set examples30% mismatches domain theory eliminated. experimentnew domains created 30%, 60%, 90% fewer mismatches.Ten different example sets created level closeness domain theory:10%, 30%, 60%, 90% fewer matches, 30%, 60%, 90% fewer mismatches. total, fortyexample sets created matched original theory less closely original3. precisely, would slightly matches 30% fewer matches featureswould randomly reassigned back original matching value.440fiRerepresenting Restructuring Domain Theories55504540% Error35C4.5TGCI302520151050-100-80-60-40-20020406080100Closeness theoryFigure 21: Seven altered promoter domains created, three closely matchedtheory original domains four less closely matched.100 x-axis indicates domain positive examples matchdomain theory 100%. negative 100 indicates domain matchpositive examples domain theory purely chance. accuracyC4.5 Tgci plotted different levels proximity domaintheory.example set, thirty example sets created matched original theoryclosely original example set. example sets tested using leaveone-out methodology using C4.5 Tgci algorithm. results summarizedFigure 21. x-axis measure theory proximity { closeness example setdomain theory. \0" x-axis indicates change original promoter examples.\100" x-axis means positive example exactly matches domain theory.\-100" x-axis means match feature value positive example441fiDonoho & Rendelldomain theory totally chance4 . datapoint Figure 21 result averagingaccuracies ten example sets level theory proximity (exceptpoint zero accuracy exact original promoter examples).One notable portion Figure 21 section 0 60 x-axis. Domainsregion greater trivial level mismatch domain theorymoderate mismatch. region Tgci's best performance.domains, Tgci achieves high accuracy standard learner, C4.5, using originalfeature set gives mediocre performance. second region examine -60 0x-axis level mismatch ranges moderate extreme. regionTgci's performance falls improvement original feature set remains highshown Figure 22 plots improvement Tgci C4.5. final tworegions notice greater 60 less -60 x-axis. levelmismatch theory examples becomes trivially small (x-axis greater 60),C4.5 able pick theory's patterns leading high accuracy approachesTgci's. level mismatch becomes extreme (x-axis less -60) theory giveslittle help problem-solving resulting similarly poor accuracy methods.summary, shown Figure 22 variants promoter problem wide rangetheory proximity (centered around real promoter problem) theory-guidedconstructive induction yields sizable improvement standard learners.2017.5error difference% Error1512.5107.552.50-100-80-60-40-20020406080100Closeness theoryFigure 22: difference error C4.5 Tgci different levels proximityexample set domain theory.4. scale 0 -100 left half graph may directly comparable scale 0 100right half graph since equal number matches mismatchesoriginal examples.442fiRerepresenting Restructuring Domain Theories8. Conclusiongoal paper present another new system, ratherstudy two qualities exible representation exible structure. capabilitiesintended frame reference analyzing theory-guided systems. two principlesprovide guidelines purposeful design. distilled essence systemsMiro, Kbann, Neither-MofN, theory-guided constructive inductionnatural synthesis strengths. experiments demonstrated evensimple application two principles effectively integrate theory knowledgetraining examples. Yet much room improvement; two principles couldquantified made precise, implementations proceedexplored refined.Quantifying representational exibility one step. Section 4 gave three degreesexibility: one measured exact match theory, one counted number matchingconditions, one allowed weighted sum matching conditions. amountexibility quantified, finer-grained degrees exibility explored.accuracy assorted domains evaluated function representationalexibility.Finer-grained structural exibility would advantageous. presented systemsmake small, incremental modifications theory lacking structural exibility. Yettheory-guided constructive induction falls extreme, perhaps allowing excessivestructural exibility. Fortunately, existing induction tools capable fashioning simpleyet highly predictive theory structures problem features suitably high-level.Nevertheless, approaches explored take advantage structureinitial theory without unduly restricted it.strength discussed Section 6.5 given attention. Althoughpromoter domain gives small example synthesizing competing theories,explored domain entire competing, inconsistent theories availablesynthesizing knowledge given multiple experts. point made Section 6.4Tgci use theory fragments evaluate contribution different partstheory. also explored further.exploration bias standard induction, Utgoff (1986) refers biases rangingweak strong incorrect correct. strong bias restricts conceptsrepresented weak bias thus providing guidance learning.bias becomes stronger, may also become incorrect ruling useful concept descriptions. similar situation arises theory revision | theory representation languageinappropriately rigid may impose strong, incorrect bias revision. languageallows adaptability along many dimensions may provide weak bias. Grendellike toolbox would allow theory translated range representationsvarying dimensions adaptability. Utgoff advocates starting strong, possibly incorrect bias shifting appropriately weak correct bias. Similarly, theory couldtranslated successively adaptable representations appropriate biasfound. implemented single tool; many open problems remain along lineresearch.443fiDonoho & Rendellconverse relationship theory revision constructive induction warrantsexamination | theory revision uses data improve theory; constructive inductionuse theory improve data facilitate learning. Since long-term goal machinelearning use data, inference, theory improve them, believeconsideration related methods beneficial, particularlyresearch area strengths lacks.analysis landmark theory revision theory-guided learning systems ledtwo principles exible representation exible structure. theory-guidedconstructive induction based upon high-level principles, simple yet achievesgood accuracy. principles provide guidelines future work, yet discussed above,principles imprecise call exploration.Acknowledgementswould like thank Geoff Towell, Kevin Thompson, Ray Mooney, Jeff Mahoneyassistance getting datapoints Kbann, LabyrinthK , Either. wouldalso like thank Paul Baffes making Neither program available advicesetting program's parameters. thank anonymous reviewers constructivecriticism earlier draft paper. gratefully acknowledge supportwork DoD Graduate Fellowship NSF grant IRI-92-04473.ReferencesBaffes, P., & Mooney, R. (1993). Symbolic revision theories M-of-N rules.Proceedings 1993 IJCAI.Bloedorn, E., Michalski, R., & Wnek, J. (1993). Multistrategy constructive induction:AQ17-MCI. Proceeding second international workshop multistrategy learning.Clark, P., & Matwin, S. (1993). Using qualitative models guide inductive learning.Proceedings 1993 International Conference Machine Learning.Cohen, W. (1992). Compiling prior knowledge explicit bias. Proceedings1992 International Conference Machine Learning.Craven, M. W., & Shavlik, J. W. (1995). Investigating value good input representation. Computational Learning Theory Natural Learning Systems, 3. Forthcoming.Drastal, G., & Raatz, S. (1989). Empirical results learning abstraction space.Proceedings 1989 IJCAI.Dzerisko, S., & Lavrac, N. (1991). Learning relations noisy examples: empiricalcomparison LINUS FOIL. Proceedings 1991 International ConferenceMachine Learning.444fiRerepresenting Restructuring Domain TheoriesFeldman, R., Serge, A., & Koppel, M. (1991). Incremental refinement approximatedomain theories. Proceedings 1991 International Conference MachineLearning.Flann, N., & Dietterich, T. (1989). study explanation-based methods inductivelearning. Machine Learning, 4, 187{226.Fu, L. M., & Buchanan, B. G. (1985). Learning intermediate concepts constructinghierarchical knowledge base. Proceedings 1985 IJCAI.Harley, C., Reynolds, R., & Noordewier, M. (1990). Creators original promoter dataset.Hirsh, H., & Noordewier, M. (1994). Using background knowledge improve inductivelearning DNA sequences. Tenth IEEE Conference AI Applications SanAntonio, TX.Matheus, C. J., & Rendell, L. A. (1989). Constructive induction decision trees.Proceedings 1989 IJCAI.Michalski, R. S. (1983). theory methodology inductive learning. Artificial Intelligence, 20 (2), 111{161.Mitchell, T. (1977). Version spaces: candidate elimination approach rule learning.Proceedings 1977 IJCAI.Mooney, R. J. (1993). Induction unexplained: Using overly-general domain theoriesaid concept learning. Machine Learning, 10 (1), 79{110.Mooney, R. J., Shavlik, J. W., Towell, G. G., & Gove, A. (1989). experimental comparison symbolic connectionist learning algorithms. Proceedings 1989IJCAI.Murphy, P., & Pazzani, M. (1991). ID2-of-3: Constructive induction M-of-N conceptsdiscriminators decision trees. Proceedings 1991 International ConferenceMachine Learning.Noordewier, M., Shavlik, J., & Towell, G. (1992). Donors original primate splice-junctiondataset.Ourston, D., & Mooney, R. (1990). Changing rules: comprehensive approach theoryrefinement. Proceedings 1990 National Conference Artificial Intelligence.Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. MachineLearning, 5 (1), 71{99.Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. MachineLearning, 9 (1), 57{94.Quinlan, J. R. (1993). C4.5: Programs Machine Learning. San Mateo, CA: MorganKaufmann.445fiDonoho & RendellRagavan, H., & Rendell, L. (1993). Lookahead feature construction learning hard concepts. Proceedings 1993 International Conference Machine Learning.Rumelhart, D. E., Hinton, G. E., & McClelland, J. L. (1986). general frameworkparallel distributed processing. Rumelhart, D. E., & McClelland, J. L. (Eds.),Parallel Distributed Processing: Explorations Microarchitecture Cognition,Volume I. Cambridge, MA: MIT Press.Schlimmer, J. C. (1987). Learning representation change. Kaufmann, M. (Ed.),Proceedings 1987 National Conference Artificial Intelligence.Thompson, K., Langley, P., & Iba, W. (1991). Using background knowledge conceptformation. Proceedings 1991 International Conference Machine Learning.Towell, G., & Shavlik, J. (1994). Knowledge-based artificial neural networks. ArtificialIntelligence, 70, 119{165.Towell, G., Shavlik, J., & Noordeweir, M. (1990). Refinement approximately correctdomain theories knowledge-based neural networks. Proceedings 1990National Conference Artificial Intelligence.Utgoff, P. E. (1986). Shift bias inductive concept learning. Michalski, Carbonell,& Mitchell (Eds.), Machine Learning, Vol. 2, chap. 5, pp. 107{148. San Mateo, CA:Morgan Kaufmann.Wogulis, J. (1991). Revising relational domain theories. Proceedings 1991 International Conference Machine Learning.446fiJournal Artificial Intelligence Research 2 (1994) 1-32Submitted 4/94; published 8/94System Induction Oblique Decision TreesSreerama K. MurthySimon KasifSteven SalzbergDepartment Computer ScienceJohns Hopkins University, Baltimore, MD 21218 USAmurthy@cs.jhu.edukasif@cs.jhu.edusalzberg@cs.jhu.eduAbstractarticle describes new system induction oblique decision trees. system,OC1, combines deterministic hill-climbing two forms randomization find goodoblique split (in form hyperplane) node decision tree. Oblique decisiontree methods tuned especially domains attributes numeric, althoughadapted symbolic mixed symbolic/numeric attributes. present extensive empirical studies, using real artificial data, analyze OC1's abilityconstruct oblique trees smaller accurate axis-parallel counterparts. also examine benefits randomization construction obliquedecision trees.1. IntroductionCurrent data collection technology provides unique challenge opportunity automated machine learning techniques. advent major scientific projectsHuman Genome Project, Hubble Space Telescope, human brain mapping initiative generating enormous amounts data daily basis. streams datarequire automated methods analyze, filter, classify presentingdigested form domain scientist. Decision trees particularly useful tool context perform classification sequence simple, easy-to-understand testswhose semantics intuitively clear domain experts. Decision trees usedclassification tasks since 1960s (Moret, 1982; Safavin & Landgrebe, 1991).1980's, Breiman et al.'s book classification regression trees (CART) Quinlan's work ID3 (Quinlan, 1983, 1986) provided foundations becomelarge body research one central techniques experimental machine learning.Many variants decision tree (DT) algorithms introduced last decade.Much work concentrated decision trees node checks valuesingle attribute (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1986, 1993a).Quinlan initially proposed decision trees classification domains symbolic-valuedattributes (1986), later extended numeric domains (1987). attributesnumeric, tests form xi > k, xi one attributes examplek constant. class decision trees may called axis-parallel, testsnode equivalent axis-parallel hyperplanes attribute space. exampledecision tree given Figure 1, shows tree partitioningcreates 2-D attribute space.c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiFigure 1: left side figure shows simple axis-parallel tree uses two attributes.right side shows partitioning tree creates attribute space.Researchers also studied decision trees test node uses booleancombinations attributes (Pagallo, 1990; Pagallo & Haussler, 1990; Sahami, 1993)linear combinations attributes (see Section 2). Different methods measuringgoodness decision tree nodes, well techniques pruning tree reduce overfittingincrease accuracy also explored, discussed later sections.paper, examine decision trees test linear combination attributesinternal node. precisely, let example take form X = x1 ; x2; : : :; xd; CjCj class label xi 's real-valued attributes.1 test nodeform:Xai xi + ad+1 > 0(1)i=1a1 ; : : :; ad+1 real-valued coecients. tests equivalent hyperplanes oblique orientation axes, call class decision trees obliquedecision trees. (Trees form also called \multivariate" (Brodley & Utgoff,1994). prefer term \oblique" \multivariate" includes non-linear combinations variables, i.e., curved surfaces. trees contain linear tests.) clearsimply general form axis-parallel trees, since setting ai = 0coecients one, test Eq. 1 becomes familiar univariate test. Noteoblique decision trees produce polygonal (polyhedral) partitionings attributespace, axis-parallel trees produce partitionings form hyper-rectanglesparallel feature axes.intuitively clear underlying concept defined polygonal space partitioning, preferable use oblique decision trees classification.example, exist many domains one two oblique hyperplanesbest model use classification. domains, axis-parallel methods ap1. constraint x1 ; : : : ; xd real-valued necessarily restrict oblique decision treesnumeric domains. Several researchers studied problem converting symbolic (unordered)domains numeric (ordered) domains vice versa; e.g., (Breiman et al., 1984; Hampson & Volper,1986; Utgoff & Brodley, 1990; Van de Merckt, 1992, 1993). keep discussion simple, however,assume attributes numeric values.2fiFigure 2: left side shows simple 2-D domain two oblique hyperplanes defineclasses. right side shows approximation sort axis-paralleldecision tree would create model domain.proximate correct model staircase-like structure, oblique tree-buildingmethod could capture tree smaller accurate.2 Figure 2gives illustration.Breiman et al. first suggested method inducing oblique decision trees 1984. However, little research trees relatively recently (Utgoff& Brodley, 1990; Heath, Kasif, & Salzberg, 1993b; Murthy, Kasif, Salzberg, & Beigel, 1993;Brodley & Utgoff, 1994). comparison existing approaches given detailSection 2. purpose study review strengths weaknesses existingmethods, design system combines strengths overcomes weaknesses, evaluate system empirically analytically. main contributionsconclusions study follows:developed new, randomized algorithm inducing oblique decision treesexamples. algorithm extends original 1984 work Breiman et al.Randomization helps significantly learning many concepts.algorithm fully implemented oblique decision tree induction systemavailable Internet. code retrieved Online Appendix 1paper (or anonymous ftp ftp://ftp.cs.jhu.edu/pub/oc1/oc1.tar.Z).randomized hill-climbing algorithm used OC1 ecientexisting randomized oblique decision tree methods (described below). fact,current implementation OC1 guarantees worst-case running timeO(log n) times greater worst-case time inducing axis-parallel trees (i.e.,O(dn2 log n) vs. O(dn2)).ability generate oblique trees often produces small trees comparedaxis-parallel methods. underlying problem requires oblique split, oblique2. Note though given oblique tree may fewer leaf nodes axis-parallel tree|whichmean \smaller"|the oblique tree may cases larger terms information content,increased complexity tests node.3fiMurthy, Kasif & Salzbergtrees also accurate axis-parallel trees. Allowing tree-building systemuse oblique axis-parallel splits broadens range domainssystem useful.remaining sections paper follow outline: remainder sectionbrie outlines general paradigm decision tree induction, discusses complexity issues involved inducing oblique decision trees. Section 2 brie reviewsexisting techniques oblique DT induction, outlines limitations approach,introduces OC1 system. Section 3 describes OC1 system detail. Section 4describes experiments (1) compare performance OC1 severalaxis-parallel oblique decision tree induction methods range real-world datasets(2) demonstrate empirically OC1 significantly benefits randomizationmethods. Section 5, conclude discussion open problems directionsresearch.1.1 Top-Down Induction Decision TreesAlgorithms inducing decision trees follow approach described Quinlan top-downinduction decision trees (1986). also called greedy divide-and-conquermethod. basic outline follows:1. Begin set examples called training set, . examples belongone class, halt.2. Consider tests divide two subsets. Score test accordingwell splits examples.3. Choose (\greedily") test scores highest.4. Divide examples subsets run procedure recursively subset.Quinlan's original model considered attributes symbolic values; model,test node splits attribute values. Thus test attributethree values three child nodes, one corresponding value.algorithm considers possible tests chooses one optimizes pre-definedgoodness measure. (One could also split symbolic values two subsets values,gives many choices split examples.) explain next, obliquedecision tree methods cannot consider tests due complexity considerations.1.2 Complexity Induction Oblique Decision TreesOne reason relatively papers problem inducing oblique decision treesincreased computational complexity problem compared axis-parallelcase. two important issues must addressed. context top-downdecision tree algorithms, must address complexity finding optimal separatinghyperplanes (decision surfaces) given node decision tree. optimal hyperplaneminimize impurity measure used; e.g., impurity might measured totalnumber examples mis-classified. second issue lower bound complexityfinding optimal (e.g., smallest size) trees.4fiFigure 3: n points dimensions(n d), n distinct axis-parallel splits,,2d nd distinct d-dimensional oblique splits. shows distinctoblique axis-parallel splits two specific points 2-D.Let us first consider issue complexity selecting optimal oblique hyperplane single node tree. domainn training instances, described using,real-valued attributes, 2d nd distinct d-dimensional oblique splits; i.e.,hyperplanes3 divide training instances uniquely two nonoverlapping subsets.upper bound derives observation every subset size n pointsdefine d-dimensional hyperplane, hyperplane rotated slightly2d directions divide set points possible ways. Figure 3 illustratesupper limits two points two dimensions. axis-parallel splits, ndistinct possibilities, axis-parallel methods C4.5 (Quinlan, 1993a) CART(Breiman et al., 1984) exhaustively search best split node. problemsearching best oblique split therefore much dicult searchingbest axis-parallel split. fact, problem NP-hard.precisely, Heath (1992) proved following problem NP-hard: givenset labelled examples, find hyperplane minimizes number misclassifiedexamples hyperplane. result implies methodfinding optimal oblique split likely exponential cost(assuming P 6= NP ).,Intuitively, problem impractical enumerate 2d nd distinct hyperplaneschoose best, done axis-parallel decision trees. However, non-exhaustivedeterministic algorithm searching hyperplanes prone getting stucklocal minima.3. Throughout paper, use terms \split" \hyperplane" interchangeably refer testnode decision tree. first usage standard (Moret, 1982), refers facttest splits data two partitions. second usage refers geometric form test.5fiMurthy, Kasif & Salzberghand, possible define impurity measures problemfinding optimal hyperplanes solved polynomial time. example, oneminimizes sum distances mis-classified examples, optimal solutionfound using linear programming methods (if distance measured along one dimensiononly). However, classifiers usually judged many points classify correctly,regardless close decision boundary point may lie. Thus standardmeasures computing impurity base calculation discrete number examplescategory either side hyperplane. Section 3.3 discusses several commonlyused impurity measures.let us address second issue, complexity building small tree.easy show problem inducing smallest axis-parallel decision treeNP-hard. observation follows directly work Hyafil Rivest (1976). Noteone generate smallest axis-parallel tree consistent trainingset polynomial time number attributes constant. doneusing dynamic programming branch bound techniques (see Moret (1982) severalpointers). tree uses oblique splits, clear, even fixed numberattributes, generate optimal (e.g., smallest) decision tree polynomial time.suggests complexity constructing good oblique trees greateraxis-parallel trees.also easy see problem constructing optimal (e.g., smallest) obliquedecision tree NP-hard. conclusion follows work Blum Rivest (1988).result implies dimensions (i.e., attributes) problem producing3-node oblique decision tree consistent training set NP-complete.specifically, show following decision problem NP-complete: given trainingset n examples Boolean attributes, exist 3-node neural networkconsistent ? easy show following question NP-complete:given training set , exist 3-leaf-node oblique decision tree consistentT?result complexity considerations, took pragmatic approach tryinggenerate small trees, looking smallest tree. greedy approach usedOC1 virtually decision tree algorithms implicitly tries generate small trees.addition, easy construct example problems optimal split nodelead best tree; thus philosophy embodied OC1 find locallygood splits, spend excessive computational effort improving qualitysplits.2. Previous Work Oblique Decision Tree Inductiondescribing OC1 algorithm, brie discuss existing oblique DTinduction methods, including CART linear combinations, Linear Machine DecisionTrees, Simulated Annealing Decision Trees. also methods inducetree-like classifiers linear discriminants node, notably methods usinglinear programming (Mangasarian, Setiono, & Wolberg, 1990; Bennett & Mangasarian,1992, 1994a, 1994b). Though methods find optimal linear discriminantsspecific goodness measures, size linear program grows fast number6fiInduction Oblique Decision Treesinduce split node decision tree:Normalize values attributes.L=0(TRUE)L = L+1Let current split sL v c, v = Pdi=1 ai xi.= 1; : : :;= -0.25,0,0.25Search maximizes goodness split v , (ai + ) c.Let , settings result highest goodness 3 searches.ai = ai , , c = c , .Perturb c maximize goodness sL , keeping a1 ; : : :; ad constant.jgoodness(sL) - goodness(sL,1)j exit loop.Eliminate irrelevant attributes fa1; : : :; ad g using backward elimination.Convert sL split un-normalized attributes.Return better sL best axis-parallel split split .Figure 4: procedure used CART linear combinations (CART-LC) nodedecision tree.instances number attributes. also less closely related workalgorithms train artificial neural networks build decision tree-like classifiers (Brent,1991; Cios & Liu, 1992; Herman & Yeung, 1992).first oblique decision tree algorithm proposed CART linear combinations (Breiman et al., 1984, chapter 5). algorithm, referred henceforth CART-LC,important basis OC1. Figure 4 summarizes (using Breiman et al.'s notation)CART-LC algorithm node decision tree. core idea CARTLC algorithm finds value maximizes goodness split.idea also used OC1, explained detail Section 3.1.describing CART-LC, Breiman et al. point still much roomdevelopment algorithm. OC1 represents extension CART-LCincludes significant additions. addresses following limitations CART-LC:CART-LC fully deterministic. built-in mechanism escaping localminima, although minima may common domains. Figure 5shows simple example CART-LC gets stuck.CART-LC produces single tree given data set.CART-LC sometimes makes adjustments increase impurity split.feature probably included allow escape local minima.upper bound time spent node decision tree. haltsperturbation changes impurity , impurity mayincrease decrease, algorithm spend arbitrarily long time node.7fiMurthy, Kasif & Salzberg1OC121Initial Loc.12CART-LC122Figure 5: deterministic perturbation algorithm CART-LC fails find correctsplit data, even starts location best axis-parallelsplit. OC1 finds correct split using one random jump.Another oblique decision tree algorithm, one uses different approachCART-LC, Linear Machine Decision Trees (LMDT) system (Utgoff & Brodley, 1991;Brodley & Utgoff, 1992), successor Perceptron Tree method (Utgoff, 1989;Utgoff & Brodley, 1990). internal node LMDT tree Linear Machine (Nilsson,1990). training algorithm presents examples repeatedly node linearmachine converges. convergence cannot guaranteed, LMDT uses heuristicsdetermine node stabilized. make training stable even settraining instances linearly separable, \thermal training" method (Frean, 1990)used, similar simulated annealing.third system creates oblique trees Simulated Annealing Decision Trees(SADT) (Heath et al., 1993b) which, like OC1, uses randomization. SADT uses simulatedannealing (Kirkpatrick, Gelatt, & Vecci, 1983) find good values coecientshyperplane node tree. SADT first places hyperplane canonicallocation, iteratively perturbs coecients small random amounts. Initially, temperature parameter high, SADT accepts almost perturbationhyperplane, regardless changes goodness score. However, system\cools down," changes improve goodness split likely accepted.Though SADT's use randomization allows effectively avoid local minima,compromises eciency. runs much slower either CART-LC, LMDT OC1,sometimes considering tens thousands hyperplanes single node finishesannealing.experiments Section 4.3 include results showing methodsperform three artificial domains.next describe way combine strengths methods mentioned,avoiding problems. algorithm, OC1, uses deterministic hill climbingtime, ensuring computational eciency. addition, uses two kindsrandomization avoid local minima. limiting number random choices,algorithm guaranteed spend polynomial time node tree. addition,randomization produced several benefits: example, means algorithm8fiInduction Oblique Decision Treesfind split set examples :Find best axis-parallel split . Let impurity split.Repeat R times:Choose random hyperplane H .(For first iteration, initialize H best axis-parallel split.)Step 1: impurity measure improve, do:Perturb coecients H sequence.Step 2: Repeat J times:Choose random direction attempt perturb H direction.reduces impurity H , go Step 1.Let I1 = impurity H . I1 < , set = I1 .Output split corresponding .Figure 6: Overview OC1 algorithm single node decision tree.produce many different trees data set. offers possibility newfamily classifiers: k-decision-tree algorithms, example classifiedmajority vote k trees. Heath et al. (1993a) shown k-decision tree methods(which call k-DT) consistently outperform single tree methods classificationaccuracy main criterion. Finally, experiments indicate OC1 eciently findssmall, accurate decision trees many different types classification problems.3. Oblique Classifier 1 (OC1)section discuss details oblique decision tree induction system OC1.part description, include:method finding coecients hyperplane tree node,methods computing impurity goodness hyperplane,tree pruning strategy,methods coping missing irrelevant attributes.Section 3.1 focuses complicated algorithmic details; i.e. questionfind hyperplane splits given set instances two reasonably \pure" nonoverlapping subsets. randomized perturbation algorithm main novel contributionOC1. Figure 6 summarizes basic OC1 algorithm, used node decisiontree. figure explained following sections.3.1 Perturbation algorithmOC1 imposes restrictions orientation hyperplanes. However, orderleast powerful standard DT methods, first finds best axis-parallel (univariate)split node looking oblique split. OC1 uses oblique splitimproves best axis-parallel split.44. pointed (Breiman et al., 1984, Chapter 5), make sense use oblique splitnumber examples node n less almost equal number features d,9fiMurthy, Kasif & Salzbergsearch strategy space possible hyperplanes defined procedureperturbs current hyperplane H new location. exponentialnumber distinct ways partition examples hyperplane, proceduresimply enumerates unreasonably costly. two main alternativesconsidered past simulated annealing, used SADT system (Heathet al., 1993b), deterministic heuristic search, CART-LC (Breiman et al., 1984).OC1 combines two ideas, using heuristic search finds local minimum,using non-deterministic search step get local minimum. (The nondeterministic step OC1 simulated annealing, however.)start explaining perturb hyperplane split training setnode decision tree. Let n number examples , numberattributes (or dimensions) example, k number categories.write Tj = (xj 1 ; xj 2; : : :; xjd; Cj ) j th example training set , xjivalue attribute Cj category label. defined PEq. 1, equationcurrent hyperplane H node decision tree written di=1(a x )+ad+1 = 0.Pdsubstitute point (an example) Tj equation H , get i=1 (aixji )+ ad+1 = Vj ,sign Vj tells us whether point Tj hyperplane H ;i.e., Vj > 0, Tj H . H splits training set perfectly, pointsbelonging category sign Vj . i.e., sign(Vi) = sign(Vj ) iffcategory(Ti) = category(Tj ).OC1 adjusts coecients H individually, finding locally optimal value onecoecient time. key idea introduced Breiman et al. works follows.Treat coecient variable, treat coecients constants.Vj viewed function . particular, condition Tj HequivalentVj > 0= Uj> xxjm , Vj defjm(2)assuming xjm > 0, ensure normalization. Using definition Uj ,point Tj H > Uj , otherwise. plugging pointsequation, obtain n constraints value .problem find value satisfies many constraintspossible. (If constraints satisfied, perfect split.) problemeasy solve optimally: simply sort values Uj , consider settingmidpoint pair different values. illustrated Figure 7. figure,categories indicated font size; larger Ui 's belong one category,smaller another. distinct placement coecient , OC1 computesimpurity resulting split; e.g., location U6 U7 illustrated here, twoexamples left one example right would misclassified (see Section 3.3.1different ways computing impurity). figure illustrates, problem simplyfind best one-dimensional split U s, requires considering n , 1 values. value a0m obtained solving one-dimensional problem considereddata underfits concept. default, OC1 uses axis-parallel splits tree nodes n < 2d.user vary threshold.10fiFigure 7: Finding optimal value single coecient . Large U's correspondexamples one category small u's another.Perturb(H,m)j = 1; : : :; nCompute Uj (Eq. 2)Sort U1; : : :; Un non-decreasing order.a0m = best univariate split sorted Uj s.H1 = result substituting a0m H .(impurity(H1 ) < impurity(H))f = a0m ; Pmove = Pstag gElse (impurity(H) = impurity(H1 ))f = a0m probability PmovePmove = Pmove , 0:1 Pstag gFigure 8: Perturbation algorithm single coecient .replacement . Let H1 hyperplane obtained \perturbing" a0m .H better (lower) impurity H1, H1 discarded. H1 lower impurity, H1becomes new location hyperplane. H H1 identical impurities,H1 replaces H probability Pstag .5 Figure 8 contains pseudocode perturbationprocedure.method locally improving coecient hyperplane, needdecide + 1 coecients pick perturbation. experimentedthree different methods choosing coecient adjust, namely, sequential, bestfirst random.Seq: Repeat none coecient values modified loop:= 1 d, Perturb(H; i)Best: Repeat coecient remains unmodified:= coecient perturbed, resultsmaximum improvement impurity measure.Perturb(H; m)R-50: Repeat fixed number times (50 experiments):= random integer 1 + 1Perturb(H; m)5. parameter Pstag , denoting \stagnation probability", probability hyperplane perturbedlocation change impurity measure. prevent impurity remainingstagnant long time, Pstag decreases constant amount time OC1 makes \stagnant"perturbation; thus constant number perturbations occur node. constantset user. Pstag reset 1 every time global impurity measure improved.11fiMurthy, Kasif & Salzbergprevious experiments (Murthy et al., 1993) indicated order perturbationcoecients affect classification accuracy much parameters,especially randomization parameters (see below). Since none orders uniformly better other, used sequential (Seq) perturbation experimentsreported Section 4.3.2 Randomizationperturbation algorithm halts split reaches local minimum impuritymeasure. OC1's search space, local minimum occurs perturbationsingle coecient current hyperplane decrease impurity measure. (Of course,local minimum may also global minimum.) implemented two waysattempting escape local minima: perturbing hyperplane random vector,re-starting perturbation algorithm different random initial hyperplane.technique perturbing hyperplane random vector works follows.system reaches local minimum, chooses random vector addcoecients current hyperplane. computes optimal amounthyperplane beP perturbed along random direction. precise,hyperplane H = di=1 ai xi + ad+1 cannot improved deterministic perturbation,OC1 repeats following loop J times (where J user-specified parameter, set 5default).Choose random vector R = (r1; r2; : : :; rd+1).Let ff amountwant perturb H direction R.Pdwords, let H1 = i=1 (ai + ffri )xi + (ad+1 + ffrd+1 ).Find optimal value ff.hyperplane H1 thus obtained decreases overall impurity, replace H H1,exit loop begin deterministic perturbation algorithm individualcoecients.Note treat ff variable equation H1 . Thereforen examples , plugged equation H1, imposes constraint valueff. OC1 therefore use coecient perturbation method (see Section 3.1) computebest value ff. J random jumps fail improve impurity, OC1 halts usesH split current tree node.intuitive way understanding random jump look atPthe dual spacealgorithm actually searching. Note equation H = di=1 ai xi + ad+1 definesspace axes coecients ai rather attributes xi . Every pointspace defines distinct hyperplane original formulation. deterministicalgorithm used OC1 picks hyperplane adjusts coecients one time. Thusdual space, OC1 chooses point perturbs moving parallel axes.random vector R represents random direction space. finding best valueff, OC1 finds best distance adjust hyperplane direction R.12fiInduction Oblique Decision TreesNote additional perturbation random direction significantly increase time complexity algorithm (see Appendix A). found experimentseven single random jump, used local minimum, proves helpful.Classification accuracy improved every one data sets perturbationsmade. See Section 4.3 examples.second technique avoiding local minima variation idea performingmultiple local searches. technique multiple local searches natural extensionlocal search, widely mentioned optimization literature (see Roth(1970) early example). steps perturbation algorithmdeterministic, initial hyperplane largely determines local minimumencountered first. Perturbing single initial hyperplane thus unlikely lead bestsplit given data set. cases random perturbation method fails escapelocal minima, may helpful simply start afresh new initial hyperplane.use word restart denote one run perturbation algorithms, one nodedecision tree, using one random initial hyperplane.6 is, restart cyclesperturbs coecients one time tries perturb hyperplanerandom direction algorithm reaches local minimum. last perturbationreduces impurity, algorithm goes back perturbing coecients one time.restart ends neither deterministic local search random jump findbetter split. One optional parameters OC1 specifies many restarts use.one restart used, best hyperplane found thus far always saved.experiments, classification accuracies increased one restart.Accuracy tended increase point level (after 20{50 restarts,depending domain). Overall, use multiple initial hyperplanes substantiallyimproved quality decision trees found (see Section 4.3 examples).carefully combining hill-climbing randomization, OC1 ensures worst case timeO(dn2 log n) inducing decision tree. See Appendix derivation upperbound.Best Axis-Parallel Split. clear axis-parallel splits suitabledata distributions oblique splits. take account distributions, OC1 computes best axis-parallel split oblique split node, picks bettertwo.7 Calculating best axis-parallel split takes additional O(dn log n) time,increase asymptotic time complexity OC1. simple variantOC1 system, user opt \switch off" oblique perturbations, thus buildingaxis-parallel tree training data. Section 4.2 empirically demonstratesaxis-parallel variant OC1 compares favorably existing axis-parallel algorithms.6. first run algorithm node always begins location best axis-parallelhyperplane; subsequent restarts begin random locations.7. Sometimes simple axis-parallel split preferable oblique split, even oblique split slightlylower impurity. user specify bias input parameter OC1.13fiMurthy, Kasif & Salzberg3.3 Details3.3.1 Impurity MeasuresOC1 attempts divide d-dimensional attribute space homogeneous regions; i.e.,regions contain examples one category. goal adding new nodestree split sample space minimize \impurity" trainingset. algorithms measure \goodness" instead impurity, differencegoodness values maximized impurity minimized. Many differentmeasures impurity studied (Breiman et al., 1984; Quinlan, 1986; Mingers,1989b; Buntine & Niblett, 1992; Fayyad & Irani, 1992; Heath et al., 1993b).OC1 system designed work large class impurity measures. Statedsimply, impurity measure uses counts examples belonging every categorysides split, OC1 use it. (See Murthy Salzberg (1994) waysmapping kinds impurity measures class impurity measures.) userplug impurity measure fits description. OC1 implementation includessix impurity measures, namely:1.2.3.4.5.6.Information GainGini IndexTwoing RuleMax MinoritySum MinoritySum VariancesThough six measures defined elsewhere literature,cases made slight modifications defined precisely Appendix B.experiments indicated that, average, Information Gain, Gini Index Twoing Ruleperform better three measures axis-parallel oblique trees.Twoing Rule current default impurity measure OC1, usedexperiments reported Section 4. are, however, artificial data setsSum Minority and/or Max Minority perform much better rest measures.instance, Sum Minority easily induces exact tree POL data set describedSection 4.3.1, methods diculty finding best tree.Twoing Rule. Twoing Rule first proposed Breiman et al. (1984). valuecomputed defined as:kXTwoingValue = (jTLj=n) (jTRj=n) (i=1jLi=jTLj , Ri=jTRjj)2jTLj (jTRj) number examples left (right) split node , nnumber examples node , Li (Ri ) number examples categoryleft (right) split. TwoingValue actually goodness measure ratherimpurity measure. Therefore OC1 attempts minimize reciprocal value.remaining five impurity measures implemented OC1 defined Appendix B.14fiInduction Oblique Decision Trees3.3.2 PruningVirtually decision tree induction systems prune trees create order avoidoverfitting data. Many studies found judicious pruning results smalleraccurate classifiers, decision trees well types machine learningsystems (Quinlan, 1987; Niblett, 1986; Cestnik, Kononenko, & Bratko, 1987; Kodratoff& Manago, 1987; Cohen, 1993; Hassibi & Stork, 1993; Wolpert, 1992; Schaffer, 1993).OC1 system implemented existing pruning method, note treepruning method work fine within OC1. Based experimental evaluationsMingers (1989a) work cited above, chose Breiman et al.'s Cost Complexity(CC) pruning (1984) default pruning method OC1. method, alsocalled Error Complexity Weakest Link pruning, requires separate pruning set.pruning set randomly chosen subset training set, approximatedusing cross validation. OC1 randomly chooses 10% (the default value) training datause pruning. experiments reported below, used default value.Brie y, idea behind CC pruning create set trees decreasing sizeoriginal, complete tree. trees used classify pruning set, accuracyestimated that. CC pruning chooses smallest tree whose accuracy within kstandard errors squared best accuracy obtained. 0-SE rule (k = 0) used,tree highest accuracy pruning set selected. k > 0, smaller tree sizepreferred higher accuracy. details Cost Complexity pruning, see Breiman etal. (1984) Mingers (1989a).3.3.3 Irrelevant attributesIrrelevant attributes pose significant problem machine learning methods (Breimanet al., 1984; Aha, 1990; Almuallin & Dietterich, 1991; Kira & Rendell, 1992; Salzberg, 1992;Cardie, 1993; Schlimmer, 1993; Langley & Sage, 1993; Brodley & Utgoff, 1994). Decisiontree algorithms, even axis-parallel ones, confused many irrelevant attributes.oblique decision trees learn coecients attribute DT node, onemight hope values chosen coecient would ect relative importancecorresponding attributes. Clearly, though, process searching good coecientvalues much ecient fewer attributes; search space muchsmaller. reason, oblique DT induction methods benefit substantially usingfeature selection method (an algorithm selects subset original attribute set)conjunction coecient learning algorithm (Breiman et al., 1984; Brodley & Utgoff,1994).Currently, OC1 built-in mechanism select relevant attributes. However, easy include several standard methods (e.g., stepwise forward selectionstepwise backward selection) even ad hoc method select features runningtree-building process. example, separate experiments data HubbleSpace Telescope (Salzberg, Chandar, Ford, Murthy, & White, 1994), used feature selection methods preprocessing step OC1, reduced number attributes 202. resulting decision trees simpler accurate. Work currentlyunderway incorporate ecient feature selection technique OC1 system.15fiMurthy, Kasif & SalzbergRegarding missing values, example missing value attribute, OC1 usesmean value attribute. One course use techniques handlingmissing values, considered study.4. Experimentssection, present two sets experiments support following two claims.1. OC1 compares favorably variety real-world domains several existingaxis-parallel oblique decision tree induction methods.2. Randomization, form multiple local searches random jumps, improves quality decision trees produced OC1.experimental method used experiments described Section 4.1. Sections 4.2 4.3 describe experiments corresponding two claims. experimental section begins description data sets, presents experimentalresults discussion.4.1 Experimental Methodused five-fold cross validation (CV) experiments estimate classificationaccuracy. k-fold CV experiment consists following steps.1. Randomly divide data k equal-sized disjoint partitions.2. partition, build decision tree using data outside partition, testtree data partition.3. Sum number correct classifications k trees divide total numberinstances compute classification accuracy. Report accuracyaverage size k trees.entry Tables 1 2 result ten 5-fold CV experiments; i.e., result testsused 50 decision trees. ten 5-fold cross validations used different randompartitioning data. entry tables reports mean standard deviationclassification accuracy, followed mean standard deviation decisiontree size (measured number leaf nodes). Good results high valuesaccuracy, low values tree size, small standard deviations.addition OC1, also included experiments axis-parallel version OC1,considers axis-parallel hyperplanes. call version, described Section 3.2,OC1-AP. experiments, OC1 OC1-AP used Twoing Rule (Section3.3.1) measure impurity. parameters OC1 took default values unless statedotherwise. (Defaults include following: number restarts node: 20. Numberrandom jumps attempted local minimum: 5. Order coecient perturbation:Sequential. Pruning method: Cost Complexity 0-SE rule, using 10% trainingset exclusively pruning.)comparison, used oblique version CART algorithm, CART-LC.implemented version CART-LC, following description Breiman etal. (1984, Chapter 5); however, may differences version16fiInduction Oblique Decision Treesversions system (note CART-LC freely available). implementationCART-LC measured impurity Twoing Rule used 0-SE Cost Complexitypruning separate test set, OC1 does. include feature selectionmethods CART-LC OC1, implement normalization.CART coecient perturbation algorithm may alternate indefinitely two locationshyperplane (see Section 2), imposed arbitrary limit 100 perturbationsforcing perturbation algorithm halt.also included axis-parallel CART C4.5 comparisons. used implementations algorithms IND 2.1 package (Buntine, 1992). defaultcart0 c4.5 \styles" defined package used, without altering parametersettings. cart0 style uses Twoing Rule 0-SE cost complexity pruning10-fold cross validation. pruning method, impurity measure defaultsc4.5 style described Quinlan (1993a).4.2 OC1 vs. Decision Tree Induction MethodsTable 1 compares performance OC1 three well-known decision tree inductionmethods plus OC1-AP six different real-world data sets. next sectionconsider artificial data, concept definition precisely characterized.4.2.1 Description Data SetsStar/Galaxy Discrimination. Two data sets came large set astronom-ical images collected Odewahn et al. (Odewahn, Stockwell, Pennington, Humphreys, &Zumach, 1992). study, used images train artificial neural networksrunning perceptron back propagation algorithms. goal classify example either \star" \galaxy." image characterized 14 real-valued attributes,attributes measurements defined astronomers likely relevanttask. objects image divided Odewahn et al. \bright" \dim"data sets based image intensity values, dim images inherentlydicult classify. (Note \bright" objects bright relation othersdata set. actuality extremely faint, visible powerfultelescopes.) bright set contains 2462 objects dim set contains 4192 objects.addition results reported Table 1, following results appearedStar/Galaxy data. Odewahn et al. (1992) reported accuracy 99.8% accuracybright objects, 92.0% dim ones, although noted studyused single training test set partition. Heath (1992) reported 99.0% accuracybright objects using SADT, average tree size 7.03 leaves. study also usedsingle training test set. Salzberg (1992) reported accuracies 98.8% brightobjects, 95.1% dim objects, using 1-Nearest Neighbor (1-NN) coupledfeature selection method reduces number features.Breast Cancer Diagnosis. Mangasarian Bennett compiled data problem diagnosing breast cancer test several new classification methods (Mangasarianet al., 1990; Bennett & Mangasarian, 1992, 1994a). data represents set patientsbreast cancer, patient characterized nine numeric attributes plusdiagnosis tumor benign malignant. data set currently 683 entries17fiMurthy, Kasif & SalzbergBright S/G98.90.24.31.0CART-LC98.80.23.91.3OC1-AP98.10.26.92.4CART-AP98.50.513.95.7C4.598.50.514.32.2AlgorithmOC1Dim S/G95.00.313.08.792.80.524.28.794.00.229.38.894.20.730.41093.30.877.97.4Cancer96.20.32.80.995.30.63.50.994.50.56.41.795.01.611.57.295.32.09.82.2Iris94.73.13.10.293.52.93.20.392.72.43.20.393.83.74.31.695.13.24.60.8Housing82.40.86.93.281.41.25.83.281.81.08.64.582.13.515.11083.23.128.23.3Diabetes74.41.05.43.873.71.28.05.273.81.011.47.573.93.411.59.171.43.356.37.9Table 1: Comparison OC1 decision tree induction methods six differentdata sets. first line method gives accuracies, second line givesaverage tree sizes. highest accuracy domain appears boldface.available UC Irvine machine learning repository (Murphy & Aha, 1994).Heath et al. (1993b) reported 94.9% accuracy subset data set (it470 instances), average decision tree size 4.6 nodes, using SADT. Salzberg(1991) reported 96.0% accuracy using 1-NN (smaller) data set. HermanYeung (1992) reported 99.0% accuracy using piece-wise linear classification, usingsomewhat smaller data set.Classifying Irises. Fisher's famous iris data, extensively studiedstatistics machine learning literature. data consists 150 examples,example described four numeric attributes. 50 examplesthree different types iris ower. Weiss Kapouleas (1989) obtained accuracies 96.7%96.0% data back propagation 1-NN, respectively.Housing Costs Boston. data set, also available part UCI ML repos-itory, describes housing values suburbs Boston function 12 continuousattributes 1 binary attribute (Harrison & Rubinfeld, 1978). category variable (median value owner-occupied homes) actually continuous, discretizedcategory = 1 value < $21000, 2 otherwise. uses data, see (Belsley,1980; Quinlan, 1993b).Diabetes diagnosis. data catalogs presence absence diabetes among PimaIndian females, 21 years older, function eight numeric-valued attributes.original source data National Institute Diabetes Digestive KidneyDiseases, available UCI repository. Smith et al. (1988) reported 76%accuracy data using ADAP learning algorithm, using different experimentalmethod used here.18fiInduction Oblique Decision Trees4.2.2 Discussiontable shows that, six data sets considered here, OC1 consistently finds bettertrees original oblique CART method. accuracy greater six domains,although difference significant (more 2 standard deviations) dimstar/galaxy problem. average tree sizes roughly equal five six domains,dim stars galaxies, OC1 found considerably smaller trees. differencesanalyzed quantified using artificial data, following section.five decision tree induction methods, OC1 highest accuracy foursix domains: bright stars, dim stars, cancer diagnosis, diabetes diagnosis.remaining two domains, OC1 second highest accuracy case. surprisingly,oblique methods (OC1 CART-LC) generally find much smaller trees axisparallel methods. difference quite striking domains|note, example,OC1 produced tree 13 nodes average dim star/galaxy problem,C4.5 produced tree 78 nodes, 6 times larger. course, domainsaxis-parallel tree appropriate representation, axis-parallel methods comparewell oblique methods terms tree size. fact, Iris data, methodsfound similar-sized trees.4.3 Randomization Helps OC1second set experiments, examine closely effect introducing randomized steps algorithm finding oblique splits. experiments demonstrateOC1's ability produce accurate tree set training data clearly enhancedtwo kinds randomization uses. precisely, use three artificial data sets(for underlying concept known experimenters) show OC1's performance improves substantially deterministic hill climbing augmentedthree ways:multiple restarts random initial locations,perturbations random directions local minima,randomization steps.order find clear differences algorithms, one needs know conceptunderlying data indeed dicult learn. simple concepts (say, two linearlyseparable classes 2-D), many different learning algorithms produce accurateclassifiers, therefore advantages randomization may detectable.known many commonly-used data sets UCI repository easylearn simple representations (Holte, 1993); therefore data sets mayideal purposes. Thus created number artificial data sets present differentproblems learning, know \correct" concept definition. allowsus quantify precisely parameters algorithm affect performance.second purpose experiment compare OC1's search strategytwo existing oblique decision tree induction systems { LMDT (Brodley & Utgoff, 1992)SADT (Heath et al., 1993b). show quality trees induced OC1good as, better than, trees induced existing systems three19fiMurthy, Kasif & Salzbergartificial domains. also show OC1 achieves good balance amounteffort expended search quality tree induced.LMDT SADT used information gain experiment. However,change OC1's default measure (the Twoing Rule) observed, experimentsreported here, OC1 information gain produce significantly differentresults. maximum number successive, unproductive perturbations allowednode set 10000 SADT. parameters, used default settings providedsystems.4.3.1 Description Artificial DataLS10 LS10 data set 2000 instances divided two categories. instancedescribed ten attributes x1 ,: : : ,x10, whose values uniformly distributed range[0,1]. data linearly separable 10-D hyperplane (thus name LS10) definedequation x1 + x2 + x3 + x4 + x5 < x6 + x7 + x8 + x9 + x10. instancesgenerated randomly labelled according side hyperplane fell on.oblique DT induction methods intuitively prefer linear separator oneexists, interesting compare various search techniques data setknow separator exists. task relatively simple lower dimensions, chose10-dimensional data make dicult.POL data set shown Figure 9. 2000 instances two dimensions,divided two categories. underlying concept set four parallel oblique lines(thus name POL), dividing instances five homogeneous regions. conceptdicult learn single linear separator, minimal-size tree still quitesmall.RCB RCB stands \rotated checker board"; data set subjectexperiments hard classification problems decision trees (Murthy & Salzberg,1994). data set, shown Figure 9, 2000 instances 2-D, belonging oneeight categories. concept dicult learn axis-parallel method, obviousreasons. also quite dicult oblique methods, several reasons. biggestproblem \correct" root node, shown figure, separateclass itself. impurity measures (such Sum Minority) fail miserablyproblem, although others (e.g., Twoing Rule) work much better. Another problemdeterministic coecient perturbation algorithm get stuck local minimamany places data set.Table 2 summarizes results experiment three smaller tables, onedata set. smaller table, compare four variants OC1 LMDT SADT.different results OC1 obtained varying number restartsnumber random jumps. random jumps used, twenty random jumpstried local minimum. soon one found improved impuritycurrent hyperplane, algorithm moved hyperplane started runningdeterministic perturbation procedure again. none 20 random jumps improvedimpurity, search halted restarts (if any) tried. trainingtest partitions used methods cross-validation run (recall results20fiInduction Oblique Decision Treeslr1otrr1-1l-11r-1rl-1rr-r-1t-1ooR344477433333334333373 4 44 44 44 41 3333 3 34 4 74 443444433 3 33 33447433 3 34 44111 1 3 33 37 7 77 7433 3 3 33 3 3414 4 44414474 43344 43 333 34 4444444 4 4 44471 1777333 3 4 43474 4444 474444 437334 4 4444433177 77 77734 434 441 17 777 7441134 7 777 7477411414 4 44 4 4 44 4 4137171 11144177 7 7 777 7444 4 444744 4 44777111 1 1 1 4 4 44 4 47 7 77 744 4141 14 44 474447 744 4 41 111 141417 77111 144 44417777 771 1 11 141 1 1114 4 44 477 774 4471774 477 7 7 7 7 7114841171 1 148277 7774 4 712 4 44 4711 1787 777111212778111122 2444 4 47 777 77 7 77 7 7 77 7 7 841 2 22 2 27188 8ll-22777 741 1 2287 722 2 22 44 4 4 7777 77888 811122 2288882 22 22571822 22212 2887 77 722211 2 282225777788222 22 282222 2277 8 8 8225577 82 2 22 2228 88 8 8 855582 22 222 28 887 82 22285 5 77888 82 2825555228255 55888 822 222 272 2282258 88 85222885558 8 8 8 8 885 52 22 2 22 22 2 22 2288 88 8552 255228 8 88 852 22 2 5 5 582 25 5688 8558225555 5 55 5 6 68 88 888822528558 886 66 8 82 2 2 2 2 2 2 2 5 5 55 5 5686882 2 2 285585688 856552 2222 2 5555556665 5 55 55655 5 5222 22 2 228 86555 5 56666685558226 66 68 85 5 52 2255666 62 225 55 5 5 556 66 8 862566 656 6655 555 6555 56 6 6655522 255 55 566 6 6655666685 56566666556 6 6655558 86 66 65 55556 66 6566625 5555566655866 6 65 5 5 55 555 56 6655 56 6856 666 6Ro-1rrr221211122222112222212 2 21 11 11 11 1122 2 21 1 12 222211122 2 22 22221222 2 22 22111 1 1 11 21 1 11 1121 2 2 22 2 2112 2 22212211 12222 22 222 22 2222222 2 2 11111 1111222 2 2 21212 1222 212222 211222 2 2222222111 11 11112 222 221 11 111 1221112 2 222 1212211212 2 22 2 2 22 2 2111111 11122122 1 1 111 1222 2 222222 2 21222111 1 1 1 1 1 11 2 21 1 11 122 2121 12 22 222221 122 2 21 111 121212 22111 122 22112122 221 1 11 111 1 1112 2 22 222 222 2221222 222 2 2 2 2 2111221121 1 122122 2222 2 211 1 11 1221 1222 2221111212221111111 1111 1 12 222 22 2 22 2 2 22 2 2 2111 12122 21 11222 212 1 1122 211 1 11 11 1 1 1222 22222 222211 1122221 11 11122222 11121 1222 22 211122 2 221111222222111 11 121112 2222 2 2 2211122 22 2 22 2112 22 2 2 211121 11 121 12 222 22 22221 1 11222 22 2211111112111 11222 222 222 212 2221112 22 21222221111 2 2 2 2 221 12 22 2 22 22 2 22 2122 22 2112 211222 2 22 212 22 2 1 1 122 21 1122 2111222211 1 11 1 1 11 22 222222222122 221 11 1 11 2 2 2 2 2 2 2 2 2 22 1 1121111 1 2 221121112 211112 2222 2 2111121112 2 21 11122 2 2111 11 1 222 21122 2 21111111222111 11 11 22 2 211 1111111 11 112 22 2 2 221 11 1 211211 121 1111 121 1222 21 1 1122111 122 21 211 1 1122111112 22211211111 1 1111221 11 11 11 11111 11 1211112 2222122112122 1 11 1 1 11 111 21 1111 11 1112 211 1Figure 9: POL RCB data setsLinearly Separable 10-D (LS10) dataR:J AccuracySizeHyperplanes0:0 89.81.2 67.05.827560:20 91.51.5 55.27.0382420:0 95.00.6 25.62.42491320:20 97.20.7 13.93.230366LMDT 99.70.2 2.20.59089SADT 95.21.8 15.55.7349067Parallel Oblique Lines (POL) dataR:J AccuracySizeHyperplanes0:0 98.30.3 21.61.91640:20 99.30.2 9.01.036020:0 99.10.2 14.21.1323020:20 99.60.1 5.50.34852LMDT 89.610.2 41.919.21732SADT 99.30.4 8.42.185594Rotated Checker Board (RCB) dataR:J AccuracySizeHyperplanes0:0 98.40.2 35.51.45730:20 99.30.3 19.70.8177820:0 99.60.2 12.01.4643620:20 99.80.1 8.70.411634LMDT 95.72.3 70.19.62451SADT 97.91.1 32.54.9359112Table 2: effect randomization OC1. first column, labelled R:J, showsnumber restarts (R) followed maximum number random jumps (J)attempted OC1 local minimum. Results LMDT SADTincluded comparison four variants OC1. Size average tree sizemeasured number leaf nodes. third column shows averagenumber hyperplanes algorithm considered building one tree.21fiMurthy, Kasif & Salzbergaverage ten 5-fold CVs). trees pruned algorithms,data noise-free furthermore emphasis search.Table 2 also includes number hyperplanes considered algorithmbuilding complete tree. Note OC1 SADT, number hyperplanes considered generally much larger number perturbations actually made,algorithms compare newly generated hyperplanes existing hyperplanesadjusting existing one. Nevertheless, number good estimate much effortalgorithm expends, every new hyperplane must evaluated accordingimpurity measure. LMDT, number hyperplanes considered identicalactual number perturbations.4.3.2 DiscussionOC1 results quite clear. first line table, labelled 0:0, givesaccuracies tree sizes randomization used | variant similarCART-LC algorithm. increase use randomization, accuracy increasestree size decreases, exactly result hoped decidedintroduce randomization method.Looking closely tables, ask effect random jumps alone.illustrated second line (0:20) table, attempted 20 randomjumps local minimum restarts. Accuracy increased 1-2% domain,tree size decreased dramatically, roughly factor two, POL RCBdomains. Note noise domains, high accuraciesexpected. Thus increases percent accuracy possible.Looking third line sub-table Table 2, see effect multiple restartsOC1. 20 restarts random jumps escape local minima, improvementeven noticeable LS10 data random jumps alone used.data set, accuracy jumped significantly, 89.8 95.0%, tree size dropped67 26 nodes. POL RCB data, improvements comparableobtained random jumps. RCB data, tree size dropped factor 3(from 36 leaf nodes 12 leaf nodes) accuracy increased 98.4 99.6%.fourth line table shows effect randomized steps. AmongOC1 entries, line highest accuracies smallest trees threedata sets, clear randomization big win kinds problems.addition, note smallest tree RCB data eight leaf nodes,OC1's average trees, without pruning, 8.7 leaf nodes. clear dataset, thought dicult one, OC1 came close finding optimaltree nearly every run. (Recall numbers table average 10 5-foldCV experiments; i.e., average 50 decision trees.) LS10 data show dicultfind simple concept higher dimensions|the optimal treesingle hyperplane (two nodes), OC1 unable find current parametersettings.8 POL data required minimum 5 leaf nodes, OC1 found minimalsize tree time, seen table. Although shown Table,8. separate experiment, found OC1 consistently finds linear separator LS10 data10 restarts 200 random jumps used.22fiInduction Oblique Decision TreesOC1 using Sum Minority performed better POL data Twoing Ruleimpurity measure; i.e., found correct tree using less time.results LMDT SADT data lead interesting insights.surprisingly, LMDT well linearly separable (LS10) data,require inordinate amount search. Clearly, data linearly separable, oneuse method LMDT linear programming. OC1 SADT diculty findinglinear separator, although experiments OC1 eventually find it, given sucienttime.hand, non-linearly separable data sets, LMDT producesmuch larger trees significantly less accurate produced OC1SADT. Even deterministic variant OC1 (using zero restarts zero random jumps)outperforms LMDT problems, much less search.Although SADT sometimes produces accurate trees, main weaknessenormous amount search time required, roughly 10-20 times greater OC1 evenusing 20:20 setting. One explanation OC1's advantage use directed search,opposed strictly random search used simulated annealing. Overall, Table 2 showsOC1's use randomization quite effective non-linearly separable data.natural ask randomization helps OC1 task inducing decision trees.Researchers combinatorial optimization observed randomized search usuallysucceeds search space holds abundance good solutions (Gupta, Smolka,& Bhaskar, 1994). Furthermore, randomization improve upon deterministic searchmany local maxima search space lead poor solutions. OC1's searchspace, local maximum hyperplane cannot improved deterministicsearch procedure, \solution" complete decision tree. significant fractionlocal maxima lead bad trees, algorithms stop first local maximumencounter perform poorly. randomization allows OC1 consider manydifferent local maxima, modest percentage maxima lead good trees,good chance finding one trees. experiments OC1 thus far indicatespace oblique hyperplanes usually contains numerous local maxima,substantial percentage locally good hyperplanes lead good decision trees.5. Conclusions Future Workpaper described OC1, new system constructing oblique decision trees.shown experimentally OC1 produce good classifiers range real-worldartificial domains. also shown use randomization improves uponoriginal algorithm proposed Breiman et al. (1984), without significantly increasingcomputational cost algorithm.use randomization might also beneficial axis-parallel tree methods. Notealthough find optimal test (with respect impurity measure)node tree, complete tree may optimal: well known, problemfinding smallest tree NP-Complete (Hyafil & Rivest, 1976). Thus even axis-paralleldecision tree methods produce \ideal" decision trees. Quinlan suggestedwindowing algorithm might used way introducing randomization C4.5, eventhough algorithm designed another purpose (Quinlan, 1993a). (The windowing23fiMurthy, Kasif & Salzbergalgorithm selects random subset training data builds tree using that.)believe randomization powerful tool context decision trees,experiments one example might exploited. processconducting experiments quantify accurately effects different formsrandomization.clear ability produce oblique splits node broadens capabilities decision tree algorithms, especially regards domains numeric attributes.course, axis-parallel splits simpler, sense description splituses one attribute node. OC1 uses oblique splits impurity lessimpurity best axis-parallel split; however, one could easily penalizeadditional complexity oblique split further. remains open arearesearch. general point domain best captured tree usesoblique hyperplanes, desirable system generate tree.shown problems, including used experiments, OC1 builds smalldecision trees capture domain well.Appendix A. Complexity Analysis OC1following, show OC1 runs eciently even worst case. dataset n examples (points) attributes per example, OC1 uses O(dn2 log n)time. assume n > analysis.analysis here, assume coecients hyperplane adjusted sequential order (the Seq method described paper). number restarts noder, number random jumps tried j . r j constants, fixedadvance running algorithm.Initializing hyperplane random position takes O(d) time. needconsider first maximum amount work OC1 finds new locationhyperplane. need consider many times move hyperplane.1. Attempting perturb first coecient (a1 ) takes O(dn + n log n) time. ComputingUi 's points (equation 2) requires O(dn) time, sorting Ui 's takesO(n log n). gives us O(dn + n log n) work.2. perturbing a1 improve things, try perturb a2 . Computing newUi 's take O(n) time one term different Ui . Re-sortingtake O(n log n), step takes O(n) + O(n log n) = O(n log n) time.3. Likewise a3; : : :; ad take O(n log n) additional time, assuming stillfound better hyperplane checking coecient. Thus total time cycleattempt perturb additional coecients (d , 1) O(n log n) =O(dn log n).4. Summing up, time cycle coecients O(dn log n)+O(dn+n log n) =O(dn log n).5. none coecients improved split, attempt make j randomjumps. Since j constant, consider j = 1 analysis. step24fiInduction Oblique Decision Treesinvolves choosing random vector running perturbation algorithm solveff, explained Section 3.2. before, need compute set Ui 's sortthem, takes O(dn + n log n) time. amount time dominatedtime adjust coecients, total time far still O(dn log n).time OC1 spend node either halting finding improvedhyperplane.6. Assuming OC1 using Sum Minority Max Minority error measure,reduce impurity hyperplane n times. clear improvementmeans one example correctly classified new hyperplane. Thustotal amount work node limited n O(dn log n) = O(dn2 log n). (Thisanalysis extends, linear cost factors, Information Gain, Gini IndexTwoing Rule two categories. apply measure that,example, uses distances mis-classified objects hyperplane.) practice,found number improvements per node much smaller n.Assuming OC1 adjusts hyperplane improves impurity measure,O(dn2 log n) work worst case.However, OC1 allows certain number adjustments hyperplaneimprove impurity, although never accept change worsens impurity.number allowed determined constant known \stagnant-perturbations". Letvalue s. works follows.time OC1 finds new hyperplane improves old one, resets counterzero. move new hyperplane different location equal impuritytimes. moves repeats perturbation algorithm. Wheneverimpurity reduced, re-starts counter allows moves equally goodlocations. Thus clear feature increases worst-case complexity OC1constant factor, s.Finally, note overall cost OC1 also O(dn2 log n), i.e., upperbound total running time OC1 independent size tree endscreating. (This upper bound applies Sum Minority Max Minority; open questionwhether similar upper bound proven Information Gain Gini Index.)Thus worst-case asymptotic complexity system comparable systemsconstruct axis-parallel decision trees, O(dn2 ) worst-case complexity.sketch intuition leads bound, let G total impurity summedleaves partially constructed tree (i.e., sum currently misclassified pointstree). observe time run perturbation algorithm nodetree, either halt improve G least one unit. worst-case analysis one noderealized perturbation algorithm run every one n examples,happens, would longer mis-classified examples treewould complete.Appendix B. Definitions impurity measures available OC1addition Twoing Rule defined text, OC1 contains built-in definitions fiveadditional impurity measures, defined follows. following definitions,25fiMurthy, Kasif & Salzbergset examples node split contains n (> 0) instances belongone k categories. (Initially set entire training set.) hyperplane H dividestwo non-overlapping subsets TL TR (i.e., left right). Lj Rj numberinstances category j TL TR respectively. impurity measures initiallycheck see TL TR homogeneous (i.e., examples belong category),return minimum (zero) impurity.Information Gain. measure information gained particular split pop-ularized context decision trees Quinlan (1986). Quinlan's definition makesinformation gain goodness measure; i.e., something maximize. OC1 attemptsminimize whatever impurity measure uses, use reciprocal standard valueinformation gain OC1 implementation.Gini Index. Gini Criterion (or Index) proposed decision trees Breiman etal. (1984). Gini Index originally defined measures probability misclassificationset instances, rather impurity split. implement followingvariation:GiniL = 1:0 ,GiniR = 1:0 ,kXi=1kXi=1(Li =jTLj)2(Ri=jTRj)2Impurity = (jTLj GiniL + jTRj GiniR)=nGiniL Gini Index \left" side hyperplane GiniRright.Max Minority. measures Max Minority, Sum Minority Sum Variancesdefined context decision trees Heath, Kasif, Salzberg (1993b).9 MaxMinority theoretical advantage tree built minimizing measuredepth log n. experiments indicated great advantagepractice: seldom impurity measures produce trees substantially deeperproduced Max Minority. definition is:MinorityL =MinorityR =kXi=1;i6=max LikXi=1;i6=max RiLiRiMax Minority = max(MinorityL; MinorityR)9. Sum Variances called Sum Impurities Heath et al.26fiInduction Oblique Decision TreesSum Minority. measure similar Max Minority. MinorityL MinorityR defined Max Minority measure, Sum Minority sumtwo values. measure simplest way quantifying impurity, simplycounts number misclassified instances.Though Sum Minority performs well domains, obvious aws.one example, consider domain n = 100; = 1, k = 2 (i.e., 100 examples, 1numeric attribute, 2 classes). Suppose examples sorted accordingsingle attribute, first 50 instances belong category 1, followed 24 instances category 2, followed 26 instances category 1. possible splits distributionsum minority 24. Therefore impossible using Sum Minority distinguish split preferable, although splitting alternations categoriesclearly better.Sum Variances. definition measure is:jXTL jjXTL jVarianceL = (Cat(TLi ) , Cat(TLj )=jTLj)2VarianceR =i=1j =1jXTRjjXTRji=1(Cat(TRi ) ,j =1Cat(TRj )=jTRj)2Sum Variances = VarianceL + VarianceRCat(Ti) category instance Ti . measure computed using actualclass labels, easy see impurity computed varies depending numbersassigned classes. instance, T1 consists 10 points category 1 3points category 2, T2 consists 10 points category 1 3 points category5, Sum Variances values different T1 T2. avoid problem,OC1 uniformly reassigns category numbers according frequency occurrencecategory node computing Sum Variances.Acknowledgementsauthors thank Richard Beigel Yale University suggesting idea jumpingrandom direction. Thanks Wray Buntine Nasa Ames Research Center providingIND 2.1 package, Carla Brodley providing LMDT code, David Heathproviding SADT code assisting us using it. Thanks also three anonymousreviewers many helpful suggestions. material based upon work supportedNational Science foundation Grant Nos. IRI-9116843, IRI-9223591, IRI-9220960.ReferencesAha, D. (1990). Study Instance-Based Algorithms Supervised Learning: Mathematical, empirical psychological evaluations. Ph.D. thesis, Department InformationComputer Science, University California, Irvine.27fiMurthy, Kasif & SalzbergAlmuallin, H., & Dietterich, T. (1991). Learning many irrelevant features. Proceedings Ninth National Conference Artificial Intelligence, pp. 547{552. SanJose, CA.Belsley, D. (1980). Regression Diagnostics: Identifying uential Data SourcesCollinearity. Wiley & Sons, New York.Bennett, K., & Mangasarian, O. (1992). Robust linear programming discrimination twolinearly inseparable sets. Optimization Methods Software, 1, 23{34.Bennett, K., & Mangasarian, O. (1994a). Multicategory discrimination via linear programming. Optimization Methods Software, 3, 29{39.Bennett, K., & Mangasarian, O. (1994b). Serial parallel multicategory discrimination.SIAM Journal Optimization, 4 (4).Blum, A., & Rivest, R. (1988). Training 3-node neural network NP-complete. Proceedings 1988 Workshop Computational Learning Theory, pp. 9{18. Boston,MA. Morgan Kaufmann.Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification RegressionTrees. Wadsworth International Group.Brent, R. P. (1991). Fast training algorithms multilayer neural nets. IEEE TransactionsNeural Networks, 2 (3), 346{354.Brodley, C. E., & Utgoff, P. E. (1992). Multivariate versus univariate decision trees. Tech.rep. COINS CR 92-8, Dept. Computer Science, University MassachusettsAmherst.Brodley, C. E., & Utgoff, P. E. (1994). Multivariate decision trees. Machine Learning,appear.Buntine, W. (1992). Tree classification software. Technology 2002: Third NationalTechnology Transfer Conference Exposition.Buntine, W., & Niblett, T. (1992). comparison splitting rules decision-treeinduction. Machine Learning, 8, 75{85.Cardie, C. (1993). Using decision trees improve case-based learning. ProceedingsTenth International Conference Machine Learning, pp. 25{32. UniversityMassachusetts, Amherst.Cestnik, G., Kononenko, I., & Bratko, I. (1987). Assistant 86: knowledge acquisitiontool sophisticated users. Bratko, I., & Lavrac, N. (Eds.), Progress MachineLearning. Sigma Press.Cios, K. J., & Liu, N. (1992). machine learning method generation neural networkarchitecture: continuous ID3 algorithm. IEEE Transactions Neural Networks,3 (2), 280{291.28fiInduction Oblique Decision TreesCohen, W. (1993). Ecient pruning methods separate-and-conquer rule learning systems. Proceedings 13th International Joint Conference Artificial Intelligence, pp. 988{994. Morgan Kaufmann.Fayyad, U. M., & Irani, K. B. (1992). attribute specification problem decision treegeneration. Proceedings Tenth National Conference Artificial Intelligence,pp. 104{110. San Jose CA. AAAI Press.Frean, M. (1990). Small Nets Short Paths: Optimising neural computation. Ph.D.thesis, Centre Cognitive Science, University Edinburgh.Gupta, R., Smolka, S., & Bhaskar, S. (1994). randomization sequential distributedalgorithms. ACM Computing Surveys, 26 (1), 7{86.Hampson, S., & Volper, D. (1986). Linear function neurons: Structure training. Biological Cybernetics, 53, 203{217.Harrison, D., & Rubinfeld, D. (1978). Hedonic prices demand clean air. JournalEnvironmental Economics Management, 5, 81{102.Hassibi, B., & Stork, D. (1993). Second order derivatives network pruning: optimalbrain surgeon. Advances Neural Information Processing Systems 5, pp. 164{171.Morgan Kaufmann, San Mateo, CA.Heath, D. (1992). Geometric Framework Machine Learning. Ph.D. thesis, JohnsHopkins University, Baltimore, Maryland.Heath, D., Kasif, S., & Salzberg, S. (1993a). k-DT: multi-tree learning method.Proceedings Second International Workshop Multistrategy Learning, pp. 138{149. Harpers Ferry, WV. George Mason University.Heath, D., Kasif, S., & Salzberg, S. (1993b). Learning oblique decision trees. Proceedings13th International Joint Conference Artificial Intelligence, pp. 1002{1007.Chambery, France. Morgan Kaufmann.Herman, G. T., & Yeung, K. D. (1992). piecewise-linear classification. IEEE Transactions Pattern Analysis Machine Intelligence, 14 (7), 782{786.Holte, R. (1993). simple classification rules perform well commonly useddatasets. Machine Learning, 11 (1), 63{90.Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees NPcomplete. Information Processing Letters, 5 (1), 15{17.Kira, K., & Rendell, L. (1992). practical approach feature selection. ProceedingsNinth International Conference Machine Learning, pp. 249{256. Aberdeen,Scotland. Morgan Kaufmann.Kirkpatrick, S., Gelatt, C., & Vecci, M. (1983). Optimization simulated annealing.Science, 220 (4598), 671{680.29fiMurthy, Kasif & SalzbergKodratoff, Y., & Manago, M. (1987). Generalization noise. International JournalMan-Machine Studies, 27, 181{204.Langley, P., & Sage, S. (1993). Scaling domains many irrelevant features. LearningSystems Department, Siemens Corporate Research, Princeton, NJ.Mangasarian, O., Setiono, R., & Wolberg, W. (1990). Pattern recognition via linear programming: Theory application medical diagnosis. SIAM WorkshopOptimization.Mingers, J. (1989a). empirical comparison pruning methods decision tree induction. Machine Learning, 4 (2), 227{243.Mingers, J. (1989b). empirical comparison selection measures decision tree induction. Machine Learning, 3, 319{342.Moret, B. M. (1982). Decision trees diagrams. Computing Surveys, 14 (4), 593{623.Murphy, P., & Aha, D. (1994). UCI repository machine learning databases { machinereadable data repository. Maintained Department Information ComputerScience, University California, Irvine. Anonymous FTP ics.uci.edudirectory pub/machine-learning-databases.Murthy, S. K., Kasif, S., Salzberg, S., & Beigel, R. (1993). OC1: Randomized inductionoblique decision trees. Proceedings Eleventh National Conference ArtificialIntelligence, pp. 322{327. Washington, D.C. MIT Press.Murthy, S. K., & Salzberg, S. (1994). Using structure improve decision trees. Tech. rep.JHU-94/12, Department Computer Science, Johns Hopkins University.Niblett, T. (1986). Constructing decision trees noisy domains. Bratko, I., & Lavrac,N. (Eds.), Progress Machine Learning. Sigma Press, England.Nilsson, N. (1990). Learning Machines. Morgan Kaufmann, San Mateo, CA.Odewahn, S., Stockwell, E., Pennington, R., Humphreys, R., & Zumach, W. (1992). Automated star-galaxy descrimination neural networks. Astronomical Journal,103 (1), 318{331.Pagallo, G. (1990). Adaptive Decision Tree Algorithms Learning Examples. Ph.D.thesis, University California Santa Cruz.Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. MachineLearning, 5 (1), 71{99.Quinlan, J. R. (1983). Learning ecient classification procedures applicationchess end games. Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), MachineLearning: Artificial Intelligence Approach. Morgan Kaufmann, San Mateo, CA.Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.30fiInduction Oblique Decision TreesQuinlan, J. R. (1987). Simplifying decision trees. International Journal Man-MachineStudies, 27, 221{234.Quinlan, J. R. (1993a). C4.5: Programs Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.Quinlan, J. R. (1993b). Combining instance-based model-based learning. ProceedingsTenth International Conference Machine Learning, pp. 236{243 UniversityMassachusetts, Amherst. Morgan Kaufmann.Roth, R. H. (1970). approach solving linear discrete optimization problems. JournalACM, 17 (2), 303{313.Safavin, S. R., & Landgrebe, D. (1991). survey decision tree classifier methodology.IEEE Transactions Systems, Man Cybernetics, 21 (3), 660{674.Sahami, M. (1993). Learning non-linearly separable boolean functions linear threshold unit trees madaline-style networks. Proceedings Eleventh NationalConference Artificial Intelligence, pp. 335{341. AAAI Press.Salzberg, S. (1991). nearest hyperrectangle learning method. Machine Learning, 6,251{276.Salzberg, S. (1992). Combining learning search create good classifiers. Tech. rep.JHU-92/12, Johns Hopkins University, Baltimore MD.Salzberg, S., Chandar, R., Ford, H., Murthy, S. K., & White, R. (1994). Decision treesautomated identification cosmic rays Hubble Space Telescope images. Publications Astronomical Society Pacific, appear.Schaffer, C. (1993). Overfitting avoidance bias. Machine Learning, 10, 153{178.Schlimmer, J. (1993). Eciently inducing determinations: complete systematicsearch algorithm uses optimal pruning. Proceedings Tenth InternationalConference Machine Learning, pp. 284{290. Morgan Kaufmann.Smith, J., Everhart, J., Dickson, W., Knowler, W., & Johannes, R. (1988). UsingADAP learning algorithm forecast onset diabetes mellitus. ProceedingsSymposium Computer Applications Medical Care, pp. 261{265. IEEEComputer Society Press.Utgoff, P. E. (1989). Perceptron trees: case study hybrid concept representations.Connection Science, 1 (4), 377{391.Utgoff, P. E., & Brodley, C. E. (1990). incremental method finding multivariatesplits decision trees. Proceedings Seventh International ConferenceMachine Learning, pp. 58{65. Los Altos, CA. Morgan Kaufmann.Utgoff, P. E., & Brodley, C. E. (1991). Linear machine decision trees. Tech. rep. 10,University Massachusetts Amherst.31fiMurthy, Kasif & SalzbergVan de Merckt, T. (1992). NFDT: system learns exible concepts based decisiontrees numerical attributes. Proceedings Ninth International WorkshopMachine Learning, pp. 322{331.Van de Merckt, T. (1993). Decision trees numerical attribute spaces. Proceedings13th International Joint Conference Artificial Intelligence, pp. 1016{1021.Weiss, S., & Kapouleas, I. (1989). empirical comparison pattern recognition, neuralnets, machine learning classification methods. Proceedings 11th International Joint Conference Artificial Intelligence, pp. 781{787. Detroit, MI. MorganKaufmann.Wolpert, D. (1992). overfitting avoidance bias. Tech. rep. SFI TR 92-03-5001,Santa Fe Institute, Santa Fe, New Mexico.32fiJournal Artificial Intelligence Research 2 (1995) 501-539Submitted 9/94; published 5/95Pac-Learning Recursive Logic Programs:Ecient AlgorithmsWilliam W. CohenAT&T Bell Laboratories600 Mountain Avenue, Murray Hill, NJ 07974 USAwcohen@research.att.comAbstractpresent algorithms learn certain classes function-free recursive logic programs polynomial time equivalence queries. particular, show singlek-ary recursive constant-depth determinate clause learnable. Two-clause programs consisting one learnable recursive clause one constant-depth determinate non-recursiveclause also learnable, additional \basecase" oracle assumed. results immediately imply pac-learnability classes. Although classes learnablerecursive programs constrained, shown companion papermaximally general, generalizing either class natural way leads computationally dicult learning problem. Thus, taken together companion paper,paper establishes boundary ecient learnability recursive logic programs.1. IntroductionOne active area research machine learning learning concepts expressed firstorder logic. Since researchers used variant Prolog represent learnedconcepts, subarea sometimes called inductive logic programming (ILP) (Muggleton,1992; Muggleton & De Raedt, 1994).Within ILP, researchers considered two broad classes learning problems.first class problems, call logic based relational learning problems,first-order variants sorts classification problems typically considered withinAI machine learning community: prototypical examples include Muggleton et al.'s (1992)formulation ff-helix prediction, King et al.'s (1992) formulation predicting drug activity, Zelle Mooney's (1994) use ILP techniques learn control heuristicsdeterministic parsers. Logic-based relational learning often involves noisy examples ect relatively complex underlying relationship; natural extension propositionalmachine learning, already enjoyed number experimental successes.second class problems studied ILP researchers, target concept Prologprogram implements common list-processing arithmetic function; prototypicalproblems class might learning append two lists, multiply two numbers.learning problems similar character studied area automaticprogramming examples (Summers, 1977; Biermann, 1978), hence might appropriately called automatic logic programming problems. Automatic logic programmingproblems characterized noise-free training data recursive target concepts. Thusproblem central enterprise automatic logic programming|but not, perhaps,logic-based relational learning|is problem learning recursive logic programs.c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiCohengoal paper formally analyze learnability recursive logic programsValiant's (1984) model pac-learnability, thus hopefully shedding lighttask automatic logic programming. summarize results, showsimple recursive programs pac-learnable examples alone, examples plussmall number additional \hints". largest learnable class identify standardlearning model class one-clause constant-depth determinate programsconstant number \closed" recursive literals. largest learnable class identifyrequires extra \hints" class constant-depth determinate programs consistingsingle nonrecursive base clause single recursive clause class describedabove. results proved model identification equivalence queries(Angluin, 1988, 1989), somewhat stronger pac-learnability. Identificationequivalence queries requires target concept exactly identified, polynomialtime, using polynomial number equivalence queries . equivalence queryasks hypothesis program H equivalent target program C ; answerquery either \yes" adversarily chosen example H C differ.model learnability arguably appropriate automatic logic programming tasksweaker model pac-learnability, unclear often approximatelycorrect recursive program useful.Interestingly, learning algorithms analyzed different existing ILPlearning methods; employ unusual method generalizing examples called forcedsimulation . Forced simulation simple analytically tractable alternativemethods generalizing recursive programs examples, n-th root finding(Muggleton, 1994), sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) recursiveanti-unification (Idestam-Almquist, 1993), rarely used experimentalILP systems (Ling, 1991).paper organized follows. presenting preliminary definitions,begin presenting (primarily pedagogical reasons) procedure identifyingequivalence queries single non-recursive constant-depth determinate clause. Then,Section 4, extend learning algorithm, corresponding proof correctness,simple class recursive clauses: class \closed" linear recursive constant-depthdeterminate clauses. Section 5, relax assumptions made make analysiseasier, present several extensions algorithm: extend algorithm linearrecursion k-ary recursion, also show k-ary recursive clause non-recursiveclause learned simultaneously given additional \basecase" oracle. discussrelated work conclude.Although learnable class programs large enough include well-knownautomatic logic programming benchmarks, extremely restricted. companion paper(Cohen, 1995), provide number negative results, showing relaxingrestrictions leads dicult learning problems: particular, learning problemseither hard learning DNF (an open problem computational learning theory),hard cracking certain presumably secure cryptographic schemes. Thus, taken togetherresults companion paper, results delineate boundary learnabilityrecursive logic programs.Although two papers independent, suggest readers wishing readpaper companion paper read paper first.502fiPac-Learning Recursive Logic Programs: Efficient Algorithms2. Backgroundsection present technical background necessary state results.assume, however, reader familiar basic elements logic programming; readers without background referred one standard texts, example(Lloyd, 1987).2.1 Logic Programstreatment logic programs standard, except usually consider bodyclause ordered set literals.paper, consider logic programs without function symbols|i.e., programs written Datalog.1 purpose logic program answercertain questions relative database , DB , set ground atomic facts. (Whenconvenient, also think DB conjunction ground unit clauses.) simplestuse Datalog program check status simple instance . simple instance(for program P database DB ) fact f . pair (P; DB ) said cover f iffDB ^ P ` f . set simple instances covered (P; DB ) precisely minimal modellogic program P ^ DB .paper, primarily consider extended instances consist two parts:instance fact f , simply ground fact, description D, finite setground unit clauses. extended instance e = (f; D) covered (P; DB ) iffDB ^ ^ P ` fextended instances allowed, function-free programs expressive enoughencode surprisingly interesting programs. particular, many programs usuallywritten function symbols re-written function-free programs, exampleillustrates.Example. Consider usual program appending two lists.append([],Ys,Ys).append([XjXs1],Ys,[XjZs1])append(Xs1,Ys,Zs1).One could use program classify atomic facts containing function symbolsappend([1,2],[3],[1,2,3]). program rewritten Datalogprogram classifies extended instances follows:Program P :append(Xs,Ys,Ys)null(Xs).append(Xs,Ys,Zs)components(Xs,X,Xs1) ^components(Zs,X,Zs1) ^1. assumption made primarily convenience. Section 5.2 describe assumptionrelaxed.503fiCohenappend(Xs1,Ys,Zs1).Database DB :null(nil).predicate components(A,B,C) means list head B tailC; thus extended instance equivalent append([1,2],[3],[1,2,3]) wouldInstance fact f :append(list12,list3,list123).Description D:components(list12,1,list2).components(list2,2,nil).components(list123,1,list23).components(list23,2,list3).components(list3,3,nil).note using extended instances examples closely related using groundclauses entailed target clause examples: specifically, instance e = (f; D)covered P; DB iff P ^ DB ` (f D). example shows, also closerelationship extended instances literals function symbolsremoved \ attening" (Rouveirol, 1994; De Raedt & Dzeroski, 1994). electeduse Datalog programs model extended instances paper severalreasons. Datalog relatively easy analyze. close connection Datalogrestrictions imposed certain practical learning systems, FOIL (Quinlan,1990; Quinlan & Cameron-Jones, 1993), FOCL (Pazzani & Kibler, 1992), GOLEM(Muggleton & Feng, 1992).Finally, using extended instances addresses following technical problem. learning problems considered paper involve restricted classes logic programs. Often,restrictions imply number simple instances polynomial; notepolynomial-size domain, questions pac-learnability usually trivial. Requiringlearning algorithms work domain extended instances precludes trivial learningtechniques, however, number extended instances size n exponential n evenhighly restricted programs.2.2 Restrictions Logic Programspaper, consider learnability various restricted classes logic programs. define restrictions; however, first introduceterminology.B1 ^ : : : ^ Br (ordered) definite clause, input variables literalBi variables appearing Bi also appear clause B1 ^ : : : ^ Bi,1 ;variables appearing Bi called output variables . Also, B1 ^ : : : ^ Brdefinite clause, Bi said recursive literal predicate symbolarity A, head clause.504fiPac-Learning Recursive Logic Programs: Efficient Algorithms2.2.1 Types Recursionfirst set restrictions concern type recursion allowed program.every clause program one recursive literal, program linearrecursive . every clause program k recursive literals, programk-ary recursive . Finally, every recursive literal program contains output variables,say program closed recursive.2.2.2 Determinacy Depthsecond set restrictions variants restrictions originally introduced MuggletonFeng (1992). B1 ^ : : : ^ Br (ordered) definite clause, literal Bideterminate iff every possible substitution unifies fact eDB ` B1 ^ : : : ^ Bi,1one maximal substitution DB ` Bi . clause determinateliterals determinate. Informally, determinate clausesevaluated without backtracking Prolog interpreter.also define depth variable appearing clause B1 ^ : : : ^ Br follows.Variables appearing head clause depth zero. Otherwise, let Bi firstliteral containing variable V , let maximal depth input variablesBi ; depth V +1. depth clause maximal depth variableclause.Muggleton Feng define logic program ij -determinate determinate,constant depth i, contains literals arity j less. paper use phrase\constant-depth determinate" instead denote class programs.examples constant-depth determinate programs, taken Dzeroski, MuggletonRussell (1992).Example. Assuming successor functional, following program determinate. maximum depth variable one, variable C secondclause, hence program depth one.less than(A,B)less than(A,B)successor(A,B).successor(A,C) ^ less than(C,B).!following program, computes C , determinate depthtwo.choose(A,B,C)zero(B) ^one(C).choose(A,B,C)decrement(B,D) ^decrement(A,E) ^505fiCohenmultiply(B,C,G) ^divide(G,A,F) ^choose(E,D,F).program GOLEM (Muggleton & Feng, 1992) learns constant-depth determinateprograms, related restrictions adopted several practical learningsystems (Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). learnabilityconstant-depth determinate clauses also received formal study,review Section 6.2.2.3 Mode Constraints Declarationsdefine mode literal L appearing clause C string initialcharacter predicate symbol L, j > 1 j -th character \+"(j , 1)-th argument L input variable \," (j , 1)-th argument Loutput variable. (This definition coincides usual definition Prolog modesarguments head clause inputs. simplification justified,however, considering clauses behave classifying extended instances,ground.) mode constraint simply set mode strings R = fs1 ; : : :; sk g,clause C said satisfy mode constraint R p every literal L bodyC , mode L R.Example. following append program, every literal annotatedmode.append(Xs,Ys,Ys)null(Xs).append(Xs,Ys,Zs)components(Xs,X,Xs1) ^components(Zs,X,Zs1) ^append(Xs1,Ys,Zs1).% mode: null+% mode: components + ,,% mode: components + +,% mode: append + ++clauses program satisfy following mode constraint:f components + ,,; components + +,; components + ,+;components , ++; components + ++; null +append + +,;append + ,+;append , ++;append + ++gMode constraints commonly used analyzing Prolog code; instance,used many Prolog compilers. sometimes use alternative syntax modeconstraints parallels syntax used Prolog systems: instance, maywrite mode constraint \components + ,," \components (+; ,; ,)".define declaration tuple (p; a0; R) p predicate symbol, a0integer, R mode constraint. say clause C satisfies declarationhead C arity a0 predicate symbol p, every literal L bodyC mode L appears R.506fiPac-Learning Recursive Logic Programs: Efficient Algorithms2.3 Model Learnabilitysection, present model learnability. first review necessarydefinitions standard learning model, model learning equivalence queries(Angluin, 1988, 1989), discuss relationship learning models.introduce extension model necessary analyzing ILP problems.2.3.1 Identification Equivalence QueriesLet X set. call X domain , call elements X instances . Defineconcept C X representation subset X , define language Langset concepts. paper, rather casual distinctionconcept set represents; risk confusion refer setrepresented concept C extension C . Two concepts C1 C2extension said (semantically) equivalent .Associated X Lang two size complexity measures , usefollowing notation:size complexity concept C 2 Lang written j C j .size complexity instance e 2 X written j ej .set, Sn stands set elements size complexity greatern. instance, Xn = fe 2 X : j ej ng Langn = fC 2 Lang : j C j ng.assume size measures polynomially related number bits neededrepresent C e.first learning model consider model identification equivalencequeries . goal learner identify unknown target concept C 2 Lang|is, construct hypothesis H 2 Lang H C . Informationtarget concept gathered equivalence queries . input equivalencequery C hypothesis H 2 Lang. H C , response query\yes". Otherwise, response query arbitrarily chosen counterexample |aninstance e symmetric difference C H .deterministic algorithm Identify identifies Lang equivalence queries iffevery C 2 Lang, whenever Identify run (with oracle answering equivalence queriesC ) eventually halts outputs H 2 Lang H C . Identifypolynomially identifies Lang equivalence queries iff polynomial poly (nt; ne )point execution Identify total running time boundedpoly (nt ; ne ), nt = j C j ne size largest counterexample seen far,0 equivalence queries made.2.3.2 Relation Pac-Learnabilitymodel identification equivalence queries well-studied (Angluin, 1988,1989). known language learnable model, also learnableValiant's (1984) model pac-learnability. (The basic idea behind resultequivalence query hypothesis H emulated drawing set random507fiCohenexamples certain size. counterexample H , one returnsfound counterexample answer equivalence query. counterexamplesfound, one assume high confidence H approximately equivalenttarget concept.) Thus identification equivalence queries strictly stronger modelpac-learnability.existing positive results pac-learnability logic programs rely showingevery concept target language emulated boolean conceptpac-learnable class (Dzeroski et al., 1992; Cohen, 1994). resultsilluminating, also disappointing, since one motivations considering firstorder representations first place allow one express concepts cannoteasily expressed boolean logic. One advantage studying exact identificationmodel considering recursive programs essentially precludes use sortproof technique: many recursive programs approximated boolean functionsfixed set attributes, exactly emulated boolean functions.2.3.3 Background Knowledge Learningframework described standard, one possible formalization usualsituation inductive concept learning, user provides set examples (incase counterexamples queries) learning system attempts find usefulhypothesis. However, typical ILP system, setting slightly different, usuallyuser provides clues target concept addition examples. ILPsystems user provides database DB \background knowledge" addition setexamples; paper, assume user also provides declaration.account additional inputs necessary extend framework describedsetting learner accepts inputs training examples.formalize this, introduce following notion \language family". Langset clauses, DB database Dec declaration, define Lang[DB ; Dec]set pairs (C; DB ) C 2 Lang C satisfies Dec . Semantically,pair denote set extended instances (f; D) covered (C; DB ). Next,DB set databases DEC set declarations, defineLang[DB ; DEC ] = fLang[DB ; Dec ] : DB2 DB Dec 2 DECgset languages called language family .extend definition identification equivalence queries language families follows. language family Lang[DB; DEC ] identifiable equivalencequeries iff every language set identifiable equivalence queries. languagefamily Lang[DB; DEC ] uniformly identifiable equivalence queries iff singlealgorithm Identify (DB ; Dec) identifies language Lang[DB ; Dec ] familygiven DB Dec .Uniform polynomial identifiability language family defined analogously:Lang[DB; DEC ] uniformly polynomially identifiable equivalence queries iffpolynomial time algorithm Identify (DB ; Dec ) identifies language Lang[DB ; Dec]family given DB Dec . Note Identify must run time polynomialsize inputs Dec DB well target concept.508fiPac-Learning Recursive Logic Programs: Efficient Algorithms2.3.4 Restricted Types Background Knowledgedescribe number restricted classes databases declarations.One restriction make throughout paper assumepredicates interest bounded arity. use notation a-DB setdatabases contain facts arity less, notation a-DEC setdeclarations (p; a0; R) every string 2 R length + 1 less.technical reasons, often convenient assume database containsequality predicate |that is, predicate symbol equal equal (ti ; ti) 2 DB everyconstant ti appearing DB , equal (ti ; tj ) 62 DB ti 6= tj . Similarly,often wish assume declaration allows literals form equal(X,Y), Xinput variables. DB (respectively DEC ) set databases (declarations)use DB = (DEC = ) denote corresponding set, additional restrictiondatabase (declaration) must contain equality predicate (respectively modeequal (+; +)).sometimes also convenient assume declaration (p; a0; R) allowssingle valid mode predicate: i.e., predicate q Rsingle mode constraint form qff. declaration called unique-modedeclaration. DEC set declarations use DEC 1 denote correspondingset declarations additional restriction declaration unique-mode.Finally, note typical setting, facts appear database DBdescriptions extended instances arbitrary: instead, representative\real" predicate (e.g., relationship list components example above).One way formalizing assume facts drawn restricted set F ;using assumption one define notion determinate mode . f = p(t1 ; : : :; tk )fact predicate symbol p pff mode, define inputs (f; pff)tuple hti1 ; : : :; tik i, i1, : : : , ik indices ff containing \+". Also defineoutputs (f; pff) tuple htj1 ; : : :; tjl i, j1 , : : : , jl indices ff containing\,". mode string pff predicate p determinate F iff relationfhinputs (f; pff); outputs (f; pff)i : f 2 Fgfunction. Informally, mode determinate input positions facts Ffunctionally determine output positions.set declarations containing modes determinate F denotedDetDEC F . However, paper, set F assumed fixed, thusgenerally omit subscript.program consistent determinate declaration Dec 2 DetDEC must determinate, defined above; words, consistency determinate declarationsucient condition semantic determinacy. also condition verifiedsimple syntactic test.2.3.5 Size Measures Logic ProgramsAssuming predicates arity less constant also allows simplesize measures used. paper, measure size database DBcardinality; size extended instance (f; D) cardinality D; size509fiCohendeclaration (p; a0; R) cardinality R; size clause B1 ^ : : : ^ Brnumber literals body.3. Learning Nonrecursive Clauselearning algorithms presented paper use generalization techniquecall forced simulation. way introduction technique, considerlearning algorithm non-recursive constant-depth clauses. result presentedprimarily pedagogical reasons, may interest own: independentprevious proofs pac-learnability class (Dzeroski et al., 1992), alsosomewhat rigorous previous proofs.Although details analysis algorithm non-recursive clauses somewhat involved, basic idea behind algorithm quite simple. First, highlyspecific \bottom clause" constructed, using two operations call DEEPENCONSTRAIN . Second, bottom clause generalized deleting literals covers positive examples: algorithm generalizing clause cover example(roughly) simulate clause example, delete literals would causeclause fail. remainder section describe analyze learningalgorithm detail.3.1 Constructing \Bottom Clause"Let Dec = (p; a0; R) declaration let B1 ^ : : : ^ Br definite clause.define^DEEPEN Dec (A B1 ^ : : : ^ Br ) B1 ^ : : : ^ Br ^ (Li )Li 2LDLD maximal set literals Li satisfy following conditions:clause B1 ^ : : : ^ Br ^ Li satisfies mode constraints given R;Li 2 LD mode predicate symbol Lj 2 LD ,input variables Li different input variables Lj ;every Li least one output variable, output variables Lidifferent other, also difference output variablesLj 2 LD .extension notation, define DEEPEN iDec (C ) result applyingfunction DEEPEN Dec repeatedly times C , i.e.,(= 0DEEPEN Dec (C ) Ci,1DEEPEN Dec (DEEPEN Dec (C )) otherwisedefine function CONSTRAIN Dec^CONSTRAIN Dec (A B1 ^ : : : ^ Br ) B1 ^ : : : ^ Br ^ (Li )Li 2LCLC set literals Li B1 ^ : : : ^ Br ^ Li satisfies modeconstraints given R, Li contains output variables.510fiPac-Learning Recursive Logic Programs: Efficient AlgorithmsExample. Let D0 declaration (p; 2; R) R contains modeconstraints mother (+; ,), father (+; ,), male (+), female (+), equal (+; +).DEEPEN D0(p(X,Y) )p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)DEEPEN 2D0(p(X,Y) ) DEEPEN D0 (DEEPEN D0 (p(X,Y) ))p(X,Y)mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^mother(XM,XMM)^father(XM,XMF)^ mother(XF,XFM)^father(XF,XFF)^mother(YM,YMM)^father(YM,YMF)^ mother(YF,YFM)^father(YF,YFF)CONSTRAIN D0(DEEPEN D0(p(X,Y) ))p(X,Y)mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^male(X)^female(X)^male(Y)^female(Y)^male(XM)^female(XM)^male(XF)^female(XF)^male(YM)^female(YM)^male(YF)^female(YF)^equal(X,X)^equal(X,XM)^equal(X,XF)^equal(X,Y)^equal(X,YM)^equal(X,YF)^equal(XM,X)^equal(XM,XM)^equal(XM,XF)^equal(XM,Y)^equal(XM,YM)^equal(XM,YF)^equal(XF,X)^equal(XF,XM)^equal(XF,XF)^equal(XF,Y)^equal(XF,YM)^equal(XF,YF)^equal(Y,X)^equal(Y,XM)^equal(Y,XF)^equal(Y,Y)^equal(Y,YM)^equal(Y,YF)^equal(YM,X)^equal(YM,XM)^equal(YM,XF)^equal(YM,Y)^equal(YM,YM)^equal(YM,YF)^equal(YF,X)^equal(YF,XM)^equal(YF,XF)^equal(YF,Y)^equal(YF,YM)^equal(YF,YF)Let us say clause C1 subclause clause C2 heads C1 C2identical, every literal body C1 also appears C2 , literalsbody C1 appear order C2. functions DEEPENCONSTRAIN allow one easily describe clause interesting property.Theorem 1 Let Dec = (p; a0; R) declaration a-DetDEC =, let X1; : : :; Xa distinctvariables, define clause BOTTOM follows:BOTTOM (Dec ) CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))constants a, following true:size BOTTOM d(Dec) polynomial j Decj ;every depth-d clause satisfies Dec (and hence, determinate) (semantically)equivalent subclause BOTTOM (Dec ).00511fiCohenbegin algorithm Force1NR (d ; Dec; DB ):% BOTTOM specific possible clauselet H BOTTOM d(Dec)repeatAns answer query \Is H correct?"Ans =\yes" return Helseif Ans negative examplereturn \no consistent hypothesis"elseif Ans positive example e+% generalize H minimally cover e+let (f; D) components extended instance e+H ForceSimNR (H ; f ; Dec; (DB [ ))H = FAILUREreturn \no consistent hypothesis"endendifendifendrepeatFigure 1: learning algorithm nonrecursive depth-d determinate clausesProof: See Appendix A. related result also appears Muggleton Feng (1992).Example. C1 D1 equivalent, C2 D2. Notice D1D2 subclauses BOTTOM 1 (D0).C1 : p(A,B) mother(A,C)^father(A,D)^ mother(B,C)^father(B,D)^male(A)D1 : p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^male(X)^equal(XM,YM)^equal(XF,YF)C2 : p(A,B) father(A,B)^female(A)D2 : p(X,Y) father(X,XF)^female(X)^equal(XF,Y)C1 D1, p(X,Y) true X 's brother. C2 D2, p(X,Y)true X 's daughter, X 's father.3.2 Learning AlgorithmTheorem 1 suggests may possible learn non-recursive constant-depth determinate clauses searching space subclauses BOTTOM ecientmanner. Figures 1 2 present algorithm called Force1 NR Decunique-mode declaration.Figure 1 presents top-level learning algorithm, Force1 NR . Force1 NR takesinput database DB declaration Dec , begins hypothesizing clauseBOTTOM (Dec ). positive counterexample e+ , current hypothesis generalized little possible order cover e+ . strategy means hypothesis512fiPac-Learning Recursive Logic Programs: Efficient Algorithmsbegin subroutine ForceSimNR(H ; f ; Dec; DB ):% \forcibly simulate" H fact ff 2 DB return Helseif head H f cannot unifiedreturn FAILUREelselet H 0 Hlet mgu f head H 0literal L body H 0substitution 0 L0 2 DB0, 0 general substitutionelsedelete L body H 0 , togetherliterals L0 supported (directly indirectly) Lendendifendforreturn H 0endifFigure 2: Forced simulation nonrecursive depth-d determinate clausesalways least general hypothesis covers positive examples; hence, negativecounterexample e, ever seen, algorithm abort message consistenthypothesis exists.minimally generalize hypothesis H , function ForceSimNR used. subroutine shown Figure 2. figure, following terminology used.output variable L input variable L0 , say L directly supports L0.say L supports L0 iff L directly supports L0, L directly supports literalL00 supports L0. (Thus \supports" transitive closure \directly supports".)ForceSim NR deletes H minimal number literals necessary let H cover e+ .this, ForceSim NR simulates action Prolog interpreter evaluating H , exceptwhenever literal L body H would fail, literal deleted, alongliterals L0 supported L.idea learning repeated generalization old one; particular, previousmethods exist learning definite clause generalizing highly-specific one. example, CLINT (De Raedt & Bruynooghe, 1992) generalizes \starting clause" guidedqueries made user; PROGOL (Srinivasan, Muggleton, King, & Sternberg, 1994)guides top-down generalization process known bottom clause; Rouveirol (1994)describes method generalizing bottom clauses created saturation. Force1 NR algorithm thus interest novelty, provably correct ecient,noted theorem below.513fiCohenparticular, let d-DepthNonRec language nonrecursive clauses depthless (and hence i-DepthNonRec[DB; j -DetDEC ] language nonrecursive ij determinate clauses). following result:Theorem 2 constants d, language familyd-DepthNonRec[DB= ; a-DetDEC =1]uniformly identifiable equivalence queries.Proof: show Force1 NR uniformly identifies language family polyno-mial number queries. begin following important lemma, characterizesbehavior ForceSimNR .Lemma 3 Let Dec declaration DetDEC =1 , let DB database, let f fact, letH determinate nonrecursive clause satisfies Dec. one following conditionsmust hold:ForceSimNR(H ; f ; Dec; DB ) returns FAILURE, subclause H 0 H satisfiesDec constraint H 0 ^ DB ` f ; or,ForceSimNR(H ; f ; Dec; DB ) returns clause H 0, H 0 unique syntacticallylargest subclause H satisfies Dec constraint H 0 ^ DB ` f .Proof lemma: avoid repetition, refer syntactically maximal subclausesH 0 H satisfy Dec constraint H 0 ^ DB ` f \admissible subclauses"proof below.Clearly lemma true H FAILURE returned ForceSim NR . remainingcases loop algorithm executed, must establish two claims(under assumptions f unify, f 62 DB ):Claim 1. L retained, every admissible subclause contains L.Claim 2. L deleted, admissible subclause contains L.First, however, observe deleting literal L may cause modeliterals violate mode declarations Dec . easy see L deletedclause C , mode literals L0 directly supported L change. Thus Csatisfies unique-mode declaration prior deletion L, deletion Lliterals L0 directly supported L invalid modes.Now, see Claim 1 true, suppose instead false. mustmaximal subclause C 0 H satisfies Dec , covers fact f ,contain L. argument above, C 0 contain L satisfied Dec , C 0contains literals L0 H supported L. Hence output variables Ldisjoint variables appearing C 0. means L addedC 0 resulting clause would still satisfy Dec cover f , leads contradictionsince C 0 assumed maximal.verify Claim 2, let us introduce following terminology. C = (A B1 ^ : : : ^ Br )clause DB database, say substitution (DB ; f )-witness514fiPac-Learning Recursive Logic Programs: Efficient AlgorithmsC iff associated proof C ^ DB ` f (or precisely, iff = f8i : 1 r; Bi 2 DB .) claim following condition invariantloop ForceSim NR algorithm.Invariant 1. Let C admissible subclause contains literals H 0 preceding L (i.e., contains literals H retained previousiterations algorithm). every (DB ; f )-witness C superset .easily established induction number iterations loop.condition true loop first entered, since initially general unifierf . condition remains true iteration L deleted, sinceunchanged. Finally, condition remains true iteration L retained:0 maximally general, may assign values output variables L,determinacy one assignment output variables L make L true. Henceevery (DB ; f )-witness C must contain bindings .Next, inductive argument Claim 1 one show every admissiblesubclause C must contain literals retained previous iterationsloop, leading following strengthening Invariant 1:Invariant 10. Let C admissible subclause. every (DB ; f )-witness Csuperset .Now, notice two types literals deleted: (a) literals L supersetmake L true, (b) literals L0 supported literal L precedingtype. case (a), clearly L cannot part admissible subclause, since supersetmakes L succeed, supersets witnesses admissible clauses.case (b), L0 cannot part admissible subclause, since declaration invalidunless L present clause, argument L cannot clause.concludes proof lemma.prove theorem, must establish following properties identificationalgorithm.Correctness. Theorem 1, target program d-DepthNonRec[DB ; Dec],clause CT equivalent target, subclauseBOTTOM (Dec ). H initially BOTTOM hence superclause CT . considerinvoking ForceSim NR positive counterexample e+ . Lemma 3, invocationsuccessful, H replaced H 0, longest subclause H covers e+ . SinceCT subclause H covers e+ , means H 0 superclauseCT . Inductively, then, hypothesis always superclause target.Further, since counterexample e+ always instance coveredcurrent hypothesis H , every time hypothesis updated, new hypothesis propersubclause old. means Force1 NR eventually identify target clause.Eciency. number queries made polynomial j Decj j DB j , since Hinitially size polynomial j Dec j , reduced size time counterexampleprovided. see counterexample processed time polynomial nr , ne ,nt, notice since length H polynomial, number repetitionsloop ForceSim NR also polynomial; further, since arity literals L bounded515fiCohena, anb + ane constants exist DB [ D, hence (anb + ane )asubstitutions 0 check inside loop, polynomial. Thus executionForceSim NR requires polynomial time.concludes proof.4. Learning Linear Closed Recursive ClauseRecall clause one recursive literal, clause linear recursive ,recursive literal contains output variables, clause closed linearrecursive. section, describe Force1 algorithm extendedlearn single linear closed recursive clause.2 presenting extension, however,would first like discuss reasonable-sounding approach that, closer examination, turnsincorrect.4.1 Remark Recursive ClausesOne plausible first step toward extending Force1 recursive clauses allow recursiveliterals hypotheses, treat way literals|that is, includerecursive literals initial clause BOTTOM , delete literals graduallypositives examples received. problem approach simpleway check recursive literal clause succeeds fails particular example.makes impossible simply run ForceSimNR clauses containing recursive literals.straightforward (apparent) solution problem assume oracle existsqueried success failure recursive literal. closed recursiveclauses, sucient assume oracle MEMBERCt (DB ; f ) answersquestionDB ^ P ` f ?Ct unknown target concept, f ground fact, DB database. Givenoracle, one determine closed recursive literal Lr retainedchecking MEMBERCT (DB ; Lr ) true. oracle close notionmembership query used computational learning theory.natural extension Force1NR learning algorithm recursive clauses|infact algorithm based similar ideas previously conjectured pac-learnclosed recursive constant-depth determinate clauses (Dzeroski et al., 1992). Unfortunately,algorithm fail return clause consistent positive counterexample.illustrate this, consider following example.Example. Consider using extension Force1NR described learnfollowing target program:append(Xs,Ys,Zs)2. reader may object useful recursive programs always least two clauses|a recursiveclause nonrecursive base case. posing problem learning single recursive clause,thus assuming non-recursive \base case" target program provided background knowledge,either background database DB , description atoms extended instances.516fiPac-Learning Recursive Logic Programs: Efficient Algorithmscomponents(Xs,X,Xs1),components(Zs,Z,Zs1),X1=Z1,append(Xs1,Ys,Zs1).program determinate, depth 1, satisfies following setdeclarations:components(+,,,,).null(+).equal(+,+).odd(+).append(+,+,+).assume also database DB defines predicate null trueempty lists, odd true constants 1 3.see forced simulation fail, consider following positive instancee = (f; D):f = append (l12 ; l3 ; l123 )= f cons(l123,1,l23), cons(l23,2,l3), cons(l3,3,nil),cons(l12,1,l2), cons(l2,2,nil),append(nil,l3,l3) gsimply \ attened" form append([1,2],[3],[1,2,3]), togetherappropriate base case append([],[3],[3]). consider beginning clauseBOTTOM 1 generalizing using ForceSimNR cover positive instance.process illustrated Figure 3. clause left figureBOTTOM (Dec ); clause right output forcibly simulatingclause f ForceSimNR . (For clarity we've assumedsingle correct recursive call remains forced simulation.)resulting clause incorrect, cover given example e.easily seen stepping actions Prolog interpretergeneralized clause Figure 3. nonrecursive literals succeed, leading subgoal append(l2,l3,l23) (or usual Prolog notation,append([2],[3],[2,3])). subgoal fail literal odd(X1), X1bound 2 subgoal, fact odd(2) true DB [ D.example illustrates pitfall policy treating recursive non-recursiveliterals uniform manner (For discussion, see also (Bergadano & Gunetti, 1993; DeRaedt, Lavrac, & Dzeroski, 1993).) Unlike nonrecursive literals, truth fact Lr(corresponding recursive literal Lr ) imply clause containing Lrsucceed; may first subgoal Lr succeeds, deeper subgoals fail.517fiCohenBOTTOM 1 (Dec ):ForceSimNR (BOTTOM 1(Dec); f; Dec; DB [ D) :append(Xs,Ys,Zs)append(Xs,Ys,Zs)components(Xs,X1,Xs1)^components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^components(Zs,Z1,Zs1)^null(Xs)^null(Ys1)^null(Ys)^equal(X1,Z1)^..odd(X1)^.odd(Y1)^null(Ys1)^odd(Z1)^null(Zs1),append(Xs1,Ys,Zs1).equal(Xs,Xs)^...equal(X1,Z1)^...equal(Zs1,Zs1)^odd(Xs)^...odd(X1)^odd(Y1)^odd(Z1)^...odd(Zs1)^append(Xs,Xs,Xs)^...append(Zs1,Zs1,Zs1).Figure 3: recursive clause generalization ForceSimNR4.2 Forced Simulation Recursive Clausessolution problem replace calls membership oracle algorithmsketched call routine forcibly simulates actions top-downtheorem-prover recursive clause. particular, following algorithm suggested.First, build nonrecursive \bottom clause", done ForceSimNR . Second, findrecursive literal Lr appending Lr bottom clause yields recursive clausegeneralized cover positive examples.nonrecursive case, clause generalized deleting literals, using straightforward generalization procedure forced simulation nonrecursive clauses.forced simulation, failing nonrecursive subgoals simply deleted; however,recursive literal Lr encountered, one forcibly simulates hypothesis clause recursively518fiPac-Learning Recursive Logic Programs: Efficient Algorithmsbegin subroutine ForceSim (H ; f ; Dec; DB ; h ):% \forcibly simulate" recursive clause H f% 1. check infinite loopsh < 0 return FAILURE% 2. check see f already coveredelseif f 2 DB return H% 3. check see f cannot coveredelseif head H f cannot unifiedreturn FAILUREelselet Lr recursive literal Hlet H 0 H , fLrg% 4. delete failing non-recursive literals ForceSimNRlet head H 0let mgu eliteral L body H 0substitution 0 L0 2 DB0, 0 general substitutionelsedelete L body H 0 , togetherliterals L0 supported (directly indirectly) Lendifendfor% 5. generalize H 0 recursive subgoal LrLr ground return ForceSim(H 0 [ fLr g; Lr; Dec; DB ; h , 1)else return FAILUREendendifendifFigure 4: Forced simulation linear closed recursive clauses519fiCohencorresponding recursive subgoal. implementation forced simulation linearclosed recursive clauses shown Figure 4.extended algorithm similar ForceSimNR , differs recursiveliteral Lr reached simulation H , corresponding subgoal Lr created,hypothesized clause recursively forcibly simulated subgoal. ensuresgeneralized clause also succeed subgoal. reasons become clearshortly, would like algorithm terminate, even original clause H entersinfinite loop used top-down interpreter. order ensure termination, extraargument h passed ForceSim . argument h represents depth bound forcedsimulation.summarize, basic idea behind algorithm Figure 4 simulate hypothesized clause H f , generalize H deleting literals whenever H would failf subgoal f .Example.Consider using ForceSim forcibly simulate following recursive clauseBOTTOM 1(Dec ) [ Lrappend(Xs,Ys,Zs)components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^null(Xs)^: : : ^null(Zs1)^odd(Xs)^: : : ^odd(Zs1)^equal(Xs,Xs)^: : : ^equal(Zs1,Zs1)^append(Xs1,Ys,Zs1)recursive literal Lr append(Xs1,Ys,Zs1). also assume ftaken extended query e = (f; D), attened versioninstance append([1,2],[3],[1,2,3]) used previous example; Decset declarations previous example; database DB[ null (nul ).executing steps 1-4 ForceSim, number failing literals deleted,leading substitution3 fXs = [1; 2], Ys = [3], Zs = [1; 2; 3], X1 = 1,Xs1 = [2], Y1 = 3, Ys1 = [], Z1 = 1, Zs1 = [2; 3]g following reducedclause:append(Xs,Ys,Zs)components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^null(Ys1)^odd(X1)^odd(Y1)^odd(Z1)^equal(X1,Z1)^append(Xs1,Ys,Zs1)Hence recursive subgoalLr = append (Xs1 ; Ys ; Zs1 ) = append ([2]; [3]; [2; 3])3. Note readability, using term notation rather attened notation Xs = l12,Ys = l3, etc.520fiPac-Learning Recursive Logic Programs: Efficient AlgorithmsRecursively applying ForceSim goal produces substitution fXs = [2],Ys = [3], Zs = [2; 3], X1 = 2, Xs1 = [], Y1 = 3, Ys1 = [], Z1 = 2, Zs1 = [3]galso results deleting additional literals odd(X1) odd(Z1).next recursive subgoal Lr = append ([]; [3]; [3]); since clause includeddatabase DB , ForceSim terminate. final clause returnedForceSim case following:append(Xs,Ys,Zs)components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^null(Ys1)^odd(Y1)^equal(X1,Z1)^append(Xs1,Ys,Zs1)Notice clause cover e.Section 3 begin analysis showing correctness forced simulationalgorithm|i.e., showing forced simulation indeed produce unique maximallyspecific generalization input clause covers example.proof correctness uses induction depth proof. Let us introduceadditional notation, write P ^ DB `h f Prolog program (P; DB )used prove fact f proof depth h less. (The notion depth proofusual one; define looking f database DB proof depth zero.)following result concerning ForceSim algorithm.Theorem 4 Let Dec declaration DetDEC =1, let DB database, let f fact,let H determinate closed linear recursive clause satisfies Dec. onefollowing conditions must hold:ForceSim(H; f; Dec; DB ; h) returns FAILURE, recursive subclause H 0 Hsatisfies Dec constraint H 0 ^ DB `h f ; or,ForceSim(H; f; Dec; DB ; h) returns clause H 0, H 0 unique syntacticallylargest recursive subclause H satisfies Dec constraint H 0^DB `h f .Proof: avoid repetition, refer syntactically maximal recursive (nonrecursive) subclauses H 0 H satisfy Dec constraint H 0 ^ DB `h f\admissible recursive (nonrecursive) subclauses" respectively.proof largely parallels proof Lemma 3|in particular, similar argumentsshow clause returned ForceSim satisfies conditions theorem wheneverFAILURE returned whenever H returned. Note correctness ForceSimH returned establishes base case theorem h = 0.case depth h > 0, let us assume theorem holds depth h , 1proceed using mathematical induction. arguments Lemma 3 show followingcondition true loop terminates.Invariant 10. H 0 unique maximal nonrecursive admissible subclause H , every(DB ; f )-witness H 0 superset .521fiCohenbegin algorithm Force1 (d ; Dec; DB ):% BOTTOM specific possible clauselet Lr1 ; : : :; Lrp possible closed recursive literals BOTTOM d(Dec)choose unmarked recursive literal Lrilet H BOTTOM d(Dec) [ fLri grepeatanswer query \Is H correct?"AnsAns =\yes" return Helseif Ans negative example e,HFAILUREelseif Ans positive example e+% generalize H minimally cover e+let (f; D) components e+H ForceSim (H ; f ; Dec; (DB [ ); (a j Dj + j DBj )a )a0 arity clause head given Dec0endifH = FAILURErecursive literals markedreturn \no consistent hypothesis"elsemark Lrichoose unmarked recursive literal Lrjlet H BOTTOM d(Dec) [ fLrj gendendifendifendrepeatFigure 5: learning algorithm nonrecursive depth-d determinate clausesNow, let us assume admissible recursive subclause H . Clearly H mustcontain recursive literal Lr H , since Lr recursive literal H . Further,nonrecursive clause H^ = H , fLr g must certainly satisfy Dec also H^ ^ DB ` f ,must (by maximality H 0) subclause H 0. Hence H must subclauseH 0 [ fLr g. Finally, Lr ground (i.e., Lr closed clause H 0 [ Lr )Invariant 10, clause H must also satisfy H ^ DB ` Lr proof depth h , 1.(This simply equivalent saying recursive subgoal Lr generated proofmust succeed.)inductive hypothesis, then, recursive call must return unique maximaladmissible recursive subclause H 0 [ Lr , argument must alsounique maximal admissible recursive subclause H .Thus induction theorem holds.522fiPac-Learning Recursive Logic Programs: Efficient Algorithms4.3 Learning Algorithm Linear Recursive ClausesGiven method generalizing recursive clauses, one construct learning algorithm recursive clauses follows. First, guess recursive literal Lr , makeH = BOTTOM [ Lr initial hypothesis learner. Then, ask series equivalencequeries. positive counterexample e+ , use forced simulation minimally generalizeH cover e+ . negative example, choose another recursive literal L0r , resethypothesis H = BOTTOM [ L0r .Figure 5 presents algorithm operates along lines. Let d-DepthLinRecdenote language linear closed recursive clauses depth less.following result:Theorem 5 constants d, language familyd-DepthLinRec[DB=; a-DetDEC =1]uniformly identifiable equivalence queries.Proof: show Force1 uniformly identifies language family polynomial number queries.Correctness query eciency. aj Dj + aj DBj constantsset DB [ D, (aj Dj + aj DB j )a a0 -tuples constants, hence(aj Dj + aj DB j )a distinct recursive subgoals Lr might produced provinglinear recursive clause C covers extended instance (f; D). Thus every terminating prooffact f using linear recursive clause C must depth (aj Dj + aj DB j )a less; i.e.,h = (aj Dj + aj DB j )a ,C ^ DB ^ `h f iff C ^ DB ^ ` fThus Theorem 4 strengthened: value h used Force1, subroutineForceSim returns syntactically largest subclause H covers example (f; D)whenever subclause exists, returns FAILURE otherwise.argue correctness algorithm follows. Assume hypothesized recursive literal \correct"|i.e., target clause CT subclauseBOTTOM [ Lr . case easy see Force1 identify CT , using argument parallels one made Force1 NR . analogy Force1 NR , easysee polynomial number equivalence queries made involving correctrecursive literal.Next assume Lr correct recursive literal. CT need subclauseBOTTOM [ Lr , response equivalence query may either positivenegative counterexample. positive counterexample e+ received ForceSimcalled, result may FAILURE, may proper subclause H coverse+ . Thus result choosing incorrect Lr (possibly empty) sequencepositive counterexamples followed either negative counterexample FAILURE. Sinceequivalence queries involving correct recursive literal answered eitherpositive counterexample \yes"4, negative counterexample FAILUREobtained, must Lr incorrect.00004. Recall answer \yes" equivalence query means hypothesis correct.523fiCohennumber variables BOTTOM bounded aj BOTTOM (Dec )j ,closed recursive literal completely defined a0-tuple variables, numberpossible closed recursive literals Lr boundedp = (aj BOTTOM (Dec )j )a0Since j BOTTOM (Dec )j polynomial j Dec j , p also polynomial j Dec j . meanspolynomial number incorrect Lr 's need discarded. sincesuccessive hypothesis using single incorrect Lr proper subclause previous hypothesis, polynomial number equivalence queries needed discard incorrectLr . Thus polynomial number equivalence queries made involving incorrectrecursive literals.Thus Force1 needs polynomial number queries identify Ct.Eciency. ForceSim runs time polynomial arguments H , f , Dec, DB [h. ForceSim called Force1, h always polynomial ne j DB j ,H always larger j BOTTOM d(Dec)j + 1, turn polynomial sizeDec . Hence every invocation ForceSim requires time polynomial ne , Dec , DB ,hence Force1 processes query polynomial time.completes proof.result somewhat surprising, shows recursive clauses learnedeven given adversarial choice training examples. contrast, implemented ILPsystems require well-choosen examples learn recursive clauses.formal result also strengthened number technical ways. Oneinteresting strengthenings consider variant Force1 maintainsfixed set positive negative examples, constructs set least generalclauses consistent examples: could done takingclauses BOTTOM [ Lr1 , : : : , BOTTOM [ Lrp , forcibly simulatingpositive examples turn, discarding clauses cover one negativeexamples. set clauses could used tractably encode version spaceconsistent programs, using [S; N ] representation version spaces (Hirsh, 1992).5. Extending Learning Algorithmconsider number ways result Theorem 5 extended.5.1 Equality-Predicate Unique-Mode AssumptionsTheorem 5 shows language familyd-DepthLinRec[DB=; a-DetDEC =1]identifiable equivalence queries. natural ask result extendeddropping assumptions equality predicate present declarationcontains unique legal mode predicate: is, result extendedlanguage familyd-DepthLinRec[DB; a-DetDEC ]524fiPac-Learning Recursive Logic Programs: Efficient Algorithmsextension fact straightforward. Given database DB declaration Dec =(p; a0; R) satisfy equality-predicate unique-mode assumptions, onemodify follows.1. every constant c appearing DB , add fact equal (c ; c ) DB .2. every predicate q k valid modes qs1 , : : : , qsk R:(a) remove mode declarations q , replace k mode stringsk new predicates qs1 , : : : , qsk , letting qsi si unique legal modepredicate qsi ;(b) remove every fact q (t1 ; : : :; ta) predicate q DB , replacek facts qs1 (t1 ; : : :; ta ), : : : , qsk (t1 ; : : :; ta).Note arity predicates bounded constant a, number modesk predicate q bounded constant 2a , hence transformationsperformed polynomial time, polynomial increase size DecDB .Clearly target clause Ct 2 d-DepthLinRec[DB ; Dec ] equivalent clauseCt0 2 d-DepthLinRec[DB 0; Dec0], DB 0 Dec 0 modified versions DBDec constructed above. Using Force1 possible identify Ct0 . (In learning Ct0, onemust also perform steps 1 2b description part every counterexample(f; D).) Finally, one convert Ct0 equivalent clause d-DepthLinRec[DB ; Dec]repeatedly resolving clause equal(X,X) , also replacing every predicatesymbol qsi q .leads following strengthening Theorem 5:Proposition 6 constants d, language familyd-DepthLinRec[DB; a-DetDEC ]uniformly identifiable equivalence queries.5.2 Datalog Assumptionfar assumed target program contains function symbols,background knowledge provided user database ground facts. convenientformal analysis, assumptions relaxed.Examination learning algorithm shows database DB used twoways.forcibly simulating hypothesis extended instance (f; D), necessaryfind substitution 0 makes literal L true database DB [ D.done algorithmically DB sets ground facts, also plausibleassume user provided oracle answers polynomial timemode-correct query L database DB . Specifically, answer oracleeither525fiCohen{ (unique) most-general substitution 0 DB ^ ` L0 L0ground;{ \no" 0 exists.oracle would presumably take form ecient theorem-prover DB .calling ForceSim, top-level learning algorithm uses DB determinedepth bound length proof made using hypothesis program. Again,reasonable assume user provide information directly,form oracle. Specifically, oracle would provide fact f polynomialupper bound depth proof f target program.Finally note ecient (but non-ground) background knowledge allowed,function symbols always removed via attening (Rouveirol, 1994). transformation also preserves determinacy, although may increase depth|in general, depthattened clause depends also term depth original clause. Thus, assumptiontarget program Datalog replaced assumptions term depthbounded constant, two oracles available: oracle answers queriesbackground knowledge, depth-bound oracle. types oraclesfrequently assumed literature (Shapiro, 1982; Page & Frisch, 1992; Dzeroski et al.,1992).5.3 Learning k-ary Recursive Clausesalso natural ask Theorem 5 extended clauses linear recursive.One interesting case case closed k-ary recursive clauses constant k.straightforward extend Force1 guess tuple k recursive literals Lr1 , : : : , Lrk ,extend ForceSim recursively generalize hypothesis clause factsLr1 , : : : , Lrk . arguments Theorems 4 5 modified showextension identify target clause polynomial number equivalence queries.Unfortunately, however, longer case ForceSim runs polynomial time.easily seen one considers tree recursive calls made ForceSim;general, tree branching factor k polynomial depth, hence exponentialsize. result unsurprising, implementation ForceSim described forciblysimulates depth-bounded top-down interpreter, k-ary recursive program takeexponential time interpret interpreter.least two possible solutions problem. One possible solutionretain simple top-down forced simulation procedure, require user providedepth bound tighter (aj Dj + aj DB j )a , maximal possible depth tree.example, learning 2-ary recursive sort quicksort, user might specify logarithmic depth bound, thus guaranteeing ForceSim polynomial-time. requiresadditional input user, would easy implement. also advantage(not shared approach described below) hypothesized program executed using simple depth-bounded Prolog interpreter, always shallow prooftrees. seems plausible bias impose learning k-ary recursive Prologprograms, many tend shallow proof trees.0526fiPac-Learning Recursive Logic Programs: Efficient Algorithmssecond solution possible high cost forced simulation k-ary recursiveprograms forcibly simulate \smarter" type interpreter|one executek-ary recursive program polynomial time.5 One sound complete theorem-proverclosed k-ary recursive programs implemented follows.Construct top-down proof tree usual fashion, i.e., using depth-first left-to-rightstrategy, maintain list ancestors current subgoal, also list VISITEDrecords, previously visited node tree, subgoal associatednode. Now, suppose course constructing proof tree one generates subgoalf VISITED list. Since traversal tree depth-first left-to-right,node associated f either ancestor current node, descendantleft sibling ancestor current node. former case, proof tree containsloop, cannot produce successful proof; case theorem-prover exitfailure. latter case, proof must already exist f 0 , hence nodescurrent node tree need visited; instead theorem prover simply assumef true.top-down interpreter easily extended forced simulation procedure:one simply traverses tree order, generalizing current hypothesis Hneeded justify inference step tree. additional point noteone performing forced simulation revisits previously proved subgoal f noden, current clause H need generalized order prove f , hencepermissible simply skip portion tree n. thus followingresult.Theorem 7 Let d-Depth-k-Rec set k-ary closed recursive clauses depth d.constants a, d, k language familyd-Depth-k-Rec[DB; a-DetDEC]uniformly identifiable equivalence queries.Proof: Omitted, following informal argument made above.Note give result without restrictions database containsequality relation declaration unique-mode, since tricks used relaxrestrictions Proposition 6 still applicable.5.4 Learning Recursive Base Cases Simultaneouslyfar, analyzed problem learning single clauses: first single nonrecursiveclause, single recursive clause. However, every useful recursive program containsleast two clauses: recursive clause, nonrecursive base case. natural askpossible learn complete recursive program simultaneously learningrecursive clause, associated nonrecursive base case.general, possible, demonstrated elsewhere (Cohen, 1995). However,several cases positive result extended two-clause programs.5. Note plausible believe theorem-prover exists, polynomialnumber possible theorem-proving goals|namely, (aj Dj + aj DB j )a possible recursive subgoals.0527fiCohenbegin algorithm Force2 (d ; Dec; DB ):let Lr1 ; : : :; Lrp possible recursive literals BOTTOM d(Dec)choose unmarked recursive literal Lrilet HR BOTTOM d(Dec) [ fLri glet HB BOTTOM d(Dec)let P = (HR; Hb)repeatAns answer query \Is HR ; HB correct?"Ans =\yes" return HR ; HBelseif Ans negative example e,P FAILUREelseif Ans positive example e+let (f; D) components e+P ForceSim2 (HR; HB ; f ; Dec; (DB [ ); (a j Dj + j DBj )a )0endifP = FAILURErecursive literals Lrj markedreturn \no consistent hypothesis"elsemark Lrichoose unmarked recursive literal Lrjlet HR BOTTOM d(Dec) [ fLrj glet HB BOTTOM (Dec)let P = (HR ; HB )endendifendifendrepeatFigure 6: learning algorithm two-clause recursive programs528fiPac-Learning Recursive Logic Programs: Efficient Algorithmsbegin subroutine ForceSim2 (HR; HB ; f ; Dec; DB ; h ):% \forcibly simulate" program HR ; HB fh < 1 return FAILURE% check see f covered HBelseif BASECASE (f )return current Hr generalized HBreturn (HR; ForceSimNR(HB ; f ; Dec; DB ))elseif head HR f cannot unifiedreturn FAILUREelselet Lr recursive literal HRlet H 0 H , fLrglet head H 0let mgu eliteral L body H 0substitution 0 L0 2 DB0, 0 general substitutionelsedelete L body H 0 , togetherliterals L0 supported (directly indirectly) Lendifendfor% generalize H 0; HB recursive subgoal LrLr ground% continue simulation programreturn ForceSim2(H 0 [ fLr g; HB; Lr; Dec; DB ; h , 1)else return FAILUREendendifendifFigure 7: Forced simulation two-clause recursive programs529fiCohensection, first discuss learning recursive clause base clause simultaneously, assuming determinate base clause possible, also assumingadditional \hint" available, form special \basecase" oracle.discuss various alternative types \hints".Let P target program base clause CB recursive clause CR. basecaseoracle P takes input extended instance (f; D) returns \yes" CB ^ DB ^ ` f ,\no" otherwise. words, oracle determines f covered nonrecursivebase clause alone. example, append program, basecase oracle return\yes" instance append(Xs,Ys,Zs) Xs empty list, \no" otherwise.Given existence basecase oracle, learning algorithm extendedfollows. before, possible recursive literals Lri clause BOTTOM generated;however, case, learner test two clause hypotheses initiallyform (BOTTOM [ Lri ; BOTTOM ). forcibly simulate hypothesis fact f ,following procedure used. checking usual termination conditions, forcedsimulator checks see BASECASE(f) true. so, calls ForceSimNR (with appropriatearguments) generalize current hypothesis base case. BASECASE(f)false, recursive clause Hr forcibly simulated f , subgoal Lr generatedbefore, generalized program recursively forcibly simulated subgoal.Figures 6 7 present learning algorithm Force2 two clause programs consistingone linear recursive clause CR one nonrecursive clause CB , assumptionequivalence basecase oracles available.straightforward extend arguments Theorem 5 case, leadingfollowing result.Theorem 8 Let d-Depth-2-Clause set 2-clause programs consisting oneclause d-DepthLinRec one clause d-DepthNonRec. constantslanguage familyd-Depth-2-Clause[DB; a-DetDEC ]uniformly identifiable equivalence basecase queries.Proof: Omitted.companion paper (Cohen, 1995) shows something like basecase oraclenecessary: particular, without \hints" base clause, learning two-clauselinear recursive program hard learning boolean DNF. However, severalsituations basecase oracle dispensed with.Case 1. basecase oracle replaced polynomial-sized set possible baseclauses. learning algorithm case enumerate pairs base clauses CBi\starting clauses" BOTTOM [ Lrj , generalize starting clause forcedsimulation, mark pair incorrect overgeneralization detected.Case 2. basecase oracle replaced fixed rule determines baseclause applicable. example, consider rule says base clauseapplicable atom p(X1; : : :; Xa) Xi non-null list. Adopting530fiPac-Learning Recursive Logic Programs: Efficient Algorithmsrule leads immediately learning procedure pac-learns exactlytwo-clause linear recursive programs rule correct.Case 3. basecase oracle also replaced polynomial-sized set rulesdetermining base clause applicable. learning algorithm casepick unmarked decision rule run Force2 using rule basecase oracle.Force2 returns \no consistent hypothesis" decision rule marked incorrect,new one choosen. algorithm learn two-clause linear recursiveprograms given decision rules correct.Even though general problem determining basecase decision rule arbitraryDatalog program may dicult, may small number decision proceduresapply large number common Prolog programs. example, recursionlist-manipulation programs halts argument reduced null listsingleton list. Thus Case 3 seems likely cover large fraction automaticlogic programming programs practical interest.also note heuristics proposed finding basecase decision rulesautomatically using typing restrictions (Stahl, Tausend, & Wirth, 1993).5.5 Combining ResultsFinally, note extensions described compatible. meanslet kd-MaxRecLang language two-clause programs consisting oneclause CR k-ary closed recursive depth-d determinate, one clause CBnonrecursive depth-d determinate, following holds.Proposition 9 constants a, k language familykd-MaxRecLang[DB; a-DetDEC ]uniformly identifiable equivalence basecase queries.5.5.1 Extensionsnotation kd-MaxRecLang may seem point unjustified; althoughexpressive language recursive clauses proven learnable,numerous extensions may eciently learnable. example, one might generalizelanguage allow arbitrary number recursive clauses, include clausesdeterminate. generalizations might well pac-learnable|given resultspresented far.However, companion paper (Cohen, 1995) presents series negative results showingnatural generalizations kd-MaxRecLang eciently learnable,kd-MaxRecLang eciently learnable without basecase oracle. Specifically, companion paper shows eliminating basecase oracle leadsproblem hard learning boolean DNF, open problem computationallearning theory. Similarly, learning two linear recursive clauses simultaneously hardlearning DNF, even base case known. Finally, following learning problemshard breaking certain (presumably) secure cryptographic codes: learning n531fiCohenlinear recursive determinate clauses, learning one n-ary recursive determinate clause,learning one linear recursive \k-local" clause. negative results holdmodel identification equivalence queries, also weaker modelspac-learnability pac-predictability.6. Related Workdiscussing related work concentrate previous formal analyses employlearning model similar considered here: namely, models (a) require computation polynomial natural parameters problem, (b) assume either neutralsource adversarial source examples, equivalence queries stochastically presented examples. note, however, much previous formal work exists reliesdifferent assumptions. instance, much work member subsetqueries allowed (Shapiro, 1982; De Raedt & Bruynooghe, 1992), exampleschoosen non-random manner helpful learner (Ling, 1992; De Raedt& Dzeroski, 1994). also work eciency requirementsimposed pac-learnability model relaxed (Nienhuys-Cheng & Polman, 1994).requirement eciency relaxed far enough, general positive results obtained using simple learning algorithms. example, model learnabilitylimit (Gold, 1967), language recursively enumerable decidable (whichincludes Datalog) learned simple enumeration procedure; modelU-learnability (Muggleton & Page, 1994) language polynomially enumerablepolynomially decidable learned enumeration.similar previous work Frazier Page (1993a, 1993b). analyzelearnability equivalence queries recursive programs function symbolswithout background knowledge. positive results provide program classessatisfy following property: given set positive examples + requiresclauses target program prove instances + , polynomial numberrecursive clauses possible; base clause must certain highly constrainedform. Thus concept class \almost" bounded size polynomial. learningalgorithm program class interleave series equivalence queriestest every possible target program. contrast, positive results exponentiallylarge classes recursive clauses. Frazier Page also present series negative resultssuggesting learnable languages analyzed dicult generalize withoutsacrificing ecient learnability.Previous results also exist pac-learnability nonrecursive constant-depth determinate programs, pac-learnability recursive constant-depth determinateprograms model also allows membership subset queries (Dzeroski et al.,1992).basis intelligent search used learning algorithms techniqueforced simulation . method finds least implicant clause C coversextended instance e. Although developed method believedoriginal, subsequently discovered case|an identical techniquepreviously proposed Ling (1991). Since extended instance e converted(via saturation) ground Horn clause, also close connection forced532fiPac-Learning Recursive Logic Programs: Efficient Algorithmssimulation recent work \inverting implication" \recursive anti-unification";instance, Muggleton (1994) describes nondeterministic procedure finding clausesimply clause C , Idestam-Almquist (1993) describes means constrainingimplicant-generating procedure produce least common implicant two clauses.However, techniques obvious applications learning,extremely expensive worst case.CRUSTACEAN system (Aha et al., 1994) uses inverting implication constrainedsettings learn certain restricted classes recursive programs. class programseciently learned system formally well-understood, appearssimilar classes analyzed Frazier Page. Experimental results showsystems perform well inferring recursive programs use function symbols certainrestricted ways. system cannot, however, make use background knowledge.Finally, wish direct reader several pieces research relevant. noted above, companion paper exists presents negative learnability resultsseveral natural generalizations language kd-MaxRecLang (Cohen, 1995). Another related paper investigates learnability non-recursive Prolog programs (Cohen,1993b); paper also contains number negative results strongly motivaterestriction constant-depth determinacy. final prior paper may interestpresents experimental results Prolog implementation variant Force2algorithm (Cohen, 1993a). paper shows forced simulation basislearning program outperforms state-of-the art heuristic methods FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993) learning randomly chosen examples.7. Conclusionsoften desirable guarantees correctness program, manyplausible contexts would highly desirable automatic programming systemoffer formal guarantees correctness. topic paper learnabilityrecursive logic programs using formally well-justified algorithms. specifically,concerned development algorithms provably sound ecientlearning recursive logic programs equivalence queries. showed one constantdepth determinate closed k-ary recursive clause identifiable equivalent queries;implies immediately language also learnable Valiant's (1984) model paclearnability. also showed program consisting one recursive clauseone constant-depth determinate nonrecursive clause identifiable equivalence queriesgiven additional \basecase oracle", determines positive example coverednon-recursive base clause target program alone.obtaining results, introduced several new formal techniques analyzing learnability recursive programs. also shown soundnesseciency several instances generalization forced simulation . method mayapplications practical learning systems. Force2 algorithm compares quite well experimentally modern ILP systems learning problems restricted classidentify (Cohen, 1993a); thus sound learning methods like Force2 might usefulfilter general ILP system like FOIL (Quinlan, 1990; Quinlan & CameronJones, 1993). Alternatively, forced simulation could used heuristic programs.533fiCohenexample, although forced simulation programs many recursive clauses nondeterministic hence potentially inecient, one could introduce heuristics would makeforced simulation ecient, cost completeness.companion paper (Cohen, 1995) shows positive results paperlikely improved: either eliminating basecase oracle languagelearning two recursive clauses simultaneously hard learning DNF, learning nlinear recursive determinate clauses, one n-ary recursive determinate clause, one linearrecursive \k-local" clause hard breaking certain cryptographic codes. positive results paper, negative results establish boundaries learnabilityrecursive programs function-free pac-learnability model. results thusgive prescription building formally justified system learning recursive programs;taken together, also provide upper bounds one hope achieveecient, formally justified system learns recursive programs random examplesalone.Appendix A. Additional ProofsTheorem 1 states: Let Dec = (p; a0; R) declaration 2 a-DetDEC = , let nr = j Rj , letX1; : : :; Xa distinct variables, define clause BOTTOM follows:0BOTTOM (Dec ) CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))0constants a, following true:size BOTTOM d(Dec) polynomial nr ;every depth-d clause satisfies Dec equivalent subclauseBOTTOM (Dec ).Proof: Let us first establish polynomial bound size BOTTOM d. Let Cclause size n. number variables C bounded an, size set LDboundedThus clause Csimilar argumentnr|{z}(|an{z)a,1}(# modes) (# tuples input variables)j DEEPEN Dec (C )j n + (an)a,1nr(1)j CONSTRAIN Dec (C )j n + (an)anr(2)Since functions DEEPEN Dec CONSTRAIN Dec give outputs polynomially larger size inputs, follows composing functions constantnumber times, done computing BOTTOM constant d, also producepolynomial increase size.Next, wish show every depth-d determinate clause C satisfies Decequivalent subclause BOTTOM . Let C depth-d determinate clause,534fiPac-Learning Recursive Logic Programs: Efficient Algorithmswithout loss generality let us assume pair literals Li Lj bodyC mode, predicate symbol, sequence input variables.6Given C , let us define substitution C follows:1. Initially setC fX1 = X1; : : :; Xa = Xa gX1; : : :; Xa arguments head BOTTOM X1 ; : : :; Xaarguments head C .Notice variables head BOTTOM distinct, mappingwell-defined.2. Next, examine literals body C left-to-right order.literal L, let variables T1; : : :Tk input variables. literal Lbody BOTTOM mode predicate symbol whose input variablesT1; : : :; Tk 8i : 1 r; TjC = Tj , modify C follows:0000C [ fU1 = U1 ; : : :; Ul = Ul gU1 ; : : :; Ul output variables L U1; : : :; Ul output variablesL .Notice assume C contains one literal L given predCicate symbol sequence input variables, output variablesliterals L BOTTOM distinct, mapping well-defined. alsoeasy verify (by induction length C ) executing procedurevariable BOTTOM always mapped input variable Ti , leastone L meeting requirements exists. Thus mapping C ontovariables appearing C .7Let head BOTTOM , consider clause C 0 defined follows:head C 0 A.body C 0 contains literals L body BOTTOM either{ LC body C{ L literal equal (Xi; Xj) XiC = XjC .claim C 0 subclause BOTTOM equivalent C . Certainly C 0subclause BOTTOM . One way see equivalent C considerclause C^ substitution ^C generated follows. Initially, let C^ = C 0let ^C = C . Then, every literal L = equal (Xi; Xj) body C^ , delete L^ ij replace ^C (^C )ij , ijC^ , finally replace C^ Csubstitution fXi = Xij ; Xj = Xij g Xij new variable previously appearing6. assumption made without loss generality since determinate clause C , outputvariables Li Lj necessarily bound values, hence Li Lj could unifiedtogether one deleted without changing semantics C .7. Recall function f : X onto range 8y 2 9x 2 X : f (x) = y.535fiCohenC^ . (Note: (^C )ij refer substitution formed replacing every occurrenceXi Xj appearing ^C Xij .) C^ semantically equivalent C 0operation described equivalent simply resolving possible L bodyC 0 clause \equal(X,X) ".following straightforward verify:^C one-to-one mapping.see true, notice every pair assignments Xi = Xj =C must literal equal (Xi; Xj) C 0. Hence following processdescribed assignments Xi = Xj = ^C would eventuallyreplaced Xij = Xij = .^C onto variables C .Notice C onto variables C , every assignment Xi = Cassignment ^C right-hand side (and assignmenteither form Xi = Xij = ). Thus ^C also onto variables C .literal L^ body C^ iff L^^C body C .follows definition C 0 fact every literal LC 0 form equal (Xi; Xj) corresponding literal C^ .Thus C^ alphabetic variant C , hence equivalent C . Since C^ also equivalentC 0, must C 0 equivalent C , proves claim.Acknowledgementsauthor wishes thank three anonymous JAIR reviewers number useful suggestions presentation technical content.ReferencesAha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication smalltraining sets. Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. LectureNotes Computer Science # 784.Angluin, D. (1988). Queries concept learning. Machine Learning, 2 (4).Angluin, D. (1989). Equivalence queries approximate fingerprints. Proceedings1989 Workshop Computational Learning Theory Santa Cruz, California.Bergadano, F., & Gunetti, D. (1993). interactive system learn functional logic programs. Proceedings 13th International Joint Conference Artificial Intelligence Chambery, France.536fiPac-Learning Recursive Logic Programs: Efficient AlgorithmsBiermann, A. (1978). inference regular lisp programs examples. IEEE Transactions Systems, Man Cybernetics, 8 (8).Cohen, W. W. (1993a). pac-learning algorithm restricted class recursive logicprograms. Proceedings Tenth National Conference Artificial IntelligenceWashington, D.C.Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. appear ArtificialIntelligence.Cohen, W. W. (1993c). Rapid prototyping ILP systems using explicit bias. Proceedings1993 IJCAI Workshop Inductive Logic Programming Chambery, France.Cohen, W. W. (1994). Pac-learning nondeterminate clauses. Proceedings EleventhNational Conference Artificial Intelligence Seattle, WA.Cohen, W. W. (1995). Pac-learning recursive logic programs: negative results. JournalAI Research, 2, 541{573.De Raedt, L., & Bruynooghe, M. (1992). Interactive concept-learning constructiveinduction analogy. Machine Learning, 8 (2).De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories PAC-learnable.Wrobel, S. (Ed.), Proceedings Fourth International Workshop InductiveLogic Programming Bad Honnef/Bonn, Germany.De Raedt, L., Lavrac, N., & Dzeroski, S. (1993). Multiple predicate learning. ProceedingsThird International Workshop Inductive Logic Programming Bled, Slovenia.Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability determinate logicprograms. Proceedings 1992 Workshop Computational Learning TheoryPittsburgh, Pennsylvania.Frazier, M., & Page, C. D. (1993a). Learnability inductive logic programming:basic results techniques. Proceedings Tenth National ConferenceArtificial Intelligence Washington, D.C.Frazier, M., & Page, C. D. (1993b). Learnability recursive, non-determinate theories:basic results techniques. Proceedings Third International WorkshopInductive Logic Programming Bled, Slovenia.Gold, M. (1967). Language identification limit. Information Control, 10.Hirsh, H. (1992). Polynomial-time learning version spaces. Proceedings TenthNational Conference Artificial Intelligence San Jose, California. MIT Press.Idestam-Almquist, P. (1993). Generalization implication recursive anti-unification.Proceedings Ninth International Conference Machine Learning Amherst,Massachusetts. Morgan Kaufmann.537fiCohenKing, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug designmachine learning: use inductive logic programming model structureactivity relationships trimethoprim analogues binding dihydrofolate reductase.Proceedings National Academy Science, 89.Lavrac, N., & Dzeroski, S. (1992). Background knowledge declarative bias inductiveconcept learning. Jantke, K. P. (Ed.), Analogical Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. LecturesArtificial Intelligence Series #642.Ling, C. (1991). Inventing necessary theoretical terms scientific discovery inductivelogic programming. Tech. rep. 301, University Western Ontario.Ling, C. (1992). Logic program synthesis good examples. Inductive Logic Programming. Academic Press.Lloyd, J. W. (1987). Foundations Logic Programming: Second Edition. Springer-Verlag.Muggleton, S. (1994). Inverting implication. appear Artificial Intelligence.Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.Journal Logic Programming, 19/20 (7), 629{679.Muggleton, S., & Feng, C. (1992). Ecient induction logic programs. Inductive LogicProgramming. Academic Press.Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structureprediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.Muggleton, S., & Page, C. D. (1994). learnability model universal representations.Wrobel, S. (Ed.), Proceedings Fourth International Workshop InductiveLogic Programming Bad Honnef/Bonn, Germany.Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.Nienhuys-Cheng, S., & Polman, M. (1994). Sample pac-learnability model inference.Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture notesComputer Science # 784.Page, C. D., & Frisch, A. M. (1992). Generalization learnability: study constrainedatoms. Inductive Logic Programming. Academic Press.Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. MachineLearning, 9 (1).Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Brazdil, P. B.(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notesComputer Science # 667.Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5 (3).538fiPac-Learning Recursive Logic Programs: Efficient AlgorithmsQuinlan, J. R. (1991). Determinate literals inductive logic programming. ProceedingsEighth International Workshop Machine Learning Ithaca, New York. MorganKaufmann.Rouveirol, C. (1994). Flattening saturation: two representation changes generalization. Machine Learning, 14 (2).Shapiro, E. (1982). Algorithmic Program Debugging. MIT Press.Srinivasan, A., Muggleton, S. H., King, R. D., & Sternberg, M. J. E. (1994). Mutagenesis:ILP experiments non-determinate biological domain. Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive Logic Programming BadHonnef/Bonn, Germany.Stahl, I., Tausend, B., & Wirth, R. (1993). Two methods improving inductive logicprogramming. Proceedings 1993 European Conference Machine LearningVienna, Austria.Summers, P. D. (1977). methodology LISP program construction examples.Journal Association Computing Machinery, 24 (1), 161{175.Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11).Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers treebanks:machine learning approach. Proceedings Twelfth National ConferenceArtificial Intelligence Seattle, Washington. MIT Press.539fiJournal Artificial Intelligence Research 2 (1995) 575-609Submitted 12/94; published 5/95Provably Bounded-Optimal AgentsStuart J. RussellComputer Science Division, University CaliforniaBerkeley, CA 94720, USADevika SubramanianComputer Science Department, Cornell UniversityIthaca, NY 14853, USArussell@cs.berkeley.edudevika@cs.cornell.eduAbstractSince inception, artificial intelligence relied upon theoretical foundation centred around perfect rationality desired property intelligent systems. argue,others done, foundation inadequate imposes fundamentallyunsatisfiable requirements. result, arisen wide gap theorypractice AI, hindering progress field. propose instead property called boundedoptimality. Roughly speaking, agent bounded-optimal program solutionconstrained optimization problem presented architecture task environment. show construct agents property simple class machinearchitectures broad class real-time environments. illustrate results usingsimple model automated mail sorting facility. also define weaker property,asymptotic bounded optimality (ABO), generalizes notion optimality classicalcomplexity theory. construct universal ABO programs, i.e., programsABO matter real-time constraints applied. Universal ABO programsused building blocks complex systems. conclude discussionprospects bounded optimality theoretical basis AI, relate similar trendsphilosophy, economics, game theory.1. IntroductionSince beginning artificial intelligence, philosophers, control theoristseconomists looked satisfactory definition rational behaviour. neededunderpin theories ethics, inductive learning, reasoning, optimal control, decision-making,economic modelling. Doyle (1983) proposed AI defined computational study rational behaviour|effectively equating rational behaviour intelligence. role definitions AI ensure theory practice correctlyaligned. define property P , hope able design systemprovably possesses property P . Theory meets practice systems exhibit P reality. Furthermore, exhibit P reality something actually careabout. sense, choice P study determines nature field.number possible choices P :Perfect rationality: classical notion rationality economics philosophy.perfectly rational agent acts every instant way maximizeexpected utility, given information acquired environment. Sinceaction selection requires computation, computation takes time, perfectly rationalagents exist non-trivial environments.c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiRussell & SubramanianCalculative rationality: notion rationality studied AI. calculatively rationalagent eventually returns would rational choice beginningdeliberation. exist systems uence diagram evaluators exhibitproperty decision-theoretic definition rational choice, systemsnonlinear planners exhibit logical definition rational choice.assumed interesting property system exhibit since constitutes\in-principle" capacity right thing. Calculative rationality limitedvalue practice, actual behaviour exhibited systems absurdlyfar rational; example, calculatively rational chess program chooseright move, may take 1050 times long so. result, AI systembuilders often ignore theoretical developments, forced rely trial-and-errorengineering achieve goals. Even simple domains chess, littletheory designing analysing high-performance programs.Metalevel rationality: natural response problems calculative rationality.metalevel rational system optimizes object-level computations performed service selecting actions. words, decision findsoptimal combination computation-sequence-plus-action, constraintaction must selected computation. Full metalevel rationalityseldom useful metalevel computations take time, metalevel decision problem often dicult object-level problem. Simpleapproximations metalevel rationality proved useful practice|for example, metalevel policies limit lookahead chess programs|but engineeringexpedients merely serve illustrate lack theoretical basis agent design.Bounded optimality: bounded optimal agent behaves well possible givencomputational resources. Bounded optimality specifies optimal programs ratheroptimal actions optimal computation sequences. former approachavoid placing constraints intelligent agents cannot metprogram. Actions computations are, all, generated programs,programs designers control.make three claims:1. system exhibits bounded optimality desirable reality.2. possible construct provably bounded optimal programs.3. Artificial intelligence usefully characterized study bounded optimality,particularly context complex task environments reasonably powerfulcomputing devices.first claim unlikely controversial. paper supports second claimdetail. third claim may, may not, stand test time.begin section 2 necessarily brief discussion relationshipbounded optimality earlier notions rationality. note particular important distinctions missed without precise definitions terms. Thus section 3provide formal definitions agents, programs, behaviour rationality.576fiProvably bounded-optimal agentsTogether formal descriptions task environments, elements allow us provegiven agent exhibits bounded optimality. Section 4 examines class agent architectures problem generating bounded optimal configurations ecientlysoluble. solution involves class interesting practically relevant optimizationproblems appear addressed scheduling literature. illustrate results showing throughput automated mail-sorting facilitymight improved. Section 5 initiates discussion bounded optimal configurationsmight learned experience environment. section 6, define weaker property, asymptotic bounded optimality (ABO), may robust tractablestrict version bounded optimality. particular, construct universal ABOprograms. program universally ABO ABO regardless specific formtime dependence utility function.1 Universal ABO programs therefore usedbuilding blocks complex systems. conclude assessment prospectsdevelopment approach artificial intelligence.2. Historical Perspectiveclassical idea perfect rationality, developed Aristotle's theories ethics,work Arnauld others choice uncertainty, Mill's utilitarianism, putformal footing decision theory Ramsey (1931) vonNeumann Morgernstern(1947). stipulates rational agent always act maximize expected utility.expectation taken according agent's beliefs; thus, perfect rationalityrequire omniscience.artificial intelligence, logical definition rationality, known philosophy\practical syllogism", put forward McCarthy (1958), reiterated stronglyNewell (1981). definition, agent take action believesguaranteed achieve goals. AI said theoretical foundation, definition rationality provided it. McCarthy believed, probablycorrectly, early stages field important concentrate \epistemological adequacy" \heuristic adequacy" | is, capability principle ratherpractice. methodology resulted involves designing programs exhibitcalculative rationality, using various speedup techniques approximationshope getting close possible perfect rationality. belief, albeit unproven,simple agent designs fulfill specification calculative rationality mayprovide good starting points approach bounded optimality. Moreover, theoretical foundation based calculative rationality cannot provide necessary guidancesearch.clear AI would embarked quest calculative rationalityoperating halcyon days formal intractability results discovered.One response spectre complexity rule bounds. LevesqueBrachman (1987) suggest limiting complexity environment calculativeperfect rationality coincide. Doyle Patil (1991) argue strongly position.1. usage term \universal" derives use scheduling randomized algorithmsLuby, Sinclair Zuckerman (1993).577fiRussell & SubramanianEconomists used perfect rationality abstract model economic entities,purposes economic forecasting designing market mechanisms. makespossible prove theorems properties markets equilibrium. Unfortunately,Simon (1982) pointed out, real economic entities limited time limited powersdeliberation. proposed study bounded rationality, investigating \: : : shapesystem effectiveness computation one important weaponssurvival." Simon's work focussed mainly satisficing designs, deliberatereaching solution satisfying preset \aspiration level." results descriptive value modelling various actual entities policies, general prescriptiveframework bounded rationality developed. Although proved possible calculateoptimal aspiration levels certain problems, structural variation allowedagent design.theory games, bounds complexity players become topicintense interest. example, troubling fact defection equilibriumstrategy unbounded agents playing fixed number rounds Prisoners' Dilemmagame. Neyman's theorem (Neyman, 1985), recently proved Papadimitriou Yannakakis (1994), shows essentially cooperative equilibrium exists agentfinite automaton number states less exponential numberrounds. essentially bounded optimality result, bound space ratherspeed computation. type result made possible shiftproblem selecting actions problem selecting programs.I. J. Good (1971) distinguished perfect \type I" rationality, metalevel\type II" rationality. defines \the maximization expected utility takingaccount deliberation costs." Simon (1976) also says: \The global optimization problemfind least-cost best-return decision, net computational costs." Although type IIrationality seems step right direction, entirely clear whethermade precise way respects desirable intuition computation important.try one interpretation, although may others.2 key issue space\maximization" \optimization" occurs. Good Simon seemreferring space possible deliberations associated particular decision.Conceptually, \object-level machine" executes sequence computationscontrol \meta-level machine." outcome sequence selectionexternal action. agent exhibits type II rationality end deliberationsubsequent action, utility maximized compared possible deliberate/act pairscould engaged. example, Good discusses one possible application typeII rationality chess programs. case, object-level steps node expansionsgame tree, followed backing leaf node evaluations show best move.simplicity assume per-move time limit. type II rational agent executewhichever sequence node expansions chooses best move, finish2. example, conceivable Good Simon really intended refer finding agent designminimizes deliberation costs general. discussions, however, seem couched termsfinding right deliberation decision. Thus, type II metalevel rationality coincidesbounded optimality bounded optimal agent designed single decision singlesituation.578fiProvably bounded-optimal agentstime limit.3 Unfortunately, computations required \metalevel machine"select object-level deliberation may extremely expensive. Good actually proposesfairly simple (and nearly practical) metalevel decision procedure chess, faroptimal. hard see type II rational agent could justify executing suboptimalobject-level computation sequence limit scope optimization problemsingle decision. diculty resolved thinking designagent program, generates unbounded set possible deliberations responseunbounded set circumstances may arise life agent.Philosophy also seen gradual evolution definition rationality.shift consideration act utilitarianism | rationality individual acts |rule utilitarianism, rationality general policies acting. shift causeddiculties individual versus societal rationality, rather considerationdiculty computing rational acts. consideration given recentlytractability general moral policies, view making understandableusable persons average intelligence (Brandt, 1953). Cherniak (1986) suggesteddefinition \minimal rationality", specifying lower bounds reasoning powersrational agent, instead upper bounds. philosophical proposal generally consistentnotion bounded optimality found Dennett's \Moral First Aid Manual"(1986). Dennett explicitly discusses idea reaching equilibrium within spacedecision procedures. uses example PhD admissions procedure philosophydepartment. concludes, we, best procedure may neither elegantilluminating. existence procedure, process reaching it,main points interest.Many researchers AI, whose work discussed below, workedproblem designing agents limited computational resources. 1989 AAAI Symposium AI Limited Rationality (Fehling & Russell, 1989) contains interestingvariety work topic. Much work concerned metalevel rationality.Metareasoning | reasoning reasoning | important technique area,since enables agent control deliberations according costs benefits.Combined idea anytime (Dean & Boddy, 1988) exible algorithms (Horvitz,1987), return better results time goes by, simple form metareasoning allowsagent behave well real-time environment. simple example providediterative-deepening algorithms used game-playing. Breese Fehling (1990) apply similar ideas controlling multiple decision procedures. Russell Wefald (1989) givegeneral method precompiling certain aspects metareasoning system eciently estimate effects individual computations intentions, giving fine-grainedcontrol reasoning. techniques seen approximating metalevel rationality; provide useful insights general problem control reasoning,reason suppose approximations used optimal sense.intuitive notion bounded optimality seems become current AIcommunity mid-1980's. Horvitz (1987) uses term bounded optimality refer\the optimization computational utility given set assumptions expected3. One would imagine cases move selected move selected Typeagent, sense \accidental" deliberation might cause programabandon it.579fiRussell & Subramanianproblems constraints reasoning resources." Russell Wefald (1991) sayagent exhibits bounded optimality given task environment \if program solutionconstrained optimization problem presented architecture." Recent workEtzioni (1989) Russell Zilberstein (1991) seen optimizing welldefined set agent designs, thereby making notion bounded optimality precise.next section, build suitable set general definitions ground up,begin demonstrate examples provably bounded optimal agents.3. Agents, Architectures ProgramsIntuitively, agent physical entity wish view terms perceptionsactions. counts first instance does, necessarily thinks,even whether thinks all. initial refusal consider constraintsinternal workings agent (such reason logically, example) helpsthree ways: first, allows us view \cognitive faculties" planning reasoningoccurring service finding right thing do; second, makes roomamong us (Agre & Chapman, 1987; Brooks, 1986) take position systemsright thing without cognitive faculties; third, allows freedom considervarious specifications, boundaries interconnections subsystems.begin defining agents environments terms actions perceptsexchange, sequence states go through. agent describedagent function percept sequences actions. treatment fairly standard(see, e.g., Genesereth & Nilsson, 1987). go \inside" agent look agentprogram generates actions, define \implementation" relationshipprogram corresponding agent function. consider performance measuresagents, problem designing agents optimize performance measure.3.1 Specifying agents environmentsagent described abstractly mapping (the agent function) perceptsequences actions. Let set percepts agent receive instant,set possible actions agent carry external world. Sinceinterested behaviour agent time, introduce set time pointsinstants, T. set totally ordered < relation unique least element.Without loss generality, let set non-negative integers.percept history agent sequence percepts indexed time. defineset percept histories OT = fOT : ! Og. prefix history OT 2 OTtill time denoted Ot projection OT [0::t]. define setpercept history prefixes Ot = fOt j 2 T; OT 2 OTg. Similarly, define setaction histories = fAT : ! Ag. set action history prefixes At, definedset projections histories 2 .Definition 1 Agent function: mappingf : Ot !580fiProvably bounded-optimal agentsAT(t) = f (Ot)Note agent function entirely abstract entity, unlike agent programimplements it. Note also \output" agent function given percept sequencemay null action, example agent still thinking do. agentfunction specifies agent time step. crucial distinctionperfect rationality calculative rationality.Agents live environments. states environment E drawn set X.set possible state trajectories defined XT = fX : ! Xg. agentnecessarily full access current state X T(t), percept received agentdepend current state perceptual filtering function fp. effectsagent's actions represented environment's transition function fe,specifies next state given current state agent's action. environmenttherefore defined follows:Definition 2 Environment E : set states X distinguished initial state X0,transition function fe perceptual filter function fpX (0) = X0X T(t + 1) = fe(AT (t); X T(t))OT(t) = fp(X T(t))state history X thus determined environment agent function.use notation effects(f; E ) denote state history generated agent functionf operating environment E . also use notation [E; ] denotestate history generated applying action sequence starting initial stateenvironment E .Notice environment discrete deterministic formulation.extend definitions cover non-deterministic continuous environments,cost additional complexity exposition. None results depend significantway discreteness determinism.3.2 Specifying agent implementationsconsider physical agent consisting architecture program.architecture responsible interfacing program environment,running program itself. architecture , associate finite programminglanguage LM , set programs runnable architecture. agentprogram program l 2 LM takes percept input internal state drawnset initial state i0. (The initial internal state depends program l,usually suppress argument.) set possible internal state histories= fI : ! Ig. prefix internal state history 2 till time denotedprojection [0::t].581fiRussell & SubramanianDefinition 3 architecture fixed interpreter agent program runsprogram single time step, updating internal state generating action:: LM !hI T(t + 1); AT(t)i = (l; T(t); OT(t))Thus, architecture generates stream actions according dictates program.physical properties architecture, running program singletime step results execution finite number instructions. program mayoften fail reach \decision" time step, result action producedarchitecture may null (or previous action, depending programdesign).3.3 Relating agent specifications implementationsrelate agent programs corresponding agent functions. sayagent program l running machine implements agent function Agent(l; ).agent function constructed following definition specifying action sequencesproduced l running possible percept sequences. Note importance\Markovian" construction using internal state agent ensure actionsbased past, future.Definition 4 program l running implements agent function f = Agent(l; ),defined follows. environment E = (X; fe; fp), f (Ot) = (t)hI T(t + 1); AT(t)i = (l; T(t); OT(t))OT (t)X T(t + 1)X T(0)T(0)====fp(X T(t))fe(AT(t); X T(t))X0i0Although every program l induces corresponding agent function Agent(l; ),action follows given percept necessarily agent's \response" percept;delay incurred deliberation, may ect percepts occurring muchearlier sequence. Furthermore, possible map every agent functionimplementation l 2 LM . define subset set agent functions fimplementable given architecture language LM :Feasible(M ) = ff j 9l 2 LM ; f = Agent(l; )gFeasibility related to, clearly distinct from, notion computability. Computability refers existence program eventually returns output specifiedfunction, whereas feasibility refers production output appropriatepoint time. set feasible agent functions therefore much smaller setcomputable agent functions.582fiProvably bounded-optimal agents3.4 Performance measures agentsevaluate agent's performance world, define real-valued utility function Ustate histories:U : XT ! <utility function seen external agent environment. definesproblem solved designer agent. agent designs may incorporateexplicit representation utility function, means required.use term task environment denote combination environment utilityfunction.Recall agent's actions drive environment E particular sequencestates accordance function effects(f; E ). define value agentfunction f environment E utility state history generates:V (f; E ) = U (effects(f; E ))designer set E environments probability distribution p them,instead single environment E , value agent E definedexpected value elements E. slight abuse notation,V (f; E) =XE2Ep(E )V (f; E )assign value V (l; M; E ) program l executed architectureenvironment E simply looking effect agent function implementedprogram:V (l; M; E ) = V (Agent(l; ); E ) = U (effects(Agent(l; ); E ))above, extend set possible environments follows:V (l; M; E) =XE2Ep(E )V (l; M; E )3.5 Perfect rationality bounded optimalitydiscussed Section 2, perfectly rational agent selects action maximizesexpected utility, given percepts far. framework, amounts agentfunction maximizes V (f; E) possible agent functions.Definition 5 perfectly rational agent set E environments agent functionfoptfopt = argmaxf (V (f; E))definition persuasive specification optimal agent function givenset environments, underlies several recent projects intelligent agent design (Dean& Wellman,1991; Doyle, 1988; Hansson & Mayer, 1989). direct implementationspecification, ignores delay incurred deliberation, yield reasonable583fiRussell & Subramaniansolution problem { calculation expected utilities takes time real agent.terms simple formal description agents introduced above, easy seediculty arisen. designing agent program, logicists decision theoristsconcentrated specifying optimal agent function fopt order guaranteeselection best action history. function fopt independent architecture .Unfortunately, real program LM implements function non-trivial environment,optimal actions cannot usually computed next percept arrives.is, quite frequently, fopt 62 Feasible(M ).Suppose environment consists games chess tournament rulespopulation human grandmasters, suppose standard personal computer.fopt describes agent always plays way maximize totalexpected points opposition, maximization moves makes.claim possible program play way. quite possible, using depth-firstalpha-beta search termination, execute program chooses (say) optimalminimax move situation, agent function induced programfopt. particular, ignores percepts dropping ag indicatingloss time.trouble perfect rationality definition arose unconstrained optimization space f 's determination fopt , without regard feasibility.(Similarly, metalevel rationality assumes unconstrained optimization space deliberations.) escape quandary, propose machine-dependent standard rationality, maximize V implementable set agent functions Feasible(M ).is, impose optimality constraints programs rather agent functionsdeliberations.Definition 6 bounded-optimal agent architecture set E environmentsagent program loptlopt = argmaxl2LM V (l; M; E)see immediately specification avoids obvious problemsType Type II rationality. Consider chess example,suppose computer262total program memory 8 megabytes. 2 possible programsrepresented machine, much smaller number play legal chess.tournament conditions, one programs best expected performance. suitable candidate lopt. Thus bounded optimality is, definition,feasible specification; moreover, program achieves highly desirable.yet ready announce identity lopt chess eight-megabyte PC,begin restricted problem.4. Provably Bounded-Optimal Agentsorder construct provably bounded optimal agent, must carry followingsteps:Specify properties environment actions taken,utility function behaviours.584fiProvably bounded-optimal agentsSpecify class machines programs run.Propose construction method.Prove construction method succeeds building bounded optimal agents.methodology similar formal analysis used field optimal control,studies design controllers (agents) plants (environments). optimal controltheory, controller viewed essentially instantaneous implementation optimalagent function. contrast, focus computation time required agent,relation computation time dynamics environment.4.1 Episodic, real-time task environmentssection, consider restricted class task environments call episodicenvironments. episodic task environment, state history generated actionsagent considered divided series episodes, terminatedaction. Let A? distinguished set actions terminate episode.utility complete history given sum utilities episode,determined turn state sequence. 2 A?, environment\resets" state chosen random stationary probability distribution Pinit .order include effects choice utility episode, notionallydivide environment state \configuration" part \value" part,configuration part determines state transitions value part determinesutility state sequence. Actions A? reset configuration part, \value"recorded value part. restrictions mean episode treatedseparate decision problem, translate following property: agent program l1higher expected utility individual episodes agent l2, higher expectedutility corresponding episodic task environment.real-time task environment one utility action dependstime executed. Usually, dependence suciently strong makecalculative rationality unacceptably bad approximation perfect rationality.automated mail sorter4 provides illustrative example episodic task environment (see Figure 1). machine scans handwritten printed addresses (zipcodes)mail pieces dispatches appropriate bins. episode starts arrivalnew mail piece terminates execution physical action recommendedsorter: routing piece specific bin. \configuration part" environment corresponds letter feeder side, provides new, randomly selected letterprevious letter sorted. \value part" state corresponds statereceiving bins, determines utility process. aim maximizeaccuracy sorting minimizing reject percentage avoiding jams. jam occurscurrent piece routed appropriate bin, rejected, arrivalnext piece.provide formal definitions three varieties real-time task environments:fixed deadlines, fixed time cost stochastic deadlines.4. See (Sackinger et al. 1992; Boser et al. 1992) details actual system. applicationsuggested us Bernhard Boser early presentation work 1992 NEC Symposium.585fiRussell & Subramaniancamerasacks mailzipcodebucketsrejectFigure 1: automated mail-sorting facility provides simple example episodic,real-time task environment.4.1.1 Fixed deadlinessimplest commonly studied kind real-time task environment containsdeadline known time. work real-time systems, deadlines describedinformally systems built meet deadline. Here, need formal specificationorder connect description deadline properties agents runningdeadline task environments. One might think deadlines part environmentdescription, fact mainly realized constraints utility function. Onesee considering opposite deadline | \starter's pistol." twodistinguished differing constraints utilities acting specifictime.Definition 7 Fixed deadline: task environment hE; U fixed deadline time tdfollowing conditions hold.Taking action A? time deadline results utility:U ([E; At1]) = U ([E; A(2td ,1) AT1 (t)])\" denotes sequence concatenation, td , AT1 (t) 2 A? , A(1t,1) A(2td ,1)contain action A?.Actions taken td effect utility:U ([E; At1]) U ([E; At2]) U ([E; At1d ]) U ([E; At2d ]) td4.1.2 Fixed time costTask environments approximately fixed time cost also common. Examplesinclude consultations lawyers, keeping taxi waiting, dithering investone's money. define task environment fixed time cost c comparingutilities actions taken different times.586fiProvably bounded-optimal agentsDefinition 8 Fixed time cost:task environment hE; U fixed time cost if,action history prefixes A11 A22 satisfying(1) AT1 (t1) 2 A? AT2 (t2) = AT1 (t1)(2) A(1t1 ,1) A(2t2 ,1) contain action A?utilities differ difference time cost:U ([E; At22 ]) = U ([E; At11 ]) , c(t2 , t1)Strictly speaking, task environments fixed time cost. Utility valuesfinite range, one cannot continue incurring time costs indefinitely. reasonably shorttimes reasonably small costs, linear utility penalty useful approximation.4.1.3 Stochastic deadlinesfixed-deadline fixed-cost task environments occur frequently designreal-time systems, uncertainty time-dependence utility functioncommon. also turns interesting, see below.stochastic deadline represented uncertainty concerning time occurrencefixed deadline. words, agent probability distributionPpd deadlinetime td . assume deadline must come eventually, t2T pd (t) = 1.also define cumulative deadline distribution Pd .deadline occur known time, need distinguishtwo cases:agent receives percept, called herald (Dean & Boddy, 1988), announcesimpending deadline. model using distinguished percept Od :OT(td ) = Odagent responds immediately, \meets deadline."percept available, case agent walking blindfolded towardsutility cliff. deliberating further, agent risks missing deadline mayimprove decision quality. example familiar readers decidingwhether publish paper current form, embellish risk\scooped." treat case current paper.Formally, stochastic deadline case similar fixed deadline case, except tddrawn distribution pd . utility executing action history prefix Eexpectation utilities state history prefix possible deadline times.Definition 9 Stochastic deadline: task environment class hE; U fixed-deadline taskenvironments stochastic deadline distributed according pd if, action historyprefix ,XU ([E; ]) = pd (t0)U ([Et ; ])2T00hEt ; U task environment hE; U fixed deadline t0.0587fiRussell & Subramanianmail sorter example well described stochastic deadline. timearrival mail pieces image processing station distributed according densityfunction pd , usually Poisson.4.2 Agent programs agent architectureconsider simple agent programs episodic task environments, constructed elements set R = fr1 ; : : : ; rn g decision procedures rules. decision procedurerecommends (but execute) action Ai 2 A?, agent program fixedsequence decision procedures. purposes, decision procedure black boxtwo parameters:run time ti 0, integer represents time taken procedurecompute action.quality qi 0, real number. gives expected reward resultingexecuting action Ai start episode:qi = U ([E; Ai])(1)Let MJ denote agent architecture executes decision procedures language J .Let tM denote maximum runtime decision procedures accommodated. example, runtime feedforward neural network proportionalsize, tM runtime largest neural network fits .architecture executes agent program = s1 : : : sm running decisionprocedure turn, providing input obtained initial percept.deadline arrives (at fixed time td , heralded percept Od ),entire sequence completed, agent selects action recommendedhighest-quality procedure executed:(s; T(td); OT(td)) = hi0; action(I T(td ))i(s; T(ts); OT(ts)) = hi0; action(I T(ts))i ts = Psi2s ti(s; T(t); Od) = hi0; action(I T(t))i(2)updates agent's internal state history T(t) action(I T(t))action recommended completed decision procedure highest quality.action executed, internal state agent re-initialized i0 . agentdesign works three task environment categories described above.Next derive value V (s; M; E ) agent program environment E runningthree real-time regimes show construct bounded optimal agentstask environments.4.3 Bounded optimality fixed deadlinesEquation 2, know agent picks action A? recommendeddecision procedure r highest quality executed deadline td arrives.588fiProvably bounded-optimal agentsPLet s1 : : : sj longest prefix program ji=1 ti td . Definition 7Equation 1, followsV (s; M; E ) = Qj(3)Qi = maxfq1; : : : ; qi g. Given expression value agent program,easily show following:Theorem 1 Let r = arg maxri 2 R;titd qi . singleton sequence r bounded optimalprogram episodic task environment known deadline td.is, best program single decision procedure maximum quality whose runtimeless deadline.4.4 Bounded optimality fixed time costEquation 2, know agent picks action A? recommended bestdecision procedure sequence, since runs entire sequence = s1 : : : smdeadline. Definition 8 Equation 1,V (s; M; E ) = Qm , cXi=1ti(4)Given expression value agent program, easily show following:Theorem 2 Let r = arg maxri 2 R qi , cti . singleton sequence r bounded optimalprogram episodic task environment fixed time cost c.is, optimal program single decision procedure whose quality, net timecost, highest.4.5 Bounded optimality stochastic deadlinesstochastic deadline distributed according pd, value agent program: : sm expectation. Definition 9, calculateP ps (t=)V s(s;1 :M;Et), hEt; U task environment fixed deadline t. Aft2Tter substituting V (s; M; Et) Equation 3, expression simplifies summation,procedures sequence, probability interruption ith proceduresequence multiplied quality best completed decision procedure:XPiV (s) V (s; M; E) = [Pd(Pij+1(5)=1 tj ) , Pd ( j =1 tj )]Qii=1RtPd (t) = ,1pd(t0)dt0 Pd (t) = 1 Pmi=1 ti.simple example serves illustrate value function. Consider R = fr1 ; r2 ; r3g.rule r1 quality 0.2 needs 2 seconds run: represent r1 = (0:2; 2).rules r2 = (0:5; 5); r3 = (0:7; 7). deadline distribution function pduniform distribution 0 10 seconds. value sequence r1 r2r3V (r1 r2r3 ) = [:7 , :2]:2 + [1 , :7]:5 + [1 , 1]:7 = :25geometric intuition given notion performance profile, shown Figure 2.589fiRussell & Subramanianq0.70.50.2p(t)257Figure 2: Performance profile r1r2 r3, pd superimposed.Definition 10 Performance profile: sequence s, performance profile Qs (t) givesquality action returned agent interrupted t:Qs(t) = maxfqi :Xj =1tj tguniform deadline density function, value sequence proportionalarea performance profile last possible interrupt time. Noteheight profile interval length ti rule running qualitybest previous rules.Definition 10, following obvious property:Lemma 1 performance profile sequence monotonically nondecreasing.also case sequence higher quality decisions times bettersequence:Lemma 2 8t Qs1 (t) Qs2 (t), V (s1) V (s2 ).case say Qs1 dominates Qs2 .use idea performance profiles establish useful properties optimalsequences.Lemma 3 exists optimal sequence sorted increasing order q's.PWithout Lemma 3, ni=1 i! possible sequences consider. ordering constraint eliminates 2n sequences. also means proofs properties sequences, need consider ordered sequences. addition, replace QiEquation 5 qi .following lemma establishes sequence always improved additionbetter rule end:Lemma 4 every sequence = s1 : : : sm sorted increasing order quality, singlestep z qz qsm , V (sz ) V (s).590fiProvably bounded-optimal agentsCorollary 1 exists optimal sequence ending highest-quality rule R.following lemma ects obvious intuition one get better resultless time, there's point spending time get worse result:Lemma 5 exists optimal sequence whose rules nondecreasing order ti .apply preparatory results derive algorithms construct boundedoptimal programs various deadline distributions.4.5.1 General distributionsgeneral deadline distribution, dynamic programming method used obtainoptimal sequence decision rules pseudo-polynomial time. construct optimalsequence using definition V (s; M; E ) Equation 5. Optimal sequences generatedmethods ordered qi, accordance Lemma 3.construct table (i; t), entry table highest valuesequence ends rule ri time t. assume rule indices arrangedPincreasing order quality, ranges start time 0 end time L = ri 2R ti .update rule is:(i; t) = maxk2[0:::i,1][S (k; , ti ) + (qi , qk )[1 , Pd (t)]]boundary condition(i; 0) = 0 rule (0; t) = 0 timeCorollary 1, read best sequence highest value row nmatrix .Theorem 3 DP algorithm computes optimal sequence time O(n2L) nnumber decision procedures R.dependence L time complexity DP algorithm means algorithm polynomial input size. Using standard rounding scaling methods,however, fully polynomial approximation scheme constructed. Althoughhardness proof problem, John Binder (1994) shown deadlinedistribution used constant-time oracle finding values P (t), algorithmrequire exponential number calls oracle worst case.4.5.2 Long uniform distributionsdeadline uniformly distributed time interval greater sumrunning times rules, call distribution long uniform distribution. Considerrule sequence = s1 : : : sm drawn rule set R. long uniform distribution,probability deadline arrives rule si sequence independenttime si starts. permits simpler form Equation 5:V (s; M; E) = Pmi=1,1 Pd (ti+1)qi + qm (1 , Pmi=1 Pd (ti))591(6)fiRussell & Subramanianderive optimal sequence long uniform distribution, obtain recursivespecification value sequence 2 R = s1 : : : sm sequenceR.V (as; M; E) = V (s; M; E) + qaPd (t1) , qmPd (ta)(7)allows us define dynamic programming scheme calculating optimal sequenceusing state function (i; j ) denoting highest value rule sequence startsrule ends rule j . Lemma 3 Equation 7, update rule is:(i; j ) = maxi<kj [S (k; j ) + Pd (tk )qi , Pd (ti)qj ](8)boundary condition(i; i) = (1 , Pd (ti))qi(9)Corollary 1, know optimal sequence long uniform distribution endsrn , rule highest quality R. Thus, need examine (i; n); 1n. entry requires O(n) computation, n entries compute. Thus,optimal sequence long uniform case calculated O(n2 ).Theorem 4 optimal sequence decision procedures long uniform deadline distribution determined O(n2) time n number decision proceduresR.4.5.3 Short uniform distributionsPni=1 Pd (ti) > 1, uniform deadline distribution Pd, call short. meanssequences longer last possible deadline time, therefore rulessequences possibility executing deadline. sequences,cannot use Equation 7 calculate V (s). However, sequence truncatedremoving rules would complete execution last possible deadline.value sequence unaffected truncation, truncated sequences useEquation 7 justified. Furthermore, optimal sequence truncatedsequence.Since update rule 8 correctly computes (i; j ) truncated sequences, useshort uniform distributions provided add check ensure sequencesconsidered truncated. Unlike long uniform case, however, identity last ruleoptimal sequence unknown, need compute n2 entries (i; j ) table.entry computation takes O(n) time, thus time compute optimal sequenceO(n3).Theorem 5 optimal sequence decision procedures short uniform deadline distribution determined O(n3) time n number decision proceduresR.592fiProvably bounded-optimal agents4.5.4 Exponential distributionsexponential distribution, Pd(t) = 1,e,fit . Exponential distributions allow optimalsequence computed polynomial time. Let pi stand probability ruleinterrupted, assuming starts 0. pi = Pd(ti ) = 1 , e,fiti : exponentialdistribution, V (s; M; E) simplifies as:V (s; M; E) =hij =1(1 , pj ) pi+1qi + mj=1(1 , pj ) qmmX,1 hi=1yields simple recursive specification value V (as; M; E) sequencebegins rule a:V (as; M; E) = (1 , pa )p1qa + (1 , pa)V (s; M; E)use state function (i; j ) represents highest value rule sequencestarting ending j .(i; j ) = maxi<kj [(1 , pi )pk qi + (1 , pi)S (k; j )]boundary condition (i; i) = qi(1 , pi). given j , (i; j ) calculatedO(n2). Corollary 1, know optimal sequence whose last elementhighest-valued rule R.Theorem 6 optimal sequence decision procedures exponentially distributedstochastic deadline determined O(n2) time n number decisionprocedures R.proof similar long uniform distribution case.4.6 Simulation results mail-sorterpreceding results provide set algorithms optimizing construction agentprogram variety general task environment classes. section, illustrateresults possible gains realized specific task environment, namely,simulated mail-sorter.First, let us precise utility function U episodes. fourpossible outcomes; utility outcome ui.1. zipcode successfully read letter sent correct bin delivery.2. zipcode misread letter goes wrong bin.3. letter sent reject bin.4. next letter arrives recognizer finished, jam. Sinceletter arrival heralded, jams cannot occur machine architecture givenEquation 2.593fiRussell & Subramanian11mu=90.80.80.60.6P(t)Accuracylambda=0.90.40.40.20.2000246Computation Time (sec)8100246Time (sec)810Figure 3: (a) Accuracy profile (1 , e,x ), = 0:9. (b) Poisson arrival distribution,mean = 9 secWithout loss generality, set u1 = 1:0 u2 = 0:0. probability rulerecommending correct destination bin pi, qi = piu1 + (1 , pi)u2 = pi . assumeu2 u3, hence threshold probability letter sentreject bin instead. therefore include rule set R rule rrejectzero runtime recommends rejection. sequence construction algorithmautomatically exclude rules quality lower qreject = u3. overall utilityepisode chosen linear combination quality sorting (qi ), probabilityrejection rejection rate (given P (t1), t1 runtime first non-rejectrule executed), speed sorting (measured arrival time mean).agent program (Boser et al. 1992) uses single neural network chip.show variety conditions optimized sequence networkssignificantly better single network terms throughput accuracy. examinefollowing experimental conditions:assume network executes time recognition accuracy pdepends t. consider p = 1,e,t . particular choice irrelevantscale chosen arbitrary. choose = 0:9, convenience (Figure 3(a)).include rreject qreject = u3 treject = 0.consider arrival time distributions Poisson varying means. Figure 3(b) shows three example distributions, means 1, 5, 9 seconds.create optimized sequences sets 40 networks execution times takenequal intervals = 1 40.compare(a) BO sequence: bounded optimal sequence;(b) Best singleton: best single rule;(c) 50% rule: rule whose execution time mean distribution (i.e.,complete 50% cases);594fiProvably bounded-optimal agents1BO SequenceBest Singleton50% Rule90% RuleAverage utility per second0.80.60.40.20510152025Mean arrival time303540Figure 4: Graph showing achievable utility per second function average timeper letter, four program types. = 0:9.(d) 90% rule: rule whose execution time guarantees complete 90%cases.last three cases, add rreject initial step; BO sequence includeautomatically.measure utility per second function mean arrival rate (Figure 4).shows optimal setting sorting machinery 6 letters perminute (inter-arrival time = 10 seconds) bounded optimal program, givenfixed 0.9.Finally, investigate effect variance arrival time relativeperformance four program types. purpose, use uniform distributioncentered around 20 seconds different widths vary variance withoutaffecting mean (Figure 5).notice several interesting things results:policy choosing rule 90% probability completion performs poorlyrapid arrival rates ( 3), catches performance best singlerule slower arrival rates ( > 4). artifact exponential accuracyprofile > 0:5, difference quality rules run timesgreater 6 seconds quite small.policy choosing rule 50% probability completion fares wellbest single rule high arrival rates ( 2), rapidly divergesthereafter, performing far worse arrival time means greater 5 seconds.595fiRussell & Subramanian1BO SequenceBest Singleton50% Rule90% RuleAverage utility per second0.80.60.40.20020406080Variance arrival time100120Figure 5: Graphs showing utility gain per second function arrival timevariance, four program types uniform distribution mean20 seconds.best sequence best single rule give best overall performancearrival rate around 6 letters per minute. performance advantageoptimal sequence best single rule 7% arrival rate.noted significant performance advantage obtainableextra computational resources. slower arrival rates ( 7), differenceperformance best rule best sequence arises decreasedrejection rate best sequence. exponential accuracy profile ( 0:5)advantage running rule shorter completion time ahead longer ruleability reduce probability rejecting letter. high arrival rates(inter-arrival times 1 4 seconds), useful short rules insteadlonger single rule.Figure 5 shows best sequence performs better best single rulevariance arrival time increases.5 performance optimal sequence alsoappears largely unaffected variance. exactly behaviour expectobserve | ability run sequence rules instead committing singleone gives robustness face increasing variance. Since realistic environmentsinvolve unexpected demands many kinds, possession variety defaultbehaviours graded sophistication would seem optimal design choicebounded agent.5. performance 50% rule uniform distributions used experimentfixed mean symmetric, 50% rule always rule runs 20 seconds.90% rule changes variance, curve exhibits discretization effects. couldeliminated using finer-grained set rules.596fiProvably bounded-optimal agents5. Learning Approximately Bounded-Optimal Programsderivations assume suitable rule set R available ab initio, correctqualities qi runtimes ti , deadline distribution known. section,study ways information learned, implicationsbounded optimality resulting system. concentrate learning rulesqualities, leaving runtimes deadline distributions future work.basic idea learning algorithms converge, time, setoptimal components | accurate rules accurate quality estimatesthem. happens, value agent constructed rules, using qualityestimates, converges value lopt. Thus two sources suboptimalitylearned agent:rules R may best possible rules | may recommend actionslower utility would recommended rules.may errors estimating expected utility rule. causealgorithms given construct suboptimal sequences, even best rulesavailable.notional method constructing bounded optimal agents (1) learns sets individual decision procedures episodic interactions, (2) arranges sequenceusing one algorithms described earlier performance agent usingsequence least good agent. assume parameterized learning algorithm LJ ;k used learn one rule possible runtimek 2 f1; : : : ; tM g. Since never need include two rules runtimeR, obviates need consider entire rule language J optimizationprocess.setting places somewhat unusual requirements learning algorithm. Likelearning algorithms, LJ ;k works observing collection training episodes E,including utility obtained episode. not, however, make assumptionsform correct decision rule. Instead, make assumptionshypotheses, namely come finite language Jk , set programsJ complexity k. setting called agnostic learning settingKearns, Schapire Sellie (1992), assumptions made environmentall. shown (Theorems 4 5 Kearns, Schapire Sellie, 1992) that,languages J , error learned approximation bounded withinbest rule Jk fits examples, probability 1 , . sample size neededguarantee bounds polynomial complexity parameter k, well 1 1 .addition constructing decision procedures, LJ ;k outputs estimatesquality qi . Standard Chernoff-Hoeffding bounds used limit error qualityestimate within q probability 1 , q . sample size estimation qualityalso polynomial 1q 1q .Thus error agnostically learned rule bounded within best rulecomplexity class probability 1 , . error quality estimationrules bounded q probability 1 , q . bounds, calculate boundutility deficit agent program construct, comparison lopt :597fiRussell & SubramanianTheorem 7 Assume architecture MJ executes sequences decision proceduresagnostically learnable language J whose runtimes range [1::tM ]. real time taskenvironments fixed time cost, fixed deadline, stochastic deadline, constructprogram lV (lopt ; M; E) , V (l; M; E) + 2qprobability greater 1 , m( + q ), number decision procedureslopt .Proof: prove theorem stochastic deadline regime, boundedoptimal program sequence decision procedures. proofs fixed costfixed deadline regimes, bounded optimal program singleton, followspecial case. Let best decision procedures E set R = fr1 ; : : : ; rn g,let lopt = s1 : : : sm optimal sequence constructed R. Let R = fr1 ; : : : rngset decision procedures returned learning algorithm. probability greater1 , m, qi , qi i, qi refers true quality ri . errorestimated quality q^i decision procedure ri also bounded: probability greater1 , mq , jq^i , qi j q i.Let = s1 : : : sm rules R come runtime classesrules s1 : : : sm R . Then, Equation 5,V (lopt ; M; E) , V (s; M; E)error V weighted average errors individual qi . Similarly,jV^ (s; M; E) , V (s; M; E)j qsuppose sequence construction algorithm applied R produces sequencel = s1 : : : sl . definition, sequence appears optimal according estimatedvalue function V^ . HenceV^ (l; M; E) V^ (s; M; E)before, bound error estimated value:jV^ (l; M; E) , V (l; M; E)j qCombining inequalities,V (lopt ; M; E) , V (l; M; E) + 2q002Although theorem practical applications, mainly intended illustrationlearning procedure converge bounded optimal configuration.additional work, general error bounds derived case ruleexecution times ti real-time utility variation (time cost, fixed deadline, deadlinedistribution) estimated training episodes. also obtain error boundscase rule language J divided smaller number coarserruntime classes, rather potentially huge number currently use.598fiProvably bounded-optimal agents6. Asymptotic Bounded Optimalitystrict notion bounded optimality may useful philosophical landmarkexplore artificial intelligence, may strong allow many interesting, generalresults obtained. observation made ordinary complexity theory:although absolute eciency aim, asymptotic eciency game. sortingalgorithm O(n log n) rather O(n2) considered significant, replacing \multiply2" \shift-left 1 bit" considered real advance. slack alloweddefinitions complexity classes essential building earlier results, obtaining robustresults restricted specific implementations, analysing complexityalgorithms use algorithms subroutines. section, begin reviewingclassical complexity. propose definitions asymptotic bounded optimalityadvantages, show classical optimality special caseasymptotic bounded optimality. Lastly, report preliminary investigationsuse asymptotic bounded optimality theoretical tool constructing universalreal-time systems.6.1 Classical complexityproblem, classical sense, defined pair predicates outputz solution input x (x) (x; z ) hold. problem instanceinput satisfying , algorithm problem class always terminates outputz satisfying (x; z ) given input x satisfying (x). Asymptotic complexity describesgrowth rate worst-case runtime algorithm function input size.define formally follows. Let Ta (x) runtime algorithm input x,let Ta (n) maximum runtime input size n. algorithmcomplexity O(f (n))9k; n0 8n n > n0 ) Ta (n) kf (n)Intuitively, classically optimal algorithm one lowest possible complexity.purposes constructing asymptotic notion bounded optimality,useful definition classical optimality mention complexitydirectly. done follows:Definition 11 Classically optimal algorithm: algorithm classically optimal9k; n0 8a0; n n > n0 ) Ta (n) kTa (n)relate classical complexity framework, need define special case taskenvironments traditional programs appropriate. task environments,input provided program initial percept, utility functionenvironment histories obeys following constraint:Definition 12 Classical task environment: hEP ; U classical task environmentproblem P(l outputs correct solution PV (l; M; EP ) = u0 (T (l; M; EP )) ifotherwise0599fiRussell & Subramanian(l; M; EP ) running time l EP , universal Turing machine,u positive decreasing function.notion problem class classical complexity theory thus corresponds classclassical task environments unbounded complexity. example, Traveling Salesperson Problem contains instances arbitrarily large numbers cities.6.2 Varieties asymptotic bounded optimalityfirst thing need complexity measure environments. Let n(E ) suitablemeasure complexity environment. assume existence environmentclasses unbounded complexity. Then, analogy definition classicaloptimality, define worst-case notion asymptotic bounded optimality (ABO).Letting V (l; M; n; E) minimum value V (l; M; E ) E E complexity n,Definition 13 Worst-case asymptotic bounded optimality: agent program l timewise(or spacewise) worst-case asymptotically bounded optimal E iff9k; n0 8l0; n n > n0 ) V (l; kM; n; E) V (l0; M; n; E)kM denotes version machine speeded factor k (or k timesmemory).English, means program basically along right lines needsfaster (larger) machine worst-case behaviour good programenvironments.probability distribution associated environment class E, useexpected value V (l; M; E) define average-case notion ABO:Definition 14 Average-case asymptotic bounded optimality: agent program l timewise(or spacewise) average-case asymptotically bounded optimal E iff9k 8l0 V (l; kM; E) V (l0 ; M; E)worst-case average-case definitions ABO, would happyprogram ABO nontrivial environment nontrivial architecture , unlessk enormous.6 rest paper, use worst-case definition ABO.Almost identical results obtained using average-case definition.first observation made ABO programs classically optimalprograms special case ABO programs:76. classical definitions allow optimality constant factor k runtime algorithms.One might wonder chose use constant factor expand machine capabilities, ratherincrease time available program. context ordinary complexity theory,two alternatives exactly equivalent, context general time-dependent utilities,former appropriate. would possible simply \let l run k times longer," programswish consider control execution time, trading solution quality. One couldimagine slowing entire environment factor k, merely less realistic versionpropose.7. connection suggested Bart Selman.600fiProvably bounded-optimal agentsTheorem 8 program classically optimal given problem Ptimewise worst-case ABO corresponding classical task environment class hEP ; U i.observation follows directly Definitions 11, 12, 13.summary, notion ABO provide degree theoretical robustnessmachine-independence study bounded systems asymptotic complexityclassical programs. set basic framework, begin exercisedefinitions.6.3 Universal asymptotic bounded optimalityAsymptotic bounded optimality defined respect specific value function V .constructing real-time systems, would prefer certain degree independencetemporal variation value function. achieve defining family V valuefunctions, differing temporal variation. mean value functionpreserves preference ordering external actions time, value functionsfamily preference ordering.8example, fixed-cost regime vary time cost c generate familyvalue functions; stochastic deadline case, vary deadline distribution Pdgenerate another family. Also, since three regimes uses quality measureactions, union three corresponding families also family.show single program, call universal program, asymptoticallybounded-optimal regardless value function chosen within particular family.Definition 15 Universal asymptotic bounded optimality (UABO): agent program lUABO environment class E family value functions V iff l ABO Eevery Vi 2 V .UABO program must compete ABO programs every individual value functionfamily. UABO program therefore universal real-time solution given task.UABO programs exist? so, construct them?turns use scheduling construction (Russell & Zilberstein,1991) design UABO programs. construction designed reduce task environments unknown interrupt times case known deadlines, insightapplies here. construction requires architecture provide program concatenation (e.g., LISP prog construct), conditional-return construct, null program. universal program lU form concatenation individual programsincreasing runtime, appropriate termination test each. writtenlU = [l0 l1 lj ]lj consists program termination test. program part ljprogram LM ABO E value function Vj corresponds fixed deadlinetd = 2j , time increment smaller execution time non-nullprogram LM .8. value function must therefore separable (Russell & Wefald, 1989), since preservation rankorder allows separate time cost defined. See chapter 9 (Keeney & Raiffa, 1976) thoroughdiscussion time-dependent utility.601fiRussell & Subramanianql U 4M0.7l opt0.50.2p(t)010Figure 6: Performance profiles lU running 4M , lopt runningproceeding statement lU indeed UABO, let us look example.Consider simple, sequential machine architecture described earlier. Supposeselect rules three-rule set r1 = (0:2; 2), r2 = (0:5; 5) r3 = (0:7; 7). Sinceshortest runtime rules 2 seconds, let = 1. look optimalprograms l0 ; l1; l2; l3 ; : : : fixed-deadline task environments td = 1; 2; 4; 8; : : :.are:l0 = ; l1 = r1; l2 = r1; l3 = r3 ; : : :Hence sequence programs lU [; r1; r1 ; r3; : : :].consider task environment class value function Vi specifies stochasticdeadline uniformly distributed range [0: : : 10]. class, lopt = r1 r2bounded optimal sequence.9 turns lU higher utility lopt providedrun machine four times faster. see plotting two performanceprofiles: QU lU 4M Qopt lopt . QU dominates Qopt, shown Figure 6.establish lU construction yields UABO programs general, needdefine notion worst-case performance profile. Let Q (t; l; M; n; E) minimumvalue obtained interrupting l t, E E complexity n. knowlj lU satisfies following:8l0; n n > nj ) Vj(lj ; kj M; n; E) Vj(l0 ; M; n; E)constants kj , nj . aim prove8Vi 2 V 9k; n0 8l0; n n > n0 ) Vi(lU ; kM; n; E) Vi(l0; M; n; E)Given definition worst-case performance profile, fairly easy show followinglemma (the proof essentially identical proof Theorem 1 Russell Zilberstein,1991):9. Notice that, simple model, output quality rule depends execution timeinput complexity. also means worst-case average-case behaviour same.602fiProvably bounded-optimal agents1BO SequenceABO sequenceAverage utility per second0.80.60.40.20510152025Mean arrival time303540, function meanFigure 7: Throughput accuracy improvement lU loptarrival time, = 0.2, Poisson arrivals.Lemma 6 lU universal program E V , li ABO E Vi 2 V ,Q(t; lU ; kM; n; E) dominates Q(t; li ; M; n; E) k 4 maxj kj , n > maxj nj .lemma establishes that, small constant penalty, ignore specific realtime nature task environment constructing bounded optimal programs. However,still need deal issue termination. possible general lUterminate appropriate time without access information concerning timedependence utility function. example, fixed-time-cost task environment,appropriate termination time depends value time cost c.general case deterministic time-dependence, help lU supplying, Vi , \aspiration level" Qi (ti; li ; M; n; E), ti timeli acts. lU terminates completed lj qj Qi (ti ; li; M; n; E).construction, happen later ti Lemma 6.Theorem 9 task environments deterministic time-dependence, lU suitableaspiration level UABO E .deadline heralds, termination test somewhat simpler requireadditional input lU .Theorem 10 task environment stochastic deadlines, lU UABO Eterminates herald arrives.Returning mail-sorting example, fairly easy see lU (which consistssequence networks, like optimal programs stochastic deadline case)ABO fixed-deadline regime. obvious also ABO particular603fiRussell & Subramanianstochastic deadline case | recall regimes considered single family.programmed constructor function universal programs, appliedmail-sorter environment class. Varying letter arrival distribution gives us different valuefunctions Vi 2 V . Figure 7 shows lU (on 4M ) higher throughput accuracyacross entire range arrival distributions.loptGiven existence UABO programs, possible consider behaviour compositions thereof. simplest form composition functional composition,output one program used input another. complex, nested compositional structures entertained, including loops conditionals (Zilberstein, 1993).main issue constructing UABO compositions allocate time amongcomponents. Provided solve time allocation problem knowtotal runtime allowed, use construction technique used generate composite UABO programs, optimality among possible compositionscomponents. Zilberstein Russell (1993), show allocation problemsolved linear time size composite system, provided composition treebounded degree.7. Conclusions Workexamined three possible formal bases artificial intelligence, concludedbounded optimality provides appropriate goal constructing intelligent systems.also noted similar notions arisen philosophy game theoryless reason: mismatch classically optimal actionscalled feasible behaviours|those generated agent program runningcomputing device finite speed size.showed careful specification task environment computingdevice one design provably bounded-optimal agents. exhibited simpleagents, likely bounded optimality strict sense dicult goalachieve larger space agent programs considered. relaxed notionsasymptotic bounded optimality (ABO) may provide theoretically robust toolsprogress. particular, ABO promises yield useful results composite agentdesigns, allowing us separate problem designing complex ABO agents discretestructural problem continuous temporal optimization problem tractablemany cases. Hence, reason optimistic artificial intelligenceusefully characterized study bounded optimality. may speculate providedcomputing device neither small (so small changes speed size causesignificant changes optimal program design) powerful (so classicallyoptimal decisions computed feasibly), ABO designs stable reasonablywide variations machine speed size environmental complexity. detailsoptimal designs may rather arcane, learning processes play large partdiscovery; expect focus type research questionsconvergence optimality various structural classes end result itself.Perhaps important implication, beyond conceptual foundations fielditself, research bounded optimality applies, design, practice artificialintelligence way idealized, infinite-resource models may not. given,604fiProvably bounded-optimal agentsway illustrating definition, bounded optimal agent: design simple systemconsisting sequences decision procedures provably better programclass. theorem exhibits bounded optimal design translates, definition,agent whose actual behaviour desirable.appear plenty worthwhile directions continue explorationbounded optimality. foundational point view, one interestingquestions concept applies agents incorporate learning component.(Note section 5, learning algorithm external agent.)case, necessarily largely stable bounded optimal configurationagent program large enough; instead, agent adapt shorter-termhorizon rewrite becomes obsolete.results preservation ABO composition, start examinemuch interesting architectures simple production system studied above.example, look optimal search algorithms, algorithm constrainedapply metalevel decision procedure step decide node expand,(Russell & Wefald, 1989). also extend work asymptotic bounded optimalityprovide utility-based analogue \big-O" notation describing performanceagent designs, including suboptimal.context computational learning theory, obvious stationarityrequirement environment, necessary satisfy preconditions PACresults, restrictive. fact agent learns may effectdistribution future episodes, little known learning cases (Aldous &Vazirani, 1990). could also relax deterministic episodic requirement allownon-immediate rewards, thereby making connections current research reinforcementlearning.computation scheduling problem examined interesting itself,appear studied operations research combinatorial optimization literature. Scheduling algorithms usually deal physical rather computational tasks,hence objective function usually involves summation outputs rather pickingbest. would like resolve formal question tractability general case,also look cases solution qualities individual processes interdependent(such one use results another). Practical extensions include computationscheduling parallel machines multiple agents, scheduling combinations computational physical (e.g., job-shop ow-shop) processes, objective functionscombination summation maximization. latter extension broadens scopeapplications considerably. industrial process, designing manufacturingcar, consists computational steps (design, logistics, factory scheduling, inspectionetc.) physical processes (stamping, assembling, painting etc.). One easily imaginemany applications real-time financial, industrial, military contexts.may turn bounded optimality found wanting theoretical framework.case, hope refuted interesting way, better frameworkcreated process.605fiRussell & SubramanianAppendix: Additional Proofsappendix contains formal proofs three subsidiary lemmata main bodypaper.Lemma 3 exists optimal sequence sorted increasing order q's.Proof: Suppose case, optimal sequence. musttwo adjacent rules i, + 1 qi > qi+1 (see Figure 8). Removal rule + 1 yieldssequence s0 Qs (t) Qs (t), Lemma 1 fact ti+2 ti+1 + ti+2 .Lemma 2, s0 must also optimal. repeat removal process s0 orderedqi , proving theorem reductio ad absurdum.20Lemma 4 every sequence = s1 : : : sm sorted increasing order quality, singlestep z qz qsm , V (sz ) V (s).Proof: calculate V (sz ) , V (s) using Equation 5 show non-negative:V (sz ) , V (s) = qz [1 , Pd ((Pmj=1 tj ) + tz )] , qm[1 , Pd ((Pmj=1 tj ) + tz )]P= (qz , qm )[1 , Pd (( mj=1 tj ) + tz )]non-negative since qz qm .2qi+2qi+2qiqi-1qi+1tii+1Figure 8: Proof ordering qi; lower dotted line indicates original profile; upper dottedline indicates profile removal rule + 1.Lemma 5 exists optimal sequence whose rules nondecreasing order ti .Proof: Suppose case, optimal sequence. musttwo adjacent rules i, + 1 qi qi+1 ti > ti+1 (see Figure 9). Removal ruleyields sequence s0 Qs (t) Qs (t), Lemma 1. Lemma 2, s0 must also0optimal. repeat removal process s0 ordered ti, proving theoremreductio ad absurdum.2606fiProvably bounded-optimal agentsqqi+1qii+1qi-1i+1tiFigure 9: Proof ordering ti ; dotted line indicates profile removal rule i.Acknowledgementswould like acknowledge stimulating discussions Michael Fehling, Michael Genesereth, Russ Greiner, Eric Horvitz, Henry Kautz, Daphne Koller, Bart Selmansubject bounded optimality; Dorit Hochbaum, Nimrod Megiddo, Kevin Glazebrook subject dynamic programming scheduling problems; NickLittlestone Michael Kearns subject agnostic learning. would also likethank reviewers many constructive suggestions. Many early ideaswork based arose discussions late Eric Wefald. Thanks alsoRon Parr work uniform-distribution case, Rhonda Righter extendingresults exponential distribution, Patrick Zieske help implementing dynamic programming algorithm. first author supported NSF grants IRI-8903146,IRI-9211512 IRI-9058427, visiting fellowship SERC sabbaticalUK, NEC Research Institute. second author supported NSFgrant IRI-8902721.ReferencesAgre, P., & Chapman, D. (1987). Pengi: implementation theory activity.Proc. 6th National Conference Artificial Intelligence, Seattle, WA. Morghan Kaufmann.Aldous, D., & Vazirani, U. (1990). markovian extension valiant's learning model.Proc. 31st Annual Symposium Foundations Computer Science, St. Louis, MO.IEEE Comput. Soc. Press.Binder, J. (1994). complexity deliberation scheduling stochastic deadlines..Boser, B. E., Sackinger, E., Bromley, J., & LeCun, Y. (1992). Hardware requirementsneural network pattern classifiers | case study implementation. IEEE Micro,12, 32{40.Brandt, R. (1953). search credible form rule utilitarianism. Nakhnikian, G., &Castaneda, H. (Eds.), Morality Language Conduct.607fiRussell & SubramanianBreese, J. S., & Fehling, M. R. (1990). Control problem-solving: Principles architecture. Shachter, R. D., Levitt, T., Kanal, L., & Lemmer, J. (Eds.), UncertaintyArtificial Intelligence 4. North Holland: Amsterdam.Brooks, R. A. (1986). robust, layered control system mobile robot. IEEE JournalRobotics Automation, 2, 14{23.Cherniak, C. (1986). Minimal rationality. MIT Press: Cambridge.Dean, T., & Boddy, M. (1988). analysis time-dependent planning. Proc. AAAI88, pp. 49{54.Dean, T. L., & Wellman, M. P. (1991). Planning control. Morgan Kaufmann: SanMateo, CA.Dennett, D. (1986). moral first aid manual. Tanner lectures human values, UniversityMichigan.Doyle, J. (1983). rational psychology? toward modern mental philosophy. AIMagazine, 4, 50{53.Doyle, J. (1988). Artificial intelligence rational self-government. Tech. rep.. Technicalreport CMU-CS-88-124.Doyle, J., & Patil, R. (1991). Two theses knowledge representation: language restrictions, taxonomic classification, utility representation services. Artificialintelligence, 48, 261{297.Etzioni, O. (1989). Tractable decision-analytic control. Proc. 1st International Conference Knowledge Representation Reasoning, pp. 114{125.Fehling, M., & Russell, S. J. (1989). Proceedings AAAI Spring Symposium LimitedRationality. AAAI.Genesereth, M. R., & Nilsson, N. J. (1987). Logical Foundations Artificial Intelligence.Morgan Kaufmann: Mateo, CA.Good, I. J. (1971). Twenty-seven principles rationality. Godambe, V. P., & Sprott,D. A. (Eds.), Foundations Statistical Inference, pp. 108{141. Holt, Rinehart, Winston.: Toronto.Hansson, O., & Mayer, A. (1989). Heuristic search evidential reasoning. ProceedingsFifth Workshop Uncertainty Artificial Intelligence, Windsor, Ontario.Horvitz, E. J. (1988). Reasoning beliefs actions computational resourceconstraints. Levitt, T., Lemmer, J., & Kanal, L. (Eds.), Uncertainty ArtificialIntelligence 3. North Holland: Amsterdam.Kearns, M., Schapire, R., & Sellie, L. (1992). Toward ecient agnostic learning. Proc. 5thAnn. Workshop Computational Learning Theory, Pittsburgh, PA. Morgan Kaufmann.608fiProvably bounded-optimal agentsKeeney, R., & Raiffa, H. (1976). Decisions multiple objectives: Preferences valuetradeoffs. Wiley: New York.Levesque, H., & Brachman, R. (1987). Expressiveness tractability knowledge representation reasoning. Computational Intelligence, 3, 78{93.Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup las vegas algorithms.Information Processing Letters, 47, 173{80.McCarthy, J. (1958). Programs common sense. Proceedings SymposiumMechanization Thought Processes, Teddington, England: HMSO.Newell, A. (1981). knowledge level. AI Magazine, 2, 1{20.Neyman, A. (1985). Bounded complexity justifies cooperation finitely repeated prisoners' dilemma. Economics Letters, 19, 227{229.Papadimitriou, C., & Yannakakis, M. (1994). complexity bounded rationality.Proc. ACM Symposium Theory Computation.Ramsey, F. P. (1931). Truth probability. Braithwaite, R. (Ed.), foundationsmathematics logical essays. Harcourt Brace Jovanovich: New York.Russell, S. J., & Wefald, E. H. (1989a). optimal game tree search using rational metareasoning. Proc. IJCAI-89.Russell, S. J., & Wefald, E. H. (1989b). Principles metareasoning. Proc. KR-89.Russell, S. J., & Wefald, E. H. (1991). right thing: Studies limited rationality.MIT Press: Cambridge, MA.Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Proc. IJCAI-91,Sydney.Sackinger, E., Boser, B. E., Bromley, J., & LeCun, Y. (1992). Application annaneural network chip high-speed character recognition. IEEE Transactions NeuralNetworks, 3, 498{505.Simon, H. A. (1976). decide do. Models bounded rationality,Volume 2.Simon, H. A. (1982). Models bounded rationality, Volume 2. MIT Press: Cambridge.von Neumann, J., & Morgenstern, O. (1947). Theory games economic behavior.Princeton University Press: Princeton.Zilberstein, S. (1993). Operational Rationality Compilation Anytime Algorithms.Ph.D. thesis, Computer Science Division, University California, Berkeley.Zilberstein, S., & Russell, S. (1993). Optimal composition real-time systems. SubmittedArtificial Intelligence.609fiJournal Artificial Intelligence Research 2 (1995) 287-318Submitted 9/94; published 1/95Truncating Temporal Differences:Ecient Implementation TD()Reinforcement LearningPawe CichoszInstitute Electronics Fundamentals, Warsaw University TechnologyNowowiejska 15/19, 00-665 Warsaw, Polandcichosz@ipe.pw.edu.plAbstractTemporal difference (TD) methods constitute class methods learning predictionsmulti-step prediction problems, parameterized recency factor . Currentlyimportant application methods temporal credit assignment reinforcementlearning. Well known reinforcement learning algorithms, AHC Q-learning, mayviewed instances TD learning. paper examines issues ecientgeneral implementation TD() arbitrary , use reinforcement learningalgorithms optimizing discounted sum rewards. traditional approach, basedeligibility traces , argued suffer ineciency lack generality. TTD(Truncated Temporal Differences ) procedure proposed alternative, indeedapproximates TD(), requires little computation per action usedarbitrary function representation methods. idea derived fairlysimple new, probably unexplored far. Encouraging experimental resultspresented, suggesting using > 0 TTD procedure allows one obtainsignificant learning speedup essentially cost usual TD(0) learning.1. IntroductionReinforcement learning (RL, e.g., Sutton, 1984; Watkins, 1989; Barto, 1992; Sutton, Barto,& Williams, 1991; Lin, 1992, 1993; Cichosz, 1994) machine learning paradigm reliesevaluative training information. step discrete time learning agent observescurrent state environment executes action . receives reinforcement value, also called payoff reward (punishment), state transition takesplace. Reinforcement values provide relative measure quality actions executedagent. state transitions rewards may stochastic, agentknow either transition probabilities expected reinforcement values state-actioncombinations. objective learning identify decision policy (i.e., state-actionmapping) maximizes reinforcement values received agent long term .commonly assumed formal model reinforcement learning task Markovian decisionproblem (MDP, e.g., Ross, 1983). Markov property means state transitionsreinforcement values always depend solely current state current action:dependence previous states, actions, rewards, i.e., state information suppliedagent sucient making optimal decisions.information agent external world task containedseries environment states reinforcement values. never told actionsexecute particular states, actions (if any) would betterc 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiCichoszactually performs. must learn optimal policy observing consequencesactions. abstract formulation generality reinforcement learning paradigmmake widely applicable, especially domains game-playing (Tesauro, 1992),automatic control (Sutton et al., 1991), robotics (Lin, 1993). formulate particulartask reinforcement learning task, one design appropriate state actionrepresentation, reinforcement mechanism specifying goal task. mainlimitation RL applications nature trial-and-error learning method,hardly applicable domains making errors costs much.commonly studied performance measure maximized RL agentexpected total discounted sum reinforcement:E"1Xt=0trt#;(1)rt denotes reinforcement value received step t, 0 1 discountfactor , adjusts relative significance long-term rewards versus short-term ones.maximize sum positive , agent must take account delayedconsequences actions: reinforcement values may received several stepsactions contributed performed. referred learning delayedreinforcement (Sutton, 1984; Watkins, 1989). reinforcement learning performancemeasures also considered (Heger, 1994; Schwartz, 1993; Singh, 1994),work limit exclusively performance measure specified Equation 1.key problem must solved order learn optimal policyconditions delayed reinforcement known temporal credit assignment problem(Sutton, 1984). problem assigning credit blame overall outcomeslearning system (i.e., long-term reinforcement values) individual actions,possibly taken several steps outcomes could observed. Discussing reinforcementlearning algorithms, concentrate temporal credit assignment ignore issuesstructural credit assignment (Sutton, 1984), aspect credit assignment RLsystems.1.1 Temporal Difference Methodstemporal credit assignment problem reinforcement learning typically solved usingalgorithms based methods temporal differences (TD). introducedSutton (1988) class methods learning predictions multi-step predictionproblems. problems prediction correctness revealed once,one step since prediction made, though partial information relevantcorrectness revealed step. information available observedcurrent state prediction problem, corresponding prediction computedvalue function states.Consider multi-step prediction problem step necessary learnprediction final outcome. could example predicting outcome gamechess subsequent board situations, predicting weather Sunday dayweek, forecasting economic indicators. traditional approach learningpredictions would wait outcome occurs, keeping track predictions288fiTruncating Temporal Differencescomputed intermediate steps, then, them, use differenceactual outcome predicted value training error. supervised learning,directed training information obtained comparing outcome predictionsproduced step. predictions modified make closeroutcome.Temporal difference learning makes unnecessary always wait outcome.step difference two successive predictions used training error.prediction modified make closer next one. fact, TD classmethods referred TD(), 0 1 called recency factor . Using > 0allows one incorporate prediction differences time steps, hopefully speedlearning.Temporal credit assignment reinforcement learning may viewed predictionproblem. outcome predict state simply total discounted reinforcementreceived starting state following current policy. predictions used modifying policy optimize performance measure givenEquation 1. Example reinforcement learning algorithms implement idea, calledTD-based algorithms , presented Section 2.2.1.2 Paper OverviewMuch research concerning TD-based reinforcement learning algorithms concentrated simplest TD(0) case. However, experimental results obtained TD( > 0)indicate often allows one obtain significant learning speedup (Sutton, 1988;Lin, 1993; Tesauro, 1992). also suggested (e.g., Peng & Williams, 1994)TD( > 0) perform better non-Markovian environments TD(0) (i.e.,less sensitive potential violations Markov property). thus importantdevelop ecient general implementation techniques would allow TD-based RLalgorithms use arbitrary . motivation work.remainder paper organized follows. Section 2 formal definitionTD methods presented application reinforcement learning discussed. Threeexample RL algorithms brie described: AHC (Sutton, 1984), Q-learning (Watkins,1989; Watkins & Dayan, 1992), advantage updating (Baird, 1993). Section 3 presentstraditional approach TD() implementation, based called eligibility traces,criticized ineciency lack generality. Section 4 analysiseffects TD algorithm leads formulation TTD (Truncated TemporalDifferences ) procedure. two remaining sections devoted experimental resultsconcluding discussion.2. Definition TD()Sutton (1988) introduced TD methods, assumed would use parameter estimation techniques prediction representation. According original formulation,states prediction problem represented vectors real-valued features, corresponding predictions computed use set modifiable parameters (weights).representation learning consists adjusting weights appropriatelybasis observed state sequences outcomes. present alternative formula289fiCichosztion, adopted Dayan (1992), simplifies analysis effects TD()algorithm. formulation states may elements arbitrary finite state space,predictions values function states. Transforming Sutton's original definitionTD() alternative form straightforward.discussing either generic RL-oriented form TD methods, consequently ignore issues function representation. assumed TD predictions functions maintained reinforcement learning algorithms representedmethod allows adjusting function values using error values, controlled learning rate parameter. Whenever write value n-argument function 'arguments p0; p1; : : :; pn,1 updated using error value , mean'(p0; p1; : : :; pn,1) moved towards '(p0; p1; : : :; pn,1) + , degree controlledlearning rate factor . general form abstract update operation writtenupdate ('; p0 ; p1; : : :; pn,1; ):(2)convention, learning algorithm defined rule uses computingerror values.2.1 Basic FormulationLet x0; x1; : : :; xm,1 sequence states multi-step prediction problem.state xt observed time step t, step m, passing whole sequence,real-valued outcome z observed. learning system required produce corresponding sequence predictions P (x0 ); P (x1); : : :; P (xm,1 ), estimatez .Following Dayan (1992), let us define state x:(x (t) =1 xt = x0 otherwise:TD() prediction error state x determined step given by:x(t) = (P (xt+1) , P (xt ))Xt,kk=0x (k);(3)0 1 P (xm ) = z definition, total prediction error state xdetermined whole observed sequence accordingly is:x =mX,1t=0x(t) =mX,1 (t=0(P (xt+1 ) , P (xtX)) t,kk=0)x (k ):(4)Thus, learning step driven difference two temporally successivepredictions. > 0, prediction difference time affects P (xt ), alsopredictions previous time steps, exponentially decaying degree.11. Alternatively, learning prediction step relies prediction differencestep, also future prediction differences. equivalent formulation play significant roleSection 4.290fiTruncating Temporal Differencestwo possibilities using defined errors learning. first compute total errors x states x, accumulating x (t) errors computed timestep t, use passing whole state sequence update predictions P (x).corresponds batch learning mode. second possibility, called incremental on-linelearning, often attractive practice, update predictions step usingcurrent error values x (t). necessary modify appropriately Equation 3,take account predictions changed step:x (t) = (Pt(xt+1 ) , Pt (xt ))Xt,kk=0x (k);(5)Pt (x) designates prediction state x available step t.Sutton (1988) proved convergence batch TD(0) linear representation,states represented linearly independent vectors, assumption state sequences generated absorbing Markov process .2 Dayan (1992) extended proofarbitrary .32.2 TD() Reinforcement Learningfar, paper presented TD general class prediction methods multi-stepprediction problems. important application methods, however, reinforcement learning. matter fact, TD methods formulated Sutton (1988)generalization techniques previously used context temporal creditassignment reinforcement learning (Sutton, 1984).already stated above, straightforward way formulate temporal creditassignment prediction problem predict time step discounted sumfuture reinforcement1Xzt = k rt+k ;k=0called TD return time t. corresponding prediction designated U (xt )called predicted utility state xt . TD returns obviously depend policyfollowed; therefore assume U values represent predicted state utilities respectcurrent policy. perfectly accurate predictions would have:U (xt) = zt = rt + zt+1 = rt + U (xt+1):Thus, inaccurate predictions, mismatch TD error rt + U (xt+1) , U (xt).resulting RL-oriented TD() equations take form:x(t) = (rt + Ut(xt+1 ) , Ut (xt))X( )t,k x (k)(6)k=02. absorbing Markov process defined set terminal states XT , set non-terminal states XN ,set transition probabilities Pxy x 2 XN 2 XN [ XT . absorbing propertymeans cycles among non-terminal states cannot last indefinitely long, i.e., startingnon-terminal state terminal state eventually reached (all sequences eventually terminate).3. Recently stronger theoretical results proved Dayan Sejnowski (1994) Jaakkola, Jordan,Singh (1993).291fiCichoszx =1Xt=0x (t) =1Xt=0((rt + Ut(xt+1) , Ut (xt ))Xk=0)( )t,kx (k) :(7)Note following additional differences equations Equations 3 4:time step subscripts used U values emphasize on-line learning mode,discount applied sum Equation 6 includes well reasonsmay unclear now, made clear Section 4.1,summation Equation 7 extends infinity, predicted final outcomenot, general, available finite number steps.TD-based reinforcement learning algorithms may viewed less direct implementations general rule described Equation 6. see this, considerthree algorithms: well known AHC (Sutton, 1984) Q-learning (Watkins, 1989; Watkins& Dayan, 1992), recent development Baird (1993) called advantage updating .algorithms rely learning certain real-valued functions defined state stateaction space task. superscript used described functionsdesignates optimal values (i.e., corresponding optimal policy). Simplified versionsalgorithms, corresponding TD(0), presented related Equation 6.presentation limited solely function update rules | elaborateddescription algorithms reader consult original publicationsdevelopers or, AHC Q-learning, Lin (1993) Cichosz (1994). closelyrelated dynamic programming methods (Barto, Sutton, & Watkins, 1990; Watkins, 1989;Baird, 1993), relations, though theoretically practically important fruitful, essential subject paper discussed.2.2.1 AHC Algorithmvariation AHC algorithm described adopted Sutton (1990). Twofunctions maintained: evaluation function V policy function f . evaluationfunction evaluates environment state essentially calledU function, i.e., V (x) intended estimate discounted sumfuture reinforcement values received starting state x following current policy.policy function assigns state-action pair (x; a) real number representingrelative merit performing action state x, called action merit . actualpolicy determined action merits using some, usually stochastic, action selectionmechanism, e.g., according Boltzmann distribution (as described Section 5).optimal evaluation state x, V (x), expected total discounted reinforcementreceived starting state x following optimal policy.functions updated step t, executing action state xt,according following rules:updateff(V; xt ; rt + Vt(xt+1 ) , Vt(xt));updatefi (f; xt; at; rt + Vt(xt+1) , Vt (xt)).292fiTruncating Temporal Differencesupdate rule V -function directly corresponds Equation 6 = 0. updaterule policy function increases decreases action merit action dependingwhether long-term consequences appear better worse expected.present this, simplified form AHC corresponding TD(0), paper proposesalternative way using TD( > 0) implemented original AHC algorithmpresented Sutton (1984).2.2.2 Q-Learning AlgorithmQ-learning learns single function states actions, called Q-function .state-action pair (x; a) assigns Q-value action utility Q(x; a), estimatediscounted sum future reinforcement values received starting state x executingaction following greedy policy respect current Q-function (i.e.,performing state actions maximum Q-values). current policy implicitlydefined Q-values. optimal Q-function learned, greedy policyrespect action utilities optimal policy.update rule Q-function is:updateff(Q; xt; at; rt + maxa Qt (xt+1; a) , Qt (xt; at)).show correspondence TD(0) version Equation 6, simply assumepredicted state utilities represented Q-values Qt (xt; at) corresponds Ut (xt)maxa Qt (xt+1 ; a) corresponds Ut (xt+1).2.2.3 Advantage Updating Algorithmadvantage updating two functions maintained: evaluation function Vadvantage function A. evaluation function essentially interpretationcounterpart AHC, though learned different way. advantage function assignsstate-action pair (x; a) real number A(x; a) representing degreeexpected discounted sum future reinforcement increased performing actionstate x, relative action currently considered best state. optimal actionadvantages negative suboptimal actions equal 0 optimal actions,related optimal Q-values by:(x; a) = Q(x; a) , maxQ(x; a0):0Similarly action utilities, action advantages implicitly define policy.evaluation advantage functions updated step applying followingrules:updateff(A; xt; at; maxa At(xt; a) , At(xt; at) + rt + Vt(xt+1) , Vt (xt));updatefi (V; xt; ff1 [maxa At+1 (xt) , maxa At(xt )]).update rule advantage function somewhat complex AHCQ-learning rules, still contains term directly corresponds TD(0) formEquation 6, replacing V U .Actually, presented simplified version advantage updating.original algorithm differs two details:293fiCichosztime step duration explicitly included update rules,presentation assumed = 1,besides learning updates , described above, called normalizing updates performed.3. Eligibility Tracesobvious direct implementation computation described Equation 6tempting. requires maintaining x (t) values statex past time step t.PNote, however, one needs maintain whole sums tk=0 ( )t,kx (k) xone (current) t, much easier due simple trick. Substitutingex (t) =X( )t,k x (k);k=0define following recursive update rule:ex(0) =ex(t) =((1 x0 = x0 otherwise;ex(t , 1) + 1 xt = xex(t , 1)otherwise:(8)quantities ex (t) defined way called activity eligibility traces (Barto,Sutton, & Anderson, 1983; Sutton, 1984; Watkins, 1989). Whenever state visited,activity becomes high gradually decays visited again. updatepredicted utility state x resulting visiting state xt time maywrittenx (t) = (rt + Ut(xt+1) , Ut (xt ))ex(t);(9)direct transformation Equation 6.technique (with minor differences) already used early works Bartoet al. (1983) Sutton (1984), actual formulation TD(). especiallysuitable use parameter estimation function representation methods, connectionist networks. Instead one ex value state x one one eivalue weight wi . eligibility traces actually used Barto et al.(1983) Sutton (1984), inspired earlier work Klopf (1982). Note caseAHC algorithm, different values may used maintaining traces usedevaluation policy functions.Unfortunately, technique eligibility traces general enough easy implement arbitrary function representation method. clear, example,could used important class function approximators memory-based(or instance-based) function approximators (Moore & Atkeson, 1992). Applied puretabular representation, significant drawbacks. First, requires additional memory locations, one per state. Second, even painful, requires modifying U (x)ex x time step. operation dominates computational complexity294fiTruncating Temporal DifferencesTD-based reinforcement learning algorithms, makes using TD( > 0) much expensive TD(0). eligibility traces implementation TD() thus, large statespaces, absolutely impractical serial computers, unless appropriate function approximator used allows updating function values eligibility traces many statesconcurrently (such multi-layer perceptron). even approximatorused, still significant computational (both memory time) additional costsusing TD() > 0 versus TD(0). Another drawback approach revealedSection 4.1.4. Truncating Temporal Differencessection departs alternative formulation TD() reinforcement learning.follow relating TD() training errors used alternative formulationTD() returns. Finally, propose approximating TD() returns truncated TD()returns, show computed used on-line reinforcement learning.4.1 TD Errors TD ReturnsLet us take closer look Equation 7. Consider effects experiencing sequencestates x0 ; x1; : : :; xk ; : : : corresponding reinforcement values r0; r1; : : :; rk ; : : :.sake simplicity, assume states sequence different (thoughcourse impossible finite state spaces). Applying Equation 7 state xtassumption have:xt = rt +h Ut(xt+1) , Ut(xt ) +rt+1 + Ut+1(xt+2) , Ut+1(xt+1) +h( )2 rt+2 + Ut+2(xt+3 ) , Ut+2 (xt+2) + : : :1X=k=0h( )k rt+k + Ut+k (xt+k+1 ) , Ut+k (xt+k ) :state occurs several times sequence, visit state yields similar update.simple observation opens way alternative (though equivalent) formulationTD(), offering novel implementation possibilities.Let0t = rt + Ut(xt+1 ) , Ut(xt)(10)TD (0) error time step t. define TD () error time using TD(0) errorsfollows:=1Xh( )k rt+k + Ut+k (xt+k+1 ) , Ut+k (xt+k ) =k=01X( )k 0t+k :k=0(11)Now, express overall TD() error state x, x , terms errors:x =1Xt=0tx (t):295(12)fiCichoszfact, Equation 7 have:x =1Xt=00tXk=0( )t,k x (k) =1 XX( )t,k0t x (k):(13)t=0 k=0Swapping order two summations get:x =1 X1Xk=0 t=k( )t,k0t x (k):Finally, exchanging k other, receive:x =1X1Xt=0 k=t( )k,t0k x (t) =1 X1Xt=0 k=0( )k 0t+k x (t) =(14)1Xt=0x(t):(15)Note following important difference x (t) (Equation 6) : formercomputed time step x latter computed stepxt. Accordingly, step error value x (t) used adjusting U (x) xused adjusting U (xt). crucial learning procedure proposedSection 4.2. applying defined errors on-line makes changes predictedstate utilities individual steps clearly different described Equation 6,overall effects experiencing whole state sequence (i.e., sums individual errorvalues state) equivalent, shown above.expressed TD() terms errors, gain insight operation role . definitions helpful. Recall TD return timedefined1Xzt = k rt+k :k=0m-step truncated TD return (Watkins, 1989; Barto et al., 1990) received takingaccount first terms sum, i.e.,mX,1[]zt = k rt+k :k=0Note, however, rejected terms rt+m + m+1rt+m+1 + : : : approximatedmUt+m,1 (xt+m ). corrected m-step truncated TD return (Watkins, 1989; Barto et al.,1990) thus:zt(m) =mX,1k=0k rt+k + mUt+m,1 (xt+m ):Equation 11 may rewritten following form:==1Xh( )k rt+k + (1 , )Ut+k (xt+k+1 ) + Ut+k (xt+k+1 ) , Ut+k (xt+k )k=01Xh( )k rt+k + (1 , )Ut+k (xt+k+1 ) , Ut (xt) +k=01Xh( )k Ut+k,1 (xt+k ) , Ut+k (xt+k ) :k=1296(16)fiTruncating Temporal DifferencesNote = 1 yields:1t =1Xkrk=0t+k , Ut (xt) += zt , Ut (xt1 hXk Uk=1t+k,1 (xt+k ) , Ut+k (xt+k )1 hX) + k Uk=1t+k,1 (xt+k ) , Ut+k (xt+k ):relax moment assumption on-line learning mode leave timesubscripts U values, last term disappears simply have:1t = zt , U (xt ):Similarly general , define TD () return (Watkins, 1989) timeweighted average corrected truncated TD returns:11hXX(k+1)kzt = (1 , ) zt = ( )k rt+k + (1 , )Ut+k (xt+k+1 )k=0k=0(17)omit time subscripts, receive:= zt , U (xt):(18)last equation brings light exact nature computation performedTD(). error time step difference TD() return steppredicted utility current state, is, learning error valuebring predicted utility closer return. = 1 quantity zt usual TDreturn time t, i.e., discounted sum future reinforcement values.4 < 1term rt+k replaced rt+k + (1 , )Ut+k (xt+k+1 ), is, actual immediate rewardaugmented predicted future reward.definition TD() return (Equation 17) may written recursivelyzt = rt + (zt+1 + (1 , )Ut(xt+1 )):(19)probably best explains role TD() learning. determines returnused improving predictions obtained. = 1, exactly actual observedreturn, discounted sum rewards. = 0 1-step corrected truncatedreturn, i.e., sum immediate reward discounted predicted utilitysuccessor state. Using 0 < < 1 allows smoothly interpolate two extremes,relying partially actual returns partially predictions.Equation 18 holds true batch learning mode, fact TD methodsoriginally formulated batch learning. incremental version, practically useful,4. observation corresponds equivalence \generic" TD() = 1 supervised learningshown Sutton (1988). receive result necessary discount prediction differencesinstead alone Equation 6, though Sutton presenting RL-oriented form TD makemodification.297fiCichoszintroduces additional term. Let Dt designate term. comparing Equations 1617 get:Dt = , (zt , Ut(xt)) =1Xk=1h( )k Ut+k,1 (xt+k ) , Ut+k (xt+k ) :(20)magnitude discrepancy term, consequently uence learningprocess, obviously depends learning rate value. examine further, supposelearning rate used learning U basis errors. Let correspondinglearning rule be:Ut+1(xt) := Ut(xt) + t:Ut+1 (xt ) , Ut(xt) = (zt , Ut (xt )) + Dt= (z , Ut (xt )) +1Xh( )k Ut+k,1 (xt+k ) , Ut+k (xt+k )k=11X(z , Ut (xt )) , 2 ( )kt+k,1;k=1(21)equality xt+k = xt+k,1 k. similar result may obtainedeligibility traces implementation, learning driven x (t) errors defined Equation 9.would have:Ut+1 (xt) , Ut(xt) = (z , Ut(xt )) , 21Xk=1( )k0t+k,1 ext+k (t + k , 1):(22)effect may considered another drawback eligibility traces implementationTD(), apart ineciency lack generality. Though small learning rateseffect Dt negligible, may still harmful cases, especially large.54.2 TTD Procedureshown TD errors zt , Ut (xt ) used almost equivalently TD()learning, yielding overall results eligibility traces implementation, has,however, important drawbacks practice. Nevertheless, impossible use either TD()errors TD() returns zt on-line learning, since available. stepknowledge rt+k xt+k required k = 1; 2; : : :, wayimplement practice. Recall, however, definition truncated TD return.define truncated TD() error truncated TD() return? appropriatedefinitions are:mX,1;m=( )k0t+k(23)k=05. Sutton (1984) presented technique eligibility traces implementation recencyfrequency heuristics . context, phenomenon examined may considered harmfuleffect frequency heuristic. Sutton discussed example finite-state task heuristic mightmisleading (Sutton, 1984, page 171).298fiTruncating Temporal Differenceszt;m ==mX,2hhh( )k rt+k + (1 , )Ut+k (xt+k+1 ) + ( )m,1 rt+m,1 + Ut+m,1 (xt+m )k=0mX,1( )k rt+k + (1 , )Ut+k (xt+k+1 ) + ( )mUt+m,1 (xt+m ):(24)k=0call ;mm-step truncated TD() error, simply TTD (; m) error timestep t, zt;m m-step truncated TD() return, TTD (; m) return time t.Note zt;m defined Equation 24 corrected , i.e., obtained simply truncating Equation 17. correction term ( )mUt+m,1 (xt+m ) results multiplyinglast prediction Ut+m,1 (xt+m ) alone instead (1 , ), virtually equivalentusing = 0 step. done order include zt;m available infor-mation expected returns time steps (t + m; + + 1; : : :) containedUt+m,1 (xt+m ). Without correction large information would almostcompletely lost.defined, m-step truncated TD() errors returns, used on-line learningkeeping track last visited states, updating step predictedutility least recent state states. idea leads call TTDProcedure (Truncated Temporal Differences ), good approximation TD()suciently large m. procedure parameterized values. m-elementexperience buffer maintained, containing records hxt,k ; at,k ; rt,k ; Ut,k (xt,k+1 )ik = 0; 1; : : :; , 1, current time step. step writing x[k] , a[k] ,r[k], u[k] refer corresponding elements buffer, storing xt,k , at,k , rt,k ,Ut,k (xt,k+1 ).6 References U subscripted time steps, sinceconcern values available current time step | practical implementationdirectly corresponds restoring function value function approximatorlook-up table. notational convention, operation TTD(; m) procedurepresented Figure 1. uses TTD(; m) returns learning. alternative version, usingTTD(; m) errors instead (based Equation 11), also possible straightforwardformulate, reason use \weaker" version (subject harmful effectsdescribed Equations 20 21) \stronger" one available cost.beginning learning, first steps made, learning takeplace. initial steps operation TTD procedure reduces updatingappropriately contents experience buffer. obvious technical detail leftFigure 1 sake simplicity.TTD(; m) return value z computed step 5 repeated applicationEquation 19. computational cost propagating return time acceptablepractice reasonable values m. function representation methods,neural networks, overall time complexity dominated costs retrievingfunction value learning performed steps 4 6, cost computing znegligible. One advantage implementation allows use adaptive values:step 5 one use k depending whether a[k,1] non-policy action,6. naturally means buffer's indices shifted appropriately time tick.299fiCichosztime step t:1. observe current state xt ; x[0] := xt ;2. select action state xt ; a[0] := ;3. perform action ; observe new state xt+1 immediate reinforcement rt;4. r[0] := rt; u[0] := U (xt+1 );5. k = 0; 1; : : :; , 1k = 0 z := r[k] + u[k]else z := r[k] + (z + (1 , )u[k]);6. update (U; x[m,1] ; a[m,1]; z , U (x[m,1]));7. shift indices experience buffer.Figure 1: TTD(; m) procedure.\how much" non-policy was. refinement TD() algorithm suggestedWatkins (1989) recently Sutton Singh (1994). Later see TTD returncomputation performed fully incremental way, using constant time steparbitrary m.Note function update carried step 6 time applies stateaction time , + 1, i.e., , 1 time steps earlier. delay experienceevent learning might found potential weakness presented approach, especiallylarge m. Note, however, baseline computing error value current utilityU (x[m,1] ) = Ut(xt,m+1) used. important point, guaranteeslearning desired effect moving utility (whatever value currently has)towards corresponding TTD return. error used step 6 z , Ut,m (xt,m+1 )instead z , Ut (xt,m+1 ), applying learning time would problematic.Anyway, seems large.TTD procedure exact implementation TD methods two reasons.First, approximates TD() returns TTD(; m) returns. Second, introducesaforementioned delay experience learning. believe, however,possible give strict conditions convergence properties TD() holdtrue TTD implementation.4.2.1 Choicereasonable choice obviously depends . = 0 best possible = 1= 1 = 1 finite value large enough accurately approximateTD(). Fortunately, seem painful. rather unlikelyapplication one wanted use combination = 1 = 1, existing300fiTruncating Temporal Differencesprevious empirical results TD() indicate = 1 usually optimal valueuse, best comparable other, smaller values (Sutton, 1984; Tesauro, 1992;Lin, 1993). Similar conclusions follow discussion choice presentedWatkins (1989) Lin (1993). < 1 < 1 would probably likevalue discount ( )m small number. One possible definition `small'could be, e.g., `much less '. obviously completely informal criterion.Table 1 illustrates practical effects heuristic. hand, large m,delay experience learning introduced TTD procedure might becomesignificant cause problems. experiments described Section 5designed order test different values fixed 0 < < 1.0:99 0:975 0:95 0:9 0:8 0:6minfm j ( )m < 101 g 231 9246 23 12 6Table 1: Choosing m: illustration.4.2.2 Reset Operationnow, assumed learning process, started, continues infinitelylong. true episodic tasks (Sutton, 1984) many real-world tasks,learning must usually stop time. imposes necessity designingspecial mechanism TTD procedure, called reset operation . resetoperation would invoked end episode episodic tasks,overall end learning.much done. problem must dealtexperience buffer contains record last steps learning takenplace yet, steps would make learning remainingsteps possible. implementation reset operation find naturalcoherent TTD procedure simulate additional fictious steps,learning takes place real steps left buffer, TTD returnsremain unaffected simulated fictious steps. corresponding algorithm, presentedFigure 2, formulated replacement original algorithm Figure 1final time step. final step, successor state, fictious successorstate utility assumed 0. corresponds assigning 0 u[0] . actual resetoperation performed step 5.4.2.3 Incremental TTDstated above, cost iteratively computing TTD(; m) return relatively smallreasonable m, function representation methods, restoringupdating function values computationally expensive, may really negligible. alsoargued reasonable values large. hand, iterativereturn computation easy understand ects well idea TTD.301fiCichoszfinal time step t:1. observe current state xt ; x[0] := xt ;2. select action state xt ; a[0] := ;3. perform action ; observe immediate reinforcement rt ;4. r[0] := rt; u[0] := 0;5. k0 = 0; 1; : : :; , 1(a) k = k0; k0 + 1; : : :; , 1k = k0 z := r[k] + u[k]else z := r[k] + (z + (1 , )u[k]);(b) update (U; x[m,1] ; a[m,1]; z , U (x[m,1]));(c) shift indices experience buffer.Figure 2: reset operation TTD(; m) procedure.presented TTD procedure form. possible, however, computeTTD(; m) return fully incremental manner, using constant time arbitrary m.see this, note definition TTD(; m) return (Equation 24) mayrewritten following form:zt;m ==mX,1( )krt+k +mX,2( )k (1 , )Ut+k (xt+k+1 ) + ( )m,1 Ut+m,1 (xt+m )k=0k=0;m;mSt + Tt + Wt;m;St;m =Tt;m =Wt;mmX,1k=0mX,2k=0( )krt+k ;( )k (1 , )Ut+k (xt+k+1 );= ( )m,1 Ut+m,1(xt+m ):Wt;m directly computed constant time m. dicult convinceoneself that:1 ;mSt;m+1 = St , rt + ( ) rt+m ;1 hT ;m , (1 , )U (x ) + (1 , )W ;mi :Tt;m=t+1+1h302(25)(26)fiTruncating Temporal Differencestwo equations define algorithm computing incrementally St;m Tt;m,consequently computing zt;m constant time arbitrary m, small computational expense. algorithm strictly mathematically equivalent algorithmpresented Figure 1.7 Modifying appropriately TTD procedure straightforwarddiscussed. drawback modification probably allowlearner use different (adaptive) values step, i.e., may possiblecombine refinements suggested Watkins (1989) Sutton Singh (1994).Despite this, implementation might beneficial one wanted use really large m.4.2.4 TTD-Based Implementations RL Algorithmsimplement particular TD-based reinforcement learning algorithms basisTTD procedure, one substitute appropriate function values U , defineupdating operation step 6 Figure 1 step 5b Figure 2. Specifically,three algorithms outlined Section 2.2 one should:AHC:1. replace U (xt+1) V (xt+1 ) step 4 (Figure 1);2. implement step 6 (Figure 1) step 5b (Figure 2) as:v := V (x[m,1] );updateff(V; x[m,1] ; z , v );updatefi (f; x[m,1] ; a[m,1]; z , v );Q-learning:1. replace U (xt+1) maxa Q(xt+1; a) step 4 (Figure 1);2. implement step 6 (Figure 1) step 5b (Figure 2) as:updateff(Q; x[m,1] ; a[m,1]; z , Q(x[m,1] ; a[m,1]));advantage updating:1. replace U (xt+1) V (xt+1 ) step 4 (Figure 1);2. implement step 6 (Figure 1) step 5b (Figure 2) as:Amax := maxa A(x[m,1] ; a);updateff(A; x[m,1]; a[m,1]; Amax , A(x[m,1] ; at) + z , V (x[m,1]));updatefi (V; x[m,1]; ff1 [maxa A(x[m,1]) , Amax]).4.3 Related Worksimple idea truncating temporal differences implemented TTD procedure new. probably first suggested Watkins (1989). paper owes muchwork. But, best knowledge, idea never explicitly7. necessarily numerically equivalent, may sometimes cause problems practicalimplementations.303fiCichoszexactly specified, implemented, tested. sense TTD procedure originaldevelopment.Lin (1993) used similar implementation TD(), calledexperience replay , actual on-line reinforcement learning. approach sequence past experiences replayed occasionally, replay experienceTD() return (truncated length replayed sequence) computed applying Equation 19, corresponding function update performed. learningmethod means computationally expensive TTD procedure (especially implemented fully incremental manner, suggested above), since requiresupdating predictions sequentially replayed experiences, besides \regular" TD(0) updates performed step (while TTD always requires one update per time step),allow learner take full advantage TD( > 0), appliedoccasionally.Peng Williams (1994) presented alternative way combining Q-learningTD(), different discussed Section 2.2. motivation better estimate TDreturns use TD errors. Toward end, used standard Q-learning errorrt + maxQt (xt+1 ; a) , Qt (xt; )one-step updates modified errorrt + maxQt (xt+1 ; a) , maxQt (xt ; a);propagated using eligibility traces, thereafter. TTD procedure achieves similar objective straightforward way, use truncated TD() returns.related work Pendrith (1994). applied idea eligibility tracesnon-standard way estimate TD returns. approach computationally ecientclassical eligibility traces technique (it requires one prediction update per timestep) free potentially harmful effect described Equation 22. methodseems roughly equivalent TTD procedure = 1 large m, thoughprobably much implementationally complex.5. Demonstrationsdemonstrations presented section use AHC variant TTD procedure.reason AHC algorithm simplest three described algorithmsupdate rule evaluation function directly corresponds TD(). Future workinvestigate TTD procedure two algorithms.tabular representation evaluation policy functions used. abstractfunction update operation described Equation 2 implemented standard way'(p0; p1; : : :; pn,1 ) := '(p0; p1; : : :; pn,1 ) + :(27)Actions execute step selected using simple stochastic selection mechanism based Boltzmann distribution. According mechanism, action selected304fiTruncating Temporal Differencesstate x probabilityProb(x; a) = Pexp(f (x; )=T ) ;exp(f (x; a)=T )(28)temperature > 0 adjusts amount randomness.5.1 Car Parking Problemsection presents experimental results learning control problem relativelylarge state space hard temporal credit assignment. call problem car parkingproblem, though attempt simulate real-world problem all. Using words`car', `garage', `parking' convention simplifies problem descriptioninterpretation results. primary purpose experiments neithersolve problem provide evidence usefulness tested algorithmparticular practical problem. use example problem order illustrateperformance AHC algorithm implemented within TTD frameworkempirically evaluate effects different values TTD parameters m.car parking problem illustrated Figure 3. car, represented rectangle,initially located somewhere inside bounded area, called driving area. garagerectangular area size somewhat larger car. important dimensionsdistances shown figure. agent | driver car | required parkgarage, car entirely inside. task episodic, though neithertime-until-success time-until-failure task (in Sutton's (1984) terminology), rathercombination both. episode finishes either car enters garagehits wall (of garage driving area). episode car resetinitial position.5.1.1 State Representationstate representation consists three variables: rectangular coordinates centercar, x , angle car's axis x axis coordinatesystem. orientation system shown figure. initial locationorientation car fixed described x = 6:15 m, = 10:47 m, = 3:7 rad.chosen make task neither easy dicult.5.1.2 Action Representationadmissible actions `drive straight on', `turn left', `turn right'. actiondriving straight effect moving car forward along axis, i.e., withoutchanging . actions turning left right equivalent moving along arcfixed radius. distance move determined constant car velocity vsimulation time step . Exact motion equations details given Appendix A.5.1.3 Reinforcement Mechanismdesign reinforcement function fairly straightforward. agent receivesreinforcement value 1 (a reward) whenever successfully parks car garage,305fiCichoszy0xxGx10x0yGlwy10123Figure 3: car parking problem. scale dimensions preserved: w = 2 m,l = 4 m, x0 = ,1:5 m, xG = 1:5 m, x1 = 8:5 m, y0 = ,3 m, yG = 3 m, y1 = 13 m.reinforcement value ,1 (a punishment) whenever hits wall. timesteps reinforcement 0. is, non-zero reinforcements received laststep episode. involves relatively hard temporal credit assignment problem,providing good experimental framework testing eciency TTD procedure.problem hard reinforcement delay, also punishmentsmuch frequent rewards: much easier hit wall park carcorrectly.reinforcement mechanism presented above, optimal policy0 < < 1 policy allows park car garage smallest possiblenumber steps.306fiTruncating Temporal Differences5.1.4 Function Representationcar parking problem continuous state space. artificially discretized | dividedfinite number disjoint regions quantizing three state variables,function value region stored look-up table. quantization thresholds are:x: ,0:5, 0:0, 0:5, 1:0, 2:0, 3:0, 4:0, 6:0 m,y: 0:5, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 8:0, 10:0 m,2129 3 31: 1920 , , 20 , : : :, 20 , 2 , 20 rad.yields 9 10 14 = 1260 regions. course many never visited.threshold values chosen make resulting discrete state space moderatesize. quantization dense near garage, becomes sparse distancegarage increases.5.1.5 Experimental Design Resultsexperiments applying TTD procedure car parking problem dividedtwo studies, testing effects two TTD parameters m. parametersettings experiments presented Table 2. symbols ff fi useddesignate learning rates evaluation policy functions, respectively. initialvalues functions set 0, since assumed knowledge availableexpected reinforcement levels.Study TTD ParametersNumber00:30:510:7250:80:9151020:91520Learning Ratesff0:70:50:50:50:50:250:250:250:250:250:25fi0:70:50:50:50:50:250:250:250:250:250:25Table 2: Parameter settings experiments car parking problem.stated above, experiments designed test effects two TTDparameters. parameters assigned values according following principles:discount factor fixed equal 0:95 experiments,307fiCichosztemperature value also fixed set 0:02, seemed equally goodexperiments,learning rates ff fi roughly optimized experiment.8experiment continued 250 episodes, number selected allowalmost runs experiments converge. results presented experimentsaveraged 25 individual runs, differing initial seed randomnumber generator. number chosen reasonable compromise reliability results computational costs. results presented plots averagereinforcement value per time step previous 5 consecutive episodes versus episodenumber.Study 1: Effects . objective study examine effects variousvalues learning speed quality, set 25. value = 25 foundlarge enough tested values (perhaps except = 1).9 Smaller values mightused small (in particular, = 1 = 0), kept constant consistency.Reinf/Step0.040.020-0.02-0.04-0.06-0.08050= 0:0= 0:3= 0:5= 0:7100 150Episode200Reinf/Step0.040.020-0.02-0.04-0.06-0.08250050= 0:7= 0:8= 0:9= 1:0100 150Episode200250Figure 4: car parking problem, learning curves study 1.learning curves study presented Figure 4. observationsbrie summarized follows:= 0 gives worst performance (not 25 runs managed convergewithin 250 episodes),increasing improves learning speed,values equal 0:7 similarly effective, greatly outperforming = 0clearly better = 0:5,8. optimization procedure cases follows: rather large value testedruns; give effects overtraining premature convergence, accepted; otherwise(usually twice) smaller value tried, etc.9. Note = 0:9, = 25, = 0:95 ( )m 0:02 0:855 = .308fiTruncating Temporal Differencesusing large caused necessity reducing learning rates (cf. Table 2) ensureconvergence.main result using large TTD procedure (including 1) alwayssignificantly improved performance. quite consistent empirical resultsSutton (1988), found performance TD() best intermediate ,worst = 1. Lin (1993), used > 0 experience replay experiments, reportedclose 1 successful, similarly work. speculated differenceresults Sutton's might caused switching occasionally (fornon-policy actions) = 0 studies.10 results, obtained held fixedtime11 , suggest good explanation. seems likely optimalvalue simply strongly depends particular problem. Another point neitherTTD(1; 25) Lin's implementation exactly equivalent TD(1).Study 2: Effects m. study designed investigate effects using severaldifferent values fixed relatively large value. best (approximately)study 1 used, 0:9. smallest tested value 5, find rathersmall value.12Reinf/Step0.040.020-0.02-0.04-0.06-0.08050m= 5= 10= 15= 20= 25100 150Episode200250Figure 5: car parking problem, learning curves study 2.learning curves study presented Figure 5. results = 25taken study 1 comparison. observations summarized follows:= 5 worst = 25 best,differences intermediate values seem statisticallysignificant,10. matter fact, non-policy actions replayed Lin's experience replay experiments.11. Except using = 0 recent time step covered TTD return, followsdefinition (Equation 24).12. = 0:95, = 0:9, = 5 ( )m 0:457, means comparable= 0:855.309fiCichoszeven smallest = 5 gives performance level much better obtainedstudy 1 small , i.e., even relatively small values allow us advantageslarge , though larger values generally better small ones,last observation probably important. also optimistic. suggeststhat, least problems, TTD procedure > 0 allows obtain significantlearning speed improvement traditional TD(0)-based algorithms practicallyadditional costs, small space time complexity induced TTDalways negligible.5.2 Cart-Pole Balancing Problemexperiments section one basic purpose: verify effectivenessTTD procedure applying AHC implementation realistic complex problem,long reinforcement delay, exist many previous results comparison.cart-pole balancing problem, classical benchmark control specialists,problem. particular, would like see whether possible obtain performance(learning speed quality final policy) worse reported Bartoet al. (1983) Sutton (1984) using eligibility traces implementation.Figure 6 shows cart-pole system. cart allowed move along one-dimensionalbounded track. pole move vertical plane cart track.controller applies either left right force fixed magnitude cart timestep. task episodic: episode finishes failure occurs, i.e., pole fallscart hits edge track. objective delay failure long possible.problem realistically simulated numerically solving system differentialequations, describing cart-pole system. equations simulation detailsgiven Appendix B. parameters simulated cart-pole system exactlyused Barto et al. (1983).5.2.1 State Representationstate cart-pole system described four state variables:x | position cart track,x_ | velocity cart,| angle pole vertical,_ | angular velocity pole.5.2.2 Action Representationstep agent controlling cart-pole system chooses one two possibleactions applying left right force cart. force magnitude fixedequal 10 N.310fiTruncating Temporal Differences2lFxFigure 6: cart-pole system. F force applied cart's center, l halfpole length, half length track.5.2.3 Reinforcement Mechanismagent receives non-zero reinforcement values (namely ,1) endepisode, i.e., failure. failure occurs whenever jj > 0:21 rad (the pole beginsfall) jxj > 2:4 (the cart hits edge track). Even beginning learning,poor policy, episode may continue hundreds time steps, maymany steps bad action resulting failure. makes temporalcredit assignment problem cart-pole task extremely hard.5.2.4 Function Representationcase car parking problem, deal continuous state spacecart-pole system dividing disjoint regions, called boxes Mitchie Chambers(1968). quantization thresholds used Barto et al. (1983), i.e.:x: ,0:8, 0:8 m,x_ : ,0:5, 0:5 m/s,: ,0:105, ,0:0175, 0, 0:0175, 0:105 rad,_: ,0:8727, 0:8727 rad/s,yields 3 3 6 3 = 162 boxes. box memory location, storingfunction value box.311fiCichosz5.2.5 Experimental Design ResultsComputational expense prevented extensive experimental studies car parkingproblem. one experiment carried out, intended replication experiment presented Barto et al. (1983). values TTD parameters seemedbest previous experiments used, = 0:9 = 25. discountfactor set 0:95. learning rates evaluation policy functionsroughly optimized small number preliminary runs equal ff = 0:1 fi = 0:05,respectively. temperature Boltzmann distribution action selection mechanismset 0:0001, give nearly-deterministic action selection. initial valuesevaluation policy functions set 0. attempt strictly replicatelearning parameter values work Barto et al. (1983), since useddifferent TD() implementation13 , also different policy representation (basedfact two actions, representation general), actionselection mechanism (for reasons), function learning rule.experiment consisted 10 runs, differing initial seed randomnumber generator, presented results averaged 10 runs. run continued 100 episodes. individual runs terminated 500; 000 time steps,completing 100 episodes. produce reliable averages 100 episodes, fictiousremaining episodes added runs, duration assigned accordingfollowing principle, used experiments Barto et al. (1983). durationlast, interrupted episode less duration immediately preceding (complete) episode, fictious episodes assigned duration preceding episode.Otherwise, fictious episodes assigned duration last (incomplete) episode.prevented short interrupted episodes producing unreliably low averages.results presented Figure 7 plots average duration (the number time steps)previous 5 consecutive episodes versus episode number, linear logarithmicscale.observe TTD-based AHC achieved similar (slightly better, exact)performance level, learning speed quality final policy (i.e.,balancing periods), reported Barto et al. (1983). final balancing periods lasted130; 000 steps, average. obtained without using 162 additional memorylocations storing eligibility traces, without expensive computation necessaryupdate time step, well evaluation policy function values.5.3 Computational Savingsexperiments presented illustrate computational savings possibleTTD procedure conventional eligibility traces. direct implementation eligibilitytraces requires computation proportional number states, i.e., 1260 carparking task 162 cart-pole task | potentially many larger tasks.Even straightforward iterative version TTD may beneficial, requirescomputation proportional m, may reasonably assumed many times less13. eligibility traces implementation, eligibility traces updated applying somewhatdifferent update rule specified Equation 8. particular, discounted aloneinstead . Moreover, two different values used evaluation policy functions.312fiTruncating Temporal DifferencesEpisode Duration140000120000100000800006000040000200000020(a)40 60Episode80Episode Duration100000100001000100101100020(b)40 60Episode80100Figure 7: cart-pole balancing problem, learning curve (a) linear (b) logarithmicscale.size state space. course, incremental version TTD, requiresalways small computation independent m, much ecient.many practical implementations, improve eciency, eligibility traces predictions updated relatively recently visited states. Traces maintainedn recently visited states, eligibility traces states assumed0.14 even \ecient" version eligibility traces, savings offeredTTD considerable. good approximation infinite traces tasks considered here, n least large m. conventional eligibility traces,always concern keeping n low, reducing , , accuracy approximation.problem occurs iterative TTD,15 incremental TTD, hand,none issue. small computation needed independent m.6. Conclusioninformally derived TTD procedure analysis updates introducedTD methods predicted utilities states, shown approximated use truncated TD() returns. Truncating temporal differences allows easyecient implementation. possible compute TTD returns incrementally constant time, irrespective value (the truncation period), computationalexpense using TD-based reinforcement learning algorithms > 0 negligible (cf.Equations 25 26). cannot achieved eligibility traces implementation.latter, even function representation methods particularly well14. modification cannot applied parameter estimation function representation techniqueused (e.g., multi-layer perceptron), traces maintained weights rather states.15. relative computational expense iterative TTD \ecient" version eligibility tracesdepends cost function update operation, always performed one stateformer, n states latter.313fiCichoszsuited (e.g., neural networks), always associated significant memory time costs.TTD procedure probably computationally ecient (although approximate)on-line implementation TD(). also general, equally good function representation method might used.important question concerning TTD procedure whether computationaleciency obtained cost reduced learning eciency. low computational costs per control action may attractive number actions necessaryconverge becomes large. now, theoretically grounded answer importantquestion provided, though unlikely answer eventuallyfound. Nevertheless, informal consideration may suggest TTD-based implementation TD methods perform worse classicaleligibility traces implementation, even advantages. followsEquations 20, 21, 22, using TD(0) errors on-line TD() learning, eligibilitytraces implementation, introduces additional discrepancy term, whose uencelearning process proportional square learning rate. term, though oftennegligible, may still harmful certain cases, especially tasks agent likelystay states long periods. TTD procedure, based truncated TD()returns, free drawback.Another argument supporting TTD procedure associated using large values,particular 1. exact TD() implementation, provided eligibilitytraces, means learning relies solely actually observed outcomes, without regardcurrently available predictions. may beneficial early stages learning,predictions almost completely inaccurate, general rather risky | actualoutcomes may noisy therefore sometimes misleading. TTD procedure neverrelies entirely, even = 1, since uses m-step TTD returns finite m,corrected always using = 0 discounting predicted utility recent stepcovered return (cf. Equation 17). deviation TTD procedure TD()may turn advantageous.TTD procedure using TTD returns learning suitable implementation TD methods applied reinforcement learning. RL partpredicted outcome available step, current reinforcement value. However,straightforward formulate another version TTD procedure, using truncatedTD() errors instead truncated TD() returns, would cover whole scopeapplications generic TD methods.experimental results obtained TTD procedure seem promising. results presented Section 5.1 show using large TTD procedure give significant performance improvement simple TD(0) learning, even relatively small m.say anything relative performance TTD eligibilitytraces implementation TD(), least suggests TTD procedure useful.best results obtained largest values, including 1. observation,contradicting results reported Sutton (1988), may positive consequenceTTD procedure's deviation TD() discussed above.experiments cart-pole balancing problem supplied empirical evidencelearning control problem long reinforcement delay TTD procedureequal outperform eligibility traces implementation TD(), even value314fiTruncating Temporal Differencesmany times less average duration episode. performance level obtainedTTD procedure much lower computational (both memory time) expense.summarize, informal consideration empirical results suggest TTDprocedure may following advantages:possibility implementation reinforcement learning algorithms mayviewed instantiations TD(), using > 0 faster learning,computational eciency: low memory requirements (for reasonable m) little computation per time step,generality, compatibility various function representation methods,good approximation TD() < 1 (or = 1 < 1),good practical performance, even relatively small m.seems one important drawback: lack theoretical analysis convergence proof. know either parameter values assure convergencevalues make impossible. particular, estimate available potential harmfuleffects using large m. advantages drawbacks cause TTD procedure interesting promising subject work. work concentrate,one hand, examining theoretical properties technique, and,hand, empirical studies investigating performance various TD-based reinforcementlearning algorithms implemented within TTD framework variety problems,particular stochastic domains.Appendix A. Car Parking Problem Detailsmotion car experiments Section 5.1 simulated applyingtime step following equations:1. r 6= 0(a) (t + ) = (t) + vr ;(b) x(t + ) = x(t) , r sin (t) + r sin (t + );(c) (t + ) = (t) + r cos (t) , r sin (t + );2. r = 0(a) (t + ) = (t);(b) x(t + ) = x(t) + v cos (t);(c) (t + ) = (t) + v sin (t);r turn radius, v car's velocity, simulation time step.experiments r = ,5 used `turn left' action, r = 5 `turn right', r = 0`drive straight on'. velocity constant set 1 m/s, simulation time315fiCichoszstep = 0:5 used. parameter settings, shortest possible pathcar's initial location (x = 6:15 m, = 10:47 m, = 3:7 rad) garage requires 21 steps.step, determining current x, , values, coordinatescar's corners computed. test intersection side carlines delimiting driving area garage performed determine whether failureoccurred. result negative, test performed corner car whetherinside garage, determine success occurred.Appendix B. Cart-Pole Balancing Problem Detailsdynamics cart-pole system described following equations motion:hF (t) + mpl _2(t) sin (t) , cos (t) , c sgn x_ (t)x(t) =mc + mp2(t)+c sgn x_ (t)g sin (t) + cos (t) ,F (t),mp l_ (mt)sinc +mph(t) =2 (t)l 43 , mmp cosc +mp, mp_p(lt)= 9:8 m/s2 | acceleration due gravity,= 1:0 kg | mass cart,= 0:1 kg | mass pole,= 0:5| half pole length,= 0:0005 | friction coecient cart track,= 0:000002 | friction coecient pole cart,= 10:0 N | force applied center cart time t.equations simulated using Euler's method simulation time step = 0:02 s.gmcmplcpF (t)Acknowledgementswish thank anonymous reviewers paper many insightful comments.unable follow suggestions, contributed much improving paper'sclarity. Thanks also Rich Sutton, whose assistance preparation finalversion paper invaluable.research partially supported Polish Committee Scientific ResearchGrant 8 S503 019 05.ReferencesBaird, III, L. C. (1993). Advantage updating. Tech. rep. WL-TR-93-1146, Wright Laboratory, Wright-Patterson Air Force Base.Barto, A. G. (1992). Reinforcement learning adaptive critic methods. White, D. A.,& Sofge, D. A. (Eds.), Handbook Intelligent Control, pp. 469{491. Van NostrandReinhold, New York.316fiTruncating Temporal DifferencesBarto, A. G., Sutton, R. S., & Anderson, C. (1983). Neuronlike adaptive elementssolve dicult learning control problems. IEEE Transactions Systems, Man,Cybernetics, 13, 835{846.Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. (1990). Learning sequentialdecision making. Gabriel, M., & Moore, J. (Eds.), Learning ComputationalNeuroscience. MIT Press.Cichosz, P. (1994). Reinforcement learning algorithms based methods temporaldifferences. Master's thesis, Institute Computer Science, Warsaw UniversityTechnology.Dayan, P. (1992). convergence TD() general . Machine Learning, 8, 341{362.Dayan, P., & Sejnowski, T. (1994). TD() converges probability 1. Machine Learning,14, 295{301.Heger, M. (1994). Consideration risk reinforcement learning. ProceedingsEleventh International Conference Machine Learning (ML-94). Morgan Kaufmann.Jaakkola, T., Jordan, M. I., & Singh, S. P. (1993). convergence stochastic iterativedynamic programming algorithms. Tech. rep. 9307, MIT Computational CognitiveScience. Submitted Neural Computation .Klopf, A. H. (1982). Hedonistic Neuron: Theory Memory, Learning, Intelligence. Washington D.C.: Hempisphere.Lin, L.-J. (1992). Self-improving, reactive agents based reinforcement learning, planningteaching. Machine Learning, 8, 293{321.Lin, L.-J. (1993). Reinforcement Learning Robots Using Neural Networks. Ph.D. thesis,School Computer Science, Carnegie-Mellon University.Mitchie, D., & Chambers, R. A. (1968). BOXES: experiment adaptive control.Machine Intelligence, 2, 137{152.Moore, A. W., & Atkeson, C. G. (1992). investigation memory-based function approximators learning control. Tech. rep., MIT Artificial Intelligence Laboratory.Pendrith, M. (1994). reinforcement learning control actions noisynon-markovian domains. Tech. rep. UNSW-CSE-TR-9410, School Computer Science Engineering, University New South Wales, Australia.Peng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. ProceedingsEleventh International Conference Machine Learning (ML-94). Morgan Kaufmann.Ross, S. (1983). Introduction Stochastic Dynamic Programming. Academic Press, NewYork.317fiCichoszSchwartz, A. (1993). reinforcement learning method maximizing undiscounted rewards. Proceedings Tenth International Conference Machine Learning(ML-93). Morgan Kaufmann.Singh, S. P. (1994). Reinforcement learning algorithms average-payoff markovian decisionprocesses. Proceedings Twelfth National Conference Artificial Intelligence(AAAI-94).Sutton, R. S. (1984). Temporal Credit Assignment Reinforcement Learning. Ph.D. thesis,Department Computer Information Science, University Massachusetts.Sutton, R. S. (1988). Learning predict methods temporal differences. MachineLearning, 3, 9{44.Sutton, R. S. (1990). Integrated architectures learning, planning, reacting basedapproximating dynamic programming. Proceedings Seventh InternationalConference Machine Learning (ML-90). Morgan Kaufmann.Sutton, R. S., Barto, A. G., & Williams, R. J. (1991). Reinforcement learning directadaptive optimal control. Proceedings American Control Conference, pp.2143{2146. Boston, MA.Sutton, R. S., & Singh, S. P. (1994). step-size bias temporal-difference learning.Proceedings Eighth Yale Workshop Adaptive Learning Systems, pp.91{96. Center Systems Science, Yale University.Tesauro, G. (1992). Practical issues temporal difference learning. Machine Learning, 8,257{277.Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, King's College,Cambridge.Watkins, C. J. C. H., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning,8, 279{292.318fiJournal Artificial Intelligence Research 2 (1994) 89-110Submitted 3/94; published 8/94Pattern Matching Discourse Processing InformationExtraction Japanese TextTsuyoshi Kitanitkitani@cs.cmu.eduYoshio EriguchiMasami Haraeriguchi@rd.nttdata.jpmasami@rd.nttdata.jpCenter Machine TranslationCarnegie Mellon UniversityPittsburgh, PA 15213 USADevelopment HeadquartersNTT Data Communications Systems Corp.66-2 Horikawa-cho, Saiwai-ku, Kawasaki-shi, Kanagawa 210 JAPANAbstractInformation extraction task automatically picking information interestunconstrained text. Information interest usually extracted two steps.First, sentence level processing locates relevant pieces information scattered throughouttext; second, discourse processing merges coreferential information generateoutput. first step, pieces information locally identified without recognizingrelationships among them. key word search simple pattern search achievepurpose. second step requires deeper knowledge order understand relationshipsamong separately identified pieces information. Previous information extraction systemsfocused first step, partly required link pieceinformation pieces. link extracted pieces information maponto structured output format, complex discourse processing essential.paper reports Japanese information extraction system merges information usingpattern matcher discourse processor. Evaluation results show high level systemperformance approaches human performance.1. Introductionrecent information extraction systems, individual pieces information extracted directly text usually identified key word search simple patternsearch preprocessing stage (Lehnert et al., 1993; Weischedel et al., 1993; Cowie et al.,1993; Jacobs et al., 1993). Among systems presented Fifth Message Understanding Conference (muc-5), however, main architectures ranged pattern matchingfull fragment parsing (Onyshkevych, 1993). Full fragment parsing systems,several knowledge sources syntax, semantics, domain knowledge combinedrun-time, generally complicated changing part system tends affectcomponents. past information extraction research, interference sloweddevelopment (Jacobs, 1993; Hobbs et al., 1992). pattern matcher, identifiespatterns interest, appropriate information extraction texts narrowdomains, since task require full understanding text.c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiKitani, Eriguchi, & Haratextract, information extraction system described here, uses pattern matchersimilar sri's fastus pattern matcher (Hobbs et al., 1992). matcher implementedfinite-state automaton. Unlike pattern matchers, textract's matcher dealsword matching problems caused word segmentation ambiguities often foundJapanese compound words.goal pattern matcher identify concepts represented wordsphrases text. pattern matcher first performs simple key-word-based conceptsearch, locating individual words associated concepts. second step templatepattern search locates phrasal patterns involving critical pieces information identified preprocessor. template pattern search identifies relationshipsmatched objects defined pattern well recognizing concept behind relationship. One typical concept relationship \economic activity" companiesparticipate other.usually dicult determine relationships among pieces informationidentified separate sentences. relationships often stated implicitly,even text explicitly mentions descriptions often located far enoughapart make detection dicult. Although importance discourse processing information extraction emphasized Message Understanding Conferences (Lehnert& Sundheim, 1991; Hirschman, 1992), system presented satisfactorily addressedissue.discourse processor textract able correlate individual pieces informationthroughout text. textract merges concepts pattern matcher identifiedseparately (and usually different sentences) concepts involve companies. textract unify multiple references company even companyname missing, abbreviated, pronominalized. Furthermore, processor segmentsdiscourse isolate portions text relevant particular conceptual relationship.discourse segmentation lessens chance merging unrelated information (Kitani, 1994).paper analyzes evaluation results textract's discourse module describes tipster/muc-5 evaluation results order assess overall system performance.2. tipster information extraction taskgoal tipster project sponsored arpa capture information interestEnglish Japanese newspaper articles corporate joint ventures microelectronics. system must fill generic template information extracted textfully automated process. template composed several objects, containing several slots. Slots may contain pointers related objects (Tipster, 1992). Extractedinformation stored object-oriented database.joint ventures domain, task extract information concerning joint venturerelationships organizations form dissolve. template structure representsrelationships tie-up-relationship objects, contain pointers organizationentity objects representing organizations involved. Entity objects contain pointersobjects person facility objects, shown Figure 1.microelectronics domain, extraction focuses layering, lithography, etching,packaging processes semiconductor manufacturing microchip fabrication. entities90fiPattern Matching Discourse ProcessingTEMPLATEdoc. no.doc. datedoc. sourcecontent (*)ENTITYnamealiaseslocationnationalitytypeentity rel. (+)person(*)facility (*)TIE-UP-REL.tie-up statusentity (+)createdentity (*)activity (*)ACTIVITYindustry (*)ENTITY-REL.entity1 (+)entity2 (*)rel. ent2ent1PERSONnamepersons entitypositionFACILITYnamelocationtypeINDUSTRYindustry typeproduct /servicedenotes instantiations multiple objects(*) points zero objects, (+) points one objectsFigure 1: Object-oriented template structure joint ventures domainextracted include manufacturer, distributor, user, addition detailed manufacturinginformation materials used microchip specifications wafer size devicespeed. microelectronics template structure similar joint venturesfewer objects slots.extraction tasks must identify individual entities also certainrelationships among them. Often, however, particular piece extracted informationdescribes part relationship. partial information must mergedpieces information referring entities. merging produce correct results,therefore, correct identification entity references crucial.3. Problem definitionsection first describes word matching problems caused word segmentation ambiguities. Diculties reference resolution company names explained. Issuesdiscourse segmentation concept merging also discussed using example text.3.1 Word segmentationJapanese word segmentation preprocessor gives rise subsequent under-matchingproblem. key word text found word segmentor's lexicon,segmentor tends divide separate words. current lexicon, example,91fiKitani, Eriguchi, & Haracompound noun \ F " (teikei-kaisyo), consisting two words, \ " (teikei: jointventure) \ F " (kaisyo: dissolve), segmented two individual nouns. Thuskey word search \ F " (teikei-kaisyo) succeed segmented sentence.hand, pattern matching process allows, default, partial matchingkey word word text. \ " (teikei) \ [Z " (gyoum-teikei),meaning \a joint venture", matched single key word \ "(teikei). exibility creates over-matching problem. example, key word\ JS " (silicon) matches \ f) JS" (nisanka-silicon: silicon dioxide), althoughdifferent materials reported microelectronics domain. segmentation diculties compound nouns also cause major problems word-based Japaneseinformation retrieval systems (Fujii & Croft, 1993).3.2 Company name referencescorporate joint ventures domain, output templates mostly describe relationshipsamong companies (as described Section 2). Information interest therefore foundsentences mention companies activities. essential extractoridentify topic companies|the main concern sentences appear in|in ordercorrelate information identified sentence. three problems makedicult identify topic companies.1. Missing subjectTopic companies usually subject sentence. Japanese sentences frequentlyomit subjects, however|even formal newspaper articles. veniex systemnec presented muc-5 identify company implied missing subjectexplicit reference immediately preceding sentence (Doi et al.,1993; Muraki et al., 1993). clear whether veniex resolve missingreference explicit reference appears sentence separatedsubjectless sentence.2. Company name abbreviationsalso seen English, company names often abbreviated Japanese text first appearance. variety ways abbreviate company names Japanesegiven (Karasawa, 1993). following examples show typical abbreviationsJapanese company names:(a) partial word\ AK' 9S$" ! \ 9S$ "(Mercedes-Benz)(Benz)(b) English abbreviation\ o%*NTT+ " ! \ NTT "(Nippon Telegraph Telephone)(c) first Katakana character + \ "\ AJffS 7L " ! \ "(American Express Corp.)92fiPattern Matching Discourse Processing(d) first character primitive segment\o % " ! \ "1(Japan Airlines)(e) randomly selected characters\o % " ! \ "(Shin-nihon Steel)Locating company name abbreviations dicult, since many identifiedcompanies either morphological analyzer name recognizer preprocessor. Another problem variety ways abbreviating names makesdicult unify multiple references one company.Almost muc-5 systems include string matching mechanism identify company name abbreviations. abbreviations specified aliases slotcompany entity object. authors' knowledge, none systemstextract detect company name abbreviations type (d) (e) withoutusing pre-defined abbreviation table.3. Company name pronounsCompany name pronouns often used formal texts. Frequently used expressionsinclude \ ! " (ryosya: companies), \ $ " (dosya: company), \ r" (jisya: company itself). shown following examples, resolvingreferences particularly important full understanding text. Direct Englishtranslation follows Japanese sentences.(a) \ X/Y( $ .WR r 6IS)'K "*Y+*X+\X Corp. tied Corp. sells products companybrand name."(Y Corp.) (X Corp.)(b) \ X/.'/ $ .p/NJ "*X+\X Corp. biggest company field. president companyMr. Suzuki."(X Corp.)Reference resolution \ $ " (dosya: company) implemented veniex (Doiet al., 1993). veniex resolves pronominal reference way identifiesmissing company references. crl/brandeis diderot system presented muc5 simply chooses nearest company name referent \dosya". algorithmlater improved Wakao using corpus-based heuristic knowledge (Wakao, 1994).systems handle pronominalized company names \dosya".three problems described section often cause individual informationcorrelated wrong company tie-up-relationship object. avoid error,topic companies must tracked context, since used determinecompany objects information fragment assigned to. Abbreviatedpronominalized company names must unified references company.1. \ o% " (nihon: Japan) \ " (koukuu: airlines) primitive segments example.93fiKitani, Eriguchi, & Hara3.3 Discourse segmentation concept mergingjoint ventures domain, tie-up-relationship object contains pointers objectseconomic activities (as shown Figure 1). company involved multiple tie-ups, merging information tie-up relationship according topic companiessometimes yields incorrect results. Consider following example:"X Corp. tied Corp. X start selling productsJapan next month. Last year X started similar joint ventureZ Inc."Obviously, sale second sentence related tie-up relationship XY. However, since topic company, subject sentence, X threesentences, sale could also related X Z tie-up relationship. incorrectmerging avoided separating text two blocks: first two sentencesdescribe X tie-up, last sentence describes X Z tie-up. Thus,discourse segmentation necessary identify portions text containing related piecesinformation. crl/brandeis diderot system segments joint ventures texttwo types text structures (Cowie et al., 1993). known well discoursesegmentation performed, however.text segmented, concepts identified pieces information mergedwithin discourse segment. example, expected income joint ventureoften stated sentence explicitly mention participating companies;appear previous sentence. case, joint venture concept identifyingcompanies income concept identifying expected income must mergedlatter linked correct entity objects.4. solutionsection describes details textract's pattern matcher discourse processorwell system architecture.4.1textract architecturetextract information extraction system developed tipster Japanese do-mains corporate joint ventures microelectronics (Jacobs, 1993; Jacobs et al., 1993).shown Figure 2, textract joint ventures system comprises four major components: preprocessor, pattern matcher, discourse processor, template generator.shorter development time, textract microelectronics system simpler configuration joint ventures system. include template pattern searchpattern matcher, discourse segmentation concept merging discourseprocessor, also shown Figure 2.preprocessor, Japanese segmentor called majesty segments Japanese textprimitive words tagged parts speech (Kitani, 1991). Next, name recognizeridentifies proper names monetary, numeric, temporal expressions. majesty tagsproper names appear lexicon; name recognizer identifies additional propernames locating name designators \ " (sya, corresponding \Inc." \Corp.")94fiPattern Matching Discourse ProcessingPattern matcherPreprocessorconceptsearchtemplatepatternsearch- concept- conceptidentificationidentification- informationmerging withinsentence- morphologicalanalysis- namerecognitionDiscourse processorcompanydiscourseconceptnamesegmentamergingunificationtion- companynamereferenceresolution- information- textsegmentation mergingwithin textjoint venturessystemTemplategenerator- outputgenerationFigure 2: textract system architecturecompany names. recognizer extends name string forward backwarddesignator meets search stop conditions (Kitani & Mitamura, 1993). namesegments grouped units meaningful pattern matching process(Kitani & Mitamura, 1994). strings extracted directly text identifiedmajesty name recognizer.Details pattern matcher discourse processor given following sections. template generator assembles extracted information creates outputdescribed Section 2.4.2 Pattern matcherfollowing subsections describe concept search template pattern searchpattern matcher identify concepts sentence. Whereas former simplysearches key words, latter searches phrasal patterns within sentence.template pattern search also identifies relationships matched objects definedpattern. course textract development, key words template patternsobtained manually system developer using kwic (Key Word Context) toolreferring word frequency list obtained corpus.4.2.1 Concept searchKey words representing concept grouped list used recognizeconcept sentence. list written simple format: (concept-name word1 word2...). example, key words recognizing dissolved joint venture concept writtenfollowing way:95fiKitani, Eriguchi, & Hara(DISSOLVEDF Fn)(DISSOLVED dissolve terminate cancel).concept search module recognizes concept locates one associatedwords sentence. simple procedure sometimes yields incorrect concepts. example, concept \dissolved" erroneously identified expression \cancelhotel reservation". Key-word-based concept search successful processing textnarrow domain words used restricted meanings.under-matching problem occurs compound noun key word listconcept fails match text instance compound textsegmented separate primitive words. avoid problem, adjacent nouns textautomatically concatenated concept search process, generating compoundnouns run-time. over-matching problem, hand, arises key wordsuccessfully matches part compound noun whole associatedconcept. Over-matching prevented anchoring beginning and/or endkey word pattern word boundaries (with symbol \>" beginning \<"end). example, \> JS <" (silicon) must matched single completeword text. Since problem rare, solution automatic: system developersattach anchors key words likely over-match.4.2.2 Template pattern searchtextract's pattern matcher implemented finite-state recognizer. choiceimplementation based assumption finite-state grammar eciently handlemany inputs context-free grammar covers (Pereira, 1990). pattern matchersimilar pattern recognizer used muc-4 fastus system developed sri(Hobbs et al., 1992).Patterns textract template pattern matcher defined rules similarregular expressions. pattern definition specifies concept associatedpattern. (For joint ventures domain, textract uses eighteen concepts.)matcher, state transitions driven segmented words grouped unitspreprocessor. matcher identifies possible patterns interest textmatch defined patterns, recognizing concepts associated patterns.inputs, matcher must skip words explicitly defined pattern.Figure 3 shows definitions equivalent Japanese English patterns recognizingconcept *joint-venture*. English pattern used capture expressions\XYZ Corp. created joint venture PQR Inc." notation \@string" representsvariable matching arbitrary string. Variables whose names begin \@cname"called company-name variables used company name expectedappear. definitions shown, string matched \@cname partner subj" likelycontain least one company name referring joint venture partner functioningsubject sentence.pattern \ /#fi:strict:P" matches grammatical particles \ / " (wa ) \ fi "(ga ), serve subject case markers. symbol \strict" specifies full string match(the default case template pattern search), whereas \loose" allows partial string96fiPattern Matching Discourse Processing(a)(JointVenture1 6@CNAME_PARTNER_SUBJ/#fi:strict:P@CNAME_PARTNER_WITH(:strict:P@SKIP:loose:VN)(b)(JointVenture1 3@CNAME_PARTNER_SUBJcreate::Vjoint venture::NPwith::P@CNAME_PARTNER_WITH)Figure 3: matching pattern (a) Japanese (b) Englishmatch. Partial string matching useful matching defined pattern compound words.verbal nominal pattern \ : loose:VN" matches compound words \ [ "(kigyo-teikei: corporate joint venture) well \ " (teikei: joint venture).first field pattern pattern name, refers concept associatedpattern. second field number indexing field pattern. field'scontents used decide quickly whether search within given string.matcher applies entire pattern string string contains textindexed field. eciency, therefore, field contain least frequent wordentire pattern (in case, \ " (teikei) Japanese \a joint venture"English).order noun phrases relatively unconstrained Japanese sentence. Casemarkers, usually attached ends noun phrases, provide strong clue identifyingcase role phrase (subject, object, etc.). Thus pattern matching driven mainlycase markers recognizes case roles well without parsing sentence.Approximately 150 patterns used extract various concepts Japanese jointventures domain. Several patterns usually match single sentence. Moreover, since patternsoften defined case markers \ / " (wa), \ fi " (ga), \ ( " (to), singlepattern match sentence one way several case markersappear sentence. template generator accepts best matched pattern,chosen applying following three heuristic rules order shown:1. select patterns include largest number matched company-name variablescontaining least one company name;2. select patterns consume fewest input segments (the shortest string match);3. select patterns include largest number variables defined words.heuristic rules obtained examination matched patterns reportedsystem. obtain reliable heuristics, large-scale statistical evaluation mustperformed. Heuristics similar problem pattern selection English discussed(Rau & Jacobs, 1991). system chooses pattern consumes inputsegments (the longest string match), opposed textract's choice shortest stringmatch second heuristic rule.22. Rau Jacobs' system, third heuristic rule seems applied second rule.case, little difference performance heuristic rules two systems.97fiKitani, Eriguchi, & HaraAnother important feature pattern matcher rules grouped accordingconcept. rule name \JointVenture1" Figure 3, example, representsconcept *joint-venture*. Using grouping, best matched patternselected matched patterns particular concept group instead matchedpatterns. feature enables discourse template generation processes narrowsearch best information fill particular slot.4.3 Discourse processorfollowing subsections describe algorithm company name reference resolutionthroughout discourse. Discourse segmentation concept merging processes alsodiscussed.4.3.1 Identifying topic companiesSince syntactic analysis performed textract, topic companies simply identified wherever subject case marker \ fi " (ga), \ / " (wa), \ B " (mo) followscompany names. topic companies found sentence, previous sentence'stopic companies inherited (even current sentence contains non-company subject). based supposition sentence introduces new companiesusually mentions explicitly subject.4.3.2 Abbreviation detection unificationCompany name abbreviations following observed characteristics:majesty tags abbreviations \unknown", \company", \person", \place";company name precedes abbreviations;abbreviation composed two characters company name,original order;characters need consecutive within company name;English word abbreviations must identical English word appearingcompany name.Thus following regarded abbreviations: \unknown", \company", \person",\place" segments composed two characters also appear companynames previously identified text. comparing possible abbreviationsknown company names, length longest common subsequence LCS (Wagner &Fischer, 1974) computed determine maximum number characters appearingorder strings.3unify multiple references company, unique number assignedsource abbreviated companies. Repeated company names contain stringsappearing earlier text treated abbreviations (and thus given unique numbers)3. example, LCS \abacbba" \bcda" \bca".98fiPattern Matching Discourse Processing1. Step 1: Initialization assign entity C unique number.C (1 cmax)C [i; \id"]done2. Step 2: Search abbreviations give unique numbersC (1 cmax)C [i; \id"] 6=# already recognized abbreviationcontinue loopLENSRC length C [i; \string"]j C (i + 1 j cmax)C [j; \id"] 6= j# already recognized abbreviationcontinue j loopLEN length C [j; \string"]LCS length LCS C [i; \string"] C [j; \string"]LCS 2C [i; \eg"] = \YES" LENSRC = LCS = LENC[j; \id"] C [i; \id"] # English word abbreviationelse C [i; \eg"] = \NO" LCS = LEN# abbreviationC[j; \id"] C [i; \id"]donedonedoneFigure 4: Algorithm unify multiple references companyalgorithm described Figure 4. pseudocode shown, identified companynames stored associative array named C . \Unknown", \company", \person",\place" segments also stored array possible abbreviations. Company namessorted ascending order starting position text numbered 1 cmax(Step 1). company name string indexed addressed C [i; \string"].ag C [i; \eg"] records whether company name English word abbreviation not.Step 2 compares company name array C names higher array(and thus later text). LCS pair earlier later company namesequal length later company name, later company name recognizedabbreviation earlier company name. Then, \id" later company namereplaced earlier company name. LCS must two characters,abbreviation English word, LCS must equal lengthearlier company name.end execution, number given C [i; \id"]. C [i; \id"] changedexecution, C [i; \string"] recognized company name abbreviation.99fiKitani, Eriguchi, & Hara4.3.3 Anaphora resolution company name pronounsapproach reference resolution described section based heuristics obtainedcorpus analysis rather linguistic theories. Three company name pronounstarget reference resolution: \ ! " (ryosya), \ $ " (dosya), \ r " (jisya), meaning\both companies", \the company", \the company itself". threefrequent company name pronouns appearing corpus provided arpa tipsterinformation extraction project. \Ryosya", \dosya", \jisya" appeared 456, 277, 129times, respectively, 1100 newspaper articles containing average 481 characters perarticle.following heuristics, derived analysis pronoun reference corpus,used reference resolution:\ryosya" almost always referred \current" tie-up company, one exceptionhundred occurrences;ninety percent \dosya" occurrences referred topic companyone possible referent sentence, but:two companies, including topic company, preceded \dosya"sentence, seventy-five percent pronoun occurrences referrednearest company, necessarily topic company;eighty percent \jisya" occurrences referred topic company.Two additional heuristic rules discovered implemented textract:four percent \jisya" occurrences referred one company;eight percent \jisya" occurrences referred entities general expressions company \ fi " (kaisya: company).result discourse processing described above, every company name, includingabbreviations pronominal references, given unique number.4.3.4 Discourse segmentation concept merging150 articles tipster/muc-5 joint ventures test set, multiple tie-up relationshipsappeared thirty-one articles included ninety individual tie-up relationships.two typical discourse models representing discourse structures tie-up relationshipsshown Figure 5.Type-I: tie-ups described sequentiallyDescriptions tie-ups appear sequentially model. One tie-up mentionednew tie-up described.Type-II: main tie-up reappears tie-ups mentionedmajor difference Type-I model description main tie-upreappears text tie-up relationships introduced. Non-maintie-ups usually mentioned brie y.100fiPattern Matching Discourse Processingtie-up-1tie-up-1tie-up-2tie-up-2..non-main tie-ups.tie-up-3...tie-up-ntie-up-ntie-up-1Type-IType-IIFigure 5: Discourse structure tie-up relationshipsEleven Type-I structures thirteen Type-II structures appeared thirty-one articles. Seven articles contained complicated discourse structures regarding tie-uprelationships.two types text structure described similar ones implementedcrl/brandeis diderot joint ventures system. difference Type-IIstructure: diderot processes tie-up relationships reappear text,reappearing main tie-up focused textract.textract's discourse processor divides text different tie-up relationshipidentified template pattern search. different tie-up relationship recognizednumbers assigned joint venture companies identical appearingprevious tie-up relationships. diderot segments discourse related piecesinformation date entity location different tie-up relationships.strict merging preferable pieces information comparison correctlyidentified. merging conditions discourse segments chosen accordingaccuracy identification information compared.discourse segmented, identified concepts extracted words phrasesmerged. Figure 6 shows merging process following text passage actuallyappeared tipster/muc-5 test set (a direct English translation follows):\ /8o;.DAffAK..o%U'.$RKRS .fi' KH+*K56'+/!fiZ&fiRfiK(B4 "\On eighth (of month), Tanabe Pharmaceuticals made jointventure contract German pharmaceutical maker, Merck Co.Inc., develop sell new medicine Japan. also agreedcompanies would invest equally establish joint venture companyfive six years start selling new medicine."101fiKitani, Eriguchi, & HaraFirst sentence:"On eighth (of month),Tanabe Pharmaceuticals madejoint venture contractGerman pharmaceutical maker,Merck Co. Inc., developsell new medicineJapan."Second sentence:"They also agreedcompanies would invest equallyestablish joint venturecompany five six yearsstart selling new medicine.""Tanabe Pharmaceuticals"Templatepatternsearch*ESTABLISH*"a joint venture"bothcompany"companies"*ECONOMICACTIVITY*"Merck Co. Inc.""Tanabe Pharmaceuticals"Discourseprocessor"bothcompanies"*ECONOMICACTIVITY**ESTABLISH*"a joint venturecompany""Merck Co. Inc."Figure 6: Example concept mergingtwo company names first sentence, \ " (tanabe seiyaku: TanabePharmaceuticals) \ AK " (ei meruku sya: Merck Co. Inc.), identifiedeither majesty name recognizer preprocessing. Next, template patternsearch locates first sentence \economic activity" pattern shown Figure 7 (a).*economic-activity* concept relating two companies recognized.template pattern search also recognizes *establish* concept second sentencetemplate pattern shown Figure 7 (b).sentence-level processing, discourse processing recognizes \ ! " (ryosya:companies) second sentence refers Tanabe Pharmaceuticals Merckfirst sentence current tie-up companies. Since second sentenceintroduce new tie-up relationship, sentences discoursesegment. Concepts separately identified two sentences mergedsubjects two sentences same. *establish* concept therefore joined*economic-activity* concept.(a)(EconomicActivityE 6@CNAME_PARTNER_SUBJ:strict:P@CNAME_PARTNER_SUBJ:strict:P@SKIP:loose:VN)(b)(Establish3 6@CNAME_PARTNER_SUBJ:strict:P@CNAME_CREATED_OBJ:strict:P@SKIP:loose:VN)/#fiRfi/#fi.$Figure 7: Economic activity pattern (a) establish pattern (b)102fiPattern Matching Discourse Processing5. Performance evaluationsection shows evaluation results textract's discourse module. muc-5 evaluation metrics overall textract performance also discussed.5.1 Unique identification company name abbreviationshundred joint ventures newspaper articles used tipster 18-month evaluationchosen blind test set evaluation. evaluation measures recall,percentage correct answers extracted compared possible answers, precision,percentage correct answers extracted compared actual answers. majestyname recognizer identified company names evaluation set recall seventyfive percent precision ninety-five percent partial matches expectedrecognized strings allowed, recall sixty-nine percent precisioneighty-seven percent exact matching condition.Company names appeared form different first appearancearticle considered company name abbreviations. Among 318 abbreviations,recall precision abbreviation detection sixty-seven eighty-nine percent, respectively. importantly, detected abbreviations unified correctly sourcecompanies long source companies identified correctly majestyname recognizer.evaluation results clearly show company name abbreviations accuratelydetected unified source companies long company names correctlyidentified preceding processes. possible, however, simple string matchingalgorithm currently used could erroneously unify similar company names, oftenseen among family companies.5.2 Anaphora resolution company name pronounsaccuracy reference resolution \ryosya", \dosya", \jisya" shown Table1. numbers parentheses obtained restricting attention pronounsreferred companies identified correctly preceding processes. Since companiesreferred \ryosya" (both companies) usually \current" tie-up companiesjoint ventures domain, reference resolution accuracy depended accuracytie-up relationships identified.company name pronounsnumberreferences\ ! " (ryosya: companies) 101 (93)\ $ " (dosya: company)100 (90)\ r " (jisya: company itself) 60 (53)resolutionaccuracy64% (70%)78% (87%)78% (89%)Table 1: Accuracy reference resolutions103fiKitani, Eriguchi, & Haramajor cause incorrect references \dosya" failure locate topic companies. simple mechanism searching topic companies using case markerswork well. typical problem seen following example: \ '/X " (Ajoint venture partner X Corp.). X Corp topic company, subject \ X "(X Corp.) followed subject case marker. errors attributedfact \dosya" always refer topic company discussed heuristic rules\dosya" reference resolution.Regarding \jisya" resolutions, five instances referred multiplecompanies bound single company. Since multiple companies usually listedusing conjunctions \ ( " (to: and) \ " (comma), identified easilysimple phrase analysis performed.became clear evaluation resolving \dosya" references non-topiccompany required intensive text understanding. Forty-seven percent occurrences\dosya" \jisya" bound topic companies inherited previous sentence.result strongly supported importance keeping track topic companies throughoutdiscourse.5.3 Discourse segmentationThirty-one 150 tipster/muc-5 evaluation test articles included ninety multiple tieup relationships. textract's discourse processor segmented thirty-one articlesseventy-one individual tie-up relationship blocks. thirty-eight blocks correctly segmented. Main tie-up relationships reappeared Type-II discourse structures detected well, caused structures incorrectly recognizedType-I. error caused fact joint venture relationships usually mentioned implicitly reappeared text. example, noun phrase,\ ./ " (the joint venture time), detected template patterns used, brought focus back main tie-up. result, textract identified eightpercent fewer tie-up relationships possible number expected tipster/muc-5evaluation. merging error must affected system performance since information reappearing main tie-up segment would correctly linkedearlier main tie-up segment.preliminary study suggested recognizing segmentation points textregarded crucial performance. template pattern matching alone goodenough recognize segmentation points. discourse processor simply segmentedtext found new tie-up relationship. discourse models, currently unusedrun-time textract, could used help infer discourse structure systemsure whether merge separate discourse segments. Reference resolution definiteindefinite noun phrases must also solved accurate discourse segmentation futureresearch.accuracy discourse segmentation might improved checking differenceidentity date entity location, well entity name, deciding whethermerge tie-up relationship. textract take date location objects accountmaking segmentation decisions, textract's identification objectsconsidered reliable enough. example, date object identified recall104fiPattern Matching Discourse Processingtwenty-seven percent precision fifty-nine percent. hand, entitiesidentified eighty percent accuracy recall precision. avoidincorrect discourse segmentation, therefore, textract's merging conditions includedentity names reliable information.5.4 Overall textract performance250 newspaper articles, 150 Japanese corporate joint ventures 100 Japanesemicroelectronics, provided arpa use tipster/muc-5 system evaluation.Six joint ventures systems five microelectronics systems, including textract developed cmu optional system ge-cmu shogun, presented Japanesesystem evaluation muc-5. scoring program automatically compared system outputanswer templates created human analysts. human decision necessary,analysts instructed scoring program whether two strings comparison completely matched, partially matched, unmatched. Finally, scoring program calculatedoverall score combined newspaper article scores. Although various evaluation metrics measured evaluation (Chinchor & Sundheim, 1993),following error-based recall-precision-based metrics discussed paper.basic scoring categories used are: correct (COR), partially correct (PAR), incorrect (INC),missing (MIS), spurious (SPU), counted number pieces informationsystem output compared possible information.(1) Error-based metricsError per response fill (ERR):wrong = INC + P AR=2 + MIS + SPUtotal COR + PAR + INC + MIS + SP UUndergeneration (UND):MISMIS =possible COR + PAR + INC + MISOvergeneration (OVG):SPU =SPUactual COR + P AR + INC + SP USubstitution (SUB):INC + P AR=2COR + P AR + INC105fiKitani, Eriguchi, & HaradomainERR UND OVG SUB REC PRE P&Rtextract (JJV)503223126068 63.8System (JJV)543627125764 60.1System B (JJV)635123124267 52.1textract (JME) 594328125163 56.4System (JME)583038146053 56.3System B (JME)655424124066 50.4Table 2: Scores textract two top-ranking ocial systems tipster/muc-5(2) Recall-precision-based metricsRecall (REC):Precision (PRE):P&R F-measure (P&R):COR + PAR=2possibleCOR + PAR=2actual2 REC PREC + PREerror per response fill (ERR) ocial measure muc-5 system performance.Secondary evaluation metrics undergeneration (UND), overgeneration (OVG),substitution (SUB). recall, precision, F-measure metrics used unocialmetrics muc-5.Table 2 shows scores textract two top-ranking ocial systems takentipster/muc-5 system evaluation results.4 textract processed Japanesedomains corporate joint ventures (JJV) microelectronics (JME), whereas twosystems processed English Japanese text. textract performed welltop-ranking systems two Japanese domains.human performance four well-trained analysts reported eightypercent recall precision English microelectronics domain (Will, 1993).thirty percent better best tipster/muc-5 systems' performanceP&R F-measure language domain. Japanese joint ventures domain,textract scored recall seventy-five percent precision eighty-one percentcore template comprising essential objects. result suggests currenttechnology could used support human extraction work task well-constrained.4. textract scores submitted muc-5 unocial. scored ocially conference.Table 2 shows textract's ocial scores.106fiPattern Matching Discourse ProcessingRunning SUN SPARCstation IPX, textract processed joint ventures articlesixty seconds microelectronics article twenty-four seconds average.human analysts took fifteen minutes complete English microelectronicstemplate sixty minutes Japanese joint ventures template (Will, 1993).Thus human-machine integrated system would best solution fast, high quality,information extraction.tipster/muc-5 systems processed Japanese English domains.systems generally performed better Japanese domains corresponding English domains. One likely reason structure Japanese articles fairly standard,particularly Japanese joint ventures domain, readily analyzedtwo discourse structure types described paper. Another possible reason characteristic writing style: expressions need identified tend appear firstsentences form suitable pattern matching.textract Japanese microelectronics system copied preprocessor, conceptsearch pattern matcher, company name unification discourse processorused textract Japanese joint ventures system. microelectronics systemdeveloped three weeks one person replaced joint ventures concepts keywords representative microelectronics concepts key words. lower performancetextract microelectronics system compared joint ventures system largelydue short development time. also probably due less homogeneous discoursestructure writing style microelectronics articles.6. Conclusions future researchpaper described importance discourse processing three aspects information extraction: identifying key information throughout text, i.e. topic companiescompany name references tipster/muc-5 domains; segmenting text select relevant portions interest; merging concepts identified sentence level processing.basic performance system depends preprocessor, however, since manypieces identified information put directly slots otherwise used fill slotslater processing. textract's pattern matcher solves matching problem causedsegmentation ambiguities often found Japanese compound words. patternmatching system based finite-state automaton simple runs fast. factorsessential rapid system development performance improvement.improve system performance pattern matching architecture, increasenumber patterns unavoidable. Since matching large number patternslengthy process, ecient pattern matcher required shorten running time.Tomita's new generalized LR parser, known one fastest parsers practicalpurposes, skips unnecessary words parsing (Bates & Lavie, 1991). parserevaluation investigate appropriate information extraction Japanesetext (Eriguchi & Kitani, 1993). Pattern matching alone, however, able improvesystem performance human levels complicated information extraction tasktipster/muc-5, even task well-defined suitable pattern matching.efforts made discourse processing discourse segmentation referenceresolution definite indefinite noun phrases.107fiKitani, Eriguchi, & Hararesearch discussed paper based application-oriented, domain-specific,language-specific approach relying patterns heuristic rules collectedparticular corpus. obvious patterns heuristic rules described papercover wide range applications, domains, languages. empirical approachdescribed worth investigating even entirely new task, however, sinceachieve high level system performance relatively short development time.linguistic theory-based systems tend become complex dicult maintain, especiallyincorporate full text parsing, simplicity empirically-based, pattern-orientedsystem textract keeps development time short evaluation cycle quick.Corpus analysis key element corpus-based paradigm. estimatedcorpus analysis took half development time textract. Statistically-basedcorpus analysis tools necessary obtain better performance shorter developmenttime. tools could help developers extract important patterns heuristicrules corpus, also monitor system performance evaluationimprovement cycle.Acknowledgementsauthors wish express appreciation Jaime Carbonell, provided opportunity pursue research Center Machine Translation, Carnegie MellonUniversity. Thanks also due Teruko Mitamura Michael Mauldin manyhelpful suggestions.ReferencesBates, J., & Lavie, A. (1991). Recognizing Substrings LR(k) Languages Linear Time.Tech. rep. CMU-CS-91-188, Carnegie Mellon University, School Computer Science.Chinchor, N., & Sundheim, B. (1993). MUC-5 Evaluation Metrics. ProceedingsFifth Message Understanding Conference (MUC-5), pp. 69{78.Cowie, J., Guthrie, L., et al. (1993). CRL/BRANDEIS: Description Diderot SystemUsed MUC-5. Proceedings Fifth Message Understanding Conference(MUC-5), pp. 161{179.Doi, S., Ando, S., & Muraki, K. (1993). Context Analysis Information Extraction SystemBased Keywords Text Structure. Proceedings Forty-seventh AnnualConference IPSJ (in Japanese).Eriguchi, Y., & Kitani, T. (1993). Preliminary Study Using Tomita's Generalized LRParser Information Extraction. Unpublished paper, Center Machine Translation, Carnegie Mellon University.Fujii, H., & Croft, B. (1993). Comparison Indexing Techniques Japanese Text Retrieval. Proceedings Sixteenth Annual International ACM SIGIR ConferenceResearch Development Information Retrieval, pp. 237{246.Hirschman, L. (1992). Adjunct Test Discourse Processing MUC-4. ProceedingsFourth Message Understanding Conference (MUC-4), pp. 67{77.108fiPattern Matching Discourse ProcessingHobbs, J., Appelt, D., et al. (1992). FASTUS: System Extracting InformationNatural-Language Text. Tech. rep. 519, SRI International.Jacobs, P. (1993). TIPSTER/SHOGUN 18-Month Progress Report. Notebook TIPSTER 18-Month Meeting.Jacobs, P., Krupka, G., et al. (1993). GE-CMU: Description Shogun System UsedMUC-5. Proceedings Fifth Message Understanding Conference (MUC-5),pp. 109{120.Karasawa, I. (1993). Detection Company Name Abbreviations Japanese Texts. Unpublished paper, Center Machine Translation, Carnegie Mellon University.Kitani, T. (1991). OCR Post-processing Method Handwritten Japanese Documents.Proceedings Natural Language Processing Pacific Rim Symposium, pp. 38{45.Kitani, T. (1994). Merging Information Discourse Processing Information Extraction.Proceedings Tenth IEEE Conference Artificial Intelligence Applications, pp. 412{418.Kitani, T., & Mitamura, T. (1993). Japanese Preprocessor Syntactic SemanticParsing. Proceedings Ninth IEEE Conference Artificial IntelligenceApplications, pp. 86{92.Kitani, T., & Mitamura, T. (1994). Accurate Morphological Analysis Proper NameIdentification Japanese Text Processing. Journal Information Processing SocietyJapan, 35 (3), 404{413.Lehnert, W., McCarthy, J., et al. (1993). UMASS/HUGHES: Description CIRCUSSystem Used MUC-5. Proceedings Fifth Message Understanding Conference (MUC-5), pp. 277{291.Lehnert, W., & Sundheim, B. (1991). Performance Evaluation Text-Analysis Technologies. AI Magazine, Fall, 81{94.Muraki, K., Doi, S., & Ando, S. (1993). NEC: Description VENIEX System UsedMUC-5. Proceedings Fifth Message Understanding Conference (MUC-5),pp. 147{159.Onyshkevych, B. (1993). Technology Perspective. Notebook Fifth Message Understanding Conference (MUC-5).Pereira, F. (1990). Finite-State Approximations Grammars. Proceedings DARPASpeech Natural Language Workshop, pp. 20{25.Rau, L., & Jacobs, P. (1991). Creating Segmented Databases Free Text TextRetrieval. Proceedings Fourteenth Annual International ACM/SIGIR ConferenceResearch Development Information Retrieval, pp. 337{346.Tipster (1992). Joint Venture Template Fill Rules. Plenary Session NotebookTIPSTER 12-Month Meeting.109fiKitani, Eriguchi, & HaraWagner, R., & Fischer, M. (1974). String-to-String Correction Problem. JournalACM, 21 (1), 168{173.Wakao, T. (1994). Reference Resolution Using Semantic Patterns Japanese NewspaperArticles. Proceedings COLING 94, pp. 1133{1137.Weischedel, R., Ayuso, D., et al. (1993). BBN: Description PLUM System UsedMUC-5. Proceedings Fifth Message Understanding Conference (MUC-5),pp. 93{107.Will, C. (1993). Comparing Human Machine Performance Natural Language Information Extraction: Results English Microelectronics MUC-5 Evaluation.Proceedings Fifth Message Understanding Conference (MUC-5), pp. 53{67.110fiJournal Artificial Intelligence Research 2 (1995) 369-409Submitted 10/94; published 3/95Cost-Sensitive Classification: Empirical EvaluationHybrid Genetic Decision Tree Induction AlgorithmPeter D. TurneyKnowledge Systems Laboratory, Institute Information TechnologyNational Research Council Canada, Ottawa, Ontario, Canada, K1A 0R6.TURNEY@AI.IIT.NRC.CAAbstractpaper introduces ICET, new algorithm cost-sensitive classification. ICETuses genetic algorithm evolve population biases decision tree induction algorithm. fitness function genetic algorithm average cost classificationusing decision tree, including costs tests (features, measurements)costs classification errors. ICET compared three algorithmscost-sensitive classification EG2, CS-ID3, IDX also C4.5, classifies without regard cost. five algorithms evaluated empirically five realworld medical datasets. Three sets experiments performed. first set examinesbaseline performance five algorithms five datasets establishes ICETperforms significantly better competitors. second set tests robustnessICET variety conditions shows ICET maintains advantage. thirdset looks ICETs search bias space discovers way improve search.1. Introductionprototypical example problem cost-sensitive classification medical diagnosis, doctor would like balance costs various possible medical testsexpected benefits tests patient. several aspects problem:benefit test, terms accurate diagnosis, justify cost test?time stop testing make commitment particular diagnosis? muchtime spent pondering issues? extensive examination variouspossible sequences tests yield significant improvement simpler, heuristic choicetests? questions investigated here.words cost, expense, benefit used paper broadest sense,include factors quality life, addition economic monetary cost. Costdomain-specific quantified arbitrary units. assumed costs testsmeasured units benefits correct classification. Benefit treatednegative cost.paper introduces new algorithm cost-sensitive classification, called ICET(Inexpensive Classification Expensive Tests pronounced iced tea). ICET usesgenetic algorithm (Grefenstette, 1986) evolve population biases decision treeinduction algorithm (a modified version C4.5, Quinlan, 1992). fitness functiongenetic algorithm average cost classification using decision tree, includingcosts tests (features, measurements) costs classification errors. ICETfollowing features: (1) sensitive test costs. (2) sensitive classificationerror costs. (3) combines greedy search heuristic genetic search algorithm. (4)handle conditional costs, cost one test conditional whether second1995 National Research Council Canada. rights reserved. Published permission.fiT URNEYtest selected yet. (5) distinguishes tests immediate results testsdelayed results.problem cost-sensitive classification arises frequently. problem medicaldiagnosis (Nez, 1988, 1991), robotics (Tan & Schlimmer, 1989, 1990; Tan, 1993), industrial production processes (Verdenius, 1991), communication network troubleshooting(Lirov & Yue, 1991), machinery diagnosis (where main cost skilled labor), automatedtesting electronic equipment (where main cost time), many areas.several machine learning algorithms consider costs tests,EG2 (Nez, 1988, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX(Norton, 1989). also several algorithms consider costs classificationerrors (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon &Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al.,1994). However, little work considers costs together.good reasons considering costs tests costs classification errors. agent cannot rationally determine whether test performed withoutknowing costs correct incorrect classification. agent must balance costtest contribution test accurate classification. agent must also consider testing economically justified. often happens benefitstesting worth costs tests. means cost must assignedtests classification errors.Another limitation many existing cost-sensitive classification algorithms (EG2, CSID3) use greedy heuristics, select step whatever test contributesaccuracy least cost. sophisticated approach would evaluate interactions among tests sequence tests. test appears useful considered isolation,using greedy heuristic, may appear useful considered combinationtests. Past work demonstrated sophisticated algorithms superiorperformance (Tcheng et al., 1989; Ragavan & Rendell, 1993; Norton, 1989; Schaffer, 1993;Rymon, 1993; Seshu, 1989; Provost, 1994; Provost & Buchanan, press).Section 2 discusses decision tree natural form knowledge representationclassification expensive tests measure average cost classificationdecision tree. Section 3 introduces five algorithms examine here, C4.5(Quinlan, 1992), EG2 (Nez, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993),IDX (Norton, 1989), ICET. five algorithms evaluated empirically five realworld medical datasets. datasets discussed detail Appendix A. Section 4 presents three sets experiments. first set (Section 4.1) experiments examines baseline performance five algorithms five datasets establishes ICETperforms significantly better competitors given datasets. second set (Section 4.2) tests robustness ICET variety conditions shows ICETmaintains advantage. third set (Section 4.3) looks ICETs search bias spacediscovers way improve search. discuss related work future work Section 5. end summary learned research statementgeneral motivation type research.2. Cost-Sensitive Classificationsection first explains decision tree natural form knowledge representation classification expensive tests. discusses measure averagecost classification decision tree. method measuring average cost handles370fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONaspects problem typically ignored. method applied standardclassification decision tree, regardless tree generated. end discussionrelation cost accuracy.2.1Decision Trees Cost-Sensitive Classificationdecision trees used decision theory (Pearl, 1988) somewhat differentclassification decision trees typically used machine learning (Quinlan, 1992).refer decision trees paper, mean standard classification decisiontrees machine learning. claims make classification decision trees alsoapply decision theoretical decision trees, modification. full discussiondecision theoretical decision trees outside scope paper.decision test must based cost tests cost classification errors. test costs $10 maximum penalty classification error $5,clearly point test. hand, penalty classificationerror $10,000, test may quite worthwhile, even information content relatively low. Past work algorithms sensitive test costs (Nez, 1988, 1991;Tan, 1993; Norton, 1989) overlooked importance also considering cost classification errors.tests inexpensive, relative cost classification errors, may rationaltests (i.e., measure features; determine values attributes) seem possibly relevant. kind situation, convenient separate selection testsprocess making classification. First decide set tests relevant, focus problem learning classify case, using resultstests. common approach classification machine learning literature.Often paper focuses problem learning classify case, without mentiondecisions involved selecting set relevant tests.1tests expensive, relative cost classification errors, may suboptimal separate selection tests process making classification. mayable achieve much lower costs interleaving two. First choose test,examine test result. result test gives us information, use influence choice next test. point, decide cost testsjustified, stop testing make classification.selection tests interleaved classification way, decision treenatural form representation. root decision tree represents first testchoose. next level decision tree represents next test choose.decision tree explicitly shows outcome first test determines choicesecond test. leaf represents point decide stop testing make classification.Decision theory used define constitutes optimal decision tree, given (1)costs tests, (2) costs classification errors, (3) conditional probabilitiestest results, given sequences prior test results, (4) conditional probabilitiesclasses, given sequences test results. However, searching optimal tree infeasible(Pearl, 1988). ICET designed find good (but necessarily optimal) tree,good defined better competition (i.e., IDX, CS-ID3, EG2).1. papers like this. Decision tree induction algorithms C4.5 (Quinlan, 1992) automaticallyselect relevant tests. Aha Bankert (1994), among others, used sequential test selection proceduresconjunction supervised learning algorithm.371fiT URNEY2.2Calculating Average Cost Classificationsection, describe calculate average cost classification decisiontree, given set testing data. method described applied uniformly decision trees generated five algorithms examined (EG2, CS-ID3, IDX, C4.5,ICET). method assumes standard classification decision tree (such generatedC4.5); makes assumptions tree generated. purposemethod give plausible estimate average cost expected real-worldapplication decision tree.assume dataset split training set testing set.expected cost classification estimated average cost classification testing set. average cost classification calculated dividing total costwhole testing set number cases testing set. total cost includescosts tests costs classification errors. simplest case, assumespecify test costs simply listing test, paired corresponding cost.complex cases considered later section. assume specifycosts classification errors using classification cost matrix.Suppose c distinct classes. classification cost matrix c c matrix,element C i, j cost guessing case belongs class i, actuallybelongs class j. need assume constraints matrix, except costsfinite, real values. allow negative costs, interpreted benefits. However, experiments reported here, restricted attention classification costmatrices diagonal elements zero (we assume correct classificationcost) off-diagonal elements positive numbers. 2calculate cost particular case, follow path decision tree.add cost test chosen (i.e., test occurs path rootleaf). test appears twice, charge first occurrence test.example, one node path may say patient age less 10 years another nodemay say patient age 5 years, charge cost determining patients age. leaf tree specifies trees guess class case.Given actual class case, use cost matrix determine cost treesguess. cost added costs tests, determine total cost classificationcase.core method calculating average cost classification decision tree. two additional elements method, handling conditional testcosts delayed test results.allow cost test conditional choice prior tests. Specifically,consider case group tests shares common cost. example, set bloodtests shares common cost collecting blood patient. common costcharged once, decision made first blood test. chargecollecting blood second blood test, since may use blood collectedfirst blood test. Thus cost test group conditional whether anothermember group already chosen.Common costs appear frequently testing. example, diagnosis aircraftengine, group tests may share common cost removing engine plane2. restriction seems reasonable starting point exploring cost-sensitive classification. future work,investigate effects weakening restriction.372fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONinstalling test cell. semiconductor manufacturing, group tests may sharecommon cost reserving region silicon wafer special test structure. imagerecognition, group image processing algorithms may share common preprocessingalgorithm. examples show realistic assessment cost using decisiontree frequently need make allowances conditional test costs.often happens result test available immediately. example,medical doctor typically sends blood test laboratory gets result next day.allow test labelled either immediate delayed. test delayed, cannotuse outcome influence choice next test. example, blood testsdelayed, cannot allow outcome one blood test play role decisionsecond blood test. must make commitment (or doing) secondblood test know results first blood test.Delayed tests relatively common. example, many medical tests must shippedlaboratory analysis. gas turbine engine diagnosis, main fuel control frequently shipped specialized company diagnosis repair. classification problem requires multiple experts, one experts might immediately available.handle immediate tests decision tree described above. handle delayed testsfollows. follow path case root decision tree appropriateleaf. encounter node, anywhere along path, delayed test,committed performing tests subtree rooted node. Sincecannot make decision tests node conditional outcome testnode, must pledge pay tests might possibly need perform,point onwards decision tree.method handling delayed tests may seem bit puzzling first. difficultydecision tree combines method selecting tests method classifyingcases. tests delayed, forced proceed two phases. first phase,select tests. second phase, collect test results classify case. example,doctor collects blood patient sends blood laboratory. doctor must telllaboratory tests done blood. next day, doctor gets resultstests laboratory decides diagnosis patient. decisiontree naturally handle situation like this, selection tests isolatedclassification cases. method, first phase, doctor uses decisiontree select tests. long tests immediate, problem. soonfirst delayed test encountered, doctor must select tests might possiblyneeded second phase.3 is, doctor must select tests subtree rootedfirst delayed test. second phase, test results arrive next day,doctor information required go root tree leaf, makeclassification. doctor must pay tests subtree, even thoughtests along one branch subtree actually used. doctor knowadvance branch actually used, time necessary orderblood tests. laboratory blood tests naturally want doctor paytests ordered, even used making diagnosis.general, makes sense desired immediate testsdesired delayed tests, since outcome immediate test used influencedecision delayed test, vice versa. example, medical doctor question3. simplification situation real world. realistic treatment delayed tests oneareas future work (Section 5.2).373fiT URNEYpatient (questions immediate tests) deciding blood tests order (bloodtests delayed tests).4tests delayed (as BUPA data Appendix A.1),must decide advance (before see test results) tests performed.given decision tree, total cost tests cases. situationstype, problem minimizing cost simplifies problem choosing best subsetset available tests (Aha Bankert, 1994). sequential order testslonger important reducing cost.Let us consider simple example illustrate method. Table 1 shows test costsfour tests. Two tests immediate two delayed. two delayed testsshare common cost $2.00. two classes, 0 1. Table 2 shows classification cost matrix. Figure 1 shows decision tree. Table 3 traces path treeparticular case shows cost calculated. first step test roottree (test alpha). second step, encounter delayed test (delta), mustcalculate cost entire subtree rooted node. Note epsilon costs $8.00,since already selected delta, delta epsilon common cost. thirdstep, test epsilon, need pay, since already paid second step.fourth step, guess class case. Unfortunately, guess incorrectly,pay penalty $50.00.Table 1: Test costs simple example.TestGroupCostDelayed1alpha$5.002beta$10.003delta$7.00 first test group A,$5.00 otherwiseyes4epsilon$10.00 first test group A,$8.00 otherwiseyesTable 2: Classification costs simple example.Actual ClassGuess ClassCost00$0.0001$50.0010$50.0011$0.004. real world, many factors influence sequence tests, length delayprobability delayed test needed. ignore many factors pay attentionsimplified model presented here, makes sense desired immediate testsdesired delayed tests. know extent actually occurs real world. Onecomplication medical doctors industrialized countries directly affected costtests select. fact, fear law suits gives incentive order unnecessary tests.374fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONalpha < 3Fbeta > 6delta = 2F0F1beta < 5epsilon < 4F100F1Figure 1: Decision tree simple example.Table 3: Calculating cost particular case.StepActionResultCost1alphaalpha = 6$5.002deltadelta = 3$7.00 + $10.00 + $8.00 = $25.003epsilonepsilon = 2already paid, step #24guess class = 0actual class = 1$50.00total cost$80.00summary, section presents method estimating average cost usinggiven decision tree. decision tree standard classification decision tree;special assumptions made tree; matter tree generated.method requires (1) decision tree (Figure 1), (2) information calculation testcosts (Table 1), (3) classification cost matrix (Table 2), (4) set testing data (Table3). method (i) sensitive cost tests, (ii) sensitive cost classificationerrors, (iii) capable handling conditional test costs, (iv) capable handling delayedtests. experiments reported Section 4, method applied uniformly fivealgorithms.2.3Cost Accuracymethod calculating cost explicitly deal accuracy; however,handle accuracy special case. test cost set $0.00 tests classification cost matrix set positive constant value k guess class equalactual class j, set $0.00 equals j, average total cost usingdecision tree pk , p [0,1] frequency errors testing dataset375fiT URNEY100 ( 1 p ) percentage accuracy testing dataset. Thus linear relationship average total cost percentage accuracy, situation.generally, let C classification cost matrix cost x diagonal,C i, = x , cost diagonal, ( j ) ( C i, j = ) , x less y, x < .call type classification cost matrix simple classification cost matrix. costmatrix simple called complex classification cost matrix.5simple cost matrix test costs zero (equivalently, test costs ignored), minimizingcost exactly equivalent maximizing accuracy.follows algorithm sensitive misclassification error costsignores test costs (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974;Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press;Knoll et al., 1994) interesting complex cost matrix.simple cost matrix, algorithm CART (Breiman et al., 1984) sensitivemisclassification error cost advantage algorithm C4.5 (Quinlan, 1992)maximizes accuracy (assuming differences two algorithms negligible). experiments paper use simple cost matrix (the exceptionSection 4.2.3). Therefore focus comparison ICET algorithms sensitivetest cost (IDX, CS-ID3, EG2). future work, examine complex cost matricescompare ICET algorithms sensitive misclassification error cost.difficult find information costs misclassification errors medical practice, seems likely complex cost matrix appropriate simple costmatrix medical applications. paper focuses simple cost matrices because,research strategy, seems wise start simple cases attempt complex cases.Provost (Provost, 1994; Provost & Buchanan, press) combines accuracy classification error cost using following formula:score = accuracy B cost(1)formula, B arbitrary weights user set particular application. accuracy cost, defined Provost (Provost, 1994; Provost & Buchanan, press), represented using classification cost matrices. representaccuracy using simple cost matrix. interesting applications, cost represented complex cost matrix. Thus score weighted sum two classification costmatrices, means score classification cost matrix. showsequation (1) handled special case method presented here. lossinformation translation Provosts formula cost matrix. meancriteria represented costs. example criterion cannot represented cost stability (Turney, press).3. Algorithmssection discusses algorithms used paper: C4.5 (Quinlan, 1992), EG2 (Nez,1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX (Norton, 1989), ICET.5. occasionally say simple cost matrix complex cost matrix. cause confusion,since test costs represented matrix.376fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION3.1C4.5C4.5 (Quinlan, 1992) builds decision tree using standard TDIDT (top-down inductiondecision trees) approach, recursively partitioning data smaller subsets, basedvalue attribute. step construction decision tree, C4.5 selectsattribute maximizes information gain ratio. induced decision tree prunedusing pessimistic error estimation (Quinlan, 1992). several parametersadjusted alter behavior C4.5. experiments C4.5, used default settings parameters. used C4.5 source code distributed (Quinlan,1992).3.2EG2EG2 (Nez, 1991) TDIDT algorithm uses Information Cost Function (ICF)(Nez, 1991) selection attributes. ICF selects attributes based information gain cost. implemented EG2 modifying C4.5 source codeICF used instead information gain ratio.ICF i-th attribute, ICF , defined follows:62 1ICF = ------------------------( Ci + 1)0 1(2)equation, information gain associated i-th attribute given stageconstruction decision tree C cost measuring i-th attribute. C4.5selects attribute maximizes information gain ratio, functioninformation gain . modified C4.5 selects attribute maximizes ICF .parameter adjusts strength bias towards lower cost attributes.= 0 , cost ignored selection ICF equivalent selection .= 1 , ICF strongly biased cost. Ideally, would selected way sensitive classification error cost (this done ICET see Section 3.5). Nez (1991)suggest principled way setting . experiments EG2, set 1.words, used following selection measure:2 1----------------Ci + 1(3)addition sensitivity cost tests, EG2 generalizes attributes using ISAtree (a generalization hierarchy). implement aspect EG2, sincerelevant experiments reported here.3.3CS-ID3CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993) TDIDT algorithm selectsattribute maximizes following heuristic function:2( )--------------Ci(4)6. inverse ICF, defined Nez (1991). Nez minimizes criterion. facilitate comparisonalgorithms, use equation (2). criterion intended maximized.377fiT URNEYimplemented CS-ID3 modifying C4.5 selects attribute maximizes(4).CS-ID3 uses lazy evaluation strategy. constructs part decision treeclassifies current case. implement aspect CS-ID3, sincerelevant experiments reported here.3.4IDXIDX (Norton, 1989) TDIDT algorithm selects attribute maximizes following heuristic function:------Ci(5)implemented IDX modifying C4.5 selects attribute maximizes (5).C4.5 uses greedy search strategy chooses step attribute highestinformation gain ratio. IDX uses lookahead strategy looks n tests ahead, nparameter may set user. implement aspect IDX. lookahead strategy would perhaps make IDX competitive ICET, would also complicate comparison heuristic function (5) heuristics (3) (4) used EG2CS-ID3.3.5ICETICET hybrid genetic algorithm decision tree induction algorithm. geneticalgorithm evolves population biases decision tree induction algorithm.genetic algorithm use GENESIS (Grefenstette, 1986).7 decision tree inductionalgorithm C4.5 (Quinlan, 1992), modified use ICF. is, decision tree inductionalgorithm EG2, implemented described Section 3.2.ICET uses two-tiered search strategy. bottom tier, EG2 performs greedysearch space decision trees, using standard TDIDT strategy. toptier, GENESIS performs genetic search space biases. biases usedmodify behavior EG2. words, GENESIS controls EG2s preference onetype decision tree another.ICET use EG2 way designed used. n costs, C , usedEG2s attribute selection function, treated ICET bias parameters, costs.is, ICET manipulates bias EG2 adjusting parameters, C . ICET, valuesbias parameters, C , direct connection actual costs tests.Genetic algorithms inspired biological evolution. individuals evolvedGENESIS strings bits. GENESIS begins population randomly generatedindividuals (bit strings) measures fitness individual. ICET,individual (a bit string) represents bias EG2. individual evaluated running EG2data, using bias given individual. fitness individual average cost classification decision tree generated EG2. next generation, population replaced new individuals. new individuals generatedprevious generation, using mutation crossover (sex). fittest individualsfirst generation offspring second generation. fixed number7. used GENESIS Version 5.0, available URL ftp://ftp.aic.nrl.navy.mil/pub/galist/src/ga/genesis.tar.Z ftp://alife.santafe.edu/pub/USER-AREA/EC/GA/src/gensis-5.0.tar.gz.378fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONgenerations, ICET halts output decision tree determined fittest individual. Figure 2 gives sketch ICET algorithm.GENESISfittestgenetic algorithmdecision treepopulationbiasesEG2datafitnessdecision treeclassifierfunctionEG2decision treeclassifierEG2decision treeclassifierFigure 2: sketch ICET algorithm.GENESIS several parameters used alter performance. parameters used listed Table 4. essentially default parameter settings(Grefenstette, 1986). used population size 50 individuals 1,000 trials,results 20 generations. individual population consists string n + 2 numbers, n number attributes (tests) given dataset. n + 2 numbersrepresented binary format, using Gray code.8 binary string used biasEG2. first n numbers string treated n costs, C , used ICF(equation (2)). first n numbers range 1 10,000 coded 12 binary digits each. last two numbers string used set CF. parameterused ICF. parameter CF used C4.5 control level pruning decisiontree. last two numbers coded 8 binary digits each. ranges 0 (costignored) 1 (maximum sensitivity cost) CF ranges 1 (high pruning) 100 (lowpruning). Thus individual string 12n + 16 bits.trial individual consists running EG2 (implemented modificationC4.5) given training dataset, using numbers specified binary string set C( = 1, , n ), , CF. training dataset randomly split two equal-sized subsets( 1 odd-sized training sets), sub-training set sub-testing set. different randomsplit used trial, outcome trial stochastic. cannot assumeidentical individuals yield identical outcomes, every individual must evaluated.means duplicate individuals population, slightly different fitnessscores. measure fitness individual average cost classificationsub-testing set, using decision tree generated sub-training set. aver8. Gray code binary code designed avoid Hamming cliffs. standard binary code, 7 represented 0111 8 represented 1000. numbers adjacent, yet Hamming distance0111 1000 large. Gray code, adjacent numbers represented binary codes smallHamming distances. tends improve performance genetic algorithm (Grefenstette, 1986).379fiT URNEYTable 4: Parameter settings GENESIS.ParameterSettingExperiments1Total Trials1000Population Size50Structure Length12n + 16Crossover Rate0.6Mutation Rate0.001Generation Gap1.0Scaling Window5Report Interval100Structures Saved1Max Gens w/o Eval2Dump Interval0Dumps Saved0OptionsacefglRandom Seed123456789Rank Min0.75age cost measured described Section 2.2. 1,000 trials, fit (lowest cost)individual used bias EG2 whole training set input. resultingdecision tree output ICET given training dataset.9n costs (bias parameters), C , used ICF, directly related true costsattributes. 50 individuals first generation generated randomly, initialvalues C relation true costs. 20 generations, values C mayrelation true costs, simple relationship. valuesC appropriately thought biases costs. Thus GENESIS searchingbias space biases C4.5 result decision trees low average cost.biases C range 1 10,000. bias C greater 9,000, i-thattribute ignored. is, i-th attribute available C4.5 include decision tree, even might maximize ICF . threshold 9,000 arbitrarily chosen.attempt optimize value experimentation.chose use EG2 ICET, rather IDX CS-ID3, EG2 parameter , gives GENESIS greater control bias EG2. ICF partly baseddata (via information gain, ) partly based bias (via pseudo9. 50/50 partition sub-training sub-testing sets could mean ICET may work well smalldatasets. smallest dataset five examine Hepatitis dataset, 155 cases.training sets 103 cases testing sets 52 cases. sub-training sub-testing sets 51 52cases. see Figure 3 ICET performed slightly better algorithms dataset(the difference significant).380fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONcost, C ). exact mix data bias controlled varying . Otherwise,reason prefer EG2 IDX CS-ID3, could easily used instead EG2.treatment delayed tests conditional test costs hard-wired EG2.built fitness function used GENESIS, average cost classification (measured described Section 2). makes relatively simple extend ICET handlepragmatic constraints decision trees.effect, GENESIS lies EG2 costs tests. lies improveperformance EG2? EG2 hill-climbing algorithm get trapped local optimum. greedy algorithm looks one test ahead builds decision tree.looks one step ahead, EG2 suffers horizon effect. term takenliterature chess playing programs. Suppose chess playing programfixed three-move lookahead depth finds loose queen three moves,follows certain branch game tree. may alternate branch program first sacrifices pawn loses queen four moves. lossqueen three-move horizon, program may foolishly decide sacrifice pawn.One move later, faced loss queen. Analogously, EG2 may tryavoid certain expensive test selecting less expensive test. One test later,faced expensive test. exhausted cheaper tests, mayforced expensive test, spite efforts avoid test. GENESIS preventshort-sighted behavior telling lies EG2. GENESIS exaggerate costcheap tests understate cost expensive test. Based past trials, GENESISfind lies yield best performance EG2.ICET, learning (local search EG2) evolution (in GENESIS) interact. commonform hybrid genetic algorithm uses local search improve individuals population(Schaffer et al., 1992). improvements coded strings representindividuals. form Lamarckian evolution. ICET, improvements due EG2coded strings. However, improvements accelerate evolution altering fitness landscape. phenomenon (and phenomena result formhybrid) known Baldwin effect (Baldwin, 1896; Morgan, 1896; Waddington, 1942;Maynard Smith, 1987; Hinton & Nowlan, 1987; Ackley & Littman, 1991; Whitley & Gruau,1993; Whitley et al., 1994; Anderson, press). Baldwin effect may explain muchsuccess ICET.4. Experimentssection describes experiments performed five datasets, taken Irvine collection (Murphy & Aha, 1994). five datasets described detailAppendix A. five datasets involve medical problems. test costs based information Ontario Ministry Health (1992). main purpose experimentsgain insight behavior ICET. cost-sensitive algorithms, EG2, CS-ID3,IDX, included mainly benchmarks evaluating ICET. C4.5 also includedbenchmark, illustrate behavior algorithm makes use cost information.main conclusion experiments ICET performs significantly bettercompetitors, wide range conditions. access Irvine collectioninformation Appendix A, possible researchers duplicateresults reported here.Medical datasets frequently missing values.10 conjecture many missing values medical datasets missing doctor involved generating dataset381fiT URNEYdecided particular test economically justified particular patient. Thusmay information content fact certain value missing. maymany reasons missing values cost tests. example, perhapsdoctor forgot order test perhaps patient failed show test. However,seems likely often information content fact value missing.experiments, information content hidden learning algorithms,since using (at least testing sets) would form cheating. Two fivedatasets selected missing data. avoid accusations cheating, decidedpreprocess datasets data presented algorithms missing values.preprocessing described Appendices A.2 A.3.Note ICET capable handling missing values without preprocessing inherits ability C4.5 component. preprocessed data avoid accusationscheating, ICET requires preprocessed data.experiments, dataset randomly split 10 pairs training testingsets. training set consisted two thirds dataset testing set consistedremaining one third. 10 pairs used experiments, order facilitatecomparison results across experiments.three groups experiments. first group experiments examines baseline performance algorithms. second group considers robust ICETvariety conditions. final group looks ICET searches bias space.4.1Baseline Performancesection examines baseline performance algorithms. Section 4.1.1, lookaverage cost classification five algorithms five datasets. Averagedacross five datasets, ICET lowest average cost. Section 4.1.2, study testexpenditures error rates functions penalty misclassification errors.five algorithms studied here, ICET adjusts test expenditures error rates functions penalty misclassification errors. four algorithms ignore penaltymisclassification errors. ICET behaves one would expect, increasing test expendituresdecreasing error rates penalty misclassification errors rises. Section 4.1.3,examine execution time algorithms. ICET requires 23 minutes averagesingle-processor Sparc 10. Since ICET inherently parallel, significant roomspeed increase parallel machine.4.1.1AVERAGE COST CLASSIFICATIONexperiment presented establishes baseline performance five algorithms.hypothesis ICET will, average, perform better four algorithms. classification cost matrix set positive constant value k guessclass equal actual class j, set $0.00 equals j. experimented seven settings k, $10, $50, $100, $500, $1000, $5000, $10000.Initially, used average cost classification performance measure,found three problems using average cost classification comparefive algorithms. First, differences costs among algorithms become relatively10. survey 54 datasets Irvine collection (URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/SUMMARY-TABLE) indicates 85% medical datasets (17 20) missing values,24% (8 34) non-medical datasets missing values.382fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONsmall penalty classification errors increases. makes difficult seealgorithm best. Second, difficult combine results five datasets fairmanner.11 fair average five datasets together, since test costs different scales (see Appendix A). test costs Heart Disease dataset, example,substantially larger test costs four datasets. Third, difficult combine average costs different values k fair manner, since weight givensituations k large situations small.address concerns, decided normalize average cost classification.normalized average cost dividing standard cost. Let f [0,1] frequency class given dataset. is, f fraction cases datasetbelong class i. calculate f using entire dataset, training set. Let C i, jcost guessing case belongs class i, actually belongs class j. Lettotal cost possible tests. standard cost defined follows:+ min (1 f i) max C i, j(6)i, jdecompose formula (6) three components:(7)min (1 f i)(8)max C i, j(9)i, jmay think (7) upper bound test expenditures, (8) upper bound errorrate, (9) upper bound penalty errors. standard cost always lessmaximum possible cost, given following formula:+ max C i, ji, j(10)point (8) really upper bound error rate, since possiblewrong every guess. However, experiments suggest standard cost betternormalization, since realistic (tighter) upper bound average cost.experiments, average cost never went standard cost, although occasionally came close.Figure 3 shows result using formula (6) normalize average cost classification. plots, x axis value k axis average cost classificationpercentage standard cost classification. see that, average (the sixth plotFigure 3), ICET lowest classification cost. one dataset ICETperform particularly well Heart Disease dataset (we discuss later, Sections 4.3.24.3.3).come single number characterizes performance algorithm,averaged numbers sixth plot Figure 3.12 calculated 95% confidenceregions averages, using standard deviations across 10 random splits11. want combine results order summarize performance algorithms five datasets.analogous comparing students calculating GPA (Grade Point Average), studentscourses algorithms datasets.12. Like GPA, datasets (courses) weight. However, unlike GPA, algorithms (students) applied datasets (have taken courses). Thus approach perhaps fairalgorithms GPA students.383fiT URNEYBUPA Liver DiseaseHeart Disease100Average % Standard CostAverage % Standard Cost1008060402001080604020010010001000010Cost Misclassification ErrorHepatitis PrognosisPima Indians DiabetesAverage % Standard CostAverage % Standard Cost604020080604020010010001000010Cost Misclassification Error100100010000Cost Misclassification ErrorThyroid DiseaseAverage Five Datasets100Average % Standard Cost100Average % Standard Cost1000010080806040200101000Cost Misclassification Error1001010080604020010010001000010Cost Misclassification Error100100010000Cost Misclassification ErrorICET:EG2:CS-ID3:IDX:C4.5:Figure 3: Average cost classification percentage standard costclassification baseline experiment.datasets. result shown Table 5.Table 5 shows averages first three misclassification error costs alone ($10,$50, $100), addition showing averages seven misclassification error costs($10 $10000). two averages (the two columns Table 5), based two groupsdata, address following argument: penalty misclassification errors increases,cost tests becomes relatively insignificant. high misclassification errorcost, test cost effectively zero, task becomes simply maximize accuracy.384fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONTable 5: Average percentage standard cost baseline experiment.AlgorithmAverage Classification Cost Percentage Standard95% ConfidenceMisclassification Error Costs10.00 10,000.00Misclassification Error Costs10.00 100.00ICET49 729 7EG258 543 3CS-ID361 649 4IDX58 543 3C4.577 582 4see Figure 3, gap C4.5 (which maximizes accuracy) algorithms becomes smaller cost misclassification error increases. Therefore benefitsensitivity test cost decreases cost misclassification error increases. couldargued one would bother algorithm sensitive test cost testsrelatively expensive, compared cost misclassification errors. Thus realistic measure performance examine average cost classification costtests order magnitude cost misclassification errors ($10 $100).Table 5 shows averages.conclusion, based Table 5, ICET performs significantly betterfour algorithms cost tests order magnitude costmisclassification errors ($10, $50, $100). cost misclassification errors dominates test costs, ICET still performs better competition, difference lesssignificant. three cost-sensitive algorithms (EG2, CS-ID3, IDX) perform significantly better C4.5 (which ignores cost). performance EG2 IDX indistinguishable, CS-ID3 appears consistently costly EG2 IDX.4.1.2TEST EXPENDITURES ERROR RATES FUNCTIONS PENALTY ERRORSargued Section 2 expenditures tests conditional penaltymisclassification errors. Therefore ICET designed sensitive cost testscost classification errors. leads us hypothesis ICET tends spendtests penalty misclassification errors increases. also expecterror rate ICET decrease test expenditures increase. two hypothesesconfirmed Figure 4. plots, x axis value k axis (1) average expenditure tests, expressed percentage maximum possible expendituretests, , (2) average percent error rate. average (the sixth plot Figure 4), testexpenditures rise error rate falls penalty classification errors increases.minor deviations trend, since ICET guess value test(in terms reduced error rate), based sees training dataset. testingdataset may always support guess. Note plots four algorithms, corresponding plots ICET Figure 4, would straight horizontal lines, since fouralgorithms ignore cost misclassification error. generate decision treesevery possible misclassification error cost.385fiT URNEY80606040402020001000100003040202010Average % Maximum Test Expenditures302020100Average % Error RateAverage % Maximum Test Expenditures4001000100008060404020200100100100010000Cost Misclassification ErrorAverage Five Datasets80Average % Maximum Test ExpendituresThyroid Disease1086064042020Average % Error RateAverage % Maximum Test Expenditures1000060Cost Misclassification Error01001000Pima Indians Diabetes40100100Cost Misclassification ErrorHepatitis Prognosis50100100Cost Misclassification Error104060Average % Error Rate10050100010000Cost Misclassification Error805040603040202010010Average % Error Rate1080Average % Error RateAverage % Maximum Test ExpendituresHeart Disease80Average % Error RateAverage % Maximum Test ExpendituresBUPA Liver Disease1000100100010000Cost Misclassification Error% Test Expenditures:% Error Rate:Figure 4: Average test expenditures average error ratefunction misclassification error cost.4.1.3EXECUTION TIMEessence, ICET works invoking C4.5 1000 times (Section 3.5). Fortunately, Quinlans(1992) implementation C4.5 quite fast. Table 6 shows run-times algorithms,using single-processor Sun Sparc 10. One full experiment takes one week (roughly23 minutes average run, multiplied 5 datasets, multiplied 10 random splits, multiplied 7 misclassification error costs equals one week). Since genetic algorithmseasily executed parallel, substantial room speed increase parallelmachine. generation consists 50 individuals, could evaluated parallel,reducing average run-time half minute.4.2Robustness ICETgroup experiments considers robust ICET variety conditions.section considers different variation operating environment ICET. ICET386fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONTable 6: Elapsed run-time five algorithms.AlgorithmAverage Elapsed Run-Time Dataset Minutes:SecondsBUPAHeartHepatitisPimaThyroidAverageICET15:4313:1410:2928:1945:2522:38EG20:10:10:10:30:30:2CS-ID30:10:10:10:30:30:2IDX0:10:10:10:30:30:2C4.50:20:10:10:40:30:2algorithm modified. Section 4.2.1, alter environment labellingtests immediate. Section 4.2.2, recognize shared costs, discountgroup tests common cost. Section 4.2.3, experiment complex classification cost matrices, different types errors different costs. Section 4.2.4,examine happens ICET trained certain penalty misclassificationerrors, tested different penalty. four experiments, find ICET continues perform well.4.2.1TESTS IMMEDIATEcritic might object previous experiments show ICET superioralgorithms due sensitivity test costs classification error costs. PerhapsICET superior simply handle delayed tests, algorithms treattests immediate.13 is, method estimating average classification cost(Section 2.2) biased favor ICET (since ICET uses method fitness function)algorithms. experiment, labelled tests immediate. Otherwise, nothing changed baseline experiments. Table 7 summarizes resultsexperiment. ICET still performs well, although advantage algorithmsdecreased slightly. Sensitivity delayed tests part explanation ICETs performance, whole story.4.2.2GROUP DISCOUNTSAnother hypothesis ICET superior simply handle groups testsshare common cost. experiment, eliminated group discounts tests sharecommon cost. is, test costs conditional prior tests. Otherwise, nothingchanged baseline experiments. Table 8 summarizes results experiment.ICET maintains advantage algorithms.4.2.3COMPLEX CLASSIFICATION COST MATRICESfar, used simple classification cost matrices, penalty classification error types error. assumption inherent ICET.13. algorithms cannot currently handle delayed tests, possible alterway, handle delayed tests. comment also extends groups tests share commoncost. ICET might viewed alteration EG2 enables EG2 handle delayed tests commoncosts.387fiT URNEYTable 7: Average percentage standard cost no-delay experiment.AlgorithmAverage Classification Cost Percentage Standard95% ConfidenceMisclassification Error Costs10.00 10,000.00Misclassification Error Costs10.00 100.00ICET47 628 4EG254 436 2CS-ID354 539 3IDX54 436 2C4.564 659 4Table 8: Average percentage standard cost no-discount experiment.AlgorithmAverage Classification Cost Percentage Standard95% ConfidenceMisclassification Error Costs10.00 10,000.00Misclassification Error Costs10.00 100.00ICET46 625 5EG256 542 3CS-ID359 548 4IDX56 542 3C4.575 580 4element classification cost matrix different value. experiment,explore ICETs behavior classification cost matrix complex.use term positive error refer false positive diagnosis, occurspatient diagnosed sick, patient actually healthy. Conversely, termnegative error refers false negative diagnosis, occurs patient diagnosed healthy, actually sick. term positive error cost costassigned positive errors, negative error cost cost assigned negativeerrors. See Appendix examples. interested ICETs behavior rationegative positive error cost varied. Table 9 shows ratios examined.Figure 5 shows performance five algorithms ratio.hypothesis difference performance ICET algorithms would increase move away middles plots, ratio 1.0,since algorithms mechanism deal complex classification cost;designed implicit assumption simple classification cost matrices. fact,Figure 5 shows difference tends decrease move away middles.pronounced right-hand sides plots. ratio 8.0 (theextreme right-hand sides plots), advantage using ICET. ratio0.125 (the extreme left-hand sides plots), still advantage using ICET.388fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONBUPA Liver DiseaseHeart Disease100Average % Standard CostAverage % Standard Cost1008060402000.18060402001.010.00.1Ratio Negative Positive Error CostHepatitis PrognosisAverage % Standard CostAverage % Standard Cost1008060402008060402001.010.00.1Ratio Negative Positive Error CostThyroid Disease10.0Average Five Datasets100Average % Standard CostAverage % Standard Cost1.0Ratio Negative Positive Error Cost1008060402000.110.0Pima Indians Diabetes1000.11.0Ratio Negative Positive Error Cost8060402001.010.00.1Ratio Negative Positive Error Cost1.010.0Ratio Negative Positive Error CostICET:EG2:CS-ID3:IDX:C4.5:Figure 5: Average cost classification percentage standard costclassification, complex classification cost matrices.interpretation plots complicated fact gap algorithms tends decrease penalty classification errors increases (as seeFigure 3 retrospect, held sum negative error cost positive error cost constant value, varied ratio). However, clearlyasymmetry plots, expected symmetrical vertical line centered1.0 x axis. plots close symmetrical algorithms,asymmetrical ICET. also apparent Table 10, focuses comparisonperformance ICET EG2, averaged across five datasets (see sixth plotFigure 5). suggests difficult reduce negative errors (on right-handsides plots, negative errors weight) reduce positive errors (on389fiT URNEYTable 9: Actual error costs ratio negative positive error cost.Ratio NegativePositive Error CostNegativeError CostPositiveError Cost0.125504000.25502000.5501001.050502.0100504.0200508.040050Table 10: Comparison ICET EG2various ratios negative positive error cost.AlgorithmAverage Classification Cost Percentage Standard95% Confidence, RatioNegative Positive Error Cost Varied0.1250.250.51.02.04.08.0ICET25 1025 829 629 434 639 639 6EG239 540 441 444 342 341 440 5ICET/EG2 (as %)64637166819598left-hand sides, positive errors weight). is, easier avoid false positive diagnoses (a patient diagnosed sick, patient actually healthy)avoid false negative diagnoses (a patient diagnosed healthy, actuallysick). unfortunate, since false negative diagnoses usually carry heavier penalty,real-life. Preliminary investigation suggests false negative diagnoses harder avoidsick class usually less frequent healthy class, makessick class harder learn.4.2.4POORLY ESTIMATED CLASSIFICATION COSTbelieve advantage ICET sensitive test costs classification error costs. However, might argued difficult calculate cost classification errors many real-world applications. Thus possible algorithmignores cost classification errors (e.g., EG2, CS-ID3, IDX) may robustuseful algorithm sensitive classification errors (e.g., ICET). addresspossibility, examine happens ICET trained certain penalty classification errors, tested different penalty.hypothesis ICET would robust reasonable differencespenalty training penalty testing. Table 11 shows happensICET trained penalty $100 classification errors, tested penalties390fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONTable 11: Performance training set classification error cost $100.AlgorithmAverage Classification Cost PercentageStandard 95% Confidence, Testing SetClassification Error Cost of:$50$100$500ICET33 1041 1062 9EG244 349 463 6CS-ID349 554 665 7IDX43 349 463 6C4.582 582 578 7$50, $100, $500. see ICET best performance five algorithms,although edge quite slight case penalty $500 testing.also examined happens (1) ICET trained penalty $500tested penalties $100, $500, $1,000 (2) ICET trained penalty$1,000 tested penalties $500, $1,000, $5,000. results show essentiallypattern Table 11: ICET relatively robust differences trainingtesting penalties, least penalties order magnitude. suggests ICET applicable even situations reliability estimatecost classification errors dubious.penalty errors testing set $100, ICET works best penaltyerrors training set also $100. penalty errors testing set$500, ICET works best penalty errors training set also $500.penalty errors testing set $1,000, ICET works best penalty errorstraining set $500. suggests might advantage situationsunderestimating penalty errors training. other, words ICET maytendency overestimate benefits tests (this likely due overfitting trainingdata).4.3Searching Bias Spacefinal group experiments analyzes ICETs method searching bias space. Section4.3.1 studies roles mutation crossover operators. appears crossovermildly beneficial, compared pure mutation. Section 4.3.2 considers happensICET constrained search binary bias space, instead real bias space. constraint actually improves performance ICET. hypothesized improvementdue hidden advantage searching binary bias space: searching binarybias space, ICET direct access true costs tests. However, advantageavailable searching real bias space, initial population biases seededtrue costs tests. Section 4.3.3 shows seeding improves performance ICET.4.3.1CROSSOVER VERSUS MUTATIONPast work shown genetic algorithm crossover performs better geneticalgorithm mutation alone (Grefenstette et al., 1990; Wilson, 1987). section391fiT URNEYattempts test hypothesis crossover improves performance ICET. testhypothesis, sufficient merely set crossover rate zero. Since crossoverrandomizing effect, similar mutation, must also increase mutation rate, compensate loss crossover (Wilson, 1987; Spears, 1992).difficult analytically calculate increase mutation rate requiredcompensate loss crossover (Spears, 1992). Therefore experimentally testedthree different mutation settings.14 results summarized Table 12. crossover rate set zero, best mutation rate 0.10. misclassification error costs$10 $10,000, performance ICET without crossover good performance ICET crossover, difference statistically significant. However,comparison entirely fair crossover, since made attempt optimizecrossover rate (we simply used default value). results suggest crossovermildly beneficial, prove pure mutation inferior.Table 12: Average percentage standard cost mutation experiment.Average Classification Cost PercentageStandard 95% ConfidenceICETCrossoverRateMutationRateMisclassificationError Costs10.00 10,000.00MisclassificationError Costs10.00 100.000.60.00149 729 70.00.0551 832 90.00.1050 829 80.00.1551 830 94.3.2SEARCH BINARY SPACEICET searches biases space n + 2 real numbers. Inspired Aha Bankert(1994), decided see would happen ICET restricted space nbinary numbers 2 real numbers. modified ICET EG2 given true costtest, instead pseudo-cost bias. conditional test costs, used nodiscount cost (see Section 4.2.2). n binary digits used exclude include test.EG2 allowed use excluded tests decision trees generated.precise, let B 1, , B n n binary numbers let C 1, , C n n real numbers. experiment, set C true cost i-th test. experiment, GENESIS change C . is, C constant given test given dataset. Instead,GENESIS manipulates value B i. binary number B used determinewhether EG2 allowed use test decision tree. B = 0 , EG2 alloweduse i-th test (the i-th attribute). Otherwise, B = 1 , EG2 allowed use i-thtest. EG2 uses ICF equation usual, true costs C . Thus modified versionICET searching binary bias space instead real bias space.hypothesis ICET would perform better searching real bias space14. three experiments took one week Sparc 10, tried three settingsmutation rate.392fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONsearching binary bias space. Table 13 shows hypothesis confirmed. appears better search binary bias space, rather real bias space.However, differences statistically significant.Table 13: Average percentage standard cost binary search experiment.AlgorithmAverage Classification Cost PercentageStandard 95% ConfidenceMisclassificationError Costs10.00 10,000.00MisclassificationError Costs10.00 100.00ICET Binary Space48 626 5ICET Real Space49 729 7EG258 543 3CS-ID361 649 4IDX58 543 3C4.577 582 4searched binary space, set C true cost i-th test. GENESISmanipulated B instead C . searched real space, GENESIS set C whatevervalue found useful attempt optimize fitness. hypothesized givesadvantage binary space search real space search. Binary space search directaccess true costs tests, real space search learns true coststests indirectly, feedback gets fitness function.examined experiment detail, found ICET well HeartDisease dataset searching binary bias space, although poorlysearching real bias space (see Section 4.1.1). hypothesized ICET,searching real space, suffered lack direct access true costsapplied Heart Disease dataset. hypotheses tested next experiment.4.3.3SEEDED POPULATIONexperiment, returned searching real bias space, seeded initial population biases true test costs. gave ICET direct access true test costs.conditional test costs, used no-discount cost (see Section 4.2.2). baselineexperiment (Section 4.1), initial population consists 50 randomly generated strings,representing n + 2 real numbers. experiment, initial population consists 49 randomly generated strings one manually generated string. manually generatedstring, first n numbers true test costs. last two numbers set 1.0 (for) 25 (for CF). string exactly bias EG2, implemented (Section3.2).hypotheses (1) ICET would perform better (on average) initialpopulation seeded purely random, (2) ICET would perform better (onaverage) searching real space seeded population searching binaryspace,15 (3) ICET would perform better Heart Disease dataset ini393fiT URNEYtial population seeded purely random. Table 14 appears support firsttwo hypotheses. Figure 6 appears support third hypothesis. However, resultsstatistically significant.16Table 14: Average percentage standard cost seeded populationexperiment.AlgorithmAverage Classification Cost PercentageStandard 95% ConfidenceMisclassificationError Costs10.00 10,000.00MisclassificationError Costs10.00 100.00ICET SeededSearch Real Space46 625 5ICET UnseededSearch Real Space49 729 7ICET UnseededSearch Binary Space48 626 5EG258 543 3CS-ID361 649 4IDX58 543 3C4.577 582 4experiment raises interesting questions: seeding populationbuilt ICET algorithm? seed whole population true costs, perturbed random noise? Perhaps right approach, prefer modifyICF (equation (2)), device GENESIS controls decision tree induction.could alter equation contains true costs bias parameters.17seems make sense current approach, deprives EG2 directaccess true costs. discuss ideas modifying equationSection 5.2.Incidentally, experiment lets us answer following question: geneticsearch bias space anything useful? start true costs tests reasonable values parameters CF, much improvement getgenetic search? experiment, seeded population individual represents exactly bias EG2 (the first n numbers true test costs last two numbers 1.0 25 CF). Therefore determine value genetic searchcomparing EG2 ICET. ICET starts bias EG2 (as seed first genera15. Note make sense seed binary space search, since already direct access truecosts.16. would need go current 10 trials (10 random splits data) 40 trials makeresults significant. experiments reported took total 63 days continuous computation SunSparc 10, 40 trials would require six months.17. idea suggested conversation K. De Jong.394fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONHeart Disease1008080Average % Standard CostAverage % Standard CostBUPA Liver Disease100604020010604020010010001000010Cost Misclassification Error808060402004020010010001000010100100010000Cost Misclassification ErrorThyroid DiseaseAverage Five Datasets1001008080Average % Standard CostAverage % Standard Cost1000060Cost Misclassification Error6040200101000Pima Indians Diabetes100Average % Standard CostAverage % Standard CostHepatitis Prognosis10010100Cost Misclassification Error604020010010001000010Cost Misclassification Error100100010000Cost Misclassification ErrorICET:EG2:CS-ID3:IDX:C4.5:Figure 6: Average cost classification percentage standard costclassification seeded population experiment.tion) attempts improve bias. score EG2 Table 14 shows valuebias built EG2. score ICET Table 14 shows genetic search bias spaceimprove built-in bias EG2. cost misclassification errorsorder magnitude test costs ($10 $100), EG2 averages 43% standard cost,ICET averages 25% standard cost. cost misclassification errorsranges $10 $10,000, EG2 averages 58% standard cost, ICET averages46% standard cost. differences significant 95% confidence. makes clear genetic search adding value.395fiT URNEY5. Discussionsection compares ICET related work outlines possibilities future work.5.1Related Workseveral algorithms sensitive test costs (Nez, 1988, 1991; Tan &Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989). discussed, main limitation algorithms consider cost classification errors. cannot rationally determine whether test performed know costtest cost classification errors.also several algorithms sensitive classification error costs (Breimanet al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al., 1994). Nonealgorithms consider cost tests. Therefore focus complex classificationcost matrices, since, tests cost classification error matrix simple,problem reduces maximizing accuracy.FIS system (Pipitone et al., 1991) attempts find decision tree minimizesaverage total cost tests required achieve certain level accuracy. approachcould implemented ICET altering fitness function. main distinctionFIS (Pipitone et al., 1991) ICET FIS learn data. informationgain test estimated using qualitative causal model, instead training cases. Qualitative causal models elicited domain experts, using special knowledge acquisitiontool. training data available, ICET used avoid need knowledgeacquisition. Otherwise, ICET applicable FIS approach suitable.Another feature ICET perform purely greedy search. Severalauthors proposed non-greedy classification algorithms (Tcheng et al., 1989; Ragavan &Rendell, 1993; Norton, 1989; Schaffer, 1993; Rymon, 1993; Seshu, 1989). general,results show advantage sophisticated search procedures. ICETdifferent algorithms uses genetic algorithm applied minimizing test costs classification error costs.ICET uses two-tiered search strategy. bottom tier, EG2 performs greedy searchspace classifiers. second tier, GENESIS performs non-greedy searchspace biases. idea two-tiered search strategy (where first tiersearch classifier space second tier search bias space) also appears (Provost,1994; Provost & Buchanan, press; Aha & Bankert, 1994; Schaffer, 1993). work goesbeyond Aha Bankert (1994) considering search real bias space, rather searchbinary space. work fits general framework Provost Buchanan (inpress), differs many details. example, method calculating cost specialcase (Section 2.3).researchers applied genetic algorithms classification problems. example, Frey Slate (1991) applied genetic algorithm (in particular, learning classifier system (LCS)) letter recognition. However, Fogarty (1992) obtained higher accuracy usingsimple nearest neighbor algorithm. recent applications genetic algorithms classification successful (De Jong et al., 1993). However, work describedfirst application genetic algorithms problem cost-sensitive classification.mentioned Section 2.1 decision theory may used define optimal solution problem cost-sensitive classification. However, searching optimal solution computationally infeasible (Pearl, 1988). attempted take decision theoretic396fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONapproach problem implementing AO* algorithm (Pearl, 1984) designingheuristic evaluation function speed AO* search (Lirov & Yue, 1991).unable make approach execute fast enough practical.also attempted apply genetic programming (Koza, 1993) problem costsensitive classification. Again, unable make approach execute fast enoughpractical, although faster AO* approach.cost-sensitive classification problem, treated here, essentiallyproblem reinforcement learning (Sutton, 1992; Karakoulas, preparation). averagecost classification, measured described Section 2.2, reward/punishment signalcould optimized using reinforcement learning techniques. somethingmight explored alternative approach.5.2Future Workpaper discusses two types costs, cost tests cost misclassificationerrors. two costs treated together decision theory, ICET firstmachine learning system handles costs together. experiments papercompared ICET machine learning systems handle test costs (Nez, 1988,1991; Tan & Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989), comparedICET machine learning systems handle classification error costs (Breimanet al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al., 1994). futurework, plan address omission. proper treatment issue would makepaper long.absence comparison machine learning systems handle classificationerror costs impact experiments reported here. experimentspaper focussed simple classification cost matrices (except Section 4.2.3).classification cost matrix simple cost tests ignored, minimizing cost exactlyequivalent maximizing accuracy (see Section 2.3). Therefore, C4.5 (which designedmaximize accuracy) suitable surrogate systems handle classification error costs.also experiment setting test costs zero. However, behaviorICET penalty misclassification errors high (the extreme right-hand sidesplots Figure 3) necessarily behavior cost testslow, since ICET sensitive relative differences test costs error costs,absolute costs. Therefore (given behavior observe extreme right-handsides plots Figure 3) expect performance ICET tend converge performance algorithms cost tests approaches zero.One natural addition ICET would ability output dont know class.easily handled GENESIS component, extending classification cost matrixcost assigned classifying case unknown. need also make smallmodification EG2 component, generate decision trees leaveslabelled unknown. One way would introduce parameter definesconfidence threshold. Whenever confidence certain leaf drops confidencethreshold, leaf would labelled unknown. confidence parameter would madeaccessible GENESIS component, could tuned minimize average classification cost.mechanism ICET handling conditional test costs limitations.397fiT URNEYcurrently implemented, handle cost attributes calculatedattributes. example, Thyroid dataset (Appendix A.5), FTI test calculatedbased results TT4 T4U tests. FTI test selected, must payTT4 T4U tests. TT4 T4U tests already selected, FTI test free(since calculation trivial). ability deal calculated test results couldadded ICET relatively little effort.ICET, currently implemented, handles two classes test results: testsimmediate results tests delayed results. Clearly continuous rangedelays, seconds years. chosen treat delays distinct test costs,could argued delay simply another type test cost. example, couldsay group blood tests shares common cost one-day wait results. costone blood tests conditional whether prepared commitone tests group, see results first test.One difficultly approach handling delays problem assigning costdelay. much cost bring patient two blood samples, instead one?include disruption patients life estimate cost? avoidquestions, treated delays another type test cost, approachreadily handle continuous range delays.cost test function several things: (1) function priortests selected. (2) function actual class case. (3)function aspects case, information aspects mayavailable tests. (4) function test result. list seemscomprehensive, may possibilities overlooked. Let us considerfour possibilities.First, cost test function prior tests selected. ICEThandles special case this, group tests shares common cost. currentlyimplemented, ICET handle general case. However, could easily addcapability ICET modifying fitness function.Second, cost test function actual class case. example,test heart disease might involve heavy exercise (Appendix A.2). patient actuallyheart disease, exercise might trigger heart attack. risk includedcost particular test. Thus cost test vary, depending whetherpatient actually heart disease. implemented this, although could easilyadded ICET modifying fitness function.Third, cost test function results tests. example, drawing blood newborn costly drawing blood adult. assign costblood test, need know age patient. age patient represented result another test patient-age test. slightly complexpreceding cases, must insure blood test always accompanied patient-age test. implemented this, although could addedICET.Fourth, cost test function test result. example, injectingradio-opaque die X-ray might cause allergic reaction patient. riskadded cost test. makes cost test function one possible outcomes test. situation like this, may wise precede injectiondie screening test allergies. could simple asking questionpatient. question may relevance determining correct diagnosis398fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONpatient, may serve reduce average cost classification. case similarthird case, above. Again, implemented this, although could addedICET.Attribute selection EG2, CS-ID3, IDX shares common form. may viewnattribute selection function { 1, , n } , takes input n informationgain values 1, , n (one attribute) generates output index oneattributes. may view C 1, , C n parameters attribute selection function. parameters may used control bias attribute selection procedure.view, ICET uses GENESIS tune parameters EG2s attribute selection function.future, would like investigate general attribute selection functions.nexample, might use neural network implement function { 1, , n } .GENESIS would used tune weights neural network.18 attributeselection function might also benefit addition input specifies depthdecision tree current node, information gain values measured.would enable bias test vary, depending many tests alreadyselected.Another area future work explore parameter settings control GENESIS(Table 4). many parameters could adjusted GENESIS. think significant ICET works well default parameter settings GENESIS, sinceshows ICET robust respect parameters. However, might possiblesubstantially improve performance ICET tuning parameters. recenttrend genetic algorithm research let genetic algorithm adjustparameters, mutation rate crossover rate (Whitley et al., 1993). Another possibility stop breeding fitness levels stop improving, instead stoppingfixed number generations. Provost Buchanan (in press) use goodness measurestopping condition bias space search.6. Conclusionscentral problem investigated problem minimizing cost classificationtests expensive. argued requires assigning cost classificationerrors. also argued decision tree natural form knowledge representationtype problem. presented general method calculating average costclassification decision tree, given decision tree, information calculationtest costs, classification cost matrix, set testing data. method applicablestandard classification decision trees, without regard decision tree generated.method sensitive test costs, sensitive classification error costs, capable handling conditional test costs, capable handling delayed tests.introduced ICET, hybrid genetic decision tree induction algorithm. ICET usesgenetic algorithm evolve population biases decision tree induction algorithm.individual population represents one set biases. fitness individualdetermined using generate decision tree training dataset, calculatingaverage cost classification decision tree testing dataset.analyzed behavior ICET series experiments, using five real-world medical datasets. Three groups experiments performed. first group lookedbaseline performance five algorithms five datasets. ICET found sig18. idea suggested conversation M. Brooks.399fiT URNEYnificantly lower costs algorithms. Although executes slowly, average time 23 minutes (for typical dataset) acceptable many applications,possibility much greater speed parallel machine. second group experiments studied robustness ICET variety modifications input.results show ICET robust. third group experiments examined ICETs searchbias space. discovered search could improved seeding initial population biases.general, research concerned pragmatic constraints classification problems (Provost & Buchanan, press). believe many real-world classification problems involve merely maximizing accuracy (Turney, press). resultspresented indicate that, certain applications, decision tree merely maximizesaccuracy (e.g., trees generated C4.5) may far performance possiblealgorithm considers realistic constraints test costs, classification errorcosts, conditional test costs, delayed test results. pragmaticconstraints faced real-world classification problems.Appendix A. Five Medical Datasetsappendix presents test costs five medical datasets, taken Irvine collection (Murphy & Aha, 1994). costs based information Ontario MinistryHealth (1992). Although none medical data gathered Ontario, reasonableassume areas similar relative test costs. purposes, relative costsimportant, absolute costs.A.1BUPA Liver DisordersBUPA Liver Disorders dataset created BUPA Medical Research Ltd.donated Irvine collection Richard Forsyth.19 Table 15 shows test costsBUPA Liver Disorders dataset. tests group blood tests thoughtsensitive liver disorders might arise excessive alcohol consumption. testsshare common cost $2.10 collecting blood. target concept defined usingsixth column: Class 0 defined drinks < 3 class 1 defined drinks3. Table 16 shows general form classification cost matrix usedexperiments Section 4. experiments, classification error cost equalspositive error cost equals negative error cost. exception Section 4.2.3,experiments complex classification cost matrices. terms positive error costnegative error cost explained Section 4.2.3. 345 cases dataset,missing values. Column seven originally used split data training testing sets. use column, since required ten different random splitsdata. ten random splits, ten training sets 230 cases ten testing sets115 cases.A.2Heart DiseaseHeart Disease dataset donated Irvine collection David Aha.20 princi19. BUPA Liver Disorders dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liverdisorders/bupa.data.20. Heart Disease dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/heart-disease/cleve.mod.400fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONTable 15: Test costs BUPA Liver Disorders dataset.TestDescriptionGroupCostDelayed1mcvmean corpuscular volume$7.27 first test group A,$5.17 otherwiseyes2alkphosalkaline phosphotase$7.27 first test group A,$5.17 otherwiseyes3sgptalamine aminotransferase$7.27 first test group A,$5.17 otherwiseyes4sgotaspartate aminotransferase$7.27 first test group A,$5.17 otherwiseyes5gammagtgamma-glutamyl transpeptidase$9.86 first test group A,$7.76 otherwiseyes6drinksnumber half-pint equivalentsalcoholic beverages drunk per daydiagnostic class: drinks < 3drinks 3-7selectorfield used split data two setsused-Table 16: Classification costs BUPA Liver Disorders dataset.Actual ClassGuess ClassCost0 (drinks < 3)0 (drinks < 3)$0.000 (drinks < 3)1 (drinks 3)Positive Error Cost1 (drinks 3)0 (drinks < 3)Negative Error Cost1 (drinks 3)1 (drinks 3)$0.00pal medical investigator Robert Detrano, Cleveland Clinic Foundation. Table 17shows test costs Heart Disease dataset. nominal cost $1.00 assignedfirst four tests. tests group blood tests thought relevantheart disease. tests share common cost $2.10 collecting blood. testsgroups B C involve measurements heart exercise. nominal cost $1.00assigned tests first test groups. class variablevalues buff (healthy) sick. fifteenth column, specified classvariable H (healthy), S1, S2, S3, S4 (four different types sick),deleted column. Table 18 shows classification cost matrix. 303 casesdataset. deleted cases missing values. reduceddataset 296 cases. ten random splits, training sets 197 cases testingsets 99 cases.A.3Hepatitis PrognosisHepatitis Prognosis dataset donated Gail Gong.21 Table 19 shows test costsHepatitis dataset. Unlike four datasets, dataset deals prognosis,21. Hepatitis Prognosis dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/hepatitis/hepatitis.data.401fiT URNEYTable 17: Test costs Heart Disease dataset.TestDescription1age2GroupCostDelayedage years$1.00sexpatients gender$1.003cpchest pain type$1.004trestbpsresting blood pressure$1.005cholserum cholesterol$7.27 first test group A,$5.17 otherwiseyes6fbsfasting blood sugar$5.20 first test group A,$3.10 otherwiseyes7restecgresting electrocardiograph$15.50yes8thalachmaximum heart rateachievedB$102.90 first test group B,$1.00 otherwiseyes9exangexercise induced anginaC$87.30 first test group C,$1.00 otherwiseyes10oldpeakST depression inducedexercise relative restC$87.30 first test group C,$1.00 otherwiseyes11slopeslope peak exercise STsegmentC$87.30 first test group C,$1.00 otherwiseyes12canumber major vesselscoloured fluoroscopy$100.90yes13thal3 = normal; 6 = fixed defect;7 = reversible defect$102.90 first test group B,$1.00 otherwiseyes14numdiagnosis heart diseasediagnostic class-BTable 18: Classification costs Heart Disease dataset.Actual ClassGuess ClassCostbuffbuff$0.00buffsickPositive Error CostsickbuffNegative Error Costsicksick$0.00diagnosis. prognosis, diagnosis known, problem determine likelyoutcome disease. tests assigned nominal cost $1.00 either involveasking question patient performing basic physical examination patient.tests group share cost $2.10 collecting blood. Note that, although performing histological examination liver costs $81.64, asking patient whetherhistology performed costs $1.00. Thus prognosis exploit informationconveyed decision (to perform histological examination) madediagnosis. class variable values 1 (die) 2 (live). Table 20 shows classification costs. dataset contains 155 cases, many missing values. ten random402fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONTable 19: Test costs Hepatitis Prognosis dataset.TestDescription1class2GroupCostDelayedprognosis hepatitisprognostic class: live die-ageage years$1.003sexgender$1.004steroidpatient steroids$1.005antiviralpatient antiviral$1.006fatiguepatient reports fatigue$1.007malaisepatient reports malaise$1.008anorexiapatient anorexic$1.009liver bigliver big physical exam$1.0010liver firmliver firm physical exam$1.0011spleen palpablespleen palpable physical$1.0012spidersspider veins visible$1.0013ascitesascites visible$1.0014varicesvarices visible$1.0015bilirubinbilirubin blood test$7.27 first test group A,$5.17 otherwiseyes16alk phosphatealkaline phosphotase$7.27 first test group A,$5.17 otherwiseyes17sgotaspartate aminotransferase$7.27 first test group A,$5.17 otherwiseyes18albuminalbumin blood test$7.27 first test group A,$5.17 otherwiseyes19protimeprotime blood test$8.30 first test group A,$6.20 otherwiseyes20histologyhistology performed?$1.00Table 20: Classification costs Hepatitis Prognosis dataset.Actual ClassGuess ClassCost1 (die)1 (die)$0.001 (die)2 (live)Negative Error Cost2 (live)1 (die)Positive Error Cost2 (live)2 (live)$0.00splits, training sets 103 cases testing sets 52 cases. filled missing values, using simple single nearest neighbor algorithm (Aha et al., 1991). missingvalues filled using whole dataset, dataset split trainingtesting sets. nearest neighbor algorithm, data normalized mini403fiT URNEYmum value feature 0 maximum value 1. distance measure usedsum absolute values differences. difference two valuesdefined 1 one two values missing.A.4Pima Indians DiabetesPima Indians Diabetes dataset donated Vincent Sigillito. 22 data collected National Institute Diabetes Digestive Kidney Diseases. Table 21shows test costs Pima Indians Diabetes dataset. tests group sharecost $2.10 collecting blood. remaining tests assigned nominal cost$1.00. patients females least 21 years old Pima Indian heritage.class variable values 0 (healthy) 1 (diabetes). Table 22 shows classification costs.dataset includes 768 cases, missing values. ten random splits, trainingsets 512 cases testing sets 256 cases.Table 21: Test costs Pima Indians Diabetes dataset.TestDescriptionGroupCostDelayed1times pregnantnumber times pregnant$1.002glucose tolglucose tolerance test$17.61 first test group A,$15.51 otherwiseyes3diastolic bpdiastolic blood pressure$1.004tricepstriceps skin fold thickness$1.005insulinserum insulin test$22.78 first test group A,$20.68 otherwiseyes6mass indexbody mass index$1.007pedigreediabetes pedigree function$1.008ageage years$1.009classdiagnostic classdiagnostic class-Table 22: Classification costs Pima Indians Diabetes dataset.A.5Actual ClassGuess ClassCost0 (healthy)0 (healthy)$0.000 (healthy)1 (diabetes)Positive Error Cost1 (diabetes)0 (healthy)Negative Error Cost1 (diabetes)1 (diabetes)$0.00Thyroid DiseaseThyroid Disease dataset created Garavan Institute, Sydney, Australia.file donated Randolf Werner, obtained Daimler-Benz. Daimler-Benz obtained22. Pima Indians Diabetes dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/pimaindians-diabetes/pima-indians-diabetes.data.404fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONdata J.R. Quinlan.23 Table 23 shows test costs Thyroid Disease dataset.nominal cost $1.00 assigned first 16 tests. tests group share cost$2.10 collecting blood. FTI test involves calculation based resultsTT4 T4U tests. complicates calculation costs three tests,chose use FTI test experiments. class variable values 1(hypothyroid), 2 (hyperthyroid), 3 (normal). Table 24 shows classification costs.3772 cases dataset, missing values. ten random splits,training sets 2515 cases testing sets 1257 cases.Table 23: Test costs Thyroid Disease dataset.TestDescription1age2GroupCostDelayedage years$1.00sexgender$1.003thyroxinepatient thyroxine$1.004query thyroxinemaybe thyroxine$1.005antithyroidantithyroid medication$1.006sickpatient reports malaise$1.007pregnantpatient pregnant$1.008thyroid surgeryhistory thyroid surgery$1.009I131 treatmentpatient I131 treatment$1.0010query hypothyroidmaybe hypothyroid$1.0011query hyperthyroidmaybe hyperthyroid$1.0012lithiumpatient lithium$1.0013goitrepatient goitre$1.0014tumourpatient tumour$1.0015hypopituitarypatient hypopituitary$1.0016psychpsychological symptoms$1.0017TSHTSH value, measured$22.78 first test group A, yes$20.68 otherwise18T3T3 value, measured$11.41 first test group A, yes$9.31 otherwise19TT4TT4 value, measured$14.51 first test group A, yes$12.41 otherwise20T4UT4U value, measured$11.41 first test group A, yes$9.31 otherwise21FTIFTI calculatedTT4 T4Uused-22classdiagnostic classdiagnostic class-23. Thyroid Disease dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/thyroid-disease/ann-train.data.405fiT URNEYTable 24: Classification costs Thyroid Disease dataset.Actual ClassGuess ClassCost1 (hypothyroid)1 (hypothyroid)$0.001 (hypothyroid)2 (hyperthyroid)Minimum(Negative Error Cost, Positive Error Cost)1 (hypothyroid)3 (normal)Negative Error Cost2 (hyperthyroid)1 (hypothyroid)Minimum(Negative Error Cost, Positive Error Cost)2 (hyperthyroid)2 (hyperthyroid)$0.002 (hyperthyroid)3 (normal)Negative Error Cost3 (normal)1 (hypothyroid)Positive Error Cost3 (normal)2 (hyperthyroid)Positive Error Cost3 (normal)3 (normal)$0.00AcknowledgmentsThanks Dr. Louise Linney help interpretation Ontario MinistryHealths Schedule Benefits. Thanks Martin Brooks, Grigoris Karakoulas, Cullen Schaffer, Diana Gordon, Tim Niblett, Steven Minton, three anonymous referees JAIRhelpful comments earlier versions paper. work presentedinformal talks University Ottawa Naval Research Laboratory. Thanksaudiences feedback.ReferencesAckley, D., & Littman, M. (1991). Interactions learning evolution. Proceedings Second Conference Artificial Life, C. Langton, C. Taylor, D. Farmer, S.Rasmussen, editors. California: Addison-Wesley.Aha, D.W., Kibler, D., & Albert, M.K. (1991). Instance-based learning algorithms, MachineLearning, 6, 37-66.Aha, D.W., & Bankert, R.L. (1994). Feature selection case-based classification cloudtypes: empirical comparison. Case-Based Reasoning: Papers 1994 Workshop, edited D.W. Aha, Technical Report WS-94-07, pp. 106-112. Menlo Park, CA:AAAI Press.Anderson, R.W. (in press). Learning evolution: quantitative genetics approach. Journal Theoretical Biology.Baldwin, J.M. (1896). new factor evolution. American Naturalist, 30, 441-451.Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification regressiontrees. California: Wadsworth.De Jong, K.A., Spears, W.M., & Gordon, D.F. (1993). Using genetic algorithms conceptlearning. Machine Learning, 13, 161-188.406fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONFogarty, T.C. (1992). Technical note: First nearest neighbor classification Frey Slatesletter recognition problem. Machine Learning, 9, 387-388.Frey, P.W., & Slate, D.J., (1991). Letter recognition using Holland-style adaptive classifiers.Machine Learning, 6, 161-182.Friedman, J.H., & Stuetzle, W. (1981). Projection pursuit regression. Journal American Statistics Association, 76, 817-823.Gordon, D.F., & Perlis, D. (1989). Explicitly biased generalization. Computational Intelligence, 5, 67-81.Grefenstette, J.J. (1986). Optimization control parameters genetic algorithms. IEEETransactions Systems, Man, Cybernetics, 16, 122-128.Grefenstette, J.J., Ramsey, C.L., & Schultz, A.C. (1990). Learning sequential decision rulesusing simulation models competition. Machine Learning, 5, 355-381.Hermans, J., Habbema, J.D.F., & Van der Burght, A.T. (1974). Cases doubt allocationproblems, k populations. Bulletin International Statistics Institute, 45, 523-529.Hinton, G.E., & Nowlan, S.J. (1987). learning guide evolution. Complex Systems,1, 495-502.Karakoulas, G. (in preparation). Q-learning approach cost-effective classification. Technical Report, Knowledge Systems Laboratory, National Research Council Canada. Alsosubmitted Twelfth International Conference Machine Learning, ML-95.Knoll, U., Nakhaeizadeh, G., & Tausend, B. (1994). Cost-sensitive pruning decision trees.Proceedings Eight European Conference Machine Learning, ECML-94, pp.383-386. Berlin, Germany: Springer-Verlag.Koza, J.R. (1992). Genetic Programming: programming computers meansnatural selection. Cambridge, MA: MIT Press.Lirov, Y., & Yue, O.-C. (1991). Automated network troubleshooting knowledge acquisition.Journal Applied Intelligence, 1, 121-132.Maynard Smith, J. (1987). learning guides evolution. Nature, 329, 761-762.Morgan, C.L. (1896). modification variation. Science, 4, 733-740.Murphy, P.M., & Aha, D.W. (1994). UCI Repository Machine Learning Databases. University California Irvine, Department Information Computer Science.Norton, S.W. (1989). Generating better decision trees. Proceedings Eleventh International Joint Conference Artificial Intelligence, IJCAI-89, pp. 800-805. Detroit, Michigan.Nez, M. (1988). Economic induction: case study. Proceedings Third EuropeanWorking Session Learning, EWSL-88, pp. 139-145. California: Morgan Kaufmann.407fiT URNEYNez, M. (1991). use background knowledge decision tree induction. MachineLearning, 6, 231-250.Ontario Ministry Health (1992). Schedule benefits: Physician services healthinsurance act, October 1, 1992. Ontario: Ministry Health.Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing misclassification costs: Knowledge-intensive approaches learning noisy data. Proceedings Eleventh International Conference Machine Learning, ML-94, pp. 217225. New Brunswick, New Jersey.Pearl, J. (1984). Heuristics: Intelligent search strategies computer problem solving. Massachusetts: Addison-Wesley.Pearl, J. (1988). Probabilistic reasoning intelligent systems: Networks plausible inference. California: Morgan Kaufmann.Pipitone, F., De Jong, K.A., & Spears, W.M. (1991). artificial intelligence approachanalog systems diagnosis. Testing Diagnosis Analog Circuits Systems,Ruey-wen Liu, editor. New York: Van Nostrand-Reinhold.Provost, F.J. (1994). Goal-directed inductive learning: Trading accuracy reduced errorcost. AAAI Spring Symposium Goal-Driven Learning.Provost, F.J., & Buchanan, B.G. (in press). Inductive policy: pragmatics bias selection. Machine Learning.Quinlan, J.R. (1992). C4.5: Programs machine learning. California: Morgan Kaufmann.Ragavan, H., & Rendell, L. (1993). Lookahead feature construction learning hard concepts. Proceedings Tenth International Conference Machine Learning, ML-93,pp. 252-259. California: Morgan Kaufmann.Rymon, R. (1993). SE-tree based characterization induction problem. ProceedingsTenth International Conference Machine Learning, ML-93, pp. 268-275. California: Morgan Kaufmann.Schaffer, C. (1993). Selecting classification method cross-validation. Machine Learning, 13, 135-143.Schaffer, J.D., Whitley, D., & Eshelman, L.J. (1992). Combinations genetic algorithmsneural networks: survey state art. Combinations Genetic Algorithms Neural Networks, D. Whitley J.D. Schaffer, editors. California: IEEEComputer Society Press.Seshu, R. (1989). Solving parity problem. Proceedings Fourth European WorkingSession Learning, EWSL-89, pp. 263-271. California: Morgan Kaufmann.Spears, W.M. (1992). Crossover mutation? Foundations Genetic Algorithms 2, FOGA92, edited D. Whitley. California: Morgan Kaufmann.408fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATIONSutton, R.S. (1992). Introduction: challenge reinforcement learning. Machine Learning, 8, 225-227.Tan, M., & Schlimmer, J. (1989). Cost-sensitive concept learning sensor use approachrecognition. Proceedings Sixth International Workshop Machine Learning,ML-89, pp. 392-395. Ithaca, New York.Tan, M., & Schlimmer, J. (1990). CSL: cost-sensitive learning system sensinggrasping objects. IEEE International Conference Robotics Automation. Cincinnati, Ohio.Tan, M. (1993). Cost-sensitive learning classification knowledge applicationsrobotics. Machine Learning, 13, 7-33.Tcheng, D., Lambert, B., Lu, S., Rendell, L. (1989). Building robust learning systemscombining induction optimization. Proceedings Eleventh International JointConference Artificial Intelligence, IJCAI-89, pp. 806-812. Detroit, Michigan.Turney, P.D. (in press). Technical note: Bias quantification stability. MachineLearning.Verdenius, F. (1991). method inductive cost optimization. Proceedings FifthEuropean Working Session Learning, EWSL-91, pp. 179-191. New York: SpringerVerlag.Waddington, C.H. (1942). Canalization development inheritance acquired characters. Nature, 150, 563-565.Whitley, D., Dominic, S., Das, R., & Anderson, C.W. (1993). Genetic reinforcement learningneurocontrol problems. Machine Learning, 13, 259-284.Whitley, D., & Gruau, F. (1993). Adding learning cellular development neural networks: Evolution Baldwin effect. Evolutionary Computation, 1, 213-233.Whitley, D., Gordon, S., & Mathias, K. (1994). Lamarckian evolution, Baldwin effectfunction optimization. Parallel Problem Solving Nature PPSN III. Y. Davidor, H.P. Schwefel, R. Manner, editors, pp. 6-15. Berlin: Springer-Verlag.Wilson, S.W. (1987). Classifier systems animat problem. Machine Learning, 2, 199228.409fiJournal Artificial Intelligence Research 2 (1995) 263{286Submitted 8/94; published 1/95Solving Multiclass Learning Problems viaError-Correcting Output CodesThomas G. Dietterichtgd@cs.orst.eduDepartment Computer Science, 303 Dearborn HallOregon State UniversityCorvallis, 97331 USAGhulum Bakirieb004@isa.cc.uob.bhDepartment Computer ScienceUniversity BahrainIsa Town, BahrainAbstractMulticlass learning problems involve finding definition unknown function (x)whose range discrete set containing2 values (i.e., \classes"). definitionacquired studying collections training examples form hx (x )i. Existing approaches multiclass learning problems include direct application multiclass algorithmsdecision-tree algorithms C4.5 CART, application binary concept learningalgorithms learn individual binary functions classes, applicationbinary concept learning algorithms distributed output representations. papercompares three approaches new technique error-correcting codesemployed distributed output representation. show output representations improve generalization performance C4.5 backpropagation widerange multiclass learning tasks. also demonstrate approach robustrespect changes size training sample, assignment distributed representations particular classes, application overfitting avoidance techniquesdecision-tree pruning. Finally, show that|like methods|the error-correctingcode technique provide reliable class probability estimates. Taken together, results demonstrate error-correcting output codes provide general-purpose methodimproving performance inductive learning programs multiclass problems.fk >ki; fk1. Introductiontask learning examples find approximate definition unknownfunction f (x) given training examples form hx ; f (x )i. cases f takesvalues f0; 1g|binary functions|there many algorithms available. example,decision-tree methods, C4.5 (Quinlan, 1993) CART (Breiman, Friedman,Olshen, & Stone, 1984) construct trees whose leaves labeled binary values.artificial neural network algorithms, perceptron algorithm (Rosenblatt,1958) error backpropagation (BP) algorithm (Rumelhart, Hinton, & Williams,1986), best suited learning binary functions. Theoretical studies learningfocused almost entirely learning binary functions (Valiant, 1984; Natarajan, 1991).many real-world learning tasks, however, unknown function f often takes valuesdiscrete set \classes": fc1; : : : ; c g. example, medical diagnosis, functionmight map description patient one k possible diseases. digit recognition (e.g.,kc 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiDietterich & BakiriLeCun, Boser, Denker, Henderson, Howard, Hubbard, & Jackel, 1989), function mapshand-printed digit one k = 10 classes. Phoneme recognition systems (e.g., Waibel,Hanazawa, Hinton, Shikano, & Lang, 1989) typically classify speech segment one50 60 phonemes.Decision-tree algorithms easily generalized handle \multiclass" learningtasks. leaf decision tree labeled one k classes, internalnodes selected discriminate among classes. call directmulticlass approach.Connectionist algorithms dicult apply multiclass problems. standard approach learn k individual binary functions f1 ; : : : ; f , one class.assign new case, x, one classes, f evaluated x, xassigned class j function f returns highest activation (Nilsson, 1965).call one-per-class approach, since one binary function learnedclass.alternative approach explored researchers employ distributed outputcode. approach pioneered Sejnowski Rosenberg (1987) widelyknown NETtalk system. class assigned unique binary string length n;refer strings \codewords." n binary functions learned, one bitposition binary strings. training example class i, desiredoutputs n binary functions specified codeword class i. artificialneural networks, n functions implemented n output units singlenetwork.New values x classified evaluating n binary functions generaten-bit string s. string compared k codewords, x assignedclass whose codeword closest, according distance measure, generatedstring s.example, consider Table 1, shows six-bit distributed code ten-classdigit-recognition problem. Notice row distinct, class uniquecodeword. applications distributed output codes, bit positions (columns)chosen meaningful. Table 2 gives meanings six columns.learning, one binary function learned column. Notice columnalso distinct binary function learned disjunction originalclasses. example, f (x) = 1 f (x) 1, 4, 5.classify new hand-printed digit, x, six functions f ; f ; f ; f ; f ; fevaluated obtain six-bit string, 110001. distance stringten codewords computed. nearest codeword, according Hammingdistance (which counts number bits differ), 110000, corresponds class4. Hence, predicts f (x) = 4.process mapping output string nearest codeword identical decoding step error-correcting codes (Bose & Ray-Chaudhuri, 1960; Hocquenghem, 1959).suggests might advantage employing error-correcting codesdistributed representation. Indeed, idea employing error-correcting, distributedrepresentations traced early research machine learning (Duda, Machanik, &Singleton, 1963).kjvlvl264hldlccolfiError-Correcting Output CodesTable 1: distributed code digit recognition task.Class0123456789vl0100110000Code Wordhl dl cc ol0 0 1 00 0 0 01 1 0 10 0 0 11 0 0 01 0 0 10 1 1 00 1 0 00 0 1 00 1 1 00000001000Table 2: Meanings six columns code Table 1.Column position AbbreviationMeaning1vlcontains vertical line2hlcontains horizontal line3dlcontains diagonal line4cccontains closed curve5olcontains curve open left6contains curve open rightTable 3: 15-bit error-correcting output code ten-class problem.Class0123456789f01010101010f11000110011f20101101001f30111001111f40100111100f50101010110f61001100110Code Wordf70111110000265f81010010101f90110011010f100110100101f111000001100f121011000000f130100000011f141011111111fiDietterich & BakiriTable 3 shows 15-bit error-correcting code digit-recognition task. classrepresented code word drawn error-correcting code. distributedencoding Table 1, separate boolean function learned bit position errorcorrecting code. classify new example x, learned functions f0 (x); : : :; f14(x)evaluated produce 15-bit string. mapped nearest tencodewords. code correct three errors 15 bits.error-correcting code approach suggests view machine learning kindcommunications problem identity correct output class newexample \transmitted" channel. channel consists input features,training examples, learning algorithm. errors introducedfinite training sample, poor choice input features, aws learning process,class information corrupted. encoding class error-correcting code\transmitting" bit separately (i.e., via separate run learning algorithm),system may able recover errors.perspective suggests one-per-class \meaningful" distributedoutput approaches inferior, output representations constituterobust error-correcting codes. measure quality error-correcting codeminimum Hamming distance pair code words. minimum Hammingdistance d, code correct least b ,2 1 c single bit errors.single bit error moves us one unit away true codeword (in Hamming distance).make b ,2 1 c errors, nearest codeword still correct codeword. (Thecode Table 3 minimum Hamming distance seven hence correct errorsthree bit positions.) Hamming distance two codewords oneper-class code two, one-per-class encoding k output classes cannot correcterrors.minimum Hamming distance pairs codewords \meaningful" distributed representation tends low. example, Table 1, Hammingdistance codewords classes 4 5 one. kinds codes, newcolumns often introduced discriminate two classes. two classestherefore differ one bit position, Hamming distance outputrepresentations one. also true distributed representation developedSejnowski Rosenberg (1987) NETtalk task.paper, compare performance error-correcting code approachthree existing approaches: direct multiclass method (using decision trees),one-per-class method, (in NETtalk task only) meaningful distributed outputrepresentation approach. show error-correcting codes produce uniformly bettergeneralization performance across variety multiclass domains C4.5 decisiontree learning algorithm backpropagation neural network learning algorithm.report series experiments designed assess robustness error-correctingcode approach various changes learning task: length code, size trainingset, assignment codewords classes, decision-tree pruning. Finally, showerror-correcting code approach produce reliable class probability estimates.paper concludes discussion open questions raised results.Chief among questions issue errors made different bitpositions output somewhat independent one another. Without independ266fiError-Correcting Output CodesTable 4: Data sets employed study.NameglassvowelPOSsoybeanaudiologySISOLETletterNETtalkNumber NumberNumberNumberFeaturesClassesTraining Examples Test Examples9621410-fold xval101152846230123,06010-fold xval3519307376692420026617266,2381,559162616,0004,00020354 phonemes1000 words =1000 words =6 stresses7,229 letters7,242 lettersdence, error-correcting output code method would fail. address question|forcase decision-tree algorithms|in companion paper (Kong & Dietterich, 1995).2. Methodssection describes data sets learning algorithms employed study.also discusses issues involved design error-correcting codes describes fouralgorithms code design. section concludes brief description methodsapplied make classification decisions evaluate performance independent test sets.2.1 Data SetsTable 4 summarizes data sets employed study. glass, vowel, soybean, audiologyS, ISOLET, letter, NETtalk data sets available Irvine Repositorymachine learning databases (Murphy & Aha, 1994).1 POS (part speech) data setprovided C. Cardie (personal communication); earlier version data setdescribed Cardie (1993). use entire NETtalk data set, consistsdictionary 20,003 words pronunciations. Instead, make experimentsfeasible, chose training set 1000 words disjoint test set 1000 wordsrandom NETtalk dictionary. paper, focus percentage letterspronounced correctly (rather whole words). pronounce letter, phonemestress letter must determined. Although 54 6 syntactically possiblecombinations phonemes stresses, 140 appear training testsets selected.1. repository refers soybean data set \soybean-large", \audiologyS" data set \audiology.standardized", \letter" data set \letter-recognition".267fiDietterich & Bakiri2.2 Learning Algorithmsemployed two general classes learning methods: algorithms learning decision treesalgorithms learning feed-forward networks sigmoidal units (artificial neural networks). decision trees, performed experiments using C4.5, Release 1,older (but substantially identical) version program described Quinlan (1993).made several changes C4.5 support distributed output representations,affected tree-growing part algorithm. pruning, confidencefactor set 0.25. C4.5 contains facility creating \soft thresholds" continuousfeatures. found experimentally improved quality class probabilityestimates produced algorithm \glass", \vowel", \ISOLET" domains,results reported domains computed using soft thresholds.neural networks, employed two implementations. domains, usedextremely fast backpropagation implementation provided CNAPS neurocomputer(Adaptive Solutions, 1992). performs simple gradient descent fixed learningrate. gradient updated presenting training example; momentum termemployed. potential limitation CNAPS inputs representedeight bits accuracy, weights represented 16 bits accuracy. Weightupdate arithmetic round, instead performs jamming (i.e., forcing lowestorder bit 1 low order bits lost due shifting multiplication).speech recognition, letter recognition, vowel data sets, employed opt systemdistributed Oregon Graduate Institute (Barnard & Cole, 1989). implementsconjugate gradient algorithm updates gradient complete passtraining examples (known per-epoch updating). learning rate requiredapproach.CNAPS opt attempt minimize squared error computeddesired outputs network. Many researchers employed error measures,particularly cross-entropy (Hinton, 1989) classification figure-of-merit (CFM, Hampshire II & Waibel, 1990). Many researchers also advocate using softmax normalizing layeroutputs network (Bridle, 1990). configurations goodtheoretical support, Richard Lippmann (1991) report squared error workswell measures producing accurate posterior probability estimates. Furthermore, cross-entropy CFM tend overfit easily squared error (Lippmann,personal communication; Weigend, 1993). chose minimize squared errorCNAPS opt systems implement.either neural network algorithm, several parameters must chosen user.CNAPS, must select learning rate, initial random seed, numberhidden units, stopping criteria. selected optimize performancevalidation set, following methodology Lang, Hinton, Waibel (1990).training set subdivided subtraining set validation set. trainingsubtraining set, observed generalization performance validation set determineoptimal settings learning rate network size best pointstop training. training set mean squared error stopping point computed,training performed entire training set using chosen parametersstopping indicated mean squared error. Finally, measure network performancetest set.268fiError-Correcting Output Codesdata sets, procedure worked well. However, letterrecognition data set, clearly choosing poor stopping points full training set.overcome problem, employed slightly different procedure determinestopping epoch. trained series progressively larger training sets (allsubsets final training set). Using validation set, determined beststopping epoch training sets. extrapolated trainingsets predict best stopping epoch full training set.\glass" \POS" data sets, employed ten-fold cross-validation assessgeneralization performance. chose training parameters based one \fold"ten-fold cross-validation. creates test set contamination, since examplesvalidation set data one fold test set data folds. However, foundlittle overfitting, validation set little effect choiceparameters stopping points.data sets come designated test sets, employed measuregeneralization performance.2.3 Error-Correcting Code Designdefine error-correcting code matrix binary values matrix shownTable 3. length code number columns code. numberrows code equal number classes multiclass learning problem.\codeword" row code.good error-correcting output code k-class problem satisfy two properties:Row separation. codeword well-separated Hamming distancecodewords.Column separation. bit-position function f uncorrelatedfunctions learned bit positions f ; j 6= i: achievedinsisting Hamming distance column columnslarge Hamming distance column complementcolumns also large.jpower code correct errors directly related row separation,discussed above. purpose column separation condition less obvious. twocolumns j similar identical, deterministic learning algorithmC4.5 applied learn f f , make similar (correlated) mistakes. Errorcorrecting codes succeed errors made individual bit positions relativelyuncorrelated, number simultaneous errors many bit positions small.many simultaneous errors, error-correcting code able correct(Peterson & Weldon, 1972).errors columns j also highly correlated bits columnscomplementary. algorithms C4.5 backpropagation treatclass complement symmetrically. C4.5 construct identical decision trees0-class 1-class interchanged. maximum Hamming distance twocolumns attained columns complements. Hence, column separationcondition attempts ensure columns neither identical complementary.j269fiDietterich & BakiriTable 5: possible columns three-class problem. Note last four columnscomplements first four first column discriminateamong classes.Classc0c1c2f0000f1001f2010Code Wordf3f4011100f5101f6110f7111Unless number classes least five, dicult satisfy properties. example, number classes three, 23 = 8 possiblecolumns (see Table 5). these, half complements half. leaves usfour possible columns. One either zeroes ones,make useless discriminating among rows. result leftthree possible columns, exactly one-per-class encoding provides.general, k classes, 2 ,1 , 1 usable columnsremoving complements all-zeros all-ones column. four classes, getseven-column code minimum inter-row Hamming distance 4. five classes, get15-column code, on.employed four methods constructing good error-correcting output codespaper: (a) exhaustive technique, (b) method selects columnsexhaustive code, (c) method based randomized hill-climbing algorithm, (d) BCHcodes. choice method use based number classes, k. Findingsingle method suitable values k open research problem. describefour methods turn.k2.3.1 Exhaustive Codes3 k 7, construct code length 2 ,1 , 1 follows. Row 1 ones. Row 2consists 2 ,2 zeroes followed 2 ,2 , 1 ones. Row 3 consists 2 ,3 zeroes, followed2 ,3 ones, followed 2 ,3 zeroes, followed 2 ,3 , 1 ones. row i, alternatingruns 2 , zeroes ones. Table 6 shows exhaustive code five-class problem.code inter-row Hamming distance 8; columns identical complementary.kkkkkkkk2.3.2 Column Selection Exhaustive Codes8 k 11, construct exhaustive code select good subsetcolumns. formulate propositional satisfiability problem applyGSAT algorithm (Selman, Levesque, & Mitchell, 1992) attempt solution. solutionrequired include exactly L columns (the desired length code) ensuringHamming distance every two columns L , d,chosen value d. column represented boolean variable. pairwise mutual270fiError-Correcting Output CodesRow11000012345210001Table 6: Exhaustive code k=5.Column3 4 5 6 7 8 9 10 11 121 1 1 1 1 1 1 1 1 10 0 0 0 0 0 1 1 1 10 0 1 1 1 1 0 0 0 01 1 0 0 1 1 0 0 1 10 1 0 1 0 1 0 1 0 113111001411101101010011511110Figure 1: Hill-climbing algorithm improving row column separation. two closestrows columns indicated lines. lines intersect, bitscode words changed improve separations shown right.exclusion constraint placed two columns violate column separationcondition. support constraints, extended GSAT support mutual exclusion\m-of-n" constraints eciently.2.3.3 Randomized Hill Climbingk > 11, employed random search algorithm begins drawing k randomstrings desired length L. pair random strings separatedHamming distance binomially distributed mean L=2. Hence, randomlygenerated codes generally quite good average. improve them, algorithmrepeatedly finds pair rows closest together Hamming distance paircolumns \most extreme" Hamming distance (i.e., either closefar apart). algorithm computes four codeword bits rowscolumns intersect changes improve row column separations shownFigure 1. hill climbing procedure reaches local maximum, algorithmrandomly chooses pairs rows columns tries improve separations.combined hill-climbing/random-choice procedure able improve minimum Hammingdistance separation quite substantially.271fiDietterich & Bakiri2.3.4 BCH Codesk > 11 also applied BCH algorithm design codes (Bose & Ray-Chaudhuri,1960; Hocquenghem, 1959). BCH algorithm employs algebraic methods Galoisfield theory design nearly optimal error-correcting codes. However, three practical drawbacks using algorithm. First, published tables primitive polynomialsrequired algorithm produce codes length 64, since largest wordsize employed computer memories. Second, codes always exhibit good columnseparations. Third, number rows codes always power two. number classes k learning problem power two, must shorten codedeleting rows (and possible columns) maintaining good row column separations.experimented various heuristic greedy algorithms code shortening.codes used NETtalk, ISOLET, Letter Recognition domains,used combination simple greedy algorithms manual intervention design goodshortened BCH codes.data sets studied, designed series error-correcting codesincreasing lengths. executed learning algorithm codes.stopped lengthening codes performance appeared leveling off.2.4 Making Classification Decisionsapproach solving multiclass problems|direct multiclass, one-per-class, errorcorrecting output coding|assumes method classifying new examples. C4.5direct multiclass approach, C4.5 system computes class probability estimatenew example. estimates probability example belongsk classes. C4.5 chooses class highest probability classexample.one-per-class approach, decision tree neural network output unitviewed computing probability new example belongs correspondingclass. class whose decision tree output unit gives highest probability estimatechosen predicted class. Ties broken arbitrarily favor class comesfirst class ordering.error-correcting output code approach, decision tree neural networkoutput unit viewed computing probability corresponding bitcodeword one. Call probability values B = hb1; b2; : : : ; b i, n lengthcodewords error-correcting code. classify new example, compute L1distance probability vector B codewords W (i = 1 : : :k)error correcting code. L1 distance B W definednL1 (B; Wi)=XjLj=0bj, W j:i;jclass whose codeword smallest L1 distance B assigned classnew example. Ties broken arbitrarily favor class comes first classordering.272fiPerformance relative MulticlassybeanudiologyLELetterNETtalkPOelowVGlassError-Correcting Output Codes*10*****0C4.5 Multiclass**-10*-20-30*C4.5 one-per-classC4.5 ECOCFigure 2: Performance (in percentage points) one-per-class ECOC methods relative direct multiclass method using C4.5. Asterisk indicates differencesignificant 0.05 level better.3. Resultspresent results experiments. begin results decision trees.Then, consider neural networks. Finally, report results series experimentsassess robustness error-correcting output code method.3.1 Decision TreesFigure 2 shows performance C4.5 eight domains. horizontal line correspondsperformance standard multiclass decision-tree algorithm. light bar showsperformance one-per-class approach, dark bar shows performanceECOC approach longest error-correcting code tested. Performance displayednumber percentage points pair algorithms differ. asteriskindicates difference statistically significant p < 0:05 level accordingtest difference two proportions (using normal approximation binomialdistribution, see Snedecor & Cochran, 1989, p. 124).figure, see one-per-class method performs significantly worsemulticlass method four eight domains behavior statisticallyindistinguishable remaining four domains. Much encouraging observationerror-correcting output code approach significantly superior multiclassapproach six eight domains indistinguishable remaining two.273fiDietterich & BakiriNETtalk domain, also consider performance meaningful distributed representation developed Sejnowski Rosenberg. representation gave66.7% correct classification compared 68.6% one-per-class configuration,70.0% direct-multiclass configuration, 74.3% ECOC configuration.differences figures statistically significant 0.05 level betterexcept one-per-class direct-multiclass configurations statistically distinguishable.3.2 BackpropagationFigure 3 shows results backpropagation five challenging domains.horizontal line corresponds performance one-per-class encodingmethod. bars show number percentage points error-correctingoutput coding representation outperforms one-per-class representation. fourfive domains, ECOC encoding superior; differences statistically significantVowel, NETtalk, ISOLET domains.2letter recognition domain, encountered great diculty successfully trainingnetworks using CNAPS machine, particularly ECOC configuration. Experimentsshowed problem arose fact CNAPS implementation backpropagation employs fixed learning rate. therefore switched much slower optprogram, chooses learning rate adaptively via conjugate-gradient line searches.behaved better one-per-class ECOC configurations.also diculty training ISOLET ECOC configuration large networks (182 units), even opt program. sets initial random weights ledlocal minima poor performance validation set.NETtalk task, compare performance Sejnowski-Rosenbergdistributed encoding one-per-class ECOC encodings. distributed encodingyielded performance 71.5% correct, compared 72.9% one-per-class encoding,74.9% ECOC encoding. difference distributed encodingone-per-class encoding statistically significant. results previousresults C4.5, conclude distributed encoding advantagesone-per-class ECOC encoding domain.3.3 Robustnessresults show ECOC approach performs well as, often better than,alternative approaches. However, several important questions mustanswered recommend ECOC approach without reservation:results hold small samples? found decision trees learned usingerror-correcting codes much larger learned using one-per-classmulticlass approaches. suggests small sample sizes, ECOC methodmay perform well, since complex trees usually require data learnedreliably. hand, experiments described covered wide range2. difference ISOLET detectable using test paired differences proportions. SeeSnedecor & Cochran (1989, p. 122.).274fiPerformance relative one-per-class10lkETtaNteLerLEelowVGlassError-Correcting Output Codes*5**0Backprop one-per-classBackprop ECOCFigure 3: Performance ECOC method relative one-per-class using backpropagation. Asterisk indicates difference significant 0.05 level better.training set sizes, suggests results may depend largetraining set.results depend particular assignment codewords classes?codewords assigned classes arbitrarily experiments reported above,suggests particular assignment may important. However,assignments might still much better others.results depend whether pruning techniques applied decision-tree algorithms? Pruning methods shown improve performancemulticlass C4.5 many domains.ECOC approach provide class probability estimates? C4.5 back-propagation configured provide estimates probability testexample belongs k possible classes. ECOC approachwell?3.3.1 Small sample performancenoted, became concerned small sample performance ECOCmethod noticed ECOC method always requires much larger decision treesOPC method. Table 7 compares sizes decision trees learned C4.5multiclass, one-per-class, ECOC configurations letter recognition taskNETtalk task. OPC ECOC configurations, tables show averagenumber leaves trees learned bit position output representation.275fiDietterich & BakiriTable 7: Size decision trees learned C4.5 letter recognition taskNETtalk task.Letter Recognition Leaves per bit Total leavesMulticlass2353One-per-class2426292207-bit ECOC1606332383NETtalkMulticlassOne-per-Class159-bit ECOCLeaves per bitphoneme stress61901600911Total leavesphoneme stress1425 15673320 3602114469 29140letter recognition, trees learned 207-bit ECOC six times largerlearned one-per-class representation. phoneme classification partNETtalk, ECOC trees 14 times larger OPC trees. Another waycompare sizes trees consider total number leaves trees.tables clearly show multiclass approach requires much less memory (many fewertotal leaves) either OPC ECOC approaches.backpropagation, dicult determine amount \network resources" consumed training network. One approach comparenumber hidden units give best generalization performance. ISOLET task,example, one-per-class encoding attains peak validation set performance 78hidden-unit network, whereas 30-bit error-correcting encoding attained peak validationset performance 156-hidden-unit network. letter recognition task, peak performance one-per-class encoding obtained network 120-hidden unitscompared 200 hidden units 62-bit error-correcting output code.decision tree neural network sizes, see that, general, errorcorrecting output representation requires complex hypotheses one-per-classrepresentation. learning theory statistics, known complex hypothesestypically require training data simple ones. basis, one might expectperformance ECOC method would poor small training sets. testprediction, measured performance function training set size twolarger domains: NETtalk letter recognition.Figure 4 presents learning curves C4.5 NETtalk letter recognition tasks,show accuracy series progressively larger training sets. figureclear 61-bit error-correcting code consistently outperforms two configurations nearly constant margin. Figure 5 shows corresponding results backpropagation NETtalk letter recognition tasks. NETtalk task, resultssame: sample size apparent uence benefits error-correcting output coding. However, letter-recognition task, appears interaction.276fiError-Correcting Output CodesNETtalkLetter Recognition75100C4 61-bit ECOCPercent Correct70C4 MulticlassC4 One-per-class90C4 Multiclass65806070556050504540403035100C4 62-bit ECOCC4 One-per-class2010010001000Training Set Size10000Training Set SizeFigure 4: Accuracy C4.5 multiclass, one-per-class, error-correcting outputcoding configurations increasing training set sizes NETtalk letterrecognition tasks. Note horizontal axis plotted logarithmic scale.NETtalkLetter Recognition100759070CNAPS 61-bit ECOCPercent Correct8065opt OPCopt 62-bit ECOC70CNAPS One-per-class6060555050451001000Training Set Size (words)401001000Training Set Size10000Figure 5: Accuracy backpropagation one-per-class error-correcting outputcoding configurations increasing training set sizes NETtalk letterrecognition tasks.Error-correcting output coding works best small training sets, statistically significant benefit. largest training set|16,000 examples|the one-per-classmethod slightly outperforms ECOC method.experiments, conclude error-correcting output coding workswell small samples, despite increased size decision trees increasedcomplexity training neural networks. Indeed, backpropagation letter recognition task, error-correcting output coding worked better small samples277fiDietterich & BakiriTable 8: Five random assignments codewords classes NETtalk task.column shows percentage letters correctly classified C4.5 decision trees.Multiclass One-per-class70.068.661-Bit Error-Correcting Code Replicationsbce73.8 73.6 73.5 73.873.3large ones. effect suggests ECOC works reducing variance learningalgorithm. small samples, variance higher, ECOC provide benefit.3.3.2 Assignment Codewords Classesresults reported thus far, codewords error-correcting codearbitrarily assigned classes learning task. conducted series experimentsNETtalk domain C4.5 determine whether randomly reassigning codewordsclasses effect success ECOC. Table 8 shows results fiverandom assignments codewords classes. statistically significant variationperformance different random assignments. consistent similarexperiments reported Bakiri (1991).3.3.3 Effect Tree PruningPruning decision trees important technique preventing overfitting. However,merit pruning varies one domain another. Figure 6 shows change performance due pruning eight domains three configurationsstudied paper: multiclass, one-per-class, error-correcting output coding.figure, see cases pruning makes statistically significantdifference performance (aside POS task, decreases performancethree configurations). Aside POS, one statistically significant changesinvolves ECOC configuration, two affect one-per-class configuration, oneaffects multiclass configuration. data suggest pruning occasionallymajor effect configurations. evidence suggest pruningaffects one configuration another.3.3.4 Class Probability Estimatesmany applications, important classifier cannot classify new caseswell also estimate probability new case belongs k classes.example, medical diagnosis, simple classifier might classify patient \healthy"because, given input features, likely class. However,non-zero probability patient life-threatening disease, right choicephysician may still prescribe therapy disease.mundane example involves automated reading handwritten postal codesenvelopes. classifier confident classification (i.e., estimated278fiPerformance relative pruninglkETNLettertaLElogyudioPOelowVGlassError-Correcting Output Codes10***50-2Pruning****C4.5 MulticlassC4.5 one-per-classC4.5 ECOCFigure 6: Change percentage points performance C4.5 without pruningthree configurations. Horizontal line indicates performance pruning.Asterisk indicates difference significant 0.05 level better.279fiDietterich & Bakiriprobabilities strong), proceed route envelope. However,uncertain, envelope \rejected", sent humanattempt read postal code process envelope (Wilkinson, Geist, Janet, et al.,1992).One way assess quality class probability estimates classifiercompute \rejection curve". learning algorithm classifies new case, requirealso output \confidence" level. plot curve showing percentagecorrectly classified test cases whose confidence level exceeds given value. rejection curveincreases smoothly demonstrates confidence level produced algorithmtransformed accurate probability measure.one-per-class neural networks, many researchers found differenceactivity class highest activity class second-highestactivity good measure confidence (e.g., LeCun et al., 1989). difference large,chosen class clearly much better others. difference small,chosen class nearly tied another class. measure appliedclass probability estimates produced C4.5.analogous measure confidence error-correcting output codes computedL1 distance vector B output probabilities bitcodewords classes. Specifically, employ difference L1distance second-nearest codeword L1 distance nearest codewordconfidence measure. difference large, algorithm quite confidentclassification decision. difference small, algorithm confident.Figure 7 compares rejection curves various configurations C4.5 backpropagation NETtalk task. curves constructed first running testexamples learned decision trees computing predicted class example confidence value prediction. generate point along curve,value chosen parameter , defines minimum required confidence.classified test examples processed determine percentage test exampleswhose confidence level less (these \rejected") percentage remaining examples correctly classified. value progressively incremented(starting 0) test examples rejected.lower left portion curve shows performance algorithmsmall, least confident cases rejected. upper right portion curveshows performance large, confident cases classified.Good class probability estimates produce curve rises smoothly monotonically.decreasing region rejection curve reveals cases confidence estimatelearning algorithm unrelated inversely related actual performancealgorithm.rejection curves often terminate prior rejecting 100% examples. occursfinal increment causes examples rejected. gives ideanumber examples algorithm highly confident classifications.curve terminates early, shows examples algorithmcould confidently classify.Figure 7, see that|with exception Multiclass configuration|the rejection curves various configurations C4.5 increase fairly smoothly,280fiError-Correcting Output CodesC4.5Backpropagation10010061-bit ECOC61-bit ECOC159-bit ECOC9595OPCOPC159-bit ECOCPercent Correct9090MulticlassDistributedDistributed8585808075757065700102030405060Percent Rejected7080901000102030405060Percent Rejected7080Figure 7: Rejection curves various configurations C4.5 backpropagationNETtalk task. \Distributed" curve plots behavior SejnowskiRosenberg distributed representation.producing acceptable confidence estimates. two error-correcting configurations smooth curves remain configurations. showsperformance advantage error-correcting output coding maintained confidencelevels|ECOC improves classification decisions examples, borderline ones.Similar behavior seen rejection curves backpropagation. configurations backpropagation give fairly smooth rejection curves. However, note159-bit code actually decreases high rejection rates. contrast, 61-bit code givesmonotonic curve eventually reaches 100%. seen behavior severalcases studied: extremely long error-correcting codes usually best methodlow rejection rates, high rejection rates, codes \intermediate" length (typically60-80 bits) behave better. explanation behavior.Figure 8 compares rejection curves various configurations C4.5 backpropagation ISOLET task. see ECOC approach markedly superioreither one-per-class multiclass approaches. figure illustrates another phenomenon frequently observed: curve multiclass C4.5 becomes quiteterminates early, one-per-class curve eventually surpasses it. suggestsmay opportunities improve class probability estimates produced C4.5multiclass trees. (Note employed \softened thresholds" experiments.)backpropagation rejection curves, ECOC approach consistently outperformsone-per-class approach close 100% correct. Note configurations backpropagation confidently classify 50% test examples100% accuracy.graphs, clear error-correcting approach (with codes intermediate length) provide confidence estimates least good providedstandard approaches multiclass problems.28190100fiDietterich & BakiriC4.5Backpropagation100101107-bit ECOC9845-bit ECOC1009630-bit ECOCMulticlassPercent Correct9499929098One Per Class8897868496One Per Class8280950102030405060Percent Rejected7080901000510152025Percent Rejected3035Figure 8: Rejection curves various configurations C4.5 backpropagationISOLET task.4. Conclusionspaper, experimentally compared four approaches multiclass learning problems:multiclass decision trees, one-per-class (OPC) approach, meaningful distributedoutput approach, error-correcting output coding (ECOC) approach. resultsclearly show ECOC approach superior three approaches.improvements provided ECOC approach quite substantial: improvementsorder ten percentage points observed several domains. Statistically significantimprovements observed six eight domains decision trees three fivedomains backpropagation.improvements also robust:ECOC improves decision trees neural networks;ECOC provides improvements even small sample sizes;improvements depend particular assignment codewords classes.error-correcting approach also provide estimates confidence classification decisions least accurate provided existing methods.additional costs employing error-correcting output codes. Decisiontrees learned using ECOC generally much larger complex trees constructed using one-per-class multiclass approaches. Neural networks learned usingECOC often require hidden units longer careful training obtainimproved performance (see Section 3.2). factors may argue using errorcorrecting output coding domains. example, domains importanthumans understand interpret induced decision trees, ECOC methodsappropriate, produce complex trees. domains training mustrapid completely autonomous, ECOC methods backpropagation cannotrecommended, potential encountering diculties training.2824045fiError-Correcting Output CodesFinally, found error-correcting codes intermediate length tend give betterconfidence estimates long error-correcting codes, even though long codesgive best generalization performance.many open problems require research. First foremost,important obtain deeper understanding ECOC method works. assumelearned hypotheses makes classification errors independently, codingtheory provides explanation: individual errors corrected codewords\far apart" output space. However, hypotheses learnedusing algorithm training data, would expect errors madeindividual hypotheses would highly correlated, errors cannot correctederror-correcting code. key open problem understand classificationerrors different bit positions fairly independent. error-correcting outputcode result independence?closely related open problem concerns relationship ECOC approachvarious \ensemble", \committee", \boosting" methods (Perrone & Cooper, 1993;Schapire, 1990; Freund, 1992). methods construct multiple hypotheses\vote" determine classification example. error-correcting code alsoviewed compact form voting certain number incorrect votescorrected. interesting difference standard ensemble methodsECOC approach ensemble methods, hypothesis attempting predictfunction, whereas ECOC approach, hypothesis predicts differentfunction. may reduce correlations hypotheses makeeffective \voters." Much work needed explore relationship.Another open question concerns relationship ECOC approachexible discriminant analysis technique Hastie, Tibshirani, Buja (In Press).method first employs one-per-class approach (e.g., neural networks)applies kind discriminant analysis outputs. discriminant analysis mapsoutputs k , 1 dimensional space class defined \center point". Newcases classified mapping space finding nearest \centerpoint" class. center points similar codewords continuousspace dimension k , 1. may ECOC method kind randomized,higher-dimensional variant approach.Finally, ECOC approach shows promise scaling neural networks largeclassification problems (with hundreds thousands classes) much better oneper-class method. good error-correcting code length nmuch less total number classes, whereas one-per-class approach requiresone output unit class. Networks thousands output units wouldexpensive dicult train. Future studies test scaling abilitydifferent approaches large classification tasks.Acknowledgementsauthors thank anonymous reviewers valuable suggestions improvedpresentation paper. authors also thank Prasad Tadepalli proof-reading283fiDietterich & Bakirifinal manuscript. authors gratefully acknowledge support National ScienceFoundation grants numbered IRI-8667316, CDA-9216172, IRI-9204129. Bakirialso thanks Bahrain University support doctoral research.ReferencesAdaptive Solutions (1992). CNAPS back-propagation guide. Tech. rep. 801-20030-04, Adaptive Solutions, Inc., Beaverton, OR.Bakiri, G. (1991). Converting English text speech: machine learning approach. Tech.rep. 91-30-2, Department Computer Science, Oregon State University, Corvallis,OR.Barnard, E., & Cole, R. A. (1989). neural-net training program based conjugategradient optimization. Tech. rep. CSE 89-014, Oregon Graduate Institute, Beaverton,OR.Bose, R. C., & Ray-Chaudhuri, D. K. (1960). class error-correcting binary groupcodes. Information Control, 3, 68{79.Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). ClassificationRegression Trees. Wadsworth International Group.Bridle, J. S. (1990). Training stochastic model recognition algorithms networkslead maximum mutual information estimation parameters. Touretzky, D. S.(Ed.), Neural Information Processing Systems, Vol. 2, pp. 211{217 San Francisco, CA.Morgan Kaufmann.Cardie, C. (1993). Using decision trees improve case-based learning. ProceedingsTenth International Conference Machine Learning, pp. 17{24 San Francisco,CA. Morgan Kaufmann.Duda, R. O., Machanik, J. W., & Singleton, R. C. (1963). Function modeling experiments.Tech. rep. 3605, Stanford Research Institute.Freund, Y. (1992). improved boosting algorithm implications learning complexity. Proc. 5th Annu. Workshop Comput. Learning Theory, pp. 391{398.ACM Press, New York, NY.Hampshire II, J. B., & Waibel, A. H. (1990). novel objective function improvedphoneme recognition using time-delay neural networks. IEEE Transactions NeuralNetworks, 1 (2), 216{228.Hastie, T., Tibshirani, R., & Buja, A. (In Press). Flexible discriminant analysis optimalscoring. Journal American Statistical Association.Hinton, G. (1989). Connectionist learning procedures. Artificial Intelligence, 40, 185{234.Hocquenghem, A. (1959). Codes corecteurs d'erreurs. Chiffres, 2, 147{156.284fiError-Correcting Output CodesKong, E. B., & Dietterich, T. G. (1995). error-correcting output coding worksdecision trees. Tech. rep., Department Computer Science, Oregon State University,Corvallis, OR.Lang, K. J., Hinton, G. E., & Waibel, A. (1990). time-delay neural network architectureisolated word recognition. Neural Networks, 3 (1), 23{43.LeCun, Y., Boser, B., Denker, J. S., Henderson, B., Howard, R. E., Hubbard, W., & Jackel,L. D. (1989). Backpropagation applied handwritten zip code recognition. NeuralComputation, 1 (4), 541{551.Murphy, P., & Aha, D. (1994). UCI repository machine learning databases [machinereadable data repository]. Tech. rep., University California, Irvine.Natarajan, B. K. (1991). Machine Learning: Theoretical Approach. Morgan Kaufmann,San Mateo, CA.Nilsson, N. J. (1965). Learning Machines. McGraw-Hill, New York.Perrone, M. P., & Cooper, L. N. (1993). networks disagree: Ensemble methodshybrid neural networks. Mammone, R. J. (Ed.), Neural networks speechimage processing. Chapman Hall.Peterson, W. W., & Weldon, Jr., E. J. (1972). Error-Correcting Codes. MIT Press, Cambridge, MA.Quinlan, J. R. (1993). C4.5: Programs Empirical Learning. Morgan Kaufmann, SanFrancisco, CA.Richard, M. D., & Lippmann, R. P. (1991). Neural network classifiers estimate bayesianposteriori probabilities. Neural Computation, 3 (4), 461{483.Rosenblatt, F. (1958). perceptron: probabilistic model information storageorganization brain. Psychological Review, 65 (6), 386{408.Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations error propagation. Parallel Distributed Processing { ExplorationsMicrostructure Cognition, chap. 8, pp. 318{362. MIT Press.Schapire, R. E. (1990). strength weak learnability. Machine Learning, 5 (2), 197{227.Sejnowski, T. J., & Rosenberg, C. R. (1987). Parallel networks learn pronounceenglish text. Journal Complex Systems, 1 (1), 145{168.Selman, B., Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiabilityproblems. Proceedings AAAI-92, pp. 440{446. AAAI/MIT Press.Snedecor, G. W., & Cochran, W. G. (1989). Statistical Methods. Iowa State UniversityPress, Ames, IA. Eighth Edition.Valiant, L. G. (1984). theory learnable. Commun. ACM, 27 (11), 1134{1142.285fiDietterich & BakiriWaibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition using time-delay networks. IEEE Transactions Acoustics, Speech, SignalProcessing, 37 (3), 328{339.Weigend, A. (1993). Measuring effective number dimensions backpropagationtraining. Proceedings 1993 Connectionist Models Summer School, pp. 335{342. Morgan Kaufmann, San Francisco, CA.Wilkinson, R. A., Geist, J., Janet, S., et al. (1992). first census optical character recognition systems conference. Tech. rep. NISTIR 4912, National Institute StandardsTechnology.286fiff fi!"$#%$&('*),+--.0/!+++21+3'-435678:9;<-.=>5#& 8?-0;-.@BADCFE0GHAIAIJALKNMPOIJE0QSRTQUGWVXAIJALKY?Z:[]\_^a`UY_bac2debw:xy{z}|e~t}z~yH3o~323F ,<<~3| 03yW" 3f3gahjiag_kmlafon paqarsn gtlun s,vZ^{^"?^?Z_`,("f3pa _kWs,_n a}l!paas%:n gtlun s,v!|e~a?o}o}} y3o~%x0 je 2ee _!7 !!uo o!:*%3:e7 u_, 7 X0$ U7 $ 3j__ 3ej!e$ UI3joe 3j?7 eU}3}oe77 },o 7$m$2 <2u 7ooeL $U($ }7%ee} a,$:_, U7 am$j$0oU ,3!$o0I} 3}0$0$ } WeX*:3 t< $ 37 ," e% 7$ W,$o$7 }oo$2 <2e: }7e7"! 7ooem $ $0a} $}oe_}u $t$ % $00< }}$"}:3$0U aW(<Loe $ ,L 0uo 7{!aa"! "" a0, ffIfi, !}fia3 !"a $#%&'a(a)*# + $,*-7!.!0/ ff21}a, fi# 3*a"!4fi,$5%a6ff7, "98ff,:#!,;$# 50/%<7, 2U 1a "!0>=5, ?(+/"+/ 0@! 4AB!o5ff7Wa<a a"%,$#oUa$1!aC28ff,:#!,;aDoIa!}fi! a3W oE, <F=5,G-7# &C% fi; e $a3 fiHfi,$#a $#ff2Iff0" $ aa a"%,$#}I!Jfi3!# &CK,M"/ Nfi9% 6 "B0O+P ,CQ $#R# a5CX+7,$#!SETUV%WX!Y[Z \L7,$#3,.o*WX^]_a`7bffX!cAWX^Y>%3ff,$#AffBfi+/MA#/ R# &0I,T UffB ",fi,7IfffiaM#&%d RB e5 ff, LfJ# &O+"fi,!<R}!Bgah !a3ff&fi;B!+\A,h"ff7, 2 A"351 $3 a!7, G0 fi% 7 JX753 +,Gmfi/ff, $I&Q51 (3 6!0/ Wff9iBW+jfiUff]_abff`}!+H fi, $#!J 7+/ L5"ff7, A5J1 A}3a# fia$H7 MB! O"ffBa, Zk% "#/ l+7"}34fi "+!0/ff, Oafi/% 3 $Ififi,] fiBl% 9an% !0O"+!0/%,"B e fi/ff, $&, ]e*!PZp!!o;C!,fiD7, Ra""+!0/ff, @a&/%, $Ifi,GZDq# &C!%E#/$"<+(J@a75J+,Ur ffBfi, $#no! sBa753 E%"4% ! *I!+3+,ZRq 3# &0%+{a0ff{affIBo7,tfffi, +/fi,ffu"De+2!3a"!4fi,"+!0/%0Oafi/ff, $&BaI!} aff fiMva!Im%7, ""!U fi%w,xZy l!+ F,{z 3C/ ?+/ '6 I+/ 3]"}34fi]B e2"+!0/%, J6afi/ff, $&"afi 0 6v a! mff7, "!,a"Z| 0# &0IN<"De77O"Aa+!JZ}+oaaffOffBfi, +/ E<# fi0I,s~ ? Nfiffff"4% !mo&/%, $IfiA7("}g<fi,"+!0/%, 0M +"l ' ,AI3De"Ra+!PaIfi'ff7, !",a3Z R0 9# &0+--3.:fiafi"$##$ 8: 8EU3O_ff6O5#& #I fio%&# #>8fiff$$*$$xO%$$ff$!$!$+$M+fiff%$+&><6""fiff$"D5">ff fi$M%&0$fff%fi+CJfi<3$P+fiff%$+&"!+!0ffff+ff5%$R6+ff+B$<B++%+PM$O+%7Mfi;BMR^&$fiffE$$+&%ff$+fi[;[$ff@%fi+ff6R27ff +7!3!&B4fi<Hff|D%fi+ff9< fffi0Iff$$!&(B! M!J$ffM$MJ!ff!fi.7*!J(B$+2&7tff7fftRB![M5ff+'&'Ds<%$3t$+ff2!A$+fiffff$ +&R!+!0ffRff+ff5%$' Bl75J2 fiRff!7+fit%++fi %$75J6B! 7fi5m%B$MMff<%*g0O5%+D $+$P+fiffff$ +& ^+!0ff ff+ff%<$"A4ff$%7ffG7$$!&!P$+B&7Pff7ffl!^'!ffl+ff++%+57P5+GI fiO+ff7ff%+0 0hffBfi+ff5dff&0IfffiP>$M$>$Peff^!57E5ff!!^ff+ff5%$R$Pfi<<$275JMfiBfi($+$+6B$P+&%ff$+fi6^+!0ff ff+ff%<$ffOff+%5%$(BM60$+fi5ff ff7ffRMBPff+O!ffBAP$fi70@75Jff@[!0ff!$fi$M$fi+fiD7%$+!MJdff<%!E% $+6B$M ff$A6"+ff7ff4ff+$+5CK%}D0M^+7>ff|7ff 6ff7ff!G!Jfi!6B$ &$A^+O!&g<fi"!+!0%<%5$+fiff%$+&E>I:ffF!ff7E$Ofi$fiff![t!ff%P$O+fiff%$+&F5fi 0ff%$ff7ff!9!PD0RffwRI0<$BA{fiJm$D7!&B4fi<R!+!0%ff5PJm$+fi%<Bff$+fiff+ff%<$'*DMfi$fiffffD0l+ff+fi!0'+$ff$ffJ!ff$B$2!+!0ffffRP$+fiffff$ +&"DB$&fi7M$A$fi+fiDff7ff!ff$D7!+B+ fi"$$>+D% ff!^fiG&aMff!a&ffQfi;E!^+6%!7Q$"+ff3sG$fi$R fiMfi$!$fiAff!+&B57$Aff$>7ff0>&+a>AJO!!%$2;gKffBfffi>I:ff ffBfi+ffDB$ff&0+&J5%PfiBD^&4fiP!+!0%<%O$ +fi%<Bff$+fiE$6ff!C!ff$5ff%mB$%$(B!OPff+ fi;J6B74Iffffff$3$hP77ff*fi!$PA!ff4">5 +m%D$+(s;J$fi<$2fiBfi<$+!(!% Iwff7fflA(%!$Aff$$5<%!0sff7^5mM!fiR.Bfi;!^+!7fifi %;7$fffi72!+7++$+D07&^+$7+fi5<%&!A^+7m++ $+D0*ff$$5<%!0s++%ff7+4ff<%! $AMB;%4s$fi$2 fi<Jfi$2!ff7ffB$7f%$$57ff!C++%ff7O!H+$7+fi57ffR7&^+GIff7ff2P6%! +ff+50$+!4ff!PBDff$P!%43+ff"I'ff7ffxA$$Pff$">5+ff2!*B7fi5l$!+7!7ffARffBe@RO'2C4%e$D%ff!g;fi+R$$xNJfiR fi!0ff2ff<ff$n!fiff}$2!7D!fi;5%+x>ffw%+0(fiffDC< %!4fi+nw5$RI+*7h+ $+$+lDfiff!7+BC7ff!RB!n+J^*Dfffi%w!7+fi27ff}P7ff!R5Jff+tfit$*fihBff2D0B!$ffR$7ff!+C<%!+.NffM!fiffB$DfihBffMD0*!2fi2ff!7+ff $+$+A$!7+fi[fi&$3$7$E[fi<%(%M!3 $+$+G$O!<+fi!ff[fiffAff+ !+5B$ffff7 ff$"ff!7+BC7ff!dDfffifffi$6fi$ffED0A$+35ff"Dff7fffi fiBDR3R$ffRA$+ff!7+7ff!+%7RffR$7P$+ff!7+7ff!+N$ffi$ffD0$+RBHB$fi&727$!7%$+fiMff7ff@J!.2ffM$+3M 7P!&B4fi<g$<9!P$A!<+fiPAfffi7fi+%$$$P$3Nff"!$B(Js$ffB!0g$B2"$ff"&$ *$7ff7I J$50ff$5"$Dff7ffw5ff+*fi$Dfi$ffPRfi+^+!'ff*$DffB!0@7$l$A!7+fi$6fi$ffE$+$EMfi+ff7M"fiR!Ee5% $ !<+fi>ff fidfi70@ff!7+BCfiP75M9ff&0!75BP>I:ffff$&$4+0D75Jfi<P75Jfi5$"fih%m+%!7+ff B$ !7+fimffBfffi "!$#$#&%'()**()'( '+ fi,-.0/!21'31141567!2()8941*!2:fi0;<(6#$#2(.!2,=1'>7!2( @?A6/"!$#$#B%C!2/,7!$DE/+.!2fiF4( 7!G#H!2fi,I'3#ff41567!2()J17!2K>LL)MfiNOQPSRTHOHOHUVOffWYXZHU<R[8\ff[@TH]HOHUVOffW^ff_a`cbedf`<b d`<g h@iejbffkmlchjanbJoj@p qCjrff_@sHdCd6tHhujvp dsffj,le`cbwsHnxzy`5{$h@{<idtHhj@p dsffjElffhbA|v`<n_@bHg hbedCrffhtffj,|v`c_vn}kesHn`<bH~:dtHhh Hhp sHd`<_@b_oIdtHhJ^el9jb {+hbffp h@i.hJ~@hdj bffjd6sHn)j,lIq`<dsffjd`<_@bYtHhnh &lVjabHbe`cbH~:te`Gl<hhjn6be`cbH~z`9qbHhp hq6qjnx@{sHnqhp _@bffkh jgz^elch`Vqd j@hbon6_@gjQd6n)jbffq^_@nd)jd`c_vbke_@g-jE`cb { hdy*)z}rffhjkv`<nhp dhkQ~@n j^Ht iCtHhnh:d6tHh-|@hn6d`9p hq kehbH_@dhzl<_Bpjd`c_vbffqJ`<bjtH_eqd`l<hhbe|@`<n_@bHgzhbAd{utHh:hke~@hqkehbH_@dh:qj,ohn_@sHdhqmon_@g_vbHhlc_Hpjd`<_@bd6_jbH_@d6tHhniCdtffjdjnhd)j@hbQd_8rffh:_@bHhj,xQy5d_vuj,xn_@sHd6hqIjnh0kehqp n`<rffhk8j@qIjm^ffj,`<n_ao"_vbHhuj,xJn_@sHdhq } { bHhj,xn_@sHdhq&`<b:dte`9qC^ffjnd`Vp sel9jn0ke_@g-j,`<b_Hpp sHnj@qj nhqselcd0_oCdtHhqdnsffp dsHnh_ao&dtHhJhbA|v`<n_@bHg hbedjbffk:_oCdtHhJ|@hte`9p6lchsffqhk8rex-d6tHhj~@hbed{bffp hnd)j,`<bed>xQ`<bdte`9qzdn)jbffq^_@nd)jad`<_@bke_@g-j,`<bjn`9qhqon_@gdtHhojvp d-dtffjad dtHhnh`Vq`<bffp _@g ^el<hdh`<beo_vng-jd`<_@bjarff_@sHd0dtHhhbffke^ff_a`cbed_oIq_@g hn6_@sHdhq*_@n`c~a`cbffjad`<bH~ on_@gq_@gzh^ffjnd`9p sel9jnlc_Hpjd`<_@bffqy`5{$h@{<idtHh g-ja^Y`9q^ffjnd`VjElGl<xsHbHebH_b.}){z5bq_@g h pj@qhqmdte`9qm`cbffp _vg ^el<hdh `<beo_@ng-jad`<_@bp _@bffp hnbffq_@bel<xjqg-jElGlCbesHgrhn_aoClc_Hpjd`<_@bffqjabffk:n_@sHd6hqtHhn6h+dtHhbesHgrhn_aoI^ff_Aqq>`crel<h hbffke^_`<bAd q0_ojn_@sHd6h*`9qIj,l9q_qg-j,ll5{20b-j~@hbedIg _|v`<bH~j,l<_@bH~JdtHhqhn_vsHdhqCebH_qfdtHh0^ff_Aqq>`crel<hj,l<dhnbffjd`c|vhqCo_@ndtHh+qd6nsffp dsHnh0_oSdtHhhbe|v`cn6_@bHg hbedjbffkpjb`9kehbAd`ox:dtHh0l<_Bpjad`<_@bffq`<djnn`c|vhqjad{C*tHh_@rHwhp d`<|@h_oCdtHh+j~@hbed*`9q*d_znhj@p6tj~`<|@hb:d)jn6~@hdlc_Hpjd`<_@bYqd)jn6d`<bH~on6_@gj~`<|@hb:`<be`<d`9j,lfl<_Bpjd`c_vb {utHhjarff_|@hh jag ^el<hq+jnhd)ja@hb8on_vgnhjElc7l`ohq`<dsffjd`<_@bffq{*tHhxjnhdxe^e`VpjElq`<dsffjd`<_@bffq0_orff_vsHbffkehksHbffp hnd)j,`<bed>x@{u`cg`Gl9jn0q`<dsffjd`<_@bffq_Hpp sHntHhbHh|@hn*h0tffj,|@hd_J_@^ffhn jdhjg-j@p6te`<bHhdtffjd*_@nHq*`<bQ_@bHh_ouqh|@hn)j,lI_@^Hd`c_vbffq{5bgjbAx8_od6tH_Aqh pj@qhqidtHh ^_Aqq`<rel<h-_vrffqhn|Ejad`<_@bffq+pjbYrhqd)jadhk.iCjbffkYdtHh:qhd_o^ff_Aq6q`<relch:hbe|v`cn6_@bHg hbedrhtffj,|@`<_@n)qzpjbrffhl`9qdhk.dtHh:j@p dsffj,l*rffhtffj,|v`c_vnitH_*h|@hnig-j,x0rffhsHbHAbH_bj^Hn`<_@n`5{fll<sHg`<bffjd`<bH~+n6hqsel<d)q n6h~Ajn)kv`<bH~0dtHhqhIh ffjg ^el<hqfjnh`cgz^elG`<hkrex-_@sHn0qdsffkex@{Ih|@hndtHh l<hqqiff*h0ffn)qdtffj|vhd_keh ffbHh+_@sHnrffjvq`9p0on)jg h*_@n.{v <V C0b@,ff7ff,9@ffy> 6- ).8}+p _@bffq`9qd)q_omjYqhd_oA,),AY@-zijqhd:_aoB Y@,9@-ijb7ff79@@: jbffk@@e@@H79@meff,9v-. :d6tffjdkehdhng`<bHhquo_@n*hj@p6tqd)jadh++8jbffk:j@p d`c_vb-dtHhbHh Hdqd)jdh,ff"Yy>e)}){@j qhk_@b-d6tHh+jrff_|@h+keh ffbe`<d`<_@bY*hpjabkeh ffbHh+tffjdj &l9jbHbe`<bH~-te`l<h hjnbe`<bH~-qxBqd6hg`9q{_@d`9p hd6tffjdI*hj@qq_Hp6`9jdh0dtHh*`<beo_vng-j,l dhng rffhtffj,|v`<_@n)J*`<dtdtHhd6hng d6n)jbffq`<d`<_@bzosHbffp d`c_vbffHitHhnhd6tHhj@p dsffjEl"rhtffj,|@`<_@n`9qdtHh+jvp dsffj,l dn)jbffq>`cd`c_vb:osHbffp d`<_@b {v <V 7*@ff9efe9Y @)9e,yf}p _vbffq`9qd)q_ojbj~vhbAdhbe|@`<n_@bHgzhbAd:qxHqdhg) .Y})i*jbffkjqhd _oH)@79v:A7@H0,<<< ei&jElGlqtffjan`<bH~d6tHh qjag h-qhd_o_@rffqhn6|Ejrel<hqd)jadhq+ziStHhnh-d6tHh j@p d6sffj,ldn jbffq`<d`<_@bosHbffp d`<_@b:`9q_@bHh_oCdtHhqh^ff_eqq`<rel<hd6n)jbffq`<d`<_@bosHbffp d`<_@bffq{_@d`9p hdtffjd0*hsffqhkYdtHhdhngA,),Az@n)jdtHhnd6tffjb wsffqd+qd)jadhq{0bY_@rffqhn|jrel<hq d)jadh_oujbQj~@hbed0`VqmtffjddtHh j~vhbAd^hn)p h `<|@hqjd+j:~`<|@hb^ff_`<bed y5h@{$~{<i`<d)q^HtexBq>`VpjElClc_Hpjd`<_@b.}n)jd6tHhn dtffjb0`<d)qfp _@g ^el<hdh*qd)jdhC_aoBebH_*l<hke~@h@{ hj@q6qsHg hCdtffjdfjbj~vhbAd pjbj,l<uj,xHq kv`9qd`<bH~@se`9qtrffhd*hhbkv` hnhbAd_@rffqhn|jrel<hqd)jadhq{C*tHh+p _@g ^el<hdh+qd)jdh0_oCAbH_*l<hke~@h+pjb:rh+keh ffbHhk8rffj@qhk_@bdtHhte`9qd_@n6x_oj@p d`<_@bffq jbffkQ_vrffqhn|Ejarelchqd)jdhqJ_od6tHhj~@hbed{*te`9q te`9qd6_@nxY`9q jb_vn)kehnhkqh vsHhbffp h-_aou_vrffqhn|Ejarelch:qd)jadhq0dtHh-j~vhbAd|v`9q`<dhkjabffkj@p d`<_@bffq`cdJ^ffhno_@ngzhk.{ H_vnh jgz^elch@i`ojbj~vhbAd^ffhno_@ng hk:jabj@p d`<_@bdtffjadIl<hk on_vg jab-_@rffqhn|Ejrel<hqd)jdh d_ jb:_@rffqhn|Ejrel<hqd)jdhfiff!#"$&%&'$$)(*+'-,.'$/0,1%2'-43*56-7859!+':";<'-="6>0";)?38?@A"B.'-59C.'-D@Eff 830"B?.'->AF:"B?G"B33$'->6"B.'-59H%&#3*6"BIJ"B(*598)8#!+(*@?K56LM8>?A"'-N?5EO'-+-5EPQ"B?.'-!+'->EM%<fi'$-G8G8!R(*?56L3*59.'-(fi'$$':.':SLT5E?<6"B>AU>A ?5EV'-V(*598,8@, (7 "+!#"$WW9X>598A"6fffiY[Zjk*lt6}m*noHpJqrm[oCrnt1o{u{rs4nt=t6}{vs={nu0wqTro{drNvsJnm^sBvnEtm*nonm*t6t#{nw}t6}t6}6|Hu*xOt6}4Ku*yr~drltB}sKt6u1t6}r46rm[o{r s}r#n{o{rt6r6JvTnu[{qonrronu[t Eu[n{xsr4u[rNvt9squrrwmzu*~[rRCuo{rEqsrd6rd{v6r t6}Usru*xym*rEvn<lU[[u*Gs6t6}r odvs6Es6svu[nus;rns66}rEvnrGEu[C1u[nr[l<u[n{q|Crntr}m8~dvu[9sSvtwEudnsvo{r9sSvsm8qstEm*t6r=vs=n6rsrn{t9m*t;vTudnsl)mdst6}vsVxCrntO}{vsSm*1rn{t9m8qm*no1s;u[1rRrEu*m*{qr t6u1o{rEn6u[{qrvsVx6t6}u[t=nr u[nrErs6sBm*vq|6rr4t6}nrn}u*qt6[lOd*9t6rnsvu[nsSu*xvtOvqq)r#odvsBEs6sroJvTn[rEtvu[nrUm[sv6u[{qrvnVqm*nrRodvs6EsBsrovn1t6}N#}{vqTr=&rm*6n{vnrxu*qqu*vnn{vn#}{vqr&rm*6n{vns|rm*[rn{t#Nl[vsUms;t9m0vTnr46rxBu[E[lno{rUm*n|Nu{s6sv{qr4t69mznsvtvu[nNx=ru*xMm[Etvu[nsSt6}m*tymz6r#rE6rs;rnt9mztvu[nHu*xOt6}s6m*tvsxAm[Et6ud6|{qm*nvst6}r4Vqm*nn{vnt6ro=vnmEu[N#}{vqTr1&rmz6n{vnr6rExu[6rm{qmznvn#}{v6}t6}Crnt=r}m8~dvu[lvnu[Eo{r4t6u[m*9m*n{t6rr=t6}st6rlK#}srtGuzxt6}}m[stBu4noHmJr6rBuErs6sfil&t6ur=st9m*tBrsyCs|s@vTtBm*tvu[nsU#}6u[rUEu[={vTnmztvu[nHu*xVqTrmz6n{vnu1qu[nm*noHm[Etvn1u[=vm8qq|st6r9rm*[rn{t1qrm*6nsrnrHm[B}{vTr~dr1rntu*x#t6}r6rvt9sU[um8qVvsUnrm*drntnEt;vTudnNvTnOM{qmzn9sr#uzx)vt+vs+u*q|{nu[[}m*u[tHt6}rrm*drnt [um0q#udtvErr9m8qAlOm{qm*nJvTd}t=rN~[r6|Eu[C{qTrE<Gnbm*[rn{t4=v[}{t1m*6v~[r[lvnt6}ro&lvxRrrUus6s@vT{qrr}m8~[vu[EsnEtvu[nxBu[vTtEs#}{vst6ud6|uzxSstEm*t6rsUvn^=ym*noHmdEtvu[ns vTn1RtBum*nstEm*6tvnr#n{srEtvu[n1m*novTn{~[rst;vT{m*t6rovT~drnmJ[um8q<<lmQd.Qd[ &:d1vsUm1{qm*n^t6}mzt#[m*9mznt6rrswt6}m*tUt6}vqqM6rm[B}m=st9m*tBr#vnNt6}m*t8l&vnb[rnn{vn*v~[rn1m*n{|4us6sv{qrGr}m8~[vu[Ou*xu*qo&l<m*nos@vTJv:qm*6rs{qt9sUm*n^r=u[&rt@BVrmbVqm*nvsm8qqTro^.{vxMt6}no{rovTnHtB}BrsrntBrorsUodvs6Es6srovnKvsBE6rt6rr m*[rn{tytBum[B}{vr~[rUvt9s+[um8q&u[n{q|1vnmUx9m[Et;vTudnAr[ld{1MuzxMt6}Gm*novs==6}Cu[6r}m*lR[d{Gm*novTnu[6vTnU t6}m*t vnEu[Bu[9mzt6rsrnt4srEt;vTudns *v=vqm*o{rEn{vtvu[ns}m[Etvu[nvn^19vt9sKqrm*6n{vnr6rUvtUnrm*nou[sr6~{6rswvt6}^u[t6}OUm8qr6nusrsld*ru[BrtvU1uo{rEqs@usrs#rn)[=xu[Gm*n^m*[rn{t+vsUmxrEvn<l[[t6u[Nm*t9m*.qv[rs;t66Et6@C6EE929lm*noH^G*8E[+[8[dxu[#tB}rn{~[v6u[nEt6u[l*K}{v:qrst6rNs @Um8qrBnusrsfilMd*Um8qr6n<lrHm*[rn{t-sUqu@#m*Nm[o{[ru[nr4m*BrGnNdy<<)Uudro^{|m[Etvn{q:vBvtq|[tBu[m*t9m@us;rns66}t6ros|6u[{qrvsSt6uKno1mGsBm*tvsxAm[Et6u[6| {qm*nCt6}m*tRm[B}{vr~[rsOmU[um8qrrn{~[v6u[nvnsrGo{urEt6u[8-s4m[Et;vTudns *v~[rnt6}1r=t6}mzt4m*nm*[rn{tm[EtEs m[sKvxyvtvsmCn{vt6rAst9m*tBr1m[B}{vnm8qGst9m*t6r@usrnsB6}v~[rnHtB}{vsw1uo{rEqAl}6rsrn{t6rorE=ru*x<usBsv{qTru[sr6~*m*tvu[ns+m*no1us6sv{qr rn{~[v6u[nnEr6t9m0vTn{t|tBuEu[n{t66u*qt6}t6}{q:vBvtq|[ }{vs#zvT~drsGsBvnEt4m*nosrEx{qM6rw}rxAm[Et9swvtUqTrmz6nudt r=6rrsvt6mzt6rom*ro{um*tvu[nsfil4m*no1rn{tKAn^t6}r=m*[rn{t-s#o{rBvsvu[nsUvqqOr4mdsroHu[nvt9s#}{vstBu[6|Nu*xRu[sr6~*m*tvu[nsUm*noHm[Etvu[nsK#}{vB}~drntC[|stBrs=@rqTurrn~dv6u[nr=m*[rn{tm8|16rm[B}m1st9mzt6r #}Crntl&mdsrou[nt6}Eu[1{qrEbt6}m*nvt9s=u[srB~0m*{qrst9mzt6r[}6r[srsu*xRvtro{u[n<t4m[s6st6}m*t+t6}t6rsyvTn^t6}rNrE<rEt9sUu*x#t6}st6uHt6}novnt6}n{vt6r[#}r1m[Et6m0qM6u[r#us6sv{qrGudsr6~0mztvu[nsOu*x&t6}l[[{m*noH{nt9m8vn6rsrn{t9m*tvu[n}m[s=m8q6rm[o{|nEtvu[nbEu[66rs;u[norsr1Eu[1{qrEstEm*t6rsKnrrEvn<l[d{2mzno vn=u[6 u[n=6rmdsu[n{vnr#u[s;r6~0m*{qr st9m*tBrsOm*6rt6}r=rn{~[v6u[nu*t6}m*tt6}^t6}{vsmdEtvu[n<l9u*qro{[rvTnodvs;t6vw}rsxu*qqu*vnr^t69m*ns;u[6tEm*tvu[no{u[m8vnrEm*1{qrmz6r^t6}[d{Vq:vt6rEm*t6rnHyr4mzns6m8|1t6}m*trm*[rn{t4vqqG{nrstEm*t6r=vt=6rm[B}t6u[Nm*t9mU@wusrns6B}u*x t6}{vs=t6u[{vm*nbrxu[m*rEq{qvnu[t1*w}u[t#*ltB}rHu[sr6~*m*{qrst9m*t6rNjk*}{vs=rnm*{qrsCst6u^u[nEtvu[nEu[66rsu[notUt6}g6hi1rn{t#r}m~dvu[vsKnr m*tvrn{tOnudt6}HrEmz1{qrs#t6}Eu[1{qrExm[EtEs m*u[ef^t6u^vnbt6}u0wqTro{dr1@Um8qr6n^usrsl<8[*m[Et6m0q)vn*c[_6rs;rnt9mztvu[nuzxKmz[rntEs}{vs=t|{rNu*x#6rrudsr6~0mz{qTrbst9mzt6rs=vnt6}m*6r#o{u[m0vTn<ldt6}_Crnt=r}m8~dvu[vsnrEvn<-sRs@vTtBm*t6ro=m*rmdEt6m8qytB9m*nsvtvu[n^xvn`baRcd_u[t v1{qvro|tB}9m0qy6rr=m[Et6m8qMtB9m*nsvtvu[nxt69mzrNrn{~[v6u[n_r rn~dv6u[nudt4r}m8~[rHm[Eu[EodvTnrrnCsrovTnNusrns6B}m*u[Z^ZsKt6u1jkwv:xymznoNu[n{q|1vxOt6}rHm*[rn{tE[99=tB}m*t4t6}rn{~[v6u[n\*]rEuddr4m[6}{vr~*m*{qr[KrnEr[l<t6}9sr1u*xr1m*drnt}mdsrsOs6m*tvsxAm[Et6ud6|1{qm*nm*nHrfififf ffffffffff!#"%$'&($)+*#,-*.)/$01"2,3"456 7 $'$8&fi9$'7 $:$*019;$)/<#$:"2,*, ,=401"2*>6 $)&("6 9?*A@*B"74>CD*E5;4#F,=$'7 !G*EF/H%$,=6I*>6 $*>5)*>5.*06="45.6 48F$@$'7JCK471LM$):"5.6 9*E6N,=6I*E6 $OQP(9/"2,R"2,N*8<$'5$'7*BHS7 $'@7 $,=$'5/6I*>6="454>C045)#"6="45*BH@/H2*>5,'OTU5+$VW01"$'5X6:@/H2*>5&("YHYHD619$'7 $CK4#7 $M047 7 $,=@Z45)[6 4\*\)/$01"2,3"45]6 71$'$^4ECD@Z4>H_X54#L:"2*BH-)/$'@6 9SO` 46="20$a94>&($'!$'7a/6 9*>6fi619$b,3"c'$b4>Cd*>5.$VW01"$'5/6-@/H2*>5Le*B_W,=6J"fHYHFZ$b$g@45$'5/6="2*BHhOiQjMkmlenSoqpdrtsusv3nSwduyxUz{sErR|Q}~ :5 6 9$R@7 $'!#"%4#,Q,=$06="45:&($N)/$5$)M*F*,3"20tCK7*>LM$'&(47 U4>C{H2*>55/"5<8&fi9/"YH%$fiy$*E7 5/"5<aX*>5)8&fi9*E6*, *>6J",C*06147 _@/H2*>5ACK47d*E5*E<$'5X6Q",O ~ 5M6 9/"2,d,J$06="45e*>5)8"5M619$(CK4EHfH4>&("5<b4#5$,Q&($(&N4#/H)^Hf"$U6 4045,3"2)/$'7U6 9$04LM@/H$gX"6=_.4>CS5)#"5<.,=019\*b@/H2*>547fi0 9$01#"%5<A6 9*>6fi*:<>"!$'5.@/H2*>5"2,fi, *E6="2,3C*#06 47 _O~ 547)/$'7fi6 4.)#",10, ,U6 9$"2, ,J$^4ECN04LM@/H$gX"6=_\&($5$'$)6 4.)#"2, 0, ,U47ULM$*,=71$,fi4>Cd04LM@/H$g/"6_a*>5)6 9$:6=_X@Z$b4>C(7 $'@7 $,=$'5/6I*>6J"%4#5,fi4>CNH2*>55/"5<\&fi9/"YH$W$*>7 5/"5<\,=_,=6 $'Le,&($^&(4/H2)HY"$M6 4AH4X4*>6OS8\AB'Z'$&("YHYHd)#",J6="5</"2,=9F$'6=&($'$'5\6 97 $'$F*#,3"20b{H2*>55/"5<.&fi9/"YH$^$*>7 5/"5<\,=_,=6 $'L7 $'@7 $,=$'5/6I*>6J"%4#5,'OU ZBdA{B'SS( 46 9.6 9$U5/L^FZ$'7fi4>C*E<$'5X6B,R4F,=$'7 !>*>F/H$b,=6I*>6 $,"hO$Oae*>5):6 9$fi5/L:F$'7(4>CS@4/, ,3"F/H$6 7I*>5,3"6="45CK506="45,RLe*B_8F$U$g@Z45$'5X6J"*GH%U"56 9$-,3"c'$U4>CS6 9$*06 *BHS7 $'@7 $,J$'5X6I*E6="45SOOUe 'MB'Z'SS P(9$(5/L:F$'7Q4>C*><#$'5X6,y4F,=$'7 !>*>F/H$-,=6I*E6 $,yL:"<9/6F$M$g@45$'5/6="2*BH("5?619$e,3"c'$e4ECD619$e,=_,=6 $'L7 $'@7 $,J$'5X6I*E6="45Sa{F6b6 9$M5/L:F$'7:4>C(@4X, ,"%F/H$6 7I*>5,3"6="45AC506="45,R"2,fi*>6(LM4X,J6d@4EH%_/54L:"2*BH"%56 9*>6(,3"c'$ONP(9/"2,R"2,(*8LM4X,=6fi*>@@Z$*BHY"5<e6=_/@$4>C7 $'@7 $,J$'5X6I*E6="45.C47fi,=_,=6 $'Le,(&D"%619.F45)/$)50$'7 6I*G"%5/6=_\=-*BH@Z$'7 5;d*>7I)#"ha IOP(9$.6 7I*>L*>h0'*>7 $e,J_,=61$'LLA$'5X6="45$)"5$06="45 "2,M*>5]$g*>LM@/H$\4EC*,=_,J6 $'L&("6 9**,"%LA4)/$'7I*E6 $\7 $'@71$,=$'5X6*>6="45SO ~ 5q,=0 9*?,J_,=61$'L&($.,=*BHYH_9*B!$;*,=$'6M4>C^*E6 4L:"204F,=$'7 !>*>6="45,h$O<OaX&9$'6 9$'7(6 9$F/H4X4).@7 $,1,=7 $U"2,(9/"<9.47RH4>&IOQP(9$5/L^FZ$'7fi4>C*E6 4L:"204F,=$'7 !>*>6="45,t"2,RHY"%5$*E7("5.6 9$-@714F/H$'L\,R"5@6aF6(6 9$U5/L^FZ$'7fi4>C@Z4X, ,3"F/H$b4#F,=$'7 !G*E6="45,"hO$OaB4F,=$'7 !>*>F/H$D,J6I*>6 $,y&9/"019M*>7 $Q6 @/H$,{4>C*E6 4L:"20Q4#F,=$'7 !G*E6="45,Z",$g@45$'5/6="2*BHhOdP(9$tHf"2,=64>CS@4X, ,"%F/H$U"5>=7="$,(6 9*>6Q6 9$U@*>6="$'5/6dL:"<9/6d9*B!$D",R,=*GHfH_@4>H_/54L:"2*BHy"5e619$-@7 4#F/H%$'L,"%5@6BOMfi$'50$a&N$:<$'6b*.#*,3"LM4)/$'7I*>61$M7 $'@7 $,=$'5/6I*>6J"%4#5;4ECD*.{H2*>55/"5<&fi9/"YH%$.y$*>715/"%5<,=_,J6 $'L\O'.A{B'SSfi 4619M6 9$fi5/L:F$'7(4>C*><$'5/6,{4F,J$'7 !G*>F/H$b,=6*>6 $,Q*>5)A6 9$5XL:F$'74>C@4/, ,3"F/H$b6 7I*E5,3"6="45CK506J"%4#5,R"2,(@4>H_/54L:"2*BHS"56 9$-71$'@7 $,=$'5/6I*>6="45\,3"c'$OdPD9/",6_/@$84>Cd7 $'@7 $,J$'5X6I*E6="45",H$, ,U<#$'5$'7I*BHd6 9*>5*e#*,3"LM4)/$'7I*>61$^71$'@7 $,=$'5/6I*>6="45SayF6U"6",,=6="YHYH$g@7 $, ,3"!$*>5)M04LA@/H%$'61$H%_5456 7J"%!#"2*BH*#,Q&($fi&("YHYHH2*>6 $'7()#"2, 0, ,'O(P(9$fi6 7*>5,=@4#7 6I*>6="45)/4Le*B"5$g*>LM@/H$4>Cb6 9$\@71$'!"4,,=$06="45",LA4)/$'7I*E6 $H_]7 $'@7 $,=$'5/6 $)]F/_]*+<7*>@9HY"%#$,=6 7 06 71$aS"50'*,J$,b&9$'7 $e619$'7 $\*>7 $e*E6^LA4X,=6b@Z4>H_X54#L:"2*BHYH%_Le*>5/_+*BH6 $'7 5*>6="!$,C47:6 9$*06 *BH,J6 7 06 7 $84>Cd6 9*>6U<7I*E@9+h$O<ZOa/"5+*M@*>716="20/H*E7b*>@@/HY"0'*E6="45SaS6 9$'71$*E7 $^*045,=6I*E5X65XL:F$'74>C@Z4X, ,3"F/"YHY"6="$,6 4e*^H%4#<X*>7="6 9L:"205XL:F$'74>C7146 $,IOb:y(bB:='G='fJ- B\> 'BIRfJ-y3h BG1N 1:Y I>1'dfJ1='G='fJBd =GJB='h1b=fi>]d#3. %/' ff1fi ffZ'( = '/'> = h !#"$%& ')(*/'>= ,+'^-.//'0>1e 1 ='X>)1h #"2$&% ')(*/'0> ='/0>!3#4+565798:>fi;,fifi<8>= > fi;? ! =1@"A#BDCFEHGI JDKMLHN#OQP2R4SGS J)NUTVGWXLGSYfiZN4[JYI\ )]'^M7_8Efi%;`fifi<8a=y E fi;b = @cQ1' @> 'de^f%>)e8gf> ;&>h(f ':/' ""j TS [eGKffSGklWY4mn f-o&%;' 'p fi8:>Hq{ ff;'/U 2 ->: 'r%f : 2f%s0>)e8<8)>04c>,d A0ffot'/:e8!3A%"hQ798:>fi;,fifi<8=y >1fi%;M = (1>re8<89%/. fi: f>1;%%c]#';1%ufi8vff='/;: gfi8:>@E]'!w*&%;A >( fi: h fi8<% xfi8:>d: >: 'r%f#H hf%s0ff()e8<8),X10%f0ffufi8d*r"%")cZ ' %fb' /qU J'X0E),: Ut'/:e82]'<zVf>)/f (0ffo# (t{'/:e8)Mff+0"| " }~h! J*T4YQh[ GK6SGklWYVm >: 'r%f#fi8:>,*fi1ff;'/% J fi 1 ='X>)*r"%")c{ff8)X%82% : 1 ='X>)@6&ufi8%x+0cVEaf1 f-o%;,fi' 'sfi8E>: 'r%f-f>@uZ>f>1 d%fi?f-'/8),@{ff8)X%8)M%"=1 { >U8w8/ fi: pfE ;%d ffyu:U :e8V>8' -l-f cfffifi:f1>dX 8<8<);'// ');'"%")c:d>4+ ;%;# =0 h %Mfi8:>xq# !ff8)]%;,y%ufi8@"h(;%; =1 `fi8E` J'X E]'<z 4"]gsw8: ,]'<zVf>!3#f - c( 'bt%fhre<8) H U1 1 4cV> >/ 3;#'ee -.;''0E >:fi8:>#"gDfi:e8l>A' hf : 8fi:fe8l 683!3#,*/ ');,%ufi8 " / ');' ;ff)]'@f-<zVfy&ufi8%/c#>/e =:'t''fs. !;%; =;,fi8:>#"H(yfi8E. J%fi8:u\1 ='/ >]'!wz ?f-'/8)%"*b /fi8E^: A, >: 'r%f-%cd ?f-'X]'<zVf>)/f - h8)f>re<8) >dfi* y/ ');' c{fid ,f1 Jb;''0> hU^fi8:>#c#'0fff"2(fi: 2ff%f1y% lE9z0 =2%/(tfi8<f-M #8< '0>- %cu > Jye8_6' ffMf n E fi>@ e Mf n E fiF e c &% +2fi' X%K'1 ?& U ,: !%2%;06@"(fi: fffi%f /: /' +1>#> J /ad>0%f:fe8 '>) cg f1% M1a# dM'X!3# ]&)% , = f)#"/'0e<8 @/ J 0>),6Ep:/ d>s* 'g&: f 3#.fE,u*%?%M@fi =S''ff8%c %& ffS''ff8%c %6 +0"fi'f%cfi@ = d1>e8<84X-^1fi f> ;%-%c]ff>)% hfi8:> f>uZ sa>dff#(D8</ ');f cp { `ufibMf%1')' ]?f-'/]'<zVf>)-f fi %cfifi:f1%*fi8<83 fi80 >: 'r%f#fi8:>#"" } JTVYM[eGKffSGk9WYVm >: 'r%f#yfi8EdqR ff;'/% hHff8)/%y:e8S J'(0>)#c1>h: fi#fi8).?f-'X!83/]'<zVffufi8%c4uf>,u%f8w8)@f%s M83;& (y:fe8<83{+%/ff8)/%y:e8Q)M%"A#B)AEHGI JDK@sYIffR#W)S+yh%fi8:x8w)oA?f-8:% - '<qM798:>fi;@fifi<8=y E fi;a = u% = / yffuff]f>1;% "(Xqff8<8)ffh;. fi80 > ')fi8f#ff8<8:> ffQ fi80 ff] dufi@/ = >M''ff8H>1'f#X tS''ff8M/ J c %% @fi = v''ff8%c %i% S''ff8%c%i +p> g-ff9: h%y 1% ufi,ffQ fi: 6' "0fi`l M) /%& )%&0e fi0ff)%yffV 9:fffi)yfi<)g4 ff-fi3!-@e&)> ff:'r%%dfi:ff,:h{h2*ff04hhH%fi)ve) h{*ff0x%,<ph>%fi%fi)?-)figfi:ffh3Hff2:ff- dfi6@ed&:erh*ghfiM%H :y%0!3& -@?-)fi>fi:ff2&3@-ff:'r%%Mfi:ff)%3*sfi06: h{*ff04hh>3Hff2:ff xfi6ad s&:erU3&),ffd?-)ff:'r%&yfi6,) &')*fiff9:fffi)dfi<)>4 ff-fi3,23-%fi)% -')fi3-0ff'))%sq!3&@*r%%fi:fffi)h),%fi))fiq%-ff)%42: 2*ff04r@-fi %%3%%/%':fi fi6ff263fi%y:ep')%bffff%M !fi3,ff)%b!%0e !:)%@&,6,h<wyb6fi3`%e))%-fiVhffQ&%-ff &)a@-ff:'r%%,fi:ff#U)y?-)H% )?-)h<<_{dr%,ff#D<)ff0%06fi363&.6fi)0ffd !06)%hff_6fi3,fiw)4 ff-fi3b'^%&))%#hh@ff%y%{@ff_6fi3`fi<)Q4 fffi)3 &')*fiff!06)%hh<<_{%D<)y0%fffi)%2Q3/ff&3-@%0e9%': )%& )% %')*fi0ffd fi0ff!3&Qff5/9:fffi)Qfi<)@4 fffi)Q@U:H%fi)6#&)@ff,?-)ff!%-%fi:ff/ff#Dw)H0%06fi3ff)%Hfi0ff fi0ff)%sffl9:fffi)fiw)4 6fi)!-@ff:&fi3ff#&)@ff,) ?-) %gff-fi3-0ff-ff:'r%%sfi6@0&0fffi)r)ff#Dw)H%%D<3ff-aw< 6,fiffM fi0ff!3&d0e@ fi)0xff&3 &')*fiff!06)%9h<<g')fi)s&<Vff)%sff fi)p-ff0&)fi0ff fi0ff!3&Q h%fi:sw)%ff1ff^fi&fi3ff29:fffi)fi<)>4 6fi)%s),fiff!06)%fi:h%*-)%:er%':fiffa6% ,fi % e%>&fi)H3fi*%dff)%@&a-fi%)%s{e%)%e{%)#-e,d%y6a&>{')fi),e&)%ffhy&)%fi yd fi:ff#) %sff#)y ffl%)%% & fi -:')%,-%22%)%#&)fi:&%ff:h*)%#%%fi)ff# %-fffi)06%%e:yff# %hfi:%->6l9rd 6fi)fi)% H)3fifi)%#&&9 0206%p %)%&:)%fi:de%)%*%e%)% g fi2fi:ff,%h&<#fiffd*%h% fi%)%s{e%%%&3d ffsq&% ) e% &%0&)-%-ff &)-fi-),h%-% yff0ffsh),)0fi:ff,fi:h: fi:e<3/&<#{h@! yff,fi -:')%@-%hd2fi:ffxeffyff&:efi)ff*d),536-ffpfi:ff#6eyff&)fi0%06fi3 0%sfflfi%:')d%p%!w*&3Hfi:ff#fi%d)fi0ff-g- ff)%2hfi:hff:h&dh5&':fi?-)fi6%V_6fi3dfi<)4 fffi)d%,)@sfi06:g&fi)),%*)&pfiff'2d,g'prfi: )%`hff ffff)%dfi0ffdr !# %3*sfi06ffg- ff)%Qff9:fffi)`fi<)@4 fffi)b/2%yd:yMff:'r%&Mfi6r #2ff`?-)ff!%-%@fi:ff4 *%>6bff%fi #-y:y@ff!%-%@fi:ff1r !#lffQ,-3fiff:'r%&fi !"$#%'&$() *,+-. /'0$+1/32+-54$687'6)796:;6-</96=?>@-5)$A1*@B -ACD>,+E*F:;)$+2G6H+-$=I2+-54$6J29062'K!6=D>L-I)$A1*@B -ACD>,+E*M/;>LCN6OP :RQR68CN6-</S>@A!-6=H/'0 >,:T>,:U+V-A-W /97;>LX>,+E*MYZ+!2G/36X!6-5Y[A7RCNA= 67+/96J:;B\:S/'6C]:OM^68)7'AX68/90687'6:;_ *L/A78CNA= 67+/'6N:;B:;/'6C]:`Ha+-$=/'06-b:S0AcQdQe0<Bf>@/g>,:J+)) *h>,2+4 *L65Y[A!78/'06D7;>,2'067D2GA-</96Gi\/8AYkj_$+!:l>LWCNA= 67+/'6J:SB\:;/96C]:O068)7'A<A1YnAYFA_7kA1opWq*h>L-6J/97+2G/+4 >h*h>L/;B]7'6:S_ *@/QR>h*h*rY A*h*LAQsY[79ACt/'06Y A*h*LAcQu>@-v?*L6CNC]+:`OwFxzy{y}|~r zpEr'D5 Lq GhVJ!<qE, ]N!\ qID!; DR!$$q<<qh8a!z, qqg hJ'`<E,9Nfi LhF ;q , $ qfin!$88<E`<`V 5< q<;N`cqJ5 , !ghI$D`<,Eq<gbq}q R V <E;q<Nh `zqS '!<zIVUGJE$GR YF/'06+1v6-</R)$67;Y A7'C]:e+E*LA-v]+- B])$+1/'0AY CNA7968/'0$+- +2G/;>LA-$:RQu>@/90A_/k*L6+79- >@-v+- B</90 >@-v5 >fiO6OH\+c*@A!-v +2G/;>LA-$:T/'068+v6- /k= A<6:T-A/Rv6/k+- BN-6Q>L- Y[A79C]+/;>LA-I+4$A!_/k/'068+2G/'_$+E*4$60$+EX!>LA7.H/'06->@/8CD_$:;/X!>:>@/D+5)$+7'/;>,2G_ *,+7VA4$:;67'X+4 *L6]:;/+/96J/;QR>,2G6HQR>L/'0A_/8v6/9/;>L-v+-<B-6Q>L- Y[A!7'C]+/;>LA-+14$A_/k/906J+2G/'_$+E*a4$60$+EX!>LA7H+-$=5/'067'6GY A7'68QR682+-}:;07S>@-K 4 BI= 7'A)) >L-v+!2G/;>LA-$:Qk0 >,290}/9A<AKI) *,+2G6?4$6/;QR66-}/906:;6JX!>,:l>L/:O8^6N2+-)$67;Y A7'C/90 >:)79A\2G6:':e_- /;>h* /'067'6DQR>h*h* 4$6D-A:;6j!_6-$2G6JAY +2G/;>LA-$:T>L-Qk0 >,2'0-AD*L6+7'- >L-vIA\22G_7G:O06*@6+17'- >L-v]AYF/'06J+v!6-</T>,:RCNA-A!/'A- >,2FQk06-6X67e>L/R*L6+7'-6=:;ACN6/'0 >L-v]+14$A_/k/90686-<X!>LW7'A-C?6-</84z60$+X!>LA7HzY _/'_7'6V>L- Y[A!7'C]+/;>LA-{2+-5_$:;/8C5+K6J/90 >:gK<-AQR*L6= v6DCNA!7'6N2GA-$2G7'6/'6OD>L-$2G6/'06V-<_CD4$67AYF)$A<:9:l>L4 *@6D4z60$+X!>LA7:u>,: H$QR68v6/k/90$+/U+?K<-AQR*L6= v6g>@-$2G796+:;62+1-A22G_7+/eCNA<:;//S>@C?6:OACD4 >@- >L-v5/'06J+4zAcX!68A4$:;67'X+/;>LA-$:T*L6+=:e/'AN/'06J= 6:>@796=I7'6:;_ *L/OwFxzy{y}|~rqk a89 Lq GfhkDE; 3hz$q< <qhaG$q<qq?\1[`h]'`<E,9Gu!$ ;Iq<5!<`q,ShE<q5a<q<EVLq5;;;`E$ qI;kl,fT\h$Dq,55$8Z Iq< EG , Dq<F8 qE GN!9D!G,G5<Rq qD\h$!qaq]f$VJE$G}m 062GA-$29>,:;67'6)7'6:;6- /+/;>LA- AY 2GA-$:l>,:;/G:DAY+/G+4 *L6H Qk067966+2906-</97'BAY8/'06/+4 *L6J2GA797'6:;)$A!-$=:R/'AN+D=!>:S/;>L-$2G/UA4$:S67'Xc+4 *L6V0 >:S/'A7'BNA1Y+-?>L-</967+2G/;>LA-IAYM/'06J+v6- /RdQR>L/'0I/'066- X>L7'A-C?6-</Hz+-$=2GA- /+E>L-$:k+1-+2G/;>LA-I/'AN4z6J/+1K6-]4 BIY A7k/'0$+/k:;)z629>h2Nl)$+7'/;>,+E* .3:'2G6-$+17;>LA$O06- _C4z67IAY=!>,:;/;>L-$2G/I6- /'7;>L6:?>L-/906/+4 *L6{ .N2+-4z6*h>LCD>L/'6=/'A>L-$29*L_$= 6bA!- *@B/'06) *,+_$:l>L4 *L6N=!>:S/;>L-$2G/k0 >,:;/'A7;>L6:Z>ZO6OLH /'06U0 >,:;/9A7;>L6:RQk0 >,2902+-]4z68v6-67+/96=z.rY A7R/'068:;B:;/'6C +-$=/'068) *,+- 068- _C4z67kAY :;_$290=!>:S/;>L-$2G/U0 >,:;/'A!7;>L6:T>,:k4zA_-$= 6=I4 B fi/'068- _CD4$67kAYM)$A<:':>@4 *L64$60$+EX!>LA7:eAYr/'06V6-<X!>L7'A-CN6- /. /;>LCN6: l/'06J=!>hop6796-</U:S/+v6:T>L-+N:;)$629>h2g>L-</967+2G/;>LA-.O-]A!7= 67 /'AVX67;>hY B/90$+/3+g) *+1- Qk0 >,2'05>,:T7'6)7'6:;6- /'6=>L-]/'0$+1/ C]+--67>,:3:9+/;>,:lYfi+2G/'A7'BH!A-6-66=:e/'ADvADAcX!67u+c**M)$A<:':>@4 *L6D4$60$+EX!>LA7:k+-$=?Y A7k6+!2'0IA-68A1Yr/'06C29062'K5/'0$+/ *L6+=:e/'AD]:vA<+c*ZOP :k+-5>LCNC?6=!>+1/'62GA79A*h*+17'B]QR68A4/G+E>L-/'06Y A*h*LAcQu>@-vzx Ex$y~rLe ,zq<{ ,E fh$DE; Rh$z, <qhaG$q<L8;k;q$D ;E <hcA-$:l>,= 67]+-+v!6-</DQk0AQu>:S06:D/'A7'6+2900 >,:]= 6:;/S>@-$+1/;>LA->L-/'06I0A<:S/;>h*@66-<X!>L7'A-CN6- /NAY62G/;>LA-sO -)7;>L-$29>L) *L6HU/9067'6ICD>Lv0 /4z66Gi)$A-6- /;>,+E*h*LB<W C5+-<B0 >,:;/'A7;>L6:DA1YA4$:S67'Xc+/S>@A!-$:D/'06fik{r$8L$$I]E$G 'te''GL'`8I'; L'E\5'$15@5,f'G!$l, L$1@ D,EhL ] 'L 'I$9 }9';,' ,affR ,e,N<;Gfi[k'J a;Rhh$ @V'N'9;<e;!;'I;1@S@!LD'G,;LGL}G!$9SkE39;!;'SL;L!U';, fiG'!'$9 fiG uh z I9@ ;L} 9G'a$$9$ $R $' $EL'R 9 L $EL<1;LNLI8<S;h@ L'?< 3eN<S@!zc!eG'$EhL;Lf' , kEe;,GD'$ !N5# "$fi $g& % L'5';, fiG'9# [N 'N;;'5 'k RL(%$!lfiN ';;'5N' ,) !N] ,I!I$; [ fi8lL$G @'$I;+ *]@!<, z} -$!<;,ELs'G'$E8' '; S@!lfi ./'e 3! 90 9 $'S@5 EL1<2!N]3"$fi $$'<EL1G$l, ;L 9L<, ,$`54N?<; !G;,E6 7 $<;` 3 {?L<;cL;, ]'S';,G;LNU1'' ;LI'D 9L<8 ,$lL$G ,8 Rhhn! $D LJ'-G'] -$!<;,EhL{] !G;L$L{']GSN U,,a) 9kL{'$ff !pN], "z; :,# %!$lfi N 98' '; ;L$` R8= <> ?A@BDC@FEHGJIGLKNM OQP=RS(T)U=VTWOfiXY[Z\R=]T^R`_baTScSFOQSed#fhgeOQaRijRT]kScOlSd`WnmW^R=Y&oApbSF\OlSdqTS(RsrtXu OQR=Sc^LWT^OvWlwT u ^Z]km8xAayTS0OvWffZz XaOlScRff^]T u ^T{aR}|u M9; LF,%! L'U'S~ ZG9'lL$G6 %!$lfi N 'k;;'5F'k;,'5G<9-\h4F-L;?U9 L'G''R;$9!F'k! !,'G$';# qT' $D ,RGhL8 9<$=p1$''9! %!$lfi N 'US\;9]LRVL$9@$ J8zL<!D,Ea< $TpS;GA k ,'{G'' z$'1} -$!<;,ER< zJ$<' L5$;';L$M$99;'<c@$SUVh,;U J$<9lfi @D L'!N<8 $$E!L8 fi'J ' ;,'J;$;! aG'kN9,;,I9,;I J<!L'N , $$E!@!8` 3N ;L1$ '8 %$!lfiN ';;']5G'' $$'G L - ;;']` k'' [ zI ff$ 'lfi LR;,'G;fi L'N L'!N<g,8 9L ;L<? LqRS]G$;;L'9]5;,'{$ zEfiL) ZL{ U;;'] ~'Uc z'!fi $=$) kD'; LD; J [!J -$ LF'$V''1] fi';\S't!9G$';?LI'8 'L$e;G;L# $ hLUR5 -z'k;;''$!SR' -\kG;L5'& $ $ [9N $!;N!]' ,;'9N r $S'cS@!$ N'8 G'R9 L kGDL u@9' ,5 $5';L!,Efij J!8'; LJS} d9$JG$9,;k9 '; ;LD 8 , e 9D,M 9L ;L!; LU < -<,;L e 9L<'S~ ZG9'# 1} - ,;R9 [9$'D a'e S@5'N ' ;,'ff , jqhLVR'' k hLJ#L8}DAsfi?' , ;G;L`3k;{'$MLM,FMhfiGLN'$1F''T, cEL;L'd'JG!NRRL'I'S~ZG9' ,3 J N '#M, L+k h@,!9 @;;'alL$Gff;$; 9,!Lk'5;$' 1- SV,]8L $1,'5'I'; L`[D',$!l,# NRG;L5:1lLDh,I'; L5,N' !Lb I9@ 'S~ ZG9',$R , uh8L LlLDh,8'; L '!;J I9@ J'S~ ZG9'# 1$e@%!$lfi N ' n1 @, e L!9 @I;;']`\1$#R'J -'$ # ? 3! \R!,'G$'S5L' [1L RLI;G;Lau f'; L9'#RL'' '; L5 c@ L'+ '!L$;G;L G` @9'9,'l S@!} M, L, k hL !' LI!9G$';fL G;L+> ?A@BDC@FEDIKM OQP=RS,TqY Z=\R=]T^R_aTSFScOlSd fhgeOlaRiRT]SFOQSed&WmWn^RYffoh\R u Ol\OlSdfhgeR^QgeR=]^QgeR]RROfiW^QWTST]n{Ol^]T]mZ]&Rr u OlRSF^ WnT^OvWQwnT u ^Z]m8xDaTS&wnZ]ff^QgeR T=dR=SF^OfiWffTS)8_XgeT]\xA]~Z{aR=Y|=kfi+tFFffAF fie57[lt8,}lfi==fififi=tF5Flfi27lF==fifiv +FFhficfie l6`b+`khlfit fiA=fitF7tF7, elk#lslfi [Ffiff,lk#~eq=fifififfF8l F=F l Dyfi 7[llfiff8LFkAlfifi=ff7F6=7lfi+ 8 vk,lqlj)k7[ql8ff8LF,fi8=Ffi+l ff 7fiffllF8lF}=fi=fififi ,Fe~fiFFfik 8,efiq7! " $# fififi % " $# & " # =vfifi& " # ')( 7 7+*,1cefificFfi -& fifilk&l.qFe~fi,fiF6/v021 13& 4 =fifiv& 46576 98;e}Dl =7,[k/9l;:<>=?@BADCFEHG%<JIh (fiFkL K=67M=fik F7 K(N( l7,l )FF=kL -e,k7.P~RQTSUH3=vfifi& ,V=fiF = .kL K=[W13ffFfi`eLcF==Lkfiff7XPY" $# F& 7Z2k[ K=j7fi 21 F&efi87ffFFfi.PY" # =fifikl/( 98;e\ -& fiO8FFfiZkL K=fiF 21 137kP~c =fifikl/P]AL ( JL Kfiq7fiF4 =fifiv& 465Lfi= k/B-&Obeq7lF _ ^ fi,=7 7fi ( fflfi=fi=`fi _ ^ -LF l F=4 =fifiv& 465 ( t7lF+fi3}lfi=,Fa`(lfi_ ^ b77l7+ fiF~e)fiklfi= -& cbsF}k l^ LFdL K=8,fi;4Hel7fflL77l7=ff7[~ehFL~F 2 ( 3L-F7lfiL fFefiFF=&kfi)( Fh98 Llff7fi}fif " #g( f " $# -k=fifi+k&l.e`L -&ickL Kfi,fiW4 e k^ AFff8FfifiqcF=fikcfi=qk/^ A%j3}ttFjJ vl elkL7ll` LAkZl 3fififi ,VnmM 2 B 3L Ffffifflfi=8 fififi Fl F=FjF7F`voFp,nbfi l fi+NQq>r 7jF kL K= fi 21 13[cF)}}` k ( Qs-& jAfiW,tcuAFe8kL K=8fiW46eFFff=cF8#=7lfi, k lfi=F F=fi _ ^ Fbl=FnoFp`fi=F=ekfi=LffF}Jefi cF=fihFFA8v~s78v9qb vkL[fikwv F vD F=kwv l FF=,kfi xFkn,+Fy( cFfikFhvj=7[F=h -!zN l7 [FjFfi 21 13 k v l F=Jllcff 7vF _ ^ F7qFbq~eFbvD F= _ ^ elk6fik v 8;fi J8}lfi=8L f l,l,F7# ( ,t9u{- q>r 7jfi8FFfi/f " $# nf " #|~}~wTT~R!9VfiFff=F8fie7F=F fie=fie=3#kh7K, hlfiyfiA=7{L5q[ ~ =8j`JlF= fi,7fiF=fiFbvFb76e=ffB j= fie&FF l8l.fiF7kLfiq0ylF bqt= ,ql1(F~v,+F7lvqFfifi0v ( fiFFfi&fin!!!wc/!]aw!!!w7&[w]])!w]w&fiLw)7!_h!;7%_7!a7!h_wL!xL]7wL67]6]s) _>waa6>/7&LwDs%;\!w]fi]H7_]!!7w]7s6\77B6]!W76L_!7!%NwL ]F]7!h7&Lw 7&[]696_]wd]]!!9B;7%w6+can!7L%]!B]! [76L7!_ 76&\!7!7L6&n%!7!7!7/6x/6!7h7!/7wLL6&fi%!+%.7%!79B7%B7!.L6H7w7L!+[wh>J 76n7!.L6L6nB]!g7!7_wL.L7!w67!!R.677[gfi!%7!/]677]!a7wD%6>].7hw6nx7!./[6]]L6fi!7ws7[D![[H!_LT7!LwL_w]w/L77!&B>]L]wLT!w&[7Rn!7\7%!/!Y>!g]H_&&L6>>]W76]&!Lc]W7!g7w6U\&L7!JhnL!]w7HL%!LY>]!/7&L6>J]!6hw/L7\6>]+wD6_RTW>>6w!/7!!7w6h\&L_Lwg]6]HL_/7!.LY>]!.7&L6>>]D[7!._[7 76]9\h~RRWa.{RHLh%Z]\F fi\BwcZ>{wYHaHYJ&wYH\ff fi!"#wLZ9s/L%$'&!()&>*\\ %gh/L,+-$./&>0\1$-2hLYwY\YB{ 3 4\%\657s/LXB8(19$8ffBw%&_ H:(1$;<~L;YwY\~B8(B=9$>ffNBw2&_ H(B=$:?57[wW@$'&_F/-\1$*A B680\1$C"#!DFEGHEI JKHEI_wL.677]!X_W&L75LgwwB%]?M5CgwLwa]CN569 7!/!&L7.[whwwB]4O=O>PQ"# ).N &R n_wLF]9!7w6SFL!6]!n6>]?T L76]!s79%7!nL]wBR7&LwDs%!ws%9B9_wL!7!/wwB%%U\ H_!N7w _&Lw]]h\!w]RLy_!/H%]7!6!fi6w6]]LHB[wL.7[%77 [9>>79B]9%F]9_!nw &[7RV 7W _JJ]U%7!c7wL_!U[H66h!h]6]B>] 6!L 7!a66]LWL!!!7 6% L_wL.>>Xwd7H[&%]7Lfi7!66]L% !+L%H!L6!Lfi"XF!n7s! ]wF>> 7!72Sy[!6!/x6J]%T L76]!/]7!_w6\&L7Rng6&[77 w%]\6L7L7w6YSFL!6]!n6>]TR[76]!s7/SyL!6]!n6>TRL76]! s_ ]Wn67c7!H!{/LT]HxG L]H!0Lwh7!6!N{nL]6&]I L]6!BP_ 6 !Hs]7!.]/L_!.7wB7 7sH&[]T!L6]!LF7!/6w6s%wn]7!/wD<SyL!6]!n6>]ZTRL_6!\&L7R[ 6\77!wL_\TR9]w_^h6H!!Ld\.!6&L_9!7w67SFL!6]!Wx6J]TRL_6!!79xLZ%Z%hZa6]!)] ]`]w7! 7 7] !&B]!)HTRh*]w_^/Law/7H! Hw6]!a+_]6 LwT!xH &s ! 6!6!~]Hs/!&B>]Hg77]!!nL7_H]g7.a+_]6 Lw!]w. L!!n>>!wL6]7a!_ !6B>9L6W]wX7!9%!&Ln Lh//Hs%!w\76Ln\ ];7!]LX_L77 Lw9BsJ>nw H7]6]>fi97s7!&s%7ca+_]H Lw6]W9[9!7! s%wLw%!N_!]LX_!7L76_]c7777!w[7]8TR9b]w66n![ \n!7w6CSyL!6]!n6>TR[76]!s79!76L!Lw76R]h\7!_w6/\&LgZ%7nF]>L~7]& 76L!].7!w&[7Rc 6]6]!9_!.LwL//!DR 6Bw \!fe =(Bg!B6._h9ff*$D%\N\Bw>w{6H>i wYH4\ffUj)kZ{YHc/lm0BBwn\o1A0B/$M+>c_U$qp2hD>Yw./0B\'&>.rstsfiuv-w.x-vIv-y-z7{P|:y-y-|y-}8~-;8X 1X_!o;8>86 >GM-;- _' ;!99_-U-; 8_Wo,:8 ;-!Z- :92- ;2;- P=n;\;8.*-%.: '_;-29-9; C-!9.8-8)_9_X)\@@;q-P;8;- ;% 'X;8 - .;9_8MY); ;!q9_-I!;-;.M;-*)_9_. ) 9=4;8.*. '**_)*-U>*@=._;-2'8*;-_)_9=9.;-2:;- .: '!2 8-o-)-*2)@_<!.;)_-*;/-.7;U-% 9\_)o6.. ),;9_!n;U) :2_n'-2)@_M!;)_-W8-,;-8 8_.,;-< )_;-b '!>.8C.>-2;-<:'=2.8b_)_9=P9.;!.:;- 2. ): X_!oXX18 H*'@/o!:!>P@8>=)*'=@4X:>=)* ! /!=%*-)8`U4! M2- i)=,%9 .'' @ //8 !8'!=,M9i-=,;-4=>=/1) //<9%!:!>=\<9 ,.8=9>Z8 )!1.>./ @2:!/1- ' <=%.=>=/1!>!198*!>P;'=8.8 * F=M@'/9- 1/=4 '>!/18.2) ; ;*_-!!;68M.P9.! U. 4.)8 8_.89._'9_7%;-C. )b)!' ,;-6--Z:89 ;)I9.!U%;-. ).8;-<- ':_;- )%> 8_ ,Q .m. )Z8:4_.2'.8*)=.7=G)!=_;; 6;8.*;/ *:)7G;8.. ) 89 ;..)_Z9;! ff-Z) fi8)_9_8M.Pb) ;U.8:8_/-) .U; -;!q '9_8.;9;:'9/;2.M -@ !.9_8n.;-_P) fi8)_9_8n/-; 8)!=.-)_-U2)@_%!.)-9-9;7-:_9_X\;9-;8.4!L. )Z .79;<_->._'-:*=@7.'_)_9=X89 ;..)_?9.;! .8WZ8-.X8._)-*=@_'*'o#_\*_'\8M; :)!;*;)_2 ;-_!; -!6. )!*-'-.28.\;-<8,_)_9=9.;M.X;-%:;-. )M= 8I28.<;-b <:'P.;-;- U 'M=Y.;4_' ;!9;! _L;.9=;:;*'@/o!:!>@ P ;_-<8:X_!oXX_ **)_9_. )%o-)-6)_*!.;)_-C9-9; C-6*)@/_9!8:=248_.9 \.)o8 --<9 \:!:;6. )!, ! ) -;<;-<9 .)=.82.: ';`*)_9_. ))=.=M:/_1:!/@X/! .: ',.8*/\!;8);_)_U'#"..: ';:;- ;=M)=.b$%!_9);8.P)_ !&"W9.;9_-M/; '48';)_M_)_9=9.; ':_!;.-%)=./_6;-<;;!9>8:_-'$%!)(n8*_)_9=9.;2.;-%:;- 2. )!'.8b; ':_!;.2;-*_)_9=28 8:_*.2;- )_;-b '!MQ+ *m!1!>:/_1:!/IZ)@/_9!8n@o4;.qo;6*)_9_. )<)o 8.?:8=9M.P)=.8i2)=;L;Z)!=_G; !M.P8)-*=) -;X-Z.>:U) fi8)q:L .-;-;!_')q:_;8.9_8.P=.-)_-C)_!.)-_I*)_9_. )Z):_8 G<;9-b/U8.)_6;8Z- ; ;96/!4;8U8;C.b_7;--'9q_I )_;-b '.,!9_.--/- m9.;b._-G10.32L\.87- !;G!;7-)!99_8.q:8U)5426 2 ;-_!; ,;- .G-48C9-;.8-*;-b 4_)_9=_ .q:.,!C;- M.86.8:-%!6;- 7 2)!99_8.q:XffQ8-Z.8) %.; -;)*_;8.2 q=\;) :oqUU9; ;/)=.68.9-:)o><)W:X)= :)_9=#- .9_88.8)_;-b '<> 8!:_)=M9; /)oC=i;-Z;9=:;;6*)_9_. )<)=.I4_)8 6/!9=U;82. )\.,-Z.: '2!7 9X!2;-U> 8_M.,;-U9-9; .8;-U!9)_:7;=<?> @6ACBEDGFH> DGDIBEJKMLK A3@GNHDG>COEPGDRQ7JSOEAUTJ@RLSKWVXFJK DGPYLKWP?KW> Z\[%NH@?JST=L]^NHKWPGDH;_^` `fiacbedRf ghbhbhijb#kml3nhif oqp#o8ghrhbhijb#ks%tus8vSwhxWy{z}|8xW~vW sxWxW~vW6vx z8vs$HxWxcvSw#z%v\Mz%ycSxW~#z}yHsshWhy~vSwhxcvSy^z}h'z%YWz%ySxx#z%!xz%~#~q'z%~!'s8vwhxWyc~#z%vShy^zHvxW'Wx'~hs%whsvSw#z}v$s8hys%6I)~hxvSy^z8v^z}!)vH5yx H!vMWz}~5xxvSxW~#!x evsvwhx'!vHYz%|8xW~!vWz8Hx$zffffxY x3ff))#x$vSwhxUtGs%)s~h|vHffs$xWz8We6I#cI 7 !%WH 87G)Gj #&{)8#I?I)$68#IW WG \?7H8W $!%8#)8878' 8.+ $W8GjW87G8^e)Gj U=)8mW8WI 7!IC$ 8$CjI\I HCjI!W8m+ #W8GjW8 G^)Gj #=)8I8c87# 8I8)%#8I8u8W8 {'Ih8)86G%$7u# ~vSw!ffWz8HxUx z8Swz%|8xW~!v\!~hs%vSwhxU|8sz7us%t6vSwhxsvSwhxWycz%|8xW~!v z%~#whxW~#xUvffffx z}ycvSw#z%vvc|8w!vcx z%y~s8~!t+z8vcz%#shvcvSwhx3#s!S!x$~!vHz7\Hv^z%vx {z}~##xWw#z7s8y^W8xW~vSw#z%vvwhxWySxs8~!zms%~hsz7~!h#xWys}tsS!xq~!vHz7$Hv^z}vSx z%~#exW~!8ySs~hxW~!vc#xWw#z7s8y^W#z}~#|%8xW~vSwhx3s%~hsz7s8h~#s~vSwhx$!xWhvSwqs%tRvSwhx!z%~#WhvSwhxWyxz%ySxUs8~!#s}!~hs8z7)m'z%~!x 8hxW~#x Xs%ts8#HxWyS%z%vHs8~#Yx z8ws%tXw!jwms%t#s%!~hs8z7?xW~h|vSwu{s}tx z8wz%|8xW~!vUvSw#z%v3z%ySx$s}t~!vSxWySx v$zff~5uxW'zE!^UcxW~#x8uffxWz}~xW~#sh!x8u~m#s}!~hs8z7{#z8x8z!x js~v^z%!xtsycx z8wz}|8xW~vXxW~vs~!~h|qs8~!vSwhx HxHx hxW~#x W6z%~#mwhx Suh~m#s}!~hs8z7vHx8hXwhxWvSwhxWycv!xWvxWyS~hx zSz%vHtYz8vSs8y!vYz}|8xW~vU!z%~6e6.cI 737}j H8GU!)GH 87#{)8#IX?)ff68#IffW7G7.=?7H8W 87#%\}^W )\878? H\mj?8)%#8I8I 883IU8 G!8hHH=HC7#G8GI8WWH%\I HW%WI$7%Wj 87H8G'!)GH 87#ffff##I?IM8#IW WG RUY?7H!%WH 87GHGW GIm87GHHuC7#G8GI8jqHI!^)8I#87WG $II!#I !c8 WW8c8W'78 I6WCMI!8I H3CjIc8G77#3GI G8$!Gj78 hu$88#) $I HCjI38Y+ W8GjW8 G^!)GH 87#=)8mqc$7u#ff ff)#xffh!vffz8utGs%)s%Wffwhxffs8#HxWyS%z%!xHv^z}vSx us%t=z%|xW~v~ ff))##xffvSwhxcWz%yvSx z%~hySsh!#vs}tRvSwhx$s#HxWySz}!xHv^z%vx \s}t&z%|xW~v~ ffvSwqvSwhx$HxWvcs%t\Hv^z}vSx WC! HH^%W 7%7 S^%W7 % % 'ffwhxM~!vHz7Hvz%vSxs%tz}|8xW~v$~ ff))c#xv^z}8xW~vSs#xvwhx'#z7ys8~#Hv~h|s%tffv3~!vHz7v^z%vSx~ z%~# WH7 z}~#mv^|8sz\v^z%xW~vSsxvSwhxHxWvs}tHvz%vSx ~cw!w zs#s8~hxW~!v 1ffwhxxW~!8ySs8~hxW~v~ ff))xzWz%yvSx z%~hySsh!#vs}tvSwhx3xWw#z78s8y^ff~ffvSwqvffsHxWv^ fi3z%~#fi%hcwhxWySx fi w#z8 HvH~#v3xxWxW~vW% cw!Swqwhx$#Hvffxx hvx|8xW~!vcff))w#z78x$zjvH~h|8!Hwhx mz8vHs8~6#Wz7)x %W %!~ v~!vHz7v^z%vSx8.ffwhxHvz%vSxvSy^z}~#vHs8~ th~#vHs8~1ff))$#xmz8M~ ffhhvcwhxW~/#xWyts8y'%W 7! % v^~hxW s#s8~hxW~!v ~vSwhxWz%ySvx z%~5hys!#vHz%~#s~!vW3ff))Sw#z}~h|8xvSwhx$w#z%~h|8xff)?#x$vs %W %! )t{z%~#s8~!)t\vSwhx3hySs x vs~s%tvwhxU~!vHz7\xWw#z78s8ycs~ fi jz%ySxUvSwhx3#s!x|8sz7&tGs8y{z%|xW~vc~ vSwhxW~vSwhx~z8hvHs8~6#z8Shxvw#z%vvSy^z}~#vHs8~tGh~#vHs8~~ ff)){Sw#z%~h|x3vSwhx$~hxWs8s8~hxW~vUs%tvSwhx$s#HxWySz}!xHv^z%vx3vS!s )tz%~#s8~!)tRvwhx~hxW.s8s8~hxW~!v{s}tRvSwhxUxW~ySs8~hxW~!vj{~v^z%vS"x hz%~#zHv^z%vxSz%vHtG~h|w#z8ffxWxW~ySx zSwhx uwhx\z%#s%8xvSyz%~#tGs8yS'z%vs~UtySs vSs z%8x uvSwhx?!xW~!vHvH$s%t#z%~$z%|8xW~!v E|8s!z7zcs8#s8~hxW~!vs%t6vSwhxff~!vHz7)h~h~hs%c~#xWw#z78s8y7\cs%ffxW8xWy 8z}|8xW~v\?z%~#~hss8vSwhxWyffz%|8xW~!vff)6s8#xWyS8xffv^&|8sz7z7tGvSxWyv^$ ##y^Hvcz8vHs8~6?v\x z8HvSsHxWxcvSw#z}v\vSwhx3z%#s%8xXvSy^z%~#tGs8ySz%vHs8~8xWxW#vSwhxHhHvSxW8#zsh!xWy^z%vSx8z%~#vw#z%vvSwhxWySxxHv^zz%vHtYz8vSs8ySm!vHYz%|8xW~!v!jz}~e~ )t$z%~#s8~!)t3vSwhxWySxx!jv^#Swz!z%~ ~ hcwhxWySxU~ x z8wz%|8xW~!v{w#zffs8~!s8~hx$sS!x$|sz7Y%&'fi()+*-,+).)+/+0 132 /+/+2/+465+7986:;=<>!?@BA@CA+DFE9G+HI-?J<LK HNMBHO>P>QIR"S"HTDHOEUV!WYXZ\[X6]_^a`cbYdfegihjklnmol-pOgCqsr!tujvwl xyjr!ozcxyg{qwl| jkJx~}zclk6kJgk|~gzcjNajl vkJgk|TpOpxyjr k6uqgik|Flk.jygjk6xr!ozcxyg{qwl| jkJx3pOlxyg{pOlxytv\zclk.gCpts qwzcgkJj!xyvwl xylzj-G+HI-?J<-KH+<<-RsG+<-SR=E9G6I-E$MI-A+A@CA+DQSG@cMCHHI-9A@CA+D!@R"<-aiMc@CA+HTE9IEI?MBHN@CA>!MCEs@CI-DHOAEIRsHR"Rs6GIRE9G+H< A+HRHR9s@C?6HI-?6<-KH= @BK HOAQE9G+HRE996E9+9H <-E9G+HTI-?6<-KH MCHO>n>QIR@CE"@R=HIRsE9<+<LK HR@C>!@cMI-9HRsMCERy<< E9G+HO<AE9H+ERaS G+HO9H3E9G+HOH@RI"6<MBA+<>!@IMcMC!?6<+A6H+A6HO9EI@CAEwI-?J<+EIT>!MCEs@CI-DHOAERRsEHO>+<=HI->PMBH@~SHNS"<M!Mc@CHTE9<6A6IT>!MCEs@CI-DHOAE"M{IASG+HO9HIRGI@cMC+9HR=<-IDHOAERa>!@CDGE3<+O+@BAPE9G6I-EOIRsHE9G+H=I-MCEsIDHOAE~>!@CDGE3A+<E3I 9G@CHOKH"@CERD<IM?++ES"HN9H@C9HE9G6I-E"E9G+HT<EG+HOIDHOAE"S"@cMcM3RsEs@cMcM?6HI-?MCHE9<nI9G@CHOKHN@CERD<IMyE9G+HOAS"HOI-ARsG+<-SE9G6I-E"E9G@R +9<?MCHO>@{R <-aiMc@CA+HE9I EI-?MCH+6R@CA+DFE9G+HI-?6<-KHTEH9G+A@ +HROn.6JJaIsMCQS"<9@BAE9G+HI-9HI<-MI-A+A@CA+DQSIR"HOK< E9HFE9<nK-I-s@C<6ROIRHR"<-MI-A+A@CA+DS"@CE9G<>PMBHOEHC@ A< 9>QI-Es@C<AFwRHOHMcMCHOAa+HOA6 MCHO+ IE9H++< >QI-AT6I-6HOR<A!E9G6I-E~E9<@~TR~9HRsHI-G@CAE9G@RTI-9HIF+<D9HR9RH@CAK-I-s@C<6RT @C9HEs@C<A6R$RHOKHOIM~@CA6HO6HOA6HOAES"<R <?6RsHO9K H.E9G6I-ENE9G+HIR9R+>n+Es@C<AE9G6I-EI!MI-A+A+HONG6IR< >nMCHOE9HT@CAy<9>QI-E@B< AF@{R+A+9HIMc@RsEs@<">QIAR@CE96I-Es@C<A6R+E9G+HRs+?+I-HInE9G6IEE9HI-ER"EG6I-EIRsJHE<-3MI-A+A@CA+DF@R6R6IMcMB9HyHO99HFE9<QI R"MI-A+A@CA+D@CA+A6HO9EI@CAE9HO9@BE<s@CHROI>nMCHR<9HRHI-G@CA E9G@RnRs+?+I9HI@BA6MC6H.S"<9< A6HO9A@CA+DA+<-S"MCHDHIA6 IEs@C<Aw<<9H~+aNIMC6HOAaJS"<9<A<A6 @CEs@C<A6ILMI-A69HIE@BK HnMI-A6R!wNHIAHMcMC>QI-Aa-IA6S"<<A@BAE9HOsMCHIK @BA+D.MI-A+A@CA+DI-A6HH+E@B< N>?+<RsyA+D HORs<AE9HOHM=G+HT9HI Es@CKHI-++9<I 9GF@R"+9< 6<RsHIR"I!E9<<-MY@CAE9G+H<AE99<M\<-9<?J<ER=<6HOI-Es@CA+D!@CA+A6HO9EI@CAHOAK@C9<A+>PHOAERI-A6@BAE9G+HffHR@CDA<-9HIMCiMc@cH< AE9<-MI-G@CE9HE9+9HRFEG6I-EnS"<M?6HI-?MCHE9<9HIE @CAIFR9I-Es@RIE9<Q>QIA+A+HOYD-@CKHOA.+A++9H @E9H.HOKHOAERw=9<<+ROa G+H@BAE9HOsMCHIK @BA+D<-3MI-A+A@CA+DI-A6HH+E@B< A>QIRs<>nHOE@B>PHR?JHQIn6RHMIMCE9HOA6I-Es@CKH!E9<<A6 @CEs@C<A6IMMI-A+A@CA+D6<LS"HOKHO@CA>QI-AF9HIMc@RsEs@n<>QIL@BA6R E9G+HO9HN@RTI!A+HOHE9<F<A6R@HOI!SG+<-MCH<MI-DHT6<Es@C<A<-I!MI-A?JH<HH@{ @CA+D<AIAIEs@C<Aa G@R=@R"EG+HOIRsHN@CAE9G+HTEI-A6RsJ<9EI-E@B< A<>FI@CAI-A6FE9G+HE9I+>QI-OI-9H!<>QI@CA.S"H @R96R9RHYHOK HO9E9G+HMCHR9ROSH!RsHOH!E9G+HT@CAE9HOsMCHIK@CA+D<-3HH+E@B< AS"@CE9G~MI-A+A@CA+DSG@cMCHHI-9A@CA+DI+9<>!@R@CA+D @BHEs@C<A<=y+E9+9H9HRsHI-GaHRsHI-G@CAE9G+H @BHEs@C<A<-<A6 @CEs@C<A6ILMMI-A6RPHIMRS@BEG M{IA6RT@CA SG@GEG+HQ<+E<>nH<-"E9G+HQI-DHOAERNIEs@C<A.>QI.IaHETE9G+H!A+HE!IEs@C<AEIHOA?.E9G+HnI-DHOAE G+HO<9HOEs@OIM"S< 9<AE9G@R @R9Rs+H!@RN>QIL@BAMCHOK<EHE<IR6HERN<-"9HIRs< A@BA+DI?6<+ENA+<-S"MCHDHFI-A6.IEs@C<A_w<<9H+NIMC6HO9Aa! +T<9DHOA6RsEHO9AaT IA6E<E9G+HMC<D-@OIMTy<9>!MI-Es@C<A<-<A6 @CEs@C<A6IMMI-A6R <RsHOA6R9G+H@CAaT- JH@c\.>nHG6I-A@Rs>QR!E9<<A6RsE996EF<A6 @CEs@C<A6IMMI-A6R@CASG@G<?6RsHOKLI-?MCHFHOKHOAERTI-A6.E9HRsERTI-9HnH+Mc@{@CEsMC HMI-9HI-9HQ @R96R9RHIRNS"HMcMHMcMB>FI-Aa"G+HRsHNIR~S"HMM\I R~E9G+H">n<9HMIR9R@OIMYS"<9T<AQ< A6 @BE@B< A6IM\MI-A6RI-9HOAa+LLI-A6S"<9E9G6I-Ey<-McMB<-S"HIA6H+E9HOA6H@BE @CAK-I-s@C<6RN @BHEs@C<A6Rw~HO<E>!@CE9Ga~+ E9@C<A@aI-A++RO+HM{YNI-JHOYHRsGa@cMMc@I->QR<AaYYG6IKH"A+<E<A6HOAEI-E9H<AnD HOA+HOIM6<>n++EI-E@B< A6IM\IRsJHER<-$$MI-A+A@CA+DQSG@cMCHHI-9A@CA+D6 +S"<Q<HRA+<E<A6HOAEI-E9HN<ARsJH@\>nHG6I-A@Rs>QR=y<"E9G+H<A6RsE96Es@C<AF<-<A6 @CEs@C<A6IMMI-A6RO IE9G+HO @CE< A6HOAE9I-E9HR< AQDHOA+HOIM<>P++EI-Es@C<A6IMaIRsJHER<-< A6 @BE@B< A6IMMI-A+A@CA+D6<>nH9HHOAEnS"<9G6I RnIMRs<?JHOHOA<A6HO9A+HS@BEG<>n++EI-Es@C<A6IMIRsJHER"<-<A6 @CEs@C<A6IMMI-A6RO?++E<A6HOAE9I-E9HF< ARsHOK HOIMaA6I-E9+ILM++A@BA+DP9MCHRE9G6IEOIAQ?JH6RsH@BAE9G+H<A6RsE96Es@C<A<$<A6 @CEs@C<A6IMMI-A6RTHOA+HRsHO9HOEGf <+9?6I-G6RsGaafi+++{6.T+C++{6fiffffff!#"%$&$(')*+',-ff'.ff0/$%'12*'$%3ff+#'/546#.ff'$%'17+#'89$%':;ff'<ff7#'/5589$= >46#?ff@'<+##'ff6 Aff'B:;4 AC4D+E<F#$ff<>G$9ff/+$%'1HI /ffB#'J$%7$%K#L#$?LM$%ff7+#>,N$J$%'17+#'89$%'JOK 1P*L#$Q46A<MR3SUTJ$?ff0$>8Gff#'#V'$%'$<W46#/$%'$%3ffXN89 3ff)*+'ffNff M$35,9YZAff''#'/[4 #$]\;$ff'#'/:(ff0'<^AffK,_`YaAff''#'/b4c!#$\;$ff'*'/@Lff$<F'$%7$%ffd89 3ff)*+'ffa%ff$%/+#$ ff'<@$%$$%'1ff#'51 M$%Se '$%g//+$$<(ff01ffh,ij Eff0''*'/h#'='$%3ff#'9$%'7+*'89$%'3%:4 1)$Dff A%ffLAfff+$<fik#$6ff6$ff$<fi<+AK#'=$$%'1)*:Aj$,_$%$<f ff;'#7$%ff Eff0'ZOl1 $%3m:nopqRSer'#7$%3ffD Aff'QAh'$s*'Q4 At$G$ff#'J,f$Gff/$%'($%7$%C 1P*L#$5$%7$%'=, $$%'7#'8s$%'1&Efi) $u$<Q$v A##Sfi$#3fA0Eff0$9/$%'$%3ffXDEff+$f, )$%8Gc#'J4$ff/+$%'1HI9ff#'9%ff0']L$> $u$<w$v !A##[#'bff0't$xy#$%'1@8Gff''$%:z#'w3<$%9C$%'ffL#$ff+8GffA67$%)!u{%ff0#'BS-|$%89$:1$%8G}ff6<('},~ffM*'=$ff0L7$fiEff+$z8Gff(LM$#'13ff3ffL#$h$%7$%'@,-$(ff/$%' ff +89 #$%$fi#',_8Gff)*+'5'5$fi$%'7#'8s$%'1fL$%ff7+*+$%(89$%4 ff0fi$Aff$<>46FA('$%'$<46#J Aff''#'/>$f4 $%$9$9/$%+/3ffA6''X4c'>OPYgff ff<+#8=##>-ff''ffffA%:jnopo>%<$%89+ fiff7+A%:;nopR3Sd|6$vff8s *$:'$f8Gff9L$f#'$%$$<=#'5u'<+#'/Gffh+$ #$ff<+#'/=,_8U'$fi#G=ff'$%646#+2ff+%$6ff'Gff0Aff$c8Gff BSd2Ez46h8Gff(LM$ 7#$%46$<@ffgff( M$Affd%ff$6,B$ /$%'$%ff,iff89$%46fi,YZAff''#'/=4 #$\$ff'#'/SZTJh'9$ <$K#/'s,{ PE%ffX{ ffg+#$%'1$%zOL$#3%:+ ff'ff'<+#$%3Rffgff%$% gff'=L$;#'='$6,{$%7$%3ffX1 1K#L*$f#$%'3ff#'Zff'<& Z#B#'9ffc$<$%$%8=#'$<#$%'3ff#'>Offff3ffff'B:MnopRg8GffGffXE)9L$h7#$%46$<ffcff9 $Aff-%ff$fi0,a2,i3ff089$%46;Sz46fA6'$%'$<=46#s$ B#'$ff'<'*'$f3ff+3ffL*=,BYZAff''#'/=4 *$fi\;$ff'#'/SU6A5$Aff$s#5t46]'$%'$<[46#W$>3ff3ff0L!#],9<+$%$%'15 $@,= Eff0''#'/Og:;ff0B:UlL3ff08Gff'Aff'B:Znooz+Eff0'<$%:gnooR3S-6Ac4D+98Gff#'#.+'$%'1ff$<'w'*'$C3ff3ff0L!#t,NffJK#'/#$%ff/+$%'15 Aff''#'/]42*b89 #$%$#',_8Gff)*+'BS@4D+'$%'3ff$='t/$%'$%3ffXc+89 3ff#'ffXff M$3=, Aff''#'/t46##'89 #$%$5#',_8Gff0#'B:'KA<$%3@ffA>8=##ff/$%'GK#ff#'m:Dff'<t<+A$9LMt'#'$.ff0'<QB!#'$3ff3ff0L!#S$%ff6fffB!#'$5<$K#/'?ff'<ff3ffL#P:Zff#/J'PE<$%$<?ff'Jff3ff#7$h #'QOP>Dff1ff$%:gnooR3:MfffLM$%$%'ff#89fi'$%/#$$<#'Q$=$$%'1h$ff3&OL($%$FOP1$fij$%''$%'#":dnooMlff8j$%''$%'#":dnooRRS$$ff3?+'J#',_$%$%'$5, u'#$%ff8Gff3ffO #7$@l1ff #$: no+pq1:-nopoRff89$=ff'ff /$%'6ff6#$zh#',i$%f$fi$fi,aff'5ff+8Gff'BSZ6$(ff/$%'zA6/#7$%'ff&*8*$<.ff+%$Q$.ff08Gff'B:Dff0'<tA@$v $$<wQ/1ff#'w$%'/w#',i8Gff#'w<$<$C$.8s *$%$$@,$ff8Gff0'BSJz?'3ff):j*'w$@,_3ff89$%46J<+A)$<Q#']A= ff0 $%:z$ff/$%'fi'$%$<fi+'*>/1ffX*'#',_8Gff)*+'Jfff46A<>$# >#'?$ff+#'/5$9/0*7+$%'J/1ffS(2$%$,i+$:#'54 ff}E2LffL#Gffh891)-'ffff%ff)$:$fiff8@ff'=A},ff##y8s !A%ff$<;:g#$ff'*'/#3=89 #$%$$=A9+89 3ff#'ffX!#>#',_$ffK#L#$S?c4D$%7+$%:ZL$#'/J'#>#'$%$$<C#'tffM$!u{/ff:g'$58GffQL$ffL#$.+L3ff#']$5'$$ffC#',i8Gff#'B:6ff'<t%$%$<*'wff/1ffX~S>K']ff<<+##'B:z4D+>'+89 3ff#'ffXD#$ff'*'/Qff89$hff0N$G/#7$%']ff08Gff'E,_!#t''$$<;:Z$%'ffL#$5$ff*'/Jff'J3ff$N0,2$yff8Gff+'ff0'<>$*8*'ff0#'/?$s'$%$<,-ff7A<+#'/3ff$D,i+8r4 A>$%3ff$ ff$h'$ffffL#$S2Efff)89 #'5ff0 $ff+8Gff'sA6,_#''$$<@8Gff97$%@46$dL$&,~ffA$&#'8Gff'9$ffX*,_$ff0 !A%ff)*+'%S2$D ff0g,{+g4D+(4 AG<+A$z8=##ff/$%'g Aff'ZAg$Eff0$<=fA$a#'G<+A#L$<effiOPz'<fiff$%:jnoppR6ff'<G$N89 #$v#P,-8=##ff/$%'fi Eff0''*'/COPj$%''$%'#"G1$%:fnop+oR3z4D$F#'17$)#/1ff$$89 3ff0#'ff<+xy#]ff0GffA$@<$C'$%3ff#'P'$%'#'/@$Nff+#7##$c,}ff0'ff<<+##'ff-ff/$%'%OPR3S3fi>t-+fi2~f E0%%3#f=%;;h9a6(%1fAf+%%1f_AKA% %%1#f#Q5##N01c6P2dKfiA=APE%XccA%Gg+%1s%G9PcG@UJD3929%%3 #Q%6%%?A#JQ02%Q=%%[+A#]%+#sD+tPf]J#GBfi3JKb+##Bj%%#?[19)XC#[_+1#_%%A9A59.+96+AsJ5sAKA%=1 66#>%%3)*+@Pj%%#J%f6%>V9E%X 1 6P2*+G%%1#.%0.M955+K#_9%%)%130#B.c6%%Z51#_%%)Es%)%130#hD5KA%t0y+%#J+%%1&i+APE%XDA#>9A%g95h~fi%.s-E##@1K#*%#h%3hJGB3h#h#%]+B%%=%1+#9%%+#3%%-G%%D+%%3;~+36 @%#s%1z6N3KfiN_I30 d5%cE@Az+%%30%1-+A)s*5fZ*%3=+3M6! fiM9%)%1A##Q*C(%%1#J>6 9#h#9E##hh %B+#gh%#%gc%691g+ADP*!Az9X2*+fi%%3#@#}E2(K#50G961%#BB+3g +Afi#5%%3=% j*%d306y0%16=#69A6+%1Aj*5fi1=%6#36)%X#930%g@GBzzXKB-)igA#)%%c@fi%h%Aa%)*mj**fiA>#=9AU_ fi%%3#50aA%2;#)A#9Ah ic%%%#5A%=6#3;;#N)AJ#J> y#%1h90E)ifiN+% {%#J-A6_6)*@%+A=A%K ?E%=6y+%19QA#J#t%#t%#dc%Gy+%1h#JA-#_G)*+?Q9%+*9%NM%#J50f*hAh#330*@At5_DA#?%%+%J_fi9%3s%%3#_ #j%%3##AJ9=%fi6%#yf>K##GM%+#3cAfiM#1+=A-#J@%)%130#GK#%3-c6%%+-K###36]}#gAz1P*##59%3+K#_9%3h%%3# %fifi=%fd)%X#930z=#1c(+%A_3+(AK_=3h+%#z9%#9Mg %B#K#s93+3#6+A>%93#h%A=2!66+KA%(P*_s%309)%Gfy#%1fiA%;cE>Afis91cK#0#B6#3D*s*@ aE0*@ #ff;#sAD+-5*%zA#6#s*%f*_@#BfizfB#fi330*fi@#fi#3#- 6%%0#9%39%@%;A#.2*?9#%=#_G0#>A(+#9#+AEcA&G%=303>fc{0iM6#GBg3+ #(#%)fiD()XWaE0*#;0#=A }_3;gf%%3#6f3#5N+9#%(AK {%#aaE0*##N%@B)=9%%3X%%1#9g=9#{%%#%+##Bfi6?+A%K#50ZA#[ ##(V5aA#bc!#;*s#=##%+G#%2=i9%6ZA#J #;#A=5%%3X-_39%6 %@A#>#%3X*t%#Q%?M+#;t6s*#tfAhi9%6;gJ5AM1K##N0@%1#-#3%*=fi!"###$%'&)(+*#$,-/.'-0"#1##$%'&2 304 57698:<;>=7?@;>AB;'5DC0E)FHGJILKNMPO%QROS,T0G UVKWUVX'Y[Z#T]\+Y^ZFHGO_Qa`@Y[Z'QRUVX#bVGcG Y[Z#K0ZPdfeWK0M'gbhGci0S,GcIjGcbgc`9klKNbUVX#GS,b+X#GO,mPknMPOLoK0e pe GcZPUgcqr);#sV;'tN;'573#;>Eu X#K'` u q>vwq,`@xKNmyobVK[knU`az#qavwq,`y{}|~OOeY[Z@`!z#qP#q@f0HN' +>0c%ff70yPn+00_Pffcq u Q#QNS_gKNZ#pnFHGgO,Gcd]M#POS_gXPS,Z#7K0e m'Y[ZPd0qu OO,GcZ@`az#q,`PxGcZ'QNO,Gcb`az#q,`#{!Y[UhG0` u q'7Q#gcqq#N0PqDL00f^_RL0>'f0qa/K0bVfY^Z+Y[MPkneY[Z#ZM#POS_gX#Gcbgcqu e#bhKfgpn<Z#NGcbgK0Z@`#z#q,`#{0UVGcGO<`a'q>00PqaZfUhGcbO,GYiNS,Z#O_Y[Z#ZPS,Z#'`'#GoM#US,K0Z)Y[Z'Q/K0ZPS,UVKNbS,Z#'q<Z)LN_P[+V0^`fm#m@q>009qK0Z'Qa` u q!xJq,`!{YNgVgGcb` q00q/L00f[_+,cnlfn+n L_N >n0'q u PO,GM#POS_gXPS,Z#)7K0bVm>K0bY[US,K0Z@qbVKPK0Tg`0+q u qP0NPq u jKN#M'gUL@Yd0GcbhGQ K0ZfUhbVK[Oa0dgUhGceklK0b7Y+/K0PSO,G+jK0'KNUqaV70P'0+Lfh0ncff0>fn0w0n0>`@0`@c^0#qd0O_Y[Z'QPGcb`+qjN0Pq7K0eWmPOGPS,UdjGgMPO,UgklK0bR0GcbS_YO\GoKNe m'KfghY[PSOS,UdaqZBLN_P[+h[0`Pm#m@qa[0'^'q\GY[Z@`[+qc q,`{FHGOO,eY[Z@`0qc!q^0#^q[L0>'f0'J70>nNq[/K0bVfY^Z+Y[MPkneY[Z#Zw M#POS_gX#GcbgcqbhK[O<`fwq,`PY[M@`P\wq,`0{0M##bY[X#eY[ZPS_Y[Z@`Nvq90NPq>ZUhX#G+7K0e mPO,GPS,UdwK[k!\K0eYS,Z#pn<Z'QPGcm'GcZ'QPGcZPUO_Y[Z#ZPS,Z#'q<ZHj0f[+V[N`fm#m@qa0#00#qUhSKNZPS`ffq%`yxY[Z#T#gc`@'q%`#FHGO_Qa`@\wq,`'\bY^m'Gcb`a\wq,`'aGgX@`@ffq,`>{FSOOS%Y^egK0Z@`@qD00q u Z u m#pm#bVKfY0ohXffUVK+O_Y[Z#ZPS,Z#JIjS,UVXffZ'oK0e mPO,GcUVGZPklK0bheY[US,K0Z@qP<Zj0_P[7f0 0cc>0/L_>yc++ff'0j0JLycc>n0n_N0> j[c0>_P0`Pm#m@q@0y0#qGcZ#GgGcbVGcUVX@`@q,`'{K0M#bh'Y[TfX'gX@`aqa+q@00PqS,e G fYiNSZ#LSm'gknK0b+ bVKNPOGce0K[O,iNSZ#/IjS,UVX<Z'oK0e mPO,GcUVGJ<ZPknK0bVeY^US,K0Z@q <ZL0f^VV[0qxYO,m'GcbVZ@`ffz9q,`L{KPgGgc`wq0[9qZ#KI~OGQPNGY[Z'QoK0e e KNZTPZ#K[IjO,GQP0G)S,ZYQNS_gUVbS,#M#UVGQGcZfiNS,bVK0Z#e GcZPUq7DGoVX@q9bVGcm@q'zW0f#N`fjqxYO,m'GcbVZ@`'z9qfwq>0NPqajGY0gK0ZPS,Z#Y^'K0M#UjTPZ#KI~OGQPNG0 u ZK[i0GcbViNS,GcIffq@Z@P0n_0fL[N'f+ffh0P7ff>0jN~L0f^fff0000cc'`0m#m@qDy[fqxYO,m'GcbVZ@`z#q'wq,`a{v7Y[bQNS<`@q'wq@0#[q/KQPGO7ohX#GoVTNS,Z#igcqaUhX#GcK0bVGcem#bVK[i0S,Z#'aY eY[ZPSklGgUhK'q<Z)L'yc+ff'0j0La'n0n00> j^0>_Pfj0f[+fJy0''n>0n_N'070'`[m#m@qa000['q)oLY[bVUhXfd0`z#q,`!{xYd0Ggc`q N0PqR0K0e GXPSO,KfgK0m#XPS_ocYO bhK0PO,GcegknbVK0eUVX#G0UY[Z'QPm>K[S,ZfUwK[ku bVUSyohS_YO<ZPUVGOOS,0GcZ'oG0q]0f'J'n_0'`0#`#P00N#qfi0#[#)##N#0fiff "!$#&%(')+*,.-0/1&243507698(!;::%<:=?>@A'CB@DA=&DFEC:ffGG!$%:FB7H%<IKJ"LGMON PRQNUTV.W0XYMOZVUV[N]\Z$X Z^^&_&`>a7bcd-0/1&2e50C>@!&'f:%:=F!4g&Aihj:lkm8< =&n!;: Fo"ffGf%:7"BdffD7Yp7K/1/&.q>@rjrs:t:!4f%:!$8s_=:'H:7uv-G/12w5G7hj:lkm8< =&i6KffG: &%f%:'yxz@o"ffGf%:'@!4: ?6{8|!4:')7rs:fi}@Lf~ Z0ZGNUX\4C~fMUZ4MU?W0XYMOZLGXTMONU~XT&V~NUXM,~XZLfZ$X Z"~X?J"L0MON P N|T&V.W0XMOZ$V|V]N]\Z$X Z4pp72+w+2w3Y_'f'YB7:::D;8<7-G/11/45GCbpY0!4f%:_%:EC:ffGG!$%:Bdf%&IFEc'%<:=F!_A8f%t*K:f%I_ G8fir:RAff'fG%:7KoKG8| !4:7KR-* ')]50KJ"LGMON P NUTVKW0XYMOZVUV]N[\&ZX ZTXY~,`MOZLF{N(NU~X*{8|'f#&%qff%<:ffG_'f'jCBd:::D48j-G/115G@7tsuY%:>@!'f:%:=xzC:tsuY%:*{ff%:ffGI.r:}yLf~ Z0ZGNUX\;m~HMUZ4MU?WGXMOZLGXT&MON|~XYTVY+~&N|XYMC~X)Z$LfZX Zi~&XJaL0MON P N|TVYW0XYMOZVUV]N[\&ZX Zj!40!40!;H!;:7h;-0/12+50oj:"oC8=H%<D%|ffojpp!&ffD"@DRocA!4f%|ffKj'%=:";x6K!40'dCf%:G'rs:}yLf~ ZGZ0NUX\;~fCMUZ"^MUWR.4C7~&X7~XT&MON|~Xy~f~,`MOZL@ NUZX Z)pp7/$/36K!4p! &%%H%<&A7bjl!4::!4l!;%|'-G/1215GqD&'f6K!4D'{%D&A!ifi!4p7r:J"MO~TMOT4T&X\T\&Z?TX}yLf~0\&LfTNUX\"4MUW0XYMOZLGXTMONU~XYTVj~VUV[~$N|}yLf~ Z0ZGNUX\;{pp7@/e$e6{7.o7q%D7{{-0/11&50b&: &%<H%<&:!$8m,:8]%:!46{8|!4::%:=r:}yLf~ ZGZ0&N|X\4i~fMUZ$MvW0XMOZ$L0XYTMONU~XTV{~&XZLfZX Z"~X?JaWc}@V]TXXYN|X\`$MOZ$pp77/2&1$`/1w>C!4! =6{:D!4F Y-0/12&1507B@Db&:48`4x,%|'ffGi*v#:cqI'f'K}@L~ ZGZGN|X\4~fiMUZWR@-0/450Y2/G+12>R%#'f$`>".uKYqffD!4p%7>".*C.-0/$12w50R,%<#&0'%fIts@!'f rs:xO:ffG4xd%<:%ojA!;0!Kr:}yLf~ Z0ZGNUX\;y~f"MUZi^4MUWR`4,d~X7~XTMONU~Xc~f~,`MOZL, NUZX Z$4pp7Ywl2$+2w>R%#'f$i>"CuKCqffD!4p%>"c*,j-0/121+50 rs:xO:ffG4xnd%:%ojA!40!Ej'%:=,%:=qA:ffG'rs:}yLf~ ZGZ0NUX\;~ffiMUZ^4MUJK`4,a~X7ZG~LG~H~,`MONUX\,pp73//G3e>@'H:'ffDG%:7dq.-G/125G7!$8B@D&f%'C4xKhj:lkm8< =&"%:?ocr@!4: ?>@gYf%|ff'@"ZCZXYZLQTMON|~&XR~,`MONUX\&7d-5043l+w>@'H:'ffDG%:7qCCh"!;G8<g8]%:=uKC6{j-0/$1250B@D'fI:D'%|'?4x &%<=;%<G!$8!&ffD%:'_k@%Dp4#l!4g8@p%|'f%|ff@p&pf%'rs:}yLf~ ZGZ0NUX\;~fCMUZ~XZLfZ$X Zy~&X7Z0~LfZ$MON TV4J"+Z MU~f"yZGT4~XNUX\T~&MRiX~&yV]ZG\Z$pp72$+1&2>@'H:'ffDG%:7,qK-0/$12/4506{8|!4:qI:D'%|'o8<&=4%|ff!$8c6{0'fpffGH%<#&?rs:}yLf~ ZGZ0NUX\;~f?MUZMUW0XMOZ$L0XYTMON|~&XTVY~N|XYMC~&XZLfZX Z"~X?J"LGMON P NUTVYW0XYMOZVUV[N]\Z$X ZqffDpp0')+&-0/12+w50EC:%#G'!$8698|!4:'{xOK>@!ffGf%#">@gY0'd%:E,:p &%|ffG0!4g8"*K:#%&::G'rs:fi}yLf~ ZGZ0&N|X\4c~f~fcJjJcJaWQHpp77/e&1$`/e;30fiC(fi"_(4_d4.G4G{+|$9dO fff`|$fi j+f U[7d @f l4_0+0 !"f#%$& G4 Kd77.!')(C**+s(,jG{04- .#0/, 1 .) 2d4K{G4G4365879;:<6=?> :)@BA):DCE:F<G=IH=I9;J<KH<GLM>N:OHCEJP<G9;<RQS9T<VU%W-Xff=I9YIZ Q:<6=\[B]FCE=I:F^_C$K7`G4bsG fH<c de04Gsd4K(`_Rf.{0Pf0_CK(0;f_<?fhgvf)2_i#Gs_skj AdJY7.:O:.LP9T<-QClJ)m4=;n-:poo=;n0q.<6=I:AO<GH=I9;J<6HX6riJ9;<G=s?J<DmE:FAd:F<G7O:!J<0Z!AO=I9 79;HX6q.<G=I:FXTXff9ffQ:<67.:$`c7 vu 70$-wyx0zj04f b(&G#&<H<&$N${|4G"4G#b$& 0;p s{jNA)J7O:.:OL9;<RQC|J)m=;nR:_[BW-^}^~:FA,s?J<DmE:FA):<G7O:!J<0Zq|H<6L,[v9T^~W-XffH=I9TJP<%Jem!:DnRHF9TJAc?3 L9;<BW-AIQn`G]47${Y0+0JA.^~W-XffH=I9TJP<%JempGA)HL:OJ)"C!9;<0jNXffH<6<G9;<RQ<6L:A0<G7O:AO=IH9;<G=I]4${47.G# 7.fiff fi!"$#%$&('*)+,,-/.1002433@BADCFEHGJIKGDL*M/EHNOADCFEQP_a`cbedgfihkjmlonqper56789:;-/<=,->?7#& :*3/<,-ASRHTIVUWIXYC[Z;L;GD\^]sutwv!xqyqz{tqy!|y }qtq~!;"qy~e~qvqiSi/4^ =^wo =w =wH4*w4 ef;nrwh9ab"*rqlq}q y1tqez{}q i}quy D~qvq* oFk/4 =DD4{io4/=9"ob"eranrwlq}q1qqyqz[~q| tquy yyu4 wk4 {"1=J^ /6="4!io e!91 { o^ $ ^!o 6(o H$ $ ou% $"* 6($ 1 =4q 4D/9J=% / % { o"!$$ *; "* koo [o *[* $^o 6 $1 1 $ * $ i* $ e!o 6i 66 9^ ooff fioe $ [ $$ *% o{q 6 [91 e 1$ ${!= $[!{$ k$ *%$99oo i$$ *9 1(o6 16 $ 9o1 ok* $ [o$ = == 9$9o 6 ${ $iko 99 9$ =% $q4DD = %1{$ $$ 9 okq ;$ ;;D u e ;o{$ q 99%o $ ! ;#" u$6^$ {6 i$ ; $$ [$ o$( %$ $ D6$ a$1 6[$$ & $ o$ 1 J* $1;$ 6;1D 1o% $'$ H$i $ kH ( $^ $o 1i 61) ' $ e^$ * De+'{$,-/.01&243i!5106879;:=<?>A@B&C9/CD@9AE&F7B@GIH;@BJEK:)LI:)E#@MON;PQ<REJST:)7MU@&<V9AWX7 B#MYCE)<V79%C[Z;7\IE&C]H;CB#E^<?_\A`CB8:^\IZIa=@b_Edce:)\;_JSC :!<R9AE#@B#9;Ce`fMU@b> <?_J<V9I@gh7MU@]WiC _E:cj:^\;_#SOC:kCd``lH;C[E)<V@9jE:4P]<VE#SmSI@H;CE)<VE)<:]@GISA<VZA<VE&a^C[\I9;> <?_@bnoc;_C9Z;@9;CE#\IBCd``RL/@GIHIB#@b:#:)@b>p<V9YC%:^EC9;>ICB>Tq;B:)E#r*7B>A@Bs`R7 D<_cjPtSA<u`V@7E#SI@B:+cj:)\;_JSYC:kvwAxy7WH;CE)<V@9AE:E#S;CE@GISA<RZA<VEta^C\I9;> <_@mS;Cdz@%SI@H;CE^<RE^<?:JnIcCB#@:)EC[E)<:)E)<_Cd`g/h\IHIH;7A:)@E#SI@CD@9AEPQC[9jE:QE#7Y\;:^@E#SA<:<V9AWX7 B#MYCE)<V79{EJ7|MYC}@O>A@b_J<: <V79;:gy~I7BU@GoCMUHA`R@c]C|>A7I_E#7BMp<RD SjE/9I@@b>{EJ7>A@b_J<>A@PSI@E#SI@BmE#7C>AM/<V9A<:)E#@BC[9jE)<VZA<V7E)<_:mE#7C|H;CBJE)<_\A`?C[BYH;CE^<R@9AEO&B)<_g7{CHIHA`VL:)EC9;>ICB>{E#7A7`:7[W>A@b_J<: <V79E#SI@7BJLOF=:)@@F=\;_@/Ce<uKCIcb ANW*7BC9m<V9AE#B#7I>A\;_E)<V79NcAE#SI@%CD@9AE]M/\;:)EC:J: <VD9OHIB#7Z;CZA<`<VE)<V@b:c7B)#()TJb**cE#7/zCB)<V7\;:]@z@9AE:g4~I7B]@G;CMHA`V@cIE#SI@>A7_EJ7BMYCdL/9I@@b>OE#7C:J: <VD9CU>A@DB#@@7W]Z;@`<V@WE#7C[9@z @9jE%:)\;_JSC:k+&B)<_/S;C:SI@H;CE)<VE)<:#nIg/@P]7\A`>E#SI@B#@W*7B#@`<V}@YEJ@b_#SI9A< \I@b:W*7B_7MHI\IE^<R9ID/>A@DB#@@b:s7WZ;@`<V@W<V9YCHIB)<V9;_J<VHA`V@b>mMYC9I9I@Bbc \;: <V9ID%Cd``lE#SI@>ICECCE&S;C9;>gs 9E#SA<:sH;CH;@BP]@<V9jz@b:^E)<VDjCE#@E#SI@HIBJ7H;@B#E^<R@b:t7W&79I@H;CB#E^<?_\A`CBtW*7B#MYCd`<:)MWX7 BQ>A7[<R9IDmE#SA<:gQSI@%M@E#SI7I>mP]@T_79;:=<?>A@Bdc;PSA<_JSP@%_Ce`u`&E#SI@U^[l/ 8/bAbcS;C:]7B^<RD[<R9;:EJS;CED7Z;C_J}EJ74@B#9I7 \A`u`<TC9;>{CHA`C_@|FdvwANgE/<:/@b:J:)@9jE^<?Ce`u`VLyC9CHIHA`<_CE)<V79y7W(PS;C[ES;C:/Z@@9+,,6-fiuqfi"$##I : :468eff8i817#& # fi%&# #*:fiAI&]IbIsQobddVbO#Il+l) (#=AIbdIs]I; tAb AV#%)#eR j)*#II;A)%]%#VA##b)JbUR# #AVIY/A#&;V#*#/A/VAIeQRbA;)yI;)*A(b # VAVI^YA)/#AJb;VT*#/AJ|;)OIJ;AR) )J)VII)V{%)8IU8=dV;#]b #;#JR)IIj)]*sIe{J;K]#]#b )AVI%; V; R A;dX8u&UARJAb#)VI)V8A#R; V A;d/;d%bJ8#I/I##)Vb&VA##b)b I#mdRU])=;)/OIA&VA##I#[)Vse %I ;) *AI; o% ;AV(I?[; Q ; )&[l#I]I;#/I#b Jb )j*A *;m l #I]AV;#I#b # #d * ;s;#IY ;)A 8XAJIO])|AbJ)V;bA#{)I;)#IV; V ?A;e?J) ;b4bJO#II;#IJb ?[#bIAJ)]K;dV 4VO#I #b *#)V];AJ#IR; V A;dm 4* VI)VI#;[AuV) )#)VII)V#IY^j# VAVO])IJIp[j%[|IdVA#[;VRyV{ j; V)VAVIm#/IdV;A)#)V( )J)VII)Vl[;#I II)VI#II# ;AuV)RI#m#AI )#)VII)Vm]I/;A * )I#II;^b#I/I)V;JRAVOV; fJ;Y#OJIjj^pU;#^?AI)V% )#)VII)Ve (#I)] )I (d&#I/])I#/[#Yb ;dRVV ?b)#)/#;#IAJT[;VV V{ ?#II#bJ)V#I*)VO[!])IJ) *VIO #;d)O#^?=XO]QIp[II#jJ)m%Ab#)Vb{IARbII %]#;dV|AIe JII#bJ)OAYdVjIA*#JI;#VO#A]tdV)VOI; YA)b+II bAV]#b) ;AV%#;V#; J# |#t#j; ;#)A)VUVj#Jb)#bpROJI()AI#^?Q;;dV]#A X)V A#;sA](]IA#;V!J%;#I)AI#^?QedVI#A X)V#( #;AV)IIj)%]Q[jJ#b) ImAYeR8Ij)AVeK;)VA;JI% =I;^R &#I*R]VIUX I]X #/A?"!.(1( 32,+"- */0465;: =<C9?>fi<fiff$#&%'! )(*+"- *879@ BA"87DFEHG )IG&JK"!Al[X G#]L>eus;)Vj]]V#OI;)V)IAVARM#I; @A^ P e GQ'"j! * IGNRS 'T L+> II#j VY[#VUT WyV ;;^RAs#;[KI AVARB#I?;dI;^R^?X@I< ]I A?eROJA? *#Yeu) ;p#I(Jb)](Jb0> [II#j VYJRWVU@ JIJ;Y> ; )V1T WVU@ V b^RDOQ;Q '! )(* oIG 8Q;Q R Z U[ L> IIJA VY#VO[WV [K;)VA;dAVI/bA@D3"A! l[XX8( X]\^#&% *u4( *8>8&_ )t#AII AVAR&# I; A@` 8 ;a ^ P eX8( X< #;?+4j# #)V;A#];;V##I)#Ab>)&_ );]I;[)V)I@ II;A)/#ITAmdV;=c:8]I]%AJY; A%dK])It]V#AYeRFj^%e );JJ;#I%)tKV; V ?A;eX%#[) X VI0 ) P (m)I;)t!JIj)#^?=X VI^"j! *AII#A Rm#VfT V #IV; VA;dJ) *VI^"A! l*Yd)#[) X) II#j VY[#V[WV ]#IV; VA;d*TJ) *|'! )(* ;JI%VA##I#)Vb]8( X &V; VA;d#) * RI."A! l*;1#& % XB9 ?&^#dVA)*#t#()I#;bd;b#b] ^ P eX8( X 8IIQRII#A Rm#V0T WV &#Ib)/)##;#IJb#bIJ#I4*)VUf)J#;#I#bVAJ0) P 4( *II]A#bs#; )VgITV;hWV ;#I];^RA#;s AVAVfi# I; ;d4I;)V)&;.&_ )4I ARAVi^ [I; ?%A #;V!KI'#;&_ ^?Q;&I;^R^?])m$) ;))6l AVktj #]#;[bdVY#A UAR#I/VA*#Y[)V#;l&_ ^?p%OJA?b#^j)dV|#Jb#b|tV#JRAb|U A?#I#U;^]O ?I T;dU#ImVAXJY)Vm#&% *;X8( Xn9 {d);)I#;o8pfiqrstHuZvxwyuz{tH|arst=}~rvvBszuZ"0W8X80"?6;l;OC'8$MP"UW8lXgn48"38X1W88"h;l"b'8MP"&^"$"PWMP"?8fZgBW."g"ngC?aU 6k 8$XWa8" .WP 80X"Cg8M"3"WH"P,;^;W88W8WX=."W;18"084.n8W8n.X"aWf;0 X"W W"."a 8 ;a8UX"W;8l8?X ;8.88l;ab68M".; .;yW4ZW;1"WP .48=c" 8.X""6;"P8 ;~W;^~"W0",h? 8~IB"&fiB"6;$WW;8;;8$fiaC X.;W; ; W;& WW8P8W?8W88"a1C81lWW8l. 8C~lW88"6cf" 8=,1 c)h4fcOW8?;mI8X)?hX;8fi&a"6c8k"./^""8fi~ ""WP a8,8~"X"W;f8/PW"?"I;~W/W8,W4IWWc)k W;c8a"FP880X.X W;yIaWW 1X W;."l k8W8;C"h ""~8;1;yZW;0""4W;1886cW1~"Wf8l8 ;k4kW1~ /$ W8];XW"8~WC 81 ;MI 6;~a"WP "XfiWCWXM;C X8""W 8anW84P833I8W48;"Z? XO/;xyP"=Z.;" ;y"C8;.,;y0 W; ;"CWk /"W;y""fiWl"3 8C84"X8~ ^ ;~" ~ ;~f88WcX..6".1XW8lC""C1 W4W6; ;."" ;;"88^U"W;B;y1WUk PW8P.W8;6;P"CyUP.W,;=f "?P ; W."." 4W6; h;80~,I";fikW;8 ga"fi&;fig ""HWX8X&08"WXWfWXOXh" "fia"k88&0"466l""acX;WaMM6&k;~"C; ;X"W66cc "";fi$W"W8n^.;W8?1^".W88"k;aaXUW;W";c"W";;W8B; ;,1C"W"Cfi/ f"W;y"84h" ;8fia"6c8h&"$B/W8;1"fiC"M"8kc1XhXPW8C8 C88b" 8;WX~";$$;X"8&W86&^h "="i;UB8M"]; &CW;"XUX8h W.";~X""W84W;~WC8 W Cccfi"W66; ;M;~88IBCM;W "CW"cWk;k6HXh ;MC.;WC"W&lX1WX;~88C6;.; ;"W66;IWhW;W;WX"W; "886 .n,8WbcXyW;^"=8"0 W."."y"W8B8y;0aXUW4nCWFk"XccWc,lX" /H"W;.;"XWcX" 6;,.WI, c^CWF.WXWc;="^8 "]~X ; ;C"W6;Wh18"1.icc/yW."81/W8~.Xy PW8 181";W1186k PW8bcWP" ;fia;W?" W;a"CXh"?8?;BZ"$k4 C"&?"8h? h"X8 ;1XUXU8 ?80?W";Wla;^"""lC?" C W;a;W";a ";1"? W ;;X"W66cc8"4"X8UhXlIcM$8'H6hC".8HIW8"fmF8$"WfiCfiff bfi"!$#"%&'#)(*!+,&-/.0%1324.5.6 #77 89 : !+883!$;#)(=< ?> %A@BC%A79D>E !F2G!+#"!H&-I#7(Q[Z \^]&_5`%&79587fiJ2G> %&#)/!+9.5@&BG!LKNM&OPfiPfiP+OEKQRNSTU> %&V>E !W2C #7(YX#)(%&#a> %&b@ -cJ7.bfid-^7 .#7(fi!+587fiJ2G> %&#)V!+9.e@&BG! :f%&.bEB49 :f#7(d-^ 7.5BG% !g&-h#7(d-^ 7.iKYMgj k PPfiP k KYQ j :flm( 75fi% >)(nKYo j 2C!E24#7( UK JUpNK RSI> %&=< 24 lq#)(Ur&l B4fis I@% !+I% !L8BG% >)24st>E !$#7"%24#"!L V#)(I87 8J7#+24&-u .0%24bEB4 .b 3#E!!7%&#+2G!F-^9 24sIfi% >)(b%&#7 .,RNvJNE1%&.b8B4 : #)(h>E !+#)"%243#UwxKNMxyWzf{ |}KN~xyWz{wEd&X!7%9!#7(%&##)(m87 8 7#+24 0&-#7(U .0%24t!7%A#+2G!F-c9J24se!+J.bU%&#7J.#7(%&#a>EJ3#"%2! K~m% !L%[>E &+>E#2G!L#Wl2C>EI#)(U87J8 7#$2Jt!7%A#+2G!F-c9J24sb%&#7 .V!#7(%&# >E 3#E%24V@ #7(=K %&0K ~ % !a>EJ&W>E#"!xR m24< =%.bEB&-Ie:l /> %&0EI#7(U #77J8395&-#7(2G!L.bEB%J!#7(Y 3#)7 89eA-#)(U< fi>E#) h #$2s#7(b8) 8 )#+24 !I&-m#7(0J2 7 #e%A#7 .V! R5STV!+(&l#7(%A#fi:% ![sJ7&lU! BG%&)s :#7( 7V%&)e.0%&39.b )*.bEBG!=l 24#7((24s (q #77 89'#7(%&l 24#7(B4&l V #77 89 R ( 7E-^ 7 :g.b?EBG!l 24#7((2sJ(#77 89b .524%&#7 RSY!+Y#7(2G!U ]& " u\7Z \^]&[ f]&_ u]A #75!+(&lD#7(%A#a 7 Y&-@EB24E24*sA2<J I% > >E EJ2s5#75#7(IE%& .b^l +BG!.b #7(H2C!g>)B!+EB49V7EBG%&#)fi0#75#7(/%J!7!F24s .b #&-N8) 8 )#+24 ! #7b%&#7J.V!L#7(%&# (% ! .V%1324.5. #77 89V%&.bJs%BB% !7!F24s .b #"!m>EJ!F2G!+#7 #ml 24#7(#7([>E !$#7"%24#"!L24.b8!+fi=@9*IbR(e>E >E #7"%&#$2J*8( .b J;)EBC%A#+24sV #77 890#7V#)(["%&J.b^l +BG! .b #7(02G!Yl EBB4rlgyW %93fi!x:mfi X:h {"RT'8(9?!W2C> !x: #7(El +BG!7%A7V#)(=83!7!W2@B4>E E%&#+24 !5&%!+9!+#7 .#+9382G> %BB49D>E !F2G!+#+24sA-U.V%&98%&7#$2C>)B4fi!5Je.H&B4fi>EBfi!x:h%&#)(V.5#7%BB49E1>)B4!F24<87 8 7#+24fi!Hy 5%&#7 .V!{U> %&@ :-c IE1%&.b8B4 :aJ%&#7.!$#"%&#7fi! R= (0>E 77fi!+8 J24s #77 89.bfi% !$7m2G!m%A#h#)(U(fi%&)#hA-!+#"%&#$2C!$#+2G> %B.bfi>7(%A2C> !g%&0#7( 7.b9%&.52G> ! Ra ( )[%&7I!+@#+B4I@#24.b8J7#"%&#J2 7 >Efi!@ #+l eJN<J24 lm8&24#h%&d#7(%A#N&-#)(h8(9!F2G>)2C!$#"! R ( .V%245 LB2fi!24>7(A2C>Eg&-BG%&s %&s RS lm%&#N#)UE187fi!7!!+ .bL24#7EBB24s #h%As 3#!r&l B4fis :&lm(2G>)(52G!lm(39l a#"%Ar "!$#7^ " B4 s&2G> % !Ja!+#"%&7#$2sg8&24#fiR ( .b3!+#!+8fi>)2u>mJ2 7 >EU>EJ>E 7!>E !+#"%A3#!+9.5@&BG! RYSd fi*#)(fi!+5@fi> %&!+5#7(d.b3!+#g243#) 7fi!+#+24s*Jfi!+#+24 !g-^ m!I%&+2G!+dlm( l [(%<!+ .Her&l B4fis V%&@ #+%&Tl 2G!+(#)*% !7!F24s 7 fi!&-m@EB24E-U#7*!+#E%&#7 .b #"!I>E >E 724s&%8%&7#$2C>EBG%&g24J24< 2G%BWRI (d8%&"%BB4EB28(9?!W2C> !gl BG*% )fi!7!m8) 8 7#$2fi!YA-%b!W2sAB58%&)#+2G>)B :lm(2G>)(=2G!msJ "%BB49*>E !F2G 7fi,#75@dlhEBBNJ#"!F2G[#7([!)>E 8d&-!$#"%&#+2G!+#+2G> %B.bfi>)(%&2G> ! R#7( glhJ7r=#7(%&#mE1%&.524fi!g#7(b>E fi>E#+24 @ #+l *E%& .l +BG!Y%&= #77J8390-^7 .8A2# &-N<J24 l >E .b8#+24s= 7 fi!L&-N@EB2E--^ -c ).eBG% !24*%I8%A7#+2G>EBG%& B4 s&2G>)'2G!m#)(%&#a&%&+2G!%& >E<!+r&%5y"fiJ {"R ( 957fi!+#7+2G>E##7(gr3&l B4fis m@%J!+m#7[>E !W2C!$#a&-%[>E &+>E#+24 b&>E !+#)"%243#E!f#7(%&#Ly^24[ #"%A#+24 f{(%< #7(-c ).w yWz{)| ayWzf{"wEdDU%AV|4|}yWz{)|C| [?:lm( 7%&d=%&7aJ%&3#$2 )-c7 -cJ7.5BC%J!243< AB<J24sI%&79I87fiJ2G> %&#)fi! B49 :&l 24#7(dU>E !+#E%&3#!+93.5@&BG! Rg # B4952G! .b3!+#L&-N#7(YE187fi!7!F24< I8l L&-"!+#)^ " LB4 s&2G>Y #h%<&%2BC%A@B24=#7(E24U%&8873%J>7(:@# #7(U!+#E%&#+2G!+#+2G> %B2-^ 7.0%&#+24 0#7(%&#a> %&0@YE1?8)fi!7!+fi2C! J24#7mB24.524#7fifRmv E1%A.b8B4 :24#2G!L #8!7!F24@B4*#7,.V%&r 0s "%Bm% !7!+ )#+24 !5%&@ #b!+#E%&#+2G!+#+2G> %BL2 8 >E RD%&+2G!d%& >E&<!+r%!+(&lD#)(%&#a#7(I 7 Y&-@EB24E-h> %&0@I>E .b8#7fi0!W2s5.V%1324.5. 3#7) 839d-^ a#)(E2LBG%&s %As R(% !+#)+2my"fi J{m(% ![%BC!$;!+(&lm!$>7('%=7fi!$B#:NA-fi%A+B49;fiJ24<%B4 #V!7>E 8 R*L#fi:% !Yl b(%<%B47fi% 9'!+s sJfi!+#7fif:l V@EB24 < #7(%A#I24#52G!2.H8 7#E%&3#5#7,B4 r%&#b%-%Ae$2C>)( dBG%&s %&s RmBG%&s %&sJn%BB4&lU!=%&)@2#)"%&79"!$#7^ " =% !7!+ )#+24 ! : -^BBeL3&B4fi%&B4 s&2G>&:I%&7@24#7E%&79'8AB9 .52G%B>E .5@24%&#+24 !d&-U!+#"%A#+2G!+#+2G> %BhE187fi!7!W2J! :%&.H 7 #7(fi!+=%&7V%BBL-cfi%A#77fi!I#)(%&#e%&)V% >E#7%BB49!+E-^BN#7br&l B4fis ^7 87fi!$ 3#"%A#+24 =8"% >E#+24#+24 E! Rhv7#7( 7.H 7 :#7(I"%&J.b^l +BG! .b #7(.V%&rJfi!m8 +-cfi>E#[!$ !+d2T#7(2G!Y+2G>7(!+ #7#$2sR[ (5s 3%B&-h#7(2G!Y8%&8 Y2G!Y#7tJ2G!7>E&< Ilg( #7( I#7(>E fi>E#+24 #7.V%124.e. 3#7) 839%BG!+(&BG! RS*!+(&l#7(%&#H.V%12.5. #77 89>E #+243fi!#7@*l 2GEB49'!+E-^B:U>E&< +24s.V%A3987 @B4 .V!H#7(%&#b%&70-%&bJ#"!F2G*#7(*!)>E 8,&-ey+%&+2G!b>E&<?!$r%: (% !+#)+2:fi ?{"Rfifiau'uYVCa?&m')= 7 b&f4b77 b7&5 * & 5VA =7GbE fiE+4 cJb 5 3$)G& &J aJU74 m437fiHfi&+4 fi7 7I&7g7 4 VN m7757[E J4+4 =JVfiJI +Y7&L4)fig ^&)V7fiJG &)fi /$+fiEm)&aV345 37) 3H*)&4bm&"$3 Y47G[ $ b J4+4 h0+&7&I7 70&7b++4 +4fi57A[&+G+43J&4 407I47 " E+4 = +h +"&$C$+G f^ 70&+4 *&t"+7^ E J&+u &+4^ Eh7&5&4b 7"&eEJ37$$J&m7Gd& [4fid4&4+4 e$ b5454"&+4 d&V45b^ 3)7 Vb )?m7fi+ &7GL& LGL 73A fi0 ^&&UxNFV)UEm+fiE+4 UJG7E7 cJ7V^"&b 7fi7+ +G4 7&b&7 fffifiU4 7fifi"T*JC)E7b);+3E&,+ V&+G &h+E&+G++G J7+ 7+4 xG7+fi &4J4&70&7E4VfiJG f&,E7bE& b^ +GYb )?^ 7V4 bFfiE+40 b+"A757b FG5)fi+4"I7A/E fiEV45i 77 07VE& b^ +G &fiE+4V 5JC)E7Y&7H+[7fi$e)fi+4"g! fiE+4 eE H"&+4 7Efi7fi g"fiE+4#b d7 77,7b7C)+5&a&7= E+^&)V7fiJG &)fi &07I fi+$J=&N& GE=&G &457U$)445&NV34577 bG $[E )4dfiE+4 &% 47;+Jb/JG7E)F4')(+*ff,.-/0213-547698;:<,56=13>?130;4@:A13,7B57G+fiE$J&hL&4 a)^ 7VE$Jb& G& & m&7aE& b^ +Gb 7f) U4@fifiD5"IVA7 +GGCA7 E4VE& 0^7 CEGF=HJILKGMNPO@QGRTSGO5R7M*&)07 7fi$7fiT4,^ 7Va4 &G mG&J& =7&0&U57E?)fi75J7+E&+G++G4cJ7V&+4 q&"$7^ " H4c )V&+4 JT;) 7EcJ7E$"&+G++G YG& &JU;VLmG)'Gb&&+G&b&/,G&J& *fiF4 fi3 7WXYfifiZ" b7=) V4 b&I7& fiJ \[//)m"$7^ " L ? ACA7 3E WC$+4b&u)fiJC A7/AVE $"&a+35&G&H4]eb+ g&&&+G&4fi _ ^m5+"&$C$+G aG& V& H 3"d+"&A","+)^ " I4 ACH 47tcJ7 &U+E&+G++GJ&3$ bc )eG`ff ab"707 7dcec `fga@hcPc i0GVkj.lm!j5mAlonqp=mArtsvuwj.lsvx!xXp=mArfb U43) 77 7fi0 IE&+4 5 +h[Ay&7&L7 7fi+ "7m7 7+4 H&N V4E4 b "[7A+GFcJ4z`ff ab"{b E744& &&747"&);+ I&m&&+G&4fiT7V+7E$&4)*^ 75G"`I gcJ=E&H +cec}|2~p=7 aT7hcec ifi7E+4fixI^ =T?fifY77 7+4 ,&h VE4 b "Y7&Y&7e)G7 &;fcPcq|2~p=.ga7hcPc fi7E$fi f^ Ufiaf)e) )+4 &h V4E4 b "Im+b7G,G9a&cecq|2~p=5 aT7Xcec iA fi)E+4fid77 7+4 =A4"g&JV4*E4 b "g7&m&7Y4*)[7G,7EG&+4 _ThG+[4&'7 7+4 5E7fi7F4 &7LcJ7o`fga@hcga@o &mG)5hm )!mArGDp}nqp=mArG<j.lm!j5mAlonqp=mArsvuwj.lsvx!xXp=mAr7x )*&=E?)fi7F4 =GL47 fi,7b 7I7I) )+4 =&a V4E4 b " 7&+GF^ 4{`Tc7J &bJI73+YE4 b 3Eh)&+GF^ 4a44 &5"&+4 ueG5G+;E WC )fi7===7J 7$JE7fi7F4 &T7V$ [&m7 7+4 TE?7fi)F4 C)43+fi / J4+4 &=54+4G &+4! .AX@q_v_q!@)w== 2=w)WYv !Gg!v3g_ffeY gq_X=vA_gq_Y 2=Yg=2hwA2 gff_f=g=.Y=gv_=ggGv=q_gq_9=Aw!_ Y= g= =!!=q_! Y= =q_!{= !v_q_==g@_==g=)w.Xwq_g2_9=Y2= Y= q_Yg_yeY_ =< =w ho@gXggT g{= _v=g= =g3q_!{_3_wg5Gwe= {Y =YX3wq_!Lw {Y=X= 2=gY3__=g=gfiZ5725 55e+.wXAZf<bvwv.!7vgvv5fgZZXYzA.y.DD vZ.vYZw5.g9A!7hLfiff +!+.Aw5.<gey!55 e.5v9D.wy5Zeh5ZvA.5X!hPv <g.ZW!!!+.Dgg.ffvAy.e.A555hPD ;.Zv9+hXAv+vZfg.wtA.AhvZX$e"!A5.<!. #<5v.Ageg% $&(' )Z;g!Av+vZ;A5. DX$y)Z5*+<.gDe2; e+Zg;!vXYe e{5vf<2g5+ Aw9A),A% +7- @Ab5DX& /.e2< PL.ff& +evw5vfw50#A e5+Z1evX 2e Az.D 3 4 2.AgevZXffe5!A5.<!. #5v.AhhP 7655w5v+AwZPevw5vXAgeW e+ ey & +5vZe& +!)w.A25Z57v)D !A5.<! .Agev!@{ ege eD9 8:# ;/Zb5 < )= e{.g+vwZ& +ffev.>DGFff e.vT HZ!? ff5v!=ff.g5A55<yZ !ge. e vZ.v @e@ - A:B Cff- 2evEA.W!+.Aw5.<geW!5 we..e& +y=e.gw WA/ A.KJff;5ffAA e 8.Zw3A{2 e!55 !g< #LNA.POQNXZ=RQI SfiHTSfiUWVVVX =R@Y3D55Ye Aw= W Z<. [$+\=R]Y3A55YZWA= ee.A" Z. [$Z= ^65!D+ e2 vA!5 w9w5 g!Av+vZ_< A7 !A5.<! ".AgevZX. #5v.AhhP` $9Z+5 .!A =- a\bvA= 0 &dce"g g fbfi hji 9a g fb c=k Lml\& V&(5eZZegey.vZe.59gvWAhPvA$D55Ye Awf Z<. e W\.A wz!+.AhPhzg5 y7eZv5v W. & +g gW 2- evXD.!+3!v!5Z={+ g5wv+vvYwA+ ed#AAgAge.;A."gn o5A555hPDz v!A.!z2 2<v+=5#Age. Ze ! WAeZ}< Ageh5p+A2eAf9<vvZg5.!h5!5qA55Ye D= e& Z<. % $!55 !g< # prJ={ yg."v cs7j g fb` ht g f@ c=k Lmw"YL.A+7 cs7j g fbX hjt % u g fb cXk A.cs7j g fb` ht q% u g fb cXk Lml\vD55Ye Aw= & $ Y+.9<vvZcs7j g fb` htg f@ c=k AA55Ye D= ezZ555ff5geWA*ey w vh x59!. 5A!A)g5.w!ge5}<WL5Z .!A 7FA #v De;$e+.<XAZy.g+<vvZg5.!h5!ff=9<bvwv{A55Ye D+!+.Agg<.ff5 w5+w- evXA.! ff=59<vvZf+ <g5v+vZXAwm)Z5Ay+7959D++A< #9{ !5!< #Z= 8.ZegeA25A& +.- +<0y7zQ? 9vAy5Aoz P,G5vzw59gvA!<.gXAZg{7- e{5|}~n<n~ & 59gv\A v=1 e^e gffgv.A759gvA .!DX =- a.% ==}- P\5!!Ye.5XAh<. )Z57vXv3@y!!Ye. .!5A}-a v=0 D5}<h<h \h<h A. c Mh c }<={"Sfi1Py z A.8.Zegv\A #DgA e.A.P\` Zg y5.Zv5<egeA.W{ ege vAh<n59gv\A)={ ePz w5<gfgvw.A!!Ye. =-1 3boDfi1 & A25}w] gl-SVVV%S` AXZ5v1fL5 <vAh{. e+A.gl-SVVV%S` DvI"9A;Ageg3@y!!Ye. .!DX=-aPboDfi1 & Af5W=LN0-A.OQN0-5v^D.-A5.<gey!55 e.ffA.5R2ff{.A5! 5{.vY.A.P\` Zg y5.Zv!- !g5.!ge.5 +ZAge.A._8.Xg% Y=<XZvWZ.AZg2 8GvAgenz 2eAf5{.g{AZ.eg5v! .Age&+yv @5555v! .Age&+{.A"57gey!55 <.ZP{Z= 8.Zege 2eAf{AZeXD5 gge&+AmZ<.Ah8.v!D.5<.h<!55 e.^r.gv` # e vZ.v@ <ZXw59g5.!ge5"f 5.<gey!55 e.Ze.5\5"#DgA efWe5!5w enB.Zv @@9vA#<evhh2<h<h k <{5vtgZ.A7 Z.Dg2 8GvAh<nfi7&&T&&&&d&&7&G(&&4T-0&-G&P`^ =:&d%&5-9M-`&\<-pEd:2:=<-\9"-%&=: <<:-`1%fi< %W-fi>5&5g5 -%&%M%:-1-:&% 5-%d&&%9<d-`=:-[:1-:=:<<:W&%%<=&&%%g:51<9`%&%\`&d-&&% 5-%=&&=<11->%1-`:-W[%`<9"%d/K%"&``=T<[W*` P%d7`-<fi-:=1@=%"%d2<% [":g&PP 2%->fip= -%@ % <<=<^1 -T2x%&&,-x1-:-%<% < -(%%&x=&fi9&"&%&< *`% [Q[W%:&fi2<^*2&fiff9&- %&"-<&\- ("9%&0[%=:`]<^-% [*`&\& 2%&"&=&E=<n&%&^ _= <-W& < %%&%&o=:<<:0&%%<=4&%`]<: &&% < %:-&&: =:& =:<<:0&%: 2<oW&o`&%-%&%]<&p9fi%P%&5[- <1<`:-\ [%-g/-:K-&&< K9%: &fi:&-fi :2[1:fi-%&1%:-&&%9<d% :<% [&%]2<-fi=]<< -[<&q=:<<:W&%:- 2<<<%&d- [\<%%&&<-<&<:%n"!#$# &% T1-^=2<1<:-%5=:<<:7&%%<K=4&`%]<:"<%%&<<&Kfi-`91:` ')+ -,./+ 0( 1--&&%[-<*( ++ \:>1&0=`:2K %1&<9fi&%: 2<<5 [\-&&% %-%"=<%=<<%%&^fi-:&fi<9%`&%fi-<-\=:<<:[dP<& 2++ 4 35+678 <fi=,%&0= <%:-, %1 [W:`q9:;=<%\%`&-9((?,%5%:-1^ &%<-`0:`W:-1:-&:1< :% QP<&0( >=:][`9m:=2@ =:&>W* = <%&% fi&\%:- <\%&= %^: `& `-q&&%9`o- -<Kfi]< -\% [:-:A@%-Bm<%nCD-2<E!#$#$F % <`&=9%=&-p[-&:-1%:m:"&&%9<d-`\ :2HGW %`&:-%=<,%&" -2<-<&d=:-<&-m&` ,:,&% <dIGW 2+ ff1%&1" W`%&\ :2<g271 << :++ %`<-d2J&< fi/^ P&&fi< < p`0 [P2++ [W&&`:KMLONQPSRUT8VWQXYWEZ[<fid=:g `&\ff9&-< K:[]_^ `a7cbd e % cf7gihdjO e % k &%[^`&5%:-`&9&1^-9:l <&& <:`d<9 &%:%o-1:& <:27< %%&%"=:<<:7&`:`<:mW-:-1-:1 << <^&&\&fiK%&,ff &* 0:']_^ +ca7cb& e % ,\fgmh&j[U e % + k9n +cf7gihdjO e % + k&:, `K[ <%.+oa7pbqe % ,2f7gihdjO e % + kM]_^ mr :7%&/%:-/`&W9&17-sl:<&1:& <: [d2@-::M<91%&] :9%g <<:`fi- % %5-<%:-1%&59&1:p-fil:&:& <:d>%2Yj \Y9,/h&u.%v WqwU4x gmh&[j sXy z@ (["`*=:][% /<%']_^ %:-/24& <:"l:@ &%-[ "%&Q%fi 9&1:(-&:& <:x(d %|{ % [>[(&(=:][% *`}]_^/~ <-<-%&&%&=%/ 1 << <<&&=% %/-d-&&` <d-%=&&=<" 9Q&/&%%W%&,<9`:<9`%&%fi--x`&`0 [=:-<d p:%fi%"-P&: ]<fi- <p %fi=<:%&5-91:*`&91 &&%9<d-`:2<d-:p%&W&%&=%/ 1 << <<& &W%"=2<1<:-%\=:<<:&%%<:W=4=JM( `"-1%:-[/<% <dW%-<fi:= - ("%&1<&+678 % [ [%&"%-"%<&1%:-_+ -,./+ [<% < ++ - ( 8+ "W^`&\%&:-:>%&\=&&%%gq`:-W% <fi, 0 << <&K&,H+ -,./+( ++ \[5%4%:-+ [,.s+ [p<% < @& 8+ % - ( ++ Ep^%-:-Q%&n%:=\gK%&"< %%&%fi<:W-K:0g 9Q-:\-7% [,&% <^>\-&&%%m=:<<:7&%:%:m,-&&%<:"-:<:%5:\& 5%& &\=&&%%]<:Q<K%&[-&:-=&= "&- :fifi7&s"B$$OU$A\m+$5A`q`.m&q`_A/im_A}$&+`1/A|mA-&Si}&+*cH[}B$&+$[A.+&|+>A[$d`dq+$/'/M`B/$$/+q>Am$/i$$Im}+$+B$q5fi`A+ Q|+ :EB\fim&I A&`$2A|q+$m_`5`d|Am8 + A/m.8AiqI QqHfi`A+ Q`+ BB\id9$AH&$mfiA5AI BJ$*I | Q|JA/'AA$2B`*m*dH/`Hmfi7m$`$5&oq&+\A&`Bfi}BI|m$BBs`A/m/B+}&+q+B+dA7&/.$&+`A}A/i"BoOs+&&m`"Bfi/$&_A_.i$fiAB+&A`AA`mq+$}m"5$A&o+Oq5A/mBB`m\/iA}$/+q+$/"A$/Aq+$/B&.H&+q+&`mq+$[m/'$``A|$mEA$/$`/Im"&m+*qs-m+$Hq`.m&q`fi.Am$/m$HEd\Am25A|i/mq+$\A$$AH&$+[EAI$AH&$U+\Im$/i$ESfidq`.m&q`U}$Afi'&Aq+/QBBBm$/i$J8Aq`&q+}Am$/mfimq+s`AO/ |A/m"/qAAB/qH|m`"A/m}mA&'mA/qfi`$}/iq+$/$q+$7A+$/`[$A'Aq+$BA\&/&+q+$m_E8&`d8`A5A&/&+q+$mm+$`'+I/&+q+$[+d/`fiA/i_/m/+/qA\mBBm/_+mJq`BmUA$/Aq+$'7A+$/BA9+/+/&;mA&+AmA}A&;s`|IOq/q|mq+$/O&H/`|&5$fim$/+q+$/"A$s$Aq+$\AA+$/}$q}}A/i;`_A`/$92qsBmo+[iiqm&+|Q$_|`Q+&A`AA`B9mqm&+9`"fi/\A}&+&A`AA`IAmA&+.mA_/+ $q+$Bm/AEfi/MA&U"&$$$BB`m}AHH&+q+&+;"Afi$/+q+$/`$U&Aq+/'m/m$$"$AAq+$Q`$`A5$AH&H2`m\/H$AqmA'B+A9$AH&HI2io+m``$`A'A$/Aq+$'$AH&}} ms+2A| sBA`&$\&*9[mO`$`AA$/Aq+$$AH&B; +>A| sHA`&$&2$m /_[mO z\ m[}//$Q$/+q+$/"A$s$Aq+$\AA /m_o+H+/m2&';&+q+&+$&HAm/mq+$+I/9A`H/ +dSE[d/`z$HA'A`.+/&`HmA'/ms`BB_A`dm\E$B}q&m/m$i EB&B`H/+2$mBA_$&+` `/$&A`A*+U/m}&+c&/`m/q9fi`\BH&+q+&+2A}+m//$/$q+$/A_Am+`|m/MmA&o+Om/\.mAq5;&+q+&+[$Bm$qmA$q`.m&q`E B$&+A_qA+$d$i|Qi/HB+HmBAA/iUq+s`AOQ$[Hm+$.`.m&q`IA2E+[A`A'fim ffQ$I/&+AH/| fi$|&`_}&`$m&2/iA|&;s`Q+`)$/q9mEB$q_B+A&$.+dA dHB&/`Q+HOBH/$$_B$q5$B$\/dA &+&A`A`|mq+$\m"A_q&;sm+m$`BA_&$.d$q &`=_&`$A!"HMmQ$/&`}q$}HB$q$#9m$`IA}&$.+% d$ d$}H+/m& ( '*)+% $fiiqm&+M$'2sm/2q}_Am+`|m/9$A$ . , E[H+H&m`$/+A+$A9$A5A$/Aq+$'7A+$*9_A&;s`"/p0214365 785 9 m/HA9$A5$AH&>=;_AA}m+:<;?A@fiBCEDEFHGJILKGEMENOFHP>CEDEFQRCESETUIVEIXWYDEZEM[GJ\]^`_ba ced fhgjikfhlaYa m"njopq`pst8r uvYwxm8gja!myEa cEfzl|{2}~kgfhgYmy[a|cEfz8fk8_baj_bms}d f`lmsi8{bfa f{bgja}kE}dgjm^`fms_baa|cEfcEfd fsvik}d aj_2l~8{2}dhA}ffdj_2}8{bfhg}d|f"_b(a|fd iEd fa fh~kg_bEqka cEf"a|m{bfd}klf}dj_2}8{bfhg>}d f_b(a fd|iEd fa fh"~kg_E"a|cEfa m{bfd}ffklfhg!t ha|cEfiEd fh_2l}a fhg}ffklmskgja}(ag!}d f_8a fd iEd|fa fh"~kg_bEocEfm8m{bfh}lmsEEfhla_fhg}k-a|cEf>kdgja msd8fds~k}ff(aj_kfdg6}d f8fkEfh-_ba cEfgja}kE}ffdy}gjc8_bms!}k$^6cEf$_b(a fd|iEd faj_bExiEd mikmsd a_mf[iEd|fhg g_bmskgYa cEfd|fh}O{(~Efdg}E_a_m!~8{baj_bi8{_2l}aj_bms!}kL}d fe_bsf$a cEf_bdga}kE}dfh}8_bEkv6a>d fR}O_bkg6a m_b(a|fd iEd fa>iEd msikmd aj_bmsxa|fd Rgv`fhl}A{ {ck}a`^`f>f{ _b_bk}a|flmsk_baj_bmsk}O{YiEd msikmd aj_bms-a fd Rg8~8{a_i8{b_bERms~EaO[gma|ck}a^`f>Effha m8fh}O{ms8{b^`_ba c~Eklmk_a_mk}O{iEd|msikmsd|aj_bmsa|fd Rgvy_Uga cEfRiEd|msikmsd|aj_bmsf[iEd|fhg g_bmsLb b |nymsd`>jhh8su8a cEfff22 8 n8hpOhh|pux njopq 8hpOhh|p hpstr uz "8c ~kg_y$ Ya cEfRiEd|msikmsd|aj_bmsfEiEd fhg g_mLb b 8fEmsa|fhga|cEfyd}slaj_bmsmya cEf~Ei8{bfhg_b ck}ffag }aj_2gy"vJmsdfk}i8{bfs b4sJn p|kub 4 [ < _2ga|cEf`yd}slaj_bmsmy8msR}O_bf{bff8aga ck}a}d|fl c8_{28d f$myqnkuvg_bEms~Ed"fkfhE_bE$my_b(a mx`^`fEm^ck}OsfgjfR}8aj_2lg6ymd`vJmsd"`^`fg }O$a ck}anopq`p8rt u _Lnjopq`pst8r u v-ae_2ggmsfaj_bfhg~kgjfy~8{`_bms~Ed"y~Ea ~Ed f-d fhgj~8{bagea_bklmsd imsd}a f6ik}d aj_2l~8{2}d}O{b~EfhgYymsda cEfa|m{bfd}klfhg_b(a|ma cEfymsd ~8{2}> v c(~kg{bfa tEr Ed fiEd fhgjf8acEfeymsd ~8{2}a ck}ffad fhg~8{ag6yd _yzfh}sl|c$A}dj_2}8{bf_2g>d fi8{2}slfh$^`_ba c_bag>A}A{~Ef-}sllmsd_bEa!rt Ea ck}ffa_2gt v8i8_2l}O{{^`f}d|f_b8a fd fhgja fh_bl|{bm(gjfhgjf(a|fklfhgEa ck}a_2g8ymsd ~8{2}sg`^_a|cEmyd|ff"A}dj_2}8{bfhgvck}a"l}sgjfs_ba>_2g"Emsa>ck}da mxgjcEm^a ck}a>a cEfA}O{b~k}a_mi8{2}hEg"Emd|m{bfsv c8~kg_y_2g"l|{bm(gjfh^`f^d_a|fRnjoptEr u d}ffa cEfda|ck}nopq`pst8r u v_bk}O{{bs_y"}ffk}d fl|{bm(gjfhymsd ~8{2}sg^`f"^dj_ba f" _ ynjoptEr u "_bi8{_bfhgnoptJr u vH4-8HO8[YU(gY^zf`fEi8{2}O_bEfh_ba cEf_b8a mE8~klaj_bms!8^zf`_bsfgjf-}(a_UlgYa|m8fsd ffhgYmykf{_bfy(lmskg_28fdj_bE}O{{^`msdj{2Egmffy g_ f mekffh~k}O{{b{_ sf{bsklmsk_baj_bms8_bEms"8}ka cEfRl|cEfhl s_bEa cEfiEd mk}8_{ _bajmsfd`a cEf"d|fhgj~8{baj_bEiEd msk}8_{_baj_2gja dj_bE~Eaj_bms!va|cEf"iEd fs_bms~kg>gjfhlaj_bms!J^zf"8fkEfh^6ck}a`_bafh}kg ymdz}gf(a fklf>xa|mkf" g }a_Ugkfhe_bx}>^zmdj{2my!g_ f ~kg_E}a m{bfd}klf>sfhla msde!rt fiv ff_bsfnua mf"a cEf"(~Efdmy^`msdj{2Eg_b g~kl ca ck}ffanjoptkr uz vff}k!r(`^f8fkEf<_bklf^zf}ffd f"a} _bE}O{{^`msd{UEga mkfefhs~k}A{ {b{ _ sf{bsa cEf8fsd ffemy f{_fy_bff_f"X^`_ba cfhgjifhlaa|}krt _2g< n "!x"un ""u< n "uy# n "u %$ Ea c8_2g8fsd|ffmykf{_bfy_2g6Emsa`^zf{{b8fkEfhvcEfl}d fy~8{d fh}s8fd>R}Ock}OsfEmaj_2lfh}imsa f8aj_2}O{iEd ms8{bf ^`_ba c$a c8_2g"8fk8_baj_bms!v dj_2laj{bgjifh} s_bEk^`fgjcEms~8{2^6dj_ba &f n 'ud}a cEfda|ck}kg_bklfa cEfgjfa6my ^`msd{UEg~Ek8fdlmskg_U8fd|}aj_bmsl|{bfh}dj{bx8fikfkEg6ms-a cEf"smEl}E~8{2}d sv (6fklfsEa cEf"8~Ekfd`my^`msd{UEgz_}O{2gjm8fikfkEgms-a cEf>sm[l}ffE~8{U}ffd sv c8~kg8kmsa|c < n u}k) < n *!"eu8fikfkmRa cEf"l|cEm_2lf+-,/.1032546257892;:50=<#4>09?@257A4B2509CD4EF8GAH4:6I <68JLKM4fiNDE5EF892OND03GM8PCQ@ND2;ND:R?03E257-ND:SE5498:50=G257M82TU4V8PCWCD09TU4 XNDE5EF892OND03GM8PCGZYA<;KM4E5:[NDG]\AE503\0=E52OND03G&4_^A\AE54:5:OND03GA:`NDG]aUbU,cedfif6gihkjlnmRo#pqArslsgt@m`uwvfihxqAqylsgz{&|>}~xzs99/3s`9kkZ/Z/kZ/3ZZ/33zs=L5{|6|VR9Z9Z93zs/P=kR#yF 5Z LR=}/9/#{5zs]y1{5zs9ks#zsZ6ziZk|V@*F W56]kL9AkA[{59zsBkL{5zzM>&/&9*kZs9Z*z{6/33{>/kZ/Z/kZkz{zsVz3&z{szZk9siB9zsA9zs6Z>yF}@AsU=}kkZysB#)ez39Z)zs3/s3s}6V#ezw9/9A/9/Zy} k/Z6##zsks9z3sz s939yD55 Z ] 9z/ ] 6] =}Bzs39/*9zs3kZz{&#z3)zsZ%/%/zs =kA}5{9) /A9Asy69Z%#SzkeM%zskk/9zyk96A/ysi /3]6 9Ak##zsknVAkMyZk69z&9A9zye6As/Ms#3/s36A/ys}k]k3/kzs@[zSZZA>zs&PkZeAU&&*A)zV3kA}]zs/PZ9sA/zs9/)R 6] &*zs)y6AS3Fk3/AU}F%/3kA#)y/9ZsP/{5zs3Z9=M%yA*z{@ 6R 6] *z#3Fk3/A%{5zsk=9}%F%zs=kZ*9zkAy&#9%9k*9zkZz{#3Fk3/AkAZ]#k3/ ] 9z/ -M-Os=/3Z5ZU {{5zsM&kZk*y%/k)Zk9s> 9] }>zsz9Z9keZZZe9/Mx3zs/ P9Z/3LkA9/`9 ] R9_k;/k96kzsy/`z{9k9=9s*_Zs}~xzs]3/ksR ] =9/39Z93/s3kzsM3ZZk=9*zsZsZk9/y3zs/_9ZkA}~xzs69]Zy/kZVz{19&//ZA#&s]9/6y1kz#Aks&/A9ZsZk9/y3zs/_9ZkA}_=s3P3sB#3/A3ZsZk9/y&3zs/_9Z/3%9z/z/=kZ9zA99/3zs/_9Z/3s}kzzs3/A3ez#Aks/s)9zks3nzs/zs9kzsM_ZsR3/3Z6Znzs/&As/ZeA})~xzs&9zk/_/A#99kekPz@Ryznze9_k9z//{5zs9Mfi3zs/zs/Z/ZsZk9/yV3zs/ P9Z/3s}~zs/=/3sfi*/zk9_k9zz9/){5zz#3zs/zs/9k)Zk)9zs/=e9Z9/ ] #ZsZk9/y)3zs/_9ZkA k ] kzkA#zs#/]ezs5/9AZ9AZk//A/y/Z#ZZ9Z9`/ U ] 3zs/ P9Ze`{5zs kzsy_Z#6Zy/9zyk*93zs/zs/>9&Zk3Ak3/s3V3z/zs/-}]/3#3zs/3Zk9=9&zs/9*s/sA9kB//Zy9k6AkV3zsZ=#zk6ZsA#z{`eZ9AA}sZ{ ] ZsZe/y3zs/_9ZkA@yzs]3kA}~xzs]3/ks[Lyn9]Zs>9/ 6] Rze99A#/Z#ZZ))3/**3`{ zs#zs s#sZ3`9ss}`_9k>Zssx{ zs>e*/9P3k@ x9]sB9zV##zV3kA} BzM#ZsZys]ZZ>s;{9> )s9zV#M1zkUx9k6Zs/&;/_/3L9]ze9zs/]nzsVsz9z}&ysz/9)zskZ6k)3zs/_kZ)9 O3 / k=Z69/)9y}6~zskPZfiff ]k/ z{ Sik{ S6&s9A9A;zM#Z6nzs/z{ S} OA z{AZ/3]B9>]z{9>k/Zn9/;Zk{ k{]]#k{13k=`{5zs#eAZ/3>/zs/kA{59zsn3z&kZsZ{@966kzeA;zsy}3 Bk3/A/yzszs/_s6Z9&PkZzs9A#6AsB/Z>/zs/*z{ S}R {@ ]kzkA3ey9Z ]k{ ]P ]}/3s;{ zsk @ ;9AZ/3 6] >y fiyL/zs/kA { 9z /zs)//3z& ;9*/ k{y6yiU3kA} e/-A#SkzBzs@/ysR9z6#zs9Vnzs1R9zskZz{izs3k9Z/3;{5zs@/9P3kMMABz{&@ }R&ZzAZeB9]//y[{ zs z{zs>k3/kPz@}!#"%$!'&)(#*,+{k{ 6] /]-.% 6]]fi/103234)57698:53;3<=4)>?03234A@B03C3D%61E36GF23H3;I57JLKMONPRQTSVUWYXZP\[L]O^[L_RS\S`aO[=bdc!PeQ3Sf]gPeQ3SihjlknmojVjpZqBrejsutvjvq\txwzy{kntY|Ljfwz}~c,_oWPePRSf]z_lyu}~c7W%X^SVO]3S^T[XPRQ3SVNN]bWWP=,NPRQ3Sf_RWYXZS\_lyu}~^NSX]3NPSVUW%XoPS\ebNXZSPRQWYXXZSVPoWN]gWPeQg[vSf9_eSfB[L_R3XN]Na3_^SVO]W#PZW#N]!W#_XZP?]3NPRSPRQO[LP=c3SfSf]aOXlW]3PRQWYX^SVO]W#PZW#N]!c1PeQ3Sf_RSB[L_RSB[L]f[XZSXQ3Sf_RSPRQ3S^Sf_RSfSNLMOSVbW#SV\^NSX]3NPSVUWYXZPNLSfSf_c[XXoNSNLNa3_bY[LPRSf_SVUO[Lb#SXXZQ3NL\cW#]:B[L]XlWPeaO[LPZW#N]OXPRQ3S]3N]3SVUWYXZPRSf]OVSTNL[z^Sf_RSfSNLMOSVbW#SVif[L]MSga3]O^Sf_VXZPRNNI^fiW#]PRaW#PZW#SVb#GvvN_W]OXoP[L]OVScXZSfSzUO[Lb#STOA[L]O^fiPRQ3SzXZa3MOXZS`a3Sf]P^WYXRVaOXRXW#N],V STVNabY^,c[=b#PRSf_R]O[LPZW# SVb#c?QO[=STP[LSf]fiPRQ3Sz^Sf_RSfSTNL\MSVbW#SVB PeNMSgPeQ3SW#]PRSf_RL[=b^SVO]3S^MbW#A bW#W]f _ lyu}~[L]O^bW#A bW#XZa3 _ lyu}~c33_RNLWY^S^S[eQBNnPRQ3SfSVUWYXZP1QWYXNabY^QO[=S?MOSfSf][OSf_oSVPZb#_RS[XZN]O[LMb#SRQ3NLWYVSNXZPNLPRQ3S_eSXZab#PXSXZP[LPeSNab%^ANzPRQ3_RNa3Q:W#PRQSf_ezbW#PRPZb#SgeQO[L]3SWSQO[^P[LSf]:PRQWYX^SVO]W#PZW#N]!a3_^SVO]W#PZW#N]zXW#bWOSX?PRQ3S\SVU3ONXlWPoWN]gXbW#QPZb#W#]O[=bb#c3S?_RSfB[L_RPRQO[LPW#PB[=BXZSfSfa3]3_RS[XZN]O[LMb#SPRNP[LSbWWPVXWS?]3NLPeQ3S\^NB[=W#]XW#fS\N_QO[=S\[MONa3]O^N]PRQ3S\^NB[=W#]gXW#fSbS[n_Zb#c3W1S]3NL[L]O^ c3PRQ3Sf]W#PXZSfSfBXN_RS_RS[XoN]O[LMb#SPRNaOXoS_ _[LPeQ3Sf_PRQO[n]_[X!Na3_^Sf_eSfSNn)MOSVbW#SVRd]O^SfS^,c3[XXZQ3NL]W#]BZ[fRQaOXSfP[b%c=L3c,[L]TNL PRQ3SW#ON_RP[L]P3_RNSf_RPZW#SXPRQO[LPQ3Nnb%^vN_\PRQ3S^Sf_RSfSNLMOSVbW#SV^SVO]3S^M_Q3NLbY^vN__ cN_[bbeQ3NLWYVSXNL[L]O^ Q3SVN]3]3SVPZW#N]fiPRNB[UW#a3Sf]PR_RNPRQO[LPSB[LSW#]PeQW%X?O[nOSf_\Q3Nnb%^3XN]b#z[LPPRQ3SbWWP=cM3a3PMOSf[LaOXoSl[XNa3_\3_eNNLdX?XZQ3NLPRQ3SVN]Sf_RSf]OVSWYX_V[LWY^,cPRQ3S^Sf_RSfSNL,MOSVbW#SV_VZyu}~\WYXPZWYf[=bb#B[Sf_RNN3^[L33_RN=UW[LPZW#N]PRNB_ ly}~\cSfSf]N_N3^Sf_[LPeSVbbY[L_eS\[L]O^N3^Sf_[LPRSVb#XZB[=bbz7O,7vd3)GO!!vLO=!L%!fif:g3vzO=)Q3SW%^S[NLB[=UW#W#VW]3jw)xmRpVO:QO[Xb%[=S^fi[L]:W#N_RP[n]P_eNLb#SW#]B[n]OSVbY^3XfcW#]Oeb#aO^W#]3PRQ3SXZPRaO^TNL13_eNMO[LMWbW%XoPZWYNI^SVbYXN_W#]Sf_e_ZW#]3^Sf_RSfSX?NLMSVbWSVl[=]3SXcQO[n]3]3N]S[=Sf_c=Ld]PRQ3SBXW#b#SXZPXZSfPRPZW#]3OcSf[L]TW#SfSf]Pe_RN[X\[_RS[b vL[=b#a3S^a3]OVPoWN]zN]O]W#PRS3_RNMO[LMWbW#PZBXZO[VSXfv WYX1[?O]W#PRS?XZSfP1[L]O^ WYX1[?3_eNMO[LMWbWPZS[XZa3_RSN] cLPRQ3SSf]PR_RN%W?X^V3]^RPNfiff#b]Pn[#b]]3S?XZP[L]O^3[L_V^[n3bWYf[LPoWN]BNL,Sf]Pe_RNWYXPRQ3SNLbb#NLW#]3Oa33NXZSS]3NL:PRQ3SXZO[VS cM3a3PQO[=S\N]bO[L_RPZWY[=bW]vN_R[LPZW#N]z[LMONa3P cSVUI3_RSXeXZS^W]PRQ3SvN_RNnVN]OXZPR_[W]PXf?3N_SVU7[LbScSW#QPQO[SB[VN]OXoPR_[=W#]PXZaOeQ[X = !L "$ #3fi%bPeQ3Na3Q:PRQ3Sf_RSB[=gMSB[L]S[Xoa3_RS&X PRQO[LP[L_RSzVN]OXlW%XoPRSf]PW#PRQQO[LPST]3NL\cPRQ3SOmVtx(w 'ftu)sjTpZ*q )&+$,-t )./) jw)xmRpVOXZa3SXZPXPRQO[nPBS[^N3PPRQO[L0P 21QWYRQQO[XPRQ3Sb%[n_RSXZPSf]Pe_RN[LN]3[bbPRQ3STVN]OXWYXZPRSf]PONXRXW#MWbW#PZW#SXf43?XW#]3zPRQ3S[n33_RN3_ZWY[LPRS^SVO]W#PZW#N]OXfcW#PBf[L]:MSXZQ3NL]:PRQO[LPPRQ3Sf_eSWYX[gXoSf]OXZSW#]QWYeQzPRQWYX 1 W#]OVN_eON_[nPRSX?PRQ35eb#S[XZ7P 6[^3^WPoWN]O[=bW#]N_eB[LPZW#N]ARQO[L]3]3N8] S[=Sf_cLV3N_1SVUO[Lb#ScWSQO[=S?]3NVN]OXZPR_[=W#]PXN] cPRQ3Sffi] 1 Wbb!MOSPRQ3SS[XZa3_RS?PRQO[LP[XRXW#]OXS`aO[=b3_RNMO[nMWbW#PZzPRN[=bb1SVb#SfSf]PXNL9 & :Na3Qb#zXZOS[nW#]3O;c 1 [XRXlW]OX?3_RNMO[LMWbW#PZW#SX[X?S`aO[=bbA[XONXRXW#Mb#SnWSf]PRQ3S\VN]OXoPR_[=W#]PXf!-<>==!@?d,z2A%CBf:L!DBf=OOEBF W#S?B[=UW#a3Sf]PR_RNcPeQ3S_[n]O^N& vN_ZbY^3XSfPRQ3NI^WYX[=bYXZN\aOXZS^PeN\^SfPRSf_RW#]3S\^Sf_RSfSXNL!MSGbW#SVW S#c3_eNMO[LMWbWPoWSX _eSVb%[nPZW#SPRN[]3NLb#S^SMO[XoSD %XWY^S_eN9PeQW%XcLWYXPRQ3Sf_RS[L]VN]3]3SVPoWN]MOSfPZSfSf]PRQ3SPZNW%^S[IX H!VNa3_XZScLPRQ3Sf_eS1WYXPRQ3S_[nPRQ3Sf_Pe_ZW#WY[=b3NMOXZSf_RL[LPZW#N]PRQO[LP_[L]O^N&vN_ZbY^3XVN]OXWY^Sf_X[a3]WvN_R3_RNMO[LMWbW#PZB^WYXZPR_oWM3a3PoWN]TdNLSf_PRQ3SXZSfPNL,N_ZbY^3X1XR[nPZWYXW#]3}~Vc[L]O^W#PW%XJLKfiMONQPSRUTWVXZYU[E\]T]NU^DV;_a`9P[E[bT]NcZdIefeghSiUj$cOifikmlCn$kkmlUdOoUiSpfqj]rmsutvpxwykmryp{zUoUkyp{j]ifij$|]dGr}n$iS~&wydGk}lCn]wkmlUdOlSp{]lUdEwykCj/wwp{zSeddGiSkmrmjv/~]oUkp{ikmlSpxwwydEIkyp{j]icZdwlUjLcn$iUj]kmlUdGrbdGi/kprdIe~tvpfDdGrmdGi/k&n$iCtsoCl tSdGdGWdGrE}Ij]iUiUdEIkpjvizCdGkycZdGdGir7n$iCtSjvs&gcZj]ryextUwZn$iCt&klUdUryp{iCp{Se{dj$q;snbSpsoUsdGiSkmrmj]S~]ZlSpxwIjviUiUdEIkyp{j]ilUj$extUwCm$b]I&-/v}0-IE}-S&b($Of7b]$G-S]Z}SGG(S(7CmI$7]$7CG$(-EZikmlSpxwGn]wydcZdGniIj]iCwptSdGrUrmj]zCn$zSpfefp{k~tvpxwykmryp{zUoUkyp{j]iCw(n$iCtp{iCn$rmkypxIoSexn$rZkmlUdsnL/p{soUs&gdGiSkmrmj]S~&tvpwgkmryp{zUoUkyp{j]iDj$|]dGrkmlUd&wydGkjq9n$kmjvswGOkj]swn$rmdj$q9IjvoUr7wyd&|]dGr~tvpfDdGrmdGi/kqrj]sWj/wmwp{zSe{dficjvryextUwGqj]rp{iCwyk7n$iCId]kmlUdGrmd*n$rmdj]iSe{~CiSp{kmdIe{~sn$iS~j$qklUdGsp{iCtSdGCdGiCtSdGiSkj$qkmlUd*tSj]snbp{i4wp{Gd7oUrkmlUdGrms&j]rd]$kmlUdOsnbSp{soUs&gdGi/krmj]S~tvpxwykmryp{zUoUkyp{j]iCw9cdOIj]iCwptSdGrZcZpfefe(ky~SSpxGnbefe~0iUj]k}zWdOoUiSpfqj]rsdG|]dGrmkmlUdIe{dEwmwGsnbSp{soUsdGiSkmrmjv/~p{ikmlSpxwiUdGcuwyCn]IdGn$ikmdIefeOoCw&nfie{j]kfin$zCj]oUkkmlUdtSdG]rmdGdEwj$qzCdIefp{dIqtSdICiUdEt*zS~rIn$iCtSj]scZj]ryextUwGi*Cn$rmkypxIoSexn$rEkmlSpxwIj]iUiUdEIkyp{j]icZpfefe9nLee{j$coCwkmjfioCwyd&snL/p{gsoUsdGiSkmrmjv/~n]wn0kmjSj$e;qj]rIj]s&UoUkpiUtSdGvrmdGdEwj$qzWdIefpdIqmdzCdIefp{dG|]dklCn$kklUdrmdEwykrypxIkyp{j]ikmjoUiCn$rm~UrdEtvpGnkmdEwpxwOiUdEIdEwmwn$rm~fiqj]rOkmlUdIj]iUiUdEIkyp{j]i*cZdn$rmdn$zCjvoUkkmj0sn$h]d]iCtSdGdEt;n]we{j]iUn]wkmlUdh/iUj$cZe{dEtS]dzCn]wydsn$hvdEwoCwydj$qnzSp{iCn$rm~&UrdEtvpGnkmdwy~SszWj$ej]r}oUiCn$r~qoUiCIkpjviwy~SszCj$e7ScZdwyoCwyWdEIkklCn$kZkmlUdGrmdpxwOiUj0oCwydIqoSeIj]iUiUdEIkyp{j]izWdGkycdGdGiklUdkcZj&n$UUrmjSn]mlUdEwn$kOnbefewydGd]dEIkyp{j]i*qj]rwyj]s0dtvpxwmIoCwmwpjviDdGkzCdkmlUdwoUzSeniU]oCn$]dj$q}cOlUdGrmdjviSe~oUiCn$rm~UrmdEtvpxGn$kmdw~/szCjew0n$iCtIj]iCwyk7ni/kwy~SszCj$exwZn$UCdEnr}p{ifiqj]rmsoSexn]wGvp{iCnrmkypxIoSexn$rEUcZdn]wmwyoUs&dkmlCn$kZdE]oCnLep{ky~&zWdGkcZdGdGikmdGrsw}tSjSdEwiUj]kjUGIoUrp{i&qj]rmsoSexn]wp{i ZyZdEGnbefe(kmlCnk;pifi ]cZdnLee{j$cdEvoCnbefpky~&zWdGkycdGdGi0kmdGrmswG$zUoUkZtvpxwmnbefe{jLcdEvoCnbefpky~zCdGkycZdGdGiUrmj]Wj]rmkyp{j]idIQUrdEwmwp{j]iCwGf dGk}zCd*kmlUd*Ij]rmrmdEwCj]iCtvp{iUwyoUzSexn$iU]oCn]dj$qZikmlSpxwwyoUzCwydEIkyp{j]iDcZdwylUj$ckmlCn$kOklUddIUUrmdEwmwp{|]dCj$cZdGrj$q}n0h/iUj$cZe{dEtS]dzCn]wyd&p{i*kmlUdexn$iU]oCn$vd9fipxw;]oSp{kmdefp{sp{kmdEt}iqn]IkbLwyoCl&nGn$idEwmwdGi/kypxnbefe{~j]iSe{~Sexn]IdOIj]iCwykr7nbp{i/kIwDj]ikmlUdUrmj]Wj]rmkyp{j]iCwj$qkmlUdn$kj]swGqCcZdZkmlUdGikmlSp{iUhj$qCkmlUdEwdn]w;Ij]iCwykmrInbp{i/k7wj]ikmlUdIUrmj]zCn$zSpfefp{kyp{dEwj$qCkmlUdn$kmjvswmUvkmlUdGicZdlCnE|vdkmlUdp{iU]rmdEtvp{dGi/kIwiUdEIdEwmwmn$rm~fikmj&n$USe{~snbSp{soUsudGi/krmj]S~]i]dEIkyp{j]i*UcZdOwylUj$ckmlCn$k;kmlUdGrmdpxw}nwykmrj]iUIjviUiUdEIkyp{j]i&zCdGkycZdGdGi&kmlUdZsnL/p{soUs&gdGiSkmrmj]S~tvpxwykmrpzUoUkpjvi&qj]oUiCtkmlSpxwc9nb~niCtfikmlUdtSdG]rmdGdj$qzWdIefpdIqZ]dGiUdGr7n$kdEtfiz/~fir7n$iCtSj]s0gcjvryextUwZs&dGkmlUjUtjwydGdficOlCn$kIj]iCwkmr7nbp{iSk7wnfiqj]rmsoSexnSexn]IdEwj]iklUd&Urmj]zCn$zSpfefp{kyp{dEwj$qn$kmjvswGWp{kpxwoCwdIqoSeOkmjIj]iS|]dGrmkOklUdqj]rsoSexnkjn0IdGrmk7nbp{iGniUj]iSpxGnbeqj]rmswOnCrIwykwykmdGkjtSj$p{iUklSpwUcZdqj]rsnbefp{GdkmlUdtSdICiSp{kyp{j]ij$qn$kmjvs>$p{|]dGifip{iklUdOp{iSkmrmjUtSoCIkyp{j]iD dGk4L $EEbm ]Ij]iCwpwkjqkmlUdoUiCn$rm~UrmdEtvpxGn$kmdwy~SszWj$exwZp{ikmlUd|]jUGn$zUoSexn$rm~fivD{EDD v$y$LpxwIj]i$yoUiCIkyp{j]ijq}kmlUdqj]rsb ; 27WcOlUdGrmddEn]l pxwdIp{kmlUdGr jvr $p{iCIdkmlUd|Ln$rypxn$zSe{d& pxwZp{rmrmdIe{dG|$n$i/kkj&j]oUrIj]iCIdGrmiCwGWcZdky~/SpxGnbefe{~wyoUUUrmdEwwZp{kn$iCttSdEwmIryp{zCdn$inkmj]sun]wOnIj]i$yoUiCIkyp{j]i*j$qkmlUdqjvrmsu EEj]kmdklCn$k&kmlUdGrmdn$rd 9n$kj]swj$|]dGr&@n$iCt kmlCn$kfikmlU dG~n$rmdsoUkmoCnbefe{~dICeoCwp|vdn$iCtdIUlCn$oCwykyp{|]d]ZlUrmjvoU]lUj]oUkkmlSpxwCn$CdGrEcZdoCwyd@kmjtSdGiUj]kmdn$iCtfiff EbEbm ff kmjtSdGiUj]kmdkmlUdn$kmjvswjL|vdGrfiSefpxwykmdEtfip{i*wyj]s&dUUdEtj]rItSdGrE8 ( ZlUdGrmdnrmn$kmjvswZj$|]dGrOa $ b G !ff 4 " $ ff# $ $ff%&" $' ff () $9lUd v$ CmIQ-$ 7+*,*-ff . *,* / EbEm *,*-ff . *,* /cZpfefeSexnb~ nwp{]iSpf(Gn$i/krmj$e{dp{ij]oUr&kdEmlUiSpxGnbetSdG|]dIe{j]Us0dGi/kEkfikmoUrmiCw0j]oUk&klCn$k pxwn*rIn$kmlUdGr&cZdEn$hexn$iU]oCn$]0d *nqj]rmsoSexn21*}tSjSdEwZefp{kmkye{ds&j]rmdklCn$iIj]iCwkmr7nbp{iklUdUrmj]Wj]rmkyp{j]ijqkmlUdn$kmj]sfiwGi*j]klUdGrOcZj]r7tUwGSqj]r354'687:9;7=<?>@9;A#B;CD>EB<?>EFHGJI@KLINMO99;7=P;MRQSB;PUTE>EFVD77XWOB;7YFOZ57YZ8B;I[B;CO7:T\>@P;7]^CO7Y9;7B;C57?_`a<!7YFHBbSINFOPU7YcHMd>\Q-bSBG.e VOM5BB;CO7U7XWOB;9f>[TYIN<!gRQS7XWHbSBhGINVOP;TYMO9;7YP<?>EFHGJIEKB;CO7U7YP;P;7YFHBbi>\QbSZO7E>EPY4jOjfiklmnLoqpsrtouv5nLwJlmnyxzl{|}p~pmuoqDY'@y[HDfi''DfiO0 ,ddH E},a }@aE0 ,fiE00EY,0fiNEOEX,0'DEEOY'N0',Y,0'd#E00EY,0'D#DE0zRz[0EzN0 NtY .0N\}z0@ H@fi,^ @0'0@Y,0':dHUE ,OH'.YE;0N H!'D\iLHY,0 OYEOE0'EY,0'H D'N0',Y,0'5E00EY,0' O:00NEO@X,0#D!H5UfX! O0N0H ,H'N0 D[H0HODHD)D;EH)8NEOEYOD0EY'H@,'NY,)EJ; d[H?"EhD'D0Jh?'\O,0H$D )@J'.Y#Y@H)ED.D ,,zzd,H EE0h?@'\ [ h0@zDNEOEX,YfiD')EH h0N5"@0'0@Y,0$EH@z[ $DE0hE0'EY,0$@HEzH:fh#5hY'YN;EzD0DDY,0'EHX, LHDY,0'EE'D8i#N,t',0ON\t')a)N0,EOYJYEH''EO @?E'D[(?E.D'=0@O EY'O@h5NDLDN0@.(? h@z[[DOENY,0!,O},[ fi0;D;8;HNDdR!&h!'D, 0h5"D0HU@HEz:DE0@,-=\,?HEhDDED0H0'EDE0}EHE}DN;hD?H0HEaN N@,HD[E' 0h5UUh'=\0!,O}U=@,YO#YH EH'N( hJ,NDD';.0HDN,})h='N\DN0DY'NY,0'H?HEO0@$N0DY'N?h0DUEJ;DD[,'yf+ff:?tXiR=N?fX:Uf! f,R=EN0?HEJD'fDEDE0h[@HEz[D'h?X,Y,00#=YU0 J[Y=!Y0DE[E0t[Y 5#Y0DE[D')Y8N0'Y.@EHE0@00}='N?HD0N.d'0EE 5D'5;0 fffi8N0DY'N.H0HD,8N0 .5,$0Ea[Y 5?D'fi JE=. 0E8@'D#@OYh0Y8N'\'DEX, ,$, ,z5N0'XhY@H'N)EO0 ,EHH.R5DH#HDN) h8 hD0H!;J.i hy.DD';N0DHD. "!DE$#DDH %! &R\8D( '#.5hD['E5. )* \ &0 %,+hDHaDN.- %[.D O8Da hD0HLHD.0)/- haNDL';.05HDN.[JE D"DE h[EHEH$'DU5@H0EHz)@ }:'DHO'HD';0',)E'H'8EHzEz@'D#@,H0EfiDUE;0Eh/- hJ= hH5,$N'0H Yh5U,fiE,H0EfiD1-0'@,D?\HHz:,HEH ?,)E@H8 N'O EHEzDDEzHH0EH325 4hz0HH.57 6ODY,0D8Uh5,94; : 7 < =J5,'H@><@?A?0 NXO:,EE:h0HH.di7 6OD\D'?[N,;D?EOY ,U? h@E5^E'D"J'.\E;0. H^E}[,E0 ,8'DEEOhHDEOhJO0 ,ddHE)0[,Efi0 ,a HE;0z'D Y'HOYY,'B[00 ,aYOD0,'E h}'OH'Y;0#'D Y'O;0EhY'@0 C - 'Y ;0Eh0[.d,'0#dYhD ,D)0EHJE'DafiHDfi'D0O 0.XhEEN0DE'D Y'HO8[ hhJ' EX, ,'OHD'\,EH.5!Y ;0Eh)HDfi.5,$D08aD'0D)E0HEHOFE?E D!'\OJEED} OD'NEH':,[EE0'EY,0)Y.DEHH.R"X)'DEYhN hDO?YD#:E'U;0?D.- EHEh?DD5,HH-G ? h@$'0OYEO0'D Y'H.0[OYEO E0'EY,0'H0YhN0D[H0HOE'D8Y'@t$@OY , O08}y'Nz[z5,DH0H X,D,) ,'DEEOhHDE,EhD0'D00 0N'D ,0?E E}JID KYM LND8@,ODY,HOOY@O'D\iLHY,0t'OHD'\OK=P LND?.5,D'00EtD'Q'Yfi'YEHzdt@ ERTSfiUCVXWZY\[^]1_a`\b@cA[AV\dM]fehgWib@bj[AVkNlmAn^opm*qsr^mAtNuwvAxy{z|t~};ok$}7tNurZ}7zy*Nn\No@}ly*tNo@kPo@yAluJy\}t}mzyjZon\tNuwm*qsz\o@kt~}7z\No@yj;7ZmZo@k1yA\o\n\No@kNk}7AoCn^m*oj auZ}ksku\mTffkMtNuy{t tNu\ooly{zproaz\mly*z\mzZ}lyT\qmA;}7Aoau\omANo\5qmAa~}lu\oy{z\Axy*Ao@kauZ}k1}kNkx\off}kmAz\offm*qMtNu\offCtu\omANovxZ}toyj}7zNo@yAkmAzk1CuZpaoCNo@ktN~}lttNu\o>tNm yFx\zy*y{z\Axy*Aoff}7ztuZ}kny*n^o@aNAoo.Ao@lt~}7mAzOqmAaqx\NtNu\o}kNlxkNk}mzM;}7AozOy*zZ.qmAxZy"}7zOly*z\mAzZ}lyjfqmANaoly{z}7ppo@}y*tNo7Zo~}7AoFqNm}7t@\}7zy k~ZzZtyAlt~}ly*z\z\ojypk~otCm{qlmAzktNyj}7zZtkmAztNu\on^m|kNk}7rZ7on\NmAn^mANt~}7mAzkam{qy*tmAkM7@MM7Mff otF>r^o>}7zOly{z\mAzZ}lyj qmAN01wolmzk~tNNxltyqmAxZyp,N>1}7zOtu\offy{z\Axy*osm*qNo@yjlmZk~o@Fo\ka}oA@m*AoftNu\oaAm\ly*r\xZy*Np*\@A$AaAyAkPqm*;7m*k*Cu\oNoa*@@jNN^y*NoffqNo@k~u*y*~}y*rZ7o@kF}k~t~}7zltqmAtNu\o>tNm*7oy*zloFTy*}y{rZo@kff$@o>onZyloo@yAlumXllx\Nozlom*q1tNu\offqmANxZypj1rZ\1\o0NonZyAloo@yAlum\llx\NNozlom*qp|ff rZH| ~1r|y*zNonZyAloo@yAlum\llx\NNozlom*qo>onZyloo@yAlumXllx\Nozlom*q7 7 Fr|mAt~}loCtNuy*tafN>F uyAktamfft~|n^o@k1m*qMTy{~}y*rZ7o@k1tNu\oCz\oTy*}y{rZo@k tNuy*t1aofxkt1}7z|tNmXZxlo@y*z.tNu\o>tNm*7oy{zlo>Ty*~}y*rZ7o@ka$zmAZoatNmo;}7}7zy*tNoFtNu\oZon^ozZozlomAztu\oCy*tNto@Zoffm*qtNozlmAzk}ZofftNu\offqmANxZypfN>p\fqmAk~mpotm*7oy*zloo@ltNmApMM7@MMMAo@ltNmk}7z}7Aoz y.qmANxZy. m*Ao>tNu\oTy*}y{rZo@k*j@@NF7otA~ MMr^optNu\opk~ot>m*q{kNy*t}kq}7z\pimAN yT7A}q> @j@~ H5\@tNu\ozQ|j@j@NAsA~ Mi}HM7@MM7fZCu\oo}kffyTyTxy{t~}7mAzOk~xlutuy*tCpau\oA5\Q ff. fi>MZoz\mAto@ ;>i}kZoz\o@tmr^optNu\ol7m|k~x\NoFm*qaA fN>\qF>}kFz\mAt>}7zHly*z\mAzZ}lyjaqmANaoNo@k~n^o@lt~}7Ao7ACu\oNo>Zoz\of>y{z ;>ffstNm0ro}katu\o>qmANxZyF}7zOly*z\mAzZ}lyjfqmANo@vxZ}7Tyj7ozZt>tNm>mr\tyj}7z\o@0r|tNu\on\Nm\lo@Zx\Noy{n\no@y*~}7z\.}7ztNu\o>n\NmZm*q1m*qsau\omANo\!#"%$'&)(JP+* ot!,ro- *.-0/$|\a}7tNutu\oy*tNmA>au\oly*z\mzZ}lyT qmANxZy>7 8- #9k1o\no@ltNo@ >kmAZoNo@yAk}z21fy*pnZ7oi43\65mAzk}Zo;:- <9=- / >: @?o@vxZ}7Tyj7ozZt>tNm>}kBA|!C*<9Hf>>y*z >lmAzk~tyj}7zk1r^mAtNuw7|ff!D*#9^7C 7 >y*zOk~oo>tNuy{t7ff~7 }oA7Zj1}k>y{n\n\Nmj|}7;> KJ j@j@N7 FEFGH(}( oA7A Cy*z tNm>r^o\fwoffyjk~my*to^sy*tCpmZk~t>IT\1au\oNoqmANoAA8GIT\ CDffL>MON;P#Q RTS UBU0VXWSTVYVXWQ[Z \4].^_`baYcedfS8gXQ Vihkjml npoRqc.rsgt4gXVXg!cTdfSuUBU0v8wVXxyU4Q gYVXWzSeV{Se|XQVXWsQ!UBt4}~t4VcedfS8gXQ xsQ rsR Q8cTdv8wVXxyU4Q gpt4rh@NON;0c.VXQ6VXWzSeV)WsQ |XQ6%QiST|XQ6.t4Q %t4r[)'Seg0S~d+c.|X}ixOUS~t4rF<xsrsQ |0VXWQ6VX|STrgUSTVt4c.r@sQzrQ FQuSe|UBt4Q | %Q)scVXWOt4gVXWs|Xc.xs.Wsc.xsV6VXWQ6yzSeyzQ |)%t4VXWsc.xsVpd+xs|XVXWQq|pRqc.}Y}YQqrV Nzfii<<8i)#+<p;ss#<%#2;%#+T T2Y 2#Y eT;e .[T+<ss e;ff#e~T8Xff . + =fT[+T b+T Y)+{#+s##Xp8+ 2[Y uKYF.;qi=e>BOqe +B8qff+ sTz+ F %ff+TT[+s86T[T; Tu ;e8 TTeTs=eeTsY+Tfiff.;TT;Fq@TT;T[s.T 6{=.{%ssseT;ffTTYTT F#Te;6 6T;T;@T@TT {6 %T;<ssTe;FO% {6)Y{T 6TF [TYTTue ~+;T ~#zY' .%TTe = T@Y 8sF+ff{+TT8T ;eTe T;fi ff !#"$ %#f'& s(*),+,-Y;+s[se uz+T. T[YT.;/ ; ;s! Y+eTs Y+2TF0.+@21XeT ;+q.~43675 8:9<;>=57L? =A@CBEDGF IH:JK8 @ = 9<; ee@;=[Y u= qNM PO;eT; q,Q+RDGF IH:JK L 57 8 = 9<;9<; ; ;TT;FqFA@>%XTTGD2F SHTJUK 7L 5 8NV XW = 9<;{. ffu2D2F SH:JK L 57 8 = 9<; i!s.sTTu+ET+sTF>Y. ffYeF+ ++s86(Z[OzY 6TT@T[ .8 eTsY+TE]\zY[s +eFT.+@_^ +T ..T.+;q TTe b;. + #+ T= TT[+ff. + Ts.+.` #X;badc ff ffffSe KS KSf ffg[h %Ubi j Olkmj "Jfi F nKS oHpqK% r ff ,HsUfto",Hgh oK'u wvj Oxkxj K hz c c "$ { g U <{6 gtDGF IH:JK L 57 8 = <{9 ; }|~ "UJ9 ;c ffL4L4[5 D2F SH:JK L 57 8 = 9<; wOv<OI5Z ui. ffu +!b +~XT eF+T.+@++ T=Y eTsT;YT+s8uFTeY+TT;e.+@;uTT beT0.Y!sT +=T. ;.Tff + =],( ( @ .e; + +s Tez@X Y+++.2TFzXT +'pXz;q%'GA V W; 9<;;TTT;VP*(2C. . @TT8;+{eFT=T. eTs ;;;6e;e + +;uTffTshzff[;ff. ;(Wi? AOY;T + + T2{ @+ L? ~. + e .@+p? + T'F(;+>Z[wO2 Z[O8E q T[8 eq . +Te =TA.+.T X;. ~T=@ ;eYL L>L ?p+ T-uz8YTt^; +ff + @;s+ 8iTYX;. + X YT z+s8[! FT@ u8. ;.Tffu T;fifis[sY{ [. Tes ; ;+2Tfi6zz*,,zd*0,#worlds4||P(x)|| x00.250.50.751X,zn,ww,fi]P',S,$NzG200.10.20.30.40.50.60.70.80.91X,zt],S[w,fizzSz,GSz,fio,6,wzUS}SU*#wREz,,w0z*#NzPbz*,z,ww:Nzpzz*zRw,d>6,#S[Gnz,,w0<Uzz0RNnI2fi zt,<0owzw#Nzwzn%,>zzSq$,w0S!tSU*#wR ',>ffff6**>2z'S,RSz<$SzS[0 S,#tww*4%$ z'#<4,#N:zp>zz0RUw,2E0,,$]60, 2,<Sod*wwR>6$z>Nz<z!S2nz,ztzS$Td:fi"!"#%$&(')$"*"+,#%-"!"#/.0"1"23&4"&657!"8"*9$:<;=?>A@CB<DEF>HGJIKffL">M>ON9PQGSR">CRTKFU3BVWX>CP>CRWX>CR@O>YGSR[Z\K]U_^>AEKL">`>CRXKffaffGSPXbScdKffL">MRTD"^MeQ>CafG<IghGSa]ViW"EBSEffEFG9@UiB<Kff>AWjghU_KffLkPQG<U_RTKOElG<ImLXU_nSLo>CRXKffaffGSPXb)E]gB<^PpBVqVGSKffL">CalgGra]ViW"E0BSEsZtnSaffG<guEvViB<affnS>SwyxzLXU3E@OGSR@O>CRXKffaB<KFU{GrR|P"L">CR"GS^s>CR"GSR}c~gh>OVV{XR"G<gR/U_R)KffL">s>OV3W/GJImE]KB<KFU3EFK]Ui@CB,VP"LXb9EU3@CEc7IGraff^0EKffL">leBSEUiEIGSaGSD"al^0B,U_Roaff>AE]DXV_KsU{RoKffLXUiElE]>A@OK]U_GSR}wKYBSEffE]>CaffKOEKffLBJKMU_KUiElPGXEffEU_eXV_>KffG|@OGS^sP"D"Kff>WX>CnSa>C>AEG<Ie>OVU_>OIBS@C@OGSaWrU_R"nMKffG`aOB<RWXGS^ghGSaFV3W"EgLXUV_>U{nrR"GSa]U_R"nB,VV%e"D"KKL"GTE]>ghGSa]ViW"EgL"GTE]>>CRTKaffGSPXbfUiE?R">AB<a^0B,NXU_^MD"^wxhL">fR">ON"KuKL">CGSaff>C^>AEffE]>CRXK]UiB,VV_bYIGSaff^0B,VU_C>AEKffLXUiEhP"L">CR"GS^>CR"GSR}wMdX%,}X}<fSOOkCC%S~d T?CSqJ~i"if"},`Xh9Jidu~<ffOSCC%ffOi/" fu <d,h Y<O"C)CAO<d<iQi"Mj}T0C<0riM? <d< VU_^Y [ VU{^oE]D"PFVU{^jU_RXI T<<CqJ fM<C< f`>aff>C^0BJafffKffLB<K}KLXU3EUiE~rDXU_Kff>BuWrU@ODXV_KKL">CGSaff>C^w>hLBAr>WrUiEff@ODEffE]>AWgLTb`>C^^0B"w_SV_>CKEDEV_GTGrHB<K ArA] f gL"GXE]>>CRXKffaffGSPXbfUiE R">ABJa ^0B,NXU_^HD"^w=?D"K~KffL">hKffL">CGSaff>C^6K>OVqViEDE7KffGV_GTGSB<KKL">f^0B,NXU{^MD"^>CRXKffaffGrPTblPG<U_RXKEG<I ff c"gLXUi@L[g>`WX>OR">AWDEU_R"nYB E]GIB<aD"R"^GSK]U_BJKff>AWE]bXRTKOBS@OK]Ui@fP"affG"@O>AWXD"aff>MB<P"PXVU{>AWKffG f whKuE]>C>C^0Eaff>ABSEFGSRB<eXV_>`KffGs>ON9PQ>A@OKfKffLB<K 9 f E]L"GSDXViWKff>OVVDE C<AS9 B<eGSD"K^G"WX>OViEG<I f wm=?D"K^0B<rU_R"nYKffLXUiEf@OGSR"R">A@OK]U_GSRP"aff>A@UiE]>Sc}BJRWYU_R[PB<affKFU3@ODXViB<aE]L"G<ghU_R"n[L"G<gKffL">l^0B,NXU_^HD"^s>CRTKffaGSPTb[PQG<U_RTKEG<I 9 ff a>OV3BJKff>0KG^G"WX>OViE`GJI f ghU_KffL/R">AB<aff^0B,NXU_^MD"^>CRTKffaGSPTbScQUiEWrUqY@ODXV{K,wG<gh>CS>CaAcQgh>WX>OI>CaHBVqVWX>CKOB,UV3EvG<IKL">HP"aGTG<IhG<IKLB<Kua>AE]DXV_K`KffGKffL">`B<P"PQ>CRWrUNwR[nS>CR">CaB,Vc}xhL">CGSa>C^"w_AM^lBAbEF>C>C^KffG0eQ>G<I~VU_^MU_Kff>AWDE]>OIDXV_R">AEffEuXR"G<ghU_R"nYKffLB<Kgh>`GSRXV_bLB,S>[KffG/V_GTGrpB<KYghGSa]ViW"ElR">AB<aKffL">[^0B,NXU_^HD"^s>CRTKffaGSPTbPG<U_RXKWXGX>AEYR"GSKEFD"eE]KB<RXK]UiB,VV_bpa>AWXD@O>KffL">MRXD"^HeQ>Ca`G<IhghGSa]ViW"Egh>HR">C>AWKffGY@OGSREUiWX>CaAw RWX>C>AWdcKffL">gL"G<V_>PQG<U_RTKG<IKffL">0@OGrR@O>CRTKffaOB<K]U_GSRP"L">CR"GS^>CR"GrRUiEKffLB<KfB,V_^GTE]KB,VVghGSa]ViW"ELB,S>fLXU_nSL[>CRTKffaGSPTbSw >CS>CaffKffL">OV_>AEffEc}BSEhKffL">aff>AE]KG<IKffLXUiEPB<PQ>CamE]L"G<guEcTKLXU3Ezaff>AE]DXV_Km@CBJRYe>HrDXU_Kff>`DEF>OIDXVgL">CR@OGS^MeXU_R">AWYghU_KffLKffL">IG<VV{G<ghU_R"nlK]gGMaff>AEFDXV{KOECwxhL">?aE]K~GJI%KffL">AE]>EffB,b"E}KLB<K7UIB,VVKffL">ghGSaFV3W"E7R">ABJa~KffL">h^0B,NXU{^MD"^>CRXKffaffGrPTbfPQG<U_RTKELB,S>Bf@O>CaffKB,U_RP"affGSPQ>CaffK]bSc"KffL">CRYgh>`E]L"GSDXViWLBAr>`WX>CnSaff>C>`GJIe>OVU_>OI`fKffLB<KhKffLXUiEP"affGSP>CaKbsUiEhKffaffD">SwqV U_^ ~ f9 fH~ zVqU_f^fdAdi3,pS}<uriOkCCdSd T~CSqJ~i"iuO"}A~fTh"<%~<ffOSCC7C%ffOQH f q,7) f<9C,~J%<9M <%,<<O<TS?TrqJ~<`l<CqM O"CYTS 0s}Tf`~d73GSafKL">TR"G<ghV_>AWXnS>leBSE]> " U_fiR ff7NB<^PXV_>Y"w_ "cQU_KvU3E>ABSEFbKGE]>C>0KLB<KfKffL">^0B,NXU_^MD"^>CRTKaffGSPXbPG<U_RXKhUiE w UqNE]GS^s>mB<aeXU{KaB<affbw ~V_>AB<a]V_bScQKffL">Caff>UiEE]GS^>GSPQ>CRE]>CK B<affGSD"RWKffLXUiEPG<U_RXKHE]D@L[KffLB<KfKffL">BSEffEF>CaffK]U_GSR _ _CL"G<ViW"EIGSa>CS>CaffblghGSa]ViWsU_R whxhL">Caff>OIGSaff>ScXgh>`@CB<R@OGSR@V_DWX>MKffLB<KSa 3 _ HCO9 ~!" Ehgh>`E]L"G<goU_R =hBS@C@LTDEh>CKfB,Vw_c}$ #%#'& crIGSaff^MDXViBSE gzU{KLWX>CnSaff>C>G<I~e>OVU_>OIv@CB<RY>AEffE]>CRXK]UiB,VV_be>sKffaff>AB<Kff>AW)(]DE]KVU_S>0GrKffL">CafTR"G<ghV_>AWXnS>U{R f wMxhLB<KUiECcKffL">0WX>CnSa>C>AEG<IeQ>OVqU_>OIfaff>OViB<K]U_S>KG fB<RW f ghUVqVhe>MUiWX>CRXK]Ui@CB,V >CS>CRUqI f BJRW f B<aff>MR"GSKV_GSnJU3@CBVqV_b>ArDXU_B,V_>CRXK +w *GSaff>IGSaff^0B,VV_bd,.-fi/!0214357698;:<=%5%0>?6@BAC1D<<$5%0EGFIH4JLK$HNMPO?QSR4TVUXWY;Z%[\[^]4_N`ba\cdZ$egfih;jklk'mnporqds9t'vw u Wxyz{bnp|}j~'Lei\ei`_eifii4%dq\'G~'N!q\'G2~+!ei s9t v u W!yz{n9| e w s9t v u W!yz{xlnw!!K$JIJNUD%t[%+4eia\c^a\a``\h;;at^a\7aZ'cc]at^t^lPWY;Z%[\[]_N`Ga\cZ$egfih jk%k'mDnp]a\t^a%fYZl`[t^%NZ4e`c[bt^aZ%`%4i`]'`c^]NZch4S%t!Z'Z'N?s9t uv W!yz{pn9|s9t v u W!yxlns9t v u Wx2yz{pnst v u W!yxlnst v u W^xyz{GnYZ%``_+ci%?h s9t'v u Wx2yz{dnGc^a\N`c^j!]a\acZ'%aXec`\h`fic^]aNt`cc^a\t^c^a\N`c^s9t'v u W!yz{fidxlnfC!c^]a%c^]a\t;]NZ'NIhs9t'v u W^xyz{Gn9]NZ%`eiGicf9Ya[\Z'_N`apst'v u W!y)x%nr`Nl_N4aIhN;ap[%N[ei_N4aGc^]NZ'c!c]ap`a[%Nt24_N[cZ.er`+c^a\N`c^+fV;]at^a`_4ecS'ee'`f`9;a`]NZ$ee`a\a;ic^]aa2c;`a[ci%?h4c^]a[%G4iNZ'ci%'9V%t'eerZt^+fij\mpZ'N);]a\%t^a\fij`l_4ic^ap7'a\t_4egf9?9'g9lNNIG!SgDe c]%_%]pc]aV[%N[a\ctZ'ci%b]a\%+a\%r`ica\t^a`ciNh$ic`?Z4e[\Z'cldcZ%[c^_NZ$eeip[%+_c4a\%t^a\a`'V7aeia;Z$%c!NaG%4ll_N`\f 'iN[ad;ab'c^]NZcZ$ei+`cZ$ee;%te`;eeV]NZla]4l]a\4c^t^%4%hNZlta[cZ'4e[\Z'ci%'C]a\%t^a\}fijG44a`!%c`_N`cZ'crZ.eeit^a4_N[apc^]ab_G7a\t';%te` ;a+G_N`c[%N`4a\tf+9a\c$hZ%` ;a+`]'ic^]4`p`a[cl?hc]a[lN[a\c^tZ'ci%c^]a\lt^a\[\Z'S%t^c^]apNZ%`r` CZ)tZ%[c[\Z$e9c^a[]4rl_aGS%t[%+_ci4a\%t^a\a` VNaeia!iZ'[\Zl`a`\fNa\%a[ci%mNfrjt^a`a\4cic^]ai4c^_4ici%N`_N4a\telic^]4` c^a[]4%_a%fg%a[ci%mNf;aG_4e%c^]a`api4c^_4ici%N`bt^a`a\4cit^a`_4ec` S%tpZt^a`c^tr[ca[eZ%`^` '%td_4eZ%` c]`al_a\tia`!]4[^]Z't^a%_NZcNa\t St^a\aS%t^G_4eZ%`+'%a\tZ_NZ't^eZ'%_NZ'laCc]Z`i'eia[%N`cZc`4GN'egf+ `4ca+'!c^]4`t^a`c^t[ci%?hZ'4';c^]aG`^`_a`GZ'tr`Xic^]a)%a\a\tZ$e[\Z%`a[\Z'7a`a\a\]a\t^a%f %t^a\'%a\t$hNZ%`;;ad`].%a[ci%mNfhc^]4` t^a`ct[c^a`_4eZ'l_NZ'%a` t[]a\%_%]c^+Z$eei'_N`;c^Ga\GNaXc;baee S4.t^%7`ici%NZ$e9Z't^Z%[]a`c^]NZ'c;Z'la_N`a'Z.iG_a\4c^t^%4` 7fi ff^ ~ ffW e`^`%?h!jk l4nCZNc^]aZ$4G_Sa\4c^t^la2c^a\N`le`^`%'g`a\Z'cr[\`)W ?a\!saZ'te h;jklk%4n4_ac^"'e` #\r4c$$h %t^t`\hsaZ'te!Wjklk%4nW`a\aZ$e`W 'e` #\G4c%h %tt`\&h saZ'tegh9jk%k%2n^nf gfi%a[clmNfmNh7;a+[%N`4a\tp!]a\c]a\tc^]aGt^a`_4eic` S%tc^]abt^a`c^t[c^aeZ'%_NZ'%a+[\Z'NaGac^a\N4aIf!aG`]'c^]NZ'c c^]a\[\Z'?h7_c`a\%a\tZ$e9l( '[_4ecGZ'N`_ceiar``_a`Zt`a%fQSR EFLH"*NH,+IH2K.-0/2143K.-53\Hfi*06ec]%_%]c^]atZN4%S;%te`+a\c^])`!4aNa4[%_4ci+;%te`\h;a[\Z'`%)a\ci+a`NN+%talit^a[cG!Z$2`c^[\Z$e[_4eZ'c^ac^]a4a\lt^a\a`';Naeiaicb%iae`\fgfiWYZl[\[^]4_N`a\cpZ$egfihjk%k'mDn;at^a`a\4cZ+4_GNa\t'V`_N[]ca[^]4l_a`\hI+`c 'V!]4[]Z'4ei%4eii%a\t^`7a[rZ.e[\Z%`a`\f!a'?c^]a`)4ea`cCZN++4`cic^_4ici%a `c^]a!S'eei';idla\t`i%+'?!]NZc9]4ei`%]a\t`;]NZ$%a!ca\t^+aq 7 9W 8;a[^]a\4NZ%[]?h!jk'm4k4nf%_N`ac^]NZ'cpZ$ee!;a+'Z'N%_c+Z'Nlil4_NZ$;e :dr``%)apZ%`^`a\t^cl=< 9W :$?n >i%c^]a\t;;%t`hNz{]NZ%`;c^]a S%t^@< 9W :$n7z{ AShZ'Nc^]ap[%N`cZ'c :44a`%cZNaZ't iz{ f e``_7`abc^]NZ'cz{+h7c^%%a\c^]a\t;ic^]ZGNZt^c[_4eZ'tc^'eia\tZ'N[a hi+4eia`c^]NZ'c B\9W Cny < 9W CIn BDG`i`%+ai4c^a\t^'Z$e EGF H%Igf;cp`a\a\`t^aZ%`lNZ'4eiadcZ't%_apc^]NZJc :r``]%_4eNaGc^taZ'c^aZl`LZ Kc44[\Z$Ne M+aeia\+a\4cd`^Zc`liO< 9W CInhDNa[\Z'_N`aG4Z%``_+ci%z{ [%4cZ$iN`)PRQfiS$T5U5V&WYX[Z\W5]5^.V&_`T5U5Vba=T5c5deX$f5XhgiU5j5]kWYlmnofiprqtsu=vwnqoLxy5ztz{Rxwno5zOqw|5{4s~}2nNx{|5{4s{prqts{}2{unz|fiw|5q0{wqOy0x{w|5{xw?vwnNxwne4xtns{Rwvo0qo0~y0fi{w|0vw2s99.4$LG% 2|finNxnNxno0fi{4{ROw|5{`4vx{5vxw~|5{pqq}no5zw|5{4qts{4ux|5q}x45N7fi&.0%fi 9 v4~| y0x{4wve&. .~.&R!~4Jfi$4h.77= ,47?r&?&R?% ? ` ?L$%kNb`&?04&9RJG%t9!.G4.N4!N|finexs{RxyfiwRYnoLqufino0vwnqo}2nw|w|5{!s~{Rxyfiw?xqpw|5{5s{4tnqty0xx{Rwnqo,5sqnNfi{Rxy0x}2nw|"v{4s0q}2{4spyfiwq q2qy5z|fix,{Rvno5z0,}2{5sq,q x{wqy0x{w|5{`prq(q}2no5zOxws?vw~{4z&2|5{0vtxnNqo0{4ofiws?vwnqtoL5|5{4o5qu{4o5qox~vR5x`w|0vwuq xw}2qse5xvs~{{4sLxnunNvsnov{4sw?v.no\x{4o0x{Oxx|5q}ono$qsq(Nvs~=54,}{`4vo=y0x{w|finNxwq0o0xqu{`vxx{4swnqto0x$w~|0vw$vs{v.uq xw2{4sw?v.nofikwsy5{ n{eR}2nw|fi{4zs~{4{qp00{n{p {4{4on(p7w|5{4vs{$o5qwqznN4v.nufin{Rfi|5{4qs{4uh5Rw|5{4ow{NxOy0xw|0vw=}2{L4vows{Rvww|5{Rx{"o5{4}vx~x{4swnqo0xOvxnpw|5{4vs{nopvw=fio5q}o}2nw|{4sw?vnofiwi|5{4ow|5{Rx{o5{4}vxx{4swnqto0xxw?vw{xwvwnNxwnN4v.fio5q}2{Rfiz{R5tw|5{44vovxwno0s{Rvx{qy5sqt50qsw~y5ofinwn{Rxwq=v5fitns{Rwnofipr{4s~{4o0{2|5{;prq(q}2no5z{Yvufi{;n(y0xw~s?vw{Rxw|finNx2nNfi{Rv5eififf%qo0xnNfi{4svO{4sLxnufi{Ofio5q}2{Rfiz{=0vtx{=q{4s!vq54v5yfiNvsqofiw?v.nofino5z"w|5{xno5z{y5o0vs5s{RtnN4vw{2|5{4s{!vs{w}2qvwquxw|finNxe??vo0q{4s=fi}2nw|nex`~{Rvszn{4o#7%$&$ $'hvo0 "! $2|5{!xqy5wnqox0v{qp$xwsv.nz| wprqs~}vs?qu5y5wvwnqo!x|5q}x7w~|0vwRprqs*0qnofiw 1) 3 24)(,+ 4w~|finex|0vxvy5ofin.-ty5{2u=v/finuy5u0{4o ws~q5 q}!0qo0x9nefi{4sw~|5{-ty5{4s 9. 65qsv(798Y{4w;: 7 ,{w~|5{pqsuyfiNv ? \ )<47)=)72fi|Nn=xxvwNnx0R{x~w5|{q0nwnqoqp$qsq(Nvs~540xqLnwprq(q}xw|0vw: 7 9$ ?> xno5z |5{4qs{4u5eR5fi}2{fio5q} w|0vw2pqsnuA@ 4nu nofip nuxy5Bnu @CCDnu @C9.4$9.(CD: 7y5w2o5q}}2{4vo=y0x{!tns~{Rw2nofipr{4s{4o0{ 5qw{`w|0vw|5{4s{5qy5s!fio5q}2{Rfiz{Rv0qy5w nNx2vy5qty0x4n{% ?549. 5{qo0~y0fi{w|0vwRtnpw|5{4s{nNxvo nunw!vwv(,w|5{4oo5{R{Rx~xvsnF q0fipqsv.G7 89.4: 7 L)47 4))E79.$)H47 4))I7F no0{w|finNxinNxwsy5{2pqs2v.J74fiw|5{qtofi0q xx9nfi{v.y5{pqss 9.(nNx K) }|finN~|nNxw|5{n{e Jt w|5{u=v/finuy5uL0 {4ofiwsqt 0qnofiwR5qw{w|0vwnwnNxv.NxqO~{Rvs}|0vw5|0v5,{4o0xvx w{4o05xw~q $s .nNxvy5{qp $MONfiPQSRUT?VXWZY[?\^]OVOQ?_W`bacRd\^\/VOQecfUg.hBijlknmLoUpqi<rUismLtvulhxwzy{knw&i^hlw&f?im|k/gqu}hxwzisolh~tn?tOu?iolthzhgUpqihxwzyknwzisOtOyHtvmLo??wxgqu?rUisvyzisi^htnlXipgqizBgqy{hxwwzf?im|k/jUgqm?misuwzy&tOo9oXtngquwhtnlw&f?icholkOiS399 kny&i<tOmLo??w&i^rkvhHkc?ulwgtvu99 kOhzhx?mgu?wzf?ip3gmgwijghxwh g3<u?tvw^~wzf?itnL ;ef?isuw&f?i^hxiLknyzi}lhxi^rwzttOmLo??w&i|Zyp3gqmhx?oknulrp3gqmIgquU9tnKKyz 9; kny&i;tOmLo??wzi^rAgulhwzi^kOr Bgqulk/p3pqO~i;tOmLo??wzi9w&f?i9p3gqm}gqwtnwzfUgho?yztOlknUg3p3gqwkvhL OtUi^hwztLsisyztXuUtOyzwz?ulknw&ipOnw&fUg.hZhwzy{knwzisv9flkOhHk9hisyxgqtOlholtOw&isuwxgk/p?o?yztOUpqismKi<&pqi^knyxpqLsknu?u?tOwtvmLo??wziZy x 9; hxisolkny{knw&iptOyi^kvzfLtJw&f?igquUluUgqwzipq|mknu}wztpisyknulivi^wztOy{h knulrwzf?isuLw{knviwzf?ip3gqm}gqw;kOh vti^hwzt|?im}gqOfw f?tOoXi;wzt|tvmLo??wzi;w&fUg.h o?y&tOlknUg3p3gwxkOhknuijSoUp3g&gqw9?ulwxgqtOutn~knulrwzf?isutOmLo??wzi}wzf?ip3gmgw/?tvygqulhxw{kuliO?gqujlknmLoUpqi}l,LKy 99 kOhtO?ulrwztli?, ^ Zknulrhtgqwg.hi^kOhw&thxisiflknwflkno?olisulh}kOh ^dA?w}wzf?isyzig.hu?tyzi^kOhtOuwztlip3gqisOiLw&flknwKy 99 ghsXgquvisu?isy{k/p%kui^kOhg3pqzflky{kOwzisyxgq^knUpqi?ulwxgqtOutn; gwgh u?tOw^wzf?isuEtvmLo??wxgqu?wzf?i}p3gqm}gqw|kOh OtUi^h9w&tsknuXiArvg3AUpqwLtvy gmolthzhgUpqiOiLtOUprp3gqOiwztlulrkk^}wzt}k/OtngrLw&fUg.hcijSoUp3g&gqwp3gqm}gqwxgqu?|o?y&tSi^hzhk/pqwztOOiswzf?isy/Hwwz?yzulhtO?wwzflknwwzfUghghZgqulrUisi^roltUhzhgqUpqigquhxtOmLi&gqy{?m|hxwknuli^hsef?im|kguyzi^OUgqyzismisuwKg.hKwzflknwZwzf?im|k/jUgm}?mLisuUwzyztvooltngquUw{htn 399 tOuvisyzOiw&twzf?im|kjgqm}?mLisuUwzyztOoU;oXtngquw{h399 dtOyB?wz?y&i<y&iisyzisuliOnu?tvwxgi9wzflknw 39 gh;w&f?iA&pqthx?y&i|tnwzf?ihxtnpq?wxgqtOuIhxolkOitnwzf?i|tvulhxwzy{k/gquUw{h9tv?w{k/gqu?i^ryztOmyzisoUpkO&gqu?k/p3pZtSs?yzy&isuli^h tn<kulrk/p3pZtSs?yzy&isuli^h tn9 umknuhxl&fskOhi^hsisknutOmo??wziKy 9; rvgqyzi^wxpqgquw&isyzm|htnw&f?imk/jgqm}?mLisuUwzyztOoUoltguUw{htn 9gqwzf?tO?ww{knvgu?pgqm}gqw{h9kwckpp%h wzf?i}tnp3pqtngqu?ijlknmLoUpqi|hf?th ~wzfUghwUoli}tntOuUwxgquUUgwxrUtUi^h9u?tOw9f?tnprgquOisu?isy{k/p%wzf?im|k/jUgqm}?mLisuw&yztOoU|oXtngquw{hctn< 39 rUtLu?tOwu?i^i^hzhzknyxg3pqtvuOisy&Oiw&tLwzf?thxi9t< 39~.q~9tvulhgrUisywzf?i;Uu?tnpqi^rUOi9lkOhxi9. q ?,q q ? ~q . |?%wgh9i^kOhxwzthxisiLw&flknw} 39 ghxlhxw} ??{? z ;ef?iLoXtngquw ?l?, gh;rvghzkppqtni^rUwzf?ihxi^tOulrtOunx?ulw^tn;tOulhgrUisy 39 tOyUwzf?isu 9 gqulrUisi^rIrUtUi^hu?tOwtvuw{kguoltngquUw{hf?isyzi9 ghu?i^kny ?lUwzf?im|k/jUgm}?mLisuUwzyztvooltngquUwtGwzfUgh hxolkOig.hi^kOhg3pqhxisisuw&tli?, tnisOisy^Xg3 Lwzf?isuwzf?isy&iLg3p3pcXiLoltguUw{hgu 39 f?isyziL g.hknyztv?ulrE?lBtOygqulhxw{kuliOZwzf?thxif?isyzi?} ?} ngquliwzf?i^hxioltguUw{hflk/OikfUgqOf?isy}isuwzy&tOowzflknuwzf?i|oXtngquwh9gquIwzf?ivg.&gquUgqwxItn?,?wzf?itOyzmLisyg3pp9rUtOm}gqulknwziOefUlhswzf?ihxisw;tm|k/jUgqm?misuwzy&tOooXtngquw{htn9 399 rUtUi^hu?tOwtvuOisy&Oi|w&tkhgu?piip3pq%rUilu?i^recfUg.h u?tvultOuvisyzOisulihxisw^flknwcgw9tOuUOisyzOi^hwzt g<kuUwzfUgqu? rUisolisulr?h9tvuf?tn vti^hwzt ?flkOhtvulhxi^O?isuli^htOyrUisvyzisi^htGXipgqiz%wghu?tOwflkny{rwzt|hf?tZy 99 skuli;igqwzf?isy?,c tOy? /SrUisoXisulrvgqu?tOu|w&f?i9o?yzi^&ghxi9yzipknwxgqtOulhxfUgqoliswxisisu n?knulr%wtnp3pqtnhwzflknwKy 99 rUtUi^hu?tOwijUghxw^9 tOyZtnih&k^wzflknwk9rUisOy&isitn~lip3gqiKKy 99 g.hKu?tOwzUsg3Jw&f?ilisflk/vgtvytnZyrUisolisulr?htOu Oti^hwzt ?up3gqmIgquUHZy x knulrpgqmhx?oKy z kOh vti^hwzt }tOwzf?isy tOyxpr?hs~u?tOu?y&tO?lhxwzu?i^hzh rUi^hzyxgqXi^hhgw&lknwxgqtOulh f?isuZy x 9; rUti^h u?tvwijUghxw9li^sknlhitnhxisulhgqwxgqvgwxwzt}wzf?i9ijlkOw&f?tngi;tHwztnpqisy{knuli^h Zi;hxflk/p3pGhisiku?m}XisytntOwzf?isyijlknmLoUpqi^htnu?tOu?yztv?lhxwzu?i^hzhgqup.kwzisyhi^wxgqtOulhswZm}gqOfUw<hxisismbwzflknwZwzf?iu?tvwxgqtOuLtnyztO?lhwzu?i^hzhBghknuLknyzwxg3%kOwtn~tO?y<ko?o?yztkO&fu|olkny&wxgUp.ky^gqwGhisism|hw&trUisoXisulr;tvu;wzf?iK kOwwzflknwBtO?ypknu?vlknOiflkvhwzf?iijSo?y&i^hzhgqOioltnisywzt9hzk/w&flknwwzf?iwtwztnpqisy{kuli^hyziso?yzi^hxisuUwGk9rvffgisyzisuUwGrUisvyzisitnkno?o?yzt/jUgmknwxgqtOuOhgqmLoUpq}9lhgu?;rvffgisyzisuUwhx?lhzyxgqo?w{hfifi"!#$%&')(*+,- ./1032546870:9:;=<?>1@A3ACBED FHGJI=A KL7NMO<PD <#D GGQ1RJDTS1@U>1RVQ1A8GQWA6XA8<Y>Z;[<\HD GGQWRBJ;=F#D >1A<RT>cF#D dTA5>1@A6XAA]_^EDIff;[>X`H>W@ED >baJRJA6a_;&6Z>X;=<ESC>X;=RT<E68ef5A3D QWAbghRT^<EaU>1RV\TA8>i>W@A3D <E6Xf5A8Qbjkmln;=<H>1@A:ACBED FHGJI=AoD gER 4TATeJ6p;=<ESCA>1@A8<q=qrs/utvKLq=q w:x09yjkzyQ1AD{I|I=`"f5RT^JI}a"gEAP>1@A?<A8\YD >X;=RT<"R ~q&qrs/utvKLq=q w032?jkzE7APf5RT^JI}aDQ1\T^A>1@ED >>1@AD <E6Xf5A8Qjkmls;}6a_;ffA8Q1A8<Y>;=<E6X>CD <ESCA6<RT>oDT6:QWADT6XRT<ED gJI=ADT65;=>oF;=\T@J>oD >:EQC6X>6ZA8A8Fy7oT^GGhRY6XAHRT<AsR ~>1@As>uf5RR ~bjkzU;=<>1@AGQ1A84T;=RT^E6ACBED FHGJI=A@EDTagEA8A8<"6pIff;[\_@Y>XI=`a_;ffA8Q1A8<Y>h~RTQnACBD FUGJI[ATe6X^GGhRY6XA?fbAU@EDTa^E6ZAajklTTUQLD >1@A8Qs>1@ED <jkzP;=<>W@AEQL6X>sR ~:>1@A8Fy7PMp<>1@J;}6S8DT6XATe*>1@A#6XASCR_<EaSCRT< X^<ESC>*;}6iA616XA8<J>X;}DIffI=`H4{D_SC^RT^E68eTD <EaHS8D<ghAc;[\_<RTQ1Aav75@A5F#D{BY;=F^FHA8<J>1Q1RTGJ`VGhR ;=<Y>*;=<P{ffo;}6*<R fjkmlTTeD <Easf5A;=<EaJA8AaHaJA8QZ;[4_A3D3aJA8\_Q1A8A5R ~ghACI|;=AC~R ~jkmlTTb;[<?rs/uKL7@Y^E68eTDQ1gJ;=>1QLD QX;ffI=`6ZF#DIffISW@ED <\TA6>1R:>1@Ac<Y^FgEA8QC6v;=<>1@AR_QX;=\ ;=<EDI,dJ<R f5I=AaJ\TAgED_6XAbS8D <S8D ^E6XAcI}D Q1\TASW@ED <\TA6;=<RT^QaJA8\_Q1A8A6R ~ghACIff;[AC~175^>5>1@A6ZA3<J^FgEA8QL6:DQ1AD{I[FURY6X>bD{I[f:D`6i>1@AoQ1A6Z^JI[>5R ~iD GGQ1RBY;=F#D>1A:RTgE6XA8QW4{D >X;=RT<E6>1@J;}6;}6cQ1ACEASC>1Aa?gJ`HRT^Q:aJASW;}6p;=RT<P>1R^E6XAoD GGQ1RBY;=F#D>1A3A]_^EDIff;=>X`HQLD >1@A8Q5>1@ED<#A]_^EDIff;=>u`Uf:@A8<PQ1AC~A8Q1QX;=<\>1RH>W@A8Fy7iMO>aJRYA65<R_>36XA8A8FQ1ADT6XRT<EDgJI[Ao>WRHgEDT6XAVDTSC>Z;[R_<E65RT<DaJA8\TQWA8AVR ~igEACIff;=AC~b>1@ED>3S8D <ySW@ED <\TA6XRaJQLDT6X>X;}S8DIffI=`;[<>W@AH~ODTSCAHR ~o6XF#DIffISW@ED <\TA6;=<>1@AHFUADT6X^Q1A8FHA8<J>R ~3aD >LD7:RT>WAH>1@ED >e;ff~3f5AdJ<R{f>1@ED >s>1@A#>Xf5R?;=<E6X>LD <ESCA6R ~3jkzaJRhe;=<"~ODTSC>e*aJA8<RT>1A?ACBDTSC>ZI[`">1@A#6WD FHA?<Y^FgEA8Qef5APS8D <Q1A8GQ1A6ZA8<Y>5>1@J;}6cgY`U^E6p;=<\H>1@A361DFHADGGQ1RBY;=F#D >WA:A]T^ED{I|;=>X`PSCRT<<ASC>X;=4TA;=<PgERT>W@a_;}6X^<ESC>L687iMp<P>1@J;}6S8DT6XATe,;[>;&6nADT6X`?>1R#6ZA8Ao>1@ED >:f5AoaJR#\TA8>5>1@AD <E6Xf5A8Q:jkml7SWI=RY6XAI=RJRTd#D >c>1@AACBD FUGJI[AV6Z@R{f6i>1@ED >c>1@A<RT<Q1RTg^E6X>W<A6165D QX;}6XA6cgEAS8D ^E6ZAoR ~>1@A3<A8\JD >1AaGQ1RTGhRTQ1>X;=RT<"ACBGQ1A616p;=RT<q=qrs/utKLq=q w":x09jkzE7?MO<EaJA8Aaveif5A#S8D <6X@R f>W@ED >o;ff~3f5A#6X>CD Q1>of5;=>1@D;=<S8D <RT<J;}S8DIi~RTQ1F>1@ED >VaJRJA6<RT>VSCRT<J>LD;=<<A8\YD >WAayGQWRTGERTQW>X;=RT<ACB,GQ1A6W6p;=RT<E6o>1@A8<eh;=<D?GQWASW;&6ZA6XA8<E6XATe5>W@Ay6XA8>HR ~oF#DBJ;=F^FUA8<Y>1QWRTGY`"GER;[<J>L6UR ~VF#DBJ;=F^FHA8<Y>WQ1RTGJ`PGER ;=<J>L6R ~:ffo7[o3<D Q1\_^FHA8<Y><ASCA616WD QX;ffI=`SCRT<Y4_A8Q1\TA6>1R>1@Ay6XA8>HR ~S8D <gEAF#DTaJA>1@ED >nfbAH6X@R_^JI&aACIff;=F;=<ED >1A<A8\YD >WAaGQWRTGERTQW>X;=RT<ACBGQ1A616p;=RT<E6~Q1R_F>1@AI}D <\T^ED\TADI=>1RT\_A8>1@A8Q7)Mp>P;}6PRT<A>1@J;=<\>1RDQ1\T^A>1@ED >6XRTFHA8>X;=FHA6nf5Ao@ED4TAV6X>LD >Z;&6Z>X;}S8DI4{D{I[^A6f:D <J>o>1RyF#DdTAsI[R_\ ;}S8DI3DT616XA8Q1>Z;[R_<E6>1R>1@J;=<dR ~3S8DT6ZA6fn@RY6XAVDTS8SC^QLD_SC`PfbAVDQ1AV^<E6X^QWAD ghRT^>eE6XRU>1@ED >:f5AI=A6166X>1QX;=<\TA8<J>V>W@ED <ACBEDTSC>V<J^FHA8QX;}S8DI5A]T^EDIff;=>X`T7PMO>;}6V@EDQLaJA8Q;=<f:@J;}S1@>W@AHRTGGERJ6p;=>1A?;}6V>WQ1^ATeiD <EaD{I|I5f5A#dJ<R f);}6s>1@ED >6ZRTFHAP6X>LD >X;}6X>Z;&S;&6<RT>?A84TA8<D GGQ1RBJ;=F#D >1ACI=`A]_^EDIo>1R6ZRTFHAy4 DI=^AT7nR{f5A84TA8Qef5AaJR<R_>#ACIff;=F;=<ED >1A<A8\JD >1AaGQ1RTGhRTQ1>X;=RT<ACB,GQWA616p;=RT<E6~Q1R_F>1@API}D <\_^ED \TATe:6p;=<ESCAf5;=>1@RT^>?>1@A8FfbAPf5RT^JI}a<RT>#ghAD gJI=A>1RGQ1R 4TAHD <D <EDI=RT\_^A>WRy5@A8RTQ1A8Fl77/u5@A8`D QX;}6XAf:@A8<f5A>1Q1`>1RUD >1>1A8<<A6X>1AaGQ1RTGER_Q1>X;=RT<ACBGQ1A616p;=RT<E68eh~RTQACBDFHGJI=AT7ffKPMO<E6X>1AD_aveEf5AV@ED4TA;}aJA8<Y>X;ffEAa"DHf5AD d_A8Q3SCRT<Ea_;=>X;=RT<>1@ED>5;}6o6X^JSW;=A8<Y>>1RoGQ1A84_A8<Y>cGQ1RTgJI=A8F#656X^ESW@#DT6i>W@ED >6XA8A8<U;[<P*BED FHGJI=A:zh7ml7cCT, CH6p;=FHGJI=`H>1A6X>L6N>1@ED ><A8\YD >Z;[R_<E6:D Q1Ao<RT>;[<J>1A8QLD_SC>X;=<\#f5;=>1@P>W@AoF#DBJ;[F^FHA8<J>1Q1R_GY`PSCRTFHG^>LD>X;=RT<?;=<D@ED QWF~^JI*fD`T7A8>*i/ oj KvgEAn>1@A5Q1A6X^JI=>R ~vQ1A8GJI}DTSW;=<\VADTSW@#6X>WQX;}SC>*;[<A]_^EDIff;=>X`;=<#*/ oj K?_b=&ffE5f5;=>1@;=>L6of5AD dTA8<Aa4TA8QL6p;=RT<7PRTQ1A~RTQ1F?DIffI[`Te*f5AHQ1A8GJI}DTSCA?ADTSW@6X^gJ~RTQ1F^JI}DR ~b>1@AU~RTQ1Fjf5;=>1@ojevD <EaAD_S1@6X^gJ~RTQ1F^JI}DHR~b>1@As~RTQ1FjHf;[>W@oj7/u5AS8D{I|I>W@ED >3>W@A6XAHD Q1A>1@ART<JI=`SCRT<E6X>1QCD;=<Y>L6nGERJ616p;=gJI=AH;=<*/ j KLe6p;=<ESCA#DIffI>1R I=A8QLD<ESCAH4{D QZ;&DgJI[A683D Q1AHDT6W6p;=\T<Aaj7ffKvffogEAvT/ j K eJf:@A8Q1AsfbAo^E6ZA;}6yL88v_}H C{;ff~V>1@Ay6XA8>L6?ffo>1R#aJA8<R_>1Ao>1@ASWI=RY6Z^Q1AoR ~i7*AV6WD`?>1@ED ><EaA8>ffo3@ED4TA?>1@A61D FHA?F#DBJ;=F^FUA8<Y>1QWRTGY`~Q1RTFBDFHGJI=AVzE7ml7b5@AVSCR_<E6X>1QLD;=<J>GER;[<J>L687nv*&*RT<E6p;}aJA8QoD \YD;=<y>W@AodY<R f5I=AaJ\TAsgEDT6XA~RTQ1F^JI}DH*/ oj K*;}6o/XD~>WA8Q6u;[FUGJI|;ffS8D >Z;[R_<vKL/Onjkl:HvMO>C6f5AD dTA8<Aa4_A8QL6p;=RT<?;}63/OjkzKvP/Ojkz3?jkzKLk/ j KLjkl:HjkzKvP/Ojkz3?jkzKLfififf:,JhTT!"$#%&')(*+*-,/.10/24357698/.:0;2<3 5>=@?A#@)BCEF&D G!HJILK 2NM$OP.:0'Q-.1R)SUTWV R X. 0ZY 3 576[\)-$]C_^ F G`HJIJK 2aC F G`HJIbK@c M$Od35>=@Qe357fSg[h?ji!(@eU*\*gk,+l@meJ@'%mn$`op)-)(*q'rq]#q\st)(h*-,mllu,(h*vwB*-Jx(,&y!nz$|{@mg HJI }(,m*-g)(*g'`!~l@,hd!*!%$m?*-Jt,`,(z-#!*Jg,)B-)(h*g'l@,h*g%$*d/g#e*,\zm#@v(h*-)J*@**-|q&rhq]#q\s)(*--,mllu,(h*e,C G!HJIK e,m(%m)-z$J*-,]*-,hgJ,C F G`HJIbK ?|'1:phveh\bhm HJI )ew: 1|`hb)g\ ]h])1-e@ :g C FD G`HJIKE 1]mhbm]g CE^ FD G`HJIKh] )bJmm 3 1]m )1tej$Etm`)-:e ete/ P hw-ve )1t+ej$]\mw:/ wv/\ ]h])1-e@J 1_g C G`HJIKE +]gJj9\ ]g)1@] :aZ:]::11@](,g,W,*-,Ue,mq\l#*-ZOd HJI S,m|Ue)*v'(/-g*-ge*-n$-b,_@eg*-st,mvn)}t,m-sq]#m|(@n/x(,nzm\{@m HJI ?ZA]q\,hg*|d!z$(:)(h*gg*-ge*g,m(J*-@*J*j"$#)-g,m#n/{u\"$#@(*g`@)-s)UOt@vg*st,mvn))Sk)(h*-)(@e],%m)b*-]%m,){#-+ c M&'[hu*-h#@wB*bJ"$#)-\{@,m#*J(z(@n$%$n#@&B1? b*-_m-e*-)-g*ge*g%mmB*k#eE*-,Jerl-q(+'s`tUer q\l)?~,m-),%m)'Bu!*|zm(`:)(*g)|*@(*-](z$#@zme,m(@dn)n{}(@nZ)(@e,%gxUOemmSe?Ayb,`,(zne@(*g,m(/el@bne@(*|$-,p(h*-)g*?Z$kpp& t,m-q]#E )e)): mu 1m *1"m#@(*g`@)-st-)k(@nJl-,mlu,m-*g,m(st-)\t,m-q]#Z(*-(zm#@z$EEO-M&0Q'555Qvm[mS9Odg,+*@*B(l@-*ge#B*]@m9(,e,m(@g*v(h*gq]{@,eSk(@nU@m,$(!Z,m(-)J%g{\p?|-'\*-@*EOg'S b:`wwvb) HJI XEOd1Sg)(h*&Ul-,$l@,h*g,m(@&BHJI b,*|t,m-qbOd'S1 HJIJ B1})-|bOdSb-g)(*g'`!/l-,$l@,h*g,m(@&(@n HJIJ n,(,m*q\)(h*!,$(?A#@)Bg#@g*]mb(A),m-)q=@?mBkZg#ll@,h*-@*JOg'Syg#qqg)]'`k*-@*JJx(,({u,m#*?EP(+*Je*g,m(pB@bt,e#@},m(/e,mq\l#*g(zZ*-n)zm)|,{@e`e OdEOd'S) HJI St,m9t,m-q]#EOd'SUq\l"$#)-Z,$ HJI ?,m*-~*-@*(g)(h*&l-,mlu,h*g,m(@'bt,m-q]#OdS|Z"$#!%')(h*~*-,n$g#(@e*g,m(,*-,$q)?,mer@q\lmBu,&%$)k*J%m,){#-/M0QeRw[hB*-bt,m-q]#\0'Od1S18R'Od1Sy"m#%&&!)(**-,J 0 Og1S8 R Od1S'8kOd1SOP)-E*-*-,mqZp-,mvn)n$1(r@q\l6? Se?,$(9-g)(*g'`l-,mlu,h*g,m(@'u,$-q]#Bh*exm\OPS{@b*-|O#("$#S_g)*_,*-,mqg#@Z*-@*bE"m#%&&!)(**-,fiff&Og1Sv?T~*-,pk}%m)J*-#l .+V Wm9l-,m{@{`*gm-d!z$(q\)(h*E*-,|*L*,mq)B)(er*-)(@n .UZl-,m{@{`*g $-zm(q\)(*J,m('`-g)(*g'`!l-,ml@,*g,m(@'_t,m-q]#mb#@(z/*-bn)(*g`:)*g,m(,_(U-g)(*g'`+l-,$l@,h*g,m(@&p,$-q]#j*-+\g)*,*,mq XZ$kpp p)*J{@(+-g)(*g'`/l-,ml@,*g,m(@',mq#? jne@(\j#(@e*!,$( "! $# XV &%(' )$E,, X! $# .p S2 *fi. 5+ff, -./fi021fi3fi465+798:5fi;fi<=46>?1fi3fi4A@ 1fiBfiC72Dfi7FEG3fiHfi;I5+JKLfiMONQPSR-RTPUWVTXZY=[\[^]`_fiN-Ma_bMWR$XVTXMaUbY=[bcdMaN-efW[ZYaRQgihkjmlYUbno?hkjml6pPqnWPsrbUfiPtVvufiPhw_bYN-VXYx[ylzcdffiUb{sVTXMaU}|Q~ +&( VvMbPa| ~ OW hk l| ~ + hT lQ |"~ whT lMaV-PV-ubYVV-uWXZRcyffiUb{sVX^MOU}XZRqffiUbnWPsrbUfiPSnpqufiPU|"~ WwhT l2fiRV-ufiPQcdM[\[MxpX^UfiNvPSRTfW[V2RTufiMptRXcg`XZRQYtR$Xe_W[PtOffiPN-]?cdMaNihwMcfiV-ufiPicyMON-eo?hk=ltlV-ufiPUY=[\[qV-ubYVe YV-V-PNsRX^U:{sMae_fiffiVX^UfiAQN-hkgql?XZR|Q~ + dhT lqcdMaNV-ffi_W[PSR` Mcqe Y=WXeffiePUWV-N-MO_]auWfbRzXU:Y}RTPUbRTPapPYN-PMaUW[]fbR$XUfi V-MnWPV-PN-eX^UfiPV-ufiP R_bYa{sPMaPNpquWXZ{vupPe Y=WXeXPPUWV-N-Ma_W]aqtY=OXUfinWPsrbUfiPSnV-uWXZRRT_bYa{sPapP{YUcyMfi{sfbRMaUoYUbngAXUnWPV-PN-eXUWXUfi}V-ufiPnWPON-PPMcbPs[\XPsc-zW6=bG`b=aI gihk=lt } Z 6 \I, ` aZ zd OZSqT 6-s fi 6 :qI \? 6 |"~ dhS l ,},[\Xe [\XeRffi_T[\Xe:XUWcS x[\X^e "N hkgihk=l,?l XUWc |"~ fi WdhS lsRTffi_ |Q~ + dhS lqfiufiP2cdM[\[MpXUfi?XRiYUXeePSnOXZYV-PqfiffiVGXe_MaN-VYUVQ{sMaN-M[\[ZYN-]Mc6VvuWXRGVvufiPMaN-PeGwV2YOR-RTPN-VRV-ubYV=X\cV-ufiPtR_bYa{sP \?bubYaRiYffiUWXZOffiPe Y=WX^effiedPUWV-N-MO_]_bMXUWVSWV-ufiPUXVR"xY=[ffiPffiUWXZOffiPs[^]}nWPV-PN-eX^UfiPSRV-ufiP_fiN-MObYWX\[XVT]"N hkgihk=l,tlz SbGzb=aI, gihk=l? zIaZ ss 6dZ z-sbI 6 qfi \t z Q| ~ WdhS l2 2WQN hTgihk=l,?l2| ~ fi W hS lff PYN-P?XUVvPN-PSRTV-PSn`XU"N hkgihk=l,lfipquWXZ{vuePSYUbRV-ubYVpPYN-P?XUVvPN-PSRTV-PSnX^UV-ufiP?[\XeXVtMcQN hTgihk=l,ltYaR W fiaffi_fi_bMRTP XZRPSRvRTPUVXYx[[]_MR$XVTXaPaufiPUW]VvufiPN-PSRTfW[VR?McV-ufiP_fiN-POXMafbRqRTPS{sVTXMaUYUbnV-ufiP{sMaUWVTXUfWXVT]Mc | ~ fi OXViXRPUfiMaffiauV-M[^MWM }nOXN-PS{sVT[]YVVvufiPte YxXeffiePUWV-N-Ma_W]_bMX^UWVRMc \?w MaN-PcyMaNve Y=[\[]afi]}{sMaeWXUWXUfiufiPMON-Pe b pXV-uQNvMa_bMRkX^VX^MOU b fipP{YURTufiMpzW6=b fi?b=aI gihT=l Z 6\Iq W$IO? \t W `W fi6-sI 6 s,6 aZ?I 2z | ~ W h= l 2QN-hTgihk=l,?l2| ~ fi W hS lff PtPs[XPaPV-ubYViV-uWXZRV-ufiPMaN-Pe pX\[\[V-ffiN-UMaffiVV-M{sMaPNqY [^MOVMc {YaRTPSRiV-ubYViMI{{sffiNX^U}_fiNsYa{sVTXZ{sPaRQMaffiNiPs+Ye`_W[^PSRiYUbn`V-ufiPtnOXZR-{sfbR-R$XMaU`XU VvufiPqUfiPsIVRTPS{sVTXMaU}RTufiMpapPMcdV-PU nWMaPVR$Xe_W[PaffiPNX^PSRYUbn UfiMp[PSnWaPbYaRTPSRV-ubYVYN-PPSR-RTPUWVTXZY=[\[]_bMWR$XVTXaPa2MaUb{sPN-UWXUfi}V-ufiPYaR-RTffie_fiVX^MOUMcYffiUWXZaffiPe Y=WXeffiedPUVvN-Ma_W]}_bMXUWVSzUfiMaV-PV-ubYV?V-ufiPPUV-NvMa_]cdffiUb{sVTXMaUXZR{sMaUWaPs YUbnRTMV-uWXZRYaR-Rffie_fiVTXMaUXZRYffiV-Mae YVTXZ{Y=[\[^]R-YVTXZR$rbPSnX\c ^tQXZRY sb RT_bYa{sPaPS{Y=[\[qV-ubYVYRT_bYa{sP XZR{sMaUWaPsX\ccdMaN Yx[[ YUbnY=[\![ fi "wWXV?XZRY=[ZRTM}V-ufiP{YaRTPV-ubY#V "$ %h "'&(QlT ufiPRT_bYa{sP\?GXRRffiN-Ps[] {sMOUaPs}X\cXV?XZRnWPsrbUfiPSnfbR$XUfiY{sMa*U )TffiUb{sVTXMaUMcQ[\XUfiPSYN{sMOUbRTV-NY=XUWVR+ffuWX\[PXVXZRq{v[PSYNT[]}_bMWR-R$XW[PV-M{sN-PSYVv+P UfiMp[PSnWaPbYaRPSRpqufiPN-P \?zubYaRefW[VTX_W[^Pe Y=WXeffie`dPUV-NvMa_],-,fi.0/21436578:96;-<5/6=>7@?BA1C;-;5/DFEHGJI4KMLONPEQ@R%SCTHUVD4WXRYZFL[GXI6\^]_GLa`bZ6IFc%KbGXEIFL%d%Ye:ROR%S2DR-c%KfKhgFT*KLbZFcigkjI6E*e:WXR-]4\R0lFT_LbR-LT*QbGLbR:QMT*QiR%WJmnGXID6QMTc%KoGcpTqWrT*D6D4WsGcpTHKbGXEIFLpt:u@RpQhgFT*DFL@Khg6R:UVELoKQhR-LbKhQbGc%KbGXvR0TLiLbZ6UVD6KbGXEIVUwT]4R:lmKhg4GLKhg6RpEQiRpUxGL!Khg6RLbRpRpUkGXI6\*WXmwGXI6I6E6c%Z6EZFLyQhR-z_Z4GJQiRpUVRpIKyKhgFT*K0{!| }~N df6t:g4GLyTLhLbZ6U D6KbGXEIwGL0El4v_GJE_ZFL[WXmI6R-c%R-LhLhT*QhmPEQKhg6R:Khg6RpEQhRpUBKhEyg6E*W]eGJKig6EZ6KGXK-Y_Khg6ROPZ6IFc%KbGXEIw{ | 6 }4~ GLfL[GXUVD4WXmVI6E_Kf]4R%FI6R-]tf0I4PEQhKiZ6IFT*KhR%WXmYe:RfLbg6E*eGXIwR-c%KbGXEIFt0KhgFT*K>Kig4GLQhR-z_Z4GXQhRpUVRpI4K>GLpYGXI+PTc%K-Y*T0LbRpv_RpQhRfEI6RGXIDFT*QhKbGc%Z4WT*Q-YGXK@D6QhRpvRpI4KMLKhg6R0Kig6RpEQhRpUPQhEUlR%GXI6\T*D6D4WsGXR-]wKhE+UVELbKOR%SCTHUVD4WXR-L]4RpQoGJv_R-]kPQhEU]4R%PT*Z4WXK:QhR-TLbEI4GXI6\FY_ZFL[GXI6\kEZ6QLbKMTHKbGLbKbGcpTW>GXI4KhRpQhD6QhRpKMTHKbGXEIE*Pf]4R%PT*Z4WXKML#N:Tcpchg4ZFLRpK'TWtXY-*CdMtRciWJE4LbRKhg4GL0LoZ6lFLbR-c%KbGXEIe:GXKhgT*IR%SFT*UVD4WXREHPKhg6R+Kig6RpEQhRpUGXIT_c%KbGXEI>t>@@6# RpK+Khg6R+WT*I6\ZFTH\R^c%EIFLGLoK+E*Pf_#o2aa2*4rH%p06pf%T*IF]Khg6Rc%EIFLbK%T*IKf%4t:g6RpQhRT*QiR0R%GX\gKyT*KhEULGXIKhg4GLWT*I6\_ZFT*\Rt RyZFLbR> > > KhE^]4RpI6E_KhR+Khg6R+T*KhEUNd Nd- Nd%Y-e0g6RpQiR GL!R%GJKig6RpQfN]4RpI6EKbGXI6\k+b6aaa*drE_Q N]4RpI6EKbGXI6\k+b2_aaadMYGLEQ NPEQ+6**MVT*IF]2H*%6YQhR-LbDFR-c%KoGJv_R%WJmdMY>T*IF] GL+EQ NPE_Q+06pOMT*IF]02f%>Y6QhR-LbDR-c%KbGXvR%WXmFd%tEIFL[G]4RpQ0Khg6RjI6E*e:WXR-]4\R+lFTLoRV+#XnNh#o2aaaqNbd@6*4r*M_Ndhd+b6aaa*NdM6*4r*%Nd % 6X06pf%6NdX 66**M_Nhf%*dMPe:R'E_QM]4RpQ:Khg6R#THKhEUL:TLO0yp'Y 2 Y2 pr Y4Khg6RpIGXK:GL0I6E_K0gFT*QM]wKhEwLbg6E*eKigFT*K'@Nh+p*dGLpFNNNNN: rr:NC q diN !F *dNC diNNC qNCq6EkFIF]Khg6RLoDFTc%Rfi ff + X eRLGJU D4WJmLoRpK 6t:g6RpIGJKGLyz_Z4GJKiR^LbKhQ%TGX\gKoPEQieTHQM]kKhEFIF]Kig6R+UTS4GJUkZ6URpI4KhQhE_DmwDFE*GXI4K0GXIKhg4GL0LbDFTc%RY6eg4Gcig>YKMT*j_GJI6\ Y_GLpN[ h*qi-iq*h hfi dfM6%66Nb CN 6Nb CNyL[GXI6\( Y6e:RcpT*Ic%EUVD6Z6KiR+vqT*QbGXEZFLyTLbm4UVD6KhE_KbGcyD6QhElFT*l4GsWsGXKbGXR-LyvRpQhmwR-TL[GsWXmtCEQ0R%SFT*UVD4WXRYu!Q Nh#o2aaaqNif%*dp+#pd{| X!#"%$&"'$)( + * !-,.fi/-$10a ~N*3 25 4 325 4663 25 4 3 25 4 6a3 25487 63 25487TLkR%S6DFR-c%KiR-]t *GXUkGsWT*QbWXmYye:RcpT*ILbg6E*eKhgFT*KVu!Q Nh06pOMNhOM*d++pX_d 6T*IF] KhgFT*KuQ Ni6pOH%Nhf%*d#o2aaaqNif%*dp+ p d: 66t:9EKhRKhgFT*KyKhg6R+FQMLoK'Kbe:EVT*IFLbe:RpQMLyTWLbE;<fi= >@?@AB5CEDFB@G@HIAJ>@?@ALKM>@N@OPC Q@CSRT?@U@GVB5WfiXY[Zfi\)\&Zfi]^Y`_3Zbadc3e@fMgbh1_3ficjh1klY[f_3fkmifon@_ph1kmiqh1nl\&fsrutve@fZ_qfadwmx1y{z|}]:elh~i3eFem{n@nmfkmjc3Zf{n@nl\h~ifil\1fh1kc3elh~ibpfxFtve@foc3elh1_g{kmp]vf_#e@Z8]cqemfico:V Ffikmgp@[~fi_3fofh1k@c3_qffic3fgLh1kmglfnmfkmglfklcx:c:h~pnfiqhP8\ipfZfiYaZ_3ffk@f_8\h1kmglfnmfkmglfkmifn@e@fk@Zafk@Zkc3emfic:fin@nl\)h1fc3Z_fikmglZa]vZ_p\~g@mpffruviiqemvfcI\-x1|}yfiwm|@tve@fZb_3fa@x'Vzx1j}mT~IT}P1mPm-kocqelhPvpfic#h&Zbko]fiZkmh~glf_:cp]vZ8{_ph~fikc Z{Y#l3`[m3@fi[[fi}){`fiT cqe@f:Y[Zfi\)\1Z8]h&k@gbh~3im3h1Zkope@Zfi]|fiZc3eifikfh)\1fifin@c3@_3fgjlZ@_TY[_fiaf]Zb_3xTtve@fvfamfg@gbh1k@]f:gbh~3im3mpfvh1anl\1fb@f_ph1fvc3e@_qZ@e@Z@c|lI\)\1Z8]h&k@mvcqZfin@nfI\c3Zc3e@f_3fpl\1cZfiYc3e@fn@_qfh1Zm:#ficph1Zkxh)\~3pZkryblzMiZkmh~glf_3fgc3e@fLn@_3Zl\1faZfiY]:emficiZbl\Pgmfsh1klY`f_q_3fgE{mZ@cc3e@fn@_qZmfilh)\)h&cpFZfiYif_3c8h&kFn@_qZnmZuh&c#h&Zbkmfih1fkpZafoiZkmpcq_Ih1kcxF@Z_fmfianl\1f|v]vfMah1elclk@Zfi]c3emfic_r1v13mz@)ofikmgLc3emfic _r3I~)fiz@@|fikmgfh1klc3f_3fpcqfgsh&k _r1v1~3)fizxZ@bel\&#nmffibh1k@m| h)\PqpZk#@fpciZan@@cph1k@Lc3elh~liZkmh~glf_ph1k@I\)\n@_3Zbmfilh)\h1cpgbh~pcq_ph1@@cph1ZkmiZkmh~pcqfkcj]vh1c3ec3e@fMiZbkmpc3_Ih1klc|fikmgsc3e@fkiZban@@cph1k@c3e@f_fik@fZ{Y88\&@fjfih1fkLc3Z_r1v13)fiz:c3e@fpfMgbh~pcq_ph1@@cph1ZkmxM5Z_3aM8\\1|Tp@n@nmZ#fZ @_\~fik@mfibfiZbkmh~pcZ{Yn@_ph1ah1cph1fon@_3ZbnmZh1cph1Zk|mfiI3@xZkmh~glf_c3e@f#fcZfiY cq_3@c3eqh1k@afklcjc3e@fpfn@_3ZnZh1cph1Zkmxf{h&bfpfaM{kcph~ic3Zn@_3Zmfilh)\)h~pcph~ipc{c3fafklcvZfif_:c3elh~\~fik@mfifh1kcqf_3aMvZfiYn@_3Zbmfilh)\h1cpgbh~pc3_ph1@@cph1ZkZfif_:cqe@fpfc^rupffrum{fih1k|m8\&nf_3k|ffih~g@glZm|TyblzTY`Z_glfcIh)\~zxfih&kmiffbi3ec3_3@c3e3h1k@afklcglfc3f_qah1k@fvc3e@fc3_3@c3efiI\1@fZfiYff_3n@_qZnmZuh&c#h&ZbkmI\Y[Z_3al\~j|@]vfifikglfc3f_qah1k@fc3e@fn@_3Zbmfilh)\h1cpZ{Y ff_qpmiqeY[Z_3al\~_r[ zTr[:zff\1ffi_p\1|T]vfifikglfc3f_3ah1k@f]:e@fc3e@f_on@_qZmfilh)\)h&cpgbh~pc3_ph1@@cph1ZkL3fic#hPfimfopfc ZfiYn@_qZmfilh)\)hP#cph~iiZkmpc3_Ih1kcxtve@fpcfikmg@{_gk@Zcph1ZkZfiY n@_qZmfilh)\)hP#cph~in@_3ZnZh1cph1ZkmI\Th1klY`f_3fkmif]Zbl\Pg3Ic3emfic E _r[z5hY _ @r[ zhP]vh1c3elh1koc3e@f_fik@f ffmY[Z_vff_3gbhP#c3_ph1@@cph1Zkoc3emfic3fic#hP fimfvc3e@fiZbkmpc3_Ih1klc h1k xf fimklh1cph1Zk|5c3e@fiZkmpcq_Ih1kcTc3emficZk@f:ifikMglf_#h&bfklY[Z_3c3@kmficqf\&|b]:elh)\1fc3elh~ hPf_3kmfic3@_I\5glY[_3Zah1cfi_qfcpnlh~iI\)\1blh&cqf]vffi}x 5Z_:c3emficv_3fbpZk| h)\PqpZkp@f#c3fgopc3_3fk@bc3e@fklh1k@Mc3elh~k@Zcph1ZkZ{Yh1klY[f_3fkmifl{n@nl\&bh1k@c3e@f:n@_#h&kmiqh1nl\1fZ{YaMIlh1a@afklc3_3Zbn _{c3e@f_c3emfikMiZbkmh~glf_ph1k@MI\)\gbh~pc3_ph1@@cph1ZkmL3ficph~Y[h1k@|@]vfiZkmh~glf_Zkl\1oc3e@fgbh~pcq_ph1@@cph1Zk}ruzv c3emficembfc3e@f_qffic3fpcfklc3_3ZnlfiaZk@oc3e@ZlpfM3fic#hPuY`bh1k@cqe@fMiZkmpc3_Ih1kcxM]vfk@Zfi]pe@Zfi]|Zk@fh&anl\h~ific#h&ZbkFZfiY:Z@__3fpl\1ch~Tc3em{cc3e@f _{kmglZa[]vZ_p\~g@Tafc3e@Z@gjn@_3Zfih~glfn@_ph1kmiqh1nl\1fgaZcph1fificph1ZkjY`Zb_c3elh~h1klc3_3Z@glmicph1ZkZfiY aM8h1a@afklc3_3Zbnoc3Zn@_3Zm{lh\)h~pcph~in@_3ZnZh1cph1ZkmI\_3fpZbklh&k@xkY icI|mc3e@fiZk@k@fic#h&Zbkmfcp]vffkn@_3Zmfilh)\)h~pcph~in@_3ZnmZlh1cph1ZkmI\m_3f#Zklh1k@fikmgj_fikmglZaS]vZ_p\~g@pe@Zbl\Pgjk@Zfi]FmfY-Ih1_p\1iq\&f{_"fiII3$ "x! tve@fn@_ph1ah1cph1fn@_3ZnZh1cph1ZkmmII3-@iZ_3_3f#nmZkmgc3Zc3e@f@kmfi_3n@_3fgbh~ific3f#! Fn@_3ZnmZlh1cph1ZkmI\lY`Z_qal\~oZfif_m I3 iZb_3_3fpnZkmg@T@klh~@f\1c3Zfikjf3pfklcph~I\)\1n@_3ZnZh1cph1ZkmI\}Y`Zb_3al\P&%Y`Zfi\)\1Zfi] ]vf_qfnl\PbiffiqeZVii@_3_qfkmifZfiYc3e@fn@_3ZnZh1cph1ZkmI\plamZfi\' ]h&cq(e ")'3r * zxc Z{Y n@_3Zbmfilh)\h~pc#hPisiZkmpc3_8h&klciZb_3_3fpnZkmg@cqZlk@Z8]\&fglbfmp,f +. -/0 12! tve@fpfiZkmpcfiklc3Y[_3ff:lk@Zfi]v\1fglfmpfiZklcIh1klh1k@MZkl\1n@_qZnmZ_qcph1ZkofVn@_qf3h1Zkmx te@fiZ_3_3fpnZk@glfkmifh~bY[Zfi\)\1Zfi]354fi687:9(;=<?>A@1B=C5DE<E7=F >HGJIK9ffC5CL<E7MONQP=RSETUVT(WXWZY\[^]_:P=R]5``aWZSEbcSVd#Ye=]dfSgRhjiARlk/mn&UoP=P]5UVR\WZb=pqWZbsrOWt`R]lP(XtUEu]5vwTx[yYe=]P=RSEPSgRY\WZSEbq]_:P=R]5``aWZSEb{zZz |}k~)n$zZz xVWZhWXtUVR\XZ[EUuSEbvgWZY\WZSEbULX1P=RSETUVT(WXWZY\[]_=P=R]5``WSgbiARk/mKz mHtnHWt`R]lP(XtUEu]5vT([|}k~n$z | }x k\~)n(MUEueuSEhPUoR\Wt`\SEbuSEb=b=]5uYWg]cWt`R]lP(XtUEu]5vTx[1)d/SER`SEh]8xUVbv]5UEuecKWYek1e=]PUoRY\Wtu(XtUVRue=SoWu]5`d/SER8Ye=]UoP=P=RSL_xWZhUVY]]5gULXWY\[quSEb=b=]5uY\WZE]5`v(Sb=SEY.hUVYY]lRWbYe(Wt`8uSEb(Y]_=Y5n1e=]SEYe=]lR]XZ]lh]lb(Y$`#YeUVYulUVbUVP=P?]5UVR#WZbUP=RSEP?SERY\WZSEbd/SERh(XtUk`\ueUE`#R$UVY\WZSEbUXb(=hT]lR$`UVbvUoR\WZYe=h]lY\WtulULXuSEb=b=]5uY\WZE]5`nR]lhUWb=bueUVb=pg]5v)=SER]_UVhP(XZ]EYe=]qd/SERh(XtUiARkZ1zZt?n8{ff1SE(XtvuSERR]5`\PSgbvYSYe=]P=RSEPSERY\WZSEbd/SERh(XtU=k\~)n$z1t)k\~)n$=x1e=]lR]Wt`AU8SEb=]l/YSE/SEb=]8uSERR]5`PSEbv(]lbu]8T?]lY\]l]lbYR=YeUE``aWZpEb=h]lb(Y$`AUVbvUVYSEh`l)Ye=]YR=YeUE``aWZpEb=h]lb(YA^uSERR]5`\P?SEbv=`YSYe=]UVYSEh{ )5L 8e=]lR] Wt` Wd 8kt:nA:SEYe=]lR1Wt`\]E^)]lY V 55 T?]Ye=]YR=YeUE``aWZpEb=h]lb(Y$`uSERR]5`\PSEbvgWZb=pyYSqYe=]UVbvsUVYSEh`1L5ffR]5`\P]5uYWg]X[E1e=]lR]Wt`USEb=]l/YSE/Sgb=]uSgRR]5`\P?SEbv(]lbu]T]lY\1]l]lbP=RSETUoT(WXWZY\[vgWt`\YR\WZT==Y\WZSEb`SVE]lRYe=]`\]lYSVdAYR=YeUE``aWZpEb=h]lb(Y$`.UobvPSoWb(Y$`KWZbffSER8]5UEueP?SVWZbxY&q(XZ]lY v(]lb=SEY]Ye=]uSERR]5`\P?SEbvgWZb=pP=RSETUVT(WXWY\[vgWt`\YRWT==YWSgbSVE]lRff8e=]lR] k/ \nqVy/x)&XZ]5UVR\XZ[E( z cm,WKk| } n$1e=]lR]d/SER]E(d/SER8ULXX ff]eULE]/lk\ nAiARE k/mn$Ke=]dfSoXXZSV1WZb=pqR]5`\(XZYv(]lhSEb`\YR$UVY]5`#Ye=]Y\WZpEexYuSEb=b=]5uY\WZSEbyT]lY\1]l]lbP=RSETUVT(WXW`Y\WtuP=RSEPS(`aWZY\WZSEbULXHR]5UE`\SEb(WZb=p`Wb=phUL_(WZh=hQ]lbxYRSEP([UVbvRUVbv(SEhQ1SER\Xtv=`l(L/gJ5rc$LfO^$ lo)fffiff xoiARk/mKz nc(lqffs?:xx/fcol$=/{!^fi"#LlEffff'&&r(*)V+,oE(mm , .iAR.-lk/m n0/c,Aff x$iARk/mKz mniAR1k|}k2Lnz |} k2Ln 43QV$%5r /nH{iAR.-lk/mKz nKe=]lSER]lh6?8796Wt`UVby]5UE`\[uSgRSVXXtUVR[SVd.1e=]lSgR]lh:687;7E Sque=]5u<YeUVYYe=]P=R]5uSEbvgWZY\WZSEb`SVdYe=]XtUVYY]lRYe=]lSER]lhUoP=P(X[E1b=SgY]YeUVYYe=]quSEb`\YR$UWb(Y$`&WZbcrQUVR]XWZb=]5UVR58Uobvc`\SqYe=]q`\PUEu]= > 3 0r ?eUE`U=b(Wtg=]hUL_xWZh=h/]lb(YRSEP([P?SVWZbxY@ BAb UEuYL?WY#Wt`&]5UE`\[YS`\e=SVOYeUVY# C W`Ye=]qk=b(Wtg=]VnhUL_(WZh=h/]lbxYRSEP([P=RSETUVT(WXWZY\[vgWt`\YRWT==YWSgbcSVE]lR`UVY\Wt`ad/[EWZb=pYe=]uSEb`YR$ULWZb(Y$`rDAbUEv=vgWZY\WZSEb 8T?]5ulUV`\]qYe=]lR]qUVR]b=Syb=]lpxUVY]5vyP=RSEPSgRY\WZSEbc]_=P=R]5``WSgb`WbrYe=]d/SERh(XtU3Ec| } k2Ln 430r ?Wt`8u]lRYULWZb(X[]5``\]lb(Y\WtULXXZ[PSx`aWZY\WZE]ESx`YUVP=P(XWtulUVY\WZSEb`SVd#P=RSETUVT(WXWt`\Y\WtuP=RSEPS(`aWZY\WZSEbULX.R]5Ug`\SEb(WZb=puSEb`aWtv(]lR`aWZhP(XZ]uSEb`YR$ULWZb(Y$`SVd8Ye=]dfSgRh`\]5vWZbYe=]Ye=]lSgR]lhAUVbvy`\S`\ue^UVP=P(XWulUoY\WZSEb`ulUVbyT]EWZ]l1]5v^UE`&E]lR[`\P]5uWtULXulUE`\]5`SVd#Ye=]RUVbv(SEh/1SER$v=`UVP=P=RSxUgue FAbydUEuY5HYe(Wt`Ye=]lSER]lh1e=]uSgb=b=]5uY\WZSEbT?]lY\]l]lbyuSE=bxYWb=pHG1SER\Xtv=`JIUVbvWt`]5``]lbxY\WtULXXZ[^Ug]lR[SVXtv,SEb=]EWZb^U`\PUEu]v(]%Kb=]5vYe=]]lb(YRSEP([qhUL_(Wh=hUE`UuSEbL=buYWSgbSVdXWZb=]5UVRuSEb`\YR$ULWZbxY`W`&E]lR[q1]XXM<(b=SV8b NAaYeUE`#T?]l]lb]_=Y]lb`aWZE]XZ[`YvgWZ]5vWZbYe=]*K]XvSodYe=]lRhS=v([xbUVhWul``\Y$UVRY\WZb=p1WZYeYe=]B7POVYeu]lb(Y=R[SgRJ<SVdREU_:1]XXUVbv4QWZT=T`l]5u]lb(Y\XZ[E=Ye(Wt`AY\[(P]8SodR]5UE`\SEb(WZb=peUg`AT]l]lbUVP=P(XWZ]5vYSP=RSET(XZ]lh`WZbUVbNSAuSEb(Y]_=YATx[iAUVRW`UVbvTPUfiVXWZYZ[]\^`_H\ZaZb[]cdWZYZ[fegWZhZi?^XjZ^lkmYZnZa'\opqsr9t u%vw'xfiy{z}|~P;.zt ; z;xJm|~P;.5Zrd5v;JyBvszxXzt Nqsr9t u%vwZxy{zx zJu%z8Jr%8r9w{ztsrPu9z xrdJZr9NzxvJrPz89rSZrtZrPu%rPxJx8Bvz;v;Z8tZ(zdMv;Jz'tZv.8v;tNvX9zZZJvz8v;t Zz8JZv;Z.4JZr(ZJrPuxrBr9zxvJZr%8zZZJvz.uJ.r95MJv;v;Z%x9v4JZrB rPxfi(v5v;ZytZv58rP;r;Nvxv0JZrB5v;JyFv;tZJv; zxuZJv;vx88v;t zXJrPz.xv;tZ8tZ"zt zXMv;JgzsZJrPxr9tzfiv.t xv5JZrNr9tJJv; 0v.ZxSu%v;tZtZrPu%fiv.t|M8t zJu%zPmJZvxrBv|szfi?xqsr9t u%v{wZxyzZ5~P;;Zm; z.xJ5~P;;'J0 zw;r8B8JrPfJZr9gxfir%w.rPxJvu%v;tZt u%8v;t xv8tZrPz(u%v;t xfiJz8tx9Z(}v;JrB;r9tZr9zsztZ. z;rN8w;rPx x(z;rPz(rPzXvz;Z.88v;t z0r%ZZJrPxJxw.rv5r9P4Zv.(r% zN8r;xB;8JrJrPz;xfiv;t z8rNJvFztJZr4z8Jvr%ZZJrPxJxJ zZv; r9JfirPxzJrF|zZZvP8gzJr%8xzxu9z88t r9 r9t r9tPDZv;Br% zN8r;55rgzP5xJvz;xxr9JBJ z5J|Nzt Pffd|NzJrt r9r9t r9tZJv; r98rPxDxJz;8tZ`85ffJ |d|9885ffJ |8 8*|? s8rPz8; xfi uJu%v;t xfiJz8tx5zJrdtZv;8tZrPz0*r9w;r9JJZr%8rPxJx$v.ZS5Zr9v;Jr9 8~;~u%vw;r9x5x u"u9z;xfirPxzt B uJ4}v;Jr;w;r9xv.tvXZJv. zxfi?uZJv; vx88v;t zsJrPz;xv;t8tZ z;xdzxvN r9r9t xrPFJv}ZJvw;r(Zv; z?xfiuNxr9gztu9xv;dr%z8JrPz.xv;t8tZ|mrPz~P;.X*r9Jrz{?xfiv Zr(u%v;tZtZrPu%8v;tJvNzt v;5v;Zx*xv58tJr9JrPxPtf zJfi?u%z8Mv8v{dx*Jv.:Xv;JvzJ ?~PN zJZrBJrPu%r9t(5v;Jy4vv?Zxfi9BPFv;Jx9zt mrPz5|~P;.0u9ztF rBr9B rPZrPF8tJZrBzt v;NM5v;ZxzNr95v;JytFJZrJrPx*v?x*xZ xrPu%8v;t5rr%'z8tJZr%8zZZJvz;uzt 4JZrr9B rPZ.8tZv;t xr9Xz*ztZ; z;ru%v;t x?xfi8tZ(vZJv; vx88v;t zZMv;JBz;xvw;r9mJZr5ZJv;vx88v;t zw{zfi?zrPxPPZ mzt r%z8rPxv5JZrBMv;J:|JrPz;$NxzJrN?u9z{8BxJZr9Jrgzt NzJr*ZJv; vx88v;t z Mv;JBz;x9 .xJ8ZZ8v;t4xxJzNv(%9;ffff%gzr%z5J8rl:(m|NB~B9tz.Z.fiv.tJvr%z8J8rPx95JZrMzNr95v;Jyfzxv r9B8xNZr xr4vgzr9z8Nu9z8v;t8t"z}J8r;z;x8tB .xJfiZZfiv.t"?xdxJzJvgxJzfi?xx u"zJ8rSm|NN~. JNPM$ P% JMff"ffff% ZffH|X*xzu%v8rPu%8v;fiffPgvZJv; z8".xJfiZZfiv.t xdv{w.r9Z z%zNr9Jr989rP9 XffP xzx rPxzNxr9vJ8rPxv.(r9w;r9J9m fixJzx rPxr9w;r9Jr%z8BJ8r ! zt fxJzfi?"x rPx(r9w.r9JtZv;tZr%zJ8#r $ xr9%vr%z8J8rPx(fiMr9tz{x v;dr9w;r9JXzfixJzx rP&x "8 (') |NB`~;xNxZvt8t | r%tZr9mrPz~P.;XfiMr9tz{8Nr9t vxJxfirPxJxrPxNztZB r9NvJrPz.xv;t z8rZJv;r9J8rPx*u9z8z;xJxvZuzJrP4r%z8dJrPz;xv.ttZ8t u .8tZzBZr%r9Jr9t u%rv.Nv;Jrxr9u*]u58tMv;Jgz8v;tsv5r9w;r9PJZr9rzJrSzdtZr9XvrPx8z8rdZJv; r98rPxX zm80vrPxmtZv;X zw;r;Nv;tZv;JZr9dJ8tZx98JJr%8r9w{zttMv;Jz8v;t4xdtZv;*8;tZv;JrP|J;r9rN"| +5z;u9u xdr9z8s~;ZsMv;zt4r%ZJr9t x8w;rB.?xu% xJx8v;tvsJxxJxZr;v(v.Zz8tz.Z.fiv.t zrPx8z8rZJv;r9J8rPx9 xfir9gztu9xsxr%ZJr9t rP}8t| vZx9?5r95z8P~ ;.Szt zZu9zfiv.tvSZrgZ8t u88rvdgzBZ:r9tJJv;;t xJrPz;vSu%v;t xr98tZHzvxJx88rBx9z;x4z vw;r;d5ru%v.t xr9"v.tDJZrfX-,Z/ . 0 132 xfi uJJ zPv.4rPz;uJ9/ . 0 1 z;x*JZrBgz8Zr9tJJv;zNv;tZ4.xJ8ZZ8v;t xJ zSfixJzxM"zXJZrJ8rPx*8t "|J;r9r| vZx9Br9Sz8~P.;Mv;5ZJrPuxrBr tfiv.t xzt NrPuJZtu9zXr9zx9*v;JrdJ z'xt u%rJZru%v;t xz8t%x xrPJvr tZr5/. 0 1 zJr5z8tZrPzP.JZr9Jrxt r9rP}zZt.ZrSx u( vtsv gz{8BZr9tJJv;; J8r"xgz5476X% 998:;Z; 8v%8 (') . 0 1 |NB4 ~;5ZrtZv;8v;t4vX9<sMz x88rgu%v;t xrP.Zr9t u%rxzt z89rP48tr9z8tf| vZx9?dr9Sz8~P;;%Zr9Jr8dxxZvtJv}tZZr9fiBzXJZr(tu%rNZv; r9JfirPxv0Mr9tz8Nr9tN|x uz;x*JZrBZJr%r9r9t u%rMv;Nv;JrNxrPu* ]uB8tMv;Jgzfiv.t8rx u9u%rPxx8.tZv;8tZ48JJr%8r9wzt8tv;gz8v;t#<; z88N v.Jzt8;Zz8;v;8JZgxzJrdZJvw;rPv.0u%v;NZZfitZBJZrd=<z x88ru%v;t xrP.Zr9t u%rPx5vzxr9vsJ8rPx58tu%r9Jz8tu9z;xrPx$>?fi@BADCFEGIHKJMLNOPGPAQ/HSRTUCVNNWGPAXBYZ%[]\W^_[`Y[#a(b;cFdZePfFghZibjkYFldmj3n;\cpoqb#Yqjkbrsdetjvuexwydiuq\d%dub#\ffZe\znu9e{)|}el~rjk;[_rFdb;d3\WlKzPFn;\c9oIb#b;[`oqbrrFbr7_cePYZ){(Zm\[b;wezZ#_c\$jvdZm\W_PuFdk{(ePZwB\Zmr[\ccb;Zb#j_[#fFlgdZm\cqjl~\db3\#rFb{\YFldBZYFlb3`e{Kdub){(ePZ[_cFde\qZmjkda(ePZrFb;ZrFb{\YFld)ZYFlb%B;U;|"m ;M|"%P\Pj_c$ePYZb\Zkl*_b;ZdZ\cqjl~\dk_ePc$e{KM_*l~jjkePc/j\ffZie\Pniu/ePdb%diuq\dMdub){ezZ[`YFl\zjMduq\dB\Zk_~jkb%YcqrFb;ZduF_~j%diZm\cqjl~\dk_ePc\Wl*lYqjkb#dubji\[#b\ffZie^F_[\db`bzYq\Wl*_d"gnePccbndv_zbh#MubZb\PjkePc7_~jduq\ddubh\ffZie\Pniue{|}el~rjk;[_rFdb;d\WlBzPFYqjvbj`dub$j\[#bh{(ePZ#\Wl*lrFb{\YFldZYFlbj;bhn;\cj_[`_*l~\ZklgdZm\cqjl~\dib\s|cePcarFb{\YFld;ZiYFlb]$e{Mdub`{(ePZ[_cdie=\]qZjkda(ePZmrFb;Z3nezcqjkdZm\W_cFdYqj_c]YcF_zb;Zmj\WlzYq\cFdk_*n;\dk_ePc/;(/&||"K|"cqrFb;ZduF_~jMdiZm\cqjl~\dk_ePc/Vwb3n;\c$fZePb%dub){(el*lewM_c]dub;ePZb;[7`FWq-(V]/W7]hq;(P/~$]q;*PP;;;hi;;/v3P(;mm;*%mcfq\Zdk_~nYFl~\ZduF_~j`dub;ezZb;[%9h*Fm;*`mq;mD%kBZF| | /P;_[fFl_bj]duq\d\xllBdubhnez[#fYdm\dk_ePcq\xlUdibnucF_~zYbj\cqrZibjkYFldmjrFbjnZk_oIbr$_c|})el~rjk;[`_~rFdb;d)\Wl/PzFn;\ZZg]ePb;ZBde#diuF_j)jkfIbni_\xl[#b;duerd\xljvejkuewjduq\d/Zm\cqrFeP[#a(wMePZkl~r)fZez_rFbjS\MfZk_cqni_fFlbrn;\Pjvb3e{Kdub%Zm\cqrFez[#a(wMePZkl~rjYqjkdk_*n;\dv_ezc3{(ePZ/diub\ffZe\Pniu|})el~rjk;[`_~rFd%b;d3\WlSPPFfZbjkb;cFd`|ePcb3wuF_niu9_~j%zYF_dibrz_b;Zib;cd{ZieP[diub_cYqjkdk_*n;\dv_ezc=_zb;c|})el~rjk;[`_~rFd)b;d\WlPPD_dmjvbl{Wm* fffiDfic Pbndk_ePcq]wMb#ZbjvdZk_~ndbr\ddb;cFdk_ePc9de=j"_[fFlb$zYb;Zk_bj;9XBYZ`[\W_cZbjkYFldKMub;ePZb;[qPzcb;brFbr$ezdub;ZB\PjjkY[#fdv_ezcqjB\PjwMbllKbjijkb;cdv_\xlfqej"_dv_z_dkgPdub%b^F_jvdb;cqnb3e{K\3YcF_~zYb3[\x^_[`Y[#ab;cFdZePfFgfIe_cd!x\cqr&dubZibPYF_Zb;[#b;cFdKduq\d"$# %&(| ('!/!boqbl*_b;PbBduq\d/duF_~j/diub;ePZb;[fi_j_cjkfF_db`e{K_dmjl*_[`_dm\dk_ePcqj\zjrFb;[#ezcqjkdZm\dibr$oFghdub#rz_~jnYqjj_ePcs_c)Pbndk_ePc*q+VYqjkb{(YFlb;Pb;Zdiublbjij;duF_~jZbjkYFld3\Wl*lexw)jBYqjde#dm\Pb3\PrFx\cdm\Pb%e{ePcFlgh\#jk[]\Wl*l{(Zm\P[b;cdBe{ePYZ)Zk_~niu$l~\cPYq\zbP-,\cwMbqcqr=\][ePZb3zb;cb;Zm\WlKdub;ePZb;[/.0B{(db;Z3\Wl*l/diuboq\zj_~nnezcqnb;cdZ\dk_ePc9ZbjkYFld`|"Mub;ezZb;[1++Duel~rj)wM_du9bjjkb;cFdk_~\Wl*lg9ceZbjvdZk_~ndk_ePcqj;c9duF_~j%jkbndk_ePc9wMbjkuewb^db;cqrUub;ePZb;[2qP]j_PcF_*n;\cFdklgP*3BewMb;Pb;Zduq\dM_d)_~j_cqrFb;br9fqejij_oFlb]dedub;Zb$\Zb$jkb;Zk_ePYqj&l*_[_d\dk_ePcqj#\cqrjkYodklb;dk_bj;b)_*l*lYqjkdZm\dibdiubjkb%fZePoFlb;[jog][#b\cqjUe{b^q\[#fFlbj;I\cqr$dub;chjvdm\db3\c$b^db;cqrFbrXBYZ#\didb;[#fd3dies\zrrFZbjjdubjkb]fZePoFlb;[j]|kjke7{\Z`\Pj&_~j`fqejij_oFlb&lb\zrj`defFl*_~n;\dbrsqcq\WlZbjkYFlddub$dub;ezZb;[ZbjkYFld\$Zm\dub;Z#neP[#ac9{\PnddiubfZiePoFlb;[j%wMbrz_~jnYqjij\Zb#\PjM_cFdb;Zbjkdv_c9\cqr7_[#fIePZdm\cFd%\PjwMbh\PndiYq\Wl*lg_zbP$dub;gublfb;cFdZePfFgP`XM{UnezYZmjkbPb;Pb;ZgYqjYcqrFb;Zmjkdm\cqr[#ePZib$e{_~jjkYb`wMbrz_~jnYqjij)_cde7[\W^F_[`Y[b;cFdZePfFgj)[\W_cdiub]l_[`_dmj]e{[\x^_[`Y[duF_~j%jkYoqjkbndv_ezc9_~j%Zbl~\dk_Pblg9[`_cePZ3neP[fq\Zbr|k\ffq\Zb;cFd;ZibjkdZk_~ndk_ePc/KwuF_niunePcqnb;Zcqj`dubYqjkbe{cePca(Ycq\ZgfZbrz_~n;\dbj54ePZ)dubZb\PrFb;Z%wBue_~jlbjij%nePcqnb;Zcbr\oIePYd)dub`ePdub;ZVlbjjkb;ZI_~jjkYbj)wMb`Zb;[\Zduq\d_d_jfqeFjj_oFlbdiejvP_f=rz_Zbndvlg7de6Pbndk_ePc7Vpb3qZjkdnePcqj"_rFb;Z`dubZbjkdZk_~ndk_ePcqj&wMbfFl~\PnbrpePcdiub/8:9K\cqrjvuexwydiubrz_;tnYFldk_bj`duq\d\Zk_~jkbU_{Swb)rFZePf#dub;[7pbjkdm\ZidKwM_dudiubBZbjkdZv_ndv_ezcde3\j_clb%[\W^F_[Y[a(b;cdZiePfgfqe_cFd$0%j<=fi>@?BABCEDGFIHJDBKBLMCEN-?BABCOP?BQBRSF@TBFVUABWBKDGXYZ\[B]_^a`cbd^a]ebfZ\gahZjik`lb6Z\[B]e`lg\]emonqpr[B]e`lg\]emtsGukvswyxj[B`z_x|{lZ\[B]}]ebZ\g\`c~f`yhMkm`fxjZ]el]eg\zr`lgjixbB]hgmPhMikmBmu(BZrikZ:`f]xbB`lZr`k`zZ\[dhZ_hZ\[B]5mPhfikmBm]ebZ\g\`l~P~`ikbfZax_hg]5xjBg\g\`lBbd]fxikmihg@bBmd]egx`Ezr`lgSBx|up[fdxe{MikbZ[B]}~Bg\]xj]ebd^a]}`Em`lg](Z[dhb`lbB]rmPhMimBm]ebZ\g\`c~f~d`ikbZ{zr]hl^a]6Z\[B]6~Bg\`lk]em`rdbdcikbBZ[B]Pg\]ahZic]ikm~`lg\Zhbd^a]l{`lgz(]aikl[ZjikbBd{$`_]hl^[mPhfikmBm]ebZ\g\`l~P~`ikbfZMu@:x}Z[B]-`k`zrikbB/]aGhm~k]:idxZ\ghZ\]x|{dZ\[ixzr]aic[fZjikbBix}`Z\]ebxj]ebdxqiZic]Z`Z\[B]Z\`k]eghbd^a]:hMkB]xeurB`lgrZ\[ixg\]hlx`lb{bB`lbBBbilB]]ebZ\g\`l~mPhMikmPh`Z\]ebk]hl6Z`bB`cbBg\`lBdxjZ\bB]xxeuSfB~B~d`fxj]JraM{hbd/^a`lbdxi]eg-Z\[B]5bB`zrk]l]:dhlxj]:nenqffwa5}Bsw/nenjwa}rBlwa:x\xjBm]:zr]:z}hbZ(Z`P^a`lm~BBZ\]5gnqnqMw :lnhbd:_`lZ\]:Z\[dhZ-Bs_iSx`lgr:B} fBs`lgr:Bcfixlnw@wubZ\[ix_^ehcxj]l{}B:_:_w@V_[dhcxrZjz(`mPhMikmBm]ebfZ\g`l~f6~d`ibZx`z^a`lbdxi]eg}Z\[B]:mPhfikmBm]ebZ\g\`l~P~`ikbfZaxr`@nqBsBaBlwyhbdnqBfBsw\u:-u$ZrixrbB`cZ}[dhgZ\`Pxj[B`z`cgZ\[dhZi({dZ\[B]ebZ\[ix:xj~dhl^a][dhlx:hBbiScB]mPhfikmBm]ebZ\g\`l~~`ikbfZ{njBs-JMB-*MwubZ\[ix^ehlxj]l{@gnqnqMw :w5Bs u*}bZ\[B]6`lZ\[B]eg[dhbd{i_ /{Z\[B]ebZ[B]BbilB]mPhMikmBm]ebfZg\`l~~d`ikbZr`yZ\[ix}xj~dhc^a]}ix-nqG$ Gsr/ w{ib/z[iS^[^ehcxj]g{rZ\[B]ebJZ[B]xj~dhl^a]}B:_B u 5xjmm]eZ\g/z(]`lBZhMikbZ\[dhZ:$gnqnqMwe :`d{h~B~Bg\`l~BgjihZ]a^\[B``fxikbBh6xj]cB]ebd^a]wrIBBu]aikZ\[B]eg:BsB{BBG{f`cgGfu$pr[dx}$gnqnjMw :wr`f]xrbB`cZ}]afixjZMuZiSxbB`lZciSxqBbd^aZjik`lbdxBe6Z\[dhZ^ehdxj]/Z\[B]/~Bg`lk]em[B]eg\]::nenqffwBsw nenjww@{@zr]^ehb mPhl]/Z[B]hlxjm~BZ\`lZji^6hMkB]`Z\[ixgahl^aZjik`lbB`:Z\`k]eghbd^a]l]^aZ\`lgax^a`lbl]eg\ikbB*Z\`BhZhdhlxj]nqnqMw :[dhcxZjzr`mPhMimBm]ebZ\g\`c~f~d`ikbZxe{}hbd)fh/BbicB]/mPhMikmBm]ebfZ\g`l~f~`ikbfZ`:_izr]/^a`lbdxi]egikbdxjZ]hlZ\[B]Bw{Z\[B]ebZ\[B]eg\]:ixbB`ci^akZjlu:pr[B]eg\]nqBBaBBwiSxhbdZ\[B]Phlxjm~BZ\`cZji^~Bg\`cdhiikZj$ganjnqMw :- w$BB{Bhlxrzr]:zr`l/z}hbZubikl[Z_`@Z[iSx]adhm~k]6nqhbd/m6hbfxqimihg:`cbB]x}zr]^ehb^a`lbdxjZg\d^aZew{Bzr]^a`lbfZibB]Z\`PhlxxjBm]Z\[dhZ-Z\[B]eg\]ix:h6xikbBk]mPhMikmBm]ebfZ\g`l~f/~`ikbfZMu:xzr]hg\lB]]hgjik]eg{zr]5]aB~d]^aZ:Z[iSx-Z`P]Z\g\B]-ikbZq~i^ehMy~Bghc^aZji^ehMh~B~i^ehZjik`lbdxe{x`Z[B]5g\]xjZgji^aZjik`lb`f]xbB`lZ_x]e]eml]eg\/xj]egjik`ldxeu]bB`ztZ\Bg\b`lBghZZ\]ebfZi`cb*Z\`Z[B]Pg\]cikg\]em]ebZZ\[dhZnM wtGuP:xzr][dhMl]PhMkg\]hl`ldxj]egl]{$Z\[ixxj]e]em6x5Z`]hb`lfcik`ldxg\]xjZ\giS^aZi`cbJZ\`mPhl]l{$^a`lbdxi]egjikbBZ\[dhZZ\[B]Bbd^aZi`cbfffinM wriSx-bB`cZ5]adbB]`lZ\[B]eg\ziSx]lu}`zr]el]eg{Z\[ix:ci^akZjixhl^aZ\dhMkh6mPhbi]xjZhZjik`lb`(hmd^\[J]e]e~d]eg~Bg\`c]emu:x:Z\[B]``zrikbB]adhm~k]x[B`z-xe{$hbf*h~B~Bg`fhl^[*Z\[dhZqdxjZdxj]xZ\[B]mPhMikmBm]ebfZg\`l~P~`ikbfZ}`(ff::zribB]^a]xx\hgjik/hMiikbxj`cm]5^ehlxj]xrz[B]eg\]nM w$Bu`lbdxi]eg}Z\[B]fbB`zrk]l]:dhcxj]!" #$ %& 'ff# ( *),+ .-"/0213546387:9;46<>=6?#9@=%AB=ABC!9EDBC6F:G#FHC6CABIDB3:=6FKJLFHMC6=646N.JL=O3QPR9@7KG'DB3LCOF;S27!NDB=ABG'DB3:7:9EP'AB7!N.75T(3LM'=646FUG'VG#F;ABM'=6CWI'VXN.CABMYZ NR9;[.4\9;=ABJKJ8FUM.C6=64\9EABM'=6CO4\9;=6?.3L4,=6?R9;M]['ABCB^QNM.JL=ABFHM_0:nek (nqffwS_w/nBnqwe (`*anqw_w(n@aBwafib:cd efffhgOiKjffk.l*f*cffm,gonqprdsk.kf*ct*uffvffv2w xLy]zKy:z:{#|}5}@w~Uw_vffuff}@y]OH@!ff*,!U.ff'# ,*,!U.ff@UWy~'{#|y.{*xQ~Uw*|2~;u26@w*Kffy'w_@y'h]}@2{#}X}@ (xX y'*@y'yw#O2yUyU(x]ffh{*x:zKy$zw_u yUvhy.~U}.>w#zKy'*y'.ffzKy~'{|ff|ffw*}@y.{*~;}@ (x]~Uw*|2~;u2xQw*|u2xQ|ffKffy'w*@y'2*]w*]{#| }@ |ff*y}.sw*]~Uw*|2xQ( y'}@ffy{Ruffy'| }@@w*v vhw#|}w#!# ]> Kffy~Uw w*H_|2{#};y.x*#K~Uw*@@y.xLvhw*|2_|ff};w!# K{#|2*~Uw*@;y.xLv2w*|2_|ff]}@w$O!,K# *{#;y!2w_}@ffo:y'|2~Uy* h('QH#6s Off*xLw:};2{#}rffy'w*@y'2*w y.x:|ffw*}:{#vffv *uff}.h{*x:zKyx@{Rh}@ffy$vff@w* y'(x:w_@y]6uff|2ff{#y'| }H{\:Kffy| 6w*@{#}Lw*|zKy]|ffy'y.\}@2{}:}@ffyvff@w*vhw*@}Lw*|&w,2*|ffvhy'|ff*u |2xK(xK'y';wffO(x:xQv &|ffw*}Kvff@y.xLy'| }5{WzKyX|ffw#z(xK}@ffy]{Ruffy'| }@@w*v v2w| } y&~'{#|w*ff}U{|}@ffyx;{#yxLv2{*~Uy ]> 8{#|2}@ u2x$}@ffyx@{#y{Ruffy'| }@@w*v vhw#|}];w*_u };y_,y'@y'|} |ffwRzry. _y2{*xLy.xQ|v2{#@}E~Uu ({#:~Uw*|2xQ( y' ]] z: (~;xQv {*x;xLy'@}Hx}@2{#}'ff# (%8H O# W*%!HffHK (x$|ffy'z |ffw#zKy. *y2{*xLy}@yU(x]u2x|ffw*}@ |ff&z:2{#}UxLwy'_y'{#hw*uff}]}@ffy$6H{*~U}Lw*|w#O2*|ffv2y'|ff*u |2x'o{#|2|\{*~U}}X(xXy.{*xE}@wxLffw#z}@2{#}]O @!ff*,!U.ff' ] :ffBff uff}>w#~Uw_uffHxLy$}:(xv2w x@xQ y}@w&_(xL}L|ff*u (xL}@ (x~'{*xLy@w_}@ffyvff@y'_w*u2x$w*|ffy]Lu2xL} w w*_|ff{}& \}]6w#w#z>x]}@2{}]|ffw@y.xLu }$|}@ffy&xLv L}w#Kffy'w*;y'2*$\z ~;Lu2xL}:u2xEy.xK}@ffy]R{Ruffyw#]ff ~'{|&2y~Uw_vff@y'ffy'|2xQ*y*rffy]yUs{#v yxEffwRzXxK}@2{#}K}@ffy]vff wxLw*vff &2y' |2Kffy'w*;y'2*X~'{#|ff|ffw*}:hy]yU};y'|2 y.&*y'@\{#.,{#}W{\%},(x%|ffy'*}H{ y}@2{#}o}@ffy'@y!zr2hyvff;w* y'x,zffy'| 2 Os uff},}o(x%|2{#}@uffH{ff}@w{*xL$z:ffy'}@ffy'5}@ffy'@yK(x!{$_y';y'|}K{#vffvff@w{*~;{}@w**y'};ffy'O|z: (~;}@ (x@y.xL}@L(~U}Lw*|~'{#|2yX@yU({ffy.K2{#}(x'_x}vhwx@xQ y]}@w~Uw*|2xL}@@u2~U}K{]}@y.~;ff| (*uffy6w*~Uw*vffuff}L|ff y'_@y'y.x5w#hyUyUO|}@ffwxLy>~'{_xLy.xz:ffy'@y xzKyy'|}Ew_|ffy.&|t*y.~U}Lw*|h*ffzKy* }>ffw*vhy}@w w&}; xX ~Uw*vffuff}E|ffL: ] ]{*x]{6uff|2~U}Lw*|w# {#|2}@ffy'|}H{#_|ff}@ffy}{_x *w y.x]};wff\|*y'|ffy'U{\O}@ (xxLy'y'x*y'@2{#H uff}.O| }@y'@y.xL}E|ff*}@ffy~Uw*vffuff}U{#}Lw*|2{:}@y.~;ff| (*uffyw#;>w#(ffxL'( }y'}{\.*_ > wy.x$u2xLy}@ (x$}Lvhyw#:v2{#U{#y'}@L(~{|2{xQ(x'5 y'w*|2xL}@U{#}L|ff}@2{#}}; |ff x_}$|ffw*}hyxLw2{_6w*:R{Lw*u2x:;y.xL}@L(~U}@y.~'{*xLy.x'KX|ffw*}@ffy'>xEw*uffH~Uyw!ffw*v2yx}@w;y'y'2y'X}@2{#}:{Ruffy'| }@@w*v &(x'%6w*]u2x',y'@yUw*|ffy}@w w#O6w*~Uw*vffuff}E|ffH{|2 w*6zKw*L(ffx] y'*@y'y.xw#:2yUyUEKffy'@y{hyw*}@ffy':{#vffvff;w{*~;ffy.x!};2{#}K v2{*x@xKy'| }@@w_vy'| }L@yU*5\|v2{@}L(~Uu ({#.2xLw_y>wW}@ffy]}@ffy'w*;y'xzKy]#*yX| {*~'~;u2xKy'}:{\%.*#ffO~'{#|&2yxLy'y'|{*xK w#|ff}@ (x'ff}@ffy.xLy]};ffy'w*@y'x5zKw}@y'|&{#vffvy'*y'|!X|ffw*}@ffy'K{*x@xLuffvff}Lw*|{* y:}@ff@w_uff*ffw*uff}:t*y.~U}Ew_|2 X(x5}@2{#}!};ffy:|ffw#zKy. *yX2{*xLyX2{*x5{xLvhy'~;({26w*@#|2{#yU X * ]: *zffy'@y xy.x@xLy'| }L({vff@w*vhwxQ}Lw*|2{,{#|2 ]] w y.xO|ffw_}~Uw*| }H{|{#| wff~'~Uuff@@y'|2~Uy.xw# #Kffyw_@y*y'|ffy'H{!}@ffy'w_@y'zKyxE}H{#}@y$({#}@y']@yU({ffy.x]}; x$xLw*y'z2{#}.{*x6w#w#z>xq|ffw#zKy. *y2{*xEy ] x]x;{(};why O6 @ .! 'ff U}2{*x$}@ffy6w*@ ]] oz:ffy';y ~Uw*| }H{|2x$|ffyU}@ffy'_u2{#| }L 2y'Hx$|ffw*vff@w*v2w_@}Lw*|2x'O{#|2 ]>~Uw*| }H{|2xK|ffw*|ffy$w#O}@ffy~Uw*|2xL}H{|}:xL 2w#(xX{#vffv2y.{#E|ff|w*5| ]\}KxLffw*u (2y]~;y.{!}@2{#}%{$*uffy';5 x5xQv yXw_ ] 8{*xO{*x@xEuffy.$|vff;y'*w*u2xKxLuff2xLy.~U}Ew_|H}@ffy'|}@ffyxLy'v2{#H{ }L~Uw*|2_}Lw*|(x:x@{#}L(x 2y.]xO};ffyKww#zK|ffyUs{v yxLffw#z>x#,zyX w|ffw*}K{*x@xLuffyXxLy'v2{#H{# }L*ffzKy>~'{#|y.{*x8;uff|| }@w|ffw*|ff@w_ffu2xL}:2y'2{_w*.w_|2xQ( y'O}@ffyO6w#wRzr|ff] |ffw#zKy. *yK2{*xLy ] w#*y',}@ffyK*wff~'{#ffu ({#@$8H $ ff $ @ $8H ] $ @U! #"%$'&($)fiff45+*,& -."0/132-&768:9;.6KJ<>=?A@CBD EFGIH:NUT5NAV:NUTNMLONIPQSRWYX[Z]\U^(_(`+\Ua0bdc+eUfSg^ihQjS`_lkk`h-_lgnmoeUcpc^q_lf-r+eUg0b'eUr_;\Uch>cj-sg;^cte:uvrm^xw-fhlyA\U^;z{0^|'_(c^eUc^;}-j-eU~Q_#\U^;f-rxrhIhQfS^xh(uvrmYeUcuhQ`qXfi[S[5%'od8;-v'0'GI((#v08l(fi-(MSQI;vd0'o0dv#Q!'Gd!]'UQ7dQ]G(KS;,(5SGK-l(dGE]'S8id(fiv0Q'oo'(0dv#(-!llESK-G((dGvGo;IEfi0%]08(v5(,(#-v0'(,0d';v0Q5;S'vS(,fi+v00fiGd!S0(EG-fi(-,']070(Iv'-7K,I-(,vG(.G(ddldEfi(0fi3(l''((-0'v0-oIIdG0v08x0;So!M'I-d-IlG(-d;didv.G-!(8]-vSdd!'x;SodQ7Q'x0dvG-;fi5Soox8;Gl-G;#5S7;3(v'dl'7(E-vAIGGv05(vxd(-4(:A(:;A#S0o;Sooox(d(-iv'vAG77E-:tlfffi'((d(5fi-;>'(GI-55GQ7UxpQ;o;];,+/:tQ'IGdvd;;0';;;-SGd(5;d)U'v,(fi-(-v.ox-(id(;(I7K0G#Q0QxII'l(-0((.dIl;5.(-ooG-]-vfi!O,o(S(fiG-;77K'vlnx;3(v'-loloo.l:t#3:tQ5ld(-iv'vAd,IGv-(7K;v-vSddK';lv'x(ooxfi(dv;d(00fi;-5;->'x(dvfi(;d80((fivId-#Y2143657198;:=<4>?365@<4CE-HKJLJM0d0#fi0((#Q';;;-SGGfi(5';dK(vinS;ov0dG(('d-Q.SnSQp('io'iU-(ldvG'id('7'(8-vG'7'>(v'ox(dt0ooIdI-(0d(d]G8I-d(-U7(fid8I>G(-v3(fi-;lv'Ifi'dUAnppd;ol;-lv'(iGfiI5SGQit>(;-G;S78(d].;dvSn(dvG-(:;AQd0#fiv I(#(;0(I'7O0v'Go77v;(Q'l(fi70G;-!"$#&%'*,+d(fi;(d;I'Si;SQG-(d;(Gl;(;0;dIKGdIG(-v3(5S7'GSx(;Sooidi(dv;'l(-#fi(!(;l;0;dvd(5fi-(d!.dlooooQix';dvG5'ld;'(0l;d-v0'('-Qv0d0(d00dGd-('('(G0d0;d5'G'GU'fidGdid0G;'S7n;0fivEfi+;v0AIUG(d'v,dUfiGId-I(o0%]0(!'S'xU!(ofi(;K8lK7dId-l;G-l;]-vfid>-(;StlSt0oodI-(0d(dAGI-0G#Q0Qo.v0K(K-IEdG30((.d3v0fiv.;(-GI(3(dQO(v'xoo>(-(,5SGK-G((fiv'G8'xiKdGGQS';vlBAG7K(;'IGdIvd##dGv'GivKv(ll'xIId(S#-5'G;-xIIvfiG vldG((v'xI0dv.G-S7vv(G(('-Qv0xdoldvdl;d50((fiv;vSl;GI;Ifi#(;.(v'id(dv#((I(GFKKdG'(d(v'(K('-Q'v0>fi-;;fiGCvS7OI3(!G-v-vdllG;08d!(-0'v00((.dv,((G>lG#fidoGd '7-'IGdvd;dinI>vl.GK(dl5S7 -GS7,I((d]d(;d,0((.dvSv-lGQ('d',S(-(v';d;-'#->dOC%0vidlo(;'x(;0#fi8dLN(G!n' v0-(BC*v0;(dQG-ofi-;-v-fi-El;lv'Ifi'dK(fi-'('(G(Q'(!';;!'v!fivS7S(dfi!#;7 x-3((#(;0(((Kv'('dvdA-AQ.xid;K(SC'IGdvd;Qo>OI-d-Svd>('7'I!0G5;'-Adv; v-Ql!-'-'v;oldG-5-iQ'l-IvQ#(dQG-fi-;-.!nd0]ddGS(5S7''(G(Q(BPRQ!"$#7T=',U WV YX 9ZY]9Cfi-;(v'o'(-,'dod(K(d;fiv'Gv-vSfifidvG-0';dv'(GS(-(v;d5;-5;-IdG,dvOIQ o'v0dv.G-oo#-(v'I_K`\[YX^Z[o;AQ]o*:tQ'vd;' OvK(-(0d(lGdfiacb&d=egf@hffi0jgkKlLfLbgmhnpoqdrkKk9fLbs2t9u=vGs(wgsBxzy|{~}^L=6vG{=}cvG{L}^vO -gK 60y|Ly|Kgff-cE,9vW^GyKt6.G!rcgy|^yKtLff4B.cEffv,tM4}wt9SG)LR2.y|y}^gyWt }}y|0tL4}K=L.y|^LyE}t6}c}^gyEtL.}s2t6.L.vG}.B6ffsBg=y460EEt6Lwg{ tL4}wt9SG!^t6}.vzD~W*-4R0gy|yvWtL4}wtGL{=GDtOvG{g6Gy0L.tL.g^vt6}yK0vG}-g96t6}Wc=v^\~,*.vt9.yLq=vy4urt sB=Gyvy4t6}yK}ut6sB=Gy(G6~@yK|t w.y(vG}vSSw}t6}yKWt6{gL}^gy|,|tL.yWvG{D=v^)L E,|t6{g{gL},.w=4yE}2=y|}y|sv{gy(=y|Ly|yK0 @y4vGy4{}gy|tL.y6}^gy)~{g60GyK=LytLyE6}gys2t9u=vs(wgsBxzy|{=}M~6vG{=})-g96EvMw=v}^ys(vGyKtLMvG{g!t Lwg}W}gy({t6}wgy*6R{gyKt6=0L.g|W)y*sw}t9L6v}=vE.L}c |Mv4L{=}.vG{~w=vG}.&cgy|{{Mv{g\}^gyD=y|Ly|yD @y4vGy4B6tzLs(w=t}t6}(vG{=L6GLyK.}xzM=y|2Mwt6{~}vy|4|Y0gy{gL}.vGL{O6.}t =vSvG}.2=y4{gyKOy4G6vffvG{=}y|{=yK}=yKt90vG}O}=vRg^L=Gy|sDR=y4{gyWvG}KL0y04.}{gy|yK}^gyW?6SG60vG{g2{gM}.vGL{D6fftOG9M|||4;7$z6OMGK$R 49L7$z6.6BEOvBt4L{6-wg{4}vM{6zLs(w=tL|?L(yKtL^t6}Ms06Ly|2=vG}v{^Gw=yK*y4utL4}.GL{gy(60~,qK-qt6{ff~,q6-crLW) )((@}gyB49L7$z6 |K?Lz4Wffz$E 6.vG}}y|{B. 9v}t }v |yc=yK4.vGg}.vGL{c=v(vG{^Gw=yK0ff~W -vSR Bt6{~, -.vSR \gqgyg^L=Gy|s2E}t },0y(ct6{~}W}Dt9L6vg|4wgEcgy|{)}gy|y*vtOsOt9u~vGs(wgsBxzy|{=}L=D@6vG{~}0vG})v |y2=yK4vg}vM{Kg c.w^}t6}v{tB{gy4vGL= L^g~g6( sB~.}W6R}gy(0L.gEt }.v?MvG{gEYt6yctL.g^vt6}yK*0vG}(L}gy|R-v |yW=yK4.vGg}.vGL{| {~}^w=v}vMy4Lr}gy0gL=Gy|s 0vG}(}=vv}^t6}ff}gy4=LMvG{t6}yK60 t9GL{gy06vGLy0w s(vGyKtLMvG{gEvG{=?Ms2t6}.vGL{Ot6Lwg}}gyc{t }wgyR 70L.g{gyKt6E Lt6{.t6Mwg}=y|L^y|yKc6R@y4Svy4 )y vMyt.w=^vGy|{~}4M{Mv}vM{)c=v)|t6{y(w.yK}t9L v}=vgL=Gy|s vG{D}gy4L{=}y4ug}c6ffLwgc}gy|My|s2|ff0=vW4L{MvG}.vGL{vy yK4}vMyt6{Ow.yKs2tL^=vG{gy|zvG{t6}v4w=t69}gyt6=vSSvG}.D}({. wg}vM{tL4yK40}t6}0vW{gy|yK=yK}Ow.y}gy*s2t9u=vGswgsxzy|{~}^L~t6gg=tLOvG{t6{~|tL.yLOMGK$ y|} yts2t9u=vGs(wgsBxzy|{~}^L=)6vG{=}(6E E, yt9}t6} v| |0vG}^yK.yK4}W}E t6{ ffvSB v{gL},4L{=}t9vG{gyKOvG{ E Kg z)yt92}^t6}EE6) 6E|z q qvSzL0y|Ly|Bs2tu~vGs(wgsBxzy|{=}L=(6vG{=}B2 EWytKMy,}t }9r Rt6{O}t }BB vc^t9?yEqv}^DyK.@yK4},}^2E t6{qgyE{gy4u&}c^yK.w=G}cv}gyELy|OgLy|^}-O6R.}t6=vSSvG}.}t6}00yE{gy|yK= 9 $= EE/6) 6(|z ($~Dff . Eff~wgE}^gy|Ly|s20vSSRw.y(}gyBtL.wgsBg}vM{!}^t6},}^gy|y(y4u~v.}ELsBy .w^}t6}Kr?Mt9S.w=zx7^vGy|{~}.s2t9S E t6{! t6^y.}t =y?M, 0)y*{gL}yE}t6}W}=vW=~yK{gL}vs=}t6L}, v{gyK4yKt .vS}gyv |y=yK^4.vGg}.vGL{)tL.g^vt6}yKO0vG}}gyEs2t9u=vGswgsxzy|{~}^L~O v{=}|-R6RR E,z!$ L{v=y|}^gy={g60GyK=LyBtLy2Ev{ut6sB=GyB rL7t {D^yK|t9S}t6}O-g96vc}gy*s2t9u=vGswgsxzy|{~}^L~O v{=}W6 E zc0gy( v |y=yK4vg}vM{Kg vff~W -~W,9-60y|Ly|}^gycs2t9u=vGswgsxzy|{~}^L~*6vG{=}R6 Er?L vtL4}wt9SG K9K K.B}^t6}0}gyt6gg^Lg.vt6}y qzL,.w^Dt vE~WW9- D~WW|-fffi! #"%$&'(*)+,".-0/#1325467,89#:<;=?>A@CBD=E E5F*>AGffHAIJ;>@CBLKC;,@CMN=?@OLKCGffPQE5;RTS,GVUW@CMX5S#SCGffKC@>Y=?HLHZG[@#=?K\X5SC;,X5HT@CML;,>GffHQ@C;6]L@^GVU=_R`=]QX5RaBLRabc;HQ@CKCG[OQFTODG?X5HQ@9#r%GffKC;sOLKC;>JX5SC;JE5F[tuUG[K,SCBQv`>JX5;HQ@\E5F%SCR`=E Em =VHZwx=aR`=Y]QX5RBLRabc;HQ@CKCGffOQF%ODGVX5HQ@pm G?Uzd0DGVU!d0Def gihkj_l UG[K`npqef gihkj_ln0h}j|~{ ;>G[HIJ;A>@CBZKC;@CMD=V@s^K ef g lCW yLhkj 0{ ML;KC;X5S=?H<GffON;HSC;@k@CMD=V@s>GffHQ@6=X5HZS<y PLBZ@{ X5@CMHLG`Gff@CML;KsR`=Y]QX5RBLRabc;HQ@CKCGffOQF%ODGVX5HQ@aGVU!d ef gihkj_l 9U#@CMQX5S0X5S_X5HLwL;;w@CML;a>Y=?SC;fftW@CML;H%@CML;aR`=V>JMQX5HL;KCFGVU#SC@6=?PQX E X5@\F@CMN=?@{ ;=?KC;=?PNGffBL@s@CGX5HQ@CKCGQwLBL>;TX5SsBZHLHZ;A>;SCS6=?KCF[tS\X5HZ>A;X5@kMZGVE5wZSsX5H=YE E!>V=?SC;Ss@CMN=?@MN=ff;PD;;HpBZHN=?PQE5;T@CGTOLKCGVff;@CMX5S9Q{;HZ;;AwX5@9skG {;ff;AKt {;fi!z}Z!0 zD%TT<DYNV<?LVcffW#*WLD#DW<D#&WLffYu#&ff%CW`upffVLWffu&QuVQL`^ffffV6u[Qp?[D?%Vc`VLYZ#T%QLWuuYu#ZffW#ffQLTLQZsV#D*WLffW#5Tff VL?LDLVYQcZ?W#AYu#ZkCWVcaA,Wuc\#ffWD?WQD#ffTY?Zc}ffW#JcQCWVcW#J}WVV#DWffW#ffDD#WC#DVVLucQDVLQ_QLcVQ<ffW#ffQ%?#DDVWpTc*cVWYDDW#Q#V<ff#WDffD!LCW[i!\CD#QQp^<W<QTD0NDcDLD#ffW#ffDffQ0uLk\ ,`DWQD#cpfiff(ff )$* ! !!" % +,ff )- !!!.# %0/21WTQWQVVLucQDVffzWffYWD3[p,ZCWQQQff5,\!! !"#$&%'W#D#LffWD#ffWD0CWVcW`#?V#D<DDYWD#3 u`QWQVffW[Z#TW`ffWD#ffLc`ffW#AYVQZW3Q?xLVffYYu#A3ffVQ#?`VDDkDVW_V%ffW?ff aD!LVYQcZVW#JW#ZCWVcW0V%?`WVTWQQffZ!QV#0%_V`QWY`DVWQW[QYVffWDA#ffff*ffWQVL?ffWVQffW3Q?fi3Zffz?#JW#ff#D#YuW54sLDWkffW[ZLD#*?ccCWVc64#?V#64 87 94:;=<->,?&@"AB4WY#V*CWVcZQWVV#YLDVVQZucVff#VJW}JffzffQ%TV?D6ff[^TDV%VQVL?LQDuQLVffWIHKJLEGMDCDWQWaLVff?puFEGWxffu#VV#ffW,*QffW#JcQNVL^Lff%VHuQVT^ff#VQxEGOM%06#VcffcDLVc*LD#T?#DffW3Q?xLVffYYuQ SR URVAD#/Q?QaVW#Q?L!Vpffu#[D[_&DVCWVk*?c%Q#DffDTVV&ffu[NVL?cffVYffW#ffDffa.PVVVQ`#V[V#DWDW#DVVLucQDVLVPQUR WT XRuLW?<VffW#[D^Dcff[TV}WDuWi DQW#ZD#?#QVQVZ9Q SR #T URVffW#ffDffDV W#N<V}LW?uVQQ[D#V}!\[ /LNY`cV}D`cVffW#Jc[DWDWWC#DVVLucQDVL3A#?#%QVQ?L<?WC#DVVW^QVLWVQ%?#D u#ZAD?ffWQVLVffWD#&,TsQYu&ffW#Y[D`c?ffW#Jc[xV&ffuDA#ff[3DV#QVQVLDDVuffuDA#ff[DupYQpDffu#[D[`TDWQ<YQDkffW#ffDffsQD#%LffW^L`c?ffW#Jc[Dp?ffWDA#ffffDVY`QVc*VWffWQVLVffW#NuQpDQ3AYQpDDDcDLQ63 !^p%QsD!ffW3Q?* uDZQVVuc\#ffWQ`ffW#Y[D[TD#3 ?Q<NcDLT`CVQNDYLD#ffu#[D[DVu#D#QVpD#?W#W?W\ ?QW,c WDZQVuuY#ffWD!ffW3Q?pLVffW#aDWQsLVffW#DWQW3pDVV[VL?ffW#<D?<Vffu#[D[ff[T%TuWLVL,,V#D?#DVD#?^] _gfff VQ#VWWDF`ba cedu`Vc`?#W^T<Zcff VQ#CCWpDVWQDVWVVQWD&ffu#[D[Qh`ia ;D#ff#, upLVffW6VWffWTDV<?VLDLWVD?%#uWc#Z W#Q#Q#QLsV#D`VVu#D6j /V#DffW#[Dkfmln6fml9VDcJ#L*DVW6j /V%Vu#D?#D%D#VDcJ#LjPoV#D`ffW#ffDOfmlqppVcJ#LOjPo%D#V<?W#DV#A#*V<V#ff`Df=lqpp?DcJ#LNrsfmln )j / =Js! !!tJf=lqu )j.v5j / ! !!2#j.vw"xzy{| }2~#| W0qq).| }2fi0b-~#qW0z0p 2}".q6~.Sq 6q=~.2P-20WyO-~#b.=P0t0|}0 )= = p " b.-~#q^W "#"| ^W ""000|}2~|W0qq2|P2yuLk\uDZQ?WffuQVLVffWTVuDDcDL`#D?#DV%CCWuc#ffffW#ffDff<D#CW^Dfi6XtUb=UUmeBfifi=^tUb q b Uq$n"-6 * tPP B " U - -"=- B=5P Ui P h U " " 6 " .-^UqU - - tfi 6 5U #- ' " q "fiU5qP ." U "t - fi Uh "5" U (.O"- 8 9&#b -" = .O"- 6 9& " U t5 "" Ps U ^ =&tN F N"- q U fffi #=^ U . -- " " = - .^ D& . " 5"-ff!^ (. " #"8 " 5 " & 8 DS "&')( +*, 8" t5 " " -. "= - * & U^ % $ & " ffP5= . 8-" P " ) U. " . - b" 5Ufi / =" q tP " 5. " 5U " - t- . " *" . " - U " - & - 1 0 -# " PBt . "" " 5 . 6 . - 0 .56 U- = ="- " 5 U " " " 5P " 2 - "" ^. " - tU UN 5t& " U . - 1 0 43256 N + 6 P .- #-. b" 5 U P "5" mt= P "# "U * " -" = UqP ".U " = .- b b.-b" " "" 6 6P -" 5U #- " . -9 n " t&h + 6 P N 5U U "- = t6-,. ' + 6 P . " PB ^ U ) U fiUkP ""U " "t 87 49;:U<>=@?ACBD?EGFHJILKMON2PRQ TS P *U >LV*WJYX[ZT*@\ff PRQ ^]`_>a P * Pcb b P Q d*eJ!fg b Q ih PjR@, P \Rk P Sl* b P i8X Z ,.meon mp bqb P fL*)*rS P ,. Q ms Pcb f P n QQ Bt NuPDQ[v PwQ PwbPDQ xYn bQ * QJb*fef P *U Jgk`JyYJ ]{z b Q mr* Q n Q *i b C P x Q P n U b Q * QJb v}| *UC\p PRQ#~/ PQ P >g+*px e>8 t8 b b r PQ mr* QQ P P`P b QJb * b P \ Pcb nc !f Q 3 5 b gncm Q mr* QJ*eJ p *C\ * PbQ *S P 3 5 *UC\ Q mr* QQ Pb fg*en P[e m* b *wo P ff* J>r 'P Q )ffLi Q tW2m Pfi 3 5 _ \ er)fi ^bL eQ P \ P C>JC* Q b fg b Q ih P7 " W3 5 O_ - fit ff3 5 O_ & -S. q"O. 6 6 9& " Pfi 3 5 _ " =' N . & "-="O. 6 9& " 5U " " ^ U U "6U " - -6" - .-" q fit ~/ U . " PmtO -S. q = - = " "^ n 6 . U5U- r%dc P C* Pcb " . - P " Bt # UPRfi[LLCLLDCOLLYLL[LLLgT+1ff)iO)L el RO) L)Rx Y1e+C L D+1Rx1e8/L e) 1ee1xx++erLff le#)Rx)xix1ecL )i#LffeLRx1eYl[xeff2)LRx Y%1rx LRW)qeffL) rDD1)LeLepDeL 81>)>pwlRU /e1x1e1e+C x1We)e L ul)LeLR#)Dex )R@8O1RDeO1e L)e1 L )L xlx Ue1x1eOix /eL xeLlLffrVC UrO)Rx)DeYe})D+11LYe1LlYL)Ri lR1lL>#ii)xL}i)LqLLR)LLgRx1e2ff2[[#@#e L ULe oexiLff lLgY R.eff1 )x d)Ld+LiLe @D1 R@oee xilL iLeR #`)Le)L U@LeLL+i x1epYr1>L ))effi#+1>1)R)>L) rL1ReeR )i)Rx)R)x xiD T+1ffLeDe )L ) ))iO 2R R@u1OOL e 8l1RU)L>e 1LYU/x)8W)L e) {ei @exR)L))#1i) D11LeL>)LlR)LiLRRxR`1p)iU p>e L D+ R1e+x1ewLY 1`ilR@e>>Rw1l4L qx1xL)Ri )effx >LD1eL)>)LLex1e[)ew/ULY L oe)lL e) eRx1e L > `)LL).L e) L e i)l>1ReWR.1ff%r)11>) /wULRx)Rx+L)e)x1eL DeL oL)L)Ri )Rc .e pei x1LY)l #e4>We)1rlw )LwL)e1 Yx d)>iL1e L)`iLeUee.)eYU)ffxo/%x)L ff)1ff1R)x1Lx1el[xLffYUx1 e lr)1L).>1e[% LffeLeYW W r#1 }ffL [Lei OiLeUe1)Y1L11lffL)Ri )8eL )i#iLe1l1V1) L L ))1Li1Lff+)L )pRe>o)e 8l s)iOiLee2)L )ffl>1lsY4Lle )R})YDff%))LOe#Dixq%eLffeY1 ff R eff1eL)L)x )x1e lLLg eL1er%e `)>LL)R)11w18)L>iLeUedeR) > @18 ))8)LeYw1 ff e e 1L>)lLL) )x1R#)L Ylxioeeei>L RwlqL1)1Y) LL))L lffe)e1 p%L) x V1 e11WrxD+eL/x l x18elR1 R)pLLlxiR 2 ) r # x Lle )Rc1ffrx#e LcrD+2)LOLgRU) 2 ) LLxqlx1x1eYUeYD1w1 ff r.1)ff}lLq% ff c)eY R+1)x1px xe >i>e )s )8x L)LeiwL) )eDeR@/LffYx11L>ffL.rw+ixx1LlLLq% U[ W r 1)`x1eL)e% )x1R @ )11d.L )eL 1x>L1LqffeL) rl)eO>)LL)R C1eL x1xL)Ri )RixLL))RT^)RWd) O)e D1 e T)i% 4DepxLT)YD1>L{ r)lerp pYwd)rUoeeffLLL ) R+11 ) exR %+)L> e1eRe1wL)wL)Ri )R O)e C e xLl)L ) %e L D[effLL Ux1eD.)R)LiLff)eWLL)ff e) RO[%V1q>De>Le oL)L)Ri )r>U1)LffLL) 21LY e) R#+1.l ei1e1LRliL1effi)ReL)e `1)LeLx Uxixxi D[e)x )p)LiLeUee2 1effLiLoeY1)R LULeeRWO x oe 1eU e e#1`iLe1)LeLwRD+xT1) e1^ L) ox x+ YLRxL% e +q)L liYeffe L V1 RWe 1eWYD1qL rl)eeD.V+@lL [g)l ff1qlRx)xi)R}1YL+i x1eeV+.x1R)/WeffLL D)fiff!"#%$&')(+*, .-0/21348-#913 ,fi:+;7=<-<+:?;2=#=>@;B;5*, 6-0/717A=72C 7DFEGEG H7DJEGEG ?#KML7#N .:8OFPfiQ%RHS?T#UVWYX#ZF[GUGR#\0V^]`_aSbZFZJUGRcd&egf0h)i?jkfilJmf0hnYo?prqYs3tsu'v%o3tGqYo3txw&y.zY{,|gt}6{Gvfiy2o#v7~3q6yAq'nYo#u43vq6y)pqay7{uFq6yAt?prq6ot{G##uFy6p{G93u'y6zYu'u'9|gtJ#p|~#|!u'y2v7{Gs?t3w&y2o#uvAt3w?{x|zY{Gv6rw#qYts#s#v7{tG2o&{GvtqpG?pM't?y4vAtG|u'?y{{G~#vYrt#G~3tGuG{G#utv4v6pr2o#u'v+y7o3ty2o3ty{x3qprw?u'v7uFw>tvpq4t3w>u'3{Hqt>AFxG?{Gv,Go3tGq6y7v6p)AFGGHA%nYo#uq6uF{G3wprqYy7{gq6~#xGuFq6y%y7o3tyaq~37otv2uFq6~?y%prq~#?pGu>y7{{G#yAtp&{Gv%y2o#u4~?^t#G~3tGuGnao#utGy+y7o3ty+zuo3tJGut9{G##uFy6p{G3u'y6zYu'u'|gtJ?p|@~#|u'y2v7{Gs?t3w>vAt3w?{x|8z{xv6rw#qpqqpG?pM't?yFb{Gv%{G#u,y7o?p#3bpy+tJ{zq%~3qy7{~#ypp'ugtJy7o#uy2{{rqy7o3ty%o3tJGu+u'u'w?u'Gu{Gs3uFw>{Gv{G|s#~#yp#|gtp|~#|u'y7v2{Gsu2pu'?y66q6u'u2{rw?|gt0YFx?Gt3wy7o#u~#v7y7o#u'vv2uu'v7u'3uFqy7o#u'v7upA?t3w,|gtJy7o?~3quFtxwy7{+u2pu'?ytJG{Gvpy2o#|gq{xv{G|s#~#y6p#@w?u'Gv7u'uFq^{M3upu0{Gvttv7Gu2rtGq7qY{#{zYuFw?Gu+3txq6uFq'tGw#wxpy6p{G03|&tJp|~#|u'y2v7{Gs?prqY#{z%9y2{@o3tJGu4|gt?gty2y7vAtGy6pGus#v7{Gsu'v7y6puFq&.xtF?#uFq'%J??A%~#vv7uFq6~?yq6o#{zq,y7o#uFq6u&s#v7{Gsu'v7y6puFqtv7u9q6o3tv2uFwy2o#ugvAt3w?{G|zY{Gv6rw#qts#s#v2{tG2opy2o#uw?{G|gtJpz%o#u'v7u>y7o#uFq6u9y6zY{ts#s#v2{tG2o#uFqtGv7u'uG3w?u'uFw%tGqq6o#{z%p.YtG'2o~3qu'y@tJ)FGbAy7o#uvAt3w?{G|zY{Gv6rw#q+ts#s#v7{?tG7oo3tGq+|gt>{%y7o#uFq6us#v7{xs3u'v7y6puFq{Gv+y7o#u~?Y#{G#~#3tv73^rt#G~3tGuG%y7o#u{xy7o#u'vo3t3w#t+?~#|3u'vfi{s#v2{Gs3u'v7ypuFq){0|gtp|~#|u'?y7v7{Gs?Gq~37ogtxq^pyAqfiw?u'su'3w?u'3u{Gy7o#ug2o#{pru{)rt#G~3tGu&t3w>pyAqp3t?ppy6y7{9o3t3wxug't~3q2tJfiv7uFtGq6{x?p#ts#s#v7{Gs#vpty7uG0o3tJGu3u'u'Iq6u'Gu'v7uv6py6pr2p'uFw`.^uFtv6JGG#,{w#q'|prw?yu'ytJFGx?A%{xyq6~#v7s#v6prqp#G,y7o#uFq6uv6py6pr2prq6|gqts#s?y7{vAt3w?{G|zY{Gv6rw#q,tGq+zYu9!wxpq2~3q7qp{G{y7o#uFq6u9v6py6pr2prq6|gq')t3wzo#u'y7o#u'vy7o#u'&v7uFtJq6o#{x~?w3u,Gpu'zYuFwtGqq6o#{Gv7yA{x|p#qY{y7o#u+vAt3w?{x|zY{Gv6rw#qY|u'y7o#{#w?prq3u'G{G3wy7o#uq7{Gsu{fiy7o?prqs3ts3u'vFy7o#u,py7u'v2uFq6y7uFwv7uFtxw?u'vq6o#{G~?rw{G3q6~?y.YtG'7o?~3q4u'y+tJfiFx3GuFy6p{GG{Gvt,|{Gv7u+y2o#{Gv7{G~#Gowxprq7~3q2qp{G{y7o#uFq6uprq7q6~#uFq4t3w9tGw#wxpy6p{G3tJv7uu'v7u'3uFq'u93upu'Guy7o3ty{G~#v{G3qu'v7typ{x3qv7u'?tvAwxp#y7o#u&p|pyAq{y2o#u{G##uFy6p{Gu'y6zu'u'y7o#uvAt3w?{x|zY{Gv6rw#q,|u'y7o#{#wt3w|&tJp|~#|u'y2v7{Gs?tv7utq{q.px?pM'tyFnYo#ux~#uFq6y6p{G{%o#{zzYprw?u|gtJ?p|~#|8u'?y7v7{Gs?ts#s?puFqpq+x~?py7u,p|s3{Gv2yAt?yFtJ?p|~#|u'y2v7{Gs?9o3tGq43u'u'?tJp?p#s#v7{G|Bp#u'3u&tGqt&|uFt3q%{w?uFtJp#>zYpy7o~#3u'v7yAtJp?y63{xy7o>p4t3w9{Gy7o#u'vtv7uFtGq'4%{zYu'Gu'vFy7o#uwxp9~?ypuFq{~3qp#y7o#u|u'y2o#{Hw{G3u>zu9|{xuy7{#{x#~#3tv7s#v7uFwxpr'ty7uFq9qu'u'|#{Gy&y7{o3tJGu3u'u'~?ts#s#v2uF2pty7uFw&v7u'y2v7{q6suFyF0y7o?prqpq#{Gy+y7o3ty+o3tvAwy7{9u#s?rtJp0^ptJ|{?q6ytJts#s?pr'ty6p{G3qYzo#u'v7u+|gtJ?p|@~#|`u'y2v7{Gs?o3tGqu'u'9~3q6uFw>.t3wBz%o#u'v7u4pyAqYts#s?pr'ty6p{G't&3u+uFq6y}6~3q6y6p3uFwpy7u'v2|gq%{fiy7o#uvAt3w?{G|zY{Gv6rw#q|u'y7o#{#wy7o#u,#{zYuFw?Gu3tGquprq4w?uFq7v6p3uFwpy7u'v7|gq{~#3tv7&s#v7uFwxpr'ty7uFq{GvFbuFG~?ptu'?y6G~#3tv7&~#3y6p{G3qzYpy7ot,3?py7uvAt#xuAfi#{xvu3t|s?uGbps#o?Hqpr'qts#s?pr'ty6p{G3qzYugtv7u,p?y7u'v7uFqy7uFwpq6~32os#v7uFwxpr'ty2uFq@txq+G~3t?y7~#|q6yAty7u&.qu'u.4u'??pGo4u'??pxo0FxG?7A5p|ptv6G%ts#s?pr'ty6p{G3q+t3w&u#s3u'v2yq6#q6y7u'|gqYy6?s?pr'tJ>~3q6u+{G?9~#3tv7s#v7uFwxpr'ty7uFqq~37otGq0q6?|s#y7{G|&q0t3wwxprq6uFtxq6uFq)7fio#u'uFq6u'|&t0JGG?Auq6~3q6suFyy2o3ty0y7o?prqpq^#{xyttG'2prw?u'?yF#t3wy2o3tyw?u'u's&s#v7{G?u'|gq)zYptv6prq6up&|{Gv2u%Gu'#u'vAtJ0'tGq6uFq')nYo?prqfis{q6uFq)t@2o3tJu'#Gu+y7{s#v7{Gs{G#u'?yAq){0|gtJ?p|~#|`u'y7v2{Gsqp3uG#u'xu'p{G#u+tG'u's#yAq)y7o#u|gtJ?p|~#|u'?y7v7{xss#vp32ps?uGy7o#uYwxpq2~3q7qp{Gt3{Gufiq~#GGuFq6yAqy7o3typy^|gtJqp|s?,3up3ts#s?pr't?uptYrtv2Gufi2rtGq7q{p?y7u'v7uFq6yp#u3t|s?uFq'fi% hfi5mdGffl7f0 i?mf0hd?MJ30fiAH&fi+G&H&A3 GJfffi7'?'7@37A?4A#'A#%9$ rM3 AGJ'&'A!"A#'?rG'fi()#*#+,.-0/1,#2#3+4)#*#+65ff)#7#89-:#-<;=*#>#2?,.@BACEDFGF!HIJLKNMPO#QBRSO#QTRVUWQXKYZK[UP\^]K_^`ffUWabBc!Med QaWf<gihkjmlnUWQobBcpKqrs\^]TbT_tK"csUud QaWfors_vbw\tck["bxc#Qcs\v["b_QaWfkyLJKpz!aMPUoaWK"c!bBfK]TbBaP\vbB{s_^KM|\}dNc#K[KMWM~bBaW`uMPQUWO!bBUib_}_]TbxaP\vbB{s_^KMir!MPKL\^cSgbBaWKk\vMPU\tc![U\yKy^c#QoURuQEqr!bBcsUP\}z!K"aM"\^c![~_tr!\^c#p#aWQQaWUP\^QcpK?#a~KMWMe\^Qc!M"sK"]K"au{s\^c!pUWO#KMWbBfiK]BbBaP\vbB{s_^KEMP`sfi{!Qx_yJLKc#K?UpUWabBc!MdQa~fg%\tcsUWQ1bBcKqrs\^]Tb_^K"csUoXdQaWfors_9bgVhjmnuuRO#K"aWKb%bBUod QaWfors_vb\vMpQc#KkR O#K"aWKc#Qqr!bBcsUP\}z!K"aM\tc![~_^r!\^c#V#aWQQaWUP\^QcVqr!bBcsUP\}z!K"aMiO!b]KR\tU~Os\tcUWO#K\^akMW[Q!Kb[Qc!MPUbBcUmQa]BbBaP\vbB{s_^KQUWO#K"amUWO!bBciU~O#K]BbBaP\vbB{s_^K MUWO#Kqr!bxcUP\}z!K"am\^UMPK_}d{s\tc!#MyQUWKUWO!bxU\^cUWOs\vMUWabxc!MedQaWfffbBUP\^QcpRuKEsQic#QUa~Kqrs\^aWKXUWO!bBUgo{!Ko[~_^QMPKGym_vMPQ!.Q{!MPK"aW]KU~O!bBUubBUWc#KM~Mm\^fis_}\tKMwUWO!bBUUWO#K"aWKEbxaWKc#Qic#KMPUWK%qr!bxcUP\}z!K"aMy}JLKsKz!c#KoUWO#KEU~abBc!Med QaWfffbxUP\^Qc{`p\^c!sr![UP\^QcLQckUWO#KoMPUWaWr![U~r#aWKEQBdg#yuO#K"aWKbxaWKEUWO#a~K"KEKbMP`MPUWK"!M6 dgE\vMbBcr#c!qr!bBcU\z!Kd QaWfors_vbM"sUWO#K"c%gXg#ygBigBXgBW gB^XffgBgsy__UWO!bBU aWK"fffb\^c!M\9MU~Qff[Qc!Me\vsK"aEqr!bBcU\z!Kd QaWfors_vbMuQBdUWO#KwdQa~fgB ^ gT ^ !QaogB gTv? UUWr#aWc!MmQr#UuUWO!bxUUWO#KEMWbxfiKUWabxc!MedQaWfffbBUP\^QcRQaW?M\^ckb_}_UWO#aWK"K["bMPKM"y=JK\}_}_^r!MPUWabxUWKUWO#KUWabBc!Med QaWfffbBU\tQci{`X_^QsQ\^c#ibBUmUWO#K["bMPK RO#K"aWKg\9MmQBdZUWO#KdQa~f^ g ^ ym`oUWO#Ku\^c!sr![UP\^]KOs`QUWO#KMe\vM"sRuK["bBcffbMWMPr#fiKUWO!bBUg \vMbBUy#QamUWO#K#r#aWQMPKMmQBdZUWOs\vMm#aWQsQBdRuKsKz!c#KNbpWx ud QaWfors_vbUWQX{!KbBcbBUWQfo\v[d QaWfors_vb\yKy9!Qc#KoQBdU~O#KEd QaWf\yKy^Qc#KuQBdGUWO#KmdQa~f yGK"UbBcs`ff]BbBaP\vbB{s_^KX\^cyGK"U_^KbBaP_^`nBW Z {!Kb_}_!{!bMe\v[MPr#{sd QaWfors_vbM=QBd!g UWO!bBUsQc#QUfiK"csUP\^Qc{Kbo]BbBaP\vbB{s_^KoQa[Qc!MUbBcsUMP`sf{QB_c#QU\tcUWO!bBUu\vM fiK"cU\tQc#Kp\^cgfr!MUQ?["[r#a\^cMPQfKE{!bMe\v[oMPr#{sd QaWfors_vbpQBdgBvGMWb`\^U\vMKbMP`kU~QMPK"KoUWO!bxU~b#aWQQaWUP\^Qc%d QaWfors_vb#QaNbpqr!bBcU\z!Kd QaWfors_vbnBW Gy["bBc#c#QUfiK"csUP\^QcbBcs`nB~ Gic#QUfK"cUP\^QcbBcs`cQUWO#K"aRuQa#M"c#QUQcs_^`sQZ ym`kU~O#K\^c!sr![UP\^]KpO`s!QU~O#KMe\vM"]TbBaP\vbB{s_^Ko\^cbBc!MPQ!{s`[Qc!MUWaWr![UP\^QcZ\^U\vM \^cUWO#K"`bT_9MQ[QcsUb\^cbT__Q#["[r#aWaWK"c![KMQBdUWO#KpQUWO#K"ao]BbBaP\vbB{s_^KMffbxc![Qc!MUbBcsUM"y]TbxaP\vbB{s_^Ko\^c0G{#r#UQUP\v[KUWO!bBUoUWOs\vMbBaWr#fiK"cUdb\}_vMw\dUWO#Ki_vbBc#r!bBKff[QcsUb\^c!MXbBc`Os\^O#bBa\tUP`#a~K\9["bxUWKM"=\^c![~_^r!\^c#Kqr!b_}\^U`y%#QaUWO#K"cgfo\^OsU\^c![~_^r!sKkMPr#{sdQaWfors_9bMQBduUWO#Kod QaWfQr#UM\9sKnmRu\^UWOQBRE_tK"UW QaEZR Os\9[~O1["bBcfo\}]TbBa\9bx{s_tKMU~O#QMPK\^cy}n W Z{!Kb_}_#UWO#KE"bBUWQfpMWuQB]K"aZRO#K"aWK\9M K\^UWO#K"aQan W yuO!bBU\9MBRK[Qc!M\9sK"abT__sd QaWfors_vbMQBR[Qc!Me\vsK"aUWO#Ko\vMr#c![U\tQcZln^ g ^uOs\vM\vMMPr#a~K_t`Kqrs\^]Tb_^K"csUU~Q^ g ^ G{!K["bBr!MPKpMPQfiKifor!MPU{!KUWaWr#KyE QBRK"]K"a#\}duRKibM~MPr#fiK\vMU~aWr#KZRuKi["bBcMe\^fis_}\}d`9 g ^ {`aWK"s_vb[~\^c#bT__uUWO#KoMPr#{sd QaWfors_vbM{`?mQaZ"""b["[Qa\tc#XUWQEP QU~KU~O!bBUUWOs\vM=\vMub_}_^QTRuKoQcs_t`o{K["bBr!MPKUWO#KsQEc#QUfiK"csUP\^Qc^ g ^ [Qc!Me\vsK"abx{s_t`ybBcs`]TbBa\9bx{s_tK \^c6GyuO#KuaWKMPrs_^U=\vM=UWO!bBU=RuK["bBciMe\^fis_}\d `oKb[~O|\vMPr#c![Un P xUWO#K"aWKR\_}_{KEc#Q [Qc!MPUbBcsUMcb[U#!{K"[xb!rPKBQQ#r#W"K]^\Q!rQ!{P"KWB]BbPU^\Qcxb!{Q#rUUWO!bBUEbp!bxaWUP\v[rs_vbBaQai]BbBaP\vbB{s_^KMQr#UMe\vsKV_^KdUpRu\^UWOs\^cUWO#K#a~Q!Qa~UP\^Qcqr!bBcsUP\}z!K"ayuOs\vMp[Qfis_^K"UWKMUWOs\vMiMPUWK"1QBdUWO#Kw\tc!sr![U\tQcZyEB\^c![KiU~O#KEQUWO#K"aqr!bBcU\z!K"aM["bBc%{!KoUWaWKbxUWKaWKMPrs_^UyMe\^fo\}_vbBaP_^`ZUWOs\vM#aWQB]KMUWO#KbBU~c#KMWMfi?s#u##Z=.e#BVW^ uW#Bff#Bfi Bx!Wxi WB#!#"%$&('()'!P*,+.-%/012 B"43G"5*768+.-:0;9 ! #% <=?>@^"pW8*=#^# ACBD^(E #WxB^ i@F$^ #GH"JI&"LKM> "B5#' W&' W^&'BP^* 6 Nm@Z #O QPSRTPVUXWY=Z #"W[P\P U BB~Z!' @JsB #&5u@> "M B:#=^^ ]L#' W&' W^ =" eV& ^=P U SJ'JB2#' W( @BZ#=t _t :#' W&' W^ ` Z#"WX#'#B#' W( Z@" ~!I@b"Sc WdP p B^ p^ BWZ B# =t _t Z#' W&!' W^ eB#5^J'JB&^ (E4BfP U ^gXZP U%hji #"AP ePk^P #B[ 5NlPmRTP U W N[@#W=BXJB W#&"on dB%#"Wk WW'& X#&'xPBT`VPVU hpi bFqD`VPRrPUsWtYFu#PVUev b"D$@J5XBJB&^ uB~'N= w%#ECBW &'xP TB,Bkx(#' W ^ %@#[ zyF`VPSRTP U WYb{uP U v "|##k?x W'4WW"^ W#]s#B5#=^^ e#' W&' W^ 2^ 1W"o@}xW&5' W&#' W^ tV" ffxCBTff#' ~&!' ~^ fi~J~ * U ~N~ u^ #5N* U NAs@^ B1 &5t x^ @2`YbF Z#' ~ =NxW e+jB8Y:+.!"m| C.^ #e 55*@Ug<=J>]^ "sW=#^` 0 `Ytb:u, %uw`YJObb^ Z#"~ & BTBW& ]> "xQ hz YJZ&"|S#P 5=#kB~e#X?B kxJJ> Bp#dP B_NWW xS=N^ >@B@^ e&Skt (E^s!' "s"s ^{P~N~ * U ~J~ hff~J~ `b~J~09n 5!' "_W'& " & ##' W=!' W_t kx(#' W {"d(#W#"iW&^gBB W"P U ^1kx(#B' W ^ 17#PRrP U W SXX!f##' ~ i71kx(#' W ^ ^SBTP SXX' ^J!> &"c k#x #^ ue5P Z'(#eX'(#W!P )"S 5&%^ #JE&sZ #"Mn e!W"^ W2i* W'(X##ECB^ !^ sx(L=gVBF&{' CJ^ &^C xJB5BW&5%( 5N=LBXkxP W"s5&SBW#ECxW g":c k?x^ #W BZ^ e*eM B^ &pkxCP W"sL( 52 :>#zCZ*@U#^ Z#"~*@Ue<=B_s!") W"2 5oZ w"^# P BsZxJB#5>]x@^ !"d" &^!* U dm7t x, &5^ B^ @m2`gbS e#' W =BW 2+4"e[EC^ {^#5* U<&J>@^ "XW=#^ @:BW=u@#e [{@LJ `b^P CO* U N<=?>@^ "XW {@LJ C[`gb"On oW'& CZ* U kB Nkx(#' W {"d!^ X?B&^ ud5P esS^&@#p2`ub eyL2`b% }+f"Z|&BgSn xw@EC]t %W'&55o* @#[ 2`b eyL2`b%CpB #d=#^ {@LJ `b"|#:!' "^JBWZP ~'[N=~> "W{*%^ ~}=#J> u#]& #"L| NP "sXJBd#^ (EC^ (EC}t sWfB#S #"c Wo B5^ ffus^ XS^ f 5&@u#5 yL2`bO^#e#' W>=^ &[P W'{#^ udsi#O> WsS^ f @#ku@#[ zyLY`bk"|#e!:P W'8~#%Bu5sff#[> `bxk^ #"5yFC[ `b e `b:&Z#hP&S @#7MP =id=#"uds#^ udJ'JB W]> x=#"f:Z:Mf5&Fg_!4F@V{QFFAw#!{Cte {@FkCg @8@ @:lkZ7@%&@7&kCg @d4 (4C={7ep+,-%/0 7 +T52Z \:@X@ 5`edb5hCtR1:@X@``Db ]!`Dbb`e2b:Rg`4b`Db+ BB=t(Eek xM`Qb h ^#u5PZ'xW^^#es&^@i(EX #eBW&F==^(EiWX #Z'#W&'W^M^8 ^B #" #sCPeB=JE()i"sS u #dPxM^k #[Nx(E&@E(H ~i#P ~^ kFJ'CP CB8e":!^X?B&^fiff ffkm| #PdXfi!#"$%&fi'(*)+,-./1023%4!5768:9;8:<>=?@;ABC?EDGF1HI8:<J=LKM@;<N(@;<JOMPQA<R7STPUS8VCKXW:R7=8N:YZK\[=?8:S8IR7ST8]R7<JP+<@C<^_A<R7SPQUST8VCK.W:R`=8NaKM<=?8G9;@W:R7bAJOXR7SPc8GHdANL=eWT?@f@fNg8RIVJ8:<@;=hR7=gKi@C<[_@;S=?8:HQjk AUUZ@fNL8m$l npoqm r7stfittsTmuGvhY2R7<VwOi8:=dxzy{n|m!y}x~[_@;Szn;stfittLsh*j?8+<JAHdb8:S@7[eUR7S=LKM^=LKM@;<N@`[=?8GVJ@;H+RfiKM<KM<f=T@+R7=@;HN2KXNd8R;WT?QNLAW?UR`S=LKM=LKM@;<QW@;HIUJOM8:=8OMPVJ8:=8:SHdKM<8Na=?87VJ8:<@;=hR`=LKM@;<[_@;Se=?8A<R7SPUS8VCKXW:R7=8N:jaE8GHdANL=GRfiOXNL@+NLUZ8WTK[_PQ=T?8]VJ8:<@;=hR`=LKM@;<Ne@`[=?8dW@;<NL=hR`<f=NLPJHdb@7OXN:j?8:S8GR7S8(R7=HI@fNL=x* ceRfiP4N2@`[WT?@J@fNKM<B=?8Ng8;je<=?8e@C=?8:S?R7<V YCc8ef<@7c=?8:S8KXNR7=2OM8R;NL=@;<8HI@VJ8Oo*s;Zl v@7[2DGFNLAWT?d=?R7=2ovnml Y;NL@G=T?8:S8=?8:S8eR7=Oi8RCNL=@;<8eW?@`K.W8;j2<[qR;W=Y;=T?8:S8KXNR7=OM8R;NL=@C<8ec@;SLOXVIQNLAWT?=?R7=o_s;Zl v nDGF[@CS8R;W?@7[=?8IceRfiP4N@7[UR7S=LKM=LKM@;<JKM<B=?8G8OM8:HI8:<J=hNa@7[=?8]VJ@;HRfiKM<EoR7<V8RCW?>NLAW?c@;SLOXV KXNK.Ng@;HI@;SU?JKXW=@vhjeKM<RfiO\OiPc8GH]ANg=WT?@f@JNL8G=?8]VJ8:<@C=hR7=LKM@;<@7[=?8<@C<^_A<R7SPUS8VCKXW:R7=8Njee@7c8:9;8:SYZmEl VJ@J8N<@;=W@;<Ng=ShRfiKM<>=?JKXNWT?@7KXW8+R7<V YbfP>R;NNLAHIU=gKi@C<Y<8KM=?8:SIVJ@f8NDGFIjd?8:S8[_@;S8d=?8I<JAHdb8:SG@7[rrNLAWT?WT?@7KXW8NKXNeNg@;HI8([_A<W=LKM@;<oxvca?JK.WT?KXNKM<VJ8:U8:<VJ8:<J=@7[m2l jE8W@C<WTOiAVJ8d=?R`=xmZl _oDGFvxdrfistfittsxuaI7:\7oLxEvxoxvxxdrsfitttTshxueq=aS8:H+RfiKM<Na=@d8NL=LKMH+R7=8xxnxdrsfittfitshxuexdrfixdfittfitqxu]k =LKMSLO\Ki<BZN{R`UUS@fifKMH+R7=gKi@C<[@;Sa=?8([}RCW=@;SLKXRfiOXN:Yca?JK.WT?NTRPN=?R`=@I@;b=hRfiKM<@;ASaS8NLAJOM=YZc8(ANL8EJn77Johoh7>vTvhtq=[@7O\OM@7c{N=?R`=e8fKXNL=GW@;<NL=R7<f=Nas]NgAW?=?R7=IG7JEe`J[_@;S{RfiO\OEj(NKM<B+=?8Ng8b@CA<VN:YR;Nc8O\OR;N=?8([}RCW=e=?R7={xuxuxQuyXrx*uxyX rxdrfixdfitttxud(xux*Yfc8GB;8:=fixuyXruxyXr Q@cY4W@C<NKXVJ8:S{=?884UST8NNKM@;<W@CHIHI@;<=@IbZ@;=?b@CA<VN:xuyX rnuXy r xnnnX_(_hLM{_7(.TLuxuXy r xxxyXra zu.}MyXr+;` X n e!"fffi# L_%$2_&fiT_eMq')(L:T+*!,.-0/1324 56 ,fiL_2_T{T_7fi_8,__9(%:7<; 5=?>fi@BADCFEGIHJLKMON?G?APQHSRUTVC MOM!G?AWYXZ?[\0]!^3_`\<ab]c\egYhifj ge<h%vg dfe8ghsutwvyx.z%{c|9}~c bsOt<kl9nLoBprqjBk g kml9nLoBprqqnaF^am^B\aXFXO^3<XOm<XOF\!WX._X0D\ ]c_F\\<Z<Zc?XLaX9Z <X93OmZYFZ\aF^uQ^3\^bX0FB\<Z`ab]!?X]_]!3\<X9_b]c\^3?X<X9<XOX9_\)]\^3Z?_yZc\<aXZc3\^3Z?_b] 0X Z ]c0.\<aF^X9_bf Xab]!?X\<aXZc3Z ^_qFX0b_F^3\^3Z?_QQ3OQyfX9\ %O eh+q^3^3\Zc6\<aXOXb]?0XO9wZ?n <]!?fqne8h.fX9\ [IX.\<aXqgO3D+ggn^3nqnnBVaXZc3Z ^3_\aX9Z?<X9XO\0]c[F^aXO]\^ a\0Z __XO0\^3Z?_`[IX9\ 9X X9_ ]c_bmqqF+!brb0 {| }r}fg~ fz% c Qqqn< {| }r})09 }0 }} f z% c fqqe8 h Z Z? X!fb0.]c<\ e ] h ^^3..XO ^]c\X \<aX9_baq\<ab]c\ e8 h \^.] Z\^3..XO ^n ]c\<X<Z? \<aX`FX0b_F^3\^3Z?_b \<ab]c\ e8 h b\.n <]\^e< F h Z e8 h% +{?} e< h VaX^_b3b^3Z?_ Q _Z Zc3Z 9qq_X` ^3<XO0\^Z _Zcb]c<\ e [ h Zc3Z ^3. XO ^]\<X03<Z n b]<\ e ] h LXO9]!\<ab]\q]c_b \<ab]c\%\aXIZc^3_\) ^3_m ]<XL^3^3\)LZc6]X ?X9_b0XZcQIZc^3_\) ^3_` n c^3_b0Xqqq^3ZXOfF^3\ Z 3Z L\ab]c\ 6nqqqZ?L\<aXZ?IZ^3\<X^3_b3b^3Z?_Qf\<aX?X9_X9)]!6\)]c\<X9? Zc \aX<ZZ6^ \<Z.aZ \<aXZcZ ^3_ffe ^ h ^. ^3X9_\3]!\<aX9_Z?]! \<aX9<X ^ Z?.XX ?X9_b0XmZcbZc^3_F\)q!O!+{?} e< h ba`\<ab]\OFZ?]! g yg \aX0ZFZ?n nnn^_b]\<XOLZc] <X]!Q^3_F\<X9?X9BF3\^3F3XOZcB g ]c_b ^cnnnBe ^^ h ^ +{ } e h ]c_b]!%^3\)0ZFZ?) ^3_b]c\XO]c<X^3_F\<X9?X9F3\^3F3XOZc g f\<aX9_qnLaF^XO]3` 0XO \<Z<Zc?X\<ab]c\qWX[bX9c^3_ ^\a \<aX<ZFZcZc e ^^ h aF^a ^ \<)]^ a\Z? ]c0f?bZFX\<aXIZc^3_\eggO!O< k gh ^^3_ +{ } e h YWYX`0Z _b\<<b0\] Zba\<ab]c\ e8h ]? Zc3Z 9.LaX.FX9_Z?\)]\^3Z?_YZcB]c\<Z?^ \<aX.X9\ZX03X9.X9_F\) OOn !]c_b`Z Z?_Q \B<X9]^_b \<ZaZFZX\<aXFX9_Z?\)]c\^3Z?_mZ ]c\Z?^ \<aXX9\!O!<\<aX`FX9_Z?\0]c\^3Z?_b Zc\<aX0Z?_b\)]c_F\) e ^3_b0X\<aXFX9_Z?\0]c\^3Z?_Zc\<aXm<XO ^9]c\<XO Zc]c^3\ <XO]c\<X9\<ab]c_ ^ ^3<<X03X9c]c_\ h .W ^3\<aZ?\ ZF<ZcL?X9_X9)]!^3\ X9]c_ ]?<.X ^ ^3_9]c_Z _F^9]%Z?<`e _Z?\O X`0Z _b^FX9 h LaFb9 ^] ^ 8_b0\^3Z?_ Zc 0Z _ 8_b0\^3Z?_buB<]!c^3_b0Xf{?} e< F h Xb\ab]!?X f{?} e h Z Z? X fmWYXbX\<ZFX0b_Xm\<aXe !h Z Z?.X]c\<Z?<Z?IX9<\^3XOZc\<aX0Z _b\)]c_F\)90Z?_F\)]!^3_b<b\aX9_ X] ?X<]\^fi! "!ff$# #!! 4%#(&,+!#*)#.431!3 18&:9 <;@A#'&#*)65=&290/213 1 473 1?> ;fiB CEDEFHGJILK,GEMENOFHPQCEDEFSR7CETEUVI WEIYXZDE[EM\GJ] ^_:`acbedgfhcfij fklffmonQhpqffrsf<rsdtqvuJaxwEkyr{ze|}6ijt~8~<%ivj7dgq EqffiEhqE`Eaz|}dtqfffjOrsq~QqEdj fdgcrdgq EqfhqfrdgqffrqEfiEhdtq~<fj?qf6 kfiEhq2hc7j thc~j frV~z.m_2`z.dt6~dthj lffrsfj?mj fdg2rsfiJ`0Ea$fe~<iEdtffqEd lh6%shj 2fij fQSgJ ~%j f<r~ph~2|}kEj q~<d~j frV~ph~e8a*bdtfhfij fQrsq,fiffr~dtq~<ffrdgqrsf8r~rsdgfj qfff6fij?f8h~<fj f%h:rf%i rsq0t$Z8E $ kj fiEhfij q~<frsq,fiEh7%sd~<Eh~<jthe {8 dgfiEh2r~<htkZfiEhd rsqfffcdgffVqEdtf6qEhh~~%j <r{sm~j?f<r~z.mZ8ffhqEd 4dtq~rffh8dtqgrsf<rsdtqr a*2iffr~r~Q~<EE<r~rsqE smgr{ffsfQfdcEd th fiEhE%dd zrqfftd sth~fh%iEqffrtEh~zdtjghlEjOrQthdthf%mta*eEZ<dtl2dtfflyh%hVj?f<rsthsm7hjt~m6r{z t$Z8E $ 2hhj qdtyhq~hfa6qffz.dt%fEqj fhsmtkyrsfQr~QqEdtfaeqfiEhcdtfiEhQij qkyrsf82dtfflhijOthh~~hqf<rjO{sm{rghj qdthq~<hfr{z hcdtffhffjthcfiEh6dEEhqh~Qd zrsq8$ lffm8a2$fQfEq~dtEfQfij fkzdtedtEeEE%d~<h~:iEhhtkf%iffrV~:hffjthhqfffer~d~~rlffshtahfZ*%8E lhf%iEho~j hojt~8$E hhEf7f%ij f7hghm0EqEqEhj fh dtq <Eqf7d zfiEhz.dg$c `. r~6%hffVjghSlm, `. abdtf<rhfij?f6fiffr~r~ch~~<hqfff<rjO{smvfiEhdtEd~rf%hfj?q~z.dg7j f<rsdtqcfd8f%iEhedtqEhe~<heiEhqffhpqffrsqEch~~<hqfff<rjOd~rsf<rsgrsfmrqnQhpqffrsf<rsdtqaa 8 rsqjO{mtkshfc {8 lh Htx 8$ $ af8fEq~QdtEfQfij fOkEzdt8jO{~<ff%rshqfm~<jO{6 kZ {8 *{8 a2iffr~8h~<ffsfkeiffrihj lyh:jt~ h7jQaxEk:r{2lh7~<fj fhj qEd thj fhOa dtqEd 42h8~<h8fiEhQsh7jfddtqfff<rsqEhfiEh6E%dd zZd zf%iEh87jOrsqh~fffOadtq~rffh6~<dthS, {8 a8$f6~<ffh~Qfd~iEdfij fQzdt8jO{LfiEhhchr~<f~6~<%ifij fz.dgjO{"6 ktf%iEhhhffr~<f~2j8yd rsqfQy t<xZZ8E $ ~<%ifij?fjO{fiEh8ddtgrqj?fh~z j hrsqfhgh*ffsf<rsffsh~d z =j qc~<%if%ij f: c ff0ffa* dtZfiEhqchej qcfj?th~<jO{hj q~<7jO{sheffx~Zfd%hj fhj~<htEhqh dtqgh rsqE6fdc 8 hqhtk\hfe0Ea*m h7jcQaxEk2hj qpqc~<dthe Htx 8$ $ ~ifij fe ff0gEaZ m6ffhpqffrsf<rsdtqkffhth%mdtq <Eqfrsqo %8E r~d zf%iEhQz.dt%( Z8 * Ek Z8 0EkEg ` Z8 kEdteg6 ` 8 kJeiEhhOr~8jyd~rsf<rsthyd smqEdtrVjactqEdthz.dtQfiEhcdthqf8fiEhdtq~fjOrsqfff~Qd z fiEhczdt'O8 e Ekj qdtq~rVffhfiEh8h7jOrsqffrsqEdtq~<fjrqfff~fij f ~j frV~ph~a*2iEh~<h6dtq~<f%jOrsqf~jrqfftd sth6~f<rfrsqEhtjrsf<rsh~kZj qf%iEh8zEqf<rsdtq~rsqtd?ghj?q j?hdtqfff<rsqffEdt~ae2iff~kEfiEh%h6hffrV~f~~dthO~<ifij f2zdtj zdteeiffr%i y0OkEfiEh~<hcdtq~<fjrqfff~Qj h6jO~<d~j frV~phlffmbd,dtq~rffh jQdtq <Eqf*d z\f%iEh zdt Z fij fr~*~j?f<r~phlmc rsqh r~Zd~rsf<rsthtkgfiffr~ij Eyhq~rzej qdgqffmr{z2fiEh8zd {sd:rqEdgqgrfrdgqoiEd?VE~z.dtQhthmdffdtgrsqj f%h ` fij?fjtfjO{smj Eyhj ~Qrsqv k2hijOthc ` Eacqj f<rffj kr{z, j q ijghf%iEh~%j h7dffdtgrsqj fh~2rsfijEhEkfiEhqvOsZ Q Jac$f8zd {sd ~8fij?fQz.dt6jS kr{z ZYOj?q j q ijOthcfiEh~j h6ddtgrqj?fh~22rsfijOsEhJkEfiEhq jO~<d~%j f<r~ph~eZ*%8EhoqEd dtq~ff fij?f~j f<r~ph~fiEhhgffrshhqfff~a hflhSfiEhrsqffhLd zfij fdtydtqEhqfff zZ 2rsfifiEhj th~f*jOsEhta heffhpqEhy0lffmdtq~rffh<rsqEhjgicd zyrf~ dtydtqEhqfff~` kgz.dg68,20S`j q ` 0` `` } ` } }$fQr~hjt~<mfdthrzmf%ij ff%iEhdgdtqEhqfff~d ze ~<E'fdta 2fiEhdtydtqEhqf~rqS kdgfiEhfij qfiEh8 fikj hQrsqhjg~<hlmj fd~<f6?va22iEhcdtdgqEhqfQ ` r~ffhhjg~<hlmj fd~<fSa hQ2r{{~<iEd 0f%ij fe ijt~fiEhQ<rstif2Edghf<rsh~zdtejO{0 kiEhh r~2~<%ifij f4rq$E` E tSa62iEhz$jtf8fij f84J` tj?j qfffhh~Qfij f8 rV~rsq zdtjO{0a2iEhQzjgfefij fet"t 6gj j qfffhh~fij fQ r~e:rf%iffrqgd z kj?qiEhqh2rsfiffrsq,d zeZ rqhg YOkrsf8zd {sd ~Qfij f7 ZYa rsqh r~6dtq~<f%fhfifffi!#"%$&'(()*,+.-/0&& (1325476 8(9:9<;76(=<1?>A@1B49DC36FE:9HG5=<2I2(J<KMLON76P139CQ6(CST!R UV,WJ39oMXLAJ39Kqprs2(N7KMLA1?LA2(N{ 1vN2 WJB9uED6 LAN7CQ132}|J32F8(9D~q9uE:EH6pc V,W4LANI1B9K96PJ?>zLO9uJ V134LCJ39oMXLAJ39C1322F>C|J39C?9uN1t6F1wLO2MNLANL x9=<2MN7=B>OX7K951B476F1QT!R Y[Z]\_^(`?acbedgf3hiQa jR ffk lmknVf L ltV 6FN7KH49uN7=<9v1349S9uNI1wLOJB91349u2(J39uELA1%LCK9<7N76F>A9LAN1B49>6PN(X76F(92 xx 2(J3EQX>6DRR lf Lmp9(p VFf Tg6MCX7C?9KLAN1349H|J322 x# X7Cw1PLO8M9uN*pCW6(CCup9Wx J32(E6 >A(9uJt6 L=QM9u2(E:9u13J3@(p9v76MC?9Q2(XJK9<7NLA1?LA2(N7C2(N1349f s2=34N76P V r2C?139 V]67JtC?13ff2(J<K9uJ4L=B4V N2 Wyx 2F>z>A2 W2F@ V((l pqC?X7C?9u1#2 x LC#C36LK132!9uuQ ` <3FffJ396 >Am=B>A2IC?9K79<>KCup/476F1sLC V LCC?9uEQLAm6 >A(9uJt6L=L x\ ` ^F}uQ:#!HA,vR Flf C?9u9x J39u9S86FJ?L6F>A9CS6FJ391349uJB9LCf?7 ?tlW!F ?tW 42IC?9v2MN>O@}N2(N3>A2(FL=u6 >gC?@EQ72F>C%6FJ39G V7MVvV,Vq6FN7K5 V C?X7=B4H13476F1 fmT B3Tl LzfnT 33TlZ Spx XN7=<1wLO2MNF V(W 49uJB96FN7KV LCCB6 LK:132!9Cw9uEQLAm6 >A(9uJt6 L=L x LA1tC#MJt6F|4R lf Tq42IC?96PN7KLCQC?9uELOm6>OM9uJt6 L=Fp/49DED6LON1322F>9:X7C?9LCv1B49Wx 2P>>A2 WLAN#tFf s2=34N76PD9u16>p V*(V |*p7F l3l\ ^ u:I(sHuuQ? ` F<3FffQ 5^F ^ <uuQ? ` F<3Fff< ^FG uk_RZ<I(f G l:*RI3:u u_fn3lZ^``%Zf G ukXJ7JtC?1SX7C?9D2 x1349}rXJ38(9(9<>O9=<1wLO2MN~q9uE:ED6LCLAN1349x 2F>z>O2 WLAN VW4L=34C36 @C1B476F1 V LAN6=<9uJ31t6LONQC?9uN7C?9 V C?9uELOm6>OM9uJt6 L=x XN7=<1?LA2(N7C,79u476 8(9<NL=<9<>O@N96FJ>zLOELO1<Cup%49s1@|79s2 x |49uN2(E:9uN2MN9L?C}4315268F2LKLCzLz>>7X?C31JP6319K}I@ CnLANWWW 4L=B45LC=<2(N1?LANX2(X7C6P1G V X1476(C/LAN7NLA139<>A@EH6FNI@>A2=u6>gED6 ;LAED6:6FN7KEQLANLOEH6:N96FJGpH% \ ( ^ }I( G uk 5 ^F ^ tuuQ ` F<3Ffft ^P<I( fmT*lGG q f G lG!*Iw5 ^ :ffGtuIMutff ` fi w<F<fftF `*a Gkv q<(X||!2IC?9 V @ W 6@Q2 x =<2(NI1BJt6(KML=<1?LA2(N V 13476F1CB6F1?LCn79Cs13494I@|72(1B49C?9Cs2 x 1349|JB2(|72ICLO1wLO2MNX11349uJB9LC/N2C?X7=B413476P1LCLON7=<JB96(CnLANHLAN51349LANI139uJB86 > Gk pg9vK9<7N96S|72FLAN1 LAN G k132!9B L x*x 2(JC?2ME:9 U Za G 3T*lqW 9476 8(9 fmT U lfmT*l p,~q9u1]!91349C?9u12 x 6 >z>_134976(KQ|!2FLANI1<CupFLAN7=<9:LCC?9uELOm6>OM9uJt6 L=QC?2SLC V CnLAN7=<9 U Z Lzv q,_FLAN7=<9 Vs9=u6FX7C?9S2 xG uk1B49=<2(N1?LANIXLA1?@52 x@6(C3CwXE:|1?LA2(N2F8(9uJ1349QJt6FN(9VfmT*lfmT U l3l3ltV }LCN2(1LAN7=<J396(CLON}LAN6FN@5LAN139uJ38F6 >6FJ3LA13J<6FJ?Lz>A@=B>A2IC?9Q1325GD6PN7KCw2G1349K9<7NLA1?LA2(N2 xU Tl@6(C3CwXE:|1?LA2(NC?9uEQLAm6 >A(9uJ<6 L==<XJB8(95fU fBf G9:=u6FN7N7K76(KkmV!W|72FLAN1tCSps@51349HrXJ38(9(9<>O9=<1wLO2MN~q9uE:ED6 V 1349uJ39LC65=<2(N1?LANX2(X7CZC?X7=341B476F1:V 1349vJ<6FN(92 xffZV!f G Fk ]vp%FLAN7=<9QGV LA1f G lV Lmp9(p Vx 2F>z>A2 WG6FN7K9=u6 >z> V 1349J39C?X>A1WZf G uk pG 1349uJ39 x 2(J39fl GpFLAN7=<9!}LCS65=<2(N1?LANIX2(X7C x XN7=<1wLO2MN VG $Fk pr2MN7CnLK9uJv1349QEQLANLAEQXE|72PLON1LAN1B49vLAN139uJ38F6 >|J32F8(9~q9uE:ED6Hpcpx 2(JD6>>G Fk!x 2MJC?2(E:9f<Fl%}2(J39|JB9=BLCw9<>O@ V >A9u1 79Q1349LAN7EQXE2 x 1349Cw9u1 UfmT*l ("qp%FLAN7=<9" G W 92M1t6 LAN13476P1 T) G:6PN7K51349uJ39 x 2(J39 T}ZE:96FN7C13476F11349uJ39LC6:|!2FLANI1 U Tx 2(J W 4L=34* fmT U l+fmT*l<VW"56FN7K pfm3lDZ:Za G k p#@GQ6FN7K5C?2 VLA16(=B4LA9u8(9Cv6HEH6 ;ILAEQXE#"Gf<a G kffltV LCC1B476F16(=B4LA9u8(9Kqp9v=u6FN5N2 WaG$49uJ39Q134LCED6 ;LAEQXEWZa G Fk&Sp%4X7C VTfnT U l'" 7 =B>A96FJw>O@LCLC76(KqpsX113476F14L=34=<2(N13Jt6MKML=<1<C1349Q=342PL=<9Q2 x9N9u9KHLC6(Cx 2F>z>A2 WCup,-/.021436587:9<;=1?>@+ACBD>CEF. GH,JILKLMON<GP>QRACBSTQDB>@LAC1 BR7VU?AWQRACXSZYR>CET[LACU=\>JST9VSZ]^STBSTQD_`Aa?1P>$U:ETAU:bc>d:eP>$1:B6SZaPAC\6YF]F\RACAW]F7O\RXe:Ef>ST1gBR;=AWEf>$1=[LeP>$[OA7$]h\RA>JEh9<ET7LQRAC_4aPA<ET_=QC.Dij7$k8AClLAC\CGP>Qm7OU`QRAn\RlLAC_4ST1365D7:9;=1?>@4ACBj>JEF.TG^,JILKOMLN<G?Q6ST1=9CABR;`AVBR;=AC7O\Rbo7]\RA>CE9<ET7OQRAC_4aPA<ET_`Q>_`XSTBRQWA<EZSTXST1P>$B6ST7O17$]d:eP>$1:B6SZaPAC\RQ36p>\RQR@LSFG^,IOq/,JN<G?BR;=AB2kD7_=A<aP1/STB6ST7O1`Qr>$\RAVAnd:e/STlO>JETAn1:BC.s:tVfiuvwx8y{z}|~y`x84vwxvzzWwhy{?Vff684?H6DLO(//8R O!H6D Vj g4D + gg`DLVF=?CF W g * g < ? ) :LC*$F< <2^: g~:? ^2:`g =^ `F: g! <2^:=26CL gC O?LF :?nLJ '$?!2 26CLO&F g$F=?CFC^L=& C ?TW R< W H RRW $C! +` j $OH2LC ? FO?CFCPfg $:L^2CO+ !H R +^2$ff:C O?=R J: H RF 24 +F*FOg$ C g* 4 ff` `?g 2g:C?HRLR H R:42o H C*?<2^:$ C ? C FL*F H ?C ?O?$= RR<fffi fi fi fiH W R 2+C ?L?=&R HVF=?CF 2C:`H L`2 (HC C :)? FLF ? FOC(J`F:OL =L Lff FL 2g ^=C C: !#"%$'&($ +:*) 2g!^ :F:OL P= F :?*C=+ ? -, LF:OJOH $: $F^?4^2C: FOoH4?H:L^2?C ^4+ C/. OJ :o2 { ^ $:42 Ffi 01 fi F 2 4 LF=LC :! :2Vg ^CF VC:`H O`2cLJ3 fi4 '5 ! "%$'&($ C$? 3 ^ V fi 01 fi 3 76 6 8 fi94FL 3 oC: `H O`2 F2+C: `H L`2o= FO ^ OF?= : :! 4<;:424 !>= C$? 3 2 C$C2LCFffL$=H<F?!F*=H WL:. OC() ^ OF?$= ? :3! P fi = 1 C!? = + fi fi LA+@ <2^:! C$O`FLo W R? !& CB 0 ED B 0 =C COP^O?`c?J8. = ?)$: `F J ?=`FCF= ( CF^^Fn ^ 2`2(gB FB :$ L^2CO !G( C$? +` J*?F G P ?C27. =o=C CO`F^O@o ($ ^CF^FC 3 $:L^JO FH= 8 fi = C^$:^CF}$ C =C$ $?g 3 = fi fi :fiIG OH FL 3 = fi fi ?(F W? K J H Rm + $F^ $ !C F?gV^2mF < F r W C ? H j CL`FOV =?CF?R:? + c?*B 0 L B 0 { B 0 ED B 0 L *B 0 ED B 0+:B 2 <FC F^ 2` H L 3 = *^M ? H TW ^ RC$O`FL r $NB 0 gO ?PB 0 ?2^:J:PF R F RO?!:L +?C27. = ! : FL Q G H 2Ln !Fff ?L-HF F=! CL`FO +& CB 0 ED B 0 ?`2 ?J8. = PR F`<2^:! CO`F^ 4 SB 0 ED B 0 Lr = F?C$O`Fc W R2AB 0 B 0 OU FL* C27. =g^2 : CO`F^ *^M ?AB B=:`rgr ? =)? 3 2 L$=H<F? ? : fi = F. OC>) ^ OF?$=?$C2L^ ED V ^E. ^FC :? ^2:`W B $? $F^B B B L W :L CL`FO $^oC ?$`2 $?C27. = * r ^$F^= ^ +? + H g $^:C F? * $OH2LC ~? P F?g=J^?/.`F=H^ff ! ? :YX 4 XZ\[^]`_Aa2bcEd9egfh9ai2cEdkjl/mn9cEeaobcEp q-cEd9adkmerfd>mn9asfdkt-dkfmavuEa8bmcEiVx w mnzy{mKy{bm|zy}j<jlAy\qqza\y{ifd*~E ?gxEw ]9fi(?'2v?9?s%9* ` rr -Fg2EI^^ UE/ g/kg-2?HgI?z9/{Ug*?-g2g-\Ekko^ g\E`g(? >V -U9: /{*-`UE^k9vI -g`gU*kQ-g`*:ff-/Q-vU>PE?K}zo1-z(> 22 > >2 %> 1 o-ok-k- # >A>*E Hff-?! 9" ff- #$ F%ff #P &ffv'fi ffz k ff k(ffk k) *-+ , k *./0{$ 1ff $ 2) 34059 6 8796ff86: A;05) (ff` ffz <ff= -<7 > #??ff8( ff7@ 2 > ?A- V 10{: E! 2BC/ ff05$ - } 0E7 > #? 6ff8*ff #%k #?-F<ffHGJILKMN &6&k) 2D-+* ) I#" PQ Q)- 2 87v D,5R 6k 05-NOSUTVNXW TYN 06Zff @ ) 340{$ kO }! 2> ff DG ff [ 2H -) \ ^] >V 7*Pff* k) ) 2Eff8( : *NOS'ff- # /< ffv$ 1ff > -) A\ '] >/ 7v$ \\`_ff-#Dk) ) 2Eff8:! 1ff N WQf- 05-* ff05& 0{$ : 1#B+Dk) ) 2b ff ff Bc05`7> #) } ff05 >Ve? 05$ U(ffg9" ^ff fi ffhR*?i[jffz Uffk05l:! EN `f- 05- ff05-#))9*0{ #)mff-) 2 -) fi7v$ `k) 2nN 9 -ff -ffz# B7U`: ffh[ENOSYToNp?2Z kiqBr]* >> (? >V Es B @ ff-) 2tGnBe kO 2- ff-) 2 C -O* ff8# N)Sr] >/b ff B$ NW u[jffz 1'>) [8ff NOS6 N @ 5Rvff-! N)S,wmN x ff- #$yD uff-) 2FC -O % " +?2fiu05O $ z2D-c kO 2? 05$ e B <5R I(: u{"|U}06Zff8/s ff(\ 7v$ <\ ~ T{)B~7 ff1[k \ u N W ?10ff-`: < kZ7fi0ff-e B 205`7> #) -ff }fiff$ {>o 1059\6 ff 2Bff\? <6ff8 \ v TU{1 $ 91\ ?2EA 105 #ff -e kaBh :+ ff/ 1 k 05` -) I2\ ]* >> 0,ffs (> \.Hff > 0E> ff B@ G > ff <k7ffh[F \ ) TU{)B` 6ff8* \ |`NW?B? 8[ ff~05 $ "A )- B)7>`: &kff$FF05 0{ #),ff8%@ ff= GnB(+o-Q- >1+ o-k- \ >A+ z G x8 G {z = z G xj G \z6< ff- #Bo-k \ >Ao-k >18+? C$6/ ] >/e P_ * G GGwUF G G zEs <6/ ffk05$E-2X#)EH- > 7H06tff <I7>#)* ff8vGwUF G z G G z GwUF * $ 5 G6 GG zf-05 Gw9F 8 G G < ffC -$ 2 *> ff2mGnB# ff05$k #? E}Eff Gi 87V:> ff9}!#s-!87V1fi+pv`hpnfi+% Xv8'+*C+sa-kcEeQ=C)j~uh p11e- $em5()*O<Y(@k>qc8(sj*8u1e%6*OHhe81,O,5*8p**+ <z8,Oq,*<fiffe)8< D8,>v( ssq5k>?s85 j 1s85cO( ssq-5k> ,8t:Xp'8!j('81,O,5<8>+" " <?>#cs$,<"8h>,) p5CjX8p>mfiuh&5% ')(+*, -/.0123415!68759;:=<>3?@(A7:B(C59EDF3G=GBH/IKJ$:=L&3!:K:=L&3!:M:=L/JN<OJP(AGPGB5IMJGBJRQ@H/JN9&7J5!6:O5!SCJN<>3!9&7JUTJR7:=5<>GV; W DYX[Z]\^_/^R`4`N`aDb:=L&3!:M759;TJN<=cJRGd:=5 / De3!9&?V6f5@<gJR3@7=LhXi3jIk3l*(CIfiH/IMmJN9;:=<=5n;1on&5p(q9;:Ur W 5pu6 "N <GBH&7OL:OL&3!:d6f5<M34S+SXoDr W (AGM3!:#SCJR3Gt:k M3423R1u6f<O5I8.hv!(C9&7J:=L/JGBn&37Jowfixy(AGU75IMn&37:4D2zJ{7N3!9|3G=GBH/IKJ$2z(C:=L/5H/:USC5GOGM5!68c@JN9/JN<>34S+(C:}1~:=L&3!:U:=L;(AGPGBJRQ@H/JN9&7J759;TJN<=cJRG:=5oGB5IMJUn&5p(q9;:Mr .zJR7N34S+S:=L&3p:gb <fiF(AGfi3P&9;(C:=J$75@Ig;(C9&3!:t(q5@9|H&G(C9/cN3!9&?/3!9&?5<;85!6g759&Gt:=<>34(C9;:>GND2L/JN<=J$JNTJN<O1GBH&7OL759&GB:=<>3l(q9;:K(GU5p68:=L/Jj6f5<OI4Ab $Z/D4 {/D@ |NN b >D&5@<@b ~NR b >D&GBH&7OL:=L&3!: (AG#3fin5G(C:B(CTJMn5!SC19/5@Ifi(A34S.8v!(C9&7JM:OL/J85!TJN<>34S+S9;H/IgJN<5!6e759&GB:=<34(C9:>GY(AG&9;(q:OJ82zJ87N3!9$3G=GtH/IMJD/3!c34(C9P2z(C:=L/5H/:Sq5;G=G5p6cJN9/JN<>34S+(C:B1D&:OL&3!:34S+S:=L/JWfi GGO3!:B(AG61Pn/<=JR7O(AGBJSC1{:OL/JGO3!IMJg75@9&GB:=<>34(C9;:>GN.zoJM7OSA34(CIy:=L&3!:F:=L/Jfi75<=<=JRGBn59&?@(C9/c$759!BH/9&7:>G(C9r/b < ! 3!<=J8G=3p:B(AG&JR?U1Ur .e'5<3g75@9!}H/9&7:5p6:=L/J6f5<OI b Z|fi9/5:=JF:=L&3!:RDp(+6Y Br W Z65<34S+S/XoD!:=L/JN98:=L;(AG34SAGB5FL/5!SA?/G3!:E:OL/JS+(CIfi(C:RDGB5F:=L&3!:bBr Z/.)759!BH/9&7:5!6:OL/Je65<=I b e:=<>3p9&GSA3!:=JRG(q9;:=5$ K(C9o < 4 GBH&7OLo759!BH/9&7:>G#3!<=Jfi:=<B(CT@(A34S+Sq1oG=3p:B(AG&JR?j13!91Pn5!(C9:(C9w x .P63759!BH/9&7:fi5!6:=L/JK6f5<OI@ MN4 F(AGfiG=3!:B(AG&JR?V6f5@<g34S+Sr W 3!9&? W Db:=L/JN93!::=L/JKS+(qI(q:fi2zJUL&3RT@Jk@}r gD2L;(7OL(Gdn/<=JR7O(AGBJSC1h:=L/JP75<=<=JRGBn59&?@(C9/co759!}H/9&7:d(C9 < 4 >.')(C9&34S+SC1D65<3U759!}H/9&7:F5!6e:=L/J6f5@<=I@b ~ b >D;(+6Br W W Br W 6f5@<34S+SXoD:=L/JN9o3!::=L/JKS+(qI(q:fi2zJUL&3RT@Jk@}r g/DE2L;(7OL3!c3l(q9V(Gd:=L/JP75<=<OJRGBn&59&?@(C9/co759!BH/9&7:d(q9 < 4 .k:65!S+Sq5!2G:=L&3!:Fr (G(CZ9 ! <#.01o3G=GtH/IMn/:B(C59EDe34S+Szn&5!(C9;:>Gfir W 3!<=Jk3p:#SCJR3GB:M fi342341u6f<=5@I d.$JN9&7JDer 7N3p9/9/5:fi&JK(q99 d.6F2JKSCJN:fio<=JNn/<=JRGBJN9;:fi:=L/JUJN9:=<O5n15!6F:=L/Jkn5!(C9:G#(C9 dDeG(C9&7mJ (G:=L/JGtJN:85!63lSSIk3l*(CIfiH/IMmJN9;:=<=5n;1Mn&5p(q9;:>G(CZ9 <#D;(C:z6f5pSSC5!2Gz:OL&3!:Br e.zeL/5;5GBJ#;$3!9&?U/GBH&7OL$:OL&3!:oBr;-y.v!(C9&7Jo:=L/JJN9:O<=5n;1h6fH/9&7:B(C59~(AGP759;:B(C9H/5@H&GND2zJ9/5!2:=L&3!:K6f5@<GBH;P7O(qJN9;:BSC1SA3!<=cJXoDBr; W .v!(C9&7J~r/W (Gj3Ik3l*(CIfiH/IMmJN9;:=<=5n;1n5!(C9:j5!fi6 " <D(C:P65!S+SC5!2G:=L&3!::=L/JfiJN9:=<O5n1P37OL;(CJNTJR?u(q9:=L;(AG#GBn&3@7J#65<GtH;7O(CJN9:BSC1uS3p<=cJ8X(AG#3!:IK5GB:;E.oJfi?;JN<B(CTJM3759;:=<>3?@(A7:B(C59fi;1gGBL/5!2z(C9/c#:=L&3p:E65<GBH;7O(CJN9:tSq1dSA3!<=c@JXoD4:=L/JN<=J(AGeGt5IMJen5!(C9:b(C9Pb < W2z(C:=LJN9;:=<=5n;1$3!:SqJR3@GB:#.8zL/JM3!<OcH/IMJN9;:(AG#3G6f5pSSC5!2GN.dJN:P JGt5IMJ8n5!(C9:(C9 d.fiv!(C9&7Jh(AG#3fiIk34*;(CIfiH/IMmJN9:O<=5n;1Pn&5!(C9;:5!*6 l <D:=L/JN<=Jfi3!<=J8n5!(C9:Gz(C9hpb < ! 3!<=;(C:=<>3p<B(+Sq1$7OSC5GtJ:=5~ .U9hn&3!<=:B(A7H;SA3!<RDb:=L/JN<=JK(AG8GB5@IMJMn&5p(q9;:Mr z pk b6 < R2L/5GBJKJN9:=<O5n1j(AG83p:#SCJR3GB:d.#Gfi2zJ$9/5!2GBL/5!28D:OL;(GKn&5p(q9;:K(GU34SAGB5(C9pk b6 < e65<k3lSS#GBH;7O(CJN9:tSq1|GBIk34S+SP .~Fc34(C9ED759&G(A?;JN<M34S+S:OL/Jk759!BH/9&7:>GF(C9b < ! G=3!:t(G}&JR?1hr 3!9&?:=L/Jk75@<=<=JRGBn59&?@(C9/cV75@9!}H/9&7:>G(C9b < >.e59!BH/9&7:>G5!6e:=L/Jd6f5@<=I ZU3!9&?$ b (C9b < ! e<OJNIk34(C9H/9&7=L&3!9/c@JR?(C9b < >.|e5@9!}H/9&7:>GK5!6#:=L/JP65<=Ib $ b #(C9b < g3!<=J7JN<=:>34(C9;SC1G=3!:t(G}&JR?;1or DG(C9&7JU:=L/Jk75<=<OJRGBn&59&?@(C9/co759!BH/9&7:#(C9b < ! >D9&3!IKJSq1hb 8/D(AG8GO3!:B(AG&JR?1rGB5j:=L&3!:M@}r go Br fi<OJR7N34S+S:OL&3!:g (AGfi3$n5G(C:B(CTJUn&5!SC1;9/5Ifi(A34S>.h')(q9&3lSSC1Dz759&G(A?;JN<M3759!BH/9&7:z(C9$b < 5!6E:=L/JF65<=I@b e b .L/J875<=<=JRGBn59&?@(C9/ck759!BH/9&7:z(C9$b < !(AGfib g.$vH/n/n5GBJ$@Br 8Z/.v!(C9&7JP:=L/JMT!34SCH/JP5!6 (G&5H/9&?;JR?h5!TJN<d:=L/Jk75IKn&37:GBn&37J#w x D@(q:65!S+SC5l2FG:=L&3!:Y6f5@<3lSSEGBH;7O(CJN9;:BSC1GBIU34S+S Br ;.zL;H&GND/@}r e }r 65<34S+SGBH;7O(CJN9;:BSC1$GBIk34S+S lD/3G<=JRQ@H;(q<OJR?.e:e65!S+SC5!2Gz:=L&3!:r (G(q9b < 6f5@<34S+SGBH;P7O(qJN9;:BSC1GBIk3lSS# 3!9&?D(C9{n&3p<=:B(A7H;SA3!<RD(q9h< ; W 65<34S+SeGBH;7O(CJN9;:BSC1PSA3!<=cJdXo.0H/:#}r ~D2L/JN<=JR3@GF2JMGBL/5!2zJR?j:=L&3!:F:=L/JfiIk34*;(CIgH/IyJN9;:=<=5@n137=L;(CJNTJR?{(q9 " <#)(Gd3!:FIM5GB:# .Rfi;/z/R/EbR4z;A$@=@ABC|/=!RMO&!k@/$=B/M/tq@4AB#BO&!P=/o&OC&C!fi=//=CBCj/RR==!B+C$/p/fi;4&bCE-&4 fffifiFN!"$#&%E')* ( +-,.fi/.012344'5367fi $8fi91:;<1 4=<fifi;> 01=?k/fi01fi50A@CFBD E "HG0IKJML N;O2<P (QSRT9'CPV( U 861:;.0K+CXW UZY +Ct/\[B+CC;<]^ :;_`+q W\d BD eefff3"HUhg C; J L lm NnO <P ([B/ J L lom N;O <P (qpsraCbc<Bi jkBi jFk4 vu w Nx Uzy! x AOCR!tq( |~ xFz/8!4C/gpe=/8/=@&=tq@$//=R=CW p&UCN {}ffeff |{ | J L N;O {(rvNvNj`\j`\ffffJ L NnO {(HQR =/NZPO/O!M8ORB;CPzM&OC&;M=&!F=/fil4C/Mps ef!x <@&4= JML lm NnO2e{(!8bCNKp&F&PC;J L lom N;O <P (!&B/J L lm NnO <P (=Rt&RBCCC;/UBi jkBi jFkB/M/tq@E J L lm N;O ffP (Fz+;&/Rf@4+ PZ!q&K=/M;N/fiC&!=AF/ R J L lm N;O( U 8M;BC;/&/&BC{!R=PUffCfi/4N;==;U&!C;Rez;&N&C& J L lm NnO <P (U E [ G4+k`Cfi/4N;==;k!CN/=/#!4C/gp J L lom N;O {(f@ {( OCt<fi=MBM PK( U 8 ;z++C=/N&qh=/M! E [v G #N=Oq;BM=uq44$@=fi/=ROABCe=/;BU!Z QR !&;&/E G O4&d=/Ff@=fi;fU E 9[. K Gr! C& QR CKUOCR!U=&!fi=/N=uUBK$BnOCNtq|Bk4+@&NtNp=/& 8 B&O=&!=;A/=@&=tq@P/=ROCjz+;&/R!&UzC=;C=/RB/&/F!4+zBA/CMz;&N?}e=!+A!=& c BD E Gef3"| FC$z/N=N/ff/&zd'/>4C$O&!+qKW&!+C W BD efff3"H| +C W BD eefff"A E G2rabcaCbc/8/!zkNp&BK=/@C=RdC;N=N&kOR=/;5@/P/B+C/RhR!B+CNRKokp=fiCON=RB=RVC=/#/=&;+CBP! eff@/N=#=/#;CKC;fOk!BCPz#&48/ qj=/H/!zCRn!&BHeffp&/N=fizfi&R@t>!BABBAN9 efgz/Rtp=fi/=ROABCo=/k&@CBC&/&;NF;A=z/N=N#!/;+qR)o8&OC&;fi=&!C W BD efff3"U E F4[ Grabc!C&fi=;Az/!A/zl7 Q-R q/RR==pB+qPO/gNB#O&!+C W7d BD eefff3"U E [ G [aCbczO<;C=Rfi;4&-2v R@Ceff7fi !fiFNNC"9#C1:;F,@ B E "G:;sdn0vfi2N4=<fifi;> 01=?fifi01P ( 67"fi.01fi2'53Ffi21fi5_60IJ L NnO ffP (Q-RF61:;0c efff3"H|<J L lm NnO < P (rfioAffH9FoffvCKv'4vK v'4Zn59ffn42;?;n;<$'v;Kvn55K 5 z K2'fiff 7v; ' ff < v!9ffn42nvnH75s v'<< !"Z#; %& $ ff' '<5 ()*)Xn; 9 4,+-/.01)K'v#2nKv&/' ! 3X; "A 9456 $ 7 !99`;4n%vn!H )!587v)n9+& $ ff C:;n5#< 4X=7n=%>?-@. ff%A vBDC EFHG&J$ -/.0!2! !vKBDC LNM EF754; n'vO& $ ffQP /'\4n '4+-/.vvHR $ S)nT+!U& $ ;B C L0M EF GVRW$ S)nT>T*B C LNM EF G &0$ ffUX Yv 55 n5&&' /$459ffn42;?;T2'945 3XnZ) !,6 $ ff[P n !v-N) P '\ ff] 2 'v#2nv2' ! 3XnZ) 9 !^6 $ 22'_`bac N;dZAnef g` hig GZjkGVl Inm o)n9>KpB C LNM EF G&N$ ff^ vn5&54 5 Z' H2'qg` hig GZjkGVl Inm ffqA vn5)r/'H !s>:-.Nn)2! vhg GVjkGVl Inm Iut Ng GVjkGVl Im Ivt B C LNM EF G &J$ Ixwh ghih ghiP nvN7,n(yvn 'WWM GVjkGVl Inm{zQ|oI1t B C LNM EF G&N$ ffW8=Y9((HZ%(2=rZ%_n(7GH 1 I^t}U~ 2 1HV=^QJ2=#(H9rxfi(NH[ ` Q rU=(27GH 20r(4S =(4xnK1u *7=GH1 -9.v7 Gfi(2GZl Inm GVl I;t7= GH (I wffv8v<Z)H2'5'jkGZ Ivt (2GZ 28GV I1t ( GV < ;45)v; 'v4 ffP <;q 7v' 5)Z2' M' v 'Kv' 'K2'5''KCn5Z ff < HV K$'v ff;K' 5)8GVl I2 5 4vT2' ff <)4' v 'K'v `nkCn5b7v}<''v /G;$ I% G;$ ff@ zKv288GVl <'TK;< K7nv'vffn 457vCn5Zv '!2' q -[. ffq U7v ^7'vrn !u Gv )K 'S 'vffn2'C ffk H 5Un(yvY7 'evVq<9'vffnCn59G '5v <<v'''2nvv'7Hn5'Cff;42n:?; ; O& $ ffs v `7vp5 "7' '<Zv2 O& $ ff;X 5C#<Cvv ffn7s G . $'K?;CuU<5 () 4<C')n 1 ffuP /' ` n5'9ffn42; )7 'V 5 $v ffnu ff u^)9#< ff) FH2!vUB C F G &N$ IUt ` GH ff?A vO)4v 'X 4}v ` G -@.N;)X9K 4;-vn 'Tv P ' ff)ff;X C9ffvU'< n 5 1?; )Z ff< !\vp!v7vZ),;vG $ . $ 2;G $ . $ 5)v)5)9 v ''T ' v)2' -9. ff <#<' n(yvn 'v Nff 2 Nff ) Cv v9ZNv<H')n( GVl 2o q C' )v*B C F G;$ -[. ff )v '4' Tv:B C F G&J$ -.K2Z & $ 5!9ffn42nOvn ` '8)(! ffP nv<; 5 ;v; )VKH2K JN7 P '@ ff)GVjkGVl Im 8Gl I;t')<'n ffB C LNM EF Gfi ` IutMZ GfiNJvqNNW;0U 2;HNW# fffi fifffi"!# $% &' fi( )*fi,+&ff-=ff+.fi0/fffi2134 56&7n8:9;6 + 6 ff-!%&ff<"!=5?>A@CBD *fiFEHGJILK( <Mff-!finffN<NfiOx%6 9P QRTSffUWVYXZ\[ 8]?^ X_\[ 8]:`bacihkjmln^^cd egf^^p(%qOrst%u%v?wYrx3ys'r2zS%}8~(~JY rxMs ~0}8~(~ } xwyc%{|jcid e~ r } S~Y r sy7 yr%s' }f- e {?RTS U [-XZ\[ 8] X_T[ 8]:`t%u v ]j~ rs%:wYrx3ys'rRS[ B2>]r r r }8~( r }8~. r } xs' } syx3r wYr . wYrffx3rw ( s'3rys3r S. }~L ywYrffx3rwL3{r } s'yHr } } xYs x '} . r [ t%u v ] ( s3r S'S r yxw x3w . yx-'} . r 93r rffy r s'3r 0}8M32 rxYs xYs t%u v [ . .0 x r{~( x3r } ] S'S r yxw r . r ~ s'ys'3r r } r7s } r = r s'3r *[ >] A0y{0RTS' [-X_\[ 8]'][8 ] ys'3r ~.} s's'r S#L[ >]xs'3r } r r ,} S'j{H{lj -O-}8~. yHy r.7}~L ~( r s' } . yx . srxMs s's'3r yx Sff}8 xMs %[-[ 8]']rxYs }8(~ rw{HX_[ 8] yFs } .}8~. ys'3r x r 0}8M32 rxYs xYsy % t%u [ 3r rF[ 8]jX_\[ 8]Y` t%u v ]t%ur } x2s'3r rffy r r ~(~.} S' } xw r ,}*S's'y yx ~ wYr%s' }j{{l{lRTS U [X Z [ 8]t%u%][ ]RTS [ B2>] } xws' } }8~(~ rr2s'r S'0 } r r ~(~ wYrffx3rwjb '"L- -L-(j{'3 r yxs'3r ys3r } xw s' } [ >g]ys' } RS'[ B2>] . x3ys r ~(~ wYrffx3rwxHs' .j{} r r } x r }g x3y x r Y~ [ rr [RT} SL\ rx }]]:y s'3r ,}8Y73 rxMslxYs r S%} } r0wYrffx3rw ~( x3r }*Sg yx S}8 xYs } xw yx ~ wYr0s' } sy S%}8~(~ '} . x3H9x3s'3r yx3x3r yx0rs rrx,w . yx } . x3 9 } xwgy xYsx3r r ''}*S(~ [ >]j{Tx t%u v - r yx ~ wYrs' } L#.2}8~. yHs'3r } r)y S}8~(~t%u v} [} ]y{3ry rs' . r } x s' } x } x S~ w '} . x3t%u v s'3r yy yx X_[ ]i L{x3r r ''}*S(~t%u v . x yx . srxMs s' X _ [ 8] } xw RS U [-X Z [ 8]i X _ [ 8]3`t%u v ] .\}8~. yx3ys{T(~~rwffr3xrw{JJ2TO'0': -:&= M=5 6&: $fi RTS U [ : t%u ]Hj lp(r s'y 3y s' } s3r r . ryrxx3r Yy 3yMy3w yx3ry r3{l}8 x x3fiff s'3r ,}8Y32 rxYs xMs y*J 3 t%u - 0s' } s\rr S' S~w t%u xs' .x3r Yy 3yYy3w2 } [ ]: { 3y rs' .. x3ysTs'3r } r { 3rx,s'3r r .J r r rx rjs' } [ ' ] t%uS~ w 8` } xw ~( U x[]nn8njj{x r2 . } ss'3r r rx[[}}~}}ff37.~}r ]ff !ff]'r r syx3ryxnn8nx } ff . }~ rwxYs '}8. xYs s%r xs'3r ~ 3S r2y s'3r r"ff{,{0rs [ r } rrxYs L}2 yxYs x x yxO] } xw Fff} [} ]Ty3ry r{{$} xw x r2s L% } r L%~ rwt%u[ fi-] t%u`# )y rr S'#` }r ~(~ { ss' . r } x } .\} x x '} r ,}8Y32 rxYs xYs yxMs S} S' s'y%s'3rwYrffx yx} xw }3 3s yxHy } (~({U 211 6 t%u*fi+xs'3r r ,}8 xwYr \s' .7 r yx r2r3ry r P's . 3S r t%u{{2|j` t%u v %\ } xw& :s'yr }# xs'3r } s'r rxYs %s' . s'3ry r }x w ~ rs rs'3r x r,}8Y32 rxMs xMsy* ' t%u{(*)fi+-,/./0214365&1/7/8902:;,/./0=<",/>/?@3-A/3BC./D/714EGFHJILKNMPOQSRLT9U*V9V*V URXWNYZ[IK \/I^]_ILK;`Gacb`ed[]fKhgGdKN]_ijZ[`Gk]lgGm/mnI*gGo_pqd/rpqdstgGd[uvpqdxwzyl{;|/IK `K \/I^]_ILm[gGogGZp}k~pqK_ige] ]_|/jm/K_pq`edCllb`edKhg9pqd[];d/`ed/I^`Ga-KX\/Ib`d[]_KhgGdKN]fijZ[`k@]pqdMylHILKlc ZnIR yI;[oh]fKm/o `GeIlK \[gGKz \[g]zm/o `eZ[gGZp}k}pqK_ilrGpqeILdllyK \/I;a`eo j|kg^ R cO4n=fi[;GztG l G GGLohcl9[IgebK |[g9k}kqifi]f\/`SK \[gGKoh l q Ot/yHILKlRlgGd[uR ZnIK_z`"b`ed[]fKhgGdK]_ijZn`Gk]pqdQSRLT9U*V*V9V UhRXWlYgGd[ub`ed[]puILoROR l; yCINgrg9pqd"|[]fIlK \/INupqo I*bKpqdaILo ILd[bIK I*bX\/dpe|/Iey`eK I^K \[gK;a`eogdic`o_ku`Ga]pqLI"K \/I^m/oX`em[`eoXK_pq`edI/m/o I*] ]pq`ed @ @ G * uILd/`eK I*]I4gebKfkiGh=yKp]K \|[]CI*ge]_iK `]_ILI;K \[gGK-o q @ G n l O6;a`eo-gdi^bX\/`GpbI`Ga yCz\|[]LZi"z\/IL`eo ILj/y@*/o ROR lN Oo ROR lG q /q G * y|/K]pqd[bIR;gGd[uR gGm/m[I*god/`G\/ILo Ipqdl;zIbLgGd|[]_I\/IL`eo ILj[yq;K `#b`ed[bXkq|[uIKX\[gGKR;OtR l; Ot/yKp@]]_K og9pqre\Kfa`eoXgohuK `ILo_p}ai=K \[gGK*]pqd[bIz p@]I*|pGg9kqILdK#KX`xgfi[dpKXIfiup]_|/d[bK_pq`edI*gebX\up]_|/d[bK`ac\pbX\pqj^mk}pI*]R^OR a`ogGK;kqI*ge]_K`d/Im[gSpo`Gab`ed[]_KhgdKh];R^gGd[u!R zIj|[]_Kl\[g9eIX lO4y]cI]_KhgGKXI*upd=eI*bK_pq`ed[y[/`e|/o;reILd/ILohg9kK I*bX\/dpe|/Ia`eoNb`j^m/|/K_pqd/r#K \/Im/oX`eZ[gGZp}k}pK_i`GacgGdgGo ZpqK ogGo ia`eoXj|kg^wp]K `m[gGo K_pqK_pq`edK \/Ilz`eo_ku/]zpqdK `#g[dpqK I^b`Gk}kI*bKfp`dfi`abXkge] ]fI*]]_|[bX\KX\[gGKwZ[IL\[g9eI*]|/dp}a`o jkqi`GeILoI*gb \bXk@g] ]lgGd[u#KX\/ILdK `#b`ej^m/|/K IK \/IoXIk@gK_pqeIcIpqre\Kh]z`a-K \/IbXkge] ]fI*]Ly]zI]_\/`GkgGK ILo*K \/IfibXkge]X]_I*]"go I#I*] ]_ILdK_pg9k}kqiuI[d/I*u&|[]pd/r&b`ej^mkqILK IuI*] bofpm/Kfp`d[]Ly\/IpoIkgGK_pqeIlzIpqre\Kb`eo oXI*]_m[`ed[u/]K `K \/Ilm/o `eZ[gGZp}k}pqK_pqI*];`GaK \/Iup}ILo ILdKb`ej^mkqILK IuI*]Xbo_pqm/K_pq`ed[]rpILdl^y9C2*@*@9lOl "s G G GGS; ^ ohs l @ [* # ;2 9 L GGL- [^s2 [ ^ - l O[l [zlff fi!ff fiV9[#" pqoh]_K*`eZ[]_ILoXeIlK \[gGKzp}a-g9k}kk}pqjpqKh];Ip]_KlgGd[uK \/IuILd/`jpqd[gGK `eozp]d/`ed/LILo `[4K \/ILdoh clVls!#l\im[`eK \/I*]p@] GK \/I-uILd/`ejpqd[gGK `op]pqd[uILI*ud/`ed/LILo `[y"/|/o KX\/ILo j^`eo eI ZilHILj^j"g;{yqeLo cl %$ cl/y'&ILd[bIo zcl Oo cclyIbLgGdK \/ILo Ia`eo Il|[]fIz\/IL`eo ILj 4yq*K `"b`ed[bXkq|[uIK \[gGK_ l Oo _ l ^ VgGo K;g `aK \/Ilm/oX`em[`]pKfp`d#a`Gk}k`G]pqj^j^I*upgGK Ikqiey`^m/o `GeIm[gGo KlZ [oXI*bLg9k}kK \[gKp];I*e|pqGg9kqILdKK `^K \/Iup@]|/d[bK_pq`ed(*) +, yi]pqj^mkqIm/o `eZ[gZp~k}p]_K_pbo I*g]_`edpqd/r[KX\/I"ge] ]_|/jm/K_pq`edK \[gGKo l /gGd[uxm[gGo KgcI^b`ed[bXkq|[uIK \[gGK^s l^s lslV) , ll-/.fi0#13254!687:9<;!=?>/6/1!@7ACBD2=?=E6/1FHGJI/KLKNM!O%P!QSRT/UVWCRXK#YTUK RXKSQLZ[U5Q]\DRQ_^a`]c b IdUefRgK RUihfjlk*mon:pRUYZqWCRXK#IqYT/O%P5rZ[QLZse5Z?KLYtSRP!QSRT/UV\<ZJOqMKSQu^IEv/ZJQ_^IQwWyx kzRXKuv+I+r{RXe|n~}<^5MK[VQL^!ZUM!O%Z[tIQLT/tuT/UQ_^!ZJtSR/^5QL^IUe~KRge5ZaTdQL^5RXKZ?MIQSRT/URXK#K RO%P5rGi:tLjSW#mon:Z[UYZ/VQ_^!ZP!tLT5rZ[OTYT/O%P!M!QNRU!:tofjWsm:tLZ?e5MYZ?KQLTI%KSZ[tNRZ?KTYTO%P!M!QoIQSRT/UKTQ_^!ZT/tLO:t ja m:T/t<v+IdtSRT/MKYTO%P5rZ[QLZwe5Z?KLYtNRP!QNRTUKJnRIdUGKNMYL^~e5Z?KLYtSRP!QSRT/UJn<Z?Y[IErrQL^IQ%Y[IUZie5Z?YTO%PTKNZ?eRU5QLTQL^!tLZ[ZPItLQK[qQL^!ZM!UItLGPItLQq?V|QL^!ZqU!T/U!M!UIdtLGPItLQ%EVIUeQL^!ZqZ?MIErRQGPIt_Q c nqpRUYZJCRXK*RU'hfjl` c b moV\<ZYT/UY_rMe5ZsQ_^IQ:` c b RgKHZ?M5Rv+IErZ[U5Q#QLTw c n:K RU!%}<^!Z[T/t_Z[On?*QS\<RXYZIdUe%KST/O%Z*P!tLT/Id5R{rRXKSQSRXYtLZ?I/KNT/U5RU!V!\<Z/Z[Q?:tofj : : ctofjtofjjtofj:``c { ` c b` cbc b mE:t j ` c bc b mE:tofj molUT/toe5Z[t*QLTiK RO%P5rRG'Q_^!ZstKSQZ!P!tLZ?KLKRTUVtLZ?Y[IErrQL^IdQU!T/U!ZqT]QL^!ZqP!tLZ?eRXY[IQ_ZJKSG5Ow8TrXKRUT!Y[YM!tIdUG5\#^!Z[tLZqRU `]c b ni}<^!Z[tLZT/tLZ/VQL^!ZJP!t_T/I5RrRQSG'Td Rv/Z[U `]c b RgKZ?MIEr3Q_TQL^!Z<P!tLTI5Rr{RQSGqQL^IQQ_^!Z]ZrZ[O%Z[U5QoK:e5Z[U!T/QNRU!Q_^!Z+jeRZ[t_Z[UQ[mYT/UKSQIUQKKLIdQSRXK GsKNT/O%ZPItLQNRgYM5rXItYT/U5/M!tIQSRT/UiTdU!T/U!M!UIdtLGaP!tLT/P8Z[tLQSRZ?K[n<lQ*KS^!T/M5rXeZwY_rZ?It*QL^IQEV!GaKSG5O%O%Z[QLt_G/VIErrKSMY_^YTU5/M!toIQSRT/UKuItLZ%Z?MIErrG'rR/ZrG/n}<^!Z[tLZT/tLZ/VQL^!Z%P!tLTI5Rr{RQSGTIUGT/U!ZfT#QL^!Z[ORXKqIYT/UKSQIUQEVZ?MIErQLTqTv/Z[tQ_^!ZwQ_T/QoIErU5M!OqZ[tT#YT/U5/M!toIdQSRT/UK[n S~ Z[Qe5Z[U!T/QLZqQL^!Z%YT/UKSQoIdUQ\#^5RXY_^aRXK#Z?MIErQ_T tofjf+ ` c b qm:TtIErrn}D^!Z*rgIKSQKSQLZ[PRXK#QLTKS^!T\Q_^IQ?V5R] RXKZ?/M5RvIErZ[UQQLT< < j moV!QL^!Z[Ui jcjE mo:tLj jctofj g j j m? tofj j jc|c8?E?[:t%jjS m[ j mq:tofj jX %E?s jlMK RU!}D^!Z[T/tLZ[Ong//KNZ[ZsZrT\mj? mo}<^!ZtoKNQKNQLZ[PRXK%K RO%P5rG~P!tLT/I5RrRXKSQSRXYtLZ?I/KSTU5RU!8n}D^!ZiKSZ?YT/Ue~KSQLZ[P~MKNZ?KqIdP!P5r{RXY[IQNRTUK%T}<^!Z[T/t_Z[On//nClQRXKaZ?I/KSGQLTKSZ[Z'Q_^IQ jS mqRXKiI~K RO%P5rZ~/M!Z[tLGT/t {l/ j L ?Ej n'Zs\<T/M5rXefrR/ZqQLT%KS^!T\QL^IQ:t%j j jS m::tofj j m{ m: +c L\#^!Z[tLZs}D^!Z[T/tLZ[On/ MKNQSRZ?K<QL^!ZrgIKSQ<Z?/MIErRQSG/n]}TqP!tLTv/Z*QL^!Z#tKSQ<Z?/MIErRQSG/V!\<ZKN^!T+\QL^IdQT/tIErr|VQ_^!ZKSPIYZ?K <IUe Nj <^I?vZ%QL^!ZiKLIOfZ%OJIE5ROwM!OfZ[UQLt_T/PGPTdRU5Q?VUIO%ZrG ns}<^5RXKRXKP!tLTv/Z?eGc I/Y_\#Itoe!KDRUe5MYQSRT/U:QL^!ZY[I/KSZuRXKQLtNRvRXIErrGQ_tLM!Z/n}<^!ZeR{Z[tLZ[UYZZ[QS\<Z[Z[UQL^!ZjamSKSQ%IUe'5QL^~Y[I/KSZRgKfQL^!ZiI/e!e5Z?eYT/USM!UYQ j moVH\#^5RXY_^IO%TM!UQoKQ_TwI/e!eRU!qQL^!Z*U!Z[\YT/UKSQ_toIERUQH n}D^!Z[tLZItLZQ\<TP8TKLK R5RrRQNRZK[n#RtoKSQ?VdR { VSEEoEoEEofiff N H ! [L ! "# $S% '&ff()&fi6'7:89;><=?_ ' ff fifi*,+.-/)0 2 1435'fi@BADC EFCG HfiIKJMLMNPO.QSRTOMUMVNPWXJMLMNZY[JM\M]#QK^MQ`_LMaMUbO.cedfgMhi`kmj ln fpo lrq h l f$gso l iMhtvuwxi l fy n o)isf n iszst n z n i|{ l w}yh~ n o)i l fgMh~ n o)~M~'hify$w4zD|weo)isfuw4~,s)hfo)iMTfgso f$hwefgMho)i|{s|ufo)w4i%' kZ fgso l l iMwxf,fgMhu n4l h4 n i|{o)i|{shh {XfgMhMyw4hyfzth n yhFfy$z4o)iMfwMy$wxhu n i,hF n l h'wxyT}Mffgso l {swh l iMw4f~ n ff$hy e|h u n l hthKfgMhisiMwetmfg n f!ypr :$ p ) r FX) fiyr : r$>X k : fieo)i|uh|wxfgwefifgMhMywM{s|uf l o)ixMh l fo)w4io)i|u$)|{sh n, n uf$w4y xo)fFo l oy$yh)h n isf nxl fw,tFgMhf$gMhyfgMhwxfgMhyfhy~ lFn 4yhh4hu n iiMwetMMfFh4hyzsfgso)iMfw44hfgMhyfwuw4i|u$)|{shf$g n ffiy> Dfi:!|y 4 | r}fi>y r > fie!. x| k j jkMywexoiM n yfhiMwt n {M{syh l$l fgMhXo Mhw2!uw4~MMfo)iMfiy rF>'w4y n n yso)fy n y$z,'w4y~s n Fw[{swfg n feth"~ l f q l fo)ixh l fo) n fh"fgMhF|hg n xo)w4ywe!y rF>'w4y l ~ n :F j fio l w4~,h l su$ohisf)zl ~ n j n i|{)hf,hfgMh l hfFwefi~ n o~M~,'hisfywxz|weo)isf l fi l$l M~,h n i|{j n yh l f n shX'w4yF%Kz[{sh q iso)fo)w4i%fgso l ~,h n l f$g n fw4yh4hy$zmkj thXg n 4h k j % hfhfgMh l hfwe l w4yt"gso#u$gm uw4isf n o)i l fgMh,uw4ieMi|uf,| reoi|uh k j wxy n :k jth~ l fg n 4hf$g n f k fi wxy n % eo)i|uho l"n u$)w l h { l hff$gso l o~so)h l fg n ffgMhyhh l f ll w4~h l |ugf$g n fw4y n :k[j n i|{'w4y n :% thXg n 4h k hfF%|hf$gMhF'w4y~s n)( r)4gMhX'we:)wtoiMMyw4w l o)fo)w4io l iMweth n4l zfwMywe4h4fiff "!$# #%& j #'()#$*,+-/.0' #%&"!$# 2 1( 1K) 1 #%&436 5#'(7#8#$*9:;<4=fi!$,%!y rF>!y rF>. [? fiy r}>> :@ACB )h n)z4l$n flq h l fgMhuw4i|{xo)fo)w4i l B w4yw2 n y$zEDMFHG| n :)wtoiM l fw[uw4i|u$)|{shfg n ffiyer> F4 eo)~o: n)z4zgMhw4yh~IG|KJ0G n i|{Tf$gMh nxll M~,Mfo)w4i l wegMhwxyh~IG|KJL.thu n i}uw4i|u$)|{sh,fg n ffiyeF4eo)i|uhfgMh,uw4ieMi|ufo)w4i}weKftw n4ll hy$fo)w4i l f$g n fFg n 4hMyw4 n so:o)fzMF n l wFg n4l Mywx n so:o)fz Fx thKu n l hgMhw4yhN~ DM FOfwuwxi|u$|{shfg n ffiy rFfiyeF>[[P wetyh u n :fg nf Q l h xso n )hif}fwTf$gMh {xo lMi|ufo)w4SR e z l fy n o)4gfpw4y$t n y{Myw4 n so:o l fouyh n4l w4iso)iM||thu n if$gMhyhwxyhuw4i|u$)|{shfg n ffiy rF[[fiy rF>[fi? !y r}>[[e >z,gMhw4yh~TDMFO n n o)i%sfiye r}> p%e fiye r}>gMh{sh l o)yh {h Myh l$l o)w4i,iMwet'we:wet lhiMwt l o)~,s:o'zf$gMhh Myh l$l o)w4i!y rF>[ p|[%!UVfiWYX[Z]\^`_ba/cde^Xffi_gihjZkdd^Xl8mnonpqrqnfistu-vfiwCxfiyz|{~}b27}fi,}}0]K/8$9y;,}b 2b2]}"$,{Y2]KM Mb {Y7"C;K2H)"$,z8yz2y)8"]bA2"]-)),z`zy]($)"MyKz()HHzA)yzA"yz(HzHyz7-}~2"[,,by8"])y-Hz(;Hl8mnnAw6j(C]()]CA(HC22/~( ( (/HY`AYH(YY(/(2]((0]-](9)]HYH](HA2YHA$/]209Y)(fi Y](4H(H]-Y/Y~H-4yHHHzA"y0( (YH|;fi0(fiA9H8bH97-0|(9]CH`8(((A)()]2A9H47(C(AA2E]K]|fiAC(]-7 2(b(~A)()]($9)]2(~7Ab)YAH9(2(AH$2]H-H(9AH(4(2(]HY9 ( 9;|H|;)/)]]H-Hj] ]9MA9-7]((2 [;](((90(//(C[]A]9A(C 7(H(NY]99H$;AAY|,;),Ayy0H/](]AA(H](C(7$]((9A((Y(4[]AY]A]A4]H9](AH7/A(H]M-](]YY]A]Y$7(H]9AM4(2(b2](A ,;),Y|]9AE -;A A0 (/8AAA(H] ~(j9(fi YH(2 /8A4]H9](AH2A]H(](AH]9(4bA[9H)b`(464bA(H~ 9(A(H]/((H ;9 ]/S8A]H( (C)29)(7] A9H8 M2]/S(H b Ck( /-)-|2(0CY [(Hb k fi~/2 C ]A(]((N0|H7/7]-H9HA)(((]Y 9 (H/Y(7/H (i( 0HHH /A2 E]K M]9Y`$ 7Y9A(H]fi /N(b k2 ]Kfi 7Y$7(T(7($()0]2(Ab2A(H$CH(E99HC(](]C0|j9 jH(H(/b 22~b {Y22]K A2/|(C(7(6 226k)-|~(06b 2 8]K8fi86k$4HA72 8]K8fi 6`A(H]-]M; HA]HC(8HY6Y()2M]KEA]H889A]0 ]MAH4H$,fi(( `(2() YH97(AA(HC9]H0Y0(7A0$(8- ( 6k|70Hk]H(Y(/HH]9( $H( 2() H7(0(kH2 ]K]fi7 A](Y9YA(0b/(C(]--M]]/|]H(0$A0$0|HY`2H]4]Y7(2$/]2/]HY9]4(0]-9 9)- ` j ]9A ]H9H(7/$`8(H(/((H(/((7(Y]987CA 99A0CH/HH7(7(HM;];] ((8)0((H(]7H A`CA(H]|j9 98](A- ]-7YA]E7AA~M]A]fi]Kfi7~(H(A)(A] (EA TH HAb((]-- Y](Y7H8$ 9H](fi(9]--0j]9YY]( 8-fiH(H$)(C(]( $(M]A]Y(4A[(H ]/7C7 /9ff fi%'&*!)(+/. +04=&>< 464 fi @? 44fi;Ffi4ff4 fiff fi fi,ff fi- fi*%3A? 4'45647fi2464 fi 64C4fi#"$2198:4ff4;fi2464 fi=BE fi GFIH@JKfiL6fi*H J fiH JNHOJ P 'QR4 SH@J:0K4 UHOJ6VWXZYfi[2\^]^_`bacd`^e^f _g\^]^_ih:\^j^kla2m^ano]^p^e `bq7rs^tuwvx;vy{z|y~}S}x;}:5676Kx zudx7}yR}^=tZ:x y~y~^S6uw7^6}ffu;CN@7y~wZ}x7}>yly~Sx7}uw@)x7i5767:x;}yi oy+wZtR:x z|z%v^^}Evuw^Sx %x tu^ibuwt+}y}9u= ^u7zo@+^ZK}^xw^s^}y~uwK}x7}9}^^uw^^x;ts^tyZx7})x;t^uw}9Z}y~uw^y~i}^|DOylyC}^EZxwEvZx7+}^+}uw}6x z^EvZt9u72suy~vz~:%x O}uD^uu5+}^)s^tuwsZt}y/u72=xw}^Z=t6zx7}9}u!567ZAyOy/ZsZZ5}Lu72RoK+Zx7=}^Zt6uwt)6uz/+}x;}%}^s^tuwvx;vy{z|y~}Ru;bEuwt+Dy~Z}z~Kzx7tDff7y~wZR}x7}567x7}yK'y+vu^x %xtuwvRuwKy~ZsZZ}:u;i7y~6=}^:s^tuwsZt}y/Lu79x7R6z~ZZ}Kx7>y~}fft6zx7}y~uw}uD7ff;EZx7v!^u5Z'y~ZsZZ5}z~iu7}^Es^tuwsZt}y/Cu7Ox=y|$ZtZ}96z~ZZ5}^}^9y|$ZtZ5}ZwZ5}6%+bff6+ ff2x;txz{zy/ZsZZ5} %@^Zt6uwtw}^9s^tuwvx;vy{z|y~}=}x7}}^Zt!y^uR uw:xy/6z~ZZ5}=x7}:xz{z}x7}A}uwwZ}^ZtOy/}S5767@x7}ylNy:x7}u5}ffL d@y+vuw^^}^=s^tuwvx7vy|z|y~}du;}^=6^}Zy~uwx5y~uwv6y~^xzlw2t6zx7}y/'}u^^'ff;@^Zt)x7t= ^ %x ^2u;}^)^uu5y~^+6z~ZZ }ffZu+}^)s^tuvx7vy|z{y~}=u7A}^x y~uwv6y/^x z9x75^Zt@y~DxCu^6zy@x7}Au} ffo= A@y}Z^A}uE)x2wu}uy~y~}w@^Zt6uw9tw}:^=6 :}Z$y~uwx y~uw7ff$ 5 $N~)xEx5s^}uw}ys^tuwvx;vy{z|y~}K97y~wZ^xw%y~ty~x z|z~w)x;ty~x+su5y~}y~uw'}us^tu7w9@^ZuwtZw^9 7 ZZG+ oG $ %76+ |iGS27 i|9%| wKfi w Z ff 75Z| ff9 $- 5! )ff fi ffG 767"Z7+ ff#ff ff76G$G> 7G &% Z'5w 9 67;');!:567"Z7EG)('7*|, +./#- ! 59Z76+ | 02143 16587fi9;:!< : !=>65! 5w9 5= ?Z @ ffBC wDff ff G7 6$5w EoZ7:wG2GIF H E 9 7 JFG 7C Z5Z|; E7* 5w 5C ff # 6L KNOM |99 5 : PZ^?G+5; QZG ff ff 7GR F UTV5W X9 [Z\ 7]_^a`cbed"gif 7h ]_W ^ajt`cX!bed" f k SbF l knm \po SbFgqh \poZ \) 5L w7+Gw79 ff fi ffG 7r" t62u ^+@y~}^uw^}z~u5u72Z^Ztffx z|y~}}x7} Z}y~uwx z|z2}^6uw}ffx7}+vu7zy~u}x7w} v + /- N xIv ff W tuwsuy~}y~uw{ z+} |bW ~X9 7 ]_^a` W ~X 9:^= =V W ~X 9 ffh\uw}%}x;}@Zx;^^uw}@xwy|z~:}ffxfiw@z|y~+y~}ff@u7 W ~ X9K)7) 2xw G!F wuA}u F vZx7}yA6^s^tW y/u:ZsZ^uw x7L}^%7x z~^u;ZsZ^uw+}^^u7y6%u7 G$F %u7@ZwZtx7s^sz~y/^ tuwsu5y~}y~uw z+N@9wZ}W ~X9 7r ]p^` W X =p W ~X 9 ff\hKKZx7^u7 }6xw}^=z|y~+y~}Kxw GF u5!W }u ^F 9ud ud}yZ W tusu5y~}y~uwz+^@^5suw7r}]_^^a`"b#+f u;}^!F }^ZuwtZy~sz~i}x7W } tX LHiuwtLuw}^ZtOylwA}^'Z^uw+y~x7}uwtZZtu^ff x;t}@x$u7$}^s^tuwsuy~}y~uw:}6z{zA@Zx;+y~w^uwt%}^u5g h k \po S$@uwzvZ \6uwsz~Z})6ty~s^}y~uw%}x7}%x7ty/6uy}Z}%@y~} + /- AZx7=^u7 x7s^sz~!sx7t}vA}u+wZ}@}^y~tz~}fi$q'wq##qVpc##Nc6"n_""#;j)#jC#fijVjCjj4e2j##,jq;"6*jq2"jjUifinjqC4#B#Bl#fie8BrB"B)qBLwBj;jqwqj"Vje"iB4B#L2}qpww#>aejL"q'Bw#"qwBj!q#w4fijjCw#jeBqL#jR#qfij;j4jV"I!i4qCNeR#;"4#ULieq#B#"C;j;"'qB;B"#laefi"j"'Nq")B,jrjN,"2$fii[CwN2"q**#L,"jpj4fij#V'4#jL>aRq[2j#N")qqq2a,wj. #RaeB4"Cwjq2lwBew4Njwwj#,4"#"#ja!"jjqBc#U; Bi4Cw4j6)'q";N#j#2fffffiff 4ff ff qfiLlw 2#"caaV'"LLRB#B4L.#j;j },pi4qj#rqB4BVje"Nq'w#i"'wffff .4'D$!#"$w%fiD'q'&,j ()(+*}-,)*.&*"e!l njj.;q4##q#jw#i"'wB*re#i;/q80ra'jV#/q1;B2 2fiB*;#%3ff.cj#4444Vqrae"#42j#)"BjVVjV4ffe#q*lw;2rBiqeiL#"Rjj#ae#i j4B"} 5qq#5"#i"*eV#2Vjj4675ff5B##i i4Bi"qjIqjB$a"fij)#B#;qjpw#j)q"fij>aC,r98V#D*)j##fi: ff*;$fi*_, <j*>=2D ?.@ *" *A*ae*!B) ;*C=)EF.GH#4ffIq#"effffI3Jqffffq,$q"* /q8qNij##Bff2 w#i%3ffKffMLON PN Q=R* SN (+TqUN *A*63fi2V=*W <RX{3 "#"% Y)ZX[#$# q4Bq#)4'VNB\0R8B'j"."Vr>ff%3#ff eVfiff*3]Vi#"ff#"wW<N,j ()(+*}##B#Bl*N#)jj#Ne"."VN)^%efffi_8Vi N"N#"`<j:"qAYraXi3" RaeB4 N# njjNq4CVNp,^%ffffIb Ljqq)#Lqq4Bq#q7c q#"a4IdqR#2 eq"j4q4jC26I,rNBi!j##fi:ff*#;q),<j*\=2D ?.@*j3 *f*B#*'B) ;=)Eg.GH#qqV4ffSJbfiff fffiq2aeV;nB42 2aeV9/c hff#L,j"!iw*j# !j*fi#*$,)*.&*%"e.;q4#RaeB4jj"w;q4##"};"fiBV")#%3KSpje"B4B2#"BjLq;q#ff*_`<)"E(jff*)Vfilkbm#."ff SJ$ffq"fiBV")B02B"jV/c18b2 !q"lnl%ff *B#fi8e.#4#Bq'#qwqj#"B4Bj <oe\#",2"$+e*Cgl%.5SfiblKS3J:fiqpVq0B2 p /c%3ffff Lw#j #Lj##BqIwBj 6B 6 2iqq"/B# 0l qB#r*#6V)B42 .44eVV;4sq,qjV,).&*%"#!w4"j*+#\:"at)+<%fi(*. Dfi"$#eqV fi.iS JbfiffqRaw)#;jj"2#jnjq8fi"quvfiwxyz:{}|~i{3z:-xyzEx||yb{}.dWl)#b#s#7%l%ff%^ff.Ab.-A[R.lo:ff+.l@..Ab9Zff .R.@a.4.>.)l!l .[}%.f)dW!d - .4 !%dldffff4ffd3f..:l%3ffff%l.ffR.aff).C)3jd.jjds.M@fffff7Z)R)WWf)a)f.)!3f7R)%#AASffl M.bff}Qa.44l!4%3ffff%9d R)b+ffSf.`7ZE3d))ff>fW)WdWdff[W))f#ZsP^E-.)$Q`!fUff>)Wfffd _.W }%ff%E37!))ff) ff$)Rffffff)fffR)Wffd).MffA+.#ff^.)#%'.[-A +ff#:+A7#%@-Q .ff4. 3ffffQ`!fUff>)Wfffd _.W }%ff%E37!))ff) ff$)Rffffff)fff$ffWffd)^`ffA+.$Wa++ -#ff Ao:ff ffA: ):+3fdff:W%ffff3ffffQ.s4l%ff%^ff7dWE.h)fiff%WW %)ff)a.3d$ W3jW)))^).oA@.\.#>^.:A)ffj ff3ff.QSff !#d-3d)44}!d -.j734!#%ff%ffff` .ES d! )) ff)3S )^V.4%@f.+:+#%34ff3bQSff!CQ3dl)4h#C -.jdh!%ffff%RRW$)ffWf!ffdWdff3)ff.jjdWd)Rff+.)ffWff^W.%>ff)-Eff.!QSff!4d"Q3dl)4C#!d4 - dh!M%ff.#%RRW$)ffWf!ffdWdff3)ff.jjdWd)>. ffWffPW%))lff)ffffff#UPMi..d$sfi&%%#ffQ.Afff-3d)4}ff!%ff%lQ.3df^.'ff%W)+ffhdff.f^.C).j dWffC @ff}):+3fdff:(*) bffff}ff3S S.%ffbff%3+ff)E.`7 )ff.'W%.``Wf3 $.fl+ @ff.3, @.- ) }%ffff3bffff3ff.R%Sb%0/)1W%.'ff'E3d)) 32SE#dff Rff!S 4WdUsj%R4 56A7#A) 8 .%$ffA%$M433:ff}4>W h)).!Wfffff[)ff3: R#ffff+9Z) .Wdff3d 9ES d!$+))ffE)b^`3SM)^:Ms%ffff3bffff34#RM%ffffb%-ff)%.`7>.fW)`7`7 .- )ffE37aO W%.)ffR:!Vs#j4-94 % :. ; .\^) )++ < :+ffAAA@ < .:=#+ffAfA+%ffV+ @43ffff 7> -ff))@ ? )Wf.-:}ff.C )%Af .^`) @+ 9ffo jf.4##ffff4ACBfiDFE*G>H@I3J:KML@NCO0I0E@PJ"QSRTGUNCNI0EVWXYX[Z\C]^7_[]>`badceXgf3Z\ih]j@_>k7_3lnmCo0o0p*qn_'rsX[W0tuwvyxzW0\Fcf@f@\iW{|u[}~ciZF\iZCc0W0h>u[h@t_"hyZZX]_[]@uwvi]_[]+`0Fc\iW0@C]:b_Ml!@_Yqn]T+iC7wii'z0z'0+Ci.5n.Y.#1FYn0y#3..z0z5Mn3w@10]0f@f_3mC0_CW0\it>chV1c>x}=ch@h]|chciZW] r7]3|chU\nchvuwivW_V>@@\it]j\_[]a_F_3lnmCo0>qn_"M@Z\iZxzZ\iZhvZ7vXci_M0Y~#'z'n]04l!>qn]@>*0o>>_cf>Xwc0vZ0]M"__'_lnmC0p>qn_nF0Y.*n>Y.+i||z@. _:h@tXYuw#i\nchXwc#u[W0hu+0zY0zn03n~>i|z.]>^W0Z\d:@>XYuwvc#u[W0h.]yZk:W0\i']mCo@m0_'vZ0]'1_^7_[]@`SFcuYc@]a_lmCo0>0q_FF71n.w_uYXgZ0]yZk:W0\_TuXwiW0h]_'lnmo00>qn_":\iWc>uYXuw#uvfiX[W0tuwv_M Mz03izw[0'n]>*]4|m**|_:c\#uw]3j@__[]`:ZhvWe@#c@]@r7_'lnmCo0o>qn_Fhi@Zcf@f>Xuwvc>uYXYuw#Wx:}~c{>ug}7@}Zh>i\iWf|=iWu[h@Z{Uc0v\iZCc0#W0h>u[h@t_Tiz.n'003@|#dT0iC0z7Mn3w@0]'0]m@_"ZCc\#X]:j@_lnmCo00*qn_=+i||zwzMn3w@=wzCg'+3z._~W0\t|chV1c>xz}~ch@h]|ch@\nchvuwivW3] cXYuYx_"ZCc\#X]jU_lnmo00o>qn_:\iW0c>uYXYuw##uwv#Z}~ch|#uwvxzW0\Fh@Wh@}W0h@W0iWh>uvfi\iZCc0#Wh>ugh@t3+r#@\0Z0_+h\nc0v@}~ch]|fi_|j@_[]>'Z0ZCi@Z0]@a_>j@_[]>`MZu[iZ\]*fi_@l!@_Yqn]>e3:4iz.n'0z0'Ci'nn.Y.#1FYC0#3..z0zs*1 00]Mf@f_+0*@mCU_MZf@\#u[h|ZCugh0*F'n.z=*0]>7_@0cxzZ\Fchj@_@"ZCc\XlZC@_Yqn]>W0\it|chV1c>x}=ch@h]'|chU\nchvuwivW] cXYuxi_[]mo0o0@]>f@f_0oo|mCU_"WXYX[W*v']j@_:_4lnmCo@qn_"@W0@h@cugWhxW\du[\iZCvFu[h>xzZ\iZhvZ0_|n=']>C]'p0p@m*p0@_MZuwv@Zh|c0v]a_lmCo|o>q_4|nn~1+i||z0_:Fh>u[0Z\u[!5Wx ceXuYxzW0\ih>uwc=:\iZCi.]*Z\iZXgZ0_0ch@h@W0h] _[]|`$ZCcCZ\C]>_3lnmo|o>qn_4|10|0z0M|#+77|30z_"Fh>u[0Z\u[!=Wx:zXYXYu[h@Wuw1:\iZCi._0c0#\#u]:_lmCo00o>q_^Zxc>X[1\iZCc0#Wh>ugh@tu[h #Z}~ch|#uwvfih@Z!MW0\*Mc xW0\}~cXYu c#u[W0hWx+\iZCvW0th>ugugWhchu[h@@Z\#u[nchvZ0_F Mz03izw[0.C]@| l!>qn]3p00*0@_0f>u[Zt0ZX[cX[iZ\C]'^7_|j@_3lnmo00>qn_':\W0c>uYXYu#uwv1\iZCc0W0h>u[h@tu[h5f@\iZCuwv#u[0Z1Z{@fZ\T*#Z}~_hV1c}~cX]:_e_[]`'Z}}Z\C]0j@_0M_l#+@_Yq]4zw .0i'zC[0.C]f@f_@@0@_FW\ii@aFWXYXwch']3rd}=#iZ\n@c}_"c\n#u]|r7_@lmCo0@mq_b1nz5|CTY. .zfi e7'5FC5l!ph7ZCu[#u[W0h'qn_Fh>u['_@Wx cXYuYxW0\h>uc=:\ZCi_fi